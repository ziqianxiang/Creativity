Under review as a conference paper at ICLR 2022
Understanding the Generalization of Adam in
Learning Neural Networks with Proper Regu-
LARIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods such as Adam have gained increasing popularity in
deep learning optimization. However, it has been observed in many deep learning
applications such as image classification, Adam can converge to a different solution
with a worse test error compared to (stochastic) gradient descent, even with a
fine-tuned regularization. In this paper, we provide a theoretical explanation
for this phenomenon: we show that in the nonconvex setting of learning over-
parameterized two-layer convolutional neural networks starting from the same
random initialization, for a class of data distributions (inspired from image data),
Adam and gradient descent (GD) can converge to different global solutions of the
training objective with provably different generalization errors, even with weight
decay regularization. In contrast, we show that if the training objective is convex,
and the weight decay regularization is employed, any optimization algorithms
including Adam and GD will converge to the same solution if the training is
successful. This suggests that the generalization gap between Adam and SGD
in the presence of explicit regularization is fundamentally tied to the nonconvex
landscape of deep learning optimization, which cannot be covered by the recent
neural tangent kernel (NTK) based analysis.
1 Introduction
Adaptive gradient methods (Duchi et al., 2011; Hinton et al., 2012; Kingma & Ba, 2015; Reddi
et al., 2018) such as Adam are very popular optimizers for training deep neural networks. By
adjusting the learning ratethis coordinate-wisely based on historical gradient information, they are
known to be able to automatically choose appropriate learning rates to achieve fast convergence
in training. Because of this advantage, Adam and its variants are widely used in deep learn-
ing. Despite their fast convergence, adaptive gradient methods have been observed to achieve
worse generalization performance compared with gradient descent and stochastic gradient de-
scent (SGD) (Wilson et al., 2017; Luo et al., 2019; Chen et al., 2020; Zhou et al., 2020) in
many deep learning tasks such as image classification (we have done some simple deep learn-
ing experiments to justify this, the results are reported in Table 1). Even with proper regu-
larization, achieving good test error with adaptive gradient methods seems to be challenging.
Several recent works provided theoretical expla-
nations of this generalization gap between Adam
and GD. Wilson et al. (2017); Agarwal et al.
(2019) considered a setting of linear regression,
and showed that Adam can fail when learning an
overparameterized linear model on certain specif-
ically designed data, while SGD can learn the
linear model to achieve zero test error. This ex-
Models	AlexNet	VGG-16	ResNet-18
SGD	75.22	93.25	94.62
Adam	73.08	92.19	92.93
Table 1: Test accuracy (%) comparison between
Adam and SGD on the CIFAR-10 dataset.
ample in linear regression offers valuable insights into the difference between SGD and Adam.
However, it is under a convex optimization setting, and as we will show in this paper (Theorem 4.2),
the performance difference between Adam and GD can be easily avoided by adding an arbitrarily
small regularization term, because the regularized training loss function is strongly convex and all
algorithms will converge to the same unique global optimum. For this reason, we argue that the
example in the convex setting cannot capture the fundamental differences between GD and Adam.
More recently, Zhou et al. (2020) studied the expected escaping time of Adam and SGD from a local
1
Under review as a conference paper at ICLR 2022
basin, and utilized this to explain the difference between SGD and Adam. However, their results do
not take NN architecture into consideration, and do not provide an analysis of test errors either.
In this paper, we aim at answering the following question
Why is there a generalization gap between Adam and gradient descent in learning neural
networks, even with proper regularization?
Specifically, we study Adam and GD for training neural networks with weight decay regularization
on an image-like data model, and demonstrate the difference between Adam and GD from a feature
learning perspective. We consider a model where the data are generated as a combination of feature
and noise patches under certain sparsity conditions, and analyze the convergence and generalization
of Adam and GD for training a two-layer convolutional neural network (CNN). The contributions of
this paper are summarized as follows.
•	We establish global convergence guarantees for Adam and GD with proper weight decay regular-
ization. We show that, starting at the same random initialization, Adam and GD can both train
a two-layer convolutional neural network to achieve zero training error after polynomially many
iterations, despite the nonconvex optimization landscape.
•	We further show that GD and Adam in fact converge to different global solutions with different
generalization performance: GD can achieve nearly zero test error, while the generalization
performance of the model found by Adam is no better than a random guess. In particular, we show
that the reason for this gap is due to the different training behaviors of Adam and GD: Adam is
more likely to fit noises in the data and output a model that is largely contributed by the noise
patches of the training data; GD prefers to fit training data based on their feature patch and finds
a solution that is mainly composed by the true features. We also illustrate such different training
processes in Figure 1, where it can be seen that the model trained by Adam is clearly more “noisy”
than that trained by SGD.
•	We also show that for convex settings with weight decay regularization, both Adam and gradient
descent converge to the exact same solution and therefore have no test error difference. This
suggests that the difference between Adam and GD cannot be fully explained by linear models or
neural networks trained in the “almost convex” neural tangent kernel (NTK) regime Jacot et al.
(2018); Allen-Zhu et al. (2019b); Du et al. (2019a); Zou et al. (2019); Allen-Zhu et al. (2019a);
Arora et al. (2019a;b); Cao & Gu (2019); Ji & Telgarsky (2020); Chen et al. (2021). It also
demonstrates that the inferior generalization performance of Adam is fundamentally tied to the
nonconvex landscape of deep learning optimization, and cannot be solved by adding regularization.
N	otation. For a scalar x, we use [x]+ to denote max{x, 0}. For a vector v = (v1, . . . , vd)>, we
denote by ∣∣vk2 := (P；=i Vj『1 / its '2-norm, and use supp(v) := {j : Vj = 0} to denote its
support.
2	Related Work
In this section, we discuss the works that are mostly related to our paper.
Generalization gap between Adam and (stochastic) gradient descent. The worse generalization
of Adam compared with SGD has also been observed by some recent works and has motivated new
variants of neural network training algorithms. Keskar & Socher (2017) proposed to switch between
Adam and SGD to achieve better generalization. Merity et al. (2018) proposed a variant of the
averaged stochastic gradient method to achieve good generalization performance for LSTM language
models. Luo et al. (2019) proposed to use dynamic bounds on learning rates to achieve a smooth
transition from adaptive methods to SGD to improve generalization. Our theoretical results for GD
and Adam can also provide theoretical insights into the effectiveness of these empirical studies.
Optimization and generalization guarantees in deep learning. Our work is also closely related to
the recent line of work studying the optimization and generalization guarantees of neural networks.
A series of results have shown the convergence (Jacot et al., 2018; Li & Liang, 2018; Du et al.,
2019b; Allen-Zhu et al., 2019b; Du et al., 2019a; Zou et al., 2019) and generalization (Allen-Zhu
et al., 2019c;a; Arora et al., 2019a;b; Cao & Gu, 2019; Ji & Telgarsky, 2020; Chen et al., 2021)
guarantees in the so-called “neural tangent kernel” (NTK) regime, where the neural network function
is approximately linear in its parameters. Allen-Zhu & Li (2019); Bai & Lee (2019); Allen-Zhu &
Li (2020a); Li et al. (2020) studied the learning of neural networks beyond the NTK regime. Our
analysis in this paper is also beyond NTK, and gives a detailed comparison between GD and Adam.
2
Under review as a conference paper at ICLR 2022
Figure 1: Visualization of the first layer of AlexNet trained by Adam and SGD on the CIFAR-10
dataset. Both algorithms are run for 100 epochs with weight decay regularization and standard data
augmentations, but without batch normalization. Clearly, the model learned by Adam is more “noisy”
than that learned by SGD, implying that Adam is more likely to overfit the noise in the training data.
Feature learning by neural networks. This paper is also closely related to several recent works that
studied how neural networks can learn features. Allen-Zhu & Li (2020b) showed that adversarial
training purifies the learned features by removing certain “dense mixtures” in the hidden layer weights
of the network. Allen-Zhu & Li (2020c) studied how ensemble and knowledge distillation work in
deep learning when the data have “multi-view” features. This paper studies a different aspect of
feature learning by Adam and GD, and shows that GD can learn the features while Adam may fail
even with proper regularization.
3	Problem Setup and Preliminaries
We consider learning a CNN with Adam and GD based on n independent training examples
{(xi, yi)}in=1 generated from a data model D. In the following. we first introduce our data model D,
and then explain our neural network model and the details of the training algorithms.
Data model. We consider a data model where the data inputs consist of feature and noise patches.
Such a data model is motivated by image classification problems where the label of an image usually
only depends on part of an image, and the other parts of the image showing random objects, or
features that belong to other classes, can be considered as noises. When using CNN to fit the data,
the convolution operation is applied to each patch of the data input separately. We claim that our data
model is more practical than those considered in Wilson et al. (2017); Reddi et al. (2018), which are
handcrafted for showing the failure of Adam in term of either convergence or generalization. For
simplicity, we only consider the case where the data consists of one feature patch and one noise patch.
However, our result can be easily extended to cover the setting where there are multiple feature/noise
patches. The detailed definition of our data model is given in Definition 3.1 as follows.
Definition 3.1. Let d = Ω(n4), each data (x,y) with X ∈ R2d and y ∈ {-1,1} is generated as
follows,
X = [X1>,X2>]>,
where one of xι and x2 denotes the feature patch that consists of a feature vector y ∙ v, which is
assumed to be 1-sparse, and the other one denotes the noise patch and consists of a noise vector ξ.
Without loss of generality, we assume v = [1, 0, . . . , 0]>. The noise vector ξ is generated according
to the following process:
1.	Randomly select s coordinates from [d]\{1} with equal probabilities, which is denoted as a vector
s∈{0,1}d.
2.	Generate ξ from distribution N(0, σp2I), and then mask off the first coordinate and other d - s - 1
coordinates, i.e., ξ = ξ s.
3.	Add feature noise to ξ, i.e., ξ = ξ - αyv, where 0 < α < 1 is the strength of the feature noise.
3
Under review as a conference paper at ICLR 2022
In particular, throughout this paper We set S = Θ(嚏),σp = Θ(及小⑺)and α = Θ(σ? ∙
polylog(n) .
The most natural Way to think of our data model is to treat x as the output of some intermediate
layer of a CNN. In literature, Papyan et al. (2017) pointed out that the outputs of an intermediate
layer of a CNN are usually sparse. Yang (2019) also discussed the setting Where the hidden nodes
in such an intermediate layer are sampled independently. This motivates us to study sparse features
and entry-Wisely independent noises in our model. In this paper, We focus on the case Where the
feature vector v is 1-sparse and the noise vector is s-sparse for simplicity. HoWever, these sparsity
assumptions can be generalized to the settings Where the feature and the noises are denser.
Moreover, We Would like to clarify that the data distribution considered in our paper is an extreme
case Where We assume there is only one feature vector and all data has a feature noise, since We
believe this is the simplest model that captures the fundamental difference betWeen Adam and SGD.
With this data model, We aim to shoW Why Adam and SGD perform differently. Our theoretical results
and analysis techniques can also be extended to more practical settings Where there are multiple
feature vectors and multiple patches, each data can either contain a single feature or multiple features,
together With pure random noise or feature noise.
Two-layer CNN model. We consider a tWo-layer CNN model F using truncated polynomial
activation function σ(z) = (max{0, z})q and fixed the Weights of second layer to be all 1’s, Where
q ≥ 3. Mathematically, given the data (x, y), the j-th output of the CNN can be formulated as
mm
Fj (W, X) = X [σ(hwj,r, XIi) + σ(hwj,r, x2川=X [σ(<Wj,r,y ∙ Vi) + σ(hwj,r , ξi儿 (3.1)
r=1	r=1
Where m is the Width of the netWork, Wj,r ∈ Rd denotes the Weight at the r-th neuron, and W
is the collection of model Weights. Here the use of the polynomial ReLU activation function is
for simplifying our analysis. It can be replaced by a smoothed ReLU activation function (e.g., the
activation function used in Allen-Zhu & Li (2020c)). If We assume the input data distribution is
Gaussian, We can also deal With ReLU activation function (Li et al., 2020). Moreover, We Would
like to emphasize that X1 and X2 denote tWo data patches, Which are randomly assigned With feature
vector or noise vector independently for each data point. The leaner has no knoWledge about Which
one is the feature patch (or noise patch).
In this paper We assume the Width of the netWork is polylogarithmic in the training sample size, i.e.,
m = polylog(n). We assume j ∈ {-1, 1} in order to make the logit index be consistent With the
data label. Moreover, We assume that the each Weight is initialized from a random draW of Gaussian
random variable 〜N(0,σ0) withσo = Θ(d-1/4).
Training objective. Given the training data {(Xi, yi)}i=1,...,n, we consider to learn the model
parameter W by optimizing the empirical loss function with weight decay regularization
1n	λ
L(W) = - ∑ Li(W) + 2kWkF,	(3.2)
n i=1
where Li(W) = 一 log ——eFyi (WFi(W,x ,)denotes the individual loss for the data (xi, yi) and λ ≥ 0
j∈{-1,1} ej
is the regularization parameter. In particular, the regularization parameter can be arbitrary as long as
it satisfies λ ∈ (0,λ0) with λ0 = Θ( d(q-i)/4n1poiyiog(n)). We claim that the λ° is the largest feasible
regularization parameter that the training process will not stuck at the origin point (recall that L(W)
admits zero gradient at W = 0.)
Training algorithms. In this paper, we consider gradient descent and Adam with full gradient.
In particular, starting from initialization W(0) = {Wj(,0r), j = {±-}, r ∈ [m]}, the gradient descent
update rule is
wjt+1) = Wjtr- η∙Vw.L(W⑴)，
where η is the learning rate. Meanwhile, Adam store historical gradient information in the momentum
m(t) and a vector V(t) as follows
mjt+1) = βιm(? + (1 - βι) ∙Vw,rL(W㈤)，	(3.3)
4
Under review as a conference paper at ICLR 2022
vjt+1) = β2vjtr + (1- β2) ∙ [Vwj,rL(W㈤)]2,	(3.4)
and entry-wisely adjusts the learning rate:
w(t+1) = W⑴-η ∙ m⑴/\甲	(3 5)
wj,r wj,r η mj,r/ j,	()
where β1,β2 are the hyperparameters of Adam (a popular choice in practice is βι = 0.9, and
β2 = 0.99), and in (3.4) and (3.5), the square (∙)2, square root √, and division •/∙ all denote entry-
wise calculations. We do not consider the initialization bias correction in the original Adam paper
(Kingma & Ba, 2015) for the ease of analysis.
4 Main Results
In this section we will state the main theorems in this paper. We first provide the learning guarantees
of Adam and Gradient descent for training a two-layer CNN model in the following theorem. Recall
that in this setting the training objective is nonconvex.
Theorem 4.1 (Nonconvex setting). Consider a two-layer CNN defined in (3.1) with d = Ω(n4)
and regularized training objective (3.2) with a regularization parameter λ > 0, suppose the network
width is m = polylog(n) and the data distribution follows Definition 3.1, then we have the following
guarantees on the training and test errors for the models trained by Adam and Gradient descent:
1.	Suppose We run Adam for T = Polns) iterations with η = p。,.), then with probability at least
1 一 O(n-1), we can find a NN model WAdam such that k VL(WAdam)||i ≤ Tn. Moreover, the
model WAdam also satisfies:
•	Training error is zero: 1 Pn=I 1 [FyKWAdam，Xi) ≤ F-yi (WAdam, Xi)] = 0.
•	TeSterrOriShigh: P(χ,y)〜D [Fy (WAdam，x) ≤ F-y (WAdam，x)] ≥ 2 .
2.	Suppose we run gradient descent for T = Poln(n) iterations with learning rate η = poly(n), then
with probability at least 1 一 O(n-1), we can findaNN model WGD such that k VL(WGD)IlF ≤
T1n. Moreover, the model WGD also satisfies:
•	Training error is zero: n Pn=I 1 [Fyi (WGd, Xi) ≤ Ff (WGd，Xi)] = 0.
•	TeSterrOriSnearIyZerO: P(χ,y)〜D [Fy(WGd, χ) ≤ F-y(WGd, x)] = PoIn).
From the optimization perspective, Theorem 4.1 shows that both Adam and GD can be guaranteed to
find a point with a very small gradient, which can also achieve zero classification error on the training
data. Moreover, it can be seen that given the same iteration number T and learning rate η, Adam can
be guaranteed to find a point with up to 1/(T η) gradient norm in `1 metric, while gradient descent
can only be guaranteed to find a point with up to 1/√Tη gradient norm in '2 metric. This suggests
that Adam could enjoy a faster convergence rate compared to SGD in the training process, which is
consistent with the practice findings. We would also like to point out that there is no contradiction
between our result and the recent work (Reddi et al., 2019) showing that Adam can fail to converge,
as the counterexample in Reddi et al. (2019) is for the online version of Adam, while we study the
full batch Adam.
In terms of the test performance, their generalization abilities are largely different, even with weight
decay regularization. In particular, the output of gradient descent can generalize well and achieve
nearly zero test error, while the output of Adam gives nearly 1/2 test error. In fact, this gap is due
to two major aspects of the training process: (1) At the early stage of training where weight decay
exhibits negligible effect, Adam and GD behave very differently. In particular, Adam prefers the
data patch of lower sparsity and thus tends to fit the noise vectors ξ, gradient descent prefers the data
patch of larger `2 norm and thus will learn the feature patch; (2) At the late stage of training where
the weight decay regularization cannot be ignored, both Adam and gradient descent will be enforced
to converge to a local minimum of the regularized objective, which maintains the pattern learned in
the early stage. Consequently, the model learned by Adam will be biased towards the noise patch
to fit the feature noise vector -αyv, which is opposite in direction to the true feature vector and
therefore leads to a test error no better than a random guess. More details about the training behaviors
of Adam and gradient descent are given in Section 5.
5
Under review as a conference paper at ICLR 2022
Theorem 4.1 shows that when optimizing a nonconvex training objective, Adam and gradient descent
will converge to different global solutions with different generalization errors, even with weight
decay regularization. In comparison, the following theorem gives the learning guarantees of Adam
and gradient descent when optimizing convex and smooth training objectives (e.g., linear model
F(w, x) = w>x with logistic loss).
Theorem 4.2 (Convex setting). For any convex and smooth training objective with positive reg-
Ularization parameter λ, suppose We run Adam and gradient descent for T = Polns) iterations,
then with probability at least 1 - n-1, the obtained parameters WAdam and WGD satisfy that
l∣VL(WA dam)k1 ≤ T1n and ∣∣VL(wA dam)k2 ≤ 吉 respectively. Moreover, let F(W, X) ∈ R be
the output of the convex model with parameter W and input x, it holds that:
•	Training errors are both zero:
nn
—X 1 Isgn(F(WAdam, Xi)) = yi] = — X 1 Isgn(F(WGD, Xi)) = yi] = 0.
n i=1	n i=1
• Test errors are nearly the same:
P(x,y)〜D [sgn(F(WAdam, Xi)) = y] = P(x,y)〜D [sgn(F(WGD, x)) = y] ± VPOlyS).
Theorem 4.2 shows that when optimizing a convex and smooth training objective (e.g., a linear model
with logistic loss) with weight decay regularization, both Adam and gradient can converge to almost
the same solution and enjoy very similar generalization performance. Combining this result and
Theorem 4.1, it is clear that the inferior generalization performance is closely tied to the nonconvex
landscape of deep learning, and cannot be understood by standard weight decay regularization.
5	Proof Outline of the Main Results
In this section we provide the proof sketch of Theorem 4.1 and explain the different generalization
abilities of the models found by gradient descent and Adam.
Before moving to the proof of main results, we first give the following lemma which shows that for
data generated from the data distribution D in Definition 3.1, with high probability all noise vectors
{ξi}i=1,...,n have nearly disjoint supports.
Lemma 5.1. Let {(Xi, yi)}i=1,...,n be the training dataset generated by Definition 3.1. Moreover,
recall that Xi = [y", ξ>]> (or Xi = [ξ>,yiv>]>), let Bi = supp(ξi)∖{1} be the support of ξi
except the first coordinate. Then with probability at least 1 -n-2, Bi ∩Bj = 0 for all i = j.
This lemma implies that the optimization of each coordinate of the model parameter W, except for
the first one, is mostly determined by only one training data. Technically, this lemma can greatly
simplify the analysis for Adam so that we can better illustrate its optimization behavior and explain
the generalization performance gap between Adam and gradient descent.
Proof outline. For both Adam and gradient descent, we will show that the training process can be
decomposed into two stages. In the first stage, which we call pattern learning stage, the weight decay
regularization will be less important and can be ignored, while the algorithms tend to learn the pattern
from the training data. In particular, we will show that the patterns learned by these two algorithms
are different: Adam tends to fit the noise patch while gradient descent will mainly learn the feature
patch. In the second stage, which we call it regularization stage, the effect of regularization cannot be
neglected, which will regularize the algorithm to converge at some local stationary points. However,
due to the nonconvex landscape of the training objective, the pattern learned in the first stage will
remain unchanged, even when running an infinitely number of iterations.
5.1	Proof sketch for Adam
Recall that in each iteration of Adam, the model weight is updated by using a moving-averaged
gradient, normalized by a moving average of the historical gradient squares. As pointed out in Balles
& Hennig (2018); Bernstein et al. (2018), Adam behaves similarly to sign gradient descent (signGD)
when using sufficiently small step size or the moving average parameters β1 , β2 are nearly zero.
This motivates us to study the optimization behavior of signGD and then extends it to Adam using
their similarities. In particular, sign gradient descent updates the model parameter according to the
6
Under review as a conference paper at ICLR 2022
following rule:
wjt+1) = wjt+1) - η ∙ Sgn(Vwj,rL(W㈤)).
Recall that each data has two patches: feature patch and noise patch. By Lemma 5.1 and the
data distribution (see Definition 3.1), we know that all noise vectors {ξi}i=1,...,n are supported on
disjoint coordinates, except the first one. For data point xi , let Bi denote its support, except the
first coordinate. In the subsequent analysis, we will always assume that those Bi ’s are disjoint, i.e.,
Bi ∩Bj = 0 if i = j.
Next we will characterize two aspects of the training process: feature learning and noise memoriza-
tion. Mathematically, We will focus on two quantities: (w(?,j ∙ Vi and XSi,r, ξi}. In particular,
given the training data (xi, yi) with Xi = [yiV>, ξ>]>, larger (w)i,r, yi ∙ Vi implies better feature
learning and larger hw(yti),r, ξii represents better noise memorization.
Stage I: Learning the pattern. Mathematically, the first stage is defined as the iterations that the
neural network output is smaller than some constant. In this stage, all training data remains under-
fitted and can provide large gradient for model training, and the effect of weight decay regularization
can be ignored due to our choice of λ. We will show that in this stage the inner product hwy(ti),r, ξii
grows much faster than hw(j,tr),jVi since feature learning only makes use of the first coordinate of
the gradient, while noise memorization could take advantage of all the coordinates in Bi (note that
|Bi | = s 1).
Lemma 5.2 (General results in Stage I). Suppose the training data is generated according to Defi-
nition 3.1, assume λ = o(σ0-σp∕n) and η = 1∕poly(d), then for any t ≤ T0 with T0 = 0(η^)
and any i ∈ [n],
hwjt+1),j ∙ vi ≤ hwjtr,j ∙ vi + θ㈤，
hw(yti+,r1),ξii = hwy(ti),r,ξii + Θe(ηsσp).
Since hwj?, ξii enjoys much faster increasing rate than that of hwj?, j ∙ v)，after a certain number
of iterations, the learning of noise patch will dominate the learning of feature patch. Thus, the model
will tend to fit the feature noise in the noise patch, leading to a flipped feature learning phenomenon.
Lemma 5.3 (Flipping the feature learning). Suppose the training data is generated according to
Definition 3.1, α ≥ Θ((sσp)1-q ∨ σ*-1) and σo < O((sσp)-1), then for any t ∈ [Tr,To] with
Tr= 0(而令E) ≤ T0,
hwjt+1), j ∙ v) = hw(tr, j ∙ v) - Θ(η).
Moreover, it holds that
∖ -Sgnj) ∙ ωc⅛),	~	k = 1,
wjT0)[k] =	∖ sgn(ξi[k]) ∙ Ω(sσ-)	or ± <0(η),	k ∈ Bi,	with yi	= j,
1±O(η),	otherwise.
From Lemma 5.3 it can be observed that at the iteration T0, the sign of the first coordinate of wj(,Tr0 )
is different from that of the true feature, i.e., j ∙ v. This implies that at the end of the first training
stage, the model is biased towards the noise patch to fit the feature noise.
Stage II: Regularizing the model. In this stage, as the neural network output becomes larger, part
of training data starts to be well fitted and gives smaller gradient. As a consequence, the feature
learning and noise memorization processes will be slowed down and the weight decay regularization
term cannot be ignored. However, although weight decay regularization can prevent the model weight
from being too large, it will maintain the pattern learned in Stage I and cannot push the model back
to “forget” the noise and learn the feature and stops at some local stationary points. We summarize
these results in the following lemma.
7
Under review as a conference paper at ICLR 2022
Lemma 5.4 (Maintain the pattern). If α = O[sσpln and η = o(λ), then let r* =
arg maxr∈[m] hwy(ti),r, ξii, for any t ≥ T0, i ∈ [n], j ∈ [2] and r ∈ [m], it holds that
hwyi,r* , ξii = θ⑴，X |wy?r*[k]HEi[k]| = θ⑴，
k∈Bi
VT ∈ [m],	hwjtr, sgn(j) ∙ v)∈ [-o(1), O(λ-1η)].
Lemma 5.4 shows that in the second stage, hw)i,r, ξi) will always be large while 川乩,yi ∙ v) is
still negative, or positive but extremely small. Next we will show that within polynomial steps, the
algorithm can be guaranteed to find a point with small gradient.
Lemma 5.5 (Convergence guarantee). If η = O(d-1/2), then for any t it holds that
L(W(HI)) — L(W⑶)≤ -ηkvL(w(t))kι + Θ(η2d).
Lemma 5.5 shows that We can pick a sufficiently small η and T = poly(n)∕η to ensure that the
algorithm can find a point with up to O(1/(T η)) in `1 norm. Then we can show that given the results
in Lemma 5.4, the formula of the algorithm output W* can be precisely characterized, which We
can show that(w* r, yi ∙ v) < 0, This implies that the output model will be biased to fit the feature
yi,
noise -αyv but not the true one v. Then when it comes to a fresh test example the model will fail to
recognize its true feature. Also note that the noise in the test data is nearly independent of the noise
in training data. Consequently, the model will not be able to identify the label of the test data and
therefore cannot be better than a random guess.
5.2	Proof sketch for gradient descent
Similar to the proof for Adam, we also decompose the entire training process into two stages.
Stage I: Learning the pattern. In this stage the gradient from training loss function is large and
and the effect of regularization can be ignored. Unlike Adam that is sensitive to the sparsity of the
feature vector or noise vector, gradient descent is more focusing on the `2 norm of them, where the
vector (which can be either feature vector or noise vector) with larger `2 norm is more likely to be
discovered and learnt by GD. Note that the feature vector has a larger `2 norm than the noise, we can
show that, in the following lemma, gradient descent will learn the feature vector very quickly, while
barely tend to memorize the noise.
Lemma 5.6. Let	Ajt)	= maXr∈m]hwjt+D,j ∙	v),	Γj,)	= maxym] (wj),ξi),	and	Γjt)	=
maxi：y.=j Γj?. Let Tj be the iteration number that Ajt) reaches Θ(1∕m) = Θ(1), then We have
Tj =Θe(σ02-q) for all j ∈ {-1, 1}.
Moreover, let T0 = maxj {Tj}, then for all t ≤ T0 it holds that Γ(jt) = Oe(σ0) for all j ∈ {-1, 1}.
Stage II: Regularizing the model. Similar to Lemma 5.4, we show that in the second stage at which
the impact of weight decay regularization cannot be ignored, the pattern of the training data learned
in the first stage will remain unchanged.
Lemma 5.7. If η ≤ O(σ0), it holds that A(jt) = Θe (1) and Γ(jt) = Oe(σ0) for all t ≥ minj Tj.
The following lemma further shows that within polynomial steps, gradient descent is guaranteed to
find a point with small gradient.
Lemma 5.8. If the learning rate satisfies η = o(1), then for any t ≥ 0 it holds that
L(W(HI)) — L(W㈤)≤ — η∣∣VL(W㈤)kF.
Lemma 5.8 shows that we can pick a sufficiently small η and T = poly(n)∕η to ensure that gradient
descent can find a point with up to O(1∕(Tη)1/2) in '2 norm. By Lemma 5.7, it is clear that the
output model of GD can well learn the feature vector while memorizing nearly nothing from the
noise vectors, which can therefore achieve nearly zero test error.
6	Experiments
In this section we perform numerical experiments on the synthetic data generated according to
Definition 3.1 to verify our main results. In particular, we set the problem dimension d = 1000, the
8
Under review as a conference paper at ICLR 2022
(a) Adam
# Iterations
(b) GD
Figure 2: Visualization of the feature learning
(mini maxrhw1,r, ξii) in the training process.
(maxr hw1,r, vi) and noise memorization
training sample size n = 200 (100 positive examples and 100 negative examples), feature vector
v = [1, 0, . . . , 0]>, noise sparsity s = 0.1d = 100, standard deviation of noise σp = 1/s1/2 = 0.1,
feature noise strength α = 0.2, initialization scaling σ0 = 0.01, regularization parameter λ =
1 × 10-5, network width m = 20, activation function σ(z) = max{0, z}3, total iteration number
T = 1 × 104, and the learning rate η = 5 × 10-5 for Adam (default choices ofβ1 and β2 in pytorch),
η = 0.02 for GD.
We first report the training error and test error achieved by the solutions found by SGD and
Adam in Table 2, where the test error is calculated on a test dataset of size 104. It is clear
that both Adam and SGD can achieve zero training error, while they have entirely different
results on the test data: SGD generalizes well and achieve zero test error; Adam generalizes
worse than SGD and gives > 0.5 test error, which verifies our main result (Theorem 4.1).
Moreover, we also calculate the inner products:
maxr hw1,r, vi and mini maxr hw1,r, ξii, representing
feature learning and noise memorization respectively, to
verify our key lemmas. Here we only consider positive ex-
amples as the results for negative examples are similar. The
results are reported in Figure 2. For Adam, from Figure
2(a), it can be seen that the algorithm will perform feature
learning in the first few iterations and then entirely forget
Algorithm	Adam	SGD
Training error	0	0
Test error	0.884	0
Table 2: Training and test errors
achieved by GD and Adam.
the feature (but fit feature noise), i.e., the feature learning is flipped, which verifies Lemma 5.3. In the
meanwhile, the noise memorization happens in the entire training process and enjoys much faster rate
than feature learning, which verifies Lemma 5.2. In addition, we can also observe that there are two
stages for the increasing of mini maxrhw1,r, ξii: in the first stage mini maxrhw1,r, ξii increases
linearly, and in the second stage its increasing speed gradually slows down and mini maxr hw1,r, ξii
will remain in a constant order. This verifies Lemma 5.2 and Lemma 5.4. For GD, from Figure 2(b),
it can be seen that the feature learning will dominate the noise memorization: feature learning will
increases to a constant in the first stage and then remains in a constant order in the second stage;
noise memorization will keep in a low level which is nearly the same as that at the initialization. This
verifies Lemmas 5.6 and 5.7.
7	Conclusion and Future Work
In this paper, we study the generalization of Adam and compare it with gradient descent. We show
that when training neural networks, Adam and GD starting from the same initialization can converge
to different global solutions of the training objective with significantly different generalization errors,
even with proper regularization. Our analysis reveals the fundamental difference between Adam and
GD in learning features, and demonstrates that this difference is tied to the nonconvex optimization
landscape of neural networks. Built up on the results in this paper, there are several important research
directions. First, our current result is for two-layer networks. Extending the results to deep networks
could be an important next step, where we will not only look at the input data but also consider the
output of each intermediate layer as “input”. Second, our current data model is motivated by the
image data, where Adam has been observed to perform worse than SGD in terms of generalization.
Studying other types of data such as natural language data, where Adam is often observed to perform
better than SGD, is another future work direction.
9
Under review as a conference paper at ICLR 2022
Ethics statement
This paper mainly concerns the theoretical understanding of the behaviors of different optimization
algorithms in deep learning. We don’t see any potential ethical issues in our work.
Reproducibility statement
Our theoretical results can be reproduced according to the assumptions and problem set up stated
in Section 3 and the detailed proof provided in the appendix. The experimental results shown in
Table 1 and Figure 1 are obtained via standard training (using SGD or Adam) on CIFAR-10 dataset
(Krizhevsky et al., 2009). Our experimental results in Figure 2 and Table 2 can also be reproduced
according to hyperparameters described in Section 6.
References
Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. Revisiting the generaliza-
tion of adaptive gradient methods. 2019.
Zeyuan Allen-Zhu and Yuanzhi Li. What can ResNet learn efficiently, going beyond kernels? In
Advances in Neural Information Processing Systems, 2019.
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020a.
Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190, 2020b.
Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020c.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in Neural Information Processing Systems,
2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In Advances in Neural Information Processing Systems, 2019c.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, 2019b.
Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of wide
neural networks. In International Conference on Learning Representations, 2019.
Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic
gradients. In International Conference on Machine Learning, pp. 404-413. PMLR, 2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd:
Compressed optimisation for non-convex problems. In International Conference on Machine
Learning, pp. 560-569. PMLR, 2018.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in Neural Information Processing Systems, 2019.
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing the
generalization gap of adaptive gradient methods in training deep neural networks. In International
Joint Conferences on Artificial Intelligence, 2020.
10
Under review as a conference paper at ICLR 2022
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is
sufficient to learn deep relu networks? In International Conference on Learning Representations,
2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent, 2012.
ArthUr Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
Representations, 2020.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-8166,
2018.
Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, pp. 2613-2682. PMLR, 2020.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm
language models. In International Conference on Learning Representations, 2018.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via
convolutional sparse coding. The Journal of Machine Learning Research, 18(1):2887-2938, 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4151-4161, 2017.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
11
Under review as a conference paper at ICLR 2022
Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards theoretically
understanding why sgd generalizes better than adam in deep learning. Advances in Neural
Information Processing Systems, 33, 2020.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning, Oct 2019.
12
Under review as a conference paper at ICLR 2022
A Discussions
In this section, we would like to reflect on some possible extensions of our work to more general
settings.
Mini-batch stochastic gradients. One natural extension of our paper is proving the separation
between mini-batch SGD and mini-batch Adam, which we believe is not difficult. In particular, it
can be shown that the learning behavior of SGD will be largely similar to that of GD since using
mini-batch stochastic gradients does not change the rates of feature learning and noise memorization.
In contrast, compared to full-batch Adam, the rate of mini-batch Adam in memorizing noise will
be scaled down by at most n times (due to the preconditioning in Adam), while the rate of feature
learning remains the same. However, the difference between SGD and mini-batch Adam still exists
(though the difference might be smaller). This is because, in our paper, the separation between Adam
and GD is characterized by a poly(d) factor: the speed of feature learning in Adam and GD, and the
rate of noise memorization in GD are both in the order of O(η) (in each step), while the rate of noise
memorization in Adam is proportional to the number of nonzero entries, which is in the order of
η ∙ poly(d). In contrast, using mini-batch stochastic gradients can only reduce the difference by at
most an O(n) factor, which is still dominated by the poly(d) separation as long as the dimension d is
sufficiently large (overparameterized).
Learnable second layer. Our results and analyses can also be extended to the case where the second
layer is trained. The reason behind this is that the difference between Adam and GD comes from the
learning of the hidden layer, while the second layer weights will not affect which patch and neuron is
more likely to be learned by different optimization algorithms. Consequently, we can still get the
result that GD tends to learn the feature patch while Adam tends to learn the noise patch.
B	Proof of Theorem 4.1: Nonconvex Case
In the beginning of the proof we first present the following useful lemma.
B.1	Preliminaries
We first recall the magnitude of all parameters:
d = Poly⑺，η = P01⅛), S = θ (二)，σp = θ ( S ∙ Polylog(n))，σ2 = θ ( d⅛)，
m = PolylogS)，α = θ(σP ∙ PolylogS)), λ = O(d(q-1)∕4 n IPolylog(n)).
Here Poly(n) denotes a polynomial function ofn with degree ofa sufficiently large constant, Poly(n)
denotes a polynomial function of log(n) with degree of a sufficiently large constant. Based on the
parameter configuration, we claim that the following equations hold, which will be frequently used in
the subsequent proof.
α = ω((sσp)1-q σq-1),
σ0 = o
Lemma B.1 (Non-overlap support). Let {(xi, yi)}i=1,...,n be the training dataset sampled according
to Definition 3.1. Moreover, let Bi = supp(ξi)∖{1} be the support of Xi except the first coordinate1.
Then with probability at least 1 - n-2, Bi ∩ Bj = 0 for all i, j ∈ [n].
ProofofLemma B.1. For any fixed k ∈ [n] and j ∈ supp(ξk)∖{1}, by the model assumption we
have
P{(ξi)j 6=0}=S/(d-1),
for all i ∈ [n]\{k}. Therefore by the fact that the data samples are independent, we have
P(∃i ∈ [n]\{k} : (ξi)j 6=0) =1- [1 - S/(d - 1)]n.
Applying a union bound over all k ∈ [n] and j ∈ supp(ξk)∖{1}, we obtain
P(∃k ∈ [n],j ∈ supp(ξk)∖{1}, i ∈ [n]∖{k} : (ξi) =0) ≤ n ∙ S ∙{1 - [1 - s/(d - 1)]n}. (B.1)
1Recall that all data inputs have nonzero first coordinate by Definition 3.1
13
Under review as a conference paper at ICLR 2022
By the data distribution assumption We have S ≤ √d∕(2n2), which clearly implies s/(d - 1) ≤ 1/2.
Therefore we have
n ∙ s ∙ [1 — (1 — s∕d)n] = n ∙ S ∙{1 — exp[nlog(1 — s∕(d — 1))]}
≤ n ∙ s • [1 — exp(n ∙ 2s∕(d - 1))]
≤ n ∙ s ∙ [1 — exp(n ∙ 4s∕d)]
≤ n ∙ s ∙ (4ns∕d)
= 4n2s2∕d
≤ n-2,
where the first inequality follows by the inequalities log(1 — z) ≥ —2z for z ∈ [0, 1∕2], the
second inequality follows by s∕(d — 1) ≥ 2s∕d, the third inequality follows by the inequality
1 — exp(—z) ≤ z for z ∈ R, and the last inequality follows by the assumption that s ≤ √d∕(2n2).
Plugging the bound above into (B.1) finishes the proof.
□
B.2	Proof for Adam
Before moving to the detailed proof, we first state the update rules of feature learning and noise
memorization when the sign gradient is applied.
hwj-t+1),j Vi = Hjr ,j vi — η ∙ <sgn(Vwj,rL(W㈤)),jv>
=Hjr, j Vi + jη ∙ sgn( X yi 'jt)[σ0 (hwjtr ,yivi) — ασ0(hwj?, ξi))] —nλwj? [1]),
i=1	(B.2)
where `(jt,i) := 1yi=j —logitj (F, xi) and logitj (F, xi)
Fj (W,xi)
e
Pk∈{-ι,ι}eFk(W,χi)
From (B.2) we can
observe three terms in the signed gradient. Specifically, the first term represents the gradient over the
feature patch, the second term stems from the feature noise term in the noise patch (see Definition
3.1), and the last term is the gradient of the weight decay regularization. On the other hand, the
memorization of the noise vector ξi can be described by the following update rule,
hwyt+υ, ξii =〈wy?r, ξii—η <gn(Vwyi,,L(W㈤)),ξi>
=hwyi),r, ξi i+η ∙ X(sgn 卜 y?” ((Wyi,r,&〉凌肉-nʌw)i,r"&网)
k∈Bi
—αyiη ∙ sgn( X 9£?』/((Wy/yi V〉)—ασ0 (hWy?r, &川-nλwy2r[1]).
i=1	(B.3)
In this subsection we first provide the following lemma that shows for most of the coordinate (with
slightly large gradient), the Adam update is similar to signGD update (up to some constant factors).
In the remaining proof for Adam, we will largely apply this lemma to get a signGD-like result for
Adam (similar to the technical lemmas in Section 5). Besides, the proofs for all lemmas in Section 5
can be viewed as a simplified version of the proofs for technical lemmas for Adam, thus are omitted
in the paper.
Lemma B.2 (Closeness to SignGD). Recall the update rule of Adam, let W(t) be the t-th iterate of
the Adam algorithm. Suppose that hWj(,tr), vi, hWj(,tr), ξii = Θe (1) for all j ∈ {±1} and r ∈ [m]. Then
if β2 ≥ β12 , we have
• For all k ∈ [d],
m(tr 肉
≤ Θ(1).
14
Under review as a conference paper at ICLR 2022
•	For every k ∈ ∪n=ιBi (including k = 1) we have either ∣Vwj,rL(W(t))[k]∣ ≤ Θ(η) or
mjtr /
Sgn(Vwj/(W⑴)[k]) ∙ Θ(1).
•	For every k ∈ Bi, we have ∣Vwj,rL(W⑴)[k]∣ ≤ <θ(ηn-1 sσp∖'(j,'[[) ≤ <Θ(ηsσp) or
HS[k]
sgn(Vw3,rL(W(t')[k]) ∙ θ(1).
Proof. First recall that the gradient Vwj,r L(W(t' ) can be calculated as
Vwj,r L(W(t')[l] = - -
σ0(hwj(,tr',ξii) + λwj(,tr' [-].
nn
Vw3,r L(W(t)) = - - X yi jiσ(hwj ,yi Vi) ∙ v + X 'ji ∙ σ0(hwj⅛yiξii) ∙ ξi + λwj).
n i=1	i=1
More specifically, for the first coordinate of Vwj,r L(W(t' ), we have
n
yijMhw-yivi) - α X yi'ji' ∙
i=1
(B.4)
For any k ∈ Bi, by Lemma B.1 we know that the gradient over this coordinate only depends on the
training data ξi , therefore, we have
Vwj,r L(W(t')[k] = --'j)σ0(hwjr, ξii)ξi[k] + λwjr [k].	(B.5)
For the remaining coordinates, we have
Vwj,rL(W(t')[k] =λwj(,tr'[k].	(B.6)
Now let us focus on the moving averaged gradient m(jt,r' and squared gradient vj(,tr' . We first show that
for all k ∈ [d], it holds that
Imjr 网
≤ Θ().
(B.7)
By the update rule of mj(,tr' , we have
mjr[k] = βιmj,-1'[k] + (1 - βι) ∙ Vwj,rL(W(t')[k]
t
=Xβτ(1 - βι) ∙Vwj,rL(W(t-τ')[k].
τ=0
Similarly, we also have
t
Vjr[k] = Xβτ(1 - β2) ∙Vwj,rL(W(t-τ')[k]2.
τ=0
Then by Cauchy-Schwartz inequality we have
(mjtr [k])2 ≤ (X ≡α-≡ ∙Vwj,r LW-')[k]2) ∙(X ɑT).
Let α- = [β1T(1ι-β2') , which forms an exponentially decaying sequence if β2 ≥ β2∙ Therefore, we
have Ptτ=0 ατ2 = Θ(1) and the above inequality implies that
(mj,r [k])2 ≤ Vjr [k] ∙ θ(d,
15
Under review as a conference paper at ICLR 2022
which proves (B.7).
Now we are going to prove the main argument of this lemma. Note that m(jt,r), which is a weighted
average of all historical gradients, where the weights decay exponentially fast, then we can take on
a threshold τ = polylog(η-1) such that PT=T βτ (1 - βι) = PolyIn_1)∙ Then for each k ∈ [d] We
have
T	t
mj,r[k] = Xβτ(1 - βι) ∙ Vwj,rLN(I))[k] + X βT(1 - βι) ∙ Vwj,L(W(t-τ))[k]
T =0	T =TT
TT	1
=X βT (1 - βl) ∙ VWj,r L(W(t-T ))[k] ± Poyn-J ,
Where in the last inequality We use the fact that |Vwj,r L(W(t-T))[k]| = Oe(1) for all k ∈ [d].
Similarly, We can also have the folloWing on vj(,tr),
TT	1
Vjtr [k]=X βτ (1 - β2) ∙ vwj,r LN(I ))[k]2 ± Poyn-J.
Here we slightly abuse the notation by using the same τ. Then we have
mj,r[k] = PT=0 βτ(1 - βι) ∙Vwj,rL(W(t-T))[k] ± poynFπ
qVj⅛i	qPT=τ βτ (1-β2) ∙Vwj,r Zl ))[k]2 ± poy⅛π .
In order to prove the main argument of this lemma, the key is to show that within T iterations,
the gradient Vwj,r L(N(t))[k] barely changes. In particular, by (B.7), we have the update of each
coordinate in one step is at most Θ(n). This implies that
IhWjr, Vi - hw(r, Vil ≤ e(nT),
IhWjtr, ξii - hwjr, ξiiι ≤ θ(nτsσp),
Wjtr[k] - W(T)[k]| ≤ θ(nτ).
Then applying the fact that ∣(wjT), v〉| ≤ Θ(1) and KWjT), ξii∣ ≤ Θ(1), we further have
IFj(W(T), Xi)- Fj(W(t), Xi)∣ ≤ Θ(mnτsσp) = Θ(ητsσp),
where we use the fact that m = Θ(1) and sσp = ω(1). Then it holds that
)=	eFj(W(T k)
j,i = Pk∈{-u} eFk(W(T),xi)
eFj (W(t) ,xi)+Θe (ηTTsσp)
^≤
eFj (W(T) ,xi)+Θe(ηTTsσp) + eF-j (W(t) ,xi)IΘe (ηTTsσp)
=sgn('jt)) ∙ θ(∣'jti |),
where we use the fact that Θ(nTsσp) = o(1). Similarly, we can also show that 'j? ≥ sgn('jt)) ∙
θ(∣'j? |), which further implies
'j? = sgn('j,)) ∙ θ(∣'j⅛)
for all τ ∈ [t - T, t]. Note that ∣'jτ | ≤ 1, then it holds that
'j”(hwjT), Vi) = sgn('jt)) ∙。(噌|) ∙ σ0(hwj? Vi)
≤ sgn('jt)) ∙。(噌|) ∙ σ0(hwjtr, Vi) +。吗|) ∙ θ(nT).
16
Under review as a conference paper at ICLR 2022
We can also similarly derive the following
47σ0((WjT), Vi) ≥ sgn('jt)) ∙ θ(I'jt)D ∙ σ0((Wjtr, Vi)- θ(Ij) I) ∙ θ (ητ),
4”((WjT), ξii) ≤ Sgna)) ∙ θ(ι'j√ι) ∙ σ0(hwjtr, ξii)+θ(ι'(t)1) ∙ θ (ητsσp),
'j√σ0(hw(-T), ξii) ≥ Sgna)) ∙。(|戏|) ∙ (Ahwjr, ξii) -。(嘿)∙ θ (ητsσp).
Combining the above results, applying (B.4), (B.5), and (B.6), we can show that for the first coordi-
nate, we have
Vwj,rL(W(T))[1] = Θ(Vwj,rL(w(t))[i]) ±θ(1 X∣'jt)∣) ∙O(ητ)±Θ(λητ);
i=1
for any k ∈ Bj , we have
Vwj,rL(W(T))[k] = θ(vwj,rL(W(t))[k]) ± θ( j) ∙ O(ητsσp)± Θ(ληT)
and for remaining coordinates, we have
Vwj,rL(W(T))[k] = Θ Vwj,rL(W(t))[k] ± Θ(λητe).
Now We can plug the above results into the formula of mj? and v(?. Using the fact that τ = Θ ⑴,
λ = o(1), and ∣'(t) ∣ ≤ 1, we have for all k = 1 or k ∈ Bi for any i,
m(j,tr) [k]
Vwj,rL(W(t))[k] ±Θe(η)
Vj(,tr)[k]	Θ IVwj,rL(W(t))[k]I ±Θe(η))
For k ∈ Bi we have
mj,r 回
Vwj,r L(W㈤)[k] ± Θ( ηsσpj ) ± Θ (λη)
Vtriki	θ(∣Vwj,r L(W㈤)[k]l) ± Θ( ηsσpj) ± Θ (λη)
Then, we can conclude that for all k = 1 or k ∈/ Bi for any i, we have either IVwj r L(W(t))[k]I ≤
Θ(η) or
mj,r [k]
Vj(,tr)[k]
Sgn(Vwj,4(W⑴)[k]) ∙ Θ(1).
For any k ∈ Bi, we have either ∣Vwj,rL(W(t))[k]| ≤ Θ(ηn-1sσp∣'jt)∣ + λη) or
m(jt,r)[k]
Vj(,tr)[k]
Sgn(Vwj/(W⑴)[k]) ∙ Θ(1).
This completes the proof.
Lemma B.3 (Lemma 5.2, restated). Suppose the training data is generated according to Definition
3.1, assume λ = o(σq-2σp∕n) and η = 1∕poly(d), then for any t ≤ To with To = O( nsσp) and
any i ∈ [n],
hwjt+1),j ∙ vi ≤ hwjtr,j ∙ vi + θ(η),
(W(yti+,r1),ξii = (Wy(ti),r,ξii + Θe(ηsσp).
17
Under review as a conference paper at ICLR 2022
Proof. At the initialization, we have
KWj0), v>∣ = Θ (σo),	KWj0), ξiil = Θ (S1/2bpb0 + α) = Θ (S1"σpσ0), wj0)[k] = Θ (σ0),
which also imply that |'j0) | = Θ(1). Besides, note that 'jt) = 1j=yi -Iogitj-(F㈤，xi), We have
sgn(yi'j?) = sgn(j),
where we recall that j ∈ {-1, 1}. Therefore, given that λ = o(σ0q T), α = o⑴，S1∕2σp = O(1),
and assume `(jt,i) = Θ(1) (which will be verified later),
nn
sgn( Xyi'j”(hWjtr,yivi)- αXyi'jti)σ0(hWjtr,Q- nλwj1[1])
i=1	i=1
=Sgn j ∙ Θ(nσq-1) - j ∙ Θ(αn(s1/7σpσo')cq-1') ± o(σq-1σp)]
= sgn(j).
Since v is 1-sparse, then by Lemma B.2, the following inequality naturally holds,
hwjt+1), j ∙ v〉≤ hwjtr, j ∙ v)- n〈mj? / qv(r,j f ≤ hwjtr, j ∙ v)+日⑹.
Additionally, in terms of the memorization of noise, we first consider the iterate in the initialization.
By the condition that η = o(1∕d) = o(1∕(sσp)) and note that for a sufficiently large fraction of
k / B. (e.g., 0.99), we have ∣ξi[k]∣ ≥ Θ(σ?) ≥ θ(ηn-1sσp∣'j0i)∣) and thus
Sgn(Vwyi,rL(W(0))[k]) = sgn«y?第0回去,ξii)ξi[k] - 〃入亚，0),同)
=-sgn[Θ((d1∕2σpσ0)q-1σp ∙ sgn(ξi[k])) ± o(σq-1 σp)] = -sgn(ξi[k]).
(B.8)
Therefore, by Lemma B.2 we have the following according to (B.3),
hwyi)r, ξii = hwy0)r, ξii -ηDmj,r IqVt,, “
≥ hwy0)r, ξii +θ(η) ∙ X hsgn(ξi[k]), ξi[k]i - o(ηsσp) - o(ηα)
k∈Bi
= hW(y0i,)r,ξi) +Θe (ηSσp),
where in the first inequality the term O(ηsσp) represents the coordinates that ∣ξi[k]∣ ≤ O(σp) (so
that we cannot use the sign information of Vyi,rL(W(0)) but directly bound it by Θ(1)) and the last
inequality is due to the fact that |Bi| ≥ S - 1 and α = o(1). For general t, we will consider the
following induction hypothesis:
hWy(ti+,r1),ξi) = hWy(ti),r,ξi) + Θe (ηSσp),	(B.9)
which has already been verified for t = 0. By Hypothesis (B.9), the following holds at time t,
hwyi),r, ξi i = hwy0)r , ξii +θ (tηsσp) = θ (S1∕2σpσ0 + tηsσp).
In the meanwhile, we have the following upper bound for |Wj(,tr) [k]|,
∣w.[k]∣ ≤ ∣wjtr[k]| + nl sign(Vwj,rL(W(t)))∣ ≤ ∣wj0)[k]| + tη = Θ(σ° + tη).	(B.10)
Besides, it is also easy to verify that for any t ≤ To = Θ(sσp1ηm) = Θ(册),we have
hWyir, ξi), hWyt!,r, j ∙ v) < Θ(1∕m) and thus ∣'jt)∣ = Θ(1). Then similar to (B.8), we have
Sgn(Vwyi,rL(W(t))[k])
=Sgn 卜 y?”(hwy?r, ξ i)ξi[k] - nλwy0)r[k])
18
Under review as a conference paper at ICLR 2022
=-sgn(Θ[(s/σpσo + tηsσp)q-1σp ∙ sgn(ξi[k])) ± o(σq-2σp ∙ (σ° + 切))]
= -sgn(ξi[k]).	(B.11)
This further implies that
hwyi+r1), ξii ≥ hwy?”&i-。㈤∙ X hsgn(Vwy”rL(w ㈤)肉),&肉〉-。55句)-。皿)
k∈Bi
= hwy(ti),r,ξii + Θe (ηsσp),
where the term -O(η2s2σp2) is contributed by the gradient coordinates that are smaller than Θ(ηsσp).
This verifies Hypothesis (B.9) at time t and thus completes the proof.	□
From Lemma B.3, note that sσ? = ω(1), then it can be seen thathwj?,j ∙ Vi increases much faster
thanhwj?,j ∙ V). By looking at the update rule ofhwj? ,j ∙ Vi (see (B.2)), it will keeps increasing
only when, roughly speaking, σ0(hwjt),j ∙ v)) > ασ0(hwjt), ξi)). Since hwjt), ξii increases much
faster than hwjt),j ∙ v), it can be anticipated after a certain number of iterations, hwjt),j ∙ v) will
start to decrease. In the following lemma, we provide an upper bound on the iteration number such
that this decreasing occurs.
Lemma B.4 (Lemma B.4, restated). Suppose the training data is generated according to Definition
3.1, α ≥ Θ((sσp)1-q ∨ σ*-1) and σ0 < O((sσp)-1), then for any t ∈ [Tr,To] with Tr =
O(ηsσpθσi0∕(q-1) ) ≤ T0,
hwjt+1), j ∙ v) = hw(tr,j ∙ V)- Θ(η).
Moreover, it holds that
∖	-Sgncj) ∙ ω(ɪ),	~	k = 1,
WjT0)[k]=彳	sgn(ξi[k]) ∙ Ω(ɪ)	or ± O(η),	k ∈ Bi,	with yi	= j,
1±O(η),	otherwise.
Proof. Recall from Lemma B.3 that for any t ≤ T0 we have
hwjtr+1), j ∙ v) ≤ hwjtr, j ∙ V) + Θ(η) ≤ hwj0), j ∙ v) + Θ(tη),
hw(yts+,r1), ξs) = hwy(ts),r,ξs) + Θe cηsσp) ≤ hw(y0s),r,ξs) + Θectηsσp).
Besides, by Lemma B.2 we also have |wj(,tr) [k]| ≤ |wj(,0r) [k]| + Octη). Then it can be verified that for
some Tr = O(ηsσpασ0∕(q-i)), we have for all i ∈ [n] and t ∈ [Tr , T0]
ασ'(hwy∙,r , ξi)) ≥ C ∙ [σ0(hwjtr,j ∙ v)) + λnlwjtr [1]|]
for some constant C. This further implies that
Sgn(Vwj,r L(W ㈤)[1])
nn
=-sgn( Xyi'jtiσ'(hwjtr,yiv))- αXyi'j"(hw(tr,ξi))- nλwjt,')[1]j
i=1	i=1
n
=-sgn[-α Xyi'j}σ'chwjtr, ξi))]
i=1
= Sgn(j),
where we use the fact that sgn(yi'jt)) = sgn(j) for all i ∈ [n]. Then by Lemma B.2 and (B.2), we
have for all t ∈ [Tr , T0],
hwjt+1),j ∙ v) = hwjtr,j ∙ v) - Θ(η) ∙ sgn(j) ∙ Sgn(Vw,「L(W(t))[1]) = hwjtr,j ∙ v) - Θ(η).
19
Under review as a conference paper at ICLR 2022
Then at iteration T0, for the first coordinate we have
1
wj： [1] = Wjr [1]+sgn(j) ∙ Θ(Trη) -sgn(j) ∙ Θ((T° - Tr)η) ≥ -sgn(j) ∙ Ω 一
j,r	j,r	sσp
For any k ∈ Bi with yi = j, we have either the coordinate will increase at a rate of Θ(1) or fall into
0. As a consequence we have either wj(,Tr0) [k] ∈ [-Θe (η), Θe (η)] or
WjT0)[k] = wj0)[k] +sgn(ξi[k]) ∙ Θ(Tοη) ≥ sgn(ξi[k]) ∙ Ω ).
For the remaining coordinate, its update will be determined by the regularization term, which will
finally fall into the region around zero since we have T0η = ω(σ0). By Lemma B.2 it is clear that
WjT0)[k] ∈ [-Θ(η), Θ(η)].	□
2
Lemma B.5 (Lemma 5.4, restated). If a = O(^p) and η = o(λ), then let r* =
arg maxr∈[m] hwy(ti),r, ξii, for any t ≥ T0, i ∈ [n], j ∈ [2] and r ∈ [m], it holds that
hwSLr* , ξii = θ⑴，X wi,r/k]l lξi[k]1 = θ⑴，
k∈Bi
Vr ∈ [m],	hwjtr, sgn(j) ∙ Vi ∈ [-O(n2),O(λ-1η)].
sσp
Proof. The proof will be relying on the following three induction hypothesis:
Mr, ξii =Ω (1),	(B.12)
X ∣wyi*k]∣∙∣ξi[k]∣ =Θ⑴，	(B.13)
k∈Bi
∀r ∈ [m], hwj>,sgn(j) ∙ v)∈ [- <5(n2),O(λ-1η)],	(B.14)
which we assume they hold for all τ ≤ t and r ∈ [m], i ∈ [n], and j ∈ [2]. It is clear that all
hypothesis hold when t = T0 according to Lemma B.4.
Verifying Hypothesis (B.12). We first verify Hypothesis (B.12). Recall that the update rule for
hw(yti),r , ξii is given as follows,
hwyiv), ξii
=hwyi,r , ξii - η ∙ gi,r/ VZV弁,r, ξi>
≥ hwy-,r,ξi) - θ(η) ∙ SgMVwyihLW))),Wi- θ(η2s2σp)
=hwyi,r, ξii+θ(η) ∙ x (Sgn 卜:?"(〈必?”&i)&网 - .必?『问),&同)
k∈Bi
nn
-ayiΘ(η) ∙ Sgnf X yi'^iσ0 (hw(r ,yv» - α X "£”©]?, ξii) - nλwj? [1])
i=1	i=1
- Θe(η2s2σp2).	(B.15)
Note that for any a and b We have sgn(a - b) ∙ a ≥ |a| - 2|b|. Then it follows that
X (sgndghwy?” ξii)ξi[k]-nλwyi)r[k]),&网)≥ X (层阿-,(”党?、
k∈Bi ∖	∖	k k	k∈Bi ×	'管普 (hwy,ξii)
nλ
≥ θ(sσp)- θ(F),
'%i,iσp/
20
Under review as a conference paper at ICLR 2022
where the last inequality follows from Hypothesis (B.12) and (B.13). Further recall that λ
o(σq-2σp∕n), plugging the above inequality to (B.15) gives
hwy(ti+,r1),ξii ≥ hwy(ti),r,ξii + Θe(ηsσp)
θ	)- θWs2σp)
'%i,iσp/
—
≥ hwy(ti),r,ξii + Θe(ηsσp)
Θ(αη) - Θ
(B.16)
—
〜
Then it is clear that hw(yti),
,r, ξii will increase by Θ(ηsσp) if `
(yt),i is larger than some constant of
q-2
order Ω(%)=Ω(^0^). We will first show that as soon as there is a iterate W(T) satisfying
'yτ,i ≤ O(脸)for some T ≤ t, then it must hold that ')： i Will also be smaller than some constant
in the order of O(%)for all τ0 ∈ [τ,t + 1]. To prove this, We first note that if '[) reaches some
constant in the order of O(*),We have for all r ∈ [m] by (B.16)
hwy(ti+,r1),ξii ≥ hwy(ti),r,ξii + Θe(ηsσp),
hw(-ty+i1,r),ξii ≤ hw-(t)yi,r,ξii + O(αη),
|hwj(,tr+1), vi| ≤ |hw(j,tr),vi| + O(η).	(B.17)
Therefore, We have
(t+1)
yi,i
eF-yi(W(t+1),xi)
j∈{-1,1}
eFj(w(t+1),Xi)
≤
≤
1
ι+eχp [Pm=ι [σ ((Wyi+1), Vi)+σ((Wyi+1), Wi) -。((W-E, Vi)-仪河"痣,良川
1
1+eχp [Pm=1 [σ(hwyi,r, Vi) + σ((Wyi),r, Wi) -σ((W-Lr, vi) - σ(hw-)yi,r, ξi川 +θ (^sσp)]
1
1+eχp [Pm=1 [o((Wy?r, vi) + σ((Wyi),r, ξii)] -σ((Wl¾i,r, vi ) - σ((W2,r , ξii)]]
where inequality follows from (B.17). Therefore, this implies that as long as，：，is larger than some
constant b = O(Sλ), then the adam algorithm will prevent it from further increasing. Besides, since
mησp2 = o(1), then we must have g：]1 ∈ 恒方媲/ 2以力.As a consequence, we can deduce that
'yt,i cannot be larger than 2b, since otherwise there must exists a iterate W(T) with T ≤ t such that
,：；∈ [b, 2b] and 力：+ 1) ≥ £：；, which contradicts the fact that 力：；should decreases if £：； ≥ b.
Therefore, we can claim that if £'r)i ≤ b = O(*) for some T ≤ t, then we have
yi ,i	sσp
(B.18)
for all T0 ∈ [T, t + 1]. Then further note that
*1)
eF-yi (W(t),xi)
Pj∈{-1,1} eFj(w(t),xi)
m
≥ exp - X [σ((Wy(ti),r, yiVi) + σ((Wy(ti),r, ξii)]
r=1
≥ exp - Θ m max
r∈[m]
σ((Wy(ti),r, ξii))),
(B.19)
21
Under review as a conference paper at ICLR 2022
where in the last inequality we use Hypothesis (B.14). Then by the fact that 4i+1) ≤ O sn2)=
o⑴ and m = Θ⑴，it is clear that exp ( - Θ(mmaxr∈m]。(〈亚!：+I), ξii))) = o⑴ so that
maxr∈[m] hwy：+1), ξii = Ω⑴.This verifies Hypothesis (B.12).
Verifying Hypothesis (B.13). Now we will verify Hypothesis (B.13). First, note that we have
already shown that hw)：+1?, ξii = Ω ⑴ so it holds that
X wyi+⅛∙ ∣ξi[k]∣+αwyi+1)[i]∣ ≥ hwy：" Wi = ω ⑴.
k∈Bi
By Hypothesis (B.14), we have |亚]：；?[1]| ≤ |w，?r* [1]| + η = o(1). Besides, since each coordinate
in ξi is a Gaussian random variable, then maxk∈Bi ∣ξi[k]∣ = O(σp). This immediately implies that
X iwy：+r*)[k]i ∙ ∣ξi[k]∣ = ω ⑴.
k∈Bi
Then we will prove the upper bound of Pk∈a∕ w)：+1) [k]| ∙ ∣ξi [k]|. Recall that by Lemma B.2, for
any k ∈ Bi such that RwyWrL(W(t))[k] ≥ Θ(n-1 ηsσp'yt)i), we have
w：+1) [k] = w)i),r [k] + θ㈤∙ Sgn (yi,iσ0((Wy：,r , Eii)&网 - nʌw:),r [k]).
Note that by Lemma B.4, for every k ∈ Bi, we have either WyTp[k] = sgn(ξi[k]) ∙ θ(去)or
|wy(T：,0r) [k]| ≤ η. Then during the training process after T0, we have either sgn(wy(t：),r[k]) = sgn(ξi [k])
or sgn(ξi[k]) ∙ wyt),r ≥ -O(η) since if for some iteration number t0 that we have Sgn(Wy：[[k])=
-sgn(ξi [k]) but Sgn(Wy：-1)[k]) = sgn(ξi [k]), then after τ = O(1) steps (see the proof of Lemma
B.2 for the definition of τ) in the constant number of steps the gradient will must be in the same
direction of ξi[k], which will push wy：,r[k] back to zero or become positive along the direction of
ξi[k]. Therefore, based on this property we have the following regarding the inner product hwy(t：),r, ξii,
hwy：,r , ξii =	X	wy：),r [k] ∙ ξi[k]
k∈Bi∪{1}
≥ X	|wy：),r[k]| ∙ ∣ξi[k]∣ - Ο(η) ∙ X	∣ξi[k]∣
kEB：U{1}	kEB：U{1}
=X	|wy：),r [k]| ∙ lξi[k]1 - <e(ηsσP),
kEB：U{1}
where the second inequality follows from the fact that the entry w(yt：),r[k] that has different sign of ξi [k]
satisfies |wy：)r [k]| ≤ 0⑺.ThenIet Bf)= Pj∈B2∪{1} Iwy?r[k] ∙ 1(IWy2r[k]l≥ Om))I ∙ lξi[k]1,
which satisfies Bi(T0 ) = Θe (1) by Lemma B.4. Then assume Bi(t) keeps increasing and reaches some
value in the order of Θ log(dnη-1) , it holds that according to the inequality above
(wy：)r, ξii = θ(log(dnη-1)) - θ(ηsσp) = θ(log(dnη-1)),
where we use the condition that η = Ο (sσp)-1 . Then by Hypothesis (B.12) and (B.14) we know
that lhwj> , vi| = o(1), hwy：,r* , ξii = ω ⑴,and KW(Jyi ,r* , ξiil = 0(切)+。|劭纥尸,vi| = o ⑴
then similar to (B.19), it holds that
eF-y：(W(t),x：)
* = P-------------Fj (W(t) x：) ≤ exP ( - θ(σ(hwy2r* , ξii))) ≤ Poly(d- ,n- ,η).
j∈{-1,1} e j , ：
Therefore, at this time we have for all k ∈ Bi ,
'y：%h(wy?r, ξii)ξi[k] ≤ poly(d-1 ,n-1,η) ∙ θ( logq-1(dnη-1)) ∙ θ(σp) ≤ nλη.
22
Under review as a conference paper at ICLR 2022
Then for all |wy(ti),r[k]| ≥ Oe(η), the sign of the gradient satisfies
Sgn(Vwyi,,L(Wc))[k]) = -Sgn (y?”((Wy?r, ξii)ξi[k] - nλwyi,r [kf)
= sgn(nλη -w(yti),r[k])
= Sgn(W(yti),r [k]).
Then note that ∣Vwyi,,L(W㈤)[k]∣ =。(|人亚£?「肉|) ≥ Θ(n-1ηsσp'^i + λη), by the update
rule of W(yti),r [k] and Lemma B.2, we know the sign gradient will dominate the update process.
Then We have ∣wyi++1)[k]∣ = ∣wyi)r T 一 Θ(η) ∙ Sgn(W!?/同)| ≤ 皿!?/[k]|, which implies that
Wi)r [k] ∙ 1(Wy?r[k]| ≥ O(η))∣ decreases so that Bitt also decreases. Therefore, We can conclude
that Bit) will not exceed θ(log(dnη-1)). Then combining the results for all i ∈ [n] gives
X |wy?r*[k『 lξi[k]1 ≤ Bit) + O(SησP) ≤ θ(log(dnηT)) + O(I) = θ⑴,
k∈Bi
where in the first inequality we again use the condition that η = o(1/d) = o (sσp)-1 . This verifies
Hypothesis (B.13). Notably, this also implies that hwyiti),r* , ξii = maxr∈[m] hwyiti),r, ξii ≤ Θe (1).
Verifying Hypothesis (B.14). In order to verify Hypothesis (B.14), let us first recall the update
rule of hwji,tr), vi:
hwji,tr+1)
Then by Lemma B.2, we know that if |Vwj,r L(Wit))[1]| ≤ Θe(η), then |mij,tr) / vji,tr) | ≤ Θ(1) and
otherwise
m--7=v, V) = -Sgn(Xyi'j”(hwj1,yM) - αXyi'j”(hwjtr,ξii) - nxwjt41]) ∙ θ(1)∙
vji,tr)	i=1	i=1
Without loss of generality we assume j = 1, then by Lemma B.4 we know that WITO)[1] = -Ω(十).
In the remaining proof, we will show that either w1it,+r 1) [1] ∈ [0, Θe (λ-1η)] or w1it,+r 1) [1] ∈	-
O(碟ɑ), 0)∙
First we will show that w1it,+r 1) [1] ∈ [0, Θe (λ-1η)] for all r. Note that in the beginning of this stage,
we have w1iT,r0) [1] < 0. In order to make the sign of wi1t,0r) [1] flip, we must have, in some iteration
t0 ≤ t that satisfies w1it,0r) [1] ∈ [0, Θe (λ-1η)], therefore
nn
-nvwι,rL(W(t0))[1] = X y'""(hwjtLyV) -α X yi'j,iσ(hw(；r, ξii) - nXwjtr[1]
i=1	i=1
≤ n[(wjtr)[1])q-2 - λ] ∙ wjtr)[1] ≤ -Θ(nη) ≤ 0,
where the second inequality holds since η = o(X(qT)/(q-2)). Note that ∣Vwι,r L(W(t0))[1]] ≥ Θ (η),
then by Lemma B.2 we know that Adam is similar to sign gradient descent and thus w1it,0r+1) [1] =
w1(t,0r) [1] - Θ(η) which starts to decrease. This implies that if w1(t,r+1) [1] is positive, then it cannot
exceed Θ(λ-1η) = o(1).
Then we can prove that if w(t++1)[1] is negative, then ∣w(t++1)[1]∣ = O(筌).In this case we have for
all t0 ≤ t,
nn
-nvw(t)L(W(t‘ ))[ι]=X yi `:" (WSr,yv)一a X 犯仪?σ (hwtr, ξii) - nʌwfr)[1]
1,r
i=1	i=1
23
Under review as a conference paper at ICLR 2022
≥-
≥-
X l'1ti)∣∙ Θ (α) + nλ∣w(t? [1]| + X ∖'(5∖-∖w^[1]∖q-1,
i:yi=1	i:yi=-1
X ∖'1ti)∖∙ Θ(α) + nλ∖w(t? [1]∖,
i:yi=1
where in the inequality we use Hypothesis (B.13) and (B.14) to get that
hwyi,r, ξii ≤ X ∖w^r[k]∖ ∙ max ∖ξi[k]∖ + α∖hwyi,r, vi∖ =θ ⑴.
k∈Bi	k∈Bi
Recall from (B.18) that we have ∖'j? ∖ = O(Sλ), therefore We have if Wjtr) [1] is smaller than some
value in the order of — Θ(笔)∙ Polylog(d), then
sσp
-nV (t) L(W(t'))[1] ≥ -θ(αn2λ) +θ(nλ na) ∙ Polylog(d) ≥ Θ(nη),
w1,r	sσp2	sσp2
which by Lemma B.2 implies that Wj(tr0)[1] will increase. Therefore, we can conclude that W(t+1) ∈
r	~ /	、	、
□
[—O(Sσα2), 0)in this case, which verifies Hypothesis (B.14).
Lemma B.6 (Lemma 5.5, restated). If the step size satisfies η = O (d-1/2), then for any t it holds
that
L(W(HI)) — L(W⑴)≤ -η∣∣vL(w㈤)∣∣ι + Θ(η2d).
Proof. Let ∆Fj,i = Fj (W(t+1), xi) - Fj (W(t), xi). Then regarding the loss function
eFyi(W,xi)
Li(W) = -log ”(W,Xi) = -Fyi(W,Xi) + log (XeFj(W,xi)).
It is clear that the function Li(W) is 1-smooth with respect to the vector [F-1(W, xi), F1(W, xi)].
Then based on the definition of ∆Fj,i, we have
LI(Wf-LlW) ≤ X dFLWW(tXC) ∙ ∆Fj,i + XOFii)2.	(B∙20)
Moreover, note that
m
Fj(W(t),xi) = X [σ(hWj(,tr), yivi) +σ(hWj(,tr),ξii).
r=1
By the results that hWj(,tr), vi ≤ Θe (1) and hWj(,tr), ξi ≤ Θe (1), for any η = O(d-1/2),we have
hw(t+1), Vi ≤ hwjtr, Vi + η ≤ Θ⑴，hwjt+1),ξii ≤ hwjtr, ξii + Θ(ηs1∕2) ≤ Θ⑴，
which implies that the smoothness parameter of the functions σ(hWj(,tr), yivi) and σ(hWj(,tr), ξii) are at
(t)	(t+1)
most Θ(1) for any w in the path between wj,r and wj,r . Then we can apply first Taylor expansion
on σ(hwj(,tr), yiVi) and σ(hwj(,tr), ξii) and bound the second-order error as follows,
σ(hwj(,tr+1), yiVi) - σ(hwj(,tr), yiVi) - vwj,r σ(hw(j,tr), yiVi), wj(,tr+1) -w(j,tr)i
≤ Θ(kwjt+1)- Wr k2)=Θ(η2d),	(B.21)
where the last inequality is due to Lemma B.2 that
[wjt+I)-Wjtr]2 = η2∣qm(∣∣2 ≤ Θ(η2d).
j,r 2
24
Under review as a conference paper at ICLR 2022
Similarly, we can also show that
卜((Wjt+1),Eii) -σ((Wjtr,Eii)-〈VW。,,σ(hw(tr,ξii),Wjt+1)-w(tri∣ ≤。Wd).	(B.22)
Combining the above bounds on the second-order errors, we have
∣∣∆Fj,i - (VWFj(W(t), xi), W(t+1) - W(t)i∣∣ ≤ Θe(mη2d) = Θe (η2d),	(B.23)
where the last equation is due to our assumption that m = Θ(1). Besides, by (B.21) and (B.22) the
convexity property of the function σ(x), we also have
∣∣σ((Wj(,tr+1),yivi) - σ((Wj(,tr), yivi)∣∣ ≤ |〈Vwj,r σ((Wj(,tr), yivi), W(j,tr+1) - Wj(,tr)i| +Θe(η2d)
=θ(Mσ0((Wjt+1),yiVi)| ∙ kvk1) + θ(η2d)
= Θe (η + η2 d);
∣∣σ((W(j,tr+1),Eii) -σ((Wj(,tr),Eii)∣∣ ≤ |〈Vwj,r σ((Wj(,tr), Eii), Wj(,tr+1) - Wj(,tr)i| +Θe(η2d)
= Θ(η∣σ0(hwjt+1), ξii)∣∙kξk1) +Θ (η2d)
= Θe (η sσp + η2d).
These bounds further imply that
∣∆Fj,i∣ ≤ Θ(m ∙ (ηsσp + η2d)) = Θ(ηsσ? + η2d).	(B.24)
Now we can plug (B.23) and (B.24) into (B.20) and get
Li(WS)-Li(w(t)) ≤ X ；tW；Xi) ∙ ∆Fj，i+XgFjj
≤ X ∂FLWW(t)Χi) YVWFj(W(t), Xi), W(t+1) - Wt)
+ Θ (η2d) + Θ((ηsσp + η2d)2)
= (VLi(W(t)), W(t+1) -W(t)i + Θe (η2d),	(B.25)
where in the second inequality we use the fact that Li(W) is 1-Lipschitz with respect to Fj (W, Xi)
and the last equation is due to our assumption that σp = O(s-1/2) so that Θe((ηsσp + η2d)2) =
Oe (η2 d).
Now we are ready to characterize the behavior on the entire training objective L(W) =
n-1 Pin=1 Li(W) + λkWk2F. Note that λkWk2F is 2λ-smoothness, where λ = o(1). Then applying
(B.25) for all i ∈ [n] gives
1n
L(W(t+1)) - L(W(t)) = — X [Li(W(t+1)) - Li(W(t))] + λ(∣∣W(t+1)kF - kW(t)kF)
n i=1
≤ (VL(W(t)), W(t+1) -W(t)i + Θe (η2d),
where the second equation uses the fact that kW(t+1) - W(t) k2F = Θe(η2d). Recall that we have
W(t+1) - W⑴=-η∙
wj,r Wj,r η
m(t)
v(t)
j,r
Then by Lemma B.2, We know that mjt)[k]/ JVjtr [k] is close to sign gradient if VL(W(t))[k] is
large. Then we have
∕vwj,rL(W(t)), mjtr= ∖ ≥ θ(∣∣Vwj,rL(W(t))∣∣1) - Θ(d ∙ η) - Θ(ns ∙ ηsσp)
vj(,tr)
25
Under review as a conference paper at ICLR 2022
≥ θ(H^wj,rL(W(t))1 -Θe(dη),
where the second and last terms on the R.H.S. of the first inequality are contributed by the small
gradient coordinates k ∈/ ∪in=1Bi and k ∈ ∪in=1Bi respectively, and the last inequality is by the fact
that ns2σp = O(d). Therefore, based on this fact (B.25) further leads to
L(W(HI)) — L(W⑴)≤ -η∣vL(w㈤)kι + Θ(η2d),
which completes the proof.
□
Lemma B.7 (Generalization Performance of Adam). Let
W* =	argmin	∣∣VL(W)∣∣ι.
W∈{W(1),...,W(T)}
Then for all training data, we have
1n
-∑1 [Fyi(W*, Xi) ≤ F-yi(W*, Xi)] =0.
n i=1
Moreover, in terms of the test data (x, y)〜D, We have
P(x,y)〜D [Fy (W*, X) ≤ F-y W, x)] ≥ - ∙
Proof. By Lemma B.6, We knoW that the algorithm Will converge to a point With very small gradient
(up to O(ηd) in `1 norm). Then in terms ofa noise vector ξi, We have
X vwyi,rL(W*)[k] ≤ O(ηd).	(B.26)
k∈Bi
Note that
nvwyi,r L(W*)[k] = '*i,iσ'(hw*i,r, ξi i)ξilk] — nλw*i,r [k],
where 'y,i = 1 一 logityi (F*, Xi). Then by triangle inequality and (B.26), we have for any r ∈ [m],
E∣%M(hw*i,r,ξii)∣ξilk]∣- nλ]T |wy“r
k∈Bi	k∈Bi
lk]| ≤ n	vwyi,r L(W*)lk] ≤ O(nηd).
k∈Bi
Then by Lemma B.5, let r* = argmaxr∈m] (w^),ξii, we have <Wyi,r*, ξii = Θ(1) and
Pk∈Bi W*i,r*[k]|，|&网=θ(1)∙ Note that |&同| = Oe(σp), we have Pk∈Bi lwyi,r* [k] | ≥
Θ(1∕σp). Then according to the inequality above, it holds that
nλ X wji,r [k] | - nηd) ≥ θ (nσλ),
k∈Bi	p
|'yi,i| ∙ θ(Sσp) ≥ θ
where the second inequality is due to our choice of η. This further implies that ∣'*.,i∣ = I'[,/ =
Θ(sσ2) by combining the above results with (B.18). Then let us move to the gradient with respect to
the first coordinate. In particular, since kvL(W*)k1 ≤ O(ηd), we have
n	n
∣nVw3,rL(W*)[1]∣ = Xyi'*”(hw;,r,yivi) - α X仍':产0(〈亚*,,,&〉)一 nλw>[1]
i=1	i=1
≤ O(nηd).	(B.27)
Then note that sgn(yi'*,J = sgn(j), it is clear that w*,r* [1] ∙ j ≤ 0 since otherwise
n	αn2 λ
∣nVwj,r* L(W )[1]∣ ≥ α X 9也,[σ (hw”, G- σ (⑼於* ,yiv))] ≥ θ( —2 ≥ ≥ Ω (nηd),
i=1	sσp
26
Under review as a conference paper at ICLR 2022
which contradicts (B.27). Therefore, using the fact that w；/* [1] ∙ j ≤ 0, We have
n	n
lnVwj,r* L(W*)[1]| = α X %'j,iσ' (hwj,r* , Eii)- X yi '；,"(IW；,r*[1]|)] - nNw；,r*[1]]
i:yi=j	i:yi=-j
Then applying (B.27)and using the fact that |'y.,/ = ∣'-y.,i∣ = Θ(%)for all i ∈ [n],it is clear that
∣wo*[1]∣≥ Θ (a1/(qT) Λ 蓑)
≥Θ
where the second equality is due to our choice of σp and α. Then combining with Lemma B.5 and
the fact that w*,r* [1] ∙ j < 0, we have
wj,r* [1]
• j ≤ -Θ
Now we are ready to evaluate the training error and test error. In terms of training error, it is clear
that by Lemma B.5, we have hw>r*, ξii ≥ Θ(1),〈w7,r, ξi〉≥ -o(1), and Ihw[,r, v)| = o(1),
KW-y.,r, ξii∣ = o(1). Then we have for any training data (Xi, yi),
m
Fyi (W*, Xi) = X [σ(hw%r,yivi) + Zyi,r ξii)] = Θ⑴，
r=1
m
F-yi (Wy, Xi) = X σ(hw-y yi,r, -yivi) + σ(hw-y yi,r, ξii)] = o(1),
r=1
which directly implies that the NN model Wy can correctly classify all training data and thus achieve
zero training error.
In terms of the test data (X, y) where X = [yv, ξ], which is generated according to Definition 3.1.
Note that for each neural, its weight wjy,r can be decomposed into two parts: the first coordinate
and the rest d - 1 coordinates. As previously discussed, for any j ∈ [2] and r = ry, we have
sgn(j) • wy,r [1] ≤ -Θ (nα∕(sσp)) and sgn(j) • Wyr [1] ≤ Θ(λ-1 η) for r = ry. Therefore, using
the fact that Θ(nɑ∕(sσp)) = ω(λ-1η) and Lemma B.5, given the test data (x, y), we have
m
Fy(Wy,X) = X σ(hwyy,r,yvi) + σ(hwyy,r, ξi)]
r=1
m
≤ X Θe
r=1
nα
.sσP
q
+
m
F-y(Wy,X)) = X σ(hwy-y,r,yvi) + σ(hw-y y,r, ξi)]
r=1
≥ Θ[∣w-y,r*[1]∣q + [Z-y,r* ]+]
≥Θ
nα
sσp 一
q
+ [ζ-y,r*]q+
+
where the random variables ζy,r and ζy,r are symmetric and independent of v. Besides, note that
α = o(1), it can be clearly shown that α • nα∕(sσp2)	nα∕(sσp2). Therefore, if the random
noise ζy,r and ζ-y,r are dominated by the feature noise term hw-y y,r* , yvi, we can directly get that
Fy(Wy, X) ≤ F-y (Wy, X)) (recall that m = Θe (1)), which implies that the model has been biased
by the feature noise and the true feature vector in the test dataset will not give any “positive” effect
to the classification. Also note that ζy and ζ-y are also independent of v, which implies that if the
random noise dominates the feature noise term, the model Wy will give at least 0.5 error on test data.
In sum, we can conclude that with probability at least 1∕2 it holds that Fy(Wy, X) ≤ F-y (Wy, X),
which implies that the output of Adam achieves 1/2 test error.	□
27
Under review as a conference paper at ICLR 2022
B.3 Proof for Gradient Descent
Recall the feature learning and noise memorization of gradient descent can be formulated by
hw(t+1), j ∙ vi = (1 - ηλ) ∙ hwjtr,j ∙ vi
nn
+ η ∙j ∙ ( Xyi'(t)σ0((Wjtr,yiVi) -αXyi'j,)σ0(hWjtr,ξii)),
n	i=1	i=1
hwyiU, ξii = (1 - Q) ∙hwyi)r, ξii + n ∙ X 曝 iσ(w 戕 r , ξii) ∙ ξi[k]2
k∈Bi
nn
+ r~~ ∙ (α X 'yi,sσ(hwy-,r, Wsi- X ys'^sσ0 (hw^r ,ysvi) ) . (B.28)
n	s=1	s=1
Then similar to the analysis for Adam, we decompose the gradient descent process into multiple
stages and characterize the algorithmic behaviors separately. The following lemma characterizes the
first training stage, i.e., the stage where all outputs Fj (W(t), xi) remain in the constant level for all j
and i.
Lemma B.8. [Lemma 5.6, restated] Suppose the training data is generated according to Definition
3.1, assume λ = o(σq-2σp∕n). Let Ajt) = maxr∈[m]〈wjt+Zj ∙ V，「j? = maxr∈[m]〈wj), ξii,
and Γjt) = maxi% =j Γj∙,). Then let Tj be the iteration number that Ajt) reaches Θ(1∕m), We have
Tj = Θe (σ02-q ∕η) for all j ∈ {-1, 1}.
Moreover, let T0 = maxj {Tj}, then for all t ≤ T0 it holds that Γ(jt) = Oe(σ0 ) for all j ∈ {-1, 1}.
We first provide the folloWing useful lemma.
Lemma B.9. Let {xt, yt}t=1,... be tWo positive sequences that satisfy
xt+1 ≥ Xt + η ∙ Axq-1,
yt+ι ≤ yt + n ∙ Byq-1,
for some A = Θ(1) and B = o(1). Then for any q ≥ 3 and suppose y0 = O(x0 ) and η < O(x0 ),
We have for every C ∈ [x0 , O(1)], let Tx be the first iteration such that xt ≥ C, then We have
Txη = Θ(x02-q) and
yTx ≤ O(x0 ).
Proof. By Claim C.20 in Allen-Zhu & Li (2020c), We have Txη = Θ(x02-q). Then We Will shoW
yt ≤ 2x0
for all t ≤ Tx. In particular, letTxη = C0x02-q for some absolute constant C0 and assume C0B2q-1 <
1 (this is true since B = o(1)), We first made the folloWing induction hypothesis on yt for all t ≤ Ta,
yt ≤ y0 + tηB0(2x0 )q-1.
Note that for any t ≤ T0 , this hypothesis clearly implies that
yt ≤ yo + TxηB02qTχ0-1 ≤ X0 + CB2q-1x^q ∙ x0-1 ≤ 2x0.
Then We are able to verify the hypothesis at time t + 1 based on the recursive upper bound of yt, i.e.,
yt+ι ≤ yt + n ∙ ByqT
≤ yo + tηB(2x0)q-1 + η ∙ ByqT
≤ y0 + (t+ 1)ηB(2x0)q-1.
Therefore, we can conclude that yt ≤ 2χ0 for all t ≤ TX. This completes the proof.	□
NoW We are ready to complete the proof of Lemma B.8.
28
Under review as a conference paper at ICLR 2022
Proofof Lemma B.8. Note that at the initialization, We have ∣hwj0), v)| = Θ(σ0) and ∣hwj0), ξ∕∣ =
Θ(s1∕2σpσo). Then it can be shown that
m
Fj(W(0), xi) = X σ(hwj(,0r), yivi) + σ(hwj(,0r), ξii) = o(1)
r=1
for all j ∈ {-1, 1}. Then we have
(0)	eFj(W(0) ,xi)
j 1 = P eFj(W(0),Xi) = θ⑴.
Then we will consider the training period where |'j?| for all j, i, and t. Besides, note that
sgn(yi'jt)) = j. Therefore, let r* = argmaxr hw(t-1),j ∙ v)，(B.28) implies that
Ajt) ≥ hw(-1'),j ∙ v)
=(1 - ηλ) ∙hwjtI) ,j ∙ vi
nn
+ n ∙ (X ∣'jT)∣σ0(hwjt-1),yiV)) - αX j-1),(hwj：1),Q)
n i=1	i=1
≥ (1 - ηλ) ∙ hwjt-υ, j ∙ v) + θ(η) ∙ [σ0(hwjtt-1∖j ∙ v)) - ασ0(Γjtτ))]
≥ (1 - ηλ)A(j)+ η ∙ θ((Ajj))qT) - η ∙ θ(α(r(τ))q-1).	(B.29)
Similarly, let r* = arg maXr hw^,r, ξi), we also have the following according to (B.28)
暇=hwy?,*, ξi) ≤ (1 -nNWyiU, ξi)+θ( ηsσp2) ”(hWyiu,&))
2n
+ θ(ηα-) ∙XσT,ξs))
s=1
≤ ryi-1)+θ (ns"Jr'! +θ (ηα- ∙ X (H))”).
Then by our definition of Γj(t) = maXi∈[n] Γ(jt,i), we further get the following for all j ∈ {-1, 1},
Γjt) ≤ r(t-1)+θ( ηsσp±2η0-.(片-1))，-1) =「11)+θ( R ∙(「11)),-)
(B.30)
where the last equation is by our assumption that α = O(sσ-∕n).
Then we will prove the main argument for general t, which is based on the following two induction
hypothesis
Ajt) ≥ AjtT) + η ∙ θ((Ajtτ))qT),	(B.31)
Γjt) ≤ Γjt-1) +θ (ηsσ- ∙ (Γjtτ))qτ) .	(B.32)
Note that when t = 0, we have already verified this two hypothesis in (B.29) and (B.30), where we
use the fact that λ = o(σ0q-2σp∕n) ≤ (Aj0))q 2 and α = o(1). Then at time t, based on Hypothesis
(B.31) and (B.32) for all τ ≤ t, we have
Γ(jτ) ≤ O(Aj(τ)),
as sσ2∕n = o(1) and Ajt) increases faster than Γjt). Besides, we can also show that λΓ(t) ≤
(Γjt))q 1, which has been verified at time t = 0, since Γjt) keeps increasing. Therefore, (B.29)
implies
Ajt+1) ≥ (1 - ηλ)Ajt) + η ∙ θ((A(t))q-1) - η ∙ θ(α(rjt))q-1)
29
Under review as a conference paper at ICLR 2022
≥ Ajt) + η ∙ θ((Λjt))qT),
which verifies Hypothesis (B.31) at t + 1. Additionally, (B.30) implies
rjt+1) ≤ rjt) + θ(ηsσp ∙ (rjt))qT
which verifies Hypothesis (B.32) at t + 1. Then by Lemma B.9, we have that Λ(jt) = Oe(1) for
all t ≤ To = θ((Aj0))2-q/η) = Θ(σ0-q/η). Moreover, Lemma B.9 also shows that Γjt+1) =
O(Λj0)) = O(σo). This completes the proof.	□
Lemma B.10 (Off-diagonal correlations). For any data (xi, yi) and for any t ≤ T-yi, it holds that
hw(-t)yi,r,ξii ≤ Θe (α).
Proof. By the update form of GD, we have for any k ∈ Bi,
w(tyi,r [k] ∙ ξi[k] = (i-ηλ) ∙ w-y i,∕k] ∙ Mk]+n ∙ x '，，(〈型”,,, &〉)∙ &同2,
k∈Bi
which keeps decreasing. Therefore, for all r and i, we have
hw-yi,r, ξii ≤ W",/" ∙ξi[lU + X w^li,r [k]ξi[k]
k∈Bi
≤ Θ(α) + Θe(σoσps1/2)
= Θe (α),
where the second inequality follows from the fact that |hwj(,tr), vi| ≤ Θe (1) for all t ≤ Tj. This
completes the proof.	□
Note that for different j, the iteration numbers when Λ(jt) reaches Θe (1/m) are different. Without
loss of generality, we can assume T1 ≤ T-1. Lemma B.8 has provided a clear understanding about
how Λ(jt) varies within the iteration range [0, Tj]. However, it remains unclear how Γ(1t) varies within
the iteration range T,T-ι] since in this period We no longer have ∣'jt)∣ = Θ(1) and the effect
of gradient descent on the feature learning (i.e., increase of hwj,r ,j ∙ Vi) becomes weaker. In the
following lemma we give a characterization of Λ(1t) for every t ∈ [T1, T-1].
Lemma B.11 (Stage I of GD: part II). Without loss of generality assuming T1 < T-1. Then it holds
that Λ(1t) = Θe (1) for all t ∈ [T1, T-1].
Proof. Recall from (B.29) that we have the following general lower bound for the increase of Λ(jt)
nn
Ajt+1) ≥ (I- ηλ) ∙ hwjtr*,j ∙ vi+n ∙( x 阀 iσ0(hw(tr*,yivi)- ° χ 乃沛皿?*,&i)
n	i=1	i=1
≥ (1 - ηλ)Λjt)+θ( η) ∙ X ∣'jt)∣∙ (Λjt))qT-Θ(αη) ∙ (rjt) ∨ Θ (α))q-1, (B.33)
i:yi=j
where the last inequality is by Lemma B.10. Note that by Lemma B.8, we have Γ(jt) = Oe (σ0) for all
t ≤ T-1 and . Then the above inequality leads to
Λjt+1) ≥ (1 - ηλ)Λjt) +θf n) ∙ X 嘿 ∣∙ (A,)--Θ(αq η),	(B.34)
i:yi=j
where we use the fact that α = ω(σ0). The the remaining proof consists of two parts: (1) proving
Ajt) ≥ Θ(1∕m) = Θ⑴ and (2) Ajt) ≤ Θ(log(1∕λ)).
30
Under review as a conference paper at ICLR 2022
Without loss of generality we consider j = 1. Regarding the first part, we first note that Lemma B.8
implies that AITI) ≥ Θ(1∕m). Then we consider the case when AIt) ≤ θ(log(1∕α)∕m), it holds
that for all yi = 1,
=	eF-ι(W(t)，Xi)
1,i= Pj∈{-i,i} eF.(W(t)，Xi)
mm
Θ X σ(hw-(t)1,r, yivi) +σ(hw(-t1),r,ξii) - Xσ(hw1(t,)r,yivi) +σ(hw1(t,)r,ξii)
r=1	r=1
≥ exp ( - Θ(mAιt)))
≥ exp(-Θ(log(1∕α)))
= Θe (α).
Then (B.34) implies that if Γ(1t) ≤ Θ(log(1∕σ0)∕m), we have
A,+1) ≥ (1 — ηλ)Aιt) + Θ(ηα) ∙ AIt) — Θ(aqη) ≥ AIt) + Θ(ηα) ∙ AIt) ≥ A，),
where the second inequality is due to λ = o(α). This implies that A(1t) will keep increases in this
case so that it is impossible that A(1t) ≤ Θ(1∕m), which completes the proof of the first part.
For the second part, (B.28) implies that
Alt+1) ≤ (1 -ηλ)Alt) + θ(n) ∙ X ∣'1ti∣∙ (AIt))q-1.	(B.35)
i:yi=1
Consider the case when Γ(1t) ≥ Θ(log(d)), then for all yi = 1,
_	eF-1(W(t) Ki)
1,i = Pj∈{-i,i} eFj(W(t),χi)
mm
Θ X σ(hw-(t)1,r, yivi) +σ(hw(-t1),r,ξii) - Xσ(hw1(t,)r,yivi) +σ(hw1(t,)r,ξii)
r=1	r=1
≤ exp ( - Θ(A1t)))
≤ exp(-Θ(log(1∕λ))
= Θe(poly(λ)).
Then (B.35) further implies that
Aιt+1)≤ (1-ηλ)AIt)+θ( Poyd)) ∙ (AIt))q-1
≤ AIt)- θ(ηAlt)) ∙ (λ - poly(λ) ∙ (AIt))q-2) ≤ A，),
which implies that A(1t) will decrease. As a result, we can conclude that λ(1t) will not exceed
Θ(log(1∕λ)), this completes the proof of the second part.
□
Lemma B.12 (Lemma 5.7, restated). If η ≤ O(σ0), it holds that A(jt) = Θe (1) and Γ(jt) = Oe(σ0) for
allt ∈ [T-1,T].
Proof. We will prove the desired argument based on the following three induction hypothesis:
Ajt+1) ≥ (1 - λη)Ajt) + θ(η) X ∣'jti)∣- Θ(αqn) ∙ 1 X ∣'jtr|,	(B.36)
j	j	n	j,	n j,r
i:yi=j	i=1
Γ(jt) = Oe(σ0),	(B.37)
31
Under review as a conference paper at ICLR 2022
Λ(jt) = Θ(1).	(B.38)
In terms of Hypothesis (B.36), we can apply Hypothesis (B.37) and (B.38) to (B.33) and get that
Λjt+1) ≥ (1 - ηλWt) + θ( n) ∙ X j) ∣∙ (Ajt))q-1 - Θ(αη) ∙ W)V Θ (α))q-1 ∙ ɪ X 点 |
i:yi=j	i=1
≥ (I-λη)Ajt) + θ( n ) X 啕 I-θ (αq η) ∙ n X Ij 1.
i:yi=j	i=1
where the last inequality we use the fact that α ≥ σ0. This verifies Hypothesis (B.36).
In order to verify Hypothesis (B.37), we have the following according to (B.36),
nn
X Λjt+1) ≥ (1 - λη) X Ajt) + Θ ⑴ Xj)I-Θ (αq η) • ； £朗1
j∈{-1,1}	j∈{-1,1}	i=1	i=1
=(1 - λη) X 卜 jt) + Θ(n )£飓1,
j∈{-1,1}	i=1
where the last equality holds since α = o(1). Recursively applying the above inequality from T-1 to
t gives
t-T-1 -1	n
X Ajt ≥	(1 - λη)t-T-1 X	AjT-1)	+ Θ(n)∙ X	(1 - λη)τ X	j-1-T)1	.
j∈{-1,1}	j∈{-1,1}	τ=0	i=1
Then by Hypothesis (B.38) we have
t-T-1 -1	n
θ(η) ∙ X (1-λη)τX j-1-τ)∣≤θ⑴.
n	τ=0	i=1
Now let us look at the rate of memorizing noises. By (B.28) and use the fact that α2 ≤ O(sσp∕n),
we have
Γjt) ≤ (1 - ηλ)rjtτ) + θ(牛)∙ X ∣'j,i∣∙ (Γjtτ))qτ
n	i=1
≤ (1-ηλ)rjtτ) + θ(ηsσpσ-1) ∙ XGI
i=1
2 q-1	t-T-1 -1	n
≤「尸 + &(ηsσpσ^)∙ X (1 - λη)τXIj-1-T)1
τ=0	i=1
≤ Θ(σo + sσpσqq-1)
≤ Θe(σo),
which verifies Hypothesis (B.37).
Given Hypothesis (B.36) and (B.37), the verification of (B.38) is straightforward by applying the
same proof technique of Lemma B.11 and thus We omit it here.	□
Lemma B.13 (Lemma 5.8, restated). If the step size satisfies, then for any t ≥ T-1 it holds that
L(W(t+1)) - L(W(t)) ≤ -η∣∣VL(W(t))kF.
Proof. The proof of this lemma is similar to that of Lemma B.6, Which is basically relying the
smoothness property of the loss function L(W) given certain constraints on the inner products
hwj,r, vi and hwj,r, ξii.
32
Under review as a conference paper at ICLR 2022
Let ∆Fj,i = Fj(W(t+1), xi) - Fj (W(t), xi), we can get the following Taylor expansion on the loss
function Li(W(t+1)),
Li(W(t+I))-Li(W㈤)≤ X dFLW]X) ∙ ∆Fj,i + XOFii)2.	(B.39)
In particular, by Lemma B.12, we know that hwj(,tr), yivi ≤ Θe (1) and hwj(,tr),ξii ≤ Θe(σ0) ≤
Θe (1). Then similar to (B.21), we can apply first-order Taylor expansion to Fj (W(t+1), xi), which
requires to characterize the second-order error of the Taylor expansions on σ(hw(j,tr+1), yivi) and
σ(hwj(,tr+1), ξii),
卜((Wjt+1),yiVi)—σ((Wjtr 毋Vi) - (Vwj.,rσ((Wjtr ,yiVi), wjt+1)—Wjtr i∣
≤ Θ(kwjt+1) — Wjrk2) =Θ(η2kVwj,rL(w(t))k2),
∣σ(hwjt+1), q -b(^jtr, ξii)—(Vw3r σ(hWjjr ξii), Wjt+1)- WjtriI
≤ Θ(kWj∙t+1)- Wjtr k2) =Θ(η2kVwj,rL(W(t))k2).	(B.40)
Then combining the above bounds for every r ∈ [m], we can get the following bound for ∆Fj,i
∣∣∆Fj,i — (VWFj (W(t), xi), W(t+1) — W(t)i∣∣ ≤Θe η2 X kVwj,rL(W(t))k22
r∈[m]
= Θ(η2 kVL(W⑴)kF).	(B.41)
Moreover, sincehWj?,yiv) ≤ Θ(1) and (w；?, ξi)≤ Θ(1) and σ(∙) is convex, then We have
lσ((Wjt+1),yivi) — σ(hwjtr,yivi) ≤ maχ{W0(hwjt+1),yivi)1, ∖σ0({wjj,'),yivi)∖} ∙ |hv,Wjt+1) — Wjri|
≤ Θ(kw(t+1) — w(t)∣∣2)
Wj,r	— Wj,r 2 .
Similarly We also have
(t+1)	(t)	(t+1)	(t)
∖σ((Wj,r，&i)”((明.： , ξii)∖ ≤ θ (kWj,r	- Wj,r l∣2).
Combining the above inequalities for every r ∈ [m], We have
∣∆Fj,i∣2≤ θ( X kWjt+1) -Wjtrk2 2) ≤ Θ(mη2∣VL(W⑴)kF) = Θ(η2∣VL(W㈤)kF).
r∈[m]
(B.42)
NoW We can plug (B.41) and (B.42) into (B.39), Which gives
Li(W(t+I))-Li(W⑴)≤ X dFLWW(tXi) ∙ ∆Fj,i + XOF")2
= (VLi(W(t)), W(t+1) — W(t)i +Θe (η2kVL(W(t))k2F).	(B.43)
Taking sum over i ∈ [n] and applying the smoothness property of the regularization function λkWk2F,
We can get
1n
L(W(t+1)) — L(W(t)) = — X [Li(W(t+1)) — Li(W(t))] + X(kW(t+1)kF — ∣W(t)∣F)
n i=1
≤ (VL(W(t)), W(t+1) —W(t)i +Θe (η2kVL(W(t))k2F)
=-(η — Θ(η2)) ∙∣VL(W(t))∣F
≤ - 2 kVL(W㈤)kF,
where the last inequality is due to our choice of step Size η = o(1) so that gives η — Θ(η2) ≥ η∕2.
This completes the proof.	□
33
Under review as a conference paper at ICLR 2022
Lemma B.14 (Generalization Performance of GD). Let
W* = arg min	∣∣VL(W(t))kF.
{W(1),...,W(T)}
Then for all training data, we have
1n
-∑1 [Fyi(W*, Xi) ≤ F-yi(W*, Xi)] =0.
n i=1
Moreover, in terms of the test data (x, y)〜D, We have
P(x,y)〜D [Fy (W*, X) ≤ F-y (W*, X)] = O⑴.
Proof. By Lemma B.12 it is clear that all training data can be correctly classified so that the training
error is zero. Besides, for test data (X, y) With X = [yv>, ξ>]>, it is clear that With high probability
hwy*,r,yvi = Θe (-) and [hwy*,r, ξi]+ ≤ Oe(σ0),then
m
Fy (W*, x) = X [σ(hwy,r ,yvi) + σ(hwy,r, ξi)] ≥ ω ⑴.
r=1
If j = -y, We have With probability at least - - -/poly(n), hw-* y r, yvi ≤ 0 and [w-* y r, ξi]+ ≤
O(α), Which leads to
m
F-y(W*, X) = X [σ(hw-*y,r,yvi) +σ(hw-*y,r,ξi)] ≤ Oe(mαq) = Oe(αq) = O(-).
r=1
This implies that GD can also achieve nearly at most -/poly(n) test error. This completes the
proof.	□
C Proof of Theorem 4.2: Convex Case
Theorem C.1 (Convex setting, restated). Assume the model is over-parameterized. Then for any
convex and smooth training objective With positive regularization parameter λ, suppose We run Adam
and gradient descent for T = poln(n) iterations, then with probability at least 1 一 n-1,the obtained
ParameterS WAdam and WGD Satisfy that k VL(WAdam)∣1 ≤ T and kVL(WAdam)k2 ≤ T
respectively. Moreover, it holds that:
• Training errors are both zero:
nn
—X1 [sgn(F (WAdam, Xi)) = yi] = — X 1 kgn(F (WG D, Xi)) = yi] =0.
n i=1	n i=1
• Test errors are nearly the same:
P(x,y)〜D [sgn(F(WAdam, xi)) = y] = P(x,y)〜D [sgn(F(WGd, x)) = y] ± o⑴.
Proof. The proof is straightforWard by applying the same proof technique used for Lemmas B.6
and B.13, Where We only need to use the smoothness property of the loss function. Then it is clear
that both Adam and GD can provably find a point With sufficiently small gradient. Note that the
training objective becomes strongly convex When adding Weight decay regularization, implying that
the entire training objective only has one stationary point, i.e., point With sufficiently small gradient.
This further imply that the points found by Adam and GD must be exactly same and thus GD and
Adam must have nearly same training and test performance.
Besides, note that the problem is also sufficiently over-parameterized, thus With proper regularization
(feasibly small), We can still guarantee zero training errors.
□
34