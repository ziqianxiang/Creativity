Under review as a conference paper at ICLR 2022
Weakly-Supervised Learning of Disentangled
and Interpretable Skills for Hierarchical
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Hierarchical reinforcement learning (RL) usually requires skills that can be appli-
cable to various downstream tasks. While many recent works have been proposed
to learn such skills in supervised or unsupervised manners, the learned skills are
often entangled, which makes it difficult to interpret those skills. To alleviate
this, we propose a novel WEakly-supervised learning approach for learning Dis-
entangled and Interpretable Skills (WEDIS) from the continuous latent represen-
tations of trajectories. We accomplish this by extending a trajectory variational
autoencoder (VAE) to impose an inductive bias with weak labels, which explicitly
enforces the trajectory representations to be disentangled into factors of interest
that we intend the model to learn. Given the latent representations as skills, a
skill-based policy network is trained to generate similar trajectories to the learned
decoder of the trajectory VAE. Additionally, we propose to train a policy network
with single-step transitions and perform the trajectory-level behaviors at test time
with the knowledge on the skills, which simplifies the training procedure for the
policy. With a sample-efficient planning strategy based on the skills, we show that
our method is effective in solving the hierarchical RL problems in experiments on
several challenging navigation tasks with a long horizon and sparse rewards.
1	Introduction
Deep reinforcement learning (RL) has achieved great success for various applications, ranging from
playing games (Mnih et al., 2013; Silver et al., 2016) to complex locomotion and robots control
(Lillicrap et al., 2015; Schulman et al., 2015; 2017; Haarnoja et al., 2017). However, several chal-
lenges such as sparse rewards or inadaptability to unlearned tasks still hinder its practical usages in
real-world problems. To alleviate these challenges, hierarchical RL (Sutton et al., 1999; Dietterich,
2000) has been studied where an agent pre-learns reusable skills from prior experiences and hier-
archically solve higher-level problems by combining the skills. Two issues need to be resolved for
the successful deployment of the hierarchical RL; how to learn useful skills and how to effectively
make use of the skills for various downstream tasks.
A possible approach for skills that can be applicable to various downstream tasks is to learn without
task-specific rewards (Eysenbach et al., 2018). Another way to achieve the useful skills is to make
them predictable. To learn those skills, (Co-Reyes et al., 2018; Sharma et al., 2019) proposed to
combine model-free and model-based RL approaches, where a skill-based predictive model, a dy-
namics model over the latent space, is trained together with a skill-based policy network.By using
the predictive model for model-based planning during testing time, these works showed to efficiently
solve various downstream tasks without the need to learn additional higher-level policies. However,
since they did not consider how the skill is embedded into the latent space, the factors consisting
of the skill often are entangled when the skill is a continuous latent variable. Compared to the en-
tangled one, the skill consisting of disentangled factors has several advantages in its applicability in
that the factors can be separately interpreted and handled.
In this paper, we introduce a novel WEakly-supervised learning approach for learning Disentangled
and Interpretable Skills (WEDIS) from the continuous latent representations of trajectories that are
composed of several generative factors, e.g., speed, direction, and curvature. To this end, we propose
1
Under review as a conference paper at ICLR 2022
a weakly-supervised trajectory variational autoencoder (WET-VAE) model that is an extension of
the trajectory VAE (Co-Reyes et al., 2018) consisting of a recurrent neural network (RNN). We
leverage the weak labels (Margonis et al., 2020) to enforce an inductive bias on the model, which
explicitly enforces the trajectory representations to be disentangled into factors of interest that we
intend the model to learn. To train the WET-VAE, we first synthetically generate a trajectory dataset
by the combination of several factors of interest, because the trajectories obtained by an online
exploration are likely to contain meaningless samples such as random walks. With the trajectory
dataset, the WET-VAE model is trained apart from a policy network. It is worthy of noting that
while this is similar to imitation learning, our data acquisition is much simpler than collecting expert
demonstration.
Sequentially, we train a skill-based policy network with the WET-VAE fixed. Given the latent rep-
resentations as skills, the skill-based policy network is trained to generate similar trajectories with
the decoder of the WET-VAE by minimizing the KL divergence between two trajectory distribu-
tions. However, training a policy to generate a trajectory given a skill is difficult since it is unlikely
to explore the corresponding trajectory in the training procedure. Instead, we propose to train the
policy network with the single-step transitions and perform the trajectory-level behaviors in the test
time, which can be achieved with the knowledge of the learned skills. This simplifies the training
procedure of the policy, and also allows for a sample-efficient large-scale planning strategy with the
scaled trajectories. In experiments in Mujoco Ant environment, we show that our disentangled and
interpretable skills are effective in solving challenging sparse reward and long-horizon problems in
2D navigation in mazes.
2	Related Works
Numerous approaches (Sutton et al., 1999; Bacon et al., 2017; Florensa et al., 2017; Hausman et al.,
2018; Haarnoja et al., 2018; Eysenbach et al., 2018; Shankar et al., 2019; Shankar & Gupta, 2020;
Co-Reyes et al., 2018; Sharma et al., 2019) have explored on learning reusable skills in RL to solve
challenging long-horizon or sparse reward problems. (Sutton et al., 1999) pioneered a way to control
higher-level abstraction by introducing an option-framework, which learns low-level primitives in a
top-down manner. (Bacon et al., 2017) proposed an option-critic architecture that learns sub-policies
of options. Also, several works (Florensa et al., 2017; Hausman et al., 2018; Haarnoja et al., 2018)
introduced to learn skills with multiple tasks in a bottom-up manner. However, designing reward
functions still requires expert knowledge and such task-specific rewards may limit a generalization
ability of the agent to the downstream tasks. To overcome this issue, recent works (Eysenbach et al.,
2018; Achiam et al., 2018; Co-Reyes et al., 2018; Sharma et al., 2019; Campos et al., 2020) proposed
an unsupervised framework that does not require a hand-specified reward function.
Model-based RL methods (Levine et al., 2016; Nagabandi et al., 2018; Chua et al., 2018; Ha &
Schmidhuber, 2018) aim to learn a dynamics model of the environment. While these works are
capable of solving unlearned tasks without the needs of an additional learning via planning through
the dynamics model, they are often at the risk of falling into over-fitting due to a huge capacity of
the required data to explore the environment. Instead of learning the underlying dynamics, some
methods (Co-Reyes et al., 2018; Sharma et al., 2019) attempted to combine the model-free and
model-based RL for learning a skill-based predictive model and a skill-based policy. Despite the
improved results, they still suffer from the lack of the interpretability of the skills.
Learning disentangled latent representations of factors of variation within dataset is beneficial to
a variety of downstream tasks such as few-shot classification and data generation, thanks to the
interpretability of the disentangled factors. (Higgins et al., 2016) proposed β-VAE, an unsupervised
method to learn the disentangled representations by modifying the weight of the KL-divergence
term of the VAE (Kingma & Welling, 2013; Rezende et al., 2014) greater than one. Afterwards,
while several variants (Kim & Mnih, 2018; Chen et al., 2018) improved the β-VAE by introducing a
total correlation (TC) term, (Locatello et al., 2019a) pointed out the inherent limitation of the purely
unsupervised approaches and emphasized the need of an inductive bias. Recent works (Locatello
et al., 2019b; Shu et al., 2019; Locatello et al., 2020; Margonis et al., 2020) proposed various forms
of weak supervision to encourage the inductive bias to learn the disentangled representations. While
there are various categories on the weak labels, we used them in terms of ones that 1) are roughly
divided into fewer classes and 2) can be obtained with programming by using the knowledge on the
factors without the need for manual labeling.
2
Under review as a conference paper at ICLR 2022
3	Preliminaries
Consider a Markov decision process (MDP) (S, A, P, r, ρ0, γ), where S is a set of states, A is a set
of action, P : S × A × S → R+ is a transition probability distribution, r : S × A → R is a reward
function, ρ0 : S → R+ is an initial state distribution and γ ∈ (0, 1) is a discount factor. We denote
a stochastic policy as π : S × A → R+. RL has a goal of maximizing the expected discounted sum
of rewards for an episode horizon HE :
HE
η(π) = Eπ[	γtr(st, at)]	(1)
t=0
Variational autoencoder (VAE) optimizes variational the lower bound of the marginal likelihood of
dataset. Given an observed datapoint x, the variational lowerbound is defined as :
logPθ(X) ≥ L(θ,φ; X)= Eqφ(z|x)[logPθ(X|z)] - DκL(qφ(z|x)kp(z)),	⑵
where P(Z) is a prior distribution of a latent variable z, the decoder pθ(X|z) is a generative model
given a latent Z parameterized by θ, and the encoder qφ(z∣χ) is an approximate posterior distribution
parameterized by φ. In Equation 2, the first term is the reconstruction term of the autoencoder, and
the second term is the KL divergence regularization. In our work, we will focus on the aspect of the
generative model of the decoder.
4	Weakly Supervised Learning of Disentangled and
Interpretable S kill (WEDIS)
Our framework consists of three stages; 1) generating trajectory training data with factors of interest
2) training the WET-VAE model, whose decoder is used for the predictive model and 3) training a
policy network to generate the similar trajectories with the predictive model conditioned on skills.
The generation process of the trajectory dataset is explained in Appendix A.1.1 due to the lack of
space. As a notation, we will use superscript for factors and subscript for time steps. The WEDIS
algorithm is summarized in Figure 2.
4.1	Learning Disentangled and Interpretable Representations of Trajectory
To learn the temporally extended behaviors, (Co-
Reyes et al., 2018) proposed a trajectory VAE
model consisting of the RNN architecture. The tra-
jectory VAE learns latent representations of trajec-
tories, which will be used as skills for a policy.
However, this model, which learns the represen-
tations in the unsupervised manner, does not con-
sider which factors of variation of a trajectory are
embedded in the latent space. Thus, the factors
that are often entangled make the interpretation of
the representations difficult, exposing limitations in
further applicability of the learned skills. To ad-
dress this, we propose a weakly-supervised trajec-
tory VAE (WET-VAE) model that leverages an in-
ductive bias in the form of weak supervision (Mar-
gonis et al., 2020) to explicitly enforce the model to
learn the disentangled representations consisting of
desired factors, yielding interpretable skills.
Consider a latent-variable generative model P(T|z)
Encoder	Decoder
Q√-l7^-y)	Pb,J,y∖z)
Figure 1: Computation graph for the WET-
VAE model. For weak supervision, we add a
set of weak labels {y1, ..., yM} to the model.
These labels encourage the model to learn a
disentangled trajectory representation Z that
consists of corresponding factors. We assume
the fixed initial state s0 at the origin.
to generate a trajectory τ given a latent variable Z . We assume the fixed initial state s0 at the ori-
gin as when given other initial states we can obtain the next states with a linear translation based
on the initial states such that P(s|s0, Z) = P(s - s0|Z). Considering M factors of interest to gen-
erate trajectories, the weak supervision can be provided by simply adding a set of M weak la-
bels y = {y1, ..., yM} to the generative model, where each label ym is one-hot encoded vector
3
Under review as a conference paper at ICLR 2022
for each factor. The idea is that a latent representation z ∈ RM , which can generate the trajecto-
ries based on the M disentangled generative factors, should also be able to reconstruct the factors.
Assuming that the trajectory and the factors that are represented as the multiple labels satisfy con-
ditional independence with respect to a given z, the generative model is extended with the labels
P(τ, y |z) = P(T|z)p(y 1|z)…P(yM |z). Then, the variational lower bound of the marginal joint
distribution p(τ, y) can be formulated as follows:
L(θ, Φ ； τ, y) = Eqφ(z∣τ,y)[logPθ(τ, y|z)] - DκL(qφ(z∣τ, y)kp(z))
=Eqφ(z k,y)[J2 log Pθ (St lsi:t-i, Z) + E log Pθ (y m |z)] - DκL(qφ(z ∣τ, y )kp (z )),
t=1	m=1
(3)
where pθ(T|z) = pθ(sι∣z)pθ(s2∣s1, Z)…pθ(ST∣sι, s2,..., ST-1, Z). Since pθ(ym|z) can be Under-
stood as a classifier for each factor, the factors should be distinctly embedded in a latent representa-
tion Z for high classification probability. As a result, this enforces a disentangled representation of
the factors.
Practically, the scales of the values of the log-likelihoods of the states and labels are different due
to the difference in dimensionality. To fill the gap, we introduce a balancing weight γ inspired by
(Margonis et al., 2020). We also use a weight β > 1 to emphasize the KL divergence term for better
disentanglement in the spirit of the β-VAE (Higgins et al., 2016). Then, the final objective function
becomes:
L(θ,φ;τ, y,β, Y)
TM
Eqφ(z∣τ,y) [ɪs logPθ(St |si：t-1, Z) + Y ∙ E logPθ(yTZ)] - β ∙ DκL(qφ(z∣τ, y)kp(z))
t=1	m=1
(4)
The WET-VAE model is trained to maximize Equation 4. To handle the sequential data, we use
the RNN architecture with LSTMs as in Figure 1. With the support of the weak supervision, this
model can learn the disentangled representations of trajectories that consist of factors of variation
contributing over different time steps.
4.2 Learning Policy with Learned Skills
4.2.1	Optimization of Policy Network to Imitate Predictive Model
After training of the WET-VAE, its decoder pθ(τ|z) is used as a skill-based predictive model of tra-
jectories. To learn the disentangled and interpretable skills, the policy network is trained to generate
similar trajectories with the predictive model based on the skills. It can be accomplished by opti-
mizing the policy to minimize the KL divergence between the distributions of trajectories generated
by the two networks.
min Ep(Z )[Dkl(pψ (T |z )kpθ (T |z))],	⑸
where ψ is a parameter of the skill-based policy network ∏ψ (a|t, Z) and pψ (τ|z) is the distribution
of the trajectories generated by the policy interacting with the environment, which can be computed
as Pψ(τ|z) = /…ja∈A[QT=ιp(st∣st-ι, at)∏ψ(at|so：t-i, Z)] da`…daτ. Since inferring pψ(τ|z)
is intractable due to the lack of knowledge on the transition probability distribution p(St |St-1, at)
of the underlying dynamics in the environment, Equation 5 cannot be optimized by direct backprop-
agation with respect to ψ. Instead, following (Co-Reyes et al., 2018), we optimize the skill-based
policy network with the RL method by rewriting (see Appendix A.2.2) the equation as follows:
max Epψ (τ,z )[log Pθ (T |z )] + H(pψ (T |z )),	(6)
where pψ (t, Z) = pψ (T|z)p(z) and H(pψ (T|z)) is the entropy of the trajectories generated by pol-
icy given skills. By using the log-likelihood logpθ(t|z) calculated by the predictive model for a
trajectory explored by the policy as a reward function, we can optimize Equation 6 with conven-
tional model-free RL algorithms (Schulman et al., 2015; 2017; Haarnoja et al., 2017) with entropy
regularization. In our implementation, we adopted the soft actor-critic (SAC) (Haarnoja et al., 2017)
algorithm that includes the entropy regularization as a part of optimization.
4
Under review as a conference paper at ICLR 2022
Note that in contrast to (Co-Reyes et al., 2018) that simultaneously optimizes both the trajectory
VAE and the policy network with trajectory samples collected from an exploration, our method
increases stability in the training since the pre-trained decoder provides reliably fixed rewards.
4.2.2	Training policy with single-step transitions
Given a skill, a policy network needs to explore the trajectory similar to the predictive model to
receive a reward in Equation 6. However, it is very unlikely to find all trajectories corresponding to
each skill by an exploration, which makes the training difficult. To overcome this, we propose to
train the policy network with single-step transitions instead of the full trajectories, and perform the
trajectory-level behaviors at test time. This can be achieved by exploiting the known knowledge on
the factors since the factors of our disentangled skills are interpretable.
Consider a latent z ∈ Z = (Z single, Z multi) ⊆ RM1+M2 that can be separated to z single ∈
Z single ⊆ RM1 contributing to single-step transition and z multi ∈ Z multi ⊆ RM2 contributing
over multiple time steps. For example, while speed and direction can be included in Z single, curva-
ture and acceleration can be included in Zmulti. As a trajectory consists ofa sequence of single-step
transitions, we can generate the same trajectory by a latent z sing le in combination with z multi. That
is, with the known relations of the factors, we can compute z single of the next time step, which has
the same effect of the execution of the full latent z . To this end, we introduce relation functions f
to relate the relevant factors. For a mth factor zm of z single, the relation function for the factor can
be expressed as ztm+1 = f m(ztm, zmulti) and properly chosen by their relationships. For instance, we
can set a relation function for the speed factor together with the acceleration factor such as zts+pe1ed =
f speed (ztspeed, zacc) = ztspeed + weight × zacc, where the weight can be properly chosen by testing
the decoder and policy heuristically after training.
In this way, we train the policy network with z single over single-step transitions in Equation 6, and
make use of the full skill z with the relation functions at test time. As the single-step transition does
not require to handle the temporal information, we use a feedforward network for the policy instead
of a RNN.
Algorithm 1: WEDIS
Require: M generative factors, weak labeling criteria
Generate: Trajectory dataset by combination of the
factors D — {τ}N=1 = {so,…，ST}N=1
Initialize: parameters φ (encoder), θ (decoder),
ψ (actor), ψ (critic), ψ (value function)
while Training do // WET-VAE training
Sample trajectory batch B ~ D
Compute labels {y1, ..., yM}B
Optimize φ and θ by Equation 4
end
Test: Check factors in z
while Training do // policy training
Sample z single from prior
Set z = (z single, z multi) wherezmulti = (0, ..., 0)
Execute actions a 〜∏ψ(a|s, Zsingle)
Collect the samples in replay buffer R
Sample transition batch {st, at, St +i}b ~ R
Compute reward r = logpθ(st+1 |st, z)
Optimize ψ, ψ and ψ with the SAC by Equation 6
end
Test: Find relation functions for each factor in z single
gninitidnC
Trajectory
Generator
Factors of Interest
Trajectories Dataset
(e.g. Speed, Direction, Curvature) (Generated)
(a) Generation of Training Trajectories
Latent
Representation
(Skill)
(b) Training of WET-VAE

…PoIiCyJ→^ζ→∕V
TrajeCtories
(ACtual)
(C) Training of PoliCy
Training BatCh


Figure 2:	Illustration of WEDIS algorithm. Two networks are trained separately. After the training
procedure of the two networks, we check the embedded factors by the latent traversal and find the
relation functions by heuristically testing the WET-VAE decoder and policy.
5
Under review as a conference paper at ICLR 2022
5 Planning with Trajectory Scaling
Algorithm 2: MPC with trajectory scaling
Require: predictive model po(τ|s, Z), policy πψ(a|s, zSiMe),
M relation functions fm for each Zm in Zsingle, reward function,
episode horizon HE, primitive horizon Hz, trajectory length T,
planning horizon Hp, initial state s0, sample size N
for i _ 1 to He/(Hz × T) do
M J s0
Sample latent sequences {z1,.., ZHP}N=1 from a distribution
for j — 1 to Hp do	// for all N samples
T = {sι,…,ST}〜Pθ(τ∣^0, Zy)
Compute distances between the states, {∆ι,…，∆τ}
THZ = {S0 + Hz ∙ ∆1,..., S0 + Hz ∙ (∆1 + …+ ∆τ)}
^0 J SHZ	// the last state of THZ
end
Evaluate the rewards of the scaled trajectory sequences
Choose the first latent z； of the best trajectory
(z single Z multi ) J_ Z ；
for t J 1 to T do
for k J 1 to HZ do
I Execute action a 〜πψ (a∖s(t-i)-Hz+(k-i), Zsingle)
end
{z m }M= 1 J{fm(zm, Zmulti )}M=ι
Zsingle J (zm )M=1
end
S0 J ST -Hz
end
Z1
z%
zMι+1
zm1+m2
zsmgle
zmulti
O
6
Z = O
O
(a) Predicted trajectory for T=3
(b) Scaled trajectory for planning
冗(α∣s, z0im3le)
HZ steps
冗(α∣s,z产典)
HZ steps
Executing with Z
(c) Actual trajectory
冗(α∣s, z；BgEe)
HZ steps
Z = [z^e, z^nulτi ]
ZSEgIe = /(ZSingLe 2multi-
Figure 3:	Illustration of trajectory scaling. This trajectory scaling provides large scale planning that
is beneficial to solving long-horizon problems.
After the sequential training of the predictive model and policy network, we can solve hierarchical
RL problems without any additional learning at test time via model-based planning. For the model-
based planning, we employ the model-predictive control (MPC) method (Garcia et al., 1989). With
a dynamics model pθ(St+1 ∣ St, a) of the environment, the MPC planner generates several numbers
of trajectory samples T = {s0, aι,..., SHP} for a finite planning horizon Hp, and then evaluates
each trajectory according a reward function given for a task. After choosing the first action of the
best trajectory maximizing the planning rewards, the agent executes the chosen action and the MPC
planner repeats iteratively this procedure on the next state until the episode horizon HE. Using the
predictive model pθ(τ ∣ Z), we can follow the MPC strategy over the latent space Z instead of the
action space A. In this strategy, we first sample sequences of latents {zι, Z2,…,ZHP} and evaluate
the latent sequences based on rewards of the trajectories generated by the predictive model given the
latents, where each latent generates sub-trajectory of length T.
Even though we can perform the trajectory-level planning based on this predictive model, the scale
of the movement lengths from actions of an agent is limited due to its inherent design specification,
which generates trivial trajectories with small scales. To perform planning with a meaningful scale
of trajectories, we propose a trajectory scaling method. In contrast to a RNN policy that processes
temporal information at every time step, our feedforward policy network can consistently perform
the single-step transition given a skill Z single from Z . By using this, we hold on Z single for Hz steps
and change it by the relation functions at every HZh step, which generates scaled trajectories by HZ
times. In the case of the predicted trajectories pθ (τ∣z) that are simulated for planning, we can simply
scale the trajectories by scaling each transition distance. This trajectory scaling method provides a
large-scale planning strategy that can plan HP × Hz × T steps and execute actions over Hz × T steps
for each skill, which is beneficial to solving long-horizon problems with high sample-efficiency by
reducing the maximum planning horizon to HE/(Hz × T). Figure 3 summarizes the MPC procedure
with trajectory scaling.
6
Under review as a conference paper at ICLR 2022
6	Experiments
In this section, we first provide training details and qualitative results of the WET-VAE and the
policy. Then, we test our algorithm on goal navigation tasks in mazes in the Mujoco (Todorov et al.
(2012); Brockman et al. (2016)) Ant environment (Figure 14), which are challenging hierarchical
RL problems. For details of the generation process of the trajectory dataset and labeling criteria,
please see Appendix A.1.1. The videos of the learned skills and maze navigation are available at
https://sites.google.com/view/iclr2022-wedis.
6.1	Qualitative Results
6.1.1	Training of the WET-VAE Network
Figure 4: The latent traversals for each skill. (a)〜(C) show the results of the WET-VAE model,
and (d) shows the results the trajectory VAE that does not use any weak supervision, where both
models are trained on trajectories with T = 5. Each row represents the trajectories generated by
latent variables changing values of corresponding dimensions of the latent variable from -1.5 to 1.5,
while keeping the others to zeros.
Without weak supervision
(d) T = 5
Using the trajectory dataset generated with T = 5, we first trained the WET-VAE with a three-
dimensional continuous latent variable z∈R3. We set the prior distribution of the latent variable as an
isotropic unit Gaussian distribution p(z) = N(0, 1). Also, for the posterior, we set the distribution
as a Gaussian distribution with a diagonal covariance matrix.
To demonstrate the benefit of the weak supervision in training the WET-VAE, we plot the latent
traversals of trajectories of the decoder in Figure 4. For each traversal, the latents are sampled by
changing values of one dimension from -1.5 to 1.5, while keeping values of the other dimensions to
zeros. Figure 4a shows single-step transitions based on each factor. While z1 and z2 clearly capture
the factors on the direction and speed, it looks like that z3 has almost no effect. Since z3 involves
the factor of the curvature, this result is reasonable. On the other hand, it is shown that z3 controls
the curvature of trajectories when the trajectory is generated with T = 5 in Figure 4b. These results
imply that the three factors are suitably disentangled according to each dimension of the latent z .
Furthermore, even when the trajectory length for generation becomes greater than the length of the
training data, the contributions of the learned factors consistently remain as shown in Figure 4c with
T = 10. Note that the weak supervision can successfully disentangle factors that contribute over
7
Under review as a conference paper at ICLR 2022
different time steps, where the factors of speed and direction affect at the each single step, and the
curvature factor affects across the multiple time-steps.
On the other hand, Figure 4d shows that some factors of skills learned from the trajectory VAE,
which is trained without weak supervision, are entangled. While the speed factor seems to be disen-
tangled in the second dimension z2, all factors seems to be still entangled in the third dimension z3.
Moreover, no factor is embedded in the first dimension z 1 as shown in the first row, implying that
this element has no effect on the generation of the trajectories.
6.1.2	Training of the Policy Network
The policy network is trained in the Mujoco Ant environment with the rewards calculated by the
WET-VAE decoder, acting as the predictive model. Since the latent z consists of the factors of
speed, direction and curvature, it is separated to z single = (z speed, zdir) and z multi = z cur.
Thus in Equation 6, the policy is conditioned on z single and the predictive model is conditioned
on z = (z single, 0) to provide log-likelihoods for T = 1. Also, instead of the full states of the
agent, we restrict the states to compute the rewards to two dimensional x - y coordinates (Sharma
et al., 2019), which enables the policy to be trained with the predictive model trained with 2D trajec-
tories. For the relation functions, we set an identity function as f speed (ztspeed, zcur) for speed and
f dir (Zdir, Zcur) = Zthr - 0.2 ∙ Zcur for direction, where We found the weight by heuristic test after
training both networks.
In Figure 5, we plot latent traversals of trajectories based on each skill, where each row show the
trajectories generated by the predictive model and the policy network, respectively. We plot the
speed and direction trajectories with HZ = 100 and T = 1, and the curvature trajectory with HZ =
40 and T = 5. We compare our algorithm against unsupervised skill learning methods (SeCTAR
(Co-Reyes et al., 2018), DADS (Sharma et al., 2019)), which are also model-based methods over the
latent space. We set HZ = 100 for DADS, and T = 50 for SeCTAR. According to the algorithmic
design, DADS fixed a trajectory length T = 1, and SeCTAR fixed a horizon HZ = 1. For WEDIS,
given each skill, it is shown that the actual trajectories are similar to the corresponding predictive
trajectories, which implies that our policy acquired disentangled and interpretable skills. On the
other hand, for SeCTAR and DADS, the interpretation of each dimension of the skill is unclear.
SeCTAR	DADS	WEDIS (Ours)
htaP detciderP htaP lautcA
Z2
Z3
Z1
z1 (Direction)
z1
Z2
z2 (Speed) z3 (Curvature)
Figure 5: The latent traversals of the decoder for each skill. (Top) Predicted trajectories by the
predictive model. (Bottom) Actual trajectories of the agent by the policy network. The colors of
(red, green, blue, cyan, yellow) correspond to the values of (-1, -0.5, 0, 0.5, 1) of latents, respectively.
6.2	Quantitative Results
To evaluate the performance on the hierarchical RL prob-
lems, we set two challenging maze navigation tasks as
shown in Figure 6, which cannot be solved with a single
skill, but require to appropriately combine several skills.
Also, due to the existence of traps that are close to the goals
to some extent, these problems require long-horizon reason-
ing even for dense rewards cases. In addition to SeCTAR
and DADS, we compare with two ablations, single-step
Figure 6: Two navigation tasks. The
goals are denoted as the red circles.
transition (WEDIS-S, T = 1) and no trajectory scaling (WEDIS-NTS, Hz = 1). All of these algo-
rithms can solve the hierarchical RL problems with zero-shot via the MPC over the latent space. The
8
Under review as a conference paper at ICLR 2022
task reward is given by negative distance to goal position r(s) = -ks - gk2. With the task reward
and an additional collision penalty, We set the planning reward as rd(S) = r(S) - Wp ∙ l(s=collision)
for dense reward and rs(S) = Wr ∙ l(∣r(s)|<e)— Wp ∙ l(s=collision)for sparse reward, where 1 is
an indicator function that outputs one if the condition is true and zero otherwise. wr and wp are a
reward weight and a penalty weight, respectively, and is an acceptable error.
6.2	. 1 Navigation via Model-based Planning
In the MPC planning, we evaluated the performance on both dense reward and sparse reward cases.
As an evaluation metric, we used the averaged distance between the goal and the final position over
ten trials of planning. In Figure 7, we plot the evaluation results according to sample size used for
the MPC for comparing the sample-efficiency. WEDIS shows the outstanding results for all the
cases, even with less planning samples. Especially, the results on the sparse reward demonstrate
that WEDIS is effective in solving long-horizon problems. WEDIS-S and DADS are superior than
WEDIS-NTS, implying a larger scale is required for an effective planning, despite of the benefit
of the trajectory-level prediction. In addition, while both perform temporally-extended behaviors
over single-step transitions, WEDIS-S outperforms DADS, which implies benefits of the disentan-
glement. For example, the controllability of speed can be beneficial to avoiding collisions to the
wall. For SeCTAR, it has poor performance since the skills do not follow predictions well. Some
examples of the MPC results are shown in Figure 8, where WEDIS plots smooth trajectories over
large scales. For the more details of the planning parameters, please see Appendix A.1.4.
Mazel (Dense)	Mazel (Sparse)	Maze2 (Dense)	Maze2 (Sparse)
SeCTAR 。DADS →- WEDIS-S ♦ WEDIS-NT5 T- WEDIS
Figure 7: The evaluation results of the MPC on the navigation problems. The dashed gray lines
denote goal-distances from the origin. WEDIS outperforms the unsupervised skill learning methods.
SeCTAR(T =	40, HZ	=1)	DADS (T	= 1, HZ	= 20)	WEDIS-S (T	= 1, HZ	=	40) WEDIS-NTS (T	= 20, HZ	= 1) WEDIS (T	= 5, HZ	= 40)
Figure 8: Examples of the MPC results on sparse rewards (Top) Maze 1. (Bottom) Maze 2. The
blue and red lines represent the predictive paths and the actual paths, respectively, and the cross
marks indicate the goal positions. The dashed gray lines denote error between the last positions of
predictive and actual trajectories at each planning step.
7	Conclusion
This work has introduced a method to learn continuous skills from the disentangled and interpretable
representations of trajectories. To do that, we proposed a WET-VAE model by extending the trajec-
tory VAE with weak labels. Using the interpretability, we proposed to train the policy network with
single-step transitions and perform the trajectory-level behaviors at test time, which simplifies the
exploration problem and provides an effective large-scale planning strategy. In the experiments of
challenging navigation tasks, we demonstrated that our method outperforms the unsupervised skill
learning methods with higher sample-efficiency.
9
Under review as a conference paper at ICLR 2022
8	Reproducibility Statement
For the reproducibility of the proposed method, we will open our code at
https://sites.google.com/view/iclr2022-wedis. We also provide the full hyperparameters in
Appendix A.1.
References
Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery
algorithms. arXiv preprint arXiv:1807.10299, 2018.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 31, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Victor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giro-i Nieto, and Jordi
Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In Interna-
tional Conference on Machine Learning, pp. 1317-1327. PMLR, 2020.
Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentangle-
ment in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep rein-
forcement learning in a handful of trials using probabilistic dynamics models. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
3de568f8597b94bda53149c7d7f5958c- Paper.pdf.
John Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey
Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajec-
tory embeddings. In International Conference on Machine Learning, pp. 1009-1018. PMLR,
2018.
Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decompo-
sition. Journal of artificial intelligence research, 13:227-303, 2000.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical rein-
forcement learning. arXiv preprint arXiv:1704.03012, 2017.
Carlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: Theory and prac-
tice—a survey. Automatica, 25(3):335-348, 1989.
David Ha and Jurgen Schmidhuber.	Recurrent world models facilitate policy evolution.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
2de5d16682c3c35007e4e92982f1a2ba- Paper.pdf.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. 2017.
Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies
for hierarchical reinforcement learning. In International Conference on Machine Learning, pp.
1851-1860. PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.
Learning an embedding space for transferable robot skills. In International Conference on Learn-
ing Representations, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
Machine Learning,pp. 2649-2658. PMLR, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In international conference on machine learning, pp. 4114-4124.
PMLR, 2019a.
Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Ratsch, Bernhard Scholkopf,
and Olivier Bachem. Disentangling factors of variation using few labels. arXiv preprint
arXiv:1905.01258, 2019b.
Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael
Tschannen. Weakly-supervised disentanglement without compromises. In International Confer-
ence on Machine Learning, pp. 6348-6359. PMLR, 2020.
Vasilis Margonis, Athanasios Davvetas, and Iraklis A Klampanos. Wela-vae: Learning alternative
disentangled representations using weak labels. arXiv preprint arXiv:2008.09879, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics
for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), pp. 7559-7566. IEEE, 2018.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International conference on machine learning,
pp. 1278-1286. PMLR, 2014.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Tanmay Shankar and Abhinav Gupta. Learning robot skills with temporal variational inference. In
International Conference on Machine Learning, pp. 8624-8633. PMLR, 2020.
Tanmay Shankar, Shubham Tulsiani, Lerrel Pinto, and Abhinav Gupta. Discovering motor programs
by recomposing demonstrations. In International Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2022
Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019.
Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disen-
tanglement with guarantees. arXiv preprint arXiv:1910.09772, 2019.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181—
211, 1999.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
A	Appendix
A. 1 Implementation Details
A.1.1 Trajectory Dataset and Weak Labels
To encourage the disentangled representation learning, we created a trajectory dataset that consists
of one-hundred thousand numbers of synthetically generated trajectories. Each trajectory is gener-
ated from the combination of several generative factors of interest that we intend the model to learn.
For 2D navigation, the trajectories consist of sequences of two-dimensional data points of x and y
coordinates. As the factors, we set three factors of speed, direction, and curvature of the trajectories.
For the speed, we limited the length of each single-step transition up to one, maintaining it consis-
tently over multiple steps. To allow for only the positive speed and direction, we limited the direction
ranging the angles of a right-sided semicircle. For the curvature, we rotated the paths with an angle
ranging from -30 to 30 degrees at each transition step. Also, to include straight lines together, we
set the angle for curvature to zero with 0.2 probability. Finally, we set the trajectory length T = 5,
which means a sequence of five transition steps that are composed of six points including an initial
point. The details of the factors to generate trajectories used in the main experiments and additional
experiments in the appendices are summarized in Table 1.
Factors	Sampling Distribution
Speed	U(-1,1) (1D), U(0,1) (2D)
Direction	U(-π∕2, π2) (semi), U(-π, π) (full)
Curvature	U (—π∕6, π∕6)
Acceleration	U(-s/5, s/5), S = sampled speed
Table 1: The generative Factors
While the training proceeds, weak labels are assigned to the batches as one-hot encoded vectors
for each factor, which are computed by programming based on the knowledge of the factors. We
roughly define the labeling criteria to divide the factors (speed, direction, and curvature) into four
(0.25 interval), four (45 degree interval), and three (straight, right, left) segments. For instance,
given a trajectory with (speed = 0.1, direction = 40 degree, curvature angle = -10 degree), then the
trajectory is assigned with label values of one, two, and three for each factor.
A.1.2 WET-VAE
We used one hidden layer with size of (64, 64) for all units in Figure 1. At each transition step, all
of the state and one-hot encoded multiple labels are concatenated as an input. To consider state-
agnostic transitions, we infer distances from the current states and next states instead of directly
12
Under review as a conference paper at ICLR 2022
-	4
-	3
-	2
1 I
Speed
Direction	Curvature Samples of the trajectory dataset
Figure 9: The labeling criteria and the trajectories in the dataset.
predicting next states. Therefore, we feed the distances as state inputs for the encoding LSTM, and
the state decoder also outputs mean and variance for the distances. For training losses, we used the
negative log-likelihood for the state outputs and the cross-entropy for the label outputs. For activa-
tion functions of the output layers, we used linear functions for the mean and log-variance of the
state decoder, and softmax functions for the label decoders. In Equation 4, we set the hyperparame-
ters as γ = 10, β = 9. We trained the model using Adam optimizer (Kingma & Ba, 2014) with the
fixed learning rate of 2e -4, the batch size of 2048, and 300 epochs.
A.1.3 Policy Network
For the policy optimization, we used the SAC algorithm (Haarnoja et al., 2017). To learn the
position-agnostic behaviors, We Used the state input S to the policy π(a|s, Z), excluding the Po-
sitional information of (x , y) coordinates in Ant and x coordinate in Half-Cheetah as in DADS
(Sharma et al., 2019). While We trained the WET-VAE With the maximum speed of one, We com-
puted the log-likelihood log pθ (τ |z) for the reWard by reducing the mean by 1/10 and the variance
by 1/20 in Equation 6, considering the alloWable length of one-step movement of the Ant. For
Half-Cheetah, We reduced them by 1/5 and 1/20, respectively. In the SAC, We used the entropy tem-
perature of 0.01. We trained the model using Adam optimizer (Kingma & Ba, 2014) With the fixed
learning rate of 3e -4, the batch size of 256, and 64 iterations per epoch for totally 30000 epochs. For
other hyperparameters, We used the discount factor of 0.99, target smoothing coefficient of 0.01, the
latent sampling distribution as the unit Gaussian distribution, and tWo hidden layers of (512, 512)
for all of the actor, critic and value function in Ant, and (256, 256) in Half-Cheetah.
A.1.4 Planning
We set primitive horizons Hz = 20 for DADS and Hz = 40 for WEDIS and WEDIS-S, Which
are determined by comparing the average speed. For SeCTAR and WEDIS-NTS, We set trajectory
lengths as T = 40 and T = 20, respectively. According to the primitive horizons and trajectory
lengths, We set episode horizons as HE = 300 for DADS and HE = 600 for others. For dense
reWard tasks, We used planning horizons HP = 4 for all methods except for HP = 3 of WEDIS.
In contrast, for sparse reWard tasks We used the maximum planning horizons HE /(Hz × T) for all
methods. These planning parameters are summarized in Table 2.
For sampling distribution of the skills, DADS that Was trained from the uniform distribution used a
uniform distribution from -1 to 1, and others that Were trained from the normal Gaussian distribution
used a uniform distribution from -1.5 to 1.5 except for the dimension of speed factor of WEDIS and
its ablations. For WEDIS and the ablations, We limit the minimum speed as the one sampled from
zero since too loW speed is useless for the navigation. Note that this strategy is also an advantage of
our method thanks to the disentanglement and interpretability of the skills. For the predictions, We
used the mean output from the WET-VAE decoder as the trajectories Without sampling. The actions
of an agent Were sampled from the distributions given by the policy.
We used the collision penalty Weight wp = 100, the reWard Weight wr = 5, and the acceptable error
= 5 in the sparse reWard cases. In Figure 6, the goals are positioned on the Cartesian coordinate of
(24, 12). As the straight paths are blocked on the Walls in the mazes, the effective distance to goals
is 36.
13
Under review as a conference paper at ICLR 2022
Algorithms	HE	Hz	T	Steps / Skill	HP (Dense / Sparse)	Sampling Distribution
SeCTAR	600	1	40	40	4/15	U (-1, 1)
DADS	300	20	1	20	4/15	U (-1.5, 1.5)
WEDIS-S	600	40	1	40	4/15	U (-1.5, 1.5), Uspeed (-1.5, 0)
WEDIS-NTS	600	1	20	20	4 / 30	U (-1.5, 1.5), Uspeed (-1.5, 0)
WEDIS	600	40	5	200	3/3	U (-1.5, 1.5), Uspeed (-1.5, 0)
Table 2: The MPC parameters
A.2 Derivation
A.2.1 Variational Lower B ound
log pθ(τ, y) = log	pθ(τ, y, z)dz
z
= log	pθ(τ, y|z)p(z)dz
z
pθ(τ, y|z)p(z)
= log L	qφ(z∣τ, y) …y)dz
≥ Z qφ(z∣τ, y)log *”)dz
Jz	qφ(z∣τ, y)
=q qφ(z|T, y)logpθ(T, y|z)dz + q qφ(z|T, y)log
zz
p(z)
qφ(Z|T, y)
dz
Eqφ(z∣τ,y)[logpθ(T y|z)] - DKL(qφ(z|T, y)kp(z)) = L(θ,φ;τ, y),
(7)
where the inequality is from Jensen’s inequality. The log-likelihood log pθ(τ, y|z) is decomposed
as below.
log pθ (T, y |z) = log [Pθ(T |z )Pθ (y |z)]
=log[pθ (si,…,ST |z )Pθ (y1,…,y M |z)]
=lθg[pθ (si|z)Pθ(S2∣S1, Z) …Pθ (ST |si: T-1, Z)Pθ (y 1∣Z) …Pθ (y M |z)]
TM
=log [YPθ(St|si：t-i, z) Y Pθ(yTZ)]
t=1	m=1
TM
=ElogPθ(St |si：t-i, z) + E logPθ(ym|z)
t=1	m=1
(8)
Therefore, the objective function of WET-VAE becomes
L(θ, φ;τ, y) = Eqφ(z∣τ,y)[logpθ(T, y |z)] - DκL(qφ(z|T, y)kp(z))
TM
Eqφ(z∣τ,y)[ɪ^logPθ(St |si：t-1, Z) + E logPθ(yTZ)] - DκL(qφ(z∣τ, y)kp(z))
t=1
m=1
(9)
By adding the additional weights to this equation, we finally obtain Equation 4.
14
Under review as a conference paper at ICLR 2022
A.2.2 Policy Reward
Ep(Z )[Dkl(pψ (T |z )∣∣pθ (T |z))] = Ep(Z)[/ pψ (T |z )log pψ (TF) d τ ]
τ	pθ (τ |z )
ZZP(Z) Zpψ (TIz )log pθ≡d T dz
/ / P(Z)pψ (TIz)log P^IdTdz
/ / p(z)pψ(tIz)[logPψ(tIz) - logPθ(t∣ζ)]dTdz
P(z)Pψ(TIz)logPψ(TIz)dTdz -
P(z)	Pψ(TIz) logPψ(TIz)dTdz -
-H(Pψ (T Iz)) - Epψ(τ,Z)[logPθ(TIz)]
P(z)Pψ(TIz) logPθ(TIz)dTdz
Pψ(T, z) logPθ(TIz)dTdz
(10)
As the above equation is an objective function for a minimization problem, we can obtain Equation
6 by changing the sign.
A.3 Additional Experimental Results
A.3.1 WET-VAE
In this section, we first check the effects of the two weights, β and γ in Equation 4. Then, we
show the results of the three additional experiments to learn other representations; 1) learning polar
coordinate representation and Cartesian coordinate representation using proper weak labels for each
representation in 2D. 2) disentanglement of speed and acceleration in 1D. 3) disentanglement of
four factors of speed, direction, curvature and acceleration in 2D. For each of the three additional
experiments, we generated the trajectory dataset again by following Table 1.
Each column in Figure 10 shows the latent traversals of models with all the same hyper-parameters
except for β and γ. All models are trained with same dataset used in Section 6.1.1. For the model in
(e), we did not reuse the model in Section 6.1.1, but reproduced it again for this experiment. These
results imply that the proper choices of both β and γ are required for the training of the WET-VAE.
Figure 11 shows the latent traversals of representations of a two-dimensional latent variable z ∈ R2
learned with the polar coordinate and Cartesian coordinate. For the polar coordinate representation,
we used weak labels of radius divided into four segments with 0.25 interval from zero to one, and
angle divided into four segments with an interval of 90 degrees from zero to 360 degrees. For the
Cartesian coordinate representation, we used weak labels of x and y coordinates divided into four
segments with 0.5 interval from -1 to 1. For comparison, we also provide the results of learning
Cartesian coordinate in an unsupervised manner without weak labels. The three models in Figure
11 are trained with the same trajectory dataset generated by the combination of the radius (speed)
and angle (direction). It is worthy of noting that the uses of different (x and y coordinates) or no
labels can result in different representations from the true generative factors (radius and angle). This
implies the importance of the inductive bias to learn the desired factors.
To disentangle speed and acceleration factors in 1D, we set the weak labels divided into four seg-
ments from -1 to 1 for speed, and three segments of zero, positive and negative values the ac-
celeration. Figure 12 shows that the WET-VAE learns the disentangled representation of a two-
dimensional latent variable z ∈ R2 with speed and direction factors in 1D. In this case, the factor
contributing to single-step transition z single is the speed factor z speed, and the factor contributing
over multiple time-steps z multi is the acceleration factor zacc. On the other hand, when there is no
weak supervision, the model does not learn the meaningful representations.
In Figure 13, it is shown that the WET-VAE can learn the disentangled representations with multiple
factors in both z single and z multi, where z single = (z speed, zdir) and z multi = (zacc, zcur). In this
15
Under review as a conference paper at ICLR 2022
case, the model learns the four-dimensional latent representations z ∈ R4 with weak labels for each
factor, where we set the full-circle range for the direction.
As shown in Figure 11 and Figure 13, even if the model was trained with dataset of the full-circle
angles with the weak supervision, the empty regions appear at some angles. The empty regions
were also shown differently at each training even with the same hyperparameters and dataset. In
this reason, we trained the model with the semicircle angles in the main experiments to completely
cover the only positive directions even though the model trained with full circle angles can cover the
larger range of the direction.
(a) S = 1, γ =0
(b) B = 1, Y = 1
(c) β = 9, Y = 1
⑹ β = 1, γ = 10
(e) S = 9, γ = 10
Figure 10: The latent traversals from -1.5 to 1.5. (a)〜(e) show the results from the models trained
with all the same hyper-parameters except for β and γ. The trajectories in all figures are plotted with
T = 5.
Polar coordinate
Without weak supervision
Cartesian coordinate
T = 1
T = 1
T = 1
Figure 11: The latent traversals from -1.5 to 1.5 (Left) polar coordinate representation (Middle)
Cartesian coordinate representation (Right) representation learned without weak supervision.
16
Under review as a conference paper at ICLR 2022
With Weak supervision	Without Weak supervision
(Pəəds) EZ (Uo-wə-əɔɔw
T = 1	T = 5	T = 5
Figure 12: The latent traversals from -1 to 1. WET-VAE can learn the disentangled representations
of the acceleration and speed in 1D. For visibility, we represent the values of the latents at the y axis
and use different colors at each time-step for T = 5. Since the speed is zero for z 1 = 0, the first
two figures of the second rows are plotted with z 1 = -1.
z2 (Acceleration)
z3 (Direction)
Figure 13: The latent traversals from -1.5 to 1.5. WET-VAE can learn the disentangled represen-
tations of the curvature, acceleration, direction and speed. For visibility, we plot the acceleration
figure with the points with different colors for each time-step, instead of the arrowed lines.
A.3.2 Policy network on Ant of full circle angles and Half-Cheetah
Ant	Half-Cheetah
Figure 14: Mujoco environments. (Left) Ant (Right) Half-Cheetah
We present the additional experimental results for the Ant and the Half-Cheetah of Mujoco in Figure
14. Figure 15 shows that the Ant learns the disentangled skills of the speed and direction of the full
circle angles in 2D. In Figure 16, it is shown that the Half-Cheetah learns the disentangled speed-
acceleration skills in 1D.
To train the policy network for the Half-Cheetah, the 1D speed-acceleration model in Figure 12 is
used as the predictive model. After training, the relation function was set as f speed (ztspeed, zacc) =
speed
Zspeed - 0.1 ∙ A ∙ Zacc.
A.3.3 More Planning Examples
We provide the additional MPC results of the main experiment for the Maze1 and Maze 2 in Figure
17. Additionally, we presents more results to show that our method is sample-efficient with the
examples in S-shaped maze and obstacles in Figure 18. In each planning procedure, we used a
sequence of just two skills for the maze and just a single skill for the obstacle, respectively.
17
Under review as a conference paper at ICLR 2022
z1 (Speed)
z2 (Direction)
(Pəəds)飞
Figure 15: The latent traversals in Ant trained with speed and the direction ranging the full circle
angles.
T = 6
Figure 16: The latent traversals from -1 to 1 in Half-Cheetah. (Top) predicted trajectories by the
predictive model. (Bottom) actual trajectories by the Half-Cheetah. For visibility, we represent the
values of the latents at the y axis and use different colors at each time-step for T = 6.
Figure 17: Additional MPC results. (Top) Maze1. (Bottom) Maze2.
Figure 18: Additional MPC results. (Top) S-shaped maze. We use T = 6 and Hz = 50 for a
sequence of two skills. (Bottom) Obstacles. T = 5 and Hz = 50 for single skill.
18