Under review as a conference paper at ICLR 2022
A Boosting Approach to Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
We study efficient algorithms for reinforcement learning in Markov decision pro-
cesses, whose complexity is independent of the number of states. This formulation
succinctly captures large scale problems, but is also known to be computationally
hard in its general form. Previous approaches attempt to circumvent the compu-
tational hardness by assuming structure in either transition function or the value
function, or by relaxing the solution guarantee to a local optimality condition.
We consider the methodology of boosting, borrowed from supervised learning, for
converting weak learners into an effective policy. The notion of weak learning we
study is that of sampled-based approximate optimization of linear functions over
policies. Under this assumption of weak learnability, we give an efficient algo-
rithm that is capable of improving the accuracy of such weak learning methods
iteratively. We prove sample complexity and running time bounds on our method,
that are polynomial in the natural parameters of the problem: approximation guar-
antee, discount factor, distribution mismatch and number of actions. In particular,
our bound does not explicitly depend on the number of states.
A technical difficulty in applying previous boosting results, is that the value func-
tion over policy space is not convex. We show how to use a non-convex variant of
the Frank-Wolfe method, coupled with recent advances in gradient boosting that
allow incorporating a weak learner with multiplicative approximation guarantee,
to overcome the non-convexity and attain global optimality guarantees.
1	Introduction
The field of reinforcement learning, formally modelled as learning in Markov decision processes
(MDP), models the mechanism of learning from rewards, as opposed to examples. Although the
case of tabular MDPs is well understood, the main difficulty in applying RL to practice is the size
of the state space.
Various techniques have been suggested and applied to cope with very large MDPs. The most
common of which is function approximation of either the value or the transition function of the
underlying MDP, many times using deep neural networks. Training deep neural networks in the
supervised learning model is known to be computationally hard. Therefore reinforcement learning
with neural function approximation is also computationally hard in general, and for this reason lacks
provable guarantees.
This challenge of finding efficient and provable algorithms for MDPs with large state space is the
focus of our study. Previous approaches can be categorized in terms of the structural assumptions
made on the MDP to circumvent the computational hardness. Some studies focus on structured
dynamics, whereas others on structured value function or policy classes w.r.t. to the dynamics.
In this paper we study another methodology to derive provable algorithms for reinforcement learn-
ing: ensemble methods for aggregating weak or approximate algorithms into substantially more
accurate solutions. Our method can be thought of as extending the methodology of boosting from
supervised learning (Schapire & Freund, 2012) to reinforcement learning. Interestingly, however,
our resulting aggregation of weak learners is not linear.
In order to circumvent the computational hardness of solving general MDPs with function approx-
imation, we assumes access to a weak learner: an efficient sample-based procedure that is capable
1
Under review as a conference paper at ICLR 2022
of generating an approximate solution to any linear optimization objective over the space of poli-
cies. We describe an algorithm that iteratively calls this procedure on carefully constructed new
objectives, and aggregates the solution into a single policy. We prove that after sufficiently many
iterations, our resulting policy is provably near-optimal.
1.1	Challenges and techniques
Reinforcement learning is quite different from supervised learning and several difficulties have to be
circumvented for boosting to work. Amongst the challenges that the reinforcement learning setting
presents, consider the following,
(a)	The value function is not a convex or concave function of the policy. This is true even in
the tabular case, and even more so if we use a parameterized policy class.
(b)	The transition matrix is unknown, or prohibitively large to manipulate for large state spaces.
This means that even evaluation of a policy cannot be exact, and can only be computed
approximately.
(c)	It is unrealistic to expect a weak learner that attains near-optimal value for a given linear
objective over the policy class. At most one can hope for a multiplicative and/or additive
approximation of the overall value.
Our approach overcomes these challenges by applied several new as well as recently developed
techniques. To overcome the nonconvexity of the value function, we use a novel variant of the
Frank-Wolfe optimization algorithm that simultaneously delivers on two guarantees. First, it finds a
first order stationary point with near-optimal rate. Secondly, if the objective happens to admit a cer-
tain gradient domination property, an important generalization of convexity, it also guarantees near
optimal value. The application of the nonconvex Frank-Wolfe method is justified due to previous
recent investigation of the policy gradient algorithm (Agarwal et al., 2019; 2020a), which identified
conditions under which the value function is gradient dominated.
The second information-theoretic challenge of the unknown transition function is overcome by care-
ful algorithmic design: our boosting algorithm requires only samples of the transitions and rewards.
These are obtained by rollouts on the MDP.
The third challenge is perhaps the most difficult to overcome. Thus far, the use of the Frank-Wolfe
method in reinforcement learning did not include a multiplicative approximation, which is critical
for our application. Luckily, recent work in the area of online convex optimization (Hazan & Singh,
2021) studies boosting with a multiplicative weak learner. We make critical use of this new technique
which includes a non-linear aggregation (using a 2-layer neural network) of the weak learners. This
aspect is perhaps of general interest to boosting algorithm design, which is mostly based on linear
aggregation.
1.2	Our contributions
Our main contribution is a novel efficient boosting algorithm for reinforcement learning. The input
to this algorithm is a weak learning method capable of approximately optimizing a linear function
over a certain policy class.
The output of the algorithm is a policy which does not belong to the original class considered. It is
rather a non-linear aggregation of policies from the original class, according to a two-layer neural
network. This is a result of the two-tier structure of our algorithm: an outer loop of non-convex
Frank-Wolfe method, and an inner loop of online convex optimization boosting. The final policy
comes with provable guarantees against the class of all possible policies.
Our algorithm and guarantees come in four flavors, depending on the mode of accessing the MDP
(two options), and the boosting methodology for the inner online convex optimization problem (two
options).
It is important to point out that we study the question from an optimization perspective, and hence,
assume the availability of an efficient exploration scheme - either via access to a reset distribution
that has some overlap with the state distribution of the optimal policy, or constraining the policy
2
Under review as a conference paper at ICLR 2022
	Supervised weak learner	Online weak learner		
Episodic model	c∞ (Π)∕α4ε5	c∞ (Π)∕α2ε3	C∞ = maxπ∈Π	* dπ dπ ∞
Rollouts w. ν-resets	D∞ ∕α4ε6	D∞/α2ε4	—* D =	d- D∞ = V	∞
Table 1: Sample complexity of the proposed algorithms for different α-weak learning models (su-
pervised & online) and modes of accessing the MDP (rollouts & rollouts with reset distribution ν),
suppressing polynomial factors in |A|, 1/(1 - γ). See Theorem 11 for details.
class to policies that explore sufficiently. Such considerations also arise when reducing reinforce-
ment learning to a sequence of supervised learning problems, e.g. Conservative Policy Iteration
(Kakade & Langford, 2002) assumes the former. One contribution we make here is to quantita-
tively differentiate between these two modes of exploration in terms of the rates of convergence they
enable for the boosting setting.
1.3	Related work
To cope with prohibitively large MDPs, the method of choice to approximate the policy and transi-
tion space are deep neural networks, dubbed “deep reinforcement learning". Deep RL gave rise to
beyond human performance in games such as Go, protein folding, as well as near-human level au-
tonomous driving. In terms of provable methods for deep RL, there are two main lines of work. The
first is a robust analysis of the policy gradient algorithm (Agarwal et al., 2019; 2020a). Importantly,
the gradient domination property of the value function established in this work is needed in order to
achieve global convergence guarantees of our boosting method.
The other line of work for provable approaches is policy iteration, which uses a restricted policy
class, making incremental updates, such as Conservative Policy Iteration (CPI) (Kakade & Langford,
2002; Scherrer & Geist, 2014), and Policy Search by Dynamic Programming (PSDP)(Bagnell et al.,
2003).
Our boosting approach for provable deep RL builds on the vast literature of boosting for supervised
learning (Schapire & Freund, 2012), and recently online learning (Leistner et al., 2009; Chen et al.,
2012; 2014; Beygelzimer et al., 2015; Jung et al., 2017; Jung & Tewari, 2018). One of the crucial
techniques important for our application is the extension of boosting to the online convex optimiza-
tion setting, with bandit information (Brukhim & Hazan, 2021), and critically with a multiplicative
weak learner (Hazan & Singh, 2021). This latter technique implies a non-linear aggregation of the
weak learners. Non-linear boosting was only recently investigated in the context of classification
(Alon et al., 2020), where it was shown to potentially enable significantly more efficient boosting.
Perhaps the closest work to ours is boosting in the context of control of dynamical systems (Agar-
wal et al., 2020b). However, this work critically requires knowledge of the underlying dynamics
(transitions), which we do not, and cannot cope with a multiplicative approximate weak learner.
The Frank-Wolfe algorithm is extensively used in machine learning, see e.g. (Jaggi, 2013), refer-
ences therein, and recent progress in stochastic Frank-Wolfe methods (Hassani et al., 2017; Mokhtari
et al., 2018; Chen et al., 2018; Xie et al., 2019). Recent literature has applied a variant of this algo-
rithm to reinforcement learning in the context of state space exploration (Hazan et al., 2019).
2	Preliminaries
Optimization. We say that a differentiable function f : K 7→ R over some domain K is L-smooth
with respect to some norm k」* if for every x,y ∈ K We have
f (y) - f(χ)-W(χ)>(y - χ)∣ ≤ 2l∣χ - yk*.
3
Under review as a conference paper at ICLR 2022
For constrained optimization (such as over ∆A), the projection Γ : R|A| → ∆A of a point x to onto
a domain ∆A is
Γ[x] = arg min kx - yk.
y∈∆A
An important generalization of convex function we use henceforth is that of gradient domination,
Definition 1 (Gradient Domination). A function f : K → R is said to be (κ, τ, K1 , K2)-locally
gradient dominated (around K1 by K2) if for all x ∈ K1, it holds that
maxf (y) - f(x) ≤ K × max {Vf(x)>(y - x)} + τ.
y∈K	y∈K2
Markov decision process. An infinite-horizon discounted Markov Decision Process (MDP) M =
(S, A, P, r, γ , d0 ) is specified by: a state space S, an action space A, a transition model P where
P (s0 |s, a) denotes the probability of immediately transitioning to state s0 upon taking action a at
state s, a reward function r : S × A → [0, 1] where r(s, a) is the immediate reward associated with
taking action a at state s, a discount factor γ ∈ [0, 1); a starting state distribution d0 over S. For any
infinite-length state-action sequence (hereafter, called a trajectory), we assign the following value
∞
V(τ = (s0,a0,s1,a1,.. .)) =	γtr(st, at).
t=0
The agent interacts with the MDP through the choice of stochastic policy π : S → ∆A it exe-
cutes, where ∆A denotes the probability simplex over A. The execution of such a policy induces a
distribution over trajectories τ = (s0 , a0, . . . ) as
∞
P(T∣π) = do(S0)[ɪ(p(st+ι∣st,at)∏(at∣st)).	(1)
t=0
Using this description we can associate a state V π(s) and state-action Qπ(s, a) value function with
any policy π. For an arbitrary distrbution d over S, define:
∞
Qπ(s) = E X γtr(st, at) π, s0 = s, a0 = a ,
t=0
Vπ(S) = Ea〜∏(∙∣s) [Qπ(s, a)l∏, s],	V∏ = Eso" [Vπ(s)l∏].
Here the expectation is with respect to the randomness of the trajectory induced by π in M . When
convenient, We shall use Vπ to denote Vn, and V * to denote max∏ Vπ.
Similarly, to any policy π, one may ascribe a (discounted) state-visitation distribution dπ = ddπ .
∞
dπ (S) = (I-Y) X Y t X P(T lπ,s0~ d)
t=0	T ：St=S
Modes of Accessing the MDP. We henceforth consider tWo modes of accessing the MDP, that are
standard in the reinforcement learning literature, and provide different results for each.
The first natural access model is called the episodic rollout setting. This mode of interaction alloWs
us to execute a policy, stop and restart at any point, and do this multiple times.
Another interaction model We consider is called rollout with ν-restarts. This is similar to the
episodic setting, but here the agent may draW from the MDP a trajectory seeded With an initial state
distribution ν 6= d0 . This interaction model Was considered in prior Work on policy optimization
Kakade & Langford (2002); AgarWal et al. (2019). The motivation for this model is tWo-fold:
first, ν can be used to incorporate priors (or domain knoWledge) about the state coverage of the
optimal policy; second, ν provides a mechanism to incorporate exploration into policy optimization
procedures.
3	Setting: Policy Aggregation and Weak Learning
Our boosting algorithms henceforth call upon Weak learners to generate Weak policies, and aggre-
gate these policies in a Way that guarantees eventual convergence to optimality. In this section We
formalize both components.
4
Under review as a conference paper at ICLR 2022
A Policy Tree
π ∈ Π(Π,N,T)
Figure 1: The figure illustrates a Policy Tree hierarchy (see Definition 5), obtained by setting N = 3
on the inner loop, and T = 2 on the outer loop, to overall get all base policies π1, ..., π6 ∈ ΠW on
the lower level. The middle level holds T = 2 Policy Shurbs (see Definition 4), where each Shrub
λt ∈ Λ(Π, N) is an aggregation of base policies. The top level is an weighted aggregation of the
projected shrubs Γ[λt] , which forms the overall Policy Tree π ∈ Π(Π, N, T).
3.1	Policy aggregation
For a base class of policies ΠW, our algorithm incrementally builds a more expressive policy class
by aggregating base policies via both linear combinations and non-linear transformations. In effect,
the algorithm produces a finite-width depth-2 circuit over some subset of the base policy class. We
start with the simpler linear aggregation.
Definition 2 (Function Aggregation). Given some N0 ∈ Z+, w ∈ RN0, (f1, . . . fN0 ) ∈ (S →
R|A| )0N0, We define f = PN= 1 Wnfn to be the unique function f : S → RA for which simultane-
ously for all s ∈ S, it holds
N0
f(s) = X wnf(s).
n=1
Next, the projection operation below may be viewed as a non-linear activation, such as ReLU, in
deep learning terms. Note that the projection of any function from S to R|A| produces a policy, i.e.
a mapping from states to distributions over actions.
Definition 3 (Policy Projection). Given a function f : S → R|A| , define a projected policy π = Γ[f]
to be a policy such that simultaneously for all S ∈ S, it holds that ∏(∙∣s) = Γ [f (s)].
The next definition defines the class of functions represented by circuits of depth 1 over a base
policy class. Note that these function do not necessarily represent policies since they take an affine
(vs. convex) combination of policies.
Definition 4 (Shrub). For an arbitrary base policy class Π ⊆ S → ∆A, define Λ(Π, N) to be a set
such that λ ∈ Λ(Π, N) if and only if there exists N ≤ N,w ∈ RN0, (π1,... πN) ∈ Π0N0 such
that λ = PnN=01 wnπn.
The final definition describes the set of possible outputs of the boosting procedure.
Definition 5 (Policy Tree). For an arbitrary base policy class Π ⊆ S → ∆A, define Π(Π, N, T) tobe
a policy class such that π ∈ Π(Π, N, T) if and only if there exists T0 ≤ T, w ∈ ∆T0, (λ1 , . . . λT0 ) ∈
Λ(Π,N产T0 such that π = PT= 1 wtΓ[λt].
It is important that the policy that the boosting algorithm outputs can be evaluated efficiently. In the
appendix we show it is indeed the case (see Lemma 15).
5
Under review as a conference paper at ICLR 2022
3.2	Models of weak learning
We consider two types of weak learners, and give different end results based on the different as-
sumptions: weak supervised and weak online learners. In the discussion below, let πr be a uniformly
random policy, i.e. ∀(s,a) ∈ S X A,∏r(a∣s) = 1/|A|.
Supervised Learning. The natural way to define weak learning is an algorithm whose perfor-
mance is always slight better than that of random policy, one that chooses an action uniformly at
random at any given state. However, in general no learner can outperform a random learner over
all label distributions (this is called the “no free lunch" theorem). This motivates the literature on
agnostic boosting (Kanade & Kalai, 2009; Brukhim et al., 2020; Hazan & Singh, 2021) that defines
a weak learner as one that can approximate the best policy in a given policy class.
Definition 6 (Weak Supervised Learner). Let α ∈ (0, 1). Consider a class L of linear loss functions
` : RA → R, and D a family of distributions that are supported over S × L, policy classes ΠW, Π.
A weak supervised learning algorithm, for every ε, δ > 0, given m(ε, δ) samples Dm from any
distribution D ∈ D outputs a policy W(Dm) ∈ ΠW such that with probability 1 - δ,
E(s,')〜D ['(W (Dm))] ≥ α max E(s,')〜d['(π*(s))] +(1 - a)E(s,')〜D ['(π,(s))] -ε.
Note that the weak learner outputs a policy in ΠW which is approximately competitive against
the class Π. As an additional relaxation, instead of requiring that the weak learning guarantee
holds for all distributions, in our setup, it will be sufficient that the weak learning assumption holds
over natural distributions. We define these below. Hereafter, we refer to Π(ΠW , N, T) as Π for
N, T = O(poly(|A|, (1 - γ)-1, ε-1, α-1, log δ-1)) specified later.
Assumption 1 (Weak Supervised Learning). The booster has access to a weak supervised learning
oracle (Definition 6) over the policy class Π, for some α ∈ (0, 1). Furthermore, the weak learning
condition holds only for a class of natural distributions D 一 D ∈ D if and only if there exists some
π ∈ Π such that
DS(s) =
`
D(s,')dμ(')
dπ(s).
In particular, while a natural distribution may have arbitrary distribution over labels, its marginal
distribution over states must be realizable as the state distribution of some policy in Π over the MDP
M . Therefore, the complexity of weak learning adapts to the complexity of the MDP itself. As an
extreme example, in stochastic contextual bandits where policies do not affect the distribution of
states (say d0), it is sufficient that the weak learning condition holds with respect to all couplings of
a single distribution d0 .
Online Learning. The second model of weak learning we consider requires a stronger assumption,
but will give us better sample and oracle complexity bounds henceforth.
Definition 7 (Weak Online Learner). Let α ∈ (0, 1). Consider a class L of linear loss functions
` : RA → R. A weak online learning algorithm, for every M > 0, incrementally for each timestep
computes a policy Wm ∈ ΠW and then observes the state-loss pair (s, `t) ∈ S × L such that
M	MM
X : 'm (Wm(Sm)) ≥ α 叮ax X : 'm(π (Sm)) + (I - α) X : 'm (πr (Sm)) - RW (M) ∙
m=1	m=1	m=1
Assumption 2 (Weak Online Learning). The booster has access to a weak online learning oracle
(Definition 7) over the policy class Π, for some α ∈ (0, 1).
Remark 8. A similar remark about natural distributions applies to the online weak learner. In par-
ticular, it is sufficient the guarantee in 7 holds for arbitrary sequence of loss functions with high
probability over the sampling of the state from dπ for some π ∈ Π. Although stronger than su-
pervised weak learning, this oracle can be interpreted as a relaxation of the online weak learning
oracle considered in (Brukhim et al., 2020; Brukhim & Hazan, 2021; Hazan & Singh, 2021). A sim-
ilar model of hybrid adversarial-stochastic online learning was considered in (Rakhlin et al., 2011;
Lazaric & Munos, 2009; Beygelzimer et al., 2011). In particular, it is known (Lazaric & Munos,
2009) that unlike online learning, the capacity of a hypothesis class for this model is governed by its
VC dimension (vs. Littlestone dimension).
6
Under review as a conference paper at ICLR 2022
4	Algorithm & Main Results
In this section we describe our RL boosting algorithm. Here we focus on the case where a supervised
weak learning is provided. The online weak learners variant of our result is detailed in the appendix.
We next define several definitions and algorithmic subroutines required for our method.
The Extension Operator. The extension operator (Hazan & Singh, 2021) operate overs functions
and modifies their value outside and near the boundary of the convex set ∆A to aid the boosting
algorithm.
FG，e [f](X) = y1X ff (y) +Gz1X ky -Zk + 2β kx - yk2
To state the results, we need the following definitions. The first generalizes the policy completeness
notion from (Scherrer & Geist, 2014). It may be seen as the policy-equivalent analogue of inherent
bellman error (Munos & SzepesvE, 2008). Intuitively, it measures the degree to which a policy in Π
can best approximate the bellman operator in an average sense with respect to the state distribution
induced by a policy from Π.
Definition 9 (Policy Completeness). For any initial state distribution μ, define
Eμ(Π, Π) = max min Es〜d∏
π∈Π π*∈Π	μ
maxQπ(s, a) — Qπ(s, ∙)>π*(∙∣s)
The following notion of the distribution mismatch coefficient is often useful to characterize the
exploration problem faced by policy optimization algorithms.
Definition 10 (Distribution Mismatch). Let π* = arg max∏ Vπ, and V a fixed initial state distribu-
tion (see section 2). Define the following distribution mismatch coefficients:1
C∞ (Π) = max
π∈Π
ν
∞
∞
4.1	RL boosting via weak supervised learning
We give the main RL boosting algorithm, assuming supervised weak learners. We use a simple
sub-routine for choosing a step size, provided in the appendix.
Algorithm 1 RL Boosting via Weak Supervised Learning
1:	Input parameters T, N, M, P, μ. Initialize a policy ∏o ∈ ∏w arbitrarily.
2:	for t = 1 to T do
3:	Set ρt,o to be an arbitrary policy in ∏w.
4:	for n = 1 to N do
5:	Execute ∏t-ι for M episodes with initial state distribution μ via Algorithm 2, to get
Dt,n = {(si,Qi)im=1}.
6:	Modify Dt,n to produce a new dataset Dt0,n = {(si, fi)}im=1, such that for all i ∈ [m]:
fi = -VFG,β [—Qi](pt,n“si))
7:	Let At,n be the policy chosen by the weak learning oracle when given data set Dt0,n
8:	UPdate pt,n = (1 — η2,n)pt,n-1 + nOnAt,n — η2,n (1 — 1) πr.
9:	end for
10:	Declare πt0 = Γ [ρt,N].
11:	Choose ηι,t = min{1, 2C∞} if μ = do else ηι,t = StepChooser(∏t-ι,π0, μ, P).
12:	Update πt = (1 — η1,t)πt-1 + η1,t πt0.
13:	end for
14:	Output Π=∏τ if μ = do else output ∏t-ι with the smallest ηt.
1For brevity, We use the shorthand C∞ where clear from context.
7
Under review as a conference paper at ICLR 2022
Theorem 11.	Algorithm 1 samples T(MN + P) episodes oflength O( ɪ-^) with probability 1 一 δ.
IntheePisodicmodel,forT = O ((「2),N = ((16AC募),M = m ((I-YAε, NT),μ = do,
P = 0, with probability 1 一 δ,
In the ν -reset model, for T
m ((I-Y)3aε
m1 8∣A∣D∞
V * 一 Vπ ≤ C∞
(i-‰, N
ν, with probability 1 一 δ,
V* 一 Vπ
≤ D∞
E(Π, Π)
+ ε.
一 Y
16A急)2, p = o(2≡>), m
EV (∏,∏)
0-7)2
+ ε.
,μ
1
4.2 Trajectory sampler
In Algorithm 2 we describe an episodic sampling procedure, that is used in our sample-based RL
boosting algorithms described above. For a fixed initial state distribution μ, and any given policy
∏, we apply the following sampling procedure: start at an initial state so 〜μ, and continue to
act thereafter in the MDP according to any policy π, until termination. With this process, it is
straightforward to both sample from the state visitation distribution S 〜dπ, and to obtain unbiased
samples of Qn(s, ∙); see Algorithm 2 for the detailed process.
Algorithm 2 Trajectory Sampler: S 〜dπ, unbiased estimate of Qn
1:	Sample state so 〜μ, and action a0 〜U(A) uniformly.
2:	Sample S 〜dπ as follows: at every timestep h, with probability γ, act according to ∏; else,
accept sh as the sample and proceed to Step 3.
3:	Take action a0 at state sh, then continue to execute π, and use a termination probability of 1 一 γ.
Upon termination, set R(sh, a0) as the undiscounted sum of rewards from time h onwards.
4:	Define the vector Qn 九,such that for all a ∈ A, Q∏h (a) = |A| ∙ R(Sh,a') ∙Iα=a'.
5:	return (Sh, Qsnh).
5 Analysis — Proof Sketch
We sketch the high-level ideas of the proof of our main result, stated in Theorem 11, and refer the
reader to the appendix for the formal proof. Throughout the analysis, We use the notation V∏ Vn to
denote the gradient of the value function with respect to the |S | × |A|-sized representation of the
policy π, namely the functional gradient of Vn .
We establish an equivalence between the outlined algorithm and an abstraction of the Frank-Wolfe
algorithm (Algorithm D) from optimization theory. This variant of the Frank-Wolfe (FW) algo-
rithm operates over non-convex and gradient dominated functions to obtain the following novel
convergence guarantees. We establish the necessary gradient domination results from the policy
completeness results.
Theorem 12.	Let f : K → R be L-smooth in some norm ∣∣ ∙ ∣∣*, H-bounded, and the diameter of K
in k ∙ k* be D. Then, fora (∈o, K2) -linear optimization oracle, the output X ofAlgorithm D satisfies
max Vf (x)>(u — X)
∕2HLD2	C
≤ V τ	+ 3e+eo.
Furthermore, iff is (κ, τ, K1, K2)-locally gradient-dominated and Xo, . . . XT ∈ K1, then it holds
* *、	£、 / 2K2 max{LD2,H},,
max f (X ) — f (x) ≤ --------+----------+ T + κ<⅝.
x * ∈K	T
The Frank-Wolfe algorithm utilizes an inner gradient optimization oracle as a subroutine. To imple-
ment this oracle using approximate optimizers, we utilize yet another variant of the FW method as
“internal-boosting” for the weak learners (by employing an adapted analysis of Theorem 13).
8
Under review as a conference paper at ICLR 2022
5.1	Internal-boosting weak learners
We utilize a variant of the Frank-Wolfe method as a form “internal-boosting” for the weak learners,
by employing an adapted analysis of previous work that is stated below.
|A|
Note that Qπ(s, ∙) produced by Algorithm 2 satisfies ∣∣Qπ(s, ∙)k = i|-Y；. We can now borrow the
following result on boosting for statistical learning from (Hazan & Singh, 2021), specializing the
decision set to be ∆A. Let Dt be the distribution induced by the trajectory sampler in round t.
Theorem 13 ((Hazan & Singh, 2021)). Let β = J±, and η2,n = min{ 2, 1}. Then, for any t, π0
produced by Algorithm 1 satisfies with probability 1 - δ that
max E(s,Q)〜Dt [Q>π(S)] - E(s,Q)〜Dt [Q>π0(S)]
2|A|
(I — Y )α
≤
5.2	From weak learning to linear optimization
In the following Lemma, we give an important observation which allows us to re-state the guarantee
in the previous subsection in terms of linear optimization over functional gradients.
Lemma 14. Applying Algorithm 2for any given policy π, yields an unbiased estimate of the gradi-
ent, such that for any π0,
(Vn Vn)>π0 = 1-Y E(s,C(s,∙))〜D h∖Qπ (s, FM"S)i ,	⑵
where π0(∙∣s) ∈ Δa, and D is the distribution induced on the outputs of Algorithm 2, for a given
policy π and initial state distribution μ.
Proof. Recall VπV π denotes the gradient with respect to the |S| × |A|-sized representation of the
policy π 一 the functional gradient. Then, using the policy gradient theorem (Williams, 1992; Sutton
et al., 2000), it is given by,
∂vμπ	1
∂∏a^ = L dμ(s)Q (s,a).	(3)
The following sources of randomness are at play in the sampling algorithm (Algorithm 2): the
distribution dπ (which encompasses the discount-factor-based random termination, the transition
probability, and the stochasticity of π), and the uniform sampling over A. For a fixed S, π, denote by
Qn as the distribution over Qc(s, ∙) ∈ RA, induced by all the aforementioned randomness sources.
To conclude the claim, observe that by construction
_	,	,	.	r	一,
EQ∏(s,∙)[Qπ(s, ∙)lπ, s] = q (s, ∙).	(4)
□
6 Conclusions
Building on recent advances in boosting for online convex optimization and bandits, we have de-
scribed a boosting algorithm for reinforcement learning over large state spaces with provable guaran-
tees. We see this as a first attempt at using a tried-and-tested methodology from supervised learning
in RL, and many challenges remain.
First and foremost, our notion of weak learner optimizes a linear function over policy space. A more
natural weak learner would be an RL agent with multiplicative optimality guarantee, and it would
be interesting to extend our methodology to this notion of weak learnability.
Another important aspect that is not discussed in our paper is that of state-space exploration. Poten-
tially boosting can be combined with state-space exploration techniques to give stronger guarantees
independent of distribution mismatch C∞ , D∞ factors.
Finally, a feature of our method is that it produces nonlinear aggregations of weak learners as per a
two layer neural network. Are simpler aggregations with provable guarantees possible?
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019.
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-
ration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020a.
Naman Agarwal, Nataly Brukhim, Elad Hazan, and Zhou Lu. Boosting for control of dynamical
systems. In International Conference on Machine Learning, pp. 96-103. PMLR, 2020b.
Noga Alon, Alon Gonen, Elad Hazan, and Shay Moran. Boosting simple learners. arXiv preprint
arXiv:2001.11704, 2020.
J Andrew Bagnell, Sham Kakade, Andrew Y Ng, and Jeff G Schneider. Policy search by dynamic
programming. In Advances in Neural Information Processing Systems, 2003.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit
algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics, pp. 19-26. JMLR Workshop and Conference
Proceedings, 2011.
Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online
boosting. In International Conference on Machine Learning, pp. 2323-2331, 2015.
Nataly Brukhim and Elad Hazan. Online boosting with bandit feedback. In Algorithmic Learning
Theory, pp. 397-420. PMLR, 2021.
Nataly Brukhim, Xinyi Chen, Elad Hazan, and Shay Moran. Online agnostic boosting via regret
minimization. In Advances in Neural Information Processing Systems, 2020.
Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online opti-
mization with stochastic gradient: From convexity to submodularity. In International Conference
on Machine Learning, pp. 814-823, 2018.
Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. An online boosting algorithm with theoretical
justifications. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, pp. 1873-1880, 2012.
Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. Boosting with online binary learners for the
multiclass bandit problem. In International Conference on Machine Learning, pp. 342-350, 2014.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning, pp. 272-279, 2008.
Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi. Gradient methods for submodular max-
imization. In Advances in Neural Information Processing Systems, pp. 5841-5851, 2017.
Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.
Elad Hazan and Karan Singh. Boosting for online convex optimization. arXiv preprint
arXiv:2102.09305, 2021.
Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy
exploration. In International Conference on Machine Learning, pp. 2681-2691. PMLR, 2019.
Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International
Conference on Machine Learning, pp. 427-435. PMLR, 2013.
Young Hun Jung and Ambuj Tewari. Online boosting algorithms for multi-label ranking. In Inter-
national Conference on Artificial Intelligence and Statistics, pp. 279-287, 2018.
10
Under review as a conference paper at ICLR 2022
Young Hun Jung, Jack Goetz, and Ambuj Tewari. Online multiclass boosting. In Advances in neural
information processing Systems, pp. 919-928, 2017.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
Varun Kanade and Adam Kalai. Potential-based agnostic boosting. In Advances in neural informa-
tion processing systems, pp. 880-888, 2009.
Alessandro Lazaric and Remi Munos. Hybrid stochastic-adversarial on-line learning. In Conference
on Learning Theory, 2009.
Christian Leistner, Amir Saffari, Peter M Roth, and Horst Bischof. On robustness of on-line
boosting-a competitive study. In IEEE 12th International Conference on Computer Vision Work-
shops, ICCV Workshops, pp. 1362-1369. IEEE, 2009.
Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554, 2018.
Remi Munos and Csaba SzePeSvðri. Finite-time bounds for fitted value iteration. Journal OfMachine
Learning Research, 9(5), 2008.
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic and con-
strained adversaries. arXiv preprint arXiv:1104.5070, 2011.
Robert E Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.
Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative pol-
icy iteration as boosted policy search. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 35-50. Springer, 2014.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradi-
ent methods for reinforcement learning with function approximation. In S. Solla, T. Leen,
and K. Muller (eds.), Advances in Neural Information Processing Systems, volume 12.
MIT Press, 2000. URL https://proceedings.neurips.cc/paper/1999/file/
464d828b85b0bed98e80ade0a5c43b0f- Paper.pdf.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Jiahao Xie, Zebang Shen, Chao Zhang, Hui Qian, and Boyu Wang. Stochastic recursive gradient-
based methods for projection-free online learning. arXiv preprint arXiv:1910.09396, 2019.
11
Under review as a conference paper at ICLR 2022
A	Appendix
It is important that the policy that the boosting algorithm outputs can be evaluated efficiently. To-
wards that end, we give the following claim.
Claim 15. For any π ∈ Π(Π,N,T), ∏(∙∣s) for any S ∈ S can be evaluated using TN base policy
evaluations and O(T × (NA + A log A)) arithmetic and logical operations.
Proof. Since π ∈ Π(Π, N, T), it is composed of TN base policies. Producing each aggregated
function takes NA additions and multiplications; there are T of these. Each projection takes time
equivalent to sorting |A| numbers, due to a water-filling algorithm (Duchi et al., 2008); these are
also T in number. The final linear transformation takes an additional TA operations.	口
B	Step-size subroutine
Below we give an algorithm for choosing step sizes used in both of the RL boosting methods (for
online, and supervised, weak learners).
Algorithm 3 StepChooser(∏t-ι,π0, μ, P)
1:	Execute ∏t-ι for P episodes with initial state distribution μ via Algorithm 2, to get
D = {(si,Qci)iP=1}.
2:	For any policy π, let Gc = P PP=I Qi>∏(∙∣Si).
3:	Return
L ((1-γ)2
ηι,t = clip[o,i] I —2—
----
-Gπt-ι
C RL B oosting via Weak Online Learning
Algorithm 4 RL Boosting via Weak Online Learning
1:	Initialize a policy ∏o ∈ ∏w arbitrarily.
2:	for t = 1 to T do
3:	Initialize online weak learners W1 , . . . WN .
4:	for m = 1 to M do
5:	Execute ∏t-ι once with initial state distribution μ via Algorithm 2, to get (st,m, Qt,m).
6:	Choose ρt,m,0 ∈ ΠW arbitrarily.
7:	for n = 1 to N do
8:	Set pt,m,n = (I - η2,n)pt,m,n-1 + Inon Wn - H2,n ( 1 - 1) πr∙
9:	end for
10:	Pass to each Wn the following loss linear ft,m,n:
ʃ	— V7Z7 Γ /ɔ 1 ( r. ( IC ʌʌ
ft,m,n = -VFG,β [-Qt,m](Pt,m,n(Isi))
11:	end for
12:	Declare π0 = Mm Pm=ι γ [_pt,m,N].
13:	Choose ηι,t = min{1, 2C∞(n)} if μ = do else set ηι,t = StepChooser(∏t-ι, π0, μ, P).
14:	Update πt = (1 - η1,t)πt-1 + η1,t πt0.
15:	end for
16:	Output π= ∏t if μ = do else output ∏t-ι with the smallest 小.
Theorem 16. Algorithm 4 samples T(M + P) episodes oflength ɪ-^ log T(M+P)
1-δ. In the episodic model, Algorithm 4 guarantees as long as T = ；：—；)(「), N =
with probability
(16∣A∣C∞(∏) ∖2
∖ (1-γ)20e ),
12
Under review as a conference paper at ICLR 2022
M = max n 1000LA)4C∞α2π) log2 Tδ, 81A1 C∞(∏)RW(M) },μ = do, we have with probability 1 - δ
V * - Vπ ≤ C∞(∏) E(π,π) + ε
1-γ
In the ν -reset model, Algorithm 1 guarantees as long as T
P = 250D∞6A22 log2 T,
(1-γ)6ε2	δ ,
probability 1 - δ
M = max {(4-Yfe log δ)2
100D∞	N _ (20∣A∣D∞ A2
(1-γ)6ε2，N =1(1-γ)3αe J ，
10%DeYRW(M) },时=ν, we have With
V* - Vπ ≤ D∞
EV (∏,∏)
O-τF
+ε
If RW(M) = √M log |W| for some measure of weak learning complexity |W |, the algorithm
samples O ( C∞(1-%二空/1) episodes in the episodic model, and O ( D∞-^严锻空|) in the V-
reset model.
D Non-convex Frank-Wolfe
In this section, we give an abstract high-level procedural template that the previously introduced RL
boosters operate in. This is based on a variant of the Frank-Wolfe optimization technique, adapted
to non-convex and gradient dominated function classes (see Definition 1).
The Frank-Wolfe (FW) method assumes oracle access to a black-box linear optimizer, denoted O,
and utilizes it by iteratively making oracle calls with modified objectives, in order to solve the
harder task of convex optimization. Analogously, boosting algorithms often assume oracle access
to a ”weak” learner, which are utilized by iteratively making oracle calls with modified objective,
in order to obtain a ”strong” learner, with boosted performance. In the RL setting, the objective is
in fact non-convex, but exhibits gradient domination. By adapting Frank-Wolfe technique to this
setting, we will in subsequent section obtain guarantees for the algorithms given in Section 4.
Setting. Denote by O a black-box oracle to an (0, K2)-approximate linear optimizer over a con-
vex set K ⊆ Rd such that for any given v ∈ Rd , we have
v>O(v) ≥ max v>u - 0.
u∈K2
Algorithm 5 Non-convex Frank-Wolfe
1:	Input: T > 0, objective f, linear optimization oracle O
2:	Choose x0 arbitrarily.
3:	for t = 1, . . . , T do
4：	Call Zt = O(Vt-1), where Vt-1 = ▽/(xt-1).
5： Choose ηt = min{1,半} in the gradient-dominated case, else choose η so that
∖LD2ηt -V-I(Zt- Xt-I)I ≤ e.
6:	Set xt = (1 - ηt)xt-1 + ηtzt.
7:	end for
8:	return X = XT in the gradient-dominated case, else χt-1 with the smallest ηt.
Theorem 17. Let f : K → R be L-smooth in some norm ∣∣ ∙ ∣∣*, H-bounded, and the diameter of K
in k ∙ k* be D. Then,for a (e0, K2)-linear optimization oracle, the output X ofAlgorithm D satisfies
max Vf (X)T(U — X)
∕2HLD2 C
≤ γ —τ---+ 3e + e0
Furthermore, iff is (κ, τ, K1, K2)-locally gradient-dominated and X0, . . . XT ∈ K1, then it holds
max f (x*) - f (X) ≤
x* ∈K
2κ2 max{LD2, H}
T
+τ + κ0.
13
Under review as a conference paper at ICLR 2022
E Analysis for Boosting with Supervised Learning (Proof of
Theorem 11)
Theorem (Formal version of Theorem 11). Algorithm 1 samples T(MN + P) episodes of length
1-1γ log T(MN +P) with probability 1 一 δ. In the episodic model, Algorithm 1 guarantees as long as
T = 11C∞)(π), N = (16AC∞α? ) , M = m (8(C∞Y∏)Oεl，NT),μ = do, we have with probability
1 - δ	°0
V * 一 Vπ ≤ C∞(∏) E(ππ) + ε
1-γ
In the v-reset model, Algorithm 1 guarantees as long as T = @-D∞ε2, N = ( ：6AD∞⅛) , P
200lAl2D∞ log 2TN M = m ((I-γ)3αε
(1-γ)6ε2 log δ , M = m，8∣A∣D∞
,μ = V, we have with probability 1 一 δ
V* 一 Vπ
≤ D EV (Π,∏)
≤	∞(1-γ )2
+ε
If m(ε, δ) = "W1 log 1 forsome measure ofweak learning complexity |W|, the algorithm samples
O ( C∞(1-YAK^^W1) episodes in the episodic model, and O ( D∞-Y)ι8θg⅛W|) in the V-reset model.
Proof of Theorem 11. The broad scheme here is to utilize an equivalence between Algorithm 1 and
Algorithm D on the function Vπ (or Vνπ in the V -reset model), to which Theorem 17 applies.
To this end, firstly, note Vπ is ι-1γ-bounded. Define anorm ∣∣ ∙ k∞,ι : RlSl×lAl → R as ∣∣χ∣∣ι,∞ =
maxs∈s Pa∈A ∣Xs,a∣. Further, observe that for any policy π : S → ∆A, k∏k∞,ι = 1. The
following lemma specifies the smoothness of Vπ in this norm.
Lemma 18. Vπ is(]-；)3 -smooth in the ∣∣ ∙ ∣∣∞,ι norm.
To be able to interpret Algorithm 1 as an instantiation of the algorithmic template Algorithm D
presents, we advance two claims: one, the step-size choices of the two algorithms conincide; two,
∏0 (Line 3-10) serves as an approximate linear optimizers for Wπt-1. Together, these imply that
the iterates produced by the two algorithms conincide. The first of these, which provides a value of
to use in the statement of Theorem 17, is established below.
Claim 19. Upon every invocation of StepChooser, the output η1,t satisfies with probability 1 一 δ
I 2nι,t
I (1-γ )3
一(VV?T )>(∏0 - ∏t-l)
16|A|	1
≤ (1 一 Y)2√ Og δ
Next, we move onto the linear optimization equivalence. Indeed, Claim 20 demonstrates that πt0
serves a linear optimizer over gradients of the function Vπ ; the suboptimality specifies 0.
Claim 20. Let β = JON, and η2,n = min{2, 1}. Then, for any t, π0 produced by Algorithm 1
satisfies with probability 1 一 δ
maX(VVnt-1 )>(π *) ≤ (T-A2ɑ
+ εW
Finally, observe that it is by construction that πt ∈ Π. Therefore, in terms of the previous section, K
is the class of all policies, K1 = Π, K2 = ∏.
In the episodic model, we wish to invoke the second part of Theorem 17. The next lemma establishes
gradient-domination properties of Vπ to support this.
Lemma 21. Vπ is
(c∞(Π), ɪC∞(Π)E(Π,Π), Π,∏
-gradient dominated, i.e. for any π ∈ Π:
V* 一 Vπ ≤ C∞(Π) (^―E(∏, Π) + max(VVπ)>(π0 — π)
14
Under review as a conference paper at ICLR 2022
Deriving κ, τ from the above lemma along with 0 from Claim 20 and from Claim 19, as a conse-
quence of the second part of Theorem 17, we have with probability 1 - NT δ
V * - Vπ ≤ C∞(Π)E (π,π)
1-γ
+ 4C∞ (Π) + 4∣A∣C∞(Π) + 2∣A∣C∞(Π),
+ (1- Y)3T + (1- Y)2α√N + (1- γ)2α W
Similarly, in the ν-reset model, the first part of Theorem 17 provides a local-optimality guarantee for
Vνπ . Lemma 22 provides a bound on the function-value gap (on Vπ) provided such local-optimality
conditions.
Lemma 22. For any π ∈ Π, we have
V * — Vπ ≤ D∞ ( ɪ EV (∏, ∏)+max(VV∏ )>(π0 - π)
1 - Y 1 - Y	π0∈Π
Again, using the bound on max∏o∈∏(VV∏)>(∏0 — ∏) Theorem 17 provides, We have that with
probability 1 - 2NTδ
V * — Vπ ≤
D∞Eν(∏,∏)	2D∞	2∣A∣D∞ / 2	λ	48∣A∣D∞ 11
(1— Yy + (1 — γ)3√T + (1- γ)3α √N+ + εW) + (1 — γ)3√P og δ
□
F Analysis for B oosting with Online Learning (Proof of
Theorem 16)
Proof of Theorem 16. Similar to the proof of Theorem 11, we establish an equivalence between
Algorithm 1 and Algorithm D on the function Vπ (or Vνπ in the ν-reset model), to which Theorem 17
applies provided smoothness (see Lemma 18).
Indeed, Claim 23 demonstrates πt0 serves a linear optimizer over gradients of the function Vπ , and
provides a bound on 0 . Claim 19 ensures that that the step size choices (and hence iterates) of the
two algorithms coincide. As before, observe that it is by construction that πt ∈ Π.
Claim 23. Let β = JON, and η2,n = min{2, 1}. Then, for any t, π0 produced by Algorithm 4
stisfies with probability 1 — δ
maχ(Wμ∏t-
1
— πt0 ) ≤
2|A|	2 2	| RW(M) + ∕16logδ-1
(1 — Yya y √N	M VM
In the episodic model, one may combine the second part of Theorem 17, which provides a bound
on function-value gap for gradient dominated functions, which Lemma 21 guarantees, to conclude
with probability 1 — Tδ
V * —	V ∏	≤	C∞(∏)E (Π,∏) +	4C∞ (Π)	+	4∣A∣C∞(Π)	+	2∣A∣C∞(Π) RW (M)	+	8∣A∣C∞(Π)log δ-1
≤	1 — Y + (1 — γ)3T + (1 — γ)2α√N + (1 — γ)2a	M +	(1 - Y )2a√M
Similarly, in the ν-reset model, Lemma 22 provides a bound on the function-value gap provided
local-optimality conditions, which the first part of Theorem 17 provides for. Again, with probability
1 — Tδ
D∞Eν(Π,Π)	2D∞	1	|A|	2 RW(M)	4 log δ-1	24|A|	1
≤	(1-y)2 +!1-Y3 l√T + ɪ l√N +	+	r √Flogδ)
□
15
Under review as a conference paper at ICLR 2022
G Proofs of Supporting Claims
G.1 Non-convex Frank-Wolfe method (Theorem 17)
Proof of Theorem 17. Non-convex case. Note that for any timestep t, it holds due to smoothness
that
f(xt) = f (xt-1 + ηt(zt - xt-1))
≥ f(Xt-I) + nN-ι(zt - Xt-I) - η2^2D2
=f (xt-ι) - 2L1D2 (LW- Vt-ι(zt - Xt-1))2 + (Vt-I(ZLDxtT))2
2LD	2LD
Using the step-size definition to bound on the middle term, and telescoping this inequality over
function-value differences across successive iterates, we have
min(V3(zt-xt-i))2 ≤ T XX(V>-i(zt - xt-i))2 ≤ 2LDH + e2
tT	T
t=1
Let t0 = arg mint ηt and t* = argmi□t (V>-ι(zt — xt-ι))2. Then
V>-ι(zt0 — xto-ι) ≤ LD2ηt0 + C ≤ LD2ηt* + C
< V>-ι(zt* — Xt*-1) + 2c ≤
∕2LD2H~~2	n
T^-+c2+2c
To conclude the claim for the non-convex part, observe √a + b ≤ √α + ʌ/b for a, b > 0, and that
since zt0 = O(Vt0-1), it follows by oracle definition that
max Vt>0-1u ≤ Vt>0-1zt0 + C0.
u∈K2	-	-
Gradient-Dominated Case. Define x* = argmaXχ∈κ f (x) and ht = f (x*) 一 f (xt).
ht ≤ ht-ι -小寸―、(Zt - xt-ι) + ηt 2D2
≤ ht-ι - ηt maxntVt-i(y - χt-ι) + η2ττD2 + ηtco
y∈K2	2
≤ ht-ι - —(f (χ*) - f (Xt-I)) + η2-D2 + ηt (CO +—)
κ	2κ
=(1 - ηt) ht-ι + η2LD2 + ηt (CO + T)
κ2	κ
The theorem now follows from the following claim.
smoothness
oracle
gradient domination
Claim 24. Let C ≥ 1. Let gt be aH-bounded positive sequence such that
gt ≤ (1 一 σt) gt-1 + σ2D + σtE.
Then choosing σt = min{1, 2C} implies gt ≤ 2C mat{2D,H} + CE.
□
G.2 Smoothness of value function (Lemma 18)
Proof of Lemma 18. Consider any two policies π, π0. Using the Performance Difference Lemma
(Lemma 3.2 in (Agarwal et al., 2019), e.g.) and Equation 2, we have
|Vπ0 - Vπ -VVπ(π0 - π)∣
=占 IEs〜d∏0 [Qπ(∙∣s)τ(∏0(∙∣s) - ∏(∙∣s)] - Es~d∏ [Qπ(∙∣s)τ(∏0(∙∣s) - ∏(∙∣s)]∣
≤ (ɪɪpkdπ0- dπkιk∏0-∏k∞,ι
16
Under review as a conference paper at ICLR 2022
The last inequality uses the fact that maxs,a Qn (s,a) ≤ ι--γ. It suffices to show ∣∣dπ0 -
dπkι ≤ ι-γk∏0 - ∏∣∞,ι. To establish this, consider the Markov operator Pπ(s0∣s) =
Pa∈A P(s0∣s, a)π(a∣s) induced by a policy π on MDP M. For any distribution d supported on
S, we have
k(PN- Pπ)dkι = E EP(S0|s, a)d(s)(∏0(a∣s) - ∏(a∣s)
s0 s,a
≤ X P (S0|S, a)∣d∣1∣π0 -π∣∞,1 ≤ ∣π0 -π∣∞,1
s0
Using sub-additivity of the l1 norm and applying the above observation t times, we have for any t
∣((Pπ0)t - (Pπ)t)d∣1 ≤ t∣π0 - π∣∞,1.
Finally, observe that
∣dπ0 - dπ∣1 ≤ (1-γ)X∞ γt∣((Pπ0)t - (Pπ)t)d0∣1
t=0
∞
≤ ∣π0 - π∣∞,1(1 -γ)Xtγt
t=0
亡 kπ0-πk∞J
□
G.3 Step-size guarantee (Claim 19)
Proof of Claim 19. Let D be the distribution induced by Algorithm 2 upon being given πt-1. Due
to Lemma 14, it suffices to demonstrate that for any π ∈ {πt0 , πt-1} the following claim holds with
δ
probability 1 - 2. The claim in turn follows from HOeffding S inequality, while noting Qπt-1 (s, ∙)
is 1Aγ -bounded in the l∞ norm.
IGπ -E(s,Qπ^(s,”[∖(s，∙)>π(∙ls)il ≤ (T8γ√Plog21δ
□
G.4 Gradient domination (Lemma 21 and Lemma 22)
ProofofLemma 21. Invoking Lemma 4.1 from (Agarwal et al., 2019) with μ = do, We have
d∏*
V * - Vπ ≤ —	max(Wπ )>(∏o - π)
dπ ∞ π0
≤ C∞(Π)(max(Wπ)>π0 — max(Wπ)>π0 + max(Wπ)>(π0 — π))
π0	π0∈Π	π0∈Π
Finally, with the aid of Equation 2, observe that
max(Wπ )>∏o — max(Wπ )>π0
π0	π0∈Π
min ɪ Es”
π0∈Π 1 -γ
[max Qn(s, a) — Qn(∙∣s)>π]
≤ 占E（n，n）
ProofofLemma 22. Invoking Lemma 4.1 from (Agarwal et al., 2019) with μ = ν, we have
V* - Vπ ≤
1 I
1 - Y
ɪ D
1-γ
*
dπ*
∞(max(WVr )>∏0 — max(WVπ )>∏0 + max(WVr )>(∏0 — ∏))
≤
ν
□
17
Under review as a conference paper at ICLR 2022
Again, with the aid of Equation 2, observe that
max(WV∏ )>∏o — max(WV∏ )>∏0
π0	ν	π0∈Π	ν
1
mm------
π0∈Π 1 — γ
Es~d∏ [maxQn(s,a) — Qn(∙∣s)>π]
≤	EV (∏,∏)
1 - Y
□
G.5 Supervised linear optimization guarantees (Claim 20)
Proof of Claim 20. The subroutine presented in lines 3-10 (which culminate in πt0) is an instantiation
of Algorithm 3 from (Hazan & Singh, 2021), specializing the decision set to be ∆A. To note the
equivalence, note that in (Hazan & Singh, 2021) the algorithm is stated assuming that the center-of-
mass of the decision set is at the origin (after a coordinate transform); correspondingly, the update
rule in Algorithm 1 can be written as
(Pt,n — πr ) = (I — η2,n)(Pt,n-1 — πr) +	(At,n — πr)
For any state s, ∏(∙∣s)= } 1∣a∣ corresponds to the center-of-masss of Δa. Finally, note that
maximizing f>x over x ∈ K is equivalent to minimizing (—f)>x over the same domain. Therefore,
we can borrow the following result on boosting for statistical learning from (Hazan & Singh, 2021)
(Theorem 13). Note that Qn(s, ∙) produced by Algorithm 2 satisfies ∣∣QCπ(s, ∙)k = --γ. Let Dt be
the distribution induced by the trajectory sampler in round t.
Theorem 25	((Hazan & Singh, 2021)). Let β = JON, and η2,n = min{ 2, 1}. Then, for any t, π0
produced by Algorithm 1 satisfies with probability 1 — δ that
max E(s,Q)~Dt [Q>π(s)] — E(s,Q)~Dt [Q>π0(s)]
2|A|
(I ― γ)«
≤
Lemma 14 allows us to restate the guarantees in the previous subsection in terms of linear optimiza-
tion over functional gradients. The conclusion thus follows immediately by combining Lemma 14
and Theorem 25.	口
G.6 Online linear optimization guarantees (Claim 23)
Proof of Claim 23. In a similar vein to the proof of Claim 20, here we state the a result on boosting
for online convex optimization (OCO) from (Hazan & Singh, 2021) (Theorem 6), the counterpart of
Theorem 13 for the online weak learning case.
Theorem 26	((Hazan & Singh, 2021)). Let β = Jα1N, and η2,n = min{ 2, 1}. Then, for any t,
Γ[ρt,m,N] produced by Algorithm 4 satisfies
MM
mπ∈aΠx X Qt,mπ(st,m) — X Qt,mΓ[ρt,m,N](st,m)
m=-	m=-
≤	2∣A∣
一(1 ― Y)«
+ RW
Next We invoke online-to-batch conversions. Note that in Algorithm 4, (St,m, Qt,m) for any fixed
t is sampled i.i.d. from the same distribution. Therefore, we can apply online-to-batch results, i.e.
Theorem 9.5 in (Hazan, 2019), on Theorem 26 to get
maxE(s，Q)~Dt [Q>n(s)]—E(s，Q)~Dt [Q>π0(s)] ≤ (1-[。(√2N+RM) + r161Mδ
We finally invoke Lemma 14.	口
18
Under review as a conference paper at ICLR 2022
G.7 Remaining proofs (Claim 24)
Proof of Claim 24. Let T* = arg maxt{t : t ≤ 2C}. For any t ≤ T*, we have σt = 1 and
gt ≤ H ≤ 2CtH .Fort ≥ T *, weproceedby induction. The base case (t = T *) is true by the
previous display. Now, assume gt-1 ≤
2C2 max{2D,H}
t-1
+ CE for some t >T*.
gt ≤
2C2 max{2D, H}
t-1
+ CE +
4C2D
≤ CE + 2C2 max{2D, H} ( -ɪ
t-1
t2
1
+再
2CE
+ ~Γ
CE + 2C2 max{2D, H}
t2 - 2t + t - 1
≤ CE + 2C2 max{2D, H}
t2(t - 1)
t( - 1)
tψ-1)
□
19