Under review as a conference paper at ICLR 2022
FedDiscrete: A Secure Federated Learning
Algorithm Against Weight Poisoning
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) is a privacy-aware collaborative learning paradigm that
allows multiple parties to jointly train a machine learning model without shar-
ing their private data. However, recent studies have shown that FL is vulnerable
to availability poisoning attacks, integrity backdoor attacks and inference attacks
via weight poisoning and inference. In this paper, we propose a probabilistic
discretization mechanism on the client side, which transforms the client’s model
weight into a vector that can only have two different values but still guarantees
that the server obtains an unbiased estimation of the client’s model weight. We
theoretically analyze the utility, robustness, and convergence of our proposed dis-
cretization mechanism and empirically verify its superior robustness against vari-
ous weight-based attacks under the cross-device FL setting.
1	Introduction
Federated learning (FL) is an emerging privacy-aware framework that trains a machine learning
model across multiple parties without accessing their local private data (Konecny et al., 2016;
McMahan et al., 2017). In FL, each client first trains the local model using its private data and
then sends the model gradients to an honest central server. The central server aggregates all local
model gradients to form a global model, which is sent back to the clients for the next round of train-
ing. However, sharing model gradients might still leads to security concerns and privacy leakage.
Recent studies have shown that FL is vulnerable to various model weight poisoning attacks. The ad-
versarial client(s) can stealthily manipulate the global model via modifying the local model updates
to achieve attack goals like, preventing model convergence, implanting backdoors into the global
model, and inferring the privacy information of clients’ private data (Gu et al., 2017; Blanchard
et al., 2017; Pyrgelis et al., 2017; Xie et al., 2019; Bagdasaryan et al., 2020; Wang et al., 2020; Tang
et al., 2020). To address such an issue, recent works start to explore different defense techniques
to robustify FL against model weight poisoning attacks (Blanchard et al., 2017; Bagdasaryan et al.,
2020; Shen et al., 2016; Geyer et al., 2017; Fung et al., 2018). However, most existing works apply
the defense or robust techniques at the server side, which changes the role of the central server from
being honest to trusted. And prior approaches mostly only focus on one type of attacks.
Our contributions. In this paper, we propose the FEDDISCRETE, a flexible FL framework that can
combine any popular FL algorithms, for example, FedAvg(McMahan et al., 2017) and FedProx(Li
et al., 2020), with our probabilistic discretization mechanism. Theoretical analyses on the util-
ity, robustness, and convergence are performed to show that FedDiscrete is inherently robust to
availability poisoning attacks (Kurita et al., 2020), integrity backdoor attacks (Gu et al., 2017) and
server inference attacks (Shokri et al., 2017). FedDiscrete is evaluated on four popular image
classification datasets under both data are i.i.d and non-i.i.d settings. The numerical results indicate
that FedDiscrete is robust to various weight-based attacks.
2	Backgrounds and Related Works
In this section, we provide necessary background information on concepts thorough the paper and
formalize the problem to be solved.
1
Under review as a conference paper at ICLR 2022
Federated learning (FL). FL is a privacy-aware collaborative learning, where N clients and one
trusted server work together to learn a global model (McMahan et al., 2017). Depending on the
application scenarios, the number of clients n can range from as small as two to several hundreds in
the cross-silo setting or can easily go beyond millions in the cross-device setting. A classical way of
formulating FL into an optimization problem is (Wang et al., 2021a)
N	1N
min F(W)：=工PiFi(w), where Fi(W) = |^ £ fi(w; ξ) and 工Pi
1.	(1)
In Eq. 1, w is the global model weight; Fi is the i-client local objective function; the local loss
functions fi (w; ξ) are often assumed to be the same across all clients; the local data Di can have
different distribution. Following the seminal work of (McMahan et al., 2017), extensive researches
have been conducted to address various challenges in FL. For instance, (Li et al., 2020; Reddi et al.,
2020) aim to design efficient optimization algorithms; (Konecny et al., 2016; Luping et al., 2019)
try to improve the communication efficiency; (Bagdasaryan et al., 2020; Xie et al., 2021) study the
security issues on both attacks and defenses.
Adversaries. For any FL attack, the server is assumed to be honest, which faithfully follow the
training protocols and any client cannot directly get the model weight of other clients. Under these
assumptions, there are two adversaries: 1) the malicious clients can manipulate the weights for
various attack purposes, such as interfering the global model training or implanting backdoors into
the global model; 2) the curious server can explore the privacy information from local clients via
inference attacks.
Availability poisoning attacks. The goal of availability poisoning attacks (APAs) is for malicious
local client(s) to destroy or reduce the global model’s utility (Shen et al., 2016; Sun et al., 2018). In
APAs, the attacker can either control the global model’s utility on target tasks or decrease the most
of the model’s utility as the optimal attack strategy. In practice, adding a large random noise can
successfully worsen the global model’s utility but can be easily discovered by anomaly detection
techniques. In this case, any advanced attack needs to know and tries to bypass the defense tech-
niques used in training. In this work, we assume the attacker has full knowledge of training process
and all possible defense techniques.
Integrity backdoor attacks. The goals of integrity backdoor attack (IBAs) are two-fold: 1) the
attacker aims to implant backdoors into the global model, through which the attacker can control the
prediction results by injecting the trigger to any clean examples; 2) at the same time, the attacker
wants the global model can still perform as good as the non-attacked model on all clean inputs
(Bagdasaryan et al., 2020). Note that, IBAs are also conducted by the malicious clients in FL.
Inference attacks. Many studies show that the attacker could explore the private information from
the model weights, such as membership and attribute information (Pyrgelis et al., 2017; Shokri
et al., 2017; Ganju et al., 2018; Melis et al., 2019) or even recover the training samples used by
clients Zhu et al. (2019). When the honest-but-curious server becomes malicious, it can explore
privacy information of each client’s local dataset through inference attacks (IAs) since the server
has full access to clients’ model weight.
Both APAs and IBAs can be achieved via weights poisoning that modifies the local model weights.
For example, as the global model converges, the deviations of local models start to cancel out,
i.e., Pin=t-11(wit - wt-1) ≈ 0 given as in Eq. 2 of Bagdasaryan et al. (2020), where t and i
the communication round and client index respectively, wt and wit are the global and local model
respectively, and nt is the number of participating clients. If the adversary aims to replace global
model wt with a target model X, then it could propose to upload a local model weight wAt =
ntX - (nt - 1)wt-1 - Pin=t-1 1(wit - wt-1) ≈ nt(X - wt-1) + wt-1 as given in Eq. 3 in
Bagdasaryan et al. (2020). Prior works (Bagdasaryan et al., 2020; Wang et al., 2020; Xie et al.,
2019) demonstrate the effectiveness of both APAs and IBAs when the training algorithm is FedAvg.
IAs can be easily achieved by many existing advanced attack methods once they have the full access
to the client model Shokri et al. (2017); Hu et al. (2021).
Related works. To address the various attacks, many prior defense works have been proposed and
achieved good performance against the attacks. However, prior defense works only address one
aspects of aforementioned three attacks. For availability poisoning attacks, Steinhardt et al. (2017)
proposed a defending framework, which can be applied to defenders that remove outliers and then
2
Under review as a conference paper at ICLR 2022
Figure 1: (a): An illustration on how adversarial clients can do APAs and IPAs via weight modifica-
tion. If there is no adversaries, then FL training algorithm will give a clean model. When there are
attackers, the clean model becomes poisoned. (b): An illustration on how discretization mechanism
helps to defend the APAs and IPAs, which brings the poisoned model to the robust area. The robust
area is a parameter space where the model would have the same performance as the clean model.
(c) An illustration on how discretization mechanism helps to defend the IAs as the discrete model
updates provide limited information compared to the continuous counterparts.
minimize a margin-based loss on the remaining data. Under assumptions, this framework provides
the approximate upper bounds on the efficacy of any data poisoning attack. However, this framework
does not fit in the FL setting, as it needs the access to all private data. Blanchard et al. (2017) studied
the presences of Byzantine adversaries and proposed Krum aggregation rule to defend under the
assumption that the participants’ training data are i.i.d, which is not necessarily true in the FL setting.
And indeed as argued in (Bagdasaryan et al., 2020), Krum can be used by the adversaries to make the
attack more effective. FoolsGold (Fung et al., 2018) can mitigate sybils data poisoning attacks under
the assumption that the honest clients can be separated from sybils by the diversity of their gradient
updates. As discussed by the authors, FoolsGold is not successful at mitigating attacks initiated by a
single adversary. For integrity backdoor attacks, Xie et al. (2021) provided a general framework that
is certifiably robust to the backdoor attacks under the FL setting via the model weight smoothing.
However, the convexity assumption is imposed on the loss function, which limits certified robustness
to more challenging and widely used deep neural network. Ozdayi et al. (2021) proposed a defense
approach based on adjusting server’s learning rate with the guidance of sign information of agents’
updates. For inference attacks, differential privacy is widely adopted approach to defend, where are
judiciously random noise added on either the clients’ model update or the global model, e.g., Geyer
et al. (2017), but it suffers from the trade-off between privacy and accuracy. Compared to this work,
neither of these works can address three types of attacks simultaneously.
3	Method
We introduce FedDiscrete, a federated learning framework with a simply yet effective discrete
mechanism and the flexibility to accommodate various FL optimization algorithms. A complete
description is given in Algorithm 1. FedDiscretedifferences FedDiscrete consists of three
stages, namely, local training, discretization and aggregation, which are discussed in details.
Local Training. Since we mainly focus on the cross-device setting, in each communication round
t, the server first selects a subset of |St | = K clients to participate the current round of training and
broadcasts the global model wt to the selected clients. The clients who receives the global model
then performs local training using any appropriate algorithm to generate a new local model, i.e.
wit+1 for all i ∈ St. The ith client can 1) use the local stochastic gradient descent(SGD) method as
is considered in FedAVG to perform a fixed number of SGD steps to improve communication effi-
ciency; 2) use adaptive method (Wang et al., 2021b) to improve the convergence; 3) approximately
minimize Fi(w) + μ ∣∣w - wt k2 for some μ > 0 as proposed in FedProX to accommodate the system
and data heterogeneity (Li et al., 2020).
Discretization. For all i ∈ St , the ith local client computes the maXimum and minimum values of
wit and sends noise-perturbed maXimum value uit and minimal value lit back to the server, where the
3
Under review as a conference paper at ICLR 2022
noise is added to protect the privacy of the local training data and the noise is sampled and added
in a way that to guarantee uit and lit are the valid upper bound and lower bound of wit+1 element-
wise. Once the server received the client-wise upper-bounds and lower-bounds, it computes the
minimum of the lower bounds lmt in and the maximum of the upper bounds utmax to prepare the
inputs for the discretization mechanism (see Definition 3.1), which outputs an unbiased estimator of
the input (see Lemma 4.1). For all i ∈ St, the ith client discretizes its continuous model wit into
M(wit; lmt in, utmax) and uploads M(wit; lmt in, utmax) to the server for aggregation.
Aggregation. Rather than simply aggregating {M(wit; ltmin, utmax)}i∈St , the sever first inspects
whether each coordinate of wit is either lmt in or utmax to exclude the any potential malicious adver-
saries that attempt to bypass the discretization process. And the server only aggregates the local
model weights that pass the sanity check. Also when the server computes lmt in and utmax , it can be
more conservative by discarding the extreme values in {lit}i∈St and {uti }i∈St by setting thresholds
on the lower quantile of {lit}i∈St and upper quantile of {uit}i∈St to prevent adversaries proposing
extremely large upper bounds and/or small lower bounds. For the ease of presentation, we do not
include this choice in the description of Algorithm 1.
Definition 3.1 (Discretization Mechanism). For any (w, l, u) ∈ R × R × R with l ≤ w ≤ u, define
the discretization mechanism as
{u,	w.p. W~τ
l,	w.p. U-W
(2)
where w.p. is the shorthand notation for “with probability”.
Algorithm 1 FEDDISCRETE
1:	Input: Initial model weight w0 ∈ Rd, total training rounds T , the participants size K, and the
learning rate {ηt}tT=-01 and a positive sequence {σi}iN=1.
2:	for t = 0, 1, 2, . . . , T - 1 do
3:	The sever randomly selects an index set St with |St | = K and broadcasts wt to the client i
if i ∈ St .
4:	For the ith client, where i ∈ St , it performs the local training to obtain wit+1 and samples
two random variables ξit and ζit from the truncated Gaussian TN(0, σi ; 0, 1). Then it com-
putes and uploads lit = minj∈[d] {[wit]j - wt} - ξit and uit = maxj∈[d] {[wit]j - wt} + ζit
to the server.
5:	The sever computes lt = mini∈St {lit} and ut = maxi∈St {uit} then broadcasts lt, ut to
clients in St .
6:	For the ith client, where i ∈ St, it applies the discretization mechanism M is an element-
wise fashion and uploads the discrete model weight M(wit+1 - wt ; lt , ut) to the server.
7:	The server conducts the sanity check for {M(wit+1 - wt; lt, ut)}i∈St, i.e., validates
whether all elements of wit+1 are either lt or ut . Form the set St0 ⊆ St to collect all the
clients’ index passing the sanity check and compute wt+1 = wt+ ηt 击 Pi∈S0 M(wt+1 -
wt; lt, ut).
8:	end for
We close this section by making following comments.
•	Discretization is a widely applied technique, for example, in influence maximization
Kempe et al. (2003), the algorithm needs to simulate the propagation from a probabilis-
tic graph that is consists of a set of edges with activation probabilities in [0, 1]. It is also can
be regarded as a special form of quantization that is widely used in the distributed optimiza-
tion (Alistarh et al., 2017; Reisizadeh et al., 2020) to improve communication efficiency.
Compared with prior discretization works, this is the first work to adopt discretization to
protect FL from weight poisoning attack.
•	Compared with the FedAvg, although an additional round of communication is required in
FedDiscrete to perform the discretization mechanism. We argue that FedDiscrete is
more communication-efficient than FedAvg. Assume each scalar takes 64bits, then for the
4
Under review as a conference paper at ICLR 2022
t-th round, the total bits communicated are 128dK for FedAvg, while for FEDDISCRETE
are 64dK + 128K + dK. For large d, FEDDISCRETE communicates less bits. 1
•	Although the discretization mechanism outputs an unbiased estimation of wit+1, the vari-
ance of the estimator M(wit+1) can be large. To mitigate this issue, one can compute more
finely-grained upper bounds and lower bounds. For example, assume each client wants to
train a neural network. Then, instead of computing the maximum and minimum values
over the entire w, the client could compute layer-wise maximum and minimum values.
And the discretization mechanism could be applied layer-wise with tighter upper bounds
and lower bounds. This leads to smaller variance in the estimator at the cost of increasing
the communicated bits.
•	Intuitively, the discretization mechanism is effective in defending weight poisoning attacks
as the judiciously crafted adversary model is discretized. For example, consider the model
replacement attack in Eq. 3 of Bagdasaryan et al. (2020), once the malicious model wAt is
discretized into M(wAt ), the model replacement becomes ineffective.
4	Theoretical Analysis
In this section, we analyze the utility of the discretization mechanism and the convergence property
of the FedDiscrete. Due to limited space, complete proofs are deferred to the appendix.
We first introduce the notations are used throughout the paper. [w]j denotes the jth coordinate of
the vector w. Unless specified otherwise, k∙k and ∣∣∙k∞ are the Euclidean norm and infinity norm
respectively. To avoid the cluttered notation, We use M(∙) instead of M(∙; ∙, ∙) when there is no
confusion. And if M is applied over a vector, we mean to apply it in a element-wise fashion. Here,
[d] represents {1, ∙∙∙ , d}.
4.1	Utility Analysis
The first lemma shows that the discretization mechanism produces an unbiased estimator with
bounded variance.
Lemma 4.1. For any w ∈ Rd and (l, u) ∈ R ×R such that l ≤ minj∈[d] [w]j < maxj∈[d] [w]j ≤ u,
then
•	E[M(w; l, u)] = w;
•	E[∣M(w; l, U) - w∣2] ≤ d(U-I) and E[∣M(w; l, U) - w∣2] ≤ kwk-.
A natural question to ask is that how far is the averaged local model weights Wt := Pii∈s^ 1 w；-1
from its discretized counterpart WM := Pii∈s^ 1 M(w；-1). The next theorem characterizes the
distance between Wt and WM.
Theorem 4.2.	For any communication round t ≤ T and any β ∈ (0, 1), there exists t =
O ((UtTt√√qlo 2d ! such that
P (Mt - WML ≤ et) ≥ 1-β
4.2 Robustness Analysis
Next, we show the discrete mechanism is robust to random weight poisoning and can recover the
original weight from local models against a limited number of independent targeted weight poison-
ings. To maximize the attack impact, the attacker choose to inverse the discrete local updates to
1Each model weight takes 64d bits. For FedAvg, server broadcasts the global model weight and clients
upload the local model weights, so the total bits are 128dK. For FEDDISCRETE, 64dK is the cost for the
server broadcasting the global model to K server. 128K is the cost for uploading and broadcasting the upper
bounds and lower bounds. Since the discretized model weight M(wit+1) can only taking two values, the clients
can indeed just upload d bits string and the server can recover the M(wit+1).
5
Under review as a conference paper at ICLR 2022
attack the global model. If the intend update of the jth weight [w]j is u, the attacker uploads l to the
server, and vice versa.
Theorem 4.3.	Suppose there are F attackers out oftotal N clients and denote WM = N PN=I Wi,
then for any j ∈ [d],
1 N -F	N	F
[E[WM]]j = N E [Wi]j- E [Wi]j +(h + l)N.	(3)
i=1	i=N -F+1
More importantly, FedDiscrete has an asymptotically non-bias estimation when the number of
clients approaches to +∞ with a fixed number of attackers.
Corollary 4.3.1. Let N be +∞, for some fixed F attackers, the expectation of the averaged discrete
weights is E[Wm] = w.
Discussion. The above theorem and corollary have shown the robustness against the weights poi-
soning attacks, such as IBAs and APAs. Besides, another important reason is the discretization
mechanism limits the modification ability of the adversaries as shown in Figure 1. For example, if
the clean aggregated model’s weight is [0.1, -2.3, 3.8], the IBA or APA could work while modify-
ing the weight from [0.1, -2.3, 3.8] to [1, 0.2, 0.4], where the weight difference between clean and
poisoned model is [0.9, -2.5, -3.4]. However, due to the discretization mechanism, while we only
have 1 attacker among 100 clients, its modification could only be 0 or ±0.01 with the upper bound
u = 1 and the lower bound l = 0. In this case, the adversary can not achieve its goal in any manner.
4.3 Convergence Analysis
In this section, we prove the convergence of Algorithm 1 by instantiating the optimization algorithm
for generating local model weights as the FedProx(Li et al., 2020). Specifically, at the tth round,
in line 4 of Algorithm 1, the ith selected client produce the local model weight Wit+1 ≈ Fi(W) +
μ kw - wt k2 in the sense that ∣∣ VFi(wt+1) + μ(wtt+1 - wt)∣∣ ≤ Y ∣∣VFi(wt)k for some properly
chosen μ > 0 and γ > 0. The Assumption 4.1 is made throughout the whole section.
Assumption 4.1.
1. (Smoothness) For all i ∈ [N], Fi(w) is L-smooth, i.e., kVFi(w) - VFi(w0)k ≤
L kw - w0k for some L > 0 and all (w, w0) ∈ Rd × Rd.
2. (Lower-bounded eigenvalue) The minimal eigenvalue of the Hessian of the client loss func-
tion V2Fi(w) is uniformly bounded below by a constant λmin ∈ R. 3 *
3. (Bounded dissimilarity) For all i ∈ [N] and any w ∈ Rd, Ei[kVFi(w)k2] ≤
B2 kVf(w)k2 , where the expectation is taken with respect to the client index i.
4.
(Algorithmic choices) In Algorithm 1 , in line 3, the ith client is picked with the probability
pi; line 7, the step size ηt = 1.
Theorem 4.4. Consider the Algorithm 1 instantiated with FedProx.
μ > λmin, and K, μ, γ is chosen properly such that κ
If μ is chosen to satisfy
LB(1+γ)	L(1+γ)2B2
--
μμ	2μ2
1-γB
μ
LBκμ+Y)2 (2√K+I) - Bμ√κ)- LNBpm+γ22 > 0, then after T rounds，
min
t∈[T -1]
E[∣Vf(wt)∣] ≤
f(w0)- f (w*)
KT
Remark
• The rate of convergence of FedDiscrete is the same as that of FedProx but with worse
constant due to the discretization mechanism.
• One can also prove the convergence of the FedDiscrete when the local clients perform
the fixed number of stochastic gradient descent steps as FedAVG. Then the convergence
result is a special case of the FedPAQ (Reisizadeh et al., 2020) when q is set to 1/4.
6
Under review as a conference paper at ICLR 2022
Con Continous ■ Discrete
9 8 8 7 7 6
) Aɔe,lnɔɔ4
con Continous ■ Discrete
QOQQOO
8 7 6 5 4 3
) Aɔe,lnɔɔ4
(d) CIFAR-10
(a) MNIST	(b) FMNIST	(C) SVHN
050505050
099887766
(<⅛) Aɔe,lnɔɔ=
?) Aɔe,lnɔɔ4
Figure 2: Effect of number of clients N with i.i.d setting.
?) Aɔe,lnɔɔ4
DoOooooo
87654321
(<⅛) Aɔe,lnɔɔ4
50	100 150 200 2 50 300 3 50
Number Of clients
50
Number of clients
350
3∞
6 4 2 0 8 6
9 9 9 9 8 8
(<⅛) Aoe-Inoo4
(a) MNIST	(b) FMNIST	(c) SVHN	(d) CIFAR-10
Figure 3:	Effect of number of clients N with non-i.i.d setting.
5	Experimental Evaluation
In this section, We examine the effect of FEDDISCRETE on the four image benchmark datasets:
MNIST (LeCun et al.,1998), Fashion-MNIST (FMNIST) (Xiao et al., 2017), SVHN (Netzer et al.,
2011), and CIFAR-10 (Krizhevsky et al., 2009). For MNIST and FMNIST, we adopt a two-layer
CNN for image classification; for SVHN and CIFAR-10, the small network from the Pytorch li-
brary2 3 only achieves around 50% accuracy, so that we re-design a small VGG (Simonyan & Zis-
serman, 2014) for them. The training data and the testing data are fed into the network directly
in each client, and for each client, the size of the training data is the total number of the training
samples divided by the number of the clients for i.i.d setting. For non-i.i.d setting, we sort the data
by digit labels first, then divide the datasets into 2N shards of size 2D|, and assign each of N clients
2 shards, which is the same setting as (McMahan et al., 2017). We use the local SGD as the opti-
mization algorithm for each client, where the local learning rate is set as 0.03 for MNIST/FMNIST
and 0.015 for SVHN/CIFAR-10. And the global learning rate ηt is set to 1 for all t. Considering
the randomness from the discrete mechanism, we run the test experiments five times independently
to obtain an averaged value. To evaluate the performance of different approaches, we use different
metrics including accuracy/utility for model performance, attack success rate (ASR)3 for attack per-
formance and the number of communication rounds (CRs), for communication cost. Any approach
with a high accuracy and a low ASR indicates a good and practical defense solution. The proposed
models are implemented using Pytorch, and all experiments are done with a single GPU NVIDIA
Tesla V100 on the local server. Experiments on MNIST and FMNIST can be finished within an hour
with 10 CRs, and experiments on SVHN and CIFAR-10 need about 2 hours with 15 CRs.
Evaluation on Discrete Mechanism. Here, we first test the effectiveness of the discrete mecha-
nism in FL. In Figure 2 and 3, the results demonstrate three observations: 1) for the discrete model,
in i.i.d setting, it performs closer to continuous model aggregation than that in the non-i.i.d setting;
2) the more complex and difficult tasks, i.e., CIFAR-10 > SVHN > FMNIST > MNIST, are more
sensitive to the discrete mechanism; 3) when increasing the number of total clients, the model utility
loss between continuous and discrete models are diminishing. For example, while having 350 clients
in FL, the difference between continuous and discrete models is less than 1% and 2% for all tasks
in the i.i.d and non-i.i.d settings, respectively. These results verify the previous analysis in Theorem
2https://pytorch.org/tutorials/beginner/blitz/Cifar10_tutorial.html. Note that, we use the default setting pro-
vided from Pytorch for MNIST and Fashion-MNIST. For SVHN and CIFAR-10, we use VGG9 that is modified
from VGG11 for balancing the memory usage and the model utility.
3The precise definition of ASR varies under different attacks. We will give the precise definition in the
subsequent sections.
7
Under review as a conference paper at ICLR 2022
123456789 101112131415
Number of attackers
(a)	MNIST
123456789 101112131415
Number of attackers
(b)	FMNIST
123456789 10
Number of attackers
(d)	CIFAR-10
123456789 10
Number of attackers
(c)	SVHN
Figure 4:	Effect of number of attackers F with 100 clients in i.i.d and non-i.i.d settings.
4.3.	In summary, the discrete mechanism is more practical for customer-related applications, e.g.,
smartphones or IoT devices, since it usually involves millions of clients during training.
Evaluation on Availability Poisoning Attacks. The integrity poisoning attacks are evaluated
from multi-angles. In Figure 4, we fix the number of clients as 100, and evaluate the increasing
number of attackers in both i.i.d and non-i.i.d settings. The attack success rate (ASR) in APAs is
the utility loss between before and after attacks. The results show that one or a few attackers, i.e., a
small fraction of attackers of all clients, cannot successfully poison the model. For example, when
there are more than 100 clients, any single adversarial client only gets the 0% ASR for all tasks in
both i.i.d and non-i.i.d settings. Figure 5 and 6 show the results of model utility with a fixed number
of attackers in both i.i.d and non-i.i.d settings. The results consistently show that for a fixed number
of attackers, increasing the number of the total clients will improve the global model’s utility.
Compared with prior defense against IBAs (Blan-
chard et al., 2017; Bagdasaryan et al., 2020; Shen
et al., 2016; Fung et al., 2018), based on our cur-
rent knowledge, not many existing works have well
studied the availability poisoning attacks in FL.
Due to the adversary have more flexible attack op-
tions of the training process in APAs, most APAs
could easily break the defense solutions for IBAs.
Evaluation on Integrity Backdoor Attacks.
Next, we evaluate the FedDiscrete against
the state-of-the-art backdoor attacks, such as
constrain-and-scale (CAS) (Bagdasaryan et al.,
2020) and DBA (Xie et al., 2019) in Table 1, where
utility loss (UL) is the difference of model utility
between clean and poisoned models, and the at-
tack success rate (ASR) in IBAs is the percentage
of poisoned samples that are classified as the target
Table 1: FedDiscrete against IBAs
	No Defense		FEDDISCRETE	
Attack	ASR	UL	ASR	UL
CAS	81.90%	2.40%	0.00%	0.00%
DBA	81.30%	4.70%	0.00%	0.00%
Table 2: Compare with other defense baselines
Defenses	ASR	UL
No defense	81.90%	2.40%
Krum	100%	35.50%
FoolsGold	100%	39.90%
Auror	100%	66.10%
FedDP	-0%-	13.30%
FedDiscrete	0%	0%
class by the backdoored model. Due to the variants of studied tasks in prior works, we evaluate the
most common task, i.e., CIFAR-10, for all backdoor attacks in the i.i.d setting. We use the default
attack settings same as the original paper: one attacker for CAS and four attackers for DBA. Here,
ASR is the backdoor attack success rate, and UL is the utility loss compared with none-attack FL.
The results show that FedDiscrete can easily defend the backdoor attacks against CAS and DBA.
By mitigating the impacts of the attackers, in addition to the successful IBAs defense, FedDis-
crete can even improve the model utility.
We also compare FedDiscrete with other backdoor defense systems against CAS in FL: Krum
(Blanchard et al., 2017), FoolsGold (Fung et al., 2018), Auror (Shen et al., 2016), and FedDP (Geyer
et al., 2017) in Table 2. The results show that Krum, FoolsGold, and Auror can not well defend the
backdoor attacks in FL. DP can achieve a good backdoor defense, but scarifies the model utility.
FedDiscrete can achieve the best ASR and UL in all defense methods.
Discussion on Client Inference Attacks. Prior inference attacks (Blanchard et al., 2017; Pyrgelis
et al., 2017; Ganju et al., 2018) require to get the original or similar local client model, i.e., a contin-
uous local model. Then, the malicious server could explore the privacy of the training data from its
8
Under review as a conference paper at ICLR 2022
(<⅛) Aɔe,lnɔɔ=
10 20 30 40 50 60 70 80 90 100
Number Gf clients
(a) MNIST
(<⅛) Aɔe,lnɔɔ=
10 20 30 40 50 60 70 80 90 100
Number Gf clients
(b) FMNIST
-=-F=1
-S- F=5
→- F = 10
10 20 30 40 50 60 70 80 90 100
Number Gf clients
(c) SVHN
10 20 30 40 50 60 70 80 90 100
Number of clients
(d) CIFAR-10
Figure 5:	Effect of number of clients N with fixed attackers (F = 1, 5, 10) in i.i.d settings.
(<⅛) Aɔe,lnɔɔ=
10 20 30 40 50 60 70 80 90 100
Number Gf clients
(a) MNIST
10 20 30 40 50 60 70 80 90 100
Number of clients
(b) FMNIST
10 20 30 40 50 60 70 80 90 100
Number of clients
(c) SVHN
10 20 30 40 50 60 70 80 90 100
Number of clients
(d) CIFAR-10
Figure 6:	Effect of number of clients N with fixed attackers (F = 1, 5, 10) in non-i.i.d settings.
continuous local model. However, a discrete local model cannot be used to infer any valuable infor-
mation. The malicious server could compute the continuous global model, but it would not be useful
to explore the privacy information of each client. Another popular inference defense technique is the
homomorphic encryption (HE) (Gentry et al., 2009). Although HE can perfectly defend the infer-
ence attacks from the malicious server, it requires high computation cost and cannot defend against
other weight-based poisoning attacks. Similarly, differential privacy (DP) (Geyer et al., 2017) could
also be used for defending inference attacks. However, Hu et al. (2021) have shown that DP will
lose too much utility of the global model while providing strong privacy protection. FedDiscrete
is a concise but practical approach that can naturally defend the client inference attacks from the
malicious server with a more efficient communication protocol.
6	Limitations and Discussions
FedDiscrete can defend both data poisoning attacks and weight poisoning attacks in FL, since
data poisoning attacks are equal to change the clean model to the poisoned model. However, Fed-
Discrete also has two limitations: (1) It could not work in the cross-silos federated learning set-
ting. If we want to mitigate the adversarial impact and achieve a non-biased aggregated model, it
requires there are many clients could join in training in each round. However, it would not be an issue
in the cross-devices setting involving thousands or even more clients in each round of training; (2)
It could not defend the byzantine attacks through our discretization aggregation mechanism, since
the aggregated model represents the majority of interests in FedDiscrete. However, the byzan-
tine defense via robust local training could still work in our framework, where the local client can
recognize the poisoned model or only leverage the valuable information from the poisoned model to
train the local model via knowledge distillation (Lee et al., 2021). Our next step is to further explore
a novel defense mechanism that can be more scalable for various adversarial environments.
7	Conclusion
In this paper, we proposed a new FL approach that applies the discrete mechanism with adaptive
weight range for protecting FL. To our best knowledge, itis the first work that don’t require the server
to do excessive computation, but successfully defend against various attacks, including availability
poisoning attacks, integrity backdoor attacks, and inference attacks. We also theoretically analyze
the utility, robustness, and convergence of our proposed discrete mechanism in FL. One limitation
of this work is the proposed system cannot defend the attacks well with a small number of clients or
face a large fraction of the attackers, which becomes the next step of our future research.
9
Under review as a conference paper at ICLR 2022
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. Advances in Neural In-
formation Processing Systems, 30:1709-1720, 2017.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938-2948. PMLR, 2020.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning
with adversaries: Byzantine tolerant gradient descent. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 118-128, 2017.
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. Mitigating sybils in federated learning
poisoning. arXiv preprint arXiv:1808.04866, 2018.
Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks
on fully connected neural networks using permutation invariant representations. In Proceedings
of the 2018 ACM SIGSAC Conference on Computer and Communications Security, pp. 619-633,
2018.
Craig Gentry et al. A fully homomorphic encryption scheme, volume 20. Stanford university Stan-
ford, 2009.
Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.
Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, and Xuyun Zhang. Source inference
attacks in federated learning. arXiv preprint arXiv:2109.05659, 2021.
David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social
network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 137-146, 2003.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models.
arXiv preprint arXiv:2004.06660, 2020.
Yann LeCun, Lgon Bottou, Yoshua Bengio, and Geoffrey Hinton. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Gihun Lee, Yongjin Shin, Minchan Jeong, and Se-Young Yun. Preservation of the global knowledge
by not-true self knowledge distillation in federated learning. arXiv preprint arXiv:2106.03097,
2021.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In MLSys, 2020.
WANG Luping, WANG Wei, and LI Bo. Cmfl: Mitigating communication overhead for feder-
ated learning. In 2019 IEEE 39th International Conference on Distributed Computing Systems
(ICDCS), pp. 954-964. IEEE, 2019.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
10
Under review as a conference paper at ICLR 2022
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended
feature leakage in collaborative learning. In 2019 IEEE Symposium on Security and Privacy (SP),
pp. 691-706. IEEE, 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Mustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R Gel. Defending against backdoors in fed-
erated learning with robust learning rate. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 35, pp. 9268-9276, 2021.
Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. Knock knock, who’s there?
membership inference on aggregate location data. arXiv preprint arXiv:1708.06145, 2017.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quan-
tization. In AIStats, pp. 2021-2031. PMLR, 2020.
Shiqi Shen, Shruti Tople, and Prateek Saxena. Auror: Defending against poisoning attacks in col-
laborative deep learning systems. In Proceedings of the 32nd Annual Conference on Computer
Security Applications, pp. 508-519, 2016.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP),
pp. 3-18. IEEE, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv, 2014.
Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified defenses for data poisoning attacks. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
3520-3532, 2017.
Lichao Sun, Yingtong Dou, Carl Yang, Ji Wang, Philip S Yu, Lifang He, and Bo Li. Adversarial
attack and defense on graph data: A survey. arXiv preprint arXiv:1812.10528, 2018.
Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. An embarrassingly simple
approach for trojan attack in deep neural networks. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pp. 218-228, 2020.
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-
yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can
backdoor federated learning. arXiv preprint arXiv:2007.05084, 2020.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Aguera y Ar-
cas, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
Suhas Diggavi, Hubert Eichner, Advait Gadhikar, Zachary Garrett, Antonious M. Girgis, Filip
Hanzely, Andrew Hard, Chaoyang He, Samuel Horvath, Zhouyuan Huo, Alex Ingerman, Mar-
tin Jaggi, Tara Javidi, Peter Kairouz, Satyen Kale, Sai Praneeth Karimireddy, Jakub Konecny,
Sanmi Koyejo, Tian Li, Luyang Liu, Mehryar Mohri, Hang Qi, Sashank J. Reddi, Peter Richtarik,
Karan Singhal, Virginia Smith, Mahdi Soltanolkotabi, Weikang Song, Ananda Theertha Suresh,
Sebastian U. Stich, Ameet Talwalkar, Hongyi Wang, Blake Woodworth, Shanshan Wu, Felix X.
Yu, Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong Zhang, Chunxiang Zheng, Chen Zhu, and
Wennan Zhu. A field guide to federated optimization, 2021a.
Jianyu Wang, Zheng Xu, Zachary Garrett, Zachary Charles, Luyang Liu, and Gauri Joshi. Local
adaptivity in federated learning: Convergence and consistency, 2021b.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv, 2017.
11
Under review as a conference paper at ICLR 2022
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against feder-
ated learning. In International Conference on Learning Representations, 2019.
Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. Crfl: Certifiably robust federated learning
against backdoor attacks. arXiv preprint arXiv:2106.08283, 2021.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proof of Lemma 4.2.
Proof. Notice that for any scalar w ∈ Rn and l ≤ w ≤ u, by the definition of M(.; l, u), one can
verify that E[M(w; l, u)] = w, therefore E[M(w; l, u)] = w. Furthermore,
d
E[kM(w; l, u) - wk2] = X E[([M(w; l, u)]j - [w]j)2]
j=1
d
=X(E[[M(w; l,u)]2] - [w]2)
j=1
d
=X((U + l)[w]j- lu -Wj)
j=1
=X"(*2-(Wj-(十 A2#
d(u — l)2
≤	4
Moreover,
d
□
A.2 Proof of Theorem 4.2.
Before proving the Theorem 4.2, a standard probability bound is required.
Lemma A.1. Let {Xi}in=1 be a sequence of i.i.d continuous random variables, whose support is on
R. Then for any b ∈ R
P max Xi ≥ b ≤ nP(Xi ≥ b).
Proof.
P (max Xi ≥ b) = 1 — P (max Xi ≤ b) = 1 — P(Xι ≤ b,…，Xn ≤ b)
n
= 1 — Y P(Xj ≤ b) = 1 — (1 — P(Xj ≥ b))n for any j
j=1
Letf(t) := 1 —nt — (1 — t)n fort ∈ [0, 1]. As f0(t) ≤ 0 andf(0) = 0, f(t) ≤ 0forallt ∈ [0, 1].
Therefore, 1 — (1 — t)n ≤ nt. Take t = P(Xj ≥ b), we arrive
P max Xi ≥ b ≤ nP(Xi ≥ b).
□
Now we are ready to prove Theorem 4.2.
13
Under review as a conference paper at ICLR 2022
Proof. For any j ∈ [d], |[w t]j - [tWM j| ≤ Ut - lt and Var ([w tj - [wM j) ≤ (Ut - lt)2 /4 due
to Lemma 4.1. By Bernstein inequality, for any j ∈ [d] and > 0
P (∣[wt]j - [wM]j I ≥ e) ≤ 2eχΡ
Ke2
2KK Pi∈St Var (Wt]j - [wM]j) + 3e(Ut- lt)
≤ 2 eχp
Ke2
+ 2 e(ut — lt)
—
By Lemma A.1,
P (m∈aχl[wtj - [wWMj| ≥e) ≤2dexp (- (Q-咛 Ke；Ut n
Therefore, for any β >	0, there exists e = O
P (maxj∈[d] | [Wt]j - [WM]j | ≤ e) holds with probability at least 1 - β.
(Ut-It)JIog 2d
√k
such that
□
A.3 Proof of Theorem 4.3
Proof. Due to the weight discretization mechanism, the adversary can only return U or l for each
coordinate of the model weight. In order to not return the correct information, the adversary could
choose to return the opposite feedback to attack the model, i.e., return U if the original return is l,
and vice versa. Therefore, we denote, for any l ≤ w ≤ U,
Madv(w)
lh,,
w.p. W-
w.p. hh--w
Under the scenario that there are F attackers, for any j ∈ [d],
[E[Wm ]]j = E
N-F
X [M(Wi)]j +
i=1
Σ
i=N -F +1
[Madv (Wi)]j
1
N
N-F	N
N X [wi]j + N X ((h + l) - [wi]j)
i=1	i=N -F +1
1	N-F	N	F
NI X [wi]j- X	[wi]j I+(h+l) N
i=1	i=N -F +1
(4)
(5)
□
A.4 Proof of Theorem 4.4
Proof. The proof is inspired by the analysis in Theorem 4 of (Li et al., 2020).
To proceed with the analysis, we first introduce some notations. At the tth round, for the all i ∈
St, define Wt+1 = wt + 南 Pi∈s0 (wt+1 - wt), Wt+1 = wt + £川改]Pi(wt+1 - wt), and
W t+1 = argmi□w hi(w; wt) := Fi (w) + 2 ∣∣w - Wt『.Wt+1 is the ghost global model as if the
discretization mechanism is not applied to the local model weights; Wt+1 is another ghost global
model as if all clients participate in the tth round training and no discretization mechanism is applied
;Wt+1 is the exact minimizer of the strongly convex function hi(w). These points reference points
are crucial for the analysis. Define the gradient residual e；+1 = VFi(wt+1) + μ(wi+1 - wt), then
wt+1 - Wt = - 1 VFi(WW+1) + 1 e；+1. Therefore,
Wt+1 - wt = X Pi(WW+1 - wt) = -1 X PiVFi(WW+1) + 1 X Pieti+1	(6)
i—,	μ i—,	μ i—,
i∈[n]	i∈[n]	i∈[n]
14
Under review as a conference paper at ICLR 2022
Since μ is chosen to satisfy μ > λmin, then hi(w; Wt) is “-strongly convex. By the strong convexity
of hi,
∖∖wt+1 - Wt+1II2 ≤ 1(wt+1 - Wt+1 )τ(Vhi(wt+1) -^hi(W^,t+1))
μ
≤ 1 ∖∖wk+1 - ^^t+1∖∖∖∖^hi(wt+1) - Vhi(Wt+1)∖∖,
μ
which, together with the fact that Wt+1 is the minimizer of hi(w), implies
∖∖wt+1 - Wt+1∖∖ ≤ 1 ∖∖Vhi(wit+1) - Vhi(Wt+1))∖∖
μ
=1 ∖∖Vhi(Wt+1))∖∖ = 1∖VFii(Wt+1)+ μ(Wt+1 - Wt)W
μ	μ
≤ γ∖∖VFi(Wt)∖∖	(7)
μ
Again use the same analysis, one has ∖∖ Wt+1 - Wt ∖∖ ≤ iVFi(Wt). Therefore, together with Eq. 7,
∖∖Wt+1- Wt∖∖ ≤ ∖∖Wt+1 - Wt+1∖∖ + ∖∖Wt+1- Wt∖∖ ≤ 1+γ ∖∖VFi(Wt) ∖∖ .	(8)
μ
Therefore, one can bound the distance from the ghost global model Wt+1 to the current global
weight as
∖∖Wt+1 - Wt∖∖ = Epi(Wt+1 - Wt)
∖i∈[N]
≤	pi ∖∖Wit+1 - Wt ∖∖
i∈[N]
≤ 中 X pi ∖∖VFi(Wt)W μ i∈[N ]	(by Eq. 8)
≤ *SX pi kVFi(wt)k2 μ	i∈ i∈[N]	(Jensen’ Inequality)
=丁 ,Ei[kVFi(wt )『] μ ≤ b⅛j2 WVf(Wt)W μ	(by Assumption 4.1 (3))
(9)
Note that
Epi(VFi(Wt+1)-et+1-VFi(Wt))
i∈[n]
≤ £ pi (∖∖VFi(Wt+1) - VFi(Wt)∖∖ + ∖∖et+1∖∖)
i∈[n]
≤ X pi (LWWt+1 - w'∖∖ + ∖∖ei+1∖∖)
i∈[n]
e≤8 (L^ + Y)X pi ∖∖VFi(wt)∖∖
' μ	i i∈[n]
=(L(IjY) + Y) Ei[VFi(wt)]
E≤8 B ( L(IjY) + Y) ∖∖Vf (Wt)∖∖ .	(10)
15
Under review as a conference paper at ICLR 2022
By Assumption 4.1 (1), one has
f (Wt+1) ≤ f (Wt) + Vf(Wt)T(Wt+1 - Wt) + 2 ∣∣Wt+1 - Wt∣∣2
E≤ 6 f (Wt) + Vf(Wt)T (-1 X PiVFi(Wt+1 ) + 1 X Piet+1 I + 2 ∣∣Wt+1 - Wt∣∣2
μ i∈[n]	μ i∈[n]
=f (wt) + Vf(Wt)T -1 X Pi (VFi(Wt+1) - et+1 - VFi(Wt)) - 1 Vf(Wt)
μ W]	μ
+ 2∣∣wt+1 - wt∣∣2
≤ f (wt) - 1∣∣f (wt)∣∣2 - 1 f(wt)τ I X Pi (VFi(Wi+1)-ei+1- VFi(Wt)))
μ	μ	i∈[n]
+ 2∣∣Wt+1- Wt∣∣2
≤ f (Wt) - 1∣∣f (Wt)∣∣2 + 1∣∣f (Wt)∣∣
μμ
EPi (VFi(Wt+1)-et+1-VFi(Wt))
i∈[n]
+ 2∣∣Wt+1 - Wt∣∣2
Eq 1≤Eq 9 f(Wt) - 1 ∣∣f(Wt)∣∣2 + B (Lto) + Y) ∣∣Vf(Wt)∣∣2 + 白)21∣Vf(Wt)∣∣
μ	μ μ	μ μ
(11)
=f (Wt) - (T - LBI^ - L(I +尸)∣∣Vf (Wt)∣∣2	(12)
μ	μμ	μ
By mean-value theorem and triangular inequality, for some α ∈ [0,1]
f(Wt+1) ≤ f(Wt+1) + ∣∣Vf(QWt+1 + (1 - α)Wt+1)∣∣ ∣∣Wt+1 - Wt+1∣∣
≤ f(Wt+1) + ( ∣ ∣ Vf(QWt+1 + (1 - α)Wt+1) - Vf(Wt)∣∣ + ∣∣Vf(Wt)∣∣) ∣∣wt+1 - Wt+1∣∣
≤ f(wt+1) + (L IlQWt+1 + (1 - α)wt+1 - wt∣∣ + ∣∣Vf(wt)∣∣) ∣∣wt+1 - wt+1∣∣
≤ f(wt+1) + (L(∣∣wt+1 - wt∣∣ + ∣∣wt+1 - wt∣∣) + ∣∣Vf(wt)∣∣) ∣∣wt+1 - wt+1∣∣
(13)
Taking expectation with respect to the random index set 必 one gets
ESt[f(Wt+1)] ≤ f(wt+1) + (L ∣∣wt+1 - Wt∣∣ + ∣∣Vf(Wt)∣∣) Eso ∣∣wt+1 - wt+11∣
+ LEso[ ∣∣wt+1 - Wt∣∣ ∣∣wt+1 - wt+1∣∣]
≤ f(wt+1) + (L ∣∣wt+1 - Wt∣∣ + ∣∣Vf(Wt)∣∣) Eso ∣∣wt+1 - wt+1∣∣
+ leso[( ∣ ∣wt+1 - wt+1∣∣ + ∣∣wt+1 - wt∣∣) ∣∣wt+1 - wt+1∣∣]
=f(Wt+1) + (2L ∣∣wt+1 - Wt∣∣ + ∣∣Vf(Wt)∣∣) Eso ∣∣wt+1 - wt+1∣∣
+ LES0 [ 11 Wt+1 - Wt+1∣∣2]	(14)
16
Under review as a conference paper at ICLR 2022
By the sampling scheme, one has
ESMwt+1 - wtT∣2 ≤ * Ei[∣M+1 - wt+1『]
ISt I
≤ 占 Ei [ 11 wt+1 - wt∣∣2 + ∣∣wt - W t+1∣∣2 + 2(wt+1 - Wt)T(Wt - Wt+1)]
lSt i
≤ 焉Ei[l∣ wt+1 - wt∣∣2	(byEi(Wt+1) = Wt+1)
lStl
e≤8 ɪ9"Ei[∣∣ VFi(Wt) ∣ ∣ 2]
lStl	μ
≤ BTT(I +2γ) ∣∣Vf(Wt)∣∣2	(by Assumption 4.1 (3)).	(15)
lStl	μ
Combining Eq. 14, Eq. 15, and Eq. 9, together with the fact that E5/ ∣∣Wt+1 - Wt+1∣∣ ≤
JESO ∣∣Wt+1 - Wt+1『 as a result of the Jesen,s inequality, one reaches to
Es0 [f(Wt+1)] ≤ f(Wt+1) +
2L B2 (1 + Y)2 + ɪ (1 + Y) + L星(1 + γ)2
PSi	μ2	+ PlSl μ + l&l 评
Vf(Wt) 2
f(wt+1)+O5+1)+1≡)∣∣vf(Wt)∣∣2	(16)
Combine Eq. 16 and Eq. 11, one reaches to
1 - YB	LB(1 + Y)	L(1 + Y)2B2
μ	μμ	2μ2
(LB2S1∣ +2Y)2 (2pSl +1) + b⅛Y2)) ∣∣Vf(Wt)∣∣2	(17)
∖	lStlμ	M√lStl ))
Taking the expectation with respect to the discretization mechanism, Em [Wt+1] = Wt+1 by
Lemma 4.1. Since f is L-smooth,
17
Under review as a conference paper at ICLR 2022
Em[∕(wt+1)] ≤ f(wt+1) + 2EM[∣∣wt+1 - wt+1∣∣2]
X (M(Wt+ι - W)-(Wt+ι - w)) ∣∣
i∈S0	I I
≤ f(Wt+1) +
2
X(Wt+1 - w)
i∈S0
(by Lemma 4.1)
≤ f(Wt+1) +
XI∣Wt+1- w∣∣2 + X(w"-W)T(Wj+1 - w)
i∈[n]	i=j
≤ f(Wt+1) +
X ∣∣wt+1 - w∣∣2 + X ∣∣wt+1 - w∣∣ ∣∣wj+1 - w∣∣
i∈[n]	i=j
≤f (W t+1)+ 8LNB (X ∣∣wt+1 - w∣∣2
t	i∈[n]
(by 2 H 网 ≤H2 + M2)
≤ f (W t+1 ) + ° ("min-- w∣∣2
≤f (W t+1 )+「("i∣∣wt+1- w∣∣2
≤f(w t+1 )+8^⅛ (Xg ⅜f∣"i(Wt)∣∣2
≤ f(wt+1) +
LN	B2(1+ Y)2
8∣S0∣2Pmin	μ2
Wf(Wt) ∣ ∣ 2
(18)
f(w t+1)+21⅛EM
L
W
L
L
Put Eq. 18 and Eq. 17 together, we reach to
Em,s0 [f (wt+1)] ≤ f(wt)-
1 - YB LB(1 + Y)	L(1 + Y)2B2
μμ
LB2(1+ y)
∣s0∣μ2
(2 HSti+1) +
2μ2
B(1 + γ)	LNB2(1 + y )2
8∣S0 ∣2Pmin"2
∣∣vf (wt) ∣ ∣ 2
μ
2
—
—
—
μ vW∣
—
(19)
When there is no adversary, then ∣S0∣ = K, then
Em,s0[f(wt+1)] ≤ f(wt) - κ∣∣Vf(Wt)∣∣2
Finally, taking the total expectation with respect to all randomness and by telescoping, one reaches
T-1
K X E[∣∣Vf(wt)∣∣]2 ≤ f(W0) - f(w*).
t=0
Divide T on both sides, then
I T-1
川n∕kVf(wt)k] ≤ T NE[∣∣Vf(wt)∣∣]2 ≤
f (W0) - f(w*)
KT
□
18