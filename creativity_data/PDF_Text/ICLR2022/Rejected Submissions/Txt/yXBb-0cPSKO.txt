Under review as a conference paper at ICLR 2022
Regularized-OFU： an efficient algorithm for
GENERAL CONTEXTUAL BANDIT WITH OPTIMIZATION
ORACLES
Anonymous authors
Paper under double-blind review
Abstract
In contextual bandit, one major challenge is to develop theoretically solid and
empirically efficient algorithms for general function classes. We present a novel
algorithm called regularized optimism inface Ofuncertainty (ROFU) for general
contextual bandit problems. It exploits an optimization oracle to calculate the well-
founded upper confidence bound (UCB). Theoretically, for general function classes
under very mild assumptions, it achieves a near-optimal regret bound O(^∕T).
Practically, one great advantage of ROFU is that the optimization oracle can be
efficiently implemented with low computational cost. I⅛us, we can easily extend
ROFU for contextual bandits with deep neural networks as the function class,
which outperforms strong baselines including the UCB and Thompson sampling
variants.
1	INTRODUCTION
Contextual bandit is a basic sequential decision-making problem which is extensively studied and
widely applied in machine learning. At each time step in contextual bandit, agent should choose an
action a∞ording to a presented context, and will receive a reward conditioned on the context and the
selected action. The goal of the agent is to maximize its cumulative reward, which is equivalent to
minimizing regret.
Algorithms for contextual bandit can be divided into two categories: agnostic algorithms and
realizability-based algorithms. The agnostic algorithms, e.g., EXP4 (Auer et al., 2002b; McMahan &
Streeter, 2009; Beygelzimer et al., 2011), provide worst-case optimal regret bounds for any function
class and data. However, the time complexity of EXP4 is linear to the cardinality of the function class
which is intractable for large function classes.
The realizability assumes that the reward is generated from an underlying model, whose form is known
but with some parameters to be determined. When the realizability is satisfied in real-world problem,
the realizability-based algorithms usually perform much better than the agnostic algorithms. The most
popular realizability-based algorithms are UCB (Auer et al., 2002a) which selects action according
to an upper confidence bound, and Thompson sampling (Thompson, 1933) which makes decisions
according to samples from the posterior distribution. Both UCB and Thompson sampling achieve
near-optimal regret bound for many function classes. However, the construction of upper confidence
bound and sampling from the posterior distribution are extremely computationally expensive for
general function classes.
Ib overcome the computational barrier, there are a line of works (Agarwal et al., 2014; Dudik et al.,
2011; Foster & Rakhlin, 2020; Foster et al., 2018) that reduce the decision-making problem to an
optimization problem, and then exploit optimization oracles to accelerate computation. Nonetheless,
the optimization oracle may not be feasible or efficient for general function classes. In face of the
challenges above, when dealing with modem function classes, such as deep neural networks, ⅛ese
theoretically solid algorithms either become computationally intractable or do not achieve low regret
in practice.
Our paper also falls into this line of research. We propose a novel algorithm, called Regularized OFU
(ROFU) which is developed upon the realizability assumption and an optimization oracle. In ROFU,
1
Under review as a conference paper at ICLR 2022
we measure ⅛e uncertainty of the reward function by a regularizer, and then calculate the optimistic
estimation by maximizing the reward function with the regularizer penalizing its uncertainty. To
the end, we give a novel formulation of upper confidence bound for general function classes. Our
algorithm achieves near-optimal regret bound under very mild assumptions, and can be efficiently
solved by standard optimization oracles, such as gradient descent. Thus, our algorithm can be easily
extended to deep neural networks in a computationally efficient manner.
We summarize our contributions as follows:
•	We propose a new UCB variant ROFU, which is designed for general function classes with
provably near-optimal regret. ROFU computes UCB with an optimization oracle which can
be efficiently implemented for complex ⅛nction classes including deep neural networks.
•	Theoretically, for general function classes under very mild assumptions, we prove that ROFU
achieves a regret of 0(y Tlog which matches the lower bound up to a logarithm
factor, where Θ is the parameter space. As a special case, we present a regret bound for
linear function class which is the same to that in (Abbasi-Yadkori et al., 2011).
•	Empirically, we evaluate ROFU on complex contextual bandits with deep neural network as
the function class. Wie show that ROFU also provides efficient UCB estimation for popular
DNN architectures including MLP and ResNet. Moreover, our algorithm enjoys a smaller
regret than strong baselines on real-world non-linear contextual bandit problems introduced
by Riquelme et al. (2018).
2 Preliminary
We consider the contextual bandit problem (Bubeck & Cesa-Bianchi, 2012) with K actions.
Definition 1 (Contextual bandit). Contextual bandit is a sequential decision-making problem where
the agent has a set of actions A. At each time step ti the agent first observes a context xt, then selects
an action at E A based on the context. After taking the action, the agent receives a reward r⅛.
Realizability-based algorithms are developed under the following assumption.
Assumption 1 (Realizability assumption). Er4 := %*(xt,血)^here %*(xt, at) is afunction with
unknown groundtruth parameters θ* ∈ Θ.
The agent aims to maximize its expected cumulative reward EXT ∕e* (¾, at) which is equiva-
lent to minimizing the regret Rτ = J2t<τ maxα(J⅛* (a的 a)— /*(3, g⅛)) under the realizability
assumption. For convenience, let a； = arg maxα 加* (% α).
3 Method
Algorithm 1 Regularized Optimism in Face of Uncertainty
1:	Input: A reward function J⅛* with unknown θ*, number of rounds T.
2:	Dq := 0.
3： fbrt= 1,..., T do
4:	Observe xt.
5:	Va ∈ A9 compute
QpjjR /	∖	IU(N九 α)	Option I for general function classes	(1)
5 I Lin-U (xt, a) Option II, an improved version for linear functions
6:	Take at = arg maxα∈j4 OFUR(a⅛, a) and receive reward rt with Ert = ∕θ* (xt, at).
7:	Let Dt ：= Df-1 U {(出古,α古,τ£)}∙
8： end for
In this section, we first formally present the optimization oracle and the algorithm. Then we discuss
the intuition behind our method. After that we provide the theoretical justification of the algorithm,
2
Under review as a conference paper at ICLR 2022
showing it,s near-optimal in terms of regret under very mild assumptions. Finally, we give an
empirically efficient implementation of the algorithm relying on gradient descent. Proofs of our
theoretical results can be found in Section 3.3 and Appendix.
Our algorithm is developed upon the following oracle.
Definition 2 (Optimization oracle). Given dataset D := {(g，, αt∕, rt∣)}tf<t before round tt for xt, at
We assume there is an optimization oracle to compute
U(xt,a) = max∕⅛(g,α) - ηtja(MSE(θ; D) + α∣∣0∣∣2),	(2)
0
where MSE(Θ; 0)= 尚 £3&产)€0(%(力，ɑ) 一 r)2 and 小⑷ ɑ > θ are constants to be specified
later. For convenience^ let θtja = argmax0 ɑ) — ιη±,a(MSE(θ; D) + α∣∣0∣∣2).
The availability of such optimization oracle is a very mild assumption in practice. For example, we
can exploit gradient-based algorithm to approximately solve Eq. (2) for differentiable functions.
As summarized in Alg. 1, our algorithm is as follows: in round ti the agent invokes the optimization
oracle to compute OFUR (g, a) for each action. Then, the agent selects at = arg maXa OFUR(Z% α).
We now provide more insights into Alg. 1 and the optimization oracle in Eq. (2). The key to minimize
regret is to trade-off exploration and exploitation (EE). In order to maximize cumulative reward, the
agent exploits collected data to take the action with high estimated reward while it also explores the
undiscovered areas to Ieam knowledge. Our method follows the Optimism in Face of Uncertainty
(OFU) principle, which is widely verified to be effective in EE trade-off. When facing uncertainty,
OFU first optimistically guesses how good each action could be and then takes the action with highest
guess.
Eq. (2) gives such an optimistic guess by maximizing J⅛(c力 a) under the regularization of mean
squared error. The intuition behind Eq. (2) is that: from the view of exploitation, if θt,a is a parameter
with a small mean squared error, and 了Gt <% a) ɪ8 large. Then we can expect ∕e*(xf, a) is also large
as 0t,a is close to θ* in general; from the view of exploration, if θtia increases the value of 鼠 Q (xt, a)
without significantly increasing MSE®。； D), then the uncertainty on the reward of (X力 a) would
be large.
3.1	Regret analysis
Besides the conciseness and clear intuitions, the algorithm also enjoys several theoretical advantages:
Theorem 1 develops a O{↑Jτ log regret under Assumption 2. The regret bound in Theorem 1
matches the lower bound as presented in Theorem 2 up to a logarithm factor, showing the algorithm
is near-optimal.
Assumption 2. There exists constants eɪ, C2 > O and a function p : Θ —> R+ U {0}, such that
∀%,α,4 ∣∕⅛3,α)—加(%α)∣ ∈ [c1g(θ),c2g(θ)].
Assumption 2 is very mild that includes many function classes. For example, if YX) α, function
hxja(θ) = ∣∕0(x, α) — /0* (x, α)∣ is Ci-Strongly convex and C2-smooth, then Assumption 2 is true.
Theorem 1 (Regret for general functions with Option /). Under Assumption 2, if∖∕θ, z, α, fe(x, a) ∈
[—1,1], let τ]t,a = ^ χ ⅛~∣θ∣t Md a = Q, then with probability at least 1 — 5, the regret of Alg. 1
Witk Option I satisfies
Rt ≤ 8 狈 ʌ/TIog 里.	(3)
Cl V	O
Theorem 2 (Lower bound). For any C2 ≥ eɪ > O, there is a function class {fθ}θeθ that satisfies
Assumption 1 and 2 with some g(θ). One can construct a context sequence {xt}t<τ such that the
expected regret of all bandit algorithms is lower bounded by p(ʌ/riog ∣Θ∣).
3
Under review as a conference paper at ICLR 2022
It is obvious that when C2∕c1 is large, the regret bound in Theorem 1 is meaningless. However,
fortunately, for linear functions, which is the most interesting function class with C2∕c1 = ∞, Alg. 1
achieves a near optimal regret bound as in Theorem 3 and 4, indicating boarder theoretical potentials
to develop regret bound of Alg. 1 for function classes beyond that in Assumption 2.
Theorem 3 (Regret for linear functions with Option I). If the function class is linear; ie,
fe(xia) = θτφ(x^ a)f and Θ = {∣∣0∣∣ < Vd}f ∣∣<∕>(rr, α)∣∣ < ∖fd. Then, ClPPlying Alg. 1
With Option I to discretized parameter space Θe which is an e-mesh. Let e = 1∕T,, a = 1
αzιd τ]t1a = ∖ʌ/βtφt,a∙^-~'Φt,a Where Λ⅛ = / + J2f,<t ΦtfΦt,f Φt-,a = 0(g,α), Φt = Φt,at>
βt = max(128dlntln(t2∕5), (∣ ln(t2J))2) is the confidence width in (Dani et al.t 2008), We have
Witk probability at least 1 — 2δ, the regret of Alg. 1 with Option I satisfies
Rτ < ^8d2TβτinT + Vd + 32y∕d2Tβτ log - = O(dVτ).
O
The ε-mesh in Theorem 3 introduces an addition regret than standard LinUCB. More importantly, the
calculation of ιηt,a ɪs expensive. We can improve the performance on linear functions with Option ∏
which invokes the optimization oracle two times for each action per round.
We postpone the proof of Theorem 3 to Appendix.
Theorem 4 (Regret for linear functions with Option II). Let θt = arg min0 MSE{Θ∖ Dt) + ∣∣^∣∣2∕∣Z)f ∣
and θtta = argmax0 t∕⅛(% a) - ηtia{MSE[θ∖ Dt) + IIelI2). Let
Lin-U{xt, α) = /&_ (% «) + ʌ/∕⅛ α(≈i, «) - fgt.1 (% ɑ).	(4)
We have Lin-U(ft, ɑ) is equivalent to upper confidence bound in LinUCB Abbasi-Yadkori et al.
(2011). Thus, setting l∕(2%α) to be the confidence width in (Abbasi-Yadkori et al.r 2011 )f We have
with probability at least 1 — ʌ, the regret of Algt 1 with Option II is
Rτ ≤ O(d√T).
Proof, LinUCB (Abbasi-Yadkori et al., 2011) uses the following upper confidence bound for ¢(Xt) a).
LiiIUCB(% a) = θ^lφt,a ÷ Q8M,4'Φt,a∙	(5)
where At = I -∖- EYy ΦtfΦ^, Φt,a =	α), Φt = 6他t, ɑi) and βt is confidence width.
Then the proof is straightforward after observing that θt-χ = Λ71 ∑2γ<£ Φt,U, and θt^a = θt-χ +
At^ι%α∕(2私α) and,thus, (⅛,α - θt-ι)γφt,a =康曜N"心	□
3.2	An empirically efficient gradient descent-based optimization oracle
In order to apply Alg. 1, we have to efficiently solve or approximate Eq. (2). In this section, we
consider the case that Eq. (2) is differentiable. Thus, naturally, one can use gradient descent methods
to approximately solve Eq. (2). However, it is still not manageable to optimize from scratch for every
(C九 a). We propose to optimize from Θt-ι for (①幻 ɑ).
More specifically, we approximately solve Eq. (2) by executing a few steps of gradient ascent starting
from θt-χ. That is, ⅛-+ι = θj + K▽&(%(% a) - ιηR(β;。)儿=匈 with θ0 :=&_ 1, where R is the
step size. The above implementation is summarized in Alg. 2.
Alg. 2 essentially performs a local search around Indeed Alg. 2 brings extra benefit for the
optimization by starting from θt-χ in this case. This is because intuitively θt-χ often gets closer to
0(∙, α) when t is larger. For example, in linear contextual bandits, %a 一 =	±,a, a∏d
Il 2^—A^"1^α∣∣ monotonically decreases as t becomes larger according to the definition of ηtja.
It is easy to see that the time complexity of Alg. 2 is O(Mp) where p is the number of parameters. In
Sec. 5, we can see that the regret of ROFU is low in practice with very small M.
4
Under review as a conference paper at ICLR 2022
Algorithm 2 An efficient implementation to estimate U(g, a)
1:	Input: DatasetDji, θt-χ = aτgmi%MSE(优 DJI) + ∣∣^∣∣* 2∕∣Z)t∣ and context-action pair
Xt, a. Learning rate κ and training steps M, hyperparameters η and a.
2:	Set0o(¾,α) := θt-i∙
3： for J = 1, ...,M do
4： θj{xt, α) := ⅛∙-ι(xt, a) + κVθ(fθ(xt, a) — 〃(MSE(19; JD) + 刖创产/昨出一1㈤,where V
is an estimator of gradient.
5:	end for
6:	return dʌr(g,ɑ).
3.3 Proof of Theorem 1
Let us start with some useful notations. For convenience, we abbreviate ηt^a as ηt since ηt^a =
η±,a∕, Vα, a! in Theorem 1. Let θtjtt = arg max0eθ ∕e(a⅛, α)—小MSE(g; D) denote the parameter
used in U(xt,a), ∆t(6∙) = (∕e(¾,αt) - rt)2 - (∕0-(¾,αt) - rt)2 and At(0) = (∕e(¾,αt)-
∕l9* (% at))2. It is easy to see ⅛at E∆i(^)=儿⑻ where the expectation is over the randomness of
the reward.
Lemma 2 presents a consentration inequality of E ∆t(0) which is derived from Lemma 1.
Lemma 1 (Freedman-type inequality, Theorem 1 in (Beygelzimer et al., 2011)). For any martingale
Zt with I Zt I ≤ Rf with probability at least 1 — δt for a ∈ (0, l∕H]f
Tl
£ Zt ≤ £ α(e - 2)E[Z2] + ⅛≡.	(6)
t=ι t<τ	a
Lemma 2 (Bounded differences). With probability at least 1 — 5, V0 ∈ Θ, i ≤ T,
2 £ A(0) + 16 log ɪʒ- > E ʌt-	⑺
t,<t	t,<t
Proof. This lemma is essentially a restatement of Lemma 4 in (Foster et al., 2018). Applying Lem 1
to martingale {EΔt(^) — A(O)}烂下, we have with probability at least 1 — δ∕(∖θ∖T')
£ E∆t(0) - ∆t(θ) ≤ α(e — 2) E E[(EΔt(0) - ∆t(0))2] + 喇：/)
t<τ	t<τ
< 4a(e — 2) E E∆t(61) +
t<τ	a
The second inequality is because E[(EΔt(0) — ∆t(0))2] ≤ 4EΔt(0). Setting a = 1/8 and rearrang-
ing, with fact 入t(θ) > 0 and applying union bound, we have with probability at least 1 — δ
yt<T,θeQ, 2 £ A ⑹ + 16 Iog 孚 > £ At
t,<t	t,<t
□
Let 碗=arg maXa Λ* (χti a), according to the definition of §t)a and the definition of ⅜, we have
<)-小MSE(e*) ≤ ®S (% <)-班MSE(瓦《),
(M 球)-班MSEa&) ≤ /&,%(% at)-加MSEa皿).
5
Under review as a conference paper at ICLR 2022
Summing up me above two inequalities leads to
∕θ*(¾, «：) ≤ fgt (% at)-%(MSEa,“； A) — MSE(0*; Dt)).	(8)
τ>at
Now we present the proof of Theorem 1.
Proof of Theorem L Assuming the events in Lemma 2 happen, we have
Rτ = £ fθ∙(χt, O - fθ∙(xt, at)
t<τ
<	(.χt>at') -¾(MSE(⅛,αt5Dt) -MSE(6»*;Z)t)) - ∕θ.(xt,at)
t<τ '
<-
L ʌ	ΘT
(¾, «t) - fθdχt, «t) - ¾∕(i -1)52 X”(仇,aj + TJt 16/(t - 1) log j-ɪ-
*	t,<t
ɪɪ lɑg	+ C2g(,θ)-涡g(0)2)
Σ9
<-
≤∑√^1θg≡
≤8^√Tlog≡
Cl V	∂
The first line is the definition of regret; the second line is according to Eq. (8); the third line is by
Lemma 2; the fourth line is by Assumption 2; the second last line is by setting ηt =
ι / ɑi(^-ɪ)
□
4	Related work
Optimism in face of uncertainty (OFU): Our algorithm is essentially a variant of OFU principle,
which is a powerful framework to trade-off exploration and exploitation for bandit problems. As
discussed in Section 3, OFU algorithms take actions according to an optimistic estimation over the
reward. Most OFU algorithms optimistically estimates the reward by the best possible value over a
confidence set of the reward functions, i.e.,
OFU5(x, a) := max%(ga),	(9)
%θj
where Θ⅛ := {θ ： P(D∣0) > δ} and P(Z)∣0) is the likelihood of D given θ. In many cases, the
constraint can be replaced by MSE(0; D) < 6. For simple function classes such as multi-armed
bandit and linear contextual bandit, OFUS has closed-form solutions ɪ.
However, for more complex tasks, OFU algorithms explicitly maintain a confidence set, e.g., see
(Foster etal., 2018; Dudik etal., 2011). The cost of explicitly maintaining a confidence set is extremely
expensive for complex problems or function classes, such as deep neural networks. One alternative
approach is to consider the Lagrangian Multiplier method, i.e., solving minρ maxτ7 α)—
MiSE(仇 DX But this is still much slower than our optimization oracle in general.
Our results, theorem 1 and theorem 4, suggest that we can effectively trade-off exploration and
exploitation while avoiding explicitly maintaining the confidence set.
1The computational cost of closed-form solutions can be expensive for high-dimensional problems even
when the function class is linear.
6
Under review as a conference paper at ICLR 2022
Figure 1: Ablation study on MLP and ResNet bandits. Notation: M = 1/5/10 means that we run
1/5/10 gradient descent updates in Alg. 2.
Contextual bandit algorithms with optimization oracle: As mentioned in section 1, some contex-
tual bandit algorithms invoke optimization oracles to accelerate computation. Agarwal et al. (2014);
Dudik et al. (2011) rely on cost-sensitive policy classification oracles and achieve an optimal regret of
O(<ξ∕Tlog(T∣Π∣∕(5)) where ∏ is the space of policies. This kind of oracles can be computational inef-
ficient for complex function classes. And these algorithm call the optimization algorithm many times
in each round to achieve a regret guarantee, e.g. , Dudik et aL (2011) calls the optimization oracle for
O(T5) times. Foster et al. (2018) access a regression oracle which is special case of Eq. (2) with
η ≈ +∞. But Foster et al. (2018) calls the oracle for O(IogT) times for each action in each round.
More importantly, they explicitly maintain a subset of Θ with MSE(0; D) < min。，MSE(7；D) +13.
And as (Hscussed above, maintaining such confidence set is infeasible for complex function classes.
Algorithms for deep contextual bandit: We note that there are attempts (Zhou et al., 2020; Zhang
et al., 2020) to extend the realizability-based algorithms to deep neural networks. NeuralUCB (Zhou
et aL, 2020) and Neural Thompson sampling (Zhang et al., 2020) conduct experiments on multi-
layer neural network with significantly simplified and approximate implementation to accelerate
computation. The analyses do not apply to general function classes.
5	Experiment
We now empirically evaluate ROFU. We only present empirical results for Option ∏ as the empirical
performances of Option ∏ is slightly better than Option I. For simplicity, in all our experiments,
we set ηt,cb = 1 and a = 1. More specifically, we train a_i by minimizing MSE(0; Dt) with
standard optimizer 2 and is trained to maximize t∕⅛(% a) — ∣D∣MSE(0; D) using Alg. 2. And
OFUR(% a) = fθt l (% a) ÷。/&通(3,。)一耳ʤ, G) as in Option U.
5.1	Analysis on MLP and ResNet bandits
The goal of this work is to develop a contextual bandit algorithm which is efficient in trading off
EE when reward is generated from a complex function while keeping a low cost on computational
resources. From Alg. 2, we can see that the time complexity of ROFU is determined by the training
step M. To reduce computational cost, we evaluate ROFU when M is small in experiments. As
suggested by the experimental results, setting M to be a relatively small value doesn,t hurt the
performance much.
To evaluate the performance of ROFU in Alg. 2 on complex tasks, we consider two contextual bandits
with a DNN as the simulator. That is, r(xt, a) is generated by a DNN model. We consider two
popular DNN architectures to generate rewards: 2-layer MLP and 20-layer ResNet with CNN blocks
and Batch Normalization as in He et al. (2016). We summarize other information of the two tested
bandits in Table. 1.
2We train θt-ι with stochastic gradient descent starting from θt-2 in all the experiments.
7
Under review as a conference paper at ICLR 2022
Bandit	Layer	Context Dim	# Arms	NN Parameters	Context Distribution	Noise
MLP	2	10	10^^	Random	Gaussian	N(0,0∙05)
ResNet	20	3 x 32 x 32	10	Trained on CifarlO	Uniform	M(0,0.5)
Table 1: Basic information about MLP and ResNet bandits.
mushroom
—RoFU(OUrS)
----Greedy
Dropcut
NeuraIUnear
----Bootstrap
5000 1MM 150M	20000
Time step
financial
—ROFUgUE)
---Greedy
Dropout
NeuraILinear
---Bootstrap
---ParamNcise
ItKK)	20M	30M
Time step
∞vβrtype
—RoFUoMlrS)
----Greedy
Dropout
NeuraILinear
----Bootstrap
SOOO 100M	150M	20000
Time step
100∞	150∞	20000
Time step
jester
ROFU (ours)
Greedy
Dropout
NeuralLinsar
BOOtBtEP
ParamNoise
NeuraIUCB
ROFU (ours)
Greedy
Dropout
NgUElLinear
Bootstrap
ParamNoise
NeuraIUCB
ROFU (ours)
Greedy
Dropout
NeuraILinear
Bootstrap
ParamNoise
NeuraIUCB
5000	10000	150∞	20000
Tims step
5DOO IQQQQ 15Q00	20000
Time step
5000	10(XX)	160∞
Time step
Figure 2: Evaluations on non-linear contextual bandits.
We use DNNs with larger size for training in Alg. 2. More specifically, for MLP-bandit, fo is chosen
as a 3-layer MLP and for ResNet20-bandit, t∕⅛ is chosen as ResNet32. Each experiment is repeated
for 16 times. We present the regret and confidence bonus in Fig. 1. From Fig. 1, we can see that (1)
ROFU can achieve a small regret on both tasks with a considerably small M even for veιy large DNN
model; (2) The confidence bonus monotonically increases with M . For each M, the confidence
bonus converges to 0 as expected. Moreover5 while the regret seems sensitive to the value of M, the
regrets of ROFU with M = 5,10 are much smaller than the case OfM = I and e-greedy.
5.2 Performance comparison on real-world datasets
Tb evaluate ROFU against powerful baselines, we conduct experiments on contextual bandits which
are created from real-world datasets, following the setting in Riquelme et al. (2018). For example,
suppose that D := {(¾, Ct)}t<τ ɪs a -classification dataset where xt is the feature vector and
ct ∈ [K] is the label. We create a contextual bandit problem as follows: at time step t <T, the agent
observes context xt, and then takes an action at. The agent receives high reward if it Successftilly
predicts the label of xt. For non-classification dataset, we can turn it into contextual bandit in similar
ways. For the details of these bandits, please refer to Riquelme et al. (2018).
For baselines, we consider NeuralUCB Zhou et al. (2020) and Thompson sampling variants from
Riquelme et al. (2018). It is noteworthy that we only evaluate the algorithms in Riquelme et al. (2018)
with relatively small regrets. We directly run the code provided by the authors. For ROFUs we fix
M = 5 for all experiments. We tune other hyper-parameters of ROFU on statlog and directly apply
the hyperparameters on statlog to other datasets except mushroom. This is because the reward scale
of mushroom is much larger than other datasets. For baselines, we directly use the best reported
hyper-parameters.
8
Under review as a conference paper at ICLR 2022
We report the regret Rγ = IEEXTTIEEXTT(g,α±) WhereM = arg maxαej4 /0∕ (xt, a)
and Q, is the parameter trained by minimizing MSE on the whole dataset. The results are presented
in Fig. 2 and Table 2. We found that the regret of NeuralUCB is occasionally linear. This might be
because that NeuralUCB uses a diagonal matrix to approximate Z to accelerate. Moreover, we can
see that ROFU significantly outperforms these baselines in terms of regret.
	Mean	Census	Jester	Adult	Covertype	Statlog	Financial	Mushroom
Dropout	1.75±o.βo	1.51±o.ιo	1.34±o.i4	l.OO±o.θ9	1.14±o.i3	1.54±0.87	3.5O±o.6θ	2.21±0.42
Bootstrap	2.23±ι.oo	2.51±o.i6	1.72±o.ιι	1.43±o.ιo	1.93±o.13	1.43±1.57	4.52±2.29	2.04±O.48
ParamNoise	2.30±ι.i2	2.28±o.23	1.59±o.i4	1.37±o.ιo	1.80±o.2o	3.88±6.4o	4.07±1.76	1.06±O.32
NeuralLinear	1.82±0.69	3.24±o.47	1.7O±o.13	1.46±o.i2	1.84±o.19	1.25±o.ιι	2.25±o.35	1.00±0.38
Greedy	2.47±i.12	2.76±i.24	1.65±o.ιo	1.56±o.ιι	2.27±o.23	3.08±4.91	4.74±2.3i	1.2O±o.4i
NeuralUCB	9.76±i4.02	1.72±o.12	1.47±o.o8	1.18±o.o5	1.86±o.16	41.42±69.51	2.74±o.5o	17.29±7.45
ROFU (ours)	1.05±o.o9	1.00±o.o9	1.00±o.2o	1.17±o.o6	1.00±o.i4	1.00±O.24	1.00±o.βo	1.22±0.37
Table 2: The final regret of each algorithm. The regrets are normalized according to the algorithm
with smallest regret.
Conclusion and future work
In this work, we propose an OFU variant, called ROFU, which is designed for contextual bandit
with general function classes. We show that the regret of ROFU is (near-)optimal under very
mild assumptions. Moreover, we propose an efficient algorithm to approximately compute the
upper confidence bound. Thus, ROFU is efficient in both computation and EE trade-off, which are
empirically verified by our experimental results.
EE trade-off is a fundamental problem that lies in the heart of sequential decision making. How-
ever, the huge computational cost of (near-)optimal EE trade-off algorithms significantly limits the
application, especially on complex domain. We hope our method could inspire more algorithms to
efficiently trade-off EE for sequential decision-making tasks beyond contextual bandit, such as deep
reinforcement learning.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems, 24:2312-2320, 2011.
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Coriference on
Machine Learning, pp. 1638-1646. PMLR, 2014.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 2002a.
Peter Auer, Nicolo Cesa-Bianchi, Ybav Freund, and Robert E Schapire. The nonstochastic multiaπned
bandit problem. SIAM journal on computing, 32(1):48-77,2002b.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit
algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics, pp. 19-26. JMLR Workshop and Conference
Proceedings, 2011.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics, 2011.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. 2008.
9
Under review as a conference paper at ICLR 2022
Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and
Tong Zhang. Efficient optimal learning for contextual bandits. arXiv preprint arXiv:1106.2369,
2011.
Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with
regression oracles. In International Conference on Machine Learning, pp. 3199-3210. PMLR,
2020.
Dylan Foster, Alekh Agarwal, Miroslav Dudfk5 Haipeng Luo, and Robert Schapire. Practical
contextual bandits with regression oracles. In International Conference on Machine Learning, pp.
1539-1548. PMLR5 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)i June
2016.
H Brendan McMahan and Matthew Streeter. Tighter bounds for multi-armed bandits with expert
advice. 2009.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for Ihompson sampling. 2018.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrikai 1933.
Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. arXiv
preprint arXiv:2010.00827, 2020.
Dongruo Zhou, Lihong Li5 and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, 2020.
10