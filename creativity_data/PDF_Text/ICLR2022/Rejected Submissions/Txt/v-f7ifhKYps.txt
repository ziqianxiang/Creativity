Under review as a conference paper at ICLR 2022
Maximum Entropy Population Based Training
for Zero-Shot Human-AI Coordination
Anonymous authors
Paper under double-blind review
Ab stract
An AI agent should be able to coordinate with humans to solve tasks. We con-
sider the problem of training a Reinforcement Learning (RL) agent without using
any human data, i.e., in a zero-shot setting, to make it capable of collaborating
with humans. Standard RL agents learn through self-play. Unfortunately, these
agents only know how to collaborate with themselves and normally do not per-
form well with unseen partners, such as humans. The methodology of how to
train a robust agent in a zero-shot fashion is still subject to research. Motivated
by maximum entropy RL, we derive a centralized population entropy objective to
facilitate learning of a diverse population of agents, which is later used to train a
robust AI agent to collaborate with unseen partners. The proposed method shows
its effectiveness compared to baseline methods, including self-play PPO, the stan-
dard Population-Based Training (PBT), and trajectory diversity-based PBT, in the
Overcooked game environment. We also conduct online experiments with real
humans and further demonstrate the efficacy of the method in the real world.
1	Introduction
Deep Reinforcement Learning (RL) has gained many successes against humans in competitive
games, such as Go (Silver et al., 2017), Dota (OpenAI, 2019), and StarCraft (Vinyals et al.,
2019a). However, it still remains a challenge to build AI agents that can coordinate and collabo-
rate with humans that the agents have not seen during training (Kleiman-Weiner et al., 2016; Lerer
& Peysakhovich, 2017; Carroll et al., 2019; Shum et al., 2019; Hu et al., 2020; Knott et al., 2021).
Zero-shot human-AI coordination is particularly important in real-world applications, such as co-
operative games (Carroll et al., 2019), communicative agents (Foerster et al., 2016), self-driving
vehicles (Resnick et al., 2018), and assistant robots (Andrychowicz et al., 2018). Ultimately, we
want to build AI systems that can assist humans and augmenting our capabilities (Engelbart, 1962;
Carter & Nielsen, 2017) to make our life better.
The mainstream method for building state-of-the-art AI agents is through self-play reinforcement
learning (Tesauro, 1994; Silver et al., 2017). Self-play-trained agents are very specialized, and
therefore suffered significantly from distributional shift when paired with humans. For example,
in the Overcooked game, the self-play-trained agents only use a specific pot and ignore the others.
However, humans use all pots. The AI agent ends up waiting unproductively for the human to deliver
a soup from the specific pot, while the human has instead decided to fill up the other pots (Carroll
et al., 2019). Since the AI agent has only encountered its own policy during training, it undergoes a
distributional shift when it is paired with human players.
We explore how to design a robust and efficient approach to train agents for zero-shot human-AI co-
ordination. To that end, we draw on the advances in maximum entropy RL (Haarnoja et al., 2018b),
diversity (Eysenbach et al., 2019), and Multi-agent RL (Lowe et al., 2017; Foerster et al., 2018).
First, maximum entropy RL augments the standard reward function with an entropy maximization
term and provides a substantial improvement in exploration and robustness (Ziebart et al., 2008;
Toussaint, 2009; Ziebart, 2010; Rawlik et al., 2013; Fox et al., 2015; Haarnoja et al., 2017; 2018b;
Zhao et al., 2019). For a population of agents, we use the maximum entropy bonus to encourage
individual diversity, i.e., each agent’s policy to be diverse and exploratory. Secondly, to acquire
diverse and distinguishable behaviors (Eysenbach et al., 2019), we further utilize the sum of cross-
entropy terms across all agent pairs in the population to encourage pairwise diversity. We define the
1
Under review as a conference paper at ICLR 2022
combination of individual diversity and pairwise diversity as population diversity. Thirdly, analog
to multi-agent RL (Lowe et al., 2017; Foerster et al., 2018), the population diversity is calculated in
a centralized fashion. Each agent in the population is rewarded to maximize the centralized popula-
tion diversity. Subsequently, we derive a safe and computationally efficient surrogate objective, i.e.,
population entropy, which is proven to be a lower bound of the original population diversity objec-
tive. The population entropy is defined as the entropy of the averaged action probability distribution
conditioned on the state across all agents in the population. Eventually, we train a new agent to col-
laborate with each agent in the diversified population using prioritized sampling of agents policies
from the population based on the learning progress (Schaul et al., 2016; Vinyals et al., 2019a;b; Han
et al., 2020). The newly trained AI agent encounters a diverse set of strategies and is in general more
robust to human behaviors (Pan & Yang, 2009; Tobin et al., 2017; Andrychowicz et al., 2018). We
evaluate the proposed Maximum Entropy Population-based training (MEP) framework in a testbed
based on the popular Overcooked game (Ghost Town Games, 2016).
Our contributions are three-fold. First, based on the novel population diversity objective that con-
siders both agents’ individual diversity and pairwise diversity, we derive a safe and computationally
efficient surrogate objective, i.e., the population entropy. Secondly, we develop the MEP frame-
work, which comprises training a diversified population and using this population to train a robust
AI agent. Last but not least, in the experiments, we show MEP’s superior performance, by compar-
ing it with state-of-the-art baseline methods. To further verify the improvements, we conduct online
experiments with real humans.
2	Preliminaries
Markov Decision Process: A two-player Markov Decision Process (MDP) is defined by a tuple
M = hS, {A(i)}, P, γ, Ri (Boutilier, 1996), where S is a set of states; A(i) is a set of the i-th
agent’s actions, where i ∈ [1, 2]; P is a transition function that maps the current state and all agents’
actions to the next state; γ is the discount factor; R is a reward function. The i-th agent’s policy is
π(i). A trajectory is denoted by τ. The shared objective is to maximize the expected sum rewards,
which is Eτ [Pt R(st, at)], where at = (a(t1), at(2)). We can extend the objective to infinite horizon
problems by the discount factor γ to ensure that the sum of expected rewards is finite.
AI Agent, Population, and Human: Throughout this paper, we use the phrase AI agent to explicitly
mean the agent that plays the AI role in human-AI coordination. The population of agents is used to
train the AI agent to make it capable of cooperating with different partner agents. The human policy
is represented as ∏(H) and a model of the human policy is Π(H). The AI agent is denoted as ∏(A).
Environment: We use the Overcooked environment (Carroll et al., 2019) as the human-AI coor-
dination testbed, see Figure 2. In the Overcooked game, to have a high score, it naturally requires
coordination and collaboration between the two players. The players are tasked to cook soups.
Maximum Entropy RL: Standard RL maximizes the expected sum of rewards Eτ [Pt R(st , at)].
At the beginning of learning, almost all actions have equal probability. After some training, some
actions have a higher probability in the direction of accumulating more rewards. Subsequently, the
entropy of the policy is reduced over time during training (Mnih et al., 2016). Maximum entropy RL
augments the standard RL objective with a maximum entropy term (Ziebart, 2010; Haarnoja et al.,
2018b), which gives a reward to the agent ifit selects the non-dominate actions during training, and
the higher the reward favors more exploration. The maximum entropy RL objective is defined as:
J (∏)= ET X R(st,at) + αH(π(∙∣s∕).	⑴
t
The parameter α adjusts the relative importance of the entropy bonus against the reward and con-
trols the stochasticity of the optimal policy. The maximum entropy RL objective has a number of
advantages. First, the policy is incentivized to explore more widely. Prior works have demonstrated
improved exploration using the maximum entropy RL objective (Haarnoja et al., 2017; Schulman
et al., 2017a). Secondly, the policy can capture multiple modes of optimal behaviors. In situations
where multiple actions are equally important, the policy will give equal probability mass to those
actions. Lastly, recent works have shown improved robustness of the policy trained with maximum
entropy RL (Haarnoja et al., 2018a; 2019).
2
Under review as a conference paper at ICLR 2022
Algorithm 1: Maximum Entropy Population
while not converged do
Sample agent from population:
π(i) 〜{∏⑴,π⑵,…,π(n)}
for t -1 to stepsjper-episode do
Sample action at 〜 ∏(i)(at∣st).
Step environment st+ι 〜p(st+ι | st,aj.
Calculate the population entropy reward
and combine it with the task reward:
r = r(st,αt) - αlog(∏(at∣sj)
Update policy π(i) to maximize Eτ [r].
Figure 1: Maximum Entropy Population: We train each agent in the population to maximize its
task reward as well as the population entropy reward to attain a maximum entropy population.
3 Method
In this section, we first define the population diversity objective, which includes the policy entropy
and the pairwise difference among policies. Secondly, we derive a safe and computationally efficient
surrogate objective, i.e., the population entropy, for optimization. Thirdly, we illustrate the MEP
framework, which comprises two parts: training a maximum entropy population and training a
robust AI agent via prioritized sampling using the population.
3.1	Population Diversity
Motivated by maximum entropy RL, we want to make the policy population exploratory and diverse.
First, by utilizing the maximum entropy bonus, we encourage each policy itself to be diverse and
multi-modal. Secondly, to encourage the policies in the population, {π(1), π(2), ..., π(n)}, to be
complementary and mutually different, we naturally utilize the cross entropy objective (Murphy,
2012). Formally, we define the Population Diversity (PD) as the combination of the sum of the
entropy of each agent’s policy and the sum of the Cross Entropy (CE) between each pair of agents
in the population. Mathematically,
nn	n
PD({π⑴，π⑵，…，π⑺},st) := XXCE(π⑴(∙ ∣st),π⑶(∙ |st))+ XH(π(i)( •阿)),⑵
where cross entropy (CE) and entropy (H) are defined as follows:
CE(n(i)( ∙ ∣st),π(j) (∙ ∣st)) = - X n(i)(at|st)log πj (a |s。，	(3)
a∈A
H(∏(i)( ∙ ∣st)) = - X ∏⑺(at∣st) logπ⑺(at∣st).	(4)
a∈A
This objective not only captures single agent’s diversity but also encourage multiple agents to be far
from each other. However, the population diversity objective is not safe to be maximized as a reward
function, since cross entropy is unbounded. Besides, evaluating the population diversity objective
has a quadratic runtime complexity of O(n2), where n is the population size.
3.2	Population Entropy
To improve the stability and the runtime complexity, we derive a safe and efficient surrogate objec-
tive for optimization, which is named the Population Entropy (PE). Population entropy is defined as
the entropy of the mean of all policies in the population. Mathematically,
1n
PE({π⑴，π(2),...,π( )},st) := H(∏(∙∣st)), where π(a|st) := — V^π( )(at∣st).	(5)
n i=1
The population entropy serves as a lower bound of the population diversity objective.
3
Under review as a conference paper at ICLR 2022
Theorem 1. Let the population diversity be defined as Equation (2). Let the population entropy be
defined as Equation (5). Then, we have
PD({π(1), π(2), ..., π(n)}, st) ≥ n2PE({π(1), π(2), ..., π(n)}, st),	(6)
where n is the population size. Proof. See Appendix A.
Compared to the population diversity objective, the population entropy objective, H(∏( ∙ ∣st)), is
safe to maximize and it has only a linear runtime complexity O(n). Therefore, we use the derived
population entropy objective for optimization. Taking a closer look at the population entropy objec-
tive, we find that it can be written into the Jensen-Shannon Divergence (JSD) and entropy form as
follows:
1n
H(∏( ∙ ∣st)) = JSD(π⑴(∙ |st),…,π⑺(∙ ∣st)) + — fH(∏W(∙版))，	⑺
n i=1
A step-by-step derivation is in Appendix B. From Equation (7), we can see that when we maximize
the population entropy objective, it attempts to push each policy away from each other via the JSD
term in Equation 7, as well as increase each policy’s entropy via the entropy term in Equation 7.
3.3	Training a Maximum Entropy Population
We want to train a population of agents, who can play well with themselves. In the meantime,
we also want their strategies to be different from each other. Subsequently, we define the reward
function for MEP as follows:
1n
J(∏) =	ET	ER(st, at)	+ αH(∏( ∙	∣st))	, where ∏( ∙ |st)	:= n E∏(i)(	∙	|st).	(8)
t	i=1
Equation (8) tells us that for each agent in the population, the agent is trained to maximize its task
reward as well as the centralized population entropy objective. The task reward is related to the agent
and its partner agent, which is a copied version of itself playing the partner role in our case. When
the centralized population entropy reward is calculated, it considers all the agents in the population.
We summarize the method of training a maximum entropy population in Algorithm 1 and Figure 1.
After having the maximum entropy population, we utilize this diverse set of agents to train the AI
agent to be ready to pair with human players. The intuition behind MEP is that the AI agent should
be more robust when paired with a group of diversified partners during training than trained only
via self-play. In the extreme case, when the AI agent can coordinate well with an infinite set of
different partners, it can also collaborate well with humans. In a more realistic sense, the more
diverse the population is, it is more likely to cover most of the human behaviors in the training set.
Subsequently, the final AI agent should be less “panicked” when facing “abnormal” human actions.
3.4	Training the Agent via Prioritized Sampling
Considering that playing with different partner agents in the population, the AI agent needs different
amounts of training time to learn to coordinate with them. We propose to use prioritized sampling
based on the learning progress (Vinyals et al., 2019b), i.e., the expected accumulated reward, to
adjust the frequency of each partner agent in the population to occur during training. Mathematically,
the probability of the i-th agent to be sampled is:
C	rank (1/EThPt R(St,a(A),ati))i)β
p(∏(i)) =-------∖ / L r-------------------旦R,	(9)
Pjn=1 rank 1/ET PtR(st,a(tA),at(j))
where the superscript (A) refers to the AI agent that we train for coordinating with humans; n is
the population size; rank(∙) is the ranking function ranging from 1 to n; β is a hyper-parameter for
adjusting the strength of the prioritization. We assign a higher priority to the agents that are relatively
harder to collaborate with. In the extreme case, at each optimization step, if we always choose the
hardest agent in the population to train the AI agent, then we optimize a performance lower bound
of the cooperation between the AI agent and any agent in the population. Mathematically,
π(A)
= arg max min	J (π(A), π(i)),	(10)
i∈{1,...,n}
4
Under review as a conference paper at ICLR 2022
Figure 2: Overcooked environment: From left to right, the layouts are Cramped Room, Asymmetric
Advantages, Coordination Ring, Forced Coordination, and Counter Circuit.
where J (π(A), π(i)) denotes the expected sum reward achieved by π(A) and π(i) collaborating with
each other. With prioritized sampling, we make the collaboration between the AI agent and any agent
in the population as good as possible. While uniform sampling does not provide any guarantee on
the worst case. For more detail on the performance lower bound, i.e., Equation (10), see Lemma 4
in Appendix C. Furthermore, we derive the performance connection between two pairs of agents,
i.e., (π(A), π(i)) and (π(A), π(j)), when the partner agent π(i) in the first pair is -close (Ko, 2006)
to the other partner agent π(j) in the second pair, see Lemma 5 in Appendix D. Based on Lemma 5,
if the population we used for training is diverse and representative enough, then we can find an
agent that is -close to the human player’s policy and have a performance lower bound of human-AI
coordination. In this case, prioritized sampling optimizes not only the performance lower bound of
the AI agent and the population, see Equation (10), but also the performance lower bound between
the human player and the AI agent, see Corollary 1 in Appendix D.
4 Experiments
Environment: To evaluate the proposed method, we first use a toy environment, i.e., the matrix
game (Lupu et al., 2021), see Figure 3, and then use the Overcooked environment (Carroll et al.,
2019), see Figure 2. The Overcooked game naturally requires human-AI coordination to achieve a
high score. The players are tasked to cook the onion soups as fast as possible. The relevant objects
are onions, plates, and soups. Players are required to place 3 onions in a pot, cook them for 20
timesteps, put the cooked soup in a plate, and serve the soup. Afterwards, the players receive a
reward of 20. The six actions are up, down, left, right, noop, and interact. There are five different
layouts, see Figure 2. Each layout has a different challenge. For example, in Asymmetric Advan-
tages, good players should discover their advantages and play to their strengths. The player in the
left has the advantage to deliver the soup. The player in the right is closer to the onions.
Experiments: First, we train the population using the population entropy reward and investigate the
effect of the entropy weight α. Secondly, we use the learned maximum entropy population to train
the AI agent with the learning progress-based prioritized sampling and report the performance. In
an ablation study, we show the effectiveness of both population entropy and prioritized sampling.
We compare our method with other methods, including Self-Play (SP) Proximal Policy Optimiza-
tion (PPO) (Schulman et al., 2017b; Carroll et al., 2019), Population Based Training (PBT) (Jader-
berg et al., 2017; Carroll et al., 2019), and Trajectory Diversity (TrajeDi)-based PBT (Lupu et al.,
2021). To test the methods, we use the protocol proposed by Carroll et al. (2019), in which a human
proxy model, HProxy, is used for evaluation. The human proxy model is trained through behavior
cloning (Bain & Sammut, 1999) on the collected human data. Furthermore, we conduct a user study
using Amazon Mechanical Turk (AMT), in which we deploy our models through web interfaces and
let real human players play with the AI agents. The experimental details are shown in Appendix E.
Our code is available as supplementary material.
Question 1. How does MEP perform in the toy environment, i.e., the matrix game?
In the single-step collaborative matrix game (Lupu et al., 2021), player 1 must select a row while
player 2 chooses a column independently. Both agents get the reward associated with the intersection
of their choices at the end of the game. We use the same evaluation protocol as proposed by Lupu
et al. (2021). From Figure 3, we can see that MEP converges faster than TrajeDi. We did an extensive
hyper-parameter search for TrajeDi, as shown in Figure 7 in Appendix F.
Question 2. Does the population entropy reward increase the entropy of the population?
5
Under review as a conference paper at ICLR 2022
	O	O	O	O	O	O	O
ɪ	O	O	O	O	O	O	O
°	O	1	"。2	O	O	O	O
ΓΞ	O	0.02	1	0.02	O	O	O
o	O	O	0.02	1	"。2	O	O
ΓΞ	O	O	O	"。2	1	。:	O
ΓΞ	O	O	O	O	"。2		0.02
o	O	O	O	O	O	0.02	1 l
ΓΞ	O	O	O	O	O	O	002
E	O	O	O	O	O	O	O I
O
O
O
O
O
O
O
O
O
O
O
O
O
O
Self-Play Return
1.0
0.8
⅛ 0.6
g
期4
0.2
0.0
Cross-Play Return
Figure 3:	Performance comparison: Train and test performances on the matrix game. Shown are
the results for Best Responses (BRs) to MEP agents, BRs to TrajeDi populations, BRs to baseline
populations, and individual agents. MEP allows faster learning compared to TrajeDi and others.
Figure 4:	Mean episode reward and population entropy learning curve: The left two plots are the
mean episode reward and population entropy with entropy reward weight α = 0 in the Asymmetric
Advantage layout. The right two plots show the quantities with α = 0.01 in the same layout.
To verify if the population entropy reward indeed increases the entropy of the population, we monitor
the population entropy during training. The learning curve of the mean episode reward of the popu-
lation and its corresponding population entropy is shown in Figure 4. This figure shows that as the
training starts, the reward increases, and the population entropy decreases. Comparing the left two
figures with α = 0 and the right two figures with α = 0.01, we can see that with the population en-
tropy reward, the population entropy indeed converges to a higher value. We also try different values
of α and investigate its effect on the reward and the population entropy, see Table 1 in Appendix F.
The parameter α controls the balance between reward and entropy. The complete collection of the
reward and population entropy learning curves with different α is shown in Appendix F.
Question 3. What does a maximum entropy population look like?
To have an intuition of what a maximum entropy population looks like, we show the populations
in the supplementary video from 0:01 to 0:21. In this video clip, we present the population trained
without the entropy reward in the first row and with the entropy reward in the second row. From
the first row, we can see that the blue agent and the green agent move synchronized most of the
time. The opaque routines among agents in the first population are similar. In the second row of
the video clip, we can see that the behaviors of the maximum entropy population are more diverse.
First, the movements of the blue agent and the green agent are less synchronized, especially in the
fifth animation of the second row. Secondly, the routines are less predictable. In general, we observe
more diverse behaviors and randomness of the policies in the maximum entropy population.
Question 4. How does MEP perform compared to baseline methods?
We pair each agent, including SP, PBT, and MEP, with the proxy human model HProxy, and evaluate
the team performance. We test the performance in each of the layouts, shown in Figure 2. Good
coordination between teammates is essential to achieve high scores in the collaborative game - Over-
cooked. Following the evaluation protocol proposed by Carroll et al. (2019), we use the cumulative
rewards over a horizon of 400 timesteps as the proxy for coordination ability. For all tests, we report
the average reward per episode and the standard deviation across 5 different random seeds. Figure 5
shows the quantitative results among different methods and the ablation tests. From Figure 5a, we
can see that MEP outperforms both SP and PBT in all environments. Additionally, we show the
ablation test in Figure 5b. From the ablation test, we can see that both the population entropy reward
the prioritized sampling are necessary components for achieving the best performance.
Question 5. Why does MEP perform better?
6
Under review as a conference paper at ICLR 2022
150
125
100
75
50
25
Performance with human proxy model
ωPOSIdωJωd PmaI"aJωΛV
Cramped Rm.
Counter Circ.
150
125
100
Asymm. Adv.	Coord. Ring	Forced Coord.
(a) Performance Comparison
Performance with human proxy model
50
25
ωPOSIdωJωd PmaI"aJωΛV
ωPOSIdωJωd PJa,Mα,Jα,sf,JωΛV
Cramped Rm.	Asymm. Adv.	Coord. Ring	Forced Coord.	Counter Circ.
(b) Ablation tests
Performance with human proxy model
150
125
100
75
50
25
0
Cramped Rm.	Asymm. Adv.	Coord. Ring	Forced Coord.	Counter Circ.
(c) Performance Comparison
Figure 5: Performance comparison and ablation test: Average episode rewards over 400 timestep
(1 min) trajectories for different methods, with standard error over 5 different random seeds, paired
with the proxy human HP roxy . The hashed bars with the slash (/) show results with the starting
position of the agents switched. Figure (a) shows the performance comparison among MEP and
baselines including SP and PBT. Figure (b) shows the ablation tests, where we use MEPα=0 and
MEPβ=0 to denote the MEP model without the population entropy reward and without the prioritized
sampling mechanism, respectively. Figure (c) shows the performance comparison with TrajeDi.
We take a closer look at some examples. The maximum entropy population contains a diverse set
of policies, shown in the supplementary video from 0:01 to 0:21. In the video, from 0:24 to 0:44,
we show three human-AI gameplay demonstrations of SP, PBT, and MEP, respectively. From the
video, we can see that the SP-trained agent and the PBT-trained agent are prone to get stuck during
the process. However, the MEP-trained agent rarely gets stuck and mostly coordinates smoothly
with its human partner. The reason is that the SP-trained agent and the PBT-trained agent are more
likely to overfit their training partners’ policies, which are usually different from humans’ strategies.
However, MEP uses a high entropy population, which contains a more diverse set of policies. When
the AI agent is trained using the high entropy population, it sees a wide range of behaviors and learns
to adapt to these policies. This helps to make the AI agent more robust during deployment.
Question 6. How does MEP perform compared to TrajeDi?
7
Under review as a conference paper at ICLR 2022
Average reward per episode in each layout
	Cramped Rm.	Asymm. Adv.	Coord. Ring	Forced Coord.	Counter Circ.
SP+HTrue	094.1	133.2	084.1	046.8	050.5
PBT+HTrue	101.6	124.9	098.4	027.6	033.5
MEP+HTrue	124.1	154.7	099.5	056.8	059.1
Figure 6: Performance with real humans
There is a recent related work on zero-shot coordination with a diversified population, which is
called TrajeDi (Lupu et al., 2021). To the best of our knowledge, TrajeDi is the most related work.
TrajeDi utilizes a trajectory-based diversity objective to obtain a diversified population, whereas
MEP formulates a novel action level diversity objective in the multi-agent population setting. To
compare TrajeDi and MEP, we show the experiment results in Figure 5c. In the figure, we use
TrajeDi to denote the original TrajeDi method and use TrajeDi(wPS) to denote the TrajeDi method
enhanced with the proposed Prioritized Sampling (PS) mechanism. From the figure, we can see that
in all settings, MEP significantly outperforms TrajeDi. In 80% settings, MEP performs superiorly in
comparison to TrajeDi(wPS). Overall, MEP shows better performance compared to TrajeDi(wPS).
Question 7. How does MEP perform with real human players?
We test the MEP-trained AI agent and measured the average episode reward when the agent was
paired with a real human player. For this human-AI coordination test, we recruited 40 users (24
male, 14 female, 2 other, ages 22-71) on Amazon Mechanical Turk (AMT) and followed the same
evaluation procedure proposed by Carroll et al. (2019). We reuse the testing results of SP and
PBT from human-AI evaluation on AMT carried out by Carroll et al. (2019). These testing results
are compatible because the evaluation procedure is the same and uses a between-subjects design,
meaning each user was only paired with a single AI agent. The results are presented in Figure 6.
The chart in Figure 6 shows that on average across all five layouts, MEP significantly outperforms
SP and PBT and its performance is on par with the Human-Human coordination performance. For
more detailed results, we take a closer look at the table in Figure 6. This table shows the performance
of each method in each layout. From this table, We can see that MEP achieves the best performance
in all 5 layouts in comparison with SP and PBT. Now, we describe some representative cases below.
Question 8. What does AI do when paired with real human players?
Here, we show and analyze some qualitative behaviors that we observed during the real human-AI
coordination experiments, which are shown in the supplementary video from 0:22 to 2:27. From
0:24 to 0:44, we observe that in the Forced Coordination layout, the MEP-trained agent is more
robust and less gets stuck during coordination, in comparison to SP and PBT. Next, from 0:44 to
1:09, in the Asymmetric Advantage layout, the SP-trained and the PBT-trained agents only learned
to put the onion into the pot. They didn’t learn to deliver the onion soup. While, the MEP-trained
agent not only learned to put the onion into the pot but also learned to deliver the onion soup when
its human partner is busy. Similarly, from 1:09 to 1:29, in the Cramped Room layout, the SP-trained
and PBT-trained agents only learned to use the plate to take the soup, whereas the MEP-trained
agent additionally learned to carry the onion to the pot. Interestingly, from 1:29 to 1:56, in the
Coordination Ring layout, the SP-trained and PBT-trained agent only learned to deliver the onion
soup in one direction, while the MEP-trained agent learned to deliver the soup both clockwise and
counterclockwise, depending on where its human partner stands. Last but not least, from 2:01 to
2:26, in the Counter Circuit layout, the SP-trained and PBT-trained agent learned only pass the
onion over the “counter”. However, the MEP-trained agent also learned to take the plate and deliver
the soup. From all these observations, we observe that the SP-trained and the PBT-trained agents
tend to overfit to their optimal opaque policies, whereas MEP-trained agent is more robust.
8
Under review as a conference paper at ICLR 2022
5 Related Work
Recent works (Lerer & Peysakhovich, 2018; Tucker et al., 2020; Carroll et al., 2019; Knott et al.,
2021) tackle the collaboration problem using some behavioral data from the partner to select the
equilibrium of the existing agents (Lerer & Peysakhovich, 2018; Tucker et al., 2020) or build and
incorporate a human model into the training process (Carroll et al., 2019; Knott et al., 2021). How-
ever, collecting a large amount of human data in real life is expensive and time-consuming. We
consider the zero-shot setting, where no behavioral data from the human partner is available dur-
ing training (Hu et al., 2020). From a Bayesian perspective, when we don’t know what the human
policies look like, we want to train the AI agent to be robust and be capable of collaborating with a
diverse set of policies (Murphy, 2012). There is a growing amount of works on diversity in maxi-
mum entropy reinforcement learning (Ziebart et al., 2008; Ziebart, 2010; Fox et al., 2015; Haarnoja
et al., 2017; 2018b), many of which leverage it as a means of encouraging exploration (Schulman
et al., 2017a; Haarnoja et al., 2018b) or discovering skills (Eysenbach et al., 2019; Zhao et al., 2021).
However, how to train a diversified population through entropy maximization is still subjective to
research. In Multi-agent Reinforcement Learning (MARL), a group of agents is trained to achieve
a common goal by Centralized Training and Decentralized Execution (CTDE) (Lowe et al., 2017;
Foerster et al., 2018). Taking inspiration from CTDE, we propose to train a population of agents
to maximize a centralized surrogate objective - population entropy, to encourage diversity in the
population. Subsequently, we train the AI agent with the maximum entropy population and dy-
namically sample the partner agent based on the learning progress, which shares similarities with
Prioritized Fictitious Self-Play (PFSP) (Vinyals et al., 2019b). PFSP is designed exclusively for
zero-sum competitive games, whereas we are concerned with cooperative games and derive the re-
lationship between prioritized sampling and cooperation performance lower bound, see Appendix C.
With prioritized sampling, we make the AI agent learn a policy that is generally suitable for all the
strategies presented in the population.
The idea of MEP shares a common intuition with domain randomization, where some features of the
environment are changed randomly during training to make the policy robust to that feature (Tobin
et al., 2017; Yu et al., 2017; Peng et al., 2018; Tan et al., 2018; Akkaya et al., 2019; Tang et al., 2020).
MEP can be seen as a domain randomization technique, where the randomization is conducted
over a set of partners’ policies. A recent related work - TrajeDi (Lupu et al., 2021) has a similar
motivation and formulates a trajectory-based diversity objective. To the best of our knowledge,
TrajeDi is the most related work to MEP. In comparison to TrajeDi, we derive the population entropy
objective as an action level diversity objective, which is suitable for the multi-agent PBT setting. In
the experiments, MEP shows superior performance compared to TrajeDi empirically. There are
also other population diversity-based methods, such as Diversity via Determinants (DvD) (Parker-
Holder et al., 2020) and Diversity-Inducing Policy Gradient (DIPG) (Masood & Doshi-Velez, 2019),
which are formulated for the single-agent setting, whereas MEP is designed for the multi-agent
cooperative setting. In games with non-transitive dynamics where strategic cycles exist, e.g., Rock-
Paper-Scissors, Policy-Space Response Oracle (PSRO)-based methods (Balduzzi et al., 2019; Perez-
Nieves et al., 2021; Liu et al., 2021) provide solutions to learn diverse behaviors. Our method is
complementary to these previous works and could be combined with them. MEP bridges maximum
entropy RL and PBT, which is generally applicable for many human-AI coordination tasks.
6 Conclusion
This paper introduces Maximum Entropy Population-based training (MEP), a deep reinforcement
learning method for robust human-AI coordination. The derived population entropy theoretical ob-
jective encourages learning a diverse set of policies. Subsequently, with the learning progress-based
prioritized sampling technique, MEP helps the AI agent to be robust to different human strategies.
In the simulated environments, we show that the developed approach achieves the best overall per-
formance in comparison to state-of-the-art methods. Furthermore, in the real world evaluation with
human players, MEP still demonstrates superior performance, which is comparable with human-
human coordination performance. In addition, the qualitative examples show that MEP-trained
policies are relatively flexible and robust to various human strategies.
9
Under review as a conference paper at ICLR 2022
References
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak,
Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang.
Solving rubik’s cube with a robot hand. arXiv preprint, 2019.
Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa-
chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.
Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelli-
gence 15, Intelligent Agents [St Catherine's College, Oxford, July 1995], pp. 103-129, Ox-
ford, UK, UK, 1999. Oxford University. ISBN 0-19-853867-7. URL http://dl.acm.org/
citation.cfm?id=647636.733043.
David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jader-
berg, and Thore Graepel. Open-ended learning in symmetric zero-sum games. In International
Conference on Machine Learning, pp. 434-443. PMLR, 2019.
Craig Boutilier. Planning, learning and coordination in multiagent decision processes. In Proceed-
ings of the 6th conference on Theoretical aspects of rationality and knowledge, pp. 195-210.
Morgan Kaufmann Publishers Inc., 1996.
Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca
Dragan. On the utility of learning about humans for human-ai coordination. In Advances in
Neural Information Processing Systems, pp. 5175-5186, 2019.
Shan Carter and Michael Nielsen. Using artificial intelligence to augment human intelligence. Dis-
till, 2017. doi: 10.23915/distill.00009. https://distill.pub/2017/aia.
Douglas C Engelbart. Augmenting human intellect: A conceptual framework. Menlo Park, CA,
1962.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.
Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in neural information
processing systems, pp. 2137-2145, 2016.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. arXiv preprint arXiv:1512.08562, 2015.
Ghost Town Games. Overcooked, 2016. https://store.steampowered.com/app/
448510/Overcooked/.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352-1361.
PMLR, 2017.
Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for
hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808, 2018a.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning, pp. 1861-1870. PMLR, 2018b.
10
Under review as a conference paper at ICLR 2022
Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning
to walk via deep reinforcement learning. In Robotics: Science and Systems, 2019.
Lei Han, Jiechao Xiong, Peng Sun, Xinghai Sun, Meng Fang, Qingwei Guo, Qiaobo Chen, Tengfei
Shi, Hongsheng Yu, and Zhengyou Zhang. Tstarbot-x: An open-sourced and comprehensive study
for efficient league training in starcraft ii full game. arXiv preprint arXiv:2011.13729, 2020.
Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. “other-play” for zero-shot
coordination. In International Conference on Machine Learning, pp. 4399-44l0. PMLR, 2020.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based train-
ing of neural networks. arXiv preprint arXiv:1711.09846, 2017.
Max Kleiman-Weiner, Mark K Ho, Joseph L Austerweil, Michael L Littman, and Joshua B Tenen-
baum. Coordinate to cooperate or compete: abstract goals and joint intentions in social interac-
tion. In CogSci, 2016.
Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, Anca Dragan, and Rohin
Shah. Evaluating the robustness of collaborative agents. In Proceedings of the 20th International
Conference on Autonomous Agents and MuItiAgent Systems, pp. 1560-1562, 2021.
Stanley Ko. Mathematical analysis. 2006.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on learning theory, pp. 1246-1257. PMLR, 2016.
Adam Lerer and Alexander Peysakhovich. Maintaining cooperation in complex social dilemmas
using deep reinforcement learning. arXiv preprint arXiv:1707.01068, 2017.
Adam Lerer and Alexander Peysakhovich. Learning social conventions in markov games. arXiv
preprint arXiv:1806.10071, 2018.
Xiangyu Liu, Hangtian Jia, Ying Wen, Yaodong Yang, Yujing Hu, Yingfeng Chen, Changjie Fan,
and Zhipeng Hu. Unifying behavioral and response diversity for open-ended learning in zero-sum
games. arXiv preprint arXiv:2106.04958, 2021.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. In Advances in Neural Information Pro-
cessing Systems, pp. 6379-6390, 2017.
Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot
coordination. In Proceedings of the 38th International Conference on Machine Learning, volume
139 of Proceedings of Machine Learning Research, pp. 7204-7213. PMLR, 18-24 Jul 2021. URL
https://proceedings.mlr.press/v139/lupu21a.html.
Muhammad A Masood and Finale Doshi-Velez. Diversity-inducing policy gradient: Using maxi-
mum mean discrepancy to find a set of diverse policies. Proceedings of the Twenty-Eighth Inter-
national Joint Conference on Artificial Intelligence (IJCAI), 2019.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.
Kevin P Murphy. Machine learning: A probabilistic perspective. adaptive computation and machine
learning, 2012.
OpenAI. OpenAI Five finals. 2019. https://openai.com/blog/
openai- five- finals/.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009.
11
Under review as a conference paper at ICLR 2022
Jack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts. Effective
diversity in population based reinforcement learning. Advances in Neural Information Processing
Systems, 33, 2020.
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of
robotic control with dynamics randomization. In 2018 IEEE international conference on robotics
and automation (ICRA),pp. 1-8. IEEE, 2018.
Nicolas Perez-Nieves, Yaodong Yang, Oliver Slumbers, David H Mguni, Ying Wen, and Jun Wang.
Modelling behavioural diversity for learning in open-ended games. In International Conference
on Machine Learning, pp. 8514-8524. PMLR, 2021.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and rein-
forcement learning by approximate inference. In Twenty-third international joint conference on
artificial intelligence, 2013.
Cinjon Resnick, Ilya Kulikov, Kyunghyun Cho, and Jason Weston. Vehicle community strategies.
arXiv preprint arXiv:1804.07178, 2018.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
International Conference on Learning Representations, 2016.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440, 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.
Michael Shum, Max Kleiman-Weiner, Michael L Littman, and Joshua B Tenenbaum. Theory of
minds: Understanding behavior in groups through inverse planning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pp. 6163-6170, 2019.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017.
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and
Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint
arXiv:1804.10332, 2018.
Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Shaolei
Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward random-
ization. In International Conference on Learning Representations, 2020.
Gerald Tesauro. Td-gammon, a self-teaching backgammon program, achieves master-level play.
Neural computation, 6(2):215-219, 1994.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23-30.
IEEE, 2017.
Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the
26th annual international conference on machine learning, pp. 1049-1056, 2009.
Mycal Tucker, Yilun Zhou, and Julie Shah. Adversarially guided self-play for adopting social con-
ventions. arXiv preprint arXiv:2001.05994, 2020.
Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Woj-
ciech M. Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo
Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dal-
ibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai,
David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen, Yuhuai
12
Under review as a conference paper at ICLR 2022
Wu, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy
Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Silver. AlphaStar:
Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/
alphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019a.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019b.
Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal
policy with online system identification. arXiv preprint arXiv:1702.02453, 2017.
Rui Zhao, Xudong Sun, and Volker Tresp. Maximum entropy-regularized multi-goal reinforcement
learning. In Proceedings of the 36th International Conference on Machine Learning, pp. 7553-
7562. PMLR, 2019.
Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state intrin-
sic control. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=OthEq8I5v1.
Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal
entropy. Carnegie Mellon University, 2010.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse
reinforcement learning. 2008.
Appendix
A Population Entropy Lower Bound
Theorem 2. Let the population diversity be defined as:
nn	n
PD({π(1), π(2), ..., π(n)}, st) := XX
CE(∏(i)( ∙ |st),∏(j)( ∙ ∣st)) + XH(∏(i)( ∙ ∣st)). (11)
i=1 j6=i	i=1
Let the population entropy be defined as:
1n
PE({∏⑴，∏⑵，…，∏(n)}, St)= H(∏( ∙ ∣st)), where ∏(at∣st) ：= 一〉2"("3即).	(12)
n i=1
Then, we have
PD({π(1), π(2), ..., π(n)}, st) ≥ n2PE({π(1), π(2), ..., π(n)}, st),	(13)
where n is the population size.
Proof. Cross entropy is given by:
CE(∏(i)( ∙ ∣st),∏j)(∙ ∣st)) = - X ∏(i)(at∣st) log∏j)(at∣st)	(14)
a∈A
Entropy is given by:
H(∏⑶(∙ |st)) = - X ∏(i)(at∣st)logn(i)(at|st).	(15)
a∈A
13
Under review as a conference paper at ICLR 2022
We can derive the follows:
nn
PD({π⑴，π⑵，…,π⑺}, St)=EECEWi)(∙ ∣st),∏⑶(∙ ∣st)) + EH(∏⑻(.厢))(16)
i=1 j6=i	i
=n2 X n12 (X -∏(i)(at∣St)log∏(j)(at∣St))	(17)
=n2 XX - I∏(i)(at∣st) X ： log∏(j)(at∣st)	(18)
≥n2 XX - Iπ⑴(at∣st)logX ；∏(j)(at∣st)	(19)
=n2 E -∏(at∣st)log∏(at∣st)	(20)
a∈A
=n2H(π( ∙ ∣st))	(21)
=n2PE({π(1), π(2),..., π(n)}, st)	(22)
From Equation (18) to Equation (19), We use Jensen's inequality (Murphy, 2012).	口
B Population Entropy and Jensen-Shannon Divergence
Lemma 3. The population entropy has the following relation with Jensen-Shannon Divergence
(JSD):
1n
H(∏( ∙ ∣st)) = JSD(∏⑴(∙ |st),…,∏(n)( ∙ ∣st)) + —	版)),
n i=1
(23)
where JSD is given by
JSD (∏⑴(∙∣st),…，∏⑺(∙∣st))
1n
n XX∏(i) (at∣St)log
i=1 a∈A
∏(i)(at∣st)
∏(at∣st).
(24)
Proof.
H(∏(∙ ∣St)) = E -∏(at∣St)log∏(at∣St)
at∈A
n1
ΣΣ 一一∏i(at∣st)log ∏(at∣st)
i=1 at∈A
=X X 1 ∏i(at∣St) (log ∏i(TS;) 一 log∏i(at厢))
1n
=JSD(π⑴(∙ |st),…,π⑺(∙ ∣st)) + — fH(∏⑺(∙ ∣st))
n i=1
(25)
(26)
(27)
(28)
□
From the derivation, We can see that maximizing the population entropy is equivalent to maximizing
the JSD of all agent’s policies in the population and the entropy of each agent’s policy. Since the
JSD measures similarity, the first term of Equation (28) encourages the policies to be different from
each other. The second term of Equation (28) encourages each policy to explore.
14
Under review as a conference paper at ICLR 2022
C Prioritized Sampling and Performance Lower B ound
Here, we use π(A) to denote the AI policy and use θ to represent the parameter of π(A) . At the
training step t, we use θt to denote the current parameter. {π(1) , ..., π(n) } is the population used to
train ∏(A). We want to find the optimal parameter θ* for the AI policy ∏(A), so that the AI agent
could cooperate well with any agent in the population.
At each training step, we first sample π(i) from the population, then let π(i) to cooperate with π(A).
Then, we use the sampled trajectory τ to train π(A). J (π(A), π(i)) is the excepted sum rewards
achieved by π(A) and π(i) together. With prioritized sampling introduced in Section 3.4, we assign
higher priority to the agent that is harder to collaborate with. To be more specific, let
i = arg min J (πθ(A), π(i)),	(29)
it
then we sample π(i) to cooperate with πθ(A). During training, θ is updated using the gradient as-
cent method with non-increasing learning rate αt . At each training step t, there exists a subset of
{1, ..., n} denoted by Kθt = {ik|k = 1, ..., l, l ≤ n}, such that
J(πθ(At ), π(i)) > J(πθ(At ), π(ik)) =Ct, ifi ∈/ Kθt,ik ∈ Kθt.	(30)
Since prioritized sampling is used, one of ik0 (k0 ∈ 1, ..., l) could be sampled. Then the gradient of
the current step t is
Vθt J (∏ΘA ),π(ik0))	(31)
The parameter is updated as following:
θt+1 = θt + αtVθt,	(32)
where αt is the learning rate at the training time step t.
Assume that { J(∏(A),∏(i))∣i = 1,…，n} are smooth towards θt, then
g(θ) =	min	J(πθ(A), π(i))	(33)
i∈{1,...,n}
is a piece-wise smooth function and Vθt J (πθ(A), π(ik0)) is equal to the gradient of g(θ) almost ev-
erywhere. Next we prove that using Vθt J (πθ(A), π(ik0)), it could also converge to a local maximum
of g(θ).
Lemma 4. π(A) with parameter θ is trained with the population {π(1), ..., π(n)}. We use the learn-
ing progress-based prioritized sampling to sample the agent from the population for training. As-
sume that J (πθ(A), π(i)) is smooth towards the parameter vector θ and has an L-Lipschitz gradient
for all i. θ is optimized using the gradient ascent with a sufficiently small constant step size. If
J (πθ(A), π(i)) converges and doesn’t go to infinity, it would converge to a local maximum of g(θ).
ʌ ʌ
That is θ converges to a neighborhood V^ of θ, where θ is define as
θ = arg max min J(πθ(A), π(i)).	(34)
θ∈V^ i∈{1,...,n}
Proof. θt denotes the parameter of π(A) at the training step t. We define the index set of i, that
J (πθ(At ), π(i)) equal to mini∈{1,...,n} J (π(A), π(i)):
Kθt = {ik|k = 1, ..., l, l ≤ n},	(35)
where ik satisfies
J(πθ(At ), π(i))	>	J(πθ(At ),	π(ik)) =Ct,	ifi	∈/ Kθt,ik	∈	Kθt.	(36)
ik0 is sampled from Kθt and current gradient is Vθt J (πθ(A), π(ik0)).
15
Under review as a conference paper at ICLR 2022
If ik0 ∈ Kθt+l for all l > 0, the optimization process could be regarded as a non-convex optimization
problem by gradient ascent, then θt converges to a local maximum almost surely by the assumption
J(πθ(A), π(i)) has an L-Lipschitz gradient (Lee et al., 2016).
ʌ
If ∩1>0 Kθt+ι = 0, then first We prove that the sequence {θt} can,t converge to a saddle point. If θ
is a saddle point, Vθ J(∏^A),∏(ik0)) = 0 for all ik，∈ K^, which means J(∏θA), ∏(ik0)) are identical
ʌ
in a neighborhood of θ. This contradicts the local minimum convergence of θt (Lee et al., 2016).
Assume the convergent point θ has a non-zero gradient V J(∏(A), ∏(ik0)), since we use a sufficiently
small constant learning rate α, if J(∏^j)χ, ∏(ik0)) > J(∏^A),∏(ik0)), this would contradict the
convergence of θ, and if J(∏^j)χ夕 ∏(ik0)) ≤ J(∏^A), ∏(ik0)), which means θ is the local maximum.
□
From Lemma 4, we can see that with prioritized sampling, we could improve the lower bound of the
cooperation performance between π(A) and the population {π(1), ..., π(n)}. In comparison, uniform
sampling does not provide any guarantee on the worst case. We call θ0 the optimal solution of mean
sampling:
θ0 = argmax X	J(πθ(A),π(i)).	(37)
θ
i∈{1,...,n}
Then, the worst cooperation between πθ0 and the population must be no greater than the cooperation
between ∏^ and the population. That is:
min J(∏^A), ∏(i)) ≥ min J(πθ(A0 ), π(i)).	(38)
i∈{1,…,n}	§	i∈{1,…,n} θ
D Relation to Human-AI Coordination Performance
To illustrate that we could improve the lower bound of human-AI coordination performance, here
we introduce the connection between -close (Ko, 2006) and return, i.e., expected sum rewards.
Definition 1. We define that π(1) is -close to π(2) at the state st if
n(1)(电版)_ 1
∏(2)(at∣st) -
(39)
for all at ∈ A. If this is satisfied at every st ∈ S, we call π(1) is -close to π(2).
Lemma 5. If an MDP has T time steps and π(1) is -close to π(2), then for all π(A), we have
(1 - )T J(π(2), π(A)) < J(π(1), π(A)) < (1 + )T J(π(2), π(A)),	(40)
where J(π(i), π(A)) denotes the expected sum reward achieved by π(i) and π(A) collaborating with
each other.
Proof. For all trajectory τ , we have
P∏(1),∏(A)(T) = P(SO)πT=01π(A)(I¾lst)π⑴(atlst)P(st+ιlst, atA), at1)).	(41)
Since
(1 - e)∏(2)(at∣st) < π(1)(at|st) < (1 + e)π⑵(a∕st),	(42)
we have
(1 - )T pπ(2),π(A) (τ) < pπ(1),π(A) (τ) < (1 + )Tpπ(2),π(A) (τ).	(43)
And because
J(π(i), π(A)) = X pπ(1),π(A) (τ)r(τ),	(44)
τ
16
Under review as a conference paper at ICLR 2022
where
r(τ) = r(s0, a0, ..., sT-1) =	r(st, at) and at = (a(i), a(A)).	(45)
t
Therefore, we have
(1 - )TJ(π(2), π(A)) < J (π(1), π(A)) < (1 + )T J (π(2), π(A)).	(46)
□
We use π(H) to denote the human player’s policy. From Lemma 5, we can see that if π(H) is similar
to any of the policy π(i) in the population {π(1), ..., π(n)} in a certain degree, measured by -close,
then its cooperation performance with the AI policy π(A), which is trained with the population,
would not deteriorate too much. Furthermore, we derive the following corollary.
Corollary 1. We call the infimum of expected sum rewards of π(A) cooperating with the population
{π(1), ..., π(n)} as:
min	J (π(A) , π(i)) = C.	(47)
i∈{1,...,n}
If π(H) is -close to the policy π(i) in the population, then we have
J(π(A),π(H)) > C(1 -)T,	(48)
where T is the total steps in the trajectory.
Proof. If π(H) is -close to π(i), based on the property of -close, see Lemma 5, we have
J(π(A), π(H)) > (1 - )T J (π(A), π(i)).	(49)
Additionally, since
J(π(A), π(i)) > C,	(50)
we have
J(π(A),π(H)) > C(1-)T.	(51)
□
Since prioritized sampling optimizes the lower bound of expected sum rewards of the AI agent
cooperating with the population, see Lemma 4, and with Corollary 1, we could say that it also
optimizes the lower bound of expected sum rewards of the AI agent cooperating with the Human
player, when the population is diverse and representative enough so that it is close to cover human
behaviors.
E Experiment Details
We ran all the methods in each environment with 5 different random seeds and report the average
episode reward and the standard deviation. The experiments of the maximum entropy population-
based training use the following hyper-parameters:
•	The learning rate is 8e-4.
•	The reward shaping horizon is 5e6.
•	The environment steps per agent is 1.1e7.
•	The number of mini-batches is 10.
•	The mini-batch size is 2000.
•	PPO iteration timesteps are 40000. The PPO iteration timesteps refer to the length in envi-
ronment timesteps for each agent pairing training.
17
Under review as a conference paper at ICLR 2022
•	The population size is 5 for training the maximum entropy population. We use the beginner
model, the middle model, and the best model of each agent in the population to form the
final population for training the AI agent.
•	The weight α for the population entropy reward is 0.01 in general. For the Forced Coordi-
nation layout, we use 0.04.
•	The number of parallel environments used for simulating rollouts is 50.
•	The discounting factor γ is 0.99.
•	The max gradient norm is 0.1.
•	The PPO clipping factor is 0.05.
•	The number of hidden layers is 3.
•	The size of hidden layers is 64.
•	The number of convolution layers is 3.
•	The number of filters is 25.
•	The value function coefficient 0.1.
•	The β for prioritized sampling is 3.
F Experimental Results
Cross-Play Return
1.0
0.8
0.6
和4
0.2
0.0
E
---- BRtoTrajeDi pop alpha-5
BRtoTrajeDi POPaIPha
---- BRtoTrajeDipopaIphaTS
----- BRtoTrajeDi popalpha≡20
----- BRto TrajeDiPoPalPha-25
----- BRtoTrajeDi popalpha"30
BRtoTrajeDi POPalPha-35
---- BRtoTrajeDi pop alpha-40
BRto MEP pop
O
40	60
Training Steps
80	IOO
Figure 7: Performance comparison: We did an extensive hyper-parameter search for TrajeDi. MEP
converges faster than TrajeDi under all the parameters.
Table 1: Best self-play reward and its corresponding population entropy with different α: In
this table, α denotes the weight of the population entropy reward in Equation (8).
α	Cramped Rm.		Asymm. Adv.		Coord. Ring		Forced Coord.		Counter Circ.	
	Rew.	Ent.	Rew.	Ent.	Rew.	Ent.	Rew.	Ent.	Rew.	Ent.
0.000	196.8	0.971	164.1	1.120	183.4	0.878	149.4	0.970	129.0	0.988
0.001	187.6	1.031	160.9	1.051	183.7	0.907	152.0	0.858	118.0	1.152
0.005	189.8	0.949	153.0	1.075	164.1	0.901	163.7	0.889	119.2	1.038
0.010	183.7	1.057	151.8	1.139	167.8	0.840	151.2	1.079	136.5	1.151
0.020	174.2	1.029	149.7	1.074	157.8	0.947	137.8	1.093	121.7	1.171
0.030	154.0	1.134	138.7	1.203	153.6	1.028	133.6	0.957	0.130	1.715
0.040	137.0	1.194	135.0	1.353	125.7	1.122	081.0	1.460	0.000	1.791
0.050	137.1	1.127	118.4	1.364	129.6	0.996	024.5	1.703	0.000	1.791
18
Under review as a conference paper at ICLR 2022
α=0.000
α=0.001
α=0.005
α=0.010
Cramped Room
Asymmetric Advantages
Ooon
5 0 5
P-3pos3u93
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Asymmetric Advantages
KdOJ-Ua u'as≈dod u?32
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
■dgua uo-s≈dod IjeaE
Asymmetric Advantages
P-3pos3u93
1.0
×107
Asymmetric Advantages
Ooo-
5 0 5
p-e„ 3pos3u93
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps ×107
p-e„ 3posa3u932
Asymmetric Advantages
150
100
50
0
1.0
×107
Kdgua u2s≈sd u?32
Forced Coordination
Ooon
5 0 5
We-Ma- 3pos-Uea
0.2	0.4	0.6	0.8
Environment timesteps
Forced Coordination
1.0
×107
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Counter Circuit
OoC
O 5
WeM.aι 3pos3 u?3
0.2	0.4	0.6	0.8
Environment timesteps
Counter Circuit
1.0
×107
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
0.0	0.2	0.4	0.6	0.8
Environment timesteps
Asymmetric Advantages
；1.8
1.6
1.4
'1.2
1.0
0.0	0.2	0.4	0.6	0.8
Environment timesteps
1.0
×107
■dgua uo-s≈dod
■dgua uo-s≈dod IjeaE
0.0	0.2	0.4	0.6	0.8
Environment timesteps
Asymmetric Advantages
；1.8
1.6
1.4
'1.2
1.0
0.0	0.2	0.4	0.6	0.8
Environment timesteps
1.0
×107
Coordination Ring
Coordination Ring
Ooon
5 0 5
WeM.aι 3pos3 u?3
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps ×107
Coordination Ring
■»■«»C
WeM.aι 3pos3 u?3
■dgua uo-s≈dod IjeaE
Kdgua u2s≈sd u?32
Forced Coordination
WeM,■ 3poUea
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Forced Coordination
;1.8
1.6
1.4
1.2
'1.0
0.8
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Counter Circuit
1≈51∞万s>≈>o
WeM.aι 3pos3 u?3
0.2	0.4	0.6	0.8
Environment timesteps
Counter Circuit
1.0
×107
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
"dguə uo-s≈8d
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Coordination Ring
⅛1∙75
E
a 1.50
■ 1.25
⅛
SΛ0°
s
裳 0.75
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Forced Coordination
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps ×107
■dgua uo-s≈dod
■dgua uo-s≈dod IjeaE
Counter Circuit
75
50
25
0
Forced Coordination
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Forced Coordination
；1.8
1.6
1.4
,1.2
1.0
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Counter Circuit
KdOJ-Ua u2s≈sd u?32
WeM.aι 3pos-d3 u?32
Table 2: Mean episode reward and population entropy with different α in all five layouts: Each
column corresponds to a different value of α in the set of [0.000, 0.001, 0.005, 0.010]. There are
five row sections, which correspond to the five layouts. Each row section contains two rows, which
are the plots of the mean episode reward and the mean population entropy of the layout, respectively.
19
Under review as a conference paper at ICLR 2022
α=0.020
α=0.030
α=0.040
α=0.050
0.0	0.2	0.4	0.6	0.8	1.0
Cramped Room
Cramped Room
Voo.
“05
We-Ma- 3pos-Uea
150
100
50
0
Cramped Room
WeM,■ 3pos-e∙Uea2
O，U3 u2s≈dod u?32
^S 150
il
,100
50 50
孝0
0.0	0.2	0.4	0.6	0.8	1.0
0.0	0.2	0.4	0.6	0.8	1.0
0.0	0.2	0.4	0.6	0.8	1.0
EnVirOnmenttimestePs	×107
Cramped Room
叙.8
'
S 1.6
I 1.4
12
S 1.2
S
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Asymmetric Advantages
Ooon
5 0 5
P-3pos3u93
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Asymmetric Advantages
KdOJ-Ua u'as≈dod u?32
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
■dgua uo-s≈dod IjeaE
Asymmetric Advantages
P-3pos3u93
1.0
×107
Asymmetric Advantages
- O O .
0»5
P-3pos-d3u93
P-3pos-s∙3u932
Asymmetric Advantages
125
100
75
50
25
0
Coordination Ring
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Coordination Ring
-dguə u'as≈8d u?32
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Forced Coordination
UooC
“05
We-Ma- 3poUea
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Forced Coordination
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
1.0
×107
0.0	0.2	0.4	0.6	0.8
Environment timesteps
0.0	0.2	0.4	0.6	0.8
Environment timesteps
Asymmetric Advantages
；1.8
1.6
1.4
：1.2
1.0
0.0	0.2	0.4	0.6	0.8
Environment timesteps
1.0
×107
Coordination Ring
lt"1∞s>C
WeM.aι 3P。S-da u?3
1.0
×107
Coordination Ring
^S 125
I 100
宫75
官50
2 25
≡ 0
Coordination Ring
0.0	0.2	0.4	0.6	0.8
Environment timesteps
Coordination Ring
；1.8
1.6
1.4
'1.2
1.0
0.0	0.2	0.4	0.6	0.8
Environment timesteps
"dguə uo-s≈8d IjeaE
1.0
×107
Kdgua u2s≈sd u?32
Forced Coordination
■«»C
WeM,■ 3poUea
1.0
×107
Forced Coordination
We-Ma- 3pos-s∙Uea2
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Forced Coordination
Forced Coordination
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Forced Coordination
■dgua uo-s≈dod IjeaE
0.0	0.2	0.4	0.6	0.8
Environment timesteps
Forced Coordination
；1.8
1.6
1.4
, 1.2
1.0
0.0	0.2	0.4	0.6	0.8
Environment timesteps
1.0
×107
■dgua uo-s≈dod IjeaE
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
1.775
1.750
1.725
1.700
1.675
0.0	0.2	0.4	0.6	0.8
Environment timesteps
1.0
×107
Counter Circuit
Counter Circuit
0.0	0.2	0.4	0.6	0.8
Environment timesteps
380
1.75
3 1.70
g 1.65
0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
Counter Circuit
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
1.0
×107
0'~0'0l-0'°-OM
WeM.aι 3pos3 u?3
Counter Circuit
1.0
×107
0.0	0.2	0.4	0.6	0.8
Environment timesteps
Counter Circuit
5 1.78
o 1.76
1.72
∣Λ74
0.0	0.2	0.4	0.6	0.8	1.0
Environment timesteps	×107
SOJ-Ua u2s≈dod u?32
WeM.aι 3posa3 u?32

WeM.aι 3pos-d3 u?32
0
0
0
0
WeM.aι 3posa3 u?32
Table 3: Mean episode reward and population entropy with different α in all five layouts: Each
column corresponds to a different value of α in the set of [0.020, 0.030, 0.040, 0.050]. There are
five row sections, which correspond to the five layouts. Each row section contains two rows, which
are the plots of the mean episode reward and the mean population entropy of the layout, respectively.
20