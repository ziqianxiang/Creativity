Under review as a conference paper at ICLR 2022
Distributed Skellam Mechanism:
a Novel Approach to Federated Learning
with Differential Privacy
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks have strong capabilities of memorizing the underlying train-
ing data; on the flip side, unintended data memorization can be a serious privacy
concern. An effective and rigorous approach to addressing this problem is to train
models with differential privacy (DP), which provides information-theoretic pri-
vacy guarantees by injecting random noise to the gradients. This paper focuses
on the scenario where sensitive data are distributed among individual participants,
who jointly train a model through federated learning, using both secure multiparty
computation (MPC) to ensure the confidentiality of individual gradients, and dif-
ferential privacy to avoid data leakage in the resulting model. A major challenge in
this setting is that common mechanisms for enforcing DP in deep learning, which
inject real-valued noise, are fundamentally incompatible with MPC, which ex-
changes finite-field integers among the participants. Consequently, most existing
DP mechanisms require rather high noise levels, leading to poor model utility.
Motivated by this, we design and develop distributed Skellam mechanism (DSM),
a novel solution for enforcing differential privacy on models built through an
MPC-based federated learning process. Compared to existing approaches, DSM
has the advantage that its privacy guarantee is independent of the dimensionality
of the gradients; further, DSM allows tight privacy accounting due to the nice
composition and sub-sampling properties of the Skellam distribution, which are
key to accurate deep learning with DP. The theoretical analysis of DSM is highly
non-trivial, especially considering (i) the complicated math of differentially pri-
vate deep learning in general and (ii) the fact that the Skellam distribution is rather
complex, and to our knowledge, has not been applied to an iterative and sampling-
based process, i.e., stochastic gradient descent. Meanwhile, through extensive
experiments on various practical settings, we demonstrate that DSM consistently
outperforms existing solutions in terms of model utility by a large margin.
1	Introduction
Deep neural networks, especially large-scale ones such as GPT-3 (Brown et al., 2020), are known
for their excellent memorization capabilities (Song et al., 2017; Feldman, 2020; Zhang et al., 2021).
However, it is rather difficult to control what exactly the neural net memorizes, and unintended data
memorization can be a serious concern when the underlying training data contains sensitive infor-
mation (Carlini et al., 2019). For instance, consider a bank that trains a GPT-like language model on
call center transcripts. Due to data memorization, it is possible to extract sensitive information by
letting the model auto-complete a prefix, e.g., “my account number is: __”. Clearly, if such a model
(or its API) is ever exposed to the adversary, it becomes a ligation machine as attackers can attempt
with various prefixes to extract sensitive data, and subsequently sue the bank for privacy violations.
Shokri et al. (2017) report that simple and intuitive measures often fail to provide sufficient protec-
tion, and the only way found to completely address the issue is to train the model with the rigorous
guarantees of differential privacy (DP) (Dwork et al., 2006).
This paper focuses on the scenario that multiple individual participants jointly train a machine learn-
ing model using federated learning (FL) (McMahan et al., 2017) through distributed stochastic gra-
dient descent (SGD) (McDonald et al., 2010; Dean et al., 2012; Coates et al., 2013; Abadi et al.,
1
Under review as a conference paper at ICLR 2022
2016a). Specifically, in every iteration, each individual computes the gradients with respect to the
current model weights based on her own data; then, gradients from all participants are aggregated
to update the model. Note that the gradients from each individual may reveal sensitive information
about her private dataset (Shokri et al., 2017; Pyrgelis et al., 2018; Yeom et al., 2018; Nasr et al.,
2019; Melis et al., 2019). A common approach to addressing this problem is by employing a secure
multiparty computation (MPC) protocol (Yao, 1986; Chaum et al., 1987; Gennaro et al., 2002; Ishai
et al., 2010; Beimel et al., 2014; Cramer et al., 2015; Ananth et al., 2018), which computes the
aggregate gradients while preserving the confidentiality of the gradients from each individual par-
ticipant. One advantage of MPC is that it is a decentralized approach that does not require a trusted
third party, which can be difficult to establish in some applications, e.g., in finance and healthcare.
Note that although MPC protects individuals’ privacy in the gradient update process by concealing
the gradient values of each participant, it does not provide any protection against data extraction
attacks caused by unintended data memorization (Dwork et al., 2015; Song & Shmatikov, 2019;
Melis et al., 2019; Song & Shmatikov, 2020). As mentioned earlier, an effective methodology to
defend against such attacks is to perturb the gradients to satisfy differential privacy (Shokri et al.,
2017). Since there is no trusted third-party in our setting, such gradient perturbations need to done
in a decentralized fashion, i.e., each FL participant adds noise to her own gradients, such that the
aggregated gradients over all participants satisfies DP, which is referred to as distributed differential
privacy (Goryczka et al., 2013; Kairouz et al., 2021).
Although gradient perturbation under DP has been studied in previous work (notably,
DPSGD (Abadi et al., 2016b)), it is far from trivial to adapt centralized DP solutions to our setting,
due to a fundamental problem: that the MPC protocol requires gradients to be represented as inte-
gers (more precisely, finite field elements (Paillier, 1999; Bonawitz et al., 2017; Bell et al., 2020)).
DPSGD, on the other hand, injects real-valued Gaussian noise to the gradients. Although real num-
bers can be quantized and (approximately) represented using large integers, the quantized random
noise have rather different mathematical properties, which render a tight privacy cost analysis much
more difficult, especially under the decentralized setting of FL. For instance, a nice property of
the continuous Gaussian distribution is that summing up n continuous noise values following i.i.d.
unit-variance Gaussian distribution results in an amplified continuous Gaussian noise of variance n.
This property does not hold, however, if the Gaussian noise values are first quantized before aggre-
gated. Further, the privacy analysis (specifically, the moment accountant analysis technique) of the
DPSGD algorithm also replies on other important properties of the continuous Gaussian distribu-
tion, which do not hold when the noise is quantized. Hence, DPSGD does not directly apply to our
setting. This issue has been neglected by many existing distributed DP solutions, e.g., (Goryczka
et al., 2013; Valovich & Alda, 2017; Truex et al., 2019).
Existing Solutions. Agarwal et al. (2018) propose cpSGD, which injects binomial noise (i.e., the
sum of multiple binary values drawn from independent Bernoulli trials) to the discretized gradients
at each participant of FL, to satisfy DP. Similar to Gaussian noise in the continuous domain, bino-
mial noise can also be aggregated, i.e., the sum of multiple i.i.d. binomial noise values also follows
a binomial distribution. However, compared to the continuous Gaussian distribution, existing the-
oretical tools for analyzing binomial noise aggregation leads to rather loose bounds; further, the
bionomial distribution is incompatible with the moment accountant analysis technique in DPSGD
(Kairouz et al., 2021). Consequently, cpSGD leads to poor utility, as demonstrated in Section 4.
Recently, the distributed discrete Gaussian mechanism (DDG) (Kairouz et al., 2021) addresses the
above issues by injecting independent discrete Gaussian noise (Canonne et al., 2020) to the gradients
at each participant. Similar to the binomial distribution, the discrete Gaussian distribution is also
defined over an integer domain; meanwhile, DDG is fully compatible with the moment accountant
analysis technique in DP-SGD, and, thus, enjoys the tight privacy cost analysis. However, the dis-
crete Gaussian distribution is not aggregatable, meaning that the sum of noise drawn from multiple
i.i.d. discrete Gaussian distributions does not follow another discrete Guassian distribution, which
renders analysis difficult under in the decentralized setting of FL, and leads to looser bounds in the
privacy analysis. Further, the privacy guarantee of the aggregated noise in DDG degrades linearly
with the dimensionality d of the gradients, leading to poor scalablity to large neural networks.
Our contribution. In this work, we propose a new mechanism for enforcing distributed differential
privacy for federated learning: the distributed Skellam mechanism (DSM), which injects random
noise drawn from the symmetric Skellam distribution. Although the Skellam distribution has been
2
Under review as a conference paper at ICLR 2022
used before in the DP literature (Valovich & Alda, 2017), the privacy analysis therein does not cover
the decentralized setting of FL, or the iterative, sampling-based SGD algorithm, both of which
require highly non-trivial mathematical analysis.
Specifically, We prove that DSM satisfies both Renyi-DP and (e, δ)-DP, defined in Section 2. Similar
to our competitor DDG described above, DSM is compatible with the DPSGD framework and
its moment accountant analysis technique, leading to tight bounds on the privacy loss analysis.
MeanWhile, unlike DDG, the privacy guarantees of DSM are independent of the dimensionality
of the gradients, Which scales Well to large models. Further, similar to the continuous Gaussian
distribution (and unlike the discrete Gaussian distribution in DDG), i.i.d. Skellam noise values can
be aggregated to form an amplified noise that still folloWs the Skellam distribution, Which leads to
clean and elegant proofs in the decentralized setting of FL, and tight bounds in the privacy analysis.
We apply DSM to federated learning With distributed SGD, With quantized gradients, e.g., as re-
quired by the MPC protocol, and present the complete training algorithm. Extensive experiments
using benchmark datasets shoW that our solution leads to consistent and significant utility gains over
its competitors, under a variety of settings With different privacy and communication constraints.
2	Preliminaries
A random variable Y folloWs a Poisson distribution of parameter λ if its probability distribution is
Pr[Y = k] = exp(-λ)λ , k = 0,1, 2,.... Both the mean and variance of Y is λ. A random variable
Z folloWs a Skellam distribution ifit is the difference betWeen tWo independent Poisson variables Y1
and Y2 . In this Work, We restrict our attention to the case Where Y1 and Y2 have the same parameter
λ. In that case, the probability distribution of Z is
Pr[Z = k] = exp(-2λ)I∣k∣ (2λ), k = 0, ±1, ±2,...,	(1)
where Iv(u)，P∞=o 加「(九+@+1)(2)2h+v is the modified Bessel function of the first kind. We
Write that Z 〜Sk(λ, λ). By linearity of expectation, Z has mean 0 and variance 2λ.
We say that two datasets X and X0 are neighboring if one can be obtained by adding or removing
one tuple from the other. The main idea of differential privacy (DP) is to ensure that the outcomes
of a randomized mechanism on neighboring datasets are always similar; intuitively, this provides
plausible deniability on whether a given data record x belongs to the dataset X or not, and, thus,
protects the privacy of the individual whose record is x. A classic definition of differential privacy
is (, δ)-DP (Dwork et al., 2006), as follows.
Definition 1 ((, δ)-Differential Privacy (Dwork et al., 2006)). A randomized mechanism M satis-
fies (, δ)-differential privacy (DP) if
Pr[M(X) ∈O] ≤ exp(e) ∙ Pr[M(X0) ∈O]+ δ,	(2)
for any set of output O ⊆ Range(M) and any neighboring datasets X and X0.
Note that (, δ)-DP can be considered as a worst-case privacy guarantee for a mechanism, as it
enforces an upper bound on the probability ratio of all possible outcomes. An alternative definition
called Renyi-DifferentiaI Privacy (RDP) (Mironov, 2017), which is built upon the concept OfRenyi
Divergence, considers the average case privacy guarantee instead.
Definition 2 (Renyi Divergence (van Erven & Harremoes, 2014)). Assuming that distributions P
and Q are defined over the same domain, and P is absolute continuous with respect to Q, then the
Renyi divergence of P from Q of finite order α ∈ (0,1) ∪ (1, ∞) is defined as:
Da(P kQ) = ^TlOg EX 〜P
α-1
〕标r]
(3)
where we adopt the convention that 0 = 0 and 0 = ∞ for any y > 0, and the logarithm is with
base e.
Definition 3 (Renyi Differential Privacy (Mironov, 2017)). A randomized mechanism M satisfies
(α, τ)-Reenyi differential privacy (RDP) ifDα(M(X)kM(X0)) ≤ τforall neighboring datasets X
and X0.
3
Under review as a conference paper at ICLR 2022
Given a function of interest, the canonical way make it differentially private is to perturb its out-
come through noise injection. Roughly speaking, the scale of the noise should be calibrated to the
sensitivity of the function of interest (Dwork et al., 2006), formally defined as follows.
Definition 4 (Sensitivity). The sensitivity S(F) of a function F : D → Rd, denoted as S(F), is
defined as
S(F) = max0kF(X)-F(X0)k,
X〜X 0
where X 〜X0 denotes that X andX0 are neighboring datasets, and ∣∣∙k is a norm.
In particular, injecting continuous Gaussian noise sampled from N (0, σ2) to each dimension of
function F satisfies (α, αSσ(F) )-RDP (Mironov, 2017), where S(F) stands for the L? sensitivity
of function F. In many applications, we also need to analyze the overall privacy guarantee of a
mechanism consisting of multiple components (e.g., training neural networks with SGD). We have
the following composition and sub-sampling lemmata for RDP mechanisms.
Lemma 1 (Composition Lemma (Mironov, 2017)). If mechanisms M1, . . . , MT satisfies
(α, τ1), . . . , (α, τT)-RDP, respectively, then M1 ◦ . . . ◦ MT satisfies (α, PtT=1 τi)-RDP.
Lemma 2 (Subsampling Lemma (Zhu & Wang, 2019; Mironov et al., 2019)). Let M be a mech-
anism that satisfies (l, τ (l))-RDP for l = 2, . . . , α (α ∈ Z, α > 2), and Sq be a procedure that
uniformly sample each record of the input data with probability q. Then M ◦ Sq satisfies (α, τ)-
RDP with
1α
——7 log (1 — q)α-1(αq — q — 1) + £
α-1
l=2
q)α-lqle(l-1)τ(l)
Finally, any mechanism that satisfies (α, τ)-RDP also satisfies (, δ)-DP, for values of and δ as
follows.
Lemma 3 (Converting (α, τ)-RDP to (, δ)-DP (Canonne et al., 2020)). For any α ∈ (1, ∞), if
Dα (M(X)∣M(X 0)) ≤ τ for any neighboring databases X andX0, then M satisfies (, δ)-DPfor
e = T +log(1∕δ) + (α - 1)lo,- 1/a)- log(α)
α-1
(4)
3	Distributed S kellam Mechanism
Section 3.1 first formalizes the problem of distributed sum estimation, and presents the proposed
distributed Skellam mechanism (DSM) for this problem. Section 3.2 establishes the privacy guar-
antees of DSM. Then, in Section 3.3, we apply DSM to our main problem setting: differentially
private federated learning.
3.1 Formal Description
Suppose that an integer-valued multi-dimensional dataset X = (x1, . . . , xn) is distributed to n
individuals (referred to as clients in the following), where client i possesses data point xi ∈ Zd, for
i = 1, . . . , n. Without loss of generality, we assume that the L2 and L∞ norms of each xi is bounded
by constants ∆2 and ∆∞, respectively, i.e., ∣xi ∣2 ≤ ∆2 and ∣xi ∣∞ ≤ ∆∞ for any xi ∈ X. An
untrusted server aims to compute the (approximate) sum of the dataset, i.e., X = Pn=ι Xi, from
the clients. The computation of X must satisfy differential privacy. In particular, We focus on the
RDP definition (Definition 3), which can be converted to the classic (, δ)-DP (Definition 1) through
Lemma 3.
Specifically, we aim to design a private mechanism M, such that for all neighboring datasets X, X0,
Dα(M(X)∣M(X0)) ≤ τ,	(5)
for some α > 1. We measure the error ofM by
ErrM = max E M(X) - X X ,	(6)
X⊂Z	x∈X	2
4
Under review as a conference paper at ICLR 2022
Algorithm 1: Distributed Skellam mechanism
Input: Private dataset X = (xι,..., Xn). Noise parameter λ. Secure aggregation protocol A(∙).
Output: x, a private estimation of Pχ∈χ x.
1	for xi ∈ X do
2	Lxi <—— Xi + Z, where Z = (Z1,..., Zd) is sampled as in Eq. (7).
3	X <——A(Xι,..., Xn); secure aggregation.
where the expectation is taken over the randomness in M.
Next, we present the proposed solution DSM for the above distributed sum estimation problem,
outlined in Algorithm 1. Each client i first independently samples a noise vector following the
Skellam distribution, and injects this noise to her data (Line 2 in Algorithm 1). In particular, the
client samples a random d-dimensional noise vector Z = (Z1, . . . , Zd), where each element is
identically and independently distributed, and follows the Skellam distribution of parameter λ, i.e.,
Z 1,...,Zd i-d Sk(λ,λ).	⑺
We assume the clients have black-box access to a secure aggregation protocol A, following the
convention in (Agarwal et al., 2018; Kairouz et al., 2021). The clients then compute the sum of their
perturbed data using A, and release the sum to the server (Line 3 in Algorithm 1).
Modular Arithmetic with communication constraints. When each element xi in X is represented
using a limited number B of bits, each client needs to perform an extra step of modulo operation
depending on B after injecting Skellam noise (Line 2), so as to avoid overflows. Accordingly, after
obtaining the sum (Line 3), the server also needs to unwrap the modulo operation to recover the
original noisy sum. We refer the reader to (Kairouz et al., 2021) for a detailed discussion on this
issue. In our experiments in Section 4, we demonstrate that the proposed solution DSM achieves
significant accuracy gains compared to existing approaches with various settings of the bit limit B .
3.2	Theoretical Analysis
The following theorem states the privacy guarantee for Algorithm 1, which is the main theoretical
result of this paper.
Theorem 1 (Privacy guarantee of the distributed Skellam mechanism). When 1 < α < ∆nλ + 1,
∆∞
Algorithm 1 with noise parameter λ satisfies (α, T)-Renyi Differential Privacy with
丁 _ 1.09α + 0.91	∆
2nλ
(8)
2
Proof. To prove the privacy guarantee of Algorithm 1, it suffices to derive the privacy guarantee of
the sum of {Skλ(xi)}in=1. Note that the secure aggregation protocol A (used as a black-box) ensures
that only the sum of the perturbed data is revealed to the server. In addition, according to properties
of the Skellam distribution,aggregating n independent Skellam noise vectors Z 〜Sk(λ, λ) results
in a single Skellam noise Z 〜Sk(nλ, nλ) (Skellam, 1946). Hence, We only need to focus on the
privacy guarantee achieved by Z. Further, in what follows, we focus on the one dimensional case of
the problem, i.e., d = 1. The proof in the general case where d > 1 then follows from the additivity
of Renyi divergence for product distributions (van Erven & Harremoes, 2014).
Without loss of generality, we let S > 0, Z 〜Sk(λ,λ) and compute Φ = exp((α-1)Dα(Z+s∣∣Z)).
We first present several useful inequalities, whose proofs can be found in the appendix. Let
Qs,t(v) := YYV + +∖ + i)2+ 串
i=1
(9)
According to Lemma 4 in the appendix, we have:
II+I) ≤ Qs，t(V).
(10)
5
Under review as a conference paper at ICLR 2022
In addition, for any 0 < a ≤ w, according to Lemmata 5-7 in the appendix, we have
w + √W2÷1
≤ e2w-α
(a - w) + ʌ/(a — w)2 + 1	,
and (W + pw2 + 1) ∙ ((a — w) + p(a — w)2 + 1) ≤ ea.
By the definition of Skellam distributions and Eq. (9)
Φ= XX (JZ|(j：))Y 1 Pr[Z = z]	≤ XX (Qs,2λ(z))ατ Pr[Z = z]
z— "∣z+s∣(2λ)/	z—
(11)
(12)
X (Qs,2λ(z))α-1 Pr[Z = z] + X ((Qs,2λ(z))α-1 + (Qs,2λ(-z))α-1) Pr[Z = z].
∣z∣≤s	z>s
We compute
(QS,2λ(Z))α-1 + (Qs,2λ(-z))a-1 = (QS,2λ(Z)QS,2入(-2)产∙(凡,21(Z) + R⅛) J
where
Rs,2λ(z)：
a-1
(Qs,21(Z) J F
IQS,2λ(T)J
By Eq. (11), when Z ≥ i,
z + i + P(Z + i)2 + (2λ)2	_ z⅛i + q/(z2λi)2 +1	Z
/	: —	/	< exp( 丁).
-z + i +，(-z + i)2 + (2λ)2	-z+i + J( -z+i )2 +1	λ
Therefore, when Z ≥ s, we have『,：；?))< exp (笺),and RS,2λ(z) < exp (S(O2λ1)z). Since the
function v → v + ɪ is increasing,
RS,2λ (Z) + R⅛) < exP (⅛h) + exP (-⅛h).
By Eq. (12), when Z ≥ i,
Hence, when Z ≥ s, we have Qs,21(z)∙Qs,21(-z) < exp (S(I+1)). Meanwhile, when-S < z < s,
according to Lemma 8 in the appendix, we have:
Qs,21(z) ≤ exp
(".
Therefore,
φ ≤ exp (S(S+『))∙ PZ=S (exp(⅛nz) + exp(-⅛)z)) Pr[Z = z]
+ Ps二S+1 exp (S(a-2L ) Pr[Z = z]
≤ exp () PZ=-∞ exp (") Pr[Z = z]
= exp (⅛li) E [exp (⅛≡)].
The moment generating function of Sk(λ, λ) is E[etz] = eλ(et+e t-2). When 0 < t < 1, we have
et + e-t - 2 < 1.09t2. Thus, when λ > S(O-D,
E [exp (s⅛z)] ≤ exp (1.09λ(⅛))2) = exp (1.0*\1)2).
As a result, when λ > S(O-I), we have Φ ≤ exp (S ⅞-D + L09s4zI-D ).	□
6
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
Algorithm 2: Federated learning with distributed Skellam mechanism
Input: Private dataset X = (x1, . . . , xn). Sampling parameter q. L2 norm clipping bound C > 0.
Quantization granularity γ. Conditional rounding parameter k. Noise parameter λ. Black-box
secure aggregation protocol A(∙). Initial model parameters θ. Number of iterations T.
Output: θ model parameters learnt on X .
for h ∈ 1..T do
The server shares the current model parameters θ to all clients.
C u.a.r {1, 2,...,n} ;	// sample a subset of clients uniformly at random from all clients using
Poisson sampling with rate q.
for i ∈ C do
gi <—— Vθ(xi);	// gradient computation.
gi <—— Cliρ(gi, C);	// gradient clipping based on L2 norm.
gi <——gi/γ;	// quantization.
gi i——ROund(gi, k); // conditional rounding.
g i——Distributed Skellam mechanism ({gi}i∈c, λ, A(∙)).
g i——Y ∙ g;	// re-scaling
θ <——Update(θ, g);	//model update based on the approximate gradient sum
Note that the privacy guarantee of DSM in Theorem 1 is independent of the data dimensionality d.
Next we analyze the error incurred by Algorithm 1, which has the following bound:
Corollary 1 (Error bound of the distributed Skellam mechanism). With 1 < α < 2∆nλ + Lthe error
∆∞
incurred by Algorithm 1 that satisfies (α, τ)-RDP is bounded by
ErrM ≤
1.09α + 0.91 d∆22
2
τ
The proof for the above corollary is omitted for brevity, which directly follows from Theorem 1 and
the linearity of expectation. Note that the error bound in the above corollary is comparable with (i.e.,
within a constant factor) that of adding continuous Gaussian noise in the centralized setting, which
is (α -1) ∙ d∙ ∆2/(2τ) (Mironov, 2017). Meanwhile, since Algorithm 1 satisfies RDP, by Lemmata 1
and 2, it allows the tight privacy accounting in DPSGD through composition and subsampling. Note
that although the above analysis restricts the value of a to 1 < α < 2nλ∕∆∞ + 1, this constraint
only affects the utility of DSM, not its privacy guarantees. Further, in practice, n (i.e, the number
of training data points) is usually much larger than ∆∞, leading to a large range of values for α. In
addition, the optimal α (order of RDP) is often relatively small (e.g. less than 10) in our experiments,
and the above range represents a sufficiently large space for tuning α.
Comparing the theoretical results DSM with those of existing solutions described in Sec-
tion 1, cpSGD (Agarwal et al., 2018) does not satisfy RDP, and does not have the cor-
responding composition and sub-sampling properties that are crucial for tight privacy anal-
ysis. Meanwhile, DDG (Kairouz et al., 2021) ensures (α,τ0)-RDP with τ0 = 2 ∙
min(1 ∙ g + 2 P ∙ d, (√n ∙ ∆ + P ∙ √d) ) when each client independently injects discrete
Gaussian noise of variance σ2 to her data, where P is defined as P := 10 ∙ £"-： exp(-2∏2σ2 ∙ k^).
Note that the above τ0 term increases with the data dimensionality d, which is not scalable for large
d. In contrast, the privacy guarantee of DSM is independent ofd. These advantages of DSM lead to
significant utility gains in our target application: FL under differential privacy, described next.
3.3	Federate Learning with Distributed S kellam Mechanism
We apply the proposed DSM described in Algorithm 1 to enforce DP on federated learning with
distributed SGD, assuming that the clients have access to a black-box secure aggregation protocol.
Algorithm 2 outlines the training process. In each iteration, the server releases the current model
parameters to all clients (Line 2). Then, DSM randomly selects a subset of clients, whose identities
are not known to the server (Line 3). Each client in the selected subset then computes the gradients
based on the current model weights and her own data (Line 5), and performs gradient clipping (Line
6), which is a standard step introduced in DPSGD to bound the sensitivity of deep learning with
7
Under review as a conference paper at ICLR 2022
DP. After that, the client quantizes the gradients (Line 7), as required by typical MPC protocols for
gradient aggregation. Here, a parameter γ controls the quantization granularity: a smaller γ leads
to more precise quantization, and vice versa. Subsequently, the client randomly rounds the gradient
to the integer grid Zd (Line 8, explained soon). Next, we apply the distributed Skellam mechanism
(Algorithm 1) to the integer-valued gradients (Line 9). Finally, the server re-scales the approximate
gradient sum by a factor of γ (Line 10) and updates the shared model accordingly (Line 11). We omit
additional details on the updating process (e.g., learning rate schedule, weight decay, etc.) as they
do not affect the general framework or the privacy guarantees. After repeating the above process for
T iterations, the training terminates, and the server obtains the final model weights θ.
Next we clarify the conditional rounding step (Line 8 of Algorithm 2). This step is necessary since
after gradient clipping (Line 6) and quantization (Line 7), we still have real-valued gradients, which
are not compatible with the MPC protocol (i.e., the black-box secure aggregation protocol A used in
DSM). To explain the conditional rounding in Line 8, we first define an element-wise unconditional
rounding process as follows: for input x ∈ R, x is rounded to f loor(x) with probability ceil(x) -
x, and to ceil(x) otherwise. The problem with this unconditional gradient rounding process is
that it introduces an additional O(d) term on the L2 sensitivity of the resulting integer gradients.
To mitigate this issue, Kairouz et al. (2021) propose to repeatedly run the unconditional rounding
algorithm until the L2 norm of the rounded result satisfies a pre-defined condition. The stopping
condition in (KairoUz et al., 2021) is rather complex, and increases the sensitivity of the resulting
gradients by a factor of O(√d). Algorithm 2 adopts this general idea, but sets a much simpler
stopping condition than the one in DDG, as follows: the rounded gradient’s L2 norm must not
exceed a pre-defined constant k (e.g., 5 in our experiments) times C∕γ, where C is the L2 clipping
bound in Line 6, and γ is the quantization granularity in Line 7. With this stopping condition, the
increase in L2 sensitivity in Line 8 is a constant independent of the gradient dimensionality d.
Regarding privacy guarantees of Algorithm 2, observe that each iteration of the training process can
be seen as running the distributed Skellam mechanism on a random subset of rounded gradients.
This is because the model sharing (Line 2), gradient sum re-scaling (Line 10), and model updating
(Line 11) do not incur any additional privacy loss, as the updated model can be reconstructed by
the re-scaled perturbed gradient sum, which, in turn, can be computed from the perturbed gradient
sum. In addition, since the identities of the random subset of clients are not known to the server,
the privacy guarantee benefits from amplification by subsampling. (We refer the reader to (Kairouz
et al., 2021) for a detailed discussion on this issue.) Hence, the privacy guarantee of Algorithm 2
follows by applying the composition lemma (Lemma 1) and the amplification lemma (Lemma 2)
on the privacy analysis of DSM (Theorem 1). A formal statement of the privacy guarantees of
Algorithm 2 is presented in Theorem 2 in the appendix.
4	Experiments
We evaluate the performance of the proposed solution DSM (Algorithm 2) on two classic benchmark
datasets: MNIST (Lecun et al., 1998) and Fashion MNIST (Xiao et al., 2017), which contains
grayscale images of handwritten digits and clothing, respectively. Both datasets represent 10-class
classification tasks with 60, 000 training data records. We regard each data record in the training
data as a client. For both tasks, we train a three-layer neural network with fully connected layers
and ReLu activation, following previous work (Agarwal et al., 2018). We set the number of neurons
per layers to 80, resulting in a model with d = 63, 610 weights. For DDG and DSM, we use the
same L2 clipping norm (C = 1) and quantization granularity (γ = 0.1). In addition, for cpSGD,
we apply the tighter privacy analysis presented in (Koskela et al., 2021), which leads to improved
prediction accuracy. For all experiments, we use the Adam optimizer (Kingma & Ba, 2015) with
learning rate η = 0.005. We do not tune the hyper-parameters in favor of any particular solution and
omit additional experiments on hyper parameter tuning. We remark that our approach is compatible
with existing differentially private parameter tuning techniques (Gupta et al., 2010; Liu & Talwar,
2019), which is an orthogonal topic to this paper.
Our evaluation uses the (, δ)-DP (Definition 1) definition instead of RDP (Definition 3, since a
competitor cpSGD supports the former but not the latter. We fix δ to 10-5, and vary the privacy
parameter from {1, 2, 3, 4, 5}, batch size m from {120, 240, 480, 960}. The model is trained for
1 epoch, i.e., when m equals to 120, 240, 480, and 960, we train the model for 500, 250, 125, and
8
Under review as a conference paper at ICLR 2022
% ycarucca tse
(a) Varying
% ycarucca tse
(b) Varying B
MNIST, = 3, B ∈ {12, 32}.
100 H-：
90，■ ——
80 Il—
70 1-----
60
50
40 -
30 jfc--''∙
20
10
0
120
-e--
ʃʃςΘi∙
• 一 no noise
-♦ DPSGD (centralized)
Skellam (B = 12)
DDG (B = 12)
cpSGD (B = 32)
240	480
m
960
(c) Varying m

Figure 1:	Evaluations on MNIST with varying privacy parameter , bit limit per weight B, and batch
size m.
Fashion MNIST, B ∈ {12, 32}, m = 120.
100
90 -
100
90 -
% ycarucca tse
80 .
70 I p
60什
50
-Q--
-θ--
40
30 -
20 4 L
10
• 一 no noise
-♦ DPSGD (centralized)
Skellam (B = 12)
DDG (B = 12)
CpSGD (B = 32)
10
0
6
80 -
70
60
50
40
30
20
Fashion MNIST, = 3, m = 120.
% ycarucca tse
8
10
12
32
B
(b) Varying B
一no noise
-Dp DPSGD (centralized)
—Skellam
-θ-- DDG
—A- CpSGD
% ycarucca tse
(c) Varying m
(a) Varying
0
1

2
3
4
5
Figure 2:	Evaluations on Fashion MNIST with varying privacy parameter , bit limit per weight B,
and batch size m.
62 rounds, respectively. For (DDG) and our solution DSM, we also vary the bit limit per weight B
from {6, 8, 10, 12} bits; for cpSGD, we fix B to 32. This is because the amount of noise injected by
cpSGD is rather large, and smaller values of B lead to very poor model utility. Note that a large B
leads to higher communication costs between the clients. We report the average test accuracy over
5 runs. The results are shown in Figures 1 and 2. In addition, we have also included the non-private
baseline and the centralized-DP baseline DPSGD (Abadi et al., 2016b) in the figures.
Overall, the proposed solution DSM (marked as “Skellam” in the figures) significantly outperforms
its competitors cpSGD and DDG on all settings. In particular, the accuracy improvement over
DDG is consistently around 15% and 10% for MNIST and Fashion MNIST, respectively. From Fig-
ures 1(a) and 2(a), we observe that the performance gap expands as decreases, which corresponds
to stronger privacy protection and requires higher noise levels. According to Figures 1(b) and 2(b),
our solution is the only mechanism that achieves acceptable accuracy when the number of bits B is
as low as 8 per weight, which corresponds to a stringent constraint on the communication costs in
the underlying MPC protocol. The test accuracy obtained with cpSGD is rather low (no more than
30%), even though it is given a generous value of B = 32. Finally, according to Figures 1(c) and
2(c), the accuracy for all methods remains stable with varying batch size m.
5 Conclusion
In this work, we propose the distributed Skellam mechanism (DSM), a novel solution for enforcing
differential privacy on machine learning models built through an MPC-based federated learning pro-
cess using distributed stochastic gradient descent. Compared to existing solutions, DSM achieves
privacy guarantee that is independent of the dimensionality of the weight vector, while allowing
tight privacy accounting due to its nice composition and sub-sampling properties. We conduct ex-
tensive experiments on two classic benchmark datasets and various practical settings, and the result
demonstrate the consistent and significant accuracy gains DSM over existing solutions.
Regarding future work, we plan to further reduce the constant factor in the privacy analysis for DSM
to improve model utility under the same level of privacy protection. Another promising direction is
to open up the black box of the MPC protocol and perform careful privacy analysis with consider-
ations for the details of MPC protocol, which might help lower the noise level further, leading to a
more favorable privacy-utility trade-off for federated learning.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
We provide our source code for our distributed Skellam mechanism implementation for re-
viewing purposes, available at https://anonymous.4open.science/r/Distributed_
Skellam_Mechanism. Our implementation simulates federated learning with a single machine.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-
scale machine learning. In OSDI, pp. 265-283, 2016a.
Martin Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep learning with differential privacy. In CCS, pp. 308-318, 2016b.
Naman Agarwal, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, and H. Brendan McMahan.
Cpsgd: Communication-efficient and differentially-private distributed sgd. In NeurIPS, pp.
7575-7586, 2018.
Naman Agarwal, Peter Kairouz, and Ziyu Liu. The skellam mechanism for differentially private
federated learning, 2021.
Prabhanjan Ananth, Arka Rai Choudhuri, Aarushi Goel, and Abhishek Jain. Round-optimal secure
multiparty computation with honest majority. In CRYPTO, pp. 395-424, 2018.
Laforgia Andrea and Natalini Pierpaolo. Some inequalities for modified bessel functions. Journal
of Inequalities and Applications, 2010.
Amos Beimel, Ariel Gabizon, Yuval Ishai, Eyal Kushilevitz, Sigurd Meldgaard, and Anat Paskin-
Cherniavsky. Non-interactive secure multiparty computation. In CRYPTO, pp. 387-404, 2014.
James Henry Bell, Kallista A. Bonawitz, Adria Gascon, Tancrede Lepoint, and Mariana Raykova.
Secure single-server aggregation with (poly)logarithmic overhead. In CCS, pp. 1253-1269, 2020.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In CCS, pp. 1175-1191, 2017.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In NeurIPS, pp. 1877-1901, 2020.
Cleiment L. Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. In NeurIPS, 2020.
Nicholas Carlini, Chang Liu, Ui lfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:
Evaluating and testing unintended memorization in neural networks. In SEC, pp. 267-284, 2019.
David Chaum, Ivan Damgard, and Jeroen van de Graaf. Multiparty computations ensuring privacy
of each party’s input and correctness of the result. In CRYPTO, pp. 87-119. Springer, 1987.
Adam Coates, Brody Huval, Tao Wang, David J. Wu, Bryan Catanzaro, and Andrew Y. Ng. Deep
learning with COTS HPC systems. In ICML, pp. 1337-1345, 2013.
Ronald Cramer, Ivan Damgard, and Jesper Buus Nielsen. Secure Multiparty Computation and Secret
Sharing. Cambridge University Press, 2015.
10
Under review as a conference paper at ICLR 2022
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. Large
scale distributed deep networks. In NeurIPS, pp. 1232-1240, 2012.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In TCC, pp. 265-284, 2006.
Cynthia Dwork, Adam D. Smith, Thomas Steinke, Jonathan R. Ullman, and Salil P. Vadhan. Robust
traceability from trace amounts. In FOCS, pp. 650-669, 2015.
Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In STOC, pp.
954-959, 2020.
Rosario Gennaro, Yuval Ishai, Eyal Kushilevitz, and Tal Rabin. On 2-round secure multiparty
computation. In CRYPTO, pp. 178-193, 2002.
Slawomir Goryczka, Li Xiong, and Vaidy Sunderam. Secure multiparty aggregation with differential
privacy: A comparative study. In Joint EDBT/ICDT 2013 Workshops, pp. 155-163, 2013.
Anupam Gupta, Katrina Ligett, Frank McSherry, Aaron Roth, and Kunal Talwar. Differentially
private combinatorial optimization. In SODA, pp. 1106-1125, 2010.
Yuval Ishai, Eyal Kushilevitz, and Anat Paskin. Secure multiparty computation with minimal inter-
action. In CRYPTO, pp. 577-594, 2010.
Peter Kairouz, Ziyu Liu, and Thomas Steinke. The distributed discrete gaussian mechanism for
federated learning with secure aggregation. In ICML, pp. 5201-5212, 2021.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Antti Koskela, Joonas Jalko, LUkas Prediger, and Antti Honkela. Tight differential privacy for
discrete-valued mechanisms and for the subsampled gaussian mechanism using FFT. In Arindam
Banerjee and Kenji Fukumizu (eds.), AISTATS, volume 130, pp. 3358-3366. PMLR, 2021.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In STOC, pp. 298-309,
2019.
Ryan T. McDonald, Keith B. Hall, and Gideon Mann. Distributed training strategies for the struc-
tured perceptron. In HLT-NAACL, pp. 456-464, 2010.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgUera y Areas.
Communication-efficient learning of deep networks from decentralized data. In AISTATS, pp.
1273-1282, 2017.
LUca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting Unintended
featUre leakage in collaborative learning. In S&P, pp. 691-706, 2019.
Ilya Mironov. Renyi differential privacy. In CSF, pp. 263-275, 2017.
Ilya Mironov, Kunal Talwar, and Li Zhang. Renyi differential privacy of the sampled gaussian
mechanism. CoRR, abs/1908.10530, 2019.
Milad Nasr, Reza Shokri, and Amir HoUmansadr. Comprehensive privacy analysis of deep learning:
Stand-alone and federated learning Under passive and active white-box inference attacks. In S&P,
pp. 739-753, 2019.
Pascal Paillier. PUblic-key cryptosystems based on composite degree residUosity classes. In EURO-
CRYPT, pp. 223-238, 1999.
Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. Knock knock, who’s there?
membership inference on aggregate location data. In NDSS, 2018.
11
Under review as a conference paper at ICLR 2022
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In S&P, pp. 3-18, 2017.
J. Gordon Skellam. The frequency distribution of the difference between two poisson variates be-
longing to different populations. Journal of the Royal Statistical Society, 109:296-296, 1946.
Congzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In
KDD, pp. 196-206, 2019.
Congzheng Song and Vitaly Shmatikov. Overlearning reveals sensitive attributes. In ICLR, 2020.
Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine learning models that remem-
ber too much. In CCS, pp. 587-601, 2017.
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi Zhou. A hybrid approach to privacy-preserving federated learning - (extended abstract). In-
form. Spektrum, 42(5):356-357, 2019.
Filipp Valovich and Francesco Alda. Computational differential privacy from lattice-based cryptog-
raphy. In NuTMiC, pp. 121-141, 2017.
Tim van Erven and Peter Harremoes. Renyi divergence and kullback-leibler divergence. IEEE
Trans. Inf. Theory, 60(7):3797-3820, 2014.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. CoRR, abs/1708.07747, 2017.
Andrew Chi-Chih Yao. How to generate and exchange secrets (extended abstract). In FOCS, pp.
162-167, 1986.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learn-
ing: Analyzing the connection to overfitting. In CSF, pp. 268-282, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Commun. ACM, 64(3):107-115, February
2021.
Yuqing ZhU and Yu-Xiang Wang. Poission subsampled Renyi differential privacy. In ICML, pp.
7634-7642, 2019.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proofs
Lemma 4. Let Qs,t(v) = Qs=I ""(v+i)*. Then, 二”Z) ≤ Qs,t(v)∙
Proof. By properties of the modified Bessel function of the first kind (Andrea & Pierpaolo, 2010),
we have inequality
Iv-1 (t)	V + √V2 + t2
Iv (t)	< t
v≥0
and recurrence relation
2V
Iv + 1(t) = Iv-1(t)--tIv (t)
which leads to
Iv+l(t) -V + √V2 +12	0
~Ufy	t , V ≥ .
When V <0, substituting V for -V - 1 in the previous formula,
Ilvl (t) = I-v ⑴
Ilv+1l (t)	I-v-1 (t)
When V ≥ 0, we also have
V + 1 +，(v + 1)2 + t
Ilvl (t) = Iv ⑴	<
Ilv+1l (t)	Iv+1(t)
Therefore, for every V ,
V +1 + Vz(V + 1)2 + t
Ilvl ⑴
I|v+s| (t)
s
<Y
i=1
V + i + VZ(V + i)2 +1
:= Qs,t (v).
Lemma 5. For any w ≥0,
W + Wν+ + 1 ≤ ew.
<
t
t
t
□
Proof. The inequality holds when W = 0. By taking the derivative, it can be verified that the function
ew - (w + √w2 + 1) is increasing with respect to w.	□
Lemma 6. For any 0 < a ≤ W,
W + √w2 + 1	≤ e2w-a
(a - w) +	(a - w)2 + 1
Proof. By Lemma 5, W + √w2 + 1 ≤ ew. Then, by Lemma 5 again,
-----------------------1	=(w - a) + P(W — a)2 + 1 ≤ ew-a.
(a — w) + p(a — w)2 + 1	-
□
Lemma 7. For any 0 < a ≤ W,
(w + pw2 + 1) ∙ ((a — w) + p(a — w)2 + 1) ≤ ea.
Proof. By Lemma 5, the inequality holds when w = a. By taking derivative, it can be verified that
the left hand side is decreasing with respect to w.	□
13
Under review as a conference paper at ICLR 2022
Lemma 8. When -s < z < s, we have
Qs,2λ(z) ≤ exp
Proof. Use Lemma 5. When 0 ≤ z < s,
Qs,2λ(z) ≤ Yexp (六)=exP (q⅛a) ≤ exP (H
When — 2 ≤ z < 0, using the equality
-b + √(-b)2 + (2λ)2
2λ
2λ
1,
we have
s
Qs,2λ(z) = Y
i=1
s
=Y
i=-2z
s
+ i + √(z + i)2 + (2λ)2
2λ
Z + i + √(z + i)2 + (2λ)2
2λ
When —s < z < — S, We have
≤ exp
i=-2z
z+i
2λ
exp
≤ exp
s(s + 2z — 1)
4λ
s(s + Z) ʌ
-2λ	).
-2z-s-1
Qs,2λ(Z) = Y
i=1
≤1
S s(s + Z)
≤exp
2λ
A.2 Privacy guarantee of Algorithm 2
Theorem 2 (Privacy guarantee of Algorithm 2). Let α ∈ Z, 2 < α < 2nCkγ + L Thenfor L? norm
clipping bound C, noise parameter λ, quantization granularity γ, conditional rounding parameter
k, sampling parameter q, and iterations T, Algorithm 2 satisfies (α, T)-Renyi Differential Privacy
with
Ο⅛og
(1 — q)α-1(αq — q —
q)α-lqle(l-1)τ(l)
(13)
d.
□
τ = T ∙
where Tl is defined as τι := 1.09l+0.91 ∙ CkY2, for l = 2,...,a.
We omit the proof for brevity, Which folloWs from our main theoretical result Theorem 1, as Well as
subsampling (Lemma 2) and composition (Lemma 1).
14
Under review as a conference paper at ICLR 2022
A.3 Overflowing in Modular Arithmetic
The key to avoid overflowing is to restrain the perturbed gradient weights within the range of the
finite integer filed, namely [-2B-1, 2B-1 - 1]. We first review the concentration property of the
symmetric Skellam distribution Z 〜Sk(λ,λ) of mean 0. Note that Z has MGF exp(λ(exp(λ) +
exp(-λ) — 2)). In addition, since X ∙ (exp(x) + exp(-x) — 2) ≤ 1.09x2 for 0 ≤ x < 1. We have
that Z is sub-exponential with parameters (√2.18,1). Namely,
E[exp(tZ)] ≤ exp(1.09t2), for |t| ≤ 1.	(14)
From here we obtain the tail bound for Z,
Pr[Z ≥ x] ≤ exp(-x/2), for x ≥ 2.18.	(15)
Hence, to avoid overflowing due to the Skellam noise, it suffices to set 2B ≥ h log d, where d is
the dimension of the gradient weights, and h is some constant to balance the communication cost
and the error due to overflowing. A relatively large h limits the error due to overflowing while
incurring a large communication cost; and vice versa. Note that a more precise reasoning can be
obtained by observing that Z approaches a normal distribution with mean 0 and variance 2λ, when
Z is of order √2λ. In fact, in our experiments, We observe that overflows are rare under a reasonable
communication constraint. To be more specific, no overflow was observed when the number of bits
per dimension reaches 10 or above, and very few overflows occured when the number of bits per
dimension is 8 (once every a thousand weights).
A.4 Concurrent work
Agarwal et al. (2021) also suggest using the Skellam noise in the distributed DP setting. Different
from our work, their privacy analysis for the Skellam noise is dependent on both the L2 norm and
L1 norm of the private input vector. Since the L1 norm of a high dimensional vector can be much
larger than its L2 norm, the privacy analysis we present is tighter. Further tightening the analysis is
a promising future direction.
15