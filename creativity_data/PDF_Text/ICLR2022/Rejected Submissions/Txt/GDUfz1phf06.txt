Under review as a conference paper at ICLR 2022
AutoNF: Automated Architecture Optimiza-
tion of Normalizing Flows Using a Mixture
Distribution Formulation
Anonymous authors
Paper under double-blind review
Ab stract
Although various flow models based on different transformations have been pro-
posed, there still lacks a quantitative analysis of performance-cost trade-offs be-
tween different flows as well as a systematic way of constructing the best flow
architecture. To tackle this challenge, we present an automated normalizing flow
(NF) architecture search method. Our method aims to find the optimal sequence
of transformation layers from a given set of unique transformations with three
folds. First, a mixed distribution is formulated to enable efficient architecture op-
timization originally on the discrete space without violating the invertibility of
the resulting NF architecture. Second, the mixture NF is optimized with an ap-
proximate upper bound which has a more preferable global minimum. Third, a
block-wise alternating optimization algorithm is proposed to ensure efficient ar-
chitecture optimization of deep flow models.
1	Introduction
Normalizing flow (NF) is a probabilistic modeling tool that has been widely used in density estima-
tion, generative models, and random sampling. Various flow models have been proposed in recent
years to improve their expressive power. Discrete flow models are either built based on elemental-
wise monotonical functions, named autoregressive flow or coupling layers (Papamakarios et al.,
2017), or built with transformations where the determinant of the flow can be easily calculated with
matrix determinant lemma (Rezende & Mohamed, 2015). In the continuous flow family, the models
are constructed by neural ODE (Grathwohl et al., 2019).
Despite the variety of flow models, there’s yet no perfect flow concerning the expressive power and
the computation cost. The flow models with higher expressive power usually have higher computa-
tional costs in either forward and inverse pass. In contrast, flows which are fast to compute are not
able to model rich distributions and are limited to simple applications. For instance, autoregressive
flows (Papamakarios et al., 2017) are universal probability approximators but are D times slower
to invert than forward calculation, where D is the dimension of the modeled random variable x
(Papamakarios et al., 2021). Flows based on coupling layers (Dinh et al., 2015; 2017; Kingma &
Dhariwal, 2018) have an analytic one-pass inverse but are less expressive than their autoregressive
counterparts. Other highly expressive NF models (Rezende & Mohamed, 2015; Behrmann et al.,
2019) cannot provide an analytic inverses and relies on numerical optimizations.
For different applications, the optimal flow model can be drastically different, especially if the com-
putation cost is taken into consideration. For generative models (Dinh et al., 2015; Kingma &
Dhariwal, 2018), flows with the fast forward pass are preferable since the forward transformations
need to be applied to every sample from the base distribution. For density estimation (Papamakarios
et al., 2017; Rippel & Adams, 2013), flows with cheap inverse will prevail. For applications where
flow is utilized as a co-trained kernel (Mazoure et al., 2020), the computation cost and performance
trade-off are more important, i.e., having a fast model with relatively good performance. However,
in the current body of work, the architecture designs of the flow models are all based on manual
configuration and tuning. To this date, there is a lack of a systematic way that could automatically
construct an optimal flow architecture with a preferred cost.
1
Under review as a conference paper at ICLR 2022
In this paper, we propose AutoNF, an automated method for normalizing flow architecture optimiza-
tion. AutoNF has a better performance-cost trade-off than hand-tuned SOTA flow models based on
a given set of transformations. Our approach employs a mixture distribution formulation that can
search a large design space of different transformations while still satisfying the invertibility require-
ment of normalizing flow. The proposed mixture NF is optimized via approximate upper bound
which provides a better optimization landscape for finding the desired flow architecture. Besides, to
deal with exponentially growing optimization complexity, we introduce a block-wise optimization
method to enable efficient optimization of deep flow models.
2	Related Work
Normalizing Flows: Various normalizing flow models have been proposed since the first con-
cept in (Tabak & Turner, 2013). Current flow models can be classified into two categories: finite
flows based on layer structure, and continuous flow based on neural ODE (Grathwohl et al., 2019).
The finite flow family includes flows based on elemental-wise transformation (Papamakarios et al.,
2017; Kingma & Dhariwal, 2018) and flows whose transformations are restricted to be contractive
(Behrmann et al., 2019). In elemental-wise transformation flows, autoregressive flow and coupling
layers are two major flavors and extensive work has been proposed to improve the expressive power
of both flow models. In Huang et al. (2018), the dimension-wise scalar transformation is imple-
mented by a sigmoid neural network, which increases the expressive power at the cost of being
not analytically invertible. In Durkan et al. (2019), piecewise splines are used as drop-in replace-
ment of affine or additive transformations (Dinh et al., 2015; 2017) and is the current SOTA flow
model. Consequently many recent research efforts have been devoted to closing the gap of expres-
sive power, albeit at the cost of more complex and expensive transformations. Moreover, there has
been no quantitative trade-off analysis between the performance and cost among different flows.
Neural Architecture Search: Many algorithms have been proposed or applied for neural architec-
ture search. For instance, reinforcement learning (Zoph & Le, 2017), genetic algorithm (Real et al.,
2017; Suganuma et al., 2018; Liu et al., 2018), Monte Carlo tree search (Negrinho & Gordon, 2017)
or Bayesian optimization (Kandasamy et al., 2018). However, these methods all face the challenge
of optimizing on a large discrete space and can take thousand of GPU days to find a good archi-
tecture. To address this issue, DARTS (Liu et al., 2019) proposes to relax the search space from
discrete to continuous and allows efficient differentiable architecture search with gradient method
which could reduce the search time to a single GPU day while still producing the SOTA architecture.
However, all current NAS methods focus on optimizing traditional neural network structures (CNN,
RNN) and there has yet been any implementation on normalizing flow.
Necessity for the Trade-off Between Performance and Cost: Despite various transformations
proposed in the literature, there is no perfect transformation with strong expressive power and low
computational cost. Autoregressive flows have better expressive power, but the inverse computa-
tion cost grows linearly with data dimension. Coupling layers’ inverse calculation is as fast as the
forward pass, but their expressive power is generally worse than autoregressive flow with the same
element-wise transformation. Even in the same autoregressive flow or coupling layer family, flows
with different element-wise transformations have different performance and computation costs. For
instance, additive or affine coupling layers (Dinh et al., 2017; 2015) have very fast forward and in-
verse calculation with limited expressive power while the flow in (Durkan et al., 2019) are highly
expressive but are more demanding on computation. In most applications, it is necessary to find the
best performance while minimizing at least one specific component of the cost. Unfortunately, the
current design of flow models is empirical and therefore cannot ensure the optimal trade-offs.
3	Method
In this work, we aim to tackle the challenge of finding an optimal flow model for a given task via an
automated architecture search algorithm.
Assumptions: In the remaining part of this paper, without losing generality, we assume that the
transformation is properly modeled such that during the training process, only forward computation
is needed. Under this assumption, when the flow model is used for density modeling (Durkan
2
Under review as a conference paper at ICLR 2022
et al., 2019), the forward calculation is the dominant computation. When the flow model is used for
random sampling (Kingma & Dhariwal, 2018), the inverse calculation is computationally intensive.
When the flow model is utilized as a module and trained together with other components, e.g., policy
network in maximum entropy learning (Mazoure et al., 2020), the training cost of the flow model is
an important consideration.
Problem Definition: Given a transformation set with m options {T1, T2, ...Tm}, the goal is to
construct an optimal flow model with n layers of transformations from the set. The flow model
pNF (x; θ) = pT1T2...Tn (x; θ) should minimize the KL divergence between the target distribution
p*(x) and itself while minimizing its computational cost CNF. Here, θ are the parameters of the
transformation in the flow model. In this paper, we use the forward KL divergence as our target loss
function (Papamakarios et al., 2021):
θ* =argmin {Dkl[p^ (x) || Pte…Tn(x; θ)] + λ ∙ Cnf}
θ
s.t.	Ti ∈ {T1, T2,...Tm}
(1)
While λ is a tuning factor capturing the relative importance of the performance-cost trade-off. Find-
ing this optimal flow model is a discrete optimization problem with exponential complexity. To
enable efficient architecture optimization, we use proposed method of relaxing the discrete search
space to continuous space as suggested in Liu et al. (2019).
3.1	Mixed Flow Ensemble
For the ith transformation layer with m options, we introduce a corresponding weight wij for each
option Tj which reflects how likely the transformation will be selected. The weight is parameterized
by a vector α and made continuous via softmax:
j
wi
eχp(αj)
Pj=I exp(αj)
(2)
By applying this parameterization for each transformation layer, we can construct a mixed flow
ensemble pMix(x; θ, α), where each layer in this mixed model reflects a weighted combination
of the effect of all possible transformations. In this case, the architecture optimization problem
is reduced to learning the weight vector for each layer and, at the end of the optimization process,
weights will be binarized and the transformation with the highest weight in one layer will be selected
as the final transformation. The mixed flow ensemble thus degrades to a normal flow model. The
whole procedure is illustrated in Fig. 1 (left).
As adopted in (Liu et al., 2019), training of the flow ensemble becomes joint optimization of the
architecture parameter α and the model parameter θ over the training and validation datasets, which
could be written as the following bi-level optimization problem:
α* = arg min DKL [p*(x) Il PMix(X; θ , a)] + λ ∙ C Mix (α)
α
s.t. θ* = argmin DKLin[p"(x) Il PMix(x； θ, α)],	(3)
θ
∀T ∈PMix, T ∈ {T1,T2,...Tm },
While the optimization problem is well defined, the key challenge is to construct the flow ensemble
within the normalizing flow framework. This is different from traditional neural architecture search,
which can mix various operations with no additional issue. Normalizing flow has its unique re-
quirement for the invertibility of transformations and a preferred simple Jacobian calculation, which
requires careful handling.
The mixed flow ensemble PMix(x; θ*, α) must satisfy two requirements. First, it must be a legal
density function such that it can be optimized by the KL divergence formulation. Second, each
transformation layer in PMix(x; θ*, α) should represent a weighted combination of all possible
transformations. Consider the ith layer in the mixed flow ensemble with input random variable xin
and output random variable xout, and Pxin (xin) and Pxout (xout) are their corresponding density
functions. This layer has m transformation options in {Ti1, Ti2, ...Tim } and wij is the corresponding
3
Under review as a conference paper at ICLR 2022
Figure 1: Left-top: the relaxation of search space and the flow ensemble is shown in Fig. 1. Left-
middle: binarization of weights. Left-bottom: degradation to normal flow architecture. Right-top:
construction flow ensemble by mixed transformations. Right-bottom: construction of flow ensemble
by mixing distributions. The blue line in right indicates transformation on random variables and the
orange line reflects change in distributions.
weight for each transformation. As discussed in Assumption, we assume all transformations directly
model the inverse transformation, i.e. xin = Tij (xout). Two approaches can be used to construct
the mixed flow ensemble.
Construction by Mixed Transformations: The straight forward way of building the ith mix flow
ensemble layer is to mix all transformations by weighted summation, as shown in Fig. 1 (right-top).
The final weighted transformation for this layer can be thus represented as:
m
Ti(Xin) = X Wj ∙ Tj(XoUt)	(4)
j=1
There are two drawbacks of this formulation despite its simplicity. First, definition of normaliz-
ing flow requires the mixed transformation Ti be invertible and differentiable in order to ensure
pxout (xout) legal density function. However, this invertibility is not guaranteed even if all candi-
date transformations are invertible. Second, even if the mixed transformation is invertible, there is
no easy way to calculate the Jacobian determinant of this weighted summation of transformations.
Meeting the requirement of invertibility and ease of calculating Jacobian determinant brings strict
restrictions on the candidate transformations and prevents the optimization of flow architectures on a
wider search space. As a result, the construction of the mixed flow ensemble by weighted summation
of transformations is not adopted in this paper.
Construction by Mixed Distributions: An alternating way is to build the mixed flow ensemble
by mixing distributions. For a given transformation Tij in this ith layer, applying the transformation
to the input random variable will result in a new distribution:
PTj (Xout)= PXin (Ti (Xout)) ∙ | det JTj (Xout) |	⑸
ii
By applying this to every transformation option in {Ti1, Ti2, ...Tim}, we can obtain k different distri-
butions, and it is possible to mix all the density functions together by their weighted summation, to
get a mixture model as shown in eq.(6).
m
PTi(Xout) = Ewj ∙ PTj (Xout)	⑹
j=1
4
Under review as a conference paper at ICLR 2022
An illustration of this process is shown in Fig. 1 (right-bottom). Different from the previous ap-
proach, the mixture model has a legal density function as: pTi (xout). By the definition of normal-
izing flow, we can assume that there exists an invertible and differentiable transformation Ti , which
transforms xin to xout , although the transformation itself can not be explicitly written out.
For the next (i + 1)th layer, the density of the mixture model will be used as the input density
function pxin (xin) as in the previous layer. By applying this formulation for n layers, the final
mixed flow ensemble can be written as:
mn	mn
pMix (x; θ, a) =	Wk ∙ PT1T2 …Tn (x, θ) = EWk ∙ Pi(x; θi)
k=1
n
where each Wk = wi
i=1
Each wi is defined in eq.(2) and we use pk(x; θk) to represent a “normal flow architecture” with n
transformation layers. Clearly, the final mixed flow ensemble is a legal density function which is in
fact, a weighted summation of all possible flow models built with n layers of transformations.
k=1
mn
(7)
and	Wk = 1
3.2	Optimization With Approximated Upper B ound
Optimizing the forward KL divergence between the target distribution and the mixed flow ensemble
can be written as:
LOMix = DKL [p* (X) || PMix(X； θ, α)]
n
mn	(8)
=-Ep*(x)[log(T Wk ∙ Pk(x; θk))]
k=1
We will demonstrate that direct optimization of this original loss can lead to underside mixture
models. In the whole search space of the flow ensemble, we are interested only in ”normal flow
architectures” points, i.e. the points where the weight of one architecture is 1 and others are all 0.
However, it can be easily proven that the global optimum of LpOMix may not be the desired normal
flow architecture (the red points in Fig. 2). Instead, optimization is very likely to end up in a mixture
model that is globally optimal with similar weight for each possible flow architecture (the green
point in Fig. 2). In this case, we will encounter difficulty when extracting a normal flow architecture
with the search result. A common practice in differentiable architecture search (Liu et al., 2019) is
to binarize the weights and select corresponding transformations. However, there is no guarantee
that the binarized architecture will have a lower loss, and finding this nearest binarization may lead
to performance drop. As a result, optimization with the original loss function is not suitable, and
could be risky.
Figure 2: An illustrative example of the original loss and upper bound for a flow ensemble with 2
possible architectures. The red points indicate desired normal flow architectures and the green point
indicates the global minimum of LpO , which is a mixture model. The parameters (a, b, θ1, θ2)
refer to the weight of architecture 1, architecture 2 and their corresponding parameters.
5
Under review as a conference paper at ICLR 2022
In this paper, we propose to optimize an upper bound of the original loss function to provide a
better global optimum for the search of best normal flow architectures. Our method utilizes Jensen’s
inequality log(P W ∙ x) ≥ P W ∙ log(χ) as follows, since We have P W = 1 and the log function
is concave, we can obtain an upper bound of the KL divergence given as:
mn	mn
LOMix= - Ep*(x)[log(X Wk ∙ Pk (x； θk)] ≤ LUMix= - EP*(X)[X Wk ∙ lOg(Pk (X； θk ))]⑼
kk
The benefit of optimizing the upper bound can be summarized as follows:
Proposition 1: The global minimum point of LpUMix is defined by a normal flow architecture.
Proof: Suppose each flow model Pk(x; θk) has an optimal parameter θ^ that minimizes the KL
divergence between p* (x) and it:
-	Ep*(x)[log(Pk(x; θk)] ≤ - Ep*(x)[log(Pk(x; θk)]
There also exists a flow architecture (pz (x; θZ)) that has the minimal KL divergence:
-	Ep*(x)[log(Pz(x; θ=)] ≤ - Ep*(x) [log(Pk(x; θk)], ∀k ∈ mn
We can then prove the proposition by showing that:
mn	mn
LUMix = - Ep*(x)[X Wk ∙log(Pk (x； θk))] ≥ - Ep*(χ)[X Wk ∙log(pk (x; θfc))]
kk
mn
≥ - Ep*(x)[X Wk ∙log(pz(x; θZ))] = - Ep*(x)[log(Pz(x； ΘZ)]
k
(10)
(11)
(12)
Proposition 2: At normal architecture points (Wk = 1, W-k = 0), LpU
LpOMix.
The proof of proposition 2 is apparent.
With the above propositions and under the assumption that the global optimum can be reached at the
end of the optimization, we can show that the solution set, i.e. all possible normal flow architectures
are the same in both LpOMix and LpUMix, and we can do optimization with proposed upper bound
without violating the original definition. Furthermore, since the global optimum of the upper bound
will always lead to a normal flow architecture, we will not end up in finding a mixture model with
the need to do heuristic and risky binarization of weights W.
3.3	Efficient Architecture Optimization for Deep Flow Models
While the flow ensemble by mixed density formulation could reflect the weighted effect of all pos-
sible transformation combinations, the architecture optimization complexity grows exponentially
with respect to the number of considered transformation types and the number of transformation
layers. In this scenario, efficient optimization of the whole flow architecture will not be possible. It
is natural to decompose the original problem into sequential optimization of few different blocks,
where each block could be optimized in one time with a limited number of layers. We propose two
methods to decompose the problem.
Grow Method: The first approach is a straightforward greedy method which we call ”Grow”.
Each time, a block is optimized until convergence, and the weights of the transformation layer are
binarized. The searched transformations in this block will be directly added to the searched layer
in the previous block. The architecture optimization of later blocks will be based on the existing
layers and, the growth of layers stops when reaching the total number of layers constraint. Despite
its simplicity, the downside of the “Grow” method is that the optimization is short-sighted. The
block being optimized has no information about the architectures which could be added later, and
the whole architecture is more likely to be trapped in local minimum.
Block Method: To avoid the issue of getting stuck in a local minimum, we propose another
method named “Block” optimization. Blocks B in this approach are optimized alternatively to
allow each block to adjust their architectures with respect to other blocks. In fact, the first “Grow”
approach is a specific case of the “Block” method, where all the blocks are initialized as identity
transformations and optimized only once.
6
Under review as a conference paper at ICLR 2022
Algorithm 1 Algorithm flow for AutoNF
Require: Transformations: {T 1,T2,…Tm}, Blocks: B = {B1,B2,…Bι}, Cost: CMix
Ensure: n-layer flow model:
1:	while not converged do
2:	for each Bi ∈ B do
3:	while not convergence do
4:	αBi =argmi□ɑB DKL[p*(x) Il PMix(x； θB,。石方 + λ ∙ CMix(αBi)
Bi
5:	Θb =argminθB DKLin[p*(x) Il PMix(x； Θb , obJ]
6:	end while
7:	Fix architecture for Bi
8:	end for
9:	end while
3.4	Cost Model and Algorithm Flow
As discussed in section II, we are interested in modeling the training cost (forward calculation cost)
and the inverse calculation cost, since each of them plays a different role based on desired ap-
plications. We use an independent experiment to model the cost of different types of flows and
summarized in a table which are included in Appendix B. With the cost model, the total cost of the
mixed flow ensemble could be extracted based on emphasize on different costs, e.g. if training cost
is the major concern, only training cost of different flows will be calculated. This total cost CMix is
then added as an regularization term into the training loss function.
In our paper, gradient based method is used for optimization which is efficient in this very high
dimensional search space. The architecture parameter α and the flow model parameter θ are opti-
mized alternatively with first order approximation in (Liu et al., 2019). The final algorithm flow of
our proposed AutoNF method can be summarized in Algorithm 1.
4	Experiments
4.1	Evaluation of Proposed Upper Bound
Setup: We use a simple example to demonstrate the necessity of doing optimization with our
proposed upper bound. We use AutoNF to build a 4 layer flow model with 2 transformation options
including planar flow and radial flow from (Rezende & Mohamed, 2015). We use the POWER
dataset as the target and optimize with original loss (name M1) and our proposed upper bound
(named M2). We use Adam optimizer for both architecture parameter and model parameter with a
learning rate of 0.002. The batch size is 512 and the training iteration is 10000.
The results are shown in Fig.3. For both M1 and M2, we present the weight for planar and radial
flow for each layer as well as the training and validation loss during the search process. The final
weight for each layer, searched architectures after binarization and the test score are shown in the
right-bottom table.
Analysis: Optimization with our proposed upper bound (M2) shows a concrete convergence of
weight to 0 or 1 for each layer, which leads to a desired normal flow architecture, while the opti-
mization with the original loss function (M1) ends up in a mixture model instead of a normal flow
architecture, as shown in Fig.3(left). This is within in our expectation as shown in Fig.2. Moreover,
although the mixture model is mostly likely to be the optimal in the original loss, the normal flow
architecture after binarization however, is not an optimal model. As shown in the right-bottom table,
the architecture found by M2 has a significantly better test score than M1, and this clearly supports
our statement of doing optimization with our proposed upper bound.
4.2	Search for flow Models with Best performance Cost Trade-off
Transformation Options and Reference Designs: To evaluate our AutoNF framework, we setup
our experiments with four types of non-linear flows and one linear flow. In autoregressive family, we
7
Under review as a conference paper at ICLR 2022
Figure 3: The result of optimization of a 4-layer flow ensemble with transformation options between
planar flow and radial flow with original loss and proposed upper bound. The left four figures are
the weight for each layer during the search process. The right-top figures are the training and vali-
dation loss during training. The right-bottom table collects final weight for each layer, the searched
architecture, and their test score (lower the better).
choose affine autoregressive flow (Papamakarios et al., 2017) and rational quadratic autoregressive
flow (Durkan et al., 2019). Affine autoregressive flow has limited expressive power but the com-
putation cost is lower, while the later has the state of art performance in autoregressive family with
higher cost. Affine coupling layer (Dinh et al., 2015) and rational quadratic coupling layer (Durkan
et al., 2019) are selected from coupling layer family. For linear transformation, we combine a re-
verse permutation and an LU linear layer together as a single layer. Random permutation (Durkan
et al., 2019; Oliva et al., 2018) is not used since it is difficult to reproduce in architecture optimiza-
tion. Every non-linear transformation layer is paired with a linear transformation layer suggested
by Durkan et al. (2019) as a final transformation option, i.e., a layer in our experiment contains a
reverse permutation, an LU-linear layer and one of the non-linear transformation layer listed above.
We use the rational quadratic flows family, including rational quadratic autoregressive flow (RQ-AF)
and Rational quadratic coupling layer (RQ-C) in (Durkan et al., 2019) which have top 2 performance
as the baseline. For fair comparison, we use RQ-AF as the baseline when emphasizing forward cost
since it has better performance and use RQ-C as the baseline when emphasizing inverse cost since
RQ-C has significantly lower inverse cost.
Evaluation Metric and Datasets: Evaluating the performance-cost trade-off is an open question
in NF, we propose to use a new metric to address the difficulty of negative log-likelyhood (NLL).
NLL is a common measurement for density estimation (lower, the better), however, the order of
magnitude of NLL is different across different datasets and it is not suitable to use percentage dif-
ference to measure how a model is exactly better than another.
In this paper, We proposed to utilize density and coverage (Naeem et al., 2020) to evaluate the
performance of NF models. Density and coverage are recently proposed method to evaluate the
sample quality of generative models. The density metric reflects the fidelity of the model and is
consistent with NLL metric. Across different datasets, density and coverage are at the same order of
magnitude and allows evaluation of architecture across datasets. In our experiments, 10000 samples
are drawn from the trained flow models and compare with 10000 samples from the test data. The
results of three independent runs are averaged as the final reported results.
To evaluate the performance-cost trade-off, we define a figure of merit (FOM) as FOM = cost re-
duction% + density drop% compared to reference SOTA designs. In principle, the weight of the two
terms can be manually adjusted to reflect the importance. For demonstration purpose, we use the
equally weighted summation to report the results.
The performance of the flow models are evaluated with density estimation for UCI (Dua & Graff,
2017) and BSDS300 (Martin et al., 2001) datasets.
8
Under review as a conference paper at ICLR 2022
Analysis: The architecture search results are reported in Table.1 which includes the test NLL,
density, coverage, cost and corresponding FOM. Table.1 shows that our AutoNF clearly helps to
find architectures that have better performance-cost trade-off. out AutoNF can reach to up to 3.66X
cost reduction and up to 75.2% improvement in FOM compared with SOTA literature results. Across
all five different datasets, AutoNF demonstrates an average improvement of 58.67% on FOM with
emphasis on forward cost and an average improvement of 52.57% on FOM with emphasis on inverse
cost.
Table 1: Performance and cost trade-off between searched architectures and human designed archi-
tectures on UCI density estimation datasets. When the emphasizing on forward cost, the cost column
reports forward cost, when emphasizing on inverse cost, the cost column reports inverse cost. The
test NLL (lower the better), density, coverage (higher the better) cost and FOM are reported.
Datasets	Cost Emphasize	Architectures	Test NLL	Density	Coverage	Cost	FOM
POWER	Forward	RQ-AF =	-0.66±0.Or=	0.9909	0.9637	16.64	0
		AutoNF	-0.24±0.01-	0.9677	0.9558	4.41	+74.4%
	Inverse	RQ-C	-0.64± 0.01	0.9715	-096-	13.65	0
		AUtONF	--0.210.01-	0.957	0.9547-	3.73	+75.2%
GAS	Forward	RQ-AF =	-13.09 ± 0.θ2=	0.7939	0.8982	16.64	0
		AUtONF	-9.12±0.02	0.6233	-0.769-	5.41	+46.00%
	Inverse	RQ-C	-13.09±0.02	0.7874	0.8965-	13.65	0
		AUtONF	-8.45±0.03	0.5186	-0.687-	4.73	+31.21%
HEPMASS	Forward	RQ-AF =	14.01 ± 0.0I=	0.9428	0.9588	16.64	0
		AUtONF	17.36±0.02	0.9074	0.9549-	6.49	+57.23%
	Inverse	RQ-C	14.75±0.03	0.7822	0.9556	13.65	0
		AUtONF	19.32±0.02	0.8841	0.9471-	5.73	+54.84%
MINIBOONE	Forward	RQ-AF =	9.22 ± 0.4厂	0.9034	0.922	16.64	0
		AUtONF	10.91±0.44	0.7957	0.9274	3.94	+64.40%
	Inverse	RQ-C	9.67 ± 0.47	0.8257	0.8982	13.65	0
		AUtONF	13.56±0.57	0.6816	0.8768-	3.73	+55.22%
BSDS300	Forward	RQ-AF =	-157.31±0.2F	0.8696	0.896	16.64	0
		AUtONF	-151.73±0.28	0.7817	0.8758-	6.41	+51.36%
	Inverse	RQ-C	-157.54±0.28	0.8624	0.9069-	13.65	0
		AUtONF	-149.79±0.28-	0.7848	0.8332	6.1	+46.40%
5	Discussion
Normalizing flow is highly parameterized module and designing a flow model and use it for appli-
cation requires a lot of hands-on experience and domain knowledge. In this paper, we show that the
AutoNF framework is very effective in balancing performance-cost trade-offs when building com-
plex flow models. Moreover, although not demonstrated in this paper, the framework could also be
used to help decide hyper parameters in complex flow model, e.g. the hidden features and number
of bins in the SOTA coupling layer (Durkan et al., 2019). In additional, the proposed optimization
method with upper bound can be easily extended to other suitable probabilistic kernels. one ex-
ample is to identify the best parameterized distribution(s) within a mixture model. We believe our
framework will be very useful in many machine learning applications where normalizing flows are
needed.
9
Under review as a conference paper at ICLR 2022
References
Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud, and Joern-Henrik Jacobsen.
Invertible residual networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceed-
ings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Ma-
chine Learning Research, pp. 573-582, Long Beach, California, USA, 09-15 JUn 2019. PMLR.
URL http://proceedings.mlr.press/v97/behrmann19a.html.
LaUrent Dinh, David KrUeger, and YoshUa Bengio. Nice: Non-linear independent components esti-
mation. CoRR, abs/1410.8516, 2015.
LaUrent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation Using real NVP. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
net/forum?id=HkpbnH9lx.
DheerU DUa and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Conor DUrkan, ArtUr Bekasov, Iain MUrray, and George Papamakarios. NeUral spline flows.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volUme 32. CUrran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
7ac71d433f282034e088473244df8c02-Paper.pdf.
Will Grathwohl, Ricky T. Q. Chen, Jesse BettencoUrt, Ilya SUtskever, and David DUvenaUd.
FFJORD: free-form continUoUs dynamics for scalable reversible generative models. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=
rJxgknCcK7.
Chin-Wei HUang, David KrUeger, Alexandre Lacoste, and Aaron CoUrville. NeUral aUtoregressive
flows. In Jennifer Dy and Andreas KraUse (eds.), Proceedings of the 35th International Con-
ference on Machine Learning, volUme 80 of Proceedings of Machine Learning Research, pp.
2078-2087. PMLR, 10-15 JUl 2018. URL https://proceedings.mlr.press/v80/
huang18d.html.
Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P
Xing. NeUral architectUre search with bayesian optimisation and optimal transport. In
S. Bengio, H. Wallach, H. Larochelle, K. GraUman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volUme 31. CUrran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
f33ba15effa5c10e873bf3842afb46a6- Paper.pdf.
DUrk P Kingma and PrafUlla Dhariwal. Glow: Generative flow with invertible 1x1 convolU-
tions. In S. Bengio, H. Wallach, H. Larochelle, K. GraUman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volUme 31. CUrran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
d139db6a236200b21cc7f752979132d0-Paper.pdf.
Hanxiao LiU, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray KavUkcUoglU. Hier-
archical representations for efficient architectUre search. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=BJQRKzbA-.
Hanxiao LiU, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architectUre search. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=S1eYHoC5FX.
D.	Martin, C. Fowlkes, D. Tal, and J. Malik. A database of hUman segmented natUral images
and its application to evalUating segmentation algorithms and measUring ecological statistics. In
Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volUme 2,
pp. 416-423 vol.2, 2001. doi: 10.1109/ICCV.2001.937655.
10
Under review as a conference paper at ICLR 2022
Bogdan Mazoure, Thang Doan, Audrey Durand, Joelle Pineau, and R Devon Hjelm. Leveraging
exploration in off-policy algorithms via normalizing flows. In Leslie Pack Kaelbling, Danica
Kragic, and Komei Sugiura (eds.), Proceedings of the Conference on Robot Learning, volume
100 of Proceedings of Machine Learning Research, pp. 430-444. PMLR, 30 Oct-01 Nov 2020.
URL https://proceedings.mlr.press/v100/mazoure20a.html.
Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable
fidelity and diversity metrics for generative models. 2020.
Renato Negrinho and Geoff Gordon. Deeparchitect: Automatically designing and training deep
architectures, 2017.
Junier Oliva, Avinava Dubey, Manzil Zaheer, Barnabas Poczos, Ruslan Salakhutdinov, Eric Xing,
and Jeff Schneider. Transformation autoregressive networks. In Jennifer Dy and Andreas Krause
(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pp. 3898-3907. PMLR, 10-15 Jul 2018. URL
https://proceedings.mlr.press/v80/oliva18a.html.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
6c1da886822c67822bcf3679d04369fa- Paper.pdf.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan. Normalizing flows for probabilistic modeling and inference, 2021.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc
Le, and Alex Kurakin. Large-scale evolution of image classifiers, 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis
Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learn-
ing, volume 37 of Proceedings of Machine Learning Research, pp. 1530-1538, Lille, France,
07-09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/rezende15.
html.
Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density
models, 2013.
Masanori Suganuma, Mete Ozay, and Takayuki Okatani. Exploiting the potential of standard con-
volutional autoencoders for image restoration by evolutionary search, 2018.
E.	G. Tabak and Cristina V. Turner. A family of nonparametric density estimation algorithms. Com-
munications on Pure and Applied Mathematics, 66(2):145-164, February 2013. ISSN 0010-3640.
doi: 10.1002/cpa.21423.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In 5th In-
ternational Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
net/forum?id=r1Ue8Hcxg.
11