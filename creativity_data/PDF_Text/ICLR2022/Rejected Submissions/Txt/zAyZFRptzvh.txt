Auditing AI models for Verified Deployment
under Semantic Specifications
Anonymous authors
Paper under double-blind review
Ab stract
Auditing trained deep learning (DL) models prior to deployment is vital for prevent-
ing unintended consequences. One of the biggest challenges in auditing is the lack
of human-interpretable specifications for the DL models that are directly useful to
the auditor. We address this challenge through a sequence of semantically-aligned
unit tests, where each unit test verifies whether a predefined specification (e.g.,
accuracy over 95%) is satisfied with respect to controlled and semantically aligned
variations in the input space (e.g., in face recognition, the angle relative to the cam-
era). We enable such unit tests through variations in a semantically-interpretable
latent space of a generative model. Further, we conduct certified training for the
DL model through a shared latent space representation with the generative model.
With evaluations on four different datasets, covering images of chest X-rays, hu-
man faces, ImageNet classes, and towers, we show how AuditAI allows us to
obtain controlled variations for certified training. Thus, our framework, AuditAI,
bridges the gap between semantically-aligned formal verification and scalability.
https://sites.google.com/view/audit-ai
1 Introduction
Deep learning (DL) models are now ubiquitously deployed in a number of real-world applications,
many of which are safety critical such as autonomous driving and healthcare (Kendall et al., 2019;
Miotto et al., 2018; Senior et al., 2020). As these models are prone to failure, especially under domain
shifts, it is important to know when and how they are likely to fail before their deployment, a process
we refer to as auditing. Inspired by the failure-mode and effects analysis (FMEA) for control systems
and software systems (Teng & Ho, 1996), we propose to audit DL models through a sequence of
semantically-aligned unit tests, where each unit test verifies whether a pre-defined specification (e.g.,
accuracy over 95%) is satisfied with respect to controlled and semantically meaningful variations in
the input space (e.g., the angle relative to the camera for a face image). Being semantically-aligned is
critical for these unit tests to be useful for the auditor of the system to plan the model’s deployment.
The main challenge for auditing DL models through semantically-aligned unit tests is that the current
large-scale DL models mostly lack an interpretable structure. This makes it difficult to quantify
how the output varies given controlled semantically-aligned input variations. While there are works
that aim to bring interpretable formal verification to DL models (Henriksen & Lomuscio, 2020; Liu
et al., 2019), the scale is still far from the millions if not billions of parameters used in contemporary
models (He et al., 2016; Iandola et al., 2014; Brown et al., 2020).
On the other hand, auditing has taken the form of verified adversarial robustness for DL mod-
els (Samangouei et al., 2018; Xiao et al., 2018a; Cohen et al., 2019). However, this has mostly
focused on adversarial perturbations in the pixel space, for example, through Interval Bound Propaga-
tion (IBP), where the output is guaranteed to be invariant to input pixel perturbations with respect to
Lp norm (Gowal et al., 2018; Zhang et al., 2019b). While these approaches are much more scalable
to modern DL architectures, the pixel space variations are not semantically-aligned, meaning they do
not directly relate to semantic changes in the image, unlike in formal verification. Consider a unit test
that verifies against the angle relative to the camera for a face image. A small variation in the angle
(e.g., facing directly at the camera versus 5° to the left) can induce a large variation in the pixel space.
Current certified training methods are far from being able to provide guarantees with respect to such
large variations in the pixel space with reasonable accuracy.
1
Model
Trained
DL Model
Generative
model bridge
Do not use
the model
for input x
Certified
Training
Unit Test 1
Data with
labels
UnitTest 2
Specification
not satisfied
UnitTest 3
Figure 1: Basic outline of AuditAI. The training phase involves the design and training of the algorithms for
a particular use case. The auditor lists a set of specifications that the trained model must satisfy. The auditor
receives the trained model from the designer and with the set of specifications has to determine whether the
model satisfies the specifications. If they are not satisfied, then the model is sent back to the designer for further
updates. After that, the model is deployed, with a model spec-sheet detailing the verified range of operation. At
deployment, latent embeddings of the input inform whether the input is within model-specifications. Based on
the spec-sheet, it can be decided in which settings to use the model.
Our Approach. In order to overcome the above limitations, we develop a framework for auditing,
AuditAI. We consider a typical machine learning production pipeline (Fig. 1) with three stages, the
design and training of the model, its verification, and finally deployment. The verification is crucial
in determining whether the model satisfies the necessary specifications before deployment.
We address the gap between scalability and interpretability by proposing to verify specifications for
variations directly in a semantically-aligned latent space of a generative model. For example in Fig. 1,
unit test 1 verifies whether a given face classification model maintains over 95% accuracy when the
face angle is within d°, while unit test 2 checks under What lighting condition the model has over
86% accuracy. Once the verification is done, the auditor can then use the verified specification to
determine whether to use the trained DL model during deployment.
For semantically-aligned latent variations, we create a bridge between the generative model and
the DL model such that they share the same latent space. We incorporate a variant of IBP (Zhang
et al., 2019b) for verification with respect to perturbations in the latent space. Further this leads to
a tighter and a much more practical bound in the output space compared to pixel-based certified
training. We also show that AuditAI can verify whether a unit test is satisfied by generating a proof
for verification based on bound propagation. Fig. 1 gives an overview of our auditing framework and
Fig. 2 elaborates on the generative model bridge.
Summary of Contributions:
1.	We develop a framework, AuditAI for auditing deep learning models by creating a bridge with
a generative model such that they share the same semantically-aligned latent space.
2.	We propose unit tests as a semantically-aligned way to quantify specifications that can be audited.
3. We show how IBP can be applied to latent space variations, to provide certifications of semantically-
aligned specifications.
We show that AuditAI is applicable to training, verification, and deployment across diverse datasets:
ImageNet (Deng et al., 2009), Chest X-Rays (Irvin et al., 2019; Wang et al., 2017), LSUN (Yu et al.,
2015), and Flicker Faces HQ (FFHQ) (Karras et al., 2019). For ImageNet, we show that AuditAI
can train verifiably robust models which can tolerate 25% larger pixel-space variations compared
to pixel-based certified-training counterparts for the same overall verified accuracy of 88%. The
variations are measured as L2 distances in the pixel-space. The respective % increase in pixel-
space variations that can be certified for Chest X-Rays, LSUN, and FFHQ are 22%, 19%, 24%.
We conclude with a human-study of the quality of the generative model for different ranges of
latent-space variations revealing that pixel-space variations up to 62% of the nominal values result in
realistic generated images indistinguishable from real images by humans.
2
2 AUDITAI: A Deep Learning Audit Framework
In this section, we describe the details of our framework, AuditAI outlined in Fig. 1 for verifying
and testing deep models before deployment. We propose to use unit tests to verify variations of
interest that are semantically-aligned. The advantage of AuditAI is that the verified input ranges
are given by several semantically aligned specifications for the end-user to follow during deployment.
In the following sub-sections, we formally define unit tests, outline the verification approach, and
describe the specifics of AuditAI with a GAN-bridge shown in Fig. 2.
2.1	Unit Test Definition
Consider a machine learning model f : X → Y that predicts outputs y ∈ Y from inputs x ∈ X in
dataset D. Each of our unit test can be formulated as providing guarantees such that:
F (x, y) ≤ 0 ∀x s.t. , ei (x) ∈ Si,in, y = f (x)
(1)
Here, i subscripts an unit test, encoder ei extracts the variation of interest from the input x, and Si,in
denotes the set of range of variation that this unit test verifies. If the condition is satisfied, then our
unit test specifies that the output of the model f (x) would satisfy the constraint given by F(∙, ∙) ≤ 0.
For example, x could be an image of a human face, and f could be a classifier for whether the
person is wearing eyeglasses (f ≤ 0 means wearing eyeglasses and f > 0 means not wearing
eyeglasses). Then e,(∙) could be extracting the angle of the face from the image, and Si,in could be
the set {ei(x)∣ ∀x ∣ei(x)∣ < 30° } constraining the rotation to be smaller than 30°. And F(x, y)=
-f (x)f (xo。) ≤ 0 says that our classifier output would not be changed from the corresponding
output of xo。, the face image of X without any rotation. In this case, when the end-user is going to
deploy f on a face image, they can first apply e,(∙) to see if the face angle lies within Si,in to decide
whether to use the model f .
2.2	Verification Outline
Given a specification F(x, y) ≤ 0, we need to next answer the following questions:
1.	How do we obtain the corresponding components (ei, Si,in) in Eq. (1)?
2.	What can we do to guarantee that F(x, y) ≤ 0 is indeed satisfied in this case?
For the sake of illustration, we first consider a scenario with a less realistic assumption. We will then
discuss how we can relax this assumption. In this scenario, we assume that we are already given
the encoder ei and a corresponding generator gi that inverts ei . Continuing the previous example,
ei can extract the face angle of a given face image. On the other hand, gi(x0。 , d°) would be able to
synthesize arbitrary face image xd。 that is the same as x0。 except being rotated by d°.
Given the generator gi and the en-
coder ei, we propose to obtain Si,in
building on interval bound propaga-
tion (IBP) (Gowal et al., 2018). We
include a treatment of IBP prelimi-
naries in Appendix A.1. Here, Si,in
is the largest set of variations such
that the specification F(x, y) ≤ 0
is satisfied. Given a set of varia-
tions Si,in, IBP-based methods can
be used to obtain a bound on the out-
put of the network Si,out = {y :
ly ≤ y ≤ uy} and it can be checked
whether the specification F(x, y) ≤
0 is satisfied for all values in Si,out.
We can start with a initial set Si0,in
Figure 2: Instead of variations at the level of input (e.g. pixels),
consider variations in the latent space. Given a specification, we use
a variant of interval-bound propagation (IBP) to verify if the spec-
ification is satisfied for -ball latent space variations for particular
latent dimensions.
and apply gi to this set. Then, we can first find out what would be the corresponding convex bound of
variations of Si0,in in the image space X. We can subsequently propagate this bound in X through f
to Y to get Si0,out and check whether F(x, y) ≤ 0 is satisfied. By iteratively searching for the largest
set Sij,in such that F(x, y) ≤ 0 is still satisfied by Sij,out, we can obtain Si,in given F.
3
2.3	Certified Training through Latent Representation
In the previous section, we are able to find the set Si,in such that Eq. (1) is satisfied given specification
F, encoder ei, generator gi. However, there are several challenges that limit the practical application
of this scenario. First and foremost, while significant progress has been made in generative models,
especially controlled generation (Brock et al., 2018; Karras et al., 2017), the assumption of having gi
apriori is still unrealistic. In addition, since the propagated bound is convex, one can imagine that the
bound in X (high dimensional image-space) would be much larger than needed. This leads to a much
narrower estimate of Si,in. While it could be true that Eq. (1) is satisfied, we still need a non-trivial
size of Si,in for the framework to be useful. For example, if Si,in constrains face angle rotations to
be within 1° then it may not be practical for the end-user.
For auditing ML models thoroughly, we require clear separation between the development and testing
phases (i.e. black-box verification). However, it might also be desirable in practice to train the model
in such a way that the unit tests are likely to be satisfied in the first place. In this case, we do have
a tighter connection between the designer and the verifier. We show that by relaxing the black-box
constraint, we can simultaneously address the previously listed challenges in a white-box setting.
Now that we have access to the internal working and design of the ML model f , we propose to
bypass the image generation step in our verification process through the use of latent disentangled
representations. More specifically, consider a ML model mapping f : X → Z → Y (denote by
fd : Z → Y the downstream task) and a generative model mapping g : X → Z → X, such that f
and g share a common latent space Z by virtue of having a common encoder e(∙) (Fig. 2).
Since Si,in is a subset of the range of e, we have Si,in ⊆ Z. This further implies that we do not have
to apply IBP through g and the whole f . Instead we only have to apply it through fd, which does
not involve the expansion to the pixel space X . We show in the experiment (Section 4) that this is
important to have a practically useful Si,in. In addition, AuditAI also alleviates the need of having
a perfect generative model, as long as we can learn a latent space Z through an inference mechanism.
Learning the latent space. There are three requirements for the latent space. First, it should be
semantically-aligned for us to specify unit tests in this space. A good example is disentangled
representation (Higgins et al., 2018; Tran et al., 2017), where a set of dimensions of the latent code
would correspond to a semantically aligned variation of the original image. As shown in Figure 2, a
latent dimension could correspond to the pose variation, and we can select ei to be the value of this
dimension, and Si,in as the proper range that the model would be verified for. Second, our model
should be verifiable, where the latent space is learned such that the corresponding Si,in is as large
as possible given a specification function F. In this case, AuditAI can be seen as doing certified
training (Zhang et al., 2019b) to improve the verifiability of the model. Finally, the latent space
should be able to perform well on the downstream task through fd . Combining these three criteria,
our full training criteria: L = Ltask + γLspec + δLgen combines three losses Lgen, Lspec, and Ltask that
would encourage interpretability through the latent space of a generative model, verifiability, and task
performance respectively. γ and δ are relative weights (between 0 and 1) for the loss function terms.
We explain each of the losses below:
Verifiability (Lspec). Let fd be a feedforward model (can have fully connected / convolutional /
residual layers) with K layers zk+1 = hk(Wkzk + bk), k = 0, ..., K - 1, where, z0 = e(x), zK
denotes the output logits, and hk is the activation function for the kth layer. The set of variations
Si,in could be such that the lp norm for the ith latent dimension is bounded by , i.e. Si,in = {z :
||zi - z0,i||p ≤ }. We can bound the output of the network Sout = {zK : lK ≤ zK ≤ uK} through
a variant of interval bound propagation (IBP). Let the specification be F(z, y) = cTzK + d ≤
0, ∀z ∈ Si,in, zK = fd(z).
For a classification task, this specification implies that the output logits zK should satisfy a linear
relationship for each latent space variation such that the classification outcome arg maxi zK,i remains
equal to the true outcome ytrue corresponding to z0 . To verify the specification, IBP based methods
search for a counter-example, which in our framework amounts to solving the following optimization
problem, and checking whether the optimal value is ≤ 0.
max F(z,y) = cTzK +d s.t. zk+1 = hk(Wkzk + bk) k = 0, ..., K - 1	(2)
z∈Sin
By following the IBP equations for bound-propagation, we can obtain upper and lower bounds
[zκ, Zk] which can be used to compute the worst case logit difference Zκ,y - Zκ,ytrue between the
4
true class ytrue and any other class y. We define ZK = Zκ,y (if y = ytrue); ZK = Zκ,yteue (otherwise).
Then, we can define LSpeC as LSpeC = CE(ZK, ytrue). In practice, we do not use IBP, but an improved
approach CROWN-IBP (Zhang et al., 2019b) that provides tighter bounds and more stable training,
based on the AutoLiRPA library (Xu et al., 2020). We mention details about this in the Appendix.
Task Performance (Ltask). Let, CE(∙) denote the standard cross-entropy loss. LtaSk captures the
overall accuracy of the downstream task and can be expressed as Ltask = CE(ZK, ytrue).
Generative Model (Lgen ). Finally, we have a loss term for the generative model, which could be
a VAE, a GAN, or variants of these. If the model is a GAN, then we would have an optimization
objective for GAN inversion. An assumption is being able to learn a disentangled latent space with
semantically aligned concepts. Let the overall loss be Lgen which could also encapsulate other losses
for more semantically-aligned disentanglement. Typically we would train this generative model on a
large amount of unlabelled data that is not available for training the classifier we are auditing.
2.4	Deployment
Based on the perturbation radius and the nominal Z0, we have different ranges [Z0n - 1i, Z0n + 1i]
for the verified error at the end of training the classifier. Here, Z0n = e(xn) ∀ n = 1, ...N in the
dataset Dtrain and 1i = 1 for the ith dimension (corresponding to the ith unit test) and is 0 otherwise.
We can convert this per-sample bound to a global bound for the ith latent code, ∖Zi,zi] by considering
an aggregate statistic of all the ranges, for example
[Zi,Zi] = [μ({zn,i - 1i≡}n=ι) - σ({zn,i - 1le}N=1), μ({z0,i + 1le}N=1) +。(*匿 + 1le}N=1)]
Here, μ(∙) denotes the mean and σ(∙) denotes the standard deviation. Note that this is a global bound
on the range of variations for only the latent dimension corresponding to the unit test, and does
not concern the other dimensions. During deployment, the end-user receives the model spec-sheet
containing values of [Zi, zj for each unit test i and results with different values of perturbation radius
. Now, if the end-user wants to deploy the model on an evaluation input xeval, they can encode it to
the latent Z space to get code zɛval and check whether Zeval,i ∈ [Z0,i, Z0 i] in order to decide whether
to deploy the model for this evaluation input.
3	Theoretical Results
We show that although AuditAI considers perturbations in the latent space that allows a large range
of semantic variations in the pixel-space, it retains the theoretical guarantees of provable verification
analogous to IBP-based adversarial robustness (Zhang et al., 2019b; Xu et al., 2020).
Theorem 1 (Verification. The auditor can verify whether the trained model from the designer satisfies
the specifications, by generating a proof of the same). Letx ∈ X be the input, Z0 = e(x) be the latent
code, fd be a feed-forward neural network with bounded derivatives, and Si,in = {Z : ||Zi - Z0,i||p ≤
} be the set of latent perturbations. Then, the auditor is guaranteed to be able to generate a proof for
verifying whether the linear specification on output logits cTZK + d ≤ 0 ∀Z ∈ Si,in, ZK = fd(Z)
(corresponding to unit test i) is satisfied.
Proof. Here we provide a proof sketch and mention details in the Appendix A.2. The verifier can
generate a proof by searching for a worst-case violation maxz∈Si,in cT ZK + d s.t. Zk+1 =
hk(WkZk + bk) k = 0, ..., K - 1. If the optimal value of this < 0, then the specification is satisfied.
The verifier applies Interval-bound Propagation (IBP) to obtain an upper bound ZK on the solution of
this optimization problem, and checks whether the upper bound ZK < 0.	□
4	Experiments
We perform experiments with four datasets in different application domains. We choose the datasets
ImageNet, LSUN, FFHQ, and CheXpert where current state of the art generative models have shown
strong generation results (Karras et al., 2019; 2017; Brock et al., 2018). We show an illustration of
what the generated images under latent perturbations look like in Fig 3. Through experiments, we
aim to understand the following:
1.	How do the verified error rates vary with the magnitude of latent perturbations?
2.	How does AuditAI compare against pixel-perturbation based verification approaches?
3.	How does AuditAI perform during deployment under domain shift?
5
4.1 ImageNet
Figure 3: Generated samples from latent space manip-
ulation of features on the ImageNet, CheXpert, LSUN
Tower, and FFHQ datasets. We respectively show two
images of a hen looking left and right, chest X-ray
images with and without pneumonia, towers with and
without vegetation and faces, with and without glasses.
These are examples of features that can be controlled
through latent space manipulations.
ImageNet contains 1000 different classes of ob-
jects. We consider a BigBiGAN model (Don-
ahue & Simonyan, 2019) as the base genera-
tive model such that the latent space is class
conditional, and we can obtain latent codes cor-
responding to specific classes. So, latent per-
turbations correspond to variations in different
images within the same class. The specification
for verification is that the classifier outputs the
correct class for all latent perturbations . This
is an unit test because we can verify with respect
to each class separately. Table 1 shows results
for different on the test set. The results con-
firm the intuition that increasing increases the
verified error due to wider latent perturbations.
In Table 2, we compare a pixel-based robust
training approach with our framework, both trained with the same underlying algorithm CROWN-
IBP (Zhang et al., 2019b). For the same verified test error, we have the corresponding latent
perturbation for AuditAI (call it 1) and the pixel perturbation for the pixel-based approach (call it
2). Based on 2 we compute the bound for the intermediate latent node, 21. Since comparing 1 and
21 directly may not be meaningful due to different nominal values of the latent code, we consider the
fraction /znom and plot the resulting values in Table 2. We observe that 1/znom1 >> 21/znom21
indicating tolerance to much larger latent variations. Since a wider latent variation corresponds to a
wider set of generated images, AuditAI effectively verifies over a larger range of image variations,
compared to perturbations at the level of pixels. In addition, when we translate the latent perturbations
of AuditAI to the pixel space of the generative model, we obtain on average 20%, and 17% larger
variations measured as L2 distances compared to the pixel-based approach for a verified error of 0.88
and 0.50 respectively.
Table 1: Verified Error fraction on a subset (100 classes) of the ImageNet dataset. Results are on the held-out
test set of the dataset and correspond to training the classifier with the same during training. S.D. is over four
random seeds of training. 100 classes is the average value of 100 unit tests over all the 100 classes while min
class and max class respectively correspond to the minimum and maximum verified errors for unit tests over
each class. Alongside each we represent the % with respect to the nominal value it corresponds to (the nominal
value of is 0.02, so = 0.005 is 25% of 2.0). Lower is better.
ImageNet	= 0.005(25%)	0.01(50%)	0.015(75%)	0.02(100%)	0.05(200%)
100 classes	0.20±0.05	0.51±0.06	0.57±0.05	0.89±0.04	0.94±0.05
min class	0.07±0.06	0.33±0.02	0.41±0.05	0.58±0.05	0.86±0.07
max class	0.41±0.04	0.68±0.04	0.73±0.06	0.93±0.05	0.99±0.05
Table 2: Comparison of pixel-based variation for CROWN-IBP and AuditAI on 100 classes of the ImageNet
dataset. We tabulate the values below relative to the nominal values of the corresponding latent codes (i.e.
/znom) and observe that 1/znom1 >> 21/znom21 . As a wider latent perturbation corresponds to a wider set
of generated images, AuditAI effectively verifies over a larger range of image variations. Higher is better.
Verified Error		0.88	0.50	
Method	Pixel-based	21 AuditAI 1 |	Pixel-based	21 AuditAI 1
% Latent variation	0.21%	20%	0.14%	12%
4.2	CheXpert
CheXpert (Irvin et al., 2019) contains 200,000 chest radiograph images with 14 labeled observations
corresponding to conditions like pneumonia, fracture, edema etc. We consider a PGGAN (Karras
et al., 2017) as the base generative model such that the latent space is class-conditional. We consider
three separate binary classification tasks with respect to the presence or absence of three conditions:
6
pneumonia, fracture, and edema. We consider the entire dataset for training the generative model,
but for the classifiers we only use positive (presence of the condition) and negative (absence of the
condition) data with an 80-20 train/test split. Table 3 shows results for different on the test set for
each classifier.
As a sanity check, in order to determine whether the generative model behaves correctly with respect
to class conditional generation, we consider an off-the-shelf classifier for pneumonia detection trained
on real images of pneumonia patients (Rajpurkar et al., 2017). We randomly sample 1000 images
from the generative model for each value of = 0.5, 1.0, 1.5, 2.0 and evaluate the accuracy of the off-
the-shelf classifier on these generated images. We obtain respective accuracies 85%, 84%, 81%, 75%
indicating highly reliable generated images.
Table 3: Verified Error fraction on the CheXpert dataset. Results are on the held-out test set of the dataset and
correspond to training the classifier with the same during training. S.D. is over four random seeds of training.
Each row is an unit test as it corresponds to verifying a classifier for a particular condition. Alongside each we
represent the % with respect to the nominal value it corresponds to (the nominal value of is 2.0, so = 0.5 is
25% of 2.0). Lower is better.
Unit Test	= 0.5(25%)	= 1.0(50%)	e = 1.5(75%)	e = 2.0(100%)	e = 4.0(200%)
Pneumonia	0.02±0.01	0.12±0.03	0.24±0.05	0.43±0.04	0.51±0.05
Edema	0.03±0.03	0.14±0.04	0.22±0.04	0.41±0.03	0.55±0.06
Fracture	0.04±0.02	0.14±0.03	0.22±0.05	0.42±0.02	0.52±0.03
Deployment under domain-shift. In order to better understand the process of deploying the trained
model on a dataset different from the one used in training, we consider the dataset NIH Chest X-
rays (Wang et al., 2017). This dataset contains (image, label) pairs for different anomalies including
pneumonia and edema. We perform certified training on CheXpert for different values of (results of
this are in Table 3) and deploy the trained model to the end-user. The end-user samples 5000 images
from the NIH Chest X-rays dataset, encodes them to the latent space with trained encoder e(∙) and
uses the trained downstream task classifier fd(∙) to predict the anomaly in the image. In Table 4, we
list the precision, recall, and overall accuracy among the 5000 images for two conditions, presence of
pneumonia and edema. We observe that the results for AuditAI are on average 5% higher than the
pixel-based approach. This intuitively makes sense because from Table 2 and section 4.1, we see
that AuditAI covers about 20% larger variations in the pixel space and so the odds of the images at
deployment lying in the verified range are higher for AuditAI.
4.3	FFHQ
Flicker Faces HQ (Karras et al., 2019) contains 70,000 unlabeled images of normal people (and
not just celebrities as opposed to the CelebA (Liu et al., 2015) dataset). We train a StyleGAN
model (Karras et al., 2019) on this dataset and use existing techniques (Bau et al., 2020a; Shen &
Zhou, 2020; Shen et al., 2020) to identify directions of semantic change in the latent space. We
identify latent codes (set of dimensions of Z) corresponding to expression (smile), pose,
and presence of eyeglasses. We define the task for the classifier to be detection of whether
eyeglasses are present and define unit tests to be variations in the latent code with respect to expression
and pose (while keeping the code for eyeglasses invariant). Table 5 shows the results of these unit
tests corresponding to different latent perturbations . We see that the verified error is up to 20%
for over 50% larger variations compared to nominal values. Based on this spec-sheet table, during
deployment, the end-user can check whether the encoding of input image lies in the desired accuracy
ranges for pose and expression, and then decide whether to use the model.
4.4	LSUN TOWER
The LSUN Tower dataset (Yu et al., 2015) contains 50,000 images of towers in multiple loca-
tions. Similar to the FFHQ setup above, we train a StyleGAN model on this dataset and use
existing techniques (Shen & Zhou, 2020; Zhu et al., 2020) to identify directions of semantic
change in the latent space. We identify latent codes corresponding to clouds, vegetation,
and sunlight/brightness. We define the task for the classifier to be detection of whether
vegetation is present and define unit tests to be variations in the latent code with respect to how cloudy
the sky is, and how bright the scene is (while keeping the code for vegetation invariant). Table 5
shows the results of these unit tests corresponding to different latent perturbations . We see that the
7
Table 4: Domain shift study. Evaluation on the NIH Chest X-Rays dataset, after being trained on the CheXpert
dataset. We perform certified training of the models with = 1.0 on CheXpert and deploy it on 5000 images of
a different dataset to quantify evaluation performance under potential domain shift. We tabulate the values of
precision, recall, and overall accuracy. Higher is better.
Method		Pixel-based		AuditAI		
Metric	Precision	Recall	Accuracy I	Precision	Recall	Accuracy
Pneumonia	0.78±0.03	0.74±0.04	0.80±0.03	0.83±0.04	0.78±0.05	0.84±0.05
Edema	0.79±0.05	0.75±0.03	0.81±0.04	0.84±0.04	0.79±0.03	0.86±0.04
Table 5: Verified Error fraction on the FFHQ and LSUN datasets. For FFHQ, the task is to classify whether
eyeglasses are present, and the unit tests correspond to variations with respect to pose and expression of the
face. For LSUN Tower, the task is to classify whether green vegetation is present on the tower, and the unit tests
correspond to variations with respect to clouds in the sky and brightness of the scene. Results are on the held-out
test set of the dataset and correspond to training the classifier with the same during training. S.D. is over four
random seeds of training. Lower is better.
FFHQ	= 0.5(25%)	= 1.0(50%)	e = 1.5(75%)	e = 2.0(100%)	e = 3.0(150%)
Pose test	0.01±0.02	0.20±0.05	0.37±0.03	0.48±0.03	0.53±0.06
Expression test	0.01±0.02	0.13±0.04	0.29±0.04	0.58±0.01	0.56±0.03
LSUN Tower	= 0.5(25%)	= 1.0(50%)	e = 1.5(75%)	e = 2.0(100%)	e = 3.0(150%)
Cloud test	0.02±0.03	0.14±0.02	0.32±0.05	0.44±0.06	0.51±0.03
Brightness test	0.01±0.01	0.18±0.03	0.35±0.05	0.49±0.04	0.54±0.03
verified error is up to 18% for over 50% larger variations compared to nominal values. Based on this
spec-sheet table, during deployment, the end-user can check whether the encoding of input image
lies in the desired accuracy ranges for clouds in the sky and brightness of the scene, and then decide
whether to use the model.
4.5	Attacking images audited by AUD I TAI in the pixel-space
In this section, we analyze what fraction of images audited by AuditAI can be attacked in the
pixel-space through adversarial corruptions. We create test examples with varying brightness of the
scene, corresponding to the setting in Table 5. These are real variations directly by modifying pixels
to changing the brightness of the scene . We evaluate AuditAI trained the same way as in Table 5,
but evaluated on this test set after certified training. For = 0.5, 96 out of 1000 images (only 9.6%)
certified by AuditAI can be attacked in the pixel-space, while for = 1.5, 85 out of 1000 images
(only 8.5%) certified by AuditAI can be attacked. We use CROWN-IBP (Zhang et al., 2019b)
to determine if an image is attackable in the pixel-space. These results show that the certification
provided by AuditAI is robust, and although verification is done in the latent space of a generative
model, the results hold under real pixel-perturbations as well.
5 Human S tudy: How realistic are the generated images ?
We employ Amazon Mechanical Turk (Buhrmester et al., 2016) to better understand the generative
model trained for the Chest X-ray dataset and determine the limits of latent space variations for which
generations are realistic. This study is to determine the maximum value of for which generations
are realistic to the human eye, such that we verify and report in the spec-sheet values of only up to
the range of realistic generations. We note that this study is to understand the generative model and
not the trained classifier. We employ a similar experimental protocol as (Isola et al., 2017; Zhang
et al., 2016) and mention details in the Appendix A.4. We generated 2000 images by sampling latent
codes within each range of variation. Alongside each we represent the % with respect to the
nominal value it corresponds to (the nominal value of is 2.0, so = 0.5 is 25% of 2.0) in Fig 4. We
see that the humans who participated in our study had close to 50% correct guesses for up to 1.0,
indicating generations indistinguishable from real images. For ≥ 4.0 the correct guess rate is above
70% indicating that the generations in these ranges of latent variations are not realistic, and so the
verified accuracy for these ranges should not be included in the spec-sheet.
8
6	Related Work
Multiple prior works have studied a subset of
the audit problem in terms of adversarial robust-
ness (Raghunathan et al., 2018a; Dvijotham
et al.). Adversarial examples are small pertur-
bations to the input (for images at the level of
pixels; for text at the level of words) which mis-
lead the deep learning model. Generating ad-
versarial examples has been widely studied (Xu
et al., 2018; Zhang et al., 2019a) and recent ap-
proaches have devised defenses against adver-
sarial examples (Xie et al., 2020; Zhang et al.,
2019b). Some of these approaches like (Gowal
et al., 2018) and (Zhang et al., 2019b) pro-
vide provable verification bounds on an out-
put specification based on the magnitude of
perturbations in the input. These approaches
have largely showed robustness and verifica-
tion to norm bounded adversarial perturbations
Mechanical Turk study results
100 r	1811
80 -	1450
ω 60 -	990	1060
I Uli
0.5 (25%)	1 (50%)	2 (100%)	4 (200%)	6 (300%)
L2 latent variation ∈
Figure 4: Amazon MTurk study involving 2000 human
evaluations for each value of in a study of CheXpert
Chest X-ray images. We showed pairs of real and gen-
erated images to the MTurk workers who chose to take
part in our study and asked them to identify which image
is real and which is fake (generated). A value close to
50% indicates that the MTurk workers were unable to
distinguish the real images from the generated ones, in-
dicating that the generated images looked as good as the
real images.
without trying to verify with respect to more semantically aligned properties of the input. Some
papers (MohaPatra et al., 2020; RuoSS et al., 2020; BalunoVic et al., 2019; Wong & Kolter, 2020)
consider semantic perturbations like rotation, translation, occlusion, brightness change etc. directly
in the pixel Space, So the range of Semantic VariationS that can be conSidered are more limited. ThiS iS
a diStinction with AuditAI, where by perturbing latent codeS directly (aS oppoSed to pixelS), the range
of Semantic VariationS captured are much larger.
A related notion to auditing iS expalainable AI (Gunning, 2017). We note that auditing iS different
from thiS problem becauSe explanationS can be incorrect while remaining uSeful. WhereaS in auditing,
our aim iS not to come up with plauSible explanationS for why the model behaVeS in a certain way, but
to quantify how it behaVeS under Semantically-aligned VariationS of the input.
Some paperS (Mandelbaum & WeinShall, 2017; Van AmerSfoort et al., 2020) haVe uSed model
confidence and ucnertainty eStimateS for rejecting out-of-diStribution SampleS at teSt time, which iS a
form of auditing. In (Mitchell et al., 2019) and (Gebru et al., 2018), the authorS propoSed a checkliSt
of queStionS that Should be anSwered while releaSing DL modelS and dataSetS that relate to their
motiVation, intended uSe caSeS, training detailS, and ethical conSiderationS. TheSe paperS formed a
part of our motiVation and proVided guidance in formalizing the audit problem for DL modelS. We
proVide a detailed SurVey of related workS in Section A.6.
7 Discussion and Conclusion
In thiS paper we deVeloped a framework for auditing of deep learning (DL) modelS. There are
increaSingly growing concernS about innate biaSeS in the DL modelS that are deployed in a wide
range of SettingS and there haVe been multiple newS articleS about the neceSSity for auditing DL
modelS prior to deployment12. Our framework formalizeS thiS audit problem which we belieVe iS a
Step towardS increaSing Safety and ethical uSe of DL modelS during deployment.
One of the limitationS of AuditAI iS that itS interpretability iS limited by that of the built-in
generatiVe model. While exciting progreSS haS been made for generatiVe modelS, we belieVe it iS
important to incorporate domain expertiSe to mitigate potential dataSet biaSeS and human error in
both training & deployment. Currently, AuditAI doeSn’t directly integrate human domain expertS
in the auditing pipeline, but indirectly uSeS domain expertiSe in the curation of the dataSet uSed for
creating the generatiVe model. Although we haVe demonStrated AuditAI primarily for auditing
computer ViSion claSSification modelS, we hope that thiS would paVe the way for more SophiSticated
domain-dependent AI-auditing toolS & frameworkS in language modelling and deciSion-making
applicationS.
1httpS://www.technologyreView.com/2021/02/11/1017955/auditorS-teSting-ai-hiring-algorithmS-biaS-big-
queStionS-remain/, Feb 11, 2021
2httpS://www.wired.com/Story/ai-needS-to-be-audited/, Jul 10, 2019
9
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. International Conference on Machine
Learning (ICML), 2018.
Mislav Balunovic, Maximilian Baader, GagandeeP Singh, Timon Gehr, and Martin Vechev. Certifying
geometric robustness of neural networks. Advances in Neural Information Processing Systems 32,
2019.
David Bau, Hendrik Strobelt, William Peebles, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba, et al.
Semantic Photo maniPulation with a generative image Prior. arXiv preprint arXiv:2005.07727,
2020a.
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata LaPedriza, Bolei Zhou, and Antonio Torralba.
Understanding the role of individual units in a deeP neural network. Proceedings of the National
AcademyofSciences,117(48):30071-30078, 2020b.
David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, and Antonio
Torralba. Paint by word. arXiv preprint arXiv:2103.10951, 2021.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared KaPlan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot
way to resist adversarial examPles. International Conference on Learning Representations, 2018.
Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon’s mechanical turk: A new
source of inexPensive, yet high-quality data? 2016.
Nicholas Carlini and David Wagner. Adversarial examPles are not easily detected: ByPassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, PP. 3-14. ACM, 2017a.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
38th IEEE Symposium on Security and Privacy (SP), PP. 39-57. IEEE, 2017b.
Nicholas Carlini, Anish Athalye, Nicolas PaPernot, Wieland Brendel, Jonas Rauber, Dimitris TsiPras,
Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness.
arXiv preprint arXiv:1902.06705, 2019.
Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Attacking visual language
grounding with adversarial examPles: A case study on neural image caPtioning. In Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), PP. 2587-2597, 2018.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, PP. 1310-1320. PMLR, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
PP. 248-255. Ieee, 2009.
Jeff Donahue and Karen Simonyan. Large scale adversarial rePresentation learning. In Advances in
Neural Information Processing Systems, PP. 10541-10551, 2019.
Krishnamurthy Dj Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Pushmeet
Kohli. Efficient neural network verification with exactness characterization.
10
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1625-1634, 2018.
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,
Hal DaUme III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010,
2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ICLR, 2015.
Sven Gowal, KrishnamUrthy Dvijotham, Robert Stanforth, RUdy BUnel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and PUshmeet Kohli. On the effectiveness of interval
boUnd propagation for training verifiably robUst models. arXiv preprint arXiv:1810.12715, 2018.
David GUnning. Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency
(DARPA), nd Web, 2(2), 2017.
ChUan GUo, Mayank Rana, MoUstapha Cisse, and LaUrens van der Maaten. CoUntering adversarial
images Using inpUt transformations. In ICLR, 2018.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Patrick Henriksen and Alessio LomUscio. Efficient neUral network verification via adaptive refinement
and adversarial search. In ECAI 2020, pp. 2513-2520. IOS Press, 2020.
Irina Higgins, David Amos, David PfaU, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, and KUrt KeUtzer.
Densenet: Implementing efficient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869,
2014.
Jeremy Irvin, Pranav RajpUrkar, Michael Ko, Yifan YU, Silviana CiUrea-IlcUs, Chris ChUte, Henrik
MarklUnd, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest
radiograph dataset with Uncertainty labels and expert comparison. In Proceedings of the AAAI
Conference on Artificial Intelligence, volUme 33, pp. 590-597, 2019.
Phillip Isola, JUn-Yan ZhU, TinghUi ZhoU, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Robin Jia, Aditi Raghunathan, Kerem Goksel, and Percy Liang. Certified robustness to adversarial
word sUbstitUtions. arXiv preprint arXiv:1909.00986, 2019.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401-4410, 2019.
Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen,
Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 International
Conference on Robotics and Automation (ICRA), pp. 8248-8254. IEEE, 2019.
Changliu Liu, Tomer Arnon, Christopher Lazarus, Christopher Strong, Clark Barrett, and Mykel J
Kochenderfer. Algorithms for verifying deep neural networks. arXiv preprint arXiv:1903.06758,
2019.
11
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Michael E Houle, Grant
Schoenebeck, Dawn Song, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. In International Conference on Learning Representations (ICLR), 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Amit Mandelbaum and Daphna Weinshall. Distance-based confidence score for neural network
classifiers. arXiv preprint arXiv:1709.09844, 2017.
Riccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley. Deep learning for
healthcare: review, opportunities and challenges. Briefings in bioinformatics, 19(6):1236-1246,
2018.
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,
Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In
Proceedings of the conference on fairness, accountability, and transparency, pp. 220-229, 2019.
Jeet Mohapatra, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Towards verifying
robustness of neural networks against a family of semantic perturbations. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 244-252, 2020.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversar-
ial examples. International Conference on Learning Representations (ICLR), arXiv preprint
arXiv:1801.09344, 2018a.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10900-10910, 2018b.
Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding,
Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-level pneumonia
detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017.
Anian Ruoss, Maximilian Baader, Mislav Balunovic, and Martin Vechev. Efficient certification of
spatial robustness. arXiv preprint arXiv:2009.09318, 2020.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers
against adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
Chongli Qin, Augustin 之idek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
structure prediction using potentials from deep learning. Nature, 577(7792):706-710, 2020.
Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. arXiv preprint
arXiv:2007.06600, 2020.
Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. Interfacegan: Interpreting the disentan-
gled face representation learned by gans. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2020.
12
GagandeeP Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, pp.
10825-10836, 2018.
GagandeeP Singh, Timon Gehr, Markus Puschel, and Martin Vechev. Robustness certification with
refinement. ICLR, 2019.
Yang Song, TaesuP Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examPles. arXiv
preprint arXiv:1710.10766, 2017.
Sheng-Hsien Gary Teng and Shin-Yann Michael Ho. Failure mode and effects analysis. International
journal of quality & reliability management, 1996.
Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled rePresentation learning gan for Pose-invariant
face recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, PP. 1415-1424, 2017.
Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deeP deterministic neural network. In International Conference on Machine Learning, PP.
9690-9700. PMLR, 2020.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety
analysis of neural networks. In Advances in Neural Information Processing Systems, PP. 6369-
6379, 2018.
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.
Chestx-ray8: HosPital-scale chest x-ray database and benchmarks on weakly-suPervised classi-
fication and localization of common thorax diseases. In Proceedings of the IEEE conference on
computer vision and pattern recognition, PP. 2097-2106, 2017.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast comPutation of certified robustness for ReLU networks. In
International Conference on Machine Learning, 2018.
Eric Wong and J Zico Kolter. Learning Perturbation sets for robust machine learning. arXiv preprint
arXiv:2007.08450, 2020.
Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Mingyan Liu, and Dawn Song. Characterizing
adversarial examPles based on sPatial consistency information for semantic segmentation. In
Proceedings of the European Conference on Computer Vision (ECCV), PP. 217-234, 2018a.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial
examPles with adversarial networks. IJCAI18, 2018b.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. SPatially transformed
adversarial examPles. ICLR18, 2018c.
Chaowei Xiao, Ruizhi Deng, Bo Li, Taesung Lee, Benjamin Edwards, Jinfeng Yi, Dawn Song,
Mingyan Liu, and Ian Molloy. Advit: Adversarial frames identifier based on temPoral consistency
in videos. In Proceedings of the IEEE International Conference on Computer Vision, PP. 3968-
3977, 2019a.
Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. Meshadv: Adversarial meshes
for visual recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, PP. 6898-6907, 2019b.
Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc V Le. Smooth adversarial training.
arXiv preprint arXiv:2006.14536, 2020.
Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Quanfu Fan, Deniz Erdogmus, Yanzhi
Wang, and Xue Lin. Structured adversarial attack: Towards general imPlementation and better
interPretability. arXiv preprint arXiv:1808.01664, 2018.
13
Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified
robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020.
Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On
completeness-aware concept-based explanations in deep neural networks. Advances in Neural
Information Processing Systems, 33, 2020.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In Advances in Neural Information
Processing Systems (NIPS), 2018.
Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S Dhillon, and Cho-Jui Hsieh. The
limitations of adversarial training and the blind-spot attack. ICLR, 2019a.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks.
arXiv preprint arXiv:1906.06316, 2019b.
Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European
conference on computer vision, pp. 649-666. Springer, 2016.
Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. arXiv
preprint arXiv:1710.11342, 2017.
Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing.
In European Conference on Computer Vision, pp. 592-608. Springer, 2020.
14
A Appendix
A. 1 Preliminaries: Interval-Bound Propagation
In this section, we provide a preliminary treatment of Interval-Bound Propagation based robust
training and verification. Let f be a feedforward classifier model that outputs one of N classes. The
input to the network is an image x0 , and it has a certain ground truth class ytrue. The classifier is
trained to minimize a certain misclassification loss. The verification problem is to check whether
there is some set Sin(x0) around perturbations of x0 such that the classification output of the model
remains invariant from that of ytrue.
Let f have K layers zk+1 = hk(Wkzk + bk), k = 0, ..., K - 1, such that z0 ∈ Sin(x0), zK
denotes the output logits, and hk is the activation function for the kth layer. If we consider the
set of variations to be Sin(X0) = {χ∣∣∣χ - χo∣∣∞ ≤ e}, then We want to verify the specification
that arg maxi zK,i = ytrue ∀ z0 ∈ Sin (x0). We can write this as a general linear specification
cTzK + d ≤ 0 ∀ z0 ∈ Sin (x0)
To verify the specification, IBP based methods search for a counter-example that violates the specifi-
cation constraint, by solving the following optimization problem and checking whether the optimal
value ≤ 0.
max cTzK + d s.t. zk+1 = hk(Wkzk + bk) k = 0, ..., K - 1	(3)
x∈Sin
Since solving this optimization problem exactly is NP-hard (Zhang et al., 2019b), IBP finds an
upper bound on the optimal value and checks whether the upper bound ≤ 0. To get an upper
bound on zK , we can propagate bounds for each activate zk through axis-aligned bounding boxes,
Zk,i(e) ≤ zk,i ≤ Zk,i(e), where i indexes dimension. Based on this, the specification is upper-
boundedby zκ,y(e) - Zκ,ytrue(e).
In addition to the verification problem, IBP can be used to perform certified training such that the
specification is likely to be satisfied in the first place at the end of training. For this, the overall
training loss would consist of two terms: usual classification loss like cross-entropy that maximizes
log likelihood l(zκ, ttrue), and a loss term that encourages satisfiying the specification l(Zκ(e), ttrue)∙
Here, zκ,y (e) = Zκ,y (e) (if y = ytrue) and zκ,y (e) = Zκ,ytrue (e) (if y = ytrue). So, the overall loss
for certified training can be represented as:
L = Xt(ZK, ttrue) + (1 - X)I(ZK(e), ttrue)
As reported in (Gowal et al., 2018; Zhang et al., 2019b; Xu et al., 2020), to stabilize training, χ is
varied during training such that for a few initial epochs X = 1 (normal training) and its value is slowly
increased so that robust training kicks in gradually.
A.2 Proof of the Theorem
Theorem 1 (Verification. The verifier can verify whether the trained model from the designer satisfies
the specifications, by generating a proof of the same). Letx ∈ X be the input, Z0 = e(x) be the latent
code, fd be a feed-forward neural network with bounded derivatives, and Si,in = {Z : ||Zi - Z0,i||p ≤
e} be the set of latent perturbations. Then, the verifier is guaranteed to be able to generate a proof for
verifying whether the linear specification on output logits cTZK + d ≤ 0 ∀Z ∈ Si,in, ZK = fd(Z)
(corresponding to unit test i) is satisfied.
Proof. Revisiting the notation in section 2, consider a network mapping f : X → Z → Y (denote
by fd : Z → Y the downstream task) and a generative model mapping g : X → Z → X, such
that f and g share a common latent space Z by virtue of having a common encoder e(∙) (Figure 2).
Here we provide a proof based on fully-connected feed-forward network architecture for fd, an
arbitrary architecture for f and using the Interval-Bound Propagation (IBP) method for obtaining
output bounds. For a general network architecture for fd, we refer the reader to a recent library
AutoLirpa (Xu et al., 2020).
Given input x ∈ X, let Z0 = e(x) be the latent code, and Si,in = {Z : ||Zi - Z0,i||p ≤ e} be the set
of latent perturbations. Let, Z be d-dimensional. Here, Zi denotes the dimensions(s) of Z that are
15
perturbed, such that Z = z&-i ㊉ z%. Without loss of generality, We can assume all the dimensions in
zi to be contiguous. The verifier can generate a proof by searching for a worst-case violation
max cTzK + d s.t. zk+1 = hk(Wkzk + bk) k = 0, ..., K - 1
For the sake of simplicity, first consider the case Where j = d i.e. all dimensions of z are perturbed.
So, Sin = {z0 : ||z0 - z00||p ≤ . Here, We have denoted by z0 the latent code in Z space, and Will
use zk+1 = hk(Wkzk + bk) k = 0, ..., K - 1 to denote the subsequent intermediate variables in
f1 : Z → Y. Based on Holder’s inequality We can obtain linear bounds for the first variable z1,
[z1,z1] as follows:
Z1 = -e||W 0∣∣q + W 0z0 + b0; Z1 = -e||W 0 ||q + W 0z0 + b0; 1/p +1/q =1
Here, ∣∣∙∣∣q denotes computation of the lq-norm for each row in the matrix and the values of W0, W0
can be computed with the functions described in Appendix A.1 of (Xu et al., 2020) depending on the
type of non-linearity in h0(∙).
Now, when j 6= d i.e. only j dimensions of d are perturbed, without loss of generality, we can
consider all of the j dimensions to be contiguous such that Z0 = Zd0-j ㊉ z0. We can then compute
[z1,zj as follows:
zl = -e||W0||q + W0Z0 + b0; Z1 = -e||W0 ||q + W0Z0+ b0; 1/p +1/q =1
We can compute Zd1-j as follows:
Zd1-j = h0(W0Zd0-j +b0)
Finally, we can obtain [z1,Z1] as follows:
Z1 = zd-j ㊉ Z1; Z1 = z1-j ㊉ Z1
Now, given the bound [z1,Z1] and the relation Zk+1 = hk (WkZk + bk) k = 0,…，K - 1, we can
obtain the bound for ZK, [zk,Zk] by recursively computing successive bounds as follows:
Zk+1 = hk
+b-|W|
Zk + Zk
-2-
+ b + |W|
Here we have assumed that hk is an element-wise monotonic function, which is is true for for most
common actions like ReLU, tanh, sigmoid etc. Using [zk,Zk], the verifier can construct an upper
and lower bound on the solution of maxz∈Si,in cTZK + d
max	cT ZK + d
zκ <z<zκ
It is important to note that the proof the verifier generates to check whether the specification holds
relies on computing an upper bound ZK and checking whether the upper bound ZK < 0, If the upper
bound is not tight, then there may be cases when the true solution < 0 (meaning the specification in
Theorem 1 is indeed satisfied) while the upper bound ZK > 0 (implying the verifier will state that
the specification is not satisfied). This form of err on the side of caution is a natural consequence of
proof by generating a worst case example, and follows directly from IBP.
□
16
A.3 Implementation Details
We use a 32-GB V100 GPU for all our experiments. The implementation is in Python with a
combination of PyTorch and Tensorflow deep learning libraries. Details about the specific experiments
are mentioned below:
ImageNet. We use a pretrained BigBiGAN https://colab.research.google.com/
github/tensorflow/hub/blob/master/examples/colab/bigbigan_with_tf_
hub.ipynb as the base generative model, such that the encoder encodes images to the latent space.
The downstream classifier from the latent space consists of 5 fully connected layers with ReLU
non-linearities. We use ADAM optimizer with a learning rate of 0.0005. For each value of L2
perturbation in Table 1, we train for 100 epochs, using the AutoLiRPA library’s implementation of
CrownIBP https://github.com/KaidiXu/auto_LiRPA. The first 5 epochs are standard
training for warming up, and the rest of the epochs are robust training. The training time for 100
epochs is about 8 hours.
CheXpert. We train a PGGAN https://github.com/tkarras/progressive_
growing_of_gans on the CheXpert dataset. In order to learn latent embeddings from raw
images, we need an inference mechanism. For this, we perform GAN inversion of this pretrained
generator, using https://github.com/davidbau/ganseeing/blob/release/run_
invert.sh to obtain an encoder for projecting images to the latent space. The downstream
classifier from the latent space consists of 5 fully connected layers with ReLU non-linearities.
We use ADAM optimizer with a learning rate of 0.0005. For each value of L2 perturbation in
Table 1, we train for 100 epochs, using the AutoLiRPA library’s implementation of CrownIBP
https://github.com/KaidiXu/auto_LiRPA. The first 5 epochs are standard training for
warming up, and the rest of the epochs are robust training. The training time for 100 epochs is about
7 hours.
LSUN and FFHQ. We train StyleGAN models https://github.com/NVlabs/
stylegan2-ada-pytorch on the FFHQ and LSUN datasets, and perform GAN inver-
sion using https://github.com/genforce/idinvert_pytorch to obtain respective
encoders for a disentangled latent space. The downstream classifier from the latent space consists of
5 fully connected layers with ReLU non-linearities. We use ADAM optimizer with a learning rate of
0.0005. For each value of L2 perturbation in Table 1, we train for 100 epochs, using the AutoLiRPA
library’s implementation of CrownIBP https://github.com/KaidiXu/auto_LiRPA. The
first 5 epochs are standard training for warming up, and the rest of the epochs are robust training. The
training time for 100 epochs is about 7 hours each.
real	0.5	1.0	2.0	4.0	6.0
Figure 5: Each column shows images from a aprticular category that are presented in the HITs for the Amazon
MTurk Experiment. For each HIT, we show pairs of images such that one image is from the real category and
the other image is a genertaed image from one of the other categories.
17
Table 6: In this experiment, we pre-train the classifier through certified training with = 1.0 and perform
verification for different values of . Verified Error fraction on the FFHQ and LSUN datasets. For FFHQ, the
task is to classify whether eyeglasses are present, and the unit tests correspond to variations with respect to pose
and expression of the face. For LSUN Tower, the task is to classify whether green vegetation is present on the
tower, and the unit tests correspond to variations with respect to clouds in the sky and brightness of the scene.
Results are on the held-out test set of the dataset and correspond to training the classifier with the same during
training. S.D. is over four random seeds of training. Lower is better.
FFHQ	= 0.5(25%)	= 1.0(50%)	e = 1.5(75%)	e = 2.0(100%)	e = 3.0(150%)
Pose test	0.005±0.01	0.20±0.05	0.39±0.02	0.51±0.04	0.56±0.05
Expression test	0.005±0.005	0.13±0.04	0.33±0.04	0.57±0.01	0.58±0.03
LSUN Tower	= 0.5(25%)	= 1.0(50%)	e = 1.5(75%)	e = 2.0(100%)	e = 3.0(150%)
Cloud test	0.01±0.01	0.14±0.02	0.36±0.04	0.47±0.06	0.52±0.04
Brightness test	0.005±0.01	0.18±0.03	0.38±0.05	0.52±0.06	0.57±0.04
A.4 Details about the Amazon MTurk study
Here we provide more details about the setup for the Amazon MTurk study whose results we presented
in Fig. 4. We designed each Human Intelligence Task (HIT) to be comprised of 10 pairs of images
sequentially presented, such that each pair consists of a real and a generated image. For different
values of we show examples of images from each of the categories presented in the HITs through
Fig. 5.
We recruited participants only from the “Master Worker” category. As defined in https://www.
mturk.com/worker/help, “a Master Worker is a top Worker of the MTurk marketplace that
has been granted the Mechanical Turk Masters Qualification.” These Workers have consistently
demonstrated a high degree of success in performing a wide range of HITs across a large number of
Requesters. The participants were paid 0.5 dollars for each HIT (i.e. for 10 pairs of images). Each
pair was shown to the participants for 2 seconds, after which the images disappeared from the screen,
but they could take as much time as they wanted to decide and form a response. We restricted the
number of HITs per participant to 20.
The participants were shown the exact text below: Choose the more realistic image between options
A and B. For each pair, you have 5 seconds to view the image and unlimited time to make the decision.
Each task consists of 10 image pairs.
A.5 Results of verification with pre-trained classifier
In this experiment, we pre-train the classifier through certified training with = 1.0 and perform
verification for different values of .
A.6 Extended Related Works
Adversarial robustness. Multiple prior works have studied a subset of the audit problem in terms of
adversarial robustness (Carlini et al., 2019; Jia et al., 2019; Gowal et al., 2018; Zhang et al., 2018;
Singh et al., 2019; Weng et al., 2018; Singh et al., 2018; Wang et al., 2018; Raghunathan et al.,
2018a;b; Dvijotham et al.). Adversarial examples are small perturbations to the input (for images at
the level of pixels; for text at the level of words) which mislead the deep learning model. Generating
adversarial examples has been widely studied (Zhao et al., 2017; Athalye et al., 2018; Carlini &
Wagner, 2017a;b; Goodfellow et al., 2015; Madry et al., 2018; Papernot et al., 2016; Xiao et al.,
2019b; 2018b;c; Eykholt et al., 2018; Chen et al., 2018; Xu et al., 2018; Zhang et al., 2019a) and
recent approaches have devised defenses against adversarial examples (Guo et al., 2018; Song et al.,
2017; Buckman et al., 2018; Ma et al., 2018; Samangouei et al., 2018; Xiao et al., 2018a; 2019a; Xie
et al., 2020; Cohen et al., 2019; Gowal et al., 2018; Zhang et al., 2019b). Some of these approaches
like (Gowal et al., 2018) and (Zhang et al., 2019b) provide provable verification bounds on an output
specification based on the magnitude of perturbations in the input. These approaches have largely
showed robustness and verification to norm bounded adversarial perturbations without trying to
verify with respect to more semantically aligned properties of the input. Some papers (Mohapatra
et al., 2020; Ruoss et al., 2020; Balunovic et al., 2019)consider semantic perturbations like rotation,
translation, occlusion, brightness change etc. directly in the pixel space, so the range of semantic
18
variations that can be considered are more limited. This is a distinction with AuditAI, where by
perturbing latent codes directly (as opposed to pixels), the range of semantic variations captured
are much larger. In (Wong & Kolter, 2020), the authors model perturbation sets for images with
perturbations (like adversarial corruptions) explicitly such that a conditional VAE can be trained.
While, for AuditAI, we identify directions of latent variations that are present in pre-trained GANs,
and as such do not require explicit supervision for training the generative model, and can scale to
datasets like ImageNet and CheXpert, and high dimensional images like that in FFHQ.
Neural network interpretability. With the rapid rise of highly reliable generative models, many
recent works have sought to analyze the latent space of such models and how the latent space can be
manipulated to obtain semantic variations in the output image. (Bau et al., 2021) use text embeddings
from CLIP (Radford et al., 2021) to perform directed semantic changes in the generated images, for
example introduction of a particular artifact in the room or change in the color of a bird. (Bau et al.,
2020a; Shen & Zhou, 2020; Shen et al., 2020) identify latent code dimensions corresponding to pose
of the body/face, expression of the face, orientation of beds/cars in the latent space of generative
models like PGGAN (Karras et al., 2017) and StyleGAN (Karras et al., 2019). By manipulating these
latent dimensions, the output images can be controllably varied. In addition, there have been other
papers that sought to understand what the neural network weights are learning in the first place (Bau
et al., 2020b; Yeh et al., 2020) in an attempt to explain the predictions of the networks.
Auditing. While we proposed a framework for auditing of DL models, a complimentary problem
is to audit datasets that are used for training DL models. In (Gebru et al., 2018), the authors
proposed a checklist of questions to be answered while releasing datasets that relate to the motivation,
composition, collection process, processing, and distribution of the dataset. In (Mitchell et al., 2019),
the authors proposed a checklist of questions that should be answered while releasing DL models
that relate to their motivation, intended use cases, training details, and ethical considerations. These
papers formed a part of our motivation and provided guidance in formalizing the audit problem for
DL models.
19