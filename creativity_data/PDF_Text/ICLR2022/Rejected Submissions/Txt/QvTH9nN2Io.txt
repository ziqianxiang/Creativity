Under review as a conference paper at ICLR 2022
Relative Entropy Gradient Sampler for Un-
normalized Distributions
Anonymous authors
Paper under double-blind review
Ab stract
We propose a relative entropy gradient sampler (REGS) for sampling from unnor-
malized distributions. REGS is a particle method that seeks a sequence of simple
nonlinear transforms iteratively pushing the initial samples from a reference dis-
tribution into the samples from an unnormalized target distribution. To determine
the nonlinear transforms at each iteration, we consider the Wasserstein gradient
flow of relative entropy. This gradient flow determines a path of probability dis-
tributions that interpolates the reference distribution and the target distribution. It
is characterized by an ODE system with velocity fields depending on the density
ratios of the density of evolving particles and the unnormalized target density. To
sample with REGS, we need to estimate the density ratios and simulate the ODE
system with particle evolution. We propose a novel nonparametric approach to esti-
mating the logarithmic density ratio using neural networks. Extensive simulation
studies on challenging multimodal 1D and 2D mixture distributions and Bayesian
logistic regression on real datasets demonstrate that the REGS outperforms the
state-of-the-art sampling methods included in the comparison.
1 Introduction
Sampling from unnormalized distributions plays a fundamental role in statistical inference and
machine learning. This problem is frequently encountered in Bayesian statistics. Conducting
Bayesian analysis requires evaluation of multi-dimensional integrals where analytical expressions for
unnormalized posterior distributions are usually not available. Consequently, sampling is necessary
for Monte Carlo approximation of these integrals. In this work, we propose a general purpose
sampling algorithm for unnormalized distributions.
Markov chain Monte Carlo (MCMC) methods (Andrieu et al., 2003; Brooks et al., 2011) are widely
used to sample from unnormalized distributions. Sampling with MCMC relies on defining an
appropriate transition kernel to construct a Markov chain whose equilibrium distribution is precisely
the target distribution. Based on rejection sampling, the MetroPolis-Hastings algorithm (Metropolis
et al., 1953; Hastings, 1970; Tierney, 1994; Dunson & Johndrow, 2019) provides a flexible framework
for general MCMC sampling. To implement a Metropolis-Hastings algorithm, one needs to specify a
proposal density and an acceptance policy. However, without a careful design of these two aspects,
the Metropolis-Hastings algorithm can be inefficient due to strong correlations, slow mixing, or low
acceptance rates, especially in the large-scale and high-dimensional settings. Moreover, proposals
through discretizing some continuous processes like Langevin diffusion and Hamiltonian dynamics
are introduced (Roberts & Tweedie, 1996; Roberts & Stramer, 2002; Duane et al., 1987; Neal, 2011;
Hoffman & Gelman, 2014) and further enhanced by stochastic gradient estimation (Welling & Teh,
2011; Chen et al., 2014).
Variational Bayesian inference (Beal, 2003), often simply referred to as variational inference (VI)
(Wainwright & Jordan, 2008; Blei et al., 2017), is another prominent approach to sampling from
unnormalized distributions. VI approximates the unnormalized posterior distribution with a restricted
parametric variational posterior distribution by minimizing the Kullback-Leibler (KL) divergence
between them. Since the true posterior distribution is intractable, VI turns to maximize a surrogate
variational objective called the evidence lower bound (ELBO). However, one is required to trade off
the parameterization flexibility of variational posteriors against the optimization complexity of ELBO
in practice.
1
Under review as a conference paper at ICLR 2022
(a) 9 Gaussians
(b) 25 Gaussians
(c) 49 Gaussians
(d) 81 Gaussians
Figure 1: Scatter plots of generated samples and histograms of generated sample counts according to
the nearest neighbor mode by REGS for mixtures of 9, 25, 49, and 81 Gaussians with equal weights.
As the plots indicate, generated samples by REGS cover every component of the mixture distributions
and are nearly equally allocated to all components.
In the spirit of VI, particle-based variational inference (ParVI) (Liu & Wang, 2016; Chen et al.,
2018; Zhu et al., 2020) iteratively optimizes a set of particles to mimic a functional gradient descent
for minimizing the KL divergence. ParVI seeks to move a variational distribution towards the
unnormalized target distribution, along a steepest descent direction of the KL divergence. In a
continuous view, these movements of variational distributions can be understood as a gradient flow in
probability measure spaces (Liu et al., 2019a;b). A key part of ParVI is how to estimate the desired
steepest descent direction (i.e., functional gradient) from the evolving random particles. An elegant
approach is the Stein variational gradient descent (SVGD) Liu & Wang (2016). In SVGD , the
functional gradient descent is embedded in a reproducing kernel Hilbert space (RKHS), which is
further recognized as a gradient flow under the Stein geometry (Liu, 2017; Lu et al., 2019; Duncan
et al., 2019). A drawback of SVGD is that it tends to collapse at part of the modes of the target, due
to a negative correlation between the data dimensionality and the repulsive force in the RKHS (Zhuo
et al., 2018).
In this work, we propose a relative entropy gradient sampler (REGS) for sampling from unnormalized
target distributions. To approximate a target distribution, we consider the Wasserstein gradient flow
of relative entropy (or KL divergence), named relative entropy gradient flow. The relative entropy
gradient flow represents a path of probability distributions that follows the functional gradient descent
direction of relative entropy. There exists an ODE system of random particles that uniquely determines
the spatial and temporal dynamics of the relative entropy gradient flow. Therefore, to sample with
REGS, we only need to simulate the ODE system with particle evolution. Evaluating the velocity
fields of this ODE system can be transformed into estimating the logarithmic density ratio between
the density of evolving particles and the unnormalized target density. Based on this observation,
we propose a novel logarithmic density ratio estimation method for unnormalized distributions. By
alternating between particle evolution and velocity field estimation, we can collect a set of stable
particles which are approximately distributed as the target distribution. Our contributions can be
summarized as follows:
(1)	Building upon the relative entropy gradient flow, we propose the relative entropy gradient sampler
(REGS) for unnormalized target distributions. REGS preserves high efficiency and strong stability
with respect to increasing singularity in mixtures of Gaussians, when the number of components
increases (as shown in Figure 1), the variance of each component decreases, and the distance
between any two components increases.
(2)	We propose to directly estimate velocity fields of the relative entropy gradient flow as gradients
of logarithmic density ratios, that is computationally stable and efficient.
(3)	We develop a nonparametric approach to estimating the density ratio between an unnormalized
density and an underlying density represented by samples, which is of independent interest.
(4)	We present experimental comparisons on varieties of multi-mode synthetic data and benchmark
data and demonstrate that REGS is a more accurate sampler than the popular samplers including
ULA, MALA and SVGD.
Related work The proposed REGS is most related to sampling methods based on the relative entropy
gradient flow, in particular, the recently proposed SVGD (Liu & Wang, 2016; Liu, 2017), which
estimates the velocity fields of the relative entropy gradient flow in a reproducing kernel Hilbert space.
See also Korba et al. (2020); Salim et al. (2021; 2020) for theoretical analysis of SVGD. In contrast,
REGS approximates the velocity fields based on a novel logarithmic density ratio estimation approach
with deep neural networks. The undesirable mode collapse feature of SVGD is not inevitable for
2
Under review as a conference paper at ICLR 2022
REGS since the approximation and expressive powers of deep neural networks is known to surpass
those of kernel methods.
MCMC algorithms constructed from overdamped Langevin diffusion can be studied as discretization
of the relative entropy gradient flow (Jordan et al., 1998). Based on the Euler-Maruyama discretization
of overdamped Langevin diffusion, unadjusted Langevin algorithm (ULA) (Roberts & Tweedie, 1996)
aims at generating samples from an approximation of the unnormalized target, but is biased for fixed
step size. When a Metropolis-Hastings step is included, Metropolis-adjusted Langevin algorithm
(MALA) (Roberts & Tweedie, 1996) is capable of correcting the bias, but leaves a large number of
intermediate samples rejected. In REGS, one only needs to estimate the deterministic velocity fields
of the relative entropy gradient flow, which differs from running ULA and MALA with randomness
from diffusion processes. All particles produced by REGS are generated from an approximation of
the target distribution.
Another line of work (Gao et al., 2019; 2021) uses Wasserstein gradient flows of f -divergences for
generative learning with samples from the underlying target distribution. In their work, evaluating
velocity fields of gradient flows also boils down to estimating density ratios. However, our current
problem is to sample from an unnormalized target density. Furthermore, we propose a novel density
ratio estimation procedure when the target distribution is only known up to a normalizing constant.
Notation Let P2(X) be the space of Borel probability measures on a support space X ⊂ Rd with
a finite second moment, and let P2a(X) be a subspace of P2(X) whose measures are absolutely
continuous w.r.t. the Lebesgue measure. All probability measures we considered thereinafter are
assumed to belong to P2a(X). To ease the notation, we use probability density functions such as
q(x), p(x), x ∈ X to express probability distributions in P2a (X). Let (P2a(X), W2) denote the metric
space P2a (X ) endowed with the 2-Wasserstein distance W2, which is referred to as the quadratic
Wasserstein space. We use V and Div to denote the gradient operator and the divergence operator,
respectively.
2 Problem formulation
Consider an unnormalized probability density function u : X → [0, ∞), where X ⊆ Rd is the
support of u. Suppose u has an intractable normalizing constant Z = X u(x)dx < ∞. Our goal is
to generate random samples from the underlying distribution p ∈ P2a(X), whose probability density
function is only known up to proportionality, i.e., p(x) = u(x)/Z, x ∈ X . The basic idea is to
gradually optimize samples from a given distribution q ∈ P2a (X ) to approximate samples from p,
where it is easy to sample from q. Optimizing samples leads to functional optimization of distributions.
We then introduce the classical relative entropy as the functional optimization objective. The relative
entropy, a.k.a., the KUllbaCk-Leibler divergence, for q,p ∈ P2(X) is the average logarithmic density
ratio, which is defined as
(qkp) =
X
q(x) log
(1)
It holds that Dre(qkp) ≥ 0 and Dre(qkp) = 0 iff q(x) = p(x) a.e. x ∈ X. Moreover, we denote the
relative entropy functional as
FH ：= Dre(∙kp): Pa(X) → [O, ∞].
(2)
To sample from the unnormalized density u = pZ, we consider the functional minimization problem
min F[q],
q∈P2a(X)
(3)
where F[q] is always minimized at the underlying target distribution p, i.e., q(x) = p(x) a.e. x ∈ X.
In a nutshell, problem (3) is an energy functional minimization problem in a metric space. To
minimize the energy functional F, it suffices to move along the corresponding gradient flow in a
metric space until the flow converges. For example, a gradient flow in the Euclidean space refers to a
curve whose tangent space contains the steepest descent direction of a given function. Analogously, a
gradient flow in the space of probability measures means a curve that points in the steepest descent
direction of a given energy functional. When equipped with the 2-Wasserstein distance, minimization
of the energy functional F naturally corresponds to a continuous path on the quadratic Wasserstein
space of distributions, which is commonly known as a Wasserstein gradient flow of the relative
entropy. We call this flow a relative entropy gradient flow for briefness.
3
Under review as a conference paper at ICLR 2022
3 Relative entropy gradient flow
In this section, we briefly review the formulation of relative entropy gradient flow and its connections
to differential equations. We consider the properties of gradient flows in the quadratic Wasserstein
space (P2a(X), W2). Recall that F in (2) is the relative entropy functional defined on (P2a (X), W2).
One can show that a curve {qt}t≥0 in (P2a(X), W2) is a relative entropy gradient flow of F if it
satisfies the continuity equation (Ambrosio et al. (2008), page 295 and Villani (2008), page 631),
∂tqt = Div (qtV乎),
δqt
(4)
where qt(x) = q(t, x) evolves over time, δF^ = log q is the first variation of the energy functional
F at qt, and V δFqqt] is the Euclidean gradient of δFqqt]. Here, We identify the gradient as the relative
entropy gradient, which is defined by
VW2 F[qt] := V 爷=V lθg P
(5)
Moreover, the relative entropy F dissipates along the relative entropy gradient flow {qt}t≥0 at the
rate (Ambrosio et al. (2008), page 295)
∂t F[qt] = -Eqt [kVW2 F[qt]k2].	(6)
Therefore, the relative entropy gradient flow {qt}t≥0 eventually converges to the target distribution p
as t → ∞. As pointed out in Ambrosio et al. (2008) (Page 175), under mild conditions the continuity
equation (4) concerning {qt}t≥0 determines a time-inhomogeneous Markov process {Xt}t≥0 that
starts at a random particle Xo 〜qo and follows the particle evolution dynamics
ddX = Vt(Xt), Xt 〜qt, t ≥ 0.	(7)
Note that the velocity fields
Vt = -VW2 F[qt] = V log J ≥ 0	(8)
qt
drive the evolution of the particle Xt in the Euclidian space, which results in the transport of qt in
(P2a (X ), W2 ). An important observation is that
Vt = V log P = V log U, t ≥ 0.	(9)
qt	qt
Therefore, the velocity fields do not involve the unknown normalizing constant Z. This is the key
motivation for us to use the relative entropy gradient flow in the proposed method.
4 Sampling as particle evolution
As indicated by the energy dissipation of relative entropy F in (6), running the relative entropy
gradient flow {qt }t≥o dynamics can provide a nice approximate solution to the functional minimiza-
tion problem (3) when time t is large enough. Therefore, to sample from the target distribution P,
it is appropriate to simulate the relative entropy gradient flow {qt }t∈[o,T] with the time horizon T
sufficiently large. A natural strategy is to discretize the particle evolution form of relative entropy
gradient flow in (7) with forward Euler iterations (LeVeque, 2007) as follows,
Xk+1 = Xk + SVk(Xk), Xo ~ qo, k = 0,1,...,K - 1,	(10)
with the velocity field at step k
Vk = -Vw2F[qk] = V log -p,	(11)
2	qk
where s > 0 is a tunable small step size, K = bT /sc is the number of iterations and qk is the
corresponding discretized gradient flow at step k, i.e., Xk 〜qk. Combining the expressions in (10)
and (11), we have that the iterations progress according to
Xk+1 = Xk + sV log -, X0 〜qo, k = 0, 1,...,K - 1.	(12)
qk
4
Under review as a conference paper at ICLR 2022
In principle, it is necessary to evaluate the velocity field Vk = V log(p∕qk) each iteration in (12). By
(9), the velocity field of the relative entropy gradient flow can be simplified to
Vk = V log -p = V log —, k = 0,1,...,K - 1,	(13)
qk	qk
where u = Zp is the given unnormalized density of the target distribution p. Then only the density
qk remains unknown for evaluating the velocity field. Ideally, qk can be estimated by evolving a
large number of particles {Xki }iN=1. However, direct estimation of qk is difficult due to the curse
of dimensionality and the potential expensive computation cost for different ks. Our solution is to
approximate the velocity field (13) as a whole.
Assuming a nice approximation Vbk of the velocity field (13) is provided, then one can implement the
following iterations for approximately sampling from qK with no effort,
Xk+1 = Xk + SVk(Xk), Xo 〜qo, k = 0,1,...,K - 1.	(14)
Through the iterations above, We can collect Xk 〜qk ≈ qk, k = 1, 2,...,K. We will discuss
approximation of the velocity field Vk = V log(u∕qk) from the perspective of estimating the
logarithmic density ratio log(u∕qk) in the next section.
5 Logarithmic density ratio estimation and the relative entropy
GRADIENT SAMPLER
In this section, we first propose a novel estimation procedure of the logarithmic density ratio log(u∕q)
based on an unnormalized density u and random samples from q.
We use a model ratio R : X → [0, ∞) to fit the true ratio R?uq = u∕q between a density q and
an unnormalized density u. Let g : R → R be a differentiable and strictly convex function. A
Bregman score (Dawid, 2007; Gneiting & Raftery, 2007; Kanamori & Sugiyama, 2014) with the
base probability measure q ∈ P2a (X ) to measure the discrepancy between R and R?uq is defined by
B(R) = EX〜q[g0(R(X))R(X) - g(R(X))] - EX〜W
g0(R(X)) ,
where w ∈ P2a(X) is an introduced and reference distribution for calculating the integral involving
u. It should be easy to sample from w and the support of ushould be included in the support of w.
Additionally, B(R) ≥ B(R?uq), where the equality holds iff R(x) = R?uq(x) (q, u)-a.e. x ∈ X.
In this work, we take g(x) = x log(x) - x. We use this function for two reasons: (a) convexity, this
is to satisfy the basic requirement of the Bregman score; (b) cancellation of the unknown normalizing
constant Z ofu. Simple calculation shows that B(R) can be written as
B(R) = EX〜q[R(X)] - EX〜W [uTX) log(R(X))
(15)
Recall that the true density ratio Ru?q can be factorized as Ru?q = u∕q = Z(p∕q). Thus the numerical
scale of the true density ratio Ru?q hinges on two factors, i.e., the normalizing constant Z ofuand
the standard density ratio p∕q. Since numerical scales of these factors are difficult to determine in
applications, the induced numerical instability can deteriorate the density ratio estimate. In order
to prevent the density ratio estimation from such instability, we consider the model ratio R on the
logarithmic scale. This will also release the nonnegative constraint on R as a byproduct.
From now on, we denote Du?q = log(Ru?q), D = log(R) : X → R. Then B(D) can be rewritten as
B(D) = Ex^[exp(D(X))] - EX〜做[uX)D(X)1 .	(16)
w(X)
It can be shown that the logarithmic density ratio Du?q is identifiable at the population level by
minimizing (16) with respect to D.
Theorem 1. For B(D) defined in (16), we have Du?q ∈ arg minD B(D). In addition, for any D with
EX〜W hW(X)D(X)] < ∞, B(D) ≥ B(Duq), with equality iff D(X) = D?q(x) (q, u)-a.e. X ∈ X.
5
Under review as a conference paper at ICLR 2022
Algorithm 1: REGS: Relative entropy gradient sampler
Input: u = Zp	// unnormalized target density
step size s > 0, an integer K > 0,	// step size, maximum loop count
i
X0 〜qo, i = 1, 2,...,n	// initial particles
w ∈ P2a(X)	// reference distribution
k — 0
while k < K do
Yk 〜w, i = 1,2,...,n	// reference samples
DDΦk ∈ argminDφ * 1 * * * * 6 Pn=IheXp(Dφ(Xk)) - WU(Yk)Dφ(Yi)i	// log density ratio
Vk(x) = VDbφk (x)	// velocity field
ii	i
Xki +1 = Xki + svbk (Xki), i = 1, 2, . . . , n	// update particles
k — k + 1
end
i
Output: XK 〜qκ ≈ p, i = 1, 2,...,n	// output particles
Based on Theorem 1, we can estimate the unknown logarithmic density ratio Du?qk = log(u/qk) with
a deep neural network Dφ with parameter φ through the sample version of (16). Let {Xki }in=1 be i.i.d.
samples from Gk ≈ qk and {Y^}n=ι be i.i.d. samples from a reference distribution w. We solve the
following deep nonparametric estimation problem via stochastic gradient descent (SGD) for Dbφk
1n
DDφk ∈ arg min B(Dφ) = - V"
Dn
Dφ	i=1
u(Yi)
exp(Dφ(Xk)) - -(Y⅛Dφ(Yk).
w(Yk)
(17)
With the logarithmic density ratio estimator Dbφk, the velocity field vk in (13) can be approximately
computed by V = ND@卜.By considering sampling as a particle evolution process discussed in
n	a ei-ɪ/ɔe i	, 1	∙ , ∙ i	, ∙ i C -χr∙i 1 r>	∙ ,ι ∙ ,	, ∙	∙ /1 λ∖ t` ιι
Section 4, REGS updates the initial particles {X0i}in=1 with iterations in (14) as follows:
Xek+1 = Xk + SVk (Xk), Xo 〜qo, i = 1, 2,...,n, k = 0,1,...,K — 1.	(18)
We summarize the proposed REGS for sampling from an unnormalized density in Algorithm 1.
6 Numerical experiments
We evaluate REGS on a large number of 1D and 2D mixture distributions and test its stability in the
high-dimensional setting with multivariate Gaussian distributions. We also use REGS to perform
Bayesian logistic regression on benchmark datasets. For comparison, we consider three existing
methods including SVGD (Liu & Wang, 2016), ULA (Roberts & Tweedie, 1996) and MALA (Roberts
& Tweedie, 1996). All experiments are done using a NVIDIA Tesla K80 GPU and common CPU
computing resources. The neural network architecture, hyperparameter values, dataset descriptions,
and additional experimental results are given in the appendix. The python code of REGS is available
at https://github.com/anonymous/REGS.
6.1 Mixture distributions
We run REGS and SVGD, ULA and MALA to generate 2000 particles for mixtures of 2, 8 and 9
Gaussians (see Scenarios 4, 5, 6 in Appendix B), and 5000 particles for a mixture of 25 Gaussians
(see Scenario 9 in Appendix B). The sampling qualities of these algorithms are compared by scatter
plots with density contours of target mixture distributions. We classify all scatter points with labels
according to the nearest mode, and plot the histograms of the label counts.
6
Under review as a conference paper at ICLR 2022
(a) REGS
*■
*“
二- 卜~二
∣r Mill Hill
tc 孑 111U Jllll
(b) SVGD-
■，
一
»	⅛ V I V
— —
ULA With 50 chains
(C)
t4-∣—l I I I
：111111
(d) MALA
∙*
w∙-
■ ■ ■ ■

Λ




Figure 2: Mixtures of 8 Gaussians With equal weights: scatter plots and histograms of generated
samples by (a) REGS, (b) SVGD, (c) ULA With 50 chains, and (d) MALA With 50 chains. From left
to right in each subfigure, the variance of Gaussians varies from σ2 = 0.2 (first column), σ2 = 0.1
(second column), σ2 = 0.05 (third column), to σ2 = 0.03 (fourth column).
Gaussian mixtures with equal weights Figure 2 shoWs the scatter plots and histograms of samples
generated by (a) REGS, (b) SVGD, (c) ULA With 50 chains, and (d) MALA With 50 chains from
mixtures of 8 Gaussians With equal weights. It shoWs that REGS is able to explore all the components
in the mixture distribution nearly equally. HoWever, SVGD is only able to find part of the modes, as
indicated in Figures 2(b). Figures 2(c) and 2(d) shoW that MALA and ULA With 50 chains find all
modes but With unequal Weights, especially as the variance of each component decreases.
(c) ULA with 50 chains
!： I I I I d I I I I d I I I I I： I I I I
I 二 ¾	¾ q
:lɪɪɪjl dɪɪɪjj dɪɪɪjj :lɪɪɪjj
>■)*>∙>•	>af*t*ι* >af⅜t*ι*	*♦*•♦，，，
— — — —
(b) SVGD
(d) MALA with 50 chains
Figure 3: Mixtures of 8 Gaussians with unequal weights: scatter plots and histograms of generated
samples by (a) REGS, (b) SVGD, (c) ULA with 50 chains, and (d) MALA with 50 chains. From left
to right in each subfigure, the variance of Gaussians varies from σ2 = 0.2 (first column), σ2 = 0.1
(second column), σ2 = 0.05 (third column), to σ2 = 0.03 (fourth column).
Gaussian mixtures with unequal weights Figure 3 shows the scatter plots and histograms of
samples generated by (a) REGS, (b) SVGD, (c) ULA with 50 chains, and (d) MALA with 50 chains
from mixtures of 8 Gaussians with unequal weights (1, 1, 1, 1, 3, 3, 3, 3)/16. Figure 3(a) shows that
the samples generated by REGS have the correct weights. Figures 3(c) and 3(d) indicate that ULA
7
Under review as a conference paper at ICLR 2022
o a	UnUnaDS»	38
5~EW⅛ E⅞s
» ιαo uo 2m so ∞
Qhiieilkii «
UniS0	an za m
ΓΛπaηim «
nrhd-BlaaaDMe
mo 1» 2m s» am
BmcnfE «

Figure 4: Monte Carlo estimates of E[h(X)] versus d for d-dimensional multivariate Gaussian
distributions of X, For d increasing from 10 to 300 with lag 10. From left to right, h(x) = αTx,
(αTx)2, exp(αTx), and 10 cos(αTx + 1/2) with α ∈ Rd, kαk2 = 1. The curves represent the
estimates using the target samples ("true", blue solid line) and the generated samples by REGS (red
solid line), SVGD (green dash line), ULA_1: gray dotted line, MALA_1: pink dotted line, ULA_50:
oringe dotted line, and MALA_50: orchid dotted line.
and MALA assign particles to modes with incorrect weights. Moreover, the quality of the samples
generated by SVGD, ULA and MALA deteriorates as the number of modes increases, while the
performance of REGS remains stable. We also included the results from ULA and MALA with a
single chain in Figures 7 and 10 in Appendix D, which show that these samplers have difficulty with
multimodal distributions if only a single chain is used.
To further analyze the performance, we report the Monte Carlo estimates of E[h(X)] using a test
function h in Table 1, where h(x) = αTx, (αTx)2, and 10 cos(αTx + 1/2) with α ∈ R2, kαk2 = 1,
and X is distributed as various Gaussian mixtures with unequal weights. By comparing the Monte
Carlo estimates of E[h(X)] using the samplers with the values based on target samples, we see
that REGS performs better and is more stable than SVGD, ULA and MALA, especially when
h(x) = 10 cos(αTx + 1/2). We include additional numerical results including more scatter plots and
histograms (Figure 6-10) and Monte Carlo estimates with equal wieights (Table 6) in Appendix D.
Table 1: Monte Carlo estimates of E[h(X)] with four samplers for 2D mixtures of Gaussians random
vectors X with unequal weights. “Target" denotes the Monte Carlo estimate with target samples.
ULA_k and MALA_k denote the ULA and MALA with k chains, respectively.
Distributions	σ2	h(x) = αTx										h(x) = (αTx)2				h(x) = 10 cos(αTx + 1/2)						
		Target	REGS	SVGD	ULA_1	MALA_1	ULA_50	MALA_50	Target	REGS	SVGD	ULA_1	MALA_1	ULA_50	MALA_50	Target	REGS	SVGD	ULA_1	MALA_1	ULA_50	MALA_50
2gaussian	0.2	-0.71	-0.61	-0.05	-2.86	-2.85	0.46	0.00	2.20	2.20	32.40	8.39	8.35	8.16	8.24	3.39	3.08	-7.92	-6.42	-6.29	-7.73	-7.43
	0.1	-0.71	-0.47	-0.07	-2.83	-2.82	0.45	0.00	2.12	2.10	32.20	8.11	8.09	8.10	8.13	3.49	2.80	-8.11	-6.54	-6.39	-8.10	-7.81
	0.05	-0.71	-0.48	-0.03	-2.84	-2.84	0.45	-0.00	2.07	2.05	32.10	8.10	8.15	8.05	8.10	3.58	2.91	-8.16	-6.75	-6.60	-8.32	-7.94
	0.03	-0.70	-0.52	0.03	-2.82	-2.83	0.45		2.03	2.03	31.90	7.98	8.18	8.04		3.69	3.08	-8.25	-6.70	-6.29	-8.40	
8gaussian	0.2	-1.20	-1.20	-0.06	-0.49	-1.72	0.09	-1.30	8.23	8.20	8.05	9.93	8.63	7.56	8.54	-3.16	-3.16	1.46	-5.24	-5.70	-2.71	-3.48
	0.1	-1.21	-1.15	-0.02	0.00	-0.68	0.40	-0.22	8.11	8.08	8.30	0.10	2.08	7.94	8.63	-3.31	-3.30	1.33	8.35	4.63	-3.30	-3.29
	0.05	-1.21	-1.12	-0.01	0.00	-2.83	0.50	-0.27	8.06	8.01	8.09	0.05	8.09	8.05	8.24	-3.41	-3.35	1.45	8.54	-6.53	-3.56	-2.43
	0.03	-1.21	-1.12	-0.03	0.00	-2.66	0.50	-0.28	8.05	8.00	8.10	0.03	7.95	8.03	8.55	-3.46	-3.40	1.41	8.64	-5.22	-3.59	-3.14
25gaussian	0.2	1.00	1.00	1.64	1.17	0.94	0.90	0.92	8.05	8.04	9.43	68.02	48.48	7.62	7.88	0.21	0.17	0.12	0.74	0.33	0.27	0.22
	0.1	1.00	1.00	0.04	2.11	0.91	0.98	0.85	7.97	7.94	2.04	53.03	51.55	7.29	7.79	0.18	0.18	3.56	-1.41	-0.28	0.52	0.33
	0.05	1.00	0.91	0.07	1.42	1.16	-0.03	0.46	7.90	7.83	1.07	13.69	47.61	4.79	7.82	0.19	0.17	5.07	-3.30	-0.08	0.53	0.41
	0.03	1.00	0.81	-0.02	0.00	0.27	-0.11	0.27	7.87	7.70	0.96	0.20	53.18	4.75	7.43	0.17	0.16	5.68	8.64	-0.02	0.08	-0.01
6.2	Multivariate Gaussian distribution
Let the target distribution be a d-dimensional Gaussian distribution with mean μ =(1,1, •一，1) ∈ Rd
and covariance matrix Σ ∈ Rd×d, ∑i,j = ρli-jl with P = 0.7. We consider four test functions h(x),
i.e., h(x) = αTx (the first moment), h(x) = (αTx)2 (the second moment), h(x) = exp(αTx) (the
moment generating function), and h(x) = 10 cos(αTx + 1/2) with α ∈ Rd satisfying kαk2 = 1.
For reference, we provide the Monte Carlo estimates of E[h(X)] using target samples. We compare
REGS with SVGD, ULA_1, MALA_1, ULA_50, MALA_50 in Figure 4, the number of particles
is 5000 for each sampler, where ULA_k and MALA_k denote the ULA and MALA with k chians.
For ULA and MALA, because of large variations of the estimates, we repeat the process 10 times
and compute the average as the final estimate. Figure 4 presents these Monte Carlo estimates as d
increases from 10to 300 with step size 10. The logarithm of the estimated E[exp(αTX)] is shown. As
shown in Figure 4, the estimates using REGS and SVGD have smaller fluctuations than those using
ULA and MALA, although all four methods can estimate E[αTX] and E[(αTX)2] well. Moreover,
the third and the fourth panels in Figure 4 show that REGS outperforms SVGD, ULA and MALA
when h(x) = exp(αTx) or 10 cos(αTx + 1/2).
8
Under review as a conference paper at ICLR 2022
6.3	Bayesian logistic regression
We apply REGS to Bayesian logistic regression for binary classification on five datasets, including
Banana, German, Image, Ringnorm, and Covertype. These datasets were analyzed in Liu & Wang
(2016) and the first four datasets had been analyzed in Gershman et al. (2012). We consider a
similar setting to that in (Liu & Wang, 2016; Gershman et al., 2012), which assigns a Gaussian
prior ∏(β∣α) = N(0, α-1I) to the regression coefficient β (including the intercept). We specify the
prior of α as π(α) = Gamma(1, 0.01). For comparison, we consider SVGD, ULA and MALA. The
inference is based on the posterior ∏(β∣data).
These datasets are partitioned randomly into two parts, the training sets (80%) and the test sets (20%).
We repeats the random partition 10 times. We evaluate the classification accuracy on test data with
5000 particles from the posterior. Table 2 lists the averages and standard errors (in parenhteses) of
test accuracy. From Table 2 we can see that REGS is comparable with SVGD, ULA and MALA. For
the Covertype dataset, MALA failed to converge, so no results from it are included in Table 2.
Table 2: Averages and standard errors (in parenhteses) of classification accuracy on test data from
five datasets, d: number of features, N : sample size.
datasets	d	N	Averages of Accuracy (%)					
			REGS	SVGD	ULA_1	MALA_1	ULA_50	MALA_50
Banana	2	5300	54.1 (3.1)	55.5 (2.9)	~55.1 (1.9)~	55.2 (1.9)	55.1 (1.9)	55.2 (1.9)
German	20	1000	77.2 (2.2)	75.6 (1.2)	76.5 (1.8)	76.6 (2.2)	76.6 (2.0)	76.6 (2.1)
Image	18	2086	83.4 (1.5)	82.8 (1.7)	82.7 (2.3)	82.9 (2.3)	82.8 (2.3)	82.8 (2.3)
Ringnorm	20	7400	76.3 (0.9)	75.9 (1.0)	75.7 (1.4)	75.7 (1.4)	75.7 (1.4)	75.2 (1.4)
Covertype	54	581012	75.0 (1.2)	75.6 (0.8)	74.1 (0.3)	-	74.2 (0.4)	-
6.4	Discussion of the experimental results
The experimental results reported above and in the appendix indicate that REGS is capable of
generating better quality samples than SVGD, ULA and MALA from Gaussian mixture distributions.
Also, the results suggest that particles generated by REGS can cross valleys in the landscape of a
multimodal distribution even if they are initialized in a different regions. An intuitive explanation is
as follows. The movement of the REGS particles is determined by the velocity field. If the velocity
field is not zero at a particle, the particle will continue to evolve towards the target distribution.
Moreover, all particles interact with each other through the velocity field, which is beneficial in
sampling from multimodal distributions. For ULA and MALA, there are no interactions among
particles or incentives for particles to cross valleys between two modes, thus it is more difficult for
these methods to sample from multimodal distributions. A possible remedy is to use multiple chains
as we did in the above experiments. To some extend, this alleviates the problem encountered in
sampling from multimodal distributions. However, the success of this strategy depends on the initial
samples being near the modes as well as having the correct proportions of the initial samples being
close to each mode. In comparison, REGS uses a principled way to move particles from an initial
reference distribution to a multimodal distribution, albeit with a higher computational cost.
7	Conclusion
We have introduced REGS, a novel gradient flow based method for sampling from unnormalized
distributions. Extensive numerical experiments demonstrate that REGS performs better than several
existing popular sampling methods in the setting of challenging multimodal mixture distributions.
In future work, we hope to establish the convergence properties of REGS generated sampling
distributions as the numbers of iterations and particles increase.
As with any sampling algorithms, there is a trade-off between sampling quality and computational
efficiency. On one hand, as our numerical experiments demonstrate, REGS can generate samples
with better quality than the three existing methods we considered in the challenging mixture model
settings. On the other hand, REGS is computationally more expensive, as it involves neural network
training in the iterations, compared with existing methods such as ULA and MALA that can be
implemented more quickly. As computational power continues to increase rapidly, REGS can be a
useful addition to the toolkit of sampling methods for multimodal distributions.
9
Under review as a conference paper at ICLR 2022
References
LUigi Ambrosio, Nicola Gigli, and GiUsePPe Savar6. Gradient Flows: in Metric Spaces and in the
Space of Probability Measures. Springer Science & Business Media, 2008.
ChristoPhe AndrieU, Nando de Freitas, ArnaUd DoUcet, and Michael I. Jordan. An introdUction to
MCMC for machine learning. Machine Learning, 50(1):5-43, 2003.
Matthew J Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, University
College London, 2003.
David M. Blei, AlP KUcUkelbir, and Jon D. McAUliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112(518):859-877, 2017.
Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng. Handbook of Markov Chain
Monte Carlo. CRC Press, 2011.
ChangyoU Chen, RUiyi Zhang, Wenlin Wang, Bai Li, and LiqUn Chen. A Unified Particle-oPtimization
framework for scalable bayesian samPling. In UAI, 2018.
Tianqi Chen, Emily Fox, and Carlos GUestrin. Stochastic gradient Hamiltonian Monte Carlo. In
Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine
Learning, volUme 32 of Proceedings of Machine Learning Research, PP. 1683-1691. PMLR,
22-24 JUn 2014.
A PhiliP Dawid. The geometry of ProPer scoring rUles. Annals of the Institute of Statistical
Mathematics, 59(1):77-93, 2007.
Simon DUane, A.D. Kennedy, Brian J. Pendleton, and DUncan Roweth. Hybrid Monte Carlo. Physics
Letters B, 195(2):216-222, 1987.
Andrew Duncan, Nikolas Nusken, and Lukasz Szpruch. On the geometry of Stein variational gradient
descent. arXiv preprint arXiv:1912.00894, 2019.
D B Dunson and J E Johndrow. The Hastings algorithm at fifty. Biometrika, 107(1):1-23, 2019.
Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. DeeP generative
learning via variational gradient flow. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, PP. 2093-2101. PMLR, 09-15 Jun 2019.
Yuan Gao, Jian Huang, Yuling Jiao, Jin Liu, Xiliang Lu, and Zhijian Yang. DeeP generative learning
with Euler Particle transPort. In Proceedings of Machine Learning Research vol 145:1-33, 2021
2nd Annual Conference on Mathematical and Scientific Machine Learning, 2021.
S. Gershman, M. Hoffman, and D. Blei. NonParametric variational inference. ICML, 2012.
Tilmann Gneiting and Adrian E Raftery. Strictly ProPer scoring rules, Prediction, and estimation.
Journal of the American statistical Association, 102(477):359-378, 2007.
W. Keith Hastings. Monte Carlo samPling methods using Markov chains and their aPPlications.
Biometrika, 57(1):97-109, 1970.
Matthew D. Hoffman and Andrew Gelman. The No-U-Turn SamPler: AdaPtively setting Path lengths
in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(47):1593-1623, 2014.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-Planck
equation. SIAM Journal on Mathematical Analysis, 29(1):1-17, 1998.
Takafumi Kanamori and Masashi Sugiyama. Statistical analysis of distance estimators with density
differences and density ratios. Entropy, 16(2):921-942, 2014.
Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A non-asymPtotic analysis
for stein variational gradient descent. Advances in Neural Information Processing Systems, 33,
2020.
10
Under review as a conference paper at ICLR 2022
Randall J LeVeque. Finite Difference Methods for Ordinary and Partial Differential Equations:
Steady-state and Time-dependent Problems, volume 98. SIAM, 2007.
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and accelerating
particle-based variational inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
ofMachine Learning Research, pp. 4082-4092. PMLR, 09-15 JUn 2019a.
Chang Liu, Jingwei Zhuo, and Jun Zhu. Understanding MCMC dynamics as flows on the Wasser-
stein space. In Kamalika ChaUdhUri and RUslan SalakhUtdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volUme 97 of Proceedings of Machine Learning
Research, pp. 4093-4103. PMLR, 09-15 JUn 2019b.
Qiang LiU. Stein variational gradient descent as gradient flow. In I. GUyon, U. V. LUxbUrg, S. Bengio,
H. Wallach, R. FergUs, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volUme 30. CUrran Associates, Inc., 2017.
Qiang LiU and Dilin Wang. Stein variational gradient descent: A general pUrpose bayesian inference
algorithm. In D. Lee, M. SUgiyama, U. LUxbUrg, I. GUyon, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, volUme 29. CUrran Associates, Inc., 2016.
Jianfeng LU, YUlong LU, and James Nolen. Scaling limit of the Stein variational gradient descent:
The mean field regime. SIAM Journal on Mathematical Analysis, 51(2):648-671, 2019.
Nicholas Metropolis, Arianna W RosenblUth, Marshall N RosenblUth, AUgUsta H Teller, and Edward
Teller. EqUation of state calcUlations by fast compUting machines. The journal of Chemical Physics,
21(6):1087-1092, 1953.
Radford M. Neal. MCMC Using Hamiltonian Dynamics, chapter 5. CRC Press, 2011.
Gareth O Roberts and Osnat Stramer. Langevin diffUsions and Metropolis-Hastings algorithms.
Methodology and computing in applied probability, 4(4):337-357, 2002.
Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of Langevin distribUtions and
their discrete approximations. Bernoulli, 2(4):341 - 363, 1996.
Adil Salim, Anna Korba, and GiUlia LUise. The wasserstein proximal gradient algorithm. arXiv
preprint arXiv:2002.03035, 2020.
Adil Salim, LUkang Sun, and Peter Richtdrik. Complexity analysis of stein variational gradient
descent Under talagrand’s ineqUality t1. arXiv preprint arXiv:2106.03076, 2021.
LUke Tierney. Markov Chains for exploring posterior distribUtions. The Annals of Statistics, 22(4):
1701-1728, 1994.
CedriC Villani. OPtimal Transport: Old and New, volume 338. Springer Science & Business Media,
2008.
Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational
inference. Foundations and Trends in Machine Learning, 1(1):1-305, 2008.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th international conference on machine learning, ICML’11, pp. 681-688.
ACM, 2011.
Michael Zhu, Chang Liu, and Jun Zhu. Variance reduction and quasi-Newton for particle-based
variational inference. In Hal Daume III and Aarti Singh (eds.), Proceedings of the 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
11576-11587. PMLR, 13-18 Jul 2020.
Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, and Bo Zhang. Message passing Stein
variational gradient descent. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 6018-6027. PMLR, 10-15 Jul 2018.
11
Under review as a conference paper at ICLR 2022
A Appendix
In this appendix, we prove Theorem 1, give a detailed description of the models in the numerical
experiments, the neural network architecture used in implementing REGS, and additional numerical
results.
B Proof of Theorem 1
Proof. By the definition of Bregman score B(R) between u and q, it is easy to check
R?u ∈ arg min B(R),
R
B(R) ≥ B(R?uq) with equality iff R(x) = R?uq(x) (q, u)-a.e. x ∈ X.
Since D?q = log(RUq),D = log(R), B(R) = B(D), When Eχ~w [W(X)D(X)] < ∞, We have
Du?q ∈ argmDinB(D),
B(D) ≥ B(Du?q) With equality iff D(x) = Du?q(x) (q, u)-a.e. x ∈ X.
C Gaus sian mixture distributions
Let the density of a d-dimensional multivariate Gaussian distribution with mean μ ∈ Rd and
covariance matrix Σ ∈ Rd×d be
f (X； μ, ς)= (2∏) 2 λpdet(∑eχp {-2(x - 〃注T(X- μ)}.
We consider several scenarios beloW for the unnormalized density function u(x).
Scenario 1. 2Gaussians_1d1. One-dimensional mixture of 2 Gaussians,
12
U(X) = 3 f (x; μι,σ2) + 3 f (x; μ2, σ2),
where μι = 1, μ2 = -2, and σ2 = 0.25, σ2 = 2;
Scenario 2. 2Gaussians_1d2. One-dimensional mixture of 2 Gaussians,
12
U(X) = 3 f (x; μι,σ2) + 3 f (x; μ2, σ2),
where μι = 3, μ2 = -3, and σ2 = 0.25, σ2 = 2;
Scenario 3. 2Gaussians_1d3. One-dimensional mixture of 2 Gaussians,
12
U(X) = 3f (x; μι,σ2) + 3f (x; μ2, σ2),
where μι = 3, μ2 = -3, and σ2 = 0.03, σ2 = 0.03;
Scenario 4. 2Gaussians. Two-dimensional mixture of 2 Gaussians,
U(X) = f(x; μι, ∑ι) + f(x; μ2, ∑2),
where μι = (r, 0)t, μ2 = (-r, 0)t, and ∑ι = ∑2 = σ2I.
Scenario 5. 8Gaussians. Two-dimensional mixture of 8 Gaussians,
8
U(X) = Ef(X； μj, ς ),
j=1
where μj = r(sin(2(j - 1)π∕8), cos(2(j - 1)π∕8))T, and Σj = σ2I for j = 1,…,8.
12
Under review as a conference paper at ICLR 2022
Scenario 6. 9Gaussians. Two-dimensional mixture of 9 Gaussians,
33
u(x) =	f(x
；μj k, ς j k ),
j=1 k=1
where μjk = 4(j - 2,k - 2)T, Σjk = σ2I for j = 1, 2, 3 and k = 1, 2, 3.
Scenario 7. 16Gaussians_1c. Two-dimensional mixture of 16 Gaussians,
16
U(X) = Ef(X； μj, ς ),
j=1
where μj = 4(sin(2(j — 1)π∕16), cos(2(j — 1)π∕16))T, and Σj = 0.03I for j =
1,…，16.
Scenario 8. 16Gaussians_2c. Two-dimensional mixture of 16 Gaussians,
88
U(X) = Ef(X μj, ς ) + Ef(X μk, ςJ
j=1	k=1
where μj = 4(sin(2(j - 1)∏∕8),cos(2(j - 1)π∕8))T, μk = 2(sin(2(j -
1)π∕8), cos(2(j - 1)π∕8))T, and Σj = ∑k = 0.03I for j = 1,…,8 and k = 1,…,8.
Scenario 9. 25Gaussians. Two-dimensional mixture of25 Gaussians,
55
U(X) =	f(X
; μjk,夕jk ),
j=1 k=1
where μjd = 2(j - 3, k - 3)T, Σjk = σ2I for j = 1, ∙∙∙ , 5 and k = 1,…，5.
Scenario 10. 49Gaussians. Two-dimensional mixture of 49 Gaussians,
77
U(X) =	f(X；μj k, ς j k ),
j=1 k=1
where μjd = 3 (j - 4, k - 4)T, Σjk = 0.03I for j = 1,…，7 and k = 1,…，7.
Scenario 11. 81Gaussians. Two-dimensional mixture of 81 Gaussians,
99
U(X) =	f(X
; μjk,夕jk ),
j=1 k=1
where μjd = 3 (j - 5, k - 5)T, Σjk = 0.03I for j = 1,…，9 and k = 1,…，9.
Scenario 12. Icircle. Let μi = 4(cos(2iπ∕N), sin(2iπ∕N ))T, i = 0,1,…，N — 1 with N = 400.
Consider three noise to be added to each point μi, including the uniform distribu-
tion U(μi, 1∕30) on a disc with center at μ% and radius 1/30, Gaussian distribution
N(μi, 0.03I), and mixed these two distributions.
Scenario 13. 2circles. Let μι =	2(cos(2iπ∕N),sin(2iπ∕N))T and μ2i =
4(cos(2iπ∕N), sin(2iπ∕N))T, i = 0,1,…，N — 1 with N = 200. Consider
three noise to be added to each point μki including uniform distribution U(μki, 1∕30)
on a disc with center at μki and radius 1/30, Gaussian distribution N(μki, 0.03I), and
mixed these two distributions, k = 1, 2.
Scenario 14. IsPiral. Let μi =喑 cos(4iπ∕N), sin(4iπ∕N))T, i = 0,1,…，N — 1 with N = 400.
Consider three noise to be added to each point μ% including the uniform distribu-
tion U(μi, 1∕30) on a disc with center at μ% and radius 1/30, Gaussian distribution
N(μi, 0.03I), and mixed these two distributions.
Scenario 15. 2spirals.	Let μι =	3N∏(cos(3iπ∕N), sin(3iπ∕N))T and μ2i	=
—3N∏(cos(3iπ∕N),sin(3iπ∕N))T, i = 0,1,…，N — 1 with N = 200. Con-
sider three noise to be added to each point μki including uniform distribution
U(μki, 1∕30) on a disc with center at μki and radius 1/30, Gaussian distribution
N(μki, 0.03I), and mixed these two distributions, k = 1, 2.
13
Under review as a conference paper at ICLR 2022
Scenario 16. moons. Let μii = (8i/N — 6,4sin(iπ∕N))T and μ2i = (8i/N — 2, 4sin(iπ∕N))T,
i = 0,1, ∙∙∙ ,N — 1 with N = 200. Consider three noise to be added to each point μki
including uniform distribution U(μk%, 1/30) on a disc with center at μki and radius 1/30,
Gaussian distribution N(μki, 0.03I), and mixed these two distributions, k = 1,2.
D	Experimental setting
D. 1 Hyperparameter
We burn in the first 1000 particles for ULA and MALA in all experiments. We initialize the particles
in SVGD, ULA and MALA as zeros or random samples from Gaussians. We provide the step size
settings in Table 3. For SVGD, we use RBF kernel k(x,x0) = exp(-h∣∣x — x0∣∣2) and set the
bandwidth as h = med2 / log n, where med is the median of pairwise distances between the particles
{xi }in=1 . We set the learning rate of neural networks as 5e-4 for density ratio estimation in REGS.
The network structures are presented in Table 4. The initial particles in REGS are sampled from
Gaussian distributions.
Table 3: Step size settings for REGS, SVGD, ULA and MALA. "BGIR" denotes four datasets
including Banana, German, Image and Ringnorm.
Methods	2Gaussians	8Gaussians	9Gaussians	25Gaussians	BGIR	Covertype
REGS	5e-4	5e-4	5e-4	5e-4	2e-3	2e-3
SVGD	2e-2	2e-2	2e-2	2e-2	5e-2	5e-2
ULA	2e-2	5e-2	1e-1	5e-2	1e-3	1e-4
MALA	5e-2	2e-1	5e-1	5e-1	1e-3	—
D.2 Neural network architecture
Table 4: Neural network architecture for log-density ratio estimation: feedforward neural networks
with equal-width hidden layers and Leaky ReLU activation. Depth ` = 3 for 2Gaussians_1d1,
2Gaussians_1d2, 2Gaussians_1d3, and 2Gaussians. ` = 4 for 8Gaussians, 9Gaussians, 1circle,
2circles, 1spiral, 2spirals, and moons. ` = 6 for 16Gaussians_1c, 16Gaussians_2c, 25Gaussians,
49Gaussians, and 81Gaussians.
Layer	Details	Output size
{i}'-1	Linear, LeakyReLU (0.2)	128
`	Linear	1
D.3 Datasets
Covertype:	https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
binary.html Banana, German, Image, Ringnorm: http://theoval.cmp.uea.ac.uk/
matlab/default.html
E Additional numerical results
14
Under review as a conference paper at ICLR 2022
Table 5: Monte Carlo estimates of E[h(X)]. Here h(x) = αTx, (αTx)2, and 10 cos(αTx + 1/2) with
ɑ ∈ R2, ∣∣αk2 = 1. "true" denotes the Monte Carlo estimate with target samples.
Distributions	h(x)	=αTx	h(x)=	：(aTx)2	h(x) = 10 cos(αTx + 1/2)	
	true	REGS	true	-REGS	true	REGS
2Gaussians_1d1	-0.9886	-0.9887	~3.7105	3.7093	0.46838	0.5418
2Gaussians_1d2	-0.9972	-1.1630	9.0032	9.0297	-8.3053	-8.2956
2Gaussians_1d3	-0.9602	-0.728	9.8727	9.8016	-6.3785	-6.1266
2Gaussians	-0.0019	0.0448	8.0243	8.0225	-8.2351	-8.2513
8Gaussians	-0.0021	-0.021	8.0118	8.0276	-3.3925	-3.3727
9Gaussians	0.0003	0.0100	10.6771	10.8063	0.7773	0.7532
16Gaussians_1c	-0.0008	-0.0077	8.0269	8.02889	-3.4393	-3.4325
16Gaussians_2c	-0.0006	-0.0060	5.3544	5.2780	-1.4260	-1.3877
25Gaussians	0.0013	-0.0141	8.0276	8.0227	0.1248	0.1189
49Gaussians	0.0006	0.0001	9.0285	9.0215	0.2043	0.1990
81Gaussians	-0.0011	-0.0009	15.0280	15.0261	0.4126	0.4196
8Gaussians r=5	-0.0011	-0.0037	12.5291	12.5356	-1.2160	-1.2214
8Gaussians r=10	0.0021	-0.0030	50.0278	49.4783	3.3872	3.4238
8Gaussians r=15	0.0003	-0.0306	112.5023	111.4456	-1.1116	-1.0930
2Gaussians σ2 = 0.01	-0.0022	-0.0009	0.5119	0.5082	6.6390	6.6504
2Gaussians σ2 = 0.005	-0.0012	0.0027	0.5019	0.4976	6.6694	6.6718
2Gaussians σ2 = 0.0001	0.0005	0.0034	0.5043	0.5049	6.6558	6.6406
Table 6: Monte Carlo estimates of E[h(X)] by four samplers for 2D mixtures of Gaussians of X
with equal weights. Here h(x) = αTx, (αTx)2 or 10 cos(αTx + 1/2) with α ∈ R2, ∣α∣2 = 1. "true"
denotes the Monte Carlo estimate with target samples. "ULA_k" and "MALA_k" denote the ULA
and MALA With k chains, respectively.____________________________________________________________________
Distributions	σ2	h(x) = αTx							h(x) = (aT x)2							h(x) = 10 cos(αTx + 1/2)						
		true	REGS	SVGD	ULA_1	MALA 1	ULA_50	MALA_50	true	REGS	SVGD	ULA_1	MALA_1	ULA_50	MALA_50	true	REGS	SVGD	ULA_1	MALA_1	ULA_50	MALA_50
2gaussian	0.2	0.02	0.00	-0.01	-1.45	0.66	0.11	-0.34	2.56	2.50	2.50	2.62	2.27	8.24	8.25	0.97	1.06	1.09	4.54	-0.43	-7.64	-7.30
	0.1	-0.00	-0.03	0.04	1.37	-1.38	0.11	-0.23	2.20	2.20	2.20	2.06	2.12	8.10	8.15	1.23	1.33	1.12	-2.62	5.71	-7.99	-7.71
	0.05	0.00	0.02	0.06	1.42	-1.44	0.11	-0.23	2.10	2.10	2.12	2.10	2.19	8.04	8.10	1.30	1.23	1.05	-3.22	5.57	-8.20	-7.83
	0.03	-0.01	0.02	0.10	-1.41	1.42	0.11	-0.23	2.02	2.05	2.01	2.04	2.10	8.03	8.18	1.42	1.28	1.12	5.98	-3.36	-8.29	-7.53
8gaussian	0.2	0.00	-0.01	0.00	-3.46	3.00	0.14	0.35	8.20	8.20	7.98	12.81	10.20	7.87	8.05	-3.05	-3.09	1.41	-7.20	-5.41	-2.95	-2.71
	0.1	-0.01	-0.02	0.02	2.83	2.84	-0.66	-0.02	8.11	8.12	8.12	8.11	8.23	8.09	8.52	-3.23	-3.26	1.51	-9.35	-9.11	-3.54	-3.71
	0.05	-0.01	-0.00	0.02	2.83	-2.83	-0.41	0.04	8.06	8.06	8.31	8.05	8.08	8.21	9.02	-3.33	-3.34	1.36	-9.58	-6.51	-2.83	-4.37
	0.03	0.00	-0.00	-0.01	-0.00	1.96	-0.41	0.18	8.04	8.05	8.05	0.02	6.19	8.19	8.40	-3.35	-3.35	1.57	8.68	-2.38	-2.86	-3.86
25gaussian	0.2	-0.00	0.00	-0.44	0.22	-0.45	0.02	0.02	8.18	8.20	9.46	8.02	7.96	8.31	8.10	0.10	0.11	0.75	0.53	0.15	0.05	0.13
	0.1	0.00	0.00	0.04	0.15	0.59	-0.05	-0.05	8.11	8.09	2.11	7.42	8.09	8.19	7.97	0.10	0.11	3.44	0.99	-0.15	0.12	0.12
	0.05	-0.00	-0.01	-0.00	0.36	-1.50	-0.33	-0.15	8.06	7.62	1.07	4.40	6.17	5.48	8.04	0.12	0.10	5.37	0.95	0.71	0.30	0.34
	0.03	-0.00	-0.02	-0.01	0.14	-0.08	0.14	-0.04	8.04	7.58	0.98	8.11	6.97	2.95	7.41	0.11	0.09	5.56	-2.40	-0.91	1.62	0.08
15
Under review as a conference paper at ICLR 2022
(a) REGS
(b) SVGD
—
(C) ULA With one chain
— —
(d) MALA with one chain
(e) ULA with 50 chains
Figure 5: Scatter plots with contours of 1000 generated samples from unnormalized mixtures of 2
Gaussians with equal weights by (a) REGS, (b) SVGD, (c) ULA and (d) MALA with one chain, (e)
ULA and (f) MALA with 50 chains. From left to right in each subfigure, the variance of Gaussians
varies from σ2 = 0.2 (first column), σ2 = 0.1 (second column), σ2 = 0.05 (third column), to
σ2 = 0.03 (fourth column).
— —
(f) MALA with 50 chains
16
Under review as a conference paper at ICLR 2022
"lιιιlιll
Zi__IlIIM
W
∙w-
**
I：
W
(a) REGS
ɪapataɪ(
■ ，
—
■ ，
一
■ ，
~*
• T ∙	'•••,
一
(b) SVGD
*■
*“
■»
(I)
!：
I 二
*■
一
ɪ
I
■ ，
一
■ ，
(d) MALA
ɪapatlɪ(
(C) ULA With one chain
(e) ULA with 50 chains
■»
h_______rtτ
g∣ι∣∣∣ιlι
""■
`»
■，
∙Λ
一
one chain
■，
一
3 I 二
I ⅜, l h, ι≡
ɪ......ɪ, 3∣ιιh∣!∣ ≡
(f) MALA with 50 chains
Figure 6: Scatter plots with contours of 2000 generated samples from unnormalized mixtures of 8
Gaussians with equal weights by (a) REGS, (b) SVGD, (c) ULA and (d) MALA with one chain, (e)
ULA and (f) MALA with 50 chains. From left to right in each subfigure, the variance of Gaussians
varies from σ2 = 0.2 (first column), σ2 = 0.1 (second column), σ2 = 0.05 (third column), to
σ2 = 0.03 (fourth column).
17
Under review as a conference paper at ICLR 2022
2
t -
" .................
二
，一’ —————————————————————
" ” “,-- ”
(a) REGS
— —
(b) SVGD
2
*一
∣∙^_________
二唧MBliI
..........~~
MMaM
xaa>>aτa
" " "
(d) MALA with one chain
,0ι!⅛
2
(W
(W
(e) ULA with 50 chains
⑴ MALA With 50 chains






Figure 7: Scatter plots with contours of 5000 generated samples from unnormalized mixtures of 25
Gaussians with equal weights by (a) REGS, (b) SVGD, (c) ULA and (d) MALA with one chain, (e)
ULA and (f) MALA with 50 chains. From left to right in each subfigure, the variance of Gaussians
varies from σ2 = 0.2 (first column), σ2 = 0.1 (second column), σ2 = 0.05 (third column), to
σ2 = 0.03 (fourth column).
18
Under review as a conference paper at ICLR 2022
■ >
(a) REGS
■	I
(b) SVGD
—
(c) ULA With one chain
— —
(d) MALA with one chain
・	♦	I	!
(e) UlA with 50 chains
(f) MALA with 50 chains
Figure 8: Scatter plots with contours of 1000 generated samples from unnormalized mixtures of
2 Gaussians with unequal weights (0.75, 0.25) by (a) REGS, (b) SVGD, (c) ULA and (d) MALA
with one chain, (e) ULA and (f) MALA with 50 chains. From left to right in each subfigure, the
variance of Gaussians varies from σ2 = 0.2 (first column), σ2 = 0.1 (second column), σ2 = 0.05
(third column), to σ2 = 0.03 (fourth column).
19
Under review as a conference paper at ICLR 2022
j j I 1 I 二 I I
I： I I I I	α	I	I	I	I	H	I	I	I I	∣: ∣	∣ I	∣
H	¾	⅞	耳
--L-L-Lt	□	I	I	I	r	d	I	I	It]	::LJ_LJ_il
ɪapataɪ(
— — — —
(b) SVGD
W
*ɪ"
∙∙
ɪɪv
I：
(d) MALA with one chain
ɪapatlɪ(
(C) ULA With one chain
Figure 9: Scatter plots With contours of 2000 generated samples from unnormalized mixtures of 8
Gaussians with unequal weights (1, 1, 1, 1, 3, 3, 3, 3)/16 by (a) REGS, (b) SVGD, (c) ULA and (d)
MALA with one chain, (e) ULA and (f) MALA with 50 chains. From left to right in each subfigure,
the variance of Gaussians varies from σ2 = 0.2 (first column), σ2 = 0.1 (second column), σ2 = 0.05
(third column), to σ2 = 0.03 (fourth column).
20
Under review as a conference paper at ICLR 2022
(a) REGS
(b) SVGD
Γ
I：
一
—*
.................... ..........................
IIU川川!∣∣∣!!∣∣!!!!!!∣∣∣^K∣川川ιι!11111.. ι il
LiiIkjIIi Il .i I. . .L.∣.	.	TlITIaiIl lI . .l 1. ...I I
■"
!：：
b'

(e) ULA With 50 chains
A
Ill≡	A............Illliiiihiil
(f) MALA with 50 chains
Figure 10: Scatter plots With contours of 5000 generated samples from unnormalized mixtures of 25
Gaussians with unequal weight by (a) REGS, (b) SVGD, (c) ULA and (d) MALA with one chain, (e)
ULA and (f) MALA with 50 chains, where each of the first 12 components has weight 1/51, and
each of the rest has weight 3/51. From left to right in each subfigure, the variance of Gaussians varies
from σ2 = 0.2 (first column), σ2 = 0.1 (second column), σ2 = 0.05 (third column), to σ2 = 0.03
(fourth column).

∙3,■1

-，	3-5	8	-5	5
Figure 11: KDE plots for 1D mixtures of 2 Gaussians. Green lines stand for target samples and
pink areas represent generated samples by REGS. From left to right, the means and variances of the
components changed and the unnormalized densities are given in Scenarios 1, 2, 3 in Appendix B.
21
Under review as a conference paper at ICLR 2022
OOO
OOO
OOO
Figure 12: KDE plots of target samples (first row) and generated samples (second row) for two-
dimensional mixtures of Gaussians with variance 0.03. The target samples are from unnormalized
density functions u(x) of mixtures of 2 Gaussians in Scenario 4, 8 Gaussians in Scenario 5 and 9
Gaussians in Scenario 6.
OOOOO
OOOOO
OOOOO
OOOOO
Figure 13: KDE plots of target samples (first row) and generated samples (second row) for 2D
mixtures of Gaussians with component variance 0.03. The corresponding unnormalized densities are
presented in Scenarios 7, 8, 9 in Appendix B.
22
Under review as a conference paper at ICLR 2022
Figure 14: Scatter plots of initial samples (first row), generated samples (second row) for two-
dimensional mixtures of 2 Gaussians, and scatter plots of target samples (last row). The target
samples are from unnormalized density functions u(x) of mixtures of 2 Gaussians with variance
σ2 = 0.01 (left column), σ2 = 0.005 (middle column) and σ2 = 0.001 (right column).
23
Under review as a conference paper at ICLR 2022
t**wM*
•*■*♦■-**›
.*⅜∙a:w'e
«•****
髭«■*・•♦
« a ⅛ * ««
⅜ « ⅜ « W «
• #«・热■
*.•*,，*
,♦■i*⅜<
Figure 15: Scatter plots of initial samples (first row), generated samples (second row) for two-
dimensional mixtures of multiple Gaussians with variance σ2 = 0.03, and scatter plots of target
samples (last row). The target samples are from unnormalized density functions u(x) of mixtures of
25 Gaussians (left column), 49 Gaussians (middle column) and 81 Gaussians (right column).
24
Under review as a conference paper at ICLR 2022
Figure 16: Scatter plots of initial samples (first row), generated samples (second row) for two-
dimensional mixtures of 8 Gaussians with variance σ2 = 0.03 and varying radius, and scatter plots
of target samples (last row). The target samples are from unnormalized density functions u(x) of
mixtures of Gaussians with radius being 5 (left column), 10 (middle column) and 15 (right column).
25
Under review as a conference paper at ICLR 2022
(C) Target samples (first row) and generated samples (second row) from mixed Gaussian and uniform distributions
Figure 17: Scatter plots from left to right are one circle (1circle, Scenario 12), two circles (2circles,
Scenario 13), one spiral (1spiral, Scenario 14), two spirals (2spirals, Scenario 15), and moons (moons,
Scenario 16).
26