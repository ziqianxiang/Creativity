Under review as a conference paper at ICLR 2022
A framework of deep neural networks via
THE SOLUTION OPERATOR OF PARTIAL DIFFERENTIAL
EQUATIONS
Anonymous authors
Paper under double-blind review
Ab stract
There is a close connection between deep neural networks (DNN) and partial
differential equations (PDEs). Many DNN architectures can be modeled by PDEs
and have been proposed in the literature. However, their neural network design
space is restricted due to the specific form of PDEs, which prevents the design
of more effective neural network structures. In this paper, we attempt to derive a
general form of PDEs for the design of ResNet-like DNN. To achieve this goal,
we first formulate DNN as an adjustment operator applied on the base classifier.
Then based on several reasonable assumptions, we show the adjustment operator
for ResNet-like DNN is the solution operator of PDEs. To show the effectiveness
of the general form of PDEs, we show that several effective networks can be
interpreted by our general form of PDEs and design a training method motivated
by PDEs theory to train DNN models for better robustness and less chance of
overfitting. Theoretically, we prove that the robustness of DNN trained with our
method is certifiable and our training method reduces the generalization gap for
DNN. Furthermore, we demonstrate that DNN trained with our method can achieve
better generalization and is more resistant to adversarial perturbations than the
baseline model.
1	Introduction
Deep neural networks (DNN) have achieved success in tasks such as image classification (Simonyan
& Zisserman, 2015), speech recognition (Dahl et al., 2012), video analysis (Bo et al., 2011), and
action recognition (Wang et al., 2016). Residual learning (He et al., 2016a;b) has revolutionized DNN
architecture design, making it practical to train ultra-deep DNN, and has ability to avoid gradient
vanishing. The idea of residual learning has motivated the development of powerful DNN such as
WideResNet (Zagoruyko & Komodakis, 2016), ResNeXt (Xie et al., 2017), and DenseNet (Huang
et al., 2017).
So how to understand Residual learning from a theoretical perspective? In (Weinan, 2017), the
author formulated the forward propagation of ResNet as a dynamic system and in (Li & Shi, 2017;
Haber et al., 2018; Li et al., 2017) bridge the connection between partial differential equations(PDE)
and ResNet. In fact, ResNet is composed of a series of residual mapping. Each residual block is
realized by adding a shortcut connection to connect the input and output of the original mapping (F).
Mathematically, the n-th residual mapping can be formulated as
xn+1 = xn + F(xn, wn)
(1)
with x0 ∈ Rd be a data point. xn is the input to the n-th residual mapping and F(xn, wn) is a
non-linear function parameterized by wn which can be learned by back propagating the training error.
The prediction of the data point xo can be expressed by y = Softmax(WFCxL). By introducing
a temporal partition: tn = nT /L, for n = 0, 1, .., L with the time interval ∆t = T/L and let
x(tn) = xn and w(tn) = wn. Without considering the dimensional consistency, ResNet can be
formulated as :
x(tn+1 ) = x(tn) +
F(x(tn), w(tn))
∆t
∆t, n = 0, 1,…，L,
1
Under review as a conference paper at ICLR 2022
which is the forward Euler discretization of the following equations with time step ∆t, that is,
dx(t) = v(x(t), t)dt, x(0) = x, t ∈ [0, T],
(2)
where v(x(tn),tn) = F(x(tn), w(tn))∕∆t and L is the number of resdiual mapping in a ResNet
architecture. The prediction can be expressed by y = Softmax(WFCx(T)). The equation 2 defines
the characteristic curves of the following TE
丁(x,t) = v(x, t)Vu(x,t), x ∈ Rd.
(3)
So the prediction y of ResNet can be seen as the solution u(x, T) of following Transport equation(TE):
d∂uu(x,t) = v(x,t)Vu(x, t), X ∈ Rd.
u(x, 0) = softmax(wFCx).
(4)
In fact, not only ResNet, similar to the process above, many ResNet-like DNN can correspond to
PDEs. For example, it has been shown (Lu et al., 2018) that PolyNet (Zhang et al., 2017) and
FractalNet (Larsson et al., 2016) can be respectively considered numerical schemes to solve a PDEs
system. Based on the link of PDEs and ResNet-like DNN, many theoretical results to understand
ResNet-like DNN were proved. In (Chen et al., 2018), a continuous form for ResNet called Neural
ODE was proposed. After that, the property of Neural ODE has been theoretically analyzed (Thorpe
& van Gennip, 2018; Avelin & Nystrom, 2021; Zhang et al., 2019a). Based on the stability theory
of differential equations, many architectures has been designed to build more robust deep networks
(Zhang et al., 2019b; Yan et al., 2019; Reshniak & Webster, 2021). Not only stability theory, inspired
by the numerical scheme of differential equations, in (Zhu et al., 2018; Tao et al., 2018), various deep
network architectures to improve testing accuracy have been proposed in success.
Despite the success of the application of PDEs in residual learning, these neural network design space
in the literature mentioned above is restricted due to the specific form of PDEs, which prevents the
design of more effective neural network structures. Existed works can not meet these requirements.
So, a natural question is that can we find the general form of the PDEs corresponding to the ResNet-
like DNN which can unify existed model as well as guide new effective DNN architecture design?
To achieve this goal, in this paper, we follow the setting of paragraph 2, the softmax model and
ResNet are the initial value and the terminal value of PDEs equation 4 respectively, so we can view
ResNet as an evolution from a softmax model. Without loss of generality, we indicate this evolutive
relationship by an adjustment operator denoted by Tt, then we have
Tt : softmax(wF C x) → ResNet(x).
If we denote the softmax model by f, then the ResNet-like DNN can be expressed as Tt(f) and Tt is
the solution operator of PDEs equation 4. To obtain the general form of the PDEs corresponding to
these ResNet-like DNN, we list a series of formal properties likely to be satisfied by the operator Tt,
and we deduce an explicit formulation from them. We see that under reasonable assumptions, the
new model u(x, t) = (Tt f)(x) is the solution of convection diffusion equations. Then, motivated by
the theory of PDEs and the framework of our general form of PDEs, we design a training method
for ResNet-like DNN models for better robustness and less overfitting risk. Our training method is
summarized as follows. First, we train a ResNet-like DNN model using natural training. Because
calculating the real solution Tt(f)(x) is difficult, we next train a new ResNet-like DNN denoted by
g(x, t) to approximate Tt(f)(x). At last, we choose g(x, T) as our final classifier.
We make the following contributions:
•	Under reasonable assumptions, we build a PDEs framework for ResNet-like DNN with the
help of operator Tt and show some effective models that can be modeled by our framework.
•	We design a training method for DNN based on the theory of PDEs. This is an essential
contribution of this paper due to certified adversarial robustness and theoretical analysis of
generalization ability for our training method.
•	We verify our training method on a synthetic dataset and the CIFAR-10/CIFAR-100 datasets,
then demonstrate that a ResNet trained by our method is more resistant to multistep `2 and
'∞ adversarial attacks than natural trained ResNet.
2
Under review as a conference paper at ICLR 2022
2	Main Results
We show under several reasonable assumptions that the adjustment operator Tt is the solution operator
of convection diffusion equations. Based on the regularity theory of PDEs, we design a training
method for DNN models and theoretically illustrate its benefits for improving the robustness of
models and reducing the generalization gap.
2.1	General form of PDEs
We denote the base DNN model by fw(x), which is uniformly continuous. We assume data points are
distributed in D ⊂ Rd, and label function l(x) is defined in D but known only in training set SN ∈ D.
Our method formulates the DNN as a continuous dynamic system, namely u(x, t) = Tt(f), t ∈ [0, T].
To get the expression of the adjustment operator Tt , we assume it has some fundamental properties.
[Locality] (Tt(f) - Tt(g))(x) = o(t) as t → 0+, for all f, g such that Dαf(x) = Dαg(x) for all
∣α∣≥ 0.
[Comparison Principle] Tt(f) ≤ Tt(g) for all t ≥ 0 and f, g such that f ≤ g.
[Markov Property] Tt+s = Tt+s,s ◦ Ts, for all s, t ≥ 0 and t + s ≤ T. Tt+s,sdenotes the flow from
time s to time t + s.
[Spatial Regularity] There exist a positive constant C such that kTt(τhf) - τh (Tt f)kL∞ ≤ Ch for
all h in Rd, t ≥ 0, where (τh f)(x) = f(x + h) and khk2 = h.
[Temporal Regularity] The following two properties hold:
1)	For all t, s, t + s ∈ [0, T] and all g ∈ Q, there exist a constant C ≥ 0 depending only on Q such
that kTt+s,s (g) - gkL∞ ≤ Ct, where Q is the subset of Cb∞ defined by
Q = {f ∈ C∞,∀n ≥ 0,∣IDaf∣∣L∞ ≤ C for all |a| = n}.
2)	For all t, s, t + s ∈ [0, T], there exist a constant C ≥ 0 such that kTt+s,t (f) - Ts,0 (f)kL∞ ≤ Cst
uniformly in s ∈ (0, T].
[Ensemble Additivity] Tt(f + g) = Tt(f) + Tt(g), and if C is a constant, then Tt(C) = C.
To illustrate the reasonableness of these assumptions above, we give the following explanation. We
first require that Tt can be expressed as a solution operator of PDEs and for small enough t, the value
of Tt (f) at any point x is determined by the behavior of the base classifier f near x, which implies
[Locality]. We then require the operator Tt is provided with an order-preserving property which
means that no enhancement is made. Thus if the confidence level for an event of one base classifier
g is higher than another base classifier f, this ordering is preserved, which implies [Comparison
Principle]. Next we require Tt+s can be computed from Tt for any s ≤ t, and T0 is of course the
identity. This is natural, since the future behaviour of the adjustment process at terminal time t + s is
likely to be deduced from the state at intermediate time t without any dependence upon the original
model f, which implies [Markov Property]. We need function Tt (f) to be spatial stable. If we
add a perturbation h to data point x, the output Tt (f)(x + h) will not change a lot, which implies
[Spatial Regularity]. We also need an additional property for giving some temporal stability to the
operator Tt . Namely, in any time interval, the adjustment will not be rapid, which implies [Temporal
Regularity]. Assume an ensemble classifier F consists of base classifiers f and g, and another
ensemble classifier consists of base classifiers Tt(f) and Tt(g). We require the prediction Tt(F) to
equal Tt(f) + Tt(g), which implies [Ensemble Additivity].
Theorem 1. Under the above conditions, there exists Lipschitz continuous function v : Rd × [0, T] →
Rd and Lipschitz continuous positive function σ : Rd × [0, T] → Rd×d such that for any bounded
and uniformly continuous base model fw (x), u(x, t) = Tt (fw)(x) is the unique viscosity solution
of the following convection diffusion equation:
(-Ux^ = v(χ,t) ∙ ^u(x,t) + 2 Pij σi,j ∂Xi∂xj (χ,t),
u(x, 0) = fw (x),
(5)
where x ∈ Rd, t ∈ [0, T]. Here σi,j is the i, j-th element of matrix function σ(x, t).
3
Under review as a conference paper at ICLR 2022
We will provide the proof of Theorem 1 in Appendix A. In this subsection, we introduce a PDEs
framework for DNN models. The framework is quite general, many existing models with residual
connections can be modeled by our framework. As examples, we briefly list some of them in the next
subsection.
2.2	Several models
In this subsection, we will show Gaussian noise injection(Wang et al., 2020; Liu et al., 2019),
randomized smoothing(Cohen et al., 2019; Li et al., 2018; Salman et al., 2019) and ResNet with
stochastic dropping out the hidden state of residual block(Srivastava et al., 2014; Sun et al., 2018)
can be interpreted by our framework.
Gaussian noise injection: Gaussian noise injection is an effective regularization mechanism for a
DNN model. For a base ResNet with L residual mapping, the n-th residual mapping with Gaussian
noise injected can be written as
xn+1 = xn + F(xn, wn) + aεn , εn 〜N (0,I),
where the parameter a is a noise coefficient. By introducing a temporal partition: tn = nT /L,
for n = 0, 1, .., L with the time interval ∆t = T/L and let x(tn) = xn and w(tn) = wn. And
let a = σ√∆t and F(xn, Wn)∕∆t = v(x, t). This noise injection technique in a discrete neural
network can be viewed as the approximation of continuous dynamic
dx(t) = v(xn, t)dt + σdB(t)	(6)
where B(t) is multidimensional Brownian motion. The output of L-th residual mapping can be
written as Ifo process equation 6 at terminal time T, X(T). So, an ensemble prediction over all the
possible sub-networks with shared parameters can be written as
y = E(Softmax(WFCx(T ))∣x(0) = x0).	(7)
According to Feynman-Kac formula(Mao, 2007), equation equation 7 is known to solve the following
convection diffusion equation
(du(XI) = v(x, t) ∙ Vu + 2σ2∆u, x ∈ Rd, t ∈ [0, T]
u(x, 0) = softmax(wFCx)
Randomized Smoothing: Consider to transform a base classifier into a new smoothed classifier by
adding Gaussian noise to the input when inference time. If we denote the base classifier by f(x) and
denote the new smoothed classifier by g(x). Then f(x) and g(x) have the following relation:
1N
g(X) = NAf(X + εi) ≈ Eε~N(0,I)[f (x + ε)]
According to Feynman-Kac formula, g(x) can be viewed as the solution of the following PDEs
(中=1 ∆u,t ∈ [0, 1]
u(x, 0) = f (x).
(8)
(9)
Especially, when f(x) is ResNet, the smoothed classifier g(x) = u(x, T + 1) can be expressed as
[d⅛t) = 2∆u,t ∈ [T,T +1]
du∂XI) = v(x, t) ∙ Vu(x, t), x ∈ Rd, t ∈ [0, T]
I u(x, 0) = softmax(wFcx).
Dropout of Hidden Units: Consider the case that disables every hidden units independently from a
Bernoulli distribution B(p) with p ∈ (0, 1) in each residual mapping
xn+1 = xn + F(xn, Wn) G)
p
=xn + F(xn, Wn) + F(xn, Wn) G (	― I),
p
4
Under review as a conference paper at ICLR 2022
where Zn 〜B(1,p) namely P(Zn = 0) = 1 — p, P(Zn = 1) = P and Θ indicates the Hadamard
product. If the number of the ensemble is large enough, according to Central Limit Theorem, we have
F(Xn，Wn) θ ( Zn - I) ≈ F(Xn，Wn) '" N。，一 ).
The similar way with Gaussian noise injection, the ensemble prediction y can be viewed as the
solution u(X， T) of following equation:
d 冲=v(x,t) ∙Vu(x,t) + 12pp Pi(vT v)i,i∂χ2 (x,t),
u(X， 0) = softmax(WFCX)
(10)
where X ∈ Rd， t ∈ [0， T].
Remark 1. In fact, similar to dropout, shake-shake regularization (Gastaldi, 2017; Huang &
Narayanan, 2018) and ResNet with stochastic depth (Huang et al., 2016) can also be formulated by
our PDEs framework.
The viewpoint that forward propagation of a ResNet is the solving process of a TE enables us to
interpret poor robustness and generalization ability as the irregularity of the solution. Morever we can
also interpret the effectiveness of the models above-mentioned in improving generalization as well
as robustness as the action of diffusion term. To mitigate this issue, based on our PDEs framework
and the knowledge that the terminal value of convection diffusion equations (whose diffusion term is
isotropic, namely σ(x, t) is σ2I) is more regular than initial value, We use the ResNet fw(x) as the
base classifier and use the solution operator of the diffusion equation
(du∂t," = v(x, t) ∙ Vu(x, t) + σ2∆u, x ∈ Rd, t ∈ [0, T]
u(X， 0) = fw (X).
(11)
as the adjustment operator Tt to smooth the ResNet for improving the robustness and generalization
ability. The case that diffusion term σ(x,t) is anisotropic (namely σ(x,t) = σ2I) is difficult for
theoretical analysis and practical experiment and we take the case as our future work. In next
subsection, we will illustrate the benefits of using the adjustment operator Tt from a theoretical
perspective.
2.3	Robustness guarantee
Consider a classification problem from Rd to blue label class Y . Let G be a classifier defined by
G(X) = arg maxi∈Y ui(X， T), where ui(X， T) is the i-th element of u(X， T). Suppose that the
new DNN classifies X the most probable class cA is returned with probability pA = ucA (X， T),
and the “runner-up” class is returned with probability pB. Our main result of this subsection is to
estimate around the data point X how large `p radius can make the classifier G(X) be robust, where
1 <p ≤ ∞.
Theorem 2.	Let the base model fw (X) be a compactly supported function and velocity v(X， t) ∈
C1(Rd × [0， T]). Let the new DNN u(X， T) be the solution of equation 11. Suppose cA ∈ Y and
ucA (X， T) = pA ≥ pB = max ui (X， T)，
i6=cA
then G(X + δ) = cA for all kδkp ≤ R, where
R =	eσ2T (PA - PB)
2d1/qeγT max，∣∣fW(x)ks，
1/P + 1/q = 1 and γ is a constant depending on Vv.
We provide the proof of Theorem 2 in Appendix B. According to Theorem 2, we can obtain the
certified radius R is large when the diffusion coefficient σ2 and the probability of the top class is
high. Not only robustness, comparing to the base classifier, but we will also show the generalization
ability of model 11 will be better in next subsection.
5
Under review as a conference paper at ICLR 2022
2.4 Generalization gap
For simplicity, we consider binary classification problems. Suppose SN ={(xi , yi)}iN=1 is drawn
from X × Y ⊂ D × {-1, +1} with X and Y being the input data and label spaces, respectively.
Assume p(x) is the underlying distribution ofX, which is defined in D. We also assume label function
Label : D → {-1, +1}, which is unknown. Let H ⊂ V X be the hypothesis class of the DNN
model, where V is another space that might be different from Y . Let ` : V × Y → [0, B] be the loss
function and B is a positive constant. Denote function class 'h := {(x, y) → '(h(x), y) : h ∈ H}.
Rademacher complexity is one of the classic measures of generalization error. We first recap on the
definition of Rademacher complexity.
Definition 1. (Ledoux & Talagrand, 2013)Let H : X → R be the space of real-valued functions on
the space X. For a given sample SN = {(xi, yi)}iN=1, the empirical Rademacher complexity of H is
defined as
1N
RSN(H) := NEσ[sup]σih(xi)],
where σ1,σ2, … ,σN are i.i.d. Rademacher random variables with P(σi = 1) = P(σi = —1) = 2.
Rademacher complexity is a tool to bound the generalization error. The smaller the generalization
gap is, the less overfitting the model is. For ∀xi ∈ Rd and constant c ≥ 0, we consider the following
two function classes:
F := nf(x)|kfkC1 ≤ co,
and
Gσ := {g(x) = u(x, 1)| du∂x,t)
v(x, t)Vu(x, t) + σ2∆u, u(x, 0) = f (x) where f ∈
The function class F represents the continuous analogue of ResNet, and function class Gσ denotes F
adjusted by operator Tt . Then we have the following theorem.
Theorem 3.	Given a training set SN = {(xi, yi)}iN=1. With probability at least 1 — 1/(2N) we have
C
RSN(F) ≤ √N [(log C + log N)1/2 + c]
C
RSN (Gσ ) ≤ K [(—σ + ln C + ln N)	+eχp(-σ )c]
Here C is a positive constant depend on D and dimension d.
We provide the proof of Theorem 3 in Appendix C. According to Theorem 3, we can obtain that for
function class Gσ, the upper bound of Rademacher complexity is low when the diffusion coefficient
σ2 is high. We next present a practical algorithm for real applications to verify our main results.
3 Algorithms
Let SN = {(x1, y1), .一,(xn, yn,)} be training set, where Xi is a data point and y% is the correspond-
ing label. Then denote the ResNet with natural training by fw (x) whose parameters can be learned
by minimized the loss lCE(yi, fw(xi)), where lCE is cross-entropy loss. . For convenience, we
assume the velocity v(x, t) is zero and the diffusion term σ(x, t) is σ2I. We can rewrite equation 5
as
(du∂χ," = σ2∆u(x, t), X ∈ Rd, t ∈ [0,T]
u(x, 0) = fw(x)
(12)
where σ2 is a hyperparameter. In this model, we set the natural trained ResNet as the initial value.
Instead of calculating the solution of equation 12, we train a DNN gw (x, t) to approximate the
real solution. The parameters of new DNN gw (x, t) are different from the initial ResNet fw (x).
Therefore, to train the DNN model gw(x, t), we design loss terms on training set SN for the initial
condition and differential equation.
6
Under review as a conference paper at ICLR 2022
We use lCE (gw (xi, 0), fw(xi)) to fit the initial condition of equation 12, where lCE is cross-
entropy loss. To improve the natural accuracy, we enhance the terminal value to fit the label, i.e.,
lCE (gw (xi, T), yi). We can write the first loss term as
1N
Lι(gw,SN) = N £[/CE (gw(χi, 0),fw (Xi)) + ICE (gw (χi,τ ),yi)].	(13)
i=1
We use mean square error loss to fit the differential equation and improve the robustness of our model,
1N
L2(gw,SN) = NE
(Xi , S) - σ ∆gw (Xi , S)	dS.
i=1
According to the trapezoid formula, the integral term equation 14 is approximated by
1N
L2(gw,SN) ≈ 2N∑S £
(Xi , S) - σ ∆gw (Xi , S)
i=1 s=0,T
Using Taylor formula, the ∆gw (Xi, S) can be approximated as
1M
△gw (Xi,s) ≈ ME
j=1
gw (Xi + hεi,j, S) + gw (Xi ― hεi,j, S) ― 2gw(xi, S)
h2
(14)
(15)
(16)
where {εi,j} is i.i.d and M is the average number. In practice, we choose the number of average M
equals 1 to reduce the computation cost, and set h to 0.1 because training converges with difficulty
when h is too small. We denote the right hand of equation 16 by ∆h,εgw(Xi, S). Combining
equation 15 and equation 16, the integral term equation 14 can be approximated by
L2 , (gw , SN ) =N XX x( T(Xi, 0) - σ2∆h,εgw(Xi, S)	.	(17)
i=1 s=0,T
Putting the two objective functions together, our training loss combines L1 (gw, SN) and
L2h,M (gw, SN), i.e., Loss(gw, SN) = L1(gw, SN) + λL2h,M (gw, SN), where λ > 0 is a coeffi-
cient to trade off the two loss terms. In practice, we use stochastic gradient descent to optimize
gw(X,t), and we use gw(X, T) as the final model.
4	Experiments
In this section, we will numerically verify that 1) can our method improve the robustness and reduce
overfitting risk of DNN? 2) what is the advantage of our method over the Gaussian noise injection
and randomized smoothing?
4.1	Preliminaries
Datasets. In our experiments, we consider both Half-moon and CIFAR10/CIFAR100 (Krizhevsky
et al., 2009) datasets. Half-moon dataset is a randomly generated 2d synthetic dataset in which we
randomly generate 500 points and 1000 points with a standard deviation of 0.3 as training set and
testing set, respectively. Both CIFAR10 and CIFAR100 contain 60K 32 × 32 color images with 50K
and 10K of them used for training and test, respectively. CIFAR10 and CIFAR100 contain 10 and
100 different classes, respectively.
Project gradient descent attack (PGD). To verify the efficiency of our method for improving the
robustness of DNN, we consider the project gradient descent (PGD)(Madry et al., 2017) in all the
experiments below. Before introducing PGD, we first introduce fast gradient sign method attack
(FGSM). The FGSM attack searches the adversarial image X0 by maximizing the linearized loss
function function L(x0, y) ≈ L(x, y) + VL(x, y)T(x0 - x) with constraint kx0 - Xkp ≤ e, where
is the maximum perturbation and p = 2, ∞. PGD iterates FGSM with step size α and clips the
perturbed image to generate the enhanced adversarial attack,
'∞ : X(m) = Clipx,e{X(m-1) + αsign(VL(X(m-1),y))},
7
Under review as a conference paper at ICLR 2022
(a)Natural training (b) λ = 0.01, σ2 = 0.01
Figure 1: Decision boundary of natural trained ResNet and ResNet trained by our method with
different hyperparameters λ and σ2 .
(C) λ = 0.1,σ2 =0.01
(d) λ = 0.1,σ2 = 0.1
(a)Atneastt(%)	(b)Atreosbt(%)	(c) Atnraatin -Atneastt(%)
Figure 2: Performance of ResNet trained by our model with different hyperparameters λ and σ2 on Half-moon
dataset.
'2 : x(m) =CliPx,e{x(m-1) + α(	2) )}，
where m = 1, ∙∙∙ ,K, x(0) = x, and let the adversarial image be x0 = X(K) with K being the total
number of iterations. For the Half-moon dataset, we apply 20 iterations PGD attack (PGD20) with step
size 0.01. The perturbation is constrained to be 0.2 under l∞ norm. For the CIFAR-10/CIFAR-100
dataset, when under '∞ norm attack, we apply 20 iterations PGD attack with step size 1/255 and
when under `2 norm attack and we apply 50 iterations PGD attack (PGD50) with step size 0.1.
Performance evaluations. For the Half-moon dataset, we consider three indicators to evaluate the
performance of the model: natural testing accuracy (Atneastt), robust testing accuracy (Atreosbt), and the
gap between training accuracy and testing accuracy(Atnraatin - Atneastt). The natural accuracy and robust
accuracy are measured on clean and adversarial images, respectively. The robust accuracy is used
to evaluate the robustness of the models, and the difference between training and testing accuracy
is used to evaluate the overfitting risk of the models. For the CIFAR-10/CIFAR-100 datasets, we
consider Atneastt and Atreosbt .
4.2	Experiments on synthetic dataset
In this subsection, we numerically verify that the efficacy of our training method in improving the
robustness and generalization ability of ResNet on the Half-moon dataset.
We first varied hyperparameters λ and σ2 and present the final results in Figure 2. The three indicators
(natural accuracy, robust accuracy, and difference between training accuracy and testing accuracy)
for ResNet with natural training were 88.6%, 62.3%, and 10.4%, respectively. Comparing natural
trained ResNet, the robustness and generalization ability of ResNet trained with our method were
improved. On the other hand, when λ and σ2 increased, the natural and robust accuracy increased,
while the gap between training and test accuracies decreased. Thus we can conclude that increasing
λ and σ2 are helpful for improving the performance of the models.
We plot the decision boundary of ResNet trained with different λ, σ 2 and natural trained ResNet in
Figure 1. From Figure 1, we can see that the decision boundary of natural training is irregular, while
that of our models is smoother. These experimental results are consistent with our theory.
4.3	Experiment on CIFAR- 1 0/CIFAR- 1 00 dataset
We further test the performance of ResNet20/ResNet56 trained with our method on the CI-
FAR10/CIFAR100 dataset. During training, we apply data augmentation including random crops and
flips. We run 200 epochs. The batch size is 128 and the initial learning rate is 0.1, which decays by a
factor of 10 at the 100th, and 150th epochs.
8
Under review as a conference paper at ICLR 2022
Models	Hyperparameters	AteSt VAinat	4 test rθbob
			C = 4∕255(l∞) I C = 8∕255(l∞) I C = O5(l2)
CIFAR-10DataSet
ResNet20	λ = 0.01,σ2 = 1 λ = 0.05, σ2 = 1 λ = 0.05, σ2 = 2	86.72% 84.78% 82.49%	40.13% 45.25% 48.29%	7.8% 13.03% 17.79%	45.33% 50.57% 53.87%
ResNet56	λ = 0.01,σ2 = 1 λ = 0.05, σ2 = 1 λ = 0.05, σ2 = 2	87.76% 86.01% 83.03%	41.85% 47.42% 48.73%	81% 14.73% 18.02%	-46.17% 53.05% 55%
OFAR-100 Dataset
ResNet20	λ = 0.01,σ2 = 1 λ = 0.05, σ2 = 1 λ = 0.05, σ2 = 2	58.77% 55.7% 52.73%	19.56% 21.79% 23.33%	3.91% 5.56% 8.27%	23.98% 26.55% 28.42%
ResNet56	λ = 0.01,σ2 = 1 λ = 0.05, σ2 = 1 λ = 0.05, σ2 = 2	60.2% 57.82% 53.99%	19.75% 23.26% 25.19%	4.49% 6.8% 9.41%	-24.93% 27.82% 29.55%
Table 1: Natural and robust accuracies of ResNets trained using our methods with different hyperparameters
on the CIFAR-10/CIFAR-100.
Models	AteSt 八nat	4 test 	ʌrob		
		c = 4∕255(l∞)	c = 8∕255(l∞)	C = 0.5 (l2)
Gaussian noise injection	89.9%	22.81%	12.38% =	0.1%
Our ResNet110	84.35%	50.76%	19.4%	56.38%
ResNet110	93.58%	0%	0%	0.01%
Table 2: Comparing the robustness against '∞ -norm and `2 -norm constrained adversarial perturbations on
CIFAR-10.
There are two hyperparameters λ, σ 2 in our algorithm that need to be determined by a large number
of experiments. In order to demonstrate the influence of the two hyperparameters, we train ResNet by
using our algorithm with different λ, σ2, then list the result in Table 1. On one hands, from Table
1, we can see that the parameter σ2 is fixed, the natural accuracy decreases and robust accuracy
increases as the increasing of parameters λ and the converse to be true. On the other hand, comparing
to the result of ResNet20 and ResNet56, we can see that the performance of our proposed method
will be better in deeper networks.
In order to quantitatively measure the performance of our proposed method, we compare the proposed
method with other defensive method without adversarial training. The result of comparison is shown
in Table 2. For our method, we choose σ2 = 2 and λ = 0.5 to train ResNet110. For the Gaussian
noise injection mentioned in subsection 2.2, the standard error of the Gaussian noise equals to
0.5. The experiment result shows that our method is more resistant to adversarial attack than other
methods.
5	Concluding Remarks
In this paper, we derived a general form of PDE that can correspond to a DNN model. Motivated by
PDE theory, we proposed a training method for ResNet. We have theoretically shown that using our
training methods, we obtain a model with better robustness and lower overfitting risk. Based on these
theoretical results, we developed an algorithm to train DNN models, and we verified them through
experiments. There are numerous avenues for future work: 1) Scale our training method to networks
that are large and expressive enough to solve problems like ImageNet. 2) Study other more complex
equations for DNN models and solve more challenging real tasks.
9
Under review as a conference paper at ICLR 2022
References
Luis Alvarez, Frederic Guichard, Pierre-Louis Lions, and Jean-Michel Morel. Axioms and fundamen-
tal equations of image processing. Archivefor rational mechanics and analysis, 123(3):199-257,
1993.
Benny Avelin and Kaj Nystrom. Neural odes as the deep limit of resnets with constant weights.
Analysis and Applications, 19(03):397-437, 2021.
Liefeng Bo, Kevin Lai, Xiaofeng Ren, and Dieter Fox. Object recognition with hierarchical kernel
descriptors. In CVPR 2011, pp. 1729-1736, 2011. doi: 10.1109/CVPR.2011.5995719.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differential equations. Advances in Neural Information Processing Systems, 2018.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and
Language Processing, 20(1):30-42, 2012. doi: 10.1109/TASL.2011.2134090.
Lawrence C Evans. Partial differential equations. Graduate studies in mathematics, 19(4):7, 1998.
Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017.
Eldad Haber, Lars Ruthotto, Elliot Holtham, and Seong-Hwan Jun. Learning across scales—
multiscale methods for convolution neural networks. In Thirty-Second AAAI Conference on
Artificial Intelligence, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Che-Wei Huang and Shrikanth S Narayanan. Stochastic shake-shake regularization for affective
learning from speech. In INTERSPEECH, pp. 3658-3662, 2018.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic depth. In European conference on computer vision, pp. 646-661. Springer, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks
without residuals. arXiv preprint arXiv:1605.07648, 2016.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. arXiv preprint arXiv:1809.03113, 2018.
Qianxiao Li, Long Chen, Cheng Tai, et al. Maximum principle based algorithms for deep learning.
arXiv preprint arXiv:1710.09513, 2017.
Zhen Li and Zuoqiang Shi. Deep residual learning and pdes on manifold. arXiv preprint
arXiv:1708.05115, 2017.
Xuanqing Liu, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. Neural sde: Stabilizing neural ode
networks with stochastic noise. arXiv preprint arXiv:1906.02355, 2019.
10
Under review as a conference paper at ICLR 2022
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks:
Bridging deep architectures and numerical differential equations. In International Conference on
Machine Learning,pp. 3276-3285. PMLR, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Xuerong Mao. Stochastic differential equations and applications. Elsevier, 2007.
Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International
Conference on Algorithmic Learning Theory, pp. 3-17. Springer, 2016.
Shahar Mendelson. A few notes on statistical learning theory. In Advanced lectures on machine
learning, pp. 1-40. Springer, 2003.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.
Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer
Science & Business Media, 2013.
Viktor Reshniak and Clayton G Webster. Robust learning with implicit residual networks. Machine
Learning and Knowledge Extraction, 3(1):34-55, 2021.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv
preprint arXiv:1906.04584, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Qi Sun, Yunzhe Tao, and Qiang Du. Stochastic training of residual networks: a differential equation
viewpoint. arXiv preprint arXiv:1812.00174, 2018.
Yunzhe Tao, Qi Sun, Qiang Du, and Wei Liu. Nonlocal neural networks, nonlocal diffusion and
nonlocal modeling. arXiv preprint arXiv:1806.00681, 2018.
Matthew Thorpe and Yves van Gennip. Deep limits of residual neural networks. arXiv preprint
arXiv:1810.11741, 2018.
Bao Wang, Binjie Yuan, Zuoqiang Shi, and Stanley J Osher. Enresnet: Resnets ensemble via the
feynman-kac formalism for adversarial defense and beyond. SIAM Journal on Mathematics of
Data Science, 2(3):559-582, 2020.
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.
Temporal segment networks: Towards good practices for deep action recognition. In Bastian Leibe,
Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision - ECCV2016, pp. 20-36, Cham,
2016. Springer International Publishing. ISBN 978-3-319-46484-8.
E Weinan. A proposal on machine learning via dynamical systems. Communications in Mathematics
and Statistics, 5(1):1-11, 2017.
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492-1500, 2017.
Hanshu Yan, Jiawei Du, Vincent YF Tan, and Jiashi Feng. On robustness of neural ordinary
differential equations. arXiv preprint arXiv:1910.05513, 2019.
11
Under review as a conference paper at ICLR 2022
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities of neural ordinary
differential equations. arXivpreprint arXiv:1907.12998, 2(4):3-1, 2019a.
Jingfeng Zhang, Bo Han, Laura Wynter, Kian Hsiang Low, and Mohan Kankanhalli. Towards robust
resnet: A small step but a giant leap. arXiv preprint arXiv:1902.10887, 2019b.
Xingcheng Zhang, Zhizhong Li, Chen Change Loy, and Dahua Lin. Polynet: A pursuit of structural
diversity in very deep networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 718-726, 2017.
Mai Zhu, Bo Chang, and Chong Fu. Convolutional neural networks combined with runge-kutta
methods. arXiv preprint arXiv:1802.08831, 2018.
12
Under review as a conference paper at ICLR 2022
Appendix
A	Proof of Theorem 1
Proof. (Alvarez et al., 1993) The proof of Theorem 1 consists of several steps. First we set
Tt(f) - Ts(f)
δt,s(f ) = ---------, δt(f ) = δt,0(f ).
t-s
Using [Temporal Regularity], for all h, t ∈ [0, T] and all f, g ∈ Q, we can get
kδt(f+hg)-δt(f)kL∞ ≤Ch.	(18)
Step 1. We begin the proof of Theorem 1 by proving the following conclusion
(A1) the functions δt (f) are Lipschitz continuous uniformly for t ∈ (0, 1](and uniformly for f
belonging to a set Q).
Let z be in Rd and kzk2 = 1, we wish to bound kτhz[δt(f)] - δt (f)kL∞ for h ∈ [0, 1]. According
to [Spatial Regularity], for f ∈ Q, we have
kτhz[δt(f)] -δt(f)kL∞
≤ kτhz[δt(f)]-δt[f(x+hz)]kL∞ +kδt[f(x+hz)]-δt[f(x)]kL∞
≤ Ch.
Then the conclusion of (A1) is proved.
Step 2. From the preceding steps, we can already deduce a compactness property for δt (f ) as t → 0.
Now we need more, since we want the whole sequence to converge to the same limit. So we need a
Cauchy estimate. Our next step is to prove the following conclusion.
(A2) For all t, S in [0, 2] We have ∣∣δt+s,t(f) - δs(f )∣∣l∞ ≤ m(t), where m(t) is some continuous,
nonnegative, nondecreasing function on [0,1 ] such that m(0) = 0, and m(t) depends only on bounds
of derivatives of f .
First, we can rewrite δs(f) as δs(f) * K - [δs(f) * K — δs(f)], where K ≥ 0, RRd Kdy = 1,
K ∈ C∞ (Rd) and K = ε-dK(∙∕ε) and * denote the convolution. Using the conclusion of (A1),
we can obtain for all ε > 0, there exist a positive constant C1 such that
∣δs(f)*Kε-δs(f)∣∞ ≤ C1ε.	(19)
Then because of [Markov Property], [Temporal Regularity] and [Comparison Principle], we
have
k Tt+s(f)-Tt(f + Sδs * Kε ) 小
sL
≤ k Tt+s,s ◦ Ts(f)- Tt ◦ Ts(f) k L 8 + k Tt ◦ Ts(fL- Tt(f + sδs * Kl Nο
sL	s	L
Ts(f) -f - sδs *Kε
≤ Ct + k--------------S——ε ∣∣L∞
s
≤ Ct + C2ε	(20)
On the other hand, since δs(f) * Kε ∈ Cb∞, [Temporal Regularity] implies that
∣Tt(f+sδs(f)*Kε)	-Tt(f)	-sδs(f)	*Kε∣L∞	≤	Cεst	(21)
for some positive constant Cε depending only on ε. Combining equation 19, equation 20 and
equation 21, we finally deduce that
∣δs+t,t(f) -δs(f)∣∞ ≤ 2C1ε+Cεt.
The (A2) holds by setting m(t) = infε∈(0,1] (2C1ε + Cεt).
Step 3. Now we give a Cauchy estimate in 0 < h ≤ t ≤ 1 for δs (f)
kδt(f) - δh(f )∣L∞ ≤ 2 —0 + m(t) Where r = t - [τ]h.	(22)
th
13
Under review as a conference paper at ICLR 2022
Observe that
δt(f) = —j^Nhhff) + qδNh+r,Nh(f).
Using (A2) with S = r, t = Nh and let N = [h], We have
Nh	r	r
kδNh+r (f) - Nh + T δNhf) - Nh + T δr (f )kL∞ ≤ Nh + T m(Nh)	(23)
Notice that
δNh(f ) = -~~Nh) δNh(f ) + NhδNh,(N-1)h(f ).
Using (A2) with s = h, t = (N - 1)h, we have
N-1	1	1
llδNh(f)	N —δ(N-1)h(f) - Nδh(f )kL∞ ≤ Nm((N - 1)h)	(24)
Combining this inequality with equation 24, we obtain
h	hT
kδt(f) - (N - 1) Tδ(N-1)h(f) - -δh (f) - tδr (f )kL∞
≤ Tm(Nh) + ；m((N - 1)h)	(25)
Reiterating the argument which lends to equation 24, we obtain after (N - 1) more steps that
Nh	T	T	h N-1
kδt(f) - δh(f) - -δr (f )k∞ ≤ tm(Nh) + — ^X m(jh)
j=1
Since m() is nondecreasing and equation 25, we deduce
lδt(f) - δh(f)lL∞
≤
≤
≤
Nh	T	T
kδt(f)--『 δh(f) - -δr (f )kL∞ + — kδh(f) - δr (f )kL∞
— m(Nh) + (N -1)hm(t) + — (∣δh(f)∣L∞ + ∣δr(f)∣L∞)
m(t) +2 C0r
(26)
(27)
Because of equation 27, we can pick hn going to 0 and δhn (f) converges uniformly on compact sets
to a bounded Lipschitz function on Rd which we denote by A[f]
lim ∣∣δt(f) - δhn (f )kL∞ ≤ lim m(t) + 2 -0-
n→∞	n→∞	—
⇒ lδt(f)-A[f]lL∞ ≤ m(—).
So δt(f) converges uniformly to A[f] when goes to 0. Similarly, there exist an operator At such
that δs,t(f) converges uniformly to At [f] when s goes to—.
Step 4. The goal of this step is to show the expression of A[f].
Let f, g ∈ Cb∞ and satisfy f(0) = g(0) = 0 (if not equal to 0, we replace f (x), g(x) by f(x) -
f(0), g(x) - g(0)), Df(0) = Dg(0) = p ∈ Rd, D2f(0) = D2g(0) = A ∈ Rd×d. Then we set
fε = f + εlxl22. Using Taylor formula, there exist a positive constant c such that for lxl2 ≤ cε we
have fε ≥ g. Let wε = w(x∕ε) where W ∈ C∞(Rd) and 0 ≤ W ≤ 1 on Rd,
w = 0 lxl2 ≤ c/2
W= 1 lxl2 ≥c
(28)
Then we introduce f0ε = Wεfε + (1 -Wε)g and it is obviously that f0ε ≥ g. Because of [Comparison
Principle], Tt(f0ε) ≥ Tt(g). Due to
f0ε(0)=fε(0) = f(0) = g(0) =0	(29)
we can also get
A(f0ε)(0) ≥A(g)(0),A(fε)(0) ≥ A(g)(0)	(30)
14
Under review as a conference paper at ICLR 2022
Because there exists a neighborhood of 0 that f0 = fε, We have Dafj(0) = Dαfε (0) for ∀ |a| ≥ 0.
In view of [Locality] we have A(f0ε)(0) = A(fε)(0). And considering the continuity of A, we can
deduce A(f0ε) converges to A(f) in L∞ When ε goes to 0. This means A(f)(0) ≥ A(g)(0). By
similar method, We can get A(f)(0) ≤ A(g)(0) Which means A(f)(0) = A(g)(0). Hence, for any
x0 ∈ Rd, if We replace 0 by x0 , the similar result can be obtained. So the value of A(f)(x) only
depends on x, f, Df, D2f. Observe that for any constant C, A(f + C) = A(f), We can get A(f)(x)
only depends on x, Df, D2f. At last, We finally get
A(f) =F(Df,D2f,x).	(31)
Here F is a continuous function. In the same Way, We can get
At(f) =F(Df,D2f,x,t).	(32)
According to [Ensemble Additivity], since
F (Df, D2f, x, t) = lim(Ttf - f)/t,
t→0
F therefore satisfies
F (rDf + sDg, rD2f + sD2g, x, t) = rF (Df, D2f, x, t) + sF (Dg, D2g, x, t)
for any real numbers r and s and any functions f and g and at any point (x, t). Since the values of
Df, Dg, D2f, D2g are arbitrary and can be independently taken to be 0, We obtain for any vectors
v1, v2 and symmetric matrices A1, A2 and any fixed point (x0 , t0 ) that
F (rv1 + sv2, rA1 + sA2,x0,t0) = rF (v1, A1, x0, t0) + sF(v2,A2,x0,t0)
F(v1,A1,x0,t0) = F(v1,0,x0,t0) +F(0,A1,x0,t0).
Thus We can finally there exist Lipschitz continuous functions v : Rd × [0, T] → Rd and Lipschitz
continuous positive definite functions σ : Rd × [0, T] → Rd×d such that u(x, t) = Tt(f) is the
solution of the equation
( d⅛t) = v(x,t) ∙Vu(x,t) + Pi,j σi,j(x,t)χ∂∂xj(x,t), X ∈ Rd,t ∈ [0, 1]
u(x, 0) = f (x),
where σ%,j (x,t) is the i, j-th element of matrix function σ(x,t).	□
The existence and uniqueness of equation 5 are guaranteed by the folloWing theorem
mi	A z -rr ι	∙Y T	1	∕rτ->l	CT t ∙ //ʌT	1 ι c∕^ιτc∖∖ 4	T 个
Theorem 4.	(Kolmogorov’s backward equation)(Theorem 8.1.1 in (Oksendal, 2013)) Assume Ito
process x(t) satisfies SDE
dx(t) = v(x(t), t)dt + σ(x(t))dB(t), x(0) = x,
where B(t) is d-m Brownian motion and v : Rd → Rd, σ : Rd → Rd×d are all Lipschitz continuous
function. Let fw (x) ∈ C02(Rd). Then u(x, t) = E[fw (x(t)|x(0) = x] is the unique solution of
equation 5.
B Proof of Theorem 2
To begin the proof of Theorem 2, we first provide the maximum principle for operator L which has
the following form
Lu = u— — σ2∆u + v(x, t)Vu,
where the coefficients v(x, t) are continuous.
Theorem 5.	(Evans, 1998) (Maximum principle) Assume U to be an open bounded subset of Rn
and as before set UT = U × (0, T] forsome fixed time T > 0. Let U ∈ C2(UT) ∩ C(UT) and UT is
the closure of UT. Denote UT — UT by「『If
Lu ≤ 0 in UT ,
then
max u = max u.
UT	γt
15
Under review as a conference paper at ICLR 2022
After the introduction of maximum principle, we next illustrate the proof of the theorem 2.
Proof. Let w(x, t) = (μu2(x, t) + ∣∣Vu(x, t)∣∣2)e-2λt, where μ and λ are constants which will be
defined later.
Note that u2 (x, t) satisfies
'(u ) + v(x,t)V(u2) = σ2∆(u2) — 2σ2∣Vu∣2,
∂t
and ∣Vu∣2 satisfies
d kvuk2 + v(x,t)V∣Vu∣2 = —2VuVv(X,t)Vu + σ2 ∆∣Vu∣2 — 2σ2(∆u)2,
∂t
therefore,
∂w	2
+V + v(x, t)Vw 一 σ2∆w
=e-2λt[-2λ(μu2 + IlVuk2) — 2μσ2∣Vuk2 — 2VuVv(x,t)Vu — 2σ2(∆u)2].
Next, let γ(x,t) = min∣∣ξk=ι ξτVv(x,t)ξ and Y = minχ,t γ(x,t), and let Lw := 絮 +
v(x, t)Vw — σ2∆w then we have
Lw ≤ —2e-2λt[λμu2 + (λ + μσ2 — Y)IlVuk2].
If We choose λ and μ large enough, such that λ + μσ 一 Y ≥ 0, then
Lw ≤ 0.
From the maximum principle (Theorem 5), we know maxχ w(x, 1) ≤ maxχ w(x, 0). Let μ = 1
and λ = Y — σ2, we can easily get
ku(x,T)kC1 ≤e-σ2TeγTkfw(x)kC1,	(33)
According to the Taylor formula and Holder inequality, we can get that for any ∣∣δ∣p = δ, we have
|fw(X)- fw(x + δ)∣ ≤ kVfw(x)kqδ ≤ d1/qkfw(x)kcιδ,	(34)
and
∣u(x,T) — u(x + δ,T)| ≤ kVu(x,T)kqδ ≤ d1/qku(x,T)kciδ,	(35)
According to equation 34 and equation 35, we can get
ucA(X + δ,T) ≥ PA — d1/qku(x,T)kcιδ
max ui(X + δ, T) ≤ pB + d1/q ku(X, T)kC1 δ.
i6=cA
Thus ucA (X + δ, T) ≥ maxi6=cA ui(X + δ, T) for all
δ <	PA — PB
≤ 2d1/qku(x,T)kci
Then combining equation 33, equation 34 and equation 35 we can easily prove the theorem. □
C Proof of Theorem 3
The goal of the learning problem is to find h ∈ H such that population risk E(h) :=
E(χ,y)['(h(x), y)] is minimized. We consider the supervised learning setting where one has ac-
cess to n i.i.d. training examples SN . A learning algorithm maps the N training examples to
a hypothesis h ∈ H. In this section, we are interested in the gap between the empirical risk
EN(h) := N PN=I '(h(xi), yi) and the population risk E(h), known as the generalization error. We
start with the following theorem which connects the population and empirical risks via Rademacher
complexity.
16
Under review as a conference paper at ICLR 2022
Lemma 1. (Theorem 3.5 in (Mohri et al., 2018)) Let SN = {xi , yi }iN=1 be samples chosen i.i.d.
according to the distribution p(x). If the loss function ` is bounded by B > 0. Then for any δ ∈ (0, 1),
with probability at least 1 - δ, the following holds for all h ∈ H,
E (h) ≤ ESN (h) + 2Rsn ('h) + 3BP(log(2∕δ)∕(2N).
In addition, according to the Ledoux-Talagrand contraction inequality(Maurer, 2016) and assume
loss function is L-Lipschitz, we have
RSN ('h) ≤ LRSN (H).
Next, we begin to prove Theorem 3.
Theorem 6. (Theorem 2.3 in (Mendelson, 2003)) Let H be a class of functions from D to [-1, 1]
and setP to be a probability measure on D. Let (Xi)in=1 be independent random variables distributed
according to μ. Let (Y, d) be a metric space and set F ⊂ Y. For every e > 0 and any n ≥ 8∕e2,
P (SUP In X f (Xi)- Z f (X)μ(X)dX∣ > e) ≤ 8Eμ[Num(e∕8, F, Lι(μn))] exp(-ne2∕128) (36)
In above theorem, Num(e, H, Lp(μn) denotes the covering numbers of H at scale E with respect
to the Lp(μn) norm. μn is the empirical measure supported on one sample of (xi)n=1. For every
E > 0, denote by Num(E, H, d) the minimal number of open balls (with respect to the metric d)
needed to cover H. That is, the minimal cardinality of the set {yι,…，ym} ⊂ Y with the property
that every f ∈ H has some yi such that d(f, yi) < e. The set {yι,…，ym} is called an E-COVer
of H . The logarithm of the covering numbers is called the entropy of the set. For every sample
{xι, •…，Xn} let μn be the empirical measure supported on that sample. For 1 ≤ p < ∞ and a
function f, kf |必(“八)=(1 pn=ι If(Xi)Ip)" and ∣∣f k∞ = max1≤i≤n |f (x,)|.
Notice that
L1(μn) ≤ L∞(μn) ≤ L∞.
We get one immediate corollary of Theorem 6.
Corollary 1. Let H be a class of functions from D to [-1, 1] and set P(X) to be a probability measure
on D. Let {Xi}iN=1 be independent random variables distributed according to P(X). For every E > 0
and any n ≥ 8∕E2,
P SuP ∣N∙ xx f(Xi) - Z f (X)P(X)dXI > E ≤ 8Num(E∕8, H, L∞) exP(-NE2∕128) (37)
f∈H N i=1	D
where Num(E, H, L∞) be the covering numbers of H at scale E with respect to the L∞ norm
Then, We get an upper bound of SuPf ∈H | N PN=I f (Xi) - JD f (X)P(X)dx|.
Corollary 2. Let H be a class of functions from D to [-1, 1]. Let {xi }iN=1 be independent random
variables distributed according to P, where P is the probability distribution whose C 1 norm is
bounded. Then with probability at least 1 - δ,
SuHHf)-PN(f)l≤ t
128	2	8
ɪ InNUm(MN，H,L∞) + lnδ，
where
1N
p(f )=/ f (x)p(x)dx, Pn (f) = — Ef(Xi).
D	N i=1
(38)
Proof. Using Corollary 1, with probability at least 1 - δ,
SuP IP(f) - PN(f)I ≤ Eδ,
f∈H
17
Under review as a conference paper at ICLR 2022
where δ is determined by
Obviously,
which gives that
Num&/8, H,L∞) ≤ Num(,N, H, L∞)
Then, we have
δ ≤ t
128	2	8
N~ (InNUm(，n, H, L∞) + In δ
which proves the corollary.
□
If the entropy bound of H is known, the upper bound of supf∈H |p(f) - pN(f)| follows from
Corollary 2. Now, the key point left becomes bounding the entropy of some given function class H.
Let us start from the function class F . To apply above corollary, we need to normalize F to make it
lie in [-1, 1]. Here we also use F to denote the normalized function class and absorb the bound of F
into the generic constant C .
According to Taylor formula, we have for any x, z ∈ D
lf(x) - f(z)l ≤ √dsup kf kc"∣x - z∣∣2∙
f∈F
Where d is the dimension of x. This gives an easy bound of Num(, F, L∞),
Num(e, F,L∞) ≤ C (Supf∈f kfkC1 )

Using Corollary 2, with probability at least 1 - 1/(2N), we have
泮 1p(f) - PN(f )l≤√N (ln N + ln(SuF kfkC1) + 1)
(39)
(40)
and
fsUGKf)- pn (f )l≤√N (ln N+ln(渭 kfkC1) + 1)
According to equation 33, we can get the following theorem.
Theorem 7.	With probability at least 1 - 1/(2N ),
sup |p(f) -PN(f )| ≤ -C= (lnN + lnC + 1)1/2 ,
f∈F	N
sup |p(f) — PN(f )| ≤ C= (lnN + lnC - σ2 + 1)1/2
f∈Gσ	N
p(f) = f f(x)p(x)dx, Pn(f) = N X f(xi),
D	xi∈SN
F and Gσ is a function class defined as
F:= nf(x)|kfkC1 ≤ Co,
and
Gσ := {g(x) = u(x, 1)| 'u(χ," = v(x,t) ∙ Vu(x, t) + 1∕2σ2∆u(x, t), u(x, 0) = f (x) where f ∈ F}.
18
Under review as a conference paper at ICLR 2022
For the purpose of proving Theorem 3, we give a inequality about Rademacher complexity as
following.
Theorem 8.	(Theorem 2.23 in (Mendelson, 2003)) Let p(x) be a probability distribution function
and set H to be a class of functions on D. Denote
1N
Z = sup L fXi)(Xi) - Ex〜p(x)(f )|,
f∈H Ni=1
where {xi}iN=1 are independent random variables distributed according to p(x). Then we have
1	Ex 〜p(x)Z ≤ RSN (H) ≤ 2Ex 〜p(x)Z +ɪ | SuP Ex 〜p(x)f∣.
2	N f∈H
Then combining Theorem 7, Theorem 8, we have
RSN (Gσ ) ≤ 2Ex 〜p(x)Z + √n | sup Ex 〜p(x)f |.
≤ √N (ln N + ln C - σ2 + 1)"2 + √N | Sup Ex〜p(x)f |
≤√N (ln N+ln c- σ2 +1)1/2+√N f∈uGσ kfkC1
≤ √= [(—σ2 + lnc + lnN)1/2 + exp(-σ2)c].
Similarly, we can estimate the upper bound of RSN (F).
D More experimental details on Half-moon dataset
In this subsection, we plot the decision boundary of ResNet trained with different hyperparameters in
Figure 3. As drawn in Figure 1, we can see as the hyperparameters λ and σ2 increasing, the decision
boundary becomes more smooth, which implies the overfitting risk reduce. These experimental
results are also consistent with the conclusion of Theorem 3.
1.5
1.0
0.5
0.0
-0.5
-1.0
(a) λ = 0.01, σ2 = 0.01
(b) λ = 0.01, σ2 = 0.05
(c) λ = 0.01, σ2 = 0.1
(d) λ = 0.05, σ2 = 0.01	(e) λ = 0.05, σ2 = 0.05	(f) λ = 0.05, σ2 = 0.1
(g) λ = 0.1, σ2 = 0.01
(h) λ = 0.1, σ2 = 0.05
(i) λ = 0.1, σ2 = 0.1
Figure 3:	Boundary decision of ResNet trained by our model with different hyper parameters λ and σ2.
E	Architectures of the Used DNNs
Figure 4	shows the architectures of ResNets used in this paper. We also plot the inputs for the ResNet
used on CIFAR-10 dataset in Figure 5 and every element of the matrix tt equals to t.
19
Under review as a conference paper at ICLR 2022
Figure 4: Architectures of the ResNet used in our experiments.
Figure 5: The inputs for the ResNet used in CIFAR-10 dataset.
20