Under review as a conference paper at ICLR 2022
Equivalence of state equations from different
methods in High-dimensional Regression
Anonymous authors
Paper under double-blind review
Ab stract
State equations (SEs) were firstly introduced in the approximate message passing
(AMP) to describe the mean square error (MSE) in compressed sensing. Since
then a set of state equations have appeared in studies of logistic regression, ro-
bust estimator and other high-dimensional statistics problems. Recently, a con-
vex Gaussian min-max theorem (CGMT) approach was proposed to study high-
dimensional statistic problems accompanying with another set of different state
equations. This paper provides a uniform viewpoint on these methods and shows
the equivalence of their reduction forms, which causes that the resulting SEs are
essentially equivalent and can be converted into the same expression through pa-
rameter transformations. Combining these results, we show that these different
state equations are derived from several equivalent reduction forms. We believe
that this equivalence will shed light on discovering a deeper structure in high-
dimensional statistics.
1 Introduction
Classical statistical methods often failed in the high-dimensional data where the number of features
is larger than the number of observed samples. Studies in high dimensional data have attracted lots
of attentions in past decades. A set of state equations (SEs) were first introduced in approximate
message passing (AMP) algorithm in (Donoho et al., 2009) to precisely characterize the mean-
square-error (MSE) and the phase transition phenomenon for true signal recovery in compressed
sensing (CS). Since then, SEs, associated to certain AMP algorithm, have played indispensable role
in various high-dimensional problems. For example, (Donoho et al., 2011) investigated the phase
transition phenomenon and the precise MSE of LASSO estimator; (Donoho & Montanari, 2016)
studied the variance of asymptotic distribution of M-estimator; (Huang, 2020) provided a precise
characterization of min-max MSE of l1 penalized robust M-estimator and the corresponding phase
transition phenomenon.
Though the SEs were first introduced through certain AMP type algorithms, researchers meet them
in a variety of models through different methods. For example, the SEs appeared in (El Karoui et al.,
2013) when they performed the leaving-one-out (LOO) analysis of M-estimator in high dimensions.
They showed that asymptotic normality, asymptotically unbiased property also hold as in the low
dimension, nevertheless the variance of asymptotic distribution of M-estimators is higher. (Sur &
Candes, 2019) employed the similar idea to analyze the properties of MLE in logistic regression
where the SEs were used to show that (1) asymptotically unbiased property does not hold; (2)
variance of asymptotic distribution increases; (3) likelihood ratio test is not distributed as chi-square.
SEs also appeared in another line of researches where Thrompoulidies et al. performed analysis of
a family of high dimensional problems through the Convex Gaussian min-max theorem (CGMT).
More precisely, (Thrampoulidis et al., 2018) characterized the MSE precisely for general regularized
M-estimator problem in high-dimensions; (Salehi et al., 2019) established the correlation and MSE
of the resulting estimator of regularized logistic regression; (Deng et al., 2019) showed the changing
trend of MSE with the growth of features in support vector machine and logistic regression.
Lastly, an insightful series of works (Barbier et al., 2019; Ricci-Tersenghi & Semerjian, 2009;
Moore, 2014; Krzakala et al., 2016; Coja-Oghlan et al., 2018; Mezard & Parisi, 2003; Del Ferraro
et al., 2014) have utilized the SEs (named as cavity method in statistical physics) as a ubiquitous
tool when they studied the high dimensional statistical problem through the perspective of statisti-
1
Under review as a conference paper at ICLR 2022
cal physics. Importantly, this tool has exhibited as a powerful weapon in applications of a lot of
fields(Mezard & Montanari, 2009; Obuchi & Kabashima, 2016; Vuffray, 2014; Lesieur et al., 2015;
2016).
Though many papers have explicitly written down the corresponding state equations, none of them
have shown that these sets of state equations are compatible. To the best of our knowledge, only
(Deng et al., 2019) mentioned there is another set of state equation but without any comparison.
Although SEs were proved to be important in high dimensional problems, it is awkward that for
one specific problem, the resulting SEs from AMP, CGMT and LOO are different. To be more
clear, let us take a look at logistic regression. The SEs derived from CGMT (20) in (Deng et al.,
2019) are obviously different from the SEs derived from LOO (19) in (SUr & Candes, 2019). This
is annoying, since the asymptotic performance for a specific high-dimensional problem should be
unique no matter which method was used.
Therefore, we are interested in the following questions:
Are SEs derived from different methods all equivalent in some sense? If so, from what viewpoint
these methods are equivalent and are there more inner equivalence?
Among them, as the most direct, accessible, basic tool, equivalence of SEs is the basis of equivalence
of methods and more inner equivalence.
Our contributions. We successfully show that for various high dimensional problem, the different
sets of SEs derived through different methods are actually equivalent to each other. More precisely,
we construct the equivalence between different sets of SEs through explicit parameter transforms for
LASSO, M-estimator and logistic regression. These transformations are inspired by the statistical
meanings of certain quantities appeared in the SEs. Moreover, we also provide a heuristic expla-
nation on the relation between the different methods: AMP, CGMT and LOO. To the best of our
knowledge, this is the first work to clearly clarify the equivalence among SEs derived from different
methods and try to establish the equivalence of different methods.
Outlines. In section 2, we show that the SEs for M-estimator from AMP, LOO and CGMT are
equivalent to each other. In section 3.1, we show the equivalence of SEs derived from AMP and
CGMT for another example and explain the essential reasons behind this equivalence. In Section
3.2, we illustrate the similar work regarding the equivalence between CGMT and LOO. Section 4
provides some discussions and future directions. Most proofs are deferred to the appendix.
Notations. Let N (0, Id),N(0, 1) denote the d-dimensional standard Gaussian distribution and 1-
dimensional standard Gaussian distribution respectively. For a vector x, we denote kxkp as the lp
norm of x. For an integer n we denote [n] as {1,…，n}. We abbreviate independent and identically
distributed to i.i.d.. For a function f : R 7→ R, variable x ∈ R and t > 0, we denote the Moreau
envelope associated with f as
Mf (x; t) :
min
z∈R
f(Z)+2t(X -Z)2
(1)
and the proximal operator, which is the solution of this minimization as
Proxf (x; t) := arg min f(z) + ^(X - z)2.	(2)
For multi-dimensional case X = (xι, •…，xd)T ∈ Rd, Moreau envelope and proximal operator are
applied element-wisely: Mf (x; t) := (Mf (xi; t)) ∈ Rd and P roxf (x; t) := (P roxf (xi; t)) ∈ Rd.
2 An illustrative example
Suppose that Xii.匕 N(0,11d) and y ∈ R satisfying that
y = XTβ* + q, for i ∈ [n]	(3)
where Ei are drawn i.i.d. from distribution Pe with mean 0 and variance σ2. We assume that the
entries βi of β* are independently distributed as Π which has finite second moment r2 = Ee〜∏β2.
2
Under review as a conference paper at ICLR 2022
Let ρ be a non-negative convex function. We are interested in the the Mean-squared-error (MSE)
performance lim%p→∞ ɪ ∣∣β - β*k2 of the M-estimator:
n
β = arg min E P0 - XTβ)	(4)
β i=1
when both n and d go to infinity satisfying that lim%d→∞ d = κ* ∈ (0, ∞).
EI ♦	Λ 1	Γ- .	.	1∙	11	ZT-11 τr	♦	.	1 CC <C∖	1	.1	1	1.1	.	. 1	> jr<-lτ-, i' A	Λ
This problem first studied by (El Karoui et al., 2013) where they showed that the MSE of β can be
characterized by a set of SEs. More precisely, they proved the following proposition.
Proposition 2.1. (El Karoui et al., 2013) Given ratio κ* < 1. Consider the following system of
nonlinear equations (SEs) regarding (τ1, γ1) :
1 - κ = E[dProxρ(W1 + TiZi; λι)]
∂x	(5)
κ*τ2 := E[Wi + TiZi - Proxp(Wi + TiZi; λι)]2
where Wi 〜Pe, Zi 〜N(0,1) is independentof Wi. IfthissystemofnonlinearequationsPossesses
a unique solution (τi,λi), then the τi Is exactly the MSE of β appeared in (4).
The M-estimatorwas also studied by (Donoho & Montanari, 2016) where they proved the following
proposition.
Proposition 2.2. (Donoho & Montanari, 2016) Given ratio κ* < 1. Consider thefollowing system
of nonlinear equations (SEs) regarding (T2, γ2 ) :
T = - λ2E[dMp(W2 + T2Z2; λ2)]2
K * ∂x
∂2 M	(6)
K* = λ2E[ —Mp(W2 + T2Z2; λ2)]
∂x2
where W2 〜 Pe, Z2 〜N (0,1) is independentof W2. Ifthis system ofnonlinear equations possesses
a unique solution (τ2,λ2), then the τ2 is exactly the MSE of β appeared in (4).
Moreover, inspired by the work (Thrampoulidis et al., 2014), we employ the CGMT techniques to
study the M -estimator and show that the asymptotic MSE can be characterized by the the following
SEs. To avoid unnecessary digression, we defer the detailed proof to the appendix A.
Proposition 2.3. Given ratio K* < 1. Consider the following system of nonlinear equations (SEs)
regarding (τ3,ɑ,μ):
0 =W - τ3√κ*------2 E[用P (W3 + T3Z3； a/M)]
2	μ ∂t
0 = 一 μ√κ* + E[Z3 ∂χP (W3 + T3Z3； α∕μ)]	(7)
0 = μ +--E[内 P (W3 + T3Z3; α∕μ)]
2 μ ∂t
where W3 〜 Pe, Z3 〜N (0,1) is independentof W3. Ifthis system ofnonlinear equations possesses
a unique solution (τ3, ɑ, μ), then the τ3 is exactly the MSE of β appeared in (4).
On the one hand, these three sets of SEs are different at the first glance. On the other hand, since
they are all supposed to describe the MSE of the M -estimators in high dimension, there shall be
some relation between these three sets of equations. A striking fact is that we can actually show
that all these three set of SEs are equivalent to each other. More precisely, we have the following
theorem.
Theorem 1. For M-estimator(4), the SEs derived from AMP (6), LOO (5) and CGMT (7) are equiv-
alent. Specifically, (6) can be converted into the same form as (5) after the following parameter
transformations:
Ti = T2 ,	λi = λ2 .	(8)
3
Under review as a conference paper at ICLR 2022
(6) can be converted into the same form as (7) after the following parameter transformations:
Tl = T3,	λι = α.	(9)
μ
The equivalence of these three sets of SEs seems straightforward, however, it suggests us that all the
three procedures: AMP, CGMT and LOO might be deeply entangled in some sense. This will be
investigated in this manuscript.
The proof of this theorem is deferred to the appendix B.
3 General results
In this section, we show that the aforementioned equivalence between different sets of SEs holds not
only for M-estimator, but also for Lasso and logistic regression in high dimensions.
3.1	Equivalence between the SEs derived from CGMT and AMP
Let us consider the following optimization
mjn1 ky - Xβk2 + λ*kβkι	(10)
β2
where yi = XTβ* + Ei With XiL出 N(0, nId), r* := limn,p→∞ k√√nk and λ* ≥ 0 is the regularized
parameter, Ei are drawn i.i.d. from distribution Pe with mean 0 and variance σ2.
We are interested in the the Mean-SqUared-error(MSE) performance limn,p→∞ n ∣∣β 一 β*∣∣2 of the
LASSO. (Donoho et al., 2011), (Mousavi et al., 2018), (Bayati & Montanari, 2011; Miolane &
Montanari, 2018; Javanmard & Montanari, 2018) have utilized the AMP to study the asymptotic
performance of the Lasso estimator. For our purpose, we briefly recall the results in (Mousavi et al.,
2018) below.
Proposition 3.1. (Mousavi et al., 2018) Given noise scale σ2 and ratio κ*, consider the following
system of nonlinear equations (SEs) regarding (τ1, γ1) :
τ1 = σ2 + κ*E[η(β1 + τ1Z1; λ* + YI) — β1]2
Yi = κ*(γι + λ*)E[η0(βι + TiZi； λ* + Yi)]
(11)
where Zi 〜N(0,1) is a standard normal variable, βι 〜Π is independent of Zi, η(∙; ∙) is the soft
threshold function:
η(x; t) := sign(x)(|x| - t)+,
x+ means max{x, 0} and
1
sign(x) :=	0
1-1
if x > 0
if x = 0.
if x < 0
Ifthis system of nonlinear equations possesses a unique solution (Ti, λi), then the Ti is exactly the
λ t c∙ I ` r A	1 ∙ ∕τc∖
MSE ofβ appeared in (10).
Inspired by the sequence of work (Thrampoulidis et al., 2014; 2015; 2018; Salehi et al., 2019), we
apply the CGMT to study the asymptotic performance of the Lasso estimator appeared in (10) and
find that it is characterized by the following set of SEs.
4
Under review as a conference paper at ICLR 2022
Proposition 3.2. Given noise scale σ2, signal strength r2 in model (3) and ratio κ*, Consider the
following system of nonlinear equations (SEs) regarding (α, σ, τ2, θ, λ, γ2):
0 = - -α + θ - 1 +
στ2
α+ λ
λ + 1
22
0 = — ɪ + 专T-----于E[(Prox∕(γ2Z2 + θβ2; λ*))2] + —
2τ2	2σ 2 τ2	2	f	λ + 1
0 = Y - r2κ*
-σ2 +
2[(α + λ)r2κ* + λσ2]
λ+i
(α + λ)2r2κ* + σ2 + λ2σ2
(IW
0 = r2κ*α - στ2κ*E[β2Proxf(γ2Z2 + θβ2; λ*)]
(12)
λ = στ2κ*E[ MXgYZ + θβ2"*) ]
∂x
22
0 = 2^2 + 2σT2	2^^E[(Proxfr(Y2Z2 + θβ2; λ*))2]
where Z2 〜N(0,1) IS a standard normal variable, β2 〜∏ IS independent of Z2, f (x) := |x|.
Ifthis system of nonlinear equations possesses a unique solution (α, σ, T2, θ, λ, γ2), then the λ2 is
exactly the MSE ofβ appeared in (10)
The detailed proof is deferred until the Appendix C. The following proposition illustrate the equiv-
alence between these two sets of SEs.
Theorem 2. The SEs of LASSO derived from AMP (11) are equivalent to the SEs derived from
CGMT (12). Specifically, (12) can be converted into the same form as (11) after the following
parameter transformations:
τι = γ22, Yi = λ^ - λ*.	(13)
The detailed proof is deferred until Appendix C.1.
We provided a heuristic explanation on the equivalence of the SEs derived from AMP and CGMT.
For the sake of the self-contentment, we briefly review the procedures of how to derive SEs from
AMP and CGMT respectively.
Deriving SEs from AMP. The derivation of SE from AMP can be divided into two stages:
(1)	Constructing an iterative algorithm
1)	AMP first transform initial optimization problem into pursuing a Bayesian posterior distri-
bution where objective function is transformed into a probability distribution.
2)	Based on the corresponding factor graph of this distribution, it invokes the message pass-
ing(MP) algorithm to compute the Bayesian posterior distribution.
3)	The MP is then further approximated by some large system limit, large β limit and the
approximation of iteration.
(2)	The asymptotic behavior of AMP is then characterized by the state evolution equations/SEs.
Deriving SEs from CGMT. The derivation of SE from CGMT can be divided into four steps.
1)	The initial optimization problem is transformed into a min-max form, which is called the
primary optimization (PO) problem.
2)	CGMT perform a dimensionality reduction on PO and obtain the auxiliary optimization
(AO) problem
3)	AO is further simplified to an optimization problem only depending on several scalar vari-
ables, which is called scalar optimization (SO) problem.
5
Under review as a conference paper at ICLR 2022
4)	SEs are derived by finding first-order optimality conditions of the asymptotic version of
SO.
Remark 3.1. We find that AO can be viewed as a relaxation of PO in the sense that the feasible
region ofAO is larger than that of PO. Concrete examples, such as M-estimator, Logistic regression,
Support vector machine and so on, are deferred to the appendix. We believe that this relaxation can
help us understand the equivalence between the resulting SEs from AMP and CGMT respectively.
We now present a uniform viewpoint on AMP and CGMT: 1) Constructing the AMP corresponds
to the first step of CGMT in LASSO, which suggests that the iteration of AMP is actually equiv-
alent to the process of solving PO. 2) Deriving the SEs from AMP corresponds to the last three
steps of CGMT. Both of them aim to deriving SEs and characterizing asymptotic performance by
approximating the initial optimization problem. We proved the first statement in Proposition 3.3.
Proposition 3.3. (Rangan et al., 2016) For LASSO, the fixed point of AMP is just the solution of
first-order optimality conditions of PO in CGMT.
Proof. For CGMT, by introducing u to constrain u = Xβ and Lagrange vectorv, the corresponding
PO can be written as:
minmax 1 ∣∣u∣∣2 - yτU + 1 ∣∣yk2 + λ*kβ∣h + VT(Xe - u).
β,u v 2	2
Consider the first-order optimality conditions of PO:
0 = λ*sign(β) + XT v
0 = u - y - v	(14)
0 = u - Xβ.
Comparing above formulas in (14) leads to
λ*sign(β)+ XT(Xe - y) =0.
For AMP algorithm, the iteration of LASSO is
βt+1 = η(et + XT zt; λ* + Yt)
∂
Zt = y - Xet + κ,zt-1h—η(βt-1 + XTzt-1; λ* + γt-1)i
∂x
∂
γ = κ*(λ* + γ	)h~^~η(e	+ X Z	;λ* + γ )i
∂x
where ∂χ acts ComPonent-Wisely. For some vector x,〈x):= Pid=ι Xi denotes the entry-sum of x.
The fixed point (e∞ , Z∞ , γ∞ ) satisfy the following equations:
0=(λ* + Y)Sign(β) + β - (β + XTZ)	(15a)
z = y - Xe + κ* z ∙ C	(15b)
Y = κ*(λ* + Y)C	(15c)
where C = c(β, z, γ) =〈∂χη(β + XTZ;λ* + γ)) and (15a) is given by the following property
about the soft thresholding function:
t ∙ sign(Z) + Z — x = 0
for z = η(x; t) and some scalar x.
Simplifying (15b) and (15c) leads to:
_ y - Xβ
Z = ^1
1 — κ*c
κ*cλ*
Y =；------.
-κ* C
(16)
6
Under review as a conference paper at ICLR 2022
Comparing (16) with (15a) gives,
Ci— sign(β) — 富一XT (y — Xe) = 0.
1 — κ*c	1 — κ*c
which finishes the proof.
□
Remark 3.2. It needs to be discussed component-wisely according to whether each entry of the
optimal β is 0 or not. The above proof holds for the entries that βi 6= 0. For i such that βi = 0,
the optimality from PO gives —λ* + (XT(Xe — y))i < 0 and λ* + (XT(Xe — y))i > 0. This is
equivalent to |(XT (Xβ —y))i| ≤ λi, where (XT (Xβ — y))i denote the i-th entry of XT (Xβ —y).
This is still equivalent to the fixed-point condition in AMP. Hence the equivalence holds for all
entries of β.
3.2 Equivalence between the SEs derived from CGMT and LOO
Suppose that Xi i~. N(0,11d) and yi ∈ { —1,1} drawn from logistic model:
P(yi = 1|xi) = ρ0(xiT βi), fori ∈ [n]
(17)
where ρ(t) = log(1 + et). Each entry ofβ is independently distributed as Π which has finite second
moment r2 = Eβ~∏β2.
We are interested in the following optimization problem:
1n
β = arg mβn n £ '(y，xTβ)
i=1
(18)
where `(t) := log(1 + e-t). When the β exists, we are interested in the the Mean-squared-
error(MSE) performance lim%p→∞ n ∣∣β — β*k2 of the Logistic regression.
Logistic regression in high dimensions have been studied recently by (CandeS & Sur, 2020; Mousavi
et al., 2018), (Deng et al., 2019). The asymptotic MSE ofe was characterized by the following two
propositions.
Proposition 3.4. (Sur & Candes, 2019) Given signal strength r2 in logistic model (17) and ratio
κi, Consider the following system of nonlinear equations (SEs) regarding (λ1, α1, σ):
a：==E[2ρ0(Q1)(λ1ρ0(ProXρ(Q2; λι)))2]
κ2i
0 = E[ρ0(Q1)Q1λ1ρ0(Proxρ(Q2; λι))]
1	_ κ = E[	2ρ0(Qι)	]
*= [1 + λιρ00(ProXρ(Q2; λι))]
(19)
where
(Q1,Q2) ~N 0;
22
r2	—or：
一σr2 σ2r2 + α2κ*
and ρ(t) := log(1 + et).
IfthiS system of nonlinear equations possesses a unique solution (λ1,α1, σ), then the MSE of β
appeared in (18) is [(σ — 1)Eβ~∏β]2 + α2.
Remark 3.3. In (Sur & CandeS,2019), it is assumed that Xij ~ N(0, ɪId) and r2 = κ*Eβ~∏β2,
which is slightly different from the setting in this paper. However, this difference only leads to a
constant change related to κi in the final parameter transformations (21) and does not affect the
equivalence of these two set of SE.
Proposition 3.5. (Deng et al., 2019) Given signal strength ri2 in logistic model (17) and ratio κi,
Consider the following system of nonlinear equations (SEs) regarding (λ2, α2,μ):
0 = E[V'0(Prox'(α2Z + μV; λ2))]
α2Ki = λ2E[('0(Prox'(a：Z + μV; λ2)))2]
_ ʌ fγ	'0(Pr0X'(α2Z + μV； λ2))
κ* = 2E[1 + λ'00(ProX'(α2Z + μV; λ2))]
(20)
7
Under review as a conference paper at ICLR 2022
where Z 〜N(0,1), V = ZιK*, in which Zi 〜N(0,1) is independent of Z and 匕* 〜
Ber(PgZι)). Ber(P) denotes the Bernoulli distribution with probability P for the value +1 and
probability 1 -pfor the value -1. If this system of nonlinear equations possesses a unique solution
(λ2,α2,μ), then the MSE of β appeared in (18) is [(-⅛ - 1)Eβ〜∏β]2 + (μ-)2.
κ*	r*
As before, we can show that (19) and (20) are equivalent.
Theorem 3. For logistic regression (18), the SEs derived from LOO (19) and CGMT (20) are equiv-
alent. Specifically, (19) can be converted into the same form as (20) after the following parameter
transformations:
αι = -Oi=,	σ = —, λι = λ2.	(21)
√κ*	r*	' ,
The proof of this theorem is deferred to appendix E
For the sake of self-contentment, we briefly review the procedure on deriving SEs from LOO.
Deriving SEs from LOO The derivation can be divided into 4 steps.
1)	First, for the original optimization problem, LOO considers first-order conditions of three
cases: a) keeping all observations and predictors, corresponding solution is denoted as β,
b) leaving one predictor, corresponding solution is denoted as β(-j) and c) leaving one
predictor and one observation, corresponding solution is denoted as β(-i),(-j)
2)	Two properties are derived from comparing three version of first-order conditions: a) The
i-th fitted value Xiβ has an asymptotic expression composed of two independent random
vectors Xi,(-j) and β(-i),(-j). b) Each coordinate βj can be written as a sum of n random
variables which are asymptotically independent.
3)	Using above two properties , βj has the same distribution as a combination of several scalar
variables when n, P → ∞. Hence every statistic of β (such as expectation, variance and
first order condition of optimization) can be expressed by these scalar variables, from which
the SEs of β are derived.
Briefly reviewing the procedures of LOO approach, we find that the sample matrix X (which is a
Rn×p Gaussian matrix)is decomposed into two independent Gaussian vectors through some special
techniques in both LOO and CGMT, which allows the law of large numbers to simplify the first-
order equations into scalar equations. This may help us understand the equivalence between CGMT
and LOO. The more intrinsic equivalence of these two methods is still under investigation.
4	Discussion and future directions
In this paper, we first showed that for the high dimensional M -estimator, the three sets of SEs
derived from AMP, CGMT and LOO are equivalent. We then further show that this equivalence
actually appears in various high dimensional problems. This strongly suggests us that there should
be a deep relation between these three approaches.
Though AMP, CGMT and LOO are different at the first glance, we find that they all can be treated as
approximations of the same first order optimality conditions. To be more precise, LOO decouples the
correlation between samples and estimator after comparing first-order optimality conditions of the
initial optimization with two leaving-one-out version; CGMT simplifies the first-order optimality
conditions by making some relaxation of the PO problem; AMP solves the first order optimality
conditions directly. All their asymptotic behaviours are characterized by the corresponding SEs
respectively. The equivalence between these SEs sheds us light on looking for a more comprehensive
theories to explain this intriguing phenomenon.
8
Under review as a conference paper at ICLR 2022
References
Jean Barbier, Florent Krzakala, Nicolas Macris, Leo Miolane, and Lenka Zdeborova. Optimal errors
and phase transitions in high-dimensional generalized linear models. Proceedings of the National
AcademyofSciences,116(12):5451-5460, 2019.
Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. IEEE Transactions on
Information Theory, 58(4):1997-2017, 2011.
Emmanuel J Candes and Pragya Sur. The phase transition for the existence of the maximum like-
lihood estimate in high-dimensional logistic regression. The Annals of Statistics, 48(1):27-42,
2020.
Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborova. Information-theoretic
thresholds from the cavity method. Advances in Mathematics, 333:694-795, 2018.
Gino Del Ferraro, Chuang Wang, Dani Mart´, and Marc Mezard. Cavity method: Message passing
from a physics perspective. arXiv preprint arXiv:1409.3048, 2014.
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-
dimensional binary linear classification. arXiv preprint arXiv:1911.05822, 2019.
David Donoho and Andrea Montanari. High dimensional robust m-estimation: Asymptotic variance
via approximate message passing. Probability Theory and Related Fields, 166(3):935-969, 2016.
David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for com-
pressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914-18919, 2009.
David L Donoho, Arian Maleki, and Andrea Montanari. The noise-sensitivity phase transition in
compressed sensing. IEEE Transactions on Information Theory, 57(10):6920-6941, 2011.
Noureddine El Karoui, Derek Bean, Peter J Bickel, Chinghway Lim, and Bin Yu. On robust regres-
sion with high-dimensional predictors. Proceedings of the National Academy of Sciences, 110
(36):14557-14562, 2013.
Hanwen Huang. Asymptotic risk and phase transition of l_{1}-penalized robust estimator. The
Annals of Statistics, 48(5):3090-3111, 2020.
Adel Javanmard and Andrea Montanari. Debiasing the lasso: Optimal sample size for gaussian
designs. The Annals of Statistics, 46(6A):2593-2622, 2018.
Florent Krzakala, Federico Ricci-Tersenghi, Lenka Zdeborova, Eric W Tramel, Riccardo Zecchina,
and Leticia F Cugliandolo. Statistical Physics, Optimization, Inference, and Message-Passing
Algorithms: Lecture Notes of the Les Houches School of Physics: Special Issue, October 2013.
Number 2013. Oxford University Press, 2016.
Thibault Lesieur, Florent Krzakala, and Lenka Zdeborova. Mmse of probabilistic low-rank ma-
trix estimation: Universality with respect to the output channel. In 2015 53rd Annual Allerton
Conference on Communication, Control, and Computing (Allerton), pp. 680-687. IEEE, 2015.
Thibault Lesieur, Caterina De Bacco, Jess Banks, Florent Krzakala, Cris Moore, and Lenka Zde-
borova´. Phase transitions and optimal algorithms in high-dimensional gaussian mixture clus-
tering. In 2016 54th Annual Allerton Conference on Communication, Control, and Computing
(Allerton), pp. 601-608. IEEE, 2016.
Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University
Press, 2009.
Marc MeZard and Giorgio Parisi. The cavity method at zero temperature. Journal of Statistical
Physics, 111(1):1-34, 2003.
Leo Miolane and Andrea Montanari. The distribution of the lasso: Uniform control over sparse balls
and adaptive parameter tuning. arXiv preprint arXiv:1811.01212, 2018.
9
Under review as a conference paper at ICLR 2022
Cristopher Moore. The cavity method, belief propagation, and phase transitions in community
detection. In APS March Meeting Abstracts, volume 2014,pp. D14-002, 2014.
Ali Mousavi, Arian Maleki, and Richard G Baraniuk. Consistent parameter estimation for lasso and
approximate message passing. The Annals ofStatistics, 46(1):119-148, 2018.
Tomoyuki Obuchi and Yoshiyuki Kabashima. Cross validation in lasso and its acceleration. Journal
of Statistical Mechanics: Theory and Experiment, 2016(5):053304, 2016.
Sundeep Rangan, Philip Schniter, Erwin Riegler, Alyson K Fletcher, and Volkan Cevher. Fixed
points of generalized approximate message passing with arbitrary matrices. IEEE Transactions
on Information Theory, 62(12):7464-7474, 2016.
Federico Ricci-Tersenghi and Guilhem Semerjian. On the cavity method for decimated random con-
straint satisfaction problems and the analysis of belief propagation guided decimation algorithms.
Journal of Statistical Mechanics: Theory and Experiment, 2009(09):P09001, 2009.
Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi. The impact of regularization on high-
dimensional logistic regression. arXiv preprint arXiv:1906.03761, 2019.
Pragya Sur and Emmanuel J Candes. A modern maximum-likelihood theory for high-dimensional
logistic regression. Proceedings of the National Academy of Sciences, 116(29):14516-14525,
2019.
Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. A tight version of the gaussian min-max
theorem in the presence of convexity. 2014.
Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A precise
analysis of the estimation error. In Conference on Learning Theory, pp. 1683-1709. PMLR, 2015.
Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized
m-estimators in high dimensions. IEEE Transactions on Information Theory, 64(8):5592-5628,
2018.
Marc Vuffray. The cavity method in coding theory. Technical report, EPFL, 2014.
10
Under review as a conference paper at ICLR 2022
A Proof of Proposition 2.3
By the following linear parameter transformation:
W := β - β*,
the M-estimator optimization problem becomes:
1n
min 一 V"ρ(∈i -
wn
i=1
T
xiT w).
(22)
Introducing the Lagrange multiplier leads to:
min max
w,v u
1n
一£p(Vi)+
n i=1
1T
J=Ui(Vi - Ci + Xi W)
where u := (u1, ..., un); v = (v1, ...,vn).
Then we rewrite it in the matrix form:
1 T 1n	1 T T
min max -J=UTXW +—— ρ^ p(vi) +-------产(UTv — uτe)
w,v u n	n	n
where := (C1, ..., Cn)T. This is just the PO problem in CGMT.
Denote X = √nX, W = √wn, then We have Xi,j i" N(0,1), W = β√-β*. This means ∣∣w∣∣2
is just the MSE of interest and X is a standard Gaussian matrix composed of iid standard normal
variable.
However, in the following, we rewrite X, W as X, W respectively for the simplicity of notation.
Using CGMT about X as in (Salehi et al., 2019) by Corollary 3 in it, then the AO problem associated
to it is the following min-max problem:
min max—^(∣∣u∣∣2gτ W + ∣∣W∣∣2hτ u) +
w,v u n
1n	1
-XP(Vi) + √(UTV - UTe)
n匕	√n
where g ∈ Rd and h ∈ Rn have i.i.d. N(0, 1) entries.
Let kWk2 = τ3, note that now
T = -llβ - β*l∣2	(23)
n
which is just the MSE.
Then the optimization becomes:
1	T 1n	1 T T
min max -(-τ3∣∣u∣∣2∣∣g∣∣2 + TβhT u) + — TP(Vi) + 7(UT V - UT e).
τ3 ,v u n	n	n
Letting ∣∣u∣∣2 = μ, then We have the following optimization:
τ	1n
min max -岑 ∣∣g∣∣2 + — £ ρg + 隼 ∣∣τ h + V - eg.
τ3,v μ>0	ʌ/n	n	ʌ/n
(24)
equivalently:
n
i=1
+
min max —	P(Vi)
τ3,v μ>0 n
I∣τ3h + v - e∣∣2
τ3
√n
||g||2).
(25)
11
Under review as a conference paper at ICLR 2022
In order to make the ∣∣τ3h + V - d∣2 separable, We use the following optimization:
2
X = min - + —
α>0 2	2α
for any X and α > 0. Replace X by √1n ∣∣τ3h + V - e∣∣2, the optimization problem (25) becomes:
min max - √ l∣g∣∣2 + 1 X P(Vi) + μ(maχ α + ɪ ∣∣τ3h + V - e||2).
τ3,v μ ʌ/n	n Zed	a>0 2	2αn
i=1
Then what we want to do the scalarization procedure: make the optimization about V becoming
optimization about a scalar. First, flipping in the order of min-max by (Thrampoulidis et al., 2018):
min max αμ -
τ3,a μ 2
-√= ||g||2 +---[min ^X P(Vi ) + F(T3 hi + Vi - Ci )2].
n	n v	2α
i=1
(26)
Introducing Moreau envelope function Mρ(X; t), the optimization problem (26) becomes:
α	τ	1n
minmax _7T^------7=||g||2 + ^X MP(Ci - τ3hi; a∕μ).
T3,α μ 2	√n	nz—z
i=1
By Lemma 9 in Appendix A in (Thrampoulidis et al., 2018), considering asymptotic n,p →
∞,p∕n → κ* leads to:
αμ τμ	1 ^n	C C αμ
-------||g||2 +	^X MP(Ci - τ3hi; α∕μ) → —-τ3μ√κ* + EMP(W3 - τ3Z3; α∕μ)
2n	n-2
i=1
where Z3 〜N(0,1) is independent of everything else.
Introduce W3 〜Pe. Then, asymptotically, we can deal with the following problem:
min max αμ — τ3μ√κζ + EMP(W3 + τ3Z; α∕μ).	(27)
τ3,a μ 2
Denoting the objective function of (27) by φ, then since φ is convex about (τ3, α) and concave about
μ, the saddle point of φ can be precisely characterized by its first order optimality condition:
=- = 0	⇒ 77	- τ3√κ^----2 E[	AJ (W3	+ τ3Z3; α∕μ)]	= 0
∂μ	2	μ2	∂t
=- = 0 ⇒ -μ√κ7 + E[Z3 A P (W3 + τ3Z3; α∕μ)] = 0
∂τ	∂X
=- = 0 ⇒ μ H---E[ A P (W3 + τ3Z3; α∕μ)] = 0.
∂a	2 μ ∂t
Combining with (23) completes the proof.
B Equivalence of SEs of M-estimator from AMP, LOO, CGMT
Recall that the SEs of M-estimator from LOO are:
1 - K* = E[dpr0x^(W1 + T1Z1; λι)]
∂X
K*T12 := E[W1 + T1Z1 - ProXP(W1 + T1Z1; λ1)]2.
SEs of M-estimator from AMP are:
T = 1 λ2 e[孚(W2 + τ2Z2; λ2)]2
K * ∂x
∂2M
κ* = λ2E[ A 2 (W2 + τ2Z2; λ2)].
∂X2
(28)
(29)
12
Under review as a conference paper at ICLR 2022
Next, we show that these two sets of SEs are equivalent.
First, Simple calculation leads to:
aM∂Tt = P(PrOXp(χ t))
∂2Mρ(x, t)
∂x2
00	∂ProX (X; t)
P0(Proxρ(x; t))——∂X
ρ00(P rOxρ(x; t))
1 + tρ00 (PrOXP(x; t))
(30)
x - Prox (x; t)
------1 ρ' - = P(PrOXP(x; t)).
Combine this, SEs from AMP can be rewritten as:
T2 = - λ2E[ρ0(Proxρ(W2 + TZ2； λ2))]2
K *
=λ E[	ρ00(ProXP(W2 + T2Z2； λ2))	]
κ* = 2 [1 + λ2ρ00(Proxρ(W2 + τ2Z2; λ2))].
(31)
which verify that SEs from LOO and SEs from AMP are equivalent.
Next we prove equivalence of SEs from AMP and SEs from CGMT by parameter transformations
suggested.
Recall the SEs from CGMT are:
a	I_	α -∂Mρ /…	“	，C
-—τ3√κ* ——2E[--^(W3 + τ3Z3; α∕μ)] = 0
2	μ ∂t
-μ√κ* + E[Z3 ∂X (W3 + T3Z3； α∕μ)] = 0
μ + —E[~κr^(W3 + T3Z3； α∕μ)] = 0.
2 μ ∂t
Let b = μ, then We have:
μb J______ α -∂Mρ /…	”…
— - T3√κ7 ——2E[二^卫(W3 + T3Z3； b)] = 0
2	μ ∂t
-μ√κ* + E[Z3 ∂X (W3 + τ3Z3; b)] = 0
μ + μE[ d；(W+TZ；b)] = 0.
Comparing these results leads to:
E[dM (W3 + T3Z3； b)]2 =警E[d∂M (W3 + T3Z3； b)] = K*.	(32)
Combining stein lemma and
ρ(W (W3 + T3Z3； b) = -ɪ [ a?P (W3 + T3Z3； b)]2.	(33)
∂t	2 ∂x
completes our proof.
C Equivalence of SEs of LASSO from AMP and CGMT
In this proof, We refer to the technique developed in (Donoho & Montanari, 2016). The Lasso
problem solves
13
Under review as a conference paper at ICLR 2022
argmin λtkβkι + ɪky - Xβk2.	(34)
β n	2n
Notice that ky - Xβk2 = Pin=1 (xiT β)2 - 2yixiTβ + yi2 . The optimization can be transformed
into
arg min - IlellI +
βn
1n 1
—X 2(XiIβ) - yiχiβ
n i=1 2
(35)
In the following proof, we consider a more general optimization than Lasso:
λ	1n
arg %n —f (β) + - E [1TP(U) — yixTβ]
βn	n
i=1
(36)
where ρ(∙) and f (∙) are separable m the sense that there exist scalar functions ρ(∙), f (∙) so that
ρ(∙) and f (∙) can be expressed as the following form: P(X) = (ρ(χι),…，ρ(xd))T and f (x)=
Pd=ι f(xi). In particular, in Lasso, ρ(t) = 212 and f (x) = ∣∣x∣∣ι.
In order to apply CGMT, we introduce a new variable u and have following optimization
min匕f (β) + 1 (ITP(U) - yTU)
β,u n	n
s.t. u = Xe = ɪ H*β
n
(37)
where H* = √nX ∈ Rn×d and hence Hji吧 N(0,1). By using Lagrange multiplier We can
rewrite (37) as a min-max optimization:
min	max —1T P(U) —yT U +—* l∣ekι +—VT (U —^H*e).	(38)
β∈Rd,u∈Rn v n	n	n	n	n
Denote P =:伊口2 as the projection matrix of true signal β* and P⊥ = Id - P as the orthogonal
complement. To apply CGMT, we need first decompose H* into
H; = H* ∙ P, H* = H* ∙ P⊥
H* =H1* + H*2 .
(39)
In addition, Recalling the linear model (3) We have y = Xβ* + e = √^H*β* = √^H*β*. Hence
(38) can be rewritten as
min max —1T P(U) —yT U +—; f (e) +—VT (U —^Hje)--J=Vr H*e∙	(40)
β∈Rd,u∈Rn v n	n	n	n	n 1 n n 2
By using CGMT for VTH2*e as in (Salehi et al., 2019) by Corollary 3 in it, the corresponding AO
of (40) is
min max，1t P(U) — LyT U + — f (β) + 工 VT (u---^HIe)
β∈Rd,u∈Rn v n	n	n	n	n 1
-n√n (VT h∣P⊥e∣2 + kv∣2gT P ⊥e)
where h 〜N(0, In) and g 〜N(0, Id) are two independent gaussian vectors.
(41)
14
Under review as a conference paper at ICLR 2022
We first consider the maximization with respect to the direction of v. The part related to v in
optimization (41) is:
max -^kvk2gTP⊥β +----VT (U----^Hlβ-----^hkP⊥βk2
v∈Rn n n	n	n	n
Denoting r := k√k^ and maximizing along the direction of v give
max r (一gTP⊥β + Il —^u--Hιβ -  ---2hkhl∣2).
r≥0	n	n n	n
(42)
(43)
Inserting this into (41) gives
min max 11T P(U) -1 yτ u + λf(β) + r (1 grP⊥β +
β,u r≥0 n	n	n	n
ɪ u-1 HW-∣⅛ h
n n1	n
(44)
In addition, introduce μ to replace β in ∣∣βkι Lagrangian and w. Then (44) can be rewritten as,
min max-^∖τP(U) — LyTU + — f(μ) + r (LgTP⊥β)
u∈Rn; r≥0; n	n	n	n
β,μ∈Rd w∈Rd
+ r √ru - 1 H；e -kP⊥βk2 h	+1 wT(μ - β).
n n	n 2 d
(45)
We define a = 鼠帛,σ = 击∣∣P⊥βk2 and q = H*√ξ∙ where ri* = k√k2. Then q is a standard
Gaussian vector and
_H*β = _H*(Pβ) = -∙ αβ* = _r 1*√nq.	(46)
nn	n	n
Decomposing w = (P + P⊥)w, then the last item in (45) can be rewritten as
dwT(μ - β) = d(PW)Tμ + d(P⊥w)tμ - d(PW)Tβ - d(P⊥w)tβ.	(47)
d	dd	dd
Inserting (46) and (47) into (45), we have,
min max-^∖τρ(u) — LyTU + — f(μ) + r (LgTP⊥β) — ^(P⊥w)τβ
u∈Rn; r≥0; n	n	n	n	d
β,μ∈Rd w∈Rd
+ r ɪU - αr1*q ——σ=h + 1(Pw)τμ + 1(P⊥w)τμ - 1(Pw)τβ.
n n n 2 d	d	d
(48)
Then we can fix Pβ and consider the minimization along the direction of P⊥β. Considering the
optimization related to P⊥β, we have
min rgτP⊥β - (~(Ρ⊥w)τβ = min(rgT - wwτ)P⊥β
P⊥β n	d	P⊥β n d
=-kP⊥βk2 ∙kLP⊥g - 1P⊥w∣2	(49)
nd
=-σ ∙ l∣-^P⊥g - ∖17—P⊥wk2.
n	dκ*
15
Under review as a conference paper at ICLR 2022
Notice that (48) reaches optimal when μ = β. Inserting (49) into (48) leads to
min	max1 ITρ(u) - 1 yτU + λ-f(μ) - σ ∙ ∣∣ -^P⊥g - ʌʌɪP⊥Wk
u∈Rn,μ∈Rd; r≥0; n	n	n	√n	V dκ-
α∈R,σ≥0 w∈Rd	(50)
+r
1 αr1- σ
√nu √n q √n
h + d(P ⊥w)τ μ.
For the simplifying procedures in the following steps in our analysis, we change ∣∣ ∙ ∣∣2 → ∣∣ ∙ ∣∣2 by
rx = min r- + rvx2
v≥0 2v	2
-σx = max ————στx2.
τ≥0 2τ	2
Applying (51), we are able to rewrite (50) as
min	max 11TP(U)-
u∈Rn,μ∈R%α∈R,v,σ≥0 w∈Rd; n
r,τ ≥0
σ στ r
2τ- τ k√n
r rv
+ 2V + T
1	αr1-	σ
尸U---Lq---尸h
2 + d(P ⊥W)T μ.
yTu U +-- f (μ)
n
n
—
(52)
Optimization with respect to w: Next we consider the maximization with respect to w . We first
extract the item related to w in (52) and apply the completion of squares:
στ
max ——
w2
στ
max ——
w2
√rnp ⊥g-
ɪ P ⊥w	+ 1(P ⊥w)t μ
d
dκ
,*
rnP ⊥g - dK dK- p ⊥w + pd/^TP ⊥μ
(53)
1
+ 2nστ
∣∣P ⊥μ + στrP ⊥g∣∣
2
2 στr2
2n
∣P⊥g∣2.
1
—
2
1) For the last item in (53), since g 〜N(0, Id) and P⊥ is a (n - 1)-dimensional projection
matrix, WederivethatkP⊥g∣2 〜∣∣d(0, (P⊥)2)∣∣2 = χd-ι and
στr2IlP⊥g∣∣2 →. στr2κ-.
2n 11 y"	2
2)	Since P⊥ = Id - P, the second item in (53) can be rewritten as
1	∣∣P⊥μ + στrP⊥g∣∣2 =1 ∣∣μ + στrg∣2 - 1 ∣∣Pμ∣2
n	nn
(54)
S kPgk2 - 2στr (Pg)Tμ.
(55)
—
n
n
The last two items of (55) can be omitted in the limit of d,n → ∞ because kPgk
n
and 1 (Pg)Tμ = Op(√n). The second item of (55) is ɪ ∣∣Pμ∣2 = 1 ∣∣Pβ∣2 =
definition.
3) The first item in (53) reaches 0 when maximizing w.
=Op( 1)
α2r12 by
The optimization (52) now can be rewritten as
16
Under review as a conference paper at ICLR 2022
min	max — 1Tρ(u)-
u∈Rn,μ∈Rd; r,T≥0 n
α∈R,v,σ≥0
r rv
+ 2V + T
yTuU +—*f(μ)- nn	σ-	2 2 + 2nστkμ+στrgk -		22 α2r2* 2στ
-	αr1* -J= U —广 q - n n	√σnh	2 .	
—
στr2κ*
2^^
(56)
Optimization respect to μ: Consider the items related to μ in (56)
min — f (μ) +
μ∈Rd n
2nστkμ+στrgk2
(57)
s.t.
Notice that g ~N(0,Id), kμ + στrg∣∣2 = ∣∣μ - στrg∣∣2. We rewrite (57) as
s.t.
min λi f (μ) +	kμ - στrg∣2
μ∈Rd n	2nστ
-β*T μ = - β*T β = - nαr2* = αr2*.
nnn
(58)
Introducing Lagrangian θ, (58) can be rewrite as
-
min max------
μ∈Rd θ∈R 2nστ
kμ - στrg∣2 + 匕f (μ) - θβ*Tμ + αθr2*.
nn
(59)
Applying the completion of squares to 1,3 items in (59) we have,
-	θ-
2nστkμ-στrgk - nβ μ = 2nστkμ - στrg - θστβ k
-黑 kβ*∣2- * gT β*∙
(60)
The third item can be omitted since g^f- = Op (√n) and the second item has limit
__(θστ) 11 W * Il2  ∖  (θστ)	2 2 —  στθ r 1 - ‰Γp∏pp ∖x∕α rα∖x∕τ4f a τ*iohf cirlp cf (AC) CIC
2nστ kβ k -→	2nστ nr1* =	2	. HenCe We rewrite right side of (60) as
-	θ	-	στθ2 r2
2nστkμ-στrgk -nβ μ = 2nστkμ-στrg-θστβ k --r~.	(61)
Next, denote f(x) as single-entry form of f (x). We can rewrite the (61) in terms of Moreau envelope
entry-wisely as follows
-
min max------
μ∈Rd θ∈R 2nστ
λθ
k"-στrgk + -f(μ)- nβ μ+αθr1*
-	στ θ2r2
max -Mλ f(στ(rg + θβ ); στ) + αθr2*------------.
(62)
Substituting (62) in (56) we have,
min
u∈Rn ;
α∈R,σ,v≥0
θ∈R
max- 1T ρ(u)--yT U + =
r,τ ≥0; n	n	2
22
σ _ α2r2*
2τ 2στ
στ r2κ*	r	-	στθ2 r2
—5— + ʒ- + —Mλ-f(στ(rg + θβ )；στ) + αθr2*-------厂
2	2v	n	2
(63)
μ = β
2
17
Under review as a conference paper at ICLR 2022
Optimization respect to u: First we consider the items related to u. The optimization is
min 11TP(U)-
u∈Rn n
1 T rv
(64)
n
Applying the completion of squares we have,
2
2
1 T rv
-ny U+τ
1	αrι*	σ J
诙 U-Tnq -√nh
rv
1	αrι*	σ	1
nuu - √nq -√nh - rv√ny
2rvn
Il ||2 r1*α T
kyk - -y q -
σT
-y h.
n
(65)
—
1
Using the strong law of large numbers we have,
—
2-kyk2 → -r⅛+£2
2rvn	2rv
-IyTq →.-r2*α
n
-σyTh →. 0.
n
(66)
Next, by substituting (65), (66) in (64), we can rewritten the optimization as,
min 11TP(U)-
u∈Rn n
1yT
n
rv
U + Τ
mRn n 1TP(U) + rv
(67)
r2*+ σ2
2rv
—
2
Then we can rewrite (67) in terms of Moreau envelope,
2
mRn 11T P(U) + rv
=1 Mρ(∙)(αr1*q + σh+ rvy; rv).
(68)
Substituting (67), (68) in (63) we have
min max α∈R; r,τ ≥0; σ,v≥0 θ∈R	nMρ(αrι^q + σh + rvy; rv	) _ r2*+ σ2 _ 2rv	-r2*α	
	22	2 σ α2rj* στr2κ* r - 2Τ - ^2στ	2 — + 2v +1 Mλ*f(στ (rg + θβ*);στ).	+ αθr2*- στ	∙θ2r2* ^2-	(69)
Final Scalarization: Using the strong law of large number (q, h, y, g, β* are entry-wise i.i.d.), We
can rewrite (69) as
18
Under review as a conference paper at ICLR 2022
min
max
α∈R, r,τ≥0;
σ,v≥0
θ∈R
σ
2T
22
α2r2*
2στ
στr2κ* r 八 2
T + 2v + αθr2*-
στθ2r2*	r2* + σ2
2rv
+ E Mρ(αr1*Z1 + σZ2 +---(rι*Zι + σ*Z3);——)
rv	rv
+ E ∣Mλ*∕(στ(rZ + θbo);στ)] • :
where Zι,Z2, Z 〜N(0,1), σ*Z3 〜Pe and bo 〜Π are all independent.
1 - TACCy-'x∕∕∖	I I EI > r	1	Tl ʃ / ∖ )	.
For LASSO, f (x) = |x|. The Moreau envelope Mλ∕(∙; ∙) has property:
Mλ f(στ(rZ + θbo); στ) = στ ∙ Mλ f(rZ + θbo; 1).
Besides, for ρ(x) = 2x2 in LASSO, the Moreau envelope Mρ(∙; ∙) has explict form:
v2
Mρ(v; t) = 2(t+I)
and the second last item of (70) can be simplified to:
E [MP(∙)(αrl*Zl + σZ2 + rv(UZI + σ*Z3); rv)
(70)
(71)
(72)
((a + rV )rι*Zι + σZ2 + σ** Z3)2
2( rv + 1)
r2*(αrv + 1)2 + r2 v2σ2 + σ2
2(1 + rv)rv
(73)
In order to simplify (73), We denote λ = rV in place of v. At this time, minv≥o is replaced by
maxλ≥0 and
r2*(αrv + 1)2 + r2v2σ2 + σ2	(α + λ)2r2* + σ2 + σ2λ2
2(1 + rv)rv
2(λ + 1)
(74)
Substituting (74) in (70) we have the final optimization for LASSO:
min max
α∈R; r,τ,λ≥0;
σ≥0	θ∈R
σ
2T
22
α2r2*
2στ
στr2κ*	r2λ 八 2
口一 + ɪ + αθr2* -
στθ2r2*	(r2* + σ2)λ
-r2*α
—
—
—
E
—
—
—
+
2
2
—
—
—
2
(α + λ)2r2* + σ2 + σ2λ2
2(λ + 1)
(75)
which is a smooth function with respect to α, σ, r, τ, λ, θ. Let φ denote the objective function of
(75).
Deriving SEs from function φ: The SEs are given by the first order optimality conditions of φ :
1)	For ∂φ = 0:
α	α+λ
- 方 + θ -1 + E = 0.
(76)
2)	For ∂σ = 0:
1
2T
—
—
τ⅛" + r2σαr- r12θ-+TK*E[Mi*f(rZ+θbo;1)] + λ+1 = 0. (77)
19
Under review as a conference paper at ICLR 2022
3)	For *=o：
2	2[(α + λ)r2* + λσ2]	(α + λ)2r2* + σ2 + λ2σ2
% + λ + l	(λ + l)2
0.
4)	For d=0:
r2*α — στκ^E∖bo(ProXλf(rZ + θb0))] = 0.
where we use the definition E[b0] = EnX2 = r 2*
5)	For ∂φ =0:
—στrκ* + rλ + στκ*E[(rZ + θbo — Proxχ f(rZ + θbo; 1))Z] = 0.
(78)
(79)
(80)
1~ι ∙	ττn Γ Γ7r)~∖	ι ττn Γ Γ7i 1 r∖ ι ■	r∙ . ∙	/	∖	. < ɪ <-	ι	ι	♦ ι
Since E[Z2] = 1, E[Zb0] = 0. For any function f (x), the Moreau envelope and proximal
operator of f stratifies
∂…，、
∂XM(x;t)
x — Prox/(x; t)
t
(81)
Then we rewrite the equation (80) as
rλ — στκ*E[Proxχ j(rZ + θb0; 1)Z] = 0.
(82)
Using Stein lemma,
E[Proxλ
■*
∂Prox∖ 7(rZ + θb0; 1)
i(rZ + θb0; 1)Z ] = E[r-λfx---------]
=E[
∂Proxχ j(rZ + θb0; 1)
∂Z
(83)
],
(82) can be rewritten as
λ
στκ*E[
∂Proxχ f(rZ + θb0; 1))
∂x
].
(84)
6) For 篝=0:
σ	σr2κ*
——---------
2τ2	2
r2 a2	r2 σθ2
+ ⅛τ —	+ σκ*E[Mλ*f(rZ + θb0[1)]
0.
(85)
For any function f (x), the Moreau envelope and proximal operator of f stratifies
Mλ*f(x; b) = λ*Mf(x; λ*b) = x2 — [Proulx, "b" , ∀λ*,b> 0,x ∈ R	(86)
Using this property, we can rewrite (85) as
σ	σr2κ*	r 2*α2	r 2*σθ2	πrr(rZ + θb0)2	[Proxλ* f(rZ + θb0;1)]2
彳——1+ 中——厂+σκ*E∣—2------------------------2-------] = 0
(87)
i.e.,
22
2^2 + 21*τ 2	2^^ El(Proxλ* 7(rZ + θb0; 1))2] = 0.	(88)
20
Under review as a conference paper at ICLR 2022
Similarly, the equation (77) derived by ∂φ = 0 can be rewritten as
1
2T
-r2κ* ι r2*α2 r2*τθ2 ι rr
—+2σ7——广+-κ*E[
(rZ + θbo )2 [Proxχ*j(rZ + θbo; 1)]
---------------------------------
i.e.,
2
-]+λ⅛=0
(89)
22
-2- + 2⅛7 - ~TE[(Proxλ*f(rZ + 例。；1))2] + λ+1 = 0.
(90)
—
—
2
2
Hence we get the SEs by summarizing equations (76), (90), (78), (79), (84), (88)
0=—金+θ―1+α+4
στ	λ + 1
22
0 = -2τ + ⅛ - -TE[(Prf*f(rZ + 映；1))2] + ʌn
0 = Tr - r2* - σ2 +
2[(α + λ)r2* + λσ2]	(α + λ)2r2* + σ2 + λ2σ2
------------------------------------------------
λ+1
(λ+1)2
0 = r2*α — στκ*E[bo(ProXλ*f(rZ + θbo; 1))]
λ = στκ*E[ IdPTfZ + θb0;I))]
∂x
22
0=2-2 + ^2σ-2^-----2~ E[(PrOxλ*f(IrZ + θb0; 1))2].
regarding (α, σ, λ, θ, r, -).
2
Since r2 = Eb0 〜∏b0 = κ1*, the SEs (91) Can be rewritten as
(91)
0 = - -α + θ - 1 + α+4
σ-	λ + 1
22
0 = -2- + ≡"-等©(Pro”(rZ + θbo;1))2] + λ⅛
(92a)
(92b)
0 = r2 — r2κ* — σ2 +
2[(α + λ)r2κ* + λσ2] (α + λ)2r2κ* + σ2 + λ2 σ2
------------------------------------------------
λ+1
(λ+1)2
(92c)
0 = r2κ*α — στκ*E[b0(ProXλ*∕(rZ + θbo; 1))]
(92d)
∂P
λ = στκ*E [-
,roXλ f(ιrZ + θbo; 1))
22
二 + r2κ*α2
2τ2	2στ2
∂x
等 E[(P：
(92e)
'roXλ*f(rZ + θbo; 1))2]∙
(92f)
—
]
regarding (α, σ, λ, θ, r, τ). This is equivalent to the SEs (12) except for the notations are slightly
different.
C.1 Equivalence of SEs
We first rewrite r, τ, Z and b0 in (92) to γ2, τ2 and Z2 β2 respectively, the equation (92e) becomes
λ = στ2κ*E[
∂Proxf(γ2Z2 + θβ2; λ*)
∂x
]=στ2κ*E[η0(γ2Z2 + θβ2; λ*)]
(93)
_ ≈,. .
for f(x) = |x|.
Then we simplify the SEs (92). Consider equations (92b) and (92f) and we have
σ2 = λ + 1,
(94)
21
UlIderreVieW as a COnferenCe PaPer at ICLR 2022
SUbStitUt5-g (94)5'(92a) We have
H 1
Λ + 1 Q72
(95)
FOrthe SeCOlld equation Of AMP5-(II-which is
TLU κ*(ql + A*)IEw(Z¾ + 7-Zr >* + Tl)L (96)
it is ObviOUSly equivalent to the equation (92e) if WehaVe ParametertranSfonnatns H ⅛ and
ɔl UA*(* — 1)，A ProPerty Ofm) is USed for the equivalence
Ty(Cra) " 7/(2LVCV 0∙ (97)
USiIlg the Parameter transformationS mentioned above and denote W H η(βl ÷71Zl~A* ÷ ɔl-the
first equation Of AMP(11) CaIl be rewritten as
2
A+FEW— /32 - 2 (98)
ig
a+ 同感— 2s⅛7⅜一) ∙ (99)
SUbStitUtg (92d(92D and 犀) UTWe have
号W2) 口岁2&1 +71zl> + w))
H(W2+zl~ 川))
Q Q
H'lIE(W2(-2 +-2 Z2-*)) (because-£-UCmz))
u±⅛z + i⅜ 一 (USing (92j)) (1。。)
—21E(WZ¾) U —2(m∕¾ +7IZL >* + W) . S
2
”—(m2 ÷ T2Z2~ A*) ∙ ∕⅜)
Q
2 ∖2 ɔ
卜(USmg (9gL
≈ Q72
then (99) Can be rewritten as
产——2 + (一 -Tlp2 2τlpJ
会——Q* + K*+ — 0Q72 + 1.」 (IOI)
USing (95) in (IOI) We have
T2(Q72)2 u+Q2 + κ*s — 1)2M∙ (102)
Besidefor CGMT=he equation (92C) CaIl be WritteIl as
(A+ 1)2T2 — S — l)2κ* — Q2 lu(IO3)
USing (95) in (IO3) We have
(Q72)2T2 u+ S — l)2κ* + Q(IO4)
The equation (IO2) and (IO4) are equivalent，HenCe the equationS (92d(92D and (92C) Of CGMT
CaIl be ShoWntO be a decomposition Ofthe first equation Of AMP in (Il) after SOmeParameter
transformatns, IIl COnCIUSion WeProVe the equience between SES from CGMT and AMP
LaSSO framework，
22
Under review as a conference paper at ICLR 2022
D relaxation phenomenon
D. 1 relaxation phenomenon of M-estimator
Notice that (24) is equivalent to
1 n	1	τ3
min n∑2ρ(vi) s.t.√nI∣τ3h + V - e∣∣2 ≤ √n∣∣g∣∣2.	(105)
3	i=1
On the other hand, by rotation invariance of Gaussian distribution, (22) becomes
1n
min — TP(Vi) s∙t∙ ∣∣v - W + ∣∣w∣∣2Z0∣∣ = 0	(106)
||w||2,v n i=1
where Z0 〜 N(0,1), e :=(印，…，tn)T. Comparing (105) with (106) can verify the relaxation
phenomenon, i.e. the only difference between AO and PO is that the feasible region of PO is a
subset of the feasible region of AO.
D.2 relaxation phenomenon of support vector machine and Logistic
REGRESSION
The relaxation phenomenon of support vector machine and logistic regression can be similarly
shown as we have done in Appendix D.1, so we omit the proof here.
E equivalence of SEs of logistic regression from LOO and CGMT
By doing the following parameter transformations:
α2 = √K7αι, μ = r*σ, λ2 = λι,
SEs of CGMT become:
0 = E[Vl0 (Proxλ1i(^√κ^ɑ1Z + r*σV))]
K (α1)2 = (λι)2E[(l0(Proxλιi(√K7αιZ + r*σV)))2]
=λ e[	In(Proxχ∖i1√K∙ɑ∖Z + r*σV))	]
*"	1	1 + λιlo0(ProXλιi(√κζαιZ + r*σV)) .
(107)
What we want to prove is that:
E[(l0(proxλιi (√κζαιZ + r*σV )))2] = E[2ρ0(Qι)(ρ0(proXλιρ(Q2)))2]
1 - λ e[	l00(ProxλιM√κ7αιZ + r*σV))	] = e[	2P0(Qi)_________]
1 1 + λιl00 (proxλιi(√K;αιZ + r*σV )),	1 + λι ρ00(proxλiρ(Q2))
E∖Vl (pτoxλιi(√κ;αιZ + r*σV))] = cE[ρ0(Q1)Q1ρ0(proxλiρ (Q2))]
where c is a constant.
First, we verify the following identity:
E[(l0(proxλι i(√κζα1Z + r*σV )))2] = E[2ρ0(Q1)(ρ0(proxλiρ(Q2)))2].	(108)
Note that
l(t) = P(-t)
l0(t) = -P0(-t)
l00(t) = P(t)
P roxλ1i(z) = -P roxλ1ρ(-z)
(109)
23
Under review as a conference paper at ICLR 2022
and the probability density function (pdf) of V = GY is
PV(v)
1 C-v2	2
√2∏e	2 1 + e-r*v
we have:
E[(l0(Proxλιi(√κ^o^ιZ + r*σV )))2]=
JJ (l(Proxλιi (√κζαιh + r*σv)))2Pz (h)Pv (v)dhdv
((,,,	,I_	、、、。1	h2+v2 ,,、
2 2 2(ρ0(Proxλiρ(-√Kζα∖h 一 r*σv))) 2∏e	2 ρ (r*v)dhdv
(110)
where PZ (h) :
1 - h2 ∙ .) y c r …	…
√2∏e 2 IsthepdfofZ.Meanwhile,
E[2ρ0(Q1 )(ρ0(proxλ1ρ(Q2)))2]
2ρ0(q1)(ρ0(proxλ1ρ(q2)))2PQ1,Q2(q1, q2)dq1dq2.
(111)
Now We introduce the following parameter transformations: qι = r*v, q2 = 一√K^a∖h 一 r*σv.
Then (111) becomes
JJ √K7αιr* * 2ρ0(r^v)(ρ0(proxλiρ(-√Kζaιh 一 r*σv))f PqiQ (r*v, -√Kαιh 一 r*σv)dhdv.
In order to verify (108), we only need to prove that:
1
2∏e
h2+v2
2
√Kζαιr*PQι,Q2 (r*v, -√κζαιh 一 r*σv).
Construct Q1, Q2 as follows: assume Z0, V0 i.i^. N(0,1) and
(Qn = (	0 r*∖(Z 0A
(Q2J	1-√K7α1 -八σ) <V0J
We can easily verify that:
E[(Q01,Q02)T] = (0, 0)T
cov[(Q1,Q2)T]= ( r2σr	V2*+r; (T2)
∖一∕*σ∕*	7κ*α] + ∕*σ )
which means (Q01 , Q02) has identical distribution of (Q1 , Q2).
On the other hand, since
PQ01,Q02(q10, q20)dq10 q20 = PZ0,V 0(h0, v0)dh0dv0
dq1 q2 __ L
而而=r* g'1
q1 = r√v0,
q2 = —√Kζα1h/ 一 r*σv0,
we have:
1
2∏e
h02 + v02
2
√K7αιr*PQι,Q2 (r*v0, 一√K7α1h0 — r*σv0)
which completes our proof of (108).
Secondly, we prove:
1 一 λ e[	l00(Proxλιi(行αιZ + r*σV)	] = e[
1	1 + λιl00(proxλιi(√K^αιZ + r*σV))
2ρ0(Q1)
1 + λιρ00(proxλiρ(Q2y)'
(112)
24
Under review as a conference paper at ICLR 2022
Left hand side (LHS) of (112) is
1
1
E[
]
1 + λι l00(proxλιi(√κζαιZ + r*σV))
λιl00 (proxλιi(√κζαιh + r*σv))
1
PZ (h)PV (v)dhdv
1	h2+v2
λιl00(proxλιi(√κ^αιh + r*σv)) 2π
1
2ρ0(r*v)dhdv
-----:------：--——  -------------e
λιρ00(proxλιρ(-√κ^ɑιh - r*σv)) 2π
h2 + v2 C 0/	…T
2 2ρ (r*v)dhdv.
(113)
e 2
1

Through parameter transformations qι = r*v, q2 = -√κ^aιh - r*σv, RHS of (112) becomes
JJ 1 + λ2ρ2ρX1ρ(q2)) PQ1,Q2 (q1 ,q2)dqidq2
JJ 1 + λιρ00(proxλ2^√v^αιh - r.σv))?PQQATrT-FQah - r*σv)r*√κ∑α1 dhdv.
Combining with
1
2∏e
h2+v
PQ1,Q2 (r*v, -√κζαιh - r*σv)τr√K7αι

2
2
completes the proof of (112).
The proof of
E,[yil0 (^proxλ1γιj(√κTα^ιZ + r*σV))] = cE[ρ0(Q1)Q1ρ0(proxλ1ρ(Q2))]
can be derived similarly. So we omit the proof here.
25