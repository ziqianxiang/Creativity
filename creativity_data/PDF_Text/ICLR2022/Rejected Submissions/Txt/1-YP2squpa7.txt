Under review as a conference paper at ICLR 2022
Deep learning via message passing algorithms
BASED ON BELIEF PROPAGATION
Anonymous authors
Paper under double-blind review
Ab stract
Message-passing algorithms based on the Belief Propagation (BP) equations con-
stitute a well-known distributed computational scheme. They yield exact marginals
on tree-like graphical models and have also proven to be effective in many problems
defined on loopy graphs, from inference to optimization, from signal processing
to clustering. The BP-based schemes are fundamentally different from stochastic
gradient descent (SGD), on which the current success of deep networks is based.
In this paper, we present and adapt to mini-batch training on GPUs a family of
BP-based message-passing algorithms with a reinforcement term that biases dis-
tributions towards locally entropic solutions. These algorithms are capable of
training multi-layer neural networks with performance comparable to SGD heuris-
tics in a diverse set of experiments on natural datasets including multi-class image
classification and continual learning, while being capable of yielding improved
performances on sparse networks. Furthermore, they allow to make approximate
Bayesian predictions that have higher accuracy than point-wise ones.
1	Introduction
Belief Propagation is a method for computing marginals and entropies in probabilistic inference
problems (Bethe, 1935; Peierls, 1936; Gallager, 1962; Pearl, 1982). These include optimization
problems as well once they are written as zero temperature limit of a Gibbs distribution that uses the
cost function as energy. Learning is one particular case, in which one wants to minimize a cost which
is a data dependent loss function. These problems are generally intractable and message-passing
techniques have been particularly successful at providing principled approximations through efficient
distributed computations.
A particularly compact representation of inference/optimization problems that is used to build
massage-passing algorithms is provided by factor graphs. A factor graph is a bipartite graph composed
of variables nodes and factor nodes expressing the interactions among variables. Belief Propagation
is exact for tree-like factor graphs (Yedidia et al., 2003)), where the Gibbs distribution is naturally
factorized, whereas it is approximate for graphs with loops. Still, loopy BP is routinely used with
success in many real world applications ranging from error correcting codes, vision, clustering, just to
mention a few. In all these problems, loops are indeed present in the factor graph and yet the variables
are weakly correlated at long range and BP gives good results. A field in which BP has a long history
is the statistical physics of disordered systems where it is known as Cavity Method (Mezard et al.,
1987). It has been used to study the typical properties of spin glass models which represent binary
variables interacting through random interactions over a given graph. It is very well known that in
spin glass models defined on complete graphs and in locally tree-like random graphs, which are
both loopy, the weak correlation conditions between variables may hold and BP give asymptotic
exact results (Mezard & Montanari, 2009). Here we will mostly focus on neural networks ±1 binary
weights and sign activation functions, for which the messages and the marginals can be described
simply by the difference between the probabilities associated with the +1 and -1 states, the so called
magnetizations. The effectiveness of BP for deep learning has never been numerically tested in a
systematic way, however there is clear evidence that the weak correlation decay condition does not
hold and thus BP convergence and approximation quality is unpredictable.
In this paper we explore the effectiveness of a variant of BP that has shown excellent convergence
properties in hard optimization problems and in non-convex shallow networks. It goes under the
1
Under review as a conference paper at ICLR 2022
name of focusing BP (fBP) and is based on a probability distribution, a likelihood, that focuses on
highly entropic wide minima, neglecting the contribution to marginals from narrow minima even
when they are the majority (and hence dominate the Gibbs distribution). This version of BP is thus
expected to give good results only in models that have such wide entropic minima as part of their
energy landscape. As discussed in (Baldassi et al., 2016a), a simple way to define fBP is to add a
"reinforcement" term to the BP equations: an iteration-dependent local field is introduced for each
variable, with an intensity proportional to its marginal probability computed in the previous iteration
step. This field is gradually increased until the entire system becomes fully biased on a configuration.
The first version of reinforced BP was introduced in (Braunstein & Zecchina, 2006) as a heuristic
algorithm to solve the learning problem in shallow binary networks. Baldassi et al. (2016a) showed
that this version of BP is a limiting case of fBP, i.e., BP equations written for a likelihood that uses
the local entropy function instead of the error (energy) loss function. As discussed in depth in that
study, one way to introduce a likelihood that focuses on highly entropic regions is to create y coupled
replicas of the original system. fBP equations are obtained as BP equations for the replicated system.
It turns out that the fBP equations are identical to the BP equations for the original system with the
only addition of a self-reinforcing term in the message passing scheme. The fBP algorithm can be
used as a solver by gradually increasing the effect of the reinforcement: one can control the size of
the regions over which the fBP equations estimate the marginals by tuning the parameters that appear
in the expression of the reinforcement, until the high entropy regions reduce to a single configuration.
Interestingly, by keeping the size of the high entropy region fixed, the fBP fixed point allows one to
estimate the marginals and entropy relative to the region.
In this work, we present and adapt to GPU computation a family of fBP inspired message passing
algorithms that are capable of training multi-layer neural networks with generalization performance
and computational speed comparable to SGD. This is the first work that shows that learning by
message passing in deep neural networks 1) is possible and 2) is a viable alternative to SGD. Our
version of fBP adds the reinforcement term at each mini-batch step in what we call the Posterior-
as-Prior (PasP) rule. Furthermore, using the message-passing algorithm not as a solver but as an
estimator of marginals allows us to make locally Bayesian predictions, averaging the predictions
over the approximate posterior. The resulting generalization error is significantly better than those of
the solver, showing that, although approximate, the marginals of the weights estimated by message-
passing retain useful information. Consistently with the assumptions underlying fBP, we find that
the solutions provided by the message passing algorithms belong to flat entropic regions of the loss
landscape and have good performance in continual learning tasks and on sparse networks as well.
We also remark that our PasP update scheme is of independent interest and can be combined with
different posterior approximation techniques.
The paper is structured as follows: in Sec. 2 we give a brief review of some related works. In Sec. 3
we provide a detailed description of the message-passing equations and of the high level structure
of the algorithms. In Sec. 4 we compare the performance of the message passing algorithms versus
SGD based approaches in different learning settings.
2	Related Works
The literature on message passing algorithms is extensive, We refer to Mezard & Montanari (2009)
and Zdeborovg & Krzakala (2016) for a general overview. More related to our work, multilayer
message-passing algorithms have been developed in inference contexts (Manoel et al., 2017; Fletcher
et al., 2018), where they have been shown to produce exact marginals under certain statistical
assumptions on (unlearned) weight matrices.
The properties of message-passing for learning shallow neural networks have been extensively studied
(see Baldassi et al. (2020) and reference therein). Barbier et al. (2019) rigorously show that message
passing algorithms in generalized linear models perform asymptotically exact inference under some
statistical assumptions. Dictionary learning and matrix factorization are harder problems closely
related to deep network learning problems, in particular to the modelling of a single intermediate
layer. They have been approached using message passing in Kabashima et al. (2016) and Parker
et al. (2014), although the resulting predictions are found to be asymptotically inexact (Maillard
et al., 2021). The same problem is faced by the message passing algorithm recently proposed for a
multi-layer matrix factorization scenario (Zou et al., 2021). Unfortunately, our framework as well
2
Under review as a conference paper at ICLR 2022
doesn’t yield asymptotic exact predictions. Nonetheless, it gives a message passing heuristic that for
the first time is able to train deep neural networks on natural datasets, therefore sets a reference for
the algorithmic applications of this research line.
A few papers advocate the success of SGD to the geometrical structure (smoothness and flatness) of
the loss landscape in neural networks (Baldassi et al., 2015; Chaudhari et al., 2017; Garipov et al.,
2018; Li et al., 2018; Pittorino et al., 2021; Feng & Tu, 2021). These considerations do not depend on
the particular form of the SGD dynamics and should extend also to other types of algorithms, although
SGD is by far the most popular choice among NNs practitioners due to its simplicity, flexibility,
speed, and generalization performance.
While our work focuses on message passing schemes, some of the ideas presented here, such as
the PasP rule, can be combined with algorithms for Bayesian neural networks' training (Herngndez-
Lobato & Adams, 2015; Wu et al., 2018). Recent work extends BP by combining it with graph
neural networks (Kuck et al., 2020; Satorras & Welling, 2021). Finally, some work in computational
neuroscience shows similarities to our approach (Rao, 2007).
3	Learning by message passing
3.1	Posterior-as-Prior updates
We consider a multi-layer perceptron with L hidden neuron layers, having weight and bias parameters
W = { W', b'}L=0. We allow for stochastic activations P'(x'+1 |z'), where z' is the neuron,s pre-
activation vector for layer ', and P' is assumed to be factorized over the neurons. If no stochasticity
is present, P' just encodes an element-wise activation function. The probability of output y given an
input x is then given by
P(y|x,W)=
dx1:L
L
Y P'+1 (x'+1 | W'x' + b'),
'=o
(1)
where for convenience we defined x0 = x and xL+1 = y. In a Bayesian framework, given a training
set D = {(xn, yn)}n and a prior distribution over the weights qθ(W) in some parametric family, the
posterior distribution is given by
P(W| D,θ) Y YP(yn | Xn, W) qθ(W).	(2)
n
Here Y denotes equality up to a normalization factor. Using the posterior one can com-
pute the Bayesian prediction for a new data-point x through the average P(y | x, D, θ) =
dW P(y | x, W) P(W | D, θ). Unfortunately, the posterior is generically intractable due to the
hard-to-compute normalization factor. On the other hand, we are mainly interested in training a
distribution that covers wide minima of the loss landscape that generalize well (Baldassi et al., 2016a)
and in recovering pointwise estimators within these regions. The Bayesian modeling becomes an
auxiliary tool to set the stage for the message passing algorithms seeking flat minima. We also need
a formalism that allows for mini-batch training to speed-up the computation and deal with large
datasets. Therefore, we devise an update scheme that we call Posterior-as-Prior (PasP), where we
evolve the parameters θt ofa distribution qθt (W) computed as an approximate mini-batch posterior,
in such a way that the outcome of the previous iteration becomes the prior in the following step. In
the PasP scheme, θt retains the memory of past observations. We also add an exponential factor ρ,
that we typically set close to 1, tuning the forgetting rate and playing a role similar to the learning
rate in SGD. Given a mini-batch (Xt, yt) sampled from the training set at time t and a scalar ρ > 0,
the PasP update reads
qθt+ι (W) ≈ [P(W| yt, Xt,θt)]ρ,	⑶
where ≈ denotes approximate equality and we do not account for the normalization factor. A first
approximation may be needed in the computation of the posterior, a second to project the approximate
posterior onto the distribution manifold spanned by θ (Minka, 2001). In practice, we will consider
factorized approximate posterior in an exponential family and priors qθ in the same family, although
Eq. 3 generically allow for more refined approximations.
3
Under review as a conference paper at ICLR 2022
Notice that setting ρ = 1, the batch-size to 1, and taking a single pass over the dataset, we recover
the Assumed Density Filtering algorithm (Minka, 2001). For large enough ρ (including ρ = 1), the
iterations of qθt will concentrate on a pointwise estimator. This mechanism mimics the reinforce-
ment heuristic commonly used to turn Belief Propagation into a solver for constrained satisfaction
problems (Braunstein & Zecchina, 2006) and related to flat-minima discovery (see focusing-BP in
Baldassi et al. (2016a)). A different prior updating mechanism which can be understood as empirical
Bayes has been used in Baldassi et al. (2016b).
3.2	Inner message passing loop
While the PasP rule takes care of the reinforcement heuristic across mini-batches, we compute the
mini-batch posterior in Eq. 3 using message passing approaches derived from Belief Propagation.
BP is an iterative scheme for computing marginals and entropies of statistical models Mezard &
Montanari (2009). It is most conveniently expressed on factor graphs, that is bipartite graphs where
the two sets of nodes are called variable nodes and factor nodes. They respectively represent the
variables involved in the statistical model and their interactions. Message from factor nodes to
variable nodes and viceversa are exchanged along the edges of the factor graph for a certain number
of BP iterations or until a fixed point is reached.
The factor graph for P(W | Xt, yt, θt) can be derived from Eq. 2, with the following additional
specifications. For simplicity, we will ignore the bias term in each layer. We assume factorized
qθt (W), each factor parameterized by its first two moments. In what follows, we drop the PasP
iteration index t. For each example (xn , yn) in the mini-batch, we introduce the auxiliary variables
Xn,' = 1,...,L, representing the layers, activations. For each example, each neuron in the network
contributes a factor node to the factor graph. The scalar components of the weight matrices and
the activation vectors become variable nodes. This construction is presented in Appendix A, where
we also derive the message update rules on the factor graph. The factor graph thus defined is
extremely loopy and straightforward iteration of BP has convergence issues. Moreover, in presence
of a homogeneous prior over the weights, the neuron permutation symmetry in each hidden layer
induces a strongly attractive symmetric fixed point that hinders learning. We work around these
issues by breaking the symmetry at time t = 0 with an inhomogeneous prior. In our experiments
a little initial heterogeneity is sufficient to obtain specialized neurons at each following time step.
Additionally, we do not require message passing convergence in the inner loop (see Algorithm 1) but
perform one or a few iterations for each θ update. We also include an inertia term commonly called
damping factor in the message updates (see B.2). As we shall discuss, these simple rules suffice to
train deep networks by message passing.
For the inner loop we adapt to deep neural networks four different message passing algorithms, all of
which are well known to the literature although derived in simpler settings: Belief Propagation (BP),
BP-Inspired (BPI) message passing, mean-field (MF), and approximate message passing (AMP). The
last three algorithms can be considered approximations of the first one. In the following paragraphs
we will discuss their common traits, present the BP updates as an example, and refer to Appendix A
for an in-depth exposition. For all algorithms, message updates can be divided in a forward pass
and backward pass, as also done in (Fletcher et al., 2018) in a multi-layer inference setting. The BP
algorithm is compactly reported in Algorithm 1.
Meaning of messages. All the messages involved in the message passing can be understood in
terms of cavity marginals or full marginals (as mentioned in the introduction BP is also known as
Cavity Method, see (Mezard & Montanari, 2009)). Of particular relevance are mk and σ2, denoting
the mean and variance of the weights Wa. The quantities χfn and ∆'n instead denote the mean and
variance of the i-th neuron’s activation in layer ` for a given input xn.
Scalar free energies. All message passing schemes are conveniently expressed in terms of two
functions that correspond to the effective free energy (Zdeborovd & Krzakala, 2016) of a single
4
Under review as a conference paper at ICLR 2022
neuron and of a single weight respectively :
d(B,A,ω,V)=log / dx dz e-1 Ax2+Bx P' (x|z) e-(ω-z)2
ψ(H,G,θ) = log / dw e-2 G^+Hw q© (W)
` = 1, . . . , L (4)
(5)
Notice that for common deterministic activations such as ReLU and sign, the function φ has analytic
and smooth expressions (see Appendix A.8). The same holds for the function ψ when qθ (w) is
Gaussian (continuous weights) or a mixture of atoms (discrete weights). At the last layer we impose
P L+1(y|z) = I(y = sign(z)) in binary classification tasks and P L+1(y|z) = I(y = arg max(z))
in multi-class classification (see Appendix A.9). While in our experiments we use hard constraints
for the final output, therefore solving a constraint satisfaction problem, it would be interesting to also
consider soft constraints and introduce a temperature, but this is beyond the scope of our work.
Start and end of message passing. At the beginning of a new PasP iteration t, we reset the
messages (see Appendix A) and run message passing for τmax iterations. We then compute the new
prior’s parameters θt+1 from the posterior given by the message passing.
BP Forward pass. After initialization of the messages at time τ = 0, for each following time we
propagate a set of message from the first to the last layer and then another set from the last to the first.
For an intermediate layer ` the forward pass reads
x'nτ→k	=	dB Ψe (B'n→fc,A'n -1,ω'-1,τ ,Vin1τ}	⑹
△黎=∂B "Birι,A'nττ,ω'-1 ;VnTI	⑺
mkiτ→n	=	∂H ψ(H'i→nl,Gkiττ,θki)	⑻
σkiτ	=	∂H ψ(H'iτ-1,Gkiτ-1,θki)	⑼
Ve	=	X ((mi→) △第+第(琛→k)2+用△黑)(IO)
ωkn→i	=	X mkτ→nx,n→k	(II)
i06=i
The equations for the first layer differ slightly and in an intuitive way from the ones above (see
Appendix A.3).
BP Backward pass. The backward pass updates a set of messages from the last to the first layer:
gkn→i	=	=∂ω 产1 (B'+	-1,τ a2+1,t ',τ	V ',τ∖ , kn , ωkn→i, Vkn		(12)
「',T Γkn	=-∂ω ,+ι (b	'+1,τ a'+1,t ',τ V', kn , kn , ωkn , Vkn	T)	(13)
a',t	- Ain	=X ((mk,i→n k	)2 + σ'iτ) Γ,n -σkiτ	(g'n→i)	(14)
b',T	- Bin→k	=X mki-ng k0 6=k	原→i		(15)
g',t	_ Gki	=X ((χ'nτ→k) n	|2 + Nnr) rkn- △in	(g',n→i)	(16)
Hfei→n	=	=X x'nτ→kg n0 6=n	',T kn0 →i		(17)
As with the forward pass, we add the caveat that for the last layer the equations are slightly different
from the ones above.
5
Under review as a conference paper at ICLR 2022
Computational complexity The message passing equations boil down to element-wise operations
and tensor contractions that we easily implement using the GPU friendly julia library Tullio.jl (Abbott
et al., 2021). For a layer of input and output size N and considering a batch-size of B, the time
complexity of a forth-and-back iteration is O(N 2B) for all message passing algorithms (BP, BPI, MF,
and AMP), the same as SGD. The prefactor varies and it is generally larger than SGD (see Appendix
B.9). Also, time complexity for message passing is proportional to τmax (which we typically set to
1). We provide our implementation in the GitHub repo anonymous.
1
2
3
4
5
6
7
8
9
10
Algorithm 1: BP for deep neural networks
// Message passing used in the PasP Eq. 3 to approximate.
// the mini-batch posterior.
// Here we specifically refer to BP updates.
// BPI, MF, and AMP updates take the same form but using
// the rules in Appendix A.4, A.5, and A.7 respectively
Initialize messages.
for τ = 1, . . . τmax do
// Forward Pass
for l = 0, . . . , L do
compute X', ∆' using (6, 7)
compute m', σ' using (8, 9)
compute V', ω' using (10, 11)
// Backward Pass
for l = L, . . . , 0 do
compute g', Γ' using (12, 13)
compute A', B' using (14, 15)
compute G', H' using (16, 17)
4	Numerical results
We implement our message passing algorithms on neural networks with continuous and binary
weights and with binary activations. In our experiments we fix τmax = 1. We typically do not observe
an increase in performance taking more steps, except for some specific cases and in particular for MF
layers. We remark that for τmax = 1 the BP and the BPI equations are identical, so in most of the
subsequent numerical results we will only investigate BP.
We compare our algorithms with a SGD-based algorithm adapted to binary architectures (Hubara
et al., 2016) which we call BinaryNet along the paper (see Appendix B.6 for details). Comparison
of Bayesian predictions are with the gradient-based Expectation Backpropagation (EBP) algorithm
(Soudry et al., 2014a), also able to deal with discrete weights and activations. In all architectures we
avoid the use of bias terms and batch-normalization layers.
We find that message-passing algorithms are able to train generic MLP architectures with varying num-
bers and sizes of hidden layers. As for the datasets, we are able to perform both binary classification
and multi-class classification on standard computer vision datasets such as MNIST, Fashion-MNIST,
and CIFAR-10. Since these datasets consist of 10 classes, for the binary classification task we divide
each dataset in two classes (even vs odd).
We report that message passing algorithms are able to solve these optimization problems with
generalization performance comparable to or better than SGD-based algorithms. Some of the
message passing algorithms (BP and AMP in particular) need fewer epochs to achieve low error than
the ones required by SGD-based algorithms, even if adaptive methods like Adam are considered.
Timings of our GPU implementations of message passing algorithms are competitive with SGD (see
Appendix B.9).
6
Under review as a conference paper at ICLR 2022
4.1	Experiments across architectures
We select a specific task, multi-class classification on Fashion-MNIST, and we compare the message
passing algorithms with BinaryNet for different choices of the architecture (i.e. we vary the number
and the size of the hidden layers). In Fig.1 (Left) we present the learning curves for a MLP with
3 hidden layers with 501 units with binary weights and activations. Similar results hold in our
experiments with 2 or 3 hidden layers of 101, 501 or 1001 units and with batch sizes from 1 to from
1024. The parameters used in our simulations are reported in Appendix B.3. Results on networks
with continuous weights can be found in Fig.2 (Right).
4.2	Sparse layers
Since the BP algorithm has notoriously been successful on sparse graphs, we perform a straight-
forward implementation of pruning at initialization, i.e. we impose a random boolean mask on the
weights that we keep fixed along the training. We call sparsity the fraction of zeroed weights. This
kind of non-adaptive pruning is known to largely hinder learning (Frankle et al., 2021; Sung et al.,
2021). In the right panel of Fig. 1, we report results on sparse binary networks in which we train
a MLP with 2 hidden layers of 101 units on the MNIST dataset. For reference, results on pruning
quantized/binary networks can be found in Refs. (Han et al., 2016; Ardakani et al., 2017; Tung &
Mori, 2018; Diffenderfer & Kailkhura, 2021). Experimenting with sparsity up to 90%, we observe
that BP and MF perform better than BinaryNet. AMP struggles behind BinaryNet instead.
5 0 5 0
(％)」。」」①
epochs
Figure 1: (Left) Training curves of message passing algorithms compared with BinaryNet on the
Fashion-MNIST dataset (multi-class classification) with a binary MLP with 3 hidden layers of 501
units. (Right) Final test accuracy when varying the layer’s sparsity in a binary MLP with 2 hidden
layers of 101 units on the MNIST dataset (multi-class). In both panels the batch-size is 128 and
curves are averaged over 5 realizations of the initial conditions (and sparsity pattern in the right
panel).
Iol5o,5,o
9 8 8 7 7
(*) ›OΛ⅛OOΛ lsəl
65
60 O IO 20	30	40	50	60	70	80	90
sparsity (%)
4.3	Experiments across datasets
We now fix the architecture, a MLP with 2 hidden layers of 501 neurons each with binary weights and
activations. We vary the dataset, i.e. we test the BP-based algorithms on standard computer vision
benchmark datasets such as MNIST, Fashion-MNIST and CIFAR-10, in both the multi-class and
binary classification tasks. In Tab. 1 we report the final test errors obtained by the message passing
algorithms compared to the BinaryNet baseline. See Appendix B.4 for the corresponding training
errors and the parameters used in the simulations. We mention that while the test performance is
mostly comparable, the train error tends to be lower for the message passing algorithms.
7
Under review as a conference paper at ICLR 2022
Dataset	BinaryNet	BP	AMP	MF
MNIST (2 classes)	1.3 ± 0.1 ^^	1.4 ± 0.2	1.4 ± 0.1	1.3 ± 0.2
Fashion-MNIST (2 classes)	2.4 ± 0.1 ^^	2.3 ± 0.1	2.4 ± 0.1	2.3 ± 0.1
CIFAR-10 (2 classes)	30.0 ± 0.3	31.4 ± 0.1	31.1 ± 0.3	31.1 ± 0.4
MNIST	2.2 ± 0.1 ^^	2.6 ± 0.1	2.6 ± 0.1	2.3 ± 0.1
Fashion-MNIST	12.0 ± 0.6	11.8 ± 0.3	11.9 ± 0.2	12.1 ± 0.2
CIFAR-10	59.0 ± 0.7	58.7 ± 0.3	58.5 ± 0.2	60.4 ± 1.1
Table 1: Test error (%) on Fashion-MNIST of various algorithms on a MLP with 2 hidden layers of
501 units, binary weights and activations. All algorithms are trained with batch-size 128 and for 100
epochs. Mean and standard deviations are calculated over 5 random initializations.
4.4	Locally Bayesian error
The message passing framework used as an estimator of the mini-batch posterior marginals allows
us to perform approximate Bayesian prediction, i.e. averaging the pointwise predictions over the
approximate posterior. We observe better generalization error from Bayesian predictions compared
to point-wise ones, showing that the marginals retain useful information. However, we roughly
estimate the marginals with the PasP mini-batch procedure (the exact ones should be computed with a
full-batch procedure, but this converges with difficulty in our tests). Since BP-based algorithms tend
to focus on dense states (as also confirmed by the local energy measure performed in Appendix B.5),
the Bayesian error we compute can be considered as a local approximation of the full one. We report
results for binary classification on the MNIST dataset in Fig. 2, and we observe the same performance
increase on different datasets and architectures. We obtain the Bayesian prediction from the output
marginal given by a single forward pass of the message passing. To obtain good Bayesian estimates
it is important that the posterior distribution does not concentrate too much, otherwise the Bayesian
prediction will converge to the prediction of a single configuration.
In Fig.2 we also perform a comparison of BP (point-wise and Bayesian) with SGD and another
algorithm able to perform Bayesian predictions, Expectation Backpropagation (Soudry et al., 2014a)
see Appendix B.7 for implementation details.
(*) Jo,uə 1səl
Binary Weights
Bayes BP
——BP
---BinaryNet
40	60 so IoO ..... bayes EBP
epochs
T舸中工工
J'，'VA产V
∙∙∙∙* ■ ∙ *x∙∙v
20	40
Continuous Weights
4 3 2
(*)-0--9 1səl
Figure 2: (Left) Test error curves for Bayesian and point-wise predictions for a MLP with 2 hidden
layers of 101 units on the 2-classes MNIST dataset. We report the results for (Left) binary and
(Right) continuous weights. In both cases, we compare SGD, BP (point-wise and Bayesian) and EBP
(point-wise and Bayesian). See Appendix B.3 for details.
40	60
epochs
80	100
epochs
60	80	100
4.5	Continual learning
Given the high local entropy (i.e. the flatness) of the solutions found by the BP-based algorithms
(see Appendix B.5), we perform additional tests in a classic setting, continual learning, where the
8
Under review as a conference paper at ICLR 2022
possibility of locally rearranging the solutions while keeping low training error can be an advantage.
When a deep network is trained sequentially on different tasks, it tends to forget exponentially fast
previously seen tasks while learning new ones (McCloskey & Cohen, 1989; Robins, 1995; Fusi et al.,
2005). Recent work (Feng & Tu, 2021) has shown that searching for a flat region in the loss landscape
can indeed help to prevent catastrophic forgetting. Several heuristics have been proposed to mitigate
the problem (Kirkpatrick et al., 2017; Aljundi et al., 2018; Zenke et al., 2017; Laborieux et al., 2021)
but all require specialized adjustments to the loss or the dynamics .
Here we show instead that our message passing schemes are naturally prone to learn multiple tasks
sequentially, mitigating the characteristic memory issues of the gradient-based schemes without the
need for explicit modifications. As a prototypical experiment, we sequentially trained a multi-layer
neural network on 6 different versions of the MNIST dataset, where the pixels of the images have
been randomly permuted (Goodfellow et al., 2013), giving a fixed budget of 40 epochs on each task.
We present the results for a two hidden layer neural network with 2001 units on each layer (see
Appendix B.3 for details). As can be seen in Fig. 3, at the end of the training the BP algorithm is able
to reach good generalization performances on all the tasks. We compared the BP performance with
BinaryNet, which already performs better than SGD with continuous weights (see the discussion
in Laborieux et al. (2021)). While our BP implementation is not competitive with ad-hoc techniques
specifically designed for this problem, it beats non-specialized heuristics. Moreover, we believe that
specialized approaches like the one of Laborieux et al. (2021) can be adapted to message passing as
well.
(％) AoE.lnooElss
(％) AoE.lnooElss
1	2	3	4	5	6	0	40	80	120	160	200	240
task #	epochs
Figure 3: Performance of BP and BinaryNet on the permuted MNIST task (see text) for a two hidden
layer network with 2001 units on each layer and binary weights and activations. The model is trained
sequentially on 6 different versions of the MNIST dataset (the tasks), where the pixels have been
permuted. (Left) Test accuracy on each task after the network has been trained on all the tasks.
(Right) Test accuracy on the first task as a function of the number of epochs. Points are averages over
5 independent runs, shaded areas are errors on the mean.
5	Discussion and conclusions
While successful in many fields, message passing algorithms, have notoriously struggled to scale
to deep neural networks training problems. Here we have developed a class of fBP-based message
passing algorithms and used them within an update scheme, Posterior-as-Prior (PasP), that makes it
possible to train deep and wide multilayer perceptrons by message passing.
We performed experiments binary activations and either binary or continuous weights. Future work
should try to include different activations, biases, batch-normalization, and convolutional layers as
well. Another interesting direction is the algorithmic computation of the (local) entropy of the model
from the messages.
Further theoretical work is needed for a more complete understanding of the robustness of our
methods. Recent developments in message passing algorithms (Rangan et al., 2019) and related
theoretical analysis (Goldt et al., 2020) could provide fruitful inspirations. While our algorithms
can be used for approximate Bayesian inference, exact posterior calculation is still out of reach for
message passing approaches and much technical work is needed in that direction.
9
Under review as a conference paper at ICLR 2022
References
Michael Abbott, Dilum Aluthge, N3N5, Simeon Schaub, Carlo Lucibello, Chris Elrod, and Johnny
Chen. Tullio.jl julia package, 2021. URL https://github.com/mcabbott/Tullio.jl.
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference
on Computer Vision (ECCV), pp.139-l54, 2θ18.
Arash Ardakani, Carlo Condo, and Warren J. Gross. Sparsely-connected neural networks: Towards ef-
ficient VLSI implementation of deep neural networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=r1fYuytex.
Carlo Baldassi, Alfredo Braunstein, Nicolas Brunel, and Riccardo Zecchina. Efficient supervised
learning in networks with binary synapses. Proceedings of the National Academy of Sciences,
104(26):11079-11084, 2007. ISSN 0027-8424. doi: 10.1073/pnas.0700324104. URL https:
//www.pnas.org/content/104/26/11079.
Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina.
Subdominant dense clusters allow for simple learning and high computational performance
in neural networks with discrete synapses. Phys. Rev. Lett., 115:128101, Sep 2015.
doi: 10.1103/PhysRevLett.115.128101. URL https://link.aps.org/doi/10.1103/
PhysRevLett.115.128101.
Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca
Saglietti, and Riccardo Zecchina. Unreasonable effectiveness of learning neural networks: From
accessible states and robust ensembles to basic algorithmic schemes. Proceedings of the National
Academy of Sciences, 113(48):E7655-E7662, 2016a. ISSN 0027-8424. doi: 10.1073/pnas.
1608103113. URL https://www.pnas.org/content/113/48/E7655.
Carlo Baldassi, Federica Gerace, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina. Learning
may need only a few bits of synaptic precision. Phys. Rev. E, 93:052313, May 2016b. doi: 10.
1103/PhysRevE.93.052313. URL https://link.aps.org/doi/10.1103/PhysRevE.
93.052313.
Carlo Baldassi, Fabrizio Pittorino, and Riccardo Zecchina. Shaping the learning landscape in neural
networks around wide flat minima. Proceedings of the National Academy of Sciences, 117(1):
161-170, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1908636117. URL https://www.pnas.
org/content/117/1/161.
Jean Barbier, Florent Krzakala, Nicolas Macris, L6o Miolane, and Lenka Zdeborova Optimal errors
and phase transitions in high-dimensional generalized linear models. Proceedings of the National
Academy of Sciences, 116(12):5451-5460, 2019. ISSN 0027-8424. doi: 10.1073/pnas.1802705116.
URL https://www.pnas.org/content/116/12/5451.
Hans Bethe. Statistical theory of superlattices. Proc. R. Soc. A, 150:552, 1935.
Alfredo Braunstein and Riccardo Zecchina. Learning by message passing in networks of discrete
synapses. Phys. Rev. Lett., 96:030201, Jan 2006. doi: 10.1103/PhysRevLett.96.030201. URL
https://link.aps.org/doi/10.1103/PhysRevLett.96.030201.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL
https://openreview.net/forum?id=B1YfAfcgl.
James Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding accurate
binary neural networks by pruning a randomly weighted network. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
U_mat0b9iv.
10
Under review as a conference paper at ICLR 2022
Yu Feng and Yuhai Tu. The inverse variance-flatness relation in stochastic gradient descent is critical
for finding flat minima. Proceedings of the National Academy of Sciences, 118(9), 2021.
Alyson K Fletcher, Sundeep Rangan, and Philip Schniter. Inference in deep networks in high
dimensions. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 1884-1888.
IEEE, 2018.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Pruning neural
networks at initialization: Why are we missing the mark? In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=Ig-VyQc-MLK.
Stefano Fusi, Patrick J Drew, and Larry F Abbott. Cascade models of synaptically stored memories.
Neuron, 45(4):599-611, 2005.
Marylou Gabri6. Mean-field inference methods for neural networks. Journal of PhysicsA: Mathe-
matical and Theoretical, 53(22):223002, 2020.
Robert Gallager. Low-density parity-check codes. IRE Transactions on information theory, 8(1):
21-28, 1962.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.
cc/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neural networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of
Machine Learning Research, pp. 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010.
PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html.
Sebastian Goldt, Marc Mezard, Florent Krzakala, and Lenka Zdeborovd. Modeling the influence of
data structure on learning in neural networks: The hidden manifold model. Physical Review X, 10
(4):041044, 2020.
Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investi-
gation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211,
2013.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In Yoshua Bengio and Yann LeCun (eds.),
4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1510.
00149.
Jose Miguel Herndndez-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable
learning of bayesian neural networks. In Proceedings of the 32nd International Conference on
International Conference on Machine Learning - Volume 37, ICML’15, pp. 1861-1869. JMLR.org,
2015.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Bi-
narized neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Asso-
ciates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
d8330f857a17c53d217014ee776bfd50-Paper.pdf.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to find them. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=SJgIPJBFvH.
Yoshiyuki Kabashima, Florent Krzakala, Marc Mezard, Ayaka Sakata, and Lenka Zdeborovd. Phase
transitions and sample complexity in bayes-optimal matrix factorization. IEEE Transactions on
Information Theory, 62(7):4228-4265, 2016. doi: 10.1109/TIT.2016.2556702.
11
Under review as a conference paper at ICLR 2022
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521-3526, 2017.
Jonathan Kuck, Shuvam Chakraborty, Hao Tang, Rachel Luo, Jiaming Song, Ashish Sabharwal, and
Stefano Ermon. Belief propagation neural networks. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 667-678. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/07217414eb3fbe24d4e5b6cafb91ca18-Paper.pdf.
Axel Laborieux, Maxence Ernoult, Tifenn Hirtzlin, and Damien Querlioz. Synaptic metaplas-
ticity in binarized neural networks. Nature Communications, 12(1):2549, May 2021. ISSN
2041-1723. doi: 10.1038/s41467-021-22768-y. URL https://doi.org/10.1038/
s41467-021-22768-y.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a41b3bb3e6b050b6c9067c67f663b915- Paper.pdf.
Antoine Maillard, Florent Krzakala, Marc Mezard, and Lenka Zdeborov直 Perturbative construction
of mean-field equations in extensive-rank matrix factorization and denoising. arXiv preprint
arXiv:2110.08775, 2021.
Andre Manoel, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Multi-layer generalized linear
estimation. In 2017 IEEE International Symposium on Information Theory (ISIT), pp. 2098-2102,
2017. doi: 10.1109/ISIT.2017.8006899.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109-165.
Elsevier, 1989.
Marc Mezard. Mean-field message-passing equations in the hopfield model and its generalizations.
Physical Review E, 95(2):022117, 2017.
Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An
Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing
Company, 1987.
Thomas P. Minka. Expectation propagation for approximate bayesian inference. In Proceedings of
the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI’01, pp. 362-369, San
Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558608001.
Marc Mezard and Andrea Montanari. Information, Physics, and Computation. Oxford University
Press, Inc., USA, 2009. ISBN 019857083X.
Jason T Parker, Philip Schniter, and Volkan Cevher. Bilinear generalized approximate message
passing—part i: Derivation. IEEE Transactions on Signal Processing, 62(22):5839-5853, 2014.
Judea Pearl. Reverend Bayes on inference engines: A distributed hierarchical approach. Cognitive
Systems Laboratory, School of Engineering and Applied Science . . . , 1982.
R. Peierls. On ising’s model of ferromagnetism. Mathematical Proceedings of the Cambridge
Philosophical Society, 32(3):477-481, 1936. doi: 10.1017/S0305004100019174.
Fabrizio Pittorino, Carlo Lucibello, Christoph Feinauer, Gabriele Perugini, Carlo Baldassi, Elizaveta
Demyanenko, and Riccardo Zecchina. Entropic gradient descent algorithms and wide flat minima.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=xjXg0bnoDmS.
Sundeep Rangan, Philip Schniter, and Alyson K Fletcher. Vector approximate message passing. IEEE
Transactions on Information Theory, 65(10):6664-6684, 2019.
12
Under review as a conference paper at ICLR 2022
Rajesh P. N. Rao. Neural models ofBayesian belief propagation., pp. 239-267. Bayesian brain: Prob-
abilistic approaches to neural coding. MIT Press, Cambridge, MA, US, 2007. ISBN 026204238X
(Hardcover); 978-0-262-04238-3 (Hardcover).
Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):
123-146, 1995.
Victor Garcia Satorras and Max Welling. Neural enhanced belief propagation on factor graphs. In
International Conference on Artificial Intelligence and Statistics, pp. 685-693. PMLR, 2021.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-
free training of multilayer neural networks with continuous or discrete weights. In
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, volume 27. Curran Associates,
Inc., 2014a. URL https://proceedings.neurips.cc/paper/2014/file/
076a0c97d09cf1a0ec3e19c7f2529f2b- Paper.pdf.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of
multilayer neural networks with continuous or discrete weights. In NIPS, volume 1, pp. 2, 2014b.
George Stamatescu, Federica Gerace, Carlo Lucibello, Ian Fuss, and Langford B. White. Critical
initialisation in continuous approximations of binary neural networks. 2020. URL https:
//openreview.net/forum?id=rylmoxrFDH.
Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks, 2021.
Frederick Tung and Greg Mori. Clip-q: Deep network compression learning by in-parallel pruning-
quantization. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
7873-7882, 2018. doi: 10.1109/CVPR.2018.00821.
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, Jose Miguel Hernandez-Lobato,
and Alexander L Gaunt. Deterministic variational inference for robust bayesian neural networks.
arXiv preprint arXiv:1810.03958, 2018.
Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Understanding Belief Propagation and Its
Generalizations, pp. 239-269. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2003.
ISBN 1558608117.
Lenka ZdebOrOvg and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms.
Advances in Physics, 65(5):453-552, 2016.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In International Conference on Machine Learning, pp. 3987-3995. PMLR, 2017.
Qiuyun Zou, Haochuan Zhang, and Hongwen Yang. Multi-layer bilinear generalized approximate
message passing. IEEE Transactions on Signal Processing, 69:4529-4543, 2021. doi: 10.1109/
TSP.2021.3100305.
13
Under review as a conference paper at ICLR 2022
Appendices
Contents
A BP-based message passing algorithms	14
A.1	Preliminary considerations ............................................ 14
A.2	Derivation of the BP equations ........................................ 15
A.3	BP equations .......................................................... 18
A.4	BPI equations ......................................................... 19
A.5	MF equations .......................................................... 20
A.6	Derivation of the AMP equations ....................................... 21
A.7	AMP equations ......................................................... 22
A.8	Activation Functions .................................................. 23
A.9	The ArgMax layer ...................................................... 23
B Experimental details	25
B.1	Hyper-parameters of the BP-based scheme ............................... 25
B.2	Damping scheme for the message passing ................................ 25
B.3	Architectures ......................................................... 26
B.4	Varying the dataset ................................................... 27
B.5	Local energy .......................................................... 27
B.6	SGD implementation (BinaryNet) ........................................ 28
B.7	EBP implementation .................................................... 28
B.8	Unit polarization and overlaps ........................................ 29
B.9	Computational performance: varying batch-size ......................... 30
A BP-based message passing algorithms
A. 1 Preliminary considerations
Given a mini-batch B = {(xn, yn)}n, the factor graph defined by Eqs. (1, 2, 18) is explicitly written
as:
P(W, x1:L	∣B,θ) (X	YY U P'+1	(xk+1	X W") U qθ(Wki),	(18)
'=0 k,n	∖	i	k k,i,'
where x0n = xn , xnL+1 = yn . The derivation of the BP equations for this model is straightforward
albeit lengthy and involved. It is obtained following the steps presented in multiple papers, books,
and reviews, see for instance (Mezard & Montanari, 2009; Zdeborovg & Krzakala, 2016; Mezard,
2017), although it has not been attempted before in deep neural networks. It should be noted that a
(common) approximation that we take here with respect to the standard BP scheme, is that messages
are assumed to be Gaussian distributed and therefore parameterized by their mean and variance. This
goes by the name of relaxed belied propagation (rBP), just referred to as BP throughout the paper.
We derive the BP equations in A.2 and present them all together in A.3. From BP, we derive other 3
message passing algorithms useful for the deep network training setting, all of which are well known
to the literature: BP-Inspired (BPI) message passing A.4, mean-field (MF) A.5, and approximate
14
Under review as a conference paper at ICLR 2022
message passing (AMP) A.7. The AMP derivation is the more involved and given in A.6. In all
these cases, message updates can be divided in a forward pass and a backward pass, as also done in
Fletcher et al. (2018) in a multi-layer inference setting. The BP algorithm is compactly reported in
Algorithm 1.
In our notation, ` denotes the layer index, τ the BP iteration index, k an output neuron index, i an
input neuron index, and n a sample index.
We report below, for convenience, some of the considerations also present in the main text.
Meaning of messages. All the messages involved in the message passing equations can be under-
stood in terms of cavity marginals or full marginals (as mentioned in the introduction BP is also
known as the Cavity Method, see Mezard & Montanari (2009)). Of particular relevance are the
quantities mk and σ2, denoting the mean and variance of the weights 卬'钎 The quantities x'n and
∆'n instead denote mean and variance of the i-th neuron,s activation in layer ' in correspondence of
an input xn.
Scalar free energies. All message passing schemes can be expressed using the following scalar
functions, corresponding to single neuron and single weight effective free-energies respectively:
log ∕dx dze-2Ax2+Bx P' (χ | Z) e-(ω-vz)2
ψ(H,G,θ) = log / dwe-1 G2w2+Hw
qθ(w).
(19)
(20)
These free energies will naturally arise in the derivation of the BP equations in Appendix A.2. For
the last layer, the neuron function has to be slightly modified:
尸1(y,ω,V)
(ω-z)2
log dz PL+1 (y | Z) e--k.
(21)
Notice that for common deterministic activations such as ReLU and sign, the function φ has
analytic and smooth expressions that we give in Appendix A.8. Same goes for ψ when qθ (w)
is Gaussian (continuous weights) or a mixture of atoms (discrete weights). At the last layer
we impose PL+1 (y|Z) = I(y = sign(Z)) in binary classification tasks. For multi-class clas-
sification instead, we have to adapt the formalism to vectorial pre-activations z and assume
P L+1(y|z) = I(y = argmax(z)) (see Appendix A.9). While in our experiments we use hard
constraints for the final output, therefore solving a constraint satisfaction problem, it would be inter-
esting to also consider generic loss functions. That would require minimal changes to our formalism,
but this is beyond the scope of our work.
Binary weights. In our experiments we use ±1 weights in each layer. Therefore each marginal can
be parameterized by a single number and our prior/posterior takes the form
qθ (Wki) Y e%Wa	(22)
The effective free energy function Eq. 20 becomes
Ψ(H, G,θQ = log 2 cosh(H + θ'i)	(23)
and the messages G can be dropped from the message passing.
Start and end of message passing. At the beginning of a new PasP iteration t, we reset the
messages to zero and run message passing for τmax iterations. We then compute the new prior
qθt+1 (W) from the posterior given by the message passing iterations.
A.2 Derivation of the BP equations
In order to derive the BP equations, we start with the following portion of the factor graph reported in
Eq. 18 in the main text, describing the contribution of a single data example in the inner loop of the
PasP updates:
15
Under review as a conference paper at ICLR 2022
Y Y P'+1	(Xk+1	X Wkix'n)	where Xnn	= xn, xL+1	= yn.	(24)
'=0 k	∖	i	)
where We recall that the quantity Xkn corresponds to the activation of neuron k in layer ' in Corre-
spondence of the input example n.
Let us start by analyzing the single factor:
P'+1 (xk+1 ∑Wfcix'nJ	(25)
We refer to messages that travel from input to output in the factor graph as upgoing or upwards
messages, while to the ones that travel from output to input as downgoing or backwards messages.
Factor-to-variable-W messages The factor-to-variable-W messages read:
^'+→ki(Wki) X Y Y dν'i0→n(Wkio) Y dνi0n→k (x'on) dν"+1) P'+1 I xk+1
i0 6=i	i0
ΣW'io X'on
i0
(26)
where νψ denotes the messages travelling downwards (from output to input) in the factor graph.
We denote the means and variances of the incoming messages respectively with mki→n, x'n→k and
σki→n, ∆in→k :
mki→n =J dνki→n(Wki) Wki
σki→n = Z dνkki→n(Wkki) ^Wkii - mki→n)
X' Xin→k	= dνi'n→k (Xi'n) Xi'n	
i'n→k	=/ dν'n→k (Xkn)(Xkn	X'	2 - Xin→k
(27)
(28)
(29)
(30)
We now use the central limit theorem to observe that with respect to the incoming messages distri-
butions - assuming independence of these messages - in the large input limit the preactivation is a
Gaussian random variable:
EW"X'on 〜N(ωkn→i,Vkn→i)	⑶)
i0 6=i
where:
ωkn→i = EVE WkiOx'on	= E mkio→n xkn→k
io 6=i	io 6=i
(32)
Vk'n→i = Varν	Wk'ioXi'on
io6=i
〉:(σki0→n ʌi'nfk + (mki0→n) ʌi'nfk + σki0→n
io 6=i
i'on→k2
(33)
Therefore we can rewrite the outgoing messages as:
16
Under review as a conference paper at ICLR 2022
C	(	C	C	〃，T	(z-ωkn→LWkix'n)2	-J …	、
ν'n→i(Wki) Bj dz dνin→k(Xn) dτ(Xk+1)e	2vkn→i	P + 卜k+1 Z)	(34)
We now assume Wkixin to be small compared to the other terms. With a second order Taylor
expansion we obtain:
Vkn→i(Wki) X / dz dνj(xk+1)e- JVk二？ p'+1 (xk+1 Z)
l Z - ωkn→i 八' T齐厂' I (z - ωkn→i) - Vkn→i
+ 万	xin→k Wki +	H
Vkn→i	2Vkn→i
(35)
Introducing now the function:
d(B,A,ω,V) =log / dxdz e-1 Aχ2+Bx P'(χ∣z) e-⅛≠
(36)
and defining:
gkn→i = ∂ω 产1(B'+1,A'+1,ωkn→iMn→i)	(37)
Γkn→i = ~∂l 产1(B'+1,A'+1,ωkn→iMn→i)	(38)
the expansion for the log-message reads:
log νkn→i (Wki) ≈ COnSt 十 xin→k gkn→iWki
1
2
(39)
Factor-to-variable-x messages The derivation of these messages is analogous to the factor-to-
variable-W ones in Eq. 26 just reported. The final result for the log-message is:
lθg νkn→i (Xin) ≈const + mki→n gkn→i xin
—
2 ((σki→n 十 (mki→n) ) ∣kn→i - σki→n (gkn→i)2) (xin)2	(40)
Variable-W-to-output-factor messages The message from variable Wki to the output factor kn
reads:
νki→n(Wki) X P(L (Wki )epf lθg VJi(WQ
≈ P' z(Wki )eH'i-nWa-2 Gki(WQ2	(41)
where we have defined:
Hki→n
Gki→n
): xin→k gkn→i
n0=n
E (0in0→k + (Xin0→k) )「kn，
n0=n
/一
(42)
(43)
Introducing now the effective free energy:
17
Under review as a conference paper at ICLR 2022
ψ(H, G,θ) = log / dW Pθ' (W) eHW-1GW2	(44)
We can express the first two cumulants of the message ν'i→n(W'i) as:
mki→n = ∂hΨ(Hki→n, Gki→n, θki)	(45)
σki→n = ∂HΨ(Hki→n, Gki→n, θki)	(46)
Variable-x-to-input-factor messages We can write the downgoing message as:
VJ(X'n) H ePk log "'n→i(x'n)
≈ eBfnX-2A'nx2	(47)
where:
Bin = X mki→n gkn→i	(48)
n
A'n = X ((σ'i-n + (mki→n)2) rkn→i - σki→n ^3))	(49)
n
Variable-x-to-output-factor messages By defining the following cavity quantities:
B'n→k = B'n→k - mki→n gkn→i
Ain→k = Ain→k - ( (σki→n + (mki→n))「kn→i - σki→n (gkn→i})
and the following non-cavity ones:
ωkn =〉: mki→n xin→k
i
Vkn = X (σ'i→n ∆'n→k + (mki→n) ∆'n→k + σki→n (x'0n→k)
i
we can express the first 2 cumulants of the upgoing messages as:
(50)
(51)
(52)
(53)
X'n→k = ∂b d(BH→k ,ω'-1,VnT)	(54)
∆'n→k = ∂B d(B'n→k,Α'n→k ,ω'-1,VnT)	(55)
Wrapping it up Additional but straightforward considerations are required for the final input and
output layers (` = 0 and ` = L respectively), since they do not receive messages from below and
above respectively. In the end, thanks to independence assumptions and the central limit theorem that
we used throughout the derivations, we arrive to a closed set of equations involving the means and
the variances (or otherwise the corresponding natural parameters) of the messages. Within the same
approximation assumption, we also replace the cavity quantities corresponding to variances with the
non-cavity counterparts. Dividing the update equations in a forward and backward pass, and ordering
them using time indexes in such a way that we have an efficient flow of information, we obtain the
set of BP equations presented in the main text Eqs. (6-17) and in the Appendix Eqs. (60-71).
A.3 BP equations
We report here the end result of the derivation in last section, the complete set of BP equations also
presented in the main text as Eqs. (6-17).
18
Under review as a conference paper at ICLR 2022
Initialization At τ = 0:
BMk=Q
An0 =。
"=0
Gkio = 0
(56)
(57)
(58)
(59)
Forward Pass At each T = 1,..., τmax, for ' = 0,...,L:
燃τ→k = ∂b d(Bi二 k,A 汇 τ,ωi-1 IVt5)	(60)
△黑=∂B d(B'nτ-1,AMT,ω'-11Vt1")	(61)
mki→n = ∂h 以砥1,GkiTT,%)	(62)
用=∂H Ψ(Hkl1,GkiτT,%)	(63)
嘀=XX (" )2 △筝十 L 依 Y 十一)(64)
ωkn→i = E mliτ→n x'n→k	(65)
i0=i
In these equations for simplicity We abused the notation, in fact for the first layer Xn=0,τ is fixed and
given by the input Xn while An=0,T = 0 instead.
Backward Pass For T = 1,..., τmaχ, for ' = L,..., 0 :
Hfei→n = E xinτ'→k 3kn'→i
n0=n
(66)
(67)
(68)
(69)
(70)
(71)
In these equations as well we abused the notation: calling L the number of hidden neuron layers,
when ' = L one should use 夕L+1(y, ω, V) from Eq. 21 instead of 夕L+1(B, A, ω, V).
A.4 BPI EQUATIONS
The BP-Inspired algorithm (BPI) is obtained as an approximation of BP replacing some cavity
quantities with their non-cavity counterparts. What we obtain is a generalization of the single layer
algorithm of Baldassi et al. (2007).
19
Under review as a conference paper at ICLR 2022
Forward pass.
Backward pass.
入',τ Zr '	- Xin	=∂b d(Binτ-1,A 煞 T,ωi-1[vnττ)	(72)
∆',τ 二 in	=∂B，(BinTT,a 筝 τ,ωeτ,vnτ1	(73)
',τ mki	=	=∂h ψ(Hkiττ,Gkr1,%)	(74)
σ'iτ	=	=∂H Ψ(HkiτT,Gkr1∕ki)	(75)
	=E ("S)2 ∆n+蒋(X 筝)2+用△'，)	(76)
',τ	_ ωkn	-	',τ WT 二	2^mki Xin i	(77)
',τ	二 kn→i 一	=%d+1 (Bk+ 1,t, Ak+1,τ, ωZ - mHXi, VknT)	(78)
「',T _ γ kn =	=-∂ω d+1 (b`+1,t ,a'+1,t ,ωknτ	(79)
a',t	— Ain	-	=E((mkτ)2+JkD 玲n-闱(盒)2 k	(80)
BinT	=	k,τ k,τ 二工 mki 9kn→i k	(81)
Gf	= ki	X E((X万)2+△煞)Γkn -△煞(急)2 n	(82)
HkiT	=	=E Xknr 9kn→i	(83)
n
A.5 MF EQUATIONS
The mean-field (MF) equations are obtained as a further simplification of BPL using only non-cavity
quantities. Although the simplification appears minimal at this point, we empirically observe a
non-negligible discrepancy between the two algorithms in terms of generalization performance and
computational time.
Forward pass.
琛 =∂b d(Bf'T,A 筝 τ,ω'-1 :VnTk)	(84)
∆'nτ = ∂B djB'nττ,4T1,d-1 ;VnTj	附
mkτ = ∂h Ψ(HkiτT,GkjT,%)	(86)
淄：=∂H ψ(Hk[T,GkjT,%)	(87)
嘀=X ((/Y湛十索司尸十闱湛)	(88)
ωkn = E mkiτ Xin	(89)
i
20
Under review as a conference paper at ICLR 2022
Backward pass.
g`nT	= p(,T _ γ kn -	=%产1 H --∂ω产1 (B	t1,τ ,A1+1,t ,ωk ,(÷1,τ λ(+1,t 'kn	，ʌkn 八	nT ,v*τ) 病,VknT)	(90) (91)
a(，T — Ain	-	=XE )2 k	+ σki[ Γfen -	∙σ(iτ(急)2	(92)
BnT	;	(,τ 2,t 二 2^mki gkn k			(93)
G(，t	- Gki	=X((X 筝)2 n	+ △黎)晦 -	△(n (gknT )2	(94)
HkiT	=	-X X(,τ√,τ -x ^Xin gkn n			(95)
A.6 Derivation of THE AMP equations
In order to obtain the AMP equations, we approximate cavity quantities with non-cavity ones in the
BP equations Eqs. (60-71) using a first order expansion. We start with the mean activation:
Xn→k=∂B d(B'nτ-1 -喧二n 盒工,4τ-1,ω'-1,T ,vn-ι,τ)
≈∂b d(B'nτT,4T-1,ω'-11ViT1,τ)
—
≈χ(nτ - mkτ-1gtτ T△(，
Analogously, for the weight,s mean we have:
mmki→n = dHMHkrT- xin→k g'n二i , G(IkL ,熊i
≈ ∂h Ψ(H'iτ-1, GkiTTTi)- X'= k gkn→i ∂H Ψ(H'iτ-1, GkiTT, %)
(,τ 入(,τ-1 2,τ-1	2,τ
≈ mki - Xin	9kn σki .
This brings us to:
2,τ	',τ 八',τ
ωkn =7 m mki→n xin→k
i
',τ 入',τ	',τ-IV^ ',τ 入',τ 入',τ-1	',τ-IV^ ',τ ',τ-1 a2,t
≈ 工 mki Xin —	9kn	工	σki	Xin	Xin	-	gkn	工 mk mk	An
ii
+(怠-1)2 X σkiτ miτ-1χ'n-1∆'n
i
(96)
(97)
(98)
i
Let us now apply the same procedure to the other set of cavity messages:
',T =∂ √+1(b(+1,t a'+1，T ω',τ - m(，T X(，T V(，T)
gkn→i = % ψ (Qkn ，Akn ，ωkn mki→n xin→k，v kn )
≈∂ω ,+1(B(n1ιA+1,τ,ωtτ ,Vknτ)
-mkiτ→n X(n→k∂ω d+1(B(n 1,τ, A+1,t, 3 k, VknT)
≈g(nT + m(，iτ X 第 M	(99)
21
Under review as a conference paper at ICLR 2022
B'nτ = E mkiτ→n g''n→i
kk
-(X'nτ )2 X σkiτ mkiτ gknτ 蹴
k
k
n
-mkiT ∑(g'nT )2∆'nτ
n
(100)
(101)
n
HkiT = E x'nτ→k gkn→i
n
≈ X χ'nτ g” + mkiτ X (χ'nτ) rk；
n
We are now able to write down the full AMP equations, that we present in the next section.
A.7 AMP equations
In summary, in the last section we derived the AMP algorithm as a closure of the BP messages passing
over non-cavity quantities, relying on some statistical assumptions on messages and interactions.
With respect to the MF message passing, we find some additional terms that go under the name of
Onsager corrections. In-depth overviews of the AMP (also known as Thouless-Anderson-Palmer
(TAP)) approach can be found in Refs. (Zdeborovd & Krzakala, 2016; M6zard, 2017; Gabria 2020).
The final form of the AMP equations for the multi-layer perceptron is given below.
Initialization At τ = 0:
Bin=o
A'n0=0
Hki = 0 or some values
Gki0 = 0 or some values
gk`,n0 = 0
(102)
(103)
(104)
(105)
(106)
Forward Pass At each τ = 1, . . . , τmax, for ` = 0, . . . , L:
χ'nτ =∂b d(B'nττ,A'nτ-1,ω'-1 ;VnTT)	(107)
∆'nτ =∂B d(B'nττ,A'nττ,ω'-1,τ,vnττ)	(108)
mki =dHψ(H'I-1, Gki -1, θki)	(109)
σ'iτ=∂H Ψ(H'iτ-1,GkiτT,θki)	(110)
VknT=X ((mkiτ)2 Nin+σkiτ 卜',n)2+σkiτδ',	T	(111)
',τ	',T 入',τ	',τ-1	',T 入',τ 入',τ-1	' ωk,n =上 mki Xin - gkn	上 σki Xin Xin	- §k ii	LX mkiτ mkiTT ∆'nτ i
+ (gknτ-1)2 X σkiτ mkiτ-1χ'nτ-1∆'nτ i	(112)
22
Under review as a conference paper at ICLR 2022
Backward Pass
g'nτ =∂ω d+1(B'n1,τ,Ak+1,τ,ωk"i,vknτ)	(⑴)
Γkn =- ∂d+1(Bk+ 1,τ,Ak+1,τ,ωknT,Vkn)	(114)
AM = X (((mk,iτ) + σ'iτ) rk,τ - σkiτ (g';))	(115)
B犷=X mkiτ g'n - Xin X (gk`n) σkiτ + xin XM；尸
kk	k
-(Xin) 2 E σ'iτ mkiτgk; rk,τ	(116)
k
CGk=X(((琛)2 +温)M-…)2)	(117)
HkiT= Xxinτ簿 + mkiTX (XnT)2喘-mil X(g'nτ)2△黎
nn	n
n
(118)
A.8 Activation Functions
A.8. 1 Sign
In most of our experiments we use sign activations in each layer. With this choice, the neuron’s free
energy 19 takes the form
以B,A,ω,V)=log I 1 X eBx H (-√V)∣ +；log(2nV),	(119)
	x∈{-1,+1}
where	H = Ierfc (√2) .	(120)
Notice that for sign activations the messages A can be dropped.
A.8.2 RELU
For ReLU (x) = max(0, x) activations the free energy 19 becomes
ψ(B, A, ω,V) = / dxdz e-1 Ax2+Bx
(ω — z)'
δ(x — max(0, Z)) e- 2V
(121)
N(ω; B∕A,V + ɪ)
+	A N (B;0,A)
BV + ω
√V + AV2
+ 1log(2πV),
(122)
H
—
where
1
N(x; μ, Σ)
—，	e
√2∏∑
(x- μ)
2Σ
(123)

A.9 The ArgMax layer
In order to perform multi-class classification, we have to perform an argmax operation on the last
layer of the neural network. Call zk, for k = 1, . . . , K, the Gaussian random variables output of the
last layer of the network in correspondence of some input x. Assuming the correct label is class k*,
the effective partition function Zi* corresponding to the output constraint reads:
23
Under review as a conference paper at ICLR 2022
0
5 0 5
(％)」。」」①
0
20
40	60
epochs
80	100
Figure 4: MLP with 2 hidden layers with 101 hidden units each, batch-size 128 on the Fashion-
MNIST dataset. In the first two layers we use the BP equations, while in the last layer the ArgMax
ones. (Left) ArgMax layer first version; (Right) ArgMax layer second version. Even if it is possible
to reach similar accuracies with the two versions, we decide to use the first one as it is simpler to use.
5 0 5
(％)」0」」①
0
20	40	60	80	100
epochs
0
Zk* = / Ildzk N(ZkMk, Vk) ɪɪ Θ(zk* - Zk)
J k	k=k*
ZZk* -
dzk* N(zk* ; ωk* , Vk* )	H (-----√v
k6=k*
(124)
(125)
Here Θ(x) is the Heaviside indicator function and we used the definition of H from Eq. 120. The
integral on the last line cannot be expressed analytically, therefore we have to resort to approximations.
A.9.1 Approach 1: Jensen Inequality
Using the Jensen inequality we obtain:
φk* = log Zk*
log Ez〜N(ωk* ,Vk* ) Y H (- Z-|k
≥ E Ez〜N(ωk* ,Vk* ) log H
k6=k*
z 一 ωk
√Vk
(126)
(127)
—
Reparameterizing the expectation we have:
φk* = X EgN(0,1) log H (-ωk* + e√V -ωk
k6=k*	Vk
(128)
The derivative ∂ωk φk* and ∂ω2 φk* that we need can then be estimated by sampling (once) :
∂ωk φk*
-√VkEe~N(0,1) K
ωk* +e√Vk* 一ωk
Σk0 = k* √V= Ee〜N(0,1) K
Vk0
k = k
k = k*
(129)
—
—
where we have defined:
K(X) = J 历.
H(X)	erfcx(x∕2)
(130)
24
Under review as a conference paper at ICLR 2022
A.9.2 Approach 2: Jensen again
A further simplification is obtained by applying Jensen inequality again to 128 but in the opposite
direction, therefore we renounce to having a bound and look only for an approximation. We have the
new effective free energy:
7 φk* =	「 X logEgN(0,1)H (-ωk* + +√V-ωk)	(131) k6=k* 2* log h(-√≡ )	(132) k6=k
This gives, for k = k*:	f--/	1	K ---ωk*-ωk 1	k = k
a I ∂ωk φk*	= I	√Vk+Vk* KI √Vk+n*J	k=	(133) -I P	/	1	K ( - ωk*-ωk0 ) k = k* (	丰 √VkO+Vk*	I √Vk0+Vk* J
Notice that ∂ωχ* φk* = - Ek=k* ∂ωk φk*. In last formulas We used the definition of K m Eq. 130.
We show in Fig. 4 the negligible difference between the two ArgMax versions when using BP on the
layers before the last one (Which performs only the ArgMax).
B Experimental details
B.1	Hyper-parameters of the BP-based scheme
We include here a complete list of the hyper-parameters present in the BP-based algorithms. Notice
that, like in the SGD type of algorithms, many of them can be fixed or it is possible to find a
prescription for their value that Works in most cases. HoWever, We expect future research to find even
more effective values of the hyper-parameters, in the same Way it has been done for SGD. These
hyper-parameters are: the mini-batch size bs; the parameter ρ (that has to be tuned similarly to the
learning rate in SGD); the damping parameter α (that performs a running smoothing on the BP fields
along the dynamics by adding a fraction of the field at the previous iteration, see Eqs. (134, 135)); the
initialization coefficient that We use to to sample the parameters of our prior distribution qθ (W)
according to θf=0 ~ eN(0,1). Different choices of E correspond to different initial distribution of
the Weights’ magnetization m`ki = tanh(θk`i), as is shoWn in Fig. 5); the number of internal steps of
reinforcement τmax and the associated intensity of the internal reinforcement r. The performances
of the BP-based algorithms are robust in a reasonable range of these hyper-parameters. A more
principled choice of a good initialization condition could be made by adapting the technique from
Stamatescu et al. (2020).
Notice that among these parameters, the BP dynamics at each layer is mostly sensitive to ρ and α,
so that in general We consider them layer-dependent. See Sec. B.8 for details on the effect of these
parameters on the learning dynamics and on layer polarization (i.e. hoW the BP dynamics tends to
bias the Weights toWards a single point-Wise configuration With high probability). Unless otherWise
stated We fix some of the hyper-parameters, in particular: bs = 128 (results are consistent With other
values of the batch-size, from bs = 1 up to bs = 1024 in our experiments), E = 1.0, τmax = 1, r = 0.
B.2	Damping scheme for the message passing
We use a damping parameter α ∈ (0, 1) to stabilize the training, changing the updated rule for the
Weights’ means as folloWs
m kτ =dH ψ (HkiTT,Gkiττ,θki)
mkiτ=αmkT1 + (1-α)希第
(134)
(135)
25
Under review as a conference paper at ICLR 2022
Figure 5: Initial distribution of the magnetizations varying the parameter . The initial distribution is
more concentrated around ±1 as increases (i.e. it is more bimodal and the initial configuration is
more polarized).
B.3	Architectures
In the experiments in which we vary the architecture (see Sec. 4.1), all simulations of the BP-based
algorithms use a number of internal reinforcement iterations τmax = 1. Learning is performed on the
totality of the training dataset, the batch-size is bs = 128, the initialization coefficient is = 1.0.
For all architectures and all BP approximations, we use α = 0.8 for each layer, apart for the
501-501-501 MLP in which we use α = (0.1, 0.1, 0.1, 0.9). Concerning the parameter ρ, we use
ρ = 0.9 on the last layer for all architectures and BP approximations. On the other layers we
use: for the 101-101 and the 501-501 MLPs, ρ = 1.0001 for all BP approximations; for the 101-
101-101 MLP, ρ = 1.0 for BP and AMP while ρ = 1.001 for MF; for the 501-501-501 MLP
ρ = 1.0001 for all BP approximations. For the BinaryNet simulations, the learning rate is lr = 10.0
for all MLP architectures, giving the better performance among the learning rates we have tested,
lr= 100, 10, 1, 0.1, 0.001.
We notice that while we need some tuning of the hyper-parameters to reach the performances of
BinaryNet, it is possible to fix them across datasets and architectures (e.g. ρ = 1 and α = 0.8 on
each layer) without in general losing more than 20% (relative) of the generalization performances,
demonstrating that the BP-based algorithms are effective for learning also with minimal hyper-
parameter tuning.
The experiments on the Bayesian error are performed on a MLP with 2 hidden layers of 101 units
on the MNIST dataset (binary classification). Learning is performed on the totality of the training
dataset, the batch-size is bs = 128, the initialization coefficient is = 1.0. In order to find the
pointwise configurations we use α = 0.8 on each layer and ρ = (1.0001, 1.0001, 0.9), while to find
the Bayesian ones we use α = 0.8 on each layer and ρ = (0.9999, 0.9999, 0.9) (these value prevent
an excessive polarization of the network towards a particular pointwise configurations).
For the continual learning task (see Sec. 4.5) we fixed ρ = 1 and α = 0.8 on each layer as we
empirically observed that polarizing the last layer helps mitigating the forgetting while leaving the
single-task performances almost unchanged.
In Fig. 6 we report training curves on architectures different from the ones reported in the main paper.
26
Under review as a conference paper at ICLR 2022
25
怎10
20
0	20	40	60	80	100
epochs
25
20
⑤15
O
Φ 10
5
0
epochs
Figure 6: Training curves of message passing algorithms compared with BinaryNet on the Fashion-
MNIST dataset (multi-class classification) with a binary MLP with 3 hidden layers of 501 units.
(Right) The batch-size is 128 and curves are averaged over 5 realizations of the initial conditions
B.4	Varying the dataset
When varying the dataset (see Sec. 4.3), all simulation of the BP-based algorithms use a number
of internal reinforcement iterations τmax = 1. Learning is performed on the totality of the training
dataset, the batch-size is bs = 128, the initialization coefficient is = 1.0. For all datasets (MNIST
(2 classes), FashionMNIST (2 classes), CIFAR-10 (2 classes), MNIST, FashionMNIST, CIFAR-10)
and all algorithms (BP, AMP, MF) we use ρ = (1.0001, 1.0001, 0.9) and α = 0.8 for each layer.
Using in the first layers values of ρ = 1 + with ≥ 0 and sufficiently small typically leads to good
results.
For the BinaryNet simulations, the learning rate is lr = 10.0 (both for binary classification and
multi-class classification), giving the better performance among the learning rates we have tested,
lr = 100, 10, 1, 0.1, 0.001. In Tab. 2 we report the final train errors obtained on the different datasets.
Dataset	BinaryNet	BP	AMP	MF
MNIST (2 classes)	0.05 ± 0.05	^^0.0 ± 0.0^^	0.0 ± 0.0	0.0 ± 0.0
FashionMNIST (2 classes)	0.3 ± 0.1	0.06 ± 0.01	0.06 ± 0.01	0.09 ± 0.01
CIFAR10 (2 classes)	1.2 ± 0.5	0.37 ± 0.01	0.4 ± 0.1	0.9 ± 0.2
MNIST	0.09 ± 0.01	0.12 ± 0.01	0.12 ± 0.01	0.03 ± 0.01
FashionMNIST	4.0 ± 0.5	^^3.4 ± 0.1 ^^	3.7 ± 0.1	2.5 ± 0.2
CIFAR10	13.0 ± 0.9	4.7 ± 0.1	4.7 ± 0.2	9.2 ± 0.5
Table 2: Train error (%) on Fashion-MNIST of a multilayer perceptron with 2 hidden layers of 501
units each for BinaryNet (baseline), BP, AMP and MF. All algorithms are trained with batch-size 128
and for 100 epochs. Mean and standard deviations are calculated over 5 random initializations.
B.5	Local energy
We adapt the notion of flatness used in (Jiang et al., 2020; Pittorino et al., 2021), that we call local
energy, to configurations with binary weights. Given a weight configuration w ∈ {±1}N, we define
the local energy δEtrain(w, p) as the average difference in training error Etrain(w) when perturbing w
by flipping a random fraction p of its elements:
δEtrain (w, p) = Ez Etrain(w	z) - Etrain(w),	(136)
where denotes the Hadamard (element-wise) product and the expectation is over i.i.d. entries for z
equal to -1 with probability p and to +1 with probability 1 - p. We report the resulting local energy
profiles (in a range [0, pmax]) in Fig. 7 right panel for BP and BinaryNet. The relative error grows
27
Under review as a conference paper at ICLR 2022
slowly when perturbing the trained configurations (notice the convexity of the curves). This shows
that both BP-based and SGD-based algorithms find configurations that lie in relatively flat minima in
the energy landscape. The same qualitative phenomenon holds for different architectures and datasets.
Figure 7: Local energy curve of the point-wise configuration found by the BP algorithm compared
with BinaryNet on a MLP with 2 hidden layers of 101 units on the 2-class MNIST dataset.
B.6	SGD implementation (B inaryNet)
We compare the BP-based algorithms with SGD training for neural networks with binary weights
and activations as introduced in BinaryNet (Hubara et al., 2016). This procedure consists in keeping
a continuous version of the parameters w which is updated with the SGD rule, with the gradient
calculated on the binarized configuration wb = sign(w). At inference time the forward pass is
calculated with the parameters wb . The backward pass with binary activations is performed with the
so called straight-through estimator.
Our implementation presents some differences with respect to the original proposal of the algorithm
in (Hubara et al., 2016), in order to keep the comparison as fair as possible with the BP-based
algorithms, in particular for what concerns the number of parameters. We do not use biases nor batch
normalization layers, therefore in order to keep the pre-activations of each hidden layer normalized
We rescale them by √N where N is the size of the previous layer (or the input size in the case of the
pre-activations afferent to the first hidden layer). The standard SGD update rule is applied (instead
of Adam), and we use the binary cross-entropy loss. Clipping of the continuous configuration w in
[-1, 1] is applied. We use Xavier initialization (Glorot & Bengio, 2010) for the continuous weights.
In Fig.2 of the main paper, we apply the Adam optimization rule, noticing that it performs slightly
better in train and test generalization performance compared to the pure SGD one.
B.7	EBP implementation
Expectation Back Propagation (EBP) Soudry et al. (2014b) is parameter-free Bayesian algorithm
that uses a mean-field (MF) approximation (fully factorized form for the posterior) in an online
environment to estimate the Bayesian posterior distribution after the arrival of a new data point.
The main differences between EBP and our approach relies in the approximation for the posterior
distribution. Moreover we explicitly base the estimation of the marginals on the local high entropy
structure. The fact that EBP works has no clear explanation: certainly it cannot be that the MF
assumption holds for multi-layer neural networks. Still, it’s certainly very interesting that it works.
We argue that it might work precisely by virtue of the existence of high local entropy minima and
28
Under review as a conference paper at ICLR 2022
25
20
冬15
os
`—
O
①10
5
0	20	40	60	80	100
epochs
W 0.010
∏J
m 0.005
epochs
一BP layer3
一AMP layer3
一MF layer3
fBP layer1
AMPIayerI
MFIayerI
100
0
50
epochs
W 0.0050
∏J
H 0.0025
0
50
epochs
(BP layer2
AMP layer2
MF layer2
100
0.4
O
b 0.2
0.04
3
巴 0.02
Cr
—BP layer3
一AMPIayer3
MFIayer3
0	50	100
epochs
0.00
0	50	100
epochs
Figure 8: (Right panels) Polarizations hq0i and overlaps hqabi on each layer of a MLP with 2 hidden
layers of 501 units on the Fashion-MNIST dataset (multi-class), the batch-size is bs = 128. (Right)
Corresponding train and test error curves.
expect it to give similar performance to the MF case of our algorithm. The online iteration could in
fact be seen as way of implementing a reinforcement.
We implemented the EBP code along the lines of the original matlab implementation
(https://github.com/ExpectationBackpropagation/EBP_Matlab_Code). In order to perform a fair
comparison we removed the biases both in the binary and continuous weights versions. It is worth
noticing that we faced numerical issues in training with a moderate to big batchsize All the experi-
ments were consequently limited to a batchsize of 10 patterns
B.8	Unit polarization and overlaps
We define the self-overlap or polarization of a given unit a as q0a = N Pi(Wa)I2, where N is the
number of parameters of the unit and {wia}iN=1 its weights. It quantifies how much the unit is polarized
towards a unique point-wise binary configuration (q0a = 1 corresponding to high confidence in a
given configurations while q0a = 0 to low). The overlap between two units a and b (considered in the
same layer) is qab = N P Wawli. The number of parameters N is the same for units belonging to the
same fully connected layer. We denote by(q0 = NUt PN=I q0 and hq°b> = NUt PN<bt qab the
mean polarization and mean overlap in a given layer (where Nout is the number of units in the layer).
The parameters ρ and α govern the dynamical evolution of the polarization of each layer during
training. A value ρ ' 1 has the effect to progressively increase the units polarization during training,
while ρ < 1 disfavours it. The damping α which takes values in [0, 1] has the effect to slow the
dynamics by a smoothing process (the intensity of which depends on the value of α), generically
favoring convergence. Given the nature of the updates in Algorithm 1, each layer presents its own
dynamics given the values of p` and a' at layer ', that in general can differ from each other.
We find that it is is beneficial to control the polarization layer-per-layer, see Fig. 8 for the correspond-
ing typical behavior of the mean polarization and the mean overlaps during training. Empirically, we
have found that (as we could expect) when training is successful the layers polarize progressively
towards q0 = 1, i.e. towards a precise point-wise solution, while the overlaps between units in each
hidden layer are such that qab 1 (indicating low symmetry between intra-layer units, as expected
for a non-redundant solution). To this aim, in most cases α' can be the same for each layer, while
tuning p` for each layer allows to find better generalization performances in some cases (but is not
strictly necessary for learning).
In particular, it is possible to use the same value p` for each layer before the last one (' < L
where L is the number of layers in the network), while we have found that the last layer tends to
29
Under review as a conference paper at ICLR 2022
polarize immediately during the dynamics (probably due to its proximity to the output constraints).
Empirically, it is usually beneficial for learning that this layer does not or only slightly polarize, i.e.
hq0i 1 (this can be achieved by imposing ρL < 1). Learning is anyway possible even when the
last layer polarizes towards hq0i = 1 along the dynamics, i.e. by choosing ρL sufficiently large.
As a simple general prescription in most experiments we can fix α = 0.8 and ρL = 0.9, therefore
leaving ρ'<L as the only hyper-parameter to be tuned, akin to the learning rate in SGD. Its value has
to be very close to 1.0 (a value smaller than 1.0 tends to depolarize the layers, without focusing on a
particular point-wise binary configuration, while a value greater than 1.0 tends to lead to numerical
instabilities and parameters’ divergence).
B.9	Computational performance: varying batch-size
In order to compare the time performances of the BP-based algorithms with our implementation
of BinaryNet, we report in Fig. 9 the time in seconds taken by a single epoch of each algorithm
in function of the batch-size, on a MLP of 2 layers of 501 units on Fashion-MNIST. We test both
algorithms on a NVIDIA GeForce RTX 2080 Ti GPU. Multi-class and binary classification present
a very similar time scaling with the batch-size, in both cases comparable with BinaryNet. Let us
also notice that BP-based algorithms are able to reach generalization performances comparable to
BinaryNet for all the values of the batch-size reported in this section.
IO2
(S) ipodə -əd ΦILΠ1
IO1
IO0
IO0	IO1	IO2	IO3
batch size
Figure 9: Algorithms time scaling with the batch-size on a MLP with 2 hidden layers of 501 hidden
units each on the Fashion-MNIST dataset (multi-class classification). The reported time (in seconds)
refers to one epoch for each algorithm.
30