Under review as a conference paper at ICLR 2022
Deep Classifiers with Label Noise Modeling
and Distance Awareness
Anonymous authors
Paper under double-blind review
Ab stract
Uncertainty estimation in deep learning has recently emerged as a crucial area of
interest to advance reliability and robustness in safety-critical applications. While
there have been many proposed methods that either focus on distance-aware model
uncertainties for out-of-distribution detection or on input-dependent label uncer-
tainties for in-distribution calibration, both of these types of uncertainty are often
necessary. In this work, we propose the HetSNGP method for jointly modeling
the model and data uncertainty. We show that our proposed model affords a fa-
vorable combination between these two complementary types of uncertainty and
thus outperforms the baseline methods on some challenging out-of-distribution
datasets, including CIFAR-100C, Imagenet-C, and Imagenet-A. Moreover, we
propose HetSNGP Ensemble, an ensembled version of our method which provides
an additional type of uncertainty and also outperforms other ensemble baselines.
1	Introduction
While deep learning has led to impressive advances in predictive accuracy, models often still suf-
fer from overconfidence and ill-calibrated uncertainties (Ovadia et al., 2019). This is particularly
problematic in safety-critical applications (e.g., healthcare, autonomous driving), where uncertainty
estimation is crucial to ensure reliability and robustness (Filos et al., 2019; Dusenberry et al., 2020).
Predictive uncertainties generally come in two flavors: model uncertainty (also known as epistemic)
and data uncertainty (also known as aleatoric) (Murphy, 2012). Model uncertainty measures how
confident the model should be based on what it knows about the world, that is, how much it can know
about certain test data given the training data it has seen. Data uncertainty measures the uncertainty
intrinsic in the data itself, for example due to fundamental noise in the labelling process. Good
model uncertainty estimates allow for out-of-distribution (OOD) detection, that is, for recognizing
data examples that are substantially different from the training data. On the other hand, good data
uncertainty estimates allow for in-distribution calibration, that is, knowing which training (or testing)
data examples the model should be more or less confident about.
Many recently proposed models for uncertainty estimation excel at one or the other of these uncer-
tainty types. For instance, the spectral-normalized Gaussian process (SNGP) (Liu et al., 2020) uses
a latent Gaussian process to achieve distance-aware model uncertainties and thus affords excellent
OOD detection. Conversely, the heteroscedastic classification method (Collier et al., 2020; 2021)
offers excellent calibration and improved accuracy thanks to modeling input- and class-dependent
label noise in the training data. However, there have not been many attempts to combine the comple-
mentary benefits of these two types of uncertainty modeling (e.g., Depeweg et al., 2018) (see related
work in Appendix A.1).
In this work, we propose the heteroscedastic SNGP (HetSNGP) model, which allows for joint mod-
eling of model and data uncertainties based on a hierarchy of two latent random variables. We show
that HetSNGP gives good in-distribution and OOD accuracy and calibration, yielding a model with
uncertainties suitable for deployment in critical applications.
Our main contributions are:
•	We propose a new model, the heteroscedastic spectral-normalized Gaussian process (Het-
SNGP), which provides both distance-aware model and data uncertainties.
1
Under review as a conference paper at ICLR 2022
-M
*∙
*∙
-a
*∙
■*
-i-1»
M
»*
»*
,
<
>
,
<
>
(b) Heteroscedastic
(a) Deterministic
卜'
1“
(g) Heteroscedastic
(f) Deterministic
,∙
1
Figure 1: Estimated uncertainties on synthetic datasets. The red cluster of points are unseen OOD
points, while the other colors represent different classes of training data. The background color
shows the normalized predictive uncertainty for the different methods. The deterministic and het-
eroscedastic methods are overconfident on the OOD data, while the other methods offer distance-
aware uncertainties. The uncertainties of the Posterior Network do not grow quickly enough away
from the data, such that only the SNGP and proposed HetSNGP provide effective OOD detection.
(c) SNGP
M
M
〉，
(h) SNGP
>.
(d) Posterior Net
»*
»*
-M
(i) Posterior Net
(e) HetSNGP
M
*∙
*∙
-W
*∙
(j) HetSNGP





•	We describe an efficient approximate inference scheme that allows training HetSNGP with
a computational budget that is comparable to standard neural network training.
•	We show empirically on different benchmark datasets that HetSNGP offers a favorable
combination of model and data uncertainty. It generally preserves the SNGP’s OOD per-
formance and the heteroscedastic in-distribution performance and even outperforms these
baselines on some datasets, where OOD and heteroscedastic uncertainties are helpful.
•	We propose an ensembled version of our model, the HetSNGP Ensemble, which addition-
ally accounts for parameter uncertainty and outperforms other ensemble baselines.
2	Background
2.1	Model uncertainty and spectral-normalized Gaussian process
Model uncertainty (or epistemic uncertainty) captures all the uncertainty about whether a given
model is correctly specified for a certain task, given the training data. Before any training data has
been observed, this uncertainty depends only on the prior knowledge about the task, which can for
instance be encoded into distributions about the model parameters or into the architecture of the
model (i.e., the model class) (Fortuin, 2021). After training data has been observed, one should
expect the model uncertainty to decrease within the support of the training data distribution, that is,
on points that are close to the training data in the input space. We will generally call these points
in-distribution (ID). Conversely, on points that are far away from the training points and thus out-of-
distribution (OOD), we should not expect the model uncertainty to decrease, since the training data
points are not informative enough to make any assertions about the correctness of the given model
on these points. In this sense, the optimal model uncertainty should be distance-aware, that is, it
should grow away from the training data (Liu et al., 2020).
The spectral-normalized Gaussian process (SNGP) (Liu et al., 2020) provides such distance-aware
model uncertainties by specifying a Gaussian process prior (Rasmussen & Williams, 2006) over
the latent data representations in the penultimate layer of the neural network. Distance-awareness
is ensured by using spectral normalization on the hidden layers (Behrmann et al., 2019), which
encourages bi-Lipschitzness of the mapping from data to latent space, constraining the model from
collapsing any input feature dimensions in the latent representations and approximately preserving
the distances between data points in the latent space (Liu et al., 2020). Note that this approach only
partially captures the model uncertainty, namely in form of the uncertainty over the latents. It does
not however capture the uncertainty over the model parameters, such as for instance Bayesian neural
networks (MacKay, 1992; Neal, 1993) or ensemble methods (Lakshminarayanan et al., 2016).
2
Under review as a conference paper at ICLR 2022
2.2	Data uncertainty and the heteroscedastic method
As opposed to the model uncertainty described above, data uncertainty is intrinsic in the data and
thus irreducible with increasing amounts of data. In the case of continuous data (e.g., regression
problems), data uncertainty often comes in the form of random noise on the measurements. For
discrete data (e.g., classification problems), it usually comes as label noise, that is, a certain number
of data points will be assigned the wrong label in the training data. This label noise can be class- and
input-dependent (Beyer et al., 2020). For instance, the Imagenet dataset (Deng et al., 2009), contains
100 different classes of dog breeds, which are often harder to tell apart for human labelers than the
other classes in the dataset. Modeling this type of data uncertainty can improve the calibration and
robustness of predictive models (Collier et al., 2020).
A model that does explicitly handle the input- and class-dependent label noise is the heteroscedas-
tic method (Collier et al., 2021). The heteroscedastic method models input- and class-dependent
noise by introducing a latent multivariate Gaussian distribution on the softmax logits of a standard
neural network classifier. The covariance matrix of this latent distribution is a function of the input
(heteroscedastic) and models inter-class correlations in the logit noise.
3	Method
3.1	Setup and Notation
Let us consider a dataset D = {(xi, yi)}iN=1 of input-output pairs, where xi ∈ Rd and yi ∈
{1, . . . , K}, that is, a classification problem with K classes. The data examples are assumed to
be sampled i.i.d. from some true data-generating distribution as (xi, y%)〜p* (x, y).
3.2	Generative process
To jointly model the two different types of uncertainty, we propose a hierarchical model of two
latent random variables, which we denote by f and u. f is a latent function value associated to
the input x (as in the Gaussian process literature) and is designed to capture the model uncertainty,
while u is a latent vector of logits (or utilities) that capture the data uncertainty, similar to the setup
in Collier et al. (2020), which was inspired by the econometrics literature (Train, 2009). Similarly to
LiU et al. (2020), we place a latent Gaussian process (GP) prior over f, as p(f) = GP(0, k§(∙, ∙)),
where kθ is a parameterized kernel function with parameters θ. Evaluating this kernel on all pair-
wise combinations of datapoints in x yields the kernel matrix Kθ(x, x)). We then define u as a
(heteroscedastically) noisy observation of f.
Stacking the variables across the whole dataset gives us the matrices F, U ∈ RN×K. We refer
to their respective rows as fi, ui ∈ RK and their columns as fc, uc ∈ RN . The columns are
independent under the GP prior, but the rows are not. Conversely, the columns are correlated in
the heteroscedastic noise model, while the rows are independent. A hierarchical model using both
uncertainties therefore leads to logits that are correlated across both data points and classes.
The full generative process is
fc 〜N(0, Kθ(x, x))	(1)
Ui 〜N(fi,∑(xi; 0)	(2)
p(yi = C | Ui ) = 1 C = arg max Uik	(3)
k
The predictive can then be computed my marginalizing, that is p(y | x) = Eu[p(y | U)] =
p(y | U) p(U | x) dU. Intuitively, f captures the model uncertainty, that is, the uncertainty about
the functional mapping between x and y on the level of the latents. It uses the covariance between
data points to achieve this distance-awareness, namely it uses the kernel function to assess the sim-
ilarity between data points, yielding an uncertainty estimate that grows away from the data. On the
other hand, U captures the data uncertainty, by explicitly modelling the per-class uncertainty on the
level of the logits.夕 can learn to encode correlations in the noise between different classes (e.g., the
dog breeds in Imagenet). It does not itself capture the model uncertainty, but inherits it through its
3
Under review as a conference paper at ICLR 2022
hierarchical dependence on f, such that the resulting probability p(y | u) ultimately jointly captures
both types of uncertainty.
In practice, we usually learn the kernel using deep kernel learning (Wilson et al., 2016), that is, we
define it as the RBF kernel kθ(xi, Xj) = kRBF(hi, hj) = exp(khi 一 hjk2∕λ) with length scale λ
and hi = h(xi； θ) ∈ Rm with h(∙; θ) being a neural network model (e.g., a ResNet) parameterized
by θ. This kernel is then shared between classes. Moreover, following Liu et al. (2020), we typically
encourage bi-Lipschitzness of h using spectral normalization (Behrmann et al., 2019), which then
leads to an approximate preservation of distances between the input space and latent space, thus al-
lowing for distance-aware uncertainty modeling in the latent space. Additionally, Σ(∙;夕)is usually
also a neural network parameterized by 夕.To ease notation, We will typically drop these parameters
in the following. Note that the prior over fc (and thus also uc) has zero mean, thus leading to a
uniform output distribution away from the data. Note that this is also reminiscent of multi-task GPs
(Williams et al., 2007), where separate kernels are used to model covariances between data points
and tasks and are then combined into a Kronecker structure.
3.3	Computational approximations
The generative process above requires computing two integrals (i.e., over u and f) and inferring
an exact GP posterior, making it computationally intractable. We make several approximations to
ensure tractability.
Low-rank approximation. The heteroscedastic covariance matrix Σ(xi;夕)is a K X K matrix
which is a function of the input. Parameterizing this full matrix is costly in terms of computation,
parameter count, and memory. Therefore, following Collier et al. (2021), we make a low-rank ap-
proximation Σ(xi;夕)= V(Xi) V(xi)> + d2(xi). Here, V(Xi) is a K × R matrix with R << K
and d2(xi) is a K dimensional vector added to the diagonal of V(xi)V(xi)>. This approximation
assumes that the similarities between different classes in the data, which in turn lead to nonzero co-
variance entries, can be approximately described as being linear in a suitable R-dimensional space.
Note that R can be tuned as a hyperparameter and can thus be made large enough, such that this
assumption holds.
Also following Collier et al. (2021), we introduce a parameter-efficient version of our method to
enable scaling to problems with many classes, for instance, Imagenet21k which has 21,843 classes.
In the standard version of our method, V(Xi) is parameterized as an affine transformation of the
shared representation hi . In this way, HetSNGP can be added as a single output layer on top of
a base network. For the parameter-efficient HetSNGP, we parameterize V(Xi) = v(Xi)1R> V
where v(Xi) is a vector of dimension R, 1R is a vector of ones of dimension R, V is a K × R
matrix of free parameters, and denotes element-wise multiplication. This additional approxi-
mation assumes that the similarity between classes is largely data-point-independent (i.e., different
classes are intrinsically similar), while the specific data point can be responsible for scaling certain
dimensions of similarity in the R-dimensional space.
Random Fourier feature approximation. Computing the exact GP posterior requires O(N3)
operations (because ones needs to invert an N × N kernel matrix), which can be prohibitive for large
datasets. Following Liu et al. (2020), we thus use a random Fourier feature (RFF) approximation
(Rahimi et al., 2007), leading to a low-rank approximation of the kernel matrix as K (x, x) = ΦΦ>
(Φ ∈ RN×m), with random features Φ% =	COs(Whi + b) where W is a fixed weight matrix
with entries sampled from N(0, 1) and b is a fixed bias vector with entries sampled from U(0, 2π).
This approximates the infinite-dimensional reproducing kernel Hilbert space (RKHS) of the RBF
kernel with a subspace spanned by m randomly sampled basis functions. This reduces the GP
inference complexity to O(N m2), where m is the dimensionality of the latent space. We can then
write the model as a linear model in this feature space, namely
ui = fi + d(Xi)	K + V (Xi)R with fc = Φβc and	(4)
p(βc) =N(0,Im); p(K) =N(0,IK); p(R) =N(0,IR)
Here, Φβc is a linear regressor in the space of the random Fourier features and the other two terms of
ui are the low-rank approximation to the heteroscedastic uncertainties (as described above). Again,
4
Under review as a conference paper at ICLR 2022
m can be tuned as a hyperparameter to trade off computational accuracy with fidelity of the model.
Since most data sets contain a lot of redundant data points, one often finds an m N that models
the similarities in the data faithfully.
Algorithm 1 HetSNGP training	Algorithm 2 HetSNGP prediction
Require: dataset D = {(xi , yi)}iN=1
Initialize θ,夕,W, b, Σ
for train_step = 1 to max^tep do
Take minibatch (Xi , yi ) from D
for s = 1 to S do
CK 〜N(0, Ik), eR 〜N(0, Ir)
us,c = φ>βc + d(xi) © eK +
V (xi)sR
end for
L =-s1 PS=IlOgP(Xi, yi |US)-kβk2
Update {θ,夕，β} Via SGD on L
if finaLepoch then
Compute {Σb c-1}cK=1as per Eq. (5)
end if
end for
Require: test example x*
Φ* = qm cos(Wh(x*) + b)
for s = 1 to S do
βc 〜p(βc |D)
eK 〜N(0,Ik), eR 〜N(0,Ir)
us*,c = Φ*> βcs + d(x*) © sK +
V (x* )sR
end for
p(y*=c | x*)=1 ps=1 PK=Pexp,u[k/T)
Predict y* = arg maxc p(y* = c | x*)
Laplace approximation. When using a Gaussian likelihood (i.e., in a regression setting), the GP
posterior inference can be performed in closed form. HoweVer, for classification problems this is
not possible, because the Categorical likelihood used is not conjugate to the Gaussian prior. We thus
need to resort to approximate posterior inference. Again following Liu et al. (2020), we perform a
Laplace approximation (Rasmussen & Williams, 2006) to the RFF-GP posterior, which yields the
closed-form approximate posterior for βc
N
p(βc |D) = N(βc, ∑c) With Σ-1 = Im + XPi,c(1 - Pi,c)ΦiΦ>	(5)
i=1
where pi,c is shorthand for the softmax output p(yi = C | Ui) as defined in Eq. (3), where Ui,c =
Φ>βc. The derivation of this is deferred to Appendix A.3. Here, Σ-1 can be cheaply computed over
minibatches of data by virtue of being a sum over data points. Moreover, βc is the MAP solution,
which can be obtained using gradient descent on the unnormalized log posterior, 一 logp(β | D) 8
- log p(D | β) - kβk2, where the squared norm regularizer stems from the standard Gaussian prior
on β. Using our likelihood, the training objective takes the form ofa ridge-regularized cross-entropy
loss. We also use this objective to train the other trainable parameters θ and 夕 of our model. Note that
this approximation is necessarily ignoring multi-modality of the posterior, since it can by definition
only fit one mode around the MAP solution. Nonetheless, recent works have shown that it can
achieve suprisingly good performance in many settings and sometimes even be competitive with the
true posterior (Immer et al., 2021b;a).
Monte Carlo approximation. Finally, we are ultimately interested in Eu[p(y | U)]	=
p(y | U) p(U | x) dU, which again would require solving a high-dimensional integral and is thus
computationally intractable. Following Collier et al. (2020), we approximate it using Monte Carlo
samples. Moreover, we approximate the argmax in Eq. (3) with a softmax. To allow for a more
controlled trade-off between bias and variance in this approximation, we introduce an additional
temperature parameter τ into this softmax, leading to
1 XS^	1 XS^	exp(uSc∕τ)
汉yi = CIxi) = S X 汉yi = CIus) = S X PK=I eχp(us,k ∕τ)	⑹
with	uis,c	=	Φi>βcs	+ d(xi)	©	sK	+ V	(xi)sR	and	βcs	〜 p(βc	| D)	and
WK 〜N(0,Ik) and WR = NaIR)
5
Under review as a conference paper at ICLR 2022
e e e e e
Train labels	Deterministic	SNGP	Heteroscedastic	HetSNGP
(acc = 0.855 ± 0.004) (acc = 0.855 ± 0.001) (acc = 0.868 ± 0.002) (acc = 0.867 ± 0.002)
Figure 2: Predicted labels on synthetic data with label noise. We see that the proposed method
performs on par with the heteroscedastic method and that they both outperform the other baselines
thanks to their label noise modeling capabilities.
It should be noted that we only use Eq. (6) at test time, while during training we replace βcs with its
mean βc instead of sampling it (See Section 3.4). Note also that, while a larger number S of Monte
Carlo samples increases the computational cost, these samples can generally be computed in parallel
on modern hardware, such that the wallclock runtime stays roughly the same. Again, the number of
samples S can be tuned as a hyperparameter, such that this approximation can be made arbitrarily
exact with increased runtime (or, as mentioned, increased parallel computational power).
3.4	Implementation
The training of our proposed model is described in Algorithm 1 and the prediction in Algorithm 2.
Intuition. The distance-aware uncertainties of the SNGP are modeled through Φ, which itself
depends on the latent representations h, which are approximately distance-preserving thanks to the
bi-Lipschitzness. This means that when we have an input xfar that is far away from the training data,
the values of the RBF kernel exp(∣∣h - hfark2∕λ) will be small, and thus the uncertainty of f will
be large. The resulting output uncertainties will therefore be large regardless of the heteroscedastic
variance Σ(xi) (since they are additive) allowing for effective OOD detection. Conversely, if we
have an in-distribution input, the kernel values will be large and the model uncertainty captured in fc
will be small. In this case, the additive output uncertainty will be dominated by the heteroscedastic
variance Σ(xi), allowing for effective label noise modeling in-distribution.
While at first it might seem straightforward to combine the two types of uncertainty, it should be
noted that the proposed hierarchical model is by no means an arbitrary choice. Rather, it is the
only design we found that robustly achieves a disentanglement of the two uncertainty types dur-
ing training. When using other architectures, for instance, using the heteroscedastic uncertainty
directly in the likelihood of the GP or using two additive random variables with respective GP and
heteroscedastic distributions, the two models will compete for the explained uncertainty and the het-
eroscedastic prediction network will typically try to explain as much uncertainty as possible. Only
in this hierarchical model setting, where the SNGP adds uncertainty to the mean first and the het-
eroscedastic uncertainty is added in the second step, can we robustly achieve a correct assignment
of the different uncertainties using gradient-based optimization.
Challenges and limitations. As outlined above, the main challenge in this model is the inference.
We chose an approximate inference scheme that is computationally efficient, utilizing a series of
approximations, and the fact that we model the distance-aware and heteroscedastic variances addi-
tively. If we wanted to make the model more powerful, at the cost of increased computation, we
could thus consider several avenues of improvement: (i) use inducing points or even full GP in-
ference for the GP posterior; (ii) use a more powerful approximation than Laplace, for instance, a
variational approximation; or (iii) model the variances jointly, such that we do not only have covari-
ance between data points in f and between classes in u, but have a full covariance between different
classes of different data points in u. All of these ideas are interesting avenues for future research.
6
Under review as a conference paper at ICLR 2022
Table 1: Results on CIFAR-100. We used Places365 as far-OOD and CIFAR-10 as near-OOD
datasets. The reported values are means and standard errors over 10 runs. Bold numbers are within
one standard error of the best performing model. Our model outperforms the baselines in terms of
accuracy on corrupted data and performs on par with the best models on OOD detection.
Method	↑ID Acc	R NLL	↑Corr Acc	IColT NLL	↑Near-ROC	JNear-FPR95		↑Far-ROC	JFar-FPR95
Det.	0.808 ± 0.000	0.794 ± 0.002	0.455 ± 0.001	2.890 ± 0.010	0.506 ± 0.006	0.963	± 0.005	0.442 ± 0.060	0.935 ± 0.041
Post.Net.	0.728 ± 0.001	1.603 ± 0.014	0.444 ± 0.001	3.086 ± 0.009	0.472 ± 0.013	1.000	± 0.000	0.518 ± 0.016	1.000 ± 0.000
Het.	0.807 ± 0.001	0.782 ± 0.002	0.447 ± 0.001	3.130 ± 0.018	0.496 ± 0.005	0.957	± 0.004	0.420 ± 0.024	0.958 ± 0.010
SNGP	0.797 ± 0.001	0.762 ± 0.002	0.466 ± 0.001	2.339 ± 0.007	0.493 ± 0.011	0.961	± 0.005	0.518 ± 0.073	0.919 ± 0.030
HetSNGP	0.799 ± 0.001	0.856 ± 0.003	0.471 ± 0.001	2.565 ± 0.007	0.499 ± 0.010	0.955	± 0.005	0.525 ± 0.038	0.910 ± 0.024
4	Experiments
We conducted experiments on synthetic data and standard image classification benchmarks. As
baselines we compare against a standard deterministic model (He et al., 2016; Dosovitskiy et al.,
2020), the heteroscedastic method (Collier et al., 2021) and SNGP (Liu et al., 2020). We also
compare against the Posterior Network model (Charpentier et al., 2020), which also offers distance-
aware uncertainties, but it only applies to problems with few classes. Note that our main motivation
for the experiments is to assess whether combining the SNGP and heteroscedastic method can suc-
cessfully trade off the complementary benefits of the two methods. Since not all tasks and data sets
require both types of uncertainty at the same time, we do not expect our proposed model to outper-
form the baselines on all tasks. We would however expect that, depending on the particular task and
required uncertainty type, it would generally outperform one of the two baselines. Our non-synthetic
experiments are developed within the open source codebases Uncertainty上aselines (Nado
et al., 2021) and robustness_metrics (Djolonga et al., 2020) (to assess the OOD perfor-
mances). Implementation details are deferred to Appendix A.2.
To measure the predictive performance of the models, we use their accuracy (Acc) and negative
log-likelihood (NLL). To measure their uncertainty calibration, we use the expected calibration error
(ECE) (Naeini et al., 2015). To measure OOD detection, we use the area under the receiver-operator-
characteristic curve when using the predictive uncertainty as a score (ROC) and the false-positive
rate (rate of classifying an OOD data point as being in-distribution) at 95% recall (FPR95).
4.1	Synthetic experiments
Synthetic OOD data Following the SNGP paper (Liu et al., 2020), to assess the distance-
awareness property of HetSNGP in a visually verifiable way, we performed experiments on two-
dimensional synthetic datasets; (i) two moons and (ii) a Gaussian mixture. We see in Fig. 1 that
neither the deterministic or heteroscedastic models offer distance-aware uncertainties and are there-
fore overconfident on the OOD data (red points). The Posterior network does offer distance-aware
uncertainties, which on these datasets grow more slowly away from the training data than the SNGP
methods. However, we will see later that scaling the Posterior network to datasets with many classes
is too expensive to be practical. Only the SNGP and our proposed HetSNGP are highly uncertain on
the OOD points, thus allowing for effective OOD detection.
Synthetic heteroscedastic data We now wish to verify in a low-dimensional setting that Het-
SNGP also retains the in-distribution heteroscedastic modelling property of the heteroscedastic
method. We use the noisy concentric circles from Berthon et al. (2021), where the three circular
classes have the same mean, but different amounts of label noise. We see that the heteroscedastic
model and our proposed method are able to capture this label noise and thus achieve a better per-
formance, while the deterministic baseline and the SNGP are not. Based on these two synthetic
experiments, it becomes apparent that our proposed HetSNGP successfully combines the desirable
OOD uncertainties of the SNGP with the heteroscedastic uncertainties on these simple datasets. We
now proceed to evaluate these properties on more challenging higher-dimensional datasets.
7
Under review as a conference paper at ICLR 2022
Table 2: Results on Imagenet. We used Imagenet-C as near-OOD and Imagenet-A as far-OOD. We report the mean and standard error over 10 runs. Bold numbers are within one standard error of the best model. HetSNGP performs best in terms of accuracy on Imagenet-C and Imagenet-A.									
Method	↑ID Acc	UD NLL	UD ECE	↑ImC Acc	UmC NLL	UmC ECE	↑ImA Acc	UmA NLL	UmA ECE
Det.	0.759 ± 0.000	0.952 ± 0.001	0.033 ± 0.000	0.419 ± 0.001	3.078 ± 0.007	0.096 ± 0.002	0.006 ± 0.000	8.098 ± 0.018	0.421 ± 0.001
Het.	0.771 ± 0.000	0.912 ± 0.001	0.033 ± 0.000	0.424 ± 0.002	3.200 ± 0.014	0.111 ± 0.001	0.010 ± 0.000	7.941 ± 0.014	0.436 ± 0.001
SNGP	0.757 ± 0.000	0.947 ± 0.001	0.014 ± 0.000	0.420 ± 0.001	2.970 ± 0.007	0.046 ± 0.001	0.007 ± 0.000	7.184 ± 0.009	0.356 ± 0.000
HetSNGP	0.769 ± 0.001	0.927 ± 0.002	0.033 ± 0.000	0.428 ± 0.001	2.997 ± 0.009	0.085 ± 0.001	0.016 ± 0.001	7.113 ± 0.018	0.401 ± 0.001
4.2	CIFAR experiment
We start by assessing our method on a real-world image dataset; we trained it on CIFAR-100 and
used CIFAR-10 as a near-OOD dataset and Places365 (Zhou et al., 2017) as far-OOD. We measure
the OOD detection performance in terms of area under the receiver-operator-characteristic curve
(ROC) and false-positive-rate at 95% confidence (FPR95). We also evaluated the methods’ gen-
eralization performance on corrupted CIFAR-100 (Hendrycks & Dietterich, 2019). In Table 1 we
see that HetSNGP yields a performance between the heteroscedastic and SNGP methods in terms
of in-distribution accuracy, but outperforms all baselines in accuracy on the corrupted data. More-
over, it performs on par with the best performing models on both near- and far-OOD detection.
This suggests that in-distribution, only the heteroscedastic uncertainty is needed, such that both
the heteroscedastic method and our HetSNGP outperform the standard SNGP in terms of accuracy.
However, on the corrupted data, which is outside of the training distribution, the SNGP outperforms
the heteroscedastic method in terms of accuracy and our HetSNGP outperforms both baselines, since
both types of uncertainty are useful in this setting.
4.3	Imagenet experiment
A large-scale dataset with natural label noise and established OOD benchmarks is the Imagenet
dataset (Deng et al., 2009; Beyer et al., 2020). The heteroscedastic method has been shown to
improve in-distribution performance on Imagenet (Collier et al., 2021). We see in Table 2 that Het-
SNGP outperforms the SNGP in terms of accuracy and likelihood on the in-distribution Imagenet
validation set and performs almost on par with the heteroscedastic model. The slight disadvantage
compared to the heteroscedastic model suggests a small trade-off due to the restricted parameteriza-
tion of the output layer and application of spectral normalization.
However, the true benefits of our model become apparent when looking at the Imagenet OOD
datasets (Table 2, right side). Here, we still have the noisy label properties from the original Im-
agenet dataset, such that heteroscedastic uncertainties are useful, but we are also outside of the
training distribution, such that distance-aware model uncertainties become crucial. On Imagenet-C
and Imagenet-A, we see that our proposed model makes good use of both of these types of uncer-
tainties and thus manages to outperform all the baselines in terms of accuracy. Additional OOD
results on Imagenet-R and Imagenet-V2 are shown in Table 5 in Appendix A.4.
4.4	Imagenet- 2 1 k
We introduce a new large-scale OOD benchmark based on Imagenet-21k. Imagenet-21k is a larger
version of the standard Imagenet dataset used above (Deng et al., 2009). It has over 12.8 million
training images and 21,843 classes. Each image can have multiple labels, whereas for standard
Imagenet a single label is given per image. Note that 999 of the 1,000 Imagenet classes are contained
in Imagenet-21k (class n04399382 is missing).
We train a ViT-B/16 (Dosovitskiy et al., 2020) vision transformer model on the Imagenet-21k
dataset. See Appendix A.2 for training details. We then evaluate the model on the 1,000 Ima-
genet classes (setting the predictive probability of class n04399382 to zero). Despite now being
in a setting where the model is trained on an order of magnitude more data and greater than 21×
more classes, we can use the standard Imagenet OOD datasets. This assesses the scalability of our
method. We also hope this new benchmark will prove useful for future OOD research.
8
Under review as a conference paper at ICLR 2022
Table 3: Results on Imagenet-21k. The reported values are means and standard errors over 5 runs.
Bold numbers are within one standard error of the best performing model. We use standard Ima-
genet, Imagenet-C, Imagenet-A, Imagenet-R, and Imagenet-V2 as OOD datasets. HetSNGP outper-
forms the baselines on all OOD datasets.
Method	↑ID prec@1	↑Im Acc	↑ImC Acc	↑ImA Acc	↑ImR Acc	↑ImV2 Acc
Det.	0.471 ± 0.000	0.800 ± 0.000	0.603 ± 0.000	0.149 ± 0.000	0.311 ± 0.000	0.694 ± 0.000
Het.	0.480 ± 0.001	0.796 ± 0.002	0.590 ± 0.001	0.132 ± 0.004	0.300 ± 0.006	0.687 ± 0.000
SNGP	0.468 ± 0.001	0.799 ± 0.001	0.602 ± 0.000	0.165 ± 0.003	0.328 ± 0.005	0.696 ± 0.003
HetSNGP	0.477 ± 0.001	0.806 ± 0.001	0.613 ± 0.003	0.172 ± 0.007	0.336 ± 0.002	0.705 ± 0.001
Table 4: Results on Imagenet. Ensemble size=4. Imagenet-C is used as the OOD dataset. HetSNGP
outperforms the baselines in terms of in-distribution accuracy and all on OOD metrics.
Method	↑ID Acc	UD NLL	UD ECE	↑ImC Acc	UmC NLL	UmC ECE
Det Ensemble	0.779	0.857	0.017	0.449	2.82	0.047
Het Ensemble	0.795	0.790	0.015	0.449	2.93	0.048
SNGP Ensemble	0.781	0.851	0.039	0.449	2.77	0.050
HetSNGP Ensemble	0.797	0.798	0.028	0.458	2.75	0.044
We see in Table 3 that the parameter-efficient heteroscedastic method has the best in-distribution
precision@1. However, its generalization performance to the OOD benchmarks is the weakest of
all the methods. To scale to over 21k output classes, we use the parameter-efficient HetSNGP
method. Our method recovers almost the full in-distribution performance of the heteroscedastic
method, significantly outperforming the deterministic and SNGP methods. Notably, it also clearly
outperforms all other methods on the OOD datasets. Similarly to the CIFAR experiment, we thus
see again that in-distribution, the heteroscedastic method and our HetSNGP both outperform the
SNGP, since OOD uncertainties are not that important there, while on the OOD datasets, the SNGP
and our HetSNGP both outperform the heteroscedastic method. Our method therefore achieves the
optimal tradeoff.
We found that SNGP and HetSNGP underfit to the data when using the posterior p(βc | D) at test
time, so instead we use the posterior mode β at both training and test time. Additionally, we found
spectral normalization was not necessary to preserve distance awareness for the ViT architecture;
hence Imagenet-21k experiments are run without spectral normalization.
4.5	Ensembling experiment
Deep ensembles are popular methods for model uncertainty estimation due to their simplicity and
good performance (Lakshminarayanan et al., 2016). We propose a variant of our HetSNGP method
which captures a further source of model uncertainty in the form of parameter uncertainty by a deep
ensemble of HetSNGP models. HetSNGP Ensemble is therefore a powerful yet simple method for
capturing three major sources of uncertainty; 1) data uncertainty in the labels, 2) model uncertainty
in the latent representations and 3) model uncertainty in the parameters.
We compare the HetSNGP Ensemble to a determinstic ensemble as well as ensembles of het-
eroscedastic and SNGP models. We see in Table 4 that in the case of an ensemble of size four,
HetSNGP Ensemble outperforms the baselines on the Imagenet-C OOD dataset in terms of all met-
rics, while also outperforming them on the standard in-distribution Imagenet dataset in terms of
accuracy. Due to computational constraints we do not have error bars in this experiment. However,
we do not expect them to be much larger than in Table 2.
5	Ethics and Reproducibility S tatements
Ethics. We present a generic classification algorithm. HetSNGP is not application-specific and
has the computational cost of the heteroscedastic and SNGP methods combined. We do not foresee
9
Under review as a conference paper at ICLR 2022
ethical issues specific to our method beyond any already associated with the heteroscedastic and
SNGP methods or deep neural network classifiers in general.
Reproducibility. Hyperparameters and further details required to reproduce our results are pro-
vided in Appendix A.2. We have open-sourced the code to reproduce our CIFAR and Imagenet
results. We plan to open-source the code to reproduce the Imagenet-21k experiments. Further de-
tails on our use of the Laplace approximation are provided in Appendix A.3.
References
Ben Adlam, Jasper Snoek, and Samuel L Smith. Cold posteriors and aleatoric uncertainty. arXiv
preprint arXiv:2008.00029, 2020.
Laurence Aitchison. A statistical theory of cold posteriors in deep neural networks. In International
Conference on Learning Representations, 2020.
Javier Antoran, James UrqUhart Allingham, and Jose MigUel Hernandez-Lobato. Depth uncertainty
in neural networks. arXiv preprint arXiv:2006.08437, 2020.
Jens Behrmann, Will GrathWohL Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573-582.
PMLR, 2019.
Antonin Berthon, Bo Han, Gang Niu, Tongliang Liu, and Masashi Sugiyama. Confidence scores
make instance-dependent label-noise learning possible. In International Conference on Machine
Learning, pp. 825-836. PMLR, 2021.
Lucas Beyer, Olivier J Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are
we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622. PMLR, 2015.
Bertrand Charpentier, Daniel Zugner, and Stephan Gunnemann. Posterior network: Uncertainty es-
timation without ood samples via density-based pseudo-counts. arXiv preprint arXiv:2006.09239,
2020.
Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann, and Richard Turner. Conserva-
tive uncertainty estimation by fitting prior networks. In International Conference on Learning
Representations, 2019.
Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, and Jesse Berent. A simple
probabilistic method for deep classification under input-dependent label noise. arXiv preprint
arXiv:2003.06778, 2020.
Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, and Jesse Berent. Correlated
input-dependent label noise in large-scale image classification. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 1551-1560, 2021.
Francesco D’Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. arXiv preprint
arXiv:2106.11642, 2021.
Francesco D’Angelo, Vincent Fortuin, and Florian Wenzel. On stein variational neural network
ensembles. arXiv preprint arXiv:2106.10760, 2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efficient and risk-sensitive learning. In
International Conference on Machine Learning, pp. 1184-1193. PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Josip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijnders,
Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/
google-research/robustness_metrics.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas Kemp, Jeremy Nixon, Ghassen Jerfel,
Katherine Heller, and Andrew M Dai. Analyzing the role of model uncertainty for electronic
health records. In Proceedings of the ACM Conference on Health, Inference, and Learning, pp.
204-213, 2020.
Angelos Filos, Sebastian Farquhar, Aidan N Gomez, Tim GJ Rudner, Zachary Kenton, Lewis Smith,
Milad Alizadeh, Arnoud De Kroon, and Yarin Gal. A systematic comparison of bayesian deep
learning robustness in diabetic retinopathy tasks. arXiv preprint arXiv:1912.10481, 2019.
Vincent Fortuin. Priors in bayesian deep learning: A review. arXiv preprint arXiv:2105.06868,
2021.
Vincent Fortuin, Adria Garriga-Alonso, Mark van der Wilk, and Laurence Aitchison. Bnnpriors: A
library for bayesian neural network inference with different prior distributions. Software Impacts,
9:100079, 2021a.
Vincent Fortuin, Adria Garriga-Alonso, Florian Wenzel, Gunnar Ratsch, Richard Turner, Mark
van der Wilk, and Laurence Aitchison. Bayesian neural network priors revisited. arXiv preprint
arXiv:2102.06571, 2021b.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059.
PMLR, 2016.
Adria Garriga-Alonso and Vincent Fortuin. Exact langevin dynamics with stochastic gradients.
arXiv preprint arXiv:2102.01691, 2021.
Alex Graves. Practical variational inference for neural networks. Advances in neural information
processing systems, 24, 2011.
Danijar Hafner, Dustin Tran, Timothy Lillicrap, Alex Irpan, and James Davidson. Noise contrastive
priors for functional uncertainty. In Uncertainty in Artificial Intelligence, pp. 905-914. PMLR,
2020.
Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Laksh-
minarayanan, Andrew M Dai, and Dustin Tran. Training independent subnetworks for robust
prediction. arXiv preprint arXiv:2010.06610, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. Proceedings of the International Conference on Learning Represen-
tations, 2019.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the sixth annual conference on Computational
learning theory, pp. 5-13, 1993.
Marius Hobbhahn, Agustinus Kristiadi, and Philipp Hennig. Fast predictive uncertainty for classifi-
cation with bayesian deep networks. arXiv preprint arXiv:2003.01227, 2020.
11
Under review as a conference paper at ICLR 2022
Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Ratsch, and Mohammad Emtiyaz Khan.
Scalable marginal likelihood estimation for model selection in deep learning. arXiv preprint
arXiv:2104.04975, 2021a.
Alexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural
nets via local linearization. In International Conference on Artificial Intelligence and Statistics,
pp. 703-711. PMLR, 2021b.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? arXiv preprint arXiv:1703.04977, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. PhD
thesis, University of Toronto, 2009.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshmi-
narayanan. Simple and principled uncertainty estimation with deterministic deep learning via
distance awareness. arXiv preprint arXiv:2006.10108, 2020.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-
tation, 4(3):448-472, 1992.
Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. arXiv
preprint arXiv:1802.10501, 2018.
Andrey Malinin, Bruno Mlodozeniec, and Mark Gales. Ensemble distribution distillation. arXiv
preprint arXiv:1905.00076, 2019.
Dimitrios Milios, Raffaello Camoriano, Pietro Michiardi, Lorenzo Rosasco, and Maurizio Filip-
pone. Dirichlet-based gaussian processes for large-scale calibrated classification. arXiv preprint
arXiv:1805.10915, 2018.
Miguel Monteiro, Lolc Le Folgoc, Daniel Coelho de Castro, Nick Pawlowski, Bernardo Marques,
Konstantinos Kamnitsas, Mark van der Wilk, and Ben Glocker. Stochastic segmentation net-
works: Modelling spatially correlated aleatoric uncertainty. arXiv preprint arXiv:2006.06015,
2020.
Kevin P Murphy. Machine learning: A probabilistic perspective, 2012.
Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael Dusenberry, Sebastian Farquhar,
Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah Liu, Zelda Mariet,
Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim Rudner, Yeming Wen, Florian Wenzel, Kevin Mur-
phy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin Gal, and Dustin Tran. Uncer-
tainty Baselines: Benchmarks for uncertainty & robustness in deep learning. arXiv preprint
arXiv:2106.04015, 2021.
Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated prob-
abilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence,
2015.
Radford M Neal. Bayesian learning via stochastic dynamics. In Advances in neural information
processing systems, pp. 475-482, 1993.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.
Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. High-quality prediction inter-
vals for deep learning: A distribution-free, ensembled approach. In International Conference on
Machine Learning, pp. 4075-4084. PMLR, 2018.
12
Under review as a conference paper at ICLR 2022
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007.
Carl Edward Rasmussen and Christopher K Williams. Gaussian processes for machine learning,
volume 2. MIT press Cambridge, MA, 2006.
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classifica-
tion uncertainty. arXiv preprint arXiv:1806.01768, 2018.
Andrew Stirn and David A Knowles. Variational variance: Simple, reliable, calibrated heteroscedas-
tic noise variance parameterization. arXiv preprint arXiv:2006.04910, 2020.
Natasa Tagasovska and David Lopez-Paz. Single-model uncertainties for deep learning. arXiv
preprint arXiv:1811.00908, 2018.
Kenneth E Train. Discrete choice methods with simulation. Cambridge university press, 2009.
Linh Tran, Bastiaan S Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V Dillon, Jasper Snoek,
Stephan Mandt, Tim Salimans, Sebastian Nowozin, and Rodolphe Jenatton. Hydra: Preserving
ensemble diversity for model distillation. arXiv preprint arXiv:2001.04694, 2020.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688.
Citeseer, 2011.
Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Swikatkowski, Linh Tran, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes
posterior in deep neural networks really? In International Conference on Machine Learning,
2020.
Chris Williams, Edwin V Bonilla, and Kian M Chai. Multi-task gaussian process prediction. Ad-
vances in neural information processing systems, pp. 153-160, 2007.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial intelligence and statistics, pp. 370-378. PMLR, 2016.
Wanqian Yang, Lars Lorch, Moritz A Graule, Srivatsan Srinivasan, Anirudh Suresh, Jiayu Yao,
Melanie F Pradier, and Finale Doshi-Velez. Output-constrained bayesian neural networks. arXiv
preprint arXiv:1905.06287, 2019.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 mil-
lion image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2017.
13
Under review as a conference paper at ICLR 2022
A	Appendix
A. 1 Related Work
Uncertainty estimation. There has been a lot of research on uncertainty estimation in recent years
(Ovadia et al., 2019), including the early observation that one needs data and model uncertainties for
successful use in real-world applications (Kendall & Gal, 2017). There have also been attempts to
directly optimize specialized loss functions to improve model uncertainty (Tagasovska & Lopez-Paz,
2018; Pearce et al., 2018). However, if one wants to directly implement prior knowledge regarding
the OOD behavior of the model, one usually needs access to OOD samples during training (Yang
et al., 2019; Hafner et al., 2020). Another popular idea is to use a hierarchical output distribution,
for instance a Dirichlet distribution (Milios et al., 2018; Sensoy et al., 2018; Malinin & Gales, 2018;
Malinin et al., 2019; Hobbhahn et al., 2020), such that the model uncertainty can be encoded in
the Dirichlet and the data uncertainty in its Categorical distribution samples. This idea was also
used in our Posterior Network baseline (Charpentier et al., 2020). Similarly to our idea of learning
separate variances, it has also been proposed to treat the output variance variationally and to specify
a hierarchical prior over it (Stirn & Knowles, 2020).
Bayesian neural networks and ensembles. The gold-standard for capturing model uncertainty over
the parameters are Bayesian neural networks (Neal, 1993; MacKay, 1992), but they are computa-
tionally quite expensive, require a careful choice of prior distribution (Fortuin et al., 2021b; Fortuin,
2021), and require specialized approximate inference schemes, such as Laplace approximation (Im-
mer et al., 2021b;a), variational inference (Hinton & Van Camp, 1993; Graves, 2011; Blundell et al.,
2015), or Markov Chain Monte Carlo (Welling & Teh, 2011; Garriga-Alonso & Fortuin, 2021; For-
tuin et al., 2021a). A more tractable model class are deep ensemble methods (Lakshminarayanan
et al., 2016; Ciosek et al., 2019; D’Angelo et al., 2021; D’Angelo & Fortuin, 2021), although they
are computationally still expensive. There are however some ideas to make them less expensive
by distilling their uncertainties into simpler models (Malinin et al., 2019; Tran et al., 2020; Havasi
et al., 2020; Antoran et al., 2020).
SNGP and heteroscedastic method. The models most relevant to our approach are the SNGP (Liu
et al., 2020) and the heteroscedastic method (Collier et al., 2020; 2021). SNGP yields distance-
aware model uncertainties, which grow away from the training data. The heteroscedastic method
models input- and class-dependent label noise data uncertainty inside the training distribution. Het-
eroscedastic data uncertainty modelling can also improve segmentation models (Monteiro et al.,
2020) and has been linked to the cold posterior effect in Bayesian neural networks (Wenzel et al.,
2020; Adlam et al., 2020; Aitchison, 2020). Prior work has primarily focused on modelling data and
distance-aware model uncertainty separately. But safety-critical practical applications require both
types of uncertainty, HetSNGP is a novel method which fulfills this requirement.
A.2 Implementation details
To assess our proposed model’s predictive performance and uncertainty estimation capabilities, we
conducted experiments on synthetic two moons data (Pedregosa et al., 2011), a mixture of Gaussians,
the CIFAR-100 dataset (Krizhevsky et al., 2009), and the Imagenet dataset (Deng et al., 2009). We
compare against a standard deterministic ResNet model as a baseline (He et al., 2016), against
the heteroscedastic method (Collier et al., 2020; 2021) and the SNGP (Liu et al., 2020) (which
form the basis for our combined model) and against the recently proposed Posterior Network model
(Charpentier et al., 2020), which also offers distance-aware uncertainties, similarly to the SNGP. We
used the same backbone neural network architecture for all models, which was a fully-connected
ResNet for the synthetic data, a WideResNet18 on CIFAR and a ResNet50 in Imagenet.
For most baselines, We used the hyperparameters from the uncertainty-baselines library
(Nado et al., 2021). On CIFAR, we trained our HetSNGP with a learning rate of 0.1 for 300 epochs
and used R = 6 factors for the heteroscedastic covariance, a softmax temperature of τ = 0.5 and
S = 5000 Monte Carlo samples. On Imagenet, We trained With a learning rate of 0.07 for 270
epochs and used R = 15 factors, a softmax temperature of τ = 1.25 and S = 5000 Monte Carlo
samples. We implemented all models in TensorFloW in Python and trained on Tensor Processing
Units (TPUs) in the Google Cloud.
14
Under review as a conference paper at ICLR 2022
We train all Imagnet-21k models for 90 epochs with batch size 1024 on 8 × 8 TPU slices. We
train using the Adam optimizer with initial learning rate of 0.001 using a linear learning rate decay
schedule with termination point 0.00001 and a warm-up period of 10,000 steps. We train using the
sigmoid cross-entropy loss function and L2 weight decay with multiplier 0.03. The heteroscedastic
method uses a temperature of 0.4, 1,000 Monte Carlo samples and R = 50 for the low rank approx-
imation. HetSNGP has the same heteroscedastic hyperparameters except the optimal temperature
is 1.5. For SNGP and HetSNGP the GP covariance is approximated using the momentum scheme
presented in Liu et al. (2020) with momentum parameter 0.999.
A.3 Laplace approximation
In this section, we will derive the Laplace posterior in Eq. (5). The derivation follows mostly from
the sections 3.4 and 3.5 in Rasmussen & Williams (2006).
First note that the log posterior of βc given the data is
logp(βc | x,y) = logp(y | βc) + logp(βc) - Z	(7)
where Z is a normalization constant that does not depend on βc . Following Rasmussen & Williams
(2006), we will denote the unnormalized log posterior as
Ψ(βc) = log p(y | βc) + log p(βc)	(8)
Recall that the first term is the likelihood and the second term is our prior from Eq. (4).
The Laplace approximation now approximates the posterior with a local second-order expansion
around the MAP solution, that is
p(βc I χ,y) ≈ N(βc, Λ-1)	(9)
with the MAP solution βc = arg maxQ。Ψ(βc) and the Hessian Λ = -V2Ψ(βc)以=§ .
The MAP solution can be found using standard (stochastic) gradient descent, while the Hessian is
given by
V2Ψ(βc) = V2 log p(y | βc) + V2 logp(βc)
= Vβ (Vu logp(y | u)Vβu) - Im
= Vβ (Vu logp(y | u)Φ) - Im
= Φ>V2ulogp(y | u)Φ - Im
= -W Φ>Φ - Im
where we used the chain rule and the fact that u = Φβ and W is a diagonal matrix of point-wise
second derivatives of the likelihood, that is, Wii = -V2 log p(yi | ui) (Rasmussen & Williams,
2006). For instance, in the case of the logistic likelihood, Wii = pi (1 - pi), where pi is a vector of
output probabilities for logits ui . To get the Hessian at the MAP, we then just need to compute this
quantity for U = Φβ.
The approximate posterior is therefore
p(βc I x,y) ≈N(βc, (WΦTΦ + Im)T)	(10)
where the precision matrix can be computed over data points (recovering Eq. (5)) as
N
Λ = Im + Xpi(1 - pi)ΦiΦi>	(11)
i=1
A.4 Additional experimental results
We add additional baselines for our Imagenet experiment from the epistemic uncertainty literature.
In particular we benchmark MC Dropout (Gal & Ghahramani, 2016) and MIMO (Havasi et al.,
2020). We use the implementations and Imagenet hyperparameters from the Uncertainty Baselines
codebase (Nado et al., 2021).
15
Under review as a conference paper at ICLR 2022
Table 5: Additional Imagenet OOD results on Imagenet-R and Imagenet-V2. The reported values
are means and standard errors over 10 runs. Bold numbers are within one standard error of the best
performing model. Our model outperforms the baselines in terms of accuracy on Imagenet-V2.
Method	↑ImR Acc	UmR NLL	UmR ECE	↑ImV2 Acc	]ImV2 NLL	]ImV2 ECE
Det.	0.229 ± 0.001	5.907 ± 0.014	0.239 ± 0.00i	0.638 ± 0.001	1.598 ± 0.003	0.077 ± 0.001
Het.	0.235 ± 0.001	5.761 ± 0.010	0.251 ± 0.001	0.648 ± 0.001	1.581 ± 0.002	0.084 ± 0.001
SNGP	0.230 ± 0.001	5.344 ± 0.009	0.175 ± 0.001	0.637 ± 0.001	1.552 ± 0.001	0.041 ± 0.001
HetSNGP	0.232 ± 0.001	5.452 ± 0.011	0.225 ± 0.002	0.647 ± 0.001	1.564 ± 0.003	0.080 ± 0.001
Table 6: Additional Imagenet baselines for Imagenet (in-dist), Imagenet-C and Imagenet-A. The
reported values are means and standard errors over 10 runs.
Method	↑ID Acc	UD NLL	UD ECE	↑ImC Acc	JImC NLL	JImC ECE	↑ImA Acc	JImA NLL	JImA ECE
MC Dropout	0.763 ± 0.001	0.930 ± 0.003	0.025 ± 0.001	0.436 ± 0.002	2.876 ± 0.016	0.045 ± 0.001	0.005 ± 0.001	7.620 ± 0.027	0.344 ± 0.001
MIMO	0.772 ± 0.001	0.901 ± 0.004	0.039 ± 0.001	0.440 ± 0.003	2.979 ± 0.003	0.101 ± 0.005	0.013 ± 0.001	7.777 ± 0.042	0.432 ± 0.005
Table 7: Additional Imagenet baselines for Imagenet-R and Imagenet-V2. The reported values are
means and standard errors over 10 runs.
Method	↑ImR Acc	JImR NLL	JImR ECE	↑ImV2 Acc	JImV2 NLL	JImV2 ECE
MC Dropout	0.244 ± 0.002	5.546 ± 0.023	0.157 ± 0.002	0.641 ± 0.001	1.543 ± 0.006	0.020 ± 0.001
MIMO	0.245 ± 0.003	5.851 ± 0.039	0.248 ± 0.004	0.654 ± 0.003	1.538 ± 0.010	0.085 ± 0.003
Table 8: Runtimes of inference per sample for different methods. Runtimes are computed on Ima-
geNet using Resnet50.
Method	RUNTIME (MS PER EXAMPLE)
Deterministic	0.04
SNGP	0.047
MIMO	0.040
MC Dropout	0.405
Het. (5000 MC samples)	0.061
HetSNGP (100 MC samples)	0.045
Table 9: Comparison to related work.
	Data uncertainty	Model uncertainty		Distance aware	SCALING
		Ensemble (size=M)	approx. Bayesian inference		
Deterministic	×	×	×	×	X
Deep ensemble	×	X	×	×	O(M)
SNGP	×	×	X	X	X
MC Dropout	×	X	X	×	O(M)
MIMO	×	X	×	×	X
Posterior Network	X	×	X	X	# CLASSES
Het.	X	×	×	×	X
HetSNGP	X	×	X	X	X
HetSNGP Ensemble	X	X	X	X	O(M)
HetSNGP + MIMO	X	X	X	X	X
16