Under review as a conference paper at ICLR 2022
Counting Substructures with Higher-Order
Graph Neural Networks: Possibility and Im-
possibility Results
Anonymous authors
Paper under double-blind review
Ab stract
While message passing Graph Neural Networks (GNNs) have become increas-
ingly popular architectures for learning with graphs, recent works have revealed
important shortcomings in their expressive power. In response, several higher-
order GNNs have been proposed that substantially increase the expressive power,
albeit at a large computational cost. Motivated by this gap, we explore alternative
strategies and lower bounds. In particular, we analyze a new recursive pooling
technique of local neighborhoods that allows different tradeoffs of computational
cost and expressive power. First, we prove that this model can count subgraphs of
size k, and thereby overcomes a known limitation of low-order GNNs. Second,
we show how recursive pooling can exploit sparsity to reduce the computational
complexity compared to the existing higher-order GNNs. More generally, we pro-
vide a (near) matching information-theoretic lower bound for counting subgraphs
with graph representations that pool over representations of derived (sub-)graphs.
We also discuss lower bounds on time complexity.
1	Introduction
Graph Neural Networks (GNNs) are powerful tools for graph representation learning (Scarselli et al.,
2008; Kipf & Welling, 2017; Hamilton et al., 2017), and have been successfully applied to molecule
property prediction, simulating physics, social network analysis, knowledge graphs, traffic predic-
tion and many other domains (Duvenaud et al., 2015; Defferrard et al., 2016; Battaglia et al., 2016;
Jin et al., 2018). The perhaps most widely used class of GNNs, Message Passing Graph Neural Net-
works (MPNNs) (Gilmer et al., 2017; Kipf & Welling, 2017; Hamilton et al., 2017; Xu et al., 2019;
Scarselli et al., 2008), follow an iterative message passing scheme to compute a graph representation.
Despite the empirical success of MPNNs, their expressive power has been shown to be limited. For
example, their discriminative power, at best, corresponds to the one-dimensional Weisfeiler-Leman
(1-WL) graph isomorphism test (Xu et al., 2019; Morris et al., 2019), so they cannot distinguish
regular graphs, for instance. Likewise, they cannot count any induced subgraph with at least three
vertices (Chen et al., 2020), or learn structural graph parameters such as clique information, diame-
ter, conjoint or shortest cycle (Garg et al., 2020). Yet, in applications like computational chemistry,
materials design or pharmacy (Elton et al., 2019; Sun et al., 2020; Jin et al., 2018), the functions we
aim to learn often depend on the presence or count of specific substructures, e.g., functional groups.
The limitations of MPNNs result from their inability to distinguish individual nodes. To resolve this
issue, two routes have been studied: (1) using unique node identifiers (Loukas, 2019; Sato et al.,
2019; Abboud et al., 2021), and (2) higher-order GNNs that act on k-tuples of nodes. Node IDs, if
available, enable Turing completeness for sufficiently large MPNNs (Loukas, 2019). Higher-order
networks use an encoding of k-tuples and then apply message passing (Morris et al., 2019), or
equivariant tensor operations (Maron et al., 2018).
The expressive power of MPNNs is often measured in terms of a hierarchy of graph isomorphism
tests, specifically the k-Weisfeiler-Leman (k-WL) hierarchy. The k-order models in (Maron et al.,
2018) and (Maron et al., 2019a) are equivalent to the k-WL and (k + 1)-WL “tests”, respectively, and
are universal for the corresponding function classes (Azizian & Lelarge, 2020; Maron et al., 2019b;
Keriven & Peyre, 2019). Yet, these models are computationally expensive, operating on Θ(nk)
1
Under review as a conference paper at ICLR 2022
tuples and according to current upper bounds requiring up to O(nk) iterations (Kiefer & McKay,
2020). The necessary tradeoffs between expressive power and computational complexity are still
an open question. However, for specific classes of tasks this full universality may not be needed.
Here, we study such an example of practical interest: counting substructures, as proposed in (Chen
et al., 2020). In particular, we study if it is possible to count given substructures with a GNN whose
complexity is between that of MPNNs and existing higher-order GNNs.
To this end, we study a generic scheme followed by many GNN architectures, including MPNNs
and higher-order GNNs (Morris et al., 2019; Chen et al., 2020): select a collection of subgraphs
of the input graph, encode these, and apply an aggregation function on this collection. First, we
study the power of pooling by itself, as a multi-set function over node features. We prove that k
recursive applications on each node’s neighborhood allow to count any substructure of size k. This
is in contrast to iterative MPNNs. We call this technique Recursive neighborhood pooling (RNP).
While subgraph pooling relates to the graph reconstruction conjecture, our strategy has important
differences. In particular, we show how the aggregation “augments” local encodings, if they play
together and the subgraphs are selected appropriately, and this reasoning may be of interest for
the design of other, even partially, expressive architectures. Moreover, our results show that the
complexity is adjustable to the counting task of interest and the sparsity of the graph.
The strategy of pooling subgraph encodings has previously been used for counting in Local Rela-
tional Pooling (LRP) (Chen et al., 2020). LRP relies on an isomorphic encoding of subgraphs, which
is expensive - e.g.,the relational pooling it uses requires O(k!) time for a subgraph of size k. Other
higher-order GNNs would be expensive, too, as high orders are needed for complete isomorphism
power. A major difference to our RNP is that our recursion uses subgraphs of varying sizes and
structures, many of them much smaller - adapted to the graph structure and specific counting task.
Furthermore, we study lower bounds on GNNs that count motifs. We show an information-theoretic
lower bound on the number of subgraphs to encode, as a function of an encoding complexity. We
also transfer computational lower bounds that apply to any counting GNN. The lower bounds show
that the recursive pooling is close to tight.
In short, in this paper, we make the following contributions:
•	We study the power of pooling encodings of subgraphs, and show that pooling, as an injective
multi-set function, is sufficient by itself for counting when applied recursively on appropriate
subgraphs, remarkably without relying on other encoding techniques or node IDs. This is different
from any other strategy we are aware of in the literature.
•	We analyze the complexity of recursive pooling, as a function of the task and input graph.
•	We provide complexity lower bounds for pooling and general GNN architectures that count motifs.
For instance, we show a lower bound on the number of subgraphs that need to be encoded.
2	Background
Message Passing Graph Neural Networks. Let G = (V , E , X ) be an attributed graph with
|V | = n nodes. Here, Xv ∈ X denotes the initial attribute of v ∈ V , where X ⊆ N is a (countable)
domain.A typical Message Passing Graph Neural Network (MPNN) first computes a representation
of each node, and then aggregates the node representations via a readout function into a representa-
tion of the entire graph G. The representation h(vi) of each node v ∈ V is computed iteratively by
aggregating the representations h(ui-1) of the neighboring vertices u:
m(vi) = AGGREGATE(i) {h(ui-1) : u ∈ N(v) }, h(vi) = COMBINE(i) h(vi-1),m(vi),	(1)
for any v ∈ V, for k iterations, and with h(v0) = Xv . The AGGREGATE/COMBINE functions are
parametrized, and {. } denotes a multi-set, i.e., a set with (possibly) repeating elements. A graph-
level representation can be computed as hg = READOUT({h(vk) : V ∈ V⅛), where Readout
is a learnable aggregation function. For representational power, it is important that the learnable
functions are injective (Xu et al., 2019).
Higher-Order GNNs. To increase the representational power of GNNs, several higher-order
GNNs have been proposed. In k-GNN, message passing is applied to k-tuples of nodes, inspired
2
Under review as a conference paper at ICLR 2022
by k-WL (Morris et al., 2019). At initialization, each k-tuple is labeled such that two k-tuples are
labeled differently if their induced subgraphs have different isomorphism types (Maron et al., 2019a;
Cai et al., 1992). As a result, k-GNNs can count (induced) substructures with at most k vertices even
at initialization. Another class of higher-order networks applies (linear) equivariant operations, in-
terleaved with coordinate-wise nonlinearities, to order-k tensors consisting of the adjacency matrix
and input node attributes (Maron et al., 2018; 2019a;b). These GNNs are at least as powerful as
k-GNNs, and hence they too can count substructures with at most k vertices. All these methods
need Ω(nk) operations. Local Relational Pooling (LRP) (Chen et al., 2020) was designed for CoUnt-
ing and applies relational pooling (Murphy et al., 2019b;a) on local neighborhoods, i.e., one pools
over evalUations of a permUtation-sensitive fUnction applied to all k! permUtations of the nodes in a
k-size neighborhood of each node.
3	Other Related Works
Expressive power. Several other works have stUdied the expressive power of GNNs as fUnction
approximators (Azizian & Lelarge, 2020). Scarselli et al. (2009) extend Universal approximation
from feedforward networks to MPNNs, Using the notion of unfolding equivalence, i.e., fUnctions on
compUtation trees. Indeed, graph distinction and fUnction approximation are closely related (Chen
et al., 2019; Azizian & Lelarge, 2020; Keriven & Peyre, 2019). Maron et al. (2019b) and Keriven &
Peyre (2019) show that higher-order, tensor-based GNNs ProvabIy achieve universal approximation
of permUtation-invariant fUnctions on graphs, and LoUkas (2019) analyzes expressive power Under
depth and width restrictions. Studying GNNs from the perspective of local algorithms, Sato et al.
(2019) show that GNNs can approximate solutions to certain combinatorial optimization problems.
For counting substricutres, Arvind et al. (2020); FUrer (2017) show that 2-WL, which is equivalent
to MPNNs, can count not necessarily induced cycles and paths of up to 7 vertices with O(n2)
complexity,
Subgraphs and GNNs. Having infromation about subgraphs can be quite helpful in various graph
representation algorithms (Liu et al., 2019; Monti et al., 2018; Liu et al., 2020; Yu et al., 2020; Meng
et al., 2018; Cotta et al., 2020; Alsentzer et al., 2020; Huang & Zitnik, 2020). For example, for graph
comparison (i.e., testing whether a given (possibly large) subgraph exists in the given model), Ying
et al. (2020) compare the outputs of GNNs for small subgraphs of the two graphs. To improve the
expressive power of GNNs, Bouritsas et al. (2020) use features that are counts of specific subgraphs
of interest. Another example is (Vignac et al., 2020), where an MPNN is strengthened by learning
local context matrices around vertices. Recent works have also developed GNNs that pass messages
on ego-nets (You et al., 2021; Sandfelder et al., 2021). With motivation from the reconstruction
conjecture, Cotta et al. (2021) process node-deleted subgraphs with individual MPNNs, and then
pool them with a DeepSets model to get a representation of the original graph.
4	Recursive Neighborhood Pooling
Let G = (V , E , X ) be an attributed input graph with |V | = n nodes, and let h(v0) = Xv be the initial
representation of each node v. In this work, we study architectures that first find representations of a
collection of m subgraphs Gi and then aggregate (pool) over these representations with a multi-set
function, i.e.,
AGGREGATE( {ψ(Gi) : i ∈ [m] }), [m] = {1, . . . ,m}.	(2)
It is clear that if the ψ can count a subgraph structure H , then the entire model can count H . In
particular, we aim to apply this strategy to obtain the representations ψ, too. To appreciate the
challenges in doing so, recall two such examples. First, MPNNs follow this pooling strategy, by
iteratively aggregating over node neighborhoods, and then aggregating all node representations into
a graph representation. However, it is known that MPNNs can count at most star structures or edges
(Arvind et al., 2020). This is because they represent a local computation tree, which loses structural
information about node identities and connectivity. Second, this strategy is at the heart of the Graph
Reconstruction Conjecture (Kelly, 1957), which conjectures that a graph G can be reconstructed
from its subgraphs {Gv = G \ {v} : v ∈ V (G) } (Appendix E). This, however, is unknown for
general graphs. Although the Gv retain some structure, we lose information about their structural
relationship. In summary, encoding structural information is the key question.
3
Under review as a conference paper at ICLR 2022
N2(v)
(N2(v) \ {v}) ∩ N2(u1)
Figure 1: Illustration of a Recursive Neigh-
borhood Pooling GNN (RNP-GNN) with re-
cursion parameters (2, 2, 1). To compute the
representation of node v in the given input
graph (depicted in the top left of the figure),
we first recurse on G(N2 (v) \ {v}) (top right
of figure). To do so, we find the represen-
tation of each node u ∈ G(N2(v) \ {v}).
For instance, to compute the representation
of u1 , we apply an RNP-GNN with recursion
parameters (2, 1) and aggregate G((N2 (v) \
{v}) ∩ (N2(u1) \ {u1})), which is shown in
the bottom left of the figure. To do so, we
recursively apply an RNP-GNN with recur-
sion parameter (1) on G((N2 (v) \ {v}) ∩
(N2(u1) \ {u1}) ∩ (N1(u11) \ {u11})), in
the bottom right of the figure.
Hence, to represent the counting function ψ over potentially large neighborhoods, an MPNN does
not suffice. But, aggregation over input node attributes, along with edge information, can count edge
types, i.e., tiny subgraphs. Hence, we recursively apply aggregation on smaller sub-neighborhoods
while remembering structural information, with node-wise aggregation as the base case. For intu-
ition, suppose we aim to count the occurrence of subgraph H in the r1-neighborhood Nr1 (v) of
a node v. To do so, we may count Hv = H \ {v} in the smaller graph Nr1 (v) \ {v}. But, to
combine these counts with the presence of v to “complete” H, we must know how the Hv are con-
nected to v in the screened graph. Hence, to retain structure information, we mark the neighbors of
v. We then recursively call neighborhood pooling to process smaller neighborhoods Nr2 (u) within
Nr1 (v) \ {v}. This could, e.g., learn to count marked versions of Hv. The radii r of neighborhoods
may differ in recursive calls. In Section 5, we relate their size to H.
Recursive neighborhood pooling RNP-GNN(G, {hiun}u∈V(G), (r1, . . . , rτ)) takes an attributed
graph along with a sequence of neighborhood radii for different recursions, and returns a set of
node encodings {hv}v∈V(G). For any v ∈ G, RNP-GNN first constructs v’s neighborhood, removes
v and marks its neighbors1:
Gv - Nri (v) \ {v},	hUn,aug =州,1[(u, V) ∈ E(Gv)]).	⑶
Here, we extracted r1 -neighborhood of v , removed the central node v, and then added the new
structural information to 1-neighbors of v. Then we aggregate over subgraph representations, i.e.,
the representations of Gv , v ∈ [n] when they considered as a set graphs. If τ = 1 (base case), we
use the input features:
hv 一 AGGREGATE(T)(hv1, {hUn,aug : U ∈ Gv»).	(4)
In other words, in this case we only combine hidden representations in each Gv without any itera-
tion/recursion. If τ > 1, we recursively represent neighborhoods of nodes in Gv:
{hv,u}u∈G0 J RNP-GNN(Gv, {hUn,aug}u∈Gν ,(『2, r3, . . . , r ))	(5)
hv J AGGREGATE(T) (hv1, {hu,v : U ∈ Gv ») .	(6)
Here, we first apply an RNP-GNN on each Gv to update the representations of nodes for each node
in Gv . Then, we combine those representations to find the representation of v. The fact that Gv has
less nodes than G allows to define the algorithm in an inductive way; this ensures the algorithm does
not have any logical loop. For aggregation, we can use, e.g., the injective multi-set function from
(Xu et al., 2019):
AGGREGATE(T) (hv, {hu}u∈Gν) = MLP(T) ((1 + e)hv + XUeG 嬴)，	(7)
11[.] is one when the condition is satisfied, and otherwise is zero.
4
Under review as a conference paper at ICLR 2022
where we use MLP modules with ReLU activation, and is an arbitrary irrational constant (Xu et al.,
2019). The final readout aggregates over the final node representations of the entire graph. Figure 1
illustrates an RNP-GNN with recursion parameters (2, 2, 1), and Appendix F provides pseudocode.
One can optionally do message passing iterations to the representations hv , v ∈ [n] to make the
model more expressive; though our results hold even without those iterations.
While MPNNs also encode a representation of a local neighborhood, the recursive representations
differ as they take into account intersections of neighborhoods. As a result, as we will see in Sec-
tion 5, they retain more structural information and are more expressive than MPNNs; one can take
r1 = 1 to simulate one iteration of MPNNs (and then iterate).
Models like k-GNN and LRP also compute encodings of subgraphs, and then update the resulting
representations via message passing. We can do the same with the neighborhood representations
computed by RNP-GNNs to encode more global information, although our representation results in
Section 5 hold even without that.
5	Expressive Power of Recursive Pooling
In this section, we analyze the expressive power of RNP-GNNs. We notice that for full expres-
siveness, we indeed need the identifiers in Eq. (3); their addition is an important insight. Still,
RNP-GNN does maintain some expressive power without the augmented identifiers. For instance,
consider the graphs G1 = two triangles and G2 = the 6-cycle, which are 1-WL equivalent and thus
cannot be distinguished by MPNNs. An RNP-GNN with r1 = 1 can distinguish these without the
augmented identifiers. This is because on the first recursion level, the 1-neighborhoods of G1 are
two nodes with an edge between them, and the 1-neighborhoods of G2 consist of two isolated nodes.
5.1	Counting (Induced) Substructures
In contrast to MPNNs, which in general cannot count substructures of three vertices or more (Chen
et al., 2020), in this section we prove that for any set of substructures, there is an RNP-GNN that
provably counts them. We begin with a few definitions.
Definition 1. Let G, H be arbitrary, potentially attributed simple graphs, where V is the set of nodes
in G. Also, for any S ⊆ V, let G(S) denote the subgraph of G induced by S. The induced subgraph
count function is defined as
C(G; H):= XS⊆V 1{G(S) = H},	(8)
i.e., the number of subgraphs of G isomorphic to H.
To relate the size of encoded neighborhoods to the substructure H, we will need a notion of covering
sequences for graphs.
Definition 2. Let H = (VH, EH) be a simple connected graph. For any S ⊆ VH and v ∈ VH,
define the covering distance of v from S as
d，H(v; S) := max d(u,v),	(9)
u∈S
where d(., .) is the shortest-path distance in H.
Definition 3. Let H be a simple connected graph on τ + 1 vertices. A permutation of vertices,
such as (v1, v2, . . . , vτ+1), is called a vertex covering sequence with respect to a sequence r =
(r1 , r2 , . . . , rτ ) ∈ Nτ called a covering sequence, if and only if
Ah0 (vi； Si) ≤ ri,	(10)
for any i ∈ [τ + 1] = {1, 2, . . . , τ + 1}, where Si = {vi, vi+1 . . . , vτ+1} and Hi0 = H(Si) is the
subgraph of H induced by the set of vertices Si. We also say that H admits the covering sequence
r = (r1, r2, . . . , rτ) ∈ Nτ if there is a vertex covering sequence for H with respect to r.
In particular, in a covering sequence we first consider the whole graph as a local neighborhood of
one of its nodes with radius r1 . Then, we remove that node and compute the covering sequence of
5
Under review as a conference paper at ICLR 2022
the remaining graph. Figure 4 shows an example of covering sequence computation. An important
property, which holds by definition, is that if r is a covering sequence for H, then any r0 ≥ r
(coordinate-wise) is also a covering sequence for H .
Note that any connected graph on k nodes admits at least one covering sequence, which is (k -
1, k - 2, . . . , 1). To observe this fact, note that in a connected graph, there is at least one node that
can be removed and the remaining graph still remains connected. Therefore, we may take this node
as the first element of a vertex covering sequence, and inductively find the other elements. Since the
diameter of a connected graph with k vertices is always bounded by k - 1, we achieve the desired
result. However, we will see in the next section that, when using covering sequences to identify
sufficiently powerful RNP-GNNs, it is desirable to have covering sequences with low r1, since the
complexity of the resulting RNP-GNN depends on r1.
More generally, if H1 and H2 are (possibly attributed) simple graphs on k nodes and H1 b H2, i.e.,
H1 is a subgraph of H2 (not necessarily induced subgraph), then it follows from the definition that
any covering sequence for H1 is also a covering sequence for H2 . As a side remark, as illustrated in
Figure 2, covering sequences need not always to be decreasing. Using covering sequences, we can
show the following result.
Theorem 1. Consider a set of (possibly attributed) graphs H on τ+1 vertices, such that any H ∈ H
admits the covering Sequence (r1,r2,..., r"). Then, there is an RNP-GNN f (∙; θ) with recursion
parameters (r1, r2, . . . , rτ) that can count any H ∈ H. In other words, for any two graphs G1, G2,
if there exists H ∈ H such that C(G1; H) 6= C(G2; H), then f(G1; θ) 6= f(G2; θ). The same result
also holds for the non-induced subgraph count function.
Theorem 1 states that, with appropriate recursion parameters, any set of (possibly attributed) sub-
structures can be counted by an RNP-GNN. Interestingly, induced and non-induced subgraphs can
be both counted in RNP-GNNs2. Also, for a given covering sequence, we can simultaneously count
a set of substructures that each admit it. This means that one RNP-GNN is able to potentially count
many substructures. We prove Theorem 1 in Appendix A.2. The main idea is to show that we
can implement the intuition for recursive pooling outlined in Section 4 formally with the proposed
architecture and multiset functions3.
The theorem holds for any covering sequence that is valid for all graphs in H. For any graph, one
can compute a covering sequence by computing a spanning tree, and sequentially pruning the leaves
of the tree. The resulting sequence of nodes is a vertex covering sequence, and the corresponding
covering sequence can be obtained from the tree too (Appendix D). A valid covering sequence for
all the graphs in H is the coordinate-wise maximum of all these sequences.
For large substructures, the sequence (r1, r2, . . . , rτ) can be long or include large numbers, and this
will affect the computational complexity of RNP-GNNs. For small, e.g., constant-size substructures,
the recursion parameters are also small (i.e., ri = O(1) for all i), raising the hope to count these
structures efficiently. In particular, r1 is an important parameter. In Section 5.3, we analyze the
complexity of RNP-GNNs in more detail.
5.2 A Universal Approximation Result for Local Functions
Theorem 1 shows that RNP-GNNs can count substructures if their recursion parameters are chosen
carefully. Next, we provide a universal approximation result, which shows that they can represent
any function related to local neighborhoods or small subgraphs in a graph.
First, we recall that for a graph G, G(S) denotes the subgraph of G induced by the set of nodes S.
Definition 4. A function ` : Gn → Rd is called an r-local graph function if
'(G) = φ(专 ψ(G(S)): S⊆V, |S| ≤ r ⅛),	(11)
where ψ : Gr → Rd0 is a function on graphs, φ is a multi-set function, and V denotes the set of all
nodes.
2For simplicity, we assume that H only contains τ + 1 node graphs. If H includes graphs with strictly less
than τ + 1 vertices, we can simply append a sufficient number of zeros to their covering sequences.
3One can also generalize this theorem to wider classes of graphs; see Remark 1. However, here in this paper
we focus on a special class of graph with covering sequences to keep the results simple and insightful
6
Under review as a conference paper at ICLR 2022
In other words, a local function only depends on small substructures.
Theorem 2. For any r-local graph function `(.), there exists an RNP-GNN f(.; θ) with recursion
parameters (r 一 1, r 一 2,..., 1) such that f(G; θ) = '(G) for any G ∈ Gn.
To prove the theorem, we use specific aggregation functions, but since MLPs with ReLU activation
are universal approximators, we can approximate those aggregations and find an RNP-GNN f(G; θ)
implemented by MLPs such that |f (G; θ) 一 '(G) | < e, for arbitrary small e. As a result, We can
provably approximate all the local information in a graph with an appropriate RNP-GNN. Note that
We still need recursions, because the function ψ(.) may be an arbitrarily difficult graph function.
HoWever, to achieve the full generality of such a universal approximation result, We need to consider
large recursion parameters (r1 = r 一 1) and injective aggregations in the RNP-GNN netWork. For
universal approximation, We may also need high dimensions if fully connected layers are used for
aggregation (see the proof in Appendix B for more details).
As a remark, for r = n, achieving universal approximation on graphs implies solving the graph
isomorphism problem. But, in this extreme case, the computational complexity of RNP is in general
not polynomial in n.
5.3 Computational Complexity
The computational complexity of RNP-GNNs is graph-
dependent. For instance, We need to compute the set of
local neighborhoods, Which is cheaper for sparse graphs.
Moreover, in the recursions, We use intersections of
neighborhoods Which become smaller and sparser.
Theorem 3.	Let f(.; θ) : Gn → Rd be an RNP-
GNN with recursion parameters (r1, r2, . . . , rτ). As-
sume that the observed graphs G1, G2, . . ., whose rep-
resentations we compute, satisfy the following property:
maxv∈[n] |Nr1 (v)| ≤ c, for a constant c. Then the num-
ber of node updates in the RNP-GNN is O (ncτ).
In other Words, if c = no(1) and τ = O(1), then RNP-
GNN requires relatively feW updates (that is, n1+o(1)).
Figure 2: For the above graph,
(v1 , v2 , . . . , v6 ) is a vertex covering se-
quence. The corresponding covering se-
quence (1, 4, 3, 2, 1) is not decreasing.
If the maximum degree of the given graphs is ∆, then c = O(r1 ∆r1 ). Therefore, similarly, if
∆ = no(1) then We can count With at most n1+o(1) updates. Additional gains may arise from rapidly
shrinking neighborhoods, Which are not yet accounted for in Theorem 3. To put this in context, the
higher-order GNNs based on tensors or k-WL Would operate on tensors of order nτ+1.
The above results shoW that When us- ing RNP-GNNs With sparse graphs,	Table 1:	Time complexity of various models.		∆ is the max-
We can represent functions of sub-	degree, and ’-’ means the complexity is not polynomial in n.			
structures With k nodes Without re- quiring k-order tensors. LRPs also	Model	Worst-case ∆ = no(1)	∆	O(log(n))	∆=O(1)
	LRP	一一	一	O(n)
encode neighborhoods of distance r1	k-WL	nk	nk	nk	nk
around nodes. In particular, all c! per- mutations of the nodes in a neighbor-	RNP	nk	n1+o(1)	~ ,. O(n)	O(n)
				
hood of size c are considered to ob-				
tain the representation. As a result, LRP netWorks only have polynomial complexity if c =
o(log(n)). Thus, RNP-GNNs can provide an exponential improvement in terms of the tolerable
size c of neighborhoods With distance r1 in the graph.
Moreover, Theorem 3 suggests to aim for small r1. The other ri’s may be larger than r1, as shoWn
in Figure 2, but do not affect the upper bound on the complexity.
6 An Information-Theoretic Lower B ound
In this section, We provide a general information-theoretic loWer bound for graph representations
that encode a given graph G by first encoding a number of (possibly small) graphs G1, G2, . . . , Gt
7
Under review as a conference paper at ICLR 2022
and then aggregating the resulting representations. The sequence of graphs G1, G2, . . . , Gt may be
obtained in an arbitrary way from G. For example, in an MPNN, Gi can be the computation tree
(rooted tree) at node i. As another example, in LRP, Gi is the local neighborhood around node i.
Formally, consider a graph representation f (.; θ) : Gn → Rd as
f(G; θ) = AGGREGATE({ ψ(Gi): i ∈ [t] ⅛),	[t] = {1,...,t}	(12)
for any G ∈ Gn, where AGGREGATE is a multi-set function, (G1, G2, . . . , Gt) = Ξ(G) where
Ξ(.) : Gn → 4 Sm∞=1 Gm t is a function from one graph to t graphs, and ψ : Sm∞=1 Gm → [s] is a
function on graphs taking s values. In short, we encode t graphs, and each encoding takes one of s
values. We call this graph representation function an (s, t)-good graph representation.
Theorem 4.	Consider a parametrized class of (s, t)-good representations f(.; θ) : Gn → Rd
that is able to count any (not necessarily induced4) substructure with k vertices. More precisely,
for any graph H with k vertices, there exists f(.; θ) such that if C(G1; H) 6= C(G2; H), then
k
f (Gi； θ) = f (G2； θ)∙ Then5 t = Ω(ns-1).
In particular, for any (s, t)-good graph representation with s = 2, i.e., binary encoding functions,
We need Ω(nk) encoded graphs. This implies that, for S = 2, enumerating all subgraphs and
deciding for each whether it equals H is near optimal. Moreover, if s ≤ k, then t = Θ(n) small
graphs Would not suffice to enable counting.
More interestingly, if k, s = O(1), then it is impossible to perform the substructure counting task
With t = O(log(n)). As a result, in this case, considering n encoded graphs (as is done in GNNs or
LRP netWorks) cannot be exponentially improved.
The loWer bound in this section is information-theoretic and hence applies to any algorithm. It may
be possible to strengthen itby considering computational complexity, too. For binary encodings, i.e.,
s = 2, hoWever, We knoW that the bound cannot be improved since manual counting of subgraphs
matches the loWer bound.
7	Time Complexity Lower Bounds for Counting S ub graphs
In this section, We put our results in the context of knoWn hardness results for subgraph counting.
In general, the subgraph isomorphism problem is knoWn to be NP-complete. Going further, the
Exponential Time Hypothesis (ETH) is a conjecture in complexity theory (Impagliazzo & Paturi,
2001), and states that several NP-complete problems cannot be solved in sub-exponential time.
ETH, as a stronger version of the P 6= NP problem, is Widely believed to hold. Assuming that
ETH holds, the k-clique detection problem requires at least nQ(k) time (Chen et al., 2005). This
means that if a graph representation can count any subgraph H of size k, then computing it requires
at least nQ(k) time.
Corollary 1. Assuming the ETH conjecture holds, any graph representation that can count any
substructure H on k vertices with appropriate parametrization needs nQ(k) time to compute.
The above bound matches the O(nk) complexity of the higher-order GNNs. Comparing With The-
orem 4 above, Corollary 1 is more general, While Theorem 4 has feWer assumptions and offers a
refined result for aggregation-based graph representations.
Given that Corollary 1 is a worst-case bound, a natural question is Whether We can do better for
subclasses of graphs. Regarding H, even if H is a random ErdoS-Renyi graph, it can only be
counted in nQ(k/ log k) time (Dalirrooyfard et al., 2019).
Regarding the input graph in Which We count, consider tWo classes of sparse graphs: strongly sparse
graphs have maximum degree ∆ = O(1), and weakly sparse graphs have average degree ∆ = O(1).
We argued in Theorem 3 that RNP-GNNs achieve almost linear complexity for the class of strongly
sparse graphs. For Weakly sparse graphs, in contrast, the complexity of RNP-GNNs is generally not
linear, but still polynomial, and can be much better than O(nk). One may ask Whether it is possible
4The theorem also holds for induced subgraphs, With/Without node attributes.
5Ω(m) is Ω(m) up to poly-logarithmic factors.
8
Under review as a conference paper at ICLR 2022
to achieve a learnable graph representation such that its complexity for weakly sparse graphs is still
linear. Recent results in complexity theory imply that this is impossible:
Corollary 2 (Gishboliner et al. (2020); Bera et al. (2019; 2020)). There is no graph representation
algorithm that runs in linear time on weakly sparse graphs and is able to count any substructure H
on k vertices (with appropriate parametrization).
Hence, RNP-GNNs are close to optimal for several cases of counting substructures with
parametrized learnable functions.
8 Experiments
In this section, we validate our theoretical findings via numerical experiments. Here, we briefly
describe our experimental setup and results — further experimental details are given in Appendix H.
Table 2: Numerical results for counting induced triangles and non-induced 3-stars, following the setup of Chen et al. (2020). We report the test MSE divided by variance of the true counts of each substructure (lower is better). The best three models for each task are bolded.					Table 3: Test accuracy on the EXP dataset with setup as in Abboud et al. (2021). Re- sults for baselines taken from Abboud et al. (2021). * Reported PPGN performance dif- fers in other work (Balcilar et al., 2021). Model	Accuracy (%)
	Erdos-Renyi		Random Regular		
	triangle	3-star	triangle	3-star	
GCN	6.78E-1	4.36E-1	1.82	2.63	GCN-RNI	98.0 ± 1.85
GIN	1.23E-1	1.62E-4	4.70E-1	3.73E-4	PPGN*	50.0
GraphSAGE	1.31E-1	2.40E-10	3.62E-1	8.70E-8	1-2-3-GCN-L	50.0
sGNN	9.25E-2	2.36E-3	3.92E-1	2.37E-2	3-GCN	99.7 ± 0.004
2-IGN	9.83E-2	5.40E-4	2.62E-1	1.19E-2	RNP-GNN (r1 = 1)	50.0
PPGN	5.08E-8	4.00E-5	1.40E-6	8.49E-5	RNP-GNN (r1 = 2)	99.8 ± 0.005
LRP-1-3	1.56E-4	2.17E-5	2.47E-4	1.88E-6	—
Deep LRP-1-3	2.81E-5	1.12E-5	1.30E-6	2.07E-6	
RNP-GNN	1.39E-5	1.39E-5	2.38E-6	1.50E-4	
Counting substructures. First, we follow the experimental setup of Chen et al. (2020) on tasks for
counting substructures. In Table 2, we report results for learning induced subgraph count of triangles
and non-induced subgraph count of 3-stars. We test on two datasets of 5000 graphs each: one of
Erdos-Renyi graphs and one of random regular graphs. Our RNP-GNN model is consistently within
the best performing models for these counting tasks, thus validating our theoretical results. Based on
the baseline results taken from (Chen et al., 2020), RNP-GNN tends to widely outperform MPNNs
(GCN (Kipf & Welling, 2017), GIN (Xu et al., 2019), GraphSAGE (Hamilton et al., 2017)), and
other models not tailored for counting: spectral GNN (Chen et al., 2018), and 2-IGN (Maron et al.,
2018). Also, RNP-GNN often beats higher-order GNNs: PPGN (Maron et al., 2019a) and LRP-1-
3 (Chen et al., 2020). RNP-GNN is mostly comparable to Deep LRP-1-3, though Deep LRP-1-3
outperforms it in a few cases. Recall that Deep LRP-1-3 is a practical version of LRP — we leave
further developments of practical variants of RNP-GNN to future work. The best r parameters for
RNP-GNN were: r = (1,1,1,1) for triangles on Erdos-Renyi, r = (1,1) for stars on Erdos-Renyi,
r = (1, 1) for triangles on random regular and r = (1, 1, 1) for stars on random regular.
Satisfiability of propositional formulas. Second, we test the expressiveness of our model in dis-
tinguishing non-isomorphic graphs that 1-WL cannot distinguish. The EXP dataset of 600 graphs
(Abboud et al., 2021) for classifying whether certain propositional formulas are satisfiable requires
higher than 1-WL expressive power to achieve better than random accuracy. As shown in Table 3,
while our RNP-GNN with r1 = 1 is unable to achieve better than random accuracy, our RNP-GNN
with r1 = 2 (we use r = (2, 1) achieves near perfect accuracy — beating all other models based on
results taken from (Abboud et al., 2021). These other models include universal models with random
node identifiers (GCN-RNI (Abboud et al., 2021)), GNNs with 3-WL power (PPGN (Maron et al.,
2019a)), and GNNs that imitate some (possibly weaker) version of 3-WL (1-2-3-GCN-L (Morris
et al., 2019), 3-GCN (Abboud et al., 2021)). Thus, our architecture, which is not developed within
common frameworks for achieving k-WL expressiveness, is in fact powerful at distinguishing non-
isomorphic graphs.
9
Under review as a conference paper at ICLR 2022
References
Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power
of graph neural networks with random node initialization. In Proceedings of the Thirtieth Inter-
national Joint Conference on Artificial Intelligence, IJCAI-21, pp. 2112-2118, 8 2021.
Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. Subgraph neural networks.
Advances in Neural Information Processing Systems, 33, 2020.
Vikraman Arvind, Frank FuhlbrUck, Johannes Kobler, and Oleg Verbitsky. On Weisfeiler-leman
invariance: subgraph counts and related graph properties. Journal of Computer and System Sci-
ences, 2020.
Walss Azizian and Marc Lelarge. Characterizing the expressive power of invariant and equivariant
graph neural netWorks. arXiv preprint arXiv:2006.15646, 2020.
Muhammet Balcilar, Pierre Heroux, Benoit GaUzere, Pascal Vasseur, SebaStien Adam, and Paul
Honeine. Breaking the limits of message passing graph neural networks. In Proceedings of the
38th International Conference on Machine Learning (ICML), 2021.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in neural information processing
systems, pp. 4502-4510, 2016.
Suman K Bera, Noujan Pashanasangi, and C Seshadhri. Linear time subgraph counting, graph
degeneracy, and the chasm at size six. arXiv preprint arXiv:1911.05896, 2019.
Suman K. Bera, Noujan Pashanasangi, and C. Seshadhri. Near-linear time homomorphism counting
in bounded degeneracy graphs: The barrier of long induced cycles, 2020.
Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improv-
ing graph neural network expressivity via subgraph isomorphism counting. arXiv preprint
arXiv:2006.09252, 2020.
Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identification. Combinatorica, 12(4):389-410, 1992.
Jianer Chen, Benny Chor, Mike Fellows, Xiuzhen Huang, David Juedes, Iyad A Kanj, and Ge Xia.
Tight lower bounds for certain parameterized np-hard problems. Information and Computation,
201(2):216-231, 2005.
Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural
networks. In International Conference on Learning Representations, 2018.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. In Advances in Neural Information
Processing Systems, pp. 15894-15902, 2019.
Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count
substructures? arXiv preprint arXiv:2002.04025, 2020.
Leonardo Cotta, Carlos H. C. Teixeira, Ananthram Swami, and Bruno Ribeiro. Unsupervised
joint k-node graph representations with compositional energy-based models. arXiv preprint
arXiv:2010.04259, 2020.
Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph repre-
sentations, 2021.
Mina Dalirrooyfard, Thuy Duong Vuong, and Virginia Vassilevska Williams. Graph pattern detec-
tion: Hardness for all induced patterns and faster non-induced cycles. In Proceedings of the 51st
Annual ACM SIGACT Symposium on Theory of Computing, pp. 1167-1178, 2019.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
10
Under review as a conference paper at ICLR 2022
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, RafaeI Bombarell, Timothy HirzeL Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing Systems, pp. 2224-2232, 2015.
Daniel C Elton, Zois Boukouvalas, Mark D Fuge, and Peter W Chung. Deep learning for molecular
design—a review of the state of the art. Molecular Systems Design & Engineering, 4(4):828-849,
2019.
Paul Erdos, Alfred Renyi, et al. On the evolution of random graphs. PubL Math. Inst. Hung. Acad.
Sci, 5(1):17-60, 1960.
Martin Furer. On the combinatorial power of the Weisfeiler-lehman algorithm. In International
Conference on Algorithms and Complexity, pp. 260-271. Springer, 2017.
Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. In Int. Conference on Machine Learning (ICML), pp. 5204-5215. 2020.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272, 2017.
Lior Gishboliner, Yevgeny Levanzov, and Asaf Shapira. Counting subgraphs in degenerate graphs.
arXiv preprint arXiv:2010.05998, 2020.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024-1034, 2017.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Kexin Huang and Marinka Zitnik. Graph meta learning via local subgraphs. arXiv preprint
arXiv:2006.07889, 2020.
Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of Computer and
System Sciences, 62(2):367-375, 2001.
Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-
graph translation for molecule optimization. In International Conference on Learning Represen-
tations, 2018.
Paul Kelly. A congruence theorem for trees. Pacific Journal of Mathematics, 7(1):961-968, 1957.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 7092-7101, 2019.
Sandra Kiefer and Brendan D McKay. The iteration number of colour refinement. In 47th Interna-
tional Colloquium on Automata, Languages, and Programming (ICALP 2020). Schloss Dagstuhl-
Leibniz-Zentrum fur Informatik, 2020.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations, 2017.
Jon Kleinberg and Eva Tardos. Algorithm design. Pearson Education India, 2006.
Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised repre-
sentation for graphs, with applications to molecules. In Advances in Neural Information Process-
ing Systems, pp. 8466-8478, 2019.
Xin Liu, Haojie Pan, Mutian He, Yangqiu Song, Xin Jiang, and Lifeng Shang. Neural subgraph
isomorphism counting. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 1959-1969, 2020.
11
Under review as a conference paper at ICLR 2022
Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International Con-
ference on Learning Representations, 2019.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations, 2018.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 2156-2167,
2019a.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. In International Conference on Machine Learning, pp. 4363-4371, 2019b.
Brendan D McKay. Small graphs are reconstructible. Australasian Journal of Combinatorics, 15:
123-126, 1997.
Changping Meng, S Chandra Mouli, Bruno Ribeiro, and Jennifer Neville. Subgraph pattern neural
networks for high-order graph evolution prediction. In AAAI, pp. 3778-3787, 2018.
Federico Monti, Karl Otness, and Michael M Bronstein. Motifnet: a motif-based graph convolu-
tional network for directed graphs. In 2018 IEEE Data Science Workshop (DSW), pp. 225-228.
IEEE, 2018.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In AAAI, 2019.
R Murphy, B Srinivasan, V Rao, and B Riberio. Janossy pooling: Learning deep permutation-
invariant functions for variable-size inputs. In International Conference on Learning Representa-
tions, 2019a.
R Murphy, B Srinivasan, V Rao, and B Riberio. Relational pooling for graph representations. In
International Conference on Machine Learning (ICML 2019), 2019b.
Dylan Sandfelder, Priyesh Vijayan, and William L Hamilton. Ego-gnns: Exploiting ego structures
in graph neural networks. In ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 8523-8527. IEEE, 2021.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation ratios of graph neural networks
for combinatorial problems. In Advances in Neural Information Processing Systems, pp. 4081-
4090, 2019.
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabilities
of graph neural networks. IEEE Transactions on Neural Networks, 20(1):81-102, 2009.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Angelika Steger and Nicholas C Wormald. Generating random regular graphs quickly. Combina-
torics, Probability and Computing, 8(4):377-396, 1999.
Mengying Sun, Sendong Zhao, Coryandar Gilvary, Olivier Elemento, Jiayu Zhou, and Fei Wang.
Graph convolutional networks for computational drug development and discovery. Briefings in
bioinformatics, 21(3):919-935, 2020.
Clement Vignac, Andreas Loukas, and Pascal Frossard. Building powerful and equivariant graph
neural networks with message-passing. arXiv preprint arXiv:2006.15107, 2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019.
Rex Ying, Zhaoyu Lou, Jiaxuan You, Chengtao Wen, Arquimedes Canedo, and Jure Leskovec.
Neural subgraph matching. arXiv preprint arXiv:2007.03092, 2020.
12
Under review as a conference paper at ICLR 2022
Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph
neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pp.10737-10745, 2021.
Yue Yu, Kexin Huang, Chao Zhang, Lucas M. Glass, Jimeng Sun, and Cao Xiao. Sumgnn: Multi-
typed drug interaction prediction via efficient knowledge graph summarization. arXiv preprint
arXiv:2010.01450, 2020.
A Proof of Theorem 1
A.1 Preliminaries
Let us first state a few definitions about the graph functions. Note that for any graph function
f : Gn → Rd, We have f (G) = f(H) for any G = H.
Definition 5. Given two graph functions f, g : Gn → Rd, we write f w g, if and only if for any
G1 , G2 ∈ Gn,
∀G1,G2 ∈Gn :g(G1) 6=g(G2) =⇒ f(G1) 6=f(G2),	(13)
or, equivalently,
∀G1,G2 ∈ Gn : f(G1) =f(G2) =⇒ g(G1) = g(G2).	(14)
Proposition 1. Consider graph functions f, g, h : Gn → Rd such that f w g and g w h. Then,
f w h. In other words, w is transitive.
Proof. The proposition holds by definition.	□
Proposition 2. Consider graph functions f, g : Gn → Rd such that f w g. Then, there is a function
ξ : Rd → Rd such that ξ ◦ f = g.
Proof. Let Gn = ti∈NFi be the partitioning induced by the equality relation With respect to the
function f on Gn. Similarly define Gi, i ∈ N for g. Note that due to the definition, {Fi : i ∈ N} is a
refinement for {Gi : i ∈ N}. Define ξ to be the unique mapping from {Fi : i ∈ N} to {Gi : i ∈ N}
Which respects the equality relation. One can observe that such ξ satisfies the requirement in the
proposition.	□
Definition 6. An RNP-GNN is called maximally expressive, if and only if
•	all the aggregate functions are injective as mappings from a multi-set on a countable
ground set to their codomain.
•	all the combine functions are injective mappings.
Proposition 3. Consider two RNP-GNNs f, g with the same recursion parameters r =
(r1 , r2 , . . . , rτ ) where f is maximally expressive. Then, f w g.
Proof. The proposition holds by definition.	□
Proposition 4. Consider a sequence of graph functions f, g1 , . . . , gk. Iff w gi for all i ∈ [k], then
k
fwXcigi,	(15)
i=1
for any ci ∈ R, i ∈ N.
Proof. Since f w gi, We have
∀G1,G2 ∈ Gn : f(G1) =f(G2) =⇒ gi(G1) = gi(G2),	(16)
for all i ∈ [k]. This means that for any G1, G2 ∈ Gn if f(G1) = f(G2) then gi(G1) = gi(G2),
i ∈ [k], and consequently Pik=1 cigi(G1) = Pik=1 cigi(G2). Therefore, from the definition We
conclude f w Pik=1 cigi . Note that the same proof also holds in the case of countable summations
as long as the summation is bounded.	□
13
Under review as a conference paper at ICLR 2022
Definition 7. Let H = (VH , EH , XH) be a attributed connected simple graph with k nodes. For
any attributed graph G = (VG, EG, XG) ∈ Gn, the induced subgraph count function C(G; H) is
defined as
C(G; H):= X 1{G(S) = H}.	(17)
S ⊆[n]
Also, let C(G; H) denote the number ofnon-induced subgraphs of G which are isomorphic to H. It
can be defined with the homomorphisms from H to G. Formally, if n > k define
C(G; H) := X C(G(S); H).	(18)
S⊆[n]
|S|=k
Otherwise, n = k, and we define
C(G; H) := X CH,h × 1{G = H},	(19)
H ∈HH(h)
where
H(H) := {H ∈ Gk : H C H},	(20)
is defined with respect to the graph isomorphism, and CHH ∈ N denotes the number of subgraphs
in H identical to H. Note that H(H) is a finite set and c denotes being a (not necessarily induced)
subgraph.
Proposition 5. Let H be a family of graphs. Iffor any H ∈ H, there is an RNP-GNN fH (.; θ) with
recursion parameters (r1, r2, . . . , rτ) such that fH w C(G; H), then there exists an RNP-GNN
f (.; θ) with recursion parameters (r1, r2, . . . , rτ) such that f w H∈H C(G; H).
Proof. Let f(.; θ) be a maximally expressive RNP-GNN. Note that by the definition f w fH for any
H ∈ H. Since w is transitive, f w C(G; H) for all H ∈ H, and using Proposition 4, we conclude
that f W Ph∈h C(G； H).	□
The following proposition shows that it is sufficient to address counting attributed graphs.
Proposition 6. Let H0 be an unattributed connected graph. Assume that for any attributed graph
H, which is constructed by adding arbitrary attributes to H0, there exists an RNP-GNN fH (.; θH)
such that fH w C(G; H), then for its unattributed counterpart H0, there exists anRNP-GNNf(.; θ)
with the same recursion parameters as fH (.; θH) such that f w C(G; H0).
Proof. If there exists an RNP-GNN fH (.; θH) such that fH w C(G; H), then for a maximally
expressive RNP-GNN f(.; θ) with the same recursion parameters as fH we also have f w C(G; H).
Let H be the set of all attributed graphs H = (V, E, X) ∈ Gk up to graph isomorphism, where
X ∈ Xk for a countable set X. Note that H = {H1, H2, . . .} is a countable set. Now we write
C(G;H0)=	1{G(S)==H0}	(21)
S⊆[n]
|S|=k
=XX
1{G(S) == Hi}	(22)
S⊆[n] i∈N
|S|=k
=XX1{G(S) == Hi}	(23)
i∈N S⊆[n]
|S|=k
= XC(G;Hi).	(24)
i∈N
(25)
Now using Proposition 4 We conclude that f W C(G; Ho) since C(G; Ho) is always finite. □
14
Under review as a conference paper at ICLR 2022
Definition 8. Let H be a (possibly attributed) simple connected graph. For any S ⊆ VH and
v ∈ VH, define
d，H(v; S) := max d(u,v).	(26)
u∈S
Definition 9. Let H be a (possibly attributed) connected simple graph with k = τ + 1 vertices.
A permutation of vertices, such as (v1, v2, . . . , vτ+1), is called a vertex covering sequence, with
respect to a sequence r = (r1, r2, . . . , rτ) ∈ Nτ, called a covering sequence, if and only if
Ah0 (vi； Si) ≤ ri,	(27)
for i ∈ [τ + 1], where Hi0 = H(Si) and Si = {vi, vi+1, . . . , vτ+1}. Let CH (r) denote the set of all
vertex covering sequences with respect to the covering sequence r for H.
Proposition 7. For any G, H ∈ Gk, if G c H (non-induced subgraph), then
CH(r) ⊆ CG(r),	(28)
for any sequence r.
Proof. The proposition follows from the fact that the function d is decreasing with introducing new
edges.	□
Proposition 8. Assume that Theorem 1 holds for induced-subgraph count functions. Then, it also
holds for the non-induced subgraph count functions.
Proof. Assume that for a connected (attributed or unattributed) graph H , there exists an RNP-GNN
with appropriate recursion parameters fH (.; θH) such that fH w C(G; H), then we prove there
exists an RNP-GNN f (.; θ) with the same recursion parameters as /h such that f W C(G; H).
If there exists an RNP-GNN fH (.; θH) such that fH w C (G; H), then for a maximally expressive
RNP-GNN f(.; θ) with the same recursion parameters as fH we also have f w C (G; H). Note that
C(G,H)= E C(G(S); H)	(29)
S⊆[n]
|S|=k
=XX ch,h × 1{G(S) = H}	(30)
SS⊆=n]H ∈H(H)
=X CHH X 1{G(S) = H}	(31)
H ∈H(H)	iS⊆=k]
= XcHi,H × C(G,Hi),	(32)
i∈N
where H(H) = {H1,H2,...}.
Claim 1. f w C(G, Hi) for any i.
Using Proposition 4 and Claim 1 we conclude that f W C(G; H) since C(G; H) is finite and
f w C(G, Hi) for any i, and the proof is complete. The missing part which we must show here
is that for any Hi the sequence (r1, r2, . . . , rt) which covers H also covers Hi. This follows from
Proposition 7. We are done.	□
At the end of this part, let us introduce an important notation. For any attributed connected simple
graph on k vertices G = (V,E,X), let Gv be the resulting induced graph obtained after removing
v ∈ V from G with the new attributes defined as
Xu := (Xu, 1{(u,v) ∈E}),	(33)
for each U ∈ V \ {v}. We may also use Xuv for more clarification.
15
Under review as a conference paper at ICLR 2022
A.2 Proof of Theorem 1
We utilize an inductive proof on τ , which is the length of the covering sequence of H. Equivalently,
due to the definition, τ = k - 1, where k is the number of vertices in H . First, we note that due
to Proposition 8, without loss of generality, we can assume that H is a simple connected attributed
graph and the goal is to achieve the induced-subgraph count function via an RNP-GNN with appro-
priate recursion parameters. We also consider only maximally expressive networks here to prove the
desired result.
Induction base. For the induction base, i.e., τ = 1, H is a two-node graph. This means that we only
need to count the number of a specific (attributed) edge in the given graph G. Note that in this case
we apply an RNP-GNN with recursion parameter r1 ≥ 1. Denote the two attributes of the vertices
inHbyX1H,X2H ∈ X. The output of an RNP-GNN f (.; θ) is
f (G; θ) = φ({ψ(XG,以{XUv ： U ∈Nrι (v)⅛)) ： V ∈ [n]}),	(34)
where we assume that f(.; θ) is maximally expressive. The goal is to show that f w C(G; H).
Using the transitivity of W, We only need to choose appropriate φ, ψ,夕 to achieve f = C(G; H) as
the final representation. Let
φ(szv: v ∈ [n]*):=2 + 2 × 1{XH = XH}=Zi
ψ(X, (z, z0)) := z × 1{X = X1H} + z0 × 1{X = X2H}	(36)
n0	n0
以《Zu : U ∈ [n0]⅛) := ( X 1{zu = (XH, 1)}, X 1{zu = (XH,1)).	(37)
i=1	i=1
Then, a simple computation shoWs that
f(G; θ) = φ( {Ψ(XG,以{XUv : U ∈Nrι (V)})) : V ∈ [n]}),	(38)
=C(G;H).	(39)
Since f(.; θ) is an RNP-GNN With recursion parameter r1 and for any maximally expressive RNP-
GNN f(.; θ) With the same recursion parameter as f We have f w f and f w C(G; H), We conclude
that f w C (G; H ) and this completes the proof.
Induction step. Assume that the desired result holds for τ - 1 (τ ≥ 2). We shoW that it also holds
for τ . Let us first define
H* := {HV1 : ∃V2,...,Vτ ∈ [k] ： (v1,v2,...,vτ) ∈ Ch (r)}	(40)
c*(H0) := 1{H0 ∈ H*} × #{v ∈ [k] : Hv = H0},	(41)
Where Hv* means the induced subgraph after removing a node, With neW attributes (see A.1). Note
that H* = 0 by the assumption. Let
kH*k :=	c*(H0).
H0∈H*
(42)
1 -	11 TΓ∩ _ C，士	∙	.<	∙	1	. ∙	1	.1	∙	.<	♦	Z	1∖	1 ʌ TL T 1 ʌ √-ITL TTL T P / 久、	∙.)
For all H0 ∈ H*, using the induction hypothesis, there is a (universal) RNP-GNN f(.; θ) With
recursion parameters (r2,r3, ...,rτ) SUch that f W C(G; H0). Using Proposition 4 we conclude
f W E	C(G; HU).	(43)
u∈[k1HU∈H*
Define a maximally expressive RNP-GNN with the recursion parameters (r1 , r2, . . . , rτ) as follows:
f (G; θ)= φ(gψ(Xf,f(G*(Nrι (v)); θ)) : V ∈ [n]⅛).	(44)
Similar to the proof for τ = 1, here we only need to propose a (not necessarily maximally expressive)
RNP-GNN which achieves the function C(G; H).
16
Under review as a conference paper at ICLR 2022
Let us define
fHU (G; θ) := φ({ψHu (XG, ξ ◦ f(G*(Nrι (v)); θ)) : v ∈ [n]⅛),	(45)
where		
φ( {zv : v ∈ [n] }) :	1n =∣m HZi	(46)
ψHu* (X, z) :	z × 1{X = XuH },	(47)
		(48)
and ξ ◦ f = C(G; Hu) Note that the existence of such function ξ is guaranteed due to Proposition
2. Now we write
∣∣H*k × C(G; H) = ∣∣H*k X 1{G(S) = H}	(49)
S ⊆[n]
=XX	1{∃u	∈	[k]	:	(G(S \	{v}))V =	Hu	∈ H*	∧ XG =	XuH}	(50)
S ⊆[n] v∈S
= X X	1{∃u	∈	[k]	:	(G(S	\	{v}))v*	==	Hu*	∈H*∧XvG=XuH}	(51)
v∈[n] v∈S⊆[n]
= X X	1{∃u	∈	[k]	:	(G(S \	{v}))v*	==	Hu*	∈H*∧XvG=XuH}
v∈[n] v∈S⊆Nr1 (v)
(52)
= X X	X	1{(G(S \ {v}))v* ==Hu*}1{XvG=XuH} (53)
v∈[n] v∈S⊆Nrι (V) u∈[k];HU∈H*
= X X	C(G*(Nr1(v));Hu*) ×1{XvG=XuH},	(54)
v∈[n] u∈[k] ; HU∈H*
which means that
X	fHu* (G; θ) wC(G;H).	(55)
u∈[k];HU∈H*
However, for a maximally expressive RNP-GNN f(.; θ) we know that f w fH* for all Hu* ∈ H and
this means that f w C(G; H). The proof is thus complete.
B Proof of Theorem 2
For any attributed graph H on r nodes (not necessarily connected) we claim that RNP-GNNs can
count them.
Claim 2. Let f(.; θ) : Gn → Rd be a maximally expressive RNP-GNN with recursion parameters
(r- 1,r-2,...,1). Then, fw C(G; H).
Now consider the function
'(G) = φ(专 ψ(G(S)): S⊆V, |S| ≤ r ⅛).	(56)
We claim that f w ` (f is defined in the previous claim) and this completes the proof according to
Proposition 2.
To prove the claim, assume that f(G1) = f(G2). Then, we conclude that C(G1; H) = C(G2; H)
for any attributed H (not necessarily connected) with r vertices. Now, we have
'(G) = Φ({ Ψ(G(S)): S⊆V, |S| ≤ r ⅛)	(57)
= φ({ψ(H) : H ∈ Gr, the multiplicity of H is C(G; H) }),	(58)
which shows that '(G1) = '(G2).
17
Under review as a conference paper at ICLR 2022
Proof of Claim 2. To prove the claim, we use an induction on the number of connected components
cH of graph H. If H is connected, i.e., cH = 1, then according to Theorem 1, we know that
fwC(G;H).
Now assume that the claim holds for cH = c - 1 ≥ 1. We show that it also holds for cH = c. Let
H1,H2,...,Hc denote the connected components of H. Also assume that Hi = Hj for all i = j.
We will relax this assumption later. Let us define
Ag ：=	{(Si, S2,..., Sc)	： ∀i	∈	[c]	：	Si	⊆	[n]; G(Si)=	Hi}.	(59)
Note that we can write
c
|AG| = YC(G;Hi)	(60)
i=1
∞
=C(G;H)+Xc0jC(G;Hj0),	(61)
j=1
where H10 , H20 , . . . are all non-isomorphic graphs obtained by adding edges (at least one edge) be-
tween c graphs H1 , H2 , . . . , Hc, or contracting a number of vertices of them. The constants c0j
are just used to remove the effect of multiple counting due to the symmetry. Now, since for any
Hi , Hj0 the number of connected components is strictly less that c, using the induction, we have
f w C(G; Hi) and f w C(G; Hj0) for all j and all i ∈ [c]. According to Proposition 4, we
conclude that f w C(G; H) and this completes the proof. Also, if Hi, i ∈ [c], are not pairwise
non-isomorphic, then we can use αC(G; H) in above equation instead of C(G; H), where α > 0
removes the effect of multiple counting by symmetry. The proof is thus complete.
Remark 1. As we explained in the above proof, one can modify Theorem 1 to hold for disconnected
graphs. To this end, we need to generalize the notion of covering sequence to hold for this class of
graphs. Since this special case is out of scope of this paper, we only restrict to a special, but more
insightful case.
C Proof of Theorem 3
To prove Theorem 3, we need to bound the number of node updates required for an RNP-GNN with
recursion parameters (r1, r2, . . . , rt). First of all, we have n variables used for the final represen-
tations of vertices. For each vertex v1 ∈ V, we explore the local neighborhood Nr1 (v1) and apply
a new RNP-GNN network to that neighborhood. In other words, for the second step we need to
update |Nr1 (v1)| nodes. Similarly, for the ith step of the algorithm we have at most
λi := max max |Nr1 (v1) ∩ Nr2 (v2) ∩ Nr3 (v3) . . . ∩ Nri (vi)|,	(62)
v1 ∈[n] vj+1 ∈Nrj (vj )
∀j∈[i-1]
updates. Therefore, we can bound the number of node updates as
τ
n × Y λi.	(63)
i=1
Since λi is decreasing in i, the desired result holds.
D Proof of Theorem 4
Let Kk denote the complete graph on k vertices.
Claim 3. For any k, n ∈ N, such that n is sufficiently large,
∣{C(G; Kk) ： G ∈ Gn}∣ ≥ (Cn/(kbg(n/k…) = Ω(nk),	(64)
where c is a constant which does not depend on k, n.
18
Under review as a conference paper at ICLR 2022
In particular, we claim that the number of different values that C(G; Kk ) can take is nk , up to
poly-logarithmic factors.
To prove the theorem, we use the above claim. Consider a class of (s, t)-good graph representations
f(.; θ) which can count any substructure on k vertices. As a result, f w C(G; Kk) for an appropriate
parametrization θ. By the definition, f(.) must take at least {C(G; Kk) : G ∈ Gn} different
values, i.e.,
∣{f (G; θ) ： G ∈ Gn}∣ ≥ ∣{C(G; Kk) ： G ∈ Gn}∣.	(65)
Also,
∣{f(G; θ) : G ∈ Gn}∣ ≤ ∣{{ψ(Gi) : i ∈ [t]卜 G ∈ Gn}∣,	(66)
where (G1 , G2, . . . , Gt) = Ξ(G). But, ψ can take only s values. Therefore, we have
∣{C(G; Kk) : G ∈ Gn}∣≤ {f (G; θ) : G ∈ Gn}∣	(67)
≤ ∣∣∣{ {ψ(Gi) : i∈ [t]} : G ∈ Gn}∣∣∣	(68)
≤ ∣{«αi : i ∈ [t]h ∀i ∈ [t]:ai ∈ [s]}∣	(69)
≤ (t + 1)s-1.	(70)
As a result, (t + 1)s-1 = ΩΩ(nk) or t = ΩΩ(n占).To complete the proof, We only need to prove the
claim.
(72)
(73)
(74)
(75)
(76)
Proof of Claim 3. Let p1 , p2 , . . . , pm be distinct prime numbers less than n/k. Using the prime
number theorem, we know that limn→∞ 田也 盛⑺/防)=1. In particular, we can choose n large
enough to ensure cn/(k log(n/k)) < m for any constant c < 1.
For any B = {b1, b2, . . . , bk} ⊆ [m], define GB as a graph on n vertices such that VGB = V0 t
(ti∈[k]Vi), and |Vi| = pbi. Also,
e =(u,v) ∈ GB ^⇒ ∃ i,j ∈ [m],i= j : U ∈ Vi & V ∈ Vj.	(71)
The graph GB is well-defined since Pik=1 pbi ≤ k × n/k = n. Note that C(GB; Kk) = Qik=1 pbi .
Also, since pi, i ∈ [m], are prime numbers, there is a unique bijection
B 匕 C(GB； Kk).
Therefore,
∣{C(G; Kk) : G ∈ Gn}∣ ≥ ∣{C(Gb; Kk) : B⊆ [m], |B| = k}∣
=mk
(m — k)k
≥	k!
(cn∕(k log(n/k)) — k)k
≥	k!	.
E Relationship to the Reconstruction Conjecture
Theorem 2 provides a universality result for RNP-GNNs. Here, we note that the proposed method is
closely related to the reconstruction conjecture, an old open problem in graph theory. This motivates
us to explain their relationship/differences. First, we need a definition for unattributed graphs.
Definition 10. Let Fn ⊆ Gn be a set of graphs and let Gv = G(V \ {v}) for any finite simple
graph G = (V, E), and any v ∈ V. Then, we say the set F is reconstructible if and only if there is a
bijection
£Gv ： v ∈ V⅛《—> G,	(77)
for any G ∈ Fn. In other words, Fn is reconstructible, if and only if the multi-set {Gv : v ∈ V }
fully identifies G for any G ∈ Fn .
19
Under review as a conference paper at ICLR 2022
It is known that the class of disconnected graphs, trees, regular graphs, are reconstructible (Kelly,
1957; McKay, 1997). The general case is still open; however it is widely believed that it is true.
Conjecture 1 (Kelly (1957)). Gn is reconstructible.
For RNP-GNNs, the reconstruction from the subgraphs GW, V ∈ [n] is possible, since We relabel
any subgraph (in the definition of X w) and this preserves the critical information for the recursion
to the original graph. In the reconstruction conjecture, this part of information is missing, and this
makes the problem difficult. Nonetheless, since in RNP-GNNs We preserve the original node’s
information in the subgraphs With relabeling, the reconstruction conjecture is not required to hold to
shoW the universality results for RNP-GNNs, although that conjecture is a motivation for this paper.
Moreover, if it can be shoWn that the reconstruction conjecture it true, it may be also possible to find
a simple encoding of subgraphs to an original graph and this may lead to more poWerful but less
complex neW GNNs.
F The RNP-GNN Algorithm
In this section, We provide pseudocode for RNP-GNNs. The algorithm beloW computes node rep-
resentations. In the algorithms, We frequently use MLP modules With ReLU activation. For a graph
representation, we can aggregate them with a common readout, e.g., hg J MLP (P^∈v hVVk)).
FolloWing (Xu et al., 2019), We use sum pooling here, to ensure that We can represent injective
aggregation functions.
Algorithm 1 Recursive Neighborhood Pooling-GNN (RNP-GNN)
Input: G =	(V, E,	{xv }v∈V)	where V =	[n], recursion parameters r1, r2, . . . , rτ	∈	N,	(i)	∈	R,
i ∈ [τ], node features {xv }v∈V.
Output: hv for all v ∈ V
hivn J xv for all v ∈ V
if τ = 1 then
hv J MLP(τ,1) (1 + (1))hivn +	X	MLP(τ,2)(hiun, 1(u, v) ∈ E) ,
u∈Nr1 (v)\{v}
for all v ∈ V .
else
for all v ∈ V do
G0v J G(Nr1 (v) \ {v}), which has node attributes {(hiun, 1(u, v) ∈ E)0}u∈Nr (v)\{v}
{h v,u}u∈GV J RNP-GNN(Gv ,g, ",..., r),(七⑵，∙ ∙ ∙，e(τ)))
hv J MLP(T) ((I + E(T))hVn + Pu∈G0 hu,v) ∙
end for
end if
return {hv }v∈V
With this algorithm, one can achieve the expressive power of RNP-GNNs if high dimensional MLPs
are allowed (Xu et al., 2019; Hornik et al., 1989; Hornik, 1991). That said, in practice, smaller
MLPs may be acceptable (Xu et al., 2019).
G	Computing a Covering S equence
As we explained in the context of Theorem 1, we need a covering sequence (or an upper bound to
that) to design an RNP-GNN network that can count a given substructure. A covering sequence can
be constructed from a spanning tree of the graph.
For reducing complexity, it is desirable to have a covering sequence with minimum r1 (Theorem
3). Here, we suggest an algorithm for obtaining such a covering sequence, shown in Algorithm 2.
20
Under review as a conference paper at ICLR 2022
For obtaining merely an aribtrary covering sequence, one can compute any minimum spanning tree
(MST), and then proceed as with the MST in Algorithm 2.
Given an MST, we build a vertex covering sequence by iteratively removing a leaf vi from the tree
and adding the respective node vi to the sequence. This ensures that, at any point, the remaining
graph is connected. At position i corresponding to vi, the covering sequence contains the maximum
distance ri of vi to any node in the remaining graph, or an upper bound on that. For efficiency, an
upper bound on the distance can be computed in the tree.
To minimize r1 = maxu∈V d(u, v1), we need to ensure that a node in arg minv∈V maxu∈V d(u, v)
is a leaf in the spanning tree. Hence, we first compute maxu∈V d(u, v) for all nodes v, e.g., by
running All-Pairs-Shortest-Paths (APSP) (Kleinberg & Tardos, 2006), and sort them in increasing
order by this distance. Going down this list, we try whether it is possible to use the respective node
as v1 , and stop when we find one.
Say v* is the current node in the list. To compute a spanning tree where v* is a leaf, We assign a
large weight to all the edges adjacent to v*, and a very low weight to all other edges. If there exists
such a tree, running an MST with the assigned weights will find one. Then, we use v* as v1 in the
vertex covering sequence. This algorithm runs in polynomial time.
Algorithm 2 Computing a covering sequence with minimum r1
Input: H = (V, E, X) where V = [τ + 1]
Output: A minimal covering sequence (r1, r2 . . . , rτ), and its corresponding vertex covering se-
quence (v1,v2, . . . ,vτ+1)
For any u, v ∈ V, compute d(u, v) using APSP
(u1,u2,..., uτ+ι) J all the vertices sorted increasingly in s(v) := maxu∈v d(u, V)
for i = 1 to τ + 1 do
Set edge weights w(u, v) = 1 + τ × 1{u = ui ∨ v = ui} for all (u, v) ∈ E
HT J the MST of H with weights w
if ui is a leaf in HT then
v1 J ui
r1 J s(ui)
break
end if
end for
for i = 2 to τ + 1 do
vi J one of the leaves of HT
ri J maxu∈VHT d(u, vi)
HT J HT after removing vi
end for
return (r1, r2, . . . , rτ) and (v1, v2, . . . , vτ+1)
H Experimental Details
Table 4: Runtime (in seconds) averaged over 10 epochs of training on the synthetic triangle counting
experiments on ErdoS-Renyi graphs. Times do not include LRP preprocessing (which takes several
minutes for this dataset).________________________________________________
Model	Parameters	Runtime (s)
RNP-GNN, r = (1, 1)	10210	153.13
RNP-GNN, r = (1, 1, 1)	10834	199.49
RNP-GNN, r = (2, 1)	10210	370.38
RNP-GNN, r = (2, 1, 1)	10834	835.77
GIN	10186	.88
Deep-LRP	10231	24.43
21
Under review as a conference paper at ICLR 2022
H. 1 Dataset and Task Details
For the counting experiments, we follow the setup of Chen et al. (2020). There are two datasets:
one consisting of 5000 Erdos-Renyi graphs (Erdos et al., 1960) and one consisting of 5000 noisy
random regular graphs (Steger & Wormald, 1999). Each ErdoS-Renyi graph has 10 nodes, and each
random regular graph has either 10, 15, 20, or 30 nodes. Also, n random edges are deleted from
each random regular graph, where n is the number of nodes.
For the experiments on distinguishing non-isomorphic graphs, we use the EXP dataset (Abboud
et al., 2021). This dataset consists of 600 pairs of graphs (so 1200 graphs in total), where each pair
is 1-WL equivalent but distinguishable by 3-WL, and each pair contains one graph that represents a
satisfiable formula and one graph that represents an unsatisfiable formula. On average, each graph
contains about 44 nodes and 110 edges (Balcilar et al., 2021). We report the mean and standard
deviation across 10 cross-validation folds. Additionally, we report the runtimes in Table 4 for the
counting experiments.
H.2 RNP-GNN Implementation Details
Here, we detail some specific design choices we make in implementing our RNP-GNN model. Most
embeddings are computed in Rd for some fixed hidden dimension d. The input node features are
first embedded in Rd by an initial linear layer. Then RNP layers are applied to compute node
representations. Finally, a sum pooling across nodes followed by a final MLP is used to compute a
graph-level output.
An RNP layer for r = (r1 , . . . , rτ ) is implemented as follows. Note that the input node features to
this layer are in Rd due to our initial linear layer. Also, note that we concatenate an extra feature
dimension due to the augmented indicator feature at each recursion step. To align these feature
dimensions, for l ∈ [τ], we parameterize the l-th GIN (Xu et al., 2019) by a feedforward neural
network MLP(l) : Rd+l → Rd+l-1. For instance, the last GIN has a feedforward network MLP(τ) :
Rd+τ → Rd+τ-1, because after τ levels of recursion we have augmented τ features. Dropout and
nonlinear activation functions are only applied in the MLPs.
H.3 Hyperparameters
For all baseline models, we take the results from other papers. Thus, for the counting experiments the
configurations for the baseline models are from Chen et al. (2020), while for the EXP experiments
the configurations for the baseline models are from Abboud et al. (2021).
RNP-GNN hyperparameters. For all experiments we ran random search over hyperparameters. In
all cases we used the Adam optimizer with initial learning rate in {.01, .001, .0001, .0005}. We train
for 100 epochs with a batch size in {16, 32, 128}. The number of stacked RNP-GNNs for computing
node representations is in {1, 2}. We use a dropout ratio in {0, .1, .5}. The recursion parameters
used varies for each task. We used two layers for each MLP used in the aggregation function. Also,
the graph-level output obtained after sum-pooling across nodes is computed by a two layer MLP.
Specifically for the counting experiments, the number of hidden dimensions is searched in
{16, 32, 64}. For all tasks we used r1 = 1. In particular, the optimal r parameters for RNP-GNN
were: r =(1,1,1,1) for triangles on Erdos-Renyi, r = (1,1) for stars on Erdos-Renyi, r =(1,1)
for triangles on random regular and r = (1, 1, 1) for stars on random regular. We use ReLU activa-
tions in the MLPs. We either decay the learning rate by half every 25, 50, or ∞ epochs (where ∞
means never decaying).
For the EXP experiments, the number of hidden dimensions is searched in {8, 16, 32, 64}. We use
either ELU or ReLU activations in the MLPs. We decay the learning rate by half at the 50th epoch.
The recursion parameters are r = (2, 1).
22
Under review as a conference paper at ICLR 2022
I More Figures
Figure 3: MPNNs cannot count substructures with three nodes or more (Chen et al., 2020). For
example, the graph with black center vertex on the left cannot be counted, since the two graphs on
the left result in the same node representations as the graph on the right.
v1
v2
v3
v1
v2
v3
v1
v2
v3
v4
v5
v6
v4
v5
v6
v4
v5
v6
Figure 4: Example of a covering sequence computed for
the
graph on the left. For this
graph, (v6, v1 , v4, v5, v3, v2) is a vertex covering sequence with respect to the covering sequence
(3, 3, 3, 2, 1). The first two computations to obtain this covering sequence are depicted in the middle
and on the right.
23