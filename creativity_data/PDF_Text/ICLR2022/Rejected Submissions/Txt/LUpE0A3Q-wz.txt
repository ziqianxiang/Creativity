Under review as a conference paper at ICLR 2022
On Convergence of Federated Averaging
Langevin Dynamics
Anonymous authors
Paper under double-blind review
Ab stract
We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty
quantification and mean predictions with distributed clients. In particular, we
generalize beyond normal posterior distributions and consider a general class of
models. We develop theoretical guarantees for FA-LD for strongly log-concave
distributions with non-i.i.d data and study how the injected noise and the stochastic-
gradient noise, the heterogeneity of data, and the varying learning rates affect the
convergence. Such an analysis sheds light on the optimal choice of local updates
to minimize communication cost. Important to our approach is that the commu-
nication efficiency does not deteriorate with the injected noise in the Langevin
algorithms. In addition, we examine in our FA-LD algorithm both independent
and correlated noise used over different clients. We observe that there is also a
trade-off between federation and communication cost there. As local devices may
become inactive in the federated network, we also show convergence results based
on different averaging schemes where only partial device updates are available.
1	Introduction
Federated learning (FL) allows multiple parties to jointly train a consensus model without sharing user
data. Compared to the classical centralized learning regime, federated learning keeps training data
on local clients, such as mobile devices or hospitals, where data privacy, security, and access rights
are a matter of vital interest. This aggregation of various data resources heeding privacy concerns
yields promising potential in areas of internet of things Chen et al. (2020), healthcare Li et al. (2020d;
2019b), text data Huang et al. (2020), and fraud detection Zheng et al. (2020).
A standard formulation of federated learning is a distributed optimization framework that tackles
communication costs, client robustness, and data heterogeneity across different clients Li et al.
(2020a). Central to the formulation is the efficiency of the communication, which directly motivates
the communication-efficient federated averaging (FedAvg) McMahan et al. (2017). FedAvg introduces
a global model to synchronously aggregate multi-step local updates on the available clients and yields
distinctive properties in communication. However, FedAvg often stagnates at inferior local modes
empirically due to the data heterogeneity across the different clients Charles & Konecny (2020);
Woodworth et al. (2020). To tackle this issue, Karimireddy et al. (2020); Pathaky & Wainwright
(2020) proposed stateful clients to avoid the unstable convergence, which are, however, not scalable
with respect to the number of clients in applications with mobile devices Al-Shedivat et al. (2021).
In addition, the optimization framework often fails to quantify the uncertainty accurately for the
parameters of interest, which are crucial for building estimators, hypothesis tests, and credible
intervals. Such a problem leads to unreliable statistical inference and casts doubts on the credibility
of the prediction tasks or diagnoses in medical applications.
To unify optimization and uncertainty quantification in federated learning, we resort to a Bayesian
treatment by sampling from a global posterior distribution, where the latter is aggregated by infrequent
communications from local posterior distributions. We adopt a popular approach for inferring
posterior distributions for large datasets, the stochastic gradient Markov chain Monte Carlo (SG-
MCMC) method Welling & Teh (2011); Vollmer et al. (2016); Teh et al. (2016); Chen et al. (2014);
Ma et al. (2015), which enjoys theoretical guarantees beyond convex scenarios Raginsky et al. (2017);
Zhang et al. (2017); Mangoubi & Vishnoi (2018); Ma et al. (2019). In particular, we examine in
the federated learning setting the efficacy of the stochastic gradient Langevin dynamics (SGLD)
1
Under review as a conference paper at ICLR 2022
algorithm, which differs from stochastic gradient descent (SGD) in an additionally injected noise for
exploring the posterior. The close resemblance naturally inspires us to adapt the optimization-based
FedAvg to a distributed sampling framework. Similar ideas have been proposed in federated posterior
averaging Al-Shedivat et al. (2021), where empirical study and analyses on Gaussian posteriors have
shown promising potential of this approach. Compared to the appealing theoretical guarantees of
optimization-based algorithms in federated learning Pathaky & Wainwright (2020); Al-Shedivat et al.
(2021), the convergence properties of approximate sampling algorithms in federated learning is far
less understood. To fill this gap, we proceed by asking the following question:
Can we build a unified algorithm with convergence guarantees for sampling in FL?
In this paper, we make a first step in answering this question in the affirmative. We propose
the federated averaging Langevin dynamics (FA-LD) for posterior inference beyond the Gaussian
distribution. We list our contributions as follows:
•	We present a novel non-asymptotic convergence analysis for FA-LD from simulating strongly
log-concave distributions on non-i.i.d data when the learning rate is fixed. The frequently
used bounded gradient assumption of `2 norm in FedAvg optimization is not required.
•	The convergence analysis indicates that the injected noise, the data heterogeneity, and the
stochastic-gradient noise are all driving factors that affect the convergence. Such an analysis
also provides a concrete guidance on the optimal selection of the number of local updates.
•	We present a convergence result for FA-LD with decaying learning rates. This strategy
accelerates the computation by a logarithmic factor to achieve the precision .
•	The algorithm yields appealing extensions: (1) we can choose to inject either independent
or correlated noise across local clients, yielding a trade-off between accuracy and efficacy
of federation; (2) we can choose whether to activate all the devices to avoid the straggler’s
effect in real-world applications.
Roadmap. In Section 2, we discuss the related work and literature. In Section 3, we present
the preliminary knowledge. In Section 4, we propose the federated averaging Langevin dynamics
algorithm for posterior inference. In Section 5, we lay out the required assumptions, sketch the proof,
and show the theoretical convergence results. In Section 6, we conclude our work.
2	Related work
Federated Learning Current federated learning follows two paradigms. The first paradigm asks
every client to learn the model using private data and communicate in model parameters. The second
one uses encryption techniques to guarantee secure communication between clients. In this paper, we
focus on the first paradigms Dean et al. (2012); Shokri & Shmatikov (2015); McMahan et al. (2016;
2017); Huang et al. (2021). There is a long list of works showing provable convergence algorithm for
FedAvg types of algorithms in the field of optimization Li et al. (2020c; 2021); Huang et al. (2021);
Khaled et al. (2019); Yu et al. (2019); Wang et al. (2019); Karimireddy et al. (2020). One line of
research Li et al. (2020c); Khaled et al. (2019); Yu et al. (2019); Wang et al. (2019); Karimireddy et al.
(2020) focuses on standard assumptions in optimization (such as, convex, smooth, strongly-convex,
bounded gradient). The other line of work Li et al. (2021); Huang et al. (2021) proves the convergence
in the regime where the model of interest is an over-parameterized neural network (also called NTK
regime Jacot et al. (2018)).
Scalable Monte Carlo methods SGLD Welling & Teh (2011) is the first stochastic gradient Monte
Carlo method that tackles the scalability issue in big data problems. Ever since, variants of stochastic
gradient Monte Carlo methods were proposed to accelerate the simulations by utilizing more general
Markov dynamics Ma et al. (2015; 2018); Chen et al. (2014), Hessian approximation Ahn et al.
(2012), parallel tempering Deng et al. (2020), as well as higher-order numerical schemes Chen et al.
(2015); Li et al. (2019c); Cheng et al. (2018); Ma et al. (2021); Mou et al. (2021); Shen & Lee (2019).
Distributed Monte Carlo methods Sub-posterior aggregation was initially proposed in
Neiswanger et al. (2013); Wang & Dunson; Minsker et al. (2014) to accelerate MCMC methods to
cope with large datasets. Other parallel MCMC algorithms Nishihara et al. (2014); Ahn et al. (2014);
2
Under review as a conference paper at ICLR 2022
Chen et al. (2016); Chowdhury & Jermaine (2018); Li et al. (2019a) propose to improve the efficiency
of Monte Carlo computation in distributed or asynchronous systems. Gurbuzbalaban et al. (2021)
proposed stochastic gradient Monte Carlo methods in decentralized systems. Al-Shedivat et al. (2021)
introduced empirical studies of posterior averaging in federated learning.
Notation For any positive integer n, We use [n] to denote the set {1, 2,…，n}. Let N denote the
number of clients. For each C ∈ [N], We use fC and VfC as the loss function and gradient of the
function fC In client c. Vf c(∙) IS the unbiased stochastic gradient of Vfc. In addition, we denotePC
as the weight of the c-th client such that PC = PNc— ∈ (0,1), where n > 0 is the number of data
i=1 ni
points in the c-th client. Let T denote the number of global steps to achieve the precision . Let K
denote the number of local steps and hence T /K denotes the number of communications.
3	Preliminaries
3.1	An optimization perspective on federated averaging
Federated averaging (FedAvg) is a standard algorithm in federated learning and is typically formulated
into a distributed optimization framework as follows
mθin '(θ) ：= P= W ,	'c(θ) ：= XX l(θ; XC,i),
C=1 nC	i=1
(1)
where θ ∈ Rd, l(θ; xC,j) is a certain loss function based on θ and the data point xC,j.
One iterate of the FedAvg algorithm requires the following three steps:
•	Broadcast: The center server broadcasts the latest model, θk, to all local clients.
•	Local updates: For any c ∈ [N], the c-the client first sets θkC = θk and then conducts K ≥ 1
local steps:
. . ~一 . 一
βC+ι = θk -ηV'°(θC),
where η is the learning rate and V'c is the unbiased estimate of the exact gradient V'c.
•	Synchronization: The local models are aggregated into a unique model θk+K :=
PCN=1 PCβkC+K and sent to the center server.
From the optimization perspective, Li et al. (2020c) proved the convergence of the FedAvg algorithm
on non-i.i.d data such that a larger number of local steps K and a higher order of data heterogeneity
slows down the convergence. Notably, Eq. (1) can be interpreted as maximizing the likelihood
function, which is a special case of maximum a posteriori estimation (MAP) given a uniform prior.
3.2	Stochastic gradient Langevin dynamics
Posterior inference offers the exact uncertainty quantification ability of the predictions. A popular
method for posterior inference with large dataset is the stochastic gradient Langevin dynamics
(SGLD) Welling & Teh (2011), which injects additional noise into the stochastic gradient and adapts
an optimization algorithm to a sampling one
θk+ι = θk - ηVf(θk) + P2Tnξk,
where T is the temperature and ξk is a standard d-dimensional Gaussian vector. f (θ) := PN=I 'c(Θ)
is a energy function. f(θ) is a unbiased estimate of f (θ). In the longtime limit, a well known result
is that θk converges weakly to the distribution π(θ) a exp(-f (θ)∕τ) Teh et al. (2016) as η → 0.
4	Posterior inference via federated averaging Langevin dynamics
The increasing concern for uncertainty estimation in federated learning motivates us to consider the
simulation of the distribution π(θ) a exp(-f (θ)∕τ) with distributed clients.
3
Under review as a conference paper at ICLR 2022
Problem formulation We propose the federated averaging Langevin dynamics (FA-LD) based
on the FedAvg framework in section 3.1. We follow the same broadcast step and synchronization
step but propose to inject random noises for local updates. In particular, we consider the following
scheme: for any c ∈ [N], the c-the client first sets θkc = θk and then conducts K ≥ 1 local steps:
βc+ι = θk - Nfe(θ) + p2ητΞk,	⑵
where Vf c(θ) = p1- V'c(θ). V/c(θ) is the unbiased estimate of V/c(θ) and Ξ1 is an independent
Gaussian vector to be defined later.
Summing Eq. (2) from clients c = 1 to N , we have the aggregated stochastic process as follows
βk+ι = θk — ηVf(θk) + p2ητξk,
where
NN	N	N
βk =	XPcβc,	θk = XPcθk,	Vf(θk)	= XPcVfc(θk),	ξk	= XPcΞk.	⑶
c=1	c=1	c=1	c=1
By the nature of the synchronization step, we always have βk = θk whether k + 1 mod K = 0 or not.
In what follows, we can write
θk+ι = θk — ηVf(θk) + pηTξk,	(4)
which resembles the SGLD algorithm except that the construction of stochastic gradients is dif-
ferent and θk is not accessible when k mod K 6= 0. Since our target is to simulate from
∏(θ) a exp(-f (θ)∕τ), We expect that ξk is a standard Gaussian vector. By the concentra-
tion property of independent Gaussian variables, it is natural to set Ξ = ξk / √Pc so that
ξk = PN=ι pcΞ = PN=ι √pcξc and ξkc is also a standard Gaussian vector. Now we present
it in Algorithm 1.
Algorithm 1 Federated averaging Langevin dynamics algorithm (FA-LD), informal version of
Algorithm 4. ηk is the learning rate at iteration k. τ is the temperature. Denote by θkc the model
parameter in the c-th client at the k-th step. Denote the immediate result of one step SGLD update
from θkc by βkc. ξkc is an independent standard d-dimensional Gaussian vector at iteration k for each
client c ∈ [N]. A global synchronization is conducted every K steps.
1:	〜
βk+ι = θk — ηk Vfc(θc) + v‰ τ∕pc ξk,	(5)
2:
(βc+ι	if k + 1 mod K = 0
θkc+1 =	N	(6)
[ Pc=I Pcβc+1	if k + 1 mod K = 0.
Algorithm 2 Hybrid federated averaging Langevin dynamics algorithm (hFA-LD), informal version
of Algorithm 5. ξk is a d-dimensional Gaussian vector shared by all the clients; ξkc is an independent
standard d-dimensional Gaussian vector at iteration k for each client c ∈ [N]. ρ denotes the
correction coefficient.____________________________________________________________________
1:	〜
βk+ι = θk — ηVfc(θk) + v‰2ξk + √2η(i — ρ2)τ∕pc ξk,
2:
ʃ βk+ι
∖ Pc=I pcβc + 1
if k + 1 mod K 6= 0
if k + 1 mod K = 0.
We observe that the local process in Eq. (5) maintains a temperature τ∕pc > τ to converge to the
stationary distribution π. Such a mechanism may limit the disclosure of individual data and shows a
potential to ensure a higher level of privacy.
4
Under review as a conference paper at ICLR 2022
5 Convergence analysis
In this section, we show that FA-LD converges to the stationary distribution π(θ) in the 2-Wasserstein
(W2)distance at a rate of O(1/√T∖) for strongly log-concave and smooth density. The W2 distance
is defined between a pair of Borel probability measures μ and V on Rd as follows
1
W2(μ,ν) := 2「inf，、(八同-β"k2dY 2(βμ, βν 力,
γ2∈Couplings(μ,ν) ∖
where ∣∣∙ ∣∣2 denotes the '2 norm on Rd and the pair of random variables (βμ, βν) ∈ Rd X Rd is
a coupling with the marginals following L(βμ) = μ and L(βν) = V. Note that L(∙) denotes a
distribution of a random variable. Such a distance is more appealing than the total variation or the
Kullback-Leibler divergence in statistical machine learning applications for providing the estimates
of the first and second order moments.
5.1	Notation and assumptions
We make standard assumptions on the smoothness and convexity of the functions f 1,f2,…，f N,
which naturally yields appealing tail properties of the stationary measure π. Thus, we no longer
require a restrictive assumption on the bounded gradient in `2 norm as in Koloskova et al. (2019); Yu
et al. (2019); Li et al. (2020c). In addition, to control the distance between Vfc and Vfc, we also
assume a bounded variance of the stochastic gradient in assumption 5.3.
Assumption 5.1 (Smoothness). For each c ∈ [N], we say fc is L-smooth if for some L > 0
fc(y) ≤ fc(χ) + NfC(X),y — Xi + L2ky — χ∣∣2 ∀χ,y ∈ Rd.
Assumption 5.2 (Strongly convex). For each c ∈ [N], fc is m-strongly convex if for some m > 0
fc(x) ≥ fc(y) + hVfc(y),χ — y + m∣∣y — χ∣∣2 ∀χ,y ∈ Rd.
Assumption 5.3 (Bounded variance, informal version of Assumption A.3). For each c ∈ [N], the
variance of noise in the stochastic gradient Vfc (X) in each client is upper bounded such that
E[∣Vfec(X) - Vfc(X)∣22] ≤ σ2d, ∀X ∈ Rd.
Quality of non-i.i.d data Denote by θ* the global minimum of f. Next, we quantify the degree
of the non-i.i.d data by Y := maXc∈[N ] ∣∣Vf c(θ*) |卜，which is a non-negative constant and yields a
larger scale if the data is less identically distributed.
5.2	Proof sketch
The proof hinges on showing the one-step result in the W2 distance. To facilitate the analysis, we
first define an auxiliary continuous-time processes (θt)t≥o without communication concerns
d& = -Vf (θt )dt + √2T dW t,	(7)
where θt = Pc=IPc^θcC, Vf(θt) = Pc=IPcVfc(OC), θt is the continuous-time variable at client c,
and W is a d-dimensional Brownian motion. The continuous-time algorithm is known to converge to
the stationary distribution π(θ) a e-fτθ), where f (θ) = PN=I pcf c(θ). Assume that & simulates
from the stationary distribution π, then it follows that Gt 〜π for any t ≥ 0.
5.2.1	Dominated contraction in federated learning
The first target is to show a certain contraction property of ∣β - θ - η(Vf(β) - Vf (θ))∣22 based on
distributed clients with infrequent communications. Consider a standard decomposition
∣β - θ - η(Vf(β) - Vf(θ))∣22
5
Under review as a conference paper at ICLR 2022
=kβ - θk2 - 2η hβ-θ, Vf (β) - Vf (θ)i +η2 kVf (β) - Vf (θ)k2 .
X--------------------------{----------}
I
Using Eq.(3), we decompose I and apply Jensen’s inequality to obtain the lower bound ofI. In what
follows, we have the following lemma.
Lemma 5.4 (Dominated contraction property, informal version of Lemma B.1). Assume assumptions
5.1	and 5.2 hold. Forany learning rate η ∈ (0, j+1m], any {θc}N=ι, {βc}N=ι ∈ Rd, we have
N
kβ - θ - η(Vf (β) - Vf (θ))k2 ≤ (1-ηm) ∙kβ - θk2 + 4ηLXp。∙ (kβc - βk2 + ∣∣θc - θ∣∣2),
c=1	S ~	z~"	}
divergence term
where β = PcN=1pcβc, θ = PcN=1 pcθc, Vf(θ) = PcN=1pcVfc(θc), and Vf(β) =
PcN=1pcVfc(βc). It implies that as long as the local parameters θc, βc and global θ, β don’t
differ each other too much, we can guarantee the desired convergence. In a special case when the
communication is conducted at every iteration, the divergence term disappears and recovers the
standard contraction Dalalyan & Karagulyan (2019).
5.2.2	Bounding divergence
The following result shows that given a finite number of local steps K , the divergence between θc in
local client and θ in the center is bounded in `2 norm. Notably, since the Brownian motion leads to a
lower order term O(η) instead of O(η2 ),a naive proof framework such as Li et al. (2020c) may lead
to a crude upper bound for the final convergence.
Lemma 5.5 (Bounded divergence, informal version of Lemma B.3). Assume assumptions 5.1, 5.2,
and 5.3 hold. For any learning rate η ∈ (0, 2/m) and ∣∣θC — θ* k2 ≤ dD2 for any C ∈ [N], we have
the `2 upper bound of the divergence between local clients and the center as follows
N
XpcE∣θkc - θk∣22 ≤ O(K2η2d) + O(Kηd).
c=1
The result also relies on showing a uniform upper bound in `2 norm, which avoids making extra
bounded gradient assumptions.
5.2.3	Coupling to the stationary process
Note that θt is initialized from the stationary distribution π. The solution to the continuous-time
process Eq.(7) follows:
θ
θo - Zt Vf (θs)ds + √2τ ∙ Wt,
0
∀t ≥ 0.
(8)
Set t → (k + 1)η and θo → Okn for Eq.(8) and consider a synchronous coupling such that W(k+i)η -
Wkn ：= √ηξk is used to cancel the noise terms, we have
θO(k+1)n
θOkn
(k+1)n
kn
Vf(θs)ds +y∕2Tηξk.
(9)
Subtracting Eq.(4) from Eq.(9) and taking square and expectation on both sides yield that
E∣∣0(k+i)n - θk+ι∣2 ≤ (1 - ηm∕2) ∙ EMkn - θk∣∣2 + divergence term + time error.
Eventually, we arrive at the one-step error bound for establishing the convergence results.
Lemma 5.6 (One step update, informal version of Lemma B.5). Assume assumptions 5.1, 5.2, and
5.3 hold. Consider Algorithm 1 with any learning rate η ∈ (0, J) and ∣∣θC — θ*∣∣2 ≤ dD2 for any
c ∈ [N], where θ* is the global minimumfor thefunction f. Then
W2(μk+ι,∏) ≤ (1 - ηm∕2) ∙ W2(μk,∏) + O(η2d(K2 + κ)),
where μk denotes the probability measure of θk and K = L/m is the condition number
6
Under review as a conference paper at ICLR 2022
5.3	Full device participation
5.3.1	Convergence based on independent noise
When the synchronization step is conducted at every iteration k, the FA-LD algorithm is essentially
the standard SGLD algorithm Welling & Teh (2011). Theoretical analysis based on the 2-Wasserstein
distance has been established in Durmus & Moulines (2016); Dalalyan (2017a); Dalalyan & Karag-
ulyan (2019). However, in scenarios of K > 1 with distributed clients, a divergence between the
global variable θk and local variable θkc appears and unavoidably affects the performance. The upper
bound on the sampling error is presented as follows.
Theorem 5.7 (Main result, informal version of Theorem B.6). Assume assumptions 5.1, 5.2, and 5.3
hold. Consider Algorithm 1 With a fixed learning rate η ∈ (0,芸]and ∣∣θC 一 θ*∣∣2 ≤ dD2 for any
C ∈ [N], we have，
W2(μk, ∏) ≤ (1 一 ηm∕4)k ∙ (√2d(D + Pτ∕m)) + 30κpηmd ∙ P(K2 + κ)H.
where μk denotes the probability measure of θk at iteration k, K denotes the numberoflocal updates,
22
K := L/m, Y := maXc∈[N] ∣∣Vfc(θ*)k2, and Ho := D2 +maxc∈[N]砧 + m⅛ + m.
We observe that the initialization, the scale of the injected noise, the heterogeneity of the data, and
the noise in the stochastic gradient all affect the convergence. Similar to the result of Li et al. (2020c),
FA-LD with K-local steps resembles the behaviour of one-step SGLD with a large learning rate.
Optimal choice of K. To ensure the algorithm to achieve the precision based on the total number
of steps T and the learning rate η, we can set
30κpηmd ∙ P(K2 + κ)Ho ≤ e∕2, exp ( — η^Te) ∙ √2d(D + pτ∕m) ≤ e∕2.
This readily leads to
ηm ≤ O( dκ2(K： + K)HO ), T ≥ ω( logm∕e)).
Plugging into the upper bound of ηm, it implies that to reach the precision e, it suffices to set
T = Ω(e-2dκ2(K2 + κ)Ho ∙ log(d∕e)).	(10)
It's obvious that H = Ω(D2) = Ω(1), thus We can conclude that the number of communication
rounds is around the order
TK = Q(K+KK),
where the value of T first decreases and then increases with respect to K, indicating that setting K
either too large or too small may lead to high communication costs and hurt the performance. Ideally,
K should be selected in the scale of Ω(√κ). Combining the definition of Te in Eq. (10), this suggests
an interesting result that the optimal K for FA-LD should be in the order of O(√Te). Similar results
have been achieved by Stich (2019); Li et al. (2020c).
5.3.2	Convergence guarantees via varying learning rates
Theorem 5.8 (Informal version of Theorem B.7). Assume assumptions 5.1, 5.2, and 5.3 hold.
Consider Algorithm 1 with an initialization satisfying ∣∣Θq 一 θ*∣2 ≤ dD2 for any c ∈ [N] and the
varying learning rate following
1
2L + (1∕12)mk'
k = 1, 2,….
Then for any k ≥ 0, we have
W2(μk, ∏) ≤ 45κp(K2 + κ)Ho ∙ (ηkmd)1/2,	Vk ≥ 0.
tFor ease of presentation, we report the result based on K2 instead of (K — 1)2. The upper bound based on
(K - 1)2 is detailed in the supplementary file.
7
Under review as a conference paper at ICLR 2022
Note that the above result implies that to achieve the precision , we require
W2(μk,∏) ≤ 45κP(K2 + κ)Ho ∙ (OT ʤ 卜I" ≤ e.
2L + (1/12)mk
We therefore require Ω(e-2d) iterations to achieve the precision e, which improves the
Ω(e-2dlog(d/e)) rate for FA-LD with a fixed learning rate by a O(log(d∕e)) factor.
5.3.3	Privacy-accuracy trade-off via correlated noises
Note that Algorithm 1 requires all the local clients to generate the independent noise ξkc . Such a
mechanism enjoys the convenience of the implementation and yields a potential to protect the privacy
of data and alleviates the security issue. However, the large scale noise inevitable slows down the
convergence. To handle this issue, the independent noise can be generalized to correlated noise based
on a correlation coefficient ρ between different clients. Replacing Eq. (5) with
βc+ι = θc - η▽产(θk) + pητρ2ξk + P2η(i-ρ2)τ∕pcξc,	(ii)
where ξk is a d-dimensional standard Gaussian vector shared by all the clients at iteration k and ξk is
dependent with ξkc for any c ∈ [N]. Following the synchronization step based on Eq. (6), we have
θk+ι = θk - η^f(θk) + p2ητξk,	(12)
where ξk = ρξk +，1 - P PN=I √pCξc. Since the variance of i.i.d variables is additive, it is clear
that ξk follows the standard d-dimensional Gaussian distribution. The inclusion of the correlated
noise implicitly reduces the temperature for each client and naturally yields a trade-off between
federation and accuracy. We refer to the algorithm with correlated noise as the hybrid federated
averaging Langevin dynamics (hFA-LD) and present it in Algorithm 2.
Since the inclusion of correlated noise doesn’t affect the iterate of Eq. (12), the algorithm property
maintains the same except the scale of the temperature τ and efficacy of federation are changed.
Based on a target correlation coefficient ρ ≥ 0, Eq. (11) is equivalent to applying a temperature
Tc,ρ = T(ρ2 + (1 - ρ2)∕pc). In particular, setting P = 0 leads to TC,o = τ∕pc, which exactly recovers
Algorithm 1; however, setting ρ = 1 leads to Tc,1 = τ, where the injected noise in local clients is
reduced by 1∕pc times. Now we adjust the analysis as follows
Theorem 5.9 (Informal version of Theorem B.8). Assume assumptions 5.1, 5.2, and 5.3 hold.
Consider Algorithm 2 With a correlation coefficient P ∈ [0,1], η ∈ (0,立]and ∣∣θC — θ*∣∣2 ≤ dD2
for any c ∈ [N], we have
W2(μk,∏) ≤ (1 - ηm∕4)k ∙ ^√2d(D + pτ∕m)) + 30κpηmd ∙ J(K2 + κ)Hρ,
22
where μk denotes the probability measure of θk, HP := D2 + mɪ- maxc∈[N] Tc,ρ + ^^ + 枭.
Such a mechanism leads to a trade-off between the efficacy of federation and accuracy and motivates
us to exploit the optimal P under the differential-privacy theories Wang et al. (2015).
5.4	Partial device participation
Full device participation enjoys appealing convergence properties. However, it suffers from the
straggler’s effect in real-world applications, where the communication is limited by the slowest
device. Partial device participation handles this issue by only allowing a small portion of devices in
each communication and greatly increased the communication efficiency in a federated network.
The first device-sampling scheme I Li et al. (2020b) selects a total of S devices, where the c-th device
is selected with a probability pc . The first theoretical justification for convex optimization has been
proposed by Li et al. (2020c).
(Scheme I: with replacement). Assume Sk = {nι, n2,…,ns}, where n7- ∈ [N] is a random
number that takes a value of C with a probability PC for any j ∈ {1,2, ∙∙∙ ,S}. The synchronization
step follows that θk = S Pc∈sfc θC.
Another strategy is to uniformly select S devices without replacement. We follow Li et al. (2020c)
and assume S indices are selected uniformly without replacement. In addition, the convergence also
requires an additional assumption on balanced data Li et al. (2020c).
8
Under review as a conference paper at ICLR 2022
(Scheme II: without replacement). Assume Sk = {n1,n2,…，ns}, where n7- ∈ [N] is a
random number that takes a value of C with a probability S for any j ∈ {1, 2, •一，S}. Assume
the data is balanced such that pi = … =PN = N. The synchronization step follows that
θk = N P- pcθk = S Pc- θk.
Algorithm 3 Hybrid federated Averaging Langevin dynamics Algorithm (FA-LD) with partial device
participation, informal version of Algorithm 6. Sk is sampled according to a device-sampling rule
based on SCheme I or II._____________________________________________________________
1:	〜
βk+i = θk - η▽产(ΘC) + √2ητρ2ξk + √2η(i - ρ2)τ∕pcξc,
2:
(βc+ι	if k + 1 mod K = 0
θkc+1 =
I Pc∈Sk+ι Sβk+1	if k + 1 mod K = 0.
Theorem 5.10 (Informal version of Theorem C.3). Assume assumptions 5.1, 5.2, and 5.3 hold.
Consider Algorithm 3 with a hyperparameter P ∈ [0,1], a fixed learning rate η ∈ (0,芸]and
∣∣θg — θ*k2 ≤ dD2 forany c ∈ [N], we have
W2(μk,∏) ≤ (1 — ηm∕4)k ∙ (√2d(D + PTTm))
+ 30κPηmd ∙ qHρ(K2 + K) + O QS(ρ2 + N(1 — ρ2))Cs),
where CS = 1 for SchemeIand CS = NN-SS for Scheme II.
We observe that partial device participation leads to an extra bias regardless of the scale of η. To
reduce such a bias, we suggest to consider highly correlated injected noise, such as P = 1, to reduce
the impact of the injected noise. By setting O( ,d∕S) ≤ e/3 and following a similar learning rate
as in section 5.3.1, we can achieve the precision e within Ω(e-2dlog(d∕e)) iterations given a large
number of devices satisfying S = Ω(e-2d).
The device-sampling scheme I provides a viable solution to handle the straggler’s effect in full device
participation and greatly accelerates the communication efficiency. In addition, scheme I is rather
robust to the data heterogeneity and doesn’t require the data to be balanced. In other words, this
device-sampling scheme is more preferred if a system is free to activate any devices at any time.
In more practical cases where a system can only operate based on the first S messages for the local
updates. The device-sampling scheme II proposes a concrete treatment to tackle this issue. Given
a balanced data across different clients and each device is uniformly sampled, we can achieve a
reasonable approximation. If S = 1, our Scheme II matches the result in the Scheme I. If S = N,
then our Scheme II recovers the result in the full device setting. If S = N - o(N), then our Scheme
II bound is better than scheme I.
6 Conclusion and future work
We propose a novel convergence analysis for federated averaging Langevin dynamics (FA-LD) with
distributed clients. Our results no longer require the bounded gradient assumption in `2 norm as in
the optimization-driven literature in federated learning. The theoretical guarantees yield a concrete
guidance on the selection of the optimal number of local updates. In addition, the convergence highly
depends on the data heterogeneity and the injected noises, where the latter also inspires us to consider
correlated injected noise to balance between the efficacy of federation and accuracy.
Our work initiated the theoretical study of standard sampling algorithms in federated learning and
paved the way for future works of advanced Monte Carlo methods, such as underdamped Langevin
dynamics Cheng et al. (2018), replica exchange Monte Carlo (also known as parallel tempering)
Deng et al. (2020) in federated learning. It is also interesting to study the optimal number of local
steps under the non-strongly convex Dalalyan (2017b) or non-convex assumptions Raginsky et al.
(2017); Ma et al. (2019).
9
Under review as a conference paper at ICLR 2022
References
Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian Posterior Sampling via Stochastic
Gradient Fisher Scoring. In Proc. of the International Conference on Machine Learning (ICML),
2012.
Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed Stochastic Gradient MCMC. In
International Conference on Machine Learning (ICML), 2014.
Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated Learning
via Posterior Averaging: A New Perspective and Practical Algorithms. In ICLR. arXiv:2010.05273,
2021.
Zachary Charles and Jakub Konecny. On the Outsized Importance of Learning Rates in Local Update
Methods. arXiv:2007.00878, 2020.
Changyou Chen, Nan Ding, and Lawrence Carin. On the Convergence of Stochastic Gradient MCMC
Algorithms with High-order Integrators. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 2278-2286, 2015.
Changyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, and Lawrence Carin. Stochastic Gradient
MCMC with Stale Gradients. In Advances in Neural Information Processing Systems (NeurIPS),
2016.
Mingzhe Chen, Zhaohui Yang, Walid Saad, Changchuan Yin, H Vincent Poor, and Shuguang Cui. A
Joint Learning and Communications Framework for Federated Learning over Wireless Networks.
IEEE Trans. on Wireless Communications, 2020.
Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. In
Proc. of the International Conference on Machine Learning (ICML), 2014.
Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped Langevin
MCMC: A non-asymptotic analysis. In Conference on Learning Theory (COLT), pp. 300-323.
PMLR, 2018.
Arkabandhu Chowdhury and Chris Jermaine. Parallel and Distributed MCMC via Shepherding
Distributions. In AISTAT, 2018.
Arnak S. Dalalyan. Further and Stronger Analogy Between Sampling and Optimization: Langevin
Monte Carlo and Gradient Descent. In Conference on Learning Theory (COLT), June 2017a.
Arnak S Dalalyan. Theoretical Guarantees for Approximate Sampling from Smooth and Log-concave
Densities. Journal of the Royal Statistical Society: Series B, 79(3):651-676, 2017b.
Arnak S Dalalyan and Avetik Karagulyan. User-friendly Guarantees for the Langevin Monte Carlo
with Inaccurate Gradient. Stochastic Processes and their Applications, 129(12):5278-5311, 2019.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large Scale Distributed Deep Networks. In
Advances in neural information processing systems (NeurIPS), pp. 1223-1231, 2012.
Wei Deng, Qi Feng, Liyao Gao, Faming Liang, and Guang Lin. Non-Convex Learning via Replica
Exchange Stochastic Gradient MCMC. In Proc. of the International Conference on Machine
Learning (ICML), 2020.
Alain Durmus and Eric Moulines. Sampling from a Strongly Log-concave Distribution with the
Unadjusted Langevin Algorithm. arXiv:1605.01559, 2016.
Mert Gurbuzbalaban, Xuefeng Gao, Yuanhan Hu, and Lingjiong Zhu. Decentralized Stochastic
Gradient Langevin Dynamics and Hamiltonian Monte Carlo. arXiv:2007.00590v3, 2021.
Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. FL-NTK: A Neural Tangent Kernel-based
Framework for Federated Learning Convergence Analysis. In International Conference on Machine
Learning (ICML), 2021.
10
Under review as a conference paper at ICLR 2022
Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. TextHide: Tackling Data
Privacy in Language Understanding Tasks. In EMNLP, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and Gener-
alization in Neural Networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571-8580, 2018.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic Controlled Averaging for Federated Learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtdrik. First Analysis of Local GD on
Heterogeneous Data. arXiv:1909.04715, 2019.
Anastasia Koloskova, Sebastian U.Stich, and Martin Jaggi. Decentralized Stochastic Optimization
and Gossip Algorithms with Compressed Communication. In Proc. of the International Conference
on Machine Learning (ICML), 2019.
Chunyuan Li, Changyou Chen, Yunchen Pu, Ricardo Henao, and Lawrence Carin. Communication-
Efficient Stochastic Gradient MCMC for Neural Networks. In Proc. of the National Conference on
Artificial Intelligence (AAAI), 2019a.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated Learning: Challenges,
Methods, and Future Directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smithy.
Federated Optimization in Heterogeneous Networks. In Proceedings of the 3rd MLSys Conference,
2020b.
Wenqi Li, Fausto Milletari, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian
Baust, Yan Cheng, Sebastien Ourselin, M Jorge Cardoso, et al. Privacy-preserving Federated Brain
Tumour Segmentation. In International Workshop on Machine Learning in Medical Imaging, pp.
133-141. Springer, 2019b.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of
FedAvg on Non-IID Data. In Proc. of the International Conference on Learning Representation
(ICLR), 2020c.
Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib, Pamela Ventola, and James S Duncan.
Multi-site fMRI Analysis using Privacy-preserving Federated Learning and Domain Adaptation:
ABIDE Results. Medical Image Analysis, 2020d.
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning on
Non-IID Features via Local Batch Normalization. In International Conference on Learning Repre-
sentations (ICLR), 2021. URL https://openreview.net/forum?id=6YEQUn0QICG.
Xuechen Li, Denny Wu, Lester Mackey, and Murat A. Erdogdu. Stochastic Runge-Kutta Accelerates
Langevin Monte Carlo and Beyond. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 7746-7758, 2019c.
Y.-A Ma, E. B. Fox, T. Chen, and L. Wu. Irreversible samplers from jump and continuous Markov
processes. Stat. Comput., pp. 1-26, 2018.
Y.-A. Ma, N. S. Chatterji, X. Cheng, N. Flammarion, P. L. Bartlett, and M. I. Jordan. Is there an
analog of Nesterov acceleration for MCMC? Bernoulli, 27:1942-1992, 2021.
Yi-An Ma, Tianqi Chen, and Emily B. Fox. A Complete Recipe for Stochastic Gradient MCMC. In
Neural Information Processing Systems (NeurIPS), 2015.
Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I. Jordan. Sampling Can Be
Faster Than Optimization. PNAS, 2019.
Oren Mangoubi and Nisheeth K. Vishnoi. Convex Optimization with Unbounded Nonconvex Oracles
using Simulated Annealing. In Proc. of Conference on Learning Theory (COLT), 2018.
11
Under review as a conference paper at ICLR 2022
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics ,pp.1273-1282. PMLR, 2017.
H. McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Areas. Federated Learning of Deep
Networks using Model Averaging. 2016.
S. Minsker, S. Srivastava, L. Lin, and D. B. Dunson. Scalable and Robust Bayesian Inference via the
Median Posterior. In International Conference on Machine Learning (ICML), 2014.
Wenlong Mou, Yi-An Ma, Martin J. Wainwright, Peter L. Bartlett, and Michael I. Jordan. High-Order
Langevin Diffusion Yields an Accelerated MCMC Algorithm. Journal of Machine Learning
Research (JMLR), 22:1-41, 2021.
W. Neiswanger, C. Wang, and E. Xing. Asymptotically Exact, Embarrassingly Parallel MCMC.
arXiv:1311.4780, 2013.
Y. Nesterov. Introductory Lectures on Convex Optimization, in: Applied Optimization. Kluwer
Academic Publishers, Boston, MA, 2004.
R. Nishihara, I. Murray, and R. P. Adams. Parallel MCMC with Generalized Elliptical Slice Sampling.
Journal of Machine Learning Research, 15(1):2087-2112, 2014.
Reese Pathaky and Martin J. Wainwright. Fedsplit: An Algorithmic Framework for Fast Federated
Optimization. arXiv:2005.05238, 2020.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex Learning via Stochastic
Gradient Langevin Dynamics: a Nonasymptotic Analysis. In Conference on Learning Theory,
June 2017.
R. Shen and Y. T. Lee. The randomized midpoint method for log-concave sampling. In Advances in
Neural Information Processing Systems, pp. 2098-2109, 2019.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving Deep Learning. In SIGSAC conference on
computer and communications security (CCS). ACM, 2015.
Sebastian U. Stich. Local SGD Converges Fast and Communicates Little. arXiv:1805.09767v3, 2019.
Yee Whye Teh, Alexandre Thiery, and Sebastian Vollmer. Consistency and Fluctuations for Stochastic
Gradient Langevin Dynamics. Journal of Machine Learning Research, 17:1-33, 2016.
Sebastian J. Vollmer, Konstantinos C. Zygalakis, and Yee Whye Teh. Exploration of the (Non-)
Asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics. Journal of Machine
Learning Research, 17(159):1-48, 2016.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive Federated Learning in Resource Constrained Edge Computing Systems.
IEEE Journal on Selected Areas in Communications, 37(6):1205-1221, 2019.
Xiangyu Wang and David B. Dunson. Parallelizing MCMC via Weierstrass Sampler.
Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for Free: Posterior Sampling and
Stochastic Gradient Monte Carlo. In ICML, pp. 2493-2502, 2015.
Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In
International Conference on Machine Learning, 2011.
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs Local SGD for Heteroge-
neous Distributed Learning. arXiv:2006.04735, 2020.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel Restarted SGD with Faster Convergence and Less
Communication: Demystifying Why Model Averaging Works for Deep Learning. In In Proc. of
Conference on Artificial Intelligence (AAAI), 2019.
12
Under review as a conference paper at ICLR 2022
Yuchen Zhang, Percy Liang, and Moses Charikar. A Hitting Time Analysis of Stochastic Gradient
Langevin Dynamics. In Proc. ofConference on Learning Theory (COLT), pp. 1980-2022, 2017.
Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang. Federated meta-learning for fraudulent credit
card detection. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial
Intelligence (IJCAI), 2020.
13
Under review as a conference paper at ICLR 2022
Roadmap. In Section A, we layout the formulation of the algorithm, basic notations, and definitions.
In Section B, we present the main convergence analysis for full device participation. We discuss
the optimal number of local updates based on a fixed learning rate, the acceleration achieved by
varying learning rates, and the privacy-accuracy trade-off through correlated noises. In Section C,
we analyze the convergence of partial device participation through two device-sampling schemes.
In Section D, we provide lemmas to upper bound the contraction, discretization and divergence for
proving the main convergence results. In Section E, we include supporting lemmas to prove results in
the previous section. In Section F, we establish the initial condition.
A Preliminaries
A. 1 Basic notations and backgrounds
Let N denote the number of clients. Let T denote the number of global steps to achieve the precision
e. Let K denote the number of local steps. For each C ∈ [N] := {1,2,…，N}, We use fC and VfC
denote the loss function and gradient of the function fc in client c. For the stochastic gradient oracle,
We denote by Vf c(∙) the unbiased estimate of the exact gradient VfC of client c. In addition, We
denote pC as the Weight of the c-th client such that pC ≥ 0 and PCN=1 pC = 1. ξkC is an independent
standard d-dimensional Gaussian vector at iteration k for each client c ∈ [N] and ξk is a unique
Gaussian vector shared by all the clients.
Algorithm 4 Federated averaging Langevin dynamics algorithm (FA-LD). Denote by θkC the model
parameter in the c-th client at the k-th step. Denote the immediate result of one step SGLD update
from θkC by βkC. ξkC is an independent standard d-dimensional Gaussian vector at iteration k for each
client c ∈ [N]. A global synchronization is conducted every K steps. This is a complete version of
Algorithm 1.
1:	〜
βk+ι = θC - ηVfc(θk) + √2ητ∕pcξC,	(13)
2:
(βC+ι	if k + 1 mod K = 0
θkC+1 =	N	(14)
[ PC=1 PcβC+ι	if k + 1 mod K = 0.
Inspired by Li et al. (2020c), We define tWo virtual sequences
NN
βk =XpCβkC,	θk =XpCθkC,	(15)
C=1	C=1
Which are both inaccessible when k mod K 6= 0. For the gradients and injected noise, We also define
N	NN
Vf (θk ) = X PcVf c(θC), Ve(θk ) = X PcVfc(θk), ξk = X √cξ.	(16)
In What folloWs, it is clear that EVf (θ) = PC=1 PCEVf C(θC) = Vf (θ) for any θC ∈ Rd and any
c ∈ [N]. Summing Eq.(13) from clients c = 1 to N and combining Eq.(15) and Eq.(16), We have
βk+ι = θk - ηVf(θk) + p2ητξk.	(17)
Moreover, We alWays have βk = θk Whether k + 1 mod E = 0 or not by Eq.(14) and Eq.(15). In
what follows, we can write	〜
θk+ι = θk — ηVf(θk) + p2ητξk,	(18)
which resembles the SGLD algorithm Welling & Teh (2011) except that the construction of stochastic
gradients is different and θk is not accessible when k mod K 6= 0. To facilitate the analysis, we also
define an auxiliary continuous-time processes (θt)t≥o
d& = —Vf(θt) ∙ dt + √2τ ∙ dWt,	(19)
14
Under review as a conference paper at ICLR 2022
where Ot = PN=IPcθt, Vf (θθt) = PN=I PC^fc(θt),用 is the continuous-time variable at client
c, and W is a d-dimensional BroWnian motion. The continuous-time algorithm is referred to as
Federated Averaging Langevin diffusion and is described as
dβc = -Vfc(θC) ∙ dt + √2T∕p: ∙ dWC
N
θOtc = XpcβOtc.
c=1
Since the synchronization step is conducted at every time step t, the Federated Averaging Langevin
diffusion performs the same as the standard Langevin diffusion with the temperature τ and conver-
gences to the stationary distribution π(θ) a exp(-f (θ)∕τ), where f (θ) = PN=I PCf c(θ). Assume
that oo simulates from the stationary distribution π, then it follows that Ot ~ π for any t ≥ 0.
A.2 Assumptions and definitions
Assumption A.1 (Smoothness). For each c ∈ [N], we say fc is L-smooth if for some L > 0
fc(y) ≤ fc(χ) + NfC(X),y — Xi + Lky — χk2 ∀χ,y ∈ Rd.
Note that the above assumption is equivalent to saying that
kVfc(y)-Vfc(X)k2 ≤Lky-Xk2,	∀X,y∈Rd.
Assumption A.2 (Strong convexity). For each c ∈ [N], fc is m-strongly convex if for some m > 0
fc(x) ≥ fc(y) + hVfc(y),χ - yi + mky - χ∣∣2 ∀χ,y ∈ Rd.
An alternative formulation for strong convexity is that
hVfc(X) - Vf c(y), X - yi ≥ mkX - yk22 ∀X,y ∈ Rd.
Assumption A.3 (Bounded variance, restatement of Assumption 5.3). For each c ∈ [N], the variance
of noise in the stochastic gradient Vfc (X) in each client is upper bounded such that
E[kVfec(X) - Vfc(X)k22] ≤ σ2d, ∀X ∈ Rd.
The bounded variance in the stochastic gradient is a rather standard assumption and has been widely
used in Cheng et al. (2018); Dalalyan & Karagulyan (2019); Li et al. (2020c). Extension of bounded
variance to unbounded cases such as E[kVfec(X) - Vfc(X)k22] ≤ δ(L2X2 + B2) for some M and
δ ∈ [0, 1) is quite straightforward and has been adopted in assumption A.4 stated in Raginsky et al.
(2017). The proof framework remains the same.
Quality of non-i.i.d data Denote by θ* the global minimum of f. Next, we quantify the degree
of the non-i.i.d data by Y := maXc∈[N ] ∣∣Vf c(θ*) |卜，which is a non-negative constant and yields a
smaller scale if the data is more evenly distributed.
Definition A.4. We define parameter Tc,ρ Hρ2, κ and γ2
Tc,ρ := τ(ρ2 + (1 - ρ2)∕Pc),
|D{z2}
initialization
γ2	σ2
---+ι	+	-9	∙
m2 d	m2
l{z}	l{z}
data heterogeneity stochastic noise
+m max，，+
、----V----}
injected noise
κ := L∕m,
Y 2 := cmaχ]kvf c(θ*)k2.
15
Under review as a conference paper at ICLR 2022
B	Full device participation
B.1 One-step update
Wasserstein distance We define the 2-Wasserstein distance between a pair of Borel probability
measures μ and V on Rd as follows
1
W2(μ,ν ):=	inf k k kβμ - βνk2dγ 2(βμ, βν)),
γ2∈Couplings(μ,ν) ∖
where ∣∣∙∣∣2 denotes the '2 norm on Rd and the pair of random variables (βμ, βν) ∈ Rd X Rd is a
coupling with the marginals following L(βμ) = μ and L(βν) = V. L(∙) denotes a distribution of a
random variable.
The following result provides a crucial contraction property based on distributed clients with infre-
quent synchronizations.
Lemma B.1 (Dominated contraction property, restatement of Lemma 5.4). Assume assumptions A.1
and A.2 hold. For any learning rate η ∈ (0, L+m], any {θc}N=1, {βc}N=1 ∈ Rd, we have
N
kβ - θ - η(Vf(β) - Vf(θ))∣∣2 ≤ (i-ηm) ∙kβ - θ∣∣2 + 4ηLXpc ∙ (∣∣βc - β∣∣2 + ∣∣θc - θ∣∣2).
c=1
where β = PcN=1 pcβc, θ = PcN=1 pcθc, Vf(θ) = PcN=1 pcVfc(θc), and Vf(β) =
PcN=1 pcVfc (βc). We postpone the proof into Section D.1. The above result implies that as
long as the local parameters θc, βc and global θ, β don’t differ each other too much, we can guarantee
the desired convergence.
The following result ensures a bounded gap between 0； and GnbSC in '2 norm for any S ≥ 0 and
c ∈ [N]. We postpone the proof of Lemma B.2 into Section D.2.
Lemma B.2 (Discretization error). Assume assumptions A.1, A.2, and A.3 hold. For any s ≥ 0, any
learning rate η ∈ (0, 2/m) and ∣∣θ; 一 θ* ∣∣2 ≤ dD2 for any C ∈ [N], the iterates of (Gs) based on the
continuous dynamics of Eq.(19) satisfy the following estimate
EllGC - Gnb S C ∣∣2 ≤ 8η2 dκ(κγ^ + LT) + 16ηdτ.
The following result shows that given a finite number of local steps K , the divergence between θc in
local client and θ in the center is bounded in '2 norm. Notably, since the non-differentiable Brownian
motion leads to a lower order term O(η) instead of O(η2) in '2 norm, a naive proof may lead to a
crude upper bound. We delay the proof of Lemma B.3 into Section D.3.
Lemma B.3 (Bounded divergence, restatement of Lemma 5.5). Assume assumptions A.1, A.2, and
A.3 hold. For any learning rate η ∈ (0, 2/m) and ∣∣θC — θ* ∣2 ≤ dD2 for any C ∈ [N], we have the
'2 upper bound of the divergence between local clients and the center as follows
N
XpcE∣θkc -θk∣22 ≤ 112(K - 1)2η2dL2Hρ + 8(K - 1)ηdτ(ρ2 + N(1 - ρ2)),
c=1
where Hρ , κ and γ2 are defined as Definition A.4.
The following presents a standard result for bounding the gap between Vf (θ) and Vf (θ). We delay
the proof of Lemma B.4 into Setion D.
Lemma B.4 (Bounded variance). Given assumption A.3, we have
E∣Vf(θ)-Vf(θ)∣2 ≤ d ∙σ2,	∀ θ ∈ Rd.
Having all the preliminary results ready, now we present a crucial lemma for proving the convergence
of all the algorithms.
16
Under review as a conference paper at ICLR 2022
Lemma B.5 (One step update, restatement of Lemma 5.6). Assume assumptions A.1, A.2, and A.3
hold. Consider Algorithm 4 with independently injected noise P = 0, any learning rate η ∈ (0,4)
and ∣∣θC 一 θ* k2 ≤ dD2 for any C ∈ [N ], where θ* is the global minimum for the function f. Then
w2(μk+ι,π) ≤ (1 - ^^2^) ∙ W2 (μk, π) + 400η2dL2Ho((K -I)2 + K),
where μk denotes the probability measure of θk, Ho, K and Y2 are defined as Definition A.4.
Proof. The solution of the continuous-time process Eq.(19) follows that
心
θo 一 /'vf(θs)ds + √2T ∙ Wt,
0
∀t ≥ 0.
(20)
Set t → (k + 1)η and &
W(k+i)η 一 Wkn= √ηξk
→ θkn for Eq.(20) and consider a synchronous coupling such that
θ(k+1)η
θkη
(k+1)η
kη
vf (θs)ds + √2τ(W(k+i)n - Wkn)
θkη
一 [(k+1)n vf(θs)ds + p2τηξk
kn
(21)
. - _ =，~ 、 _ ~ ~ 、一一 .... ...............................................
We first denote ζk := vf(θk) 一 vf (θk). Subtracting Eq.(18) from Eq.(21) yields that
θ(k+1)n 一 θk+1
θkn
一 θk
(k+1)n
+ηvfe(θk) 一
kn
Vf(θs)ds
(k+1)n
Gkn 一 θk 一 η Vf((θk + Gkn 一 θk ) 一 v f(θk ) J 一 J
-Vf(θkn) ds
(22)
(k+1)n
Gkn —。®— η( Vf(θk + Gkn - 9Q—Vf(9k) \ - J	(Vf(Gs)—Vf (Gkn ))ds+ηZk.
=Xk	S  -----------V-------------}
:=Yk
Taking square and expectation on both sides, we have
E∣θG(k+1)n 一 θk+1 ∣22
= E∣θGkn 一 θk 一 ηXk 一 Yk ∣22 + E∣ηζk ∣22 + 2η EhθGkn 一 θk 一 ηXk 一 Yk, ζki
X------------{------------}
Eζk =0 and mutual independence
≤ (1 + q) ∙ EIIGkn 一 θk 一 ηXk k2 + (1 + 1∕q) ∙ EIlYk ∣∣2 + EknZkk2
N
≤ (1 + q)・((1一 nm) ∙EkGkn-θk k2 +4「L X Pc ∙ (EkGkn- Gknk2 + 旧口优-Ok k2))
c=1
+ (1 + 1/q) ∙ EkYk∣∣2 + n2σ2d
≤ (1 + q) ∙ f (1 -nm) EkGkn 一 θk k2 + 448n3d(K 一 1)2L3Ho + 32(K 一 1)η2dLτN^
φ
+ (1 + 1/q) ∙ EkYkk2 + η2σ2d,	(23)
where the first inequality follows by the AM-GM inequality for any q > 0, the second inequality
follows by Lemma B.1 and Assumption A.3. The third inequality follows by Lemma B.3 with P = 0;
moreover, the continuous-time process conducts synchronization at any time step, hence θGkcn = θGkn .
Since the learning rate follows 芸 ≤ m^ ≤ m, the requirement of the learning rate for Lemma B.1
and Lemma B.3 is clearly satisfied.
17
Under review as a conference paper at ICLR 2022
Recall that φ = 1 - ηm, We get 1++φ =
1 - 2 ηm. In addition, we have 1 + 1 =
(1 + q) ∙ (1 一 ηm) ≤ 1 一 ∣ηm,
1 - 1 ηm. Choose q = 1+φφ - 1 so that (1 + q)φ =
=1+q = 1-φ ≤ ηm .Itfollowsthat
1 — 1 ηm
1 + q ≤	2	≤ 1.5,
1 - ηm
2
(1+1㈤≤而,
(1+φ)
2
(24)
where the second inequality holds because η ∈ (0,4]≤ *.
For the term EkYk k22 in Eq.(23), we have the following estimate
(
EkYkk22 = E
kη
2
(k+1)η
-Vf (θkη) ds
2
≤ η I(k+1)ηE||vf®S)-Vf(%)∣∣2ds
kη
Z (k+1)η
kη
XXPC(VfC(阳-Vfc(%))∣ ds
(k+1)η N	2
≤ η	£p「e∣∣Vfc闸-Vfc(%)∣∣2ds
kη c=1
(k+1)η N	2
≤ ηL2 /	EPC ∙ e∣BC -%∣∣2ds
kη c=1
≤ ηL2 Zk(k+i)η (βη2dκ(κγ2 + LT) + 16ηdτ) ds
=8η4dL4H0 + 16η3L2dτ,
(25)
where the first inequality follows by Holder,s inequality, the second inequality follows by Jensen's
inequality, the third inequality follows by Assumption A.1, and the last inequality follows by Lemma
B.2. The last equality holds since KY2 + Lτ ≤ LmHo and K = L/m.
Plugging Eq.(24) and Eq.(25) into Eq.(23), we have
Ek^(k + 1)η - θk + 1k2 ≤ (1 - -ɪ) ∙ EMkn - θk k2
+ 672η3d(K - 1)2L3H0 + 48η2d(K - 1)Lτ N
L2
+ 16η3dL3κHo + 32η2d一T + η2σ2d.
m
Choose the specific Langevin diffusion θ in stationary regime, we have W3(μk, π) = EMkn - θk k2
and W2(μk+ι,π) ≤ E||「(k+i)n - θk+ι∣∣2∙ Arranging the terms, we have
W2 (μk+1, π) ≤ (1 - -ɪ) ∙ W2 (μk,π) + 400η2dL2HO((K -I)2 + K),
where η ≤ 芸,K ≥ 1, mτ ≤ Lτ ≤ LtN ≤ L maXc∈[N] TC,o ≤ LmHo, and σ2 ≤ L2Ho are
applied to the result.
□
B.2 Convergence via independent noises
Theorem B.6 (Restatement of Theorem 5.7). Assume assumptions A.1, A.2, and A.3 hold. Consider
Algorithm 4 with a fixed learning rate η ∈ (0,立]and ∣∣Θq — θ* k2 ≤ dD2 for any C ∈ [N ], we have
W2(μk,∏) ≤(1 - ηm) ∙ (√2d(D + pτ∕m)) + 30κpηmd ∙ P((K - 1)2 + κ)Ho.
where μk denotes the probability measure of θk, Ho, K and Y2 are defined as Definition A.4.
18
Under review as a conference paper at ICLR 2022
Proof. Iteratively applying Theorem B.5 and arranging terms, we have that
「	ι -(i - ηm )k∕
W2(μo,π) +	ηm^ 400η2dL2HO((K - I)2 + K)
1 - (I	2^) ∖
W2 (μo, ∏) +-(400η2dL2Ho((K — 1)2 + K))
ηm
W2 (μo, π) + 800κ2ηmd((K — 1)2 + κ)H0,
(26)
where K = L .By Lemma F.1 and the initialization condition ∣∣θC — θ* k2 ≤ dD2 for any C ∈ [N ],
we have that
W2(μ0, ∏) ≤ √2d(D + pτ∕m).
Applying the inequality (1 — ηm) ≤ (1 — ηmm)2 completes the proof.
□
Discussions
Optimal choice of K. To ensure the algorithm to achieve the precision based on the total number
of steps T and the learning rate η, we can set
-ηm T≡ √√2d(D+pr/m)) ≤ ∣.
This directly leads to
ηm ≤ min 1条。(2((K [丁 + E )], T ≥ ◎( log≡).
2L	dK2 ((K — 1)2 + K)H0	mη
Plugging into the upper bound of η, it implies that to reach the precision level , it suffices to set
Ti(dκ2((K -2)2 + κ)H0 ∙ log (d
Since H0 = Ω(D2 + m), we observe that the number of communication rounds is around the order
T
K
ω (K + V)
where the value of T first decreases and then increases with respect to K, indicating that setting K
either too large or too small may lead to high communication costs and hurt the performance. Ideally,
K should be selected in the scale of Ω(√κ). Combining the definition of Te in Eq.(27), this suggests
an interesting result that the optimal K should be in the order of O(√T). Similar results have been
achieved by Stich (2019); Li et al. (2020c).
B.3 Convergence via varying learning rates
Theorem B.7 (Restatement of Theorem 5.8). Assume assumptions A.1, A.2, and A.3 hold. Consider
Algorithm 4 with an initialization satisfying ∣∣Θq — θ* ∣2 ≤ dD2 forany c ∈ [N] and varying learning
rate following
1
ηk =2L +(1∕12)mk,	k =1，2，…
Then for any k ≥ 0, we have
W2(μk,∏) ≤ 45κP((K - 1)2 + κ)Ho ∙(加md)"2,	Vk ≥ 0,
19
Under review as a conference paper at ICLR 2022
Proof. We first denote
CK = 30κP((K - 1)2 + κ)Ho.
Next, we proceed to show the following inequality by the induction method
W2(μk,∏) ≤ 1.5Cκ (oS" 卜Y =1.5Cκ(ηkmd)“2,	Vk ≥ 0,
2L+ (1/12)mk
where the decreasing learning rate follows that
_	1
ηk = 2L + (1/12)mk.
(i)	For the case of k = 0, since
CK ≥ 4√κpH0 ≥	4√K Jd2 +	— max Tc 0	≥ 4pκ∕d√√dD2	+ ↑ —	max Tc 0
m c∈[N]	,	m	c∈[N]	,
≥ 4pκ∕dW2(μo,π),
where the last inequality follows by Lemma F.1 and ∣∣Θq 一 θ* k2 ≤ dD2 for any C ∈ [N].
(28)
(29)
It is clear that W2(μo,∏) ≤ 4 CKqm ≤ 1.5Cκ√η0md byEq.(29).
(ii)	If now that Eq.(28) holds for some k ≥ 0, it follows by Lemma B.5 that
W2(μk+ι,π) ≤ (1 一 ) ∙ W2(μk,π)+ 400η2dL2Ho((K -I)2 + K)
22
≤ (I-等)∙ W22(μk,∏) + ηmCKd
≤ (I-等)∙ 2.25CKnkmd + ηk3m2.25CKnkmd
≤ (1 - "6—) ∙ 2.25CKnkmd.
Since (1 - η6m) ≤ (1 -喑)2, we have
W2 (μk+1, π) ≤ (1 - "12 ) ∙ 1.5C K(nkmd)..
To prove W2(μk+1,∏) ≤ 1.5Ck (nk+ιmd)1/2, it suffices to show (1 - ηk2m)n1/2 ≤ nk+ι, which is
detailed as follows
(I	nkm) 1/2 _ √12(24L + mk - m)
(-可Jnk = —(24L + mk)3/2一
< √12(24L + mk - m)1/2
≤	24L + mk
√12	=
一(24L + m(k + 1))1/2 := nk+1,
where the last inequality follows since
(24L + mk - m)(24L + mk + m)) ≤ (24L + mk)2.
□
The above result implies that to achieve the precision , we require
md	1/2
W2(μk,π) ≤ 1.5CK(2L +(1∕12)mk)	≤ e.
The means that we only require k = Ω(^⅛) to achieve the precision e. By contrast, the fixed learning
rate requires Te = Ω (*∙ log (d∕e)), which is much slower than the algorithm with varying learning
rate by O log d∕e times.
20
Under review as a conference paper at ICLR 2022
B.4 Privacy-accuracy trade-off via correlated noises
Note that Algorithm 4 requires all the local clients to generate the independent noise ξkc . Such a
mechanism enjoys the convenience of the implementation and yields a potential to protect the privacy
of data and alleviates the security issue. However, the scale of noises is maximized and inevitable
slows down the convergence. For extensions, it can be naturally generalized to correlated noise
based on a hyperparameter, namely the correlation coefficient ρ between different clients. Replacing
Eq.(13) with
βc+ι = θc - η▽产(θk) + pητρ2ξk + P2η(i-ρ2)τ∕pc薪,	(30)
where ξk is a d-dimensional standard Gaussian vector shared by all the clients at iteration k, ξkc is a
unique d-dimensional Gaussian vector generated by client c ∈ [N] only. Moreover, ξk is dependent
with ξkc for any c ∈ [N]. Following the same synchronization step based Eq.(14), we have
θk+ι = θk - ηVfe(θk) + P2ηTξk,	(31)
where ξk = ρξk +，1 - P PN=I √pCξC. Since the variance of i.i.d variables is additive, it is clear
that ξk follows the standard d-dimensional Gaussian distribution. The inclusion of the correlated
noise implicitly reduces the temperature and naturally yields a trade-off between federation and
accuracy. We refer to the algorithm with correlated noise as the generalized Federated Averaging
Langevin dynamics (gFA-LD) and present it in Algorithm 5.
Since the inclusion of correlated noise doesn’t affect the formulation of Eq.(31), the algorithm
property maintains the same except the scale of the temperature τ and federation are changed.
Based on a target correlation coefficient ρ ≥ 0, Eq.(30) is equivalent to applying a temperature
Tc,ρ = τ(ρ2 + (1 - ρ2)∕pc). In particular, setting ρ = 0 leads to Tc,0 = (1 - ρ2)∕pc, which exactly
recovers Algorithm 4; however, setting ρ = 1 leads to Tc,1 = τ, where the injected noise in local
clients is reduced by 1∕pc times. Now we adjust the analysis as follows
Theorem B.8 (Restatement of Theorem 5.9). Assume assumptions A.1, A.2, and A.3 hold. Con-
SiderAlgorithm 5 with a correlation coefficient P ∈ [0,1], a fixed learning rate η ∈ (0,芸]and
∣∣θc — θ*∣∣2 ≤ dD2 forany C ∈ [N], we have
W2(μk,∏) ≤(1 — ηm) ∙ (√2d(D + Pτ∕m)) +30KPmd ∙ J((K - 1)2 + K)HP,
where μk denotes the probability measure of θk, Hρ, K and γ2 are defined as Definition A.4.
Algorithm 5 Hybrid federated averaging Langevin dynamics algorithm (hFA-LD). Denote by θkc the
model parameter in the C-th client at the k-th step. Denote the immediate result of one step SGLD
update from θkc by βkc. ξkc is an independent standard d-dimensional Gaussian vector at iteration k for
each client C ∈ [N] and ξk is a d-dimensional standard Gaussian vector shared by all the clients. P
denotes the correlation coefficient of the injected noises. A global synchronization is conducted every
K steps. This is a complete version of Algorithm 2.
1:	〜
βc+ι = θk — ηVfc(θk) + √2ητρ2ξk + /2η(i-ρ2)τ∕pc ξc,
2:
c=1 pcβkc+1
if k + 1 mod K 6= 0
if k + 1 mod K = 0.
C Partial device participation
Full device participation enjoys appealing convergence properties. However, it suffers from the
straggler’s effect in real-world applications, where the communication is limited by the slowest
device. Partial device participation handles this issue by only allowing a small portion of devices in
each communication and greatly increased the communication efficiency in a federated network.
21
Under review as a conference paper at ICLR 2022
C.1 Unbiased sampling schemes
The first device-sampling scheme I Li et al. (2020b) selects a total of S devices, where the c-th device
is selected with a probability pc . The first theoretical justification for convex optimization has been
proposed by Li et al. (2020c).
(Scheme I: with replacement). Assume Sk = {n1,n2,…,ns}, where n7- ∈ [N] is a random
number that takes a value of C with a probability Pc for any j ∈ {1,2, ∙∙∙ ,S}. The synchronization
step follows that θk = S Pc∈s^ θc.
Another strategy is to uniformly select S devices without replacement. We follow Li et al. (2020c)
and assume S indices are selected uniformly without replacement and the synchronization step is the
same as before. In addition, the convergence also requires an additional assumption on balanced data
Li et al. (2020c).
(Scheme II: without replacement). Assume Sk = {n1,n2,…，ns}, where n ∈ [N] is a
random number that takes a value of C with a probability S1 for any j ∈ {1, 2, •一，S}. Assume
the data is balanced such that pi = … =PN = N. The synchronization step follows that
θk = N Pc∈Sk pcθk = S Pc∈Sk θk.
Algorithm 6 Hybrid federated averaging Langevin dynamics algorithm (hFA-LD) with partial device
participation. ξkc is the independent Gaussian vector proposed by each client C ∈ [N] and ξk is a
unique Gaussian vector shared by all the clients. ρ denotes the correlation coefficient. A global
synchronization is conducted every K steps. Sk is a subset that contains S indices according to a
device-sampling rule based on scheme I or II. This is a complete version of Algorithm 3.
1:	~
βc+i = θk - η▽产(θc) + √2ητρ2ξk + √2η(i - ρ2)τ∕pcξc,
2:
βkc+1	ifk+ 1 modK 6= 0
θkc+1 =
1Pc∈Sk+ι Wβc+1 if k +1 mod K = 0.
Lemma C.1 (Unbiased sampling scheme). For any k mod K = 0 based on scheme I or II, we have
N
Eθk = E X θk = βk ：= XPcek.
c∈Sk	c=1
Proof. According to the definition of scheme I or II, we have θk = 1 Pc∈s^ θk. In what follows,
Eθk = SEPc∈Sk θc = S Pc0∈Sk PN=I Pcek = PN=IPcek, where Pi = P2 =…=PN for
scheme II in particular.	口
C.2 Bounded divergence based on partial device
Lemma C.2 (Bounded divergence based on partial device). Assume assumptions A.1, A.2, and A.3
hold. Consider Algorithm 6 with a correlation coefficient ρ ∈ [0, 1], any learning rate η ∈ (0, 2/m)
and ∣∣θc 一 θ*∣∣2 ≤ dD2 forany C ∈ [N], we have thefollowing results
For Scheme I, the divergence between θk and ek is upper bounded by
112	8
EIlek — θk∣∣2 ≤ ~^~K η dL HP + WKndT(P + N(I 一 P )).
SS
For Scheme II, assuming the data is balanced such that Pi = •一=PN = NN, the divergence between
θk and ek is upper bounded by
Ekβk - θkk2 ≤ VN一 S'(112K2η2dL2HP + 8Kηdτ(ρ2 + N(1 - ρ2)].
S(N - 1)
where HP, κ and γ2 are defined as Definition A.4.
22
Under review as a conference paper at ICLR 2022
Proof. We prove the bounded divergence for the two schemes, respectively.
For scheme I with replacement, Gk = Pc∈s^ 1 βC for a subset of indices Sk. Taking expectation
with respect to Sk, we have
1S	1N
Ekθk - βk k2 = S EEkeni- βkk2 = S Epc kβc - βkk2,	(32)
i=1	c=1
where the first equality follows by the independence and unbiasedness of θkni for any i ∈ [S ].
To further upper bound Eq.(32), we follow the same technique as in Lemma B.3. Since k mod K = 0,
k0 = k - K is also the communication time, which yields the same θkc for any c ∈ [N]. in what
follows,
NN
Xpckβkc-βkk22=Xpckβkc-θk0-(βk-θk0)k22
c=1	c=1
N
≤ Xpc kβkc -θk0k22 ,	(33)
c=1
where the last inequality follows by βk = PcN=1pcβkc and Ekx - Exk22 ≤ Ekxk22 . Combining
Eq.(32) and Eq.(33), we have
1N
Ekθk - βk k2 ≤ S Epc kβk - θk0 k2
c=1
1N
≤ S X pc l∣βc- θc0∣l2
c=1
1 N	k-1	2
≤ S XPcE X 2Kη2 ∣∣Vfc(θC)∣∣2 +4Kηdτ(ρ2 + Q-PMPc)
c=1	k=k0
1 N	k-1	2
≤ S XPc X 2Kη2E∣∣Vfc(θQ∣∣2 +4Kηdτ(ρ2 + Q-P)IPc)
c=1	k=k0
≤ 28K2η2dL2Hρ + 4KndT(P + N(1 - ρ2))
SS
where the last inequality follows a similar argument as in Lemma B.3.
For scheme IL given p1 = p = ∙
Ekθk -βkk22 =E
PN = Nn, we have θk = 1 Pc∈Sk βk, which leads to
2
=S2 E
2
N
X Ic∈Sk (βkc - βk )
c=1
2
2
where IA is an indicator function that equals to 1 if the event A happens.
Plugging the facts that P(C ∈ Sk) = N and P(c1,c2 ∈ Sk) = N(N-I)) for any ci = c2 ∈ [N] into
the above equation, we have
Ekθk -βkk22
=X P(C ∈Sk) kβc - βkk2 + X P(c1,c2 ∈Sk)hβcι- βk,βc2-βki
c∈[N]	c1 6=c2
S - 1
SN(N - 1)
N
X kβkc -βkk22 ,
c=1
hβkc1 -βk,βkc2 -βki
1 - S
S(N - 1)
23
Under review as a conference paper at ICLR 2022
where the last equality holds since Pc∈[N] kβkc - βkk22 + Pc16=c2hβkc1 - βk,βkc2 - βki =
kβk - βkk22 =0.
Eventually, we have
Ekθk - βkk2 = SN--SI) E N X kβc - βk k2
NS 1N
≤ S(N - 1)EN X kβk - θk0 k2
c=1
≤ SN-SI)(28K2η2dL2Hρ + 4KηdτUP + N(1 - ρ2)fj,
where the first inequality follows a similar argument as in Eq.(33) and the last inequality follows by
Lemma B.3.
□
C.3 Convergence via partial device participation
Theorem C.3 (Restatement of Theorem 5.10). Assume assumptions A.1, A.2, and A.3 hold. Con-
SiderAlgorithm 6 with a correlation coefficient P ∈ [0,1], a fixed learning rate n ∈ (0,芸]and
∣∣θc — θ*∣∣2 ≤ dD2 forany C ∈ [N], we have
W2(μk,∏) ≤ (I- 勺
+ 30κpηmd ∙,Hρ((K - 1)2 + K) + 2 JCmT(P2 + N(1- p2))Cs,
where CK = —ηmK,κ , CS = 1 for Scheme I and CS = N-SS for Scheme IL
1 —e-	2
Proof. Note that
E∣∣∙(k+1)η - θk + 1 ∣∣2
=E∣∣0(k+1)η - βk+1 + βk+1 - θk + 1 ∣∣2
=E∣∣%+1)η - βk+11∣2 + EIlβk+1 - θk + 1 k2 + E2S(k+1)η - 8k+1, βk+1 - θk+1i
=E∣B(k+1)η - βk+1∣∣2 + Ellek+1 - θk + 1∣∣2,
where the last equality follows by the unbiasedness of the device-sampling scheme in Lemma C.1.
If k + 1 mod K 6= 0, we always have βk+1 = θk+1 and E∣βk+1 - θk+1 ∣22 = 0. Following the same
argument as in Lemma B.5, both schemes lead to the one-step iterate as follows
W2 (μk+ι,π) ≤ (I ― ɪɪ) ∙ W2 (μk, π) + 400η2dL2HP((K ―I)2 + K).	(34)
If k + 1 mod K = 0, combining Lemma C.2 and Lemma B.5, we have
W2 (μk+1, π) ≤ (1 ― ^2^) ∙ W2 (μk, π) + 450η2dL2HP(K2 + κ) +
4Kdητ(ρ2 + N(1 - p2))Cs,
S	(35)
where CS = 1 for Scheme I and CS = NN--I for Scheme II.
Repeatedly applying Eq.(34) and Eq.(35) and arranging terms, we have that
W2 (μk, π) ≤ (1 - ɪɪ) W2 (μo,π) + ηm 050η2dL2Hρ(K2 + K)
24
Under review as a conference paper at ICLR 2022
+
(1 - (1 - ηm)k)bk∕Kc
1 - (1 -与)k
4Kdητ (P2 + N(1-P2
S
≤ (1 —	) W2(μ0,π) + 9QQηmdκ2H0((K — 1)2 + K)
ηmκ	4KdnTf 2	2、S
+------ηmκ —^(P + N(1 - P ))CS,
1 - e~ηmKS
、---V----}
CK
=(1 — ɪ-) W2 (μo,∏) + 9O0nmdκ2H0((K — 1)2 + K)
+ 4CKdT(ρ2 + N(1 - ρ2))Cs,
Sm
where the second inequality follows by (1 - r)κ ≤ e-rK for any r ≥ 0.
□
D Bounding contraction, discretization, and divergence
D.1 DOMINATED CONTRACTION PROPERTY
ProofofLemma B.1. Given a client index C ∈ [N], applying Theorem 2.1.12 Nesterov (2004) leads
to
hy - x, Vfc(y)	-	VfC(x)i	≥	7mL-	Ily	- x∣∣2 + vɪ ^V∕c(y)	- VfC(X)∣∣2,	∀x,y ∈	Rd.
L + m	L + m
(36)
in what follows, we have
M - θ - η(Vf (β) -Vf (θ))∣2
=Ile - θ∣∣2 - 2η hβ - θ, Vf(β) - Vf(θ)i +η2 ∣∣Vf(β) - Vf(θ)∣∣2.	(37)
'-------V--------'
I
For the second item Z in the right hand side, we have
N
I = XPc(β - θ, Vf c(βc) - Vfc(θc)}
c=1
N
=X Pc(β - βc + βc - θc + Θc - Θ, Vf c(βc) - Vf c(θc)>
c=1
N
=-XPc (<βc - β, Vfc(βc) - Vfc(θc)> + <θ - θc, Vfc(βc) - Vfc(θc)>)
c=1
N
+ X Pc<βc-θc, Vfc(βc)-Vfc(θc)>
c=1
N1
≥-∑ Pc ∙ ((m + L) ∣∣βc - β∣∣2 + (m + L) ∣∣θc - θ∣∣2 + 小 + L) ∣∣Vfc(βc) - Vf c(θc)∣∣2 )
c=1	( -+ )
N mL	1
+X Pc ∙ (LTm kβc - θc∣2 + L+m Wfc (βc) - Vf c(θc)k2)
c=1
≥ -(m + L)X Pc (∣∣βc - β∣∣2 + kθc - θ∣∣2) + LmLm kβ - θ∣∣2
c=1
+ 2(L+m)kVf (β)-Vf (θ)k2，	(38)
25
Under review as a conference paper at ICLR 2022
where the first inequality follows by the AM-GM inequality and Eq.(36), respectively; the last
inequality follows by Jensen’s inequality such that
N
X pckβc - θck22 ≥
c=1
N
XPc kVfc(βc)-Vfc(θc)k2 ≥
c=1
N
X pc(βc - θc)
c=1
2
= kβ - θk22
2
kVf(β)-Vf(θ)k22.
Plugging Eq.(38) into Eq.(37), we have
kβ - θ - η ∙ (Vf(β) -Vf(θ))k2
≤ O - 2ηmL) ∙kβ - θk2 + η(η - -ɪv) ∙kVf(β) - Vf(θ)k2
m+L	m+L
|	∙二 }
≤0 if η≤ m+L
N
+ 2η(m + L) Xpc ∙ (kβc-βk2 + kθc-θk2)
c=1
N
≤ (1 - ηm) kβ - θk2 + 4ηL XPc ∙ (kβc - βk2 + ∣∣θc - θ∣∣2),
c=1
where the last inequality follows by m2+LL ≥ 1, m ≤ L, 1 - 2a ≤ (1 - a)2 for any a, and
η ∈ (O，m+L].
□
D.2 Discretization error
Proof of Lemma B.2. For any s ∈ [0, ∞), there exists a certain k ∈ N such that s ∈ [kη, (k + 1)η).
By the continuous dynamics of Eq. (19), we have
θ = %SC +(s- kη)Vfc(%SC
)+ √2T I dW t,
kη


which suggests that
s∈[k 券1)JMC -纵 S CL ≤(S - knMVfc% S C)L + s∈[k ;;.)〃)
s∈[kη,(k+1)η)	s∈[kη,(k+1)η)
/ √2T dW t
kη
2
We first square the terms on both sides and take Young’s inequality and expectation
E∈[kηsUP+ι∕θc-% S 北 ≤ 狎(S - "fl S 建
+ 2E	sup
s∈[kη,(k+1)η)
Zs
kη
2
√2TdW t	.
∣2
Then, by Burkholder-Davis-Gundy inequality (50) and It6 isometry, We have
ds
E sup W -纵SC∣∣2 ≤ 2E∣∣(s - kη)Vfc(%Sc)∣∣2 +8EE	2τdt
s∈[kη,(k+1)η)	η	η	i=1 kη
≤ 2η2E∣∣Vfc(纵Sc)∣∣2 + 16ηdτ.
(39)
By Young’s inequality and the smoothness assumption A.1, we have
EkVfc(Mb η c)k2 = EkVfc% S C)- Vfc(θ*) + Vfc(θ*)k2
26
Under review as a conference paper at ICLR 2022
≤ 2EkVfc(%(_SC)- <fc(θ*)k2 + 2kVfc(θ*)(2
≤ 2L2E∣%SJ -θ*∣∣2+2γ2
≤ 2L2 (— (γ2 + 2dτ)) + 2γ2
mm
≤ 4dκ ( KY——+ 4Lτ) ,	(40)
where the third inequality follows by Lemma E.2, the fourth step holds since κ ≥ 1. Combining
Eq. (39) and Eq. (40), we have
E SuP	IlHc — 用| S 1112 ≤ 8η2dκ(κγ- + LTI +16ηdτ.
s∈[kη,(k+i)η)"	儿ηC 2	∖ d
□
D.3 B ounded divergence
ProofofLemma B.3. For any k ≥ 0, consider ko = K [ K C such that k ≤ ko and θco = θk° for any
k ≥ 0. It is clear that k - k0 ≤ K - 1 for all k ≥ 0. Consider the non-increasing learning rate such
that ηk0 ≤ 2ηk for all k - k0 ≤ K - 1.
By the iterate Eq.(18), we have
N
XpcEkθkc -θkk22
c=1
N
= X pcEkθkc -θk0 -(θk -θk0)k22
c=1
N
≤ X pcEkθkc -θk0k22
c=1
N	k-1	2
≤ XPcEX 2(K — 1)η2 IlVfc(θc)∣∣2 + 4(K — 1)ηkdτ(ρ2 + (1 — ρ2)∕pc)
c=1	k=k0
N	k-1	2
≤ XPc( X 2(K - I)η20E∣∣vfc(eC)∣∣2 + 4(K — I)ηk0赤苫 + (1 — ρ2)/Pc)
c=1	k=k0
≤ 112(K — 1)2ηk2dL2Hρ + 8(K — 1)ηkdτ(ρ2+N(1 — ρ2)),
where the first inequality holds by Ekθ — Eθk22 ≤ Ekθk22 for a stochastic variable θ; the second
inequality follows by (PiK=-11 ai)2 ≤ (K — 1) PiK=-11 ai2; the last inequality follows by Lemma E.3
and ηk2 ≤ 4ηk2. Hρ is defined in Definition A.4.
□
D.4 B ounded variance
Proof of Lemma B.4. By assumption A.3, we have
E∣∣∣Vf(θ) — Vfe(θ)∣∣∣2 = E
X Pc(Vfc(ec)— v 产(θc))∣∣
N
XPc2E∣∣Vfc(θc) — Vfec(θc)
c=1
2
2
27
Under review as a conference paper at ICLR 2022
N	N2
≤ dσ2 X pc2 ≤ dσ2 X pc	:= dσ2 .
c=1	c=1
□
E Uniform upper b ound
E.1 Discrete dynamics
Lemma E.1 (Discrete dynamics). Assume assumptions A.1, A.2, and A.3 hold. We consider the
generalized formulation in Algorithm 5 with the temperature
Tc,ρ = T (P2 + (I-P2)/Pc)
given a correlation coefficient P. For any learning rate η ∈ (0, 2/m) and ∣∣θc 一 θ* k2 ≤ dD2 for any
c ∈ [N], we have the `2 norm upper bound as follows
SuPE∣θc 一 θ*∣2 ≤ dD2 + -( max TcP + — + ɪ-
k	2	m c∈[N]	m md
where Y := maxc∈[N ] ∣∣Vf c(θ*)∣∣2 and θ* denotes the global minimum for thefunction f.
Proof. First, We consider the k-th iteration, where k ∈ {1, 2,…，K 一 2, (K 一 1)-} and (K 一 1)-
denotes the (K 一 1)-step before synchronization. Following the iterate of Eq.(13) in a local client of
c ∈ [N], we have
叫%1-。*俏 ~	~
=E∣θC 一 θ* 一 nV产(θk)k2 + P8ηTjE(θC 一 θ* 一 nV产(θC),ξki + 2ηTc,ρE∣ξkk2
=E∣θC - θ* - ηVfc(θk)k2 +2ηdTc,ρ,	(41)
where the last equality follows from Eξk = 0 and the conditional independence of θk 一 θ* 一 f c(θk)
and ξk . Note that
E∣θk 一θ*-η产(θk)k2	~
=E∣θk 一 θ* - ηVfk(θk)k2 + η2EkVfk(θk) - Vfk(θk)∣2
. . .. .. ~一 . ...
+ 2ηEhθk 一 θ* - ηVfk(θk), Vfk(θQ - Vfk(θQi
=E∣θk 一 θ* - ηVfk(θk)k2 + η2EkVfk(θk) - Vfk(θk)k2
≤ E∣θk 一。*一 ηVfk(θk)k2+ η2dσ2,	(42)
where the first step follows from simple algebra, the second step follows from the unbiasedness of
the stochastic gradient, and the last step follows from Assumption A.3. For any q > 0, we can upper
bound the first term of Eq.(42) as follows
E∣θk 一θ*-ηVfc(θk)k2
=E∣θk 一 & 一 η(Vfk(θk) — vfc(θ*))-ηVfc(θ* )∣2
≤ (i + q)E∣θk 一 θ* — η(Vfk(θk) — Vfc(θ*))∣∣2 + η2 (1 +q) kVfk(θ*)∣∣2
≤(I +q) (1 - ^2^) Ekθk 一。*12 + η2(1 + -] γ2,	(43)
--}	q/
ψ2
where the first inequality follows by the AM-GM inequality; the second inequality is a special
case of Lemma B.1 based on Assumption A.2, where no local steps is involved before the syn-
chronization step. Similar results have been achieved in Theorem 3 Dalalyan (2017a). In addition,
Y := maXk∈[N] ∣Vfk(θ≠)∣2.
28
Under review as a conference paper at ICLR 2022
Choose q = (12ψψ)2 - 1 so that (1 + q)ψ2 = (1+ψ) . Moreover, since ψ = 1 - ηm, We get
1+ψ = 1 - 1 ηm. In addition, we have 1 + 1 = 1+q = . (1+2*、≤ ɪ. It follows that
2	4	,	q	q (1-ψ)(1+3ψ) ηm
η2(1 + q) ≤ R	(44)
Combining Eq. (41), Eq. (42), Eq. (43), and Eq. (44), we have the following iterate
Ekθc+i-θ*k2 ≤ (1 - η-^~) Ekθk - θ*k2 + 2ηdTc,ρ + η2dσ2 + ηγ .
X-------V--------}
∙∙=g(η)
Note that 1 g(η)  ηm (J ηm) ≤ ηm given η ∈ (0, m). Recursively applying the above equation k
times, where k ∈{1, 2,…，K - 1, K-} and K_ denotes the K-step without synchronization, it
follows that
EkθC - θ*k2 ≤ g(η)kkθC - θ*k2 + IJn))k ∙ "ηdTc,P + η2dσ2 +
≤ kθc - θ*ll2 +----∙卜2ηdTc,ρ + η2dσ2 +
ηm
≤ dD2 + 6d ( max Tc,. + σ2 + J),
m c∈[N]	m md
X---------------------------}
(45)
^{^―
:=U
where the second inequality holds by g(η) ≤ 1, the third inequality holds because ∣∣Θq - θ* k2 ≤ dD2
for any C ∈ [N] and η < m. In particular, the K-th step before synchronization yields that
EkθK- - θ*k2 ≤ dD2 + U.	(46)
Having all the results ready, for the K-local step after synchronization, applying Jensen’s inequality
N	2
EkθK -θ*k2 = E XPcθK- -θ*
c=1	2
N
≤ XPcE∣∣θK- -θ*∣∣2
c=1
≤ dD2 + U,	(47)
Now starting from iteration K, we adapt the recursion of Eq.(45) for the k-th step, where k ∈
{K +1,…，2K - 1, (2K)-} and (2K)- denotes the 2K-step without synchronization, we have
e∣θQ -θ*k2
≤ g(η)k-K ∙ EkθK - θ*k2 + 1 -g(η)k-K ∙ (2ηd max Tc,P + η2dσ2 + 2ηγ2)
2	1 - g(η)	c∈[N]	m
≤g(η)k-K (dD2 + U)+ 1-g(η)k-K 等U
mη∕3	3
≤dD2 + g(η)k-KU + (1 - g(η)k-K)U
≤dD2 + U,	(48)
where the second inequality follows by Eq.(47), the fact that 1 - g(η) ≥ ηm∕3 and η ≤ mm, and the
definition of U. The third one holds since g(η) ≤ 1.
By repeating Eq.(47) and (48), we have that for all k ≥ 0, we can obtain the desired uniform upper
bound.	口
Discussions: Since the above result is independent of the learning rate η, it can be naturally applied
to the setting with decreasing learning rates. The details are omitted.
29
Under review as a conference paper at ICLR 2022
E.2 Continuous diffusion
Lemma E.2 (Continuous time). Assume assumption A.2 holds. We have the `2 norm upper bound as
follows
SuP EIHc — θ* 112 ≤ — + +-+ 2dτ),
t	2 mm
where Y :二 maXc∈[N ] ∣∣Vf c(θ*)∣∣2 and θ* denotes the global minimum for thefunCtion f.
Proof. Since the synchronization is conducted at every time t, the essential temperature applied to
each client is τ. Let q(θt) = ∣θc 一 θ* |[. For any time t ≥ 0, applying It6's lemma leads to
dq(∕) = 一2曹 一 θ*, Vf c(θc)idt + 2dτdt + √8TGi 一 θ*, dWti
≤
≤
≤
一2〈7 一 θ*, Vfc(θc) 一 Vfc(θ*) + Vfc(θ*)〉dt + 2dτdt + √8Γhθt 一 θ*, dWti
-2m∣∣∕ 一	θ*∣∣2	dt	一	2(7 - θ*, Vfc(θ*)>dt + 2dτdt	+ √8TR 一 θ*, dWt)
-2m ||皆 一	θ*	∣∣2	dt	+	m ||夕c 一 θ* ∣∣2 dt + k"f (。")"2	dt + 2dτdt + √8τ居	一 θ*, dWt
一mq(∕)dt + (Y——+ 2dτ∖ dt + √8τ〈/ — θ*, dW.,
tm	t
where the first inequality follows by Assumption A.2; the second inequality follows by the AM-GM
inequality; the third inequality follows by the definition that γ2 = maxc∈[N] ∣∣Vfc(θ*) ∣∣2.
In other words, we have
d(emtq(θc)) = memtq(θc)dt + emtdq((9c)
≤ memtq(θc)dt + emt	mq((9c)dt +	+ 2dτ) dt + √8Γ(θc - θ*, dWQ
≤ emt fγ2 + 2dτ] dt + √8τemth.θc 一 θ*, dWt).
mt
The solution is upper bounded by
emtq(θt) ≤ em∙0q(θc) + /t (ems 仁 + 2dτ) ds + √8⅞msh四 一 θ*, dWSi).
By the martingale property of It6 integral, taking expectations yields
Eq(θc) ≤ e-mtEq(θc) + e-mt 仁 + 2dτ) ZQ' emsds
1	e-mt	2
=e-mtEq(θc) +--------( L + 2dτ)
mm
≤ e-mtEq(θc)+1 - e mt ( γ2 +2dτ),	(49)
mm
'---V--}
:=V
where the last inequality follows since the synchronization is conducted at any time step t. Since Oc is
simulated from the stationary distribution π, by Lemma 12 Durmus & Moulines (2016) or Theorem
17 Cheng et al. (2018), we have
dτ 1 Y2	V
Eq(θc) = E∣θc 一 θ*k2 ≤ — ≤ -(L + 2dτ)=一,
m mm	m
which completes the proof.
□
30
Under review as a conference paper at ICLR 2022
E.3 Bounded gradient
Lemma E.3 (Bounded gradient in `2 norm). Given assumptions A.1, A.2, and A.3 hold, for any
client C and any learning rate η ∈ (0,2/m) and ∣∣θC 一 θ* k2 ≤ dD2 for any C ∈ [N], we have the '2
norm upper bound as follows
EkVfc(θQk2 ≤ 14dL2Hρ,
Where Hp = D2 + mm maχc∈[N] Tc,ρ + mγ2d + m∙
Proof. Decompose the `2 of the gradient as follows
E∣lv∙ec(θc )∣∣2=E∣lv∙ec(θc)- VfC (θk)+—hi2
=EkVfC(θC)k2 + E∣Vec(θC) -Vfc(θk)∣2
+2EDvfeC(θkC)-vfC(θkC),vfC(θkC)E
≤EkVfC(θkC)k22+σ2d
=EkVfC(θC) — Vfc(θ*) + Vfc(θ*)k2 + σ2d
≤ 2EkVfc(θQ - Vfc(θ*)k2 + 2E∣∣Vfc(θ*)∣∣2 + σ2d
≤ 2L2E∣∣θC - θ*∣∣2 + 2γ2 + σ2d
≤ 2dL2D2 + I)" ∙ ( max TCP + σ——+ γ~~) + 2γ2 + σ2d
m	C∈[N] , m md
≤ 14dL2 ∙ fD2 + — max TCP +——γ^; + ^σv 1 ：= 14dL2Hp,
m C∈[N]	,	m2d	m2
where the first inequality follows by Assumption A.3; the second inequality follows by Young’s
inequality; the third inequality follows by Assumption A.1 and the definition that γ :=
maxC∈[N] ∣∣Vf°(。*)|〔2； the fourth inequality follows by Lemma E.1; the last inequality follows
byK := L ≥ 1.	口
F Initial condition
Lemma F.1 (Initial condition). Let μo denote the Dirac delta distribution at θ0. Then, we have
W2(μ0,∏) ≤ √2(kθ0 - θ*k2 + pdT/m).
Proof. By Cheng et al. (2018), there exists an optimal coupling between μ° and ∏ such that
W2(μo,∏) ≤ Eθ〜∏[∣∣θo - θk2]
≤ 2Eθ〜∏[kθo - θ*k2] + 2Eθ〜∏[kθ -θ*k2]
= 2∣Θo - θ*k2 + 2Eθ〜∏[kθ - θ*k2]
≤ 2∣Θo — θ*k2 + 2dτ∕m,
where the second step follows from triangle inequality, the last step follows from Lemma 12 Durmus
& Moulines (2016) and the temperature T is included to adapt to the time scaling.	□
Burkholder-Davis-Gundy inequality Let φ : [0, ∞) → Rr×d for some positive integers r and
d. In addition, we assume ER0∞ ∣ψ(s)∣2ds < ∞ and let Z(t) = R0 ψ(s)dWs, where Ws is a
d-dimensional Brownian motion. Then for all t ≥ 0, we have
E sup
0≤s≤t
|Z(s)|2 ≤ 4E
Zt IΦ(s)l
0
2ds
(50)
31