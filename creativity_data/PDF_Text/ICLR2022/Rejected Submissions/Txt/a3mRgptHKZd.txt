Under review as a conference paper at ICLR 2022
Faster No-Regret Learning Dynamics for
Extensive-Form Correlated Equilibrium
Anonymous authors
Paper under double-blind review
Abstract
A recent emerging trend in the literature on learning in games has been concerned
with providing accelerated learning dynamics for correlated and coarse correlated
equilibria in normal-form games. Much less is known about the significantly
more challenging setting of extensive-form games, which can capture sequential
and simultaneous moves, as well as imperfect information. In this paper, we de-
velop faster no-regret learning dynamics for extensive-form correlated equilibrium
(EFCE) in multiplayer general-sum imperfect-information extensive-form games.
When all agents play T repetitions of the game according to the accelerated dy-
namics, the correlated distribution of play is an O(T -3/4)-approximate EFCE.
This significantly improves over the best prior rate of O(T -1/2). One of our
conceptual contributions is to connect predictive (that is, optimistic) regret mini-
mization with the framework of Φ-regret. One of our main technical contributions
is to characterize the stability of certain fixed point strategies through a refined
perturbation analysis of a structured Markov chain, which may be of independent
interest. Finally, experiments on standard benchmarks corroborate our findings.
1 Introduction
Game-theoretic solution concepts describe how agents should rationally act in games. Over the
last two decades there has been tremendous progress in imperfect-information game solving and
algorithms based on game-theoretic solution concepts have become the state of the art. Prominent
milestones of this were an optimal strategy for Rhode Island hold’em poker (Gilpin & Sandholm,
2007), a near-optimal strategy for limit Texas hold’em (Bowling et al., 2015), and a superhuman
strategy for no-limit Texas hold’em (Brown & Sandholm, 2017). In particular, these advances rely
on algorithms that approximate Nash equilibria (NE) of two-player zero-sum extensive-form games
(EFGs). EFGs are a broad class of games that capture sequential and simultaneous interaction,
and imperfect information. For two-player zero-sum EFGs, it is by now well-understood how to
compute a Nash equilibrium at scale: in theory this can be achieved using accelerated uncoupled no-
regret learning dynamics, for example by having each player use an optimistic regret minimizer and
leveraging suitable distance-generating functions (Hoda et al., 2010; Kroer et al., 2020; Farina et al.,
2021c) for the EFG decision space. Such a setup converges to an equilibrium at a rate of O(T -1).
In practice, modern variants of the counterfactual regret minimization (CFR) framework typically
lead to better performance, although the worst-case convergence rate is O(T -1/2) (Zinkevich et al.,
2007). CFR is also an uncoupled no-regret learning dynamic.
However, many real-world applications are not two-player zero-sum games, but instead have
general-sum utilities and often more than two players. In such settings, Nash equilibrium suffers
from several drawbacks when used as a prescriptive tool. First, there can be multiple equilibria,
and an equilibrium strategy may perform very poorly when played against the “wrong” equilibrium
strategies of the other player(s). Thus, the players effectively would need to communicate in order
to find an equilibrium, or hope to converge to it via some sort of learning dynamics. Second, find-
ing a Nash equilibrium is computationally hard both in theory (Daskalakis et al., 2006; Etessami &
Yannakakis, 2007) and in practice (Berg & Sandholm, 2017). This effectively squashes any hope of
developing efficient learning dynamics that converge to general-sum Nash equilibria.
A competing notion of rationality proposed by Aumann (1974) is that of correlated equilibrium
(CE), typically modeled via a trusted mediator who privately recommends actions to the players.
1
Under review as a conference paper at ICLR 2022
Unlike NE, it is known that the latter can be computed in polynomial time and, perhaps even more
importantly, it can be attained through uncoupled learning dynamics, where the players only need
to reason about their own observed utilities. This overcomes the often unreasonable presumption
that players have knowledge about the other players’ utilities. At the same time, uncoupled learning
algorithms have proven to be a remarkably scalable approach for computing equilibria in large-scale
games, as described above. The basic CE notion is defined for normal-form games, and there it has
long been known that uncoupled no-regret learning dynamics can converge to CE or the coarse
correlated equilibrium (CCE) variant at a rate of O(T -1/2) (Hart & Mas-Colell, 2000; Celli et al.,
2019). More recently, it was shown that accelerated uncoupled no-regret learning dynamics can
compute CCE and CE at a rate of O(T -3/4) (Syrgkanis et al., 2015; Chen & Peng, 2020).
In the context of EFGs, the idea of correlation is much more intricate, and there are several no-
tions of correlated equilibrium, based on when the mediator gives recommendations and how the
mediator reacts to players who disregard the advice. One of the most compelling notions for EFGs
is the extensive-form correlated equilibrium (henceforth EFCE) (von Stengel & Forges, 2008) for
extensive-form games with perfect recall. Because of the sequential nature, the presence of private
information in the game, and the gradual revelation of recommendations, the constraints associated
with EFCE are significantly more complex than for normal-form games. For these reasons, the
question of whether uncoupled learning dynamics can converge to an EFCE was only very recently
resolved by Celli et al. (2020). Moreover, in a follow-up work they also established an explicit
rate of convergence of O(T -1/2) (Farina et al., 2021a). Our paper is concerned with the following
fundamental question: Can one develop faster uncoupled no-regret learning dynamics for EFCE?
Contributions. Our primary contribution is to answer this question in the positive:
Theorem 1.1. On any finite perfect-recall general-sum multiplayer extensive-form game, the un-
coupled no-regret learning dynamics described in this paper lead to a correlated distribution of
play that is an O(T-3/4)-approximate EFCE, where the O(∙) notation suppresses game-specific
parameters polynomial in the size of the game.
We achieve this result using the framework of predictive (also known as optimistic) regret mini-
mization (Chiang et al., 2012; Rakhlin & Sridharan, 2013b). One of our conceptual contributions
is to connect this line of work with the framework of Φ-regret minimization of Greenwald & Jafari
(2003); Gordon et al. (2008), by providing a general template for stable-predictive Φ-regret min-
imization. The importance of Φ-regret is that it leads to substantially more powerful notions of
hindsight rationality, beyond the usual external regret (Gordon et al., 2008), including the powerful
notion of swap regret (Blum & Mansour, 2007). Moreover, one of the primary insights behind the re-
sult of Farina et al. (2021a) is to cast convergence to an EFCE as a Φ-regret minimization problem.
Given these prior connections, we believe that our stable-predictive Φ template is of independent
interest, and could lead to further applications in the future.
Theorem 1.1 extends and strengthens several prior papers in the literature, including the seminal
work of Syrgkanis et al. (2015) that provides accelerated dynamics for coarse correlated equilib-
rium in normal-form games, as well as the more recent result of Chen & Peng (2020) which showed
O(T-3/4) convergence to a correlated equilibrium in normal-form games. For the more challeng-
ing class of extensive-form games, accelerated rates were previously known only for finding a Nash
equilibrium in the special case of two-player zero-sum games, where an O(T-3/4) rate was achieved
via a stable-predictive CFR setup (Farina et al., 2019a) and an O(T -1) rate was achieved via opti-
mistic regret minimizers coupled with good distance-generating functions (Farina et al., 2019c).
From a technical standpoint, in order to apply our generic template for accelerated Φ-regret mini-
mization, we establish two separate ingredients. First, we develop a stable-predictive external regret
minimizer for the set of transformations Φ associated with EFCE. This differs from the construction
by Farina et al. (2021a) in that we have to additionally guarantee and preserve the stability—and sub-
sequently the predictivity—throughout the construction. The second component consists of sharply
characterizing the stability of fixed points of trigger deviation functions. This turns out to be par-
ticularly challenging, and direct extensions of prior techniques appear to only give a bound that is
exponential in the size of the game. In this context, one of our key technical contributions is to pro-
vide a refined perturbation analysis for a Markov chain consisting of a rank-one stochastic matrix,
employing tools that have not been used before in this line of work, and substantially extending the
techniques of Chen & Peng (2020). This leads to a rate of convergence that depends polynomially
2
Under review as a conference paper at ICLR 2022
on the description of the game, which is crucial for the applicability of the accelerated dynamics.
Finally, we support our theoretical findings with experiments on several general-sum benchmarks.
Further Related Work. The line of work on accelerated no-regret learning for Nash equilibrium
was pioneered by Daskalakis et al. (2015), showing that one can bypass the adversarial Ω(T-1/2)
barrier for the incurred average regret if both players in a zero-sum game employ an uncoupled vari-
ant of the excessive gap technique (Nesterov, 2005), leading to a near-optimal rate of O(log T /T).
Subsequently, Rakhlin & Sridharan (2013a) showed that the optimal rate of O(1/T) can be obtained
with a remarkably simple variant of Online Mirror Descent which incorporates a prediction term in
the update step. While these results only hold for zero-sum games, Syrgkanis et al. (2015) showed
that O(T-3/4) rate can be obtained for multiplayer general-sum normal-form games. In a recent
result, Chen & Peng (2020) strengthened the regret bounds of Syrgkanis et al. (2015) from external
to swap regret using the celebrated construction of Blum & Mansour (2007). We also acknowledge
a recent result of Daskalakis et al. (2021) which establishes a near-optimal rate of convergence of
Oe(1/T) to a coarse correlated equilibrium when all players employ the Optimistic Multiplicative
Weights Update (OMWU) algorithm in a normal-form game. Extending their result to extensive-
form games presents considerable technical challenges since their analysis crucially hinges on the
closed-form softmax-type structure of OMWU on the simplex.
Correlated equilibrium in extensive-form games is much less understood than Nash equilibrium.A
feasible EFCE can also be computed efficiently through a variant of the Ellipsoid algorithm (Pa-
padimitriou & Roughgarden, 2008; Jiang & Leyton-Brown, 2015), and an alternative sampling-
based approach was given by DUdik & Gordon (2θθ9). However, those approaches perform poorly
in large-scale problems, and do not allow the players to arrive at EFCE via distributed learning.
Celli et al. (2019) devised variants of the CFR algorithm that provably convergence to normal-form
coarse correlated equilibria, a solUtion concept mUch less appealing than EFCE in extensive-form
games Gordon et al. (2008). Finally, Morrill et al. (2021a;b) characterize hindsight rationality no-
tions and associate a set of solUtion concepts with sUitable O(T -1/2) no-regret learning dynamics.
2	Preliminaries
Extensive-form Games. An extensive-form game is abstracted on a directed and rooted game tree
T. The set of nodes of T is denoted with H; non-terminal nodes are referred as decision nodes,
and are associated with a player who acts by selecting an action from a set of possible actions A(h),
where h ∈ H represents the decision node. By convention, the set of players [n] ∪{c} inclUdes a
fictitious agent c who “selects” actions according to fixed probability distribUtions dictated by the
natUre of the game (e.g., the roll of a dice); this intends to model external stochastic phenomena
occUrring dUring the game. For a player i ∈ [n] ∪ {c}, we let H(i) ⊆Hbe the sUbset of decision
nodes wherein a player i makes a decision. The set of leaves Z⊆H, or eqUivalently the terminal
nodes, correspond to different oUtcomes; once the game transitions toa terminal node z ∈Z, payoffs
are assigned to each player based on a set of normalized Utility fUnctions {u(i) : Z→[-1, 1]}i∈[n].
It will also be convenient to represent with p(c) (z) the prodUct of probabilities of “chance” moves
encoUntered in the path from the root Until the terminal node z ∈ Z.
Imperfect Information. To model imperfect information, the set of decision nodes H(i) of player
i are partitioned into a collection of sets J (i), which are called information sets. Each information
set j ∈J(i) groUps nodes which cannot be distingUished by i. ThUs, for any nodes h, h0 ∈ j we
have A(h) = A(h0). As UsUal, we assUme that the game satisfies perfect recall: players never
forget information once acquired. We will also define a partial order Y on J(i), so that j Y j0, for
j,j0 ∈ J(i), if there exist nodes h ∈ j and h0 ∈ j0 sUch that the path from the root to h0 passes
through h. Ifj Y j0, we will say that j is an ancestor ofj0, or equivalently, j is a descendant ofj0.
Sequence-form Strategies. For a player i ∈ [n], an information set j ∈ J(i), and an action
a ∈A(j), we will denote with σ =(j, a) the sequence of i’s actions encountered on the path from
the root of the game until (and included) action a. For notational convenience, we will use the special
symbol 0 to denote the empty Sequence. Then, i's set of sequences is defined as Σ(i) := {(j, a) : j ∈
J(i), a ∈ A(j)} ∪ {0}; We will also use the notation ΣIi) := Σ(i) \ {0}. For a given information
set j ∈ J(i) we will use σ(i) (j) ∈ Σ(i) to represent the parent sequence; i.e. the last sequence
3
Under review as a conference paper at ICLR 2022
encountered by player i before reaching any node in the information set j , assuming that it exists.
Otherwise, We let σ(i)(j) = 0, and We say that j is the root information set of player i. A strategy
for a player specifies a probability distribution for every possible information set encountered in the
game tree. For perfect-recall EFGs, strategies can be equivalently represented in sequence-form:
Definition 2.1 (Sequence-form Polytope). The sequence-form strategy polytope for player i ∈ [n]
is defined as the folloWing (convex) polytope:
Q ⑴:=[q ∈ R≥∑(i)1	:	q[0]	= 1,	q[σ⑺(j)] = X	q[(j,a)],	∀j	∈	J (i)∖.	(1)
a∈A(j)
Analogously, one can define the sequence-form strategy polytope for the subtree of the partially
ordered set (J⑴,Y) rooted at j ∈ J((i), which will be denoted as Q(i). Moreover, the set of
deterministic sequence-form strategies for player i ∈ [n] is the set Π(i) = Q(i) ∩ {0,1}|s(i)|, and
similarly for Π(i). The joint set of deterministic sequence-form strategies of the players will be
represented with Π := × ∈ Π(i). As such, an element π ∈ Π is an n-tuple (π(1),...,π(n))
specifying a deterministic sequence-form strategy for every player i ∈ [n]. Finally, the utility of
player i ∈ [n] under a profile π ∈ Π can be expressed as
u(i) (π) := X p(c) (z)u(i) (z)1{π(k) [σ(k) (z)] = 1, ∀k ∈ [n]}.	(2)
z∈Z
We summarized in Table 1 the EFG notation that we will be using most often throughout the paper.
An Illustrative Example. To clarify some of the concepts we have introduced, we illustrate a simple
two-player EFG in Figure 1. Black nodes belong to player 1, white round nodes to player 2, square
nodes are terminal nodes (aka leaves), and the crossed node is a chance node. Player 2 has two
information sets, J(2) := {C, D}, each containing two nodes. This captures the lack of knowledge
regarding the action played by player 1. In contrast, the outcome of the chance move is observed by
both players. At the information set C, player 2 has two possible actions, A(C) := {5, 6}. Thus,
one possible sequence for player 2 is the pair σ =(C, 5) ∈ Σ(2).
Figure 1: Example of a two-player EFG.
i) j)i i) i)
( ((i (j(
JAΣ QD
Description
Information sets of player i
Actions at information set j
Set of sequences of player i
Sequence-form strategies rooted at j ∈J(i)
Maximum depth of any j ∈J(i)
Table 1: Summary of the basic notation.
Regret, Φ-Regret and Optimistic Regret Minimization. Consider a convex and compact set X⊆
Rd representing the space of strategies of some agent. In the online decision making framework,
a regret minimizer R can be thought of as a black-box device which interacts with the external
environment via the following two basic subroutines:
•	R. NEXTSTRATEGY(): The regret minimizer returns the strategy xt ∈Xat time t;
•	R.OBSERVEUTILITY ('t): The regret minimizer receives as feedback a linear utility function
`t : X3x 7→ h`t, xi, and may alter its internal state accordingly.
The decision making is online in the sense that the regret minimizer can adapt to previously received
information, but no information about future utilities is available. The error of a regret minimizer is
typically measured in terms of external regret, defined, for a time horizon T , as follows:
TT
RT = maχ X<χ*, 'ti - Xhχt, `ti,	⑶
x ∈	t=1	t=1
4
Under review as a conference paper at ICLR 2022
That is, the performance of the online algorithm is compared with the best fixed strategy in hindsight.
Φ-Regret. A conceptual generalization of the concept of external regret is the so-called Φ-regret.
Specifically, in this framework the performance of the learning algorithm is measured based on a set
of transformations Φ : X → X, leading to the notion of cumulative Φ-regret:
TT
RT := maxXhφ*(xt),'ti- Xhxt,'ti.	(4)
φ*∈Φ ʌ-/
t=1	t=1
When the set of transformations Φ coincides with the set of constant functions, one recovers the
notion of external regret given in Equation (3). However, Φ-regret is substantially more expressive
and yields a more appealing notion of hindsight rationality (Gordon et al., 2008), incorporating the
notion of swap regret (Blum & Mansour, 2007).
We will employ the following definition, which is a slight modification of the RVU property intro-
duced by (Syrgkanis et al., 2015, Definition 3).
Definition 2.2 (Stable-PredictiVity). Let R be a regert minimizer and let ∣∣ ∙ ∣∣ be a norm. R is said
to be K-stable with respect to ∣∣ ∙ ∣∣ if for all t ≥ 2, the strategies output by R satisfy
∣xt - xt-1∣ ≤κ,	(5)
Moreover, it is said to be (α, β)-predictive with respect to ∣∣ ∙ ∣∣ if for all t ≥ 1 its regret RT satisfies
T
RT ≤ α(Τ) + β X ∣'t-'t]信	(6)
t=2
no matter the sequence of utility vectors '1,..., 'T, where ∣∣ ∙ ∣* is the dual norm of ∣ ∙ ∣∣.
Optimistic Follow the Regularized Leader. Let d be a 1-strongly convex function with respect to
some norm ∣∣ ∙ ∣∣, and η > 0 the learning rate. OFTRL's update rule takes the following form:
xt := arg max	x, 2't-1 + X'τ
x∈X	τ=1
(OFTRL)
where x1 := arg minx∈X d(x). Syrgkanis et al. (2015) established the following property:
Lemma 2.3. (OFTRL) is 2η-stable and (Ωd∕η, η)-predictive with respect to any norm ∣∣ ∙ ∣ for which
d is 1 -strongly convex, where Ωd is the range of d on X, that is, Ωd := maxχ,χo∈χ {d(x) 一 d(x0)}.
In this paper, we consider the entropic regularizer with respect to the simplex d(x) :=
Pd xi log xi, which is 1-strongly convex with respect to the `1 norm. The pair of dual norms
in the predictivity bound will therefore be (∣ ∙ ∣ι, ∣∣ ∙ ∣∣∞). We call this OFTRL setup Optimistic
Multiplicative Weights Updates (OMWU).
Extensive-Form Correlated Equilibrium. We will work with the definition of EFCE due to
Farina et al. (2019e), which is equivalent to that of von Stengel & Forges (2008). First, let us
introduce the concept of a trigger deviation function.
Definition 2.4. Consider some player i ∈ [n], a sequence σ = (j, a) ∈ ∑i, and joint sequence-
form strategies π ∈ Π(i). A trigger deviation function with respect to a trigger sequence σ and
continuation strategy ∏ is any linear function f :股口⑺1 → 股囚⑸1 with the following properties.
•	Any strategy π ∈ Π(i) which does not prescribe the sequence σ remains invariant. That is,
f (π) = π for any π ∈ Π⑴ such that π[σ] = 0;
•	Otherwise, the prescribed sequence σ = (j, a) is modified so that the behavior at j, as well as
all its descendants is replaced by the behavior specified by the continuation strategy:
f(∏)[σ]={π[σ]	ifσij;	⑺
for all σ ∈ Σ⑴ and π ∈ Π⑴ such that π[σ] = 1.
5
Under review as a conference paper at ICLR 2022
We will let Ψ(i) := {φ(i→充：σ = (j, a) ∈ Σ?), ∏ ∈ Π(i)} be the set of all possible linear mappings
defining trigger deviation functions for player i. We are ready to introduce the concept of EFCE.
Definition 2.5 (EFCE). For e ≥ 0, a probability distribution μ ∈ ∆lπl is an e-approXimate EFCE
if for every player i ∈ [n] and every trigger deviation function φ(i→)	∈ Ψ(i), it holds that
E∏~μ [u⑴(φσi→∏(π⑴)，π(T))-U⑺(π)i ≤ e,	(8)
where π = (∏ι,..., ∏n) ∈ Π. A probability distribution μ ∈ ∆lπl is an EFCE if it is a 0-EFCE.
Theorem 2.6 (Farina et al. (2021a)). For every player i ∈ [n], let π(i),1,..., π(i),T ∈ Π(i) be
a sequence of deterministic sequence-form strategies whose cumulative Ψ(i)-regret is R(i),T with
respect to the sequence of linear utility functions
'⑴,t : Π⑴ 3 π⑴ → U⑴(π⑺,π(-i),t) .	(9)
Then, the empiriCalfrequency of play μ ∈ ∆lπl is an e - EFCE, where e := ɪ maxi∈[n] R(i),T.
3	Accelerating Φ-REGRET Minimization via Optimism
In this section we develop a general template for accelerated Φ-regret minimization for general sets,
and then we instantiate the template for dynamics for EFCE. Our approach combines a framework of
Gordon et al. (2008) with the framework of stable-predictive (aka. optimistic) regret minimization.
As in Gordon et al. (2008), in our template we combine 1) a regret minimizer that outputs a linear
transformation φt ∈ Φ at every time t, and 2) a fixed-point oracle for each φt ∈ Φ. However, in our
framework, we further require that 2) is stable (in the sense of Definition 2.2). To achieve this, we
will focus on regret minimizers that have the following property:
Definition 3.1. Consider a set of functions Φ such that φ(X ) ⊆Xfor all φ ∈ Φ, and a no-regret
algorithm RΦ for the set of transformations Φ which returns a sequence φt ∈ Φ. We say that RΦ is
fixed point G-stable, for G ≥ 0, if the following conditions hold:
•	Every φt admits a fixed point. That is, there exists xt ∈Xsuch that φt(xt) = xt.
•	For any xt such that xt = φt(xt), there exists xt+1 with xt+1 = φt+1(xt+1) such that
kxt+1 - xtk≤G.
We will show how to construct an accelerated Φ-regret minimizer starting from the following:
1.	RΦ: A κ-stable (α, β)-predictive fixed point G-stable regret minimizer for Φ;
2.	STABLEFPORACLE(φ; xe, G, e): A stable fixed point oracle which returns a point x ∈Xsuch
that (i) kφ(x) -xk≤e, and (ii) kx-xek ≤ G (the existence of such a fixed point is guaranteed
by the fixed point G-stability assumption for the regret minimizer).
Given these two components, our next theorem builds a stable-predictive Φ-regret minimizer.
Theorem 3.2 (Accelerated Φ-Regret Minimization). Consider a κ-stable (α, β)-predictive regret
minimizer Rφ for a set of linear transformations Φ, with respect to the '1 norm ∣∣ ∙ kι. More-
over, assume that RΦ is fixed point G-stable with respect to Φ. Then, if we have access to a
StableFPOracle, we can construct a G-stable algorithm with Φ-regret RT bounded as
TT
Rt ≤ α(T) + 2βD'κ2T + 2β X ∣'t - 't-1∣∞ + D' X e"	(10)
where et is the error of STABLEFPORACLE at time t, and D' is an upper bound on the '∞ norm of
`t ’s. It is also assumed that ∣x∣∞ ≤ 1for all x ∈X.
The proof is similar to that of Gordon et al. (2008), and is included in Appendix B.
3.1	Constructing a Stable-Predictive Regret Minimizer for Ψ(i)
Here we develop a regret minimizer for the set coΨ(i), the convex hull of the set of trigger deviation
functions. Given that co Ψ(i) ⊇ Ψ(i), this will immediately imply a regret minimizer for the set
6
Under review as a conference paper at ICLR 2022
Figure 2: An overview of the overall construction. For notational convenience We have let Σ*i) :=
{1, 2,..., m}. The symbol 0 in the figure denotes a multilinear transformation of the inputs. We
also note that blue corresponds to the iterates, while red corresponds to the utilities.
Ψ(i). An overview of the algorithm is given in Figure 2. Farina et al. (2021a) observed that the
set co Ψ(i) can be evaluated in two stages. First, for a fixed sequence σ = (j, a) ∈ ΣIi) we define
the set Ψ?) := co {φ^→∏ : ∏ ∈ ∏(i)}; then, we take the convex hull of all Ψ^), that is, co Ψ(i) =
co{Ψ^) : σ ∈ Σ⑦.Correspondingly, we first develop a stable-predictive regret minimizer for the
set Ψ叉 for any σ ∈ ∑9 and these individual regret minimizers are then combined using a regret
circuit to conclude the construction in Theorem 3.4. All the omitted poofs and pseudocode for this
section are included in Appendix B.1.
Stable-Predictive Regret Minimizer for the set Ψ麒.Consider a sequence σ ∈ ΣIi). Farina et al.
(2021a) observed that the set of transformations Ψ(i) := co {φσ→π : ∏ ∈ ∏(i)} is the image of
Q(i) under the affine mapping h(i) : q 7→ φ(i→) . Hence, it is well-known that a regret minimizer for
Ψ(i) can be constructed starting from a regret minimizer for Q(i). We now show that the same can
be said if one restricts to stable-predictive regret minimizers. In particular, we have the following.
Proposition 3.3. Consider a player i ∈ [n] and any trigger SequenCe σ = (j, a) ∈ ∑i. There
(i)
exists an algorithm which constructs a deterministic regret minimizer R with access to a K -stable
(AT,B)-predictive deterministic regret minimizer R(Qi) for the set Q(i), such that R(i) is K -stable
and (AT , B)-predictive.
In Appendix A we describe a stable-predictive variant of CFR for the set Q(i), for each j ∈J(i),
following the construction of Farina et al. (2019a).
Stable-Predictive Regret Minimizer for co Ψ(i). The next step consists of combining the regret
minimizers Ψ^), for all σ ∈ Σ^i), to a composite regret minimizer for the set co Ψ(i). To this end,
we employ regret circuits (Farina et al., 2019d), leading to the main result of this section:
Theorem 3.4. Consider a κ-stable (α, β)-predictive regret minimizer R(i) for the the simplex
Δiς'L and K-stable (A, B)-predictive regret minimizers R(i) for each σ ∈ Σi, all with re-
SPect to the pair of norms (k ∙ kι, k ∙ k∞). Then, there exists an algorithm which constructs a regret
minimizer RS) for the set co Ψ(i) such that (i) RiiS O(K + ∣∑(i)∣κ)-stable, and (ii) under any
sequence of linear utility functions L1,...,LT the regret incurred can be bounded as
T
Rψ ≤ O(α(T) + A(T) + βDLK2T) + O(B + β∣Σ⑴[2) X IlLt- LtTlI∞,	(11)
t=
where ILt I∞ ≤ DL.
7
Under review as a conference paper at ICLR 2022
3.2	Stability of the Fixed Points
In this subsection we complete the construction of the Ψ(i)-regret minimizer by establishing a stable
fixed point oracle for any φ ∈ co Ψ(i). All of the proofs of this section are included in Appendix B.2.
Multiplicative Stability. A sequence {zt}, with zt ∈ Rd≥ , is said to be κ-multiplicative-stable
if (1 - κ)zt-1 ≤ zt ≤ (1 + κ)zt-1, for any i ∈ [d], and for all t ≥ 2. Importantly, this notion of
multiplicative stability is guaranteed by OMWU (see Lemma B.2). Thus, if D(i) is the depth of i’s
actions and Dx(i) is an upper bound on the `1 norm in the treeplex, we can show the following:
Lemma 3.5. When each regret minimizer R(i) is constructed using predictive CFR instantiated
with OMWU with learning rate η (Theorem A.4) such that for all σ ∈ Σ Ii), the output sequence is
O(η(D(i))2Dxi)d`)-multiplicatively-stable. Moreover, ifthe regret minimizer R(i) is realized using
OMWU with learning rate η, it will output an O(η∖Σ⑶ ∖D`)-multiplicatively-stable sequence.
This characterization will be crucial for establishing the stability of the fixed points. In particular,
following the approach of Farina et al. (2021a), let us introduce the following definitions:
Definition 3.6. Consider a player i ∈ [n] and let J ⊆J(i) be a subset of i’s information sets. We
say than J is a trunk of J(i) if, for every j ∈ J, all predecessors ofj are also in J.
Definition 3.7. Consider a player i ∈ [n], a trunk J ⊆J(i), and φ ∈ co Ψ(i). A vectorX ∈ R≥∑(i)1
is a J -partial fixed point of φ if the following conditions hold:
•	x[0] = 1 and x[σ⑴(j)]=工一]⑶ x[(j, a)], for all j ∈ J;
•	φ(x)[0] = x[0] = 1, and φ(x)[(j, a)] = x[(j, a)], for all j ∈ J, and a ∈ A(j).
An important property is that a J-partial fixed point can be efficiently “promoted” to a J ∪ {j*}-
partial fixed point by computing the stationary distribution of a certain Markov chain. However, a
significant concern is whether this fixed point operation can potentially cause a substantial degrada-
tion in terms of stability. One of our key results is that the associated Markov chain has a particular
structure, which enables us to substantially improve the stability bound and thereby obtain a poly-
nomial degradation in stability. More precisely, this boils down to the following technical lemma.
Lemma 3.8. Let M and M0 be transition matrices of m-state Markov chains such that M =
v1> + C and M0 = v01> + C0, where C, C0 , v, v0 have strictly positive entries. Moreover, let π
and π0 be the (unique) stationary distributions of M and M0 respectively. Then, if (i) the entries
of the matrices C and C0 are κ-multiplicatively-close, (ii) the entries of the vectors v and v0 are
γ -multiplicatively-close, and (iii) the sum of the entries of v and v0 are κ-multiplicatively-close,
then π and π0 are (γ + O(κm))-multiplicatively-close, for a sufficiently small κ = O(1/m).
Using a slightly more general result (Corollary B.10), we manage to obtain the following:
Proposition 3.9. Consider a player i ∈ [n], and let φ = P..∈ 夕⑶ λ[σ]φ^→q& be a transformation
in co Ψ(i) such that the sequence of λt s and qlt's is K-multiplicatively-stable, for all σ ∈ ΣIi). If Xt
is a γ -multiplicatively-stable J -partial fixed point sequence, there is an algorithm which computes
a (J ∪ {j *}) -partial fixed point (xt)0 of φ such that the SeqUenCe of (x0)t s is (γ + O(κ∣A(j*)∣))-
multiplicatively-stable, for any sufficiently small K = O(1∕∣A(j*)|).
Thus, using our technical lemma, we manage to bypass the substantial overhead of the term
Y∣A(j*)∣, which would follow using techniques similar to Chen & Peng (2020). This turns outto be
crucial for obtaining a polynomial dependence on the size of the game. Finally, we can inductively
employ this proposition to show the overall stability of the fixed points:
Theorem 3.10. Consider a player i ∈ [n], and let φ = P..∈ς⑸ λ[σ]φ^→q& be a transformation in
co Ψ(i) such that the sequence of λt，s and qt ,s is K-multiplicatively-stable, for all σ ∈ Σ*i). Then,
there exists an algorithm which computes a fixed point qt ∈ Q(i) ofφ such that the sequence ofqt ’s
is O(K|A(i) |D(i))-multiplicatively-stable, where |A(i) | := maxj∈J (i) |A(j)|, and for a sufficiently
small K = O(1/(|A(i)|D(i))).
Finally, if we use the stability values derived in Lemma 3.5, we arrive at the following conclusion:
8
Under review as a conference paper at ICLR 2022
Corollary 3.11. For K =O((Dx) (D ⑴)2 + ∣Σ(i)∣)∣A(i)∣D(i)D') ,the sequence of fixed points will
be (ηκ)-multiplicatively-stable, for any sufficiently small n =O(1∕κ).
Putting Everything Together. Finally, having established these ingredients, we can use the template
of Theorem 3.2 to obtain Theorem 1.1, as we formally show in Appendix B.3.
4	Experiments
In this section we experimentally investigate the performance of our stable-predictive algorithm
compared to two other popular approaches based on a CFR-style decomposition of regrets into lo-
cal regret-minimization problems: the existing algorithm by Farina et al. (2021a) instantiated with
(i) regret matching+ (RM+) (Tammelin, 2014) for each simplex (in place of regret matching), and
(ii) using the vanilla MWU algorithm for each simplex. In accordance to the theoretical predic-
tions, the stepsize for OMWU is set as η = τ ∙ t-1/4 (cf. Corollary B.13), and for MWU it is set
as ηt = T ∙ t-1/2, where the parameter T is chosen by picking the best-performing value among
{0.01, 0.1, 1, 10, 100}. In particular, we evaluate their performance based on the following popular
benchmark games: (i) a three-player variant of Kuhn poker (Kuhn, 1950); (ii) a two-player bargain-
ing game known as Sheriff (Farina et al., 2019e)—a benchmark game introduced specifically for the
study of correlated equilibria; and (iii) a three-player version of Liar's dice (LiSy et al., 2015). A
detailed description of each of the three game instances is available in Appendix D.
Kuhn poker
Sheriff
Figure 3: The performance of MWU, OMWU, and RM+ on three general-sum EFGs.
Liar’s dice
Figure 3 shows the performance of each of the three learning dynamics for computing EFCE. On
the x-axis we plot the number of iterations performed by each algorithm, and on the y-axis we plot
the EFCE gap, defined as the maximum advantage that any player can gain by defecting optimally
from the mediator’s recommendations. It should be noted that one iteration costs the same for every
algorithm, up to constant factors. We see that on every game, OMWU performs better than or on
par with RM+ and MWU. On Sheriff, OMWU performs significantly better than both RM+ and
MWU, by about an order of magnitude. One caveat to these results is that we did not use two tricks
that help CFR+ in two-player zero-sum EFG solving: alternation and linear averaging. These tricks
are known to retain convergence guarantees in that context (Tammelin et al., 2015; Farina et al.,
2019b; Burch et al., 2019), but it is unclear if they still guarantee convergence in the EFCE setting.
5	Conclusions
We described uncoupled no-regret learning dynamics so that if all agents play T repetitions of the
game according to the dynamics, the correlated distribution of play is an O(T -3/4)-approximate
EFCE. This substantially improves over the prior best rate of O(T -1/2 ). One of our conceptual
contributions is to connect the line of work on optimistic regret minimization with the framework
of Φ-regret. One of our main technical contributions is to characterize the stability of the fixed
points associated with trigger deviation functions through a refined perturbation analysis ofa certain
structured Markov chain, which may be of independent interest. Finally, experiments conducted on
standard benchmarks corroborated our theoretical findings.
9
Under review as a conference paper at ICLR 2022
References
Robert Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical
Economics ,1:67-96,1974.
Kimmo Berg and Tuomas Sandholm. Exclusion method for finding nash equilibrium in multiplayer
games. In AAAI Conference on Artificial Intelligence (AAAI), 2017.
Avrim Blum and Yishay Mansour. From external to internal regret. J. Mach. Learn. Res., 8:1307-
1324, 2007.
Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em
poker is solved. Science, 347(6218), January 2015.
Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
top professionals. Science, pp. eaao1733, Dec. 2017.
Neil Burch, Matej Moravcik, and Martin Schmid. Revisiting CFR+ and alternating updates. Journal
of Artificial Intelligence Research, 64:429-443, 2019.
Andrea Celli, Alberto Marchesi, Tommaso Bianchi, and Nicola Gatti. Learning to correlate in
multi-player general-sum sequential games. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS), volume 32, 2019.
Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for
extensive-form correlated equilibrium. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020.
Xi Chen and Binghui Peng. Hedging in games: Faster convergence of external and swap regrets.
In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS),
2020.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and
Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory,
pp. 6-1, 2012.
Constantinos Daskalakis, Paul Goldberg, and Christos Papadimitriou. The complexity of computing
a Nash equilibrium. In Proceedings of the Annual Symposium on Theory of Computing (STOC),
2006.
Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms
for zero-sum games. Games and Economic Behavior, 92:327-348, 2015.
Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning
in general games. CoRR, abs/2108.06924, 2021.
Miroslav DUdik and Geoffrey J. Gordon. A sampling-based approach to computing equilibria in SUc-
cinct extensive-form games. In Jeff A. Bilmes and Andrew Y. Ng (eds.), UAI 2009, Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, Montreal, QC, Canada,
June 18-21, 2009, pp. 151-160. AUAI Press, 2009.
Kousha Etessami and Mihalis Yannakakis. On the complexity of Nash equilibria and other fixed
points (extended abstract). In Proceedings of the Annual Symposium on Foundations of Computer
Science (FOCS), pp. 113-123, 2007.
Gabriele Farina, Christian Kroer, Noam Brown, and Tuomas Sandholm. Stable-predictive optimistic
counterfactual regret minimization. In International Conference on Machine Learning (ICML),
2019a.
Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Online convex optimization for sequential
decision processes and extensive-form games. In AAAI Conference on Artificial Intelligence,
2019b.
10
Under review as a conference paper at ICLR 2022
Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Optimistic regret minimization for
extensive-form games via dilated distance-generating functions. In Advances in Neural Infor-
mation Processing Systems, NeurIPS 2019,, pp. 5222-5232, 2019c.
Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Regret circuits: Composability of regret
minimizers. In International Conference on Machine Learning, pp. 1863-1872, 2019d.
Gabriele Farina, Chun Kai Ling, Fei Fang, and Tuomas Sandholm. Correlation in extensive-form
games: Saddle-point formulation and benchmarks. In Conference on Neural Information Pro-
cessing Systems (NeurIPS), 2019e.
Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Simple uncoupled no-regret
learning dynamics for extensive-form correlated equilibrium, 2021a.
Gabriele Farina, Andrea Celli, and Tuomas Sandholm. Efficient decentralized learning dynamics
for extensive-form coarse correlated equilibrium: No expensive computation of stationary distri-
butions required. ArXiv preprint, 2021b.
Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Better regularization for sequential deci-
sion spaces: Fast convergence rates for Nash, correlated, and team equilibria. In ACM Conference
on Economics and Computation, 2021c.
Andrew Gilpin and Tuomas Sandholm. Lossless abstraction of imperfect information games. Jour-
nal of the ACM, 54(5), 2007.
Geoffrey J Gordon, Amy Greenwald, and Casey Marks. No-regret learning in convex games. In
Proceedings of the 25th international conference on Machine learning, pp. 360-367. ACM, 2008.
Amy Greenwald and Amir Jafari. A general class of no-regret learning algorithms and game-
theoretic equilibria. In Conference on Learning Theory (COLT), Washington, D.C., 2003.
Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68:1127-1150, 2000.
Samid Hoda, Andrew Gilpin, Javier Pena, and TUomas Sandholm. Smoothing techniques for Com-
puting Nash equilibria of sequential games. Mathematics of Operations Research, 35(2), 2010.
Albert Xin Jiang and Kevin Leyton-Brown. Polynomial-time computation of exact correlated equi-
librium in compact games. Games Econ. Behav., 91:347-359, 2015.
Christian Kroer, Kevin Waugh, Fatma Kiling-Karzan, and Tuomas Sandholm. Faster algorithms for
extensive-form game solving via improved smoothing functions. Mathematical Programming,
2020.
Alex Kruckman, Amy Greenwald, and John R. Wicks. An elementary proof of the Markov chain
tree theorem. Technical Report 10-04, Brown University, 2010.
H. W. Kuhn. A simplified two-person poker. In H. W. Kuhn and A. W. Tucker (eds.), Contributions
to the Theory of Games, volume 1 of Annals of Mathematics Studies, 24, pp. 97-103. Princeton
University Press, Princeton, New Jersey, 1950.
Viliam Lisy, Marc Lanctot, and Michael Bowling. Online Monte Carlo Counterfactual regret min-
imization for search in imperfect information games. In Autonomous Agents and Multi-Agent
Systems, pp. 27-36, 2015.
Dustin Morrill, Ryan D’Orazio, Marc Lanctot, James R. Wright, Michael Bowling, and Amy R.
Greenwald. Efficient deviation types and learning for hindsight rationality in extensive-form
games. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference
on Machine Learning, ICML 2021, volume 139 of Proceedings of Machine Learning Research,
pp. 7818-7828. PMLR, 2021a.
11
Under review as a conference paper at ICLR 2022
Dustin Morrill, Ryan D’Orazio, Reca Sarfati, Marc Lanctot, James R. Wright, Amy R. Greenwald,
and Michael Bowling. Hindsight and sequential rationality of correlated play. In Thirty-Fifth
AAAI Conference on Artificial Intelligence, AAAI2021, pp. 5584-5594. AAAI Press, 2021b.
Yurii Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal of
Optimization, 16(1), 2005.
Christos H. Papadimitriou and Tim Roughgarden. Computing correlated equilibria in multi-player
games. J. ACM, 55(3):14:1-14:29, 2008.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Confer-
ence on Learning Theory, pp. 993-1019, 2013a.
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se-
quences. In Advances in Neural Information Processing Systems, pp. 3066-3074, 2013b.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of reg-
ularized learning in games. In Advances in Neural Information Processing Systems, pp. 2989-
2997, 2015.
Oskari Tammelin. Solving large imperfect information games using CFR+. arXiv preprint, 2014.
Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit
Texas hold’em. In Proceedings of the 24th International Joint Conference on Artificial Intelli-
gence (IJCAI), 2015.
Bernhard Von Stengel and Frangoise Forges. Extensive-form correlated equilibrium: Definition and
computational complexity. Mathematics of Operations Research, 33(4):1002-1022, 2008.
Martin Zinkevich, Michael Bowling, Michael Johanson, and Carmelo Piccione. Regret minimization
in games with incomplete information. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NIPS), 2007.
12