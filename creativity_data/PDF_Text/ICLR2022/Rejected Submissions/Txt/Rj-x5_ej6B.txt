Under review as a conference paper at ICLR 2022
Partial Information as Full:
Reward Imputation with Sketching in Bandits
Anonymous authors
Paper under double-blind review
Ab stract
We focus on the setting of contextual batched bandit (CBB), where a batch of re-
wards is observed from the environment in each episode. But these rewards are
partial-information feedbacks where the rewards of the non-executed actions are
unobserved. Existing approaches for CBB usually ignore the potential rewards of
the non-executed actions, resulting in feedback information being underutilized.
In this paper, we propose an efficient reward imputation approach using sketch-
ing in CBB, which completes the unobserved rewards with the imputed rewards
approximating the full-information feedbacks. Specifically, we formulate the re-
ward imputation as a problem of imputation regularized ridge regression, which
captures the feedback mechanisms of both the non-executed and executed actions.
To reduce the time complexity of reward imputation on a large batch of data, we
use randomized sketching for solving the regression problem of imputation. We
prove that the proposed reward imputation approach obtains a relative-error bound
for sketching approximation, achieves an instantaneous regret with a controllable
bias and a smaller variance than that without reward imputation, and enjoys a
sublinear regret bound against the optimal policy. Moreover, we present two ex-
tensions of our approach, including the rate-scheduled version and the version for
nonlinear rewards, which makes our approach more feasible. Experimental results
demonstrated that our approach can outperform the state-of-the-art baselines on a
synthetic dataset, the Criteo dataset, and a dataset from a commercial app.
1 Introduction
Contextual bandits have been widely used in real-world sequential decision-making problems (Li
et al., 2010; Lan & Baraniuk, 2016; Yom-Tov et al., 2017; Yang et al., 2021), where the agen-
t updates the decision-making policy fully online (i.e., at each step) according to the context and
corresponding reward feedback so as to maximize the cumulative reward. In this paper, we con-
sider a more complex setting—contextual batched bandits (CBB), where the decision process is
partitioned into N episodes, the agent interacts with the environment for B steps in one episode,
collects the reward feedbacks and contexts at the end of the episode, and then updates the policy
using the collected data for the next episode. CBB is more practical in some real-world applications,
since updating the policy once receiving the reward feedback is rather unrealistic due to its high
computational cost and decision instability.
In bandit settings, it is inevitable that the environment only reveals the rewards of the executed
actions to the agent as the feedbacks, while hiding the rewards of non-executed actions. We refer to
this category of limited feedback as the partial-information feedback (also called “bandit feedback”).
Existing batched bandit approaches in the CBB setting discard the information contained in the
potential rewards of the non-executed actions, address the problem of partial-information feedback
using an exploitation-exploration tradeoff on the context space and reward space (Han et al., 2020;
Zhang et al., 2020).But in the CBB setting, the agent usually estimates and maintains reward models
for the action-selection policy, and the potential rewards of the non-executed actions have been
somehow captured by the policy. This additional reward structure information is estimated and
available in each episode, however, are not utilized by existing batched bandit approaches.
In contextual bandit settings where the policy is updated fully online, several bias-correction ap-
proaches have been introduced to address the partial-information feedback. Dimakopoulou et al.
1
Under review as a conference paper at ICLR 2022
(2019) presented linear contextual bandits integrating the balancing approach from causal inference,
which reweight the contexts and rewards by the inverse propensity scores. Chou et al. (2015) de-
signed pseudo-reward algorithms for contextual bandits, which use a direct method to estimate the
unobserved rewards for the upper confidence bound (UCB) strategy. Kim & Paik (2019) focused
on the feedback bias-correction for LASSO bandit with high-dimensional contexts, and applied
the doubly-robust approach to the reward modification using average contexts. Although these ap-
proaches have been demonstrated to be effective in contextual bandit settings, little efforts have been
spent to address the under-utilization of partial-information feedback in the CBB setting.
Theoretical and experimental analyses in Section 2 indicate that better performance of CBB is
achievable if the rewards of the non-executed actions can be received. Motivated by these observa-
tions, we propose a novel reward imputation approach for the non-executed actions, which mimics
the reward generation mechanisms of environments. We conclude our contributions as follows.
•	To fully utilize feedback information in CBB, we formulate the reward imputation as a problem of
imputation regularized ridge regression, where the policy can be updated efficiently using sketching.
•	We prove that our reward imputation approach obtains a relative-error bound for sketching ap-
proximation, achieves an instantaneous regret with a controllable bias and a smaller variance than
that without reward imputation, has a lower bound of the sketch size independently of the overall
number of steps, enjoys a sublinear regret bound against the optimal policy, and reduces the time
complexity from O(Bd2) to O(cd2) for each action in one episode, where B denotes the batch size,
c the sketch size, and d the dimensionality of inputs, satisfying d < c < B.
•	We present two practical variants of our reward imputation approach, including the rate-scheduled
version in which the imputation rate is set without tuning, and the version for nonlinear rewards.
•	We carried out extensive experiments on the synthetic data, public benchmark, and the data col-
lected from a real commercial product to demonstrate our performance, empirically analyzed the
influence of different parameters, and verified the correctness of the theoretical results.
Related Work. Recently, batched bandit has become an active research topic in statistics and learn-
ing theory including 2-armed bandit (Perchet et al., 2016), multi-armed bandit (Gao et al., 2019;
Zhang et al., 2020; Wang & Cheng, 2020), and contextual bandit (Han et al., 2020; Ren & Zhou,
2020; Gu et al., 2021). Han et al. (2020) defined linear contextual bandits, and designed UCB-type
algorithms for both stochastic and adversarial contexts, where true rewards of different actions have
the same parameters. Zhang et al. (2020) provided methods for inference on data collected in batch-
es using bandits, and introduced a batched least squares estimator for both multi-arm and contextual
bandits. Recently, Esfandiari et al. (2021) proved refined regret upper bounds of batched bandits in
stochastic and adversarial settings. There are several recent works that consider similar settings to
CBB, e.g., episodic Markov decision process (Jin et al., 2018), LASSO bandits (Wang & Cheng,
2020). Sketching is another related technology that compresses a large matrix to a much smaller
one by multiplying a (usually) random matrix with certain properties (Woodruff, 2014), which has
been used in online convex optimization (Calandriello et al., 2017; Zhang & Liao, 2019).
2 Problem Formulation and Analysis
First, we introduce some notations. Let [x] = {1, 2, . . . , x}, S ⊆ Rd be the context space, A =
{Aj}j∈[M] the action space containing M actions,[A; B] = [A|, B|]|, kAkF, kAk1 kAk2 denote
the Frobenius norm, 1-norm, and spectral norm of a matrix A, respectively, kak1 and kak2 be the
'ι-norm and the '2-norm of a vector a, σmin(A) and σmaχ(A) denote the minimum and maximum
of the of singular values of A. In this paper, we focus on the setting of Contextual Batched Bandits
(CBB) (see Algorithm 1), where the decision process is partitioned into N episodes, and in each
episode, CBB consists of two phases: 1) the policy updating approximates the optimal policy based
on the received contexts and rewards; 2) the online decision selects actions for execution following
the updated and fixed policy p for B steps (also called the batch size is B), and stores the context-
action pairs and the observed rewards of the executed actions into a data buffer D . The reward R in
CBB is a partial-information feedback where rewards are unobserved for the non-executed actions.
Different from existing batch bandit setting (Han et al., 2020; Esfandiari et al., 2021), where the
true reward feedbacks for all actions are controlled by the same parameter vector while the contexts
received differ by actions at each step, we assume that in the CBB setting, the mechanism of true
reward feedback differs by actions and the received context is shared by actions. Formally, for any
context si ∈ S ⊆ Rd and action A ∈ A, we assume that the expectation of the true reward Rit,rAue
2
Under review as a conference paper at ICLR 2022
Algorithm 1 Contextual Batched Bandit (CBB)
INPUT: Batch size B, number of episodes N, action space A = {Aj}j∈[M], context space S ⊆ Rd
1:	Initialize policy po — 1/M, sample data buffer Di = {(s0,b,Aι0 h ,R0,b)}b∈[B] using initial policy po
2:	for n = 1 to N do
3:	Update the policy pn on Dn {Policy Updating}
4:	for b = 1 to B do
5:	Observe context sn,b
6:	Choose AIn,b ∈ A following the updated policy pn(sn,b) {Online Decision}
7:	end for
8:	Dn+i — {(sn,b,Aιn,b ,Rn,b}b∈[B],where Rn,b denotes the reward of action Aιn,b on context sn,b
9:	end for
is determined by an unknown action-specific reward parameter vector θA ∈ Rd: E [RtrAe | Si]=
hθA, Sii (the linear reward will be extended to the nonlinear case in Section 5). This setting for
reward feedback matches many real-world applications, e.g., each action corresponds to a different
category of candidate coupons in coupon recommendation, and the reward feedback mechanism of
each category differs due to the different discount pricing strategies.
Figure 1: Average rewards of batch UCB
policy (Han et al., 2020) under different pro-
portions of received reward feedbacks, in-
teracting with the synthetic environment in
Section 6, where x% feedback means that
x% of actions can receive their true rewards
Next, we provide deeper understandings of the influence of unobserved feedbacks on the perfor-
mance of policy updating in the CBB setting. We first conducted an empirical comparison by
applying the batch UCB policy (Han et al., 2020) to environments under different proportion-
s of received reward feedbacks. In particular, the agent under full-information feedback can re-
ceive all the rewards of the executed and non-executed actions, called Full-Information CBB (FI-
CBB) setting. From Figure 1, we can observe that the partial-information feedbacks could be
damaging in terms of hurting the policy updating, and batched bandit could benefit from more
reward feedbacks, where the performance of 80% feedback is very close to that of FI-CBB.
Then, we demonstrate the difference of instantaneous
regrets between the CBB and FI-CBB settings as in
Theorem 1. The detailed description and proof of
Theorem 1 can be found in Appendix A.
Theorem 1. For any action A ∈ A and contex-
t Si ∈ S, let θAn be the reward parameter vec-
tor estimated by the batched UCB policy in the n-
th episode. The upper bound of instantaneous regret
(defined by ∣hθAA, Sii 一 hθA, Sii∣) in the FI-CBB Set-
ting is tighter than that in CBB setting (i.e., using the
partial-information feedback).
From Theorem 1, we can conclude that the price
paid for having access only to the partial-information
feedbacks is the deterioration in the regret bound.
Ideally, the policy would be updated using the full-
information feedback. In CBB, however, the full-
information feedback is unaccessible. Fortunately, in
CBB, different reward parameter vectors need to be maintained and estimated separately for each
action, and the potential reward structures of the non-executed actions have been somehow cap-
tured. So why don’t we use these maintained reward parameters to estimate the unknown rewards
for the non-executed actions? Next, we propose an efficient reward imputation approach that uses
this additional reward structure information for improving the performance of the bandit policy.
3 Efficient Reward Imputation for Policy Updating
In this section, we present an efficient reward imputation approach tailored for policy updating in
the CBB setting.
Formulation of Reward Imputation. Since the true reward parameters differ by actions in the CBB
setting, we need to maintain a different estimated reward parameter vector for each action. As shown
in Figure 2, in contrast to CBB that ignores the contexts and rewards of the non-executed steps of
Aj , our reward imputation approach completes the missing values using the imputed contexts and
rewards, approximating the full-information CBB setting. Specifically, in the (n+ 1)-th episode, for
3
Under review as a conference paper at ICLR 2022
Context n
Matrix	AJ
N,「
-oo.∆
Nn
CBB
Reward
Vector
Imputed -
B 一
OO …Qo!oi:lQi
Reward
VeCtor
一NR
Imputed
Reward
Nj	Vector
Rn
Context Matrix
B -
B
Full-Information CBB
CBB with Our Reward Imputation
Reward
Vector
-OOO :.ooo_
Figure 2: Comparison of the stored data corresponding to the action Aj ∈ A in CBB, CBB with our
reward imputation, and full-information CBB, in the (n + 1)-th episode
each action Aj ∈ A,j ∈ [M], we store context vectors and rewards corresponding to the steps in
which the action Aj is executed, into a context matrix SA ∈ RNn ×d and a reward vector R%, ∈
RNn, respectively, where Nj denotes the number of steps (in episode n+1) in which the action AjiS
executed. Additionally, for any Aj ∈ A,j ∈ [M], we store context vectors corresponding to the non-
executed steps of action Aj (denote the number of non-executed steps by Nn, i.e., Nn = B - Nn),
Nn ×d -
into an imputed context matrix SA ∈ RNj	, and compute the imputed reward vector as follows:
n
RA j= {rn,1(Aj ),rn,2(Aj ),...,Tn,Nn (Aj )} ∈ R : j ∈ [M],
where rn,b(Aj) := hθAn , sn,bi denotes the imputed reward for each step b ∈ [Njn], and sn,b is the
b-th row of SAn . Then, we obtain several block matrices by concatenating the context and reward
matrices from the previous episodes: LA = [SA,.; ∙∙∙ ; Sn] ∈ RLn×d, TA = [RA,.; ∙∙∙ ; RAJ ∈
RLn, Ln = Pn=0Nj, LAj = [SAj；…；SAj] ∈ RLn×d,TAj = RAj；…；RAj] ∈ RLn, Ln =
PΑ=0 Nj. In the (n + 1)-th episode, the estimated parameter vector @A：1 of the imputed reward
for action Aj can be updated by solving the following imputation regularized ridge regression:
睨1=ajθ∈mm ILAjθ - TAjI+Y
'-----------------} I
^^^^^{^^^^^™
Observed Term
Lb nAjθ- TbAnj 2 +λkθk22,
_ - /
{z
Imputation Term
n = 0, 1, . . . , N - 1,
(1)
where γ ∈ [0, 1] is the imputation rate which controls the degree of reward imputation and measures
a trade-off between bias and variance (Remark 1&2), λ > 0 is the regularization parameter, yielding
θA+1 = (ψa+ 1)T (bA+1 + YbA+1)，	(2)
that can be obtained by the closed least squares solution, where ΨnA+1 := λId + ΦnA+1 + γΦb nA+1,
ΦA+1 = ΦAj + SAISA j,	bA+1 = bAj + SA| RAj,	(3)
ΦA+1 = ηΦAj+ SAISnj,	^A+1 = ηbAj + SAIRAj,	(4)
and η ∈ (0, 1) is the discount parameter which controls how fast the previous imputed rewards are
forgotten, and can help guaranteeing the regret bound in Theorem 2.
Efficient Reward Imputation using Sketching. As shown in the first 4 columns in Table 1, the
overall time complexity of the imputation for each action is O(Bd2) in each episode, where B
represents the batch size, and d the dimensionality of the input. Thus, for all the M actions in one
episode, reward imputation increases the time complexity from O(Bd2) of the approach without
imputation to O(M Bd2). To address this issue, we design an efficient reward imputation approach
using sketching, which reduces the time complexity of each action in one episode from O Bd2
to O cd2 , where c denotes the sketch size satisfying d < c < B and cd > B . Specifically, in
the (n + 1)-th episode, the imputation regularized ridge regression Eq.(1) can be approximated by a
sketched ridge regression as
θA+1 = argmdin IlnAj(LAjθ - TAj) ∣∣2 + Y IInAj (LAjθ - TAj) ∣∣2 + λkθk2,	(5)
4
Under review as a conference paper at ICLR 2022
Table 1: The time complexities of the original reward imputation in Eq.(1) (first 4 columns) and the
reward imputation using sketching in Eq.(5) (last 4 columns) for action Aj in the (n+ 1)-th episode,
where Njn (Njn) denotes the number of steps in which the action Aj is executed (non-executed) in
episode n+ 1, Nbjn +Njn = B, and the sketch size c satisfying d < c < B and cd > B (MM: matrix
multiplication; MI: matrix inversion; Overall: overall time complexity for action Aj in one episode)
Original reward imputation in Eq.(1)				Reward imputation using sketching in Eq.(5)			
Item	Operation	Eq.	Time	Item	Operation	Eq.	Time
ΦnA+j 1, Φb nA+j 1 bA+ 1, bA+ 1	MM	(3), (4)	O(Bd2)	GA+ 1, G A+ 1 PAj 1，PA+ 1	MM	(7), (8)	O(cd2)
	MM	(3), (4)	O(Bd)		MM	(7), (8)	O(cd)
(ΨnA+j1)-1	MI	(2)	O(d3)	(WA+1)-1	MI	(6)	O(d3)
	—			ΓAj, ΛAj	Sketching	一	O(Njnd)
	—			bAj, Λ Aj	Sketching	一	O(Nbjnd)
Overall	一	—	O(Bd2)	Overall	一	一	O(cd2)
where θA+1 denotes the estimated parameter vector of the imputed reward using sketching for action
A ∈ A, CAn ∈ Rc×Njn and CbAn ∈ Rc×Njn are the sketch submatrices for the observed term and the
imputation term, respectively, and the sketch matrices for the two terms can be represented as
∏Aj = [cAj,CAj,…，CAji ∈ Rc×Ln，∏Aj = [CA,，CA。，…，0工]∈ Rc×Ln.
We denote the sketches of the context matrix and the reward vector by ΓnA := CAn SAn ∈ Rc×d
Aj	Aj Aj
and ΛnA := CAn RAn ∈ Rc, the sketches of the imputed context matrix and the imputed reward
vector by ΓbnA := CbAn SbAn ∈ Rc×d and ΛbnA := CbAn RbAn ∈ Rc, and obtain the solution of Eq.(5):
θA+ 1 = (wa+1)T (PA1 + yPA+ 1)，	⑹
where η ∈ (0, 1) denotes the discount parameter, WAn+1 := λId + GnA+1+γGbnA+j1,and
GA+ 1 = GA。+ rAjrAj,	pA+ 1 = PA。+ γa∣ λa,,	⑺
Gn+
1 = ηGbAj + γA| γa, , pa+ 1 = ηpA + γA| λa,.	⑻
Using the parameter 0A：1, We obtain the sketched version of imputed reward as rn,b(Αj):=
hθAn , sn,bi at step b ∈ [Njn]. Finally, we specify that the sketch submatrices {CAn }A∈A,n∈[N] and
{CAn }A∈A,n∈[N] are the block construction of Sparser Johnson-Lindenstrauss Transform (SJLT)
(Kane & Nelson, 2014), where the sketch size c is divisible by the number of blocks D1. As shown
in the last 4 columns in Table 1, sketching reduces the time complexity from O(M Bd2) to O(M cd2)
for reward imputation of all M actions in one episode, where c < B. When Mc ≈ B, the overall
time complexity of our reward imputation using sketching is even comparable to that without reward
imputation which has a O(Bd2) time complexity.
Updated Policy using Imputed Rewards. Inspired by the UCB strategy (Li et al., 2010), the updat-
ed policy for online decision of the (n + 1)-th episode can be formulated using the imputed rewards
(parameterized by 8A+1 in Eq.(2)) or the sketched version of imputed rewards (parameterized by
θA+1 in Eq.(6)). Specifically, for a new context s,
origin policy pn+1 selects the action following A — argmaxh8A+1, Si + ω[sl (ΨA+1)-1s] 1,
A∈A
sketched policy Pn+1 selects the action following A — argmax〈0A+1, Si + α[sl( Wn+1)-1s] 1,
A∈A
where ω ≥ 0 and α ≥ 0 are the regularization parameters in policy and their theoretical values are
given in Theorem 4. We summarize the reward imputation using sketching and the sketched poli-
cy into Algorithm 2, called SPUIR. Similarly, we call the updating of the original policy that uses
reward imputation WithOUt sketching, the Policy Updating with Imputed Rewards (PUIR).
1Since we set the number of blocks of SJLT as D < d, we omit D in the complexity analysis.
5
Under review as a conference paper at ICLR 2022
Algorithm 2 Sketched Policy Updating with Imputed Rewards (SPUIR) in the (n + 1)-th episode
INPUT: Policy pn, Dn+ι, A = {Aj}j∈[M], α ≥ 0,η ∈ (0,1), Y ∈ [0,1], λ > 0, WAj = λId, GAj =
GAj = Od, PAj = PAj = 0, θAj = 0, j ∈ [M], batch size B, sketch size c, number of block D
OUTPUT: Updated policy Pn+ι
1:	For all j ∈ [M], store context vectors and rewards corresponding to the steps in which the action Aj is
executed, into ΓnAj ∈ RNjn×d and ΛnAj ∈ RNjn
2:	For all j ∈ [M], store context vectors corresponding to the steps in which the action Aj is not executed
into ΓA. ∈ RNbn×d, where Nj — B - Nj
j	jj
3:	Tn,b(Aj) - hβAj, Sn,bi, for all Aj ∈ A and b ∈ [Nn], where Sn,b is the b-th row of ΓAj∙
4:	Compute imputed reward vector RbA ^ {;rn,i(Aj),...,r Nn(Aj)} ∈ RNn for any j ∈ [M]
j	n,Nj
5:	for all action Aj ∈ A do
6:	GA+1 - GAj + ΓA∣ΓAj, pA+ 1 - PAj + ΓA∣ ΛAj {Eq.(7)}
7:	GA+1	一	ηGAj+ γA∣rAj,	PA+1	一 ηPAj+	bA| λAj	{Eq.(8)}
8:	WA+1 - λid + GA+1 + YGA+1, θA+1 一 (WA+1 )-1(pA+ 1 + YPA+1) {Eq.(6)}
9:	end for
10:	Pn+1(s) selects action A — argmaxA∈A h8A+1, Si + α[sl (WA+1)	s] 2 for a new context S
11:	return {θn+1}A∈A, {(WA+1)-1}a∈a
4 Theoretical Analysis
We provide the instantaneous regret bound, prove the approximation error of sketching, and analyze
the regret in the CBB setting. The detailed proofs can be found in Appendix B. We first demonstrate
the instantaneous regret bound of the original solution θA in Eq.(1).
Theorem 2 (Instantaneous Regret Bound). Let η ∈ (0, 1) be the discount parameter, γ ∈ [0, 1] the
imputation rate. In the n-th episode, if the rewards {Rn,b}b∈[B] are independent2 * and bounded by
CR, then, for any b ∈ [B] and ∀A ∈ A, with probability at least 1 - δ,
KθA, sn,b) - hθA, sn,bi∣ ≤ hλkθA ∣∣2 + V + Y 2 η-2 CImPi [s；b (田A)I sn,b] 2 ,	⑼
where Ψ% = λId + ΦA + γΦA, V = [2CR log(2MB∕δ)] 2, and CImP > 0. Thefirst term on the
right-hand side of Eq.(9) can be seen as the bias term for the reward imputation, while the second
term is the variance term. The variance term of our algorithm is not larger than that without the
reward imputation, i.e,forany S ∈ Rd, [sl (Ψ%)-1 s] 2 ≤ [sl (λId + Φ%)-1s] 2. Further, a larger
imputation rate Y leads to a Smaller variance term [sl (Ψ%)-1 s] 2.
Remark 1 (Smaller Variance). From Theorem 2, we can observe that the proposed reward impu-
tation achieves a SmaUer variance ([s∖ b (Ψ%)-1 Sn,b] 1) than that without the reward imputation.
Remark 2 (Controllable Bias). Our reward imputation approach incurs a bias term Y 1 η-2 CImP in
addition to the two bias terms λ∣θA∣∣2,ν that exist in every UCB-basedpolicy. But this additional
bias term is controllable due to the presence of imputation rate Y that can help controlling the addi-
tional bias. Moreover, the term CImP in the additional bias can be replaced by a function fImP(n),
and fImP(n) is monotonic decreasing w.r.t. number of episodes n provided that the mild condition
√η 二 Θ(d-1) holds (the definition and analysis about fImp can be found in Appendix B.1). Over-
all, the imputation rate Y controls a trade-off between the bias term and the variance term, and we
will design a rate-scheduled approach for choosing the imputation rate Y in Section 5.
Although some approximation error bounds using SJLT have been proposed (Nelson & Nguyen,
2013; Kane & Nelson, 2014; Zhang & Liao, 2019), it is still unknown what is the lower bound of
the sketch size while applying SJLT to the sketched ridge regression problem in our SPUIR. Next,
we prove the approximation error as well as the lower bound of the sketch size for the sketched ridge
regression problem. For convenience, we drop all the superscripts and subscripts in this result.
Theorem 3 (Approximation Error Bound of Imputation using Sketching). Denote the imputation
regularized ridge regression function by F(θ) (defined in Eq.(1)) and the sketched ridge regression
2The assumption about conditional independence of the rewards is commonly used in the bandits literature,
which can be ensured using a master technology as a theoretical construction (Auer, 2002; Chu et al., 2011).
6
Under review as a conference paper at ICLR 2022
function by FS(θ) (defined in Eq.(5)) for reward imputation, whose solutions (i.e., the estimated re-
ward parameter vectors) are θ = argmi□θ∈Rd F (θ) and θ = argmi□θ∈Rd F S(θ). Let Y ∈ [0,1] be
the imputation rate, λ > 0 the regularization parameter δ ∈ (0,0.1], ε ∈ (0,1), Lall = [L; √γL],
and ρλ = kLall k22 /(kLall k22 + λ). IfΠ and Πb are SJLT, assuming that D = Θ(ε-1 log3(dδ-1)) and
the sketch size C = Ω (d Polylog (dδ-1) /ε2) , with probability at least 1-δ,F (O) ≤ (1+ρλε)F (θ)
and ∣∣θ — 创2 = O (√ρλε) hold.
To measure the convergence of approximating the optimal policy in sequential decision-making, we
define the regret of SPUIR against the optimal policy as follows:
Reg ({AIn,b}n∈[N],b∈[B]) := maxA∈A Pn∈[N],b∈[B] [hθA, sn,bi -hθAIn b , sn,bi],
where In,b denotes the index of the executed action using the sketched policy Pn (parameterized by
{θAn }A∈A) at step b in the n-th episode. Finally, we demonstrate the regret bound of SPUIR.
Theorem 4 (Regret Bound of SPUIR). Let T = BN be the overall number of steps, η ∈ (0, 1)
be the discount parameter, γ ∈ [0, 1] the imputation rate, λ > 0 the regularization parameter,
CMax = maxA∈A ∣∣θA∣∣2, CImP be a positive constant. Assume that the conditional independence
assumption in Theorem 2 holds and the upper bound of rewards is CR, M = O(Poly(d)), T ≥ d2,
V = [2CR log(2MB∕δι)]2 with δι ∈ (0,1),
ω = λC-ax + v + γ1 η-2 CImp,	α = ωC°,
where Cα > 0 which decreases with increase of 1∕ε and ε ∈ (0, 1). Let δ2 ∈ (0, 0.1], ρλ < 1 be the
constant defined in Theorem 3, and Creg be a positive constant that decreases with increase of 1∕ε.
For the sketch matrices {ΠnA}A∈A,n∈[N] and {Π}A∈A,n∈[N], assuming that the number of blocks in
SJLT D = Θ(ε-1 log3(dδ-1)), and the SketCh size satisfying C = Ω (d Polylog (dδ2^1) ∕ε2) , then,
for an arbitrary sequence of contexts {sn,b}n∈[N],b∈[B], with probability at least 1 — N(δ1 + δ2),
Reg({AIn,b}n∈[N],b∈[B]) ≤ 2αCreg√10M log(T + 1)(√dT + dB) + O (TPpλEd/B) . (IO)
Remark 3. Setting B = O(，T/d) and ρλe = 1/d yields a Sublinear upper bound of regret of
order O(,MdT)3 provided that the sketch size C = Ω(ρλd3 Polylog(dδ-1)). We can observe that
the lower bound of C is independent of the OVerall number of steps T, and a theoretical value of
the batch size is B = CB ,T∕d = CBN/d, where setting CB ≈ 25 is a suitable choice that has
been verified in the experiments in Section 6. In particular, when ρλ = O(1/d), the sketch size of
order C = Ω(d polylogd) is sufficient to achieve a sublinear regret, which has been demonstrated
in our experimental results. Since the lower bound of regret for contextual batched bandit in (Han
et al., 2020) assumes that there are only two actions and both the actions share the same true reward
model, it can not be applied to our CBB setting where each action corresponds to a different reward
model. Despite the lack of the lower bound in CBB setting, from the theoretical results of regret,
we can observe that our SPUIR admits several advantages: (a) The order of our regret bound is
not higher than those in the literature of contextual bandits in the fully-online setting (i.e., B = 1)
(Li et al., 2019; Dimakopoulou et al., 2019), which is a more simple setting than ours; (b) The
first term in the regret bound Eq.(10) measures the performance of policy updating using imputed
rewards (called “policy error”). From Theorem 2 and Remark 1&2, we obtain that, in each episode,
our policy updating has a smaller variance than the policy without the reward imputation, and
incurs a decreasing additional bias under mild conditions, leading to a tighter regret (i.e., smaller
policy error) after some number of episodes. (c) The second term on the right-hand side ofEq.(10)
is of order O(Typxed/B), which is incurred by the sketching approximation using SJLT (called
“sketching error”). This sketching error does not have any influence on the order of regret of SPUIR,
which may even have a lower order with a suitable choice of ρλε, e.g., setting ρλε = T-1/4d-1
yields a sketching error oforder O(T3/8d1/2) provided that C = Ω(ρλd3 Polylog(dδ-1)√T).
5 Extensions of Our Approach
To make the proposed reward imputation approach more feasible and practical, we tackle the follow-
ing two research questions by designing variants of our approach following the theoretical results:
3We use the notation of Oe to suppress logarithmic factors in the overall number of steps T .
7
Under review as a conference paper at ICLR 2022
RQ1 (Parameter Selection): Can we set the imputation rate γ without tuning?
RQ2 (Nonlinear Reward): Can we apply the proposed reward imputation approach to the case
where the expectation of true rewards is nonlinear?
Rate-Scheduled Approach. For RQ1, we equip PUIR and SPUIR with a rate-scheduled approach,
called PUIR-RS and SPUIR-RS, respectively. From Remark 1&2, a larger imputation rate γ leads
to a smaller variance while increasing the bias, while the bias term includes a monotonic decreasing
function w.r.t. number of episodes under mild conditions. Therefore, we can gradually increase γ
with the number of episodes, avoiding the large bias at the beginning of reward imputation. Specifi-
cally, we set γ = X% for episodes from (X - 10)% × N to X% × N, where X ∈ [10, 100].
Application to Nonlinear Rewards. For RQ2, we provide nonlinear versions of reward imputa-
tion. We use linearization technologies of nonlinear rewards, rather than directly setting the rewards
as nonlinear functions (Valko et al., 2013; Chatterji et al., 2019), avoiding the linear regret or curse
of kernelization. Specifically, instead of using the linear imputed reward rn,b(Aj) := hθ%, SnQ,
we use the following linearized nonlinear imputed rewards, denotes by Tn,b(θ, A):
1)	SPUIR-Exp. We assume that the expected reward is an exponential function as GE(θ, s) =
exp (θls). Then Tn,b(θ, A) = (θ, VθGe(Θ, snb) , where NeGe(Θ, Sn,b) = exp (θlSn,b) Sn,b.
2)	SPUIR-Poly. When the expected reward is a polynomial function as Gp(θ, S) = (θls)2. Then
Tn,b(θ,A) = hθ, VθGp(Θ, Snb), where NeGp(Θ, Sn,b) = 2(θlSn,b) sn.,b.
3)	SPUIR-Kernel. Consider that the underlying expected reward in a Gaussian reproducing kernel
Hilbert space (RKHS). We use Tn,b(θ, A) = hθ, φ(Sn,b)) in random feature space, where the ran-
dom feature mapping φ can be explicitly defined as in (Rahimi & Recht, 2007).
For SPUIR-Exp and SPUIR-Poly, combining the linearization of convex functions (Shalev-Shwartz,
2011) with Theorem 4 yields the regret bounds of the same order. For SPUIR-Kernel, using the
approximation error of random features (Rahimi & Recht, 2008), We can obtain a regret bound with
an additional error of order O(B∕√dj, where d is the dimension of the random features.
6	Experiments
We empirically evaluated the performance of our algorithms on 3 datasets: the synthetic dataset,
publicly available Criteo dataset4 (Criteo-recent, Criteo-all), and dataset collected from
a real commercial app for coupon recommendation (commercial product).
Experimental Settings. We compared our algorithms with: Sequential Batch UCB (SBUCB) (Han
et al., 2020), Batched linear EXP3 (BEXP3) (Neu & Olkhovskaya, 2020), Batched linear EXP3
using Inverse Propensity Weighting (BEXP3-IPW) (Bistritz et al., 2019), Batched Balanced Linear
Thompson Sampling (BLTS-B) (Dimakopoulou et al., 2019), and Sequential version of Delayed
Feedback Model (DFM-S) (Chapelle, 2014). We implemented all the algorithms on Intel(R) X-
eon(R) Silver 4114 CPU@2.20GHz, and repeated the experiments 20 times. We tested the perfor-
mance of algorithms in streaming recommendation scenarios, where the reward is represented by
a linear combination of the click and conversion behaviors of users. According to Remark 3, we
set the batch size as B = CB2 N/d, the constant CB ≈ 25, and the sketch size c = 150 on all the
datasets. The average reward was used to evaluate the accuracy of algorithms.
Performance Evaluation. Figure 3(a)-(c) reports the average reward of SPUIR with its variants and
the baselines. We observed that SPUIR and its variants achieved higher average rewards, demon-
strating the effectiveness of our reward imputation. Moreover, SPUIR and its rate-scheduled version
SPUIR-RS had similar performances compared with the origin PUIR, which indicates the practi-
cal effectiveness of our variants and verifies the correctness of the theoretical analyses. The results
on commercial product in Table 2 indicate that SPUIR outperformed the second-best base-
line with the improvements of 1.07% CVR (conversion rate) and 1.12% CTCVR (post-view click-
through&conversion rate). Besides, our reward imputation approaches were more efficient than
DFM-S, BLTS-B. The variants using sketching of our algorithms (SPUIR, SPUIR-RS) significantly
reduced the time costs of reward imputation, and took less than twice as long to run compared to the
baselines without reward imputation (SBUCB, BEXP3, BEXP3-IPW). Figure 3(d) illustrates per-
formances of SPUIR and its nonlinear variants, where SPUIR-Kernel achieved the highest rewards
indicating the effectiveness of the nonlinear generalization of our approach. For different decision
tasks, a suitable nonlinear reward model needs to be selected for better performances.
Parameter Influence. From the regret bound Eq.(10), we can observe that a larger batch size B
results in a larger first term (of order O(B), called policy error) but a smaller second term (of order
4https://labs.criteo.com/2013/12/conversion-logs-dataset/
8
Under review as a conference paper at ICLR 2022
Table 2: Performance comparison of coupon recommendation on commercial product
Algorithm	CVR (mean ± std)	CTCVR (mean ± std)	Time (sec., mean ± std)
DFM-S	0.8656 ± 0.0473	0.3317 ± 0.0218	302.3140 ± 8.3045
SBUCB	0.8569 ± 0.0037	0.4277 ± 0.0084	43.5435 ± 0.3659
BEXP3	0.4846 ± 0.0205	0.2425 ± 0.0116	53.5001 ± 0.9220
BEXP3-IPW	0.4862 ± 0.0187	0.2436 ± 0.0113	56.0101 ± 1.4142
BLTS-B	0.8663 ± 0.0178	0.4285 ± 0.0157	218.2109 ± 1.8198
PUIR	0.8807 ± 0.0053	-^0.4411 ± 0.0029	184.3575 ± 2.2346
SPUIR	0.8770 ± 0.0059	0.4397 ± 0.0032	81.5753 ± 1.5879
PUIR-RS	0.8763 ± 0.0056	0.4389 ± 0.0030	180.4999 ± 1.7763
SPUIR-RS	0.8758 ± 0.0058	0.4391 ± 0.0031	80.8003 ± 2.9030
0.44
OiiO-
_ _ __ __
PJeΛ∖3sυπEυ><
10 SO 30	40 SO ββ 70	80 W	O
Number of Episodes N
εeM3H 36EaΛ4
50	1∞	150	200	250	3∞
Number of Episodes N
_ ___ ____
PJeM8α 36eJ3><
20	40	¢0	80	100	12。
Number of Episodes N
(a) synthetic data
_ __ ___
P」"M8H 36e∙l3><
s0ΛΛω≈Φ6eω>v
(b) Criteo-all
PJeM0,a06e∙!0><
(c) commercial product
10%N 20%N 30%N 40%N 50%N 60%N 70%N 80%N 90%N
N
0.260-
0.258-
0.256-
0.254-
0.252-
Number of Episodes N
Sketch SiZe C
(d) nonlinear reward	(e) batch size B	(f) sketch size c
Figure 3: (a), (b), (c): Average rewards of the compared algorithms, the proposed SPUIR and
its variants on synthetic dataset, Criteo dataset, and the real commercial product data, where we
omitted the curves of algorithms whose average rewards are 5% lower than the highest reward; (d):
SPUIR and its three nonlinear variants on synthetic dataset; (e): SPUIR with different batch sizes
on Criteo-recent; (f): SPUIR and SPUIR-RS with different sketch sizes on synthetic dataset
O(1/B), called sketching error), indicating that a suitable batch size B needs to be set. This conclu-
sion was empirically verified in Figure 3(e), where B = 1, 000 (CB = 25) yields better empirical
performance in terms of the average reward. Similar phenomenon can also be observed on Criteo
dataset and commercial product. Allofthe results verified the theoretical results in Remark 3:
B = CB pT/d = CBN/d is a suitable choice while setting CB ≈ 25. From the results in Fig-
ure 3(f) we observe that, for our SPUIR and SPUIR-RS, the performances significantly increased
when the sketch size c reached 10%B (≈ dlog d), which demonstrates the conclusion in Remark 3
that only the sketch size of order C = Ω(d Polylogd) is needed for satisfactory performance.
7	Conclusion
Partial-information feedback is ubiquitous in real-world applications, where reward feedback is usu-
ally underutilized for learning. This paper proposes a theoretically sound and computationally ef-
ficient reward imputation approach for contextual batched bandits, which mimics the reward gen-
eration mechanism of the environment approximating the setting of full-information feedback. The
proposed reward imputation approach reduces the time complexity of imputation on large batches
of data using sketching, achieves a relative-error bound for sketching approximation, has an instan-
taneous regret with a controllable bias and a smaller variance, and enjoys a sublinear regret bound
against the optimal policy. The theoretical formulation and algorithmic implementation may provide
an efficient reward imputation scheme for online learning under limited feedback.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
To verify the effectiveness and efficiency of our algorithms on real products, we conducted experi-
ments on a real dataset collected from a commercial social app. We call this dataset commercial
product, where the data were collected after the users gave consent, and did not contain any
personally identifiable information or offensive content.
Reproducibility S tatement
The source code of the proposed algorithms is submitted as supplementary materials. For theoretical
results, clear explanations of any assumptions and a complete proof of the claims are included as an
appendix. The detailed experimental settings are also provided in the appendix. Since the dataset
commercial product is non-public, we did not provide a URL. We will release this non-public
dataset after the publication of this paper, and the link to download this non-public dataset is to be
included in the final paper.
References
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3:397-422, 2002.
Ilai Bistritz, Zhengyuan Zhou, Xi Chen, Nicholas Bambos, and Jose Blanchet. Online EXP3 learn-
ing in adversarial bandits with delayed feedback. In Advances in Neural Information Processing
Systems 32, pp. 11349-11358, 2019.
Jean Bourgain, Sjoerd Dirksen, and Jelani Nelson. Toward a unified theory of sparse dimensionality
reduction in Euclidean space. In Proceedings of the 47th Annual ACM on Symposium on Theory
of Computing, pp. 499-508, 2015.
Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Efficient second-order online kernel
learning with adaptive embedding. In Advances in Neural Information Processing Systems 30,
pp. 6140-6150, 2017.
Olivier Chapelle. Modeling delayed feedback in display advertising. In Proceedings of the 20th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1097-
1105, 2014.
Niladri S. Chatterji, Aldo Pacchiano, and Peter L. Bartlett. Online learning with kernel losses. In
Proceedings of the 36th International Conference on Machine Learning, pp. 971-980, 2019.
Ku-Chun Chou, Hsuan-Tien Lin, Chao-Kai Chiang, and Chi-Jen Lu. Pseudo-reward algorithms for
contextual bandits with linear payoff functions. In Proceedings of the 6th Asian Conference on
Machine Learning, pp. 344-359, 2015.
Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the 14th International Conference on Artificial Intelligence and
Statistics, pp. 208-214, 2011.
Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. Balanced linear contextu-
al bandits. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, pp. 3445-3453,
2019.
Hossein Esfandiari, Amin Karbasi, Abbas Mehrabian, and Vahab S. Mirrokni. Regret bounds for
batched bandits. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, pp. 7340-
7348, 2021.
Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem.
In Advances in Neural Information Processing Systems 32, pp. 501-511, 2019.
Quanquan Gu, Amin Karbasi, Khashayar Khosravi, Vahab Mirrokni, and Dongruo Zhou. Batched
neural bandits. arXiv preprint arXiv:2102.13028, 2021.
10
Under review as a conference paper at ICLR 2022
Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose H. Blanchet, Peter W. Glynn, and Yinyu Ye.
Sequential batch learning in finite-action linear contextual bandits. CoRR, abs/2004.06321, 2020.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is q-learning Provably effi-
cient? In Advances in Neural Information Processing Systems 31, 2018.
Daniel M Kane and Jelani Nelson. SParser Johnson-Lindenstrauss transforms. Journal of the ACM,
61(1):4:1-4:23,2014.
Gi-Soo Kim and Myunghee Cho Paik. Doubly-robust lasso bandit. In Advances in Neural Informa-
tion Processing Systems 32, PP. 5869-5879, 2019.
Andrew S. Lan and Richard G. Baraniuk. A contextual bandits framework for Personalized learning
action selection. In Proceedings of the 9th International Conference on Educational Data Mining,
PP. 424-429, 2016.
Lihong Li, Wei Chu, John Langford, and Robert E SchaPire. A contextual-bandit aPProach to
Personalized news article recommendation. In Proceedings of the 19th International Conference
on World Wide Web, PP. 661-670, 2010.
Shuai Li, Wei Chen, Shuai Li, and Kwong-Sak Leung. ImProved algorithm on online clustering of
bandits. In Sarit Kraus (ed.), Proceedings of the 28th International Joint Conference on Artificial
Intelligence, volume Li2019ImProved, PP. 2923-2929, 2019.
Jelani Nelson and Huy L Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser
subsPace embeddings. In Proceedings of the 54th Annual Symposium on Foundations of Comput-
er Science, pp. 117-126, 2013.
Gergely Neu and Julia Olkhovskaya. Efficient and robust algorithms for adversarial linear contextual
bandits. In Proceedings of the 33rd Conference on Learning Theory, pp. 3049-3068, 2020.
Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit prob-
lems. The Annals of Statistics, 44(2):660-681, 2016.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems 20, pp. 1177-1184, 2007.
Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in Neural Information Processing Systems 21, pp.
1313-1320, 2008.
Zhimei Ren and Zhengyuan Zhou. Dynamic batch learning in high-dimensional sparse linear con-
textual bandits. arXiv preprint arXiv:2008.11918, 2020.
Yuta Saito, Gota Morishita, and Shota Yasui. Dual learning algorithm for delayed conversions. In
Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval, pp. 1849-1852, 2020.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and TrendsR
in Machine Learning, 4(2):107-194, 2011.
Michal Valko, Nathaniel Korda, Remi Munos, Ilias N. Flaounas, and Nello Cristianini. Finite-time
analysis of kernelised contextual bandits. In Proceedings of the 29th Conference on Uncertainty
in Artificial Intelligence, 2013.
Chi-Hua Wang and Guang Cheng. Online batch decision-making with high-dimensional covariates.
In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, pp.
3848-3857, 2020.
Shusen Wang, Alex Gittens, and Michael W. Mahoney. Sketched ridge regression: Optimization
perspective, statistical perspective, and model averaging. In Proceedings of the 34th International
Conference on Machine Learning, pp. 3608-3616, 2017.
David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and TrendsR in
Theoretical Computer Science, 10(1-2):1-157, 2014.
11
Under review as a conference paper at ICLR 2022
Jiaqi Yang, Wei Hu, Jason D. Lee, and Simon Shaolei Du. Impact of representation learning in
linear bandits. In Proceedings of the 9th International Conference on Learning Representations,
2021.
E. Yom-Tov, G. Feraru, M. Kozdoba, S. Mannor, and I. Hochberg. Encouraging physical activity in
patients with diabetes: Intervention using a reinforcement learning system. Journal of Medical
Internet Research, 19(10):e338, 2017.
Yuya Yoshikawa and Yusaku Imai. A nonparametric delayed feedback model for conversion rate
prediction. arXiv:1802.00255v1, 2018.
Kelly W. Zhang, Lucas Janson, and Susan A. Murphy. Inference for batched bandits. In Advances
in Neural Information Processing Systems 33,pp. 9818-9829, 2020.
Xiao Zhang and Shizhong Liao. Incremental randomized sketching for online kernel learning. In
Proceedings of the 36th International Conference on Machine Learning, pp. 7394-7403, 2019.
12
Under review as a conference paper at ICLR 2022
A Detailed Problem Formulation and Detailed Proof in Problem
Analysis
In this part, we give the detailed problem formulation and the detailed proof of Theorem 1 in problem
analysis in Section 2.
A. 1 Detailed Problem Formulation of CBB
In this paper, we focus on the setting of contextual batched bandits (CBB), which can be formulated
as a 6-tuple hS, A, p, R, N, Bi:
Context space S ⊆ Rd means a vector space containing the context information received at each
step, e.g., context summarizes the information of both the user and items in recommendation sce-
narios.
Action space A = {Aj }j∈[M] contains M candidate actions for execution. As an example, in rec-
ommender systems, each action corresponds to a candidate item, and selecting an action means that
the corresponding item is recommended.
Policy p determines which action to take at each step, which is a function of the context s ∈ S and
outputs an action for execution (or a selection distribution over action space A).
Reward R in CBB is a partial-information feedback where rewards are unobserved for the non-
executed actions. Consider a stochastic bandit setting, where the expectation of the true reward is
assumed to be a function of the context s ∈ S. In particular, different from the shared expectation
function of true rewards in existing batch bandits (Han et al., 2020), we assume that the expectation
functions of true rewards are different for each action, where each expectation function corresponds
to an unknown parameter vector θA ∈ Rd, A ∈ A. This setting for rewards matches many real-
world applications, e.g., each action corresponds to a different category of candidate coupons in
coupon recommendation.
Number of episodes N . The decision process in CBB is partitioned into N episodes. Within one
episode, the agent updates the policy using the collected data, and then interacts with the environ-
ment for multiple steps using the updated and fixed policy.
Batch size B is the number of steps in each episode. That is, in each episode, the agent interacts
with the environment B times using a fixed policy, and stores the contexts, executed actions, and
observed rewards into a data buffer D at the end of each episode.
A.2 Detailed Description and Proof of Theorem 1 in Problem Analysis
We present some theoretical findings about the regret difference between the partial-information
feedback and the full-information feedback. Assuming that the agent in the CBB setting can observe
the rewards of all the candidate actions from the environment at each step, we apply the batched UCB
policy (Han et al., 2020) to this setting (see Algorithm 3), We demonstrate an instantaneous regret
bound in Theorem 5, where Theorem 5 is a detailed version of Theorem 1 in Section 2.
Algorithm 3 Batch UCB Policy Updating in the (n+ 1)-th episode in Full-Information CBB Setting
INPUT: Policy pn, data buffer Dn+ι, action space A = {Aj }j∈[M], θA j = 0, j ∈ [M], batch size B
OUTPUT: Updated policy pn+1
1:	Let Len ∈ R(n+1)B×d be the matrix that stores all the context vectors till the n-th episode as the row
vectors
2:	For ∀A ∈ A, let TeAn ∈ R(n+1)B be the reward vector that stores all the rewards of action A ∈ A till the
n-th episode
3:	// Policy Updating
4:	Υn + 1 — LnLn
5:	for all action A ∈ A do
6:	θA+1 一 (Id + Υn+l)-1L nτn
7:	end for
8:	For a new context s, pn+ι(s) is to choose the action following: A — arg max (θA+1, S
A∈A
9:	return {θA+1}a∈a
13
Under review as a conference paper at ICLR 2022
Theorem 5 (Instantaneous Regret Bound in Full-Information CBB Setting, Detailed Version of
Theorem 1). Let Ln-1 ∈ RnB×d be the matrix that stores all the context vectors till the (n - 1)-th
n1
episode as the row vectors, and TAn-1 ∈ RnB be the reward vector that stores all the rewards of
action A ∈ A till the (n - 1)-th episode. Given the action space A = {Aj }j∈[M], in the n-th
episode, assume that the rewards are independent and bounded by CR. Then, with probability at
least 1 - δ, for any b ∈ [B] and ∀A ∈ A, we have the following instantaneous regret bound in the
n-th episode
lhθA, Sn,bi-hθA, Sn,bil≤ kθA k2 +，2CR lθg(2MB∕δ) Jsn,b (Id + Yn)Tsn,b,	(11)
|
where Υn = L|n-1Ln-1 and the parameter of reward model θAn in the batched UCB policy is
obtained by
θA ：= argmin IlLn-Iθ - TnTl2 + kθk2 = (Id + Yn)T Ln-ITn-1∙
θ∈Rd	2
Further, the instantaneous regret bound Eq.(11) in FI-CBB setting is tighter than that in
CBB setting (i.e., using the Partial-information feedback). In particular, the variance term
s|n,b (Id + Yn)-1 sn,b is smaller than that in CBB setting.
Proof of Theorem 1. By the formulation of θAn and the triangle inequality, we first obtain that
lhθA, sn,biTθA, sn,bil
=sn,b (Id+Yn)TL n-iTn-1 - sn,bθA∣
=sn,b (Id+Yn)ThLn-ιTτι - (Id+Yn)θa i∣
=sn,b(Id+Yn)ThLn-ιTτι - (Id+Ln-iLm)θai∣
=sn,b (Id + Yn)TLn-1 (TAT- Ln-lθA) - sn,b (Id + Yn)T θa ∣
≤ sn,b (Id + Yn)T Ln-1 (TAT- Ln-lθA) ∣ + lsn,b (Id + Yn)T θA∣
Next, we bound the two terms in the last row of Eq.(12).
(12)
Bounding ∣sn,b (Id + Yn)TLn-i (TnT- Ln-ιθA) ∣:
Since E [Tn-1] = Len-iθA and the received rewards are independent, by the AzUma-Hoeffding
bound, we have
—
(13)
V2sn,b (Id + Yn)-1sn,b
≤ 2 exp -
2CR kLn-1 (Id + Yn)Tsn,b k2
,
where ν > 0 is some constant. Since
kLen-1 (Id + Yn)-1 sn,bk22 = s|n,b
≤ s|n,b
≤s
s
Pr	Isn,b
combing with Eq.(13) implies the following results
≤ 2 exp
(14)
14
Under review as a conference paper at ICLR 2022
Combing Eq.(14) with the union bound, yields that, with probability at least 1 - δ, for any b ∈ [B]
and ∀A ∈ A,
卜 n,b (Id + Yn)T L n-1 (TAT- L n-lθA )| ≤ V q S^b (Id + Υn ) — 1 Sn,b,	(15)
where the failure probability is
δ = 2MB exp 卜 2C2 },
yielding that V = P2CR log(2MB∕δ).
Bounding 卜：,b (Id + Yn)-1 θj:
Since Yn is positive semi-definite, combining with the Holder inequality, We obtain
卜 n,b (Id + Yn)-1 ΘA∣ ≤ kθA k2∣∣(Id + Yn)-1 Sn,b∣l2
=kθAk2 Jsn,b (Id + Yn)T (Id + Yn)Tsn,b
VZ_________________________________________ (16)
≤ kθ"∣2 √sn,b (Id + Yn)T (Id + Yn)(Id + Yn)Tsn,b
=kθAk2 qsn,b (Id + Yn) 1 sn,b.
Combing Eq.(15) and Eq.(16) concludes the proof.
Similarly to the proof of Eq.(29), we can obtain that the variance term in full-information setting is
smaller than that in partial-information setting.	口
B	Detailed Proofs in Theoretical Analysis
In this section, we provide the instantaneous regret bound in each episode, prove the approximation
error of sketching, and analyze the regret for policy updating in the CBB setting. Figure 4 describes
the dependence structure of our theoretical results.
Figure 4: The dependence structure of our theoretical results, where the proof of instantaneous re-
gret bound (Theorem 2) is provided in Appendix B.1, the analysis of approximation properties of
sketching (Theorem 6) is given in Appendix B.2, the approximation error bound of reward imputa-
tion (Theorem 3) is proven in Appendix B.3, and the regret of SPUIR (Theorem 4) is analyzed in
Appendix B.4
15
Under review as a conference paper at ICLR 2022
B.1	Proof of Theorem 2
Before we provide the detailed proof of Theorem 2, we first demonstrate a lemma about the conver-
gence and monotonicity of the sum of functions, which is the main tool for analyzing the additional
bias of reward imputation.
Lemma 1 (Convergence and Monotonicity). Let f (n) = P；=i an-j ∙ g(j), where a ∈ (0,1) and
n is a positive integer. Then,
1)	when g(j) is convergent, the limit limn→∞ f(n) exists. Moreover,
lim f (n) =	— lim g(n).	(17)
n→∞	1 - a n→∞
2)	f(n) is a monotonic decreasing function if and only if g(j) satisfies, for any positive integer
j≥2,
(j - 1)aj-1g(1)
g(j) ≤((I- a) [ajT-(I- a)jT]
I--------20-1---------g ⑴
a= 1/2,
a 6= 1/2.
(18)
Proof. Letting b(j) = a-j ∙ g(j), ∀j ∈ [n], and S(n) = P；=i b(j), f(n) can be rewritten as
f(n) = an S (n).
1)	Rewriting f(n) = S(n)/a-n, from the Stolz’s theorem, we have
lim f(n)
n→∞
lim
n→∞
S(n) -S(n- 1)
a-n — a-(nT)
lim
n→∞ a
b(n)
-n — a-(nT)
a-n ∙ g(n)
lim -------7-ττ
n→∞ a-n - a-(n-1)
1	1∙，、
;---lim g(n).
1 - a n→∞
2)	The condition that f (∙) is a monotonic decreasing function is equivalent to the following Condi-
tion: for and positive integer n,
f(n+ 1) ≤ f(n)
⇔ an+1S(n + 1) ≤ anS(n)
⇔ a[S (n) + b(n + 1)] ≤ S(n)
⇔ b(n + 1) ≤ (1/a - 1) S(n).	(19)
From the equivalent condition Eq.(19), we obtain the following recursion formula:
b(n + 1) ≤ (1/a - 1) S(n)
b(n) ≤ (1/a - 1) nX-1 b(j)
j=1
.
.
.
b(3) ≤ (1/a - 1) [b(1) + b(2)]
b(2) ≤ (1/a - 1) b(1),
yielding that, for any positive integer j ≥ 2,
b(j) ≤ [(1/a - 1) + (1/a - 1)2 + ∙→ (1/a - 1)j-1] b(1).	(20)
16
Under review as a conference paper at ICLR 2022
From Eq.(20), for a = 1/2,
b(j) ≤
(1/a - 1) [1 - (1∕a - U-1] Mn
1 - (1/a - 1)	( )
(I - a) [1 - (I/a - 1)jτ]
2a - 1	()
(21)
and substituting the definition of b(j) into Eq.(21) yields the equivalent condition
gCj) ≤
(I - a) [1 - (1/a - W-1]
2a - 1
aj-1
g(1)∙
For a =1/2, we have the condition b(j) ≤ (j - 1)b(1), which is equivalent to
a-j ∙ g(j) ≤ (j - 1)a-1 ∙ g(1) ⇔ g(j) ≤ (j - 1)aj-1g(1).
□
Next, we provide the detailed proof of Theorem 2.
ProofofTheorem 2. From the formulation of。么 and the triangle inequality, we can obtain that, for
each action A ∈ A,
∣<θA, sn,b) - hθA, Sn,bi∣
= ∣sn,b (ψA)-1(bA + YBA)-Sn,b θA ∣
sι
s,
鼠(ψn )-1[(LnT)T TTI+γ
九 &n )-1[(LnT)T TTI+Y
T ∕bn— 1
TA
= ∣sn,b (ΨA)-1{(LAT)T 以T + Y
卜 Id+(LnT)TLnT+Y (L n-1
∕bn— 1
TA
∕bn— 1
TA
-ΨAθA ]∣
-八乙 + ΦA + YφA) θA]∣
= ∣sn,b (ψn )-1(l『)TeTAnT- LnT
LAT] θA力
ΘA) - λsn,b (ΨA)-1 θA+
T
T
—
sn,b (ψn)—1Y (l n-1)T (宣 T- L 1旬)∣
≤ 卜 n,b (ψn )-1(l『)T(TAnT- ljθn ) ∣+λ 卜 n,b (ψn)-1 θ"+
I
XAI)
卜 n,b @n)—1Y (L n-1)T (TAnT- L『
X---------------------------------
/ * X---------V----------Z
XA2)
,
χ(3)
XA
Next, we bound X『,Xf, and X(3). For convenience, we drop all the superscripts and subscripts
about n and b. Similarly to the proof of Theorem 1, we bound X(D + X(2) as follows: with
probability at least 1 - δ,
X(I) + XF ≤ (λkθ; ∣∣2 + V) JsTm—1s,
where V = ,2CR log(2MB/6). For X(3), using the Cauchy-Schwarz inequality, we have
X(3) ≤ YIILA亚-1s∣∣∕TA- LAθ1∣2
=√YJsτψ41 (YLALA)亚—1s IITA- LAθA∣∣2
≤ VYjsTR—1s ∣∣Ta- LAθA∣∣2.
(22)
(23)
17
Under review as a conference paper at ICLR 2022
Now We need to bound the term ITA - LAθA∣∣ . Since using the discount parameter η ∈ (0,1)
in Eq.(4) is equivalent to multiplying both the imputed contexts and the imputed rewards by the
parameter √η in each episode, we have, in the n-th episode,
Il 穹T- L a-1θa∣2 = „∆n-j∣2,	(24)
where ∆7}b-ι = {η(n-i-1)/2 IRi,b}i∈[n-i],b∈[B] denotes an exponential-decay vector of the in-
stantaneous regrets, and IRi,b denotes the instantaneous regret at step b in the i-th episode, i.e,
IRi,b = K@A, Si,b) - hθA, Si,bi∣. From Eq.(24), letting
CIRi = X IRi,b	(25)
b∈[B]
be the cumulative instantaneous regret in the i-th episode, we can obtain the upper bound of Eq.(24)
as follows:
∣∣ 宣T-L AT%||2 = ian-iii2
≤ ∣∣∆ηn-1∣∣1
=X	In-"2 IRi,b∣	(26)
i∈[n-1],b∈[B]	(26)
=X η(IT)/2 CIRi
i∈[n-1]
=η-2 fImp (n),
where
fimp(n) := E (√η)n-i CIRi.	(27)
i∈[n-1]
From monotone bounded theorem, we have that the limit of CIRi exists. From Eq.(17) in Lemma 1,
we get that fImp (n) is convergent and then has an upper bound. We denotes the upper bound of
fImp(n) by CImp > 0, and then from Eq.(26) we have
||穹T- LA-1θA∣∣2 ≤ η-1 Cimp.	(28)
Substituting Eq.(28) into Eq.(23) yields the upper bound of XA(3).
Then, we prove that
JsT (ΨA)-1s ≤ JsT (λId + ΦA)-1 S.	(29)
holds, which is equivalent to
sT λId+Φ+γΦb-1s ≤ sT (λId + Φ)-1 s.	(30)
Letting Θ = λId + Φ, by Sherman-Morrison-Woodbury formula, we have
Θ +γΦb-1 = Θ +γSbTSb-1 = Θ-1 -γΘ-1SbT Id +γSbΘ-1SbT-1 SbΘ-1
I	-1	(31)
=θ-1 - θ-1Sbτ (Id + Sθ-1St)	Sbθ-1,
yielding that Eq.(30) is equivalent to
sTΓs ≥ 0,	(32)
where
γ = θ-1Sbτ (id∕γ + SθTSbτ)- Sbθ-1.
__ _1 /O_	^ ʌ ^ 1	.	. ^
Let S = UdΣd VdT, Sb = UbdΣbd VbdT be the Singular Value Decomposition (SVD) of S and Sb,
TT
respectively. Note that Φ = VdΣdVdT , Φ = VdΣdVdT . We can obtain that Γ is a square symmetric
positive semi-definite matrix, since Γ can be decomposed into
Γ=QTQ,
18
Under review as a conference paper at ICLR 2022
一 _ __________ _ , ^ _ -l ^. 一
where PYΛ7P| is the SVD of Id∕γ + Sθ-1Sl and
Q = A-1/2P|SeT.
Thus, Eq.(32) holds, yielding that Eq.(30) also holds.
Finally, we prove that a larger imputation rate γ leads to a smaller variance term
From Eq.(31), the variance term can be represented as follows:
√sl (Ψ)-1 s.
Js∣(Ψ)-1s = [slΘ-1s - slΘ-1SlM-1SθTsi1/2,	(33)
where MY = Id∕γ + SΘ-1 Sl. Lettmg MY = UMYΛMγ UM be the SVD of MY, and Z =
|
UMl SΘ-1s, from Eq.(33) we can written the variance term as follows:
个sl (Ψ)-1 s = hslΘ-1s — zlΛM1 Zi .	(34)
In Eq.(34), we can observed that
zlAMγZ = k(AMY)T/2zk2 ∈ ------Jq1/ ∣∣zk2, --------Jq1/ Ilzk2
σmax(M)+1∕γ
σmin(M)+1∕γ	2
1
where M = SΘ-1Sl, which indicates that a larger imputation rate γ leads to a smaller variance
term.	□
Finally, we provide a deeper understanding of the additional bias in Theorem 2.
Remark 4 (Controllable Bias). Our reward imputation approach incurs a bias term Y 1 η- 1 CImP
in addition to the two bias terms λ∣θA∣∣2 and V that exist in every UCB-based policy. But this
additional bias term is controllable due to the presence of imputation rate γ that can help controlling
the additional bias. Moreover, from the proof of Eq.(26), we can obtain that, the term CImP in the
additional bias can be replaced by a function fImP(n) (defined in Eq.(27)), and the additional bias
term turns out to be Y 1 η- 1 fImp(n). Since fImp(n) has the same functional form as the function
f(n) in Lemma 1, we can find the conditions that fImP (n) is monotonic decreasing following Eq.(18)
in Lemma 1. Specifically, letting CIRi be the cumulative instantaneous regret in the i-th episode
defined in Eq.(25),
1) when √η = 1/2, the condition of a monotonic decreasing function fImp(∙) is equivalent to, for
any positive integer i ≥ 2,
(1 - √η) ∣^√ηiT-(I- √η)iτ^∣
CIRi ≤---------------------------⅛IR1,
≤	2√η - 1	1,
indicating that the regret after N episodes satisfies
L	(I-√η) [√ηi-i - (1 - √η)i-1]
X CIRi ≤ CIRi x----------4√n-1----------i
2≤i≤N	2≤i≤N	η
CIR1
CIR1
1 — √η
2√η - 1
1
X [√ηi-i - (1 - √η)i-i]
2≤i≤N
√η(2√η -I)
[2√η —1 + (1 — √η)N+1 — (√η)N+1
CIRi ɪ
√η
1+
(1 — √η)N+1 — (√η)N+1 ]
2 √η — 1
(35)
2) for the case √η = 1/2, the condition of a monotonic decreasing function fImp(∙) is equivalent
to CIRi ≤ (i — 1)(√η)i-1CIRι for any positive integer i ≥ 2, indicating that the regret after
19
Under review as a conference paper at ICLR 2022
N episodes satisfies
E CIRi ≤ CIRi £ (i - 1)(√η)i-1
2≤i≤N	2≤i≤N
UW CIRI
一 1
一 (i-√n)2 +
(√η)N CIRi
—
"12+1) CIRi
Jn-(1" )√ηN-1
CIR1.
(36)
From Eq.(35) and Eq.(36), we can conclude that a monotonic decreasingfunction fimp(∙) indicates
the upper bound Ofregret after N episodes is oforder O(CIRι∕√n). The conclusion also indicates
that setting the discount parameter as √η = Θ(CIRι∕N) achieves a O(N) regret bound (i.e., a
O(√dT) regret bound following Remark 3). Note that setting the discount parameter as √η =
Θ(CIRi /N) is a mild condition, since the cumulative instantaneous regret CIRi is typically of
order O(B)(B =O(/T/d) in Remark 3) yielding that √η = Θ(d-i). Overall, since a larger
imputation rate γ leads to a smaller variance while increasing the bias (variance analysis can be
found in Remark 2), γ controls a trade-off between the bias term and the variance term. When
fImp is a monotonic decreasing function w.r.t. number of episodes n, the additional bias term
Y 2 η-1 fimp(n) can be easily controlled, e.g., gradually increasing Y with the number of episodes,
avoiding the large bias from fImp (n) at the beginning of reward imputation. We design a rate-
scheduled approach for choosing the imputation rate Y in Section 5.
Remark 5 (Relationship to Exploration and Exploitation Trade-off). Exploration-exploitation
dilemma is the key challenge in online learning under bandit settings. In the full-information set-
ting, agent (e.g., UCB policy) receives the rewards from all the actions, does not need to consider
the choice of exploring the feedback mechanisms, and achieves a lower variance part in the re-
gret upper bound (Theorem 1). Along this line, our reward computation approach is proposed to
approximate the setting of full-information feedback, which somewhat relaxes the explore/exploit
dilemma and also brings a lower variance part and a controllable additional bias part in the regret.
Extra information that pushes the policy towards exploitation and away from exploration comes
from the estimated reward structures of the non-executed actions maintained in each episode, and
the proposed reward imputation can be seen as an effective and efficient tool to capture this extra
information.
B.2 Approximation Properties of Sketching
Although some error bounds of approximation using SJLT have been proposed (Nelson & Nguyen,
2013; Kane & Nelson, 2014; Bourgain et al., 2015), it is still unknown what is the lower bound
of the sketch size while applying SJLT to the sketched ridge regression problem in our SPUIR.
To address this issue, we first prove two approximation properties of SJLT which are necessary to
achieve approximation error bound of the sketched ridge regression using SJLT. For convenience,
we drop all the superscripts and subscripts in these theoretical results.
Lemma 2 ((Nelson & Nguyen, 2013)). Let U ∈ RL ×d be a matrix with orthonormal columns,
Π ∈ Rc×L the SJLT. Assuming that D = Θ(εσ-i log3(dδ0-i)) for Π, εσ ∈ (0, 1) and d ≤ c, with
probability at least 1 - δ0 all singular values of ΠU
σi(ΠU) = 1±εσ,	i ∈ [d],
as long as
Further, this holds ifthe hash function h and σ defining the Π is Ω (log(dδ-1))-wise independent.
Theorem 6 (Approximation Properties of SJLT). Let U ∈ RL×d be a matrix with orthonormal
columns, and A be a matrix of any proper size. If Π ∈ Rc×L is the SJLT satisfying the assumptions
in Lemma 2, and d ≤ c ≤ L, then Π has the following two properties:
20
Under review as a conference paper at ICLR 2022
1)	Subspace embedding property: set C = Ω (d polylog (dδ-1) /ε2) ,for εs ∈ (0,1), with Proba-
bility at least 1 - δs ,
IlUTnTnU - Idk2 ≤ εs;
2)	Matrix multiplication property: set C = Ω(d∕(εmδm)) ,for εm ∈ (0,1), with probability at least
1 - δm ,
IUTnTnA - UTAI2F ≤εmIAI2F.
Proof of Theorem 6. 1) From Lemma 2, by setting C = Ω (d polylog (dδ-1) /ε0), We can obtain
the upper bounds of eigenvalues: with probability at least 1 - δs ,
λi (UTnTnU) =σi2(nU) ∈ [(1 - εσ)2, (1 + εσ)2] ⊆ [1 -2εσ,1+3εσ],	(37)
Which yields that
∣λi (UTnTnU - Id)∣≤ 3εσ.	(38)
Eq.(39) is equivalent to
IUTnTnU - Id I2 ≤ 3εσ .
Letting εs = 3εσ and εσ ∈ (0, 1/3) yields the subspace embedding property.
2)	From Lemma 1 in (Zhang & Liao, 2019), We have
2	2d
E [kUTnTnA - U|AkF] ≤ CkUkFkAkF = -CIAkF.	(39)
Combining Eq.(39) With the Markov’s inequality, We obtain that, With probability at least 1 - δm,
kUTnTnA - u TAkF ≤ 卷 kAkF.
Letting εm = yields the matrix multiplication property.
δmC
□
B.3 Proof of Theorem 3
Next, using the approximation properties of SJLT in Theorem 6, We prove that the objective function
value of the imputation regularized ridge regression problem for reWard imputation can be approx-
imated Well With a relative-error bound. Moreover, We prove that the solution solving the sketched
ridge regression problem for reWard imputation is also a good approximation of the solution solving
the imputation regularized ridge regression. The folloWing theorem is a detailed version of Theo-
rem 3.
Theorem 7 (Approximation Error Bound of Imputation using Sketching, Detailed Version of The-
orem 3). Let γ ∈ [0, 1] be the imputation rate, λ > 0 the regularization parameter, n ∈ Rc×L
and nb ∈ Rc×L be the SJLT, and L ∈ RL×d, Lb ∈ RL×d, T ∈ RL , Tb ∈ RL , θ ∈ Rd. Denote the
imputation regularized ridge regression function F and sketched ridge regression function F S for
reward imputation by
F(θ)= kLθ - T k22 + γ Lbθ - Tb2 + λkθk22,
FS(θ) = kn(Lθ-T)k22+γnb Lbθ - Tb2 + λkθk22,
and the solutions of these regression problems by
8 = argmin F(θ) and θ = argmin FS(θ).
θ∈Rd	θ∈Rd
Let δ ∈ (0, 0.1], ε ∈ (0, 1), ρλ = kLallk22/(kLallk22 + λ). For n and nb, assuming that D =
Θ(ε-1 log3(dδ-1)) and
c = Ω (d Polylog (dδ-1) /ε2),
21
Under review as a conference paper at ICLR 2022
with probability at least 1 - δ,
~
F (θ)
..~ —..
kθ - θk2
(1 + ρλε)F (θ),
WF⑻
σmin (Lλll) ,
(40)
(41)
≤
≤
where Lλιι = [l； √γL； √λId]
O
b
∈ R(L+L+d)×d . Furthermore, if there is a constant fraction of
the norm of Ta0ll lies in the column space of Laλll, then Eq.(41) can be strengthened. Formally,
assuming that a mild structural assumption on the context matrix and the reward vector is satisfied,
i.e., kUallUa|llTa0llk2 ≥ ξkTa0llk2 with a constant ξ ∈ (0, 1], then with probability at least 1 - δ,
kθ — θk2 ≤ κ(Lλll)√ξ-2-Γ	√Pλlθk2,
(42)
b
where κ(A) denotes the condition number of A, Ta0ll = [T ; Tb; 0d] ∈ R(L+L+d), and Laλll
Uall∑all VJl is the SVD of l东.
Proof of Theorem 7. We first introduce some more notation of block matrices that will simplify
the proof of the theorem:
ΠO
Πall = O	Πb
Lall = ( √LL
Tall =	TTb
(43)
Then the regression functions can be rewritten as follows:
F(θ) = kLallθ - Tallk22 + λkθk22,	FS(θ) = kΠall (Lallθ - Tall)k22 + λkθk22.
Obviously, Πall is still an SJLT. Combining Theorem 6 with theorem 19 in (Wang et al., 2017), we
can obtain, setting
C = Ω (max{d polylog (dδ-1) /ε2, d∕(εmδm)}),
with probability at least 1 - (δs + δm),
,~.	,—.	,—.
F(θ) - F(θ) ≤ PλτF(θ),
(44)
where ρλ = £『2+入 and T = 2maX-}m}. Letting εs = εm := ε0, Eq.(44) can be rewritten as
F(θ) - F(θ) ≤ 2ρλε0-F(θ),
1 - ε0
(45)
Assuming that δs = δm := δ∕2 ∈ (0,0.1] and ε° ∈ (0,1/3), setting E = -∣2ε^ ∈ (0,1), from
1-ε0
Eq.(45) we obtain the upper bound Eq.(40).
Next, we bound the difference between the solutions solving the sketched ridge regression problem
and the original regression problem. Since σm2 in(A)kxk22 ≤ kAxk22 for any A and x with proper
sizes, we have
σmm(Lall)kθ - θk2 ≤ U
Lall(θ — θ)∣∣2 .
(46)
ThekeyingredientofboUnding ∣∣θ-θ∣∣2 is to bound ∣∣Lai1(θ-θ)k2. Let Lλn
R(L+L+d)×d, Tall = [T; T; 0d] ∈ R(L+L+d), Lλll = uall∑all%ll be the SVD of Lλll, and denote
bb
a matrix with orthonormal columns by Ua⊥ll ∈ R(L+L+d)×(L+L) which satisfies
UallUJll + U⊥l(U⊥l)1 = IL+Lb+d	and UaJll Ua⊥ll = O.
Then, We can rewrite the solution θ as follows:
θ = arg min F(θ) = arg min 厄东。-T0llU2
θ∈Rd	θ∈Rd
=(Lλll)tTall = Vall∑-iu∣liτoll,


〜
〜
ʌ
d
2
22
Under review as a conference paper at ICLR 2022
which yields that
Tall- Lλnθ=Tall- LλιιVaa∑-iuaaτaιι
=Tall- Uαll∑αllvιlvαll∑-lι u∣llτ0ll
= Ta0ll - Uall Ua|llTa0ll
=Ua⊥l (Ua⊥I)TTal l.
Thus, TaIl - Lλllθ is orthogonal to Uall, and consequently to Lλll(θ - θ), and We can obtain the
following equality by Pythagoras’s theorem:
∣∣Lλll(θ - θ)∣∣2 = ∣∣Lλll θ - TalllI2 - ι∣Lλllθ - TallIl2.	邰)
Combining Eq.(48) With Eq.(40) yields that
∣∣Lλll(θ - θ)∣∣2 = F(θ) - F(θ) ≤ PλεF(θ).	(49)
Substituting Eq.(49) into Eq.(46) concludes the proof of Eq.(41).
If We make a mild structural assumption on the context matrix and the reWard vector, We can provide
a stronger bound of ∣∣θ - θ∣∣2. Specifically, assuming that kUallUanTajlk2 ≥ ξkT0]lk2 with a
constant ξ ∈ (0, 1], from Eq.(47) and Pythagoras’s theorem We have
F(θ) = kLλllθ - Tallk2
= kTa0llk22 - kUallUa|llTa0llk22
≤ (ξ-2- 1)kUallU∣llTallk2
= (ξ-2- 1)kLλllθk2	( )
≤ (ξ-2- 1)kLλllk2 kθk2
≤ (ξ-2- 1)σmaχ(Lλll)∣Bk2.
Combining Eq.(50) with Eq.(41) yields Eq.(42).	□
B.4 Proof of Theorem 4
Proofof Theorem4. In our sketched policy, letting C券ax = maxA∈A ∣∣θA∣∣2, CImP > 0, V =
P2CR log(2MB∕δ), and
ω = λCθmax + V + Y 2η-2 CImp,
from Eq.(9) in Theorem 2 we obtain that
|〈@A, Sn,b〉-hθA, Sn,bi∣ ≤ 3^Jψ^^b.	(51)
Before proving the upper bound of ∣(0裳,SnA -("A，Sn,bi∣, We need to provide a technical tool
as follows. For convenience, we also drop all the superscripts and subscripts. The goal is to find a
constant Ca SUCh that
√STΨ-1S ≤ Cɑ√sτwTs,	(52)
which is equivalent to the condition that the matrix Cα2 W-1 - Ψ-1 is positive semidefinite.
|
Let Lall and Πall be the matrices defined in Eq.(43), Lall = Uall Σall Va|ll be the SVD of Lall,
and σ1 ≥ σ2 •… ≥ σd be the singular values of Lall. Then the i-th eigenvalue of Ψ-1 =
(λId + LIllLall)-1 can be represented as λi(Ψ-1) = 1∕(σ2 + λ), and the i-th eigenvalue of
W-1 = (λId + L∣llΠ∣ll∏allLall)-1 is λi(W-1) = 1∕(λi + λ), where 3 is the i-th eigenvalue of
Σe all UeallΠallΠallUeall Σe all .
From the Lidskii’s theorem and Eq.(37), we have
λi ∈ [σ2(1 - 2εσ),σ2(1 + 3εσ)].	(53)
23
Under review as a conference paper at ICLR 2022
Assuming that the positive semi-definiteness of Cα2 W-1 - Ψ-1 is satisfied, we obtain that
Cα2λi(W-1) - λi(Ψ-1) ≥ 0 for i ∈ [d], and combining this inequality with Eq.(53) yields that
Ca = J[σ2(1 + 3εσ ) + λ]∕(σ2 + λ) .
From the proof of Theorem 6 and Theorem 3, We can obtain that εσ = ε∕(6 + 3ε), yielding that
S σ2[1 + ε∕(2 + ε)] + λ
σ	σj+λ
which decreases with increase of 1∕ε. Similarly to the proof of Ca satisfying Eq.(52), we can obtain
that
√s∣WTs ≤ Creg√s∣Ψ-1s,
(54)
provided that
C - I	σ2+λ
Creg = V σ2[1- 2ε∕(6 + 3ε)] + λ.
Obviously, Creg also decreases with increase of 1∕ε.
Then, letting α = ωCa, from Eq.(51) and Eq.(52) we have
lDθA, sn,bE - hθA, Sn,bi I ≤ IDθA, SQ-EA, sn,b〉| + K8A, sn,b) — h。A, sn,bi ∖
≤ IDθA - θA, sn,bEI+ ω∕sn,b (ΨA)-1sn,b	(55)
≤ Yn,b + α,sn,b (WZ)T sn,b,
where Yn,b denotes the upper bound of ∣( θA - θA, sn,b ∖ ∖ for any A ∈ A.
Next, using the compatibility of norm, we give a specific representation of the sum of Yn,b as follows:
E γn,b = AaA kSA (Θa- θa)ki
b∈[B]
_ ʃ" _ — _
≤ AaA kSA kιkθA- θA kι
≤ AaA kSA kι√dkθA - θA k2.
(56)
Further, we give a more specific upper bound in Eq.(56) under mild structural assumption in Theo-
rem 3. Let κamllax denote the maximum of the condition numbers of Laλll(A, n) for A ∈ A, n ∈ [N],
and Lλu(A, n) = [l%; √γLA； √λl∕∣, and Uall(A, n) be the left singular matrix of Lλ11(A, n).
Letting Ta0ll (A, n) = [TAn;TbAn;0d], assuming that kUall(A,n)Uall(A,n)|Ta0ll(A,n)k2 ≥
ξkTa0ll(A, n)k2 with a constant ξ ∈ (0, 1], substituting the upper bound Eq.(42) in Theorem 3 into
Eq.(56) yields that
X γn,b ≤ (κamαx√ξ-y-ι) CSCIrp^	⑶7
b∈[B]
where CS = maXn∈[N],a∈a ∣∣SA∣∣i, CmaX = maxA∈A,n∈[N] ||8A∣∣2.
24
Under review as a conference paper at ICLR 2022
From Eq.(54), Eq.(55), Eq.(57) and the definition of our sketched policy, letting CY
(KmaxPξ-2 - 1) CSCmax, We obtain that
Reg({AIn,b}n∈[N],b∈[B])
Σ I
n∈[N],b∈[B]
A∈ax hθA, SnG- DθAIn,b , sn,b)]
A∈aχ ((θA, sn,b) + αqsn,b (Wn)	sn,b) + Yn,b - (θAIn b , sn,b
≤
n∈[N],b∈[B]
≤ 2αCreg
Sn,b + 2NCγ y/ρ∖ed
√B X∖Xsn,b (ΨAln,b)-1
n∈[N]
b∈[B]
2αCreg
√B XtX(
n∈[N]	b∈[B]
2αCreg √B E
n∈[N]
jχ DSAISn, (ΨA厂 1E + 2NCypPλZd
2αCreg√B X IX tr (SATSA (ΨA)-1) + O(NPP^d),
n∈[N]	A∈A
≤ 2αCreg√BM X jmax {tr (SnISA (ΨA)-1)0 + O(NPρλɪd).
n∈[N]	∈
(58)
When the structural assumption in Theorem 3 is not satisfied, from Eq.(41), We can obtain that the
second term in Eq.(58) is also of order Ο(√ρλed), which does not influence the order of the final
regret bound. Finally, combining Eq.(58) With lemma 3 in (Han et al., 2020) gives the final regret
bound.	口
C Detailed Experimental Settings and More Experimental
Results
In this section, we provide more details and results in the experiments.
C.1 Description of Datasets
Table 3 summarizes the description of datasets used in the experiments.
Next, we provide more details about the three datasets.
Synthetic Data. Inspired by the experiments in (Saito et al., 2020), the synthetic data generation
procedure was formulated as follows, which simulates the streaming recommendation environment.
•	Context Si ∈ Rd : we drew elements of Si independently from a Gaussian distribution
N(0.1, 0.22), where d = 40;
•	Click-Through-Rate (CTR): the CTRs for the 5 actions were respectively set as
{10%, 15%, 25%, 20%, 30%};
25
Under review as a conference paper at ICLR 2022
Table 3: Description of datasets in the experiments (T : number of instances; B: batch size; N :
number of episodes; d: dimensionality of context; M : number of actions; CB satisfying B =
CB2 N/d)
Dataset	T	B	N	d	M	CB
synthetic data	126,000	1,400	90	40	5	25.00
Criteo-recent	75,000	1,000	75	50	5	25.82
Criteo-all	1,276,000	4,000	319	50	15	25.04
commercial product	216,568	1,700	128	50	5	25.83
•	The indicator variables of click events:
C = 1 a click occurs in context si ,
i 0 otherwise.
We sampled the click index set according to the uniform distribution.
•	Conversion rate (CVR) in context si: when Ci = 1,
CVR(Si) =SigmOid((Wc, Sii),= ι + eχp(-hwc, Sii)，
where the coefficient vector wc ∈ Rd is sampled according to a Gaussian distribution as
Wc 〜N(κc1d,σ2Id), and We set different means and standard deviations for different
action with κc ∈ [0 : -0.2 : -0.8] and σc ∈ [0.01 : +0.01 : 0.05];
Criteo Data. We used the publicly available Criteo dataset5, consisting of Criteo’s traffic on display
ads over a period of tWo months (Chapelle, 2014), Where each context consists of 8 integer features
and 9 categorical features. FolloWing the experiments in (YoshikaWa & Imai, 2018), the categorical
features Were represented as one-hot vectors and then concatenated to the integer features. We
reduced the dimensionality of the feature vectors to 50 using principal component analysis (PCA).
All of the algorithms Were tested in a simulated online environment that Was trained on users’ logs
in the Criteo dataset. Specifically, We chose several campaigns from the Criteo dataset, Where each
campaign represents a category of items and corresponds to an action. This online environment
contains a prediction model for the CVR, Which Was Well trained by applying DFM (Chapelle,
2014) using the true user feedbacks. This environment model Was trained for each chosen campaign,
Whose AUCs are ranging from 70% to 90%, assuring that the online environment can provide nearly
realistic feedbacks. To simulate the uncertainty of user behaviors, Gaussian noises With zero-mean
Were added to the model parameters. At each step, the online environment randomly selected a
campaign and samples one context from this campaign, and revealed the context to the agent With
a preset CTR. To generate a reasonable sequence of instances, the environment kept the order of
timestamps of the contexts in each campaign. We tested our algorithms and the baselines With the
folloWing tWo online environments on the Criteo dataset: Criteo-recent contains 5 campaigns
(75, 000 instances) chosen from the recent campaigns, corresponding to 5 actions; Criteo-all
contains 15 campaigns (1, 276, 000 instances) chosen from all the campaigns, corresponding to 15
actions.
Data Collected from a Real Commercial App for Coupon Recommendation. To verify the ef-
fectiveness and efficiency of our algorithms on real products, We conducted experiments on a real
dataset collected from a commercial social app. We call this dataset commercial product,
Where the data Were collected after the users gave consent, and did not contain any personally iden-
tifiable information or offensive content. Since this dataset from a commercial app is proprietary,
We did not provide a URL. We Will release this dataset after the publication of this paper. In this
commercial app, after clicking a recommended coupon, a user may convert the coupon after some
time, or just leave it there. The dataset Was collected during a 1-month period With a subsampling,
and consists of 216, 568 instances from 5 categories of coupons, Where each context is described by
86 numerical features and 16 categorical features. The timestamps of clicks and conversions Were
5https://labs.criteo.com/2013/12/conversion-logs-dataset/
26
Under review as a conference paper at ICLR 2022
also recorded. Following the settings on the Criteo data, we also represented the categorical features
as a one-hot vector, reduced the dimensionality of the feature vectors to 50 by PCA. The action
space contains 5 actions, where each corresponding to one coupon category. Due to the limitation
of real online experiments, in this experiment, we still trained DFM using the true user feedbacks as
the online environment, where AUCs range from 75% to 90%.
To simulate the real environment under partial-information feedback, The experiments were con-
ducted in environments where the distribution of the initialization data is atypical. Specifically,
in the experiments, we set different numbers of the initialization instances for each action. In
the synthetic environment, we set the number of the initial instances as 140, 210, 350, 280, 420
for the 5 actions, respectively. In Criteo-recent, we set the proportion of the initial in-
stances as 0.1, 0.15, 0.25, 0.2, 0.3 for the 5 actions, and set the number of the initial instances as
[100 : 23 : 423] for the 15 actions in Criteo-all.
C.2 Detailed Specification of Hyperparameters
In these experiments, the true reward is defined by Rit,rAue = λcCi,A + (1 - λc)Vi,A (Ci,A and Vi,A
denote true binary variables of user click and conversion when executing action A given context si),
where λc = 0.01 on the synthetic data, Criteo Data, and commercial product data, respectively. As in
most contextual bandit literature (Li et al., 2010; Chu et al., 2011), we set the regularization parame-
ter λ = 1 in the Euclidean regularization. According to theoretical analysis in Remark 3, we set the
batch size as B = CB2 N/d, set the constant CB ≈ 25 and the sketch size c = 150 on all the dataset-
s (B = 1400, 1000, 4000, 1700 for synthetic data, Criteo-recent, Criteo-all, and
commercial product). The regularization parameters ω, α in our policy and that in the batch
UCB policy were tuned in [0.2 : +0.2 : 1.2]. For the SJLT in SPUIR and its variants, sketch
size was set as c = 150 and the number of block D was selected in {1, 2, 4, 6}. Except for the
rate-scheduled variants of our approaches, the imputation rate γ was selected in [0.1 : +0.2 : 0.9].
Besides, the discount parameter η was tuned in [0.1 : +0.2 : 0.9]. In the nonlinear variant of our
approach SPUIR-Kernel, we selected the dimension of the random features dr in {50, 100, 200} and
the kernel width of Gaussian kernel in {2-(i+1)/2 , i = [-12 : 2 : 12]}.
Rate-Scheduled Approach. We equip PUIR and SPUIR with a rate-scheduled approach, called
PUIR-RS and SPUIR-RS, respectively. We design a rate-scheduled approach following the theoret-
ical results about the imputation rate γ . From Remark 1&2, we can obtain that a larger imputation
rate γ leads to a smaller variance while increasing the bias. From Remark 4, we conclude that the
additional bias term includes a monotonic decreasing function w.r.t. number of episodes under mild
conditions. Therefore, instead of using a fixed imputation rate, we can gradually increase γ with the
number of episodes, avoiding the large bias at the beginning of the reward imputation while achiev-
ing a small variance. Specifically, we set γ = X% for episodes from (X - 10)% × N to X% × N,
where X ∈ [10, 100].
C.3 More Experimental Results
For better illustration, in Figure 3 of the manuscript, we omitted the curves of algorithms whose av-
erage rewards are 5% lower than the highest reward. Now we provide the curves of all the algorithms
in Figure 5.
27
Under review as a conference paper at ICLR 2022
PJeMtuα υσsυ><
0	10	20	30	40	50	60	70
Number of Episodes N
(b) Criteo-recent
(c) Criteo-all
0.450
0.425
0.400
p
ra 0.375
Φ
C≤ 0.350
Φ
2 0.325
ω
2 0.300
0.275
0.250
20	40	60	80	100	120
Number of Episodes N
(d) commercial product

Figure 5: Average rewards of the compared algorithms, the proposed SPUIR and its variants on
synthetic dataset, Criteo dataset, and the real commercial product data
28