Under review as a conference paper at ICLR 2022
A Distributional Robustness Perspective on
ADVERSARIAL TRAINING WITH THE ∞-WASSERSTEIN
Distance
Anonymous authors
Paper under double-blind review
Ab stract
While ML tools are becoming increasingly used in industrial applications, adver-
sarial examples remain a critical flaw of neural networks. These imperceptible
perturbations of natural inputs are, on average, misclassified by most of the state-
of-the-art classifiers. By slightly modifying each data point, the attacker is creating
a new distribution of inputs for the classifier. In this work, we consider the ad-
versarial examples distribution as a tiny shift of the original distribution. We thus
propose to address the problem of adversarial training (AT) within the framework
of distributional robustness optimization (DRO). We show a formal connection
between our formulation and optimal transport by relaxing AT into DRO problem
with an ∞-Wasserstein constraint. This connection motivates using an entropic
regularize]- a standard tool in optimal transport— for our problem. We then prove
the existence and uniqueness of an optimal regularized distribution of adversarial
examples against a class of classifier (e.g., a given architecture) that we eventually
use to robustly train a classifier. Using these theoretical insights, we propose to
use Langevin Monte Carlo to sample from this optimal distribution of adversar-
ial examples and train robust classifiers outperforming the standard baseline and
providing a speed-up of respectively ×200 for MNIST and ×8 for CIFAR-10.
1	Introduction
We call adversarial example an input which is a human-imperceptible -perturbation1 compared to
a real example that results in an incorrect classification from a classifier [Goodfellow et al., 2014;
Sun et al., 2018; Athalye et al., 2018; Santurkar et al., 2019; Nguyen et al., 2015a; Kurakin et al.,
2016; Moosavi-Dezfooli et al., 2016]. The particularity of these examples, which justifies their study,
is the fact that most of the so-called adversarial examples are misclassified by a large majority of
state-of-the-art neural networks. Long seen as bugs, Santurkar et al. [2019] asserts that the existence
of adversarial examples is explained by the presence of easy-to-perturb patterns within the data
distribution that are not perceptible by humans but useful for the classification task.
In response to this discovery, different methods have been developed to train robust classifiers and
to craft adversarial attacks by determining optimal perturbations [Nguyen et al., 2015b; Papernot
et al., 2016a; Goodfellow et al., 2014; Papernot et al., 2016b; Tramer et al., 2017; Madry et al., 2017].
Among these training methods, adversarial training [Goodfellow et al., 2014; Tramer et al., 2017;
Madry et al., 2018] has settled as one of the strongest baselines to train robust classifiers. This method
is relatively simple: it consists of training the classifier directly on batches of adversarial examples,
leading to the following optimization problem [Madry et al., 2018]:
mRd E(x,y)~Pdata [陵 maX 4'fθ(X，y))]
θ∈R	kx-xk∞ ≤
(1)
where ` is usually the cross-entropy loss and pdata is the dataset distribution. Our goal is to
rigorously connect adversarial training and robustness by providing a distributional robustness
1Adversarial example can be defined with respect to any notion of distance that captures the fact that an
Gperturbation is imperceptible by a human. In this work, for the sake of simplicity, we focus on the '∞ norm,
which is the most common one.
1
Under review as a conference paper at ICLR 2022
perspective [Delage and Ye, 2010; Ben-Tal and Nemirovski, 1998; Sinha et al., 2018] for adversarial
training. Distributional robustness is a framework to study predictive models that aim at being robust
against distribution shift. Given a set of possible distribution P that a given predictive model fθ could
encounter the distributionally robust classification task is:
min max,E"padv['fθ(χ,y))]	⑵
θ∈Rd padv ∈P
Note that while standard adversarial training aims to use the point-wise optimal adversarial Xj
examples (i.e. for each natural input xi), our perspective considers the optimal adversarial distribution.
Working at the scale of distributions leads us to draw inspiration from the Kantorovich relaxation
in Optimal Transport theory. To do so, we will define the set P using coupling measures between
natural and adversarial distributions with the help of ∞-∞-Wasserstein distance.
Contributions. Our contributions are three-fold: First, we provide a strong connection between
adversarial training and adversarial robustness and distributional robustness by using some tools
from optimal transport, we call this formulation adversarial transport. Our second contribution
is to show that, in this specific setting, there exists, against any given classifier fθ, an optimal
distribution of adversarial examples and to provide a closed-form solution for this distribution. Our
third contribution is to use these theoretical insights to come up with a practical training method using
Langevin Monte-Carlo sampling to jointly find the optimal classifier and the optimal distribution of
adversarial examples. By using this new technique for adversarial training we obtain robust classifiers
outperforming the standard baseline [Madry et al., 2018] in terms of robustness and clean accuracy.
Moreover, this training technique provides a speed-up of respectively ×200 for MNIST and ×8 for
CIFAR in terms of training time.
Related Work. Among the various works about defenses and attacks in the context of adversarial
examples, Adversarial Training [Madry et al., 2018] is the most common baseline. However, in this
procedure, adversarial examples are generated independently of the others by searching for the optimal
perturbation for each one. Thus, AT seeks point-wise optimality and not global optimality. Here, the
goal is to make our classifier robust to any unknown adversarial example distribution. It justifies the
consideration of the DRO framework. DRO enables a mathematical formulation for dealing with
uncertainty in complex systems [Delage and Ye, 2010; Lam, 2018; Rahimian and Mehrotra, 2019].
In ML, DRO tries to minimize the loss over the worst-case distribution in a neighborhood of the
observed training data distribution [Duchi and Namkoong, 2016; 2018; Chouzenoux et al., 2019;
Duchi et al., 2016]. Thus, DRO is pertinent in the case of adversarial examples and allows to bring a
principled distributional perspective on AT.
Within the DRO framework, one crucial point is to properly define the uncertainty set which contains
the underlying data distribution. The importance of this set lies in the fact that it allows to reduce the
computation time and the search area of the underlying distribution [Delage and Ye, 2010; Ben-Tal
and Nemirovski, 1998; Lam, 2018]. Different methods have been proposed to construct uncertainty
sets. Among them, one considers the help of OT operators and defines the set with all the probability
measures such as the p-Wasserstein distance from the observed distribution is less than a given
constant (p-Wasserstein ball)[Blanchet and Murthy, 2019].
Some works have started to study p-Wasserstein DRO in the case of adversarial examples. Zheng et al.
[2019] defines Distributionally Adversarial Attacks (DAA) which is a generalization of PGD attacks
on the space of probability measures with W2-DRO and [Staib and Jegelka, 2017] did the Wp-DRO
generalization of AT. E.Wong et al. [2019] also considers the Wp ball using an explicit formulation
of the uncertainty set with coupling matrices. However, while the previous works modified the AT
problem—the Wp constraint on the adversarial distribution does not correspond to the distribution of
adversarial examples computed in AT—our work aims at overcoming that. The original AT problem
stays a the scale of the example and considers OT to deal with the displacement of pixels. This
perspective leads to the discovery that the standard AT problem corresponds to an ∞-Wasserstein
DRO problem with the '∞ underlying geometry.
In the meantime, as in our paper, entropic regularization is used to find closed forms of the coupling
matrix. To extend this understanding of adversarial examples in the case of distributions we generalize
this uncertainty set with the ∞-∞-Wasserstein distance and add entropic regularization. 2-∞-
Wasserstein DRO has already started to receive consideration in the case of AT [Gao et al., 2017].
Nevertheless, the use of the '∞ within the ∞-Wasserstein distance allows both to consider the DRO
problem at the dataset scale but also to ensure that the adversarial condition is respected for each
2
Under review as a conference paper at ICLR 2022
element of the adversarial distribution. Together with the use of coupling matrices in our formulation,
we found a concave and tractable formulation of our problem which results in a different technique to
sample from this distribution (namely Langevin Monte Carlo). Finally, we propose to keep them in
memory to reuse the past samples for the update of the robust classifier.
Notations. In this work, we will consider a measurable finite dimensional input space X ⊂ Rd. We
will note B(X) the Borel σ-algebra of X and M(X)1+ the set of all probability measures on X.
If not precised, a map T : X → X is considered measurable. We, recall that given a separable metric
space X s.t. any probability measure on X is a Radon measure, for T : X → X, the push-forward
measure of β = T]α ∈ M(X)+ of some α ∈ M(X)+ satisfies: ∀h ∈ C(X), Rχ h(x)dβ(x)=
X h(T (x))dα(x).
2	Adversarial Training, Transport, and Distributional
Robustness
In this work, We say that X ∈ X is an adversarial example of f atthe data point (x, y) if ∣∣X - χ∣∣∞ ≤ e
[Goodfellow et al., 2014]. Given a loss function ', an optimal adversarial example X aims at
maximizing the following optimization problem:
X ∈ arg max	'(fθ (X), y).	(3)
∣∣≈-xk∞≤e
In practice, a large value for '(fθ (X),y) will yield a misclassification of the example y. By considering
the adversarial training problem (1), one can notice that the distribution of input-output pairs seen by
the classifier fθ depends on the value of its own parameters. We will call this distribution padv . Such
a distribution can be described as the pushforward of a transport map T that respect the adversarial
example constraint kT (x) - xk∞ ≤ e , ∀x ∈ X. In other words we have,
Padv = T]Pdata : (X,v)〜Padv ⇔ ∃(x, y)〜Pdata s.t. X = T (x) and ∣∣X - x∣∣∞ ≤ e. (4)
This remark leads us to our first proposition that explicitly proposes a distributional robust optimiza-
tion (DRO) formulation of the adversarial training problem.
Proposition 2.1 (DRO formulation of adversarial training). The adversarial training problem (1)
can be formulated as a DRO problem (2) where:
P = {Padv : ∃T : X → X , Padv = T]Pdata and ∣T (X) - X∣∞ ≤ e , ∀X ∈ X} (5)
Interestingly, one can draw parallels with Monge’s formulation of optimal transport (OT), were given
an initial measure μ and a target measure V, the task is to solve the following problem
inf / c(x, T(X))dμ(X) subject to V = T]μ .	(6)
On the one hand, (6) and the DRO problem defined in Proposition 2.1 are drastically different problems
in essence. It can be explained by the difference between the constraints on the transportation map and
the fact that while Equation 6 aims at minimizing a function of the transportation map T, Equation 2
aims at maximizing a function of the transported distribution Padv .
On the other hand, it is insightful to notice how both problems depend on a set of transportation maps
to connect the initial and the target distributions.
The limitations of Monge’s problem are well known in the OT community: due to its non-linear
constraint, the problem is often intractable and may not have a solution (i.e. the infimum may not
be achieved). That is why Kantorovich [1940], proposed to relax (6) into a problem with linear
constraints by considering coupling measures (an approach that allows a mass splitting from a
natural example Xi to different locations Xj). Such a relaxation had the advantage to provide a better
understanding of the original problem leading to novel theoretical guarantees (e.g., the existence
of solutions for Kantorovich’s problem, duality, and in some situations equality between Monge’s
problem and Kantorovich’s problem) as well as tractable algorithms [Sinkhorn, 1964; Cuturi, 2013b].
In a similar vein, we believe our perspective on adversarial examples that combine ideas from
distributional robustness and adversarial transport leads to a better understanding of the problem
of adversarial training. Thus, we will use that insight to consider coupling measures to relax the
adversarial training problem (1). It will lead to a new DRO formulation for adversarial training that
we call adversarial transport.
3
Under review as a conference paper at ICLR 2022
3 Adversarial Transport Formulation
We first define the notion of coupling measure, that we will use to define a relaxed version of the
DRO formulation described in Proposition 2.1.
Definition 3.1 (Transport plan). LetPadv andPdata be two distributions on X. π ∈ B(X)③B(X) →
[0, 1] is a transport plan between padv and pdata if:
∀A∈B(X), π(A× X) = Pdata(A) and ∀B ∈B(X), π(X × B) =Padv(B)	(7)
The space of transport plan between Pdata and Padv is denoted Π(Pdata, Padv ).
Informally, the coupling measure ∏ : B(X)③ B(X) describes the spatial modification of the initial
distribution Pdata to the adversarial distribution Padv . For instance, in the discrete case where we
have the datapoints (Xi) and the potential adversarial examples (Xj), a coupling is a matrix P such
that the value Pi,j represents the amount of mass flowing from the natural example i to the adversarial
example j .
3.1	Incorporating the proximity constraint to the coupling.
Adversarial examples are tied to the original examples by a proximity constraint. Thus, a DRO
formulation of adversarial training must encompass such a constraint. For a coupling measure it
corresponds to the fact that it can transport (x, y)〜Pdata to (X, y)〜Padv only if ∣∣x - X∣∣∞ ≤ e.
Proposition 3.1 (DRO relaxation of adversarial training). The DRO problem (2) can be relaxed by
considering coupling measure. In that case the constraint set P in Equation 2 take the form:
Pconv = {Padv : ∃∏ ∈ ∏(Pdata∣Padv) with supp(π) ⊂ {(x,y,X,y) |||X - x∣∣∞ ≤ e, y ∈Y }}.
This proposition proposes a convex relaxation of the initial constrain set (5). Intuitively, the set P
constraints the distribution Padv to be e-close to Pdata . Because this set is convex we can hope it
corresponds to a notion e-ball. We show in the next subsection that this intuition can be formalized
by using the ∞-Wasserstein distance.
3.2	CONNECTION WITH ∞-WASSERSTEIN
Given two distributions μ and V and a distance function d, the p-p0-Wasserstein distance corresponds
to the minimal average 'p norm to the power of P over the transport plans between μ and ν. Formally,
by calling z = (X, y) and Z := X × Y, it can be defined as
Wp,p0(μ,ν) :=	inf J, [ Ilz - zkP0dπ(z,z))/° .	⑻
π∈π(μ,ν) ∖ JZ×Z
The P-∞-Wasserstein distance can be defined as the limiting case P0 → ∞. Thus, it can be seen as
the minimal amount of point-wise transport (w.r.t. the '° norm) necessary to transport μ to V,
Wp,∞(μ, v) =	inf sup	Ilz - zkp.	(9)
π∈π(μ,ν) (ζ,2)∈supp(∏)
Thus by considering P = ∞, we can establish that the set introduced in Proposition 3.1 can be
described using the ∞-∞-Wasserstein distance.
Proposition 3.2. [convex relaxation of adversarial training]For any e < 1,2 the convex DRO
relaxation of adversarial training can be reformulated as
mRd Padvmaxdata) E(X 〃卜Pa/" 3,加
(10)
where B(Pdata) = {P | W∞,∞ (P, Pdata) ≤ e} and WP,∞ is defined in (9).
In other words, adversarial training can be seen as a DRO problem restricted to the distributions that
are e-close to the data distribution with respect to the ∞-∞-Wasserstein (∞-Wasserstein distance
within a space with the '∞ metric).
The P-P0-Wasserstein distance has been widely studied in the context of optimal transport [Champion
et al., 2008; Villani, 2003]. The closeness of our formulation with OT encourages us to add an
entropic regularization [Peyr6 and Cuturi, 2019; Cuturi, 2013a].
2This assumption is only for simplicity of exposition, what we need in general is that < miny6=y0 |y - y0 |
which can always be achieved by reparametrizing the discrete labels.
4
Under review as a conference paper at ICLR 2022
3.3	Entropy-regularized Formulation
Entropic regularization has been a very successful approach for finding an approximate solution to
OT problems [Cuturi, 2013b; Genevay et al., 2016]. From a theoretical point of view, the entropic
regularization prevents the sparsity of the coupling matrix, which is justified from a practical aspect
in the context of transport by the fact that in practice the transport network is diffuse [Wilson, 1969].
In the context of adversarial examples, this is justified by the fact that the adversarial examples are
mostly located in areas of low probability for the input distribution [Song et al., 2017; Ma et al.,
2018]. We suppose that pdata admits a density. For an arbitrary coupling measure π, we can define
a relative entropy function as H(∏) = - Jz×z(log(∏(z, Z)) - 1)dπ(z, Z) with H(π) = +∞ is π
does not admit a density. Thus our entropy-regularized problem is:
min
θ∈Rd
max
π∈Π(pdata ,padv )
s.t. padv ∈B (pdata )
E(x,y)~Padv['(fθ(x),y)] + λH(∏).
(11)
The objective is strictly concave in π, therefore our regularized problem becomes strongly concave
which ensures the existence and uniqueness of an optimal solution. It will allow us to obtain an
explicit formulation for the optimal coupling π and then for the optimal adversarial distribution padv .
The solution of this new problem will be an approximate solution of the original problem (A). By
calling the optimal coupling nɪ associated to this regularized problem (II)We have the following
property:
Proposition 3.3. If λ → ∞ ,then nʌ → π*.
3.4	Solutions to the Adversarial Transportation Problem
In this section, we study the general framework where the two measures pdata and padv are defined on
metric spaces X. No particular assumption is made on the form of pdata and padv. The inner-objective
of penalized adversarial transportation problem is the following:
g(θ)=max hθ(π) :=	,max	E(x,y)^padv ['(fθ(X),y)] + 1 H(∏),	(12)
π	π∈Π(pdata ,padv )
s.t. ∃padv ∈B (pdata )
where B(pdata) = {p | W∞,∞(p,pdata) ≤ }. For any given θ ∈ Rd, the function hθ is strictly-
concave and the constraints on π are convex (cf Appendix A for proof). Then, by using convex
duality, we can deduce that the problem (12) has a unique closed-form solution:
Proposition 3.4. For any given θ ∈ Rd, The optimization problem (12) has a unique solution,
Π*((x, y), (x, y)) H Pdata(x, y) exp(λ'(fθ(x), y))1kx-xk∞≤e1y=y .	(13)
For a given natural example X with a label y, π((x, y), (X, y)) gro^ws with the propensity of X to be
better than the other elements of Bx(C) (the '∞-ball of radius e) tofool the classifier. We then deduce
the explicit form of the adversarial distribution:
n* , (χ ) = E	[	( ∣χ) eχp(λ`fθ(x),y))1kχ-χk∞≤e ]	(14)
Padv(X,y) Ex~px,data [pdata(y|X) Rχ∈Be ⑺ exp(λ'(fθ (X),y))dx ]	(14)
From this proposition, we notice that if X ∈ Bx(e), then π(z, Z) > 0, i.e. any examples which
satisfies the adversary condition has a non-zero probability to be considered as an adversarial example
of x. However, when λ is large enough, most of the mass of the distribution lies where '(fθ (X), y) is
close to its maximum value. Moreover it is interesting to notice that the mass of Padv (X, y) can come
from the transport of several input data-points as a potentially infinite number of input data-point X
verify the constraint 1kx-xk∞ and thus could be transported to X. We will seen in the next section
that is never happens in practice.
As we noticed before, the inner maximization problem is strictly-concave in π and the constraints are
convex. By strong duality, we know that the dual form is equal to the primal. The primal can be ex-
pressed with a min over a lagrange multiplier a s.t. α(X, y) = -1 log( Ry()eXd(λi'fθ(x),y)))dx))
(c.f. Appendix A for proof) and is convex w.r.t. this variable. Thus, we can express the original
min-max optimization problem as a convex minimizaion problem in θ which leads to our central
theorem.
5
Under review as a conference paper at ICLR 2022
Theorem 3.1 (Characterization of the solutions). For any λ > 0, the solution set of the regularized
adversarial training problem (11) is:
θ* ∈ argmin E(χ,y)〜
pdata
[1 log( I
J ||X —x∣∣≤e
exp(λ'(fθ(X),y))dX)] =: arg min g(θ),
and the optimal adversarial distribution associated with this adversarial training problem is
Padv (X, y) = Ex~Px,data
[pdata(y|x)
exP0'fθ* (X),y"1kx-引∣∞≤e
∕χ∈Be(x) exP(λ'(fθ (x),y))dx
(15)
(16)
]
In order to solve (12) we would need to sequentially sample from π to compute the gradient of g
(See §4) for the exact derivation of Vg(θ). Because of its normalization constant, the computation
transport plan (29) for any given value of θ may be expensive to compute. However we will see in
the next sections that: a) in practice, one computes this plan simultaneously with the training on the
classifier and b) it has a simpler form in practice when dealing with a finite dataset.
A Zero-Sum game perspective on Adversarial Transport. One perspective on (11) is that it
corresponds to game between a classifier fθ and a transport plan π that aims at transporting the data
distribution pdata into an adversarial distribution padv that is -close to pdata with respect to the
∞-∞-Wasserstein distance. While the classifier aims at minimizing the average cross-entropy loss
over the adversarial dataset the transport plan aims at maximizing the errors of the classifier and its
own entropy. This perspective is closely related to Adversarial examples games (AEG) [Bose et al.,
2020] where the authors tried to learn an adversarial distribution with a generator conditioned on the
input examples. This work differs from AEG by its perspective (we connect adversarial examples
with DRO), the problem formulation (we look for a regularized transport plan while AEG focused on
a generator), the algorithmic implementation (we consider Langevin to find the transport plan), and
the finale focus since AEG concentrated on the generation of transferable adversarial example while
our focus is on training robust classifiers.
This perspective also gives us an insight: the optimal distribution of adversarial examples may provide
adversarial attacks that transfer well across models. The latter point can be supported by the following
minimax theorem that is a simple extension of the result from [Bose et al., 2020, Prop. 1].
Proposition 3.5. Let us call E(x,y)〜「。店['(fθ(X),y)] + 1 H(π) = φ(θ,∏). If {fθ | θ ∈ Θ} is a
convex set, then we have that
min max ω(θ, π) = max min 夕(θ,π)	(17)
θ∈Rd π∈P	π∈P θ∈Rd
Pconv = {padv ： ∃π ∈ 口(Pdata |padv) with SUpp(π) ⊂ {(x, y, χ, y) | ∣∣χ ― xk∞ ≤ €, y ∈ Y }}..
The assumption that the class of function {fθ | θ ∈ Θ} is convex is not always true in practice but it
holds in the case of linear classifier or when one assumes that we have infinite capacity classifiers (i.e.
we can achieve a measurable function). Moreover, considering the class of neural networks, it has
been argued that they were spanning a set that is almost convex [Gidel et al., 2021].
3.5 Practical case: semi-discrete Adversarial Transportation Problem
In practice, we do not have access to the true distribution pdata but only to a finite dataset sampled
from this distribution (e.g MNIST/ CIFAR-10, etc). This can be seen as considering the empirical
distribution Pdata = 1 Pn δχi. However, even when considering empirical distributions, the dis-
tribution of adversarial examples padv is an arbitrary measure on metric space X as an adversarial
example could be any point in the '∞-ball of radius e ball from the initial point (for datapoint Xi we
call that ball B(xi)). Its analysis is thus similar to the continuous case. However, in such a practical
case the optimal transport map has a simpler form under the following assumption
Assumption 3.1 (Empty intersection). We have that B(Xi) ∩ B(Xj) for all 1 ≤ i 6= j ≤ n.
For real world dataset, this assumption is almost always true. It means that two datapoints are not e
close which would mean that they are perceptually indistinguishable. We can actually show that if
the inputs are high dimensional Assumption 3.1 hold with an exponentially high probability,
Proposition 3.6. Let us assume that the datapoints Xi are independently sampled from a distribution
P over [0, 1]d that is absolutely continuous with respect to Lesbegue measure and has a upper-bounded
value, i.e. p(x) ≤ M , x ∈ [0,1]d. Then P(∃i = j ∈ [n], kXj- Xik∞ ≤ e) = O(n2ed).
6
Under review as a conference paper at ICLR 2022
such a proposition could be extended to the more challenging setting where the data distribution is
supported by a manifold but this is out of the scope of this paper since its purpose is purely illustrative.
Theorem 3.2. [Characterization of the solutions in the semi discrete case] For any λ > 0, the
solution set of the regularized adversarial training problem (11) is:
1n
θ ∈ arg min 方 X Iog (J ~	exp(λ'(fθ (x), yj)dx) =:argmm g(θ),	(18)
and under Assumption 3.1 the optimal adversarial distribution associated with this adversarial
training problem is
(	exp(λ'(fθ (x),y))
Padv(X,y)= < n∕x∈a(Xi)eχPC'fθ(x),y))dx
I	0
if ∃i ∈ [n] s.t. kxi — xk∞ ≤ e
otherwise.
(19)
The practical insight from this result is that the distribution of adversarial example can, in practice be
generated example-wise. Meaning that for each datapoint xi, i ∈ [n] one can generate the distribution
of adversarial examples around Xi distributed as Padv (X,y) H exp(λ'(fθ (x),y)), ∀X∣∣X - Xik ≤ e.
Practically, We will maintain a finite dataset of the adversarial examples (X2i,k) associated with the
datapoint Xi .
4	Adversarial Transport Algorithm
We want to solve the minimization problem (40). We can solve this problem using gradient descent:
1	ʌ r	eλ'(fθ(X),yi))
θt+ = θt - ηvθ g where vθ g = n∑ JB 37θ '(fθ(X),yi)) ∕b()eλ'(fθ ⑸,yi))dx dx (20)
To evaluate this gradient we need to sample from the distribution q(X) H exp(λ'(fθ(X), yi)) defined
over B(Xi). This can be done via Langevin Monte Carlo [Durmus and Moulines, 2016] as the next
section explains.
4.1	Langevin Monte Carlo
Unadjestued Langevin Algorithm [Durmus and Moulines, 2016] is convenient to sample high-
dimensional probability distribution q. Having the following density w.r.t. the Lesbegue measure
q(X) H exp(-U(X)), one can show that q is the stationnary measure of the following Stochastic
Differential Equation:
Xt+1 = Xt- Yt+ιVU(Xt) + p2γt+ιξt+ι	(21)
With (ξt)t i.i.d N(0, Id) and (γt)t constant or decreasing towards 0. L-smoothness of U gives
existence and uniqueness of a solution. Langevin MC can be seen as the sampling analog of gradient
descent in optimization. In this work, we consider a projected version of the Langevin algorithm since
we need to sample within some '∞ balls. Such an algorithm has been analyzed by Bubeck et al. [2018]
in the case where U(x) is convex. In our practical setting we will consider Ui(X) = -λ'(fθ(X), yi))
for X ∈ Be(Xi), i ∈ [n] which are non-convex function. However, some results have been proven in
that context by Ma et al. [2019]. Moreover, the Langevin algorithm has been recently used with great
success in the context of diffusion models for deep learning for which the function U is not convex
(see [Weng, 2021] for an overview).
4.2	Practical setting
As described in Algorithm 1, the training process has two loops:
1.	Given a minibatch (Xi, yi)i∈B, the inner loop samples the adversarial distributions qi(X) against
the current classifier fθ via Langevin MC where each samples are then projected onto Bi (e)
(operator PBi (e)) and onto the space X (operator PX). It allows us to estimate vg.
7
Under review as a conference paper at ICLR 2022
2.	The outer loop consists of the stochastic gradient descent step (20).
Sign of the Gradient. Similarly to standard adversarial training and attack methods we use the sign
of the gradient of g to update θ. It can be motivated by the fact that the sign of gradient is well suited
to the geometry defined by the '∞ ball since it corresponds to the steepest descent direction [Boyd
et al., 2004] in the '∞ geometry [Balles et al., 2020].
Reusing the Examples. In practice, we restart the Langevin MC from the adversarial examples
found at the previous iterations, this allow us to only use one step of Langevin MC in the inner
loop and significantly speed-up training. This idea is justified by the fact that the distribution
Padv(X, y) 8 exp(λ'(fθ(x), y)) is actually a continuous function of θ. It can be seen with duality
argument from (12) where the function hθ(π) is strictly concave with respect to π which implies the
continuity of θ 7→ arg maxπ hθ(π). We leave as a future work the formalization of this argument.
5	Experimental Results
We investigate the Adversarial Trans-
port Algorithm at training robust clas-
sifiers (for a given hypothesis class)
against adversarial attacks on MNIST
and CIFAR-10.
Experimental setup The experiments
are realized on MNIST and CIFAR-10.
We perform all attacks and training with
= 0.3 for MNIST and = 8.0 for
CIFAR-10 (w.r.t. to the '∞ norm). We
train our robustly trained classifier us-
ing stochastic gradient descent with the
cross-entropy loss. We use a learning
rate lr = 0.1 in the case of Adversarial
Transport and update θ with the sign of
the gradient.
Algorithm 1 Adversarial Transport with Langevin
1:	input: dataset (xi, yi)in=1, step-size: η, number of
adversarial examples: K, noise variance σ.
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Initialization: X%,k = Xi, i ∈ [n], k ∈ [K]
for n : nb of steps do
Sample a minibatch: (Xi, yi)i∈B, B ⊂ [n]
Load the attacks: X(0) J Xi,k, i ∈ B, k ∈ [K]
for T : nb langevin iter do
for i ∈ B, k ∈ [K] do
X(t+1) J X⑴ + sio∙n(V '( f (X(t+1) ))
xi,k J xi,k + η Signl V χ'f θ(xi,k , yi))
X(t+1) J PBi(e) [X(t+1) + σξ]
X(t+I) J PX [X(t+1)]
Vθg=
|B|K Pi∈B,k∈[K] Vθ '(fθ (X(t+1),yi))
θ J θ - ηVθg
Save the attacks: Xi,k J X(Tk), i ∈ B, k ∈ [K]
Return: θ
For both datasets, we train our classifier with 100 epochs, a batch size of 100. For Langevin MC
parameters, we take K = 1, ηι(λ) = 0.2 and σ = 0.2 (i.e. λ =缪=10). In theory, we must take λ
big enough to have a good approximate of the solution. Appendix B.1 shows that in practice λ = 10
is the best value to ensure convergence of our algorithm.
Baselines We test our robust classifier against first-order attacks: Fast Gradient Sign Method (FGSM)
attacks [Goodfellow et al., 2014; Szegedy et al., 2014], Projected Gradient Descent (PGD) attacks
with 40 iterations (PGD 40) and PGD attacks with 100 iterations (PGD 100) [Madry et al., 2017;
2018]. We compare those results with the performance of classifiers adversarially trained with PGD
[Goodfellow et al., 2014; Madry et al., 2017; 2018] on the same attacks (we trained PGD classifiers
according to the method described in the paper and use the same parameters: lr = 0.01, 100 epochs,
trained with PGD 40). All the attacks are generated via the library Advertorch of Pytorch3. The
attacks are '∞ bounded. Additional details concerning the tuning of hyperparameters and hypothesis
classes used are given in the Appendix B.14. We evalute white-box attacks (i.e. we have access to the
gradient of the model we want to attack). The results are provided below:
5.1 Performance on MNIST
We first compare both methods on MNIST, the architecture is the same for both frameworks and
is described in the Appendix B.2. We make two main observations, first training the model using
Langevin improves the overall robustness of the classifier against all attackers we tested against as
shown in Table 1. Secondly, by re-using the samples from the previous iterations we converge much
3https://github.com/BorealisAI/advertorch.git
4Code : https://anonymous.4open.science/r/Adversarial-Training-BCA4
8
Under review as a conference paper at ICLR 2022
faster, indeed for each update of the classifiers we only need to do one back-propagation instead. This
leads to a very important speed up as observed in Fig 2.
Performance	Langevin	PGD
Train Adversarial	98.48%	94.81%
Clean test set	99.16%	98.82%
FGSM	96.53%	94.95%
PGD 40	93.12%	92.28%
PGD 100	91.87%	91.18%
Autoattack	88.55%	89.10%
Table 1: Comparison of the performance
for MNIST of robust classifiers trained with
Langevin and PGD (with 40 iterations). The
architecture of the two classifiers is architecture
A. The attacks are FGSM, PGD 40, PGD 100,
and autoattack (Croce and Hein [2020]). The
classifier trained with Langevin is more robust
than the one trained with PGD for each measure
except Autoattack.
Figure 1:	Comparison of Langevin and PGD adversarial
training on MNIST. We plot the error rate against FGSM
as a function of the number of backprops. We observe that
Langevin converges much faster as it only needs to compute
1 gradient per update of the classifier by being able to re-use
the samples from the previous iterations. After 8 iterations,
Langevin achieves a score that is not even reached by PGD
after 1600 iterations.
5.2 Performance on CIFAR-10
Performance	Langevin	PGD
Train Adversarial	99.82%	99.38%
Clean test set	78.72%	68.93%
FGSM	53.40%	45.58%
PGD 40	49.57%	44.70%
PGD 100		49.50%	43.74%
Table 2: Comparison of the performance for
CIFAR-10 of robust classifiers trained with
Langevin and PGD (with 40 iterations). The ar-
chitecture of the two classifiers is RESTNET18.
The attacks are FGSM, PGD 40, and PGD
100. As for MNIST, the classifier trained with
Langevin is much more robust than the one
trained with PGD for each measure.
Figure 2:	Comparison of Langevin and PGD adversarial
training on CIFAR. We plot the error rate against FGSM
as a function of the number of backprops. We observe that
Langevin converges much faster as it only needs to compute
1 gradient per update of the classifier by being able to re-use
the samples from the previous iterations.
6 Conclusion
In this paper, we introduce the Adversarial Transport Framework. We derived this framework from
Adversarial Training relaxation through regularized Distributionally Robust Optimization (DRO)
with ∞-∞-Wasserstein distance. The outcome of this work provides a close form of the optimal
adversarial distribution against a given classifier which can be sampled via Langevin Monte-Carlo.
From that, we have designed an algorithm that optimally trains (for a given hypothesis class) a
classifier against adversarial examples. The major asset of our work, which surely justifies these
outstanding results, is that it considers the problem at the scale of the entire dataset and not at the
scale of the example.
Our algorithm brings several novelties: at each iteration, we perform a gradient ascent on the
distribution and a gradient descent on the classifier. For this, we keep in memory the current
distribution of adversarial examples and iterate on it at each step. In addition, only one iteration
of Langevin MC is necessary at each step. As a result, our experiments show that the Adversarial
Transport classifier outperforms PGD trained classifier on MNIST and CIFAR-10, but also that our
algorithm allows us to train it much faster than with PGD thanks to the novelties described above.
9
Under review as a conference paper at ICLR 2022
References
A. Athalye, L. Engstrom, and K. K. A. Ilyas. Synthesizing robust adversarial examples. Synthesizing
robust adversarial examples. In The Thirty-fifth International Conference on Machine Learning
(ICML), 2018.
L. Balles, F. Pedregosa, and N. L. Roux. The geometry of sign gradient descent. arXiv preprint
arXiv:2002.08056, 2020.
A. Ben-Tal and A. Nemirovski. Robust convex optimization. Math OPerRes, 23, pp. 769-805,1998.
J. Blanchet and K. Murthy. Quantifying distributional model risk via optimal transport. Mathematics
of OPerations Research, 44(2):565-600, 2019.
J. Bose, G. Gidel, H. Berard, A. Cianflone, P. Vincent, S. Lacoste-Julien, and W. Hamilton. Adversar-
ial example games. Advances in Neural Information Processing Systems, 33, 2020.
S.	Boyd, S. P. Boyd, and L. Vandenberghe. Convex oPtimization. Cambridge university press, 2004.
S.	Bubeck, R. Eldan, and J. Lehec. Sampling from a log-concave distribution with projected langevin
monte carlo. Discrete & ComPutational Geometry, 2018.
T.	Champion, L. de Pascale, and P. Juutinen. The ∞-wasserstein distance: local solutions and
existence of optimal transport map. SIAM Journal on Mathematical Analysis, 2008.
E.	Chouzenoux, H. Gerard, and J. Pesquet. General risk measures for robust machine learning.
Foundations of Data Science 1, 249, 2019.
F.	Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In ICML, 2020.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Adv. in Neural
Information Processing Systems, Pages 2292-2300, 2013a.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transportation distances. Advances
in Neural Information Processing Systems 26, Pages 2292-2300, 2013b.
E. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with application
to data-driven problems. OPerations research, 2010.
J. Duchi and H. Namkoong. Stochastic gradient methods for distributionally robust optimization with
fdivergences. In Advances in neural information Processing systems, PP. 2208-2216, 2016.
J. Duchi and H. Namkoong. Learning models with uniform performance via distributionally robust
optimization. arXiv PrePrint arXiv:1810.08750, 2018.
J. Duchi, P. Glynn, and H. Namkoong. Statistics of robust optimization: A generalized empirical
likelihood approach. arXiv PrePrint arXiv:1610.03425, 2016.
A. Durmus and E. Moulines. High-dimensional bayesian inference via the unadjusted langevin
algorithm. arXiv:1605.01559, 2016.
E.Wong, F. R. Schmidt, and J. Z. Kolter. Wasserstein adversarial examples via projected sinkhorn
iterations. Proceedings of the 36th International Conference on Machine Learning, PMLR 97:6808-
6817,, 2019.
R. Gao, X. Chen, and A. J. Kleywegt. Wasserstein distributional robustness and regularization in
statistical learning. arXiv PrePrint arXiv:1712.06050v2, 2017.
A. Genevay, M. Cuturi, G. Peyre, and F. Bach. Stochastic optimization for large-scale optimal
transport. NeurIPS, 2016.
G. Gidel, D. Balduzzi, W. Czarnecki, M. Garnelo, and Y. Bachrach. A limited-capacity minimax
theorem for non-convex games or: How i learned to stop worrying about mixed-nash and love
neural nets. In International Conference on Artificial Intelligence and Statistics, pages 2548-2556.
PMLR, 2021.
10
Under review as a conference paper at ICLR 2022
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples.
arXiv:1412.6572, 2014.
L. V. Kantorovich. A new method of solving of some classes of extremal problems. In Dokl. Akad.
Nauk SSSR, volume 28, pages 211-214, 1940.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. arXiv:1611.01236,
2016.
H. Lam. Recovering best statistical guarantees via the empirical divergence- based distributionally
robust optimization. Operations Research, 2018.
X. Ma, B. Li, Y. Wang, S. M. Erfani, S. Wijewickrema, M. E. Houle, G. Schoenebeck, D. Song, and
J. Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality. ICLR 2018,
2018.
Y.-A. Ma, Y. Chen, C. Jin, N. Flammarion, and M. I. Jordan. Sampling can be faster than optimization.
Proceedings of the National Academy of Sciences, 2019.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant
to adversarial attacks. arXiv:1706.06083 [stat.ML], 2017.
A. Madry, A. M. L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learning Representations, 2018.
Q. Merigot and B. Thibert. Optimal transport: discretization and algorithms. 2020.
S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to fool
deep neural networks. Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2574-2582, 2016.
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 427-436, 2015a.
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. Computer Vision and Pattern Recognition (CVPR 2015).
IEEE, 2015b.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of
deep learning in adversarial settings. Security and Privacy (EuroS&P), 2016 IEEE European
Symposium on, pages 372-387, 2016a.
N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. Security and Privacy (SP), 2016 IEEE Symposium on,
pages 582-597. IEEE, 2016b.
G.	Peyre and M. CUtUrL Computational optimal transport. Foundations and Trends in Machine
Learning, vol. 11, no. 5-6, pp. 355-607, 2019.
H.	Rahimian and S. Mehrotra. Distributionally robust optimization: a review. arXiv preprint
arXiv:1908.05659, 2019.
A. I. S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry. Adversarial examples are not
bugs, they are features. arXiv:1905.02175, 2019.
A. Sinha, H. Namkoong, R. Volpi, and J. Duchi. Certifying some distributional robustness with
principled adversarial training. ICLR, 2018.
R. Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. Ann.
Math. Statist., 35:876-879, 1964.
Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman. Pixeldefend: Leveraging generative
models to understand and defend against adversarial examples. ICLR 2018, 2017.
11
Under review as a conference paper at ICLR 2022
M. Staib and S. Jegelka. Distributionally robust deep learning as a generalization of adversarial
training. In NIPS Machine Learning and Computer Security Workshop, 2017.
L. Sun, M. Tan, and Z. Zhou. A survey of practical adversarial example attacks. Cybersecurity,
1(1):9, 2018.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. International Conference on Learning Representations (ICLR),
2014.
F. Tramer, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble adversarial
training: Attacks and defenses. International Conference on Learning Representations (ICLR),
2017.
F. Tramer, A. Kurakin, N. Papernot, D. Boneh, and P McDaniel. Ensemble adversarial training:
Attacks and defenses. arXiv:1705.07204 [stat.ML], 2017.
C. Villani. Topics in optimal transportation, volume 58. American Mathematical Soc., 2003.
C. Villani. Optimal transport: old and new. Springer, 2009.
L. Weng. What are diffusion models? lilianweng.github.io/lil-log, 2021. URL https:
//lilianweng.github.io/lil- log/2021/07/11/diffusion- models.html.
A. Wilson. The use of entropy maximizing models, in the theory of trip distribution, mode split and
route split. Journal ofTransport Economics and Policy, pages 108-126, 1969.
S.	Zagoruyko and N. Komodakis. Wide residual networks. 2017.
T.	Zheng, C. Chen1, and K. Ren. Distributionally adversarial attack. The Thirty-Third AAAI
Conference on Artificial Intelligence, 2019.
12
Under review as a conference paper at ICLR 2022
A Proofs of theoretical results
Proposition 2.1. The adversarial training problem (1) can be formulated as a DRO problem (2)
where:
P = {padv : ∃T : X → X , padv = T]pdata and kT (x) - xk∞ ≤ , ∀x ∈ X} (22)
Proof of Proposition 2.1. Let us start with the original adversarial training problem:
min E(x,y)~Pdata[LmaX- 'fθ(X,y))]	(23)
θ∈Rd	∣∣x-xk∞≤e
Let Us call T(x) := X, thus, We have
E(x,yhPdata [	max《'fθ (T (X)J))] =	maX《守二木 ElxS—Pdatal'f。(T (X)，y))]]
kT (x)-xk∞ ≤	T , kT (x)-xk∞ ≤ , ∀x∈X
By definition of the pushforWard measure We have that
E(Xiata [kT(XmaX∞≤∕(fθ (T (X)y))]= T/T ⑺学之1%%，。)〜。—"(X，y))]]
which conclude our proof since then it is equivalent to maximize over T or Padv = T]Pdata.	口
Proposition 3.1. The DRO problem (2) can be relaxed by considering coupling measure. In that
case the constraint set P in Equation 2 take the form:
Pconv = {Padv ： ∃∏ ∈ ∏(Pdata∣Padv) With SUpp(∏) ⊂ {(X,y,X,y) |||X - X∣∣∞ ≤ e, y ∈Y }}.
Proof of Proposition 3.1. We will prove that
P ⊂ Pconv	(24)
and that Pconv is convex. First let us start by showing that,
P ⊂ Pconv.	(25)
Let us consider padv ∈ P. By definition of P there exists T : X → X such that padv = T]pdata
and kT (X) - Xk∞ ≤ e. Now, we can define the coupling measure πT := (Id, Id, T, Id)]pdata
where (Id, Id, T,Id)(X, y) = (x, y, T(x), y). We can verify that ∏t ∈ ∏(pdata∣T]Pdata) and that by
definition of T, we have SuPP∏t ⊂ {(x, y, X, y) s.t. ∣∣X 一 x∣∣∞ ≤ e, x, x0 ∈ X , y ∈ Y }.
Finally, let us prove that Pconv is convex. Let us consider p1, p2 ∈ Pconv and λ ∈ [0, 1]. By definition
of PCOnv, we know that there exists ∏ι ∈ ∏(pdata∣Pι) and ∏2 ∈ Π(pdata∣p2) such that supp(∏ι) and
supp(∏2) are included in {(x, y, X, y) s.t. ∣∣X - x∣∣∞ ≤ e, x,x0 ∈ X , y ∈ Y }. By linearity of the
marginalization we have that λ∏ι + (1 - λ)∏2 ∈ ∏(pdata∣λpι + (1 - λ)p2). Moreoevr, we have
that supp(λ∏ι + (1 — λ)∏2) = supp(∏ι) ∪ supp(∏2) ⊂ {(x, y, X, y) s.t. ∣∣X 一 x∣∣∞ ≤ e, x, x0 ∈
X , y ∈ Y }. Thus we have λpι + (1 - λ)p2 ∈ PCOnv. It concludes the fact that PCOnv is convex. 口
Proposition 3.2 (convex relaxation of adversarial training). The convex DRO relaxation of adversar-
ial training can be reformulated as
min maX
θ∈Rd padv ∈B (pd
ata)
E(X,y)~Padv ['fθ (X, y)]
where Be(Pdata) = {p | W∞,∞(p, Pdata) ≤ e} and Wp,∞ is defined in (9).
Proof of Proposition 3.2. Under mild assumptions (see the assumption of [Villani, 2009, Prop 5.21])
Be(Pdata) = {p | W∞,∞(p, pdata) ≤ e} = PCOnv.
padv ∈ Pconv < ⇒ ∃π ∈ π(pdata |padv) with supp(π) ⊂ {(x,y,x,y) | kx - xk∞ ≤ e, y ∈ Y }
(26)
^⇒ ∃∏ ∈ Π(pdata∣Padv) With SuP ∣∣z - Z∣∣∞ ≤ e	(27)
(z,Z)∈supp(π)
^⇒ min	SuP	∣∣z — Z∣∞ ≤ e	(28)
π∈π(Pdata |padv ) (z, £) ∈supp(∏ )
Note that the second equivalence is possible because e < miny6=y0 |y - y0 | and the third one because
the minimum is achieved by compactness of Π(pdata ∣Padv) (see [Villani, 2009, Prop 5.21]). 口
13
Under review as a conference paper at ICLR 2022
Proposition 3.3. If λ → ∞ ,then nʌ → π*.
Proofof Proposition 3.3. The proof is similar to the proof of Proposition 4.1 from Peyre and Cuturi
[2019] since P is compact.	□
Proposition A.1. For any given θ ∈ Rdr, the function hθ (π) = E(或y)〜padv ['(fθ (X), y)] + 1 H (∏)
is strictly-concave and the constraints on P are convex.
Proof. P and F are convex sets: both constraints of P are linear, and is convex by F by assumptions.
Moreover,
• hθ(π) is the sum of a concave term and a strongly concave term. Indeed:
-∏ → E.,y)〜Padv ['(fθ (x), y)] concave in ∏ by linearity.
-π → 1H(π) is strongly concave in π.
• θ → hθ(π) is convex by convexity of l (e.g. mean squared loss, cross entropy loss, ...).
□
Proposition 3.4. For any given θ ∈ Rd, The optimization problem (12) has a unique solution,
π*((x,y), (x,y)) H Pdata(X,y)exp(λ'(fθ (χ),y))1kχ-Xk∞≤e1y=y .	(29)
For a given natural example X with a label y, π(z, Z) gro`ws with the propensity of X to be better than
the other elements of Bχ(e) (the '∞ -ball of radius e) to fool the classifier. We then deduce the explicit
form of the adversarial distribution:
Padv(X,y)
Eχ~px,data
[pdata(y|X)
exP0'fθ(X),yD1kx-X∣∣∞≤e
∕χ∈Be(x) exP(λ'(fθ (X),y))dX
∏*(z, Z) H Pdata(x, y) exp(λ'(fθ(X), y)) 11∣χ-划∞ & 1y=y .
Proofof Proposition 3.4. The objective function is concave and at the optimum π*((x, y), (X, y)) >
0 for each (x, X, y) s.t. ∣∣x 一 Xk ≤ e. The objective function is C1 in the neighborhood of π*. Thus,
We can introduce dual variable α : R → R s.t. V∏L(π*, α) = 0. The Lagrangian function L is:
L(π, α)
/
Z×Z
'(fθ(X),y)dπ((X,y)(X,y)) -
1/
λ Z×Z
(Iog(n((X,y), (X,y)) — l)dπ((X,y)(X,y))
α(X, y)
Z×Z
Pdata(X,y)dX 一
/
Z×Z
dπ((X,y)(X,y))
(30)
+
L(π, α) = [	('(fθ(X),y) — 1(log(π((X加侬4)))一 1) 一 a(X,y))dn((X,y)(X,y))
Z×Z	λ
+
α(X, y)Pd
ata (X, y)
Z
(31)
dX
dL(π, a)) = '(fθ(X), y) 一 1 log(π((X, y)(X, y))) — a(X, y) = 0	(32)
∂π	λ
So at the optimum, for each ((x, y), (X, y)) s.t. ∣∣x — Xk ≤ e, we have
π*((x, y), (X, y)) = exp(λ('(fθ(X), y)))exp(一λɑ(x, y))).	In addition, we know
14
Under review as a conference paper at ICLR 2022
that	/(2,y)eBe((x,y)) d∏((x,y), (X,y))	=	Pdata(X,y),	thus exp(-λα*(x,y))
Pdata(X, y) Rx∈B (x) exp(λ('(fθ0),y)))d- ∙ Therefore，for each ((X, U), (X, V) s∙t∙ kx - Xk ≤ e：
π*((χ,y), (χ,y))
e e	exp(λ('(fθ (X),y)))	1
Pdata(X，y) Rseβe(x) exp(λ('(fθ(X),y)))dX IE
And π((x, y)(X, y)) = 0 for each ((x, y), (X, y)) s.t. ∣∣x — Xk > e, i.e.:
π*((X,y),(X,y))
Pdata (X, y)
exp(λ('(fθ (X),y)))
R方∈Be(x) exP(λ('(fθ(X), y))dX
Ikx-Xk≤eiy=y
∏*((x, y)(X, y)) H Pdata(x, y) exp(λ'(fθ (X), y))1--训 ∞≤Jy=y .
Therefore, we have that:
sup L(∏,α) = 1 + / α(X, y)pdata (x, y)d(X, y)
1	1	Pdata (X, y)
SuPLga) = λ - λ ZzPdata(X,y)log( KBeeXP(λ('(fθ(X),y)))dX)d(X,y)
(33)
(34)
sup L(π, α)
π
Pdata(Z)log( /	exp(λ('(fθ (X), y))) dX)))dz — / Pdata(Z)To虱 Pdata(Z))dz + 1
√5∈Be(x)	JZ
(35)
□
Proposition 3.5. Let us call E(X,切~「。4。['(fθ(X),y)] + 1 H(∏) = ψ(θ,∏). If {fθ | θ ∈ Θ} is a
convex set, then we have that
min max ω(θ, ∏) = max min 夕(θ,π)	(36)
θ∈Rd π∈P	π∈P θ∈Rd
where P =	{Padv	：	∃∏	∈	∏(Px,data∣x,Px,adv)	with	∏((z), (X,y)) = 0 ,	∀x,X ∈ X s.t.	kX	—
Xk∞ > e }.
Proof. The proof is similar to the proof of Theorem 28 from Merigot and Thibert [2020]. 口
Proposition 3.6. Let us assume that the datapoints Xi are independently sampled from a distribution
P over [0, 1]d that is absolutely continuous with respect to Lesbegue measure and has a upper-bounded
value, i.e. p(x) ≤ M , x ∈ [0,1]d. Then P(∃i = j ∈ [n], ∣∣Xj — Xik∞ ≤ e) = O(n2ed).
Proof of Proposition 3.6. Let us consider the quantity
P(∃i = j ∈ [n] , kXj - Xik∞ ≤ e) ≤ n(n. I)P(IlX — x∣∣∞ ≤ e)	(37)
≤ n(n ~ 1) MVοl(B(x,e))	(38)
=%=4 Med	(39)
□
15
Under review as a conference paper at ICLR 2022
Theorem 3.2 (Characterization of the solutions in the semi discrete case). For any λ > 0, the solution
set of the regularized adversarial training problem (11) is:
1n
θ* ∈ arg min 方 X log ( J 〜	exp(λ'(fθ (X),yJ)dx) =: arg min g(θ),	(40)
and under Assumption 3.1 the optimal adversarial distribution associated with this adversarial
training problem is
(	exp(λ'(fθ (x),y))
Padv(X,y)= < n∕x∈a(Xi)eχPC'fθ(x),y))dx
I	0
if ∃ i ∈ [n] s.t. ∣∣Xi 一 Xk∞≤
(41)
otherwise.
Proof of Theorem 3.2.
min maχD E(x,0y)〜π(z,(x,y))['fθ(X), y)] + 1H H(π)	(42)
fθ ∈F padv ∈P	λ
From Proposition 3.5:
=min	(min( Z α(X, y)Pdata(X, y)dχ)	+	1 ) for	α(x,	y)	= 一1 log(R---------Pdata(Xy∖	∖∖.3
fθ∈Fl ɑ JX	λJ	λ	J∈BB((x) eχp(λ('(fθ(X),y))))dx
(43)
Pdata(X, y) log(Pdata(X, y))dX + 1
(44)
□
min
fθ∈F
Pdata(x, y)log( /	exp(λ('(fθ(X), y)))dx)dx — /
Jkx-X∣l≤e	JX
Finally, the probability distribution of the adversarial examples Padv is written:
padv (X, y) = /
x∈X
dπ((X,y, )(X,))
eχp(λ'(fθ(X),y)))	1 d
Padv(X,y) = JXPdata(X,y) RX exp.'5(X),y)))dX 1kx-训三皿
PK (Xy)= E	[p . + (y∣X) exp(λ`(fθ(M,y))1kχ-χk∞≤e ]
adv	,	X〜pχ,data data	Rx∈b (x) eχp(λ'(fθ(x),y)) 1
(45)
(46)
(47)
In practice: ∀i Pdatai = 1, so we have:
(	exp(λ'(fθ (X),y))
Padv(X,y) = < n RX∈B,(Xi) exp(λ'(fθ(X),y))
(I	0
and then,
if ∃i ∈ [n] s.t. ∣Xi — X∣∞ ≤ e
otherwise.
1n
θ* ∈ arg min - ^X log ( J
exp(λ'(fθ (X),yi))dX)=: arg min g(θ),
B Additional results
B.1	Tuning of hyperparameters
In this section, we detail the tuning of different hyperparameters. Among them:
16
Under review as a conference paper at ICLR 2022
•	T : The number of Langevin iterations for crafting adversaries during the training process.
•	σ : Represents the noise which is the particularity of the Langevin. Indeed, it differs from
the gradient descent by the addition of Gaussian noise. The noise allows adding randomness
in the crafting of adversarial examples. Thus, the size of the noise, as well as its shape, are
of primary importance in the performance of Langevin. For example, a too important noise
will generate totally random examples and will erase the weight of the gradient.
•	ηl = λ × γ : represents the learning rate of Langevin MC. The parameter represents the
learning rate of Langevin.
In general, even if T ≥ 2, we only keep the last batch of adversarial examples for updating θ. Here,
all the results are presented for T = 1 which gives the best results and the best computation time.
A further study could analyze the influence of using more batches of adversarial examples at each
epoch of the training. The results for the influence of σ and ηl (λ) are provided below:
Figure 3: Influence of σ (noise_scale) on the performance of the classifier when training with
Adversarial Transport framework with architecture A. These results are for T = 1 and ηl (λ) = 0.2.
We did the same experiment with other values of ηl (λ) but whatever the value of σ, ηl (λ) = 0.2
gives the best result.
Figure 4: Influence of ηl(λ) (eta_langevin) on the performance of the classifier when training with
Adversarial Transport framework with architecture A. These results are for T = 1 and σ = 0.2. We
did the same experiment with other values of σ but whatever the value of ηl (λ), σ = 0.2 gives the
best result.
The two curves are similar. We notice that the method converges only for small range of values
of these hyperparameters. Regarding that λ = 22, these experiments show that the best value for
λ we can take is λ = 10. These graphs justify the fact that our whole study was conducted with
ηl(λ) = 0.2 and σ = 0.2.
B.2	Models architecture
We study the influence of the architecture of the neural network on the performance of our model.
We test our algorithm with the two models below. Model B is the one used by Madry et al. [2018] for
the PGD framework.
17
Under review as a conference paper at ICLR 2022
Architecture A
Conv(1,64,5)
ReLU()
Conv(64,64,5)
ReLU()
Dropout(0.25)
Linear(64 × 20 × 20, 128)
Dropout(0.5)
Linear(128,10)
Performance	Langevin	PGD
Train Adversarial	98.48%	94.81%
Clean test set	99.16%	98.82%
FGSM	96.53%	94.95%
PGD40	93.12%	92.28%
PGD 100		91.87%	91.18%
Table 3: Comparison of the performance of robust classifiers trained with the Adversarial Transport
framework (Langevin) and PGD (trained with 40 iterations). The architecture of the two classifiers is
the above architecture A. The robustness of these two models is tested against FGSM attacks, PGD
with 40 iterations (PGD 40) and PGD with 100 iterations (PGD 100). The trained classifier with
Langevin is more robust than the one with PGD.
Architecture B		
Conv(1,32,3,padding = 1) ReLU() Conv(32,64,3, padding = 1, stride = 2) ReLU() Conv(32,64,3, padding = 1) ReLU() Conv(32,64,3, padding = 1) ReLU() Flatten() Linear(7 × 7 × 64, 100) ReLU() Linear(100,10)			
	Performance	Langevin
	Train Adversarial Clean test set	99.42% 99.10%
	FGSM PGD40 PGD 100		97.73% 94.82% 94.18%
		
Table 4: Performance of robust classifiers trained with the Adversarial Transport framework
(Langevin) with architecture B. The robustness of this model is tested against FGSM attacks, PGD
with 40 iterations (PGD 40) and PGD with 100 iterations (PGD 100). The results are better than for
architecture A. Especially, we notice that there is no loss of performance between PGD 40 attacks
and PGD 100 attacks. As explained in Madry et al. [2018], networks with larger capacity are stronger
against adversarial attacks.
Performance	Langevin	PGD40
Train Adversarial	98.90%	97.42%
Clean test set	84.07%	83.54%
FGSM	56.47%	59.51%
PGD40	55.64%	58.88%
PGD 100	54.50%	57.97%
Autoattack	26.12%	26.04%
Table 5: Performance comparison of a wide ResNet (Zagoruyko and Komodakis [2017]) trained with
the Adversarial Transport framework (Langevin) and PGD 40 on CIFAR-10. No hyperparameter
tuning was done; the same setup as described in Section 5 is used with the exception of only training
for 36 epochs instead of 100. The Langevin framework took about 2h10m to train, while PGD 40
took 43h43m to train.
18