Under review as a conference paper at ICLR 2022
Learning Diverse Options via
InfoMax Termination Critic
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of autonomously learning reusable temporally extended
actions, or options, in reinforcement learning. While options can speed up transfer
learning by serving as reusable building blocks, learning reusable options for un-
known task distribution remains challenging. Motivated by the recent success of
mutual information (MI) based skill learning, we hypothesize that more diverse
options are more reusable. To this end, we propose a method for learning termina-
tion conditions of options by maximizing MI between options and corresponding
state transitions. We derive a scalable approximation of this MI maximization via
gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. Our
experiments demonstrate that IMTC significantly improves the diversity of learned
options without extrinsic rewards, combined with intrinsic rewards. Moreover, we
test the reusability of learned options by transferring options into various tasks,
confirming that IMTC helps quick adaptation, especially in complex domains
where an agent needs to manipulate objects.
1 Introduction
Behavior learning from environmental interaction is a fundamental problem in artificial intelligence
and robotics. Recently, combined with deep neural networks (DNN), reinforcement learning (RL)
has successfully learned complex behaviors guided by human-designed reward functions (Heess
et al., 2017; OpenAI et al., 2019). To reuse trained agents across multiple reward functions, or shortly
tasks, abstracting a course of action as a higher-level building block (Barto and Mahadevan, 2003;
Barto et al., 2013) would be useful. A representative formulation of action abstraction in RL is the
options framework (Sutton et al., 1999), where each option consists of a sub policy and its termination
condition. Serving as reusable behavioral blocks, options have the potential of accelerating learning
in new tasks (Brunskill and Li, 2014). For example, if a wheeled robot already had learned to lean
and turn left and right, it can learn to drive on unfamiliar roads faster than learning from scratch.
However, discovering reusable options remains challenging due to the difficulty of defining and
measuring reusability a priori. Although we can measure the reusability of options when we know the
task distribution (e.g., in a Bayesian sense (Solway et al., 2014)), it is difficult to measure it without
prior knowledge. Hence, in such cases, we need a reasonable heuristic assumption about future tasks.
In this paper, we follow a common assumption that diversifying options improves the reusability of
options (e.g., Barto et al. (2004)). That is, we assume that tasks are uniformly distributed over the
state space. Diverse options are expected to work to some extent for any task, thus we argue that this
is a reasonable heuristics. Under this assumption, the next problem is to measure the diversity of
options. The most intuitive strategy would be visiting all states as equally as possible. However, this
can be computationally too expensive when the environment has continuous or large state spaces. As
an alternative, in the context of learning skills without termination conditions, mutual information
(MI) maximization has been widely applied for learning diverse policies (Eysenbach et al., 2019;
Baumli et al., 2021; Choi et al., 2021). An advantage of MI maximization is the existence of tractable
approximations with DNN, scaling it up to more complex domains.
For learning diverse options, we maximize MI between options and their terminating states con-
ditioned by a starting state. This MI maximization diversifies the destinations of an agent when
choosing different options at a state while keeping the transition as deterministic as possible, leading
to diverse and meaningful options. Specifically, we propose to maximize this MI by optimizing
1
Under review as a conference paper at ICLR 2022
IMTC + VIC
OC + VIC
Option O	Option 1
0.8
0.6
0.4β(x)
0.2
0.0
0.8
0.6
0.4β(x)
0.2
0.0
Figure 1: Left: PointBilliard environment. Right: Learned options by IMTC (upper row) and
OC (lower row). Arrows show policies, and heatmaps show termination conditions of options.
the termination conditions of options. We derive an unbiased gradient estimator of this MI w.r.t.
termination conditions using the termination gradient theorem (Harutyunyan et al., 2019). As a
key contribution, we reformulate the estimated gradient using Bayes’ rule and derive a tractable
approximation with a leaned classifier, yielding the InfoMax Termination Critic (IMTC) algorithm.
The easiness of approximation is advantageous compared to the Termination Critic (TC, Harutyunyan
et al. (2019)). The same objective is used by Variational Intrinsic Control (VIC, Gregor et al. (2016))
for diversifying sub-policies, but the original VIC used a constant termination probability for all
states and options. However, since VIC depends on relating starting and terminating states to an
option, we argue that diversifying termination conditions is also important. We show that our method
helps VIC learn clearer options.
In this paper, we first introduce backgrounds and notations, followed by the derivation of IMTC. We
implement IMTC on the Option Critic Architecture (OC, Bacon et al. (2017)) and Proximal Policy
Optimization (PPO, Schulman et al. (2017)). As a minor contribution, our implementation includes a
newly proposed optimistic advantage estimation that speeds up option learning for a single task. In
experiments, we qualitatively demonstrate that IMTC successfully learns diverse and meaningful
options in a reward-free RL (Jin et al., 2020) setting. Figure 1 shows the learned options by IMTC
and OC combined with VIC rewards in PointBilliard domain. We can see that options learned
by IMTC are clearly separated and different. Then we test the reusability of learned options in task
adaptation experiments: we first pre-train options without rewards and then transfer them on various
tasks. We show that IMTC helps quick adaptation to specific tasks especially in complex tasks where
object manipulation is required. Since IMTC is only for learning terminating conditions of options,
we can combine it with other methods for learning diverse sub-policies. Our experiments observed
that IMTC improves the performance of VIC and RVIC (Baumli et al., 2021).
2 Background and Notation
We assume the standard RL setting in the Markov decision process (MDP), following Sutton and
Barto (2018). MDP M consists of a tuple (X , A, p, r), where X is the finite set of states, A
is the finite set of actions, p : X × A × X → [0, 1] is the state transition function, r : X ×
A → [rmin, rmax] is the reward function, where rmin, rmax are minimum and maximum. We use
Xt and At for denoting random variables of the state and action experienced at time t. We define
G d=ef Pt∞=0 γtRt as discounted cumulative return, where Rt = r(Xt, At) is the reward received at
time t and 0 < γ < 1 is the discount factor. We consider maximizing expected discounted return
E [G∣∏]. As an agent, We consider a memoryless policy ∏ : X ×A→ [0,1]. ∏ induces two value
def
functions: action-value function Qπ(x, a) = Eπ [G|X0 = x, A0 = a, π] and state-value function
Vπ(x) = Pa ∏(a∣χ)Qπ(x, a). We use dπ for denoting an agent's average occupancy measure
dπ(x) = lim 叫Pt=0 IXt=x], where I is the indicator function.
n→∞	n ,
Assuming that π is differentiable by the policy parameters θπ , the policy gradient (PG)
method (Williams, 1992) maximizes Gπ by updating θπ via gradient ascent. A common formulation
of PG estimates the gradient by Vθπ Gn = Eχ,a,∏ ∣^Vθπ log π(a∣x)A(x, a)], where A(x, a) is an
estimation of the advantage function Aπ(x, a) d=ef Qπ(x, a) - Vπ(x). Among many variants of PG
2
Under review as a conference paper at ICLR 2022
methods, we implemented our method on PPO (Schulman et al., 2017). At each gradient step, PPO
updates θ∏ to maximize clip(∏∏(aaxX) A, -e, e), where clip(x, -e, e) = max(-e, min(e, x)) and π0id
is the policy before being updated. This clipping heuristics prevents π from updating too rapidly.
Options Framework Options (Sutton et ai., 1999) provide a framework for formuiating temporaiiy
abstracted actions in RL. An option o ∈ O consists of a tupie (Io, βo, πo), where Io ⊆ X is the
initiation βo : X → [0, 1] is a termination function, and πo is an intra-option poiicy. Foiiowing
reiated studies (Bacon et ai., 2017; Harutyunyan et ai., 2019), we assume that Io = X focuses on
learning βo and ∏o. We let μ : X ×O → [0,1] denote a policy over options. A typical RL agent
sample At from ∏(∙∣Xt). Analogously, at time t, an RL agent with options first samples a termination
Tt from βOt (Xt). If Tt = 1, Ot+ι is sampled from μ(∙∣Xt) and if not, the current option remains
the same. The next action At is sampled from ∏Ot+1 (∙∣Xt). For option learning methods, we use ∏
to denote the resulting policy induced by μ, ∪o∈o∏o, and ∪o∈oβo.
Option value functions With options, we have three option-value functions QO, VO, and UO .
QO is the option-value function denoting the value of selecting an option o at state x defined
def
by QO(x, o) = E [G|X0 = x, O0 = o]. Similar to the relationship between Qπ and Vπ, we let
VO denote the marginalized option-value function VO(x) = P。μ(o∣x)Qo(x,o). UO(x, o) =
(1 - βo(x))QO(x, o) + βo(x)VO(x) is called the option-value function upon arrival (Sutton et al.,
1999) and denotes the value of reaching a state x with o and not having selected the new option. We
use these notations in Section 4.2.
Termination gradient theorem Analogously to p, we let Po : X × O × X → [0, 1] denote the
state transition probability induced by options. When an agent is at xs and having an option o, the
probability that o ends at xf is given by:
P o(χf |xs) = βo(χf )Iχf=χs + (1 - βo(χs)) X pπo (χ∣χs)Po(χf |x),	(1)
x
where pπo is the policy-induced transition function pπo (x0∣x) = Pa∈∕ ∏o(a∣x)P(x0∣x,a). Here
we assume that all options eventually terminate, such that Po is a valid probability distribution over
xf. Interestingly, Po is differentiable with respect to the parameter of βo. Harutyunyan et al. (2019)
introduced the termination gradient theorem:
Theorem 1. Let βo be parameterized by θg, and let '§。denote the logit of βo, i.e.,与。=
log( I-；%)). Wehave
VθβPo(χf |xs) = XPo(χ∣Xs)Vθβ'；o(X)(Ixf=X - Po(χf |x)).	(2)
x
We use this theorem to derive an unbiased estimator of our target gradient.
3	InfoMax Termination Critic
We now present the InfoMax Termination Critic (IMTC) algorithm. To learn diverse options, we
propose to maximize the following MI at each state xs :
I(Xf; O|xs) =H(Xf|xs) -H(Xf|xs,O) = H(O|xs) - H(O|Xf, xs),	(3)
where I denotes the MI I(A; B|c) = H(A|c) - H(A|B, c), H denotes entropy, and O is a random
variable denoting options experienced by an agent. Precisely, we let η denote the probability of having
an option o when leaving a state x: η(o∣x) = βo(x)μ(o∣x) + (1 — βo(x)) Pxo∈χpπo (x∣x0)η(o∣x0)
and let ηπ denote η marginalized over d∏: ηπ(o) = Px∈χ d∏(χ)η(o∣χ). Then, we define O as aran-
dom variable with ηπ. By decomposing this MI as I(Xf; O|xs) = H(Xf|xs) - H(Xf|xs, O),
we can interprete this MI maximization as maximization of H(Xf |xs) and minimization of
H(Xf |xs, O). Maximizing H(Xf|xs) diversifies possible destinations of an agent. Thus, the
resulting options are expected to be diverse in the sense that they likely lead to different destinations.
On the other hand, minimizing H(Xf |xs, O) makes option state transitions more deterministic,
leading to more meaningful options.
3
Under review as a conference paper at ICLR 2022
Figure 2: IMTC options (left) and TC options (right).
To build intuition, we searched options that maximize the MI (3) by a simple brute-force search
from a limited set of πo and βo in a 3 × 3 Gridworld environment. Specifically, we considered
only deterministic intra-option polciies and 0.1 or 0.8 as the value of βo(x). The number of options
is fixed to |O| = 3. In this environment, an agent can take four actions: Up, Down, Left, and
Right. The taken action fails with probability 0.1, and it takes a uniformly random action. Figure 2
shows the result. Arrows represent intra-option policies and colors of the room represent termination
probabilities. Here, we can see the tendency that IMTC prefers options that have almost separated
termination regions and different policies per option. We also visualized TC (Harutyunyan et al.,
2019) options that maximize -H (Xf |o). For TC options, we can see a similar tendency as IMTC
options, but options 0 and 1 of TC share the intra-option policies on the left side of the room. Thanks
to the diversity term H(Xf |xs), IMTC successfully avoids such a policy overlap. In addition, we can
see that IMTC has smaller termination regions than TC, which can enhance long-range abstraction.
We propose to maximize the MI (3) by updating βo via gradient ascent w.r.t. θβ . For this pur-
pose, we now derive an unbiased estimator of the gradient. First, we write the gradient of the
MI using the option transition model Po and marginalized option-transition model P(xf|xs) =
Po ηπ(o)Po(χf |xs).
Proposition 1. Let βo be parameterized using a sigmoid function. Given a trajectory τ =
xs,...,x,...,xf sampled by πo and βo, we can obtain unbiased estimations of Vθe H(Xf |x§)
and VθβH(Xf ∣Xs,O) by
Vθβ H(Xf |xs) = Ed∏,η [-Vθβ 'βo (x)β o(x)(log P(x|x§) - log P (Xf 山川	(4)
VθβH(Xf ∣Xs,O) = Ed∏,η [-Vθβ'β° (x)βo(x)(log Po(x∣Xs) - log Po(Xf E))] ,	(5)
where '§。(x) denotes the logit of βo(x).
Note that the additional term βo is necessary because X is not actually a terminating state. The proof
is based on Section 4 in Harutyunyan et al. (2019) and is given in Appendix A.1.
Now the targetting gradient can be written as:
VθβI(Xf; O∣Xs) = VθβH(Xf ∣Xs) - VθβH(Xf ∣Xs,O)
=Edn,η [-Vθβ'βo(x)βo(x) (logP(XIXs)- logP(Xf E) - (logPo(x∣Xs) - logPo(Xf E)))],
(6)
which means that we can approximate MI maximization by estimating Po and P. However, in
RL, learning a density model over the state space in RL is often difficult, especially with large or
continuous state spaces. For example, it has been tackled with data compression methods (Bellemare
et al., 2016) and generative models (Ostrovski et al., 2017). Hence, we reformulate the gradient using
Bayes’ rule to avoid estimating Po and P . The resulting term consists of the inverse option transition
function pO(o|Xs, Xf), which denotes the probability of having an o given a state transition Xs, Xf.
Lemma 1. We now have
VθβI(Xf; O∣Xs) = VθβH(Xf ∣Xs) - VθβH(Xf ∣Xs,O)
=Edn,η Re'βo(X)βo(X)(logpo(o∣Xs,X) - logPO(o∣Xs,Xf))] .	(7)
The proof is provided in Appendix A.2. Equation (7) requires estimating ofpO per updating πo and
βo, which is computational quite expensive. Thus, we approximate approximate the gradient (7) by
regressing a classification model over options PO(o∣Xs,Xf) on sampled option transition.
4
Under review as a conference paper at ICLR 2022
4 Implementation
Since IMTC can be combined with any on-policy RL methods, we choose PPO (Schulman et al., 2017)
as a base algorithm because of its stability and ease of implementation. As notable implementation
details, this section explains the estimation of pO and advantage estimation. We provide further
details and the full description of the algorithm in Appendix B.
4.1	ESTIMATING pO
To estimate PO,We employ a classification model over options PO(o|x§,xf) and regress it on
sampled option transitions, as per Gregor et al. (2016). However, our preliminary experiments
observed that this online regression could be unstable because the supply of transition data depends
on the termination probability and can drastically increase or decrease during training. To address
this problem, We maintain a replay buffer BO, Which stores option state transitions {(o, xs, xf)} to
stabilize the regression of PO.Note that using older option state transitions can introduce bias to
Po because it depends on the current policy. However, we found that this is not harmful when the
capacity of the replay buffer is reasonably small.
4.2	Advantage Estimation
The original PPO implementation employs GAE (Schulman et al., 2015b) for estimating the advantage,
which is important for learning performance (Andrychowicz et al., 2021). Therefore, we employed
two variants of GAE in experiments for option learning according to the experimental setup. In
the following paragraphs, we let N denote the rollout length used for advantage estimation and let
t + k denote the time step at which the current option ot terminates. Thus, we need to consider
the effect of option-switching in advantage estimation When k < N . Furthermore, we use two
variants of the option-specific TD errors δ(ot) = Rt + γQO(xt+1, ot) - QO(xt, ot) and δU(ot) =
Rt + γUO(xt+1, ot) - QO(xt, ot).
Independent GAE for Reward-Free RL For no-reward experiments with VIC, we used the
following variant of GAE:
min(k,N)
AoId = -QO(Xt, Ot) +	X (Yλ)iδ(ot+i)	(8)
i=0
Here, we ignore the future rewards produced by other options after the current option ot terminates.
This formulation enhances learning diverse intra-option policies per option.
Upgoing GAE for task adaptation For single-task learning, increasing the rollout step N often
speeds up learning (Sutton and Barto, 2018). However, future rewards after option termination heavily
depend on the selected option and have high variance, especially when learning diverse options. This
high variance of future rewards slows advantage learning and causes underestimation of Ao . Thus,
to prevent underestimation, we introduce an upgoing GAE (UGAE) for estimating advantage with
options:
Pk=0(Yλ)iδo+i + max ( X (γλ)iδ(ot+i),0)	(k < N)
AoPg = -QO (xt, Ot) +	、	∖i=k+1 .	U	(9)
UPgoing estimation
.pN-1(Yλ)iδO+i + (Yλ)N δU(Ot+Ν )	(otherwise).
Like the upgoing policy update (Vinyals et al., 2019), the idea is optimistic regarding future rewards
after option termination by taking the maximum over 0. We use AoPg for task adaptation experiments
in Section 5, and confirmed its effectivity in the ablation study in Appendix C.6.
5	Experiments
As presented in this section, we conducted two series of experiments to analyze the property of IMTC.
First, we qualitatively evaluated the diversity of options learned by IMTC with intrinsic rewards,
without any extrinsic rewards. Second, we quantitatively test the reusability of learned options by
5
Under review as a conference paper at ICLR 2022
IMTC + VIC
Option 0
IMTC + RVIC
Option 0
IMTC (R=0.01)
Expected value of Policy OC + VIC
2	：-「	pt on	pt on ι
—E peteda—ef P VIC (β=0 1)
Option 2	Option 3	Option 0	Option 1
—Expected value of Policy RVIC (β = 0.1)
Option 2	Option 3	Option 0	Option 1
0..
0.
0,
0.：
0,
1.
0.1
0.
0,
0.：
0,
1.
0.1
0.
0,
0.：
0,
∣,8
∣,6
(
∣,4
∣,2
∣,0
,0
∣,8
∣,6
(
∣,4
∣,2
∣,0
,0
∣,8
∣,6
(
∣,4
∣,2
∣,0
β(x)
1
β(x)
1
β(x)
1
Figure 3: Learned intra-option policies (πo) and termination probabilities (βo) for each option in
PointMaze after training 4 × 106 steps. Arrows show the expected value of action, and heatmaps show
probabilities of each βo . Left: Options learning by IMTC with three different intrinsic rewards. We
can see that intra-option policies have various directions, and termination regions are clearly separated
across options. To see the difference between intrinsic rewards, RVIC and constant rewards have
an overlapped pair of options (option 1 and 3). Also, we can see that the magnitude of intra-option
policies tends larger with constant rewards. Right: Options learned by other methods. OC produces
a dead option 3 that terminates everywhere and never-ending options 0 and 1. About intra-option
policies, all methods successfully avoided learning the same policy, but they only have two directions.
task adaptation on a specific task. As a baseline of termination learning method, we compared
our method with OC (Bacon et al., 2017). OC is trained with VIC (Gregor et al., 2016) rewards
during pre-training. We did not compare IMTC with TC (Harutyunyan et al., 2019) because our TC
implementation failed to learn options with relatively small termination regions as reported in the
paper, and there is no official public code for TC1. During pre-training without extrinsic rewards,
IMTC receives intrinsic rewards when the current option terminates. We compare three IMTC
variants with different intrinsic rewards: (i) VIC (Gregor et al., 2016), (ii) RVIC (Baumli et al., 2021),
and (iii) constant value (RIMTC = 0.01). Note that RIMTC = 0.01 is chosen from [0.1, 0.05, 0.01]
based on the task adaptation results. We also compare IMTC with vanilla VIC and RVIC with fixed
termination probabilities. We used ∀xβo(x) = 0.1 since it performed the best in task adaptation
experiments, while 0.05 was used in Gregor et al. (2016). Note that RVIC’s objective I(Xs; O|xf) is
different from ours, while IMTC and VIC share almost the same objective. Thus, the use of VIC is
more natural, and the combination with RVIC is tested to show the applicability of IMTC. Further
details of our VIC and RVIC implementation are found in Appendix B. In order to check only the
effect of the different methods for learning beta βo, the rest of the implementation is the same for all
these methods. That is, OC, vanilla VIC, and vanilla RVIC are also based on PPO and advantage
estimation methods in Section 4.2. In this section, we fix the number of options as |O| = 4 for all
option-learning methods. We further investigated the effect of the number of options Appendix C,
where we confirmed that |O| = 4 is sufficient for most domains. All environments that we used for
experiments are implemented on the MuJoCo (Todorov et al., 2012) physics simulator. We further
describe the details in Appendix C.
Option Learning From Intrinsic Rewards We now qualitatively compare the options learned
by IMTC with options of other methods. Learned options depend on the reward structure in the
environment, which enables manually designing good reward functions for learning diverse options.
Thus, we employed a reward-free RL setting where no reward is given to agents. Instead, each
compared method uses some intrinsic rewards, as explained. We fix μ as μ(o∣χ)= 吉 in this
experiment, since we assume that the future tasks are uniformly distributed. Intra-option policies are
trained by PPO (Schulman et al., 2017) and independent GAE (8). We show network architectures
and hyperparameters in Appendix C. We set the episode length to 1 × 104, i.e., an agent is reset
to its starting position after 1 × 104 steps. For all visualizations, we chose the best one from five
independent runs with different random seeds.
1 Note that we also could not find any unofficial open source implementation.
6
Under review as a conference paper at ICLR 2022
(a) PointReach
(e) AntBilliard
(b) SwimmerReach (c) AntReach (d) PointBilliard
(f) PointPush (g) AntPush
Figure 4: Domains used in task adaptation experiments.
We visualized learned options in PointReach environment shown in Figure 4a. In this environment,
an agent controls the ball initially placed at the center of the room. The state space consists of positions
(x, y) and velocities (∆x, ∆y) of an agent, and the action space consists of acceralations (∆χ, ∆t).
Figure 3 shows the options learned in this environment after 4 × 106 steps. Each arrow represents the
mean value of intra-option policies, and the heatmaps represent βo . In this experiment, we observed
the effect of IMTC clearly, for both termination regions and intra-option policies. Interestingly, we
don’t see clear differences between options learned with VIC and RVIC rewards, while constant
rewards tend to make options peaker. OC failed to learn meaningful termination regions: option 0
and 1 never terminate, and option 3 terminates almost everywhere. This result confirms that IMTC
can certainly diversify options. Moreover, compared to vanilla VIC and RVIC, intra-option policies
learned by IMTC with VIC or RVIC rewards are clearer, in terms of both the magnitude and directions
of policies. We believe that this is because diversifying termination regions gives more biased samples
to the option classifiers employed by VIC and RVIC. Figure 1 also show options learned by IMTC
and OC in PointBilliard domain, where we can see the same tendency.
Transferring skills via task adaptation Now we quantitatively test the reusability of learned
options by task adaptation with specific reward functions. Specifically, we first trained agents with
intrinsic rewards as per the previous section. Then we transferred agents to an environment with the
same state and action space but with external rewards. We prepared multiple reward functions, which
we call tasks, for each domain and evaluated the averaged performance over tasks. We compare IMTC
with OC, vanilla VIC, vanilla RVIC, and PPO without pre-training. Also, we compare three variants
of IMTC with different intrinsic rewards during pre-training. For a fair comparison, UGAE (9) and
PPO are used for all options learning methods. Note that we found UGAE is very effective in this
experiments, as we show the ablation study in Appendix C.6. For vanilla VIC and vanilla RVIC,
termination probability is fixed to 0.1 through pre-training and task adaptation. -greedy based on
QO with e = 0.1 is used as the option selection policy μ. We hypothesize that diverse options learned
by IMTC can help quickly adapt to given tasks, supposing the diversity of tasks.
Figure 4 shows all domains used for task adaptation experiments. For simplicity, all tasks have goal-
based sparse reward functions. I.e., an agent receives Rt = 1.0 when it satisfies a goal condition, and
otherwise the control cost -0.0001 is given. Red circles show possible goal locations for each task.
When the agent fails to reach the goal after 1000 steps, it is reset to a starting position. PointReach,
SwimmerReach, and AntReach are simple navigation tasks where an agent aim to just navigate
itself to the goal. We also prepared tasks with object manipulation: in PointBilliard and
AntBilliard an agent aims to kick the blue ball to the goal position, and in PointPush and
AntPush, it has to push the block out of the way to the goal. We pre-traine options learning
agents for 4 × 106 environmental steps and additionally trained them for 1 × 106 steps for each
task. Figure 5 shows learning curves and scatter plots drawn from five independent runs with
7
Under review as a conference paper at ICLR 2022
PoIntReach
Ol 0.4 0β 0» U
Environmental Steps (Ioe)	1*
AntReach	PoIntBIIIIard
*	14
1J)
uβ
03 OA oβ o»
Environmental Steps (10*)
o.2 oλ oβ oβ i∙α
Environmental Steps (Ioe) 1*
03 OA 0β 0»	14
Environmental Steps (10β) Iee
o-ι
on
-0-1
PoIntPush
AntPush
Method
IMTC + VIC
Ol OA 0β 0»	14
Environmental Steps (Ioe) 1*
-o-ι —
03 OA 0β 0»	14
Environmental Steps (Ioe) 1∙*
IMTC + RVIC
IMΓC(R=0.01)
OC + VIC
n VIC(β=0.1)
.RVIC(P=O l)
PPO
Figure 5: Learning curves for transfer learning experiments.
different random seeds per domain. 2 Here, we observed that IMTC with VIC or RVIC rewards
performed the best or was compatible with baselines. IMTC with VIC performed better than OC
with VIC except for AntRearch, which backs up the effectiveness of diversifying termination
regions for learning reusable options. Also, IMTC with VIC and IMTC with RCIC respectively
performed better in most of the tasks than VIC and RVIC with fixed termination probabilities.
This result suggests that IMTC can boost the performance of option learning methods based on
option classifiers, even when the objective is different as with RVIC. On the other hand, IMTC
with constant rewards (RIMTC = 0.01) performed worse than IMTC with VIC or RVIC rewards,
although it also learned diverse options as we show in Figure 3, suggesting the importance of adjusting
rewards. We further analyzed the evolution of intrinsic rewards of VIC and RVIC in Appendix C.5.
In addition, we can observe that IMTC’s performance is especially better than other methods in
relatively complex PointBilliard, AntBilliard, and AntPush, where object manipulation
is required. Considering that manipulated balls and boxes move faster than agents in these domains, a
choice of options can lead to larger differences in the future state. IMTC is suitable to these domains
since it maximizes the diversity of the resulting states, while PPO struggles to learn. Contrary,
IMTC’s performance is close to other methods in Reach tasks, where the goal states are relatively
close to the starting states in terms of euclidian distances.
Gridworld experiments and limitation of the method Although IMTC successfully learned
diverse options in MuJoCo experiments, our analysis in Figure 2 shows the possibility of learning
options that are not interesting but have large MI. We further investigated this possibility by visualizing
options in a classical four rooms gridworld in Appendix C.8. Interestingly, we observed that IMTC
could fall into diverse but unmeaningful options in that environment. We believe that IMTC is often
sufficient in a large environment where a randomly-initialized agent rarely gets the same trajectory.
However, when the number of possible trajectories is small, diversifying the destinations could be
insufficient. In such cases, it can be necessary to extend IMTC to diversify trajectories as done in
Sharma et al. (2020).
6	Related Work
Discovering diverse options Since Sutton et al. (1999) first formulated the options, discovering
good options has been challenging. A classic concept for the goodness of options is a bottleneck
region (McGovern and Barto, 2001), which refers to important states for reaching diverse areas,
such as a passage between two rooms. Once an important region is discovered, we can construct an
option that guides an agent to that area. Various approaches have been proposed to define bottleneck
regions concisely and compute bottleneck options efficiently, including betweenness (Simsek and
Barto, 2008), Eigen options (Machado et al., 2017), successor options (Ramesh et al., 2019), and
covering options (Jinnai et al., 2019). Barto et al. (2013) discussed the importance of diverse options
for exploration, referring to an option construction method with a graph-based decomposition of
an MDP Vigorito and Barto (2010). We share the same motivation with these methods but have a
2We used seaborn (Waskom, 2021)’s lmplot with order=3 is used to draw learning curves.
8
Under review as a conference paper at ICLR 2022
different approach. While these methods construct point options that bridge two states, we capture a
set of states by learning βo directly, making it easy to scale up with function approximation. To our
knowledge, (Jinnai et al., 2020) only succeeded in scaling up point options to continuous options
using approximated computation of the Laplacian.
End-to-end learning of options As described in the previous paragraph, many studies have at-
tempted to construct options and then train intra-option policies. However, motivated by the recent
success of RL with DNN, Bacon et al. (2017) proposed OC to train intra-option policies and termi-
nation functions in parallel using a neural network as a function approximator. OC updates βo by
gradient ascent, so that maximize QO . Thus, the resulting terminating regions heavily depend on
the reward function. OC also proposed a PG-style method for learning intra-option policies. We
used a similar method to OC for learning policies and values, but we employed a different method
that does not depend on rewards for learning termination functions. Also, we proposed UGAE (9)
for enhancing OC-style policy learning. While OC maximizes the option-value function directly,
many heuristic objectives have been proposed with similar architectures, including deliberation cost
(Harb et al., 2018), interest (Khetarpal et al., 2020), and safety (Jain et al., 2018). Kamat and Precup
(2020) extended OC so that resulting options are diverse by maximizing the divergence between
intra-option policies, while IMTC considers the diversity of destinations. Notably, Harutyunyan
et al. (2019) proposed the termination critic (TC) that maximizes an information-theoretic objective
referred to as predictability -H (Xf |o). Our method is inspired by TC and maximizes a similar
information-theoretic objective for diversity rather than predictability. In addition, TC requires
estimating Po(xf|xs) and a marginal distribution of Po, which can be quite difficult in environments
with large or continuous state spaces. We avoid such difficult approximations using Bayes’ rule,
making IMTC more scalable.
Mutual Information, Empowerment, and Skill Acquisition MI also appears in the literature
regarding intrinsically motivated RL (Singh et al., 2004), as a driver of goal-directed behavior.
A well-known example is the empowerment (Klyubin et al., 2005; Salge et al., 2014), obtained
by maximizing MI between sequential k actions and the resulting state I(at, ..., at+k; xt+k|xt) =
H(xt+k|xt) - H(xt+k|at, ..., at+k, xt). Empowerment represents both large degree of freedom
and good preparedness: i.e., larger H(xt+k|xt) implies that there can be more diverse future states,
while smaller H(xt+k|at, ..., at+k, xt) indicates that an agent can realize its intention with greater
certainty. In RL literature, empowerment is often implemented by maximizing the variational lower
bounds as intrinsic rewards (Mohamed and Rezende, 2015; Zhao et al., 2020). We can interpret our
objective I(Xf; O|xs) as a variant of empowerment where a fixed number of options represents action
sequences. Gregor et al. (2016) also employed this interpretation and introduced VIC for training
intra-option policies with fixed termination probabilities in a no reward RL setting. Our experiments
observed that IMTC helps VIC to learn meaningful intra-option policies. As a variant of VIC, Baumli
et al. (2021) proposed to use the reversed MI I(Xs; O|xf). MI has been used for discovering diverse
skills without termination functions. Eysenbach et al. (2019) proposed maximizing MI between skills
and states I(O; X), and Sharma et al. (2020) extended the objective to a conditional MI I(O; X0|x).
These methods also maximize variational lower bounds of MI as rewards.
7	Conclusion
In this paper, we considered the problem of learning diverse options in RL. To learn diverse ter-
mination regions of options in a scalable way, we proposed to maximize MI between options and
terminating states per starting state. We derived an unbiased gradient estimator to approximately
maximize this MI, yielding the InfoMax Termination Critic (IMTC) algorithm. Also, we proposed a
practical implementation of IMTC with enhanced advantage estimation in Section 4.2. In reward-free
experiments, we visualized that IMTC helped learn diverse and clear options combined with intrinsic
rewards. We also showed that options learned by IMTC help an agent to quickly adapt to a specific
reward function by transferring learned options. Although our experiments observed that IMTC can
learn clear and meaningful options, a potential problem is that learning of a classification model PO
heavily depends on exploration. For example, an agent cannot explore a room well, it would not be
able to learn sufficiently diverse options. The relationship between diversity and exploration would be
interesting. Also, our analysis in Figure 2 and Gridworld experiments in Appendix C.8 suggest that
IMTC options can fall into small loops, forming uninteresting options. To prevent this, considering
distances between states, e.g., by using bisimulation metric (Castro and Precup, 2010), is a plausible
research direction.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement
We publish anonymized source code used for all our experiments on https://
anonymous.4open.science/r/imtc-anonymized-code-E5D1/.
References
M. Andrychowicz, A. Raichuk, P. Stanczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist,
O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters for on-policy deep actor-
critic methods? a large-scale study. In 9th International Conference on Learning Representa-
tions, ICLR 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=
nIAxjsniDzg&.
P. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the Thirty-
First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California,
USA, pages 1726-1734, 2017. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/
paper/view/14858.
A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discret. Event
Dyn. Syst., 13(1-2):41-77, 2003. doi: 10.1023/A:1022140919877. URL https://doi.org/
10.1023/A:1022140919877.
A.	G. Barto, S. Singh, N. Chentanez, et al. Intrinsically motivated learning of hierarchical collections
of skills. In Proceedings of the 3rd International Conference on Development and Learning, pages
112-19. Piscataway, NJ, 2004.
A. G. Barto, G. D. Konidaris, and C. M. Vigorito. Behavioral hierarchy: Exploration and representa-
tion. In G. Baldassarre and M. Mirolli, editors, Computational and Robotic Models of the Hierarchi-
cal Organization of Behavior, pages 13-46. Springer, 2013. doi: 10.1007/978-3-642-39875-9\_2.
URL https://doi.org/10.1007/978-3- 642-39875-9_2.
K. Baumli, D. Warde-Farley, S. Hansen, and p Volodymyr Mnih. Relative variational intrinsic control.
In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, February 2-9, 2021,
2021.
M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-
based exploration and intrinsic motivation. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon,
and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages
1471-1479, 2016. URL http://papers.nips.cc/paper/6383-unifying-count-
based-exploration-and-intrinsic-motivation.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai
gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/abs/1606.01540.
E. Brunskill and L. Li. Pac-inspired option discovery in lifelong reinforcement learning. In Pro-
ceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China,
21-26 June 2014, volume 32 of JMLR Workshop and Conference Proceedings, pages 316-324.
JMLR.org, 2014. URL http://proceedings.mlr.press/v32/brunskill14.html.
P. S. Castro and D. Precup. Using bisimulation for policy transfer in mdps. In M. Fox and D. Poole,
editors, Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010,
Atlanta, Georgia, USA, July 11-15, 2010. AAAI Press, 2010. URL http://www.aaai.org/
ocs/index.php/AAAI/AAAI10/paper/view/1907.
J. Choi, A. Sharma, H. Lee, S. Levine, and S. S. Gu. Variational empowerment as representation
learning for goal-conditioned reinforcement learning. In M. Meila and T. Zhang, editors, Pro-
ceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 1953-1963.
PMLR, 2021. URL http://proceedings.mlr.press/v139/choi21b.html.
10
Under review as a conference paper at ICLR 2022
R. Coulom. Reinforcement Learning Using Neural Networks, with Applications to Motor Control.
(APPrentissage par renforcement Utilisant des reseaux de neurones, avec des applications au
contr∂le moteur). PhD thesis, Grenoble Institute of Technology, France, 2002. URL https：
//tel.archives- ouvertes.fr/tel- 00003985.
Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement
learning for continuous control. In M. Balcan and K. Q. Weinberger, editors, Proceedings of
the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA,
June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 1329-1338.
JMLR.org, 2016. URL http://proceedings.mlr.press/v48/duan16.html.
B.	Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a
reward function. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019, 2019. URL https://openreview.net/forum?id=
SJx63jRqFm.
A. Geramifard, C. Dann, R. H. Klein, W. Dabney, and J. P. How. Rlpy: a value-function-based
reinforcement learning framework for education and research. J. Mach. Learn. Res., 16:1573-1578,
2015. URL http://dl.acm.org/citation.cfm?id=2886799.
K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. CoRR, abs/1611.07507,
2016. URL http://arxiv.org/abs/1611.07507.
J. Harb, P. Bacon, M. Klissarov, and D. Precup. When waiting is not an option: Learning options
with a deliberation cost. In Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18),
and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18),
New Orleans, Louisiana, USA, February 2-7, 2018, pages 3165-3172, 2018. URL https:
//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17421.
A. Harutyunyan, W. Dabney, D. Borsa, N. Heess, R. Munos, and D. Precup. The termination
critic. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS
2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 2231-2240, 2019. URL http://
proceedings.mlr.press/v89/harutyunyan19a.html.
N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, S. M. A.
Eslami, M. A. Riedmiller, and D. Silver. Emergence of locomotion behaviours in rich environments.
CoRR, abs/1707.02286, 2017. URL http://arxiv.org/abs/1707.02286.
A. Jain, K. Khetarpal, and D. Precup. Safe option-critic: Learning safety in the option-critic
architecture. CoRR, abs/1807.08060, 2018. URL http://arxiv.org/abs/1807.08060.
C.	Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement learn-
ing. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 4870-
4879. PMLR, 2020. URL http://proceedings.mlr.press/v119/jin20d.html.
Y. Jinnai, J. W. Park, D. Abel, and G. D. Konidaris. Discovering options for exploration by minimizing
cover time. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pages 3130-3139. PMLR, 2019. URL
http://proceedings.mlr.press/v97/jinnai19b.html.
Y. Jinnai, J. W. Park, M. C. Machado, and G. D. Konidaris. Exploration in reinforcement learning
with deep covering options. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://
openreview.net/forum?id=SkeIyaVtwB.
A. Kamat and D. Precup. Diversity-enriched option-critic. CoRR, abs/2011.02565, 2020. URL
https://arxiv.org/abs/2011.02565.
11
Under review as a conference paper at ICLR 2022
K. Khetarpal, M. Klissarov, M. Chevalier-Boisvert, P. Bacon, and D. Precup. Options of interest:
Temporal abstraction with interest functions. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Con-
ference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI2020, New York, NY USA, February 7-12, 2020, pages 4444-4451. AAAI Press, 2020. URL
https://aaai.org/ojs/index.php/AAAI/article/view/5871.
D.	P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun,
editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,
USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/
1412.6980.
A. S. Klyubin, D. Polani, and C. L. Nehaniv. All else being equal be empowered. In M. S.
Capcarrere, A. A. Freitas, P J. Bentley, C. G. Johnson, and J. Timmis, editors, Advances in
Artificial Life, 8th European Conference, ECAL 2005, Canterbury, UK, September 5-9, 2005,
Proceedings, volume 3630 of Lecture Notes in Computer Science, pages 744-753. Springer, 2005.
doi: 10.1007/11553090\_75. URL https://doi.org/10.1007/11553090_75.
M. C. Machado, M. G. Bellemare, and M. H. Bowling. A laplacian framework for option discovery
in reinforcement learning. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 2295-2304, 2017. URL
http://proceedings.mlr.press/v70/machado17a.html.
A. McGovern and A. G. Barto. Automatic discovery of subgoals in reinforcement learning using
diverse density. In C. E. Brodley and A. P. Danyluk, editors, Proceedings of the Eighteenth
International Conference on Machine Learning (ICML 2001), Williams College, Williamstown,
MA, USA, June 28 - July 1, 2001, pages 361-368. Morgan Kaufmann, 2001.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In Proceedings of the 33nd Interna-
tional Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24,
2016, pages 1928-1937, 2016. URL http://jmlr.org/proceedings/papers/v48/
mniha16.html.
S. Mohamed and D. J. Rezende. Variational information maximisation for intrinsically motivated rein-
forcement learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information
Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2125-2133,
2015. URL http://papers.nips.cc/paper/5668- variational- information-
maximisation-for-intrinsically-motivated-reinforcement-learning.
O. Nachum, S. Gu, H. Lee, and S. Levine. Data-efficient hierarchical reinforcement learning.
In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal,
Canada, pages 3307-3317, 2018. URL https://proceedings.neurips.cc/paper/
2018/hash/e6384711491713d29bc63fc5eeb5ba4f- Abstract.html.
OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino,
M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan,
W. Zaremba, and L. Zhang. Solving rubik’s cube with a robot hand. CoRR, abs/1910.07113, 2019.
URL http://arxiv.org/abs/1910.07113.
G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos. Count-based exploration with
neural density models. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, vol-
ume 70 of Proceedings of Machine Learning Research, pages 2721-2730. PMLR, 2017. URL
http://proceedings.mlr.press/v70/ostrovski17a.html.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chil-
amkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style,
12
Under review as a conference paper at ICLR 2022
high-performance deep learning library. In H. M. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alch6-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems 32: Annual Conference on Neural Information Processing Sys-
tems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 8024-8035,
2019. URL http://papers.nips.cc/paper/9015-pytorch-an-imperative-
style-high-performance-deep-learning-library.
R. Ramesh, M. Tomar, and B. Ravindran. Successor options: An option discovery framework for
reinforcement learning. In S. Kraus, editor, Proceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 3304-
3310. ijcai.org, 2019. doi: 10.24963/ijcai.2019/458. URL https://doi.org/10.24963/
ijcai.2019/458.
C. Salge, C. Glackin, and D. Polani. Empowerment-An Introduction, pages 67-114. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2014. ISBN 978-3-642-53734-9. doi: 10.1007/978-3-642-53734-
9_4. URL https://doi.org/10.1007/978-3-642-53734-9_4.
A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning
in deep linear neural networks. In Y. Bengio and Y. LeCun, editors, 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
Proceedings, 2014. URL http://arxiv.org/abs/1312.6120.
J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In
F. R. Bach and D. M. Blei, editors, Proceedings of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference
Proceedings, pages 1889-1897. JMLR.org, 2015a. URL http://proceedings.mlr.press/
v37/schulman15.html.
J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous
control using generalized advantage estimation. CoRR, abs/1506.02438, 2015b. URL http:
//arxiv.org/abs/1506.02438.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.
A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised discovery
of skills. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/
forum?id=HJgLZR4KvH.
O. Simsek and A. G. Barto. Skill characterization based on betweenness. In D. Koller, D. Schu-
urmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Sys-
tems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Process-
ing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008, pages 1497-1504.
Curran Associates, Inc., 2008. URL http://papers.nips.cc/paper/3411-skill-
characterization- based- on- betweenness.
S. P. Singh, A. G. Barto, and N. Chentanez. Intrinsically motivated reinforcement learning. In
Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems,
NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pages 1281-1288,
2004. URL http://papers.nips.cc/paper/2552- intrinsically-motivated-
reinforcement-learning.
A. Solway, C. Diuk, N.C6rdova, D. Yee, A. G. Barto, Y. Niv, and M. Botvinick. Optimal behavioral
hierarchy. PLoS Computational Biology, 10(8), 2014. doi: 10.1371/journal.pcbi.1003779. URL
https://doi.org/10.1371/journal.pcbi.1003779.
R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 2018.
R. S. Sutton, D. Precup, and S. P. Singh. Between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artif. Intell., 112(1-2):181-211, 1999. doi: 10.1016/S0004-
3702(99)00052-1. URLhttps://doi.org/10.1016/S0004-3702(99)00052-1.
13
Under review as a conference paper at ICLR 2022
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vil-
amoura, Algarve, Portugal, OCtober 7-12, 2012, pages 5026-5033. IEEE, 2012. doi: 10.1109/
IROS.2012.6386109. URL https://doi.org/10.1109/IROS.2012.6386109.
C. M. Vigorito and A. G. Barto. Intrinsically motivated hierarchical skill learning in structured environ-
ments. IEEE Trans. Auton. Ment. Dev., 2(2):132-143, 2010. doi: 10.1109/TAMD.2010.2050205.
URL https://doi.org/10.1109/TAMD.2010.2050205.
O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai,
J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden,
Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama,
D. Wunsch, K. McKinney, O. Smith, T. SchaUL T. Lillicrap, K. KavUkcUoglu, D. Hassabis,
C. Apps, and D. Silver. Grandmaster level in StarCraft II using multi-agent reinforcement
learning. Nature, 575(7782):350-354, 2019. doi: 10.1038/s41586-019-1724-z. URL https:
//doi.org/10.1038/s41586-019-1724-z.
M. L. Waskom. seaborn: statistical data visUalization. Journal of Open SourCe Software, 6(60):3021,
2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. MaCh. Learn., 8:229-256, 1992. doi: 10.1007/BF00992696. URL https://doi.org/
10.1007/BF00992696.
Y. WU, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba. Scalable trUst-region method for deep
reinforcement learning Using kronecker-factored approximation. In I. GUyon, U. von LUxbUrg,
S. Bengio, H. M. Wallach, R. FergUs, S. V. N. Vishwanathan, and R. Garnett, editors, AdvanCes
in Neural Information ProCessing Systems 30: Annual ConferenCe on Neural Information
ProCessing Systems 2017, 4-9 DeCember 2017, Long BeaCh, CA, USA, pages 5279-5288,
2017. URL http://papers.nips.cc/paper/7112- scalable- trust- region-
method-for-deep-reinforcement-learning-using-kronecker-factored-
approximation.
R. Zhao, P. Abbeel, and S. Tiomkin. Efficient online estimation of empowerment for reinforcement
learning. CoRR, abs/2007.07356, 2020. URL https://arxiv.org/abs/2007.07356.
14
Under review as a conference paper at ICLR 2022
A Omitted Proofs
A.1 Proof of Proposition 1
First, we make an assumption on the dependence of η from β.
Assumption 1. The empirical distribution ofoptions n(o|x§) is independent oftermination conditions
∪o∈Oβo.
This assumption does not strictly hold, and we can consider using, e.g., two-time scale optimization
to suppress the distribution shift of η by updating βo . We employed a replay buffer to mitigate this
issue in Section 4.1.
Lemma 2. The following equations hold.
VθβH(Xf |xs) = - Xη(o∣Xs) Xpo(x∣Xs)Vθβ'βo(x)
hlogP(x|xs) + 1 - XPo(xf|x)logP(xf|xs) + 1i
xf
(10)
Vθβ H(Xf ∣χs,θ) = - X η(o∣χs) XP o(χ∣χs)Vθβ 'e。(x)
hlogPo(x|xs) + 1 - XPo(xf|x)logPo(xf|xs) + 1i
xf
(11)
Then, sampling x, xf from dπ and o from η,
VθβH(Xf |xs)= Ed∏,η [-Vθβ'β°(x)βo(x)(log P(x|Xs)- logP(Xf 区川	(12)
Vθβ H (Xf ∣xs,O) = Edn ,η [-Vθβ 与。(x)βo(x)(log P o(x∣xs) - log P o(xf |xs))] .	(13)
Proof of Lemma 2
Proof. First, we prove Equation (10). We have:
VθβH(XfIXs) = -VθβXP(XfIXs)logP(XfIXs)
xf
V©e P (Xf IXs)
P(Xf IXs)
—
X 'θβP(Xf ∣Xs)logP(Xf ∣Xs) + P(Xf ∣Xs)
xf
-XVθβP(XfIXs)logP(XfIXs) + 1
xf
-XX n(o|xs) Vθβpo (Xf |xs) (log P(Xf IXs) +1)
^^^^^^{l^^^^^^~
Apply eq. (2)
xf o
-	η(oIXs)EPo(X∣Xs)Vθβ'e。(X)(Ixf=X - Po(Xf ∣X))(logP(Xf ∣Xs) + 1
xf o
x
一 Xη(o∣Xs) XPo(X∣Xs)Vθβ'βo(x) X(Ixf=X 一 Po(Xf ∣X))(logP(Xf ∣Xs) + 1)
o
x
xf
一Xη(o∣Xs)XPo(X∣Xs) Vθβ'βo(x) × [logP(XIXs) + 1 - XPo(Xf |x) (logP(Xf ∣Xs) + 1)].
ox
、--{-'、--{--}
sample sample
xf
'------{-------}
sample
15
Under review as a conference paper at ICLR 2022
Sampling X, Xf, o, we get (12). Then we prove Equation (11).
VθβH(Xf ∣χs,O) = -Vθβ Xη(ο∣χs) XPo(χf ∣χs)logPo(χf ∣χs)
o
xf
-Xη(o∣χs) X (^VθβPo(χf ∣χs)logPo(Xf |xs) + Po(χf |xs)
xf
V&eP O(XfIXs)
P o(χf ∣χs)
—X n(o|Xs) X Vθβpo (Xf |xs) (log Po(Xf IXs) +1)
xf
^^^^^^{l^^^^^^~
Apply eq. (2)
一 Eη(ο∣χs) £ £ Po(χ∣χs)Vθβ'βο (χ)(Ixf=x 一 Po(χf ∣χ))(log Po(Xf ∣χs) +1
xf x
一 Xη(o∣Xs) XPo(X∣Xs)Vθβ'βο(x) X(Ixf=X 一 Po(Xf ∣X))(logPo(Xf ∣Xs) + 1)
xf
一Xη(ο∣χs)XPo(χ∣χs) Vθβ'βο(χ) × [logPo(χ∣χs) + 1 - XPo(χf ∣χ) (logPo(χf ∣χs) + 1)]
ox
、--{-'、--{--}
sample sample
xf
'------{------}
sample
Sampling X, Xf, o, we get eq. (13).
A.2 Proof of Lemma 1
Proof. First, using Bayes’ rule, we have:
Po(Xf|Xs)
Pr(o|Xf, Xs) Pr(Xf |Xs)	pO (o|Xf, Xs)P(Xf |Xs)
Pr(o|Xs)
η(ο∣χs)
Then, we have:
logPo(Xf |Xs) - log P (Xf |Xs) = log
Po(Xf|Xs)
log
log
P(χf Iχs)
PO(o∣xs,Xf )P (xf |xs)
______η(OIxs)
P(χf Iχs)
PO(θ∣Xs,Xf )
η(ο∣χs)
o
o
o
o
x
□
Applying this equation to the eq. (6), we have:
VθβI(Xf; O∣χs) = VθβH(Xf ∣χs) - VθβH(Xf ∣χs,O)
Edπ,η
Edπ ,η
Edπ,η
Edπ,η
'Vθβ 'βο (χ)βο(χ)(log P (χ∣χs) - log P (χf ∣χs) - log P o(χ∣χs) + log Po(Xf ∣χs))]
[Vθβ'β°(χ)((logPo(χ∣χs) - log P(XIXs)) — (logPo(χf ∣χs) - logP(Xf IXs)))]
pO(oIχs, χ)	pO(oIχs, χf)
V'e'βo(X)(log IHXT - log	η(ο∣χs))]
[Vθβ 'βο(x)(log PO(θ∣xs,x) - log PO(θ∣Xs,Xf ))]
□
B Implementation Details
Clipped β loss Common PPO implementation updates πθ multiple times. However, our preliminary
experiments observed that performing multiple updates for βo led to destructively large updates and
16
Under review as a conference paper at ICLR 2022
Algorithm 1 InfoMax Termination Critic with VIC, PPO Style
1	: Given: Initial option-value QO , option-policy πo, and termination function βo .
2	: Let BO be a replay buffer for storing option-transitions.
3	: for k = 1, ... do
4	:	for i = 1, 2, ..., N do	. Collect experiences from environment
5	:	Sample termination variable bi from βoi (xi)
6	:	if bi = 1 then
7	:	Store (xs, xf, oi), (xs+1,xf, oi), ..., (xs+h,xf, oi) to the replay buffer BO
8	:	end if
9	:	Choose next option oi+1 by e-Greedy
10	Receive reward R and state xi+i, taking ai 〜∏oi+ι (Xi)
11	:	end for
12	:	for k = 1, 2, ..., Num. of PPO epochs do	. Optimize πo, QO, and βo
13	:	for all xi in the trajectory do
14	:	Compute RVIC from the target network
15	:	Compute Aiond by (8) using RVIC
16	Update πo(αi∣Xi) via PPO using Ao
17	Update QO (xi, o) to regress to Ao
18	:	if oi has already terminated then
19	:	Update βo(xi) via (7)
20	:	end if
21	:	end for
22	:	end for
23	Train P and μ by option-transitions sampled from BO
24	:	if k mod KVIC then
25	:	Update the target network for VIC
26	:	end if
27	: end for
resulted in the saturation of βo to zero or one. Hence, to perform PPO-style multiple updates, we
introduce a clipped objective of eq. (7):
LCLIP(θβ) = clip('βo(x) -'βoold(x),-eg,eβ)βodd(x)(logPO(o∣Xs,x) - logPO(o∣Xs,Xf)),
(14)
where eβ is a small coefficient, and βoold is an old βo before updating. We also add maximization of
the entropy of βo for preventing the termination probability saturating on zero or one. To this end,
we maximize LCLIP (θβ) + cHβH(βo(x)) via gradient ascent w.r.t. θβ, where cHβ is a weight of the
entropy bonus.
Full description of the algorithm Algorithm 1 shows a full description of our implementation
of IMTC on PPO when combined with VIC rewards. As of the original PPO, it is built on the
A2C-style (Mnih et al., 2016; Wu et al., 2017) architecture with multiple synchronous actors and
a single learner. First, we collect N -step experiences interacting with environments. At line 7, we
append tuples corresponding to option transitions (xs, xf, oi), ..., (xs+h, xf, oi) to BO. Here, we do
not use all transitions and store first hmax options to prevent memory shortage. We used hmax = 10 or
hmax = 20. Then we update πo, QO, and βo. Line 13 is done via minibatch sampling in the actual
implementation. We also update pO for estimating the gradient (7), sampling from the replay buffer
BO . We empirically found that rapidly changing RVIC leads to unstable learning, especially when
IMTC is used in parallel. Thus, we employ a target network to compute RVIC and periodically update
it at line 26. We used KVIC = 20 in the experiments.
Our implementation of VIC and RVIC In experiments, we show that IMTC helps an agent
learn diverse options without reward signals. For this purpose, we employ VIC (Gregor et al.,
2016) and RVIC (Baumli et al., 2021) for providing intrinsic rewards. Here we explain our
VIC and RVIC implementation. VIC updates intra-option policies to maximize the lower
bound of MI (3) H(O|xs) - H(O|xs, Xf) ≥ H(O|xs) + Eo,xf [log q(o|xs, xf)] as rewards,
17
Under review as a conference paper at ICLR 2022
Figure 6: Neural Network architecture used for the Gridworld experiments (top) and continuous
control tasks (bottom).
where q can be any distribution and called an option inference model. We use PO as q. We
also learn η(o∣xs) from sampled option transitions and approximate H(O∣Xs) by H(O∣Xs) =
-∑2o∈o η(o∣Xs) logη(o∣Xs) = -E [logη(o∣Xs)] ≈ logη(o∣Xs). To this end, we giving an agent
RVIC = CVIC (logPO(o∣Xs,xf) — logη(o∣Xs)) as an immediate reward when an option o terminates,
where cVIC is a scaling coefficient. This is different from the original VIC implementation where
H (O|xs) is treated as a constant. However, we empirically found this formulation helps diversity
options in our preliminary experiments. In case of RVIC, We add a another classifier q(o∣xf), and
constructed rewards by RRVIC = CVIC (logPO(o∣Xs,Xf) — logq(o∣xf)) q is learned in the same
fashion as PO and η using the replay buffer BO.
C	Experimental Details
Our anonymized code used for all our experiments is on https://
anonymous.4open.science/r/imtc-anonymized- code-E5D1/.
C.1 Network Architecture
Figure 6 illustrates the neural network architecture used in our experiments. We used a shared
convolutional encoder for Gridworld experiments, and two separated fully connected layers with
64 units for continuous control experiments. πo is parameterized as a Gaussian distribution with
separated networks for standard derivations per option, similar to Schulman et al. (2015a). We
used ReLU as an activator for all hidden layers and initialized networks by the orthogonal (Saxe
et al., 2014) initialization in all experiments. Note that the last layer for intra-option policy πo was
initialized with small values, following the standard practice (Andrychowicz et al., 2021). We used
the Adam (Kingma and Ba, 2015) optimizer in all experiments. Unless otherwise noted, we used the
default parameters in PyTorch (Paszke et al., 2019) 1.8.1.
18
Under review as a conference paper at ICLR 2022
Description	Value
Y	0.99
Adam Learning Rate	3 X 10-4
Adam e	1 × 10-4
Clip parameter for 'βo (eβ)	-0:05-
Num. timesteps per rollout	-256
Num. actors	16
GAE λ	-0:95-
Num. epochs for PPO	10
Minibatch size for PPO	-1024-
Weight of H(πo) (CH)	-0.001-
Weightof H(βo) (CHe)	-	-0:01-
Gradient clipping	0.5
Capacity of BO	-8192-
Max num. option transitions to store (hmax)	20
Num. epochs for training p^ and μ	4
Minibatch size for training p^ and μ	-2048-
SCaIing of RVIC (CVIC)	一	-0.005-
Synchronizing interval of the target network for VIC (KVIC)	20
Table 1: Used hyperparameters
C.2 Hyperparameters
Table 1 shows all hyperparameters used in IMTC + VIC experiments on MuJoCo continuous control
tasks.
C.3 Environment Implementation
Gridworld is based on RLPy (Geramifard et al. (2015), BSD3 License). We constructed continuous
control environments on the MuJoCo (commercial license, Todorov et al. (2012)), using OpenAI
Gym (MIT license, Brockman et al. (2016)). Especially, point environments are implemented based
on “PointMaze” in rllab (MIT license, Duan et al. (2016)) with some modifications, mainly around
collision detection. We also refered to the modified PointMaze code3 (Apache 2.0 license) relased by
Nachum et al. (2018). The swimmer robot is originally used in Coulom (2002).
C.4 Computational Resources
All experiments are conducted on a private cluster with NVIDIA P100 GPUs. On the cluster, training
IMTC with VIC on MuJoCo PointMaze domain for 4 × 106 steps takes about 27 minutes.
C.5 Comparison of VIC and RVIC rewards
Figure 7 shows the
C.6 Effectivity of UGAE
To investigate the effect of UGAE (9), we compared IMTC and OC with and without UGAE in
task adaptation experiments. IMTC (no UGAE) and OC (no UGAE) estimate the adavantage
ignoring the future rewards after switching options, similar to advantage estimation proposed by
Bacon et al. (2017). Figure 8 shows the result. We can see that UGAE improves the performance in
all domains for both IMTC and OC.
19
Under review as a conference paper at ICLR 2022

0.000
-0.005
-0.010
-0.015
-0.020
Comparison of Intrinsic Rewards
0.0	0.5	1.0	1.5	2.0 2.5	3.0 3.5	4.0
Environmental Stens (1O6) le6
Comparison of log p(o∖×s, Xf)
-i	........
(X ixo)d 6θ
Environmental Steos (1O6)
Ie6
during training.
Right: Comparison of
PomtReach
SwimmerReach
Figure 7: Left: Comparison of intrinsic rewards
logpo(o∣xs,x) during training.
AntReach
0.2
0.1
0.0
-0.1
0.2	0.4	0.6	0.8	1.0
Environmental Steps (10e) lβ6
0.2	0.4	0.6 o.s ι.o
Environmental Steps (106) lβ6
PointBiIhsrd
0.2	0.4	0.6	0.8	14)
Environmental Steps (10e) lβ6
0.2	0.4	0.6 O.S 1.0
Environmental Steps (106) lβ6
AntPush
AntBiIIisrd
PointPush
02	0.4	0.6	0∙β ι.o
Environmental Steps (10β) lβ6
0.2	0.4	0.6 O.S 1.0
Environmental Steps (106) lβ6
Method
4.lrι4⅛mthp* . IMTC
IMTC (no UGAE)
0 OC
• OC (no UGAE)
0.2	0.4	0.6 O.S 1.0
Environmental Steps (106) lβ6
Figure 8:	Learning curves for transfer learning experiments with
C.7 Number of Options
Figure 9 shows learning curves of IMTC agents with variable number of options (2, 4, 6, 8, 10, 12) in
task adaptation experiments. Increasing the number of options sometimes improves the performance
(e.g., in SwimmerReach), but the relationship is not obvious. However, we can confirm that
increasing the number of options does not have a bad effect on the performance, while it reduces
training steps per each intra-option policy, thanks to network sharing Appendix C.1.
C.8 Options learned in four rooms
Figure 10 show the options learned in the classifical Four Rooms Gridworld (Sutton et al., 1999).
3https://github.com/tensorflow/models/tree/v2.3.0/research/efficient-hrl
20
Under review as a conference paper at ICLR 2022

Environmental Steps (10β)
AntReach
PointBiIhsrd
0.5
0.4
0.2	0.4	0∙6 O.S 1.0
Environmental Steps (10β) lβ6
0.2	0.4	0.6	0.8
Environmental Steps (106)
Iq
ieβ
0.2	0.4	0.6	0∙β
Environmental Steps (106)
0.2	0.4	0.6	Q.8
Environmental Steps (IOe)
AntPush
0.5
Num. Options
・2
4
02	0.4	0.6 o.s ι.o
Environmental Steps (106) lβ6
02	0.4	0.6	0.8	1.0
Environmental Steps (106) lβ6
6
8
.12
0 16
Figure 9:	Learning curves for transfer learning experiments.
IMTC + VIC
Option 0
IMTC (R=0.01)
} +，，4 Ir + ⅜ ⅜ ⅜
r + ÷ ÷ + - ÷H÷∣-⅞
M+4 + <	+ ÷ Ψ -{
Option 0
Option 2
Option 2
IMTC + RVIC
Option 0
Option 1
Option 2
OC + VIC
Option 0
Option 1
Option 2
Option 0
Option 1
Option 2
RVIC (β=0.1)
Option 0
Option 2
Figure 10:	Options learned in four rooms environment
21