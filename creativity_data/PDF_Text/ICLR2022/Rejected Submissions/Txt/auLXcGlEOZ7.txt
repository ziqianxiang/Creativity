Under review as a conference paper at ICLR 2022
On Margin Maximization in Linear and
ReLU Networks
Anonymous authors
Paper under double-blind review
Ab stract
The implicit bias of neural networks has been extensively studied in recent years.
Lyu & Li (2019) showed that in homogeneous networks trained with the expo-
nential or the logistic loss, gradient flow converges to a KKT point of the max
margin problem in the parameter space. However, that leaves open the question of
whether this point will generally be an actual optimum of the max margin prob-
lem. In this paper, we study this question in detail, for several neural network ar-
chitectures involving linear and ReLU activations. Perhaps surprisingly, we show
that in many cases, the KKT point is not even a local optimum of the max margin
problem. On the flip side, we identify multiple settings where a local or global
optimum can be guaranteed. Finally, we answer a question posed in Lyu & Li
(2019) by showing that for non-homogeneous networks, the normalized margin
may strictly decrease over time.
1	Introduction
A central question in the theory of deep learning is how neural networks generalize even when
trained without any explicit regularization, and when there are far more learnable parameters than
training examples. In such optimization problems there are many solutions that label the training
data correctly, and gradient descent seems to prefer solutions that generalize well (Zhang et al.,
2016). Hence, it is believed that gradient descent induces an implicit bias (Neyshabur et al., 2014;
2017), and characterizing this bias has been a subject of extensive research in recent years.
A main focus in the theoretical study of implicit bias is on homogeneous neural networks. These are
networks where scaling the parameters by any factor α > 0 scales the predictions by αL for some
constant L. For example, fully-connected and convolutional ReLU networks without bias terms are
homogeneous. Lyu & Li (2019) proved that in linear and ReLU homogeneous networks trained
with the exponential or the logistic loss, if gradient flow converges to zero loss1, then the direction
to which the parameters of the network converge can be characterized as a first order stationary
point (KKT point) of the maximum margin problem in the parameter space. Namely, the problem
of minimizing the `2 norm of the parameters under the constraints that each training example is
classified correctly with margin at least 1. They also showed that this KKT point satisfies necessary
conditions for optimality. However, the conditions are not known to be sufficient even for local
optimality. It is analogous to showing that some unconstrained optimization problem converges to a
point with gradient zero, without proving that it is either a global or a local minimum.
In this work we consider several architectures of homogeneous neural networks with linear and
ReLU activations, and study whether the aforementioned KKT point is guaranteed to be a global
optimum of the maximum margin problem, a local optimum, or neither. Perhaps surprisingly, our
results imply that in many cases, such as depth-2 fully-connected ReLU networks and depth-2 di-
agonal linear networks, the KKT point may not even be a local optimum of the maximum-margin
problem. On the flip side, we identify multiple settings where a local or global optimum can be
guaranteed.
We now describe our results in a bit more detail. We denote by N the class of neural networks
without bias terms, where the weights in each layer might have an arbitrary sparsity pattern, and
1They also assumed directional convergence, but (Ji & Telgarsky, 2020) later showed that this assumption
is not required.
1
Under review as a conference paper at ICLR 2022
Table 1: Results on depth-2 networks.
	Linear	ReLU
Fully-connected	Global (Thm.3.1)	Not local- (Thm.3.2)
NnO-Share	Not local (Thm. 4.1)	Not local (Thm.3.2)
Nno-share assuming non-zero weightS vectorS	Global (Thm. 4.2)	Not local (Thm. 4.2)
Nno-Share aSSuming non-zero inputS to all neuronS	Global (Thm. 4.2)	Local, Not global (Thm. 4.3)
N aSSuming non- zero inputS to all neuronS	Not local (Thm. 4.4)	Not local (Thm. 4.4)
Table 2: Results on deep networks.
	Linear	ReLU
Fully-connected	Global (Thm.3.1)	Not local (Thm.3.2)
Nno-Share aSSuming non-zero inputS to all neuronS	Not local (Thm. 5.1)	Not local (Thm. 5.1)
N - max margin for each layer Sep- arately	Global (Thm. 5.2)	Not local (Thm. 5.3)
N - max margin for each layer Sep- arately, aSSuming non-zero inputS to all neuronS	Global (Thm. 5.2)	Local, Not global (Thm. 5.4)
weights might be shared2 . The class N contains, for example, convolutional networks. Moreover,
we denote by Nno-share the subclass of N that contains only networks without shared weights, such
as fully-connected networks and diagonal networks (cf. Gunasekar et al. (2018b); Yun et al. (2020)).
We describe our main results below, and also summarize them in Tables 1 and 2.
Fully-connected networks:
•	In linear fully-connected networks of any depth the KKT point is a global optimum3.
•	In fully-connected depth-2 ReLU networks the KKT point may not even be a local optimum.
Moreover, this negative result holds with constant probability over the initialization, i.e., there
is a training dataset such that gradient flow with random initialization converges with constant
probability to the direction of a KKT point which is not a local optimum.
Depth-2 networks in N:
•	The positive result on fully-connected linear networks does not extend to networks with sparse
weights: In linear diagonal networks the KKT point may not be a local optimum.
•	In our proof for the above negative result, the KKT point contains a neuron whose weights
vector is zero. However, in practice gradient descent often converges to networks that do not
contain such zero neurons. We show that for linear networks in Nno-share, if the KKT point has
only non-zero weights vectors, then it is a global optimum. We also show that even for the
simple case of depth-2 diagonal linear networks, the optimality of the KKT points can be unex-
pectedly subtle, in the context of margin maximization in the predictor space (see Remark 4.1).
•	For ReLU networks in Nno-share, in order to obtain a positive result we need a stronger assump-
tion. We show that if the KKT point is such that for every input in the dataset the input to every
hidden neuron in the network is non-zero, then it is guaranteed to be a local optimum (but not
necessarily a global optimum).
•	For linear or ReLU convolutional networks, even if the above assumptions hold, the KKT point
may not be a local optimum.
Deep networks in N:
•	We show that the positive results on depth-2 linear and ReLU networks in Nno-share (under the
assumptions described above) do not apply to deeper networks.
•	We study a weaker notion of margin maximization: maximizing the margin for each layer
separately. For linear networks of depth m ≥ 2 in N (including networks with shared weights),
we show that the KKT point is a global optimum of the per-layer maximum margin problem.
For ReLU networks the KKT point may not even be a local optimum of this problem, but under
the assumption on non-zero inputs to all neurons it is a local optimum.
2See Section 2 for the formal definition.
3We note that margin maximization for such networks in the predictor space is already known (Ji & Tel-
garsky, 2020). However, margin maximization in the predictor space does not necessarily imply margin maxi-
mization in the parameter space.
2
Under review as a conference paper at ICLR 2022
In the paper, our focus is on understanding what can be guaranteed for the KKT convergence points
specified in Lyu & Li (2019). Accordingly, in most of our negative results, the construction as-
sumes some specific initialization of gradient flow, and does not quantify how “likely” they are
to be reached under some random initialization. An exception is our negative result for depth-2
fully-connected ReLU networks (Thm. 3.2), which holds with constant probability under reason-
able random initializations. Understanding whether this can be extended to the other settings we
consider is an interesting problem for future research.
Finally, we consider non-homogeneous networks, for example, networks with skip connections or
bias terms. Lyu & Li (2019) showed that a smoothed version of the normalized margin is mono-
tonically increasing when training homogeneous networks. They observed empirically that the nor-
malized margin is monotonically increasing also when training non-homogeneous networks, but did
not provide a proof for this phenomenon and left it as an open problem. We give an example for a
simple non-homogeneous network where the normalized margin (as well as the smoothed margin)
is strictly decreasing (see Thm. 6.1).
The paper is structured as follows: In Section 2 we provide necessary notations and definitions, and
discuss the most relevant prior results. Additional related works are discussed in Appendix A. In
Sections 3, 4 and 5 we state our results on fully-connected networks, depth-2 networks in N and
deep networks in N respectively, and provide some proof ideas. In Section 6 we state our result on
non-homogeneous networks. All formal proofs are deferred to Appendix C.
2	Preliminaries
Notations. We use bold-faced letters to denote vectors, e.g., x = (x1, . . . , xd). For x ∈ Rd we
denote by ∣∣xk the Euclideannorm. Wedenoteby 1(∙) the indicator function, for example 1(t ≥ 5)
equals 1 if t ≥ 5 and 0 otherwise. For an integer d ≥ 1 we denote [d] = {1, . . . , d}.
Neural networks. A fully-connected neural network Φ of depth m ≥ 2 is parameterized by a
collection θ = [W (l)]lm=1 of weight matrices, such that for every layer l ∈ [m] we have W(l) ∈
Rdl×dl-1 . Thus, dl denotes the number of neurons in the l-th layer (i.e., the width of the layer). We
assume that dm = 1 and denote by d := d0 the input dimension. The neurons in layers [m - 1] are
called hidden neurons. A fully-connected network computes a function Φ(θ; ∙) : Rd → R defined
recursively as follows. For an input x ∈ Rd we set h00 = x, and define for every j ∈ [m - 1] the
input to the j-th layer as hj = W(j)h0j-1, and the output of the j-th layer as h0j = σ(hj), where
σ : R → R is an activation function that acts coordinate-wise on vectors. Then, we define Φ(θ; x) =
W (m) h0m-1 . Thus, there is no activation function in the output neuron. When considering depth-2
fully-connected networks we often use a parameterization θ = [w1 , . . . , wk, v] where w1, . . . , wk
are the weights vectors of the k hidden neurons (i.e., correspond to the rows of the first layer’s weight
matrix) and v are the weights of the second layer.
We also consider neural networks where some weights can be missing or shared. We define a
class N of networks that may contain sparse and shared weights as follows. A network Φ in N is
parameterized by θ = [u(l)]lm=1 where m is the depth of Φ, and u(l) ∈ Rpl are the parameters of the
l-th layer. We denote by W(l) ∈ Rdl ×dl-1 the weight matrix of the l-th layer. The matrix W(l) is
described by the vector u(l), and a function gl : [dl] × [dl-1] → [pl] ∪ {0} as follows: Wi(jl) = 0
if gl (i, j) = 0, and Wi(jl) = uk if gl (i, j) = k > 0. Thus, the function gl represents the sparsity
and weight-sharing pattern of the l-th layer, and the dimension pl of u(l) is the number of free
parameters in the layer. We denote by d := d0 the input dimension of the network and assume that
the output dimension dm is 1. The function Φ(θ; ∙) : Rd → R computed by the neural network is
defined recursively by the weight matrices as in the case of fully-connected networks. For example,
convolutional neural networks are in N . Note that the networks in N do not have bias terms and
do not allow weight sharing between different layers. Moreover, we define a subclass Nno-share of
N, that contains networks without shared weights. Formally, a network Φ is in Nno-share if for every
layer l and every k ∈ [pl] there is at most one (i, j) ∈ [dl] × [dl-1] such that gl (i, j) = k. Thus,
networks in Nno-share might have sparse weights, but do not allow shared weights. For example,
diagonal networks (defined below) and fully-connected networks are in Nno-share.
3
Under review as a conference paper at ICLR 2022
A diagonal neural network is a network in Nno-share such that the weight matrix of each layer is
diagonal, except for the last layer. Thus, the network is parameterized by θ = [w1 , . . . , wm] where
Wj ∈ Rd for all j ∈ [m], and it computes a function Φ(θ; ∙) : Rd → R defined recursively as
follows. For an input x ∈ Rd set h0 = x. For j ∈ [m - 1], the output of the j-th layer is
hj = σ (diag(Wj)hj -1). Then, we have Φ(θ; x) = Wm> hm-1.
In all the above definitions the parameters θ of the neural networks are given by a collection of
matrices or vectors. We often view θ as the vector obtained by concatenating the matrices or vectors
in the collection. Thus, kθk denotes the `2 norm of the vector θ.
The ReLU activation function is defined by σ(z) = max{0, z}, and the linear activation is σ(z) =
z. In this work we focus on ReLU networks (i.e., networks where all neurons have the ReLU
activation) and on linear networks (where all neurons have the linear activation). We say that a
network Φ is homogeneous if there exists L > 0 such that for every α > 0 and θ , x we have
Φ(αθ; x) = αLΦ(θ; x). Note that in our definition of the class N we do not allow bias terms, and
hence all linear and ReLU networks in N are homogeneous. With the exception of Section 6 which
studies non-homogeneous networks, all networks considered in this work are homogeneous.
Optimization problem and gradient flow (GF). Let S = {(xi, yi)}in=1 ⊆ Rd × {-1, 1} be a bi-
nary classification training dataset. Let Φ be a neural network parameterized by θ ∈ Rm . For a loss
function ' : R → R the empirical loss of Φ(θ; ∙) on the dataset S is L(θ) := Pn=ι '(yiΦ(θ; Xi)).
We focus on the exponential loss '(q) = e-q and the logistic loss '(q) = log(1 + e-q), and consider
gradient flow (GF) on the objective L(θ). This setting captures gradient descent with an infinitesi-
mal step size. Let θ(t) be the trajectory of GF. Starting from an initial point θ(0), the dynamics of
θ(t) is given by the differential equation ddt) = -VL(θ(t)). Note that the ReLU function is not
differentiable at 0. Practical implementations of gradient methods define the derivative σ0(0) to be
some constant in [0, 1]. We note that the exact value of σ0(0) has no effect on our results.
Convergence to a KKT point of the maximum-margin problem. We say that a trajectory θ(t)
converges in direction to θ if limt→∞ |嚣§ = ^^. In this work We rely on the following theorem:
Theorem 2.1 (Paraphrased from Lyu & Li (2019); Ji & Telgarsky (2020)). Let Φ be a homogeneous
linear or ReLU neural network. Consider minimizing either the exponential or the logistic loss over
a binary classification dataset {(Xi, yi)}in=1 using GF. Assume that there exists time t0 such that
L(θ(t0)) < 1, namely, Φ classifies every Xi correctly. Then, GF converges in direction to a first
order stationary point (KKT point) of the following maximum margin problem in parameter space:
min1 ∣∣θ∣∣2	St ∀i ∈ [n] yiΦ(θ; Xi) ≥ 1 .
θ2
(1)
Moreover, L(θ(t)) → 0 and kθ(t)k → ∞ as t → ∞.
In the case of ReLU networks, Problem 1 is non-smooth. Hence, the KKT conditions are defined
using Clarke’s subdifferential, which is a generalization of the differential for non-differentiable
functions. See Appendix B for a formal definition. We note that Lyu & Li (2019) proved the above
theorem under the assumption that θ converges in direction, and Ji & Telgarsky (2020) showed that
such a directional convergence occurs and hence this assumption is not required.
Lyu & Li (2019) also showed that the KKT conditions of Problem 1 are necessary for optimality. In
convex optimization problems, necessary KKT conditions are also sufficient for global optimality.
However, the constraints in Problem 1 are highly non-convex. Moreover, the standard method for
proving that necessary KKT conditions are sufficient for local optimality, is by showing that the
KKT point satisfies certain second order sufficient conditions (SOSC) (cf. Ruszczynski (2011)).
However, even when Φ is a linear neural network it is not known when such conditions hold. Thus,
the KKT conditions of Problem 1 are not known to be sufficient even for local optimality.
A linear network with weight matrices W(1), . . . , W(m) computes a linear predictor X 7→ hβ, Xi
where β = W(m) ∙ ... ∙ W⑴.Some prior works studied the implicit bias of linear networks in
the predictor space. Namely, characterizing the vector β from the aforementioned linear predictor.
Gunasekar et al. (2018b) studied the implications of margin maximization in the parameter space
on the implicit bias in the predictor space. They showed that minimizing kθk (under the constraints
4
Under review as a conference paper at ICLR 2022
in Problem 1) implies: (1) Minimizing ∣∣βk2 for fully-connected networks; (2) Minimizing kβk2∕L
for depth-L diagonal networks; (3) Minimizing kβk2∕L for depth-L convolutional networks With
full-dimensional filters, where β are the Fourier coefficients of β. However, these implications may
not hold if GF converges to a KKT point which is not a global optimum of Problem 1.
For some classes of linear networks, positive results were obtained directly in the predictor space,
without assuming convergence to a global optimum of Problem 1 in the parameter space. Most
notably, for fully-connected linear networks (of any depth), Ji & Telgarsky (2020) showed that
under the assumptions of Thm. 2.1, GF maximizes the `2 margin in the predictor space. Note that
margin maximization in the predictor space does not necessarily imply margin maximization in
the parameter space. Moreover, some results on the implicit bias in the predictor space of linear
convolutional networks with full-dimensional convolutional filters are given in Gunasekar et al.
(2018b). However, the architecture and set of assumptions are different than what we focus on.
3 Fully-connected networks
First, we show that fully-connected linear networks converge to a global optimum of Problem 1.
Theorem 3.1. Let m ≥ 2 and let Φ be a depth-m fully-connected linear network parameterized by
θ. Consider minimizing either the exponential or the logistic loss over a dataset {(xi , yi)}in=1 using
GF. Assume that there exists time t0 such that L(θ(t0)) < 1. Then, GF converges in direction to a
global optimum of Problem 1.
Proof idea (for the complete proof see Appendix C.2). Building on results from Ji & Telgarsky
______ . 一	《  "、	,	,	/ ~r	. ,.	.	—	. X
(2020) and Du et al. (2018), we show that GF converges in direction to a KKT point θ =
[W⑴，...，W(m)] such that for every l ∈ [m] We have W((I = C ∙ vιv[], where C > 0 and
Vo,..., Vm are unit vectors (with Vm = 1). Also, we have ∣W(m) ∙ ... ∙ W⑴ ∣ = Cm =
min ∣u∣ s.t. yiu>xi ≥ 1 for all i ∈ [n]. Then, we show that every θ that satisfies these proper-
ties, and satisfies the constraints of Problem 1, is a global optimum. Intuitively, the most “efficient”
way (in terms of minimizing the parameters) to achieve margin 1 with a linear fully-connected net-
work, is by using a network such that the direction of its corresponding linear predictor maximizes
the margin, the layers are balanced (i.e., have equal norms), and the weight matrices are aligned. □
We now prove that the positive result in Thm. 3.1 does not apply to ReLU networks. We show
that in depth-2 fully-connected ReLU networks GF might converge in direction to a KKT point of
Problem 1 which is not even a local optimum. Moreover, it occurs under conditions holding with
constant probability over reasonable random initializations.
Theorem 3.2. Let Φ be a depth-2 fully-connected ReLU network with input dimension 2 and two
hidden neurons. Namely, for θ = [w1, w2, V] and x ∈ R2 we have Φ(θ; x) = P(2=1 v(σ(w(>x).
Consider minimizing either the exponential or the logistic loss using GF. Consider the dataset
{(x1,y1), (x2,y2)} where xι = (1, ɪ) , x2 = (—1, ɪ) , and yι = y = L Assume Ihat the
initialization θ(0) is such that for every i ∈ {1, 2} we have hw1(0), xii > 0 and hw2(0), xii < 0.
Also, assume that v1(0) > 0. Then, GF converges to zero loss, and converges in direction to a KKT
point of Problem 1 which is not a local optimum.
Proof idea (for the complete proof see Appendix C.3). By
on the given dataset, we show that it converges to zero
loss, and converges in direction to a KKT point θ such
that W1 = (0,2)>, Vι = 2, W2 = 0, and V2 = 0.
Note that W2 = 0 and V2 = 0 since W2(t),v2(t) remain
constant during the training and limt→∞ ∣θ(t)∣ = ∞.
This is illustrated in the figure on the right: For time t
we denote ui(t) = vi(t)Wi(t). We observe u1(t), u2(t)
for times t1 < t2. As t → ∞ we have ∣u1(t)∣ → ∞
and u1 converges in direction to (0, 1). The vector u2
remains constant during the training. Hence 宿畏 → 0.
analyzing the dynamics of GF
5
Under review as a conference paper at ICLR 2022
Then, we show that for every 0 < < 1 there exists some θ0 such that kθ0 - θk ≤ , θ0 satisfies
yiΦ(θ0; Xi) ≥ 1 for every i ∈ {1, 2}, and ∣∣θ0k < |网|. Such θ0 is obtained from θ by slightly
changing Wι, W2, and v2. Thus, by using the second hidden neuron, which is not active in θ, We
can obtain a solution θ0 with smaller norm.	□
We note that the assumption on the initialization in the above theorem holds with constant probability
for standard initialization schemes (e.g., Xavier initialization).
Remark 3.1 (Unbounded sub-optimality). By choosing appropriate inputs X1 , X2 in the setting of
Thm. 3.2, it is not hard to show that the sub-optimality of the KKT point w.r.t. the global optimum
can be arbitrarily large. Namely, for every large M > 0 we can choose a dataset where the angle
~
kθk
between xι and X2 Is SufficientIy close to π, such that ^^ ≥ M, where θ is a KKTpoint to which
GF converges, and θ* is a global optimum OfProblem 1. Indeed, as illustrated in the figurefrom the
proof idea of Thm. 3.2, if one neuron is active on both inputs and the other neuron is not active on
any input, then the active neuron needs to be very large in order to achieve margin 1, while if each
neuron is active on a single input then we can achieve margin 1 with much smaller parameters. We
note that such unbounded sub-optimality can be obtained also in other negative results in this work
(in Thm. 4.1, 4.3, 4.4 and 5.4).
Remark 3.2 (Robustness to small perturbations). Thm. 3.2 holds even if we slightly perturb the
inputs X1 , X2. Thus, it is not sensitive to small changes in the dataset. We note that such robustness
to small perturbations can be shown also for the negative results in Thm. 4.1, 4.3, 5.1 and 5.4.
4 DEPTH-2 NETWORKS IN N
In this section we study depth-2 linear and ReLU networks in N. We first show that already for
linear networks in Nno-share (more specifically, for diagonal networks) GF may not converge even to
a local optimum.
Theorem 4.1.	Let Φ be a depth-2 linear or ReLU diagonal neural network parameterized by θ =
[W1, W2]. Consider minimizing either the exponential or the logistic loss using GF. There exists a
dataset {(X, y)} ⊆ R2 × {-1, 1} of size 1 and an initialization θ(0), such that GF converges to zero
loss, and converges in direction to a KKT point θ of Problem 1 which is not a local optimum.
Proof idea (for the complete proof see Appendix C.4). Let X = (1, 2)> and y = 1. Let θ(0) such
that W1(0) = W2(0) = (1, 0)>. Recalling that the diagonal network computes the function X 7→
(W1 ◦W2)>X (where ◦ is the entry-wise product), we see that the second coordinate remains inactive
〜
during training. It is not hard to show that GF converges to the KKT point θ with W1 = W2 =
(1, 0)>. However, itis not a local optimum, since for small > 0 the parameters θ0 = [W01, W20 ] with
Wi = w2 = (√1 - e, p∕l∣)> satisfy the constraints of Problem 1, and we have ∣∣θ0k < kθ∣.	□
By Thm. 3.2 fully-connected ReLU networks may not converge to a local optimum, and by Thm. 4.1
linear (and ReLU) networks with sparse weights may not converge to a local optimum. In the proofs
of both of these negative results, GF converges to a KKT point such that one of the weights vectors
of the hidden neurons is zero. However, in practice gradient descent often converges to a network
that does not contain such disconnected neurons. Hence, a natural question is whether the negative
results hold also in networks that do not contain neurons whose weights vector is zero. In the
following theorem we show that in linear networks such an assumption allows us to obtain a positive
result. Namely, in depth-2 linear networks in Nno-share, if GF converges in direction to a KKT point
of Problem 1 that satisfies this condition, then it is guaranteed to be a global optimum. However, we
also show that in ReLU networks assuming that all neurons have non-zero weights is not sufficient.
Theorem 4.2.	We have:
1.	Let Φ be a depth-2 linear neural network in Nno-share, parameterized by θ. Consider minimizing
either the exponential or the logistic loss over a dataset {(Xi, yi)}in=1 using GF. Assume that
there exists time t0 such that L(θ(t0)) < 1, and let θ be the KKT point of Problem 1 such
that θ(t) converges to θ in direction (such θ exists by Thm. 2.1). Assume that in the network
6
Under review as a conference paper at ICLR 2022
parameterized by θ all hidden neurons have non-zero incoming weights vectors. Then, θ is a
global optimum of Problem 1.
2.	Let Φ be a fully-connected depth-2 ReLU network with input dimension 2 and4 hidden neurons,
parameterized by θ. Consider minimizing either the exponential or the logistic loss using GF.
There exists a dataset and an initialization θ(0), such that GF converges to zero loss, and
converges in direction to a KKT point θ of Problem 1, which is not a local optimum, and in the
network parameterized by θ all hidden neurons have non-zero incoming weights.
Proof idea (for the complete proof see Appendix C.5). We give here the proof idea for part (1). Let
k be the width of the network. For every j ∈ [k] we denote by wj the incoming weights vector to the
j-th hidden neuron, and by vj the outgoing weight. Let uj = vj wj . We consider an optimization
problem over the variables u1 , . . . , uk where the objective is to minimize Pj∈[k] kuj k and the
constrains correspond to the constraints of Problem 1. Let θ = [w 1,..., Wk, v] be the KKT point
of Problem 1 to which GF converges in direction. For every j ∈ [k] we denote Uj = Vj Wj. We show
that Uι,..., Uk satisfy the KKT conditions of the aforementioned problem. Since the objective there
is convex and the constrains are affine, then it is a global optimum. Finally, we show that it implies
global optimality of θ.	□
Remark 4.1 (Implications on margin maximization in the predictor space for diagonal linear net-
works). Thm. 4.1 and 4.2 imply analogous results on diagonal linear networks also in the predictor
space. As we discussed in Section 2, Gunasekar et al. (2018b) showed that in depth-2 diagonal lin-
ear networks, minimizing kθk2 under the constraints in Problem 1 implies minimizing kβk1, where
β is the corresponding linear predictor. Thm. 4.1 can be easily extended to the predictor space,
namely, GF on depth-2 linear diagonal networks might converge to a KKT point θ of Problem 1,
such that the corresponding linear predictor β is not a local optimum of the following problem:
arg min kβk1 s.t.	∀i ∈ [n] yihβ, xii ≥ 1 .	(2)
β
Moreover, by combining part (1) of Thm. 4.2 with the result from Gunasekar et al. (2018b), we de-
duce that if GF on a depth-2 diagonal linear network converges to a KKT point θ of Problem 1 with
non-zero weights vectors, then the corresponding linear predictor is a global optimum of Problem 2.
By part (2) of Thm. 4.2, assuming that GF converges to a network without zero neurons is not
sufficient for obtaining a positive result in the case of ReLU networks. Hence, we now consider a
stronger assumption, namely, that the KKT point θ is such that for every xi in the dataset the inputs
to all hidden neurons in the computation Φ(θ; xi) are non-zero. In the following theorem we show
that in depth-2 ReLU networks, if the KKT point satisfies this condition then it is guaranteed to be
a local optimum of Problem 1. However, even under this condition it is not necessarily a global
optimum. The proof is given in Appendix C.6 and uses ideas from the previous proofs, with some
required modifications.
Theorem 4.3. Let Φ be a depth-2 ReLU network in Nno-share parameterized by θ. Consider mini-
mizing either the exponential or the logistic loss over a dataset {(xi, yi)}in=1 using GF. Assume that
there exists time t0 such that L(θ(t0)) < 1, and let θ be the KKT point of Problem 1 such that θ(t)
converges to θ in direction (such θ exists by Thm. 2.1). Assume that for every i ∈ [n] the inputs to all
hidden neurons in the computation Φ(θ; xi) are non-zero. Then, θ is a local optimum of Problem 1.
However, it may not be a global optimum, even if the network Φ is fully connected.
Note that in all the above theorems we do not allow shared weights. We now consider the case of
depth-2 linear or ReLU networks in N, where the first layer is convolutional with disjoint patches
(and hence has shared weights), and show that GF does not always converge in direction to a local
optimum, even when the inputs to all hidden neurons are non-zero (and hence there are no zero
weights vectors).
Theorem 4.4. Let Φ be a depth-2 linear or ReLU network in N, parameterized by θ = [W, v] for
W, v ∈ R2, such that for x ∈ R4 we have Φ(θ; x) = Pj2=1 vj σ(W>x(j)) where x(1) = (x1, x2)
and x(2) = (x3, x4). Thus, Φ is a convolutional network with two disjoint patches. Consider mini-
mizing the exponential or the logistic loss using GF. Then, there exists a dataset {(x, y)} of size 1,
7
Under review as a conference paper at ICLR 2022
and an initialization θ(0), such that GF converges to zero loss, and converges in direction to a KKT
point θ= [W, v] ofProblem 1 which is not a local optimum. Moreover, X, x(j)i = 0 for j ∈ {1,2}.
Proof idea (for the complete proof see Appendix C.7). Let x =
θ(0) such that w(0) = (0,1)> and v(0) = (*, *)
(4，√2,-4, √2)> and y = LLet
Since x(1) and x(2) are symmetric w.r.t.
w(0), and v(0) does not break this symmetry, then w keeps its direction throughout the training.
Thus, GF converges in direction to a KKT point θθ where W = (0,1)> and V = (√, =) . It is not
a local optimum, since for every small e > 0 the parameters θ0 = [w0, v0] with w0 = (√l, 1 - e)>
~..
and v0
5 DEEP NETWORKS IN N
satisfy the constraints of Problem 1, and kθ0k < kθk.
□
In this section we study the more general case of depth-m neural networks inN, where m ≥ 2. First,
we show that for networks of depth at least 3 in Nno-share, GF may not converge to a local optimum
of Problem 1, for both linear and ReLU networks, and even where there are no zero weights vectors
and the inputs to all hidden neurons are non-zero. We prove this claim for diagonal networks.
Theorem 5.1. Let m ≥ 3. Let Φ be a depth-m linear or ReLU diagonal neural network parame-
terized by θ . Consider minimizing either the exponential or the logistic loss using GF. There exists
a dataset {(x, y)} ⊆ R2 × {-1, 1} of size 1 and an initialization θ (0), such that GF converges to
zero loss, and converges in direction to a KKT point θ of Problem 1 which is not a local optimum.
Moreover, all inputs to neurons in the computation Φ(θ; x) are non-zero.
Proof idea (for the complete proof see Appendix C.8). Let x = (1, 1)> and y = 1. Consider the
initialization θ (0) where wj (0) = (1, 1)> for every j ∈ [m]. We show that GF converges in di-
rection to a KKT point θ = [W 1,..., Wm] such that Wj = (2-1/m, 2-1/m)> for all j ∈ [m].
Then, we consider the parameters θ0 = [w10 , . . . , w0m] such that for every j ∈ [m] we have
Wj = ((?)1/m ,(q)1/m) , and show that if e > 0 is sufficiently small, then θ0 satisfies
the constraints in Problem 1 and We have ∣∣θ0k < |网].	□
Note that in the case of linear networks, the above result is in contrast to networks with sparse
weights of depth 2 that converge to a global optimum by Thm. 4.2, and to fully-connected networks
of any depth that converge to a global optimum by Thm. 3.1. In the case of ReLU networks, the
above result is in contrast to the case of depth-2 networks studied in Thm. 4.3, where itis guaranteed
to converge to a local optimum.
In light of our negative results, we now consider a weaker notion of margin maximization, namely,
maximizing the margin for each layer separately. Let Φ be a network of depth m inN, parameterized
by θ = [u(l)]lm=1. The maximum-margin problem for a layer l0 ∈ [m] w.r.t. θ0 = [u(0l)]lm=1 is:
min1 ∣∣u(l0)∣∣	s.t. ∀i ∈ [n] yiΦ(θ0; Xi) ≥ 1 ,	(3)
where θ0 = [u(01), . . . , u(0l0-1), u(l0), u(0l0+1), . . . , u(0m)]. For linear networks we have the following:
Theorem 5.2. Let m ≥ 2. Let Φ be any depth-m linear neural network in N, parameterized
by θ = [u(l)]lm=1. Consider minimizing either the exponential or the logistic loss over a dataset
{(Xi, yi)}in=1 using GF. Assume that there exists time t0 such that L(θ (t0)) < 1. Then, GF converges
in direction to a KKT point θ = [u(l)]m=1 of Problem 1, such that for every layer l ∈ [m] the
parameters vector U(I) is a global optimum ofProblem 3 w.rt. θ.
The theorem follows by noticing that if Φ is a linear network, then the constraints in Problem 3 are
affine, and its KKT conditions are implied by the KKT conditions of Problem 1. See Appendix C.9
for the formal proof. By Thm. 4.1, 4.4 and 5.1, linear networks inN might converge to a KKT point
8
Under review as a conference paper at ICLR 2022
θ which is not a local optimum of Problem 1. However, by Thm. 5.2 each layer in θ is a global
optimum of Problem 3. Thus, any improvement to θ requires changing a few layers simultaneously.
While in linear networks GF maximize the margin for each layer separately, in the following theorem
we show that this claim does not hold for ReLU networks: Already for fully-connected networks of
depth 2 GF may not converge to a local optimum of Problem 3 (see Appendix C.10 for the proof).
Theorem 5.3. Let Φ be a fully-connected depth-2 ReLU network with input dimension 2 and 4
hidden neurons parameterized by θ. Consider minimizing either the exponential or the logistic loss
using GF. There exists a dataset and an initialization θ(0) such that GF converges to zero loss, and
converges in direction to a KKT point θ of Problem 1, such that the weights of the first layer are not
a local optimum of Problem 3 w.r.t. θ.
Finally, we show that in ReLU networks in N of any depth, if the KKT point to which GF converges
in direction is such that the inputs to hidden neurons are non-zero, then it must be a local optimum
of Problem 3 (but not necessarily a global optimum). The proof follows the ideas from the proof of
Thm. 5.2, with some required modifications, and is given in Appendix C.11.
Theorem 5.4. Let m ≥ 2. Let Φ be any depth-m ReLU network in N parameterized by
θ = [u(l)]lm=1. Consider minimizing either the exponential or the logistic loss over a dataset
{(xi, yi)}in=1 using GF, and assume that there exists time t0 such that L(θ(t0)) < 1. Let
θ 二 [U (l)]m=ι be the KKT point of Problem 1 such that θ(t) converges to θ in direction (such θ
exists by Thm. 2.1). Let l ∈ [m] and assume that for every i ∈ [n] the inputs to all neurons in layers
≥ l in the computation Φ(θ; Xi) are non-zero. Then, the parameters vector U(l) is a local optimum
of Problem 3 w.r.t. θ. However, it may not be a global optimum.
6 Non-homogeneous networks
Let γ(θ) :二 mi□i∈[n] yiΦ (高;Xi) be the normalized margin. If Φ is homogeneous then maxi-
mizing γ(θ) is equivalent to solving Problem 1, i.e., minimizing |网| under the constraints (cf. Lyu
& Li (2019)). In this section we study the normalized margin in non-homogeneous networks.
Lyu & Li (2019) showed under the assumptions from Thm. 2.1, that a smoothed version of the
normalized margin is monotonically increasing when training homogeneous networks. More pre-
cisely, there is a function γ(θ) which is an O (∣∣θ∣∣-L)-additive approximation of γ(θ) such that
γ(θ) is monotonically non-decreasing. This result does not apply to non-homogeneous networks,
such as networks with skip connections or bias terms. Lyu & Li (2019) observed empirically that
the normalized margin is monotonically increasing also when training non-homogeneous networks.
However, they did not provide a proof for this phenomenon and left it as an open problem. Their
experiments are on training convolutional neural networks (CNN) with bias on MNIST. In the fol-
lowing theorem we show an example for a simple non-homogeneous network where the normalized
margin is monotonically decreasing during the training. This example implies that in order to obtain
such a result for non-homogeneous networks some additional assumptions must be made.
Theorem 6.1. Let Φ be a depth-2 linear network with input dimension 1, width 1 anda skip connec-
tion. Namely, Φ is parameterized by θ = [w, v, u] where w,v,u ∈ R, and Φ(θ; X) = V ∙ W ∙ X + U ∙ X.
Consider the size-1 dataset {(1, 1)}, and assume that θ(0) 二 [2, 2, 2]. Then, GF w.r.t. either the ex-
θ(t)
t-∞ ɪθ(m
ponential loss or the logistic loss converges to zero loss, converges in direction (i.e., lim
exists), and the normalized margin is monotonically decreasing during the training, i.e.,丽(θ(t)) < 0
for all t ≥ 0. Moreover, we have γ(θ(0)) > 0.9 and limt→∞ γ(θ(t)) = 2.
We note that the proof readily extends in a few directions: It applies also for a depth-2 network
without a skip connection but with a bias term in the output neuron. In addition, it also holds for
ReLU networks. Finally, the theorem applies also for the smoothed version of the normalized margin
considered in Lyu & Li (2019). The proof of the theorem is given in Appendix C.12. Intuitively,
note that if 向,荷=0 and kUk = 1 then γ(θ = 1, and if kUk = 0 and 向二荷=√2 then
γ(θ) = 2. Also, since the partial derivative of the loss w.r.t. v, W depends on w, V (respectively) and
on X, and the partial derivative w.r.t. u depends only on X, then v, w grow faster than u during the
training. Hence, as t increases,商 decreases and 向,尚 increase.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems, pp. 7413-7424, 2019.
Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake Woodworth, Nathan Srebro, Amir
Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond infinitesimal
mirror descent. arXiv preprint arXiv:2102.09769, 2021.
Mohamed Ali Belabbas. On implicit regularization: Morse functions and applications to matrix
factorization. arXiv preprint arXiv:2001.04264, 2020.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. Nonsmooth analysis and
control theory, volume 178. Springer Science & Business Media, 2008.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Sys-
tems, pp. 384-395, 2018.
Joydeep Dutta, Kalyanmoy Deb, Rupesh Tulshyan, and Ramnik Arora. Approximate kkt points and
a proximity measure for termination. Journal of Global Optimization, 56(4):1463-1499, 2013.
Armin Eftekhari and Konstantinos Zygalakis. Implicit regularization in matrix sensing: A geometric
view leads to stronger results. arXiv preprint arXiv:2008.12091, 2020.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. In Advances in Neural Information Processing Systems, pp.
3202-3211, 2019.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018b.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-
bro. Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018c.
Meena Jagadeesan, Ilya Razenshteyn, and Suriya Gunasekar. Inductive bias of multi-channel linear
convolutional networks with bounded weight norm. arXiv preprint arXiv:2102.12238, 2021.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018a.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018b.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. arXiv
preprint arXiv:2006.06657, 2020.
Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algo-
rithmic Learning Theory, pp. 772-804. PMLR, 2021.
ZiWei Ji, Miroslav Dudk Robert E Schapire, and MatUs Telgarsky. Gradient descent follows the
regularization path for general losses. In Conference on Learning Theory, pp. 2109-2136. PMLR,
2020.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2-47. PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent
for matrix factorization: Greedy low-rank learning. arXiv preprint arXiv:2012.09839, 2020.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex sta-
tistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion.
In International Conference on Machine Learning, pp. 3345-3354. PMLR, 2018.
Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee, Nati Srebro, and Daniel
Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. Ad-
vances in Neural Information Processing Systems, 33, 2020.
Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In
International Conference on Machine Learning, pp. 4683-4692. PMLR, 2019a.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd
International Conference on Artificial Intelligence and Statistics, pp. 3420-3428. PMLR, 2019b.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by
norms. arXiv preprint arXiv:2005.06398, 2020.
Andrzej Ruszczynski. Nonlinear optimization. Princeton university press, 2011.
Ohad Shamir. Gradient methods never overfit on separable data. arXiv preprint arXiv:2007.00028,
2020.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Gal Vardi and Ohad Shamir. Implicit regularization in relu networks with the square loss. In Con-
ference on Learning Theory, pp. 4224-4258. PMLR, 2021.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. arXiv
preprint arXiv:2002.09277, 2020.
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training
linear neural networks. arXiv preprint arXiv:2010.02501, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
A More related work
Soudry et al. (2018) showed that gradient descent on linearly-separable binary classification prob-
lems with exponentially-tailed losses (e.g., the exponential loss and the logistic loss), converges to
the maximum `2 -margin direction. This analysis was extended to other loss functions, tighter con-
vergence rates, non-separable data, and variants of gradient-based optimization algorithms (Nacson
et al., 2019b; Ji & Telgarsky, 2018b; Ji et al., 2020; Gunasekar et al., 2018a; Shamir, 2020; Ji &
Telgarsky, 2021).
11
Under review as a conference paper at ICLR 2022
As detailed in Section 2, Lyu & Li (2019) and Ji & Telgarsky (2020) showed that GF on homo-
geneous neural networks with exponential-type losses converge in direction to a KKT point of the
maximum margin problem in the parameter space. Similar results under stronger assumptions were
previously obtained in Nacson et al. (2019a); Gunasekar et al. (2018b). The implications of margin
maximization in the parameter space on the implicit bias in the predictor space for linear neural
networks were studied in Gunasekar et al. (2018b) (as detailed in Section 2) and also in Jagadeesan
et al. (2021). Margin maximization in the predictor space for fully-connected linear networks was
shown by Ji & Telgarsky (2020) (as detailed in Section 2), and similar results under stronger assump-
tions were previously established in Gunasekar et al. (2018b) and in Ji & Telgarsky (2018a). The
implicit bias in the predictor space of diagonal and convolutional linear networks was studied in Gu-
nasekar et al. (2018b); Moroshko et al. (2020); Yun et al. (2020). The implicit bias in infinitely-wide
two-layer homogeneous neural networks was studied in Chizat & Bach (2020).
Finally, the implicit bias of neural networks in regression tasks w.r.t. the square loss was also ex-
tensively studied in recent years (e.g., Gunasekar et al. (2018c); Razin & Cohen (2020); Arora et al.
(2019); Belabbas (2020); Eftekhari & Zygalakis (2020); Li et al. (2018); Ma et al. (2018); Wood-
worth et al. (2020); Gidel et al. (2019); Liet al. (2020); Yun et al. (2020); Vardi & Shamir (2021);
Azulay et al. (2021)). This setting, however, is less relevant for our work.
B Preliminaries on the KKT condition
Below we review the definition of the KKT condition for non-smooth optimization problems (cf.
Lyu & Li (2019); Dutta et al. (2013)).
Let f : Rd → R be a locally Lipschitz function. The Clarke subdifferential (Clarke et al., 2008) at
x ∈ Rd is the convex set
∂◦f (x) := Conv I lim Vf (Xi) I lim Xi = x, f is differentiable at xΛ .
i→∞	i→∞
If f is continuously differentiable at X then ∂o f (x) = {Vf (x)}.
Consider the following optimization problem
min f(X)	s.t. ∀n ∈ [N] gn(X) ≤ 0 ,	(4)
where f, g1 , . . . , gn : Rd → R are locally Lipschitz functions. We say that X ∈ Rd is a feasible
point of Problem 4 ifX satisfies gn(X) ≤ 0 for all n ∈ [N]. We say that a feasible point X is a KKT
point if there exists λ1 , . . . , λN ≥ 0 such that
1.	0 ∈ ∂°f (x) + En∈[N] λn∂°gn(x);
2.	For all n ∈ [N] we have λn gn (X) = 0.
C Proofs
C.1 Auxiliary lemmas
Throughout our proofs we use the following two lemmas from Du et al. (2018):
Lemma C.1 (Du et al. (2018)). Let m ≥ 2, and let Φ be a depth-m fully-connected linear or
ReLU network parameterized by θ = [W1, . . . , Wm]. Suppose that for every j ∈ [m] we have
Wj ∈ Rdj ×dj-1. Consider minimizing any differentiable loss function (e.g., the exponential or the
logistic loss) over a dataset using GF. Then, for every j ∈ [m - 1] at all time t we have
d (kWjkF-kWj+ιkF)=0.
Moreover, for every j ∈ [m - 1] and i ∈ [dj] we have
d (kWj[i, :]k2 -kWj+1 [:,i]k2) =0 ,
where Wj [i, :] is the vector of incoming weights to the i-th neuron in the j-th hidden layer (i.e.,
the i-th row of Wj), and Wj+1[:, i] is the vector of outgoing weights from this neuron (i.e., the i-th
column of Wj +1 ).
12
Under review as a conference paper at ICLR 2022
Lemma C.2 (Du et al. (2018)). Let m ≥ 2, and let Φ be a depth-m linear or ReLU network in N,
parameterized by θ = [u(1), . . . , u(m)]. Consider minimizing any differentiable loss function (e.g.,
the exponential or the logistic loss) over a dataset using GF. Then, for every j ∈ [m - 1] at all time
t we have
ddt Qu(j)}N")=0.
Note that Lemma C.2 considers a larger family of neural networks since it allows sparse and shared
weights, but Lemma C.1 gives a stronger guarantee, since it implies balancedness between the in-
coming and outgoing weights of each hidden neuron separately. In our proofs we will also need
to use a balancedness property for each hidden neuron separately in depth-2 networks with sparse
weights. Since this property is not implied by the above lemmas from Du et al. (2018), we now
prove it.
Before stating the lemma, let us introduce some required notations. Let Φ be a depth-2 network
in Nno-share. We can always assume w.l.o.g. that the second layer is fully connected, namely, all
hidden neurons are connected to the output neuron. Indeed, otherwise we can ignore the neurons
that are not connected to the output neuron. For the network Φ we use the parameterization θ =
[w1, . . . , wk, v], where k is the number of hidden neurons. For every j ∈ [k] the vector wj ∈ Rpj
is the weights vector of the j -th hidden neuron, and we have 1 ≤ pj ≤ d where d is the input
dimention. For an input x ∈ Rd we denote by xj ∈ Rpj a sub-vector of x, such that xj includes the
coordinates of x that are connected to the j -th hidden neuron. Thus, given x, the input to the j -th
hidden neuron is hwj, xji. The vector v ∈ Rk is the weights vector of the second layer. Overall,
we have Φ(θ; x) = Pj∈[k] vjσ(wj>xj ).
Lemma C.3. Let Φ be a depth-2 linear or ReLU network in Nno-share, parameterized by θ =
[w1, . . . , wk, v]. Consider minimizing any differentiable loss function (e.g., the exponential or the
logistic loss) over a dataset using GF. Then, for every j ∈ [k] at all time t we have
d (kwj- k2 - Vj) =0.
Proof. We have
L(θ) = X ` (yiφ(θ; Xi)) = X 'Iyi X Vlσ(WJXj)).
i∈[n]	i∈[n]	l∈[k]
Hence
d (kwjk2) = 2hwj, -tjii = -2hwj, VwjL(θ)i
dt	dt
=-2 X '0 (yi XVlσ(WJxi)) ∙ yivjσ0(w>Xj)Wjrχj
i∈[n]	l∈[k]
=-2 X 小 XVlσ(w>χi)) ∙ yivjσ(w>χj) .
i∈[n]	l∈[k]
Moreover,
d	dVj
dt (Vj) = 2vj Ir = -2vjνvjL(θ)
=-2vj X '卜y X °lσ(WJχi)) ∙ yiσ(w>χj).
i∈[n]	l∈[k]
Hence the lemma follows.	□
Using the above lemma, we show the following:
13
Under review as a conference paper at ICLR 2022
Lemma C.4. Let Φ be a depth-2 linear or ReLU network in Nno-share, parameterized by θ =
[w1, . . . , wk, v]. Consider minimizing any differentiable loss function (e.g., the exponential or the
logistic loss) over a dataset using GF starting from θ(0). Assume that limt→∞ kθ(t)k = ∞ and
that θ(t) converges in direction to θ = [W 1,..., Wk, v], i.e., θ = kθ∣∣∙ limt→∞ 口鲁(；)口. Then, for
every l ∈ [k] we have kW11∣ = ∣Vι |.
Proof. For every l ∈ [k], let ∆l = kWl(0)k2 - vl(0)2. By Lemma C.3, we have for every l ∈ [k]
and t ≥ 0 that kWl(t)k2 - vl(t)2 = ∆l, namely, the differences between the square norms of the
incoming and outgoing weights of each hidden neuron remain constant during the training. Hence,
We have	__________
r I	∣∣,5∣∣	1∙	∣Vl (t)| IIAl 1. VZkWl⑴k2 - Z
|vl | =	kθk	∙ t→∞	WI	= kθk	∙ t→∞ -kθW-.
Thus, if limt-∞ ∣∣wl(t)k = ∞, then we have |vl| = ∣∣θk ∙ limt-∞，湍"=∣∣wl∣∣.
一 ..	...... 一	.. -Ir 一 .. .	..	.. r..	U”,. f-ι-∖∖∖
Assume now that ∣wl(t)∣ → ∞. By the definition of θ we have ∣wl∣∣ = ∣θ∣ ∙ limt→∞ 1湍」1.
11 τn	k ^ʧl (t) k qy；CfC ClnrI 11 τn	____ kw than ka∖7Q 11 τn	k ^ʧl (t) k   (ɔ T4^anca
Since limt—>∞ k θ(古) k- exists and limt—>∞ ∣ θ (t) ∣ — oo, then We have limt—>∞ -k g(t) k— - o. ∏,ence,
limt→∞ kθ(t)1 = limt→∞ 小嗡才一包=0. Therefore ∣Wl∣ = Vl =0.	□
C.2 Proof of Thm. 3.1
Suppose that the network Φ is parameterized by θ = [W(1), . . . , W(m)]. By Thm. 2.1, GF converges
in direction to a KKT point θA = [WA (1), . . . , WA (m)] of Problem 1. For every l ∈ [m] let ∆l =
W(l)(0)2F - W(1)(0)2F. By Lemma C.1, we have for every l ∈ [m] and t ≥ 0 that
W (j+1)(0)2 - W (j)(0)2
Hence, we have
kθk ∙ ,lim
t→∞
IlW(I) (t)∣∣F
kθ(t)k
..~ ..
kθk ∙ ,lim
t→∞
J∣∣w ⑴⑴ IIF + ∆l
Since by Thm. 2.1 we have limt→∞ ∣θ(t)∣ = o, then limt→∞ IW(1)(t)IF = o, and we have
∣∣W(l)∣∣	= ∣θ∣ ∙ lim 11W(J(t)llF = IIWW⑴∣∣	:= C.
H IIf 11 11 t→∞	∣∣θ(t)∣	II IIf
By Ji & Telgarsky (2020) (Proposition 4.4), when GF on a fully-connected linear network w.r.t. the
exponential loss or the logistic loss converges to zero loss, then we have the following. There are
unit vectors v0 , . . . , vm such that
1. W (l)(t)
t→∞ ∣∣w(l)(t)∣∣F
vlvl>-1
for every l ∈ [m]. Moreover, we have vm = 1, and v0 = u where
>
u := arg max min yiu xi
kuk=1 i∈[n]
is the unique linear max margin predictor.
Note that we have
W (I)	W (I)	kθk∙limt→∞ wp(⅛
丁=ML=血丁、F
r W (l)(t)
t→∞ HW(I) (t)∣∣F
vlvl>-1 .
14
Under review as a conference paper at ICLR 2022
Thus, W(l) = Cvιv>-ι forevery l ∈ [m].
Let U = W(m) ∙... ∙ W⑴=Cmu. Since θ is a KKT point of Problem 1, We have for every l ∈ [m]
W(I) = X λiy 睦⑹ X ,
乙	∂W (l)
i∈[n]
where λi ≥ 0 for every i, and λ% = 0 if yiΦ(θ; Xi) = 1. Since W(I) are non-zero then there is i ∈ [n]
such that 1 = yiΦ(θ; Xi) = yiu 1 Xi = yQmu 1 Xi. Likewise, since θ satisfies the constraints of
Problem 1, then for every i ∈ [n] we have 1 ≤ yiΦ(θ; Xi) = yQmu 1 Xi. Since, U is a unit vector
that maximized the margin, then we have
∣∣Uk = Cm = min ∣∣u0k s.t. yiU0>Xi ≥ 1 for all i ∈ [n] .	(5)
A	.	1	.	1 ∙	.1	. .1	♦	Λ !	∙ .1	I I	/ʌ/ I I	I I Λ I I	. 1	.	. ∙	Γ∙ .1	.	∙	♦ C Λ
Assume toward contradiction that there is θ0 with ∣θ0 ∣ < ∣θ ∣ that satisfies the constraints in Prob-
lem 1. Let u0 = W0(m) ∙ ... ∙ W0(1). By Eq. 5 we have ∣∣u0k ≥ ∣∣U∣∣ = Cm. Moreover, we have
∣∣u0k = ∣∣W 0(m) ∙ ... ∙ WO(I) Il ≤ Qι∈[m] ∣∣WO(I) ∣∣f due to the SUbmU山PIiCatiVity of the Frobenius
norm. Hence Ql∈[m] W 0(l) F ≥ Cm. The following lemma implies that
∣θo∣2 = x∣∣wo(I)UF ≥ m	C 2=x∣∣w (I)UF=∣∣θ∣∣2
l∈[m]	l∈[m]
in contradiction to our assumption, and thus completes the proof.
Lemma C.5. Let a1, . . . , am be real numbers such that j∈[m] aj ≥ Cm for some C ≥ 0. Then
Pj∈[m] a2 ≥ m ∙ C 2 ∙
Proof∙ It suffices to prove the claim for the case where j∈[m] aj = Cm. Indeed, if j∈[m] aj >
Cm then we can replace some aj with an appropriate aOj such that |aOj | < |aj | and we only decrease
Pj∈[m] aj2. Consider the following problem
min I X a： s.t. Y aj = Cm .
j∈[m]	j∈[m]
Using the Lagrange multipliers we obtain that there is some λ ∈ R such that for every l ∈ [m]
we have aι = λ ∙ Qj=ι aj. Thus, a2 = λ ∙ Qj∈[m] aj. It implies that a2 = ... = am. Since
Qj∈[m] aj = Cm then ∣aj∙∣ = C for every j ∈ [m]. Hence, Pj-∈[m] aj = mC2.	□
C.3 Proof of Thm. 3.2
Consider an initialization θ(0) is such that w1 (0) satisfies hw1 (0), X1i > 0 and hw1 (0), X2i > 0,
and w2 (0) satisfies hw2 (0), X1i < 0 and hw2 (0), X2i < 0. Moreover, assume that v1 (0) > 0.
Note that for every θ such that hw2, X1i < 0 and hw2, X2i < 0 we have
22
Vw2L(θ) = X '0(yiΦ(θ; Xi)) ∙ yiVw2Φ(θ; Xi)= X '0(yiΦ(θ; Xi)) ∙ nNw2 v1σ(w1>Xi) + v2σ(w2>Xi)
i=1	i=1
2
=E'0(yiΦ(θ; Xi)) ∙ yiV2σ0(w>Xi闲=0 .
i=1
and
22
Vv2L(θ) = X '0(yiΦ(θ; Xi)) ∙ yiVv2Φ(θ; Xi) = X '0(yiΦ(θ; Xi)) ∙ nW? [vισ(w>Xi) + vjσ(w>Xi)]
i=1	i=1
2
=X '0(yiΦ(θ; Xi)) ∙ yiσ(w>Xi) = 0 .
i=1
15
Under review as a conference paper at ICLR 2022
Hence, w2 and v2 get stuck in their initial values. Moreover, we have
22
▽viL(θ) = X '(yΦ(θ; Xi)) ∙ yiVvι [vισ(w>Xi) + v2σ(w>xj] = X '(yiΦ(θ; Xi)) ∙ σ(w>xj ≤ 0 .
i=1	i=1
Therefore, for every t ≥ 0 we have v1 (t) ≥ v1 (0) > 0.
We denote w1 = (w1[1], w1 [2]). Since hw1(0), Xji > 0 for j ∈ {1, 2} then w1[2](0) > 0. Assume
w.l.o.g. that w1[1](0) ≥ 0 (the case where w1[1](0) ≤ 0 is similar). For every w1 that satisfies
w1[2] ≥ 0 and 0 ≤ w1 [1] ≤ w1 [1](0) we have hw1, X1i > hw1, X2i > 0. Thus,
2
▽wi L(θ) = X'0(yiΦ(θ; Xi)) ∙ yiVw] [vισ(w>Xi) + v2σ(w>xj]
i=1
2
=E'0(yi(vισ(w>Xi) + 0)) ∙ yiV1σ0(w>Xi)xi
i=1
2
=E'0(viw>Xi) ∙ VlXi .
i=1
Since '0 is negative and monotonically increasing, and since viw>xι > vιw>X2, then dW,1] ≤ 0.
Also, dWt[2] > 0. Moreover, if wi [1] = 0 then viw>xi = v1w>x2 and thus dW；* 1] = 0. Hence,
for every t we have w1[2](t) ≥ w1[2](0) > 0 and 0 ≤ w1[1](t) ≤ w1[1](0).
If L(θ) ≥ 1 then for some i ∈ {1, 2} We have '(yiΦ(θ; Xi)) ≥ 11 and hence '(yiΦ(θ; Xi)) ≤ C for
some constant c < 0. Since we also have vi ≥ vi(0) > 0, we have
dw■回 ≥-c ∙ Vi(0) ∙ 1 .
dt -	1() 4
Therefore, if the initialization θ(0) is such that L(θ) ≥ 1 then wi [2](t) increases at rate at least
(-c)4vι(0) while wι[1](t) remains in [0,wι[1](0)]. Note that for such wi[1] and vi ≥ vi(0) > 0, if
wi[2] is sufficiently large then We have vi hwi, Xii ≥ 1 for i ∈ {1, 2}. Hence, there is some t0 such
that L(θ(to)) ≤ 2'(1) < 1 for both the exponential loss and the logistic loss.
Therefore, by Thm. 2.1 GF converges in direction to a KKT point of Problem 1, and we have
limt→∞ L(θ(t)) = 0 and limt→∞ kθ(t)k = ∞. It remains to show that it does not converge in
direction to a local optimum of Problem 1.
Let θ = limt→∞ 尚(；)口. We denote θ = [wi, W 1,Vi, V1]. We show that Wi = √11 (0,1)>, vi =
√12, W2 = 0 and V1 = 0. By Lemma C.1, we have for every t ≥ 0 that vi(t)2 - kwi(t)k2 =
vi(0)2 - kwi(0)k2 := ∆. Since for every t we have w2(t) = w2(0) and v2(t) = v2(0), and since
limt→∞ kθ(t)k = ∞ then we have limt→∞ kwi(t)k = ∞ and limt→∞ |vi(t)| = ∞. Also, since
limt→∞ kwi(t)k = ∞ and wi[1](t) ∈ [0, wi [1](0)] then limt→∞ wi[2](t) = ∞. Note that
kθ(t)k = √kwi(t)k2 + vi(t)2 + kw1(0)k2 + V2(0)2 = J∆ + 2 kwi(t)k2 + kw1(0)k2 + V2(0)2 .
Since wi [1](t) ∈ [0, wi [1](0)] and kθ(t)k → ∞, we have
wi [1](t)
Wi[1] = lim UzW 、“ = 0 .
i[] t→∞ kθ(t)k
Moreover,
W i[2]
lim w≡
t→∞ kθ(t)k
㈣ f∆ + 2(wi[1](t))2 + 2((Wzt))))22+ kw2(0)； =√2
and
lim
t→∞
W2(t)
lim w2≡
t→∞ kθ(t)k
0.
W 2
16
Under review as a conference paper at ICLR 2022
Finally, by Lemma C.4 and since vι(t) > 0, We have vι = ∣∣Wιk =表.By Lemma C.4 We also
have ∣V21 = IIW2k = 0.
Next, We shoW that θ does not point at the direction of a local optimum of Problem 1. Let θ =
[w 1, W2, vι, v2] be a KKT point of Problem 1 that points at the direction of θ. Such θ exists since
θ(t) converges in direction to a KKT point. Thus, we have W2 = 0, V2 = 0, W1 = α(0,1)> and
vι = α for some α > 0. Since θ satisfies the KKT conditions, we have
2
W1 = X λ"wι
i=1
2
(yiΦ(θ; Xi)) = X λiy (V1σ0(W>Xi)x,,
i=1
1	、	/-∖ ∙ Γ∙	T . Λ	∖	! Λ TL T .	. 1	. . 1 TT- TT--I'	∙>♦,♦	1	111	.	. 1 Z-XI 1
where λi ≥ 0 and λi = 0 if yiΦ(θ; Xi) 6= 1. Note that the KKT condition should be w.r.t. the Clarke
subdifferential, but since W> Xi > 0 for i ∈ {1, 2} then we use here the gradient. Hence, there is
i ∈ {1, 2} such that yiΦ(θ; Xi) = 1. Thus,
2
1 = yiΦ(θ; Xi) = Vισ(W>Xi) + V2σ(W>Xi) = α ∙ 4 + 0= ɪ
Therefore, α = 2 and we have W1 = (0, 2)> and Vι = 2.
In order to show that θ is not a local optimum, we show that for every 0 < 0 < 1 there exists some
θ0 such that ∣∣θ0 一 θ∣∣ ≤ e0, θ0 satisfies Φ(θ0; Xi) ≥ 1 for every i ∈ {1, 2}, and ∣∣θ0k < kθ∣. Let
E =卷 < 2. Let θ0 = [w1 , W02, v1 ,v2] be such that W∖ = (2,2 一 2e)>, w2 = (-√2e, 0)>, v1 = 2
and v2 = √2e. Note that
φ(θ0; XI) = 2 ∙ σ ((2,2 - 2E)(I, G>) + √2e ∙ σ ^(-√2e, O)(1, 4)>
=2 ∙σ (E + 1-E) + √ ∙σ(-√E) = I,
and
φ(θ0; X2) = 2 ∙σ ((2,2 一 2E)(—1,4)>) + √2e ∙σ (j√2e,0)(-1,4)>
=2 ∙ σ (一2E + 1 — E) + √2E ∙ σ (√2E) = 1 一 2e + 2e = 1 .
We also have
∣∣θ0 一 θ∣∣ = ||w； — W 1∣2 + ∣∣w2 一 W 2∣2 + (v1 一 v1 )2 + (v2 一 V2)2
=(W + 4e2) + 2e + 0 + 2e < 9e = E02 .
Finally, we have
∣∣θ0k2 = E2 +4 一 8e + 4e2 + 2e + 4 + 2e = 8 一 4e + 1^ < 8 一 4e + ɪ < 8 = ∣∣θ∣∣2 .
Thus, kθ0k < I∣θ∣∣.
C.4 Proof of Thm. 4.1
Let X = (1, 2)> and y = 1. Let θ(0) such that W1(0) = W2(0) = (1, 0)>. Note that L(θ(0)) =
'(1) < 1 for both linear and ReLU networks with the exponential loss or the logistic loss, and
therefore by Thm. 2.1 GF converges in direction to a KKT point θ of Problem 1, and we have
limt→∞ L(θ(t)) = 0 and limt→∞ ∣θ(t)∣ = ∞. We denote W1 = (W1 [1], W1[2])> and W2 =
(W2 [1], W2 [2])>. Note that the initialization θ(0) is such that the second hidden neuron has 0 in
both its incoming and outgoing weights. Hence, the gradient w.r.t. W1 [2] and W2 [2] is zero, and the
second hidden neuron remains inactive during the training. Moreover, W1 [1] and W2 [1] are strictly
17
Under review as a conference paper at ICLR 2022
increasing. Also, by Lemma C.3 We have for every t ≥ 0 that wι[1](t)2 = w2[l](t)2. Overall, θ
is such that W1 = W2 = (1,0)>. Note that since the dataset is of size 1, then every KKT point of
Problem 1 must label the input x With exactly 1.
It remains to shoW that θ is not local optimum. Let 0 <	< 1, and let θ0 = [w01, w20 ] With
Wi = w2 = (√1 - e, p∕l∣) . Note that θ0 satisfies the constraints ofProblem 1, since y∙Φ(θ0; x)=
1 - E + 2 ∙ j = 1. Moreover, we have kθ∣∣2 = 2 and ∣∣θ0k2 = 2(1 - E + 2) = 2 - E and therefore
~
kθ0k < kθ∣.
C.5 Proof of Thm. 4.2
C.5. 1 Proof of part 1
We assume w.l.o.g. that the second layer is fully-connected, namely, all hidden neurons are con-
nected to the output neuron, since otherwise we can ignore disconnected neurons. For the network
Φ we use the parameterization θ = [W1, . . . , Wk, v] introduced in Section C.1. Thus, we have
φ⑹ x) = Pι∈[k] vlw>χl.
By Thm. 2.1, GF converges in direction to θ = [wι,..., Wk, v] which satisfies the KKT conditions
of Problem 1. Thus, there are λ1 , . . . , λn such that for every j ∈ [k] we have
Wj = X λ"wj (yiΦ(θ; Xi)) = X λiyiVjXj ,	(6)
i∈[n]	i∈[n]
and we have λi ≥ 0 for all i, and λi = 0 if y%Φ(θ; Xi) = yi £k肉 ViW>Xi = 1. By Thm. 2.1, we
also have limt→∞ ∣∣θ(t)∣ = ∞. Hence, by LemmaC.4 we have ∣Wj∣ = |Vj| for all j ∈ [k].
Consider the following problem
min X ∣ul ∣	s.t.	∀i ∈ [n] yi X ul>Xli ≥ 1 .	(7)
l∈[k]	l∈[k]
For every l ∈ [k] we denote Ui = Vl ∙ Wi. Since we assume that Wi = 0 for every l ∈ [k], and
since ∣Wl∣∣ = |Vl|, then Ul = 0 for all l ∈ [k]. Note that since Wι,..., Wk, V satisfy the constraints
in Problem 1, then Uι,..., Uk satisfy the constraints in the above problem. In order to show that
Uι,..., Uk satisfy the KKT condition of the problem, we need to prove that for every j ∈ [k] we
have
for some λ0i ≥ 0 such that λ0i
every l ∈ [k], we have
Uj
∣uj k
λ0iyiXij
i∈[n]
0 if y Pl∈[k] U>xi = 1. From Eq. 6 and since ∣Wl ∣∣
(8)
|Vl| for
Uj =	vj	∙Wj = Vj E %yivjχj	= v2,E λiyiχj	= kvjwjk	E λiyiχj	= IIujk E	λiyiχj.
i∈[n]	i∈[n]	i∈[n]	i∈[n]
Note that we have λi ≥ 0 for all i, and λi = 0 if y% £仁肉 U>Xi = yi Pl∈[k]而W>Xi = 1. Hence
Eq. 8 holds with λ01 , . . . , λ0n that satisfy the requirement. Since the objective in Problem 7 is convex
and the constraints are affine functions, then its KKT condition is sufficient for global optimality.
Namely, Uι,..., Uk are a global optimum for problem 7.
We now deduce that θ is a global optimum for Problem 1. Assume toward contradiction that there
is a solution θ0 = [W10 , . . . , W0k, v0] for the constraints in Problem 1 such that ∣θ0∣2 < kθk2. Let
U0l = Vl0Wl0 . Note that the vectors U0l satisfy the constraints in Problem 7. Moreover, we have
X kulk	= X	∣v0l∙kw0k	≤ X	1 (M2 +	kwlk2)	= 1 kθ0k2	< 1 眄『=X 2 (∣vlI2	+ kwlk2).
l∈[k]	l∈[k]	l∈[k]	l∈[k]
Since kWlk = ∣Vl∣,theabove equals
X kwlk2 = X IVlI∙kwlk = X kUlk ,
l∈[k]	l∈[k]	l∈[k]
which contradicts the global optimality of U1,..., Uk.
18
Under review as a conference paper at ICLR 2022
C.5.2 Proof of part 2
Let {(Xi, yi)}i4=1 be a dataset such that yi = 1 for all i ∈ [4] and we have X1 = (0, 1)τ,
X2 = (1, 0)τ, X3 = (0, 一1) and X4 = (一1, 0). Consider the initialization θ(0) =
[W1(0), W2(0), W3(0), W4(0), v(0)] such that Wi(0) = 2Xi and Vi(0) = 2 for every i ∈ [4].
Note that L(θ(0)) = 4'(4) < 1 for both the exponential loss and the logistic loss, and there-
fore by Thm. 2.1 GF converges in direction to a KKT point θ of Problem 1, and we have
limt→∞ L(θ(t)) = 0 and limt→∞ ∣θ(t)∣ = ∞.
We now show that for all t ≥ 0 we have Wi(t) = α(t)Xi and Vi(t) = α(t) where α(t) > 0 and
limt→∞ α(t) = ∞. Indeed, for such θ(t), for every j ∈ [4] we have
4
4
dwj
d
VwjL(θ) = E'0(yiΦ(θ; Xi)) ∙ yiVwjΦ(θ; Xi) = f'
i=1
4
'0(a2) ∙ α ∙ ɪ2σ0(w>Xi)Xi = '0(α2) ∙ αXj ,
i=1
i=1
(X vισ(WJXi)) ∙ (vjσ0(WjrXi)Xi)
and
dvj
dt
Vv∕(θ) = E'0(yiΦ(θ; Xi)) ∙ yVvjΦ(θ; Xi) = f'
i=1
4
'0(α2) ∙ ^Xσ(w>Xi) = '(α2) ∙ α .
i=1
(X vισ(w>Xi)) ∙ σ(w>Xi)
i=1
Moreover, since limt→∞ kθ(t)k = ∞ then limt→∞ α(t) = ∞.
Hence, the KKT point θ is such that for every j ∈ [4] the vector Wj∙ points at the direction Xj, and
We have Vj = ∣∣Wj∙ ∣∣. Also, the vectors Wι, W2, W3, W4 have equal norms. That is, Wj∙ = dXj∙ and
Vj = α for some α > 0. Moreover, since it satisfies the KKT condition of Problem 1, then we have
4
Wj= EλijiVwjΦ(θ; Xi),
i=1
where λi ≥ 0 and λi = 0 if yiΦ(θ; Xi) 6= 1. Hence, there is i such that yiΦ(θ; Xi) = 1.Therefore,
a2 = 1. Thus, we conclude that for all j ∈ [4] we have Wj = Xj and Vj = 1. Note that Wj = 0 for
all j ∈ [4] as required.
Next, we show that θ is not a local optimum of Problem 1. We show that for every 0 < < 1 there
exists some θ0 such that ∣∣θ0 一 θ∣∣ ≤ e, θ0 satisfies the constraints of Problem 1, and ∣∣θ0k < kθ∣.
Let e0 = 2√2. Let θ0 be such that Vj = Vj = 1 for all j ∈ [4], and we have w1 = (e0,1 一 e0)τ,
W20 = (1 - 0, -0)r, W30 = (-0, -1 + 0)r and W40 = (-1 + 0, 0)r. It is easy to verify that θ0
satisfies the constraints. Indeed, we
θ0 - θ
Il = √4 ∙ 2e02 = 2√2e0 = e.
have Φ(θ0; Xi) = ((1 - 0) + 0 + 0 + 0) = 1. Also, we have
Finally,
∣∣θ0k2 = 4 ∙ (e02 + (1 一 e0)2) +4 = 8 + 8e0 (e0 一 1) < 8 = ∣∣θ∣∣2 .
C.6 Proof of Thm. 4.3
We assume w.l.o.g. that the second layer is fully-connected, namely, all hidden neurons are con-
nected to the output neuron, since otherwise we can ignore disconnected neurons. For the network
Φ we use the parameterization θ = [W1, . . . , Wk, v] introduced in Section C.1. Thus, we have
Φ(θ; X) = Pl∈[k] Vlσ(WlτXl).
~ -
We denote θ = [w 1,..., Wk, v]. Since θ is a KKT point of Problem 1, then there are λι,..., λn
such that for every j ∈ [k] we have
Wj= X λiVwj (yiΦ(θ; Xi)) = X λiyiVjσ0(W>Xj)Xj ,
i∈[n]
i∈[n]
(9)
—
—
〜
4
4
〜
〜
〜
19
Under review as a conference paper at ICLR 2022
and We have λ% ≥ 0 for all i, and λ% = 0 if yiΦ(θ; Xi) = y% £丘肉 Vισ(W>Xi) = 1. Note that the
KKT condition should be w.r.t. the Clarke subdifferential, but since for all i,j we have W>Xi = 0 by
our assumption, then We can use here the gradient. By Thm. 2.1, We also have limt→∞ kθ(t)k = ∞.
Hence, by Lemma C.4 we have ∣∣Wi ∣∣ = |Vi| for all j ∈ [k].
For i ∈ [n] and j ∈ [k] let Aij = 1(W>Xj ≥ 0). Consider the following problem
min	∣ul	∣ s.t.	∀i	∈	[n]	yi	Ail	ul>Xli	≥ 1 .	(10)
l∈[k]	l∈[k]
For every l ∈ [k] let Uι = Vl ∙ Wι. Since we assume that the inputs to all neurons in the computations
Φ(θ; Xi) are non-zero, then we must have W ι = 0 for every l ∈ [k]. Since we also have ∣w 11∣ = ∣vι∣,
then U ι = 0 for all l ∈ [k]. Note that since W ι,..., W k, V satisfy the constraints in Probelm 1, then
Uι,..., Uk satisfy the constraints in the above problem. Indeed, for every i ∈ [n] we have
y E AiIU>Xi = y E I(WJXi ≥ 0)vιW>Xi = yi £ vισ(W>Xi) ≥ 1.
ι∈[k]	ι∈[k]	ι∈[k]
In order to show that U1,..., Uk satisfy the KKT condition of Probelm 10, we need to prove that for
every j ∈ [k] we have
⅛=XxyiAijXj
(11)
for some λ1,..., λ^ such that for all i we have λi ≥ 0, and λ[ = 0 if yi £k网 AiιU>Xi = 1. From
Eq. 9 and since ∣Wι∣ = ∣Vι| for every l ∈ [k], we have
Uj =	Vj，Wj	= vj SXJ	λiyivj AijXi= Vj SXJ λiyiAijXi	= IIvjWj k SXJ λiyiAijXi	= IlUj k	SXJ	λiyiAijXi	.
i∈[n]	i∈[n]	i∈[n]	i∈[n]
Note that we have λi ≥ 0 for all i, and λi = 0 if
yi X AiIU>Xi = yi X vι1(W>Xi ≥ 0)W>Xi = yi X vισ(W>Xi) = 1.
ι∈[k]	ι∈[k]	ι∈[k]
Hence Eq. 11 holds with λ01, . . . , λ0n that satisfy the requirement. Since the objective in Problem 10
is convex and the constraints are affine functions, then its KKT condition is sufficient for global
optimality. Namely, U1,..., U k area global optimum for Problem 10.
We now deduce that θis a local optimum for Problem 1. Since for every i ∈ [n] and l ∈ [k] we
have W>Xi = 0, then there is e > 0, such that for every i, l and every W0 with kW0 一 W11∣ ≤ E
we have 1(W>Xi ≥ 0) = 1(W1τXi ≥ 0). Assume toward contradiction that there is a solution
θ0 = [w1 ,..., Wk, V0] for the constraints in Problem 1 such that kθ0 ― θk ≤ E and kθ0k2 < kθk2.
Note that we have kW0 一 Wιk ≤ E for every l ∈ [k]. We denote Ul = v；w；. The vectors u；, ..., Uk
satisfy the constraints in Problem 10, since we have
yi X AiIUlTXi = yi XI(W>Xi ≥ 0)v0W0TXi = yi X I(WOTXi ≥ 0)v0W0TXi
ι∈[k]	ι∈[k]	ι∈[k]
= yi X Vι0σ(Wι0TXιi) ≥ 1 ,
ι∈[k]
where the last inequality is since θ0 satisfies the constraints in Probelm 1. Moreover, we have
X ku；k = X ETkWOk ≤ X 1 (M2 + kW』2) = 1 kθ0k2 < 1	=X 2 (∣VιI2 + kWιk2).
ι∈[k]	ι∈[k]	ι∈[k]	ι∈[k]
Since kWιk = ∣Vι∣,theabove equals
X kWιk2 = X ∣Vι∣∙kWιk = X kUιk ,
ι∈[k]	ι∈[k]	ι∈[k]
which contradicts the global optimality of Uι,..., Uk.
It remains to show that θmay not be a global optimum of Problem 1, even if the network Φ is fully
connected. The following lemma concludes the proof.
20
Under review as a conference paper at ICLR 2022
Lemma C.6. Let Φ be a depth-2 fully-connected ReLU network with input dimension 2 and two
hidden neurons. Consider minimizing either the exponential or the logistic loss using GF. Then,
there exists a dataset {(xi, yi)}in=1 and an initialization θ(0), such that GF converges to zero loss,
converges in direction to a KKT point θ = [w ι, W 2, v] of Problem 1 such that X j, Xii = 0 for all
j ∈ {1, 2} and i ∈ [n], and θ is not a global optimum.
Proof Let xl = (1,	4)	, χ2 =	(-1,	4)	, χ3 = (O, -1), y1 =	y2	=	y3	=	1.	Let
{(X1, y1), (X2, y2), (X3, y3)} be a dataset. Consider the initialization θ(0) such that w1(0) = (0, 3),
vι(0) = 3, w2(0) = (0,-2) and v2(0) = 2. Note that L(θ(0)) = 2' (44) + '(4) < 1 for both the
exponential loss and the logistic loss, and therefore by Thm. 2.1 GF converges in direction to a KKT
point θ of Problem 1.
Note that for θ such that wι = α ∙ (0,1)> and W2 = β ∙ (0, -1)> for some a,β > 0, and vι, v > 0,
we have
33
VwiL(θ) = X '0(yiΦ(θ; Xi)) ∙ yiVwι Φ(θ; Xi) = X ' (vισ(w>xi) + v2σ(w>Xi)) ∙ v1σ0(w>Xi)xi
i=1	i=1
22
=X '0(v1 σ(w>Xi)) ∙ ViXi = v1'0 }1 £) X Xi ,
i=1	i=1
and
33
VviL(θ) = X '0(yiΦ(θ; Xi)) ∙ yiVviΦ(θ; Xi) = X '0(yiΦ(θ; Xi)) ∙ σ(w>Xi).
i=1	i=1
Hence, -Vwi L(θ) points in the direction (0, 1)> and -VviL(θ) > 0. Moreover, we have
33
Vw2L(θ) = X '0(yiΦ(θ; Xi)) ∙ yiVw2Φ(θ; Xi) = X '0(v1σ(w>Xi) + v2σ(w>Xi)) ∙ v2σ0(w>Xi)Xi
i=1	i=1
='0(v2σ(w>X3)) • v2X3 = v2'0(v2β)X3 ,
and
33
Vv2L(θ) = X'0(yiΦ(θ; Xi)) ∙ yiVv2Φ(θ; Xi) = X'0(v1σ(w>Xi) + V2σ(w>Xi)) ∙ σ(w>Xi)
i=1	i=1
='0(v2σ(w>X3))σ(w>X3) = '0(v2β) • β .
Therefore, -Vw2 L(θ) points in the direction (0, -1)> and -Vv2 L(θ) > 0. Hence for every t
we have w1 (t) = α(t) • (0, 1)> for some α(t) > 0 and v1(t) > 0. Also, we have w2 (t) =
β(t) • (0, -1)> for some β(t) > 0 and v2(t) > 0. By Lemma C.1, we have for every t ≥ 0 that
kw1(t)k2 - v1(t)2 = kw1(0)k2 - v1(0)2 = 0 and kw2(t)k2 - v2(t)2 = kw2(0)k2 - v2(0)2 = 0.
Hence, We have vi(t) = α(t) and v2(t) = β(t). Therefore, We have W1 = α • (0,1)> and Vi = α
>
for some a ≥ 0. Likewise, we have W2 = β • (0, -1)1 and v2 = β for some β ≥ 0. Since θ satisfies
the constraints in Probelm 1, then a ≥ 2 and β ≥ 1. Note that(wj, Xii = 0 for all j ∈ {1,2} and
i∈{1,2,3}.
We now show that there exists a solution θ0 to Problem 1 with a smaller norm, and hence θ is not a
global optimum. Let θ0 = [w0i, w20 , v0] such that w1 = α⅛, v1 = α, w2 = β. • (-4, -1), and
V20 = β. It is easy to verify that θ0 satisfies the constraints in Problem 1, and we have
kθ0k2 = J2 + α2 + ɪ (25 + 1)+ β2 < 1 + α2 + 1 ∙ 3 + β2 <β2 + α2 + α2 + β2 = |例2.
α2	β2	16	4
□
21
Under review as a conference paper at ICLR 2022
C.7 PROOF OF THM. 4.4
Let X =(4,表,-4,表)and y = 1. Let θ(0) = [w(0), v(0)] where w(0) = (0,1)> and
v(0)=(表,表).Note that Ψ(θ(0); x) = 1 and hence L(θ(0)) < 1 for both the exponential
〜
loss and the logistic loss. Therefore, by Thm. 2.1 GF converges in direction to a KKT point θ of
Problem 1, and we have limt→∞ L(θ(t)) = 0 and limt→∞ ∣∣θ(t)∣∣ = ∞.
The symmetry of the input x and the initialization θ(0) implies that the direction of w does not
change during the training, and that we have vι(t) = v (t) > 0 for all t ≥ 0. More formally, this
claim follows from the following calculation. For j ∈ {1,2} we have
VvjL(θ) = 20(yΦ(θ; x)) ∙ yVvjΦ(θ; x) = 20(yΦ(θ; x)) ∙ σ(w>x⑶).
Moreover,
VwL(θ) = '0(yΦ(θ; x))∙yVwΦ(θ; x) = 20(yΦ(θ; x))∙ (vισz(wτx(1))x(1) + V2σz(wτx(2))x(2)).
Hence, if vι = v2 > 0 and W points in the direction (0,1)τ, then it is easy to verify that v^ɪ L(θ)=
VviL(θ) < 0 and that VWL(θ) points in the direction of -(X⑴ + x(2)) = -(0, √2)τ. Further-
more, by Lemma C.2, for every t ≥ 0 we have ∣∣w(t)∣∣2 -∣∣v(t)∣∣2 = ∣∣w(0)∣∣2 - ∣∣v(0)∣∣2 = 0.
一	一	一	____ ɪ	「一 一J .	一 一	一	.	.	.	'一 一、T 一	一	一
Therefore, the KKT point θ = [w, v] is such that W points at the direction (0,1)τ, v1 = v2 > 0,
and ∣∣w∣∣ = ∣∣v∣∣. Since θ satisfies the KKT conditions of Problem 1, then we have
W = λVw (yφ(θ; x)),
where λ ≥ 0 and λ = 0 if yΦ(θ; x) = 1. Hence, we must have yΦ(θ; x) = 1. Letting Z := v1 = v2
and using 2z2 = ∣∣v∣2 = ∣w∣∣2 = W2, we have
1 = vισ(wT x(1)) + v2σ(w TX(2)) = ZW TX(I) + ZW TX(2)= ZWT
(x(1)+ x(2)) = Z ∙ W2√2
√2 ∙ w2√2=走.
Therefore, W = (0,1)τ and V
.Note that we have(W, x(1))= 0 and(W, x(2))= 0.
It remains to show that θ is not a local optimum of Problem 1. We show that for every 0 < E < 1
there exists some θ0 = [w0, v0] such that ∣∣θ0 - θ∣∣ ≤ E, θ0 satisfies the constrains in Problem 1, and
~	02
∣∣θl∣< ∣θ∣. Let E =%
Note that
2
∈ (0,1/2), and let w0 = (√e, 1 - E)T and v0
2
〜
. 〜
θ0 - θ
=2-E+E2
Moreover,
设『=IIWil2 + IIV
E
-2e + 1 + -
2
Finally, we show that θ0 satisfies the constraints:
Φ(θ0; x) = v1 σ(w0TX(I)) + v2σ(w,τx(2))
=(√2+ 号)(4√e+√2 ∙(1 -E))+ (√2- √E) (-4w+√2 Yj))
=√2，(1-E) (√2 + √e + √2 -√e ) +4√e (√2 + √E -√2 + √τ)
=1 - E + 4e =1 + 3e ≥ 1 .
22
Under review as a conference paper at ICLR 2022
C.8 Proof of Thm. 5.1
Let x = (1, 1)> and y = 1. Consider the initialization θ(0) = [w1(0), . . . , wm(0)], where wj (0) =
(1,1)> for every j ∈ [m]. Note that L(θ(0)) = '(2) < 1 for both linear and ReLU networks with
the exponential loss or the logistic loss, and therefore by Thm. 2.1 GF converges in direction to a
KKT point θ of Problem 1, and we have limt→∞ L(θ(t)) = 0 and limt→∞ kθ(t)k = ∞. It remains
to show that it does not converge in direction to a local optimum of Problem 1.
From the symmetry of the network Φ and the initialization θ(0), it follows that for all t the network
Φ(θ(t); ∙) remains symmetric, namely, there are aj(t) such that Wj(t) = (αj(t), αj(t)). Moreover,
by Lemma C.2, for every t ≥ 0 and j, l ∈ [m] we have αj(t) = αl(t) := α(t). Thus, GF converges
in direction to the KKT point θ = Wι,..., Wm] such that Wj = (2-1∕m, 2-1/m)> for all j ∈ [m].
Note that since the dataset is of size 1, then every KKT point of Problem 1 must label the input x
with exactly 1.
We now show that θ is not a local optimum of Problem 1. The following arguments hold for both
linear and ReLU networks. Let 0 < e < 2. Let θ0 = [wɪ,..., wm] such that for every j ∈ [m] We
have wj =((号)1/m ,(宁)1/m) .Wehave
y", X)=(中) + (*) = 1.
Hence, θ0 satisfies the constraints in Problem 1. We now show that for every sufficiently small > 0
we have ∣∣θ0k2 < kθ∣∣2. Weneedtoshowthat
m
W V + m
< 2m
Therefore, it suffices to show that
(1 + )2/m +(1 - )2/m <2.
Let g : R → R such that g(s) = (1 + s)2/m + (1 -s)2/m. We have g(0) = 2. The derivatives ofg
satisfy
g0(S) = 2 (1 + S)焉-1 ———(1 — S) mm-1 ,
mm
and
g00(s) = —(— — 1) (1 + S)m-2 + — f— — 1) (1 — S)m-2 .
mm	mm
Since m ≥ 3 we have g0 (0) = 0 and g00 (0) < 0. Hence, 0 is a local maximum of g. Therefore for
every sufficiently small > 0 we have g() < 2 and thus ∣θ0∣2 <kθk2.
Finally, note that the inputs to all neurons in the computation Φ(θ; X) are positive.
C.9 Proof of Thm. 5.2
By Thm. 2.1 GF converge in direction to a KKT point θ = [u(l)]m=ι of Problem 1. We now show
that for every layer l ∈ [m] the parameters vector U(l) is a global optimum of Problem 3 w.r.t. θ.
Since θ is a KKT point of Problem 1, then there are λ1, . . . , λn such that for every l ∈ [m] we have
X Ad (" Xi))
U =乙 Ai	∂u(l),
i∈[n]
where	Ai	≥ 0 for all	i,	and	Ai	= 0 if	y%Φ(θ; Xi)	=	1. Letting	θ0(U(I))
[U⑴，...，U(IT), u(l), U(l+1),..., U(m)], the above equation can be written as
∂ (yiΦ(θ0(U(l)); Xi))
∂u(1)
u Ill= EAi
i∈[n]
23
Under review as a conference paper at ICLR 2022
where λi ≥ 0 for all i, and λi = 0 if yiΦ(θ0(u(l)); Xi) = yiΦ(θ; Xi) = 1. Moreover, if the
constraints in Problem 1 are satisfies in θ, then the constrains in Problem 3 are also satisfied for
every l ∈ [m] in U(D w.r.t. θ. Hence, for every l ∈ [m] the KKT conditions of Problem 3 w.r.t. θ
hold. Since the constraints in Problem 3 are affine and the objective is convex, then this KKT point
is a global optimum.
C.10 Proof of Thm. 5.3
Let {(Xi, yi)}i4=1 be a dataset such that yi = 1 for all i ∈ [4] and we have X1 = (0, 1)>, X2 =
(1, 0)>, X3 = (0, -1) and X4 = (-1, 0). In the proof of Thm. 4.2 (part 2) we showed that for an
appropriate initialization, for both the exponential loss and the logistic loss GF converges to zero
loss, and converges in direction to a KKT point θ of Problem 1. Moreover, in the proof of Thm. 4.2
we showed that the KKT point θ is such that for all j ∈ [4] we have Wj = Xj and Vj = 1.
We show that Wι, W2, W3, W4 is not a local optimum of Problem 3 w.r.t. θ. It suffices to prove that
for every 0 < e < 1 there exists some θ0 such that Vj = Vj for all j ∈ [4], ∣∣θ0 - θ∣∣ ≤ e, θ satisfies
the constraints, and kθ0k < kθk. The existence of such θ0 is shown in the proof of Thm. 4.2. Hence,
we conclude the proof of the theorem.
C.11 Proof of Thm. 5.4
By Thm. 2.1 GF converge in direction to a KKT point θ = [U(l)]m=ι of Problem 1. Let l ∈ [m]
and assume that for every i ∈ [n] the inputs to all neurons in layers l, . . . , m - 1 in the computation
Φ(θ; Xi) are non-zero. We now show that the parameters vector UQ) is a local optimum ofProblem 3
w.r.t. θ.
For i ∈ [n] and k ∈ [m - 1] we denote by Xi(k) ∈ Rdk the output of the k-th layer in the computation
(0)
Φ(θ; Xi), and denote Xi = Xi. If l ∈ [m - 1] then we define the following notations. We
denote by 力：Rdl → R the function computed by layers l + 1,..., m of Φ(θ; ∙). Thus, we have
Φ(θ; Xi) = fι(Xy)) = fι ◦ σ (W(I)XyT)), where W⑴ is the weight matrix that corresponds to
U(l). For i ∈ [n] we denote by h the function u(l) → f ◦ σ(W(l)X(l-1)) where W(l) is the weights
matrix that corresponds to u(l). Thus, Φ(θ; Xi) = hi (U (l)). If l = m then we denote by h the
function u(m) → W (m)X(m-1), thus we also have Φ(θ; Xi) = hi (U (m)).
Since θ is a KKT point of Problem 1, then there are λ1, . . . , λn such that
U(l) = X λi d (yiφ(θ; Xi)) = X λi总[y, ∙ hi(U(I))i ,
∂U(l)	∂U(l)
i∈[n]	i∈[n]
where λi ≥ 0 for all i, and λi = 0 if yi ∙ hi(U(l)) = 1. Note that since the inputs to all neurons in
layers l, . . . , m - 1 in the computation Φ(θ; Xi) are non-zero, then the function hi is differentiable
at U(l). Therefore in the above KKT condition we use the derivative rather than the Clarke subdiffer-
ential. Moreover, if the constraints in Problem 1 are satisfies in θ, then the constrains in Problem 3
are also satisfied in U(l) w.r.t. θ. Hence, the KKT condition of Problem 3 w.r.t. θ holds.
Also, note that since the inputs to all neurons in layers l, . . . , m - 1 in the computation Φ(θ; Xi)
are non-zero, then the function h is locally linear near U(l). We denote this linear function by hi.
Therefore, U(l) is a KKT point of the following problem
min ɪ ∣∣U(l) U	s.t. ∀i ∈ [n] yihii(U(l)) ≥ 1 .
Since the constrains here are affine and the objective is convex, then U(l) is a global optimum of the
above problem. Thus, there is a small ball near U(l) where U(l) is the optimum of Problem 3 w.r.t.
θ, namely, it is a local optimum.
24
Under review as a conference paper at ICLR 2022
Finally, note that in the proof of Lemma C.6 the parameters vector θ0 is obtained from θ by changing
only the first layer. Hence, in ReLU networks GF might converge in direction to a KKT point of
Problem 1 which is not a global optimum of Problem 3, even if all inputs to neurons are non-zero.
C.12 Proof OF Thm. 6.1
We have
du
dt
dw
dt
dv
dt
∂'(yΦ(θ; x))	∂'(vw + u)
∂u	∂u
∂'(yΦ(θ; x))	∂'(vw + u)
∂w	∂w
∂'(yΦ(θ; x))	∂'(vw + u)
∂w
∂v
=—0(vw + u).
=-C(vw + u) ∙ v .
一' I(VW + u) ∙ w .
—
—
—
—
—
—
Since we also have v(0) = w(0) then for every t ≥ 0 we have v(t) = w(t). Note that C(vw+u) < 0
and hence all parameters u, w, V are strictly increasing. Since v(0) = w(0) > 1 then for every t ≥ 0
we have v(t) = w(t) > 1 and hence d^ < dwt = dv(t). Therefore u(t) ≤ v(t) = w(t) for all t.
We now calculate the derivative of
vw u
Y⑴=廊,廊+廊
vw	u
V2 + w2 + u2 + √v2 + w2 + u2
We have
d
vw
dt v2 + w2 + u2
and by plugging in w =
1
(2v2 + u2)2
------ʒ-----1(-20 (VW + u)w2 — 20(vw + u)v2) (v2 + w2 + u2)
(v2 + w2 + u2)2
—vw (2v(—20(vw + u)w) + 2w(—20(vw + u)v) + 2u(—20(vw + u)))],
v the above equals
[(—220(v2 + u)v2) (2v2 + u2) + v220(v2 + u)(4v2 + 2u)]
=-22X；U))2v2 [(2v2 + u2) - (2V2 + u)]
Next, we have
—2C(Vl2 + u)v2u(u — 1)
(2v2 + u2)2
d 一
dt -√v2 +
u
w2 + u2
= F----h(----5^ h(-20(VW + u)) PV2 + w2 + u2
v2 + w2 + u2
-u-	]	(2v(-20 (VW + u)w) + 2w(-20(vw + u)v) + 2u(-20(vw + u)))
2 v2 + w2 + u2
一20(vw + u)
v2 + w2 + u2
pv2 + w2 + u2 — U-/	1	(4vw +
2 v2 + w2 + u2
and by plugging in W = V the above equals
-C(V2 + u)
2v2 + u2
P2v2 + u2 — u——/ 1 二(4v2 +
2√2v2 + u2
一20(v2 + u)
2v2 + u2 — U(2v2 + U)
2口 (v2 + u)v2(u — 1)
2v2 + u2
overall, we have
dKθ)
dt
√2v2 + u2
2v2 + u2
√2v2 + u2 .
—220 (v2 + u)v2u(u — 1) + 220 (v2 + u)v2(u — 1)
(2v2 + u2)2
—2' (v2 + u)v2(u — 1)
2v2 + u2
1
√2v2 + u2
u
—
2v2 + u2
—2' (v2 + u)v2(u — 1)
2v2 + u2
2v2 + u2	√2v2 + u2
U — vz2v2 + u2
2v2 + u2
< 0 ,
1
1
25
Under review as a conference paper at ICLR 2022
where the inequality is since u > 1 and v > 0.
We now show that limt→∞ L(θ(t)) = 0. Assume that kθk 6→ ∞. Let M > 0 be such that
VW + U ≤ M for all t. Then, —'0(vw + U) ≥ C for some constant C > 0. Thus, ddu ≥ C
for all t, which implies that u → ∞ in contradiction to our assumption. Hence, we must have
kθk → ∞. Since v = w ≥ U ≥ 1 for all t, then we have v → ∞ and w → ∞. Therefore
limt→∞ L(θ(t)) = limt→∞ `(vw + U) = 0.
It is easy to verify that γ(θ(0))== + √^ > 0.9. In order to obtain limt→∞ γ(θ(t)) = ɪ We first
show that limt→∞ V = 0. Let M > 4 be some large constant. We show that there exists some t0
such that u(t) ≥ M for all t ≥ t0. Since V → ∞, then there is some tι such that v(tι) = 2M, and
∆ > 0 such that v(t1 + ∆) = 2M + M3. Since U ≤ v then U(t1 ) ≤ 2M. For every t ≥ t1 we have
dv⑴ — —'0(Vw + u) ∙ V ≥ —'0(Vw + u) ∙ 2M Also we have du⑶ — —'0(Vw + U) ≤ ɪ ∙ dMt)
dt	IV w,	I UJ V ≥	IV w,	I UJ	vj- .	/ʌiso, We nav e dt	IV w,	∖ U ≤	? JM	dt .
Hence, we have
U(t1 +△)	= U(ti)	+ Z	d ddt	≤ 2M +	Z	2M	d d	dt = 2M +	2M (V(t1	+△) - V(ti))
t	dt	t	2M	dt	2M
=2M + ɪ (2M + M3 — 2M) = 2M + M ≤ M2 .
Thus,
V(tι +△) ≥ 2M + M3 ≥ M
U(tι +△) _ M2 一 .
Denote t0 = t1 + △. Note that for every t ≥ t0 we have
V(t)
≥
≥
V(t0) + Z ʃdt = V(t0) + Z (—'0(vw + U))wdt = V(t0) + Z (—'0(vw + U))Vdt
t0 dt	t0	t0
V(t0) + Z (—'0(vw + U))M3dt = V(t0) + M3 Z ʃdt
t0	t0 dt
M ∙ Ur + M3 (U(t) — U(t0)) ≥ M ∙ Ur + M (U(t) — U(t0)) = M〃(t).
We have
2
Vw	U	V2	U
Y 力	V2 + W2 + U2 + √V2 + w2 + U2	2v2 + U2 + √2v2 + U2
Since limt→∞ V =
limt→∞ γ(θ(t)) = 2
then kθk converges to
21
0 then limt→∞√V+2 = 0 and limt→∞ 2vv+u2 = 2. Hence,
as required. Finally, since limt→∞ V = 0 and w(t)
[√2，√⅛，0]. Thus, θ converges in direction.
V(t) > 0 for all t,
26