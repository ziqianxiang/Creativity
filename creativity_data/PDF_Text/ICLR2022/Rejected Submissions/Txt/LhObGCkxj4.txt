Under review as a conference paper at ICLR 2022
New Perspective on the Global Convergence
of Finite-Sum Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks (DNNs) have shown great success in many machine learn-
ing tasks. Their training is challenging since the loss surface of the network ar-
chitecture is generally non-convex, or even non-smooth. How and under what as-
sumptions is guaranteed convergence to a global minimum possible? We propose
a reformulation of the minimization problem allowing for a new recursive algo-
rithmic framework. By using bounded style assumptions, we prove convergence
to an ε-(global) minimum using O(1∕ε3) gradient computations. Our theoretical
foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded style
assumptions. This new direction broadens our understanding of why and under
what circumstances training of a DNN converges to a global minimum.
1 Introduction
In recent years, deep neural networks (DNNs) have shown a great success in many machine learn-
ing tasks. However, training these neural networks is challenging since the loss surface of network
architecture is generally non-convex, or even non-smooth. Thus, there have been a long-standing
question on how optimization algorithms may converge to a global minimum. Many previous work
have investigated Gradient Descent algorithm and its stochastic version for over-parameterized set-
ting (Arora et al., 2018; Soudry et al., 2018; Allen-Zhu et al., 2019; Du et al., 2019a; Zou & Gu,
2019). Although these works have shown promising convergence results under certain assumptions,
there is still a lack of new efficient methods that can guarantee global convergence for machine
learning optimization. In this paper, we address this problem using a different perspective. Instead
of analyzing the traditional finite-sum formulation, we adopt a new composite formulation that ex-
actly depicts the structure of machine learning where a data set is used to learn a common classifier.
Representation. Let	{(^χ(i),y(i))}n=1	be a given training set with	x(i)	∈	Rm,y(i)	∈	Rc,	We
investigate the following novel representation for deep learning tasks:
理{F (W) = n XX φi(h(w; i))}，
(1)
where h(∙; i) : Rd → Rc, i ∈ [n] = {1,...,n}, is the classifier for each input data x(i); and
i : Rc → R, i ∈ [n], is the loss function corresponding to each output data y(i). Our composite for-
mulation (1) is a special case of the finite-sum problem minw∈>d {F(W) = n Pn=ι f (w; i)} where
each individual function f (∙; i) is a composition of the loss function φi and the classifier h(∙; i). This
problem covers various important applications in machine learning, including logistic regression and
neural networks. The most common approach for the finite-sum problem is using first-order meth-
ods such as (stochastic) gradient algorithms and making assumptions on the component functions
f (∙; i). As an alternative, we further investigate the structure of the loss function φi and narrow
our assumption on the classifier h(∙; i). For the purpose of this work, we first consider convex and
Lipschitz-smooth loss functions while the classifiers can be non-convex. Using this representation,
we propose a new framework followed by two algorithms that guarantee global convergence for the
minimization problem.
Algorithmic Framework. Representation (1) admits a new perspective. Our key insight is to (A)
define zi(t) = h(W(t); i), where t is an iteration count of the outer loop in our algorithmic framework.
1
Under review as a conference paper at ICLR 2022
Next (B), we want to approximate the change zi(t+1) - zi(t) in terms of a step size times the gradient
Vφi(Z(t) ) = (d0i(Z)/dZa)a∈[c]∣z=z(t),
and (C) we approximate the change h(w(t+1); i) - h(w(t); i) in terms of the first order derivative
Hft = (dha (w； i"dwb)a∈[c],b∈[d]L=w(t).
Finally, we combine (A), (B), and (C) to equate the approximations of Zi(t+1) - Zi(t) and
h(w(t+1); i) - h(w(t); i). This leads to a recurrence on w(t) of the form w(t+1) = w(t) - η(t)v(t),
where η(t) is a step size and which involves computing v(t) by solving a convex quadratic sub-
problem, see the details in Section 4. We explain two methods for approximating a solution for the
derived subproblem. We show how to approximate the subproblem by transforming it into a strongly
convex problem by adding a regularizer which can be solved in closed form. And we show how to
use Gradient Descent (GD) on the subproblem to find an approximation v(t) of its solution.
Convergence Analysis. Our analysis introduces non-standard bounded style assumptions. Intu-
itively, we assume that our convex and quadratic subproblem has a bounded solution. This allows
Us to prove a total complexity of O(*) to find an ε-(global) solution that satisfies F(w) - F ≤ ε,
where F is the global minimizer of F. Our analysis applies to a wide range of applications in
machine learning: Our results hold for squared loss and softmax cross-entropy loss and applicable
for a range of activation functions in DNN as We only assume that the h(∙; i) are twice continuously
differentiable and their Hessian matrices (second order derivatives) as well as their gradients (first
order derivatives) are bounded.
Contributions and Outline. Our contributions in this paper can be summarized as follows.
•	We propose a new representation (1) for analyzing the machine learning minimization prob-
lem. Our formulation utilizes the structure of machine learning tasks where a training data
set of inputs and outputs is used to learn a common classifier. Related work in Section 2
shows how (1) is different from the classical finite-sum problem.
•	Based on the new representation we propose a novel algorithm framework. The algorith-
mic framework approximates a solution to a subproblem for which we show two distinct
approaches.
•	For general DNNs and based on bounded style assumptions, we prove a total complexity
of O( ε3) to find an ε-(global) solution that satisfies F (W) - F ≤ ε, where F is the global
minimizer of F .
We emphasize that our focus is on developing a new theoretical foundation and that a translation
to a practical implementation with empirical results is for future work. Our theoretical foundation
motivates further study, implementation, and optimization of the new algorithmic framework and
further investigation of its non-standard bounded style assumptions. This new direction broadens
our understanding of why and under what circumstances training of a DNN converges to a global
minimum.
The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 describes
our setting and deep learning representation. Section 4 explains our key insight and derives our
Framework 1. Section 5 presents our algorithms and their global convergence. All technical proofs
are deferred to the Appendix.
2	Related Work
Formulation for Machine Learning Problems. The finite-sum problem is one of the most im-
portant and fundamental problems in machine learning. Analyzing this model is the most popular
approach in the machine learning literature and it has been studied intensively throughout the years
(Bottou et al., 2018; Reddi et al., 2016; Duchi et al., 2011b). Our new formulation (1) is a spe-
cial case of the finite-sum problem, however, it is much more complicated than the previous model
since it involves the data index i both inside the classifiers h(∙; i) and the loss functions φi. For a
comparison, previous works only consider a common loss function l(y, y) for the predicted value
2
Under review as a conference paper at ICLR 2022
y and output data y (ZoU et al., 2018; Soudry et al., 2018). Our modified version of loss function
φi is a natural setting for machine learning. We note that when h(w; i) is the output produced by
a model, our goal is to match this output with the corresponding target y(i) . For that reason, the
loss function for each output has a dependence on the output data y(i), and is denoted by φi. This
fact reflects the natural setting of machine learning where the outputs are designed to fit different
targets, and the optimization process depends on both outer function φi and inner functions h(∙; i).
This complication may potentially bring a challenge to theoretical analysis. However, with separate
loss functions, we believe this model will help to exploit better the structure of machine learning
problems and gain more insights on the neural network architecture.
Other related composite optimization models are also investigated thoroughly in (Lewis & Wright,
2016; Zhang & Xiao, 2019; Tran-Dinh et al., 2020). Our model is different from these works as
it does not have a common function wrapping outside the finite-sum term, as in (Lewis & Wright,
2016). Note that a broad class of variance reduction algorithms (e.g. SAG (Le Roux et al., 2012),
SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), SARAH (Nguyen et al., 2017)) is
designed specifically for the finite-sum formulation and is known to have certain benefits over Gra-
dient Descent. In addition, the multilevel composite problem considered in (Zhang & Xiao, 2021)
also covers empirical risk minimization problem. However our formulation does not match their
work since our inner function h(w; i) is not an independent expectation over some data distribution,
but a specific function that depends on the current data.
Global Convergence for Neural Networks. A recent popular line of research is studying the dy-
namics of optimization methods on some specific neural network architectures. There are some early
works that show the global convergence of Gradient Descent (GD) for simple linear network and
two-layer network (Brutzkus et al., 2018; Soudry et al., 2018; Arora et al., 2019; Du et al., 2019b).
Some further works extend these results to deep learning architectures (Allen-Zhu et al., 2019; Du
et al., 2019a; Zou & Gu, 2019). These theoretical guarantees are generally proved for the case when
the last output layer is fixed, which is not standard in practice. A recent work (Nguyen & Mondelli,
2020) prove the global convergence for GD when all layers are trained with some initial conditions.
However, these results are for neural networks without bias neurons and it is unclear how these anal-
yses can be extended to handle the bias terms of deep networks with different activations. Our novel
framework and algorithms do not exclude learning bias layers as in (Nguyen & Mondelli, 2020).
Using a different algorithm, Brutzkus et al. (2018) investigate Stochastic Gradient Descent (SGD)
for two-layer networks in a restricted linearly separable data setting. This line of research continues
with the works from Allen-Zhu et al. (2019); Zou et al. (2018) and later with Zou & Gu (2019). They
justify the global convergence of SGD for deep neural networks for some probability depending on
the number of input data and the initialization process.
Over-Paramaterized Settings and other Assumptions for Machine Learning. Most of the mod-
ern learning architectures are over-parameterized, which means that the number of parameters are
very large and often far more than the number of input data. Some recent works prove the global
convergence of Gradient Descent when the number of neurons are extensively large, e.g. (Zou &
Gu, 2019) requires Ω(n8) neurons for every hidden layer, and (Nguyen & Mondelli, 2020) im-
proves this number to Ω(n3). If the initial point satisfies some special conditions, then they can
show a better dependence of Ω(n). In Allen-Zhu et al. (2019), the authors initialize the weights
using a random Gaussian distribution where the variance depends on the dimension of the problem.
In non-convex setting, they prove the convergence of SGD using the assumption that the dimension
depends inversely on the tolerance . We will discuss how these over-paramaterized settings might
be a necessary condition to develop our theory.
Other standard assumptions for machine learning include the bounded gradient assumption (Ne-
mirovski et al., 2009; Shalev-Shwartz et al., 2007; Reddi et al., 2016; Tran et al., 2021). It is also
common to assume all the iterations ofan algorithm stays in a bounded domain (Duchi et al., 2011a;
Levy et al., 2018; GUrbUzbalaban et al., 2019; Reddi et al., 2018; Vaswani et al., 2021). Since We
are analyzing a new composite formulation, it is understandable that our assumptions may also not
be standard. However, we believe that there is a strong connection between our assumptions and the
traditional setting of machine learning. We will discuss this point more clearly in Section 4.
3
Under review as a conference paper at ICLR 2022
3	Background
In this section, we discuss our formulation and notations in detail. Although this paper focuses on
deep neural networks, our framework and theoretical analysis are general and applicable for other
learning architectures.
Deep Learning Representation. Let {(x(i), y(i))}in=1 be a training data set where x(i) ∈ Rm is a
training input and y(i) ∈ Rc is a training output. We consider a fully-connected neural network with
L layers, where the l-th layer, l ∈ {0, 1, . . . , L}, has nl neurons. We represent layer 0-th and L-th
layer as input and output layers, respectively, that is, n0 = d and nL = c. For l ∈ {1, . . . , L}, let
W(l) ∈ Rnl-1 ×nl and b(l) ∈ Rnl, where {(W(l), b(l))lL=1} represent the parameters of the neural
network. A classifier h(w; i) is formulated as
h(w; i) =W(L)>σL-1(W(L-1)>σL-2(...σ1(W(1)>x(i) +b(1))...)+b(L-1))+b(L),
where w = vec({W (1), b(1), . . . , W(L), b(L)}) ∈ Rd is the vectorized weight and {σl}lL=-11 are some
activation functions. The most common choices for machine learning are ReLU, sigmoid, hyperbolic
tangent and SoftPlus. For j ∈ [c], hj(∙; i) : Rd → R denotes the component function of the output
h(∙; i), for each data i ∈ [n] respectively. Moreover, we define h* = arg minz∈Rc φi(z),i ∈ [n].
Loss Functions. The well-known loss functions in neural networks for solving classification and
regression problems are softmax cross-entropy loss and square loss, respectively:
(Softmax) Cross-EntroPy Loss: F(W) = n1 En=I f (w； i) With
f(w; i) = -y(i)> log(softmax(h(w; i))).	(2)
Squared Loss: F(W) = -1 Pn=I f (w； i) with
f(w; i) = 2l∣h(w; i) - y(i)k2.	(3)
We provide some basic definitions in optimization theory to support our theory.
Definition 1 (L-smooth). Function φ : Rc → R is Lφ-smooth if there exists a constant Lφ > 0
such that, ∀x1, x2 ∈ Rc,
∣∣Vφ(xι) - Vφ(x2)k ≤ Lφ∣xι - X2∣∣.	(4)
Definition 2 (Convex). Function φ : Rc → R is convex if∀x1, x2 ∈ Rc,
φ(x1) - φ(x2) ≥ hVφ(x2),x1-x2i.	(5)
The following corollary shows the properties of softmax cross-entropy loss (2) and squared loss (3).
Corollary 1. For Sofmax cross-entropy loss (2) and squared loss (3), there exist functions h(∙; i):
Rd → Rc and φn : Rc → R such that, for i ∈ [n], φn(z) is convex and Lφ-smooth with Lφ = 1, and
f(w； i) = φi(h(w; i)) = φn(Z)Iz=h(w;i).	⑹
4 New Algorithm Framework
4.1	Key Insight
We assume f(w； i) = φn (h(w； i)) with φn convex and Lφ-smooth. Our goal is to utilize the con-
vexity of the outer function φn. In order to simplify notation, we write Vzφn (h(w(t)； i)) instead of
Vzφi(z)∣z=h(w(t).i) and denote z(t) = h(w(t); i). Starting from the current weight w(t), we would
like to find the next point w(t+1) that satisfies the following approximation for all i ∈ [n]:
h(w(t+1); i) =	z(t+1)	≈	Zi(I- α(t)Vzφi(z(t))	=	h(w(t);	i)	- α(t)Vzφi(h(w⑶;i)).	⑺
4
Under review as a conference paper at ICLR 2022
We can see that this approximation is a “noisy” version of a gradient descent update for every
function φi , simultaneously for all i ∈ [n]. In order to do this, we use the following update
w(t+1) = w(t) - η(t)v(t),	(8)
where η(t) > 0 is a learning rate and v (t) is a search direction that helps us approximate equation
(7). If the update term η(t)v(t) is small enough, and if h(∙; i) has some nice smooth properties, then
from basic calculus we have the following approximation:
h(w(t+1); i) = h(w(t) — η(t)v(t); i) ≈ h(w(t); i) — Hit) (η(t)v(t)),	(9)
where Hi(t) is a matrix in Rc×d with first-order derivatives. Motivated by approximations (7) and
(9), we consider the following optimization problem:
11 n
v≠ = arg min -- V" ∣∣∏i (ηit)v) - αi ^Vzφi(h(w(tl; i))k2.	(10)
v∈Rd 2 n
i=1
Hence, by solving for the solution Vit) of problem (10) We are able to find a search direction for the
key approximation (7). This yields our new algorithmic Framework 1, see below.
Framework 1 NeW Algorithm FrameWork
Initialization: Choose an initial point wi0) ∈ Rd ;
fort = 0,1,…，T — 1 do
Solve for an approximation v(t) of the solution Vit) of the problem in (10)
n
arg min ɪɪ	∣∣η(t)Hi(t)
v∈Rd 2 n	i
v — αit)Vzφi(h(w⑶;i))∣2
i=1
Update wit+1) = wit) — ηit)Vit)
end for
4.2 Technical Assumptions
Assumption 1. The loss function φi is convex and Lφ-smooth for i ∈ [n]. Moreover, we assume
that it is lower bounded, i.e. infz∈Rc φi(z) > 一∞ for i ∈ [n].
We have shoWn the convexity and smoothness of squared loss and softmax cross-entropy loss in
Section 3. The bounded property of φi is required in any algorithm for the Well-definedness of (1).
NoW, in order to use the Taylor series approximation, We need the folloWing assumption on the
neural netWork architecture h:
Assumption 2. We assume that h(∙; i) is twice continuously differentiable for all i ∈ [n] (i.e. the
second-order partial derivatives of all scalars hj (∙; i) are ContinUoUSfor all j ∈ [c] and i ∈ [n]),
and that their Hessian matrices are bounded, that is, there exists a G > 0 such that for all w ∈ Rd,
i ∈ [n] andj ∈ [c],
∣Mi,j(w)k = kJw (Vwhj(w; i))∣∣≤ G,	(11)
where Jw denotes the Jacobian1 .
Remark 1 (Relation to second-order methods). Although our analysis requires an assumption on
the Hessian matrices of h(w; i), our algorithms do not use any second order information or try to
approximate this information. Our theoretical analysis focused on the approximation of the clas-
sifier and the gradient information, therefore is not related to the second order type algorithms.
It is currently unclear how to apply second order methods into our problem, however, this is an
interesting research question to expand the scope of this work.
1For a continuously differentiable function g(w) : Rd → Rc We define the Jacobian Jw(g(w)) as the matrix
(dga (W)/dWb) a∈[c] ,b∈ [d] .
5
Under review as a conference paper at ICLR 2022
Assumption 2 allows Us to apply a Taylor approximation of each function hj(∙; i) with which We
prove the following Lemma that bounds the error in equation (9):
Lemma 1. Suppose that Assumption 2 holds for the classifier h. Then for all i ∈ [n] and 0 ≤ t < T,
h(w(t+1); i) = h(w(t) - η(t)v(t); i) = h(w(t) ; i) - η(t)Hi(t)v(t) + i(t) ,	(12)
where
Hi(t) = Jw(h(w; i))|w=w(t) ∈ Rc×d	(13)
is defined as the Jacobian matrix of h(w; i) at w(t) and entries i(,tj), j ∈ [c], of vector i(t) satisfy
kgl≤ 1(n(t))2kv(t)k2G.	(14)
2
In order to approximate (7) combined with (9), that is, to make sure the right hand sides of (7) and
(9) are close to one another, we consider the optimization problem (10):
11 n
V，)= arg min -- T kη(t)Hi V - O`Zzφi(h(w⑻;i))k ∙
v∈Rd 2 n
i=1
The optimal value of problem (10) is equal to 0 if there exists a vector v(t) satisfying n(t) HatVat =
α(t)Vzφi(h(w(tt; i)) for every i ∈ [n]. Since the solution v(t) is in Rd and Nzφi(h(w(tt; i)) is in
Rc, this condition is equivalent to a linear system with n ∙ C constraints and d variables. In the over-
parameterized setting where dimension d is sufficiently large (d》n ∙ c) and there are no identical
data, there exists almost surely a vector V，(t) that interpolates all the training set, see the Appendix
for details.
Let us note that an approximation of V，(t) serves as the search direction for Framework 1. For this
reason, the solution V，(t) of problem (10) plays a similar role as a gradient in the search direction
of (stochastic) gradient descent method. It is standard to assume a bounded gradient in the ma-
chine learning literature (Nemirovski et al., 2009; Shalev-Shwartz et al., 2007; Reddi et al., 2016).
Motivated by these facts, we assume the following Assumption 3, which implies the existence of a
near-optimal bounded solution of (10):
Assumption 3. We consider an over-parameterized setting where dimension d is sufficiently large
enough to interpolate all the data and the tolerance ε. We assume that there exists a bound V > 0
such that for ε > 0 and 0 ≤ t < T as in Framework 1, there exists a vector V(I) with ∣∣V(ε) ∣∣2 ≤ V
so that
n
21 X kn(t)H(t)V(ε - αit)Vzφi(h(w(t); i))k2 ≤ ε2.
n i=1
Our Assumption 3 requires a nice dependency on the tolerance ε for the gradient matrices Hi(t) and
Vz φi(h(w(t); i)). We note that at the starting point t = 0, these matrices may depend on ε due to
the initialization process and the dependence of d on ε. This setting is similar to previous works,
e.g. Allen-Zhu et al. (2019).
5 New Algorithms and Convergence Results
5.1	Approximating the solution using regularizer
Since problem (10) is convex and quadratic, we consider the following regularized problem:
(11 n	ε2
Ψ(v) = 2n X kn(t)Hi(t)v - α(t)Vzφi(h(w(t); i))k2 + 工kv∣2 ∖ ,
(15)
6
Under review as a conference paper at ICLR 2022
for some small ε > 0 and t ≥ 0. It is widely known that problem (15) is strongly convex, and has a
unique minimizer v(treg. The global minimizer satisfies NvΨ(v(tTeg) = 0. We have
n
VvΨ(v) = - X[n(t)H^(t)>H(t)n(t)v - Ottn⑶Hie)τNzφi(h(w⑶;i))] + ε2 ∙ V
i=1
1XX η(t)H产 Httn⑶ + ε2ι)v-
1XXOttn⑴Hie)τVzφi(h(w⑶;i))).
Therefore,
v(treg= (n XX n(t)Hi(t)τHi(t)n(t) + ε2I!	(i XX a*n(t)Hf)>Vz φi (h(w(t); i))) . (16)
If ε2 is small enough, then v(treg is a close approximation of the solution v(t) for problem (10). Our
first algorithm updates Framework 1 based on this approximation.
Algorithm 1 Solve for the exact solution of the regularized problem
Initialization: Choose an initial point wi0) ∈ Rd, tolerance ε > 0;
fort = 0,1,…，T — 1 do
Update the search direction v(t) as the solution v(treg of problem in (15):
V⑴=v(treg = (1 XX n⑴ H严 H*⑴ + ε2I
n i=1
n
X αiit)nit)Hiit)τVzφi(h(wit); i))
i=1
Update wit+1) = wit) - nit)Vit)
end for
The following Lemma shows the relation between the regularized solution v(treg and the optimal
solution of the original convex problem V，.
Lemma 2. For given ε > 0, suppose that Assumption 3 holds for bound V > 0. Then, for iteration
0 ≤ t < T ,the optimal solution v(t)eg of problem (15) satisfies ∣∣v(t)eg ∣∣2 ≤ 2 + V and
n
2n X kn(t)Hi(t)v(treg - α(t) Vzφi(h(w⑶;i))k2 ≤ (1 + V2)ε2.	(17)
n i=1
Based on Lemma 2, we guarantee the global convergence of Algorithm 1 and prove our first theorem.
Since it is currently expensive to solve for the exact solution of problem (15), our algorithm serves
as a theoretical method to obtain the global convergence for the finite-sum minimization.
Theorem 1. Let wit) be generated by Algorithm 1 where we use the closed form solution for the
search direction. We execute Algorithm 1 for T = β outer loops for some constant β > 0. We
assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds
for V > 0. We set the step size equal to n(t) = D√ε for some D > 0 and choose a learning rate
α(t) = (1 + ε)α(t-1t = (1 + ε)tα(0). Based on β, we define ɑi0) = ^L^ With α ∈ (0, 3). Let F
be the global minimizer of F, and h* = arg minz∈Rc φi(z), i ∈ [n]. Then
T g[F (w(t)) - F*] ≤ ⅜⅛+⅛ ∙1XX kh(w ⑼;i)-矶2 ∙ε
+	+2) [c(4+(V + 2)GD2)2 +8 + 4V]∙ ε. (18)
We note that β is a constant for the purpose of choosing the number of iterations T. The analysis
can be simplified by choosing β = 1 with T = ɪ. Notice that the common convergence criteria for
7
Under review as a conference paper at ICLR 2022
finding a stationary point for non-convex problems is 1 PT=I ||VF(wt)||2 ≤ O(ε). This criteria
has been widely used in the existing literature for non-convex optimization problems. Our conver-
gence criteria T P=I [F(Wt) 一 F*] ≤ O(ε) is slightly different, in order to find a global solution
for non-convex problems.
Our proof for Theorem 1 is novel and insightful. It is originally motivated by the Gradient Descent
update (7) and the convexity of the loss functions φi . For this reason it may not be a surprise that
Algorithm 1 can find an ε-global solution after O (ɪ) iterations. However, computing the exact
solution in every iteration might be extremely challenging, especially when the number of samples
n is large. Therefore, we present a different approach to this problem in the following section.
5.2	Approximation using Gradient Descent
In this section, we use Gradient Descent (GD) algorithm to solve the strongly convex problem (15).
It is well-known that if ψ(x) 一 2 ∣∣x∣∣* 2 is convex for ∀x ∈ Rc, then ψ(x) is μ-strongly convex (see
e.g. Nesterov (2004)). Hence Ψ(∙) is ε2-strongly convex. For each iteration t, We use GD to find a
search direction v(t) which is sufficiently close to the optimal solution v*(tr)eg in that
kv(t) 一 v*(t)regk ≤ ε.	(19)
Our Algorithm 2 is described as follows.
Algorithm 2 Solve the regularized problem using Gradient Descent
Initialization: Choose an initial point w(0) ∈ Rd, tolerance ε > 0;
fort = 0,1,…，T — 1 do
Use Gradient Descent algorithm to solve Problem (15) and find a solution v(t) that satisfies
kv(t) 一 v*(tr)egk ≤ ε
Update w(t+1) = w(t) 一 η(t)v(t)
end for
Since Algorithm 2 can only approximate a solution within some ε-preciseness, we need a supple-
mental assumption for the analysis of our next Theorem 2:
Assumption 4. Let Hi(t) be the Jacobian matrix defined in Lemma 1. We assume that there exists
some constant H > 0 such that, for i ∈ [n], ε > 0, and 0 ≤ t < T as in Algorithm 2,
∣∣H(t)∣∣≤√ε.	(20)
Assumption 4 requires a mild condition on the bounded Jacobian of h(w; i), and the upper bound
may depend on ε. This flexibility allows us to accommodate a good dependence of ε for the theo-
retical analysis. We are now ready to present our convergence theorem for Algorithm 2.
Theorem 2. Let w(t) be generated by Algorithm 2 where v(t) satisfies (19). We execute Algorithm
2 for T = β outer loops for some constant β > 0. We assume Assumption 1 holds. Suppose
that Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for
H > 0. We set the step size equal to η⑴ =D√ε for some D > 0 and choose a learning rate
α(t = (1 + ε)α(t-1) = (1 + ε)tα(0). Based on β, we define ɑi0) = ^L^ with α ∈ (0, 4). Let F*
be the global minimizer ofF, and hi* = arg minz∈Rc φi(z), i ∈ [n]. Then
T X1[F(Ww- F*] ≤ e⅛(4+⅛) ∙ n X &W⑼;i) 一矶2 ∙ ε
+ eβLφ(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2 + 2 + V] ∙ ε.
2α(1 一 4α)
8
Under review as a conference paper at ICLR 2022
Theorem 2 implies Corollary 2 which provides the computational complexity for Algorithm 2. Note
that for (Stochastic) Gradient Descent, we derive the complexity in terms of component gradient
calculations for the finite-sum problem (1). As an alternative, for Algorithm 2 we compare the
number of component gradients in problem (15). Such individual gradient has the following form:
Vvψi(v) = η⑴H产Httn⑻V - α(t%⑴H,>Vz小心心；力.
In machine learning applications, the gradient of f (∙; i) is calculated using automatic differentiation
(i.e. backpropagation). Since f (∙; i) is the composition of the network structure h(∙; i) and loss func-
tion φi(∙), this process also computes the Jacobian matrix Hitt and the gradient Vzφi(h(w(t); i)) at
a specific weight w(t). Since matrix-vector multiplication computation is not expensive, the cost for
computing the component gradient of problem (15) is similar to problem (1).
Corollary 2. Suppose that the Conditions in Theorem 2 hold with n(t) = D√2 for some D > 0 and
0 < ε ≤ N (that is, we set ε = ε∕N), where
N _ eβ Lφ Pn=1 kh(w"i)-h"∣2	7eβ Lφ[D2 H2 + c(2 + (V+3)GD2)2 + 2+V]
N =	n(1-4α)αβ	+	2α(1-4α)	.
Then, the total complexity to guarantee min°≤t≤τ-ι[F(w(t)) — F*] ≤ T PT-)1[F(w(t)) — F*] ≤ ε
is O (n警(D2H2 + (ε2∕N))log(N)).
Remark 2. Corollary 2 shows that O (1∕ε) outer loop iterations are needed in order to reach an
ε-global solution, and it proves that each iteration needs the equivalent of O (∣⅛ log( ^)) gradient
computations for computing an approximate solution. In total, Algorithm 2 has total complexity
O (ε⅜- log( ^)) for finding an ε-global solution.
For a comparison, Stochastic Gradient Descent uses a total of O(*) gradient computations to
find a stationary point satisfying E[∣∣VF(W)k2] ≤ ε for non-convex problems (Ghadimi & Lan,
2013). Gradient Descent has a better complexity in terms of ε, i.e. O( n) such that ∣∣VF(W)k2 ≤ ε
(Nesterov, 2004). However, both methods may not be able to reach a global solution of (1). In order
to guarantee global convergence for nonconvex settings, one may resort to use Polyak-Lojasiewicz
(PL) inequality (Karimi et al., 2016; Gower et al., 2021). This assumption is widely known to be
strong, which implies that every stationary point is also a global minimizer.
6	Further Discussion and Conclusions
This paper presents an alternative composite formulation for solving the finite-sum optimization
problem. Our formulation allows a new way of exploiting the structure of machine learning prob-
lems and the convexity of squared loss and softmax cross entropy loss, and leads to a novel algorith-
mic framework that guarantees global convergence (when the outer loss functions are convex and
Lipschitz-smooth). Our analysis is general and can be applied to various different learning architec-
tures, in particular, our analysis and assumptions match practical neural networks; in recent years,
there has been a great interest in the structure of deep learning architectures for over-parameterized
settings (Arora et al., 2018; Allen-Zhu et al., 2019; Nguyen & Mondelli, 2020). Algorithm 2 demon-
strates a gradient method to solve the regularized problem, however, other methods can be applied
to our framework (e.g. conjugate gradient descent).
Our theoretical foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded style assumptions.
Possible research directions include more practical algorithm designs based on our Framework 1,
and different related methods to solve the regularized problem and approximate the solution. This
potentially leads to a new class of efficient algorithms for machine learning problems. This paper
presents a new perspective to the research community.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
This paper does not contain ethics concerns.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 242-252. PMLR, 09-15 JUn 2019. URL http://Proceedings.
mlr.press/v97/allen-zhu19a.html.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Jennifer Dy and Andreas KraUse (eds.), Proceedings of
the 35th International Conference on Machine Learning, volUme 80 of Proceedings of Machine
Learning Research, pp. 244-253. PMLR, 10-15 JUl 2018. URL http://proceedings.
mlr.press/v80/arora18a.html.
Sanjeev Arora, Simon DU, Wei HU, ZhiyUan Li, and RUosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neUral networks. In Kamalika
ChaUdhUri and RUslan SalakhUtdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volUme 97 of Proceedings of Machine Learning Research, pp. 322-332.
PMLR, 09-15 JUn 2019. URL http://proceedings.mlr.press/v97/arora19a.
html.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018. doi: 10.1137/16M1080173.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=rJ33wwxRb.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pp. 1646-1654, 2014.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pp. 1675-1685. PMLR, 09-15 Jun 2019a. URL
http://proceedings.mlr.press/v97/du19c.html.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(61):2121-2159, 2011a. URL
http://jmlr.org/papers/v12/duchi11a.html.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011b.
S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM J. Optim., 23(4):2341-2368, 2013.
Robert Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions:
Learning rates, minibatching and interpolation. In Arindam Banerjee and Kenji Fukumizu (eds.),
Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume
130 of Proceedings of Machine Learning Research, pp. 1315-1323. PMLR, 13-15 Apr 2021.
URL https://proceedings.mlr.press/v130/gower21a.html.
10
Under review as a conference paper at ICLR 2022
M. Gurbuzbalaban, A. Ozdaglar, and P. A. Parrilo. Convergence rate of incremental gradient and
incremental newton methods. SIAM Journal on Optimization, 29(4):2542-2565, 2019. doi: 10.
1137/17M1147846. URL https://doi.org/10.1137/17M1147846.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the PoIyak±OjasieWicz condition. In Paolo Frasconi, Niels Landwehr,
Giuseppe Manco, and Jilles Vreeken (eds.), Machine Learning and Knowledge Discovery in
Databases, pp. 795-811, Cham, 2016. Springer International Publishing.
Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponen-
tial convergence rate for finite training sets. In NIPS, pp. 2663-2671, 2012.
Kfir Y. Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and
acceleration. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf.
Adrian S. Lewis and Stephen J. Wright. A proximal method for composite minimization. Mathe-
matical Programming, 158:501-546, 2016.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM J. on Optimization, 19(4):1574-1609, 2009.
Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.
Kluwer Academic Publ., Boston, Dordrecht, London, 2004. ISBN 1-4020-7553-7.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 2613-2621. JMLR. org, 2017.
Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide
layer followed by pyramidal topology. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
11961-11972. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In Maria Florina Balcan and Kilian Q. Weinberger (eds.),
Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceed-
ings of Machine Learning Research, pp. 314-323, New York, New York, USA, 20-22 Jun 2016.
PMLR. URL https://proceedings.mlr.press/v48/reddi16.html.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient
solver for svm. Association for Computing Machinery, 2007. doi: 10.1145/1273496.1273598.
URL https://doi.org/10.1145/1273496.1273598.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. J. Mach. Learn. Res., 19(1):2822-2878, January
2018. ISSN 1532-4435.
Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. Smg: A shuffling gradient-based method
with momentum. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pp. 10379-10389. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/
v139/tran21b.html.
11
Under review as a conference paper at ICLR 2022
Quoc Tran-Dinh, Nhan Pham, and Lam Nguyen. Stochastic Gauss-Newton algorithms for noncon-
vex compositional optimization. In Hal DaUme In and Aarti Singh (eds.), Proceedings ofthe 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learn-
ingResearch,pp. 9572-9582. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.
press/v119/tran-dinh20a.html.
Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon Lacoste-
Julien. Adaptive gradient methods converge faster with over-parameterization (but you should do
a line-search), 2021.
Junyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental vari-
ance reduction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
a68259547f3d25ab3c0a5c0adb4e3498- Paper.pdf.
Junyu Zhang and Lin Xiao. Multilevel composite stochastic optimization via nested variance reduc-
tion. SIAM Journal on Optimization, 31(2):1131-1157, 2021. doi: 10.1137/19M1285457. URL
https://doi.org/10.1137/19M1285457.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep
neural networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Flo-
rence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural In-
formation Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
2053-2062, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
6a61d423d02a1c56250dc23ae7ff12f3- Abstract.html.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks, 2018.
12
Under review as a conference paper at ICLR 2022
Appendix
A Table of notations
Notation Meaning
F Global minimization function of F in(1)
F = mi□w∈Rd F(W)
h h = argminz∈Rc φi(z), i ∈ [n]
v(t)	Solution of the convex problem in (10)
minv∈Rd * 1 n Pn=1 kη(t)H(t)v - α(tMφi(h(w(t); i))k2
v(t)	An approximation of v(t) which is used as the search direction in Framework 1
v(ε)	A vector that satisfies
21 Pi=I kη⑴H(t)v - a(t)Vzφi(h(w⑴;i))k2 ≤ ε2
for some ε > 0 and ∣∣v(l) ∣∣2 ≤ V, for some V > 0.
v(treg	Solution of the strongly convex problem in (15)
minv∈Rd n 2n1 Pi=ι kn(t)Hi(t)v - α(t)vzφi(h(w(t); i))k2 + ε2∣∣vk20
B Useful Results
The following lemmas provide key tools for our results.
Lemma 3 (Squared loss). Let b ∈ Rc and define φ(z) = 1 ∣∣z 一 b∣2 for Z ∈ Rc. Then φ is convex
and Lφ-smooth with Lφ = 1.
Lemma 4 (Softmax cross-entropy loss). Let index a ∈ [c] and define
cc
φ(z) = log	exp(zk - za) = log	exp(wk>z)
k=1	k=1
for z = (z1, . . . , zc)> ∈ Rc, where wk = ek -ea with ei representing the i-th unit vector (containing
1 at the i-th position and 0 elsewhere). Then φ is convex and Lφ-smooth with Lφ = 1.
The following lemma is a standard result in (Nesterov, 2004).
Lemma 5 ((Nesterov, 2004)). If φ is Lφ-smooth and convex, then for ∀z ∈ Rc,
kVφ(z)∣2 ≤ 2Lφ(φ(z) - φ(z*)),	(21)
where z* = argmin2 φ(z).
The following useful derivations could be used later in our theoretical analysis. Since φi is convex,
by Definition 2 we have
φi (h(w; i)) ≥ φi (h(w0; i)) + Vzφi (z)	, h(w; i) - h(w0; i) .	(22)
z=h(w0;i)
If φi is convex and Lφ-smooth, then by Lemma 5
2
Vz φi(z)	≤ 2Lφ [φi(h(w; i)) - φi(hi*)] ,	(23)
z=h(w;i)
where hi* = arg minz∈Rc φi(z).
We compute gradients of f(w; i) in term of φi(h(w; i)).
13
Under review as a conference paper at ICLR 2022
•	Gradient of softmax cross-entropy loss:
vφi(Z)Lh(w;i)
∂φi(z) I
z=h(w;i)	∂zc	z =h(w;i)
>
where for j ∈ [c],
{exp([h(w；i)]j-[h(w；i)]I(y(i)))
Pk=I eXP([h(W；i)]k-[h(W；i)]i(y(i)))
Pk = I(y(i)) exp([h(w㈤]k-[h(w；i)]i(y(i)))
Pk=I exp([h(w；i)]k-[h(w；i)]i(y(i)))
•	Gradient of squared loss:
vφi(Z)Iz="w；i) =h(W; i)- y(i).
, j 6= I(y(i))
, j = I(y(i))
(24)
(25)
C Additional Discussion
C.1 About Assumption 2
We make a formal assumption for the case h(∙; i) is closely approximated by k(∙; i).
Assumption 5. We assume that for all i ∈ [n] there exists some approximations k(w; i) : Rd → Rc
such that
|kj (w; i) - hj (w; i)| ≤ ε, ∀w ∈ Rd, i ∈ [n] andj ∈ [c],	(26)
where k(∙; i) are twice continuously differentiable (i.e. the Second-order partial derivatives of all
scalars kj (∙; i) are Continuousfor all i ∈ [n]), and that their Hessian matrices are bounded:
kMi,j (w)k = kJw (vw kj (w; i))k ≤ G, ∀w ∈ Rd, i ∈ [n] andj ∈ [c].	(27)
Assumption 5 allows us to prove the following Lemma that bound the error in equation (9):
Lemma 6. Suppose that Assumption 5 holds for the classifier h. Then for all i ∈ [n] and 0 ≤ t < T,
we have:
h(w(t+1); i) = h(w(t) - η(t)v(t); i) = h(w(t); i) - η(t)Hi(t)v(t) + i(t) ,	(28)
where Hi(t) is defined to be the Jacobian matrix of the approximation k(w; i) at w(t):
Hi(t) := Jwk(w; i)|w=w(t)
∂k1 (w;i)
∂wι
∂k1 (w;i)
∂wd
∈ Rc
×d
(29)
dkc(w;i)
∂wι
dkc(w;i)
∂wd .
w=w(t)
Additionally we have,
∣e∙S∣≤ 1(η(t))2kv(t)k2G + 2ε, j ∈ [c].
2
(30)
Note that these result recover the case when h(∙; i) is itself smooth. Hence we analyze our algorithms
using the result of Lemma 6, which generalizes the result from Lemma 1.
C.2 About Assumption 3
In this section, we justify the existence of the search direction in Assumption 3 (almost surely). We
argue that there exists a vector v(ε) satisfying
n
2-X kη(t)HietvTε -斓Vzφi(h(w⑺;i))k2 ≤ ε2.
n i=1
14
Under review as a conference paper at ICLR 2022
It is sufficient to find a vector v satisfying that
η⑴Hittv = α(t) Vzφi(h(w⑴;i)) for every i ∈ [n].
Since the solution v is in Rd and Vzφi(h(w⑶;i)) is in Rc, this condition is equivalent to a linear
system with n ∙ c constraints and d variables. Let A and b be the following stacked matrix and vector:
H1(t)η(t)
A =	...
HnW
∈ Rn∙c×d
∈ Rn∙c,
then the problem reduce to finding the solution of the equation Av = b. In the over-parameterized
setting where dimension d is sufficiently large (d》n ∙ c), then rank A = n ∙ C almost surely and
there exists almost surely a vector v that interpolates all the training set.
To demonstrate this fact easier, we consider a simple neural network where the classifier h(w; i) is
formulated as
h(w; i) = W(2)>σ(W(1)>x(i)),
where c = 1, W(1) ∈ Rm×l and W(2) ∈ Rl×1, w = vec({W (1), W(2)}) ∈ Rd is the vectorized
weight where d = l(m + 1) and σ is sigmoid activation function.
Hi(t) is defined to be the Jacobian matrix of h(w; i) at w(t):
Hitt := Jw h(w; i)lw=w(t) = [dh∂W?	... dh∂⅛r]	∈ R1×d,
w=w(t)
then
H1itt
a = η(t)	...	= η(t)
Hnt)
dh(w;1)
∂wι
dh(w;1)
∂wd
∈R
n×d
dh(w;n)
∂wι
dh(w;n)
∂wd	.
We want to show that A has full rank, almost surely. We consider the over-parameterized setting
where the last layer has at least n neuron (i.e. l = n and the simple version when c = 1. We argue
that rank of matrix A is greater than or equal to rank of the submatrix B created by the weights of
the last layer Wi2t ∈ Rn:
B
dh(w;1)
dW(2)
dh(w;1)
∂wn2)
∈R
n×n
dh(w;n)
∂W(2)
∂hι (w;n)
∂wn2) .
Note that h(∙, i) is a linear function of the last weight layers (in this simple case W⑵ ∈ Rn and
σ(W i1t>xiit) ∈ Rn), we can compute the partial derivatives as follows:
d∂W⅛Γ = σ(W⑴>x(it); i ∈ H
Hence
一 σ(W (1t>x⑴)一
B =	...	∈ Rn×n.
σ(W (1t>x(nt)
Assuming that there are no identical data, and σ is the sigmoid activation, the set of weights W(1t
that make matrix B degenerate has measure zero. Hence B has full rank almost surely, and we have
the same conclusion for A. Therefore we are able to prove the almost surely existence ofa solution
v of the linear equation Av = b for simple two layers network. Using the same argument, this result
can be generalized for larger neural networks where the dimension d is sufficiently large (d nc).
15
Under review as a conference paper at ICLR 2022
C.3 Initialization example
Our Assumption 3 requires a nice dependency on the tolerance ε for the gradient matrices Hi(0) and
Vzφi(h(w⑼;i)). We note that at the starting point t = 0, these matrices may depend on ε due
to the initialization process and the dependence of d on ε. In order to accommodate the choice of
learning rate η(0) = D√ε in our theorems, in this section We describe a network initialization that
satisfies ∣∣H(0)k = Θ (√1ε) where the gradient norm ∣∣Vzφi(h(w⑼;i))k is at most constant order
with respect to ε. To simplify the problem, we only consider small-dimension data and networks
without activation.
About the target vector: We choose φi to be the softmax cross-entropy loss. By Lemma 7 (see
below), we have that the gradient norm is upper bounded by a constant c, where c is the output
dimension of the problem and is not dependent on ε. Note that when we stack all gradients for n
data points, then the size of new vector is still not dependent on ε.
About the network architecture: For simplicity, we consider the following classification problem
where
•	The input data is in R2. There are only two data points {x(1), x(2)}. Input data is bounded
and non-degenerate (we will clarify this property later).
•	The output data is (categorical) in R2: {y(1) = (1, 0), y(2) = (0, 1)}.
We want to have an over-parameterized setting where the dimension of weight vector is at least
nc = 4. We consider a simple network with two layers, no biases and no activation functions. Let the
number of neurons in the hidden layer be m. The flow of this network is (in) R2 → Rm → R2 (out).
First, we consider the case where m = 1.
•	The first layer has 2 parameters (w1 , w2) and only 1 neuron that outputs z(i) = w1x(1i) +
w2x(2i) (the subscript is for the coordinate of input data x(i)).
•	The second layer has 2 parameters (w3, w4). The final output is
h(w, i) = [w3(w1x(1i) + w2x(2i)), w4(w1x(1i) + w2x(2i))]> ∈ R2,
with w = [w1, w2, w3, w4]> ∈ R4. This network satisfies that the Hessian matrices of h(w; i) are
bounded. Let Q and b be the following stacked matrix and vector:
Q
H1(0)
H2(0)
∈ R4×4
and b
Vzφ1(h(w(0); 1))
Vzφ2(h(w(0); 2))
∈ R4,
Then we have the following:
Q = Q(w)
H1(0)
H2(0)
Vw[w3(w1x(11) + w2x(21))]
Vw[w4(w1x(11) + w2x(21))]
Vw [w3(w1x(12) + w2x(22))]
Vw [w4(w1x(12) + w2x(22))]
(1)
w3 x1
(1)
w4x1
(2)
w3 x1
(2)
w4x1
w3 x2
w4x(21)
w3 x2
w4x(22)
w1x(11) + w2x(21)
0
w1x(12) + w2x(22)
0
The determinant of this matrix is a polynomial of the weight w and the input data. Under some mild
non-degenerate condition of the input data, we can choose some base point w0 that made this matrix
invertible (note that if this condition is not satisfied, we can rescale/add a very small noise to the
data - which is the common procedure in machine learning).
16
Under review as a conference paper at ICLR 2022
Hence the system Qu = b always has a solution. Now we consider the following two initializations:
1.	We choose to initialize the starting point at w(0) = √1εw0 and note that Q(W) is a linear function
of w and Q(w0) is independent of ε. Then the norm of matrix Q(w(0)) has the same scale with √1ε.
2.	Instead of choosing m = 1, we consider an over-parameterized network where m = ɪ (recall
that m is the number of neurons in the hidden layer). The hidden layer in this case is:
z(i)
zm
(i)
Z，
The output layer is:
Ivi)	— Ji)n∕2)	+	…+	Nc‰⑵=(w(1)χ(i)	+ w(1)√i))w⑵	+ …+	履⑴ ”)+	w(1) χ(i))w⑵
)y1	= z1 w1,1	+	+	Zm wm,1	=	(w1,1 x1	+ w2,1 x2 )w1,1	+	+(w1,mx1	+	w2,mx2 )wm,.
\ ”⑻=2(%”(2)+	…+	z(%”(2)=	(, ,∕1)τ(i)	+	⑴ 7(力?”(2)+ ∙∙∙	+	(?”⑴ T ⑴	+	w∕l) ʃ(i)), ,/2)
(y2	= z1 w1,2	+	+	Zm wm,2	=	(w1,1 x1	+ w2,1 x2 )w1,2	+	+	(w1,mx1	+	w2,mx2 )wm∕
with w = [w1(1,1), . . . , w1( Hence,		1,m) ,w2(1,1), . . . , w2(1,m) ,w1(2,1),w1(2,2),		(2) . . . , wm,1 ,	w(2) ]> wm,2]	∈ R4m.		
	(2) (1) w1,1x1	.	(2)	(1) . . wm,1x1	w1(2,1)x(21)	...	wm(2,)1x(21)	(1) Z1	0	...	Z(1) Zm	0
Q(w) =	(2) (1) w1,2 x1	(2)	(1) . . wm,2x1	w1(2,2)x(21)	. . .	wm(2,)2x(21)	0	Z(1) Z1	.	..0	Z⑴ Zm
	(2) (2) w1,1 x1	(2)	(2) . . wm,1x1	w1(2,1)x(22) ...	wm(2,)1x(22)	Z1(2)	0	...	Zm(2)	0
	(2) (2) w1,2x1	.	(2)	(2) . . wm,2x1	w1(2,2)x(22)	. . .	wm(2,)2x(22)	0	Z1(2)	.	..0	Zm
Hence, the number of (possibly) non-zero elements in each row is 3m = ɜ.
For matrix A of rank r, we have ∣∣Ak2 ≤ IlAIIF ≤ “k A∣∣2. Since the rank of Q(W) is at most 4
(nc = 4, independent of ε), we only need to find the Frobenius norm of Q(w). We have
u 4	4m
IQ(W)IF = tuXX |qij|2.
i=1 j=1
Let qmin and qmax be the element with smallest/largest magnitude of Q(W). Suppose that x(i) 6=
(0,0) and choose w = 0 such that z = 0, qmin > 0 and independent of ε. Hence, √∣qmin∣ ≤
IlQ(W)IlF ≤ √√ε2 |qmax|.
Hence, ∣Q(w)k = Θ (√ε). Therefore this simple network initialization supports the dependence
on ε for our Assumption 3. We note that a similar setting is found in (Allen-Zhu et al., 2019), where
the authors initialize the weights using a random Gaussian distribution with a variance depending
on the dimension of the problem. In non-convex setting, they prove the convergence of SGD using
the assumption that the number of neurons m depends inversely on the tolerance ε.
Lemma 7. For softmax cross-entropy loss, and x = h(w; i) ∈ Rc, for ∀w ∈ Rd and i ∈ [n], we
have
2
Vz φi(x)l	≤ c.
x=h(w;i)
(31)
Proof. By (24), we have for i = 1, . . . , n,
17
Under review as a conference paper at ICLR 2022
• For j 6= I(y(i)):
∂φi(x) I
∂xj	x=h(w;i)
exp ([h(w； i)j — [h(w； i)]I沙力)	丫
Pk=I exP ([h(w； i)]k - [h(w； i)]I(yd))) J
exP [h(w； i)]j - [h(w； i)]I(y(i))
1 + Pk=I(y(i)) exP ([h(w; i)]k - [h(w;切I(y(i)))
2
≤ 1.
• Forj = I(y(i)):
∂φi(x) I
∂xj	Ix=h(w;i)
Pk=I (y(i)) exP ([h(w; i)]k - [h(w； "]I(y(i))) !2
Pk=I exP ([h(w； i)]k - [h(w； i)]I(y(i)))	J
Pk=I(y(i)) exP ([h(w； i)]k - [h(w； i)]I(y(i))) !2 ≤ 1
1 + Pk = I(y(i) ) exP ([h(w； i)]k - [h(w； i)]I(y(i))) J —
Hence, for i = 1, . . . , n,
Vz φi(x)∣
x=h(w;i)
2
This completes the proof.
□
D Proofs of Lemmas and Corollary 1
Proof of Lemma 1
Proof. Since h(・；i) are twice continuously differentiable for all i ∈ [n], We have the following
Taylor approximation for each component outputs hj(•； i) where j ∈ [c] and i ∈ [n]:
hj(w(t+1)； i) = hj(w(t) - n(t)v(t)； i)
=hj(w(t)；i) - Jwhj(w； i)lw=w(t)n(t)v(t) + 1(η(t)v(t))>Mi,j(w(t))(n(t)v(t)),
(32)
where Mij(W(t)) is the Hessian matrices of hj(∙； i)at W(t) and W(t) = αw(t) + (1 — α)w(t+1) for
some α ∈ [0, 1]. This leads to our desired statement:
h(w(t+1)； i) = h(w(t) - n(t)v(t)； i) = h(w(t)； i) — n(t)H(t)v(t) + Eit),
where
e(tj = 1(η(t)v(t))>Mi,j(w(t))(n(t)v(t)), j ∈ [c],
2
Hence we get the final bound:
第 ≤ 1|(n(t)v(t))>Mij(W(t))(n(t)v(t))|
≤ ∣(η(t))2kv(t)k2 ∙kMi,j(w(t))k
(≤[(η⑴)2kv⑴k2G, j ∈ [c].
□
18
Under review as a conference paper at ICLR 2022
Proof of Lemma 2
Proof. From Assumption 3, We know that there exists v(ε) so that
n
X1 X kη(t) H(t^ε -αftNz φi(h(w(t); i))k2 ≤ ε2,
2n
i=1
and ∣∣V(ε)k2 ≤ V, for some V > 0. Hence,
n	22
21 X kη(t)Hi(t)V(ε) - α(t)Vz φi(h(w(t); i))k2 + - kv(ε)k2 ≤ ε2 + - V = (1 + ^ε2.
2n	2	2	2
i=1
Since v(2eg is the optimal solution of the problem in (15) for 0 ≤ t < T, we have
n	2V
2n X kη⑴H(t)v(treg - aT)Vzφi(h(w(t); i))k2 + -kv(t)egk2 ≤ (1 + 12)ε2.
i=1
Therefore, we have (17) and ∣∣v(treg∣∣2 ≤ 2 + V for 0 ≤ t < T.	口
Proof of Lemma 3
Proof. 1. We want to show that for any α ∈ [0, 1]
φ(αz1 + (1 - α)z2) ≤ αφ(z1) + (1 - α)φ(z2), ∀z1, z2 ∈ Rc,	(33)
in order to have the convexity of φ with respect to z (see (Nesterov, 2004)).
For any α ∈ [0, 1], we have for ∀z1, z2 ∈ Rc,
αkz1 - bk2 + (1 - α)kz2 - bk2 - kα(z1 - b) + (1 - α)(z2 - b)k2
= αkz1 - bk2 + (1 - α)kz2 - bk2 - α2kz1 - bk2 - (1 - α)2 kz2 - bk2
- 2α(1 - α)hz1 - b, z2 - bi
≥ α(1 一 α)∣∣zι — bk2 + (1 一 α)α∣∣Z2 — b∣∣2 — 2α(1 一 α)∣∣zι - b∣∣∙ ∣∣z2 一 b∣∣
= α(1 - α) (kz1 - bk - kz2 - bk)2 ≥ 0,
where the first inequality follows according to Cauchy-Schwarz inequality ha, bi ≤ IlalHIb∣. Hence,
2kαzι + (1 一 α)z2 一 bk2 ≤ 2kzι 一 bk2 + -~~2~~k^kz2 一 bk2.
Therefore, (33) implies the convexity of φ with respect to z.
2. We want to show that ∃Lφ > 0 such that
∣Vφ(zι) — Vφ(z2)k ≤ 刀0|0— Z2∣, ∀z1,z2 ∈ Rc.	(34)
Notice that Vφ(z) = z 一 b, then clearly ∀z1, z2 ∈ Rc,
∣Vφ(zi) —Vφ(z2)k = ∣Z1 — Z2∣.
Therefore, (34) implies the Lφ-smoothness of φ with respect to Z with Lφ = 1.	口
Proof of Lemma 4
Proof. 1. For ∀z1, z2 ∈ Rc and 1 ≤ k ≤ c, denote uk,1 = exp(wk>z1) and uk,2 = exp(wk>z2) and
using Holder inequality
c
Eak ∙ bk ≤
k=1
where ɪ + ɪ = 1,
(35)
19
Under review as a conference paper at ICLR 2022
we have
c
φ(αz1 + (1 - ɑ)z2) = log EeXP(W[(azι + (1 - a)z2)) = log
"=1	.
c
X碇,「峻-a)
.k=1
(35)
≤ log
cc
α log EeXP(W>zι) + (1 - α)log EeXP(W>z2)
==1	」	U =1
αφ(zι) + (1 - α)φ(z2),
where the first inequality since log(x) is an increasing function for ∀x > 0 and exp(v) > 0 for
∀v ∈ R. Therefore, (33) implies the convexity of φ with respect to z.
2. Note that ∣∣V2φ(z)k ≤ Lφ if and only if φ(z) is Lφ-smooth (see (Nesterov, 2004)). First, we
compute gradient of φ(z):
•	For i = a:
∂φ(z) _	exp(zi - Za)
—：----	--------：--------.
dzi	Pk = ι exp(z k - Za)
•	For i = a:
∂φ(z) _ - Pk=α exp(Zk - Za) _ - PC = 1 exp(zk - z0) + 1
—：----=-  -----------：-----：—=----- ---------：------：--
dzi	Pk=1 exp(Zk - Za)	Pk=1 exp(Zk - Za)
=-1 +  _________1________ = -1+ CeXP(Zi-Za)
Pk=1 eXp(zk - za)	Pk=1 eXp(zk - za)
We then calculate MjdZ∙
(dφ(z)、
k ∂zi J
•	For i = j:
∂2φ(z) __ exp(Zi - Za)[pk=1 exp(Zk - z0)] - exp(zi - z°)exp(zi - Za)
-...Σ-- - ------------------- ---------:-------TT------------------
∂Zj∂Zi	[£k=1 exp(zk - Za)]2
_ exp(Zi — Za )[pk=1 exp(zk — Za) — exp(zi — Za )]
[pk=1 exp(Zk - za)]2	^
•	For i = j :
∂2φ(z) _ - exp(Zj - Za) exp(Zi - Za)
—---二--=________C-------：------TT----
∂Zj∂Zi	[£k = 1 exp(zk - Za)]2
Denote that yi = exp(zi - Za) ≥ 0, i ∈ [c], we have:
•	For i = j :
∂2φ(z)
∂zj ∂Zi
yi(P⅛=1 yk - yi)
(Pk=I y )2
•	For i = j :
Id 2φ (Z) J =	Iyiyjl
I dzdzi 「(Pk=I yk)2
20
Under review as a conference paper at ICLR 2022
Recall that for matrix A = (aij) ∈ Rc×c: kAk2 ≤ kAk2F = Pic=1 Pjc=1 |aij |2. We have:
1
(Pk=ι y )4
Therefore,
c
yi2 (X yk -yi)2+X(yiyj)2
1
(Pk= y )4
1
(Pk=ι y )4
cc
yi2(Xyk)2 -2yi2Xyk.yi + yi4 + X(yiyj)2
k=1	k=1	j6=i
c	cc
yi2(Xyk)2-2yi3Xyk+yi2Xyk2
k=1	k=1	k=1
cc
kv2Φ(z)k2 ≤ XX
i=1 j=1
∂2φ(z)
∂zj∂zi
1
≤ (Pk=ι y )4
cc	cc	cc
(Xyi2)(Xyk)2 - 2(X yi3)(X yk) + (X yi2)(X yk2)
i=1	k=1	i=1 k=1	i=1	k=1
(Pc=I y2)(Pk=ι yk)2
(Pk= y )4
≤ (Pk= yQ4 = 1
≤ (Pk=ι y )4 =,
where the last inequality holds since
cc	cc	c
(Xyi2)(Xyk2) ≤(Xyi3)(Xyk)⇔(Xyk2)≤
i=1 k=1	i=1 k=1	k=1
cc
(Xyi3)(Xyk),
i=1	k=1
which follows by the application of Holder inequality (35) with p = 2, q = 2, ak = yk3/2 , and
bk = yk1/2 (Note that yk ≥ 0, k ∈ [c]). Hence, kv2φ(z)k ≤ Lφ with Lφ = 1 which is equivalent to
Lφ-smoothness of φ.	□
Proof of Lemma 6
Proof. Since k(∙; i) are twice continuously differentiable for all i ∈ [n], We have the following
Taylor approximation for each component outputs kj(∙; i) where j ∈ [c] and i ∈ [n]:
kj (w(t+1); i) = kj (w(t) - η(t)v(t); i)
=kj(w(t); i) — Jwkj(w; i)∣w=w(t)η(t)v(t) + 1(η(t)v⑴)，Mi,j(W(t))(n(t)v(t)),
(36)
where Mij(W(t)) is the Hessian matrices of kj(∙; i)at W(t) and W(t) = αw(t) + (1 — α)w(t+1) for
some α ∈ [0, 1].
Shifting this back to the original function hj (∙; i) we have:
hj (w(t+1); i) = kj (w (t+1); i) + (hj (w(t+1); i) — kj (w(t+1); i))
(=) kj(w(t); i) — Jwkj(w; i)lw=w(t)η(t)v(t) + 2(η(t)v(t))>Mi,j(W㈤)(η㈤V㈤)
+ (hj (w(t+1); i) — kj (w(t+1); i)),
=hj(w(t); i) — Jwkj(w; i)∣w=w(t)η(t)v(t) + 2(η(t)v(t))>Mi,j(W(t))(η(t)v(t))
+(hj(w(t+1);i) — kj(w(t+1); i)) + (kj (w(t); i) — hj(w(t); i)),
which leads to our desired statement:
h(w(t+1); i) = h(w(t) — η(t)v(t); i) = h(w(t); i) — η(t)Hi(t)v(t) + i(t),
21
Under review as a conference paper at ICLR 2022
where
的=1(#”TMij (W㈤)(η⑴V㈤)
,2
+ (hj (w(t+1); i) - kj (w(t+1); i)) + (kj (w(t); i) - hj (w(t); i)), j ∈ [c],
Hence we get the final bound:
∣⅛j∣≤ 1∣(η㈤ V ㈤)> Mij (W㈤)(η㈤V㈤)|
+ |hj (w(t+1); i) - kj (w(t+1); i)| + |kj(w(t); i) - hj(w(t);i)|
(<) 1 ∣(η⑴ V ㈤)>Mi,j (W㈤)(η㈤ V ㈤)| +2ε,
≤ 1(η(t))2kV(t)k2 TM"(W㈤)k +2ε
(< 1(η㈤向。⑴k2G + 2ε, j ∈ [c].
□
Proof of Corollary 1
Proof. The proof of this corollary follows directly by the applications of Lemmas 3 and 4. 口
E Technical Proofs for Theorem 1
Lemma 8. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds for V > 0, and
V(t) = V(t)eg. Consider η(t) = D√ε for some D > 0 and ε > 0. For i ∈ [n] and 0 ≤ t < T, we
have
ke(t)k2 ≤ 1 c(4 + (V + 2)GD2)2ε2.	(37)
Proof. From (14), for i ∈ [n], j ∈ [c], and for 0 ≤ t < T, by Lemma 1 and Lemma 6 we have
kfjl ≤ 2(η(t))2kV(t)k2G + 2ε ≤ 2(V + 2)GD2 ε + 2ε =2 ε(4+(V + 2)GD2),
where the last inequality follows by the fact ∣∣V(t)∣∣2 = |国%||2 ≤ 2 + V of Lemma 2 and η(t) =
D√ε. Hence,
c
k¥k2 = X H"I2 ≤ 4c(4 + (V + 2)GD2)2ε2.
j=1
□
Lemma 9. Let W(t) be generated by Algorithm 1 where we use the closed form solution for the
search direction. We execute Algorithm 1 for T = β outer loops for some constant β > 0. We
assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds
for V > 0. We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
ɑ(t) ≤ Lα, for some α ∈ (0, 3). For i ∈ [n] and 0 ≤ t < T, we have
kh(w(t+1); i) - h*k2 ≤ (1+ ε)kh(w⑴;i) - h*k2 - 2(1 - 3α)αit)[φi(h(w⑴;i)) - φi(h↑)]
+ (3ε +2) c(4 + (V + 2)GD2)2 ∙ ε
+ 3ε^kη(t)H(')Vi^')reg - α(t)Vzφi(h(w⑴;i))k2	(38)
22
Under review as a conference paper at ICLR 2022
Proof. Note that We have the optimal solution v(tTeg for the optimization problem (15) for 0 ≤ t <
T. From (12), we have, for i ∈ [n],
h(w(t+1); i) = h(w⑶一η⑴v(treg； i)
=h(w(t); i) - η⑴Hit)v(t?eg + eit)
=h(w(t); i) - α(t)Vzφi(h(w(e); i)) + ?-胪)Hittvtt维-α(t)Vzφi(h(w(t^; i))].
Hence, We have
kh(w(t+1)； i) - h* k2
=kh(w(t); i) - h - α(t)Vzφi(h(w⑶;i)) + ;一[η⑴Hit)Vag- α(tt Vzφi(h(w⑴;i))]∣∣2
=kh(w(t); i) - h*k2 + (α(t))2kVzφi(h(w(>; i))k2
+	+ kn(t)Hi(t)v(treg - α(t)Vzφi(h(w(t); i))∣∣2
-	2 ∙ (h(w(t); i) - %靖Vzφi(h(w(t); i))i
+ 2 ∙ hh(w(t); i) - h-,e(t)i
-	2 ∙ hh(w(t); i) - hi, η⑴ Hyv%- α(t)Vz φi(h(w ⑶;i))i
-	2 Yait)Vz6仆代；i)),例
+ 2 Yait)Vzφi(h(w6; i)),n(t)Hi(t)v(tteg - a(t)Vzφi(h(w⑶;i))i
-	2 ∙ he(((FttHFg-a、φi(h(w(); i))i,
Where We expand the square term. NoW applying Young’s inequalities: 2|hu, vi| ≤ kε∕22 + (ε∕2)kvk2
for ε > 0 and 2|hu, vi| ≤ kuk2 + kvk2 We have:
kh(w(t+1); i) -hiik2
= kh(w(t); i) - hiik2 + (ai(t))2kVzφi(h(w(t); i))k2
+	kei(t)k2 + kη(t)Hi(t)vi(tr)eg - ai(t)Vz φi(h(w(t); i))k2
-	2a(t) hh(w(t); i) - hi, Vzφi(h(w⑴;i))i
+	∣kh(w(t); i)-hik2 + εke(t)k2
+	∣kh(w(t); i) - h*k2 + ∣kn(t)Hi(t)v(tteg - ait)Vzφi(h(w∖ i))k2
+ 2(a(t))2kVzφi(h(w(t); i))k2 +2牌)『
+	2kη(t)Hi(t)vi(t)reg - ai(t)Vzφi(h(w(t); i))k2
(22)
≤ (1 + ∣)kh(w(t); i) - hiik2 + 3(ai(t))2kVzφi(h(w(t); i))k2
+(3+ 2)博)『+(3+|) kn(t)Hi(t)v(treg - a(t)Vzφi(h(w(t); i))k2
- 2ai(t)[φi(h(w(t); i)) -φi(hii)]
Note that from (23) We get that kVz φi(h(w(t); i))k2 ≤ 2Lφ[φi(h(w(t); i)) - φi(hii)]. Applying this
and using the fact that a(t) ≤ La, for some a ∈ (0, ɪ), we are able to derive:
kh(w(t+1); i) -hiik2
≤ (1 + ∣)kh(w(t); i) -hiik2 -2(1 - 3a)ai(t)[φi(h(w(t); i)) -φi(hii)]
+ — ke(t)k2 + — kn(t)Hi(t)v(treg - α(t)Vzφi(h(w(t); i))k2
≤ (1 + ∣)kh(w(t); i) - hiik2 - 2(1 - 3a)ai(t)[φi(h(w(t); i)) -φi(hii)]
23
Under review as a conference paper at ICLR 2022
+ 3ε+24c(4 +(V + 2)GD2)2ε2 + 3^旧⑴耳㈤。(%- α(t) Vzφi(h(wi∖ i))∣∣2
where the last inequality follows by Lemma 8.	口
Lemma 10. Let w(t) be generated by Algorithm 1 where we use the closed form solution for the
search direction. We execute Algorithm 1 for T = β outer loops for some constant β > 0. We
assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds
for V > 0. We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
α(t) = (1+ ε)α(t-1) = (1+ ε)tα(0). Based on β, we define α(0) = e0^ with α ∈ (0, 3).
We have
1 XX 1 XX [f (w(t)∙ i) - φ, (h*)]	≤	eβLφ(I + ε ∙ 1 XX	∣∣h(w(0).	i)	- h*k2	∙ ε
TLn 2Uf(W ; i) φi(hi)]	≤	2(1 - 3α)αβ n 匚	kh(w ;	i hik
t=0 i=1	i=1
eβL
+ a J、(3ε + 2) [c(4 +(V + 2)GD2)2 +8 +4V ] ∙ ε.
8α(1 - 3α)
(39)
Proof. Rearranging the terms in Lemma 9, we have
φi(h(w⑴;i)) - φ。(吟 ≤ 2∕1 ； ((ɪɪkh(w(t); i) - h：k2 - -4kh(w(t+1); i) - h：k2)
2(1 - 3α)	αi	αi
+ 8(i-3α) ∙ 01) ∙ ε(3ε + 2)c(4 +(V + 2)GD2)2
+	∙ 4) ∙ 3ε+^kη(t)Hi(t)v(treg - α(t)Vzφi(h(w⑴;i))∣2
2(1 - 3α) αi( ) ε
≤ 2(1⅛ (+ kh(w(t); i) - h：k2 - *kh(w(t+1); i) - h：k2!
eβL
+ an Φ、∙ ε(3ε + 2)c(4 + (V + 2)GD2 )2
8α(1 - 3α)
+ ?	∙ 3ε±2∣η⑴Hi㈤v(tr	a(t)Vzφi(h(w㈤;i))∣2.
2α(1 - 3α)	ε
The last inequality follows because the learning rate satisfies α(0) = ^^^_ ≤ La and for t =
1,...,T = β for some β > 0
α(t) = (1 + ε)α(tT) = (1 + ε)tɑ(0) ≤ (1 + ε)τα(0) = (1 + ε)βhε^α~ ≤ >
eβLφ	Lφ
since (1 + x)1/x ≤ e, x > 0. Moreover, We have -1t)≤ -1o)= e--Lφ-, t = 0,...,T — 1.
Taking the average sum from t = 0, . . . , T - 1, we have
T TX[Φi(h(w(t); i)) - φi(h"] ≤ 2(1 -13α)T ∙ (^kh(w⑼;i)一叫2
eβL
+ rτr-⅛5 ∙ ε(3ε + 2)c(4 + (V + 2)GD2)2
8α(1 - 3α)
+ ? 1K)∙ 3ε^τ1 ∑ kη(t)Hi(t)v(treg - α(t)Vzφi(h(w㈤;i))∣2
2α(1 - 3α)	εT
=；：LT Iε ∙kh(w⑼;i)-h"l2
2(1 - 3α)αβ
24
Under review as a conference paper at ICLR 2022
+ 8αe1 ,Φ3α) ∙ ε(3ε + 2)c(4 + (V + 2)GD2)2
+ ?”)∙ 3ε+^1 X kη(t)Ht)v2g - ait)Vzφi(h(w(t); i))k2
2α(1 - 3α) εT
Taking the average sum from i = 1, . . . , n, we have
1 T -1 1 n
T E-E[φi(h(w(t); i)) - Φi(h*)]
t=0 n i=1
eβLφ(1 + ε)
2(1 - 3α)αβε
-n
n X Ilh(W ⑼;i)-h"∣2
n i=1
+ 8θ⅛⅛ ∙ ε(3ε + 2)c(4+(V + 2)GD2)2
+⅛⅛ ∙ 2 T X n X 旧"4% - α' 弧应"；i))k2
t=0 i=1
(≤7) eβLφ(1+ ε)
_ 2(1 - 3α)αβε
1n
-X kh(w ⑼;i)-h"∣2
n i=1
+	eβ lφ
8α(1 — 3α)
+	eβ LΦ
2α(1 - 3α)
∙ε(3ε+2)c(4+ (V+2)GD2)2
∙ 7(2 +V)ε2.
ε
(40)
Note that
T -1	n	T -1	n
T X - X[Φi(h(w(t); i)) - Φi(hD] = T X - X[f (W⑴;i) - Φi(h"]∙
t=0 n i=1	t=0 n i=1
(41)
Therefore, applying (41) to (40), we have
1 T -1 1 n
T X - X[f (W⑴;i) - Φi(hD]
t=0	i=1
≤ ⅛⅛+⅛ ∙ - X M(w⑼；if ∙ ε
eβL
+ a J、(3ε + 2) [c(4 +(V + 2)GD2)2 +8 +4V] ∙ ε.
8α(1 - 3α)
which is our desired result.
Proof of Theorem 1
Proof. We have
F* = min F(W) = min ( X	fi(w)
w∈Rd	w∈Rd ∖- £
-理(X %(W))
1n
≥ - X
i=1
1n 1n
Wmni (fi(W)) = - X fi ≥ - X Mh"
(42)
i=1
i=1
≤
□
n
Hence F* - 1 pn=1 φi(h*) ≥ 0. Therefore
1T	1T 1n
T ∑[F(W㈤)-F*] = TE - £[f (w㈤;i) - φi(h*)]-
t=1	t=1	i=1
F* - - X φi(h*)
i=1
C
25
Under review as a conference paper at ICLR 2022
1T1n
≤ T ∑-∑[f (w(t)； i) — Φi(hi)]
t=1	i=1
(39)eβLφ(- + ε)
≤ 2(1 - 3α)αβ
1n
—Ekh(W ⑼;i)-h"∣2 ∙ ε
n i=1
+ eβLΦl- X)) [c(4 +(V + 2)GD2)2 + 8 + 4V]∙ ε
□
F Technical Proofs for Theorem 2
Lemma 11. For 0 ≤ t < T, suppose that Assumption 3 holds for V ≥ 0 and v(t) satisfies (19).
Then
kv(t)k2 ≤ 2(ε2+V+2).
Proof. From ∣∣v(t) - v*egk ≤ ε. Using ∣∣ak2 ≤ 2∣∣a - b∣∣2 + 2|网|2, We have
(19)
kv(t)k2 ≤ 2kv(t) - v(t)egk2 + 2kv(tregk2 ≤ 2ε2 +4 + 2V.
where the last inequality follows since |忖(?^||2 ≤ 2 + V for some V > 0 in Lemma 2.
□
Lemma 12. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds for V > 0.
Consider η(t) = D√ε for some D > 0 and ε > 0. For i ∈ [n] and 0 ≤ t <T, we have
ki(t)k2 ≤ c(2+ (V+ε2 + 2)GD2)2ε2.	(43)
Proof. From (14), for i ∈ [n], j ∈ [c], and for 0 ≤ t < T, by Lemma 1 and Lemma 6 we have
k(tj∣ ≤ -(η(t))2kv(t)k2G + 2ε ≤ -2(ε2 + V + 2)GD2ε + 2ε = ε(2 + (V + ε2 + 2)GD2),
,2	2
where the last inequality follows by the application of Lemma 11 and η(t) = D√ε. Hence,
c
ki(t)k2=X|i(,tj)|2 ≤c(2+(V+ε2+2)GD2)2ε2.
j=1
□
Lemma 13. Let w(t) be generated by Algorithm 2 where v(t) satisfies (19). We execute Algorithm
2 for T = β outer loops for some constant β > 0. We assume Assumption 1 holds. Suppose that
Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for H > 0.
We set the SteP size equal to η(t) = D√ε for some D > 0 and choose a learning rate α(t) ≤ La,
for some α ∈ (0,1). For i ∈ [n] and 0 ≤ t < T, we have
kh(w(t+1); i) - h*k2 ≤ (1+ ε)∣h(w⑴;i) - h*k2 - 2(1 - 4a)a「[φi(h(w⑴;i)) - φi(h*)]
+ε(4ε+3) D2H2 +c(2+(V+ε2 + 2)GD2)2
+ 4ε^kη(t)H(')vi^')reg - α(t)Vzφi(h(w⑴;i))k2	(44)
26
Under review as a conference paper at ICLR 2022
Proof. Note that v(t) is obtained from the optimization problem (15) for 0 ≤ t < T. From (9), we
have, for i ∈ [n],
h(w(t+1); i) = h(w(t) - η(t)v(t); i)
= h(w(t) ; i) - η(t)Hi(t)v(t) + i(t)
=h(w(t); i) - η㈤H(t)(v⑴-v(t)eg) - α(t)Vzφi(h(w⑶;i)) + e(t)
-[η(t)H(t)v* - α(t)Vzφi(h(w⑶;i))].
Hence, we have
kh(w(t+1); i) - h*k2
=∣∣h(w(t); i) - h — η(t)H(t)(v(t) - v(2eg) - α(t)Vzφi(h(w(t^; i))
+ ¥ -[η⑶Httvag-婕Vzφi(h(w⑶;i))]∣2
=kh(w(t); i) - h"2 + kη(t)H(t)(v(t) - v(treg)k2 + (α(tt)2kVzφi(h(w⑴;i))∣2
+ Htk2 + kη(t)H(t)v(treg - α(t)Vzφi(h(w6, i))k2
-	2 ∙ hh(w(t); i) - h"η⑴H(t(v⑴-V(^eg)
-	2 ∙ hh(w(t); i) - h*,α(t)Vzφi(h(w⑶;i))i
+ 2 ∙ hh(w(t); i)-%君、
-	2 ∙ hh(w(t); i) - h*,η(t)Hi(t)v(treg - α(ttVzφi(h(w6, i))i
+	2 ∙hη(t)H(t)(v(t) - v(treg),α(t)Vzφi(h(w(t); i))i
-	2 ∙hη⑴Htt(V⑺-V久g),e%
+ 2 ∙ hη⑶Hy(V© - V黑g), η⑶Hyv黑g - α?Vzφi(h(w⑴;i))i
-	2 ∙ hα(ttVzφi(h(w⑶;i)),e(tti
+	2 ∙ hα(t)Vzφi(h(w⑶;i))")Hittv2g-OttVzφi(h(w(t; i))i
-	2 Vt,秒Hitvag-婕Vφi(h(w", i))i,
where we expand the square term. Now applying Young’s inequalities: 2|hu, Vi| ≤ kε∕32 + (ε∕3)kv∣2
for ε > 0 and 2|hu, Vi| ≤ ∣u∣2 + ∣V∣2 we have:
kh(w(t+1t; i) - h*k2
=kh(w(tt; i) - h*k2 + kη(ttH(tt(v(tt - v(treg)k2 + (α(tt)2∣Vzφ<h(w⑶;i))∣2
+ Htk2 + kη(ttH(ttv(treg - α(ttVzφi(h(w(tt; i))∣2
+3 ∣h(w(tt; i) - ^k2+ε kη(ttH(tt(v(tt - v(treg)k2
-	2α(tthh(w(tt; i)-传,Vzφi(h(w⑶;i))i
+ 3kh(w(tt; i)-h∙Tk2 + 1 k≡(ttk2
+ 3kh(w(tt; i)-传『+ εkη(ttH(ttv(treg - α(ttVzφi(h(w(tt; i))k2
+ 3(η(tt)2kHi(tt(v(tt - v(tteg)k2 + 3(α(tt)2kVzφ4(h(w(tt; i))k2 + 3符『
+ 3kη(ttHi(ttv(treg - α(ttVzφi(h(w⑴;i))k2
(22t
≤ (1+ ε)kh(w(tt; i) - h*k2 +4(α(tt)2kVzφi(h(w(tt; i))k2
+(4+|) kη(ttH(tt(v(tt - v(treg)k2 + (4+ε)ke(ttk2
27
Under review as a conference paper at ICLR 2022
+ (4+?) kη(t)Hi(t)v(treg-α(t)vz6仆心"))『
—	2α(t)[φi(h(w(-); i)) - φi(h*)]
Notethatfrom (23) WegetthatkVzφi(h(w(t'); i))k2 ≤ 2Lφ[φi(h(w(t); i)) 一 φi(h*)]. Applying this
and using the fact that α(t) ≤ La, for some α ∈ (0,1), we are able to derive:
kh(w(t+1); i) — h*k2
≤ (1+ ε)kh(w㈤;i) — h*k2 — 2(1 - 4α)α(%φi(h(w⑴;i)) 一 φi(h*)]
+ ― kη(t)Hi(t)(v(t) - v(treg)k2 + 一符『
+ 4ε^kη⑴Hi㈤v(2eg — α(t)vzφi(h(w(∖ i))k2
≤) (1 + ε)kh(w⑴;i) — h*k2 - 2(1 - 4α)α(t)[φi(h(w(t); i)) — φi(h*)]
+ ― D2εHε2 kv(t) - v(tregk2 + T 甘)『
+ 4ε^kη(t)Hi(t)v(treg — α(t)Vzφi(h(w㈤;i))k2
≤) (1 + ε)kh(w㈤;i) — h*k2 - 2(1 - 4α)α(t)[φi(h(w(t); i)) — φi(h↑)]
+ ”土刍D2H2 ∙ ε + ”+刍∙ c(2 + (V + ε + 2)GD2)2ε2
εε
+ 4ε^kη⑴Hi㈤v(2eg — α(t)Vzφi(h(w⑶;i))k2
= (1+ ε)kh(w㈤;i) — h*k2 - 2(1 - 4α)α(t)[φi(h(w(t); i)) — φi(h*)]
+ε(4ε+ 3) D2H2 +c(2+ (V+ε2 + 2)GD2)2
+ 4ε^kη(t)Hi(t)v(treg — α(t)Vzφi(h(w㈤;i))k2
where (a) follows by using matrix vector inequality kHv k ≤ kH kkv k, where H ∈ Rc×d and
V ∈ Rd and Assumption 4 in (20) and η(t) = D√ε for some D > 0 and ε > 0; (b) follows by the
fact that kv(t) — v(trɛgk2 ≤ ε2 in (19) and Lemma 12.	□
Lemma 14. Let w(t) be generated by Algorithm 2 where v(t) satisfies (19). We execute Algorithm
2 for T = β outer loops for some constant β > 0. We assume Assumption 1 holds. Suppose
that Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for
H > 0.We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
α(t) = (1+ ε)α(t-1) = (1+ ε)tα(0). Based on β, we define α(0) = eβL^ With α ∈ (0, 4).
We have
1 T-1 1 n
T ∑n∑[f (w(t); i) - φi(h"]
t=0 n i=1
≤ e≡+⅛ ∙ 1 X kh(w(0); ik2 ∙ ε
+ e： Lφ[4ε + 3) [D2H2 + c(2 +(V + ε2 + 2)GD2)2 + 2 + V ] ∙ ε.	(45)
2α(1 — 4α)
Proof. Rearranging the terms in Lemma 13, we have
Φi (h(w(t); i)) — Φi(K) ≤ 2J4、((ɪɪ kh(w(t); i) — h：k2 - -4 kh(w(t+1); i) — Kk2
2(1 — 4α) αi	αi
28
Under review as a conference paper at ICLR 2022
ι	1
+ 2(1 - 4α)
ι	1
+ 2(1 - 4α)
ɪ ∙ ε(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2]
αi
4) ∙ 4f^kn(t)H(t)v(treg - α(t)Vzφi(h(w⑴;i))k2
αi	ε
≤ 2(T⅛ (+kh(w㈤;ik2 — ⅛+⅛kh(w(t+1)；ik2
+ 2αe1 Jφ4α) ∙ ε(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2]
+ y^β⅛1 ∙ 4ε^kn(t)H㈤v* — a(t)Vzφi(h(w㈤;i))k2.
2α(1 - 4α)	ε
(46)
The last inequality follows because the learning rate satisfies α(O) = ^^^ ≤ La and for t
1,...,T = β for some β > 0
α(t) = (1+ ε)α(tτ) = (1+ ε)tɑ(0) ≤ (1 + ε)Ta(0) = (1 + ε产ε☆ ≤ 言，
eβ Lφ	Lφ
since (1 + x)1/x ≤ e, x > 0. Moreover, We have -⅛- ≤ -⅛) = e-Lφ, t = 0,...,T — 1.
αi αi	α
Taking the average sum from t = 0, . . . , T - 1, we have
1 T-1	1
T X[φi(h(w㈤;i)) — φi(hi)] ≤ 2(1-4°)T -
+ eβ Lφ
+ 2α(1—4α)
⅛ε) kh(w⑼;i) — hi k2
αi
- ε(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2]
+	eβ Lφ
+ 2α(1—4α)
T-1
―T X kη⑴"衰 —
ε T t=0
α(ttVz φi (h(w㈤;i))k2
⅜L⅛+⅛ε ∙kh(W(O); iik2
βL
+ φ ∙ ε(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2]
2α(1 — 4α)
+ ? ：；Lφ)∙ 4ε^^ ∑ kn(t)H(t)v(treg — α(t)Vzφi(h(w㈤;i))k2
2α(1 — 4α)	εT
Taking the average sum from i = 1, . . . , n, we have
1 T-1 1 n
T E-E[Φi(h(w(t); i)) - Φi(hD]
t=O n i=1
≤ ⅛⅛⅛ ε ∙ n X kh(w⑼;i) - hik2
βL
+ φ ∙ ε(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2]
2α(1 — 4α)
+ 2θ(⅛ - 3T ∑ n X kn(t)Hi(t)v(treg - α(t)vz…t; i))k2
t=O	i=1
(P eβLφ(1+ ε)
_ 2(1 — 4α)αβε
1n
-X kh(w(0); i) - hik2
n i=1
29
Under review as a conference paper at ICLR 2022
e l@
2a(1 - 4α)
eβ Lφ
2a(1 - 4α)
+
• ε(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2]
3(2 +V )ε2.
ε
(47)
+
Note that
T-1 n	T-1 n
不 X — X[φi(h(w ㈤;i)) - φi(h*)]=不 X — Xf (W ⑴;i) - φi(h-)].	(48)
T t=0 n i=1	i T t=0 n i=1	i
Therefore, applying (48) to (47), we have
1 T-1 1 n
T ∑~∑[f(w(t)； i) — Φi(h：)]
t=0 n i=1
≤ e：Lφ(1 + ε) • 1 X kh(w⑼;i) - h： k2 • ε
2(1 - 4α)αβ n
eβ L
+ φ	(4ε + 3) [D2H2 + c(2 + (V + ε2 + 2)GD2)2 + 2 + V] • ε.
2α(1 - 4α)
□
Proof of Theorem 2
Proof. From (42) We have F： - 1 Pn=I φi(h*) ≥ 0. This leads to
1T	1T 1n
T Xf(W⑴)-f：] = T X - Xf(w⑴;i) - φi(h*)]-
t=1	t=1	i=1
F：- - X φi(hj)
(42) T n
≤ T X- Xff (w(t); i) - φi(h*)]
(45)
≤
⅛⅛+⅛ ∙ 1X kh(w(0); i)-h：k2 • ε
+ :力产 + 3)[D2H2 + c(2 + (V + ε2 + 2)GD2)2 +2 + V] • ε.
2α(1 - 4α)
(49)
□
Proof of Corollary 2
Proof. For each iteration 0 ≤ t < T, We need to find v(t) satisfying the folloWing criteria:
kv(t) - v：(tr)eg k2 ≤ ε2 ,
for some ε > 0. Using Gradient Descent We need O(nL log(表))= O(2nL log(ɪ)) number of
gradient evaluations (Nesterov, 2004), where L and μ = ε2 are the smooth and strongly convex
constants, respectively, of Ψ. Let
ψi(v) = 1 kη(t)H(t)v - α(t)Vzφi(h(w(t); i))k2, i ∈ H	(50)
Then, for any v ∈ Rc
Vvψi(v) = η(t)H(t)>[η(t)Hi(t)v - α(t)Vzφi(h(w(t); i))], i ∈ [n].	(51)
30
Under review as a conference paper at ICLR 2022
Consider η(t) = D√ε for some D > 0 and ε > 0, we have for i ∈ [n] and 0 ≤ t < T
kVVψi(v)k = (η(t))2kHi(t)>H(t)k ≤ (η⑴)2kH(t)k∙kH(t)k (≤ D2H2.
Hence, ∣WΦ(v)k ≤ ∣∣VVψi(v)k + ε2 for any V ∈ Rc which implies that L = D2H2 + ε2 (Nes-
terov (2004)) and L = D H2+ε . Therefore, the complexity to find v(t) for each iteration t is
μ⅛	Cε
O(2nd2H+2 log( 1)).
Let us choose 0 < ε ≤ 1. From (49), we have
T-1	β	n
1 X [F (w(t)) - F*] ≤ n e4 φ R ∙ 1 X kh(w(0); i) - hik2 ∙ ε
T t=0	(1- 4α)αβ n i=1	i
+	/产工；[D2H2 + c(2 + (V + 3)GD2)2 + 2 + V] ∙ ε = Nε,
2α(1 - 4α)
where
n
N =	_eβLφ___X Ilh(W(O)∙	i)	_ h* k2 +_7eβLφ	[D2 H2 + c(2 + (V + 3)GD2)2	+ 2 + V]
N =	(1-4α)αβ n	乙 Ilh(W ；	i) hi k +	2α(1-4α)	LDH + c(2+( V + 3)GD )	+2+ V J	.
i=1
Let ε = Nε with 0 < ε ≤ N. Then, We need T = N^β for some β > 0 to guarantee
mino≤t≤τ-ι[F(w(t)) 一 F*] ≤ T PT01[F(w(t)) 一 F*] ≤ ε. Hence, the total complexity is
O (nN^3β(D2H2 + (ε2∕N))log(N)).	口
31