Under review as a conference paper at ICLR 2022
SGEM: stochastic gradient with energy and
MOMENTUM
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose SGEM, Stochastic Gradient with Energy and Momen-
tum to solve a large class of general non-convex stochastic optimization problems,
based on the AEGD method that originated in the work [AEGD: Adaptive Gradi-
ent Descent with Energy. arXiv: 2010.05109]. SGEM incorporates both energy
and momentum at the same time so as to inherit their dual advantages. We show
that SGEM features an unconditional energy stability property, and derive energy-
dependent convergence rates in the general nonconvex stochastic setting, as well
as a regret bound in the online convex setting. A lower threshold for the energy
variable is also provided. Our experimental results show that SGEM converges
faster than AEGD and generalizes better or at least as well as SGDM in training
some deep neural networks.
1 Introduction
In this paper, we propose SGEM: Stochastic Gradient with Energy and Momentum to solve the
following general non-convex stochastic optimization problem
min f (θ) := Eξ[f(θ; ξ)],	(1)
θ∈Rd
where Eξ [∙] denotes the expectation with respect to the random variable ξ. We assume that f is
differentiable and bounded from below, i.e., f * = infθ∈Rd f (θ) > -C for some c > 0.
Problem (1) arises in many statistical learning and deep learning models (LeCun et al., 2015; Good-
fellow et al., 2016; Bottou et al., 2018). For such large scale problems, it would be too expensive to
compute the full gradient Vf (θ). One approach to handle this difficulty is to use an unbiased esti-
mator of Vf (θ). Denote the stochastic gradient at the t-th iteration as gt, the iteration of Stochastic
Gradient Descent (SGD) (Robbins & Monro, 1951) can be described as:
θt+1 = θt - ηtgt,
where ηt is called the learning rate. Its convergence is known to be ensured if ηt meets the sufficient
condition:
∞∞
Xηt = ∞, Xηt2 < ∞.	(2)
However, vanilla SGD suffers from slow convergence due to the variance of the stochastic gradient,
which is one of the major bottlenecks for practical use of SGD (Bottou, 2012; Shapiro & Wardi,
1996). Its performance is also sensitive to the learning rate, which is tricky to tune via (2). Dif-
ferent techniques have been introduced to improve the convergence and robustness of SGD, such
as variance reduction (Defazio et al., 2014; Lei et al., 2017; Johnson & Zhang, 2013; Osher et al.,
2019), momentum acceleration (Allen-Zhu, 2018; Sutskever et al., 2013), and adaptive learning rate
(Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2017). Among these, momentum and
adaptive learning rate techniques are most economic since they require slightly more computation
in each iteration. However, training with adaptive algorithms such as Adam or its variants typically
generalizes worse than SGD with momentum (SGDM), even when the training performance is better
Wilson et al. (2018).
The most popular momentum technique, Heavy Ball (HB) (Polyak, 1964) has been extensively
studied for stochastic optimization problems (Liu et al., 2020b; Jin et al., 2018; Qian, 1999). SGDM,
1
Under review as a conference paper at ICLR 2022
also called SHB, as a combination of SGD and momentum takes the following form
mt = μmt-i + gt, θt+ι = θt - ηmt,
where mo = 0 and μ ∈ (0,1) is the momentum factor. This helps to reduce the variance in stochastic
gradients thus speeds up the convergence, and has been found to be successful in practice (Sutskever
et al., 2013).
AEGD originated in the work Liu & Tian (2020) is a gradient-based optimization algorithm that
adjusts the learning rate by a transformed gradient v and an energy variable r. The method includes
two ingredients: the base update rule:
rt
θt+ι = θt + 2ηrt+ι vt,	rt+ι = ι + 2小2,	(3)
and the stochastic evaluation of the transformed gradient vt as
Vt = 一/	gt	.	(4)
2Pf (θt; ξt) +c
AEGD is unconditionally energy stable with guaranteed convergence in energy regardless of the size
of the base learning rate η > 0 and how vt is evaluated. This explains why the method can have a
rapid initial training process as well as good generalization performance (Liu & Tian, 2020).
In this paper, we attempt to incorporate both energy and momentum at the same time so as to inherit
their dual advantages. We do so by keeping the base AEGD update rule (3), but taking
______mt______
2(i- βt)PMiξy+^
mt = βmt-1 + (1 - β)gt,	β ∈ (0, 1).
(5)
We call this novel method SGEM. An immediate advantage is that with such vt one can significantly
reduce the oscillations observed in the AEGD in stochastic cases. Regarding the theoretical results,
in this work we develop a convergence theory for SGEM, in both stochastic nonconvex setting and
online convex setting. While in Liu & Tian (2020), convergence analysis is provided mainly in
deterministic setting, and the result in the stochastic setting is only an upper bound on the norm of
the stochastic transformed gradient V rather than on Vf (θ).
We highlight the main contributions of our work as follows:
•	We propose a novel and simple gradient-based method SGEM which integrates both energy
and momentum. The only hyperparameter requires tuning is the base learning rate.
•	We show the unconditional energy stability of SGEM, and provide energy-dependent con-
vergence rates in the general stochastic nonconvex setting, and a regret bound for the online
convex framework. We also obtain a lower threshold for the energy variable. Our assump-
tions are natural and mild.
•	We empirically validate the good performance of SGEM on several deep learning bench-
marks. Our results show that
-	The base learning rate requires little tuning on complex deep learning tasks.
-	Overall, SGEM is able to achieve both fast convergence and good generalization per-
formance. Specifically, SGEM converges faster than AEGD and generalizes better or
at least as well as SGDM.
Related works. The essential idea behind AEGD is the so called Invariant Energy Quadratiza-
ton (IEQ) strategy, originally introduced for developing linear and unconditionally energy stable
schemes for gradient flows in the form of partial differential equations (Yang, 2016; Zhao et al.,
2017). As for gradient-based methods, there has appeared numerous works on the analysis of con-
vergence rates. In online convex setting, a regret bound for SGD is derived in Zinkevich (2003); the
classical convergence results of SGD in stochastic nonconvex setting can be found in Bottou et al.
(2018); For SGDM, we refer the readers to Yu et al. (2019); Yan et al. (2018); Liu et al. (2020b)
for convergence rates on smooth nonconvex objectives. For adaptive gradient methods, most con-
vergence analysis are restricted to online convex setting (Duchi et al., 2011; Reddi et al., 2018; Luo
et al., 2019), while recent attempts, such as Chen et al. (2019); Zou et al. (2019), have been made to
analyze the convergence in stochastic nonconvex setting.
2
Under review as a conference paper at ICLR 2022
This paper is organized as follows. We first review AEGD in Section 2, then introduce the proposed
algorithm in Section 3. Theoretical analysis including unconditional energy stability, convergence
rates in both stochastic nonconvex setting and online convex setting are presented in Section 4. In
Section 5, we report some experimental results on deep learning tasks.
Notation For a vector θ ∈ Rn, we denote θt,i as the i-th element of θ at the t-th iteration. For vector
norm, We use k ∙ k to denote l2 norm and use ∣∣∙∣∣∞ to denote l∞ norm. We also use [m] to represent
the list {1, ..., m} for any positive integer m.
2 Review of AEGD
Recall that for the objective function f, We assume that f is differentiable and bounded from beloW,
i.e., f(θ) > -c for some c > 0. The key idea of AEGD introduced in Liu & Tian (2020) is the use
of an auxiliary energy variable r such that
Vf (θ) = 2rv, V := Va∕/(θ) + c,
(6)
where r, taking as，f (θ) + C initially, will be updated together with θ, and V is dubbed as the
.	i'	Λ	Λ∙ . EI	Λ∙ . n	A	X_7 f / Zi∖ ∙ .1	1	1 Λ
transformed gradient. The gradient flow θ = -Vf (θ) is then replaced by
θ = -2rv, r = v ∙ θ.
A simple implicit-explicit discretization gives the following AEGD update rule:
Vf (θt)
Vt = ,	=.
2 √f(θt)+ C
θt+1 = θt - 2ηrt+1Vt,
rt+1 — r = Vt ∙ (θt+ι — θt).
(7a)
(7b)
(7c)
This yields a decoupled update for r as rt+1 = rt∕(1 + 2η∣vt∣2), which serves to adapt the learn-
ing rate. For large-scale problems, stochastic sampling approach is preferred. Let f(θt; ξt) be a
stochastic estimator of the function value f(θt) at the t-th iteration, gt be a stochastic estimator of
the gradient Vf (θt), then the stochastic version of AEGD is still (7) but with Vt replaced by
gt
2Pf(θt; ξt) + C
Usually, gt should be required to satisfy E[gt] = Vf(θt) and E[kgtk2] bounded. Correspondingly,
an element-wise version of AEGD for stochastic training reads as
gt,i
vt,i =2" ξt)+ C, i ∈ [n],
rt+1,i = 1 1Wv2 ,， r1,i = Pf (θ1; ξ1) + C，
θt+1,i = θt,i — 2ηrt+1,iVt,i .
(8a)
(8b)
(8c)
The element-wise AEGD allows for different effective learning rates for different coordinates, which
has been empirically verified to be more effective than the global AEGD (7). For further details, we
refer to Liu & Tian (2020). We will focus only on the element-wise version of SGEM in what
follows.
3 The proposed algorithm
In this section, we present a novel algorithm to improve AEGD with added momentum in the fol-
lowing manner:
mt = βmt-1 + (1 — β)gt, m0 = 0,
mt
Vt = --------,	,
2(1 — βt)Pf (θt; ξt) + c
(9a)
(9b)
3
Under review as a conference paper at ICLR 2022
where β ∈ (0, 1) controls the weight for gradient at each step. With vt so defined, the update
rule for r and θ are kept the same as given in (8b, c). The relation between the energy and the
momentum in the algorithm is realized through relating mt ( as an approximation to Vf) to Vt
(as an approximation of VF = 2√+), where Vt is used to update the energy rt+ι. In machine
learning tasks, f as a loss function is often in the form of f (θ) = ^^ Pm=I li(θ), where li, measuring
the distance between the model output and target label at the i-th data point, is typically bounded
from below, that is, li (θ) > -c, ∀i ∈ [m], for some c > 0. Hence c in (9b) can be easily chosen
in advance so that f(θt; ξt) as a random sample from {li(θt)}im=1 is bounded below by -c for all
t ∈ [T]. We summarize this in Algorithm 1 (called SGEM, for short).
A key feature of SGEM is that it incorporates momentum into AEGD without changing the overall
structure of the AEGD algorithm (the update of r and θ remain the same) so that it is shown (in
Section 4) to still enjoy the unconditional energy stability property as AEGD does. In addition, by
using mt instead of gt, the variance can be largely reduced. In fact, as proved in Liu et al. (2020b),
under the assumption Eξt [kgt - Vf(θt)k* 1 2 * 4] = σg2 < ∞, mt, which can be expressed as a linear
combination of the gradients at all previous steps,
t
mt=(1-β)Xβt-jgj,	(10)
j=1
enjoys a reduced “variance” in the sense that
Eξt	mt-(1-β)Xt βt-jVf(θj)2 ≤ (1-β)σg2.
Algorithm 1 SGEM. Good default setting for parameters are η = 0.2, β = 0.9
Require: the base learning rate η; a constant c such that f(θt; ξt) + c > 0 for all t ∈ [T]; a
momentum factor β ∈ (0, 1).
Require: Initialization: θι; m° = 0; ri =，f (θι; ξι) + C1
1:	for t = 1 to T - 1 do
2:	Compute gradient: gt = Vf(θt; ξt)
3:	mt = βmt-ι + (1 — β)gt (momentum update)
4:	Vt = mt∕(2(1 一 βt)y/f (θt; ξt) + c) (transformed momentum)
5:	rt+1 = rt/(1 + 2ηVt	Vt) (energy update)
6:	θt+1 = θt 一 2ηrt+1 Vt (state update)
7:	end for
8:	return θT
Remark 3.1. (i) In Algorithm 1, we use x y to denote element-wise product, x/y to denote
element-wise division of two vectors x, y ∈ Rn.
(ii) It is clear that mt defined in (10) is not a convex combination of gj, this is why there is a factor
1 一 βt in (9b); such treatment is dubbed as bias correction in Kingma & Ba (2017) for Adam.
(iii) In most machine learning problems, we have f(θ) ≥ 0, for which a good default value for c in
Algorithm 1 is 1.
4 Theoretical results
In this section, we present our theoretical results, including the unconditional energy stability of
SGEM, the convergence of SGEM for the general stochastic nonconvex optimization, a lower bound
for energy rT , and a regret bound in the online convex setting.
4.1 Unconditional energy stability
Theorem 4.1. (Unconditional energy stability) SGEM in Algorithm 1 is unconditionally energy
stable in the sense that for any step size η > 0,
E[r2+ι,i] = E[r2,i] 一 E[(rt+i,i -『t,i)2] 一 η-1E[(θt+ι,i - θt,i)2],	i ∈ H (11)
4
Under review as a conference paper at ICLR 2022
that is E[rt,i] is strictly decreasing and convergent with E[rt,i] → E[r*] as t → ∞, and also
∞
lim E[(θt+1,i - θt,i)2] =0,	X E[(θt+1,i - θt,i)2] ≤η(f(θ1)+c), ∀i ∈ [n].	(12)
t→∞
t=1
Remark 4.1. (i) The unconditional energy stability only depends on (8b, c), irrespective of the
choice for Vt. This property essentially means that the energy variable r, which serves to approxi-
mate ，f (θt) + C, is strictly decreasingfor any η > 0.
(ii)(12) indicates that the SequenCe k θt+ι 一 θt k converges to zero at a rate ofat least 1∕√t. We note
that this does not guarantee the convergence of {θt} unless additional information on the geometry
of f is available.
Proof. From (8b, c) we have
(θt+1,i - θt,i)2 = 4η2rt2+1,ivt2,i (By 8c)
= (2ηrt+1,i)(rt,i - rt+1,i) (By 8b)
= η((rt2,i - rt2+1,i) - (rt,i - rt+1,i)2).
This upon taking expectation ensures the asserted properties. Such proof with no use of the special
form of vt, is the same as that for AEGD (See LiU & Tian (2020)).	□
4.2 Convergence analysis
Below, we state the necessary assUmptions that are commonly Used for analyzing the convergence
of a stochastic algorithm for nonconvex problems, and notations that will be Used in oUr analysis.
Assumption 4.1. 1. (Smoothness) The objective function in (1) is L-smooth: for any x, y ∈ Rn,
f (y) ≤ f(x) + Vf (χ)>(y - x) + Lky - xk2.
2.	(Independent samples) The random samples {ξt}t∞=1 are independent.
3.	(Unbiasedness) The estimator of the gradient and function value are unbiased:
Eξt [gt] = Vf(θt), Ea [f(θt; ξt)] = f(θt).
Denoting the variance of the stochastic gradient and fUnction valUe by σg and σf, respectively:
Eξt[kgt - Vf(θt)k2] = σg, Eξt[|f(θt;ξt) - f(θt)∣2] = σf.
We have the following resUlts.
Theorem 4.2. Let {θt } be the solution sequence generated by Algorithm 1 with a fixed η > 0.
Under Assumption 4.1 and assume that the stochastic gradient and function value are bounded such
that kgtk∞	≤ G∞ and 0 < a ≤ f(θt;	ξt) + c ≤ B,	then σg ≤ G∞ and for all	T	≥ 1,
T e" min rτ,i XX	皿(%)『]≤ CI + C2n +°3吗	,
T i t=1	ηT
where C1, C2, C3 are constants depending on β, η,L, G∞, a, B, n and f(θ1) + c.
Remark 4.2. (i) Numerically we observe that for reasonable choice ofη, rt,i decays much slower
than 1∕√t (See Figures 1), thus the convergence result in Theorem 4.2 is meaningful. The question
of how rT depends on T is theoretically interesting but subtle to characterize. Nevertheless, in The-
orem 4.3 below, we identify a sufficient condition for ensuring a lower threshold for E[rT,i], from
which we see that in the absence of noise, i.e σg = 0, mini r↑ > 0 can be ensured, then the rate of
O(1/T) is recovered in Theorem 4.2.
(ii)	The assumption that the magnitude of the stochastic gradient is bounded is standard in non-
convex stochastic analysis (Bottou et al., 2018). As for the upper bound on the stochastic func-
tion value, we recall the new introduced update rule in SGEM (9a,b): to bound vt, we don’t
need an upper bound on f; while such upper bound is technically needed to bound mt since
mt = 2(1 — βt)√f + Cvt.
5
Under review as a conference paper at ICLR 2022
Figure 1: mini rt,i of SGEM with default base learning rate 0.2 in training DL tasks.
We only present a sketch of proofs for Theorem 4.2 and 4.3 here, using notation Ft = * 1
f θ(θt; ξt) + c, ηt = n/Ft and viewing rt+1 as a n X n diagonal matrix that is made UP of
[rt+1,1, ..., rt+1,i, ..., rt+1,n]. Detailed proofs, including two crucial lemmas and the full proof for
Theorem 4.4, are deferred to the appendix.
Proof. Using the L-smoothness off , we have
f(θt+ι) - f (θt) ≤ V/(θt)>(θt+ι - θt) + 2kθt+ι - θtk2.	(13)
The first term on the RHS is carefully regrouped as
—1β VfR )> nt-ι rt gt +1β vf(θt )>(nt-ι rt — nt rt+ι )gt — 7ɪt VfR )> nt rt+ι mt-ι∙
1 -βt	1 -βt	1 -βt
Taking a conditional expectation on the first term gives
^ent-1 Vf (θt)trtVf(θt) ≥ (1 - β)√n= minrt,z∣∣Vf(θt)∣∣2∙
1-βt	B i
We manage to bound the other two terms in terms of Pin=1 PtT=1 rt+1,igt2,i and
Pin=1 PtT=1 rt+1,imt2,i. Their bounds are presented in Lemma A.2. The asserted bound then fol-
lows by further summation in t with telescope cancellation for f(θt+1) - f(θt) and bounding the
last term in (13) using (12).	□
4.3 Lower bound for the energy
First note that the L-smoothness of f (θ) implies the LF-smoothness of F(θ) = f θ(θ) + c with
LF =
1
2F (θ *)
L+
2F 2(θ*))
(14)
This will be used in the following result and its proof.
Theorem 4.3 (Lower bound of rT). Under the same assumptions as in Theorem 4.2, we have
minE[rTi] ≥ max{F(θ*) - nD1 - βD2 - σD3, 0},
i,
(15)
where σ = max{σf , σg}≤ max{G∞ , B} with LF given in (14) and
L	LF nF 2 (θι)	n	√BnF (θι)
DI =	2,	D2 =(i- β)√a,
Moreover, in the absence of noise, we have
minrτ,i > minr* > 0 if nD1 + μD2 < F(θ*).
(16)
6
Under review as a conference paper at ICLR 2022
Remark 4.3. (i) (16) is only a sufficient condition, not used as a guide for choosing η. We observe
from our experimental results that the upper bound for η to guarantee the positiveness of r* can be
much larger (See Figure 1).
(ii)	In Theorem 4.3, we measure how far r* can deviate from F (θ*) in the worst situation. Under
the stochastic nonconvex setting, ηD1 is the error brought by the step size η, βD2 is due to the use
of momentum, and σD3 is responsible for the existence of noise.
(iii)	In the case of no momentum and no noise, we have
min rT i > min ri* > 0
i, i
F(θ*)
If η<-DT
This captures the result for the deterministic AEGD obtained in Liu & Tian (2020).
Proof. Using the LF -smoothness of F (θ), we have
F(Θt+1) - F(θt) ≤ VF(θt)>(θt+ι - θt) + L2Fkθt+ι - θtk2,	(17)
in which the key term VF(θt)>(θt+ι 一 θt) can be decomposed into three terms:
(VF(θt) - 2F)>(θt+ι - θt),	(2F - ι -β)>(θt+ι - θt), (I -β)>(θt+ι - θt),
The first two terms are bounded by using the bounded variance assumption and (12), respectively.
We convert the last term, using (recall 7c)
rt+1,i - rt,i = vt,i(θt+1,i - θt,i),
into expressions in terms of rt+1,i - rt,i, which upon summation is bounded by rT,i. The last term
in (17) is bounded again by using (12).	□
4.4 Regret bound for Online convex optimization
Our algorithm is also applicable to the online optimization that deals with the optimization problems
having no or incomplete knowledge of the future (online). In the framework proposed in Zinkevich
(2003), at each step t, the goal is to predict the parameter θt ∈ F , where F ⊂ Rn is a feasible set,
and evaluate it on a previously unknown loss function ft . The nature of the sequence is unknown
in advance, the SGEM algorithm needs to be modified. This can be done by replacing f(θt, ξt) by
ft(θt) and taking gt = Vft(θt) invt defined in (9), i.e.,
mt =βmt-1+(1 - β)Vft(θt),
mt
Vt =-----------,	==.
2(1 — βt)Pft(θt) + C
(18a)
(18b)
This algorithm is also unconditional energy stable as pointed out in Remark 4.1. For convergence,
we evaluate our algorithm using the regret, that is the sum of all the previous difference between the
online prediction ft(θt) and the best fixed point parameter ft(θ*) from a feasible set F:
T
R(T) = X[ft(θt) - ft(θ*)],
t=1
where θ* = argminθ∈F PtT=1 ft(θ). For convex objectives we have the following regret bound.
Theorem 4.4. Let {θt} be the solution sequence generated by SGEM with a fixed η > 0. Assume
that kx - yk∞ ≤ D∞ for all x, y ∈ F, 0 < a ≤ ft(θt) + c ≤ B, and θt ∈ Ffor all t ∈ [T]. When
F and ft are convex, SGEM achieves the following bound on the regret, for all T ≥ 1,
r(t)≤ c pnτ∕η (XX.!	,	(19)
where C is a constant depending on β, B, D∞ and f1(θ1) + c.
7
Under review as a conference paper at ICLR 2022
Remark 4.4. (i) If rT,i > ri > 0, then R(T) is of order O( √T), which is known the best possible
bound for online convex optimization (Hazan, 2019, Section 3.2). Our experimental results show
thatfor η in a reasonable range, rτ,i decays much SlOwer than 1∕√T (See Figure 1), for which the
convergence holds true in the sense that
lim竽
T→∞ T
0.
(ii) The bound on θt is typically enforced by projection onto F (Zinkevich, 2003), with which the
regret bound (19) can still be proven since projection is a contraction operator (Hazan, 2019, Chap-
ter 3). As for the upper bound on the function value, just like we remarked for Theorem 4.2, it is
technically needed to bound mt.
5	Numerical experiments
In this section, we compare the performance of the proposed method with several other methods, in-
cluding AEGD, SGDM, AdaBelief (Zhuang et al., 2020), AdaBound (Luo et al., 2019), RAdam (Liu
et al., 2020a), Yogi (Zaheer et al., 2018), and Adam (Kingma & Ba, 2017), when applied to train-
ing deep neural networks. 1 We consider three convolutional neural network (CNN) architectures:
VGG-16 (Simonyan & Zisserman, 2015), ResNet-34 (He et al., 2016), DenseNet-121 (Huang et al.,
2017) on the CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009); we also conduct
experiments on the ImageNet dataset (Russakovsky et al., 2015) with the ResNet-18 architecture
(He et al., 2016).
For experiments on CIFAR-10 and CIFAR-100, we employ the fixed budget of 200 epochs and
reduce the learning rates by 10 after 150 epochs. The weight decay and minibatch size are set as
5 × 10-4 and 128 respectively. For the ImageNet tasks, we run 90 epochs and use similar learning
rate decaying strategy at the 30th and 60th epoch. The weight decay and minibatch size are set as
1 × 10-4 and 256 respectively.
In each task, we only tune the base learning rate and report the one that achieves the best final
generalization performance for each method:
•	SGEM: For CIFAR10 & 100 tasks, we use the default parameter η = 0.2; for the ImageNet
task, the learning rate is set as η = 0.3.
•	SGDM, AEGD: We search learning rate among {0.05, 0.1, 0.2}.
•	AdaBelief, AdaBound, Yogi, RAdam, Adam: We search learning rate among
{0.0005, 0.001, 0.01}, other hyperparameters such as β1, β2, are set as the default val-
ues in their literature.
From the experimental results of CIFAR10 & 100, we see that in all tasks, SGEM and AEGD
achieve higher test accuracy than the other methods while the oscillation of AEGD in test accuracy
is significantly reduced by SGEM as expected. We also observe that the differences between these
methods are more obvious in experiments on CIFAR-100.
For the ImageNet task, since all previous experiments show that SGDM gives the highest test accu-
racy, we focus on the comparison between SGDM and SGEM, and only run Adam as a represen-
tative of other adaptive methods. The results are presented in Figure 3. It can be seen that SGEM
still shows fast convergence and is able to achieve comparable test accuracy as SGDM in the end
of training. Here the highest test accuracy achieved by SGDM and SGEM are 69.89 and 69.92,
respectively.
6	Conclusion
In this paper, we propose SGEM, which integrates AEGD with momentum. We show that SGEM
still enjoys the unconditional energy stability property as AEGD, while the use of momentum helps
to reduce the variance of the stochastic gradient significantly, as verified in our experiments. We
1Code is available at https://anonymous.4open.science/r/SGDEM-0042.
8
Under review as a conference paper at ICLR 2022
(a) VGG-16 on CIFAR-10
% A□sr∞4⅛但
9492908886
% A□sr∞<tt但
(b) ResNet-34 on CIFAR-10
(c) DenseNet-121 on CIFAR-10
AdaBeIief
AdaBound
(d) VGG-16 on CIFAR-100
(e) ResNet-34 on CIFAR-100
(f) DenseNet-121 on CIFAR-100
0 5 0 5 0
8 7 7 6 6
% A1Jsnω3<⅛QL
Figure 3: Training loss and test accuracy for ResNet-18 on ImageNet
Figure 2: Test accuracy for VGG-16, ResNet-34 and DenseNet-121 on CIFAR-10/100
Oooooo
7 6 5 4 3 2
% AUBJnUUq⅛QL
also provide convergence analysis in both online convex setting and the general stochastic noncon-
vex setting. Since our convergence results depend on the energy variable, a lower bound on the
energy is also presented. Finally, we empirically show that SGEM converges faster than AEGD and
generalizes better or at least as well as SGDM on several deep learning benchmarks.
Based on our observations in this paper, we list some problems for future work. First, we believe
there is a threshold for η*, such that rτ either tends to a positive number or decays slower than
1/VT if η < η*. A further theoretical investigation on this issue is desirable. Second, since rt is
strictly decreasing, there is a room to limit rt for controlling its decay whenever necessary. A proper
energy limiter should be obtained.
References
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal
of Machine Learning Research, 18(221):1-51, 2018. URL http://jmlr.org/papers/
v18/16-410.html.
Leon Bottou. Stochastic Gradient Descent Tricks, volume 7700 of Lecture Notes in Com-
puter Science (LNCS). Springer, neural networks, tricks of the trade, reloaded edition, Jan-
9
Under review as a conference paper at ICLR 2022
uary 2012. URL https://www.microsoft.com/en-us/research/publication/
stochastic- gradient- tricks/.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAMRev., 60(2):223-311, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-type
algorithms for non-convex optimization. In International Conference on Learning Representa-
tions, 2019.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural In-
formation Processing Systems, volume 27, 2014. URL https://proceedings.neurips.
cc/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Elad Hazan. Introduction to online convex optimization. arXiv, abs/1909.05207, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016. doi: 10.1109/CVPR.2016.90.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2261-2269, 2017. doi: 10.1109/CVPR.2017.243.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. In Proceedings of the 31st Conference On Learning Theory,
volume 75, pp. 1042-1085, 2018.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predic-
tive variance reduction. In Advances in Neural Information Processing Systems, vol-
ume 26, 2013. URL https://proceedings.neurips.cc/paper/2013/file/
ac1dd209cbcc5e5d1c6e28598e8cbbe8- Paper.pdf.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv,
abs/1412.6980, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Uni-
versity of Toronto, 2009.
Yann LeCun, Y. Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:436-44, 05 2015. doi:
10.1038/nature14539.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimiza-
tion via SCSG methods. In Advances in Neural Information Processing Systems, vol-
ume 30, 2017. URL https://proceedings.neurips.cc/paper/2017/file/
81ca0262c82e712e50c580c032d99b60- Paper.pdf.
Hailiang Liu and Xuping Tian. AEGD: Adaptive gradient decent with energy.	arXiv,
abs/2010.05109, 2020.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Ji-
awei Han. On the variance of the adaptive learning rate and beyond. In International Confer-
ence on Learning Representations, 2020a. URL https://openreview.net/forum?id=
rkgz2aEKDr.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. In NeurIPS, 2020b.
10
Under review as a conference paper at ICLR 2022
Liangchen Luo, Yuanhao Xiong, and Yan Liu. Adaptive gradient methods with dynamic bound of
learning rate. In International Conference on Learning Representations, 2019.
Stanley Osher, Bao Wang, Penghang Yin, Xiyang Luo, Farzin Barekat, Minh Pham, and Alex Lin.
Laplacian smoothing gradient descent. arXiv, abs/1806.06317, 2019.
B.	T. Polyak. Some methods of speeding UP the convergence of iterative methods. Z. VycisI Mat i
Mat. Fiz.,4:791-803,1964. ISSN0044-4669.
Ning Qian. On the momentUm term in gradient descent learning algorithms. Neural Net-
works, 12(1):145-151, 1999. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(98)
00116-6. URL https://www.sciencedirect.com/science/article/pii/
S0893608098001166.
Sashank Reddi, Satyen Kale, and Sanjiv KUmar. On the convergence of Adam and beyond. In
International Conference on Learning Representations, 2018.
Herbert Robbins and SUtton Monro. A stochastic approximation method. Ann. Math. Statistics,
22:400-407, 1951. ISSN 0003-4851. doi: 10.1214/aoms/1177729586. URL https://doi.
org/10.1214/aoms/1177729586.
Olga RUssakovsky, Jia Deng, Hao SU, Jonathan KraUse, Sanjeev Satheesh, Sean Ma, Zhiheng
HUang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale VisUal Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
A. Shapiro and Y. Wardi. Convergence analysis of gradient descent stochastic algorithms. J. Optim.
Theory Appl., 91(2):439-454, 1996. ISSN 0022-3239. doi: 10.1007/BF02190104. URL https:
//doi.org/10.1007/BF02190104.
K. Simonyan and Andrew Zisserman. Very deep convolUtional networks for large-scale image recog-
nition. arXiv, abs/1409.1556, 2015.
Ilya SUtskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentUm in deep learning. In Proceedings of the 30th International Conference on
Machine Learning, volUme 28, pp. 1139-1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. RMSprop: Divide the gradient by a rUnning average of its
recent magnitUde. COURSERA: Neural networks for machine learning, 4(2):26-31, 2012.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
marginal valUe of adaptive gradient methods in machine learning. arXiv, abs/1705.08292, 2018.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A Unified analysis of stochastic mo-
mentUm methods for deep learning. In Proceedings of the 27th International Joint Conference on
Artificial Intelligence, pp. 2955-2961, 2018.
Xiaofeng Yang. Linear, first and second-order, Unconditionally energy stable nUmerical schemes for
the phase field model of homopolymer blends. Journal of Computational Physics, 327:294-316,
2016.
Hao YU, Rong Jin, and Sen Yang. On the linear speedUp analysis of commUnication efficient mo-
mentUm sgd for distribUted non-convex optimization. In ICML, 2019.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv KUmar. Adaptive
methods for nonconvex optimization. In S. Bengio, H. Wallach, H. Larochelle, K. GraUman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volUme 31. CUrran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf.
Jia Zhao, Qi Wang, and Xiaofeng Yang. NUmerical approximations for a phase field dendritic crystal
growth model based on the invariant energy qUadratization approach. International Journal for
Numerical Methods in Engineering, 110:279-300, 2017.
11
Under review as a conference paper at ICLR 2022
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon
Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief
in observed gradients. In Advances in Neural Information Processing Systems, volume 33,
pp. 18795-18806, 2020. URL https://proceedings.neurips.cc/paper/2 02 0/
file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the Twentieth International Conference on International Conference on Machine
Learning, ICML, pp. 928-935, 2003.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for con-
vergences of adam and rmsprop. 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 11119-11127, 2019.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proof of Theorem 4.2
For the proofs of Theorem 4.2 and Theorem 4.3, we introduce notation
Ft := Pf(θt;&) + c.	(20)
The initial data for r is taken as rι,i = Fl. We also denote the update rule presented in Algorithm
1 as
θt+1 = θt - 2ηrt+1vt,	(21)
where rt+1 is viewed as a n × n diagonal matrix that is made up of [rt+1,1, ..., rt+1,i, ..., rt+1,n].
Lemma A.1. Under the assumptions in Theorem 4.2, we have for all t ∈ [T],
(i)	kVf(θt)k∞ ≤ G∞.
(ii)	E[(Ft)2] = F2(θt) = f(θt) + c.
(Ui) E[Ft] ≤ F(θt). In particular, E[rι,i] = E[Fι] ≤ F(θι) for all i ∈ [n].
(iv)	σg = E[kgt - Vf(θt)k2] ≤ G∞ and σf = E[∣f(θt; ξt) - f(θt)∣2] ≤ B2.
(v)	E[|F(θt) - Ft|] ≤ 2√aσf.
(Vi) E[kVF(θt) - 2Ftk2] ≤ G∞σ2 + 2ασg.
Proof. (i) By assumption kgt k∞ ≤ G∞, we have
kVf(θt)k∞ = kE[gt]k∞ ≤E[kgtk∞] ≤G∞.
(ii)	This follows from the unbiased sampling of
f (θt) = Eξjf(θt; ξt)].
(iii)	By Jensen’s inequality, we have
E[Ft] ≤ TEFi = √FW = F(θt).
(iv)	By assumptions kgtk∞ ≤ G∞ and f(θt; ξt) + c < B, we have
σg2 = E[kgt - Vf(θt)k2] = E[kgtk2] - kVf(θt)k2 ≤ G2∞,
σf = E[kf(θt; ξt) - f(θt)k2] = E[kf(θt; ξt)k2] -kf(θt)k2 ≤ B2.
(v)	By the assumption 0 < a ≤ f (θt; ξt)+ C = Ft, We have
E[|F(θt) - Ft|] ≤ E 吗)-叫ξt)	≤BE[∣f(θt)- f(θt; ξt)∣] ≤/σf.
F (θt ) + Ft	2 a	2 a
(vi)	By the definition of F (θ), We have
kVF(θt) - Mk2 = 2Ft	Vf(θt) - 也 2F (θt)	2Ft	2	
. ..~ .. 1 Vf (θt)(Ft- F(θt)) - 	 4	F (θt)Ft			,Vf (θt) - gt 2 + —F-
. ..~ .. ≤ 1 Vf(θt)(Ft- F(θt)) 2 2	F(θt)Ft			2, 1 Vf (θt) - gt 2 + 2	Ft
G2	1
≤ G∞IFt- F(θt)∣2 + -kVf(θt)-gtk2,
13
Under review as a conference paper at ICLR 2022
where both the gradient bound and the assumption that 0 < a ≤ f (θt; ξt) + C = Ft are essentially
used. Take an expectation to get
g	G2	1
E[kVF (θt)-* k2] ≤ G∞ E[∣Ft - F (% )|2] + - E[∣∣Vf (θt) - gtk2].
2Ft	2a2	2a
Similar to the proof for (iv), we have
E[∣Ft- F(θt)∣2] ≤ 4aσf.
This together with the variance assumption forgt gives
E[kVF㈣-2∣k2]≤ Sσ2 Qσg.
□
Lemma A.2. For any T ≥ 1, we have
⑴ EhPT=I v> rt+ιvti ≤ nFη1.
(ii)	EhPT=I m>-irt+ιmt-ii ≤ EhPT=I m>rt+ιmj ≤	Fθθ1.
(iii)	EhPt=I krt+ιmtk2i ≤ 历广廿.
(iv)	EhPT=I g>rt+ιgt] ≤ 8BnF(θη).
(V) EhPt=Ikrt+ιgtk2i ≤ ^F^.
Proof. From Algorithm 1 line 5, we have
rt,i - rt+1,i = 2ηrt+1,ivt2,i.
Taking summation over t from 1 to T gives
T
r1,i - rT +1,i = 2η	rt+1,ivt2,i
t=1
T
X2	r1,i
…rt+1,ivt,i ≤ 方.
From which we get
T	n T	手
X vt>rt+1vt = XX
rt+1,ivt,i ≤ nF.
t=1	i=1 t=1	2η
Taking expectation and using (iii) in Lemma A.1 gives (i).
Recall that mt = 2(1 - βt)Ftvt and Ft ≤ B, we further get
TT
mt>rt+1mt ≤ 4B	vt>rt+1vt
t=1	t=1
2BnFι
η
Using rt+1,i ≤ rt,i and m0,i = 0, we also have
n T	n T	n T-1	n T
XX
rt+1,imt2-1,i ≤XX
rt,imt2-1,i = XX
rt+1,imt2,i≤XXrt+1,imt2,i.
Connecting the above two inequalities and taking expectation gives (ii).
(22)
Using rt+1,i ≤ r1,i, the above inequality further implies
T	nT	nT
Xkrt+1mtk2=XXrt2+1,imt2,i ≤XXr1,irt+1,imt2,i
lrt,imt,i∖Fι ≤ 2BnF2∕η.
14
Under review as a conference paper at ICLR 2022
Taking expectation and using (ii) in Lemma A.1 gives (iii).
By mt = βmt-ι + (1 一 β]Qt, We have
T
n T
EgJrt+ιgt = EErt+ι,ig2,i
t=1
i=1 t=1
nT
X X
一βmt,i - τ-βmt-1,i
1
β
2
< (1 - β)2
n t	2 R2 n T
XX
rt+1,im2,i + (1■‰ XXrt+1,im2-1,i
i=1 t=1	( β) i=1 t=1
2
2(1+ β2)
-(1 - β)
T
EmJrt+ιmt <
t=1
_ ~
8BnF∖
(1 - β)2η
<
2
Here the third inequality is by (a + b)2 < 2a2 + 2b2; (22) and 0 < β < 1 are used in the fourth
inequality. Taking expectation and using (iii) in Lemma A.1 gives (iv).
Similar as the derivation for (ii), we have
T
X ik2 <
t=1
rt,ig2,i	Fi <
_	~ r>
8BnF2
(1 - β)2η∙
Taking expectation and using (ii) in Lemma A.1 gives (v).
□
We are now ready to prove Theorem 4.2. The upper bound on σg is given by (iv) in Lemma A.1.
Since f is L-smooth, we have
f (θt+ι) < fR) + vf(θt)τ(θt+ι - Ot) + 2llθt+ι - θt∣∣2.	(23)
Denoting ηt = η∕Ft, the second term in the RHS of (23) can be expressed as
Vf (θt)τ(θt+ι - θt)
=Vf (θt)τ(-2ηrt+ιvt)
1
=-1 - βt Vf (θt)τηtrt+ιmt (since mt = 2(1 - βt)FtVt)
=-fJ Vf (θt)τηtrt+ι(βmt-ι+ (1 - β)gt)	(24)
1	- βt
=-1-βt VfR)Tηtrt+ιgt - I -ββt Vf(Pt)Tηtrt+ιmt-i
=-1~βtVf(Pt)Tηt-irtgt + 1~βtVf(θt)τ(ηt-irt - ηtrt+ι)gt
1 - βt	1 - βt
-	I -ββt Vf(θt)τηtrt+ιmt-i∙
We further bound the second term and third term in the RHS of (24), respectively. For the second
term, we note that 11-τ | < 1 and
∣Vf(θt)τ(ηt-irt - ηtrt+ι)gt∣
=∣Vf (θt)τηt-i(rt - rt+ι)gt + Vf (θt)τ(ηt-i - ηt"ιgt∣
=lVf (θt)τηt-ι(rt - rt+ι)gt+ (ηt-ι - ηt)gjrt+igt
+ (ηt-ι - ηt)(Vf(θt) - gt)τrt+ιgt∣
<	llVf(θt)k∞∣ηt-ι∣krt -rt+ιkι,ιkgtk∞ + ∣ηt-ι -ηt∣gJrt+ιgt
+ ∣ηt-ι -ηt||(Vf(θt) -gt)τrt+ιgt∣
<	(ηG∞∕√α)(krtkι,ι - krt+ι∣∣ι,ι) + (2η∕√α)gJrt+ιgt
+ (功/—(%) - gt)τrt+ιgt∣∙
(25)
15
Under review as a conference paper at ICLR 2022
The third inequality holds because for a positive diagonal matrix A, XT Ay ≤ kxk∞kA∣∣ιjkyk∞,
where IlAk 1,1 = Pi aii. The last inequality follows from the result rt+ι,i ≤ rt,i for i ∈ [n], the
assumption ∣∣gt∣∣∞ ≤ G∞, Ft ≥ √α, and (i) in Lemma (A.1).
For the third term in the RHS of (24), we note that
一ι βRtVf(θt)Tntrt+ιmt-ι ≤ n Brn 1∣Vf(θt)τηtrt+ιmt-ι∣,
1 — P	(1 — p) ʌ/ a
in which
∣Vf(θt)τ rt+ιmt-i∣
=IgTrt+imt-1 + (Vf (θt) — gt)τrt+ιmt-i∣
≤ 2g>rt+ιgt+ 2m>-ιrt+ιmt-ι + |(Vf(θt) 一 gt)τrt+ιmt-ι∣,	(26)
where the last inequality is because for a positive diagonal matrix A, XTAy ≤ ɪXTAx + ɪyτAy.
Substituting (25) and (26) into (24), we get
vf (θt)τ(θt+ι 一 θt) ≤ 一 ι — PtVf (θt)τηt-ιrtgt+ n√∞ (IlrtIlι,ι 一 ιιrt+ι∣∣ι,ι)
i ，2n	βη τ τ	l Pn T
+ √a+ + 2(1 — P)√aIgt rt+ιgt + 2(1 — P)√amt-ιrt+ιmt-1
+ √η= I(Vf @)- gt)τrt+ιgt∣+ ∩η~√~τ I(Vf G)- gt )τrt+ιmt-ι∣∙
√α	(1 — P) √α
With (27), we take an conditional expectation on (23) with respect to (θ) and rearrange to get
；—Pt Vf (θt)Tηt-1rtVf (θt) = Eξt 1 _ PtVf (θt)τηt-irtgt
≤ Eξ[f(θt) — f(θt+1) + η√∞(Ilrtk1,1 — I∣rt+1∣∣1,1)
2η	βη >	βη >
+ l√α + 2(1 — P)√a)gtrt+19t + 2(1 — P)√ɑmt-1rt+1mt-1
+ √√= I(Vf (θt) — 9t)τrt+19t∣
+ (1 —η)√α∣(Vf(θt)— gt)τrt+1mt-1∣+ 2ιιθt+1— θtk2 ,
(27)
(28)
where the assumption Eξt [gt] = Vf (θt) is used in the first equality. Since ξ1,…，ξt are independent
random variables, we set E = Eξ1 Eξ? ...Eξτ and take a summation on (28) over t from 1 to T to get
E £ 1⅛Vf(θt)τηt-1rtVf(θt)
_ t=1 1 P	.
≤ E[f(θ1) — f(θτ +1)] + η√∞E[∣∣r1∣∣1,1 — krτ +1k1,1]
/入	X Γ T	1	C
+ (斗 + Qk)E E 9*t+19t + Qk E
∖Va 2(1 -P) vα∕	t=1	2(1 -P) Va
T
Em二 Irtmt-I
t=1	.
(29)
2η
+ -≠E
a
T
E I(Vf R) — gt)τ rt+19t∣
t=1
—Pn— E
(1 — P)√ɑ
+
T
E I(Vf(θt) — 9t)τrt+1mt-1∣
t=1
L T
+ 2E E l∣θt+1 — θtk2
16
Under review as a conference paper at ICLR 2022
Below we bound each term in (29) separately. By the Cauchy-Schwarz inequality, we get
T
E X I(Vf(Ot)- gt)>rt+ιmt-ιl
t=1
≤ E XkVf(θt) -gtkkrt+1mt-1k
t=1
≤E
E
kVf(θt)-gt
T
XkVf(θt)-gtk
t=1
1/2
krt+1mt-1k2
T	1/2
Xkrt+1mt-1k2
≤ vz2BnT∕ηF(θι)σg,
(30)
where Lemma A.1 (ii) and the bounded variance assumption were used. We replace mt-1 in (30)
by gt and use Lemma A.1 (v) to get
E
T
X I(Vf (θt) - gt)>rt+ιgt∣
t=1
≤E
T
XkVf(θt)-gtk2
t=1
E
T	1/2
Xkrt+1gtk2
< 2,2∕nT∕ηF(θι)σg
≤	1-β
(31)
By (12), the last term in (29) is bounded above by
∞
2E X kθt+ι-θtk2
2 t=0
≤ LnF2(θ1).
(32)
Substituting Lemma A.1 (i) (iii), (32), (31), (30) into (29) to get
E
T1β
∑β-VfF Vf (θt)τηt-irtVf (θt)
t=1 1 - β
≤ (f”f*) + η√∞nF(θι)
(2	β	ʌ 8BnF(θ1)	βBnF(θι)
+ V√a + 2(1 - β)√a) (i - β)2 + (i - β)√a
l (4 + β)√2Bη	F l Lnn 口2"、
+ (i-β)√a F(01)vnTσg+ — F (θ1).
(33)
Note that the left hand side is bounded from below by
(1 - β)√BE 卜iinrτ,i X kVf(θt)k2
where We used ∣ ~1-∣t ∣ ≥ 1 一 β and ηt ≥ η/√B. Thus We have
E [minrτ,i X kVf(%)『]≤ CI + C2n + °3吗√T,
i t=1	η
17
Under review as a conference paper at ICLR 2022
where
C1
(f(θι)- f *) √B
1 - β
VBηG∞F(θι)	(ɪ	β	∖ 8B3/2F(θι)
(1- β)√a	+ √a + 2(1- β)√a) (1 - β)3
eB3/2F(θι) + VBLn F2
(i - β)2√a	2(i - β)2
C3
(4 + β)B √2n
(I- β)√a
F(θ1).
A.2 Proof of Theorem 4.3
First note that by (iv) in Lemma A.1, max{σg, σf} ≤ max{G∞, B}.
Recall that F(θ) = f θ(θ) + c, then for any χ,y ∈ {θt}T=o We have
kVF (X)-VF (y)k
Vf(X) _ Vf(y) Il
2F(x)	2F(y) K
=1 Vf(X)(F(y)- F(x)) + Vf (X)- Vf (y)
=2	F (x)F (y)	+	F (y)
G1
≤ 2F∞y ∣f (y) - F (χ) I + 2F(θτy kVf (χ) - Vf (y)k.
One may check that
G
IF(y) - f(χ)I ≤ 2F∞)kχ-yk∙
These together With the L-smoothness of f lead to
kVF (X) - VF (y)k ≤ LFkX-yk,
Where
L.= ɪ (L+	G∞	Y
F	2,f(θ*) + C I +2(f(θ*)+ C)
This confirms the LF -smoothness of F, Which yields
F(θt+ι) - F(θt) ≤ VF(θt)>(θt+ι - θt) + LFkθt+ι - θtk2
=(VF(θt) -器)>(θt+ι - θt) + (M - η~~^-vt)>(θt+ι - θt)
2Ft	2Ft	1 - β
+ (-j—βvt)>(θt+ι - θt) + Fkθkθt+ι - θtk2.
1-β	2
Summation of the above over t from 1 to T and taken With the expectation gives
4
E[F(θT+1)-F(θ1)] ≤ XSi,
i=1
(34)
18
Under review as a conference paper at ICLR 2022
where
Si = E ]X 1f-ββtv>(θt+ι — θt),
S2 = E ]x(3 - 1 --ββ vt)>(θt+1 - θt)
S3 = E [XX(VF(θt) - M)>(θt+i - θt)
t=1	2Ft
TL
S4 = E ∑^Fkθt+1-θtk2 .
t=1
Below we bound S1, S2, S3, S4 separately. To bound S1, we first note that
rt+1,i - rt,i = -2ηrt+1,ivt,i = vt,i (-2ηrt+1,ivt,i) = vt,i (θt+1,i - θi)
from which we get
T1	βt
Si= E ∑T-4Vt(θt+ι-θt)
t=11	- β
n	T1	βt
=E XX T-β (rt+1,i - rt,i)
i=i t=i1	- β
nT
≤E	rt+i,i - rt,i	(Since rt+i,i ≤ rt,i)
i=i t=i
n
=XE[rτ +i,i] - nE[Fi].
i=i
For S2 , we have
S2 = E
∑(2⅜ -1 vt)>(θt+1 - θt)
n T 1 β
XX(-2F~1-β mt-i,i)> (2ηrt+i,i vt,i)
i=i t=i	2Ft1 - β
nT
βη
(I- β)√a
E	rt+i,imt-i,ivt,i
≤
E
—βη- E
(1 - β)√a
β √BnF (θi)
≤
i/2 n T	i/2
rt+i,imt2-i,i	XX
rt+i,ivt2,i
≤
(1 - β)√a ,
where the fourth inequality is by the Cauchy-Schwarz inequality, the last inequality is by Lemma
A.1 (i) (ii).
19
Under review as a conference paper at ICLR 2022
For S3 , by the Cauchy-Schwarz inequality, we have
S3 = E
T
X(VF(θt) -卷)>(θt+1 - θt)
t=1	2Ft
T
≤ E X kVF(θt)-M )kkθt+ι-θt)k
t=1	2Ft
≤ E TkVF(θt)-
T	1/2
kθt+1 - θtk2
≤E
T
X kVF (θt) - M )k2
t=1	2Ft
E
T	1/2
X kθt+1 - θtk2
≤ F("I)PnTVG3 σ2 + 2aσg，
where the last inequality is by (vi) in Lemma A.1 and (12) in Theorem 4.1.
For S4 , also by (12) in Theorem 4.1, we have
S4 = LF E [X kθt+ι-θtk2[ ≤ LF ηnF 2(θ1).
t=1
With the above bounds on S1, S2 , S3, S4, (34) can be rearranged as
F此-β√⅛√2 — F(θι)pnτjG3σ2 + aσg — LFnnF2^
n
≤ X E[rτ +ι,i] — nE[Fι] + F(Θ1)
i=1
≤ (minE[rτ +ι,i] + (n — 1)E[F1]) — (n — 1)E[F1]+ (F@) — E[F1 ])
--	r	- -. — ，-、	3∙r
≤ minE[rτ +ι,i] + E[|F(θι) — F1|]
≤ minE[rτ +ι,i] + +σf,
where (iii) in Lemma A.1 was used. Hence,
min E[rτi] ≥ max{F(θ*) - nDι - βD2 - σD3,0},
i,
where σ= max{σf, σg} and
LFnF2(θ1) DI = -2—， D3 = TTr + F (θ1), 2a	D = BnnF (θι) 2 = (1 — β)√a, PnT r g∞∞+-. 4a3	a
A.3 Proof of Theorem 4.4
Using the same argument as for (iv) in Lemma A.2, we have
nT
ΣΣrt+1,igt2,i ≤
8Bnvzfι(θι) + C
(1 — β )2n
20
Under review as a conference paper at ICLR 2022
With this estimate and the convexity of ft , the regret can be bounded by
TT
R(T) = Xft(%) - ft(θ*) ≤ Xg>(% - θ*)
≤
nT
≤	lgt,∕√rt+1,i
i=1 t=1
∣θt,i-θ"
√r+17
≤
2D∞ √2B
1 — β
(f1(θ1) + c)1/4
1/2
where the fourth inequality is by the Cauchy-Schwarz inequality, and the assumption kx - y k∞ ≤
D∞ for all x, y ∈ F is used in the last inequality.
21