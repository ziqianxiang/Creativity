Under review as a conference paper at ICLR 2022
Scalable Robust Federated Learning with
Provab le Security Guarantees
Anonymous authors
Paper under double-blind review
Ab stract
Federated averaging, the most popular aggregation approach in federated learn-
ing, is known to be vulnerable to failures and adversarial updates from clients
that wish to disrupt training. While median aggregation remains one of the most
popular alternatives to improve training robustness, relying on secure multi-party
computation (MPC) to naively compute the median is unscalable. To this end, We
propose a secure and efficient approximate median aggregation with MPC privacy
guarantees in the multi-silo setting, e.g., across hospitals, With tWo semi-honest
non-colluding servers. The proposed method protects the confidentiality of client
gradient updates against both semi-honest clients and servers. Asymptotically, the
cost of our approach scales only linearly With the number of clients, Whereas the
naive MPC median scales quadratically. Moreover, we prove that the convergence
of the proposed federated learning method is robust to a Wide range of failures
and attacks. Empirically, we show that our method displays similar robustness
properties as the median while converging faster than the naive MPC median for
even a small number of clients.
1	Introduction
Federated Learning (FL) (McMahan et al., 2017) has emerged as a leading approach for training
shared models among clients who do not wish to release their datasets publicly. While FL avoids
releasing client data, revealing client models in the clear creates privacy vulnerabilities, e.g., using
model inversion attacks (Fredrikson et al., 2015). To this end, global model aggregation is often
implemented using Secure Multi-Party Computation (MPC) to improve privacy (Bonawitz et al.,
2016). MPC (Yao, 1986; Goldreich et al., 1987) is a cryptographic primitive that makes it possible to
evaluate a function on encrypted inputs while ensuring that the only information revealed throughout
the computation is the final output of the function. Thus, to avoid revealing each client’s local
weights, clients encrypt their weights before sending them to the central server. MPC allows the
central server to recover a single aggregate update that it sends back to individual clients. Specialized
MPC protocols, that allow the server to recover the average of client updates have been widely
deployed for Federated Learning.
In addition to privacy, robustness is a major concern in federated learning implementations (Kairouz
et al., 2019). The distributed nature of the computation creates a variety of vulnerabilities to ad-
versarial training attacks and client failures such as hardware, software, data, and communication
errors (Xie et al., 2019a; 2021). One way to improve robustness is to replace an aggregation proce-
dure that computes means with a more robust aggregate such as the median (Yin et al., 2018). Using
the median, up to half of inputs can be from a faulty distribution before the median fails as a robust
average estimator. While median aggregation alone is insufficient to guarantee Byzantine robustness
(Xie et al., 2020) - median-based robustness guarantees are sufficient against most published attacks
and failures. Unfortunately, the best known approaches for median aggregation with MPC are slow,
requiring computation that grows quadratically with the number of clients (Tueno et al., 2019).
Our work is motivated by applications to multi-silo federated learning (Kairouz et al., 2019), de-
signed for learning across institutions. For example, an individual hospital may be limited to using
only their own patients’ data, and sharing patient information among hospitals is generally difficult
due to privacy and intellectual property concerns. Here, federated learning can provide a mechanism
for widely distributed medical institutions to jointly train a model.
1
Under review as a conference paper at ICLR 2022
The main contribution of this work is a new approximate-median aggregation for robust, privacy-
preserving federated learning in the multi-silo, two-server setting. Our approach is the first to have
all of the following features:
•	Robustness against failures. By relying on (an approximation of) the median of client updates,
we achieve robust estimates even in settings where updates sent by up to half the clients are faulty.
In addition, we empirically evaluate robustness against noise distributions that take the form of bit
flips, label switching, and Gaussian noise in up to half of the clients.
•	Preserving Client Privacy via Secure Computation. We prove that any subset of clients col-
luding with each other and with one of the two servers obtains no information about the private
inputs of other clients, beyond the aggregated (approximate) median that is returned at each step.
This provides meaningful privacy to every client and is a meaningful defense, e.g., against model-
inversion attacks that take advantage of individual client updates.
•	Scalability. The running time of our protocol increases linearly with the number of clients; match-
ing the order-wise scaling (with number of clients) of the standard averaging aggregation. This
is in contrast to a quadratic factor blowup for a federated learning system that implements me-
dian naively in MPC. We empirically evaluate the convergence of our system on various datasets,
models and under different types of faults.
2	Related Work
Federated Learning (McMahan et al., 2017) is a widely adopted technique for training models with
data from edge devices. In its plainest form, client updates are sent without encryption to the central
server at each round, and the global model is sent back to each client. However, this basic imple-
mentation of a distributed machine learning system is not robust against privacy attacks or failures.
For instance, (Wang et al., 2019; Geiping et al., 2020) show how model inversion attacks can be
successful in a federated learning setting. (Bonawitz et al., 2017; 2016) provide a protocol for se-
cure aggregation of the client updates and computation of the global model in both a semi-honest
and malicious setting, with increased runtime and communication overhead. In addition to encryp-
tion of client updates, existing work has also considered robustness to failures and Byzantine client
attacks, e.g., (Xie et al., 2019a; 2021) prune faulty client updates by assigning each update a score,
and only aggregating the updates that have the top scores. Other methods of providing byzantine
robustness including trimmed mean (Xie et al., 2019b), Krum (He et al., 2020; So et al., 2021), and
norm-based elimination (Gupta et al., 2021) have been studied. In this paper, we strengthen the re-
striction on client faults and only consider semi-honest clients. Although a semi-honest setting may
seem restrictive by forcing parties to follow the dictated protocol, it applies to settings where parties
are assumed to have no ill intentions. Furthermore, developing semi-honest protocols are a step
towards developing protocols in the malicious security model, where more expensive cryptographic
primitives such as zero-knowledge proofs are added on top of the existing semi-honest protocol.
Secure Multi-Party Computation (MPC) provides a way for multiple parties to compute a function
on their individual data and only reveal the final output of the function. (Bonawitz et al., 2016; 2017)
provide a method for federated learning where client updates have a pairwise additive mask, such
that when all the encrypted client updates are summed together, the masks cancel out. In (Tueno
et al., 2019), the authors propose a rank computation system where clients send a pairwise masked
version of their data point to the central server. The central server uses pairwise comparisons in MPC
to calculate the desired ranked element in the clients’ combined data. However, pairwise compar-
isons are expensive in MPC, as additional values for each unique comparison must be generated in
advance. Furthermore, agreeing upon pairwise additive masks grows quadratically with the number
of parties.
The generalization of a central server to two non-colluding central servers is not a new idea in
privacy-preserving analytics and machine learning. For instance, (Corrigan-Gibbs & Boneh, 2017)
introduce a cryptographic protocol called secret-shared non-interactive proofs, and show that these
can be used to construct an efficient privacy preserving system for the release of aggregate statis-
tics with guarantees on security in the malicious setting. The setting of the paper consists of a
small number of servers jointly computing aggregate statistics over private client data. (Mohassel &
Zhang, 2017) introduce efficient distributed ML protocols that utilize a two server abstraction and
MPC versions ofML computations. However they do not seek to provide robustness. To our knowl-
edge, (He et al., 2020) is the first work to propose an FL system that combines both robustness and
privacy, using private calculation of the distance-based aggregation. They also make the assumption
2
Under review as a conference paper at ICLR 2022
of the central server being two non-colluding central servers, and utilize client side secret sharing of
models and server side two party MPC. The two central server setting allows their protocol to scale
well with number of parties, compared to a single central server setting. However, their protocol
leaks the pairwise distance of client updates to one of the servers, which may be undesirable.
Our proposed approach uses the 2-Party MPC comparison protocols presented in (Rathee et al.,
2020). We also adopt a pairwise comparison based computation of the k-th ranked element proposed
in (Tueno et al., 2019) and a bucketing approximate median idea proposed in (Corrigan-Gibbs &
Boneh, 2017), and apply them to the setting of distributed machine learning. Finally, we note that
this work focuses on security and robustness of centralized model training. Existing work has shown
that personalization and mitigating data heterogeneity can be addressed by fine-tuning the robust
centralized model (Li et al., 2021). These details are left for future work.
3	Preliminaries
Federated Learning. We denote N as the set of positive integers and R+ as the set of non-negative
real numbers. Given n ∈ N, we denote [n] = {1, . . . , n}. We use bold lowercase letters, e.g.,
w ∈ Rd, to denote vectors, and w(k) to denote its value at the kth dimension. We consider a two-
server multi-client FL system, let S0, S1 denote the two central severs, and ci for i ∈ [n] represent
the clients, where n is the total number of clients. Client ci holds dataset Di. The agreed upon model
has d parameters. wg denotes the global model sent from servers to clients, while wi denotes the
local model held by client i. All parties agree beforehand to run r cycles of FL. The loss function
for client ci is Fi : Rd → R+ . Usually, because clients have different data, the loss functions Fi are
different. Therefore, the goal of our setting is to minimize the averaged loss across all clients, i.e.,
argminw 1 Pi=I Fi(W).
We first detail a basic federated learning system with one central server S and n clients. Clients are
generally considered to be edge nodes and do not communicate with each other. In one iteration, the
system follows the following generic protocol:
1.	For i ∈ [n], client ci receives global model wg from central server S.
2.	For i ∈ [n], client ci performs step(s) of Stochastic Gradient Descent of wg on its dataset
Di to obtain new local model wi . They then send this model to S.
3.	Upon receiving all client updates, S aggregates them to obtain the new global model. The
new global model is computed as Wg = 1 Pn=1 Wi.
4.	S sends wg0 to all the clients.
Secure Multi-Party Computation. Secure Multi-party Computation (MPC) (Goldreich et al.,
1987) is a cryptographic primitive which allows n parties, each with input value xi , to jointly com-
pute a mutually agreed upon function f(x1, x2 . . . xn ), with the guarantee that no party learns any-
thing other than what can be inferred from its own input and the joint output.
One of the most prominent techniques used in MPC is secret sharing. A (t, n) secret sharing in-
volves splitting a secret s into n shares, then distributing a share to each party, such that any subset
of t - 1 parties cannot learn anything about s, while any t or more parties can jointly reconstruct s.
In this paper, we make use of (2, 2) additive secret sharing over power-of-two sized integer rings,
which involves splitting S ∈ Z' into si, s? ∈ Z' s.t. sɪ + s2 = s, and individually si, i ∈ {0,1}
has a uniform random distribution. We usehsito denote secret sharing of S over z`, and〈s〉o,〈sii
to denote the shares of the two parties. We use FComp, *qual to refer to the “greater than” and
“equals to” ideal functionalities respectively, i.e. where parties have access to trusted functionalities
computing the said functions and use the same to describe our protocols. To implement the pro-
tocols for FComp, FEqual, We make use of protocols proposed in CrypTFlow2 (Rathee et al., 2020).
CrypTFlow2 provides a customized 2-party protocol for solving the millionaire’s, and hence also
the comparison/equality problems in z` (more details on all this in Appendix C).
4	Problem Description
In this section, we introduce a baseline solution to secure and robust FL. Median provides well-
studied robustness against outliers compared to mean (Xie et al., 2018). In order to make our MPC
protocols for this efficient, we abstract the central server in FL to two non-colluding central servers.
The servers each receive one of the (2, 2) secret shares of the client updates, and perform 2-party
based secure median computation. Figure 1 provides a representation of our model.
3
Under review as a conference paper at ICLR 2022
Figure 1: Our proposed two server FL system. Hospitals compute local models wi , and secret share
them among the two servers. Servers securely compute the dimension-wise median and end up with
the global model. Each hospital receives the global model from S0, and repeats the cycle.
4.1	Security Threat Model
We model the adversary using the semi-honest/honest-but-curious model, whereby it is assumed that
the parties controlled by the adversary will follow the protocol specifications, but try to learn more
information about other parties’ data using the protocol transcript. We also assume static corruption,
i.e., that the adversary chooses the parties to corrupt at the beginning of the protocol and cannot
change them once protocol execution starts. In addition, we assume the two servers S0 , S1 do not
collude with each other. The adversary is then allowed to corrupt any arbitrary subset of the clients
and at most one of the servers - and once corrupted gets to see their internal state as well as the
transcript of their communication. Furthermore, we are concerned with the multi-silo setting, where
clients are assumed to be large institutions and thus client drop out or limitations on computational
resources are not considered.
4.2	Baseline solution
We propose a baseline solution for computing the exact median in our two-server multi-client setting.
In order to send their updates to the servers, clients additively secret-share their updates and sends
one share to each server. Once all the clients have done so, the two servers now have a (2, 2)
additive secret share of all the clients’ updates and perform a MPC based median computation.
We take inspiration from the ideas in Tueno et al. (2019) to design a 2-party secure protocol for
median computation by breaking it up into pairwise comparisons between all elements and some
number of equality checks. In Appendix A, we provide a formal description of our algorithm in the
FC'omp, F^quai hybrid (described in Algorithm 2), along with a correctness sketch and other details for
the same. Our final protocol is then obtained by replacing FComp, F^quM With CrypTFlow2 (Rathee
et al., 2020) based secure comparison/equality protocols (described in detail in Appendix C). We
refer to this protocol as “pairwise comparison based median” in the rest of the paper.
Complexity. The complexity of this protocol is at least dn(n - 1)/2 calls to FComp, in addition to
dn calls to F^qu仃 Note that the number of secure comparisons/equalities grows quadratically with
the number of clients n. In the next section, we will introduce our approximate median protocol,
whose number of secure operations is independent ofn. We provide a brief summary in Appendix C
on why we use number of secure comparisons as a metric for complexity.
5	Bucketing based median computation
In this section, we propose an approximate median, which provides increased computational effi-
ciency, to serve as the replacement of the exact median computation. We provide an abridged version
of the protocol in Algorithm 1 and a formal description of the protocol in Algorithm 3.
Summary of the key idea. Divide the set of possible values that client updates may take into a fixed
set of buckets. Let b designate the number of buckets. Next, instead of sending actual updates to the
server, each client will send a unit vector that has a 1 in the bucket containing the client’s update, and
0 everywhere else. Upon receiving all client updates, the (two) servers build a frequency distribution
of client updates and use this to find the bucket which contains the median value. The median is then
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Succinct version of our bucketed FL system
Input: For i ∈ [n], ci holds dataset Di.
d = model size, b = number of buckets, p are public parameters.
∀i ∈ [n], client ci does:
1.	Receive global model wg and bucket ranges B from server S0 .
2.	Compute local model wi as SGD(wg, Di).
3.	Compute Bucketize(wg , B, b, wi) and obtain bucketized local model ei.
4.	Creates (2, 2) shares heii0, heii1 of ei.
5.	Send heii0 to S0, send heii1 to S1.
∀j ∈ {0, 1}, server Sj does:
1.	Receive share heiij from client ci, ∀i ∈ [n].
2.	Add shares he1ij . . . henij to obtain hPin=1 eiij.
3.	Call 2-party secure protocol FCOmP (hPn=ι eiij, d2]) and obtain d-length vector m, where
m(k) indicates which bucket holds the median for dimension k ∈ [d].
4.	S0 calls Quantize(wg, B, b, m) and obtains new global model wg0 .
5.	S0 computes new bucket range B0 as 2k(wg0 - wg)k1 + p.
6.	S0 releases wg0 and B0 to all clients.
approximated by the middle value of the range of this bucket. All this while, servers only obtain
(2, 2) additive secret shares of client vectors, and rely on MPC to perform the computation outlined
above and find the median. We now describe these steps in additional detail.
We “bucketize” clients’ updates by dividing each dimension of the model into b buckets. This
enables the servers to build a secret sharing of the frequency distribution of clients’ updates and find
the bucket which contains the median value. The approximate median is then chosen as the middle
value in that bucket.
At the beginning of a given round each client ci receives bucket range B and global model wg from
the central servers, and follow by running SGD on its dataset Di to obtain local model wi . The
bucket range implicitly fixes the buckets around the global model wg, i.e. in any given dimension
k ∈ [d], the b buckets are centered around wg(k) with total range of B. The ending buckets are
taken to represent all the remaining range outside B - i.e. the first and the last buckets represent the
range (-∞, Wg(k) - B], [wg(k) + B, +∞). The middle b - 2 buckets evenly divide the range
Wg(k) ± B. Ci then creates, ∀k ∈ [d], a unit vector ei(k) ∈ {0,1}b with 1 in place for the bucket
which contains wi(k); we refer to this transformation as “bucketizing”. ci then sends secret shares
of ei to both servers
The servers add all the client updates to get a secret-sharing of the frequency distribution of the
clients updates and compute the bucket containing the median value for each dimension. This is
done by performing a secure comparison to compare each bucket,s share of frequency against dTn].
The comparison result is then revealed to learn which bucket contains the median value and the
servers take the middle value of that bucket to be the value of the new global model gw0 for that
dimension. We refer to this transformation of converting a bucketed model back to a model where
each dimension has a numerical value associated with it as “quantizing”. The bucket range is then
updated as 2kWg0 - Wg k1 + p, where p is a parameter agreed by all parties.
Complexity For each dimension, the servers need to only perform b calls to FComP - hence a total of
db calls. Compared to dn(n - 1)/2 calls for pairwise comparison median computation, we note that
ifb is a constant = 10, then while pairwise comparison median needs number of secure comparisons
that grows quadratically with the number of clients, the number of calls in our method is independent
of the number of clients.
5
Under review as a conference paper at ICLR 2022
6 Theoretical guarantees
In this section, we theoretically analyze the proposed method in terms of security, robustness, and
convergence guarantees.
6.1	Robustness and Convergence
We first introduce the notation and settings specific to the robustness and convergence analysis. The
proofs in this subsection are provided in section D in the appendix. Given a data distribution D, we
assume data samples ξ~D are i.i.d. sampled. Data individual loss function is denoted as f (∙ ; ξ):
Rd → R+, i.e., f(w; ξ) ≥ 0 given a parameter w ∈ Rd. Based on the individual loss function, we
denote the population loss function F = Eξ~D [f (∙; ξ)], and we assume it is differentiable. Draw N
i.i.d. samples for each of the n clients (in total nN samples): {ξi,j}i∈[n],j∈[N]. Therefore the client
empirical loss function is
1N
Fi = Nff(∙; ξi,j)	for i ∈ [n].	⑴
The goal of our setting is to minimize the averaged loss across all clients, i.e.,
n
argmιnw 1 Ei=I Fi(w). In the convergence analysis, We consider gradient descent With step
size μ. The range of the buckets at iteration t is denoted as [-Bt/2, Bt/2], and the model parameter
are denoted as wt ∈ Rd . As detailed in Algorithm 3, we consider the bucket range updated by
Bo — Po,	Bt — ηkwt - wt-1k1 + Pi,	(2)
where p0 , p1, η > 0 are constants. There is a convex parameter set W ⊆ Rd such that the pa-
rameters during gradient descent for all clients are within W. Given a function g : Rd → Rd and
p,q ∈ [1, ∞],wedenote the (p, q)-norm of g as Ilgkp,q = (JW ∣∣g(w)kp dμ(w))1/' . In particular,
kgk
∞,q = supw∈W kg(w)kq.
The coordinate-wise median operator Med(∙) takes a set of vectors {vi ∈ Rd}n=ι and outputs a
vector v ∈ Rd such that for each dimension k, v(k) is the standard median of {vi(k) ∈ R}in=1.
The bucket median is calculated as described in Algorithm 3, and we denote it as BucketMed(∙).
Therefore, in the convergence analysis, we consider the global parameter update be
Wt+1 - Wt - BucketMed({μVFi(wt)}i∈[n] ； B,b),
where B is the bucket range and b is the number of buckets.
We make the following assumptions for the robustness and convergence analysis.
Assumption 1 (β∞-smoothness). F is β∞-smooth. Formally, there exists β ≥ 0 such that for
∀w1, w2 ∈ W we have
∣∣VF(wι) - VF(w2)k∞ ≤ β∣∣wι - W2k1,
Note that ∣∣∙∣ι is the dual norm of ∣∣ ∙ ∣∣∞.
Assumption 2 (Weak Law of Large Number). We assume VF = Eξ~D [Vf (∙ ; ξ)] exists and the
sample mean converges to VF in probability in ∣∣ ∙ ∣∣∞,∞. Formally, let ξj ~ D be i.i.d. sampled,
and denote δN,e := Pr {∣∣ VF — N PN=I Vf (∙ ; ξj )∣∣	> , . We assume
∀ > 0 : lim δN = 0.
N→∞ N,
Therefore, by definition, δN,1 ≥ δN,2 if 1 ≤ 2 . Denote N,δ = inf{ : δN, ≤ δ}, and we can
see that N,δ1 ≥ N,δ2 if δ1 ≤ δ2. Moreover, we have the following proposition.
Proposition 6.1. With Assumption 2, we have ∀δ > 0 :	limN →∞ N,δ = 0.
With the above assumptions, the following result shows that the proposed bucket median is robust to
some extent if ratio of adversarial clients is α < 1/2. We provide similar results for dimension-wise
median Med(∙) in Theorem D.1 in the appendix. Note that the previous analysis on dimension-wise
median in the FL setting (Yin et al., 2018) requires α to be smaller than an easily negative value. We
prove the robustness in a new way which holds for any α < 1/2.
6
Under review as a conference paper at ICLR 2022
n
Theorem 6.1 (Robustness). With Assumption 1 &2, consider nι ι.ι.d. samples {VFi}n=ι (equa-
tion 1) and n2 adversarial vectors {vi ∈ Rd}in=21. Denote n := n1 + n2 and α := n2/n being
the ratio of adversaries. Given the bucket range B > 0, the number of buckets b and the gradient
descent step size μ. If α < 1/2 thenfor ∀e > EN 1-2α with probability at least 1 一 (δN,e) 2(1-2α),
,2(1 —a)
we have
∀(Vi ∈ Rd}n= 1 : ∣∣BucketMed({μVFi(w)}n=ι ∪ {物雇i； B, b) - μVF(w)k∞ ≤ μe + Bb,
for ∀w ∈ W satisfying μ∣∣VFi(w)k∞ ≤ B for all i ∈ [ni]. With the bucket median being robust,
we can prove the convergence guarantee for smooth (possibly non-convex) function.
Note that as N → ∞, both δN, → 0 and EN,δ → 0.
Theorem 6.2 (Robust Convergence). With Assumption 1&2, consider n1 normal clients with i.i.d.
n
samples {VFi}in=1 1 (equation 1) and n2 faulty clients that can send arbitrary vectors following the
proposed protocol. Denote n := n1 +n2andα := n2/n being the ratio of faulty clients. The bucket
range adaption is defined by equation 2, where we assume Bt ≤ B for all iteration t for a constant
B > 0, and po ≥ 点 ∣∣VFi(wo)k∞ for ∀i ∈ [ni ]. If α < 1/2 ,for ∀e > EN ；-2a)and ∀E0 > 0, the
proposed method with parameters μ = 看,pi ≥ 2μe + B, η ≥ 1 + βμ and b ≥ d2Bβ can achieve
omt≤τ mmkVF (Wt )k2 ≤
with probability at least 1 — (6n,6) 2(1-2α)
2βF (wo)
T +1
+ (E + E0)2,
Thus, with sufficient number of buckets b and number of data N, the proposed method can converge
w.r.t. the true loss function F with high probability, while α < 1/2 of clients send incorrect local
updates.
6.2 Overview of Security Analysis
We argue the security of our bucketed protocol by means of the standard real/ideal simulation
paradigm in cryptography (defined formally in Appendix B). As stated in section 4.1, we assume a
semi-honest adversary statically corrupting up to n 一 1 clients and at most one server, and prove the
security of one epoch of the FL training using the simulation paradigm. The security of the entire
protocol then follows by applying the sequential composition theorem to the security of each epoch.
We provide a brief overview of the security argument. The use of secure 2-party computation be-
tween the servers allows them to compute the desired approximate median without revealing any-
thing other than the input/output of the functionality. At the beginning of each round, the clients
secret-share their inputs between the two servers and thereafter, the two servers use secure 2-party
protocols to compute the shares of a unit vector with 1 for the bucket in which the median value lies.
Revealing this unit vector allows the servers to learn the median bucket index and find the median
value, which is then sent to clients. An adversary corrupting one of the servers only sees random
shares throughout the protocol (guaranteed by the secure protocol implementing FComp, FEquai, de-
scribed in Appendix C) in addition to bucket range (which is computed solely using public values)
and median bucket index, which it sees in the clear. Additionally corrupting any number of clients
does not allow an adversary to recover any honest client’s information, since honest client’s data are
only secret-shared between the servers and not shared in plain between clients.
We revisit the simulation paradigm and provide the formal security proof in Appendix B.
7	Experimental Results
In this section, we evaluate our proposed bucketed median scheme on different models and datasets.
First, we compare convergence behaviour over epoch of FL of bucketed median, pairwise compari-
son median, and mean methods. We also detail the quadratic versus linear relationship the pairwise
comparison and bucketed median methods respectively have with the number of clients. In the next
subsection, we formally describe the failures that faulty clients can exhibit. We then compare the
robustness of median and our bucketed median against mean. In Appendix F, we detail a breakdown
of one FL cycle and analyze sensitivity of parameters.
Experiments were conducted on an Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GHz virtual ma-
chine with 112 cores. All parties involved in the FL process were separate processes on this machine,
7
Under review as a conference paper at ICLR 2022
O
一把airwise
^Bucketing
2	3 Number of⅛lients ð 1
(b) Communication cost in megabytes per
number of clients
Figure 3: Single-threaded median computation over 1000 dimensions.
-<,airwise
^Bucketing
(a) Time taken per number of clients
Figure 4: Training loss convergence behaviour for 3 clients FL system using different server-side
methods.
(c) MLP, Gaussian noise failure
with a bandwidth cap of 377 MBps to simulate a LAN network. We test on the following combina-
tions of (model, dataset): (MLP, MNIST), (CNNMnist, Mnist), and (CNN, CIFAR-10). The MLP
model has consits of 1 hidden layer with 200 units with ReLu activation for 199, 210 dimensions.
We use the same CNNMnist as McMahan et al. (2017), which has 1,663,370 dimensions. The CNN
model we use consists of two 5 × 5 convolution layers, the first followed by a 2 × 2 max pooling,
and the second followed by three linear layers for a total of 581,866 dimensions. Model training was
done using the CPU. Training batch size is set to 20, learning rate is set to 0.01, and client datasets
are sampled i.i.d. from the global dataset. Number of buckets b = 8.
7.1	Convergence Results
We first test convergence behaviour when all parties are benign. We show that our bucketed median
FL system exhibits similar training loss convergence as our baseline: a mean based FL system. For
all three methods, we run the same number of iterations of FL and note the average training loss
when all clients are benign.
Figure 2: Results of training loss convergence on 3 benign parties.
We also show the relationship between number of parties and the time taken for secure median
computation in both the bucketing and pairwise median methods. We test for number of clients
n ∈ [2, 10] for dimension-wise median when d = 1000. Figure 3 depicts the constant and quadratic
relationships between time and communication costs with number of clients for both bucketing and
pairwise methods. In Appendix F, we provide scalability results with larger number of clients.
8
Under review as a conference paper at ICLR 2022
Figure 5: Training loss convergence behaviour for 3 clients FL system using different server-side
methods.
(c) CNN, Gaussian noise failure
-⅛Bucketing
•Median
♦Mean
(c) CNNMnist, Gaussian noise fail-
ure
system using different server-side
(a) CNNMnist, bitflip failure (b) CNNMnist, label flip failure
Figure 6: Training loss convergence behaviour for 3 clients FL
methods.
7.2	Client Failure Types
We assume the updates sent from clients can be corrupted with errors arising from machine noise,
network transmission errors, data poisoning etc. The failures we test with are:
•	Bit Flip Failure: The most significant bit, or the bit that controls the sign is flipped. The client
sends the negative gradient instead of the true gradient to the servers.
•	Label Switching Failure: Clients compute gradients with switched labels on the training data.
For example, label ∈ {0, ..., 9} is replaced by 9 - label.
•	Gaussian Noise Failure: Instead of sending the correct gradient to the server, clients send an
update with values sampled from a Gaussian distribution with mean of zero and standard deviation
of 200.
7.3	Robustness Results
In this subsection, we empirically show that the robustness of our bucketed median FL system
behaves similarly to a median based FL system. We show that for bit-flip, label-flip, and Gaussian
noise errors, our system still converges. We test with three clients, one of which exhibits failures
starting from the third epoch. Figures 4, 5, 6 shows results of our bucketed median method versus
a median and mean method. In our implementation, the faulty client performs label flipping during
the training process, while bitflip and Gaussian noise are manually computed on the local model
returned from model training. The mean method results in loss of convergence for both bitflip and
Gaussian noise failures, but performs well against label flip failures. However, both the median and
bucketed median methods show continued training loss decrease against all three failures.
8	Conclusion
We presented a dual central-server FL system that is robust and private in the semi-honest setting.
We propose a bucketing approximation technique in order to sacrifice accuracy for computational
efficiency, which also leads to better scalability with the number of clients. The advantage our work
provides are both privacy and robustness in distributed ML.
9
Under review as a conference paper at ICLR 2022
Ethics Statement
Our work focuses on theoretically and empirically studying robust federated learning. All the
datasets and packages we use are open-sourced. Potential ethical concerns are primarily in the
observation that robust federated learning tends to remove outliers, and it can be hard to determine
if data from minority groups are outliers or adversarial.
Reproducibility Statement
We have tried our best to provide training details to facilitate reproducing our results. We pro-
vide detailed results on how to train and evaluate our model. We will open-source our code once
accepted.
References
Cryptflow2: Practical 2-Party Secure Inference. https://github.com/mpc-msri/EzPC/
tree/master/SCI, 2021.
K. A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated
learning on user-held data. In NIPS Workshop on Private Multi-Party Machine Learning, 2016.
URL https://arxiv.org/abs/1611.04482.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, CCS '17, pp. 1175-1191, New York, NY, USA, 2017. Associa-
tion for Computing Machinery. ISBN 9781450349468. doi: 10.1145/3133956.3133982. URL
https://doi.org/10.1145/3133956.3133982.
Henry Corrigan-Gibbs and Dan Boneh. Prio: Private, robust, and scalable computation of
aggregate statistics. In 14th USENIX Symposium on Networked Systems Design and Im-
plementation (NSDI 17), pp. 259-282, Boston, MA, March 2017. USENIX Association.
ISBN 978-1-931971-37-9. URL https://www.usenix.org/conference/nsdi17/
technical-sessions/presentation/corrigan-gibbs.
Ivan Damgard, Matthias Fitzi, Eike Kiltz, Jesper BUUs Nielsen, and Tomas Toft. Unconditionally
secure constant-rounds multi-party computation for equality, comparison, bits and exponentiation.
In Shai Halevi and Tal Rabin (eds.), Theory of Cryptography, pp. 285-304, Berlin, Heidelberg,
2006. Springer Berlin Heidelberg. ISBN 978-3-540-32732-5.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic coUntermeasUres. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, CCS ’15, pp. 1322-1333, New York, NY, USA,
2015. Association for CompUting Machinery. ISBN 9781450338325. doi: 10.1145/2810103.
2813677. URL https://doi.org/10.1145/2810103.2813677.
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients -
how easy is it to break privacy in federated learning?, 2020.
O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game. In Proceedings of
the Nineteenth Annual ACM Symposium on Theory of Computing, STOC ’87, pp. 218-229, New
York, NY, USA, 1987. Association for Computing Machinery. ISBN 0897912217. doi: 10.1145/
28395.28420. URL https://doi.org/10.1145/28395.28420.
Oded Goldreich. The Foundations of Cryptography - Volume 2: Basic Applications. Cambridge
University Press, 2004. ISBN 0-521-83084-2. doi: 10.1017/CBO9780511721656. URL http:
//www.wisdom.weizmann.ac.il/%7Eoded/foc-vol2.html.
Nirupam Gupta, Shuo Liu, and Nitin H. Vaidya. Byzantine fault-tolerant distributed machine learn-
ing using stochastic gradient descent (sgd) and norm-based comparative gradient elimination
(cge), 2021.
10
Under review as a conference paper at ICLR 2022
Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Secure byzantine-robust machine learning,
2020.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael
G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria
Gascon, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie
He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail
Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, TanCrede
Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgui r, Rasmus Pagh,
Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Se-
bastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Ad-
vances and open problems in federated learning. CoRR, abs/1912.04977, 2019. URL http:
//arxiv.org/abs/1912.04977.
Nishant Kumar, Mayank Rathee, Nishanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul
Sharma. Cryptflow: Secure tensorflow inference. In 2020 IEEE Symposium on Security and
Privacy (SP),pp. 336-353. IEEE, 2020.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated
learning through personalization, 2021.
H.	Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agui era y Arcas.
Communication-efficient learning of deep networks from decentralized data, 2017.
Payman Mohassel and Peter Rindal. Aby3: A mixed protocol framework for machine learning. In
Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security,
pp. 35-52, 2018.
Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine
learning. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 19-38, 2017. doi: 10.1109/
SP.2017.12.
Takashi Nishide and Kazuo Ohta. Multiparty computation for interval, equality, and comparison
without bit-decomposition protocol. In Tatsuaki Okamoto and Xiaoyun Wang (eds.), Public Key
Cryptography - PKC 2007, pp. 343-360, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.
ISBN 978-3-540-71677-8.
Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem
Rastogi, and Rahul Sharma. Cryptflow2: Practical 2-party secure inference. Proceedings of
the 2020 ACM SIGSAC Conference on Computer and Communications Security, Oct 2020. doi:
10.1145/3372297.3417274. URL http://dx.doi.org/10.1145/3372297.3417274.
Jinhyun So, Basak Guler, and A. Salman Avestimehr. Byzantine-resilient secure federated learning,
2021.
Anselme Tueno, Florian Kerschbaum, Stefan Katzenbeisser, Yordan Boev, and Mubashir Qureshi.
Secure computation of the kth-ranked element in a star network, 2019.
Zhibo Wang, Song Mengkai, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi. Beyond
inferring class representatives: User-level privacy leakage from federated learning. pp. 2512-
2520, 04 2019. doi: 10.1109/INFOCOM.2019.8737416.
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized byzantine-tolerant sgd, 2018.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent
with suspicion-based fault-tolerance. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pp. 6893-6901. PMLR, 09-15 Jun 2019a. URL
http://proceedings.mlr.press/v97/xie19b.html.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Slsgd: Secure and efficient distributed on-device
machine learning, 2019b.
11
Under review as a conference paper at ICLR 2022
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant
Sgd by inner product manipulation. In Uncertainty in Artificial Intelligence, pp. 261-270. PMLR,
2020.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd, 2021.
Andrew Chi-Chih Yao. How to generate and exchange secrets (extended abstract). In 27th Annual
Symposium on Foundations of Computer Science, Toronto, Canada, 27-29 October 1986, pp.
162-167. IEEE Computer Society, 1986. doi: 10.1109/SFCS.1986.25. URL https://doi.
org/10.1109/SFCS.1986.25.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650-5659. PMLR, 2018.
12
Under review as a conference paper at ICLR 2022
A Full Description of our algorithms
Here we provide full descriptions of both our median based and approximate bucketed median based
FL algorithms (Algorithm 2, Algorithm 3), along with more details and correctness sketch for the
same.
A.1 Median based FL system
This is formally specified in full in Algorithm 2.
Correctness sketch. We sketch the idea behind the protocol, which is inspired from similar ideas
used in (Tueno et al., 2019). To compute the median of n values - {x1 , x2 . . . xn}, we first com-
pute all pairwise comparison results - bij = (xi > xj), ∀i, j ∈ [n], i 6= j. This allows us
to compute for each element xi, its position in the sorted list of {x1, x2 . . . xn}, by computing
posi = Pk∈[n],k6=i bik. Finally, we find element for which posi = dn/2e.
In our actual computation, we only compute bij , ∀i, j ∈ [n] s.t. j > i. Then bji is taken as 1 - bij .
The correctness of this median depends on all the values being different, which we assume to happen
in our protocol (and abort if this is not true).
Float-to-fixed conversion. Our protocol for exact median involves mapping the client’s local
floating point values to integers, since it is well known that most cryptographic protocols are much
more efficient when working over integer rings than directly over floating point values (Mohassel
& Zhang, 2017; Mohassel & Rindal, 2018; Kumar et al., 2020). Previous work in this area (Mo-
hassel & Zhang, 2017; Mohassel & Rindal, 2018; Kumar et al., 2020), has made use of fixed point
representation for the same, where real number r is mapped to integer Pr = [r * 2s], where S is
a parameter called the scaling factor. Hence, given a real number r, we map it to its fixed point
representation Pr, which is then mapped to an integer ring like Z2'. In the following, We use WiInt
to denote fixed point version of wi .
F2I',s and I2F',s are functions which are used for the mapping real numbers to (fixed-point) integers
and vice versa respectively. In particular, F2I',s : R → z`, is defined as F2I',s(x) = [x * 2s].
I2F',s : Z2' → R is defined as I2F',s(x) = x∕2s (where X ∈ Z2' is first interpreted as a signed
integer and then the division is performed).
Since we use ` = 64, s = 24 and 32-bit floating point values throughout and which have only 23
bits of precision (and since we are only performing addition over the fixed point values/the values
involved at any intermediate stage are small) we don’t suffer any accuracy loss/convergence rate loss
due to the usage of fixed point values.
A.2 Bucketed median based FL
In Algorithm 3 we describe in full our algorithm to compute the bucketed median.
13
Under review as a conference paper at ICLR 2022
Algorithm 2 One training epoch of exact median based FL
Input: ∀i ∈ [n], ci holds dataset Di
Parameters: Bitlength `, fixed point scale s
∀i ∈ [n], client ci does:
1.	Receive fixed-point shares of global modelhwgntij ∈ Z' from server Sj, ∀j ∈ {0,1}.
2.	Compute WF = hwgntio + hwgntiι and Wg = I2F',s(wgnt).
3.	Compute local model wi = SGD(wg , Di).
4.	Convert back to fixed point as Wint = F2I',s (wi).
5.	Create fresh (2, 2) secret shares of Wint by choosing hwintio at random from zg` and com-
puting hWiInti1 = WiInt - hWiInti0.
6.	∀j ∈ {0, 1}, send hWiIntij to server Sj.
∀j ∈ {0, 1}, server Sj performs the following:
1.	∀i ∈ [n], receive share hWiIntij from client ci.
2.	∀k ∈ [d], compute shares of median in dimension j across all the n clients as follows:
(a)	∀x ∈ [n], y ∈ [x + 1, n], compute hzxk,yij where zxk,y = (WxInt(k) > WyInt(k)) by
Caning FComPWith inputs hwχnt⑻ij, hwynt㈤%. Set hzy,xij =j - hzk,y ij.
(b)	∀i ∈ [n], compute hposiij = Px∈[n],x6=ihzik,xij.
(c)	Set mid = (n + 1)/2 ifn is odd, else set mid = n/2.
(d)	∀i ∈ [n], check if Posi = mid by calling FEquM with inputs hposiij ,j * mid and
receiving output heqiij in return.
(e)	∀i ∈ [n], send heqiij to server S1-j and compute eqi = heqii0 + heqii1.
(f)	Find i ∈ [n] s.t. eqi(k) = 1 and set hWgInt0(k)ij = hWiInt(k)ij.
3.	Send hWgInt0ij to each client ci, ∀i ∈ [n].
14
Under review as a conference paper at ICLR 2022
Algorithm 3 One training epoch of bucketed median based FL
Input: ∀i ∈ [n], clients ci holds dataset Di
Parameters: Number ofbuckets b, bitlength ',total epochs r, constant values po, pi, Pt = p1, where
t is the current epoch number.
[h] ∀i ∈ [n], client ci does:
1.	Receive global model wg and bucket range B from server S0 .
2.	Compute local model wi = SGD(wg , Di).
3.	Bucketize wi to get ei ∈ {0, 1}d×b. Specifically, for each dimension k in [d], compute the
kth row of bucketed model ei as follows:
•	If Wi(k)	≤ Wg(k)	-	B2,	set	ei(k)	as	[1, 0,..., 0].
•	If Wi(k)	≥ Wg(k)	+	B2,	set	ei(k)	as	[0,..., 0,1].
•	Else, let Z = [wi(k)B(Wg-2-B/2)C + 1. Set ei(k) as [0,..., 0,1,0,...,0], where
ei (k, z) is set to 1.
4.	Create (2,2) secret shares of ei by choosing(£加 at random from Z2×× and computing
heii1 = ei - heii0.
5.	∀j ∈ {0, 1}, send heiij to server Sj.
∀j ∈ {0, 1}, server Sj performs the following:
1.	∀i ∈ [n], receive share heiij from client ci.
2.	Set hhij =Pi∈[n]heiij.
3.	For each dimension k in [d], calculate the approximate median:
(a)	∀y ∈ [2, b], set hh(k, y)ij = Pz∈[y] hh(k, z)ij.
(b)	Find the bucket that contains the median:
i.	Computehm(k)〉j, where m(k) is of form [0,..., 1,..., 0], by calling FComP
with So,s input being hh(k)io, [d n2 ]]d×b and Si's input being hh(k)ii, [0]d×b. Set
hm(k)ij = [j]b- hm(k)ij .
ii.	Sj shares hmij with Si-j .
iii.	S0 computes m = hm(k)i0 Lhm(k)ii.
iv.	S0 selects the bucket y that contains the median as miny∈[0,b-i])[m(k, y) ≡ 1].
(c)	S0 quantizes the approximate median value Wg0 (k) as follows:
•	If y == 0, wg(k) = Wg(k) - BB
•	If y == b - 1, wg(k) = Wg(k) + B2
•	Else, Wg(k) = ((Wg(k) - B2 + (y - 1) *(昌))+ (Wg(k) - B2 + y *(昌)))/2.
4.	S0 creates new bucket range B0 = 2kWg0 - Wgki + pi,t, where pi,t is predetermined.
5.	S0 sends Wg0 , B0 to client ci , ∀i ∈ [n].
To initialize FL, server S0 performs the following:
1.	Initialize Wg as model with parameter values of 0.
2.	Set B = p0 .
3.	S0 sends Wg, B to each client ci , ∀i ∈ [n].
B Formal security Proof
In this work, we use the semi-honest or honest-but-curious adversarial model, where it is assumed
that the adversary follows the protocol specifications, but tries to glean information about the other
party’s inputs using the protocol transcript. As stated previously, we assume a semi-honest adver-
sary, statically (i.e. before starting the protocol) corrupting up to n - 1 clients and one of the servers,
and prove full security of the protocol using the simulation paradigm.
15
Under review as a conference paper at ICLR 2022
B.1	Defining Secure Multiparty Computation
Here, we provide a formal definition of secure multiparty computation. Parts of this section have
been borrowed verbatim from (Goldreich, 2004).
A multi-party protocol is cast by specifying a random process that maps pairs of inputs to pairs of
outputs (one for each party). We refer to such a process as a functionality. The security of a protocol
is defined with respect to a functionality f. In particular, let n denote the number of parties. A
non-reactive n-party functionality f is a (possibly randomized) mapping of n inputs to n outputs. A
multiparty protocol with security parameter λ for computing a non-reactive functionality f is a pro-
tocol running in time polynomial in λ and satisfying the following correctness requirement: if parties
P1, . . . , Pn with inputs (x1, . . . , xn) respectively all run an honest execution of the protocol, then
the joint distribution of the outputs y1, . . . , yn of the parties is statistically close to f(x1, . . . , xn).
A reactive functionality f is a sequence of non-reactive functionalities f = (f1, . . . , f`) computed
in a stateful fashion in a series of phases. Let xij denote the input of Pi in phase j, and let sj denote
the state of the computation after phase j . Computation of f proceeds by setting s0 equal to the
empty string and then computing (yj,...,yjrj, Sj) J f (SjT,x《,...，Xn) for j ∈ ['], where yj
denotes the output of Pi at the end of phase j . A multi-party protocol computing f also runs in
` phases, at the beginning of which each party holds an input and at the end of which each party
obtains an output. (Note that parties may wait to decide on their phase-j input until the beginning of
that phase.) Parties maintain state throughout the entire execution. The correctness requirement is
that, in an honest execution of the protocol, the joint distribution of all the outputs {y1j , . . . , yj }jj=ι
of all the phases is statistically close to the joint distribution of all the outputs of all the phases in a
computation of f on the same inputs used by the parties.
Defining Security. The security of a protocol (with respect to a functionality f) is defined by
comparing the real-world execution of the protocol with an ideal-world evaluation of f by a trusted
party. More concretely, it is required that for every adversary A, which attacks the real execution
of the protocol, there exist an adversary Sim, also referred to as a simulator, which can achieve the
same effect in the ideal-world. Next, we specify what each of these worlds mean.
The real execution In the real execution of the n-party protocol π for computing f is executed in
the presence of an adversary A. The honest parties follow the instructions of π. The adversary A
takes as input the security parameter k, the set I ⊂ [n] of corrupted parties, the inputs of the cor-
rupted parties, and an auxiliary input z. A obtains the local randomness and the state of all corrupted
parties, but all corrupted parties do follow the strategy outlined in the protocol specification.
The interaction ofA with a protocol π defines a random variable REALπ,A(z),I (λ, ~X) whose value is
determined by the coin tosses of the adversary and the honest players. This random variable contains
the output of the adversary (which may be an arbitrary function of its view, which includes the state
of all corrupted parties) as well as the outputs of the uncorrupted parties. We let REALπ,A(z),I
denote the distribution ensemble {REAL∏,∕(z),/(λ, ~)}k∈N,h~,z>∈{o,i}*.
The ideal execution - security with abort. In this second variant of the ideal model, fairness and
output delivery are no longer guaranteed. This is the standard relaxation used when a strict majority
of honest parties is not assumed. In this case, an ideal execution for a function f proceeds as follows:
• Send inputs to the trusted party: As before, the parties send their inputs to the trusted
party, and we let X0i denote the value sent by Pi . Once again, for a semi-honest adversary
we require Xi = Xi for all i ∈ I .
• Trusted party sends output to the adversary: The trusted party computes
f(X01, . . . , X0n ) = (y1, . . . , yn ) and sends {yi}i∈I to the adversary.
• Adversary instructs trusted party to abort or continue: This is formalized by having
the adversary send either a continue or abort message to the trusted party. (A semi-honest
adversary never aborts.) In the latter case, the trusted party sends to each uncorrupted party
Pi its output value yi . In the former case, the trusted party sends the special symbol ⊥ to
each uncorrupted party.
• Outputs: Sim outputs an arbitrary function of its view, and the honest parties output the
values obtained from the trusted party.
16
Under review as a conference paper at ICLR 2022
The interaction of Sim with the trusted party defines a random variable IDEALf,A(z) (λ, ~x) as
above,and We let {IDEALf,/(z),i(λ,~)}k∈N,h~,zi∈{o,i}*. Having defined the real and the ideal
worlds, we now proceed to define our notion of security, where a semi-honest adversary is defined
as an (interactive) Turing Machine Whose specifications match those of the protocol.
Definition 1. Let k be the security parameter. Let f be an n-party randomized functionality, and
Π be an n-party protocol for n ∈ N. We say that Π t-securely computes f in the presence of semi-
honest adversaries if for every semi-honest adversary A there exists a semi-honest adversary Sim
such that for any I ⊂ [n] with |I| ≤ t the following quantity is negligible:
|P r[REALΠ,A(z),I (λ, ~x) = 1] - P r[IDEALf,A(z),I (λ, ~x) = 1]|
where X = {xi}i∈[n] ∈ {0,1}* and Z ∈ {0,1}*.
B.2	Simulation
Since our model is semi-honest and our protocols use secure 2-party computation, the simulator in
our case is straightforWard. Given the inputs of the adversary and the bucket index containing the
median value across clients, the simulator sends random shares to the corrupted server on behalf of
the honest clients. Finally, during secure bucketed median computation, it invokes the simulator for
the secure protocol implementing F^mp, and to this simulator it provides as input the bucket index
of the median value.
C Protocols implementing FC°mp, FEquai
For exposition purposes, We described our protocols for median computation by Writing them in a
“hybrid" model, where parties have access to trusted functionalities FComp, FEqual, computing secret
sharing of comparison and equality of tWo inputs respectively (described in Figure 7 and Figure 8
respectively). In this section, we provide protocols implementing the said functionalities. Our final
protocol is then obtained by composing these with our own protocols. We then provide reasoning
on why we choose secure comparisons as the metric for protocol complexity.
FC omp
Parties - P0 , P1
•	∀b ∈ {0,1}, Pb sends xb, y ∈ Z* to FComp.
•	FComp computes x = xo + xι	∈ Z2', y =	yo + yι ∈	Z2' and bit Z = (x > y).
•	FComp randomly chooses zo	∈ Z' and	computes	zι = Z - zo ∈ Z' and	sends Zb to
Pb, ∀b ∈ {0, 1}.
Figure 7:	Ideal functionality for 2-party comparisons: givenhx〉，〈y〉computehx 〉 y).
FEquaI
Parties - Po , P1
•	∀b ∈ {0,1}, Pb sends xb, y ∈ Q2 to FEqUa「
•	F-Equal computes X = xo + xι	∈	Z2', y = yo + yι ∈	‰j' and bit Z = (x = y).
•	FEqual randomly chooses Zo	∈	Z' and computes	zi = Z — Zo ∈ Z' and	sends Zb	to
Pb, ∀b ∈ {0, 1}.
Figure 8:	Ideal functionality for 2-party equality: given hx〉, hy〉 compute hx = y〉.
Notation Before going further, lets setup some notation. For a boolean variable Z, we use notation
1{Z} to indicate the indicator variable, which is 1 iffZ is true. While hx〉o, hx〉1 denote (2, 2) additive
secret shares of X over Z', we use notationhx〉B,〈x〉B to denote the (2, 2) additive secret shares of
x over Z2 .
To implement FComp, Equal，we use and modify a bit the protocols proposed in CrypTFlow2 (Rathee
et al., 2020). We describe the protocols for these in the following hybrids:
17
Under review as a conference paper at ICLR 2022
•	FMIill： This is described in Figure 9. It computes the solution to the millionaire,s problem
over z` and provides boolean secret shares of the answer bit to the two parties.
•	F^2A： This is described in Figure 10. It takes in boolean shares of a bit from the two parties
and provides shares of the same over rZ2.
•	FM山网：This is described in Figure 11. It is similar to FMill, except instead of doing
comparison, it checks equality of the two received values.
Each of the above hybrids can be implemented directly using protocols from CrypTFlow2 (Rathee
et al., 2020). In more detail, while protocols for FM山,FR、are described directly in (Rathee et al.,
2020), the protocol for FMill,eq can be obtained directly by a small tweak1 to the protocol for FM山.
Since these protocols follow directly from (Rathee et al., 2020), we skip providing detailed descrip-
tions and proofs of them here and refer the reader to CrypTFlow2 (Rathee et al., 2020) for the same.
Given the above three hybrids, we provide the protocols for implementing 邛册,FE'qual in Algo-
rithm 4, Algorithm 5 respectively. We then have the following theorems (where “securely” refers to
simulation-based security according to Definition 1).
Theorem C.1. ∏Cj0mp securely implements FCOmP in the FM-1, FB2λ hybrid as long as the signed
inputs of the two parties x, y (whose secret-shares they input into the protocol) satisfy |x| + |y| <
Theorem C.2. πEquαl SeCureIy implements FEqual in the FM加留,FB2a hybrid.
Composing the corresponding implementation of FMM山，*2a, FMMill eq (as described above), gives us
SeCure protocols for FComp, FEqual.
FM ill
Parties - P0 , P1
•	Po sends X ∈ {0,1}' to FMM山，while Pi sends y ∈ {0,1}' to FM山.
•	FMM山 computes Z = (x < y), chooseshz〉B J {0,1} at random and computeshz〉B
〈z〉B ㊉ z. FM山 sends〈z〉B to Pb,∀b ∈ {0,1}.
Figure 9:	Ideal functionality for solving the Millionaire,s problem
FB2A
Parties - P0 , Pi
•	∀b ∈ {0,1}, Pb sends〈x〉B to FB2、.
•	*2a computes X =(x〉B ㊉〈xiB, chooseshx〉o J Z' at random and computes(x)i ∈ Z'
as hxii = x - hxi0. It sends hxib to Pb, ∀b ∈ {0, 1}.
Figure 10:	Ideal functionality for converting boolean to arithmetic shares
1 (Rathee et al., 2020) provides protocols for FM山 and FLa directly. The protocol for FM山 works by
breaking up the ` bits of inputs into a balanced binary tree and computing at each level the answer of both
comparison and equality checks of the inputs corresponding to that subtree. Hence, while not stated directly in
the protocol in (Rathee et al., 2020), a protocol for FMiU)eq can be easily obtained from the protocol for FM山.In
addition, the public implementation of CrypTFlow2 (cry, 2021), contains functions which allow us to compute
the same without tweaking anything underlying.
18
Under review as a conference paper at ICLR 2022
FM ill,eq
Parties - P0 , P1
•	Po sends X ∈ {0,1}' to FM山用，while Pi sends y ∈ {0,1}' to FM山用.
•	FMMill eq computes Z = (X = y), chooseshz〉B J {0,1} at random and computeshziB =
hz)B ㊉ z. FMiiι,eq sendshz〉B to Pb,∀b ∈ {0,1}.
Figure 11:	Ideal functionality checking if two parties hold the same value
Algorithm 4 ' bit signed comparison protocol, ∏C°mp.
Input: ∀b ∈ {0,1},Pb holdshxib,〈y%.
Output: ∀b ∈ {0, 1}, Pb holds h1{X > y}ib
1.	∀b ∈ {0, 1}, Pb computes hzib = hXib - hyib.
2.	Compute boolean shares of msb(z) as follows:
(a)	∀b ∈ {0, 1}, Pb parses hzib as msbb||yb.
(b)	Po and Pi invoke FMlmI with Po's input 2'-i — y° and Pi's input y、. ∀b ∈ {0,1}, Pb
learns hcarryibB .
(c)	∀b ∈ {0,1}, Pb computes〈t〉B = msbb ㊉ hcarryiB.
3.	Po and Pi invoke FR、with inputsht〉B to learn ©b
Algorithm 5 ' bit equality protocol, ∏Equa].
Input: ∀b ∈ {0,1},Pb holdshxib,〈y%.
Output: ∀b ∈ {0, 1}, Pb holds h1{X = y}ib
1.	∀b ∈ {0, 1}, Pb computes 〈zib = 〈Xib - 〈yib.
2.	Check if z = 0 by doing the following:
(a)	Po and Pi invoke computes FMineq with Po's input〈z〉o, while Pi's input being
(-〈zii) mod 2'. ∀b ∈ {0,1},Pb learns〈eq〉B.
(b)	Po and Pi invoke 岑?、with inputs (eq)B to learnheq%.
C.1 Why we use secure comparison as complexity metric
MPC is efficient on linear operations when using arithmetic shares. The sum of two arithmetic
shares hai + hbi result in share ha + bi. However, this is not the case for non-linear operations such
as hai > hbi, since the operation performed and the type of secret sharing are not of the same nature.
These type of non-linear operations such as multiplication, comparisons, and equality require in-
creased computations, such as generation of correlated randomness between parties. There have
been many works on improving the efficiency of secure comparisons. (Damgard et al., 2006) pro-
vide a secure comparison protocol which uses O(' log ') secure multiplications, where ' = logP
and p is the prime used for the field the secret shares are computed over. (Nishide & Ohta, 2007)
improved the complexity of secure comparison to O(') secure multiplications.
Thus a large portion of the evaluation of a function in MPC is due to the computation of non-linear
operations. Computing median intuitively involves expensive comparison operations, and therefore
we choose the number of secure comparisons needed as a metric for computational complexity of
the protocols.
D Detailed Robustness and Convergence Analysis
Proposition D.1 (Proposition 6.1 Restated). With Assumption 2, we have
∀δ > 0 : lim N δ = 0.
N→∞ ,
19
Under review as a conference paper at ICLR 2022
Proof. Given a δ > 0, for ∀ > 0, by Assumption 2 there exists N such that ∀M : M > N we have
δM,e < δ∕2. Therefore, em,s ≤ ^M,δ∕2 = inf{e0 ： Sm^ ≤ δ∕2} ≤ e.	□
n
Theorem D.1. With Assumption 1 &2, Consider nι i.i.d. samples {VFi}i=1 (equation 1) and n
adversarial vectors {vi ∈ Rd}in=21. Denote n := n1 + n2 and α := n2∕n being the ratio of
adversaries. If α < 1/2, then for ∀e > EN ；-2a)with probability at least 1 一 (δN,e) 2(1-2α), we
have
∀{vi ∈ Rd}n= 1 :	∣∣Med({VFi(w)}21 "勺}建］)一 VF(W)I∣∞ ≤ e,
for ∀w ∈ W. Note that as N → ∞, both δN,e → 0 and EN,δ → 0.
Proof. By definition of the median operation, we have
ʌ __
∣Med({VFi(w)}n=ι ∪ {vi}建 J- VF(w)k∞
,_ ʌ , _ , ...
= ∣∣Med({VFi(w) - VF(w)}；^ ∪ {vi - VF㈤雁/屋.	⑶
Invoking Lemma E.2, we have
ʌ __
(3) ≤ Med({∣∣VFi(W)- VF(w)k∞}=ι ∪ {∣vi - VF(w)k∞}=ι)
._ .. ʌ ... _ __ _ __
=Med({∣∣VFi(w) - VF(w)k∞}n=ι ∪ {电雁6	(4)
where ai = Ivi - VF(W)I∞ ∈ R.
As We can see, (4) depends on the variance of VA = NN PN=ι Vf (∙ ; ξi,j), where ξi,j 〜D are
i.i.d. sampled. Denote
Xi,e := 1{kVF - VFill∞,∞ ≤ e},
where 1(∙) is the indicator function. By Assumption 2, we can see that Xi is a Bernoulli random
variable with
Pr{Xi,e = l} = 1 - δN,e.
Therefore, Xe ：= Pn= ι Xi,e forms a binomial distribution. By the Chernoff bound for binomial
distribution, for k ∈ N and k ≤ n1(1 - δN,e), we have
-nι DKL ~^( Il (1-δN e)
Pr(Xe ≤ k) ≤ e	ln1"	(	(5)
where DKL is the Kullback-Leibler divergence, i.e., for q1,q2 ∈ (0,1)
DκL(qι kq2) ：= qι log (q1) +(1 - qι)log f1q1)
q2	1 - q2
≥ q1 log q1 + (1 - q1) log(1 - q1) - (1 - q1) log(1 - q2)
≥ (I - qι)log τ⅛,
where the first inequality is due to that q2 < 1 and the last inequality is because that q1 log q1 + (1 -
q1) log(1 - q1) is the entropy which is non-negative.
Combining the above inequality with equation 5, we have
k1
Pr(Xe ≤ k) ≤ e-n1(1-ni)log 市=(δN,e)n1-k .	(6)
Denoting event
Ee ：= {Xe >「},	⑺
we can see that if Ee happens then by Lemma E.1 we have (4) ≤ E. It left to show the probability
of event Ee happens. Therefore, when n2+n2 ≤ (1 - δN,e),we can apply equation 6 to have
Pr(Ee) = 1 - Pr(Xe ≤ n1+n2) ≥ 1 - (δN,e)2(1-2α).	(8)
The condition of n2+n2 ≤ (1 - δN,e) is equivalent to δ∣e ≤ 2(-2a), and it has a sufficient condition
of € > € τ∖τ 1 —2α .
N，2(1 —a)
□
20
Under review as a conference paper at ICLR 2022
n
TheoremD.2 (Theorem 6.1Restated). With Assumption 1 &2, consider nι i.i.d. SamplesBFi}n=ι
(equation 1) and n2 adversarial vectors {vi ∈ Rd}in=21. Denote n := n1 + n2 and α := n2/n being
the ratio of adversaries. Given the bucket range B > 0, the number of buckets b and the gradient
descent step size μ. If α < 1/2 thenfor ∀e > EN 1-2α with probability at least 1 一 (δN,e) 2(1-2α),
,2(1 —a)
we have
B
∀{vi ∈ Rd}n=ι : ∣∣BucketMed({μVFi(w)}n= 1 ∪ 3雁^ Bm- μVF(w)k∞ ≤ μe 十赤
for ∀w ∈ W satisfying μ∣∣VFi(w)k∞ ≤ B forall i ∈ [n].
Proof. First, by the definition of BucketMed(∙), for any input vector Vi ∈ Rd to the BucketMed(∙),
there exists a vector vi0 ∈ Rd with kvi0k∞ ≤ B such that
BucketMed({μVFi(w)Kι ∪ {g}n= 1； B, b) = BucketMed({μVF,(w)}n= ι ∪ {vi}n=ι; B, b).
Denote E as the event defined in equation 7 By equation 8 we can see that
Pr(Ee) ≥ 1-(δN,e)2(1-2α).
Given the event E, by Theorem D.1 we can see that
.. ,, ʌ , . . .. „ ...
kMed({μVFi(W)}i=ι ∪ {v0}，=I)- 〃VF(W)k∞ ≤ μe,	⑼
for ∀w ∈ W. Moreover, for ∀w ∈ W satisfying μ∣∣VF0(w)k∞ ≤ B for all i ∈ [nι], by Lemma E.3
we have
B
kBucketMed({μVF0(w)}n=1 ∪ {v0}n= 1； B, b) - Med({μVF0(w)}n=1 ∪ {v0}n=J∣∣∞ ≤ -.
(10)
Therefore, combining equation 9 and equation 10 gives the theorem.	口
Theorem D.3 (Theorem 6.2 Restated). With Assumption 1&2, consider n1 normal clients with i.i.d.
n
samples {VF0}0n=1 1 (equation 1) and n2 faulty clients that can send arbitrary vectors following the
proposed protocol. Denote n := n1 +n2andα := n2/n being the ratio of faulty clients. The bucket
range adaption is defined by equation 2, where we assume Bt ≤ B for all iteration t for a constant
7^^> - C	1	、 1 11 X—7	∖ 11	C ∖ / ■ _ Γ 1 ri'	, i /rʌ /■ ∖ /	1 ∖ / !	ι~∖ . »
B > 0, and Po ≥ 彘 IlVFi(Wο)k∞ for ∀i ∈ [nι ]. If α < 1/2 ,for ∀e >j-2α and∀60 > 0, the
proposed method with parameters μ = dβ, pi ≥ 2μe + B, η ≥ 1 + βμ and b ≥ dBβ Can achieve
omt≤τ mmkVF IWt )k2 ≤
2βF (wo)
T +1
+ (E + E0)2,
with probability at least 1 — (6n,6) 2(1-2α)
Proof. Denote Ee := {Xe > n1+ n2} as the event defined in equation 7, where
n1
Xe := £1{||VF -VFill∞,∞ ≤ e}.
i=1
By equation 8 we have
n
Pr(Ee) ≥ 1-(δN,e)2(1-2α).
Now condition on the event Ee, and denote the set of clients
I := {i ∈ [n1] : kVF - VFill∞,∞ ≤ e},	(II)
and We can see that |I| > n1+ n2.
Recall that the training procedure of Algorithm 3 is
_ _ _ _ _,,_____________________________ʌ , _________ , ________ _
Wt = Wt-1 - BucketMed({μVFi(wt-ι)}i=i ∪{vi,t-1}i=1; Bt-i,b),
where vi,t-1 are sent from the faulty clients i at iteration t. In the following, for notational ease we
denote
_ _ _ _ _,,_____ʌ , ______ , ___________ _
gt-i := BucketMed({μVFi(Wt-I)卜% ∪ {vi,t-i}i=i； Bt-ι,b).
Moreover recall the bucket range Bt at iteration t is updated by
Bt = ηlWt - Wt-1l1 +p1 = ηlgt-1l1 +p1.
Now, given any i ∈ I , we make the following claim.
21
Under review as a conference paper at ICLR 2022
Claim 1. Conditionedon event E, for any iteration step t ≥ 0 we have
___ʌ . .. _
μ∣NFi(Wt)k∞ ≤ Bt,	(12)
B
llμVF(Wt) - gtk∞ ≤ μe + χ7.	(13)
Proof of Claim 1. Note that equation 12 is true for t = 0 by assumption, and we prove the claim by
induction. Assuming equation 12 holds for iteration t - 1, we prove equation 12 for iteration t, and
prove equation 13 for iteration t - 1 by the way. We begin by
..__ʌ , ...
μ∣∣VFi(wt)k∞
=μk VFi(Wt)- VF(Wt) + VF(Wt)- VF(Wt-I) + VF(Wt-I)- gt-i∕μ + gt-i∕μk∞
___ʌ ， 、_______， , .. __________ 、______， ,..
≤μ∣∣VFi(Wt) - VF(Wt)k∞ + μ∣∣VF(Wt)- VF(Wt-ι)k∞
`----------{z----------} `------------------------}
a1	a2
+ IImvf(Wt-I) - gt-ιk∞ + IIgt-Ill
×-----------{--------/	X—{z-
(14)
∞,
一
a3
a4
where the last step is by triangle inequality. We will show that a1, a2, a4 have straight-forward upper
bounds, respectively. For the a1, by equation 11 we have
aι ≤ μe.
By Assumption 1, we have
a2 ≤ μβ∣∣Wt - Wt-IIlI = μβkgt-1k1.
For a4, by definition of the '1 norm and '∞ norm, We directly have
a4 ≤ Igt-1I1.
Note that a3 is exactly We Want to prove for equation 13. For a3, We need to prove a slightly
different version of Theorem 6.1, hoWever, using similar techniques. Recall the definition of gt-1
and I ⊆ [n1], We have
..___， 、 _ _ _ _ _,,__________ʌ , _______ , _______ _ _...
a3 = ∣μVF(Wt-ι) - BucketMed({μVFi(Wt-1)}i=1 ∪ {vi,t-ι}n= 1； Bt-1, b)k∞
=kμVF(Wt-ι) - BucketMed({μVFi(Wt-I)}i∈ι ∪ {μVFi(W)}i∈[m]∖ι ∪ {vi,t-ι}n= 1； Bt-1, b)k∞.
n
NotethatWehaveassUmedthatkμVFi(Wt-1)k ≤ Bt-1. For {μVFi(W)}i∈[m]∖ι ∪ {vi,t-1}i=1,
there exists n1 + n2 - |I| vectors vi0 ∈ Rd satisfying Ivi0I∞ ≤ Bt-1 for i ∈ [n1 + n2 - |I|] such
that
_ _ _ _ _,,_____ʌ , 、、一 , ____________ _ _.
BucketMed({μVFi(Wt-1)}i=1 ∪ {vi,t-1}i= 1； Bt-1, b)
=BucketMed({μVFi(Wt-1)}i∈ι ∪{μVA(W)}i∈g]∖ι ∪{vi,t-1∏= 1； Bt-1, b)
=BucketMed({μVFi(Wt-1)}i∈ι ∪{vi}n=+n2TIl ； Bt-1, b)	(15)
Therefore, denoting V := {μVFi(Wt-1)}i∈ι ∪ {vi}n=+n2-|I|, by LemmaE.3, we have
∣∣BucketMed(V; Bt-1, b) - Med(V)k∞ ≤ Bt-1 ≤ B	(16)
Moreover, since |I| > n1++n2 and ∣∣VF(Wt-1) - VF(Wt-1)k∞ ≤ C for any i ∈ I, by Lemma E.1
We have
∣Med(V) - VF(Wt-1)k∞ = ∣Med({μVFi(Wt-1 )}i∈ι ∪ {v0建+n2-1I1) - VF(Wt-1)k∞
≤ μc.	(17)
Combining eqUation 16 and eqUation 17 by triangle ineqUality, We have
∣∣BucketMed(V; Bt-1, b) - μVF(Wt-1)k∞ ≤ μc + 2b.
Noting that V = {μVFi(Wt-1)}i∈ι ∪ {vi}n=+n2-lIl, we apply equation 15 to have
B
∣∣BucketMed({μVFi(Wt-1)}i=1 ∪ {vi,t-1}i=1; Bt-1, b) - μVF(Wt-1)k∞ ≤ μc + 2b,
22
Under review as a conference paper at ICLR 2022
which proves equation 13 for iteration t - 1, and equivalently a3 ≤ μe + B. Collecting our upper
bounds for a1 , a2 , a3 and a4 , we finally have
(14) = aι + a2 + a3 + a4 ≤ μe + μβkgt-ikι + μe + — + kgt-ikι
=2μe + 2b + (I + μβ)kgt-1k1.
Recall the condition that pi ≥ 2μe + B and η ≥ 1 + βμ. We can see that
(14) ≤ ηkgt-1k1 +p1 = Bt.
By induction, it concludes the claim.
□
Given the Claim 1, we continue the proof of Theorem 6.2. By the smoothness assumption (Assump-
tion 1), and given that the parameter domain W is defined to be convex, it is known that
F (wt+1) - F (wt)
β2
≤ hVF(wt), wt+i - Wti + 2l∣wt+i — wtk2
=-hVF (wt), gti + 2 kgtk2
≤ -hvF (Wt), gti + -2- Ilgtk2
=-hVF(Wt), gt - μVF(Wt) + μVF(wt)i + dβkgt - μVF(Wt) + μVF(Wt)k2
=-μ∣VF(Wt)k2 - hVF(Wt),gt - μVF(Wt)i
+ dβ(μ2IIVF(Wt)k2 + kgt - μVF(Wt)k2 + μhVF(Wt),gt - 2μVFIWt))∖
2	(18)
Substituting into μ =焉，we can continue as
(18) = -2dβkVF(Wt)k2 + dβkgt - μVF(Wt)k2
≤ -2dβkVF(Wt)k2 + dβkgt - μVF(Wt)k∞.
By Claim 1, we have
(18) ≤ -2d∣β kVF(Wt)k2 + 与(μe + 2b)
2β	2	2
-2∣βkVF (Wt)k2 +
2
Therefore, rearranging the inequality, and denote
+√2 dBb
we have
dllVF(Wt)k2 ≤ 2β(F(Wt)- F(Wt+i)) + ρ.
Accordingly, for number of iterations T ≥ 1:
1 T1
TzlXdkVF(Wt)k2 ≤
+	t=0
2β
TZI(F(WO) - F(WT +1)) + P
≤ 2βF(wo ) +
≤ T + 1	+ P
Given any e0 > 0 and choosing b > dBβ, equation 19 implies
min 1 kVF(Wt)k2 ≤ '*(WO) + (e + C
0≤t≤T ∣	t 2	T+ 1
□
23
Under review as a conference paper at ICLR 2022
E Auxiliary Lemmas
Lemma E.1. Given a set of real number A = {xi ∈ R}in=1 and an interval [a, b] ⊂ R, if |A ∩
[a, b]| > |A ∩ [a, b]c| then we have Med(A) ∈ [a, b].
Proof. Without loss of generality, we assume A is sorted, i.e, xi ≤ xi+1 for ∀i ∈ [n - 1]. By
the definition of median, there are at least dn2] numbers in A that are less or equal to Med(A).
Therefore, if Med(A) < a, We can see that there are at least d2] numbers in A that are outside
[a, b], which contradicts to the condition of |A ∩ [a, b]| > |A ∩ [a, b]c|. Hence, Med(A) ≥ a.
Similarly, we can show Med(A) ≤ b. That being said, Med(A) ∈ [a, b].	口
Lemma E.2. Given n vectors {vi ∈ Rd}in=1, we have
kMed({vi}in=1)k∞ ≤ Med({kvik∞}in=1).
Proof. Denoting vi (k) as the kth dimension of vi ∈ Rd, we have
kMed({vi}in=1)k∞ =max |Med({vi(k)}in=1)| ≤ max Med({|vi(k)|}in=1).	(20)
k∈[m]	k∈[m]
Denote k? ∈ arg maxk∈[m] Med({|vi(k)|}in=1), and we have
(20) = Med({|vi(k?)|}in=1) ≤ Med({ max |vi(k)|}in=1) = Med({kvik∞}in=1).
k∈[m]
□
Lemma E.3. Given the bucket range B > 0, the number of buckets b, for any n vectors {vi}in=1 in
Rd, if kvi k∞ ≤ B for all i ∈ [n], then we have
∣∣BucketMed({vi}i=ι; B,b) - Med({vi}i=ι)k∞ ≤ B
Proof. Since kvik∞ ≤ B for all i ∈ [n] are within the bucket range, the BucketMed({vi}in=1; B, b)
is the same as Med({vi0}in=1) for some vi0 ∈ Rd satisfying
B
kvi-vik∞ ≤ 五.
Denote
Wl = Vi — —,	Wr = Vi + —,
i	2b, i +2b,
where the ±B is done on every dimension of v. We can see that Wi ≤ Vi ≤ Wr, where the ≤ is
also dimension-wise. Noting that
Med({vi}i=ι) - B = Med({wi}i=ι) ≤ Med({v0}i=ι) ≤ Med({wr}i=ι) = Med({vi}i=ι) + B
we can conclude that
B
∣Med({vi}i=ι) - Med({vi}i=ι)k ≤ 2b∙
Replacing the Med({vi }n=J by BucketMed( {Vi }f=ι; B, b) gives the lemma.	口
F Computation Breakdown and Parameter Sensitivity
In this subsection, we detail a temporal breakdown of a round of federated learning in our approach
and the pairwise comparison median. We show the relationship between server-side computation
time and increasing number of clients. In the bucketing technique, parameters are agreed upon at
the start of the FL protocol to initiate values for the bucketing ranges. We empirically show that these
parameters are not sensitive, and that convergence still holds while varying the order of magnitude
of these parameter values.
24
Under review as a conference paper at ICLR 2022
16000
仑14000
g
ω12000
ζ10000
8000
6000
4000
2000
+ Total Server-side Computation
• Median Computation Only
10
40
70
100
Number of Clients
130
160
JPJndUlOo ɪəʌ-iəs
0
Figure 12: Relationship between number of clients and 1. total server-side computation time after
receiving all client updates, 2. bucketed median computation time.
Computation Breakdown for one round of FL					
Insecure Me- dian	Time (s)	PairWise Compar- ison Median	Time (s)	Bucketed Median	Time (s)
Clients train local model	24	Clients train local model	24	Clients train local model	24
				Clients bucketize model	飞
		Clients compute shares		Clients compute shares	
Server insecurely computes median	6	Servers calls C++ backend to compute median	〜500	Servers calls C++ backend to compute median buckets	〜125
				Server quantizes median buckets	
Total	30	Total	525	Total	160
Table 1: CNNMnist, 8 clients, simulated LAN network
In Table 1, we detail the time in seconds each part of the FL process takes for both the pairwise com-
parison based median method, and our proposed bucketed median method. The bucketed method
had the additional overhead on client and server side of converting to and from a bucketed format,
and this takes around 5 seconds per transformation for a CNNMnist model. The time for both
methods are dominated by the two server secure computation of median. We see that the pairwise
comparison based median FL system increases by a factor of 17.5, and the bucketed median FL
system increases by a factor of 5.5.
In Figure 12, we show the scalability of our protocol when the number of clients range from 10 to 160
in increments of 30. We detail the number of clients on the x-axis and the server-side computation
time (after receiving all client updates) in milliseconds on the y-axis. The server-side computation
consists of the following parts: 1. adding client shares to compute a histogram for each dimension,
2. performing MPC computation of the bucketed median for each dimension, 3. converting the
chosen median buckets to a global model. Steps 2. and 3. are independent of the number of clients.
We test on the MLP model. We see that the time for bucketed median computation stays constant,
yet the time taken for steps 1-3 roughly increases when number of clients increase due to the time
taken for step 1.
In Figure 13, we show the performance of our bucketed protocol with different parameter values.
In our protocol, we determine bucket range as B = {p0 , kwg0 - wg k1 + p1,t }, where p0 is chosen
during the initial epoch, and kwg0 - wg k1 + p1,t is chosen in subsequent epochs. In our convergence
analysis, We use p1 and prove convergence for this choice. In our experiments, We use Pt = pi,
where t is the current epoch count. Both choices empirically result in decreasing training loss per
epoch, albeit at slightly different rates. We note that in our implementation, We set both p0 and p1 as
25
Under review as a conference paper at ICLR 2022
Figure 13: Variable parameter values for bucket range tested on a three client FL system and CNN-
Mnist model.
equivalent values, and use the CNNMnist model. For parameter values less than 1, we see that our
bucketed mechanism results in a similar rate of training loss convergence. For each model, one can
solve for specific p0 , p1 values for the best convergence rate, but as the convergence behaviour does
not strictly depend on the parameter values, we do not focus our attention towards this optimization.
26