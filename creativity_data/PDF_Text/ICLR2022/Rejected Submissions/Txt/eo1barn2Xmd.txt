Under review as a conference paper at ICLR 2022
SLIM-QN: A Stochastic, Light, MOMENTUMIZED
QUASI-NEWTON Optimizer FOR Deep Neural Net-
works
Anonymous authors
paper under double-blind review
Ab stract
We propose SLiM-QN, a light stochastic quasi-Newton optimizer for training
large-scale deep neural networks (DNNs). SLiM-QN addresses two key barriers in
existing second-order methods for large-scale DNNs: 1) the high computational
cost of obtaining the Hessian matrix and its inverse in every iteration (e.g. KFAc);
2) convergence instability due to stochastic training (e.g. L-BFGS). To tackle the
first challenge, SLiM-QN uses the BFGS update rule that directly approximates the
Hessian inverse using past parameters and gradients, without explicitly constructing
the Hessian matrix and then computing its inverse. To achieve stable convergence,
SLiM-QN introduces momentum in Hessian updates together with an adaptive
damping mechanism. We provide rigorous theoretical results on the convergence
of SLiM-QN in a stochastic setting. We also demonstrate that SLiM-QN has much
less compute and memory overhead compared to existing second-order methods.
To better understand the limitations and benefits of SLiM-QN, we evaluate its
performance on various datasets and network architectures. For instance on large
datasets such as imageNet, we show that SLiM-QN achieves near optimal accuracy
1.5× faster when compared with SGD (1.36× faster in wall-clock time) using the
same compute resources. We also show that SLiM-QN can readily be applied to
other contemporary non-convolutional architectures such as Transformers.
1	Introduction
Second-order methods have been extensively investigated in the classical convex optimization
literature. indeed variants such as quasi-Newton methods are known to deliver faster convergence
than gradient descent (GD) and achieve the best overall run time for many tasks (Gao & Goldfarb,
2019; Rodomanov & Nesterov, 2021). However, the Achilles heel of second-order methods that has
impeded their wide adoption for large-scale machine learning problems is their substantial compute
and memory cost, rendering them less favorable than popular stochastic first-order methods such as
SGD (Saad, 1998) and its variants (Duchi et al., 2011; Kingma & Ba, 2014).
The aforementioned barriers stem from computing second-order information (loss Hessian w.r.t
model parameters and its inverse), which typically dominates run-time, especially for large-scale
models such as ResNet (He et al., 2016) and Vision Transformer (Dosovitskiy et al., 2020a), on large
datasets such as imageNet (Deng et al., 2009). To mitigate these issues, approximation methods have
been developed to either approximate Fisher information matrix (expected Hessian matrix under
negative log-likelihood loss) (Martens & Grosse, 2015; Ba et al., 2016; pauloski et al., 2020) or
directly approximate the Hessian inverse (Fletcher, 2013; Mokhtari & Ribeiro, 2015; Moritz et al.,
2016; Gower et al., 2016; Gao & Goldfarb, 2018). A prominent example of the first category is
KFAc (Martens & Grosse, 2015; Ba et al., 2016) which utilizes a gradient conditioner based on
Fisher information and also approximates the matrix as Kronecker product of small sub-matrices,
therefore simplifies matrix inversion. An example from the second category is L-BFGS (Nocedal,
1980; Liu & Nocedal, 1989) which aims to directly approximate the Hessian inverse by iterating over
the past parameter and gradient changes, without explicitly constructing the Hessian matrix itself.
These methods, while to some extent alleviate the compute and memory costs of second order
methods, still experience performance or convergence issues when used for training on large-scale
1
Under review as a conference paper at ICLR 2022
models. For instance, even though KFAC has faster per-iteration convergence compared to SGD, this
gain is often significantly neutralized by the need to run backward passes multiple times to estimate
Fisher information in each mini-batch iteration and then perform costly matrix inversion (Pauloski
et al., 2020). Similarly, stochastic BFGS variants (Mokhtari & Ribeiro, 2015; Moritz et al., 2016;
Gower et al., 2016) have yet to be proven efficient in training large-scale models such as ResNet
in practice. A key challenge is that computationally intensive techniques are required to address
convergence instability issues in this literature (Mokhtari & Ribeiro, 2015; Moritz et al., 2016; Chang
et al., 2019), which unfortunately offset the compute benefits. More recently, authors in (Goldfarb
et al., 2020) propose to lessen the computation burden of matrix inversion in KFAC via BFGS-like
updates with promising results on simple architectures. However the efficacy of this approach is yet
to be demonstrated on practical DNNs and large-scale datasets.
To simultaneously mitigate the computation and instability barriers in second-order methods, we
propose SLIM-QN, a stochastic light stable BFGS-like method that achieves convergence advantages
of second-order methods, while only using modest compute and memory cost compared to other
techniques such as KFAC. SLIM-QN addresses the barriers in second-order methods in two ways.
To reduce compute cost while maintaining fast convergence of second-order methods, SLIM-QN
introduces momentum into the Hessian update. By utilizing momentum on past parameter and
gradient changes, SLIM-QN smooths out the Hessian approximation without incurring costly variance
reduction methods (e.g. a separate large batch size to estimate the Hessian). Furthermore, to ensure
stable convergence, SLIM-QN uses an adaptive damping mechanism to adjust gradient changes so as
to guarantee positive definiteness of the approximated Hessian inverse in stochastic settings. This
adaptive damping scheme effectively restrains abnormal eigenvalues in the Hessian inverse and steers
the optimization trajectory towards desirable directions.
SLIM-QN delivers faster convergence compared to SGD in various models and datasets. The
convergence advantage is even more striking on large-scale models and datasets. Furthermore, due to
its simplicity, SLIM-QN enjoys much better wall-clock convergence gains than other second-order
methods such as KFAC.
In summary, our main contributions are as follows:
1.	We develop SLIM-QN, a stochastic quasi-Newton algorithm targeting large-scale models, that
achieves both fast and stable convergence and low computation complexity, via introducing momen-
tum and damping into the Hessian updates.
2.	We provide a rigorous analysis for SLIM-QN showing that this algorithm converges at a linear rate
for stochastic optimization problems.
3.	We provide complexity analysis that demonstrates that SLIM-QN is lighter than other second-order
methods leading to reductions in overall wall-clock training time.
4.	Finally, we carry out comprehensive evaluations on various models and datasets that show
that SLIM-QN delivers faster convergence compared to SGD, especially for large datasets such as
ImageNet. For instance, to reach near optimal accuracy when training ResNet-50 on ImageNet,
SLIM-QN is 1.5× than SGD (1.36× faster in wall clock time). Furthermore, when training Vision
Transformer models, SLIM-QN also achieves faster convergence and higher accuracy.
2	Preliminaries
In this paper we consider a typical empirical loss minimization problem of the form min L(θ, X) :=
θ
N PN=I '(θ, Xi), where θ denotes the parameters of the model to be optimized, and X = {xi}N=1
are the training data where xi consists of both features and labels. The loss function is typically
minimized through some variant of gradient descent, that uses the local gradients gt = VθL(θt)
directly to update the model parameters via iterates of the form
θt+1 = θt - ηtgt,	(1)
where ηt denotes the step size (learning rate) at iteration t. However, such GD updates are typically
slow especially for ill-conditioned problems (Nesterov, 2003). To speed up the convergence, often
second-order methods are used. In particular, Quasi-Newton (QN) methods find an approximate
Hessian inverse HH-1 to pre-condition the gradient vector and apply the following update to minimize
2
Under review as a conference paper at ICLR 2022
the loss:
θt+ι = θt - ηt ∙ H-1gt.
In stochastic training, the gradient vector is evaluated on a mini-batch input St ⊆ X, namely
gt = VθL(θt, St). If H-1 is the identity matrix, the update above reduces to SGD, whereas if H-1
is a diagonal matrix, it reduces to adaptive training algorithms such as Adagrad (Duchi et al., 2011) or
Adam (Kingma & Ba, 2014). However, to incorporate more curvature information in the optimization
1
process, it often requires approximating H-1 with a full symmetric matrix.
A prime challenge in QN methods is the evaluation of H and in particular its inverse. To address this
challenge the well-known Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm has been proposed.
BFGS approaches the Hessian inverse as a minimization problem:
min	IHT - Hk-* I 2Il , s.t. HT ∙ y- = s-,	HT is symmetric,
where sk = θk - θk-1 denotes the parameter changes, and yk = gk - gk-1 the gradient changes in
two consecutive updates 1. Knowing H-II from the previous update, the current HT is obtained
via:
1	TT 1	T	T
UpdateHessian: Hk-1 = (I - ρkykskT)THk--11(I - ρkykskT) + ρkskskT,
(2)
where Pk = τ1 . Therefore, H-1 is constructed in an iterative manner without explicitly computing
yk sk
the Hessian itself. Given such an update rule, methods such as Greedy BFGS (Rodomanov & Nesterov,
1
2021) show H-1 can converge to the real Hessian at a linear rate.
In real-world problems, θ usually consists of millions of parameters. As a result, it is infeasible to
12
store the whole H- 1 matrix with O(∣θ∣ ) memory cost. To reduce memory footprint and simplify
computation, H-1 in BFGS is stored in the form of a sequence of history vectors {yi } and {si }. By
1
exploiting the HeSSian update formula in equation 2, the matrix-vector product H- ∙ gt necessary to
pre-condition the gradient can be replaced by a sequence of fast vector-vector products as shown in
Algorithm 1. Furthermore, to limit memory and compute costs, a limited-memory version of BFGS,
L-BFGS (Nocedal, 1980) is proposed that only uses the latest M history vectors when approximating
the Hessian inverse.
Algorithm 1 Hessian-Vector in L-BFGS
Input: gt, {yi}iM=1 , {si}iM=1
Output: gt
1: for i = 0,…，M — 1 do
2:	Pi = si ∙ yi
5
-1 12
4
10
3
8
∕
2
3： for i = 0,∙∙∙ ,M - 1 do
4:
5:
6:
αi =
T
sM -i-1 gt
ρM -i-1
gt = gt - αi ∙ yM-i-ι
gt
=H-1 ∙ gt
.H-1
UZ> Loss
• Optimal
→-SGD
-Eχ- Exact NeWton
→-Naive BFGS
-A-BFGS with momentum
7:	for i = 0,…，M - 1 do
8:	βi = yTgt
ρi
9:	gt = gt + (αM-i-ι - βi) ∙ s
-1 -------------------------1---——'-~a~' / ://："，：/'一
-5	-4	-3	-2	-1	0	1	2	3	4	5
Figure 1: Optimization using SGD, Naive BFGS,
BFGS with momentum and Exact Newton.
• I
1
0
外
6
4
2
3 SLIM-QN
While BFGS achieves faster convergence compared to GD in full-batch training, it still suffers
convergence instability in the stochastic setting, especially for large-scale DNNs. Specifically, Naive
BFGS, which simply uses parameter θt and gradients gt at each iteration to calculate s- and y- ,
suffers from severe instability due to stochastic noise introduced by mini-batch training (See the
1 “k" rather than “t" is used in the equation as parameter/gradient used might be different from the one in
equation 1
3
Under review as a conference paper at ICLR 2022
Table 1: Sk and yk for Naive BFGS and for BFGS with momentum for L = 2 ∣∣θk2
	Naive BFGS	BFGS with momentum
Sk	θk+1 - θk	(I- βI)(θk + l - θk)
yk	θk+1 - θk + (nk+1 - nk)	(1 - β2)(θk+1 - θk) + (1 - β2)(nk+1 - nk)
RV	2σ2	(1-β2)∙2σ2
	θk+1-θk	θk+1 -θk
ablation study in Sec. 5.2). To stabilize the optimization, a common solution is to use a separate large
batch of data when estimating sk and yk (Moritz et al., 2016; Chang et al., 2019) in order to reduce
stochastic noise. However, this dramatically increases the computation cost and negates performance
gains in wall-clock time.
In order to reduce variance in HHT without incurring such expensive additional computation as
in (Moritz et al., 2016; Chang et al., 2019), we propose a novel quasi-Newton method, SLIM-QN
which introduces momentum and damping to the Hessian updates. In SLIM-QN, we first obtain the
momentum of θt and gt , from which we subsequently derive parameter and gradient changes sk and
yk. To guarantee positive definiteness of HH-1, we further introduce adaptive damping into the update
of yk . With the momentumized and damped parameter and gradient changes, we can construct a
consistent HHT throughout the whole optimization process.
3.1	Introduce Momentum into the Hessian Update
Inspired by the success of momentum in first-order methods, we demonstrate that the Hessian
update in L-BFGS can be stabilized via momentum, without requiring a large batch size or even full
gradients to reduce stochastic noise. In particular, in this paper we apply momentum Mθt , Mgt to
past parameters θt and gradients gt during mini-batch training as follows:
θ ： Mθt = β1∙Mθ-1 +(1 - βι)θt,
g : Mgt = β2 ∙ Mgt-1 +(1 - β2)gt,
where β1 and β2 are the momentum coefficients for θt and gt respectively.
Assuming that HH-1 is updated for every L mini-batch iterations, Sk and gk are obtained as
sk = Mθ(k+1)L - MθkL, yk = Mg(k+1)L - MgkL .
This simple but effective technique works surprisingly well when gradients are noisy. To intuitively
show improvements of BFGS with momentum over the naive version, we compare the stochastic
optimization of a simple quadratic loss function, L = 2 ∣∣θ∣2. In stochastic training, we write
gradients in each mini-batch iteration as gt = θt + nt , where nt denotes the stochastic noise. For
the sake of simplicity, we model the noise as i.i.d. Gaussian, that is n 〜N(0, σ2).
Table 1 lists the expression for Sk and yk for naive BFGS and for BFGS with momentum in terms of θ
and the stochastic noise. We compare the stability of yk in the two algorithms via their element-wise
relative variance RVj = Vrykj), where yk (j) denotes the jth element of yk. As shown in Table 1,
it is easy to observe that, RVjmom. using momentum is much lower than RVjnaive in naive BFGS.
Hence, given a large momentum, SLIM-QN can significantly suppress noise and obtain a more
consistent yk. With less noise in yk, BFGS with momentum leads to a better approximation of the
real Hessian. Figure 1 visualizes the optimization trajectory for SGD, naive BFGS, BFGS with
momentum and the exact Newton method. With the same initialization, BFGS with momentum is as
fast as the exact Newton method, and almost twice faster than SGD. On the other hand, naive BFGS
has difficulties finding the right optimization path due to the noise in yk.
3.2	Guarantee Positive Definiteness of the Hessian via Damping
Even though adding momentum stabilizes the Hessian inverse, it cannot guarantee that the ap-
proximated Hessian is always positive definite, especially in practical stochastic and non-convex
optimization. As analysed in Sec 5.2, negative or very small positive values in the Hessian spectrum
4
Under review as a conference paper at ICLR 2022
are harmful, and they immediately derail the optimization. To effectively prevent radical changes in
the Hessian, SLIM-QN further introduces an adaptive damping mechanism to the Hessian update.
Positive definiteness of the Hessian hinges on stable and smooth gradient change vectors {yi},
especially in non-convex and stochastic settings. Hence, we choose to dampen yi by
y^i = T ∙ yi + (1 - τ) ∙ Si,
where ryi is the damped version of yi, and T is calculated as
(min( 1TqL,τo)	μ ≤ σL < 1
T = min(σμ-,τo) μ ≥ °h > 1
I	τo	otherwise
(3)
(4)
where μ = STy, ol and qh are the lower and upper thresholds for restraining eigenvalues in H-1
and 0 < τ0 < 1 is a constant coefficient.
The above damping scheme prevents sudden changes of siTyi, and guarantees the smoothness of
H -1 . Equivalently, equation 3 can be considered as scaling the undamped H and adaptively shifting
its spectrum by a positive constant: T ∙ H + (1 - T) ∙ I. As a result, the eigenvalues of H are well
controlled.
3.3	The Overall Description
With momentum and damping introduced above, in this section we present the complete SLIM-
QN algorithm. As shown in Algorithm 2, for each mini-batch iteration, we use Mθt and Mgt to
accumulate momentum of parameters and gradients. For every L iterations, we compute sk and
yk, apply damping, and then update the Hessian approximation. Note that in UpdateHessian the
inverse Hessian is never explicitly computed, instead we only compute and store the history vectors
Sk and τ^k that are necessary to perform the pre-conditioning step. Since at the first 2L iterations,
H-1 is not ready yet, we use SGD to conduct a warmup training. After 2L iterations, we use H-1 to
first pre-condition gradients gt and then apply updates to θ. Unlike KFAC, SLIM-QN is compatible
to various regularizers such as L2 and gradient regularization(Smith et al., 2021), as long as we
can derive gradients from them. Finally, we add momentum to the parameter update ∆θt after
pre-conditioning following the same procedure as in SGD with momentum (which is omitted in
Algorithm 2).
Algorithm 2 SLIM-QN algorithm
1	for t = 1, •…，max_iter do
2	:	Randomly choose mini-batch input St ∈ X
3	:	Compute gradients gt given St (Forward/Backward)
4	Add weight decay: gt = gt + wd ∙ θt
5	Compute momentum on θ: Me, = βι ∙ Mej + (1 - βι) ∙ θt	. Mθo = θ0
6	Compute momentum on g: Mgt =	β2	∙ Mgt-ι	+ (1 -	β2)	∙	gt	. Mg°	=	go
7	:	if t ≤ 2L then
8	Warmup: θt+ι = θt - ηt ∙ gt
9	:	else
10	Pre-condition: ∆θt = H-I ∙ gt	. Algorithm 1
11	Update: θt+ι = θt - ηt ∙ ∆θt
12	:	if t%L == 0 and t > L then
13	:	k=k+1
14	:	S: Sk = Met - Met-L
15	:	y: yk = Mgt - Mgt-L
16	Damping: τ^k = T ∙ yk + (1 - T) ∙ Sk	. τ from equation 4
17	11 H 1: Hk = UpdateHessian(Hk-I, Sk, yk)	. equation 2, not explicitly computed
5
Under review as a conference paper at ICLR 2022
4 Theoretical Guarantees for SLIM-QN
In this section, we first present convergence guarantees for SLIM-QN, and then discuss the compute
and memory costs for SGD, KFAC, and SLIM-QN.
4.1	Convergence Guarantees
Following the framework of quasi-Newton method in Wang et al. (2017) in stochastic optimization,
we prove that SLIM-QN converges to the optimum at a linear rate, under proper assumptions as
follows:
AS 1. L(θ) is λ-PL in that it satisfies Polyak-Lojasiewicz (PL) condition for a constant λ > 0:
kVL(θ)k2 ≥ λL(θ).
AS 2. 'i(θ) is A-Smoothfor 1 ≤ i ≤ N, A > 0: ∀θι, θ2, 'i(θ2) ≤ 'i(θι) +〈▽'"/ θ2 — θιi +
λ kθ2 - θ1k2.
AS 3. For every sequence θι, θ2,…such that limt→∞ L(θt) = 0, then for all 1 ≤ i ≤ N,
limt→∞ 'i(θt) = 0
We note that compared to typical strong convexity assumptions the PL condition in AS 1 (Polyak,
1963) applies to much broader settings including when the loss is nonconvex as it only requires lower
bounded variance in gradients, rather than strict positive definiteness required for the Hessian with
strong convexity. AS 3 assumes that the global minima of the summands `i are the same as the global
minima of their sum. This is in line with what has been observed in over-parameterized deep learning
models (Ma et al., 2018).
Before We state our main result We require several auxiliary lemmas. First Lemma 1 states that ST ∙ y^i
always lies in [σL,σH] ∙ STs%. With Lemma 1 in place, in Lemma 2 we bound the approximation
of the Hessian H-I at kth Hessian update. Lemma 3 further establishes smoothness on L(θ) given
each `i is smooth. While Lemma 4 further bounds gradient variance in mini-batch training.
Lemma 1. Given damping scheme in equation 3, if we choose τ according to equation 4, then
σL ≤ STy ≤ σH.
1
Lemma 2. Given Lemma 1, at the k-th Hessian update, Hk-1 during the optimization is bounded by
ξI W H-1 W ΞI, where ξ =
for Sry in Lemma 1.
(M + 1) σ-, and ξ = /. Ql and oh is the lower and upper bound
Lemma 3. Assume AS 2 holds, then loss function L(θ) is at least A-smooth.
Lemma 4. Assume AS 2-3 holds, with Lemma 3, at iteration t with mini-batch input St, where each
sample is randomly sampled from X with replacement, gradient VL(θt; St) satisfies
Est [kVL(θt; St)『]≤ 2A ∙L(θt)
1
With Hk-1 and gradient variance bounded, we can derive the following convergence theorem.
Theorem 1. Assume AS 1-3 hold at each iteration t of SLIM-QN with mini-batch input St where
each sample is randomly sampled from X with replacement, then the expectation of L(θt) satisfies
ESt[L(θt)] ≤ αt-1ESt-1 [L(θt-1)],
where αt-1 = 1 - ηt-1λξ + ηt2-1A2Ξ2.
Proofs for the lemmas and theorem are provided in the appendix.
Remark 1. By choosing ηt-1 such that αt-1 < 1, SLIM-QN converges at a linear rate.
Remark 2. We note that Theorem 1 also applies to convex settings, as strong convexity implies
kVL(θ)k2 is lower bounded by L(θ) for an appropriate λ > 0 and hence AS 1 holds.
6
Under review as a conference paper at ICLR 2022
Table 2: Computations and Memory in SGD, KFAC, and SLIM-QN
	SGD	KFAC	sL-BFGS	SLIM-QN
Computation				
Fwd&Bwd	bCfb	αιkθk+ γbCfb + L P(d3 +(喇)3)	ɑ2 ∣θ∣ +2bCfb + 1 bHCfb	bCfb + α3 ∣θ∣
Opt	α kθk	“o ∣∣θ∣∣ + 2 P(di + 暨)l&l	α0 ∣θ∣ +2M∣θ∣	α0 ∣θ∣ +2M∣θ∣
Memory				
Fwd&Bwd	bMfb	bMfb + βι ∣∣θk	bMfb + β2 ∣θ∣ + 1 bHMft	bMfb + β3 ∣θ∣
Opt	βo kθk	βo ∣∣θk+2P(d2 + (曙)2)	β0 ∣θ∣ +2M∣θ∣	β0 ∣θ∣ +2M∣θ∣
• di:	input dim in layer i. b: batch size. bH: batch size for the Hessian approx. kθik: #params in layer i.			
4.2 Compute and Memory Cost
As mentioned before, SLIM-QN aims to reduce the complexity of approximating the Hessian. In this
section, we summarize the compute and memory cost of SGD, KFAC, stochastic L-BFGS (sL-BFGS)
variants(Chang et al., 2019) and SLIM-QN, and demonstrate the cost advantage of SLIM-QN.
Given a model with parameter θ, we use Cfb and Mfb to represent the compute and memory cost
of a forward/backward (Fwd/Bwd) pass with a batch size of b = 1. Furthermore, Copt denotes the
compute cost of model updates (Opt) which consists of gradient reduction, computing the update ∆θ,
and applying it to θ.
Table 2 summarizes the compute and memory cost of SGD, KFAC, sL-BFGS and SLIM-QN.
Compared to SGD, during the forward and backward passes, SLIM-QN needs to additionally
compute Mθ, Mg, for which the complexity increases linearly with model size (α2 kθk). The main
extra compute SLIM-QN introduces is the Hessian-vector product, in which we need to iterate over
{si}iM=1 and {yi}iM=1, as shown in Algorithm 1. The complexity increases linearly with the number of
stored history vectors and model size (2M kθk). While, compared to O(bCfb) complexity in forward
and backward pass, such operations add relatively marginal cost.
As a comparison, KFAC though only approximates diagonal blocks of the Fisher matrix, it still adds
significant additional computations through 1) multiple backward passes to update factors (γbCfb
with Y ≥ 1), 2) matrix inversion (P(d3 + (kdik)3)) for every L iterations, and 3) Matrix-vector
products (2 P(di + kθk) ∣∣θi∣∣). If the Fisher matrix is updated more frequently (that is for small L),
then the amortized cost for matrix inversion is even more striking. On the other hand, sL-BFGS also
resorts to computation-intensive operations including full-batch gradients and a separate large batch
to estimate the Hessian, which respectively adds amortized costs of O(bCfb) and =bHCfb.
As for memory usage, compared to SGD, SLIM-QN mainly needs O(2M kθk) to store history
vectors {si}iM=1 and {yi}iM=1. sL-BFGS needs the same storage for si , yi, and amortized cost of
O(LbHMfb) for additional backward passes. While KFAC needs O(2 P(d2 + (kθik)2)) to store
sub-matrices and their inverse, where the actual memory footprint hinges on model architectures. In
practice, M is set to be 10 〜20, which ensures that memory usage is manageable in SLIM-QN.
5	Empirical Analyses
We conduct various experiments on computer vision (CV) problems, where SGD has been widely used.
Methods like Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al., 2011) greatly under-perform
SGD (Defazio & Jelassi, 2021). Two metrics are used to evaluate the performance: iteration-wise
convergence and wall-clock convergence. The iteration-wise metric shows the pure convergence
promises of the optimizer; while the wall-clock convergence further captures the impacts of computa-
tion complexity on the run-time. Furthermore, we also conduct an ablation study that investigates
how the components in SLIM-QN (momentum and damping) affect the optimization process.
The current implementation in PyTorch (Paszke et al., 2019) supports various DNN models in a
multi-GPU system. During training, after gradients are synchronized across GPUs, we keep updating
the momentum, Mθt and Mgt on each GPU. As a result, each GPU stores a copy of the Hessian
7
Under review as a conference paper at ICLR 2022
Figure 2: Training loss (left) and validation accuracy (right) of SLIM-QN, KFAC and SGD on
ImageNet using ResNet-50 model. The model trained with SLIM-QN benefits from faster early-stage
convergence and achieve comparable generalization performance as SGD. We plot the mean and
standard error over 3 runs with different random seeds.
inverse and locally performs gradient conditioning without communicating across GPUs. We test
models: ResNet18, ResNet50, Vision Transformer on datasets: CIFAR-10(Krizhevsky et al., 2009)
and ImageNet(Deng et al., 2009). We also run SGD and KFAC as two baselines for comparison.
5.1	Experiments on ImageNet Classification
ImageNet classification has been the gold standard for evaluating performance of different optimiza-
tion algorithms for CV models. Compared to CIFAR-10, ImageNet consists of much more training
and test images (〜1.2M training and 〜50K test images), categorized into 1000 classes. Therefore,
convergence on ImageNet can better reveal the optimizer’s promises in practical problems.
During data pre-processing, we resize images to 256 × 256, and randomly crop to 224 × 224, and
then randomly flip each image. Each image is normalized using pre-computed mean and variance.
5.1.1	RESNET-50
Figure 2 shows iteration-wise (Top) and wall-clock (Bottom) convergence on ResNet-50 using SGD,
KFAC and SLIM-QN. Detailed hyper-parameter settings are provided in the appendix. SLIM-QN
enjoys very fast per-iteration convergence, and reaches near optimal accuracy 1.5× faster than SGD,
and even 2× faster in the early-stages. Furthermore, it also generalizes well on the validation set, and
finally reaches comparable validation accuracy to SGD.
The benefit of SLIM-QN is even more striking in terms of wall-clock time. Due to light compute
costs, it is 1.75 × /1.36× faster in the early and late stages compared to SGD. Whereas in KFAC, the
wall-clock performance is significantly neutralized by its additional compute costs.
5.1.2	Vision Transformer
As a step towards understanding the efficacy of second-order optimizers on contemporary Transformer-
based CV models, we perform experiments on a small (10M parameters) Vision Transformer (ViT)
(Dosovitskiy et al., 2020b) using SLIM-QN on ImageNet. Details on hyperparameters, model
architecture and experiments on further datasets are deferred to the Appendix.
As shown in Fig. 3, SLIM-QN benefits from faster early-stage convergence compared to the SGD,
which is consistent with our findings for ResNet. Furthermore, we observe that SLIM-QN finds a
solution with good generalization achieving a final validation accuracy slightly higher than SGD.
5.2	Ablation Study: The Effects of Momentum and Damping
In this section, we give more insight into the effects of momentum and damping used in SLIM-QN.
To this end, we ablate two critical components in SLIM-QN: momentum and damping in the Hessian
8
Under review as a conference paper at ICLR 2022
Figure 3: Training loss (left) and validation accuracy (right) of SLIM-QN and SGD on ImageNet
using a Vision Transformer model. The model trained with SLIM-QN benefits from faster early-stage
convergence and achieve better generalization performance compared to SGD. We plot the mean and
standard error over 3 runs with different random seeds.
5040302010
(F) .ɔɔ G uθ⅛p=E>
Figure 4: Ablation analysis for SLIM-QN on ResNet-18/CIFAR-10 (batch size: 256).
approximation, and then use the ablated version to train ResNet-18 on CIFAR-10. We focus on
CIFAR-10 since we observed more convergence instability on this dataset compared to others.
Figure 4 shows convergence using the ablated SLIM-QN with only momentum (black), with only
damping (purple), and with no momentum or damping (red). Due to stochastic noise, the ablated
version of SLIM-QN without momentum or damping diverges easily in the early stages. With
momentum (black), the whole optimization is significantly stabilized. However, it still fails to
converge when there is an abrupt change in the loss landscape (for example, when learning rate
decays). With damping (purple), the Hessian approximation is effectively restrained, especially when
such sudden changes in the loss landscape happen. It is interesting to observe that while damping
prevents divergence, the whole training is still largely affected by stochastic noise. Notable fluctuation
in the loss and accuracy is commonly observed during training. As a comparison, the complete
SLIM-QN (blue) effectively addresses these issues achieving much more stable convergence.
6	Conclusion
In this paper, we propose SLIM-QN, a quasi-Newton method that simultaneously mitigates computa-
tion and convergence instability barriers in second-order methods. SLIM-QN introduces momentum
and damping into the Hessian update, which obviates the need for estimating the Hessian with high
costs. Empirical analyses on CV models, such as ResNet-50 and Vision Transformer show that
SLIM-QN achieves faster convergence in the early stages, and reaches comparable accuracy to SGD.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
Implementation of SLIM-QN in a multi-GPU platform is described at the beginning of Sec 5. A
copy of code is included in supplementary materials. Appendix B lists all hyperparameters for
obtaining results on ImageNet using ResNet-50 and ViT models. Moreover, Appendix C presents
more results on CIFAR-10 using ResNet-18 and ViT models. We also describe some tips of tuning
hyperparameters in Appendix F.
For theoretical results, proofs of all lemmas and theorems are provided in Appendix A.
References
Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kronecker-
factored approximations. 2016.
Daqing Chang, Shiliang Sun, and Changshui Zhang. An accelerated linearly convergent stochastic
l-bfgs algorithm. IEEE transactions on neural networks and learning systems, 30(11):3338-3346,
2019.
Aaron Defazio and Samy Jelassi. Adaptivity without compromise: a momentumized, adaptive, dual
averaged gradient method for stochastic optimization. arXiv preprint arXiv:2101.11075, 2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020a.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020b.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 2013.
Wenbo Gao and Donald Goldfarb. Block bfgs methods. SIAM Journal on Optimization, 28(2):
1205-1231, 2018.
Wenbo Gao and Donald Goldfarb. Quasi-newton methods: superlinear convergence without line
searches for self-concordant functions. Optimization Methods and Software, 34(1):194-217, 2019.
Donald Goldfarb, Yi Ren, and Achraf Bahamou. Practical quasi-newton methods for training deep
neural networks. arXiv preprint arXiv:2006.08877, 2020.
Robert Gower, Donald Goldfarb, and Peter Richtdrik. Stochastic block bfgs: Squeezing more
curvature out of data. In International Conference on Machine Learning, pp. 1869-1878. PMLR,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1):503-528,1989.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. In International Conference on
Machine Learning, pp. 3325-3334. PMLR, 2018.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417. PMLR, 2015.
Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. The
Journal of Machine Learning Research, 16(1):3151-3181, 2015.
Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic l-bfgs
algorithm. In Artificial Intelligence and Statistics, pp. 249-258. PMLR, 2016.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Jorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computation,
35(151):773-782, 1980.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
J. Gregory Pauloski, Zhao Zhang, Lei Huang, Weijia Xu, and Ian T. Foster. Convolutional Neural
Network Training with Distributed K-FAC. In Proceedings of the International Conference for
High Performance Computing, Networking, Storage and Analysis, SC ’20. IEEE Press, 2020. ISBN
9781728199986. doi: 10.5555/3433701.3433826.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal vychislitel’noi
matematiki i matematicheskoi fiziki, 3(4):643-653, 1963.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. 2018.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations
toward training trillion parameter models. In SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.
Anton Rodomanov and Yurii Nesterov. Greedy quasi-newton methods with explicit superlinear
convergence. SIAM Journal on Optimization, 31(1):785-811, 2021.
David Saad. Online algorithms and stochastic approximations. Online Learning, 5:6-3, 1998.
Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit
regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021.
Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927-956, 2017.
11
Under review as a conference paper at ICLR 2022
A Proofs of Lemmas and Theorems
This appendix are organized as follows:
1.	Section A.1-A.4 presents the proof of Lemma 1-4.
2.	Section A.5 presents the proof of Theorem 1.
A.1 Proof of Lemma 1
Proof. According to equation 3, STi^i = ST(Tyi + (1 一T)si) = (μτ + 1 一 T)sf Si, where μ = STyi
si si
For μ ≤ σL, two cases need to be considered: 1) T = t0;2) T
1-σL
1-μ .
If T = to, then 1--σL ≥ t0, and μr +1 — T ≥ σL If T = I--L, then μr +1 — T = σL
Therefore, when μ ≤ ql, STy^i ≥ olSTSi
For ql < μ < qh :
We can write μτ +1-τ = μτ0+1 — to. Itis easy to show that μτ0+1-τ0 —ql ≥ (1-ql)(1-to) > 0
andμτo + 1 — to — qh ≤ (1 — qh)(1 — To) < 0.
Therefore, when QL <μ < QH, QLSTSi < STτ^i < QHSTSi.
For μ ≥ QH, similarly two cases might arise: 1) T = τ0; 2) T
σH — 1
μ-1 .
If τ = to, then σH--1 ≥ to, and μτ + 1 - T ≤ QH. If T = σH--1, then μτ + 1 — T = QH.
Therefore, when μ ≥ QH, ST i^i ≤ QH ST Si
In summary, QL ≤ STy ≤ QH.
□
A.2 Proof of Lemma 2
Proof. Lower bound: HT is initialized as H—1 = ^y ∙ I. According to Lemma 1, there exists
Ho(θ) W QHI such that y° = Ho ∙ so.
TherefCr s ST y I - ST HOsO	I—	(STH1/2)(HI/2S0) IA 1 I
Therefore, yTy T = s0' H0∙H0s0 T = (ST H0∕2)∙Ho∙(H0∕2so) '1 ~ 福 I .
Then for k ≥ 1, assuming Hk-I - σ⅛^ I hold, based on equation 2, H—1 = (I -Pk ryk ST )t Hk-II(I -
T
Pkyksk ) + S⅛.
Because (I — PkrykST)tHk-II (I - Pkɪ/kST) is positive definite, we can bound H—1 as: H—1 -
Sk ST = SkST	— ɪ I
STy^k	ST ∙Hk ∙Sk — σH
Therefore, lower bound of HJ, ξ =	.
Upper bound: Since H°(θ) - ql, we can get STyO ∙ I = (SOH0, )(H0[SO) ∙ I Y ɪ.
”, 一 L	5 yT y0	(ST Hl/2)-H0-(Hl/2S0)	一 σL
Similarly, for k ≥ 1, we assume H—11 Y kσ1- hold.
For the first part in equation 2, let Q = (I — PkyksT). Then for ∀ x = 0, XT ∙ Qt H—⅛ ∙ X =
(Qx)T ∙ H-11 ∙ (Qx) ≤ σkLXt ∙ (QtQ) ∙ X.
Let P = STyT yTyT - YTy产,then QT Q = I + P.
12
Under review as a conference paper at ICLR 2022
Because P is rank-1 matrix with eigenvalue -1 and eigenvector ryk, We have XT ∙ QT H-11Q ∙ X ≤
σkL xT ∙(I + P) ∙ x ≤ σkL xT x
TT
For the second part in equation 2, we can directly get STy^ = ST H：.Sk W -1 i.
Therefore, XT ∙ H-1 ∙ X ≤ ɪ xτ X + ɪ xτ X = k+1 xτ X
,	k	σL	σL	σL
In SLIM-QN, k is at most M, which is the length of history vector, therefore Ξ = (M + 1)言
In summary, -HI W H-1 W (M + 1) -LI
□
A.3 Proof of Lemma 3
Proof. Given AS 2 hold, we have
kV'i(θ1) -V'i(θ2)k ≤ A kθ1- θ2k.
For L, we have
1N
VL(θ1) - VL(θ2) = NN N V'i(θ1) - V'i(θ2)
Then,
1
kVL(θ1) -VL(θ2)k = NN ]TV'i(θ1) -V'i(θ2)
i=1
TI 1 N
≤ N E iv`i(θi)- v`i(θz)k
1N
≤ N EA kθ1 - θ2k
= A kθ1 - θ2k
TI" indicates triangle inequality. Therefore, L is at least A-smooth.
□
A.4 Proof of Lemma 4
Proof Est [kVL(θt; St)k2] = Est [DbV P：=1 'i(θt), ɪ V P：=1 6(%)〉]
Expand summation and regroup,
Est [kVL(θt; St)『]=ESt 层 Pb=1 kV'i(θt)k1 2 + b2 Pb=1 Pb=1,=i hV'i(θt), V'j(θt)i]
Take expectation on each sample,
Es, [kVL(θt; St)k2] = 52 Pb=1 Exi [kV'i(θt)k2] + 1 Pb=1 Pb=1,=i Ex. [hV'i(θt), V'j(θt)i]
Because Xi and Xj are independent, the second part can be simplified as,
Est [kVL(θt; St)『]=52 Pb=1 Exi [kV'i(θt)k2] + s2 Pb=1 Pb=1,=i kVL(θt)k2
With further simplification, we get
Est[kVL(θt; St)k2] = b2 Pb=1 Exi[kV'i(θt)k2] + b-1 kVL(θt)k2
13
Under review as a conference paper at ICLR 2022
Given AS 2 and Lemma 3, we have
kv`i(θt)k2 ≤ 2Λ ∙ 'i(θt) and ∣∣VL(θt)k2 ≤ 2Λ ∙ L(θt)
Therefore,
1b	b1
Est [kVL(θt; St)『]≤ 庐 EExi [2Λ'i(θt)] + -- 2ΛL(θt)	(5)
b i=1	b
1	- - 1
=-2λlw∙ +......—— 2λl(θ∙	(6)
=2Λ ∙L(θt)	⑺
□
A.5 Proof of Theorem 1
Proof. Given AS 2 and Lemma 3, L(θt) can be bounded by L(θt) ≤ L(θt-1) + VL(θt-1)T (θt -
θt-ι) + 2 kθt- θt-1k2 for Vet-1, θt
In SLIM-QN, θt = θt-ι 一 ηt-ιHH- 1VL(θt-1; St-ι). Therefore, We can upper bound L(θt) as：
L(θt) ≤L(θt-ι) - ηt-ι∙NL(θt-ι)H-1VL(θt-ι; St-ι)
+ n2-i Ml。-1VL(θt-i; St-i)∣∣2
≤L(θt-ι) - ηt-1∙VL(θt-1)TH-1VL(θt-ι; St-ι)
ΛΞ2
+ nt-ι~2 kVL(et-i;St-I)Il
2
1
Since Hk-1 is independent With St-1, We take expectation W.r.t St-1 and St,
Est [L(θt)∣St-ι] ≤L(θt-ι) - ηt-ι∙NL(θt-ι)H-IESt-JVL(θt-ι; St-ι)]
ΛΞ2
+ n2-i-2-Est-ι [kVL(θt-i; St-1 )『]
=L(θt-ι) - ηt-ιNL(θt-ι)H-1VL(θt-ι)
-Ξ2
+ n2-i-2-Est-ι [kVL(θt-i; St-i)k2]
According AS 1, We have
L(θt-ι) ≤ λ ∣VL(θt-ι)k2
According to Lemma 4, We have
Est-ι[kVL(θt-ι;St-ι)k2] ≤ 2ΛL(θt-ι)
Therefore, E& [L(θt)∣St-ι] ≤ L(θt-ι) - ηt-1λξL(θt-1) + η2-ι-2Ξ2L(θt-ι).
After simply regrouping, We can get
Est [L(θt)∣St-ι] ≤ (1 - ηt-ιλξ + η2-ι-2Ξ2)[L(θt-ι]
Apply total expectation rule W.r.t St, We have
Est[L(θt)] ≤ (1 -ηt-1λξ+ηt2-12Ξ2)Est-1[L(θt-1)]
□
14
Under review as a conference paper at ICLR 2022
B Settings for Model Training on ImageNet
B.1	ResNet-50
We train ResNet-50 on ImageNet in a multi-GPU platform, which has 8 Nvidia Quadro RTX 5000
GPUs. PyTorch (≥1.8) and the Distributed Data Parallel (DDP) communication package is used
during training. Table 3 lists hyperparameters used in SGD, KFAC, and SLIM-QN. Initial learning
rate is 0.1, decaying by a factor of 10 at 30, 60, 90th epoch. We use a learning warmup for in the first 2
epochs. Batch size is 256. In SLIM-QN, we use a smaller weight decay (wd) as SLIM-QN considers
weight decay when conditioning gradients. Therefore a smaller weight decay can achieve as strong
regularization as SGD with wd = 0.0005. For the Hessian update frequency (L), in SLIM-QN we
update the Hessian for every 30 mini-batch iterations; while for KFAC, considering the compute cost,
we update the Hessian for every 50 iterations. Lower and upper threshold for restraining eigenvalues
in the Hessian (σL, σH) is set to be (0.01, 1) in all experiments. Initial damping (1 - τ0) is 0.05,
then adapted during training according to equation 4. We run 5 random seeds for each optimizer.
Table 3: Hyperparameters for SGD, KFAC, SLIM-QN on ResNet-50/ImageNet
Optimizer	lr	momentum	wd	damping ∣ β1∕β2		L	M
SGD	0.1	0.9	0.0005	-	-	-	-
KFAC	0.1	0.9	0.0002	0.001	-	50	-
SLIM-QN	0.1	0.9	0.0002	0.05	0.9/0.9	30	10
β1∕β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (s, y)
B.2	ViT
We use a small Vision Transformer model with 6 layers, 8 attention heads, a patch size of 16, and
both hidden and MLP dimension of 512 for a total of about 10M parameters. We train for 90 epochs
with linear learning rate warmup in the first 5 epochs, and decay the learning rate at 80 epochs for
SLIM-QN. For SGD, we train for 100 epochs, decaying the learning rate at 30, 60, 90 epochs. A
batch size of 1024 is used for both algorithms. We perform 3 runs with different random seeds. Table
4 shows the selected hyperparameters.
Table 4: Hyperparameters for SGD and SLIM-QN on ViT/ImageNet
Optimizer	lr	momentum	Wd	damping	β1∕β2	L	M
SGD	0.1	0.9	0.0001	-	-	-	-
SLIM-QN	0.1	0.9	0.0	0.01	0.99/0.99	100	20
βι /β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (s, y)							
C Experiments on CIFAR- 1 0
C.1 ResNet- 1 8
Hyperparameter is shown as in Table 5. Initial learning rate is 0.1, decaying by a factor of 10 at 150th
epoch. For both SGD and SLIM-QN we used a linear learning rate warmup for 5 epochs. Batch
size is 256 for both SGD and SLIM-QN. Figure 5 shows convergence on ResNet-18 using SGD and
SLIM-QN. On small dataset CIFAR-10, SGD and SLIM-QN both deliver fast convergence at early
stages, while SLIM-QN is slightly better. Moreover, SLIM-QN achieves more faster convergence
and high validation accuracy at later stages.
C.2 ViT
We use the same ViT model as in the ImageNet experiment, but with a patch size of 16. Table 6
shows the hyperparameters used in the CIFAR-10 experiments. For both SGD and SLIM-QN we
used a linear learning rate warmup for 5 epochs. We perform 3 runs with different random seeds.
15
Under review as a conference paper at ICLR 2022
Table 5:	Hyperparameters for SGD, SLIM-QN on ResNet-18/CIFAR-10
Optimizer	I lr	I momentum	I Wd	I damping ∣ β1∕β2		I L	I M
SGD SLIM-QN	0.1 0.1	0.9 0.9	0.0005 0.0005	- 0.05	- 0.9/0.9	- 100	- 10
βι/β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (S, y)
Figure 5: Convergence on ResNet-18/CIFAR-10 using SGD and SLIM-QN.
90807060
y)3 UOu
We use a batch size of 1024 and train for 90 epochs and decay the learning rate by a factor of 10 at
100 epochs for SLIM-QN. We train for 150 epochs and decay the learning rate at 140 epochs for SGD.
Experimental results on CIFAR-10 using Vision Transformer are depicted in Fig. 6. We observe that
SLIM-QN converges to a solution with better generalization than SGD and in overall less iterations.
On small datasets such as CIFAR-10 we do not observe significant early-stage speedup on ViT, which
is consistent with ResNet-18/CIFAR-10 experiments. Moreover, ViT is better suited for large-scale
vision datasets due to the weaker implicit bias resulting from the non-convolutional architecture.
Figure 6: Convergence on ViT/CIFAR-10 using SGD and SLIM-QN.
Ooooooo
8 7 6 5 4 3 2
≡) § Uou
Table 6:	Hyperparameters for SGD, SLIM-QN on ViT/CIFAR-10 with batch size 1024
Optimizer ∣	lr	I momentum	I Wd	I damping	I βι /β I	L I	M
SGD	0.1	0.9	0.0001	-	-	-	-
SLIM-QN	0.025	0.9	0.0001	0.01	0.99/0.99	100	10
βι/β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (S, y)
D	Stochastic Training using the Classical L-BFGS
In this section, we demonstrate that the classical L-BFGS suffers convergence instability in stochastic
training, even using large batch sizes. We train ResNet-18 on CIFAR-10, and vary batch size from 64
to 2048. Learning rate decays from 0.1 to 0.001 in 100 epochs. Weight decay is 0.0005. The Hessian
approximation is updated using the L-BFGS formula for every 50 iterations, with at most 10 history
vectors.
Fig. 7 shows training accuracy w.r.t iterations (log scale). We can observe that L-BFGS fails to
converge under various batch sizes. Training using large batches though is a little more stable than
small ones at the beginning, still diverges due to stochastic noises or sudden changes in loss landscape.
Therefore, large batch only is not a sufficient solution to address convergence instability in the
classical L-BFGS.
16
Under review as a conference paper at ICLR 2022
Figure 7: Convergence on ResNet-19/CIFAR-10 using the classical BFGS.
Figure 8: Block-wise SLIM-QN for distributed systems. Models are divided into blocks, which are
then optimized by SLIM-QN in multiple nodes.
E SLIM-QN in Distributed Systems
As mentioned in Sec. 4.2, SLIM-QN requires O(2M kθk) storage to store required history statistics
sk and yk . When training models using data parallelism, each node further needs to store the whole
copy of these history vectors. Such high memory footprints make it difficult to be used to train very
large models such as BERT and GPT (Devlin et al., 2018; Radford et al., 2018). To address such a
limitation, we propose a block-wise SLIM-QN which is much more memory-efficient in distributed
systems.
As shown in Fig. 8, for a neural network, we divide model parameters into multiple blocks, where
each block might consist of one or more layers. During training, these blocks are optimized in parallel
using independent SLIM-QN optimizers. In a distributed system, each compute node can conduct
optimization on one or more blocks, depending on its capabilities. Furthermore, the model can be
divided in a way such that each node can store the required statistics sk and yk for at least one block.
Combining with ZeRO data parallelism design (Rajbhandari et al., 2020), block-wise SLIM-QN are
capable of training very large models as long as there are enough compute nodes.
17
Under review as a conference paper at ICLR 2022
Figure 9: Convergence on ResNet-18/CIFAR-10 using SGD, SLIM-QN, and block-wise SLIM-QN
Epochs
E.1 Convergence Guarantees
In this section, we prove that block-wise SLIM-QN also converges in a linear rate given assumption
in Sec. 4.1. Furthermore, the convergence property is guaranteed in any arbitrary way of dividing
models.
Theorem 2. Assume AS 1-3 hold at each iteration t of block-wise SLIM-QN with mini-batch input
St, and the k -th Hessian update for block i(i = 1, ∙ ∙ ∙ ,p), Bk (B = H-1) during the optimization
is bounded by ξiI Bki ΞiI, then the expectation of L(θt) satisfies
ESt[L(θt)] ≤αt-1ESt-1[L(θt-1)],
where αt-1 = 1 - ηt-1λξ + ηt2-1Λ2Ξ2, ξ = minξi, Ξ = maxΞi.
Proof. The bound of Bki for block i is guaranteed by SLIM-QN. Then according to the proof in
1
Theorem 1, if we can prove H- = dιag(B1, B2, ∙ ∙ ∙ , Bp) is also bounded, then We can show
block-wise SLIM-QN converges in a linear rate.
1
For any arbitrary vector X = 0, XT ∙ H- ∙ X can be written as
(XIT ∙ Bi,…，XpT ∙ Bp) ∙ (x1T,…，XPT)t = Pp=I XiT ∙ Bi ∙ Xi,
where Xi is a sub-vector corresponding to block i.
Let ξ =
min ξi and Ξ
= max Ξi , therefore, ξ ≤ XT
. H-1∙ X ≤ Ξ.
Following the same proof as in Theorem 1, block-wise SLIM-QN also converges in a linear rate. □
E.2 Empirical Analyses
We evaluate block-wise SLIM-QN on ResNet-18/CIFAR-10 using 5 random seeds. Layers in a
ResBlock are grouped into one block. Initial learning rate is 0.1, decaying by a factor of 10 at 150th
epoch. We used a linear learning rate warmup for 5 epochs. Batch size is 256 for all runs. Table 7 list
detailed hyper parameters settings. As shown in Fig. 9, block-wise SLIM-QN achieves even slightly
faster convergence performance as SLIM-QN. Both SLIM-QN and the block-wise version achieve
higher validation accuracy compared to SGD.
Due to the limited time, we have not completed experiments on large models/datasets. We will add
that in the future.
Table 7:	Hyperparameters for SGD, SLIM-QN and block-wise SLIM-QN on ResNet-18/CIFAR-10
Optimizer	∣	Ir	∣	momentum	∣ Wd ∣ damping ∣	β1 /β2	∣ L IM
block-wise SLIM-QN	∣	0.1	∣	0.9	∣ 0.0003 ∣	0.01	∣	0.9/0.9	∣ 50 ∣ 10
βι/β2: momentum for the Hessian; L: frequency for updating the Hessian; M: length of history vector (S, y)
18
Under review as a conference paper at ICLR 2022
F Hyperparameter Tuning
SLIM-QN involves additional hyperparameters besides common parameters in SGD: learning rate,
momentum, and weight decay. While tuning the hyperparameters is not a huge burden since some
parameters are fixed in all the experiments, such as lower and upper threshold for restraining
eigenvalues in the Hessian: σL , σH, and length of history vectors: M . For the rest of parameters:
damping (τ0), momentum (β1, β2) and update frequency (L) for the Hessian, it is easy to conduct
a grid search on a small models and datasets, explore how these parameters affect optimization,
then apply them to large-scale model training. For example, after training on CIFAR-10, we found
that small L improves generalization performance, and large damping stabilizes optimization but
causes optimizer to behave like SGD. With these notions, it is easy to tune these parameters, and then
achieve optimal convergence performance and accuracy.
19