Under review as a conference paper at ICLR 2022
Adaptive Behavior Cloning Regularization
for Stable Offline-to-Online Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Offline reinforcement learning, by learning from a fixed dataset, makes it pos-
sible to learn agent behaviors without interacting with the environment. How-
ever, depending on the quality of the offline dataset, such pre-trained agents may
have limited performance and would further need to be fine-tuned online by in-
teracting with the environment. During online fine-tuning, the performance of the
pre-trained agent may collapse quickly due to the sudden distribution shift from
offline to online data. While constraints enforced by offline RL methods such as a
behaviour cloning loss prevent this to an extent, these constraints also significantly
slow down online fine-tuning by forcing the agent to stay close to the behavior pol-
icy. We propose to adaptively weigh the behavior cloning loss during online fine-
tuning based on the agent’s performance and training stability. Moreover, we use
a randomized ensemble of Q functions to further increase the sample efficiency of
online fine-tuning by performing a large number of learning updates. Experiments
show that the proposed method yields state-of-the-art offline-to-online reinforce-
ment learning performance on the popular D4RL benchmark.
1	Introduction
Offline or batch reinforcement learning (RL) deals with the training of RL agents from fixed datasets
generated by possibly unknown behavior policies, without any interactions with the environment.
This is important in problems like robotics, autonomous driving, and healthcare where data collec-
tion can be expensive or dangerous. Offline RL has been challenging for model-free RL methods
due to extrapolation error where the Q networks predict unrealistic values upon evaluations on out-
of-distribution state-action pairs (Fujimoto et al., 2019). Recent methods overcome this issue by
constraining the policy to stay close to the behavior policy that generated the offline data distribu-
tion (Fujimoto et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021; Fujimoto & Gu, 2021), to
demonstrate even better performance than the behavior policy on several simulated and real-world
tasks (Siegel et al., 2020; Singh et al., 2020; Nair et al., 2020).
However, the performance of pre-trained policies will be limited by the quality of the offline dataset
and it is often necessary or desirable to fine-tune them by interacting with the environment. Also,
offline-to-online learning reduces the risks in online interaction as the offline pre-training results in
reasonable policies that could be tested before deployment. In practice, offline RL methods often fail
during online fine-tuning by interacting with the environment. This offline-to-online RL setting is
challenging due to: (i) the sudden distribution shift from offline data to online data. This could lead
to severe bootstrapping errors which completely distorts the pre-trained policy leading to a sudden
performance drop from the very beginning of online fine-tuning, and (ii) constraints enforced by
offline RL methods on the policy to stay close to the behavior policy. While these constraints help
in dealing with the sudden distribution shift they significantly slow down online fine-tuning from
newly collected samples.
We propose to adaptively weigh the offline RL constraints such as behavior cloning loss during
online fine-tuning. This could prevent sudden performance collapses due to the distribution shift
while also enabling sample-efficient learning from the newly collected samples. We propose to
perform this adaptive weighing according to the agent’s performance and the training stability. We
1
Under review as a conference paper at ICLR 2022
start with TD3+BC, a simple offline RL algorithm recently proposed by Fujimoto & Gu (2021)
which combines TD3 (Fujimoto et al., 2018) with a simple behavior cloning loss, weighted by an
α hyperparameter. We adaptively weigh this α hyperparameter using a control mechanism similar
to the proportional-derivative (PD) controller. The a value is decided based on two components:
the difference between the current return and the target return (proportional term) as well as the
change of return between current episode and the last episode (derivative term). We demonstrate
that these simple modifications lead to stable online fine-tuning after offline pre-training on datasets
of different quality. We also use a randomized ensemble of Q functions (Chen et al., 2021) to
further improve the sample-efficiency. We attain state-of-the-art online fine-tuning performance on
locomotion tasks from the popular D4RL benchmark.
2	Related Work
Offline RL. Offline RL aims to learn a policy from pre-collected fixed datasets without interacting
with the environment (Lange et al., 2012; Agarwal et al., 2020; Fujimoto et al., 2019; Kumar et al.,
2019; Nachum et al., 2019; Siegel et al., 2020; Levine et al., 2020; Peng et al., 2019). Off-policy RL
algorithms allow for reuse of off-policy data (Konda & Tsitsiklis, 2000; Degris et al., 2012; Haarnoja
et al., 2018; Silver et al., 2014; Lillicrap et al., 2015; Fujimoto et al., 2018; Mnih et al., 2015) but
they typically fail when trained offline on a fixed dataset, even if it’s collected by a policy trained
using the same algorithm (Fujimoto et al., 2019; Kumar et al., 2019). In actor-critic methods, this is
due to extrapolation error of the critic network on out-of-distribution state-action pairs Levine et al.
(2020). Offline RL methods deal with this by constraining the policy to stay close to the behavioral
policy that collected the offline dataset. BRAC (Wu et al., 2019) achieves this by minimizing the
Kullback-Leibler divergence between the behavior policy and the learned policy. BEAR (Kumar
et al., 2019) minimizes the MMD distance between the two policies. TD3+BC (Fujimoto & Gu,
2021) proposes a simple yet efficient offline RL algorithm by adding an additional behavior cloning
loss to the actor update. Another class of offline RL methods learns conservative Q functions, which
prevents the policy network from exploiting out-of-distribution actions and forces them to stay close
to the behavior policy. CQL (Kumar et al., 2020) changes the critic objective to also minimize the
Q function on unseen actions. Fisher-BRC (Kostrikov et al., 2021) achieves conservative Q learning
by constraining the gradient of the Q function on unseen data. Model-based offline RL methods
(Yu et al., 2020; Kidambi et al., 2020) train policies based on the data generated by ensembles of
dynamics models learned from offline data, while constraining the policy to stay within samples
where the dynamics model is certain. In this paper, we focus on offline-to-online RL with the
goal of stable and sample-efficient online fine-tuning from policies pre-trained on offline datasets of
different quality.
Offline pre-training in RL. Pre-training has been vastly investigated in the machine learning com-
munity from computer vision (Sharif Razavian et al., 2014; Donahue et al., 2014; Yosinski et al.,
2014) to natural language processing (Devlin et al., 2018; Turian et al., 2010). Offline pre-training
in RL could enable deployment of RL methods in domains where data collection can be expensive
or dangerous. (Silver et al., 2016; Gupta et al., 2019; Rajeswaran et al., 2017) pre-train the policy
network with imitation learning to speed up RL. QT-opt (Kalashnikov et al., 2018) studies vision-
based object manipulation using a diverse and large dataset collected by seven robots over several
months and fine-tune the policy with 27K samples of online data. However, these methods pre-train
using diverse, large, or expert datasets and it is also important to investigate the possibility of pre-
training from offline datasets of different quality. Yang & Nachum (2021); Ajay et al. (2020) use
offline pre-training to accelerate downstream tasks. AWAC (Nair et al., 2020) and Balanced Replay
Lee et al. (2021) are recent works that also focus on offline-to-online RL from datasets of different
quality. AWAC updates the policy network such that it is constrained during offline training while
not too conservative during fine-tuning. Balanced Replay trains an additional neural network to pri-
oritize samples in order to effectively use new data as well as near-on-policy samples in the offline
dataset. We compare with AWAC and Balanced Replay to attain state-of-the-art offline-to-online
RL performance on the popular D4RL benchmark.
Ensembles in RL. Ensemble methods are widely used for better performance in RL (Fauβer &
Schwenker, 2015; Osband et al., 2016; Chua et al., 2018; Janner et al., 2019). In model-based RL,
PETS (Chua et al., 2018) and MBPO (Janner et al., 2019) use probabilistic ensembles to effectively
model the dynamics of the environment. In model-free RL, ensembles of Q functions have been
2
Under review as a conference paper at ICLR 2022
shown to improve performance (Anschel et al., 2017; Lan et al., 2020). REDQ (Chen et al., 2021)
learns a randomized ensemble of Q functions to achieve similar sample efficiency as model-based
methods without learning a dynamic model. We utilize REDQ in this work for improved sample-
efficiency during online fine-tuning. Specific to offline RL, REM (Agarwal et al., 2020) uses random
convex combinations of multiple Q-value estimates to calculate the Q targets for effective offline RL
on Atari games. MOPO (Yu et al., 2020) uses probabilistic ensembles from PETS to learn policies
from offline data using uncertainty estimates based on model disagreement. MBOP (Argenson &
Dulac-Arnold, 2020) uses ensembles of dynamic models, Q functions, and policy networks to get
better performance on locomotion tasks. Balanced Replay (Lee et al., 2021) uses ensembles of
pessimistic Q functions to mitigate instability caused by distribution shift in offline-to-online RL.
While ensembling of Q functions has been studied by several prior works (Lan et al., 2020; Chen
et al., 2021), we combine it with behavioral cloning loss for the purpose of robust and sample-
efficient offline-to-online RL.
Adaptive balancing of multiple objectives in RL. Ball et al. (2020) train policies using learned dy-
namics models with the objective of visiting states that most likely lead to subsequent improvement
in the dynamics model, using active online learning. They adaptively weigh the maximization of cu-
mulative rewards and minimization of model uncertainty using an online learning mechanism based
on exponential weights algorithm. In this paper, we focus on offline-to-online RL using model-free
methods and propose to adaptively weigh the maximization of cumulative rewards and a behavioral
cloning loss. Exploration of other online learning algorithms such as exponential weights algorithm
is a line of future work.
3	Background
3.1	Reinforcement Learning
Reinforcement learning (RL) deals with sequential decision making to maximize cumulative re-
wards. RL problems are often formalized as Markov decision processes (MDPs). An MDP consists
of a set of states S, a set of actions A, a transition dynamics st+ι 〜p(∙∣st, at) that represents
the probability of transitioning to a state st+1 by taking action at in state st at timestep t, a scalar
reward function rt = R(st, at), and a discount factor γ ∈ [0, 1].
A policy function π of an RL agent is a mapping from states to actions and defines the behavior of
the agent. The value function Vπ (s) of a policy π is defined as the expected cumulative rewards
from state s: Vπ(s) = E[Pt∞=0 γtR(st, at)|s0 = s], where the expectation is taken over state
transitions st+ι 〜 p(∙∣st, at) and policy function at 〜 π(st). Similarly, the state-action value
function Qπ(s, a) is defined as the expected cumulative rewards after taking action a in state s:
Qπ(s, a) = E[Pt∞=0 γtR(st, at)|s0 = s, a0 = a]. The goal of RL is to learn an optimal policy
function πθ with parameters θ, that maximizes the expected cumulative rewards:
πθ
arg max Es〜S [vπθ (s)]
arg max Es〜S
θ
Qπθ (s, πθ(s)) .
We use the TD3 algorithm for reinforcement learning (Fujimoto et al., 2018). TD3 is an actor-critic
method that alternatingly trains: (i) the critic network Qφ to estimate the Qπθ (s, a) values of the
policy network πθ, and (ii) the policy network to produce actions that maximize the Q function:
Vθ Qφ(s,∏θ ⑸).
3.2	Offline Pre-training
Offline reinforcement learning or batch reinforcement learning assumes that the agent is not able to
interact with the environment but is given a fixed dataset D of (s, a, r, s0) tuples to learn from. The
data is assumed to be collected by an unknown behavioural policy (or a collection of policies).
The problem with using actor-critic methods for offline RL is extrapolation error due to the evalua-
tion of the critic network on the next state and next action values Q(s0, a0) to compute the temporal
difference error. Here the next action a0 is sampled from the policy network a0 〜∏θ(s0) and this
could lead to out-of-distribution evaluations of the critic network. This is problematic as erroneous
predictions of the critic on unfamiliar actions could be propagated to other critic predictions due to
3
Under review as a conference paper at ICLR 2022
bootstrapping in temporal difference learning. This will also lead to the policy network preferring
actions with unrealistic value predictions. This problem can be overcome either by constraining
the policy network to stay close to the data distribution (Fujimoto & Gu, 2021) or by enforcing
conservative estimates of the critic network on out-of-distribution samples (Kumar et al., 2020).
Fujimoto & Gu (2021) propose TD3+BC, a simple offline RL algorithm that regularizes policy
learning in TD3 with a behavior cloning loss that constraints the policy actions to stay close to the
actions in the offline dataset D. This is achieved by adding a behavior cloning term to the policy
loss:
∏θ = argmaxE(s,a)〜D [Q(s,∏θ(S)) - α(∏θ(S) - a)2]
where α is a weighing hyperparameter and
(1)
Q(s,πθ (S))=IPb⅛*
normalizes the Q values which help in balancing both losses. The sum in the denominator is taken
over a mini-batch and the gradients do not flow through the critic term in the denominator.
4	Online Fine-tuning
RL agents trained from offline data tend to have limited performance and would further need to be
fine-tuned online by interacting with the environment. During online fine-tuning, the performance
of the pre-trained agent may collapse quickly due to the sudden distribution shift from offline data to
online data. Keeping the constrain used in offline pre-training, such as in Equation 1, could mitigate
the collapse. However, this will force the policy to stay close to the behavior policy (used to collect
the dataset), thus leads to slow improvement. In this section, we describe the two components of our
online-tuning algorithm that enables stable and sample-efficient online fine-tuning.
4.1	Adaptive Weighing of B ehavior Cloning Loss
The most straightforward way to fine-tune the pre-trained policy is by just removing the constrains
used in offline pre-training. For example, Balanced Replay (Lee et al., 2021) uses CQL (Kumar et al.,
2020) during offline pre-training and uses SAC (Haarnoja et al., 2018) in fine-tuning. However,
this strategy often leads to a performance collapse at the beginning of fine-tuning, as shown in
Fig. 1 (with α = 0) and the TD3-ft in Fig. 2. In the TD3+BC algorithm we consider in this paper,
an α hyperparameter is used to balance the RL objective and the behaviour cloning term which
constrains the policy to stay close to the behavior policy (see Equation 1). We use αoffline and αonline
to distinguish the α hyperparameter value used during offline and online training respectively. By
default, we use αoffline = 0.4 in all our experiments. We use αonline = 0.4 for TD3+BC and we
observe that this prevents sudden performance drops at the initial steps of online fine-tuning, at the
cost of very slow learning due to the strong behavior cloning constraint. On the other hand, setting
αonline = 0 leads to sample-efficient learning on some tasks at the cost of complete instability in other
tasks. This is due to the sudden distribution shift causing the policy network to change significantly.
In Fig. 1, we present the influence of αonline on the TD3+BC during fine-tuning by trying different
values of αonline from [0.0, 0.1, 0.3]. We can clearly see that using the behavior cloning loss with
proper αonline enables stable fine-tuning. However, the value of αonline depends on the quality of the
offline dataset and has significant influence of the fine-tuning performance. For example, αonline = 0
fits well on the Hopper-Random task while causes immediate collapse on Hopper-Medium and
Hopper-Medium-Expert tasks.
In our experiments, we found that when the offline dataset has narrow distribution or when the policy
has already converged to a desired performance (comparable to the expert), it is usually beneficial
to maintain a higher αonline. When the data distribution is broader or when we still need to improve
the agent by a large margin, a smaller αonline works better. During experiments, we can not find a
single αonline that is suitable for all tasks and its value needs to be tuned carefully per tasks, which
makes this method hard to be used in practice.
To solve this problem, we propose to adapt the weight of the behavior cloning loss according to two
factors: (i) the difference between current episodic return and the target return, and (ii) the episodic
4
Under review as a conference paper at ICLR 2022
3500
30∞
25∞
2000
15∞
1000
500
3500
3∞0
2500
2∞0
1500
1∞0
500
O
3500
30∞
25∞
2000
15∞
1000
500
Hopper-Medium-Expert
50	100	150	200	250
Figure 1: Results of online fine-tuning on the D4RL benchmark using TD3+BC with different αonline
hyperparameters. We plot the mean and standard deviation across 3 runs. Using the behavior cloning
loss with proper αonline enables the stable fine-tuning. But the optimal value of αonline differs between
datasets.
return between current episode and the last episode. We adaptively change the αonline hyperparameter
as:
∆(αonline )
KP ∙ (Rcurrent - Rtarget) + KD ∙ max(0, Rlast - RcUrrent)
(2)
where we constrain αonline between 0 and 0.4 (the value used during offline pre-training). Rcurrent and
Rlast are normalized following the return normalization procedure used in D4RL. Rtarget is the target
episodic return, which we set as 1 (corresponding to the expert policy) for all tasks. KP controls
how fast we decrease the αonline according to current performance and KD determines how fast we
increase the αonline when the performance drops. Intuitively, when the agent’s performance reaches
the target episodic return, we try to maintain it during fine-tuning. But when the agent’s performance
is low, we decrease the αonline to allow the agent improving further. The second term increases the
αonline when performance drops during training to mitigate performance collapse. Equation 2 allows
for adaptive weighing of the behavior cloning loss throughout online fine-tuning. This learning
algorithm can automatically adjust the constraint enforced by the behavior cloning loss.
After offline pre-training the replay buffer is filled with offline samples and during online fine-
tuning, they are slowly replaced by online samples. Uniformly sampling mini-batches from this
replay buffer for online fine-tuning is inefficient as it is dominated by offline samples. After offline
learning we simply remove 95% of random offline samples from the replay buffer to deal with this
problem. Our results show that the data down-sampling allows efficient usage of novel data without
destroying the training.
4.2	Randomized Ensembles of Critic Networks
We propose to use an ensemble of Q functions to better deal with the distribution shift from offline
pre-training and to improve the sample-efficiency of online fine-tuning. We use the Randomized
Ensembled Double Q-learning (REDQ) method proposed by Chen et al. (2021) to learn an ensemble
of critic networks.
The critic network is trained to satisfy the Bellman equation: Qπ(s, a) = r + γQπ(s0, πθ(s0)).
REDQ maintains an ensemble of N critic networks and randomly samples M networks for each
critic update. Given a mini-batch B of B transitions (s, a, r, s0), all critic networks in the ensemble
are updated towards the same target:
vΦiτ⅛	X	(QΦi(s,a)-r-γminQφi(s,a0))2	⑶
|B|	i∈M
(s,a,r,s0)∈B
where M is a random subset of M critic networks and a0 = clip(πθ(st+1) + , alow, ahigh). Here
is Gaussian exploration noise with standard deviation σpolicy and [alow , ahigh] is the action range.
REDQ updates the policy network to maximize the average predictions of the critic networks:
1	1 N
vθ |B| ∑S N ΣSQΦi (s,πθ(s)).
|B| s∈B N i=1
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Offline-to-online RL with adaptive behaviour cloning and ensembles of critic networks
Initialize REDQ agent with critic parameters Φi,...,Φn and policy parameters θ
Initialize target parameters θ0 J θ and φi J φi, for i = 1,...,N
Initialize replay buffer R with offline data D
for k = 0 to K do
Sample mini-batch B ofB transitions (s, a, r, s0) from R
Update critic parameters φ1 , . . . , φN using Equation 3
Update actor parameters θ using Equation 4 with α = αoffline
Update target networks θ0 J τθ + (1 - τ)θ0 and φ0i J τφi + (1 - τ)φ0i
end for
Randomly remove 95% of offline samples from R
Initialize αonline = αoffline
Initialize Rcurrent and Rlast to store the return of current and previous episodes
Initialize environment for online fine-tuning
for every training episode do
for t = 0 to T do
Act with exploration noise at 〜∏θ(St) + N(0, σexpl)
Observe next state st+1 and reward rt
Add (st, at,rt, st+1) to R
for g = 0 to G do
Sample mini-batch B ofB transitions (s, a, r, s0) from R
Update critic parameters φ1 , . . . , φN using Equation 3
Update actor parameters θ using Equation 4 with α = αonline
Update target networks θ0 J τθ + (1 - τ)θ0 and φ0i J τφi + (1 - τ)φ0i
end for
end for
Set Rlast = Rcurrent and Rcurrent = Pt=0 rt
Adapt αonline based on Rlast and Rcurrent using Equation 2
end for
We combine this REDQ policy update with a behaviour cloning loss (like in Equation 1) for robust
learning (Fujimoto & Gu, 2021):
1	1N	2
Re|B| E N∑SQΦi(S,πθ(S)) - α(πθ(S) - Q).
(s,a)∈B	i=1
(4)
We show that this simple modification of ensembling the critic networks (which can be run in paral-
lel) improves offline-to-online learning. We call this algorithm REDQ+AdaptiveBC. Our algorithm
is outlined as Algorithm 1.
5	Experiments
5.1	Online Fine-tuning on D4RL benchmark
The goal of our experiments is to evaluate the stability and sample-efficiency of the proposed al-
gorithm on online fine-tuning after offline pre-training on datasets of different quality. We evaluate
our algorithm on online fine-tuning after offline pre-training on the D4RL benchmark (Fu et al.,
2020). D4RL includes three locomotion tasks (halfcheetah, hopper, and walker) implemented in the
MuJoCo simulator (Todorov et al., 2012), wrapped in OpenAI Gym API (Brockman et al., 2016).
D4RL provides five different offline datasets for each task: Random, Medium, Medium-Replay,
Medium-Expert, and Expert. The Random datasets are collected by random policies, Medium
datasets are collected by an early-stopped soft actor-critic (SAC) (Haarnoja et al., 2018) agent with
medium-level performance, Medium-Replay datasets consist of all samples in the replay buffer after
training a medium-level agent, Medium-Expert datasets are mixed with expert demonstrations and
sub-optimal demonstrations from a medium-level agent, and Expert datasets are expert demonstra-
tions. The “expert” in these datasets is a fully trained soft-actor critic agent. We ignore the Expert
6
Under review as a conference paper at ICLR 2022
14000
HaIfCheetah-Random
12∞0
10∞0
g a∞o
« 6∞0
4∞0
100	150
REDQtAdaptiveBC (OUrS)
AW
Balancec Reptay
—TO3_ft
REDQ (scratch)
HaIfCheetah-Medium
HaIfCheetah-Medium-RepIay
1∞OO
8∞0
MOO
6∞0
eooo
400。
4000
200C
5€
HaIfCheetah-Medium-Expert
12000
1∞00
æoo
eooo
4000
O
Time Steps (1e3)
2000
2∞0
2000
O
O
O
O
Time Steps (1e3)
Time Steps (1e3)
Time Steps (1e3)
Figure 2: Results of online fine-tuning on the D4RL benchmark. We plot the mean and standard
deviation across 5 runs. Our REDQ+AdaptiveBC method attains performance competitive to the
state-of-the-art. It should be noticed that, unlike other methods, our results do not collapse immedi-
ately at the beginning of training.
datasets in this paper as offline RL algorithms already achieve expert-level performance on these
tasks and there is little to no benefit in online fine-tuning.
In Figure 2, we compare our REDQ+AdaptiveBC algorithm with two state-of-the-art offline-to-
online RL algorithms (AWAC and Balanced Replay) and two baseline methods (TD3-ft and REDQ):
•	Advantage Weighted Actor-Critic (AWAC) (Nair et al., 2020) is an actor-critic method
for offline-to-online RL that implicitly constraints the policy network to stay close to the
behavior policy. We produce the results for AWAC using code taken from https://
github.com/ikostrikov/jaxrl.
•	Balanced Replay (Lee et al., 2021) is an offline-to-online RL method that prioritizes near-
on-policy samples from the replay buffer. This method also uses an ensemble of Q func-
tions to prevent overestimation of Q values in the initial stages of online fine-tuning. We
reproduced the results for this method using our own implementation. For a fair compari-
son, we base our implementation on TD3+BC (instead of CQL originally used by Lee et al.
(2021)) while ensuring that we are able to reproduce the original results.
•	TD3-ft is the standard TD3 algorithm (Fujimoto et al., 2018) that was pre-trained offline
using TD3+BC (Fujimoto & Gu, 2021).
•	REDQ (scratch) (Chen et al., 2021) is an RL method trained from scratch, without any
access to the offline data. This baseline emphasizes the importance of offline pre-training
and online fine-tuning. We base our REDQ implementation on TD3 (instead of SAC used
by Chen et al. (2021)) for compatibility with TD3+BC.
All methods (except AWAC) are implemented on top on TD3 and are run from the same codebase
for a fair comparison. For simplicity, we do not perform any state normalization like in the original
TD3+BC implementation (Fujimoto & Gu, 2021).
During offline pre-training, all algorithms are pre-trained on the offline dataset for one million gra-
dient steps. After pre-training, we fine-tune the agents for 250,000 time steps by interacting with
the environment. We evaluate the agent every 5000 time steps and each evaluation consists of 10
7
Under review as a conference paper at ICLR 2022
episodes. We attain performance competitive to the state-of-the-art in this benchmark with our
method stably improving the performance during online fine-tuning.
We significantly outperform all others methods in the HalfCheetah domain for three datasets. We
perform slightly worse than Balanced Replay on Walker2d-Medium and Walker2d-Medium-Replay
but outperform it or get similar results in all other cases. We need to mention that in both Walker2d-
Medium and Walker2d-Medium-Replay tasks, our method already reaches the target performance
(predefined following D4RL), and the α is increased automatically to maintain the performance and
thus fail to improve further. We significantly outperform REDQ on all tasks, which demonstrates
that we considerably benefit from offline pre-training. TD3-ft is able to improve from online fine-
tuning but suffers from significant performance drops due to the sudden distribution shift and the
learning progress is slow due to the replay buffer being dominated by offline samples. It should
be noticed that, unlike other methods, our algorithm does not collapse immediately on all three
Medium-Expert tasks.
Both Balanced Replay and our method (REDQ+AdaptiveBC) use an ensemble of 10 Q networks,
but in different ways. Balanced Replay maintains a pair of five ensemble networks, average the
predictions across each of the five networks and then takes the minimum of the averages as the final
prediction. In our method, we simply consider the average of all 10 networks as the prediction but
randomly sample a pair of Q networks to compute the critic targets (Equation 3). We show that this
simple modification enables stable and sample-efficient online fine-tuning without the need for any
complex sampling scheme from the replay buffer.
Similar to prior works (Fujimoto & Gu, 2021), we use feed-forward networks with two hidden
layers as actor and critic networks for all the methods. We use a batch size of 256 to train the
network for all methods, except for AWAC where we use a larger batch size of 1024 (Nair et al.,
2020). During offline learning, we use αoffline = 0.4 for all tasks, except Walker-Random where we
use αoffline = 100 since the dataset has a very narrow distribution. We list all the hyperparameters
used in our experiments in Table 1 in the Appendix.
5.2	Algorithmic Investigations
Adaptive Weighing of αonline : To evaluate whether the proposed method can correctly select a good
αonline for stable online fine-tuning, in Figure 3, we compare the results obtained with the automat-
ically tuned αonline with manually tuned results. To manually tune the αonline, for each domain and
each dataset, we do a grid search on αonline over [0.0, 0.1, 0.2, 0.3] and pick the best αonline separately
for each task.
We can see that with manually tuned αonline, our method consistently outperformances other methods
in Figure 2, except the Balanced Replay on Walker2d-Medium. Our results with automatically tuned
αonline are slightly worse than manually tuned results on HalfCheetah tasks. However, our method
successfully find the similar αonline on Halfcheetah tasks as we manually selected after roughly 10-15
episodes. On the rest tasks, our automatically tuned results are competitive to the carefully picked
results but saving lots of labor and computational resources.
Offline Dataset Downsampling: Balanced Replay (Lee et al., 2021) trains a neural network to
estimate the priority of samples from offline data and online data. In their methods, three replay
buffer need to be maintained: offline dataset (0.1-2M samples), online dataset (0.25M samples)
and a prioritized replay buffer (0.35M-2.25M samples) (Schaul et al., 2015), making it memory
consuming (0.7M-4.5M samples). Our method simply dowsamples the offline dataset by 95%, thus,
our method only maintains one replay buffer to store online data but is prefilled with 0.05M offline
data points, roughly saves 65% - 95% memory.
To demonstrate the effectiveness of dataset downsampling, We compare TD3ft with and without
dataset downsampling on three random datasets, shown in Figure 4. Our results show that the
downsampling procedure allows the agent effectively sampling the novel data encountered during
fine-tuning. This is even important when the data quality of the offline data is not good enough, such
as when the dataset is collected by a random policy.
8
Under review as a conference paper at ICLR 2022
9∞0
8∞O
7∞0
6∞O
5000
3500
3∞0
2500
2∞0
4000
375C
3500
1500
3250
20C
Walker2d-Medium-Replay
2500
1∞0
Time Steps (1e3)
200	250
Time Steps (1e3)
Figure 3: Comparison of results with automatically tuned αonline and carefully picked results. It
shows that our proposed method can effectively find the suitable αonline for all tasks.
3000
275C
W
Time Steps (1e3)
50	100	1 50	20C
Walker2d-Medium-Expert
Hopper-Random
Time Steps (1e3)
Wal ker2d-Random
14000
12000
10000
8000
6000
4000
2000
O 50	100	150	200	250

Figure 4:	Comparison of TD3-ft with and without dataset downsampling. We plot the mean and
standard deviation across 3 runs. Downsampling enables effective usage of novel data encountered
during fine-tuning.
6 Conclusion
We consider the problem of offline-to-online RL where an agent is first pre-trained on offline data
collected (by a possibly unknown behavior policy) and the agent is then fine-tuned online by inter-
acting with the environment. This is desirable as pre-trained agents may have limited performance
depending on the quality of the offline dataset. Offline-to-online RL is challenging due to the sud-
den distribution shift from offline data to online data, and also the constraints enforced by offline
RL algorithms (such as a behavior cloning loss) during pre-training. In this paper, we propose a
simple mechanism to adaptively weight a behavior cloning loss during online fine-tuning, based on
agent performance and training stability. We demonstrate that a randomized ensemble further helps
to deal with these challenges to enable sample-efficient online fine-tuning performance. We attain
performance competitive to the state-of-the-art online fine-tuning methods on locomotion tasks from
the popular D4RL benchmark.
9
Under review as a conference paper at ICLR 2022
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, pp. 104-114. PMLR,
2020.
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline prim-
itive discovery for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611,
2020.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-DQN: Variance reduction and stabi-
lization for deep reinforcement learning. In International conference on machine learning, pp.
176-185. PMLR, 2017.
Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. arXiv preprint
arXiv:2008.05556, 2020.
Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts.
Ready policy one: World building through active learning. In International Conference on Ma-
chine Learning, pp. 591-601. PMLR, 2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.
Xinyue Chen, Che Wang, Zijian Zhou, and Keith W Ross. Randomized ensembled double Q-
learning: Learning fast without a model. In International Conference on Learning Representa-
tions, 2021.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114,
2018.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint
arXiv:1205.4839, 2012.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Inter-
national conference on machine learning, pp. 647-655. PMLR, 2014.
Stefan Fauβer and Friedhelm Schwenker. Neural network ensembles in reinforcement learning.
Neural Processing Letters, 41(1):55-69, 2015.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep
data-driven reinforcement learning, 2020.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy
learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint
arXiv:1910.11956, 2019.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018.
10
Under review as a conference paper at ICLR 2022
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. arXiv preprint arXiv:1906.08253, 2019.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,
2018.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774-5783. PMLR, 2021.
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin Q-learning: Controlling the
estimation bias of Q-learning. In International Conference on Learning Representations, 2020.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45-73. Springer, 2012.
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-
online reinforcement learning via balanced replay and pessimistic q-ensemble. arXiv preprint
arXiv:2107.00591, 2021.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. arXiv preprint arXiv:1906.04733, 2019.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing systems, 29:4026-4034, 2016.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
11
Under review as a conference paper at ICLR 2022
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-
the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition workshops, pp. 806-813, 2014.
Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert,
Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked:
Behavior modelling priors for offline reinforcement learning. In International Conference on
Learning Representations, 2020.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387-395. PMLR, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Cog:
Connecting new skills to past experience with offline reinforcement learning. arXiv preprint
arXiv:2010.14500, 2020.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for
computational linguistics, pp. 384-394, 2010.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Mengjiao Yang and Ofir Nachum. Representation matters: Offline pretraining for sequential deci-
sion making. arXiv preprint arXiv:2102.05815, 2021.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? arXiv preprint arXiv:1411.1792, 2014.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.
12
Under review as a conference paper at ICLR 2022
40∞
30∞
2000
1000
O
4∞0
3500
3∞0
2500
2∞0
1500
1∞0
Hopper-Medium-Expert
Figure 5:	Comparison of usages of ensembles on the hopper domain. We plot the mean and standard
deviation across 3 runs. Randomized ensembled double Q-Learning stabilizes the training, but it is
not necessary to avoid performance collapse during fine-tuning.
Table 1: Hyperparameters used in our experiments
Hyperparameter		Value
	Optimizer	Adam
	Learning rate	3e-4
	Batch size	256
TD3	Target update rate	5e-3
	Policy noise std	0.1
	Policy noise clip	0.5
	Policy update frequency	2
	Hidden layers	2
Architecture	Hidden units	256
	Activation function	ReLU
	Number of networks N	10
REDQ	Randomly sampled networks M	2
	Number of updates G	20
Offline BC	αoffline	0.4
Adaptive BC	-Kp	3e-5
	Kd	8e-5
A Ablation on ensembles
In our experiments, we use ensembles to represent the critic network. In 5, we compare the training
results with and without ensembles as well as the way to use ensembles. Our results show that
using ensembles is not necessary to avoid performance collapse, however, it stabilizes training in
most cases. Also, the way to use ensembles matters. We compare calculating target Q values with
randomly sampled Q predictions and with minimal predictions. Our results show that using minimal
Q predictions to calculate target Q values hurts the performance in most cases.
B Hyperparameters
All the hyperparameters and network architectures used in experiments are listed in Table 1.
13