Under review as a conference paper at ICLR 2022
Weakly Supervised Label Learning Flows
Anonymous authors
Paper under double-blind review
Ab stract
Supervised learning usually requires a large amount of labelled data. However,
attaining ground-truth labels is costly for many tasks. Alternatively, weakly su-
pervised methods learn with cheap weak signals that only approximately label
some data. Many existing weakly supervised learning methods learn a determin-
istic function that estimates labels given the input data and weak signals. In this
paper, we develop label learning flows (LLF), a general framework for weakly
supervised learning problems. Our method is a generative model based on nor-
malizing flows. The main idea of LLF is to optimize the conditional likelihoods
of all possible labelings of the data within a constrained space defined by weak
signals. We develop a training method for LLF that trains the conditional flow
inversely and avoids estimating the labels. Once a model is trained, we can make
predictions with a sampling algorithm. We apply LLF to three weakly supervised
learning problems. Experiment results show that our method outperforms many
state-of-the-art alternatives.
1	Introduction
Machine learning has achieved great success in many supervised learning tasks. However, in prac-
tice, data labeling is usually human intensive and costly. To address this problem, practitioners are
turning to weakly supervised learning (Zhou, 2018), which trains machine learning models with
only noisy labels that are generated by human specified rules or pretrained models for related tasks.
Recent research shows that these models trained with weak supervisions can also perform well.
Many existing weakly supervised learning methods (Ratner et al., 2016; 2017; Bach et al., 2019;
Arachie & Huang, 2021b) learn a deterministic function that estimates the unknown labels y given
input data x and weak signals Q. Since the observed information is incomplete, the predictions
based on it can be varied. However, these methods ignore this uncertainty between x and y. In this
paper, we develop label learning flows (LLF), a general framework for weakly supervised learning
problems. LLF is a flow-based generative model (Dinh et al., 2014; Rezende & Mohamed, 2015;
Dinh et al., 2016; Kingma & Dhariwal, 2018). The main idea behind LLF is that we define the rela-
tionship between x and y with a probability distribution p(y|x), which is modeled by a conditional
flow. In training, we use the weak signals Q to define a constrained space for y and then optimize
the likelihood of all possible y that are within this constrained space. Therefore, this model captures
all possible relationships between the input x and output y. Learning LLF can be defined as a con-
strained optimization problem. We develop a learning method for LLF that trains the conditional
flow inversely and avoids estimating the labels. For prediction, we use sample-based method (Lu &
Huang, 2020) to estimate the labels.
We apply LLF to three weakly supervised learning problems: weakly supervised classifica-
tion (Arachie & Huang, 2021b; Mazzetto et al., 2021b), weakly supervised regression, and unpaired
point cloud completion (Chen et al., 2019; Wu et al., 2020). These three problems have very different
label types and weak signals. Our method outperforms all other state-of-the-art methods on weakly
supervised classification and regression, and it can perform comparably to other recent methods on
unpaired point cloud completion. These experiments show that LLF is versatile and powerful.
2	Background
In this section, we introduce weakly supervised learning and conditional normalizing flows.
1
Under review as a conference paper at ICLR 2022
Weakly Supervised Learning. Given a dataset D = {x1, ..., xN}, and weak signals Q, weakly
supervised learning finds a model that can predict the unknown label yi for each input data xi .
Some methods, e.g., ALL (Arachie & Huang, 2021b), define a set of constrained functions based
on Q and x. These functions form a space of possible y. The methods then look for one possible
y within this constrained space. In this work, we follow this idea and use constrained functions to
restrict the predicted y.
Conditional Normalizing Flows. A normalizing flow (Rezende & Mohamed, 2015) is a series of
invertible functions f = fι ◦ f2 ◦ ∙ ∙ ∙ ◦ fκ that transform the probability density of output variables
y to the density of latent variables z. In conditional flows (Trippe & Turner, 2018), a flow layer
function fi is also parameterized by the input variables x, i.e., fi = fx,φi, where φi is the parameters
offi. With the change-of-variable formula, the log conditional distribution log p(y|x) can be exactly
and tractably computed as
log p(y∣x) = log PZ (fχ,φ (y)) + X log det (fφi ) |,	(1)
where PZ (Z) is a tractable base distribution, e.g. Gaussian distribution. The fφ is the Jacobian
∂ ri-1
matrix offx,φi. The ri = fx,φi(ri-1), r0 = y, and rK = z.
To use normalizing flows, we need to develop flow layers that are invertible and have tractable
Jacobian determinant. In this paper, we use affine coupling layer (Dinh et al., 2014; 2016) to form
normalizing flows. It splits the input to two parts, and force the first part only relate to the second
part, so that the Jacobian is a triangular matrix. For conditional flows, we can define conditional
affine coupling layer as
ya, yb = split(y),
zb = s(x, ya) yb + b(x, ya),
z = concat(ya, zb),
where s and b are two neural networks. The split() and the concat() functions split and concatenate
the variables.
3	Proposed Method
In this section, we introduce the label learning flows (LLF) framework for weakly supervised learn-
ing. Given Q and x, we define a set of constraints to restrict the predicted label y. These constraints
can be inequalities, formatting like f(x, y, Q) ≤ b, or equations, formatting like f(x, y, Q) = b.
For simplicity, We represent this set of constraints as C(x, y, Q). Let Ω be the constrained space
of all possible y defined by C(x, y, Q). Previous methods only look for one possible y within Ω.
In contrast, LLF optimizes the conditional log-likelihood of all possible y within Ω, resulting in the
following objective
maxEpdata(X)Ey〜U⑸)[logp(y∣x, φ)],	(2)
where U(Ω) is a uniform distribution within Ω, andp(y∣x) is a continuous density model of y.
Let y be the true label. We assume that each data point Xi only has one unique label yi, so that
Pdata(x, y) = Pdata(x). Let q(y∣x) be a certain model of y. Traditional supervised learning learns a
q(y |x) that maximizes the cross entropy Epdata(x,y) [log q(y∣x, φ)]. Following theorem indicates that
maximizing logp(y∣x) can be interpreted as maximizing a lower bound of log q(y∣x).
Theorem 1 Let Ω* ⊆ Ω is a tight enough space satisfying that y ∈ Ω* and for two different y
and yj, the Ω* and Ωj are non-overlapped. The volume of Ω* is bounded such that ∣Ω^∣ ≤ M.
The relationship between p(y∣x) and q(y∣x) can be defined as: q(y|x) = Jy∈Q* p(y∣x)dy. Then
maximizing log p(y∣x) can be interpreted as maximizing the lower bound of log q(y∣x). That is,
Ep data (X)Ey 〜U (Ω*) [log P(y|x, φ)] ≤ M Ep data (x,y) [log 9(y |x, φ)]	⑶
The complete proof is in appendix. Theorem 1 indicates that, when Ω is well-defined, learning LLF
is analogous to dequantization (Theis et al., 2015; Ho et al., 2019). That is, the method optimizes the
2
Under review as a conference paper at ICLR 2022
likelihood of dequantized true labels. In practice, the real constrained space, i.e., Ω, may be loose
and does not fulfill the conditions in Theorem 1, which will result in inevitable errors that come from
the weakly supervised setting. Moreover, for some regression problems, the ideal Ω* only contains
a single point: the ground truth label, i.e., Ω* = {y}.
3.1	Learning and Prediction
Since y is unobserved, directly optimizing the conditional likelihood, i.e., Equation 2, is impossible.
Using the invertibility of normalizing flows, we can rewrite log p(y|x) as
log p(y|x)
K
logpZ(fx,φ(y)) +	log det
i=1
K
log pZ (z) -	log det
i=1
∂ dgχ,Φi ʌ
k dri )
(4)
where gx,φi = fx-,φ1i is the inverse flow.
With the inverse flow, Equation 2 can be interpreted as a constrained optimization problem
mφaxEpdata(x)EpZ(z)
K
log pZ (z) -	log det
i=1
∂ dgχ,φi ʌ
[dri )
(5)
s.t. C(x,gx,φ(z),Q).
Note that in Equation 5, the original constraint for Z is gx,φ(z) ∈ Ω, and this constraint can be
replaced with C(x, gx,φ(z), Q). For efficient training, this constrained optimization problem can
be approximated with the penalty method, resulting in the objective
mφaxEpdata(x)EpZ(z)
logPZ(Z)- Xlog det Cgr") + λCr(x, gχ,φ(z), Q)
(6)
where λ is the penalty coefficient, and Cr () means we reformulate the constraints to be penalty
losses. For example, an inequality constraint will be redefined as a hinge loss.
In practice, the expectation with respect to pZ (Z) can be approximated with Monte Carlo esti-
mate with L samples. Since we only need to obtain stochastic gradients, we follow previous
works (Kingma & Welling, 2013) and set L = 1.
Given a trained model and a data point xi , prediction requires outputting a label yi for xi . We
follow (Lu & Huang, 2020) and use a sample average, i.e., yi = PjL=1 gxi,φ(Zj) as the prediction.
In our experiments, we found that L = 10 is enough for generating high-quality labels.
4	Case Study
In this section, we illustrate three scenarios of using LLF to address weakly supervised learning
problems.
4.1	Weakly Supervised Classification
We follow previous works (Arachie & Huang, 2021b; Mazzetto et al., 2021b) and consider binary
classification. For each example, the label y is a two-dimensional vector within a one-simplex. That
is, the y ∈ Y = {y ∈ [0, 1]2 : Pi y[i] = 1}, where y[i] is the ith dimension of y. Each ground
truth label y ∈ {0,1}2 is a two-dimensional one-hot vector. We have M weak labelers, which
will generate M weak signals for each data point xi, i.e., qi = [qi,1, ..., qi,M]. Each weak signal
qi,m ∈ Q = {q ∈ [0, 1]2 : Pi q[i] = 1} is a soft labeling of the data. In practice, if a weak labeler
m fails to label a data point Xi, the qi,m can be null, i.e., qi,m = 0 (Arachie & Huang, 2021a).
Following Arachie & Huang (2021b), we assume we have access to expected error rate bounds of
3
Under review as a conference paper at ICLR 2022
these weak signals b = [b1, .., bM]. Therefore, the weak signals imply constraints
N
X (1 - yi)	qi,m + yi	(1 - qi,m) ≤ Nmbm	∀m ∈ {1, ..., M},	(7)
i=1,qi,m = 0
where Nm is the number of data points that are labeled by weak labeler m.
This problem can be solved with LLF, i.e., Equation 5, by defining C(x, gx,φ(z, Q) to be a com-
bination of weak signal constraints, i.e., Equation 7, and simplex constraints, i.e., y ∈ Y. The
objective function of LLF for weakly supervised classification is
max
φ
logPZ (Z) - X log det Cgr,φ )
+λ1 [gx,φ(z)]2+ + λ2 [1 - gx,φ(z)]2+ + λ3
X gx,φ(Z)[i] - 1!
2
MN
+λ4∑l E(1 - gx,φ(Z)i )	qi,m + gx,φ(Z)i	(1 - qi,m ) - Nmbm
m=1 i=0
qi,m = 0
, (8)
+
where the second row describes the simplex constraints, and the last row is the weak signal con-
straints. The [.]+ is a hinge function that returns its input if positive and zero otherwise. We omit
the expectation terms for simplicity.
4.2	Weakly Supervised Regression
For weakly supervised regression, we predict one-dimensional continuous labels y ∈ [0, 1] given
input dataset D = {x1, ..., xN} and weak signals Q. We define the weak signals as follows. For the
mth feature of input data, we have access to a threshold m, which splits D to two parts, i.e., D1, D2,
such that for each xi ∈ D1, the xi,m ≥ m, and for each xj ∈ D2, the xj,m < m. We also have
access to estimated values of labels for subsets D1 and D2, i.e., bm,1 and bm,2. This design of weak
signals tries to mimic that in practical scenarios, human experts can design rule-based methods for
predicting labels for given data. For example, in disease prediction, a medical doctor can predict
the disease rates for patients based on their ages. For a group of people whose age is greater than a
threshold, an experienced physician would know an estimate of their average disease rate. Assuming
that we have M rule-based weak signals, the constraints can be mathematically defined as follows:
1
∣Dm,1∣
yi
i∈Dm,1
KJi .X yj = M,
, j∈Dm,2
m ∈ 1,..., M.
(9)
Plugging in Equation 9 to Equation 6, we have
max
φ
K
log pZ (z) -	log
i=1
det三
+ λ1 [gx,φ(z)]2+ + λ2 [1 - gx,φ(z)]2+
+λ3 X(套,X gχ,φ(z)i- bm,1)+ (看,X gχ,φ(z)j-bmJ
m=1	m, i∈Dm,1	m, j∈Dm,2
(10)
4.3	Unpaired Point Cloud Completion
Unpaired point cloud completion (Chen et al., 2019; Wu et al., 2020) is a practical problem in 3D
scanning. Given a set of partial point clouds Xp = {x(1p) , ..., x(Np) }, and a set of complete point
clouds Xc = {x(1c), ..., x(Nc)}, we want to restore each xi(p) ∈ Xp. Each point cloud is a set of points,
4
Under review as a conference paper at ICLR 2022
i.e., xi = {xi,1 , ..., xi,T }, where each xi,t ∈ R3 is a 3D point, and the counts T represent the
number of points in a point cloud.
Note that the point clouds in Xp and Xc are unpaired, so directly modeling the relationship between
x(c) and x(p) is impossible. This problem can be interpreted as a weakly supervised learning prob-
lem, in which the weak signals Q are derived from the referred complete point clouds Xc. We predict
complete point clouds y ∈ Y for partial point clouds in Xp. The conditional distribution p(y|xp) is
an exchangable distribution. We follow previous works (Yang et al., 2019; Klokov et al., 2020) and
use De Finetti’s representation theorem and variational inference to compute its lower bound as the
objective.
Tc
logp(y|xp)	≥ Eq(u|xp)	logp(yi|u,xp) - KL(q(u|xp)||p(u)),	(11)
i=1
where q(u|xp) is a variational distribution of latent variable u. In practice, it can be represented by
an encoder, and uses the reparameterization trick (Kingma & Welling, 2013) to sample u. The p(u)
is a standard Gaussian prior. The p(yi |u, xp) is defined by a conditional flow. We follow Chen et al.
(2019); Wu et al. (2020) and use the adversarial loss and Hausdorff distance loss as constraints. The
final objective function is
max
φ
Eq(u∣χp) X log PZ (Zt)-X log det ( dgu‘xp'" )	- KL(q(u∣Xp)∣∣p(u))
t=1	i=1	∂rt,i
+Eq(u|xp) λ1(D(gu,xp ,φ(z)) - 1)2 + λ2dH(gu,xp ,φ(z), xp) ,	(12)
where D() represents the discriminator of a least square GAN (Mao et al., 2017), and dH() repre-
sents the Haudorsff distance, which measures the distance between a generated complete point cloud
its corresponding input partial point cloud. For clarity, we use zt and rt to represent variables of the
tth point in a point cloud, and gu,xp ,φ(z) to represent a generated point cloud. Detailed derivations
of Equation 12 are in appendix.
Training with Equation 12 is different from the previous settings, because we also need to train the
discriminator of the GAN. The objective for D() is
mDin Epdata(xc) (D(xc) - 1)	+ Epdata(xp),pZ (z),q(u|xp) D(gxp ,u,φ(z))	.	(13)
The training process is similar to traditional GAN training. The inverse flow gu,xp ,φ can be roughly
seen as the generator. In training, we train the flow to optimize Equation 12 and the discriminator to
optimize Equation 13, alternatively.
5	Related Work
In this section, we introduce the research that most related to our work.
Weakly Supervised Learning. For weakly supervised classification, we use the same strategy as
adversarial label learning (ALL) (Arachie & Huang, 2021b) to define constraint functions based on
weak signals. ALL then uses a min-max optimization to learn the model parameters and estimate y
alternatively. In contrast to ALL, LLF is a generative model, so it can learn the model parameters and
output y simultaneously, and it does not need a min-max optimization. Moreover, LLF optimizes the
likelihoods of all possible ys within Ω, while ALL only estimates one possible y. Other methods
also constrain the label space of the predicted labels using weak supervision (Arachie & Huang,
2021a;b; Mazzetto et al., 2021a;b). These methods are deterministic and developed for classification
tasks. However, LLF can be used for regression and uses sampling during inference.
Non-constraint based weak supervision methods typically assume a joint distribution for the weak
signals and the true labels of the data. These methods use a latent variable model to estimate the
labels of the data while accounting for the dependency among the weak signals (Ratner et al., 2016;
2019; Fu et al., 2020). Like these methods, we assume a family of distributions for the label space
of the data. This space is defined by the constraints of the weak supervision and the data. Unlike
these methods, we use a flow network rather than a graphical model to solve for the label of the
5
Under review as a conference paper at ICLR 2022
data. Additionally, we do not solve for the dependence amongst the weak signals thereby avoiding
the need for making extra assumptions.
Normalizing Flows. Normalizing flows (Dinh et al., 2014; Rezende & Mohamed, 2015; Dinh et al.,
2016; Kingma & Dhariwal, 2018) have gained recent attention because of their advantages of ex-
act latent variable inference and log-likelihood evaluation. Specifically, conditional normalizing
flows have been widely applied to many supervised learning problems (Trippe & Turner, 2018;
Lu & Huang, 2020; Lugmayr et al., 2020; Pumarola et al., 2020) and semi-supervised classifica-
tion (Atanov et al., 2019; Izmailov et al., 2020). However, normalizing flows have not previously
been applied to weakly supervised learning problems.
Our inverse training method for LLF is similar to that of injective flows (Kumar et al., 2020). Injec-
tive flows are used to model unconditional datasets. They use an encoder network to map the input
data x to latent code z, and then they use an inverse flow to map z back to x, resulting in an autoen-
coder architecture. Different from injective flow, LLF directly samples z from a prior distribution
and uses a conditional flow to map z back to y conditioned on x. We use constraint functions to
restrict y to be valid, so that does not need an encoder network.
Point Cloud Modeling. Recently, Yang et al. (2019) and Tran et al. (2019) combine normaliz-
ing flows with variational autoencoders (Kingma & Welling, 2013) and developed continuous and
discrete normalizing flows for point clouds. The basic idea of point normalizing flows is to use a
conditional flow to model each point in a point cloud. The conditional flow is conditioned on a la-
tent variable generated by an encoder. To guarantee exchangeability, the encoder uses a PointNet (Qi
et al., 2017) to extract features from input point clouds.
The unpaired point cloud completion problem is first defined by Chen et al. (2019). They develop
pcl2pcl—a GAN (Goodfellow et al., 2014) based model—to solve it. Their method is two-staged.
In the first stage, it trains autoencoders to map partial and complete point clouds to their latent
space. In the second stage, a GAN is used to transform the latent features of partial point clouds to
latent features of complete point clouds. In their follow-up paper (Wu et al., 2020), they develop
a variant of pcl2pcl, called multi-modal pcl2pcl (mm-pcl2pcl), which incorporates random noise to
the generative process, so that can capture the uncertainty in reasoning.
In contrast to pcl2pcl, LLF can be trained end-to-end. When applying LLF to this problem, LLF has
a similar framework to VAE-GAN (Larsen et al., 2016). The main differences are that LLF models
a conditional distribution of points, and its encoder is a point normalizing flow.
6	Empirical S tudy
In this section, we evaluate LLF on the three weakly supervised learning problems.
Model architecture. For weakly supervised classification and unpaired point cloud completion, the
labels y are multi-dimensional variables. We follow Klokov et al. (2020) and use flows with only
conditional coupling layers. We use the same method as Klokov et al. (2020) to define the condi-
tional affine layer. Each flow model contains 8 flow steps. For unpaired point cloud completion,
each flow step has 3 coupling layers. For weakly supervised classification, each flow step has 2
coupling layers. For weakly supervised regression, since y is a scalar, we use simple conditional
affine transformation as flow layer, which is defined as:y = S(X) * Z + b(x), where S and b are two
neural networks that take x as input and output parameters for y. The flow for this problem contains
8 conditional affine transformations.
For the unpaired point cloud completion task, we need to also use an encoder network, i.e., q(u|Xp)
and a discriminator D(). We follow Klokov et al. (2020); Wu et al. (2020) and use PointNet (Qi
et al., 2017) in these two networks to extract features for point clouds.
Experiment setup. In weakly supervised classification and regression experiments, we assume that
the ground truth labels are inaccessible, so tuning hyper-parameters for models are impossible. We
use default settings for all hyper-parameters of LLF, e.g., λs and learning rates. We fix λ = 10
and use Adam (Kingma & Ba, 2014) with default settings, i.e., η = 0.001, β1 = 0.9 and β2 =
0.999. We use an exponential learning rate scheduler with a decreasing rate of 0.996 to guarantee
convergence. We track the decrease of loss and when the decrease is small enough, the training
stops. Following previous works (Arachie & Huang, 2021b;a), we use full gradient optimization
6
Under review as a conference paper at ICLR 2022
to train the models. For fair comparison, we run each experiment 5 times with different random
seeds {0, 10, 100, 123, 1234}. We split each dataset to training, simulation, and test sets. We follow
Arachie & Huang (2021b;a) and create weak signals with randomly chosen features, and estimate
error bounds and thresholds on simulation set.
For experiments with unpaired point cloud completion, the datasets contain validation sets, so we
tune the hyper-parameters using these. We use Adam with an initial learning rate η = 0.0001 and
default βs. The best coefficients for the constraints in Equation 12 are λ1 = 10, λ2 = 100. We use
stochastic optimization to train the models, and the batch size is 32. Each model is trained for at
most 2000 epochs.
6.1	Weakly Supervised Classification
Datasets. We follow Arachie & Huang (2021a;b) and conduct experiments on 12 datasets. Specif-
ically, the Breast Cancer, OBS Network, Cardiotocography, Clave Direction, Credit Card, Statlog
Satellite, Phishing Websites, Wine Quality are tabular datasets from UCI repository (Dua & Graff,
2017). The Fashion-MNIST (Xiao et al., 2017) is an image set with 10 classes of clothing types. We
choose 3 pairs of classes, i.e., dresses/sneakers (DvK), sandals/ankle boots (SvA), and coats/bags
(CvB), to conduct binary classification. We follow Arachie & Huang (2021b) and create 3 synthetic
weak signals for each dataset. The IMDB (Maas et al., 2011), SST (Socher et al., 2013) and YELP
are real text datasets. We follow Arachie & Huang (2021a) and use keyword-based weak super-
vision. Each dataset has more than 10 weak signals. For the experiments on tabular datasets, we
set the maximum epochs to 2000. Since the experiments on real text datasets are larger, we set the
maximum epochs to 500.
Baselines. We compare our method with state-of-the-art methods for weakly supervised classifi-
cation. For the experiments on tabular datasets and image sets, we use ALL (Arachie & Huang,
2021b), generalized expectation (GE) (Druck et al., 2008; Mann & McCallum, 2010) and averaging
of weak signals (AVG). For experiments on text datasets, we use CLL (Arachie & Huang, 2021a),
Snorkel MeTaL (Ratner et al., 2019), Data Programming (DP) (Ratner et al., 2016), regularized min-
imax conditional entropy for crowdsourcing (MMCE) (Zhou et al., 2015), and majority-vote. We
also show supervised learning (SL) results for reference.
Results. We report the mean and standard deviation of accuracy on test sets in Table 1 and Table 2.
LLF outperforms all baseline methods on almost all datasets. On some datasets, LLF can perform
as well as supervised learning methods. These results prove that LLF is powerful and effective.
In our experiments, we also found that the performance of LLF will also be impacted by different
initialization of weights. This is why LLF has relatively larger variance on some datasets.
Table 1: Test set accuracy on tabular and image datasets. We report the mean accuracy of 5 experi-
ments, and the subscripts are standard deviation. LLF outperforms other baselines on 10 datasets.
	LLF	ALL	GE	AVG	SL
	 Fashion MNIST (DvK)	1.0000.000	O.995o.000	0.9790.000	0.8350.000	1.0000.000
Fashion MNIST (SvA)	0.944o.0θi	0.9080.000	0.5010.000	0.7910.000	0.972o.00o
Fashion MNIST (CvB)	0.9160.038	0.8050.000	0.5010.000	0.7400.000	0.9880.000
Breast Cancer	0.9680.008	0.9370.019	0.9330.016	0.9110.023	0.9730.007
OBS Network	0.684o.o06	0.6910.011	0.6760.010	0.7090.024	0.7040.032
Cardiotocography	0.9310.010	0.7950.011	0.6630.061	0.9020.047	0.9410.008
Clave Direction	0.8580.017	0.7500.013	0.7560.028	0.7070.003	0.9630.00i
Credit Card	0.6800.022	0.6780.021	0.4920.088	0.6020.010	0.7170.031
Statlog Satellite	0.9970.002	0.9590.008	0.9870.012	0.9150.011	0.9990.001
Phishing Websites	0.9060.003	0.8960.005	0.8700.009	0.8480.002	0.9290.00i
Wine Quality	0.6470.017	O.623o.000	0.4450.014	0.5550.000	0.6850.000
Ablation Study. We can directly train the model using only the constraints as the objective function.
In our experiments, we found that training LLF without likelihood (LLF-w/o-nll) will still work.
However, the model performs worse than training with likelihood. We believe that this is because
the likelihood helps accumulate more probability mass to the constrained space Ω, so the model will
more likely generate y samples within Ω, and the predictions are more accurate.
7
Under review as a conference paper at ICLR 2022
Table 2: Test set accuracy on real text datasets. LLF outperforms all other baselines.
		LLF	CLL	MMCE	DP	MV	MeTaL	SL
SST	0.766o.o02	0.7290.001	0.727	0.720o.0oi	0.7200.001	0.7280.001	0.792o.ooi
IMDB	O.804o.000	0.7400.005	0.551	0.6230.007	0.7240.004	0.7420.004	0.820o.o03
YELP	0.861o.00o	0.8400.001	0.680	0.7600.o05	0.7980.007	0.7800.002	0.879o.ooi
0.100
0.075
Epoch	Epoch
UoA-O_>
Epoch
Epoch
method — LLF (test) — LLF (train) — LLF-W/o-nll (test)— LLF-W/o-nll (train)
u。A-。_>
0.025
0.000
Violation of MNIST
Epoch
Figure 1: Evolution of accuracy, likelihood and violation of weak signal constraints. Training with
likelihood makes LLF accumulate more probability mass to the constrained space, so that the gen-
erated y are more likely to be within Ω, and the predictions are more accurate.
6.2	Weakly Supervised Regression
Datasets. We use 3 tabular datasets from the UCI repository Dua & Graff (2017): Air Quality,
Temperature Forecast, and Bike Sharing dataset. For each dataset, we randomly choose 5 features
to develop the rule based weak signals. We split each dataset to training, simulation, and test sets.
The simulation set is then used to compute the threshold s, and the estimated label values bs. Since
we do not have human experts to estimate these values, we use the mean value of a feature as its
threshold, i.e., em, = D~^ Pi∈pval,d Xi[m]. We then compute the estimated label values bm,,ι and
bm,2 based on labels in the valid set. Note that the labels in the simulation set are only used for
generating weak signals, simulating human expertise. In training, we still assume that we do not
have access to labels. The original label is within an interval [ly , uy]. We normalize the original
label to within [0, 1] by computing y = (y - ly)/(uy - ly). In prediction, we recover the predicted
label to original value by computing y = y(uy - ly ) + ly .
Baselines. To the best of our knowledge, there are no methods specifically designed for weakly
supervised regression of this form. We use average of weak signals (AVG) and LFF-w/o-nll as
baselines. We also report the supervised learning results for reference.
Results. We use root square mean error (RSME) as metric. The results of test set are in Table 3. In
general, LLF can predict reasonable labels. Its results are much better than AVG or any of the weak
signals alone. Similar to the classification results, training LLF without using likelihood will reduce
its performance.
Table 3: Test set RMSE of different methods. The numbers in brackets indicate the label’s range.
LLF outperforms other baselines on all datasets.
		LLF	LLF-w/o-nll	AVG	SL
Air Quality (0.1847 〜2.231)	0.2110.009	0.2660.004	0.373o.o05	0.1230.002
Temperature Forecast (17.4 〜38.9)	2.552o.05o	2.6560.055	2.827o.o27	1.4650.031
Bike Sharing (1 〜999)	157.348o.54i	162.6971.585	171.338i.3oo	141.9201.280
8
Under review as a conference paper at ICLR 2022
6.3	Unpaired Point Cloud Completion
Datasets. We use the Partnet (Mo et al., 2019) dataset in our experiments. We follow Wu et al.
(2020) and conduct experiments on the 3 largest classes of PartNet: Table, Chair, and Lamp. We
treat each class as a dataset, which is split to training, validation, and test sets based on official splits
of PartNet. For each point cloud, we remove points of randomly selected parts to create a partial
point cloud. We follow Chen et al. (2019); Wu et al. (2020) and let the partial point clouds have
1024 points, and the complete point clouds have 2048 points. We let the latent variable u of the
VAE to be a 128-dimensional vector.
Metrics. We follow Wu et al. (2020) and use minimal matching distance (MMD) (Achlioptas et al.,
2018), total mutual difference (TMD), and unidirectional Hausdorff distance (UHD) as metrics.
MMD measures the quality of generated. A lower MMD is better. TMD measures the diversity of
samples. A higher TMD is better. UHD measures the fidelity of samples. A lower UHD is better.
Baselines. We compare our method with pcl2pcl (Chen et al., 2019), mm-pcl2pcl (Wu et al., 2020),
and LLF-w/o-nll. We use two variants of mm-pcl2pcl. Another variant is called mm-pcl2pcl-im,
which is different from the original model in that it jointly trains the encoder of modeling multi-
modality and the GAN.
Table 4: Evaluation results on three classes of PartNet. LLF performs comparable to baselines.
PartNet	∣		Chair	I	Lamp	J					Table	
I MMD；		TMD↑	UHDJ I MMDJ		TMD↑	UHDJ I MMDJ		TMD↑	UHDJ
LLF	1.72	0.63	5.74	2.11	0.57	4.71	1.57	0.55	5.42
LLF-w/o-nll	1.79	0.47	5.49	2.21	0.41	4.61	1.57	0.43	5.13
pcl2pcl	1.90	0.00	4.88	2.50	0.00	4.64	1.90	0.00	4.78
mm-pcl2pcl	1.52	2.75	6.89	1.97	3.31	5.72	1.46	3.30	5.56
mm-pcl2pcl-im	1.90	1.01	6.65	2.55	0.56	5.40	1.54	0.51	5.38
Results. We list the test set results in Table 4. In general, pcl2pcl has the best fidelity, i.e., lowest
UHD. This is because pcl2pcl is a discriminative model, and it will only predict one certain sample
for each input. This is also why pcl2pcl has the worse diversity as measured by TMD. Mm-pcl2pcl
has the best diversity. However, we found in our experiments that some samples generated by mm-
pcl2pcl are invalid, i.e., they are totally different from the input partial point clouds. Therefore,
mm-pcl2pcl has the worse fidelity. LLF scores between pcl2pcl and mm-pcl2pcl. It has better UHD
than mm-pcl2pcl, and better TMD and MMD than pcl2pcl. The LLF-w/0-nll has a slightly better
UHD than LLF. We believe this is because, without using the likelihood, LLF-w/o-nll is trained
directly by optimizing the Hausdorff distance. However, the sample diversity and quality, i.e., TMD
and MMD, are worse than LLF. As argued by Yang et al. (2019), the current metrics for evaluating
point cloud samples all have flaws, so these scores cannot be treated as hard metrics for evaluating
model performance. We therefore visualize some samples in appendix, which show that LLF can
generate samples that comparable to mm-pcl2pcl.
7	Conclusion
In this paper, we propose label learning flows, which represent a general framework for weakly
supervised learning. LLF uses a conditional flow to define the conditional distribution p(y|x), so
that can model the uncertainty between input x and all possible y. Learning LLF is a constrained
optimization problem that optimizes the likelihood of all possible y within the constrained space
defined by weak signals. We develop a specific training method to train LLF inversely, avoiding
the need of estimating y. We apply LLF to three weakly supervised learning problems, and the
results show that our method outperforms many state-of-the-art baselines on the weakly supervised
classification and regression problems, and performs comparably to other new methods for unpaired
point cloud completion. These results indicate that LLF is a powerful and effective tool for weakly
supervised learning problems.
9
Under review as a conference paper at ICLR 2022
References
Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representa-
tions and generative models for 3d point clouds. In International conference on machine learning,
pp. 40-49. PMLR, 2018.
Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. In Inter-
national Conference in Uncertainty in Artificial Intelligence, 2021a.
Chidubem Arachie and Bert Huang. A general framework for adversarial label learning. Journal of
Machine Learning Research, 22(118):1-33, 2021b.
Andrei Atanov, Alexandra Volokhova, Arsenii Ashukha, Ivan Sosnovik, and Dmitry Vetrov. Semi-
conditional normalizing flows for semi-supervised learning. arXiv preprint arXiv:1905.00505,
2019.
Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik
Sen, Alex Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in de-
ploying weak supervision at industrial scale. In Proceedings of the 2019 International Conference
on Management of Data, pp. 362-375, 2019.
Xuelin Chen, Baoquan Chen, and Niloy J Mitra. Unpaired point cloud completion on real scans
using adversarial training. arXiv preprint arXiv:1904.00069, 2019.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. arXiv
preprint arXiv:1605.08803, 2016.
Gregory Druck, Gideon Mann, and Andrew McCallum. Learning from labeled features using gen-
eralized expectation criteria. In Proceedings of the 31st annual international ACM SIGIR confer-
ence on Research and development in information retrieval, pp. 595-602, 2008.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher Re. Fast
and three-rious: Speeding up weak supervision with triplet methods. In International Conference
on Machine Learning, pp. 3280-3291. PMLR, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-
based generative models with variational dequantization and architecture design. arXiv preprint
arXiv:1902.00275, 2019.
Pavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew Gordon Wilson. Semi-supervised learn-
ing with normalizing flows. In International Conference on Machine Learning, pp. 4615-4630.
PMLR, 2020.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
Roman Klokov, Edmond Boyer, and Jakob Verbeek. Discrete point flow networks for efficient point
cloud generation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK,
August23-28, 2020, Proceedings, PartXXIII16,pp. 694-710. Springer, 2020.
10
Under review as a conference paper at ICLR 2022
Abhishek Kumar, Ben Poole, and Kevin Murphy. Regularized autoencoders via relaxed injective
probability flow. In International Conference on Artificial Intelligence and Statistics, pp. 4292-
4301. PMLR, 2020.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. AUtoen-
coding beyond pixels using a learned similarity metric. In International conference on machine
learning, pp. 1558-1566. PMLR, 2016.
You Lu and Bert Huang. Structured output learning with conditional generative flows. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 5005-5012, 2020.
Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and Radu Timofte. Srflow: Learning the super-
resolution space with normalizing flow. In European Conference on Computer Vision, pp. 715-
732. Springer, 2020.
Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies, pp. 142-150, 2011.
Gideon S Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised learn-
ing with weakly labeled data. Journal of machine learning research, 11(2), 2010.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE international confer-
ence on computer vision, pp. 2794-2802, 2017.
A. Mazzetto, C. Cousins, D. Sam, S. H. Bach, and E. Upfal. Adversarial multiclass learning under
weak supervision with performance guarantees. In International Conference on Machine Learn-
ing (ICML), 2021a.
Alessio Mazzetto, Dylan Sam, Andrew Park, Eli Upfal, and Stephen Bach. Semi-supervised ag-
gregation of dependent weak supervision sources with performance guarantees. In International
Conference on Artificial Intelligence and Statistics, pp. 3196-3204. PMLR, 2021b.
Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao
Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object under-
standing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 909-918, 2019.
Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, and Vittorio Ferrari. C-flow: Condi-
tional generative flow models for images and 3d point clouds. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 7949-7958, 2020.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 652-660, 2017.
Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Re.
Snorkel: Rapid training data creation with weak supervision. In Proceedings of the VLDB En-
dowment. International Conference on Very Large Data Bases, volume 11, pp. 269. NIH Public
Access, 2017.
Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christo-
pher Re. Training complex models with multi-task weak supervision. In Proceedings ofthe AAAI
Conference on Artificial Intelligence, volume 33, pp. 4763-4771, 2019.
Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher Re. Data
programming: Creating large training sets, quickly. Advances in neural information processing
systems, 29:3567-3575, 2016.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
11
Under review as a conference paper at ICLR 2022
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing ,pp.1631-1642, 2013.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.
Dustin Tran, Keyon Vafa, Kumar Krishna Agrawal, Laurent Dinh, and Ben Poole. Discrete flows:
Invertible generative models of discrete data. arXiv preprint arXiv:1905.10347, 2019.
Brian L Trippe and Richard E Turner. Conditional density estimation with Bayesian normalising
flows. arXiv preprint arXiv:1802.04908, 2018.
Rundi Wu, Xuelin Chen, Yixin Zhuang, and Baoquan Chen. Multimodal shape completion via
conditional generative adversarial networks. arXiv preprint arXiv:2003.07717, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan.
Pointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 4541-4550, 2019.
Dengyong Zhou, Qiang Liu, John C Platt, Christopher Meek, and Nihar B Shah. Regularized
minimax conditional entropy for crowdsourcing. arXiv preprint arXiv:1503.07240, 2015.
Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 5(1):
44-53, 2018.
A Proof of Theorem 1
The proof of Theorem 1 is similar to the proof of dequantization (Theis et al., 2015; Ho et al., 2019).
The complete proof is as follows.
Proof.
Epdata(X)Ey 〜U (Ω*) [log P5x, φ)]
X pdata (x) / Q ∣Ω1*i logp(y|X)dy
≤
M X pdata (x) log	p(y|x)dy
Mp
data (χ, y) log q(y ∣χ)
χ,y
MEpdataGM [log 9团X)]
(14)
In the second row, We use the properties that ∣Ω*j ≤ M, and the Jensen,s inequality. In the third row,
We use the assumption thatPdata(X) = Pdata(x, y), and the relationship between p(y∣x) and q(y|x).
B Label Learning Flow for Unpaired point Cloud Completion
In this section, we provide complete derivations of LLF for unpaired point cloud completion. The
conditional likelihood log P(y|xp) is an exchangeable distribution. We use De Finetti’s representa-
tion theorem and variational inference to derive a tractable lower bound for it.
12
Under review as a conference paper at ICLR 2022
logp(y|xp) =	p(y, u|xp)du
=	p(y|u, xp)p(u)du
≥ Eq(u|xp) [logp(y|u,xp)] - KL(q(u|xp)||p(u))
Tc
≥ Eq(u|xp)	logp(yi|u,xp) - KL(q(u|xp)||p(u)),	(15)
i=1
where in the third row, we use Jensen’s inequality to compute the lower bound, and in the last row,
we use De Finetti’s theorem to factorize p(y|u, xp) to the distributions of points.
The least square GAN discriminator and the Hausdorff distance for generated complete point clouds
can be treated as two equality constraints
D(y) = 1
dH (y, xp)	= 0.
Note that the dH () is non-negative. Convert these two constraints to penalty functions, we have
max
φ
Eq(u∣xp) XlogPZ(Zt) — Xlog det	— KL(q(u∣Xp)∣∣p(u))
t=1	i=1	t,i
+Eq(u|xp) λ1(D(gu,xp,φ(Z)) - 1)2 + λ2dHL(gu,xp,φ(z), Xp) .	(16)
C Experiment Details
In this section, we provide more details on our experiments to help readers reproduce our results.
C.1 Model Architecture
For experiments of weakly supervised classification, and unpaired point cloud completion, we use
normalizing flows with only conditional affine coupling layers Klokov et al. (2020). Each layer is
defined as
ya, yb = split(y)
s = ms(wy(ya)	wx(X) + wb(X))
b = mb(cy(ya)	cx(X) + cb(X))
Zb = s yb + b
Z = concat(ya,Zb),	(17)
where m, w, c are all small neural networks.
For LLF, we only need the inverse flow, i.e., gx,φ, for training and prediction, so in our experiments,
we actually define gx,φ as the forward transformation, and let y = s Z + b. We do this because
multiplication and addition are more stable than division and subtraction.
C.1.1 Weakly Supervised Classification
In this problem, we use a flow with 8 flow steps, and each step has 2 conditional affine coupling
layers. These two layers will transform different dimensions. Each w and c are small MLPs with
two linear layers. Each m has one linear layer. The hidden dimension of linear layers is fixed to 64.
C.1.2 Weakly Supervised Regression
In this problem, since the label y is a scalar, we use conditional affine transformation introduced in
Section 6, as a flow layer. A flow has 8 flow layers. The s and b in a flow layer are three layer
MLPs. The hidden dimension of linear layers is 64.
13
Under review as a conference paper at ICLR 2022
C.1.3 Unpaired Point Cloud Completion
The model architecture used LLF used for this problem is illustrated in Figure 2. We use the same
architecture as DPF (Klokov et al., 2020) for point flow. Specifically, the flow has 8 flow steps,
and each step has 3 conditional affine coupling layers, i.e., Equation 17. Slightly different from the
original DPF, the conditioning networks wx, cx, wb, and cb will take the latent variable u and the
features of partial point cloud xp as input. The ws and cs are MLPs with two linear layers, whose
hidden dimension is 64. The ms are one layer MLPs.
We use a PointNet (Qi et al., 2017) to extract features from partial point cloud xp. Following Klokov
et al. (2020), the hidden dimensions of this PointNet is set as 64-128-256-512. Given the features
of xp, the encoder E then uses the reparameterization trick (Kingma & Welling, 2013) to generate
latent variable u. The encoder has two linear layers, whose hidden dimension is 512.
The GAN discriminator uses another PointNet to extract features from (generated) complete point
clouds. We follow Wu et al. (2020) and set the hidden dimensions of this PointNet as 64-128-128-
256- 128. The discriminator D is a three layer MLP, whose hidden dimensions are 128-256-512.
Figure 2: Model architecture of LLF for unpaired point cloud completion. The E represents the
encoder, and the D represents the GAN discriminator.
C.2 More Experiment Details
C.2. 1 Weakly Supervised Classification
We use the same way as Arachie & Huang (2021b;a) to split each dataset to training, simulation,
and test sets. We use the data and labels in simulation sets to create weak signals, and estimated
bounds. We train models on training sets and test model on test sets. We assume that the models do
not have access to any labels. The labels in simulation sets are only used to generated weak signals
and estimate bounds.
For experiments on Fashion-MNIST and the tabular datasets, we follow Arachie & Huang (2021b)
and choose 3 features to create weak signals. We train a logistic regression with each feature on the
simulation set, and use the label probabilities predicted by this logistic regression as weak signals.
We compute the error of this trained logistic regression on simulation set as estimated error bound.
For experiments on real text datasets, we use the same keyword-based method as Arachie & Huang
(2021a) to create weak supervision. Specifically, we choose key words that can weakly indicate
positive and negative sentiments. Documents containing possitive words will be labeled as positive,
and vice versa.
We list some main features of these datasets in Table 5. We refer to their original papers for more
details. For those datasets without official splits, we randomly split them with a ratio of 4 : 3 : 3.
14
Under review as a conference paper at ICLR 2022
Table 5: Summary of datasets used in weakly supervised classification experiments. The “—
indicates this dataset does not have a official split
Dataset	Size	Train Size	Test Size	No. features	No. weak signals
Fashion MNIST (DvK)	14, 000	12,000	2, 000	784	3
Fashion MNIST (SvA)	14, 000	12, 000	2, 000	784	3
Fashion MNIST (CvB)	14, 000	12, 000	2, 000	784	3
Breast Cancer	569	—	—	30	3
OBS Network	795	—	—	21	3
Cardiotocography	963	—	—	21	3
Clave Direction	8, 606	—	—	16	3
Credit Card	1, 000	—	—	24	3
Statlog Satellite	3, 041	—	—	36	3
Phishing Websites	11, 055	—	—	30	3
Wine Quality	4, 974	—	—	11	3
IMDB	49, 574	29, 182	20, 392	300	10
SST	5, 819	3, 998	1, 821	300	14
YELP	55, 370	45, 370	10, 000	300	14
C.2.2 Weakly Supervised Regression
We use three datasets from the UCI repository. For each dataset, we randomly split it to training,
simulation, and test sets with a ratio of 4 : 3 : 3. We use the simulation set to create weak signals and
estimated label values. We choose 5 features to create weak signals for each dataset. The detailed
introduction of these datasets are as follows. Table 6 summarize the statistical results of them.
Air Quality. In this task, we predict the absolute humidity in air, based on other air quality features
such as hourly averaged temperature, hourly averaged NO2 concentration etc. The raw dataset has
9, 358 instances. We remove those instances with Nan values, resulting in a dataset with 8, 991 in-
stances. We use hourly averaged concentration CO, hourly averaged Benzene concentration, hourly
averaged NOx concentration, tungsten oxide hourly averaged sensor response, and relative humidity
as features for creating weak signals.
Temperature Forecast. In this task, we predict the next day maximum air temperature based on
current day information. The raw dataset has 7, 750 instances, and we remove those instances with
Nan values, resulting in 7, 588 instances. We use present max temperature, forecasting next day
wind speed, forecasting next day cloud cover, forecasting next day precipitation, solar radiation as
features for creating weak signals.
Bike Sharing. In this task, we predict the count of total rental bikes given weather and date in-
formation. The raw dataset has 17, 389 instances, and we remove those instances with Nan values,
resulting in 17, 379 instances. We use season, hour, if is working day, normalized feeling tempera-
ture, and wind speed as features for creating weak signals.
Table 6: Summary of datasets used in weakly supervised regression experiments
Dataset	Size	No. features	No. weak signals
Air Quality	8, 991	12	5
Temperature Forecast	7, 588	24	5
Bike Sharing	17, 379	12	5
C.2.3 Unpaired Point Cloud Completion
Datasets. We use the same way as Wu et al. (2020) to process PartNet Mo et al. (2019). PartNet
provides point-wise semantic labels for point clouds. The original point clouds are used as complete
point clouds. To generate partial point clouds, we randomly removed parts from complete point
15
Under review as a conference paper at ICLR 2022
clouds, based on the semantic labels. We use Chair, Table, and Lamp categories. The summary of
these three subsets are in Table 7.
Table 7: Summary of datasets used unpaird point cloud completion experiments
Dataset	Train Size	Valid Size	No. Test Size
Chair	4, 489	617	1, 217
Table	5, 707	843	1, 668
Lamp	1, 545	234	416
Metrics. Let Xc be the set of referred complete point clouds, and Xp be the set of input par-
(p)
tial point clouds. For each partial point cloud xi , we generate M complete point cloud samples
yi(1), ..., yi(M). All these samples form a new set of complete point clouds Y. In our experiments,
we follow Wu et al. (2020) and set M = 10.
The MMD Achlioptas et al. (2018) is defined as
MMD =j X de(xi, NN(Xi)),
(18)
where NN(x) is the nearest neighbor of x in Y. The dC represents Chamfer distance. MMD
computes the distance between the set of generated samples and the set of target complete shapes,
so it measures the quality of generated.
The TMD is defined as
|Xp |	M M
TMD = i⅛XlM2-1XX de(y(j),y(k))∣∙
p i=1	j=1 k=j+1
(19)
TMD measures the difference of generated samples given an input partial point cloud, so it measures
the diversity of samples.
The UHD is defined as
|Xp|	M
UHD = ɪ X ∣ M X dH (χi, y(j)))，
(20)
where dH represents the unidirectional Hausdorff distance. UHD measures the similarity between
generated samples and input partial point clouds, so it measures the fidelity of samples.
Samples. We compare LLF to mm-pc2pc in Figure 3, and more samples of LLF in Figure 4,
Figure 5, and Figure 6. LLF can generate samples that are as good as mm-pc2pc. Mm-pc2pc has a
higher diversity in samples, but some generated samples may be unreasonable.
16
Under review as a conference paper at ICLR 2022
而一tedU.-- OaWOaiEE
Figure 3: Random sample point clouds generated LLF and mm-pc2pc. The point clouds generated
by LLF are as realistic as mm-pc2pc. Mm-pc2pc has a higher diversity in samples. However,
sometimes it may generate unreasonable or invalid shapes.
Figure 4: Random chair samples generated by LLF. The first row is partial point clouds, and the
second row is generated complete point clouds.
Figure 5: Random lamp samples generated by LLF.
17
Under review as a conference paper at ICLR 2022
Figure 6: Random table samples generated by LLF.
18