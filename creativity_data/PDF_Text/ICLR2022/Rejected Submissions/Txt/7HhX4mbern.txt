Under review as a conference paper at ICLR 2022
Randomized Signature Layers
for Signal Extraction in Time Series Data
Anonymous authors
Paper under double-blind review
Ab stract
Time series analysis is a widespread task in Natural Sciences, Social Sciences,
and Engineering. A fundamental problem is finding an expressive yet efficient-to-
compute representation of the input time series to use as a starting point to perform
arbitrary downstream tasks. In this paper, we build upon recent works that use the
Signature of a path as a feature map and investigate a computationally efficient
technique to approximate these features based on linear random projections. We
present several theoretical results to justify our approach and empirically validate
that our random projections can effectively retrieve the underlying Signature of
a path. We show the surprising performance of the proposed random features on
several tasks, including (1) mapping the controls of stochastic differential equa-
tions to the corresponding solutions and (2) using the Randomized Signatures as
time series representation for classification tasks.When compared to correspond-
ing truncated Signature approaches, our Randomizes Signatures are more compu-
tationally efficient in high dimensions, and often lead to better accuracy and faster
training. Besides providing a new tool to extract Signatures and further validating
the high level of expressiveness of such features, we believe our results provide
interesting conceptual links between several existing research areas, suggesting
new intriguing directions for future investigations.
1	Introduction
Modeling time series is a common task in finance, physics, and engineering. A frequent challenge is
finding a transformation mapping a newly observed time series into a target one (seq2seq modeling)
or into a label summarizing its salient properties (classification). In the absence of any principled
model describing such a mapping, one has to infer it from data. The last few years have witnessed the
rise of deep neural networks, which have found successful application to problems involving time
series in numerous domains (Fawaz et al., 2019; Gamboa, 2017). Nevertheless, their outstanding
performance comes at the price of over-parametrization, data hungriness, and expensive training
cost (Werbos, 1990; Brown et al., 2020; Teubert et al., 2021; Neyshabur et al., 2018; Marcus, 2018).
Furthermore, even if sufficient data is available, the resulting models learn representations of the
input data that are highly specialized to the training task and difficult to adapt in different contexts. In
addition, the remarkable performance of these methods is often the result of a substantial engineering
effort and is not supported by theoretical results.
Reservoir Computing (RC) (Schrauwen et al., 2007) offers an intriguing alternative strategy to cope
with the limitations above, yet retaining the universal approximation properties typical of deep neu-
ral networks. In RC, the learning is divided into two phases: first, data is passed through an untrained
reservoir which extracts a set of task-independent features; second, a simple and efficient-to-train
linear map (the readout map) projects such features into the desired output. The critical point is that
the design of the reservoir determines the expressiveness of the features, and several alternatives can
be found in the literature (see (Gauthier et al., 2021) and references therein).
A powerful reservoir is offered by the Signature Transform, often simply referred to as Signature,
stemming from rough path theory (Ben Hambly, 2010; Friz and Hairer, 2020). The Signature of a
path consists in enhancing the path with additional curves, which, in the smooth case, corresponds
to iterated integrals of the curve with itself. A profound mathematical result (Levin et al., 2013)
guarantees that the solution of a (rough) differential equation can be approximated arbitrarily well
1
Under review as a conference paper at ICLR 2022
Figure 1: Reservoir computing via random Signature layers. Given an input time-series, our approach can be
seen as random Signature layer, acting as a reservoir, extracting a set of random features preserving the strong
theoretical properties of the Signature of the input path. A simple linear readout can the be applied to the so
extracted features to perfrom arbitrarily downstream tasks.
by a linear map of the Signature of the input signals (a.k.a. controls). In the machine learning jargon,
the Signature of a path can be interpreted as a feature map extracting all its geometrical properties
and thus allowing a simple linear map to approximate any function of it. On the other hand, itis often
the case that the reservoir features are very high dimensional, and hence are particularly expensive
to calculate and use in downstream tasks. In addition, the high-dimensionality of the Signature
reservoir poses additional challenges for modern gradient-based optimizers as convergence rates
suffer from a linear dependence in the model dimension (Bottou et al., 2018).
Inspired by the remarkable theoretical properties of the Signature reservoir and motivated to fix
its practical pitfalls, our contribution is to showcase the effectiveness of Randomized Signatures
(Cuchiero et al., 2021b;a), a recently introduced reservoir of random features. These features prov-
ably hold the same geometric properties and approximation power as the Signature, yet are often
more efficient to compute and of lower dimensionality. To extract Randomized Signatures, we nu-
merically integrate a set of random linear stochastic differential equations driven by the path itself.
We assess the expressiveness of the proposed random features on several tasks, including non-
parametric (black-box) system identification problems arising from complex controlled nonlinear
dynamical systems as well as time series classification.
The paper is structured as follows: In Section 2, we link the Randomized Signature approach with
related research areas in the literature; in Section 3, we present relevant theoretical results motivating
our method; in Section 4, we assess the effectiveness of Randomized Signatures on various system
identification and time series classification tasks, showing promising results when compared to other
signature approaches for reservoir computing. Finally, in Section 5, we discuss our conclusions and
possible future developments.
2	Related Works
Random Features and Reservoir Computing. The idea of extracting features based on random
operations is not new and has seen a number of successful applications over the past years. Of
particular note, the seminal work of Rahimi and Recht (2008) proposes to accelerate kernel machines
by learning random features whose inner product matches that ofa target shift-invariant kernel. The
trade-off between generalization and computational efficiency of learning with random features has
then been rigorously studied by Rudi and Rosasco (2017). A conceptually very similar rationale
is introduced by a parallel series of works exploring the topic of Reservoir Computing (Schrauwen
et al., 2007). Similarly to us, Echo State Networks (Jaeger, 2003) evolve the input state by a series
of fixed random projection (the reservoir) and generate the output by applying a trainable linear
projection over the hidden states. However, we make the additional step of linking the random
features to the Signature of the input path and, as shown below, the evolution of the features is
dictated by a randomly-evolved stochastic differential equation driven by the input path.
Controlled Differential Equations. Our work is also related with a series of recent papers inves-
tigating the problems of how to process irregular time series and to condition a model on incoming
information through the lens of controlled differential equations (CDEs) (Kidger et al., 2020; Mor-
rill et al., 2021). Interestingly, Kidger et al. (2020) show that the action of a linear layer on the final
2
Under review as a conference paper at ICLR 2022
output of a trained Neural CDEs - the extension of Neural ODEs (Chen et al., 2018) to CDEs -
results in a universal function approximator. Differently from them, our method is less computa-
tionally expensive as the only parameters we need to train are those of the final linear readout layer.
Fundamentally different to our work, the approach of Morrill et al. (2020) first randomly projects the
possibly high dimensional controls into a lower dimensional space and then extracts the Truncated
Signature from such compressed input. Our method instead extracts a random compression of the
signature of the original input controls.
Rough Path Theory. Rough path theory is about describing how possibly highly oscillatory
(rough) control path interact with nonlinear systems (Lyons, 2014). The concept of Signature is
introduced in this context to provide a powerful description of the input path, removing the need to
look at the fine structure of the path itself. Recent years have seen a resurgence of these ideas, which
have been revisited from a machine learning perspective (Bonnier et al., 2019; Kidger and Lyons,
2021). Our analysis is strongly influenced by the work of Cuchiero et al. (2021a;b), who, starting
from the observation that the Signature is an infinite dimensional reservoir, establishes that its in-
formation content can be efficiently compressed by a random projection performed by a dynamical
system driven by random vector fields. As we show later, this results in a random reservoir which
preserves the properties of the Signature while living in a finite dimensional space.
3	Background
We provide the theoretical tools supporting the Randomized Signature approach for feature extrac-
tion in times series. A formal discussion can be found in (Friz and Hairer, 2020) and (Cuchiero
et al., 2021b).
3.1	Rand omized Signature of a Path
Let X : [0, T] → Rd be a continuous piecewise smooth d-dimensional path X = (X1,…，Xd).
We will refer to X as the control and to its single components Xi as controls. We denote by
{e1, . . . , ed} the canonical basis of Rd.
Definition 1 (Signature) For any t ∈ [0, T], the Signature of a continuous piecewise smooth path
X : [0, T] → Rd on [0, t] is the countable collection St :=(1, S1, S2,..∙) ∈ Q∞=0 (RdLk where,
for each k ≥ 1, the entries Stk are the iterated integrals defined as
Sk=	X	(/	dXSl …dXSk) eiι 乳…乳 eik.	(1)
(iι,...,ik)∈{i,...,d}k 'J0≤sι≤…≤sk≤t	)
We define the Truncated Signature of X of order M ≥ 0 as
M
SM ：= (ι,St1,…,sM) ∈ γ (Rd)0k =: TM (Rd).	⑵
k=0
A practical example of computation of Signatures is presented in Appendix A.3.
The definition in the last paragraph suggests that the Signatures of X can be used to approximate
any regular enough function of X , for instance the solution to differential equations controlled by
X . The following result makes this argument precise in the multidimensional setting.
Theorem 1 (Signature is a Reservoir) Let Vi : Rm → Rm, i = 1, . . . , d be vector fields regular
enough s.t. dYt = Pid=1 Vi(Yt) dXti, Y0 = y ∈ Rm, admits a unique solution Yt : [0, T] → Rm.
Then, for any smooth test function F : Rm → R and for every M ≥ 0 there is a time-homogeneous
linear operator L : TM Rd → R which depends only on (V1, . . . , Vd, F, M, y) s.t.
F (Yt) = L (SM) + O (tM+1), t ∈ [0,T].	(3)
This theorem suggests the first M entries of the Signature ofX are sufficient to linearly explain the
solution of any differential equation driven by it. In addition to this, it supports the claim that such
features are valuable for any downstream usage.
3
Under review as a conference paper at ICLR 2022
S1, First Signature
S21 Second Signature
S81 Third Sigoature
Z1, Randomized Sigoatuie
Za, Randomized Signature
Z31 Randomized Signature
Figure 2: To enhance intuition, We show the Signatures and Randomized Signatures for X (smoothed BroW-
nian Motion). Note that Xt = St1 + X0 for each t, and that S2 and S3 get smaller and smaller in magnitude,
yet more and more regular (smoother, as a result of iterated integrals). The Randomized Signatures resemble a
mix (i.e. a random linear combination of) the true Signatures: Z1 resembles a shifted version of -S1, While Z2
resembles a combination of S1 and S3 . Indeed, Thm. 2 guarantees that Randomized Signatures approximate
the full Signature With no need for computation of iterative integrals. We validate this experimentally in Sec. 4.
3.2	Randomized Signature of a Path and its theoretical guarantees
Calculating SM requires the calculation of dM-1-1 iterated integrals (Appendix A.3) - which in
total quickly becomes computationally expensive. Several computational techniques have been de-
veloped to circumvent this problem, see, e.g. (Kidger and Lyons, 2021). The next result provides
a practical description of how it is possible to reduce the computational burden without losing too
much explanatory power. As such, this results provides the theoretical foundation for our approach.
Theorem 2 (Randomized Signature (Informal)) For any k ∈ N big enough and appropriately
chosen random matrices A1, . . . , Ad in Rk×k, random shifts b1, . . . , bd in Rk×1, random starting
point z in Rk×1, and any fixed activation function σ, the solution of
d
dZt =Xσ(AiZt+bi)dXti,	Z0	=z ∈	Rk,	t∈	[0,T].	(4)
i=1
一 called the Randomized Signature of X - has comparable approximation power as the Signature
itself and maintains its geometric properties.
The formal statement of such a result is the combination of Theorem 8, Theorem 9, and Definition
10 (see Appendix). A complete formal discussion as well as the original statement of this theorem
can be found in the paper (Cuchiero et al., 2021a). In a nutshell, the Randomized Signature Z
follows a dynamics which provides an efficient and powerful compression of the Signature through
a low-dimensional random projection. The expressiveness of such projection is guaranteed by a
standard Johnson-Lindenstrauss argument (Cuchiero et al., 2021a).
Computational complexity and dimensionality of Randomized Signatures. The computational
complexity of calculating Zt is O(k2d), while its dimensionality is O(k). However, we should then
ask the crucial question “how should we compare k with M ?”. We show experimentally in Sec-
tion 4 that, in order to match the approximation guarantees of the Truncated Signature of order M,
the number of required Randomized Signatures k is not too big — in particular it is not exponen-
tial in M . This confirms that working with Randomized Signatures is often less computationally
demanding and results in lower-dimensional features.
We conclude the section remarking the fascinating implications of choosing a random Ais and bis.
Theorem 3 (Density of Randomized Signatures (Informal)) For any sequences of time points
0 ≤ t0 < . . . < tN ≤ T and points zt0, . . . , ztN ∈ Rk we can find a smooth control X such
that the solution Z of Equation 4 at time ti is such that Zti = zti,for i = 0, . . . , N. Additionally, if
the control X is a d-dimensional Brownian motion, then the solution of Equation 4 at any point in
time t > 0 admits a smooth density with respect to Lebesgue measure.
4	The Expres sive Power of Randomized Signatures
To compute the Randomized Signature, we rely on an Euler-Maruyama approximation; details are
given in Algorithm 1. To start, we experimentally validate some of the results presented above. In
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Generate Randomized Signature
Require: X ∈ Rd sampled at 0 = to < •… < tN = T, Randomized Signature dimension k,
activation function σ.
Initialize: Z0 ∈ Rk, Ai ∈ Rk×k, bi ∈ Rk to have iid standard normal entries
for n in {1..N} do
Ztn= Ztn-1 + Pd=1 σ (AiZtn^-1 + bi) (Xtn- Xtn-I)
end for
particular, we show that the Truncated Signature of a control lies in the linear span of its Randomized
Signatures, thus validating Theorem 2. As a second proof of concept, we reconstruct a control X as
a linear combination of its Randomized Signatures, showcasing the expressiveness of such random
features. Finally, we combine Theorem 1 and Theorem 2 and we use Z to learn the dynamics of
complex nonlinear systems of differential equations and to perform time series classification. Our
result show that the Randomized Signature is a powerful random reservoir which guarantees —
both in theory and practice — the same geometrical properties and approximation power as the
Signature itself, while often being of lower dimensionality and less computationally expensive both
at extraction and at deployment time.
Note on the choice of σ. While the choice of σ does not affect the theoretical results (Cuchiero
et al., 2021a;b), we found that a careful choice is needed for optimal results. Inspired by seminal
works on the stability of deep linear networks (Glorot and Bengio, 2010) and by the connection
to neural ODEs (Chen et al., 2018), We selected σ to be linear, with slope d√^. This choice,
complemented with additional fine-tuning, leads to the best performance for downstream tasks.
4.1	Learning the S ignatures
In this experiment, we show that we can approximate the Truncated Signature of a control us-
ing a linear combination of its Randomized Signatures. We fix an equally-spaced partition D =
{to, ∙∙∙ √tN} of [0,1] and calculate the truncated Signature of order M of a d-dimensional path X
on D, i.e. SM, for i = 0, ∙∙∙ ,N. For a fixed value of k, we use Algorithm 1 to generate a realiza-
tion of the Randomized Signature Z of X on D. For sake of clarity, the truncated Signatures on D
are reshaped into an (N X ((d(M+1) - 1)/(d - 1) - 1))-dimensional matrix called SMue while
the Randomized Signatures on D is represented as an (N × k)-dimensional matrix called ZkRand.
Finally, we train a Ridge Regression to find a parameter matrix W ∈ Rkx((d(M+i)-I)/(d-I)-I)
mapping the Randomized Signatures ZkRand into the truncated Signatures SMtrue .
More precisely, we consider a 10-dimensional control X = (t, Wt) where W is a 9-dimensional
Brownian motion with independent components, and fix M = 6. For each k ∈ {1, ∙∙∙ , 200},
we generate ZkRand, perform a simple linear regression and calculate the L2 approximation error.
Results in Figure 3 indicate that the approximation error decreases as k increases. This confirms
that, as ensured by Theorem 2, the random features constructed as prescribed by Equation 4 hold the
same information content as the Signature and thus maintain its approximation power and geometric
properties. Finally, we highlight that, if we were to be satisfied with an approximation error of
order 10-4, we could achieve it with roughly with k = 190 Randomized Signatures. To conclude,
this means that, instead of calculating d(M+1) - 1 /(d - 1) - 1 = 1111110 integrals per time
step, we could just perform k2d = 36100 calculations per time step — which is 3 times cheaper.
More remarkably, this high feature dimensionality (1111110 for Signature) makes any model for
downstream tasks exponentially more complex than the that based on Randomized Signature (only
190 features). Modeling choices and details are reported in Appendix B.2.
4.2	Reconstruction and Randomized S ignature Autoencoders
The Randomized Signature of X is rich enough to reconstruct it almost perfectly. In practice, we
consider an “autoencoder” architecture, whose encoder is our random feature generator and the
decoder is a trained linear projection mapping the Randomized Signatures into the original path.
For this experiment, we fix an equally-spaced partition D = {to, ∙∙∙ ,tN} and consider a single
5
Under review as a conference paper at ICLR 2022
Figure 3: Randomized Signature approx- Figure 4: Performance of the Randomized Signature autoencoder.
imates the Signature as k increases.
realization XTrain of an Ornstein-Uhlenbeck process. Ultimately, We train a Ridge Regressor to
map the Randomized Signature Z of the one and only train sample XTrain into XTrain itself. Table
6 in Appendix shoWs the average reconstruction error on NT est = 10000 test samples as We repeat
the experiment for different choices of k. Such results are in accordance With Figure 4 Which shoWs
an example of the true and generated time series of a test sample. This confirms that the Randomized
Signature of a path holds an equivalent information content as the path itself While being more usable
for any doWnstream tasks. Modeling choices and details are reported in Appendix B.2.
4.3 Learning SDE solutions with a linear map on the Randomized S ignature
In this series of experiments, We provide evidence of the potential of Randomized Signatures in
several doWnstream tasks. First of all, as shoWn in Appendix B, We prove that our approach can
accurately learn the dynamics of several rough differential equations: We highlight that the algo-
rithm has only access to the input controls and the output response of the system but it is completely
agnostic with respect to the law of the dynamics itself. To start With, We focus on the nonlinear
dynamics of 1-dimensional stochastic Double Well. Secondly, We study the multidimensional case
of the 4-dimensional Ornstein-Uhlenbeck. Then, We investigate the performance on the extremely
rough dynamics of Fractional Geometric BroWnian Motion even for loW values of the Hurst param-
eter H. Additionally, We obtain promising results in learning the dynamics of an Enzyme-Substrate
Reactions (Ingalls, 2013). We also demonstrate that this approach has sustainable performance even
When the input control is irregularly sampled as Well as Neutral Controlled Differential Equations.
Due to space constraints, these results are presented in the Appendix. Here, We shoW the perfor-
mance in learning highly nonlinear dynamical systems With a real World application.
Tumor Growth Kinetics. In this experiment, We consider predicting the output of a complex
controlled nonlinear system using Randomized Signatures. The Simeoni Tumor model (Simeoni
et al., 2004) is Widely used to predict the evolution of tumor Weight Yt under the concentration of a
treatment drug Xt . The system is specified by the folloWing equations
du1 =	λou1	[l +	(λo∕λιwt)ψ]	— kzXtu1	dt,	du2	=	[kzXtu1	—	kιu2]	dt,
(5)
dut3 = k1	ut2	—	ut3	dt,	dut4	=	k1	ut3	— ut4	dt, Yt = ut1	+	ut2	+ ut3	+	ut4 .
We fix an equally spaced partition D of [0, 10] and perform Ridge Regression to map 10000 instances
of the Randomized Signature of the controls into the respective integrated solution Yt. We sample Xt
from the law of a Scaled Squared Brownian motion 0.25∙W2 where Wt is a 1-dimensional Brownian
Motion (BM) and the left side of Figure 5 shoWs the comparison of the true and the generated time
series on a test sample along with the respective path of the control. To test the robustness of our
approach, we use controls that do not follow the law used in training. More precisely, the middle
of Fig. 5 shows what happens if we use 1{w-2>1} as input control instead of 0.25 ∙ Wt2 while its
right does the same as we use Wt . These two experiments prove that the algorithm learnt much
more than merely predicting the behaviour of the system under ordinary circumstances. The first
shows that the algorithm has “understood” that the absence of drug leads to the growth of the tumor
while it dies back once the medicine is injected. On the other hand, the second suggests that the
algorithm cannot behave properly when the input becomes negative — not allowed by the problem
definition (negative drug concentration). Further details are reported in Appendix B.2.
Electrochemical Battery Model. In this experiment, we learn the dynamics of the electrochemi-
cal battery model proposed in (Daigle and Kulkarni, 2013) which returns the voltage Y as the current
6
Under review as a conference paper at ICLR 2022
Figure 5: Tumor Growth Kinetics stimulated with Scaled Squared BM (left) - Step Function (center) - BM
(right). Model is only trained on scaled squared BMs, so the input in the central plot is out of distribution.
Figure 6:	Electrochemical Battery Model: Example of fit for a Train Sample (left) - Predictions on Test Sample
for different k (left) - Comparison with NNARX (right)
X is drawn from the battery. We use the open source NASA Prognostic Model Package (Teubert
et al., 2021) to simulate voltage trajectories given input current control paths. On a fixed equally
spaced partition D of [0, 500], we model the input current with step functions taking values 0 or 1
on random sub-interval of [0, 500]. To test the robustness of our approach, we train a Ridge Regres-
sion to map NT rain = 1000 instances of k-dimensional Randomized Signature of the controls into
the respective solutions to which we add white noise. The left of Figure 6 shows the comparison
between the ground truth and our prediction on an in-sample trajectory. In the same figure, we com-
pare the generated time series for several choices of k on a test sample along with the path of the
input control. In Figure 6 we show the performance of our method in comparison with a nonlinear
neural ARX (NNARX) — which is often used in the modern control theory/system identification
literature as a benchmarch (see e.g. (Masti and Bemporad, 2018)), given its strong theoretical guar-
antees (Schoukens and Ljung, 2019). Table 8 in Appendix reports the performances in terms of
MSE on 1000 test samples for our models compared to two versions of NNARX: NNARX 22 which
has around 1000 trainable parameters just like our model with k = 1000, and NNARX 1000 which
matches our best model in terms of MSE but has around 106 trainable parameters. Details are
reported in Appendix B.2.
Comparison with Neural Controlled Differential Equations. In Appendix B, we benchmark the
performance of our model with that of NCDEs (Kidger et al., 2020), a recently proposed competitive
deep learning technique for controlled dynamical system modelling. We test the two models on
the task of learning the dynamics of an Ornstein-Uhlenbeck Process, and showcase the superior
performance of our method both in terms of computational complexity and accuracy.
4.4 Comparison with Truncated S ignatures
In this experiment, We learn the dynamics of the Fractional Ornstein-Uhlenbeck process using our
Randomized Signature model and benchmark our results against a Truncated Signature model of
order M . The computational cost of extracting the Truncated Signature of order M = 3 for d
controls is O(d3) While for Randomized Signature it is O(k2d). To keep the computational cost
of extracting features equal across models, We select k = d. As a result, the number of features
of the Signature-based model is O(d3) While that of our model is O(d). In the spirit of Reservoir
Computing, We choose both models to be linear. Hence, the number of trainable parameters is equal
to the number of features. This implies that, the number of trainable parameters of our model is
orders of magnitude smaller than that of our benchmark (left of Figure 7) — this severely affects
the training time (see e.g. (Bottou et al., 2018)). Figure 7 shoWs that our training time is almost
independent on d. Instead, the training complexity of the Signature-model is exponential in d and
7
Under review as a conference paper at ICLR 2022
the Out-of-Sample performance degenerates as the underlying optimization problem explodes in
dimension. This result clearly highlights the data-hungriness of Signature-based models when the
number of dimensions is high. To conclude, we emphasize that if d = 80, the storage cost of
the Truncated Signature of a path is 200MB while its Randomized Signature is just 32KB. As a
consequence, training the former on NT rain = 10000 would take an unpractical amount of time and
storage memory while we were able to train ours in 4.8896 seconds using just 320MB and attaining
an Out-of-Sample Error on NT est = 10000 equal to 0.075329. Modeling choices and details are
reported in Appendix B.2
20	40	60	80
# controls
20	40	60	80
# controls
Signature (NtraM = 20)
-∙- Signature (NtraM = 50)
Rand. Signature {Ntrain = 20)
T- Rand. Signature (Ntrain ɪɔθ)
20	40	60	80
# controls
Figure 7:	Randomized Signature vs. Truncated Signature model. (Left) Number of trainable parameters
for Randomized Signature is significantly smaller regardless of the number of controls. (Center) Truncated
Signature is much slower than Randomized Signature in high dimensions. (Right) As opposed to Randomized
Signature, the performance of Truncated Signature degrades as the number of controls increases.
4.5 Time Series Classification
In time series classification (TSC) the goal is to map an input time series to the corresponding label.
Given the complex high-dimensional nature of temporal signals, the vast majority of classification
algorithms aims at effectively finding expressive representations of the original time series and then
maps them into the final prediction. This last operation is often realized via a relatively simple map-
ping, e.g. a linear map or a shallow neural network. Deep learning algorithms used for TSC include
various types of deep convolutional neural networks which extract a hierarchical representation of
the input time series by learning the parameters of their filters via backpropgation. These features
are finally mapped into the logits associated with each class via a trainable linear projection. Last but
not least, it is key to remark that these networks are typically over-parametrized and their training
can be computationally demanding as well as challenging from an engineering perspective.
In this section, we investigate how we can actually replace deep neural networks used for feature
extraction with relatively cheap random layers which iteratively generate a reservoir. Among other
notable works, Rocket (Dempster et al., 2020) arguably proposes the most successful solution.
Rocket. This algorithm prescribes the generation of k 1 unidimensional filters characterized by
randomly drawn lengths, weights, and jumps. Given a filter of length lf ilter, weights ω ∈ Rlfilter,
and jump j, it can be easily shown that the feature Zi resulting from its action on an input path X ,
from position i in X , evolves according to the following iterative update:
Zk = Zk-1 + Xi+(k×j) × ωk,	k ∈{0,…，filter - 1}	(6)
where Z0i = b and b is a random real number. We define Zi to be equal to Zli -1. Depending
on the values of j and lfilter and on the length lX of the path X , this update produces a vector
Z = (Z1, ∙ ∙ ∙ , ZM) of M := IX — (l — 1) ∙ j random features, used to extract only two nonlinear
features: max(Z) and -M PM=I 1{Zi>0}. Ultimately, this means that if We use k kernels, We extract
2k features per time series which are then used as input to a Ridge classifier to obtain the final
prediction. Rocket results in state-of-the-art performances in TSC both in terms of accuracy and
computational efficiency, outperforming several deep learning baselines which often involve order
of magnitude more parameters to optimize.
An alternative interpretation. Looking at Eq. 6, it is natural to wonder if it can be replaced with
an alternative and possibly more principled update scheme driven by the input path and evolved by
random projections. With the intent of aligning Rocket to the framework presented in this paper,
we replace Eq. 6 with the update equation in Algorithm 1, thus exploiting the expressive power
of Randomized Signatures. As detailed in the Appendix B.2, the current implementation of this
8
Under review as a conference paper at ICLR 2022
Figure 8: Mean rank of the modified version of Rocket described in the text and a number of deep learning
baselines (results taken from (Fawaz et al., 2019)) on 128 datasets from the UCR archive. Our method is
referred to as “rocket_ours”. The other methods are, from left to right: Time Le-Net (Le GuenneC et al., 2016),
Multi Channel Deep Convolutional Neural Network (Zheng et al., 2014), Time Warping Invariant Echo State
Network (Tanisaro and Heidemann, 2016), Time-CNN (Zhao et al., 2017), Multi Layer PerCeptron (Wang et al.,
2017), Encoder (Serra et al., 2018), Fully Convolutional Neural Network (Wang et al., 2017), Residual Network
(Wang et al., 2017).
Accuracy
10	987654321
tlenet
mcdcnn
twiesn
resnet
fen
encoder
空迎rocket
rocket_ours
algorithm is equivalent to assuming that the random vector field A is approximately diagonal and
autonomous. In other words, the Randomized Signatures are never mixed during their evolution
and, differently from the update in Eq. 6, the filter weights do not depend on time. These represent
two strong limitations which pave the way to several enhancements in future work. Despite these
drastic shortcomings, the resulting algorithm outperforms several deep learning baselines on 128
TSC datasets from the UCR archive, as shown in Fig. 8. The original algorithm using Eq. 6
provides better results than ours but does not enjoy the same theoretical guarantees based on the
theory of Randomized Signatures presented in Section 3.
5 Conclusions
Reservoir Computing and the Concept of Signature. The concept of reservoir computing is
based on an simple and intriguing idea: generating a large number of features and then training a
linear readout to perform an arbitrary downstream task. This simple approach promises to reduce
the number of model parameters yet retaining a high level of expressive power. Inspired by results
from rough path theory showing that the Signature Transform enjoys the status of reservoir, we
propose a random reservoir, i.e. the Randomized Signature, to approximate the Signature Transform
and thus inheriting its geometrical properties and expressive power. Following rigorous theoretical
arguments, we do so by evolving a latent differential equation controlled by a path and evolved
according to random vector fields. The empirical investigations presented in this paper provide
evidence supporting that this strategy is successful and can be deployed on several tasks including
non-parametric system identification and TSC.
Future research and open questions. From a theoretical standpoint, Eq. 4 is written in terms
of autonomous vector fields Ai for i = 1,…，d, whose entries are drawn according to Gaussian
distributions with zero mean and unit variance, and an activation function σ. It is natural to wonder
if choosing time-dependent matrices Ai leads to increased expressive power and to what extent the
quality of the resulting features depend on a particular choice of σ. Furthermore, since our exper-
iments suggest that the dimensionality k of the random features play a significant role in terms of
expressive power, it is not trivial to identify a rigorous criterion to determine it. A precise character-
ization of this quantity and a clever way to select its value could potentially lead to further savings
in terms of computational cost.
From a practical point of view, our method proved to be able to accurately predict the response of dif-
ferent dynamical systems driven by heterogeneous and possibly irregular control paths, even when
such controls were sampled from different laws with respect to the one used in training. In light of
these results, it would be interesting to explore whether the method of Randomized Signatures could
be extended to model the response of systems governed by controlled partial differential equations.
In the context of TSC, we established a link with existing works using time series representations
based on random features and our approach based on the update in Eq. 4. We implemented a first
baseline algorithm combining our Randomized Signatures with the algorithm proposed in (Demp-
ster et al., 2020), resulting in superior performance compared to strong deep learning baselines but
sub-optimal results compared to the original algorithm. We believe such a gap can be filled by ad-
dressing the limitation of the current version of the algorithm, e.g. the assumptions it makes on the
vector fields, as discussed at the end of Section 5. As a final consideration, we believe an inter-
esting research direction being to investigate whether our random feature extraction pipeline could
be incorporated into hierarchical models, such as deep neural networks, to form hybrid methods
including both random and learned components operating in symbiosis.
9
Under review as a conference paper at ICLR 2022
References
T. L. Ben Hambly. Uniqueness for the signature of a path of bounded variation and the reduced path
group. Annals of Mathematics ,171:109-167, 2010.
F. Biagini, Y. Hu, B. 0ksendal, and T. Zhang. Stochastic Calculusfor Fractional Brownian Motion
and Applications. Springer, 2008.
P. Bonnier, P. Kidger, I. P. Arribas, C. Salvi, and T. Lyons. Deep signature transforms. In Proceedings
of the 33rd International Conference on Neural Information Processing Systems, pages 3105-
3115, 2019.
L.	Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning.
Siam Review, 60(2):223-311, 2018.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-
shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,
Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran As-
sociates, Inc., 2020.
R. T. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations.
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pages 6572-6583, 2018.
C. Cuchiero, L. Gonon, L. Grigoryeva, J.-P. Ortega, and J. Teichmann. Expressive power of ran-
domized signature. In The Symbiosis of Deep Learning and Differential Equations, 2021a.
C. Cuchiero, L. Gonon, L. Grigoryeva, J.-P. Ortega, and J. Teichmann. Discrete-time signatures
and randomness in reservoir computing. IEEE Transactions on Neural Networks and Learning
Systems, 2021b.
M.	Daigle and C. S. Kulkarni. Electrochemistry-based battery modeling for prognostics. In Annual
Conference of the PHM Society, volume 5, 2013.
H.	A. Dau, A. Bagnall, K. Kamgar, C.-C. M. Yeh, Y. Zhu, S. Gharghabi, C. A. Ratanamahatana, and
E. Keogh. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):1293-1305,
2019.
A. Dempster, F. Petitjean, and G. I. Webb. Rocket: exceptionally fast and accurate time series
classification using random convolutional kernels. Data Mining and Knowledge Discovery, 34
(5):1454-1495, Jul 2020. ISSN 1573-756X. doi: 10.1007/s10618-020-00701-z.
H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller. Deep learning for time series
classification: a review. Data mining and knowledge discovery, 33(4):917-963, 2019.
P. K. Friz and M. Hairer. A course on rough paths. Springer, 2020.
J. C. B. Gamboa. Deep learning for time-series analysis. arXiv preprint arXiv:1701.01887, 2017.
D. J. Gauthier, E. Bollt, A. Griffith, and W. A. S. Barbosa. Next generation reservoir computing.
Nature Communications, 12(1), Sep 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-25801-2.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics,
pages 249-256. JMLR Workshop and Conference Proceedings, 2010.
B. P. Ingalls. Mathematical modeling in systems biology: an introduction. MIT press, 2013.
H. Jaeger. Adaptive nonlinear system identification with echo state networks. NIPS, 06 2003.
10
Under review as a conference paper at ICLR 2022
P. Kidger and T. Lyons. Signatory: differentiable computations of the signature and logsignature
transforms, on both CPU and GPU. In International Conference on Learning Representations,
2021.
P. Kidger, J. Morrill, J. Foster, and T. Lyons. Neural controlled differential equations for irregular
time series, 2020.
A. Le Guennec, S. Malinowski, and R. Tavenard. Data augmentation for time series classification
using convolutional neural networks. In ECML/PKDD workshop on advanced analytics and
learning on temporal data, 2016.
D. Levin, T. Lyons, and H. Ni. Learning from the past, predicting the statistics for the future,
learning an evolving system. arXiv preprint arXiv:1309.0260, 2013.
T. Lyons. Rough paths, signatures and the modelling of functions on streams. arXiv preprint
arXiv:1405.4537, 2014.
G. Marcus. Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631, 2018.
D. Masti and A. Bemporad. Learning nonlinear state-space models using deep autoencoders. In
2018 IEEE Conference on Decision and Control (CdC), pages 3862-3867. iEeE, 2018.
J. Morrill, A. Fermanian, P. Kidger, and T. Lyons. A generalised signature method for multivariate
time series feature extraction. arXiv preprint arXiv:2006.00873, 2020.
J. Morrill, P. Kidger, L. Yang, and T. Lyons. Neural controlled differential equations for online
prediction tasks. arXiv preprint arXiv:2106.11028, 2021.
B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro. The role of over-parametrization
in generalization of neural networks. In International Conference on Learning Representations,
2018.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, vol-
ume 20. Curran Associates, Inc., 2008.
A.	Rudi and L. Rosasco. Generalization properties of learning with random features. In NIPS, pages
3215-3225, 2017.
J. Schoukens and L. Ljung. Nonlinear system identification: A user-oriented road map. IEEE
Control Systems Magazine, 39(6):28-99, 2019.
B.	Schrauwen, D. Verstraeten, and J. Van Campenhout. An overview of reservoir computing: theory,
applications and implementations. In Proceedings of the 15th european symposium on artificial
neural networks. p. 471-482 2007, pages 471-482, 2007.
J. Serra, S. Pascual, and A. Karatzoglou. Towards a universal neural network encoder for time series.
In CCIA, pages 120-129, 2018.
M. Simeoni, P. Magni, C. Cammia, G. De Nicolao, V. Croci, E. Pesenti, M. Germani, I. Poggesi, and
M. Rocchetti. Predictive pharmacokinetic-pharmacodynamic modeling of tumor growth kinetics
in xenograft models after administration of anticancer agents. Cancer research, 64(3):1094-1101,
2004.
P. Tanisaro and G. Heidemann. Time series classification using time warping invariant echo state
networks. In 2016 15th IEEE International Conference on Machine Learning and Applications
(ICMLA), pages 831-836, 2016. doi: 10.1109/ICMLA.2016.0149.
T. L. Terry J. Lyons, Michael Caruana. Differential Equations Driven by Rough Paths. Springer,
2004.
C. Teubert, M. Corbetta, C. Kulkarni, and M. Daigle. Prognostics models python package, Aug.
2021.
11
Under review as a conference paper at ICLR 2022
Z. Wang, W. Yan, and T. Oates. Time series classification from scratch with deep neural networks:
A strong baseline. In 2017 International joint conference on neural networks (IJCNN), pages
1578-1585. IEEE, 2017.
P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE,
78(10):1550-1560, 1990. doi: 10.1109/5.58337.
B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu. Convolutional neural networks for time series
classification. Journal of Systems Engineering and Electronics, 28(1):162-169, 2017. doi:
10.21629/JSEE.2017.01.18.
Y. Zheng, Q. Liu, E. Chen, Y. Ge, and J. L. Zhao. Time series classification using multi-channels
deep convolutional neural networks. In WAIM, 2014.
12
Under review as a conference paper at ICLR 2022
Appendix
A	Theory Details
A. 1 Basic Definitions
First of all, we define the concept of admissible tensor norms, which we assume to have in this work.
Definition 2 (Admissible Tensor Norms) Let E := Rd and 0 be a tensor product such that the
tensor powers of E, (E脸k : k ≥ 1), are equipped with a family (∣∣∙ ||石馋七:k ≥ 1) of norms Satis-
fying:
1.	For j, k ∈ N andfor all h ∈ E0j and l ∈ E0k
∣∣h0 1∣∣e须j+矽 ≤ IlhIIE馋j||1||石馋七;
2.	For any permutation σ of {1, . . . , k},
kl1 0 …0 lk ∣∣EΘk = ∣∣lσ(1) 0 …0 lσ(k)∣∣Eg>k ；
3.	For any bounded Unearfunctionals f on E领 and g on E®k, there exists a unique bounded
linearfunctional, denoted as f 0 g, on E须j+k) Such thatfor all h ∈ V0j and l ∈ E0k
f 0 g(h 0 l) = f(h)g(l).
A family of tensor norms satisfying these conditions is called a family of admissible tensor norms.
Then we define the space in which the Signature of a path lies in, that is the following Tensor
Algebra.
Definition 3 (Tensor Algebra T Rd ) We define the tensor algebra on Rd as
∞
T (Rd) := Y (Rd)0k
k=0
as well as its truncated version of order M ≥ 0 as
M
TM(Rd) := Y (Rd)0k ,
k=0
where (Rd)室k is the space of tensors ofshape (d,...,d) given by Rd 0∙∙∙0 Rd for k times.
Definition 4 (Concatenation Operation) We define the Concatenation Operation * such that for
any given couple of continuous piecewise smooth paths X : [0, s] → Rd and Y : [s, t] → Rd, their
image through * is the continuous piecewise smooth path X * Y : [0, t] → Rd defined by
X	Y Xu	ifu ∈	[0, s]
X*Yu	:= Xs+Yu-Ys	ifu∈	[s, t]
Definition 5 (Inverse Operation) We define the Inverse Operation — such thatfor any continuous
piecewise smooth path X : t → Rd, its image through J is the continuous piecewise smooth path
Xt := XT-t, for each t ∈ [0, T].
Definition 6 (Topological Space) A topological space is an ordered pair (X, τ), where X is a set
and τ is a collection of subsets of X, satisfying the following axioms:
1.	The empty set and X itself belong to τ;
13
Under review as a conference paper at ICLR 2022
2.	Any arbitrary (finite or infinite) union of members of τ still belongs to τ ;
3.	The intersection of any finite number of members of τ still belongs to τ.
The elements of τ are called open sets and the collection τ is called a topology on X.
Definition 7 (Arcwise Connected Topological Space) A topological space (X, τ ) is said to be ar-
cwise connected if any two distinct points x, y ∈ X can be joined by an arc, that is a continuous
map α : [0, 1] → X such that α(0) = x and α(1) = y.
We say that X is uniquely arcwise connected if for any two distinct points x, y ∈ X, there exists a
unique path in X that joins them.
Definition 8 (R-tree) An R-tree is a uniquely arcwise connected metric space, in which the arc
between two points is isometric to an interval.
Definition 9 (Tree-Like) A continuous piecewise smooth path x : [0, T] → Rd is tree-like if there
exists an R-tree τ, a continuous map φ : [0, T] → τ and a map ψ : τ → V such that φ(0) = φ(T)
and x = ψ ◦ φ.
A.2 Supporting Results
Lemma 4 (Johnson-Lindenstrauss Lemma, (Cuchiero et al., 2021a)) Given an M -dimensional
HiIbert space (H,〈., )h) and Q a N-point subset of H, for any 0 < e < 1, for each k ∈ N
satisfying the so called Johnson-Lindenstrauss constraint
24logN
k ≥ ---------,
—3e2 - 2e3 ,
there exists a linear map f {Q,M,k} : H → Rk that embeds Q into Rk in an almost isometric manner.
More specifically, we have that
(1-)ka1-a2k2H≤	f {Q,M,k}	(a1)	- f {Q,M,k} (a2)2 ≤(1+)ka1-a2k2H,
for each a1 , a2 ∈ Q.
Remark 1 When there is no need to specify the dependence off{Q,M,k} from Q, M, or k, we will
omit them UP to referring to is Simply as f. Finally, we define f * : Rk → H to be the adjoint map of
f with respect to a fixed inner product〈•, •)in Rk.
To apply such Lemma in our context, we select M ≥ 0, equip TM Rd with an inner product such
that
heil X …X eiM , eji 0∙∙∙0 ejM i := Mji …δiM jM ,
where {e” X …X eiM}^	iM∈{i	d} is the canonical basis of TM (Rd). Therefore, We have
that (TM (Rd) ,〈•，•〉) is an HiIbert space.
Theorem 5 (Existence and Uniqueness of the Signature, Lemma 2.10 (Terry J. Lyons, 2004))
The following controlled differential equation
d
dSt = X St X eidXti,	S0 = 1
i=1
has a unique solution which, at each t ∈ [0, T] is the Signature St ofX on [0, t].
Notation 1 When there is no ambiguity about X, we will refer to its Signature as S, while we use
SX if more paths are involved.
Theorem 6 (Theorem 2.29, (Terry J. Lyons, 2004)) Given a continuous piecewise smooth path
X : [0, T ] → Rd, its Signature is the banal Signature 1 := (1,0,0,•…)if and only if X is tree-like.
In particular SX*X = 1.
14
Under review as a conference paper at ICLR 2022
Now, we provide results to show the relevance of these features. First of all, the following theorem
ensures that the Signature of a path encodes its essence and characterizes it completely.
According to Theorem 6, the concatenation X *X- of X with its inverse1 X- has the same Signature
as the constant path, but cannot be reparametrised to be constant. Similarly, if X, Y, Z are non-
constant paths, then Sx*y *Y *Z* Z *X = 1, but X * Y *歹 * Z *"⅛*X- is not a path of the form
Y * x- for any path γ. The formal definition of tree-like path is given in Definition 10.
Theorem 7 (Characterizing Nature of the Signature, Corollary 1.4 (Ben Hambly, 2010))
Given a couple of continuous piecewise Smooth paths X and X, then SX = SX if and only if
X-
X * X is tree-like.
This result is actually much stronger as it implies that the solution of any differential equation con-
trolled by X is fully determined by the vector fields and S. In particular, Theorem 1 shows that the
solution of any differential equation controlled by X is essentially linear in S.
Adapting Theorem III.8. (Cuchiero et al., 2021b) as done in (Cuchiero et al., 2021a) one obtains:
Theorem 8 (Randomized Signature) For any fixed integer M ≥ 0, any fixed partition D =
{to, ∙∙∙ ,tN} of t such that 0 ≤ to < •…< tN ≤ T, let us consider the Truncated Signa-
ture of order M of X at such times, that is Q := {S*, •…，SMN } such that its elements all lie
in (TM (Rd) ,〈•,•)). Let us now SeleCt 0 < e < 1, k ∈ N satisfying the associated Johnson-
Lindenstrauss constraint, let f be the implied Johnson-Lindenstrauss map and f * its adjoint map.
Then, the solution of the controlled differential equation in Rk
d
dZt =Xf(f*(Zt)ei)dXti, Zo=f(1)
i=1
on D, that is {Zt0, ∙∙∙ , ZtN }, are called the Randomized Signature of X on D. It holds that each
Ztk is the projection of SM from (TM (Rd) ,〈•,•〉) onto Rk via f. To conclude, since f * (Ztk) is
close to SM in the norm of TM (Rd), we conclude that Z preserves the geometric properties and
the approximation power of S.
Adapting Theorem III.7 in (Cuchiero et al., 2021b) it can be be shown that (asymptotically) the JL
projected vector fields stem from random matrices:
Theorem 9(Z is a random dynamical system) For k ∣-~→ ∞, for each i ∈ {1, ∙∙∙ ,d} the lin-
ear vector fields f{k} f {k}*(∙)ei) : Rk → Rk are square matrices with asymptotically normally
distributed, independent entries.
Leveraging this results, we define the following:
Definition 10 (Localized Randomized Signature) For any random matrices A1, . . . , Ad in Rk×k
and shifts b1, . . . , bd in Rk×1 such that maximal non-integrability holds on a random starting point
z ∈ Rk, any fixed activation function σ, and d-dimensional control X, the solution of
d
dZt = X σ (AiZt + bi ) dXti , Zo = z, t ∈ [0, T].	(7)
i=1
is called the Localized Randomized Signature ofX and we often refer to it as Randomized Signature.
Theorem 10 (Signature is a Reservoir (Restatement of Theorem 1)) Let Vi : Rm → Rm, i =
1, . . . , d be vector fields regular enough such that dYt = Pid=1 Vi (Yt) dXti, Yo = y ∈ Rm, admits
a unique solution Yt : [0, T] → Rm. Then, for any smooth test function F : Rm → R and for every
M ≥ 0 there is a time-homogeneous linear operator L : TM Rd → R which depends only on
(Vι,...,Vd,F,Μ,y) such that F (K)= L (SM) + O (tM+1), and t ∈ [0,T ].
1 X t := XT -t
15
Under review as a conference paper at ICLR 2022
Proof: We provide just a sketch. For every smooth test function F : Rm → R, the formal Taylor
expansion along controls together with the fundamental theorem of calculus gives us that
dt
F (Yt) =F(y)+	ViF(Ys)dXsi, t≥0.	(8)
i=0 0
For y ∈ Rm, this equation can be inserted into itself leading to a generalized version of Taylor
expansion for controlled ordinary differential equations
M
X X	Vi1
k=0 (i1,...,ik)∈{0,...,d}k
...VikF(y)
0≤t1≤...≤tk≤t
dXti1 . . . dXtik +RM(F,t)
for M ≥ 0, with the remainder satisfying
RM(F,t)=	X	Vi1...ViM+1F(Yt1)dXti11...dXtiMM++11
(i1,...,iM+1)∈{0,...,d}M+1	0≤t1≤...≤tM+1≤t

A.3 Practical example of the computation of S ignatures (one dimension).
To enhance intuition, we exemplify the computation of Signatures in the simplest setting possible.
Let X : [0, T] → R, then St1 = R0t dXs — which is exactly Xt - X0 . To get St2 , we instead have
to compute the following iterated integral: S2 = Rt (Rv dXs) dXv. Signatures of higher order St
are computed in a similar way, by iteratively integrating the path j times. As a practical example,
let Xt = t. Then it is easy to see that Sj = j. Now let Y(t) be an analytic function of time for
which We have Y(t) = P∞=o Yt(j) j. Taylor,s theorem combined with the previous computation of
the Signatures implies that Y can be approximated as a linear map of the Signatures of t. Finally,
note that Stj in this case gets smaller and smaller in magnitude as j increases. This suggests that the
truncated Signature can be safely used to approximate Y.
Computational Complexity and dimensionality of Signatures. The computational complex-
ity for computing StM is O(dM). Indeed, consider d = 2: St2 is a 2 × 2 matrix with ele-
Rtv	1	1 tv 1	2tv 2	1	tv 2	2
o o dXs1 dXv1,	o o dXs1 dXv2, o o dXs2 dXv1 and o o dXs2 dXv2. For M =
3,	the object to compute	is instead	a 2 ×	2	× 2 tensor, containing all integrals of the type
Rt (ROw (Rv dXS1) dχvv2) dXW for all i1,i2 ,i3 ∈ {1, 2}. Hence, the complexity 一 as well as the
features dimensionality — scales exponentially in M .
16
Under review as a conference paper at ICLR 2022
B Experiments
In this section, we prove that our approach can accurately learn the dynamics of several rough dif-
ferential equations. First of all, we focus on dynamics of 1-dimensional stochastic Double Well
which is characterized by a highly nonlinear dynamics. Secondly, we study the multidimensional
case of the 4-dimensional Omstein-Uhlenbeck. Then, We investigate the performance on the rough
dynamics of Fractional Geometric Brownian Motion even for choices of extremely low values of
the hurst parameter H . We also obtain promising results in learning the dynamics of an Enzyme-
Substrate Reactions. Next, We shoW hoW in high dimensions randomized signatures are computa-
tionally efficient, and also present one experiment in comparison With neural controlled differential
equations (Kidger et al., 2020). To conclude, We shoW hoW the randomized signature approach can
be used in cases Where the time grid is irregularly sampled.
B.1 Preliminary Results: Rough Differential Equations
1-Dimensional Stochastic Double Well. Let us recall that the dynamics of the 1-Dimensional
Stochastic Double Well process is given by
dYt = θYt (μ —匕2) dt + σdWt, Y0 = yo ∈ R, t ∈ [0,1].
where Wt is a 1-dimensional Brownian motion, and (μ, θ,σ) ∈ R X R+ X R+. Let Us fix yo = 1 and
(μ = 2,θ = 1,σ = 1), and the partition D of [0,1] to have N = 101 equally spaced time steps. We
train a Ridge Regression with regularization parameter λ = 0.001 to map instances of Randomized
Signature of the controls Z into the respective solution Yt . We repeat the experiment on different
values of the number NTrain of train samples and dimension k of the Randomized Signature. On the
left of Figure 9, we plot an example of the trajectory of Z while, on its right, we plot the comparison
of the true and the generated time series on an out-of-sample case. The following table shows the
performance in terms of L2 relative error on 10000 test samples:
	NTrain = 1	NTrain = 10	NTrain = 100	NTrain = 1000	NTrain = 10000
k =111	0.319443	0.100310	^^0.007727^^	0.005854	0.005932
k = 222	0447563	0.357439	^^0.030950^^	0.005193	0.004558
k = 444	0504822	0.46000	0.092313 —	0.005690	0.0043893
Table 1: Double Well: Relative L2 Error
Figure 9: Double Well: Randomized Signatures (left) - Test Sample (right).
4-Dimensional Ornstein-UhlenbeCk process. Let us recall that the dynamics of the 4-
Dimensional Ornstein-Uhlenbeck process is given by
dYt =	(μ	- ΘYt)	dt + ΣdWt,	Y0 =	y0	∈	R4,	t ∈	[0,1]
where Wt is a 4-dimensional Brownian motion, and (μ, Θ, Σ) ∈ R4 X R4×4 X R4×4. Let us fix
yo = 1, μ = 1, Σ = I4, Θi.j = i/j, the partition D of [0,1] to have N = 101 equally spaced time
steps, and k = 708. Finally, we train a Ridge Regression with λ = 0.001 on NTrain train sample
and Figure 10 shows the comparison of out-of-sample generated and true trajectories while Table 2
reports the performance in terms of L2 relative error on 10000 test samples.
17
Under review as a conference paper at ICLR 2022
Figure 10: 4-Dimensional Ornstein-UhlenbeCkProcess: Test Sample.
k = 708	NTrain = 1	NTrain = 10	NTrain = 100	NTrain = 1000	NTrain = 10OOO
Y1	0.025960	0.010615	^^0.005145^^	0.001467	0.000929
Y2	0.047323	0.023858	^^0.009432^^	0.001838	0.001247
Y3	0.033357	0.024392	^^0.009694^^	0.002008	0.001228
Y 4	0.027637	0.020390	0.007798^^	0.001775	0.001140
Table 2: 4-Dimensional Ornstein-Uhlenbeck process: Relative L2 Error.
1-Dimensional Geometric Fractional Brownian Motion. Let us recall that the dynamics of the
1-Dimensional Geometric Fractional Brownian Motion process given by
dYt = Yt (μdt + σdB(H)) , Yo = y° ∈ R+,
where μ ∈ R, σ ∈ R \ {0}, and BtH) is a one-dimensional fractional Brownian motion of hurst
parameter H ∈ (0,1). In this experiment, We fix y0 = 1 and (μ = 1, σ = 1), we let H vary in
{0.1, 0.2, 0.3, 0.4, 0.5}, and the partition D of [0, 1] is made ofN equally spaced times. We highlight
that, as per Theorem D.3.2 in (Biagini et al., 2008), when H < 1/4, the fractional Brownian motion
is so rough that it is no longer a Rough Path and its Signature is no longer defined. Finally, we
train a Ridge Regression with λ = 0.001 on NTrain = 10000 train samples of Z into the logarithm
of the respective solution log(Yt) which implies that we need to then apply the exp function to our
predictions. The right of Figure 11 shows the comparison of the generated and true trajectories on
an out-of-sample trajectory while the following table shows the Relative L2 Error as we vary the
number of time steps N and hurst parameter H :
	H = 0.1	H=0.2	H = 0.3	H=0.4	H = 0.5
一^N = 11	2.44 ∙10-3	7.07 ∙ 10-4	5.73 ∙ 10-5	5.79 ∙ 10-6	2.72 ∙ 10-8 -
- N = 101	3.04 ∙10-4	6.96 ∙ 10-5	1.58 ∙ 10-5	1.76 ∙ 10-6	3.93 ∙ 10-8 -
- N = 1001	3.78 ∙10-5	1.11 ∙ 10-5	4.03 ∙ 10-6	1.46 ∙ 10-6	1.32 ∙ 10-6 -
Table 3: Relative L2 Error Fractional Geometric Brownian Motion.
Enzyme-Substrate Reactions. First, we consider a set of highly nonlinear CODEs modeling the
interaction between a catalyst (enzyme, with abundance Ct) and a substrate S (with abundance St).
Substrate (e.g. lactose) is injected via the control variable Xt , reacts with the enzyme (e.g. lactase)
18
Under review as a conference paper at ICLR 2022
Out Of Sample
Out Of Sample
-6HN Q
Illl
8n>
Extracted Yt
■— True Yt
Extracted
--TrueYt
0.0	0.2	0.4	0.6	0.8	1.0
Time
Figure 11:	Double Well: Irregularly Sampled Test Sample (left) - Fractional Geometric Brownian
Motion Test Sample (right).
and the concentration Yt of the product chemical (e.g. glucose) is observed (Ingalls, 2013).
dSt = (Ct - k1St (1 -Ct))dt+Xtdt
dCt = - (Ct - k1St (1 - Ct)) dt - k2Ctdt
dYt = k2 Ctdt.
Following (Ingalls, 2013) we choose (k1, k2) = (30, 10), set (S0, P0, Y0) = (0, 0, 0) and consider
the evolution on 0 ≤ t ≤ 1. We fix the time grid to have N = 101 equally spaced time steps
and the control Xt to follow the law of Wt2 where Wt is a 1-dimensional Brownian Motion (to
ensure positivity). We train a Ridge Regression with regularization parameter λ = 10-3 to map 105
instances of 222-dimensional Randomized Signature Z of the controls into the respective solution
Yt . On the left of Figure 12, we plot the comparison of the true and the generated time series on a
test sample. As we can see, the model has learnt to correctly map a trajectory of Xt to the respective
system response Yt . More surprisingly, the right of such a figure shows that our model is able
to predict the correct output even if we stimulate the system with substrate injection that follows
a completely different law such as 0.5 ∙ 1{印2>0.5}. This suggests that the system was correctly
identified out of distribution.
",n">
Out Of Sample
0.5
Figure 12:	Enzyme-Substrate Reactions stimulated with Squared Brownian Motion (left) - Step
Function (right).
Comparison with Neural Controlled Differential Equations In this experiment, we benchmark
the performance of our Randomized Signature model with that of Neural Controlled Differential
Equations (NCDEs) (Kidger et al., 2020), a recently proposed competitive deep learning technique
for controlled dynamical system modelling. We test the two models on the task of learning the
dynamics of the 1-dimensional Ornstein-Uhlenbeck Process. As usual, to train our model, we fix
an equally spaced partition D of [0, 1], extract the Randomized Signature of the controls for NT rain
samples, and train a Ridge Regression to map them into the respective integrated solutions. The
NCDE models instead parametrize the potentials of a latent controlled differential equation with
feed-forward neural networks with 1 hidden layer of nnodes. Figure 13 shows the comparison of our
model against several version of the NCDEs. Additionally, Table 4 and Table 5 show the number of
trainable parameters, average L2 Relative Errors, and training times for all the models for NT rain =
100 and NT rain = 1000 respectively. We observe that our model is the best one from all aspects
and that not even a considerable amount of training data is beneficial for NCDEs.
19
Under review as a conference paper at ICLR 2022
Figure 13: Out-of-Sample comparison of NCDE(nchannels , nnodes) models, Randomized Signature model
and Ground Truth.
	Rand. Sig Model	NCDE(4,10)	NCDE(8,100)	NCDE(16,512)
Trainable Param	107	107	1733	16961
Training Time	0.047390	~1833.00~	1837.858	2485.80
Error	1.23 ∙ 10-6~	0.081516	0.080829	0.0800839—
Table 4: Randomized Signature vs. NCDEs - d = 80, NTrain = 100
	Rand. Sig Model	NCDE(4,10)	NCDE(8,100)	NCDE(16,512)
Trainable Param	107	107	1733	16961
Training Time	0.238423	~2853.00~	3972.79	6510.95
Error	2.45 ∙ 10-7~	0.076195	0.072763	0.073845
Table 5: Randomized Signature vs. NCDEs - d = 80, NTrain = 1000
B.2 Additional Details
Learning the Signatures. In this experiment, We fix the partition D = {t0,…，tN} of [0,1]
to have 201 equally-spaced time steps and the regularization parameter of the Ridge Regression is
λ = 0.001.
Reconstruction and Randomized Signature Autoencoders. In this experiment, We fix the par-
tition D = {t0, ∙ ∙ ∙ , tN} of [0,1] to have 101 equally-spaced time steps, fix k = 111, and consider
a single realization XTrain of an Ornstein-Uhlenbeck process of asymptotic mean μ = 2, mean
reversion θ = 1, and volatility σ = 1. We train a Ridge Regression With regularization parame-
ter λ = 0.001 to map the Randomized Signature Z of the one and only train sample XT rain into
XT rain itself.
	k =	^I11	k=	222	k = 444 J
NTest = 10000^	8.35	∙ 10-9	2.26	∙ 10-8	2.27 ∙ 10-9 -
Table 6: Relative L2 Error Autoencoder.
Comparison with Vanilla Signature Approach Let us recall that the dynamics of the d-
Dimensional Fractional Ornstein-Uhlenbeck process is given by
dYt = (μ - ΘYt) dt + ΣdBH,	Yo = yo ∈ Rd, t ∈ [0,1]
Where BtH is a d-dimensional Fractional BroWnian motion of hurst parameter H ∈ (0, 1), and
(μ, Θ, Σ) ∈ Rd X RdXd X RdXd. Let us fix y0 = 1, μ = 1, Σ = 1d, Θi.j = i/j, the partition D of
[0, 1] to have N = 101 equally spaced time steps, and H = 0.3.
20
Under review as a conference paper at ICLR 2022
1-Dimensional Stochastic Double Well - Irregularly Sampled Time Grid. Let us recall that the
dynamics of the 1-Dimensional Stochastic Double Well process is given by
dYt = θYt (μ — Y2)	dt + σdWt, Y0 = yo ∈ R, t ∈	[0,1]	(9)
where Wt is a	1-dimensional BroWnian	motion, and	(μ, θ,σ) ∈ R X	R+ X	R+. For each	train and
test sample, the partition D of [0, 1] is made of N randomly drawn times. More precisely, D =
{0,tι, ∙∙∙ ,tN-ι, 1} such that tk = 1/(1 一 exp(-sk)) and {sι, ∙∙∙ ,sn-i} are N 一 2 independent
realizations ofa uniform distribution U [0, 1] sorted increasingly. As a result, the probability that two
samples share the same D is null. We train a Ridge Regressor with NTrain = 10000 train samples and
Figure 11 shows the comparison of an out-of-sample generated and true trajectory. Finally, Table
7 shows the Relative L2 Error on 10000 test samples as we vary the number of time steps N and
k and while comparing it to the respective experiment in case the time grid is regularly spaced. As
we can see, even though the performance are worse than the regularly sampled setup, this technique
proves to be anyway reliable on irregularly sampled regimes. In this experiment, we fix y0 = 1 and
(μ = 2,θ = 1,σ = 1) while the regularization parameter of the Ridge Regression is λ = 0.001.
	(N,k) = (11,111)	(N,k) = (101, 222)	(N,k) = (1001, 332)
Irregular	0.082735	0.016885	0.010902
Regular	0.026759	0.004465	0.003004
Table 7: Irregularly Sampled Double Well: Relative L2 Error.
Tumor Growth Kinetics. We set (k1, k2, λ0, λ1, Ψ) = (10, 0.5, 0.9, 0.7, 20), and
(u0,uQ,u3,u4,Y0) = (2,0,0,0,2). We fix the partition D of [0,10] to have 101 equally
spaced time steps and we again train a Ridge Regression with regularization parameter λ = 0.001
to map 10000 instances of the 222-dimensional Randomized Signature Z of the controls into the
respective solution Yt .
Electrochemical Battery Model. In this experiment, we fix the partition D of [0, 500] to have
1001 equally spaced time steps, the regularization parameter of the Ridge Regression is λ = 10-3
while k ∈ {50, 166, 1000}. Regarding the latest experiment, we simply learn to map the Random
Signature of the controls into the respective Y to which we added white noise with variance 0.01.
Regarding NNARX, for the regressor we follow the results presented in (Masti and Bemporad,
2018), where na = nb = 12, and nk = 1 lead to the best results. Regarding the neural network, we
use a feedforward neural network with input dimension na + nb + 1 = 25 and 2 hidden layers each
with either 22 (NNARX22) or 1000 (NNARX1000) hidden units. For the loss function, we used the
mean square error optimized using Adam with learning rate 0.01 for 100 epochs. Table 8 reports the
performances in terms of MSE on 1000 test samples for our models compared to NNARX.
	k =	50	k=	166	k = 1000	NNARX 22	NNARX 1000
NTest = 1000~	3.15	10-4	2.66	10-4	2.59 ∙ 10-4	3.88 ∙ 10-4	^^2.62 ∙ 10-4
Table 8: Electrochemical Battery Model: MSE Error Comparison.
Time Series Classification
Dataset Details We run our TSC experiments on the UCR archive, one of the most popular
benchmarks for TSC methods (Dau et al., 2019; Fawaz et al., 2019). It consists of 128 heterogeneous
datasets of different sizes comprising possibly irregularly sampled one-dimensional time series of
various lengths. Rocket is one of the best performing algorithms on this benchmark.
ROCKET When implementing Rocket, we generate k kernels each characterized by a certain ran-
dom length lfilter, random weights ω ∈ Rlfilter, and random jump j. Since the lengths and the
jumps are randomly sampled, the length M of the vector Z = (Z1,…，ZM) of the random fea-
tures varies accordingly. Given that the number of possible combinations of the values of lfilter and
21
Under review as a conference paper at ICLR 2022
j is limited, we can group the kernels based on such combinations. As a result, among each of these
groups, the length M is fixed and, additionally, each kernel is applied to the very same sub-time-
series (Xi, Xi+j, ∙∙∙ , *心+(“获』-i)×j) of X starting from position i. If We now focus on one of
these subgroups and let us assume it contains k kernels of length lfilter and jump j. As a result of
the application of each kernel on X , we obtain the following matrix
ΓZ 0,1	Z1,1	. . .	...Z M,1-
Z 0,2 .	Z1,2	. . . ..	...Z M,2 ..
. . _ ~ Z 0,k	.. .. 〜 Z 1,k ...	.. --1 ...Z M,k
where Zi,r is the result of the action of the r-th kernel on the sub-time-series
(Xi,Xi+j,…，xi + (l filter-1)×j of X starting from position i. Finally, out of each row, we ex-
tract the features described above.
Turning to our approach, we generate k kernels each characterized by a certain length lfilter , weights
ω = (ω0, ω1) ∈ R2, and jump j. As before, let us again group them based on all the possi-
ble combinations of lfilter and j and focus on only one of such subgroups. As a consequence,
we have k kernels each characterized by weights (ω0, ω[), r ∈ {1, ∙∙∙ ,k} with fixed length and
jump. The application of each of these k kernels following the update rules proposed in Algorithm
1 is equivalent to extracting a 1-dimensional Randomized Feature of each of the sub-time-series
Xiix Xi+j, •…，Xi+(ifiiter-i)×j). As a result, we obtain the following matrix
ΓZ0,1 Z1,1	... Z 0,2 Z 1,2	... .	..	...Z M,1 ...Z M,2 ..
.	.. .	.. C J-	T 7 Z0,k	Z1,k	. . .	..	.. --= ...Z M,k
where Zi,r is the 1-dimensional Randomized Signature of(X0, Xj, ∙∙∙ , X(Ifilter-i)×j) via the r-th
kernel. Let us we define the random matrix A guiding the evolution of the Randomized Signature as
the k × k diagonal matrix having the weights ω0r on the diagonal while the random bias b is a column
vector having the ω1r as entries. Then, the i-th column is the k-dimensional Randomized Signature
of the sub-time-series (Xi,Xi+j, ∙∙∙ ,Xi+(Ifilter-i)×j). Finally, out of each row, we extract the
features described above.
Comparison of Computational Complexities and Dimensions The following table reports key
figures regarding the computational complexity and dimensionality of both Signature and Random-
ized Signature.
	Comp. complexity	Dimension	Guarantees
Truncated Sign. (M- components):	O(dM)	O(dM)	VZ(Thm 1)
Randomized Sign. (d - dimensional):	O(k2 d)	O(k)	〃Thm 2)
Table 9: Properties of (Randomized) Signatures of a d-dimensional path.
22