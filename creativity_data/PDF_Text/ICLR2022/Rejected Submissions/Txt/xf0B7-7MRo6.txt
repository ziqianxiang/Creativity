Under review as a conference paper at ICLR 2022
AIR-Net: Adaptive and Implicit Regulariza-
tion Neural Network for Matrix Completion
Anonymous authors
Paper under double-blind review
Ab stract
Conventionally, the matrix completion (MC) model aims to recover a matrix from
partially observed elements. Accurate recovery necessarily requires a regulariza-
tion encoding priors of the unknown matrix/signal properly. However, encoding
the priors accurately for the complex natural signal is difficult, and even then, the
model might not generalize well outside the particular matrix type. This work
combines adaptive and implicit low-rank regularization that captures the prior dy-
namically according to the current recovered matrix. Furthermore, we aim to an-
swer the question: how does adaptive regularization affect implicit regularization?
We utilize neural networks to represent Adaptive and Implicit Regularization and
named the proposed model AIR-Net. Theoretical analyses show that the adaptive
part of the AIR-Net enhances implicit regularization. In addition, the adaptive
regularizer vanishes at the end, thus can avoid saturation issues. Numerical ex-
periments for various data demonstrate the effectiveness of AIR-Net, especially
when the locations of missing elements are not randomly chosen. With complete
flexibility to select neural networks for matrix representation, AIR-Net can be ex-
tended to solve more general inverse problems.
1 Introduction
The matrix completion (MC) problem, which aims to recover a matrix X* ∈ Rm×n from its par-
tially observed elements, has arisen in numerous domains, ranging from computer vision (Wen et al.,
2012), recommender system (Netflix, 2009), and drug-target interaction (DTI) (Mongia & Majum-
dar, 2020). This fundamental problem is ill-posed without assumptions on X * since we have many
completions. So it is essential to impose additional information or priors on the unknown ma-
trix/signal.
To describe the prior for natural signal, or restrict the solution in the corresponding space is diffi-
cult. Classical methods for MC are mainly based on low-rank, sparsity or piece-wise smoothness
assumption (Rudin et al., 1992; Buades et al., 2005; Romano et al., 2014; Dabov et al., 2007).
These priors describe simple structural signal well, but may lead to a poor approximation of X *
with complex structures (Radhakrishnan et al., 2021) especially when the observed entries are not
sampled uniformly at random. Recently, deep neural networks (DNN) have shown a strong ability
in extracting complex structures from large datasets(Li et al., 2018; Mukherjee et al., 2021). How-
ever, such a large number of data sets cannot be obtained in many scenarios. Fortunately, DNN
also works in solving some inverse problems without any extra training set (Ulyanov et al., 2018).
Over-parametric DNN performs well on a single matrix is a mysterious phenomenon. One of the
explanations is there exists implicit regularization during training (Arora et al., 2019; Xu et al.,
2019; Rahaman et al., 2019; Chakrabarty & Maji, 2019). Although DNN with implicit regulariza-
tion outperforms some classical methods, it is insufficient to describe the space of complex X*.
Extra-explicit regularization can improve its performance in signal recovery (Metzler et al., 2018;
Boyarski et al., 2019a; Liu et al., 2019; Li et al., 2020). However, such explicit priors are often
valid only for specific data or sampling patterns. A more flexible regularization is required to meet
practical MC problems.
We introduce flexibility in this paper by firstly representing the explicit regularization using DNN
without any extra training set. The explicit regularization we begin with is Dirichlet Energy (DE),
which is formulated as tr(X>LX), with L a Laplacian matrix describing the similarity between
1
Under review as a conference paper at ICLR 2022
columns. Note that L in DE is fixed during iteration. Building an exact L based on incomplete
observation is very challenging. Therefore, we parameterize L with DNN, and revise L iteratively
during training. Furthermore, We combine the learned DE, which is an adaptive regularizer, with
implicit regularization to form a new regularization method for MC named AIR-Net. The interaction
between explicit regularization and implicit regularization in solving MC problems is further stud-
ied. The results show that combining the two can obtain a new, more flexible regularization model
and enhance the low-rank preference of implicit regularization. In many examples, AIR-NET has
a more vital feature representation ability and more comprehensive application range and shows
state-of-art performance.
2 Adaptive and implicit regularization neural network
Our model is proposed as follow:
N
min Lall = LY (A(X*), A(X)) + X λi ∙ R^ (Ti (X))	⑴
X,Wi
i=1
where A (X) = X ∣ω= { Xij,
(i,j) ∈ Ω
(i,j) ∈ Ω
and Ω are the observed coordinates set, and the
other entries are missing. Different from other regularization models for MC, here X is represented
by a neural network which tends to be low-rank implicitly (Section 2.1), and RWi is an adaptive
regularization with a forward neural network represented Laplacian matrix(Section 2.2). The de-
tailed notations will be introduced in the corresponding sections. A specific case of Equation 1 for
matrix completion is given in Section 2.3.
2.1	DMF as an implicit regularization
In order to make the model suitable for more matrix types, we need a more general data prior. The
low-rank is a very general prior in various matrix types. There are two main ways to encode the
low-rank prior into model: (a) Adding an explicit regularization term such as rank and nuclear norm
(Candes & Recht, 2009; Lin et al., 2010). (b) Using a low-dimensional latent variable model to
represent X, including matrix factorization (MF) and its varieties (Koren et al., 2009; Fan & Cheng,
2018). The first case suffers from the saturation issue, which is induced by explicit regularization.
The second one faces the problem of estimating a proper latent variable dimension.
Unlike the existing MF model, which constricts the size of the shared dimension of the factorized
matrix, DMF can take a large shared dimension and still preserve the low-rank property without
explicit regularization. This is the so-called implicit low-rank regularization of DMF:
X(t) = W[L-1](t)W[L-2](t)...W[1](t)W[0](t) ∈ Rm×n,
where L is the depth of MF. W [l] (t) represents the l-th matrix at the step t during training. The
results are given under a mild assumption 1 in Section A.2. This property helps avoiding dimension
estimation and saturation issues. As for the details of the implicit low-rank we will discuss in
Section A.2.
2.2	Adaptive regularizer
Apart from the low-rank prior, self-similarity is also a typical prior. The patch in the image and the
rating behavior of users are all examples of self-similarity. For example, there is always a certain
degree of self-similarity between the blocks in the image. A classical way to encode the similarity
prior to X is Dirichlet Energy (DE) which is formulated as tr(X>LX). But DE will face two
problems in applications: (a) L is unknown in MC problem, construct L based on incomplete X
may induce worse prior. (b) The formulation of DE only encodes the similarity of the columns of
X . Other similarities such as block similarity cannot be captured. To address both of these issues,
we parameterize L with DNN and replace X by a transformed Ti (X) to capture the self-similarity
flexibly.
The adaptive regularization is defined as
RWi (Ti(X)) = tr Ti (X)> Li (Wi) Ti (X) ,i= 1,2,...,N
2
Under review as a conference paper at ICLR 2022
where Li ∈ Rmi×mi is parameterized by Wi ∈ Rmi ×mi. To keep the Laplacian prop-
erties of Li , special design for the parameterized structure is important. We design a for-
ward neural network which encodes the properties of Laplacian matrix in structure. The de-
tails are discussed in A.4. Ti : Rm×n 7→ Rmi ×ni transforms X into special domain, which
makes the AIR-Net possible to capture various relationships embedded in data. The common
choice can be Ti (X) = X which captures the relationship between columns. Regulariza-
tion captures the relationship between rows when Ti (X) = X>. Especially if Ti (X) =
vec (block(X))1 , vec (block(X))2 , . . . , vec (block(X))n , where vec (block(X))j ∈ Rmi is
the vectorization of j-th block in X row by row, then the similarity among blocks can be obtained.
A natural problem that arises is what the Li looks like during training.
Obviously, RWi reaches minimum when Li = 0, and this is called a trivial solution. The most ex-
citing thing is that when we minimize Equation 1 with the gradient descent algorithm, {Li(Wi(t))}
converges to a non-trivial solution. Another expected phenomenon is that RWi vanishes at the end
and will not cause the so-called saturation issue. The saturation issue is a bias term that dominates
the overall estimation error due to explicit regularization. We illustrate these phenomena both by
theoretical analysis (Theorem 2 in Section 3.2) and numerical experiments (Section 4).
2.3	AIR-NET FOR MC
In this subsection, we will focus our model on the MC problem. We select T1 (X) = X, T2 (X) =
X> , N = 2 to capture the relationship both in rows and columns of X. Overall, the theoretical
analyses for general inverse problem is based on Equation 2.
min Lall =	LY	(A(X* )	, A(X)) + λr	∙ RWr	(X)	+	λc ∙ RWc	(X>),	⑵
X,Wr,Wc	r	c
where X = W[L-1]W[L-2] ... W[1]W[0], RWr (X) = XLr(Wr)X>, RWc(X>)=
X>Lc(Wc)X. Specially, our experiments focus on the MC problem which can reform Equation 2
as follows:
min	Lall
W[l],Wr,Wc
E	IXij- Xj	I +	λr	∙ RWr	(X) +	λc	∙ RWc	(X>),
(i,j)EQ
(3)
with l = 0,1,…，L - 1. The parameters in Equation 3 is updated by gradient descent
algorithm or its variations. We stop the iteration until IRWr (T +1) - RWr(T) I < δ and
∣RWc(τ +1) - RWc(T) I < δ. The recovered matrix is X(T) = W[L-1] (T)…W[0] (T).
Some works which combine implicit and explicit regularization also can be regarded as a special
case of Equation 1. Both the Total Variation (TV) and DE can be regarded as a fixed L. Therefore,
the framework of Equation 1 also contains DMF+TV (Li et al., 2020), DMF+DE (Boyarski et al.,
2019a). So far, we cannot see any essential difference between Equation 3 and these models. We
will illustrate the amazing properties of the model in the next section. 3
3 Theoretical analysis
In this section, we will analyze the properties based on the dynamics of Equation 3. Theorem 1
shows that our proposed regularization enhances the implicit low-rank regularization of DMF. The-
orem 2 shows that the adaptive regularization will converge to a minimum while capturing the inner
structure of data flexibly. Although this paper focus on MC problem, the following theoretical ana-
lyzes is satisfied for the general inverse problems. As A and A (X*) are fixed during optimization,
we simplify LY (A (X*) , A (X)) as LY (X) below. Ui,j is the (i, j) th entry of U, U:,k and Uk,:
are the k-th column and the k-th row of U respectively.
3.1 AIR-Net enhances the implicit low-rank regularization
To simplify the analysis, we keep Lr and Lc fixed. Then the RWi, i = r, c only varies with X. We
will demonstrate what the adaptive regularizer brings to the implicit low-rank regularization.
3
Under review as a conference paper at ICLR 2022
Theorem 1. Consider the following dynamics with initial data satisfying the balance initialization
Assumption 1(see A.2):
∂
W [l](t) = - dWW[lτ Lall(X (t)),	t ≥ 0, l = 0,...,L - 1,
where Lall (X) = LY(X) + λr ∙ RWr (X) + λc ∙ RWc (X). Thenfor k = 1, 2,...,we have
σk(t) = - L (σ2(t))1-1(VwLY(X(t)), U：,k(t)k>(t)〉
C	3 — 1	(4)
—2L (σ2⑴)2 L Yk⑴，
where X(t) = U(t)S(t)V>(t) is the SVD for X (t), W = W[0], W[1], .. ., W[L-1], X =
P*U：,sk>,Yk(t) = U>LrU：,k + V>LcV：,k ≥ 0.
s
Proof. Directly calculate the gradient of Lall at W and utilize Equation 4 will obtain the result. The
details of proof can be found in A.3.	口
Compared with the results of vanilla DMF whose order of σk (t) is 2 - L. This Theorem demon-
Strates that AIR-Net's σk (t) has a higher dynamics order which is 3 — L. Notice that the adaptive
regularizer keeps Yk (t) ≥ 0. In this way, a bigger convergence speed gap appears between differ-
ent singular values σr than vanilla DMF. Therefore, the AIR-Net enhances the implicit tendency of
DMF toward low-rank.
3.2 The dynamics of adaptive regularizer
Now suppose X is given and fixed. We focus on the converge property of RTi (W) based on the
evolutionary of the dynamics ofLi. RTi(W) vanishes at the end and avoids AIR-Net suffering from
saturation issues.
Theorem 2. Consider the gradient flow model, where Ti (X)k,: = 1 and Ti (X)k,l > 0. Ifwe
initialize Wi (0) = ε1mi ×mi, then W (t) will keep symmetric during optimization. We can get the
following element-wise convergence relationship
Li(k,l)(t) - Ll(k,l)
(mi + 2ki) ∙ exp(-D ∙ t),
exp (-D ∙ t),
(mi - 1) ∙ (mi + 2ki) exp(-D ∙ t),
(k, l) ∈C1
(k, l) ∈C2
k=l
where C1	= (k,	l)	| k	6=l,Ti(X):,k	6=Ti(X):,l	,C2=	(k, l) | k	6= l, Ti	(X):,k =	Ti	(X):,l ,
τ,*
Li(k,l)
0,
Y,
_ Pmi	L*
-	l0=1,l06=l Li(k,l0),
(k, l) ∈C1
(k, l) ∈C2
k=l
Li(k,l)(t) is (k, l)-th the element ofLi (t), 1mi×mi is a matrix of all-one. Y
2
2
mi+2ki '
∣{C^,^=0}∣
D is a constant defined in A.4 which equals to zero if and only ifX = 1mi×ni
Proof. We prove this theorem in A.4.	口
This Theorem gives the limit point L* and convergence rate of Li(t). (k,l) ∈ Ci Li(k, l) = 0.
unless Ti (X): k = Ti (X): l or k = l, that is to say, in the end, L* will only think that the exact same
columns in Ti (X) are related. Li(k,l) (t) converges faster when (k, l) ∈ C2 than (k, l) ∈ C1. In
another word, adaptive regularizer captures the similarity first. This convergence rate gap products
a multi-scale similarity which will be discussed in Section 4.1. Additionally, it’s not difficult to find
RW* = 0, the convergence rate is given as follow:
Corollary 1. In the setting of Theorem 2, we further have 0	≤	RWi (t)	≤
2 (mi + 2ki)(mi — 1) mi ∙ exp(-Dt), i = 1, 2,...,N.
4
Under review as a conference paper at ICLR 2022
Proof. We prove this theorem in appendix A.5.
□
According to Corollary 1, lim RWi (t) → 0. Therefore, the regularization will vanish at the end
t→+∞	i
and not induce the saturation issue.
Remark 1. Notice that we have no restriction on specific Ti, A or representation of X in the above
proof. Therefore, the conclusion in this subsection is a general result for inverse problem.
In this subsection, we demonstrate AIR-Net’s fantastic theoretical properties. It can both enhance the
implicit low-rank and avoid saturation issues. We will verify these properties and the effectiveness
of AIR-Net in applications experimentally.
4 Experimental Analysis
Now we demonstrate the adaptive properties of AIR-Net by numerical experiments: (a) Lr and Lc
capture the structural similarity in data from large scale to small scale (Section 4.1); (b) The com-
prehensive similarity in all scales contribute to successful MC, therefore the adaptive regularizer is
necessary (Section 4.2); (c) Because AIR-Net is adaptive to data, it avoids over-fitting and achieves
good performance. (Section 4.3).
Data type and sampling pattern Three types of matrices are considered: gray-scale image, user-
movie rating matrix, and drug-target interaction (DTI) data. Three standard test gray images of size
240 × 240(Monti et al., 2017) are included in the image type (Baboon, Barbara, and Cameraman).
The user-movie rating matrix is Syn-Netflix which is of 150 × 200, and the DTI data has Ion chan-
nels (IC) and G protein-coupled receptor (GPCR) are shaped 210 × 204 and 223 × 95 respectively
(Boyarski et al., 2019b; Mongia & Majumdar, 2020). The sampling patterns include random miss-
ing, patch missing and textural missing, which are listed in Figure 4. The random missing rate varies
in different experiments, and the default is 30%.
Xj-X上」
Parameter settings We set λ = μ = max.n min to ensure the fidelity and the regularization are
in the same order of magnitude, where Xmax and Xm® are maximum and minimum of X*. The
δ is a threshold which We set as ImO by default. All the parameters in AIR-Net are initialized with
Gaussian distribution, which owns zero mean and 10-5 as its variance. The Adam is chosen as the
optimization algorithm by default (Kingma & Ba, 2015).
4.1	AIR-Net capture relationship adaptive to both s patial and time domain
In this section, we will verify the previously proposed theorems. This section provides a few slices
of Lr and Lc during training to demonstrate what AIR-Net can learn. The heatmap of Lr (t) and
Lc(t) for Baboon at t = 4000, 7000, 10000 respectively are shown in Figure 1. The according
results for Syn-Netflix are shown in Figure 5 in A.1. The first row shows the heatmap of Lr (t) and
the second one shows the heatmap of Lc(t).
As Figure 1 shows, both Lr(t) and Lc(t) first appear many blocks (t = 4000). Specially, we
sigh two of Lc (t = 4000) out. These blocks indicate that these corresponding blocks columns
are highly related. These blocks correspond to columns in which the eyes of Baboon are located,
which are indeed highly similar. However, the slight difference between these columns induces the
relationship captured by adaptive regularizer focusing on the related columns (t = 7000), which is
similar to TV(Rudin et al., 1992). The columns of Baboon are not fully the same. The regularization
gradually vanishes (t = 10000), which matches the results of Theorem 2 (Figure 1). Except the
gray-scale images, the results on Syn-Netflix give similar conclusion.
These results illustrate that AIR-Net captures the similarity from large scale to small scale. Mean-
while, a natural question is raised: does there exist a moment that both Lr and Lc are captured
accurately? If yes, we can train AIR-Net with these fixed Lr and Lc to obtain better recovery per-
formance. The experiments below show that the Lr and Lc captured by AIR-Net are necessary for
MC.
5
Under review as a conference paper at ICLR 2022
Figure 1: First row (column) shows the heatmap of Lr (Lc) at different t. A darker color indicates
a stronger similarity captured by the adaptive regularizer. The (i, j)-th element in the heatmap of
Lr(t) has a darker color than the (i, j0)-th element indicate that the t-th row is more related to j-th
row compared with j0-th row. The area in the middle of the dotted line corresponding to the small
block in the figure represents the part of the adaptive positive that is considered similar.
4.2	The necessity of utilizing an adaptive regularizer
In this section, the necessity of adaptive updating Lr and Lc is explored. Let AIR-Net have a
fixed regularizer, which is an adaptive regularizer learned at a specific step. The Normalized Mean
Absolute Error (NMAE) is adopted to measure the distance between the recovered matrix X and
the actual matrix X *:
1
NMAE =
(χmax - χmU ∣ωi
X	IXij
(i,j)∈Ω
*
- Xij
where Ω is the complement set of Ω. We utilize the regularization captured by AIR-Net at t =
4000, 7000, 9000 respectively. All of the training hyper-parameters keep the same as AIR-Net.
The Baboon under all the three missing patterns are tested.
Figure 2 shows how the NMAE changes with the epoch of training. AIR-Net, which updates the reg-
ularization during training, achieves the best performance in all missing patterns. The fixed regular-
ization can accelerate the convergence speed of the algorithm. In random missing case, RWr (9000)
and RWc (9000) is the best fixed regularizer among three time steps while other missing cases are
RWr (7000) and RWc (7000). Fixed regularizer based methods will face two problems: (a) How
to determine the best step? (b) How to estimate the regularization based on the partially observed
matrix before training? These problems are not easy to solve. AIR-Net solves these problems from
another perspective by updating the regularization during training. The adaptive property of AIR-
Net is essential to the effectiveness of AIR-Net.
4.3	AIR-Net adaptive to both varies data and missing pattern
Now we apply AIR-Net for matrix completion on three data types under different missing patterns.
Peered methods The peered methods include KNN(Goldberger et al., 2004), SVD(Troyanskaya
et al., 2001), PNMC(Yang & Xu, 2020), DMF(Arora et al., 2019) and RDMF(Li et al., 2020) in
image type. Here RDMF is replaced by DMF+DE(Boyarski et al., 2019a) because it is more suitable
in the Syn-Netflix experiment.
Avoid Over-fitting. Figure 3 shows how the NMAE of DMF and AIR-Net changes with the training
step. Compared with vanilla DMF, AIR-Net avoids over-fitting and achieves better performance on
6
Under review as a conference paper at ICLR 2022
step=4000fixed
step=7000_fixed
SteP=40。O-PatCh
step=7000-patch
step=90Oo-PatCh
2000	4000	∞00	8000
epoch
(c) Patch missing
(a) Random 30%
(b) Texture missing
epoch
epoch
---AIR-Net_random
---step=4000 random
---step=7000_random
---step=9000-random
Figure 2: Compare adaptive regularizer with fixed regularizer. The NMAE of recovered Baboon
under three types of sampling, including random missing 30% pixels, patch missing, and texture
missing, respectively. The blue line indicates the NMAE during training vanilla AIR-Net. Take the
Lr (t) and Lc (t) out, t equals 3000, 7000, 9000 respectively. The remind three lines in each figure
indicate replacing the Lr and Lc with fixed Lr (t) and Lc(t).
all the three data types and missing patterns. The Syn-Netflix and DIT data can be found in Figure 6
at A.1.
Figure 3: NMAE during training of DMF and AIR-Net. All the figures show the NMAE changes
with the training step. The first row shows the results of Cameraman with (a) random missing
(The proportion of different percentage figures show that the random missing) (b) textural missing
(c) patch missing. The second row shows the remind data type with random missing respectively,
including (d) Syn-Netflix, (e)IC and (f) GPCR.
Adaptive to data. Our proposed method achieves the best-recovered performance in most tasks.
Table 1 shows the efficacy of AIR-Net on the various data types. More surprising is that our meth-
ods perform better than other methods, which are well designed for the particular data type. The
recovered results are shown in Figure 4. In this figure, the existing methods perform well on specific
missing pattern data. Such as the RDMF achieved good performance on the random missing case but
performed not OK on reminding missing patterns. PNMC completed the patch missing well while
obtaining worse results on texture missing. Thanks to the proposed model’s adaptive properties, our
method achieves promising results both visually and by numerical measures.
7
Under review as a conference paper at ICLR 2022
Table 1: NMAE values of compared algorithms with different missing patterns in differ-
ent images. The bold font-type indicates the best performance. KNN(Goldberger et al.,
2004), SVD(Troyanskaya et al., 2001), PNMC(Yang & Xu, 2020), DMF(Arora et al., 2019),
DMF+DE(Boyarski et al., 2019a), RDMF(Li et al., 2020), AIR-Net(proposed). Some elements
without value are not suitable for that data type.
Data	Missing	KNN	SVD	PNMC	DMF	RDMF	DMF+DE	Proposed
	30%	0.083	0.0621	0.0622	0.0613	0.0494	-	0.0471
Barbara	Patch	0.1563	0.2324	0.2055	0.7664	0.3025	-	0.1195
	Texture	0.0712	0.1331	0.1100	0.3885	0.1864	-	0.0692
	30%	0.0831	0.1631	0.0965	0.2134	0.0926	-	0.0814
Baboon	Patch	0.1195	0.1571	0.1722	0.8133	0.2111	-	0.1316
	Texture	0.1237	0.1815	0.1488	0.5835	0.2818	-	0.1208
	70%	0.0032	0.0376	-	0.0003	-	0.0008	0.0002
Syn-Netflix	75%	0.0046	0.0378	-	0.0004	-	0.0009	0.0003
	80%	0.0092	0.0414	-	0.0014	-	0.0012	0.0007
IC	20%	0.0169	0.0547	-	0.0773	-	0.0151	0.0134
GPCR	20%	0.0409	0.0565	-	0.1513	-	0.0245	0.0271
(a)
Random
missing
(b)
Texture
Missing
(C)
Patch
Missing
Observed KNN SVD PNMC DMF RDMF Proposed
Figure 4: Compared KNN(Goldberger et al., 2004), SVD(Troyanskaya et al., 2001), PNMC(Yang
& Xu, 2020), DMF(Arora et al., 2019), RDMF(Li et al., 2020), AIR-Net(proposed) on Babara with
three types of data respectively.
8
Under review as a conference paper at ICLR 2022
5 Conclusion
We have proposed AIR-Net which aims to solve the MC problem without knowning the prior in
advance. We show that our AIR-Net can adaptively learn the regularization according to different
data at different training steps. In addition, we demonstrate that AIR-Net can avoid the saturation
issue and over-fitting issue simultaneously. In fact, the AIR-Net is a general framework for solving
the inverse problem. In the future work, we will combine other implicit regularization such as F-
Principle(Xu et al., 2019) with more flexible Ti for other inverse problems.
References
Sanjeev Arora, Nadav Cohen, W. Hu, and Yuping Luo. Implicit regularization in deep matrix fac-
torization. In NeurIPS, 2019.
A. Boyarski, Sanketh Vedula, and A. Bronstein. Deep matrix factorization with spectral geometric
regularization. arXiv: Learning, 2019a.
A. Boyarski, Sanketh Vedula, and A. Bronstein. Spectral geometric matrix completion. 2019b.
A. Buades, B. Coll, and J. Morel. A non-local algorithm for image denoising. 2005 IEEE Computer
Society Conference on Computer Vision and Pattern Recognition (CVPR,05), 2:60-65 vol. 2,
2005.
Emmanuel J. Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of Computational Mathematics, 9:717-772, 2009.
Prithvijit Chakrabarty and Subhransu Maji. The spectral bias of the deep image prior. ArXiv,
abs/2107.01125, 2019.
Kostadin Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-
domain collaborative filtering. IEEE Transactions on Image Processing, 16:2080-2095, 2007.
Jicong Fan and Jieyu Cheng. Matrix completion by deep matrix factorization. Neural networks :
the official journal of the International Neural Network Society, 98:34-41, 2018.
Jacob Goldberger, Sam T. Roweis, Geoffrey E. Hinton, and Ruslan Salakhutdinov. Neighbourhood
components analysis. In NIPS, 2004.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2015.
Yehuda Koren, Robert M. Bell, and Chris Volinsky. Matrix factorization techniques for recom-
mender systems. Computer, 42, 2009.
Housen Li, Johannes Schwab, Stephan Antholzer, and M. Haltmeier. Nett: Solving inverse problems
with deep neural networks. ArXiv, abs/1803.00092, 2018.
Zhemin Li, Zhi-Qin John Xu, Tao Luo, and Hongxia Wang. A regularized deep matrix factorized
model of matrix completion for image restoration. ArXiv, abs/2007.14581, 2020.
Zhouchen Lin, Minming Chen, and Yuliang Ma. The augmented lagrange multiplier method for
exact recovery of corrupted low-rank matrices. ArXiv, abs/1009.5055, 2010.
Jiaming Liu, Yu Sun, Xiaojian Xu, and U. Kamilov. Image restoration using total variation regular-
ized deep image prior. ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 7715-7719, 2019.
Christopher A. Metzler, A. Mousavi, Reinhard Heckel, and Richard Baraniuk. Unsupervised learn-
ing with stein’s unbiased risk estimator. ArXiv, abs/1805.10531, 2018.
Aanchal Mongia and A. Majumdar. Drug-target interaction prediction using multi graph regularized
nuclear norm minimization. PLoS ONE, 15, 2020.
9
Under review as a conference paper at ICLR 2022
Federico Monti, Michael M. Bronstein, and Xavier Bresson. Geometric matrix completion with
recurrent multi-graph neural networks. In NIPS, 2017.
SUbhadiP Mukherjee, Marcello Carioni, O. Oktem, and C. Schonlieb. End-to-end reconstruction
meets data-driven regularization for inverse problems. ArXiv, abs/2106.03538, 2021.
Netflix. Netflix Prize rules, 2009. https://www.netflixprize.com/assets/rules.
pdf.
Adityanarayanan Radhakrishnan, G. Stefanakis, Mikhail Belkin, and Caroline Uhler. SimPle,
fast, and flexible framework for matrix comPletion with infinite width neural networks. ArXiv,
abs/2108.00131, 2021.
Nasim Rahaman, A. Baratin, D. Arpit, Felix Draxler, Min Lin, F. Hamprecht, Yoshua Bengio, and
Aaron C. Courville. On the sPectral bias of neural networks. In ICML, 2019.
Yaniv Romano, M. Protter, and Michael Elad. Single image interpolation via adaptive nonlocal
sparsity-based modeling. IEEE Transactions on Image Processing, 23:3085-3098, 2014.
L. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica
D: Nonlinear Phenomena, 60:259-268, 1992.
O. Troyanskaya, M. Cantor, G. Sherlock, P. Brown, T. Hastie, R. Tibshirani, D. Botstein, and R. Alt-
man. Missing value estimation methods for dna microarrays. Bioinformatics, 17 6:520-5, 2001.
Dmitry Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior. 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9446-9454, 2018.
Zaiwen Wen, Wotao Yin, and Yin Zhang. Solving a low-rank factorization model for matrix com-
pletion by a nonlinear successive over-relaxation algorithm. Mathematical Programming Com-
putation, 4:333-361, 2012.
Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Z. Ma. Frequency principle: Fourier
analysis sheds light on deep neural networks. ArXiv, abs/1901.06523, 2019.
Mingming Yang and Songhua Xu. A novel patch-based nonlinear matrix completion algorithm for
image analysis through convolutional neural network. Neurocomputing, 389:56-82, 2020.
A	Appendix
A. 1 Experiments Results
In this section, we place the experiments mentioned before. Figure 5 shows the heatmap of Lr and
Lc learned by adaptive regularizer. Eventually, adaptive regularizer obtain the Lr and Lc which are
highly similar to real Lr and Lc in first column.
Figure 6 shows the NMAE of Syn-Netflix, IC and GPCR during training, respectively. This experi-
ment result also shows the ability to avoid over-fitting.
A.2 Introduction of DMF
Assumption 1. Factor matrices are balanced at initialization, i.e.,
W [l+1] > (0)W [l+1] (0) = W[l](0)W[l]>(0),	l = 0,...,L-2.
Under this assumption, Arora et al. studied the gradient flow of the non-regularized risk function
LY, i.e.,
∂
W [l](t) = - ∂W[ξ LY (X (t)), t ≥ 0, l = 0,...,L - 1,	(5)
10
Under review as a conference paper at ICLR 2022
Step=5000	Step=7000	Step=IOOOO
0112233445566力8899110121132143154165176187198
S 1 111
m
Ie
Figure 5: The first column shows the realistic relationship among columns and rows respectively.
The remind three columns are the Laplacian matrix learned by AIR at different step.
Figure 6: The first row shows the real data of Syn-Netflix, IC and GPCR respectively. The second
row shows the corresponding NMAE during training.
11
Under review as a conference paper at ICLR 2022
where the empirical risk LY can be any analytic function of X(t). According to the analyticity of
LY, X(t) has the following singular value decomposition where each matrix is an analytic function
of t:
X(t) = U(t)S(t)V>(t),
where U(t) ∈ Rm,min{m,n}, S(t) ∈ Rmin{m,n},min{m,n}, and V(t) ∈ Rmin{m,n},n are analytic
functions of t; and for every t, the matrices U (t) and V (t) have orthonormal columns, while S(t)
is diagonal (its diagonal entries may be negative and may appear in any order). The diagonal entries
of S(t), which we denote by σ1 (t), . . . , σmin{m,n}(t), are signed singular values of X(t). The
columns ofU(t) and V (t), denoted by U1(t), . . . , Umin{m,n}(t) and V1(t), . . . , Vmin{m,n}(t), are
the corresponding left and right singular vectors respectively. Based on these notation, Arora derive
the following singular values evolutionary dynamics equation.
Proposition 1 ((Arora et al., 2019, Theorem 3)). Consider the dynamics Equation 5 with initial data
satisfying Assumption 1. Then the signed singular values σk (t) of the product matrix X(t) evolve
by:
σk(t) = -L (σ2(t))1-L NXLY(X(t)), U"k(t)k>(t)> ,	⑹
k = 1, . . . , min {m, n} .
If the matrix factorization is non-degenerate, i.e., has depth L ≥ 2, the singular values need not be
signed (we may assume σk(t) ≥ 0 for all t ).
Arora et al. claimed the terms ⑸⑴)1 l enhance the movement of large singular values, and on
the other hand, attenuate that of small ones. The enhancement/attenuation becomes more significant
as L grows.
A.3 Proof of Theorem 1
We first give the details of the proposed adaptive regularizer with a iterative definition:
	'RWi(T (X)) = tr Wi (X)> LiTi (X)) Li = (Ai ∙ 1mi×mi ) © Imi- Ai
<	Ai = Ai (1mi ×mi - Imi )	, A = exp(Wi + W>) [i =	kexp(Wi)k1
Theorem 1. Consider the following dynamics with initial parameters satisfying Assumption 1:
∂
W [l](t) = - ∂W[ξ Lall(X (t)), t ≥ 0, l = 0,...,L - 1,
where Lall(X) = LY(X) + λr ∙ RWr (X) + λc ∙ RWc (X). Then we haveforany k = 1,2,...
σ k (t) = - l S2(t))1-1(Vw Lγ(X (t)), u：,k (t)k> (t)〉
C	3一1
-2L (σ2⑴)2 L Yk⑴，
where X(t) = U(t)S(t)V>(t), X = P*。：,,忆>,Yk(t) = U>LrU：,k + 忆>L匕® ≥ 0.
s
Proof. This is proved by direct calculation:
∂tr (λr ∙ X>LrX + λc ∙ XLcXτ)
VW (λr ∙ Rr + λc ∙Rc) = ~U-------r c--------c-L
∂X
= 2λr ∙ LrX + 2λc∙ XLc
=2λr ∙ Lr XσsU:,SWT + 2λc ∙ XσsU:,SWTLc.
s
s
Note that
hV:,S,	VS0i =	hU:,S, US0i =	δSS0	=	0,.	s	6=	s0
12
Under review as a conference paper at ICLR 2022
Therefore
U>k "W (λr , Rr + λc ∙ Rc)) V:,k = 2σk (λr - U›Lr U:,k + λc , V>LcV:,k )
二2σk Yk (t),
where the term γk(t) = 2σk(λr ∙ U>LrU：,k + λc ∙ V：>LcV：,k) ≥ 0. Furthermore, according to
Equation 6, we have U>VWLYV：,k = -L (σ2(t))1-L DVWLi(X(t)), U：,k(t)V>(t)).
Finally, according to W⑶(t) = -dWrrLaU(X(t)), we have
σk(t) = -L (σ2(t))1-L (VwLY(X(t)), U：,k(t)V>(t)> - 2L (σ2(t))2-L Yk(t).
□
A.4 PROOF OF THEOREM 2
Proposition 2. VWi(RWi(X)) = 2C Θ Ai- 2tr (CAi) A'i, where A'i = k或g¾∣, Ai =
Ai + Aiτ and C = 1me×m^ ∙(行(X) Ti (X)τ Θ Imi) -Ti (X) Ti (X)>.
Proof. We denote X = T (X) ∈ Rmi×ni, then we consider d [tr (XTLiX)]
d [tr (XTLiX)]
=tr [d (LiXXt)]
=tr [(dAi Θ (1miXmi - Imi) ∙ ImiXmi)Θ In ∙ XXT
-dAi Θ (ImiXmi- Imi) XX>]
=tr [(XXτ)τ (Imi Θ (dAi Θ (1mi Xmi - Imi ) ∙ 1mi Xmi ))
-(XXT)T((ImiXmi- Imi) Θ dAi)]
=tr [(xx' θ Imi) dAi θ (ImiXmi - Imi) ∙ 1mi Xmi
-(XXT θ (ImiXmi
Imi))T
—
=tr [((XXT Θ Imi)ImiXmi)T (dAi θ (ImiXmi - Imi))
-(XXT θ (ImiXmi
Imi))T
—
=tr [ (((XXT θ Imi) ∙ ImiXmi) θ (ImiXmi- Imi)-(XXT θ (ImiXmi
=tr [((XXT Θ Imi) 1miXmi- XXT) dA/
—
13
Under review as a conference paper at ICLR 2022
We denote C = (XX> Θ Imi) 5小皿-XX>,$叫=Ui ∙ exp (Wi) ∙ 1m“ then
d Itr(X>LiX)]
=tr (CdAi)
=/tr [C (SWi ∙ exp(Wi + WiT) Θ d(Wi + WiT))
SWi
-	C (imi (exp(Wi) Θ dWi) Im) exp(Wi + W>)]
=tr [C ∙ (Ai Θ d (Wi + Wi>))]
-	∕tr[1mi×mi (exp(Wi) Θ dWi)] ∙ tr [C ∙ exp (Wi + Wi>)]
SWi
=tr [C ∙ (Ai Θ d (Wi + Wi>))]
-	∕tr[(1mi×mi Θ exp(Wi)) dWi] ∙ tr [C ∙ exp (Wi + Wi>)]
SWi
=tr [C ∙ (Ai Θ d (Wi + Wi>))] - tr[tr(C ∙ Ai) ∙ AidWi]
=tr [((C> Θ Ai)> + C> Θ Ai- tr(C ∙ Ai) Ai) dWi]
Therefore,
VWitr(X>LiX) = (C> © Ai)> + C> Θ Ai- tr(C ∙ Ai) Ai
= 2CAi -2tr(CA0i)A0i
Notice that X = Ti (X) ∈ Rmi×ni, the proposition is proved.
□
Theorem 2. Consider the gradient flow model, assume Ti (X)k,:	= 1 and Ti (X)k,l > 0, if we
initialize Wi (0) = ε1mi×mi, then W (t) will keep symmetric during optimization. We can get the
element-wise convergence relationship
I j	(mi + 2ki) ∙ exp(-D ∙ t),	(k,l) ∈ Ci
Li(k,l)⑴一Li(k,l)∖ ≤ V	exp (-D ∙t),	(k,l) ∈ C2
[(mi — 1) ∙ (mi + 2ki) exp(-D ∙ t),	k = l
where C1
n(k, l) | k 6= l, Ti (X):,k 6= Ti (X):,l o, C2 = n(k, l) | k 6= l, Ti (X):,k = Ti (X):,l o,
0,
τ *
Li(k,l)
-	lm0=i 1,l06=l Li*(k,l0),
(k, l) ∈C1
(k, l) ∈C2
k=l
Li(k,l)(t) is the element of Li (t) at the k-th row and l-th column, 1mi×mi is all one elements
matrix. Y = ∣ » 2 C]∣ =——‰r
∣{c^,^=0}∣	mi+2ki
D is a constant defined in A.4 which equals to zero if and only
if X = 1mi×ni.
Proof. We rewritten the gradient in proposition 2 with element wise formulation:
, ,. ,. ..
WTi(k,i) (t) = (2Cα(t) - 4Ck,ι) ∙ Ai(k,i)(t),
where Ca(t) = tr (CA0i) and the sub index denote the element in matrix.
With the assumption that Ti (X)k,:	= 1 and Ti (X)k,l > 0, we have 0 ≤
(Ti(X)Ti(X)T)	≤	1.	Therefore	Ck,l	=	(Ti(X)Ti(X)T)	-	(Ti(X)Ti(X)T)	≥ 0
and SPeCiany Ck,k = 0, as Ai(W = |需卷"1 > 0, we have
mi
Ca(t) = tr (CA0i) = X Ck,lA0i(k,l)(t) > 0
k=1,l=1
14
Under review as a conference paper at ICLR 2022
Denote C^^ ∈ min Ckl we have C^^ ≤ Ck,ι and then consider
2.
2.
• ∙
WTi(^,^) (t)- WTi(k,i)⑴
=2Ca⑴(Ai(^,^)⑴-Ai(k,i)⑴)-4 (C^,^Ai(^,^)⑴-ck,ιAr0(k,i)
As we initialize Wi (0) = ε1m°…,therefore Ai(k,i)(0)=焉,∀k,l. Therefore Wτi(^,^) (0)-
WTig) (0)	=	-4	(c^,^-Ck,i)	Ai(k,i)(0)	= - m (c^,^-Ck,i) ≥	0.	Then we have
W7i(^,^)⑴ ≥ WTi(k,i)⑴ and Ai(^,^)W ≥ Ai(k,i)⑴，the equal istoken if and only if t = 0
_L
2.
or C^ ^ = Ck,ι Furthermore, Wτi(^,^) (t) - W7i(
k,i)⑴ ≥ -4 (c^,^- CkG Ai(k,i)(0), then
WTi(k,D (t) - WTiki) (t) ≥ D^,i,k,ι ∙t, where Dkm = -4 (CkL CkQ Ai(k,i)(0) ≥ 0. NeXt,
we consider
A，. (t) =	exp(Wi(^Q
i(k,l)	∣∣eχp(W7i(k,0 )∣∣1
=eχP(Wi(^,^))
=P exp(WTw))
1
Wexp(WWl,- W"))
1
P exp(-Dk,ι,k,ι∙t
k,i
As Ck k = 0 and Ck i ≥ 0, therefore Ckk ∈ min Ck i=0. It is not difficult to show that C^ ^ = 0 if
,	,	, k,i ,	k,i
and only if Ti (X[^ = Ti (X)：/ If ∣{c^,^ = 0} ∣ = m% + 2ki, then when C^,^ = 0, A[^ ^) (t) ≥
1
,where E^ ^ (+∞) = 0. Notice that E Ai(k I) (t) = 1, We have
,	k,l ,
mi+2ki + E^,^(t)
1
mi + 2ki + E^,^⑴一
Ai(Kf) (t) ≤ m i + 2kr
Therefore,
A0
Aig)
(+∞)
2A0	=
2Ai(k^)=
tion of Li,
2
mi + 2ki
I mi + 2ki
γ, Ai(k,l) (+∞) =
,Ti(X):,k 6=Ti(X):,l
,Ti(X):,k =Ti(X):,l .
0	,Ti(X):,k 6=Ti(X)
γ ,Ti(X):,k =Ti(X)
Furthermore, Ai(^ ^)
:,l . According to the defini-
:,l
0
1
T *
Li(k,l)
Li(k,l) (t)(+∞)
0	k 6=l,Ti(X):,k 6=Ti(X):,i
γ	k 6=l,Ti(X):,k=Ti(X):,i
- Pi0=* i 1,i06=k Li*(k,i0)	k = l
Until now, we have prove that the adaptive regularization part of AIR-Net will convergence at the
end. That is the upper bound of Li*(k,i) - Li(k,i)(t). Next we will focus on the convergence rate
of AIR-Net. We discuss the rate under the three cases in the aforementioned formulation separately.
We simplify the notation furthermore before continue.
{(k,l) |	k = l, Ti	(X ):,k = Ti	(X )：,i}, C =	{(k,l)	| k = l, Ti	(X ):,k	=	Ti	(X ):,i}.	Then
the formulation is simplified as
W" Pmi 0 L*
-	i0=1,i0 6=k Li(k,i0)
(k, l) ∈C1
(k, l) ∈C2
k=l
15
Under review as a conference paper at ICLR 2022
If (k,l) ∈ C2 or k = l, we denote D = minDk,ι, according to the definition of Ek,ι(t), we have
Ek,ι(t) ≤ exp(-D ∙ t)
I闽(k,l)- 4(k,l")∣=2 m. +12ki -
≤ 2	Ek,ι(t)
一(mi + 2ki)2
1	一
mi + 2ki + Ek,l (t)_
≤ 2 ∙
fl) ∙ exp(-D ∙ t)
(mi + 2ki)2
≤ exp (-D ∙ t)
Specially, when (k, l) ∈ C2 we have
I Li(k,l) - Li(k,l)⑴ I = I A↑(k,l) - Ai(k,l)⑴ I ≤ exp (-D ∙t)
If (k,l) ∈ Ci,
1 Li(k,l) - Li(k,l)(t)| = Ai(k,l) - Ai(k,l)(t)| = I Ai(k,l)⑴ 1
≤ E	Ai(k0,l0)⑴
(k0,l0)∈Cι∪sC3
=2 - X	Ai(k0,l0)⑴
(k0,l0)∈C2∪C3
=Y Ymi + 2ki) -	Ai(k0,l0)⑴
(k0 ,lz)∈C2UC3
= E	(Y- Ai(k0,l0)⑴)
(k0,l0)∈C2UC3
≤	X	i Y - Ai(k0,l0)⑴ i
(k0,l0)∈C2UC3
=	X	I Ai(kz,lz) - Ai(k0,l0)⑴	I ≤ (mi	+ 2ki)	∙ exp(-D ∙ t)
(k0,l0)∈C2UC3
If k = l,
,	,	mi	,	、
I Li(k,l) - Li(k,l)⑴ I = I X	(Li(k,lz) - Li(k,l0)⑴)I ≤ (mi - I) ∙ (mi + 2ki) ∙ exp(-D t)
l0 = 1,l0 = k
□
A.5 Proof of Corollary 1
Corollary 1. In the setting of Theorem 2, we further have 0	≤	RWi (t)	≤
2 (mi + 2ki)(mi — 1) mi ∙ exp(-Dt).
Proof.
RWi(t) = X Li(k,l)⑴⑴ M(X)：,k 一万(X)：,l IlF
k,l
≤ 2 ∙ fLi(k,l)(t)(t) ≤ mi (mi - 1) (mi + 2ki) ∙ exp(-Dt)
k=l
□
16