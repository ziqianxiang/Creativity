Under review as a conference paper at ICLR 2022
AA-PINN: Attention Augmented Physics In-
formed Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Physics Informed Neural Networks has been quite successful in modelling the com-
plex nature of fluid flow. Computational Fluid Dynamics using parallel processing
algorithms on GPUs have considerably reduced the time to solve the Navier Stokes
Equations. CFD based approaches uses approximates to make the modelling easy
but it comes at the cost of decrease in accuracy. In this paper, we propose an
attention based network architecture named AA-PINN to model PDEs behind fluid
flow. We use a combination of channel and spatial attention module. We propose a
novel loss function which is more robust in handling the initial as well as boundary
conditions imposed. Using evaluation metrics like RMSE, divergence and thermal
kinetic energy, our network outperforms previous PINNs for modelling Navier
Stokes and Burgers Equation.
1	Introduction
Computational Fluid Dynamics (CFD) has become the core technology behind almost every fluid
simulation. Fluid mechanics has been traditionally concerned with big data, thus making deep
learning an obvious choice in modelling the inherent complexity of the problem. Neural Networks
of late has been quite successful in understanding, predicting, optimizing, and controlling fluid
flows. Neural Network has proven to improve optimization performance and reduce convergence
time drastically. Neural network is also used for turbulence modelling and identifying low and high
dimensional flow regimes. Deep learning algorithms are able to take into account inherent complexity
of the problem thus optimizing for the performance, robustness or convergence for complex tasks.
Understanding the physics behind fluid flows is a complex problem which can be solved using neural
networks by feeding lots of training data. It helps in providing a general purpose framework for
interpretability, generalizability and explainability of the results achieved.
2	Related Work
Neural network to solve Reynolds Averaged Navier Stokes Equation was proposed by (Ling et al.,
2016). The Reynolds stress term was modelled using DNS equation by (Wang et al., 2017). Neural
network was used to model turbulent flows using Large Eddy simulation (Zhou et al., 2019). A
convolutional neural network was used to model the velocity field over a cylinder (Jin et al., 2018).
(Wu et al., 2020) proposed a similar CNN based method to model the unsteady flow in arbitrary
fluid regimes. A thorough study of data driven methods using machine learning approaches for
modelling the turbulence was studied by (Duraisamy et al., 2019). (Brunton et al., 2020) also did
a comprehensive study of machine learning approaches for modelling different kind of problems
in fluid mechanics. (Raissi et al., 2017) proposed physics informed neural networks for solving
nonlinear partial differential equations using neural network.
This work was further improved in (Raissi et al., 2019). The theoretical exact solution of the 3d Navier
Stokes equation was shown by (Ethier and Steinman, 1994). Deep feedforward neural networks
was used (Lui and Wolf, 2019) for modelling complex flow regimes. CNN were used for making
faster fluid simulation (Tompson et al., 2017). A novel neural network was proposed for solving
the function approximation and inverse PDE problems (Meng and Karniadakis, 2020). (Khoo et al.,
2021) used neural network for solving parametric PDEs. (Meng et al., 2020) proposed a neural
network for solving unsteady PDEs. Bayesian neural network was used to quantify the uncertainty
1
Under review as a conference paper at ICLR 2022
while solving PDEs (Yang et al., 2021). Data driven approaches for solving PDEs was proposed by
(Long et al., 2018) and (Long et al., 2019). c(Sirignano et al., 2020) used neural network for solving
PDEs in the context of large-eddy simulation.
(Bar and Sochen, 2019) was one of the first works to use unsupervised learning for solving PDEs.
(Thuerey et al., 2020) proposed a deep learning approach for solving Reynolds-averaged Navier-
Stokes equation around airfoils. Another approach for solving PDEs using deep learning was used
(Miyanawala and Jaiman, 2017) in the context of unsteady wake flow dynamics. A comprehensive
study of deep learning approaches for modelling and solving fluid mechanics problem was done
by (Kutz, 2017). (Ranade et al., 2021) proposed a deep learning based solver for Navier-Stokes
equations using finite volume discretization. Neural networks was used for solving incompressible
Navier-Stokes equations (Jin et al., 2021). A method for predicting turbulent flows using deep
learning was proposed by (Wang et al., 2020).
Our main contributions can be summarized as:
•	A novel network architecture combining channel and spatial attention mechanism is used for
modelling the inherent complexity in fluid flow problems.
•	We train and test our network using a more robust loss function for solve PDEs behind incompressible
Navier Stokes and Burgers Equation.
•	Our network achieves better results than previous PINNs using commonly used evaluation metrics
while still running at good enough speed.
3 Background
3.1	Navier Stokes Equation
The incompressible transient two dimensional Navier-Stokes equations for mass and momentum
conservation are written as defined in the below set of equations:
V ∙ u = 0	(1)
Ux ∂ux +uy ∂ux = -1 ∂p + νV2 Ux + gx	⑵
∂ x	∂y	ρ ∂ x
∂Uy	∂Uy	1 ∂p
Ux^∂X + Uy西=-Pdy + VV Uy+gy	⑶
in which U is the velocity field (with xand y components for 2 dimensional flows). Here g represents
the gravitational acceleration and μ the dynamic viscosity of the fluid.
3.2	Momentum Equations
When the difference operators are expanded using uniform grid spacing h and time step k results in:
Uij — ~Γ( (UiTj + UijT — 4Ui,j +Ui,j + 1 + Ui+1,j) + 工(Un+1,jUi+1,j — UnjUij)
h	k	h	k	(4)
+ h (Vnjeij- Vnj-IeijT) = Unj- h (Pi+1j - Pij)
Where variables without superscripts denote advanced time level results to be computed. Using the
formulas of the averages and collecting the terms results in equation below:
k
-A1Ui-1,j - A2Ui,j-1 + A3Uij - A4Uij+1 - A5Ui+1 j = bij - h (Pi+1,j - pi,j)	(5)
2
Under review as a conference paper at ICLR 2022
The various coefficients in the above equation are given using the set of equations as follows:
(6)
(7)
(8)
(9)
(10)
It is to be noted that in the continuous equations, we ignored effects of correction quantities in
advective and diffusive terms. The u-component velocity correction can be written in the form as
defined in Equation below:
UIj = Akh (pi,j- pi+ι,j
(11)
We now present analogous results for the y-momentum equation. The v-component velocity correction
can be written in the form as defined in Equation below:
* = B3h S" -pij+ι)
(12)
3.3 Pressure Poisson Equation
By substituting the velocity corrections into the discrete continuity equation for grid cell (i, j) results
in:
ui,j - ui-1,j
+ vi,j - vi,jT = 0
hy
(13)
After substituting the decomposed velocity components, the above equation can be written as:
(U* + UO)i,j - (U* + UO)i-1,j + (V* + VO)i,j - (V* + VO)i,j-1 =0
hx	hy
(14)
For simplicitiy, we sethx =hy = h, and rewrite this as:
—
1O	1O
AZTj Pij- Bj pi,j-1+
十 1
A3,i-1,j
1O
+ B3,i,j + B3,i,j-1
B^pi,j+1 - Epi+1,j
h2
——D* .
k Dij
(15)
—
1
1
1
It can alternatively written in a more compact form similar to that used for the momentum equations:
C1pOi-1,j	+	C2pOi,j-1	+	C3pOi,j	+	C4pOi,j+1	+	C5pOi+1,j =	di*,j	(16)
3
Under review as a conference paper at ICLR 2022
The various coefficients in the above equation is defined as follows:
C1 — A3,i-1,j,	C2 — B3,i,j-1 ,	C3 —	(A3,i,j + A3,i-1,j + B3,i,j + B3,i,j-1
1.	—	1 「一一 1	z7*	— h *
C4 — B3,i,j ,	C5 — A3,i,j ,	di,j — k Dij
(17)
3.4 Burger’ s Equation
In one space dimension, the Burger’s equation along with Dirichlet boundary conditions is defined
using the below set of equations:
Ut + Uux 一 (0.01∕∏)uχχ = 0, X ∈ [-1,1],	t ∈ [0,1]
u(0, x) = - sin(πx)	(18)
U(t, -1) = U(t, 1) = 0
Here, tiu, xiu, Ui Nu i=1 denotes the initial and boundary training data on U(t, x) and tif, xif Nf i=1
denotes the collocations points for f (t, x). The loss MSEu corresponds to the initial and boundary
data while MSEf enforces the structure used by equation at a finite set of collocation points.
4 Method
4.1	Spatial Attention Module
The spatial attention module is used for capturing the spatial dependencies of the feature maps. The
spatial attention (SA) module used in our network is defined below:
fSA(x) = fsigmoid (W2 (fReLU (W1(x))))	(19)
where W1 and W2 denotes the first and second 1 × 1 convolution layer respectively, x denotes the
input data, fSigmoid denotes the sigmoid function, fReLU denotes the ReLu activation function.
The spatial attention module used in this work is shown in Figure 1:
Figure 1: Details of our spatial attention module
4.2	Channel Attention Module
The channel attention module is used for extracting high level multi-scale semantic information. The
channel attention (CA) module used in our network is defined below:
fCA(x) = fsigmoid(W2(fReLU(W1fA1vgPool(x))))	(20)
where W1 and W2 denotes the first and second 1 × 1 convolution layer, x denotes the input data.
fA1 vgP ool denotes the global average pooling function, fSigmoid denotes the Sigmoid function, fReLU
denotes ReLU activation function.
The channel attention module used in this work is shown in Figure 2:
4
Under review as a conference paper at ICLR 2022
Figure 2: Details of our channel attention module
4.3	Network Architecture
We use deep convolutional neural network in this work. The input is the spatial and temporal co-
ordinates of the points in the fluid flow domain. This information is propagated to three Residual
blocks sequentially. In between the blocks, channel attention module is used to weight the usefulness
of important features and spatial attention module is used for modelling the inter-spatial relationship
of features. Fusion operator is used to merge the individual features. The output is the spatio-temporal
pressure and velocity fields predicted. The complete network architecture used in this work is shown
in Figure 3:
Figure 3: Illustration of our network architecture. A residual block denotes convolution, max pooling,
relu activation function and batch normalization layer sequentially; CA and SA denotes channel and
spatial attention module respectively; x denotes fusion operator.
4.4	Loss Functions
The MSE loss function is used for both the X and Y components of momentum equation which is
defined as:
1 Nu
MSEu = Nr Xlu O -UT
Nu
i=1
(21)
1 Nf
MSEf = Nτ∑∣f (tf ,χf)∣2	(22)
f i=1
The shared parameters between the neural networks u(t, x) and f(t, x) can be learned by minimizing
the mean squared error loss as defined using in the equation below:
MSEt = αMSEu + βMSEf
(23)
The weighting coefficients α and β are used to balance different terms of the loss function and
accelerate convergence in the training process. The individual loss function terms Le, Lb and Li
represent loss function components corresponding to the residual of the Navier-Stokes equations, the
boundary conditions, and the initial conditions, respectively. The loss function is defined using the
set of equations below:
5
Under review as a conference paper at ICLR 2022
4 Ne
Le = N XX IeVPiI2	(24)
e
i=1 n=1
1 Nb
Lb =瓦 X Iun- un∣2	(25)
b n=1
1 Ni
Li = NN X Iun- un∣2	(26)
Ni n=1
Where Nb, Ni and Ne denote the number of training data for different terms. The above 3 terms can
be combined to give:
Lt = γLe + δLb + ρLi	(27)
The weighting coefficients γ, δ and ρ are used to balance different terms of the loss function and
accelerate convergence in the training process. The complete loss function for training the parameters
of our network is defined as follows:
Lf inal = MSEt + Lt	(28)
4.5	Optimization Details
For a general gradient descent algorithm, the iterative formulation of the parameters of our network
can be expressed as:
θ(k+1) = θ(k) - ηγVθLe - ηδVθLb - ηρVθLi -邛式θMSEu -邪口MSEf	(29)
where θ denotes the parameters of the neural network, namely the weights of all the layers, k is the
iteration step, and η is the learning rate.
4.6	Evaluation Metrics
Root Mean Square Error (RMSE) is the most popularly used metric for quantifying the prediction
performance. The downside of using it is that it only measures indivdual pixel differences. There
is a need to check whether the predictions are physically meaningful and preserve desired physical
quantities, such as Turbulence Kinetic Energy and Divergence. In this work, we use following metrics
for evaluation.
1.	Root Mean Square Error: We calculate the RMSE of all predicted values from the ground truth
for each pixel.
2.	Divergence: We use the average of absolute divergence over all pixels at each prediction step as
an evaluation metric.
3.	Turbulence Kinetic Energy: The turbulence kinetic energy is characterised by measured root
mean square velocity fluctuations as defined by:
1 T
((UO)2 +(VO)2) /2,	(UO)2 = τ X(U⑴-U)2	(3O)
T t=0
where t is the time step. We calculate the turbulence kinetic energy for each predicted sample of 100
velocity fields.
6
Under review as a conference paper at ICLR 2022
4.7 Implementation Details
An adaptive optimization algorithm, Adam (Kingma and Ba, 2014), is used to optimize the loss
function. The parameters of the neural networks are randomly initialized using the Xavier intitalization
scheme. We simulate turbulent channel flow at Reτ = 9.99 × 102 using our network. The time step
value of 0.005 is used for evaluating the residuals our network. We feed the training data using
mini-batches to train our network in this study. There are three parts in the input data corresponding
to the initial conditions, the boundary conditions and the residuals of equations respectively. We
place 100,000 points inside the domain and 25,000 points on the boundary sampled at each time
step, and 150,000 points at the initial time step to determine the loss function. The total number of
iterations in one training epoch used is 100. The hyper-parameter values are α = 100, β = 100.
5	Results
The performance comparison of our network with previous state of the art is shown in Table 1:
Table 1: Comparion of SOTA networks using the number of parameters, the best number of input
frames, the best number of accumulated errors for back-propogation and training time for one epoch.
Models	TF-net	U-net	GAN	ResNet	ConvLSTM	SST	DHPM	Ours
number of parameters(106)	15.9	25.0	26.1	21.2	11.8	49.9	2.12	0.53
input length	25	25	24	26	27	23	23	20
accumulated errors	4	6	5	5	4	5	5	2
time for one epoch(min)	0.39	0.57	0.73	1.68	45.6	0.95	4.591	0.72
The exact and learned dynamics solution for the Burgers equation using our network is shown in
Figure 4:
Figure 4: A solution of the Burger’s equation (left panel) is compared to the corresponding solution
of the learned partial differential equation (right panel).
The exact and learned dynamics solution for the Navier Stokes equation using our network is shown
in Figure 5:
Figure 5: A solution of the NavierStokes equation (left panel) is compared to the corresponding
solution of the learned partial differential equation (right panel).
The actual and and the predicted dynamics of the velocity components u and v using our network at
different timeframes is shown in Figure 6:
7
Under review as a conference paper at ICLR 2022
Dynamics of u
Figure 6: The first row shows the images of the true dynamics. The last two rows show the images of
the predicted dynamics using our network.
5.1	Ablation Studies
A study of with and without using channel and spatial attention module on the performance is shown
in Table 2:
Table 2: Ablation study using variations of spatial and channel attention modules.			
Metrics	Only SA	Only CA	Both
number of parameters(106)	1.23	0.71	0.53
accumulated errors	5	3	2
time for one epoch(min)	1.05	1.16	0.72
6	Conclusions
In this paper, we present a attention based physics informed neural network named AA-PINN to
simulate incompressible Navier Stokes and Burgers Equations. We formulate our network using
Pressure-Velocity coupling. The spatial and temporal co-ordinates of the domain are input while
instantaneous pressure and velocity fields are output. We use the initial and boundary conditions
as supervised data-driven parts, while residual of the Navier-Stokes and Burgers equations as the
unsupervised part in the loss function while training our network. We propose a more robust loss
function to handle both the boundary conditions as well as initial conditions. We test the performance
our network using RMSE, divergence and TKE as the evaluation metrics. We demonstrate our
designed network is more robust while modelling the complex flow physics. In the future, we would
like to study the effect of attention mechanism for solving compressible and steady Navier Stokes
Equations.
Acknowledgments
We would like to thank Nvidia for providing the GPUs for this work.
References
L. Bar and N. Sochen. Unsupervised deep learning algorithm for pde-based forward and inverse
problems. arXiv preprint arXiv:1904.05417, 2019.
A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in
machine learning: a survey. Journal of machine learning research, 18, 2018.
S. L. Brunton, B. R. Noack, and P. Koumoutsakos. Machine learning for fluid mechanics. Annual
Review ofFluid Mechanics, 52:477-508, 2020.
S. Cai, S. Zhou, C. Xu, and Q. Gao. Dense motion estimation of particle images via a convolutional
neural network. Experiments in Fluids, 60(4):1-16, 2019.
8
Under review as a conference paper at ICLR 2022
K. Duraisamy, G. Iaccarino, and H. Xiao. Turbulence modeling in the age of data. Annual Review of
FluidMechanics, 51:357-377, 2019.
C.	R. Ethier and D. Steinman. Exact fully 3d navier-stokes solutions for benchmarking. International
Journal for Numerical Methods in Fluids, 19(5):369-375, 1994.
X. Jin, P. Cheng, W.-L. Chen, and H. Li. Prediction model of velocity field around circular cylinder
over various reynolds numbers by fusion convolutional neural networks based on pressure on the
cylinder. Physics of Fluids, 30(4):047105, 2018.
X. Jin, S. Cai, H. Li, and G. E. Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed
neural networks for the incompressible navier-stokes equations. Journal of Computational Physics,
426:109951, 2021.
Y. Khoo, J. Lu, and L. Ying. Solving parametric pde problems with artificial neural networks.
European Journal of Applied Mathematics, 32(3):421-435, 2021.
D.	P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
J.	N. Kutz. Deep learning in fluid dynamics. Journal of Fluid Mechanics, 814:1-4, 2017.
J.	Ling, A. Kurzawski, and J. Templeton. Reynolds averaged turbulence modelling using deep neural
networks with embedded invariance. Journal of Fluid Mechanics, 807:155-166, 2016.
Z. Long, Y. Lu, X. Ma, and B. Dong. Pde-net: Learning pdes from data. In International Conference
on Machine Learning, pages 3208-3216. PMLR, 2018.
Z. Long, Y. Lu, and B. Dong. Pde-net 2.0: Learning pdes from data with a numeric-symbolic hybrid
deep network. Journal of Computational Physics, 399:108925, 2019.
H. F. Lui and W. R. Wolf. Construction of reduced order models for fluid flows using deep feedforward
neural networks. arXiv preprint arXiv:1903.05206, 2019.
K.	O. Lye, S. Mishra, and D. Ray. Deep learning observables in computational fluid dynamics.
Journal of Computational Physics, 410:109339, 2020.
X. Meng and G. E. Karniadakis. A composite neural network that learns from multi-fidelity data:
Application to function approximation and inverse pde problems. Journal of Computational
Physics, 401:109020, 2020.
X. Meng, Z. Li, D. Zhang, and G. E. Karniadakis. Ppinn: Parareal physics-informed neural network
for time-dependent pdes. Computer Methods in Applied Mechanics and Engineering, 370:113250,
2020.
T. P. Miyanawala and R. K. Jaiman. An efficient deep learning technique for the navier-stokes
equations: Application to unsteady wake flow dynamics. arXiv preprint arXiv:1710.09099, 2017.
M. Raissi. Deep hidden physics models: Deep learning of nonlinear partial differential equations.
The Journal of Machine Learning Research, 19(1):932-955, 2018.
M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics informed deep learning (part i): Data-driven
solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561, 2017.
M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential
equations. Journal of Computational Physics, 378:686-707, 2019.
R. Ranade, C. Hill, and J. Pathak. Discretizationnet: A machine-learning based solver for navier-
stokes equations using finite volume discretization. Computer Methods in Applied Mechanics and
Engineering, 378:113722, 2021.
A. Sagar. Aa3dnet: Attention augmented real time 3d object detection, 2021a.
A. Sagar. Dmsanet: Dual multi scale attention network. arXiv preprint arXiv:2106.08382, 2021b.
9
Under review as a conference paper at ICLR 2022
V. Sekar, Q. Jiang, C. Shu, and B. C. Khoo. Fast flow field prediction over airfoils using deep learning
approach. Physics of Fluids, 31(5):057103, 2019.
J. Sirignano, J. F. MacArt, and J. B. Freund. Dpm: A deep learning pde augmentation method with
application to large-eddy simulation. Journal of Computational Physics, 423:109811, 2020.
L. Sun, H. Gao, S. Pan, and J.-X. Wang. Surrogate modeling for fluid flows based on physics-
constrained deep learning without simulation data. Computer Methods in Applied Mechanics and
Engineering, 361:112732, 2020.
N. Thuerey, K. WeiBenow, L. Prantl, and X. Hu. Deep learning methods for reynolds-averaged
navier-stokes simulations of airfoil flows. AIAA Journal, 58(1):25-36, 2020.
J. Tompson, K. Schlachter, P. Sprechmann, and K. Perlin. Accelerating eulerian fluid simulation with
convolutional networks. In International Conference on Machine Learning, pages 3424-3433.
PMLR, 2017.
J.-X. Wang, J.-L. Wu, and H. Xiao. Physics-informed machine learning approach for reconstructing
reynolds stress modeling discrepancies based on dns data. Physical Review Fluids, 2(3):034603,
2017.
R. Wang, K. Kashinath, M. Mustafa, A. Albert, and R. Yu. Towards physics-informed deep learning
for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pages 1457-1466, 2020.
P. Wu, J. Sun, X. Chang, W. Zhang, R. Arcucci, Y. Guo, and C. C. Pain. Data-driven reduced order
model with temporal convolutional neural network. Computer Methods in Applied Mechanics and
Engineering, 360:112766, 2020.
L. Yang, X. Meng, and G. E. Karniadakis. B-pinns: Bayesian physics-informed neural networks for
forward and inverse pde problems with noisy data. Journal of Computational Physics, 425:109913,
2021.
Z. Zhou, G. He, S. Wang, and G. Jin. Subgrid-scale model for large-eddy simulation of isotropic
turbulent flows using an artificial neural network. Computers & Fluids, 195:104319, 2019.
7 Additional Materials
7.1	Galerkin Procedure
Navier-Stokes equations can also be written as using the set of equations below:
ut + u2 x + (uv)y =	二-Px + FZu Re	(31)
Vt + (Uv)X + (v2)y =	1 一py + Re Z	(32)
ux + vy	=0	(33)
The dependent variables expressed in terms of Fourier Series is defined using the set of equations
below:
∞
u(x, y,t) = E ak(t)Pk(x, y)	(34)
k
∞
v(χ,y,t) = Ebk⑴^k(X,y)	(35)
k
10
Under review as a conference paper at ICLR 2022
∞
p(χ,y,t) = Eck⑴^k(X,y)	(36)
k
The exponential are used as basis functions as shown below:
Ψk(x, y) = eik∙x = ei(kιx+k2y) = eikιxeik2y	(37)
Substituting the above equation in momentum Navier Stokes Equation results in:
∞
i E (kιak + k2bk) Wk = 0	(38)
k
Since, this is true for all the points in the domain considered, hence it can be written as:
k1ak + k2bk =0 ∀k	(39)
On substituting these expansions into the x-momentum equation, we obtain:
∂∂	∂
∂t E a`2' + ∂χ ∑S a`amg'2m + 济 Σ a`bm夕'2m
`	`,m	`,m
∂	1 ∂ 2	∂ 2
花工 c'w'+Rek2 ? "+源?".
(40)
The process of commuting, summation and differentiation gives:
Ea'2g+i£ ('1 + mi) a`am2'2m+i£ ('2 + m2) a`bm夕'2m
`,m
`,m
-i X "'-得 X ('2 +'2) a'W'
`	e`
(41)
`
Theorem 1.1:	Let u0 , FB and tf > 0 be given such that
uo(x) ∈ H(Ω),	and FB(x,t) ∈ L2 (0,tf; H)	(42)
Then ∃a unique solution u ∈ (u1, u2)T
∂u
ui, Tj- ∈ L 9 × (O,tf)),	i,j = 1,2	(43)
∂xj
and u is continuous from [θ, tf] into H. Moreover, the following energy equation holds fort ∈ [θ, tf]:
2 d ku(t)kL2 + νku(t)kH1 = hF (t), u(t)i
Theorem 1.2:	Suppose u0, FB are given and are such that:
uo(x) ∈ V(Ω), and	FB(x,t) ∈ L2 (0,tf; H)
for tf > 0. Then ∃ a unique (strong) solution u = (u1, u2)T with
∂ui ∂ui	∂2ui	2
ui,lΓ,∂Xj,∂Xj∂Xk ∈ L (ω X (0,tf)), i,j,k = 1,2
(44)
(45)
(46)
and u is continuous from [0, tf] into V .
11
Under review as a conference paper at ICLR 2022
7.2	SIMPLE ALGORITHM
SIMPLE algorithm is a finite-volume based scheme, which uses a staggered grid which is derived
using terms of discrete equations rather than from discretization of a continuous function.
The flow variables can be expressed at any given time step as defined below:
U = u* + u , V = v* + v0 and P = p* + p	(47)
where * denotes an initial estimate, and 0 represents a correction term. This can be substituted in the
Navier Stokes Equation to produce the equation below:
(u* + u0)t + (u* + u0)2	+ ((v* + v0) (u* + u0))y = - (p* + p0)x + ν∆ (u* + u0)	(48)
We ignore effects of the corrections on advective and diffusive terms thus giving the set of equations
below in terms of both x and y momentum:
ut* + (unu*)x + (vnu*)y = -pxn + ν∆u*	(49)
u0t = -p0x	(50)
vt* + (unv*)x + (vnv*)y = -pyn+ν∆v*	(51)
vt0 = -p0y	(52)
The differentiation with respect to time of Pressure Poisson Equation is defined in the equation below:
D0 = u0x + vy0
(53)
∂D0	∂
~∂t = ∂t (ux + Vy) = Utx + Vty	(54)
Using a simple forward-difference approximation, we have:
∂D0	D0n+1 - D0n
~ti^ =	∆t	(55)
Using the set of Navier-Stokes equations, this can be written as:
∂D0
∂t
—
Next, a PPE for the correction pressure can be obtained as:
∆P0 = Dt
The boundary conditions to be employed with this PPE are defined using equations below:
p0 = 0
(56)
(57)
(58)
∂p0
∂n
0
(59)
12
Under review as a conference paper at ICLR 2022
Once p0 has been calculated, we can then update the pressure and velocity components using the set
of equations below:
pn+1 = pn + p0	(60)
un+1 = u* — p∆∆t ≡ u* + u0	(61)
∂x
vn+1 = v* — p∆∆t ≡ v* + v0	(62)
∂y
13