Under review as a conference paper at ICLR 2022
Finite-Time Error Bounds for Distributed Lin-
ear Stochastic Approximation
Ab stract
This paper considers a novel multi-agent linear stochastic approximation algo-
rithm driven by Markovian noise and general consensus-type interaction, in which
each agent evolves according to its local stochastic approximation process which
depends on the information from its neighbors. The interconnection structure
among the agents is described by a time-varying directed graph. While the con-
vergence of consensus-based stochastic approximation algorithms when the inter-
connection among the agents is described by doubly stochastic matrices (at least
in expectation) has been studied, less is known about the case when the intercon-
nection matrix is simply stochastic. For any uniformly strongly connected graph
sequences whose associated interaction matrices are stochastic, the paper derives
finite-time bounds on the mean-square error, defined as the deviation of the output
of the algorithm from the unique equilibrium point of the associated ordinary dif-
ferential equation. For the case of interconnection matrices being stochastic, the
equilibrium point can be any unspecified convex combination of the local equi-
libria of all the agents in the absence of communication. Both the cases with
constant and time-varying step-sizes are considered. In the case when the convex
combination is required to be a straight average and interaction between any pair
of neighboring agents may be uni-directional, so that doubly stochastic matrices
cannot be implemented in a distributed manner, the paper proposes a push-type
distributed stochastic approximation algorithm and provides its finite-time bounds
for the performance by leveraging the analysis for the consensus-type algorithm
with stochastic matrices.
1	Introduction
The use of reinforcement learning (RL) to obtain policies that describe solutions to a Markov deci-
sion process (MDP) in which an autonomous agent interacting with an unknown environment aims
to optimize its long term reward is now standard Sutton & Barto (1998). Multi-agent or distributed
reinforcement learning is useful when a team of agents interacts with an unknown environment or
system and aims to collaboratively accomplish tasks involving distributed decision-making. Dis-
tributed here implies that agents exchange information only with their neighbors according to a
certain communication graph. Recently, many distributed algorithms for multi-agent RL have been
proposed and analyzed (Zhang et al., 2019). The basic result in such works is of the type that
if the graph describing the communication among the agents is bi-directional (and hence can be
represented by a doubly stochastic matrix), then an algorithm that builds on traditional consensus
algorithms converges to a solution in terms of policies to be followed by the agents that optimize
the sum of the utility functions of all the agents; further, both finite and infinite time performance of
such algorithms can be characterized (Doan et al., 2019; Zhang et al., 2018b).
This paper aims to relax the assumption of requiring bi-directional communication among agents
in a distributed RL algorithm. This assumption is arguably restrictive and will be violated due to
reasons such as packet drops or delays, differing privacy constraints among the agents, heteroge-
neous capabilities among the agents in which some agents may be able to communicate more often
or with more power than others, adversarial attacks, or even sophisticated resilient consensus al-
gorithms being used to construct the distributed RL algorithm. A uni-directional communication
graph can be represented through a (possibly time-varying) stochastic - which may not be doubly
stochastic - matrix being used in the algorithm. As we discuss in more detail below, relaxing the
assumption of a doubly stochastic matrix to simply a stochastic matrix in the multi-agent and dis-
tributed RL algorithms that have been proposed in the literature, however, complicates the proofs
of their convergence and finite time performance characterizations. The main result in this paper is
to provide a finite time bound on the mean-square error for a multi-agent linear stochastic approx-
imation algorithm in which the agents interact over a time-varying directed graph characterized by
a stochastic matrix. This paper, thus, extends the applicability of distributed and multi-agent RL
1
Under review as a conference paper at ICLR 2022
algorithms presented in the literature to situations such as those mentioned above where bidirec-
tional communication at every time step cannot be guaranteed. As we shall see, this extension is
technically challenging and requires new proof techniques that may be of independent interest.
Related Work (See Appendix B for more related work) Many distributed reinforcement learning
algorithms have now been proposed in the literature. In this setting, each agent can receive informa-
tion only from its neighbors, and no single agent can solve the problem alone orby ‘taking the lead’.
A backbone of almost all distributed RL algorithms proposed in the literature is the consensus-type
interaction among the agents, dating back at least to Tsitsiklis (1984). Many works have analyzed
asymptotic convergence of such RL algorithms using ODE methods (Zhang & Zavlanos, 2019;
Zhang et al., 2018b; Suttle et al., 2020; Zhang et al., 2018a). This can be viewed as an application
of ideas from distributed stochastic approximation (Kushner & Yin, 1987; Stankovic et al., 2010;
Huang, 2012; Stankovic & Stankovic, 2016; Bianchi et al., 2013; Stankovic et al., 2016). Finite-time
performance guarantees for distributed RL have also been provided in works, most notably in Doan
et al. (2019; 2021); Wang et al. (2020); Zhang et al. (2021); Sun et al. (2020); Zeng et al. (2020).
The assumption that is the central concern of this paper and is made in all the existing finite-time
analyses for distributed RL algorithms is that the consensus interaction is characterized by doubly
stochastic matrices (Doan et al., 2019; 2021; Wang et al., 2020; Zhang et al., 2021; Sun et al., 2020;
Zeng et al., 2020) at every time step, or at least in expectation, i.e., W 1 = 1 and 1>E(W) = 1>
(Bianchi et al., 2013). Intuitively, doubly stochastic matrices imply symmetry in the communica-
tion graph, which almost always requires bidirectional communication graphs. More formally, the
assumption of doubly stochastic matrices is restrictive since distributed construction of a doubly
stochastic matrix needs to either invoke algorithms such as the Metropolis algorithm (Xiao et al.,
2005) which requires bi-directional communication of each agent’s degree information; or to utilize
an additional distributed algorithm (GhareSifard & Cortes, 2012) which significantly increases the
complexity of the whole algorithm design. Doubly stochastic matrices in expectation can be guar-
anteed via so-called broadcast gossip algorithms which still requires bi-directional communication
for convergence (Bianchi et al., 2013). In a realistic network, especially with mobile agents such
as autonomous vehicles, drones, or robots, uni-directional communication is inevitable due to var-
ious reasons such as asymmetric communication and privacy constraints, non-zero communication
failure probability between any two agents at any given time, and application of resilient consen-
sus in the presence of adversary attacks (Vaidya et al., 2012; LeBlanc et al., 2013), all leading to
an interaction among the agents characterized by a stochastic matrix, which may further be time-
varying. The problem of design of distributed RL algorithms with time-varying stochastic matrices
and characterizing either their asymptotic convergence or finite time analysis remains open.
As a step towards solving this problem, we propose a novel distributed stochastic approximation
algorithm and provide its convergence analyses when a time-dependent stochastic matrix is being
used due to uni-directional communication in a dynamic network. One of the first guarantees to be
lost as the assumption of doubly stochastic matrices is removed is that the algorithm converges to
a “policy” that maximizes the sum of reward functions of all the agents. Instead, the convergence
is to a set of policies that optimize a convex combination of the network-wise accumulative reward,
with the exact combination depending on the limit product of the infinite sequence of stochastic
matrices. Nonetheless, by defining the error as the deviation of the output of the algorithm from
the eventual equilibrium point, we derive finite-time bounds on the mean-square error. We consider
both the cases with constant and time-varying step sizes. In the important special case where the
goal is to optimize the average of the individual accumulative rewards of all the agents, we provide
a distributed stochastic approximation algorithm, which builds on the push-sum idea (Kempe et al.,
2003) that has been used to solve distributed averaging problem over strongly connected graphs, and
characterize its finite-time performance. Thus, this paper provides the first distributed algorithm that
can be applied (e.g., in Temporal difference (TD) learning, see Appendix D) to converge to the policy
maximizing the team objective of the sum of the individual utility functions over time-varying, uni-
directional, communication graphs, and characterizes the finite-time bounds on the mean-square
error of the algorithm output from the equilibrium point under appropriate assumptions.
Technical Innovation and Contributions There are two main technical challenges in removing
the assumption of doubly stochastic matrices being used in the analysis of distributed stochastic
approximation algorithms. The first is in the direction of finite-time analysis. For distributed RL al-
gorithms, finite-time performance analysis essentially boils down to two parts, namely bounding the
consensus error and bounding the “single-agent” mean-square error. For the case when consensus
2
Under review as a conference paper at ICLR 2022
interaction matrices are all doubly stochastic, the consensus error bound can be derived by analyzing
the square of the 2-norm of the deviation of the current state of each agent from the average of the
states of the agents. With consensus in the presence of doubly stochastic matrices, the average of the
states of the agents remains invariant. Thus, it is possible to treat the average value as the state of a
fictitious agent to derive the mean-square consensus error bound with respect to the limiting point.
More formally, this process relies on two properties of a doubly stochastic matrix W, namely that
(1) 1>W = 1>, and (2) if xt+1 = Wxt,then kxt+1 - (1>xt+1)1k2 ≤ σ2(W)kxt - (1>xt)1k2
where σ2(W) denotes the second largest singular value of W (which is strictly less than one if W is
irreducible). Even if the doubly stochastic matrix is time-varying (denoted by Wt), property (1) still
holds andproperty (2) can be generalized as in Nedic et al. (2θl8). Thus, the square of the 2-norm
kxt - (1>xt)1k22 is a quadratic Lyapunov function for the average consensus processes. Doubly
stochastic matrices in expectation can be treated in the same way by looking at the expectation. This
is the core on which all the existing finite-time analyses of distributed RL algorithms are based.
However, if each consensus interaction matrix is stochastic, and not necessarily doubly stochastic,
the above two properties may not hold. In fact, it is well known that quadratic Lyapunov functions
for general consensus processes xt+1 = Stxt , with St being stochastic, do not exist (Olshevsky
& Tsitsiklis, 2008). This breaks down all the existing analyses and provides the first technical
challenge that we tackle in this paper. Specifically, we appeal to the idea of quadratic comparison
functions for general consensus processes. This was first proposed in Touri (2012) and makes use of
the concept of “absolute probability sequences”. We provide a general analysis methodology and re-
sults that subsume the existing finite-time analyses for single-timescale distributed linear stochastic
approximation and TD learning as special cases (see Appendix D).
The second technical challenge arises from the fact that with stochastic matrices, the distributed RL
algorithms may not converge to the policies that maximize the average of the utility functions of
the agents. To regain this property, we propose a new algorithm that utilizes a push-sum protocol
for consensus. However, finite-time analysis for such a push-based distributed algorithm is chal-
lenging. Almost all, if not all, the existing push-based distributed optimization works build on the
analysis in Nedic & Olshevsky (2014); however, that analysis assumes that a convex combination
of the entire history of the states of each agent (and not merely the current state of the agent) is
being calculated. This assumption no longer holds in our case. To obtain a direct finite-time error
bound without this assumption, we propose a new approach to analyze our push-based distributed
algorithm by leveraging our consensus-based analyses to establish direct finite-time error bounds for
stochastic approximation. Specifically, we tailor an “absolute probability sequence” for the push-
based stochastic approximation algorithm and exploit its properties. Such properties have never
been found in the existing literature and may be of independent interest for analyzing any push-sum
based distributed algorithm.
We now list the main contributions of our work. We propose a novel consensus-based distributed
linear stochastic approximation algorithm driven by Markovian noise in which each agent evolves
according to its local stochastic approximation process and the information from its neighbors. We
assume only a (possibly time-varying) stochastic matrix being used during the consensus phase,
which is a more practical assumption when only unidirectional communication is possible among
agents. We establish both convergence guarantees and finite-time bounds on the mean-square error,
defined as the deviation of the output of the algorithm from the unique equilibrium point of the
associated ordinary differential equation. The equilibrium point can be an “uncontrollable” convex
combination of the local equilibria of all the agents in the absence of communication. We consider
both the cases of constant and time-varying step-sizes. Our results subsume the existing results on
convergence and finite-time analysis of distributed RL algorithms that assume doubly stochastic ma-
trices and bi-directional communication as special cases. In the case when the convex combination
is required to be a straight average and interaction between any pair of neighboring agents may be
uni-directional, we propose a push-type distributed stochastic approximation algorithm and estab-
lish its finite-time performance bound. It is worth emphasizing that it is straightforward to extend
our algorithm from the straight average point to any pre-specified convex combination. Since it is
well known that TD algorithms can be viewed as a special case of linear stochastic approximation
(Tsitsiklis & Roy, 1997), our distributed linear stochastic approximation algorithms and their finite-
time bounds can be applied to TD algorithms in a straight-forward manner; (see distributed TD(λ)
in Appendix D).
3
Under review as a conference paper at ICLR 2022
Notation We use Xt to represent that a variable X is time-dependent and t ∈ {0, 1, 2, . . .} is
the discrete time index. The ith entry of a vector x will be denoted by xi and, also, by (x)i when
convenient. The ijth entry of a matrix A will be denoted by aij and, also, by (A)ij when convenient.
We use 1n to denote the vectors in Rn whose entries all equal to 1’s, and I to denote the identity
matrix, whose dimension is to be understood from the context. Given a set S with finitely many
elements, We use |S| to denote the cardinality of S. We used•] to denote the ceiling function.
A vector is called a stochastic vector if its entries are nonnegative and sum to one. A square non-
negative matrix is called a roW stochastic matrix, or simply stochastic matrix, if its roW sums all
equal one. Similarly, a square nonnegative matrix is called a column stochastic matrix if its column
sums all equal one. A square nonnegative matrix is called a doubly stochastic matrix if its roW sums
and column sums all equal one. The graph of an n × n matrix is a direct graph With n vertices
and a directed edge from vertex i to vertex j Whenever the ji-th entry of the matrix is nonzero. A
directed graph is strongly connected if it has a directed path from any vertex to any other vertex.
For a strongly connected graph G, the distance from vertex i to another vertex j is the length of the
shortest directed path from i to j ; the longest distance among all ordered pairs of distinct vertices i
and j in G is called the diameter of G. The union of tWo directed graphs, Gp and Gq, With the same
vertex set, Written Gp ∪ Gq , is meant the directed graph With the same vertex set and edge set being
the union of the edge set of Gp and Gq . Since this union is a commutative and associative binary
operation, the definition extends unambiguously to any finite sequence of directed graphs.
2	Distributed Linear Stochastic Approximation
Consider a netWork consisting of N agents. For the purpose of presentation, We label the agents
from 1 through N. The agents are not aWare of such a global labeling, but can differentiate betWeen
their neighbors. The neighbor relations among the N agents are characterized by a time-dependent
directed graph Gt = (V, Et) Whose vertices correspond to agents and Whose directed edges (or arcs)
depict neighbor relations, Where V = {1, . . . , N} is the vertex set and Et = V × V is the edge set
at time t. Specifically, agent j is an in-neighbor of agent i at time t if (j, i) ∈ Et, and similarly,
agent k is an out-neighbor of agent i at time t if (i, k) ∈ Et. Each agent can send information to its
out-neighbors and receive information from its in-neighbors. Thus, the directions of edges represent
the directions of information floW. For convenience, We assume that each agent is alWays an in- and
out-neighbor of itself, Which implies that Gt has self-arcs at all vertices for all time t. We use Nti
and Nti- to denote the in- and out-neighbor set of agent i at time t, respectively, i.e.,
Nti ={j ∈V : (j, i) ∈ Et}, Nti- ={k∈V : (i, k) ∈ Et}.
It is clear that Nti and Nti- are nonempty as they both contain index i.
We propose the folloWing distributed linear stochastic approximation over a time-varying neighbor
graph sequence {Gt}. Each agent i has control over a random vector θti Which is updated by
θti+1 = X wtijθtj+αt A(Xt) X wtijθtj+bi(Xt) , i∈V, t∈ {0,1,2,...},	(1)
j∈Nti	j∈Nti
Where wtij are consensus Weights, αt is the step-size at time t, A(Xt) is a random matrix and bi(Xt)
is a random vector, both generated based on the Markov chain {Xt} With state spaces X. It is Worth
noting that the update of each agent only uses its in-neighbors’ information and thus is distributed.
Remark 1 The work of Kushner & Yin (1987) considers a different consensus-based networked
linear stochastic approximation as follows:
θi+ι= X wtjθj + αt (A(Xt)θi + 巩 Xt) , i ∈ V, t ∈{0,1, 2,...},	⑵
j∈Nti
whose state form is Θt+1 = WtΘt+αtΘtA(Xt)>+αtB(Xt), and mainly focuses on asymptotically
weakly convergence for the fixed step-size case (i.e., αt = α for all t). Under the similar set of
conditions, with its condition (C3.4’) being a stochastic analogy for Assumption 6, Theorem 3.1 in
Kushner & Yin (1987) shows that equation 2 has a limit which can be verified to be the same as θ*,
the limit of equation 1. How to apply the finite-time analysis tools in this paper to equation 2 has
so far eluded us. The two updates equation 1 and equation 2 are analogous to the “combine-then-
adapt” and “adapt-then-combine” diffusion strategies in distributed optimization (Chen & Sayed,
2012).
4
Under review as a conference paper at ICLR 2022
We impose the following assumption on the weights wtij which has been widely adopted in consen-
SUs literature (Jadbabaie et al., 2003; Olfati-Saber et al., 2007; Nedic & Liu, 2017).
Assumption 1 There exists a constant β > 0 such that for all i, j ∈ V and t, wtij ≥ β whenever
j ∈ Nti. For all i ∈ V and t,	j∈Ni wtij = 1.
Let Wt be the N × N matrix whose ijth entry equals wtij if j ∈ Nti and zero otherwise. From
Assumption 1, each Wt is a stochastic matrix that is compliant with the neighbor graph Gt. Since
each agent i is always assumed to be an in-neighbor of itself, all diagonal entries of Wt are positive.
Thus, if Gt is strongly connected, Wt is irreducible and aperiodic. To proceed, define
Θt
(θt1)>
.
.
.
(θtN)>
(b1(Xt))>
B(Xt)=	.
(bN(Xt))>
Then, the N linear stochastic recursions in equation 1 can be combined and written as
Θt+1 = WtΘt + αtWtΘtA(Xt)> + αtB(Xt), t ∈ {0, 1, 2, . . .}.	(3)
The goal of this section is to characterize the finite-time performance of equation 1, or equiva-
lently equation 3, with the following standard assumptions, which were adopted e.g. in Srikant &
Ying (2019); Doan et al. (2019).
Assumption 2 There exists a matrix A and vectors bi, i ∈ V, such that
lim E[A(Xt)] = A, lim E[bi(Xt)] = bi, i ∈ V.
t→∞	t→∞
Define bmax = maxi∈V supx∈X kbi(x)k2 < ∞ and Amax = supx∈X kA(x)k2 < ∞. Then,
kAk2 ≤ Amax and kbi k2 ≤ bmax, i ∈ V.
Assumption 3 Given a positive constant α, we use τ(α) to denote the mixing time of the Markov
chain {Xt } for which
(	∣∣E[A(Xt)-	A∣Xo	= X]∣∣2 ≤ α,	∀X,	∀t	≥	T(α),
∖	∣∣E[bi(Xt)-	bi∣Xo	= X]∣∣2 ≤ α,	∀X,	∀t	≥	τ(α),	∀i	∈V.
The Markov chain {Xt} mixes at a geometric rate, i.e., there exists a constant C such that τ(α) ≤
-C log α.
Assumption 4 All eigenvalues of A have strictly negative real parts, i.e., A is a Hurwitz matrix.
Then, there exists a symmetric positive definite matrix P, such that A>P + PA = -I. Let γmax
and γmin be the maximum and minimum eigenvalues of P, respectively.
Assumption 5 The step-size sequence {αt} is positive, non-increasing, and satisfies	t∞=0 αt = ∞
and	t∞=0 αt2 < ∞.
To state our first main result, we need the following concepts.
Definition 1 A graph sequence {Gt} is uniformly strongly connected if there exists a positive integer
L such that for any t ≥ 0, the union graph ∪tk+=Lt -1 Gk is strongly connected. If such an integer exists,
we sometimes say that {Gt} is uniformly strongly connected by sub-sequences of length L.
Remark 2 Two popular joint connectivity definitions in consensus literature are “B-connected”
(Nedic et al., 20θ9) and “repeatedly jointly strongly connected” (Cao et al., 2008). A graph
sequence {Gt } is B-connected if there exists a positive integer B such that the union graph
∪t(=k+k1B)B-1Gt is strongly connected for each integer k ≥ 0. Although the uniformly strongly con-
nectedness looks more restrictive compared with B-connectedness at first glance, they are in fact
equivalent. To see this, first it is easy to see that if {Gt} is uniformly strongly connected, {Gt} must
5
Under review as a conference paper at ICLR 2022
be B-connected; now supposing {Gt} is B-connected, for any fix t, the union graph ∪tk+=2tB-1Gk
must be strongly connected, and thus {Gt } is uniformly strongly connected by sub-sequences of
length 2B. Thus, the two definitions are equivalent. It is also not hard to show that the uniformly
strongly connectedness is equivalent to “repeatedly jointly strongly connectedness” provided the
graphs under consideration all have self-arcs at all vertices, as “repeatedly jointly strongly con-
nectedness” is defined upon “graph composition”.
Definition 2 Let {Wt} be a sequence of stochastic matrices. A sequence of stochastic vectors {πt}
is an absolute probability sequence for {Wt} if πt> = πt>+1Wt for all t ≥ 0.
This definition was first introduced by Kolmogorov (Kolmogoroff, 1936). It was shown by Black-
well (Blackwell, 1945) that every sequence of stochastic matrices has an absolute probability se-
quence. In general, a sequence of stochastic matrices may have more than one absolute probability
sequence; when the sequence of stochastic matrices is “ergodic”, it has a unique absolute probability
sequence (Nedic & Liu, 2017). It is easy to see that when Wt is a fixed irreducible stochastic matrix
W, πt is simply the normalized left eigenvector of W for eigenvalue one. More can be said.
Lemma 1 Suppose that Assumption 1 holds. If {Gt} is uniformly strongly connected, then there
exists a unique absolute probability sequence {πt} for the matrix sequence {Wt } and a constant
πmin ∈ (0, 1) such that πti ≥ πmin for all i and t.
Let hθit = PiN=1 πtiθti, which is a column vector and convex combination of all θti. It is easy to
see that hθit = (πt>Θt)> = Θt>πt. From Definition 2 and equation 3, we have πt>+1Θt+1 =
πt>+1WtΘt +αtπt>+1WtΘtA(Xt)> +αtπt>+1B(Xt) = πt>Θt +αtπt>ΘtA(Xt)> +αtπt>+1B(Xt),
which implies that
hθit+1 = hθit + αtA(Xt)hθit + αtB(Xt)>πt+1.	(4)
Asymptotic performance of equation 1 with any uniformly strongly connected neighbor graph se-
quence is characterized by the following two theorems.
Theorem 1	Suppose that Assumptions 1, 2 and5 hold. Let {θti}, i ∈ V, be generated by equation 1.
If {Gt} is uniformly strongly connected, then limt→∞ kθti - hθitk2 = 0 for all i ∈ V.
Theorem 1 only shows that all the sequences {θti}, i ∈ V, generated by equation 1 will finally
reach a consensus, but not necessarily convergent or bounded. To guarantee the convergence of the
sequences, we further need the following assumption, whose validity is discussed in Remark 3.
Assumption 6 The absolute probability sequence {πt } for the stochastic matrix sequence {Wt}
has a limit, i.e., there exists a stochastic vector π∞ such that limt→∞ πt = π∞.
Theorem 2	Suppose that Assumptions 1-6 hold. Let {θi}, i ∈ V ,be generated by equation 1 and
θ* be the unique equilibrium point of the ODE
N
θ = Aθ + b, b = X π∞ bi,	(5)
i=1
where A and bi are defined in Assumption 2 and π∞ is defined in Assumption 6. If{Gt} is uniformly
strongly connected, then all θt will converge to θ* both with probability 1 and in mean square.
Remark 3 Though Assumption 6 may look restrictive at first glance, simple simulations show that
the sequences {θti}, i ∈ V, do not converge if the assumption does not hold. It is worth empha-
sizing that the existence of π∞ does not imply the existence of limt→∞ Wt, though the converse
is true. Indeed, the assumption subsumes various cases including (a) all Wt are doubly stochastic
matrices, and (b) all Wt share the same left eigenvector for eigenvalue 1, which may arise from
the scenario when the number of in-neighbors of each agent does not change over time (Olshevsky
& Tsitsiklis, 2013). An important implication of Assumption 6 is when the consensus interaction
among the agents, characterized by {Wt}, is replaced by resilient consensus algorithms such as
Vaidya et al. (2012); LeBlanc et al. (2013) in order to attenuate the effect of unknown malicious
6
Under review as a conference paper at ICLR 2022
agents, the resulting dynamics of non-malicious agents, in general, will not converge, because the
resulting interaction stochastic matrices among the non-malicious agents depend on the state val-
ues transmitted by the malicious agents, which can be arbitrary, and thus the resulting stochastic
matrix sequence, in general, does not have a convergent absolute probability sequence; of course,
in this case, the trajectories of all the non-malicious agents will still reach a consensus as long as
the step-size is diminishing, as implied by Theorem 1. Further discussion on Assumption 6 can be
found in Appendix C.
We now study the finite-time performance of the proposed distributed linear stochastic approxi-
mation equation 1 for both fixed and time-varying step-size cases. Its finite-time performance is
characterized by the following theorem.
Let ηt = kπt - π∞k2 for all t ≥ 0. From Assumption 6, ηt converges to zero as t → ∞.
Theorem 3 Let the sequences {θti}, i ∈ V, be generated by equation 1. Suppose that Assump-
tions 1-4, 6 hold and {Gt} is uniformly strongly connected by Sub-Sequences of length L. Let qt
and mt be the unique integer quotient and remainder of t divided by L, respectively. Let δt be the
diameter of ∪tk+=Lt -1Gk, δmax = maxt≥0 δt, and
2bmax	πminβ	2L	2bmax	L
e = 1 + A-------^δ--- (1 + αAmaχ)	- A——(1 + aAmaχ) ,	(6)
Amax	2δmax	Amax
where 0 < α < min{Kι, , bog2、, τr0.1—}.
1,	Amaxτ (α) , K2γmax
1)	Fixed step-size: Let αt = αfor all t ≥ 0. For all t ≥ T1,
N	N	t-T1
X∏iEhIlθi -θ*∣U ≤ 2eqtX∏m,EhWm,-〈。0,|： + CI(1 - -)	+ C2
i=1	i=1	max
+ 32αZ4Xηt+i-kfι - 09α)；	⑺
γmin	γmax
2)	Time-varying step-size: Let at =器 With ɑ0 ≥ Yim9x. For all t ≥ LT2,
NN
X ∏iEmi-叫 2] ≤ 2eqt-T2 X ∏Lτ2+mt E "θLτ2+mt-hθiLT2+mt)
i=1	i=1
+ C3
q qt -1
(a0e 2 + a
d
t
X ηk +
k=LT2
(8)
Here T1 , T2, K1 , K2, C1 - C6 are finite constants whose definitions are given in Appendix A.1.
Since πti is uniformly bounded below by πmin ∈ (0, 1) from Lemma 1, it is easy to see that the
above bound holds for each individual E[k θi - θ*k∣]. To better understand the theorem, We provide
the following remark.
Remark 4 In Appendix E.2.1, we show that both e and (1 - 09α) lie in the interval (0,1). It is
γmax
easy to show that e is monotonically increasing for δmax and L, monotonically decreasing for β and
πmin. Also, limt→∞ Pk=TI ηt+1-k (1 - 09∣ )k ≤ limt→∞ Ymax [n「t-Tι ] + 小(I - 09∣) 2 ]=
0. Therefore, the summands in the finite-time bound equation 7 for the fixed step-size case are
exponentially decaying except for the constant C∣, which implies that lim supt→∞ PiN=1 πtiE[kθti -
θ*k∣] ≤ C∣, providing a constant limiting bound. From Appendix A, C∣ is monotonically increasing
forγmax, δmax, bmax and L, and monotonically decreasing for γmin, πmin and β. In Appendix E.2.2,
we show that limt→∞ 1 Pk=ι ηk = 0, which implies that the finite-time bound equation 8 for the
time-varying step-size case converges to zero as t → ∞. We next comment on 01 in the inequality
defining a. Actually, we can replace 01 with any constant c ∈ (0, 1), which will affect the value of
e and the feasible set ofa, with the latter becoming 0 < a < min{K1,
log 2	C
AmaXT(α)，K2γmax
} Thus,
7
Under review as a conference paper at ICLR 2022
the smaller the value of c is, the smaller is the feasible set of α, though the feasible set is always
nonempty. For convenience, we simply pick c = 0.1 in this paper; that is why we also have 0.9 in
equation 7. Lastly, we comment on a° in the time-varying step-size case. We set a0 ≥ γ0.9x for the
purpose of getting a cleaner expression ofthe finite-time bound. For ɑ0 < Ymax, our approach still
works, but will yield a more complicated expression. The same is true for Theorem 5.
Technical Challenge and Proof Sketch As described in the introduction, the key challenge of
analyzing the finite-time performance of the distributed stochastic approximation equation 1 lies
in the condition that the consensus-based interaction matrix is time-varying and stochastic (not
necessarily doubly stochastic). To tackle this, we appeal to the absolute probability sequence
πt of the time-varying interaction matrix sequence and introduce the quadratic Lyapunov com-
parison function PN=I ∏iE[∣∣θi - θ*k2]. Then, using the inequality PN=I ∏iE[∣∣θi - θ*k2] ≤
2PN= ι∏iE[k°i - h°it∣∣2] + 2E[khθ>t - θ*∣∣2], the next step is to find the finite-time bounds of
PN=I ∏tiE[kθi - hθitk2] and E[∣∣hθ>t - θ* k2], respectively. The latter term is essentially the “single-
agent” mean-square error. Our main analysis contribution here is to bound the former term for both
fixed and time-varying step-size cases.
3	PUSH-SA
The preceding section shows that the limiting state of consensus-based distributed stochastic approx-
imation depends on π∞ , which leads to a convex combination of the local equilibria of all the agents
in the absence of communication, but the convex combination is in general “uncontrollable”. Note
that this convex combination will correspond to a convex combination of the network-wise accumu-
lative rewards in applications such as distributed TD learning. In an important case when the convex
combination is desired to be the straight average, the existing literature e.g. Doan et al. (2019; 2021)
relies on doubly stochastic matrices whose corresponding π∞ = (1/N)1N . As mentioned in the
introduction, doubly stochastic matrices implicitly require bi-directional communication between
any pair of neighboring agents; see e.g. gossiping (Boyd et al., 2006) and the Metropolis algo-
rithm (Xiao et al., 2005). A popular method to achieve the straight average target while allowing
uni-directional communication between neighboring agents is to appeal to the idea so-called “push-
sum” (Kempe et al., 2003), which was tailored for solving the distributed averaging problem over
directed graphs and has been applied to distributed optimization (Nedic & Olshevsky, 2014). In this
section, we will propose a push-based distributed stochastic approximation algorithm tailored for
uni-directional communication and establish its finite-time error bound.
Each agent i has control over three variables, namely yii, θ and θ∖i, in which yi is scalar-valued
with initial value 1,京i can be arbitrarily initialized, and θ0 = θ0. At each time t ≥ 0, each agent i
ji	ji
sends its weighted current values Wj yi and Wj (θi + αtA(Xt)θt + αtbi(Xt)) to each of its current
out-neighbors j ∈ Nti-, and updates its variables as follows:
yi+ι = X Wij Vt,	y0 =1,
j∈Nt
< M= X Wijh的 + αi (A(Xt)θj + bj(Xt))],	⑼
j∈Nt
~ .
θi∣-l	-	~∙
Qi 一 t+1	Qi 一 Qi
θt+1 = Mi ,	θ0 = θ0,
yt+1
where Wij = 1∕∣Nj-∣. It is worth noting that the algorithm is distributed yet requires that each
agent be aware of the number of its out-neighbors.
Asymptotic performance of equation 9 with any uniformly strongly connected neighbor graph se-
quence is characterized by the following theorem.
Theorem 4	Suppose that Assumptions 2-5 hold. Let {θi}, i ∈ V, be generated by equation 9 and
θ* be the unique equilibrium point ofthe ODE
1N
θ = Aθ + N Eb,	(10)
i=1
8
Under review as a conference paper at ICLR 2022
where A and bi are defined in Assumption 2. If {Gt } is uniformly strongly connected, then θti will
converge to θ* in mean SquarefOr all i ∈ V.
In this section, We define ⑥ t = N PL θt and hθit = N PiN=1 θti . To help understand these
definitions, let Wt be the N X N matrix whose ij-th entry equals Wijif j ∈ Ni, otherwise equals
zero. It is easy to see that each Wt is a column stochastic matrix whose diagonal entries are all
positive. Then, ∏t = N 1n for all t ≥ 0 can be regarded as an absolute probability sequence of
{Wt}. Thus, the above two definitions are intuitively consistent with hθit in the previous section.
Finite-time performance of equation 9 with any uniformly strongly connected neighbor graph se-
quence is characterized by the following theorem.
ɪ	Il Λ / -VZ- ∖ / / n∖	/ X∖ ∖ II ɪ &	τ -T-I	i	, i , II / n∖	/ X∖ II	,
Let μt = ∣∣A(Xt)(hθit 一〈。4)、.In Appendix E.3, we show that ∣Kθ. 一 hθ>t∣∣2 converges to zero
as t → ∞, so does μt.
Theorem 5	Suppose that Assumptions 2-4 hold and {Gt} is uniformly strongly connected by sub-
sequences oflength L. Let {θ1}, i ∈ V, be generated by equation 9 with a = t0肯 and ao ≥ Ym9x.
1	1
Then, there exists a nonnegative e ≤ (1 - NNL) L such thatfor all t ≥ T,
N
^X E h∣∣θi+1 - θ* ∣∣2i ≤ C7 €t + C8 ^α0<≡2 + αdt e) + C9αt
i=1
+ - (C10 log2 (£) + C11 ^X μk + ci2) ,	(II)
where T€ and C7 - C12 are finite constants whose definitions are given in Appendix A.2.
In Appendix E.3, we show that limt→∞ 1 Pk=I μk = 0, which implies that the finite-time bound
equation 11 converges to zero as t → ∞. It is worth mentioning that the theorem does not consider
the fixed step-size case, as our current analysis approach cannot be directly applied for this case.
Proof Sketch and Technical Challenge Using the inequality PN=I E[∣θi+1 - θ*∣∣2] ≤
2PN=1 E[∣θi+1 -hθit∣2] + 2NE[∣hθit- θ*∣∣2],our goal is to derive the finite-time bounds of
PN=I E[∣θi+ι - h^itk2] and E[∣hθit - θ*∣2], respectively. Although this looks similar to the proof
of Theorem 3, the derivation is quite different. First, the iteration of hθit is a single-agent SA plus
a disturbance term hθit - hθit, so we cannot directly apply the existing single-agent SA finite-
time analyses to bound E[khθ>t - θ*∣2]; instead, we have to show that <θ>t - <θ>t will diminish
and quantify the diminishing “speed”. Second, both the proof of showing diminishing hθit - hθit
and derivation of bounding PN=I E[∣θi+1 - h%∣∣2] involve a key challenge: to prove the sequence
{θti } generated from the Push-SA equation 9 is bounded almost surely. To tackle this, we introduce a
novel way to constructing an absolute probability sequence for the Push-SA as follows. From equa-
tion 9, θi+ι = PjN=IWij[θj + αtA(Xt)θj + αtjXt)], where Wij = (Wijyj)/(PN=IWikyk).
yt	yt
ij
We show that each matrix Wt = [w∕] is stochastic, and there exists a unique absolute probability
sequence {∏t} for the matrix sequence {Wt} such that ∏i ≥ ∏min for all i ∈ V and t ≥ 0, with
the constant ∏min ∈ (0,1). Most importantly, we show two critical properties of {Wt} and {∏t},
namely limt→∞ (∏S=0WS) = N 1n 1( and * = N for all i, j ∈ V and t ≥ 0, which have never
been reported in the literature though push-sum based algorithms have been extensively studied.
4 Concluding Remarks
In this paper, we have established both asymptotic and non-asymptotic analyses for a consensus-
based distributed linear stochastic approximation algorithm over uniformly strongly connected
graphs, and proposed a push-based variant for coping with uni-directional communication. Both
algorithms and their analyses can be directly applied to TD learning. One limitation of our finite-
time bounds is that they involve quite a few constants which are well defined and characterized
but whose values are not easy to compute. Future directions include leveraging the analyses for
resilience in the presence of malicious agents and extending the tools to more complicated RL.
9
Under review as a conference paper at ICLR 2022
References
P. Bianchi, G. Fort, and W. Hachem. Performance of a distributed stochastic approximation algo-
rithm. IEEE Transactions on Information Theory, 59(11):7405-7418, 2013.
D. Blackwell. Finite non-homogeneous chains. Annals OfMathematics, 46(4):594-599, 1945.
S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Randomized gossip algorithms. IEEE Transactions
on Information Theory, 52(6):2508-2530, 2006.
M. Cao, A. S. Morse, and B. D. O. Anderson. Reaching a consensus in a dynamically changing
environment: a graphical approach. SIAM Journal on Control and Optimization, 47(2):575-600,
2008.
J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and learning
over networks. IEEE Transactions on Signal Processing, 60(8):4289-4305, 2012.
T.T. Doan, S.T. Maguluri, and J. Romberg. Finite-time analysis of distributed TD(0) with linear
function approximation on multi-agent reinforcement learning. In 36th International Conference
on Machine Learning, pp. 1626-1635, 2019.
T.T. Doan, S. T. Maguluri, and J. Romberg. Finite-time performance of distributed temporal-
difference learning with linear function approximation. SIAM Journal on Mathematics of Data
Science, 3(1):298-320, 2021.
B. Gharesifard and J. Cortes. Distributed strategies for generating weight-balanced and doubly
stochastic digraphs. European Journal of Control, 18(6):539-557, 2012.
M. Huang. Stochastic approximation for consensus: anew approach via ergodic backward products.
IEEE Transactions on Automatic Control, 57(12):2994-3008, 2012.
A. Jadbabaie, J. Lin, and A. S. Morse. Coordination of groups of mobile autonomous agents using
nearest neighbor rules. IEEE Transactions on Automatic Control, 48(6):988-1001, 2003.
D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate information. In 44th
IEEE Symposium on Foundations of Computer Science, pp. 482-491, 2003.
A. Kolmogoroff. Zur theorie der markoffschen ketten. Mathematische Annalen, 112(1):155-160,
1936.
H.J. Kushner and G. Yin. Asymptotic properties of distributed and communicating stochastic ap-
proximation algorithms. SIAM Journal on Control and Optimization, 25(5):1266-1290, 1987.
H. J. LeBlanc, H. Zhang, X. Koutsoukos, and S. Sundaram. Resilient asymptotic consensus in robust
networks. IEEE Journal on Selected Areas in Communications, 31(4):766-781, 2013.
A. Nedic and J. Liu. On convergence rate of weighted-averaging dynamics for consensus problems.
IEEE Transactions on Automatic Control, 62(2):766-781, 2017.
A. Nedic and A. Olshevsky. Distributed optimization over time-varying directed graphs. IEEE
Transactions on Automatic Control, 60(3):601-615, 2014.
A. Nedic, A. Olshevsky, A. Ozdaglar, and J. N. Tsitsiklis. On distributed averaging algorithms and
quantization effects. IEEE Transactions on automatic control, 54(11):2506-2517, 2009.
A. Nedic, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation
tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953-976, 2018.
R. Olfati-Saber, J. A. Fax, and R. M. Murray. Consensus and cooperation in networked multi-agent
systems. Proc. IEEE, 95(1):215-233, 2007.
A. Olshevsky and J. N. Tsitsiklis. On the nonexistence of quadratic lyapunov functions for consensus
algorithms. IEEE Transactions on Automatic Control, 53(11):2642-2645, 2008.
A. Olshevsky and J. N. Tsitsiklis. Degree fluctuations and the convergence time of consensus algo-
rithms. IEEE Transactions on Automatic Control, 58(10):2626-2631, 2013.
10
Under review as a conference paper at ICLR 2022
R.	Srikant and L. Ying. Finite-time error bounds for linear stochastic approximation and TD learn-
ing.In 32nd Conference on Learning Theory, volume 99, pp. 2803-2830. Proceedings ofMachine
Learning Research, 25-28 JUn 2019.
M. S. Stankovic, N. Ilic, and S. S. Stankovic. Distributed stochastic approximation: weak conver-
gence and network design. IEEE Transactions on Automatic Control, 61(12):4069-4074, 2016.
M.S. Stankovic and S.S. Stankovic. Multi-agent temporal-difference learning with linear function
approximation: Weak convergence under time-varying network topologies. In American Control
Conference, pp. 167-172, 2016.
S.	S. Stankovic, M. S. Stankovic, and D. Stipanovic. Decentralized parameter estimation by consen-
sus based stochastic approximation. IEEE Transactions on Automatic Control, 56(3):531-543,
2010.
J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang. Finite-time analysis of decentralized
temporal-difference learning with linear function approximation. In International Conference on
Artificial Intelligence and Statistics, pp. 4485-4495. PMLR, 2020.
W. Suttle, Z. Yang, K. Zhang, Z. Wang, T. Bayar, and J. Liu. A multi-agent off-policy actor-critic
algorithm for distributed reinforcement learning. In 21st IFAC World Congress, 2020.
R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. Cambridge: MIT press,
1998.
B. Touri. Product of Random Stochastic Matrices and Distributed Averaging. Springer Science &
Business Media, 2012.
J.	N. Tsitsiklis. Problems in Decentralized Decision Making and Computation. PhD thesis, Depart-
ment of Electrical Engineering and Computer Science, MIT, 1984.
J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approxi-
mation. IEEE Transactions on Automatic Control, 42(5):674-690, 1997.
N. H. Vaidya, L. Tseng, and G. Liang. Iterative approximate byzantine consensus in arbitrary di-
rected graphs. In Proceedings of the 2012 ACM symposium on Principles of distributed comput-
ing, pp. 365-374, 2012.
G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun. Decentralized TD tracking with linear
function approximation and its finite-time analysis. In 34th Conference on Neural Information
Processing Systems, 2020.
L. Xiao, S. Boyd, and S. Lall. A scheme for robust distributed sensor fusion based on average
consensus. In Proceedings of the 4th International Conference on Information Processing in
Sensor Networks, pp. 63-70, 2005.
S. Zeng, T.T. Doan, and J. Romberg. Finite-time analysis of decentralized stochastic approximation
with applications in multi-agent and multi-task learning. arXiv preprint arXiv:2010.15088, 2020.
K. Zhang, Z. Yang, and T. Basyar. Networked multi-agent reinforcement learning in continuous
spaces. In 57th IEEE Conference on Decision and Control, pp. 2771-2776, 2018a.
K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basyar. Fully decentralized multi-agent reinforcement
learning with networked agents. In 35th International Conference on Machine Learning, pp.
5872-5881, 2018b.
K. Zhang, Z. Yang, and T. Basyar. Multi-agent reinforcement learning: A selective overview of
theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
K.	Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basyar. Finite-sample analysis for decentralized batch
multi-agent reinforcement learning with networked agents. IEEE Transactions on Automatic Con-
trol, 2021.
Y. Zhang and M.M. Zavlanos. Distributed off-policy actor-critic reinforcement learning with policy
consensus. In 58th IEEE Conference on Decision and Control, pp. 4674-4679, 2019.
11