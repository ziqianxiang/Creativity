Under review as a conference paper at ICLR 2022
Provably Improved Context-Based Offline
Meta-RL with Attention and Contrastive
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Meta-learning for offline reinforcement learning (OMRL) is an understudied prob-
lem with tremendous potential impact by enabling RL algorithms in many real-
world applications. A popular solution to the problem is to infer task identity as
augmented state using a context-based encoder, for which efficient learning of
robust task representations remains an open challenge. In this work, we provably
improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating
intra-task attention mechanism and inter-task contrastive learning objectives, to
robustify task representation learning against sparse reward and distribution shift.
Theoretical analysis and experiments are presented to demonstrate the superior
performance and robustness of our end-to-end and model-free framework compared
to prior algorithms across multiple meta-RL benchmarks. 1
1	Introduction
Deep reinforcement learning (RL) has achieved many successes with human- or superhuman-level
performance across a wide range of complex domains (Mnih et al., 2015; Silver et al., 2017; Vinyals
et al., 2019; Ye et al., 2020). However, all these major breakthroughs focus on finding the best-
performing strategy by trial-and-error interactions with a single environment, which poses severe
constraints for scenarios such as healthcare (Gottesman et al., 2019), autonomous driving (Shalev-
Shwartz et al., 2016) and controlled-environment agriculture (An et al., 2021; Cao et al., 2021) where
safety is paramount. Moreover, these RL algorithms require tremendous explorations and training
samples, and are also prone to over-fitting to the target task (Song et al., 2019; Whiteson et al., 2011),
resulting in poor generalization and robustness. To make RL truly practical in many real-world
applications, a new paradigm with better safety, sample efficiency and generalization is in need.
Offline meta-RL, as a marriage between offline RL and meta-RL, has emerged as a promising
candidate to address the aforementioned challenges. Like supervised learning, offline RL restricts
the agent to solely learn from fixed and limited data, circumventing potentially risky explorations.
Additionally, offline algorithms are by nature off-policy, which by reusing prior experience, have
proven to achieve far better sample efficiency than on-policy counterparts (Haarnoja et al., 2018).
Meta-RL, on the other hand, exploits the shared structure of a distribution of tasks and enables the
agent to adapt to new tasks with minimal data. One popular approach is by learning a single universal
policy conditioned on a latent task representation, known as context-based method (Hallak et al.,
2015). Alternatively, the shared skills can be learned with a meta-controller (Oh et al., 2017).
In this work we restrict our attention on context-based offline meta-RL (COMRL), an understudied
framework with a few existing algorithms (Li et al., 2019; Dorfman & Tamar, 2020; Mitchell et al.,
2020; Li et al., 2021a), for a set of tasks that differ in reward or transition dynamics. One major
challenge associated with this scenario is termed Markov Decision Process (MDP) ambiguity (Li
et al., 2019; Dorfman & Tamar, 2020), namely the task-conditioned policies spuriously correlate task
identity with state-action pairs due to biased distribution of the fixed datasets. This phenomenon can
be interpreted as a special form of memorization problem in classical meta-learning (Yin et al., 2019),
where the value and policy functions overfit the training distributions without capturing causality
1Source code is provided in the supplementary material.
1
Under review as a conference paper at ICLR 2022
from reward and transition functions, often leading to degenerate task representations (Li et al.,
2021a) and poor generalization. To alleviate such over-fitting, Li et al. (2021a) proposes a framework
named FOCAL which decouples the learning of task inference from control by using self-supervised
distance metric learning. However, they made a strong assumption on the existence of an injective
map from each transistion tuple {s, a, s0 , r} to its task identity. Under extreme scenarios such as
sparse reward, where a considerable portion of aggregated experience provides little information
regarding task identity, efficient and robust learning of task representations is still challenging.
To address the aforementioned problem, in this paper we propose intra-task attention mechanism
and inter-task contrastive learning objectives to achieve robust task inference. More specifically, for
each task, we apply a batch-wise gated attention to recalibrate the weights of transition samples, and
use sequence-wise self-attention (Vaswani et al., 2017b) to better capture the correlation within the
transition (state, action, reward) dimensions. In addition, we implemented a matrix-form objective of
the Momentum Contrast (MoCo) (He et al., 2020) for task-level representation learning, by replacing
its dictionary queue with a meta-batch sampled on-the-fly. We provide theoretical analyses showing
that our objective serves as a better surrogate than naive contrastive loss for task inference and the
proposed attention mechanism on top can also reduce the variance of task representation. Moreover,
empirical evaluations demonstrate that the proposed design choices of attention and contrastive
learning mechanisms not only boost the performance of task inference, but also significantly improve
its robustness against sparse reward and distribution shift. We name our new method FOCAL++.
2	Related Work
Attention in RL Although attention mechanism has proven a powerful tool across of a broad spectrum
of domains (Mnih et al., 2014; VasWani et al., 20l7a; Wang & Shen, 2017; VelickoVic et al., 2018;
Devlin et al., 2018), to our best knowledge, its applications in RL remain relatively understudied.
Most of preVious Works in RL (Mishra et al., 2018; Sukhbaatar et al., 2019; Kumar et al., 2020;
Parisotto et al., 2020) focus on applying temporal attention in order to capture the time-dependent
correlation in MDPs or POMDPs. Raileanu et al. (Raileanu et al., 2020) uses transformer as the
default dynamics/policy encoder for meta-RL, similar to our proposed sequence-Wise attention,
Without giVing any intuition or comparatiVe study on such design choice. Wang et al. (2021) recently
implemented attention in meta-RL but didn’t consider the offline setting.
The closest Work We found by far (Barati & Chen, 2019; Li et al., 2021b) employ attention in
multi-VieW/multi-agent RL, to learn different Weights on Various Workers or agents, aggregated by a
global netWork to form a centralized policy. Analogous to our proposal, such architecture has the
adVantage of adaptiVely accounting for inhomogeneous importance of each input in the decision
making process, and makes the global agent robust to noise and partial obserVability.
Contrastive Learning ContrastiVe learning (Chopra et al., 2005; Hadsell et al., 2006) has emerged
as a poWerful frameWork for representation learning. In essence, it aims to capture data structures
by learning to distinguish betWeen semantically similar and dissimilar pairs. Recent progress in
contrastiVe learning focuses mostly on learning Visual representations as pretext tasks. MoCo (He
et al., 2020) formulates contrastiVe learning as dictionary look-up, and builds a dynamic dictionary
With a queue and a moVing-aVeraged encoder. SimCLR (Chen et al., 2020) further pushes the
SOTA benchmark With careful composition of data augmentations. HoWeVer, all these algorithms
concentrate primarily on generating pseudo-labels and contrastiVe pairs, Whereas in COMRL scenario,
the task labels and transition samples are naturally giVen.
There are a feW recent Works Which apply contrastiVe learning in RL (Laskin et al., 2020) or meta-RL
(Fu et al., 2020) settings. Fu et al. (2020) employs InfoNCE (Oord et al., 2018) loss to train a
contrastiVe context encoder. They inVestigated the technique in the online setting, Where the encoder
requires an information-gain-based exploration strategy to be effectiVe. In contrast, this paper focuses
on hoW contrastiVe learning performs in the fully-offline setting.
Context-Based Offline Meta-RL (COMRL) Context-based offline meta-RL employs models
With memory such as recurrent (Duan et al., 2016; Wang et al., 2016; Fakoor et al., 2020), recursiVe
(Mishra et al., 2018) or probabilistic (Rakelly et al., 2019) structures to achieVe fast adaptation by
aggregating experience into a latent representation on Which the policy is conditioned. To address
the bootstrapping error problem (Kumar et al., 2019) for offline learning, frameWork like FOCAL
2
Under review as a conference paper at ICLR 2022
enforces behavior regularization (Wu et al., 2019), which constrains the distribution mismatch
between the behavior and learning policies in actor-critic objectives. We follow the same paradigm.
3	Method
To tackle the COMRL problem, we follow the procedure described in FOCAL (Li et al., 2021a), by
first learning an effective representation of tasks on latent space Z, on which a single universal policy
is conditioned and trained with behavior-regularized actor-critic method (Wu et al., 2019). As an
improved version of FOCAL, our main contribution is twofold:
1.	To our best knowledge, we are the first to apply attention mechanism in offline multi-
task/meta-RL setting, for learning robust task representations. We combine batch-wise gated
attention with sequence-wise transformer encoder, and demonstrate its lower variance as
well as robustness against sparse reward and MDP ambiguity compared to prior COMRL
methods.
2.	On top of attention, we incorporate a matrix reformulation of Momentum Contrast (He
et al., 2020) for task representation learning, with theoretical guarantees and provably better
performance than ordinary contrastive objective.
3.1 Problem Setup
Consider a family of stationary MDPs defined by M = (S, A, P, R, γ) where (S, A, P, R, γ) are
the corresponding state space, action space, transition function, reward function and discount factor.
A task T is defined as an instance of M, which is associated with a pair of time-invariant transition
and reward functions, P(s0|s, a) ∈ P and R(s, a) ∈ R, respectively. In this work, we focus on tasks
which share the same state and action space. Consequently, a task distribution can be modeled as a
joint distribution of P and R, which usually can be factorized:
p(T) := p(P, R) =p(P)p(R).	(1)
In the offline setting, each task Ti (i being the task label) is associated with a static dataset of transition
tuples Di = {g} = {(si, ai, si, Ri(si, aj)}, for which P(Di) = P(Ti). Each tuple Ci ~ Di is a
sequence along the so-called transition/sequence dimension. A meta-batch B is a set of mini-batches
Bi ~ Di. Consider a meta-optimization objective in a multi-task form (Rakelly et al., 2019; Fakoor
et al., 2020),
L(θ, Ψ) = EDi~p(D) [Lactor(Di; θ) + LCritic(Di； ψ)]	(2)
=EDi~p(D)[LDi(θ,ψ)],	(3)
where LDi (θ, ψ) is the objective evaluated on transition samples drawn from Di, parameterized by θ
and ψ. Assuming a common uniform distribution for a set of n tasks, the meta-training procedure
turns into minimizing the average losses across all training tasks
1n
1
θmeta, ψmeta = arg mm—〉E [£。忆(θ, ψ)].	(4)
θ,ψ n k=1
For COMRL problem, a task distribution corresponds to a family of MDPs on which a single universal
policy is supposed to perform well. Since the MDP family is considered partially observed if no task
identity information is given, a task inference module Eφ(z∖c) is required to map context information
C ~ D to a latent task representation Z ∈ Z to form an augmented state, i.e.,
SaUg -SxZ,	SaUg J concat(s, z).	(5)
Such an MDP family is formalized as Task-Augmented MDP (TA-MDP) in FOCAL. Additionally,
Li et al. (2021a) proves that a good task representation z is crucial for optimization of the task-
conditioned meta-objective in Eqn 4, which is the prime focus of this paper. We now show how to
address the issue with the proposed attention architectures and contrastive learning framework.
3.2	Attention Architectures
3
Under review as a conference paper at ICLR 2022
Figure 1: Context encoder as a stack of attention blocks.
We employ two forms of intra-task attention in
the context encoder Eφ(z|c): batch-wise gated
attention and sequence-wise self-attention,
for learning better task representations. The ar-
chitectures are shown in Figure 2.
Batch-Wise Gated Attention When perform-
ing task inference, transitions inside the same
batch may contribute differently to the represen-
tation learning, especially in sparse reward situa-
tions. For tasks that differ in rewards, intuitively,
transition samples with non-zero rewards con-
tain more information regarding the task identity.
Therefore, we utilize a gating mechanism sim-
ilar to (Hu et al., 2018) along the batch dimen-
sion to adaptively recalibrates this batch-wise
response by computing a scalar multiplier for
every sample as in Figure 1.
Figure 2: Attention modules for task inference.
BA: batch-wise attention. SA: sequence-wise at-
tention.
Sequence-Wise Self-Attention A naive MLP encoder maps a concatenated 1-D sequence
(s, a, s0 , r ) from context buffer to a 1-D embedding z. This seq2seq model can be implemented
with sequence-wise attention to apply self-attention along the sequence dimension. The intuition
behind sequence-wise attention is that the attentive context encoder should in principle better capture
the correlation in (s, a, s0, r) sequence related to task-specific reward function R(s, a) and transi-
tion function P (s0|s, a), compared to normal MLP layers employed by common context-based RL
algorithms.
Illustrated in Figure 1, since two attention modules operate on separate dimensions, we connect them
in parallel to generate task embedding z by addition.
Figure 3: Inter-task matrix-form momentum contrast.
Given two meta-batches of transitions {cq } and {ck }, a
quickly progressing query encoder and a slowly progress-
ing key encoder compute the corresponding batch-wise
mean task representations in latent space Z . A matrix mul-
tiplication is performed between the set of query and key
vectors to produce the supervised contrastive loss in Eqn
8. T, C, Z are the meta-batch, transition and latent space
dimensions respectively.
4
Under review as a conference paper at ICLR 2022
3.3	The Contrastive Learning Framework
Inspired by the successes of contrastive learning in computer vision (He et al., 2020), we process the
raw transition data with momentum encoders to generate a latent query vector zq as classifier and a
set of K latent key vectors {z0k, z1k, ..., zKk } as task representations. Suppose one of the keys z+k is
the only match to zq, we employ the InfoNCE (Oord et al., 2018) objective as the building block:
Lz = -Iog	Kxp(Zq ∙ ZzJT)
P= exp(zq ∙ Zz/T)
(6)
where τ is a temperature hyper-parameter (Wu et al., 2018).
To ensure maximum sample efficiency, for each pair of meta-batches B = {Bi 〜Di|i = 1,…,T}
where T is the meta-batch size, one can construct T InfoNCE objectives by taking the average
latent vector of each task as the key, which is also crucial for our theoretical analysis (Theorem
3.1). Namely, given a meta-batch of encoded queries {zq 〜Eφ(ZiIBi)Ii = 1,...,T} and keys
{zz 〜Ez (ZiIBi)Ii = 1,...,T}, our proposed contrastive loss is
T
Lz = - X log
i=1
which can be written in a matrix-form
exp(Zq ∙ ZzIT)
PT=I eχp(Zq∙ Zz/T)
(7)
Lz = - Tr(M),
Mij
lo	eχp(Zq ∙ Zz∕τ)
θg PT=1 eχp(Zq∙ ZzIT)
(8)
The training scheme of our proposed inter-task momentum contrast is illustrated in Figure 3.
Now we provide a theoretical analysis of the objective in Eqn 8. Intuitively, it is the log loss of a
T -way softmax-based classifier trying to classify each Ziz as Ziq . With this interpretation, we compare
it to a linear classifier with supervised loss and show that it can be recovered by the linear classifier if
the weight matrix is a specific mean task classifier (Theorem 3.1). Furthermore, we prove that our
proposed objective is a better surrogate than traditional contrastive loss for task inference.
Definition 3.1 (Supervised Contrastive Loss)
Lsup(T,g) := E 5〜P(T) ['({g(ci) — g(ci0)})].
Ci 〜Di,q∕ 〜Di/
(9)
where ` can be standard hinge or logistic losses as in (Saunshi et al., 2019).
Consider a linear classifier g(c) = W E(c), where the encoded latent vector E(c) is used as a
deterministic representation (Li et al., 2021a) and W ∈ RN×Z is a weight matrix trained to minimize
Lsup(T, WE), Z is the dimension of the task latent space Z. Such construction of contrastive
objective enables self-supervised task representation learning for task inference, without requiring
access to full labels of all possible tasks, which is flexible and has better potential for generalization.
Hence the supervised loss of E on T is defined as
Lsup(T,E) =	inf	Lsup(T,WE).	(10)
W ∈RN ×Z
Since the optimal W requires full knowledge and labels of the underlying task distribution, which is
infeasible given only training tasks. As with Saunshi et al. (2019), we consider a particular choice of
W *:
Definition 3.2 (Mean Task Classifier) For an encoder function E and a task set T of cardinality
N ,the mean task classifier Wμ is an N X Z weight matrix whose ith row is the mean latent vector
μi ofinputs with task label i. We use as a shorthand for its loss Lμup(T, E) := Lsup(T, W *E).
In pratice, we estimate the mean task representation of Zq and Zz using its batch-wise mean
μq,z ：= E	"Di	[ζq,z] ≈ E	ci,忆 q,z ],	(ii)
zq,k 〜Eφ,k(Zi∣Ci)	zq,k 〜Eq,k(Zi∣Ci)
which induces the following definitions:
5
Under review as a conference paper at ICLR 2022
Definition 3.3 (Averaged Supervised Contrastive Loss) Average supervised loss for an encoder
function E on T -way classification of task representation is defined as
Lsup(E) := E	Lsup({Ti}iT=1, E) .	(12)
{Ti}T=ι 〜P(T)
The average supervised loss of its mean classifier (Definition 3.2) is
Lμup(E) ：.、TE , ∖[Lμup({Ti}T=ι,E)].	(13)
{Ti}T=1 〜P(T)
When the loss function ` is the convex logistic loss, we prove in Appendix B that
Theorem 3.1 The matrix-form momentum contrast objective Lz (Eqn 8) is equivalent to the average
supervised loss ofits mean classifier L&p (Eqn 13) if E = Ek (z|c) is the key encoder and the mean
task classifier Wμ whose ith row is the mean oflatent query vectors with task label i.
If we compare our proposed loss function with the classical unsupervised contrastive loss
Definition 3.4 (Unsupervised Contrastive Loss)
Lun(E) := E ['({E(C)T(E(c+) - E(c-))})] .	(14)
Given T as the number of distinct tasks in meta-batches, c, c+ are contexts from the same task, and
c- is from the other T - 1 tasks. Such construction is employed by prior COMRL methods like
FOCAL, which allows for task interpolation during meta-testing.
By Lemma 4.3 in (Saunshi et al., 2019), using convexity of ` and Jensen’s inequality, assuming no
repeated task labels in each meta-batch, we have
Theorem 3.2 For all context encoder E
LsuP(E) ≤ Lμup(E) ≤ Lun(E).	(15)
Combined with Theorem 3.1, it shows that our proposed contrastive objective in Eqn 8: Lz ≡
LSuP(Eφ(z∣c)) serves as a better surrogate for LsuP than the ordinary unsupervised contrastive losses
employed by prior methods, to ensure similarity-preserving task representation for COMRL.
3.4 Variance of Task Embeddings by FOCAL++
In experiments, we found that our proposed algorithm, FOCAL++, which combines attention mech-
anism and matrix-form momentum contrast, exhibit significant smaller variance compared to the
baselines on tasks with sparse reward (Table 2). We provide a proof of this observation for a simplified
version of FOCAL++, by only considering the batch-wise attention along with contrastive learning
objective defined in Eqn 8, in presence of sparse reward. Assuming all tasks differ only in reward
function, we begin with the following definition:
Definition 3.5 (Absolutely Sparse Transition) Given a set of tasks {T} which only differ by reward
function, a transition tuple (s,a,s’,r) is absolutely sparse if ∀Ti ∈ {T}, Ri(s, a) = constant.
According to policy invariance under reward transformations (Ng et al., 1999), without loss of
generality, we assume the constant above to be zero for the rest of the paper.
Definition 3.6 (Task with Sparse Reward) For a dataset Di = {(si, ai, s0i, Ri(si, ai))} sampled
from any task Ti with sparse reward, it can be decomposed as a disjoint union of two sets of
transitions:
Di = {(si, ai, s0i, Ri(si, ai))} ∪ {(si, ai, s0i, 0)}	(16)
= {cn} ∪ {cs },	(17)
where {cs } is the set of absolutely sparse transitions (Definition 3.5), which by definition are shared
across all tasks. {cn} consists of the rest of the transitions, and is unique to task Ti.
6
Under review as a conference paper at ICLR 2022
Definition 3.7 (Batch-Wise Gated Attention) The batch-wise gated attention assigns inhomoge-
neous weights W for batch-wise estimation ofthe mean task representation of μq,k in Eqn 11:
μq,k (W ):= Ec 〜Di [W (c)Eq'k (c)]	(18)
=pnE[W(cn)Eq,k(cn)] + psE[W (cs)Eq,k(cs)],	(19)
where pn , ps are the measures of {cn}, {cs } respectively and W is normalized such that
Ec〜Di [W(c)] = 1. Pn + Ps = 1 by Definition 3.6.
Theorem 3.3 Given a learned batch-wise gated attention weight W and context encoder E that
minimize the contrastive learning objective Lμup(W, E), we have
Var(μ?k(W)) ≤ Var(μq,k),	(20)
when the sparsity ratio exceeds a threshold.
i.e., the variance of learned task embeddings with batch attention is upper-bounded by its counterpart
without attention given the dataset is sparse enough. We prove Theorem 3.3 in Appendix B.
4 Experiments
In the following experiments, we show FOCAL++ outperforms the existing COMRL algorithms by a
clear margin in three key aspects: a) asymptotic performance of learned policy; b) task representations
with lower variance; and c) robustness to sparse reward and MDP ambiguity.
All trials are averaged over 3 random seeds. The offline training data are generated in accordance with
the protocol of FOCAL by training stochastic SAC (Haarnoja et al., 2018) models for every distinct
task and roll out policies saved at each checkpoint to collect trajectories. The offline training datasets
can be collected as a selection of the saved trajectories, which facilitates tuning of the performance
level and state-action distributions (Table 3). Both training and testing sets are pre-collected, making
our method fully-offline. Rewards are sparsified by constructing a neighborhood of goal in state or
velocity space, where transition samples which lie outside the area are assigned zero reward. Since
the focus of this paper is robust task representation learning which can be decoupled from control
according to FOCAL, we use sparse-reward data only when training the context encoders. Learning
of meta-policy in presence of sparse reward is another active but orthogonal area of research where
quite a few successful solutions have been found (Andrychowicz et al., 2017; Eysenbach et al., 2020).
A concrete description of the hyper-parameters and experimental settings is covered in Appendix D.
Table 1: Average testing return (standard deviation in parenthesis) of FOCAL and variants of
FOCAL++.
Algorithm	Sparse-Point-Robot	Point-Robot-Wind	SParSe-Cheetah-Dir	SParSe-Ant-Dir	SParSe-Cheetah-Vel	Walker-2D-ParamS
FOCAL	11.84(1.05)	-5.61(0.59)	1351.40(90.46)	429.92(41.52)	-183.32(40.16)	302.70(12.94)
FOCAL++ (contrastive) FOCAL++	12.53(0.31)	-5.78(0.44)	1309.76(115.33)	504.00(145.80)	-158.95(21.36)	366.35(55.08)
(batch-wise) FOCAL++	12.54(0.23)	-5.57(0.34)	1330.56(162.03)	687.37(85.95)	-150.58(11.75)	376.52(36.59)
(seq-wise)	12.64(0.14)	-5.09(0.01)	1293.40(129.99)	573.26(186.22)	-140.63(11.52)	375.67(45.72)
FOCAL++	12.96(0.09)	-5.39(0.57)	1470.52(68.29)	719.77(57.58)	-137.31(7.06)	391.02(42.44)
Table 2: Variance of context embeddings averaged over all training tasks and latent dimensions.
Algorithm	SParSe-Point-Robot	Point-Robot-Wind	Sparse-Cheetah-Dir	Sparse-Ant-Dir	Sparse-Cheetah-Vel	Walker-2D-Params
FOCAL	8.54E-5	3.05E-3	4.31E-3	2.24E-3	2.57E-3	1.06E-2
FOCAL++						
(contraStive) FOCAL++	7.83E-5	1.68E-3	6.86E-4	1.77E-3	1.73E-3	5.79E-3
(batch-wiSe) FOCAL++	7.73E-5	1.70E-3	4.66E-4	7.51E-4	1.04E-3	5.85E-3
(Seq-wiSe)	7.94E-5	1.84E-3	9.43E-4	8.00E-4	9.76E-4	5.46E-3
FOCAL++	8.27E-5	1.68E-3	7.82E-4		1.35E-3	1.06E-3	5.23E-3
7
Under review as a conference paper at ICLR 2022
——FOCAL++ ——FOCAL ——Batch PEARL ——Contextual BCQ ——MBML
(a) FOCAL++ vs. 4 baselines.
FoCAL	FOCAL++
(b) Point-Robot-Wind
(c) Sparse-Ant-Dir
Figure 4: Left: Test-task performance vs. transition steps sampled for meta-training. Right: t-SNE
visualization of the learned task embeddings zq on Point-Robot-Wind and Sparse-Ant-Dir. Each
point represents a query vector which is color-coded according to its task label.
4.1	Asymptotic Performance
We evaluate FOCAL++ on 6 continuous control meta-environments of robotic locomotion (Todorov
et al., 2012) adopted from FOCAL. 4 (Sparse-Point-Robot, Sparse-Cheetah-Vel, Sparse-Cheetah-Fwd-
Back, Sparse-Ant-Fwd-Back) and 2 (Point-Robot-Wind, Walker-2D-Params) environments require
adaptation by reward and transition functions respectively. For inference, FOCAL++ aggregates
context from a fixed test set to infer task embedding, and is subsequently evaluated online. Besides
FOCAL, three other baselines are compared: an offline variant of the PEARL algorithm (Rakelly et al.,
2019) (Batch PEARL), a context-based offline BCQ algorithm (Fujimoto et al., 2019) (Contextual
BCQ) and a two-stage COMRL algorithm with reward/dynamics relabelling (Li et al., 2019) (MBML).
Shown in Figure 4a, FOCAL outperforms other methods across almost all domains with context
embeddings of higher quality in Figure 4b,4c. In Table 1, our ablation studies also show that each
design choice of FOCAL++ alone can improve the performance of the learned policy, and combining
the orthogonal intra-task attention mechanism with inter-task contrastive learning yields the best
outcome.
4.2	Robustness to MDP Ambiguity and Sparse Reward
In our experimental setup, an ideal context encoder should capture the generalizable information for
task inference, namely the difference between reward/dynamics functions across a distribution of
tasks. However, as discussed in Section 1, there are two major challenges that impede conventional
COMRL algorithms from learning robust representations:
Table 3: Average testing return of FOCAL and FOCAL++ on Sparse-Point-Robot with different
distributions of training/testing sets. The numbers in parenthesis represent performance drop due to
distribution shift. Additional experiments are presented in Apppendix C.
Environment	Training	Testing	FOCAL	FOCAL++
Sparse- Point- Robot	expert	expert medium random	-8716 7.12(1.04) 4∙43(3.73)	∏260 12.47(0.13) 10.17(2.43)
	medium	medium expert random	8.44 8.25(0.19) 6.76(1.68)	12.54 12.44(0.10) 10.49(2.05)
Walker-2D- Params	mixed	mixed expert	302.70 271.69(31.01)	391.02 377.46(13.56)
8
Under review as a conference paper at ICLR 2022
(a) State distribution of relabeled (b) Test-task performance	(c) Batch-wise attention weight
dataset
Figure 5: Result on the relabeled Sparse-Point-Robot dataset. (a) State distributions of the expert
datasets for 20 distinct tasks, with goals uniformly distributed on a semicircle. (b) On mixed dataset,
FOCAL completely fails in this scenario whereas FOCAL++ variants with batch-wise attention are
able to learn. (c) Probability distribution of the batch-wise attention weight of samples with absolutely
zero and non-zero reward. Binary classification AUC = 0.969.
MDP ambiguity arises due to COMRL algorithms’ sensitivity to fixed dataset distributions (Li et al.,
2019; Dorfman & Tamar, 2020). Take Sparse-Point-Robot for example, as in Figure 5a, for tasks
with a goal on the semicircle, the state-action distribution exhibits specific pattern which may reflect
task identity. Given D = {(s, a, s0, r)} as input, the context encoder may learn a spurious correlation
between state-action distributions and task identity, which causes performance degradation under
distribution shifts (Table 3).
Sparse reward in meta-environments could exacerbate MDP ambiguity by making a considerable
portion of transitions uninformative for task inference, such as the samples outside any goals in Figure
5a. Attention mechanism, especially the batch-wise channel attention, helps the context encoder
attend to the informative portion of the input transitions, and therefore significantly improve the
robustness of the learned policies.
To demonstrate the robustness of FOCAL++ in presence of the two challenges above, we tested it
against distribution shift by using datasets of various qualities: expert, medium, random and mixed
which combines all three. Shown in Table 3, we observe that overall the performance drop due to
distribution shift is significantly lower when attention and contrastive learning are applied.
Moreover, we are aware that even mixing of datasets generated by different behavior policies cannot
fully eliminate the risk of MDP ambiguity since the state-action distributions for each task still
do not completely overlap. To show that the attention modules introduced by FOCAL++ indeed
works as intended by capturing the reward-task dependency, we create a new dataset on Sparse-
Point-Robot by merging the state-action support across all tasks and relabelling the sparse reward
according to the task-specific reward functions. In principle, this fully prevents information leakage
from the state-action distributions, forcing the context encoder to learn to distinguish the reward
functions between tasks while minimizing the contrastive loss. Shown in Figure 5b, we experimented
with 3 attention variants of FOCAL++ on the relabeled dataset, and found that batch-wise attention
significantly improves the performance as intended. Additionally, we visualize the density distribution
of batch-wise attention weights assigned to samples in Figure 5c. We see a clear tendency for the
batch-attention module to assign zero weight to samples with zero rewards (the absolutely sparse data
points which lie outside all goal circles in Figure 5a) and maximum weights to the non-zero-reward
transitions, with binary classification AUC = 0.969, which is clear evidence of FOCAL++ learning
the correct correlation for task inference by attending to the informative context.
5 Conclusion
In this work, we address the understudied COMRL problem and provably improve upon the existing
SOTA baselines such as FOCAL, by focusing on more effective and robust learning of task represen-
tations. Key to our framework is the combination of intra-task attention mechanism and inter-task
contrastive learning, for which we provide theoretical grounding and experimental evidence on the
superiority of our design.
9
Under review as a conference paper at ICLR 2022
References
Zhicheng An, Xiaoyan Cao, Yao Yao, Wanpeng Zhang, Lanqing Li, Yue Wang, Shihui Guo, and
Dijun Luo. A simulator-based planning framework for optimizing autonomous greenhouse control
strategy. In Proceedings of the International Conference on Automated Planning and Scheduling,
volume 31, pp. 436-444, 2θ21.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
In Advances in neural information processing systems, pp. 5048-5058, 2017.
Elaheh Barati and Xuewen Chen. An actor-critic-attention mechanism for deep reinforcement
learning in multi-view environments. arXiv preprint arXiv:1907.09466, 2019.
Xiaoyan Cao, Yao Yao, Lanqing Li, Wanpeng Zhang, Zhicheng An, Zhong Zhang, Shihui Guo,
Li Xiao, Xiaoyu Cao, and Dijun Luo. igrow: A smart agriculture solution to autonomous
greenhouse control. arXiv preprint arXiv:2107.05464, 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597-1607. PMLR, 2020.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR’05), volume 1, pp. 539-546. IEEE, 2005.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Ron Dorfman and Aviv Tamar. Offline meta reinforcement learning. arXiv preprint arXiv:2008.02598,
2020.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2 : Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
Benjamin Eysenbach, Xinyang Geng, Sergey Levine, and Ruslan Salakhutdinov. Rewriting history
with inverse rl: Hindsight inference for policy improvement. arXiv preprint arXiv:2002.11089,
2020.
Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. Meta-q-learning. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=SJeD3CEFPH.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. arXiv preprint arXiv:1703.03400, 2017.
Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu.
Towards effective context for meta-reinforcement learning: an approach based on contrastive
learning. 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062, 2019.
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale
Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nat Med,
25(1):16-18, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning, pp. 1861-1870. PMLR, 2018.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’06), volume 2, pp. 1735-1742. IEEE, 2006.
10
Under review as a conference paper at ICLR 2022
Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv
preprint arXiv:1502.02259, 2015.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132-7141, 2018.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing
Systems, pp. 11784-11794, 2019.
Shakti Kumar, Jerrod Parker, and Panteha Naderian. Adaptive transformers in rl. arXiv preprint
arXiv:2004.03761, 2020.
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. In International Conference on Machine Learning, pp. 5639-5650.
PMLR, 2020.
Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Keith Ross, Henrik Iskov
Christensen, and Hao Su. Multi-task Batch Reinforcement Learning with Metric Learning. arXiv
e-prints, art. arXiv:1909.11373, September 2019.
Lanqing Li, Rui Yang, and Dijun Luo. FOCAL: Efficient fully-offline meta-reinforcement learning
via distance metric learning and behavior regularization. In International Conference on Learning
Representations, 2021a. URL https://openreview.net/forum?id=8cpHIfgY4Dj.
Wenhao Li, Xiangfeng Wang, Bo Jin, Dijun Luo, and Hongyuan Zha. Structured cooperative
reinforcement learning with time-varying composite action space. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2021b.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive
meta-learner. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=B1DmUzWAW.
Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline meta-
reinforcement learning with advantage weighting. arXiv preprint arXiv:2008.06043, 2020.
Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual
attention. In Proceedings of the 27th International Conference on Neural Information Processing
Systems-Volume 2, pp. 2204-2212, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Icml, volume 99, pp. 278-287, 1999.
Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with
multi-task deep reinforcement learning. In International Conference on Machine Learning, pp.
2661-2670. PMLR, 2017.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,
Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers
for reinforcement learning. In International Conference on Machine Learning, pp. 7487-7498.
PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. Fast adaptation to new environ-
ments via policy-dynamics value functions. In International Conference on Machine Learning, pp.
7920-7931. PMLR, 2020.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In International conference on
machine learning, pp. 5331-5340, 2019.
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. arXiv preprint arXiv:1810.06784, 2018.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A
theoretical analysis of contrastive unsupervised representation learning. In International Confer-
ence on Machine Learning, pp. 5628-5637. PMLR, 2019.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354-359, 2017.
Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overfitting
in reinforcement learning. arXiv preprint arXiv:1912.02975, 2019.
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention
span in transformers. arXiv preprint arXiv:1905.07799, 2019.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 6000-6010, 2017a. URL
http://papers.nips.cc/paper/7181- attention- is- all- you- need.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017b.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv
preprint arXiv:1611.05763, 2016.
Jane X Wang, Michael King, Nicolas Pierre Mickael Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie
Deck, Peter Choy, Mary Cassin, Malcolm Reynolds, H Francis Song, et al. Alchemy: A benchmark
and analysis toolkit for meta-reinforcement learning agents. In Thirty-fifth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.
Wenguan Wang and Jianbing Shen. Deep visual attention prediction. IEEE Transactions on Image
Processing, 27(5):2368-2378, 2017.
Shimon Whiteson, Brian Tanner, Matthew E Taylor, and Peter Stone. Protecting against evaluation
overfitting in empirical reinforcement learning. In 2011 IEEE symposium on adaptive dynamic
programming and reinforcement learning (ADPRL), pp. 120-127. IEEE, 2011.
12
Under review as a conference paper at ICLR 2022
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3733-3742, 2018.
Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,
Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforce-
ment learning. In AAAI, pp. 6672-6679, 2020.
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. arXiv preprint arXiv:1912.03820, 2019.
13
Under review as a conference paper at ICLR 2022
Appendix A	Pseudo-code2
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
1
2
3
4
5
6
7
8
Algorithm 1: FOCAL++ Meta-training
•	Pre-collected batch Di = {(sj, aj, s0j, rj)}j:1,...,N from a set of training tasks {Ti}i=1,...,n
drawn from p(T)
•	Learning rates α1, α2, α3, temperature τ, momentum m
Initialize context replay buffer Ci for each task Ti
Initialize context encoder network Eφ,k (z|c), learning policy ∏θ(a∣s,z) and Q-network
Qψ(s, z, a) with parameters φq, φk, θ and ψ
while not done do
for each Ti do
for t = 0, T - 1 do
I Sample mini-batches ofB transitions {(si t, ai t, s[t, r t)}t:i	B ~ Di and update
ICi	',,,,,
end
end
Sample a pair of query-key meta-batches of T tasks 〜p(T)
for step in training steps do
for each Ti do
Sample mini-batches Ci and bi 〜Ci for context encoder and policy training (bi, Ci are
identical by default, the rewards in bi are always non-sparse)
Compute ziq = Eφq(Ci)
for each Tj do
Sample mini-batches Cj from Cj and compute zjk = Eφk (Cj)
Mij = Mz(Zq, zj)
end
Liactor = Lactor (bi, Eφq (Ci))
Licritic = Lcritic (bi, Eφq (Ci))
end
Lz = Tr(M)
φq - φq - α1^φq Lz
φk - mφk + (1 - m)φq
θ J θ - α2Vθ Pi Lactor
ψ . ψ - αKψ Pi Lcritic
end
. matrix-form momentum contrast
. momentum update
end
Algorithm 2: FOCAL++ Meta-testing
•	Pre-collected batch Di0 = {(sj0, aj0, s0j0, rj0)}j0:1,...,M from a set of testing tasks
{Ti0}i0=1...m drawn from p(T)
Initialize context replay buffer Ci0 for each task Ti
for each Ti0 do
for t = 0, T - 1 do
Sample mini-batches of B transitions &，= {(sio,t,aio,t,sio,t,rio,t)}t:ι,...,B ~DV and
update Ci0
Compute ziq0 = Eφq (Ci0 )
Roll out policy ∏θ(a|s, ziq) for evaluation
end
end
2To prevent conflict or misunderstanding, all non-hyperlink equation/theorem numbers in the appendix refer
to those in the main text.
14
Under review as a conference paper at ICLR 2022
Appendix B	Definitions and Proofs
B.1	Proof of Theorem 3.1
Consider a task set {T} = {T1, ..., TT} drawn uniformly from p(T). In Definition 3.1, the loss
incurred by g on point (c, Ti) ∈ C × {T}3 is defined as '({g(c)i - g(c)i，}i，=i), which is a function
of a T -dimensional vector of differences in the coordinates. Given the definition of the mean task
classifier Wμ that g(c) = WE(C) and '(v) = log(1 + Pi exp(-vi)) the standard logistic loss as
in (Saunshi et al., 2019), the supervised contrastive loss defined in Eqn 10 can be rewritten as
Lsup(T,g) := ETi〜P(T) log 1 + ∑>xP £ (WiojE(ci)j - WijE(ci)j)	.	(21)
"D	I	i0=i	Ij	〃
Since the ith row of W is the mean of latent key vectors with task label i, and E = Ek(z|c) is the
key encoder, Eqn 21 turns into
Lsup(T, g) = ETi〜p(T) log11+∑S eχρ (zko ∙Zq -Zk ∙Zq )11.	(22)
In practice, we estimate the latent vectors Ziq,k using batch-wise mean to approximate the the mean
task representation μq,k. Therefore Lsup in 22 is equivalent to the mean task classifier LSup defined
in Definition 3.2. One step futher, assuming uniform distribution of the task set {T}4, the averaged
supervised contrastive loss by Definition 3.3 is
LIsup(E).TEE ,τJLμup({τi}乙,E)]
{Ti}T=I 〜P(T)
=T x log (1+x exp (zifco ∙Zq -Zk ∙Zq)
i=1	i0 6=i
二」X log	eχP(zq ∙ Zk)
T i=1	PT=I exP (Zi ∙ Zj ),
(23)
(24)
which is precisely the matrix-form momentum contrast objective (Eqn 8,9) if one rescales W by a
factor of τ .
B.2	Proof of Theorem 3.3
With Definition 3.5, 3.6 and 3.7, we hereby provide a simplified proof by assuming a constant weight
W(c) on the non-sparse set {cn} and the absolutely sparse set {cs} (Definition 3.5) respectively,
then we have
μqk (W )= PnW (Cn)Ecn 〜{cn }[Eq,k (Cn)] + PsW (Cs)Ecs 〜{c,} [Eq,k (cs)],	(25)
where the normalization condition Ec 〜Di[W (c)] = 1 implies PnW (cn)+ PsW (cs) = 1. Therefore,
adding the batch-wise attention is effectively modulating Pn andPs. Since Pn +Ps = 1, without loss
of generality, we apply the following notations:
3C = {(si , ai , s0i , Ri (si , ai))} is the context space
4Note that the task set {T} discussed here is a subset of the whole task set and does not necessarily
cover the whole support of p(T). It is sampled for the sole purpose of computing the contrastive loss.
15
Under review as a conference paper at ICLR 2022
pn = p, pnW (cn) = p0	(26)
EcnYcn}[E q,k(cn)] = Xnk, Ecs 〜{cs}[Eq,k(cs)] = xq,k.	(27)
Assuming i.i.d xn and xs, which gives
Var(μq,k (W)) = Var(p0χn,k + (1 - p0)χs,k) = (PyVar (Xnk) + (1 - p0)2Var(χS,k)	(28)
Var(μq,k) = Var(PXnk + (1 - p)xq，k) = p2Var(xqt,k) + (1 - p)2Vαr(xq,k).	(29)
By B.1, the averaged supervised loss Lμup(W, E) is equivalent to the matrix-form contrastive
objective, which can be written as
1T
Lμup(W ,E ) = TE
T i=1
1T
=T x
i=1
log 1 + ∑Seχp ((μko- μk) ∙ μq)
i06=i
log 11 + Eeχp (p0(XkO-Xk) ∙ μq)
i06=i
(30)
where we use the definition of μ in Eqn 25 and the fact that Xqk is the same across all tasks. Since
the learned W, E ∈ arg minw∈a,e∈eLμup(W, E), andp0 ≈ P by the identity map initialization of
.1	∙ 1 1	t 1	1	Γ∙ ι	t ^r ^ t ^
the residual attention module, We have, for learned p, X and μ,
p0 ≥ p,	(Xlk- Xk) ∙ μq < 0.
(31)
Now subtract Eqn 28 by 29, we have
Var(μq,k(W))- Var(μq,k) = [(p0)2 - p2]Var(Xn,k) + [(1 - p0)2 - (1 - p)2]Var(Xq,k)
= (PX0 - P) h(PX0 + P)Var(Xqn,k) - (2 - P - PX0)Var(Xqs,k)i
Vn Y	vʃɔj2- P)Var(XS,k) - PVat(Xnk
≤ 0, if P ≤ P0 ≤	sn.
Var(Xn，k) + Var(Xs，k)
(32)
The left inequality automatically holds by Eqn 31, the RHS is satisfied when
/ Var(Xq,k)
P ≤ Var(Xn,k) + Var(Xs,k)
or equivalently,
Var(Xq,k)
PS = (1 - p) ≥ ---——十,
’—Var(Xnk) + Var(Xq，k),
(34)
which means when the sparsity of reward exceeds the threshold, a learned batch attention module can
reduce the variance of the mean task representation μfk. Eqn 34 is corroborated by our experiments
on the relabeled Sparse-Point-Robot dataset (Figure 6).
16
Under review as a conference paper at ICLR 2022
Appendix C Additional Experiments
In Table 4, we present more experimental evidence that FOCAL++ is more robust against distribution
shift compared to FOCAL on Walker-2D-Params, which is consistent with Table 3 in the main text.
Table 4: Extension of Table 3 in the main text. Average testing return of FOCAL and FOCAL++ for
more settings of distribution shift on Walker-2D-Params.
Environment	Training	Testing	FOCAL	FOCAL++
Walker-2D- Params	expert	expert mixed random	373.92 322.24(51.68) 284.94(88.98)	364.75 340.60(24.15) 297.43(67.32)
	mixed	mixed expert random	302.70 271.69(31.01) 260.02(42.68)	391.02 377.46(13.56) 346∙95(44.07)
Figure 6: The variance-sparsity relation for FOCAL++/FOCAL on the relabeled Sparse-Point-Robot
dataset. The y-axis measures the variance of the bounded task embeddings z ∈ (-1, 1)l averaged
over all l latent dimensions. See more details in D.2.
Moreover, to testify our conclusion in B.2, we present the variance of task embedding vectors of
FOCAL++ and FOCAL under various sparsity levels. Shown in Figure 6, the variance of the weighted
embeddings μfk (W) becomes lower than its unweighted counterpart μq,k when sparse ratio exceeds
a threshold about 0.6. The observation matches well with Eqn 34 we derived in B.2.
17
Under review as a conference paper at ICLR 2022
Appendix D	Experimental Details and Hyperparameter
D. 1 Overview of the Meta Environments
The meta-environments could be divided into two categories: meta-environments that only differ in
reward function and that only differ in transition function. For the meta-environments that only differ
in reward functions, we additionally introduce sparsity to the reward function.
•	Sparse-Point-Robot is a 2D-navigation task with sparse reward, introduced in Rakelly et al.
(2019). Each task is associated with a goal sampled uniformly on a unit semicircle. The
agent is trained to navigate to set of goals, then tested on a distinct set of unseen test goals.
Tasks differ in reward function only.
•	Point-Robot-Wind is another variant of Sparse-Point-Robot. Each task is associated with
the same reward but a distinct ”wind” sampled uniformly from [-l, l]2 . Every time the
agent takes a step, it drifts by the wind vector. We set l = 0.05 in this paper. Tasks differ in
transition function only.
•	Sparse-Cheetah-Vel, Sparse-Ant-Fwd-Back, Sparse-Cheetah-Fwd-Back are sparse-
reward variants of the popular meta-RL benchmarks Half-Cheetah-Vel, Sparse-Ant-Dir
and Sparse-Cheetah-Fwd-Back based on MuJoCo environments, introduced by Finn et al.
(2017) and Rothfuss et al. (2018). Tasks differ in reward function only.
•	Walker-2D-Params is a unique environment compared to other MuJoCo environments.
Agent is initialized with some system dynamics parameters randomized and must move
forward. Transitions function is dependent on randomized task-specific parameters such as
mass, inertia and friction coefficients. Tasks differ in transition function only.
The way we sparsify the reward functions is as follows.
sparsified reward
reward—goal radius
| goal radius |
0,
if reward > goal radius
otherwise .
(35)
Intuitively, we set rewards of states that lie outside a neighborhood of the goal to 0, and re-scaled
the rewards otherwise so that the sparse reward function is continuous. For each of the sparsified
environments other than the relabeled Sparse-Point-Robot, we set its goal radius to achieve a non-
sparse rate of about 50%. Note that only the transitions used for training the context-encoder are
sparsified, since the focus of this paper is learning effective and robust task representations.
D.2 Relabeled Dataset
As discussed in Section 4.3, to prevent information leakage of task identity from state-action distri-
bution, we construct the relabeled Sparse-Point-Robot dataset from a pre-collected dataset of the
Sparse-Point-Robot environment.
Figure 7 illustrates the generating process for task 2 of the original dataset. The original state
distribution of five example tasks on Sparse-Point-Robot is shown in the upper-left. After merging
the transition state-action support across all tasks, the (state, action, next state) distribution are
identical for every specific task. Then we recompute the reward for each transition according to the
task-specific reward functions and sparsify the result. We perform the merge-relabel-sparsify process
for all tasks on Sparse-Point-Robot to enhance the importance of the non-sparse samples for task
inference. The sparse samples in Figure 7 of the main text are those that lie outside of all goals, i.e.
transitions with zero reward across all tasks.
The dataset can be accessed and downloaded from relabeled_dataset.
D.3 Hyperparameters
Tables 5 and 6 describe the hyperparameters used in our empirical evaluations.
18
Under review as a conference paper at ICLR 2022
Sparsify
rewards
Figure 7: Generating process of the relabeled Sparse-Point-Robot dataset.
Table 5: Specifications of the environments experimented in our paper.
Training Set	Training Tasks	Testing Tasks	Goal Radius
SParse-Point-Robot	80	20	-0.2
SParse-Point-Robot (relabeled)	80	20	-0.5
Point-Robot-Wind	40	10	N/A
SParse-Cheetah-Vel	80	20	-0.1
SParse-Ant-FWd-BaCk	2	2	3
Sparse-Cheetah-Fwd-Back	2	2	6
Walker2d-Rand-Params	20	5	N/A
D.4 Implementation
All experiments are carried out on 64-bit CentOS 7.2 with Tesla P40 GPUs. Code is implemented
and run with PyTorch 1.2.0. One can refer to the source code in the supplementary material for a
complete list of dependencies of the running environment.
19
Under review as a conference paper at ICLR 2022
Table 6: Hyperparameters used for training to produce Figure 4(a). Meta-batch size refers to the
number of distinct tasks for computing the DML or contrastive loss at a time. Larger meta-batch size
leads to faster convergence but requires greater computational power. For Fwd-Back environments, a
meta-batch size of 4 suffices for stability and efficiency.
HyPerParameterS	Point-Robot	Mujoco
reward scale	100	5
discount factor	0.9	0.99
maximum episode length	20	200
target divergence	N/A	0.05
behavior regularization Strength(α)	0	500
latent space dimension	5	20
meta-batch size	16	16*
dmldr(αι)	1e-3	3e-3
actor」r(a2)	1e-3	3e-3
Critic」r(a3)	1e-3	3e-3
DML loss weight(β)	1	1
contrastive T	0.5	0.5
contrastive m	0.9	0.9
buffer size (per task)	1e4	1e4
batch size (sac)	256	256
batch size (context encoder)	512	512
g」r(f-divergence discriminator)	1e-4	1e-4
transformer hidden size (context encoder)	128	128
multihead (if enabled)	8	8
reduction (batch attention)	16	16
transformer blocks (context encoder)	3	3
dropout (context encoder)	0.1	0.1
network width (others)	256	256
network depth (others)	3	3
20