Under review as a conference paper at ICLR 2022
Provab le Hierarchy-Based
Meta-Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Hierarchical reinforcement learning (HRL) has seen widespread interest as an ap-
proach to tractable learning of complex modular behaviors. However, existing
work either assume access to expert-constructed hierarchies, or use hierarchy-
learning heuristics with no provable guarantees. To address this gap, we analyze
HRL in the meta-RL setting, where a learner learns latent hierarchical structure
during meta-training for use in a downstream task. We consider a tabular set-
ting where natural hierarchical structure is embedded in the transition dynamics.
Analogous to supervised meta-learning theory, we provide “diversity conditions”
which, together with a tractable optimism-based algorithm, guarantee sample-
efficient recovery of this natural hierarchy. Furthermore, we provide regret bounds
on a learner using the recovered hierarchy to solve a meta-test task. Our bounds
incorporate common notions in HRL literature such as temporal and state/action
abstractions, suggesting that our setting and analysis capture important features of
HRL in practice.
1	Introduction
Reinforcement learning (RL) has demonstrated tremendous successes in many domains (Schulman
et al., 2015; Vinyals et al., 2019; Schrittwieser et al., 2020), learning near-optimal policies despite
limited supervision. Nevertheless, RL remains difficult to apply to problems requiring temporally
extended planning and/or exploration (Ecoffet et al., 2021). A promising approach to this problem
is hierarchical reinforcement learning (HRL), which has seen continued interest due to its appealing
biological basis. In its most basic form, HRL seeks to decompose tasks into a sequence of skills,
each of which is easier to learn individually than the full task. By restricting the agent to using
learned skills, the search space over policies can be greatly reduced. Furthermore, learned skills can
induce simpler state and/or action spaces, simplifying the learning problem. Finally, learned skills
with useful semantic behavior can be reused across tasks, enabling transfer learning.
Naturally, a hierarchy-based learner is limited by the quality of skills that are made available and/or
learned. Accordingly, many empirical works have proposed algorithms for online skill learning in
the context of a single RL task (Nachum et al., 2019a; 2018). These approaches have been exper-
imentally demonstrated to be effective in finding useful and interpretable skills. Other approaches
consider the skill learning problem in the context of meta-RL (Frans et al., 2018), or in the reward-
free setting (Eysenbach et al., 2018). Nevertheless, the heuristics and algorithms proposed in these
empirical works do not provide any provable guarantees on the quality of learned skills.
On the other hand, theoretical analyses have mostly focused on how learners benefits from having
access to skills. For example, Fruit & Lazaric (2017) provide a regret bound on learning with skills
in the infinite-horizon average reward case. Meanwhile, in the meta-RL setting, Brunskill & Li
(2014) considers the problem of finding and using skills in a continual learning setting and provides
a sample complexity analysis. However, these analyses either sidestep the question of how the skills
are obtained, or do not address the problem in a computationally tractable manner.
In this work, we aim to take a step towards providing provable guarantees for hierarchy learning
through tractable algorithms. We focus on the meta-RL setting, in which a learner extracts skills
from a set of provided tasks which are then used in a downstream task. We work in the tabular case,
assuming the transition dynamics of the given tasks share latent hierarchical structure induced by
predetermined clustering and bottlenecks. Our contributions are as follows:
1
Under review as a conference paper at ICLR 2022
1.	“Diversity conditions” ensuring hierarchy recovery. We develop natural optimism-
based coverage conditions which ensure that bottlenecks embedded in the transition dy-
namics are detectable by solving provided meta-training tasks.
2.	A tractable hierarchy-learning algorithm. We provide an algorithm that provably learns
the latent hierarchy from interactions, assuming the coverage conditions above. Our
method has sample complexity scaling as O(TKS) in the leading term compared to
O(T S2A) for a brute-force method, where T are the number of tasks, S is the number
of states, A is the number of actions, and K SA is the number of skills to learn.
3.	Regret bounds on downstream tasks. We provide regret bounds for learners that apply
the extracted hierarchy from meta-training on downstream tasks. Furthermore, we show an
exponential regret separation between hierarchy-based and hierarchy-oblivious learners for
a family of task distributions, corroborating prevailing intuitions regarding when/Why HRL
helps. In particular, hierarchy-based learners incur regret bounded by O(VH2N) while
hierarchy-oblivious learners incur worst-case regret of at least O(2H/2 VH2N).
2	Notation
We now introduce notation which we will use throughout the paper. We write [K] := {1, . . . , K}.
Furthermore, we use the standard notations O, Θ, Ω to denote orders of growth, and O, Θ, Ω to
indicate suppressed logarithmic factors. We use δ(x) to denote the Dirac delta measure on x.
We work with finite-horizon Markov decision processes (MDPs), defined as a tuple M =
(S, A, P, r, H), where S is the set of states, A is the set of actions, P : S × A × S → [0, 1]
are the transition dynamics, r : S × A → [0, 1] is the reward function, and H is the horizon. We
assume stationary dynamics unless otherwise noted, in which case P(h) is the dynamics at time
step h. For constants relating to horizons, we will define [H] := {0, . . . , H - 1}. Given a policy
π : [H] × S → A, we define the value functions
H-1
Vhπ (s) := E r(sk, ak ) sh = s
k=h
H-1
and Qhπ(s, a) := E	r(sk, ak) (sh, ah) = (s, a)
k=h
where sk+ι 〜 P(∙ | Sk ,aQ and ak = ∏k (Sk). Furthermore, we write V * and Q* to denote optimal
value functions obtained by maximizing over ∏ (and are attained by the optimal policy ∏*). When a
learner plays π1 , . . . , πN in M, we define its regret as
N
RegretN (M) := XV0*(S0) - V0πt(S0).
t=1
We use ㊀ to denote a terminal state. We let τ∏ denote the (random) trajectory generated by ∏. For a
state S and length-H trajectory τ, we write S ∈ τπ ifSh = S for some h ∈ [H]. We define (S, a) ∈ τπ
similarly. Finally, given an MDP M, M(2H) denotes a copy ofM with a doubled horizon.
3	Setting
We work in the tabular meta-RL setting. The learner can access T meta-training MDPs
{(S,A,Pt,rt,H)}t∈[T]={Mt}t∈[T].Note that Pt and rt both vary across tasks. We set S := |S|
and A := |A|. After interacting with these tasks, the learner is presented with a meta-test MDP
(S, A, PTg, rTg, H), where the learner seeks to minimize its regret. We assume, without loss of
generality, that the MDPs have a shared starting state S0. For meta-learning to succeed, the MDPs
must have shared structure. We focus on the following notion of shared hierarchical structure:
Definition 3.1 (Latent Hierarchy). Let {Zc} be a partition of the state space S into clusters. We
associate with each cluster Z a set of entrances Ent(Z) ⊆ Z and a set of exits Ext(Z) ⊆ Z × A.
We say that the tasks have a latent hierarchy with respect to ({Zc} , Ent(∙), Ext(∙)) if for any Z/
(a)	For any (s,a) ∈ (Zc XA) \ Ext(Zc), Pt(∙ | s,a) is constant over t and supported on Zc. ◊
2
Under review as a conference paper at ICLR 2022
(b)	For any (s, a) ∈ EXt(Zc), there exists t, t0 with t = t0 SUch that Pt(∙ | s, a) = Pt，(∙ | s, a).
Furthermore, Pt(∙ | s, a) is supported on Uc Ent(Zc) for any t ∈ [T].
Definition 3.1 partitions the shared state space of the MDPs into
clusters such that (1) non-exit (s, a) dynamics do not change be-
tween the MDPs and (2) exits are bottlenecks between clusters.
Example 3.1 (Gated Four-Room). Figure 1 illustrates the gated
four-room environment along with a sample task. The environment
has a latent hierarchy with respect to the rooms outlined by colored
gates (which can be open or closed). Entrances are colored aqua,
while exits are marked by arrows.
Although we assume a single fixed starting state, we can model
task-dependent initial states by appending a dummy state s0 . A
dummy action a0 then takes the agent to the starting state for the
task. Observe that (s0, a0) is an exit, and therefore must transition
to an entrance.	y
To see how Definition 3.1 captures intuitive notions of hierarchy in
practical settings, we provide an example of a continuous setting
roughly fitting into our framework:
Example 3.2 (The Alchemy benchmark). Alchemy (Wang et al.,
2021) is a recently proposed empirical benchmark for meta-RL,
where the agent needs to place a stone in a series of potions to ob-
tain some desired appearance, as illustrated in Figure 2. Dipping
a stone into a potion traverses an edge (determined by the potion)
in a graph where nodes are possible stone appearances. We focus
on task distributions that randomize the edges of this graph (i.e.,
potion positions and feasible stone appearances are fixed). Then,
the set of obtainable MDPs has a latent hierarchy where dipping the
Figure 1: The gated four-
room setting, with a task re-
quiring navigation from the
green square to the star.
# ■ ■ ■ ,H,
stone into any of the potions is an exit. Indeed, other than dipping the stone into a potion, all other
actions (e.g., moving the stone around the room) have the same dynamics in all tasks.	y
For convenience, we will define several relevant notions. First,
for any cluster Z, we define its interior Z◦ := (Z ×A) ∖Ext(Z).
Furthermore, we let Ent(S) := Uc Ent(Zc) denote the set of
all entrances and Ext(S) := Uc Ext(Zc) the set of all exits.
Finally, we define
K := |Ext(S)| L := |Ent(S)|
M := sup |Ext(Zc)|
c
so that K and L are the total number of exits and entrances,
respectively, while M is the maximal number of exits from any
cluster.
Figure 2: Alchemy. Placing
stones in potions moves the agent
through a latent graph of object
properties.1
Query Model. We work in the online setting, where the agent interacts with the tasks by playing
policies from the initial state s0 . During meta-training, we allow the agent to interact with the
environments using an unbounded number of timesteps for each trajectory before resetting. We then
compute query complexity in terms of the total number of timesteps spent in all tasks in total.
4	Meta-Training Analysis
In this section, we outline an algorithm for uncovering the latent structure that can be used for
downstream tasks. Recall that exits are defined by changing dynamics between tasks. To quantify
the number of samples needed for exit detection, we define the following quantity:
1Image from Wang et al. (2021), extracted from a larger figure with no other modifications (License).
3
Under review as a conference paper at ICLR 2022
Definition 4.1 (β-dynamics separation). There exists β > 0 such that for any t, t0 ∈ [T] and
(s,a) ∈ Ext(S), Pt(∙ | s,a) = Pt，(∙ | s,a) =⇒ ∣∣Pt(∙ | s,a) — Pt，(∙ | s,a)∣ITV ≥ β.	◊
In defining β above, We ensure high-probability exit detection with O(S∕β2) samples. Note that
there is a brute-force approach to learning the underlying structure: one can learn Pt(∙ | s,a) for all
(s, a) and t ∈ [T], and iterate over (s, a) to check for changing dynamics. This can be done with
query complexity O(TS2A∕β2). However, under reasonable “coverage” assumptions outlined in
the next section, this query cost can be lowered to O(TKS/β2).
4.1 Defining a Notion of Coverage
In supervised meta-learning, “diversity conditions” ensure that the meta-training tasks reveal the
underlying latent structure (Tripuraneni et al., 2020; Du et al., 2020). We provide analogous condi-
tions ensuring that M1, . . . , MT “cover” the latent hierarchy. Since solving maxπ V π(s0) requires
fewer samples than learning P, we expect such conditions to provide sample complexity gains.
Visitation Probabilities and α-Importance. Minimally, exits should be visited by optimal poli-
cies of the meta-training tasks for coverage. We thus define the following notion:
Definition 4.2. Fix an MDP M = (S, A, P, r, H), and let (s, a) ∈ S × A. Construct a modified
MDP M \ (s, a), where (s, a) brings the agent to a terminal state with no reward. Then, we say that
(s, a) is α-importantfor M if V0M'(s,a),*(s0) < V0M,*(s0) — α.	◊
A Preliminary Coverage Assumption? Lemma 4.1 suggests a simple
coverage condition: for any (s, a) ∈ Ext(S), assume that there exists
t,t0 ∈ [T] so that (s, a) is α-important for t and t0, and Pt(∙ | s, a)=
Pto(∙ | s, a). However, as the following example shows, this condition
excludes natural settings:
Figure 3: Illustrating α-
importance. Since the
black arrow is the only
path to the goal, it is
V0f (so)-important.
Example 4.1. In Example 3.1, gates are either open or closed. For closed gates, the associated (s, a)
Figure 4: The black arrow is not α-significant for one of the tasks, Figure 5: Using optimistic imag-
but is nevertheless “covered” by optimistic imagination.	ination for exit detection.
4
Under review as a conference paper at ICLR 2022
Figure 4 illustrates how the proposed condition fails in covering an exit marked with a black arrow.
While (s,a) is unused by optimal policies in the first task, it is V0* (s0)-important for the second. 」
Optimistic Imagination as a Coverage Mechanism. The proposed assumption fails because
there are cases where exits are only α-important in certain configurations (e.g., only open corridors
are α-important in Example 3.1). In such cases, near-optimal policies only ever see one configura-
tion of the dynamics for those exits and thus are insufficient for formalizing exit coverage.
As an alternative, consider the following hypothetical scenario in the context of Figure 4: an agent
has solved both tasks, achieving optimal values V* and V*. Additionally, in the process of learning
the second task, the agent has learned P2 (∙ | 4).If the agent then relearns the first task while setting
Pι(∙ | V) J P2(∙ | V), it would obtain a new value V》V*. Thus, it can reasonably conclude
that the black arrow must have been an exit. We illustrate this process in Figure 5, where πI is the
optimal policy after “borrowing dynamics.” The learner could then run πI for exit detection.
We refer to the counterfactual reasoning about the dynamics used above as optimistic imagination.
Unlike the preliminary condition, optimistic imagination only requires that an exit be important for
one task and induce a value gap in another when borrowing dynamics - a weaker condition in many
cases. With the above intuition in mind, we now present the main coverage assumption.
Assumption 4.1 ((α, ζ)-coverage). Assume (Mt)t∈[T] have a latent hierarchy with respect to
({Zk } , Ent(∙), Ext(∙)). There exists α,Z > 0 such thatforany {(s1,a1),..., (Sn, a。)} ⊆ Ext(S),
(a)	For any i ∈ [n], (si, ai) is α-important for some meta-training MDP Mi.
(b)	For some Mt with t ∈ [T], ifwe construct a new MDP Mt = (S, A, P, rt, H) via
P(. | Sa)= /PMi (∙ 1 S,a) (S,a) = (Si,ai)
,	[pMt(∙ | s,α) otherwise ,
i	.e. we replace (si, ai) dynamics with thosefrom Mi, then VMt,*(s0) > VMt,*(so) + Z.
Informally, (α, ζ)-coverage ensures that optimistic imagination can borrow dynamics for unknown
exits from other tasks to find a better optimal policy.2 Most reasonable task distributions satisfy
(α, Z)-coverage with enough tasks - We provide a heuristic explanation in the appendix.
4.2	Algorithm Outline
In this section, we outline the algorithm that we use to detect exits. Our procedure can be naturally
divided into three phases: a task-solving phase, a reward-free phase, and an exit detection phase.
Throughout, we illustrate our steps in Figure 5, showing how Phases I and II allow the learner to
find the imagined policy πI . The full details of the algorithm are provided in Section A.1.
4.2.1	Phase I: Task-Specific Dynamics Learning
First, we solve M1, . . . , MT with UCBVI. Using UCBVI regret bounds
from Azar et al. (2017) together with Lemma 4.1, we can guarantee that
all α-important exits are sufficiently visited. Thus, during optimistic
imagination, the learner would be able to borrow high-quality estimates
of exit dynamics from other tasks. For example, a learner that has solved
both tasks in Figure 4 can borrow open blue gate dynamics for use in the
first task during optimistic imagination, as in Figure 6.
4.2.2	Phase II: Reward-Free RL
Figure 6: Phase I con-
tribution to learning πI
in Figure 5, marked
with an arrow.
In order to perform optimistic imagination, the learner also needs to sim-
ulate non-borrowed (s, a) dynamics. This is done by fully learning the
dynamics of one
of the tasks, proving a template P0 . Learning P0 is
achieved using reward-free RL (Jin et al., 2020).
2Incorporating the preliminary condition into Assumption 4.1 can further weaken the required task diversity.
However, our algorithm handles this extension trivially, and thus we focus on optimistic imagination.
5
Under review as a conference paper at ICLR 2022
To understand the necessity of Phase II, note that in Figure 7, near-
optimal policies have no coverage over states past the blue gate (as
shown by the red region). Therefore, dynamics estimates from Phase
I are insufficient for optimistic imagination. On the other hand, by learn-
ing the dynamics fully in one of the tasks (and thus learning the green
region), the learner can nevertheless simulate the dynamics if the blue
gate were open and successfully recover πI .
4.2.3	Phase III: Exit Detection
Figure 7: Phase II con-
tribution. Optimal pol-
icy state coverage (red)
is insufficient for learn-
ing πI , which requires
the green region.
Having completed the previous two phases, the learner can use optimistic
imagination to detect exits. In particular, we consider a modified value
iteration method where the learner optimistically chooses dynamics esti-
mates from Phases I and II to perform Bellman backups. This implicitly
defines an MDP whose optimal value is at least as large as M t in As-
sumption 4.1, as Mt would have been feasible for this process. Analo-
gously with α-importance, this value gap implies that the corresponding
optimal policy πI for this new MDP must visit an (s, a) pair whose dynamics are borrowed. There-
fore, by playing πI , the learner can determine a new exit.
4.3	Meta-Training Guarantee
We now outline our main result for the algorithm in Section 4.2. We first define a “hierarchy oracle”
that will be used in downstream tasks:
Definition 4.3 (Hierarchy oracle). Let ㊀S and ㊀F denote successful and failed termination, respec-
tively. Consider any tuple (x,f,r,H) such that X ∈ Ent(S), f : Ext(S) → {㊀S,㊀F }, r is areward
function, and H ≤ H. Such tuple induces an MDP M(x,f, r, H) = (S ∪ {㊀S,㊀F} , A, Pf ,r, H)
whose starting state is x and whose transition dynamics is given by
(δ(f (s,a))	(s,a) ∈ Ext(S)
Pf(∙∣ s,a)=	δ(s)	S ∈{㊀S,㊀F}
1Pt(∙ | s, a) otherwise, for any t ∈ [T].
An ε-Suboptimal hierarchy oracle, when queried with any valid (x, f, r, Hi), returns an ε-suboptimal
policy for M(x, f, r, H).
To motivate Definition 4.3, observe that learning a new MDP with the same latent hierarchy only
requires visiting exits (as reward-free learning has coverage over cluster interiors). This, in turn, can
be achieved by an agent that has a policy for reaching any exit from every entrance in any cluster.
We emphasize that the learner does not need to know the states in the actual clusters themselves.
Formally, for an entrance x in cluster Z and exit e ∈ Ext(Z), we can query the hierarchy oracle with
(x, 1 [(∙, ∙) = e], 1 [∙=㊀S], H) to obtain the desired reaching policy. Disconnecting the clusters
ensures that the learner cannot use any other exits, and thus the validity of the policy under any MDP
that has the same latent hierarchy.3
Our meta-training guarantee ensures that the hierarchy oracle is implementable:
Theorem 4.1	(Meta-training guarantee, informal). Under Assumption 4.1 and other assumptions in
Section A.2, the data obtained from the algorithm in Section 4.2 allows for:
(a)	implementing an ε-suboptimal hierarchy oracle, and
(b)	determining, for every s ∈ Ent(S), the reachable exits in the cluster containing s,
simultaneously with probability at least 1 - p. Furthermore, this is achieved with query complexity
C Γ / KL KS SA KS2A)	S4A	S2A	]
∖mmin(Z, β)2 + αZ2 + min(α, ζ)2 + α ) + min(ε, ζ) + min(ε, Z)2 Poy .
3As a side effect, the reachability of e from x can be determined from the predicted value for the prior query.
6
Under review as a conference paper at ICLR 2022
As a point of comparison, we have the following guarantee on brute-force hierarchy learning:
Theorem 4.2.	The brute-force approach outlined in Section A.5, under Assumption 4.1(a), deter-
mines the set of exits with high probability and query complexity
O
poly(H).
When α, β, and ζ are of the same order, we see that the proposed method incurs a smaller query
complexity compared to a brute force learner that has only learned the exits. We provide proofs of
both results in Section A, along with all other necessary assumptions and full algorithm details.
5	Meta-Test Analysis
In this section, we provide regret bounds on learning an MDP MTg using the hierarchy oracle. We
first characterize a family of tasks for which one can achieve improved regret bounds. Furthermore,
we provide sufficient conditions ensuring that using the hierarchy incurs low suboptimality.
5.1	Assumptions
In this section, we outline the assumptions that we make to prove a regret bound on the meta-test
task. Let MTg = (S, A, Pτg,rτg,H) be the meta-test MDP. We assume that PTg(∙ | s,a)=
Pt(∙ | s, a) for any (s, a) ∈ Ext(S) and t ∈ [T]. Our first assumption restricts our attention to
meta-test tasks which are compatible with the hierarchical structure:
Assumption 5.1 (Task Compatibility). There exists a cluster Z* such that rτg is supported on
(Z * )◦ ∪ Ext(S). Furthermore, there exists an optimal policy π* satisfying
(a)	Conditioned on sh ∈ Z*, we have that (sh0, ah0) 6∈ Ext(Z *) for h0 ≥ h almost surely.
(b)	The number of exits encountered by π* is bounded by Heff with probability ζ.
Hierarchical compatibility. Intuitively, Assumption 5.1 suggests that the task can be decom-
posed into a (Z*)-searching phase and a within-(Z*) phase. This decomposition captures the goal-
conditioned RL setting, which has been studied extensively in recent empirical works (Nachum
et al., 2019a; Levy et al., 2018; Nachum et al., 2018). We expect the hierarchy oracle to reduce
the complexity of exploration in both phases. Thus, these conditions ensure compatibility with the
learned hierarchy.
Temporal Abstraction. Since a hierarchical learner makes decisions upon entering a new cluster
(to decide which exit to use/whether to stay), hierarchical planning can reduce the planning horizon.
Condition (b) quantifies this reduction. Note that in most practical settings, a learner that has access
to good sub-skills can achieve low failure probability ζ even with modest values of Heff .
Hierarchical Suboptimality. Restricting the learner to executing hierarchical policies can greatly
reduce the search space. While this reduction leads to improved regret bounds, this also incurs
approximation error, as the optimal policy may not lie in this restricted class. We refer to this error
as hierarchical suboptimality. We will show that hierarchical suboptimality is controllable with
appropriate conditions on PT, which require the following notions of reaching times:
Definition 5.1 (Reaching times). Fix a cluster Z, starting and goal states s, g ∈ Z, and planning
horizon H ≤ H. For any policy π, let (so, si,..., SH) be the states visited by π from s, where
s0 = s. Then, we define
TH (s, g) ：= min {h ∈ {o,...,H}卜 h = g and sh，∈ Z for h0 < h} ∪ {L}
TH(s,g) ：= inf E [TH(s,g)]
T min(s, g) := inf min {h ∈ N | P (Tπ(s, g) = h) > 0} .
π

7
Under review as a conference paper at ICLR 2022
In words, TH(s,g) is the time ∏ takes to reach a state g from S while remaining within the same
cluster. By minimizing this quantity in expectation over all policies, We obtain TH(s, g). Finally,
Tmin(s, g) is the minimum time for which it is possible to reach g from s.
Definition 5.2 (Regular and low-variance dynamics). There exists α, β, γ > 0 such that for any
cluster Z, states s,g ∈ Z, and horizon H ‹ H,
(a)	((α, β)-unreliability) For any deterministic policy π with E[TH(s,g)] 一 TH(s,g) < α,
TH(s, g) has a sub-Gaussian upper tail with variance proxy β2E[TH (s, g)]2.
(b)	(γ-goal-reaching suboptimality) TH(s,g) ≤ (1 + Y)Tmin(s,g).	◊
To understand (α, β)-unreliability, note that the condition only considers near-optimal deterministic
policies. Therefore, (a) quantifies the randomness in TH(s, g) derived from the transition dynamics.
On the other hand, γ-goal-reaching suboptimality quantifies the regularity of the dynamics, measur-
ing whether near-optimal goal-reaching policies nearly achieve the optimal expected reaching time.
Deterministic environments satisfies these conditions with α = ∞ and β = γ = 0. We provide an
extended discussion of why these quantities control hierarchical suboptimality in Section B.5. Note
that the guarantees of Definition 5.2 scales with the cluster width, and thus we have the following
final assumption:
Assumption 5.2. For any cluster Z and s, g ∈ Z with s 6= g, Tmin(s, g) ≤ W. Furthermore,
Heff W	H.
Assumption 5.2 limits the length of the subtasks within each cluster. This is consistent with
hierarchy-based methods in practice, with skills only being executed for a limited amount of time.
This width bound, together with Assumption 5.1, suggests that ∏* requires O(HeffW) timesteps
with high probability. Therefore, the condition HeffW H means that the task horizon is much
longer than the minimum time required to complete the task, which often holds in practice.
5.2 Meta-test Regret Guarantee
We note that Assumption 5.1 implies that the hierarchy oracle induces a “high-level” MDP. We have
the following regret bound on applying Euler to this MDP:
Theorem 5.1. We work under Assumptions 5.1 and 5.2. Furthermore, assume that the learner has
access to an ε-suboptimal hierarchy oracle as guaranteed by Theorem 4.1, where ε < α. Then, a
learner that applies the procedure in Section B.2 to MTg incurs regret
Regret(N) . pH2HffWLMN + Nεsub°pt
εsubopt = (1 + Heff + βpHeff)ε + [γHeff + β(1+ Y)PHeff W + ZH.
with high probability.
Observe that the irreducible hierarchical suboptimality εsubopt (i.e.
when ε = 0) tends to zero as Y, β, ζ → 0. In particular, environ-
ments with deterministic in-cluster dynamics do not incur hierar-
chical suboptimality. We prove this regret bound in Section B.
When does knowing the hierarchy help? Consider the binary
tree environment in Figure 8. All of the leaves take the learner to a
state with exits with probability 1/2, with the exception ofa special
leaf '* that does so with probability (1/2)+ ε. Rewards can only be
collected upon performing one of the exits (blue/purple). To achieve
low regret, a learner has to quickly identity '* and the correct exit.
We consider the set of task distributions indexed by ' that ran-
domize the reward-granting exit. Knowing the hierarchy amounts
to knowing '*, reducing the exploration problem to determining the
correct exit. However, a hierarchy-oblivious learner needs to explore the tree, leading to regret that
is exponential in the tree depth. Formally, we have the following result:
ZI
Z2
Z3
Figure 8: The hard instance in
Theorem 5.2.
8
Under review as a conference paper at ICLR 2022
Theorem 5.2. There exists a family of task distributions such that any hierarchy-oblivious learner
incurs expected regret lower bounded by Ω(2W/2ʌ/H2N) on at least one task distribution. In con-
trast, a learner with access to a 0-suboptimal4 hierarchy oracle incurs regret bounded by O( VH2N)
with high probability, over any sampled task from any of the task distributions.
We prove this result in Section B.4.3, using recent results by Domingues et al. (2021) which demon-
strate that the set of binary tree subproblems above form a set of minimax instances for any RL
algorithm. This separation result suggests that hierarchy-based learners gain in situations where
temporally extended exploratory behaviors are needed. This corroborates the experimental findings
of Nachum et al. (2019b), which attributes the benefits of hierarchical RL to improved exploration.
6	Related Work
Hierarchical reinforcement learning has been studied extensively (Sutton et al., 1999; Parr & Russell,
1998; Dietterich et al., 1998; Vezhnevets et al., 2017). An early approach formalizing the use of
hierarchies in RL is the options framework (Sutton et al., 1999), which fixes a finite set of available
skills/options. Since then, a large body of work has focused on designing methods for learning and
adapting these options during the learning process (McGovern & Barto, 2001; Menache et al., 2002;
SimSek & Barto, 2004; Mann et al., 2014). Of particular note is the work of Frans et al. (2018),
which learns a finite set of neural network sub-skills in the meta-RL setting. On the other hand,
Laplacian-based option discovery in Machado et al. (2017; 2018) defines options using proto-value
functions (Mahadevan, 2005), which captures global features of the state space. In more theoretical
directions, Fruit & Lazaric (2017); Brunskill & Li (2014) provide regret and sample complexity
bounds, respectively, for learning with options. Additionally, Mann & Mannor (2014) demonstrate
that options can improve the convergence rate of approximate value iteration.
More recent empirical work has considered the problem of hierarchy learning beyond the options
framework in a wide variety of settings. Nachum et al. (2019a); Levy et al. (2018) provide algo-
rithms for learning hierarchies based on goal-conditioned policies, reducing the learning problem to
choosing subgoals. Nachum et al. (2018) considers a more general case when learned representa-
tions are used to map states to goals. Other works such as Co-Reyes et al. (2018); Eysenbach et al.
(2018); Sharma et al. (2019) provide intrinsic objectives for learning hierarchies without rewards.
Closely related is the work of Wen et al. (2020), which introduces a similar latent structure on the
state space, and provides sufficient conditions ensuring sample-efficient and tractable hierarchy-
based learning. However, their works hinges on prior knowledge of the latent structure, while a
major focus of our work is discovering the structure itself from interactions.
Several heuristics have been proposed in prior work for the detection of useful bottlenecks in MDPs
(McGovern & Barto, 2001; Menache et al., 2002; SS imsSek & Barto, 2004; SS imsSek et al., 2005). In
particular, SS imsSek & Barto (2004) define the notion of access states, states which maximize short-
term novelty of future states. Although exits as defined in Definition 3.1 meet this heuristic, we
note that cluster interiors may also contain bottlenecks also meet these conditions. In contrast, our
algorithm would not detect these bottlenecks; however, this behavior is desirable as such bottlenecks
are unimportant for meta-learning.
7	Conclusion
We have demonstrated that certain natural coverage conditions allow for learning useful hierarchies
from tasks. Interesting future directions include analyzing hierarchy-based multi-task RL and ex-
tending the ideas in this work to continuous state and/or action spaces. Another interesting direction
would be to provide sample-efficient algorithms for learning additional structures that can be im-
posed on the learned hierarchy, such as cluster equivalences as in Wen et al. (2020).
4We use a 0-suboptimal hierarchy oracle for the separation result for ease of presentation.
9
Under review as a conference paper at ICLR 2022
References
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272. PMLR, 2017.
Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning.
In International conference on machine learning, pp. 316-324. PMLR, 2014.
John Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey
Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajec-
tory embeddings. In International Conference on Machine Learning, pp. 1009-1018. PMLR,
2018.
Thomas G Dietterich et al. The maxq method for hierarchical reinforcement learning. In ICML,
volume 98, pp. 118-126. Citeseer, 1998.
Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic rein-
forcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learning
Theory, pp. 578-598. PMLR, 2021.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then
explore. Nature, 590(7847):580-586, 2021.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. In International Conference on Learning Representa-
tions, 2018.
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. The information geometry of
unsupervised reinforcement learning. arXiv preprint arXiv:2110.02719, 2021.
Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared
hierarchies. In International Conference on Learning Representations, 2018.
Ronan Fruit and Alessandro Lazaric. Exploration-exploitation in mdps with options. In Artificial
Intelligence and Statistics, pp. 576-584. PMLR, 2017.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration
for reinforcement learning. In International Conference on Machine Learning, pp. 4870-4879.
PMLR, 2020.
Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierarchies
with hindsight. In International Conference on Learning Representations, 2018.
Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for op-
tion discovery in reinforcement learning. In International Conference on Machine Learning, pp.
2295-2304. PMLR, 2017.
Marlos C Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray
Campbell. Eigenoption discovery through the deep successor representation. In International
Conference on Learning Representations, 2018.
Sridhar Mahadevan. Proto-value functions: Developmental reinforcement learning. In Proceedings
of the 22nd international conference on Machine learning, pp. 553-560, 2005.
Timothy Mann and Shie Mannor. Scaling up approximate value iteration with options: Better poli-
cies with fewer iterations. In International conference on machine learning, pp. 127-135. PMLR,
2014.
Timothy Mann, Daniel Mankowitz, and Shie Mannor. Time-regularized interrupting options (trio).
In International Conference on Machine Learning, pp. 1350-1358. PMLR, 2014.
10
Under review as a conference paper at ICLR 2022
Amy McGovern and Andrew G Barto. Automatic discovery of subgoals in reinforcement learning
using diverse density. In Proceedings of the Eighteenth International Conference on Machine
Learning,pp. 361-368, 2001.
Ishai Menache, Shie Mannor, and Nahum Shimkin. Q-cut—dynamic discovery of sub-goals in
reinforcement learning. In European Conference on Machine Learning, pp. 295-306. Springer,
2002.
O Nachum, S Gu, H Lee, and S Levine. Data-efficient hierarchical reinforcement learning. In 32nd
Conference on Neural Information Processing Systems (NeurIPS 2018), pp. 3303-3313. Curran
Associates, Inc., 2019a.
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation learning
for hierarchical reinforcement learning. In International Conference on Learning Representa-
tions, 2018.
Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, and Sergey Levine. Why does
hierarchy (sometimes) work so well in reinforcement learning? arXiv preprint arXiv:1909.10618,
2019b.
Ronald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in
neural information processing systems, pp. 1043-1049, 1998.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015.
Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. In International Conference on Learning Representations, 2019.
Ozgur Simsek and Andrew G Barto. Using relative novelty to identify useful temporal abstractions
in reinforcement learning. In Proceedings of the twenty-first international conference on Machine
learning, pp. 95, 2004.
Ozgur Simyek, Alicia P Wolfe, and Andrew G Barto. Identifying useful subgoals in reinforcement
learning by local graph partitioning. In Proceedings of the 22nd international conference on
Machine learning, pp. 816-823, 2005.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-
211, 1999.
Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance
of task diversity. Advances in Neural Information Processing Systems, 33, 2020.
Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In
International Conference on Machine Learning, pp. 3540-3549. PMLR, 2017.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Jane X Wang, Michael King, Nicolas Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie Deck, Peter
Choy, Mary Cassin, Malcolm Reynolds, Francis Song, et al. Alchemy: A structured task distri-
bution for meta-reinforcement learning. arXiv preprint arXiv:2102.02926, 2021.
Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Barreto, Benjamin Van Roy, and Satinder
Singh. On efficiency in hierarchical reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020.
11
Under review as a conference paper at ICLR 2022
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning,pp. 7304-7312. PMLR, 2019.
12
Under review as a conference paper at ICLR 2022
A Meta-Training Proofs
A.1 Algorithm
In this section, we provide the complete algorithm for exit detection with optimistic imagination.
For readability, we separate the three phases.
A.1.1 Phase I: Task-Specific Learning
Algorithm 1 Exit Detection, Phase I: Task-Specific Learning
Require: Tasks M1, . . . , MT, NUCBVI UCBVI iterations, NTS policy samples, threshold NtThSresh
1:	for all t ∈ [T] do
2:	Dt — 0.
3:	Obtain policy set Φt — UCBVI(Mt, Nucbvi).
4:	for all n = 1, . . . , NTS do
5:	Sample π 〜Unif(Φt).
6:	Play ∏ in Mt, add all (s, a, s0) pairs to Dt, get sum of rewards V(n).
7:	Form estimated dynamics model Pt from Dt.
1 NTS
8:	Form optimal value estimate E — —— E V(n)
TS n=1
9:	Nt(s,a) - ∣{(x,u,x0) ∈ Dt | X = s,u = a}|.
10:	for all (s, a) ∈ S × A do
11:	if Nt(s,a) < NTSesh then Pt( I s,a) - 0.
12:	return dynamics estimates Pt and value estimates Vt for t ∈ [T].
A.1.2 Phase II: Learning Reference Dynamics
Algorithm 2 Exit Detection, Phase II: Learning Reference Dynamics
Require: MDP M, NERUFLER Euler iterations, NRF policy samples
1:	Set policy class Ψ - 0 and dataset DRF - 0
2:	for all g ∈ S do
3:	Create MDP Mg from M with horizon 2H and P (㊀ ∣ g,a) = 1 for any a.
4:	rg(s, a) — 1 [s = g] for any (s, a) ∈ (S ∪ {㊀}) X A.
5:	Φg - EULER(Mg , rg , NREFULER)
6:	∏h(∙ ∣ g) — Unif(A) for ∏ ∈ Φg, h ∈ [H].
7:	Add policies in Φg to Ψ.
8:	for all n = 1, . . . , NRF do
9:	Sample ∏ ~ Unif (Ψ).
10:	Play π in M and obtain trajectory (s0, a0, . . . , s2H).
11:	Sample h ~ Unif ([2H]) and add (s%, a%, sh+ι) to Drf.
12:	return reference dynamics P° formed from Drf.
13
Under review as a conference paper at ICLR 2022
A.1.3 Phase III: Exit Detection
Algorithm 3 Exit Detection, Phase III: Exit Detection
Require: NED, NtEhDresh, NEEULLER, NEL policy samples
1:	Initialize ISEXIT[s, a] J FALSE for (s, a) ∈ S × A.
2:	while True do
3:	for all t ∈ [T] do
4:	Po(∙ | s,a) J Pt(∙ | s, a) for (s, a) ∈ S × A With ISEXIT[s, a].
~ ɪ ʌ , ʌ ʌ
5:	Vt, Qt J OPTIMGVI(Po, (Pi,..., Pt ), r, IsEXIT)
6:	if Vt(SO)- Vt > (2∕3)Z then
7:	Run greedy policy With respect to Q NED times and form estimate P for (s, a) pairs
visited at least Nt0hresh times.
8:	for all (s, a) ∈ S × A with P(∙ | s, a) ≡ 0 do
9:	if (∃t ∈ [T]) Pt(∙ | s,a) ≡ 0 and ∣∣P(∙ | s, a) 一 Pt(∙ | s, a)∣∣	> β∕2 then
10:	ISEXIT[s, a] J True
11:	Pt(∙ | s,a) J LEARN-EXIT(Mt, (s,a), NEkER,Nel) for all t ∈ [T]
12:	if no new exits found after passing through T tasks since last found exit then
13:	return ISEXIT
Algorithm 4 Borrowing Optimistically Across Tasks during Value Iteration (BOAT-VI)
1:	procedure BOAT-VI(Reference dynamics P0, Estimated dynamics (P1, . . . , PT),
Reward function r, Table ISEXIT[S × A])
2:	VH(s) J 0 for S ∈ S.
3:	for all h = H - 1, . . . , 0 do
4:	for all (s, a) ∈ S × A do
5:	if ISEXIT[s, a] then
6:	Qh(s, a) J r(s, a) + P0Vh+1(s, a)
7:	else
C	K∕∖.∕∖,	TpħτV∕∖
8:	Qh(s, a) J r(s, a) + maxt=0,...,T PtVh+1(s, a)
9:	Vh(s) J maxa Qh(s, a) for s ∈ S.
10:	return V, Q
Algorithm 5 Exit-learning subroutine
1: procedure LEARN-EXIT(MDP M, exit (s, a), NEEULLER EULER iterations, NEL policy samples)
2:	Create MDP M from M so P(㊀ | s, a) = 1.
3:	r(s0, a0) J 1 [(s0, a0) = (s, a)] for any (s0, a0) ∈ (S ∪ {㊀}) X A.
4:	Ψ J EULER(M√F,NELLer)
5:	for all n = 1, . . . , NEL do
6:	Sample π 〜Unif(Ψ).
7:	Play π in M and obtain trajectory (s0, a0, . . . , sH).
8:	return reference dynamics P(∙ | s, a) formed from all trajectory data.
14
Under review as a conference paper at ICLR 2022
A.2 Other Assumptions and Relevant Definitions
The remaining assumptions quantify the reachability of certain states. First, we have the following
assumption, which in effect ensures that one can reach most states regardless of exit configuration
in the meta-training tasks:
Assumption A.1 (Non-limiting exit configurations). Let M = (S, A, P, H) be any reward-free
environment with time-varying dynamics
P(h)(∙ | s,a) = Pt(h,s,a)(∙ | s,a) for some t ：[H ] × S × A→ [T ].
Then, there exists C > 1 such that for any s and t ∈ [T],
max PM (s ∈ τπ) ≤ C max PMt (s ∈ τπ)
ππ
Intuitively, the assumption states that the reachability of a state in Mt would not be significantly
improved even under an optimal configuration of the exits. Therefore, running reward-free RL on
one of the meta-training tasks is sufficient for learning all non-exit (s, a) pairs.
Remark A.1. We note that Assumption A.1 is restrictive in that it requires that every state be
roughly reachable in any of the meta-training MDPs. This may not hold in practice, e.g., consider a
four-room environment where one of the rooms is blocked off for one of the tasks. However, this can
be weakened to requiring that s be reachable in at least one of N arbitrarily chosen meta-training
tasks. This would require that the algorithm run Phase II over N meta-training tasks, which results
in a benign increase in the query complexity of the algorithm, so long as N is a constant much
smaller than T. We focus on the N = 1 case for ease of presentation.	y
To simplify the presentation of the rest of the assumptions, we recall the following definition of
δ-significance in Jin et al. (2020):
Definition A.1. A state s is δ-significant if maxπ P (s ∈ τπ) ≥ δ. Additionally, we say that (s, a)
is δ-significant if s is δ-significant.
Note that we have modified the definition to remove the dependence on the timestep h ∈ [H]. This
is because the dynamics are stationary, and thus it does not matter when s is visited in a trajectory.
Note that Assumption A.1 implies that a reachable entrance in one task is reachable in all tasks
(i.e., is significant in the sense of Definition A.1). Therefore, we can quantify the minimum level of
significance:5
Definition A.2 (ρ-significant entrances). For any s ∈ Ent(S), s is ρ-significant for any task.
Finally, we want to quantify the reachability of every exit from any entrance in the same cluster,
assuming that it is indeed reachable:
Definition A.3 (In-cluster exit reachability). Fix any cluster Z, entry s ∈ Ent(Z), and exit
(g, a) ∈ Ext(Z). Consider the reward-free environment Mt|Z = (Z, A, Pt|Z, H), where Pt|Z
is the restriction of Pt to Z × A and the starting state is s. Then, if g has nonzero significance in
Mt|Z for any t ∈ [T], then it is ε0-significant.
The requirement that the assumption hold for any t ∈ [T] is without loss of generality since non-exit
dynamics do not change.
A.3 Verifying Exit Detection
In this section, we demonstrate that the algorithm in Section A.1 can successfully discover Ext(S)
with high probability. Formally, we have the following result:
Theorem A.1 (Provable exit detection). Assume we run the algorithm in Section A.1 with the pa-
rameter choices given in Table 1. Then, with probability at least 1 - p, the algorithm returns an
array ISEXIT satisfying:
{(s, a) | ISEXIT[s, a]} = Ext(S).
To prove this result, we proceed with a phase-by-phase analysis of the algorithm in Section A.1,
which we then compile into proof of the desired result.
5One can weaken this assumption in a way that is compatible with the weakened form of Assumption A.1.
15
Under review as a conference paper at ICLR 2022
Parameter	Value
NUCBVI	H2SA	2 HSAT min(α,Z)2 0g P
NthSesh	H4 1	SAHT max (R ,β2) log Pa min(β,Z)
NTS	H5 H	SAHT	H2	SAT S max (θZ2, aβ2 J log Pa min(β,ζ) + min(a,Z)2 log 丁
NRLER	H2S4A	3 HSA min(ρmin(ε,εo),Z∕C) og P
Nrf		H 5S 2A	 A min(ρmin(ε,ε0)2,Z2∕C) og P
NtEDesh	S	SAH 炉 log K
NED	HKS SAH H2K2	K 飞声log K + T log万
NtELesh	H4 1	CSAHT max (T2 E log Pa min(β,ζ)
NEULER	CH3S2A]οg3 ( HSAT ) a	∖ P J
NEL	CH5 CH	CSAHT	C2H2	SAT max V^a<r, aβ2) og Pa min(β,ζ) + a2 og -P-
Table 1: Table of parameters for the results in Theorem A.1. Since K ≤ SA and L ≤ S, the agent
does not need to know K or L in advance, at the expense of a worse sample complexity bound.
A.3.1 Phase I Analysis
First, we prove that during Phase I, Algorithm 1 sufficiently visits all relevant exits and that all value
estimates are sufficiently close. Formally, we have the following result:
Proposition A.1. Set
NTSesh = Ω 卜 max (H 指)log SAHTNTS
and consider the following procedure applied to one of the meta-training tasks Mt:
1. UCBVI is run for
NUCBVI = Ω
H2SA	2 HSAT
min(α,Z)2 og	P
iterations, generating policies π1(t) , . . . , πN(t)
16
Under review as a conference paper at ICLR 2022
2.	The learner uniformly samples
H TS	H2	TK
NTS = ω taNthresh + min(α,ζ)2 log V
policies from the previous step, runs each policy in Mt, and obtains a dataset of transitions
Dt and returns V⑴，...，V(NTS),
Then, with probability at least 1 - p/3T,
(a)	We have the regret bound
唱(s0)-
1
NUCBVI
NUCBVI
X V0πk(t) (s0)
k=1
<6.
(b)	The set of obtained returns satisfy
NTS
ɪ X V(i)-
NTS J
i=1
NUCBVI
N X Vοπk (so)
k=1
<6.
(c)	If (s, a) is α-important for Mt, then Nt(s, a) ≥ Nthresh.
(d)	For every (s, a) pair such that Nt(s, a) ≥ Nthresh,
sup
f = S→[0,H]
Ih(Pt - Pt)fi (s,a)∣
< min
ζ ζ βH∖
124H, 2 ).
To prove the above result, we first recall the following regret bound on UCBVI, as proven by Azar
et al. (2017):
Lemma A.1 (UCB VI regret bound). For sufficiently large N, with probability at least 1 - p/6,
…ʌ 1 Λτz∏fcz ʌ “ rH2SA 1	HSAAN∖
吟(Se- Ν∑v>πk (so) . V -ɪ log (-r- )-
As we will see later on, with our choice of NUCBVI , we obtain the desired regret bound in (a).
Additionally, by Hoeffding’s inequality, the average of NTS returns concentrates around the desired
quantity with high probability, proving (b). Thus, all that remains is ensuring that every α-important
exit is sufficiently visited, and thus their dynamics are sufficiently well-estimated.
Recall from the main text that the key step is demonstrating that a near-optimal policy for a task
must visit its α-important states with non-negligible probability:
Lemma A.2. Let (s, a) be α-important for M, and letπ be an ε-suboptimal policy for ε <α. Then,
P((s,a) ∈ τ∏) > H(α - ε).
Proof. By α-importance,
α ≤ VM,*(sο) - ⅛M∖(s,a),*(sο) ≤ h⅛M,*(so) - VM,π(so)] + h⅛M,π(so) — 愣""),*(s0)]
≤ h⅛M,π(so) — ⅛M∖(s,a),*(so)i + ε.
Therefore, by applying Lemma A.17 and noting that {∆ ∩ τ∏ = 0} = {(s,a) ∈ τ∏}, we obtain the
desired result.	□
Through the prior result, we can relate the UCBVI regret bound to the probability that a randomly
chosen UCB VI-generated policy visits an α-important state:
17
Under review as a conference paper at ICLR 2022
Lemma A.3. Let (s, a) be α-important for M. Assume that UCB VI, when run for
H2SA	2 HSA
NUCBVI = ω ( ----2— log -----)
α2	p
iterations, generates policies π1, . . . , πN. If we sample π uniformly from these policies and let τ be
the (random) trajectory generated by this randomly selected policy, then
P((s,a) ∈ T) > 券
2H
conditioned on the high probability event in Lemma A.1.
Proof. By Lemma A.2, for any fixed π, we can write
P((s, a) ∈ τ∏) ≥ ɪ(ɑ - [V0*(s0) - V0π(S0)])+,
H
where x+ = xl [x > 0]. Then, since ∏ is chosen randomly from the policies generated by UCBVI,
1N	1 N
P((s,a) ∈ τ) = N 与P((s,a) ∈ τ∏k) ≥ HN ∑(α - V*(s。)- V∏k(s。)])+
1
≥ —
≥ H
1N
α - N £ v0" (S0) - Vnk (So).
k=1
Therefore, by applying the regret bound in Lemma A.1 and the choice of N, we find that
P((s,a) ∈ τ) > 券
2H
With all of the above intermediate results, we can now prove Proposition A.1.
□
Proof of Proposition A.1. Throughout this proof, we condition on the high-probability event in
Lemma A.1, instantiated to occur with probability at least p/12T.
(a)	By the choice of NUCBVI,
H2SA 2 HSAT
NUCBVI & -72- log ---------,
ζ2	p
and thus, we obtain the desired bound by plugging this value into the regret bound provided
by Lemma A.1.
(b)	Note that (V(i)) are i.i.d., bounded in [0, H], and for any i ∈ [Nτs],
E 卜叫=N XX V0πkt) (so).
k=1
Therefore, by applying Hoeffding’s inequality, with probability at least 1 - p/12T ,
NTS
X V⑺-
i=1
1
NUCBVI
NUCBVI
X	V0πk(t) (s0)
k=1
The result immediately follows from the fact that NTS & (H 2∕Z 2)log(T∕p).
(c)	The result simply follows from Lemma A.16 instantiated with failure probability 1 -
p∕12T, together with the choice of NtThSresh.
(d)	With the choice of NUCBVI, the conclusion of Lemma A.1 can be made to hold with proba-
bility at least 1 - p∕24T. Fix an α-important exit (s, a) for Mt, so that the probability that
(s, a) is visited by the procedure is at leastα∕2H. By Lemma A.15, sampling NTS trajec-
tories is sufficient to ensure that Nt(s, a) ≥ NtThSresh with probability at least 1 -p∕24TK.
Therefore, by performing a union bound over the set of α-important exits (which contains
at most K elements), Nt(s, a) ≥ NtThSresh for any α-important exit with probability at least
1 - p∕24T. Thus, overall, this event occurs with probability at least 1 - p∕12T.
Since each part fails with probability at most p∕12T, the overall failure probability is at most p∕3T,
the desired result.	□
18
Under review as a conference paper at ICLR 2022
A.3.2 Phase II Analysis
In this section, we provide guarantees on the dataset DRF obtained by performing reward-free RL
in Algorithm 2. Formally, we have the following high-probability result:
Proposition A.2. For any δ > 0 and failure probability p, if Algorithm 2 is run with parameters
F	H2S2A	3 HSA
NRULER = O(^^ log3 丁)
Nrf = O max CC2, ——τ-1---------G) H5S2Alog A
ζ2 ρ min(ε, ε0)2	p
Then, with probability at least 1 - p/3:
(a)	The distribution μ generating each Sample in DRF satisfies
S ∈ S is δ-significant in Mι(2H) =⇒ max P((S, a) ∈ τ^π) ≤ 4SAH.
a,π	μ(s, a)
(b)	The estimated dynamics model Po satisfies
max max
f ：S→[0,H] ν：S→A
Ih(P-P)fi(s,a)∣ i[a =V(S)]
Z2	P min(ε,εo)2 ʌ	1
4 ∙ 242C,	16 H H3SA.
The details of the proof of Proposition A.2 follow that of Jin et al. (2020), which we provide here
for completeness. First, we adapt the regret bound from Zanette & Brunskill (2019) for any MDP
and reward function used in Algorithm 2.
Lemma A.4. For any g ∈ S, running EULER in Mg for N iterations returns N policies π1, . . ., πN
satisfying the regret bound
V0*(S0)- ⅛ XX qk (so) . S4V0*(s0)SA log SAHN + S2AH2 log3 SAHN
N	Np	N	p
k=1
with probability at least 1 - p.
Proof. Observe that
1N
NH X
k=1
r(Sh, ah) - V0πk(S0)
S0
r(Sh, ah)! + (Voπk(So))2
So
2N
≤ NH X Enk
k=1
2 N	H-1	∣
≤ NH EEnk Er(Sh,ah) + Vrk (so) so
k=1	h=1	∣
4
≤ 77V0 (SO).
H
Therefore, by applying the regret bounds from Zanette & Brunskill (2019), we obtain the regret
bound
…、1 ∙N,M、/ CZTTSA—SAHN	S2A2H2, 3 SAHN
V0*(So) - NΣVoπk (SO) . y4Vo*(So) 犷 log	+	log3
with probability at least 1 - p.
□
19
Under review as a conference paper at ICLR 2022
With the regret bound above, we now proceed to prove Proposition A.2.
Proof of Proposition A.2. (a) Fix a δ-significant g ∈ S. Note that for rg, V0π(s0) = P (g ∈ τπ) for
any policy π. Therefore, via the regret bound from Lemma A.4 and the choice of NERUFLER, we obtain
1	NERUFLER	1
max P (g ∈	Tn) -	ZRF	72	P(g	∈ Tn)	≤ KmaX P (g	∈ Tn)
π	NERUFLER	k=1	2 π
2
=⇒ max P (g ∈ Tn) ≤ NEULER ∑ P(g ∈ Tn)
NRF	n∈Φg
with probability at least 1 - p/2S. Now, since π(∙ | g) ~ Unif (A), We have that for any a,
maxP((g, a) ∈ Tn) ≤
n
2A
NEULER X P/ ∈ Tn)∙
RF n∈Φg
Finally, by applying the same argument above across all δ-significant g ∈ S, we have that for any
(g, a),
maxP((g, a)	∈ Tn)	≤ XmaaxP((g,a)	∈ Tn)	≤ 2SA SNEULER	X P((g,a)	∈ Tn)
n	g∈S a,n	RF	n∈Ψ
with probability at least 1 - p/2. To complete the proof of (a), observe that
1
SNRFLER
X 2hp((g, a) ∈ Tn) ≤ μ(s, a)
n∈Ψ
=⇒ max
s,a,n
P ((s, a) ∈ Tn )
μ(s, a)
≤ 4SAH,
since conditioned on (g, a) ∈ Tn, the probability that (g, a) is sampled is at least 1/2H.
(b) The result follows by following the same proof of Lemma C.2 in Jin et al. (2020), with failure
probability p/2. Note that the dynamics are stationary, and thus we do not need to perform a union
bound over the time step h ∈ [H].	口
A.3.3 Phase III Analysis
Having analyzed the previous two phases, we now show that Algorithm 3 successfully finds all exits
during Phase III. As part of this, we prove the following guarantee:
Proposition A.3. Assume that Algorithm 3 is at Line 3, having just arrived at this step for the first
time, or after finding a new exit. Let E = {(s, a) | ISEXIT[s, a]}. We assume:
(a)	E ⊆ Ext(S).
(b)	The high-probability events in Proposition A.1 (for any t ∈ [T]) and Proposition A.2 (for
δ ≤ Z/24CH2S) both hold, providing estimators Po, P1,..., PT.
(c)	For every (s, a) ∈ E and t ∈ [T], we have access to an estimator Pt(∙ | s, a) for Pt (∙ | s, a)
satisfying
f:用XHJh(Pt - Pt)fi (s,a)l≤ min (2⅛,βH).
Then, if E = Ext(S), the algorithm terminates after passing through T tasks. Otherwise, ifE 6=
Ext(S), the following events hold simultaneously with probability at least 1 - p/3K:
(a)	For one of the next T tasks that the algorithm inspects, there exists at least one t ∈ [T] such
that
lVt- Vtl > 3 ζ∙
20
Under review as a conference paper at ICLR 2022
(b)	For the task in (a), running Lines 7-11 finds at least one (s, a) ∈ Ext(S) \ E (and only
(s, a) pairs in this set), and learns an estimator Pt(∙ | s, a) for Pt(∙ | s,a) satisfying
f：用XhJ h(Pt- Pt)fi (s,a)l≤ min ($，¥).
To prove the above result, we will consider the following special set of MDPs:
Definition A.4 (Imaginable MDPs). Fix a task Mt. Furthermore, let E ⊆ Ext(S). For any function
I : S × A × [H] → {0, . . . , T}, we can construct an associated MDP MI = (S, A, PI, rt, H) via
PIhkI s,a) = PI(s,a,h)(∙ | s,a).
We define the set of imaginable MDPs to be the set Mt(E) to be the set of MDPs generated by any
I satisfying
{t}	(s,a)∈E
{0} ∪ {k l Nk(s, a) ≥ Nthresh} otherwise .
I(s, a, h) ∈
Informally, Mt(E) is the set of obtainable MDPs by borrowing dynamics for (s, a) pairs that are
not known to be exits. This set is of particular interest in our analysis, since BOAT-VI performs a
maximization over the MDPs in this set:
Lemma A.5 (Optimism). Assume the preconditions of Proposition A.3. Over the course of running
Algorithm 4, the algorithm implicitly defines an index function I : S × A × [H] → {0, . . . , T }. This
function I satisfies MI ∈ Mt(E), and MI is a maximizer of
max max ^p0M,π (s0).
M∈Mt (E) π 0
Proof. To see that MI ∈ Mt(E), note that if (s, a) ∈ E, then Ih(s, a) = t for any h ∈ [H].
Otherwise, note that although the maximum is over all indices, Pk (s0 | s, a) = 0 for any s0 if
Nt(s, a) < Nthresh. Therefore, since the estimated value function is always positive, the maximum
is effectively only over any k with Nk (s, a) ≥ 0. Thus, MI ∈ Mt (E).
Now, we prove that MI = M is a maximizer of the estimated value function, which we prove by
induction. Let 10 be another index function satisfying M0 = Mi ∈ Mt(E). Clearly, VVM,*(S)=
0 ≤ VM '*(s). Then, for any h ∈ [H] and (s, a),
QM,* (S, a) = r(s, a) + PI(s,a,h)VVM1* (S, a)
≥ r(s, a) + PI0(s,a,h)VVMfi(S, a) ≥ r(s, a) + Pio(s,a,h)VVM∕* (S, a)
=QM ,*(S, a),
where the first inequality follows from the definition ofI, and the second follows from the inductive
hypothesis. Therefore, for any S,
VM,*(s) = maxQM,*(S,a) ≥ maxQ^M,*(S, a) = VhMO,*(s)
Thus, by induction, VM,* (so) ≥ VM ,*(so). Since the argument applies for any 10, We have shown
the desired optimality result.	口
Note that M is contained in Mt(E) via our assumed preconditions, suggesting that the BOAT-
VI should find an MDP with a sufficiently over-optimistic value. However, the maximization above
makes use of estimated dynamics, and thus we need to prove that every MDP inMt(E) is sufficiently
well-estimated. We now show that the preconditions of Proposition A.3 are sufficient for estimation.
To this end, we recall the performance difference lemma:
Lemma A.6 (Performance Difference). Fix two MDPs M = (S, A, r, P, H) and M0 =
(S, A, r, P0, H). Then, for any policy π,
H-1	l
VM0,π(so)- VM,π(so)= EM,∏ X [(Ph - Ph)Vh+ι](sh,ah) S0 .
h=0	l
21
Under review as a conference paper at ICLR 2022
We now present the estimation result:
Lemma A.7. For any π and t ∈ [T], let V0M,π (s0) be the value of a policy π in M ∈ Mt(E), and
V∕0M,π (s0) an estimate using available quantities from the preconditions ofProposition A.3. Then,
SUp SUp	IVM,π(s0) - V0M,π(so)∣ < ζ.
t∈[T] M∈Mπt(E)	6
Proof. Fix a t ∈ [T], M ∈ Mt(E) and policy π, with associated index function I. Lemma A.6
implies that
H-1
∣VM,π(so) - VM,π(s0)∣ ≤ X Em,∏ h∣h(P(h) - P㈤)V⅛ι] (sh,ah)∣i
h=0
H-1
≤ X X∣[(P(h)- P(h))V∏+ιi (s,a)∣PM,π(s,a).
h=0 (s,a)
We now define the following sets:
Uδ = {(s, a) is δ-insignificant for M1}
Bh = {(s,	a)	|	Ih (s,	a)	6= 0} \ (E ∪ Uδ)
Rh = {(s,	a)	|	Ih (s,	a)	= 0} \ Uδ.
Note that Rh is estimated via reference dynamics from Phase II, while Bh is estimated using task-
specific dynamics from Phase I. Then, for a fixed h, we can decompose the inner sum above as
X∣[(P㈤-P(h))V¾ιi (s,a)∣PM,π(s,a)
(s,a)
	≤ X ∣[(PS- P(h))Vπ+ιi (s,a)∣PM,π(s,a) (s,a)∈E '	{z	} =:(I) + X	∣[(P(h)-P(h))V∏+ιi (s,a)∣PM,π(s,a) (s,a)∈Rh X	V	} =:(II) + X	∣[(P(h)-P(h))Vπ+ιi (s,a)∣PM,π(s,a) (s,a)∈Bh V	V	} =:(III) + X	∣[(P(h)- P(h))Vπ+ιi (s,a)∣PM,π(s,a) (s,a)∈Uδ 		{z	} =:(IV)
Note the inequality since E ∩ Uδ is not necessarily disjoint. We now bound the four terms above
separately.
Bounding (I): Dynamics Error from Known Exits. We first bound (I), which we note derives from
errors in estimating the dynamics of known exits. Recall that by precondition (c) in Proposition A.3,
SUp
f = S→[0,H]
Ih(Pt- PM (s,a)∣≤ 24H.
Therefore,
(I)	= X ∣[(P㈤-P(h))Vf∏+ιi (s,a)∣PM,π(s,a)
(s,a)∈E
X ∣[(P t - Pt)Vπ+ιi (s,a)∣PM,π (s,a) ≤ 2ζH X PM (s,a)
(s,a)∈E	(s,a)∈E
≤ 2ah.
22
Under review as a conference paper at ICLR 2022
Bounding (II): Reference Dynamics Error. Note that within Rh, Ph = P0, which we estimate via
Po. Therefore, We bound the error resulting from using DRF to estimate P°. This part of the proof
follows that of Jin et al. (2020). First, by Cauchy-Schwarz,
(II)	= X Ih(P㈤-P(h))Vh+ιi (s,a)∣PM,π(s,a)
(s,a)∈Rh
=X I h(P0 - P0)V+1i (s,a)∣PM,π(s,a)
(s,a)∈Rh
≤ X I h(Po - P0)Vh+1i (s,a)∣2 PMRs,a).
(s,a)∈Rh
Observe that Vhπ+1 only depends on π through timesteps h + 1, . . . , H - 1. Therefore,
X	∣[(p0- P0)Vh+1i (s,a)∣2 PhM,π(s,a)
(s,a)∈Rh
≤ νm→A X	∣[(p0- P0)Vh+1i (s,a)∣2 PMH(s)l[ν(s) = a].
(s,a)∈Rh
By applying Assumption A.1,
PhM,π (S) ≤ PM (S ∈ τπ) ≤ maxPM(S ∈ τπ) ≤ C maxPM1 (S ∈ τπ)
h	ππ
≤ C max P M1(2H)(s ∈ τ∏) ≤ 4CHSAμ(s,a),
π
Where We have applied Assumption 4.1 to move from M to M1. Substituting into the earlier
expression,
νm→A X ∣ [(Po- PO)Vh+ι] (s,a)∣2 PMK (s)l[ν (S) = a]
(s,a)∈Rh
≤ 4CHSA max
ν :S →A
≤ 4CHSA max
ν :S →A
X	Ih(PO- PO)Vh+J (s, a)∣ l[α = V(S)] μ(s,a)
(s,a)∈Rh
X Ih(PO- PO)V+J (s, a)∣ 1 [a = (IV) V * * (S)] μ(s,a)
s,a
4CHSA max E(Sa)〜〃
v：StA (s,α) μ
- PO)Vh+ι] (s,a)∣ l[a = V(S)].
Thus, by applying the bound on the right-hand side provided by Proposition A.2,
(II) = X	∣[(P㈤- P㈤)V+ιi (s,a)∣phM,π(s,a) ≤ 24H.
(s,a)∈Rh
Bounding (III): Error from Task-Specific Dynamics. Recall that on Bh, Ph = Pk for some
k 6= 0. Thus, (III) is the error resulting from dynamics estimation in Algorithm 1. By folloWing the
same argument as that used to bound (I) and applying Proposition A.1, We find that
X ∣[(P㈤- P(h))Vh+ιi (s,a)∣PM,π(s,a) ≤ 24H.
(s,a)∈Bh
Bounding (IV): Error from δ-Insignificance. The remaining set of (S, a) pairs are those such that
S is δ-insignificant in M1. Note that
(IV) = X	∣[(P㈤-PS))Vh+J (s,a)∣PM,π(s,a) ≤ H X PiM(s,a)
(s,a)∈Uδ	(s,a)∈Uδ
= H X PhM,π(S).
s∈Uδ
23
Under review as a conference paper at ICLR 2022
As a result,
PhM,π (s) ≤ PM (s ∈ τπ) ≤ max PM (s ∈ τπ) ≤ C max PM1 (s ∈ τπ)
ππ
≤ Cδ.
By setting δ = Z/24CH2S when performing reward-free RL in Phase II, We thus find that
X I h(P(h) - P(h))V+ιi (s, a) I PMRs, a) ≤ 24H.
(s,a)∈Uδ
Concluding. By combining the bounds on (I) through (IV) and summing across h = 0, . . . , H - 1,
we find that
IV,M,π(S0)- vM,π(S0)| ≤ 6.
Note that this argument simultaneously applies to any such M; therefore, the desired conclusion
follows.	□
The prior estimation result, together with Assumption 4.1, suggests that BOAT-VI should find an
MDP that sufficiently overestimates the value of the task so long as not all exits have been found.
This ensures that the exit-finding routine is triggered. Formally,
Lemma A.8. Assume the preconditions of Proposition A.3, and that E 6= Ext(S). Additionally, let
t ∈ [T] be the task with a Z-Overoptimistic value when borrowing exits Ext(S) \ E. Finally, let Vt
be the value function returned by Algorithm 4 on Mt. Then,
Vt(SO)- Vt > 3C∙
Proof. Throughout this proof, we omit the timestep 0 and the initial state s, for brevity. Define M*
and ∏* to be the maximizers of
max max VM,π .
M∈Mt (E) π
Furthermore, let M be the imagined MDP guaranteed by Assumption 4.1 on top of Mt, such that
VM,* > VMt,* + Z. Then, we have that
V M,∏ - V Mt,*
≥0
>-Z∕6
Furthermore,
VMt,*
V =	V Mt,*
—
—
|
NUCBVI	NUCBVI
—1— X VMt,∏kt)	+ —1— X VMt,∏k
NUCBVI	NUCBVI
k=1	k=1
------{----------------} `---------------{--------
≥0	≥-Z∕6
—
}
where the first follows from optimality, while the second follows from Proposition A.2. Thus,
putting the two inequalities together,
Vt -玄 > 3ζ.
□
While the prior algorithm ensures that at least one of the tasks will trigger the exit condition, the
actual task that triggers the condition may not be the same one invoked in the proof above. Never-
theless, we can prove that the trigger condition ensures that the algorithm will find a new exit.
24
Under review as a conference paper at ICLR 2022
Lemma A.9. Assume the preconditions of Proposition A.3, and that E 6= Ext(S). Let t ∈ [T] be a
task such that the value estimate Vt returned by Algorithm 4 satisfies V⅞(s0) — Vt > (2∕3)Z, and
let π be the optimal Policyfor V. Then, there exists (s, a) ∈ Ext(S) \ E such that for some t0 = t
(a)	Nt (s,a) ≥ NTresh and Pt(∙ I s, a) = Pt，(∙ | s,a).
(b)	PMt((s,a) ∈ τπ) > ζ∕6KH.
Proof. Let M be the implicit MDP defined by Algorithm 4 in the process of computing Vt. We
will prove a value gap between M and Mt, which implies that π must visit state-action pairs with
imagined dynamics.
|
≥-Z∕6
} X--------{-------}
≥(2∕3)Z
+ Vt -
|
NUCBVI
ɪ X VMt"k (so)
NUCBVI k=1
}
+
sz
≥-Z∕6
NUCBVI
1 X VMt,∏k
NUCBVI y 0
k=1
X-----------------------
}
''^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^
≥-Z∕6
where the first inequality comes from Lemma A.7 and the last two inequalities come from Proposi-
tion A.1. Thus, V0M,π (s0) - VMt,*(s0) ≥ Z∕6.
We now leverage this value gap to show that π must use some exit (s, a) whose dynamics
in M have been modified from Mt with some probability. Formally, define the set ∆ =
{(s, a, h) I Pt(∙∣ s,a) = PM4(∙ ∣ s, a)}. By construction, ∆ ⊆ Ext(S) X [H ], and for any (s, a, h) ∈
∆, there exists t0 such that N(s, a) ≥ NTSesh and Pt，(∙ I s,a) = PM(∙ ∣ s, a). Furthermore, ∆
must be non-empty, as otherwise, Pt = PM for all h, and thus VM,π (s0) ≤ VMt,* (so), a contra-
diction. Therefore, by applying Lemma A.17, we find that
6H<PMt (τ∏ ∩ ∆ = 0) ≤ X	PMt ((s,a) ∈ τ∏),
{(s,a) | (s,a,h)∈∆}
which implies the desired result, as {(s, a) ∣ (s, a, h) ∈ ∆} has at most K elements.	□
Because of Lemma A.9, we simply need to run π enough times and threshold at the number of sam-
ples needed to reliably determine which (s, a) pairs have an O(β) change in TV distance between
tasks.
Lemma A.10. We work in the setting of Lemma A.9. Set
ED	S	SAHNED	HK ED	H2K2	K
NEDesh = ω (部 log---------p-----a and NED =ω( — NEDesh +	log 万)
Then, ifwe execute π within Mt NED and let N(s, a) be the number of times that (s, a) is played
in this process, then with probability at least 1 - p∕6K, the following hold:
(a)	For the (s, a) pair and task t0 in Lemma A.9, N(s, a) ≥ NtEhDresh and
∣P(∙∣ s,a) - Pt0 (∙∣ s,a)∣IT v>2.
(b)	For any (s, a) 6∈ Ext(S) with N(s, a) ≥ NtEhDresh and t0 with Nt， (s, a) ≥ 0,
|P t(∙ I s,a) - Ptο(∙∣ s,a)∣Iτ v≤ 2.
25
Under review as a conference paper at ICLR 2022
Proof. By the choice of NED and the lower bound P Mt ((s, a) ∈ τ∏) > Z/6KH from Lemma
A.9, we guarantee that N(s, a) ≥ NtEhDresh with probability at least 1 - p/12K. Furthermore, due
to the choice of NtEhDresh, with probability at least 1 - p/12K, we have that for any (s, a) with
N(s,a)≥NtEhDresh,
M∣ s,a) - pt(∙∣ s,a)∣∣τV<4,
by applying Lemma A.16. We condition on these two events simultaneously for the rest of the proof,
which occurs with probability at least 1 - p/6K.
We now prove each part separately. For brevity, we omit (s, a) wherever it is understood.
(a)	By applying the triangle inequality,
kPt - P" kτv ≤ 帕-PBtv + 眄 - Pt0 Btv + 恒-四, Btv ≤ 眄 - PMTv + 2.
Therefore, by lower bounding the left-hand side using β-dynamics separation in Definition
4.1, we find that
BP(∙∣ s,a)- Pto (∙∣ s,a)∣∣τ v>2.
(b)	The triangle inequality implies that
P - Pt0 BTV
≤BP - PtBTV+kPt-Pt'kτv+
、	{z	' 、	=0	'
≤β∕4	0
≤ 2,
F-⅛
≤β∕4
where the bound on the first term is provided by Proposition A.1.
□
The prior result demonstrates that if the exit-finding condition is detected at any point by the al-
gorithm, then the algorithm finds a previously undiscovered exit in Ext(S). At this point, all that
remains is to ensure that the algorithm sufficiently learns the dynamics of the newly-found exit in
all of the meta-training tasks.
Lemma A.11. Fix an (s, a) ∈ Ext(S), which was found via exit detection, and let
MEL	厂	(H 4 O1	SAHNelT
Nthresh = ω [L max (了，e J log P
Assume we run the exit-learning subroutine with
NEULER = ω
-CS 2AH3
α
(SAHT
k P
C2H2 SAT
丁log-
in each of the tasks. Then, with probability at least 1 - P/6K,
max
f=S→[0,H]
Ih(Pt- Pt)fi (S,a)∣≤ mm(2⅛,βH)
for every t ∈ [T].
Proof. Fix a task t ∈ [T]. Note that Assumption 4.1 implies that (s, a) is (α∕H)-significant for
some task. Then, (s, a) must be (α∕CH)-significant for all of the other tasks by Assumption A.1.
By applying Lemma A.4, the set of policies found by the exit-learning subroutine for every task
t ∈ [T] satisfies
1	NEEULLER
max P ((S,a) ∈ Tn ) - jvEL E P((s,a) ∈ Tnk )
π	NEULER k=1
/	二，ʌ SA 1	SAHTNlELER S2AH2	1	3	SAHTNEULER
.ʌ/maxP((s,a)	∈ Tn) -E^ log--------EULER	+ ^E^	log3------EULER
n	NEEULLER	P	NEEULLER	P
26
Under review as a conference paper at ICLR 2022
with probability at least 1 - p/18T K. By setting
CS2AH3	3 HSAT
NELLER & --- log3 I—
α
p
we thus find that for any t ∈ [T],
2CaH ≤ 2 max P((s,a) ∈ Tn) ≤ NEL	X P (Ga) ∈ Tn).
EULER π∈Φt (s,a)
Note that the right-hand side is exactly the probability that the trajectory of a randomly chosen policy
in Φt(s, a) contains (s, a) in the trajectory. Therefore, by applying Lemma A.15, playing
NEL = Ω
CH 2NtELesh + CH4 log SAT
α	α2 p
is sufficient to guarantee that We obtain at least NELesh SamPles from Pt(∙ | s, a) With probability at
least 1 - p∕18TK. Since (s, a) ∈ Ext(S), Pt(∙ | s, a) (and by extension, Pt(∙ | s, a)) is supported
on Ent(S). Therefore, We can modify the proof in Lemma A.16 so that With probability at least
1 - p/18T K We get the bound
fS→axHJh(Pt- Pt)fi Ga)K f:Entma→[0,H] I h(Pt- Pt)fi (s，a)l
≤ min
ζ Z BH'
(24H, 2 )
With NtEhLresh depending linearly on L instead of S.
Note that by performing a union-bound, all events occur With probability at least 1 - p/6TK.
Performing a second union-bound over all of the available tasks results in the desired failure proba-
bility.	口
At this point, We have effectively proven the second half of our Phase III guarantee. All that remains
is to prove that if {(s, a) | ISEXIT[s, a]} = Ext(S), then the algorithm terminates Without triggering
the exit-finding condition.
Lemma A.12. Assume that E = {(s, a) | ISEXIT[s, a]} = Ext(S). Then, under the preconditions
of Proposition A.3, every task satisfies
IVt(SO)- VtI ≤ 3 u
Proof. Fix a task t ∈ [T]. Once E = Ext(S), then Mt(E) = {Mt}, since the only (s, a)-dynamics
that can be substituted from other tasks are those of non-exits, Which do not change betWeen tasks.
Therefore, by Lemma A.5, Vt(so) = V0m,*(so). Finally, by applying LemmaA.7, we thus find that
Vt(so) - E = [VM,*(S0) - VM,*(so)] + h⅛M,*(so)-同 < 3.
The desired result follows since the argument holds for any task t.	口
A.3.4 Proof of Theorem A.1
In this section, we compile the guarantees provided by each of the three phases into a proof of
Theorem A.1.
Proof of Theorem A.1. We condition on the following high-probability events:
(a)	Proposition A.1 guarantees for all Mt with t ∈ [T].
(b)	Proposition A.2.
27
Under review as a conference paper at ICLR 2022
Via a union-bound, this holds with probability at least 1 - (2/3)p.
To prove the theorem, we provide an induction-based analysis of Phase III. In particular, we will
show that while E = {(s, a) | ISEXIT[s, a]} ( Ext(S), Phase III will add at least one state-action
pair to E that belongs to Ext(S) \ E.
Formally, let Fk denote the internal state of the algorithm after it has added k state-action pairs.
Note that with k = 0, {(s, a) | ISEXIT[s, a]} in Fk is empty. Thus, Fk satisfies the preconditions of
Proposition A.3, which in turn implies that the algorithm adds a new state-action pair in Ext(S) and
sufficiently learns its dynamics for all tasks with probability at least 1 - p/3K. In short, the internal
state of the algorithm at time F1 also satisfies the preconditions of Proposition A.3 with probability
at least 1 - p/3K. More generally, Proposition A.3 ensures that if Fk satisfies the preconditions of
Proposition A.3, then so does Fk+1. Therefore, with probability at least 1 - p/3, FK satisfies the
preconditions of Proposition A.3, which necessarily implies that {(s, a) | ISEXIT[s, a]} = Ext(S)
in FK , and thus the algorithm exits as desired. By performing a union bound, all this occurs with
probability at least 1 一 p.	□
A.4 Proving the Meta-Training Guarantee
Having demonstrated that Ext(S) can be successfully recovered by interacting with the environ-
ment, we now show that the data can also be used to determine exit reachability and implement the
hierarchy oracle.
We formally state our main result here:
Theorem A.2. Assume that M1, . . . , MT have a latent hierarchy with respect to
({Zc} , Ent(∙), Ext(∙)), and assume that these tasks satisfy the (α, Z)-coverage condition in
Assumption 4.1. Furthermore, we assume the additional assumptions in Section A.2. Then, by
running the algorithm in Section A.1 with the parameters in Table 1, with probability at least 1 一 p,
the collected data can be used to implement the following:
(a)	An ε-suboptimal hierarchy oracle.
(b)	A function AvExt(s) : Ent(S) → P (Ext(S)) such that, given s ∈ Ent(Zs), returns
Ext(Zs).
The algorithm achieves both of these with query complexity
ð Γ________S4A__________ ____________S2A__________
min(ρmin(ε, ε0), Z/C) + min(ρmin(ε, ε0)2,Z2∕C)
SA KS	K2	CKS2A	CKL
+	(min(α,Z)2 + Zβ2 + Z2 + α + αmax(Z,β)2J Poy ^
A.4. 1 Implementing the Hierarchy Oracle
We first show that we can implement the hierarchy oracle in this section. In particular, we have the
following result:
Proposition A.4. Let M be the MDP corresponding to the index (x,f, r, Hr) as described in Defi-
nition 4.3. Then, given (x, f, r, H ), we can form the following estimator for Pf:
…	(δ(f(s,a))	(s,a) ∈ Ext(S)
Pf (∙ | s,a) = < δ(s)	S =㊀S or S =㊀F ,
1Po(∙ | s, a) otherwise
where Po is the estimator obtainedfrom Phase II in Section A.1. Assuming that the high-probability
^vent in Proposition A.2 holds for δ ≤ ρε∕2SH2, value iteration using Pf returns a policy π such
that V0M,*(x) — vM,π (xo) ≤ ε.
Throughout the rest of this section, We fix the tuple (x, f, r, H) and the corresponding MDP M.
Furthermore, we write Z for the cluster containing x.
28
Under review as a conference paper at ICLR 2022
To prove Proposition A.4, we will show that M can be sufficiently simulated so that the value of
any policy can be reasonably estimated. Given this simulation result, we can then show that value
iteration finds the desired policy. This simulation result depends on the following intermediate result,
which provides insight as to why Phase II data is sufficient:
LemmaA.13. For any s* ∈ Z,
[max P Ml(X ∈ τ∏)] [max P M(s* ∈ τ∏)] ≤ max P M1(2H )(s* ∈ τ∏)
ππ	π
Proof. First, We note that there exists an MDP such that PM1 (2H) (s* ∈ τ∏) is the corresponding
value function. In particular, modifying M1 (2H) so that any action from s* leads to a terminal state
and defining r(s, a) = 1 [s = s*] results in such an MDP.
Now, let ∏χ and n5* be the policies achieving
max PM1 (x ∈ τπ) and max PM (s* ∈ τπ),
ππ
respectively. Consider the concatenation of ∏χ and n§* into a history-dependent policy that runs ∏χ
until the agent reaches s, and switches to ∏s* thereafter. This policy reaches S with probability at
least
[max PM1 (x ∈ τπ)] [max PM(s* ∈ τπ)] .
within the modified MDP described above. Since the optimal value among all policies is achieved
by a history-independent policy, we obtain the desired inequality.	□
Informally, the prior result states that if x is reachable within horizon H, then any state reachable
from x within Z is also reachable in M1 within a 2H horizon. Therefore, performing reward-
free RL with horizon 2H during Phase II provides coverage over all clusters. Now, we prove the
simulation result.
Lemma A.14. Assume that the Phase II guarantee in Proposition A.2 is instantiated for δ ≤
ρε∕4SH2. Then, If Vπ is the value of π under M, and Vπ is its CorresPondmg estimate under
Pf, then
IVn(X)- Vn(x)| ≤ 2.
Proof. The proof follows similarly to that of Lemma A.7. By the performance difference lemma,
H-1
∣Vπ (S)- Vn (s)∣≤ X em,∏ [∣[(pf - Pf)Vn+l] (sh, ah) |]
h=0
H-1
≤ X X ∣[(Pf - Pf) Vhπ+ι] (s,a)∣ Ph(s,a).
h=0 (s,a)
Observe that if s ∈ S \ Z, Phn (s, a) = 0 for any π. Furthermore, since the dynamics within
{㊀S,㊀F} are known, (Pf - Pf )Vh+ι(s, a) = 0 for S ∈ {㊀S,㊀F}. Therefore, we can restrict the
sum to be over Z × A.
Now, let Zδ denote the set of δ-significant (s, a) pairs in Z × A from X, for some δ to be determined.
For a fixed h ∈ [H], we can decompose the inner sum as
X∣h(Pf - Pf) V∏+1 ] (s,a)∣Ph(s,a)
(s,a)
≤ X	∣[(pf	-Pf)	Vh+ι](s,a)∣Pπ(s,a)+	X	Ih(Pf-Pf)Vh+ι] (s,	a)∣∣∣	Phn(s,a) .
(s,a)∈Zδ	(s,a)6∈Zδ
`-------------------{z-------------------} `----------------------------------------}
(I)	(II)
29
Under review as a conference paper at ICLR 2022
Bounding (II): Error from δ-Insignificance By the definition of δ-significance,
(II)= X Ih(Pf- Pf) V∏+ιi (s,a)∣P∏(s,a) ≤ H X Ph(S) ≤ HSδ ≤ 4H,
(s,a)6∈Zδ	s6∈Zδ
where the last inequality follows from setting δ = ε∕4SH2.
Bounding (I): Reference Dynamics Error. By the Cauchy-Schwarz inequality,
(I)= X Ih(Pf-Pf)V+1i (s, a)III Phπ (s, a)
(s,a)∈Zδ
1/2
≤ X ∣h(Pf -Pf) V+1i Ga)I Ph(S,a) I .
(s,a)∈Zδ
Then,
X Ih(Pf- Pf)V+1i (S, a)III Phh (S, a)
(s,a)∈Zδ
≤ Vma→a	X	Ih(Pf-Pf)V+1i	(s,a)I	Ph(S)I	[ν(S)	=	a].
(s,a)∈Zδ
Since x is ρ-significant in M1(2H) by Definition A.2, Lemma A.13 together with δ-significance in
M implies ρδ-significance in M1 (2H). Therefore,
Ph (s) ≤ max P M(S ∈ τπ) ≤ 1 max P MIQH)(S ∈ τπ) ≤ 4HSAμ(s,a),
h	h	ρh	ρ
where the last inequality follows by part (a) of the Phase II guarantee in Proposition A.2. Substituting
into the prior expression,
VmaxA X Ih(Pf-Pf) V+1i (S,a)I Ph(S)I [ν(S) = a]
(s,a)∈Zδ
≤ 4HSA VmaXA X I h(Pf - Pf) V+J (S,a)I ι[ν(S) = a]μ(s,a)
P ”' →	(s,a)∈Zδ
4HSA
≤ — VmaXA 跖，。)~“
Ih(Pf- Pf) V+J (S,a)I 1[V(S) = a] .
Thus by applying part (b) of the Phase II guarantee in Proposition A.2, we have that
(I) ≤
4HSA
ρ-
VmaXA E(S，a)~"
Ih(Pf- Pf) V+J (S,a)I 1[V(S) = a]
ε
≤ 4H.
Concluding. By combining the bounds on (I) and (II), We obtain the desired result.	口
With this estimation result, we can now prove Proposition A.4.
Proofof Proposition A.4. Let π be the policy found by value iteration using Pf, which achieves the
maximal value in the corresponding MDP. Then, by Lemma A.14
吟(X)- Vh (x) ≤
[V)*(SO)-	Vr	(SO)i +	hVh	(SO)-	Vr(SO)i + hVh(SO)-	Vr(SO)i≤ ε
'---------{z--------} '-----------{z--------} '----------V---------}
≤ε∕2
≤O
≤ε∕2
□
30
Under review as a conference paper at ICLR 2022
A.4.2 Determining Available Exits
In this section, we prove that we can determine the set of available exits. We have the following
formal result:
Proposition A.5. Assume access to the ε-suboptimal hierarchy oracle from the previous section
and that the guarantee in Theorem A.1 holds. Then, we can implement the function AvExt(s) :
Ent(S) → P (Ext(S)) which, given s ∈ Ent(Zs), returns the subset of Ext(Zs) that is reachable
from s.
Proof. Fix an input x ∈ Ent(S), which we assume belongs to some cluster Zx. It suffices to
demonstrate that We can implement 1 [e ∈ Zχ] for any fixed e ∈ Ent(S). Define
/e"=匕 OSha：
and re(s,a) = 1 [(s,α) = e]. By Definition A.3, the MDP M corresponding to the tuple
(x, fe, re, H) has optimal value V * = ε0l [e ∈ Zx ∧ e reachable from x]. Additionally, by Lemma
A.14, ∣V0∏(S) 一 V0∏(x)| ≤ ε∕2. We now proceed by cases. If e ∈ Zx or e is not reachable
from x, then V0π (x) = 0 for any policy π, and thus value iteration can only find a policy π With
V0∏(x) ≤ εo∕3. Otherwise, for e ∈ Zx, V0*(x) = ε0, and thus value iteration necessarily must find
a π with V0∏ (x) ≥ 2ε0∕3. Putting these together, if V is the optimal estimated value in M, then
2
i [e ∈ Zx] = i V ≥ 3εo
Note that this is implementable for all e ∈ Ext(S) (and returns a subset of Ext(Zx) for the query
above) since the set of exits are already known.	□
A.4.3 Finalizing the Guarantee: Query Complexity
In this section, we finalize the proof of the meta-training guarantee by computing the query com-
plexity.
Proof of Theorem A.2. As demonstrated by Proposition A.4 and Proposition A.5, running the algo-
rithm in Section A.1 with the parameters in Table 1 provides the desired guarantees with probability
at least 1 一 p.
To compute the query complexity, observe that we perform the following number of trajectories
while executing the algorithm in Section A.1.
O T (NUCBVI + NTS) + NERUFLER + NRF + KNED + T K(NEEULLER + NEL) .
Ignoring terms that do not depend on T or ε, we obtain the claim.	□
A.5 Brute-Force Learning of the Hiearchy
Theorem A.3. Assume that Algorithm 6 is run with parameters satisfying
NEULER = ω
CH3S2A
SAHT
P
α
and
Nthresh = ω (铲 log
SAHNT
p
CH	C2H2	SAT
and N =ω( HNthresh + 丁 lθg —
Then, the set returned by the algorithm is exactly Ext(S) with probability at least 1 一 p. Further-
more, the algorithm achieves this result with query complexity
O T
CS4A	CS2A
α α ɑβ2
poly(H).
31
Under review as a conference paper at ICLR 2022
Algorithm 6 Brute-force learning of the latent hierarchy.
1:	procedure LEARNHIERARCHY((M1, . . . , MT), NEULER iterations, N policy samples, thresh-
old Nthresh)
2:	for all t ∈ [T], s ∈ S do
3:	Create MDP Ms so P (㊀ | s,a) = 1 for any a.
4:	rs(s0, a0) — 1 [s0 = s].
5：	Ψs J EULER(Ms, r, NEULER)
6:	for all t ∈ [T], s ∈ S, a ∈ A do
7:	Modify policies in Ψts to play a on s.
8:	for all n ∈ [N] do
9:	Sample π 〜Unif(Ψs).
10:	Play π in Mt, collect sample (s, a, s0n) if (s, a) is encountered
11:	Nt(s, a) J number of times (s, a) is encountered above.
12:	Pt(∙ | s,a) J estimate of (s, a) dynamics in t.
13:	return {(s, a) ∣ (∃t = t0) ∣∣Pt - Pto∣∣τV > β∕2, min(Nt(s, a),Nt，(s, a)) ≥ Nthresh}.
Proof. For any (s, a) ∈ Ext(S), Lemma A.2 implies that S is α∕H-significant for some task t ∈ [T].
Therefore, s is α∕CH-significant for any task t ∈ [T], by Assumption A.1.
Now, by an argument similar to that used in the proof of Lemma A.2, we have that with probability
at least 1 - p∕3T, the choice of NEULER implies
1
NEULER
PMt(s∈τπ) ≥
π∈Ψts
α
2CH
for any exit (s, a) ∈ Ext(S) and a fixed task t ∈ [T]. Therefore, by a union-bound over the tasks,
the same guarantee holds for all tasks simultaneously with probability at least 1 - p∕3.
Now, for any fixed (α∕CH)-significant (s, a) pair, sampling from Ψts at least N times guarantees
that with probability at least 1 - p∕3S AT, Nt(s, a) ≥ Nthresh. Therefore, once again performing
the necessary union-bound, we obtain the same result uniformly over any (α∕CH)-significant (s, a)
and t ∈ [T] with probability at least 1 - p∕3.
Finally, for a fixed (s,a) and t, the estimator for Pt(∙ | s,a) satisfies the property that when
N(s, a) > 0,
l∣Pt(∙ I S, a) - Pt(∙ I S, a) Il ≤ SH2T11 log SAHNT + H log SAHNT
TV	Nt(s, a)	p	Nt(s, a)	p
with probability at least 1 -p∕3SAT, using an argument similar to that used in Lemma A.16. Again,
by a union bound, the same guarantee holds for any (S, a) and t ∈ [T]. In particular, for any (S, a)
with Nt(S, a) ≥ Nthresh,
∣∣Pt(∙∣ s,a) - Pt(∙∣ s,a)∣∣τ y≤ 4.
Therefore, by a similar argument to Lemma A.10, the following are true:
(a)	If (S, a) ∈ Ext(S), then there exists t, t0 for which
∣∣Pt(∙∣ s,a) - Pto(∙∣ s,a)∣∣τ v>2.
(b)	If (S, a) 6∈ Ext(S), then for any t 6= t0 with Nt(S, a), Nt0 (S, a) ≥ Nthresh,
∣Pt(∙ I s,a) — Pt(∙∣ s,a)∣∣τ v≤ β,
Putting everything together, we see that the set returned by Algorithm 6 is exactly Ext(S), with
probability at least 1 一 p.	□
32
Under review as a conference paper at ICLR 2022
A.6 Technical Lemmas
Lemma A.15. Let X1, . . . , XM be i.i.d. Ber (p) random variables. Then, if
M = Ω (N + !log；),
p	p2	δ
then with probability at least 1 - δ,
M
X 1 [Xi = 1] ≥ N.
i=1
Proof. By applying Hoeffding’s inequality,
P(X![Xi = 1]<N)= PGXIXi = 1]-p<N-P)
=P(MMX1 [Xi =。11-P) >p- N
≤ exp -2M (P - N)
Setting the final expression to the failure probability δ and solving, we obtain the quadratic inequality
P2M2 -12NP + 1log1 J M + N2 ≥ 0.
Finally, via solving this inequality for M, we find that
M ≥ 2N + x⅛ log；
P 2P2 δ
is sufficient to guarantee the desired event with failure probability δ, as desired.
□
Lemma A.16 (Dynamics estimation error bound). Fix a policy π, MDP with stationary dynamics
M = (S, A, P, r, H), and N ∈ N. Assume that π is played N times in M, and all transitions are
used toform an estimator P(∙ | s, a) using empirical averages. For any (s, a) ∈ S ×A, let N(s, a)
be the number of times (s, a) is encountered in this process. Then, with probability at least 1 - P,
any (s, a) with N(s, a) > 0 satisfies
sup
f = S→[0,H]
Ih(P - P)f] (s,a)
H2S SAHN
Nsray Og —P — +
HS SAHN
N(s,a) Og -P~
Proof. Assume that the obtained samples are given by {(sk, ak, s0k) | k ∈ [HN]}, so that
(sHn+r, aHn+r, s0Hn+r+1) is the rth time step in the nth execution ofπ in M for any 0 ≤ n ≤ N- 1
and 0 ≤ r ≤ H - 1.
Fix any (s, a) ∈ S × A, and assume that (s(j),a(j), sj is the jth sample from P(∙ | s, a). FUr-
thermore, let mj (s, a) denote the index at which the jth sample is obtained. We claim that for any
s* ∈ S and 0 < M ≤ HT,
I1 M
M Xi [mj (s, a) ≤ HT ] (INj)=s[- P(s* | s, a))
I j=1
/P(s0 | s,a)	S 1 S
≤ V -M -log I+ Mlog δ∙
Let Fi be defined as the σ-algebra induced by the set of random variables
n(mj (a),ι hs(j)=s*i) ∣ j ≤ io.
33
Under review as a conference paper at ICLR 2022
Clearly, (Fi) is a filtration such that the jth term in the sum above is measurable with respect to Fj.
Furthermore, observe that
E [1 [mj (s, a) ≤ HT] (Ihs(力=s] — P(s* | s,a)) ∣ Fj-Ii
=E [1 [s(j) = s*i — P(s* | s,a) ∣ Fj-ι, mj(s, a) ≤ HTi P(mj‘(s，°)≤ HT)
= 0.
Therefore, the random variables in the sum forms martingale difference sequence. Furthermore, the
sequence is bounded in [—1, 1], and satisfies
Var [1 mj(s, a) ≤ HT] (1 [sj) = s[ — P(s* | s, a)) ∣ Fj-Ii
=E [Var [1 [s(j) = s] — P(s* | s, a) ∣ Fj-ι, mj(s, a) ≤ HTi I Fj-Ii
≤ P(s* | s,a).
Therefore, by applying Azuma-Bernstein, we have that
1M
M X1 [mj(S, a) ≤ HT] (INj) = s]
j=1
—P(s* | s,a))
产(s0∣ s,a) SAHN + 工
≤ V M g δ + Mg
SAHN
δ-
with probability at least 1 — p/SAH N.
By applying a union bound on (s, a, s*) and M, We thus have that with probability at least 1 一 p,
1M
M X1 [mjGa) ≤ HT] (IMj)
j=1
≤r
2P(s0 | s, a)
SAHN 2 SAHN
log -ɪ + Mlog -ɪ
M
holds for any (s, a, s*) and M.
N(s, a) > 0,
Conditioned on this event, we thus have that for any (s, a) with
Pt(∙ | s,a) — Pt(∙ | s,a)
TV
=2 x IPt (s0 | s,a)-Pt(s0 | s,a)∣
2 s0∈S
/ L /P(s0 | s,a) 1 ^^SAHN	S 1 SAHN
.⅛ XFsL -ɪ + Nsaylog -ɪ
s0∈S
/ I^^S^^^^^SAHN	S 1 SAHN
.N N(s,a) og	δ+ N(s,a) og	δ.
The final result follows simply by noting that
∣ [(Pt - Pt)fi (s, a)∣ . ∣∣Pt(∙∣ s,a) — Pt(∙ I s, a)∣∣τV kf k∞ ∙
□
Lemma A.17. Fix two MDPs M = (S, A, P, r, H) and M0 = (S, A, P0, r, H). Let ∆ denote the
subset of S ×A× [H] for which Ph (∙ ∣ s, a) = Ph (∙ ∣ s, a). Then, for any policy π,
V0M0,π (s0) — V0M," (s0) >P F PM (τ∏ ∩ ∆ = 0)
Pmo (τ∏ ∩ ∆ = 0) > -ρ.
H
34
Under review as a conference paper at ICLR 2022
Proof. Write q = Pm，(τ∏ ∩ ∆ = 0). Note that V0M ,π(s0) can be decomposed as
H-1
V0M ,π (s0 ) = qEM0 X rh(sh, ah) τπ ∩ ∆ 6= 0
h=0
H-1
+ (1 - q)EM0	rh(sh,ah) τπ ∩ ∆ = 0
h=0
H-1
≤ qH + (1 - q)EM0 X rh(sh, ah) τπ ∩ ∆ = 0 .
h=0
Since P and P0 agree on (S × A × [H]) \ ∆, the dynamics of M and M0 agree up until π performs
an action in ∆, and thus
PM (τπ ∩ ∆ 6= 0) = PM0 (τπ ∩ ∆ 6= 0)
H-1
EM	rh (sh, ah) τπ ∩ ∆ = 0
h=0
H-1
EM0	rh(sh, ah) τπ ∩ ∆ = 0
h=0
Furthermore,
h-1
(1-q)E Xrh(sh,ah) τ∏ ∩ δ = 0 ≤ V0M,π(so) ≤ V0M,*(so).
h=0
Putting everything together,
V0M0,π (so) ≤ qH + V0M,*(so) =⇒ q>jρ.
H
□
A.7 Why Should Optimistic Imagination Hold?
In this section, we provide a heuristic explanation as for why optimistic imagination can be expected
to hold in most scenarios for which Definition 3.1 holds.
Recall from the main text that if an exit e satisfies the preliminary coverage condition, then e can
be detected using only Phase I data. Therefore, we can restrict our discussion of Assumption 4.1 to
subsets of exits that only have nonzero importance for a task t if Pt(∙∣ e) is some distribution de. In
other words, we can only guarantee that near-optimal policies for the source tasks observe de . We
will provide a heuristic reason as to why we can then expect to find an MDP such that condition (b)
of Assumption 4.1 holds for such a subset S.
State visitation measures. Recall that the value function in an MDP with a stationary reward
function is given by
H-1
VoM,π(so) = X EM,π [r(sh, ah) | so] .
h=o
Now, if we assume that the reward is only a function of the state (as in goal-conditioned RL), and
view r as a vector in R|S| , then we can write VoM,π(so) = Hρ>M,πr, where
1 H-1
PM,∏(S) = Hf P M,π (Sh = s | so).
h=o
That is, PM,π is the state visitation measure of π within M with horizon H. This geometric view-
point was explored by Eysenbach et al. (2021) in the context of unsupervised RL in the discounted
infinite-horizon setting. We note that for any fixed M, {PM,π} is a convex subset of the probability
simplex.
35
Under review as a conference paper at ICLR 2022
Constructing the desired reward function. Let M be any source task such that the set T =
{e ∈ S I PM(∙ | e) = de} is non-empty. Furthermore, let M be the corresponding MDP described
in Assumption 4.1(b) after borrowing dynamics, so that
P(∙ | s,a)
• | s,a) (s, a) ∈ S
sa)	otherwise
is the dynamics of M. Since ∣∣PM(∙ | e) - de∣bv > β for any e ∈ S, heuristically, We expect the
following to be true:
Assumption A.2. There exists a π* such that PM ∏* ∈ {pm,∏}.
Stated less formally, we expect there to be states that ∏ can reach better in M compared to any
policy in M.
Example A.1. In the four-room example, this intuition is captured by the fact that the agent can
spend more time in states past open gates compared to if they were closed.
y
Under Assumption A.2 above, since {ρM,π } is convex, we can find a reward direction separating
{PM,∏ } and PM,π*, or more formally,
PM ,∏*r > SUp PM ,∏* r.
π
Then, the MDP M with reward function r satisfies the desired condition in Assumption 4.1(b).
Thus, under Assumption A.2, the coverage condition can indeed hold with sufficiently diverse source
tasks.
36
Under review as a conference paper at ICLR 2022
B Meta-Test Proofs
We now provide an analysis of the regret incurred by a learner using an approximately learned
hierarchy at meta-test time. We first show that the hierarchy oracle from the source tasks can provide
useful temporally extended behavior. We then show that using these policies results in bounded
suboptimality and achieves a better regret bound compared to standard UCB-VI.
Throughout this section, We fix an optimal π* satisfying the conditions of Assumption 5.1. FUrther-
more, we assume that we have access to a hierarchy oracle that provides ε-suboptimal policies as
defined in Definition 4.3.
B.1	Using the Hierarchy Oracle
In this section, We shoW that the hierarchy oracle can be used to implement tWo useful behaviors:
(1) reaching exits and (2) behaving optimally Within a cluster.
B.1.1	Near-Optimal Goal Reaching
Assume that the agent is currently at a state z ∈ {s0} ∪ Ent(S) at time step h, and intends to exit the
current cluster Z via exit g = (s*, a*) ∈ Ext(Z). We obtain a policy implementing the high-level
intent as folloWs:
(1)	Define the termination for any (s, a) ∈ Ext(S) as:
fg Ga) = {1F
(s, a) = g
otherWise
(2)	Define reward as r㊀S (s,a) := 1 [s = ㊀S]
(3)	Provide (z, fg,r㊀S, H 一 h) to the hierarchy oracle and obtain a policy ∏z,g,h.
For simplicity, we will write THhi-erh(z, g) for THπz-,gh,h (z, g) throughout our analysis. The following
proposition quantifies the performance of the obtained policy:
Proposition B.1. T hier satisfies the following inequality:
E [THi-rh(z,g)] ≤ TH-h(z,g) + ε.
Proof. Due to the definition of Pfg and r, observe that for any π,
Hh
V0π(z) = E X r(sh, ah) s0 = z = (H 一 h) 一 E THπ -h(z, g) .
h=0
Therefore,
(H - h) - TH-h(z,g) —ε ≤ (H - h) - E [TH-rh(z,g)]
=⇒ E [τH-rh(z,g)] ≤ TH-h(z,e) + ε.
□
B.1.2	Near-Optimal Within-Cluster B ehavior
Assume that the agent is currently at a state z ∈ {s0} ∪ Ent(S) at time h, and intends to remain
in the current cluster Z while maximizing a given reward function r. We obtain a policy for this
high-level intent as follows:
(1)	Define transition dynamics for any (s, a) ∈ Ext(Z) as P(∙ | s,a) = δ(㊀F).
(2)	Provide P, r, and planning horizon H - h to the hierarchy oracle, and obtain a policy π.
37
Under review as a conference paper at ICLR 2022
B.2	Formal Learning Procedure
In this section, we describe the procedure for learning a policy using the oracle-provided policies
described in the previous section. Formally, we construct a surrogate MDP whose dynamics are
determined by M and the oracle. We can then apply any tabular learning method to this new MDP
(in our case, Euler), obtaining a policy in the surrogate MDP that readily translates into a policy
in M.
The components defining the surrogate Mhl = (Z, G, Phl, Rhl , Heff) are as follows:
Meta-state space Z. We set
Z := (Ent(S) × {0,...,H + 1}) ∪ {θ},
where H is a high-probability bound on the time to move through Heff exits (to be determined
later). We incorporate the time step into the meta-state to ensure that both the dynamics and reward
are computable from the state information (ensuring that Mhl is indeed an MDP).
Meta-action space G. Given a current meta-state (s, h) where s ∈ Z, the available meta-actions
G can be identified with Ext(Z) ∪ {θ}.
Algorithm 7 Performing a Meta-Transition	
1	: procedure PERFORMMETATRANSITION((z, g) ∈ Z × G) . Executes the desired meta-transition in the original MDP M.
2	:	if z= θ then
3	:	return θ
4	:	else if z= (s, h) then
5	if h ≤ H then
6	:	if s ∈ Z* or g = θ then
7	:	Execute within-cluster policy from oracle until termination.
8	:	return θ
9	:	else
10	Execute ∏z,g,h obtained from oracle until g is performed or h = H.
11	s0,h0 J current state and time step
12	:	if g was performed then
13	:	return (s0, h0)
14	:	else
15	return (s, H +1)
16	:	else
17	:	if s ∈ Z* or g = θ then
18	:	return θ
19	:	else
20	:	return (s, h)
Meta-dynamics Phl. Fix (z, g) ∈ Z × G for some z 6= θ, so that z = (s, h). We consider the
procedure in Algorithm 7 for generating the meta-dynamics. Intuitively, we execute a meta-action
g = θ by running the oracle-provided policy until the learner encounters g, or has acted for H
timesteps in the current episode. On the other hand, ifg = θ, the agent executes the oracle-provided
ε-suboptimal policy that remains within the current cluster and acts for H - h timesteps.
Formally, the next state z0 is given by
θ S ∈ Z * or g = θ
Z = < (s0, h0) h ≤ H
[(s,h) otherwise
38
Under review as a conference paper at ICLR 2022
where s0 and h0 are generated given THhi-erh(s, g) as
h0 | THi-rh(s, g) = min(h + TH-h(s, g), H+1)
∫P(∙∣ g) h0≤ H
δ(s)	otherwise .
Note that the learner can only execute meta-aCtions while h ≤ H. Furthermore, given access to M,
one can easily simulate the dynamics of Mhl .
Meta-reward Rhl. Fix ((s, h), g) ∈ Z × G. Recall that the reward function of M is supported
on Ext(S) ∪ (Z*)◦. Thus, this reward function can be lifted onto Mhi. Formally, We define the
following reward function:
(Wh(S) Z = (s, h), S ∈ Z* and h ≤ H
Rhl(z,g) = < r(g)	z = (s, h), s ∈ Z* and h ≤ H ,
10 otherwise
where Wh(S) is the random sum of rewards obtained by playing a within-cluster policy starting from
S0 for the rest of the episode. Note that Rhl depends on Phl and is thus random. Furthermore, this
reward function is consistent with how meta-transitions are performed in Algorithm 7.
Meta-horizon Heff . Recall that there exists an optimal policy that encounters at most Heff exits
with high probability. Accordingly, we limit the learner to being able to choose Heff high-level
actions, which recall can be choices of exits.
Solving Mhl. To obtain the desired policy, we apply EULER to Mhl . By the construction in
Algorithm 7, the policy set returned by EULER easily translates into policies on M. Furthermore,
the value of this policy is the same on both MDPs.
B.3 Proving the Regret Bound
Having defined the procedure for learning a policy using the hierarchy, we now proceed with the
regret analysis. Our analysis proceeds by constructing a policy expressible in Mhl that achieves
near-optimal returns by imitating the high-level decisions made by π*. We then use this policy as a
comparator policy when applying EULER regret bounds to Mhl .
To formally construct the desired comparator policy, we need to first define the notion of a meta-
history, which contains the set of high-level decisions made by any policy:
Definition B.1. Fix a policy π, which given some horizon L, generates a (random) trajectory
(S0, a0, . . . , SL). Let Ext(π) be the number of exits performed in the trajectory, i.e.
L-1
Ext(π) = X l[(sh,ah) ∈ EXt(S)].
h=0
The meta-history Hhl(π) corresponding to this trajectory is the sequence
(z0 , g0 , z1 , g1 , . . . , zExt(π) ) = (Si0 , (Sj0 , aj0 ), Si1 , (Sj1 , aj1 ) . . . , SiExt(π) ),
where
0	n=0
in :=
jn-1 + 1 otherwise
jn ：= min 1 [(sh, ah) ∈ Ext(S)].
h=in,...,L-1
Note that zi ∈ Ent(S) and gi ∈ Ext(S) for all i = 0, . . . , Ext(π). We omit π in writing Hhl when
the underlying policy π is understood.
Informally, Hhl tracks all entrances and exits contained in a trajectory generated by π. We define
the length ofa meta-history Hhl, denoted as |Hhl|, as the number of exits contained in Hhl.
39
Under review as a conference paper at ICLR 2022
B.3.1	Policy Construction
We now proceed with constructing the desired policy. Intuitively, the comparator imitates the dis-
tribution over Hhi(∏*), conditioned on ∣Hhi(∏*)∣ ≤ Heff. To see Why this is sufficient for near-
optimality, recall that the reward on MTg is supported on Ext(Z*) ∪ (Z*)◦. Consequently, by
imitating the distribution over meta-histories, the policy is expected to obtain roughly the same sum
of rewards in expectation from the exits. Therefore, all that remains is to ensure that the learner
collects roughly the same sum of rewards from Z*, which is the same as ensuring that this policy
does not take too long to reach Z*.
Construction. Let H be the running meta-history, containing k ≤ Heff actions. The optimal policy
induces a distribution q(∙ | H) over Ahl representing the next exit it takes6. We then define ∏ as
∏(∙∣ z,H)={∣HH)
Z = (s, h),h < H
otherwise.
Observe that ∏ terminates the episode upon reaching H. Furthermore, this policy is dependent on
the meta-history. However, since Mhl is an MDP, there exists a stationary policy that achieves at
least the same value.
B.3.2	Suboptimality Analysis
In this section, we prove that π achieves bounded suboptimality. Rather than analyzing π directly
in Mhi, we construct a new Mhl and ∏ to better track the meta-history. In particular, conditioned
on the event that ∏ requires more than H time steps to execute, then the agent would not be able to
imitate the full meta-history generated by ∏*, even after having performed less than Heff exits.
Constructing a surrogate for analysis. We now formalize the construction of the surrogate MDP
Mhl and the policy π corresponding to π in this MDP. To obtain Mhl, we redefine the dynamics
from Mhl so that s0 | h 〜P(∙ | g) in Mhl. In effect, we allow the policy to continue performing
transitions beyond H, although without any reward. Accordingly, we define ∏ as ∏(∙ | z, H)=
q(∙ | H). The following lemma formalizes how ∏ and Mhl have desirable properties for the analysis:
Lemma B.1 (Surrogate Policy Characterization). Let μ* denote the distribution of
Hhl(π*) | ∣Hhl(∏*)∣ ≤ Heff in MTg, and μ the distribution of H(∏) in Mhl. Then, (1 — Z)μ* ≤ μ.
Proof. Let V* be the distribution induced by the following procedure:
(1)	Sample a meta-history from the distribution Hhl(∏*) | |Hhl(∏*)∣ > Heff.
(2)	Truncate the obtained meta-history to length Heff .
It is easy to see from the definition of ∏ that μ = (1 一 Z)μ* + Zν*. The desired result follows. □
Thus, we have indeed shown the desired property that ∏ properly tracks the (truncated) meta-history
generated by ∏*. To justify performing our analysis on (Mhl, ∏), we have the following result,
which shows that any result on the value of the pair above applies to the value of π in Mhl.
—	—一	一_	_ _ a7,,彳，、	_ _ 卜人…k ,	、
Lemma B.2. As constructed above, V0 hl, (s0) = V0 hl, (s0).
Proof. We write M := Mhl and M := Mhl. Similarly, we write π := π. We proceed by proving
a chain of equalities.
(V π0,M0 (s0) = V π,M0 (s0)). We omit M0 in this part of the argument for clarity. By the perfor-
mance difference lemma, we have that for any k ∈ [Heff] and z ∈ Shl,
Heff -1
V0∏(so) — V∏0(so)= X Ezf [A∏0(z,∏)].
j=0
6The distribution q can return ㊀ if the learner stays in the cluster until episode termination.
40
Under review as a conference paper at ICLR 2022
Let ∆ := {z ∈ Shi I Z = (s, h), S ∈ Z*, h ≥ H}, which is the set on which π and π0 disagree.
Observe that for any π and k, Vkπ(z) = 0 for any z∈	∆, and thus Akπ0 (z, π) = 0 for all such states.
For any other z, Akπ0 (z, π) is clearly 0. Thus, we obtain the desired result.
(V π,M0 (s0) = V π,M(s0)) We omit π in this part of the argument for clarity. Using the simulation
lemma,
Heff -1
V0M(so) - VoM0(so) = E E(z,g)〜dM，[[(Pm - PM，)VjMi](z,g)].
j=0
Observe that the behavior of the two MDPs are identical conditioned on h0 ≤ H. On the other hand,
conditioned on h0 > H, ∏ can no longer receive rewards from either MDP. Therefore, [(Pm -
PM，)VjM](z, g) = 0 for any j, z, g by decomposing the relevant expectations along the two events.
We thus obtain the desired result.	□
Analyzing the surrogate. With the results above, we now proceed to analyze the difference in
values
VOMTg,* (so) — VOM hl,π (so),
which then implies the desired suboptimality result. First, we have the following lemma character-
izing the time ∏ requires to fully execute a given meta-history in the base MDP M:
Lemma B.3. Fix any Hhl = (zo, go, . . . ) such that |Hhl| ≤ Heff. Furthermore, define the sequence
of reaching times
To := 0 and Tk := Tk-1 + THhi-erTk-1 (zk-1, uk-1).
We define T hier (Hhl) to be the time required by the hierarchy to execute Hhl, which is formally
given by T|Hhl | in the sequence above. Then,
(a)	E [Thier(Hhl) ≤ [1+(1+γ)W+ε]Heff.
(b)	Let σ2 := β2[(1 + γ)W + ε]2Heff. Then, for any t > 0,
P (Thier(Hhi) ≥ [1 + (1 + Y)W + ε]Heff +1) ≤ e-t32σ2.
Proof. We prove the two parts separately:
(a)	We will prove via induction that E [Tk] ≤ k [1 + (1 + γ)W + ε]. For any k and Tk-1,
E hTHi-rTk-ι (Zk-1,gk-I)ITk-Ii ≤ E [TH-Tk-I(Zk-1, gk-I)I Tk-Ii + 羯
=1 + E [τH-τk-1 (Zk-1, s(gk-i)) I Tk-ii + ε
≤ 1+(1+γ)W+ε,
where the first inequality uses properties of the hierarchy oracle, while the final inequality
follows by combining Definition 5.2(b) and Assumption 5.2. Therefore, by linearity and
the tower property of expectation,
E [Tk] = E [Tk-1] +E hTHhi-erTk-1 (Zk-1, gk-1)i
=E[Tk-1]+E hE hTHhi-erTk-1 (Zk-1, gk-1) III Tk-1ii
≤ E [Tk-1] + 1 + (1 +γ)W+ε.
The desired result then follows by induction.
(b)	Let Bk := k [1 + (1 + γ)W + ε] and fk(t) := E [THhi-ert(Zk, gk). Note that for any k and
t, Bk-1 + fk-1(t) ≤ Bk, by following the argument in (a). Therefore, for any λ > 0,
E [exp {λ (Tk - Bk)}]
=E[E[exp{λ(Tk -Bk)} | Tk-1]]
≤E	hE	hexp	nλ	Tk-1	+ THhi-erTk-1 (Zk-1, gk-1) -	Bk-1	-	fk-1(Tk-1)o	III	Tk-1ii ,
41
Under review as a conference paper at ICLR 2022
where the last inequality uses the monotonicity of the exponential function. Therefore, by
applying the sub-Gaussian condition given in Definition 5.2,
E [exp {λ (Tk - Bk)}]
≤ E hexp {λ (Tk-1 - Bk-1)}
E hexp nλ THhi-erTk-1 (zk-1, gk-1) - fk-1 (Tk-1)o Tk-1ii
≤ E [exp {λ(Tk-ι - Bk-ι)}]exp [λ2C2/2],
where we have used the fact that THhi-erT	(zk-1, s(gk-1)) has a sub-Gaussian upper tail
with variance proxy
C2 =β2E hTHπ -Tk-1 (zk-1, s(gk-1)) Tk-1i2
≤β2[(1+γ)W+ε]2.
Note that we have once again used the properties of the hierarchy oracle, and Assumption
5.2.	Therefore, by induction, E [exp {λ (Tk 一 Bk)}] ≤ E [λ2(√kC)2/2^, from which the
desired tail bound follows by making use of Chernoff's inequality.	口
As We have shown that ∏ closely tracks the meta-history of ∏* and have analyzed the distribution of
time it takes to execute a given meta-history, we can now analyze its suboptimality:
Lemma B.4. There exists a policy π expressible in Mhl such that
VOMTg,*(S0)- VOMTg,π(S0). (1 + Heff + β√Hff)ε + [γHeff + β(1 + Y)√Hθff] W + ZH.
Proof. Assume that ∏* generates a (random) meta-history of length N given by Hhl =
(z0, go, zι, gι,..., ZN). Furthermore, let T* denote the (random) time ∏* takes to reach ZN. Then,
given Hhl and T*, observe that we can write
N-1
V0*(s0) = E [RT* (Hhl)] , where RT(Hhl) := VT(ZN)1 [zN ∈ Z*]+ X r(gk),
k=0
using the assumptions on the reward function and condition (a) in Assumption 5.1. Subsequently,
letting E be the event {N ≤ Heff }, we can bound the right-hand side as
V0*(s0) = E [R*T* (Hhl)] ≤ (1 - ζ)E [R*T* (Hhl) | E]+ζH,
where we have used Assumption 5.1 to bound the probability that N > Heff .
Our goal for the rest of this proof is to transform the expectation on the right-hand side into a form
that lower bounds V0π(s0). To this end, we define
N-1
RTier(Hhl) := Vhier(ZN)1 [zn ∈ Z*]+ X r(gk),
k=0
and the sequence of times
T0 = 0 and Tk := Tk-1 + THhi-erTk-1 (Zk, gk).
Note that Rhier and TN are analogous to R* and T*, respectively. Then, letting F be the event
{TN ≤ H}, note that
吟(S0) ≤ (1 - Z)E[RT* | E]+ ZH
=(1-ζ)E[R*T* - RhTiNer + RhTiNer E] +ζH
≤ E [(RT* - RTNr) 1 [F] ∣E] +(1 - Z)E [RTNrl [F] I E] + [Z + P(FCl E)] H
×---------------V-----------} X------------V-----------}
(I)	(II)
42
Under review as a conference paper at ICLR 2022
We bound (I) and (II) separately.
Bounding (I). Let G be the event E ∩ {zn ∈ Z*}. Then, if We define
N-1
Tmin = X Tmin(Zk, uk) ≤ N(W + 1)
k=0
as the minimum time needed to execute Hhl, We then have that
E [(RT* - RTNr) 1 [F] ∣ E] ≤ E [RT* - RTNrI E]
≤ E [VT* (ZN) — VTNer(Zn) I G]
≤ E [VTmin (ZN) — VTNer(ZN) |G]
H
≤ / PMmin(ZN) — VTer(ZN) >α∣G) dα.
0
Note that the bound on Tmin folloWs from Assumption 5.2. To convert the different in values into a
difference of times, observe that if TN - Tmin ≤ α - ε, then
VTmin (ZN ) - VTr(ZN ) = VTmin (ZN ) - VTN (ZN ) + VTN (ZN ) - VTNer(ZN )
≤ (T — Tmin) + ε
≤ α.
Therefore,
H
P P (吗Ln (ZN) - VTNer(ZN) >α∣G) dα
0
H
P (TN - Tmin > α - ε | G) dα
≤0
≤E
≤E
Z HP(TN - Tmin > α-ε | Hhl) dα ∣∣∣ G
0∣
Z HP(TN - [1+(1+Y)W+ε]Heff > α-ε-Heff(YW+ε) |Hhl) dα
0
G
≤ ε + Heff(γW + ε) + E
∞
P(TN - [1 + (1 + Y)W + ε]Heff > α | Hhl) dα
0
G
.ε + Heff (YW + ε) + β[(1 + Y)W + ε]√Hff,
where the final inequality integrates the tail bound provided in Lemma B.3. Overall, we have that
by rearranging,
(I) . (1 + Heff + βPHff)ε + hγHeff + β(1 + Y)PHffi W.
Bounding (II). By the characterization of π in Lemma B.1,
(1 — Z)E [RTNr(Hhi(∏*))1 [F] I E] ≤ E [RTiNr(Hhi(∏))l [F]] ≤ 暗⑶)，
where the final inequality uses the fact that RT(Hhi(∏)) is the return of ∏ in Mhi, given F.
Concluding. Putting all of the previous bounds together, We find that
VQ* (SO) ≤ Vn (SO) + (1 + Heff + βPHeff )ε + bHeff + β(1 + Y)PHeffi W
+ [Z + P (FC I E)] H.
By setting H to
H = Heff [1 + (1+ γ)W + ε]+ β[(1+ γ)W + ε] JHffogJ ≪ H,
sub-Gaussian tail bounds on TN implies that P FC I E ≤ ζ. Finally, by Lemma B.2,
VjM hl,π (so) = VjMhl,π(so) = V0MTg,π,
where the last equality follows by the construction ofMhl. We thus obtain the desired suboptimality
bound.	□
43
Under review as a conference paper at ICLR 2022
B.3.3 Regret Analysis
As earlier suggested, We now make use of ∏ as a comparator policy in order to prove a regret bound
on a learner making use of the procedure outlined in Section B.2.
Theorem B.1. Assume that EULER generates policies π1, . . . , πN on Mhl, as constructed in Sec-
tion B.2. Then, we have the following regret bound:
N
X Vo(so) - V∏k(S0). Hh2HLMN + Nεsubopt,
k=1
where
εsubopt :=(1+ Heff + Bp Heff )ε + [YHeff + β(I + Y)PHeff] W + CH∙
Proof. Throughout the proof, we consider applying EULER to Mhl where the rewards are scaled by
1/H to ensure that rewards are bounded in [0, 1]. As a result, we can bound G ≤ 1 in the EULER
regret bound in Zanette & Brunskill (2019), since the sum of rewards in Mhl is also the sum of
rewards in M, and scaling by 1/H gives the desired bound on G. Therefore,
N
,Mhl (s0) - V0πk (s0) . H
k=1
Furthermore,
V0*(S0) - V0*,Mhl(so) ≤ V0*(s0) - Vn(so) + VrMhl(S0)- Vo*,Mhl(S0)≤ εsubopt.
We thus obtain the desired result.	口
--H LMHeff N = HtHHsMMN.
Heff
B.4	An Exponential Regret S eparation for a Hierarchy-Oblivious Learner
In this section, we provide proof of the exponential regret separation between a hierarchical learner
and a learner oblivious to the hierarchy. The overall idea behind our proof is the reduction of solving
the family of minimax instances described in Domingues et al. (2021) to a particular family of task
distributions.
B.4	. 1 The Hard Task Distribution Family
In this section, we describe the family of task distributions that forces any meta-training-oblivious
learner to incur exponential regret. For any string s, we write |s| for its length.
We now define the family of binary tree room MDPs MW of depth W . We index a member of this
family by a tuple ('*,a* ,e*), where '* is a binary string of length W 一 1, and a*,e* ∈ {0, 1}. The
MDP M('*,a*,e*) = (S, A, P('*,a*,e*),r, H) corresponding to this tuple is constructed as follows:
State Space S. We create a root state sroot , 2W - 1 states indexed by binary strings of length at
most W - 1 collected into a set T = {s0, s1, s00, s01, . . . }, a gate state sgate, and terminal states
㊀trap,㊀S,㊀F.
Action Space A. The set of available actions at every state is the set {0, 1}.
Transition Dynamics P('*,a*,e*). We define the dynamics as follows:
'δ(Sa)
δ(sta)
bδ(Sgate) + (1 - b)6(㊀traP)
l* ,a* ,e* ) (∙ | s,a)= <
bδ(sgate) + (1 - b)δ(㊀trap)
δ(㊀ S)
3( ㊀ F )
s = sroot
s = st ∈ T, |t| < W - 1
s = st ∈ T, |t| = W - 1,
s = s`*, b ~ Ber (1/2)
s = s`*, b ~ Ber (1/2 + εl [a = a*])
s = sgate, a = e
s = sgate , a 6= e* .
44
Under review as a conference paper at ICLR 2022
Reward Function r. The reward function is r(s, a) = 1 [s =㊀S] + 1 [s = sgate, a = a*].
Having described all the components of every member of MW, all that remains is to construct the
family of task distributions TW. Each member of this family will be indexed by (`*, a*), where `*
and a* are as described above. Then, the task distribution 7('*,α*) ∈ TW chooses uniformly within
the set {M('*,a*,o), Mp,a*,i)}. Note that this implicitly defines the latent hierarchy so that the
clusters are {sroot, sgate,㊀trap} ∪ T, {㊀S}, and {㊀F}. Furthermore, the set of exits for the first
cluster is {(sgate, 0), (sgate , 1)}.
B.4.2 A Family of Hard Instances
In this section, we describe the family of hard instances which we reduce to solving the task distri-
bution above. Intuitively, if an algorithm incurs low regret throughout MW, then it must be able to
quickly find a policy to reliable reach the gate state sgate for any MDP in the family.
Constructing the hard instances. Accordingly, we define a new MDP family NW, which now is
only indexed by (`* , a* ), and is constructed similarly as any member of MW, but ignoring states
outside {sroot, sgate,㊀trap} ∪ T. Additionally, We redefine the reward function r for any member to
be r(s,a) := 1 [s = Sgate]. We note that this is exactly the set of hard tasks used to prove a minimax
regret bound in Domingues et al. (2021).
The lower bound. We state the lower bound result from Domingues et al. (2021), in a slightly
more restricted form for ease of proof and presentation. In particular, we consider the following
more restricted definition of an algorithm:
Definition B.2. Let Hn be the trajectory data generated by playing a policy πn in an MDP M.
That is, Hn = ((s0, a0, r0, s1), (s1, a1,r1, s2), . . . , (sH-1, aH-1, rH-1, sH)), where s0 and a0 are
fixed, rh = r(sh, ah), and sh+ι 〜Pm(∙ | sh, ah). Additionally, we set Ho = 0.
Then, a valid algorithm A for our purposes is one which, for the nth episode, outputs a deterministic,
non-stationary policy π that is solely a function of the current state and action and Sin=-11 Hi . That
is, A does not output policies that adapt to the current running episode.
We again emphasize that this restriction is not necessary but that many algorithms nevertheless
satisfy this condition (including UCBVI and Euler). We then have the following hardness result:
Theorem B.2 (Domingues et al. (2021), Theorem 9, restated). Assume that W ≥ 2 and H ≥ 3W.
Then, for every algorithm A, there exists an MDP M ∈ NW such that
-N	-
EM,A X V0*(sroot) - Vnn(Sroot) & 2W/2VH2N.
n=1
Algorithm 8 The reduction of learning NW to learning MW in Section B.4.3.
1	: procedure PA(M ∈ NW)
2	:	Initialize H0 = 0
3	:	for all n ∈ [N] do
4	:	Obtain πn = A(H0, . . . , Hn-1).
5	:	Play πn in M, get history Gn = ((s0, a0,r0, s1), . . . , (sH-1, aH-1,rH-1, sH)).
6	:	if sW+1 = sgate then
7	SW+1 J SW +1
8	:	for all h = W + 1, . . .,H - 1 do
9	:	if h = W then
10	sh+1 -㊀S if ah = 1 else ㊀F.
11	rh — 1 [ah = 1]
12	:	else
13	:	S0h+1 J S0h, rh0 J rh
14	:	Replace (Sh, ah,rh, Sh+1) with (S0h, ah,rh0 , S0h+1) in Gn
15	Hn — Gn.
45
Under review as a conference paper at ICLR 2022
B.4.3 Proving the Hardness Result
We now use the hardness result in the previous section to demonstrate that no algorithm can incur
sub-exponential regret in W on all tasks in MW. We do so by proving that an algorithm solving all
tasks in MW can be used to construct an algorithm for solving all tasks in NW.
Formally, let A be any algorithm for learning any MDP in MW . We then construct an algorithm PA
for learning any MDP in NW as in Algorithm 8.
Given this reduction, we aim to prove the following result:
Proposition B.2. For any Mp ,&*)∈ NW, we have that
N
EM('* ,a*),PA X V0* (Sroot)- V0∏n (Sroot)
n=1
N
≤ EM('*,a*,ι),A X V00* (Sroot) - Vnn(Sroot)
n=1
To prove this result, We first prove that PA can simulate M('*,&* ,1):
Lemma B.5. For any n, the distribution over (H0 , . . . , Hn) induced by running Algorithm 8 over
M('*,a*) ∈ NW is equal to that induced by running A over M('*,a*,i) ∈ MW.
Proof. We proceed by induction. The result holds trivially for n = 0.
NoW, assume that the result holds for some n. We condition on the histories (H0 , . . . , Hn) Then,
note that both algorithms play the same policy πn+1 , since PA uses A to obtain the next policy. As
a result, by the construction of M('*,&*)and M('*,a*,i), the distribution over (sh, ah,rh, sh+ι) are
equal for h ≤ W. Furthermore, Lines 6 — 14 simulates the dynamics of M('*,a*,i)conditioned
on SW +ι = Sgate, While conditioned on SW +ι =㊀trap, the dynamics of the two MDPS are the
same. Therefore, conditioned on any (H0 , . . . , Hn), the distribution over Hn+1 induced by the tWo
algorithms are also the same. Thus, the claim holds by induction.	口
Finally, We can prove Proposition B.2.
Proof of Proposition B.2. Throughout this proof, We omit the starting state Sroot and the timestep 0
in the value. We prove the result by induction. Clearly, the result holds for N = 0.
Assume that the bound holds for some N . Then, We have that
N+1
EM('*,a*),PA X V*- Vπn
n=1
N
=EM('*,a*),PA X V*- Vπn + EM('*,a*),PA [V*- VπN+1]
n=1
N
≤ EM('*,a*,i),A X V*- Vπn + EM('*,a*),PA [E [V*- VπN+1 | (H0,...,Hn)]],
n=1
Where the final inequality uses the inductive hypothesis and the toWer property of expectation. NoW,
recall from Lemma B.5 that
EM('*,a*),PA [E [V* - VπN+1 | (H0,..., Hn)]]
=EM('*,a*,i),A [E [V* - VπN+1 | (H0,..., Hn)]].
We emphasize that the value functions are still with respect to M('*,a*). However, for any policy ∏
output by A,
V* - Vπ = EM('*,a*),∏ [(H - W -I)I [SW +1 = Sgate]]
≤ EM('*,a*),π [(H - W - I)I [SW +1 = Sgate or aW +1 = 1]]
≤ EM('*,a*,1),π [(H - W - 1)1 [SW +1 = Sgate or aW +1 = 1]].
Note that the right-hand side is the regret in M('* ,a*, 1) for playing ∏. Therefore, since both
algorithms play the same policy ∏n +1,we thus obtain the desired result by induction.	口
46
Under review as a conference paper at ICLR 2022
With Proposition B.2, we can now formally state and prove the separation result:
Theorem B.3. There exists a task distribution T('*,a*) ∈ TW such that an algorithm A, without
access to the meta-training tasks (and thus without access to the hierarchy), incurs expected regret
lower bounded as
EM~∙⅛*,a*? [RegretN(M, A)] & 2W/2√H2N∙
On the other hand, for any task distribution in the family, the hierarchy-based learner P in Section
B.2, with access to a 0-suboptimal hierarchy oracle, achieves regret bounded by √H2N with high
probability on any sampled task.
Proof. Fix any algorithm A. Using Theorem B.2, there exists M('* ,α*)such that
-N	-
EM('* ,a*) ,PA X V0*(sroot)- Vrn(Sroot) & 2W∕2√H2N
n=1
Thus, by Proposition B.2,
-N	-
EM('*,a*,1),A X V0* (Sroot) - V0πn (Sroot) & 2W∕2√HN.
n=1
Note that the proof in Proposition B.2 can be extended for M (`* ,a* ,0)with appropriate modifications
to PA, and thus the same inequality holds. Consequently,
EM~∙⅛*,a*) [RegretN(A)] & 2W∕2√H2N.
On the other hand, with access to the 0-suboptimal hierarchy oracle, observe that the learner only
has to plan at timesteps 0 and W + 1, allowing us to obtain tighter bounds (as Shl is smaller than the
construction in Section B.2). Furthermore, the suboptimality of planning with the hierarchy oracle
is 0 for any task distribution in the family. We thus obtain the desired bound.	口
B.5 A Discussion of Definition 5.2
In this section, we discuss why the values defined in Definition 5.2 control the suboptimality of the
hierarchical learner. In particular, we provide examples of MDPs that satisfy Assumption 5.1, and
are thus in a sense tasks that are “compatible with the hierarchy”, but nevertheless force a hierarchy-
based learner to incur O(H) suboptimality.
B.5.1	(α, β)-UNRELIABILITY
Consider the MDP in Figure 9 with horizon H + 2 and two actions a* and aι. The optimal policy
chooses a* at every step, achieving a value of H - O(1), since
VH +1(SO) = 2 H + 2 VH (SI) = 2H + 4(H - I) + 4 VHH-I(S2)
HH
=H X 2h- 2 X 2h=H - O(1).
h=1	h=1
Now, assume that the MDP has a latent hierarchy so that the set of exits are given by (ti, a) for any
i ∈ [H] and a ∈ A. Clearly, the optimal hierarchy-based learner would always choose (t0, a*) or
(t0, a1) as its high-level action. However, if the agent fails to transition to t0 at the first timestep due
to stochasticity, it will go to the end of the chain, back to S0 and try a* once more. This is because
it already has set a meta-action, and does not replan until an exit is performed. Thus, the optimal
agent on the meta-MDP achieves a value of H/2, and is therefore O(H)-suboptimal, even with a
0-suboptimal hierarchy oracle.
Intuitively, hierarchy-based learners as formulated in Section B.2 fail on the MDP in Figure 9 be-
cause such learners commit to a skill until completion. Thus, when such skills exhibit high variance
in completion times, hierarchy-based learners fare worse than other learners which are able to replan
based on the current state (e.g., in this case, choose another exit if a* fails to take the agent to the
current subgoal). Thus, (α, β)-reliability serves to eliminate such MDPs, ensuring that the skills
corresponding to reaching exits are reliable.
47
Under review as a conference paper at ICLR 2022
Figure 9: An MDP that does not satisfy low (α, β)-unreliability, where a* is in
blue, and a1 is in red (and purple for both actions). State shading represents
state clusters, and rewards are 0 unless indicated otherwise.
B.5.2	γ-GOAL-REACHING SUBOPTIMALITY
In this section, we show that even when a hierarchy-based learner has access to highly reliable skills
as in the previous section, the learner may still incur high hierarchical sub-optimality. Consider the
MDP in Figure 10, where we focus on a single room for simplicity. Furthermore, assume that there
are two exits, one from lH/2 and one from rH/2 . Note that a 0-suboptimal hierarchy oracle has
highly reliable goal-reaching policies for reaching both of these exit states, requiring exactly H/2
timesteps with no stochasticity.
However, given the values assigned to lH/2 and rH/2, the optimal policy would opt to take the
state t, which transitions to either state with probability at least 1/2 in only two environment steps.
Therefore, the optimal policy achieves an optimal value of H - O(1). However, the optimal policy,
in having to commit to exactly one of the exits, will achieve a value of H/2, and thus be O(H)-
suboptimal despite having a perfect hierarchy oracle.
Hierarchy-based learners fail on the MDP in Figure 10 because an optimal policy for goal-reaching
does not necessarily reach a goal as quickly as possible. Thus, γ-goal-reaching suboptimality is a
regularity condition that ensures that this is indeed the case.
Figure 10: An MDP that does not satisfy low γ-goal-reaching suboptimality,
with three actions indicated by red, blue, and purple, and exits lh and rh . The
MDP satisfies (∞, 0)-unreliability, yet nevertheless exhibits high hierarchical
suboptimality.
48