Under review as a conference paper at ICLR 2022
Evaluating Robustness of Cooperative MARL
Anonymous authors
Paper under double-blind review
Ab stract
In recent years, a proliferation of methods were developed for multi-agent rein-
forcement learning (MARL). In this paper, we focus on evaluating the robustness
of MARL agents in continuous control tasks. In particular, we propose the first
model-based approach to perform adversarial attacks for continuous MARL. The
attack aims at adversarially perturbing the states of agent(s) to misguide them to
take damaging actions that lower team rewards. A deep neural network is trained
to represent the dynamics of the environment. We then solve an optimization prob-
lem with the learned dynamics model to yield small perturbations. In addition, we
discuss several strategies to optimally select adversary agents for the attack. Nu-
merical experiments on multi-agent Mujoco tasks verify the effectiveness of our
proposed approach.
1	Introduction
Deep neural networks are known to be vulnerable to adversarial examples, where a small and often
imperceptible adversarial perturbation can easily fool the state-of-the-art deep neural network clas-
sifiers (Szegedy et al., 2013; Nguyen et al., 2015; Goodfellow et al., 2014; Papernot et al., 2016).
Since then, a wide variety of deep learning tasks have been shown to also be vulnerable to adversar-
ial attacks, ranging from various computer vision tasks to natural language processing tasks (Jia &
Liang, 2017; Zhang et al., 2020b; Jin et al., 2020; Alzantot et al., 2018).
Perhaps unsurprisingly, deep reinforcement learning (DRL) agents are also vulnerable to adversarial
attacks, as first shown in Huang et al. (2017) for atari games DRL agents. In Huang et al. (2017), the
authors study the effectiveness of adversarial examples on a policy network trained on Atari games
under the situation where the attacker has access to the neural network of the target policy. In Lin
et al. (2017), the authors further investigate a strategically-time attack by attacking trained agents on
Atari games at a subset of the time step. Meanwhile, Kos & Song (2017) use the fast gradient sign
method (FGSM) to generate adversarial perturbation on the A3C algorithm (Mnih et al., 2016) and
explore training with random noise and FGSM perturbation to improve resilience against adversarial
examples. While the above research endeavors focused on actions that take discrete values, another
line of research tackles a more challenging problem on DRL with continuous action spaces (Weng
et al., 2019; Gleave et al., 2019). Specifically, Weng et al. (2019) consider a two-step algorithm
which determines adversarial perturbation to be closer to a target state using a learnt dynamics
model, and Gleave et al. (2019) propose a physically realistic thread model and demonstrates the
existence of adversarial policies in zero-sum simulated robotics games.
While most of the existing DRL attack algorithms focus on the single DRL agent setting1, in this
work we propose to study the vulnerability of multi-agent DRL, which has been widely applied in
many safety-critical real-world applications including swarm robotics (Dudek et al., 1993), electric-
ity distribution, and traffic control (OroojlooyJadid & Hajinezhad, 2019). In particular, we focus on
the collaborative multi-agent reinforcement learning (c-MARL) setting, where a group of agents is
trained to generate joint actions to maximize certain (team) reward function. We note that c-MARL
is a more challenging yet interesting setting than the single DRL agent setting, as the interactions
between agents commands consideration of additional layers of complications.
Contribution. Our contribution can be summarized as follows:
1We are aware of only one recent work (Lin et al., 2020) on attacking c-MARL agents with discrete action
space, rather than continuous action space considered in our work.
1
Under review as a conference paper at ICLR 2022
•	We propose a new adversarial attack framework on MARL with continuous action space,
where we name it cMBA (model-based attack on c-MARL). The attack comprises two
steps: learning a representation of the environment dynamics and then determining proper
observation perturbations based on the learned dynamics model.
•	We formulate the process of selecting an victim agent as a mixed-integer programming
problem and propose an approximate formulation that can be efficiently solved by a first-
order method.
•	We evaluate our attack framework with two baselines and four multi-agent MuJoCo tasks
(Peng et al., 2020), with number of agents ranging from 2 to 6, to verify the effectiveness
of our approach. Our model-based attack consistently outperforms the two baselines in all
tested environments.
Paper outline. Section 2 discusses related works in adversarial attacks for MARL along with the
problem settings. We describe the main attack framework cMBA in Section 3, including the training
of the dynamics model and solving the sub-optimization problem. In Section 3.3, we introduce a
mixed-integer program to find the optimal set of victim agents and propose an approximate formu-
lation that can be efficiently solved using a first-order optimization method. Section 4 presents the
evaluation of our approach on several multi-agent MuJoCo environments. Therein, we also study
the performance of different variants of our model-based attack as discussed in Section 3.3.
2	Related work
Adversarial attacks for DRL agents. In the c-MARL setting, although there exists a significant
literature focusing on the adversarial training of MARL agents (Phan et al., 2020; 2021; Zhang et al.,
2020a), yet a very few of them addresses the scenario of adversarial attacks in c-MARL. Lin et al.
(2020) propose a two-step attack on a single agent in a c-MARL environment, where they extend
existing methods to generate adversarial examples guided by a learned adversarial policy. The attack
in Lin et al. (2020) is evaluated under the StarCraft Multi-Agent Challenge (SMAC) environment
(Samvelyan et al., 2019) where the action spaces are discrete. To the best of our knowledge, there
has not been work considering adversarial attacks on the c-MARL setting with continuous action
spaces. The closest approach to ours is the enchanting attack in Lin et al. (2017) and the model-
based attack in Weng et al. (2019); however, these two works focused on single DRL agent setting.
The enchanting attack in Lin et al. (2017) uses a video prediction model to predict the future state
and finds perturbation that minimizes a distance function between the predicted future state and a
target state. Similarly, Weng et al. (2019) consider two formulations for observation and action
perturbation that incorporates a learned dynamic model for future state prediction. In addition to the
model-based approach, in this paper, we consider the selection of victim agents, which is unique to
the multi-agent DRL setting.
Training c-MARL agents. One simple approach to train c-MARL agents is to let each agent learn
their own policy independently as in Independent Q-Learning (IQL) (Tan, 1993; Tampuu et al.,
2017). However, this training strategy does not capture the interaction between agents. Alterna-
tively, one can follow a common paradigm called centralized training with decentralized execution
(CTDE) (Oliehoek et al., 2008; Oliehoek & Amato, 2016; Kraemer & Banerjee, 2016). Gupta et al.
(2017) extend DQN (Mnih et al., 2015), TRPO (Schulman et al., 2015), and DDPG (Lillicrap et al.,
2015) to c-MARL, where a parameter sharing neural network policy is trained in a centralized fash-
ion but still allows decentralized behavior across agents. Similar to this, COMA (Foerster et al.,
2018) is an actor-critic method that uses a centralized critic to estimate counterfactual advantage
for decentralized policies. While COMA uses a single critic for all agents, MADDPG (Lowe et al.,
2017) learns a centralized critic for each agent and evaluates on continuous control. Instead of hav-
ing fully centralized critic, DOP (Wang et al., 2020b) and FACMAC (Peng et al., 2020) apply the
value decomposition idea (Sunehag et al., 2017; Rashid et al., 2018; 2020; Son et al., 2019; Wang
et al., 2020a) into multi-agent actor-critic by having a centralized but factorized critic.
2
Under review as a conference paper at ICLR 2022
3	cMBA: Model-based attack for c-MARL
3.1	Problem setting
We consider multi-agent tasks with continuous action spaces modeled as a Decentralized Partially
Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016). A Dec-POMDP
has a finite set of agents N ≡ {1, ∙∙∙ ,n} associated with a set of states S describing global states,
a set of continuous actions Ai , and a set of individual partial observation Oi for each agent i ∈ N.
Given the	current	observation oit ∈ Oi , the action ati	∈ Ai is selected by a parametrized policy
πθi :	Oi → Ai .	The next state agent i moves to is	determined by the state transition function
Pi :	S ×	Ai →	S . After that, agent i receives a reward rti calculated from a reward function
Ri :	S ×	Ai →	R and observes a local observation	oit+1 ∈ Oi correlated with the global state
st+1 ∈ S. In addition, Dec-POMDP is associated with an initial state distribution P0 anda discount
factor γ. In this setting, each agent tries to find the policy that maximizes its own total expected
return Ri = PtT=0 γtrti where T is the sampling horizon.
3.2	Problem formulation
Our goal is to generate adversarial perturbations imposed to the victim agents’ input (state) in order
to deteriorate the total team reward. The added noise encourages the victim agents’ observation to
be close to a desired adversarial state corresponding to low reward. To avoid sampling from the
environment, we use a pre-trained model that learns the dynamics of the environment to predict
the next state from the perturbed observation and current action, then find the suitable noise that
minimizes the distance between the predicted next state and a predefined target state. This process
can be formulated as an optimization problem as follows.
Formally, we consider a multi-agent setting with |N| = n agents, each agent i ∈ N observes state
Si locally and takes action a；. Let St = (s^t1, ∙∙∙ , s;) ∈ S be the global state at time step t which
is concatenated from local states St for each agent i = 1,…，n. We also denote the joint action
at = (a1,…，* concatenated from each agent,s action at. Let Lt ∈ N be the set of victim agents
at time step t, i.e. the set of agents that can be attacked. Let fd : S × A → S be a parametrized
function that approximates the dynamics of the environment, where A is the set of concatenated
actions, one from each Ai. We assume that a pre-trained MARL policy ∏(∙) is given. Let Starget
be the target state which can lead to poor performance to the agent. We denote as an upper bound
on budget constraint w.r.t some p-norm |卜|). The state perturbation Δs = (Δs1, ∙一，∆Sn) (we
suppress the dependence on t of ∆S to avoid overloading the notation) to St is the solution to the
following problem
min	d(st+1, starget)
∆s = (∆s1,…,∆sn)	g
s∙t.	@t+1 = fd(st, at)
ait = π(sit + ∆si )
∆si = 0, ∀i ∈/ Lt
's ≤ St + ∆s ≤ US
k∆skp ≤
(1)
where 0 is a zero vector, and the observation space is constrained within the (vectorized) intervals
between 's and US.
Let us first provide some insights for the formulation (1). For each agent, using the trained policy π,
we can compute the corresponding action ait given its (possibly perturbed) local state sit or sit + ∆si .
From the concatenated state-action pair (st, at), We can predict the next state ^t+ι via the learned
dynamics model fd . Then by minimizing the distance between the predicted next state and the target
state under the budget constraint, we are forcing the agent to move closer to a damaging state in the
next transition. We also note that the last equality constraint indicates that we only allow perturbing
agents within the victim set Lt .
Problem (1) can be efficiently solved by proximal-gradient-based methods. Firstly, by substituting
the definition of at into ^t+ι, (1) is equivalent to
min	d(fd(St, π(St + x)), Starget)
x
s.t. x ∈ Ctp
(2)
3
Under review as a conference paper at ICLR 2022
where Cp := {x = (xι,…，xn) ： IlxIlp ≤ e, 's 一 St ≤ X ≤ US — St and Xi = 0 for i ∈ Lt}. If
we choose the distance function as d(a, b) = ka - bk2 * *, we can use the projected gradient descent
(PGD) algorithm (Nesterov, 2003) to solve (2). The PGD iteration to update yk at iteration k can be
described as
yk+1 =ProjCp [yk — ηVyd(fd(st,∏(st + y)), Starget)]	(3)
where ProjCp (∙) is the projection to the convex set Cp and η is the learning rate. The projection is
simple to calculate since Ctp is the intersection ofa unit ball in the p-norm and a box.
The whole attack process can be summarized in Algorithm 1.
Algorithm 1 cMBA algorithm at timestep t
1:	Initialization:
Given St, Starget, π, fd, Lt ; initialize ∆S0 ; choose learning rate η > 0
2:	For k := 0,…，K — 1 do
3:	Compute at = (a；,…，a?) as
π(Sit +∆Si) ifi ∈ Lt
π(Sit)	otherwise.
4:	Compute ^t+ι = fd(st,at)
5:	Update ∆S as
△■sk + 1 = ProjCt [δSk 一 nV∆sd(st+1, Starget)]
6:	End For
Learning dynamics model. The most important factor that affects the solution of (2) is the quality
of the learned dynamics model fd . If the dynamics is known, we can solve (2) exactly so that
the solution is indeed the optimal perturbation. However, it is hardly the case in practice because
we often approximate the dynamics using function approximators such as neural networks. The
parameter w for fd is the solution of the following optimization problem
mwin X fd(Sicur, w) — Sinext2	(4)
i∈D
where D is a collection of state transition {(Sicur , Sinext)}|iD=|1 where Snext is the actual state that
the environment transitions to after taking action at determined by a given policy. In particular, we
separately collect transitions using the pre-trained policy π and a random policy to obtain Dtrain
and Drandom . Then the dataset D is built as D = Dtrain ∪ Drandom . Problem (4) is a standard
supervised learning problem and can be solved using gradient-based learning. We describe the full
process of training the dynamics model in Algorithm 2 where the GradientBasedUpdate step
is any gradient-descent-type update.
Algorithm 2 Training dynamics model
1:	Initialization:
Given pre-trained policy πtr and a random policy πrd ; initialize w0
2:	Form D := Dtrain ∪
Drandom by collecting a set of transitions Dtrain
and Drandom using
policy πtr and πrd, respectively.
3:	For k := 0,1,…do
wk+1 = GradientBasedUpdate(D, wk)
4:	End For
3.3 Victim agent selection
In this subsection, we discuss different strategies to select victim agents when performing adversarial
attacks in the MARL setting. Suppose that we would like to perform an attack on any agent but the
budget on the victim agents is na ≤ n. In other words, an attacker can only choose a subset of
4
Under review as a conference paper at ICLR 2022
agents to attack during each time step, and the goal of an attacker is to choose the most vulnerable
agents so that the total reward is utmost affected. We note that this scenario is unique in the setting
of multi-agent DRL setting, as in the single DRL agent setting we can only attack the same agent all
the time. To start with, we first formulate a mixed-integer program to perform the attack on a set of
optimally victim agents as below:
min∆s,w	d(st+1, Starget)
s.t.	@t+1 = fd(st, at)
at = π(st + h∆s, wi)
's ≤ St + ∆s ≤ US	(5)
k∆skp ≤
wi ∈ {0, 1}
i wi = na
where we introduce a new binary variable w to select the appropriate agents’ input to perturb and
na is the total number of victim agents we can attack.
Due to the existence of the new binary variable, problem (5) is much harder to solve than before.
We instead solve a proxy of (5) as follows
min∆s,θ	d(st+1, starget)
st	Wt+1 = fd(st, at)
at = π(St + h∆S, W(St, θ)i)
's ≤ St + ∆s ≤ US
(6)
k∆Skp ≤
0≤W(St,θ) ≤1
where W (∙,θ) is a function parametrized by θ that takes current state as input and outputs the
weight to distribute the noise to each agent. Suppose We represent W(∙, θ) by a neural network, We
can rewrite the formulation (6) as
min∆s,θ	d(fd(St, π(St + h∆S, W(St, θ)i)), Starget)
s.t.	∆S ∈ Ctp	(7)
because the last constraint in (6) can be enforced by using a softmax activation in the neural network
W(∙,θ). As a result, (7) can be efficiently solved by using PGD. We present the pseudo-code of
the attack in Algorithm 3. After K steps of PGD update, we define the index in-j as the j-th
largest value within W (st,θK) ∈ Rn, i.e. we have Wi(n)(St ,θκ) ≥ Wi(n-i)(St ,θκ) ≥ ∙∙∙ ≥
Wi(1) (St, θK). Let Ij be the index set of top-j largest outputs of the W(St, θK) network. The
1
final perturbation returned by our ViCtlm agent selection strategy will be Δs = ((Δs)1, ∙∙∙ , (∆S)n)
where (∆cS)i = 0 ifi ∈/ Ina and (∆cS)i = (∆SK)i ifi ∈ Ina.
Algorithm 3 cMBA with victim agent selection at timestep t
1:	Initialization:
Given St, Starget, π, fd, na ; initialize ∆S0 ; choose learning rate η > 0 λ > 0.
2:	For k := 0,…，K 一 1 do
3:	Compute at = π(St + h∆S, W(St, θ)i)
4:	Compute ^t+ι = fd(St,at)
5:	Update Δs: ∆Sk+1 = ProjCp [∆Sk — η^∆sd(St+1, Starget)]
6:	Update θ: θk+1 = θk — λVθd(^t+1, Starget)
7:	End For
8:	ComputeZna = {i(n),…，i(n-na)} SUCh that
Wi(n) (St, θK ) ≥ Wi(n-1) (St, θK ) ≥ …≥ Wi ⑴(St, θK )
9:	Return Δs = ((Δs)1, ∙∙∙ , (∆S)n) such that
(∆cS)i =	(∆SK)i ifi∈Ina
0 otherwise
Remark 3.1 For this attack, we assume each agent i has access to the other agent’s observation to
form the concatenated state St.
5
Under review as a conference paper at ICLR 2022
4 Experiments
We perform the attack on various multi-agent MuJoCo (MA-MuJoCo) environments including
Walker2d (2x3), HalfCheetah (2x3), HalfCheetah (6x1), and Ant (4x2). The pair environment
name (config) indicates the name of MuJoCo environment along with the agent partition,
where a configuration of 2x3 means there are in total 2 agents and each agent has 3 actions. Note
that we cannot directly apply methods such as the fast gradient sign method (FGSM) or JSMA (an
attack using saliency map) as in Lin et al. (2020), since those methods require a “target” action
which is not available in continuous action space. Therefore, we consider two baselines: Uniform
and Gaussian baselines where the perturbation follows either Uniform distribution U (-, ) or
Normal distribution N(0, ).
Variants of model-based attack. We consider the following variants of our model-based attack:
•	Model-based attack on fixed agents: perform Algorithm 1 using a fixed set of victim agents
L.
•	Best model-based attack on fixed agents: among the model-based attack on fixed agents,
pick the subset that achieves the best performance.
•	Model-based attack on random agents: perform Algorithm 1 with Lt , which is sampled
uniformly from N such that |Lt | = na (na is the total victim agents).
•	Model-based attack with learned victim selection: perform Algorithm 3 to select vulnerable
agents and perform the attack on them.
•	Greedy victim selection: perform a sweep over subsets of agents with size na . For each
subset, perform Algorithm 1 to obtain proper perturbation and retrieve the corresponding
objective value (distance between predicted state and target state). Select the subset corre-
sponding to the lowest objective value as the victim agents.
Experiment setup. We first use MADDPG (Lowe et al., 2017) to train MARL agents for the
four MA-MuJoCo environments listed above. Using the trained agents, we collect datasets con-
taining one million transitions to train the dynamics model for each environment. The dynamics
model is a fully connected neural network with three hidden layers of 1000 neurons. We also use
a fully-connected neural network for W (st, θ) in (6) with two hidden layers of 200 neurons. We
use AdamW (Loshchilov & Hutter, 2017) as the optimizer and select the best learning rate from
{1, 5} × {10-5, 10-4, 10-3} (the best learning rate is the one achieving lowest prediction error on
a test set of 80, 000 samples). For our model-based attack, we run PGD for K = 30 steps to solve
(2) and (5). We perform each attack over 16 episodes then average the rewards. We also illustrate
the standard deviation of rewards using the shaded area in the plots.
Model-free baselines vs model-based attack. In this experiment, we run the two baseline attacks
along with our model-based attack on the four MA-MuJoCo environments, in which there is only
one victim agent (na = 1). Figure 1 illustrates the performance when we perform these attacks
on specific agents under multiple budget levels using '∞-norm. For a fixed agent, our model-based
attack consistently outperforms the two baselines. In particular, our model-based attack yields much
lower rewards under low budget constraints (small ) compared to the two baselines.
To better visualize the performance difference, 2 illustrates the environment with and without attacks
captured at different time-steps. From Figure 2, our model-based attack is able to make the MuJoCo
agent flip, which terminates the episode at the 409-th timestep. The episode length and total rewards
for each variant are: No attack(1000, 2644.60), Uniform(1000, 1758.97), Gaussian(891, 1399.51),
Ours(409, 287.06).
We also investigate how the state values change during these attacks. Figure 3 presents different
recordings of state values under adversarial attacks compared to no attack. Consider state index 8,
which represents the horizontal velocity of the agent. For the HalfCheetah environment, as the
goal is to make the agent move forward as fast as possible, we expect the reward to be proportional
to this state value. From Figure 3, all three attacks have fairly sparse red fractions across timesteps,
which result in a much lower reward compared to the no-attack setting. Among the three attacks,
our model-based ones appear to have the most sparse red fractions leading to the lowest rewards. In
addition, the model-based attack appears to show its advantage in environments with more agents.
6
Under review as a conference paper at ICLR 2022
3000-
Mean rewards - Walker2d (2x3)
5000
Mean rewards - HaIfCheetah (2x3)
2500-
2000
4000
Ooo
Ooo
Ooo
3 2 1
PJeM* CSΞ
PJeMS Ue"W
0.0	0.1	0.2	0.3	0.4	0.5
Noise Level, ∣∣∆s∣∣α, ≤ε
Uniform noise - Agent 0	---Gaussian noise - Agent 1
Uniform noise - Agent 1	--- eMBA (ours) - Agent 0
--- Gaussian noise - Agent 1 eMBA (ours) - Agent 1
Mean rewards - HaIfCheetah (6xl)
0：0	0：l	0：2	03	0：4	o：5
Noise Level, ∣∣∆s∣∣,≤ε
Uniform noise - Agent 0	- Gaussian noise - Agent 1
Uniform noise - Agent 1	-- eMBA (ours) - Agent 0
--- Gaussian noise - Agent 0	— eMBA (ours) - Agent 1
Mean rewards - HaIfCheetah (6xl)
3000
2500-
2000
1500-
1000
500-
PJeM* ueωw
3000
2500-
2000
1500-
1000-
500-
0.2	0.3
Noise Level, ∣∣∆s∣∣oo≤ε
0.4	0.5
3000
1000
-1000
2000
0Λ	0：2	03	0：4	0：5
Noise Level, ∣∣∆s∣∣β,≤ε
Uniform noise - Agent 0	----Gaussian noise - Agent 2
Uniform noise - Agent 1	---- eMBA (ours) - Agent 0
Uniform noise - Agent 2	— eMBA (ours) - Agent 1
Gaussian noise - Agent 0	---eMBA (ours) - Agent 2
Gaussian noise - Agent 1
Mean rewards - Ant (4x2)
Uniform noise - Agent 3
Uniform noise - Agent 4
Uniform noise - Agent 5
---- Gaussian noise - Agent 3
----Gaussian noise - Agent 4
----Gaussian noise - Agent 5
---- eMBA (ours) - Agent 3
eMBA (ours) - Agent 4
----eMBA (ours) - Agent 5
Mean rewards - Ant (4x2)
3000
PJeMw Ue"W
2000
-1000
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
Noise Level, ∣∣∆s∣∣≡ ≤ ε
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
Noise Level, ∣∣Δs∣∣ib ≤ε
-	Uniform noise - Agent 0	----Gaussian noise - Agent 1
•	Uniform noise - Agent 1	----- eMBA (ours) - Agent 0
-	Gaussian noise - Agent 0	----eMBA (ours) - Agent 1
— Uniform noise - Agent 2	----Gaussian noise - Agent 3
— Uniform noise - Agent 3	---- eMBA (ours) - Agent 2
---- Gaussian noise - Agent 2	---eMBA (ours) - Agent 3
PJeM* ueωw
Figure 1: Model based attack vs baseline attacks on fixed agents.
7
Under review as a conference paper at ICLR 2022
Step O Step 80 Step 160 Step 240 Step 320 Step 400 Step 409
UI』。S=Un Cα~ww3αo £30
Figure 2: Various attacks on Agent 0 in Ant (4x2) environment with k∆sk∞ ≤ 0.1. It can be seen
that the agent flip at the end of episode under our model-based attack, demonstrating the effective-
ness of our algorithm.
In particular, our approach results in lower rewards under a smaller budget as seen in HalfCheetah
(6x1) and Ant (4x2) environments.
Iimesteps
HalfCheetah 2x3 - Gausslan noise, ∣∣Δs∣∣<x>≤0.2 (Agent O), Reward: 1782.
Figure 3: Recordings of state values in an episode under different attack on Agent 0 in HalfCheetah(
2x3) environment.
8
Under review as a conference paper at ICLR 2022
Effectiveness of learned victim selection. We further demonstrate the performance of our model-
based attack with and without learned victim selection on one victim agent. Figure 4 illustrates
the performance of four model-based variants: the best performing model-based attack on the fixed
agent (Best Fixed Agent), model-based attack on the random agent (Random Agent), model-based
with learned victim selection as in Algorithm 3 (Learned victim Selection), and model-based with
greedy victim selection (Greedy). Surprisingly, even though the greedy approach employs a brute-
force strategy to select the agent with the closest distance, its performance reveals that selecting
victim agents greedily might not be a good strategy. The learned victim selection seems to be
better than the random or greedy strategy and is comparable with the random one in HalfCheetah
environment. The best-fixed agent variant has good performance under low budget constraints in
HalfCheetah (6x1) and Ant (4x2) environments. We want to emphasize that although the learned
victim selection strategy in Section 3.3 is sub-optimal, it works well in these four environments.
3ooo OUr CMBA Variants - Walker2d (2x3)
2500
PJeM * Ue"W
2000
1500
1000
500
0.0	0.1	0.2	0.3	0.4	0.5
Noise Level, ∣∣∆s∣∣.> ≤ε
Best Fixed Agent - Learned Victim Selection
---Random Agent ------ Greedy
Our eMBA Variants - HaIfCheetah (6xl)
3000
2000
1000
4000
5θoo- θur CMBA Variants - HaIfCheetah (2x3)
PJeM* Ue"W
0：0	6.1	0.2	0.3 OA 0.5
Noise LeVel, ∣∣∆s∣∣s ≤ε
Best Fixed Agent -- Learned Victim Selection
----Random Agent ------- Greedy
Our eMBA Variants - Ant (4x2)
3000
2000
1500
1000
500
2500
PJeM* Ue"W
0：0	0.1	0.2	0.3 OA
Noise Level, ∣∣∆s∣∣a> ≤c
Best Fixed Agent ----- Learned Victim Selection
----Random Agent ---------- Greedy
PJeM* Ue"W
-1000
3000
00
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
Noise Level, ∣∣∆s∣∣β> ≤ε
Best Fixed Agent ----- Learned Victim Selection
----Random Agent ---------- Greedy
Figure 4: Performance of different variants of our proposed model-based attack (c-MBA) with dif-
ferent choices on victim agents.
0：5
5 Conclusions
In this paper, we propose a new attack algorithm named cMBA for evaluating the robustness of
c-MARL environment with continuous action space. Our cMBA algorithm is the first to consider
the c-MARL and the adversarial perturbation is computed by solving an optimization problem with
learned dynamics models. The cMBA approach outperforms the other baselines attack by a large
margin under 4 multi-agent MuJoCo environments especially ones with larger number of agents.
Unique to multi-agent setting, we also study different strategies to select victim agents. Extensive
experiment results on standard c-MARL benchmarks show that our proposed model-based attack
and victim-selection strategy can successfully degrade the performance of well-trained c-MARL
agents while outperforming other baselines by a large margin. A future direction is to consider a
timed attack strategy where the perturbation is added at certain timesteps. We can also consider
longer planning step, i.e. trying to reach a target state within the next T steps instead of 1.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
This paper does not contain ethics concerns.
References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating natural language adversarial examples. arXiv preprint arXiv:1804.07998,
2018.
Gregory Dudek, Michael Jenkin, Evangelos Milios, and David Wilkes. A taxonomy for swarm
robots. In Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS'93), volume 1,pp. 441-447. IEEE, 1993.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adver-
sarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66-83. Springer, 2017.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017.
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328, 2017.
Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline
for natural language attack on text classification and entailment. In Proceedings of the AAAI
conference on artificial intelligence, volume 34, pp. 8018-8025, 2020.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint
arXiv:1705.06452, 2017.
Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing, 190:82-94, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot. On
the robustness of cooperative multi-agent reinforcement learning. In 2020 IEEE Security and
Privacy Workshops (SPW), pp. 62-68. IEEE, 2020.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tac-
tics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748,
2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
10
Under review as a conference paper at ICLR 2022
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 427-436, 2015.
Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs.
Springer, 2016.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value func-
tions for decentralized pomdps. Journal of Artificial Intelligence Research, 32:289-353, 2008.
Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep rein-
forcement learning. arXiv preprint arXiv:1908.03963, 2019.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European sympo-
sium on security and privacy (EuroS&P), pp. 372-387. IEEE, 2016.
Bei Peng, Tabish Rashid, Christian A Schroeder de Witt, Pierre-Alexandre Kamienny, Philip HS
Torr, Wendelin Bohmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. arXiv preprint arXiv:2003.06709, 2020.
Thomy Phan, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, Bernhard Kempter, Cornel Klein,
Horst Sauer, Reiner Schmid, Jan Wieghardt, Marc Zeller, et al. Learning and testing resilience
in cooperative multi-agent systems. In Proceedings of the 19th International Conference on Au-
tonomous Agents and MultiAgent Systems, pp. 1055-1063, 2020.
Thomy Phan, Lenz Belzner, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, and Claudia Linnhoff-
Popien. Resilient multi-agent reinforcement learning with adversarial value decomposition. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11308-11316,
2021.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforce-
ment learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob N Fo-
erster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent rein-
forcement learning. J. Mach. Learn. Res., 21:178-1, 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 5887-5896. PMLR, 2019.
11
Under review as a conference paper at ICLR 2022
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PloS one, 12(4):e0172395, 2017.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
ofthe tenth international conference on machine learning, pp. 330-337, 1993.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020a.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy multi-
agent decomposed policy gradients. arXiv preprint arXiv:2007.12322, 2020b.
Tsui-Wei Weng, Krishnamurthy Dj Dvijotham, Jonathan Uesato, Kai Xiao, Sven Gowal, Robert
Stanforth, and Pushmeet Kohli. Toward evaluating robustness of deep reinforcement learning
with continuous control. In International Conference on Learning Representations, 2019.
Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust multi-
agent reinforcement learning with model uncertainty. In NeurIPS, 2020a.
Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. Adversarial attacks on
deep-learning models in natural language processing: A survey. ACM Transactions on Intelligent
Systems and Technology (TIST), 11(3):1-41, 2020b.
12
Under review as a conference paper at ICLR 2022
A Mode Details on Experiment Setup in Section 4
Specifying target state for each environment. To perform our model based attack, we need to
specify a target state that potentially worsens the total reward. In multi-agent MuJoCo environments,
each agent has access to its own observation of the agent consisting the position-related and velocity-
related information. The position-related information includes part of x, y, z coordinates and the
quarternion that represents the orientation of the agent. The velocity-related information contains
global linear velocities and angular velocities for each joint in a MuJoCo agent. We refer the reader
to Todorov et al. (2012) for more information about each MuJoCo environment. Now we describe
the design of this target state for each environment as follows:
•	Walker (2x3) environment: Since the episode ends whenever the agent falls, i.e. the z
coordinate falls below certain threshold. In this environment, the target state has a value of
0 for the index that corresponds to the z coordinate of the MuJoCo agent (index 0).
•	HalfCheetah (2x3) and HalfCheetah (6x1) environments: As the goal is to make agent
moves as fast as possible, we set the index corresponding to the linear velocities to 0 (index
8).
•	Ant (4x2) environment: As the agent can move freely in a 2D-plan, we set the index
corresponding to the x, y linear velocities to 0 (indices 13 and 14).
B Additional Experiments
In this section, we present experimental results in addition to ones presented in Section 4.
Figure 5 illustrates the environment with and without attacks captured at different timesteps. From
Figure 5, our model based attack is able to make the MuJoCo agent fall down which terminates
the episode at the 65-th timestep. The episode length and total rewards for each variant are: No
attack(478, 1736.44), Uniform(382, 1037.73), Gaussian(90, 32.24), Ours(63, -34.26). Figure 6
illustrates how the state values change during an episode. These state values correspond to the agent
shown in Figure 5. If we look at how the noise values change as in Figure 7, the noise generated by
our approach appears to maximize the permissible noise budget.
In addition to the '∞-norm budget constraint, We also evaluate adversarial attacks using the 'ι-
norm constraint. Note that using 'ι-norm for budget constraint is more challenging as the attack
needs to distribute the noise across all states while in the '∞-norm the computation of perturba-
tion for individual state is independent. Figure 8 illustrates an episode of a Walker agent With and
without attack. Our cMBA approach is able to make the agent fall at the 255-th timestep with the
episode length and rewards for each setting as: No attack(661, 2513.57), Uniform(403, 1144.77),
Gaussian(455, 1637.16), Ours(253, 759.85). From the noise values in Figure 7, our cMBA
method appears to put noise on a few selective states at each timestep, similar to the Gausian noise
setting.
Additional experiments using the approach in Lin et al. (2020) for continuous action spaces:
In this experiment, we follow the approach in Lin et al. (2020) in the Ant (4x2) environment where
we train an adversarial policy for one agent trying to minimize the total team reward while the
remaining agents use the trained MARL policy. The adversarial policy is trained for 1 million
timesteps. The results using this approach compared with our approach and the two baselines are
presented in Figure 11.
From Figure 11, our cMBA approach outperforms Lin et al. (2020)’s approach as it is able to achieve
lower team rewards when attacking the same agent. We also note that Lin et al. (2020)’s approach
is better than the two baselines using Uniform and Gaussian noise.
Effect of using ∣∣∙kι budget constraint: In this experiment, we replace the ∣∣∙k∞ by the ∣∣∙kι for
budget constraint. Using ∣∣∙∣ι constraint will produce a significant different adversarial noise pattern
13
Under review as a conference paper at ICLR 2022
Step O
Step 15 Step 30 Step 45 Step 60 Step 65
u~ssn",°
ULI2≡n
S-Ino
Figure 5: Various attacks on Agent 0 in Walker (2x3) environment with k∆sk∞ ≤ 0.2.
Walker 2x3 - No attack (Agent O), Reward: 1736.44
16u12108 -
Xup
-10.0
7.5
5.0
-2.5
0.0
-2.5
-5.0
-7.5
10	20	30	40	50	60
10.0
7.5
5.0
-2.5
0.0
-2.5
-5.0
-7.5
10.0
7.5
5.0
-2.5
0.0
-2.5
-5.0
-7.5
16
14
12
10
I 8
- 6
4
2
0
Walker 2x3 - eMBA (ours), ∣∣∆s∣∣α, <0.2 (Agent O), Reward: -34.26
o
10
20
40
50
60
30
Iimesteps
rl0.0
1 7.5
I 5.0
2.5
0.0
--2.5
I [-5.0
Γ75
■--10.0
Figure 6: Record of state values in an episode under different attack on Agent 0 in Walker (2x3)
environment with k∆sk∞ ≤ 0.2.
14
Under review as a conference paper at ICLR 2022
Figure 7: Record of noise values in an episode under different attack on Agent 0 in Walker (2x3)
environment with with k∆sk∞ ≤ 0.2.
Step O Step 50 Step IOO Step 150 Step 200 Step 250 Step 255
Figure 8: Various attacks on Agent 0 in Walker (2x3) environment with k∆sk1 ≤ 0.5.
15
Under review as a conference paper at ICLR 2022
Index	Index	Index	Index
Walker 2x3 - No attack (Agent O), Reward: 2513.57
16
14
12
10
8
6
0	50
Walker 2x3 -
16	"
14
10
100	150	200
Timesteps
Uniform noise, ∣∣∆s∣∣ι ≤ 0.5 (Agent 0)f Reward: 1144.77
8 6 4 2 0
2
0
0
o
50
100
150
200
16
14-
12-
10-
8
6
16
14-
12-
10-
8
6
2
0
Timesteps
Walker 2x3 - Gaussian noise, ∣∣∆s∣∣ι ≤ 0.5 (Agent 0)f Reward: 1637.16
50
200
250
Figure 9: Record of state values in an episode under different attack on Agent 0 in Walker (2x3)
environment with with k∆sk1 ≤ 0.5.
100	150
Timesteps
Timesteps
Walker 2x3 - eMBA (ours)f ∣∣∆s∣∣ι ≤ 0.5 (Agent 0)f Reward: 759.85
16
Under review as a conference paper at ICLR 2022
Walker_2x3 - Uniform noise, ∣∣∆s∣∣ι ≤ 0.5 (Agent O), Reward: 1144.77
642086420
Illl
Xdp
16
14-
12-
10-
I 8
~ 6
Walker 2x3 - eMBA (ours)f ∣∣∆s∣∣ι ≤ 0.5 (Agent O)f Reward: 759.85
0
50
100	150	200	250
Timesteps
0.4
0.2
0.0
-0.2
-0.4
Figure 10:	Record of noise values in an episode under different attack on Agent 0 in Walker (2x3)
environment with with ∣∆s∣1 ≤ 0.5.
PJeM* cnωs
-IOOO-
Mean rewards - Ant (4x2)
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
Noise Level, ∣∣∆s∣∣a, ≤ε
Mean rewards - Ant (4x2)
-1000
0.025
0.050 0.075 0.100 0.125 0.150 0.175
Noise Level, ∣∣∆s∣∣. ≤f
0.200
PJeM* cnωs
—— Uniform noise - Agent 0	---- Lin et al (2020) - Agent 0
--Uniform noise - Agent 1	— Lin et al (2020) - Agent 1
---- Gaussian noise - Agent 0	--- eMBA (ours) - Agent 0
---Gaussian noise - Agent 1	--eMBA (ours) - Agent 1
—— Uniform noise - Agent 2
一 Uniform noise - Agent 3
— Gaussian noise - Agent 2
-- Gaussian noise - Agent 3
—Lin et al (2020) - Agent 2
--Lin et al (2020) - Agent 3
---- eMBA (ours) - Agent 2
---eMBA (ours) - Agent 3
Figure 11:	Adversarial attacks using ∣∣∙k∞ budget constraint in Ant (4x2) environment.
17
Under review as a conference paper at ICLR 2022
compared to ∣∣∙k∞ as producing adversarial noise when using ∣∣∙k∞ is independent for each state
while it is not the case for ∣∣∙k「We run different attacks on two environments, HalfCheetah (6x1)
and Ant (4x2), and the results are presented in Figure 12 and Figure 13. From these figures, cMBA
outperforms other baselines. In particular, it is able to achieve much lower rewards under smaller
budget constraint which shows the advantage of our approach.
Mean rewards - HaIfCheetah (6xl)	Mean rewards - HaIfCheetah (6xl)
PJeM* ue,υ≡
— Uniform noise - Agent 0	---Gaussian noise - Agent 2
一 Uniform noise - Agent 1	---- eMBA (ours) - Agent 0
—— Uniform noise - Agent 2	—— eMBA (ours) - Agent 1
一 GaUSSian noise - Agent 0	---eMBA (ours) - Agent 2
— GaUSSian noise - Agent 1
PJeM* ue,υ≡
—— Uniform noise - Agent 3
一 Uniform noise - Agent 4
—-Uniform noise - Agent 5
---- Gaussian noise - Agent 3
-- Gaussian noise - Agent 4
Gaussian noise - Agent 5
---- eMBA (ours) - Agent 3
----eMBA (ours) - Agent 4
----eMBA (ours) - Agent 5
Mean rewards - Ant (4x2)
p,JeMlυH ueα,w
-500-
-1000-
2	4	6	8	IO
Noise Level, ∣∣∆s∣∣ι≤ε
— Uniform noise - Agent 0	---Gaussian noise - Agent 1
—Uniform noise - Agent 1	--- eMBA (ours) - Agent 0
---- Gaussian noise - Agent 0	--eMBA (ours) - Agent 1
-500
-1000
Figure 12: Adversarial attacks using ∣∣∙kι budget constraint in HalfCheetah (6x1) environment.
3000
2500
2000
Mean rewards - Ant (4x2)
p,JeMlυH ueα,w
2	4	6	8	10
Noise Level, ∣∣∆s∣∣ι ≤e
—— Uniform noise - Agent 2	— Gaussian noise - Agent 3
—— Uniform noise - Agent 3	---- eMBA (ours) - Agent 2
---- Gaussian noise - Agent 2	---eMBA (ours) - Agent 3
Figure 13: Adversarial attacks using ∣∣∙kι budget constraint in Ant (4x2) environment.
Adversarial attacks using dynamics model with various accuracy: In this test, we compare the
performance of our attack when using trained dynamics model with different mean-squared error
(MSE). We use the Ant (4x2) environment and consider the following 3 dynamics models:
•	The first model is trained using 1 million samples for 100 epochs and select the model with
the best performance. We denote this model ”1M - Best epoch”.
•	The second model is trained using 1 million samples for only 1 epoch. We denote this
model ”1M - 1st epoch”.
•	We further reduce the number of samples used for training the dynamic model to only
200,000 and train for only 1 epoch. We denote this model ”200K - 1st epoch”.
18
Under review as a conference paper at ICLR 2022
These models are evaluated on a predefined test set consisting of 100,000 samples. The test MSE of
these models are 0.33, 0.69, 0.79, respectively, with the initial test MSE of 1.241.
Effect Of CiynamiCS model accuracy - Ant (4斗2)
3000
Ooooo
PJeM* Ue"W
0.025
0.050 0.075 0.100 0.125 0.150 0.175 0.200
Noise Level, ∣∣∆s∣∣β≤ε
IM - Best epoch ----- IM - 1st epoch ----- 200K - 1st epoch
PJeM* Ue"W
EffeCt Of CiynamiCS model accuracy - Ant (4夕2)
3000
Oooooo
4	6
Noise Level, ∣∣∆s∣∣ι≤ε
-IOOOj-
10
IM - Best epoch -----IM - 1st epoch ------ 200K - 1st epoch
Figure 14: Adversarial attacks using 3 dynamics models in Ant (4x2) environment.
Figure 14 depicts the attacks using these three models on the same agent in Ant (4x2) environment
using ∣∣∙k∞ budget constraint. Interestingly, the dynamics model trained with only 200,000 sam-
ples for 1 epoch can achieve comparable performance with the other two models using much more
samples.
Comparison between attacking all agents vs attacking 1 agent: In this experiment, we compare
performance of different approaches when attacking one victim agent or simply perturbing all agents
under the same budget constraint. We compare different approaches in the Ant (4x2) environment.
In particular, we use the two baselines and our cMBA approach to attack each agent out of 4 agent,
denoted as (agent i) for i = 0, 1, 2, 3. We also use these approach in which we simultaneously
perturb the inputs to all agents, denoted as (4 agents). In addition, we also illustrate the performance
of our victim selection scheme when we only want to attack either 1 or two agents at the same
time. We report the mean and standard deviation of rewards using these approach with several
values of budget level ε in Table 1 where we use ∣∣ ∙ ∣∣∞ for the budget constraint. From Table 1,
the performance is getting better as the attacker is stronger, i.e. more agents are attacked at the
same time. This makes sense as for ∣∣ ∙ ∣∞, the adversarial noise for each agent can be computed
independently.
Now, We consider the k ∙ ∣∣ι budget constraint. This is a more interesting setting as the total noise
budget is fixed the adversarial noise of one agent will affect one from the other. The results are
shown using several values of budget level ε in Table 2. We can observe that in this case attacking
more agents is not always effective.
19
Under review as a conference paper at ICLR 2022
Table 1: Comparison between adversarial attacks on a single agent and all agents in Ant (4x2)
environment under ∣∣∙∣∣∞ ≤ ε budget constraint.
	Rewards: Mean (standard deviation)			
MethOdS	ε = 0.025	ε = 0.05	ε = 0.075	ε = 0.1
Uniform noise (agent 0)	2332 (594)	2296 (88)	1773 (941)	1542 (554)
Uniform noise (agent 1)	2233 922)	2033 (565)	1579 (856)	1293 (385)
Uniform noise (agent 2)	2028 (695)	1713 (649)	988 (650)	322 (652)
Uniform noise (agent 3)	2579 (66)	2484 (73)	2159 (623)	2208 (279)
Uniform noise (4 agents)	2089 (450)	1093 (753)	299 (781)	62 (921)
Gaussian noise (agent 0)	2450 (134)	2030 (405)	1660 (663)	1067 (536)
Gaussian noise (agent 1)	2204 (709)	1992 (337)	1447 (406)	1141 (381)
Gaussian noise (agent 2)	2256 (297)	1313 (977)	287 (923)	154 (570)
Gaussian noise (agent 3)	2550 (83)	2331 (567)	1927 (1250)	2115(312)
Gaussian noise (4 agents)	2022 (303)	761 (1083)	86 (688)	-133 (809)
CMBA (agent 0)	923 (340)	244 (737)	516(737)	-8 (306)
CMBA (agent 1)	1417 (48)	553 (269)	108 (303)	-60 (309)
cMBA (agent 2)	227 (955)	-652 (1097)	-241 (660)	-631 (911)
cMBA (agent 3)	2024 (87)	861 (1298)	567 (1012)	-172 (725)
CMBA (4 agents)	1116(379)	314(734)	72 (407)	-239 (281)
Leaned Victim Selection (1 agent)	-165 (805)	624 (103)	256 (201)	-2 (123)
Learned Victim Selection (2 agent)	405 (308)	-176(853) 一	-588 (1026) 一	-325 (730)
Table 2: Comparison between adversarial attacks on a single agent and all agents in Ant (4x2)
environment under ∣∣ ∙ kι ≤ ε budget constraint.
	Rewards: Mean (standard deviation)			
Methods	e = 1.5	e = 2.0	e = 2.5	e = 3.0
Uniform noise (agent 0)	2189(151)	2093 (148)	1615 (524)	1421 (780)
Uniform noise (agent 1)	2039 (139)	1715 (282)	1387 (586)	1243 (303)
Uniform noise (agent 2)	1542 (826)	1073 (755)	581 (920)	436 (317)
Uniform noise (agent 3)	2468 (75)	2332 (282)	2306 (131)	2136 (319)
Uniform noise (4 agents)	2224 (353)	2037 (412)	1734 (541)	1473 (558)
Gaussian noise (agent 0)	1752 (877)	1676 (650)	1431 (774)	1493 (554)
Gaussian noise (agent 1)	1775 (344)	1442 (619)	1257 (487)	1036 (554)
Gaussian noise (agent 2)	1326 (766)	1049 (666)	776 (763)	516.(842)
Gaussian noise (agent 3)	2103 (559)	2319(104)	2086 (480)	1896 (603)
Gaussian noise (4 agents)	1614 (776)	1553 (793)	1544 (568)	1470 (476)
CMBA (agent 0)	2068 (145)	1699 (169)	1432 (136)	1282 (136)
CMBA (agent 1)	2005 (355)	1682 (346)	1505 (222)	1189 (585)
CMBA (agent 2)	1554 (781)	1080 (792)	433(1202)	513 (987)
cMBA (agent 3)	2302 (80)	2032 (528)	2099 (92)	1604 (1069)
cMBA (4 agents)	2212 (225)	1909 (332)	1671 (212)	1504 (201)
Leaned Victim Selection (1 agent)	706(1118)	355 (635)	-326 (931)	-139 (608)
Learned Victim Selection (2 agent)	1274 (779)	720 (939)	152 (787)	156 (638) 一
20