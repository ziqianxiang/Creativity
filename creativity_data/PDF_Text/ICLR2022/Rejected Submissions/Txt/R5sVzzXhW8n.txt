Under review as a conference paper at ICLR 2022
Demystifying How Self-Supervised Features
Improve Training from Noisy Labels
Anonymous authors
Paper under double-blind review
Ab stract
The advancement of self-supervised learning (SSL) motivates researchers to apply
SSL on other tasks such as learning with noisy labels. Recent literature indicates
that methods built on SSL features can substantially improve the performance of
learning with noisy labels. Nonetheless, the deeper reasons why (and how) SSL
features benefit the training from noisy labels are less understood. In this paper,
we study why and how self-supervised features help networks resist label noise
using both theoretical analyses and numerical experiments. Our result shows that,
given a quality encoder pre-trained from SSL, a simple linear layer trained by
the cross-entropy loss is theoretically robust to symmetric label noise. Further,
we provide insights for how knowledge distilled from SSL features can alleviate
the over-fitting problem. We hope our work provides a better understanding for
learning with noisy labels from the perspective of self-supervised learning and can
potentially serve as a guideline for further research.
1	Introduction
Deep Neural Networks (DNNs) have achieved remarkable performance in many areas including
speech recognition (Graves et al., 2013), computer vision (Krizhevsky et al., 2012; Lotter et al.,
2016), natural language processing (Zhang & LeCun, 2015) etc. The high-achieving performance
often builds on the availability of quality-annotated datasets. In real world scenario, data annotation
inevitably brings in label noise which degrades the performance of the network, primarily due to
DNNs’ capability in “memorizing” noisy labels (Zhang et al., 2016).
In the past few years, a number of methods have been proposed to tackle the problem of learning
with label noise including robust loss design (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Liu &
Guo, 2020), sample selection (Han et al., 2018; Yu et al., 2019; Cheng et al., 2021) and noise tran-
sition matrix estimation (Patrini et al., 2017; Zhu et al., 2021b). Among all these methods, arguably
the most efficient treatment is to adopt robust losses, since sample selection and noise transition
matrix estimation always involve training multiple networks or need multi-stage training. Nonethe-
less, though the designed losses are theoretically proven robust, they often suffer from significant
performance drop when noise rate is high (Wang et al., 2019; Ma et al., 2020; Cheng et al., 2021;
Zhu et al., 2021a), hinting that the ability of converging to the optimal classifier is also important.
Very recent works (Zheltonozhskii et al., 2021; Nodet et al., 2021; Ghosh & Lan, 2021; Yao et al.,
2021; Tan et al., 2021) started applying self-supervised learning to solving the problem of learning
from noisy labels. The experiments show that methods built on the self-supervised features can
achieve exceptional performance even when the noise rate is high and largely outperform previously
reported SOTA approaches. Despite the empirical observations, the reasons why self-supervised
features lead to significant performance improvement are not well understood. In this paper, we
provide theoretical insights to understand how self-supervised features improve classification with
label noise and perform extensive experiments to support our theory. Our analysis provides a new
understanding on learning with noisy labels from the perspective of self-supervised learning. We
summarize our main contributions below:
•	We theoretically and experimentally show that by using self-supervised features to fine-tune the
network on noisy datasets, Cross Entropy itself is robust to label noise (Theorem 1-2). The theory
also answers the question of whether or not to fix the encoder when performing fine-tuning.
1
Under review as a conference paper at ICLR 2022
•	We theoretically and experimentally show that by using self-supervised features, a regularizer
commonly used in knowledge distillation (Hinton et al., 2015) (where the dataset does not contain
label noise) can greatly alleviate over-fitting problem of DNN on noisy datasets (Theorem 3-4).
1.1	Related Works
Learning with Noisy Labels: Due to the over-fitting problem of DNN, many works design robust
loss to improve the robustness of neural networks. (Ghosh et al., 2017) proves MAE is inherently
robust to label noise. However, MAE has a severe under-fitting problem. (Zhang & Sabuncu, 2018)
propose a loss which can combine both the advantage of MAE and CE, exhibiting good performance
on noisy datasets. (Liu & Guo, 2020) introduces peer loss, which is proven statistically robust to
label noise without knowing noise rate. The extension of peer loss also shows good performance on
instance-dependent label noise (Cheng et al., 2021; Zhu et al., 2021a). Another efficient approach
to combat label noise is by sample selection (Jiang et al., 2018; Han et al., 2018; Yu et al., 2019;
Northcutt et al., 2021; Yao et al., 2020; Wei et al., 2020; Zhang et al., 2020). These methods regard
“small loss” examples as clean ones and always involve training multiple networks to select clean
samples. Semi-supervised learning is also popular and effective on learning with noisy labels in
recent years. Some works (Li et al., 2020; Nguyen et al., 2020) first perform clustering on the
sample loss and divide the samples into clean ones and noisy ones. Then drop the labels of the
”noisy samples” and perform semi-supervised learning on all the samples.
Self-Supervised Learning: The goal of self-supervised learning (SSL) is to learn good presentation
without using the information of the labels. Generally, the methods of SSL can be divided into two
categories: designing pretext tasks or designing loss functions. The designed tasks or losses do not
involve any labels. Some popular tasks include patch orderings (Doersch et al., 2015; Noroozi &
Favaro, 2016), tracking (Wang & Gupta, 2015) or clustering features (Caron et al., 2018; 2019).
However, the SSL performance of pretext tasks is limited. Recent SOTA methods for SSL is by
designing contrastive loss functions. The representative works include Moco (He et al., 2020) and
SimCLR (Chen et al., 2020) which train neural networks based on InfoNCE loss (Oord et al., 2018).
In our paper, we also adopt InfoNCE for performing self-supervised training to get SSL pre-trained
features. The first part of our paper relates to the works that apply SSL features to perform fine-
tuning on noisy dataset (Nodet et al., 2021; Ghosh & Lan, 2021) and our goal is to build theoretical
understanding on this aspect.
Knowledge Distillation: The second part of our paper is very related to the research field of knowl-
edge distillation (KD). The original idea of KD can be traced back to model compression (BUciIUa
et al., 2006), where authors demonstrate the knowledge acquired by a large ensemble of models can
be transferred to a single small model. (Hinton et al., 2015) generalize this idea to neUral networks
and show a small, shallow network can be improved throUgh a teacher-stUdent framework. DUe to
its great applicability, KD has gained more and more attention in recent years and nUmeroUs meth-
ods have been proposed to perform efficient distillation (Mirzadeh et al., 2020; Zhang et al., 2018;
2019). However, the dataset Used in KD is assUmed to be clean. ThUs it is hard to connect KD with
learning with noisy labels. In this paper, we theoretically and experimentally show that a regUlarizer
generally Used in KD (Park et al., 2019) can alleviate the over-fitting problem on noisy data by Using
SSL featUres which offers a new alternative for dealing with label noise.
2	Preliminary
We introdUce preliminaries and notations inclUding definitions and problem formUlation.
Problem Formulation: Consider a classification problem on a set of N training examples denoted
by D := {(χn,yn)}n∈[N], where [N] := {1, 2,…，N} is the set of example indices. Examples
(xn, yn) are drawn according to random variables (X, Y ) from a joint distribUtion D. The classi-
fication task aims to identify a classifier C that maps X to Y accUrately. OUr theoretical analyses
focUs on binary classifications thUs Y ∈ {0, 1}. In real-world applications, the learner can only
observe noisy labels. For instance, hUman annotators may wrongly label some images contain-
ing cats as ones that contain dogs accidentally or irresponsibly. The label noise of each instance
is assUmed to be class-dependent (LiU & Tao, 2015), i.e., P(Ye |Y ) = P(Ye |X, Y ). ThUs the er-
.	1 /'	1	TΓΛ	Cl1 ∖	TΓΛ /	1 I，广	z-> ∖ EI	1 •
ror rates are defined as e+ = P(Y = 0|Y = 1), e- = P(Y = 1|Y = 0). The corresponding
2
Under review as a conference paper at ICLR 2022
Figure 1: Illustration of different learning paths.
〜
〜
noisy dataset and distribution are denoted by D := {(χn,yjn)}n∈[N] and D. Define the expected
risk of a classifier C as R(C) = ED [1(C(X) = Y)]. The goal is to learn a classifier C from the
noisy distribution D which also minimizes R(C), i.e., learn the Bayes optimal classifier such that
C Bayes (x) = arg maxi∈{0,1} P(Y
notations: X+ = X |Y = 1, X- =
i|X = x). For better presentation, we define the following
〜
〜
X|Y = 0, and Xclean = X|Y = Ye, Xnoisy = X|Y 6= Ye.
Evaluation of SSL (Self-Supervised Learning): SSL is usually evaluated by two steps: First, use
SSL to train an encoder f with only unlabeled data X, then add a linear classifier g following the
pre-trained encoder f and only fine-tune g on (X, Y ) with a fixed f. The high-level intuition is
that, if the encoder f is well learned by SSL, only fine-tuning linear classifier g is often sufficient
to achieve good performance on test data. If the test performance is comparable to SL (Supervised
Learning), we call the gap between SSL and SL is small (Chen et al., 2020). Denote by G the space
of linear classifier g. Fine-tuning linear layer g on (X, Y)〜D can be represented as:
min ED[CE(g(f(X)), Y)],
g∈G
where CE denotes the Cross-Entropy loss. Note the dimension of f (X) is determined by the
network structure, e.g., 512 for ResNet34. In binary classifications, g(f (X)) ∈ [0, 1] where
g(f (X)) < 0.5 indicates predicting class-0 and g(f (X)) > 0.5 corresponds to class-1. g(f(X)) is
supposed to predict the same label as CBayes (x).
3	Robustness of Cross-Entropy with SSL Features
We will analyze the robustness of Cross-Entropy with SSL features by comparing three different
learning paths as illustrated in Figure 1. Path-1 is the traditional learning path that learns both
encoder f and linear classifier g at the same time. Path-2 is the strategy applied in (Ghosh & Lan,
2021) that firstly pre-trains encoder f with SSL, then treats the pre-trained model as a network
initialization and jointly fine-tunes f and g. Path-3 is an alternate SSL-based path that first learns
the encoder f then only fine-tunes the linear classifier g with fixed f.
3.1	Theoretical Tools
We prepare some theoretical tools for our analyses. Our first theorem focuses on demonstrating the
effectiveness of only fine-tuning linear classifier g as in Path-3. We present Theorem 1 below.
Theorem 1 Let g1 = arg ming∈G ED[CE(g(f(X)), Y)], g2 = argming∈GEDe[CE(g(f(X)),Ye)].
Then if e+ = e- < 0.5, we have:
Round(g1(f(X))) = Round(g2(f(X)))	(1)
where f is fixed encoder and g is the linear classifier g(f (∙)) denotes the output whose value ranges
from 0 to 1. Round(p) is a predictor function that outputs 1 ifp > 0.5 and outputs 0 otherwise.
Theorem 1 shows with balanced error rates, simply fine-tuning a linear classifier g on the noisy
data distribution D can achieve the same decision boundary as the optimal linear classifier obtained
from the corresponding clean distribution D. i.e., g1 (f) and g2(f) have the same predictions for
all the samples. Theorem 1 can be generalized to the case with an arbitrary classifier beyond linear.
3
Under review as a conference paper at ICLR 2022
Figure 2: Tsne visualization of the self-supervised encoder outputs f (X) on DogCat (Kaggle),
CIFAR10 and CIFAR100 (Krizhevsky et al.). Different colors stand for different classes. SimCLR
(Chen et al., 2020) is selected for SSL training. Quality of encoders: (a)	(b)	(c).
However, admittedly with limited data, training complicated classifiers is hard to converge to the
optimal decision boundary. We defer more details to the next subsection.
We then evaluate the former part of Path-3, i.e., the performance of SSL. Recall in Section 2, SSL
is usually evaluated by performance gap between f ◦ gBayes and CBayes, where gBayes is the optimal
linear classifier trained on D, f ◦ gBayes denotes the joint model given by gBayes (f (X)). We consider
a tractable case in Assumption 1.
Assumption 1 The encoder outputs f(X+) and f(X-) follow Gaussian distribution with parame-
ters (μι, Σ) and (μ2, Σ), where Σ = σ2 ∙ I, I is the identity matrix.
Assumption 1 states that the self-supervised features for each class follow simple Gaussian distribu-
tions. We check the effectiveness of this assumption by Figure 2. It can be observed that the features
of each class may have overlaps, but a good SSL method is supposed to return features with good
separations (small overlaps). In Assumption 1, We use ∣∣μι 一 μ21| and σ to capture the overlapping
area of two classes. If ∣∣μι 一 μ21| is large and σ is small, then there exists small overlapping. Based
on this assumption, We shoW the performance of SSL in Theorem 2.
Theorem 2 If P(Y = 1) = P(Y = 0), the risk (error rate) of Bayes optimal classifier f ◦ gBayes
follows as:
R(f ◦ gBayes) = 1 - Φ (llμ2-；2||)	⑵
where Φ is the cumulative distribution function (CDF) of the standard Gaussian distribution.
Wrap-up With Theorem 1, we know CE is robust, the performance of which is subject to f .
Theorem 2 implies that if SSL features learned by f exhibit good property, i.e., when ∣∣μι 一 μ21| is
large and σ is small, only fine-tuning g can approach the Bayes optimal classifier CBayes. Therefore,
good SSL features induce high performance. In summary, Theorem 1 and Theorem 2 connect SSL
features with robustness and generalization ability of CE loss, providing an insight on why SSL
features improve classification with label noise.
3.2	Can We Fix the Encoder?
We compare the performance of Path-2 with Path-3 in this subsection.
3.2.1	Theoretical Analyses
Denote by VC(G) the VC-dimension of G.
Always fix good encoders When we fine-tune g on fixed f, the learning errors will come from two
parts: the encoder error and the classification error. The encoder error is bounded in Theorem 2.
Noting that g ∈ G is a linear classifier with relatively low VC-dimension and CE is a calibrated and
convex function, the optimization can achieve global optimum with enough samples (Bartlett et al.,
2006), e.g., w.p. 1 一 δ, the learning error will be ε with Θ(VC(G)+ln1∕δ) i.i.d. instances (Vapnik,
4
Under review as a conference paper at ICLR 2022
Figure 3: (a) (b) (c): Performance of CE on DogCat, CIFAR10 and CIFAR100 under symmetric
noise rate. For each noise rate, the best epoch test accuracy is recorded. The blue line represents
training with fixed encoder and the red line represents training with unfixed encoder; (d): test ac-
curacy of CIFAR10 on each training epoch under symmetric 0.6 noise rate. We use ResNet50 for
DogCat and ResNet34 for CIFAR10 and CIFAR100. SimCLR is used to deploy SSL pre-training.
Detailed experimental setting are reported in the Appendix.
2013)	. Recall Theorem 1 also guarantees the optimality of g learned under the noisy distribution D.
Therefore, given a good encoder with large enough kμ1-μ2k, We should keep it fixed since the simple
linear classifier g trained under label noise only requires N = Θ(VC(G)+ln1∕δ) i.i.d. instances in De
to achieve around ε errors with high probability, which performs well enough to approximate CBayes.
Updating f is not stable If f is unfixed, due to the non-convexity and over-fitting capability of f,
network trained on vanilla D will likely and ultimately overfit to noisy labels (Zhang et al., 2016),
particularly when the number of instances is insufficient. For example, w.p. 1 - δ, jointly learning
f and g and achieving error ε requires Θ( VC(FoG)+ln1∕δ) i.i.d. instances, which is far more than
the number required in only tuning g due to the huge capacity of DNNs. Note the effective i.i.d.
instances would decrease with the increase of noise rates. Although a good initialization contributes
to better convergence, f will be misled by label noise in extreme cases. Therefore it is unreasonable
to assume that CE will stably induce the network to approximate the Bayes optimal classifier under
label noise. In other words, if f is not fixed, due to the overfitting capability of f, the outputs of
f(X) would be contaminated by the label noise before g. In contrast, the original SSL features are
independent from the label noise if f is fixed. We therefore conclude that CE on vanilla D is not
robust. Note Nodet et al. (2021) claimed the encoder should not be fixed during fine-tuning based
on some experiments. We include more discussions in Appendix.
3.2.2	Empirical evidence
We conduct experiments in Figure 3 to support our theorems and analyses above.
Overall performance From Figure 3(a)-(c), when encoder f is fixed, the performance of CE
(fixed encoder) exhibits great robustness as noise rate increases and only has a little drop when the
symmetric noise rate approximates the maximum theoretical value for each dataset (0.5 for DogCat,
0.9 for CIFAR10) which verifies Theorem 1 and Theorem 2. Note that the drop is because, to
reach the condition specified in Equation (1), a substantial number of training samples are needed.
However, for example, each class only has 500 samples in CIFAR100. As the noise rate increases,
the number of clean samples will further decrease which makes the classifier hard to learn from the
distribution.
Fixed encoder benefits extreme label noise Next, we observe the performance gap between SSL
and SL when datasets are clean. This gap can be visualized in Figure 3 (a) (b) (c) at noise rate = 0.
For DogCat , the gap is very small, thus CE with fixed encoder always exhibits better performance.
For CIFAR10, the gap is moderate, the performances of CE (fixed encoder) and CE (unfixed en-
coder) are close when noise rate is small. However, when noise rate is high, CE with fixed encoder
still exhibits better performance. For CIFAR100, the gap is large, thus CE with fixed encoder shows
better performance only when noise rate is over 0.7.
Fixed encoder has better convergence Note that the performance in Figure 3 (a) (b) (c) are selected
from best epoch accuracy. If we take the last epoch accuracy into consideration. CE with fixed
encoder has great advantage over unfixed encoder. Figure 3 (d) depicts the performance of CE in
5
Under review as a conference paper at ICLR 2022
Table 1: Comparison of test accuracies of each method under instance-based label noise. CE (un-
fixed f with random init.) is a common baseline adopted in the literature which trains a random
initialized DNN using CE loss on noisy dataset.
Method	ε = 0.2	Inst. CIFAR10 ε = 0.4	ε = 0.6	Inst. CIFAR100		
				ε = 0.2 ε	= 0.4	ε	= 0.6
CORES (Cheng et al., 2021)	89.50	82:84	79.66	61.25	47.81	37.85
CE (unfixed f with random init.)	87.16	75.16	44.64	58.72	41.14	25.29
CE (fixed f, no sampling)	88.74	75.71	25.7	59.38	46.13	24.75
CE (fixed f, down-sampling)	90.12	84.19	82.06	62.88	61.1	58.93
terms of training epochs under 0.6 symmetric noise rate on CIFAR10. We find that even though
these two approaches have almost same best epoch accuracy, CE with fixed encoder is more robust
as training proceeds. For CE with unfixed encoder, since the self-supervised pre-trained network’s
weights serve as a better initialization, it shows good performance in the beginning. However,
because of the memorizing effect of DNN, it will still over-fit to noisy labels in the end.
Generalize to class- and instance-dependent label noise: Theorem 1 shows that CE is robust
to label noise when e- = e+ . However, for a more general label noise such as asymmetric class-
dependent label noise or instance-dependent label noise, CE with fixed encoder on vanilla (f(X), Ye)
is hard to be guaranteed to be robust because for instance noise, noise rate varies for each class. But
it does not mean SSL features can not help. We may still fix the encoder and perform a simple down-
sampling to balance the noisy dataset before fine-tuning the linear classifier. The down-sampling is
conducted to make P(Y = i) = P(Y = j) in the noisy dataset which can well approximate the
condition in Theorem 1 (Illustration is deferred to Appendix). Note that with down-sampling, the
target distribution of clean labels also changes. However, Theorem 1 is proved without specifying
the distribution of clean labels, suggesting that classifier is consistent even for imbalanced dataset
under symmetric label noise. Secondly, with SSL features, classifier is robust to imbalanced imbal-
anced dataset (Yang & Xu, 2020). Thus down-sampling strategy to reduce error rate imbalances is
helpful for other noise types beyond symmetric. We perform an experiment in Table 1 to verify
the effectiveness of the down-sampling in the setting of instance-dependent label noise (generation
of label noise is followed by (Cheng et al., 2021)). From the experiments, if no sampling strategy
is conducted, CE with fixed encoder does not show many benefits. In some cases, it even performs
worse than CE baseline (unfixed f with random init). However, with down-sampling, CE with
fixed encoder even outperforms CORES (Cheng et al., 2021) by a large margin. Even though our
main purpose in the paper is not to develop new methods for achieving SOTA, we think this down-
sampling strategy motivated by Theorem 1 can inspire further research on instance-dependent label
noise by using SSL features.
3.3	Takeaways
Our both theoretical and empirical results demonstrate that: We need to simply fix the encoder when
1) the encoder is sufficiently good, 2) the high noise rate decreases the number of effective i.i.d.
instances to a certain level, 3) the over-parameterized model makes it easy to overfit label noise.
However, there are some concerns: 1) If the gap between SL and SSL is relatively large, CE with
unfixed encoder has more advantage on small noise rate (e.g., Figure 3 (c)); 2) Consider an online
classification system which always needs new training data to update itself. If we only fine-tune the
linear classifier for keeping robustness to label noise, then due to the limited learnability of affine
functions, this online classification system may learn very limited knowledge from unseen data.
Therefore, it is necessary to update the encoder while keeping it stable and robust to label noise,
which is the focus in the next section.
4	Regularizer from KD improves network robustness
In this section, we show that a regularizer from knowledge distillation (KD) (Park et al., 2019) can
well alleviate the over-fitting problem by using the structure of SSL features. Different from classical
KD settings where the dataset is clean, our analyses aim at regularizing classifiers learned with noisy
6
Under review as a conference paper at ICLR 2022
h
a
SSL training
SL training <- Y
Figure 4: In this training framework, We adopt CE for SL training and InfoNCE for SSL training.
During training, SSL features are utilized to perform regularization on SL features.
supervisions by SSL features. Understanding the working mechanism is important since it may
contribute to the community a new perspective to learn with label noise by distilling information
from SSL features. In the rest of this section, we will first provide theoretical understanding towards
the regularizer under label noise, then perform experiments on different settings to test our analyses.
4.1	Training Framework
The training framework is shown in Figure 4, where a new learning path (SSL training) f → h
is added to be parallel to Path-2 f → g (SL-training) in Figure 1. The newly added projection
head h is one-hidden-layer MLP (Multi Layer Perceptron) whose output represents SSL features
(after dimension reduction). Its output is employed to regularize the output of linear classifier g as
Reg(h(f (X)), g(f (X))). InfoNCE from SimCLR (Chen et al., 2020) is adopted for SSL training
and CE is for SL training. InfoNCE and CE share a common encoder, inspired by the design of self
distillation (Zhang et al., 2019). The loss function is defined as:
L =EDe nCE(g(f(X)),Ye) + InfoNCE(h(f(X))) + Reg(h(f(X)),g(f(X)))o	(3)
Intuitively, SL features is supposed to be benefited from the structure information from SSL features,
e.g., clusterability (Zhu et al., 2021b) that instances with similar SSL features should have the same
true label and instance with different SSL features should have different true labels. Mathematically,
let ti = h(f (xi)), si = g(f (xi)) and XN to be the set of N tuples of data samples. The distance
between t and tj can be represented as φw(ti,tj) = ml∣∣ti - tj ∣∣w, where W ∈ {1,2} and m is a
normalization term:
m = ∣X2I	X	Ilti - tj Ilw.
(xi,xj)∈X2
Then the expectation of Reg(h(X), g(X)) in Equation (3) can be estimated by:
EX [Reg(h(X),g(X))] ≈ ɪ X	d(φw(ti,tj),φw(si,sj∙))
(xi,xj)∈X2
(4)
(5)
where d(∙) is a distance measure for two inputs. Popular choices are lι, l2 or square l2 distance.
4.2 Analytical Framework
Under the label noise setting, denote by di,j = d(φw (ti, tj), φw(si, sj)), we have the following
decomposition:
1
IX2I
di,j
(xi,xj)∈X 2
X---------V---------}
Term-1
di,j +
(xi ,xj)∈Xn2oisy
X---------------------}
^z∖∕f^~
Term-2
xi ∈Xclean ,xj ∈Xnoisy
X--------------------
^z∖∕f^~
Term-3
2 ∙ di,j). (6)
木（j.
}
where X = Xclean S Xnoisy, Xnoisy denotes the set of samples whose labels are flipped. Note the
regularizer mainly works when SSL features “disagree” with SL features, i.e., Term-3. For further
analyses, we write Term-3 in the form of expectation with d chosen as square l2 distance, i.e., MSE
loss:
7
Under review as a conference paper at ICLR 2022
Lc = EX clean ,X noisy
IIgf(Xclean))- g(f(Xnoisy))∣∣1	∣∣h(f(XClean))- h(f(Xnoisy))∣∣2
m1	m2
where m1 and m2 are normalization terms in Equation (4). Note in Lc, we use w = 1 for SL
features and W = 2 for SSL features.1 Denote the variance by var(∙). Define notations:
X+noisy :=XI(Ye = 1,Y = 0),	X-noisy := XI(Ye = 0,Y = 1).
To theoretically measure and quantify how feature correction relates to network robustness, we make
three assumptions as follows:
Assumption 2 (Memorize clean instances) ∀n ∈ {n∣yn = yn}, CE(g(f (Xn)),yn) = 0.
Assumption 3 (Same overfitting) var(g(f (X+noisy))) = 0 and var(g(f (X-noisy))) = 0.
Assumption 4 (Gaussian-distributed SSL features) The SSL features follow Gaussian distribu-
tions, i.e., h(f (X+ι))〜N(μι, Σ) and h(f (X-ι))〜N(μ2, ∑).
Assumption 2 implies that a DNN has confident predictions on clean samples. Assumption 3 im-
plies that a DNN has the same degree of overfitting for each noisy sample. For example, an over-
parameterized DNN can memorize all the noisy labels (Zhang et al., 2016; Liu, 2021). Thus these
two assumptions are reasonable. Assumption 4 follows Assumption 1. Note in Figure 4, SSL fea-
tures are from h(f (X)) rather than f(X).
4.3	Theoretical Understanding
Based on Assumptions 2-4, We present Theorem 3 to analyze the effect of L- Recall XRisy ：=
XI(Ye = 1,Y = 0), X-noisy := XI(Ye = 0,Y = 1).
Theorem 3 When e- = e+ and P(Y = 1) = P(Y = 0), minimizing Lc respect to g(f(Xnoisy)) on
DNN results in the following solutions:
EXnoiSyhg(f(X+oisy))i = 1 - --1--------------T , EXnoiSyhg(f (X-oisy))i = 1 + --1-----------T ,
X+	+	2	2 + ∆(Σ, μ1, μ2)	X-	-	2	2 + ∆(Σ, μ1, μ2)
where ∆(Σ, μ1,μ2) := 8 ∙ tr(∑)∕∣∣μι 一 μ2∣∣2, tr(∙) denotes the matrix trace,.
Theorem 3 reveals a clean relationship betWeen the quality of SSL features (given by h(f (X)))
and the netWork robustness on noisy samples. Note the true label for X+noisy is 0 and the true label
for X-noisy is 1. Theorem 3 shoWs the regularizer is leading the classifier to predict the feature
With corrupted label to its corresponding true label, and the guidance is particular strong When
∆(Σ, μ1, μ2) → 0, e.g., tr(Σ) → 0 or kμ1 - μ2 k → ∞, such that EXnoisy [g(f (X+noisy))] → 0 and
EXnoisy [g(f (X-noisy))] → 1. Beyond this ideal case, We discuss more interesting findings as:
-
•	Worst case: The regularizer can alWays have a positive effect to avoid memorizing Wrong la-
bels as long as the model f ◦ h is better than randomly initialized models. This is because
∆(Σ, μ1, μ2) > 0 When μ1 and μ2 are different, and the regularizer is leading the classifier
to the true direction When ∆(Σ, μ1, μ2) > 0.
•	Practical case: The model f ◦ h is alWays better than randomly initialized models since SL
training can guarantee the performance of encoder f on clean instances, Which may generalize
to noisy instances, and SSL training can further improves the quality of f ◦ h. Thus in practical
cases, using SSL features to regularize learning With noisy labels is alWays beneficial.
•	More possibilities: Note the proof of Theorem 3 does not rely on any SSL training process.
From our first finding, We knoW any encoder helps ifit is better than random models. This makes
it possible to use some pre-trained encoders from other tasks.
High-level understanding on structure regularization: Even though We have built Theorem 3 to
shoW SL features can benefit from the structure of SSL features by performing regularization, there
1Practically, different choices make negligible effects on performance. See more details in Appendix.
8
Under review as a conference paper at ICLR 2022
training epochs
training epochs
training epochs
Figure 5: Experiments with respect to the regularizer on CIFAR10. ResNet34 is deployed for all
the experiments. (a) (b): Encoder is pre-trained by SimCLR. Symmetric noise rate is 20% and 40%,
respectively; (c) (d): Encoder is randomly initialized. Symmetric noise rate is 40% and 60%, re-
spectively. The value of hyper-parameters and other detailed setting in the experiments are reported
in the Appendix.
still lacks high-level understanding of what the regularization is exactly doing. Here we provide an
insight in Theorem 4 which shows the regularization is implicitly maximizing mutual information
between SL features and SSL features.
Theorem 4 Suppose there always exists a mapping ξ to map h(f (X)) to g(f (X)). I.e., g(f (X)) =
ξ (h(f (X))). Then minimizing Equation (5) is implicitly maximizing Mutual Information be-
tween h(f (X)) and g(f (X)). I.e., when Equation (5) achieves 0, the mutual information
I (h(f (X)), g(f (X))) achieves maximum.
The above results facilitate a better understanding on what the regularizer is exactly doing. Note that
Mutual Information itself has several popular estimators (Belghazi et al., 2018; Hjelm et al., 2018).
It is a very interesting future direction to develop regularizes based on MI to perform regularization
by utilizing SSL features.
4.4	Experiments
We provide experiments on CIFAR10 to verify our theory and analysis. The overall experiments
are shown in Figure 5. In the experiments, Regularizer is added at the very beginning since recent
studies show that for random initialized network, the model tends to fit clean labels first (Arpit et al.,
2017) and we hope the regularizer can improve the network robustness when DNN begins to fit
noisy labels. From Figure 5 (c) (d), for CE training, the performance first increases then decreases
since the network over-fits noisy labels as training proceeds. However, for CE with regularizer, the
performance is more stable after it reaches the peak. For 60% noise rate, the peak point is also much
higher than vanilla CE training. For Figure 5 (a) (b), since the network is not randomly initialized,
it over-fits noisy labels at the very beginning and the performance gradually decreases. However,
for CE with regularizer, it can help the network gradually increase the performance as the network
reaches the lowest point (over-fitting state). This observation supports Theorem 3 that the regularizer
can prevent DNN from over-fitting to noisy labels. More experiments and ablation studies can be
found in the Appendix.
5	Conclusions and Discussions
In this paper, we have provided a theoretical understanding on why and how self-supervised features
improve training from noisy labels and perform experiments to verify our analysis.
•	We show that with SSL pre-trained encoder, CE itself is theoretically robust to symmetric label
noise. With the down-sampling strategy, it can also exhibit great power on class- and instance-
dependent label noise. This part answers why SSL features help (Theorem 1-2).
•	We show that by utilizing SSL features, the regularizer in KD can help DNN alleviate the over-
fitting problem on the noisy dataset and build theoretical insights on how SSL features relate to
the network robustness. This part answers how SSL features help (Theorem 3-4).
Future directions can be done to develop more efficient sampling strategies for solving instance-
based label noise with theoretical guarantees and develop efficient regularizers to combat label noise.
The key for these future works is to leverage the nice structural property of SSL features.
9
Under review as a conference paper at ICLR 2022
References
Devansh Arpit, StanisIaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer
look at memorization in deep netWorks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 233-242. JMLR. org, 2017.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101(473):138-156, 2006.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai RajesWar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint
arXiv:1801.04062, 2018.
Cristian BUcilUa, Rich Caruana, and Alexandru NicUleScU-MiziL Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 535-541, 2006.
Mathilde Caron, Piotr BojanoWski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 132-149, 2018.
Mathilde Caron, Piotr BojanoWski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of
image features on non-curated data. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 2959-2968, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frameWork for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597-1607. PMLR, 2020.
Hao Cheng, ZhaoWei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning With instance-
dependent label noise: A sample sieve approach. In International Conference on Learning Rep-
resentations, 2021.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE international conference on computer vision, pp.
1422-1430, 2015.
Aritra Ghosh and AndreW Lan. Contrastive learning improves model robustness under label noise.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2703-2708, 2021.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural netWorks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition With deep recur-
rent neural netWorks. In 2013 IEEE international conference on acoustics, speech and signal
processing, pp. 6645-6649. IEEE, 2013.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural netWorks With extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knoWledge in a neural netWork. arXiv
preprint arXiv:1503.02531, 2015.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan GreWal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
10
Under review as a conference paper at ICLR 2022
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International Conference
on Machine Learning,pp. 2304-2313. PMLR, 2018.
Kaggle. Kaggle dogcat dataset. URL https://www.kaggle.com/c/dogs- vs- cats.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/~kriz/cifar.html.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-
supervised learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HJgExaVtwr.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2015.
Yang Liu. Understanding instance-level label noise: Disparate impacts and treatments. In Interna-
tional Conference on Machine Learning, pp. 6725-6735. PMLR, 2021.
Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise
rates. In International Conference on Machine Learning, pp. 6226-6236. PMLR, 2020.
William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video pre-
diction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016.
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Nor-
malized loss functions for deep learning with noisy labels. In International Conference on Ma-
chine Learning, pp. 6543-6553. PMLR, 2020.
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan
Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, pp. 5191-5198, 2020.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong
Nguyen, Laura Beggel, and Thomas Brox. Self: Learning to filter noisy labels with self-
ensembling. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=HkgsPhNYPS.
Pierre Nodet, Vincent Lemaire, Alexis Bondu, and Antoine Cornuejols. Contrastive representations
for label noise require fine-tuning. arXiv preprint arXiv:2108.09154, 2021.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European conference on computer vision, pp. 69-84. Springer, 2016.
Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset
labels. Journal of Artificial Intelligence Research, 70:1373-1411, 2021.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3967-3976,
2019.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
11
Under review as a conference paper at ICLR 2022
Cheng Tan, Jun Xia, Lirong Wu, and Stan Z Li. Co-learning: Learning from noisy labels with
self-supervision. arXiv preprint arXiv:2108.04063, 2021.
Vladimir Vapnik. The nature of statistical learning theory. Springer Science & Business Media,
2013.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.
In Proceedings ofthe IEEE international conference on computer vision, pp. 2794-2802, 2015.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross en-
tropy for robust learning with noisy labels. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 322-330, 2019.
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A
joint training method with co-regularization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 13726-13735, 2020.
Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learn-
ing with noisy labels revisited: A study using real-world human annotations. arXiv preprint
arXiv:2110.12088, 2021.
Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced learning.
arXiv preprint arXiv:2006.07529, 2020.
Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James T Kwok. Searching to exploit memo-
rization effect in learning with noisy labels. In Proceedings of the 37th International Conference
on Machine Learning, ICML ’20, 2020.
Yazhou Yao, Zeren Sun, Chuanyi Zhang, Fumin Shen, Qi Wu, Jian Zhang, and Zhenmin Tang.
Jo-src: A contrastive approach for combating noisy labels. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 5192-5201, 2021.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does
disagreement help generalization against label corruption? arXiv preprint arXiv:1901.04215,
2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3713-3722,
2019.
Xiang Zhang and Yann LeCun. Text understanding from scratch. arXiv preprint arXiv:1502.01710,
2015.
Xuchao Zhang, Xian Wu, Fanglan Chen, Liang Zhao, and Chang-Tien Lu. Self-paced robust learn-
ing for leveraging clean labels in noisy data. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 6853-6860, 2020.
Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4320-
4328, 2018.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In Advances in neural information processing systems, pp. 8778-8788, 2018.
Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex M Bronstein, and Or Litany. Con-
trast to divide: Self-supervised pre-training for learning with noisy labels. arXiv preprint
arXiv:2103.13646, 2021.
Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-
dependent label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 10113-10123, 2021a.
12
Under review as a conference paper at ICLR 2022
Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when
learning with noisy labels. arXiv preprint arXiv:2102.05291, 2021b.
13
Under review as a conference paper at ICLR 2022
Appendix
The Appendix is arranged as follows: Section A proves Theorem 1 and Theorem 2 in the main
paper. Section B proves Theorem 3 and Theorem 4 in the main Paper. Section C illustrates why
down-sampling can decrease the gap of noise rates. Section D provides additional discussions and
experiments including further discussion of whether or not to fix the encoder when fine-tuning DNN
on noisy dataset; analyses of why robust losses (Wang et al., 2019; Ma et al., 2020) may suffer
severe performance degradation under high noise rates; the effect of distance measure in Equation
(4) (w = 1 or 2); ablation study in Section 4; visualization of features trained by vanilla CE loss;
the effect of different SSL pre-trained methods; Experiments with human-annotated label noise.
Section E elaborates the detailed experimental setting of all the experiments in the paper. Section F
generalizes Theorem 1 to multi-class classification. Section G provides theoretical and experimental
proof on why directly fine-tuning SSL features cannot be robust on asymmetric and instance label
noise.
A	Proof for Theorem 1,2
A.1 Proof for Theorem 1
Define g1 = ming ED[CE(g(f(X)), Y)] and g2 = ming EDe[CE(g(f(X))), Ye)]. Suppose g1Bayes
and g2Bayes are Bayes optimal classifiers under P(f (X), Y) and P(f (X), Ye ), respectively. Since CE
is a calibrated and convex loss, with enough samples, g1 and g2 will converge to (approximate to)
g1Bayes and g2Bayes (Bartlett et al., 2006). Thus to prove g1 and g2 have the same prediction for f(X),
it is identical to prove g1Bayes and g2Bayes have the same predictions for f(X).
Let p+(x) be the distribution of f(X+) and p-(x) be the distribution of f(X-), according to the
bayes rules:
P(Y = 11x) = P(Y = 1)P(Y(=) + P(Y(=)0) ∙ p-(x)
P(Y = 01X) = P(Y J)+P(Y(=)0)∙ p-(χ)
(8)
(9)
Define ψ(x) = P(Y = 1) ∙ p+(x) - P(Y = 0) ∙ p-(x). Thus ψ(x) = 0 is the decision boundary
for optimal Bayes classifier g1Bayes on clean dataset. Since when ψ(x) > 0, P(Y = 1|x) > P(Y =
0|x) from Equation (8) (9), the classifier will output the prediction as 1 and vice versa. Since the
generation of label noise is independent to f(X), the samples for each class whose labels are flipped
also follows original distribution. Denote pe+ (x) to be the distribution of samples whose observed
labels are 1 under label noise. Thus pe+(x) andpe-(x) can be represented as:
p+(x) = mi ∙{P(Y = 1) ∙ (1 — e+) ∙ p+(x) + P(Y = 0) ∙ e- ∙ p-(x)}
P-(X) = m2 ∙ {P(Y = 1) ∙ e+ ∙p+(x) + P(Y = 0) ∙ (1 — e-i) ∙ p-(x)}
where m1 and m2 are normalization term to make distribution valid. Specifically, m1 =
P(Y =1)∙(1-e+1)+P(Y=0)∙e- and m = P(Y =1)…(Y =0)∙(1-e-) . Under label noise，P(Y = I) =
P(Y = 1) ∙(1 - e+) + P(Y = 0) ∙ e- and P(Y = 0) = P(Y = 1) ∙ e+ + P(Y = 0) ∙ (1 - e-).
According bayes rules, we have:
P(Ye = 1|X)
_ ，〜 , ， ,
P(Y = 1) ∙ p+(x)
P(Y = 1) ∙p+(x) + P(Y = 0) ∙P-(X)
(10)
〜
〜
P(Ye = 0|X)
_ ,r<	.	， .
P(Y = 0) ∙ P-(X)
_ ，二	.、 ，、 _ 一、 ，、
P(Y = 1) ∙p+(x) + P(Y = 0) ∙P-(X)
(11)
14
Under review as a conference paper at ICLR 2022
Without loss of generality, we assume there exits a x0 in the feature space which satisfy ψ(x0) > 0.
Thus for clean dataset, P(Y = 1) ∙ p+(x) > P(Y = 0) ∙ p-(x) and P(Y = 1|xo) > P(Y = 0|xo).
We aim to show that for noisy dataset, P(Y = 1|x0) > P(Y = 0|x0), which means ψ(x) = 0 is
also the decision boundary that gives Bayes optimal classifier g2Bayes under label noise.
According to Equation (10) and Equation (11),
P(Ye = 1|x0) - P(Ye = 0|x0)
_(1 - 2 ∙ e+) ∙ P(Y = 1) ∙p+(x0) - (1 - 2 ∙ e-) ∙ P(Y = 0) ∙p-(x0)	(12)
normalization
一 一	_ , ~ 一、 _ , _ ,~	_、 _ , _ __________________ .、 , .
where normalization =	P(Y	= 1) ∙ p，(x)	+ P(Y =	0) ∙ P-(X)	> 0.	Since P(Y	= 1)	∙ p+(x0)	>
P(Y = 0) ∙P- (xo), if e+ = e- < 0.5, We have P(Y = 1|xo) - P(Y = 0|xo) > 0. Thus ψ(x) = 0
is also the decision boundary of g2Bayes. g1Bayes and g2Bayes will output the same prediction for all the
samples.
Proof done.
A.2 Proof for Theorem 2
Let d = ∣∣μι - μ2∣∣. Without loss of generality, we assume μι = (-d,0,...,0) and μ2 =
(d, 0,..., 0) (This can be done by applying affine transformation on original coordinates without
changing actual positions of f(X+) and f(X-)). Under the distribution of Gaussian assumption,
we write the distribution of f(X+) as follows:
P+(X) = (2π:σk exp{-2σ2(X - μι)τ(X - μι)}	(13)
where x = (x1, . . . , xk)T ∈ Rk, xk denotes the variable of x in the k-th dimension. Similarly, the
pdf of f(X-) can be represented as:
p-(X) = (2n； σk exp{- 2σ2(X - 〃2)T(X - 〃2)}	(14)
Following notations from A.1, solving ψ(X) = 0 under the conditions of P(Y = 1) = P(Y = 0)
gives x1 = 0 (x1 is the variable of X in the first dimension). Thus, when x1 < 0, the Bayes optimal
classifier f ◦ gBayes outputs prediction as 1. I.e., for samples whose labels are 1, f ◦ gBayes gives right
prediction when x1 < 0 and wrong prediction when x1 > 0. The risk ofof f ◦ gBayes can be written
as:
R(f ◦ gBayes) = Ex,y I(f ◦ gBayes(X) = Y)
=EY Eχ∣γ l(f ◦ gBayes(X )= Y)
=P(Y = 1) ∙ Eχ∣γ=ιl(f ◦ gBayes(X) = 1)+ P(Y = 0) ∙ Eχ∣γ=0l(f ◦ gBayes(X) = 0)
=2 ∙ Ex∣y =1 l(f ◦ gBayes(X) = 1) + 2 ∙ Eχ∣γ=ol(f ◦ gBayes(X) = 0)
=Ex∣y =1 l(f ◦ gBayes(X) = 1)
P+(X)dX > ∙ 0+ < /	p+(X)dX > ∙ 1
|x1 <0	x|x1 >0
(=)1- Φ(六)
2 ∙ σ
(a) is satisfied because of the symmetry ofP+ (X) andP-(X). We derive (b) as follows:
15
Under review as a conference paper at ICLR 2022
/
x|x1 <0
p+ (x)dx
1	1T
LxI<o (2∏)iσk exp{-取(X -μI)(X - μ1)}dx
/	_1_ exp {-(XI + 2 产 + x2 + …+Xk dx
J…(2π) 2σk p{	2σ2	dx
I	肉—exp{- 22}d}dx2 … I 肉—exp{- yk2}dxk I	昌—exp{- ( 1 22)}dx1
x2	2πσ	2σ2	xk 2πσ	2σ2	x1<0	2πσ	2σ2
一 /11 (X1 + d )2"
1 • 1 ••…JxI<0 √2πσexp{--2σ^}dx1
Since Rxl √∏σ exp{- (x2+2)}dxι is one dimensional gaussian with mean - 22 and deviation σ, it
is easy to calculate that Rx1<0 √∏σ exp{-(X∖+2) }dχι = Φ(£), where Φ is the Cdf of standard
gaussian distribution.
ThUSRx1>0 √2∏σ exp{-⅛2)2}dxι = 1 - RxI<0 √2∏σ exp{-(x2+d)2}dχι = 1 - φ 2dσ)and
Equation (b) is satisfied.
Proof done.
B Proof for Theorem 3,4
Lemma 1 If X and Y are independent and follow gaussian distribution: X 〜N(μχ, Σχ) and
Y 〜N(μγ, ∑y), Then: Eχ,γ(∣∣X - Y||2) = ∣∣μx - μγ||2 + tr(∑x + ∑y)∙
B.1 Proof for Theorem 3
Before the derivation, we define some notations for better presentation. Define the labels of X clean
as Y clean and the labels of Xnoisy as Ynoisy. Under the label noise, it is easy to verify P(Y clean = 1) =
P(Y=1)∙(1-e+)
P(Y =1)∙(1-e+)+P(Y=0)∙(1-e-)
and P(Y noisy = 1)
P(Y=0)∙e-
P(Y =0)∙e-+P(Y =1)∙e+
Let p1 = P(Y clean
1),
p2 = P(Y noisy = 1), g(f(X)) andh(f(X)) tobesimplifiedasgf(X) andhf(X).
In the case of binary classification, gf (X) is one dimensional value which denotes the network
prediction on X belonging to Y = 1. Lc can be written as:
Eɪciean	(∣∣gf(XCIean)- gf(Xnoisy))∣∣1 - ∣∣hf(Xclean)- hf(Xnoisy)∣∣2)2
Xcean,Xnosy	m1	m2
、-----------------------------------{-----------------------------------}
denoted as Ψ (X clean,X noisy)
(a)	clean noisy
= E (X clean Y clean ) Ψ (X , X )
(X noisy Y noisy )
=P1∙ P2 ∙ EX沪n,χoisyΨ(X+lean, X+oisy) + (1 - pi) ∙ P2 ∙ EXCIean,χ,yΨ(X-ean, X+oisy)
+ pi ∙ (1 - P2) ∙ EXcIean,χ noisy Ψ (Xfean ,X-oisy) + (1 - pQ ∙ (1 - p2 ) ∙ EX	noisy Ψ (X-ean ,Xnoisy)
where mi and m2 are normalization terms from Equation (4).	(a) is satisfied because
Ψ(Xclean, Xnoisy) is irrelevant to the labels. We derive Ψ(X+clean, X+noisy) as follows:
16
Under review as a conference paper at ICLR 2022
EXclean,XnoisyΨ(X+clean,X+noisy)
=)E d8an noisy(Ill-gf(x+0isy)ll1 - ∣∣hf(x+ean)-hf(x+0isy)∣∣2
x+ean,x+ y	m1	m2
=)E S _ 1 - gf (X+oisy) - ∣∣hf(X+ean)- hf (X+oisy)∣∣2
χ+ean,χ+ y	mi	m2
= EXClean XnOisy ( "("+
X+ ,X+	m1
-(ɪ -
m1
l∣hf(x+ean)- hf(x+oisy)ll2 ))2
m2
(b)	is satisfied because from Assumption 2, DNN has confident prediction on clean samples. (c) is
satisfied because g(f (x)) is one dimensional value which ranges from 0 to 1. From Assumption
4, hf (X+) and hf (X-) follows gaussian distribution with parameter (μι, Σ) and (μ2, Σ). Thus
according to Lemma 1, we have EX “ X noisy ∣∣hf (X+ean) - hf (X+oisy)∣∣2 = ∣∣μι - μ2∣∣2 +2 ∙ tr(Σ).
Similarly, one can calculate EXCIean XnOisy ∣∣hf (Xclean) - hf (XRisy) ∣∣2 = 2 ∙ tr(Σ). It can be seen that
(d) is function with respect to gf (X+noisy). Similarly, Ψ(X-clean, X+noisy) is also a function with respect
to gf (X+noisy) while Ψ (X+clean, X-noisy) and Ψ (X-clean, X-noisy) are functions with respect to gf (X-noisy).
Denote d(+, +) = EXclean,Xnoisy||hf(X+clean) -hf(X+noisy)||2. After organizing Ψ (X+clean, X+noisy) and
Ψ(X-clean,X+noisy), we have:
min pi ∙ P2 ∙ EXCIean XnoisyΨ(X+ean, X^^) + (1 - pi ) ∙ P2 ∙ EXCIean Xnoisy Ψ(X-ean, X^y)
gf(X+noisy )	+ ,	+	- ,	+
⇒ min (EXnoisygf(X+noisy))2
gf(X+noisy ) X+
-(2 ∙ pi(1 - m1∙ d(+，+)) +2 ∙ (1 - pi)( m1∙ d(-,+))) ∙ EXnoisy gf (X+oisy)
m2	m2	X+
+ constant with respect to gf (X+noisy)
(15)
Note in Equation (15), we use (EXnoisygf (X+noisy))2 to approximate EX noisy g f (X+noisy)2 since from
Assumption 3, var(g(f(X+noisy))) → 0. Now we calculate mi and m2 from Equation (4):
mi = p1 ∙ p2 ∙ (1 - EXnoisygf (x+foisy)) + (1 - PI) ∙ p2 ∙ EX%sygf (X，Sy)
+ Pi ∙ (1 - P2) ∙(I- EXnoisygf (X-oISy)) + (1 - pi) ∙ (1 - p2) ∙ EXnoisygf (Xl-OlSy)
(16)
m2 = Pi ∙ P2 ∙ d(+, +) + (1 - Pi) ∙ P2 ∙ d(-, +)+ Pi ∙ (1 - P2) ∙ d(+, -) + (1 - Pi)(1 - P2) ∙ d(-, -)
Under the condition of P(Y = 1) = P(Y = 0), e- = e+, We have Pi = p = 2,
m2 = 4"r3)+2*1-*2||2, mi = 2, which is constant with respect to EXnOisygf(X+Joιsy) and
EXnoisygf(X-noisy) in Equation (16). Thus Equation (15) is a quadratic equation with respect to
EXnoisygf(X+noisy). Then when Equation (15) achieves global minimum, we have:
EXnoisygf (XnoiSy) = Pi - m-(pi ∙ d(+, +) - (1 -Pi) ∙ d(-, +))
_ 1	m2	1	(17)
=2	2 +	8∙tr(∑)
2+ I∣μ1-μ2∣l2
Similarly, organizing Ψ(X+clean, X-noisy) andΨ(X-clean, X-noisy) gives the solution ofEXnoisygf(X-noisy):
-
17
Under review as a conference paper at ICLR 2022
EXnoisygf (XnOiSy) = p1 + mɪ (P1 ∙ d(-, -) - (1 - PI) ∙ d( + , -))
_ 1 ι 1
=2+ 2 I	8∙tr(∑) 一
2+ Ilμ1-μ2ll2
(18)
Proof Done.
B.2	Proof for Theorem 4
We first refer to a property of Mutual Information:
I (X; Y ) = I (Ψ(X); Φ(Y))	(19)
where ψ and φ are any invertible functions. This property shows that mutual information is invariant
to invertible transformations (Cover, 1999). Thus to prove the theorem, we only need to prove that
ξ in Theorem 4 must be an invertible function when Equation (5) is minimized to 0. Since when ξ
is invertible, I(h(f(X)), g(f(X))) = I(h(f(X)), ξ(h(f(X)))) = I(h(f(X)), h(f(X))).
We prove this by contradiction.
Let ti = h(f(xi)) and si = g(f(xi)). Suppose ξ is not invertible, then there must exists si and
sj where si 6= sj which satisfy tj = ξ(si) = ti. However, under this condition, ti - tj = 0 and
si - sj 6= 0, Equation (5) can not be minimized to 0. Thus when Equation (5) is minimized to 0, ξ
must be an invertible function.
Proof done.
B.3	Proof for Lemma 1
By the independence condition, Z = X - Y also follows gaussian distribution with parameter
(μx — μγ, ∑x + ∑y).
Write Z as Z = μ + LU where U is a standard gaussian and μ = μχ 一 μγ, LLT = Σχ + Σγ.
Thus
∣∣Z∣∣2 = Z T Z = μT μ + μT LU + U T LT μ + U T LT LU	(20)
Since U is standard gaussian, E(U) = 0. We have
E(||Z ||2) = μT μ + E(U T LT LU)
=μT μ + E(X(LT L)k,ιUk Ul)
k,l
(=)μTμ + X(LTL)k,k	QI)
k
=μT μ + tr(LT L)
=∣∣μx — μγ∣∣2 + tr(∑x + ∑y )
(a) is satisfied because U is standard gaussian, thus E(Uk2) = 1 and E(UkUl) = 0 (k 6= l).
Proof Done.
C Illustrating down-sampling strategy
We illustrate in the case of binary classification with e+ + e- < 1. Suppose the dataset is balanced,
at the initial sate, e+ > e-. After down-sampling, the noise rate becomes e* and e-. We aim to
prove two propositions:
18
Under review as a conference paper at ICLR 2022
Proposition 1 If e+ and e- are known, the optimal down-sampling rate can be calculated by e+
and e_ to make eζ = e-
Proposition 2 If e+ and e_ are not known. When down-sampling strategy is to make P(Y = 1)
_ , ~ 一、 一 _
P(Y = 0), then 0 < eζ — e_ < e+ _ e_.
Prooffor Proposition 1: Since dataset is balanced with initial ej > e_, we have P(Y = 1) <
TΓΛ ∕^Γ5-	Γ∖ ∖ EI	1	1 ∙	1 . 1 .	1	1	F	111 1	CC	. 1
P(Y = 0). Thus down-sampling is conducted at samples whose observed label are 0. Suppose the
random down-sampling rate is r, then ej
have:
r ∙e+
1一2+ jr∙e+
and el
e-
r-(1 —e- )+e—
.If ej = e_, we
r ∙ ej	e-
1 — ej + r ∙ ej	r ∙ (1 — e-) + e-
(22)

Thus the optimal down-sampling rate r =
known.
e—.(l-e+)
e+-(l-e-)
,which can be calculated if e_ and e+ are
Prooffor Proposition 2: If down sampling strategy is to make P(Y = 1) = P(Y = 0), then
r ∙ (ej + 1 — e_) = 1 — e j + e_, we have r =二十 je- . ThUS ej can be calculated as:
l	r ∙ ej
ej = --------j-----
1 — ej + r ∙ ej
(1 — ej + e-) ∙ ej
(1 — e+)∙(I —
-e++e-
e_ + e+) + e+ ∙ (1 — e+ + e_)
DenOte α = (一+).(-=:+)7+(―+je-) . SinCe ej > e_, 1 — e_ + ej > 1 — ej + e_,
α = ______1_e+je-______ < _______1_e+je-______ _ 1
―(1-e+)-(1-e—je+)je+ •(1_e+je_)	(1_e+)∙ (1_e+je—)je+ •(1_e+je—) — .
Similarly, e-
can be calculated as:
el
e_
e_ + r ∙ (1 — e_)
_____________(1 — e_ + ej) ∙ e____________
e_ ∙ (1 — e_ + ej) + (1 — e_) ∙ (1 — ej + e_)
Denote β
l-e— +e+
e--(1-e-+e+) + (1-e—)-(1-e+ + e—)
1 — e—+e+
e——(1-e—+e+) + (1-e—)-(1-e++e—)
. Since e+ >
1-e—+e+
e_, 1 — e_ + e+ > 1 — e+ + e_,
e—∙ (1 — e—+e+) + (1 —e— ).(1 — e_+e+)
1. Since α ∙ e+ < e+
and β ∙ e_ > e_, we have ej — e_ = α ∙ e+ — β ∙ e_ < e+ — e_.
Next, we prove ej > e_, following the derivation below:
el
β
>
>
r ∙ ej
1 — ej + r ∙ ej
1 — ej + e_
=⇒  ----j----
1 — e_ + ej
ej ∙ e-
=⇒ ej ∙(1 - ej)+ ----
1 — ej
e_
>
e_ + r ∙ (1 — e_)
"∙ (1 - ej)	
ej ∙ (1	-eJ
/e_ ∙ (1	— ej)
e+ ∙ (1 — Q
/	、	e_ ∙ ej
…-e-) + =7
(23)
>
>
>
e ∙e2	e ∙e2
Let f (e j) = ej ∙ (1 — e j) + ⅛e— — e_ ∙ (1 — e_) — 3⅛e+. Since we have assumed e_ < e j and
e_ + ej < 1. Thus proving ej > e! is identical to prove f (e j) > 0 when e_ < ej < 1 — e_.
Firstly, it is easy to verify when ej = e- or ej = 1 — e-, f(ej) = 0. From Mean Value Theory,
there must exists a point eŋ which satisfy f 0(eo) = 0 where ej < eŋ < 1—e_. Next, wedifferentiate
19
Under review as a conference paper at ICLR 2022
Figure 6: Visualizing decreased gap by down-sampling strategy.
f (e+ ) as follows:
f0(e+)
(1 - e+)2 ∙ (1 - e-) + e- ∙ (1 - e—) - 2 ∙ e+(1 - e+)2
(1-e+)2 ∙(1-e-)
(24)
It Can be verified that f(e-) = (I-J-e(1-R > 0 and f 0(1 - e-) = e2d—力=0.
Further differentiate f0(e+), we get when e+ < 1 - ((1 - e-) ∙ e-)1, f00(e+) < 0 and when
e+ > 1 - ((1 - e-) ∙ e-)1, f00(e+) > 0. Since e- < e+ and e- + e+ < 1, we have e- < 2 and
e- < 1 - ((1 - e-) ∙ e-) 1 < 1 - e-, i.e., 1 - ((1 - e-) ∙ e-)3 locates in the point between e-
and 1 - e-. Thus, when e- < e+ < 1 - ((1 - e-) ∙ e-)3, f (e+) is a strictly concave function and
when 1 - ((1 - e-) ∙ e-)3 < e+ < 1 - e-, f (e+) is a strictly convex function.
Since f(e-) > 0 and f 0(1 - e-) = 0, e0 must locates in the point between e- and 1 - ((1 - e-) ∙
e-) 1 which satisfy f0(eo) = 0. Thus when e- < e+ < eo, f(e+) monotonically increases and
when e0 < e+ < 1 - e-, f(e+) monotonically decreases. Since f(e-) = f(1 - e-) = 0. We have
f(e+) > 0 when e- < e+ < 1 - e-.
Proof done.
We depict a figure in Figure 6 to better show the effect of down-sampling strategy. It can be seen the
curves in the figure well support our proposition and proof. When e+ - e- is large, down-sampling
strategy to make P(Y = 1) = P(Y = 0) can well decrease the gap even we do not know the true
value of e- and e+. Thus down-sampling strategy can approximate the condition in Theorem 1,
which explains why CE with fixed encoder can also work well on instance-dependent label noise in
Table 1.
D More Discussions and Experiments
D.1 Whether or not to fix the encoder
Nodet et al. (2021) experimentally test many methods and losses based on the pre-trained SSL
features and draw the conclusion that the encoder should not be fixed during fine-tuning. We argue
that this conclusion is not complete and neglects the specialty of each loss and the gap between SSL
and SL varies for each dataset. First, the robustness of CE in Equation (1) is based on the fact that
CE is a calibrated and convex loss respect to the linear classifier. However, the losses tested by
Nodet et al. (2021) do not all satisfy this property when performing fine-tuning. For example, GCE
20
Under review as a conference paper at ICLR 2022
Table 2: Comparing CE (fixed encoder) with robust losses on CIFAR10
Method	0.2	Symm 0.4	0.6		0.8
SCE	87.63	85.34	80.07	53.81
APL	89.22	86.05	79.78	55.06
Peer	90.70	88.29	82.10	33.03
CE (fixed encoder)	91.06	90.73	90.2	88.24
(Zhang & Sabuncu, 2018) is not convex with respect to linear classifier. Second, if the gap between
SSL and SL is large, then the encoder should not be fixed since supervised features can exhibit better
performance. However, if the gap between SSL and SL is very small, it is guaranteed by Theorem
1 and Theorem 2 that CE on noisy dataset can approximates SL on clean dataset when encoder is
fixed. The observation can be seen in Figure 3 (a). Thus, the above argument suggests that whether
to fix the encoder or not is determined by the gap between SSL and SL for a given dataset.
D.2 Problem of other robust losses
Many robust losses have been proposed in recent years. For example, some losses (Natarajan et al.,
2013; Liu & Guo, 2020) are also proven statistically robust which satisfy:
-........... .- ..........................~,
min Eχ,γ[CE(g(f (X )),Y)] 0 min EXY ['(g(f (X )),Y)]	(25)
g,f	g,f ,
where ` is the robust loss and f, g in Equation (25) are simultaneously updated. However, Equation
(25) not only needs sufficient training samples to achieve small learning error ε, but also does not
consider the optimization difficulty of deep neural networks. Because of the great non-convexity of
f, it is very hard to guarantee that f and g will converge to global optimal when noise rate varies.
Thus the performance of ` in Equation (25) may suffer severe performance degradation under high
noise rate. However, for Equation (1), when f is fixed, CE loss with respect to g is strictly convex
which is very friendly for optimization.
To further verify our analysis, we compare CE with SSL pre-trained encoder with some robust
losses including SCE (Wang et al., 2019), APL (Ma et al., 2020) and Peer loss (Liu & Guo, 2020)
on CIFAR10 with symmetric label noise. The results are shown in Table 2. It can be observed
that when noise rate is very high, the performance of these robust losses will drop significantly.
However, when SSL pre-trained encoder is fixed, CE loss with respect to linear classifer is strictly
convex. Thus the performance is more stable when noise rate increases.
D.3 The effect of distance measure in Equation (4)
In this paper and experiment, we use l2 norm to calculate the feature distance between SL features
and square l2 norm to calculate the distance between SSL features. This choice can lead to good
performance from Theory 3 and Figure 5. Practically, since structure regularization mainly cap-
tures the relations, different choice does not make a big effect on the performance. We perform an
experiment in Figure 7 which shows that the performance of both types are quite close.
D.4 Ablation study
In Figure 4, SSL training is to provide SSL features to regularize the output of linear classifier g.
However, SSL training itself may have a positive effect on DNN. To show the robustness mainly
comes from the regularizer rather than SSL training, we perform an ablation study in Figure 8.
From the experiments, it is the regularizer that alleviates over-fitting problem of DNN.
D.5 Visualizing features of DNN trained by CE when encoder is not fixed
We visualize the features of DNN trained by CE when encoder is not fixed. Figure 9 shows the Tsne
visualization of features before linear classifier under different noise rate. It can be seen, as analyzed
21
Under review as a conference paper at ICLR 2022
(a) 20% symm (init: SimCLR)
ιoo -
β084020
Λ3Fnuue isəa
0	20	40	60	80	100
training epochs
(a) 40% symm (init: SimeLR)
100-
--Type 1
—Type 2
0	20	40	60	80	100
training epochs
(a) 40% symm (ιnιt: Random)
100
O O
6 4
An:DMIS
20	40	60	80	100	120	140
training epochs
Figure 8: Ablation study of using the regularizer to train DNN on noisy dataset.
(c) 40% symm (ιnιt: Random)
100-
50	100
training epochs
(d) 60% symm (ιnιt: Random)
100 ■
8 6 4 2
50	100	150
training epochs
150	0
(b) 60% symm (ιnιt: Random)
100
Figure 7: Comparing difference choices of distance measure in Equation (4). Type 1 denotes using
l2 norm to calculate distance between SL features and square l2 norm to calculate distance between
SSL features, which is adopted in our paper. Type 2 denotes using l2 norm to calculate distance for
both SL and SSL features.
6040
O
20	40	60	80 IOO 120	140
training epochs
60
40
20
0
-20
-40
-60
Figure 9: Tsne visualization of supervised features before linear classifier on CIFAR10 under 20%,
60% and 80% symmetric label noise, respectively.
in the paper, due to non-convexity and great over-fitting capability of f (encoder), the features of
f (X) have already been contaminated by the label noise before linear classifier, thus CE on vanilla
(X, Y ) is not robust.
D.6 The effect of different SSL-pretrained methods
Our analyses and experiments are not restricted to any specific SSL method. Experimentally, other
SSL methods are also adoptable to pre-train SSL encoders. In Figure 3, SimCLR (Chen et al., 2020)
is adopted to pre-train SSL encoder. For a comparison, we pre-train a encoder with Moco (He et al.,
2020) on CIFAR10 and fine-tune linear classifier on noisy labels in Table 3.
22
Under review as a conference paper at ICLR 2022
Table 3: Comparing different SSL methods on CIFAR10 with symmetric label noise
Method	Symm label noise ratio 0.2	0.4	0.6	0.8
CE (fixed encoder with SimCLR init) CE (fixed encoder with MoCo init)	91.06 90.73	90.2	8824^ 91.55 91.12 90.45	88.51
It can be observed that different SSL methods have very similar results, which further verifies our
Theorems in the paper.
D.7 Experiments on Human-annotated Noisy Dataset
We perform experiments on CIFAR100-N (A challenging human-annotated noisy dataset) (Wei
et al., 2021) with structure regularization. From Table 4, it can be observed that by using SSL
features, Structure Regularization can outperform many benchmark methods on real-world human
annotated noisy dataset.
Table 4: The best epoch (clean) test accuracy for each method on CIFAR100-N.
Method J CE ForWard T GCE JoCoR Peer Loss ELR StruCture Regularization
Acc. 155.50	57.01	56.73 59.97	57.59	58.94	61.12
E	Detailed setting of experiments
Datasets: We use DogCat, CIFAR10 and CIFAR100 for experiments. DogCat has 25000 images.
We randomly choose 24000 images for training and 1000 images for testing. For CIFAR10 and
CIFAR100, We folloW standard setting that use 50000 images for training and 10000 images for
testing.
Setting in Section 3: SimCLR is deployed for SSL pre-training With ResNet50 for DogCat and
ResNet34 for CIFAR10 and CIFAR100. Each model is pre-trained by 1000 epochs With Adam
optimizer (lr = 1e-3) and batch-size is set to be 512. During fine-tuning, We fix the encoder and only
fine-tune the linear classifier on noisy dataset With Adam (lr = 1e-3) for 100 epochs and batch-size
is set to be 256.
Setting in Section 4: The basic hyper-parameters are identical to Section 3, except When We train
from scratch (random initialization), learning rate is set to be 0.1 at initial state and is decayed by
0.1 at 50 epochs. In Equation (7), We use MSE loss for measuring the relations betWeen SL features
and SSL features. HoWever, since MSE loss may cause gradient exploration When prediction is far
from ground-truth, We use smooth l1 loss instead. Smooth l1 loss is an enhanced version of MSE
loss. When prediction is not very far from ground-truth, smooth l1 loss is MSE, and MAE When
prediction is far.
The code With running guideline has already been attached in the supplementary material.
F Generalize Theorem 1 to multi-class classification
Theorem 5 For K-class classification, let g1 = arg ming∈G ED [CE(g (f (X)), Y )], g2 =
arg ming∈G EDe [CE(g(f (X)), Y)]. Then if symmetric noise ratio e < KKI, for each Sample Xi,
we have
argmaxg1(f(xi))k = argmaxg2(f(xi))k	(26)
kk
where f is fixed encoder and g is the linear classifier, g(f (∙)) denotes the network output after
softmax layer.
23
Under review as a conference paper at ICLR 2022
proof: Following the proof for Theorem 1, suppose g1Bayes and g2Bayes are Bayes optimal classifiers
under P(f (X), Y ) and P(f (X), Ye), respectively. We aim to show g1Bayes and g2Bayes are consistent.
I.e., the decision boundaries are the same.
Denote Xi = X|Y = i. Define pi(x) to be the distribution of f(Xi). According to the bayes rules:
P(Y = i|x)
P(Y = i) ∙Pi(X)
P=IP(Y = j) ∙ Pj(x)
(27)
Let X be the space of f (X):
X = Xi ∪X2 ∙∙∙∪Xk
where Xi = {X ∈ f(X)|P(Y = i|X) > P(Y =j|X),i 6=j}.
(28)
Let Pi(X) be the distribution of samples whose observed labels are i under symmetric label noise
rate . We have:
Pt (x) = mi ∙ ∣P(Y = i) ∙ (1 - e) ∙ Pi(X) + X P(Y = j) ∙ K- 1 ∙ Pj (x) ʃ (29)
where mi is the normalization term to make distribution valid. Specifically, mi =
P(Y=iX1-e)+Pj=iP(Y=j)∙ W. Underlabel noise, P(Y = I)= P(Y = i) ∙ (1 - e) + Pj=i P(Y =
j) ∙ k-i = ml-. According to the bayes rules, We have:
_ ，〜 . ，.
P(Y = i|X) = PK=iP (Y = j)i ∙ Pj (X)
(30)
〜
〜
Let X = Xt ∪ Xt …∪ XK, where Xi = {x ∈ f(X)∣P(Y = i|x) > P(Y = j∣x),i = j}. To
prove gBayes and gBayes are consistent, it is equivalent to show Xi = Xi when e < KK1.
Without loss of generality, we prove Xi = Xt when e < KK1.
When j 6= 1, we have:
P(Ye = 1|X) - P(Ye = j|X)
(1-e- K-I) ∙ P(Y =1) ∙ PI(X)-(1-e- K¾) ∙ P(Y = j) ∙ Pj (x)	(31)
normalization
Since Xi = {x ∈ f(X)∣P(Y = 1|x) > P(Y = j|x), j = 1}, Xi = Xf if 1 - e - Kt-ɪ > 0, i.e.,
e < K-I
e < K .
Proof Done.
G Illustrating Bayes clas sifier is not consistent under
ASYMMETRIC AND INSTANCE LABEL NOISE
Asymmetric label noise: We give an example on K-class classification (K=3). Following the def-
inition and notation in Section F, suppose the asymmetric label noise ratio is e, which is generated
between two adjacent label (1 ⇔ 2,2 ⇔ 3,3 ⇔ 1), we have:
P(Ye = 1|X) - P(Ye = 2|X)
(1 - e) ∙ P(Y = 1) ∙ P1(x) - (2 ∙ e - 1) ∙ P(Y = 2) ∙ p2(x) - e ∙ P(Y = 3) ∙ p3(x)	(32)
normalization
24
Under review as a conference paper at ICLR 2022
P(Ye = 1|x) - P(Ye = 3|x)
_(1 - 2 ∙ e)	∙	P(Y = 1) ∙p1(x)	- e ∙ P(Y	= 2)	∙p2(x) - (1 -	e)	∙ P(Y = 3)	∙p3(x)	(33)
normalization
Since X1 = {x ∈ f(X)|P(Y = 1|x) > P(Y=j|x),j 6= 1} and X1 = {x ∈ f (X)|P(Ye = 1|x) >
P(Y = j|x),j 6= 1} , it is easy to verify X1 = X1 only when e = 0. Thus asymmetric label noise
does not induce consistent Bayes classifier. We also provide an experiment on CIFAR10 to further
verify our analysis in Table 5. It can be seen that for asymmetric label noise, there is only marginal
improvement compared to symmetric label noise in Figure 3.
Table 5: Comparing CE (fixed encoder) with baseline (CE with random initialized unfixed encoder)
on CIFAR10 with asymmetric label noise
Method	Asymm label noise ratio 0.1	0.2	0.3	0.4
CE (baseline, unfixed encoder with random init) CE (fixed SSL pre-trained encoder)	90.69	88.59 86.14 80TΓ^ 90.85	89.92 88.25 82.46
Instance-dependent label noise: Instance-dependent label noise behaves like symmetric label
noise, i.e., the noisy labels may exist in all the classes. However, the noise ratio for each class
is not the same and dependent on the features. From Section F, when noise ratio for each class is
not the same, the Bayes classifier is not consistent. However, from Table 1 and Section C we the-
oretically and experimentally show that simple down-sampling strategy can approximate condition
in Theorem 1 and Theorem 5 which is helpful for instance level label noise.
25