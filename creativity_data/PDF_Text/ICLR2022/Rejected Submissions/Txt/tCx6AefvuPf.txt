Under review as a conference paper at ICLR 2022
Node-Level Differentially Private Graph
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks (GNNs) are a popular technique for modelling graph-
structured data that compute node-level representations via aggregation of infor-
mation from the local neighborhood of each node. However, this aggregation
implies increased risk of revealing sensitive information, as a node can participate
in the inference for multiple nodes. This implies that standard privacy preserv-
ing machine learning techniques, such as differentially private stochastic gradi-
ent descent (DP-SGD) - which are designed for situations where each data point
participates in the inference for one point only - either do not apply, or lead to
inaccurate solutions. In this work, we formally define the problem of learning
1-layer GNNs with node-level privacy, and provide an algorithmic solution with a
strong differential privacy guarantee. Even though each node can be involved in
the inference for multiple nodes, by employing a careful sensitivity analysis and
a non-trivial extension of the privacy-by-amplification technique, our method is
able to provide accurate solutions with solid privacy parameters. Empirical evalu-
ation on standard benchmarks demonstrates that our method is indeed able to learn
accurate privacy preserving GNNs, while still outperforming standard non-private
methods that completely ignore graph information.
1	Introduction
Graph Neural Networks (GNNs) are powerful modeling tools that capture structural information
provided by a graph. Consequently, they have become popular in a wide array of domains such
as biology (Ktena et al., 2018), medicine (Ahmedt-Aristizabal et al., 2021), chemistry (McCloskey
et al., 2019), computer vision (Wang et al., 2019), and text classification (Yao et al., 2019).
GNNs allow aggregation of data from the neighbors of a given node in the graph, thus evading the
challenge of data scarcity per node. Naturally, such solutions are quite attractive in modeling users -
each node of the graph is represented by the user and the connections represent interactions between
the users - for a variety of recommendation/ranking tasks, where it is challenging to obtain and store
user data (Fan et al., 2019; Budhiraja et al., 2020; Levy et al., 2021).
However, such solutions are challenging to deploy as they are susceptible to leaking highly sensitive
private information about the users. It is well-known that standard ML models - without GNN
style data aggregation - can leak highly sensitive information about the training data (Carlini et al.,
2019). The risk of leakage is significantly higher in GNNs as each prediction is based on not just the
individual node, but also an aggregation of data from the neighborhood of the given node. In fact,
there are two types of highly-sensitive information about an individual node that can be leaked: a) the
features associated with each node/user, b) the connectivity information of an individual node/user.
In this work, we study the problem of designing algorithms to learn GNNs while preserving node-
level privacy, i.e., preserving both the features as well as connectivity information of an individual
node. We use differential privacy as the notion of privacy (Dwork et al., 2006) of a node, which
roughly-speaking requires that the algorithm should learn similar GNNs despite perturbation of an
entire node and all the data points or predictions associated with that node.
Example scenarios for such a solution include ranking/recommendation of entities like docu-
ments/emails in an organization. Here, the graph can be formed by a variety of means like how
users interact with each other, and the goal would be to learn user features that can enable more
1
Under review as a conference paper at ICLR 2022
accurate ranking of emails/documents. Naturally, user interaction data as well as individual users’
features (like the topics in which user is interested in) would be critical to preserve, and any reve-
lation of such data can be catastrophic. Furthermore, once GNNs are learned to model users while
preserving privacy, they can be used in different settings based on the problem requirement. For ex-
ample, in settings where a node can access it’s r-hop neighbors data, we can directly apply r-layer
GNNs (if they are trained with DP). Similarly, in certain scenarios, we would want to learn GNNs
over a large enterprise and deploy the same model for a small enterprise, where at inference time
neighborhood information (like managerial reporting structure) might be publicly accessible within
the enterprise but not across enterprises. See Section 4 for a detailed discussion.
Recent works have explored the problem of differentially private learning of GNNs, but they either
consider a restricted setting of edge-level privacy which is often insufficient for real-world problems
or they restrict themselves to simpler settings like bipartite graphs or node-level privacy without
preserving individual connectivity information (Wu et al., 2021a;b; Zhou et al., 2020).
In contrast, our proposed method preserves the privacy of the features of each node (‘user’), their
labels as well as their connectivity information. To this end, we adapt the standard DP-SGD method
(Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016) to our setting. But, analysis of the standard
DP-SGD method does not directly extend to GNNs, as each gradient term in GNNs can depend on
multiple nodes. The key technical contribution of our work is two-fold: i) we provide a careful
sensitivity analysis for the special case of 1-layer GNNs, ii) we extend the standard privacy by
amplification technique to GNNs where one gradient term can depend on multiple users. Note that
the standard privacy by amplification method only applies to scenarios where each point corresponds
to one user/entity. By combining the above two results with the standard Renyi Differential Privacy
(RDP) accounting, we obtain a formal proof of privacy for our method.
Finally, we evaluate our DP-GNN method on standard benchmarks. We demonstrate that DP-GNN
is reasonably accurate compared to the standard 1-layer GCN models, while providing privacy pa-
rameters of about ≤ 30 which are close to the industry standard. More critically, compared to stan-
dard MLP (multi-layer perceptron) based methods that completely discard graph side-information,
our method can be 5-6% more accurate while still providing strong privacy guarantees. That is,
we demonstrate that GNN based techniques can indeed be deployed in practice with the benefits of
improved accuracy over vanilla MLP style methods while still preserving sensitive user data.
Contributions: We propose a Node-Level Differentially Private Graph Neural Network that works
well in practice and provides formal privacy guarantees. This is the first work, to the best of our
knowledge, to provide such strong privacy guarantees for each individual node in the graph learning
regime. Our main contributions are organised as follows:
•	Formulation: In Section 3, we formalize the problem of node-level differentially private GNNs,
and discuss various important settings in which a solution to the problem is applicable.
•	Method: In Section 4, we describe our algorithm that adapts standard DP-SGD to train differen-
tially private GNNs, with a strong privacy guarantee that extends standard privacy amplification
by sampling.
•	Empirical Evaluation: In Section 5, we evaluate our framework on multiple benchmark graph
datasets on the task of node classification. We demonstrate that our DP-GNN method can outper-
form non-private and private MLP methods that cannot utilize graph information.
2	Related Work
Mechanisms to make the training process of machine learning models private primarily fall into
two categories: model-agnostic methods such as PATE (Papernot et al., 2017), and model-aware
methods such as DP-SGD (Abadi et al., 2016), which augment the standard paradigm of gradient-
based training to be differentially private. DP-SGD, in particular, has been used successfully to train
neural network models to classify images (Abadi et al., 2016) and text (Anil et al., 2021).
Today, there are many varieties of graph neural networks employed: Graph Convolutional Neural
Networks (KiPf & Welling, 2016), Graph Attention Networks (Velickovic et al., 2018), GraPhSAGE
(Hamilton et al., 2017), and Message-Passing Neural Networks (Gilmer et al., 2017), to name a few.
Broadly, these models comPute node-level rePresentations via aggregation of neighbourhood-level
2
Under review as a conference paper at ICLR 2022
information, that can lead to diffusion of private information across multiple nodes, thus making
application of standard DP-SGD like techniques non-trivial.
There has been recent work in learning and evaluating edge-level private GNNs (Wu et al., 2021b)
but they do not preserve node-level data. Private GNNs have also been studied from the perspective
of local privacy (Sajadmanesh & Gatica-Perez, 2020), where each node performs its share of the
GNN computation locally. In such a setting, each node sends noisy versions of its features and labels
to neighbouring nodes in order to learn shared weights, resulting in a elaborate learning algorithm
that needs to correct for the bias in both the features and labels. (Wu et al., 2021a) utilizes private
GNNs for recommendation systems, but their method assumes a bipartite graph structure, and cannot
naturally handle homogeneous graphs. Other approaches employ federated learning (Zhou et al.,
2020), but only guarantee that the GNN neighbourhood aggregation step is differentially private,
which is insufficient to guarantee privacy of each node’s neighborhood. Finally, other attempts
(Shan et al., 2021) to create privacy-preserving GNNs exist, but these do not use the formal notion
of DP.
Model-agnostic methods, such as PATE, have recently been investigated to train GNNs (Olatunji
et al., 2021). In their current form, however, such methods require access to public data samples,
which may not always be available for the task at hand.
In contrast to previous approaches which protect the privacy of a node’s features and labels only, we
additionally seek to protect every node’s adjacency vector, which is its private list of connections
to neighbouring nodes. This is because the existence of communication between a pair of nodes
can often be sensitive information in itself. Further, our approach extends the standard approaches
of gradient-based training to scalably train node-level differentially private GNNs in a centralized
setting, without any access to public data. Depending on the required privacy setting, this mechanism
can be composed with locally differentially private mechanisms to generate node-level predictions.
In different contexts, there has been extensive work on node-level DP (Raskhodnikova & Smith,
2016; Karwa et al., 2011; Borgs et al., 2015; 2018). But these methods generally deal with modeling
‘global’ graph-level statistics and do not support learning methods such as GNNs. In contrast, our
approach aims to predict ‘local’ node-level statistics (like the label of a node) while preserving
node-level privacy.
3	Problem Formulation and Preliminaries
Consider a graph dataset G = (V, E, X, Y) with directed graph G = (V, E) represented by a
adjacency matrix A ∈ {0, 1}n×n. n is the number of nodes in G, V denotes the node set, E denotes
the edge set. Each node v in the graph is equipped with a feature vector Xv ∈ Rd ; X ∈ Rn×d
denotes the feature matrix. Y ∈ Rn×Q is the label matrix and yv is the label for the v-th node
over Q classes. Note that many of the labels in the label vector can be missing, which models the
semi-supervised setting. In particular, we assume that node labels yv are only provided for a subset
of nodes Vtr ⊂ V , called the training set.
Given the graph dataset G, the goal is to learn parameters of a one-layer GNN while preserving
privacy of individual nodes. A GNN can be represented by the following operations:
ybv = GNN(A, X, v; Θ) := fdec (fagg ({fenc(Xu) |Avu 6= 0}))	(1)
where ybv is the prediction from the GNN fora given node v, fenc is the encoder function that encodes
node features with parameters Θenc, fagg is the neighborhood aggregation function with parameters
Θagg, fdec is the prediction decoder function with parameters Θdec, and Θ := (Θenc , Θagg, Θdec).
While our results apply to most 1-layer GNN models (Hamilton et al., 2017; Velickovic et al., 2018;
Xu et al., 2018), for simplicity, we focus on 1-layer Graph Convolutional Network (GCN) models1
(Kipf & Welling, 2016). These GCN models use a multi-layer perceptron (MLP) for encoder and
decoder functions, with non-linear activation function σ:
ybv = GCN(A, X, v; Θ) := MLPdec (Avσ(MLPenc(X))Θagg)	(2)
1 As is common in practice, we allow any normalization and addition of self-loops to A.
3
Under review as a conference paper at ICLR 2022
Thus, “learning” a GCN is equivalent to finding parameters Θ := (Θenc, Θagg, Θdec) that minimize
a suitable loss:
Θ* = arg min X '(bv; NP)	(3)
Θ	v∈V
X-----{------}
L(G,Θ)
where ` : RQ×Q → R is a standard loss function such as categorical cross-entropy.2
As mentioned earlier, we use differential privacy as the notion of privacy of a node. Before defining
differential privacy, we first define the notion of adjacent graph datasets:
Definition 1 (Adjacent Graph Datasets). Two graph datasets G and G0 are said to be node-level
adjacent if one can be obtained by adding or removing a node (with its features, labels and associ-
ated edges) to the other. That is, G and G0 are exactly the same except for the v-th node, i.e., Xv,
yv and Av differ in the two datasets.
Informally, A is said to be node-level differentially-private algorithm if the addition or removal of a
node in A’s input does not affect A’s output significantly.
Definition 2 (Node-level Differential Privacy). Consider any randomized algorithm A that takes
as input a graph dataset. A is said to be (α, Y) node-level Renyi differentially-private (Mironov,
2017b) if, for every pair of node-level adjacent datasets G and G0:
Dα (A(G) k A(G0)) ≤ γ,
where Renyi divergence Da oforder a between two random variables P and Q is defined as:
Da(Pk Q) = ɪ lnEx^Q [P7⅛l .
α - 1	Q(x)
Note that We use Renyi differentially-private (RDP) (Mironov, 2017b) as the formal notion of dif-
ferential privacy (DP), as it allows for tighter composition of DP across multiple steps. This notion
is closely related to the standard (ε, δ)-differential privacy (DWork et al., 2006); Proposition 3 of
Mironov (2017b) states that any (α, Y)-RDP mechanism also satisfies (γ + lOg-/，, δ)-differential
privacy for any 0 < δ < 1.
Thus, the goal is to find Θ by optimizing equation 3 While ensuring RDP (Definition 2). It is clear
that node-level privacy is essential When training models on graph datasets With sensitive node-level
information. HoWever, node-level privacy is significantly harder to achieve than the Weaker notion
of edge-level privacy. In the context of GNNs, the representation for a node is computed using not
just the node’s individual features, but also features of other nodes from the local neighbourhood.
Thus, the removal of a node from a graph dataset affects its entire local neighbourhood, Which can
be a very large set of nodes. This is in contrast to the standard non-graph setting for differentially
private models, Where the representation of individual users Would only depend on the user’s oWn
data.
We noW define tWo concepts that are critical in our design and analysis of a private GNN learning
method.
Definition 3. The node-level sensitivity ∆(f) of a function f defined on graph datasets is:
∆(f) = max kf(G)-f(G0)k2
node-level adjacent
G,G0
The K -restricted node-level sensitivity ∆K(f) of a function f defined on graph datasets is:
∆K(f) = max 0 kf(G) - f(G0)k2
deg(G), deg(G0)≤K
node-level adjacent
G,G0
Definition 4. We define the clipping operator Clip。(.) as: Clip。(V) = min(1, kC-) ∙ V ,for any
vector or matrix v.
2 The analysis here holds for multi-label settings as Well, Which Would instead use loss functions such as
sigmoidal cross-entropy, for example.
4
Under review as a conference paper at ICLR 2022
Algorithm 1: DP-GNN (SGD): Differentially Private Graph Neural Network with SGD
Data: Graph G = (V, E, X, Y), GNN definition GNN, Training set V↑λ,, Loss function L,
Batch size m, Maximum degree K, Learning rate η, Clipping threshold C, Noise
standard deviation σ, Maximum training iterations T .
Result: GNN parameters ΘT .
Note that Vtr is the subset of nodes for which labels are available (see Paragraph 1 of Section 3).
Using Vtr, construct the set of training subgraphs Str with Algorithm 2.
Construct the 0 - 1 adjacency matrix A: Avu = 1 ^⇒ (v,u) ∈ Str
Initialize Θ0 randomly.
for t = 0 to T do
Sample set Bt ⊆ Vtr of size m uniformly at random from all subsets of Vtr.
Compute the update term ut as the sum of the clipped gradient terms in the batch Bt :
Ut 一 X CliPc(Vθ' (GNN(A, X,v; Θt); yv))
v∈Bt
Add independent Gaussian noise to the update term: Ut -Ut + N (0,σ2I)
Update the current estimate of the parameters with the noisy update: Θt+ι J Θt —	Ut
end
4	Learning Graph Convolutional Networks (GCN) via DP-SGD
In this section, we provide a variant of DP-SGD (Bassily et al., 2014) designed specifically for GCNs
(Equation 2), and show that our method guarantees node-level DP (Definition 2).
The first step in our method is to subsample the neighborhood of each node to ensure that each node
has only K neighbors. This is important to ensure that influence of a single node is restricted to
only K other nodes. Next, similar to standard mini-batch SGD technique, we sample a subset Bt of
m nodes chosen uniformly at random from the set Vtr of training nodes. In contrast to the standard
mini-batch SGD, that samples points with replacement for constructing a mini-batch, our method
samples mini-batch Bt uniformly from the set of all training nodes. This distinction is important for
our privacy amplification result. Once we sample the mini-batch, we apply the standard DP-SGD
procedure of computing the gradient over the mini-batch, clipping the gradient and adding noise to
it, and then use the noisy gradients for updating the parameters.
DP-SGD requires each update to be differentially private. In standard settings where each gradient
term in the mini-batch corresponds to only one point, We only need to add O(C) noise - where C is
the clipping norm of the gradient - to ensure privacy. However, in the case of GCNs with node-level
privacy, perturbing one node/point vb can have impact loss term corresponding to all its neighbors
Nvb. So, to ensure the privacy of each update, we add noise according to the sensitivity of aggregated
gradient V®L(Bt； Θt):= Pv∈B古 Clipc(Vθ' (GCN(A, X, v; Θt); yv)) wrt an individual node b.
To this end, we provide a finer bound in Lemma 1 on the sensitivity of VΘL(Bt; Θt) based on the
maximum degree of the graph G.
In traditional DP-SGD, a crucial component in getting a better privacy/utility trade-off over just
adding noise according to the sensitivity of the minibatch gradient, is privacy amplification by sam-
pling (Kasiviswanathan et al., 2008; Bassily et al., 2014). This says that if an algorithm A is ε-DP
on a data set Di, then on a random subset D? ⊆ Di it satisfies roughly |D2| (eε - 1)-DP. Unlike
traditional ERMs, we cannot directly use this result in the context of GCNs. The reason is again that
on two adjacent data sets, multiple loss terms corresponding to vb and its neighbors Nvb get modified.
To complicate things further, the minibatch Bt that gets selected may only contain a small random
subset ofNvb. To address these issues, we provide anew privacy amplification theorem (Theorem 1).
To prove the theorem, we adapt (Feldman et al.,2018, Lemma 25) 一 that shows a weak form of con-
vexity of Renyi divergence - for our specific instance, and provide a tighter bound by exploiting the
special structure in our setting along with the bound on sensitivity discussed above.
Theorem 1 (Amplified Privacy Guarantee for any 1-Layer GCN). Consider the loss function L
of the form: L(G, Θ) = v∈V ` (GCN(A, X, v; Θt); yv) . Recall, N is the number of training
nodes Vtr, K is an upper bound on the maximum degree of the input graph, and m is the batch size.
5
Under review as a conference paper at ICLR 2022
For any choice of the noise standard deviation σ > 0 and clipping threshold C, every iteration t of
Algorithm 1 is (α, γ) node-level Renyi DP, where:
Y = α-ιln EP
exp I α(α — 1) ∙
2ρ2C2
σ2
,P 〜Hypergeometric(N, K +1,m).
Hypergeometric denotes the standard hypergeometric distribution (Forbes et al., 2011).
By the Standard Composition theoremfor Renyi Differential Privacy (Mironov, 2017b), over T iter-
ations, Algorithm 1 is (α, YT) node-level Renyi DP, where Y and a are defined above.
See Appendix A for a detailed proof.
Remark 1: Roughly, for m K and for T = O(1), the above bound implies σ = O(K) noise to
be added per step to ensure RDP with α = O(1) and Y = O(1). In contrast, the standard DP-SGD
style privacy amplification do not apply to our setting as each gradient term can be impacted by
multiple nodes.
Remark 2: We provide node-level privacy, that is the method preserves neighborhood information
of each node as well. But, we require asymmetric/directed graph, that is, changing a row in the
adjacency matrix does not impact any other part of the matrix. This is a natural assumption in a
variety of settings, for example, in social networks when the graph is constructed by “viewership”
data, edge (v, v0) exists iff user v viewed a post from user v0.
Remark 3: While we provide a formal privacy guarantee for 1-layer GCNs, the same applies for
any 1-layer GNN model.
Remark 4: We adapt a DP version of the Adam (Kingma & Ba, 2014; TFP) optimizer to the GNN
setting, called DP-GNN (Adam), with details in Appendix D.
Privacy at Inference Time: Note that Theorem 1 guarantees that the GCN parameters Θ that are
learnt via Algorithm 1 preserve privacy. However, unlike standard ML models where prediction
for each point depends only on the model parameters Θ and the point itself, the privacy of Θ does
not imply that inference using the GCN model (or any GNN model) will be privacy preserving. In
general, the inference about node v can reveal information about its neighbors Nv . Broadly, there
are three settings where we can infer labels for a given node while preserving privacy:
1.	Each node has access to the features of its neighbors. In this setting, the aggregation of fea-
tures from the neighbors does not lead to any privacy loss. Several real-world problems admit
such a setting: for example, in social networks where any user has access to a variety of activi-
ties/documents/photos of their friends (neighbors).
2.	Node features are completely private. In this setting, a node v does not have direct access to the
features of its neighbors Nv. Here, the standard GCN model is not directly applicable, but we can
still apply GCNs by aggregating the neighborhood features with noise. Generally, the resulting
prediction for a node would be meaningful only if the degree of the node is reasonably large.
3.	Training and test graph datasets are disjoint. In this setting, the goal is to privately learn Θ
using the training graph, that can be ‘transferred’ to the test graphs. Additionally, the feature
information is shared publicly within test graph dataset nodes. A variety of problems can be
modeled by this setting: organizations can be represented by a graph over its employees, with the
goal to learn a private ranking/recommendation model that can easily be adapted for completely
distinct organizations.
While there are multiple problems that can be modeled by the above mentioned settings, we focus
on the first setting for our empirical results.
5	Experimental Results
In this section, we present empirical evaluation of our method on standard benchmarks from the
widely used Open Graph Benchmark (OGB) suite (Hu et al., 2020). The goal is to demonstrate that
our method (DP-GNN) can indeed learn privacy preserving 1-layer GCNs accurately.
6
Under review as a conference paper at ICLR 2022
Table 1: Test accuracy of DP-GNN compared to the baselines on different datasets. DP-GNN clearly performs better than the Private and Non-Private MLP baselines across these datasets.			
Algorithm	ogbn-arxiv	ogbn-products	ogbn-mag
GCN	68.422 ± 0.267=Z	76.139 ± 0.519	34.680 ± 0.424
DP-GNN (Adam)	63.934 ± 0.469	69.576 ± 0.276	30.059 ± 0.252
DP-GNN (SGD)	64.137 ± 0.621	69.041 ± 0.126	30.147 ± 0.241
MLP	55.236 ± 0.317	61.364 ± 0.132	26.969 ± 0.361
DP-MLP	53.462 ± 0.242	61.064 ± 0.110	25.259 ± 0.321
As mentioned earlier, in several data critical scenarios, practitioners cannot use sensitive graph in-
formation, and have to completely discard GNN based models due to privacy concerns. Hence, the
main benchmark of our evaluation is to demonstrate that DP-GNN is able to provide more accurate
solutions than standard methods that completely discard the graph information. The key baselines
for our method are both standard non-private MLP models as well as differentially private MLP
models trained using DP-SGD and DP-Adam. We also compare against the standard 1-layer GCNs
(without any privacy guarantees) as it bounds the maximum accuracy we can hope to achieve out of
our method.
5.1	Datasets and Setup
OGB datasets: We use three moderate-to-large sized node classification datasets from OGB suite3:
ogbn-arxiv, ogbn-products and ogbn-mag. The ogbn-arxiv and ogbn-mag datasets consist of papers
extracted from the Microsoft Academic Graph (MAG) dataset (Wang et al., 2020). The ogbn-arxiv
dataset is a paper citation network of arxiv papers and consists of around 169K nodes, while the
ogbn-mag dataset is a heterogenous graph with node types papers, authors, institutions and topics
and consists of around 1.9M nodes. However, following the standard approach in (Hu et al., 2020)
we create a homogeneous graph of papers (736K nodes) from the ogbn-mag dataset. The ogbn-
products dataset is an Amazon products co-purchasing network and consists of 2.4M nodes. Each
dataset consists of edges, node features and labels (multi-class), and is split into standard train,
test and validation sets (Hu et al., 2020). Finally, following (Hu et al., 2020), we consider the
transductive semi-supervised setting for all the datasets, i.e., the entire graph is available during
training but only a few nodes in Vtr have labels available. See Appendix E for additional details
about the datasets.
Gradient Clipping: For DP-GNN, we perform layer-wise gradient clipping, i.e., the gradients cor-
responding to the encoder, aggregation and decoder functions are clipped independently with dif-
ferent clipping thresholds. For each layer, the clipping threshold C in Algorithm 1 is chosen as
Cf × C% where Cf is a scaling factor and C% is the 75th percentile of gradient norms for that layer
at initialization on the training data. We finetune the Cf parameter for each dataset. We set the noise
for each layer σ such that the noise multiplier λ = 2(k+i)c is identical for each layer, where σ∕λ
is essentially the sensitivity. It is not hard to observe that the overall privacy cost only depends on λ.
Methods: We benchmark the following methods: a) DP-GNN: Our method (Algorithm 1) special-
ized for a 1-layer GCN with an MLP as the encoder and the decoder, b) GCN: A 1-layer GCN with
an MLP encoder and decoder. This defines the highest possible numbers for our method but due to
privacy concerns, non-private GCN might not be suitable for deployment in practice, c) MLP: A
standard multi-layer perceptron (MLP) architecture on the raw node features as proposed in prior
works (Hu et al., 2020). This model does not utilize any graph level information, d) DP-MLP: A
DP version of MLP (with standard architecture) trained using DP-Adam (TFP).
Detailed Setup and Hardware: DP-GNN and all the aforementioned baselines are implemented in
TensorFlow 2.0 (Abadi et al., 2015) using Graph Nets4 and Sonnet5. All experiments are performed
on 2x2 TPU v2 Pods. We perform model selection for all the methods based on their performance
on the validation set. We run each experiment nine times and report the mean and standard deviation
for performance on the test set in Table 1.
Hyperparameter Tuning: We perform exhaustive grid search over batch size, learning rate, ac-
tivation functions, and number of encoder and decoder MLP layers for the non-private baselines.
3 ogb.stanford.edu/docs/nodeprop 4 github.com/deepmind/graph_nets 5 github.com/deepmind/sonnet
7
Under review as a conference paper at ICLR 2022
(a) ogbn-arxiv	(b) ogbn-products	(c) ogbn-mag
Figure 1: Performance of the proposed DP-GNN method as well as the baselines on the ogbn-
arxiv, ogbn-products and ogbn-mag datasets. Clearly, DP-GNN offers a performance better than
both of the Non-Private MLP and DP-MLP methods, with a reasonable privacy budget of ε ≤ 30.
Additionally, we tune over noise multiplier (σ in Algorithm 1) and clipping thresholds for the pri-
vate baselines. We provide detailed information regarding the hyperparameters in Appendix E.
Results: Table 1 compares DP-GNN’s accuracy against baselines on the ogbn-arxiv, ogbn-products
and ogbn-mag datasets. We extensively tune baselines on the three datasets as mentioned above and
are able to replicate, and in some cases, improve the reported performance numbers for the baselines
(Hu et al., 2020). We use the higher number of the two for comparison with our method.
Overall, we observe that our proposed method DP-GNN significantly outperforms the Non-Private
MLP (without any usage of the graphs) and DP-MLP (trained using standard DP-Adam) baselines
on all of the datasets and with a reasonable privacy budget of ε ≤ 30. For example, for ogbn-arxiv
dataset, our method DP-GNN (SGD) is about 8% more accurate than MLP and 10% more accurate
than DP-MLP. Similarly, for ogbn-products our method is about 5% more accurate than both MLP
and DP-MLP. Note that we also present numbers for DP-GNN (Adam) (see Appendix D) that uses
Adam as the optimizer instead of SGD, as mentioned in Algorithm 1. Also, note that for the rest of
the section we use DP-GNN (Adam) for generating accuracy numbers.
Next, Figure 1 provides a comparison of epsilon vs test set accuracy for the three benchmark
datasets. Note that for ε ≥ 10, DP-GNN is significantly more accurate than DP-MLP. It is in-
teresting to note that for about ε ≥ 10, the accuracy of the DP-MLP saturates and does not increase
significantly. In contrast, the accuracy of DP-GNN keeps on increasing with larger ε, and is in gen-
eral much higher than both MLP and DP-MLP for higher values of ε. Finally, on ogbn-products,
DP-GNN is about 5% more accurate than DP-MLP for the entire range of considered values for ε,
and is about 2% more accurate than MLP for ε = 10.
Typically, for training non-convex learning models with user-level DP, ε ≤ 10 has become a popu-
lar choice (Papernot et al., 2020; Kairouz et al., 2021). But as the problem is more challenging in
the case of GNNs - multiple nodes can affect inference for a given node and We intend to protect
privacy at the node-level - higher ε seems like a reasonable choice to encourage reasonable solu-
tions. Moreover, as We observe on the ogbn-products dataset, larger dataset sizes can ensure better
performance for the standard ε values as well. Also, our algorithms satisfy stronger Renyi DP prop-
erties (Mironov, 2017b), Which provide additional protection over traditional (ε, δ)-DP guarantees.
5.2	Ablation Studies
Batch size m: As has been noted in other DP-SGD works (Abadi et al., 2016; Bagdasaryan et al.,
2019), we empirically observe that increasing the batch size helps the performance of the learnt
DP-GNN, up to a point. There are multiple effects at play here.
Larger batch sizes imply that the effective noise added per DP-SGD update step is smaller. Thus,
training is more stable with larger batch sizes, as Figure 2 shows. Furthermore, effective privacy
budget (ε) provided by amplification result has a term of the form exp(ε0) - 1 where ε0 is the
privacy budget for a step. So, unless ε0 is small enough, i.e., the batch size is large enough, the
amplification result would be weak. On the other hand, larger batch sizes tend to hurt generalization
and training speed, even in the non-private case, as the second column of Table 2 shows.
Thus, there is a trade-off between model performance, privacy budget and batch size. As the last
column of Table 2 shows, the difference in performance between private and non-private models
8
Under review as a conference paper at ICLR 2022
Table 2:	GCN and DP-GNN on the ogbn-
arxiv dataset with different batch sizes.
The privacy budget for DP-GNN is ε ≤ 30.
Table 3:	GCN and DP-GNN on the ogbn-
arxiv dataset with different degrees. The
privacy budget for DP-GNN is ε ≤ 30.
Batch SiZe GCN (AGCN)	DP-GNN (ADP-GNN) AGCN - ADP-GNN
^T00	68.075	40.814
500	68.393	58.882
1250	68.572	61.307
2500	68.356	63.025
5000	68.490	64.345
10000	68.062	64.304
20000	68.491	62.062
27.261
9.511
7.265
5.331
4.145
3.758
6.429
Degree GCN (AGCN) DP-GNN (ADP-GNN) AGCN - ADP-GNN
-3	68563	63.439	5.124
5	69.020	63.940
7	68.945	64.599
10	68.372	64.103
15	68.224	63.522
20	68.642	63.054
32	68.152	61.901
5.080
4.346
4.269
4.702
5.588
6.251
(a) Varying Batch SiZe m
(b) Varying Maximum Degree K
Figure 2: Ablation studies on DP-GNN on the ogbn-arxiv dataset. (a) shows privacy-utility
curves for a range of batch siZes for the DP-GNN. (b) shows privacy-utility curves when varying
maximum degree K for the DP-GNN. In both analyses, the other hyperparameters are kept fixed.
tends to diminish as the batch siZe increases. However, for the reasons pointed out above, beyond a
batch siZe of 10000, the accuracy goes down, as quantified by Table 2.
Maximum Degree K: Compared to the batch siZe, the maximum degree K has less of an effect on
both non-private and private models trained on ogbn-arxiv, as Table 3 shows. Generally, there is still
a trade-off: a smaller K means lesser differentially private noise added at each update step, but also
fewer neighbours for each node to aggregate information from.
Finally, we also conduct experiments to understand performance of DP-GNN conditioned on the
frequency of a class (how often a class appears in the dataset), with details in Appendix F. On the
whole, these experiments suggest that DP-GNN is able to classify data points of “frequent” classes
with reasonable accuracy, but struggles with classification accuracy on the data points of “rarer”
classes. This observation is in line with previous claims from (Bagdasaryan et al., 2019; Fioretto
et al., 2021) that differentially-private models generally perform worse on low-frequency classes,
and represent a critical future direction to study.
6	Conclusions and Future Work
In this work, we proposed a method to privately learn 1-layer GNN parameters, that outperforms
both private and non-private baselines that do not utiliZe graph information. Our method ensures
node-level differential privacy, by a careful combination of sensitivity analysis of the gradients and
a privacy amplification result extended to the GNN style settings. We believe that our work is a
first step in the direction of designing powerful GNNs while preserving privacy. Promising avenues
for future work include learning more general class of GNNs, investigating inference mechanisms
mentioned in Section 4 such as different train and test graph datasets, and understanding utility
bounds for GNNs with node-level privacy.
7	Reproducibility Statement
We have taken all efforts to ensure that the results produced in the paper and the submitted ma-
terial are reproducible, and the methodology is easy to follow. For our theoretical contributions,
we have discussed the problem setup and preliminaries in Section 3, provided a detailed algorithm
for our proposed methodology in Section 4 for a sound theoretical understanding of the problem
9
Under review as a conference paper at ICLR 2022
and our solution. For our empirical results, we have detailed the information needed to reproduce
the empirical results in Section 5 of the main paper and Appendix E. We supply all the required
information regarding the datasets, their pre-processing and source, implementation details for our
method and the baselines, specifics regarding the architectures, hyperparameter search spaces and
the best hyperparameters corresponding to our experiments. We are working towards an open source
implementation, in the spirit of reproducible research.
8	Ethics Statement
The interest in differentially-private models largely stems from a need to protect the privacy of data
samples used to train these models. While we have proposed a mechanism here to learn GNNs
in privacy-preserving manner, differential privacy seems to exacerbate existing fairness issues on
underrepresented classes as Appendix F indicates. This is a concern across all models trained with
differential privacy (Bagdasaryan et al., 2019) that needs to be addressed before such models can be
deployed in the real world. While there have been recent attempts (Jagielski et al., 2018; Fioretto
et al., 2021) to mitigate the disparate effect of differentially private training, there is still a need for
an effective practical solution. We anticipate no other negative consequences of our work.
References
TensorFlow Privacy. https://github.com/tensorflow/privacy. Accessed: 2021-10-
05.
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, An-
drew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Man-
junath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray,
Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Mar-
tin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Ma-
chine Learning on Heterogeneous Systems, 2015. URL https://www.tensorflow.org/.
Software available from tensorflow.org.
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep Learning with Differential Privacy. In Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security, CCS ,16, pp. 308-318, New
York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341394. doi:
10.1145/2976749.2978318. URL https://doi.org/10.1145/2976749.2978318.
David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton Fookes, and Lars Pe-
tersson. Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and
Future. arXiv preprint arXiv:2105.13137, 2021.
Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-Scale Dif-
ferentially Private BERT. CoRR, abs/2108.01624, 2021. URL https://arxiv.org/abs/
2108.01624.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate im-
pact on model accuracy. Advances in Neural Information Processing Systems, 32:15479-15488,
2019.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private Empirical Risk Minimization: Efficient
Algorithms and Tight Error Bounds. In Proc. of the 2014 IEEE 55th Annual Symp. on Foundations
of Computer Science (FOCS), pp. 464-473, 2014.
Christian Borgs, Jennifer T. Chayes, and Adam Smith. Private Graphon Estimation for Sparse
Graphs, 2015.
Christian Borgs, Jennifer Chayes, Adam Smith, and Ilias Zadik. Revealing Network Structure,
Confidentially: Improved Rates for Node-Private Graphon Estimation, 2018.
10
Under review as a conference paper at ICLR 2022
Amar Budhiraja, Gaurush Hiranandani, Darshak Chhatbar, Aditya Sinha, Navya Yarrabelly, Ayush
Choure, Oluwasanmi Koyejo, and Prateek Jain. Rich-Item Recommendations for Rich-Users:
Exploiting Dynamic and Static Side Information, 2020.
Nicholas Carlini, Chang Liu, UJlfar Erlingsson, Jernej Kos, and DaWn Song. The Secret Sharer:
Evaluating and Testing Unintended Memorization in Neural Networks. In 28th USENIX Security
Symposium (USENIX Security 19), pp. 267-284, Santa Clara, CA, August 2019. USENIX As-
sociation. ISBN 978-1-939133-06-9. URL https://www.usenix.org/conference/
usenixsecurity19/presentation/carlini.
Cynthia DWork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating Noise to Sensitivity
in Private Data Analysis. In Shai Halevi and Tal Rabin (eds.), Theory of Cryptography, pp. 265-
284, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-32732-5.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and DaWei Yin. Graph neural
netWorks for social recommendation. In The World Wide Web Conference, pp. 417-426, 2019.
Vitaly Feldman, Ilya Mironov, Kunal TalWar, and Abhradeep Thakurta. Privacy Amplification by
Iteration. In 59th Annual IEEE Symp. on Foundations of Computer Science (FOCS), pp. 521-532,
2018.
Ferdinando Fioretto, Cuong Tran, and Pascal Van Hentenryck. Decision Making With Differential
Privacy under a Fairness Lens. CoRR, abs/2105.07513, 2021. URL https://arxiv.org/
abs/2105.07513.
C. Forbes, M. Evans, N. Hastings, and B. Peacock. Statistical Distributions. Wiley, 2011. ISBN
9781118097823. URL https://books.google.co.in/books?id=YhF1osrQ4psC.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neu-
ral Message Passing for Quantum Chemistry. CoRR, abs/1704.01212, 2017. URL http:
//arxiv.org/abs/1704.01212.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large
Graphs. CoRR, abs/1706.02216, 2017. URL http://arxiv.org/abs/1706.02216.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BoWen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020.
MattheW Jagielski, Michael J. Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-
Malvajerdi, and Jonathan R. Ullman. Differentially Private Fair Learning. CoRR, abs/1812.02696,
2018. URL http://arxiv.org/abs/1812.02696.
Peter Kairouz, Brendan Mcmahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng
Xu. Practical and Private (Deep) Learning Without Sampling or Shuffling. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 5213-5225. PMLR, 18-24 Jul
2021. URL https://proceedings.mlr.press/v139/kairouz21b.html.
Vishesh KarWa, Sofya Raskhodnikova, Adam Davison Smith, and Grigory Yaroslavtsev. Private
analysis of graph structure. Proceedings of the VLDB Endowment, 4(11):1146-1157, August
2011. ISSN 2150-8097.
Shiva Prasad KasivisWanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam D.
Smith. What Can We Learn Privately? In 49th Annual IEEE Symp. on Foundations of Computer
Science (FOCS), pp. 531-540, 2008.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N. Kipf and Max Welling. Semi-Supervised Classification With Graph Convolutional Net-
Works. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907.
11
Under review as a conference paper at ICLR 2022
Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, and Daniel
Rueckert. Metric learning with spectral graph convolutions on brain connectivity networks. Neu-
roImage,169:431-442, 2018.
Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, and
Ananda Theertha Suresh. Learning with User-Level Privacy. CoRR, abs/2102.11845, 2021. URL
https://arxiv.org/abs/2102.11845.
Kevin McCloskey, Ankur Taly, Federico Monti, Michael P Brenner, and Lucy J Colwell. Using
attribution to decode binding mechanism in neural network models for chemistry. Proceedings of
the National Academy of Sciences, 116(24):11624-11629, 2019.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations Sym-
posium (CSF), pp. 263-275. IEEE, 2017a.
Ilya Mironov. Renyi Differential Privacy. CoRR, abs/1702.07476, 2017b. URL http://arxiv.
org/abs/1702.07476.
Iyiola E. Olatunji, Thorben Funke, and Megha Khosla. Releasing Graph Neural Networks with
Differential Privacy Guarantees, 2021.
Nicolas Papernot, Martin Abadi, UJlfar Erlingsson, Ian J. Goodfellow, and Kunal Talwar. Semi-
supervised Knowledge Transfer for Deep Learning from Private Training Data. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings. OpenReview.net, 2017. JRL https://openreview.net/
forum?id=HkwoSDPgg.
Nicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Jlfar Erlingsson. Making
the Shoe Fit: Architectures, Initializations, and Tuning for Learning with Privacy, 2020. JRL
https://openreview.net/forum?id=rJg851rYwH.
Sofya Raskhodnikova and Adam Smith. Lipschitz Extensions for Node-Private Graph Statistics and
the Generalized Exponential Mechanism. In 2016 IEEE 57th Annual Symposium on Foundations
of Computer Science (FOCS), pp. 495-504, 2016. doi: 10.1109/FOCS.2016.60.
Sina Sajadmanesh and Daniel Gatica-Perez. When Differential Privacy Meets Graph Neural Net-
works. CoRR, abs/2006.05535, 2020. JRL https://arxiv.org/abs/2006.05535.
Chuanqiang Shan, Huiyun Jiao, and Jie Fu. Towards Representation Identical Privacy-Preserving
Graph Neural Network via Split Learning. CoRR, abs/2107.05917, 2021. JRL https://
arxiv.org/abs/2107.05917.
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differ-
entially private updates. In 2013 IEEE Global Conference on Signal and Information Processing,
pp. 245-248. IEEE, 2013.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph Attention Networks, 2018.
Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia.
Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396-
413, 2020.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.
Dynamic Graph CNN for Learning on Point Clouds. Acm Transactions On Graphics), 38(5):
1-12, 2019.
Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. FedGNN: Federated Graph
Neural Network for Privacy-Preserving Recommendation. CoRR, abs/2102.04925, 2021a. JRL
https://arxiv.org/abs/2102.04925.
Fan Wu, Yunhui Long, Ce Zhang, and Bo Li. LinkTeller: Recovering Private Edges from Graph
Neural Networks via Influence Analysis, 2021b.
12
Under review as a conference paper at ICLR 2022
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural
Networks? CoRR, abs/1810.00826, 2018. URL http://arxiv.org/abs/1810.00826.
Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 7370-7377,
2019.
Jun Zhou, Chaochao Chen, Longfei Zheng, Xiaolin Zheng, Bingzhe Wu, Ziqi Liu, and Li Wang.
Privacy-Preserving Graph Neural Network for Node Classification. CoRR, abs/2005.11903, 2020.
URL https://arxiv.org/abs/2005.11903.
Appendix
This appendix is segmented into four key parts:
1.	Section A gives detailed proofs for the Lemmas discussed in the main paper and additional
lemmas that could not be included in the paper due to space constraints.
2.	Section D provides a detailed description of learning Graph Convolutional Networks via
DP-Adam.
3.	Section E discusses experimental details for reproducing the results in the main paper.
4.	Section F provides additional results for analysing the performance of the DP-GNN model
as compared to the GCN baseline.
A Lemmas and Proofs
Lemma 1 (Node-Level Sensitivity of any 1-Layer GCN). Consider the loss function L of the form:
L(G, Θ)= X ' (GCN(A, X, v; Θ); yv).
v∈V
Let Bt be any choice of m unique nodes from a graph G with maximum degree bounded above by
K. Consider the following quantity ut from Algorithm 1:
Ut(G) = X CliPc(Vθ' (GCN(A, X, v; Θt); y))
v∈Bt
Note that Ut(G) is a'clipped' version of VθL(Bt; Θt, G):
V®L(Bt； Θt, G)= X Vθ' (GCN(A, X, v; Θt); y)
v∈Bt
Then, the following inequality holds:
∆K(Ut) < 2(K+ 1)C.
Proof. Let G be an arbitrary graph dataset with adjacency matrix A and maximum degree bounded
above by K . Consider an adjacent graph dataset G0 with adjacency matrix A0 formed by removing
a single node vb from G. We wish to bound the following quantity:
kUt(G)-Ut(G0)kF
For convenience, for any node v, We denote the corresponding loss terms 'v and 'V as:
`v = ` (GCN(A, X, v; Θt); yv)
'V = ' (GCN(A0, X0,v; Θt); yv)
From the definition of 'v, it is clear that the only gradient terms Vθ'v affected when adding or
removing node vb, are those of its neighbors and vb itself. Thus,
Ut(G) - Ut(G0)
=ClipC (vθ'b) ∙ I[b ∈ Bt] + X (ClipC (vθ'u) - ClipC (Vθ'U)) ∙ I[u ∈ Bt]
u∈Nvb
13
Under review as a conference paper at ICLR 2022
where I is the indicator random variable. Taking norms:
kut(G)-ut(G0)kF
=CliPC(V®'b) ∙I[V ∈ Bt] + X (CliPC(Vθ'u) - CliPC(V®'U)) ∙I[u ∈ Bt]
u∈Nvb	F
≤ kClipc(Vθ'v) ∙I[b eBt]∣∣F + X k(Clipc(Vθ'u) - CliPC(Vθ'U)) ∙I[u ∈ Bt]kp
u∈Nvb
(triangle inequality)
≤ kClipC(Vθ'v)kF + X k(ClipC(Vθ'u) - ClipC(Vθ'U))kF
u∈Nvb
(I∈{0,1})
≤ kClip。(Vθ'v)kF + X (kClip。(Vθ'u)kF + kClip。(Vθ'U)kF)
u∈Nvb
(triangle inequality)
≤ C+ X(C+C)
u∈Nvb
(gradient clipping)
=C + dv ∙ 2C
(definition of dvb )
= C(2dvb + 1) ≤ C(2K + 1) < 2(K+ 1)C.
(dvb ≤ K, C > 0)
As G and G0 were an arbitrary pair of node-level adjacent graph datasets,
∆K(ut) =	max	kut(G) - ut(G0)kF
deg(G), deg(G0)≤K	F
node-level adjacent
G,G0
< 2(K+ 1)C.
The proof for the bound on ∆K(ut(G)) when a new node vb is added to the graph G follows analo-
gously.	口
Lemma 2 (Un-amplified Privacy Guarantee for Each Iteration of Algorithm 1). Every iteration t
of Algorithm 1 is (α, Y) node-level Renyi DP for when run on graphs with maximum degree ≤ K
where:
_ α ∙ (∆κ(ut))2
Y =	2σ2
Here ∆κ(∙) is the K-restriCted node-level Sensitivityfrom Definition 3.
Proof. Follows directly from (Mironov, 2017a, Corollary 3).
□
Lemma 3 (Distribution of Loss Terms Per Minibatch). For any iteration t in Algorithm 1, Con-
sider the minibatCh Bt of subgraphs. For any subset S of d unique nodes, define the ran-
dom variable ρ as |S ∩ Bt |. Then, the distribution of ρ follows the hypergeometriC distribution
Hypergeometric(N, d, m):
Pi = P [ρ = i] =驾-d)
where N is the total number of nodes in the training set Vtr and |Bt | = m is the batCh size.
Proof. The minibatches Bt in Algorithm 1 are formed by sampling nodes from Vtr without re-
placement. When ρ = i, one needs to pick i nodes from S and the remaining m - i nodes from
14
Under review as a conference paper at ICLR 2022
Vtr -S to form a batch of size m. Clearly, there are (IS|) = (d) Ways to do the first step, and
(IStrHS|) = (N-d) to do the second step. Finally, there are (N) ways to choose a minibatch Bt of
m-i	m-i	m	t
size m, each choice equally likely. In conclusion, we can claim:
which is exactly the Hypergeometric(N, d, m) distribution.
□
Lemma 4 (Adaptation of Lemma 25 from Feldman et al. (2018)). Let μo,...,μn and νo,...,νn
be probability distributions over some domain Z such that:
Dα(μo k νo) ≤ εo
...
Dα(μn k Vn) ≤ εn
for some given ε0 , . . . , εn.
Let P be a probability distribution over [n] = {0,...,n}. Denote by μρ (respectively, VP) the
probability distribution over Z obtained by sampling i from ρ and then outputting a random sample
from μi (respectively, νi). Then:
n
Dα(μρ k VP) ≤ lnEi〜PHigT)] = α-1lnXPieHaf.
Proof. Let μρ (respectively VP) be the probability distribution over [n] X Z obtained by sampling i
from ρ and then sampling a random X from μi (respectively, Vi) and outputting (i,χ). We can obtain
μρ from μ" by applying the function that removes the first coordinate; the same function applied
gives VP from VP0 . Therefore, by the post-processing properties of the Renyi divergence, we obtain
that:
Dα(μρ k VP) ≤ Da(μρ k VP).
Now, observe that for every i ∈ [n] and X ∈ Z, μP(i, X)= ρi ∙ μi(x). Therefore,
Da(MP k VP)
1
α- 1
1
α- 1
1
α- 1
1
α- 1
1 w	「〃P(i,x)]
ln E(i, X) ~ν0 [”P (i,χj
ln Ei 〜PHi(a-1)]
n
ln X Pieεi(a-1)
i=1
as required.
□
Lemma 5. Let X be a non-negative continuous random variable with cumulative distribution func-
tion FX and density fX. Letg : R≥0 → R be a differentiable function. Then:
∞
E[g(X)] = g(0) +	g0 (X)(1 - FX (X)) dX
0
15
Under review as a conference paper at ICLR 2022
Proof.
∞∞
g0(x)(1 - FX (x)) dx =	g0 (x) Pr [X > x] dx
00
∞ g0(x)	∞fX(t)dtdx
0x
g0(x)fX (t) dt dx
0x
Z Z g0 (x)fX (t) dx dt
00
fX (t) Z g0 (x) dx dt
/
0
Z
0
∞ fX (t) (g(t) - g(0)) dt
E[g(X) - g(0)] = E[g(X)] - g(0).
as claimed.
□
An analogous inequality holds for discrete random variables, taking values on Z.
Lemma 6. Let X be a discrete random variable taking values on Z with cumulative distribution
function FX and probability mass function fX . Let g : Z → R be a function. Then:
∞
E[g(X)] = g(0) + X(g(x+ 1) - g(x))(1 - FX (x)).
x=0
Proof. The proof is identical to that of Lemma 5, by replacing integrals with sums.	口
Lemma 7. Let ρ and ρ0 be two random variables with the hypergeometric distribution:
P 〜Hypergeometric(N, k,m)
ρ0 〜Hypergeometric(N, k0, m)
such that k ≥ k0. Then, ρ stochastically dominates ρ0:
Fρ0 (i) ≥ Fρ(i) for all i ∈ R
where Fρ (respectively, Fρ0 ) is the cumulative distribution function (CDF) of ρ (respectively, ρ0).
Proof. Note the following representation of the hypergeometric random variable as the sum of de-
pendent Bernoulli random variables:
m
ρ= XXi
i=1
where each Xi 〜Bernoulli(N). Similarly, We have:
m
ρ0 = XXi0
i=1
where each Xi ~ Bernoulli(N). Now, as k ≥ k0, by a simple analysis for Bernoulli random
variables, each Xi0 is stochastically dominated by Xi :
FXi0 ≥ FXi .
for each i ∈ {1, . . . , m}. Thus, as sums preserve stochastic dominance:
Fρ0 = FPiN=1 Xi0 ≥ FPiN=1 Xi = Fρ	(4)
as required.
□
16
Under review as a conference paper at ICLR 2022
Lemma 8. Let ρ and ρ0 be two non-negative random variables such that ρ stochastically dominates
ρ0:
Fρ0 (i) ≥ Fρ(i) for all i ∈ R
where Fρ (respectively, Fρ0 ) is the cumulative distribution function (CDF) of ρ (respectively, ρ0).
Let g : R≥0 → R be a non-decreasing differentiable function. Then, the following inequality holds:
E[g(ρ0)] ≤ E[g(ρ)].
Proof. We first argue for the case where both ρ and ρ0 are continuous. By Lemma 5, we have that:
∞
E[g(ρ)] = g(0) +	g0(x)(1 - Fρ(x)) dx
0
∞
E[g(ρ0)] = g(0) +	g0(x)(1 - Fρ0 (x)) dx.
0
and hence:
∞
E[g(ρ)] - E[g(ρ0)] =	g0(x)(Fρ0 (x) - Fρ(x)) dx.
0
As g is non-decreasing, we have that g0 ≥ 0 everywhere. The theorem now follows directly.
The case where both ρ and ρ0 are discrete can be handled analogously, by using Lemma 6 above
instead.
□
We are now ready to supply the proof of the main theoretical result in this paper, Theorem 1.
Proof of Theorem 1. We borrow notation from the proof of Lemma 1. Let G be an arbitrary graph
with adjacency matrix A and maximum degree bounded above by K . Consider an adjacent graph
G0 with adjacency matrix A0 formed by removing a single node vb from G. For convenience, for any
node v, We denote the corresponding loss terms 'v and 'V as:
'v = ' (GCN(A, X,v; Θ); yv)
'V = ' (GCN(A0, X0,v; Θ); yv)
As in Lemma 1,
ut(G) - ut(G0)
=CliPC(Vθ'b) ∙ I[b ∈ Bt] + ^X (CliPC(Vθ'u) - CliPC(Vθ'U)) ∙ I[u ∈ Bt]	(5)
u∈Nvb
Where I is the indicator function. With the notation from Algorithm 1, We have:
Ut(G) = Ut(G)+ N (0,σ2I),
U t(G0) = Ut(G0) + N (0,σ2I).
We need to shoW that:
Da(Ut(G) k ut(G0)) ≤ γ.
LetS = {u | u = v oru ∈ Nbv} be the set of nodes ‘affected’ by the removal of vb. From Equation 5,
We see that the sensitivity of Ut dePends on the number of nodes in S that are Present in Bt :
kUt(G)-Ut(G0)kF
=CliPC (vθ'v)	∙	I[b	∈	Bt]	+ X	(ClipC (Vθ'U)- CliPC (Vθ'U)) ∙ I[u	∈	Bt]
u∈Nvb
F
Let ρ0 be the distribution over {0, 1, . . . dvb + 1} of the number of ‘affected’ nodes in S Present in
Bt, that is, ρ0 = |S ∩ Bt|. Lemma 3 then gives us that the distribution of ρ0 is:
ρ0 ~ Hypergeometric(N, dv + 1,m).	(6)
17
Under review as a conference paper at ICLR 2022
In particular, when ρ0 = i, exactly i nodes are sampled in Bt . Then, it follows by the same argument
in the proof of Lemma 1 that:
Thus, conditioning on ρ0 where:	∆K (ut | ρ0 = i) < 2iC. =i, we see that every iteration is (α, Yi ) node-level Renyi DP, by Lemma 2 (2iC)2	2i2C2 Yi =α ∙ 1σ2^ =α ∙	.	⑺
Define the distributions μ% and Vi for each i ∈ {0,...,dv + 1}, as follows:
	μi = [U(G) | ρ0 = i] Vi= [U(GO) | ρ0 = i]
Then, by Equation 7:	Da(μi k Vi) ≤ Yi
τ-, . i	∙	τ~τ / x-»\ ι	τ~τ /	τ	a	. 11	. ι
For the mixture distributions μρo = U(G) and Vρ0 = U(G0), Lemma 4 now tells us that:
Da(U(G) k U(G0)) = Dα(μρ0 k v。，)
	≤ & - JnEi〜ρ0 性Xp(Yi(α - 1))] 1	2i2C2 =	T ln Ei" exp a(a - 1) ∙ 一厂 α - 1	σ2 1	2ρ02C2 =	ln Eρ0 exp Q(Q — 1)  	K- α - 1	σ2 =」Tln E[f (P0)].	(8) Q-1
where:	f(ρ0) = exp 卜(Q - 1) ∙ 2ρ C )
Define another distribution ρ as:
	P 〜Hypergeometric(N, K +1,m).
As dvb ≤ K, by Lemma 7, ρ stochastically dominates ρ0 . Then, as f is non-decreasing, Lemma 8
gives us:	E[f(P0)]≤E[f(P)].	(9)
It follows from Equation 8 and Equation 9 that:
Da(U(G) k U(G0)) ≤ a—ɪlnEP exp (α(α - 1) ∙ 2'C ) = γ.
As this holds for an arbitrary pair of node-level adjacent graphs G and G, we are done.	□
18
Under review as a conference paper at ICLR 2022
B S ampling Subgraphs
To bound the sensitivity of the mini-batch gradient in Algorithm 1, we must carefully bound both
the in-degree and out-degree of any node in the graph across all training subgraphs. Algorithm 2
outputs a set of training subgraphs that ensures these degree constraints are met.
Note that once the model parameters have been learnt, no such degree restriction is needed at infer-
ence time. This means predictions for the ‘test’ nodes can use the entire neighbourhood information.
Algorithm 2: Sampling Subgraphs with In-Degree and Out-Degree Constraints
Data: Graph G = (V, E, X, Y), Training set Vtr, Maximum degree K.
Result: Set of training subgraphs Str .
for v ∈ V do
I Initialize Countv J 0. Initialize subgraph Sv — {v}.
end
Shuffle Vtr .
for v ∈ Vtr do
for u ∈ Nv do
If countu = K, continue.
If countv = K, break.
Add node u to subgraph Sv .
Add node v to subgraph Su .
Increment countu by 1.
Increment countv by 1.
end
end
Construct Str J {Sv | v ∈ Vtr}.
return Str.
C Experiments with Different GNN Architectures
As mentioned in Section 4, the DP-GNN training mechanisms can be used with any 1-layer GNN
architecture.
We experiment with different GNN architectures, namely GIN (XU et al., 2018) and GAT (Velickovic
et al., 2018) on the ogbn-arxiv dataset and report the results for the respective private and non-private
models in Table 4. We use a variant of the original GAT architecture, utilizing dot-product attention
instead of additive attention, with 10 attention heads.
We observe that DP-GNN performs reasonably well across different architectures.
Table 4: Test accuracy of DP-GNN (Adam) on the ogbn-arxiv dataset with a privacy budget of
ε ≤ 30.	Architecture Non-PrivateGNN	DP-GNN GCN	68.422 ±	0.267	63.934	±	0.469 GIN	67.485 ±	0.391	63.888	±	0.709 GAT	65.702 ±	0.674	58.853	±	0.246
19
Under review as a conference paper at ICLR 2022
D Learning Graph Convolutional Networks (GCN) via DP-Adam
In Algorithm 3, we provide the description of DP-Adam, which adapts Algorithm 1 to use the pop-
ular Adam (Kingma & Ba, 2014) optimizer, instead of SGD. The privacy guarantee and accounting
for Algorithm 3 is identical to that of Algorithm 1, since the DP clipping and noise addition steps
are identical.
Algorithm 3: DP-GNN (Adam): Differentially Private Graph Neural Network with Adam
Data: Graph G = (V, E, X, Y), GNN definition GNN, Training set Vtr ,Loss function L,
Batch size m, Maximum degree K, Learning rate η, Clipping threshold C, Noise
standard deviation σ, Maximum training iterations T, Adam hyperparameters (β1, β2).
Result: GNN parameters ΘT .
Note that Vtr is the subset of nodes for which labels are available (see Paragraph 1 of Section 3).
Using Vtr, construct the set of training subgraphs Str with Algorithm 2.
Construct the 0 一 1 adjacency matrix A: Avu = 1 ^⇒ (v,u) ∈ Str
Initialize Θ0 randomly.
for t = 0 to T do
Sample set Bt ⊆ Vtr of size m uniformly at random from all subsets of Vtr
Compute the gradient term ut as the sum of the clipped gradient terms in the batch Bt :
Ut 一 X CliPC(Vθ' (GNN(A, X,v; Θt); yv))
v∈Bt
Add independent Gaussian noise to the gradient term: Ut J Ut + N(0,σ2I)
Update first and second moment estimators with the noisy gradient, correcting for bias:
ft —	β1	∙	ft-1 + (I 一	βI)	∙	Ut
St —	β2	∙	st-1 + (1 —	β2)	∙	(Ut Θ	Iit)
fbt J
sbt J
ft
ι — β1
St
ŋ!
Update the current estimate of the parameters with the noisy estimators:
Θt+1 J Θt 一
^
η	ft
m p2 + ε
end
E Experimental Details and Reproducibility
Table 5 provides details on the benchmark node classification datasets from the OGB suite used in
the experiments. The following 3 datasets were used to demonstrate the effectiveness of our method:
ogbn-arxiv6 and ogbn-mag7 dataset consisting of papers extracted from the Microsoft Academic
Graph (MAG) dataset (Wang et al., 2020) and ogbn-products8 dataset which is a co-purchasing
network of Amazon products.
Hyperparameter configurations for all methods: We use the following ‘inverse-degree’ normal-
ization of the adjacency matrix for all GCN models:
Ab = (d + I)-1(A + I).
Adam (Kingma & Ba, 2014) with β1 = 0.9 and β2 = 0.999, and SGD optimizers were used for
training all methods for each of the datasets. We fix C% as 75.
6 https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv 7 https://ogb.stanford.edu/docs/nodeprop/#ogbn-mag
8 https://ogb.stanford.edu/docs/nodeprop/#ogbn-products
20
Under review as a conference paper at ICLR 2022
Table 5: Statistics of datasets used in our experiments. On all of these datasets, the task is to
classify individual nodes into one of multiple classes.
Dataset	# Nodes	Avg. Degree	# Features	# Classes	Train/Val/Test Splits
ogbn-arxiv	169,343	13.7	128	40	0.54/0.18/0.28
ogbn-products	61,859,140	50.5	100	47	0.08/0.02/0.90
ogbn-mag	736,389	21.7	128	349	0.85/0.09/0.05
A dataset-specific grid search was performed over the other hyperparameters for each method, men-
tioned below. lr refers to the learning rate, nenc refers to the number of layers in the encoder MLP,
ndec refers to the number of layers in the decoder MLP, λ refers to the noise multiplier, Cf refers to
the clipping scaling factor, and K refers to the sampling degree.
ogbn-arxiv:
•	Non-Private GCN: lr in {0.001, 0.002, 0.003}, nenc in {1, 2}, ndec in {1, 2}, Batch Size
in {1000}, Activation in {ReLU}, K in {7, 10}.
•	DP-GNN: lr (Adam) in {0.001, 0.002, 0.003}, lr (SGD) in {0.2, 0.5, 1.0}, nenc in {1, 2},
ndec in {1, 2}, Batch Size in {10000}, Activation in {Tanh}, λ in {1.0}, Cf in {1.0}, K in
{7, 10}.
•	Non-Private MLP: lr in {0.001, 0.002, 0.003}, nenc in {1, 2}, ndec in {1, 2}, Batch Size
in {1000}, Activation in {ReLU}.
•	DP-MLP: lr in {0.001, 0.002, 0.003}, nenc in {1, 2}, ndec in {1, 2}, Batch Size in {10000},
Activation in {Tanh}, λ in {1.0}, Cf in {1.0}.
ogbn-products:
•	Non-Private GCN: lr in {0.001, 0.002, 0.003}, nenc in {1, 2}, ndec in {1, 2}, Batch Size
in {1000, 4096}, Activation in {ReLU, Tanh}, K in {10}.
•	DP-GNN: lr (Adam) in {0.001, 0.002, 0.003}, lr (SGD) in {0.01, 0.1, 1.0}, nenc in {1, 2},
ndec in {1, 2}, Batch Size in {1000, 4096, 10000}, Activation in {ReLU, Tanh}, λin {0.8,
0.9, 1.0}, Cf in {1.0}, K in {10}.
•	Non-Private MLP: lr in {0.001, 0.002, 0.003}, nenc in {1, 2}, ndec in {1, 2}, Batch Size
in {1000, 4096, 10000}, Activation in {ReLU, Tanh}.
•	DP-MLP: lrin {0.001, 0.002, 0.003}, nenc in {1, 2}, ndec in {1, 2}, Batch Size in {1000,
4096, 10000}, Activation in {ReLU, Tanh}, λin {0.8, 0.9, 1.0}, Cf in {1.0}.
ogbn-mag:
•	Non-Private GCN: lr in {0.001, 0.002, 0.003}, nenc in {1, 2}, ndec in {1, 2}, Batch Size
in {1000, 4096, 5000, 10000}, Activation in {ReLU, Tanh}, Kin {3, 5, 10}.
•	DP-GNN: lr (Adam) in {0.001, 0.003, 0.01}, lr (SGD) in {0.1, 0.5, 0.8, 1.0}, nenc in {1,
2}, ndec in {1, 2}, Batch Size in {1000, 4096, 5000, 10000}, Activation in {ReLU, Tanh},
λ in {1.0, 0.8, 0.5}, Cf in {1.0, 2.0, 4.0}, K in {3, 5, 10}.
•	Non-Private MLP: lrin {0.001, 0.003, 0.01}, nenc in {1, 2}, ndec in {1, 2}, Batch Size in
{1000, 4096, 5000, 10000}, Activation in {ReLU, Tanh}.
•	DP-MLP: lr in {0.001, 0.003, 0.01}, nenc in {1, 2}, ndec in {1, 2}, Batch Size in {1000,
4096, 5000, 10000}, Activation in {ReLU, Tanh}, λ in {1.0, 0.8, 0.5}, Cf in {1.0, 2.0,
4.0}.
Additionally, the best hyperparameters corresponding to each experiment to reproduce the results in
the main paper are reported in Table 6.
21
Under review as a conference paper at ICLR 2022
Table 6: Best hyperparameters corresponding to each method across datasets to reproduce the results
in the main paper.
Method	Dataset	lr	Batch Size	Activation	nenc	ndec	λ	Cf	K
	ogbn-arxiv	0.001	1000^^	ReLU	2	1		10
Non - Private GCN	ogbn-products	0.001	1000	ReLU	2	1		10
	ogbn-mag	0.001	5000	ReLU	1	1		5
	ogbn-arxiv	0.003	10000^^	Tanh	1	2	1.0	1.0	7
DP-GNN (Adam)	ogbn-products	0.003	10000	ReLU	1	1	0.8	1.0	10
	ogbn-mag	0.001	4096	Tanh	1	2	1.0	4.0	5
	ogbn-arxiv	1.0	10000^^	Tanh	1	1	1.0	1.0	7
DP-GNN (SGD)	ogbn-products	1.0	10000	ReLU	1	1	0.8	1.0	10
	ogbn-mag	1.0	10000	ReLU	1	2	1.0	1.0	5
	ogbn-arxiv	0.001	1000^^	ReLU	2	1		
Non - Private MLP	ogbn-products	0.001	1000	ReLU	1	2		
	ogbn-mag	0.01	1000	Tanh	2	2		
	ogbn-arxiv	0.003	10000^^	Tanh	1	2	1.0	1.0
Private MLP	ogbn-products	0.002	10000	ReLU	1	2	0.8	1.0
	ogbn-mag	0.001	10000	ReLU	2	2	1.0	1.0
F Class-wise Analysis of Learnt Models
To better understand the performance of the private model as compared to the non-private baseline
for our considering setting of multi-class classification at a node-level, we compare the accuracy
of these two models for each dataset at a class-wise granularity. These results are summarized in
Figure 3. We empirically observe that the performance of the private model degrades as the fre-
quency of training data points for a particular class decreases. This indicates that the model is able
to classify data points of “frequent” classes with reasonable accuracy, but struggles with classifica-
tion accuracy on the data points of “rarer” classes. This observation is in line with previous claims
from (Bagdasaryan et al., 2019; Fioretto et al., 2021) that differentially-private models generally
perform disparately worse on under-represented classes.
22
Under review as a conference paper at ICLR 2022
1.0
Metħod: GCN {Non-Pπvate MOdel)
1.0

5	10	15	20	25	30	35	40
Class * (Ordered by Frequency)

Method: GCN
0.8
0.0
0	5	10	15	20	25	30	35	40	45 47
Class * (Ordered by Frequency)
Method: DP-GNN
0.0
0
Method: DP-GNN (Private Model)
&ESWW<
0.0
0
10	15	20	25	30
Class * (Ordered by Frequency)

5	10	15	20	25	30	35	40	45 47
Class * (Ordered by Frequency)
0.0
(a) ogbn-arxiv	(b) ogbn-products
Method： GCN (Non-Private Model)
o.8-
&0.6
E
n
u
< 0.4-
0.2-
Class # (Ordered by Frequency)
Method: DP-GNN (Private Model)
1.0
0.8
&0.6
ID
L-
3
< 0.4
0.2
0.0
Class # (Ordered by Frequency)
(c) ogbn-mag
Figure 3: Comparison of class-wise test accuracies of the non-private GCN model and private
DP-GCN model on all datasets, ordered by the decreasing frequency of occurrence of classes
in the training data from left to right. The dotted lines indicate the overall accuracy for the
corresponding models. We observe that the private model performs relatively better for classes
which have a high frequency in the training data, and the performance degrades as the frequency of
the classes decreases.
23