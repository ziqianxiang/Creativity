Under review as a conference paper at ICLR 2021
Deep Banach Space Kernels
Anonymous authors
Paper under double-blind review
Ab stract
The recent success of deep learning has encouraged many researchers to explore
the deep/concatenated variants of classical kernel methods. Some of which in-
cludes MLMKL, DGP and DKL. These methods have proven to be quite helpful
in various real-world settings. But so far, we have only been utilizing kernels
from Hilbert spaces, which has their own limitations. In this paper, we address
these shortcomings by introducing a new concatenated kernel learning approach
that uses the kernels from the reproducing kernel Banach spaces(RKBSs) instead.
We present a general framework of construction for these Deep RKBS models
and then provide a representer theorem for regularized learning problems. We
also describe the relationship with its deep RKHS variant as well as standard deep
Gaussian processes. In the end, we construct and implement a two-layer deep
RKBS model and demonstrate it on a range of machine learning tasks.
1	Introduction
In recent years, promising new variants of kernel learning methods, namely deep ker-nel learning
and multi-layer-MKL (MLMKL) algorithms have been developed. These concatenated kernel learn-
ing approach includes, see e.g. Cho & Saul (2009); Damianou & Lawrence (2013); Zhuang et al.
(2011). Although they have proven to be very successful in regression and classification tasks. we
are still only utilizing kernel from Hilbert Space. There has be a recent interest in studying a space
which is similar to RKHS but contains alot more functions i.e, reproducing kernel Banach space
(RKBS). In this paper, we will define a general framework of concatenated RKBSs as well as pro-
vide a concatenated representer theorem which can be used to develop a generalized variant of all
the concatenated kernel learning(Bohn et al. (2019)) methods described earlier. We will now give a
brief overview of our paper.
In Section 2, we briefly review the interpolation problem in RKBS and discuss how we can recast this
to RKHS. In Section 3, we introduce the optimal concatenated approximation problem for arbitrary
loss function and regularizers. We first present a general framework to construct a deep/concatenated
RKBS and then propose a representer theorem for this problem in multi-layer case. we then examine
the concrete example of a two-layer RKBS kernel. Furthermore, we discuss the relation of our
method to classical concatenated kernel learning in RKHSs which describe the connect with multiple
array of methods such as DeepGP, DKL, MLMKL etc. In section 4 we look at a simple experiment
and compare our Deep RKBS kernels with Deep RKHS ones. we also briefly talk about our library
we have built to easily perform experimentation with Deep RKBS Kernels. Finally, we conclude
with some related works and a summary of our paper.
2	Reproducing Kernel Banach Spaces
In this section, we briefly define RKBS and its reproducing kernel then we will present the repre-
senter theorem for minimal norm interpolation problem. for more comprehensive overview we refer
the readers to Lin et al. (2019b).
A reproducing kernel Banach space B on a prescribed nonempty set X is a Banach space of certain
functions on X such that every point evaluation functional δx , x ∈ X on B is continuous, that is,
there exists a positive constant Cx such that
∣δχ(f )| = ∣f(x)∣≤ CxkfkB forall f ∈B.
1
Under review as a conference paper at ICLR 2021
this definition is the natural generalization of the classical definition of RKHS. Now before moving
forward, we will present another definition as well as general framework of construction for RKBS
which we will be using throughout this paper.
(Construction of RKBS using feature maps) Let W(1) , W(2) be two Banach spaces, and
h∙, ∙iw(i) ×w(2) be a continuous bilinear form on W(1) X W(2). Suppose there exist two nonempty
sets Ω(1)	and	Ω(2),	and mappings	Φ(1)	:	Ω(1)	→ W⑴,Φ(2)	:	Ω(2)	→ W(2) such that with re-
spect to the bilinear form spanΦ⑴(Ω(1)) is dense in W⑴，spanΦ⑵(Ω(2)) is dense in W⑵.We
construct
B(1) := fv(x) := hΦ(1) (x), viW(1) ×W (2) : v ∈ W⑵,x ∈ Ω⑴}	(2.1)
and
B⑵：={gu(y) := hu,Φ⑵(y)iw(i)×w(2) : U ∈ W⑴，y ∈ Ω⑵}	(2.2)
with norm kfvkB(1) := kvkW(2) and kgukB(2) := kukW(1) respectively.
Theorem 2.1 Let B(1) andB(2) be constructed as above. Then with the bilinear form on B(1) × B(2)
hfv, guiB(1) ×B(2)	:=	hu, viW(1) ×W(2)	for all fv	∈	B(1)	and all gu	∈	B(2),	(2.3)
B(I) is an RKBS on Ω(1) with the adjoint RKBS B(2) on Ω(2). Moreover
K(x,y):= hΦ⑴(x),Φ⑵(y)iw⑴×W⑶,X ∈ Ω⑴,y ∈ Ω(2),	(2.4)
is a reproducing kernel for B(1). Using the reproducing property we can rewrite f ∈ B(1) as:
f (x) = hf, K(x, ∙)iB1×B2 for all X ∈ Ω(1) and all f ∈ Bi,	(2.5)
where, K(x, ∙) ∈ B⑵.
2.1	Interpolation Problem in RKB S
The minimal norm interpolation problem looks for the minimizer
finf := arg inf kf kB(1) where Sx,t = nf ∈ B(1) : f(Xj) = tj, j ∈ Nmo	(2.6)
vinf := arg inf kvkW(2)
v∈Vx,t
(2.7)
with
Vx,t := v∈W(2) : hΦ(1)(Xj),viW(1)×W(2) =tj, j∈Nm .	(2.8)
Theorem 2.2 (Representer Theorem) Assume the same assumptions as in Theorem 2.1. In ad-
dition, suppose that W(2) is reflexive, Strictly convex and Gateaux differentiable, and the set
{Φ(1) (Xj) : j ∈ Nm} is linearly independent in W(1). Then the minimal norm interpolation prob-
lem (2.7) has a unique solution vinf ∈ W(2) and it satisfies
G (Vinf) ∈ ((φ("x))')⊥.	(2.9)
If Ω ⑴=Ω ⑵，W ⑵* = W⑴ and W⑵ is reduced to a Hilbert Space then we can recover the
classical representer theorem(Scholkopf et al. (2002)) for minimal norm interpolation in RKHS.
3	A representer Theorem for concatenated kernel Learning in
Banach Space
In this section, we will be deriving a concatenated RKBS representer theorem for an arbitrary
number L ∈ N of concatenations of vector-valued RKBS spaces. For more comprehensive treatment
of vector-valued RKBS we refer the readers to Chen et al. (2019), Zhang & Zhang (2013) and Lin
et al. (2019a) respectively.
2
Under review as a conference paper at ICLR 2021
Let Bi,…,Bl be reproducing kernel Banach Spaces with finite dimensional domain Ω(1) and ranges
Rl ⊆ Rdl with di ∈ N for l = 1,…，L such that Rl ⊆ Ω (-)1 for l = 2,...,L, Ωf) = Ω and R1 ⊆ C.
We consider learning a function from a prescribed set of finite sampling data
Z := {(xi,ti): i ∈ N}⊆ Ω X C
Let furthermore L : R2 → [0, ∞] be an arbitrary continuous and convex loss function and let
Θ1, ..., ΘL : [0, ∞) be continuous, convex and strictly monotonically increasing functions. For each
arbitrary function fl ∈ Bl : ∀l = 1, ...L, we set
NL
J(fi,…,fL) := XL(ti,f1 ◦ ... ◦ fL(Xi)) + XΘl(kfιkBl)	(3.10)
i=1	l=1
and our objective is:
inf f1 ◦...◦fL ∈B1 ×...×BL J(f1 ◦ ... ◦ fL)	(3.11)
Since, ∀l = 1, ..., L
力(X(I)) := fvl (X(I)) = hΩ(1)(x(i)), VliW(1)×W(2) ,x(l) ∈ Ω(1),vl ∈ SPan{Φ(2)(y(l)) : y(l) ∈ Ω(2)}
Thus 3.11 reduces to
v1, ...vLinf	:= arg infv1,...,vL∈W1(2),...,WL(2)	L	ti,	hΦ1	,	v1iW1(1)	×W1(2)	◦ ... ◦	hΦL	(Xi), vLiWL(1) ×WL(2)
L
+XΘl(kvlkW(2))
l=1
(3.12)
Even if L is a convex loss function (3.11) is still a highly non-linear optimization problem. we
therefore assume that, there are w ∈ N optimal composite functions which minimizes J. We Let
Fj = f1*,…,fW be a set of all the optimal functions in Bl for all l = 1,…,L .
Theorem 3.1 In addition to above assumption, suppose that Wl(2) is reflexive, strictly convex and
Gateaux differentiable for all l = 1,...L, and the set Φ(1)(xj) : Xj ∈ Dl,∀j ∈ N is linearly
independent in Wl(I). Provided that Fl* is non-empty for all l = 1,..., L then, there exist a set of W
minimizers where each vlinf ∈ Wl(2) : ∀l = 1, ...L satisfies
G(vlinf) ∈ (Vlx,0)⊥
where,
Vlχ,0 := {vl ∈ Wl⑵：hΦ(1)(xjl)),vliw(1)×W(2) = 0 : xjl) ∈ Ω(1),j ∈ N}
Proof: Suppose there exist a minimizer fl* ∈ Fl* ∈ Bl then we create a data set Dl:
Dl := (Xj, fl* (Xj)) : j ∈ Nm
By theorem 2.2 there exists a unique solution vinf ∈ W2 for the min norm interpolation prob-
lem with the samples Dl and it satisfy that G(vlinf) ∈ (Vlx,0)⊥ It follows that flvinf =
hΦ(1)(∙),vlinfiW⑴ ×W(2)interpolates the sample data Dl and for all Vl ∈ Wl(2):
||vlinf ||W(2) ≤ ||vl||W(2)
ThUS, fvinf (X) = fv* (X).
3
Under review as a conference paper at ICLR 2021
We can extend this result for all l = 1, ..., L such that:
G(Vlinf) ∈ (V‰,O)⊥
as long as Fl* ∈ Bl is non-empty. The proof is complete. □	□
3.1 Example
In this section, we will define a concrete example of a 2 layer RKBS using our framework of con-
struction. Suppose, our inner vector-valued RKBS B2 is endowed by l1 norm and our outer vector-
valued RKBSB1 is endowed by Lp norm where, 1 < p < ∞. using the representer theorem (see,
Lin et al.(2019a) and Chen et al. (2019)) We can rewrite a vector-valued f2(∙) ∈ B2 as:
N d2
f2(∙)=ΣΣci,k2 K2 (Xi, ∙) ek2
i=1 k2=1
for certain coefficients &也 ∈ R. Furthermore, we have that fι ∈ Sχ = span {K1 (f2 (Xi), ∙) |
i = 1, . . . , N} and thus
N	N d2
fι(∙) = X ɑj Ki XXci,k2 K2 (Xi, Xj ) ek2 , ∙ j
j=1	i=1 k2=1
The concatenated function is then given by h(∙) := fi f2(∙) = PN=i ɑj K (xj, ∙) with the following
deep RKBS kernel
N	d2	N	d2
K(X, y) = K1 ΣΣci,k2 K2 (Xi , X) ek2 ,ΣΣci,k2 K2 (Xi, y) ek2	(3.13)
i=1 k2=1	i=1 k2=1
Therefore, instead of considering the infinite-dimensional optimization problem of finding f1 ∈ B1
and f2 ∈ B2 that minimize
N
J(f1,f2) =XL(yi,f1(f2(Xi)))+Θ1 kf1k2B1 +Θ2 kf2k2B2
i=1
we can restrict ourselves to finding the N + N ∙ d2 coefficients aj,eik for i,j = 1,...,N and
k2 = 1, . . . , d2
3.2 Relation to Deep Kernel Learning
As briefly mentioned in 2.2, we can recover the classical representer theorem for interpolation prob-
lem ( the detailed proof is given in the appendix ). If we assume B1, ...BL to be H1, ..., HL where
Hl ∀l = 1, ...L as a set of RKHS of vector-valued function then we can find a set of minimizers
f1, ..., fL fl ∈ Hl of 3.10 which satisfies that for all l = 1, ...L:
fl ∈ spanKl(fl+i ◦ ... ◦ fL(xi), ∙)ekl : i = 1,...Nandkl = 1,...,dl	(3.14)
where Kl denotes the reproducing kernel ofHl and ekl ∈ Rdl is the kl-th unit vector.
which is exactly the representer theorem for concatenated kernel learning in hilbert space as de-
scribed in Bohn et al. (2019), if we define a probability measure on each Hl above then we can
recover the Deep gaussian processes. which means that the methods such as Cho & Saul (2009),
Damianou & Lawrence (2013), Zhuang et al. (2011), Wilson et al. (2016) are the special cases of our
framework. It will be interesting to see the RKBS equivalent of these methods using our framework
in future.
4
Under review as a conference paper at ICLR 2021
4	Experiment
Although, we are leaving more thorough experimentation on deep RKBS kernels for future works.
Here, for the sake of completeness we simply perform some preliminary experiments on two syn-
thetic data which is generated as follows:
Wechoose Ω = [0,1]2
hi : Ω → R	h1 (x, y) :	= (0.1 + |x - y|)-1
h2 : Ω → R	h2(x, y) :	_ ʃ 1 if X ∙ y > 20 0 else
We define our deep RKBS kernel similar to 3.13 where we use the following kernels:
K2(x, y) := 1 - |x - y|
which is the reproducing kernel for C([0,1]) and for K1:
Kι(x, y) := min{x, y}- t * Xy :0 <t< 1
which is the generalization of Brownian bridge kernel. now, for deep RKHS we take the polynomial
kernel of degree 2 as inner kernel and Matern kernel as its outer kernel.
We compare deep RKHS and deep RKBS in regularized regression setting which is regularized by
l2 . the results are presented in table below where the performance is calculated by averaging the
MSE loss of 50 trials.
avg. MSE
Deep RKHS 0.00304
Deep RKBS 0.00112
The difference isn’t significant, deep RKBS does slightly outperform deep RKHS in this setting.
Since, Deep RKHS is a subset of Deep RKBS we can see the practical significance of this method.
Although a more thorough empirical study is required.
For this experiment we have also implemented a small library which contains implementation of
multiple RKBS kernels, the library also allows to easily Create deep Kernels by simply specifying
the kernel and its ranges.
5	Related Works
While concatenated kernel learning methods such as deepGP have been a well-established line of
research, deep RKBS is still in this infency. although there are some very promising recent works
in this direction such as Bartolucci et al. (2021), but to the best of authors knowledge, concatenated
RKBS kernel learning is not been presented in the literature so far.
6	Conclusion
We proposed a general framework to construct deep RKBS Kernels, which give rise to a new direc-
tion in concatenated kernel learning literature. we started by presenting the general RKBS and its
reproducing kernel. we then define a new class of concatenated kernel learning in RKBS spaces and
presented a representer theorem along with some examples of deep RKBS, we then derived the con-
nection between the classical concatenated kernel learning methods which includes DKL, MLMKL,
DeepGP among other, we finished our paper by describing our new Library to do experiments with
Deep RKBS as well as showed some preliminary results of comparing deep RKHS and deep RKBS.
Acknowledgments
We would like to thank ICLR committe for organising ICLR CoSubmitting Summer 2022 this year.
5
Under review as a conference paper at ICLR 2021
References
Francesca Bartolucci, Ernesto de Vito, Lorenzo Rosasco, and Stefano Vigogna. Understanding
neural networks with reproducing kernel banach spaces. ArXiv, abs/2109.09710, 2021.
Bastian Bohn, Christian Rieger, and Michael Griebel. A Representer Theorem for Deep Ker-
nel Learning. Technical report, 2019. URL http://jmlr.org/papers/v20/17-621.
html.
Liangzhi Chen, Haizhang Zhang, and Jun Zhang. Vector-valued Reproducing Kernel Banach Spaces
with Group Lasso Norms*. Technical report, 2019.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In NIPS, 2009.
Andreas C. Damianou and Neil Lawrence. Deep gaussian processes. ArXiv, abs/1211.0358, 2013.
Rongrong Lin, Guohui Song, and Haizhang Zhang. Multi-task Learning in Vector-valued Repro-
ducing Kernel Banach Spaces with the 1 Norm. Technical report, 2019a.
Rongrong Lin, Haizhang Zhang, and Jun Zhang. On Reproducing Kernel Banach Spaces: Generic
Definitions and Unified Framework of Constructions. Technical report, 2019b.
Bemhard Scholkopf, Alex Smola, Alexander Smola, and A Smola. Support vector machines and
kernel algorithms. Encyclopedia of Biostatistics, 5328-5335 (2005), 04 2002.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning.
ArXiv, abs/1511.02222, 2016.
Haizhang Zhang and Jun Zhang. Vector-valued reproducing kernel Banach spaces with applications
to multi-task learning. Technical Report 2, 2013.
Jinfeng Zhuang, Ivor Wai-Hung Tsang, and Steven C. H. Hoi. Two-layer multiple kernel learning.
In AISTATS, 2011.
A Appendix
You may include other additional sections here.
6