Under review as a conference paper at ICLR 2022
Heterogeneous Wasserstein Discrepancy for
Incomparable Distributions
Anonymous authors
Paper under double-blind review
Ab stract
Optimal Transport (OT) metrics allow for defining discrepancies between two
probability measures. Wasserstein distance is for longer the celebrated OT-distance
frequently-used in the literature, which seeks probability distributions to be sup-
ported on the same metric space. Because of its high computational complexity,
several approximate Wasserstein distances have been proposed based on entropy
regularization or on slicing, and one-dimensional Wassserstein computation. In
this paper, we propose a novel extension of Wasserstein distance to compare two
incomparable distributions, that hinges on the idea of distributional slicing, embed-
dings, and on computing the closed-form Wassertein distance between the sliced
distributions. We provide a theoretical analysis of this new divergence, called het-
erogeneous Wasserstein discrepancy (HWD), and we show that it preserves several
interesting properties including rotation-invariance. We show that the embeddings
involved in HWD can be efficiently learned. Finally, we provide a large set of
experiments illustrating the behavior of HWD as a divergence in the context of
generative modeling and in query framework.
1	Introduction
Optimal Transport-based data analysis has recently found widespread interest in machine learning
community, since its significant usefulness to achieve many tasks arising from designing loss func-
tions in supervised learning (Frogner et al., 2015), unsupervised learning (Arjovsky et al., 2017), text
classification (Kusner et al., 2015), domain adaptation (Courty et al., 2017), generative models (Ar-
jovsky et al., 2017; Salimans et al., 2018), computer vision (Bonneel et al., 2011; Solomon et al.,
2015) among many more applications (Kolouri et al., 2017; Peyre & Cuturi, 2019). Optimal Transport
(OT) attempts to match real-world entities through computing distances between distributions, and
for that it exploits prior geometric knowledge on the base spaces in which the distributions are valued.
Computing OT distance equals to finding the most cost-efficiency way to transport mass from source
distribution to target distribution, and it is often referred to as the Monge-Kantorovich or Wasserstein
distance (Monge, 1781; Kantorovich, 1942; Villani, 2009).
Matching distributions using Wasserstein distance relies on the assumption that their base spaces must
be the same, or that at least a meaningful pairwise distance between the supports of these distributions
can be computed. A variant of Wasserstein distance dealing with heterogeneous distributions and
overcoming the lack of intrinsic correspondence between their base spaces is Gromov-Wasserstein
(GW) distance (Sturm, 2006; Memoli, 2011). GW distance allows to learn an optimal transport-like
plan by measuring how the similarity distances between pairs of supports within each ground space
are closed. It is increasingly finding applications for learning problems in shape matching (Memoli,
2011), graph partitioning and matching (Xu et al., 2019), matching of vocabulary sets between
different languages (Alvarez-Melis & Jaakkola, 2018), generative models (Bunne et al., 2019),
or matching weighted networks (Chowdhury & Memoli, 2018). Due to the heterogeneity of the
distributions, GW distance uses only the relational aspects in each domain, such as the pairwise
relationships to compare the two distributions. As a consequence, the main disadvantage of GW
distance is its computational cost as the associated optimization problem is a non-convex quadratic
program (Peyre & Cuturi, 2019), and as few as thousand samples can be computationally challenging.
Based on the approach of regularized OT (Cuturi, 2013), in which an entropic penalty is added to
the original objective function defining the Wasserstein OT problem, Peyre et al. (2016) propose an
entropic version called entropic GW discrepancy, that leads to approximate GW distance. Another
1
Under review as a conference paper at ICLR 2022
Figure 1: We measure the discrepancy between two distributions living respectively in Rp and Rq .
Our approach is based on generating random slicing projections distributions in each of the metric
spaces Rp and Rq through the mappings φ and ψ of a random projection vector sampled from an
optimal distribution Θ in Rd. As each of the projected distribution results in a 1D distribution, we
can then compute 1D-Wasserstein distance. It enables us to learn the best projection mappings φ and
ψ and to optimize over the distributional part of the generating projection distribution Θ.
approach for scaling up the GW distance is Sliced Gromov-Wasserstein (SGW) discrepancy (Vayer
et al., 2019), which leverages on random projections on 1D and on a closed-form solution of the
1D-Gromov-Wasserstein.
In this paper, we take a different approach for measuring the discrepancy between two heteroge-
neous distributions. Unlike GW distance that compares pairwise distances of elements from each
distribution, we consider a method that embeds the metric measure spaces into a one-dimensional
space and computes a Wasserstein distance between the two 1D-projected distributions. The key
element of our approach is to learn two mappings that transform vectors from the unit-sphere of a
latent space to the unit-sphere of the metric space underlying the two distributions of interest, see
Figure 1. In a nutshell, we learn to transform a random direction, sampled under an optimal (learned)
distribution (optimality being made clear later), from a d-dimensional space to a random direction into
the desired spaces. This approach has the benefit of avoiding an ad-hoc padding strategy (completion
of 0 of the smaller dimension distributions to fit the high-dimensional one) as in SGW method (Vayer
et al., 2019). Another relevant feature of our approach is that the two resulting 1D distributions
are now compared through Wasserstein distance. This point, in conjunction, with other key aspect
of the method, will lead to a relevant discrepancy between two distributions, called heterogeneous
Wasserstein discrepancy (HWD). Although we lose some properties of a distance, we show that HWD
is rotation-invariant, that it is robust enough to be considered as a loss for learning generative models
between heterogeneous spaces. We also establish that HWD boils down to the recent distributional
sliced Wasserstein distance (Nguyen et al., 2020) if the two distributions live in the same space and if
some mild constraints are imposed on the mappings.
In summary, our contributions are as follows:
•	we propose HWD, a novel slicing-based discrepancy for comparing two distributions living
in different spaces. Our chosen formulation is based on comparing 1D random-projected
versions of the two distributions using a Wasserstein distance;
•	The projection operations are materialized by optimally mapping from one common space
to the two spaces of interest. We provide a theoretical analysis of the resulting discrepancy
and exhibit its relevant properties;
•	Since the discrepancy involves several mappings that need to be optimized, we depict an
alternate optimization algorithm for learning them;
•	Numerically, we validate the benefits of HWD in terms of comparison between heteroge-
neous distributions. We show that it can be used as a loss for generative models or shape
objects retrieval with better performance and robustness than SGW on those tasks.
2
Under review as a conference paper at ICLR 2022
2	Background of OT distances
For the reader’s convenience, we provide here a brief review of the notations and definitions,
that will be frequently used throughout the paper. We start by introducing Wasserstein and
Gromov-Wasserstein distances with their sliced versions SW and SGW, where we consider
these distances in the specific case of Euclidean base spaces (Rp, k ∙ ∣∣) and (Rq, k ∙ ∣∣). We
denote P(X ) and P(Y) the respective sets of probability measures whose supports are con-
tained on compact sets X ⊆ Rp and Y ⊆ Rq. For r ≥ 1, we denote Pr(X) the sub-
set of measures in P(X) with finite r-th moment (r ≥ 1), i.e., Pr(X) = η ∈ P(X) :
∕x ∣∣x∣r dη(x) < ∞}. For μ ∈ P (X) and V ∈ P (Y), We write Π(μ,ν) ⊂ P (X X Y)
for the collection of joint probability distributions with marginals μ and ν, known as couplings,
Π(μ,ν) = {γ ∈ P(X ×Y): ∀A ⊂ X,B ⊂ Y,γ(A ×Y)= μ(A),γ(X × B) = V(B)}.
2.1	OT distances for homogeneous domains
We here assume that the distributions μ and V lie in the same base space, for instance P = q. Taking
this into account, we can define the Wasserstein distance and its sliced variant.
Wasserstein distance The r-th Wasserstein distance is defined on Pr (X) by
Wr (μ, ν)=
inf
Vγ∈π(μ,ν)
I
X×Y
l∣x — ykrdγ(χ,y)) r.
(1)
The quantity Wr (μ, V) describes the least amount effort to transform one distribution μ into another
one V. Since the cost distance used between sample supports is the Euclidean one, the infimum in (1)
is attained (Villani, 2009), and any probability γ which realizes the minimum is called an optimal
transport plan. In a finite discrete setting, Problem (1) can be formulated as a linear program, that
is challenging to solve algorithmically as its computational cost is of order O(n5/2 log n) (Lee &
Sidford, 2014), where n is the number of sample supports.
Contrastingly, for the 1D case (i.e. p = 1) of continuous probability measures, the r-th Wasser-
stein distance has a closed-form solution (Rachev & Ruschendorf, 1998), namely, Wr (μ,ν) =
(´1 |F-I(U)- F-I(U)Irdt) 1 where F-I and F-Iare the quantile functions of μ and V. For em-
pirical distributions, the 1D-Wasserstein distance is simply calculated by sorting the supports of the
distributions on the real line, resulting to a complexity of order O(n log n). This nice computational
property motivates the use of sliced-Wasserstein (SW) distance (Rabin et al., 2012; Bonneel et al.,
2015), where one calculates an (infinity) of 1D-Wasserstein distances between linear projection
pushforwards of distributions in question and then computes their average.
To precisely define SW distance, we consider the following notation. Let Sp-1 := {U ∈ Rp :
∣∣u∣ = 1} be the unit sphere in P dimension in '2-norm, and for any vector θ in Sp-1, we define
Pθ the orthogonal projection onto the real line Rθ = {αθ : α ∈ R}, that is Pθ (x) = hθ, xi, where
h∙, ∙i stands for the Euclidean inner-product. Let μθ = Pθ#仙 the measure on the real line called
pushforward of μ by Pθ, that is μθ(A) = μ(P-1(A)) for all Borel set A ⊆ R. We may now define
the SW distance.
Sliced Wasserstein distance The r-th order sliced Wasserstein distance between two probability
distributions μ, v ∈ Pr(X) is given by
1
r
SWr (μ, ν)
Wrr (μθ ,vθ
(2)
where Ap is the area of the surface of Sp-1, i.e., Ap = Γ∏p∣2) with Γ : R → R, the Gamma
function given as Γ(U) = 0∞ tu-1e-tdt. Thanks to its computational benefits and its valid metric
property (Bonnotte, 2013), the SW distance has recently been used for OT-based deep generative
modeling (Kolouri et al., 2019; Deshpande et al., 2019; Wu et al., 2019). Note that the normalized
integral in (2) can be seen as the expectation for θ 〜σp-1, the uniform surface measure on
Sp-1, that is SWr(μ, V) = (E6〜σp-ι [Wr(μθ, vθ)])r. Therefore, the SW distance can be easily
approximated via a Monte Carlo sampling scheme by drawing uniform random samples from Sp-1:
3
Under review as a conference paper at ICLR 2022
SW r (μ, V) ≈ κ1 PK=I Wr (μθk, νθk) where θι,...,θκ i.^d，σp-1 and K is the number of random
projections.
2.2	OT distances for heterogeneous domains
To get benefit from the advantages of OT in many machine learning applications involving heteroge-
neous and incomparable domains (p = q), the Gromov-Wasserstein distance (Memoli, 2011) stands
for the basic OT distance dealing with this setting.
Gromov-Wasserstein distance The r-th Gromov-Wasserstein distance between two probability
distributions μ ∈ Pr (X) and V ∈ Pr (Y) is defined by
GWr(μ,ν) =	inf	Jr(Y)	d=.	1(	[[	lkx-x0∣ITly-y0klrdY(X,y)dY(X0,yO))	r.⑶
γ∈∏(μ,ν)	2' JJ	)
X2×Y2
Note that GWr (μ, V) is a valid metric endowing the collection of all isomorphism classes metric
measure spaces of Pr(X) × Pr(Y), see Theorem 5 in (Memoli, 2011). The GW distance learns an
optimal transport-like plan which transports samples from a source metric space X into a target metric
space Y, by measuring how the similarity distances between pairs of samples within each space are
close. Furthermore, GW distance enjoys several geometric properties, particularly translation and
rotation invariance. However, its major bottleneck consists in an expensive computational cost, since
problem (3) is non-convex and quadratic. A remedy to such a heavy computational burden lies in an
entropic regularized GW discrepancy (Peyre et al., 2016), using Sinkhorn iterations algorithm (Cuturi,
2013). This latter needs a large regularization parameter to guarantee a fast computation, which,
unfortunately, entails a poor approximation of the true GW distance value. Another approach to scale
up the computation of GW distance is sliced-GW discrepancy (Vayer et al., 2019). The definition of
SGW shows 1D-GW distances between projected pushforward of an artifact zero padding of μ or V
distribution. We detail this representation in the following paragraph.
Sliced Gromov-Wasserstein discrepancy Assume that p < q and let ∆ be an artifact zero padding
from X onto Y, i.e. ∆(X) = (X1, . . . , Xp, 0, . . . , 0) ∈ Rq. The r-th order sliced Gromov-Wasserstein
discrepancy between two probability distributions μ ∈ Pr (X) and V ∈ Pr (Y) is given by
SGW ∆,r(μ,V) = (Eθ~σq-1 [GW r ((△#〃)© ,Vθ)])1.	(4)
Itis worthy to note that SGW∆,r is depending on the ad-hoc operator ∆, hence the rotation invariance
is lost. Vayer et al. (2019) propose a variant of SGW that does not depend on the choice of △, called
Rotation Invariant SGW (RI-SGW) forp = q, defined as the minimizer of SGW∆,r over the Stiefel
manifold, see (Vayer et al., 2019, Equation 6). In this work, we are interested in calculating an
OT-based discrepancy between distributions over distinct domains using the slicing technique. Our
approach is different from the SGW one in many points, specifically (and most importantly) we use a
1D-Wasserstein distance between the projected pushforward distributions and not a 1D-GW distance.
In the next section, we detail the setup of our approach.
3 Heterogeneous Wasserstein discrepancy
Despite the computational benefit of sliced-OT variant discrepancies, they have an unavoidable
bottleneck corresponding to an intractable computation of the expectation with respect to uniform
distribution of projections. Furthermore, the Monte Carlo sampling scheme can often generate an
overwhelming number of irrelevant directions; hence, the larger number of sample projections, the
more accurate approximation of sliced-OT values. Recently, Nguyen et al. (2020) have proposed
the distributional-SW distance allowing to find an optimal distribution over an expansion area of
informative directions. This performs the projection efficiently by choosing an optimal number of
important random projections needed to capture the structure of distributions. Our approach for
comparing distributions in heterogeneous domains follows a distributional slicing technique combined
with OT metric measure embedding (Alaya et al., 2020).
4
Under review as a conference paper at ICLR 2022
Let us first introduce additional notations. Fix d ≥ 1 and consider two nonlinear mappings
φ : Sd-1 → Sp-1 and ψ : Sd-1 → Sq-1. For any constants Cφ, Cψ > 0, we define the fol-
lowing probability measure sets: Mcφ = {Θ ∈ P(SdT) : Eg.〜θ[∣hφ(θ),φ(θ0)i∣] ≤ Cφ} and
Mcψ = {Θ ∈ P(SdT) : We”〜θ[∣hΨ(θ),Ψ(θ0)i∣] ≤ Cψ}. Wesay that Cφ, Cψ are (φ,ψ)
admissible constants if the intersection sets MCφ ∩ MCψ is not empty. We hereafter denote
μφ,θ = P@(e)#M and νψ,e = Pψ(θ)#v the pushforwards of μ and V by projections over unit sphere
Pφ(θ) and Pψ(θ), respectively.
Informal presentation While the distributions μ and V are valued in different spaces, X ⊂ Rp and
Y ⊂ Rq, any projected distributions will live in real line, enabling the computation of 1D-Wasserstein
distance (Figure 1, right). In order to generate random 1D projections in each of the spaces, we map a
common random projection distribution from Sd-1 into each of the projection spaces Sp-1 and Sq-1,
through the mappings φ and ψ (see Figure 1, left). Hence, the main components of the heterogeneous
Wasserstein discrepancy will be the distribution Θ ∈ P(Sd-1), and the two embeddings φ and ψ
which will be wisely chosen. The resulting directions φ(θ) and ψ(θ) form the projections Pφ(θ) and
Pψ(θ) (see Figure 1, center) used to compute several 1D-Wasserstein distances.
3.1	Definition and properties
Herein we state the formulation of the proposed discrepancy and exhibit its main theoretical properties.
Definition 1 The heterogeneous Wasserstein discrepancy (HWD) of order r ≥ 1 between μ ∈
Pr(X) and V ∈ Pr(Y) reads as
1
HWDr(μ,ν) = inf sup	(⅛θ〜θ [Wrr(μφ,θ,νψ,e)])r.	(5)
φ,ψ Θ∈MCφ∩MCψ
HWD belongs to a family of projected OT works (Paty & Cuturi, 2019; Rowland et al., 2019;
Lin et al., 2021) with a particularity for seeking nonlinear projections minimizing a sliced-OT
variant. HWD further inherits the distributional slicing benefit by finding an optimal probability
measure Θ of slices on the unit sphere Sd-1 coupled with an optimum couple (φ, ψ) of embeddings.
Note that this optimal Θ verifies the double conditions Eθ,θo〜θ[∣ cos(φ(θ), φ(θ0))∣] ≤ Cφ and
Eθ,θ0〜θ[| cos(ψ(θ), ψ(θ0))∣] ≤ Cψ. This gives that Cφ,Cψ ≤ 1, hence the sets Mcφ and Mcψ
belong to M1 = {Θ ∈ P(Sd-1)} the set of all probability measures of the unit sphere Sd-1. It is
worthy to note that for small regularizing (φ, ψ)-admissible constants, the measure Θ is forced to
distribute more weights to directions that are far from each other in terms of their angles (Nguyen
et al., 2020).
Now, in order to guarantee the existence of (φ, ψ)-admissible constants, we assume that the
couple(φ, ψ)-embeddings are approximately angle preserving.
Assumption 1 (Approximately angle preserving property) For any couple (φ, ψ)-embeddings ,
assume that there exists two non-negative constants Lφ and Lψ , such that the following holds
Ihφ(θ), φ(θ0)i∣ ≤ Lφ∣hθ,θ0i∣ and ∣hψ(θ), ψ(θ0)i∣ ≤ Lψ ∣hθ,θ0i∣, for all θ,θ0 ∈ Sd-1.
In Proposition 1, we deliver lower bounds of the regularizing (φ, ψ)-admissible constants Cφ and
Cψ, depending on the dimension of the latent space d and on the levels (Lφ, Lψ) of approximately
angle preserving property. These bounds ensure the non-emptiness of the sets MCφ and MCψ .
Proposition 1 Let Assumption 1 hold and consider regularizing (φ, ψ)-admissible constants such
that Cφ ≥ √∏Lφ(Γd+2)∕2) and Cψ ≥ ^^；，+/：% ∙ Then the sets Mcφ and Mcψ contain the uniform
measure σd-1 and σ = Pk=I dδθk, where {θι,..., θd} forms any orthonormal basis in Rd. Note
that by GautschYs inequality (Gautschi, 1959)for the Gamma function, we have that Cφ ≥ LdL and
厂、Lψ
Cψ ≥ d .
Proof of Proposition 1 is presented in Appendix A.2. Together the admissible constants, the levels of
angle preserving property, and the dimension d of the latent space form the hyperparameters set of
5
Under review as a conference paper at ICLR 2022
HWD problem. For settings of large d, the admissible constants could take smaller values, that force
the measure Θ to focus on far-angle directions. However, for smaller d, we may lose the control on
the distributional part, the set MC tends to M1 the entire set of probability measure on Sd-1, hence
it boils down on a standard slicing approach that needs an expensive number of projections to get an
accurate approximation. Next, we give a set of interesting theoretical properties characterizing HWD.
Proposition 2 For any r ≥ 1, HWD satisfies the following properties:
r-1
(i)	HWDr (μ, V) is finite, that is HWDr (μ, V) ≤ 2	(Mr (μ) + Mr (V)) where Mr (∙) is the
r-th moment ofthe given distribution, i.e. Mr (μ) = ( JX ∣∣xkrdμ(x))r.
(ii)	HWDr (μ, v) is non-negative, Symmetric and verifies HWDr (μ, μ) = 0.
(iii)	HWDr (μ, V) has a discrepancy equivalence given by
/1、ι
(ʒ)r inf max Wr(μφ,θ,vψ θ) ≤ HWDr(μ,v) ≤ inf max Wr(μφθ,vψ θ).
d φ,ψ θ∈Sd-1 r φ, ψ,	r	φ,ψ θ∈Sd-1 r φ, ψ,
(iv)	For p = q, HWD is upper bounded by the distributional sliced Wasserstein distance.
(v)	HWDr is rotation invariant, namely, HWDr (R#m,Q#V) = HWDr (μ, v), for any
R ∈ Op = {R ∈ Rp×p : R>R = Ip} andQ ∈ Oq = {Q ∈ Rq×q : Q>Q = Iq}, the
orthogonal group of rotations of order p and q, respectively.
(vi)	Let Tα and Tβ be the translations from Rp into Rp and from Rq into Rq with vectors α and
β, respectively. Then HWDr (Ta#M,Te #v ) ≤ 2r-1(HWDr (μ, v) + ∣∣α∣ + Ilel∣)).
Proof of Proposition 2 is given in Apprendix A.1. From property (i) HWD is finite provided that the
distributions in question have a finite r-th moments. Note that the supremum over the probability
measure sets Mcφ and Mcψ guarantees the property HWDr (μ, μ) = 0. For P = q, if the infimum
over the couple (φ, ψ)-embedding in (iii) is realized in the identity mappings, then HWD verifies a
metric equivalence with respect to the max-sliced Wasserstein distance (Deshpande et al., 2019). The
property (v ) highlights a rotation invariance of HWD, which is well verified by the GW distance.
3.2	Algorithm
Computing HWD requires a resolution of an optimization problem as given in (5). In what follows,
we propose an algorithm for computing an approximation of this discrepancy based on samples
X = {xi}n=ι from μ and samples Y = {yj}m=ι from V. At first, let us note that We have min-max
optimization to solve; the minimization occuring over the embeddings φ and ψ and the maximization
over the distributions on the unit-sphere. This maximization problem is challenging due to both
the constraints and because We optimize over distributions. Similarly to Nguyen et al. (2020), We
approximate the problem by replacing the constraints With regularization terms and by replacing the
optimization over distributions by an optimization over a push-forWard of the uniform probability
measure σd-1 by a Borel measurable function f : Sd-1 → Sd-1. Hence, assuming that We have
draWn from a uniform distribution K directions {θk}kK=1, the numerical approximation of HWD is
obtained by solving the folloWing problem:
1 K	1/r
m in max 任1 =. (衣 X Wr(X>Φ[f(θk )],Y>Ψ[f(θk)]))	}
k=1
+ mfin λC nL2 d=ef. X φ[f (θk)]> φ[f (θk0)] + Xψ[f(θk)]>ψ[f(θk0)]o
k,k0	k,k0
+ min λa{L3 =. X (φ[f(θk)]>φ[f(θk0)] - θ>θk0)2 + X (ψ[f (θk)]>ψ[f (θk0)] - θ>θk0)2O
k,,k0	k,k0
Where the first term in the optimization is related to the sliced Wasserstein, the second term is related
to the regularization term associated to Eθ,θo〜㊀[∣hφ(θ), φ(θ0)i∣], and Eθ,θo〜θ[∣hψ(θ),ψ(θ0)i∣], and
the third term is the angle-preserving regularization term. Note that the min term With respect to
f is due to the fact that We Want those regularizers to be small. TWo hyperparameters λC and λa
6
Under review as a conference paper at ICLR 2022
Algorithm 1 COMPUTING HETEROGENEOUS WASSERSTEIN DISCREPANCY (SEE (5))
1:	Input: Source and target samples: (X, μ) and (Y, V); order r; the set of random direction
{θk}kK=1; T number of global iterations; N number of iterations for each of the alternate scheme;
2:	Output: HWD
3:	function COMPUTE LOSSES(X, Y, φ[f (θk)] , ψ[f (θk)])
4:	compute the average of 1D-Wasserstein L1 between X>φ[f(θk)] and Y>ψ[f(θk)]
5:	compute the inner-product penalty L2
6:	compute the angle-preserving penalty L3
7:	end function
8:	for t = 1,…，T do
9:	fix φ and ψ
10:	for i = 1,…，N do
11:	computeφ[f(θk)] andψ[f(θk)]
12:	L1,L2,L3 - Compute Losses (X, Y, φ[f (θk)], ψ[f(θk)]))
13:	L= -L1+L2 +L3
14:	f — f — Yk VL
15:	end for
16:	fix f
17:	for i = 1,…，N do
18:	computeφ[f(θk)] and ψ[f (θk)]
19:	L1,L2,L3 - Compute Losses (X, Y, φ[f(θk)] , ψ[f (θk)])
20:	L	= L1 + L2 + L3
21:	φ	J φ — γk VL
22:	ψ	J ψ — YkVL
23:	end for
24:	end for
25:	HWD J compute the average over {θk}kK=1 of closed-form 1D Wasserstein between
X>φ[f(θk)] and Y>ψ[f(θk)]
26:	Return: HWD
control the impact of these two regularization terms. In practice, φ, ψ and f are parametrized as
deep neural networks and the min-max problem is solved by an alternating optimization scheme : (a)
optimizing over f with ψ and φ fixed then (b) optimizing over ψ and φ with f fixed. Some details of
the algorithms are provided in Algorithm 1.
Regarding computational complexity, if we assume that the mapping f, φ, ψ are already trained, that
we have K projections, and that φ[f (θk)], ψ[f(θk)] are precomputed, then the computation of HWD
(line 25 of Algorithm 1) is in O(K (n logn + np + nq)), where n is the number of samples in X
and Y . When taking into account the full optimization process, then the complexity depends on the
number of times we compute the full objective function we are optimizing. Each evaluation requires
the computation of the sum in L1 which is O(K(n logn + np + nq)) and the two regularization
terms L2 and L3 require both O(K2(p + q + 2d)). Note that in terms of computational complexity,
SGW is O(Kn log n) whereas HWD is O(TNKnlogn), with T × N being the global number of
objective function evaluations. Hence, complexity is in favor of SGW. However, one should note that
in practice, because we optimize over the distribution of the random projections, we usually need less
slices than SGW and thus depending on the problem, TNK can be of the same magnitude than the
number of slices involved in SGW (similar findings have been highlighted for Sliced Wasserstein
distance (Nguyen et al., 2020)).
4	Numerical experiments
In this section, we analyze HWD, exhibit its rotation-invariant property, and compare its performance
with SGW in a generative model context.
Translation and Rotation We have used two simple datasets for showing the behavior of HWD
with respect to translation and rotation. For translation, we consider two 2D Gaussian distributions
one being fixed, the other with varying mean. For rotation, we use two 2D spirals from Scikit-Learn
7
Under review as a conference paper at ICLR 2022
Figure 2: Examples of distance computation
between (left) two-translating Gaussian distri-
butions. (right) two spirals.
」∙ ∙ ∙
∙
-3⅛^^^^ɪ^^；^^I^^ɪ^^i
Xl
Figure 3: Examples of target distribu-
tions for our generative models (left) 3D
4-mode. (right) 2D 5-mode.
target
library, one being fixed and the other being rotated from 0 to π∕2. For these two cases, We have
drawn 500 samples, used 100 random directions for SGW and RI-SGW. For our HWD, we have
used only 10 slices and T = 50, N = 5 iterations for each of the alternate optimization. The results
we obtain are depicted in Figure 2. From the first panel, we remark that both SGW and RI-SGW
are indeed insensitive to translation while HWD captures this translation, which is also verified by
property (vi) in Proposition 2. For the spiral problem, as expected HWD and RI-SGW are indeed
rotation-invariant while SGW is not.
Generative models For checking whether our distribution discrepancy behaves appropriately, we
have used it as a loss function in a generative model. Our task here is to build a model able to generate
a distribution defined on a space having a different dimensionality from the target distribution space.
As such, we have considered the same toy problems as in Bunne et al. (2019) and investigated
two situations: generating 2D distributions from 3D data and the other way around. The 3D target
distribution is a Gaussian mixture model with four modes while the 2D ones are 5-mode. Our
generative model is composed of a fully-connected neural network with ReLU activation functions.
We have considered 3000 samples in the target distributions and batch size of 300. For both HWD
and SGW, we have run the algorithm for 30000 iterations with an Adam optimizer, stepsize of 0.001
and default β parameters. For the 4-mode problem, the generator is a MLP with 2 layers while for
the 5-mode, as the problem is more complex, it has 3 layers. In each case, we have 256 units on
the first layer and then 128. For the hyperparameters, we have set λC = 1 and λa = 5 or λa = 50
depending on the problem. Note that for SGW, we have also added a '2-norm regularizer on the
output of the generator in order to avoid them to drift (see Figures 8 and 9 in Appendix C), as the loss
is translation-invariant. Examples of generated distributions are depicted in Figure 4. We remark that
our HWD is able to produce visually correct distributions whereas SGW struggles in generating the 4
modes and its 3D 5-mode is squeezed on its third dimension.
Scalability We consider the non-rigid shape world dataset (Bronstein et al., 2006) which
consists of 148 three-dimensional shapes from 12 classes. We draw randomly n ∈
{100, 250, 500, 1000, 1500, 2000} vertices {xi ∈ R3}in=1 on each shape and use them to measure the
similarity between a pair of shapes {xi ∈ R3}in=1 and {yj ∈ R3}jn=1. Figure 5 reports the average
time to compute on a single core such a similarity for 100 pairs of shapes using respectively GW,
SGW and HWD. As expected GW exhibits a slow behavior while the computational burden of HWD
is on par with SGW.
Classification under various transformations This experiment,
whose details are provided in Appendix B.2, aims to evaluate the ro-
bustness of SGW and HWD (the computationally efficient methods)
to different transformations in terms of classification accuracy. To
that purpose we employ the Shape Retrieval Contest (SHREC’2010)
correspondence dataset, see Bronstein et al. (2010). It includes high
resolution (10K-50K) triangular meshes. The shapes are of 3 classes
(see Figure 10 in Appendix C) with 9 different transformations and
the null shape (no transformation). Each transformation is applied
up to five strength levels (weak to strong). Along with the null shape,
we consider all strengths of the "isometry", "topology", "scale",
"shotnoise" transformations leading to 63 samples. We perform a 1-
NN classification. Obtained performances over 10 runs are depicted
Figure 6: Classification perfor-
mance under transformations.
8
Under review as a conference paper at ICLR 2022
iteretion 10	iteration 1OOOO	itemtion 20000
"1
iteration 30000
Iteratlon aoow
iteration 10
iterβtion 30000
Figure 4: Examples of generated distributions across iterations (10, 10000, 20000, and 30000) for
two targets.From top to bottom (first-row) HWD for the 4-mode. (second-row) SGW for the 4-mode
(third-row) HWD for 3D 5-mode. (fourth-row) SGW for 5-mode. For each row, the last panel shows
the evolution of the loss over the 30000 iterations.
Figure 5: Computation time with respect to n, the number of vertices on each shape. (Left-panel)
Instances of 3D objects. (Right-panel) Running time.
□
in Figure 6. They highlight the ability of HWD to be robust to perturbations. HWD achieves slightly
better mean classification accuracy than SGW with a competitive computation time (see Figure 5).
Notice that GW and RISGW are unable to run under reasonable time-budget constraint.
5	Conclusion
We introduce in this paper HWD a novel OT-based discrepancy between distributions lying in different
spaces. It takes computational benefits from distributional slicing technique, which amounts to find an
optimal number of random projections needed to capture the structure of data distributions. Another
feature of this discrepancy consists in projecting the distributions in question through a learning
of embeddings enjoying the same latent space. We showed a nice geometrical property verified
by the proposed discrepancy, specifically a rotation-invariance. We illustrated through extensive
experiments the applicability of this discrepancy on generative modeling and shape objects retrieval.
We argue that the implementation part faces the standard deep learning bottleneck of tuning the
model’s hyperparameters. A future extension line of this work is to deliver theoretical guarantees
regarding the regularizing parameters, both of distributional and angle preserving properties.
9
Under review as a conference paper at ICLR 2022
References
M. Z. Alaya, M. Berar, G. Gasso, and A. Rakotomamonjy. Theoretical guarantees for bridging metric
measure embedding and optimal transport, 2020.
D. Alvarez-Melis and T. Jaakkola. Gromov-Wasserstein alignment of word embedding spaces. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.
1881-1890. Association for Computational Linguistics, 2018.
M.	Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In Doina
Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214-223, International
Convention Centre, Sydney, Australia, 2017. PMLR.
N.	Bonneel, M. van de Panne, S. Paris, and W. Heidrich. Displacement interpolation using lagrangian
mass transport. ACM Trans. Graph., 30(6):158:1-158:12, 2011.
N. Bonneel, J. Rabin, G. Peyre, and H. Pfister. Sliced and Radon Wasserstein barycenters of measures.
51(1), 2015.
N. Bonnotte. Unidimensional and Evolution Methods for Optimal Transportation. Theses, Universite
Paris Sud - Paris XI ; Scuola normale superiore (Pise, Italie), December 2013.
A. Bronstein, M. Bronstein, U. Castellani, B. Falcidieno, A. Fusiello, A. Godil, L. Guibas, I. Kokkinos,
Z. Lian, M. Ovsjanikov, G. Patane, M. Spagnuolo, and R. Toldo. SHREC 2010: robust large-scale
shape retrieval benchmark. Eurographics Workshop on 3D Object RetrieVal(2010), Norrkoping, -1,
2010-05-02 2010.
A. M Bronstein, M. M. Bronstein, and R. Kimmel. Efficient computation of isometry-invariant
distances between surfaces. SIAM Journal on Scientific Computing, 28(5):1812-1836, 2006.
C. Bunne, D. Alvarez-Melis, A. Krause, and S. Jegelka. Learning generative models across incompa-
rable spaces. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 851-861, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
S. Chowdhury and F. Memoli. The Gromov-Wasserstein distance between networks and stable
network invariants. CoRR, abs/1808.04337, 2018.
N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation.
IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-1865, 2017.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Informa-
tion Processing Systems 26, pp. 2292-2300. Curran Associates, Inc., 2013.
I. Deshpande, Y.-T. Hu, R. Sun, A. Pyrros, N. Siddiqui, S. Koyejo, Z. Zhao, D. Forsyth, and A. G.
Schwing. Max-sliced Wasserstein distance and its use for gans. In 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 10640-10648, 2019.
R.	Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos,
K. Fatras, N. Fournier, L. Gautheron, N. T.H. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko,
A. Rolet, A. Schutz, V. Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer. Pot: Python
optimal transport. Journal of Machine Learning Research, 22(78):1-8, 2021.
C. Frogner, C. Zhang, H. Mobahi, M. Araya, and T. A. Poggio. Learning with a Wasserstein loss. In
C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 28, pp. 2053-2061. Curran Associates, Inc., 2015.
W. Gautschi. Some elementary inequalities relating to the gamma and incomplete gamma function.
Journal of Mathematics and Physics, 38(1-4):77-81, 1959.
L. Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk, 2:227-229, 1942.
10
Under review as a conference paper at ICLR 2022
S.	Kolouri, S. R. Park, M. Thorpe, D. Slepcev, and G. K. Rohde. Optimal mass transport: Signal
processing and machine-learning applications. IEEE Signal Processing Magazine, 34(4):43-59,
July 2017.
S.	Kolouri, P. E. Pope, C. E. Martin, and G. K. Rohde. Sliced Wasserstein auto-encoders. In
International Conference on Learning Representations, 2019.
M.	Kusner, Y. Sun, N. Kolkin, and K. Weinberger. From word embeddings to document distances. In
Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine
Learning, volume 37 of Proceedings of Machine Learning Research, pp. 957-966, Lille, France,
07-09 Jul 2015. PMLR.
Y. T. Lee and A. Sidford. Path finding methods for linear programming: Solving linear programs in
O(Vrank) iterations and faster algorithms for maximum flow. In Proceedings ofthe 2014 IEEE 55th
Annual Symposium on Foundations of Computer Science, FOCS ’14, pp. 424-433, Washington,
DC, USA, 2014. IEEE Computer Society.
N.	Lerner. A Course on Integration Theory. Springer Basel, 2014.
T. Lin, Z. Zheng, E. Chen, M. Cuturi, and M. Jordan. On projection robust optimal transport:
Sample complexity and model misspecification. In Arindam Banerjee and Kenji Fukumizu (eds.),
Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume
130 of Proceedings of Machine Learning Research, pp. 262-270. PMLR, 13-15 Apr 2021.
F.	Memoli. Gromov-Wasserstein distances and the metric approach to object matching. Foundations
of Computational Mathematics, 11(4):417-487, 2011.
G.	Monge. MemOire sur la theotie des deblais et des remblais. Histoire de l,Academie Royale des
Sciences, pp. 666-704, 1781.
K. Nguyen, N. Ho, T. Pham, and H. Bui. Distributional sliced-Wasserstein and applications to
generative modeling, 2020.
F.-P. Paty and M. Cuturi. Subspace robust Wasserstein distances. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 5072-5081, Long Beach, California,
USA, 2019. PMLR.
G. Peyre, M. Cuturi, and J. Solomon. Gromov-Wasserstein averaging of kernel and distance matrices.
In Proceedings of the 33rd International Conference on International Conference on Machine
Learning - Volume 48, ICML’16, pp. 2664-2672. JMLR.org, 2016.
G. Peyre and M. Cuturi. Computational optimal transport. Foundations and Trends® in Machine
Learning, 11(5-6):355-607, 2019.
J. Rabin, G. Peyre, J. Delon, and M. Bernot. Wasserstein barycenter and its application to texture
mixing. In A. M. Bruckstein, B. M. ter Haar Romeny, A. M. Bronstein, and M. M. Bronstein
(eds.), Scale Space and Variational Methods in Computer Vision, pp. 435-446, Berlin, Heidelberg,
2012. Springer Berlin Heidelberg.
S.T. Rachev and L. Ruschendorf. Mass Transportation Problems: Volume I: Theory. Mass Trans-
portation Problems. Springer, 1998.
M. Rowland, J. Hron, Y. Tang, K. Choromanski, T. Sarlos, and A. Weller. Orthogonal estimation
of Wasserstein distances. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of
the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of
Proceedings of Machine Learning Research, pp. 186-195. PMLR, 16-18 Apr 2019.
T. Salimans, H. Zhang, A.Radford, and D. Metaxas. Improving GANs using optimal transport. In
International Conference on Learning Representations, 2018.
J.	Solomon, F. de Goes, G. Peyre, M. Cuturi, A. Butscher, A. Nguyen, T. Du, and L. Guibas.
Convolutional Wasserstein distances: Efficient optimal transportation on geometric domains. ACM
Trans. Graph., 34(4):66:1-66:11, 2015.
11
Under review as a conference paper at ICLR 2022
K.	T. Sturm. On the geometry of metric measure spaces. ii. Acta Math., 196(1):133-177, 2006.
T. Vayer, L. Chapel, R. Flamary, R. Tavenard, and N. Courty. Fused Gromov-Wasserstein distance for
structured objects: theoretical foundations and mathematical properties. CoRR, abs/1811.02834,
2018.
T. Vayer, R. Flamary, N. Courty, R. Tavenard, and L. Chapel. Sliced Gromov-Wasserstein. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d AlchE-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 32, pp. 14726-14736. Curran Associates, Inc.,
2019.
C. Villani. Optimal Transport: Old and New, volume 338 of Grundlehren der mathematischen
Wissenschaften. Springer Berlin Heidelberg, 2009.
J. Wu, Z. Huang, D. Acharya, W. Li, J. Thoma, D. P. Paudel, and L. V. Gool. Sliced Wasserstein
generative models, 2019.
H. Xu, D. Luo, and L. Carin. Scalable Gromov-Wasserstein learning for graph partitioning and
matching. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
BC, Canada, pp. 3046-3056, 2019.
12
Under review as a conference paper at ICLR 2022
A Proofs
A.1 Proof of Proposition 1
We use the following result:
Lemma 1 [Theorem 3 in (Nguyen et al., 2020)] For uniform measure σd-1 on the unit sphere Sd-1,
we have
^	∣hθ,θ0i∣dσdT(θ)dσdT(θ0)=.
Sd-1 ×Sd-1	πΓ((d + 1)/2)
Hence,
(Assumption 1)
Ld ɪ Md JhΦ(θ),Φ(θθ)i∣dσdT(θ)dσdT(θ0)	≤	Lφ £ ɪ Md ɪ lhθ, θ0i∣dσd-1(θ)dσd-1(θ0)
(Lemmal)	LφΓ(d∕2)
≤	√∏r((d + i)∕2).
Therefore, as long as the (φ, ψ)-admissible constants Cφ ≥ √LΓ(Γd+Zj∕2) and Cψ ≥ √LΓ(Γd+42∕2)，
we have σd-1 ∈ MCφ ∩ MCψ . Now using a Gautschi’s inequality (Gautschi, 1959) for the Gamma
function, it yields that √πr¾d+¾∕2) ≥ √π(3∕2 ≥ 1/d. Letσ = Pd=ι d加ι, Where {θι, ...,θd}
form an orthonormal basis in Rd . We then have
1	(Assumption 1)	1	L
Eda*[∣hΦ(θ),Φ(θo)i∣] = E (d)2∣hΦ(θk),ΦG)i∣	≤	Lφ £ (d)2∣hθk,θ0i∣ = L
d	dd
1≤k,l≤d	1≤k,l≤d
Therefore We get the loWer bounds for the (φ, ψ)-admissible constants Cφ and Cψ given in Proposi-
tion 1, that guarantee σd-1,σ ∈ Mcφ ∩ Mc^.
A.2 Proof of Proposition 2
Let us first state the tWo folloWing lemmas: Lemma 2 Writes an integration result using push-forWard
measures; it relates integrals With respect to a measure η and its push-forWard under a measurable
map f : X → Y . Lemma 3 proves that the admissible set of couplings betWeen the embedded
measures are exactly the embedded of the admissible couplings betWeen the original measures.
Lemma 2 [See Lerner (2014) p. 61] Let f : S → T be a measurable mapping, letη be a measurable
measure on S, and let g be a measurable，Unction on T. Then JT gdf#n = JS(g ◦ f )dη.
Lemma 3 [Lemma 6 in Paty & Cuturi (2019)] For all φ, ψ and μ ∈ P(X), V ∈ P(Y), one has
口(0#仙,砂#V) = {(φ 0 @)#Y s.t. γ ∈ Π(μ, v)}, where φ 0 ψ : X×Y→X×Y such that
(φ 0 ψ(χ,y) = (φ(χ), ψ(y)) forall XJy ∈X ×Y.
•	(i) HWDr (μ, μ) is finite. In one hand, we assume that μ ∈ Pr (X) and V ∈ Pr (Y), hence its r-th
moments are finite, i.e., Mr(μ) = (JX IlxlIrdμ(x))1" < ∞ and Mr(V) = (Jy Ilykrdv(y))1/r <
∞. In the other hand, the following holds for all parameter θ ∈ Sd-1 and a couple (φ, ψ)-embeddings,
Wr (μφ θ, vψ θ) =	inf	/	|u — UTdπ(u,u0)
r	nen(Pe(0)#“Me)#v)7r×r
(Lemma 3)
≤ 2r-1
2r-1
∙γ∈inf	JX y IΦ(Θ)>x — Ψ(θ)>y∣rdγ(x, y)
"f /	(∣φ(θ)> x∣r + ∣ψ(θ)>y∣r )dγ(x,y)
γ∈π(μ,ν) √χ×γ
7ejnf V)(JX IΦ(θ)>x∣rdμ(x) + L ∣ψ(θ)>y∣rdv(y)),
13
Under review as a conference paper at ICLR 2022
where we use the facts that (s + t)r ≤ 2r-1 (sr + tr), ∀s, t ∈ R+ and that any γ transport plan has
marginals μ on X and V on Y. By CaUchy-SchWarz inequality, We get
wr(μφ,θ,νψ,θ) ≤ 2r
kφ(θ)kr kχkr dμ(χ) +
Y
kψ(θ)kr kykr dν (y)) =2r-1(M; (μ)+ Mr (ν)).
Then, (Eθ〜θ[WT(μφ,θ,νψ,θ)])1" ≤ 2r-1(M；(〃)+ Mr(V))1/r ≤ 2r-1 (Mr(μ) + Mr(V)).
Finally, one has that HWDr (μ,ν) ≤ 2r-1 (Mr (μ) + Mr(V)).
•	(ii) Non-negativity and symmetry. Together the non-negativity, symmetry of Wasserstein distance
and the decoupling property of iterated infima (or principle of the iterated infima) yield the non-
negativity and symmetry of the distributional sliced sub-embedding distance.
•	(ii) HWDr (μ,μ) = 0. Let φ and φ0 two embeddings for projecting the same distribution μ.
Without loss of generality, We suppose that the corresponding (φ, φ0)-admissible constants Cφ0 ≤ Cφ,
hence MC 0 ⊆ MCφ. Using the fact that sup(A ∩ B) ≤ sup A ∧ sup B, (With a ∧ b) = min(a, b)),
se have, straightforWardly,
HWDr(μ, μ) = inf SUp (Eθ〜Θ [W；(μφ,θ, μφ,,θ)] r
φ,φ0 Θ∈MCφ ∩MC 0
≤ inf ( SUp (eθ〜θ[Wrr(μφ,θ,μφo,θ)])r ∧ sup	饱〜θ [Wrr(μφ,θ,μφ0,θ)])r
φ,φ0	Θ∈MCφ	Θ∈MCφ0
inf inf SUp (Eθ〜θ [Wrr(μφ,θ,μφ0,θ)] ) r ∧ SUp
φ φ0	Θ∈MCφ	Θ∈MCφ0
(Eθ 〜θ
[Wr(μφ,θ ,μφ0,θ
≤ inf SUp (Eθ〜θ [Wr(μφ,θ,μφ,θ)] ) r ∧
φ	Θ∈MCφ
1
r
SUp	(Eθ〜㊀[Wrr (μφ,θ, μφ,θ)]
Θ∈MCφ0
≤ inf SUp	(Eθ〜θ[Wrr(μφ,θ,μφ,θ)] ) r
φ Θ∈MCφ
0.
•	(iii) One has (d)7 inf φ,ψ maxθ∈sd-ι Wr(μφ,θ ,νψ,θ) ≤ HWDr(μ,ν) ≤ inf φ,ψ maxj∈sd-ι Wr(μφ,θ ,νψ,θ).
Since Mc@ ∩ Mcψ ⊂ Mi and Wrr(μφ,θ,νψ,θ) ≤ maxθ∈sd-ι Wr(μφ,θ,νψ,θ) we find that
1
SUp	(Eθ〜θ[Wr(μφ,θ,Vψ,θ)])r ≤ SUp (Eθ〜θ[Wr(μφ,θ,Vψ,θ)]
Θ∈MCφ ∩MCψ	Θ∈M1
1
r
≤ ( max Wr(μφ,θ,νψ,θ))1/r
θ∈Sd-1
≤ max Wr(μφ,θ,νψ,θ),
θ∈Sd-1
which entails that HWDr(μ, V) ≤ infφ,ψ maxθ∈sd-ι Wr(μφ,θ, νψ,θ). Moreover, since the (φ, ψ)-
admissible constants Cφ and Cψ satisfy Cφ ≥ 辱 and Cψ ≥ uuψ hence σ = Pd= i dδθ% ∈ Mcφ ∩
Mcψ, where we set θi = argmaxθ∈sd-ι Wr (μφ,θ, Vψ,θ). We then obtain
HWDr (μ,ν) ≥ inf (Eθ* [W"μφ,θ ,νψ,θ)]`
d1
=φnψ (X dWr(μφ,θι,νΨ,θι)
, l=1
≥ (1 )1/r inf Wr (μφ,θι ,νΨ,θι )
d φ,ψ
1
r
1
r
1 )1/r inf max Wr(μφ,θ,νψ θ).
d φ,ψ θ∈Sd-1	r φ, ψ,
14
Under review as a conference paper at ICLR 2022
•	(iv) For p = q, HWD is upper bound by the distributional Wasserstein distance (DSW). Let US first
recall the DSW distance: let C > 0 and set MC = {Θ ∈ P(SdT):坦乂夕〜㊀[|〈40，)|] ≤ C}.
1
DSW r (μ,ν )= sup (Eθ 〜θ[M (μθ ,Vθ )])r.
Θ∈Mc、	,
We have that the case of a identity couple of embeddings, φ = Id, ψ = Id, the probability measure
set Me6, Mcψ = Mc , then it is trivial that HWDr (μ,ν) ≤ DSWr (μ,ν).
•	(v) Rotation invariance. Note that (R^μ)ψ,θ = P@(s)#(R#m) = (Pφ(θ) o R)#M, and for
all x ∈	Rp,	using the adjoint operator	R*,	(R* =	R-1),	(Pφ⑻ o	R)(x)	=	{φ(θ),	R(x))=
(R*(φ(θ)),χ> = Pβ*oφ(θ)(x). Then, (R#m)0,j =(。冗*。双6))#从 Analogously, one has
(Q#V)ψ,θ = (PQ*oψ(θ))#ν. Moreover,
Mcφ = {Θ ∈ P(SdT) : Eθ,θ，〜θ[∣hφ(θ),φ(θ')i∣]}
={Θ ∈ P(SdT) : Eθ,θ，〜θ[∣h(R* o φ)(θ),(R* o φ)(θ0)i∣]}
=MCR *°φ .
Then we have similarly Mcψ = Mcq*o^ . This implies
1
HWDr (R#μ, Q#V) = inf sup (脸 〜θ [W； ((R#〃)μ, (Q#v)*)])'
φ,ψ Θ∈Mcφ∩Mcψ v	/
inf sup	(Eθ 〜θ[W"(PR*oφ(θ)用μ,PQ*oψ(θ)
φ,ψ Θ∈Mcφ∩Mcψ V
1
r
inf sup	(Eθ〜θ [Wr(μκ*oφ,θ,μR*oφ,θ])
φ,ψ Θ∈Mcφ∩M4、	/
inf	sup	(Eθ〜θ[Wr(μR*oφ,θ,vq*oφ,θ]) r
φ,ψ θ∈MCR*θφ ∩MCQ*oψ '	7
inf	sup	(Eθ〜θ [W∕(μR*oφ,θ,vq*o吸θ])r
φ,ψ θ∈MCR*oφ ∩MCQ*oψ
inf,	SUP	(Eθ 〜θ[Wr (μφ',θ ,νψ0,θ]
φ0=R*oφ,ψ0=Q*oψ θ∈Maφ0 ∩Mc砂0 v
HWDr (μ, V).
• (Vi) Translation quasi-invariance. We have
HWDr(T。#〃, Tβ#V) = inf sup	Eθ〜θ [W；((T°#〃)源, (Te#v)ψ,θ)]
φ,ψ θ∈Mcφ ∩Mcψ v
1
r
By Lemmas 3 and 2 , we have
Wr ((T。#〃)源,(τβ #v)评
=	inf	∣ ∣u - v∣rdγ(u,v)
γ∈n((Taφμ)φ,θ,(Tβ≠ν)ψ,θ)) ∕r2
=Y∈∏((Pφ(θ)oTα)inμ ,(Pψ(θ)oTβ )#v) LjU - v∣ dMu, V)
=Ie既“/ y ∣Pφ(θ) o Tα(x) - Pψ(θ) o Tβ)(y)∣rdγ(x,y)
=IM "/ J(Pφ(θ)(X)- pψ(θ)(y)) + (Pφ(θ)(O)- pψ(θ)(0)∣rdγ(x,y)
≤ 2r-1
≤ 2r-1
Jnf	XJ(Pφ(θ)(x)- Pψ(θ)(y))∣r dγ(x,y) + ∣Pφ(θ)(α)- Pψ(θ)(β))∣r
∙γeinf "/ J(PΦ(θ)(X)- pψ(θ)(y))∣rdγ(x, y) + (IIak + ∣∣β||)r).
15
Under review as a conference paper at ICLR 2022
Thanks to Minkowski inequality,
SUp	(Eθ〜θ[w;((T。#〃状(Te#V)ψ)]
Θ∈MCφ ∩MCψ
1
r
≤ 2r-1	SUp	(Eθ 〜θ[ iqf、/	l(Pφ(θ)(x) - Pψ(θ)(y))∣r dγ(x,y)
Θ∈Mcφ∩Mcψ	γ∈π(μ,V) X×γy
+ 2r-1(kα∣ + IlelI)	SUp	(θ(Sd-1)) r
Θ∈MCφ∩MCψ
1
r
≤ 2r-1	sup	(Eθ〜θW(μφ,θ,νψ,θ)])1 +2r-1 (kαk + kβk).
Θ∈MCφ ∩MCψ
Therefore, we get HWDr(T。#〃,7产#v) ≤ 2r-1HWDr(μ, ν) +2r-1(kɑk + ∣∣βk).
B Implementation
This section graphically describes the learning procedure in Algorithm 1. It also provides the training
details not exposed in the main body of the paper.
B.1 Learning scheme
We present in Figure 7 the updated graphics of our approach, highlighting the main components :
the distributional part is ensured by a first deep neural network as is each of the mappings. As each
of the networks should be learned, we included the part of the loss functions associated with each
network (blue fonts correspond to minimization, whereas red fonts correspond to maximization, see
Algorithm section).
∑fc,,^ (此仇)「幽(初]-0照，)2
Σ,k,,k',Φ[f(θk')]τ,Φ[f(.θk'')]
pφ(f(θ^)
)pΦ(i(βκ}}
-----pφ(∕(βχ))
∑k,k>Φlf(θk)]τΦ[f(θkl)]
∑⅛,,v (N∕(W)]T 血/(源，)]-。"G
WM城,”)弧四”)…WXw∕*e)
11/r
⑵3川产加(即6M∙≡]
-1
Figure 7: The implemented approach. Both the distributional and mappings parts are achieved by
deep neural networks. A number K of projections is used to compute 1D-Wasserstein distances.
B.2 Training details
Our experimental evaluations on shape datasets for scalability contrast GW, SGW and HWD. Re-
garding or classification under isometry transformations, we additionnally consider RI-SGW. Used
16
Under review as a conference paper at ICLR 2022
hyper-parameters for those experiments are detailed below. Notice that SGW, RI-SGW and HWD
rely on K, the number of projections sampled uniformly over the unit sphere. This K may vary from
a method to another.
1.	SGW: K .
2.	RI-SGW: λRI-SGW, the learning rate and T, the maximal number of iterations for solving 4
over the Stiefeld manifold.
3.	HWD: beyond K and the latent space dimension d, it requires the parametrization of φ, ψ
and f as deep neural networks and their optimizers. For solving the min-max problem by an
alternating optimization scheme we use N inner loops and T number of epochs.
For SGW and RI-SGW we use the code made available by their authors and cite the related reference
Vayer et al. (2018) as they require. We use POT toolbox Flamary et al. (2021) to compute GW
distance.
Scalability This experiment measures the average running time to compute OT-based distance
between two pairs of shapes made of n 3D-vertices. 100 pairs of shapes were considered and n varies
in {100, 250, 500, 1000, 1500, 2000}.
We choose KSGW = 1000 (as a default value).
For HWD, the mapping function f is designed as a deep network with 2 dense hidden layers of
size 50. Regarding both φ and ψ, they have also the same architecture as f (with adapted input and
output layers) but the hidden layers are 10-dimensional. Adam optimizers with default parameters
are selected to train them. Finally we consider KHWD = 10, d = 5, T = 50, N = 1 as default values.
Notice also that the regularization parameters λC and λa are set to 1.
The used ground cost distance for GW distance is the geodesic distance.
Classification under transformations invariance For this experiment, we consider the same set
of hyper-parameters as for Scalability evaluation on shape datasets. Besides, the supplementary com-
petitor RI-SGW was trained by setting KRI-SGW = 1000 = KSGW = 1000, λRI-SGW = 0.01,
TRI-SGW = 500. Notice that due to the high-resolution of the meshes (more than 19K three-
dimensional vertices), RI-SGW and GW were not able to produce the pairwise-distance matrix
used in 1NN classification after several hours.
C Additional experimental results
iteration 10
iteration IOOOO
iteration 20000
iteration 30000
iteration io
iteration 10000
iteration 20000
iteration 30000
Figure 8: Comparing (top) HWD and (bottom) SGW on generating 2D distributions from 3D target.

17
Under review as a conference paper at ICLR 2022
iteration 10
iteration 10000
iteration 20000
iteration 30000
target
iteration 10000
Figure 9: Comparing (top) HWD and (bottom) SGW on generating 3D distributions from 2D target.
isometry 1 isometry 2 isometry 3 isometry 4 isometry 5 null 0
isometry 1 null 0 isometry 5 isometry 4 isometry 2 isometry 3
null 0 isometry 5 isometry 2 isometry 4 isometry 3 isometry 1
HHH HHU JHHI
Figure 10: Instances of the shape dataset with null and isometry transformations. The classes
are respectively human,dog and horse. For the experiments of Figure 6 we also consider the
"topology", "scale", "shotnoise" transformations that respectively amount to deform, to upscale and
to add noise to the shapes of each class.
18