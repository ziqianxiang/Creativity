Under review as a conference paper at ICLR 2022
Robust Models Are More Interpretable
Because Attributions Look Normal
Anonymous authors
Paper under double-blind review
Ab stract
Recent work has found that adversarially-robust deep networks used for image
classification are more interpretable: their feature attributions tend to be sharper,
and are more concentrated on the objects associated with the image’s ground-
truth class. We show that smooth decision boundaries play an important role in
this enhanced interpretability, as the model’s input gradients around data points
will more closely align with boundaries’ normal vectors when they are smooth.
Thus, because robust models have smoother boundaries, the results of gradient-
based attribution methods, like Integrated Gradients and DeepLift, will capture
more accurate information about nearby decision boundaries. This understanding
of robust interpretability leads to our second contribution: boundary attributions,
which aggregate information about the normal vectors of local decision bound-
aries to explain a classification outcome. We show that by leveraging the key fac-
tors underpinning robust interpretability, boundary attributions produce sharper,
more concentrated visual explanations—even on non-robust models.
1	Introduction
Feature attribution methods are widely used to explain the predictions of neural networks (Binder
et al., 2016; Dhamdhere et al., 2019; Fong & Vedaldi, 2017; Leino et al., 2018; Montavon et al.,
2015; Selvaraju et al., 2017; Shrikumar et al., 2017; Simonyan et al., 2013; Smilkov et al., 2017;
Springenberg et al., 2014; Sundararajan et al., 2017). By assigning an importance score to each input
feature of the model, these techniques help to focus attention on parts of the data most responsible for
the model’s observed behavior. Recent work (Croce et al., 2019; Etmann et al., 2019) has observed
that feature attributions in adversarially-robust image models, when visualized, tend to be more
interpretable—the attributions correspond more clearly to the discriminative portions of the input.
One way to explain the observation relies on the fact that robust models do not make use of non-
robust features (Ilyas et al., 2019) whose statistical meaning can change with small, imperceptible
changes in the source data. Thus, by using only robust features to predict, these models naturally
tend to line up with visibly-relevant portions of the image. Etmann et al. take a different approach,
showing that the gradients of robust models’ outputs more closely align with their inputs, which
explains why attributions on image models are more visually interpretable.
In this paper, we build on this geometric understanding of robust interpretability. With both analyti-
cal (Sec. 3) and empirical (Sec. 5) results, we show that the gradient of the model with respect to its
input, which is the basic building block of all gradient-based attribution methods, tends to be more
closely aligned with the normal vector of a nearby decision boundary in robust models than in “nor-
mal” models. Leveraging this understanding, we propose Boundary-based Saliency Map (BSM) and
Boundary-based Integrated Gradient (BIG), two variants of boundary attributions (Sec. 4), which
base attributions on information about nearby decision boundaries (see an illustration in Fig. 1a).
While BSM provides theoretical guarantees in the closed-form, BIG generates both quantitatively
and qualitatively better explanations. We show that these methods satisfy several desireable formal
properties, and that even on non-robust models, the resulting attributions are more focused (Fig. 1b)
and less sensitive to the “baseline” parameters required by some attribution methods.
To summarize, our main contributions are as follows. (1) We present an analysis that sheds light
on the previously-observed phenomeon of robust interpretability showing that alignment between
the normal vectors of decision boundaries and models’ gradients is a key ingredient (Proposition 1).
1
Under review as a conference paper at ICLR 2022
Figure 1: (a) Visualizations of geometrical interpretations of Saliency Map (SM), Boundary-based
Saliency Map (BSM), Integrated Gradient (IG) and Boundary-based Integrated Gradient (BIG). Gra-
dient computations can be viewed as projecting the input onto a particular decision boundary. While
SM projects to a nearby boundary (H1), BSM projects to the nearest one (H2). IG (the red dashed
path) from a global baseline xg, i.e. zeros, aggregates boundaries in colorful shaded areas; BIG
(the green dashed path) integrates from the point xbd on the nearest boundary H2 to x and therefore
aggregates nearby boundaries, H1 and H2 in gray shaded areas. (b) Visualizations of Integrated Gra-
dient and the proposed improvement of it, Boundary-based Integrated Gradient, which is sharper,
more concentrated and less noisy.
Integrated Gradient
Boundary-based Integrated Gradient
(b)
In particular, we derive a closed-form result for one-layer networks (Theorem 1) and empirically
validate the take-away of our theorem generalizes to deeper networks. (2) Motivated by our anal-
ysis, we introduce boundary attributions, which leverage the connection between boundary normal
vectors and gradients to yield explanations for non-robust models that carry over many of the fa-
vorable properties that have been observed of explanations on robust models. (3) We empirically
demonstrate that one such type of boundary attribution, called Boundary-based Integrated Gradi-
ents (BIG), produces explanations that are more accurate than prior attribution methods (relative to
ground-truth bounding box information), while mitigating the problem of baseline sensitivity that is
known to impact applications of Integrated Gradients Sundararajan et al. (2017) (Section 6).
2	Background
We begin by introducing our notations. Throughout the paper we use italicized symbols x to denote
scalar quantities and bold-face x to denote vectors. We consider neural networks with ReLU as
activations prior to the top layer, and a softmax activation at the top. The predicted label for a given
input x is given by F (x) = arg maxc fc(x), x ∈ Rd, where F (x) is the predicted label and fi(x)
is the output on the class i. As the softmax layer does not change the ranking of neurons in the top
layer, we will assume that fi(x) denotes the pre-softmax score. Unless otherwise noted, we use ||x||
to denote the `2 norm of x, and the `2 neighborhood centered at x with radius as B(x, ).
Explainability. Feature attribution methods are widely-used to explain the predictions made by
DNNs, by assigning importance scores for the network’s output to each input feature. Convention-
ally, scores with greater magnitude indicate that the corresponding feature was more relevant to the
predicted outcome. We denote feature attributions by z = g(x, f), z, x ∈ Rd. When f is clear from
the context, we simply write g(x). While there is an extensive and growing literature on attribution
methods, our analysis will focus closely on the popular gradient-based methods, Saliency Map (Si-
monyan et al., 2013), Integrated Gradient (Sundararajan et al., 2017) and Smooth Gradient (Smilkov
et al., 2017), shown in Defs 1-3.
Definition 1 (Saliency Map (SM)) The SalienCy Map gS(x) is g^ven by gS (x) := fx∙
Definition 2 (Integrated Gradient (IG)) Given a baseline input xb, the Integrated Gradient
gIG(x; Xb) is given by gig(x； Xb) ：= (X - Xb) R01 df ((X-Xb)t+xb)dt
2
Under review as a conference paper at ICLR 2022
Figure 2: Different classifiers that partition the space into regions associated With apple or
banana. (a) A linear classifier where n is the only faithful explanations and V is not. (b) A deep
network with ReLU activations. Solid lines correspond to decision boundaries while dashed lines
correspond to facets of activation regions. (c) Saliency map of the target instance may be normal to
the closest decision boundary (right) or normal to the prolongation of other local boundaries (left).
Definition 3 (Smooth Gradient (SG)) Given a zero-centered Gaussian distribution N with a stan-
dard deviation σ, the Smooth Gradient gSG(x; σ) is given by gSG(x; σ) := Ee〜N(0,σ2I)。"射。.
Besides, we will also include results from DeepLIFT (Shrikumar et al., 2017) and grad × input
(element-wise multiplication between Saliency Map and the input) (Simonyan et al., 2013) in our
empirical evaluation. As we show in Section 3.2, Defs 1-3 satisfy axioms that relate to the local
linearity of ReLU networks, and in the case of randomized smoothing (Cohen et al., 2019), their
robustness to input perturbations. We further discuss these methods relative to others in Sec. 7.
Robustness. Two relevant concepts about adversarial robustness will be used in this paper: pre-
diction robustness that the model’s output label remains unchanged within a particular `p norm ball
and attribution robustness that the feature attributions are similar within the same ball. Recent work
has identified the model’s Lipschitz continuity as a bridge between these two concepts (Wang et al.,
2020c) and some loss functions in achieving prediction robustness also bring attribution robustness
(Chalasani et al., 2020). We refer to robustness as prediction robustness if not otherwise noted.
3	Explainability, Decision B oundaries, and Robustness
In this section, we begin by discussing the role of decision boundaries in constructing explanations
of model behavior via feature attributions. We first illustrate the key relationships in the simpler
case of linear models, which contain exactly one boundary, and then generalize to piecewise-linear
classifiers as they are embodied by deep ReLU networks. We then show how local robustness causes
attribution methods to align more closely with nearby decision boundaries, leading to explanations
that better reflect these relationships.
3.1	Attributions for linear models
Consider a binary classifier C(x) = sign(w>x + b) that predicts a label {-1, 1} (ignoring “tie”
cases where C(x) = 0, which can be broken arbitrarily). In its feature space, C(x) is a hyperplane
H that separates the input space into two open half-spaces S1 and S2 (see Fig. 2a). Accordingly,
the normal vector n of the decision boundary is the only vector that faithfully explains the model's
classification while other vectors, while they may describe directions that lead to positive changes in
the model’s output score, are not faithful in this sense (see v in Fig. 2a for an example). In practice,
to assign attributions for predictions made by C , SM, SG, and the integral part of IG (see Sec. 2)
return a vector characterized by Z = k1 n + k2 (Ancona et al., 2018), where k1 = 0 and k2 ∈ R,
regardless of the input x that is being explained. In other words, these methods all measure the
importance of features by characterizing the model’s decision boundary, and are equivalent up to the
scale and position of n.
3
Under review as a conference paper at ICLR 2022
3.2	Generalizing to piecewise-linear boundaries
In the case of a piecewise-linear model, such as a ReLU network, the decision boundaries comprise
a collection of hyperplane segments that partition the feature space, as in H1, H2 and H3 in the
example shown in Figure 2b. Because the boundary no longer has a single well-defined normal,
one intuitive way to extend the relationship between boundaries and attributions developed in the
previous section is to capture the normal vector of the closest decision boundary to the input being
explained. However, as we show in this section, the methods that succeeded in the case of linear
models (SM, SG, and the integral part of IG) may in fact fail to return such attributions in the more
general case of piecewise-linear models, but local robustness often remedies this problem. We begin
by reviewing key elements of the geometry of ReLU networks (Jordan et al., 2019).
ReLU activation polytopes. For a neuron u in a ReLU network f (x), we say that its status is ON
if its pre-activation u(x) ≥ 0, otherwise it is OFF. We can associate an activation pattern denoting
the status of each neuron for any point x in the feature space, and a half-space Au to the activation
constraint u(x) ≥ 0. Thus, for any point x the intersection of the half-spaces corresponding to its
activation pattern defines a polytope P (see Fig. 2b), and within P the network is a linear function
such that ∀x ∈ P, f (x) = wP>x + bP , where the parameters wp and bP can be computed by
differentiation (Fromherz et al., 2021). Each facet of P (dashed lines in Fig. 2b) corresponds to
a boundary that “flips” the status of its corresponding neuron. Similar to activation constraints,
decision boundaries are piecewise-linear because each decision boundary corresponds to a constraint
fi(x) ≥ fj (x) for two classes i, j (Fromherz et al., 2021; Jordan et al., 2019).
Gradients might fail. Saliency maps, which we take to be simply the gradient of the model with
respect to its input, can thus be seen as a way to project an input onto a decision boundary. That
is, a saliency map is a vector that is normal to a nearby decision boundary segment. However, as
others have noted, a saliency map is not always normal to any real boundary segment in the model’s
geometry (see the left plot of Fig. 2c), because when the closest boundary segment is not within the
activation polytope containing x, the saliency map will instead be normal to the linear extension of
some other hyperplane segment (Fromherz et al., 2021). In fact, iterative gradient descent typically
outperforms the Fast Gradient Sign Method (Goodfellow et al., 2015) as an attack demonstrates that
this is often the case.
When gradients succeed. While saliency maps may not be the best approach in general for captur-
ing information about nearby segments of the model’s decision boundary, there are cases in which it
serves as a good approximation. Recent work has proposed using the Lipschitz continuity ofan attri-
bution method to characterize the difference between the attributions of an input x and its neighbors
within a `p ball neighborhood (Def. 4) (Wang et al., 2020c). This naturally leads to Proposition 1,
which states that the difference between the saliency map at an input and the correct normal to the
closest boundary segment is bounded by the distance to that segment.
Definition 4 (Attribution Robustness) An attribution method g(x) is (λ, δ)-locally robust at the
evaluated point X if ∀x0 ∈ B(x, δ), ∣∣g(x0) — g(x)∣∣ ≤ λ∣∣x0 — x||.
Proposition 1 Suppose that f has a (λ, δ)-robust saliency map gS at x, x0 is the closest point on
the closest decision boundary segment to X and ||X0 - X|| ≤ δ, and that n is the normal vector of
that boundary segment. Then ||n 一 gS(x)|| ≤ λ∣∣x 一 x0∣∣.
Proposition 1 therefore provides the following insight: for networks that admit robust attribu-
tions (Chen et al., 2019; Wang et al., 2020c), the saliency map is a good approximation to the
boundary vector. As prior work has demonstrated the close correspondence between robust predic-
tion and robust attributions (Wang et al., 2020c; Chalasani et al., 2020), this in turn suggests that
explanations on robust models will more closely resemble boundary normals.
As training robust models can be expensive, and may not come with guarantees of robustness,
post-processing techniques like randomized smoothing (Cohen et al., 2019), have been proposed
as an alternative. Dombrowski et al. (2019) noted that models with softplus activations (y =
1∕β log(1 + exp (βx))) approximate smoothing, and in fact give an exact correspondence for Single-
layer networks. Combining these insights, we arrive at Theorem 1, which suggests that the saliency
map on a smoothed model approximates the closest boundary normal vector well; the similarity is
inversely proportional to the standard deviation of the noise used to smooth the model.
4
Under review as a conference paper at ICLR 2022
Theorem 1 Let m(x) = ReLU (W x) be a one-layer network and when using randomized smooth-
ing, we write mσ (x). Let g (x) be the SM for mσ (x) and suppose ∀x00 ∈ B(x, ||x-x0||), ||g(x00)|| ≥
c where x0 is the closest adversarial example, we have the following statement holds: ||g(x) -
g(x0)|| ≤ λ where λ is monotonically decreasing w.r.t σ.
Theorem 1 suggests that when randomized smoothing is used, the normal vector of the closest
decision boundary segment and the saliency map are similar, and this similarity increases with the
smoothness of the model’s boundaries. We think the analytical form for deeper networks exists but
its expression might be unnecessarily complex due that we need to recursively apply ReLU before
computing the integral (i.e., the expectation). The analytical result above for one-layer network
and empirical validations for deeper nets in Figure 11, if taken together, shows that attributions and
boundary-based attributions are more similar in a smoothed model.
4	B oundary-Based Attribution
Without the properties introduced by robust learning or randomized smoothing, the local gradient,
i.e. saliency map, may not be a good approximation of decision boundaries. In this section, we build
on the insights of our analysis to present a set of novel attribution methods that explicitly incorporate
the normal vectors of nearby boundary segments. Importantly, these attribution methods can be
applied to models that are not necessarily robust, to derive explanations that capture many of the
beneficial properties of explanations for robust models.
Using the normal vector of the closest decision boundary to explain a classifier naturally leads to
Definition 5, which defines attributions directly from the normal of the closest decision boundary.
Definition 5 (Boundary-based Saliency Map (BSM)) Given f and an input x, we define
Boundary-basedSaliency Map BS(x) asfollows: BS(x) = ∂fc(x0)∕∂x0, where x0 is the closest ad-
versarial example to X, i.e. C = F(X) = F(x0) and∀Xm.∣∣Xm —x|| < ||x0 — x|| → F(X) = F(Xm).
Incorporating More Boundaries. The main limitation of using Definition 5 as a local explanation
is obvious: the closest decision boundary only captures one segment of the entire decision surface.
Even in a small network, there will be numerous boundary segments in the vicinity of a relevant
point. Taking inspiration from Integrated Gradients, Definition 6 proposes the Boundary-based Inte-
grated Gradient (BIG) by aggregating the attributions along a line between the input and its closest
boundary segment.
Definition 6 (Boundary-based Integrated Gradient(BIG)) Given f, Integrated Gradient gIG and
an input X, we define Boundary-based Integrated Gradient BS(X) as follows: BIG(X) := gIG(X; X0),
where X is the nearest adversarial example to X, i.e. C = F(x) = F(x0) and ∀Xm.∣∣Xm — x|| <
||X0 — X|| → F(X) = F(Xm).
Geometric View of BIG. BIG explores a linear path from the boundary point to the target point.
Because points on this path are likely to traverse different activation polytopes, the gradient of
intermediate points used to compute gIG are normals of linear extensions of their local boundaries.
As the input gradient is identical within a polytope Pi , the aggregate computed by BIG sums each
gradient wi along the path and weights it by the length of the path segment intersecting with Pi .
In other words, one may view IG as an exploration of the model’s global geometry that aggregates
all boundaries from a fixed reference point, whereas BIG explores the local geometry around X. In
the former case, the global exploration may reflect boundaries that are not particularly relevant to
model’s observed behavior at a point, whereas the locality of BIG may aggregate boundaries that
are more closely related (a visualization is shown in Fig. 1a).
Finding nearby boundaries. Finding the exact closest boundary segment is identical to the problem
of certifying local robustness (Fromherz et al., 2021; Jordan et al., 2019; Kolter & Wong, 2018;
Lee et al., 2020; Leino et al., 2021b; Tjeng et al., 2019; Weng et al., 2018), which is NP-hard for
piecewise-linear models (Sinha et al., 2020). To efficiently find an approximation of the closest
boundary segment, we leverage and ensemble techniques for generating adversarial examples, i.e.
PGD (Madry et al., 2018), AutoPGD (Croce & Hein, 2020) and CW (Carlini & Wagner, 2017), and
use the closest one found given a time budget. The details of our implementation are discussed in
Section 5, where we show that this yields good results in practice.
5
Under review as a conference paper at ICLR 2022
CIFAR10	standard	'210.5							
SM-BSM. IG-AGI IG-BIG	59.96 28.20 31.22	1.23 1.43 2.73							
					Corr.	Loc.	EG	PP	Con.
ImageNet	standard '2∣3.0	'∞l 24	5	'	∞l 2⅛	一 SM-BSM 一 IG-AGI	0.40 0.24	0.46 0.25	-0.19 0.05	0.07 -0.03
SM-BSM IG-AGI	8.48	0.41 13.52	0.36	2.25 1.19		1.61 0.86	IG-BIG	0.35	0.30	0.20	-0.03
									
IG-BIG	17.07	0.69	1.74		1.45					
(a)	(b)
Figure 3: (a): `2 differences between SM, IG and their boundary variants for robust models. The
heading of each column reports the respective training epsilon and the corresponding `p norm con-
straint; Appendix B.4 reports the corresponding boxplot. (b): Linear correlation coefficients be-
tween the alignment of SM and IG with nearby boundary vectors, and the localization metrics. For
each row starting with X-Y , the alignment is defined as -||X - Y ||. For each column, the localiza-
tion results are measured with approach in bold font, a.k.a X.
Model	Metrics	BIG	BSM	AGI	SM	GTI	SG	IG	DeepLIFT
	Loc.	0.38	0.33	0.33	0.33	0.35	0.34	0.34	0.34
standard	EG	0.54	0.47	0.48	0.47	0.46	0.55	0.5	0.49
	PP	0.87	0.50	0.58	0.50	0.50	0.50	0.51	0.53
	Con.	4.35	3.88	4.01	3.92	3.94	4.06	3.97	3.93
	Loc.	0.39	0.33	0.39	0.33	0.33	0.34	0.33	0.33
'2∣3.0	EG	0.74	0.6	0.64	0.6	0.63	0.62	0.65	0.64
	PP	0.92	0.50	0.88	0.50	0.55	0.51	0.65	0.77
	Con.	5.03	4.12	4.32	4.10	4.25	4.23	4.37	4.34
Table 1: Results of several attribution methods over 1500 images of ImageNet using a standard and
robust ResNet50 (training is reported in the first column). BIG: Boundary-based Integrated Gra-
dient. BSM: Boundary-based Saliency Map. AGI: Adversarial Gradient Integration. SM: Saliency
Map. GTI: grad×input. SG: Smoothed Gradient. IG: Integrated Gradient. See Appendix E for
the corresponding boxplot.
5	Evaluation
In this section, we first validate that the attribution vectors are more aligned to normal vectors of
nearby boundaries in robust models(Fig. 3a). We secondly show that boundary-based attributions
provide more “accurate" explanations - attributions highlight features that are actually relevant to the
label - both visually (Fig. 4 and 5) and quantitatively (Table 1). Finally, We show that in a standard
model, whenever attributions more align with the boundary attributions, they are more “accurate”.
General Setup. We conduct experiments over two data distributions, ImageNet (Russakovsky et al.,
2015) and CIFAR-10 (Krizhevsky et al.). For ImageNet, we choose 1500 correctly-classified images
from ImageNette (Howard), a subset of ImageNet, with bounding box area less than 80% of the
original source image. For CIFAR-10, We use 5000 correctly-classified images. All standard and
robust deep classifiers are ResNet50. All weights are pretrained and publicly available (Engstrom
et al., 2019). Implementation details of the boundary search (by ensembling the results of PGD, CW
and AutoPGD) and the hyperparameters used in our experiments, are included in Appendix B.2.
5.1	ROBUSTNESS → B OUNDARY ALIGNMENT
In this subsection, we show that SM and IG better align with the normal vectors of the decision
boundaries in robust models. For SM, we use BSM as the normal vectors of the nearest decision
boundaries and measure the alignment by the `2 distance between SM and BSM following Propo-
sition 1. For IG, we use BIG as the aggregated normal vectors of all nearby boundaries because
6
Under review as a conference paper at ICLR 2022
IG also incorporates more boundary vectors. Recently, Pan et al. (2021) also provides Adversarial
Gradient Integral (AGI) as an alternative way of incorporating the boundary normal vectors into IG.
We first use both BIG and AGI to measure how well IG aligns with boundary normals and later
compare them in Sec. 5.2, followed by a formal discussion in Sec. 7.
Aggregated results for standard models and robust models are shown in Fig. 3a. It shows that
adversarial training with bigger encourages a smaller difference between attributions and their
boundary variants. Particularly, using `2 norm and setting = 3.0 are most effective for ImageNet
compared to '∞ norm bound. One possible explanation is that the '2 space is special because
training with '∞ bound may encourage the gradient to be more LiPSChitz in '1 because of the duality
between the Lipschitzness and the gradient norm, whereas `2 is its own dual.
5.2	B OUNDARY ATTRIBUTION → BETTER LOCALIZATION
In this subsection, we show boundary attributions (BSM, BIG and AGI) better localize relevant
features. Besides SM, IG and SG, we also focus on other baseline methods including Grad ×
Input (GTI) (Simonyan et al., 2013) and DeepLIFT (rescale rule only) (Shrikumar et al., 2017)
that are reported to be more faithful than other related methods (Adebayo et al., 2018; 2020).
In an image classification task where ground-truth bounding boxes are given, we consider features
within a bounding box as more relevant to the label assigned to the image. Our evaluation is per-
formed over ImageNet only because no bounding box is provided for CIFAR-10 data. The metrics
used for our evaluation are: 1) Localization (Loc.) (Chattopadhyay et al., 2017) evaluates the in-
tersection of areas with the bounding box and pixels with positive attributions; 2) Energy Game
(EG) (Wang et al., 2020a) instead computes the portion of attribute scores within the bounding box.
While these two metrics are common in the literature, we propose the following additional metrics:
3)Positive Percentage (PP) evaluates the portion of positive attributions in the bounding box be-
cause a naive assumption is all features within bounding boxes are relevant to the label (we will
revisit this assumption in Sec. 6); and 4) Concentration (Con.) sums the absolute value of attri-
bution scores over the distance between the “mass” center of attributions and each pixel within the
bounding box. Higher Loc., EG, PP and Con. are better results. We provide formal details for the
above metrics in Appendix B.1.
We show the average scores for ResNet50 models in Table 1 where the corresponding boxplots
can be found in Appendix B.4. BIG is noticeably better than other methods on Loc. EG, PP and
Con. scores for both robust and standard models and matches the performance of SG on EG for a
standard model. Notice that BSM is not significantly better than others in a standard model, which
confirms our motivation of BIG - that We need to incorporate more nearby boundaries because a
single boundary may not be sufficient to capture the relevant features.
We also measure the correlation between the alignment ofSM and BSM with boundary normals and
the localization abilities, respectively. For SM, we use BSM to represent the normal vectors of the
boundary. For IG, we use AGI and BIG. For each pair X-Y in {SM-BSM, IG-AGI, IG-BIG}, we
measure the empirical correlation coefficient between -||X - Y ||2 and the localization scores ofX
in a standard ResNet50 and the result is shown in Fig. 3b. Our results suggest that when the attribu-
tion methods better align with their boundary variants, they can better localize the relevant features
in terms of the Loc. and EG. However, PP and Con. have weak and even negative correlations. One
possible explanation is that the high PP and Con. of BIG and AGI compared to IG (as shown in
Table 1) may also come from the choice of the reference points. Namely, compared to a zero vector,
a reference point on the decision boundary may better filter out noisy features.
We end our evaluations by visually comparing the proposed method, BIG, against all other attri-
bution methods for the standard ResNet50 in Fig. 4 and for the robust ResNet50 in Fig. 5, which
demonstrates that BIG can easily and efficiently localize features that are relevant to the prediction.
More visualizaitons can be found in the Appendix E.
Summary. Taken together, we close the loop and empirical show that standard attributions in robust
models are visually more interpretable because they better capture the nearby decision boundaries.
Therefore, the final take-away from our analytical and empirical results is if more resources are
devoted to training robust models, effectively identical explanations can be obtained using much
less costly standard gradient-based methods, i.e. IG.
7
Under review as a conference paper at ICLR 2022
Figure 4: Visualizations of attributions for two examples classified by a standard ResNet50.
Figure 5: Visualizations of attributions for two examples classified by a robust ResNet50 (`2 |3.0).
The second example from Fig. 4 is not correctly classified so we replace it with another image.
6	Discussion
Baseline Sensitivity. It is natural to treat that BIG frees users from the baseline selection in ex-
plaining non-linear classifiers. Empirical evidence has shown that IG is sensitive to the baseline
inputs (Sturmfels et al., 2020). We compare BIG with IG when using different baseline inputs,
white or black images. We show an example in Fig 6b. For the first two images, when using the
baseline input as the opposite color of the dog, more pixels on dogs receive non-zero attribution
scores. Whereas backgrounds always receive more attribution scores when the baseline input has
the same color as the dog. This is because gIG(x)i 8 (X - Xb)i (See Def. 2) that greater differences
in the input feature and the baseline feature can lead to high attribution scores. The third example
further questions the readers using different baselines in IG whether the network is using the white
dog to predict Labrador retriever. We demonstrate that conflicts in IG caused by the sensi-
tivity to the baseline selection can be resolved by BIG. BIG shows that black dog in the last row is
more important for predicting Labrador retriever and this conclusion is further validated by
our counterfactual experiment in Appendix D. Overall, the above discussion highlights that BIG is
significantly better than IG in reducing the non-necessary sensitivity in the baseline selection.
Limitations. We identify two limitations of the work. 1) Bounding boxes are not perfect ground-
truth knowledge for attributions. In fact, we find a lot of examples where the bounding boxes either
fail to capture all relevant objects or are too big to capture relevant features only. Fixing mislabeled
bounding boxes still remain an open question and should benefit more expandability research in gen-
eral. 2) Our analysis only targets on attributions that are based on end-to-end gradient computations.
That is, we are not able to directly characterize the behavior of perturbation-based approaches, i.e.
Mask (Fong & Vedaldi, 2017), and activation-based approaches, i.e. GradCAM (Selvaraju et al.,
2017) and Feature Visualization (Olah et al., 2017).
7	Related Work
Ilyas et al. (2019) shows an alternative way of explaining why robust models are more interpretable
by showing robust models usually learn robust and relevant features, whereas our work serves as a
geometrical explanation to the same empirical findings in using attributions to explain deep models.
Our analysis suggests we need to capture decision boundaries in order to better explain classifiers,
8
Under review as a conference paper at ICLR 2022
Properties	black IG	AGI	BIG
Boundary-based	X	✓	^^✓
Boundary Search	N/A	PGD	Any
Geometry	Global	Local	Local
Symmetry	✓	X	✓
Completeness	✓	✓	✓
(a)
&/0-.1.
Lab6a%76
6.-61.8.6
5∕26∕%76
6.-61.8.6
20/34 () *hi-e ()	'()
(b)
Figure 6: (a): Qualitative comparisons between IG with black baseline, BIG and AGI. BIG can use
any boundary search approaches or an ensemble of them while AGI uses PGD only. AGI fails to
meet the symmetry axiom (Sundararajan et al., 2017) where BIG satisfies all axioms that IG satisfies,
i.e. completeness. (b): Comparisons of IG with black and white baselines with BIG. Predictions are
shown in the first column.
whereas a similar line of work, AGI (Pan et al., 2021) that also involves computations of adversarial
examples is motivated to find a non-linear path that is linear in the representation space instead of the
input space compared to IG. Therefore, AGI uses PGD to find the adversarial example and aggre-
gates gradients on the non-linear path generated by the PGD search. We notice that the trajectory of
PGD search is usually extremely non-linear, complex and does not guarantee to return closer adver-
sarial examples without CW or AutoPGD (see comparisons between boundary search approaches
in Table B.2). We understand that finding the exact closest decision boundary is not feasible, but
our empirical results suggest that the linear path (BIG) returns visually sharp and quantitative better
results in localizing relevant features. Besides, a non-linear path should cause AGI fail to meet the
symmetry axiom (Sundararajan et al., 2017) (see Appendix C for an example of the importance of
symmetry for attributions). We further summarize the commons and differences in Table 6a.
In the evaluation of the proposed methods, we choose metrics related to bounding box over other
metrics because for classification we are interested in whether the network associate relevant features
with the label while other metrics (Adebayo et al., 2018; Ancona et al., 2017; Samek et al., 2016;
Wang et al., 2020b; Yeh et al., 2019), e.g. infidelity (Yeh et al., 2019), mainly evaluates whether
output scores are faithfully attributed to each feature. Our idea of incorporating boundaries into
explanations may generalize to other score attribution methods, e.g. Distributional Influence (Leino
et al., 2018) and DeepLIFT (Shrikumar et al., 2017). The idea of using boundaries in the explanation
has also been explored by T-CAV (Kim et al., 2018), where a linear decision boundary is learned for
the internal activations and associated with their proposed notion of concept.
When viewing our work as using nearby boundaries as a way of exploring the local geometry of the
model’s output surface, a related line of work is NeighborhoodSHAP (Ghalebikesabi et al., 2021), a
local version of SHAP (Lundberg & Lee, 2017). When viewing our as a different use of adversarial
examples, some other work focuses on counterfactual examples (semantically meaningful adver-
sarial examples) on the data manifold (Chang et al., 2019; Dhurandhar et al., 2018; Goyal et al.,
2019).
8	Conclusion
In summary, we rethink the target question an explanation should answer for a classification task,
the important features that the classifier uses to place the input into a specific side of the decision
boundary. We find the answer to our question relates to the normal vectors of decision boundaries
in the neighborhood and propose BSM and BIG as boundary attribution approaches. Empirical
evaluations on STOA classifiers validate that our approaches provide more concentrated, sharper
and more accurate explanations than existing approaches. Our idea of leveraging boundaries to
explain classifiers connects explanations with the adversarial robustness and help to encourage the
community to improve model quality for explanation quality.
9
Under review as a conference paper at ICLR 2022
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, 2018.
Julius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. Debugging tests for model expla-
nations, 2020.
Gunjan Aggarwal, Abhishek Sinha, Nupur Kumari, and Mayank Kumar Singh. On the benefits of
models with perceptually-aligned gradients. ArXiv, abs/2005.01499, 2020.
Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding of
gradient-based attribution methods for deep neural networks, 2017.
Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding
of gradient-based attribution methods for deep neural networks. In International Conference on
Learning Representations, 2018.
Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, Klaus-Robert Muller, and Wojciech
Samek. Layer-wise relevance propagation for neural networks with local renormalization layers.
In International Conference on Artificial Neural Networks, pp. 63-71. Springer, 2016.
Nicholas Carlini and D. Wagner. Towards evaluating the robustness of neural networks. 2017 IEEE
Symposium on Security and Privacy (SP), pp. 39-57, 2017.
Prasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Xi Wu, and Somesh Jha. Concise expla-
nations of neural networks using adversarial training. In International Conference on Machine
Learning, pp. 1383-1391. PMLR, 2020.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and D. Duvenaud. Explaining image classifiers
by counterfactual generation. In ICLR, 2019.
Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-
cam++: Generalized gradient-based visual explanations for deep convolutional networks. arXiv
preprint arXiv:1710.11063, 2017.
Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha. Robust attribution regular-
ization. In Advances in Neural Information Processing Systems, 2019.
Junsuk Choe and Hyunjung Shim. Attention-based dropout layer for weakly supervised object lo-
calization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 2219-2228, 2019.
Jeremy M. Cohen, Elan Rosenfeld, and J. Z. Kolter. Certified adversarial robustness via randomized
smoothing. In ICML, 2019.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020.
Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu net-
works via maximization of linear regions. AISTATS 2019, 2019.
Kedar Dhamdhere, Mukund Sundararajan, and Qiqi Yan. How important is a neuron. In Interna-
tional Conference on Learning Representations, 2019. URL https://openreview.net/
forum?id=SylKoo0cKm.
A. Dhurandhar, P. Chen, Ronny Luss, Chun-Chen Tu, Pai-Shun Ting, Karthikeyan Shanmugam, and
Payel Das. Explanations based on the missing: Towards contrastive explanations with pertinent
negatives. In NeurIPS, 2018.
Ann-Kathrin Dombrowski, M. Alber, Christopher J. Anders, Marcel Ackermann, K. Muller, and
P. Kessel. Explanations can be manipulated and geometry is to blame. In NeurIPS, 2019.
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library), 2019. URL https://github.com/MadryLab/robustness.
10
Under review as a conference paper at ICLR 2022
Christian Etmann, Sebastian Lunz, Peter Maass, and Carola Schoenlieb. On the connection between
adversarial robustness and saliency map interpretability. In Proceedings of the 36th International
Conference on Machine Learning, 2019.
R. C. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation.
In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 3449-3457, 2017. doi:
10.1109/ICCV.2017.371.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful pertur-
bation. In Proceedings of the IEEE international conference on computer vision, pp. 3429-3437,
2017.
Aymeric Fromherz, Klas Leino, Matt Fredrikson, Bryan Parno, and Corina Pasareanu. Fast ge-
ometric projections for local robustness certification. In International Conference on Learning
Representations (ICLR), 2021.
Sahra Ghalebikesabi, Lucile Ter-Minassian, Karla Diaz-Ordaz, and Chris C. Holmes. On locality of
local explanation models. arxiv, abs/2106.14648, 2021.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.
Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual
explanations. In Proceedings of the 36th International Conference on Machine Learning, pp.
2376-2384, 2019.
Jeremy Howard. imagenette. URL https://github.com/fastai/imagenette/.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, 2019.
Matt Jordan, J. Lewis, and A. Dimakis. Provable certificates for adversarial examples: Fitting a ball
in the union of polytopes. In NeurIPS, 2019.
Been Kim, M. Wattenberg, J. Gilmer, C. J. Cai, James Wexler, F. Viegas, and Rory Sayres. Inter-
pretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).
In ICML, 2018.
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan
Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-
Richardson. Captum: A unified and generic model interpretability library for pytorch, 2020.
J. Z. Kolter and E. Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In ICML, 2018.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/~kriz/cifar.html.
Sungyoon Lee, Jaewook Lee, and Saerom Park. Lipschitz-certifiable training with a tight outer
bound. Advances in Neural Information Processing Systems, 33, 2020.
Klas Leino, Shayak Sen, Anupam Datta, Matt Fredrikson, and Linyi Li. Influence-directed expla-
nations for deep convolutional networks. In 2018 IEEE International Test Conference (ITC), pp.
1-8. IEEE, 2018.
Klas Leino, Ricardo Shih, Matt Fredrikson, Jennifer She, Zifan Wang, Caleb Lu, Shayak Sen, Di-
vya Gopinath, and , Anupam. truera/trulens: Trulens, 2021a. URL https://zenodo.org/
record/4495856.
Klas Leino, Zifan Wang, and Matt Fredrikson. Globally-robust neural networks, 2021b.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceed-
ings of the 31st international conference on neural information processing systems, pp. 4768-
4777, 2017.
11
Under review as a conference paper at ICLR 2022
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Gregoire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek, and Klaus-Robert Muller.
Explaining nonlinear classification decisions with deep taylor decomposition, 2015.
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017. doi:
10.23915/distill.00007. https://distill.pub/2017/feature-visualization.
Deng Pan, Xin Li, and Dongxiao Zhu. Explaining deep neural network models with adversarial
gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI),
2021.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark
the robustness of machine learning models. In Reliable Machine Learning in the Wild Workshop,
34th International Conference on Machine Learning, 2017. URL http://arxiv.org/abs/
1707.04131.
Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast
adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensor-
flow, and jax. Journal of Open Source Software, 5(53):2607, 2020. doi: 10.21105/joss.02607.
URL https://doi.org/10.21105/joss.02607.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007∕s11263-015-0816-y.
Wojciech Samek, Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, and Klaus-Robert
Muller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions
on neural networks and learning systems, 28(11):2660-2673, 2016.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International Conference on Machine Learning, pp. 3145-
3153. PMLR, 2017.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps, 2013.
Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional
robustness with principled adversarial training, 2020.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net, 2014.
Pascal Sturmfels, Scott Lundberg, and Su-In Lee. Visualizing the impact of feature attribution
baselines. Distill, 2020. doi: 10.23915/distill.00022. https://distill.pub/2020/attribution-baselines.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319-
3328. JMLR. org, 2017.
Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations, 2019.
12
Under review as a conference paper at ICLR 2022
Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and
Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Work-
shops,pp. 24-25, 2020a.
Zifan Wang, Piotr Mardziel, Anupam Datta, and Matt Fredrikson. Interpreting interpretations: Orga-
nizing attribution methods by criteria. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, pp. 10-11, 2020b.
Zifan Wang, Haofan Wang, Shakul Ramkumar, Piotr Mardziel, Matt Fredrikson, and Anupam Datta.
Smoothed geometry for robust attribution. InH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 13623-
13634. Curran Associates, Inc., 2020c.
Tsui-Wei Weng, Huan Zhang, H. Chen, Zhao Song, C. Hsieh, D. Boning, I. Dhillon, and L. Daniel.
Towards fast computation of certified robustness for relu networks. In ICML, 2018.
Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Sai Suggala, David I Inouye, and Pradeep Ravikumar. On
the (in) fidelity and sensitivity for explanations. In Advances in Neural Information Processing
Systems, 2019.
13
Under review as a conference paper at ICLR 2022
A	Theorems and Proofs
A.1 Proof of Proposition 1
Proposition 1 Suppose that f has a (λ, δ)-robust saliency map gS at x, x0 is the closest point on
the closest decision boundary segment to x and ||x0 - x|| ≤ δ, and that n is the normal vector of
that boundary segment. Then ||n 一 gS(x)|| ≤ λ∣∣x - x0||.
To compute n can be efficiently computed by taking the derivatice of the model’s output w.r.t to
the point that is on the decision boundary such that n = d∂(χ0) and ∀xm, ∈ Rd, F(Xm) = F(x) if
||xm - x|| ≤ ||x0 - x||.
Because we assume ||X - X0|| ≤ δ, and the model has (λ, δ)-robust Saliency Map, then by Def. 4
we have
||n - gS(X)|| ≤ λllx - x0l1
A.2 Proof of Theorem 1
Theorem 1 Let m(X) = ReLU (W X) be a one-layer network and when using randomized
smoothing, we write mσ (X). Let g(X) be the SM for mσ (X) and suppose ∀X00 ∈ B(X, ||X -
X0||), ||g(X00)|| ≥ c where X0 is the closest adversarial example, we have the following statement
holds: ||g(X) - g(X0)|| ≤ λ where λ is monotonically decreasing w.r.t σ.
Proof:
We begin our proof by firstly introducing Randomized Smoothing.
Definition 7 (Randomized Smoothing (Cohen et al., 2019)) Suppose F(X) = arg maxc fc(X),
the smoothed classifier G(X) is defined as
G(X) := arg max Pr [F(X + ) = c]	(1)
c
where e 〜 N (0, σ2I)
Now the rest of the proof of is three-fold: 1) firstly we will show that there exist a non-linear activa-
tion function Er(X) such that the output of the smoothed ReLU network mσ (X) is equivalent when
replacing the ReLU activation with Er activation; 2) secondly derive the difference between the
saliency map of the network with Er activation; and 3) lastly, we show that the difference between
SM and BSM of the network with Er activation is bounded, which is inversely proportional to the
standard deviation used to create the smoothed ReLU network mσ (X).
(1) Step I: Error activation (Er) function and randomized smoothing. 1
Randomized Smoothing creates a smoothed model that returns whichever the label that the base
classifier most likely to return under the perturbation generated by the Gaussian noise. Now we take
a look at the output of each class under the Gaussian noise. Consider yi is the output of the i-th class
of the network ReLU(W X), that is
yi = Ee 〜N (0,σ2i)ReLU(w> (x + e))	(2)
To simplify the notation, We denote Ee〜N(o,σ2i) as E. We expand Equation (2):
yi = E ReLU(wi>x + wi>e) = E [ReLU(u + e0)]	(3)
Where We denote u = wi> x and e0 = wi> e. u is a scalar and e0 folloWs a zero-centered univariate
Gaussian with the standard deviation S α σ because the dot production between the constant weight
vector wi and the random vector e can be seen as a linear combination of each dimension ofe and the
covariance between each dimension of e is 0 for the Gaussian noise used for randomized smoothing
1We appreciate the discussion with the author Pan Kessel of Dombrowski et al. (2019) for the derivations
from Equation (6) to (7)
14
Under review as a conference paper at ICLR 2022
in the literature (Cohen et al., 2019). By expanding the expectation symbol to its integral form, we
obtain:
yi
∞
exp(-
∞
02
「)ReLU(u + e0)de0
2s2
(4)
Let τ = u+	0 and notice that ReLU (τ) = 0 if τ < 0, the equation above can be rewritten as:
1	∞	(τ u)2
yi = S√2Π L exp(--2^~)τdτ	⑸
=√1- exp(-K)s + U [1 + Erf(√=-R	⑹
2π	2s2	2	2s
(7)
where Erf is the error function defined as Erf(X) = √∏ ʃX exp(-t2)dt. We therefore define an Er
activation for an input u with the standard deviation s as
1	u2	u
Er(M S) = √∏ exp(-率)s + 2
1+ Erf(
(8)
and we show that
yi = Ee〜N(0,σ2i) [ReLU(w>(X + e))] = Er(w>x; s(σ))	(9)
That is, to analyze the gradient of the output for a smoothed model w.r.t the input, we can alterna-
tively analyze the gradient of an equivalent Er network. We plot three examples of the Er activations
in Fig. 7 for the readers to see what does the function look like.
2)	Step II: the Saliency Map for an Er network.
By the definition of Saliency map (Def. 1) and the chain rule, we have:
∂yi	∂yi ∂u
SM(x) =	= yτTΓ (Let u = w>x)	(10)
∂x	∂u ∂x
∂
=^-(Er(u; s)) ∙ Wi	(11)
∂u
• Wi	(12)
2 1+ Erf(
The transition between Equation (11) to (12) is based on the fact that the derivative of Erf(x) is
√2∏ eχp(-x2).
3)	Step III: the difference between SM and BSM for an Er network.
Let x0 be the closest point on the decision boundary for the smoothed classifier mσ and ||x-x0|| = r
(for the closed-form expression of r, see Cohen et al. (2019)). Based on the definition of BSM, we
have
∂yi(x0)	1
BSM(X) = -yiχ-2 =2
The difference between SM and BSM therefore is computed as
u0 = Wi>x0
(13)
15
Under review as a conference paper at ICLR 2022
Figure 7: The graph of Er(u; s) w.r.t different standard deviations s.
||BSM(x) - SM(x)||
=||2
=2IE
1
≤ —
一2
(Triangle Inequality)
(14)
(15)
(16)
We notice that the u0 is bounded because u0 = wi>x0 ≤ llwil1 ∙ ||x0|| ≤ lwiM∙ (IIWi|| + r) and
similarly for U such that U = w>X ≤ ∣∣w∕∣∙ (∣∣x∣∣ + r). BecauseErf function is increasing w.r.tthe
input and s > 0, we arrive at the following inequality:
IIBSM(X) - SM(X)II ≤ λ	(17)
where
λ = Erf( "wi1*" + r)) ∙∣∣Wi∣∣	(18)
2s
We take the absolute symbol out because the output of an Erf is positive when its input is posi-
tive. Now, given that IIwi II, r and IIXII are constants when for a given input X, the upper-bound
Erf(llwill∙√2χll+r)) ∙ IIWi∣∣ is monotonically increasing as S decreases. From the Step I, We know
that s α σ, therefore we prove there exist an upper-bound λ of the difference between the SM and
BSM for a smoothed classifier and λ is monotonically decreasing w.r.t the standard deviation of the
Gaussian noise.
B	Experiment Details and Additional Results
B.1 Metrics with Bounding Boxes
We will use the following extra notations in this section. Let X , Z and U be a set of indices of all
pixels, a set of indices of pixels with positive attributions, and a set of indices of pixels inside the
bounding box for a target attribution map g(X). We denote the cardinality of a set S as ISI.
Localization (Loc.) (Chattopadhyay et al., 2017) evaluates the intersection of areas with the
bounding box and pixels with positive attributions.
Definition 8 (Localization) For a given attribution map g(X), the localization score (Loc.) is de-
fined as
LL)O := IUI + IZ ∩ (X \ U)I	(19)
16
Under review as a conference paper at ICLR 2022
Localization (Loc.)
Energy Game (GE)
Figure 8: Localizaiton performance for attributions on a standard ResNet
Energy Game (EG) (Wang et al., 2020a) instead evaluates computes the portion of attribute
scores within the bounding box.
Definition 9 (Energy Game) For a given attribution map g(x), the energy game EG is defined as
EG :=	∑2i∈z∩u g(x)i
' Pi∈X max(g(x)i, 0)
(20)
Positive Percentage (PP) evaluates the sum of positive attribute scores over the total (absolute
value of) attribute scores within the bounding box.
Definition 10 (Positive Percentage) Let V be a set of indices pf all pixels with negative attribution
scores, for a given attribution map g(x), the positive percentage PP is defined as
PP :
______Ei∈Z∩U g(X)i_____
Pi∈Z∩U g(X)i - Pi∈V∩U g(X)i
(21)
Concentration (Con.) evaluates the sum of weighted distances by the “mass” between the “mass”
center of attributions and each pixel within the bounding box. Notice that the computation of cx and
Cy can be computed with scipy.ndimage.center_of _mass. This definition encourages that
pixels with high absolute value of attribution scores to be closer to the mass center.
Definition 11 (Concentration) Fora given attribution map g(X), the concentration Con. is defined
as follws
Con. := X g(X)/ J(ix - cx)2 + (iy - Cy)2
i∈U
(22)
where g is the normalized attribution map so that gi = gi/ 52i∈u |gi |∙ iχ, iy are the coordinates of
the pixel and
i∈U ixg(X)	= ∑i∈u iyg(x)i
Pi∈u ^(x)i ,cy= Pi∈u g(X)i
(23)
Besides metrics related to bounding boxes, there are other metrics in the literature used to evaluate
attribution methods (Adebayo et al., 2018; Ancona et al., 2017; Samek et al., 2016; Wang et al.,
2020b; Yeh et al., 2019). We focus on metrics that use provided bounding boxes, as we believe that
they offer a clear distinction between likely relevant features and irrelevant ones.
17
Under review as a conference paper at ICLR 2022
Localization (Loc.)
1.0
AGI
SM
Gl
SG
BSM
IG DeepLlFT
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
BIG
Energy Game (GE)
Positive Percentage (PP)
1.0-
0.9-
0.8
0.7
0.6
0.5
AGI
SM
Gl
SG
BSM
IG DeepLlFT
°-4 BiG
0.8
0.6
而
百
^SG^
Concentration (Con.)
IG DeepLIFT
BSM
DeepLIFT
Figure 9: Localizaiton performance for attributions on a robust ResNet ('213.0)
PipeIine	Avg Distance	Success Rate
(ImageNet) Standard ResNet50		
PGDs	0.549	72.1%
+ CW	0.548	72.1%
	+ AutoPGD	0.548	72.1%
(ImageNet) Robust ResNet50 ('213.0)		
PGDs	2.870	74.1%
+ CW	2.617	74.1%
	+ AutoPGD	2.617	74.1%
(ImageNet) Robust ResNet50 ('∞ ∣4∕255)		
PGDs	2.385	98.9%
+ CW	2.058	98.9%
	+ AutoPGD	2.058	98.9%
(ImageNet) Robust ResNet50 ('∞∣8∕255)		
PGDs	2.378	99.1%
+ CW	1.949	99.1%
	+ AutoPGD	1.949	99.1%
	(CIFAR-10) Standard ResNet50			
PGDs	0.412	98.7%
+ CW	0.120	98.7%
	+ AutoPGD	0.120	98.7%
	(CIFAR-10) Robust ResNet50 ('2∣0.5)			
PGDs	1.288	99.9%
+ CW	1.096	99.9%
	+ AutoPGD	1.096	99.9%
(a)
CIFAR10	standard	robust
	0.5	1.0
topk	10	10
max iters	15	15
ImageNet	standard	robust
	2.0	6.0
topk	15	15
max iters	15	15
(b)
Figure 10: (a): Pipeline: the methods used for boundary search. Avg Distance: the average `2
distance between the input to the boundary. Success Rate: the percentage when the pipeline returns
an adversarial example. Time: per-instance time with a batch size of 64. We are using much bigger
s for robust models, so the success rates are higher than a standard model. (b): Hyper-parameters
used for AGI. We use the default parameteres from the authors’ implementation for ImageNet and
make minimal changes for CIFAR-10.
B.2	Implementing B oundary Search
Our boundary search uses a pipeline of PGDs, CW and AutoPGD. Adversarial examples returned
by each method are compared with others and closer ones are returned. If an adversarial example is
not found, the pipeline will return the point from the last iteration of the first method (PGDs in our
case). Hyper-parameters for each attack can be found in Table 2. The implementation of PGDs and
CW are based on Foolbox (Rauber et al., 2020; 2017) and the implementation of AutoPGD is based
18
Under review as a conference paper at ICLR 2022
	CIFAR10	standard	robust
	Es	[0.2,0.4, 0.6,0.8,1.0]	[0.25, 0.5, 1.0, 1.5, 2.0]
	max steps	100	100
	step size	5e-3	5e-3
PGDs	ImageNet	standard	robust
	Es	[36/255., 64/255., 0.3, 0.5,0.7, 0.9,1.1]	[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
	max steps	100	100
	step size	adaptive	adaptive
	CIFAR10	standard	robust
	E	1.0	2.0
	max steps	100	100
	step size	1e-3	1e-3
CW	ImageNet	standard	robust
	E	1.0	6.0
	max steps	100	100
	step size	1e-2	5e-2
	CIFAR10	standard	robust
	E	1.0	2.0
	max steps	100	100
	step size	6e-3	1.6e-2
AutoPGD	ImageNet	standard	robust
	E	1.1	6.0
	max steps	100	100
	step size	2.3e-2	1.2e-1
Table 2: Hyper-parameters used for adversarial attacks. adaptive means the actual step size is
determined by 2 * E / max steps.
on the authors’ public repository2 (we only use apgd-ce and apgd-dlr losses for efficiency
reasons). All computations are done using a GPU accelerator Titan RTX with a memory size of 24
GB. Comparisons on the results of the ensemble of these three approaches are shown in Fig. 10a.
B.3	Hyper-parameters for Attribution Methods
All attributions are implemented with Captum (Kokhlikyan et al., 2020) and visualized with Tru-
lens (Leino et al., 2021a). For BIG and IG, we use 20 intermediate points between the baseline
and the input and the interpolation method is set to riemann_trapezoid. For AGL We base on
the authors’ public repository3. The choice of hyper-paramters follow the default choice from the
authors for ImageNet and We make minimal changes to adapt them to CIFAR-10 (see Fig. 10b).
To visualize the attribution map, We use the HeatmapVisualizer With blur=10,
normalization_type="signed_max" and default values for other keyword arguments from
Trulens.
B.4	Detailed Results on Localization Metrics
We show the average scores for each localizaiton metrics in Sec. 5. We also show the boxplots of
the scores for each localization metrics in Fig. 8 for the standard ResNet50 model and Fig. 9 for the
robust ResNet50 ('213.0). All higher scores are better results.
2https://github.com/fra31/auto-attack
3https://github.com/pd90506/AGI
19
Under review as a conference paper at ICLR 2022
0 5 0 5
LLNN
- - - -
6O- C-① UUEP
τ^π^i


Figure 11: `2 distances in logarithm between SG and BSG against different standard deviations σ of
the Gaussian noise. Results are computed on ResNet50.
black IG white IG BIG
Figure 12: Full results of Fig. 6b in Sec. 6. For the third, fourth and fifth example, we compute
the attribution scores towards the prediction of the third example, Labrador retriever. IG
with black or white attributions show that masked area contribute a lot to the prediction while BIG
“accurately” locate the relevant features in the image with the network’s prediction.
B.5	Additional Comparison with AGI
We additionally compare the localization ability of relevant features between BIG and AGI if we
only use PGDs to return closest boundary points, that is we recursively increase the norm bound
and perform PGD attack until the first time we succeed to find an adversarial point. We denote this
approach as BIGp. Note that BIGp is still different from AGI by the type of the path, i.e. lines
20
Under review as a conference paper at ICLR 2022
Model	Metrics	BIG	BIGp	AGI	IG
	Loc.	0.38	0.38	0.33	0.34
standard	EG	0.54	0.54	0.48	0.5
	PP	0.87	0.87	0.58	0.51
	Con.	4.35	4.35	4.01	3.97
	Loc.	0.39	0.39	0.39	0.33
'2∣3.0	EG	0.74	0.76	0.64	0.65
	PP	0.92	0.96	0.88	0.65
	Con.	5.03	5.10	4.32	4.37
Table 3: Comparisons among BIG, BIGp (BIG using PGD only to run the boundary search), AGI
and IG.
and curves, over which the integral is performed. That is AGI also aggregates the path integral
starting from a set of adversarial points found by the targeted PGD attack, where BIGp starts from
the adversarial pointed returned by untargeted PGD attack. We use the same parameters for both
PGD and AGI from Fig. 2 and we run the experiments over the same dataset used in Sec. 5.1. For
reference, we also include the results of IG. The results are shown in Table. 3. We notice that after
removing CW and AutoPGD, BIGp actually performs better than AGI, and even slightly better than
BIG for the robust model. One reason to explain the tiny improvement from BIGp might be that
for a robust network, the gradient at each iteration of the PGD attack is more informative and less
noisy compared to a standard model so that the attack can better approximate the closest decision
boundary. The results in Table. 3 therefore demonstrates that BIG and BIGp are able to localize
relevant features better than AGI.
B.6	Additional Localization Metric
Besides the localization metrics used in Sec. 5.1, we discuss an additional localization metric fre-
quently used for evaluating attention and CAM-based explanations: Top1-Loc Choe & Shim (2019);
Aggarwal et al. (2020). Top1-Loc is calculated as follows: an instance is considered as Top1-Loc
correct given an attribution if 1) the prediction is Top1-correct; and 2) GT-Loc correct - namely,
the IoU of the ground-truth bounding box and area highlighted by the attribution is more than 50
%. When only using the images that are Top1-correct, Top1-Loc reduces to GT-Loc. Top1-Loc is
different from other localization metrics used for evaluating attribution methods because it takes the
prediction behavior of the target model into the account, which in general is not an axiom when
motivating a gradient-based attribution method. In the previous evaluations, we are only interested
in images that the model makes correct Top1 predictions, in this section we will use the same images
that are true-positives. In this case, Top1-Loc accuracy reduces to GT-Loc accuracy, and so we mea-
sure the GT-Loc directly. To determine the which part of the image is highlighted by the attribution,
we compute a threshold for each attribution map and a pixel is considered within the highlight region
if and only if the attribution score is higher than the threshold. For a given attribution map, we con-
sider a threshold value t as the q-th percentile for the absolute values of attribution scores. We plot
the GT-Loc accuracy against q in Fig. 13. We notice that attention-based and CAM-based attribu-
tions usually produce a cloud-like visualization because of the blurring technique or upsample layers
used to compute the results. To ensure GT-Loc will work from gradient-based attributions we are
interested in this paper, we also include results (Fig. 14) where we apply a Gaussian Blur (σ = 3.0)
to the attribution map first before calculating the GT-Loc accuracy. The results are aggreated over
1500 images from ImageNette on a standard ResNet50 and a robust ResNet50, respectively. Higher
GT-Loc scores are better.
Behavior of BIG. The results in Fig. 13 and 14 show that BIG is better than all other attributions
on standard models excluding SG and uniformly better including SG on a robust model. Before we
provide some explanations about the behaviors of SG (green curves) on standard models in the next
paragraph, we also observe that: 1) blurring only changes the GT-Loc scores but not the overral
rankings across attributions; 2) a threshold corresponding to a percentile near 40% provides the best
GT-Loc scores for all methods; 3) gradient-based attributions generally provide worse GT-Loc (or
Top1-Loc) scores compared to CAM-based and attention-based approaches in the literature Choe &
21
Under review as a conference paper at ICLR 2022
Shim (2019); Aggarwal et al. (2020), which is not surprising because gradient-based approaches are
usually axiomatically-justified to be faithful to the model. Therefore, it is expected that the model
will more or less learn spurious features from the input, which makes the gradient-based attributions
noisy than attention and CAM-based ones. Therefore, when localizing relevant features, users may
want to consult activation-based approaches, i.e. CAMs, but when debugging and ensuring the net-
work learns less spurious and irrelevant features, users should instead use gradient-based approaches
because of the axioms behind these approaches.
Behavior of SG in Standard Models. SG is uniformly better than all other approaches in terms
of the Gt-Loc accuracies on a standard model, which is surprising but not totally unexpected. We
beleive the reason behind this result is that, SG is actually the gradient from a smoothed counterpart
of the standard model (see discussions near Theorem 1), which is more robust. Therefore, it does
not seem to be an apple-to-apple comparison between SG and other approaches because it can be
less faithful to the standard model - namely SG is more faithful to the smoothed classifier. That is
very likely why SG is worse than BIG in Fig. 13b and 14b when the smoothing technique becomes
marginal for improving the robustness for a model that has already been robustly trained.
B.7	Sanity Check for BIG
We perform Sanity Checks for BIG using Rank Order Correlations between the absolute values
of BIGs when randomizing the weights from the top layer to the bottom (Adebayo et al., 2018).
To ensure the output of the model does not become NaN, when randomizing the weights of each
trainable layer, we ensure that we replace the weight matrix with a random matrix with the same
norm as follows.
1 def _randomized_models():
all_Parameters =[]
3 for param in model.parameters():
all_Parameters.append(param)
5	for step, param in enumerate(all_parameters[::-1]):
6	random_w = torch.randn_like(param)
7	## We make sure the randomized weights have the same norm to
prevent the network to output nan results
8	param.data = torch.nn.parameter.Parameter(
9	random_w * torch.norm(param.data) / torch.norm(random_w.data))
10	if step % num_blocks == 0 or step == len(all_Parameters):
11	yield model
12
For each iteration, we continuously replace randomized 5 layers in the reversed sequence returned
by model.parameters() and the results are plotted in Fig. 15. We consider BIG passes the
sanity check as the results are similar compared with the top row of Fig 4 in Adebayo et al. (2018).
B.8 Additional Experiment with Smoothed Gradient
Theorem 1 demonstrates that for a one-layer network, as we increase the standard deviation σ of the
Gaussian distribution used for creating the smoothed model mσ (Cohen et al., 2019), the difference
between the saliency map and the boundary-based saliency map computed in mσ is bounded by a
constant λ, which is monotonically decreasing w.r.t σ. That is, greater σ will produce a smoothed
model, where the saliency map (SM) explanation of mσ is a good approximation for the boundary-
based saliency map (BSM). However, as the depth of the deep network increases, a closed-form
analysis may be difficult to derive. Therefore, in this section, we aim to empirically validate that the
take-away from Theorem 1 should generalize to deeper networks.
Computing SM for mσ. One practical issue to compute any gradient-related explanations for the
smoothed model mσ is that mσ is defined in an integral form, which can not be directly built with
tf.keras. However, Theorem 2 shows that the smoothed gradient of the original model m is
equivalent to the saliency map of the smoothed model mσ . Namely, the order of smoothing and
integral is exchangeable when computing the gradient.
22
Under review as a conference paper at ICLR 2022
50505050
33'2'211QQ
Oooooooo
U3⅛
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
Threshold
(a) standard
-2A
O O
US.1
——SM
BSM
——SG
——IG
Gl
——DL
AGl
→- BIG
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
Threshold
(b) robust (`2 |0.3)
4∙3∙2,∙0
Ooooo
。。1
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
Threshold
(a) standard
Figure 14: GT-Loc scores for different attributions when applying a Gaussian blur kernel (σ = 3.0)
to the attribution maps before thresholding the attribution maps.
Figure 13: GT-Loc scores for different attributions. GT-Loc measures the portion of instances where
the IoUs of between the groudtruth bounding box and the bounding box generated by thresholded
the attributions are greater than 0.5. The x-axis is the percentile used to threshold an attribution map
to determine the highlighted area and y-axis is the GT-Loc score aggregated over all the instances.
2 2 IO
UoTIo
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
Threshold
(b) robust (`2 |0.3)
Theorem 2 (Proposition 1 from Wang et al. (2020c)) Suppose a model f(x) satisfies
max |f (x)| < ∞. For Smoothed Gradient gSG(x), we have
/ ʌ	d(f ~ q)(x)
gSG (X) = -----∂X-----	(24)
where q(x) = N(0, σ2I) and ~ denotes the convolution operation.
Computing BSM for mσ. Another practical issue is computing the decision boundary for a
smoothed model mσ is not defined in a deterministic way as randomized smoothing provides a prob-
abilistic guarantee. In this paper, we do the following steps to approximate the decision boundary
of a smoothed model. To generate the adversarial examples for the smoothed classifier of ResNet50
with randomized smoothing, we need to compute back-propagation through the noises. The noise
sampler is usually not accessible to the attacker who wants to fool a model with randomized smooth-
ing. However, our goal in this section is not to reproduce the attack with similar setup in practice,
instead, what we are after is the point on the boundary. We therefore do the noise sampling prior to
run PGD attack, and we use the same noise across all the instances. The steps are listed as follows:
1.	We use numpy.random.randn as the sampler for Gaussian noise with its random seed
set to 2020. We use 50 random noises per instance.
2.	In PGD attack, we aggregate the gradients of all 50 random inputs before we take a regular
step to update the input.
3.	We set e = 3.0 and We run at most 40 iterations with a step size of 2 * e/40.
23
Under review as a conference paper at ICLR 2022
Figure 15: The rank order correlations of the absolute values of BIGs against the number of layers
(counting from top to bottom) where trainable weights are replaced with random matrices.
4.	The early stop criteria for the loop of PGD is that when less than 10% of all randomized
points have the original prediction.
5.	When computing Smooth Gradient for the original points or for the adversarial points, we
use the same random noise that we generated to approximate the smoothed classifier.
Results. We run the experiment with 500 images from ImageNet on ResNet50 as this computation
is significantly more expensive than previous experiments. We compute the `2 distances between
SM and BSM obtained from the steps above for several values as shown in Fig. 11. Notably, the
trend of the log difference against the standard deviation σ used for the Gaussian noise validates
that the qualitative meaning of Theorem 1 holds even for large networks. That is, when the model
becomes more smoothed, saliency map explanation is a good approximation for the boundary-based
saliency map.
C S ymmetry of Attribution Methods
Sundararajan et al. (2017) prove that a linear path is the only path integral that satisifes symmetry;
that is, when two features’ orders are changed for a network that is not using any order information
from the input, their attribution scores should not change. One simple way to show the importance
of symmetry by the following example and we refer Sundararajan et al. (2017) to readers for more
analysis.
Example 1 Consider a function f(x, y) = min(x, y) and to attribute the output of f to the inputs
at x = 1, y = 1 we consider a baseline x = 0, y = 0. An example non-linear path from the
baseline to the input can be (x = 0, y = 0) → (x = 1, y = 0) → (x = 1, y = 1). On this path,
f(x, y) = min(x, y) = y after the point (x = 1, y = 0); therefore, gradient integral will return 0
for the attribution score of x and 1 for y (we ignore the infinitesimal part of (x = 0, y = 0) → (x =
1, y = 0)). Similarly, when choosing a path (x = 0, y = 0) → (x = 0, y = 1) → (x = 1, y = 1),
we find x is more important. Only the linear path will return 1 for both variables in this case.
D	Counterfactual Analysis in the Baseline Selection
The discussion in Sec. 6 shows an example where there are two dogs in the image. IG with black
baseline shows that the body of the white dog is also useful to the model to predict its label and the
black dog is a mix: part of the black dog has positive attributions and the rest is negatively contribute
to the prediction. However, our proposed method BIG clearly shows that the most important part
is the black dog and then comes to the white dog. To validate where the model is actually using
the white dog, we manually remove the black dog or the white dog from the image and see if the
model retain its prediction. The result is shown in Fig. 12. Clearly, when removing the black dog,
the model changes its prediction from Labrador retriever to English foxhound while
removing the white dog does not change the prediction. This result helps to convince the reader that
24
Under review as a conference paper at ICLR 2022
BIG is more reliable than IG with black baseline in this case as a more faithful explanation to the
classification result for this instance.
E	Additional Visualizations for BIG
More visualizations comparing BIG with other attributions can be found in Fig. 16 and 17. We
show several examples in Fig. 18 when there are more than one objects in the input and we explain
the model’s Top1 prediction, where we show that BIG is able to localize the objects that are actually
relevant to the predicted label.
25
Under review as a conference paper at ICLR 2022
SM	GTI	IG	SG	DeepLIFT	AGI	BIG
Figure 16: Visualizations of different attributions for a standard ResNet50
26
Under review as a conference paper at ICLR 2022
Figure 17: Visualizations of different attributions for a robust ('213.0) ResNet50
27
Under review as a conference paper at ICLR 2022

SM
GTI
IG
SG
DeepLIFT
AGI
BIG

rock python


basketball
ʃ 11 *
ESkimO dog
1

jacamar
■

Figure 18: Visualizations of different attributions for a standard ResNet50 where there are usually
more than one objects in the input. We also label each input with the Top 1 prediction made by the
classifier.
28