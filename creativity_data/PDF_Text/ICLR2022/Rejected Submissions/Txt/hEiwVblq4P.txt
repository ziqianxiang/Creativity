Under review as a conference paper at ICLR 2022
Proper Straight Through Estimator: Break-
ING SYMMETRY PROMOTES CONVERGENCE TO TRUE
MINIMUM
Anonymous authors
Paper under double-blind review
Ab stract
In the quantized network, its gradient shows vanishing except for non-
differentiable points. The network thus cannot be learned by the standard back-
propagation, so that an alternative approach called Straight Through Estimator
(STE), which replaces the part of the gradient with a simple differentiable func-
tion, is used. While STE is known to work well for learning the quantized network
empirically, it has not been established theoretically. A recent study by Yin et al.
(2019) has provided theoretical support for STE. However, its justification is still
limited to the model in the one-hidden layer network with the binary activation
where Gaussian generates the input data, and the true labels are output from the
teacher network with the same binary network architecture. In this paper, we dis-
cuss the effectiveness of STEs in more general situations without assuming the
shape of the input distribution and the labels. By considering the scale symmetry
of the network and specific properties of the STEs, we find that STE with clipped
Relu is superior to STEs with identity function and vanilla Relu. The clipped
Relu STE, which breaks the scale symmetry, may pick up one of the local minima
degenerated in scales, while the identity STE and vanilla Relu STE, which keep
the scale symmetry, may not pick it up. To confirm this observation, we further
present an analysis of a simple misspecified model as an example. We find that
all the stationary points are identical with the vanishing points of the cRelu STE
gradient, while some of them are not identical with the vanishing points of the
identity and Relu STE. Finally we have numerically confirmed the observation
for the mixture Gaussian model with various teacher network.
1	Introduction
Quantization of the weights and the activations is a promising technique to save the memory and
accelerate the inference speed in deep neural networks (DNNs) which have been getting wider and
deeper in recent years. There are two main approaches of the quantization for DNNs (Krishnamoor-
thi (2018)): Post-training quantization (PTQ) and quantization-aware training (QAT). In the PTQ,
the pre-trained networks are simply quantized without re-training the model. This approach allows
us to achieve the nearly floating point accuracy at 8-bits, while below 8-bits, this results in significant
accuracy degradation. Although there have been recent attempts to alleviate the accuracy degrada-
tion in PTQ (Banner et al. (2018); Choukroun et al. (2019); Zhao et al. (2019); Kryzhanovskiy et al.
(2021)), the QAT, where the quantized weights and activations are trained (e.g., Zhou et al. (2016);
Hubara et al. (2017); Rastegari et al. (2016)), usually leads to better accuracy.
The difficulty in QAT is that the weights and the activations are discretized, and intrinsically non-
differentiable. If we take the derivatives forcibly, they either vanish or diverge. To avoid this prob-
lem, we replace them with the derivatives of some differentiable function in the backward pass only,
called the Straight-Through Estimator (STE) (Hinton et al. (2012); Bengio et al. (2013)). Since the
replacement leads to bias, it is not always possible to learn the network successfully. However this
approach can be applied to the low bits below 8-bits with tolerable accuracy degradation (Zhou et al.
(2016); Choi et al. (2018); Esser et al. (2019); Bhalgat et al. (2020)).
1
Under review as a conference paper at ICLR 2022
Originally, STE was introduced as the replacement with the derivative of the identity function by
Hinton et al. (2012). Later, the term ”STE” has been extensively used as the replacement with vari-
ous functions. In the binary network, Bengio et al. (2013) studied the replacement with the derivative
of sigmoid function as well as the original STE, and Hubara et al. (2017) used the derivative of the
identity function clipped in the region of |x| ≤ 1. In low-bits network, Zhou et al. (2016) used the
quantized gradient after the replacement with the identity function, and Choi et al. (2018), Esser
et al. (2019), and Bhalgat et al. (2020) used the the derivative of the clipped Relu (cRelu) for leaning
the step size.
1.1	Main contribution
In this paper, we refer to the proxy of the gradient as the STE gradient 1. Although STE has been
much success in training quantized DNNs empirically, the theoretical justification is very limited to
the specific situations. It also remains unclear what differentiable functions should be used for the
STE gradient. Our aim is to shed light on a new factor to determine the functions.
Without assuming the shape of the input distribution and the loss function, we first discuss the
properties of the three STEs, identity STE, Relu STE, and cRelu STE in one-hidden-layer network
with binary activation from the perspective of the intrinsic symmetry. We find that because the
identity STE and Relu STE keep the scale symmetry, the STE gradients should be zero for any scale
to converge to the true (local) minimum of the loss function, while in the case of the cRelu STE,
which breaks the scale symmetry, even if its gradient is not zero at the minimum at some scale and
it becomes zero at other scale, the cRelu STE converges to the minimum. Therefore, the back-
propagation using the cRelu STE is most likely to converge to the minimum of the loss function
among three STEs. This result reveals differences between Relu STE and cRelu STE, which could
not be found in the models by Yin et al. (2019) and Long et al. (2021).
Recently, Kunin et al. (2020) discussed the symmetry embedded in non-quantized neural networks,
and the effect of its breaking such as the discretization, the weight decay, and the stochasticity during
training. We discuss the new effect of breaking symmetry by the proxy of the gradient in training
the quantized neural networks.
Next, to confirm this observation, we have studied a similar model of the Gaussian input as discussed
in Yin et al. (2019) as an example. We have employed a misspecified model suitable for most
practical situations. Unlike the previous study, the true labels are presumed to be generated by the
non-quantized network architecture. Consequently, we find that ignoring the scale degeneracy for
the weight in front of the activation, all the stationary points are identical with the vanishing points
in the cRelu STE gradient, while some of them are not identical with the vanishing points in the
identity and the Relu STE gradients. In particular, at the global minimum point, both identity and
Relu STE gradients do not vanish. Finally, to confirm it in more general case, we have numerically
studied the model of the mixture Gaussian input.
1.2	Related Works
Recently, there have been a few theoretical studies on the justification of the STEs. Shekhovtsov &
Yanush (2020) derived STE by the linearization of their proposed estimator in a stochastic binary
deep network where the noises are injected before the binary activation to be a smooth network.
Cheng et al. (2019) argued that STE can be interpreted as a projected Wasserstein gradient flow
under certain conditions. Yin et al. (2019) that inspired our study examined which of the three
STE’s, identity STE, Relu STE, and cRelu STE, converges to the true minimum in one-hidden-
layer network with binarized activation and Gaussian input data. They clarified that if the labels
are obtained from a teacher network of the same architecture (Du et al. (2018)), all the three STEs
show the non-negative correlation with the population gradient, but only the identity STE gradient
does not become zero in a local minimum. This indicates that the back-propagation using either
Relu STE or cRelu STE may convergence to the local minimum, while it is impossible to show
the convergence to the local minimum using the identity STE. Furthermore, they showed that all
the three STEs gradients vanish at the global minimum, indicating that they may achieve the global
minimum independent of the choice of the STE, if we start with an appropriate initial value.
1It is called “coarse” gradient in Yin et al. (2019).
2
Under review as a conference paper at ICLR 2022
Very recently, Long et al. (2021) have discussed the justification of a class of STEs with certain
monotonicity, which generalizes the Relu STE, in one-hidden-layer network for the hinge loss,
claiming that the STE gradients vanish at the global minimum.
1.3	Notations
∣∙∣ denotes the Euclidean norm of a vector. A capital letter in bold denotes the matrix, e.g., Z, and
its component is given by the letter with lower indices, e.g., Zij. A small letter in bold denotes the
vector, e.g., g, and its component is given by the letter with lower indices, e.g., gj. Im×m is the
m × m identity matrix, and 1m is the m-dimensional vector of all ones.
2 General discussion on STEs in one-hidden layer CNN with
BINARY ACTIVATION
2.1 preliminaries
The one-hidden layer convolutional neural network with a binary activation function is realized as
y(Z, v, w) = vT σ(Zw) =	viσ(	Zij wj),	(1)
ij
where σ(∙) is the step function,
σ(x) =	01	xx ≤> 00,,	(2)
Z ∈ Rm×n is constructed by deividing the input data, e.g. an image, into m patches with size n,
and w ∈ Rn and v ∈ Rm are the trainable weights, i.e. kernels, in the first and second layers,
respectively (Du et al. (2018)). We assume that Z is generated by a continuous distribution P(Z).
Due to the property of the step function, the network is invariant under the scale transformation for
w, w → kw (k > 0).
Using the loss for each sample, ' (y(v, w, Z); y*) with y* being the labels generated by the true
distribution q(y*∣Z), We can express the population loss function as
L(v, w;y*) = EZ ['(y(v, w, Z); y*)].	(3)
2.2 Back-propagation and STE gradients
The gradients of the loss for each sample w.r.t v and w can be formally expressed as
■=ιy⅛=∂'σ(χ Zij Wj),	⑷
∂' X ∂' ∂Xi	X ∂' X
西=N 西西=N ∂yviδ IN ZijO WjJ Zj
(5)
where xi = σ( j ZijWj) and we used the property that the derivative of the step function is Dirac’s
delta function, dσ(z)∕dz = δ(z). The gradient w.r.t V can be utilized for learning the network, while
it is impossible to use the gradient w.r.t w due to the presence of the delta function. Remarkably,
both population gradients w.r.t v and w are smooth functions due to its average over the continuous
distribution,
EZ
EZ
∂vi
∂'
∂Wj
/ (YdZiOJP({zi,j0 o) ∂∂'σ
w X Fij (V w,zii =O),
i1|w| ,
(6)
Fij(V, w,ZiI) ≡ /( Y	dZZi0j0 j P (n^zi0j0 O) ∂j-vi X Ok OjZikO ,	⑺
(iO,jO)6=(i,1)	y kO
3
Under review as a conference paper at ICLR 2022
〜
〜
Where We used the orthogonal transformation, Zik0 = j0 Oki 0j0 Zij0 , Zij0 = k0 Oki 0j0 Zik0 , With
O1ij0 = wj0 /|w| and Pj0 Oki 0j0 Oli0j0 = δk0,l0. We can easily see that the gradient W.r.t. w does not
have the component in w-direction, Pj WjEZ [∂∂'^]
0. Its vanishing originally follows from
the scale symmetry of the network: Because the loss has the scale symmetry, its gradient in the
w-direction is obviously zero.
Instead of the population gradient, we use the STE in practice, which replaces the delta function
With the derivative of some differentiable function μsτE, to train the kernel w,
gSTE(V, w, Z) ≡ X ∂yviμSTE (X Zij0Wj )Zij.
(8)
We consider three types of μsτE(x): (i) the identity type μsτE(x) = X , (ii) vanilla ReLU type
Mste(x) = xσ(x) and (iii) the cReLU type μsτE(x) = xσ(x)σ(r - x) with the upper clipping
value r. The surrogate back-propagation using the STE is described in Algorithm 1 (Yin et al.
(2019)).
Algorithm 1 Surrogate back-propagation using STE for learning one-hidden-layer CNN.
Input: initialization v0 ∈ Rm, w0 ∈ Rn, learning rate η.
for t = 0, 1, , . . . do
vt+1= Vt- η EZ [ ∂'(vt, wt; Z)]
wt+1 = wt - ηEZ gSTE(vt, wt; Z)
end for
The average of the STE gradients over the input data is also expressed as
EZ [gjSTE(V, w; Z)]
E d dZiiμSτE
i
(ZiIwI) Fij(v, w,Zii).
(9)
Note that in general, the STE gradients have the non-zero component in the w-direction due to their
partial collapse of the form of the gradient, Pj WjEZ gjSTE(V, w; Z) 6= 0.
It is obvious from Eq.(9) that the only difference between the STE gradients with the identity func-
tion, Relu, and cRelu is the region of the integration over Zi ι,
EZ
[gjA(V,w;Z)] =Xi RA
7 rz 7-ι /	rz ∖
Zi1Fij (V, w, Zi1),
(10)
where A = id, Relu, cRelu, and the integration reigion RA are given as Rid = (-∞, +∞),
RRelU = [0, +∞), and RcRelU = [0,向]∙
Following the scale invariance of Fij(V, w, Zi1) for w, Fi0j (V, kw, Zi01) = Fi0j(V, w, Zi01) (k >
0), the STE gradients have the following interesting features:
1.	The identity and Relu STE gradients have scale invariance for w,
EZ hgjid/RelU(V, kw; Z)i =EZ hgjid/RelU(V, w; Z)i .	(11)
2.	The cRelu STE gradient does not have scale invariance for w due to the clipping effect,
EZ [gjcRelU(V, kw, r; Z)] 6= EZ [gjcRelU(V, w, r; Z)],	(12)
where we have explicitly shown the dependence of the upper clipping value r.
3.	Instead, the scale transformation of w for the cRelu STE gradient can be compensated by
that of r:
EZ [gjcRelU(V, w, kr; Z)] =EZ [gjcRelU(V, w/k, r; Z)] .	(13)
4
Under review as a conference paper at ICLR 2022
Ifwe take kr with fixed r large enough to cover most of the distribution P (Z), the left-hand
side is identical with the Relu STE gradient. Here we define the spread of the distribution
as ρ. When We redefine w/k as W on the right hand, the CRelU STE gradient for |w| < r/p
is approximately identical with the Relu STE gradient,
EZ [gCRelu(v, w, r; Z)] ' EZ [gRelu(v, w; Z)] for |w| < r/p. (14)
This resUlt is intUitively obvioUs: When w is small enoUgh to keep most of the pre-activation
valUes in the clipped range, the cRelU gradient and the RelU gradient are approximately
identical.
The important issUe is that there is no gUarantee that the Weights obtained by the STE back-
propagation are the ones at the (local) minimUm of the loss fUnction. In other Words, at the stationary
points, (v, w) = (vs, ws), defined as the vanishing points of the popUlation gradient,
EZ
(15)
the STE gradient is not zero in general. Here We consider the local minimUm point that the STE
gradient does not vanish,
EZ [gjSTE(vs, ws; Z)] 6= 0.
(16)
Note that (v, w) = (vs, kws) for any k > 0 is also the local minimUm point dUe to the scale
invariance of the loss. In the case of the RelU/identity STEs, their gradient has the same finite valUe
for any scales dUe to the scale invariance shoWn in Eq.(11),
EZ hgjid/Relu(vs, kws; Z)i = EZ hgjid/Relu(vs,ws; Z)i 6= 0.	(17)
In the case of the cRelU STE, hoWever, the valUe of its gradient changes as the scale changes dUe to
the breaking of the scale symmetry shoWn in Eq.(12). Ifit crosses zero as a fUnction of scale,
EZ [gjcRelu(vs, k0ws, r; Z)] = 0 for some k0,	(18)
the back propagation Using cRelU STE can converge at the local minimUm as Was done by Using
the popUlation gradient. Therefore, We conjectUre that dUe to breaking scale symmetry, the back-
propagation Using cRelU STE is the most likely to achieve the (local) minimUm of the loss fUnction
in the three STEs. This also implies that the cRelU STE is less biased than the other. We confirm
the conjectUre analytically for the GaUssian inpUt With RelU-type teacher netWork in Sec.3, and
nUmerically for the mixtUre GaUssian inpUt With varioUs teacher netWork in Sec. 4.
Since the above discUssion is focUsed only on symmetry, the conjectUre can be generalized to net-
Works With more symmetries in a straightforWard Way. In that case, ifWe employ an STE that breaks
all of those symmetries instead of cRelU STE, it is more likely to achieve the minimUm.
The similar mechanism can be also foUnd in physical phenomena. For instance, this is the case of
the ferromagnet, Which has rotational symmetry in energy (or Hamiltonian). In the system, if We
impose the magnetic field, Which breaks the rotational symmetry, the spin states, corresponding to
minimUm points, become aligned With the same direction.
3 One-hidden layer CNN with b inary activation: Gaussian input
and labels generated by non-quantized Relu network
We consider the simple model similar to Yin et al. (2019) given by the GaUssian inpUt With
the variance σ2, Z 〜 P(Z) = (√=p) e--PPij 2i2. We use the squared loss function,
'(y(v, w, Z); y* (v*, w*, Z)) = 1 (y - y*)2. Unlike in Yin et al. (2019), the true labels are as-
sumed to be generated by the non-quantized Relu netWork,
y*(v*, w*, Z) = X vfRelu (χ Zij wj ) ,	(19)
5
Under review as a conference paper at ICLR 2022
with the Relu activation defined as fRelu (x) = xσ(x).
Note that the models are misspecified, i.e., the population loss cannnot be zero even at the global
minimum. In fact, it is inferred from the accuracy degradation in most practical situations (Hubara
et al. (2017)) that the binary networks are considered as misspecified models. On the other hand, if
the true labels are generated by the same binarized architecture as was used in Yin et al. (2019), the
network becomes a specified model.
As shown below, this misspecified model leads to a more striking difference between three STEs
than the specified model. Particularly, the behaviors of three STE gradients at the global minimum
are completely different from each other.
3.1	population loss, its gradient and stationary points
The population loss L(v, w; v*, w*) is derived in Appendix A.1 as
L(v, w; v*, w*) = EZ [`(v, w, Z; v*, w*)]
=EvT (Im×m + 1m1m) V ― --∕7√=zv^^ (Cos 夕Im×m + 1m1m) V
8	2 2π
+ 4∏ |w*|2v*T ((π - I)Im×m + 1m1m) v*
(20)
where φ is the angle between w* and w. Note that the loss is scale invariant for w, and thus its
derivative satisfies ew ∙ ∂∂W = 0. The stationary points are given by
*
V
∂L C	2σ∣w* |	1 — cos 夕
=0 ^⇔ v =	扇— I CoS 中Im×m +	.^^；
∂v	2π	m + 1
=0 = 0 ⇔ VTv* = 0 or 夕=0 or 夕=π
∂w
Where We USed (Im×m + 1m1m)	= Im×m - m+ι 1m1m.
There are three stationary points 2 as classified in Appendix A.2:
(21)
(22)
1.
middle saddle point given by V = V ≡ 2√∏l (cosOIm×m + 1肃干 1m1mm) v*,ψ
O ≡ arccos
(v*T 1m)2
-(m+1)(v*)2 + (v*T 1m)2
which satisfies VTv* = 0. This exists if and only
if (m + 1)(V*)2 ≥ 2 (v*t 1m)2.
2.	local minimum if (m + 1) (v*)2 ≥ 2 (v*t 1m)2, otherwise saddle point given by V =
Vn ≡ 2√w । (-Im×m + m+1 1m1m) v*,° = π
2σlw*l
3.	global minimum given by V = v0 ≡ √∏ l v*,o = 0. At the global minimum, the
population loss does not become zero, L(v = vo, W = w*) = σ |w 4∏(π-2) v*Tv.
3.2	STE gradient
The STE gradient is given by
gSTE = X 3〃Ste(x Zij0 WJZij
×	Xvi0σ XZi0j0wj0	- X vi*0 fRelu	XZi0j0wj*0	.	(23)
i0	j0	i0	j0
2The definition of the stationary points is not mathematically rigorous in our paper. Even if points are
non-differentiable, but the gradient gives zero at the points in the one-side limit, we call them the stationary
points.
6
Under review as a conference paper at ICLR 2022
The converging points by the back-propagation in Algorithm 1 are characterized by vanishing the
gradients,
0, EZ gjSTE (v, w; Z) = 0.	(24)
∂', 一
EZ W一 (V, w; Z)
∂vi
We discuss the explicit form of the three STE gradients and the associated vanishing points. The
detailed derivation of the STE gradients is shown in Appendix A.3-A.5.
3.2.1	identity STE
The identity STE gradient is given by
2
EZ [gid] = √2∏高(VTv) - ⅜w*”v*)	(25)
Combined with Eq.(21), we find that the equation in Eq.(24) has no solution. Therefore, if we use
the back-propagation by identity STE, it can not converge to any points.
3.2.2	Relu STE
The Relu STE gradient is expressed as
σ W
EZ [grelu] = 2√2∏ 丽
[vT (Im×m + 1m1m) v]- σπ 篇W [vT (-Im×m + 1m1m) v*]
.2	*	|w*| Sinφ	(vtv*)
-σ2 ((π - 6 w* + -1WΓW) V∙
(26)
Combined with Eq.(21), the gradients vanish at the following points:
1.	If v* Satisfies (m + I) (v*)2 ≥ 2 (v*T 1m)2,6 = O = arccos -(m+1(VJ*Sm)v*τIm)2
v = v = 2√W∏1 (cos OIm×m + Im+sT 1m1m) v*.
2.	Ψ = π, v = v = 2√w∏1 (-Im×m + m+1 1m1m).
We find the solutions are identical with two stationary points shown in item 1 and 2 in Sec.3.1,
leading to the the saddle point or local minimum, while the global minimum point at 6= 0, v = v0
cannot be obtained. Therefore, by using the back-propagation with Relu STE, it always converges
to φ = π if (m +1) (v*)2 ≥ 2 (v*TIm) , and it does not converge to any points, otherwise.
3.2.3 clipped Relu STE
The cRelu STE is given by
σ W
EZ [gcrelu] = 2√2∏ 而
(1 - e-1 (σlwl )2) vT (Im×m + 1mim) v
—
会备(1 - e-1 ( σwτ )2) W [vT (-Im×m + /K) v*]
2π |W|
-σ2 卜w，6^W∣ + S(w，O) (s⅛高-cot中篇)卜vTv*)，
(27)
where C(|W|, 6) and S(|W|, 6) are given in Eq.(106). At 6 = 0,π they are simplified as
C (∣W∣, O) = ⅛i (∏ - σ^W e- 1(σTwT )2 √2∏ - ∏erfc (√⅛ )),C(∣W∣,π) = S(W∣,∏)=
S (|W|, 0) = 0.
Remarkably, the cRelu STE gradient is proportional to the Relu STE gradient in the case ofvTv* =
0 or 6= π:
EZ [gcrelu] LT v* =0 or w=π = (1 - e 2 (司3)) EZ [grelu]]VT v* =0 or w=π .	(28)
7
Under review as a conference paper at ICLR 2022
As shown in Sec.3.2.2, allthe vanishing points in the Relu STE can be found at VTv* = 0 or 夕 =π,
leading to the saddle point or the local minimum, so that they are also found in the vanishing points
of cRelu STE. On the other hand, the behavior of cRelu STE gradient at 夕=0 is not related to that
of Relu STE gradient. The cRelu STE gradient at 夕=0 is written as
EZ [gcrelu] = ~^7==w* (1 - e 2 ( σlwl ) ) vT / ∣-**y (Im×m + 1m1Tn) V
2 2π	|w* |
---√= ((-l + λ) Im×m + 1m1m) v* 1 ,
2π
where
λ = λ(IwI)
π 一向e-1 (σfa )2 √2π - πerfc (√⅛)
1 - e- 2(^∣w∣)
(29)
(30)
This becomes zero if
v
2σ	ι ,
/--|w | (Im×m + 1m 1m)	((-1+ λ) Im×m + 1m1m) V
2π
2σ . *.
-^|w*|
√2∏
(-1 + λ) Im×m +
V* .
(31)
To be consistent with Eq.(21) at φ = 0, only λ = 2 is allowed. The solution provides the global
minimum shown in item 3 in Sec.3.1.
In fact, λ monotonically increases from 0 to ∏ as σrw∣ increases, and thus the solution at λ =
2, corresponding to the global minimum point, can be obtained for any clipping value r and the
variance σ2 by changing the scale |w|. Consequently, We get all the stationary points are found in
cRelu STE:
1.	If v* satisfies (m + 1)(v*)2 ≥ 2 (v*t 1m)2 ,夕=O = arccos
V = v = 2√1Π1 (cos ΨIm×m + 1m+sT 1m1m) v*∙
2.	O = π, V = v = 2√w∏1 (-Im×m + m+1 1m1m) ∙
(v*T 1m)2
-(m+1)(v*)2 + (v*T 1m)2
3.	0 =0, |w| = wo ≡ σ co, V = Vo ≡ 2√2W∏ | v*, wherewo (or co) is given by λ(wo) = 2.
Note that while the global minimum point is degenerated in scales for w, i.e. if (w, V) = (wo, Vo)
gives the global minimum, (w, V) = (kwo, Vo) for any k > 0 also gives the global minimum, the
cRelu STE picks up the point at the scale determined by λ = 2.
4	Experiments
To confirm our conjecture in more general case, we have numerically studied the Gaussian mixture
input with various mean values. As teacher networks, we have tested tanh-type and sin-type as
well as Relu-type. For all the examined setups, cRelu STE behaves like the population gradient,
while id/Relu STEs show qualitatively different behaviors as described below. To calculate the
population loss and STE gradients, we have generated the mixture Gaussian samples and have taken
their average. The population gradient has been obtained by calculating the finite difference of the
population loss. We have demonstrated the back propagation with learning rate η = 0.01 given in
Algorithm 1.
Shown in Fig. 1 are the results of ten mixture Gaussian input with random mean values for each
components of Z. We employed m = 20, n = 25, and the tanh-type teacher network. We find
that the population gradient and cRelu STE show similar results, while id/Relu STEs are completely
different. This indicates cRelu STE is less biased than id/Relu STEs.
8
Under review as a conference paper at ICLR 2022
In the case of id/Relu STEs, at early steps up to around 500 step, |w| decreases around the magnitude
of the update quantity, so that it begins to oscillate. Then it escapes the oscillation, and the loss
function shows the convergence to a point different from the local minimum achieved by population
gradient, while |w | becomes larger and larger due to their scale invariance. Interestingly, the values
of the loss function are small compared to the one obtained by population gradient, which implies
that id/Relu STEs avoid being trapped in the local solution due to their large bias. However, note
that even at that point, the magnitude of w continues to grow and eventually become numerically
unstable.
Figure 1: Numerical results of the back-propagation by population gradient and three STEs. We
generate 10000 samples which follow ten mixture Gaussian input with random mean values. We
employ the tanh-type teacher network.
5	Summary
We have found that breaking symmetry embedded in the network by STEs enhances the possibility
of convergence to the true (local) minimum of the loss function. We have demonstrated that if an
STE breaks the scale symmetry embedded in the one-hidden-layer network with a binary activation,
it is more likely to achieve the local minimum than the one which keeps the symmetry. The discus-
sion can be generalized to the network with more symmetries in a straightforward way. The more
symmetries embedded in the network an STE breaks, the more likely it is to converge. To confirm
the mechanism, we have studied three STEs, identity STE, Relu STE, and cRelu STE, in a sim-
ple misspecified model with Gaussian input. We have found that the back-propagation by the cRelu
STE, which breaks the scale symmetry, can converge to the global minimum, while identy/Relu STE
cannot. Finally we have numerically confirmed the mechanism for the mixture Gaussian model with
various teacher network.
References
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of
convolution networks for rapid-deployment. arXiv Preprint arXiv:1810.05723, 2018.
YoShUa Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv PrePrint arXiv:1308.3432, 2013.
9
Under review as a conference paper at ICLR 2022
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving
low-bit quantization through learnable offsets and better initialization. In Proceedings of the
IEEE/cVf Conference on CompUter Vision and Pattem Recognition Workshops, pp. 696-697,
2020.
Pengyu Cheng, Chang Liu, Chunyuan Li, Dinghan Shen, Ricardo Henao, and Lawrence
Carin. Straight-through estimator as projected wasserstein gradient flow. arXiv PrePrint
arXiv:1910.02176, 2019.
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv PrePrint arXiv:1805.06085, 2018.
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net-
works for efficient inference. In ICCV WOrkShops, pp. 3009-3018, 2019.
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer cnn: Don,t be afraid of spurious local minima. In International COnference on
Machine Learning, pp. 1339-1348. PMLR, 2018.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. arXiv PrePrint arXiv:1902.08153, 2019.
Geoffrey Hinton, Nitsh Srivastava, and Kevin Swersky. Neural networks for machine learning.
Coursera, video lectures, 264(1):2146-2153, 2012.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quan-
tized neural networks: Training neural networks with low precision weights and activations. The
JOUrnaI OfMachine Learning ReSearch, 18(1):6869-6898, 2017.
Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A
whitepaper. arXiv PrePrint arXiv:1806.08342, 2018.
Vladimir Kryzhanovskiy, Gleb Balitskiy, Nikolay Kozyrskiy, and Aleksandr Zuruev. Qpp: Real-
time quantization parameter prediction for deep neural networks. In PrOceedingS of the IEEE/CVF
COnference on COmPUter ViSiOn and Pattern Recognition, pp. 10684-10692, 2021.
Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.
Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv
PrePrint arXiv:2012.04728, 2020.
Ziang Long, Penghang Yin, and Jack Xin. Learning quantized neural nets by coarse gradient method
for nonlinear classification. ReSearch in the MathematicaI Sciences, 8(3):1-19, 2021.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In EUrOPean conference on computer
ViSion, pp. 525-542. Springer, 2016.
Alexander Shekhovtsov and Viktor Yanush. Reintroducing straight-through estimators as principled
methods for stochastic binary networks. arXiv PrePrint arXiv:2006.06880, 2020.
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Under-
standing straight-through estimator in training activation quantized neural nets. arXiv PrePrint
arXiv:1903.05662, 2019.
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network
quantization without retraining using outlier channel splitting. In International conference on
machine Iearning, pp. 7543-7552. PMLR, 2019.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv PrePrint
arXiv:1606.06160, 2016.
10
Under review as a conference paper at ICLR 2022
A derivation of Sec.3
In the Appendix, we provide the results of the Gaussian with variance 1, N(0, 1). For some function
f (Z), the expectation value of the Gaussian with the generic variance σ, N(0, σ2), shown in Sec.3
is written as
EZ 〜N (0,σ2)[f (Z)]
∕2∏1^2 /y dZij f(z )e-
i,j
r1 Z Y dZoj 削 Z)e-
EZ 〜N (0,i)[f (σ Z)]
Pij Zj
2^2
Pij Zij
(32)
2
where We have changed the integral variables from Z to Z0 = Z/σ. Therefore We get the results
with the variance σ by only changing from f (Z) to f (σZ).
A. 1 derivation of loss function
The expectaion of the loss is divided into the following three terms:
L(v, w； v*, w*) =2EZ [vTσ(Zw)vTσ(Zw)] - EZ [vτσ(Zw)v*TfReiu(Zw*)]
+ 2EZ [v*TfRelu(Zw* )V*TfRelu(Zw*)]	(33)
The calculation of the first term and the third term are given in “Proof of Lemma 1” in Yin et al.
(2019) and “Proof of Section 3” in Ref. Du et al. (2018), respectively:
2EZ [vTσ(Zw)vτσ(Zw)] + 2EZ [v*TfReiu(Zw*)v*TfReiu(Zw*)]
=8 VT (Im×m + Im*) V + 4∏ (w*)2V*T ((∏ - 1)Im×m + 1m1∑) V*	(34)
To discuss the second term, we evaluate the following quantity:
Fij = EZ [σ(Zw)fRelu(Zw*)]ij
mn I' Y dZki] e-1 Pk,ι 琮 σ X ZVmWmo
k,l	m0
Zj m0wm0
(35)
(i) i 6= j case:
where we used the orthogonal transformation in the second line:
~
Zil
Olim0Zim0,
m0
X Oljm0Zjm0
m0
~
Zjl
(37)
11
Under review as a conference paper at ICLR 2022
with O1m0 = wmo/|w| and Ok，= Wm，/|w*|.
(ii) i = j case:
Fii
e-1 Pl Z2l σ
l
Without loss of generality, we can choose
*
w*
w
EZimwm	EZimwm σ EZimwm
mm	m
(38)
(w*, 0n-1),
(w1 , w2, 0n-2)
(39)
with w* > 0, so that this can be written as
Fii
dZiidZi2e-2 (z2ι+z22)σ
Zimwm Zi1w*σ (Zi1)
dZdθZe-1Z σ (cos(夕一θ)) Z cos θw*σ (cos θ)
=ɪ Z∞ dZ /∏∕2 dθZ2e-2Z2 cos θw*
2π 00	Jφ-π/2
=-^=w*(1 + cos ω)
2√2∏
where 夕 is the angle between w* and w. Eqs.(36) and (40) are combined as
Fij = 2√2∏ |W*|(I + cos ^)δij + 2√2∏ |W*|(I - δij ),
so that the second term in Eq. (33) is expressed as
EZ [vT σ(Zw)V*T fRelu (Zw*)] = JwJ- VT (cos φIm×m + 1m1m)
2 2π
(40)
(41)
v* .
(42)
Consequently, the loss function is summarized as
1	|w*|
L(v, W； V , W)= VT (Im×m + 1m1m) V -	VT (cos φIm×m + 1m1m V
8	2 2π
+ 4∏ |w* |2 V*T ((π -I)Im×m + 1m1m) v*∙
The population gradient w.r.t V and w is thus given by
(43)
∂L
∂ V
∂L
∂ W
ι	τ、 σ∣w*|	τ、 *
I (Im×m + 1m1m) V - cos (cos /Im×m + 1m1m) V ,
4	2 2π
(44)
where without loss
1 ∂L	σ∣w*∣	. T T *∖
e° =	Sinφ VVe°,
|w| ∂φ ψ 2√2∏∣W∣	+' ɪ
of generality, we can take ew =
(45)
(cos /，Sin /，0n-2), e中
(-sin /，cos /，0n-2). Note that ew ∙ ∂L = 0 is satisfied due to the scale symmetry for the loss.
A.2 classification of stationary points
Using Eq.(21), the loss at the three stationary points is rewritten as
L =-誓V*T (cos2 OIm×m + 1-^^ + ^	V*
+ I 2Π v*t ((π - I) Im×m + 1m1m)v*∙
(46)
12
Under review as a conference paper at ICLR 2022
Obviously, this shows that C = 0 is the global minimum point,
L ≥ L∖φ=o = ∖w ∖ (π - 2)v*τv.
4π
(47)
At c = π, this becomes
L∖φ=π
宇v*τ (lm×m+ (」+ 1) 1m1m)v*
4π ∖	∖m + 1	)	)
∖w4∏∖-v*τ ((π - 1) Im×m + 1m1m) v*
与EV*τ((π - 2) Im×m + ɪ
4π ∖	m + 1
*
v*
(48)
At φ = Cp = arccos
"1m)2
-(m+1)(v*)2+(v*T 1m)2
,this becomes
L∖φ = cp
∖w*∖2ι,*J(	(v*τ1m)2	)2 i
=V	[1 -(m + 1)(V*)2 + (v*τ 1m)2 ∫ lm×m
-(m + 1) {
-(m + 1) (v*)2 + (v*τ 1m)2
)+1 j 1m1rrn
*
v*
∖w*∖2 v*τ
4π
∖w*∖2
((π - 1) Im×m + 1m1m) v*
(V*τ1m)2(v*)2
4π
-(m + 1)(v*)2 + (v*τ 1m)2
+ (v*T1m)2}
+
—
—
+
+
—
(
∖w* ∖2
+---4∏~V	((π - 1) Im×m + 1m1m) V ∙
Using these results, we obtain
L∣W=0 - L∖w=π
(49)
∖w*∖2
(v*T1m)2 (V*)2
4π -(m + 1)(v*)2 + (v*τ 1m)
2+
∖w* ∖2
∖w*∖2
4π
{(m + 1)(v*)2 - 2 (v*τ1m)2}
4π
2
m + 1) (v*)2 - (v*τ 1m)21 (m + 1)
(50)
—
—
ɪ …)2 m⅛
Note that because (Im + 1) (v*)2 ≥ 2 (v*τ 1m)2 is satisfied to exist the stationary point at C
the loss at C = ∏ is smaller than the one at at C = C,
Llv=V,w=p ≥ l∖v=v∏,φ=π∙
(51)
is satisfied. When (m + 1) (v*)2 = 2 (v*τ 1m)2, L∖φ=ip = L∖φ=∏, because the middle stationary
point at C = C is merged into the point at C = π.
This implies that C = 0 is the global minimum, C = π is the local minimum, and C = C is the
saddle point or the local maximum if (m + 1) (v*)2 ≥ 2 (v*τ 1m).
To fully clarify whether the stationary points are local minimum, maximum or saddle point, we have
calculated the hessian matrix,
H
1 IIm×m + 1m1T)	2√2∏ Sin Cv*
|w*| .	*τ	|w*| T *
⅛sin Cv	⅛V V cos C
(52)
13
Under review as a conference paper at ICLR 2022
(i) V = V, φ = Cp:
For z = (x, y)T (x ∈ Rm, y ∈ R),
zTHz
1XT (Im×m + Imim) X + lw= V Sin C (XTV*)
4	2π
4 (XTIm)2 +
(2X+
∖w*∖y sin C *
---------V
√2∏
∖w* ∖2y2 sin2 C (v*)2
2∏
(53)
—
where we used VpTV* = 0. Therefore, depending on X andy , the Hessian can be either positive or
negative, which means the stationary point is a saddle point.
Qi) V = Vn ≡ 2√w∏l (一Im×m + m+1 1m1m) v* ,ψ = π
4 4 (Im ×m + 1m 1Tm
H=	0
0
-Iwa VT V*
2√2∏ π
(54)
Note that VTv* = 2√∣∏lv*t (-Im×m + m2+ι 1m1m V*. If (m + 1) (v*)2 ≥ 2 (v*t 1m)2, then
VπT V* ≤ 0, so that the matrix becomes a positive definite matrix: For any X ∈ Rm ,y ∈ R, this
satisfies
彳XT (Im×m + 1m1m) X - ɔ √-^-VTVV ≥ 0,
4	2 2π
(55)
Which means the stationary point is the local minimum. On the other hand, if (m + 1) (V*)2 ≤
2 (v*t 1m)2, the matrix becomes either positive or negative, depending on X and y, so that this
stationary point becomes a saddle point.
(iii) v = V0 ≡ 2√W2∏lv*,C = 0
4 4 (Im ×m + 1m 1Tm
H=	0
0
V0T V*
(56)
The Hessian becomes a positive definite matrix: For any X ∈ Rm ,y ∈ R, this satisfies
彳XT (Im×m + 1m1m) X + ~~7^vVTv*V2 ≥ 0∙
4	2 2π
(57)
A.3 derivation of identity STE gradient
By substituting μ0(X) = 1 into Eq.(23), We obtain
EZ [g]i=( √2∏)	/(Y dz) e
1 Pk,ι Zkl	X Zjivj
×	vkσ	Zklwl	-	vk*fRelu
kl	k
We calculate each term as folloWs.
.
(58)
A.3.1 calculation of the first term
We calculate the first term as Was done in Lemma 9 (and also Lemma 11) in Yin et al. (2019).
To discuss the first term, We evaluate
Fijk ≡ EZ [Zji [σ(Zw)]k] .
(59)

14
Under review as a conference paper at ICLR 2022
Using Fiijdk, the first term can be expressed as
first term =	Fiijdk vj vk .
j,k
(60)
(i)	j 6= k case:
Fid = 0
ijk ,
because of the odd symmetry for i-th and j-th components in the integrand.
(ii)	j = k case:
(61)
e-2 PI Zjl Zj"
dZji) e-1 Pi Z2ι(X OjmTOZj"σ(Z")
dZjι	e-2 Pi ZOjITZjiσ(Zji)
(62)
where we used the orthogonal transformation in the second line:
Zjl = EOjmO Zjmo
m0
(63)
with O1i m0 = wm0 /|w|.
The first term is reduced to
first term
1 Wi
√2∏ |w|
(64)
A.3.2 calculation of the second term
To discuss the second term, we evaluate
Gidk= EZ [Zji [fRelu(Zw*)]k].
(65)
Using Giidjk, the second term can be expressed as
second term = ɪ2 Gidkvjvk∙
j,k
(66)
(i) j 6= k case:
Giidjk = 0,
(67)
because of the odd symmetry.
15
Under review as a conference paper at ICLR 2022
(ii) j
=(√2π) / dZjie-2ZjIOjIT劣ι∣w*∣σ(∣w*∣Zji)
=⅛⅛1T = lw⅛
1 *
=2 Wi
where we have used the orthogonal transformation in the second line:
Zjl = X OjmoZjmo
m0
with O1i mo = wm* o /|w* |, and also have used
ZCo dχχ2e-α∕2W =邙
02
The second term is written as
second term
1
2 Wi
(68)
(69)
(70)
(71)
A.4 derivation of Relu STE gradient
By substituting μ0(x) = σ(x) into Eq.(23), We obtain
EZ [grelu]i = (√2∏)	Z (Y dZkl^ e-^ Pk,l Zkl (X Zjivjσ (X ZjI^^
×	Xvkσ	XZklWl	-Xvk*fRelu	XZklWl*	.
(72)
We calculate each term as folloWs.
A.4. 1 calculation of the first term
We calculate the first term as Was done in Lemma9 (and also Lemma11) in Yin et al. (2019). We
evaluate
Firjeklu = EZ Zji [σ(Zw)]j [σ(Zw)]k .
(73)
Using Firjeklu, the first term can be expressed as
first term =	Firjekluvjvk.
j,k
(74)
16
Under review as a conference paper at ICLR 2022
(i)	j 6= k case:
Oij1T
1
2√2∏
1
dZjidZk) e-1 Pi Z2l+Z21 Zjiσ (X Zjlw) σ (X Zklw)
(UdZjldZk) e 1 Pi ZMl(X OjmTO Zjm)σ (Zj) σ (Zk)
dZjidZkie-2(Zj1+ZkI)OiITZjισ (ZjI) σ(Z®i)
wi
2√2π |w|,
where we used the orthogonal transformation in the second line:
Zjl = X OjmO ZjmO
m0
(75)
(76)
with O1i m0 = wm0 /|w|.
(ii)	j = k case:
We get the same expression given in Eq.(62),
1 Wi
√2∏ |w|
e-1 El Z2l Zjiσ
(77)
The first term is written as
first term
1	wi T
^~~Fv
2√2π |w|
×m + 1m 1 m v.
(78)
A.4.2 calculation of second term
To discuss the second term, we evaluate
GrekU = EZ hZji h[σ(Zw)]j fReiu(Zw*)]J .	(79)
Using Girjeklu, the second term can be expressed as
second term = X Grekuvj vk.	(80)
j,k
(i) j 6= k case:
dZjldZkl)e-1 Pl Z2l+Z2l Zjiσ (X Zjl W) fRelu (X Zklwl*
IIdZjldZkl) e-2 Pl Zjl+Zkl	XOjmTZjm
l	mO
0 j σ (ZjI) fRelu (∣W*∣Zki)
dZjιdZkie-2(Z2ι+ZkI)OjITZjισ (Zjι) ∣w*∣Zkiσ 停)
|w
^2∏
1 ∣w*l
2∏ 西 wi,
(81)
17
Under review as a conference paper at ICLR 2022
where we used the orthogonal transformation in the second line:
Zjl = X Ojm0 Zjm0,
m0
Zkl = X OkO Zkmo
m0
withO1im0 = wm0/|w| and O1jm0 = wm* 0/|w*|.
(ii) j = k case:
(82)
/
Z
w* Γ dZZ3e-1Z2 Zπ/2
2π 0 0	— —/2 + ^φ
e-2 Pl Zjl Zjiσ
e-2 El Z2l Zjiσ
Zjlwl	fRelu (w*Zj1)
2∏ / (Y dZjl' e-2 Pl=1 Zjl Zji ("ii + δi2) σ
Zjlwl	fRelu (w*Zj1)
Zjlwl	fRelu
dZdθZe-2Z (Zcos θδii + Zsin θδi2) σ (Zw cos (θ —夕))fReiu (w*Z cos θ)
dZdθZe-2Z (Zcos θδii + Zsin θδi2) σ (Zw cos (θ —夕))w*Zcos θσ (CoS θ)
(83)
dθ (cos2 θδii + cos θ sin θδi2)
%+ 2(∏ - 0卜1 + 三产 M
1	,	、c , sin 中, t∙ , . t∙、'
2	(π -夕)δi1 +--2~ (cos 夕δi1 + Sin 夕δi2)
where we have chosen
(84)
*
w*
(w*, 0n-1),
(w1,w2,0n-2)
(85)
w
with w* > 0 and φ is defined as the angle between w* and w. We also have used the formula,
0
Z
Z
dxx3e-(1/2)x2 = 2,
dθ cos2 θ = sin2θ + 1 θ,
4	+ 2 ,
dθ cos θ sin θ = —ɪ cos 2θ.
(86)
Note that
w*
|w*|
Wi
|w|
δi1 ,
cos φδii + sin φδi2.
(87)
This reduces to
1
relu
Gijj
1	*	sin 0|w* |
2(π -0 w*+—画 Wi
(88)
π
In summary, the second term is written as
1 |W | T	T *
second term = 2∏-Pw-w* [v (-Im×m + 1m 1mJ V J
A 、 *	|w*| sin 2
+ (π - 0 Wi +	|W|	Wi
(VT v*)
2π
(89)
18
Under review as a conference paper at ICLR 2022
A.5 derivation of clipped Relu STE gradient
By substituting μ0(x) = σ(x)σ(r — x) into Eq.(23), We obtain
EZ [gcrelu]i =
dZkl
Pk,l Zk2l
_ 1
e 2
× (X Zjivjσ (X Zjiwι) σ (r- X Zjlwl))
×	Xvkσ XZklwl)-X VfcfRelu (X Zklw))
(90)
We calculate each term as folloWs.
A.5.1 calculation of the first term
To discuss the first term, We evaluate
Ficjrkelu = EZ hZji [σ(Zw)	σ(r - Zw)]j [σ(Zw)]ki .	(91)
Using Ficjrkelu, the first term can be expressed as
first term = X Ficjrkeluvjvk.	(92)
j,k
(i) j 6= k case:
crelu
Fijk
Y dZjldZkl	e-1 Pl Z2l+Zkl Zji
1
-IwlZjl) σ (Zkl)
Oj1T(1 — e-22(向)2
2√2∏
1
2√2∏
1-e-1(向巧 W
1	I ιwι,
(93)
Where We used the orthogonal transformation in the second line:
Zjl = EOjmO Zjm
m0
With O1i m0 = wm0 /IwI.
(94)
19
Under review as a conference paper at ICLR 2022
(ii) j = k case:
PCreI
Fijj
∏ dzj e 1 Pl Zj Zji (X ZjIWI)σ (r - X ZjIW)
∏ dj e-1	Pl Z (X	吗 Zjj	σ (Zji)	σ (r	- |w|Z”
.l	)	∖rn'	)
σ (r TwIZjI)
(95)
In summary, the first term is written as
first term
1	Wi
2√2π |w|
(1 — e-1 (向) ) vτ (Imxm + ɪmɪm) v.
(96)
A.5.2 CALCULATION OF THE SECOND TERM
To discuss the second term, we evaluate
GCjtU = EZ [Zji [[σ(Zw) Θ σ(r — Zw)j fReiu (Zw* )]J .
Using GCrklu, the second term can be expressed as
second term = ^X GcrkIUVjV晨
j,k
(97)
(98)
⑴ j = k case:
GCrIIU =(√2∏)	/ (∏ dZjldZk) e-2 Pl z"+ZklZji
× σ (X Zjiwι) σ (r - X ZjIW) fReiu (X Z∣ιw：)
=(√⅛)2n / (∏ dZjldZkl) e-1 Pl %瑞(x j j
〜
〜
σ (r - |w|Z”) fReiu (|w*|Zki)
Z dZjidZkie-1(Z21 +zk1)Oj1τZji
× σ (Zji) σ (r - |w|Z八) |w*|Z1々 (Z∣ι)
=(√= )2 [ dZjie-2 琢 OjT Zjiσ (Zji) σ (r - |w|Z八) |w*|
-r/|w|
dZjie-1 Zj OjIT Zji|w*|
∖ V2π√ √o
粤OjiT (1 - e-1(向)2)
理(1 - e-1 (向)2) R,
2π V	) |w|
(99)
20
Under review as a conference paper at ICLR 2022
where we used the orthogonal transformation in the second line:
Zil = X Oim0Zim0,
~
Zkl
m0
X Olkm0Zkm0
m0
*|.
(100)
×σ
withO1im0 = wm0/|w| and O1jm0 = wm* 0/|w
(ii) j = k case:
e-2 Pi Z2ι Zji
e∏ 2 Pl Zjl Zji
r -	Zjlwl fRelu	X Zjlwl*
X σ	X2 Zjlwl	σ r - X Zjlwl	fRelu (w*Zj1)
=2∏ Z (Y dZjl) e∏2 PI=I ZjI Zji (δii + δi2)
× σ X Zjlwl σ r - X Zjlwl fRelu (w*Zj1)
=ɪ / dZdθZe∏1Z2 (Z CoS θδi1 + Z Sin θδi2)
X σ (Zw cos(θ -2))σ (r - X Zjlwl) fReiu (w*Z CoS θ)
=ɪ / dZdθZe∏1Z2 (Z CoS θδi1 + Z sin θδi2)
X σ (Zw CoS (θ —夕))σ (r — Zw CoS (θ —夕))w* Z cos θσ (CoS θ)
(101)
dθ (CoS2 θδi1 + cos θ sin θδi2) /
___r___
EE dZZ3e∏2Z2
dθ (cos2 θδii + cos θ Sin θδi2)
×
w* /π/2
2π -∏/2+^φ
w* /π/2
2π J∏π∣2 +中
-----r-----]	+ 2! e∏ 2 ( W cosrθ-M )2 )
W cos(θ-0 J+2J e	∫
(102)
where we have chosen
w* = (w*, 0n∏1),
w = (w1,w2,0n∏2)
(103)
with w* > 0 and φ is defined as the angle between w* and w. We also have used the formula,
Z dxx3e∏2χ2 = 2 -(F +2)e∏112
0
(104)
Note that
w*
|w*|
Wi
|w|
δi1 ,
cos φδii + sin φδi2.
(105)
21
Under review as a conference paper at ICLR 2022
Using C(W)夕)，S(w,夕)defined as
C(w, ω) ≡ W- / dθ cos2 θ J 2 — I (-----r------) + 2 I e-1(w c°sCθ-^))I ,
( )"	2π 匚…	∖	NWcos(θ ― φ)) + )	∫,
S(w, ω) ≡ W- /	dθ sin θ cos θ J 2 — ( (-r----) + 2 ∣ e-1(W cosrθ-M)\ ,
…)2π 匚…	∖	NWcos(θ ― φ)) + )	广
(106)
If 夕=0, π, GcreIU is formally written as
1	,	、 w*	,	、
G 渭U = C(w")尚 + SgS
1 Wi
sin夕 |w|
-cot S w*
If 夕=0 or π, GcreIU is formally written as
W , W w*
G 涝lu = c (w, 0) |W| for S = 0,
G涝lu = 0 for s = π
where we used S(w, 0) = S(w, π) = C(w, π) = 0 and
C(w, 0)
∕*π∕2
dθ cos2 θ
J-π∕l2
π ― ^e-1( W
W
with the following formula
∕*π∕2
I	dθe-
√-π∕2
ɑ2
2 cos2 θ
πerfc
∕π/2	C	a2
dθ cos2 θe- 2 cos2 θ =
√-π∕2
In summary, the second term is written as
a (αe-ɪ √2π + (1 — α2) πerfc
second term = ɪ lw-τl (1 — e-1 (向))Wi ∖vτ (—1m×m +
2π |w| ∖	J l v
W	w*
+ {°(WM M + S(WM
1	Wi
sin夕 |w|
一cot 夕
(107)
(108)
(109)
(110)
(111)
(112)
(113)
22