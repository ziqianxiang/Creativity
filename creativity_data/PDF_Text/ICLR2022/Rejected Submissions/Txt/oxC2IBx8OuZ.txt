Under review as a conference paper at ICLR 2022
Towards	Federated	Learning	on	Time-
Evolving Heterogeneous Data
Anonymous authors
Paper under double-blind review
Ab stract
Federated Learning (FL) is an emerging learning paradigm that preserves privacy
by ensuring client data locality on edge devices. The optimization of FL is chal-
lenging in practice due to the diversity and heterogeneity of the learning system.
Despite recent research efforts on improving the optimization of heterogeneous
data, the impact of time-varying heterogeneous data in real-world scenarios, such
as changing client data or intermittent clients joining or leaving during training,
has not been well studied. In this work, we propose Continual Federated Learning
(CFL), a flexible framework, to capture the time-varying heterogeneity of FL.
CFL covers complex and realistic scenarios—which are challenging to evaluate
in previous FL formulations—by extracting the information of past local datasets
and approximating the local objective functions. Theoretically, we demonstrate
that CFL methods achieve a faster convergence rate than FedAvg in time-varying
scenarios, with the benefit being dependent on approximation quality. In a series of
experiments, we show that the numerical findings match the convergence analysis,
and CFL methods significantly outperform the other SOTA FL baselines.
1	Introduction
Federated Learning (FL) has recently emerged as a critical distributed machine learning paradigm to
preserve user/client privacy. Clients engaged in the training process of FL only communicate their
local model parameters, rather than their private local data, with the central server.
As the workhorse algorithm in FL, FedAvg (McMahan et al., 2017) performs multiple local stochastic
gradient descent (SGD) updates on the available clients before communicating with the server.
Despite its success, FedAvg suffers from the large heterogeneity (non-iid-ness) in the data presented
on the different clients, causing drift in each client’s updates and resulting in slow and unstable
convergence (Karimireddy et al., 2020b). To address this issue, a new line of study has been
suggested lately that either simulates the distribution of the whole dataset using preassigned weights
of clients (Wang et al., 2020; Reisizadeh et al., 2020; Mohri et al., 2019; Li et al., 2020a) or adopts
variance reduction methods (Karimireddy et al., 2020b;a; Das et al., 2020; Haddadpour et al., 2021).
However, the FL formulation in these approaches always assumes a fixed data distribution among
clients throughout all training rounds, while in practice this assumption does not always hold because
of the complex, uncontrolled, and unpredictable client behaviors. Local datasets, for example, often
vary over time, and intermittent clients can join or depart during training without prior notice.
The difficulty in addressing the time-varying heterogeneity ofFL lies in the stateless nature of the local
datasets, which includes the unpredictable future datasets and the impossibility of retaining all prior
local datasets. To this end, we first provide a novel Continual Federated Learning (CFL) formulation
and then propose a unified CFL framework as our solution. This framework encompasses a variety of
design choices for approximating the local objective functions in the previous training rounds, with
the difference between actual and estimated local object functions described as information loss for
further analysis.
To capture the time-varying data heterogeneity in the CFL framework, we expand the theoretical
assumption on the client drift—which has been extensively utilized recently in Karimireddy et al.
(2020b); Khaled et al. (2020); Li et al. (2020b)—to include both client drift and time drift. This allows
us to quantify the difference between local client objective function and global objective function for
the considered time-varying heterogeneous clients.
We provide convergence rates for our unified CFL framework in conjunction with information loss and
new models of both client and time drifts. Our analysis reveals a faster convergence of CFL framework
than FedAvg, with the benefit dependent on approximation quality. The rate of FedAvg obtained from
1
Under review as a conference paper at ICLR 2022
our framework is consistent with previous work, while a similarly simplified rate on Continual Learn-
ing (CL) (French, 1999; Kirkpatrick et al., 2017) for the convex and strongly convex cases is novel.
Finally, in extensive empirical results, we demonstrate that CFL methods—stemmed from the CFL
framework with different approximation techniques—significantly outperform the SOTA FL competi-
tors on various realistic datasets. These numerical observations corroborate our theoretical findings.
Contribution. We summarize our key contributions below.
•	We present a unified framework, termed Continual Federated Learning (CFL), together with a
novel client and time drift modeling approach, to capture complex FL scenarios involving time-
varying heterogeneous data. This is the first theoretical study, to our knowledge, that describes the
time-varying nature of FL.
•	We provide rigorous convergence analysis for the CFL methods. Our theoretical analysis explains
the faster and more stabilized optimization of the CFL methods over that of FedAvg, and conjecture
their variance reduction effect. In addition, we provide tight convergence rates for standalone CL
methods on convex and strongly-convex problems: to the best of our knowledge, we are the first
to provide such guarantees for CL on SGD.
•	We examine several approximation techniques for CFL framework: the insights therein offer a
valuable practical guideline. We demonstrate the efficacy and necessity of CFL methods over the
SOTA FL baselines across a range of time-varying heterogeneous data scenarios and datasets.
2	Related Work
Federated Learning. FedAvg (McMahan et al., 2017; Lin et al., 2020b) is the de facto standard
FL algorithm, in which multiple local SGD steps are executed on the available clients to alleviate
the communication bottleneck. While communication efficient, heterogeneity, such as system
heterogeneity (Li et al., 2018; Wang et al., 2020; Mitra et al., 2021; Diao et al., 2021) and
statistical/objective heterogeneity (Li et al., 2018; Wang et al., 2020; Mitra et al., 2021; Lin et al.,
2020a; Karimireddy et al., 2020b;a), results in inconsistent optimization objectives and drifted clients
models, impeding federated optimization considerably.
A line of work has been proposed to address the heterogeneity in FL. FedProx (Li et al., 2018)
adds the regularization term on the distance of local and global models when performing local
training—similar formulations can be found in other recent FL works (Hanzely & Richtdrik, 2020;
Dinh et al., 2020; Li et al., 2021) for various purposes. To address the issue of objective heterogeneity
e.g. caused by heterogeneous data, works like SCAFFOLD (Karimireddy et al., 2020b; Mitra et al.,
2021) introduce the idea of variance reduction on the client local update steps. FedNova (Wang et al.,
2020) further proposes a general framework for unifying FedAvg and FedProx, and argues that while
averaging, local updates should be normalized to minimize heterogeneity induced by different number
of local update steps. However, most of these prior works focus on the fixed heterogeneity across
clients and throughout the entire optimization procedure; we instead consider the novel scenario with
time-varying data heterogeneity.
The theoretical study on the convergence of FedAvg can date back to the parallel SGD analysis on
the identical functions Zinkevich et al. (2010) and recently is improved by Stich (2019); Stich &
Karimireddy (2020); Patel & Dieuleveut (2019); Khaled et al. (2020); Woodworth et al. (2020b). For
the analysis of heterogeneous data, Li et al. (2020b) first give the convergence rate of FedAvg on
non-iid datasets with random selection, assuming that the client optimum are -close. Woodworth
et al. (2020a); Khaled et al. (2020) give tighter convergence rates under the assumption of bounded
gradient drift. All above works give a O (1/T) convergence rate for convex local objective functions.
More recently, Karimireddy et al. (2020b); Koloskova et al. (2020) give the convergence analysis of
local SGD for non-convex objective functions under bounded gradient noise assumptions, and obtain
a O(1∕√T) convergence rate. Our theoretical analysis framework covers more challenging time-
varying data heterogeneity in FL, which has not been considered in the community yet—our rate can
be simplified to the standard FL scenario, matching the tight analysis in Karimireddy et al. (2020b).
Continual Learning. Continual learning, also known as incremental learning or lifelong learn-
ing, aims to learn from (time-varying) sequential data while avoiding the problem of catastrophic
forgetting (French, 1999; Kirkpatrick et al., 2017). There exists a large amount of works, from
the perspectives of regularization (Kirkpatrick et al., 2017; Li & Hoiem, 2017; Zenke et al., 2017),
experience replay (Castro et al., 2018; Rebuffi et al., 2017), and dynamic architectures (Maltoni &
Lomonaco, 2019; Rusu et al., 2016). In this paper, we examine both regularization and experience
replay based methods, and compare their empirical performance in depth.
2
Under review as a conference paper at ICLR 2022
Despite the empirical success, the theoretical analysis ofCLis limited: only a very recent preprint (Yin
et al., 2020b) provides a viewpoint of regularization-based continual learning, and only for the single
worker scenario. In this work, we provide tight convergence analysis for both CL and CFL on SGD.
Continual Federated Learning. To our knowledge, the scenario of CFL was originally described
in Bui et al. (2018) in order to federated train Bayesian Neural Network and continually learn for
Gaussian Process models—it is orthogonal to our optimization aspect in this paper.
FedCurv (Shoham et al., 2019) blends EWC (Kirkpatrick et al., 2017) regularization with FedAvg,
and empirically shows a faster convergence. FedWeIT (Yoon et al., 2021) extends the regularization
idea, with a focus on selective inter-client knowledge transfer for task-adaptive parameters. In a very
recent parallel (and empirical) work, CDA-FedAvg (Casado et al., 2021) uses a locally maintained
long-term data samples, with asynchronous communication; however, this design choice lacks
sufficient numerical results to justify the performance gain (even over FedAvg) and it cannot be
applied to more general scenarios. Our theoretically sound CFL framework covers the regularization
component of FedCurv and the core set part of CDA-FedAvg, and is orthogonal to the neural
architecture manipulation idea in FedWeIT.
3	Continual Federated Learning Framework
3.1	Formulation
Conventional FL Formulation. The standard FL typically considers a sum-structured distributed
optimization problem as below:
f? = minω∈Rd hf (ω) := PiN=1pifi(ω)i ,	(1)
where the objective function f(ω) : Rd → R is the weighted sum of the local objective functions
fi(ω) := EDi [Fi(ω)] of N nodes/clients, and pi is the weight of client i.
In practice, it may be infeasible to select all clients each round, especially for cross-device
setup (McMahan et al., 2017; Kairouz et al., 2019). The standard FedAvg then randomly selects S
clients to receive the model parameters from the server (S ≤ N) in each communication round and
performs K local SGD update steps in the form of ωt,i,k = ωt,i,k-ι - ηιEfi(ωt,i,k-ι) + νt,i,k-ι),
with local step-size ηl and gradient noise νt,i,k-1. The selected clients then communicate the updates
∆ωt,i = ωt,i,κ — ωt with the server for the model aggregation: ωt+ι = ωt — ηg PS=I ∆ωt,i.
Continual FL Formulation. Despite its wide usage, the standard FL formulation given in Equation
(1) cannot properly reflect actual time-varying scenarios, such as local datasets changing over time
or intermittent clients joining or leaving during training. To address this problem, we propose the
Continual Federated Learning (CFL) formulation:
f? = minω∈Rd hf (ω) := PtT=1 Pi∈St pt,ift,i(ω)i ,	(CFL)
where ft,i(ω) represents the local objective function of client i at time t. St is a subset of clients
sampled from all clients set Ω, where |St| = S. For the time-varying scenarios, client i could have
different local objective functions ft,i (ω) due to the changing local datasets on different t.
3.2	Approximation of CFL
The challenge of addressing (CFL) formulation stems from the stateless nature of the local datasets,
which include unpredictable future datasets, as well as the difficulty of keeping all the previous local
datasets. The original definition of (CFL) is theoretically and empirically infeasible. To handle the
second case (while ignoring the intractable former), a straightforward approach is to approximate
the prior local objective functions (Kirkpatrick et al., 2017; Zenke et al., 2017; Li & Hoiem, 2017),
which may be accomplished by retraining information from earlier rounds in accordance with privacy
protection standards. Thus, we can express the approximated CFL formulation as:
ft* = minω∈Rd [ft3 ：= Pi∈st pt,ift,i(ω) + PT=I Pi∈st pτ,ifτ,i(ω)i ,	⑵
where ft,i (ω) denotes the approximated local objective function of client i at time t. In practice,
different approximation methods can be used to calculate fτ,i(ω), and we refer the detailed illustration
and discussions of these approximation algorithms in Section 5.1, Section 5.2, and Appendix C.2.2
We give the formal definition of CFL framework in Algorithm 1. CFL methods are a collection of
methods that make use of different approximation techniques and are based on Algorithm 1. We
retrieve the objective function of conventional Continual Learning (CL), by setting S = 1 in (2).
Take note that in most cases, the previous local object functions cannot be properly approximated.
Due to the fact that such approximation impairs optimization, we define the information loss below.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Approximated CFL Framework
Require: initial weights ω0, global learning rate ηg, local learning rate ηl, number of training rounds T
Ensure: trained weights ωT
1
2
3
4
5
6
7
8
9
10
for round t = 1, . . . , T do
communicate ωt to the chosen clients.
for client i ∈ St in parallel do
initialize local model ωt,i,o = ωt.
for k = 1 ,...,K do
gt,i,k (ωt,i,k-1 ) = (^Pt,iV ft,i (3t,i,k-I) + PT =1 pτ,iV fτ,i(ωt,i,k-1^)j + νt,i,k∙
ωt,i,k <- ωt,i,k-1 - ηlgt,i,k(ωt,i,k-1).
communicate ∆ωt,i — ωt,i,κ 一 ωt.
Zt 一 ηg Pi∈St ∆ωt,i.
ωt+ι — ωt + ∆ωt.
Definition 3.1 (Information Loss). We define the information loss as the difference between the
approximated local objective function ft,i (ω) and the real local objective function ft,i (ω), i.e.
∆t,i(ω) = Vft,i(ω) -^ft,i(ω) .	(3)
In practice, we use k∆t,i(ω)k2 to measure the information loss of different approximation methods.
We show large information loss can impede the convergence theoretically (c.f. Theorem 4.1) and
empirically (e.g. Figure 1 in Section 5.2).
3.3	Gradient Noise Model
To analyze Equation (2) and Algorithm 1 in-depth, we propose the Gradient Noise Model (Equation
(5)) to capture the dynamics of performing SGD with data heterogeneity. We first recap the standard
definition of gradient noise in Sammut & Webb (2010); Gower et al. (2019).
Gradient noise in SGD. For objective function f(ω), the gradient with stochastic noise can be
defined as Vf (ω) = g(ω) + V, where g(ω) is the stochastic gradient, and V is a zero-mean noise.
Client drift in FL. To analyze the impact of data heterogeneity, recent works (Karimireddy et al.,
2020b; Khaled et al., 2020; Li et al., 2020b) similarly use the gradient noise to capture the distribution
drift between local objective function and global objective function:
Vf (ω) = Vfi(ω) + δi ,	(4)
where δi is a zero-mean random variable which measures the gradient noise of client i.
Client drift and time drift in CFL framework. In (CFL), we use {t, i} pair instead of i to
represent client i at time t. Considering the distribution drift in the dimension of client and time, we
further modify the gradient noise model in FL as,
Vf (ω) = Vft,i(ω) + δt,i +ξt,i.	(5)
Note that we extend the gradient noise to two terms: δt,i and ξt,i. δt,i is a time independent (of t),
zero-mean random variable that measures the drift of client i. The zero-mean ξt,i measures the drift of
client i at time t: this value of client i may change over time t, due to the time-varying local datasets.
Remark 3.2. We assume different ξt,i are independent to simplify the analysis. We also empirically
examine the performance of CFL methods under overlapped time-varying local data in Section 5.2.
3.4	Assumptions
To ease the theoretical analysis of (CFL) framework, we use the following widely used assumptions.
Assumption 1 (Smoothness and convexity). Assume local objective functions ft,i (ω) are L-smooth
and μ-convex, i.e. 2 ∣M — ω2 ∣∣2 ≤ ft,i(ωι) — ft,i(ω2)一"加包),ωι 一 ω2≤ L ∣∣ωι 一 口2『.
A widely used corollary is that if a function ft,i is both L-smooth and μ-convex, then it satisfies
2L l∣Vft,i(x) — Vft,i(y)∣2 ≤ ft,i(X)- ft,i(y) — Vft,i(x)T(x — y), and L ≥ μ.
Assumption 2 (Bounded noise in stochastic gradient). Let gt,i,k (ω) = Vft,i,k (ω) + Vt,i,k, where
Vt,i,k is the stochastic noise of client i on round t at k-th local update step. We assume that
E [ν ∣ω] = 0, and E ∣∣ν ∣2 ∣ω] ≤ σ2.
4
Under review as a conference paper at ICLR 2022
Table 1: Number of communication rounds required to reach ε + 夕 accuracy for μ strongly convex and
general convex functions under time-varying FL scenarios (Assumption 1-4). We can recover the rates in
conventional FL setups by setting D = 0 and A = 0. Note that g=0 in FedAvg. Our convergence rate of
FedAvg matches the results in Karimireddy et al. (2020b). Our SGD rate of CL on strongly-convex case is novel.
Algorithm	Strongly Convex	General Convex
SGD		
	σ1 2	+ I μNKε + μ	σ2	+ 1 NKε2 + ε
FedAvg		
Li et al. (2020b)	σ2	+ (G2+D2)K μ2NKε +	μ2ε	-
Khaled et al. (2020)	σ2+G2+D2	+	σ+G+D	,	N(A2+B2) μNKε	+ μ√ε	+	μ	σ2+G2+D2	σ+G+D	∣	N(A2+B2) -NKε2	1	3	1	ε με 2
Karimireddy et al. (2020b)	σ2	+ G+D + CPB μNKε + μ√ε + μ	σ2	l G+D l Cpb (D2 + G2) KNε2 + ~2l	+	ε
Woodworth et al. (2020a)	√cPB) + σ2cPB + G+D + σ K√μ 十 NKe2 十 μ√e 十 μ√Ke	CPB I σ2cPB I (G+D)CPB I σCpB. 年 + 标丁 + F — + √K2
Ours	σ2	+ G2+D2 + √K+ G+D + CPB μN Kε -I-	με -I-	μ√ε	μ μ	CPB + σ2	+ G2+D2 + σ + g+d ε + NKε2 +	E2	1	^2
CL
Yin et al. (2020b)	μ (GD)	-
OurS	i+a2+μ⅛+μ+qμ+r⅞F + 惇	CpB+√cB+ +√c+忌+⅛2+r⅛A
CFL	一
OurS	cpB++cA+G2+乒+r 哈+cA+G2+q 疗 Cp+√cB+√cB+_2L++cA+G+r σ2+cA+G2
μ 1 μNKe 1 μe	μe	μ2e	μ2e	ε	1 N Kε2 1 ε2	ε3
CpB = 1 + B2 + A2, CA = RD2+D22, CB =(r2/6)2)2. N is the number of chosen clients in each round, and
K is the number of local iterations.
Assumption 1 and 2 are common assumptions in FL (Karimireddy et al., 2020b; Li et al., 2020b).
Considering a relaxed assumption like the bounded noise at optimum in Khaled et al. (2020) will be
an interesting future research direction. Besides, we introduce the following assumptions to better
characterize the client and time drifts in CFL framework (i.e. (5)).
Assumption 3 (Bounded gradient drift of CFL framework). Let f (ω) = Vft,i(ω)+δt,i + ξt,i, where
δt,i measures client drift and ξt,i indicates the time driftof {t, i}. We assume E [δ∣ω] = 0, E [ξ∣ω]=
0, and thus E |||b『∣ω] ≤ G2 + B2E∣∣Vf(ω)∣∣2 and E |||g『∣ω] ≤ D2 + A2E∣∣Vfi(ω)∣2.
We can derive the corollary from Assumption 3 that E |||g『∣ω] ≤ DD2 +A2G2 +A2B2E∣∣Vf (ω)∣2,
which can simplified to E |||g『∣ω] ≤ D2 + A2E∣∣Vf(ω)k2, by assuming A2 = A2B2, D2 =
D^2+A2g2.
Assumption 3 assumes the bounded E |||b『∣ω] and E |||g『∣ω] in CFL framework, which is
equivalent to the widely used (G, B) gradient drift assumption1 (Gower et al., 2019; Karimireddy
et al., 2020b) on the bounded E kVfi(ω)k2 . We prove this claim in Appendix B.1.
Assumption 4 (Bounded information loss). We assume the information loss ∆t,i(ω) can be bounded
by an arbitrary non-negative value R, i.e. k∆t,i (ω)k ≤ R.
Remark 3.3. The exact information loss in Assumption 4 may be intractable for some approximation
methods. In Lemma B.3 of Appendix B.3, we showcase a tight analysis of information loss for Taylor
extension based regularization methods.
4	Theoretical results
In this section, we analyze the theoretical performance of Algorithm 1 under Assumption 1-4. We
prove that CFL methods converges faster than FedAvg, despite that the term (refer to 夕t in Theorem
4.1) induced by information loss concurrently trades off the optimization. Additionally, our results
include the FedAvg rate (and match the prior work) and a novel convergence rate for SGD on CL.
In Table 1, we summarize the convergence rate of different algorithms2. The proof details refer to
Appendix B.4 and Appendix B.5.
4.1	Convergence rate of CFL methods
We start our analysis of Algorithm 1 from the one round progress of CFL methods in the Theorem
stated below.
1The (G,B) gradient drift: E [∣Nfi(ω)∣∣[ ≤ G2E [∣Nf(ω)∣∣[ + B2, where G2 ≥ 1 and B2 ≥ 0.
2To match the notations of prior works that include all clients in all training rounds, we use the abbreviation
N to denote the number of selected clients in each round in this section.
5
Under review as a conference paper at ICLR 2022
Theorem 4.1 (One round Progress of CFL methods). When {ft,i(ω)} satisfy Assumption 1-4, and
let η = Kηgηl, we have the one round progress of CFL methods as,
f (ωt) - f (ω*) ≤ 1(1 - μη)E∣∣ωt - ω*∣∣2 - 1 E∣∣ωt+ι - ω*∣∣2 + ∣ιη + C21j +夕t,	(6)
、------------------{-------------------'	A2
A1
Where ci = 2cPG+NK+2cR, c2 = LpG+6Lσκ+LnRL, cPG = N PN=I G2+D2(PT=ι p2,i) ,cr
N PN=I(PT=IL Pτ,i)2R2. ψt is a constant satisfying ψt = O " PN=1 PT=I Pt,iR kωo - ω*k
N is the number of chosen clients in each round, and K is the number of local iterations.
Theorem 4.1 shows the one round progress of CFL methods. The AL part of (6) indicates the
linear convergence rate, while the A2 part illustrates the worsened convergence induced by gradient
noise and information loss.夕t is a constant related to the information loss that causes the drift of
the optimum. When the upper limit on information loss R exceeds 0, we cannot reach the exact
optimum due to the approximation error: the iterates instead reach the neighborhood of e + 夕，where
中=¾Tfor some sequence qt. We show in Section 5 that in PraCtice,the approximation
methods with small information loss converge much better than methods with larger information loss.
Theorem 4.2 (Convergence rate of CFL methods). Assume {ft,i (ω)} satisfy Assumption 1-
4, the output of Algorithm 1 has expected error smaller than e + 夕,for η = 1, η ≤
v∕3+4(1+B2+A2)-v∕4(1+B2 + A2)
6KL	1+B2+A2
D2
, pτ,i = tD2 + (t-i)r2 (T < t), and pt,i
When {ft,i(ω)} are μ-strongly COnveXfunCtions, we have,
(t-1)R2 + D2
tD2 + (t-1)R2
on round t.
O L Lco + -σ__+
μ + μNKe +
+
(7)
T
and when {ft,i(ω)} are general COnveXfunCtiOnS (μ = 0), we have
co _ √ c cOF + √cB F+ √cD F 2 σ2F	CAF .	/ CC F：
T = O ----------6---------+ NK2 + ― + V -^3"
(8)
and when {ft,i(ω)} are non-convex, setting η = Kng ηι = √KN, and when T PT=I E [∣∣Vf (ωt)∣∣2]
Converge to e, we have
T = O LL2 (fo - f*)2 + ɪ ( √KNcRι + σ Y + √KNcr2 + (KN)1 CR2
一 ∖ NKc2ne2	e2 V L	√NK)	CmeL	(eL)3
(9)
where cO = 1 + A2 + B2, cA = G2 +
D2R2 cR	=	—D—	S	= Lσ2	+	Lca	cn	=	Lcb
R2 + D2, CB	=	(R2+D2)2,	CC	= ηg K	+	ηg '	CD	=	ηg
cRi =氏2：202, cr2 = (R2DD2)2, Cm is a constant related to A, B and R, and F = ∣∣ωo — ω*『.N
is the number of Chosen Clients in eaCh round, and K is the number of loCal iterations. We elaborate
the choice of pt,i in Appendix B.4.3.
Remark 4.3. When setting N = 1 in Theorem 4.2, we recover the convergence rate of standalone
CL methods. To the best of our knowledge, we are the first to provide such theoretical guarantees for
SGD: the recent work (Yin et al., 2020a) only gives the convergence rate of GD for regularization
based CL methods on general convex case, under the constraint of R = 0.
Remark 4.4. For stateless FL scenarios (clients only appear once during training), we can derive
tighter bounds than that of Theorem 4.2. Applying Lemma B.2 to the considered stateless scenario,
(G2 + D2)R2	(D2 + G2)3	c	ca I ΓCB、
CA and CB in Theorem 4.2 become ca，= *2 +d2：r2 and cp，= (R2+D2+G2)2, where CA + J μ ≥
CAO + ∖ 褰 and CA + ∖ CμB COrreSPOndS to the rate ofstrongly convex setting in Theorem 4.2.
4.2	Convergence rate of FedAvg under time-varying scenarios
To show the faster convergence of CFL methods than FedAvg, we provide the convergence rate of
FedAvg under time-varying scenarios. Results are derived from Theorem 4.1, by setting pt,i = 1.
Note that when pt,i = 1, we can set Wt to 0.
6
Under review as a conference paper at ICLR 2022
Theorem 4.5 (Convergence rate of FedAvg under time-varying scenarios). Assume {ft,i(ω)} satisfy
Assumption 1-4, the output of FedAvg has expected error smaller than e, for η = 1 and ηι ≤
√3+4(1+B2+A2)-,4(1+B2 + A2)
6KL	1+B2+A2
. When {ft,i(ω)} are μ-strongly convex functions, we have,
LcO I σ2 I cA I	/ CC
T = O ( μ + μNK + μe + V μ2 e
(10)
and when {ft,i(ω)} are general ConvexfunCtionS (μ = 0), we have
T = O (COF + N⅛ + cAF + qCF2) ,	(11)
where CO = 1 + A2 + B2, ca = G2 + D2, c0 = ^K + LcA, and F = ∣∣ωo 一 ω*k2. N is the
number of chosen clients in each round, and K is the number of local iterations.
Remark 4.6. CFL methods accelerates the convergence by reducing the variance term. As stated
in Theorem 4.2 and Theorem 4.5, FedAvg (under time-varying scenarios) is a special case of CFL
methods by setting pt,i = 1, and we can show by proof (in Appendix B.4) that it is equivalent to
setting the upper bound of information loss R → ∞ (we can also observe this result by setting
R → ∞ in pt,i = tD+)R-+)R2 in Theorem 4.2). The upper bound R is linearly correlated to
gradient noise, where CA = G2 + R2+R22 in Theorem 4.2 increases to CA = G2 + D2 in Theorem 4.5.
5 Experiments
In our numerical investigation, we first use the Noisy Quadratic Model—a simple convex model—to
verify our theoretical results stated in Section 4. The results (in Appendix C.1) demonstrate that CFL
methods converge much faster than baselines like FedAvg and FedProx, with a smoother convergence
curve. These findings are corroborate Remark 4.6 about the variance reduction effect of CFL methods.
We further provide extensive empirical evaluations below by comparing CFL methods with various
strong FL competitors on different realistic datasets. We explore several approximation techniques
in CFL methods and shed light on achieving practical efficient federated learning with time-varying
data heterogeneity.
5.1	Setup
Time-varying heterogeneous local datasets. We consider federated learn an image classifier us-
ing ResNet18 on split-CIFAR10 and split-CIFAR100 datasets, as well as a two layer MLP on the
split-Fashion-MNIST dataset. The “split” follows the idea introduced in Yurochkin et al. (2019); Hsu
et al. (2019); Reddi et al. (2021), where we leverage the Latent Dirichlet Allocation (LDA) to control
the distribution drift with parameter α (See Algorithm 4). Larger α indicates smaller drifts.
Unless specifically mentioned otherwise our studies use the following protocol. All datasets are parti-
tioned to 210 subsets for 7 distinct clients: all clients are selected and trained for 500 communication
rounds, and each client randomly samples one of the corresponding 30 subsets for the local training—
this challenging time-varying scenario mimics the realistic client data sampling scheme (from some
underlying distributions). Notably, the training strategy used in this section is applicable to all FL
baselines and CFL methods, and we assess the model performance using a global test dataset3. We
carefully tune the hyper-parameters in all algorithms (details refer to Appendix C.2.1), and report
the optimal results (i.e. mean test accuracy across the past 5 best epochs) after repeated trials.
Approximation techniques in CFL methods. Below, we review three representative types of
information approximation techniques in CL, and use them to investigate the effect of various
types of information loss under the (CFL) formulation. We refer to Appendix C.2.2 for a more
comprehensive introduction and discussion.
•	Regularization methods. Instead of keeping datasets from previous rounds, we keep track
the gradients and Hessian matrices of local objective functions, and use Taylor Extension to
approximate the local objective functions in previous rounds. Note that the trade-off between
Hessian estimation and computational overhead constrains the practical feasible of such approach.
•	Core set methods. Another simple yet effective treatment in CL lines in the category of Exemplar
Replay (Rebuffi et al., 2017; Castro et al., 2018). This method selects and regularly saves previous
core-set samples (a.k.a. exemplars), and replays them with the current local datasets.
3The training loss/accuracy on (CFL) formulation are closely aligned with that of global test data, as shown
in Figure 5 of Appendix C.2.3. We here only present the global test results for the common interests in practice.
7
Under review as a conference paper at ICLR 2022
Table 2: Top-1 accuracy for different choices of approximation techniques in CFL. We train ResNet18 on
split-CIFAR10 dataset (w/ α = 0.2) for 300 communication rounds, and the dataset is partitioned to 300 subsets
for 10 different clients. All examined algorithms use FedAvg as the backbone.
Algorithm	Regularization Methods		Core Set Methods				Generative Methods
	PyHessian	Fisher Information Matrix	Naive (Small Set)	Naive (Large Set)	iCaRL (Small Set)	iCaRL (Large Set)	MCMC
Accuracy	74.15 ± 0.66	73.70 ± 0.39	77.84 ± 0.06	78.90 ± 0.09	76.97 ± 0.16	77.09 ± 0.09	74.18 ± 0.08
Table 3: Top-1 accuracy of various CFL methods on diverse datasets for training ResNet18 with 500
communication rounds. In order to observe a noticeable performance difference on Fashion-MNIST, we
use α = 0.1 instead. All examined algorithms use FedAvg as the backbone. Both CFL-Regularization and
CFL-Regularization-Full are regularization based method, and the difference lies on where the regularization
is applied: the full version applies regularization to all layers while the other only considers the top layers. Rr
indicates the accuracy of CFL-Regularization, while Rf denotes the accuracy of FedAvg.
Algorithm	Accuracy on Fashion-MNIST (%)	Accuracy on CIFAR10 (%)	Accuracy on CIFAR100 (%)
FedAvg	86.75 ± 0.14	70.51 ± 0.19	49.97 ± 0.19
CFL-Regularization	87.02 ± 0.21	70.86 ± 0.31	50.69 ± 0.06
CFL-Core-Set	88.32 ± 0.12	81.48 ± 0.24	53.17 ± 0.08
CFL-Regularization-Full	77.37 ± 0.63	33.09 ± 1.45	14.84 ± 0.12
Metric	Improvement on Fashion-MNIST (%)	Improvement on CIFAR10 (%)	Improvement on CIFAR100 (%)
Absolute (Rr - Rf)	0.27	0.35	0.72
Ratio ((Rr - Rf)/Rf)	0.31	0.50	1.44
•	Generative methods. Maintaining a core set for each client may become impractical when learn-
ing scales to millions of clients. To ensure a privacy-preserved federated learning, the generative
models (Goodfellow et al., 2014) could be used locally to capture the local data distribution: fresh
data samples will be generated on the fly and combined with the current local dataset. For the
sake of simplicity, we use Markov Chain Monte Carlo (Nori et al., 2014) in our assessment.
5.2	Results
Comments on different CFL approximation techniques. In Table 2, we examine the perfor-
mance of several approximation techniques using the split-CIFAR10 dataset. We can conclude that
(1) Core set methods outperform other methods by a significant margin. The simple choice of “naive
core set”, i.e. randomly and uniformly sample data from the local dataset, surpasses the sampling
technique described in iCaRL (Rebuffi et al., 2017)4 for CL, despite their faster convergence in
the initial training phase. (2) The quality of Hessian estimation matters for regularization based
methods. PyHessian (Yao et al., 2020), as a method to approximate diagonal Hessian matrix, is
slightly preferable than Fisher Information Matrix, though the latter one involves less computation.
(3) The performance of generative methods is restricted, and we hypothesize that the poor quality of
generated samples contributes to the constraint.
In the subsequent evaluation, we consider naive core set sampling for CFL-Core-Set method, and use
PyHessian for CFL-Regularization. We exclude the results of generative methods, due to the trivial
performance gain and significant computational overhead.
Understanding various approximated CFL implementations on different datasets. Table 3
experimentally studies the impacts of various information approximation techniques in CFL methods—
as discussed in Section 5.1—and compares them with the backbone algorithm of CFL methods, i.e.
FedAvg. We have the following consistent findings on different datasets.
1.	The improvement of CFL methods over FedAvg becomes larger for more challenging tasks, as
shown in the bottom of the Table 3 for regularization based methods. This might reflect the
fact that the precision of the information approximation is more crucial for complicated tasks.
Similarly, we demonstrate in Figure 6 (in Appendix C.2.3) that CFL methods offer better resistance
to time-varying non-iid data and the significance of such finding is depending on the task difficulty.
2.	For CFL-Regularization method, applying regularization terms on top layers works better than
that on all layers. This observation matches the recent work on decoupling feature extractor and
classifier (Collins et al., 2021; Chen & Chao, 2021; Luo et al., 2021): the bottom layers are more
generic across tasks and can serve as a global feature extractor, while the top layers are subject to
task-specific information.
3.	The core-set method consistently outperforms FedAvg and other CFL approximation techniques
by a large margin.
We further investigate the connection between the information loss and the learning performance. We
examine CFL methods with naive core sets, where the degree of information loss5 can be changed
4The algorithmic details of the sampling method in iCaRL refer to Algorithm 3 in Appendix C.2.2.
5We estimate the information loss by using 焉 PS=I PT=ι k∆τ,i(ω)k, where ∆τ,i(ω) is defined in
Equation (3), S is the number of clients chosen in each round, and t is the number of communication rounds.
8
Under review as a conference paper at ICLR 2022
by altering the core set size from 20 to 150, with the same random seed and optimizers. Figure 1
depicts that the performance of models is highly linked to the value of information loss: the lesser the
information loss, the higher the performance.
Superior performance of CFL methods over other strong FL baselines. Alongside the com-
parison between CFL methods and FedAvg on various datasets (Table 3), in Table 4 we verify the
efficacy of CFL methods over other strong FL baselines, on split-CIFAR10 dataset. We also examine
MimeLite (Karimireddy et al., 2020a), a method suggested for stateless FL scenarios. Note that
we exclude the results illustration for methods like SCAFFOLD (Karimireddy et al., 2020b) and
Ditto (Li et al., 2021), due to their infeasibility to be applied in our continual scenarios6.
We further relax the difficulty of federated	^	π.
continual learning, from challenging non-
overlapping time-varying heterogeneous data
(e.g. in Table 2 and Table 3) to a moderate
time-varying case (i.e. the local data evolves
with the overlapping, while the size of local
datasets stay unchanged). Table 5 illustrates the
performance of FL baselines and CFL methods,
under different degrees of overlapping (the
construction details refers to Appendix C.2.1):
the improvement of CFL methods is consistent
to our previous results, while the overlap
parameter has no obvious connection with the final global test performance. We believe that both
the overlap degree and the new arriving data influence final performance, and we leave future work
on realistic time-varying FL datasets to gain a better understanding.
Table 4: Comparing the SOTA FL baselines with several CFL methods, for training ResNet18 on split-
CIFAR10 dataset with different degrees of non-iid-ness α (and with total 500 communication rounds).
FL baselines	CFL methods
Accuracy ____________________________________ _______________________________________________________
FedAvg FedProx MimeLite CFL-Core-Set CFL-Regularization CFL-Regularization + FedProx
α = 0.1	70.51 ± 0.19	71.57 ± 0.17	69.55 ± 0.36	81.48 ± 0.24	70.86 ± 0.31	71.50 ± 0.07
α=0.2	77.98 ± 0.36	78.40 ± 0.22	78.65 ± 0.26	84.41	± 0.11	78.76 ± 0.26	79.04 ± 0.01
E 0.02
i
0.01 L
20
0.8
—1----------1----------1----------1----------1---------1----------0.75
40	60	80	100	120	140	160
Core Set Size
Figure 1: Information loss (left y-axis) and accuracy
(right y-axis) for training ResNet18 on split-CIFAR10
with different core set sizes (x-axis) and α = 0.2.
Table 5: Benchmarking FL baselines and CFL methods on different degrees of local dataset overlapping,
for training ResNet18 on split-CIFAR10 dataset. The overlap reduces the degree of non-iid-ness. In order to
observe a noticeable performance difference, we use a larger data distribution gap between rounds (i.e. α = 0.1).
Overlap	FL baselines		CFL methods	
	FedAvg	MimeLite	CFL-Core-Set	CFL-Regularization
0%	80.66 ± 0.40	80.78 ± 0.07	84.87 ± 0.11	81.27 ± 0.38
25%	80.16 ± 0.01	80.17 ± 0.11	84.66 ± 0.11	80.67 ± 0.27
50%	79.97 ± 0.23	80.35 ± 0.39	84.59 ± 0.07	80.91 ± 0.25
Investigating other time-varying scenarios. In previous numerical investigations, we examine
the stateful clients and assume that these clients would learn and retain their client states throughout
the training process. The remainder of this section will discuss how to learn with stateless clients (i.e.
each client only appear once). To do this, we change the data partition strategy such that fresh data
partitions are sampled on the fly for each client and communication round.
Table 6 evaluates the above-mentioned scenario. We find that CFL-Regularization methods consis-
tently outperform alternative FL baselines, similar to the observations in Table 4. However, due to the
nature of stateless under the privacy concern, the advancements of core set method in CFL algorithm
cannot be seen in this scenario. Additionally, we discuss the applicability of different algorithms in a
variety of time-varying scenarios in Table 8 of Appendix C.2.3.
Table 6: Learning with stateless clients, regarding training ResNet18 on split-CIFAR10 with α = 0.2. All
examined algorithms use FedAvg as the backbone. The details w.r.t. CFL-Regularization refer to Appendix C.2.2.
Algorithm	FL baselines			CFL methods
	FedAvg	MimeLite	FedProx	CFL-Regularization CFL-Regularization + FedProx
Accuracy	78.07 ± 0.06	78.47 ± 0.19	78.48 ± 0.05	79.07 ± 0.25	79.32 ± 0.07
6We also naively adapted and examined these methods, but we cannot observe significant performance gains.
9
Under review as a conference paper at ICLR 2022
References
Thang D Bui, Cuong V Nguyen, Siddharth Swaroop, and Richard E Turner. Partitioned variational
inference: A unified framework encompassing federated and continual learning. arXiv preprint
arXiv:1811.11206, 2018.
Fernando E. Casado, Dylan Lema, Marcos F. Criado, Roberto Iglesias, Carlos V. Regueiro, and
Senen Barro. Concept drift detection and adaptation for federated and continual learning. CoRR,
abs/2105.13309, 2021. URL https://arxiv.org/abs/2105.13309.
Francisco M Castro, Manuel J MaHn-Jim6nez, Nicolds Guil, Cordelia Schmid, and Karteek Alahari.
End-to-end incremental learning. In Proceedings of the European conference on computer vision
(ECCV), pp. 233-248, 2018.
Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning. arXiv
preprint arXiv:2107.00778, 2021.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared represen-
tations for personalized federated learning. In arXiv preprint arXiv:2102.07078, 2021.
Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S Dhillon, and Ufuk
Topcu. Faster non-convex federated learning via global and local momentum. arXiv preprint
arXiv:2012.04061, 2020.
Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient feder-
ated learning for heterogeneous clients. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=TNkPBBYFkXg.
Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau
envelopes. In Advances in Neural Information Processing Systems, 2020.
Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3
(4):128-135, 1999.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtdrik. Sgd: General analysis and improved rates. In International Conference on Machine
Learning, pp. 5200-5209. PMLR, 2019.
Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated
learning with compression: Unified analysis and sharp guarantees. In International Conference on
Artificial Intelligence and Statistics, pp. 2350-2358. PMLR, 2021.
Filip Hanzely and Peter Richtdrik. Federated learning of a mixture of global and local models. arXiv
preprint arXiv:2002.05516, 2020.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U
Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in
federated learning. arXiv preprint arXiv:2008.03606, 2020a.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020b.
10
Under review as a conference paper at ICLR 2022
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local Sgd on identical
and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pp.
4519-4529. PMLR, 2020.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521-3526, 2017.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized sgd with changing topology and local updates. In International Conference
on Machine Learning, pp. 5381-5393. PMLR, 2020.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020a. URL https://openreview.net/
forum?id=ByexElSYDr.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning, pp. 6357-6368. PMLR,
2021.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations, 2020b. URL
https://openreview.net/forum?id=HJxNAnVtDS.
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935-2947, 2017.
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model
fusion in federated learning. In Advances in Neural Information Processing Systems, 2020a.
Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. In International Conference on Learning Representations, 2020b. URL https:
//openreview.net/forum?id=B1eyO1BFPr.
Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity:
Classifier calibration for federated learning with non-iid data. arXiv preprint arXiv:2106.05001,
2021.
Davide Maltoni and Vincenzo Lomonaco. Continuous learning in single-incremental-task scenarios.
Neural Networks, 116:56-73, 2019.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Achieving linear convergence in
federated learning under objective and systems heterogeneity. arXiv preprint arXiv:2102.07053,
2021.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97
of Proceedings of Machine Learning Research, pp. 4615-4625. PMLR, 2019. URL http:
//proceedings.mlr.press/v97/mohri19a.html.
Aditya Nori, Chung-Kil Hur, Sriram Rajamani, and Selva Samuel. R2: An efficient mcmc sampler
for probabilistic programs. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 28, 2014.
11
Under review as a conference paper at ICLR 2022
Kumar Kshitij Patel and Aymeric Dieuleveut. Communication trade-offs for synchronized distributed
sgd with large step size. In Advances in Neural Information Processing Systems, 2019.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classifier and representation learning. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pp. 2001-2010, 2017.
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=LkFG3lB13U5.
Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of affine distribution shifts. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Pro-
cessing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/
paper/2020/hash/f5e536083a438cec5b64a4954abc17f1-Abstract.html.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.
Claude Sammut and Geoffrey I. Webb (eds.). Neuro-Dynamic Programming, pp. 716-716. Springer
US, Boston, MA, 2010. ISBN 978-0-387-30164-8. doi: 10.1007/978-0-387-30164-8_588. URL
https://doi.org/10.1007/978-0-387-30164-8_588.
Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron Mor-Yosef, and Itai
Zeitak. Overcoming forgetting in federated learning on non-iid data. CoRR, abs/1910.07796, 2019.
URL http://arxiv.org/abs/1910.07796.
Sebastian U. Stich. Local SGD converges fast and communicates little. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
S1g2JnRcFX.
Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd
with delayed gradients and compressed updates. Journal of Machine Learning Research, 21:1-36,
2020.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020.
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous
distributed learning. In NeurIPS 2020 - Thirty-fourth Conference on Neural Information Processing
Systems, 2020a.
Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan,
Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International
Conference on Machine Learning, pp. 10334-10343. PMLR, 2020b.
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W. Mahoney. Pyhessian: Neural networks
through the lens of the hessian. In Xintao Wu, Chris Jermaine, Li Xiong, Xiaohua Hu, Olivera
Kotevska, Siyuan Lu, Weija Xu, Srinivas Aluru, Chengxiang Zhai, Eyhab Al-Masri, Zhiyuan Chen,
and Jeff Saltz (eds.), IEEE International Conference on Big Data, Big Data 2020, Atlanta, GA, USA,
December 10-13, 2020, pp. 581-590. IEEE, 2020. doi: 10.1109/BigData50022.2020.9378171.
URL https://doi.org/10.1109/BigData50022.2020.9378171.
Dong Yin, Mehrdad Farajtabar, and Ang Li. SOLA: continual learning with second-order loss
approximation. CoRR, abs/2006.10974, 2020a. URL https://arxiv.org/abs/2006.
10974.
12
Under review as a conference paper at ICLR 2022
Dong Yin, Mehrdad Farajtabar, Ang Li, Nir Levine, and Alex Mott. Optimization and generaliza-
tion of regularization-based continual learning: a loss approximation viewpoint. arXiv preprint
arXiv:2006.10974, 2020b.
Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual
learning with weighted inter-client transfer. In International Conference on Machine Learning, pp.
12073-12086. PMLR, 2021.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International
Conference on Machine Learning, pp. 7252-7261. PMLR, 2019.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In International Conference on Machine Learning, pp. 3987-3995. PMLR, 2017.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E Dahl, Christo-
pher J Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. In Advances in Neural Information Processing Systems, 2019.
Martin Zinkevich, Markus Weimer, Alexander J Smola, and Lihong Li. Parallelized stochastic
gradient descent. In NIPS, volume 4, pp. 4. Citeseer, 2010.
13
Under review as a conference paper at ICLR 2022
Contents of Appendix
A Techniques	14
B Convergence rate of CFL	15
B.1	Bound of Gradient Noise ................................................. 15
B.2	Bounded Gradient Noise of CFL ........................................... 15
B.3	Bounded Approximation Error ............................................. 16
B.4	Proof of Theorem 4.2 .................................................... 16
B.4.1	Startup........................................................... 17
B.4.2	One Step Progress ................................................ 17
B.4.3	Weights ofrounds.................................................. 20
B.4.4	Convergence results .............................................. 20
B.5	Proof of convergence rate for non-convex setting ........................ 21
C Experiment details	25
C.1	Noisy quadratic	model ...................................................... 25
C.2	Realistic datasets ......................................................... 26
C.2.1	Setup ............................................................... 26
C.2.2	Approximation Methods ............................................... 27
C.2.3	Additional Experiments .............................................. 28
C.3	Algorithms ................................................................. 28
A Techniques
Here we show some technical lemmas which are helpful in the theoretical proof.
Lemma A.1 (Linear convergence rate (Karimireddy et al., 2020b, Lemma 1)). For every non-
negative Sequence {dr-ι}r≥ι, and any parameter μ > 0, T ≥ ?叮 1 *, there exists a Constant
step-size η ≤ ηmaχ and weights ωt = (1 一 μη)1-t such thatfor WT = PT+ι1 ωt
T+1
Φt = 一 X
Wt ⅛
(ωF(I-μη)dtτ一 ωtdt
O(μdοexp(-μηmaxT)).
(12)
Lemma A.2 (Sub-linear convergence rate (Karimireddy et al., 2020b, Lemma 2)). For every non-
negative sequence {dr-1}r≥1 and any parameters ηmax > 0, c1 ≥ 0, c2 ≥ 0, T ≥ 0, there exists a
constant step-size η ≤ ηmax such that
φT=τ+ι X (筌 一 dt+c1η+c2η2
d0
2 √C1d0
≤ ηmax(T + 1) + √T+Γ
2
+ 2 (T⅜1) c23.
(13)
Lemma A.3 (Relaxed triangle inequality (Karimireddy et al., 2020b, Lemma 3)). Let {v1, ..., vτ}
be τ vectors in Rd. Then the following is true:
1τ	1τ
k 1 X Vik2 ≤ 1 X kVik2 .	(14)
Lemma A.4 (Separating mean and variance (Stich & Karimireddy, 2020, Lemma 14)). Let
Ξ1, Ξ2, ..., Ξτ be τ
random variables in Rd which are not necessarily independent.
First sup-
pose that their mean is E[Ξi] = ξi, and variance is bounded as EkΞi 一 ξik2 ≤ Mkξik2 + σ2, then
the following holds
τ
EX Ξi
i=1
2
τ
i=1
τσ2
(15)
Lemma A.5 (Perturbed strongly convexity (Karimireddy et al., 2020b, Lemma 5)). The following
holdsfor any L-smooth and μ-strongly Convexfunction h, and any X, y, Z in the domain of h:
(Vh(x),z — yi ≥ h(z) — h(y) + 4∣∣y 一 z∣∣2 — LkZ — x∣∣2.	(16)
Lemma A.6. Define S = PT=ι pt where PT=IPt = 1, and Pt ≤ pt+ι then S ≤ T PT=I ɪ =
O (lnT).
14
Under review as a conference paper at ICLR 2022
B Convergence rate of CFL
B.1	Bound of Gradient Noise
Lemma B.1 (Bound of Gradient Noise). Suppose fi(ω) is the local objective function, and f(ω) =
E fi(ω)] is the global objective function. Define Vf (ω) = Vfi (ω) + ςi, E&] = 0, and assume
E [∣∣ςk2 ∣ω] ≤ A2E [∣∣Vf (ω)k2] + B2, we have
E kVfi(ω)k2 ≤ (A2 + 1)E kVf(ω)k2 +B2.	(17)
Proof.
EhkVfi(ω)k2i =EhkVf(ω)+ςik2i	(18)
= E hkVf(ω)k2i +E hkςik2i	(19)
≤ (A + 1)E hkVf (ω)k2i +B.	(20)
□
B.2	Bounded Gradient Noise of CFL
Lemma B.2 (Bounded Gradient Noise of CFL). For Formulation (CFL), consider CFL and FedAvg,
we can bound the gradient drift as
(1)	FedAvg only optimize current local objective functions, thus,
EkVft,i(ω)k2 ≤ (1+ A2 + B2)E∣Nf(ω)k2 + G2 + D2 .
(2)	For CFL, in general (same clients can appear in different rounds), we have
t 2
EX pτ,iVfτ,i(ω)	≤
τ=1
1 + B2 + A2 Xt pτ2,i	EkVf(ω)k2
t
+G2+D2Xpτ2,i.
τ=1
(3)	For CFL, when clients only participate in training once(δt,i are independent for all t and i), we
have
EXt pτ,iVfτ,i(ω)	≤	1+(B2+A2)Xt pτ2,i! EkVf(ω)k2
τ=1	τ=1
t
+ (G2+D2)Xpτ2,i.
τ=1
Proof. Using Assumption 3, together with the fact that Vft,i (ω) = Vf (ω) + δt,i + ξt,i, we have
EkVft,i(ω)k2 =EkVf(ω)+δt,i+ξt,ik2	(21)
=EkVf(ω)k2+Ekδt,ik2+Ekξt,ik2	(22)
≤ (1+A2+B2)EkVf(ω)k2+G2+D2.	(23)
The second inequality is based on the independence of noise, and directly use Assumption 3 we get
the last inequality. Similarly, we have
t
EX pτ,iVfτ,i(ω)
τ=1
2 t 2
= EX pτ,i (Vfτ,i(ω) + δi + ξτ,i)
τ=1
t
EkVf (ω)k2 + Ekδik2 + EX pτ,iξτ,i
τ=1
2
≤	1+B2+A2Xt pτ2,i	EkVf(ω)k2+G2+D2Xt	pτ2,i.
τ=1	τ=1
(24)
(25)
(26)
15
Under review as a conference paper at ICLR 2022
For the last inequality, when δt1,i and δt2,i are independent for all t1, t2,
t
E XPτ,iVfr,i(ω)
τ=1
2 t 2
= EX pτ,i(Vfτ,i(ω) + δt,i + ξτ,i)
τ=1
t
EkVf(ω)k2+EXpτ,iδt,i
τ=1
2 t 2
+ E	pτ,iξτ,i
τ=1
≤	1+ (A2+B2)Xt pτ2,i	EkVf (ω)k2 + (G2 + D2) Xt	pτ2,i.
τ=1	τ=1
(27)
(28)
(29)
□
B.3	Bounded Approximation Error
Lemma B.3 (Bounded Approximation Error). For ∆t,i (ω) = Vft,i(ω)-Vft,i(ω) (c.f. Definition 3),
when use Taylor Extension,
▽九i(ω) = Vft,i(ωt) + V2 ft,i(ωt)(ω - ωt),
we have
k∆t,i(ω)k≤ € kω - ωt,ik .
Proof.
k∆t,i(ω)k = ∣∣Vft,i(ω) -Vft,i(ω)∣∣	(30)
= kVft,i(ω) - Vft,i (ωt,i,K) - HtjK(ω - ωt,i,K)k	(31)
= ∣∣V2ft,i(ωξ,t)(ω - ωt,i,K) - HtjK (ω - ωt,i,K)∣∣	(32)
= ∣∣(V2ft,i(ωξ,t) -HtjK)(ω-ωt,i,K)∣∣	(33)
≤ kω - ωt,i,K k .	(34)
The first two equations come from the mean value theorem which says for continuous function f in
closed intervals [a, b] and differentiable on open interval (a, b), there exists a point c ⊆ (a, b) such
that
f 0(c) = f (b) - f (a)
b-a
The last inequality is based on Assumption 4.
(35)
□
B.4	Proof of Theorem 4.2
In this section we will give the complete proof of convergence rate of algorithm 2 when the local
objective functions are convex.
Algorithm 2 CFL Framework
Require: initial weights ω0, global learning rate ηg, local learning rate ηl
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
for round t = 1, . . . , T do
communicate ωt to the chosen clients.
for client i ∈ St in parallel do
initialize local model ωt,i,0 = ωt.
for k = 1,..., K do
gt,i,k (ωt,i,k-I) = (Pt,iV ft,i(ωt,i,k-1) + PT =1 pT,iV ft,i (ωt,i,k-1
ωt,i,k《-ωt,i,k-1 一 ηlgt,i,k (ωt,i,k-1 ).
communicate ∆ωt,i — ωt,i,κ - ωt.
Zt J ηSg Pi∈St ∆ωt,i.
ωt+ι J ωt + ∆ωt.
νt,i,k.
16
Under review as a conference paper at ICLR 2022
B.4. 1 Start up.
The local objective function of round t on client i is
t-1
ft,i(ω) = (pt,ift,i(ω) + Xpτ,ifτ,i(ω).	(36)
τ=1
Without loss of generality, assume Ptτ=1 pτ,i = 1. Then follow the steps in algorithm 2, we have
t-1
V九i(ω) = PtNftM3)+ XPτ,iVfΓ,i(ω).	(37)
τ=1
Then the noisy gradient in mini-batch SGD can be described as
gt,i,k(3t,i,k-1) = V ft,i(3t,i,k-1) + νt,i,k	(38)
Then follow the steps in algorithm 2, we have
NK
∆3t=-Nηg XXgt,i(3t,i,k-1) ,	(39)
i=1 k=1
NK
E[∆ωt] = -Nηg XXVft,i(3t,i,k-1) .	(4O)
Then let η = Kηlηg, we have
NK
δ5 = N⅛EE%i(3t,i,k-1) ,	(41)
i=1 k=1
NK
E@3t] = nK XX vft,i(ωt,,i,k-ι).	(42)
i=1 k=1
B.4.2 One Step Progress
Consider the one-step progress we have
E∣∣3t + ∆3t - 3*k2 = kωt - ω*k2 - 2Ehωt - ω*, ∆ω∕ + Ek∆ωt∣∣2 .	(43)
`-------V-------} `---V----}
A1	A2
Firstly we consider A1 part in equation (43),
—2E hωt — ω*, ∆ωti
2η	N K NKE EWft,i(3t,i,k-1) 3* - 3ti			(44)
2η NK	i=1 k=1 N K	t-1 EEhPt,iVft,i(3t,i,k>+ EpT,iVfτ,i(3t,i,k ), 3* - 3ti i=1 k=1	τ=1		(45)
2η NK	NK t X X XhpT,iVfT,i(3t,i,k), 3* - 3ti i=1 k=1 T=1	N K t-1 -NK XXX pT,i^Ti(3t,i,k),3* i=1 k=1 T=1	- 3ti (46)
N K t	N t-1
≤ NK XXXpT,ihvfτ,i(ωt,i,k),ω*-ωti + NnXXpτ,iRIi3*- ωtk.	(47)
i=1 k=1 τ=1	i=1 τ=1
We Use equality (42) for first inequality, and directly use Assumption 4 and CaUchy-SchWarz
inequality for last inequality.
Then We can consider the first term of inequality (47). Firstly, by Lemma A.5 We see that
hV fτ,i(3t,i,k-1), 3* - 3ti ≤ fτ,i(3* ) - fτ,i(3t) - 4 k3t - 3*k2 + Lk3t,i,k-1 - 3tk2 . (48)
17
Under review as a conference paper at ICLR 2022
That is, based on (47) and (48), we have,
一 2E(ωt - ω*, ∆ω∕
μ	C	2ηL ΛΛ	C
≤ -2η(ft (ωt) - ft(ω ) + 4 kωt - ω Il ) + NK XX kωt,i,k-1 - ωtk
i=1 k=1
t-1
+ 2η XPτ,iR ∣∣ω* - ωtk .
τ=1
(49)
Then consider A2 in (43), anddefinecpB,i = 1+B2+A2(Ptτ=1pτ2,i), cpG,i = G2+D2(Ptτ=1 pτ2,i),
CpB = N Pi=1 CpB ,i, and CpG = N Pi=1 cpG,i We have
Ek∆ωtk2
Lem A.4
≤
Lem A.3
≤
N K	t-1
ΣΣ ∖pt,ik ft,i(ωt,i,k-1) + X : pτ,iN fτ,i(ωt,i,k-1) + νt,i,k
N K t	t-1
XX X
pτ,ik fτ,i(ωt,i,k-1) - X : pτ,i△『,ilatZk - l) +
NK
i=1 k=1
t	t-1
EpEfT,i(ωt,i,k-1 -Epτ,i ∆τ,i (ωt,i,k-1 )
τ=1	τ=1
2NK
红yy E
nk i=1 k=1 \
t
EpTNfT,i(ωt,i,k-1
τ=1
2 + η2σ2
+ NK
2	t-1
+ E	pT,i∆T,i(ωt,i,k-1)
T=1
+ η2σ2
+ NK
(50)
(51)
(52)
(53)
NK E
NKE
NK X E
2 N K	t	t	2 N t-1	2 2
≤ NKXX ((1+B2+A2(Xpτ,i)) Ekvf(ωt,i,k-ι)k2 + (g2+D2(Xpτ,i)j j+弋X(Xpτ,i)2R2 + NK
i=1 k=1	T=1	T=1	i=1 T=1
(54)
Lem A 3	2 N K	2 N t-1	2 2
≤ V‰ XX CpB ,i (EkVf (ωt,i,k-1) - vf (ωt)k2 + Ekvf (ωt)k2) + 2η2cPG + ɪ X(X pτ,i)2 R2 + η^-f7
NK	N	NK
i=1 k=1	i=1 T=1
(55)
Ass 1	2	2 N K	2 N t-1	2 2
≤	16η2 LcpB (f (ωt) - f (ω*)) + η,κ X X CpB ,iEkωt,i,k-1 - ωtk2 + 2η2cpG + ~η- X(X pτ,i)2R2 + "nk .
NK	N	NK
i=1 k=1	i=1 T=1
(56)
Then consider equation (49) and (56), to solve EIωt,i,k-1 - ωt I2, We have
k
Ekωt,i,k - ωtk2 = n2E Xgt,i(ωt,i,τ-1)
τ=1
Lem A.4
≤
k
kη2 X EIIVft,3t,i,τ-ι)∣∣2 + kη2σ2.
τ=1
Here We directly use Lemma A.4 to get inequality (58).
(57)
(58)
2
18
Under review as a conference paper at ICLR 2022
Then for ElMft,i(ω)∣∣2, We have
EllW⅛(ω)∣∣2
t-1
Pt,iVft,i(ω) + X Pτ,iVfτ,i(ω)
τ=1
t-1
E Epa- Epτ,i∆τ,i(ω)
τ=1
Lem A.3
≤	2E
Lem ??
≤
(59)
(60)
τ=1
l2
t-1
Epτ,"fτ,i(ω)	+2E fpm
τ=1
τ=1
t-1
2cpB,iEkVf(ω)k2+2cpG,i+2(	pτ,i)2R2
τ=1
t-1
Ass 1
≤ 4L cpB,i Ekω - ωtk + 8LcpB,i (f(ωt) - f(ω )) + 2cpG,i + 2( pτ,i) R .
τ=1
Combine equation (58) and (63) we get,
Ekωt,i,k - ωt k
(61)
(62)
(63)
t-1
≤ 4k2η2L2cpB,iEkωt,i,k-1 - ωtk2 + 8k2η2LcpB,i(f(ωt) - f (ω*)) + 2k2η2cpG,i + 2k2η2(f Pτ,i)2R2 + kη2σ2 .
τ=1
(64)
That is,
Ekωt,i,k - ωt k
k-1
t-1
≤ 汇(8K2η2LcPB,i(f(ωt) - f (ω*))+ 2K2η2cpG,i+ 2K2n2(£Pτ,i)2R2 + κ*σX4KηlLcpB,i)τ
τ=0
τ=1
E
t
t
2
2
2
(65)
≤ 8Kl2ηHcpB,i(f(ωt) - f (3*))+ 2 *K2η2cpG,i+ 2K2η2(PT=IPτ,i)2R2 + Kη2σ2
-	1 - 4K2η2 L2cpB ,i	.
Combine the equation (43), (49), (56), and (66), We get
Ekωt + ∆ωt - ω*k2
≤ (1 - 等)Ekωt - ω*k2 + (-2η + 16η2LcpB + N X 16KTUK^+:LcpB"i
2	N i=1	1 - 4K ηl L cpB ,i
(66)
)(f(ωt)- f *)+2η2CpG
2 2	N	t-1
+NK+N X (2η2(X pτ,ii2+
i=1	τ=1
4K2ηηl2L(Ptτ-=11 pτ,i)2(1 + 4ηLcpB,i)
1 - 4K2 η2 L2 cpB ,i
R2
1 X 2Kηη2L(2KcpG,i + σ2)(1+ 4ηLcpB ,i)	2η X X PIl ,.	*∣∣
+ N i=1	1 - 4K2η2L2cpB,i	+ N ⅛T=1 pτ,iRkωt-ω k .
Then We notice that
-2η +16η2LcpB + N X 16K2η*KB工 +4ηLcpBB,i ≤ 0.
N i=1	1 - 4K ηl L cpB ,i
Use the fact that η = Kηlηg, and solve above inequality, We notice that if η satisfy
(67)
(68)
η≤
3ηg2 +4cpB,iηg4 -	4cpB,iηg4
6L√cPB ,i
(69)
19
Under review as a conference paper at ICLR 2022
for any CpB,i, convergence can be promised. Define CpB as the largest CpB,i, let η ≤
√3η2 +%B%√4⅛⅛, We have 1 - 4K2η2L2cpB,i ≥ 33-8n2% +4嚷衿日 +4WpB , and 眦n
12LyfcPB
1+4ηLcPB,i	≤ J__________________7____________ ≤ 1
1-4K2η2L2cPB,i - 12 ~ 33+√3η2cpB +4η"pB-2η2cPB ^ 12.
Define CR,i	=	(PT=I Pτ,i)2R2,	and	CR	= N	PN=ICR,i. Assume	ci	=	2cpG	+ NK +	2cr,
Lcp Lσ2 Lc	2 N t-1
C2 =	^nG +	6η∣K	+ Lnf,	and C3	= N ∑i=i	∑τ=1 Pτ,iR,	inequality (67) become
E∣∣ωt + ∆ωt - ω*∣∣2 ≤ (1 - μ)E∣∣ωt - ω*∣∣2 - η(f(ωt) - f*) + Cin2 + c2η3 + c3η ∣∣ωt - ω*∣∣ .
(70)
Move f(ωt) - f * to the left side, we get
f(ωt) - f(ω*) ≤ 1(1 - μn)E∣∣ωt - ω*∣2 - 1 E∣ωt+ι - ω*∣2 + Cin + 5能 + Wt ∙	(71)
η2	η
Here Wt is a constant bounded by C3 ∣ω0 - ω* ∣. Because of Wt, the model cannot converge to an
arbitrary ; instead, it can only converge to the neighborhood of + W, where W is the weighted sum
of the sequence Wt . The following parts give the convergence rates when models converge to + W
for convenience.
B.4.3 Weights of rounds.
We can carefully tune the pτ,i to minimize the noise while speeding up the convergence. Here we
give the lemma of the best choice of pτ,i .
Lemma B.4. On round t, when setting ρτ,i = tD2+(t-i)R2 for any τ < t, andpt,i
we can minimize C1 and C2 to get best convergence.
(t-1)R2 + D2
tD2 + (t-1)R2 ,
Proof. Minimizing C1 and C2 is equal to minimize CpG + CR, that is,
Nt
min N X(1-PtJR + G2 +D2(X pT,i),
i=1	τ=1
t
s.t. X pτ,i = 1, ∀i = 1, . . . , N .	(72)
τ=1
Solving above optimization problem, we get the results.	□
Using lemma B.4, we have min{CpG + cr} = G2 + D2 (⅛++t-⅛)R2). Notice that the value of Ci,
C2, and C3 are changing over t. We use Ct1, Ct2, and Ct3 to represent C1, C2, and C3 on round t.
B.4.4 Convergence results
Strongly convex functions. By equation (80), when ft,i(ω) are strongly convex functions, using
Lemma A.1, and define qt = (1 - μ2n)1-t, we have
f(ωF) - f(ω*) ≤ O (μkωo - ω*k2eχp(-μ2^)) + PT— qt (Cin + C2n2 + 2t) .	(73)
By Lemma A.6, we have,
Pb X 加+CpG )=P⅛ X qt (G2+D2 (⅛⅛⅛))
P⅛ X qt 卜+D2
R2
R2 + D2
D4
+ (R2 + D2)2 T-
M)
R2 + D2
O G2 + D2
R2
R2 + D2
D4	ln T ))
+ (R2 + D2)2 T)).
(74)
20
Under review as a conference paper at ICLR 2022
For φt, by Lemma B.4, We have,
W= P⅛t X qtφt ≤O (R (D⅛ - (D2 DR2)2 lnTT) kω0 - ω*k)
Then define Co = Pτ1^ PT=I qtc1 C = P± qt PT=I qM，We have,
(75)
f(ωF) - f (ω* ) - ψ ≤ O (μ llω0 - ω*k2 exp(-^~2-) + Con + Coη2)	(76)
=O (μ kω0 - ω*/exp(-LTO)+ μNσκτ + μT + μcB2+ μ⅜ + μcD3)
(77)
where CO =	1+ A2 + B2, cA	= G2	+ RD+RJ2 ,	cB = (R2DD2)2 , CC	= LσK	+ LcA,	cD =	LcB.
+	(	+	)	ηg	ηg	ηg
General convex functions. When ft,i(ω) are general convex functions (μ = 0), directly use
Lemma A.2 We get,
f(ωF) - f 3) - 夕 ≤ O
cτF+JNKT+JFA+cBF++
3[cDF2 ∖
T-),
(78)
Where CO =	1 + A2	+ B2, cα =	G2 + RD+D2, cβ	= (r2dd2)2 , CC	= LK +	LcA,	CD = LcB,	and
+	(	+	)	ηg	ηg	ηg
F = ∣∣ωo - ω*∣2.
The drift of optimal point. Because of information loss, We can’t get the true optimal point When
considering previous rounds’ information. Instead, the drift can be described as
D2	D4	ln T
W = c3 kω0 - ω*k = O (R (d2^2 - WWT) kω0 - ω*k) .	(79)
Convergence of FedAvg. Notice that FedAvg is a special case of CFL by setting pt,i = 1 on round
t. Having this, We derive the one round convergence of FedAvg under our formulation as,
f(ωt) - f(ω*) ≤ 1(1 - 等)E∣ωt - ω*∣∣2 - 1 E∣ωt+1 - ω*∣∣2 + Con + C2n2 ,	(80)
η2	η
where Co = 2(G2 + D2) + NσK, c = L(G2n+") + 6^⅛. Then using Lemma A.1, we get the
convergence rate when ft,i (ω) are μ strongly convex.
f(ωt) - f (J) ≤ O (μ kωo - ω*k2 eχp( ^μ^)+μτ+丛；：2),	(81)
where CO = 1+ A2 + B2, CA = G2 + D2, CC = LK + LcA. Besides, when ft,i(ω) are general
convex, we have,
fM )-f(ω*) ≤O &+rNKT+rFTA+ri?),
where CO = 1 + A2 + B2, CA = G2 + D2, CC = LK + LncA, and F = ∣∣ωo - ω*∣2.
(82)
B.5 Proof of convergence rate for non-convex setting
E [f(ωt+o)] ≤ E [f(ωt)] + E [hVf(ωt), ∆ωti] + L Eh∣∆ωt∣
X--------------------------------------------/ 2、
*{z
A1
X--------
A2
2]
.}
(83)
21
Under review as a conference paper at ICLR 2022
For A1 part, we have,
E[hVf(ωt), ∆ωti]
NK
Nn XXE[hvf(ωt), vfti(ωt,i,k)i]
i=1 k=1
(84)
NK
工XX E
NK乙乙
i=1 k=1
NK
≤ N-K XX E
i=1 k=1
t	t-1
hvf (ωt), X pτ,ivfτ,i(ωt,i,k) - X pτ,i∆τ,i(ωt,i,k)i	(85)
τ=1	τ=1
tN
hVf(ωt), Xpτ,iVfτ,i(ωt,i,k)i +2N X(1- Pt,i) (E gf(ωt)∣[ + R2)
τ=1	i=1
(86)
NK
，-XX E
2nk Wyi
Vf(ωt)- XPτ,iVfτ,i(ωt,i,k)| j - E [皿。)『[-E ]gPτ,iVfτ,i(ωt,i,k)
N
+ 2N X(1- P) (E [kVf (ωt)k2] + R2)	(87)
i=1
From equation (53) we have
E k∆ωtk2 ≤
2NK
NK XX (e
i=1 k=1
t
pτ,iVfτ,i(ωt,i,k)
τ=1
2	||t-1	||2
+ E Epτ,i∆τ,i(ωt,i,k)
T =1
+ n!σ!
+ NK
(88)
Then when η ≤ 2L, the E∣PT=1 Pτ,iVfτ,i(ωt,i,k)|| term can be ignored, since the coefficient
number will less than 0. Then the remaining term in A2 is 咚 PN=I(I — Pt,i)2R2 + TNK.
Then we are willing to consider the ∣∣ Vf (ωt) - PT =1 Pτ,iVfτ,i(ωt,i,k)∣∣ . Then we have
t
Vf(ωt) - X pT,iVfT,i(ωt,i,k)
T=1
t
Vf(ωt) - Vf (ωt,i,k) + Vf (ωt,i,k) - X pT,iVfT,i(ωt,i,k)
T=1
≤2E kVf(ωt) - Vf(ωt,i,k)k2
B2+A2Xt pT2,i	EhkVf(ωt,i,k)k2i +
(89)
(90)
≤ 2L2E hkωt - ωt,i,k k i +
G2 +D2Xt pT2,i
T=1	(91)
E
E
≤ 2L2E hkωt - ωt,i,kk2i + 2L2	B2 + A2 X pT2,i	E hkωt,i,k - ωtk2i
+2	B2+A2Xt pT2,i	E hkVf(ωt)k2i + G2+D2Xt pT2,i
Combine equation (87) and (92) we have
(92)
22
Under review as a conference paper at ICLR 2022
E [hVf (ωt), ∆ωti]
ηL2 N K	PN p
≤ NK XX CpB ,iE [kωt,i,k - ωtk + + η(CpB - 1---------------‘2N , )E [kVf (ωt)k ]
i=1 k=1
(93)
Then we have
ηL2 N K	PN p
E [f (ωt+ι)] ≤ E [f (ωt)] + NK XX CpB间|向必-ωt∣∣2] + η(cpB - 1 - FNt,i )E [∣∣Vf 3)『]
i=1 k=1
+2 fcpG+N X(1- pt,i)R2!+LN- X(1 - pt,i)2R2+LNK	(94)
i=1	i=1
Because we know
Ekωt,i,k - ωt k
t-1
≤ 4k2ηl2L2CpB,iEkωt,i,k-1 -ωtk2 +4k2ηl2CpB,i(E kVf(ωt)k2 ) + 2k2ηl2CpG,i + 2k2ηl2(X pτ,i)2R2 + kηl2σ2 .
τ =1
(95)
That is,
Ekωt,i,k - ωt k
k-1	t-1
≤ X 4 4k2ηιcpB,i(E [kVf (ωt)k2]) + 2k2η2cpG,i + 2k2η2(XPτ,i)2R2 + kη2σ2 1 @kk*LepB,,r
r=0	τ=1
(96)
(4k2η2—,i(E hkvf (ωt)k2i) + 2k2η2 cpG,i + 2k2η2(PT=I Pτ,i)2R2 + kη2σ2)
≤	1 - 4k2η2 L2cpB ,i	(97)
Let 8K2ηl2CpB,i ≤ 1, we have,
1 NK
NKHcpB,iEkωt,i,k-1 - ωtk2
1 X	(4K 2η2cpB ,i(E [kVf (ωt)k2]) + 2K 端叱 + 2K 2η2(PT=1 Pτ,i)2R2 + Kη2σ2)
≤ N N CpB,i	1 - 4K2η2L2cpB,i
(98)
1 N	1	1 t-1	σ2
≤ N X CpB ,iE [kVf (ωt)k2] + 2 cpG,i + 2(X ρτ,i)2R2 + 4K
N	2	2	4K
i=1	τ=1
1	1	N t-1	σ2
CpBE [kVf(ωt)k2∣ + 2CpG + 2N X(Xρτ,i)2R2 + 4K
2	2N	4K
i=1 τ=1
(99)
(100)
Then we have
23
Under review as a conference paper at ICLR 2022
E [f(ωt+1)] ≤E[f(ωt)]+η
(L2 + 1)cpB - 1 - P=Pi) E [kVf(3t)『]
+2 1PG+N X(I - pt,i)R2∖+空 X(I - PMi)2R2+2nk
i=1	i=1
N t-1	2
+ηL2( 2 CPG+—X(X Pτ,i)2R2+4K)	(IOI)
i=1 τ=1
Then we must to choose the value of Pt,i for better convergence. Follow the idea that we want to train
a better model when convergent, we choose Pt,i that can minimize the constant term, which is by
solving
t	t-1
min G2 + D2 X Pτ2,i + (X Pτ,1)2R2
P	τ=1	τ=1
t
s.t. X Pτ,1 = 1	(102)
τ=1
D2
We get same results as when the objective function is convex, that is, pτ,i = tD2+D-i)R2 for
any τ < t, and p,,ι = /*-+焉,and min{G2 + D2 PT=ι IlPri + (PT=1 Pτ,ι)2R2} = G2 +
D2 (D)2+?-11))R2). Then We simplify the equation by using co(t), cι(t), and c2(t) to denote the
constant terms, we have
E[f(ωt+1)] ≤ E [f(ωt)]	-	ηc0(t)E	kVf(ωt)k2	+ ηc1(t) +	η2c2(t)	(103)
Then we have
ɪ Xco(t)E hkVf(ωt)k2i ≤ E[f(ω0)] - EM + ηɪ Xc2(t) + 中
T	ηT
t=	t=
(104)
Consider c0, because the value of B2 and A can’t be constrained, the algorithm can’t converge for
very large A and B. Here we first consider when co < 0, and use Cm to denote the min c°. Then let
η = √KN, we derive the convergence rate as
τ XE hkvf (ωt )k2i=O( √fκm+√t √kγ~( R⅛)+
ι Kkn	d4
+ TL Cm(R2 + D2)2
+ TKN ( (R2DD2)2 ) )
(105)
Then we want to analyse when the algorithm won’t converge. When the algorithm won’t converge,
that means c0 ≥ 0. Because c0 = (L2 + 1)cpB _ 1 一 Pa=Npt,i = N P=I(L2 + 1)(1 + A2 +
B2 PT=1 P2,i) 一 1 一 p22i. Then the value of A2,B2, L,p decide if co ≥ 0.
Notice that in FedAvg, pti, = 1, and co,avg = (L2 + 1)(1 + A2 + B2) — 3, and in CFL, we can
setting different P to get better convergence. For example, when B is large, setting pT,i = -ɪ, we have
co, =(L2 +I)(I + A2 + 牛) 一1 一 J. Thus, we draw the conclusion as
Then we have following observations:
•	CFL COnVergefaSter than FedAvg by reducing the variance terms. For co = N PN=I (L2 +
1)(1 + A2 + B2 PT=ιPT,i) — 1 — p2i, in FedAvg, we have Pt,i = 1, then ⑴叵。=
(L2 + 1)(1 + A2 + B2) - 3. However, in CFL, we can adjust p, such as when setting
Pτ,i = t, we have c0 = (L2 + 1)(1 + A2 + B) - 1 一 击,then the variance term reduce
from B2 to B.
24
Under review as a conference paper at ICLR 2022
•	The convergence rate of CFL become better for larger t, since cpB in c0 become smaller for
larger t.
•	Improve N, K will speed UP the convergence by reducing the terms about fo 一 f and σ2.
However, larger N and K will cause larger information loss and round drift (terms with R
and D), thus, it should be a trade off in practice.
C	Experiment details
C.1 Noisy quadratic model
(c) Small L, general convex, small round drift
(d) Large L, general convex, small round drift
Figure 2: Performance of different algorithms on noisy quadratic model. The curves all evaluate the loss on
global test datasets.
We use the noisy quadratic model introduced in Zhang et al. (2019) to simulate the assumptions
we used in the proof. We use the model f(ω) = ωT Aω + BTω + C, where ω ∈ Rn is the
parameter we want to optimize, and A ∈ Rn×n, A 0, B ∈ Rn, and C ∈ R. We construct A by
firstly constructing a diagonal matrix Λ and then constructing A by let A = UT ΛU, where U is
a unitary matrix. Here we control the eigenvalues of A to control the convexity of model, that is,
25
Under review as a conference paper at ICLR 2022
SRD	BRD
Method	SL-SC	LL-SC	SL-GC	LL-GC	SL-SC	LL-SC	SL-GC	LL-GC
FAVG	0.03	0.08	0.33	0.09	0.02	0.01	0.09	0.02
FPROX	0.04	0.07	0.35	0.09	0.02	0.01	0.26	0.02
CFL	0.2	0.09	0.35	0.09	0.25	0.08	0.3	0.08
Table 7: Best learning rate of different algorithms on noisy quadratic model
μ ≤ λmin (A) ≤ λmax (A) ≤ L. Because A and A have same eigenvalues, it's simple to control the
eigenvalues of A by control the eigenvalues of Λ.
To simulate Assumption 2, 3, we formulate the gradient noise model by
gt,i(ω) = Aω + B + δi + ξt,i + νt,i,k ,	(106)
where δi denotes the client drift of client j, ξt,i denotes the round drift of client j on round i, and
νt,i,k denotes the noise of gradient of client j on round i, iteration k. All of above drifts and noise
are generated from Normal distribution with zero mean and different variance.
In practice, to show if the numerical results match our theoretical results, we tried eight settings: large
and small L, large and small round drift, and general and strongly convex conditions. For different
types of L, we set L = 20 for large L, and L = 5 for small L. For convexity, We set μ = 1 for
strongly convex, and μ = 0 for general convex. For different round drift, We set E∣∣ξt,i ∣∣2 = 100 for
big round drift, and Ekξt,ik2 = 0.01 for small round drift. Besides, we set fixed variance of δi and
νt,i,k for E∣δi∣2 = 0.01, and E∣νt,i,k∣2 = 0.00001.
We do the gradient descent by let ωt,i,k+1 = ωt,i,k - ηlgt,i(ωt,i,k), and We use ∣Aω + B∣ as loss
because for convex functions, When it reaches the global optimal point, We should have ∣Aω + B ∣ =
0, and if it is far aWay from global optimal, ∣Aω + B ∣ Will be large.
Besides, We shoW that CFL algorithms can tolerate a larger learning rate here. Table 7 shoWs the best
learning rates for different FL algorithms on noisy quadratic model. In Table 7, We denote small
round drift as SRD, big round drift as BRD, small L as SL, large L as LL, strongly convex as SC,
and general convex as GC. We shoW that the best learning rate of CFL is the largest. The difference
betWeen CFL methods and other FL baselines becomes large When models are strongly convex or
With big round drift.
Figure 2 shoWs the performance of different algorithms, and CFL outperforms other FL baselines
significantly in all settings. Besides, the loss curves of CFL are smoother than FL methods, Which
implies that CFL can reduce the variance of gradients.
C.2 Realistic datasets
C.2. 1 Setup
We consider federated learning an image classifier on split-CIFAR10 and split-CIFAR100 datasets
With ResNet18, and split-Fashion-MNIST dataset With a tWo-layer MLP. The “split” folloWs the idea
introduced in Yurochkin et al. (2019); Hsu et al. (2019); Reddi et al. (2021), Where We leverage the
Latent Dirichlet Allocation (LDA) to control the distribution drift With parameter α (See Algorithm
4). Larger α denotes smaller drifts here.
In our experiments, unless specifically mentioned otherWise all datasets are partitioned to 210 subsets
for 7 different clients: all clients are selected and trained for 500 communication rounds, and each
client samples one of the corresponding 30 subsets randomly for the local training. Note that unless
mentioned otherWise the training strategy here applies to all FL baselines and CFL methods, and We
evaluate the performance of models on global test datasets. We carefully tuned the hyper-parameters
in all algorithms, and We report the results under the optimal settings after many trials. For CFL-
Regularization, We set the Weight of regularization term for β = 1 on fc layer, and β = 0.1 for last
block (layer). For FedProx, We set the weight of proximal term μ = 0.1, and for MimeLite, We
set the global momentum Weight for γ = 0.01. Besides, for all experiments, We set fixed learning
rate lr = 0.01. We also naively examined Warm-up tuning strategies but can not observe significant
improvement on final results.
Construct local datasets with overlap. Fol-
loW the partition methods of disjoint local
datasets; We first split the Whole dataset to
Figure 3: Process of constructing local datasets With
overlap. Blue lines bound the current local datasets. Red
—I - -4- ，、 L - I - ɪ - -4- -fck C ,-⅛ -4-	.⅛ _ — 一 ~一~ . ⅛ 一 一- 4 1 1 4O ，
26
Under review as a conference paper at ICLR 2022
M × S subsets for S different clients: each
client has M disjoint local subsets, and we put
these M subsets in sequence. For each client,
we use a pointer to point to the start position
of the current local dataset. Figure 3 show the
case when M = 5: D1 - D5 are 5 disjoint local
subsets of client i. The pointer point to the start
point of a local dataset of the current round and the blue lines bound current subsets. At the beginning
of training, pointers belong to each client will point to the beginning of the sequence. At the end of
each round, the pointer moves s steps, and if it moves to the end of all the sequence, it will come
back to the start point.
In practice, the Cifar10 dataset is partitioned to 210 subsets for 7 clients, and set the size of local
dataset Sl = 285. Then when we set s = 285, the local datasets are disjoint, which means the overlap
is 0%. Otherwise, when set s = 213, the overlap is 25%, and when set s = 142, the overlap is 50%.
C.2.2 Approximation Methods
Regularization Methods. We use Taylor Extension to approximate the objective functions of
previous rounds, that is,
f(ω) = f(ω) + V/(ω)T(ω - ω) + 1(ω - ω)TV2/(ω)(ω - ω),	(107)
where ω are the parameters of previous rounds. Then the gradient of /(ω) is,
Vf(ω) = V/(ω) + V2∕(ω)(ω - ω).	(108)
Having this, we can approximate the gradients of objective functions of previous rounds. The problem
is how to approximate V2/(ω). Here we use two kinds of methods to calculate the Hessian matrices.
The first is to use the Fisher Information Matrix as introduced in EWC (Kirkpatrick et al., 2017). The
Fisher Information Matrix is equal to the diagonal Hessian matrix when using cross entropy loss
functions. The Fisher Information Matrix can be calculated by,
Fij = [V/(ω)]2j ,	(109)
where Fij are the entries of Fisher Information Matrix. Another method is to calculate the diagonal
Hessian matrix. We use the PyHessian package (Yao et al., 2020) in this experiment.
In addition, if the parameters or the Hessian matrix changes too much, the performance of this method
is constrained. To formulate this problem, assume V2/ (ω) ≤ for some arbitrary , we have
k∆t,i(ω)k≤ e kω - ω∣∣2 .
The corresponding proof refers to B.3.
In each round, the server will collect Hessian matrices from clients, and store them in a buffer.
Then at the beginning of next round, server combine send latest 40 Hessian matrices, gradients, and
parameters, and send them to chosen clients. Each clients use these to calculate regularization terms.
In practice, we only add regularization terms to the top layers (last block and fc layer of ResNet18).
This is based on the assumption that top layers contain more personal information while bottom
layers contain more general information. We verified that this strategy perform better than adding
regularization terms to all layers in experiments.
Generation Methods. We use MCMC to generate samples in each round. In practice, we initialize
x from uniform distribution, and update x by,
xk = xk-1 - ηVxE(xk-1) + ω,	(110)
where E(xk - 1) is the function that can measure the distance between xk-1 and the real local
distribution. ω 〜N(0, σ).
In practice, we generate 50-100 samples of each local dataset and add them to the current local
datasets for training. However, because of the low quality of the generated data, the improvement is
limited. Besides, the generated data will pollute the batch normalization(BN) layer, and we should
use real data to refresh BN layer at the end of each round.
27
Under review as a conference paper at ICLR 2022
Algorithm 3 iCaRL Construct Core Set
Require: Image set X = {x1, x2, x3, ..., xn} of class y, m: target number of samples, φ : X → Rd:
current feature function
1:μ J n Px∈xφ(X)
2:	for k = 1, ..., m do
3:	I Pk — argminχ∈χ Uμ - 1 (φ(X)+ Pk-I φ(Pi))∣∣
4:	P J (p1,p2, ...,pm)
Core Set Methods. Another simple yet effective treatment in CL lines in the category of Exemplar
Replay (Rebuffi et al., 2017; Castro et al., 2018). This approach stores past core-set samples (a.k.a.
exemplars) selectively and periodically, and replays them together with the current local datasets.
We tried two sample methods. First is so-called Naive method, in which the samples are uniformly
chosen from local datasets. Except of naive select core sets, we also tried another core set sampling
method introduced in iCaRL (Rebuffi et al., 2017). See Algorithm 3 for details.
In practice, we save 100 figures of each local dataset and combine them with the current local datasets.
Saving core sets perform best compared with these three approximation methods; however, only valid
when the number of clients is limited.
C.2.3 Additional Experiments
Applicability of different algorithms. We list the applicability of different algorithms under
various time-varying scenarios in Table 8.
Table 8: The applicability of various algorithms under different time-varying scenarios.
FL baselines	CFL methods
Scenarios	___________________________________ _______________________________
FedAvg FedProx SCAFFOLD MimeLite CFL-Regularization CFL-Core-Set
Stateful clients	√	√	×	√	√	√
Stateless clients	√	√	×	√	√	×
Convergence curves for different settings. Figure 4 show convergence curves of CFL and FedAvg
on different datasets. The settings are the same as results in Table 3. Models are trained on partitioned
datasets with α = 0.1, and all datasets are partitioned to 210 subsets for 7 clients. To show the
difference between different algorithms more clearly, all curves are smoothed by a 1D-Mean-Filter.
Results show CFL can converge to a better optimum compare with FedAvg.
Investigating resistance of CFL to time-evolving scenarios. Figure 6 shows the loss curve of
CFL-Regularization and FedAvg on different datasets. Models are trained on partitioned datasets with
α = 0.1, and all datasets are partitioned to 210 subsets for 7 clients. The first column shows the loss
curve of the first 300 rounds, and the second column shows the loss of some chosen details. Because
of the non-iidness of local datasets, the loss will suddenly rise. Notice that CFL-Regularization has
an apparent mitigation effect on this situation.
Difference between training and test loss. Figure 5 show the loss on global test data and past
appeared training data. We evaluate the model with stateful clients, set α = 0.2, and use FedAvg
algorithm. We show that there is no significant difference between loss value on global test data and
past appeared training data.
C.3 Algorithms
28
Under review as a conference paper at ICLR 2022
Aoe.Inooa
0.2
0
)	100	200	300	400	500
Communication Round
(a) Performance on Fashion-MNIST
0.1
8765432
Ooooooo
A0e.ln8∖/
(b) Performance on Cifar10
0∙0∙0∙0∙0∙0∙
Aoe.Inooa
0
0	100	200	300	400
Communication Round
(c) Performance on Cifar100
500
Figure 4:	Models are trained on various datasets with α = 0.1. CFL Without Core Set method use regularization
methods, and CFL With Core Set methods use core set methods. All these two CFL algorithms use FedAvg as
backbone.
3.5 I-------1-------1-------1-------1--------；	,	,	,	J 0.7
Loss on Global Test Data
Loss on Past Appeared Data
3 1	J 0.6
SSOl
10	20	30	40	50	60	70	80	90
Communication Round
0	10	20	30	40	50	60	70	80	90
Communication Round

(a) Loss on global test data and past appeared (b) Accuracy on global test data and past ap-
train data	peared train data
Figure 5:	Evaluation on global test data and past appeared training data. We trained ResNet18 on split-Cifar10
dataset with α = 0.2 for 85 rounds.
29
Under review as a conference paper at ICLR 2022
(a) Loss on Fashion-MNIST
(b) Loss on Fashion-MNIST (Local)
(c) Loss on Cifar10
Figure 6: Models are trained on split-Fashion-MNIST and split-Cifar10 datasets with α = 0.1. The loss is
evaluated on global test datasets. Left column is the full curve of 300 rounds, and figures in right column are
partially enlarged curves.
(d) Loss on Cifar10 (Local)
Algorithm 4 Data splitting
Require: S: a list of datasets split by labels, M : the number of clients, N : the size of local datasets,
α: the Dirichlet distribution parameter
Ensure: D: split datasets
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
D J []
G J [0,1, 2,...,len(S)]
for m = 1, 2, ..., M do
P = [Pl,P2,…，Plen(S)], Where pt,i
θ J DiriChlet(α,p)
Dm J 0
while Ien(Dm) < N do
i J multinomial (θ, 1)
y J G[i]
data J uniform(S[y])
Dm J Dm ∪ {data}
S[y] J S[y]∕data
if len(S[y]) == 0 then
G J G/y
θ J renormalize(θ, i)
D.append(Dm)
denotes the fraction of class i in total dataset.
Algorithm 5 renormalize
Require: θ: weights for different classes, i: the class that should be deleted
Ensure: θ: renormalized Weights
1:	for j = 1, 2, ..., len(θ), j 6= i do
2:	I θ[j] J θ[j]/sum(th^ta/theta[i])
30
Under review as a conference paper at ICLR 2022
Algorithm 6 SplitdataMain
Require: S: total dataset split by class, T: rounds, K, N, ɑ, β
Ensure: Dfinal: split dataset
1: D — SpIitData(S, K, N, α)
2： Dfinal — []
3: for i = 1, 2, ..., K do
4:	Si J SpIitByCIaSS(D[i]
5：	Nlocal J Ien(Dm)/T
6:	Di J SpIitData(Si, cluster_num, Nlocal, β)
7:	Dfinal J Dfinal ∪ Di
31