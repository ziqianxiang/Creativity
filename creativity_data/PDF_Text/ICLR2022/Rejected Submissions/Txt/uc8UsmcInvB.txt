Under review as a conference paper at ICLR 2022
Statistically Meaningful Approximation: a The-
oretical Analysis for Approximating Turing
Machines with Transformers
Anonymous authors
Paper under double-blind review
Abstract
A common lens to theoretically study neural net architectures is to analyze the functions
they can approximate. However, constructions from approximation theory may be
unrealistic and therefore less meaningful. For example, a common unrealistic trick is to
encode target function values using infinite precision. To address these issues, this work
proposes a formal definition of statistically meaningful (SM) approximation which
requires the approximating network to exhibit good statistical learnability. We study
SM approximation for two function classes: boolean circuits and Turing machines.
We show that overparameterized feedforward neural nets can SM approximate boolean
circuits with sample complexity depending only polynomially on the circuit size, not the
size of the network. In addition, we show that transformers can SM approximate Turing
machines with computation time bounded by T with sample complexity polynomial
in the alphabet size, state space size, and logpT q. We also introduce new tools for
analyzing generalization which provide much tighter sample complexities than the
typical VC-dimension or norm-based bounds, which may be of independent interest.
1	Introduction
Dating back to the seminal works on universal approximation (Cybenko, 1989; Hornik et al., 1989; Park
& Sandberg, 1991; Leshno et al., 1993), a common way to theoretically study neural nets has been through
their expressivity, which measures the ability of neural nets to approximate well-behaved functions. This
perspective has shaped how researchers perceive different types of deep learning architectures: a basic
way to theoretically justify new architectures is to study their approximation capabilities. This has led to
a number of analyses studying universal approximation capabilities for various widely-used architectures,
such as recurrent neural nets (RNNs) (Schfer & Zimmermann, 2007), graph neural nets (Scarselli et al.,
2008), convolutional networks (Bao et al., 2014; Zhou, 2020; Yarotsky, 2021), residual networks (Lin &
Jegelka, 2018), transformers (Yun et al., 2019), and neural ODEs (Teshima et al., 2020; Zhang et al., 2020).
However, approximation theoretic results often misalign with more meaningful end-to-end guarantees,
because models constructed in the literature often exhibit unrealistic properties. For example, a common
technique in the universal approximation literature is to rely strongly on infinite-precision weights and
activations, or exponentially many parameters to encode the desired function values (Hornik et al., 1989;
Cybenko, 1989; Leshno et al., 1993; Lin & Jegelka, 2018; Yun et al., 2019; Sannai et al., 2019). This issue
even arises outside of universal approximation, e.g., various papers demonstrate the ability of RNNs and
transformers to simulate various computational models such as Turing machines and automata, but require
strong reliance on arbitrary precision (SiegeImann & Sontag, 1995; Perez et al., 2019; Korsky & Berwick,
2019; Bhattamishra et al., 2020). Infinite precision can inflate the expressivity of an architecture in a
unrealistic and misleading way: for example, finite width RNNs with infinite precision can simulate Turing
machines, but finite-precision, finite-width RNNs cannot, as implied by streaming lower bounds (Alon
et al., 1999). As another example, Park et al. (2020) exploit infinite precision in the parameters to show
that a neural net with parameter count sublinear in n can memorize n arbitrary input-label pairs. However,
a simple counting argument reveals that this result cannot be proven using finite precision networks - there
are 2n input-labeling pairs, but only 2opnq finite precision networks with opnq parameters.
More broadly, the ideal theoretical perspective should consider not only whether target functions can be
expressed, but also whether the constructed networks are plausibly learnable. Learnability is important
because empirical settings do not operate in the infinite data, unbounded computation regime - they
require fitting the target function with access to limited number of samples from an empirical distribution.
The question of studying learnability can be decomposed into studying optimization and generalization.
1
Under review as a conference paper at ICLR 2022
Unfortunately, a rigorous analysis of optimization is unresolved even for simple two-layer nets (Mei et al.,
2018). Generalization is more tractable, so we propose to study expressivity and generalization together.
Towards the goal of studying more meaningful notions of approximation, this work proposes the notion
of statistically meaningful (SM) approximation. This definition requires not only the existence of an
approximating network, but also that it has good statistical learnability. Consider a setting where the aim
is to fit the target function G using the approximating family F and a finite sample of training data. SM
approximation requires existence of a loss whose empirical risk minimizer in F leads to a model with low
approximation error in fitting G. We define the sample complexity of the approximation as the number of
training samples needed to guarantee at most approximation error and study SM approximation with low
sample complexity bounds. SM approximation essentially eliminates all statistical concerns for learnability
(optimization-related concerns can remain).
We present two case studies on SM approximation. First, we demonstrate that overparameterized feed-
forward neural nets can SM approximate boolean circuits with a low sample complexity that depends
only on the intrinsic circuit size. Though it is simple to construct neural nets to approximate boolean
circuits, bounding the sample complexity of the approximation is challenging. For example, standard
norm-based generalization bounds for the naive construction scale exponentially in depth (Bartlett et al.,
2017). Furthermore, VC dimension-based bounds would scale polynomially in the number of parameters
in the network (Harvey et al., 2017), which is problematic because for practical optimization concerns,
neural nets are typically overparameterized in terms of width (Zhang et al., 2016). In contrast, our sample
complexity bound for SM approximation depends only on the intrinsic circuit size, up to logarithmic factors.
Our second case study is on SM approximating Turing machines with transformers. We consider
a class of Turing machines with bounded computation time T and construct encoder-decoder-based
transformers (Vaswani et al., 2017) which SM approximate these Turing machines. The sample
complexity of the approximation depends on a polynomial in logT and the sizes of the state space and
the alphabet of the Turing machine. Though constructions for approximating Turing machines from
prior work (Siegelmann & Sontag, 1995; Perez et al., 2019; Bhattamishra et al., 2020) have not been
formally studied from a sample complexity perspective, existing bounds would depend at least linearly on
T. Furthermore, our construction only uses loglogT precision, compared to at least logT in prior works,
allowing us to achieve the exponential improvement in the sample complexity.
Proving sample complexity guarantees for our statistically meaningful approximation results is nontrivial
and requires additional insights, for both the constructions and the generalization analyses. To obtain
our sample complexity bounds, we leverage a recent approach to bound generalization in terms of
data-dependent notions of Lipschitzness (Wei & Ma, 2019b). We develop theoretical tools to convert a
broad class of neural nets, with possibly large Lipschitzness, into ones with small Lipschitzness on the
training data, by introducing a number of new layers that is linear in depth. Our result applies to neural
nets where each entry in the hidden representations on the training data takes values from a finite set (e.g.,
binary entries), and may be of independent interest.
In summary, our contributions are: 1) we propose a new notion of statistically meaningful approximation,
intended to provide more meaningful approximation guarantees by requiring that the approximating family
have good statistical learnability; 2) we prove that feedforward neural nets can meaningfully approximate
boolean circuits with sample complexity that depends polynomially on the width and depth of the circuit;
and 3) we show that transformers can meaningfully approximate Turing machines with sample complexity
logarithmic in the computation time.
1.1	Related works
Classifical approximation theory for neural networks has a long history. Hornik et al. (1989); Cybenko
(1989), and Leshno et al. (1993) show that neural nets with one hidden layer are universal approximators
but require the hidden layer size to grow exponentially in input dimension. Barron (1993) uses the Fourier
transform to write target functions as infinite-width networks and subsamples neurons to obtain widths
which depend only on target function properties. Lee et al. (2017); Ji et al. (2020) prove recent related
developments in this direction of universal approximation.
Many works study benefits of deep networks over shallow ones (Bengio & Delalleau, 2011; Arora et al.,
2016; Telgarsky, 2016; Eldan & Shamir, 2016; Daniely, 2017; Chatziafratis et al., 2020; 2019). Bengio &
Delalleau (2011) show separation for exact representation, whereas Telgarsky (2016) shows separation for
approximate representations with univariate inputs. Eldan & Shamir (2016) demonstrate high-dimensional
functions that can be approximated by two-layer polynomial-sized neural networks, but cannot be approx-
imated by one-layer neural nets with subexponential hidden units. Via reduction to certain complexity theo-
2
Under review as a conference paper at ICLR 2022
retic questions, Vardi & Shamir (2020) show that proving constant depth separations may be hard. Malach
et al. (2021) analyze the relationship between optimization and approximability, showing in various settings
that deeper networks cannot be optimized if shallow networks cannot approximate them. This demonstrates
that depth separation results (Telgarsky, 2016) from approximation theory can be misleading in the sense
that gradient descent anyways cannot optimize the deep networks used to construct the approximation.
Another area of study is on the ability of deep networks to memorize training data (Zhang et al., 2016;
Yun et al., 2018; Park et al., 2020; Vershynin, 2020). Yun et al. (2018) show that Θpnq parameters
are sufficient to memorize Θpnq training points for ReLU nets with at least 3 layers, and Park et al.
(2020) reduce the parameter requirement to sublinear in n. Similar results have been proven for residual
architectures (Hardt & Ma, 2016) and convolutional nets (Nguyen & Hein, 2018). Bartlett et al. (2019)
analyze the VC-dimension of neural nets, leading to upper and lower bounds on the parameter count
needed to fit training data. Other works study expressivity via connections to tensor approximation and
sum-product networks (Cohen & Shashua, 2016; Cohen et al., 2016).
There is a long line of work on studying the ability of neural nets to recognize and represent formal languages.
The seminal work of Siegelmann & Sontag (1995) shows that RNNs are Turing complete but leverages in-
finite precision in the hidden activations. Chen et al. (2018) extend this result to ReLU activations and study
implications in language modeling. Many variants of transformers are shown to be Turing-complete, but
these constructions also rely on arbitrary precision (Perez et al., 2019; Bhattamishra et al., 2020). A number
of recent works have also proven results for generating or recognizing formal languages with finite-precision
neural nets (Weiss et al., 2018; Korsky & Berwick, 2019; Hewitt et al., 2020), but these results do not con-
sider Turing machines or analyze statistical properties of their constructions. Bounding the sample complex-
ity of SM approximation requires additional complications in both the construction and statistical analysis.
1.2	Notation
Let f og denote the composition of functions f and g. For a family of functions G, let f QG fi {f Og: gPG}
denote the family of compositions between f and functions in G. For a set S and function f: S → Y, let
f (S) denote the set {f (Sq: S P S}& Y. We use 1d to denote the all-one,s vector in d dimensions, with
the subscripted omitted if clear. For iP rds, we let 1dpiq denote the one-hot embedding in d-dimensions,
which is 1 at index i and 0 everywhere else. We use the notation O(∙) to hide poly-logarithmic factors in
the argument. The notation <,> indicates the existence of a constant factor such that the inequality holds.
-denotes that the > and < relations simultaneously hold. We use poly(∙) to indicate the existence of a
polynomial in the argument which makes the equation true. For a set A (e.g., the set of alphabet symbols for
a Turing machine) let A* denote the set of all sequences of elements of A, where sequence length can vary.
Let P denote a distribution over a space of inputs X. Let ξ1,...,ξn be n i.i.d. Rademacher variables sampled
from {—1,+1}. The expected n-sample Rademacher complexity of F on P is as follows: Radn,p(F) fi
Er `n i.i.dp[Eξι,...,ξn “suPFPFnΣn=1ξiF(xi)‰‰, where (xi)n=I denotes n i.i.d. samples from P.
pxiqi“1 „ P
2	Statistically meaningful approximation
We consider settings where we wish to approximate every member G in a real-valued function class G
with some function F in function class F. In this work, F is some family of neural networks. Fix a loss
':RXR →[0,1]. The classical definition of eapproximation states that F eapproximates G with respect
to ',P if forall GP G, there exists FPF such that Ex〜P ['(F(χ),G(χ))]≤ 邑
The issue with this classical notion of approximation is that in machine learning settings, we only have
access to G, the function we wish to learn, through its values on a finite training set: (xi,G(xi))in“1. If
we disregard this fact, we could end up constructing functions F which approximate G, but could have
a number of unrealistic characteristics such as infinite precision. These drawbacks would mean that F
cannnot be realistically learned from the training sample.
This work studies a stronger notion of approximation, statistically meaningful (SM) approximation, to elim-
inate statistical concerns related to fitting G on a finite sample. SM approximation, defined below, requires
that G is learnable via empirical risk minimization using models from F, when data is generated from P.
Definition 2.1 (SM approximation). F e-SM approximates G with respect to ',P with sample complexity
n ifthere exists a loss s: F XX XR→ [0,1] such that thefoUOwing holdsfor all GPG:
3
Under review as a conference paper at ICLR 2022
^
Define F PF to be the empirical minimizer of the loss s` for fitting G on an i.i.d. sample pxi,Gpxiqqin“1

of n examples labeled by G: F fi argminFPFnXn“i'(F，Xi，G(Xi))∙ Then Withprobability 0.99 over the
draw of (xi)i“i, F approximates G in the classical sense: Eχ~p ['(F(x),G(x))]w e.
Definition 2.1 eases the statistical concerns associated with classical approximation theory: given a finite
sample (xi,G(xiqqin“1, the empirical risk minimizer ofs` over F is guaranteed to -approximate G on the
population distribution. It is important that the losses s` (which can be interpreted as a training surrogate
loss) and ` can be different, as this allows the empirical risk to include regularization.
Though Definition 2.1 may be reminiscent of PAC-learnability, there is a major conceptual difference:
SM approximation unifies expressivity and generalization, whereas PAC-learnability is only concerned
with generalization. The main focus of PAC-learnability is achieving a low loss relative to the best function
in the hypothesis class, which is assumed to have 0 loss in the realizable case. SM approximation also
requires proving that the best function in F achieves near-zero loss.
2.1	Background and tools
To prove SM-approximation guarantees, Definition 2.1 requires a loss surrogate ` such that the empirical
risk minimizer ofs` on the training data can approximate functions in G. The following proposition
provides several conditions ons` which lead to SM approximation guarantees.
Proposition 2.2. For loss function ' : R X R →[0,1] and distribution P, suppose there exists a loss
':F XX XR → [0,1], intended as a surrogate lossfor ', satisfying thefollowing properties:
1)	For all FPF, xP X, y PR, '(F,x,y)> '(F(xq,y}.
2)	For all G P G, consider the function class LG fi {x → s(F,x,G(x)) : F P F}. Then the n-sample
Rademacher complexity of LG is bounded: Radn,p(LG) ≤ e.
3)	For all GP G, there exists FPF with small surrogate loss: Ex〜Prs(F,x,G(x))]W e.
Then F
O (e' ?1n)-SM approximates G with respect to ',P with sample complexity
n.
By Proposition 2.2, it suffices that ` upper bounds the target loss ` and has low complexity, and F
approximates G with respect to s`, P in the classical sense. The proof follows from standard techniques
for bounding generalization based on Rademacher complexity and is provided in Section A.
All-layer margin loss. We introduce one particular construction fors` used in subsequent sections, which
is motivated by the all-layer margin generalization bound proposed by (Wei & Ma, 2019b). This bound
is based on data-dependent Lipschitzness measures (Nagarajan & Kolter, 2019; Wei & Ma, 2019a), and
can provide stronger guarantees than classical norm-based bounds (Neyshabur et al., 2015; Bartlett et al.,
2017; Neyshabur et al., 2017; Golowich et al., 2018).
We focus on the binary classification setting, where G(x) P t0,1u, and study approximation with respect
to the 0-1 loss '0_1 (z,y) fi l((y—0.5)z ≤0) where y p{0,1} is assumed to be a binary label. We consider a
family of functions F parameterized by p-dimensional parameters θ P Θ J Rp, with a general architecture
function F: X XRp →R. Thus, F ={x→ F(x,θ): θPΘ}, and we sometimes use θ to identify an element
of F. Throughout the paper, we define Θ as a }∙}ι-normboundedset: }θ}ι ≤ α, VθP Θ. We define the
parameter-based all-layer margin PF: Rp XX X fθ,1}→ R as follows:
ρF (θ,x,y) fimin }δ}2
subject to (y—0.5)∙F(x,θ'δ)≤0
(2.1)
We omit the architecture from the subscript when it is clear from context. This quantity measures the
stability of the model around an input x in parameter space. As is the case for the standard output margin,
a larger all-layer margin, or better stability, implies better generalization.
We modified the definition in (Wei & Ma, 2019b) to consider perturbations δ in parameter space,
whereas Wei &Ma (2019b) consider perturbations to the hidden layers. The parameter-space formulation is
simpler and subsumes the results in (Wei & Ma, 2019b). Our formulation also accounts for weight sharing,
which is important for our Turing machine results, whereas the formulation of (Wei & Ma, 2019b) could not.
A key and immediate property of the all-layer margin is that it is strictly positive if and only if F (x,θ)
predicts the correct label. We can leverage this property to construct a surrogate loss. For some parameter
4
Under review as a conference paper at ICLR 2022
γ intended to lower bound the all-layer margins, We define the loss sγ as follows:
sγ (θ,χ,y)=< 1 —
1 if ρ(θ,x,y)W 0
ρpθχyq if 0 < ρ(θ,x,yq≤ γ
(2.2)
0 if ρ(θ,χ,yq> Y
Note that sγ composes the classical ramp loss, which is used to prove margin-based generalization
complexity bounds, with the value of the all-layer margin. By our construction, it immediately follows
that sγ(θ,χ,G(χ))》'0-1(F(χ,θ),G(χ)), as is required of a surrogate loss.
We show that to obtain sample complexity bounds for SM approximation of G in a classification setting,
it suffices to prove that functions in F can fit labels ofGPG with large all-layer margin.
Lemma 2.3. Fix any neural net architecture F: X X Rp → R, and define Fa 冬{x l→ F(x,θ): θ P Θ},
where we assume Θ 三 Rp is such that }θ}ι ≤ α for all θP Θ. Fix E20. Suppose thatfor all GP G, there
exists θPΘ such that the following holds:
Ex〜P [l(ρ(θ,x,G(x))< γ)[W E
(2.3)
Then Fa E-SM approximates G with respect to '0-1,P with sample complexity O (表
´ a2iogppq
(γ2
`1	.
We note that O hides poly-logarithmic factors in the arguments, in this case, poly log (a ；2gppq) factors.
The proof closely follows (Wei & Ma, 2019b), is deferred to Section A. In Section A, we also state a
generalization bound for 0-1 loss based on (2.1), which may be of independent interest. We use (2.2) and
Lemma 2.3 to prove that neural nets can SM approximate Boolean circuits and Turing machines.
3	SM approximation of Boolean circuits with feedforward nets
This section shows that feedforward neural nets can SM approximate Boolean circuits with sample
complexity that depends polynomially on the size of the circuit. A boolean circuit G : t0,1um → t0,1u
on m inputs bits is described by a directed acyclic graph, with vertices of this graph referred to as “gates”.
The graph contains m input gates of indegree 0, which are identified with the input bits. The remaining
gates each compute a boolean function taking values at their parents as arguments, and a designated output
gate produces the output of the entire circuit. We consider boolean circuits consisting of AND, OR, and
NOT gates, which compute the corresponding boolean functions on 2, 2, and 1 inputs, respectively and
are sufficient to compute any boolean function (Savage, 1998). We also allow identity (ID) gates, which
take 1 input and output the same value.
We consider layered circuits, where we can partition the gates into layers such that the only edges in the
graph occur from gates in layer i to gates in layer i`1 for some i. Note that we can transform any boolean
circuit into a layered one by adding ID gates. Letting q denote the number of layers and r the maximum
number of gates in any layer, we say that the circuit has depth q and width r. We say that a circuit with
S total gates has size s. Our convention will be that the set of input gates is considered a layer, so rem.
We consider the following class of boolean circuits:
Gq,r,s “ tG: t0,1um → t0,1u :G computed by circuit with depth q, size s, and width ru
We will approximate Gq,r,s using a family of width w, depth d feedforward ReLU nets pa-
rameterized by linear weights and biases θ “ (W0, b0, ... , Wd, bd) computed as follows:
FW,d(χ, θ) “ Wdφ(W^dτφ(…φ(Woχ ' bo)…)' bd´i) ' bd, where all intermediate layers have
width w for simplicity and φ denotes the coordinate-wise ReLU activation. The weight parameters are
set so that for 1 ≤ i ≤ d — 1, Wi PRw*w, Wo PRwχm, and Wd PR1xw. The bias parameters are such
that bi PRw for 0 ≤ i ≤ d—1, and bd PR. To control the sample complexity, we restrict our attention to
the set of parameters with total }∙} 1-norm bounded by a, giving the following function class:
Fw,d,a = {x→Fw,d(x⑼：}θ}ι ≤ α}
The following theorem states that feedforward neural nets can statistically meaningfully approximate
boolean circuits with sample complexity polynomial in the circuit size.
Theorem 3.1. Consider the class Gq,r,s of size-s,width-r, and depth-q layered boolean circuits, and the
class Fw,d,a ofneural nets above. Suppose W > r, α — S, and d - q.
Thenfor all E > 0 and any input distribution P over {0,1}m, Fw,d,a E-SM approximates G with respect
to '0-1,P with sample complexity poly(s)O (Iog(Pwdq
.
5
Under review as a conference paper at ICLR 2022
We note that the bound in Theorem 3.1 only scales logarithmically in the width w of the network, even
if w is arbitrarily greater than the circuit width r. This ensures that even heavily overparameterized nets
will have low sample complexity of the approximation.
For this setting, the all-layer margin loss in (2.2) is essential for proving tight sample complexity bounds, as
other surrogate losses s` would give weaker results. For example, if we choose `0-1 as the surrogate loss, VC-
dimension bounds (Harvey et al., 2017) imply that Fw,d,α statistically meaningfully approximates Gq,r,s
with sample complexity scaling in polypwqq under the conditions of Theorem 3.1. This suffers a polynomial
dependence on the overparameterized width w, which is not ideal for realistic settings, where neural nets
are often wider than necessary to facilitate optimization. In contrast, our dependence on w is logarithmic.
Another possible surrogate loss is the output margin-based ramp loss, which can be used to prove norm-
based sample complexities (Bartlett et al., 2017). However, these bounds depend on ∏‰ }Wi}op (or
related quantities), which would be exponentially large in d for the naive construction in Section 3.1.
3.1 Proof sketch for Theorem 3.1
There are two key steps in the proof. First, given any layered circuit G P G, we construct a neural net
that directly simulates G by computing the layers of G one-by-one, which is simple to do by directly
constructing ReLU and linear layers to simulate the AND, OR, NOT, and ID gates.
Lemma 3.2. In the setting of Theorem 3.1, let G denote the layered boolean circuit, which we aim to
compute using a neural net Let gi: {0,1}riτ → {0,1}ri denote function computed between the i — 1 -th
and i-th layers of G, which we assume have r´ and r gates, respectively, so G“gq—1 o∙∙∙ogι. Then
there existfunctions fι,…,fq—i, where each f is computed by afeedforward ReLU net with two linear
and activation layers, such that for all iP rq —1s and xP t0,1um
fi。…。fι(x) = gi。…。gι(x)
Thus, the composition F (∙,θ) fi fq—。…。fi satisfies F (x,θ) = G(X) for all X p{0,1}m. Note that we
omitted the dependency of fq—1,...,f1 on parameters θ for simplicity.
Lower bounding all-layer margin. The next step for proving SM approximation is to construct a loss
s` so that the empirical risk minimizer ofs` on the training data has good sample complexity. This crucially
requires the all-layer margin tool developed in Section 2.1, as other complexity measures (e.g. norm-based)
would not give good sample complexity bounds.
Recall that the all-layer margin ρF (θ,X,G(X)) measures the stability of the output F (X,θ) to perturbations
in to θ, and, by Lemma 2.3, it suffices to show that F has large all-layer margin on X P t0, 1um.
Unfortunately, we cannot guarantee that the naive construction from Lemma 3.2 has large all-layer margin
without further modifications. To remedy this issue, Theorem D.6 introduces a generic way to convert the
model F(∙,θ), with possibly small all-layer margin on Xp{0,1}m, into a new architecture and parameter
set F 1(∙,θ1), with provably large all-layer margin on X p{0,1}m, such that F 1(χ,θ1) “ F(χ,θ) on all inputs
X P t0,1um . The construction relies on introducing new layers to F to obtain F1 and increases the total
number of layers by only a constant factor. This step of the proof is formally stated in the following lemma.
Lemma3.3. IntheSeuingOfLemma 3.2,let F (∙,θ) “ fq—i o∙∙∙ofι be the neural net with parameters θ
constructed to compute the circuit G. There exist “correction functions” Zi,…,Zq—2, where Zi is computed
by a neural net with two activation and linear layers, such that the composition
F 1(∙,θ1) fi fq—1。Zq—2 0fq—2。…。ZlOfl
has large all-layer margin. Here θ1 denotes the collection ofallparameters. Concretely, PF1 (θ1,x,G(x))》
pol1(s) for all xp{0,1}m. Note that we omitted the dependency of fi,ζi on parameters θ1 for simplicity.
We convey the core intuitions for Lemma 3.3 in a simplified toy setting as follows. Consider the case
where we start with an initial architecture f computing f (χ,(W1,…,Wd)) “ (∏2ιWi) X — 05 where
Wi PR. In this simplified setting, we consider Wi “ 1 @i. For input X“ 1 and target y“ 1, the all-layer
margin is small: ρf ((1,…,1),1,1) < ?, where the architecture is in the subscript. Indeed, choosing δi “ d,
we have f (1,(1 — d,…,1 — 3 )) = (1 — d )d — 0.5 « exp(—3)—0.5 V 0. Thus, by the definition of all-layer
margin, Pf ((1,...,1),1,1)< a∑iδ2 < ?.
Now we will insert ReLU layers in f to increase the all-layer margin to Ω(1). We use ReLU layers to
implement the round function, which has the key property that round(z) = 1 Vz22/3.
6
Under review as a conference paper at ICLR 2022
'0	if Z V1/3
Proposition 3.4. For any Z P R, WecanimplementthefUnction round(z)=< 3x—1	if 1/3 ≤ Z V 2/3
% 1	if z22/3
ViaafeedforwardReLU net, ɑsfθllows: round(z) = 3φ(z—1/3) —3φ(z—2/3).
We consider the following function f, which inserts round between every layer in f:
〜, ____ _____一 __________ ________ ____________ . 一
rPx,pWι,...,Wd)) “ round(Wdround(Wd´i …round(Wιx)∙∙∙))-0.5	(3.1)
For this demonstration, we ignore the parameters of round, though the actual proof considers these param-
eters. The following claim shows that (3.1) preserves the output of f while increasing the all-layer margin:
1
Claim 3.5. In the setting above, it holds that f (1,(1,...,1)) “ f (1,(1,...,1)) and Pf ((1,…,1),1,1)21.
This reflects a significant increase in the all-layer margin, while only increasing depth by a constant
factor. The proof is simple: We observe that if δi ≤ 3 for all i, the function output will not change because
round(z) “ 1 @z23. This immediately gives the all-layer margin lower bound ∣.
To apply this construction more generally, we note that round corrects errors in previous layers. In
the more general setting, we insert “correction functions” ζ between each layer satisfying the key
property that ζ(h1) “ h if h is the intended output of the layer and h1 is any perturbed value satisfying
}h1 一 h} 2 ≤ 3∙ Since intended outputs of layers in the function constructed by Lemma 3.2are binary-valued
in t0,1uw because F simulates a boolean circuit, we can simply apply the function round constructed
in Proposition 3.4 elementwise as the correction function. By the construction, this can be implemented
by adding two additional feedforward ReLU layers per correction function. Following the intuition
for Claim 3.5, we prove that inserting these correction functions guarantees a large all-layer margin
(Theorem D.6) on all xP t0,1um. This leads to the proof of Lemma 3.3. We can complete the proof of
Theorem 3.1 by invoking Lemma 2.3, as shown in Section B.
4	SM approximation of Turing machines with transformers
In this section, we show that transformers SM approximate Turing machines with computation time
bounded by T, using sample complexity polynomial in logT and the state space and alphabet sizes of the
Turing machine. Constructions from prior work would require the sample complexity of the approximation
to be linear in T (Siegelmann & Sontag, 1995; Chen et al., 2018; Perez et al., 2019; Bhattamishra et al.,
2020). Thus, we obtain an exponential improvement in the dependency on T.
We briefly describe a Turing machine; see (Sipser, 2013) for a more thorough survey. A Turing machine
is a model for computation specified by a tuple (Z,A,S,Zterm) containing a set of states Z, a tape alphabet
A, a transition function S: Z XA → Z XAX{—1,+1}, and set of terminal states Zerm indicating accept or
reject. For simplicity, we assume the Turing machine has a single tape, as any single-tape Turing machine
can simulate a multi-tape one with only quadratic increase in runtime (Sipser, 2013). Given an input
X P A* recorded on the left-most part of the tape, the Turing machine performs computation in a sequence
of timesteps. In each timestep, the machine determines the next state, symbol to write, and direction to
move the head via the transition function.
We let TMpZ,A,S,Ztermq denote the function computed by the Turing machine, which produces an output
in t0,1u (if the machine halts). Fixing the alphabet A, we consider the class of binary functions computed
by Turing machines with at most k states terminating in T steps:
Gk,T fi {x→TM(Z,A,s,Zterm)(x): ∣Z∣≤ k, and @xPX,TMpz,人,$品皿)terminates in T steps}	(4.1)
4.1	Transformer architecture for SM-approximating Turing machines
We study approximation of G with a family of architectures consisting of both an encoder and decoder com-
ponent (Vaswani et al., 2017), described as follows. The encoder architecture is simple and only performs
an embedding of the input symbols, using learnable symbol embeddings E P RwXAI and fixed positional
encodings β(1),β(2),...PRw. Given input xPA* with m symbols, the encoder produces m output vectors
in Rw via EnCi (x,E) “ E：,Xi 'β(i), where EnCi denotes the output of the encoder at the i-th position.
The decoder iteratively computes an output, running for T steps. We define a transformer layer of the
decoder as a sequence of modules consisting of decoder self-attention, followed by encoder-decoder
attention, followed by three feedforward ReLU layers.
7
Under review as a conference paper at ICLR 2022
Attention layers. Attention layers consist of key, value, and query functions K,V,Q, each of which com-
putes a linear transformation. We omit parameters here for simplicity. Restricted to a single decoder timestep,
the attention layer takes two types of inputs: a sequence of previously-computed representations h1,...,hi,
and a current input representation h1. The layer first applies the key, value, and query functions as follows:
T0,Tι,...,Ti = Q(h1)jK0,Q(h1)jK(h1),...,Q(h1)jK(hi)
v0,v1,...,vi “V0,V ph1q,...,V phiq
where K0 and V0 are fixed “null” key and value vectors which are learned parameters of the layer. Letting
J denote the set of indices {j: Tj = max{τ0,…,^ }},the attention layer performs hard-max attention (Perez
et al., 2019) to compute the output, as follows:
Atmph1,(hι,…,hi)) “ H ` --∣- X Vj
|J | jPJ
Our theory also applies to the standard softmax attention used in practice, but we focus on the hard-max
case for a simpler proof. Let htpjq denote the representation computed by the j-th layer of the decoder at
timestep t. At timestep i, decoder self-attention atthe (j + 1)-th layer computes Atmphpjq,(hjq,…,hpjq)).
Letting e1,...,em denote the encoder outputs, encoder-decoder self-attention at the (j ` 1q-th layer and
i-th step would compute Attn(hipjq,(e1,...,emqq.
Transformer layers. We use feedforward layers which apply 3 standard ReLU layers, as follows:
FFph) “ φ(W3φ(W2φ(W1h'b1)'b2)+b3). Our theory also allows for residual feedforward layers, and
the architecture here is chosen mainly to simplify the construction.
A transformer layer applies these constructions in sequence. Letting Hipjq “ (hp1jq,...,hipjq) denote the output
after the j-th transformer layer for timesteps 1 ≤ t ≤ i, and θpjq the parameters of the layer, We compute
hpj'1,decq “ Atmphpj),Hj),θ( + 1,dec-attn))
hpj'1,encq “ Atmphpj'1,dec),(eι,…,em)0 + 1,enc-attn))
Trphij)Hj,(eι,…,em),θpj+1))= FFph(升"),。。+ 1*
Note that we included the explicit dependence of the attention layers on the parameters for completeness.
Wenow set hpj'1q “Tr(hpjq,Hijq,(eι,..,em),θij'1q).
Decoder outputs. We consider d-layer decoders, so oi fi hipd) denotes the output of the decoder at time
i, which is also inputted to the decoder at time i'1 as follows: hp0qι “ hp砌 +β(i + 1). The initial decoder
input hp00) is a trainable parameter. The decoder runs for a fixed number of timesteps T1 and produces the
prediction θcJls hpTd1). For simplicity, we assume T1 “T, the computation time of the Turing machine family.
Note that our architecture allows long (length T) decoding sequences, whereas typical architectures in
practice use decoding sequences with roughly the same length as the input (Vaswani et al., 2017). The
architecture we study is similar to ones studied by (Perez et al., 2019; Bhattamishra et al., 2020).
We use χ∣→ FW,d,τ (χ,θ) to denote the described transformer architecture with parameters θ, w-dimensional
hidden layers, d transformer layers in the decoder, and T decoder steps. This leads to the following class
of transformer functions: Fw,d,α,τ = {x → FW,d,τ(χ,θ): }θ} 1 ≤ α}. The following theorem states that
this class of transformers SM approximates the Turing machine family G defined in (4.1) with sample
complexity polynomial in logT, k and |A|.
Theorem 4.1. In the setting above, consider the class G of functions computed by Turing machines with
at most k states, alphabet A, and computation time bounded by T steps for inputs xPX. Suppose that
W > k∣A∣'logT, d-IogT, and a = poly(k,∣A∣,logT).
Thenfor all E > 0 and any input distribution P over X, Fw,d,α,τ E-SM approximates G with respect to
'0-1,P with sample complexity poly(k,∣A∣,logT)θ´ IogPwd)).
As with Section 3, we set the surrogate loss s` in Definition 2.1 to be the all-layer margin loss defined in
Section 2.1. Commonly-used alternatives for the surrogate loss would not suffice for either our construction
or ones in prior work (Siegelmann & Sontag, 1995; Chen et al., 2018; Perez et al., 2019; Bhattamishra et al.,
2020). First, the VC dimension of Fw,d,α,τ is at least Ω(wT). This is because transformer architectures
8
Under review as a conference paper at ICLR 2022
which contain a decoder component can express RNNs, which by lower bounds have VC dimension at least
wT (Koiran & Sontag, 1998). This indicates that using `0-1 as the surrogate loss would lead to sample com-
plexities that are suboptimal in both the overparameterized width w and the computation T. Second, the cor-
rect norm-based Rademacher complexity bound to use for transformers is unclear; however, the RNN-based
equivalent would scale with the T-th power of some parameter norm, or exponentially in T. Thus, as in
Section 3, the all-layer margin surrogate loss (2.2) is essential for obtaining our sample complexity bounds.
4.2	Proof sketch for Theorem 4.1
Following Lemma 2.3, our goal is to construct a transformer which can simulate Turing machines with
large all-layer margin, namely, ω (poly(k,∣A∣,logTq )
.The fundamental limitation of prior work (Perez et al.,
2019) towards attaining this is that the positional embeddings are required to store values as small as
PoiypTy. Our construction cannot afford to rely on values this small - informally, if the construction relies
on the exact values of these small entries, then the all layer margin would be at most PolypTq because
perturbing the layer by the small entries could change the prediction. Instead, we propose using Binpiq,
the binary encoding ofi in rlogT s bits, as the positional encoding for timestep i. This allows us to use
unique positional encodings for each timestep which do not rely on arbitrary precision.
We describe the construction of the transformers. Fix a Turing machine GPG. We first require notation to
describe the computation of G. For input xPX, we define zipxq, aipxq to be the Turing machine state and
symbol under the tape head at the conclusion of step i. We let lipxq denote the location of the Turing machine
head at the conclusion of step i. During the timestep, the Turing machine computes S(Zi´i (x),a-1 (x)),
writes a new symbol under the head at location li´1 pxq, and moves the head either left or right. Let uipxq
denote the symbol written during timestep i, and qi(xq P tleft,rightu the movement direction of the head.
Following (Perez et al., 2019) with several key modifications, we simulate the Turing machine using
the transformer as follows. Each timestep will maintain the invariance that oi contains an encoding of
zi(xq,ai(xq, and li(xq. Given that this invariance holds until timestep i, the transformer simulates timestep
i`1 of the Turing machine with the following steps:
1)	Use feedforward layers to apply transition S on zi(xq and ai(xq, which can be read from oi,
to obtain zi`i (x), ui+ι(x), and movement direction qi'i (x) P {left, right}.
2)	Using feedforward layers, compute li'ι(x) from qi'ι(x) and the encoding of li(x) in oi.
3)	Compute ai'ι(x). We use decoder self-attention to search over past timesteps which wrote to
li+ι(x). Our aim is to find ui(x), where i1 “max{j ≤ i' 1: j´ι(x) “ li'ι(x)}. We implement
a binary search over past timesteps j, which is needed to find the largest j ≤ i ' 1 where
j´i (x) “ li`i (x). The binary search can be implemented with O(JlogTS) decoder self-attention
layers, and the construction ensures large all-layer margin.
4)	If no such i1 from the previous timestep existed, we check whether li'i(x) contained an input
symbol using encoder-decoder attention and copy this input symbol if so.
5)	If no symbols were found in 3) or 4), li`i (x) must contain the blank symbol (meaning it wasn't
visited yet by the head). Thus, we have computed ai`i (x), so we have all the information needed
to compute the new embedding θi`i.
To lower bound the all-layer margin of the constructed transformer, we use Theorem D.6, which requires
existence of a “correction function” which can correct outputs in previous layers. Since we construct a
network with intermediate layer entries in {0,1}, we can use the same correction function as Section 3.1,
which rounds to the nearest bit. The full proof is provided in Section C.
5	Conclusion
This work proposes a new definition of approximation, statistically meaningful approximation, which
ensures that the approximating family not only has sufficient expressivity, but also exhibits good statistical
learnability. Towards a first analysis with this definition, we show approximability of two function classes:
boolean circuits and Turing machines, with strong sample complexity guarantees depending only on the
intrinsic properties of these function classes. There are several interesting directions to extend our study
of statistically meaningful approximation. Examples include proving more upper and lower bounds for
statistically meaningful approximation for different target functions and neural net architectures, and using
our definition as a lens to compare architectures.
9
Under review as a conference paper at ICLR 2022
6	Ethics and reproducibility statements
An ethics statement is not applicable for this work - this work is mainly theoretical and is several layers
removed from empirical applications.
Section A contains the proofs for Section 2. Section B contains the formal construction and proof for
boolean circuits. Section C contains the formal construction and proof for Turing machines. Section D
rigorously introduces the correction function machinery and lower bounds the all-layer margin in terms
of properties of the correction functions.
References
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal of Computer and system sciences, 58(1):137-147, 1999.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.
Chenglong Bao, Qianxiao Li, Zuowei Shen, Cheng Tai, Lei Wu, and Xueshuang Xiang. Approximation
analysis of convolutional neural networks. work, 65, 2014.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930-945, 1993.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. The Journal of Machine Learning
Research, 20(1):2285-2301, 2019.
Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In International
conference on algorithmic learning theory, pp. 18-36. Springer, 2011.
Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of trans-
formers and its implications in sequence modeling. In Proceedings of the 24th Con-
ference on Computational Natural Language Learning, pp. 455-475, Online, November
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.conll-1.37. URL
https://www.aclweb.org/anthology/2020.conll-1.37.
Vaggos Chatziafratis, Sai Ganesh Nagarajan, Ioannis Panageas, and Xiao Wang. Depth-width trade-offs
for relu networks via sharkovsky’s theorem. arXiv preprint arXiv:1912.04378, 2019.
Vaggos Chatziafratis, Sai Ganesh Nagarajan, and Ioannis Panageas. Better depth-width trade-offs for
neural networks through the lens of dynamical systems. In International Conference on Machine
Learning, pp. 1469-1478. PMLR, 2020.
Yining Chen, Sorcha Gilroy, A. Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks
as weighted language recognizers. In NAACL-HLT, 2018.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions.
In International Conference on Machine Learning, pp. 955-963. PMLR, 2016.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on learning theory, pp. 698-728. PMLR, 2016.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Amit Daniely. Depth separation for neural networks. In Satyen Kale and Ohad Shamir
(eds.), Proceedings of the 2017 Conference on Learning Theory, volume 65 of Pro-
ceedings of Machine Learning Research, pp. 690-696. PMLR, 07-10 Jul 2017. URL
http://proceedings.mlr.press/v65/daniely17a.html.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
on learning theory, pp. 907-940. PMLR, 2016.
10
Under review as a conference paper at ICLR 2022
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural
networks. In Conference On Learning Theory,pp. 297-299. PmLr, 2018.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise
linear neural networks. In Conference on Learning Theory, pp. 1064-1068. PMLR, 2017.
John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D Manning. Rnns can generate
bounded hierarchical languages with optimal memory. arXiv preprint arXiv:2010.07515, 2020.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural networks, 2(5):359-366, 1989.
Ziwei Ji, Matus Telgarsky, and Ruicheng Xian. Neural tangent kernels, transportation mappings, and
universal approximation. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HklQYxBKwS.
Pascal Koiran and Eduardo D Sontag. Vapnik-chervonenkis dimension of recurrent neural networks.
Discrete Applied Mathematics, 86(1):63-79, 1998.
Samuel A Korsky and Robert C Berwick. On the computational power of rnns. arXiv preprint
arXiv:1906.06349, 2019.
Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets
to express distributions. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference
on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1271-1296. PMLR,
07-10 Jul 2017. URL http://proceedings.mlr.press/v65/lee17a.html.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks
with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):
861-867, 1993.
Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approximator.
arXiv preprint arXiv:1806.10909, 2018.
Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. The connection between approx-
imation, depth separation and learnability in neural networks. arXiv preprint arXiv:2102.00434, 2021.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers
neural networks. Proceedings of the National Academy of Sciences, pp. E7665-E7671, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Deterministic pac-bayesian generalization bounds for deep
networks via generalizing noise-resilience. arXiv preprint arXiv:1905.13344, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.
In Conference on Learning Theory, pp. 1376-1401. PMLR, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.
Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In International
conference on machine learning, pp. 3730-3739. PMLR, 2018.
Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks.
Neural computation, 3(2):246-257, 1991.
Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin. Provable memorization via deep neural networks
using sub-linear parameters. arXiv preprint arXiv:2010.13363, 2020.
Jorge Perez, Javier Marinkovic, and Pablo Barcel6. On the turing completeness of modern neural network
architectures. arXiv preprint arXiv:1901.03429, 2019.
Akiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation
invariant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939, 2019.
J. Savage. Models of computation - exploring the power of computing. 1998.
11
Under review as a conference paper at ICLR 2022
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
Computational capabilities of graph neural networks. IEEE Transactions on Neural Networks, 20(1):
81-102, 2008.
Anton Maximilian Schafer and Hans-Georg Zimmermann. Recurrent neural networks are universal
approximators. International journal of neural systems, 17(04):253-263, 2007.
Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of
computer and system sciences, 50(1):132-150, 1995.
Michael Sipser. Introduction to the Theory of Computation. Course Technology, Boston, MA, third
edition, 2013. ISBN 113318779X.
Matus Telgarsky. benefits of depth in neural networks. In Vitaly Feldman, Alexander Rakhlin, and Ohad
Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine
Learning Research, pp. 1517-1539, Columbia University, New York, New York, USA, 23-26 Jun 2016.
PMLR. URL http://proceedings.mlr.press/v49/telgarsky16.html.
Takeshi Teshima, Koichi Tojo, Masahiro Ikeda, Isao Ishikawa, and Kenta Oono. Universal approximation
property of neural ordinary differential equations. arXiv preprint arXiv:2012.02414, 2020.
Gal Vardi and Ohad Shamir. Neural networks with small weights and depth-separation barri-
ers. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 19433-19442. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
e1fe6165cad3f7f3f57d409f78e4415f-Paper.pdf.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
Roman Vershynin. Memory capacity of neural networks with threshold and relu activations. arXiv preprint
arXiv:2001.06938, 2020.
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation. arXiv preprint arXiv:1905.03684, 2019a.
Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classification
via an all-layer margin. arXiv preprint arXiv:1910.04284, 2019b.
Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite precision
rnns for language recognition. arXiv preprint arXiv:1805.04908, 2018.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive
Approximation, pp. 1-68, 2021.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight analysis
of memorization capacity. arXiv preprint arXiv:1810.07770, 2018.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transform-
ers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities of neural odes and
invertible residual networks. In International Conference on Machine Learning, pp. 11086-11095.
PMLR, 2020.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and computational
harmonic analysis, 48(2):787-794, 2020.
12
Under review as a conference paper at ICLR 2022
A Proofs for Section 2
We prove Proposition 2.2 and Lemma 2.3.
Proof of Proposition 2.2. Let pxiqin“1 denote a n i.i.d. training examples drawn from P and fix GPG. De-
fine L(F) fi Ex〜p['(F,x,G(x))S and L(F) fi 1 Xi=ι'(F,Xi,G(xi)). Let FP F denote argmmFPFL(F),
the empirical risk minimizer of L, which we aim to show has population loss for fitting G bounded by
O(e' 力).By standard arguments using Rademacher complexity, We have with probability 1 — 6,
V
.. __ O_______	,.
sup∣L(F) — L(F) ∣≤ 2Radn,p (Lg) '
FPF
log(2/δ)
n
& 2e+∖ —	(A.1)
n
Now note that by the condition 3) on ', there exists F< with L(F<) ≤ U Now we have
^ ^ ʌ ʌ ʌ ʌ ʌ ʌ
L(F)´L(F <) ≤ (L(F)´L(F)) + (L(F)´L(F *)) + (L(F <) —L(F <))
We bound the first and last term in parenthesis by applying (A.1), and the middle term is bounded by
0, by definition of F . It follows that
L(F)—L(F <)≤ 4e+2cj logB^
n
一 L(F)W 5e+2∖Clog(^M
n
1	1 T / L 八 一 L 11	.1	..1	. ^Λ	1	1	/1 TTTl	rc∕6∕∖x'Y∕∖∖^l	- T /
where we used L(F<) ≤ 巳 Finally, we use the fact that' upper bounds', so Ex„p ['(F(χ),G(χ))S ≤ L(F).
Plugging in δ “ 0.01 gives the desired result.	□
ProofofLemma 2.3. We first observe that sγ (θ,x,y) ≤ l(ρ(θ,x,y) V γ) by definition, so by (2.3), for all
GPG we have
inf Ex〜P [S(θ,x,G(x))S≤ E
θPΘ
Thus, it remains to check the Rademacher complexity condition for applying Proposition 2.2. Fixing any
GPG, define the function class LG as in Definition 2.1.
We first observe that following the same argument as Claim A.4 of (Wei & Ma, 2019b) (except we apply
the perturbations to the parameters, rather than the hidden layers), ∣ρ(θ,χ,y) —ρ(θ1,χ,y)∣ ≤ }θ 一 θ1}2 for
any θ,θ1 PRp. Let N∣∣.}2 (ε,Θ) denote the ε-covering number of Θ in ∣∣∙}2-norm, andN}∙∣8 (e,Lg) the ε-
covering number ofLG in the norm defined by }H —H1}8 “maxxPX |H(x)—H1(x)| for any H,H1 PLG.
The arguments of (Wei & Ma, 2019b) imply that logN∣∣.}8 (e/g)w logN∣∣.}2 (γε,Θ) ≤ O(『Yog”]),
where the last inequality is from standard covering number bounds for }∙}ι balls. Now we can apply
this covering number bound in the Dudley entropy integral, another standard step to bound Rademacher
complexity, to obtain that for all n, Radn,p(LG) W alog；?^gppq (see arguments in (Wei & Ma, 2019b) for
more detail). Solving for n such that the r.h.s. of this equation is bounded by E gives the desired result. □
Note that from the proof of Lemma 2.3, we would also obtain the following parameter-space all-layer
margin generalization bound as a corollary, which may be of independent interest:
Corollary A.1. In the setting of Lemma 2.3, let Q denote a distribution over (x,y) pairs, with (xi,yi)in“1
denoting a set of n i.i.d. training samples from Q. With probability 1 — δ over the draw of the training
samples, all classifiers F(∙,θ)pF which achieve zero 0-1 training loss satisfy
Ex„Q ['0-1 (F (x,θ),y)S ≤ o( aa?g(p) f 卜 t	1	2]+ξ	(A.2)
∖	√n ∖ n“ρ(θ,χi,yi)2,
where ξ < O ´ Iog(I/^^仙))is a low-order term.
The proof of Corollary A.1 simply follows by plugging in the coverning number bound on ρ derived in
the proof of Lemma 2.3 into Lemma 2.2 of (Wei & Ma, 2019b).
13
Under review as a conference paper at ICLR 2022
B	Proofs for Section 3
This section completes the proof of Section 3. The following lemma formally states that we can construct
the neural net to simulate the circuit layerwise.
Lemma B.1. In the setting of Theorem 3.1, let G denote the layered boolean circuit, which we aim to
compute using a neural net Let Gi: {0,1}riτ → {0,1}ri denote function computed between the i — 1 -th
and i-th layers ofG, which we assume have ri´1 and ri gates, respectively. Let f denote the following
2-layer neural net architecture, parameterized by θ =(W1,b1,W2,b2):
f(h,θ)= φ(W2φ(W1h'b1)'b2)
Then there exist θ with }θ}ι “ Opriq SuCh thatfor any hp{0,1}riτ,
.〜	---r-r
f (h,θq=Gi(h
where h takes h and appends w—ri´i zeros, and likewisefor Gi(h).
We note that the proof of Lemma 3.2 follows by applying Lemma B.1 q— 1 times. Using Lemma B.1,
we can complete the proof of Theorem 3.1.
Proof of Theorem 3.1. Our proof will construct a neural network to compute any boolean circuit with
all-layer margin lower bound Polypr 勺).By Lemma 2.3, this will be sufficient to guarantee meaningful
approximation.
There are two steps in our construction: first, given any layered circuit G P Gq,r,s, we construct a neural net
that directly simulates G by computing the layers of G one-by-one. Our construction shows that we can
compute every layer in G using two feedforward ReLU layers, and results in a neural net F computing G,
next step is to convert Fp into a neural net with large all-layer
but with possibly small all-layer margin. The
margin, i.e., implement Lemma 3.3. To do this, we insert “correction functions” (Definition D.1) between
every group of layers in Fp. These correction layers leverage the knowledge that unperturbed outputs of
these layers should be contained in t0,1uw and perform elementwise rounding to map perturbed values
back to t0,1uw. Theorem D.6 formally shows that by introducing these correction layers can guarantee a
lower bound on the all-layer margin roughly depending on the Lipschitz constants of each individual layer.
Furthermore, each correction layer can be computed via two feedforward ReLU layers, so introducing
the correction layers only increases depth by a constant factor.
We implement the proof plan by first applying Lemma B.1 q times in order to obtain the function F
computing G (with padding) mentioned above. The total }∙ }ι-norm of the parameters so far is at most
s. Now we use the correction function described in Proposition 3.4, which we apply coordinate-wise on
non-padding coordinates. We apply the correction functions after each layer constructed in Lemma B.1.
Note that each correction function requires at most double the width of the corresponding layer in the
circuit, and the parameters for all correction functions add total }∙ }ι-norm at most O(s).
Note that at this point, minor modifications are still required in order to apply Theorem D.6. The neural
net output is in t0,1uw, not t—1,1u; we can remedy this by setting the last layer to compute the linear
transformation Z → 2z — 1 on the single non-padding coordinate corresponding to the output. Second,
to make the depth of the architecture consistently d, we can add sequences of identity functions before this
last linear layer just constructed, followed by correction layers, until each of the constructed approximating
functions reaches the desired fixed depth d. This finally gives us parameters θ with } ∙ }ι-norm bound
O(s ` dq, so that the set of constructed functions is contained in Fw,d,α. Thus, we showed that for
G P Gq,r,s, there exists θ such that F (x,θq “2G(xq—1 for all x P t0,1um.
Finally, itis straightforward to check that Condition D.3 for Theorem D.6 is satisfied for Lipschitzness
parameters which are polynomial in the circuit width r. Thus, we apply Theorem D.6 to obtain a lower
bound P “ Polypr qqePolyPsq on the all-layer margin for every input X p{0,1}m. Finally, We directly apply
Lemma 2.3 using Y “ P to obtain the desired result.	□
The following proposition will be used to construct basic gates in the circuit with a simple feedforward
ReLU network.
Proposition B.2. Let x “ x1 P t0, 1u2 be binary inputs to AND and OR gates. The following
x2
feedforward ReLU networks compute the AND and OR functions: FAND(xq “ φ(x1 ` x2 — 1q, and
FOR(X) “ 1 —0(1—xι —x2).
14
Under review as a conference paper at ICLR 2022
Proof of Lemma B.1. Each row of W1 and value in b1 will correspond to a single entry in the output
of Gi. The same applies for W2,b2. W2 Will be set to a diagonal matrix With entries in {—1,0,1}. For
the 0 entries which only serve to pad the dimension, we set corresponding values in W1,b1,W2,b2 to be
0. For the remainder of the entries of Gi corresponding to actual gates in the circuit, in the case that the
gates compute AND or OR, We fill in the values of corresponding roWs in W1,b1,W2,b2 to implement
the constructions for AND and OR in Proposition B.2. The construction for ID and NOT are even simpler.
For example, to implement NOTpzq “ 1—z for zP{0,1} on coordinate j, We can set the j-th roW ofW1
to have -1 on the diagonal and 0 everywhere else, (b) “ 1, (b2)j = 0, and (W2)j,j = 1. It is easy to check
that }θ}1 “Opriq With this construction.
C	Proof of Theorem 4.1
C.1 Additional setup and notation
We fix any Turing machine G P G and construct a transformer Which can simulate G. Throughout this
section, a superscript Will be used to index layer indices, and a subscript to index timesteps.
We assume that the initial state of the tape has the input Written at the left-most positions. The Turing
machine always starts at a fixed initial state zint. We let [0]p A denote the blank symbol, which initially
fills all positions on the tape Which aren’t part of the input. We construct a transformer that simulates
the Turing machine up until it reaches a terminal state in Zterm, at which the transformer will loop in that
state until it hits a computation time T .
We introduce some notation which will appear throughout the construction. Define wpos fi rlog2 Ts.
We use wpos to denote the effective dimension of the position embedding, as only wpos coordinates will
be non-zero. For 0 ≤ i ≤ T, define Bin(i) P Rwpos to be the vector containing the binary encoding of i:
Binpiqj “ 1 if the binary representation of i contains 1 in the j-th bit and 0 otherwise.
For simplicity, the proof will focus on the setting without overparameterization, where we choose the di-
mension W “ WTM 冬 ∣Z∣'2∣A∣' 3wpos' WSCr for storing all the hidden representations of the model, where
WSCr =O(WPOS' |A| + ∣Z∣). We can extend our analysis to allow for arbitrary over-parameterization using
w > WTM by designating a certain subset of the coordinates to always equal 0, and performing calculations
using only a subset of WTM coordinates. We group the WTM coordinates using the following symbols: st for
encoding the state, sym1, sym2 for encoding symbols, pos1 and pos2, pos3 for encoding position, and scr,
which is used as scratch space. Thus, for hPRw, we can index its coordinates via the groups as follows:
h=
hsym2
hpos1
hpos2
hpos3
hscr
PR|Z|
P R|A| ffi
P R|A| ffiffi
P Rwpos
P Rwpos
P Rwpos
P Rwscr
When the meaning is clear from context, we use the superscript to index coordinate groups as described.
The position embedding β(iq is defined formally so that β(iqpos1 = Bin(iq, and β(iq is 0 in all other
coordinates. The encoder embedding matrix E is such that
EnCi(X)SymI= l∣A∣(x)
Enci(xqpos1 = Bin(iq
(C.1)
where EnCi(X) has 0,satall other coordinates. embedding function e: A→ Rd for the encoder is defined
such that e(x)sym1 = 1|A|(x), the one-hot encoding for xPA, and 0 everywhere else. We use o1,...,oT
to refer to the output embeddings of the decoder. Our construction maintains the invariant that the output
embedding oi encodes zi(X), ai(X), li(X) for each i. To achieve this, we maintain
oist = 1|Z|(zi(X))
osiym1 =1|A|(ai(X))
oipos2 = Bin(li(X))
(C.2)
15
Under review as a conference paper at ICLR 2022
and oi has 0 at all other coordinates. Thus, the input oi+β(i+1) to the decoder at step i' 1 is of the form
(θi+β(i+1))st = 1∣z∣(zi(x))
(θi+β(i+1))symι= 1∣A∣(ai(x))
(θi+β(i+1))posι= Bin(i)
(θi'β(i'1))pos2 “Bin(li(x))
(C.3)
C.2 Completing the proof
We implement the first step 1) in Section 4.2 using the following lemma. Note that the lemma uses two
consecutive feedforward ReLU layers, but in our actual proof we will simulate this using two transformer
layers where the attention parameters are all 0, and only the feedforward layers are instantiated.
Lemma C.1. Let O denote the set ofdecoder inputs in theform (C.3) encoding zi´1(x'), a-ι (x), li´ι (x)
for some timestep i. For parameters θ “ (W1,b1,W2,b2), consider the following function computing a
sequence of two feedforward ReLU layers: f (h,θ) “ φ(W2φ(W1h ` b1) ` b2). There exist parameters
θ such that for decoder inputs h P O,
f(h,θ)st “1|Z|(zi(x))
f(h,θ)sym2 “1|A|(ui(x))
f(h,θ)pos1 “Bin(i)
(C.4)
f (h,θ)pos2 “Bin(liτ(x))
Furthermore, f (h,θ)scr will contain a one-hot encoding for qi(x), and besides this, f (h,θ) is 0 at all other
coordinates. The parameters satisfy }θ}ι “ O(∣Z∣∣A∣'Wpos).
Proof. We follow the construction used in Lemma B.2 of (Perez et al., 2019). The first layer computes
a one-hot encoding of the state, symbol input pair. We choose Wi:RwTM →RlZllAl'wτM so that the first
|Z |A| rows are described by:
(W1)sptz,aq,:“1|Z|(z)
(W1)spyzm,a1q,:“1|A|(a)
and 0 everywhere else. The remaining rows of wTM rows of W1 simply implement the identity mapping.
We choose b1 so that its first |Z ||A| entries are -1, and all other entries are 0. We observe that from this
construction, for all hPO where h encodes ziτ(x),ajι(x),
φ(Wιh+bι) =
1∣Z∣∣A∣((ziτ(x),aiτ(X)))
h
This is because before the ReLU, the first |Z||A| entries of Wih will have 2 on the (ziτ(x),aiτ(x))-th
entry and be bounded by 1 everywhere else, so adding α1 and applying the activation will zero out all
but one entry.
Now it is simple to pick W2 so that f(h,θ) is as desired because we can construct it to exactly encode
the output of S(z,a) for each of its first (z,a) columns and copy over the other necessary entries of h
as needed by (C.4).	口
The next lemma demonstrates that we can use an additional sequence of feedforward ReLU layers to
produce Bin(li(x)), given Bin(liτ(x)) and qi(x).
Lemma C.2. In the setting of Theorem 4.1 and Lemma C.1 above, there is a function f parameterized
by θ composed of O(wpos) feedforward ReLU layers such that for any h computed by the function in
Lemma C.1 in the form (C.4) at timestep i,
f(h,θ)st “1|Z|(zi(x))
f(h,θ)sym2 “1|A|(ui(x))
f(h,θ)pos1 “Bin(i)
(C.5)
f (h,θ)pos2 “Bin(liτ(x))
f(h,θ)pos3 “Bin(li(x))
At all other coordinates, F (h, θ) takes value 0. Furthermore, the parameters satisfy
}θ}i “O(Wpos(∣Z∣'∣A∣'Wpos)).
16
Under review as a conference paper at ICLR 2022
Proof. As the construction of Lemma C.1 encoded qipxq, the movement direction of the head, we can use
feedforward ReLU layers to implement binary addition to either add or subtract 1 from lι(χ). Let v1,v2
denote the bits in the scratch dimensions indicating the head movement, where v1 “ 1,v2 “0 indicates left
and v1 “ 0,v2 “ 1 indicates right. Then more specifically, we first use Opwposq feedforward ReLU layers to
compute li´i(x)—vι, and then O(WPoS) additional feedforward ReLU layers to compute li´i(x)—vι 'v2.
Note that the output would always be lipxq by the definition of v1,v2.
It remains to implement a module which computes Bin(j — v1) given v1,Bin(j), and Bin(j ` v2) given
v2,Bin(j) for anyjP rTs. We can express the binary addition by a depth-O(wpos) binary circuit, which can
in turn be expressed by a neural net with O(wpos) layers where each weight matrix has }∙ }ι-norm (∣Z∣'
∣A∣'Wpos) (which is required to implement the identity mapping to copy forward the other dimensions
of h which aren,t involved in the binary addition). This gives the desired total }∙ }ι-norm bound. □
The next lemmas implement steps 3), 4), 5) in Section 4.2. For the following lemmas, it will be helpful
to further index the scratch dimensions as follows: for a vector h P wscr,
hscr “
» hscr1 PR|A| fi
— hscr2 PR|A| ffi
hscr3 P Rwpos
hscr4 PR3
Lemma C.3. In the setting of Theorem 4.1 and Lemma C.2 above, fix any timestep i and define
i1 “ max{1 ≤ t ≤ i: l-ι(x) = li(x)}. If j such that ltτ(x) “ li(x) exists, we define i1 “ 0 otherwise.
Consider any Hi “ (h1,...,hi), where ht is computed by the layer in Lemma C.2 for timestep t, and in
the form (C.5). There is a function f parameterized by θ consisting of O(wpos) total self-attention and
linear layers such thatfor all such Hi, the following holds:
f(hi,Hi,θ)st “1|Z|(zi(x))
f(hi,Hi,θ)sym2 “1|A|(ui(x))
f (hi,Hi,θ)pos1 “Bin(i)
f (hi,Hi,θ)pos2 “Bin(liτ(x))
f(hi,Hi,θ)pos3 “Bin(li(x))
f (hi,Hi,θ 产“"iQi'(X))
F (hi,Hi,θ)1cr4 = l(i1 > 0)
At all other coordinates, F(H, θ) takes value 0.
}θ}l “O(Wpos(∣Z∣ ' | A| 'Wpos)).
if i1 > 0
otherwise
(C.6)
Furthermore, the parameters satisfy
The proof plan will roughly implement a binary search to find i1 , leveraging the attention layers. The
first step in the binary search is to verify whether i1 > 0, described below.
Claim C.4. In the setting of Lemma C.3, let Hi “ h1,...,hi be the input representations for timesteps 1,...,i.
Suppose that each ht for 1 ≤ t ≤ i satisfies thefollowing:
htpos1 “ Bin(t)
hpos2= Bin(ltτ(x))
(C.7)
Additionally, suppose that hi is of the form in (C.5). Then there is a function fp0q parameterized by θ
such that
f p0q(hi,Hi,θ)scr1 “0
fp0q(hi,Hi,θ)scr3 “0	(C.8)
f p0q(hi,Hi,θ)lcr4 = l(i1 > 0)
The function fp0q can be computed by a single decoder self-attention layer with }θ}1 “O(wpos).
Next, we implement the binary search itself, using wpos self-attention layers. Each step of the binary
search reveals a single bit of i1, so the j-th attention layer will compute a representation storing the j most
17
Under review as a conference paper at ICLR 2022
significant bits ofi1. We letBinjplq P t0,1uwpos to denote the binary encoding ofthej most significant bits
of l: (Binj(l))ji =(Bin(l))ji for 1 ≤ j1 ≤ j, and (Binj(l))j, “0 for j1 > j. We also SetBin°(l) = 0. We use
the superscript pjq to indicate the j -th set of layers in the binary search. The following claim implements
each step of the binary search rigorously.
Claim C.5. In the setting above and of Lemma C.3, let Hipjq “ hp1jq, ... , hipjq be the representations
computed after the j-th group oflayersfor timesteps 1 through i, for 0 ≤ j ≤ WpoS — L Suppose that each
hpjq for 1 ≤ t ≤ i satisfies thefollowing:
htpjq,pos1 “Bin(tq
hjq,pos2 “Binplt_1(x))
(C.9)
In addition, suppose that hipjq satisfies:
hipjq,scr1 “0
hj),scr3 = " Binjpi1q	f i1 >0	(C10)
i	0	otherwise	.
(hPjq,scr4 )1 = l(i1 > oq
with all other coordinates matching the quantities prescribed in (C.5). Then there is a function f ρj'ιq
parameterized by θ such that
f pj'1q(hPjq,HPjq,θ)scr1 “ 0
f"'"WHpjq,θqscr3 “ " BMIpiIq	f>wise	(CII)
fpj'1q(hPjq,Hpjq,θqicr4 “ l(i1 >0)
with all other coordinates matching those prescribed in (C.5). We note that f pj'1q consists ofa single
decoder self-attention layerfollowed by single feedforward ReLU layer, with }θ}ι “ O(∣Z∣' ∣A∣'wpos).
At the end of the wpos-th application of the binary search, we would have found Bin(i1q exactly. It remains
to apply another attention layer which attends directly to timestep i1 and copies ui1 (xq.
Claim C.6. In the setting above and of Lemma C.3, let Hi “ h1,...,hi be the representations computed
after the wpos-th group of layers constructed in Claim C.5 for timesteps 1 through i. Suppose that each
ht for 1 ≤ t ≤ i satisfies thefollowing:
htym2 = l∣A∣(ut(x)q
htpos1 “ Bin(tq	(C.12)
hpos2 = Binplt´i(Xqq
In addition, suppose that hi satisfies:
hsicr1 =0
Bin(i1q
0
ifi1>0
otherwise
(C.13)
(hicr4 )i = l(i1 > oq
with all other coordinates matching the quantities prescribed in (C.5). Then there is afunction f PwPoS'1q
parameterized by θ such that f PwpoS'1q(hi,Hi,θq COmPuteS the desired output in (C.6). Furthermore,
f PwpOS'1q consists ofa single decoder self-attention layer followed by a single feedforward ReLU Iayer
and }θ}ι = O(∣Z∣'∣A∣'wposq.
Putting these together, we complete the proof of Lemma C.3.
Proof of Lemma C.3. For the purposes of this proof, we index the layers by a superscript to avoid
confusion with indexing timesteps. We set fP0q to be the function defined in Claim C.4. We note that
18
Under review as a conference paper at ICLR 2022
layers output by fp0q satisfy the condition of Claim C.5, so we can apply Claim C.5 inductively to obtain
layers f p1q,...,f pwposq where their applying their composition results in representations satisfying (C.12)
and (C.13). Now We set f PwPos'1q to be the function constructed in Claim C.5, which gives the desired
output. Finally, we note that by summing the }∙}ι bounds for the parameters constructed in each layer,
we can finally obtain }θ}ι “OpwPosP∣Z∣'∣A∣'Wpos)).	□
We fill in the proofs of Claims C.4, C.5, and C.6 below.
Proof of Claim C.4. To construct the decoder self-attention, the query function will be of the form
Qphq “ WQh'bQ and Kph “ WKh'bκ, where WqWk PRpwpos'1qχw and bQ,bκ PRwpos'1. We
choose the parameters such that the following equations hold:
Qphq1:wpos “2hpos3 ´1
Q(hqwpos'1 = 1
and
K(h)Lwpos= 2hpos2 ´l
Kphqwpos `1 “ 0
The value function V(h) is such that V(h)1cr4 = 1, and V(h)' = 0 on all other coordinates ', which
can be implemented by a linear transformer. Finally, we set the null key K0 and value V0 such that
(Ko)wpos'i “ wpos — 1, with 0 everywhere else, and V0 = 0. Letting θatt∩ denote the attention parameters,
the layer is of the form
f ⑼(hi,Hi 阳“Attn(hi,Hi,θ)
Ib see that f p0q satisfies (C.8), observe that if i1 > 0, Q(hi)JK(hii) “ wpos by (C.7) and construction
of Q,K. On the other hand, Q(hiqJK0 “ wpos — 1. Thus, argmaxtQ(hiqJK(htq P ris, which implies
that f p0q(hi, Hi, θ)s1cr4 “ 1 by the construction of V. In the other case where i1 “ 0, we note that
Q(hi)JK(ht)^ wpos — 2 forall 1 ≤ t ≤ i, so the null position is attended to. By construction of V0, this
implies f p0q(hi,Hi,θ)1cr4 = 0. As V,Vq are 0 on all other coordinates, it follows that (C.8) holds. It,s also
easy to observe that the }θ}ι is as desired.	□
ProofofClaim C.5. The first layer in f pj'1q computes decoder self-attention. The query function is of
the form Q(h) “ WQh'bQ, and the key function is of the form K(h) “ WK h'bh, where WqWk P
RPwPOS'j'2qχw and bQ,bκ PRPwPoS'j'2q. We choose the parameters so that the following equations hold:
Q(h)1:wpos “2hpos3—1
Q(h)w 豌 `i:wpos`j “ 2h1cj3 ´ 1
Q(h)wpos `j`1 “ 1
Q(h)wpos `j`2 “ 1
and
K(h)1:wpos “2hpos2 —1
K(h)
wpos'1 :wpos'j'1 “ 2h1j'1 — 1
K(h)wpos'j'2 “ 0
Both of these functions can be constructed via linear transformations of h, with }Wq}i '}Wk }i ' ∣Mq}i '
}bκ}ι = O(wpos). Now we construct the value function V(h) = WVh'bv such that V(h)3cr4 = 1 and
V(h)' = 0 on all other coordinates, which is also easily implemented by a linear layer. For the attention, the
last quantities to construct are the null key K and value V0. K will satisfy (K0)wpos'j'2 = wpos'j, with
0 everywhere else. V0 will simply be 0 on all coordinates. Letting θattn = (WQ,bQ,Wκ,bκ,WV,bV,K0,V0)
denote the attention parameters, the first layer will now be in the form
f(升力^^区⑶伍出)=Attn(hpjq,Hipjq£attn)
19
Under review as a conference paper at ICLR 2022
where Attn uses the constructed key, value, and query functions. We claim that f pj+1),1ph?),Hij) £加
satisfies the following:
f (j+1),1(hpjq,Hijq,θattn)3cr4
"1 if i1 > 0 and has (j ' 1)-th bit 1
0 otherwise
(C.14)
For all other coordinates ', fO1 (hj,Hijj^^' “(hpjq)'. To see this, We first observe that
Q(hipjqqJK0 “ wpos ` j. Next, we observe that Q(hiijqq1:wpos produces the encoding of li(xq using
binary {—1, +1} bits, and K(hpjqqi：wpos produces the encoding of lι(x) using binary {—1, +1}
bits by (C.9). In addition, Q(hpjq)wpos'Lwpos'j “ 2Binj(i1) — 1 if i1 > 0 and all 0's otherwise, and
K(htjq)wpos'Lwpos'j'i = 2Binj+ι (t) — 1. Note that by our construction, the maximum possible value of
Q(hiijqqJK(hitjqq is wpos+j+1, and the next largest possible value is wpos +j —1. Now there are 3 cases:
Case 1: i1 = 0. In this case, we note that li(x) never matches lt´ι(χ) for 1 ≤ t ≤ i. Thus, by construction
of the first wpos coordinates of Q and K, the largest possible value of Q(hiijq)JK(hitjq) is wpos +j — 1,
so the attention will always only attend to the null position, so the layer adds V0 = 0 to hiijq, preserving
its value. Note that (hiijq,scr4 )3 =0 in this case, which matches the desired behavior.
Case 2: i1 > 0, and has (j + 1)-th bit 0. In this case, we note that for all t > i1, Q(hpj))J K (hpjq) ≤ wpos+j —
1, because by definition such t must satisfy lt´ ι (x) ‰ li(x), so the first Wpos coordinates contribute at most
Wpos — 2 to the dot product. On the other hand, if t ≤ i1, t must have (j + 1)-thbit 0, so K(htjq)wpos'j'i =
—1. This doesn,t match the (wpos + j + 1)-th bit of the query, so Q(hpjq)JK(htjq) ≤ Wpos + j — 1 again.
Thus, in this case, the null position is attended to again. The same reasoning as Case 1 then applies.
Case 3: i1 > 0 and has (j + 1)-th bit 1. In this case, maxtQ(hpjq)JK(htjq) = Wpos + j + 1: for example, t =
i1 achieves this maximum by our construction. As a result, the null position is not attended to. All the values
in the positions attended to satisfy V (htijq)s3cr4 = 1, which matches the (j+1)-thbit of i1. Thus, (C.14) holds.
Finally, to complete the proof we simply append an additional feedforward ReLU layer which copies the
value f (j+ιq,1(hpjq,Hpjq,θattn)3r4 to the output bit corresponding to the position indexed by •；'rThis layer
will also set the output bit corresponding to ∙3cr4 to 0. Note that these operations can be implemented with a
linear layer, and applying a ReLU activation after won,t change the output, which is in {0,1}w. By (C.10),
the constructed function will thus satisfy (C.11). It,s also easy to observe that }θ}ι is as desired. □
Proof of Claim C.6. The attention layer uses key and query functions which each compute linear
transformations from Rw to R2wpos'1. The value function is also linear. We choose parameters such that
Q(h)1:wpos =2hpos3 —1
Q(h)wpos'1：2wpos= 2hscr3 — 1
Q(hq2wpos'1 = 1
and
K(h)1:wpos =2hpos2—1
K(h)wpos'i：2wpos= 2hpos1 — 1
K(hq2wpos'1 = 0
and
V (h)scr1 = hsym2
Furthermore, We choose null keys and positions such that (K0)2wpos'i = 2wpos — 1, and V0 = 0. To follow
the attention layer, we construct a linear layer which simply zeros out coordinates indexed by ∙scr3 and pre-
serves all other coordinates. Note that because all outputs are either 0 or 1, applying a ReLU activation won,t
change the result. To see that this construction computes (C.6), we observe that if i1 > 0, Q(hi)JK(hii)=
2wpos. Otherwise, if i1 = 0, Q(hi)JK(ht)≤ 2wpos — 2 forall 1 ≤ t ≤ i. On the other hand, it always hold
that Q(hi)JKo = 2wpos — 1. Thus, if i1 > 0, the attention attends exactly to i1, so the value function satisfies
20
Under review as a conference paper at ICLR 2022
V phi1 q “ 1|A|pui1pxqq, which would produce the output in (C.6), as desired. On the other hand, ifi1 “0, the
attention attends to the null position, so the attention layer sets f PwPOs'1q (hi,HiMqSCcI = 0. Thus, f PwPos'1q
also produces the desired output in this case. It,s also easy to observe that the }θ}ι is as desired. □
The next steP is to comPlete steP 4) in Section 4.2 using encoder-decoder attention. The following lemma
provides this construction.
Lemma C.7. In the setting of Theorem 4.1 and Lemma C.3, consider any timestep i and let h denote an
output of the function constructed in Lemma C.3, in the form (C.6). Let e1,...,em denote the outputs of the
encoder, in the form (C.1). There is a function f with parameter θ consisting ofa single encoder-decoder
attention layer such that for all such h in the form (C.6), the following holds:
	f (h,(eι,…,em),θ)st = 1∣z∣(zi(x)) f(h,(e1,...,emq,θqsym2 =1|A|(ui(xqq f(h,(e1,...,emq,θqpos1 =Bin(iq f (h,(eι,…,em),θ)pos2= Bin(lτ(x)) f(h,(e1,...,emq,θqpos3 =Bin(li(xqq 、巾1	"1∣A∣(Uii (x))	i i1 >0	C.15) f(h,(e1,...,emq,θqscr1 = 0	otherwise f………双="I""'	fR: f (h,(e1,∙∙∙,em),θ)icr4=i(i1 > oq f (h,(eι,…,em),θ)2cr4 = l(li(x)W m)
At all other coordinates, fph, pe1, ... , emq, θq takes value 0. Furthermore, the parameters satisfy
}θ}l =O(∣A∣'Wpos)∙
Proof. We choose the encoder-decoder attention layer so that the key, value, and query functions are linear
transformations. The key and query functions map Rw to Rwpos `1 and compute the following:
Q(hq1:wpos “2hpos3 ´1
Q(hqwpos `1 “ 1
and
K(h)i：wpos= 2hpos1 ´l
K(h)wpos'1= 0
The value function computes
V	(hqscr2 “ hsym1
V	(hqs2cr4 “ 1
with 0's in all other coordinates. The null key Ko satisfies (Ko)wpos'i “ WPoS — 1, with 0's in all other
coordinates. The null value V0 satisfies V0 “ 0. We set
f (h,(e1,...,emq,θq “Attn(h,(e1,...,emq,θq
where Attn is the decoder-encoder attention using the key, value, and query described above. Now we
observe that from this construction, if h is in the form provided in (C.6), then Q(hq1:wpos “ Bin(li(xqq.
In addition, we have K(ejq1:wpos “ ejpos1 “ Bin(jq for 1 ≤ j ≤ m. Thus, by construction of V,Ko,V0,
if li(x) ≤ m, the attention attends to position li(x) in the embedding. The value function for this
position satisfies V (eιi(x))scr2 = esym1) = l∣A∣(xιipχ)). Thus, in this case F(h,θq computes the desired
output in (C.15). On the other hand, if li(xq > m, then the attention will attend to the null position, as
Q(hqJK0 =wpos —1, and the largest possible score for all other positions is wpos —2. In this case, (C.15)
holds again. It is also easy to check that the desired bound on }θ}ι would hold.	□
Finally, we implement step 5) of the outline in Section 4.2 in the following lemma.
21
Under review as a conference paper at ICLR 2022
Lemma C.8. In the setting of Theorem 4.1 and Lemma C.7, consider any timestep i and any h output
by the function in Lemma C.7 taking the form in (C.15). Then there is a function f with parameters θ
consisting of a constant number of feedforward ReLU layers satisfying the following:
fph,θqst “1|Z|pzipxqq
f (h,θ)symι= lA∣(ai(x))	(C.16)
fph,θqpos2 “Binplipxqq
At all other coordinates, F (h, θq takes values 0.	Furthermore, the parameters satisfy
}θ}1 “ O(|Z|'|A|'wpos) ∙
Proof. It suffices to construct a sequence of layers which performs the following operations:
1)	Compute the following vector v P R3:
»1fi
0	if hs1cr4 “ 1
0
0
hs2cr4	if hs1cr4 “0
」—h2cr4」
Note that V encodes the location of the symbol a% (x), as a% (x) “ u (x) if i1 > 0, a% (x) “ Xlipxq
if i1 “ 0 and "(x) ≤ m, and °i(x) “ [0] otherwise. The vector V is a one-hot vector indicating
which of these three cases holds.
2)	We can take V1 and compute AND with all bits of hscr1 , which computes
1|A|(uii(x)) “ 1∣A∣(ai(x)) if i1 >0, and 0 otherwise.
3)	We take V2 and compute AND with all bits of hscr2, which computes 1|A|(xlipxqq ifV2 “ 1, and
0 otherwise.
4)	We take V3 and compute AND with all bits of 1|A|(r0sq, which computes 1|A|(ai(xqq ifV3 “ 1.
5)	We add the outputs of 2), 3), and 4) together, which gives 1|A| (ai(xqq. We copy this quantity
into the output coordinates indexed by ∙sym1. Then we set coordinates not listed in (C.16) to 0,
producing the desired output.
Each of these operations can be computed by a constant number of feedforward ReLU layers, with total
parameter norm satisfying }θ}ι “ O(|Z ∣'∣A∣'Wpos).	口
Proof of Theorem 4.1. We construct a neural net to compute any Turing machine with all-layer margin
lower bound po^pk ∣A∣ 匕且丁)and apply Lemma 2.3 to turn this into a statement about statistically meaningful
approximation.
For our Turing machine construction, we follow the outline laid out in Section 4.2. Fix any GP G. As
mentioned, we first consider the case where w“wTM exactly, as overparameterization is easy to deal with
by always designating some subset of extra coordinates to be 0. We construct a transformer F to compute G.
First, we note that Lemma C.1 constructs a layer to compute the functionality described in 1). Next, the layer
in Lemma C.2 performs the functionality in 2). Likewise, Lemmas C.3, C.7, C.8 construct layers which
perform 3), 4), and 5). Thus, by applying the layers constructed from these lemmas in sequence, we obtain
a transformer such that the output oT contains an onehot encoding for zT (xq: 1|Z|(zT (xqq. We can now
apply a linear weight vector θcls on the output to obtain θcJlsoT, where (θclsqz “ 1 for accept states z P Zterm
and (θclsqz “ ´1 for reject states. For inputs xPX, by our construction this computes the desired TM(xq.
Next, following Theorem 3.1, we insert correction functions (Definition D.1) between every group of
constructed layers, which can be implemented via two feedforward ReLU layers following Proposition 3.4.
rɪ-rt	.	Γ∙	11	. ∙	Γ∙	. ∙	11.	.	1 Il Il	.	.	1	/ 7 I Λ I 1 ΓΠ∖ T .	6/ 公、
The parameters for all correction functions add total} ∙ }ι-norm at most poly(k, ∣A∣,logT). Let F(χ,θ)
denote the transformer constructed this way, with parameters θ. Note that for all X P X, F(χ,θ) “ 2G(x) — 1.
22
Under review as a conference paper at ICLR 2022
Next, there are several steps remaining to convert F into the fixed architecture Fwtr d T . First, we need
to convert the layers in F into transformer layers. This is achievable because every single decoder
self-attention or encoder-decoder attention layer or feedforward ReLU module can be converted into a
transformer layer by setting the two unused modules in the transformer layer to implement the identity
function. This only increases the }∙}ι-normbypoly(k,∣A∣,logT). Note that in particUlar, we can perform
this conversion such that the correction functions form the last 2 feedforward ReLU layers in every
transformer layer. The first 3 layers in the transformer layer correspond to ones constructed in the lemmas.
Second, we need to expand the dimension to a consistent width w. This is achievable by padding each
layer with coordinates designated to be 0, without affecting any of the }∙} ι-norm bounds on the parameters.
Third, we need to expand the depth to a fixed depth d. We can achieve this by appending transformer
layers which compute the identity function (and also include correction functions) as needed.
Now we aim to apply Theorem D.6 by viewing the transformer as a very deep network with depth
d “ OpTlogT), by applying each of the steps in the transformer computation in sequence. Note that
our construction for the transformer layers is such that we can view the self-attention, encoder-decoder
attention, and single feedforward ReLU layer as a single function in the setting of Theorem D.6. The
correction function corresponds to the last 2 feedforward ReLU layers in the transformer layer. (We observe
that there are actually m layers which depend on the input x, not a single layer f0 as in the setting of
Theorem D.6, but this is a minor difference where the same argument of Theorem D.6 still easily applies.)
Note that this network uses layer-based weight sharing, which is handled by Theorem D.6. Furthermore,
the depth of this network doesn’t affect the all-layer margin because Theorem D.6 doesn’t depend on
the number of layers. We also observe that Condition D.4 holds for λ “polyp|Z |,|A|,logT ), because all
of the intermediate layers are sparse binary vectors with at most |Z|' ∣A∣'logT nonzero entries.
Finally, it remains to check that Condition D.3 can hold for all of the defined layers for parameters that
are polynomial in |Z |,|A|,logT. This is straightforward to check for transformer layers where the attention
layers have parameters 0, as standard results on the Lipschitzness of a single ReLU network would apply.
For layers where the functionality comes from the attention mechanism, we observe that for valid inputs
xPX, the largest attention score is always greater than the second largest by a margin of 1. Furthermore,
ties only occur when all of the value vectors for the attended positions are already the same. As a result,
the positions attended to by the layer will not change unless we perturb the parameters and inputs by
Ω(poly11(∣Z∣, ∣A∣,logT)). This reasoning can be used to conclude that Condition D.3 with Lipschitz
constants polyP∣Z∣,∣A∣,logT), and distance parameters Ω(polyT(∣Z∣,∣A∣,logT)) holds. As a result, the
all-layer margin bound from applying Theorem D.6 will also be Ω(poly-1(∣Z∣,∣A∣,logT)), as desired.
Finally, applying Lemma 2.3 with Y “ Ω(polyT(∣Z∣, |A∣,logT)) and using the fact that the parameter
}∙} ι-norms are bounded by α gives the desired result.	□
D All-layer margin lower bounds via correction functions
We consider a generalized architecture for a d-layer network as follows. Let fo: X ^Θo → Rw map space
of inputs xPX and parameters θPΘ0 to w-dimensional space. For simplicity we assume all intermediate
layers have dimension w, and let f : Rw X Θi → Rw be the i-th function in the neural net for d > i21.
We define fd to output values in R. Let θ =(θ0,…,θd)p Θ denote the full vector of parameters. The i-th
hidden layer hi computes the following value, defined recursively:
ho(x,θ) = f0(x,θ0)
hi(x,θ) = fi(ho(x⑶,…,hjι(x ⑼Siq
The model computes output hd(x,θ). We will assume the existence of “correction” functions ζ parame-
terized by ξ “ (ξo,…,ξdτ) P Ξo X ∙^Ξdτ which correct errors in the model output for inputs X:
Definition D.1 (Correction functions). Let F1 : X →R be a model defined by layer functions f0,...,fd.
Then Zo,…,Zd—ι: Rw → Rw, ξ is a s^tof correction functions andParametersfOr F1, θ with radius q&
iffor all i p[d—1],x P X and h P RX satisfying }p—hi(x,θ)}2 ≤ σζ,
Zi(h,ξi) = hi(x,θ)
We now define the function output F with correction layers recursively by
g0(x,θ,ξ) =f0(x,θ0)
∙> , 一 ,、 ., , —
ri(x,θ,ξ) = Zi(giτ(x,θ,ξ),ξi) @0≤i≤d—1
gi(x,θ,ξ) = fi(ro(x,θ,ξ),…,hiτ(x,θ,ξ),θi,ξi) @1 ≤i≤d
F (x,θ,ξ) =gd(x,θ,ξ)
(D.1)
23
Under review as a conference paper at ICLR 2022
We note that for all xPX, F px,θ,ξq “ hdpx,θq.
The key observation is that by adding correction layers to the model, we can transform a model with
possibly small all-layer margin on the input data to one with large all-layer margin. We first need to
characterize the Lipschitzness of the individual layers.
Definition D.2. We say that a function f (∙,θ): D → Dout is Pκθ,μ,σh,σθ) -nice on H 三 D with respect
to ∣∣∣∙∣∣∣ Vfthefollowing hold:
... . ^ ., ^.. ........................ - ^..
}f (h,θ)´f (h,P)}2 ≤ kθ}Θ—P}2max{M|||,1}	@旭一。}W σθ,h P H
}f (h,p)´f (p,pq}2 ≤〃|卜—可	@[h—h|kσh,}θ-p}Wσθ,hPH
We will focus on the following norm on tuples of inputs (v1,...,viq, where hjPRw for alljP ris:
|||(v1,...,viq||| “max}vj}2	(D.2)
j
We analyze the function F output by a model with correction layers satisfying the following assumptions:
Condition D.3. There are constants κθ,κξ,μ,σ%,σθ,σζ such that thefollowing hold.
For i21, suppose that f is (κθ,μ,σh,σθ)-nice at θi on (h0,…,hiτ)(X) with respect to ∣∣∣∙∣∣∣.
In addition, suppose that fo satisfies }fo(x,θ)-f0(x,θ)}2 ≤μo}θ—θ}2 for all XPX,θPΘo.
Furthermore, suppose thatfor all i, Zi satisfies }Zi(h,ξi) — Zi(h,ξ)}2 ≤ κξmax{}h}2,l}}ξi — ξ}2 for all
^ . ,,. ^,, 一一 ______________
P with }ξi —p}2 ≤σξ and hPRw.
These conditions are all standard Lipschitzness-based conditions on the individual layer functions. Our
lower bound for the all-layer margin will be expressed in terms of the constants here.
We will also need to assume a bound λ on the norms of each of the layers computed by hi.
Condition D.4. The norms of the true layer values are bounded, that is, Dλ such that for all 0 W i W d
and x P X,
maxt}hi(x,θ)}2,1uWλ	(D.3)
We will also consider models with weight sharing, which allows our analysis to apply to architectures
such as the transformer in Section 4.
Definition D.5 (Layer-based weight sharing). Let Θg Rw1, Θo ɑ Rw0,…,Θd ɑ Rwd be some spaces of
real-valued parameters. Suppose we wish to perform copying on parameters θ1PΘ1 to produce parameters
θ =(θo,…θd)P Θ “ Θo X …Θd, where θ% is the set ofparameters given to layer function f. We say that
a tuple offunctions T “ (τ0,…,τd) :θ → Θ is a layer-based weight sharing scheme ifeach Ti is oftheform
Ti(θ1) = (θ∏ 1 ,…必 bi)	(D.4)
where π1,...,πbi is aset of distinct indices taking values in rw1s. Note that this ensures that parameters
are not duplicated within a layer.
We will now prove our main lower bound for the all-layer margin based on inserting correction functions
at every layer.
Theorem D.6. In the above setting, suppose that Conditions D.3 and D.4 hold for a function F in the
form given by (D.1) parametrized by θ with correction layers Z0,…Zd—1 parameterized by ξ with correction
radius σg V 1. Suppose that F (x)p{—1,+1}@x P X. Then for all X P X, we can bound the all-layer
margin ofF (defined in (2.1))as follows:
ρF((θ,ξ),x,I(F(x,θ,ξ)>0))》mint -,-Z,σθ,σξ,5-,D ZA,9 hʌ,TyZ-,^;-------------u (D.5)
μo μo	2kθ 2κθλ 2κξλ 4λμκξ 4μκξ
Here the subscript F makes it explicit that the all-layer margin is for the architecture F. Furthermore, if
we consider any layer-based weight-shared model F 1(X,θ1) fiF(X,Tp1q(θ1),Tp2q(θ1)) for valid weight-tying
mappings T⑴,τp2q (Definition D.5), the same bound holdsfor PF1 (θ1,x,l(F 1(x,θ1) > 0)).
24
Under review as a conference paper at ICLR 2022
Our proof will first consider the case without weight sharing. We use P =(po,…Sdq and P =(po,…,pd´ 1)
to denote a perturbed set of parameter vectors. Furthermore, define the partially perturbed parameter sets
Pi fi (po,…,pi,θi`i,…,θd) andPi fi (po,…,pi,ξi'i,…,ξd). Wealso use p´i fi θ andp´i fiξ when convenient.
We consider perturbations such that the following norm bounds hold:
}p0一θ0}2 ≤ mint -λ,σζ }	(D.6)
μo μo
Il θi — %}2 ≤ mintσθ,5—, D ∖ }	(D.7)
2κθ 2κθλ
}pi-pi} 2 ≤ mintσξ, yhT, G Z , J-----}	(D.8)
2κξλ 4λμκξ 4μκξ
We show that such perturbations won’t change the label predicted by the model, and so therefore the
minimum of these quantities immediately gives a lower bound on the all-layer margin. Our proof will
be by induction, with the following lemma providing the base case.
Lemma D.7. In the setting of Theorem D.6, suppose that (D.6) holds. Then the following hold:
7 / a 八 7 / nλ
ho(x,θ,ξ) = ho(x,θ)
}go(x,θ,ξ)-h0(x,θ)}2 ≤mm{λ,σζ}
The next lemma provides the inductive step. Starting with the base case, we show that because of the
presence of the correction functions, the perturbations with our given bounds won’t change the next
layer output by too much. This allows the correction function to fix the output of the next layer, and this
argument can extend inductively.
Lemma D.8. In the setting ofTheorem D.6, fix some 1 ≤ i ≤ d. Suppose thatfor all 0 ≤ j V i, it holds
that for all xPX,
4	, χ>χv 、 一 ,八
rj (x,p,pjτ) “ hj (x,θ)	(D.9)
and
}gj (x,θ,ξ) ´hj (x,θ) }2 ≤ mm{λ,σζ}
In addition, suppose that Pθ,θ,Pξ,ξ satisfy (D.7) and (D.8). Then it follows that for all xPX,
}gi(x,θ,ξ)-hi(x,θ)}2 ≤min{λ,σζ}
Furthermore, for 1 ≤ i ≤ d一 1,we additionally have
hi(x,θ,ξiτ) = hi(x,θ)
Combined, the two lemmas above allow us to inductively show that the prediction of the model is not
changed whenever the perturbations are bounded by (D.6), (D.7), and (D.8). Next, we show that this
translates directly to an all-layer margin lower bound.
Lemma D.9. In the setting of Theorem D.6, suppose there exist norm bounds a0,...,ad, b0,...,bd´1 such
that whenever }θi´θi}2 ≤ ai and }ξi-ξi∣∣2 ≤ bi, |F(x,θ,ξ)-F(x,θ,ξ)∣< 1 for all XPX. Then we obtain
the following lower bound on the all-layer margin, for all xP X:
Pf ((θ,ξ),x, I(F (x,θ,ξ)> 0))> min{a0,...,ad,b0,...,bdτ}
The same lower bound applies if we consider models that use layer-based weight sharing, defined by
F 1(x,θ1q fiF(x,τp1q(θ1q,τp2q(θ1qqforvalidweight-tying mappings τ p1q, τp2q (Definition D.5).
We can combine these steps to formally complete the proof of Theorem D.6.
Proof of Theorem D.6. Assuming the perturbation bounds (D.6) (D.7), and (D.8) hold, we can apply
induction with Lemma D.7 as the base case and Lemma D.8 as the inductive step to conclude that
IL/ 公个、 L/ 八八 I 一	.TC 11 C,57∙	1 T	1 ʌ zʌ . 1 . ∙ .1 -I	1 1	1
|F (χ,θ,ξ)-F (χ,θ,ξ)∣≤ σς < 1 forall X P X. We cannow apply Lemma D.9 to obtain the desired bound
on the all-layer margin.	□
We fill in the proofs of the supporting lemmas below.
25
Under review as a conference paper at ICLR 2022
Proof of Lemma D.7. By our definitions and Condition D.3, we have
}g0(x,p,p) —h0(x,θ)}2 = }fo(x,p0)—f0(x,θ0)}2 ≤ 〃01同一p0}2 ≤ min{λ"ζ }
Now we can apply the Definition D.1 of the correction function to get
rh0(x,pθ,ξq =ζ0(g0(x,pθ,pξq,ξ0q =h0(x,θq
□
Proof of Lemma D.8. By expanding the expression for hi, we observe that
hi(x,ffq = fi(ho(x,θ),…,hiτ(x,θ),θi)
ʌ ,∙>z , O . ` ∙>z , O ^ `	4	, O ^	、一、	, _.
=fi(h0(x,p,ξ),h1(x,p,p0)…,hiτ(x,p,pτ),θi)	(D.10)
We obtained the equality via (D.9). Now we write
gi(x,p,pq= fi(ho(x,p,p),...,riτ(x,p,p),pi)	(D.11)
We subtract the two expressions and add and subtract fi(h0(x,p,ξ),r1(x,p,ξ0)…,ri´ι(x,p,ξ-ι),pi) to
obtain
gi(x,θ,ξq—hi(x,θq=E1'E2
where
— .,z> , ^ ^ z› ^ ^ ^ .
Ei &fi(ro(x,p,p),…,hi—i(x,p,p),pi)
一fi(ro(x,p,£),hi(x,p,p0)…,hi—i(x,p&—2),pi)
Ez =fi(h0(x,θ,ξ),h1(x,θ,ξ0)…,hiτ(x,θ,ξτ),θi)
´fi(h0(x,p,ξ),r1(x,p,p0)…,riτ(x,p,pτ),θi)
We first bound Ei. We note that forall 0 ≤ j ≤ i — 1
..ɔ- , ^ ^. ɔ- , ^ ^ ... ........^ ^ ^ . , , ^ ^ ....
}hj (x,P,pq — hj (x,P,pj´1q}2 = }G Pgj (x,P,pq,pjq—Zj Pgj (x,p,p),ξj)}2
≤ κξmaxt}gj(x,p,p)}2,1}}p-ξj}2
The last inequality used Condition D.3 and }p∙ —ξj ∣∣2 ≤ σξ. Now defining H1 fi (ro(x,p,p),…,riτ(x,p,p))
and Hfi (h0(x,θ,ξ),h1(x,θ,ξ0)…,hi´i(x££—2)), it follows that
IIIH´h 1∣∣∣=n.max κξmaχt}gj(x,p,pq}2,1}}pj-ξj}2
一	, O ^ ,,	,, ,	,	, O ^	,	,	一一一	一 ，c , ,,	_
Plugging m }gj(x,p,p)}2 ≤ }hj(x,θ)}2 + }gj(x,p,p) — hj(x,θ)}2 <2λ, λ> 1, and Ipj—ξj}2 ≤ 2σhλ, We
obtain |||H—H1∣∣∣≤σ九. Furthermore, we note that Hp(h0,…,hiτ)(X), so we can apply Condition D.3
and Definition D.2 to obtain
.^ . . ^ ...
}E1}2 = }fi(H 1pi) — fi(Hpi)Il2
≤μ∣∣∣H—H 1∣∣∣	(since }pi—%}2≤©9 and ||H—H11∣≤σ九)
,C ∖	UU All
≤ 2λμκξmax}ξj —ξj }2
TL ɪ	F	ILF	1	∙ C f .∙ CC	Icr-L♦	1 ʌ O	♦	♦ Il 公 Zi Il 一
Next, we bound Ez by applying Condition D.3 and Definition D.2 again, using }θi — θi}2 ≤©9:
.... ^. . ...
}E2}2 = }fi(H,Pi)—fi(H,θi)}2
,^ _ _____________ 、
≤ Kθ}pLθi}2max{∣∣∣H∣∣∣,l}
UR Cll	( W 1	( C' Il 、 Cr'
=Kθ}θi-θi}2max{}hj(x,θ)}2}jY{l}
≤ κθ∣∣pi — θi}2λ
where we applied Condition D.4. By triangle inequality, follows that
^ ^ . ...
}gi(x,P,p) —hi(x,θ)|2 ≤}E1}2 + }E2}2
^
^
≤Kθ}θi ― θi}2λ + 2λμκξmax}ξj —ξj }2
j
26
Under review as a conference paper at ICLR 2022
ʌ τ 1	,1	J	ll^zi 八 Il	1 IlU >≈ Il	1	1	.1	. .1	1	♦	1	IIl	( ʌ 、
Now by the assumptions on }θi — θi}2 and }ξj —ξj}2, we can check that the r.h.s. is bounded by mm{λ,σζ}.
Finally, we note that by Definition D.1 of the correction function, we have
ri(χ,p,piτ)“Zipgipx,p,pq,ξi)=hi(χ,θ)
1	IjC ,,1, H / 公公 7 / 八'll 一	I-1
WhereWeUSedthefaCtthat }gi(x,θ,ξ)-hi(x,θ)}2 ≤q4.	□
>λ	CC▼	ʃʌ rʌ ʌɪ .	.1	. ∙i- Il /Zi >≈λ /公 U∖ll . — λ	(	1	1	Λ .1	1	.1	1∙.∙
ProofofLemma D.9. Note that if }(θ,ξ)-(θ,ξ)}2 Va冬mintao,...,ad,bo,...,bd´ 1U, then by the conditions
of the lemma, |F (x,θ,ξ)-F (x,θ,ξ)∣v 1. However, because F (x,θ,ξ)p{-1,+1} for all x P X, the sign
of the output is unchanged, which means F(x,θ,ξ)F(x,θ,ξ) > 0. This means that we must perturb (θ,ξ)
by }∙} 2-norm at least a to satisfy the constraint in the all-layer margin definition, giving us the lower bound.
We note that a similar argument applies to layer-based weight sharing because there are no parameters
shared within a layer, so if the perturbation to θ1 has '2 norm less than a, the parameters in Tp1q (θ1), Tp2q (θ1)
will also have a perturbation of at most a in each layer. The same reasoning as before then applies. □
27