Under review as a conference paper at ICLR 2022
Can Reinforcement Learning Efficiently Find
Stackelberg-Nash Equilibria in General-Sum
Markov Games ?
Anonymous authors
Paper under double-blind review
Ab stract
We study multi-player general-sum Markov games with one of the players des-
ignated as the leader and the rest regarded as the followers. In particular, we
focus on the class of games where the state transitions are only determined by
the leader’s action while the actions of all the players determine their immediate
rewards. For such a game, our goal is to find the Stackelberg-Nash equilibrium
(SNE), which is a policy pair (π*, ν*) such that (i) π* is the optimal policy for the
leader when the followers always play their best response, and (ii) V* is the best
response policy of the followers, which is a Nash equilibrium of the followers’
game induced by π * . We develop sample efficient reinforcement learning (RL)
algorithms for solving SNE for both the online and offline settings. Respectively,
our algorithms are optimistic and pessimistic variants of least-squares value itera-
tion and are readily able to incorporate function approximation for handling large
state spaces. Furthermore, for the case with linear function approximation, we
prove that our algorithms achieve sublinear regret and suboptimality under online
and offline setups respectively. To our best knowledge, we establish the first prov-
ably efficient RL algorithms for solving SNE in general-sum Markov games with
leader-controlled state transitions.
1	Introduction
Reinforcement learning (RL) has achieved striking empirical successes in solving complicated real-
world sequential decision-making problems (Mnih et al., 2015; Duan et al., 2016; Silver et al., 2016;
2017; 2018; Agostinelli et al., 2019; Akkaya et al., 2019). Motivated by these successes, multi-agent
extensions of RL algorithms recently have gained great popularity in decision-making problems in-
volving multiple interacting agents (Busoniu et al., 2008; Hernandez-Leal et al., 2018; 2019; Oroo-
jlooyJadid & Hajinezhad, 2019; Zhang et al., 2019). Multi-agent RL is often modeled as a Markov
game (Littman, 1994) where, at each time step, each player (agent) takes an action simultaneously
at each state of the environment, observe her own immediate reward, and the environment evolves
into a next state. Here both the reward of each player and the state transition depends on the actions
of all players. From the perspective of each player, her goal is to find a policy that maximizes her
expected total reward in the presence of other agents.
In Markov games, depending on the structure of the reward functions, the relationship among the
players can be either collaborative, where each player has the same reward function, or competitive,
where the sum of the reward function is equal to zero, or mixed, which corresponds to a general-sum
game. While most of existing theoretical results focus on the collaborative or two-player competitive
settings, the mixed setting is oftentimes more pertinent to real-world multi-agent applications.
Moreover, in addition to having diverse reward functions, the players might also have asymmetric
roles in the Markov game — the players might be divided into leaders and followers, where the
leaders’ joint policy determines a general-sum game for the followers. Games with such a leader-
follower structure is popular in applications such as mechanism design (Conitzer & Sandholm, 2002;
Roughgarden, 2004; Garg & Narahari, 2005; Kang & Wu, 2014), security games (Tambe, 2011;
Korzhyk et al., 2011; Balcan et al., 2015), incentive design (Zheng et al., 1984; Ratliff et al., 2014;
Chen et al., 2016; Ratliff & Fiez, 2020), and model-based RL (Rajeswaran et al., 2020). Consider
a simplified economic system that consists of a government and a group of companies, where the
1
Under review as a conference paper at ICLR 2022
companies purchase or sell goods, and the government collects taxes from transactions. Such a
problem can be viewed as a multi-player general-sum game, where the government serves as the
leader and the companies are followers (Zheng et al., 2020). In particular, when the government sets
a tax rate, the companies form a general-sum game themselves, whose reward functions depend on
the tax rate. Each company aims to maximize their own revenue, and thus ideally they achieve a
Nash equilibrium (NE) of the induced game. Whereas the goal of the government might be achieving
the social welfare, which might be measured via certain fairness metrics computed by the revenues
of the companies.
In multi-player Markov games with such a leader-follower structure, the desired solution concept
is the Stackelberg-Nash equilibrium (SNE) (BaSar & Olsder, 1998). In the setting where there is
a single leader, SNE corresponds to a pair of leader,s policy ∏* and followers' joint policy V* that
satisfies the following two properties: (i) when the leader adopts ∏*, V* is the best-response policy
of the followers, i.e., ν* is a Nash equilibrium of the followers’ subgame induced by π*; and (ii) π*
is the optimal policy of the leader assuming the followers always adopt the best response.
We are interested in finding an SNE in a multi-player Markov game when the reward functions and
Markov transition kernel are unknown. In particular, we focus on the setting with a single leader
and the state transitions only depend on the leader’s actions. That is, the followers’ actions only
affect the rewards received by the leader and followers. For such a game, we are interested in the
following question:
Can we develop reinforcement learning methods that provably find Stackelberg-Nash equilibria in
leader-controlled general-sum games with sample efficiency?
To this end, we consider both online and offline RL settings, where in the former, we learn the
SNE in a trial-an-error fashion by interacting with the environment and generating data, and in the
latter, we learn the SNE from a given dataset that is collected a priori. For the online setting, as
the transition model is unknown, to achieve sample efficiency, the equilibrium-finding algorithm
also needs to take the exploration-exploitation tradeoff into consideration. Although the similar
challenge has been studied in zero-sum Markov game, it seems unclear how to incorporate popular
exploration mechanisms such as optimism in the face of uncertainty (Sutton & Barto, 2018) into
SNE finding. Meanwhile, under the offline setting, as the RL agent has no control of data collection,
it is ideal to design an RL algorithm with theoretical guarantees for an arbitrary dataset that might
not be sufficiently explorative.
Our contributions Our contributions are three-fold. First, for the episodic leader-controlled
general-sum game, under the online and offline settings respectively, we propose optimistic and
pessimistic variants of the least-squares value iteration (LSVI) algorithm. In particular, in a version
of LSVI, we estimate the optimal action-value function of the leader via least-squares regression and
construct an estimate of the SNE by solving the SNE of the multi-matrix game for each state, whose
payoff matrices are given by the leader’s estimated action-value function and the followers’ reward
functions. Moreover, we add a UCB exploration bonus to the least-squares solution to achieve op-
timism in the online setting. Whereas in the offline setting, pessimism is achieved by subtracting a
penalty function constructed using the offline data, which is equal to the negative bonus function.
Moreover, these algorithms are readily able to incorporate function approximators and we showcase
the version with linear function approximation. Second, under the online setting, we prove that our
optimistic LSVI algorithm achieves a sublinear O(H2 √d3K) regret, where K is the number of
episodes, H is the horizon, d is the dimension of the feature mapping, and O(∙) omits logarithmic
terms. Finally, under the offline setting, we establish an upper bound on the suboptimality of the
proposed algorithm for an arbitrary dataset with K trajectories. Our upper bound yields a sublinear
O(H2 d3 /K) rate as long as the dataset has sufficient coverage over the trajectory induced by the
desired SNE.
Related work In the sequel, we discuss the related works on learning Stackelberg games. We
defer more related works on RL for solving NE in Markov games and single-agent RL to §A.
Learning Stackelberg games As for solving Stackelberg-Nash equilibrium, most of the existing
results focus on the normal form game, which is equivalent to our Markov game with H = 1.
Letchford et al. (2009); Blum et al. (2014); Peng et al. (2019) study learning Stackelberg equilibrium
with a best response oracle. In addition, Fiez et al. (2019) study the local convergence of first-
order methods for finding Stackelberg equilibria in general-sum games with differentiable reward
functions, and Ghadimi & Wang (2018); Chen et al. (2021a); Hong et al. (2020) analyze the global
convergence of first-order methods for achieving global optimality of bilevel optimization. A more
2
Under review as a conference paper at ICLR 2022
related work is Bai et al. (2021), which studies the matrix Stackelberg game with bandit feedback.
This work also studies an RL extension where the leader has a finite action set and the follower is
faced with an MDP specified by the leader’s action. In comparison, we assume the leader knows
the reward functions and the main challenge lies in the unknown and leader-controlled transitions.
Thus, our setting is different from that in Bai et al. (2021). Furthermore, a more relevant work
is (Bucarey et al., 2019b), which establishes the Bellman equation and value iteration algorithm
for solving SNE in leader-controlled Markov games. In comparison, we establish modifications of
least-squares value iteration that are tailored to online and offline settings.
Notation See §B for details.
2	Preliminaries
In this section, we introduce the formulation of the general-sum simultaneous-move Markov games,
Stackelberg-Nash equilibrium, and the linear structure we use in this paper.
2.1	General-Sum Simultaneous-Move Markov Games
In this setting, two levels of hierarchy in decision making are considered: one leader l and N
followers {fi}i∈[N] . Specifically, we define an episodic version of general-sum simultaneous-
moves Markov game by the tuple (S,Al,Af = {Afi}i∈[N], H, rl, rf = {rfi}i∈[N],P), where
S is the state space, Al and Af are the sets of actions of the leader and the followers respec-
tively, H is the number of steps in each episode, rl = {rl,h : S × Al × Af → [-1, 1]}hH=1 and
rfi = {rfi ,h : S × Al × Af → [-1, 1]}hH=1 are reward functions of the leader and the followers
respectively, and P = {Ph : S × Al × Af × S → [0, 1]}hH=1 is a collection of transition kernels.
Here Al XAf = Al ×Afι X …XAfN. Throughout this paper, We also let ? be some element in
{l, fι,…，/n}. Moreover, for any (h, x, a) ∈ [H] ×S × Al and b = {bi ∈ AfJi∈[N], we use the
shorthands r?,h(x, a, b) = r?,h(x, a,bι,…，bχ) and Ph(∙ | x, a, b) = Ph(∙ | x,a,bι,…，bχ).
Policy and Value Function. A stochastic policy π = {πh : S → ∆(Al )}hH=1 of the leader is a
set of probability distributions over actions given the state. Meanwhile, a stochastic joint policy of
the followers is defined by ν = {νfi}i∈[N], where νfi = {νfi,h : S → ∆(Afi)}hH=1. We use the
notation πh(a | x) and νfi,h(bi | x) to denote the probability of taking action a ∈ Al or bi ∈ Afi for
state x at step h under policy π, νfi respectively. Throughout this paper, for any ν = {νfi}i∈[N] and
b = {bi}i∈[N],we usethe shorthand νh(b | x) = ν%,h(bι | x) X … X VfN,h(bN | x).
Given policies (π, ν = {νfi}i∈[N]), the action-value (Q) and state-value (V) functions for the leader
and followers are defined by
Q∏,ν(x,a,b) = E∏,ν,h,χ,a,b £r?,h(xt,at,bt) ,	KX(X) = Ea〜∏.(∙∣ x),b〜ν.(∙∣ x)Q∏,ν(x,a,b), (2.1)
t=h
where the expectation Eπ,ν,h,x,a,b is taken over state-action pairs induced by the policies (π, ν =
{νfi }i∈[N] ) and the transition probability, when initializing the process with the triplet (s, a, b =
{bi}i∈[N] ) at step h. For notational simplicity, when h, x, a, b are clear from the context, we omit
h, x, a, b from Eπ,ν,h,x,a,b. By the definition in (2.1), we have the Bellman equation
VnnhV = hQππ,,ν,πh	X	VhiAl×Af,	Qππ,,ν	=	r?,h + phV∏h+ι,	∀? ∈ {l,f1,…，fN},	(2.2)
where ∏h X Vh represents ∏h X νf ,h x∙∙∙x VfN ,h. Here Ph is the operator which is defined by
(Phf )(x, a, b) = E[f (x0) | x0 〜Ph(x01 x, a, b)]	(2.3)
for any function f : S → R and (x, a, b) ∈ S X Al X Af .
2.2	Stackelberg-Nash Equilibrium
Given a leader policy ∏, a Nash equilibrium (Nash, 2016) of the followers is a joint policy ν* =
{νf. }i∈[N], such that for any X ∈ S and (i, h) ∈ [N] X [H]
VK (x) ≥ Vlhfi,νf-i (x),	∀Vfi.	(2.4)
3
Under review as a conference paper at ICLR 2022
Here -i represents all indices in [N] except i. For each leader policy π, we denote the set of best-
response policies of the followers by BR(π), which is defined by
BR(π) = {ν = {νfi}i∈[N] | ν is the NE of the followers given the leader policy π}.	(2.5)
Given the best-response set BR(∏), We denote V*(∏) the worst-case responses, which break ties
against favor of the leader 1. Specifically, we define ν*(∏) by
0
v*(π) =	{ν ∈ BR(π)	|	Vι,h	(x)	≤	Vι,h	(x),∀x	∈	S,h ∈ [H], ν0 ∈ BR(π)}.	(2.6)
The Stackelberg-Nash equilibrium for the leader is the “best response to the best response”, that is,
SNEl = {π | Vr∏f(π)(x) ≥ 匕"ν*(π0)(x), ∀x ∈S,h ∈ [H ],π0}	(2.7)
A Stackelberg-Nash equilibrium of the general-sum game is a policy pair (∏*, V* = {vf.}i∈[N])
such that ν* ∈ ν*(π*) and π* ∈ SNEi.
Our goal is to find the Stackelberg equilibrium: the leader’s optimal strategy, assuming the fol-
lowers play their best response (Nash equilibrium) to the leader. We study this challenging bilevel
optimization problem in both the online setting (Section 3) and the offline setting (Section 4).
2.3 Leader-Controller Linear Markov Games
Inspired by the linear MDP studied in Jin et al. (2020b) for the single-agent RL, we study the
linear Markov games (Xie et al., 2020), where the transition dynamics are linear in a feature map.
Specifically, there exists a feature map φ0 : S × Al × Af → Rd such that
Ph(∙ | x, a,b) = hΦ0(x,a, b),μh(∙)i
for any (x, a,b) ∈ S × Ai XAf and h ∈ [H]. Here μh = (μh1),μh2),…，μhd)) are d unknown
signed measures over S. Moreover, throughout this paper, we focus on the leader-controller game
(Filar & Vrieze, 2012; Bucarey et al., 2019a), where the future state only depends on the current
state and the leader’s action, that is,
Ph(∙ | x,a,b) = Ph(∙ | x,a)
for any (x, a, b) ∈ S × Ai × Af and h ∈ [H]. Hence, itis naturally to define leader-controller linear
Markov games as follows.
Assumption 2.1. Markov game (S, Ai, Af = {Afi }i∈[N] , H, ri, rf = {rfi }i∈[N] , P) is a leader-
controller linear Markov game if there exists a feature map φ : S × Ai → Rd such that
Ph(∙ | x, a,b) = hΦ(x, a),μh(∙)i
for any (x, a,b) ∈ S × Ai × Af and h ∈ [H]. Here μh = (μh1),μh2), ∙∙∙ , μ(hd)) are d unknown
signed measures over S. Without loss of generality, we assume that kμh(S) k ≤ √d for all h ∈ [H].
The linear Markov game above is an extension of linear MDP studied in Jin et al. (2020b) for the
single-agent RL. Specifically, when the followers play fixed and known policies, the linear Markov
games reduce to the linear MDP.
3 Main Results for the Online Setting
In this section, we study the online setting, where a central controller controls one leader l and
N followers {fi}i∈[N]. Our goal is to learn a Stackelberg-Nash equilibrium. In what follows, we
formally describe the setup and learning objectives, and then present our algorithm and provide
theoretic guarantees.
1This is also known as pessimistic tie breaking (Conitzer & Sandholm, 2006). We also remark that our
subsequent analysis still holds for the optimistic setting (Breton et al., 1988; Bucarey et al., 2019a).
4
Under review as a conference paper at ICLR 2022
3.1	Setup and Learning Objective
We consider the setting where the reward functions rl and rf = {rfi}i∈[N] are revealed to the learner
before the game. This is reasonable since in practice the reward functions are usually artificially
designed. Moreover, we focus on the episodic setting. Specifically, a Markov game is played for K
episodes, each of which consists of H timesteps. At the beginning of the k-th episode, the leader and
followers determine their policies (πk, νk = {νfk }i∈[N]), and a fixed initial state x1k = x1 is chosen.
Here we assume the fixed initial state just for ease of presentation, and our subsequent results can
be generalized to the setting where x1k is picked from a fixed distribution. Then the game proceeds
as follows. At each step h ∈ [H], the leader and the followers observe state xkh ∈ S and pick their
own actions ah 〜∏k(∙ | Xh) and bh = {bk,h 〜 νfi,h(∙ | xhh)}i∈[N]. Subsequently, the environment
transitions to the next state xh+i 〜Ph(∙∣ xh ah，bh). Each episode terminates after H timesteps.
Learning Objective. By the definition in (2.5), given a leader’s policy, the best response for the
followers is the Nash equilibrium of followers’ game induced by this leader’s policy. Recall the
definition of Nash equilibrium in (2.4), for any policies (π, ν = {νfi }i∈[N] ), it is natural to define
the following objective to measure the suboptimality of νfi :
SUbOPtfi (x) = Wn) (x) - Vfi,Vfi ,ν-i(π)(x).
Meanwhile, we evaluate the performance of the leader’s policy π by the following suboptimality
gap:
_*	*	一 ..*/一 ∖
SubOptl (x) = Vl,1 , (x) - Vl,1,	(x).
Putting these two suboptimality gaps together, we formally define the regret as follows.
Definition 3.1 (Regret). Let (πk, νk = {νfk }i∈[N] ) denote the policies executed by the algorithm
in the k-th episode. After a total of K episodes, the regret is defined as
K	NK	kk* k
Regret(K) = X Vlπ,1*,ν*(x1k) - Vlπ,1k,ν*(πk)(x1k) + X X Vfπi,k1,ν*(πk)(x1k) - Vfπi,1,νfi,νf-i(π )(x1k).
k=1
^^^^{^^^^≡
Regretl (K )
i=1 k=1
X----------------------------{----
Regretf (K )
(3.1)
The goal is to design algorithms with regret that is sublinear in K, and polynomial in d, H. Here K
is the number of episodes, d is the dimension of the feature map φ, and H is the episode horizon.
3.2	Algorithm
We now present our algorithm, Optimistic Value Iteration to Find Stackelberg-Nash Equilibrium
(OVI-SNE), which is given in Algorithm 1.
At a high level, in each episode, our algorithm first construct the policies for all players through
backward induction with respect to the timestep h (line 4-11), and then execute the policies to play
the game (line 12-16).
In detail, at h-th step of k-th episode, OVI-SNE estimates leader’s Q-function based on the (k - 1)
historical trajectories. Inspired by previous optimistic least square value iteration (LSVI) algorithms
(Jin et al., 2020b), for any h ∈ [H], we estimate the linear coefficients by solving the following ridge
regression problem:
k-1
wh — argmin X[Vk+1(Xh+1)- φ(xh,ah)>w]2 + llwk2,
w∈Rd τ=1
where 心(.)=hQh+ι(∙, ∙, ∙),∏k+ι(∙ ∣∙) × νhk+ι(∙ ∣∙)>4×Af.
By solving the ridge regression problem in (3.2), we have
h-1
wh = (Λh)-1 (X φ(xh,ah) ∙ Vk+1(xh+1)),
h-1
where Λhh = X φ(xτh, aτh)φ(xτh, aτh)> + I.
τ=1
(3.2)
(3.3)
5
Under review as a conference paper at ICLR 2022
To encourage exploration, we additionally adds a bonus function to estimate the leader’s Q-function:
Qh(∙, ∙, ∙) J rl,h(∙, ∙, ∙) + π-H-h—/wk+rh(∙,∙)},
where W) = β ∙ √≠M>Λ)-Φm∙
(3.4)
Here Γkh : S × Al → R is a bonus function and β > 0 is a parameter which will be specified later.
This form of bonus function is common in the literature of linear bandits (Lattimore & Szepesvari,
2020) and linear MDPs (Jin et al., 2020b).
Then, we construct policies for the leader and followers by the subroutine -SNE (Algorithm 2).
Specifically, let Qkh be the class of functions Q : S × Al × Af → R that takes form
Q(∙, ∙, ∙) = rl,h(∙, ∙, ∙) + πH-h{φ(∙, ∙)>w + β ∙ (φ(∙, [丁--1*, ∙)) 1/2 },	(3.5)
where the parameters (w, Λ) ∈ Rd X Rd×d satisfy ∣∣wk ≤ H√dk and λmin(Λ) ≥ 1. Moreover, let
Qh,e be a fixed e-covering of Qh with respect to the '∞ norm. By Lemma C.10, We have Qh ∈ Qh,
which allows us to pick a Qe ∈ Qkh, such that ∣Qe - Qkh ∣∞ ≤ and calculate policies by
(∏h(∙ I x), {νki,h(∙ I x)}i∈[N]) J SNE(Q(x, ∙, ∙), {rfi,h(x, ∙, ∙)}i∈[N] Nx	(3.6)
When there is only one follower, such a problem can be transformed to a linear programming (LP)
problem (Conitzer & Sandholm, 2006; Von Stengel & Zamir, 2010), and thus can be solved ef-
ficiently. For the multi-follower case, however, solving such a matrix game in general is hard
(Conitzer & Sandholm, 2006; Basilico et al., 2017a;b; Coniglio et al., 2020). Given this computa-
tional hardness, we focus on the sample complexity and explicitly assume access to the following
computational oracle:
Assumption 3.2. We assume access to an oracle that implements Line 3 of Algorithm 2 when there
are multiple followers (i.e., N ≥ 2).
Now we explain the motivation for using the subroutine e-SNE to construct policies instead of solv-
ing the matrix games with payoff matrices (Qh (x, ∙, ∙), {3,h(x, ∙, ∙)}i∈[N]) directly. By the defini-
tion of Qkh in (3.4), we know Qkh relies on the previous data via the estimated value function Vhk+1
and feature maps {φ(xτh, aτh, bτh)}τk-=11. Similar to the analysis for linear MDPs (Jin et al., 2020b),
we need to use a covering argument to establish uniform concentration bounds for all value Vhk+1 .
Jin et al. (2020b) directly constructs an e-net for the value functions and establishes a polynomial
log-covering number for this e-net. This analysis, however, relies on that the policies executed by
the players are greedy (deterministic), which is not valid for our setting. To overcome this technical
issue, we construct an e-net for Q-functions and solve an approximate matrix game. Fortunately, by
choosing a small enough e, we can handle the errors caused by this approximation. See §C for more
details. Moreover, as shown in Xie et al. (2020), this subroutine can be implemented efficiently
without explicitly computing the exponentially large e-net.
Finally, the leader and the followers play the game according to the obtained policies.
3.3	Theoretical Results
Our main theoretical result is the following bound on the regret incurred by Algorithm 1. Recall that
the regret is defined in Definition 3.1 and T = KH is the total number of timesteps.
Theorem 3.3. Under Assumptions 2.1 and 3.2, there exists an absolute constant C > 0 such that,
for any fixed P ∈ (0,1), by setting β = C ∙ dH√ι with ι = log(2dT∕p) in Line 7 of Algorithm 1
and e = KH in Algorithm 2, then with probability at least 1 一 p, the regret incurred by OVI-SNE
satisfies that
Regret(K) ≤ O(√d3H3Tι2).
Proof. See §C for a detailed proof.
□
Learning Stackelberg Equilibria. When there is only one follower, Stackelberg-Nash equilib-
rium reduces to the Stackelberg equilibrium (Simaan & Cruz, 1973; Conitzer & Sandholm, 2006;
Bai et al., 2021). Thus, we partly answer the open problem in Bai et al. (2021) on how to learn
Stackelberg equilibria in (leader-controller) Markov games.
6
Under review as a conference paper at ICLR 2022
Algorithm 1 Optimistic Value Iteration to Find Stackelberg-Nash Equilibria
1:	Initialize V,h+i(∙) = Vf,H+ι(∙) = 0.
2:	for k = 1,2,…，K do
3:	Receive initial state x1k .
4:	for step h = H, H - 1,…，1 do
5： Ah — PT-I φ(xh, ah)φ(xh, ah)>+I.
6：	Wk 一 (Ah)-1 Pk-1 φ(xh, ah) ∙ Vk+ι(xh+ι).
7： rκ,∙) 一 β ∙ (φ(∙, ∙)>(Λh)-ιφ(∙, ∙))1∕2.
8：	Qh(∙, ∙, ∙) J rl,h(∙, ∙, ∙) + πH-h{φ(∙, ∙)>wk+rh(∙, ∙)}.
9：	(πh(T x), {νki,h(∙l x)}i∈[N]) - e-SNE(Qh(X, ∙, ∙),{rfi,h(χ, ∙, ∙)}i∈[N]), ∀x. (Alg. 2)
10:	Vh (x) <- Ea~∏k(∙ | x),b1 〜Vfl h (∙ | x),∙∙∙ ,bN〜VfkN h(∙ | X)Qh(X，a，b1, ' ' ' , bN), ∀x∙
11： end for
12:	for h = 1, 2, ∙, H do
13：	SamPIe ah 〜πk(∙ | Xxhh bk,h 〜fl-. | xh),…，bN,h 〜VfN ,h (∙ | xh).
14:	Leader takes action akh; Followers take actions bkh = {bik,h}i∈[N] .
15： Observe next state Xkh+1 .
16： end for
17： end for
Algorithm 2 e-SNE
1： Input: Qkh , X, and parameter e.
2： Select Qe from Qkh, satisfying kQe - Qkhk∞ ≤ e.
3： For the input state x, let (∏f (∙ | x), {νf,hr(∙ | χ)}i∈[N]) be the Stackelberg-Nash equilibrium for
the matrix game With payoff matrices (Q(x, ∙, ∙), {rfi,h(x, ∙, ∙)}i∈[N ]).
4： OUtPUt： (πf (∙ | x), {νfi,h(∙l x)}i∈[N] ).
OPtimality of the BoUnd. Assuming that the action of the folloWer Won’t affect the transition
kernel and reWard function, the linear Markov games reduces to the linear MDP (Jin et al., 2020b).
MeanWhile, the loWer bound established in Azar et al. (2017); Jin et al. (2018) for tabular MDPs
and the lower bound established in Lattimore & Szepesvari (2020) for linear bandits directly imply
a lower bound Ω(dH√T) for the linear MDPs, which further yields a lower bound Ω(dH√T) for
our setting. Ignoring the logarithmic factors, there is only a gap of √dH between this lower bound
and our upper bound. We also point out that, by using the “Bernstein-type” bonus (Azar et al., 2017;
Jin et al., 2018; Zhou et al., 2020), we can improve our upper bound by a factor of √H. Here we
don’t apply this technique for the clarity of the analysis.
MissPecification. For ease of presentation, we assume the Markov games are leader-controller in
Assumption 2.1. When the transitions do not ideally satisfy the leader-controller assumption, we can
potentially consider cases that transitions satisfy, for instance, ∣Ph(∙ | x, a, b) - Ph(∙ | x, a)k∞ ≤ %
for any (h, x, a, b) ∈ [H] × S × Al × Af, Here % is the misspecification error. We can still follow
the above method to tackle the misspecified cases. However, because of the misspecification error
cumulated during T steps, an extra term O(%T) will appear in the final result. In particular, When
% is small, that is the Markov games have approximately leader-controller transitions, the extra
term O(%T) should be small, which further indicates that we can find SNEs efficiently in some
misspecified general-sum Markov games.
Unknown Reward Setting. At a high level, we first conduct a reward-free exploration algorithm
(Algorithm 4 in §D), a variant of Reward-Free RL-Explore algorithm in Jin et al. (2020a), to obtain
estimated reward functions {bl, bʌ,…bf. }. As asserted before, we can use Algorithm 1, to find the
SNE with respect to the known estimated reward functions {bl, rfγ, ∙∙∙ bf.}. Hence, we can obtain
the approximate SNE if the value functions of estimated value functions are good approximation of
the true value functions. See §E for more details.
7
Under review as a conference paper at ICLR 2022
4 Main Results for the Offline Setting
In this section, we study the offline setting, where the central controller aims to find a Stackelberg-
Nash equilibrium by an offline dataset. Below we describe the setup and learning objective, followed
by our algorithm and theoretical results.
4.1 Setup and Learning Objective
We study the offline setting, where the learner has access to the reward functions (rl, rf = {rfi}iN=1)
and a dataset D = {(xτh, aτh, bτh = {biτ,h}iN=1)}τK,h,H=1, which is collected a priori by some experi-
menter. Then we make a minimal assumption for the offline dataset.
Assumption 4.1 (Compliance of Dataset). We assume that the dataset D is compliant with the
underlying Markov game (S, Al , Af , H, rl , rf , P), that is, for any x0 ∈ S at step h ∈ [H] of each
trajectory τ ∈ [K],
PD (xτh+1 = x | {xh, ah, bh, xh+1 }jτ=-1 ∪ {xτh, aτh, bτh}) = P (xh+1 = x | xh = xτh, ah = aτh).
Here the probability on the left-hand side is with respect to the joint distribution over dataset D and
the probability on the right-hand side is with respect to the underlying Markov game.
Assumption 4.1 is adopted from Jin et al. (2020c), which indicates the Markov property of the dataset
D and that xτh+1 is generated by the underlying Markov game conditioned on (xτh, aτh, bτh). As a
special case, Assumption 4.1 holds when the experimenter follows fixed behavior policies. More
generally, Assumption 4.1 allows the experimenter to choose actions aτh and bτh arbitrarily, even in
an adaptive or adversarial manner. In particular, we can assume that aτh and bτh are interdependent
across each trajectory τ ∈ [K]. For instance, the experimenter can sequentially improve the behavior
policy using any online algorithm for Markov games.
Learning Objective. Similar to the online setting, we define the following performance metric
N	*	，、
SUbOPt(π,ν,x) = Vl∏ι*,ν* (X)- Vl∏f(π)(x) + X[Vfi,V*(π)(x) - VUfi⑺〕，
X------------{------------} i=1
SUbOPtl	、	V	/
SUbOPtf
(4.1)
which evaluates the suboptimality of policies (∏, V = {νfJN=ι) given the initial state X ∈ S.
4.2 Algorithm and Theoretical Results
As is known to us, the key challenge of online setting is the the tradeoff between exploration and
exploration. In the online setting. by following the “optimism in the face of uncertainty” principle
(Sutton & Barto, 2018), we use bonus functions to incentivize exploration and thus achieve sample-
efficient. This intrinsic challenge of online setting disappears in the offline setting because we do
not need exploration any more. But another challenge arises: we only have access to the limited
data. To tackle this challenge, we need add some penalty functions to achieve robustness against the
uncertainty due to the finite data. This is also known as pessimism (Yu et al., 2020; Jin et al., 2020c;
Liu et al., 2020b; Buckman et al., 2020; Kidambi et al., 2020; Kumar et al., 2020; Rashidinejad
et al., 2021). Here we simply flip the sign of bonus functions defined in (3.4) to serve as penalty
functions. See Algorithm 3 for details.
Suppose that (b, b) are the output policies of Algorithm 3. Then we evaluate the performance of
(b, b) by establishing an upper bound for the optimality gap defined in (4.1).
Theorem 4.2. Under Assumptions 2.1, 3.2, and 4.1, there exists an absolute constant C > 0 such
that, for any fixed P ∈ (0，1),by setting β0 = C ∙ dH,log(2dHK∕p) in Line 6 of Algorithm 3 and
e = KdH in Algorithm 2, then with probability at least 1 一 p, we have
H
SubOpt(b, b, x) ≤ 3β0 X E∏*,χ [(φ(sh, a%)>(Ah)-1φ(sh, ah)) 1/2]，	(4.2)
h=1
where E∏*,χ is taken with respect to the trajectory incurred by ∏* in the underlying leader-controller
Markov game when initializing the progress at X. Here Λh is defined in Line 4 of Algorithm 3.
8
Under review as a conference paper at ICLR 2022
Proof. See §F for a detailed proof.
□
Algorithm 3 Pessimistic Value Iteration to Find Stackelberg-Nash Equilibria
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
Input: D = {xτh,aτh,bτh = {biτ,h}i∈[N]}τK,h,H=1 and reward functions {rl,rf = {rfi}i∈[N]}.
.... ^ ，、 .
Initialize VbH+ι(∙) = 0.
for step h = H, H 一 1,…，1 do
Ah — PK=ι φ(Xh, ah)φ(xh, ah)> + I.
Wh - (Ah)-1 PK=ι φ(xh, ah) ∙ Vh+1(xh+1).
Γh(∙, ∙) 一 β0 ∙ (φ(∙, ∙)τ(Λh)-1φ(∙, ∙))1/2.
Qbh(∙, ∙, ∙) - rl,h (., ∙, ∙) + πH-h{φ(∙, ∙)>wh - rh(∙,，)}.
(πh(∙ | x), {νfi,h(∙ | x)}i∈[N]) - e-SNE(Qh(χ, ∙, ∙),{rfi,h(χ, ∙, ∙)}i∈[N]), ∀x. (Alg. 2)
Vh(X) 4- Ea〜∏h(∙∣ x),bι〜bf],h(∙∣ x),…，bN〜bfN,h(∙∣ X)Qh(x, a, b1, ∙ ∙ ∙ , bN), ∀x.
end for
Output: (πb= {πbh}hH=1,νb= {νbfi = {νfi,h}hH=1}iN=1).
Minimal Assumption Requirement: Theorem 4.2 only relies on the compliance of the dataset
with linear Markov games. Compared with existing literature on offline RL (Bertsekas & Tsitsiklis,
1996; Antos et al., 2007; 2008; Munos & Szepesvari, 2008; Farahmand et al., 2010; 2016; Scherrer
et al., 2015; Liu et al., 2018; Chen & Jiang, 2019; Fan et al., 2020; Xie & Jiang, 2020), we impose
no restrictions on the coverage of the dataset. Meanwhile, we need no assumption on the affinity
between (πb, νb) and the behavior policies that induce the dataset, which is often employed as a
regularizer (Fujimoto et al., 2019; Laroche et al., 2019; Jaques et al., 2019; Wu et al., 2019; Kumar
et al., 2019; Wang et al., 2020; Siegel et al., 2020; Nair et al., 2020; Liu et al., 2020b).
Dataset with Sufficient Coverage: In what follows, we specialize Theorem 4.2 to the setting where
we assume the dataset with good “coverage”. Note that Ah is determined by the offline dataset D
and acts as a fixed matrix in the expectation, that is, the expectation in (4.2) is only taken with the
trajectory induced by ∏*. As proofed in the following theorem, when the trajectory induced by ∏*
is “covered” by the dataset D sufficiently well, we can establish that the suboptimality incurred by
Algorithm 3 diminishes at rate of Oe(1/√K).
Corollary 4.3. Suppose it holds with probability at least 1 一 p/2 that
Ah 占 I + C ∙ K ∙ E∏*,χ[φ(sh, ah)φ(sh,ah)τ]
for all (x, h) ∈ S × [H]. Here c > 0 is an absolute constant and E∏*,χ is taken with respect to
the trajectory incurred by ∏* in the underlying leader-controller Markov game when initializing the
progress at x. Under Assumptions 2.1, 3.2 and 4.1, there exists an absolute constant C > 0 such
that, for any fixed P ∈ (0,1),by setting β0 = C ∙ dH,log(4dHK∕p) in Line 6 of Algorithm 3 and
e = κdH in Algorithm 2, then it holds with probability at least 1 一 P that
SUbOPt(b, b, x) ≤ C ∙ d3/2H2 Plog(4dHK∕p)∕K
for all x ∈ S. Here C is another absolute constant that only depends on C and C.
Proof. See §G for a detailed proof.	□
Note that, unlike the previous literature (Antos et al., 2007; Munos & Szepesvari, 2008; Farahmand
et al., 2010; 2016; Scherrer et al., 2015; Liu et al., 2018; Chen & Jiang, 2019; Fan et al., 2020; Xie
& Jiang, 2020) which relies on the “uniform coverage” assumption, Corollary 4.3 only assumes that
the dataset has a good coverage of the trajectory incurred by the policy ∏*.
Optimality of the Bound: Assuming the dummy followers, that is, the actions taken by
the followers won’t affect the reward functions and transition kernels, the Markov games re-
duces to the linear MDP (Jin et al., 2020b). Together with the information-theoretic lower
bound Ω(PH=ι E∏*,χ[(φ(sh, ah)T(Ah)-Iφ(sh, ah))1/2]) established in Jin et al. (2020c) for lin-
ear MDPs, we immediately obtain the same lower bound for our setting. In particular, our upper
bound established in Theorem 4.2 matches this lower bound up to β0 and absolute constants and
thus implies that our algorithm is nearly minimax optimal.
9
Under review as a conference paper at ICLR 2022
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In NIPS, volume 11,pp. 2312-2320, 2011.
Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on Learning Theory, pp. 67-83. PMLR, 2020.
Forest Agostinelli, Stephen McAleer, Alexander Shmakov, and Pierre Baldi. Solving the rubik’s
cube with deep reinforcement learning and search. Nature Machine Intelligence, 1(8):356-363,
2019.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019.
AndraS Antos, Remi Munos, and Csaba SzePeSVarL Fitted q-iteration in continuous action-space
mdps. 2007.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71(1):89-129, 2008.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463-474. PMLR, 2020.
Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax PAC bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325-349, 2013.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272. PMLR, 2017.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In Inter-
national Conference on Machine Learning, pp. 551-560. PMLR, 2020.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. arXiv
preprint arXiv:2006.12007, 2020.
Yu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efficient learning of stackelberg equilibria
in general-sum games. arXiv preprint arXiv:2102.11494, 2021.
Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Commitment without
regrets: Online learning in stackelberg security games. In Proceedings of the sixteenth ACM
conference on economics and computation, pp. 61-78, 2015.
Tamer BaSar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.
Nicola Basilico, Stefano Coniglio, and Nicola Gatti. Methods for finding leader-follower equilibria
with multiple followers. arXiv preprint arXiv:1707.02174, 2017a.
Nicola Basilico, Stefano Coniglio, Nicola Gatti, and Alberto Marchesi. Bilevel programming ap-
proaches to the computation of optimistic and pessimistic single-leader-multi-follower equilibria.
In SEA, volume 75, pp. 1-14. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik GmbH, Dagstuhl
Publishing, 2017b.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming. Athena Scientific, 1996.
Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Learning optimal commitment to overcome
insecurity. 2014.
Michele Breton, Abderrahmane Alj, and Alain Haurie. Sequential stackelberg equilibria in two-
person games. Journal of Optimization Theory and Applications, 59(1):71-97, 1988.
10
Under review as a conference paper at ICLR 2022
Victor Bucarey, EUgenio Della Vecchia, Alain Jean-Marie, and Fernando Ordofiez. Stationary
Strong Stackelberg Equilibrium in Discounted Stochastic Games. PhD thesis, INRIA, 2019a.
Victor Bucarey, Alain Jean-Marie, Eugenio Delia Vecchia, and Fernando Ordonez. On the value
iteration method for dynamic strong Stackelberg equilibria. In ROADEF 2019-20eme Congres
annuel de la societe Francaise de Recherche Operationnelle et d'Aide a la Decision, 2019b.
Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020.
Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent rein-
forcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications
andReviews), 38(2):156-172, 2008.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy opti-
mization. In International Conference on Machine Learning, pp. 1283-1294. PMLR, 2020.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In International Conference on Machine Learning, pp. 1042-1051. PMLR, 2019.
Tianyi Chen, Yuejiao Sun, and Wotao Yin. A single-timescale stochastic bilevel optimization
method. arXiv preprint arXiv:2102.04671, 2021a.
Zhuoqun Chen, Yangyang Liu, Bo Zhou, and Meixia Tao. Caching incentive design in wireless d2d
networks: A stackelberg game approach. In 2016 IEEE International Conference on Communi-
cations (ICC), pp. 1-6. IEEE, 2016.
Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021b.
Stefano Coniglio, Nicola Gatti, and Alberto Marchesi. Computing a pessimistic stackelberg equi-
librium with multiple followers: The mixed-pure case. Algorithmica, 82(5):1189-1238, 2020.
Vincent Conitzer and Tuomas Sandholm. Complexity of mechanism design. arXiv preprint
cs/0205075, 2002.
Vincent Conitzer and Tuomas Sandholm. Computing the optimal strategy to commit to. In Proceed-
ings of the 7th ACM conference on Electronic commerce, pp. 82-90, 2006.
Qiwen Cui and Lin F Yang. Minimax sample complexity for turn-based stochastic game. arXiv
preprint arXiv:2011.14267, 2020.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. 2008.
Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. arXiv preprint arXiv:2101.04233, 2021.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International conference on machine learning,
pp. 1329-1338. PMLR, 2016.
Yonathan Efroni, Lior Shani, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization
with bandit feedback. arXiv preprint arXiv:2002.08243, 2020.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Learning for Dynamics and Control, pp. 486-489. PMLR, 2020.
Amir Massoud Farahmand, Remi Munos, and Csaba Szepesvari. Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems, 2010.
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesvari, and Shie Mannor. Reg-
ularized policy iteration with nonparametric function spaces. The Journal of Machine Learning
Research, 17(1):4809-4874, 2016.
11
Under review as a conference paper at ICLR 2022
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning dynamics in stack-
elberg games. arXiv preprint arXiv:1906.01217, 2019.
Jerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business
Media, 2012.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Dinesh Garg and Yadati Narahari. Design of incentive compatible mechanisms for stackelberg
problems. In International Workshop on Internet and Network Economics, pp. 718-727. Springer,
2005.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018.
Amy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In ICML, volume 3, pp.
242-249, 2003.
Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly poly-
nomial for 2-player turn-based stochastic games with a constant discount factor. Journal of the
ACM (JACM), 60(1):1-16, 2013.
Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. Is multiagent deep reinforcement learn-
ing the answer or the question? a brief survey. Learning, 21:22, 2018.
Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. A survey and critique of multiagent deep
reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6):750-797, 2019.
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039-1069, 2003.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
Zeyu Jia, Lin F Yang, and Mengdi Wang. Feature-based q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423, 2019.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? arXiv preprint arXiv:1807.03765, 2018.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration
for reinforcement learning. In International Conference on Machine Learning, pp. 4870-4879.
PMLR, 2020a.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020b.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline RL? arXiv
preprint arXiv:2012.15085, 2020c.
Xin Kang and Yongdong Wu. Incentive mechanism design for heterogeneous peer-to-peer networks:
A stackelberg game approach. IEEE Transactions on Mobile Computing, 14(5):1018-1030, 2014.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.
Dmytro Korzhyk, Zhengyu Yin, Christopher Kiekintveld, Vincent Conitzer, and Milind Tambe.
Stackelberg vs. Nash in security games: An extended investigation of interchangeability, equiva-
lence, and uniqueness. Journal of Artificial Intelligence Research, 41:297-327, 2011.
12
Under review as a conference paper at ICLR 2022
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Michail Lagoudakis and Ron Parr. Value function approximation in zero-sum markov games. arXiv
preprint arXiv:1301.0580, 2012.
Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with
baseline bootstrapping. In International Conference on Machine Learning, pp. 3652-3661.
PMLR, 2019.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the opti-
mal strategy to commit to. In International Symposium on Algorithmic Game Theory, pp. 250-
262. Springer, 2009.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.
Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322-
328, 2001.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. arXiv preprint arXiv:1810.12429, 2018.
Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. arXiv preprint arXiv:2010.01604, 2020a.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-
ment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020b.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal OfMachine
Learning Research, 9(5), 2008.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
John F Nash. Non-Cooperative Games. Princeton University Press, 2016.
Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep rein-
forcement learning. arXiv preprint arXiv:1908.03963, 2019.
Binghui Peng, Weiran Shen, Pingzhong Tang, and Song Zuo. Learning optimal strategies to commit
to. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 2149-2156,
2019.
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum markov games. In International Conference on Machine Learning, pp.
1321-1329. PMLR, 2015.
Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model
based reinforcement learning. In International Conference on Machine Learning, pp. 7953-7963.
PMLR, 2020.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.
13
Under review as a conference paper at ICLR 2022
Lillian J Ratliff and Tanner Fiez. Adaptive incentive design. IEEE Transactions on Automatic
Control, 2020.
Lillian J Ratliff, Ming Jin, Ioannis C Konstantakopoulos, Costas Spanos, and S Shankar Sastry.
Social game for building energy efficiency: Incentive design. In 2014 52nd Annual Allerton
Conference on Communication, Control, and Computing (Allerton), pp. 1011-1018. IEEE, 2014.
Tim Roughgarden. Stackelberg scheduling strategies. SIAM journal on computing, 33(2):332-350,
2004.
Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu Geist.
Approximate modified policy iteration and its application to the game of tetris. J. Mach. Learn.
Res., 16:1629-1676, 2015.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095-1100, 1953.
Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player
games with near-optimal time and sample complexity. In International Conference on Artificial
Intelligence and Statistics, pp. 2992-3002. PMLR, 2020.
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing
what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint
arXiv:2002.08396, 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Marwaan Simaan and Jose B Cruz. On the stackelberg strategy in nonzero-sum games. Journal of
Optimization Theory and Applications, 11(5):533-555, 1973.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057-
1063. Citeseer, 1999.
Milind Tambe. Security and game theory: algorithms, deployed systems, lessons learned. Cam-
bridge university press, 2011.
Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Provably efficient online agnostic learning
in markov games. arXiv preprint arXiv:2010.15020, 2020.
Bernhard Von Stengel and Shmuel Zamir. Leadership games with convex strategy sets. Games and
Economic Behavior, 69(2):446-457, 2010.
ZiyU Wang, Alexander Novikov, Konrad Zoina, Jost Tobias SPnngenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020.
ChristoPher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games.
arXiv preprint arXiv:1712.00579, 2017.
14
Under review as a conference paper at ICLR 2022
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory,pp. 3674-3682. PMLR, 2020.
Tengyang Xie and Nan Jiang. Q?-approximation schemes for batch reinforcement learning: A
theoretical comparison. arXiv preprint arXiv:2003.03924, 2020.
Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995-7004. PMLR, 2019.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746-10756. PMLR, 2020.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. Bridging exploration
and general function approximation in reinforcement learning: Provably efficient kernel and neu-
ral value iterations. arXiv preprint arXiv:2011.04622, 2020.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pp. 7304-7312. PMLR, 2019.
Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric.
Frequentist regret bounds for randomized least-squares value iteration. In International Confer-
ence on Artificial Intelligence and Statistics, pp. 1954-1964. PMLR, 2020a.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978-10989. PMLR, 2020b.
Kaiqing Zhang, ZhUoran Yang, and Tamer Bayar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
Kaiqing Zhang, Sham M Kakade, Tamer Basyar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020a.
Zihan Zhang, Xiangyang Ji, and Simon S Du. Is reinforcement learning more difficult than bandits?
a near-optimal algorithm escaping the curse of horizon. arXiv preprint arXiv:2009.13503, 2020b.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learningvia
reference-advantage decomposition. Advances in Neural Information Processing Systems, 33,
2020c.
Yulai Zhao, Yuandong Tian, Jason D Lee, and Simon S Du. Provably efficient policy gradient
methods for two-player zero-sum markov games. arXiv preprint arXiv:2102.08903, 2021.
Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C Parkes,
and Richard Socher. The AI economist: Improving equality and productivity with AI-driven tax
policies. arXiv preprint arXiv:2004.13332, 2020.
Ying-Ping Zheng, Tamer Basar, and Jose B Cruz. Stackelberg strategies and incentives in multiper-
son deterministic decision problems. IEEE transactions on Systems, Man, and Cybernetics, (1):
10-24, 1984.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture markov decision processes. arXiv preprint arXiv:2012.08507, 2020.
15