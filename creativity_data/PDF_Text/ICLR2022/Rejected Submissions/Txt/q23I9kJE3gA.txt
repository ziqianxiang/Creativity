Under review as a conference paper at ICLR 2022
Conditional set generation using seq2seq
MODELS
Anonymous authors
Paper under double-blind review
Ab stract
Conditional set generation learns a mapping from an input sequence of tokens
to a set. Several popular natural language processing (nlp) tasks, such as entity
typing and dialogue emotion tagging, are instances of set generation. Sequence-
to-sequence models are a popular choice to model set generation but this typical
approach of treating a set as a sequence does not fully leverage its key properties,
namely order-invariance and cardinality. We propose a novel data augmentation
approach that recovers informative orders for labels using their dependence infor-
mation. Further, we jointly model the set cardinality and output by listing the set
size as the first element and taking advantage of the autoregressive factorization
used by seq2seq models. Our experiments in simulated settings and on three
diverse NLP datasets show that our method improves over strong seq2seq base-
lines by about 9% on absolute F1 score. We will release all code and data upon
acceptance.
1	Introduction
Conditional set generation is the task of modeling the distribution of an output set given an input
sequence of tokens (Kosiorek et al., 2020). Several natural language processing (nlp) tasks are
instances of set generation, including open-entity typing (Choi et al., 2018; Dai et al., 2021) and
fine-grained emotion classification (Demszky et al., 2020). The recent successes of pretraining-
finetuning paradigm have encouraged a formulation of set generation as a sequence-to-sequence
generation task (Vinyals et al., 2016; Yang et al., 2018; Ju et al., 2020).
In this paper, we argue that modeling set generation as a vanilla seq2seq generation task is sub-
optimal as the seq2seq formulations do not explicitly account for two key properties ofa set output:
order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation modeling
treats a set as a sequence, and thus assumes an arbitrary order between the elements it outputs.
Similarly, the cardinality of sets is ignored, as the number of elements to be generated is typically
not explicitly modeled. Although prior work has highlighted the importance of modeling the order-
invariant nature of both set inputs (Zaheer et al., 2017) and outputs (Vinyals et al., 2016; Rezatofighi
et al., 2018), the question of effectively modeling set output using seq2seq models still remains an
open challenge.1
Our method addresses the challenges above by taking advantage of the auto-regressive factorization
used by SEQ2SEQ models and (i) imposing an informative order over the label space, and (ii) explic-
ity modeling cardinality. First, the label sets are converted to sequences using informative orders by
grouping labels and leveraging their dependency structure. A natural way to model this is to search
exhaustively for the best label orders. To efficiently search for such informative orders over a com-
binatorial space, our method imposes a partial order graph over the labels, where the nodes are the
labels and the edges denote the conditional dependence relations. We then generate the training data
with a fixed input and orders over the label set that are sampled by performing topological traversals
over the graph. Labels that are not constrained by dependency relations are augmented in different
positions in each sample, reinforcing the order-invariance. We then create an augmented training
dataset, where each input instance is paired with various valid label sequences sampled from the
dependency graph. Next, we jointly model a set with its cardinality by simply appending the size of
the set as the first element in the sequence.
1Our work focuses on settings where the input is a sequence, and the output is a set.
1
Under review as a conference paper at ICLR 2022
｛厂,口,5
｛4令门｝
xz∙→[3,□ ,Γ,×>]
Xf 3一,口,门
XjT [3,/ ,厂，I ]
x7→[3,Z15 I 5∏
χ左→[4,Ez,4∣-,∣]
x%→[4,令，/, I ,厂]
Figure 1: The figure illustrates a sample task where given an input x, the output is a set of
shapes (e.g., triangle, half-square, line). The partial order graph (middle) arranges the label space
such that specific labels (triangle) come before more general labels (line). Listing the specific labels
first gives the model more clues about the rest of the set, leading to more informative sequences.
The size of each set is also added as the first element for joint modeling of output with size.
Figure 1 illustrates the key intuitions behind our method using sample task where given an input x,
the output is a set of shapes and their constituents (Y). To see why certain orders might be more
meaningful, consider a case where the output is a triangle consisting of a half-square and a line.
After first generating triangle as a shape, the model can generate a half-square with certainty (a
triangle will always contain a half-square). In contrast, the reverse order (generating half-square
first) still leaves room for two possible shapes: square and triangle. The order [triangle, half-square]
is thus more informative than [half-square, triangle]. The cardinality of a set can also be helpful. In
our example, a triangle is composed of two shapes, and a star with three. A model that first predicts
the number of shapes to generate can be more precise in its output and avoid over-generation, a
major challenge with language generation models (Welleck et al., 2019; Fu et al., 2021).
Empirically, we establish the utility and soundness of our approach by showing gains on three real-
world NLP datasets (~10% in F-scores). This result is significant - We effectively show that simple
techniques such as augmenting cardinality and automated data augmentation approaches can sub-
stantially improve sequence to set generation tasks without any additional annotation overhead or
architecture changes. We also provide a theoretical grounding for our approach. Treating the order
as a latent variable, we show that tsample serves as a better proposal distribution when viewed via
a variational inference framework. Finally, we perform an in-depth analysis of the reasons behind
the sensitivity of the seq2seq framework on order by experimenting with a simulated experiment
that realistically mimics a conditional set generation setting.
Our contributions (i) we show an efficient way to model sequence-to-set prediction as an
seq2seq task by jointly modeling the cardinality and proposing a novel tsample data augmen-
tation approach to add informative sequences. (ii) we show theoretically and empirically that our
approach is better suited for set generation tasks than existing approaches.
2	Background and related work
Notation Our focus is the setting where we are given a corpus D of {(xt, Yt)}tm=1 where xt is
a sequence of tokens and Yt = {y1 , y2, . . . , yk} is a set. For example, in multi-label fine-grained
sentiment classification, xt is a paragraph, and Yt is a set of sentiments expressed by the paragraph.
We use yi to denote an output symbol, [yi , yj , yk] to denote an ordered sequence of symbols and
{yi,yj,yk} to denote a set.
2.1	Set generation using seq2seq model
Task Given a corpus {(xt, Yt)}tm=1, the task of conditional set generation is to efficiently estimate
p(Yt | xt).
2
Under review as a conference paper at ICLR 2022
In this work, we adopt SEQ2SEQ models for the task. SEQ2SEQ models factorize p(Yt | xt) in an
autoregressive (ar) fashion using the chain rule:
p(Yt | xt) =p(y1,y2, . . . ,yk | xt)
k
= p(y1 | xt)	p(yj | xi, y1 . . . yj-1)	(1)
j=2
where we have used the order Yt = [y1, y2, . . . , yk] to factorize the joint distribution using chain
rule. In theory, any of the k! orders can be used to factorize the same joint distribution. In practice,
however, the choice of order is important. For instance, Vinyals et al. (2016) show that output
order affects language modeling performance when using lstm based seq2seq models for set
generation.
Consider an example (xt, Yt = {y1, y2}) pair. By chain rule, we have the following equivalent fac-
torizations of this sequence: p(Yt | xt) = p(y1 | x)p(y2 | x, y1) = p(y2 | x)p(y1 | x, y2). How-
ever, order-invariance is only guaranteed with true conditional probabilities, whereas the conditional
probabilities used to factorize a sequence are estimated by a model from a corpus. Thus, dependen-
ing on the order, the sequence factorizes as eitherp(y1 | x)p(y2 | x, yj orp(y2 | x)p(yι | x, y2),
which are not necessarily equivalent. Further, one of the two factorizations might closely approxi-
mate the true distribution, thus being a better choice.
2.2	Existing techniques for set generation
Set generation for computer vision problems has received considerable attention. Specifically,
Rezatofighi et al. (2018; 2020) investigate set outputs for vision tasks. Their learning procedure
involves jointly learning the order and the cardinality of the set. However, their method relies on
searching through a combinatorial space of permutations.
Zhang et al. (2019a) propose deep set prediction networks (DSPN), using an auto-encoder frame-
work with a set encoder for conditional generation of digits and image tags with a fixed maximum
number of elements. Kosiorek et al. (2020) extend DSPN by additionally modeling the cardinality
of the output using an MLP. Finally, Zhang et al. (2020) explore the usage of energy-based models
for set prediction. Their learning and inference procedure relies on drawing samples from the set
distribution, which is prohibitively expensive for extremely high-dimensional spaces like text. Other
examples include works such as Salvador et al. (2019), who aim to extract set of ingredients from
food images.
Our approach differs from their work in several important ways: i) instead of performing an ex-
haustive search over the sample space, we add informative order over labels in the input as a data
augmentation step, ii) we model cardinality simply by listing the set size as the first element of the
sequence, and thus jointly learn both it with the set output, and iii) Image classification and tagging
typically involves a small, independent number of tags. In contrast, nlp tasks have richer and larger
label space. Our method is more suitable for such tasks as it does not rely on exhaustive search over
label space and leverages label dependencies.
Chen et al. (2021) explored the generation of an optimal order for graph generation given the nodes.
They observed that ordering nodes before inducing edges improves graph generation. However, in
our case, since the labels themselves are being generated, conditioning on the labels to create the
optimal order is not possible for non-trivial setups.
Non-seq2seq set generation These include using deep reinforcement learning for multi-label
classification (Yang et al., 2019) and combinatorial problems such as Sudoku (Nandwani et al.,
2020), and pointer networks (Ye et al., 2021) for extracting and generating keyphrases. Unlike
these works, our focus is on methods that can optimally adapt existing seq2seq models for set
generation, without doing using external knowledge (Wang et al., 2020; Zhang et al., 2019b). Since
our approach does not involve directly changing the model parameters or training procedure, we can
leverage the advantages of the pretraining-finetuning paradigm and large-scale language models,
which have shown immense promise in several nlp tasks.
Connection with Janossy pooling Murphy et al. (2019) generalize deep sets by proposing to
encode a set of N elements by pooling permutations of P(N, k) tuples. With k = N, their method
3
Under review as a conference paper at ICLR 2022
is the same as pooling all N ! sequences, and with k = 1, it reduces to deep sets. Our approach
shares the spirit of tractable searching over N ! with Janossy pooling. However, instead of iterating
over all possible 2-tuples, our method imposes pairwise constraints on the order of the elements.
2.3	Modeling set input
A number of techniques have been proposed for encoding set-shaped inputs (Santoro et al., 2017;
Zaheer et al., 2017; Lee et al., 2019; Murphy et al., 2019; Huang et al., 2020; Kim et al., 2021).
Specifically, Zaheer et al. (2017) propose deep sets, wherein they show that pooling the represen-
tations of individual set elements and feeding the resulting features to a non-linear network is a
principled way of representing sets. Lee et al. (2019) present permutation-invariant attention to en-
code shapes and images using a modified version of attention (Vaswani et al., 2017). We note that
our work focuses on settings where the input is a sequence, and the output is a set.
3	Method
In this section, we present tsample, a novel method that tractably creates informative orders over
sets. We also present our approach of jointly modeling cardinality and set output.
3.1	Adding informative orders for set output
As discussed in Section 2, seq2seq formulation requires the output to be in a sequence. Prior work
(Vinyals et al., 2016; Rezatofighi et al., 2018; Chen et al., 2021) has noted that adding orders that
have the highest conditional likelihood given the input is an optimal choice. Unlike these meth-
ods, we create training data using orders sampled from tsample, thus completely sidestepping
exhaustive searching during training.
Our core insight is that knowing the optimal order between pairs of symbols in the output drastically
reduces the possible number of permutations. We thus impose pairwise order constraints for a subset
of labels. Specifically, given an output set Yt = y1, y2, . . . , yk, if yi, yj are independent, they can
be added in an arbitrary order. Otherwise, an order constraint is added to the order between yi , yj .
Learning pairwise constraints We estimate the dependence between elements yi , yj using point-
wise mutual information: pmi(yi,yj) = logp(yi,yj)/p(yi)p(yj). Here, pmi(yi,yj) > 0 indi-
cates that the labels yi , yj co-occur more than would be expected under the conditions of indepen-
dence (Wettler & Rapp, 1993). We use pmi(yi, yj) > α to filter our such pairs of dependent pairs,
and perform another check to determine if the order between them should be fixed. For each de-
pendent pair yi, yj, the order is constrained to be [yi,yj] if log p(yj | yi) - log p(yi | yj) > β (yj
should come after yi), and [yj, yi] otherwise. Intuitively, log p(yj | yi) - log p(yi | yj) > β implies
that knowledge that a set contains yi , increases the probability of yj being present. Thus, fixing the
order to [yi , yj] will be more efficient for generating a set with {yi , yj }.
Generating samples To systematically create permutations that satisfy these constraints, we con-
struct a topological graph Gt where each node is a label yi ∈ Yt , and the edges are determined
using the pmi and the conditional probabilities as outlined above (Algorithm 1). The required per-
mutations can then simply be generated as topological traversals Gt (Figure 2). To generate diverse
samples, we begin the traversal from a different starting node. We call this method tsample. Later,
we show that tsample can be interpreted as a proposal distribution in variational inference frame-
work, which distributes the mass uniformly over informative orders constrained by the graph.
Do pairwise constraints hold for longer sequences? While TSAMPLE uses pairwise (and not
higher-order) constraints for ordering variables, we note that the pairwise checks remain relevant
with extra variables. First, dependence between pair of variables is retained in joint distributions
involving more variables (yi 6⊥⊥ yj =⇒ yi 6⊥⊥ yj, yk) for some yk ∈ Y (Appendix A.1). Further,
if yi,yj ⊥⊥ yk, then it can be shown that p(yi | yj) > p(yj | yi)	=⇒ p(yi | yj,yk) >
p(yj | yi , yk) (Appendix A.2). The first property shows that the pairwise dependencies hold in the
presence of other elements of the set. The second property shows that an informative order continues
4
Under review as a conference paper at ICLR 2022
to be informative when additional independent symbols are added to it. Thus, our criterion of using
pairwise dependencies between the elements of a set is still effective. Finally, we note that using
higher-order dependencies might be suboptimal for practical reasons: higher-order dependencies (or
including xt) might not be accurately discovered due to sparsity, and thus causing spurious orders.
Algorithm 1 Generating permutations for Yt
Input: Set Yt, number of permutations n
Parameter: α, β
Output: n topological sorts over Gt (V, E)
1:	Let V = Yt,E = 0.
2:	for yi , yj ∈ Yt do
3:	if pmi(yi, yj) > α and log p(yi | yj) -
log p(yj | yi) > β then
4:	E = E ∪ yj → yi
5:	end if
6:	end for
7:	return topo.sort(Gt (V, E), n)
yi,y"k,yι,ym .
ym.yi,yj,yι.yk .
yk，yi，yi，yj，ym ×
y,.yk-yι>yj-ym ×
topological
sort
Figure 2: Our method first builds a graph Gt
over the set Yt , and then samples orders from
Gt using topological sort (topo_sort). The
topological sorting rejects samples that do not
follow the conditional probability constraints.
Complexity analysis Let Y be the label space (i.e., set of all possible labels), (xt, Yt) be a partic-
ular training example, N be the size of the training set, and c be the maximum number of elements
for any set Yt in the input. Our method requires three steps: i) iterating over the training data to learn
conditional probabilities and pmi, and ii) given a Yt , building the topo-graph Gt (Algorithm 1), and
iii) traversing Gt to create samples for (xt, Yt).
The time complexity of the first operation is O(N c2): for each element of the training set, the
pairwise count for each pair yi , yj and unigram count for each yi is calculated. The pairwise counts
can be used for calculating joint probabilities. In principle, we need O(|Y|2) space for storing the
joint probabilities, but only a small fraction of the possible combinations appear together in practice.
Given a set Yt, the graph Gt is created in O(c2) time. Then, generating k samples from Gt requires
a topological sort, for O(kc) (or O(c) per traversal). For training data of size N, the total time
complexity is O(N ck).
The entire process (building the joint counts and creating graphs and samples) takes less than five
minutes for all datasets for our experiments (on an 80-core Intel Xeon Gold 6230 CPU) .
Why should augmenting with permutations help? We show that our method of augmenting
permutations to the training data can be interpreted as an instance of variational inference with the
order as a latent variable, and TSAMPLE as an instance of a richer proposal distribution. Let πj
be the jth order over Yt (out of |Yt |! possible orders Π), and πj (Yt) be the sequence of elements
in Yt arranged with order πj . Treating π as a latent random variable, the output distribution can
then be recovered by marginalizing over Π: log pθ (Yt | xt) = log π ∈Π pθ (πz (Yt) | xt), Π:
log pθ (Yt	|	xt)	= log	πz∈Π pθ (Yt , πz	|	xt)	where	pθ	is the SEQ2SEQ conditional generation
model. While summing over Π is intractable, standard techniques from the variational inference
framework allow us to write a lower bound (elbo) on the actual likelihood:
log pθ (Yt | xt)
logpθ(πz(Yt) | xt)
log X Pθ (nz (Yt)Ixt) ≥ Eqφ (∏z ) -----V-ʒ---------
∏z ∈∏	、L	{q(Z)
L(θ, φ)
ELBO
In practice, the optimization procedure draws k samples from the proposal distribution q to optimize
a weighted ELBO (Burda et al., 2016; Domke & Sheldon, 2018). Crucially, q can be fixed (e.g., to
uniform distribution over the orders), and in such cases only θ are learned (Appendix C).
tsample can thus be seen as a particular proposal distribution that assigns all the weights to the
topological ordering over the label dependence graphs. We also experiment with sampling from a
5
Under review as a conference paper at ICLR 2022
uniform distribution over the samples (referred to as uniform experiments in our baseline setup).
We note that the idea of using an informative proposal distribution over space of structures to do
variational inference has also been used in the context of grammar induction (Dyer et al., 2016) and
graph generation (Jin et al., 2018; Chen et al., 2021). Our formulation is closest in spirit to Chen
et al. (2021). However, in their graph generation setting, the set of nodes to be ordered is already
given. In contrast, we infer the order and the set elements jointly from the input.
3.2	Modeling cardinality
Let m = |Yt | be the cardinality of Yt (or the number of elements in Yt). Our goal is to jointly
estimate m and Yt (i.e., p(m, Yt | xt)). Additionally, we want the model to use the cardinal-
ity information for generating Yt . To this end, we simply add the order information at the be-
ginning of the sequence. That is, we convert a sample (xt, Yt) to (xt, [|Yt|, π(Yt)]), and then
train our SEQ2SEQ model as usual from x → [|Yt|, π(Yt)]. As SEQ2SEQ models use autore-
gressive factorization, listing the order information first ensures that the sequence factorizes as
p([∣Yt∣,∏(Yt)] | Xt) = p(|Yt| | xt)p(∏(Yt) | |Yt|, xt). Thus, the generation of Yt is conditioned
on both the input and the cardinality as desired (note the p(π(Yt) | |Yt|, xt) term).
Why should cardinality help? Unlike models like deep sets (Zhang et al., 2019a), SEQ2SEQ
models are not restricted by the number of elements generated in the output. However, the informa-
tion about the number of elements to be generated has two potential benefits: i) it can help avoid
over-generation (Welleck et al., 2019; Fu et al., 2021), and ii) unlike free-form text output, the distri-
bution of the set output size (p(|Yt| | xt)) might benefit the model to adhere to the set size constraint.
Thus, information on the predicted size can be beneficial for the model to predict the elements to be
generated. In the following section, we extensively test our proposed method via a simulated setting
and empirical analysis on diverse real-world datasets.
4	Experiments
4.1	Simulation
We design a simulation to investigate the effects of
output order and cardinality on conditional set gen-
eration, following prior work that has found simu-
lation to be an effective for studying properties of
deep neural networks (Vinyals et al., 2016; Khan-
delwal et al., 2018).
Data generation We use a graphical model (Fig-
ure 3) to generate conditionally dependent pairs
(x, Y), with different levels of interdependencies
Figure 3: Generative process for simulation.
among the labels in Y. Let Y = {y1, y2, . . . , yN}
be the label space (i.e., label space ). We sample
a dataset of the form {(x, y)}im=1. x is an N dimensional multinomial sampled from a dirichlet
parameterized by α. The output set y = {y1, y2, . . . , yBk} is created in B blocks, each block of
size k and yi ∈ Y. A block is created by first sampling k - 1 labels (yp ) independently from
Multinomial(x). The kth label (ys) is sampled from either a uniform distribution with a probability
= or is deterministically determined from the preceding k - 1 labels. For block size of 1 (k = 1),
the output is simply a set of size B sampled from x where all the labels are independent. Similarly,
k = 2 simulates a situation with a high degree of dependence: each block is of size 2, with yp sam-
pled independently from the input, and the ys determined deterministically from yp . . Gradually
increasing the block size increases the number of independent elements.
4.1.1	S imulation Results
We use the architecture of bart-base (Lewis et al., 2020) without pre-training for all simulations2.
2All the simulations were repeated using three different random seeds, and we report the averages.
6
Under review as a conference paper at ICLR 2022
tsample leads to higher set overlap and helps across all sampling types: To test our method
against UNIFORM, we use perplexity and jaccard coefficient. Jaccard coefficient captures the ability
of the model to generate more informative sequences, whereas perplexity measures model’s sensi-
tivity to order. We gradually augment the training data with orders sampled from a uniform distribu-
tion over orders (uniform) and tsample, and evaluate the learning and the final set overlap using
training perplexity and Jaccard score, respectively. The results show that augmentations done using
tsample help the model converge faster, and to a lower perplexity (Figure 4 left). tsample also
consistently outperforms uniform across block sizes (Figure 4 right). We observe that the efficacy
of tsample reduces with increasing block size. This can be understood by noting that as the num-
ber of independent elements increase, the effect of order on the joint distribution diminishes (proof
in Appendix A.3). Further, we found that tsample is not sensitive to the sampling type: across five
different sampling types, including nucleus (Holtzman et al., 2020) and greedy sampling, augment-
ing with tsample permutations yields significant gains (Table 5 in Appendix E).
Figure 4: Effect of tsample on perplexity (left) and set overlap (right).
seq2seq models can learn cardinality
and use it for better decoding : We cre-
ated sample data from Figure 3 where the
length of the output is determined by sum
of the inputs X . We experimented with
and without including cardinality as the
first element. We found that training with
cardinality increases step overlap by over
15%, from 40.54 to 46.13. Further, the
	avg/min/max labels per sample	unique labels	train/test/dev samples per split
go-emo	3.03/3/5	28	0.6k/0.1k/0.1k
OPENENT	5.4/2/18	2519	2k/2k/2k
REUTERS	2.52/2/11	90	0.9k/0.4k/0.3k
version with cardinality accurately gen-	Table 1: Dataset statistics.
erated sets which had the same length as
the target 70.64% of the times, as opposed to 27.45% for the version without cardinality. A num-
ber of other findings, including conditions where order matters the most, effect of randomness and
independence on our task are included in Appendix E.
4.2	Real-world tasks
To establish the efficacy of our approach in real-world data settings, we experiment with three dif-
ferent multi-label classification tasks (examples in Table 3):
•	Go-Emotions classification (GO-EMO, Demszky et al. (2020)): Generating a set of emotions for
an input paragraph.
•	Open Entity Typing (OPENENT, Choi et al. (2018)): Assigning open types (free-form phrases)
to the tagged entities in the input text. Here, the set of possible entity types is open, this task
allows us to investigate our method in situations where the label space is not constrained.
•	Reuters-21578 (REUTERS, Lewis (1997)): A collection of newswire documents from Reuters,
where each article has to be labeled with a set of economic subjects mentioned in it.
We treat all the problems as open-ended generation problems, and do not use any specialized pre-
processing. For all the datasets, we filter out samples with a single label. For each training sample,
we create n permutations over TSAMPLE to create the training data.
7
Under review as a conference paper at ICLR 2022
Baselines We experiment with the following four baselines (Table 2):
•	BART-MULTI-LABEL: A multi-label classifier where the input is encoded using bart-base, and
used to make independent (pointwise) predictions the output labels. This baseline represents the
standard method for doing multi-label classification (e.g., Demszky et al. (2020)). During infer-
ence, we take top k = [1, 3, 5, 10, 50] labels as the true labels, and report the average (Table 9
shows experiments with bert-base-uncased).
•	SET SEARCH: each training sample (x, {y1, y2, . . . , yk}) is converted into k different training
examples {(x, yi)}ik=1. During inference, unique elements generated by beam search are re-
turned as the set output. The size of the beam is set to the maximum possible set size in the
training data (Table 1). This is a popular approach for one-to-many generation tasks (Hwang
et al., 2021).
•	SEQ2S EQ: set elements are listed in a random order, and each sample is repeated n times.
•	UNIFORM: n permutations are uniformly sampled from the possible permutations of labels.
Model We use BART-base (Lewis et al., 2020) with pre-trained weights for all the tasks. We use
n = 2 for TSAMPLE and UNIFORM. For all the results, we use three epochs and the same number of
training samples. This controls for models trained with augmented data improving only because of
factors such as longer training time. All the experiments were repeated for three different random
seeds, and we report the averages. We conduct a one-tailed proportion of samples test (Johnson
et al., 2000) to compare the best model with seq2seq (we do not use set search for calculating
significance) and underscore all results that are significant with p < 0.0005. For Algorithm 1, we
experiment with α = {0.5, 1, 1.5} and β = {log2 (2), log2(3), log2(4)}, and use the implementation
of topological sort provided by networkx (Hagberg et al., 2008) and ignore cycles. We found from
our experiments that hyperparameter tuning over α, β did not affect the results in any significant
way. For all the experiments reported, we use α = 1 and β = log2 (3). We use a single GeForce
RTX 2080 Ti for all our experiments. Additional hyperparameter details in Appendix D.
Results Table 2 summarizes the empirical results on the tasks. We report macro precision, re-
call, and F -measure on individual datasets. We observe that across all the datasets, incorporating
cardinality and using tsample improves the performance significantly. When used with baseline
approaches across all the datasets, modeling cardinality as part of the output provides significant per-
formance gains. To complement, our tsample further improves the performance across datasets.
More specifically, we observe that both precision and recall improves, showing the overall efficacy
of our approach. TSAMPLE improves over UNIFORM and SEQ2SEQ by about 1% absolute F -score
on average. Modeling cardinality provides a consistent performance gain of about 6% for seq2seq,
6% for UNIFORM, and 8% F -score for TSAMPLE. Overall, we achieve a net gain of 9% absolute
F -score by incorporating both informative orders and cardinality.
In further analysis, we observed that the comparatively lower performance of set search baseline
is due to two specific reasons - repeated generation of the same set of terms (e.g., person, business
for openent) and generating elements not present in the test set. We also note that uniform
does not improve over seq2seq consistently (both with and without card), showing that merely
augmenting with random permutations does not help.
4.3 Analysis
To understand the nature of the label dependencies, we use qualitative examples from the datasets
for an in-depth analysis. For this analysis, we selected a random subset of 100 samples from each
of the datasets from the validation set.
What kinds of permutations does tsample create? As discussed in Section 3.1, TSAMPLE
encourages highly co-occuring pairs (yi , yj) to be in the order yi , yj if p(yj | yi) > p(yi | yj). In
our analysis, this dependency in the datasets shows that the orders exhibit a pattern where specific
labels appear before the generic ones. For example, in case of entity typing, the more generic entity
event is generated after the more specific entities home game and match Figure 4.3 (left).
Increasing the number of permutations (n) We compare TSAMPLE and UNIFORM as n increases
from n = 2 to 10. Figure 4.3 (right) shows that both TSAMPLE and UNIFORM improve as n is
increased, with TSAMPLE outperforming UNIFORM across n.
8
Under review as a conference paper at ICLR 2022
go-emo			OPENENT					REUTERS	
	p	r	F	P	r	F	p	r	F
bart-multi-label	20.8	42.4	22.4	16.4	25.1	14.3	19.7	43.4	21.7
SET SEARCH	10.7	7.0	7.4	26.5	31.4	26.3	10.9	7.1	7.5
seq2seq	27.4	26.2	23.4	55.4	42.4	44.6	24.8	13.8	15.6
UNIFORM	32.5	19.9	22.7	62.6	41.7	46.9	26.7	12.7	15.2
TSAMPLE	36.7	19.8	23.3	60.0	44.5	48.0	26.5	12.8	15.8
seq2seq +card	33.0	28.3	26.8	62.5	44.7	50.5	34.1	21.8	24.3
uniform + card	35.6	26.5	27.5	68.6	42.3	50.4	35.3	22.1	24.7
tsample + card	36.1	30.5	30.0	653	47.5	53.5	36.7	24.1	26.7
Table 2: Our main results: using permutations generated by tsample and adding cardinal-
ity gives the best overall performance in terms of macro precision, recall, and F -score. BART-
multi-labelis the standard multi-label classification approach. Statistically significant results are
underscored. CARD stands for cardinality.
Figure 5: Left: label dependencies used by TSAMPLE for OPENENT: TSAMPLE puts specific enti-
ties (e.g., volleyball) before generic ones (e.g., event). Right: TSAMPLE ((T)) consistently outper-
forms UNIFORM ((U)) as n is increased.
Role of cardinality From the results in Table 2, we observe that cardinality is crucial to modeling
set output. To study whether the models learn to condition on predicted set length, we compute an
agreement score - defined as the % of times the predicted cardinality matches the number of elements
generated by the model. We observe that the model effectively predicts the cardinality almost exactly
in both go-emo and reuters datasets (average 95%). While the exact match agreement is low in
OPENENT (35%), the model is within an error of ±1 in 93% of the cases.
Reversing the order In order to check our hypothesis of whether only informative orders helping
with set generation, we invert the label dependencies returned by tsample for all the datasets and
train with the same model settings. Across all datasets, we observe that reversing the order leads to
an average of 12% drop in F -score. The reversed order not only closes the gap between TSAMPLE
and uniform, but in many instances, the performance is slightly worse than uniform.
5 Conclusion
We present a novel method for performing conditional set generation using seq2seq models that
leverages both incorporating informative orders and adding cardinality information. Experiments
in simulated settings and real-world datasets show that our method is more effective than strong
baselines at set generation. We also present an in-depth analysis of our method along with the
empirical results. In the future, we want to extend this work to explore better proposal distributions
and to incorporate cardinality information in open-ended generation tasks like dialogue.
9
Under review as a conference paper at ICLR 2022
Ethics and Reproducibility S tatement
We take the following steps for reproducibility of our results:
1.	All the experiments are performed for three different random seeds. In addition, we conduct
a proportion of samples hypothesis test to establish the statistical significance of our results.
We did not perform extensive hyperparameter tuning and used the same set of defaults for
baselines and our proposed method.
2.	For all data augmentation experiments, we match the number of training samples and
epochs; all the models are trained for the same duration. This alleviates the concern that
the models perform well with augmented data merely because of the longer training time.
3.	We conduct a proportion of samples test for all the experiments conducted on real-world
datasets and use a small p = 0.0005 to measure highly significant results, which are indi-
cated with an underscore.
Our work aims to promote the usage of existing resources for as many use cases as possible. In
particular, all our experiments are performed on the BASE-version of the model (BART) that can
relatively lower parameter count to conserve resources and help lower our impact on climate change.
We propose a method to use existing pre-trained language models more efficiently for set generation.
Our downstream datasets in this work do not contain any societally impactful or social themes.
Hence, we do not anticipate any misuse as-is. To the best of our knowledge, we did not encounter
any downstream tasks that can leverage our method for any negative impact. Despite that, it is
certainly possible we might have missed something, and we are happy to engage anonymously with
the reviewers, and the chairs and help address the concerns that may arise.
References
Yuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
ICLR (Poster), 2016.
Xiaohui Chen, Xu Han, Jiajing Hu, Francisco JR Ruiz, and Liping Liu. Order matters: Probabilistic
modeling of node sequence for graph generation. arXiv Preprint arXiv:2106.06189, 2021.
EUnsol Choi, Omer Levy, Yejin Choi, and LUke Zettlemoyer. Ultra-fine entity typing. In Proceedings
of the 56th Annual Meeting of the Association for CompUtational LingUistics (VolUme 1: Long
Papers), pp. 87-96, 2018.
Hongliang Dai, YangqiU Song, and HaixUn Wang. Ultra-fine entity typing with weak sUpervision
from a masked language model. arXiv PrePrint arXiv:2106.04098, 2021.
Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, GaUrav Nemade, and
Sujith Ravi. Goemotions: A dataset of fine-grained emotions. In PrOceedings of the 58th AnnUal
Meeting of the AssOciatiOn for COmPUtatiOnal LingUistics, pp. 4040T054, 2020.
Justin Domke and Daniel Sheldon. Importance weighting and variational inference. In Proceedings
of the 32nd International COnference on NeUral InfOrmatiOn PrOcessing Systems, pp. 4475^484,
2018.
Chris Dyer, AdhigUna KUncoro, MigUel Ballesteros, and Noah A Smith. RecUrrent neUral network
grammars. In PrOceedings OfNAACL-HLT,pp. 199-209, 2016.
Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 889-898, 2018.
Zihao FU, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition
problem in text generation. In PrOceedings of the AAAI COnference on Artificial Intelligence,
volume 35, pp. 12848-12856, 2021.
10
Under review as a conference paper at ICLR 2022
Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and func-
tion using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
(United States), 2008.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OPenRevieW.net, 2020. URL https://openreview.
net/forum?id=rygGQyrFvH.
Qian Huang, Horace He, Abhay Singh, Yan Zhang, Ser-Nam Lim, and Austin Benson. Better set
representations for relational reasoning. Advances in NeUraI Information Processing Systems,
2020.
Jena D HWang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosse-
lut, and Yejin Choi. (comet-) atomic 2020: On symbolic and neural commonsense knoWledge
graphs. In Proceedings of the AAAI Conference on ArtificiaI Intelligence, volume 35, pp. 6384-
6392, 2021.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In IntemationaI conference on machine Iearning, pp. 2323-2332.
PMLR, 2018.
Richard A Johnson, Irwin Miller, and John E Freund. ProbabiIity and statistics for engineers, volume
2000. Pearson Education London, 2000.
Xincheng Ju, Dong Zhang, Junhui Li, and Guodong Zhou. Transformer-based label set generation
for multi-modal multi-label emotion detection. In Proceedings of the 28th ACM International
Conference on Multimedia, pp. 512-520, 2020.
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural
language models use context. In Proceedings of the 56th AnnUal Meeting of the Association for
Computational LingUistics (VoIUme 1: Long Papers), pp. 284-294, 2018.
Jinwoo Kim, Jaehoon Yoo, Juho Lee, and Seunghoon Hong. Setvae: Learning hierarchical composi-
tion for generative modeling of set-structured data. In Proceedings of the IEEE/CVF Conference
on ComPUter Vision and Pattern Recognition, pp. 15059-15068, 2021.
Adam R Kosiorek, Hyunjik Kim, and Danilo J Rezende. Conditional set generation with transform-
ers. arXiv PrePrint arXiv:2006.16841, 2020.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set trans-
former: A framework for attention-based permutation-invariant neural networks. In International
Conference on Machine Learning, pp. 3744-3753. PMLR, 2019.
David Lewis. Reuters-21578 text categorization test collection, distribution 1.0. http://www.
research/. att. com, 1997.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. In Proceedings of the
58th AnnUal Meeting of the Association for ComPUtational LingUistics, pp. 7871-7880, 2020.
R Murphy, B Srinivasan, V Rao, and B Riberio. Janossy pooling: Learning deep permutation-
invariant functions for variable-size inputs. In International Conference on Learning
RePresentations (ICLR 2019), 2019.
Yatin Nandwani, Deepanshu Jindal, Parag Singla, et al. Neural learning of one-of-many solutions
for combinatorial problems in structured output spaces. arXiv PrePrint arXiv:2008.11990, 2020.
Hamid Rezatofighi, Roman Kaskman, Farbod T Motlagh, Qinfeng Shi, Anton Milan, Daniel Cre-
mers, Laura Leal-Taixe, and Ian Reid. Learn to predict sets using feed-forward neural networks.
arXiv PrePrint arXiv:2001.11845, 2020.
11
Under review as a conference paper at ICLR 2022
S Hamid Rezatofighi, Roman Kaskman, Farbod T Motlagh, Qinfeng Shi, Daniel Cremers, Laura
Leal-Taixe, and Ian Reid. Deep perm-set net: Learn to predict sets with unknown permutation
and cardinality using deep neural networks. arXiv PrePrint arXiv:1805.00613, 2018.
Amaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and Adriana Romero. Inverse cooking:
Recipe generation from food images. In ProceedingS of the IEEE/CVF Conference on ComPUter
ViSion and Pattern Recognition, pp. 10453-10462, 2019.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning.
AdvanCeS in NeUraI InfOrmatiOn Processing Systems, 30, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In AdvanCeS in neuralinformation
processing systems, pp. 5998-6008, 2017.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for
sets. In Yoshua Bengio and Yann LeCun (eds.), 4th International COnferenCe on Learning
RePreSentations, ICLR 2016, San Juan, PUertO Rico, May 2-4, 2016, COnferenCe TraCk
Proceedings, 2016. URL http://arxiv.org/abs/1511.0 6391.
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang,
Ming Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv
PrePrint arXiv:2002.01808, 2020.
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.
Neural text generation with unlikelihood training. In International Conference on Learning
RePreSentations, 2019.
Manfred Wettler and Reinhard Rapp. Computation of word associations based on co-occurrences
of words in large corpora. In Very Large Corpora: ACademiC and IndUStrial Perspectives, 1993.
URL https://aclanthology.org/W93-0310.
Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, and Houfeng Wang. Sgm: Sequence gen-
eration model for multi-label classification. In PrOCeedingS of the 27th International COnferenCe
on Computational LingUiStics, pp. 3915-3926, 2018.
Pengcheng Yang, Fuli Luo, Shuming Ma, Junyang Lin, and Xu Sun. A deep reinforced sequence-
to-set model for multi-label classification. In Proceedings of the 57th Annual Meeting of the
ASSOCiatiOn for Computational LingUiStics, pp. 5252-5258, 2019.
Jiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, and Qi Zhang. One2set: Generating diverse keyphrases
as a set. arXiv PrePrint arXiv:2105.11134, 2021.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, BarnabaS POczos, Ruslan Salakhutdinov,
and Alexander J. Smola. Deep sets. In Isabelle Guyon, Ulrike von Luxburg, Samy Ben-
gio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.),
AdvanCeS in Neural InfOrmatiOn Processing SyStemS 30: AnnUaI COnferenCe on Neural
InfOrmatiOn PrOCeSSing SyStemS 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
3391-3401, 2017. URL https://proceedings.neurips.cc/paper/2 017/hash/
f22e4747da1aa27e363d86d40ff442fe- Abstract.html.
David W Zhang, Gertjan J Burghouts, and Cees GM Snoek. Set prediction without imposing struc-
ture as conditional density estimation. arXiv PrePrint arXiv:2010.04109, 2020.
Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett. Deep set prediction networks. AdvanceS in
NeUraI InfOrmatiOn PrOCeSSing Systems, 32:3212-3222, 2019a.
Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced
language representation with informative entities. In PrOCeedingS of the 57th AnnUaI Meeting of
the ASSOCiatiOn for Computational LingUiStics, pp. 1441-1451, 2019b.
12
Under review as a conference paper at ICLR 2022
A	Proofs
Let Ybe the output space, yi, yj, yk ∈ Y, and yk ∈ Y-yi -yj be a subset of the symbols excluding
yi, yj. We assume that all the distributions are non-negative (i.e., p(y) > 0, ∀y ∈ Y)
Lemma A.1 yi 6⊥⊥ yj =⇒ yi 6⊥⊥ (yjyk)
Proof Let yi ⊥⊥ (yjyk) by contradiction. Then:
p(yi , yjyk) = p(yi)p(yjyk)	(2)
Also,
p(yi, yj ) =	p(yi, yjyk)
yk∈Z
=	p(yi)p(yjyk)	(equation 2)
yk ∈Z
= p(yi)	p(yjyk)
yk∈Z
= p(yi)p(yj)	(3)
However, yi 6⊥⊥ y thus yi 6⊥⊥ y =⇒ yi 6⊥⊥ (yjyk).
Lemma A.2
p(yi |	yj)	> p(yj	| yi)	=⇒	p(yi	| yj,yk)	>	p(yj	|	yi,yk)
ifyi,yj ⊥⊥ yk
Proof We have:
p(yi | yj) > p(yj | yi)
=⇒ p(yj) < p(yi)	(4)
p(yj , yk) = p(yk | yj )p(yj )
< p(yk | yj )p(yi)
= p(yk | yi)p(yi)
= p(yi , yk )
(Equation 4)
(yi , yj ⊥⊥ yk =⇒ p(yk | yj ) = p(yk | yi) = p(yk))
(5)
Thus,	p(yi, yj , yk) p(yi | yj，yk ) = p(yi,yj,yk) > 	±	 p(yi,yk) = p(yj | yi, yk)	(6)
Lemma A.3 If yi ⊥⊥ yj ∀yi, yj ∈ Y, the order is guaranteed to not affect learning.
Proof Let πj be the jth order over Y (out of |Y|! possible orders Π), and πj (Y) be the sequence
of elements in Y arranged with πj .
p(yi | yj) = p(yi)	(yi ⊥⊥ yj ∀yi, yj)
=⇒ p(yi,yj,yk) = p(yi)p(yj | yi)p(yk | yi,yj)
= p(yi)p(yj)p(yk)
=⇒ p(πm(yi,yj,yk)) =p(πn(yi,yj,yk)) ∀πm,πm ∈ Π
In other words, when all elements are mutually independent, all possible joint factorizations will
simply be a product of the marginals, and thus identical.
13
Under review as a conference paper at ICLR 2022
Lemma A.4 The graphs constructed to sample orders for TSAMPLE cannot have cycles.
Proof Let yi , yj , yk form a cycle: yi → yj → yk → yi . By construction, the following conditions
must hold for such a cycle to be present:
log p(yj | yi) - log p(yi	|	yj)	>	β	=⇒	logp(yi)	<	log p(yj)
log p(yk | yj) - log p(yj	|	yk)	>	β	=⇒	logp(yj)	<	logp(yk)
log p(yi | yk) - log p(yk	|	yi)	>	β	=⇒	log p(yk)	<	logp(yi)
Putting the three implications together, we get log p(yi) < logp(yj) < logp(yk) < logp(yi),
which is a contradiction. Hence, the graphs constructed for tsample cannot have a cycle.
B Dataset
	Input	Output
Fine-grained emotion classification, [28] (Demszky et al., 2020)	So there’s hope for the rest of us! Thanks for sharing. What helped	curosy, graue, optimism} you get to where you are?	P	)
Open-entity typing [2519]	Some 700,000 cubic meters of	{colony, region,
(Choi et al., 2018)	caustic sludge and water burst	location, hamlet, inundating [SPAN] three west	area, village, Hungarian villages [SPAN] and spilling.	settlement, community}
Reuters [90]	India is reported to have bought
(Lewis, 1997)	two white sugar cargoes for. . .	{ship, sugar} . . .cargo sale, they said.
Table 3: Real world tasks used for experiments
C Fixing the proposal distribution in the vae formulation
log pθ (Y | x) = log	pθ(πz(Y) | x)
πz ∈Π
log
πz ∈Π
qφ(π)
q0(nz)
pθ (πz (Y) | x)
log Eqφ (πz)
Pθ(∏z(Y) | x)
一	qφ(πz)
≥ Eqφ(πz) [log pθ (Y, πz | x)] -Eqφ(πz) [logqφ(πz)]
log pθ (Y| x) = log	pθ (πz (Y) | x) ≥ Eqφ(πz)
∏z∈∏	X______
logpθ(πz(Y) | x)
qφ(πz )
-- /
>z
L(θ, φ)
ELBO
(7)
Where equation 7 is the evidence lower bound (elbo). The success of this formulation depends on
the quality of the proposal distribution q from which the orders are drawn. When q is fixed (e.g.,
to uniform distribution over the orders), learning only happens for θ. This can be clearly seen from
splitting Equation 7 into terms that involve just θ and φ:
VφL(θ,φ)=0
NBL(θ,φ) = VθEqφ(∏z) [logPθ(Y,∏z | x)]
14
Under review as a conference paper at ICLR 2022
D Hyperparameters
We list all the hyperparameters in Table 4.
Hyperparameter	Value
GPU	GeForce RTX 2080 Ti
gpus	1
auto_select_gpus	false
accumulate_grad_batches	1
max_epochs	3
precision	32
Iearning_rate	1e-05
adam_epsilon	1e-08
num_workers	16
warmup_prop	0.1
seeds	[15143, 27122, 999888]
add_lr_scheduler	true
lr_scheduler	linear
max_source_Iength	120
max_target_length	120
val_max_target_length	120
test_max_target」ength	120
Table 4: List of hyperparameters used for all the experiments.
E Exploring the influence of order on seq2seq models with a
SIMULATION
We design a simulation to investigate the effects of output order and cardinality on conditional set
generation, following prior work that has found simulation to be an effective for studying properties
of deep neural networks (Vinyals et al., 2016; Khandelwal et al., 2018).
Data generation We use a graphical model (Figure 3) to generate conditionally dependent pairs
(x, Y), with different levels of interdependencies among the labels in Y. Let Y = {y1, y2, . . . , yN}
be the set of output labels. We sample a dataset of the form {(x, y)}im=1. x is an N dimensional
multinomial sampled from a dirichlet parameterized by α, and y is a sequence of symbols with each
yi ∈ Y. The output sequence y is created in B blocks, each block of size k. A block is created
by first sampling k - 1 prefix symbols independently from Multinomial(x), denoted by yp The
kth suffix symbol (ys) is sampled from either a uniform distribution with a probability = or is
deterministically determined from the preceding k - 1 prefix terms. For block size of 1 (k = 1), the
output is simply a set of size B sampled from x (i.e., all the elements are independent). Similarly,
k = 2 simulates a situation with a high degree of dependence: each block is of size 2, with the prefix
sampled independently from the input, and the suffix determined deterministically from the prefix.
Gradually increasing the block size increases the number of independent elements.
15
Under review as a conference paper at ICLR 2022
Figure 6: The generative process for simulation
% Randomness
Figure 7: Perplexity vs. Randomness for varying
block sizes
E.1 Major Findings
We now outline our findings from the simulation. We use the architecture of bart-base Lewis
et al. (2020) (six-layers of encoder and decoder) without pre-training for all simulations. All the
simulations were repeated using three different random seeds, and we report the averages.
Finding 1: seq2seq models are sensitive to order, but only if the labels are conditionally
dependent on each other. We train with the prefix yp listed in the lexicographic order. At test
time, the order of is randomized from 0% (same order as training) to 100 (appendixly shuffled).
As can be seen from Figure 7 the perplexity gradually increases with the degree of randomness.
Further, note that perplexity is an artifact of the model and is independent of the sampling strategy
used, showing that order affects learning.
Finding 2: Training with random orders makes the model less sensitive to order As Figure 8
shows, augmenting with random order makes the model less sensitive to order. Further, augmenting
with random order keeps helping as the perplexity gradually falls, and the drop shows no signs of
flattening.
Finding 3: Effects of position embeddings can be overcome by augmenting with a sufficient
number of random samples Figure 8 shows that while disabling position embedding helps the
baseline, similar effects are soon achieved by increasing the random order. This shows that disabling
position embeddings can indeed alleviate some concerns about the order. This is crucial for pre-
trained models, for which position embeddings cannot be ignored.
Figure 8: Augmenting dataset with multiple orders help across block sizes. Augmentations also
overcome any benefit that is obtained by using position embeddings.
Finding 4: tsample leads to higher set overlap We next consider blocks of order 2 where the
prefix symbol yp is selected randomly as before, but the suffix is set to a special character y0p with
50% probability. As the special symbol y0p only occurs with yp , there is a high pmi between each
(yp, y0p) pair as p(yp | y0p) = 1. Different from finding 1, the output symbols are now shuffled
to mimic a realistic setup. We gradually augment the training data with random and topological
orders and evaluate the learning and the final set overlap using training perplexity and Jaccard score,
respectively. The results are shown in Figure 9. Similar trends hold for larger block sizes, and the
results are included in the Appendix in the interest of space.
16
Under review as a conference paper at ICLR 2022
Figure 9: Effect of TSAMPLE on perplexity and set overlap. Left: Augmentations done TSAMPLE
helps the model converge faster and to a lower perplexity. Right: Using TSAMPLE, the overlap
between training and test set increases consistently, while consistently outperforming uniform.
	Beam	Random	Greedy	Top-k	Nucleus
UNIFORM	0.39 ± 0.05	0.39 ± 0.02	0.35 ± 0.05	0.39 ± 0.02	0.39 ± 0.02
TSAMPLE	0.67 ± 0.05	0.67 ± 0.05	0.71 ± 0.04	0.67 ± 0.05	0.68 ± 0.05
Table 5: Set overlap for different sampling types with 200% augmentations. The gains are con-
sistent across sampling types. Similar trends were observed for 100% augmentation and without
positional embeddings. Top-k sampling was introduced by (Fan et al., 2018), and Nucleus sampling
by (Holtzman et al., 2020).
Finding 5: tsample helps across all sampling types We see from Table 5 that our approach
is not sensitive to the sampling type used. Across five different sampling types, augmenting with
topological orders yields significant gains.
Finding 6: seq2seq models can learn cardinality and use it for better decoding We created
sample data from Figure 6 where the length of the output is determined by sum of the inputs X .
We experimented with and without including cardinality as the first element. We found that training
with cardinality increases step overlap by over 13%, from 40.54 to 46.13. Further, the version with
cardinality accurately generated sets which had the same length as the target 70.64% of the times,
as opposed to 27.45% for the version without cardinality.
F	Additional results
We present all the results for the three tasks in Tables 6, 7, and 8.
F.1 Classification results with BERT
Table 9 includes results from a multi-label classification baseline where bert-base-uncased is used
as the encoder.
G Sample graphs
In this section, we present examples from reuters and go-emo datasets to further understand the
permutations generated by our method.
What kinds of permutations does tsample create? As discussed in Section 3.1, TSAMPLE
encourages highly co-occuring pairs (yi, yj) to be in the order yi , yj if p(yj | yi) > p(yi | yj). In
our analysis, this dependency in the datasets shows that the orders exhibit a pattern where specific
labels appear before the generic ones. For example, in case of entity typing, the more generic entity
event is generated after the more specific entities home game and match Figure 4.3 (left).
17
Under review as a conference paper at ICLR 2022
	pmicro	pmacro	rmicro	rmacro	Fmicro	Fmacro	jaccard
SET SEARCH	47.17	10.68	13.09	7.02	10.7	7.36	7.4
seq2seq	41.65	27.39	35.19	26.21	27.4	23.41	23.4
seq2seq + card	39.77	33	38.02	28.31	33	26.79	26.8
uniform + card	44.77	35.6	32.96	26.54	35.6	27.53	27.5
tsample + card	43.37	36.08	34.51	30.54	36.1	30.01	30
uniform- card	48.85	32.45	27.75	19.86	32.5	22.67	22.7
tsample- card	50	36.68	29.84	19.84	36.7	23.31	23.3
Table 6: Results for go-emo.							
	pmicro	pmacro	rmicro	rmacro	Fmicro	Fmacro	jaccard
SET SEARCH	70.04	10.92	34.9	7.1	46.56	7.54	37.49
seq2seq	66.36	24.74	42.28	13.78	51.64	15.58	44.3
seq2seq + card	73.02	34.17	53.8	21.85	61.95	24.28	59.08
uniform + card	74.26	35.31	54.33	22.13	62.75	24.74	58.95
tsample + card	75.65	36.67	55.54	24.13	64.05	26.66	61.14
uniform- card	69.56	26.68	38.15	12.71	49.27	15.2	42.24
tsample- card	76.55	26.49	41.78	12.77	54.06	15.78	47.34
Table 7: Results for reuters.							
	pmicro	pmacro	rmicro	rmacro	Fmicro	Fmacro	jaccard
SET SEARCH	24.65	26.5	29.98	31.44	23.92	26.25	13.39
seq2seq	52.78	55.4	39.84	42.42	41.45	44.63	24.6
seq2seq + card	61.26	62.48	41.87	44.68	48.07	50.48	27.84
uniform + card	67.56	68.59	39.61	42.25	47.98	50.4	26.89
tsample + card	64.58	65.53	44.6	47.46	51.2	53.48	29.39
uniform- card	60.93	62.57	39.09	41.69	44.2	46.85	25.26
tsample- card	58.02	59.88	42.63	44.95	46.54	48.86	26.82
Table 8: Results for openent.							
Figure 10: Label dependencies used by tsample for go-emo (left) and reuters (right) shows
that the method puts specific entities before generic ones.
18
Under review as a conference paper at ICLR 2022
	go-emo				OPENENT			REUTERS		
	p	r	F	P	r	F	p	r	F	
bert @1	31.8	10.3	15.6	38.0	10.3	15.9	31.7	12.3	17.6	
bert @3	23.8	23.4	23.6	19.7	14.0	16.1	23.4	28.3	25.5	
bert @5	20.6	34.0	25.7	15.5	18.0	16.4	18.8	37.6	24.9	
bert @10	16.5	54.3	25.3	11.8	26.0	16.0	15.1	61.8	24.2	
bert @20	14.1	93.2	24.5	8.4	34.3	13.5	9.5	75.9	16.8	
bert @50	-	-	-	2.6	50.2	4.9	8.9	-	-	-
BERT	21.4	43.0	22.9	16.0	25.5	13.8	19.7	43.2	21.8	
bart @1	31.7	10.3	15.5	38.0	10.3	15.6	31.8	12.3	17.6	
bart @3	21.2	21.0	21.0	19.7	14.0	15.8	23.1	28.1	25.2	
bart @5	14.1	33.4	25.6	15.5	18.0	16.2	18.7	37.6	24.8	
bart @10	16.3	53.4	25.0	11.7	26.0	15.9	15.1	62.0	24.1	
bart @20	14.1	93.3	24.5	8.4	34.3	13.4	9.6	77.1	17.1	
bart @50	-	-	-	4.9	48.0	8.9	-	-	-	
BART	20.8	42.4	22.4	16.4	25.1	14.3	19.7	43.4	21.7	
SET SEARCH	10.7	7.0	7.4	26.5	31.4	26.3	10.9	7.1	7.5	
seq2seq	27.4	26.2	23.4	55.4	42.4	44.6	24.8	13.8	15.6	
UNIFORM	32.5	19.9	22.7	62.6	41.7	46.9	26.7	12.7	15.2	
TSAMPLE	36.7	19.8	23.3	60.0	44.5	48.0	26.5	12.8	15.8	
seq2seq +card	33.0	28.3	26.8	62.5	44.7	50.5	34.1	21.8	24.3	
uniform + card	35.6	26.5	27.5	68.6	42.3	50.4	35.3	22.1	24.7	
tsample + card	36.1	30.5	30.0	655	47.5	53.5	36.7	24.1	26.7	
Table 9: Our main results: using permutations generated by tsample and adding cardinality gives
the best overall performance in terms of macro precision, recall, and F-score score. Statistically
significant results are underscored. CARD stands for cardinality. BERT @k / BART @k denotes the
pointwise classification baseline using BERT/ BART where the top k labels are used as the model
output. The average is denoted by bert/ bart.
19