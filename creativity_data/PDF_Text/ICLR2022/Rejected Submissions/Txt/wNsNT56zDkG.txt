Under review as a conference paper at ICLR 2022
Adversarial Rademacher Complexity of Deep
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks are vulnerable to adversarial attacks. Adversarial training is
one of the most effective algorithms to increase the model’s robustness. However,
the trained models cannot generalize well to the adversarial examples on the test
set. In this paper, we study the generalization of adversarial training through
the lens of adversarial Rademacher complexity. Current analysis of adversarial
Rademacher complexity is up to two-layer neural networks. In adversarial settings,
one major difficulty of generalizing these results to deep neural networks is that
we cannot peel off the layer as the classical analysis for standard training. We
provide a method to overcome this issue and provide upper bounds of adversarial
Rademacher complexity of deep neural networks. Similar to the existing bounds
of standard Rademacher complexity of neural nets, our bound also includes the
product of weight norms. We provide experiments to show that the adversarially
trained weight norms are larger than the standard trained weight norms, thus
providing an explanation for the bad generalization performance of adversarial
training.
1	Introduction
Deep neural networks (DNNs) (Krizhevsky et al. (2012); Hochreiter & Schmidhuber (1997)) have
become successful in many machine learning tasks such as computer vision (CV) and natural language
processing (NLP). But they are shown to be vulnerable to adversarial examples (Szegedy et al. (2013);
Goodfellow et al. (2014)). A well-trained model can be easily attacked by adding a small perturbation
to the original data. Adversarial training is one of the most effective algorithms to defend against
adversarial attacks. However, generalization is one of the main issues of adversarial training. An
adversarially-trained model will overfit the adversarial examples on the training dataset, and it cannot
generalize well to the adversarial examples on the testset. For example, in the experiment of training
ResNet (He et al. (2016)) on CIFAR-10 (Krizhevsky et al. (2009)), Projected gradient descent (PGD)
adversarial training achieves 100% robust accuracy on the training set, but it only gets 45% robust
accuracy on the test set (Madry et al. (2017)). Recent works (Gowal et al. (2020); Rebuffi et al.
(2021)) mitigate the overfitting issue, but it still has a 20% generalization gap between robust test
accuracy (60%) and training accuracy (80%). On the other hand, a standard trained model can
generalize well to the test set with a 5% generalization gap. To understand why the generalization of
adversarial training behaves differently from standard training, we study the generalization issue of
adversarial training through the lens of Rademacher complexity.
In classical machine learning theory, Rademacher complexity measures the generalization capacity
of machine learning models. For depth-d neural networks, assuming that the weight matrices
W1,W2,…，Wd in each of the d layers have FrobeniUs norms bounded by Mi,…，Md, and all
the data x have `2 -norm bounded by B, given m training samples, the generalization gap between
population risk and empirical risk scales as O(B2d Qd=i Mι∕√m) with high probability (Neyshabur
et al. (2015)). Another works provide different norm-based complexity, such as ∣∣ ∙ kι,∞-norm
(Bartlett & Mendelson (2002)) and spectral norm (Bartlett et al. (2017)). The work of (Golowich
et al. (2018)) reduces the dependence on depth-d from 2d to √d. The proofs of the above bounds are
based on the induction on layers, which is also called the ‘peeling off’ techniques. For more details,
see section 3.
1
Under review as a conference paper at ICLR 2022
In adversarial training, adversarial Rademacher complexity was first introduced in (Yin et al. (2019);
Khim & Loh (2018)) to measure the robust generalization gap. They prove that the robust generaliza-
tion gap of linear function (X → WTx) scales as O((B + e)M∕√m), where M is the upper bound of
norm of the weights w and is the perturbation intensity of adversarial attacks. Awasthi et al. (2020)
provides an upper bound in two-layers neural network cases. For depth-2, width-h neural networks,
with high probability, the generalization gap scales as O((B + e)√hqM1M2，log m/m), where q
is the dimension of the data x.
One might think it is straightforward to use the induction methods in (Neyshabur et al. (2015);
Golowich et al. (2018)) to extend the adversarial Rademacher complexity in (Yin et al. (2019); Khim
& Loh (2018)) to multi-layers cases. However, it seems challenging to apply their induction methods
to adversarial cases. Let the adversarial loss be maxkχ-χθk≤e '(f (x0),y) and f is a DNN. We cannot
peel off the layer because of the max operation in the adversarial loss. The work of (Khim & Loh
(2018)) and (Gao & Wang (2021)) also indicate the difficulty of analyzing adversarial Rademacher
complexity of DNNs. They analyze other variants of adversarial Rademacher complexity, which are
quite different from the original adversarial Rademacher complexity. See more detailed discussions
of these works in section 3. To our knowledge, direct analysis of the original adversarial Rademacher
complexity is largely missing.
In this paper, we analyze the adversarial Rademacher complexity of deep neural networks. Specifically,
for depth-d, width-h fully connected neural networks, with high probability, the robust generalization
gap scales as
Of (B + e)h√≡gd Qd=ι Ml
√m
Similar to the existing bounds of standard Rademacher complexity of deep neural nets, the bound
includes the product of weight norms Qld=1 kWl k, but they are trained by different algorithms. We
empirically show that the adversarially trained weight norms are larger than the standard trained
weight norms, which provide an explanation why adversarial training did not generalize well. Our
contributions are listed as follow:
1.	We provide a method and give upper and lower bounds for the adversarial Rademacher
complexity of deep neural nets. Compared to standard Rademacher complexity, the bound
has a higher-order dependence on the depth and width and an additional factor e.
2.	We provide experiments to analyze the relationship between the generalization gap and
the adversarial Rademacher complexity. We show that one of the reasons why adversarial
training cannot generalize well is the large weight norms of an adversarially-trained model.
2	Preliminaries
2.1	Generalization Gap and Rademacher Complexity
Generalization Gap. We start from the classical machine learning framework. Let F be the
hypothesis class (e.g. Linear functions, Neural networks). The goal of the learning problem is to find
f ∈ F to minimize the population risk R(f) = E(χ,y)~D ['(f (x), y)], where D is the true distribution,
'(∙) is the loss function. Since D is unknown, we minimize the empirical risk in practice. Given m
i.i.d samples S = {(x1,y1),…，(xm, ym)}, the empirical risk is Rm(f) = m1 pm=1['(f (xi),yi)].
The generalization gap or generalization error is defined as follow:
Generalization Gap := R(f) - Rm(f).
Rademacher Complexity. A classical measure of the generalization error is Rademacher com-
plexity (Bartlett & Mendelson (2002)). Given the hypothesis class H, the (empirical) Rademacher
complexity is defined as
1m
RS(H)= Eσ- [ sup y^σih(xi,yi)],
m h∈H i=1
where σi are i.i.d Rademacher random variables, i.e. σi equals to 1 or -1 with equal probability.
Define the function class 'f = {'(f (x), y)|f ∈ F}, we have the following generalization bound.
2
Under review as a conference paper at ICLR 2022
Proposition 1. (Mohri et al. (2018); Bartlett & Mendelson (2002)) Suppose that the range of the loss
function `(f (x), y) is [0, C]. Then, for any δ ∈ (0, 1), with probability at least 1 - δ, the following
holds for all f ∈ F,
AogT
V 2m .
R(f) ≤ Rm(f) + 2CRS('f) + 3C
2.2	Robust Generalization Gap and Adversarial Rademacher Complexity
TΓ* 1	/ Z^^	1∙	，♦	Z^^	T , 7/ "/	∖	∖	C/ "/ Z ∖	∖ 1	.1	1	∙	1 1
Robust Generalization Gap. Let '(f (x), y) := maxkχo-χkp≤e '(f (χ0), y) be the adversarial loss.
The adversarial population risk and the adversarial empirical risk are
m
0	10
R(f) = E(χ,y)〜DU max '(f(x),y) and Rm(f) =	max '(f(xjyi),
kx0-xkp≤	m	i=1 kx0-xkp≤	i
respectively. In this paper we consider general `p attacks forp ≥ 1. The robust generalization gap is
defined as follow:
Robust Generalization Gap := R(f) - Rm(f).
Let the adversarial hypothesis class be 'f = {'(f (x), y)|f ∈ F}, according to Proposition 1, We
have the following adversarial generalization bound.
Proposition 2. (Yin et al. (2019)) Suppose that the range of the loss function `(f (x), y) is [0, C].
Then, for any δ ∈ (0, 1), with probability at least 1 - δ, the following holds for all f ∈ F,
点2
~ . ~ , ~
R(f) ≤ Rm(f) + 2CRS('f) + 3C
Binary Classification. We first discuss the binary classification case, then We discuss the extension
to the multi-class classification case in section 5. FolloWing (Yin et al. (2019); AWasthi et al. (2020)),
We assume that the loss function can be Written as `(f (x), y) = φ(yf (x)) Where φ is a non-increasing
function. Then
max '(f(x0),y) = Φ(min yf (x0))∙
x0	x0
Assume that the function φ is Lφ-Lipschitz, by Talagrand’s Lemma (Ledoux & Talagrand (2013)),
1	Zn / 7	∖ /T Zn / -r'∖	1	FC ,ι f	∙ ι t' . ∙	ι
we have RS('f) ≤ LφRs(F), where we define the adversarial function class as
~	- 二 ， . . ,... 一、
F =	{f	： (χ,y) →	ll inf	yf(χ )|f	∈F}∙	⑴
kx-x0kp≤
Adversarial Rademacher Complexity. We define RS(F) as adversarial Rademacher complexity.
Our goal is to give upper bounds for adversarial Rademacher complexity. Then, it induces the
guarantee of the robust generalization gap.
Hypothesis Class. We consider depth-d, width-h fully-connected neural networks,
F = {x → WdP(Wd-IP(…P(WIx)…))，IIWlk≤ Mι,l = 1 …，d},	(2)
where ρ(∙) is an element-wise LP-LiPSChitz activation function, Wl are hl X hl-ι matrices, for
l = 1, ∙∙∙ ,d. We have hd = 1 and h° = q is the dimension of the data x. Let h = max{ho, •一，hd}
be the width of the neural networks. Convolution neural networks are also included in this hypothesis
class because convolution layer can be viewed as a special form of fully-connected layer. Denote the
(a, b)-group norm IW Ia,b as the a-norm of the b-norm of the rows of W. We consider two cases,
Frobenius norm and ∣∣ ∙ ∣∣ι,∞-norm in equation (2). Additionally, we assume that ∣X∣∣p,∞ = B.
2.3	Rademacher Complexity and Covering Number
Covering Number. Our solution for adversarial Rademacher complexity is based on the covering
number. We first provide the definition of covering number.
3
Under review as a conference paper at ICLR 2022
Definition 1 (ε-cover). Let ε > 0 and (V,d(∙, ∙))bea metric space, where d(∙, ∙) is a (Pseudo)-metric.
C ⊂ V is an ε-cover of V, if for any v ∈ V, there exists v0 ∈ C s.t. d(v, v0) ≤ ε. Define the smallest
|C| as ε -covering number of V and denote as N (V,d(∙, ∙),ε).
Next, we define the ε-covering number of a function class F . Given the sample dataset S =
{(x1,y1),…，(xm,ym)} with Xi ∈ Rd, let kfkS = m Pm=If(Xi,yiy be a pseudometric of
F. Define the ε-covering number of F be N(F, k ∙ ∣∣s, ε). Let D be the diameter of F with
D = 2maxf∈F kfkS.
Proposition 3 (Dudley’s integral). The Rademacher complexity R(F) satisfiy
RS(F) ≤
12	/ D/2
√m 0o
√log N (F, k ∙ ∣s ,ε) dε.
The proof of Dudley’s integral can be found in statistic textbooks (e.g. (Wainwright (2019))). Based
on this, we can bound the covering number of the function class F to give an upper bound of the
Rademacher complexity.
3	Related Work
Adversarial Attacks and Defense. Starting from the work of (Szegedy et al. (2013)), it has now
been well known that deep neural networks trained via standard gradient descent based algorithms
are highly susceptible to imperceptible corruptions to the input data (Goodfellow et al. (2014); Chen
et al. (2017); Carlini & Wagner (2017); Madry et al. (2017)). This has led to a series of work aimed at
training neural networks robust to such perturbations (Wu et al. (2020); Gowal et al. (2020); Wu et al.
(2020)) and works aimed at designing more sophisticated attacks to attack the classifiers (Athalye
et al. (2018); Tramer et al. (2020); Chen et al. (2017)).
Adversarial Generalization. The work of (Schmidt et al. (2018); Raghunathan et al. (2019); Zhai
et al. (2019)) have shown that in some scenarios achieving adversarial generalization requires more
data. The work of (Attias et al. (2021); Montasser et al. (2019)) explains generalization in adversarial
settings using VC-dimension. Cullina et al. (2018) studies PAC-learning guarantees in the adversarial
setting via VC-dimension. VC-dimension usually depends on the number of parameters in the model,
while Rademacher complexity usually depends on the weight matrices. Rademacher complexity
usually provides tighter generalization bounds (Bartlett (1998)). Neyshabur et al. (2017b) uses a
pac-bayesian approach to provide a generalization bound for neural networks. Sinha et al. (2017)
study the generalization of an adversarial training algorithm in terms of distributional robustness. The
work of (Xing et al. (2021a;b); Javanmard et al. (2020)) study the generalization properties in the
setting of linear regression. Gaussian mixture models are used to analyze adversarial generalization
(Taheri et al. (2020); Javanmard et al. (2020); Dan et al. (2020)). The work of (Allen-Zhu & Li
(2020)) explains adversarial generalization through the lens of feature purification.
Adversarial Rademacher Complexity. Researchers have analyzed adversarial Rademacher com-
plexity in linear and two-layers neural networks cases. In linear cases, the upper bounds can be
directly derived by definition (Khim & Loh (2018); Yin et al. (2019)). In two-layers neural networks
cases, an upper bound is derived using Massart’s Lemma (Awasthi et al. (2020)). It seems that these
proofs cannot be extended to multi-layers cases. Moreover, based on the definition of adversarial
function class F in equation (1), the candidate functions are not composition functions, but with an
inf operation in front of the neural networks. Then, the induction on layers seems not applicable in
calculating adversarial Rademacher complexity for deep neural networks. The works of (Khim &
Loh (2018)) and (Gao & Wang (2021)) indicate the difficulty of analyzing adversarial Rademacher
complexity. They analyze other variants of adversarial Rademacher complexity of DNNs. The first
one introduce tree transformation, but it overestimates the adversarial loss. The second one considers
fast gradient sign methods (FGSM) adversarial examples, but it also requires additional assumption
on the gradient.
In Appendix B, we provide the details of the above bounds and discuss why these methods seem
not applicable in multi-layers cases. We also provide a comparison of adversarial generalization
in Rademacher complexity framework and other frameworks. In the next section, we provide our
solution of adversarial Rademacher complexity based on covering number and analyze each factor in
the upper bound.
4
Under review as a conference paper at ICLR 2022
4	Our Solution of Adversarial Rademacher Complexity
The folloWing Theorem states an upper bound of adversarial Rademacher complexity in the Frobenius
norm cases.
Theorem 1 (Frobenius Norm Bound). Given the function class F in equation (2) under Frobbe-
nius Norm, and the corresponding adversarial function class F in equation (1). The adversarial
Rademacher complexity of deep neural networks RS(F) satisfies
d
d
一,二、	24	,. ii______
RS(F) ≤ —/= max{1, q2 p }(kXkp,∞
+)Lρd-1t	hlhl-1 log(3d)	Ml.
l=1
l=1
(3)
By assuming that LP = 1, P ≤ 2, ∣∣X∣∣p,∞ = B, and h = max{ho,…，hd}, we have
.~.
Rs (F) ≤O
(B + E)h，dlog(d) Qd=I Ml
√m
(4)
Because of the inf operation of the function in F, We cannot Peel off the layers or calculate the
covering number by induction on layers. Our proof is based on calculating the covering number of F
directly. BeloW We sketch the proof. The completed proof is provided in Appendix A.
〜
〜
Step 1: Diameter of F. We first calculate the diameter of F. We have
d
2max kf∣∣s ≤ 2Ld-1 max{1,q 2 - 1 }(∣X ∣p,∞ + E) Y Ml = D.
f∈F	R
Step 2: Distance to F . Let Cl be δl-covers of {∣∣Wl IIF ≤ Ml}, l = 1, 2,…，d. Let
Fc = {fc : X → W(CP(W(C-ιρ(∙∙∙ ρ(WCx) ∙∙∙)), Wlc ∈Cl,l = 1, 2 ∙∙∙ , d}
... --
and F= {f : (x,y) →	inf	yf(x )∣f ∈ F }.
kx-x0kp≤
〜

For all f ∈ F, We need to find the smallest distance to Fc, i.e. We need to calculate the
..~	~ C ..
max min kf - f ks∙
f∈F fc∈Fc
∀(x,y) ∈ D, given f and f c, let x* = arg inf kχ-χo kp yf (x0) and Xc = arginf kx-χ0kp yfc(x0).
X xc if f(x*) ≥ fc(xc)	,,	,	,	.........
Let Z = ↑ X if f (x*)<fc(xc) and gb(z) = WbP(…Wa+1P(W;…P(WCz)…))). Then
X if f(X ) < f (X )
If(z) - fc(z)l = |g0(z) - gd(z)l ≤ Ig0(z) -gl(z)l + …+ ∣gd-1(z) - gd(z)l ≤ XXDM1.
Let δl = 2Mlε∕dD, l = 1,…，d, we have maxf⅛F minf⅛∈Fc ∣∣∕ — fc∣s ≤ Pd=I 蛛 ≤ ε.
Step 3: Covering Number of F. Then, We can calculate the ε-covermg number N (F, ∣∙ ∣∣s ,ε).
Because Fc is a ε-cover of F. The cardinality of Fc is
N(F,∣H∣s,ε) = I,Fcl ≤ (32dD)Pd
hlhl-1
(5)
Step 4: Integration. By Dudley’s integral, we obtain the bound in Theorem 1.
□
Remark: Step 2 is the critical step in the proof. In words, if we calculate the covering number of the
class of d-layers neural nets directly, we only need to define the optimal adversarial example one
time. Then we can calculate the other things using this adversarial example. In contrast, if we want to
do it layer by layer, the optimal adversarial example will be changed when we add or peel off a layer.
This is why the induction methods fail in adversarial settings.
5
Under review as a conference paper at ICLR 2022
Theorem2 (∣∣∙ ∣∣ι,∞-Norm Bound). Given Ihefunction class F in equation (2) under ∣∣ ∙ ∣∣ι,∞-norm,
and the corresponding adversarial function class F in equation (1). The adversarial Rademacher
complexity of deep neural networks RS (F) satisfies
.~ .
RS (F) ≤
dd
Xhlhl-1log(3d)YMl.
l=1	l=1
(6)
In the case of ∣∣ ∙ ∣∣ι,∞-norm, the bound is similar to the bound in the Frobenius norm case except the
term max{1, q1/2-1/p}. Therefore, for all P ≥ 1, the ∣∣ ∙ ∣∣ι,∞-norm bound have the same order in
equation (4).
Theorem 3 (Lower Bound). Given the function class of DNNs F in equation (2), and the corre-
sponding adversariaIfunction class F in equation (1). Exist sample dataset S, s.t. the adversarial
Rademacher complexity of deep neural networks RS(F) satisfies
Rs(F) ≥ θ().
(7)
The proof of the above Theorem is based on constructing a scalar network and is provided in Appendix
A. The gap between the upper bound and the lower bound is the dependence on depth-d and width-h,
h√d log d. In the next section, we extend the adversarial Rademacher complexity to the Multi-class
classification cases.
5 Margin Bounds for Multi-Class Classification
5.1 Setting for Multi-Class Classification
The setting for multi-class classification follows (Bartlett & Mendelson (2002)). In a K-class
classification problem, let Y = {1,2,…，K}. The functions in the hypothesis class F map X to
RK, the k-th output of f is the score of f(x) assigned to the k-th class.
Define the margin operator M(f(x), y) = [f (x)]y -maxy06=y[f(x)]y0. The function makes a correct
prediction if and only if M (f (x), y) > 0. We consider a particular loss function `(f (x), y) =
φγ (M (f (x), y)), where γ > 0 and φγ : R → [0, 1] is the ramp loss:
φγ(t) = ] 1 -
∣0
t≤0
γ 0 <t < γ
t ≥ γ.
The loss function `(f (x), y) satisfies:
1(y = arg ym∈aK][f (x)]y0) ≤ 队f (X)，y) ≤ 1(f(x)]y ≤ γ+m=x f(x)]y0).	⑻
Define the function class 'f := {(x,y) → φγ(M(f(x),y)) : f ∈ F}. Since φγ(t) ∈ [0,1] and
φγ(∙) is 1/γ-Lipschitz, by combining (8) with Theorem 1, we can obtain the following direct corollary
as the generalization bound in the multi-class classification.
Corollary 1 (Mohri et al. (2018)). Consider the above multi-class classification setting. For any
fixed γ > 0, we have with probability at least 1 - δ, for all f ∈ F,
P(x,y卜D ty = arg yr∈aK][f (x)]y0
1m
≤mm X 1([f (Xi)]yi
i=1
≤ Y + max[f (Xi)]yo) + 2Rs('f) + 3
y06=y
In adversarial training, let Bpx () = {x0 : ∣x0 - x∣p ≤ } and we define the adversarial function
1 ∕Γ	r /	∖	.	八 、，一 -r-T 5τ 1
class 'f := {(x,y) → maXχ0∈Bχ9 '(f (x0),y): f ∈F}. We have
6
Under review as a conference paper at ICLR 2022
Corollary 2 (Yin et al. (2019)). Consider the above adversarial multi-class classification setting.
For any fixed γ > 0, we have with probability at least 1 - δ, for all f ∈ F,
P(x,y)〜D ∃ ∃ x0 ∈ BX(e) s.t. y = arg max [f(x0)]yθ
y0∈[K]
AogT
V 2m .
i=1
∈Bpxi(e)s.t. [f(x0i)]yi ≤γ+
max[f (Xi)]y0) + 2Rs ('f ) + 3
y0 6=y
5.2	Adversarial Rademacher Complexity
Under the multi-class setting, we have the following bound for adversarial Rademacher complexity.
Theorem 4. Given the function class F in equation (2) under Frobbenius Norm, and the correspond-
ing adversarial function class F in equation (1). The adversarial Rademacher complexity ofdeep
neural networks RS ('f) satisfies
RS ⑪ ≤ γ√Km maχ{1,q 1- 1 XkX kp,∞+巧-、
dd
X hlhl-1 log(3d) Y Ml.
l=1	l=1
(9)
The k ∙ ∣∣ι,∞-norm bound is similar, except the term max{1, q1 - 1 }. Below We sketch the proof.
Step L Let Fk = {(x,y) → inf∣w-χk≤e([f(x0)]y - [f(x])]k),f ∈ F}, then RS('f) ≤
KRS('fk). Step 2: By the Lipschitz property of φγ(∙), RS('fk) ≤ YRS(Fk). Step 3: The
calculation of RS (F' k) follows the binary case.
5.3	Comparison of the Bounds
Now, we compare the difference between the bounds for (standard) Rademacher complexity and
adversarial Rademacher complexity. We have shown that
RS('f) ≤ O(Tm")and RS('f) ≤ O((B + e)”Q3 M ),	(10)
where we use the upper bound of RS (F) in (Golowich et al. (2018)).
Algorithm-Independent Factors. In the two bounds, the algorithm-independent factors include
Sample size B, perturbation intensity e, depth-d, and Width-h. To simplify the notations, we let
Cstd = B√d and Cadv = (B + e)h√dlog dbe the constants in standard and adversarial Rademacher
complexity, respectively. We simply have Cadv > Cstd .
Algorithm-Dependent Factors. In the two bounds, the margins γ and the product of the ma-
trix norms Qld=1 kWl k depend on the training algorithms. To simplify the notations, we de-
fine Wstd ：= Qι=ι ∣Wιk∕γ if the training algorithm is standard training. Correspondingly, let
Wadv ：= Qd=IkWιk∕Y if the training algorithm is adversarial training. In the next section, we
conduct experiments to show that Wadv > Wstd .
Notation of generalization gaps. In the next section, we use E(∙) and E(∙) to denote the standard
and robust generalization gap. We use fstd and fadv to denote the standard- and adversarially-trained
model. Our goal is to understand why the robust generalization gap of an adversarial training model
is large, which is quite different from the standard generalization gap of a standard-trained model,
i.e., we want to analyze why E (fadv) > E(fstd). Based on the standard and adversarial Rademacher
complexity bounds, it is suggested that
E(fadv ) H Cadv VWadv and E(fstd) H CstdWstd∙
To analyze the individual effect of factors Cadv and Wadv , we further introduce two kinds of
generalization gaps, the robust generalization gap of a standard-trained model (E(fstd)) and the
standard generalization gap of an adversarially-trained model (E (fadv)). The standard and adversarial
Rademacher complexity suggest that
E (fst ) H Cav Wst	and E (fav ) H CstWav .
7
Under review as a conference paper at ICLR 2022
6 Experiment
As we discuss in the previous section, the product of weight norm Qld=1 kWl k and the margin γ are
algorithm-dependent factors controlling the generalization gap. We provide experiments comparing
the difference between these terms in standard and adversarial settings. Since the bounds also hold
for convolution neural networks, we consider the experiments of training VGG networks (Simonyan
& Zisserman (2014)) on CIFAR-10 (Krizhevsky et al. (2009)). We use the experiments on VGG-19
to illustrate the results. Other experiments are provided in Appendix C.
Figure 1: Product of the Frobenius norm in the experiments on CIFAR-10. The red lines are the results
of standard training. The blue lines are the results of adversarial training. (a): Standard Generalization
gap, the blue line represents E (fadv) and the red line represents E(fstd). (b): Robust Generalization
Gap, the blue line represents E(fadv) and the red line represents E(fstd) (c): Qd=IIlWl IlF of the
neural networks. (d): Qd=ι IlWlllF/γ of the neural networks, the blue line represents Wadv and the
red line represents Wstd .
Training Settings. For both standard and adversarial training, we use the stochastic gradient
descent (SGD) optimizer, along with a learning rate schedule, which is 0.1 over the first 100 epochs,
down to 0.01 over the following 50 epochs, and finally be 0.001 in the last 50 epochs. For adversarial
settings, We adopt the '∞ PGD adversarial training (Madry et al. (2017)). The perturbation intensity
is set to be 8/255. We set the number of steps as 20 and further increase it to 40 in the testing phase.
For the stepsize in the inner maximization, we set it as 2/255. In Corollary 2, we need to use the
optimal adversarial examples to calculate the margin and the robust generalization gap, but it is
unknown in practice. We use the PGD adversarial examples as substitutes.
Calculation of Margins. We adopt the setting in (Neyshabur et al. (2017a)). In standard training,
we set the margin over training set to be 5th-percentile of the margins of the data points in S. i.e.
Prc5{f(xi)[yi] - maxy6=yi f(x)[y]|(xi, yi) ∈ S}. In adversarial settings, we set the margin over
training set to be 5th-percentile of the margins of the PGD-adversarial examples of S. The choice of
5th-percentile is because the training accuracy is 100% in all the experiments. We provide ablation
studies about the percentile in the Appendix C.
Standard and Robust Generalization Gap. In Figure 3 (a) and (b), we plot the standard and
robust generalization gap of both standard-trained and adversarially-trained models. We use the
results using 50000 training samples to discuss the experiments. Firstly, in Figure 3 (a), we can see
that E (fstd) is small (=10.45%). On the other hand, an adversarial-trained model has a larger standard
generalization gap (E (fadv)=26.34%). It is a widely observed phenomenon that adversarial training
hurts standard generalization. One reason is that adversarial training overfits the adversarial examples
and performs worse on the original examples. Secondly, in Figure 3 (b), the robust generalization
gap of a standard-trained model is very small (E (fstd) = 0). It is because the standard-trained model
can easily be attacked on both the training set and test set. Then, the robust training accuracy and
test accuracy are closed to 0%. Therefore, the robust generalization gap is also 0. On the contrary,
E (fadv)=58.90%, i.e. the adversarial generalization is bad. This is also observed in the previous
studies, and we aim to discuss the reasons.
Adversarially-trained Models Have Larger Weight Norms, i.e. Wadv > Wstd. In Figure 3 (c),
we can see that the Qld=1 IWlIF of adversarial training is much larger than the Qld=1 IWl IF of
standard training. In (d), the Qld=1 IWl IF is divided by γ, we can see that the Figures is similar
to the Figures in (c), Wadv is larger than Wstd. One of the reasons why Wadv > Wstd is neural
networks need more capacity to fit the adversarial examples.
8
Under review as a conference paper at ICLR 2022
Table 1: Comparison of the four kinds of generalization Gap introduced in Section 5.3. The
experiments are training VGG-19 on CIFAR-10. Notice that E(fstd)=0% is a degenerated case, with
training error=100%. In the other three cases, the training errors≈0%.
Standard-trained models	Adversarially-trained models
Types of Generalization Gaps	Standard	Robust	Standard	Robust
Training Errors	0%	100%	0%	0.02%
Test Errors	10.45%	100%	26.34%	58.92%
Generalization Gaps	E(fstd)=10.45%	Efstd)=0%	E(fadv)=26.34%	E(fadν )=58.90%
片/ C ∖	i∖. E ♦	-I	. -I	TrEKFY	F	F,,	1'	11 1 ∙	1
E(fstd)=0% is a degenerated case. In Table 1, we show the training and test errors for all kinds
of generalization gaps. We can see that the robust training error for a standard-trained model is equal
to 100%. Since the model does not fit any adversarial examples in the training set, there is nothing to
generalize to the adversarial examples in the test set. The generalization gap becomes meaningless.
And the Rademacher complexity bound E(fstd) ≤ O(Cadv Wstd/√m) becomes a trivial bound. In
the other three cases, the training errors are all ≈0%. The generalization gaps are meaningful. We
aimtoanalyzewhyE(fadv) > E(fstd) by analyzing E (fadv) > E (fadv) > E(fstd).
mi GG J i∙ C	XTT {'	,F 1 ∙ Γ∙Γ∙	1	G∕"	∖	FC/" ∖	ɪɪ τ
The effect of Cadv.	We first compare	the difference between	E(fadv)	and E (fadv).	We can see
that E (fadv) = 58.90% > E(fadv ) = 26.34%. For an adversarially-trained model, the robust
generalization gap is larger than the standard generalization gap. If we use the bounds of adversarial
and standard Rademacher complexity as approximations of the robust and standard generalization
gap, i.e., E(fadv) Y Cadv Wadv and E(fadv) Y CstdWadv, E(fadv) > E(fadv) can be explained by
Cadv > Cstd since Wadv are the same in the two bounds.
The effect of Wadv. Similarly, we compare the difference between E (fadv ) and E(fstd). We
can see that E(fadv) = 26.34% > E(fstd) = 10.45%. This is a widely observed phenomenon
that adversarial training hurts standard generalization. It can also be explained by the Rademacher
bounds. If we use the bounds of standard Rademacher complexity as approximations, i.e., E (fadv ) Y
CstdWadv andE(fstd) Y CstdWstd, E (fadv) > E(fadv) can be explained by Wadv > Wstd.
In summary, we can use a simple formula to explain why E (fadv) > E (fadv) > E(fstd) through the
lens of Rademacher complexity. That is
Cadv Wadv > CstdWadv > CstdWstd .
The difficulty of adversarial generalization comes from two parts, the constant Cadv and the weight
norms Wadv . The first part Cadv is independent of the algorithms. It comes from the minimax
problem of adversarial training itself, and it cannot be avoided. The second part Wadv depends on
the algorithms. Therefore, the product of the weight norms is an important factor for the robust
generalization of adversarial training.
Ablation Studies. We provide other ablation studies in Appendix C. First, we consider different
VGG architecture and give the experiments on VGG-11, 13, and 16. Secondly, We consider different
percentile of the margins of the training dataset. Thirdly, We provide the experiments on ∣∣ ∙ kι,∞-norm.
We can see that the ∣∣ ∙ ∣ι,∞-norm bound has a larger magnitude than the Frobenius norm bound.
Then, We provide the experiments on CIFAR-100. Finally, the large Weight norms suggest adding
a regularization term on the Weights during training. We provide experiments With and Without
Weight decay and see that the one With Weight decay has a smaller generalization gap and a smaller
Qd=IkWI ∣∣f/γ. These experiments suggest the strong relationship between robust generalization
gap and the product of Weight norms.
7 Conclusion
In this paper, we first provide upper bounds for the adversarial Rademacher complexity of deep neural
networks. Then, we experimentally investigate these bounds and show that the product of weight
norms is a key factor explaining why adversarial training cannot generalize well. We think our results
will motivate more theoretical research to understand adversarial training and empirical research to
improve the generalization of adversarial training.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190, 2020.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Idan Attias, Aryeh Kontorovich, and Yishay Mansour. Improved generalization bounds for adversari-
ally robust learning. 2021.
Pranjal Awasthi, Natalie Frank, and Mehryar Mohri. Adversarial learning guarantees for linear
hypotheses and neural networks. In International Conference on Machine Learning, pp. 431-441.
PMLR, 2020.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE transactions on Information Theory,
44(2):525-536, 1998.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26, 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. Pac-learning in the presence of evasion
adversaries. arXiv preprint arXiv:1806.01471, 2018.
Chen Dan, Yuting Wei, and Pradeep Ravikumar. Sharp statistical guaratees for adversarially robust
gaussian classification. In International Conference on Machine Learning, pp. 2345-2355. PMLR,
2020.
Qingyi Gao and Xiao Wang. Theoretical investigation of generalization bounds for adversarial
learning of deep neural networks. Journal of Statistical Theory and Practice, 15(2):1-28, 2021.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg,
and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering
the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint
arXiv:2010.03593, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
10
Under review as a conference paper at ICLR 2022
Adel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani. Precise tradeoffs in adversarial training
for linear regression. In Conference on Learning Theory,pp. 2034-2078. PMLR, 2020.
Justin Khim and Po-Ling Loh. Adversarial risk bounds via function transformation. arXiv preprint
arXiv:1810.09519, 2018.
Marc Khoury and Dylan Hadfield-Menell. On the geometry of adversarial examples. arXiv preprint
arXiv:1811.00525, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
2018.
Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially robustly learnable,
but only improperly. In Conference on Learning Theory, pp. 2512-2530. PMLR, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401. PMLR, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-
ization in deep learning. arXiv preprint arXiv:1706.08947, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017b.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. Adversarial
training can hurt generalization. arXiv preprint arXiv:1906.06032, 2019.
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann. Fixing data augmentation to improve adversarial robustness. arXiv preprint
arXiv:2103.01946, 2021.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances in Neural Information Processing
Systems, pp. 5014-5026, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2, 2017.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. arXiv
preprint arXiv:1710.10766, 2017.
11
Under review as a conference paper at ICLR 2022
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis. Asymptotic behavior of adversarial
training in binary classification. arXiv preprint arXiv:2010.13275, 2020.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. arXiv preprint arXiv:2004.05884, 2020.
Yue Xing, Qifan Song, and Guang Cheng. On the generalization properties of adversarial training. In
International Conference OnArtificial Intelligence and Statistics, pp. 505-513. PMLR, 2021a.
Yue Xing, Ruizhi Zhang, and Guang Cheng. Adversarially robust estimate and risk analysis in linear
regression. In International Conference on Artificial Intelligence and Statistics, pp. 514-522.
PMLR, 2021b.
Dong Yin, Ramchandran Kannan, and Peter Bartlett. Rademacher complexity for adversarially robust
generalization. In International Conference on Machine Learning, pp. 7085-7094. PMLR, 2019.
Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John Hopcroft, and Liwei Wang. Adversarially
robust generalization just requires more unlabeled data. arXiv preprint arXiv:1906.00555, 2019.
12
Under review as a conference paper at ICLR 2022
A Proof of the Theorems
A.1 Proof of Theorem 1
Theorem 1 (Frobenius Norm Bound). Given the function class F in equation (2) under Frobbe-
nius Norm, and the corresponding adversarial function class F in equation (1). The adversarial
Rademacher complexity of deep neural networks RS(F) satisfies
一,二、	24 ,一 ιι_________
RS(F) ≤ —/= max{1, q2 p }(kXkp,∞
d
d
+)Lρd-1t	hlhl-1 log(3d)	Ml.
l=1
l=1
By assuming that LP = 1, P ≤ 2, ∣∣X∣∣p,∞ ≤ B, and h = max{ho,…，hd}, we have
(B + E)h，dlog(d) Qd=I Ml
.~.
Rs (F) ≤O
√m
Because of the inf operation of the function in F, We cannot Peel off the layers or calculate the
covering number by induction on layers. Our proof is based on calculating the covering number of F
directly. Before We provide the proof, We first introduce the folloWing lemma.
Lemma 1 (Covering number of norm-balls). LetB be a `p norm ball with radius W. Let d(x1, x2) =
∣∣xι — x2∣∣p. Define the ε-covering number of B as N (B, d(∙, ∙), ε), we have
N(B,d(∙,∙),ε) ≤ (1 + 2W∕ε)q.
In the case of Frobenius norm ball of m × n matrices, We have the dimension q = m × n and
N (B, ∣∣∙ ∣∣f ,ε) ≤ (1 + 2W∕ε)m×n ≤ (3W∕ε)m×n.
Lemma 2. if 戏 ∈ {xi|kxi - Xikp ≤ }, we have
∣x"∣r* ≤ max{1,q1-r-P}(∣X∣∣p,∞ + e).
Proof: If p ≥ r*, by Holder’s inequality with 1∕r* = 1/p +1∕s,
kx"∣r* ≤ sup k1kskx"∣p = k1ks∣∣x"∣p = qP kχ"∣p=q1-r - P kx"∣p.
Equality holds when all the entries are equal. If p < r*,we have
kx"∣r* ≤kx"lp.
Equality holds when one of the entries of θ equals to one and the others equal to zero. Then
kχ"∣r* ≤ maχ{1,q1-r - P }∣∣χ"∣p
≤max{1,q1-P-P}(∣∣Xikp + Ilxi- x"∣p)
≤ max{1,q1-r - P }(∣∣X∣∣p,∞ + e).
□
Lemma 3. Let A be a m × n matrix and b be a n-dimension vector, we have
kA ∙ b∣2 ≤∣A∣fkb∣2.
Proof: let Ai be the rows of A, i = 1 •…m, we have
m
X∣Ai∣22∣b∣22
i=1
∖
m	___
X ∣Aik2√Γb1 = IAkFkb∣2.
i=1
□
13
Under review as a conference paper at ICLR 2022
Step 1: Diameter of F. We first calculate the diameter of F. ∀f ∈ F, given (Xi, yi), let x* =
inf ∣∣χi-χθ, kp≤ep yf (xi), and We let Xi be the output of x* pass through the first to the l - 1 layer, We
have
.~, ...
lf(χi,yi)1 = | ll inf _ yf(Xi)I
∣xi -xi ∣p ≤p
=∣WdP(Wd-ιχd-1)∣
(i)
≤ kWdkF ∙kρ(Wd-lxd-1)k2
=kWdkF ∙kρ(Wd-lxd-1) - ρ(0)k2
(ii)
≤ LρMdkWd-1(xid-1)k2
≤∙∙∙
d
≤ Lρd-1YMlkW1xi*k2
l=2
d
≤ LdT Y Ml ∙kx:k2
l=1
(iii)	√ ITdr	,	11、，““	、
≤ LPT ∏M1 max{1,q2-p}(∣∣Xkp,∞ + ),
l=1
Where inequality (i) is because of Lemma 3, inequality (ii) is because of the Lipschitz propertiy of
activation function ρ(∙), inequality (iii) is because of Lemma 2. Therefore, we have
1 m	2	d
2maχ kfks = 2( 一 £|『出加12 ) ≤ 2Ld-1 max{1,q2 - P }(∣∣X kp,∞ + e) ɪɪ Ml = D.
f ∈f	mi =	/	国
Step 2: Distance to Fc. Let Cl be δl-covers of {∣∣Wl IIF ≤ Ml}, l = 1, 2,…，d. Let
Fc = {fc : x → W(CP(W之 ιρ(∙∙∙ ρ(WCx) ∙∙∙ )),Wlc ∈Cl,l = 1, 2 …，d}
... --
and F= {f : (x,y) → inf	yf(x )∣f ∈ F }.
∣x-x0∣p≤
For all f ∈ F, We need to find the smallest distance to Fc, i.e. We need to calculate the
..~	~一 ..
max min kf - fcks∙
f∈F fc∈Fc
∀(xi,yi),i = 1, ∙∙∙ , n, given f and fc with ∣∣Wl - WlckF ≤ δl, l = 1, ∙∙∙ , d, consider
.~,	、 ~一 ， ..
If(xi,yi) - f (xi,yi)I
=| inf0 yif(x0i) -	inf0 yifc(x0i)|
∣xi-x0i ∣p	∣xi-x0i ∣p
Let
xi* = arg inf0 yif(x0i), and xic = arg inf0 yifc(x0i),
∣xi-x0i ∣p	∣xi-x0i ∣p
we have
.~, 、 ~一 ， ..
If(xi,yi) - f (xi,yi)I
=|yif(xi*)-yifc(xic)|
=|f(xi*) - fc(xic)|.
Let
xic if f(xi*) ≥ fc(xic)
zi =	xi if f(xi*) < fc(xic)
14
Under review as a conference paper at ICLR 2022
Then,
|f (xi , yi) - fc (xi , yi)|
=If(x：) - fc(χC)l
≤If(Zi)- fc(zi)∣.
Define ga(∙) as
gb⑺=Wbρ(Wb-iρ(∙∙∙ Wa+ιρ(W(C …P(Wcz)…))).
In words, for the layers b ≥ l > a in g((∙), the weight is Wι, for the layers a ≥ l ≥ 1 in g((∙), the
weight is Wlc. Then we have f(zi) = gd0(zi), f(zi) = gdL(zi). We can decompose
|f(zi)-fc(zi)|
=|gd0(zi) - gdd(zi)|
= lg0(zi) - gd (Zi) +----+ gd-' (Zi)- gd (Zix
≤lg0(zi) - gd (Zi)I +	+ lgd-1 (Zi)- gd(zi)h
To bound the gap If(Zi) - f c(Zi)∣, we first calculate IgdT(Zi) - gd(Zi)I for l = 1,…，d.
Igdl-d(Zi) - gdl (Zi)I
=IWdρ(gdl--dd(Zi)) - Wdρ(gdl -d(Zi))I
(i)
≤kWdkFkρ(gdl--dd(Zi))-ρ(gdl-d(Zi))k2
(ii)
≤ LρMdkgd-d(Zi) - gd-d(Zi)k2
(i=ii)LρMdkWd-dρ(gdl--d2(Zi)) -Wd-dρ(gdl-2(Zi))k2
≤∙∙∙
d
≤Lρd-l Y MjkWlρ(gll--dd(Zi)) -Wlcρ(gll--dd(Zi))k2
j=l+d
where (i) is due to Lemma 3, (ii) is due to the bound of IlWLk and the Lipschitz of ρ(∙), (iii) is
because of the definition of gba (Z). Then
Igdl-d(Zi) - gdl (Zi)I
d
≤Lρd-l Y MjkWlρ(gll--dd(Zi))-Wlcρ(gll--dd(Zi))k2
j=l+d
d
=Lρd-l Y Mjk(Wl -Wlc)ρ(gll--dd(Zi)))k2
j=l+d
(i)	d
≤Lρd-l	MjkWl -WlckFkρ(gll--dd(Zi)))k2
j=l+d
(ii)	d
≤ Lρd-l	Mjδlkρ(gll--dd(Zi)))k2,
j=l+d
(12)
15
Under review as a conference paper at ICLR 2022
where inequality (i) is due to Lemma 3, inequality (ii) is due to Lemma 3 the assumption that
kWl - WlckF ≤ δl. Itis lefted to bound kρ(gll--11(zi)))k∞, we have
	kρ(gll--11(zi)))k2 =ρ(gll--11(zi))) - ρ(0)k2 ≤Lρkgll--11(zi))k2 =LρkWlC-1ρ(gll--22(zi)))k2 ≤LρkWlC-1kFkρ(gll--22(zi)))k2	(13) ≤LρMl-1kρ(gll--22(zi)))k2 ≤… l-1 ≤L- Y Mj max{1, q 1-P }(kXkp,∞ + O j=1
combining inequalities (12) and (13), we have
	|gdl-1(zi) - gdl (zi)| Qd M ≤Ld- -A√—δι max{ 1,q2 PXkXkp,∞ + e)	(14) Ml Dδl =	. 2Mι
Therefore, combining inequalities (15) and (14), we have
	|f(zi)-fC(zi)| ≤lg0(zi) - gd(zi )| +	+ lgd-1(zi) - gd (Zi)I d	d	(15) ≤ XX Dδι. 一白 2Ml
Then	max min ∣∣∕ 一 fckS ≤ 7X ^-. f∈F fc∈Fc kf	f kS ≤ = 2Mι
Let δl =	:2Mlε∕dD,l = 1,…，d, we have d Dδl max min kf — fckS ≤		 ≤ ε. f∈F fc∈Fc kf	f kS ≤ ⅛ 2Mι ≤
Step 3: Covering Number of F. We then calculate the ε-covering number N (F, ∣∣∙∣∣s ,ε). Because
ɪ_ .	_ ɪ 一. _ ɪ_ .
FC is a ε-cover of F. The cardinality of FC is
	N (F, k∙kS ,ε) =IFcI = Y ICiI (≤) Y (半)hlhl-1 =(3dD )Pd=ι hlhlτ, δl	2ε l=	l=	l
where inequality (i)) is due to Lemma 1.
Step 4, Integration. By Dudley’s integral, we have
	RS(F) ≤ √m Z	q∕logN(F, k ∙ kS, ε)dε 12 ∕*d∕2	dL^ ≤ √—	t (/J hιhi-1)log(3dD∕20dε	(16) m 0	l=d 12Dλ∕pd=I hihj M Z	 = 	————	 plog(3d∕2ε)dε. m0
16
Under review as a conference paper at ICLR 2022
Integration by part, we have
1/2
,log(3d∕2ε)dε
0
1
2
3√πerfc( plog3d) + Plog 3d
1
≤-
-2
3 √πexp (- plog 3d) + plog3d
(17)
+ ,log 3d
≤1 0Plog3d
= log 3d.
Plug equation (17) to equation (16), we have
,~.
RS (F) ≤
max{1,q1 - 1 }(∣∣X ∣∣
p,∞
ud	d
+ )Ldρ-1 utX hlhl-1 log(3d) Y Ml.
l=1	l=1
□
A.2 Proof of Theorem 2
Theorem2 ((∣∙ ∣∣ι,∞-NormBound). Given thefunCtion class F in equation (1) under ∣ ∙ ∣∣ι,∞-norm,
and the corresponding adversarialfunction class F in equation (2). The adversarial Rademacher
complexity of deep neural networks RS(F) satisfies
.~ .
RS (F) ≤
∣X ∣p,∞
ud	d
+ )Ldρ-1 utXhlhl-1log(3d)YMl.
l=1	l=1
The proof is mimilar to the proof of the Frobenius norm bound. We first introduce the following
inequality.
Lemma 4. Let A be a m × n matrix and b be a n-dimension vector, we have
IlA ∙ bk∞ ≤ IlAl11,∞∣∣bk∞.
Proof: let Ai be the rows of A, i = 1 •…m, we have
∣∣A ∙ b∣∞ = max |Aib| ≤ max ∣∣Aikι∣bk∞ = ∣A∣ι,∞kb∣∞.
□
Step 1: Diameter of F. We first calculate the diameter of F. ∀f ∈ F, given (Xi, yi), let x* =
inf ∣∣χi-χθ. kp≤∈p yf (xi), and We let Xi be the output of x* pass through the first to the l - 1 layer, We
have
17
Under review as a conference paper at ICLR 2022
.~ , ...
lf(χi,yi)1 = | ll	innf _ yf(Xi)I
kxi -xi kp ≤p
=∣WdP(Wd-ιχd-1)∣
(i)
≤ kWdkι,∞ ∙ kρ(Wd-ιχd-1 )k∞
=kWdkF ∙kρ(Wd-lχd-1) - ρ(0)k∞
(ii)
≤ LρMdkWd-1(xid-1)k∞
≤∙∙∙
d
≤ LdT Y Mι∙kx"∞
l=1
(iii)	d
≤ Lρd-1	Ml(kXkp,∞ +),
l=1
where inequality (i) is because of Lemma 4, inequality (ii) is because of the Lipschitz propertiy of
activation function ρ(∙), inequality (iii) is because of Lemma 2. Therefore, We have
11 m	、2	d
2ιmax kfks = 2(mm 并『(")『)≤ 2Ld-1(kXkp,∞ + e) RMl = D.
Step 2: Distance to Fc. Let Ci be δl-covers of {∣∣Wli∣∣ι ≤ Ml}, l = 1,2,…，d, i = 1,…，hl,
Where Wli is the ith roW of Wli . Let
Fc = {fc : X → W(CP(W(C-IP(…P(Wcx)…)),Wlci ∈Ci,i = 1,…，hl, l = 1, 2 …，d}
... --
and F= {f : (x,y) → inf	yf (x )∣f ∈ F }.
kx-x0kp≤
For all f ∈ F, We need to find the smallest distance to Fc, i.e. We need to calculate the
..~ ~一 ..
max min Ilf - fcks∙
f∈F fc∈Fc
∀(xi,yi),i = 1,…,n,given f and fc with ∣Wli - Wlci∣ ≤ δl, i = 1,…,hl, l = 1,…,d,wehave
IWl - WlcI1,∞ ≤ δl. By the same argument as the step 2 of the proof o Theorem 3, We have
..~ ~一..
max min kf - fcks
f∈F fc∈Fc
Let δl = 2Mlε∕dD, l = 1,…，d, we have
..~	~一 ..
maχ min kf 一 fcks ≤
f∈F fc∈Fc
( Dδl
≤ X 2Ml.
l=1 l
X 箸 ≤ ε.
Step 3: Covering Number of F. We then calculate the ε-covering number N (F, k∙ks, ε)∙ Because
Fc is a ε-cover of F. The cardinality of Fc is
N(F,k∙ks,ε) =I-FcI = YYICiI (≤) Y(学产 1-1 =(32D)Pd=ιhιhι-ι,
l=1 i=1	l=1
where inequality (i)) is due to Lemma 1.
18
Under review as a conference paper at ICLR 2022
Step 4, Integration. By the same argument as the step 4 of the proof o Theorem 1, integration by
part, we have
RS(F) ≤ √m(IIXllp,∞ + e)Lp-1 t ^X hιhi-i log(3d) Y Ml.
□
A.3 Proof of Theorem 3
Theorem 3 (Lower Bound). Given the function class F in equation (2), and the coressponding
adversarial function class F in equation (1). Exist sample dataset S, s.t. the adversarial Rademacher
complexity of deep neural networks RS (F) satisfies
,~ .
Rs(F) ≥ Ω
(B +e) Qd=ι Mi
√m
The proof of the above Theorem is based on constructing a scalar network. By the definition of
Rademacher complexity, if H0 is a subset of H, we have
1m	1m
RS(H ) = Eσ	[ sup Xσih(xi,yi)] ≤ Eσ [ SuP Xσih(xi,y/] = RS(H).
m h∈H0 i=1	m h∈H i=1
Therefore, it is enough to lower bound the complexity of F0 in a particular distribution D, where F0
is a subset of F. Let
F = {x → inf	yMd ∙ M2wTx|w ∈ Rq, ∣w∣2 ≤ M1}.
kx0-xkp≤
We first prove that F0 is a subset of F. In F, We let the activation function ρ(∙) be a identity mapping.
Let
W1 =	w 0 . .	∈ Rh1×h0,	Wl =	ΓMι 0 . .	0 ∙ 0 ∙ . .	・	0- ・	0 . .	∈ Rhl×hl-1,
	. 0			. 0	. 0 ∙	. ・ 0	
l = 2, ∙∙∙ ,d.
(18)
Then, We have IWl I ≤ Ml and F With additional constraint in equation (18) reduce to F0. In other
words, F0 is a subset of F.
It turns out that We need to loWer bound the adversarial Rademacher complexity of linear hypothesis.
The results are given by the work of (Yin et al. (2019); Awasthi et al. (2020)). Below we state the
result.
Proposition 4. Given the function class G = {x → ywτ x|w ∈ Rq, ∣∣w∣∣r ≤ W} and G = {x →
inf kx0 -xkr ≤ ywTx|w ∈ Rq, IwIr ≤ W}, the adversarial Rademacher complexity RS(G) satisfies
Rs(G) ≥ max Rs(G),
e max{1, q1-1- P }W
2√m
Since the standard Rademacher complexity
Wm
RS (G ) = 一Eσ k	σiXikr*,
m
i=1
Ietkxik = B with equal entries for i = 1, ∙∙∙ , m,by Lemma 2 we have
,、w	l5m ,	「	i_I」、
RS(G) = 一Eσ |	σi∣ max{1, q1 r p}B.
m
i=1
19
Under review as a conference paper at ICLR 2022
By Khintchine’s inequality, we know that there exists a universal constant c > 0 such that
m
Eσ | ɪ2 σi | ≥ c√m.
i=1
Then, we have
cW	1	1
RS(G) = √= max{1, q1 r P }B.
Therefore,
RS(G) ≥ max RS(F),
e max{1, q1-r-P }W
2√m
1
≥-------
- 1 + 2c
c
≥ ------
- 1 + 2c
Let W = Qld=1 Ml, we have
2c	2c Vemaχ{1,q1- 1- 1 }W
RS(F) + 1+2c X-------2√m--------
(B + e) max{1, q1- 1- 1 }W \
√m
Rs (F) ≥ Ω
max{1,q1-r-P }(B + e) Qd=1 Ml
where r = 2 for frobenius norm bound and r = 1 for ∣∣ ∙ kι,∞-norm bound.
A.4 Proof of Theorem 4
Theorem 4. Given the function class F in equation (2) under Frobbenius Norm, and the correspond-
ing adversarial function class F in equation (1). The adversarial Rademacher complexity of deep
neural networks RS ('f) satisfies
4 二、	48K	, i_1_____
RS('F) ≤ γ√mmax{1,q2 p}(kXkp,∞
dd
X hlhl-1 log(3d) YMl.
l=1	l=1
The (1, ∞)-norm bound is similar, except the term max{1, q2- 1 }.
Proof: Firstly, we have
'(f(x),y)=ll max	φγ(M(f(χ),y))
kx-x0k≤
=φγ(	inf	M (f (x), y))
kx-x0k≤
=φγ(	inf ([f (x)]y - max[]f (x)]y0))
kx-x0k≤	y0 6=y
=φγ(	inf	i0nf([f(x)]y- [f(x)]y0))
kx-x0k≤ y0 6=y
=φγ(i0nf	inf	([f(x)]y- [f(x)]y0)).
y06=y kx-x0k≤
=maxφγ(	inf	([f (x)]y - [f (x)]y0)).
y0 6=y	kx-x0k≤
Define
hk (x, y) =	inf ([f(x)]y - [f(x)]k) +γ1(y = k),
kx-x0k≤
we now prove that
my06=axy φγ(kx-inx0fk≤([f(x)]y - [f(x)]y0)) = mkaxφγ(hk(x,y)).
If
inf
y0 6=y k
inf ([f(x)]y - [f(x)]y0 ) ≤ γ,
x-x0 k≤
〜
□
20
Under review as a conference paper at ICLR 2022
we have
i0nf	inf	([f (x)]y - f (x)]y0)) = inf hk(x, y).
y06=y kx-x0k≤	k
If
inf inf	([f(x)]y - [f(x)]y0) > γ,
y0 6=y kx-x0k≤
we have
φγ(inf	inf	([f(x)]y — [f (x)]y0)) = φγ(inf hk(x, y)) =0.
y0 6=y kx-x0 k≤	k
Therefore, we have
'(f (x),y) = φγ (inf hk (x,y)) = max φγ (hk (χ,y)).
kk
Define Hk = {hk(x, y) = infkx-x0k≤([f(x)]y — f (x)]k) + γ1(y = k)|f ∈ F}, we have
(i)	(ii) K
R('f) ≤ KR(φγ ◦Hk) ≤ — R(Hk),
(19)
where inequality (i) is the Lemma 9.1 of (Mohri et al. (2018)), inequality (ii) is due to the Lipschitz
property of φγ(∙). Now, define fk(x,y) = infkχ-χ0∣∣≤e([f (x)]y — f(x)]k), we have hk(x,y)=
fk (x, y) + γ1(y = k). Define the function class
Fk={fk(x,y)=	inf	([f(x)]y — [f(x)]k)|f ∈ F}.
kx-x0k≤
We have
m
R(Hk)=—Eσ SUP XOihk(xi,yi)
m	hk∈Hk i=1
1m
=一 Eσ SUp X σi fk(x, y) + γ1(y = k)
m	hk∈Hk i 1
i=1
1	m	1m
=一 Eσ SUp X Oifk (x,y) +-Eσ X σiγ1(y = k)
m	hk∈Hk i=1	m i=1
1m
=一Eσ SUp £ σifk(x,y)
m fk∈Fk i=1
=R(Fk)
Finally, we need to bound the Rademacher complexity of R(Fk). Notice that
[f (x)]y — [f (x)]k = (Wy — WdJ )ρ(Wd-i(ρ(∙∙∙ Wι(x) •••))),
and we have kWdy - Wdk kF ≤ 2Ml . By Theorem 3 (the results in binary classification case), we
have
R(Fk) ≤ √41maχ{1,q 1 - 1 }(∣∣Xkp,∞ + e)L『、
l=1
hlhl-1 log(3d)	Ml .	(20)
l=1
m
d
d
Combining inequalities (20) and (19), we obtain that
d
d
RS ⑪ ≤ Y√Km max{1,q 2 - 1 }(kX kp,∞+CLLt
l=1
hlhl-1 log(3d)	Ml.
l=1
□
B	Discussion on Existing Methods for Rademacher Complexity
In this section, we discuss the related work, discuss the existing methods in calculating Rademacher
complexity, and identify the difficulty of analyzing adversarial Rademacher complexity.
21
Under review as a conference paper at ICLR 2022
B.1	Existing Methods
‘Layer Peeling’ Bounds. The main idea of calculating the Rademacher complexity of multi-layers
neural networks is the ‘peeling off’ technique (Neyshabur et al. (2015)). We denote g ◦ F as the
function class {g ◦ f|f ∈ F}. By Talagrand’s Lemma, we have RS(g ◦ f) ≤ LgRS(F). Based
on this property, we can obtain RS(Fl) ≤ 2LρMlRS(Fl-1), where Fl is the function class of
l-layers neural networks. Since the Rdemacher complexity of linear function class is bounded by
O(BMι/√m), We can get the upper bound O(B2dLp-1 QQd=1 Mι∕√m) by induction. We can
remove the Lρ by assuming that Lρ = 1 (e.g. Relu activation function).
Golowich et al. (2018) improves the dependence on depth-d from 2d to √d. The main idea is to
rewrite the Rademacher complexity Eσ [∙] as Eσ exp ln[∙]. Then, we can peel off the layer inside the
ln(∙) function and the 2d now appears inside the ln(∙).
Covering Number Bounds. Bartlett et al. (2017) uses a covering numbers argument to show that
the generalization gap scale as
O( B nd=1kW1k (X kWιk2/3 ʌ 3/2)
I	√m	kw23	,
where ∣∣ ∙ ∣∣ is the spectral norm. The proof is based on the induction on layers. Let Wl be the weight
matrix of the present layer and Xl be the output of X pass through the first to the l - 1 layer. Then,
one can compute the matrix covering number N({WlXl}, ∣ ∙ ∣∣2, e) by induction.
Adversarial Generalization Bounds. Researchers have analyzed adversarial Rademacher com-
plexity in linear and two-layers neural networks cases. In linear cases, the upper bounds can be
directly derived by definition (Khim & Loh (2018); Yin et al. (2019)). In two-layers neural networks
cases, an upper bound is derived using Massart’s Lemma (Awasthi et al. (2020)). These proofs cannot
be extended to multi-layers cases. Moreover, based on the definition of adversarial function class
F in equation (1), the candidate functions are not composition functions, but with an inf operation
in front of the neural networks. Then, the induction on layers seems not applicable in calculating
adversarial Rademacher complexity for deep neural networks. The works of (Khim & Loh (2018))
and (Gao & Wang (2021)) indicate the difficulty of analyzing adversarial Rademacher complexity.
They analyze other variants of adversarial Rademacher complexity of DNNs.
‘Tree Transformation’ Bound. Khim & Loh (2018) introduces a tree transformation T and shows
that maxkχ-χθk≤e '(f (x), y) ≤ '(Tf(x),y). Then, we have the following upper bound for the
adversarial population risk. For δ ∈ (0, 1),
~. . . . .
R(f) ≤ R(Tf) ≤ Rm(Tf) + 2LRs(T OF) + 3
It gives an upper bound of the robust population risk by the empirical risk and the standard Rademacher
complexity of T O f. RS(T O F) can be viewed as an approximation of adversarial Rademacher
complexity. However, the empirical risk Rm(Tf) is not the objective in practice. This analysis does
not provide a guarantee for robust generalization gaps.
FGSM Attack Bound. The work of (Gao & Wang (2021)) tries to provide an upper bound for
adversarial Rademacher complexity. To deal with the max operation in the adversarial loss, they
consider FGSM adversarial examples. Then, the adversarial loss maxkχo-χk≤e '(x, y) becomes
'(f (XFGSM),y). By some assumptions on the gradient, they provide an upper bound for the
Rademacher complexity of '(f (XFGSM), y). However, the bound includes some parameters of the
assumptions on the gradients, and FGSM underestimates the adversarial examples. It is hard to
use this bound to analyze adversarial generalization. Therefore, the existing bounds give limited
interpretations in understanding the generalization of adversarial training.
22
Under review as a conference paper at ICLR 2022
B.2	Why Layer Peeling is not Applicable in Adversarial Setting
We first take a look at the layer peeling technique.
1m
RS (H) = Eσ 一 [ sup X σih(xi)
m h∈H i=1
1m
=Eσ	[ sup	X σiWdρ(h0(xi))
m h0∈Hd-1,kWdk≤Md i=1
≤ MdEσ ɪ
m
sup
h0∈Hd-1
m
σiρ(h0(xi))
i=1
≤ 2MdLpEσ 一
m
m
sup	σih0(xi)
h0∈Hd-1 i=1
2MdLρRS(Hd-1),
In adversarial settings, if we directly apply the layer peeling technique, we have
1m
RS(H)= Eσ— [ sup	σi max	h(x0)^∣
S	σm h∈H i=1 i kxi-x0ik≤	i
m
=Eσ	[ SUp	X σiWdρ(h0(x*(h)))]
m h0∈Hd-1,kWdk≤Md i=1
≤ MdEσ -
m
sup
h0∈Hd-1
m
X σi ρ(h0(χ∙(h)))
i=1
1m
≤ 2MdLpEσ —	sup Xσih0(x*(h))
m h0∈Hd-1 i=1	i
m
=2MdLpEσ —	sup X σih0(x*(h0))
m h0∈Hd-1 i=1
=2MdLpRs (H d-ι),
where Xnh) is the optimal adversarial example given a d-layers neural networks, x*(h0) is the
optimal adversarial example given a d - 1-layers neural networks. x* (h) = x*(h0) is the main
reason why layer peeling cannot be directly extended to the adversarial settings.
This is the main reason why the work we introduce above studied the variants of adversarial
Rademacher complexity. Once they take off the max operation by some approximation (e.g., let
maxkx-x'k≤e '(f (x), y) ≤ '(Tf (x), y)), they don't have the issue x*(h) = x*(h0) and they can
use the layer peeling technique to bound the variants of adversarial Rademacher complexity. The
main drop back is that they change the definition of adversarial Rademacher complexity. These
bounds cannot provide theoretical guarantee on the robust generalization gap.
B.3	Why covering number can help avoiding this issue?
In our opinion, it is hard to modify the procedure of layer peeling such that it is applicable in
adversarial settings. Therefore, we try to bound the adversarial Rademacher complexity in a different
way, using the covering number. In the proof of Theorem 1, we can see that we can avoid the issue of
x*(h) = x*(h0). Specifically, when We calculate the covering number of the whole function class F
directly, we only need to define the optimal adversarial examples xi* for a d-layer neural networks.
We don’t need to consider the optimal adversarial examples of neural networks with fewer layers.
This is the benefit of covering numbers.
23
Under review as a conference paper at ICLR 2022
B.4	Comparison of Different Adversarial Generalization Bounds
VC-Dimension Bounds. A classical approach in statistical learning is to use VC dimension to
bound the generalization gap. It is thus natural to apply the VC-dim framework to adversarial setting,
as (Cullina et al. (2018); Montasser et al. (2019); Attias et al. (2021)) did. However, these works did
not provide a computable bound on the adversarial generalization gap, as explained next. let H be
the hypothesis class (e.g. the set of neural networks with a given architecture).
In the work of (Cullina et al. (2018)), the authors defined adversarial VC-dim (AVC) and gave an
bound on adversarial generalization gap with respect to AV C(H). However, they did not show how
to calculate AVC of neural works. Therefore, their paper did not provide a computable bound for
adversarial generalization gap.
In the work of(Montasser et al. (2019)), the authors defined the adversarial function class as LUH
, where L is the loss and U is the uncertainty set. They bound the adversarial generalization gap
by LUH, which is different from AV C(H) of (Cullina et al. (2018)). However, the authors did not
provide a computable bound of as well, which means that their paper did not provide a computable
bound of the adversarial adversarial generalization gap.
In the work of (Attias et al. (2021)), the authors assume that the perturbation set U(x) is finite, i.e.,
for each sample x, there are only k adversarial examples that can be chosen. They showed that the
adversarial generalization gap can be bounded by
O 1 -12(PkVC(H) log(3 + a)kVC(H)) + log 1).
ε2	2	δ
Note that there is a computable bound of VC(H), which is the number of parameters, thus in terms
of ”computable”, this bound is stronger than the previous two. However, this comes at a price: their
bound depends on k, the number of allowed examined perturbed samples. This is a deviation from the
original notion of adversarial generalization, where U(x) is assumed to be an infinite set (k6= +∞).
In contrast, our bound is for the ”original” adversarial generalization gap, which allows k= +∞.
Adversarial Generalization Bounds in Other Settings. The work of (Xing et al. (2021a;b); Ja-
vanmard et al. (2020)) study the generalization properties in the setting of linear regression. Gaussian
mixture models are used to analyze adversarial generalization (Taheri et al. (2020); Javanmard et al.
(2020); Dan et al. (2020)).
Certified robustness. A series of works study the certified robustness within the norm constraint
around the original data. Cohen et al. (2019) privides an analysis on certified robustness via random
smoothing. Lecuyer et al. (2019) studies certified robustness through the lens of differential privacy.
Other Theoretical Studies on Adversarial Examples. A series of works (Gilmer et al. (2018);
Khoury & Hadfield-Menell (2018)) study the geometry of adversarial examples. The off-manifold
assumption tells us that the adversarial examples leave the underlying data manifold (Szegedy et al.
(2013)). Pixeldefends (Song et al. (2017)) uses a generative model to show that adversarial examples
lie in a low probability region of the data distribution. The work of (Ma et al. (2018)) uses Local
Intrinsic Dimensionality (LID) to argues that the adversarial subspaces are of low probability, and lie
off the data submanifold.
C Additional Experiments
In this section, we provide additional experiments.
C.1 Experiments on VGG-11 and VGG-13
In Figure 2, we show the experiments on VGG-11 and VGG-13. As we can see, the results are
the same as the results in Figure 3, the gap of product of Frobenius norm between standard and
adversarial training is large, which yields bad generalization.
24
Under review as a conference paper at ICLR 2022
20000	30000	40000	50000	10000	20000	30000	40000	50000
numberofsamples	number of samples
(a)	(b)
20000	30000	40000	50000	10000	20000	30000	40000	50000
number of samples	number of samples
(C)	(d)
Generalization Gap	Margin
Product of Frobenius Norm	τen	Product of norms/Margin
(g)
(h)
(e)
(f)
Figure 2: Product of the Frobenius norm in the experiments on VGG networks. The red lines are
the results of standard training. The blue lines are the results of adversarial training. The first row
are the experiments on VGG-11. The second row are the experiments on VGG-13. (a) and (e):
Generalization gap. (b) and (f): Margin γ over training set. (c) and (g): Qld=1 ∣Wl ∣F of the neural
networks. (d) and (h): Qd=ι IlWl IlF/γ of the neural networks.
Standard Generalization Gap	Robust Generalization Gap	ɪeɪʒ ProdUet of FrObeniUS Norm
9'oo>
(c)
lel4	Product of norms/Margin
(d)
(e)
⑴
(g)
(h)
Figure 3: Product of the Frobenius norm in the experiments on CIFAR-10. The red lines are the
results of standard training. The blue lines are the results of adversarial training. The first row is
the experiments on VGG-16. The second row is the experiments on VGG-19. (a) and (e): Standard
Generalization gap. (b) and (f): Robust Generalization Gap. (c) and (g): Qld=1 kWl kF of the neural
networks. (d) and (h): Qd=ι IlWl IlF/γ of the neural networks.
k ∙ k ι,∞-Norm Bounds. The ∣∣ ∙ ∣∣ι,∞-norm bounds are shown in Figure 4. Similar the the Frobenius
norm bounds,The gap of Qld=1 IWl I1,∞ between adversarial training and standard training are large.
But the magnitude of Qld=1 ∣Wl ∣1,∞ is larger than the magnitude of Qld=1 ∣Wl ∣F.
25
Under review as a conference paper at ICLR 2022
(b)
(d)
Figure 4: Product of the ∣∣ ∙ ∣∣ι,∞-Norm in the experiments on CIFAR-10. The red lines are the
results of standard training. The blue lines are the results of adversarial training. (a) Qld=1 kWl k1,∞
of VGG-16networks. (b) Qd=ι IlWl∣∣ι,∞∕γ of VGG-16networks. (c) Qd=ι IlWlllι,∞ of VGG-19
networks. (d) Qd=IIlWll∣ι,∞∕γ of VGG-19 networks.
C.2 Ablation Study of Margins
In Figure 5, we show the results of the margins in 1th, 3th, and, 5th-percentile of the training dataset.
Since the (robust) training accuracy is 100%, the choice of percentile will not affect the results. As
we can see in the Figure, in all the cases, the margins of standard training are larger than the margins
of adversarial training. Since the margins appear in the divider in the upper bound of Rademacher
complexity, the margins of the training dataset have some small effects on the bad generalization of
adversarial training.
C.3 Experiments on CIFAR- 1 00
Perfomance. In Table 2, we show the performance of standard training and adversarial training
on CIFAR-100 using VGG-16 and 19 networks. We can see that using smaller number of training
samples is unable to train an acceptable VGG-networks on CIFAR-100. Therefore it is hard use only
50000 training samples to study the trends of the weight norm using the experiments on CIFAR-100.
We compare the product of weight norm between standard and adversarial training.
Product of Weight Norms. In Figure 6, we show the results of on training VGG-19-16 and VGG-
19 on CIFAR-100. Similar to the experiments on CIFAR-10, we can see that the adversarially trained
models have larger weight norm that that of the standard trained model.
Table 2: Accuracy of standard and adversarial training on CIFAR-100 using VGG-16 and 19 networks.
For standard training model, we shows the clean accuracy. For adversarial training model, we show
the robust accuracy against PGD attacks.
No. of Samples	10000	20000	30000	40000	50000
VGG-16-STD	0.26	0.44	0.54	0.60	0.63
VGG-16-ADV	0.12	0.15	0.17	0.18	0.19
VGG-19-STD	0.32	0.47	0.53	0.58	0.62
VGG-19-ADV	0.12	0.16	0.17	0.19	0.21
C.4 Weight Decay
The upper bounds of adversarial Rademacher complexity suggest adding a regularization term on the
weights to improve generalization, which is essentially weight decay. In Figure 7, we provide the
experiments of adversarial training with and without weight decay. In Figure 7 (a) and (c), we can
see that adversarial training with weight decay has a smaller robust generalization gap. In Figure 7
(b) and (d), adversarial training with weight decay have a smaller product of weight norms. These
experiments show the relationship between the robust generalization gap and the product of weight
norms.
26
Under review as a conference paper at ICLR 2022
(k)
Figure 5: Ablation study of margins. The first to the 4th rows are the experiments on VGG-11, 13,
16, and 19, respectively.
number of sa∣mples
20000	30000	40000	50000	IOOOO 20000	30000	40000	50000
Iiumberofsamples	number of samples
(a)	(b)
IOOOO 20000	30000	40000	50000	10000	20000	30000	40000	50000
number of samples	Iiumberofsamples
(C)	(d)
Generalization Gap	Margin
Product of Frobenius Norm	ɪpɔProduct Of Frabenius NOrmS/Margin
(h)
number of samples
(e)	(f)	(g)
Figure 6: Product of the Frobenius norm in the experiments on VGG networks on CIFAR-100. The
red lines are the results of standard training. The blue lines are the results of adversarial training.
The first row are the experiments on VGG-16. The second row are the experiments on VGG-19. (a)
and (e): Generalization gap. (b) and (f): Margin γ over training set. (c) and (g): Qld=1 kWl kF of the
neural networks. (d) and(h): Q1=1 IlWl IlF/γ of the neural networks.
27
Under review as a conference paper at ICLR 2022
Figure 7: Experiments on the effects of weight decay. (a) Robust generalization gap with or without
weight decay on VGG-16. (b) Frobenius norm with or without weight decay on VGG-16. (c) Robust
generalization gap with or without weight decay on VGG-19. (d) Frobenius norm with or without
weight decay on VGG-19.
D Open Problem
In this section, we list some open problems.
How to bridge the gap between the upper bound and the lower bound? There are two ways:
one way is to show a depth/width-dependent lower bound as the reviewer suggested (increase lower
bound); another way is to show a depth/width-independent upper bound (reduce upper bound). We
briefly discuss which way is possible, and then discuss the technical challenges in both ways.
Which way is more likely to be true? If the upper bound can be improved to be depth-width-
independent (thus matching our lower bound), then fundamentally the lower bound cannot be
improved. Such a possibility exists. Actually, we are more inclined to this possibility, i.e., we tend to
believe it is more promising to reduce the upper bound to be depth-width-independent, rather than
increasing the lower bound. Anyhow, we don’t have strong evidence of this possibility.
Technical challenge on increasing the lower bound (obtain a depth/width-dependent lower bound). In
the current analysis, we construct a class of scalar networks to provide the lower bound. We obtain a
closed-form expression of the adversarial examples. To obtain a depth/width-dependent lower bound,
we need to: i) construct a more general function class of neural networks, and ii) then calculate the
optimal adversarial examples in this class of neural networks. Currently, the challenge lies in the first
step (construction). We have not tried hard to construct the function class so far, and we leave it to
future work.
Can We reduce the upper bound (remove the dependence on depth/width in the UPPer bound)? This
seems quite difficult by the current analysis. More specifically, the dependence h√dlogd is probably
unavoidable by our current approach of calculating the covering number. Despite the technical
difficulty, we suspect that reducing the upper bound is doable by using a new tool other than covering
number and layer peeling. This is surely nontrivial.
Why adversarial training yields larger weight norms? In our opinion, it is because we require
the additional capacity of the neural networks to fit the adversarial examples. As in the discussion of
the work of (Neyshabur et al. (2017a)), we require more capacity of the model to fit random labels.
A model with larger weight norms has a better ability to fit the training data. There might be other
reasons, for example, the loss landscape of the minimax problem of adversarial training, the implicit
bias of PGD attacks, or the implicit bias of adversarial training.
Is the widely used regularization techniques essentially reducing weight norms? In adversarial
training, there are many training tricks to reduce overfitting and yield better generalization, for
example, stochastic weight averaging, early stopping, adversarial weight perturbation, and cyclic
learning. It is an open problem that whether these techniques are related to the weight norms.
How to design better algorithms to improve generalization? Our analysis suggests that adding
regularization to the weight norms could improve generalization. The explicit regularization to
28
Under review as a conference paper at ICLR 2022
control the weight norms is weight decay, which is widely used. How to design implicit control on
the weight norms is an open problem.
29