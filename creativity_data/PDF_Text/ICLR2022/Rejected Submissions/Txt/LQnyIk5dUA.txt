Under review as a conference paper at ICLR 2022
ZeroSARAH: EFFICIENT NONCONVEX FINITE-SUM
Optimization with Zero Full Gradient Compu-
TATIONS
Anonymous authors
Paper under double-blind review
Ab stract
We propose ZeroSARAH—a novel variant of the variance-reduced method
SARAH (Nguyen et al., 2017)—for minimizing the average of a large number of
nonconvex functions 1 PZi fi(x). To the best of our knowledge, in this noncon-
vex finite-sum regime, all existing variance-reduced methods, including SARAH,
SVRG, SAGA and their variants, need to compute the full gradient over all n data
samples at the initial point x0, and then periodically compute the full gradient once
every few iterations (for SVRG, SARAH and their variants). Note that SVRG,
SAGA and their variants typically achieve weaker convergence results than vari-
ants of SARAH: n2/3/2 vs. n1/2/2. Thus we focus on the variant of SARAH.
The proposed ZeroSARAH and its distributed variant D-ZeroSARAH are the first
variance-reduced algorithms which do not require any full gradient computations,
not even for the initial point. Moreover, for both standard and distributed set-
tings, we show that ZeroSARAH and D-ZeroSARAH obtain new state-of-the-art
convergence results, which can improve the previous best-known result (given by
e.g., SPIDER, SARAH, and PAGE) in certain regimes. Avoiding any full gradient
computations (which are time-consuming steps) is important in many applications
as the number of data samples n usually is very large. Especially in the distributed
setting, periodic computation of full gradient over all data samples needs to pe-
riodically synchronize all clients/devices/machines, which may be impossible or
unaffordable. Thus, we expect that ZeroSARAH/D-ZeroSARAH will have a prac-
tical impact in distributed and federated learning where full device participation is
impractical.
1	Introduction
Nonconvex optimization is ubiquitous across many domains of machine learning (Jain & Kar, 2017),
especially in training deep neural networks. In this paper, we consider the nonconvex finite-sum
problems of the form
min If(X) :=1 X fi(x)| ,	⑴
x∈Rd	n
where f : Rd → R is a differentiable and possibly nonconvex function. Problem (1) captures the
standard empirical risk minimization problems in machine learning (Shalev-Shwartz & Ben-David,
2014). There are n data samples and fi denotes the loss associated with i-th data sample. We
assume the functions fi : Rd → R for all i ∈ [n] := {1, 2, . . . , n} are also differentiable and
possibly nonconvex functions.
Beyond the standard/centralized problem (1), we further consider the distributed/federated noncon-
vex problems:
1n	1m
mind f f (x) := nEfi(x)卜 fi(x) := m	fi,j(x),	⑵
x∈R	n i=1	m j=1
1
Under review as a conference paper at ICLR 2022
where n denotes the number of clients/devices/machines, fi denotes the loss associated with m data
samples stored on client i, and all functions are differentiable and can be nonconvex. Avoiding any
full gradient computations is important especially in this distributed setting (2), periodic computation
of full gradient over all data samples needs to periodically synchronize all clients, which may be
impossible or very hard to achieve.
There has been extensive research in designing first-order (gradient-type) methods for solving cen-
tralized/distributed nonconvex problems (1) and (2) such as SGD, SVRG, SAGA, SCSG, SARAH
and their variants, e.g., (Ghadimi & Lan, 2013; Ghadimi et al., 2016; Allen-Zhu & Hazan, 2016;
Reddi et al., 2016; Lei et al., 2017; Li &Li, 2018; Zhou et al., 2018; Fang et al., 2018; Wang et al.,
2018; Ge et al., 2019; Pham et al., 2019; Li, 2019; Li & Richtarik, 2020; Horvath et al., 2020; Li
et al., 2021). Note that SVRG and SAGA variants typically achieve weaker convergence results than
SARAH variants, i.e., n2/3/e2 vs. √n∕e2. Thus the current best convergence results are achieved
by SARAH variants such as SPIDER (Fang et al., 2018), SARAH (Pham et al., 2019) and PAGE (Li
et al., 2021; Li, 2021).
However, all of these variance-reduced algorithms (no matter based on SVRG, SAGA or SARAH)
require full gradient computations (i.e., compute Vf (x) = n P2ι Vfi(χ)) without assuming ad-
ditional assumptions except standard L-smoothness assumptions. We would like to point out that
under an additional bounded variance assumption (e.g., Ei[kVfi(x) - Vf(x)k2] ≤ σ2, ∀x ∈ Rd),
some of them (such as SCSG (Lei et al., 2017), SVRG+ (Li & Li, 2018), PAGE (Li et al., 2021)) may
avoid full gradient computations by using a large minibatch of stochastic gradients instead (usually
the minibatch size is O(σ2∕e2)). Clearly, there exist some drawbacks: i) σ2 usually is not known;
ii) if the target error is very small (defined as E[kVf (xb)k2] ≤ 2 in Definition 1) or σ is very large,
then the minibatch size O(σ2∕e2) is still very large for replacing full gradient computations.
In this paper, we only consider algorithms under the standard L-smoothness assumptions, with-
out assuming any other additional assumptions (such as bounded variance assumption mentioned
above). Thus, all existing variance-reduced methods, including SARAH, SVRG, SAGA and their
variants, need to compute the full gradient over all n data samples at the initial point x0, and then
periodically compute the full gradient once every few iterations (for SVRG, SARAH and their vari-
ants). However, full gradient computations are time-consuming steps in many applications as the
number of data samples n usually is very large. Especially in the distributed setting, periodic com-
putation of full gradient needs to periodically synchronize all clients/devices, which usually is im-
practical. Motivated by this, we focus on designing new algorithms which do not require any full
gradient computations for solving standard and distributed nonconvex problems (1)-(2).
2	Our Contributions
In this paper, we propose the first variance-reduced algorithm ZeroSARAH (and also its dis-
tributed variant D-ZeroSARAH) without computing any full gradients for solving both standard
and distributed nonconvex finite-sum problems (1)-(2). Moreover, ZeroSARAH and Distributed
D-ZeroSARAH can obtain new state-of-the-art convergence results which improve previous best-
known results (given by e.g., SPIDER, SARAH and PAGE) in certain regimes (see Tables 1-2 for
the comparison with previous algorithms). ZeroSARAH is formally described in Algorithm 2, which
is a variant of SARAH (Nguyen et al., 2017). See Section 4 for more details and comparisons
between ZeroSARAH and SARAH. Then, D-ZeroSARAH is formally described in Algorithm 3 of
Section 5, which is a distributed variant of our ZeroSARAH.
Now, we highlight the following results achieved by ZeroSARAH and D-ZeroSARAH:
•	ZeroSARAH and D-ZeroSARAH are the first variance-reduced algorithms which do not require
any full gradient computations, not even for the initial point (see Algorithms 2-3 or Tables 1-2).
Avoiding any full gradient computations is important in many applications as the number of data
samples n usually is very large. Especially in the distributed setting, periodic computation of full
gradient over all data samples stored in all clients/devices may be impossible or very hard to achieve.
We expect that ZeroSARAH/D-ZeroSARAH will have a practical impact in distributed and federated
learning where full device participation is impractical.
•	Moreover, ZeroSARAH can recover the previous best-known convergence result O(n + VnLδ0 )
(see Table 1 or Corollary 1), and also provide new state-of-the-art convergence results without any
2
Under review as a conference paper at ICLR 2022
Table 1: Stochastic gradient complexity for finding an -approximate solution of nonconvex prob-
lems (1), under Assumption 1
Algorithms	Stochastic gradient complexity			Full gradient computation
GD (Nesterov, 2004)	O( nL∆0)			Computed for every iteration
SVRG (Reddi et al., 2016; Allen-Zhu & Hazan, 2016), SCSG (Lei et al., 2017), SVRG+ (Li & Li, 2018)	O	n + n*LA'		Computed for the initial point and periodically computed for every l iterations
SNVRG (Zhou et al., 2018), Geom-SARAH (Horvath et al., 2020)	Oe	(n + √¾δ0 )		Computed for the initial point and periodically computed for every l iterations
SPIDER (Fang et al., 2018), SpiderBoost (Wang et al., 2018), SARAH (Pham et al., 2019), SSRGD (Li, 2019), PAGE (Li et al., 2021)	O	(n + √nL∆0 )		Computed for the initial point and periodically computed for every l iterations
ZeroSARAH (this paper, Corollary 1)	O	(n + √¾δ0 )		Only computed once for the initial point1
ZeroSARAH (this paper, Corollary 2)	。（	√n(L∆0+G0)		Never computed 2
1 In Corollary 1, ZeroSARAH only computes the full gradient Vf (x0) = 1 Pn=I Vfi(x0)
once for the initial point x0, i.e., minibatch size bo = n, and then bk ≡ √n for all iterations
k ≥ 1 in Algorithm 2.
2 In Corollary 2, ZeroSARAH never computes full gradients, i.e., minibatch size bk ≡ √n for
all iterations k ≥ 0.
full gradient computations (see Table 1 or Corollary 2) which can improve the previous best result
in certain regimes.
•	Besides, for the distributed nonconvex setting (2), the distributed D-ZeroSARAH (Algorithm 3) en-
joys similar benefits as our ZeroSARAH, i.e., D-ZeroSARAH does not need to periodically synchro-
nize all n clients to compute any full gradients, and also provides new state-of-the-art convergence
results. See Table 2 and Section 5 for more details.
•	Finally, the experimental results in Section 6 show that ZeroSARAH is slightly better than the
previous state-of-the-art SARAH. However, we should point out that ZeroSARAH does not compute
any full gradients while SARAH needs to periodically compute the full gradients for every l iter-
ations (here l = √n). Thus the experiments validate our theoretical results (can be slightly better
than SARAH (see Table 1)) and confirm the practical superiority of ZeroSARAH (avoid any full gra-
dient computations). Similar experimental results of D-ZeroSARAH for the distributed setting are
provided in Appendix A.2.
3 Preliminaries
Notation: Let [n] denote the set {1,2,…，n} and ∣∣ ∙ ∣∣ denote the Euclidean norm for a vector
and the spectral norm for a matrix. Let hu, vi denote the inner product of two vectors u and v. We
use O(∙) and Ω(∙) to hide the absolute constant, and O(∙) to hide the logarithmic factor. We will
write ∆o := f (x0) - f*, f* := min>∈Rd f(x), G° := 1 P乙 ∣∣Vfi(x0)k2, ∆0 := f (x0) - f*,
fb := n Pn=ι m5∈Rd fi(x) and G0 :=焉 PnjmIJ |尸兀(x0)∣2.
3
Under review as a conference paper at ICLR 2022
Table 2: Stochastic gradient complexity for finding an -approximate solution of distributed non-
convex problems (2), under Assumption 2
Algorithms	Stochastic gradient complexity		Full gradient computation
DC-GD 1 (Khaled & Richtarik, 2020; Li & Richtarik, 2020)	O( mL∆∆0)		Computed for every iteration
D-SARAH 2 (Cen et al., 2020)	O (m + √m log2mLA0 )		Computed for the initial point and periodically computed across all n clients
D-GET 2 (Sun et al., 2020)	O (m +呼今)		Computed for the initial point and periodically computed across all n clients
SCAFFOLD 3 (Karimireddy et al., 2020)	O (m + nm3 堂	)	Only computed once for the initial point
DC-LSVRG/DC-SAGA 1 (Li & RichtOrik, 2020)	O (m+m2/3 堂	)	Computed for the initial point and periodically computed across all n clients
FedPAGE 3 (Zhao et al., 2021)	O (m +恭堂)		Computed for the initial point and periodically computed across all n clients
(Distributed) SARAH/SPIDER/SSRGD 4 (Nguyen et al., 2017; Fang et al., 2018; Li, 2019)	O (m + P堂		Computed for the initial point and periodically computed across all n clients
D-ZeroSARAH (this paper, Corollary 3)	O (m + Pm堂)		Only computed once for the initial point
D-ZeroSARAH (this paper, Corollary 4)	O (Pf lδο+g0 )		Never computed
1 Distributed compressed methods. Here we translate their results to this distributed setting (2).
2 Decentralized methods. Here we translate their results to this distributed setting (2).
3 Federated local methods. Here we translate their results to this distributed setting (2).
4 Distributed version of previous SARAH-type methods (see e.g., Algorithm 4 in Appendix A.2).
Definition 1 A point xb is called an -approximate solution for nonconvex problems (1) and (2) if
E[kVf(b)k2] ≤ e2.
To show the convergence results, we assume the following standard smoothness assumption for
nonconvex problems (1).
Assumption 1 (L-smoothness) A function fi : Rd → R is L-smooth if ∃L > 0, such that
kVfi(x) - Vfi(y)k ≤Lkx-yk, ∀x,y∈Rd.	(3)
It is easy to see that f (x) = * PZi fi(x) is also L-smooth under Assumption 1. We can also
relax Assumption 1 by defining Li -smoothness for each fi . Then if we further define the average
L2 := * pn=ι L2, We know that f (x) = ɪ P*=i f (x) is also L-smooth. Here We use the same L
just for simple representation.
For the distributed nonconvex problems (2), we use the following Assumption 2 instead of Assump-
tion 1. Similarly, we can also relax it by defining Li,j-smoothness for different fi,j . Here we use
the same L just for simple representation.
Assumption 2 (L-smoothness) A function fi,j : Rd → R is L-smooth if ∃L > 0, such that
kVfi,j(x)-Vfi,j(y)k ≤Lkx-yk,	∀x,y∈Rd.	(4)
4
Under review as a conference paper at ICLR 2022
Algorithm 1 SARAH (Nguyen et al., 2017; Pham et al., 2019)
Input: initial point x0, epoch length l, stepsize η, minibatch size b
0
1: x = x0
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
for s = 0, 1, 2, . . . do
x0 = xe
n
v0 = Vf (x0) = 1 P Vfi (x0)	// compute the full gradient once for every l iterations
i=1
x1 = x0 - ηv0
for k = 1, 2, . . . , l do
Randomly sample a minibatch data samples Ib with |Ib | = b
Vk = b P (Vfi(Xk) -Vfi(XkT)) + VkT
i∈Ib
Xk+1 = Xk - ηVk
end for
Xe randomly chosen from {Xk}k∈[l] or Xe = Xl+1
end for
Algorithm 2 SARAH without full gradient computations (ZeroSARAH)
Input: initial point X0, stepsize {ηk}, minibatch size {bk}, parameter {λk}
1:	X-1 = X0
2:	V-1 = 0, y-1 = y—1 =…=y-1 = 0	// no full gradient computation
3:	for k = 0, 1, 2, . . . do
4:	Randomly sample a minibatch data samples Ibk with |Ibk | = bk
1n
P (Vfi(XkT)-yk-1) + n P
i∈Ib	i∈Ibk	j=1
// no full gradient computations for vks
6:	Xk+1 = Xk - ηkVk
7:	yk	Vfi(Xk)	for	i∈Ibk
i	yik-1	for	i	∈/	Ibk
// the update of {yik} directly follows from the stochastic gradients computed in Line 5
8:	end for * 2
4 ZeroSARAH Algorithm and Its Convergence Results
In this section, we consider the standard/centralized nonconvex problems (1). The distributed setting
(2) is considered in the following Section 5.
4.1 ZeroSARAH algorithm
We first describe the proposed ZeroSARAH in Algorithm 2, which is a variant of SARAH (Nguyen
et al., 2017). To better compare with SARAH and ZeroSARAH, we also recall the original SARAH
in Algorithm 1.
Now, we highlight some points for the difference between SARAH and our ZeroSARAH:
• SARAH requires the full gradient computations for every epoch (see Line 4 of Algorithm 1).
However, ZeroSARAH combines the past gradient estimator Vk-1 with another estimator to avoid
periodically computing the full gradient. See the difference between Line 8 of Algorithm 1 and Line
5 of Algorithm 2 (also highlighted with blue color).
• The gradient estimator Vk in ZeroSARAH (Line 5 of Algorithm 2) does not require more stochastic
gradient computations compared with Vk in SARAH (Line 8 of Algorithm 1) if the minibatch size
bk = b.
• The new gradient estimator Vk of ZeroSARAH also leads to simpler algorithmic structure, i.e.,
single-loop in ZeroSARAH vs. double-loop in SARAH.
5
Under review as a conference paper at ICLR 2022
• Moreover, the difference of gradient estimator vk also leads to different results in expectation, i.e.,
1) for SARAH: Ek [vk - Vf (xk)] = VkT - Nf(Xk-1); 2) for ZeroSARAH: Ek[vk - Vf (xk)]=
(1 - λk)(vk-1 -Vf(xk-1)).
4.2 Convergence results for ZeroSARAH
Now, we present the main convergence theorem (Theorem 1) of ZeroSARAH (Algorithm 2) for solv-
ing nonconvex finite-sum problems (1). Subsequently, we formulate two corollaries which present
the detailed convergence results by specifying the choice of parameters. In particular, we list the
results of these two Corollaries 1-2 in Table 1 for comparing with convergence results of previous
works.
Theorem 1 Suppose that Assumption 1 holds. Choose stepsize ηk ≤
l(i+" f0rany k ≥ 0，
where Mk+ι := ʌ~~2------+ 8%+1n . Moreover let λ0 = 1, γo ≥ 舞 and a0 ≥ 2n^η0. Then the
λk+1bk+1	bk+1	2λ1	b1
following equation holds for ZeroSARAH (Algorithm 2) for solving problem (1), for any iteration
K ≥ 0:
E[kVf(xbK)k2] ≤
2∆o	+ (n - bo)(4γo + 2αobo)Go
^K^K—1	L v-''K—1
k=0 ηk	nb0	k=0 ηk
(5)
Remark: Note that we can upper bound both terms on the right-hand side of (5). It means that there
is no convergence neighborhood of ZeroSARAH and hence, ZeroSARAH can find an -approximate
solution for any > 0.
In the following, we provide two detailed convergence results in Corollaries 1 and 2 by specifying
two kinds of parameter settings. Note that the algorithm computes full gradient in iteration k if the
minibatch bk = n. Our convergence results show that without computing any full gradients actually
does not hurt the convergence performance of algorithms (see Table 1).
In particular, we note that the second term of (5) will be deleted ifwe choose minibatch size b0 = n
for the initial point x0 (see Corollary 1 for more details). Here Corollary 1 only needs to compute the
full gradient once for the initialization, and does not compute any full gradients later (i.e., bk ≡ √n
for all k > 0).
Also note that even if we choose b0 < n, we can also upper bound the second term of (5). It means
that ZeroSARAH can find an -approximate solution without computing any full gradients even for
the initial point, i.e., minibatch size bk < n for all iterations k ≥ 0. For instance, we choose
bk ≡ √n for all k ≥ 0 in Corollary 2 , i.e., ZeroSARAH never computes any full gradients even for
the initial point.
Corollary 1 Suppose that Assumption 1 holds.
(1+⅛L for any k ≥ 0，
Choose stepsize ηk ≤
minibatch size bk ≡ √n and parameter λk = bn for any k ≥ 1. Moreover let bο = n and λo = 1.
Then ZeroSARAH (Algorithm 2) can find an -approximate solution for problem (1) such that
E[kVf(xbK )k2] ≤2
and the number of stochastic gradient computations can be bounded by
#grad:= IX bk ≤ n + 2(1 + √8√lδ°^ = O Q + √n丝
k=0
Remark: In Corollary 1, ZeroSARAH only computes the full gradient Vf (x0) = n Pn=ι Vfi(X0)
once for the initial point x0, i.e., minibatch size b0 = n, and then bk ≡ √n for all iterations k ≥ 1
in Algorithm 2.
In the following Corollary 2, we show that ZeroSARAH without computing any full gradients even
for the initial point does not hurt its convergence performance.
6
Under review as a conference paper at ICLR 2022
Corollary 2 Suppose that Assumption 1 holds.
(1+√8)L for any k ≥ 0，
Choose stepsize ηk ≤
minibatch size bk ≡ √n for any k ≥ 0，and parameter λ0 = 1 and λk 二联 for any k ≥ 1. Then
ZeroSARAH (Algorithm 2) can find an -approximate solution for problem (1) such that
E[kVf(bK)k2] ≤ e2
and the number of stochastic gradient computations can be bounded by
#grad = O
JnLX + Go)
2
一,,，，/›	τ τ ，	—	∕›	，”公	∙ r	,1	，	7 ,1	,	1
Note that G0 can be bounded by G0 ≤ 2L∆0 via L-smoothness Assumption 1， then we also have
√	√	√ ( Vn(LXo + lδ 0)、
#grad = OI -----N-------.
Remark: In Corollary 2, ZeroSARAH never computes any full gradients even for the initial point,
i.e., minibatch size bk ≡ √n for all iterations k ≥ 0 in Algorithm 2. If We consider L, ∆o, Go or
∆o as constant values then the stochastic gradient complexity in Corollary 2 is #grad = O(咨),
i.e., full gradient computations do not appear in ZeroSARAH (Algorithm 2) and the term ‘n’ also
does not appear in its convergence result. Also note that the parameter settings (i.e., {ηk}, {bk} and
{λk} in Algorithm 2) of Corollaries 1 and 2 are exactly the same except for bo = n (in Corollary 1)
and bo = √n (in Corollary 2). Moreover, the parameter settings (i.e., {ηk}, {bk} and {λk}) for
Corollaries 1 and 2 only require the values of L and n, Which is the same as all previous algorithms.
If one further alloWs other values, e.g., , Go or ∆o, for setting the initial bo, then the gradient
complexity can be further improved (see Appendix D for more details).
5 D-ZeroSARAH Algorithm and Its Convergence Results
Now, we consider the distributed nonconvex problems (2), i.e., mi%∈Rd {f (x) := 1 pn=1 fi(x)}
with fi(x) := m Pj=I fi,j(x), where n denotes the number of clients/devices/machines, fi denotes
the loss associated with m data samples stored on client i.
5.1	D-ZeroSARAH algorithm
To solve distributed nonconvex problems (2), we propose a distributed variant of ZeroSARAH (called
D-ZeroSARAH) and describe it in Algorithm 3. Same as our ZeroSARAH, D-ZeroSARAH also does
not need to compute any full gradients at all. Avoiding any full gradient computations is important
especially in this distributed setting, periodic computation of full gradient across all n clients may be
impossible or unaffordable. Thus, we expect the proposed D-ZeroSARAH (Algorithm 3) will have a
practical impact in distributed and federated learning where full device participation is impractical.
5.2	Convergence results for D-ZeroSARAH
Similar to ZeroSARAH in Section 4.2, we also first present the main convergence theorem (The-
orem 2) of D-ZeroSARAH (Algorithm 3) for solving distributed nonconvex problems (2). Subse-
quently, we formulate two corollaries which present the detailed convergence results by specifying
the choice of parameters. In particular, we list the results of these two Corollaries 3-4 in Table 2
for comparing with convergence results of previous works. Note that here we use the smoothness
Assumption 2 instead of Assumption 1 for this distributed setting (2).
Theorem 2 Suppose that Assumption 2 holds. Choose stepsize ηk ≤
L(1+√W+1) forany k ≥0'
where Wk+1 ：=入巾二U∣ + 争4. Moreover' Iet λο = 1 and θo :=硒矍氏 + 4nj笫0b0.
k+1sk+1 k+1	sk+1bk+1	(nm-1) 1	s1b1
Then the following equation holds for D-ZeroSARAH (Algorithm 3) for solving distributed problem
(2)' for any iteration K ≥ 0:
E[kVf(xbK)k2] ≤
(nm - sobo )ηoθoG0o
nmsobo PkK=-o1 ηk
(6)
7
Under review as a conference paper at ICLR 2022
Algorithm 3 Distributed ZeroSARAH (D-ZeroSARAH)
Input: initial point x0, parameters {ηk }, {sk }, {bk }, {λk }
1:
2:
3:
4:
5:
6:
7:
vx-1 == 0x, y1-1 = y2-1
for k = 0, 1, 2, . . . do
yn-1 = 0	// no full gradient computation
Randomly sample a subset of clients Sk from n clients with size |Sk | = sk
for each client i ∈ Sk do
Sample the data minibatch Ibk (with size |Ibk | = bk ) from the m data samples in client i
Compute its local minibatch gradient information:
8:
9:
end for
vk
10:
11:
12:
bk P Wij (xk), gk,prev = bk P Vfij (XkT), % =看 P yj
j∈Ibki	j∈Ibki	j∈Ibki
Vfij (Xk)	fOr	j ∈	Iki yk	= ɪ	P	yk.
yk- 1	for	j∈Iki' yi mj=ι	yij
Sk P (gkcurr
i∈Sk
gk,prev) + (I- λk )vk-1 + λk+ P (gk,prev - yk,prev) + λk yk-1
i∈Sk
// no full gradient computations for vks
xk+1 = xk - ηk vk
n
yk = n Eyk	H here yk = yk-1 for client i / Sk
i=1
end for
Corollary 3 Suppose that Assumption 2 holds.
Choose stepsize ηk ≤
(1+√8L for any k ≥ 0，
clients subset size Sk ≡ √n，minibatch size bk ≡ √m and parameter λk = 2^ for any k ≥ 1.
Moreover， let s0 = n， b0 = m， and λ0 = 1. Then D-ZeroSARAH (Algorithm 3) can find an
-approximate solution for distributed problem (2) such that
—
E[kVf(XbK)k2] ≤2
and the number of stochastic gradient computations for each client can be bounded by
#grad
Corollary 4 Suppose that Assumption 2 holds. Choose stepsize ηk ≤(1+%工 for any k ≥ 0，
clients subset size Sk ≡ √n and minibatch size bk ≡ √m for any k ≥ 0，and parameter λo = 1 and
λk = Inmk for any k ≥ 1. Then D-ZeroSARAH (Algorithm 3) Canfindane-approximate SolUtionfor
distributed problem (2) such that
E[kVf(XbK)k2] ≤2
and the number of stochastic gradient computations for each client can be bounded by
#gra
L∆0 + G00
e2
Remark: Similar discussions and remarks of Theorem 1 and Corollaries 1-2 for ZeroSARAH in
Section 4.2 also hold for the results of D-ZeroSARAH (i.e., Theorem 2 and Corollaries 3T).
6	Experiments
Now, we present the numerical experiments for comparing the performance of our ZeroSARAH/D-
ZeroSARAH with previous algorithms. In the experiments, we consider the nonconvex robust linear
regression and binary classification with two-layer neural networks, which are used in (Wang et al.,
2018; Zhao et al., 2010; Tran-Dinh et al., 2019). All datasets used in our experiments are down-
loaded from LIBSVM (Chang & Lin, 2011). The detailed description of these objective functions
and datasets are provided in Appendix A.1.
8
Under review as a conference paper at ICLR 2022
SARAH
ZeraSARAH
» ∞
n n
*grad∕π
Pynm (StePSlZe c = 0.01)
abalone (stepsize η = l)
='sUA= E⅛c 3U∙,-PB9
mg (s⅛psaeo=O.OT)
T- SARAH
ZeroSARAH
SARAH
ZeroSARAH
mg (stepsize η = l)
triazines (stepsize n = l)
SARAH
ZeroSARAH
pyrim (stepsize n=0.1)
< < < <
≡s6EOU ⅞u=s∂
→- SARAH
ZeroSARAH
I
=∙sutE⅛c 3U∙,-PB9
Figure 1: Performance between SARAH and ZeroSARAH under different datasets (columns) with
respect to different stepsizes (rows). In rows, we have stepsizes 0.01, 0.1, 1, respectively. In
columns, we have LIBSVM datasates ‘abalone’, ‘triazines’, ‘mg’, ‘pyrim’, respectively.
In Figure 1, the x-axis and y-axis represent the number of stochastic gradient computations and
the norm of gradient, respectively. The numerical results presented in Figure 1 are conducted on
different datasets with different stepsizes. Regrading the parameter settings, we directly use the
theoretical values according to the theorems or corollaries of SARAH and ZeroSARAH, i.e., We do
not tune the parameters. Concretely, for SARAH (Algorithm 1), the epoch length l = √n and the
minibatch size b = √n (see Theorem 6 in Pham et al. (2019)). For ZeroSARAH (Algorithm 2),
the minibatch size bk ≡ √n for any k ≥ 0, λo = 1 and λk = 霁 ≡ 2√n for any k ≥ 1
(see our Corollary 2). Note that there is no epoch length l for ZeroSARAH since it is a loopless
(single-loop) algorithm While SARAH requires l for setting the length of its inner-loop (see Line 6
of Algorithm 1). For the stepsize η, both SARAH and ZeroSARAH adopt the same constant stepsize
n = O(L). However the smooth parameter L is not known in the experiments, thus here we use
three stepsizes, i.e., η = 0.01, 0.1, 1.
Remark: The experimental results validate our theoretical convergence results (our ZeroSARAH can
be slightly better than SARAH (see Table 1)) and confirm the practical superiority of ZeroSARAH
(avoid any full gradient computations). To demonstrate the full gradient computations in Figure 1,
we point out that each circle marker in the curve of SARAH (blue curves) denotes a full gradient
computation in SARAH. We emphasize that our ZeroSARAH never computes any full gradients. Note
that in this section we only present the experiments for the standard/centralized setting (1). Similar
experiments in the distributed setting (2) are provided in Appendix A.2, e.g., Figure 2 demonstrates
similar performance between distributed SARAH and distributed ZeroSARAH.
7	Conclusion
In this paper, we propose ZeroSARAH and its distributed variant D-ZeroSARAH algorithms for solv-
ing both standard and distributed nonconvex finite-sum problems (1) and (2). In particular, they are
the first variance-reduced algorithms which do not require any full gradient computations, not even
for the initial point. Moreover, our new algorithms can achieve better theoretical results than previ-
ous state-of-the-art results in certain regimes. While the numerical performance of our algorithms
is also comparable/better than previous state-of-the-art algorithms, the main advantage of our algo-
rithms is that they do not need to compute any full gradients. This characteristic can lead to practical
significance of our algorithms since periodic computation of full gradient over all data samples from
all clients usually is impractical and unaffordable.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International Conference on Machine Learning, pp. 699-707, 2016.
Shicong Cen, Huishuai Zhang, Yuejie Chi, Wei Chen, and Tie-Yan Liu. Convergence of distributed
stochastic variance reduced methods without sampling extra data. IEEE Transactions on Signal
Processing, 68:3976-3989, 2020.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM
transactions on intelligent systems and technology (TIST), 2(3):1-27, 2011.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. In Advances in Neural Informa-
tion Processing Systems, pp. 687-697, 2018.
Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang. Stabilized SVRG: Simple variance reduction
for nonconvex optimization. In Conference on Learning Theory, pp. 1394-1448, 2019.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Samuel Horvath, LihUa Lei, Peter Richtarik, and Michael I Jordan. Adaptivity of stochastic gradient
methods for nonconvex optimization. arXiv preprint arXiv:2002.05359, 2020.
Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. Foundations
and TrendsR in Machine Learning, 10(3-4):142-336, 2017.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning.
In International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Ahmed Khaled and Peter Richtarik. Better theory for SGD in the nonconvex world. arXiv preprint
arXiv:2002.03329, 2020.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
SCSG methods. In Advances in Neural Information Processing Systems, pp. 2345-2355, 2017.
Zhize Li. SSRGD: Simple stochastic recursive gradient descent for escaping saddle points. In
Advances in Neural Information Processing Systems, pp. 1521-1531, 2019.
Zhize Li. A short note of PAGE: Optimal convergence rates for nonconvex optimization. arXiv
preprint arXiv:2106.09663, 2021.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 5569-5579, 2018.
Zhize Li and Peter Richtarik. A unified analysis of stochastic gradient methods for nonconvex
federated optimization. arXiv preprint arXiv:2006.07013, 2020.
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik. PAGE: A simple and optimal prob-
abilistic gradient estimator for nonconvex optimization. In International Conference on Machine
Learning, pp. 6286-6295. PMLR, arXiv:2008.10898, 2021.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer, 2004.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for ma-
chine learning problems using stochastic recursive gradient. In International Conference on Ma-
chine Learning, pp. 2613-2621, 2017.
10
Under review as a conference paper at ICLR 2022
Nhan H Pham, Lam M Nguyen, Dzung T Phan, and Quoc Tran-Dinh. ProxSARAH: An effi-
cient algorithmic framework for stochastic composite nonconvex optimization. arXiv preprint
arXiv:1902.05679, 2019.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314-
323, 2016.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from theory to algo-
rithms. Cambridge University Press, 2014.
Haoran Sun, Songtao Lu, and Mingyi Hong. Improving the sample and communication complexity
for decentralized non-convex optimization: Joint gradient estimation and tracking. In Interna-
tional Conference on Machine Learning, pp. 9217-9228. PMLR, 2020.
Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradi-
ent descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920,
2019.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. SpiderBoost and momentum:
Faster stochastic variance reduction algorithms. arXiv preprint arXiv:1810.10690, 2018.
Haoyu Zhao, Zhize Li, and Peter Richtarik. FedPAGE: A fast local stochastic gradient method for
communication-efficient federated learning. arXiv preprint arXiv:2108.04755, 2021.
Lei Zhao, Musa Mammadov, and John Yearwood. From convex to nonconvex: a loss function anal-
ysis for binary classification. In 2010 IEEE International Conference on Data Mining Workshops,
pp. 1281-1288. IEEE, 2010.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 3925-3936, 2018.
11
Under review as a conference paper at ICLR 2022
A Extra Experiments
In this appendix, we first describe the details of the objective functions and datasets used in our
experiments in Appendix A.1. Then in Appendix A.2, we present the experimental results in the
distributed setting (2).
A. 1 Objective functions and datasets in experiments
The nonconvex robust linear regression problem (used in Wang et al. (2018)) is:
min (f (x) := 1 XX "xTai))，
(7)
2
where the nonconvex loss function '(χ) := log(X + 1).
The binary classification with two-layer neural networks (used in Zhao et al. (2010); Tran-Dinh et al.
(2019)) is:
min I f (x) ：= 1 X ' (aTx,bi) + λI∣xk2 |	(8)
x∈Rd	n	2
where {ai} ∈ Rd, bi ∈ {-1,1}, λ ≥ 0 is an '2-regularization parameter, and the function ' is
defined as
'(χ,y) = (1- 1 + exP(-xy)).
All datasets are downloaded from LIBSVM (Chang & Lin, 2011). The summary of datasets infor-
mation is provided in the following Table 3.
Table 3: Metadata of datasets
Dataset	n (# of datapoints)	d (# of features)
a9a	32561	123
abalone	4177	8
mg -	1385	6
mushrooms	8Γ24	∏2
phishing	ΓΓ055	68
pyrim	74	27
triazines	186	60
w8a	49749	300
A.2 Experiments for the distributed setting
Before presenting the experimental results in the distributed setting (2), we also need a distributed
variant of SARAH-type methods in order to compare with our distributed variant of ZeroSARAH.
Here we describe one possible version in Algorithm 4. Note that distributed SARAH also requires
to periodically computes full gradients (see Line 7 of Algorithm 4), but it is not required by our
D-ZeroSARAH (Algorithm 3).
In order to mimic distributed setup, we represented clients as parallel processes. We implement the
training process using Python 3.8.8, mpi4py library. We run it on the workstation with 48 Cores,
Intel(R) Xeon(R) Gold 6246 CPU @ 3.30GHz. We partition the dataset among 10 threads; having
M datapoints and n clients, k-thread gets datapoints in range k [MC +1,..., (k + 1) [MC. In case
of n[MC + 1 ≤ M, datapoints n[MC +1,...M are ignored.
12
Under review as a conference paper at ICLR 2022
Algorithm 4 Distributed SARAH-type methods (one possible version)
Input: initial point x0, epoch length l, stepsize η, client minibatch size s, data minibatch size b
1:	x-1 = x0
2:	for k = 0, 1, 2, . . . do
3:	if k mod l = 0 then
4:	for each client i ∈ {1, . . . , n} do
m
5:	Compute full gradient of each client: gk =今 P Nfij (χk)	// = Vfi(Xk)
j=1
6:	end for
n
7:	Vk = — P gk	// full gradient computations
i=1
8:	else
9:	Randomly sample a subset of clients Sk from n clients with size |Sk| = s
10:	for each client i ∈ Sk do
11:	Sample minibatch Iik of size |Iik | = b (from the m data samples in client i)
12:	Compute the local minibatch gradient information:
gk,curr = b P Vfij (Xk)	and	gk,prev = b P Vfij (XkT)
j∈Iik	j∈Iik
13:	end for
14:	Vk = 1 P (gk,Curr- gk,prev) + VkT
i∈Sk
15:	end if
16:	Xk+1 = Xk - ηkVk
17:	end for
Dhishinq
mushrooms
2000
4000 6000 8∞0 IOOOO
Iteration
-×- Zero-SARAH
-A- Zens-SARAH
-*- Zens-SARAH
-⅝- SARAH
-*∙- SARAH
SARAH
-×- Zeπ>-SARAH
-4- Zeπ>-SARAH
-*- ZeiV-SARAH
-×- SARAH
-⅛- SARAH
-∙*- SARAH
v/Ba
.* ZeiU-SARAH
-A- Zeπ>-SARAH
-⅛- ZeiV-SARAH
-×- SARAH
-⅛- SARAH
Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH
a9a
Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH
2000 4000 6000 8000 10000
Iteration
mushrooms
4	10	12
# epochs
■ ■
=3=
phishiπq
=3=
Zera-SARAH
Zens-SARAH
Zero-SARAH
SARAH
SARAH
SARAH
10	15	20	25	30
# epochs
400
Figure 2: Performance between distributed SARAH and distributed ZeroSARAH under different
datasets (columns). We show convergence using the theoretical stepsize in red lines; in blue lines,
we scale it by factor ×3; in green lines, we scale it by factor ×9.
In Figure 2, we present numerical results for the distributed setting. The solid lines and dashed lines
denote the distributed ZeroSARAH and distributed SARAH, respectively. Regrading the parameter
settings, we also use the theoretical values according to the theorems or corollaries. In particular,
from Tran-Dinh et al. (2019), we know that the smoothness constant L ≈ 0.15405 maxi kaik2 + λ
for objective function (8). We choose the regularizer parameter to be λ = 0.15405∙10-6 maxi |旧『.
In order to obtain comparable plots similar to the standard/centralized setting (Figure 1), we also
use multiple stepsizes. We choose the theoretical stepsize from Corollary 4 (i.e.,([+%工)scaled
by factors of ×1 (red curves), ×3 (blue curves), ×9 (green curves), respectively.
Remark: Similar to Figure 1, the experimental results in Figure 2 also validate our theoretical
convergence results (distributed ZeroSARAH (D-ZeroSARAH) can be slightly better than distributed
SARAH (see Table 2)) and confirm the practical superiority of D-ZeroSARAH (avoid any full gradi-
ent computations) for the distributed setting (2).
13
Under review as a conference paper at ICLR 2022
B	Missing Proofs for ZeroSARAH
In this appendix, we provide the missing proofs for the standard nonconvex setting (1). Concretely,
We provide the detailed proofs for Theorem 1 and Corollaries 1-2 of ZeroSARAH in Section 4.
B.1 Proof of Theorem 1
First, We need a useful lemma in Li et al. (2021) Which describes the relation betWeen the function
values after and before a gradient descent step.
Lemma 1 (Li et al. (2021)) Suppose that function f is L-smooth and let xk+1 := xk - ηkvk. Then
for any v k ∈ Rd and ηk > 0, we have
f(xk+1) ≤ f(Xk) - η2kkVf(xk)k2 -(*-2)kxk+1 - xkk2 + η2kkvk - Vf(Xk)k2.	(9)
Then, We provide the folloWing Lemma 2 to bound the last variance term of (9).
Lemma 2 Suppose that Assumption 1 holds. The gradient estimator vk is defined in Line 5 of
Algorithm 2, then we have
2L2
Ek[kvk - Vf (χk)k2] ≤ (1 - λk)2∣IvkT- Vf(XkT)k2 + — IlXk - XkTll2
bk
2λ2 1 n
+ -bλk- X IVfj(XkT)-yk-1k2.	(10)
bk n
j=1
Proof of Lemma 2. First, according to the gradient estimator vk of ZeroSARAH (see Line 5 of
Algorithm 2), We knoW that
n
vk =彳 X (Vfi(Xk) - Vfi(XkT)) + (1 - λk)vk-1 + λk (r X (Vfi(XkT)- yk-1) + - X yk-1)
b	bn
k i∈Ibk	k i∈Ibk	j=1
(11)
NoW We bound the variance as folloWs:
Ek[Ivk - Vf(Xk)I2]
(=11) Ek
Ek
Ek
b- X (Vfi(Xk) - Vfi(Xk-1)) + (1- λk)vk-1
k i∈Ibk
+ λk(b- X (Vfi(XkT)- yk-1) + - Xyk-1) - Vf(Xk)『]
k i∈Ibk	n j=1
ɪ X (Vfi(Xk) - Vfi(Xk-1)) + Vf (Xk-1) - Vf (Xk) + (1 - λk)(vk-1 - Vf (Xk-1))
bk	k
i∈Ibk
+ λk(b- X (Vfi(XkT)- yk-1) + - Xyk-1 - Vf(XkT)
k i∈Ibk	j=1
b- X (Vfi(Xk) - Vfi(Xk-1)) + Vf (Xk-1) - Vf (Xk)
k i∈Ibk
2
+ λk( J X (Vfi(XkT)- yk-1) + - Xyk-1 - Vf(XkT)
k i∈Ibk	j=1
+ (1 - λk)2Ivk-1 - Vf(Xk-1)I2
2
14
Under review as a conference paper at ICLR 2022
≤ 2Ek b1 X(Vfi(Xk)-Vfi(XkT)) + Vf(XkT)-Vf(Xk)l∣2
k i∈Ibk
+ 2Ek λ J X(Vfa-I +1Xyk-1-vf(XkT)∣∣2]
bk i∈Ibk	n j=1
+ (1 - λk)2kvk-1 - Vf(Xk-1)k2
2L2	2λ2 1 n
≤ rkXk -XkTk2 + -λkI X kVfj(XkT)- yk-1k2 + (1 - λk)2kvk-1 - Vf(XkT)k2,
k	k	j=1
(12)
where (12) uses the L-smoothness Assumption 1 and the fact that E[kX - EXk2] ≤ E[kXk2], for any
random variable X.
To deal with the last term of (10), we use the following Lemma 3.
Lemma 3 Suppose that Assumption 1 holds. The update of {yik} is defined in Line 7 of Algorithm 2,
then we have, for ∀βk > 0,
1n
Ek - EkVfj(Xk) - yk k2
1 j=1
≤ (1 - bk)(1 + βk) 1X kVfj(XkT)- yk-1k2
j=1
+ (1 - -)(1 + ∣)L2kXk-XkTk2.
1	βk
(13)
Proof of Lemma 3. According to the update of {yik} (see Line 7 of Algorithm 2), we have
1n
Ek - EkVfj(Xk) - yk k2
1
j=1
=(1 - bk) 1X kVfj (Xk) - yk-1k2	(14)
=(1 - bk) 1 X kVfj(Xk) - Vfj (XkT) + Vfj (XkT)- yk-1k2
j=1
≤ (1 - bk)(1 + βk) 1 X kVfj(XkT)- yk-1k2 + (1 - bk)(1 + β)L2kXk -XkTk2, (15)
j=1	k
where (14) uses the update of {yjk} (see Line 7 of Algorithm 2), and (15) uses Young’s inequality
and L-smoothness Assumption 1.
Now We combine Lemmas 1-3 (i.e., (9), (10) and (13)) to prove Theorem 1.
Proof of Theorem 1. First, we take expectation to obtain
E f (Xk+1) - f * + (Yk- η-k) kvk - Vf (Xk)k2 + (*-L) ∣∣Xk+1 - Xkk2 + ɑk 1 X ∣∣Vfj(XkIy k2
≤ E f(Xk) - f* - η-kkVf(Xk)k2 + γk(1 - λk)2kvk-1 - Vf(Xk-1)k2
+ 半kXk - Xk-1k2 + 宇1 X kVfj(XkT)- yk-1k2
bk	bk 1
j=1
n
+ αk(1----k)(1 + 不)刀2口/ - XkTk2 + αk(1----k)(1 + βk)— X kVf (Xk-1) - ykTk2
1	βk	1	1	j
15
Under review as a conference paper at ICLR 2022
E f (χk) - f * -与∣Nf (Xk)k2 + γk(1- λk)2∣IvkT- W(XkT)II2
+ (21；L + Qk(I — -)(1 + 7Γ)L2'} kxk - XkTk2
' bk	n	Pk
+ ("4 + Qk(1 - 号)(1 + Pk)) n X l∣Vf7-(XkT)- yk-1 Il2 .	(16)
k	j=ι	.
Now we choose appropriate parameters. Let Yk = 瑞1 and Yk ≤ 7k-ι, then Yk (1 一 λk)2 ≤ γk-ι 一
ηk-1 . Let Pk = 2bn, αk = 2n 'bnk-1 and Qk ≤ αk-1, we have ^^-k + αk (1 - bk )(1 + Pk ) ≤ αk-1.
We also have 2γkL	+ Qk(1	- 运)(1	+ ɪ)L2 ≤	------L by	further letting stepsize
bk	八 n/' βk '	- 2ηk-ι	2	‘
1
ηk-1 ≤ L(1 + √M),
(17)
where Mk :=熹 + 8λ⅛n2.
入 kbk	%
Summing UP (16) from k = 1 to K - 1, we get
一	K-1
0 ≤ E f (x1) - f* - X ηIlVf(Xk)k2 + 71(1 - λ1)2kv0 - Vf(X0)I2
,	k = 1
+ (竽 + Q1(1-n )(1 + ∣n )L2)∣X1-X0k2
+(半+Q1(1 - b1 )(1+2n))1X kvfj (x0) - yjk2.	(18)
For k = 0, we directly uses (9), i.e.,
E[f (x1) - f*] ≤ Ef (x0) - f* - η0IlVf(x0)∣2 -(*-L) ∣x1 - X0I2 + η0Iv0 - Vf(x0)∣2].
(19)
Now, we combine (18) and (19) to get
-k—1	一
E X ηkIVf(xk)I2
_ k=0	.
≤ E[f(x0) - f* + (7ι(1 - λι)2 + η0)Iv0 - Vf(x0)I2
+ (T+Q1(1 - b1 )(1+*))1X IVfj(x0)-姆『]	(20)
1	j=i
≤ E[f (x0)	- f * + η0(I- U1-"I)) Iv0	- Vf (x0)I2 +	与型 1	X IVfj (x0)	- y0I2]
L	2"1	b-∣ n	J
1	j = 1
(21)
1 n
≤ E[f (x0) - f * + γ0Iv0 - Vf (x0)I2 + Q0- X IVfj(x0) - y0I2]	(22)
≤ f (x0) - f* + Y0(；]：)% 1X IVfj(x0)I2 + Q0(1 - Ib) 1X IVfj(x0)I2	(23)
=f (x0) - f* + (γ0 (nn--1b)0bo + Q0(1 - bn0)) 1 X IVfj(x0)I2,	(24)
16
Under review as a conference paper at ICLR 2022
where (20) follows from the definition of no in (17), (21) uses γι =翁 and αι = 2n；V0 ,(22)
holds by choosing γo ≥ 2¾- ≥ η0(f-λI)) and α0 ≥ 2nλ^, and (23) uses λo = L By
randomly choosing xbK from {xk}kK=-01 with probability ηk/ PtK=-01 ηt for xk, (24) turns to
E[kVf(bK)k2] ≤
2(f(x0)- f *),
LK-I	+
k=0 ηk
2	n - b0
PK=-⅛ U0 (n-1)b0
+αo(I- T)) n X kvfj(XO) k2
j =1
2(f(x0)- f *)
v-^K-1
k=0 ηk
(n - b0)(4γ0 + 2α0b0)1 X kVfj(X0)k2.
nb0 PkK=-01 ηk
(25)
j=1
≤
+

B.2 Proofs of Corollaries 1 and 2
Now, we prove the detailed convergence results in Corollaries 1-2 with specific parameter settings.
Proof of Corollary 1. First we know that (25) with b0 = n turns to
E[kVf(bκ)k2] ≤ 2(Pxl-- f*)
k=0 ηk
(26)
Then if We set λk =然 and bk ≡ √n for any k ≥ 1, then We know that Mk := χ2^ + 8λknb ≡ 8
and thus the stepsize ηk ≤
into (26), we have
______1______
L ( 1 + √ Mk + 1 )
≡ (i+⅛for anyk ≥0. By plugging n ≤ (1+W
E[kVf(bK)k2]≤ 2(1 + ^Ly(x0)-f*) = e2,
K
where the last equality holds by letting the number of iterations K = 2(1+λ78)Lf (X )-f ). Thus the
number of stochastic gradient computations is
K-1
#grad =	bk
k=0
K-1
b0+	bk
k=1
n + (K — 1)√n ≤ n +
2(1 + √8)√nL(f (x0) - f *)
i2

Proof of Corollary 2. First we recall (25) here:
E[kVf(xbK)k2] ≤
2(f(x0) - f*) + (n - b0)(¾-+ 2α0b0)1 X kVfj(x0)k2.
■K^K — 1
k=0 ηk
nb0 PkK=-01 ηk
(27)
j =1
In this corollary, we do not compute any full gradients even for the initial point. We set the minibatch
size bk ≡ √n for any k ≥ 0. So we need consider the second term of (27) since b∣ = √n is
not equal to n. Similar to Corollary 1, if we set λk
Mk := λ2r- + 8λk⅛n2 ≡ 8 and thus the stepsize nk ≤ -
λk bk	bk
2k for any k ≥ 1, then we know that
the second term, we recall that γo ≥ 翁 =(1+√∣)L and a∣ ≥
L(l + √Mk+ι) ≡ (1 + √8)L
for any k ≥ 0. For
2nλιηo
see that γo ≥ α∣b∣ since b∣ = √n ≤ n. Now, we can change (27) to
b- = (i+√8)L√n.
It is easy to
E[kVf(xbK)k2] ≤
2(1 + √8)L(f (x0) — f*) + 6(n -√n) 1 X kVf(x0)k2
nK
j =1
K
1
1
1
≤ 2(1 +√8)L(f(x0) — f*) + 6Go
(28)
K
2,
17
Under review as a conference paper at ICLR 2022
where (28) is due to the definition Go := n P2ι ∣∣Vfi(χ0)k2, and the last equality holds by letting
the number of iterations K = 2(1+√¾Lfx )-f )+6G。. Thus the number of stochastic gradient
computations is
-Λ	V-1 1	厂&	√-2(I	+	V8>)L(f (x0) -	f*)	+ 6Go	c∕√n(Lδ0 +	Go八
#grad = Tbk = VZnK = √n--------------------------------= O ( ------------卜
k=o
Note that Go can be bounded by Go ≤ 2L(f (x0) — f *) via L-SmoothneSS Assumption 1, then We
have
√ a √ (√n(Lδo + lδo)∖
#grad = OI --------N------.
Note that	∆o	:=	f(χo)	一	f*,	where	f*	:= minx f(χ),	and	∆o	:=	f(χo)	—	f*,	where	fb	:=
n Pn=I minx fi(X).	□
C Missing Proofs for D-ZeroSARAH
In this appendix, we provide the missing proofs for the distributed nonconvex setting (2). Concretely,
we provide the detailed proofs for Theorem 2 and Corollaries 3-4 of D-ZeroSARAH in Section 5.
C.1 Proof of Theorem 2
Similar to Appendix B.1, we first recall the lemma in Li et al. (2021) which describes the change of
function value after a gradient update step.
Lemma 1 (Li et al. (2021)) Suppose that function f is L-smooth and let xk+1 := xk 一 ηkvk. Then
for any v k ∈ Rd and ηk > 0, we have
f (xk+1) ≤ f(xk) 一 η2k ∣Vf (xk )∣2 —(* 一 L) kχk+1 一 Xk k2 + η2k ∣vk 一 Vf (xk )k2. (29)
Then, we provide the following Lemma 4 to bound the last variance term of (29).
Lemma 4 Suppose that Assumption 2 holds. The gradient estimator vk is defined in Line 9 of
Algorithm 3, then we have
2L2
Ek[∣vk 一 Vf(Xk)k2] ≤ (1 一 λk)2kvk-1 - Vf(XkT)II2 + FIIXk - XkTll2
skbk
λ2	n,m
+ 瑞嬴 X kVfij(XkT)-yk,j1k2∙	(30)
i,j=1,1
Proof of Lemma 4. First, according to the gradient estimator vk of D-ZeroSARAH (see Line 9 of
Algorithm 3), we know that
Vk = - X (gkcurτ - gkprev) + (I- λk)VkT + λk — X (gk,prev - yk,prev) + λkUkT	(31)
sk i∈Sk	sk i∈Sk
Now we bound the variance as follows:
Ek[Ivk
(=31) Ek
Ek
一 Vf(Xk)I2]
一 X (gkcurτ - gk,prev) + (I- λk)VkT + λk (一 X (gk,prev - yf,prev) + yk-1) -Vf (Xk)11
sk i∈Sk	sk i∈Sk
ɪ X (gk,curr - gk,prev) + Vf(XkT)- Vf(Xk) + (1 一 λk)(VkT- Vf(XkT))
sk i∈Sk
18
Under review as a conference paper at ICLR 2022
- yik,prev	+ yk-1
-Vf(XkT
≤ 2Ek
-X (gk,curr - gk,prev) + Vf(XkT)- Vf (xk)
sk i∈Sk
+ λk (~ X (gk,prev - yk,prev) + yk - 1 - Vf(XkT)) ||
+ (1 - λk)2kvk-1 -Vf(Xk-1)k2
士 XX (Vfij (Xk)-Vfi,j(Xk-1))+Vf(Xk-1) -Vf(Xk)2
skbk i∈Skj∈Ibki
+ 2Ek λk sɪ- X X (Vfij(XkT)-ykj1) + y
skbk
k-1 - Vf(xk-1)2
i∈Sk j∈Ibki
+ (1 - λk)2kvk-1 - Vf(xk-1)k2
2	2	n,m
≤ 丁kXk -XkTk2 + ~f — X kVfi,j(XkT)- yj1k2 + (1- λk)2kvk-1 - Vf(XkT)k2,
sk bk	sk bk nm
i,j=1,1
(32)
where (32) uses the L-smoothness Assumption 2, i.e., kVfi,j (X) - Vfi,j(y)k ≤ LkX - yk, and the
fact that E[kX - EXk2] ≤ E[kXk2] for any random variable X.
To deal with the last term in (30), we uses the following Lemma 5.
Lemma 5 Suppose that Assumption 2 holds. The update of {yik,j } is defined in Line 7 of Algo-
rithm 3, then we have, for ∀βk > 0,
n,m
Ek	nm X	kVfi,j(Xk)-yk,jk2
i,j=1,1
n,m
≤ (1 - skbk )(1 + βk )ɪ X kVfi,j (XkT)-yj1k2
nm	nm	i,j
i,j=1,1
+ (1 -也)(1 + -1 )L2kXk-XkTk2.	(33)
nm βk
Proof of Lemma 5. According to the update of {yik,j} (see Line 7 and Line 11 of Algorithm 3), we
have
n,m
n,m
Ek nm X kVfi,j(Xk)-yk,jk2
i,j=1,1
n,m
(1 -吟』X kVfij(Xk) - yk,-1k2
i,j=1,1
n,m
(1 -鬻)—X kVfij(Xk)-Vfi,j(XkT) + Vfij(XkT)-yj1k2
nm nm
i,j=1,1
(34)
≤ (1 -心)(1 + βk )ɪ X kVfij (Xk-I)- yj1k2 + (1 -也)(1 + ɪ )L2kXk -XkTk2,
nm	nm	i,j	nm	βk
i,j=1,1	k
(35)
where (34) uses the update of {yik,j } in Algorithm 3, and (35) uses Young’s inequality and L-
smoothness Assumption 2.
Now we combine Lemmas 1, 4 and 5 (i.e., (29), (30) and (33)) to prove Theorem 2.
19
Under review as a conference paper at ICLR 2022
Proof of Theorem 2. First, we take expectation to obtain
E
f (xk+1) - f * + (Yk-号)∖∖vk - W(Xk )k2 + (圭-2 )∖∖xk+1 - xk k2
Qk---
nm
n,m
X ∖Vfi,j(Xk)-%k∕∣2
≤ E f(xk) - f* -号∖Vf(xk)∖2 + Yk(1 - λk)2∖vk-1 - Vf(XkT)I∣2
；Xk-Xlk 2 +
Skbk nm
n,m
X ∖Vfi,j (XkT)-yjl∖2
i,j=i,i
+ αk(1---k-k)(1 + 不)刀2||/ - XkTk2
nm	βk
n,m
+ αk(1----)(1 + Bk )	X HVfi;j (XkI)- ykj1k2
nm	nm 乙』	ij
2,j=l,l
E f(Xk) - f* - η2k∖Vf(Xk)∖2 + Yk(1 - λk)2∖vk-1 - Vf(XkT)k2
+ (2⅛ + Qk”警)(1 + B R2)kXk-XkTk2
+ (2S⅛+Qk(I-陪)(1+Bk))nm X kVfi，j(XkT)-yj1k
(36)
Now we choose appropriate parameters. Let Yk = η⅛z1 and Yk ≤ Yk-1, then γk (1 - λk)2 ≤ Yk-I -
2人k
ηk⅛T1. Let Bk = fnbk, Qk = 2nm点然-1 and Qk ≤ Qk-1,wehave 2γk∣k + Qk(I- sn⅛ )(1+ Bk ) ≤
k k
Qk-i. Wealso have 2⅛f + Qk(I -嚅)(1 + 看)L2 ≤ 2⅛r - L byfurtherging StePSiZe
1
ηk-1 ≤ L(1 + √Wk),
(37)
where Wk ：= ⅛ + 8⅛产
Summing UP (36) from k = 1 to K - 1, we get
■	K-1
f(x1) - f* - X η2k∖Vf(Xk)k2 + Y1(1 - λι)2kv0 - Vf (X0)k2
k = 1
+ (笺+Ql (1 -曹)(1 + ≡ )L2)kX = X0k2
+ (2s⅛k+QI(I-粤)(1 + 2nb1))nm X kVfi，j(x0)-y0,jk2
2,j=l,l
(38)
For k = 0, we directly uses (29), i.e.,
E[f(x1) - f*] ≤ E[f(x0) - f* - η20∖Vf(X0)k2 -(*-2)∖x1 - X0k2 + η20∖v0 - Vf(X0)k2]∙
(39)
Now, we combine (38) and (39) to get
-K-1
E X η2kkVf(Xk)k2
k=0
0 ≤ E
+
1
i,j=i,i
2Yk λ 1
20
Under review as a conference paper at ICLR 2022
≤ E[f(x0)- f* +(Yi(1 — λι)2 + η20)kv0 - Vf(x0)k2
+
n,m
+ αl(1-----a )(1 + ^~)))------ X kvfi,j (XO) - y0jk2i	(4O)
nm 2nm nm	,
i,j =1,1
≤E f(x0)-f*+
η0(1 - λ1(1 - λ1))
2nmλ1η0 1
2λ1
n,m
kv0 -Vf(x0)k2
s2b1
nm
X	kVfi,j(x0)-yi0,jk2
(41)
≤ f (χ0) — f* +
η0
i,j=1,1
nm - s0b0	1
2λ1 (nm - 1)s0b0 nm
2nmλ1η0 nm - s0b0 1
n,m
X kVfi,j(x0)k2
i,j =1,1
n,m
s2b1
nm nm
kVfi,j(x0)k2
i,j =1,1
(42)
f(x0) - f* +
(nm - s0b0 )η0θ0
2nms0b0
(43)
+
+
where (40) follows from the definition of no in (37), (41) uses γι = 翁 and αι
(42) uses λo = 1, and (43) uses the definitions θo :
_ 2nmλιηo
=s1b1
and G0o :=
nm 4	4nmλι s0b0
(nm-1)λι +	s2b2
nm pnj%,ιkvfi,j (χo)k2.
By randomly choosing xbK from {xk}kK=-01 with probability ηk/ PtK=-01 ηt for xk, (43) turns to
E[kVf(xbK)k2] ≤
2(f (x0) - f )	(nm - sobo)ηoθoGo
-「k—1	+
k=o ηk
nms0
bo PKo1 η
(44)

C.2 Proofs of Corollaries 3 and 4
Now, We prove the detailed convergence results in Corollaries 3-4 with specific parameter settings.
Proof of Corollary 3. First we know that (44) with s0 = n and b0 = m turns to
E[kVf(xbK)k2] ≤
2(f(x0) - f*)
k^K — 1
k=0 ηk
(45)
Then if we set λk = Inbk, Sk ≡ √n, and bk ≡ √m for any k ≥ 1, then we know that Wk :
λ⅛ + 8λknm2 ≡ 8 and thus the StePSiZe ηk ≤
plugging ηk ≤ (1+√8)L into (45), we have
1
1
L(l+√Wk+l) ≡ (1+√8)L
for any k ≥ 0. By
E[kVf(xbK)k2] ≤
2(1 + √8)L(f (x0) - f *)
2,
where the last equality holds by letting the number of iterations K
number of stochastic gradient computations for each client is
2(1 + √8)L(f(x0)-f *)
T2
. Thus the
K
K—1
#grad = bk = m+
k=o
(K -1) √m
≤ n + rm2(1 + ^)L(f(X0)-f*)
n
Proof of Corollary 4. First we recall (44) here:
2

E[kVf(xbK)k2] ≤
2(f (x0) — f *) + (nm — so bo)ηoθoG0
1 ηk
(46)
21
Under review as a conference paper at ICLR 2022
In this corollary, we do not compute any full gradients even for the initial point. We set the client
sample size Sk ≡ √n and minibatch size bk ≡ √m for any k ≥ 0. So We need consider the
second term of (46) since s°bo = √nm is not equal to nm. Similar to Corollary 3, if we set
λk = Skbk for any k ≥ 1, then we know that Wk := ʌ 2, + 8λkn 3n ≡ 8 and thus the stepsize
k	2nm	,	k	λkskbk	b3ks3k
ηk ≤	√==j ≡ (i+√8)l for any k ≥ 0. Now, we can change (46) to
E[kVf(bK)k2] ≤
2(1 + √8)L(f(x0)- f*) ] (nm - sobo)θoG0
K	nms0b0K
2(1 + √8)L(f(x0)- f*)+4G0
K
(47)
where (47) holds by plugging the initial values of the parameters into the last term, and the last
equality holds by letting the number of iterations K = 2(1+v78)Lf(X )-f )+4G0. ThUS the number
of stochastic gradient computations for each client is
K-1
#grad =	bk
k=0
2(1 + √8)L(f (x0) - f *) + 4G0
2
L∆0 + G00
2
Note that ∆o := f (x0) — f * where f * := minx f (x).

D Further Improvement for Convergence Results
Note that all parameter settings, i.e., {ηk}, {bk} and {λk} in ZeroSARAH for Corollaries 1-2, only
require the values of L and n, and {ηk}, {sk}, {bk}, {λk} in D-ZeroSARAH for Corollaries 3-4
only require the values of L, n and m, both are the same as all previous algorithms. If one further
allows other values, e.g., , G0 or ∆b0, for setting the initial b0, then the gradient complexity can be
further improved. See Appendices D.1 and D.2 for better results of ZeroSARAH and D-ZeroSARAH,
respectively.
D. 1 Better result for ZeroSARAH
Corollary 5 Suppose that Assumption 1 holds. Choose stepsize ηk ≤
(i+√8)l for any k ≥ 0, mini-
batch size bk ≡ √n and parameter λk
2n for any k ≥ L
Moreover, let b0 = min
and λ0 = 1. Then ZeroSARAH (Algorithm 2) can find an -approximate solution for problem (1)
such that
E[kVf(xbK)k2] ≤2
and the number of stochastic gradient computations can be bounded by
#grad
Similarly, G0 can be bounded by G0 ≤ 2L∆b 0 via Assumption 1. Let b0
we also have
, then
#grad
Remark: The result of Corollary 5 for ZeroSARAH is the best one compared with Corollaries 1-2.
In particular, it recovers Corollary 1 when bo = n. In the case bo < n (never computes any full
gradients even for the initial point), then #grad = O (√n(L∆∆0 + ʌ/GGo)) which is better than the
result O (Vn(L》o+Go)
in Corollary 2. Similar to the Remark after Corollary 2, if we consider L,
22
Under review as a conference paper at ICLR 2022
△o, Go or ∆o as constant values then the stochastic gradient complexity in Corollary 5 is #grad =
O( Wn), i.e., full gradient computations do not appear in ZeroSARAH and the term ‘n' also does not
appear in its convergence result. If we further assume that loss functions fi ’s are non-negative, i.e.,
∀x, fi(x) ≥ 0 (usually the case in practice), we can simply bound △bo := f (x0) - b ≤ f (x0) and
then bo can be set as min
n √nLfx0)
n
for Corollary 5.
Proof of Corollary 5. First we recall (25) here:
E[∣Vf(bK)k2] ≤
2(f (XO)- f*) +(n - bo)(4γ0 ±2αob°) 1 XX kvfj (xo)k2.
v-^K — 1
k=o ηk
nbo PkK=-o1 ηk
(48)
j=1
Note that here we also need consider the second term of (48) since bo may be less than n. Similar
to Corollary 2, if we set λk = 2nk and bk ≡ √n for any k ≥ 1, then we know that Mk :=
ɪ +
λkbk
≡ 8 and thus the stepsize ηk ≤
second term, we recall that γo ≥ 翁=q+√∣)L and ao ≥
that γo ≥ αobo since bo ≤ n. Now, we can change (48) to
L (l + √Mk+ι) — (1+√8)L
for any k ≥ 0. For the
2nλιη0
^b2- = (1 + √8)L√n.
It is easy to see
E[kVf(xK)k2] ≤
2(1 + √8)L(f (xo) - f*) + 6(n - bo) 1 XX ∣∣vf .(χθ)k
√nboK
j=1
2(1 + √8)L(f (x0) - f *) + 6(n 一 bo)Go
√nboK
(49)
K
K
1
1
1
2
2,
where (49) is due to the definition Go := n Pnn=] IlVfi(Xo) k2, and the last equality holds by letting
2(1 + √8)L(f(x0)-f *)
the number of iterations K =
gradient computations is
K—1	K—1
+ 6(√nb0)G0. Thus the number of stochastic

#grad =	bk = bo +	bk
k=o
k=1
bo + (K 一 1)√n ≤ bo +
2(1 + √8)√nL(f (x0) - f *) + 6(n - bo)Go
bo2
2
By choosing bo = min{ JnGO, n}, We have
#grad ≤
2(1 + √8)L(f (x0)-广)
+ min 7
O
Similarly, Go can be bounded by Go ≤ 2L(f(xo) 一 fb*) via Assumption 1 and let bo
min{ \/安
en we have
Note that △o :
Pin=1 minx fi(x).
#grad = O
f(x0) 一 f*, where f* := minx f(x), and △ o := f(x0) 一 f*, where f" :
1
n

D.2 Better result for D-ZeroSARAH
Corollary 6 Suppose that Assumption 2 holds.
(1+W for any k ≥ 0，
Choose stepsize ηk ≤
clients subset size Sk ≡ √n，minibatch size bk ≡ √m and parameter λk = ∙∣nbk for any k ≥ L
23
Under review as a conference paper at ICLR 2022
Moreover, let s0 = min
and b0 = m (or b0 = min
, m and s0 = n),
and λ0 = 1. Then D-ZeroSARAH (Algorithm 3) can find an -approximate solution for distributed
problem (2) such that
E[kVf(bK)k2] ≤ /
and the number of stochastic gradient computations for each client can be bounded by
#grad
Proof of Corollary 6. First we recall (44) here:
E[kVf(xbK)k2] ≤
2(f (x0) - f *) + (nm - sobo)ηoθoG0
nms0
bo PK=OI η
(50)
Similar to Corollary 4, here we also need consider the second term of (50) since sobo may be less
than nm. Similarly, if We set λk = Inbmk, Sk ≡ √n, and bk ≡ √n for any k ≥ 1, then We know that
Wk := λ⅛ + 8⅛产 ≡ 8 and thus the StePsiZe ηk ≤
Then (50) changes to
1
1
L(l + √Wk+ι) ≡ (1 + √8)L
for any k ≥ 0.
E[kVf(xbK)k2] ≤
2(1 + √8)L(f(x0) - f*) + (nm - sobo)θoG0
K
nms0b0K
2(1 + √8)L(f(x0) - f*) + 6(nm - s°bo)G0
K
√nrnsoboK
(51)
2,
where (51) by figuring out θo with the initial values of the parameters, and the last equality holds
by letting the number of iterations K = 2(1+√8)Lf (X )=f ) + Wnn=；：：：孕.ThUS the number of
stochastic gradient computations for each client is
K=1
#grad =	bk
k=o
so
—bo +
n
(K — 1)√m
m
2(1 + √8)L(f (x0) - f*) + sobo + 6(nm — s°bo)G0
n	nso bo 2
≤
n
2
). The last equation uses the
min n *, no and bo = m (or so = n and bo = min
definition ∆0 := f (x0) — f * where f * := minχ f (x).
24