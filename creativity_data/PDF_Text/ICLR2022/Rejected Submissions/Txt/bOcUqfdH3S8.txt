Under review as a conference paper at ICLR 2022
Provably Calibrated Regression Under Dis-
tribution Drift
Anonymous authors
Paper under double-blind review
Ab stract
Accurate uncertainty quantification is a key building block of trustworthy machine
learning systems. Uncertainty is typically represented by probability distributions
over the possible outcomes, and these probabilities should be calibrated, e.g. the
90% credible interval should contain the true outcome 90% of the times. In the
online prediction setup, existing conformal methods can provably achieve calibra-
tion assuming no distribution shift; however, the assumption is difficult to verify,
and unlikely to hold in many applications such as time series prediction. Inspired
by control theory, we propose a prediction algorithm that guarantees calibration
even under distribution shift, and achieves strong performance on metrics such
as sharpness and proper scores. We compare our method with baselines on 19
time-series and regression datasets, and our method achieves approximately 2x
reduction in calibration error, comparable sharpness, and improved downstream
decision utility.
1	Introduction
Accurate uncertainty quantification is crucial for machine learning predictions used in high-stakes
decision making. Typically, uncertainty is represented by probability distributions over the possible
outcomes, and these probabilities should be calibrated. In the regression setup, for example, the true
label should be below the predicted 95% quantile for 95% of the samples (Gneiting et al., 2007).
Calibration can convey confidence to decision makers because extreme values (e.g. true label above
95% quantile) are guaranteed to be rare. In addition to calibration, these probabilities should be
sharp (i.e. concentrated and have low variance). Sharp probabilities are useful to decision makers
because they are informative.
If data is i.i.d., then recalibration (Kuleshov et al., 2018) and conformal prediction (Vovk et al., 2020)
algorithms can achieve low calibration error and good sharpness in the regression setup. However,
the i.i.d. assumption is unlikely to hold in most time-series prediction tasks. Under distribution drift,
it is possible to adapt regret minimization algorithms (Cesa-Bianchi & Lugosi, 2006; Kuleshov
& Ermon, 2017) to achieve calibration. However, regret minimization calibration algorithms are
designed for the asymptotic regime, and empirically we show that they are effective only with large
sample size, and have very poor calibration and sharpness with short time series (e.g. 50 samples),
limiting their practical utility.
Our goal is to design an algorithm to achieve good calibration and sharpness in the online regres-
sion setup even for short time series. We start with an existing prediction algorithm that has good
calibration and sharpness for i.i.d. data (such as conformal prediction), and track the empirical fre-
quency that the labels are below e.g. the 75% quantile. If the empirical frequency is significantly
below 75%, we move future predictions up so more labels are below the prediction; vice versa. We
design a very efficient adjustment method that can guarantee near perfect calibration with only tens
of samples. We call this the “basic” prediction algorithm shown in Figure 1 (upper right).
Our main technical contribution is an improved algorithm that addresses two short-comings of the
“basic” algorithm without breaking guaranteed calibration. First, the predictions should be feasible,
e.g. the 75% quantile should never be smaller than the 25% quantile. The basic algorithm might
violate this requirement because we adjust each quantile separately with no constraint on their rela-
tion. Second, the predictions should be stable if the distribution drift stops; empirically instability
harms sharpness and proper scores. Our improved algorithm is based on an analogy between the
1
Under review as a conference paper at ICLR 2022
-- predicted 25% quantile
predicted 75% quantile
true 25% quantile
true 75% quantile
observed label
Original Prediction
Basic Algorithm
0	10	20	30	40	50	60
time step
Basic + Feasibility Constraint
0	10	20	30	40	50	60
time step
Basic + Feasibility Constraint + PID Control
0	10	20	30	40	50	60	0	10	20	30	40	50	60
time step	time step
Figure 1: An illustrative example of our algorithm. The x-axis is the time step (for the online
prediction setup), and the y-axis is the prediction. We look at the 75% and the 25% quantile
of the prediction. The purple markers are the true labels. The true label is initially drawn from
Uniform[-2, 2] but drifts to Uniform[-5, -1]. Upper Left: The original predictions that is not
calibrated because distribution drifted at time step 5. Upper Right: The “basic” algorithm: we move
the quantiles up or down to achieve calibration. However, the prediction is sometimes infeasible (e.g.
the 25% quantile is above the 75% quantile at time step 43) and oscillates even when distribution
drift stops. Lower Left: We add feasibility constraints. The quantiles are calibrated and feasible.
Lower Right: We use PID control to stabilize the predictions. The quantiles are calibrated, feasible
and do not oscillate. The predictions adapts very quickly in response to a new distribution within
tens of samples.
prediction task and a mechanical system. We encode the feasibility constraints as the feasible states
of a mechanical system, and use proportional-integral-derivative (PID) control (Minorsky, 1922) to
stabilize the system. Our final algorithm makes predictions that satisfy all three desirable properties:
provable calibration, feasibility, and stability (Figure 1 bottom right).
We test our algorithm on two real world time series datasets (stock earnings and COVID cases) and
a large benchmark of 17 common regression datasets. We reduce calibration error by approximately
2x compared too all baselines, while achieving comparable or better sharpness. We also simulate
COVID response decisions based on predictions on the COVID dataset, and observe improved deci-
sion loss compared to baselines during periods of distribution drift (e.g. when case numbers surge).
2	Problem Setup and Background
We consider regression problems. Let X ∈ X denote the input feature and Y denote the label.
We assume that the label is bounded by B, i.e. Y cannot take any value outside [-B, B] for some
pre-specified B > 0.
We consider quantile predictions with a set of K equally spaced quantiles 0 < α1 < …< ακ < 1.
For example, when K = 9 We choose α1 = 10%, α2 = 20%,∙∙∙ ,ακ = 90%. A quantile
prediction is a vector of K numbers (denoted by Z) where Zk should ideally predict the αk -th
quantile of the label Y |X conditioned on the input feature X . We say a quantile prediction is
feasible, if Yk < Yk+1 , ∀k, i.e. a lower quantile is always smaller than a higher quantile. In the
limit of infinitely many quantiles K → ∞, a quantile prediction is equivalent to a probability
distribution, however, for technical reasons that will become evident later, we only consider finitely
many quantiles in this paper.
Our setup is an online prediction setup, where the forecaster sequentially makes the predictions.
Consider a COVID prediction example: at time t, the forecaster observes some features Xt (such
as current vaccination rate) and makes a prediction Zt about next week’s case number. After the
forecaster makes a prediction, the true case number Y t is revealed; then we move on to time step
2
Under review as a conference paper at ICLR 2022
t + 1 and repeat this process. Following standard terminology we will use “nature” to refer to the
process that generates the features Xt and the true label Y t . Our setup is formalized by the following
interaction between forecaster and nature: for t = 1,2,…，T
1.	Nature reveals feature X t
2.	Forecaster makes a quantile prediction Zt ∈ RK where Zkt is the αk-th quantile.
3.	Nature reveals label Y t ∈ [-B, B]
We call a finite sequence of interactions a transcript, i.e. a sequence X1, Z1, Y1,…,XT, ZT, YT.
A forecasting algorithm ψ is a function that maps a transcript and a new feature to a prediction:
ψ : X 1,Z 1,Y1,…，Xt-1,Zt-1,Yt-1,Xt → Zt.	(1)
We will consider two types of assumptions on nature. We say that nature is i.i.d. if ∀t, Xt , Yt are
drawn from some i.i.d. random variable (but we make no assumption on the probability law on this
random variable). We say that nature is causal if nature chooses Xt , Yt without depending on the
future. Specifically, Xt, Yt can only depend on the variables that precede it in the transcript (i.e.
Xt can depend on X1, ∙∙∙ , Yt-1 and Yt can only depend on X1, ∙∙∙ , Yt-1,Xt, Zt). Causal is
an extremely weak assumption. For example, nature can even adversarially choose the label Yt to
increase prediction error after observing the forecaster’s prediction Zt.
2.1	Calibration and Achieving Calibration with Conformal Methods
A natural property that we can request is (probabilistic) calibration (Gneiting et al., 2007; Kuleshov
et al., 2018). Intuitively, ∀k, the label Yt should be below the predicted αk-th quantile for an
αk proportion of the samples. Formally, given a transcript of any length, consider the empirical
frequency that the label Yt is below the predicted quantile Zkt up to time T (for any T that’s less
than the length of transcript)
FkT = X I(Yt ≤ Zkt ).	(2)
1≤t≤T
Ideally, the label Yt should be below the αk-th quantile exactly αkT many times, i.e., ∀k, FkT /T =
αk . Achieving this perfectly is difficult, so we define an approximation. For any function b :
N → R+ We say that a transcript is b-calibrated if ∀k = 1,…，K,∀T, |FT/T - αk| ≤ b(T).
Correspondingly, a forecasting strategy is calibrated if the resulting transcript is calibrated:,
Definition 1. For any function b : N → R+, a forecasting algorithm ψ is b-calibrated under i.i.d.
(or causal) assumptions if for any nature’s strategy that is i.i.d. (or causal), the resulting transcript
is b-calibrated almost surely.
Conformal Calibration Conformal calibration (Vovk et al., 2020) is a forecasting algorithm based
on conformal prediction ideas (Vovk et al., 2005; Shafer & Vovk, 2008). It is a wrapper algorithm
that transforms a initial predictor (such as an off-the-shelf predictor) into new predictions that are
provably calibrated under i.i.d. assumptions. Conformal calibration is based on the following intu-
ition: for example, if 75% of past labels are below the initial prediction’s mean, then we can use the
initial prediction’s mean as the 75%-quantile prediction in the future. More generally, we identify
a statistic of the initial prediction, such that αk -proportion of the past labels are below that statistic.
We use this statistic as the αk -th quantile of the future prediction.
If the data is i.i.d., conformal calibration is a very useful algorithm. This is because it has extremely
strong calibration properties (Proven in Proposition 1 of (Vovk et al., 2020)), where the random
variables I(Yt ≤ Zk),t = 1, 2, •…is a sequence of i.i.d. Bernoulli random variables with mean ak.
This is impressive because even ifwe have access to the oracle forecaster (i.e. Ykt is equal to the true
conditional αk -th quantile of Yt |Xt), we cannot achieve better calibration — I(Yt ≤ Zkt) is also a
sequence of i.i.d. Bernoulli random variables with mean αk . Empirically, conformal prediction also
has very good sharpness if the initial prediction function is reasonable (Burnaev & Vovk, 2014).
A minor property (but needed for proofs) is that the predictions should bounded in [-B, B]. Con-
formal calibration satisfies this property when t > K, i.e. there are more samples than predicted
quantiles. To bypass this limitation, we can initialize the conformal calibration algorithm with at
least K offline samples. Alternatively, ifwe know B in advance, we can clip the prediction by B.
3
Under review as a conference paper at ICLR 2022
3 Calibration under Distribution Drift
This section introduces our algorithm that guarantees calibration even when nature is not i.i.d. We
start from a basic algorithm in Section 3.1, and discuss its shortcomings. Then we move on to more
advanced algorithms in Section 3.2 and 3.3 to address the shortcomings.
3.1	A Basic Calibrated Prediction Algorithm
Conformal calibration achieves good calibration and sharpness when nature is i.i.d. When we are
unsure if nature is i.i.d., our basic idea is to use conformal calibration until it has failed to achieve
calibration. Specifically, when nature is i.i.d., I(Yt ≤ Zkt) is a sequence of Bernoulli random
variables, so Fk is the sum of these Bernoulli random variables (i.e. a binomial). We use bδ(T) to
denote the confidence interval of a binomial distribution, i.e. Pr[FkT /T ∈ αk± bδ(T)] = 1 — δ.
We fix some small δ (such as 0.05) and if under the conformal calibration algorithm FkT /T 6∈
ɑk ± bδ (T), We know (with 0.95 confidence) that nature is not i.i.d, so We will make adjustments to
salvage calibration.
From a high level, Fk > αkt + bδ(t)t implies that the true label is below the ak -th quantile too
often, so our prediction Zkt has been too large and we should reduce it; vice versa; There are some
design freedom in choosing how much to reduce or increase the prediction. We choose an adjust-
ment that grows exponentially with larger calibration error. The reason for this choice is to make
very large adjustments when the calibration error is large, so we can tightly bound the calibration
error. Formally, let Zkt denote the initial prediction generated by the conformal calibration prediction
algorithm, the prediction of our algorithm Ykt is given by
〜	(1 - eβ(Fk-akt-bδ(t)t)	Fk	> αkt + bδ(t)t
Zk+1 =	Zk+1	+ Ek+1,	where E" = < eβ(αtY⑴t-Fk)- 1	Fk	< αkt - bδ(t)t	⑶
(0	otherwise
Intuitively, if Fk is within the correct range we make no adjustments (i.e. Z「= Zk+1); when
Fkt falls outside the correct range, we make aggressive adjustments that increase exponentially with
how much it falls outside the correct range. β > 0 is a hyper-parameter that controls how large
the adjustments are (we will show in Section 3.4 that these hyper-parameters are easy to choose).
Note that Ekt+1 is for the (t + 1)-th time step (instead of t). This is because any adjustments can
only depend on past information, so we will only have access to Fkt at the (t + 1)-th time step. The
following theorem shows that such adjustments indeed guarantee calibration.
Theorem 1. Let Zk be generated by any forecasting algorithm and bounded in [—B, B], for any
δ > 0, the prediction algorithm defined by Eq.(3)is b-calibrated where b(T) = b1(T) + Iog(2B+1)+e.
Note that the guarantee of Theorem 1 does not require that conformal calibration generate the ini-
tial prediction Zk. The use of conformal calibration is motivated by its strong empirical perfor-
mance (Vovk et al., 2020) when the data is i.i.d. or close to i.i.d. (which is further supported by our
experiments).
The calibration guarantee is strong because under conformal calibration with i.i.d. data (or the or-
acle forecaster) we can expect to see bδ(T) calibration error (with 1 - δ probability); without i.i.d.
assumptions, our algorithm can ensure that the calibration error only increases by O(log B/T). No-
tably, the calibration error scales logarithmically with the assumed upper bound log B, so choosing
a loose bound B does not significantly degrade the guarantee.
The basic idea has two shortcomings illustrated in Figure 1 that we will address in the next sections.
First, in a feasible quantile prediction, the 75% quantile should never be smaller than the 25% quan-
tile, but our adjustments might violate this requirement because we adjust each quantile separately
with no constraint. Second, the basic idea might lead to oscillation and instability. Intuitively, when
the empirical frequency is incorrect, we make adjustments to correct it, but once the empirical fre-
quency improves, we reduce the adjustments, which will cause the empirical frequency to become
incorrect again, leading to oscillation. Empirically instability can hurt performance metrics such as
sharpness.
4
Under review as a conference paper at ICLR 2022
3.2 Improving the Basic Algorithm to Ensure Feasibility
In this section, we address the feasibility problem. We first define some convenient notation
shorthand. For our prediction Zt , we denote the distance between (neighboring) quantiles as
∆Zk = Zk+1 - Zk for k = 0, ∙ ∙ ∙ , K. We can make a similar definition for the initial PrediC-
tion ∆Zkt = Zkt +1 - Zkt. Under these new definitions, requiring feasibility is equivalent to requiring
Positive distance between neighboring quantiles, i.e. ∆Zkt > 0, ∀k.
The main challenge is that the basic adjustment algorithm Eq.(3) might not satisfy this new require-
ment, so we need to find a comPromise. We draw insPiration from a mechanical system illustrated
in Figure 2 where We imagine that our prediction quantiles Zt, ∙ ∙ ∙ , ZK are a sequence of points in
sPace. We imagine that each requirement (satisfying Eq.(3) or feasibility constraint) can be thought
of as a “force” that “push” the quantiles to satisfy the requirement. These forces will contend with
each other (because the requirements are conflicting), so the equilibrium of these forces is the natural
compromise between the conflicting requirements.
Specifically, to enforce the requirement that the interval size ∆Zkt cannot be close to 0 or negative,
we design a force that pushes the interval to be larger whenever it is small. Intuitively, we can think
of this as inserting a “spring” (illustrated in Figure 2) into the interval. The spring will resist being
compressed, and exert an opposing force whenever the interval becomes small. The smaller the
interval, the more compressed the spring will be, so the larger the opposing force; in the limit where
the interval size tends to 0, the opposing force will tend to ∞. This prevents the interval size from
going to zero (or negative) and ensures feasibility.
We add another desiderata: no force is generated only when the interval size ∆Zkt equals the initial
prediction’s size ∆Zkt. Intuitively, we want to use the initial conformal prediction as much as possi-
ble (which we argued in Section 3.1 is desirable). This can be achieved by imaging that the spring
has initial length ∆Zkt and resists both compressing and stretching.
Figure 2: Left: An illustration for Section 3.2 where we draw analogy to a mechanical system. We
attach a “spring” between neighboring quantiles that resists being compressed or stretched. The
final prediction Zkt is the equilibrium state of this mechanical system. This is the natural way to
compute the compromise of several conflicting requirements. Right: Illustration of the effect of PID
in Section 3.3. Here we plot the error Fkt - αkt when the initial prediction systematically under-
predicts. P corresponds to the basic algorithm, and the error oscillates around 0. PI corresponds to
adding an integral term, and the error converges to 0. PID adds an additional derivative term, which
reduces overshoot and improves convergence.
To ensure that no prediction exceeds the bounds [-B, B], we also create two additional quantiles
α0 = 0 and αK+1 = 1, and define Z0 = -B and ZK+1 = +B, so that assuming feasibility we
will always have Z1 > Z0 = -B and ZK < ZK+1 = B . We formalize the above intuition as the
following list of forces on each quantile:
1.	Forces from miscalibration Ekt defined by Eq.(3) and deviation from initial prediction
Dk := Zk — Zk. If we only have these two forces, we recover the basic algorithm. This is
because if we solve the equilibrium state of the system Dk + Et = 0, we get Zk = Zk + Ek
which is exactly Eq.(3).
2.	Force from the spring in the interval above Ak := η(∆Zk/∆Z1 — ∆Z]/∆Ztt) where η
is a hyper-parameter (we will show in Section 3.4 that setting these parameters is easy).
Intuitively, if the interval above is larger than its initial size (i.e. ∆Zkt > △ Zk) then the
5
Under review as a conference paper at ICLR 2022
spring “stretches“ which generates an upward force Atk > 0; conversely if the interval
above is smaller than its initial size (i.e. ∆Z1tt < ∆Zk) then the spring above “compresses”
which generates a downward force Ak < 0. If the interval has the same size, then no force
is generated because Atk = 0.
3.	Force from the spring in the interval below Bk := η(∆Zk _、/ ∆Zk-ι - ∆Zk-1 ∆Z∖ -J.
We define the final prediction Zkt as the solution to the following equilibrium equations (i.e. all
forces sum to 0 for all the quantiles)
Ek + Dtk + Ak + Bk =0,∀k =1,…，K.	(4)
The resulting forecasting algorithm is provided as Algorithm 1.
Algorithm 1 Basic + Feasibility Forecasting Algorithm
1:	For each k = 1,…，K, initialize Fk = 0
2:	for t = 1,2,•…do
3:	Observe Xt and use initial prediction algorithm (e.g. conformal calibration) to predict Zt.
4:	For each k = 1,…，K, compute Ek according to Eq.(3).
5:	Use a physics simulation to find Zkt that solves Eq.(4) given Zt and Ekt, output Zkt.
6:	Observe the true label Yt. For each k = 1,…，K, set Fk = FkT + I(Yt ≤ Zk).
7:	end for
Calibration Properties We will show that Algorithm 1 achieves calibration except when the “like-
lihood” is exponentially high (which is also very desirable). Specifically, we define a notion similar
to likelihood
L(Zt,Yt) = (αk+ι - αk)∕∆Zt, for the k thatsatisfies Yt ∈ %,Zt+ι].	(5)
Intuitively, L(Zt, Yt) is large if the true label Yt belongs to a small interval (i.e. ∆Zkt is small).
In the limit of infinitely many quantiles K → ∞, the quantile prediction becomes equivalent to a
probability distribution, and Eq.(5) becomes the likelihood of the probability distribution.
Theorem 2 (Simplified). Under the same condition as Theorem 1, for any C > 2∕β, Algorithm 1 is
b-calibrated where b = O(K∕√T + c/T) on any transcript such that ∀t,L(Zt, Yt) ≤ eθ(c).
Theorem 2 is a simplified version of the full theorem (we replace all the constants with asymptotics).
For a non-asymptotic version and proof see Appendix A. Intuitively, for any transcript generated
by Algorithm 1, if the “likelihood” is never exponentially high, then the transcript is O(K∕√T)-
calibrated (c will be a bounded constant). When T is large, the calibration error will also go to 0
at a rate of O(1∕√T). On the other hand, if the likelihood is exponentially large, We are “almost”
perfectly predicting the label, so there is little uncertainty. Uncertainty quantification metrics such
as calibration become less relevant.
3.3	Improving Stability with PID control
Finally we address the stability problem. As we argued, the key problem is the constant need to
increase or decrease the adjustment Ekt+1 based on whether the empirical frequency Fkt is different
from αkt. Our key insight is to think of this problem as a control problem: we would like Fkt
to match αkt and the error is measured by Ekt (which we want to bring close to 0), so we can
use control algorithms to stabilize the system. We show (heuristically) that a very popular control
algorithm called proportional-integral-derivative (PID) is well suited for the task.
Our new algorithm replaces Ekt in line 4 of Algorithm 1 with the weighted sum of three terms:
proportional Eptk	=	Ekt,	integral	Eitk	= Ptτ =0	Ekτ	and derivative	Edtk	=	Ekt	-	Ekt-1.	The
proportional term Eptk is the same as Eq.(3), so if we only have the proportional term we would
reduce to Algorithm 1. The integral term Eitk is key to addressing the stability problem because it
does not decrease even when the Ekt goes to 0, so it applies a permanent adjustment when distribution
drifts. The intuition of the integral term is demonstrated in Figure 2. Using the integral term is prone
6
Under review as a conference paper at ICLR 2022
to a well known problem called overshoot. Intuitively, the integral term will increase until Ekt goes
to 0, at which point the (permanent) adjustment might already be too large. The standard method
to reduce overshoot is to use a derivative term Edtk (Minorsky, 1922), and we illustrate its effect in
Figure 2. The overall adjustment is equal to
EPID-k = kPEpk + CLIP[-B,B] (kiEtk + kdEdk)	⑹
where kp , ki , kd ≥ 0 are the hyper-parameters (again, we will show in Section 3.4 that choosing
these hyper-parameters is not difficult). We also clip the integral and derivative terms to be within
[-B, B] to ensure Corollary 1 even in the worst case scenario. For any real data in our experiments,
the clipping never took effect.
We show that applying PID control does not break the calibration guarantee in Theorem 2.
Corollary 1 (Simplfied). Under the same condition as Theorem 2, for any kp , ki , kd ≥ 0, Algo-
rithm 1 with Ek replaced by EP2—k in Εq.(6) is b-calibrated where b = O(K∕√T + c/T) on any
transcript such that ∀t, L(Z t, Y t) ≤ eΘ(c).
A non-asymptotic version is also available in the appendix. With PID control Corollary 1 is es-
sentially identical to Theorem 2 (in fact exactly the same for the simplified asymptotic version).
Therefore, PID improves stability without degrading the calibration guarantees.
3.4	Hyper-parameter Selection
Our algorithm consists of hyper-parameters which on first sight, seem difficult to choose for a prac-
titioner. Specifically, the hyper-parameters are β, δ in Eq.(3), which specify how large is the ad-
justment Ekt; the “elastic coefficient” η in the definition of Atk and Bkt, and the PID coefficients
kp, ki, kd.
Fortunately, we show that our algorithm is very robust to hyper-parameter choices. In fact, we tune
these parameters on a few simple synthetic examples (similar to Figure 1), and fix them throughout
all the experiments on real data. We show (in Figure 7 in the appendix) that even if we were to
finetune these hyper-parameters specifically for each dataset, we would only see marginal improve-
ments compared to using fixed hyper-parameters. Therefore, any practitioner can simply use the
fixed hyper-parameters that we recommend, and generally do not have to worry about tuning them
in practice.
4 Related Work
In the regression setup, there are several notions of calibration. The most common one (which
is studied in our paper) is probabilistic calibration (Gneiting et al., 2007; Kuleshov et al., 2018;
Vovk et al., 2020). Other notions include marginal calibration (Gneiting et al., 2007), individual
calibration (Zhao et al., 2020), distribution calibration (Song et al., 2019), threshold calibration, etc.
Though not considered in our paper, calibration has also been extensively studied in the classification
setup, calibration has been studied under (Brier, 1950; Murphy, 1973; Dawid, 1984; Cesa-Bianchi &
Lugosi, 2006; Platt et al., 1999; Zadrozny & Elkan, 2001; 2002; Niculescu-Mizil & Caruana, 2005;
Guo et al., 2017; Lakshminarayanan et al., 2017). There are also other notions of calibration such
as multi-calibration (Hebert-Johnson et al., 2018), decision calibration (Zhao et al., 2021), classwise
calibration (Kull et al., 2019), etc.
Many applications only require a confidence interval. For confidence intervals a notion very similar
to calibration is coverage, i.e. the label should belong to the interval with high probability. Con-
formal prediction methods are the gold standard for predicting intervals with high coverage. (Vovk
et al., 2005; Shafer & Vovk, 2008; Papadopoulos, 2008; Romano et al., 2019).
The above research primarily focus on the i.i.d. setup (or exchangeable setup), while many real
world data are not i.i.d. In the online setup with distribution shift, regret minimization algorithms
can guarantee asymptotic calibration (Cesa-Bianchi & Lugosi, 2006; Kuleshov & Ermon, 2017) for
classification. For conformal prediction, when the likelihood ratio of the distribution shift is known,
(Tibshirani et al., 2019) can still guarantee coverage. A concurrent work (Gibbs & Candes, 2021)
can guarantee asymptotic coverage for conformal confidence intervals. Our work differs in that we
focus on calibration, and solve additional feasibility and stability challenges.
7
Under review as a conference paper at ICLR 2022
105
① n-B>φqB-
25% quantile
--75% quantile __
observed label
XX=---
	1	1—
15	20	25
time step
S ①≡UBnb -Bn Ep
5
15
10
Θn-B> aqg
25% quantile
--75% quantile
observed label
X正气」
x__ ^ A
工依」二一
，	20	25
time step
target quantiles

Figure 3:	Example predictions on the stock prediction dataset (for Amazon). Left is conformal
calibration only, and right is conformal calibration + our method. We plot both the sequence of pre-
dictions, and the reliability diagram (i.e. the relationship between target frequency αk and empirical
frequency FkT /T). Amazon growth has been faster than expected by analysts at times (such as steps
21-26, or 30-33), conformal calibration failed to capture this, while our method rapidly adapted to
this distribution shift within a few time steps.
target
Figure 4:	Reliability diagram for different methods on stock prediction. Each blue line is the reli-
ability diagram for a single stock. The yellow shaded area is the 99% confidence interval for the
reliability diagram of an oracle predictor (i.e. it is αk ± b,(T) for δ = 0.01). From left to right
we use original predictions, conformal calibration, our method, and an regret minimization baseline.
Our method achieves achieves the best calibration. Even though the regret minimization baseline
can also guarantee calibration asymptotically, for short data sequences the calibration is incorrect.
5	Experiments
This experiment aims to show on real time series data and standard regression benchmarks that our
algorithm achieves strong calibration, sharpness and proper scores.
Datasets We use two time series prediction tasks (stock and COVID) and a regression benchmark
with 17 datasets. For times series we start from expert predictions (such as from stock analysts or
COVID forecasting teams), use conformal calibration to transform the expert predictions, and finally
apply our algorithm to adjust the output of conformal calibration. For the regression benchmark we
split each dataset into train and test. We use the training set to learn a prediction function offline,
and assume that the test data are available sequentially online. For each dataset we add the following
types of drifts: linear shift: add a constant bias to the label that increases over time; cycle: add a
bias to the labels that moves up or down repeatedly; scale shift: multiple the label by a constant that
increases with time; jump: add a constant bias at some ∀t > t0 for some time step t0 . More details
about the datasets are provided in appendix.
Performance Metrics : Calibration loss We use a notion similar to the expected calibration
error (Guo et al., 2017) LeCe = * Ek |Ff—αkT|. Pinball loss We use the average pinball loss (aka.
hinge loss) Lpinball(Y, Z) = ~KK Pk akReLU(Z — Yk) + (1 — αk)ReLU(K - Z). The pinball loss
is a proper score, i.e. the oracle quantile prediction minimizes the expected pinball loss. Sharpness
We use the average interval size to measure the sharpness LSharpneSS(Z)=* Ek ∣Zk — ZK-k|.
Baselines: We compare three baselines. original is the original expert predictions. We use kernel
density estimation (Van Kerm, 2003) to transform the expert predictions into a probability; con-
formal is conformal calibration only (without Algorithm 1); regret is an adaptation of the regret
minimization algorithm in (Kuleshov & Ermon, 2017) (details in the appendix).
8
Under review as a conference paper at ICLR 2022
Figure 5: The decision loss reduction if we use conformal / ours instead of the original expert
predictions for a COVID response decision task. The shaded area is 1 standard deviation computed
by bootstrap sampling geographical locations, and the dashed line is the total case number. Both
conformal / ours reduce decision loss (so all curves are below 0), but our method reduces loss more
compared to conformal when distribution changes drastically, i.e. when case numbers spike.
	pinball	calibration (ece)	sharpness
	orig. conf. ours regret	orig. conf. ours regret	orig. conf. ours regret
stock	0.052 0.045 0.046 0.145	0.243 0.089 0.051 0.151	0.140 0.196 0.198 0.422
COVID	0.231 0.181 0.181 0.W	0.152 0.054 0.042 0.079	0.893 1.122 1.085 0.952
Bench-shift	2.540 0.921 0.299 0.5W=	0.494 0.436 0.149 0.232	0.676 2.842 1.737 2.768
Bench-scale	2.848 2.111 2.062 2.191	0.249 0.110 0.049 0.058	0.676 7.722 9.139 10.90
Bench-jump	1.396 0.777 0.533 0.762	0.286 0.201 0.073 0.172	0.676 2.164 1.972 2.637
Bench-cycle	0.924 0.697 0.496 0.714	0.220 0.165 0.047 0.146	0.676 2.595 2.775 3.045
Table 1: Performance comparison ours and several other alternatives on time series datasets and
regression benchmarks. Our method (ours) achieves far better ECE than other methods. For pinball
loss our method is on-par or better than baselines. The original prediction (orig.) is sharp, but the
prediction is incorrect (the calibration and pinball loss are poor), hence sharpness is not meaningful.
If we exclude (orig.), then our method achieves better or on-par sharpness than any other baseline.
Regret minimization (regret) has poor performance on stock and COVID (with 50 samples per data
sequence) and better performance on UCI (around 100 samples per data sequence) but still much
worse than ours.
Qualitative Results In Figure 3 and Figure 8,9 (in the Appendix) we plot some example prediction
sequences and reliability diagrams. Our algorithm is able adjusts the predictions up or down to
ensure calibration, and does not hurt sharpness (compared to conformal calibration only) when there
is no distribution drift. In Figure 4 we plot the reliability diagram for the stock dataset. Our method is
always calibrated (i.e. within the tolerance shaded in yellow), conformal calibration is uncalibrated
(without the tolerance) for about 10% of the samples, while the regret minimization baseline is
generally uncalibrated (because all time series are very short with around 50 samples).
Quantitative Results In Table 1 we show the performance of our algorithm compared to several
baselines. Our method achieves the best calibration error (ece) by far, and comparable or better
pinball loss compared to baselines. Even though the original prediction achieves the best sharpness
overall, the pinball loss and calibration error of the original prediction are so large, that sharpness
is meaningless. If we exclude the original prediction, then our method also achieves comparable or
better sharpness than other methods that have reasonable calibration.
Downstream Task For COVID we simulate a response strategy (i.e. whether to close businesses)
based on forecasted cases. We use the median of the prediction to select the best action. For details
see Appendix C.1. The results are shown in Figure 5. Both conformal calibration and our method
improves decision loss compared to using the original prediction; our method has an edge when the
distribution changes rapidly, e.g. when cases surge.
6	Conclusion
This paper introduces a novel type of online algorithms to ensure calibration, where we start with an
algorithm that guarantees calibration in the i.i.d. setup, and make modifications whenever it fails to
achieve calibration. Future work can explore its application to other calibration definitions or more
generally, other online learning problems.
9
Under review as a conference paper at ICLR 2022
References
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM
SIGKDD international conference on knowledge discovery & data mining, pp. 2623-2631, 2019.
Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review,
78(1):1-3, 1950.
Evgeny Burnaev and Vladimir Vovk. Efficiency of conformalized ridge regression. In Conference
on Learning Theory, pp. 605-622. PMLR, 2014.
Nicolo Cesa-Bianchi and Ggbor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
A Philip Dawid. Present position and potential developments: Some personal views statistical theory
the prequential approach. Journal of the Royal Statistical Society: Series A (General), 147(2):
278-290, 1984.
Isaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift. arXiv
preprint arXiv:2106.00170, 2021.
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration
and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):
243-268, 2007.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017.
Ursula HCbert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Cal-
ibration for the (computationally-identifiable) masses. In International Conference on Machine
Learning, pp. 1939-1948. PMLR, 2018.
Volodymyr Kuleshov and Stefano Ermon. Estimating uncertainty online against an adversary. In
Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. In International Conference on Machine Learning, pp. 2796-2804.
PMLR, 2018.
Meelis Kull, Miquel Perello-Nieto, Markus Kangsepp, Hao Song, Peter Flach, et al. Beyond temper-
ature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration. arXiv
preprint arXiv:1910.12656, 2019.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems, pp. 6402-6413, 2017.
Nicolas Minorsky. Directional stability of automatically steered bodies. Journal of the American
Society for Naval Engineers, 34(2):280-309, 1922.
Allan H Murphy. A new vector partition of the probability score. Journal of applied Meteorology,
12(4):595-600, 1973.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learn-
ing. In Proceedings of the 22nd international conference on Machine learning, pp. 625-632.
ACM, 2005.
Harris Papadopoulos. Inductive conformal prediction: Theory and application to neural networks.
INTECH Open Access Publisher Rijeka, 2008.
John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. Advances in large margin classifiers, 10(3):61-74, 1999.
10
Under review as a conference paper at ICLR 2022
Evan L Ray, Nutcha Wattanachit, Jarad Niemi, Abdul Hannan Kanji, Katie House, Estee Y Cramer,
Johannes Bracher, Andrew Zheng, Teresa K Yamana, Xinyue Xiong, et al. Ensemble forecasts of
coronavirus disease 2019 (covid-19) in the us. MedRXiv, 2020.
Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. Ad-
Vances in Neural Information Processing Systems, 32:3543-3553, 2019.
Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning
Research, 9(Mar):371-421, 2008.
Hao Song, Tom Diethe, Meelis Kull, and Peter Flach. Distribution calibration for regression. In
International Conference on Machine Learning, pp. 5897-5906. PMLR, 2019.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel J Candes, and Aaditya Ramdas. Conformal
prediction under covariate shift. arXiv preprint arXiv:1904.06019, 2019.
Philippe Van Kerm. Adaptive kernel density estimation. The Stata Journal, 3(2):148-156, 2003.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic learning in a random world.
Springer Science & Business Media, 2005.
Vladimir Vovk, Ivan Petej, Paolo Toccaceli, Alexander Gammerman, Ernst Ahlberg, and Lars Carls-
son. Conformal calibrators. In Conformal and Probabilistic Prediction and Applications, pp.
84-99. PMLR, 2020.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classifiers. In Icml, volume 1, pp. 609-616. Citeseer, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass proba-
bility estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pp. 694-699. ACM, 2002.
Shengjia Zhao, Tengyu Ma, and Stefano Ermon. Individual calibration with randomized forecasting.
arXiv preprint arXiv:2006.10288, 2020.
Shengjia Zhao, Michael P Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating predic-
tions to decisions: A novel approach to multi-class calibration. arXiv preprint arXiv:2107.05719,
2021.
11
Under review as a conference paper at ICLR 2022
A Proofs
Theorem 1. Let Zk be generated by any forecasting algorithm and bounded in [—B, B], for any
δ > 0, the prediction algorithm defined by Eq.(3)is b-calibrated where b(T) = b1(T) + Iog(2弓+1)+e.
Proofof Theorem 1. We prove that at any time t We cannot have ∣Fk — αkt| > 货(t) + log(1 +
2B)∕β + 1. This is equivalent to two statements that we prove separately by induction
Fk ≥ αkt — bδ(t)- log(1 + 2B)∕β — 1	(7)
Fk ≤ αk t+bδ ⑴+IOg(I+2B)/e+1	⑻
To prove Eq.(7) suppose at time t we have Fk ≥ αkt —疗(t) — log(1 + 2B)∕β — 1, then there are
two situations:
Case 1.	Fk ≥ αkt — bg (t) — log(1 + 2B)∕β, then we naturally have
Fk+1 ≥ Fk ≥ αk t- bδ ⑴一lOg(I+2B)/e > αk (t+I)- bδ (t+I)- log(1 +2B)/e — 1
(9)
Case 2.	Fk < αkt — bδ(t) — log(1 + 2B)∕β, then by Eq.(3) we have
Ekt+1 > 1 + 2B — 1 = 2B
By the boundedness assumption,
Zt+1 ≤ B < —B + Ek+1 < Ztt+1 + Ek+1 = Yt+1
so we must have Fkt+1 = Fkt + 1, so
Fk+1 = Fk + 1 ≥ αkt — bδ(t) — IOg(I + 2B)/e > αk(t + 1) — bδ(t) — IOg(I + 2B)/e — 1
(10)
Similarly to prove Eq.(8) suppose at time t we have Fk ≤ αkt + 疗(t) + log(1 + 2B)/e + 1 then
at time t + 1 we must have
Fk+1 ≤ αk (t + 1) + bδ (t + 1) + IOg(I + 2B)/e + 1	(II)
□
Theorem 2. Suppose the initial predictions arefeasιble (ι.e. Zk < Zk+1, ∀t, k). For some C > 2/e,
Algorithm 1 is b-calibrated where b(T) = (2Kbδ (T) + Kc/T, 0) on any transcript that satisfies
∀t ≤ T,L(Yt,Zt) ≤ 2η⅛7(eβ(C-I)/2 — 4B).
Proof of Theorem 2. We first sketch our proof strategy. The proof will follow 3 steps:
1.	We prove that if an interval contained the label too often, there will be a large force difference
Ekk+1 — Ekk .
2.	We prove that if there is a large force difference, then the interval will be small.
3.	If at time t an interval contains the label N times too often, at some previous time step t0 ≤ t, it
must have contained the label N — 1 times too often and contained the label once again. However,
at t0 the interval must have been small, hence the likelihood of the label must be large. Therefore,
unless the likelihood is large, no interval can contain the label too often.
First we show a simple Lemma based on boundedness assumptions. This Lemma is a simple conse-
quence of the boundedness Y0 = —B, YK+1 = B. We do not use this Lemma directly but it will be
used in the proof of the other Lemmas.
Lemma 1. ∀t, ∀k = 0, ∙∙∙ ,K + 1, Ak < B, Bk > —B, |Dk | < B
The following Lemma shows that intuitively, if the ground truth label belongs to an interval too
often, then there Ekk+1 must be significantly less than Ekk
12
Under review as a conference paper at ICLR 2022
Lemma 2. For any c > 2∕β, if Fkt+ι - Fk = ∆αkt + 2埸(t) + C then Etk++ι - Ett+1 < -eβc/2.
Next we show that if the forces Ekt+1 is significantly smaller than Ekt, then intuitively, this difference
in force will “compress“ the interval, making it small.
Lemma 3. For any L > 0, if Ek +ι - Ek < -L we have ∆Z1t∕∆Y; > L-；B.
Based on these lemmas We can complete the proof. Suppose for some k ∈ 1,…，K and c > 2 We
have Fk+ι - Fk = ∆αkt + 2b^ (t) + c, then consider last time that the label belongs to the interval,
i.e. We Want to find the largest t0 ≤ t such that Z t0 ∈ [Ykt0,Ykt+0 1).For t0 We have
Fkt0+-11 - Fkt0-1 = Fkt+1 - Fkt - 1	(12)
> ∆αkt + 2bδ(t) + c — 1	(13)
=∆αk t0 + 2b^(t0) + (c + ∆ακ (t — t0) + 2bδ(t) — 2bδ(t0) — 1)	(14)
≥ ∆αkt0 + 2bg (t0) + C — 1	(15)
Where the last inequality is because bδ is monotonically non-decreasing in t. Therefore by Lemma 2
We have
Ek0+1 - Ek0 < -eβ(CT)/2
So by Lemma 3 We can conclude
(16)
	~0	O	eβ(CT)/2 - 4B 2η	(17)
which implies whenever eβ(CT)/2 > 4B, or c > 2log(4B)∕β + 1
∆Y∕t, < 2η∆Zk (e-β(CT)/2 - 4B) < 2ηB(e-β(c-1)/2 - 4B) = 2ηB(e-β(c-1)/2 - 4B) (18)
Hence. by the definition of the “likelihood“
L(Yt0, Zt0)> YB αk (eβ(cτ"2 - 4B) =	B(K	(eβ(CT)/2 -4B)	(19)
2ηB	2ηB (K + 1)
In other words, unless ∃t0 such that L(Yt',Zt') > 2ηB(K+i) (eβ(C-I)/2 - 4B), we cannot have
Fk+1- Fk =∆αkt + 2bδ(t)+ c.	”
Finally we connect the bound on Fkt+1 - Fkt with the quantile calibration error.
Lemma 4. For any b > 0, if∀k ∈ K, Fkt+1 - Fkt ≤ ∆αkt + b, then
max |Fk — akt∣ ≤ Kb	(20)
k
So combined we have if ∀t,L(Yt, Zt) ≤ 2B(K+1) (e(CT)/2 - 4B), then
max |Fk — akt∣ ≤ 2Kbδ(t) + KC	(21)
k
Finally we prove the lemmas used in this theorem.
ProofofLemma 2. Suppose Fkt+ι — Fk = ∆αkt + 2bδ(t) + C We first use simple algebraic trans-
formation to get
(Fk+1 - αk+It)-(Fk - αkt) = C + 2bKt)	(22)
For notation simplicity denote the first term LHS as u = Fkt+1 - αk+1t and second term as v =
Fk — akt, we can simplify the equation as so U — V = 2疗(t) + c.
First we consider the scenario where the interval is not the first interval α0 , α1 or the last interval
αK, αK+1, in other words, we consider the situation where k ∈ [1, K - 1]. There are three different
13
Under review as a conference paper at ICLR 2022
possible situations: U ≥ 0, v ≥ 0 and U ≥ 0,v < 0 and u < 0,v < 0. Note that we can never have
u < 0,v ≥ 0. We separately consider the three situations.
1.	If u ≥ 0, v ≥ 0 then
Ek+1 - Ek+1 = -eβ(u-bδ(t)) + 1 - (-eβ(VY(t)) + 1)	(23)
=-eβ(u-bδ(t)) + eβ(V-bδ(t)) V 1 - eβ(c+bδ ⑴)	(24)
where the final inequality achieves equality when U = C + 2b^(t), v = 0
2.	If u ≥ 0, v < 0 then
E+1 - Ek+1 = -eβ(u-bδ(t)) + 1 - (e-β(v+bδ (t)) - 1)	(25)
=-eβ(UY⑴)-e-β(V+bδ⑴)+ 2 < -2eβ°∕2 + 2	(26)
where the final inequality achieves equality when u = (c + 2bδ(t))∕2,v = -(c +2bδ(t))∕2.
3.	If u < 0,v < 0 then
Ek+1 - Ek+1 = (e-β(u+bδ(t)) - 1) - (e-β(v+bδ(t)) - 1)
=e-β(u+bδ(t)) - e-(v+bδ(t)) ≤ 1 - eβ(c+bδ(t))
(27)
(28)
where the final inequality achieves equality when U = 0,v = -c - 2%(t).
Next we consider the situation where k = 0. In this case V = Fo - α^t = 0 by assumption. So
Et+1 - Et+1 = Et+1 = -eβ(c+2bδ (t)-bδ(t)) + 1 = -eβ(c+bδ(t)) + 1	(29)
Next we consider the situation where k = K. Similar to the previous argument, U = 0 by assump-
tion, so
EK+1 - Et+1 = -Ej+1 = -(ee(c+闻(t)Y(t)) - 1) = 1 - eβ(c+bδ(t))	(30)
Combined we have ∀k, we take the maximum of the lower bounds and have
Eg； - Ek+1 < max(-2eβc/2 +2,1 - ee(c+埸(t)))	(31)
Since c > 2∕β we can further simplify this expression by observing -2eβc/2 + 2 < -eβc/2 and
1 - eβ(c+bδ(t)) < 1 - eβc < eβc/2, so we have
Ek+1 - Ek+1 < max(-2eβc/2 + 2,1 - eβ(c+bδ(t))) < -eβc/2	(32)
□
ProofofLemma 3. Observe the equilibrium equation Dk + Bk + Ak + Ek = 0 and Dfc+1 + Bfc+1 +
Ak+1 + Ek+1 = 0; by Lemma 1
Ak < B,Bk > -BjDk I < B
so consequently we have
Ak - Bk+1 = Dk+ 1 + Ek+1 + Bk+1 - Dk - Ek - Ak < -L + 4B
By the definition of Ak and Bk+1 we have
2η (要-∆⅜)<-L+4b
(33)
which implies that
.~ _ _ .__________ _ _
∆Zk	L - 4B	∆Yk	L - 4B
∆Yk > ~^Γ + ∆Zk > ~^Γ
(34)
□
14
Under review as a conference paper at ICLR 2022
ProofofLemma 4. We prove the contra-positive statement of the Lemma. If maxk ∖Fjt-ak t| ≤ Kb
is violated, there must be one of two situations:
1.	there exists some k such that Fkt - αkt ≥ Kb then
Kb ≤ Fkt - αkt - F0t +α0t	F0t ≡ 0,α0t ≡ 0	(35)
= X (Fjt - Fjt-1) - (αj - αj-1)t	Telescope (36)
0≤j≤k
≤ (k + 1) max(Fjt - Fjt-1) - (αj - αj-1)t	(37)
j≤k
2.	there exists some k such that Fkt - αkt ≤ -b then
Kb ≤ -Fkt + αkt + FKt +1 - αK+1t	FKt +1 ≡ t, αK+1t ≡ t (38)
= X (Fjt - Fjt-1) - (αj - αj-1)t	Telescope (39)
k≤j≤K+1
≤ (K + 1 - k) max(Fjt - Fjt-1) - (αj - αj-1)t	(40)
j≥k
In both cases, there must be some j such that
(Fjt - Fjt-1 ) - ∆αj-1t ≥ b	(41)
□
□
Corollary 1. Suppose the initial predictions are feasible (i.e. Zkt < Zkt+1, ∀t, k). For some c >
2∕β,the prediction algorithm in Section 3.3 is b-calibrated where b(T) = 2Kb (T) + Kc/T on any
transcript that∀t,L(Yt,Zt) ≤ 2ηB(K+1)"S-1"2 - 6B).
Proof of Corollary 1. To prove this corollary we only have to modify Lemma 2, and the rest of the
proof follows as Theorem 2.
Lemma 5. For any C > 2β, if Fk+1 一 Fk = ∆αk + 2货(t) + C then Ep+1-k+ι - EPID_k <
-eβc/2 + 2B.
Lemma 5 holds because EPt+ID1-k differs from Ekt+1 by at most B, so
Ep+D-k+1 - Ep+D-k < Ek+1 - Ek+1 + 2B< -eβc/2 + 2B	(42)
□
B	Additional Experiment Results
B.1 Hyper-parameter Tuning
We tune hyper-parameters that minimize a weighted combination of the expected calibration error
(ece) and the pinball loss.
Lmix(wmix) = wmixLece + (1 - wmix )Lpinball	(43)
where wmix is a weight coefficient we choose to be 0.4. We use Optuna (Akiba et al., 2019) to seek
promising hyper-parameter candidates. We try 100 hyper-parameter candidates and pick the best
hyper-parameter that minimizes the loss function Lmix(wmix).
As we mentioned in Section 3.4, we use simple synthetic tasks for tuning. We first define the label
of a task as follows:
Yt = a + bet, Et 〜N(0, 1) (t = 1, .. ., T)	(44)
15
Under review as a conference paper at ICLR 2022
weight (Wmix)
0.0	0.2	0.4	0.6	0.8	1.0
pinball
calibration(ece)
sharpness
0.155 0.155 0.159 0.155 0.166 0.178
0.021 0.015 0.009 0.022 0.007 0.008
0.811 0.826 0.859 0.812 0.921 0.972
Table 2: Comparison of performance metrics between weight wmix .
Figure 6: Comparison of loss values between synthetic data and other datasets, where each point
corresponds to a hyper-parameter candidate which is obtained during hyper-parameter search. The
x-axis and y-axis show loss values on synthetic dataset and other datasets, respectively. The perfor-
mance on simple synthetic data is highly correlated with the performance on real data.
where coefficients a and b are sampled by a 〜N(0,1) and b 〜U(0,1). We also set predictions
Zt = F-1(αk) where F is the cumulative distribution function of standard Normal distribution.
After tuning, we evaluate the best hyper-parameters for each wmix on other synthetic tasks. Ta-
ble 2 compare performance metrics between different wmix . Large wmix leads to small calibra-
tion loss values and large pinball loss/sharpness values. We adopt wmix = 0.4 since the hyper-
parameter provides good calibration and pinball values. The obtained hyper-parameter values are
δ = 0.47, β = 0.16, kd = 0.08, kp = 1.00, η = 0.96. As for ki, we assume ki depends on quantile
level k as follows:
ki,k = ki,max - (ki,max - ki,min)|1 - 2(k - 1)/(K - 1)|, ∀1 ≤ k ≤ K.	(45)
The obtained hyper-parameters are ki,max = 0.09, ki,min = 0.04.
Next we verify that our algorithm is robust to hyper-parameter choice. In Figure 6 we compare the
performance of a hyper-parameter on synthetic sequences and real data. They are highly correlated,
which indicates that hyper-parameters that do well on synthetic sequences also perform well on real
data.
Finally, we demonstrate that the adopted hyper-parameter in the paper works well compared to
“optimal” hyper-parameters for each dataset. Concretely, we tune hyper-parameter on regression
benchmark datasets with distribution drifts, and compare the tuned hyper-parameter and the recom-
mended ones (which are tuned on synthetic sequences). Figure 7 shows that tuning on each dataset
separately (which is only possible for off-line data) only improves performance marginally com-
pared to using fixed hyper-parameters. This suggests that the recommended hyper-parameter is a
reasonable choice in a variety of cases.
B.2 Visualizations of predictions on various datasets
In addition to Figure 3 in Section 5, we visualize additional example predictions on COVID and re-
gression benchmark datasets. Figure 8 shows example predictions on the COVID prediction dataset,
and Figure 9 shows example predictions on regression benchmark datasets with distribution drifts.
Our method adapts to distribution drifts.
16
Under review as a conference paper at ICLR 2022
Figure 7: Evaluating the robustness to hyper-parameter choice. The x-axis and y-axis shows the
pinball loss and ece, respectively. The star is the hyper-parameter we fix throughout the experiments.
The red crosses are the hyper-parameters that are optimized on regression benchmark datasets for
each distribution drift type respectively. Each red cross in a figure corresponds to different weight
wmix . Even though optimizing hyper-parameters for each dataset separately can further improve
performance, the improvement is minor.
10
8
25% quantile
75% quantile
observed label
25% quantile
--75% quantile
observed label
Figure 8: Example predictions on the COVID prediction dataset. Left is conformal calibration only,
and right is conformal calibration + our method. We plot both the sequence of predictions, and
the reliability diagram (i.e. the relationship between target frequency αk and empirical frequency
FkT /T).
17
Under review as a conference paper at ICLR 2022
—25% quantile
75% quantile
true label
—25% quantile
75% quantile
true label
----25% quantile
—75% quantile
true label
Figure 9: Example predictions on regression benchmark datasets with distribution drifts. From top
to bottom, we apply ’linear shift’, ’scale shift’, ’jump’, and ’cycle’, respectively. We plot both the
sequence of predictions, and the reliability diagram (i.e. the relationship between target frequency
αk and empirical frequency FkT /T).
18
Under review as a conference paper at ICLR 2022
C Experiment Details
Stock Dataset For stock we predict quarterly earnings per share (EPS) for S&P 500 stocks from
2010/01 to 2021/03. We use analyst earnings forecasts provided by I/B/E/S as the original predic-
tion. To remove stocks with low quality data, we apply the following filtering rule for each stock.
1.	exclude analyst forecasts whose missing rates are over 20%
2.	if the number of analysts is less than five, remove the stock
3.	if all remaining analysts do not predict earnings at least a quarter, remove the stock
By the above procedure, 279 stocks are selected from 500 stocks in S&P 500. The average number
of analysts per stocks is 9.3 and each time series length is 45 data points.
COVID dataset For COVID we predict COVID cases for the next week. We use expert forecasts
from the COVID forecast hub (Ray et al., 2020). Because the data is heavy tailed, we transform all
forecasts and labels by Z 7→ log(1 + Z) and predict the transformed label. We select the teams and
locations that do not have missing predictions during the period between Aug 2020 and Aug 2021
(6 teams) as the original prediction, and randomly sample 100 locations. The time series length is
56 data points (so approximately one year of data).
Regression Benchmarks We use the following UCI datasets: blog, boston, concrete, crime,
energy-efficiency, fbcomment-1, fbcomment-2, forest-fires, mpg, naval, power-plant, protein, su-
perconductivity, wine, and yacht. In addition to UCI datasets we also use two common regression
datasets in the uncertainty quantification literature, which are kin8nm and medical-expenditure. We
use 100 samples for each test dataset, i.e., the length is 100.
Distribution drifts For the regression benchmarks we consider four types of artificial distribution
drifts. The definition of the distribution drift is given by
•	linear shift: add linear drifts to the labels, i.e., Yt = Yt + βt
•	scale shift: change the scales of the labels, i.e., Yt = Yt(1 + β√7)
•	jump: jump the mean at the halfway point, i.e., Yt = Yt (t < T /2), Yt = Yt + β (t ≥
T/2)
•	cycle: add fluctuations to the labels, i.e., Yt = Yt + β sin(2∏t∕T)
In the experiments, we set coefficient β = 0.1, 1.0, 3.0, 3.0 for linear shift, scale shift, jump, and
cycle, respectively.
C.1 Downstream Task Details
COVID Decision Making We consider the following simple decision loss: for each county, there
are four tiers of response 0,1,2,3. A higher tier response is needed whenever there are more fore-
casted cases. Let a denote the response, and z denote the (log) cases, we choose the loss function as
the sum of COVID losses and shut-down losses, respectively defined as
		a = 0	a = 1	a = 2	a = 3
COVID loss	-z≤4-	-0	-0	-0	-0
	4 < z ≤ 6	1	0	0	0
	6 < z ≤ 8	3	2	0	0
	z> 8	6	5	3	0
Shutdown loss		-0-	0.3	0.7	1.0
C.2 Online Learning Baseline
We adopt Algorithm 1 in (Kuleshov & Ermon, 2017) based on internal regret minimization, we
use exactly the same algorithm except that (Kuleshov & Ermon, 2017) minimizes the L2 loss or
Brier loss (which are proper scoring rules for the conditional expectation), while we minimize the
19
Under review as a conference paper at ICLR 2022
pinball loss (which is the proper scoring rule for quantiles). In addition, we separately calibrate the
different quantiles Y1, ∙∙∙ ,YK (i.e. for each k We run a separate instance of modified Algorithm 1
in (Kuleshov & Ermon, 2017)).
The original procedure in (Kuleshov & Ermon, 2017) can guarantee calibration in the binary classifi-
cation setting; We give intuition on Why our modification can guarantee calibration in the regression
setting. We first bin the possible regression outputs into M equal sized bins -B
yo < …<
yM = B . As a notation convenience, denote the center of each bin as ymc
ym + ym— 1
2
Consider the αk-th quantile, our goal is to minimize the interval regret under the pinball loss, define
by∀c ∈ [-B, B]
Rinterval =	I(Ykt = c)Lpinball (c, Zt) - inyf	I(Ykt = c)Lpinball(y, Zt)
t	yt
Intuitively, among the time that the true label equals c there is not a better alternative prediction y
that achieves loWer pinball loss. Because pinball loss is a proper scoring rule, Ricnterval = 0 implies
that c is the αk-quantile of {Zt | Ykt = c}, i.e. for this subset of samples, the true label Zt is beloW
the prediction Ykt = c for αk proportion of times. This is exactly the definition of calibration. To
implement the algorithm realistically, We have to choose a suitable binning strategy, as Well as a
internal regret minimization algorithm. We use the same choice as the experiments in (Kuleshov &
Ermon, 2017). Note that to choose the binning strategy We have to knoW the loWer bound and upper
bound of the data. We give the baseline an unfair advantage by choosing the loWer bound as the
loWest value in the data, and the upper bound as the highest value in the data (up to a 10% margin).
20