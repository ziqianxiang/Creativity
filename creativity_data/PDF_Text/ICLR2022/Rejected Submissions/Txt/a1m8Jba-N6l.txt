Under review as a conference paper at ICLR 2022
k-MIXUP REGULARIZATION FOR DEEP LEARNING VIA
Optimal Transport
Anonymous authors
Paper under double-blind review
Ab stract
Mixup is a popular regularization technique for training deep neural networks that
can improve generalization and increase adversarial robustness. It perturbs input
training data in the direction of other randomly-chosen instances in the training
set. To better leverage the structure of the data, we extend mixup to k-mixup by
perturbing k-batches of training points in the direction of other k-batches using
displacement interpolation, i.e. interpolation under the Wasserstein metric. We
demonstrate theoretically and in simulations that k-mixup preserves cluster and
manifold structures, and we extend theory studying the efficacy of standard mixup
to the k-mixup case. Our empirical results show that training with k-mixup further
improves generalization and robustness across several network architectures and
benchmark datasets of differing modalities.
Standard mixup (Zhang et al., 2018) is a data augmentation approach that trains models on weighted
averages of random pairs of training points. Averaging weights are typically drawn from a beta
distribution β(α, α), with parameter α such that the generated training set is vicinal, i.e., it does not
stray too far from the original dataset. Perturbations generated by mixup may be in the direction of
any other datapoint instead of being informed by local distributional structure. As shown in Figure 1,
this is a key weakness of mixup that can lead to poor regularization when distributions are clustered
or supported on an embedded manifold. With larger α, the procedure can result in averaged training
points with incorrect labels in other clusters or in locations that stray far from the data manifold.
w ∙
4-mixup	32-mixup
Figure 1: We train a fully-connected network on three synthetic datasets for binary classification,
with 1-mixup through 32-mixup regularization (α = 1). The plotted functions show the network
output and demonstrate that higher k-mixup better captures local structure (visible through less blur,
increased contrast) while retaining reasonable, even smoothing between the classes.
To address these issues, we present k-mixup, which averages random pairs of sets of k samples from
the training dataset. This averaging is done using optimal transport, with displacement interpolation.
The sets of k samples are viewed as discrete distributions and are averaged as distributions in a
geometric sense. If k = 1, we recover standard mixup regularization. Figures 1 and 2 illustrate how
k-mixup produces perturbed training datasets that better match the global cluster or manifold support
structure of the original training dataset. Note that the constraints of optimal transport are crucial, NEW
1
Under review as a conference paper at ICLR 2022
as for instance a nearest-neighbor approach would avoid the cross-cluster matches necessary for
smoothing. In Section 4, we provide empirical results that justify the above intuition.
Our contributions are as follows.
• Empirical results:
-	We show improved generalization results on standard benchmark datasets showing k-mixup with
k > 1 often improves on standard mixup and at worst does no harm.1
-	We show that k-mixup further improves robustness to adversarial attacks.
• Theoretical characterizations that speak to the claims above in a more precise manner:
-	We argue that as k increases, one is more and more likely to remain within the data manifold
(Section 2.1).
-	In the clustered setting, we provide an argument that shows intercluster regularization interpolates
nearest points and better smooths interpolation of labels (Section 2.2).
-	We extend the theoretical analysis of Zhang et al. (2020) and Carratino et al. (2020) to our
k-mixup setting, showing that it leverages local training distribution structure to make more
informed regularizations (Section 3).
Related works. We tackle issues noted in the papers on adaptive mixup (Guo et al., 2019) and
manifold mixup (Verma et al., 2018). The first refers to the problem as “manifold intrusion” and
seeks to fix it by training datapoint-specific weights α and considering convex combinations of
more than 2 points. This requires additional networks to detect the data manifold and to assign
weight combinations, adding substantial complexity to their approach. Manifold mixup deals with
the problem by relying on the network to parametrize the data manifold, interpolating in hidden
layers of the network. We show in Section 4 that k-mixup can be performed in hidden layers to
boost performance of manifold mixup. A related approach is that of GAN-mixup (Sohn et al., 2020a) NEW
which trains a conditional GAN and uses it to generate data points between different data manifolds.
As with the other approaches above, the need for training of an additional GAN results in far greater
complexity than our k-mixup method.
PuzzleMix (Kim et al., 2020) also combines optimal transport ideas with mixup, extending CutMix
(Yun et al., 2019) to combine pairs of images. PuzzleMix uses transport to shift saliency regions of
images, producing meaningful combinations of input training data. Their use of OT is fundamentally
different from ours and does not generalize to non-image data. A recent follow-on work CoMix (Kim
et al., 2021), considers a similar approach based on sub/supermodular optimization.
Performing optimal transport between empirical samples of distributions has been considered in NEW
studies of the sample complexity of Wasserstein distance (e.g. Weed & Bach (2019)). Ours is the
first application where the underlying source and target distribution are the same, and a theoretical
investigation of the generalized notion of k-variance is in Solomon et al. (2020). In other works,
transport between empirical samples has been dubbed minibatch optimal transport and has been used
in generative models (Genevay et al., 2018; Fatras et al., 2020) and domain adaptation (Damodaran
et al., 2018; Fatras et al., 2021).
1	Generalizing Mixup
Standard mixup. Mixup uses a training dataset of feature-target pairs {(xi, yi)}iN=1; the target yi is
a one-hot vector for classification. Weighted averages of training points construct a vicinal dataset:
(xij ,yij ) := (λxi + (1 - λ)xj, λyi + (I- λ)yj).
λ is sampled from a beta distribution, β(α, α), with parameter α > 0 usually small so that the averages
are near an endpoint. Using this vicinal dataset, empirical risk minimization (ERM) becomes:
Emix (f ):= Ei,j,λ [' (f(xj) ,yj)] , i,j 〜U{1,...,N },λ 〜β(α,α),	(1)
where f is a proposed feature-target map and ` is a loss function. Effectively, one trains on datasets
formed by averaging random pairs of training points. As the training points are randomly selected,
1 Where α is optimized for both methods.
2
Under review as a conference paper at ICLR 2022
this construction makes it likely that the vicinal datapoints may not reflect the local structure of the
dataset, as in the clustered or manifold-support setting.
k-mixup. To generalize mixup, we sample two random subsets of k training points {(xiγ, yiγ)}ik=1
and {(xiξ, yiξ)}ik=1. For compactness, let xγ := {xiγ}ik=1 and yγ := {yiγ}ik=1 denote the feature and
target sets (and likewise for ξ). A weighted average of these subsets is formed with displacement
interpolation and used as a vicinal training set. This concept is from optimal transport (see, e.g.,
(Santambrogio, 2015)) and considers (xγ, yγ) and (xξ, yξ) as uniform discrete distributions μγ, μξ
over their support. In this setting, the optimal transport problem becomes a linear assignment problem
(Peyre & Cuturi, 2019). The optimal map is described by a permutation σ ∈ Sk minimizing the cost:
1k
W22(μγ ,μξ ) = k EkxY -χ5(i)k2.
k i=1
Here, σ can be found efficiently using the Hungarian algorithm (Bertsimas & Tsitsiklis, 1997). Figure
2 gives intuition for this identification. When compared to the random matching used by standard
mixup, our pairing is more likely to match nearby points and to make matchings that better respect
local structure—especially by having cross-cluster matches between nearby points on the two clusters.
k = 1: (1-)mixup
k = 32: k-mixup
Figure 2: Optimal transport couplings and vicinal datasets for k = 1 (left) and k = 32 (right) in 3
simple datasets. In the bottom row, α = 1 was used to generate vicinal datasets of size 512.
Given a permutation σ and weight λ, the displacement interpolation between (xγ , yγ) and (xξ , yξ) is:
DIλ((xγ, yγ), (xξ,yξ)) := nλ(xiγ, yiγ) + (1 - λ)(xξσ(i), yσξ (i))o	.
As in standard mixup, We draw λ 〜β(α,α). For the loss function, We consider all (NN) subsets of k
γ γ k	(Nk)
random samples, i.e., {{(xiγ, yiγ)}ik=1}γ=k 1 :
Emix(f ) = Eγ,ξ,λ ['(f(DIλ(xγ ,xξ )),DIλ(yγ ,yξ))] , γ,ξ 〜U{1,..., (N )},λ 〜β(α,α).
The localized nature of the matchings makes it more likely that the averaged labels Will smoothly
interpolate over the decision boundaries. A consequence is that k-mixup is robust to higher values of
α, since it is no longer necessary to keep λ close to 0 or 1 to avoid erroneous labels. This can be seen
in our empirical results in Section 4, and theoretical analysis appears in Section 3.
Pseudocode and Computational Complexity We Would like to emphasize the speed and simplicity NEW
of our method (especially relative to other mixup variants). We have included a brief PyTorch
pseudocode in supplement Section A, and note that With CIFAR-10 and k = 32 the use of k-mixup
added 8-9 seconds per epoch. Also in that section is a more extended analysis of computational cost,
shoWing that there is little computational doWnside to generalizing 1-mixup to k-mixup.
2	Manifold and Cluster Structure Preservation
The use of an optimal coupling for producing vicinal data points makes it likely that vicinal datapoints
reflect the local structure of the dataset. BeloW We argue that as k increases, the vicinal couplings
Will preserve manifold support, preserve cluster structure, and interpolate labels betWeen clusters.
3
Under review as a conference paper at ICLR 2022
2.1	Manifold support
Suppose our training data is drawn from a distribution μ on a d-dimensional embedded submanifold
S in X × Y ⊂ RM, where X and Y denote feature and target spaces. We define an injectivity radius:
Definition 1 (Injectivity radius). Let B (S) = {p ∈ X × Y | d(p, S) < } denote the -neighborhood
of S where d(p, S) is the Euclidean distance from p to S. Define the injectivity radius RS of S to be
the infimum of the ’s for which B(S) is not homotopy equivalent to S.
As S is embedded, B(S) is homotopy equivalent to S for small enough , so RS > 0. Essentially,
Definition 1 grows an neighborhood until the boundary intersects itself. We have the following:
Proposition 1. For a training distribution μ supported on an embedded submanifold S with injectivity
radius RS, with high probability any constant fraction 1 - δ (for any fixed δ ∈ (0, 1]) of the couplings
induced by k-mixup will remain within B (S), for k large enough.
The proof of this proposition is in supplement Section B. Hence, for large enough k, the interpolation
induced by optimal transport will approximately preserve the manifold support structure. While the
theory requires low dimension d and high k to achieve a tight bound, our empirical evaluations show
good performance in the small-k regime. Some schematic examples are shown in Figures 1 and 2.
2.2	Clustered distributions
With a clustered training distribution we preserve global structure in two ways: by mostly match-
ing points within clusters and by matching (approximately) nearest points across clusters. These
characterizations are only true approximately, but are achieved exactly as k → ∞, as argued below.
To refine the definition of a clustered distribution, we adopt the (m, ∆)-clusterable definition used
in Weed & Bach (2019); Solomon et al. (2020). In particular, a distribution μ is (m, ∆)-clusterable
if supp(μ) lies in the union of m balls of radius at most ∆. Now, if our training samples (xi, yi)
are sampled from such a distribution, where the clusters are sufficiently separated, then intracluster
matchings will be prioritized over cross-cluster matchings under optimal transport.
Lemma 1. Draw two batches of samples {pi}iN=1 and {qi}iN=1 from a (m, ∆)-clusterable distribution,
where the distance between any pair of covering balls is at least 2∆. If ri and si denote the number
of samples in cluster i in batch 1 and 2, respectively, then the optimal transport matching will have
2 E/ri — si| cross-cluster matchings.
The proof of the above (supplement Section C) involves the pigeonhole principle and basic geometry.
We also argue that the fraction of cross-cluster identifications approaches zero as k → ∞ and
characterize the rate of decrease. The proof (supplement Section D) follows via Jensen’s inequality:
Theorem 1. Given the setting of Lemma 1, with probability masses p1, . . . , pm, and two batches
of size k matched with optimal transport, the expected fraction of cross-cluster identifications is
O ((2k)-1∕2 pm=ι Ppi(I- Pi)).
Note that the absolute number of cross-cluster matches still increases as k increases providing more
information in the voids between clusters. We emphasize that the (m, ∆) assumption is designed
to create a “worst-case scenario” i.e. a setting where clusters are well-separated such that cross-
cluster identifications are as unlikely as possible. Hence in real datasets the fraction of cross-cluster
identifications will be larger than indicated in Theorem 1. Finally, we show that these cross-cluster
matches are of length close to the distance between the clusters with high probability (proved in
supplement Section E), i.e., the endpoints of the match lie in the parts of each cluster closest to
the other cluster. This rests upon the fact that we are considering the W2 cost, for which small
improvements to long distance matchings yield large cost reductions in squared Euclidean distance.
NEW
NEW
Theorem 2.	Suppose density p has support on disjoint compact sets (clusters) A, B whose boundaries
are smooth, where p > 0 throughout A and B. Let D be the Euclidean distance between A and B,
and let RA, RB be the radii of A, B respectively. Define A to be the subset of set A that is less than
D(1 + ) distance from B, and define B similarly. Consider two batches of size k drawn from p and
matched with optimal transport. Then, for k large enough, with high probability all cross-cluster
matches will have an endpoint each in A and B, where
max(RA ,RB )2
D2
4
Under review as a conference paper at ICLR 2022
Theorem 2 implies for large k that the vicinal distribution created by sampling along the cross-cluster
matches will almost entirely lie in the voids between the clusters. If the clusters correspond to
different classes, this will directly encourage the learned model to smoothly interpolate between the
class labels as one transitions across the void between clusters. This is in contrast to the randomized
matches of 1-mixup, which create vicinal distributions that can span a line crossing any part of
the space without regard for intervening clusters. The behavior noted by Theorems 1 and 2 are
visualized in Figures 1 and 2, showing that k-mixup provides smooth interpolation between clusters,
and strengthens label preservation within clusters.
3 Regularization Expansions
Two recent works analyze the efficacy of 1-mixup perturbatively (Zhang et al., 2020; Carratino
et al., 2020). Both consider quadratic Taylor series expansions about the training set or a simple
transformation of it, and they characterize the regularization terms that arise in terms of label and
Lipschitz smoothing. We adapt these expansions to k-mixup and show that the resulting regularization
is more locally informed via the optimal transport coupling.
In both works, perturbations are sampled from a globally informed distribution, based upon all
other samples in the training distribution. In k-mixup, these distributions are defined by the optimal
transport couplings. Given a training point xi, we consider all k-samplings γ that might contain it,
and all possible k-samplings ξ that it may couple to. A locally-informed distribution is the following:
Di :
1
(N-M)
N-1
k-1
) (Nk)
γ=1 ξ=1
δσγξ(xi)
(
where σγξ denotes the optimal coupling between k-samplings γ and ξ . This distribution will be more
heavily weighted on points that xi is often matched with. We use “locally-informed” in this sense of
upweighting of points closer to the point of interest that are likely to be matched to it by k-mixup.
Zhang et al. (2020) expand about the features in the training dataset DX := {x1, . . . , xn}, and the
perturbations in the regularization terms are sampled from D. We generalize their characterization to
k-mixup, with D replaced by Di. We assume a loss of the form '(f (x), y) = h(f (x)) - y ∙ f (x) for
some twice differentiable h and f . This broad class of losses includes the cross-entropy for neural
networks and all losses arising from Generalized Linear Models.
Theorem 3.	Assuming a loss ` as above, the k-mixup loss can be written as:
3
Emix(f) = EStd + Ej = 1 Rj + Eλ〜β(α+1,α)[(1 - λ)2φ(1 - λ)]
where lima→0 φ(a) = 0, Estd denotes the standard ERM loss, and the three Ri regularization terms
are:
R1
R2
R3
Eλ~β(α+1,a)[1~λ] XN (h(f (Xiy) - yi)Nf(xi)TEr〜Di [r - Xi]
n	i=1
Eλ~β(α+1,α)[(1- λ)2] XN1 h00(f(xi))Vf (Xi)TEr〜Di[(r - Xi)(r - Xi)T]Vf (g)
2n	i=1
Eλ~β(α+1;""^λy~] XN (h0(f (Xi)) - yi)Er〜Di[(r - Xi)V2f (Xi)(r - Xi)T].
2n	i=1
A proof is given in Section F of the supplement and follows from some algebraic rearrangement and
a Taylor expansion in terms of 1 - λ. The higher-order terms are captured by Eλ〜β(α+ι,α) [(1 一
λ)2φ(1 - λ)]. Estd represents the constant term in this expansion, while the regularization terms
Ri represent the linear and quadratic terms. These effectively regularize Vf(Xi) and V2f(Xi) with
respect to local perturbations r - Xi sampled from Di , ensuring that our regularizations are locally-
informed. In other words, the regularization terms vary over the support of the dataset, at each point
penalizing the characteristics of the locally-informed distribution rather than a global distribution.
This allows the regularization to adapt better to local data (e.g. manifold) structure. For example, R2
and R3 penalize having large gradients and Hessian respectively along the directions of significant
variance of the distribution Di of points Xi is likely to be matched to. When as k = 1, this Di will
NEW
5
Under review as a conference paper at ICLR 2022
not be locally informed, and will instead effectively be a global variance measure. As k increases, the
Di will instead be dominated by matches to nearby clusters, better capturing the smoothing needs in
the immediate vicinity of xi . Notably, the expansion is in the feature space alone, yielding theoretical
results in the case of 1-mixup on generalization and adversarial robustness.
An alternate approach by Carratino et al. (2020) characterizes mixup as a combination of a reversion
to mean followed by random perturbation. In supplement Section G we generalize their result to
k-mixup via a locally-informed mean and covariance.
4 Empirical Results
We empirically test the efficacy of k-mixup with toy datasets, UCI datasets, image, and speech datasets
employing a variety of neural network architectures. Across these experiments we find that k-mixup
for k > 1 tends to improve upon (1-)mixup for fixed α, and if not remains comparable. Additional
experiments with CIFAR-10 compare k-mixup with manifold mixup, explore the combination of the
two methods, and demonstrate improvements in adversarial robustness from k-mixup. Experiments
were performed in various laptop, cluster, and cloud environments using PyTorch. Unless otherwise
stated, a standard SGD optimizer was used, with learning rate 0.1 decreased at epochs 100 and 150,
momentum 0.9, and weight decay 10-4.
Toy datasets. Results for the toy datasets of Figures 1 and 2 (denoted “One Ring,” “Four Bars,” and
“Swiss Roll”) are shown in Figure 3. We used a fully-connected 3-layer neural network (130 and
120 hidden units). As the datasets are very clustered and have no noise, smoothing is not needed for
generalization and performance without mixup is typically 100%. Applying mixup to these datasets
thus provides a view to the propensity of each variant to oversmooth, damaging performance. For
each dataset and each α, higher k-mixup outperforms (1-)mixup. For larger α, the performance gap
between the baseline (1-)mixup and k > 1 mixup becomes quite significant, reaching 8%, 50%, and
40% respectively for k = 16 and α = 64. These results quantitatively confirm the intuition built NEW
in Figures 1 and 2 that k-mixup regularization more effectively preserves these structures in data,
limiting losses from oversmoothing.
k	α = .25	a = 1	a = 4	α =16	ɑ = 64	α = .25	a = 1	a = 4	α =16	ɑ = 64	α = .25	a = 1	a = 4	α = 16	ɑ = 64
T~	94.487	90.930	86.527	86.467	86.899	-100-	^T00^	100-	50.110	50.017	99.708	99.670	67.942	61.633	59.573
2Γ	95.230	93.197	90.653	90.043	90.027	-1QQ-	^T00^	100-	60.725	50.107	99.653	99.667	81.696	69.327	64.397
4~	96.190	94.110	92.633	92.191	91.807	-100-	^T00^	lea-	98.750	92.477	99.703	99.717	99.721	80.064	73.773
8Γ	96.547	95.714	94.377	93.807	93.463	-100-	^T00^	lea-	99.920	99.853	99.693	99.69	99.646	99.710	98.897
16	96.717	95.961	95.097	95.017	95.193	100	~ro-	~ro-	99.997	99.987	99.760	99.707	99.692	99.787	99.761
(a) One Ring	(b) Four Bars	(c) Swiss Roll
Figure 3: Test accuracy on toy datasets, averaged over 5 Monte Carlo trials.
UCI datasets. Scaling up from the toy datasets, we tried k-mixup on UCI datasets (Dua & Graff,
2017) of varying size and dimension: Iris (150 instances, dim. 4), Breast Cancer Wisconsin-
Diagnostic (569 instances, dim. 30), Abalone (4177 instances, dim. 8), Arrhythmia (452 instances,
dim. 279), HTRU2 (17898 instances, dim. 9), and Phishing (11055 instances, dim. 30). For Iris,
we used a 3-layer network with 120 and 84 hidden units; for Breast Cancer, Abalone, and Phishing,
we used a 4-layer network with 120, 120, and 84 hidden units; for Arrhythmia we used a 5-layer
network with 120, 120, 36, and 84 hidden units; and lastly, for HTRU2, we used a 5 layer network
with 120, 120, 84, 132 hidden units. Each entry is averaged over 20 Monte Carlo trials. Test
classification performance is shown in Figure 4. k-mixup improves over (1-)mixup in each case,
although for Arrhythmia no mixup outperforms both, and for Phishing, no mixup statistically matches
the performance of k-mixup. In these small datasets, the best (or at least good) performance seems to
be achieved with relatively small α = 0.1 and moderate k (4 or 8).
Data Set	None	k = 1	k = 2	k = 4	k = 8	k = 16	k = 1	k = 2	k = 4	k = 8	k = 16	k = 1	k = 2	k = 4	k = 8	k = 16
Abalone	27:68	28.00	27.99	28.12	27.93	27.75	27.50	27.51	27.43	28.59	28.12	27.08	27.47	27.38	27.66	27.93
Arrhythmia	57:07	52.12	56.41	53.86	54.73	54.73	53.04	55.16	52.12	55.11	54.40	55.05	53.37	53.70	54.46	56.58
Cancer	92:81	92.10	92.91	93.79	91.95	91.99	91.76	90.82	89.96	89.26	87.70	91.72	92.27	92.32	91.58	91.10
HTRU2	97:50	97.47	97.65	97.48	97.52	97.49	97.37	97.45	97.51	97.38	97.42	97.51	97.52	97.42	97.51	97.42
Iris	-95.9-	96.2	94.8	96.4	96.9	96.5	91.9	92	88.8	89.1	-86-	87.4	81	79.5	77.1	76.6
Phishing	9657	96.26	96.39	96.44	96.56	96.30	95.89	95.89	95.99	96.01	96.33	94.73	95.06	94.90	95.56	96.00
(a)α = 0.1	(b)α = 1.0	(c) α = 10.0
Figure 4: Test accuracy on UCI datasets using fully connected networks.
6
Under review as a conference paper at ICLR 2022
k	α = .05	a = . 1	a = .2	a = .5	a = 1	α =10	α =100
	99.09	99.09	99.05	99.03	■9898	98.79	98.64
27	99.11	99.12	99.07	99.02	^9899	98.88	98.75
4~	99.13	99.11	99.09	99.06	■99.04	98.91	98.85
8~	99.12	99.12	99.09	99.06	■99.01	99.00	98.99
16	99.18	99.15	99.14	99.08	■99.04	99.08	99.12
32	99.18	99.15	99.17	99.10	^^9Ir	99.16	99.18
k	α = .05	a = . 1	a = .2	a = .5	a = 1	α =10	α = 100
T~	0.068	0.148	0.239	0.436	0.617	1.347	1.732
2Γ	0.070	0.129	0.214	0.388	0.586	1.228	1.585
T~	0.049	0.111	0.191	0.338	■0.542	1.078	1.399
8~	0.037	0.092	0.149	0.305	0.449"	0.930	1.191
16	0.035	0.070	0.122	0.241	^034T	0.719	0.938
32	0.029	0.039	0.076	0.152	^022T	0.456	0.586
(a) Performance (test accuracy)	(b) Average squared distance of vicinal distribution from training set
Figure 5:	Results for MNIST with a LeNet architecture (no mixup performance: 99.0%), averaged
over 20 Monte Carlo trials (±.02 confidence on test performance). Note that k-mixup doubles the
improvement of 1-mixup over ERM.								
	k	α = .05	a = . 1	a = .2	a = .5	a = 1	α = 10	α = 100
	1	94.785	95.025	95.27	95.645	95.63	94.82	94.085
	2	94.8	94.92	95.285	95.645	95.79	95.105	94.14
	4	94.76	94.99	95.3	95.65	95.745	95.205	94.315
	8	94.79	94.925	95.255	95.625	95.815	95.285	94.61
	16	94.68	94.98	95.215	95.595	95.735	95.33	94.92
	32	94.74	94.905	95.11	95.465	95.675	95.375	95.165
(a)	Performance (test accuracy)
k	α = .05	α=.1	a = .2	a = .5	a = 1	α =10	α =100
	29.5	58.2	107.1	198.1	^280^	624.1	803.1
2-	27.0	54.0	94.4	192.6	^768	564.9	721.2
~	22.3	58.6	87.3	163.0	^406	494.3	637.6
8-	24.1	43.1	78.4	141.7	T995^	431.3	554.8
16	16.8	32.6	70.7	114.3	!866	365.2	468.7
32	14.9	28.4	50.4	96.9	^1424	288.6	371.7
(b)	Average squared distance of vicinal distribution from training set.
Figure 6:	Results for CIFAR-10 with Resnet18 architecture (no mixup performance: 94.4%), averaged
over 20 Monte Carlo trials (±.03 confidence on test performance). Difference between best k-mixup
and best 1-mixup is 0.17% for fixed high α (α = 100), the improvement increases to 1.2%.
Image datasets. Results for MNIST (LeCun & Cortes, 2010) using a convolutional neural network,
i.e., LeNet modified slightly to accommodate grayscale images, are in Figure 5, with a joint sweep
over k and α. Each table entry is averaged over 20 Monte Carlo trials. For each generated point
in the vicinal distribution, we compute its closest squared distance to the matched pair from which
it was generated. The results are averaged over the vicinal dataset and reported in part (b) of the
figure. It shows that k-mixup yields significantly closer matches as k increases, causing the generated
vicinal distribution to deviate less (in terms of squared distance) from the original training set. This
is empirical evidence of our method respecting manifold support and cluster structure, as noted in
Section 2. In spite of this lower variation, for each α the best generalization performance is for some
k > 1, with α = 100 yielding 99.18% accuracy for k = 32 compared to 98.64% accuracy for k = 1.
Generally, for fixed α, increasing k improves performance for this dataset.
Figure 6 shows analogous results for CIFAR-10, using the
PreAct Resnet-18 architecture as in Zhang et al. (2018).
Again k-mixup succeeds in finding matches that lie signifi-
cantly closer together as k increases. For each α except the
small α = .1, the best generalization performance is still
for some k > 1, with α = 100 yielding 95.165% accu-
racy for k = 32 compared to 94.085% accuracy for k = 1.
The best performance overall is achieved at k = 8 with
parameter α = 1. While the best k-mixup performance
exceeds that of the best 1-mixup by only 0.17%, recall that
in this setting 1-mixup outperforms ERM by 1.4% (Zhang
et al., 2018), so when combined with the low overall error
rate, small gains are not surprising. Tables of results for
DenseNet and WideResnet architectures can be found in
Figure 9, with the best k-mixup outperforming the best
1-mixup by 0.44% and 0.28% respectively. Note that in
the case of Densenet, k = 16 outperforms or (statistically)
matches k = 1 for all values of α.
k	a = . 1	a = .2	a = .5	a = 1	α = 10
1	76.54	77.22	77.88	7776	74.88
ɪ	76.54	77.38	78.19	78.4T	76.09
~	76.55	77.36	77.94	7825^	76.51
ɪ	76.51	77.18	77.82	78.03-	76.69
16	76.43	77.08	77.77	7787	76.86
32	76.29	76.93	77.51	7754	76.90
CIFAR-100 performance (±.05 confidence)
k	a = . 1	a = .2	a = .5	a = 1	α = 10
1	97.13	97.12	97.14	■9717	96.76
~	97.16	97.14	97.19	97.29	96.81
~	97.11	97.13	97.16	^9724	96.87
ɪ	97.16	97.11	97.14	^9723	96.94
16	97.09	97.11	97.08	^9716	96.93
32	97.16	97.10	97.05	W.08	96.94
(b) SVHN performance (±.02 confidence)
Figure 8: Test accuracy on CIFAR-100
and SVHN, averaged over 20 Monte
Carlo trials (±.03 confidence).
For both MNIST and CIFAR-10 datasets, with coupling distances reported, one could compare NEW
settings with similar average squared distances. For example, a close inspection of Figures 5 and
6 show that higher k again tends to have the advantage, indicating that performance gains are not
simply due to having a closer vicinal distribution.
Additionally, we show training curves (performance as a function of epoch) for Resnet-18 on CIFAR-
10 in Figure 7, averaged over 20 random trials. The training speed (test accuracy in terms of epoch)
for 1-mixup and 32-mixup shows no loss of convergence speed with k = 32 mixup, with (if anything)
k = 32 showing a slight edge. The discontinuity at epoch 100 is due to our reduction of the learning
7
Under review as a conference paper at ICLR 2022
Ooooo
9 8 7 6 5
uoeuy-ssu UMmEOM *
0	50	100 150 200
Epoch
(a) Test acc., α = 1.
Ooooo
8 7 6 5 4
uoeuy-ssu UMmEOM *
O 50 IOO 150 200
Epoch
(b) Train acc., α = 1.
Oooooo
9 8 7 6 5 4
uoeuy-ssu UMmEOM *
0	50	100 150 200
Epoch
(c) Test acc., α = 10.
0	50	100 150 200
Epoch
(d) Train acc., α = 10.
Figure 7: Training convergence of k = 1 and k = 32 mixup on CIFAR-10, averaged over 20 random
trials. Note that both train at roughly the same rate (k = 32 slightly faster), the train accuracy
discrepancy is due to the more class-accurate vicinal training distribution created by higher k-mixup.
k	α = .5	a = 1	α = 2	a = 4
~	96.65	96.71	96.58	96.43
16	96.62	97.02	97.15	97.08
(a) DenseNet-BC-190 architecture
(±.03 confidence, no mixup performance: 96.3%)
k	a = .05	a = .2	a = .5	a = 1	α = 2	a = 4
T~	88.46	88.47	88.38	88.22	87.75	87.01
τ~	88.64	88.53	88.73	88.59	88.41	87.84
16	88.47	88.41	88.75	88.66	88.62	88.26
(b) WideResnet-101 architecture
(±.09 confidence, no mixup performance: 88.4%)
Figure 9: CIFAR-10 test accuracy for DenseNet-BC-190 and WideResnet-101 architectures. For
DenseNet, the difference between best k-mixup and best 1-mixup is 0.44%, for fixed high α (α = 4),
the improvement increases to 0.65%. For WideResnet, the difference between best k-mixup and best
1-mixup is 0.28%, for fixed high α (α = 4), the improvement increases to 1.25%.
rate at epochs 100 and 150 to aid convergence (used throughout our image experiments). The train
accuracy shows similar convergence profile between 1- and 32-mixup; the difference in absolute
accuracy here (and the reason it is less than the test accuracy) is because the training distribution
is the mixup-modified vicinal distribution. The curve for k = 32 is higher, especially for α = 10,
because the induced vicinal distribution and labels are more consistent with the true distribution, due
to the better matches from optimal transport. The large improvement in train accuracy is remarkable
given the high dimension of the CIFAR-10 data space, since it indicates that k = 32 mixup is able to
find significantly more consistent matches than k = 1 mixup.
Figure 8 shows results for CIFAR-100 and SVHN, with a Resnet-18 architecture. As before, for fixed
α, the best performance is achieved for some k > 1. The improvement of the best k-mixup over the
best 1-mixup is 0.53% for CIFAR-100 and 0.12% for SVHN. For fixed high α = 10, the k-mixup
improvement over 1-mixup rises to 2.02% for CIFAR-100, possibly indicating that the OT matches
yield better interpolation between classes, aiding generalization.
Speech dataset. Performance is tested on a speech dataset:
Google Speech Commands (Warden, 2018) using a LeNet
architecture are in Figure 10. Each table entry is averaged
over 20 Monte Carlo trials (±.014 confidence on test per-
formance). We augmented the data in the same way as
Zhang et al. (2018). The difference between best k-mixup
and best 1-mixup is 0.4%.
Manifold mixup. We compare to Manifold Mixup
(Verma et al., 2018), which helps in settings like those
k	a = . 1	a = .2	a = .5	a = 1	α = 10
T~	89.05	89.03	89.40	89.42	88.28
2Γ	89.17	89.12	89.40	89.43	88.67
4Γ	89.07	89.06	89.82	89.61	89.13
8Γ	88.98	89.05	89.73	89.50	89.49
16	88.97	88.96	89.58	89.68	89.74
Figure 10: Google Speech Commands
test accuracy using LeNet architecture,
averaged over 20 Monte Carlo trials
(±.014 confidence).
in Figure 1 by making interpolations more meaningful. It performs mixup at random layers (random
per minibatch), not only in the input space. This extends to “manifold k-mixup:” k-mixup in the
hidden layers as well as the input layers. We use settings in Verma et al. (2018), i.e., for PreAct
Resnet18, the mixup layer is randomized (coin flip) between the input space and the output of the
first residual block.2
Results for CIFAR-10 with a Resnet18 architecture are in Figure 11. Numbers in this experiment are
averaged over 20 Monte Carlo trials (±.03 confidence on test performance). While manifold 1-mixup
outperforms the standard 1-mixup from Figure 6, it is matched by manifold k-mixups with k = 4
and outperformed by standard k-mixup (Figure 6, k = 8). We also tried randomizing over (a) the
2See https://github.com/vikasverma107 7/manifold_mixup
8
Under review as a conference paper at ICLR 2022
outputs of all residual blocks and (b) the outputs of (lower-dimensional) deep residual blocks only,
but found that performance of both 1-mixup and k-mixup degrades in these cases. This underscores
that mixup in hidden layer manifolds is not guaranteed to be effective and can require tuning.
Adversarial robustness. Figure 12 shows results on
white-box adversarial attacks generated by the FGSM
method (implementation of Kim (2020)3) for various val-
ues of maximum adversarial perturbation. As in Verma
et al. (2018), we used FGSM over PGD, since the iterative
PGD attacks are significantly more effective, making any
performance improvements seem less relevant in practice.
Additionally, we expect simple gradient attacks to be par-
ticularly meaningful for evaluating the success of mixup,
k	a = . 1	a = .2	a = .5	a = 1	α = 10
T	95.00	95.27	95.59	95.74	95.19
ɪ	94.91	95.22	95.56	95.69	95.29
T	94.99	95.22	95.55	95.75	94.98
ɪ	94.92	95.22	95.58	95.72	95.39
^16	94.86	95.18	95.58	95.69	95.47
32	94.86	95.09	95.42	95.53	95.46
Figure 11: Manifold mixup test accuracy
on CIFAR-10 using Resnet18 architec-
ture, averaged over 20 Monte Carlo trials
(±.03 confidence).
whose goal is to approximately linearly interpolate between classes. We show CIFAR-10 accuracy on
white-box FGSM adversarial data (10000 points), where the maximum adversarial perturbation is set
to /255; performance is averaged over 30 Monte Carlo trials (±0.6 confidence). All performances
are without adversarial training. Note that k = 2 outperforms k = 1 uniformly by as much as 8.64%,
and the k > 1 mixups all outperform k = 1 mixup on larger attacks. Similar results for MNIST are
in Figure 12(b), with the FGSM attacks being somewhat less effective. Here k = 8 is best for smaller
attacks, and k = 2 is best for larger attacks.
NEW
The improved robustness shown by k-mixup speaks to a key goal of mixup, that of smoothing the
predictions in the parts of the data space where no/few labels are available. This smoothness should
make adversarial attacks require greater magnitude to successfully “break” the model.
k	e = .5	e = 1	e = 2	e = 4	e = 8	e = 16
丁	78.46	72:07	66:02	3972	48.61	23.71
2	79.48	74722	70718	66758	57725	26.23
4	78.82	TΣ7T	67.82	63:60	54.80	2732
8	78.00	7045	■6424	390T	3032	26.24
k	e = .5	e =1	e = 2	e = 4	e = 8	e = 16
T	98.26	97^	96:03	92:97	86:89	77.14
2	98.32	97:45	96:15	93716	^8734	78.24
4	98.30	97^	95:81	92^	84:89	72.48
8	98.36	^746	■9596	■9233	^8370	69.33
(a) CIFAR-10 (±0.6 confidence)	(b) MNIST (±0.1 confidence)
Figure 12: Adversarial robustness: accuracy on white-box FGSM adversarial attacks.
5 Conclusions and Future Work
The experiments above demonstrate that k-mixup often improves the generalization and robustness
gains achieved by (1-)mixup, and at least does no harm. This is seen across a diverse range of
datasets and network architectures. It is simple to implement, adds little computational overhead
to conventional (1-)mixup training, and may also be combined with related mixup variants. As k
increases, the regularization induced more accurately reflects the local structure of the training data,
especially in the manifold support and clustered settings, as seen in Sections 2 and 3. Empirical results
show that performance is robust to variations in k, ensuring that extensive tuning is not required.
Our experiments show the greatest improvement from using k-mixup on low-dimensional datasets,
while the improvement on high-dimensional datasets remains positive but is smaller (recall that classic
mixup also has somewhat small gains over no mixup in these settings). This difference could be due
to the diminishing value of Euclidean distance for characterizing dataset geometry in high dimensions
(Aggarwal et al., 2001), but intriguingly this effect was not remedied by doing OT in the possibly
lower-dimensional manifolds created by the higher layers in our manifold mixup experiments. To
remedy this issue, in future work we will consider alternative metric learning strategies, with the goal
of identifying effective high-dimensional metrics for displacement interpolation of data points.
Mixup has been incorporated into a broad range of learning applications, many of which we have not
empirically investigated here. The original work demonstrated robustness to label corruption and
improved GAN training stability, which could easily benefit from k-mixup as well. Additionally,
mixup has been used for augmentation in semi-supervised learning (Berthelot et al., 2019; Sohn
et al., 2020b). Given its simplicity, a k-mixup augmentation may be able to generate more informed
shifts of labelled and unlabelled training points. This same philosophy applies to a recent method for
generalizing fair classifiers (Chuang & Mroueh, 2021), which already leverages manifold mixup.
3Software has MIT License
9
Under review as a conference paper at ICLR 2022
Ethics Statement The k-mixup regularization procedure we propose and our experimentation on
it do not bring up additional ethical concerns outside of general concerns associated with machine
learning as a field, and the use of neural network models. Standard benchmark datasets and tasks
were used, with the assumption that dataset originators have done their due diligence with respect to
concerns on IRBs, privacy, fairness, and the like.
Reproducibility Statement With regards to reproducibility, we note that k-mixup is exceedingly
simple, and may be easily incorporated into almost any neural network as a data perturbation step.
For our experiments, publicly available benchmark datasets have been used and are clearly stated
in Section 4. Additionally, the models used and the relevant training parameters are described for
each task considered. Upon acceptance, we would be glad to make a full implementation of our
experiments available on Github under an open source MIT license. Lastly, for theoretical results,
full proofs of our statements are provided in the appendices following the references.
References
Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the surprising behavior of distance
metrics in high dimensional space. In International conference on database theory, pp. 420-434.
Springer, 2001.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. MixMatch: A Holistic Approach to Semi-Supervised Learning. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf.
Dimitris Bertsimas and John Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1st
edition, 1997. ISBN 1886529191.
Luigi Carratino, Moustapha Cisse, Rodolphe Jenatton, and Jean-Philippe Vert. On Mixup Regulariza-
tion. arXiv:2006.06049 [cs, stat], June 2020. URL http://arxiv.org/abs/2006.06049.
arXiv: 2006.06049.
Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=DNl5s5BXeBn.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems, 26:2292-2300, 2013.
Bharath Bhushan Damodaran, Benjamin Kellenberger, Remi Flamary, Devis Tuia, and Nicolas Courty.
DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation. In
Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision
—ECCV2018, pp. 467-483, Cham, 2018. Springer International Publishing. ISBN 978-3-030-
01225-0.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
Kilian Fatras, Younes Zine, Remi Flamary, Remi Gribonval, and Nicolas Courty. Learning with mini-
batch Wasserstein : asymptotic and gradient properties. In Proceedings of the Twenty Third Interna-
tional Conference on Artificial Intelligence and Statistics, pp. 2131-2141. PMLR, June 2020. URL
https://proceedings.mlr.press/v108/fatras20a.html. ISSN: 2640-3498.
Kilian Fatras, Thibault Sejourne, Remi Flamary, and Nicolas Courty. Unbalanced minibatch Optimal
Transport; applications to Domain Adaptation. In Proceedings of the 38th International Conference
on Machine Learning, pp. 3186-3197. PMLR, July 2021. URL https://proceedings.mlr.
press/v139/fatras21a.html. ISSN: 2640-3498.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning Generative Models with Sinkhorn
Divergences. In Proceedings of the Twenty-First International Conference on Artificial Intelligence
and Statistics, pp. 1608-1617. PMLR, March 2018. URL https://proceedings.mlr.
press/v84/genevay18a.html. ISSN: 2640-3498.
10
Under review as a conference paper at ICLR 2022
Hongyu Guo, Yongyi Mao, and Richong Zhang. MixUp as Locally Linear Out-of-Manifold
Regularization. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3714-
3722, July 2019. ISSN 2374-3468. doi: 10.1609/aaai.v33i01.33013714. URL https:
//ojs.aaai.org/index.php/AAAI/article/view/4256. Number: 01.
S. K. Katti. Moments of the Absolute Difference and Absolute Deviation of Discrete Distributions.
The Annals of Mathematical Statistics, 31(1):78-85, 1960. ISSN 0003-4851. URL https:
//www.jstor.org/stable/2237495. Publisher: Institute of Mathematical Statistics.
Hoki Kim. Torchattacks: A pytorch repository for adversarial attacks. arXiv preprint
arXiv:2010.01950, 2020.
Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle Mix: Exploiting Saliency and Local
Statistics for Optimal Mixup. In International Conference on Machine Learning, pp. 5275-
5285. PMLR, November 2020. URL http://proceedings.mlr.press/v119/kim20b.
html. ISSN: 2640-3498.
JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint
mixup with supermodular diversity. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=gvxJzw8kW4b.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Gabriel Peyre and Marco Cuturi. Computational Optimal Transport: With Applications to Data
Science. Foundations and Trends® in Machine Learning, 11(5-6):355-607, February 2019. ISSN
1935-8237, 1935-8245. doi: 10.1561/2200000073. URL https://www.nowpublishers.
com/article/Details/MAL-073. Publisher: Now Publishers, Inc.
Filippo Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations,
PDEs, and Modeling. Progress in Nonlinear Differential Equations and Their Applications.
Birkhauser Basel, 2015. ISBN 978-3-319-20827-5. doi: 10.1007/978-3-319-20828-2. URL
https://www.springer.com/gp/book/9783319208275.
Jy Yong Sohn, Jaekyun Moon, Kangwook Lee, and Dimitris Papailiopoulos. Gan-mixup: Augmenting
across data manifolds for improved robustness. ICML Workshop on Uncertainity and Robustness
in Deep Learning, 2020a.
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A. Raffel,
Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. FixMatch: Simplifying Semi-Supervised
Learning with Consistency and Confidence. Advances in Neural Information Processing Systems,
33:596-608, 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/
06964dce9addb1c5cb5d6e3d9838f733- Abstract.html.
Justin Solomon, Kristjan Greenewald, and Haikady N Nagaraja. k-variance: A clustered notion of
variance. arXiv preprint arXiv:2012.06958, 2020.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Aaron Courville, Ioannis Mitliagkas,
and Yoshua Bengio. Manifold Mixup: Learning Better Representations by Interpolating Hidden
States. September 2018. URL https://openreview.net/forum?id=rJlRKjActQ.
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint
arXiv:1804.03209, 2018.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of
empirical measures in wasserstein distance. Bernoulli, 25(4A):2620-2648, 2019.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon
Yoo. CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features.
pp. 6023-6032, 2019. URL https://openaccess.thecvf.com/content_ICCV_
2019/html/Yun_CutMix_Regularization_Strategy_to_Train_Strong_
Classifiers_With_Localizable_Features_ICCV_2019_paper.html.
11
Under review as a conference paper at ICLR 2022
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond Em-
pirical Risk Minimization. February 2018. URL https://openreview.net/forum?id=
r1Ddp1-Rb.
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How Does Mixup
Help With Robustness and Generalization? September 2020. URL https://openreview.
net/forum?id=8yKEo06dKNo.
12
Under review as a conference paper at ICLR 2022
A k-MIXUP PSEUDOCODE & COMPUTATIONAL COST
NEW
Below is code for one epoch of k-mixup training, in the style of Figure 1 of Zhang et al. (2018).
# y1, y2 should be one-hot vectors
for (x1, y1), (x2, y2) in zip(loader1, loader2):
cost = numpy.inf * [x1.shape[0], x1.shape[0]]
for i in range(x1.shape[0] // k):
cost[i * k:(i+1) * k, i * k:(i+1) * k] = scipy.spatial.
distance_matrix(x1[i * k:(i+1) * k], x2[i * k:(i+1) * k])
_, idx = scipy.optimize.linear_sum_assignment(cost)
x2[i * k:(i+1)	* k]	= x2[idx,	i	* k:(i+1)	*	k]
y2[i * k:(i+1)	* k]	= y2[idx,	i	* k:(i+1)	*	k]
lam = numpy.random.beta(alpha, alpha)
x	=	Variable(lam *	x1 +	(1 - lam)	*	x2)
y	=	Variable(lam *	y1 +	(1 - lam)	*	y2)
optimizer.zero_grad()
loss(net(x), y).backward()
optimizer.step()
Computational cost. While the cost of the Hungarian algorithm is O(k3), it provides k data points
for regularization, yielding an amortized O(k2) complexity per data point. For the smaller values
of k that we empirically consider, approximate Sinkhorn-based methods (Cuturi, 2013) are slower
in practice, and the Hungarian cost remains small relative to that of gradient computation (e.g. for
CIFAR-10 and k = 32, the Hungarian algorithm costs 0.69 seconds per epoch in total). Computing
the distance matrix input to the OT matching costs O(k2d) where d is dimension, yielding an
amortized O(kd) complexity per data point. With the high dimensionality of CIFAR, a naive CPU
implementation of this step of the computation adds about 8 seconds per epoch.4 Moreover, training
convergence speed is unaffected, unlike manifold mixup (Verma et al., 2018), which in our experience
converges slower and has larger computational overhead. Overall, there is little computational
downside to generalizing 1-mixup to k-mixup.
B	Proof of Proposition 1
NEW
Finite-sample convergence of empirical measures implies that for an arbitrary sampling of k points
μk, we have W2(μ,^k) ≤ O(k-2/d) with high probability (e.g., 1 - 1/k2 probability, from Theorem
9.1 of Solomon et al. (2020) when combined with results from Weed & Bach (2019)). The triangle
inequality then implies that the Wasserstein-2 distance between our batches of k samples will tend to
0 at the same asymptotic rate: W2 (j^γ, μ() ≤ O(k-2/d) with high probability.
Recalling that by the definition of the optimal coupling permutation σ(i), we have 1 Pk=IHxY -
x：(i)H2 = W2(μY,μk) ≤ O(k-2/d) with high probability. Hence, for any I ⊆ [1,k] with IlxY -
xξσ(i)k22 > k-1/d for all i ∈ I, |I| ≤ O(k1-1/d) ≤ δk for k large enough and any δ ∈ (0, 1].
In essence, long-distance identifications are rare, and their fraction is bounded for large enough k .
Therefore, for k large enough, again with high probability there exists a subset I with |I| ≥ (1 - δ)k
such that IIxY - x：(i)∣∣2 ≤ RS for all i ∈ I. Since xY and x：(i)lie in S, the proposition results.
C Proof of Lemma 1
We argue by contradiction and prove the result for m = 2 first. Suppose that the number of cross-
cluster matchings exceeds |r1 - s1|. Then by the pigeonhole principle, there must be at least two
such matchings, which we’ll say are between pi and qi, and pi+1 and qi+1, WLOG. As the cost of
the cross-cluster matchings is ≥ (2∆)2, and the cost of intra-cluster matchings is ≤ (2∆)2, a more
4It is possible to accelerate this computation with parallelization/GPU usage, but as this cost is a small
portion of the overall training cost we did not optimize it.
13
Under review as a conference paper at ICLR 2022
optimal matching would pair pi and qi+1, and pi+1 and qi. This contradicts optimality of the initial
pairing.
In the scenario with m clusters, an analogous argument works. As above, |ri - si | is the number
of cluster i elements that must be matched in a cross-cluster fashion, and half the sum of these
quantities is the minimum number of cross-cluster matchings overall. If this number is exceeded, by
the pigeonhole principle, there must be additional cross-cluster matches that form a cycle in the graph
over clusters. As above, the cost would be reduced by matching points within the same clusters, so
this contradicts optimality.
D	Proof of Theorem 1
We first prove the result for two clusters of weight p and 1 - p. With our clustered assumption, we
may assume that all identifications that can be intracluster will be intracluster. Thus, if s1 and s2
denote the number of points in cluster 1 from batch 1 and 2, then the resulting number of cross-cluster
matches is |s1 - s2|. As the samples for our batches are i.i.d., these random variables follow a simple
binomial distribution B(k,p). We can bound the expectation of this quantity with Jensen’s inequality:
(E[|s1 -	s2 |])	≤	E[|s1	-	s2 |2]	= 2Var(s1 )	=	2kp(1	- p)
With some algebraic manipulation, our desired rate bound results. It is also possible to get an exact
rate with some hypergeometric identities (Katti, 1960), but these simply differ by a constant factor,
so we omit the exact expressions here.
For the general case, our clustered assumption again allows us to assume that OT will prioritize
intracluster identifications. Thus, ifwe let ri and si denote the number of samples in cluster i in batch
1 and 2, respectively, then the number of cross-cluster matchings will be 1 Pi 屋一Si |. Ultimately, r
and si are sampled from a binomial distribution B(k, pi), so we may repeat the argument above and
note that E [ri = Si] ≤ d2kpi(1 - Pi). Simple algebraic manipulations lead to the desired result.
E Proof of Theorem 2
By the smooth boundary and positive density assumptions, we know that P (Aδ) > 0 and P (Bδ) > 0
for any δ > 0. Hence, for fixed δ and k large enough, we know that with high probability the sets Aδ
and Bδ each contain more points than the number of cross-cluster identifications.
Now consider A and B for = 2δ + (max(RA, RB)2)/(2D2). All cross-cluster matches need to
be assigned. The cost of assigning a cross-cluster match to a point in Aδ and a point in Bδ is at most
D2(1 + 2δ)2 (since we are using W2). Furthermore, the cost of assigning a cross-cluster match that
contains a point in A outside A and an arbitrary point in B is at least D2(1 + )2. Consider the
difference between these two costs:
D2(1 + e)2 - D2(1 + 2δ)2 = D2(2(e - 2δ) + e2 - 4δ2) > 2D2 max(RA,RB产 ≥ RA.
2D2
Since this difference > 0 and we have shown Aδ contains sufficient points for handling all assign-
ments, this assignment outside of A will only occur if there is a within-cluster pair which benefits
from using the available point in A more than is lost by not giving it to the cross-cluster pair (> R2A).
The maximum possible benefit gained by the within-cluster pair is the squared radius of A, i.e. R2A.
Since we have shown that the lost cost for the cross-cluster pair is bigger than R2A, we have arrived at
a contradiction. The proof is similar for the B side.
We have thus shown that for k large enough (depending on δ), with high probability all cross-cluster
matches have an endpoint each in A and B where = 2δ + (max(RA,RB )2 )/(2D2 ). Setting
δ = (max(RA, RB)2)/(4D2) completes the proof.
F Proof of Theorem 3
We mostly follow the notation and argument of Zhang et al. (2020) (c.f. Lemma 3.1), modifying
it for our setting. There they consider sampling λ 〜Beta(α, β) from an asymmetric Dirichlet
14
Under review as a conference paper at ICLR 2022
distribution. Here, we assume a symmetric Dirichlet distribution, such that α = β, simplifying most
of the expressions. The analogous results hold in the asymmetric case with simple modifications.
Consider the probability distribution Dλ with probability distribution: β(α + 1, α). Note that this
distribution is more heavily weighted towards 1 across all α, and for α < 1, there is an asymptote as
you approach 1.
Let Us adopt the shorthand notation Xi,σγξ(i)(λ) := λxj + (1 - λ)x∣γξ for an interpolated feature
point. The manipulations below are abbreviated, as they do not differ much for our generalization.
1	(Nk) k
Emix (f) =	N 2 Eλ〜β(α,α) X Xh(f (χi,σγξ(i)(λ))) - (λyY + (1 - λ)yσγξ(i))f(xi,σγξ(i)(λ))
k k	γ,ξ=1 i=1
1	(Nk) k
=N 2 Eλ~β(α,α)EB~Bern(λ) E £ B [h(f (xi" (i) (λ))) -y『f (xi,σ,ξ (i) (λ))]
k k	γ,ξ=1 i=1
+ (1- B)[h(f (Xi,σγξ(i)(λ))) - y，Yg(i)f (Xi,σγξ(i)(λ))]
1	(Nk )	k
=Ν-2 X X Eλ~β(α+1,α)h(f(xi,σγξ(i)(λ))) -yγf(xi,σγξ(i)(λ))
k k	γ,ξ=1 i=1
For the third equality above, the ordering of sampling for λ and B has been swapped via conjugacy:
λ 〜β(α, α), B∣λ 〜Bern(λ) is equivalent to B 〜U{0, 1}, λ∣B 〜β(α + B,α + 1 — B). This is
combined with the fact that Xi,σγξ(i)(1 一 λ) = Xσγξ(i),i(λ) to get the last line above.
Now we can swap the sums, grouping over the initial point to express this as the following:
1N
EmiYf) = NN NEl〜β(α+1,α)Er〜Dih(f (λxi + (1- λ)r)) - yf(λxi + (1- λ)r),
where the probability distribution Di is as described in the text.
The remainder of the argument performs a Taylor expansion of the loss term h(f (λxi + (1 - λ)r)) -
yiγ f (λxi + (1 - λ)r) in terms of 1 - λ, and is not specific to our setting, so we refer the reader to
Appendix A.1 of (Zhang et al., 2020). for the argument.
G k-MIXUP AS MEAN REVERSION FOLLOWED BY REGULARIZATION
Theorem 4. Define (X：, y?) as
xγ = xγ + G(X - xγ)
yγ = yγ + θ(yγ - yγ),
where X： = (N) P(=1 x∣ 式i) and y： = (N) P(=1 y; 式^ are expectations Under the matchings
and θ 〜 β[i∕2,i] (α, α). Further, denote the zero mean perturbations
初=(θ - G)XY + (1 - θ)x∣
∙γξ(i)-(1-办xY

号=(θ - θ)yγ + (1-θ)yσγξ(i)-(1- θ)yY.
Then the k-mixup loss can be written as
N
(
)
k
k
E OT mixup
1k	1
(f) = 7νv XEθ,ξ k X'(yγ + 以,f(xγ + δ)) .
k γ=1	k i=1
The mean XGi： being shifted toward is exactly the mean of the locally-informed distribution Di .
Moreover, the covariance structure of the perturbations is detailed in the proof (simplified in Section
G.1) and is now also derived from the local structure of the distribution, inferred from the optimal
transport matchings.
15
Under review as a conference paper at ICLR 2022
Proof. This argument is modelled on a proof of Carratino et al. (2020), so we adopt analogous
notation and highlight the differences in our setting and refer the reader to Appendix B.1 of that paper
for any omitted details. First, let us use shorthand notation for the interpolated loss function:
mγξ (X) = '(f (λxγ +(I- 2xσ,ξ(i)), λyγ + (1 - λ)yξξ γξ(i)).
Then the mixup objective may be written as:
1	(Nk) k
Emix(f)= K X XEλmYξ(X).
k k	γ,ξ=1 i=1
As λ 〜β(α, α), we may leverage the symmetry of the sampling function and use a parameter
θ 〜 β[i∕2,i] (α, α) to write the objective as:
1 (Nk)	1	(Nk) k
Emrix (f) = PJ X 'i,	where ' = 店 XXEθmYξ(θ)
To obtain the form of the theorem in the text, we introduce auxiliary variables X, yγ to represent the
mean-reverted training points:
xγ = Eθ,ξ hθxγ + (1 - *γξ(i)i
yY = %ξ hθyγ + (1 - θ)ylγξ (i)i，
and 镇,eiγ to denote the zero mean perturbations about these points:
初=θxγ + (1- θ)xσγξ(i)- Eθ,ξ hθxγ + (1- θ)xσγξ(i)i
可=θyγ + (1 - θ)y∣γξ(i) - Eθ,ξ hθyγ + (1 - θ)y∣,$(/ .
These reduce to the simplified expressions given in the theorem if we recall that θ and ξ are indepen-
dent random variables. Note that both the mean-reverted points and the perturbations are informed by
the local distribution Di.	□
G. 1 Covariance structure
As in (Carratino et al., 2020), it’s possible to come up with some simple expressions for the covariance
structure of the local perturbations, so we write out the analogous result below. As the argument is
very similar, we omit it.
Lemma 2. Let σ2 denote the variance of β[i∕2,i] (α, α), and V2 := σ2 + (1 一 H)2. Then thefollowing
expressions hold for the covariance of the zero mean perturbations:
Eθ,ξ 碎(碎)> =	_ σ2(xγ 一 xγ)(xγ — X)> + V2ςxy图 =	θ
Eθ,ξ Μ㈢)> =	= σ2(yγ - yγ )(yγ - 京)> + ν2∑yγ yγ =	θ2
~- , - , -∣- Eθ,ξ SY 层)> =	σ2(xY -Xi)(y： 一 y：)> + ν2∑*yγ =	θ	,
where Σxγ xγ, Σyγ yγ, Σæγ yγ denote empirical CoVariance matrices.
i i	ii	ii
Note again, that the covariances above are locally-informed, rather than globally determined. Lastly,
there is also a quadratic expansion performed about the mean-reverted points X：, y with terms that
regularize f, but we omit this result as the regularization of Theorem 3 is more intuitive (c.f. Theorem
2 of (Carratino et al., 2020)).
16