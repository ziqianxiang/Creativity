Under review as a conference paper at ICLR 2022
DESTA: A Framework for Safe Reinforcement
Learning with Markov Games of Intervention
Anonymous authors
Paper under double-blind review
Ab stract
Exploring in an unknown system can place an agent in dangerous situations,
exposing to potentially catastrophic hazards. Many current approaches for tackling
safe learning in reinforcement learning (RL) lead to a trade-off between safe
exploration and fulfilling the task. Though these methods possibly incur fewer
safety violations, they often also lead to reduced task performance. In this paper, we
take the first step in introducing a generation of RL solvers that learn to minimise
safety violations while maximising the task reward to the extent that can be tolerated
by safe policies. Our approach uses a new two-player framework for safe RL called
Distributive Exploration Safety Training Algorithm (DESTA). The core of DESTA
is a novel design of game between two RL agents: Safety Agent that is delegated
the task of minimising safety violations and Task Agent whose goal is to maximise
the reward set by the environment task. Safety Agent can selectively take control
of the system at any given point to prevent safety violations while Task Agent is
free to execute its actions at all other states. This framework enables Safety Agent
to learn to take actions that minimise future safety violations (during and after
training) by performing safe actions at certain states while Task Agent performs
actions that maximise the task performance everywhere else. We demonstrate
DESTA’s ability to tackle challenging tasks and compare against state-of-the-art RL
methods in Safety Gym Benchmarks which simulate real-world physical systems
and OpenAI’s Lunar Lander.
1	Introduction
Reinforcement learning (RL) is a framework that enables autonomous agents to learn complex
behaviours from interactions with the environment (Sutton & Barto, 2018). Inspired by concepts
within neuroscience, RL has had notable successes in a number of practical domains such as robotics
and video games (Deisenroth et al., 2011; Peng et al., 2017). During its training phase, an RL agent
explores using a trial and error approach in order to determine the best actions. This process can
lead to the agent selecting actions whose execution in some states may result in critical damage; for
example, an aerial robot attempting to fly at high velocities can result in the helicopter crashing and
subsequent permanent system failure. Consequently, a major challenge in RL is to produce methods
that solve the task and ensure safety both during and after training.
A common approach to tackle the problem of safe exploration in RL is to use a constrained Markov
decision process (c-MDP) formulation (Altman, 1998). In this framework, the agent seeks to max-
imise a single objective subject to various safety constraints. Although c-MDP can be solved if
the model is known, extending this formalism to settings in which the model is unknown remains
a significant challenge (Chow et al., 2019a). One of the main tools for tackling the c-MDP prob-
lem setting in RL is the Lagrangian approach for solving a constrained problem, that is, solving
maxθ minλ f(θ) - λg(θ) by gradient descent in λ and ascent in θ instead of solving maxθ f(θ)
subject to g(θ) ≤ 0. This approach introduces a trade-off between competing objectives: maximising
the task reward and minimising safety violation. This trade-off which is calibrated by λ, undermines
the ability of the agent to optimise both criteria simultaneously (Srinivasan et al., 2020; Kahn et al.,
2017). Consequently, the resulting policies do not offer guarantees of safety during training Platt
& Barr (1987); Song & Leland (1998) and can lead to safety violations both during training and at
convergence (Chow et al., 2018; Stooke et al., 2020).
1
Under review as a conference paper at ICLR 2022
In contrast, to ensure their safety, animals exhibit a vast number of safety reflexes: reactive intervention
systems designed to override and assume control in dangerous situations to prevent injury. One such
example is the diving reflex, a sequence of physiological responses to the threat of oxygen deprivation
(asphyxiation) that overrides the body’s basic behavioural and regulatory (homeostatic) systems. This
reflex is fundamental that it has been found in all air-breathing vertebrates (Butler & Jones, 1997).
Inspired by such naturally occurring systems, in this paper, we tackle the challenge of learning safely
in RL with a new two-agent framework for safe exploration and learning, DESTA. The framework
entails an interdependent interaction between an RL agent, Task Agent whose goal is to maximise
the set of environment rewards and an additional RL agent, Safety Agent whose goal is to ensure
the safety of the system. The Safety Agent has the authority to override the Task Agent and assume
control of the system and, using a deterministic policy to avoid unsafe states while Task Agent can
perform its actions everywhere else to maximise rewards. As such, the goals of completing the task
set by the environment and ensuring safety are decoupled and each is delegated to an individual agent.
Transforming these components into a workable framework requires a formalism known as Markov
games (MGs). MGs augment MDPs to tackling strategic interactions between agents (Littman, 1994).
To bridge the gap between safety solutions offered in nature and machine learning, we develop a new
type of MG namely a nonzero-sum MG of interventions. In this novel type of MG, two agents with
distinct goals cooperate and exchange control of the system to minimise safety violations meanwhile
maximising the task objective. Our framework, which consists of a strategic interaction between two
independent RL agents confers several key advantages:
•	Decoupled Objectives & Safety Planning: The tasks of maximising the environment reward
and minimising safety violations are fully decoupled. This means Safety Agent pursues its
safety objectives without trading-off safety for environment rewards and, performs safe planning
to minimise safety violations at future system states. Moreover, since the agents are trained in a
shared system, each agent learns a policy which acts in response to the behaviour of the agent.
In particular, Safety Agent anticipates Task Agent’s future actions (after and during training)
and acts to reduce safety violations (see Experiment 1).
•	Selective (deterministic) interventions: Safety Agent acts only at states in which its action
ensures the safety criteria. At states in which the safety criterion is irrelevant only actions that
are relevant to the task are played. Moreover Safety Agent uses a deterministic policy which
eliminates inadvertent actions that may lead to safety violations (see Lunar Lander experiment).
•	Plug & play: DESTA is highly flexible and accommodates various notions of safety within
Safety Agent’s objective. Unlike various approaches in which safety contingencies are manually
embedded into the policy e.g. Schulman et al. (2015), DESTA accommodates any RL policy.
This also enables safe policies to be (re)used and applied only at states where necessary.
2	Related Work
Recent works in safe RL include assumptions from knowing the set of safe states and access to a safe
policy (Koller et al., 2018; Berkenkamp et al., 2017), knowing the environment model (Fisac et al.,
2019; Dean et al., 2019), having access to the cost function (Chow et al., 2019b), having a continuous
safety cost function (Cowen-Rivers et al., 2020) and using reversibility as a criterion for safety
(Eysenbach et al., 2018). Constrained Policy Optimization (CPO) (Achiam et al., 2017) extends trust
region policy optimisation (TRPO) (Schulman et al., 2015) with the aim of ensuring that a feasible
policy stays within the constraints in expectation. Similarly in the context of multi-agent RL (Yang &
Wang, 2020), Gu et al. (2021) develops MACPO by extending multi-agent TRPO (Kuba et al., 2021)
with safe constraints. However, the convergence of these methods is still challenging; the learning
dynamics tend to oscillate Stooke et al. (2020), and the methodology does not readily accommodate
general RL algorithms (Chow et al., 2018). In Tessler et al. (2018), a reward-shaping approach is used
to guide the learned policy to satisfy the constraints, however their approach provides no guarantees
during the learning process. In Dalal et al. (2018), a safety layer is introduced that acts on top of an
RL agent’s possibly unsafe actions to prevent safety violations though their framework does not deal
with negative long-term consequences of an action. In Bharadhwaj et al. (2021), a similar framework
to CPO is used with a sparse binary safety signal where the Q function is overestimated to provide
tuneable safety. Recently, Stooke et al. (2020) investigated the oscillation issue from a dynamical
system point of view and introduced a treatment by applying a PID controller on the dual variable.
2
Under review as a conference paper at ICLR 2022
To combat the limitations of the c-MDP formulation, several methods transform the original constraint
to a more conservative one to ease the problem. For example El Chamie et al. (2016) replace the
constraint cost with a conservative step-wise surrogate constraint. A significant drawback of these
approaches is their conservativeness undermines task performance (the extent of the sub-optimality
has yet to be characterised). Other approaches manually embed engineered safety-responses that are
executed near safety-violating regions (Eysenbach et al., 2018; Turchetta et al., 2020). For example,
in Turchetta et al. (2020), a safe teacher-student framework in which the teacher’s objective is the
value of the student’s final policy, the agent is endowed with a pre-specified library of reset controls
that it activates close to danger. These approaches can require time-consuming human input contrary
to the goal of autonomous learning. Moreover, these approaches do not allow the safety response to
anticipate future the behaviour of the RL agent and do not perform safety planning.
3	Preliminaries
Reinforcement Learning (RL). In RL, an agent sequentially selects actions to maximise its expected
returns. The underlying problem is typically formalised as an MDP hS, A, P, R, γi where S ⊂ Rp
is the set of states, A ⊂ Rk is the set of actions, P : S × A × S → [0, 1] is a transition probability
function describing the system’s dynamics, R : S × A → R is the reward function measuring the
agent’s performance and the factor γ ∈ [0, 1) specifies the degree to which the agent’s rewards are
discounted over time (Sutton & Barto, 2018). At time t ∈ 0, 1, . . . , the system is in state st ∈ S and
the agent must choose an action at ∈ A which transitions the system to a new state st+ι 〜 P (∙∣st,at)
and produces a reward R(st , at). A policy π : S × A → [0, 1] is a probability distribution over
state-action pairs where ∏(a∣s) represents the probability of selecting action a ∈ A in state S ∈ S.
The goal of an RL agent is to find a policy ∏ ∈ Π that maximises its expected returns given by the
value function: vπ(S) = E[P∞=0 YtR(st, at)% 〜∏(∙∣st)] where Π is the agent,s policy set.
Safety in RL. A key concern for RL in control and robotics settings is the idea of safety. This is
handled in two main ways (GarCia et al., 2015): using prior knowledge of safe states to constrain
the policy during learning or modifying the objective to incorporate appropriate penalties or safety
constraints. The constrained MDP (c-MDP) framework (Altman, 1999) is a central formalism for
tackling safety within RL. This involves maximising reward while maintaining costs within certain
bounds which restricts the set of allowable policies for the MDP. Formally, a c-MDP consists of
an MDP hS, A, P, R, γi and C = {(Li : S × A → R, di ∈ R)|i = 1, 2 . . . n}, which is a set of
safety constraint functions L := (L1, . . . Ln) that the agent must satisfy and {di} which describe
the extent to which the constraints are allowed to be not satisfied. Given a set of allowed policies
ΠC := {π ∈ Π : vLπi ≤ di ∀i = 1, . . . , n} where vLπi(S) = E[Pt∞=0 γtLi(St, at)|S0 = S], the
c-MDP objective is to find a policy π? such that π? ∈ arg maxπ∈ΠC vπ (S), for all S ∈ S. The
accumulative safety costs can be represented using hard constraints, this captures for example
avoiding subregions U ⊂ S. When Li is an indicator function i.e. takes values {0, 1}, it is easy
to see that each vLi represents the accumulated probability of safety violations since vLπ (S) =
E[P∞=0 Yt1(st)∣so = s] = P(Violation).
Safe exploration in RL seeks to address the challenge of learning an optimal policy for a task while
minimising the occurrence of safety violations (or catastrophic failures) during training (Hans et al.,
2008). Since in RL, the model dynamics and reward function are a priori unknown, the aim is to keep
the frequency of failure in each training episode as small as possible.
Markov games. Our framework involves a system of two agents each with their individual objectives.
Settings of this kind are formalised by MGs, a framework for studying self-interested agents that
simultaneously act over time (Littman, 1994). In the standard MG setup, the actions of both agents
influence both each agent’s rewards and the system dynamics. Therefore, each agent i ∈ {1, 2} has
its own reward function Ri : S × (×i2=1Ai) → R and action set Ai and its goal is to maximise its
own expected returns. The system dynamics, now influenced by both agents, are described by a
transition probability P : S × (×i2=1Ai) × S → [0, 1]. Unlike classical MGs, in our MG, Safety
Agent does not intervene at each state but is allowed to assume control of the system at certain states
which it decides using a form of control known as impulse control (Mguni, 2018).
3
Under review as a conference paper at ICLR 2022
4	Our Framework
We now describe our framework which consists of two core components: firstly an MG between
two agents, Task Agent and a second agent, the Safety Agent and, an impulse control component
which is used by the Safety Agent. The impulse control component allows the Safety Agent to
be selective about the set of states that it assumes control (and in doing so influence the transition
dynamics and reward) so that actions geared towards safety concerns are performed only at relevant
states. This leaves Task Agent to maximise the environment reward everywhere else. Unlike the
c-MDP formulation, the goal of minimising safety violations and maximising the task reward are
now delegated to two individual agents that now have distinct objectives.
4.1	A Markov game of Interventions on One Side
We introduce our new MG of interventions on one side. Formally, our MG is defined by a tuple
G = hN, S, A, A2,safe, P, R1, R2, γi where the new elements are the set of agents N = {1, 2},
A2,safe ⊆ A which is the action set for Safety Agent and the functions Ri : S × A × A2,safe → R
the one-step reward for agent i ∈ {1, 2}. The transition probability P : S × A × A2,safe × S → [0, 1]
takes the state and action of both agents as inputs. Task Agent and Safety Agent use the Markov
policies π : S × A → [0, 1] and π2,safe : S × A2,safe → [0, 1] respectively each of which is contained
in the sets Π and Π2,safe ⊂ Π which are a stochastic policy set and a deterministic policy subset
respectively. Therefore, whenever Safety Agent assumes control, random exploratory actions are
switched off. Lastly, Safety Agent also has a (categorical) policy g2 : S → {0, 1} which it uses to
determine whether or not it should intervene.
Denote by {τk}k≥0 the intervention times or points at which the Safety Agent decides to take an
action so for example if the Safety Agent chooses to take an actions at state s6 and again at state s8,
then τ1 = 6 and τ2 = 8 (we will shortly describe these in more detail). At any instance the transition
dynamics are affected by only the Safety Agent whenever it decides to act (Task Agent influences the
dynamics at all other times). With this in mind, define the function P : S × A × A2,safe × S → [0, 1]
by P (st+1, at, P0≤k≤t at2,safeδtτk , st) where the function δba is the Kronecker-delta function (so
P0≤k≤t at2,safeδtτk is aτ2,safe whenever t = τ1, τ2, . . . and null otherwise). The transition dynamics
are determined by the probability transition function given by
P(s0,a,a2,safe,s) = P(s0,a,s) (1 - 1∕2,safe(a2,sαfe)) + P(s0,a2，safe,s)lA2,safe(a2，safe).	(1)
Note that if Safety Agent plays a fixed policy then the MG reduces to an MDP.
4.2	The Task Agent Objective
The goal of Task Agent is to maximise its expected cumulative reward set by the environment (note
that this does not include safety which is delegated to Safety Agent). To construct the objective for
Task Agent, we begin by defining the function R1 : S × A × A2,safe → R by R1(st, at, at2,safe) =
R(st , at )(1 - 1A2,safe (at ,safe)) + R(s, at2,safe)1A2,safe(at2,safe) where 1Y (y) is the indicator function
which is 1 whenever y ∈ Y and 0 otherwise. The objective that Task Agent seeks to maximise is:
Vπ,(π2,safe,g2)(S) = E χχ γtRι ($”":一) 6^0 ≡ S ,	⑵
t≥0 0≤k
where a% 〜π (∙∣st) is Task Agent's action and a2,safe 〜π2,safe(∙∣ st) isan action chosen by the Safety
Agent. Therefore, the reward received by Task Agent is R(st, at2,safe) when t = τk, k = 0, 1, . . . i.e.
whenever the Safety Agent decides to take an action and R(st, at) otherwise.
4.3	The Safety Agent Objective
The goal of the Safety Agent is to minimise safety violations both during and after training. Unlike
Task Agent whose actions incur no cost, for the Safety Agent, each intervention incurs a cost applied
4
Under review as a conference paper at ICLR 2022
by a cost function c : A2,safe → R>0. The cost ensures any Safety Agent interventions are warranted
by an increase in safety. The objective that Safety Agent seeks to maximise is:
v∏,(π2,safe,g2)(s)=E XXYt (-L(st,αt,αTksafe) - c(aT,Tfe)δTk) .	(3)
t≥0 0≤k
where L : S ×A× A2,safe → R is defined by L(st,at, a2,safe) = L(st,aJ(1 一 1/2,safe(a2,safe)) 十
L(s, at2,safe)1A2,safe (at2,safe) and L := (L1, . . . Ln) is a set of constraint functions that indicate
how much a given constraint has been violated and is provided by the environment. Therefore to
maximise its objective, the Safety Agent must determine the sequence of points {τk} at which the
benefit of performing a precise action overcomes the cost of doing so. We specialise to the case
c(a2,sαfe) = K ∙ a2,safe where K is a positive constant. Each function Li can represent a (possibly
binary) signal that indicates a visitation to an unsafe state.
We now mention some key aspects of the framework. In particular, now the task of ensuring safety is
delegated to Safety agent whose sole objective is minimise safety violations throughout the course
of the problem. Secondly, the presence of the (strictly negative) intervention cost induces a selection
process for Safety agent whereby it seeks only to intervene at the set of states for which the reduction
in cumulative safety violations is sufficiently high to merit an intervention. This means that at all
other states, task agent can freely act and in doing so learn to play actions that deliver task rewards
whenever there is no potential for safety violations. Lastly, since the agents learn how to respond to
one another, the strategic interaction between the two agents leads to policies in which task agent
anticipates the actions of the task agent and vice-versa.
4.4	THE Safety Agent IMPULSE CONTROL MECHANISM
The problem for the Safety Agent is to determine at which states it should assume control and what
actions it should perform. We now describe how at a given state Safety Agent decides to intervene
and overwrite Task Agent or not and the magnitudes of such interventions . In our setup at each
state the Safety Agent first makes a binary decision to decide to assume control. Therefore, the
intervention times {τk} are rules that depend on the state that are given by τk = inf {t > τk-1 |st ∈
T, g2(st) = 1} where T is the or sequence of states induced by the joint actions of Task Agent
and the Safety Agent and the probability kernel P . Therefore, by learning an optimal g2 , the Safety
Agent learns the useful states to perform an intervention. As we later explain, these intervention
times are determined by a condition on the state which is easy to evaluate (see Prop. 1).
Unlike (Eysenbach et al., 2018; Turchetta et al., 2020), our approach enables learning a safe interven-
tion policy during training without the need to preprogram manually engineered safety responses and
minimises safety violations during training unlike (Tessler et al., 2018; Dalal et al., 2018). Unlike
(Fisac et al., 2019; Dean et al., 2019; Chow et al., 2019b; Fisac et al., 2019), which require access to
information which is not available without a priori knowledge of the environment, our framework
does not require a priori knowledge of the model of the environment or the unsafe states.
Learning to solve an MG involves finding a stable point in the independent agents’ policies. In our
MG, this means the Safety Agent learns to assume control at a subset of states and minimise safety
violations given Task Agent’s policy and, Task Agent learns to execute actions that maximise the
task objective at all other states (given the actions of Safety Agent).
5	The Learning Method
A key aspect of our framework is the presence of two RL agents that each adapt their play according
to each other’s behaviour. This produces two concurrent learning processes each designed to fulfill
distinct objectives. At a stable point of the learning processes the Safety Agent minimises safety
violations while Task Agent maximises the environment reward. Additionally, Safety Agent learns
the set of states in which to perform an action to maintain safety at the current or future states.
Therefore, central to this process is the decision of where Safety Agent should intervene. The
intervention times are characterised by an ‘obstacle condition’ which can be evaluated online.
We now derive the condition that characterises Safety Agent’s intervention times. We begin by first
defining a key object in preparation for this characterisation:
5
Under review as a conference paper at ICLR 2022
Algorithm 1: Distributive Exploration Safety Training Algorithm (DESTA)
1:	Initialise replay buffers D = {0}, D2,safe = {0}, Dint = {0},
2:	for Nepisodes do
3:	State s0
4:	for t = 0, 1, . . . do
5:	Sample a task action a% 〜π(∙∣st), a safe action a2,safe 〜π2(∙∣st), and an intervention
action atnt 〜g2(∙∣st) (∈ {0,1})
6:	if aitnt = 0 then
7:	Apply task action &弋 so st+1 〜P(・|。力 st). Set a = &弋
8:	else if aitnt = 1 then
9:	Apply safe action at2,safe so st+1 〜P(∙∣a2,safe, st). Set a = a2,safe
10:	end if
11:	Receive reward R(st, a) and cost L(st, a)
12:	Set R1 = R(st, a), R2 = -L(st, a) - c(a), Rint = -L(st, a) - c(a)aitnt
13:	Add the sample (st, a, st+1, R1) to D, the sample (st, a, st+1, R2) to D2,safe, the sample
(st, aitnt, st+1, Rint) to Dint
14:	end for
15:	// Learn the individual policies
16:	Update the policy π using D, the policy π2,safe using D2,safe, the policy g2 using Dint
17:	end for
We first introduce the following object which is required for constructing the Bellman operator:
Definition 1 Let π ∈ Π and π2,safe ∈ Π2,safe be a Task Agent and a Safety Agent policy re-
spectively, then for any sτk ∈ S and for any τk ∈ τ1, τ2, . . ., define by Q2π,π ,	(sτk, πτ2,safe) :=
2,safe
-L(sτk , πτ2k,safe) - c(πτ2k,safe) + γ S dsP (s; πτ2k,safe, sτk )v2 ,	(s). We define the intervention
operator Mπ,π2,safe by the following: Mπ,π2,safe v2π,π ,sae (sτk ) := max Q2π,π ,sae (sτk, a0).
a0∈A2,safe
The quantity Mv2 measures the expected future stream of rewards for Safety Agent after an immedi-
ate (optimal) intervention minus the cost of intervening. The following result characterises the Safety
Agent policy g2 and the times that Safety Agent must perform an intervention.
Proposition 1 For any s ∈ S, the Safety Agent intervention policy g2 is given by the following
expression g2(s) = H (Ea〜∏ ∣Mπ2v∏,π，	(s) — Q∏,π，	(s, a)]) (s) where, Q∏,π，	(s, a):=
-L(s, a) + γ RS ds0P(s0; a, s)v2π,π ,sae (s0) and H is the Heaviside function, moreover Safety
Agent,s intervention times are given by Tk = inf {τ > Tk-ι∣Mπ2,safev∏,π， = v∏,π， }.
Hence, Prop. 1 also characterises the (categorical) distribution g2. Moreover, the times {τk} can be
determined by evaluating if Mv2 = v2 holds. The result yields a key aspect of our algorithm for
executing Safety agent’s activations.
While implementing the intervention policy appears to be straightforward by comparing value
functions, it requires the optimal value functions in question. Furthermore, learning the intervention
policy and these value functions simultaneously resulted in an unstable procedure. As a solution we
propose to learn g2 using an off-the-shelf policy gradient algorithm (such as TRPO, PPO or SAC).
This policy is categorical with values {0, 1} and has a reward Rint equal to —L(st, a) + c(a) if
the safety agent intervenes with an action a = at2,safe, and —L(st, a) if the safety agent does not
intervene, i.e., the task agent applies an action a = at.
Implementing both the safety policy, which maximise the safety associated reward, and the task
policy, which maximises the task reward, can be done using any off-the-shelf policy gradient methods
without modifications. To summarise, we will learn three policies: the first one learning the actions
of the task agent, the second learning the action of the safety agent, if the safety agent intervenes,
and the final learning to intervene.
6
Under review as a conference paper at ICLR 2022
Figure 1: Diagram of the T-Junction environment
We take an off-policy approach and every policy is an instance of SAC with appropriate action spaces
and rewards. Since we learn off-policy every agent can collect the same triplets st, a, st+1, where a
is the current action (i.e., either the task action at or the safety action at2,safe). The rewards that the
policies will receive will differ though. The task policy will receive R1 = R(st, a), the safety policy
will get R2 = -L(st, a)-c(at), and the intervention policy rewards are Rint = -L(st, a)-c(a)aint,
where aint 〜g2(∙∣st). Note that We chose to use the same triplets for all policies since We use
off-policy algorithms, which allows us to use the acquired data more efficiently.
6	Experiments
We performed a series of challenges to see if DESTA 1) learns to perform safe planning 2) learns to
select the appropriate state to perform safe override interventions Which avoids a trade-off betWeen
safety and task performance 3) learns to use deterministic controls to ensure precise actions. In these
tasks, We compared the performance of DESTA With state-of-the-art RL methods for safe learning:
PPO (Schulman et al., 2017), Lagrangian PPO, TRPO (Schulman et al., 2015), CPO (Achiam
et al., 2017), and WCPG (Tang et al., 2019). We then compared DESTA against these baselines on
performance benchmarks in challenging high dimensional problems in safety gym benchmarks (Yang
et al., 2021) Which simulate real-World physical environments and OpenAi’s Lunar Lander.
Experiment 1. Safe Planning: Having an individual agent that is responsible for safety enables our
method learn to plan ahead to minimise safety violations. In particular, it is able to learn to take
actions to avoid safety violations at future states. To demonstrate this, We designed a T-Junction
environment With tWo goal states. The environment is a T-shaped grid of three corridors. The agent
starts at one end of the neutral corridor and must advance to the junction at the other end. From
here it must choose to go left or right into a safe or an unsafe corridor respectively. They each have
a reWard at the far end but the unsafe corridor additionally has a safety cost. In our experiments,
the safe goal has a reWard of 50, the unsafe goal a reWard of 100 and all other squares a one-shot
reWard of 10. Each square in the unsafe corridor has a 10% chance of giving a safety cost of 100.
Therefore, the agent must learn to forego a larger reWard to stay safe, With Safety Agent directing it
doWn the safe corridor. The maximum possible safe return per episode is 170, any higher and the
unsafe corridor Was breached.
We compared our algorithm to PPO, Lagrangian PPO, TRPO, Lagrangian TRPO, SAC and CPO.
DESTA-SAC (ours), Lagrangian PPO and Lagrangian TRPO Were able to solve the environment by
going doWn the correct corridor, With the others turning into the unsafe one. Ours trains faster and is
more stable across seeds than the other baselines that solve the environment.
Experiment 2. Safe precision control using the Lunar Lander (Brockman et al., 2016): Since
Safety Agent uses determinstic controls to perform its actions, Safety Agent is able to perform
precise actions to ensure the safety of the system. This alloWs our method to avoid safety violations
in instances Where precision is required. To verify this claim, We used the Lunar Lander environment
Within OpenAI gym (Brockman et al., 2016). Given that there is no strict definition of safety violation
as in the safety-gym environments (Brockman et al., 2016), We manually defined that a safety
violation is incurred Whenever the spacecraft transitions outside a fixed horizontal threshold radius
from the origin. By introducing such safety violation our goal Was to test if DESTA can exploit
the interplay betWeen the reWard maximisation and cost minimisation (i.e., ensure the spacecraft is
alWays on the near-optimal path), hence enabling stronger and more efficient learning. We include
additional experiment details in the Appendix.
7
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)
Figure 2: Empirical evaluation of DESTA and baselines on T-Junction environment. 32 evaluation
episodes were run every 1000 steps and 5 seeds were used per algorithm. Intervention costs of both
0.1 and 0.5 were used and learning rates were 0.001. Note that 20 SAC gradient steps were required
to achieve the reported performance. (a) Average return of evaluation episodes. On this environment,
170 is the maximum safe return per episode, obtained by moving through all centre squares, the
junction square and all squares in the safe corridor without turning into the unsafe corridor. Any
higher means the unsafe corridor was entered. (b) Average cost of evaluation episodes. (c) Average
training cost per step so far. (d) Average number of correct (left) goals per evaluation episode.
(d)
Figure 3: The lander must land on the pad between two flags.
In Fig. 4(a) we observe that the DESTA agent outperforms all the baseline agents, in terms of both
the sample efficiency and the asymptotic score. Moreover, we observe that DESTA enables more
stable training, possibly due to the reduced action space given the intervention of the Safety Agent.
In Fig. 4(b) we show the accumulative safety violation cost throughout the course of training, and
we again observe that DESTA yields the lowest accumulative cost amongst all the evaluated agents,
whilst maintaining the stability of the cost across different random seeds. Such behaviour indicates
that the Safety Agent is well-trained such that the safety-violation states are much more rarely
encountered comparing to the baseline agents.
We note that the consistent low variance across various independent training runs indicates that
DESTA enables low sensitivity with respect to the randomness given different random seeds that
plagues many of the existing RL algorithms (Colas et al., 2018). We also examined the behaviours of
the intervention through the course of training (Fig. 8), which nicely corresponds with our intuition
that the interventions mostly occur during the early phase of each episode. See the Appendix 10 for
further discussion.
Safety Gym Benchmark Experiments: We compared DESTA with the baselines in challenging
environments within a Safety Gym Benchmark in (Yang et al., 2021). These environments, each
representing simulated physical systems vary in the number and size of hazards, and generation of
goal and hazards’ locations. In the first experiment, the hazards are static. We then tested in a setting
with moving hazards. In terms of performance, DESTA outperforms the best competing baselines.
8
Under review as a conference paper at ICLR 2022

Evaluation Return
ιoo-
0	2000	4∞0	6000	8000	100∞
epoch
0
300000
250000 -
200000
W 150000 -
100000 -
50000
Training AccumuIativeCost
0	2000
4000	6000
epoch
βooo
10000
Figure 4: Empirical evaluation of DESTA on the LunarLander task. (a) Evaluation of the agents on
the LunarLander task. (b) Accumulative safety violation cost during training.
4 2 0
500 600
500 600
0
Figure 6: StaticEnv-v0
10° ‰s3(⅞°10⅛?)0 5。。6。。
ILlməpi
Evaluation Return
Evaluation Cost
150
0.10
0 100
0.02
ω0.08
≤0.06
00.04
0	100 200 300 400 500 600
Steps (x 1000)
100 200 300 400
Steps (x 1000)
Training Cost Rate
100 200^^300 400^^500 600
Steps (x 1000)
---Ours
---CPO
---TRPO
TRPO-
Lagrangian
---WCSAC
500 600	0
Figure 7: DynamicEnv-v0
In terms of safety violations, DESTA incurs a lower number of violations as compared to the best
competing baselines.
7 Conclusion
In this paper, we presented a novel
game theoretical framework to solve
the problem of learning safely. Our
Markov game framework of a Task
agent and a second Safety agent de-
couples the tasks of ensuring safety
and maximising the task reward. This
enables one of the agents to learn com-
plex behaviours at subsets of states to
ensure the goal of minimising safety
violations and at the same time ex-
Figure 5: Safety Gym Benchmark environments. Left: Static
environment with one hazard. Right: Dynamic environment
with multiple hazards.
hibiting adaptive behaviour to the
Task agent that maximises the environment reward at all other states. By presenting a theoreti-
cally solid and empirically robust approach to solving the safe learning problem, our method opens up
the applicability of RL to a range of real-world control problems with sophisticated safety constraints.
In future, we believe our approach of marrying RL, MARL and game can be adopted to solve other
open challenges in RL.
9
Under review as a conference paper at ICLR 2022
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization, 2017.
Eitan Altman. Constrained markov decision processes with total cost criteria: Lagrangian approach
and dual linear program. Mathematical methods ofoperations research, 48(3):387—417, l998.
Eitan Altman. Constrained Markov Decision Processes. CRC Press, 1999.
Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Neural Information Processing Systems
(NeurIPS), 2017. URL https://arxiv.org/abs/1705.08551.
Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and
Animesh Garg. Conservative safety critics for exploration. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=iaO86DUuKi.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Patrick J Butler and David R Jones. Physiology of diving of birds and mammals. Physiological
reviews, 77(3):837-899, 1997.
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-
based approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708, 2018.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031, 2019a.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control, 2019b.
C. Colas, O. Sigaud, and P. Y. Oudeyer. How many random seeds? statistical power analysis in deep
reinforcement learning experiments. arXiv preprint arXiv:1806.08295, 2018.
Alexander I. Cowen-Rivers, Daniel Palenicek, Vincent Moens, Mohammed Abdullah, Aivar Sootla,
Jun Wang, and Haitham Ammar. Samba: Safe model-based & active reinforcement learning, 2020.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
S. Dean, S. Tu, N. Matni, and B. Recht. Safely learning to control the constrained linear quadratic
regulator. In 2019 American Control Conference (ACC), pp. 5582-5588, 2019. doi: 10.23919/
ACC.2019.8814865.
Marc Peter Deisenroth, Carl Edward Rasmussen, and Dieter Fox. Learning to control a low-cost
manipulator using data-efficient reinforcement learning. Robotics: Science and Systems VII, pp.
57-64, 2011.
Mahmoud El Chamie, YUe Yu, and Behget Agikmege. Convex synthesis of randomized policies for
controlled markov chains with density safety upper bound constraints. In 2016 American Control
Conference (ACC), pp. 6290-6295. IEEE, 2016.
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine. Leave no trace: Learning to
reset for safe and autonomous reinforcement learning. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=S1vuO-bCW.
J. F. Fisac, A. K. Akametalu, M. N. Zeilinger, S. Kaynama, J. Gillula, and C. J. Tomlin. A general
safety framework for learning-based control in uncertain robotic systems. IEEE Transactions on
Automatic Control, 64(7):2737-2752, 2019. doi: 10.1109/TAC.2018.2876389.
Jaime F Fisac, Neil F Lugovoy, Viceng Rubies-Royo, Shromona Ghosh, and Claire J Tomlin. Bridging
hamilton-jacobi safety analysis and reinforcement learning. In 2019 International Conference on
Robotics and Automation (ICRA), pp. 8550-8556. IEEE, 2019.
10
Under review as a conference paper at ICLR 2022
Javier Garcia, Fern, and o Ferndndez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research ,16(42):1437-1480, 2015. URL http://jmlr.org/
papers/v16/garcia15a.html.
Shangding Gu, Jakub Grudzien Kuba, Munning Wen, Ruiqing Chen, Ziyan Wang, Zheng Tian,
Jun Wang, Alois Knoll, and Yaodong Yang. Multi-agent constrained policy optimisation. arXiv
preprint arXiv:2110.02793, 2021.
Alexander Hans, Daniel SchneegaB, Anton Maximilian Schafer, and Steffen Udluft. Safe exploration
for reinforcement learning. In ESANN, pp. 143-148. Citeseer, 2008.
Gregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, and Sergey Levine. Uncertainty-aware
reinforcement learning for collision avoidance. arXiv preprint arXiv:1702.01182, 2017.
Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model
predictive control for safe exploration. In Proc. of the Conference on Decision and Control (CDC),
2018. URL http://arxiv.org/abs/1602.04450.
Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong
Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint
arXiv:2109.11251, 2021.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.
David Mguni. Optimal selection of transaction costs in a dynamic principal-agent problem. arXiv
preprint arXiv:1805.01062, 2018.
Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang.
Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning
to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.
John C Platt and Alan H Barr. Constrained differential optimization. In Proceedings of the 1987
International Conference on Neural Information Processing Systems, pp. 612-621, 1987.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pp. 1889-1897, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.
press/v37/schulman15.html.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
Qiang Song and Robert P Leland. An optimal control model of neural networks for constrained
optimization problems. Optimal Control Applications and Methods, 19(5):371-376, 1998.
Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn. Learning to be
safe: Deep rl with a safety critic. arXiv preprint arXiv:2010.14603, 2020.
Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by pid
lagrangian methods. In International Conference on Machine Learning, pp. 9133-9143. PMLR,
2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Yichuan Charlie Tang, Jian Zhang, and Ruslan Salakhutdinov. Worst cases policy gradients. arXiv
preprint arXiv:1911.03618, 2019.
Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. arXiv
preprint arXiv:1805.11074, 2018.
Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe rein-
forcement learning via curriculum induction. arXiv preprint arXiv:2006.12136, 2020.
11
Under review as a conference paper at ICLR 2022
Qisong Yang, Thiago D Simao, Simon H Tindemans, and Matthijs TJ Spaan. Wcsac: Worst-case soft
actor critic for safety-constrained reinforcement learning. In Proceedings of the Thirty-Fifth AAAI
Conference on Artificial Intelligence. AAAI Press, online, 2021.
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game
theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.
12
Under review as a conference paper at ICLR 2022
Part I
Appendix
Table of Contents
8 Hyperparameter Settings
14
9 Proof of Technical Results
15
10 LunarLander experiment histogram
16
13
Under review as a conference paper at ICLR 2022
T junction Lunar lander StaticEnv DynamicEnv
8 Hyperparameter Settings
Table 1: Hyperparameters for DESTA.
JOUUnX
# gradient steps	0	0	8	8
# environment steps	0	0	600k	600k
Agent frequency update	0	0	4	4
Agent batch size	0	0	64	64
Policy network dimensions	{0}	{0}	{256, 256}	{256, 256}
Policy networks activations	ReLU	ReLU	ReLU	ReLU
Value network layer dims	{0}	{0}	{256, 256}	{256, 256}
Value networks activations	ReLU	ReLU	ReLU	ReLU
Discount factor	0	0	0.99	0.99
Polyak update scale	0	0	0.005	0.005
Intervention cost	0	0	0.1	0.5
resimitp
Opt Algorithm	Adam	Adam	Adam	Adam
Policy learning rate	0	0	10-4	10-4
Value function learning rate	0	0	10-4	10-4
Temperature learning rate	0	0	10-4	10-4
14
Under review as a conference paper at ICLR 2022
9 Proof of Technical Results
Proof of Proposition 1
Proof: [Proof of Prop. 1] The proof is given by establishing a contradiction. Therefore suppose that
Mπ,π2 ψ(sτk , I(τk)) ≤ ψ(sτk , I(τk)) and suppose that the intervention time τ10 > τ1 is an optimal
intervention time. Construct the safety agent π02 ∈ Π2 and π2 policy switching times by (τ0, τ1,...,)
and π02 ∈ Π2 policy by (τ00, τ1, . . .) respectively. Define by l = inf{t > 0;Mπ,π2ψ(st,I0) =
ψ(st, I0)} and m = sup{t; t < τ10}. By construction we have that
v2π,π02(s,I0)
=E hR(s0,a0)+E h...+γl-1E hR(sτ1-1,aτ1-1)+...+γm-l-1E hR(sτ10-1, aτ10-1) + γMπ,π02v2π,π02(s0, I(τ10))iiii
< E [r(so, a。) + E [... + Y l-1E [r(st1-i, aτ1-i) + Y Mπ,π2 谡/(sτ1 ,[(乃))]]]
We now use the following observation E
IR(STι-1,aτι-l) + YMπ,π2v2π,π (STI ,I(TIH
,π02(sτ1, I(τ1)), max	R(sτk, aτk) + Y Ps0∈S
aτ1 ∈A
P(S0;aT1,
ST1)v2π,π2(S0, I(τ1))i	.
Using this we deduce that
v2π,π02 (S, I0) ≤ E R(S0, a0) +E . . .
+ Y l-1E R(sτι-I,aτ1-1) + Y max< Mπ,π2 GF (sτ1 ,I (τι)), max	R(STk ,ατk) + Y X P (s0； aτι,Sτι)v2" (S,I (τι))
aτ1∈A	s0∈S
=E [r(so, a。)+ E [... + Y l-1E [冗⑸1,。…)+ Y [置产](sτι ,I(τι ))口] = v∏,π2 (s,Io))
where the first inequality is true by assumption on M. This is a contradiction since π02 is an optimal
policy for safety agent. Using analogous reasoning, we deduce the same result for τk0 < τk after
which deduce the result. Moreover, by invoking the same reasoning, we can conclude that it must be
the case that (τo, τι,..., Tk-ι,Tk, Tk+ι,...,) are the optimal intervention times.	□
15
Under review as a conference paper at ICLR 2022
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
Intervention Density Plot
-1.5	-1.0	-0.5	0.0	0.5
1.0	1.5
Figure 8: Histogram of intervention location during training.
LunarLander experiment histogram
10
16