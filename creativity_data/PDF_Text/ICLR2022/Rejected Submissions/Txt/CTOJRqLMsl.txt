Under review as a conference paper at ICLR 2022
On the Convergence of Nonconvex Continual
Learning with Adaptive Learning Rates
Anonymous authors
Paper under double-blind review
Ab stract
One of the objectives of continual learning is to prevent catastrophic forgetting in
learning multiple tasks sequentially. The memory based continual learning stores
a small subset of the data for previous tasks and applies various methods such as
quadratic programming and sample selection. Some memory-based approaches
are formulated as a constrained optimization problem and rephrase constraints on
the objective for memory as the inequalities on gradients. However, there have
been little theoretical results on the convergence of continual learning. In this
paper, we propose a theoretical convergence analysis of memory-based continual
learning with stochastic gradient descent. The proposed method called nonconvex
continual learning (NCCL) adapts the learning rates of both previous and current
tasks with the gradients. The proposed method can achieve the same convergence
rate as the SGD method for a single task when the catastrophic forgetting term
which we define in the paper is suppressed at each iteration. It is also shown
that memory-based approaches inherently overfit to memory, which degrades the
performance on previously learned tasks. Experiments show that the proposed
algorithm improves the performance of continual learning over existing methods
for several image classification tasks.
1	Introduction
Learning new tasks without forgetting previously learned tasks is a key aspect of artificial intelligence
to be as versatile as humans. Unlike the conventional deep learning that observes tasks from an
i.i.d. distribution, continual learning train sequentially a model on a non-stationary stream of data
(Ring, 1995; Thrun, 1994). The continual learning AI systems struggle with catastrophic forgetting
when the data access of previously learned tasks is restricted (French & Chater, 2002). To overcome
catastrophic forgetting, continual learning algorithms introduce novel methods such as a replay
memory to store and replay the previously learned examples (Lopez-Paz & Ranzato, 2017; Aljundi
et al., 2019; Chaudhry et al., 2019a), regularization methods that penalize neural networks (Kirkpatrick
et al., 2017; Zenke et al., 2017), Bayesian methods that utilize the uncertainty of parameters or data
points (Nguyen et al., 2018; Ebrahimi et al., 2020), and other recent approaches (Yoon et al., 2018;
Lee et al., 2019). In this paper, we focus on the online continual learning with replay memory. The
learner stores a small subset of the data for previous tasks into a memory and utilizes the memory
by replaying samples to keep the model staying in a feasible region corresponding to moderate
suboptimal region. Gradient episodic memory (GEM) (Lopez-Paz & Ranzato, 2017) first formulated
the replay based continual learning as a constrained optimization problem. This formulation rephrases
the constraints on objectives for previous tasks as the inequalities based on the inner product of
loss gradient vectors for previous tasks and a current task. However, the theoretical convergence
analysis of the performance of previously learned tasks, which implies a measure of catastrophic
forgetting, has not been rigorously studied in the literature. Without convergence analysis, this
intuitive reformulation of constrained optimization does not provide theoretical guarantee to prevent
catastrophic forgetting.
Nonconvex finite-sum optimization problem offers a solution to analyze catastrophic forgetting by
measuring the convergence of previously learned tasks, which is related to the performance. Most
deep learning problems are defined as nonconvex optimization, and the target objective is composed
of the sum of objectives for each data point. Now we express our continual learning problem of the
1
Under review as a conference paper at ICLR 2022
form
1n
mRdf (X) = n Efi(X)，
i=1
(1)
where we assume that each objective fi (X) with a model X and a data point i is nonconvex with
Lipschitz gradient. Here, we expect that a stochastic gradient descent based algorithm reaches a
stationary point instead of the global minimum in the nonconvex optimization. This generic form
is well studied to demonstrate the convergence and complexity of stochastic gradient methods for
a nonconvex setting (Zhou & Gu, 2019; Lei et al., 2017; Reddi et al., 2016a; Zaheer et al., 2018).
Unlike the convex case, the convergence is generally measured by the expectation of the squared
norm of the gradient Ek▽/(χ)k2. The theoretical complexity is derived from the e-accurate solution,
which is also known as a stationary point with Ek▽/(χ)k2 ≤ e. Suppose We divide the entire
sum of objectives into two terms for previous tasks and current tasks, and measure the convergence
on each term. Then, we can observe the transition of convergences on the previous and current
tasks respectively while learning sequentially from a data stream. We consider this transition of
convergence on the previous task as catastrophic forgetting if Ek Vfp (x) k2 with the set of data points
from previous tasks P increases over iterations.
In this work, we formulate continual learning problem as a nonconvex finite-sum optimization with
stochastic gradient descent algorithm that updates both previously learned tasks from replay memory
and the current task simultaneously, and present a theoretical convergence analysis of continual
learning problems by leveraging the replay-based update method. This extends the continual learning
algorithm such as ER-Reservoir (experience replay with reservoir sampling) with fixed and same
learning rates on both two tasks to our adaptive method which controls the relative importance
between tasks at each step with theoretical guarantee. In addition, the replay-based continual learning
has the critical limitation of overfitting to memory, which also degrades the performance of previously
learned tasks like catastrophic forgetting by interference. Itis known that choosing the perfect memory
for continual learning to prevent catastrophic forgetting is an NP-hard problem by (Knoblauch et al.,
2020). We present that the inductive bias by replay memory, which prevents perfect continual learning
is inevitable in view of optimization.
2	Backgrounds
The continual learning algorithm with a replay memory with size m cannot access the whole dataset
of the previously learned tasks with nf samples, but uses limited samples in the memory when a
learner trains on the current task. This limited access does not guarantee to completely prevent
catastrophic forgetting, and causes the overfitting problem with the biased gradient on a memory.
In Section 3, we provide the convergence analysis of the previously learned tasks f (X), which are
vulnerable to catastrophic forgetting.
As we denote fi (X) as the component, which indicates the loss of sample i from the previously
learned tasks with the model parameter X, we define Vfi (X) as its gradient. We use It, Jt as the
mini-batch of samples at iteration t and denote the mini-batch size |It | and |Jt | as bf , bg throughout
the paper. We also note that gj (X), which denotes the loss for the current task and will be defined in
Section 3.1, satisfies the above and following assumptions.
To formulate the convergence over iterations, we introduce the Incremental First-order Oracle
(IFO) framework (Ghadimi & Lan, 2013), which is defined as a unit of cost by sampling the pair
(Vfi(X), fi(X)). For example, a stochastic gradient descent algorithm requires the cost as much as
the batch size bt at each step, and the total cost is the sum of batch sizes PtT=1 bt. Let T (e) be the
minimum number of iterations to guarantee e-accurate solutions. Then the average bound of IFO
complexity is less than or equal to PtT=(1) bt .
To analyze the convergence and compute the IFO complexity, we define the supremum of loss gap
between a local optimal point χ0 and the global optimum x* as
∆f = supf(x0) - f(x*).
x0
(2)
Suppose that sup f(x0) is same with f(x*), then we have ∆f = 0, which might be much smaller
than the loss gap of general SGD. Without the continual learning scenario, a general nonconvex SGD
2
Under review as a conference paper at ICLR 2022
updates the parameters from an randomly initialized point, which is highly likely to have the large
loss f (x0). Then, ∆f > 0 is the key constant to determine the IFO complexity for convergence
as ∆f is in the numerator of Equation 11. However, a continual learning algorithm has already
converged to a local optimal point x0 for the previous task f(x) and might get a much smaller ∆f
than the general SGD. It means that ∆f for nonconvex continual learining in Equation 11 dose not
have a large impact on the IFO complexity. To generalize the theoretical result, we define the worst
local minimum to explain the upper bound of convergence rate in Equation 2. This implies that ∆f is
not a critical reason for moving away from stationary points of f by catastrophic forgetting, which
we will explain in Section 3.
We also define σf and σg for f and g, respectively, as the upper bounds on the variance of the
stochastic gradients of a given mini-batch. For brevity, we write only one of them σf,
σf = sup — XkVfi(X)-Vf (x)k2.	⑶
x nf i=1
Throughout the paper, we assume the L-smoothness.
Assumption 1. fi is L-smooth that there exists a constant L > 0 such that for any x, y ∈ Rd,
kVfi(x) - Vfi(y)k ≤Lkx-yk	(4)
where |卜|| denotes the Euclidean norm. Then thefollowing inequality directly holds that
-Lkx - yk2 ≤ fi(x) - fi(y) - hVfi(y),χ - y ≤ Lkx - yk2.	(5)
We derive the inequality in Appendix B. With Assumption 1, we can successfully handle individual
noncovex objectives for each data point. In the next section, we investigate nonconvex continual
learning with adaptive learning rates to overcome catastrophic forgetting.
3	Nonconvex Continual Learning
We first present a theoretical convergence analysis of memory-based continual learning in nonconvex
setting. We use the convergence rate of stochastic gradient methods, which denotes the IFO complex-
ity to reach an -accurate solution for smooth nonconvex finite-sum problem (Reddi et al., 2016a).
This generic form enables both deep learning and optimization communities to formulate various
accelerated gradient methods with theoretical guarantee. We seek to understand why catastrophic
forgetting happens in terms of the convergence rate, and propose non-convex continual learning
(NCCL) algorithms with theoretical convergence analysis.
3.1	Problem Formulation
Given two finite sets P and C at the initial time step t = 0, we let two sets denote the sets of
indices for previously learned data points and upcoming data points, respectively. Note that the task
description for a continual learner is two separate sets. In this section, we will show a convergence
analysis of the model parameter that we have trained on P and starts to learn C. Thus, we simply
denote a data stream of continual learning as two consecutive sets P and C.
We consider our goal as a smooth nonconvex finite-sum optimization problem with two objectives
min F(X) = f(x) + g(x) = — X fi(x) + — X gj(x),	⑹
x∈Rd	nf	n
x∈	f i∈P	g j∈C
where fi(x) and gj (x) denote the objectives of data points i ∈ P andj ∈ C, respectively. In addition,
nf and ng are the numbers of elements for P and C . To ease exposition, we use a different notation
gj(x) for a data point j ∈ C, which is usually the same objective function for a data point i ∈ P.
To formulate a theoretical convergence analysis of continual learning, we consider a replay memory
based method of which memory is a subset of P ∪ C. Let a random variable Mt ⊂ P ∪ C be the
replay memory at time step t ∈ [0, T], whose union is of the form M := ∪tMt. We focus both the
3
Under review as a conference paper at ICLR 2022
episodic memory and the replay memory with dropping rule. The episodic memory based methods
include GEM (Aljundi et al., 2019), A-GEM (Chaudhry et al., 2019a), and ORTHOG-SUBSPACE
(Chaudhry et al., 2020). ER-Reservoir (Chaudhry et al., 2019b) is a replay memory based method
with dropping rule, which replaces the dropped sample d ∈ Mt with a sample in the stream for C .
We now define the gradient update of continual learning
xt+1 = Xt- α∙Ht VfIt (Xt)- βHtVgJt (xt),	(7)
where It ⊂ Mt and Jt ⊂ C denote the mini-batches from the replay memory and the current data
stream, respectively. Here, Ht is the union of It and Jt. The adaptive learning rates of VfIt (Xt)
and VgJt (Xt) are denoted by αHt and βHt which are the functions of Ht. Strictly speaking, the
mini-batch It from Mt might contain a datapoint d ∈ C for ER-Reservoir. We describe the details of
the problem in Appendix B and assume that the notation It indicates a subset of P for convenience.
Equation 7 is a generalized version of continual learning algorithms, which is our novelty to prove
the convergence rates in the nonconvex setting for the proposed method, A-GEM, and ER-Reservoir
later.
3.2	Memory-based Nonconvex Continual Learning
Unlike the conventional smooth nonconvex finite-sum problem where each mini-batch is iid-sampled
from the dataset P ∪ C, the replay memory based continual learning encounters a non-iid stream of
data C and has access to a small sized memory M . Algorithm 1 presents the pseudocode of NCCL
by Equation 7. By the limited access to P, the gradient update for f(X) in Equation 7 is a biased
estimate of the gradient Vf(Xt). Specifically, at the timestep t,
VfMt(Xt) =EIt VfIt(Xt)|Mt =EIt Vf(Xt) + et|Mt =Vf(Xt)+eMt,
where et and eMt denote the error term VfIt (Xt) - Vf(Xt) and the expectation value over It given
Mt, respectively. We note that the given replay memory Mt with small size at timestep t induces the
inevitable overfitting bias. We first state an intermediate result for a single gradient update of NCCL.
For ease of exposition, we define the overfitting term Bt and the catastrophic forgetting term Γt as
follows:
Bt = (Lα2Ht - αHt )hVf (Xt), eti + βHt hVgJt (Xt), eti,
β2 L
Γt =?kVgjt (xt)k2 - βHt (1 - αHtL)hVfIt (xt), VgJt (xt)i,
where L is a constant for Lipschitz smoothness. Under Assumption 1, a single gradient update by
Equation 7 satisfies the following bound by letting X J xt+1 and y J xt:
(a% - La%) kVf (xt)k2 ≤ f (xt) - f (xt+1) +Γt + Bt + Lα% |同|2.	(8)
This reveals the basic qualitative difference between the conventional nonconvex SGD and NCCL in
the convergence rate. Compared to the nonconvex SGD, there exist two terms Bt and Γt in Equation
8. We group the terms containing et as Bt and the other terms as Γt . We note that the catastrophic
forgetting term Γt has hVfIt (Xt), VgJt (Xt)i, which is the key aspect of interference and transer
(Riemer et al., 2018), and Bt includes the error term between the batch of M and the entire dataset
P . Then, we can quantify the amount of overfitting by tracing Bt .
To compute the expectation over the stochasticity of NCCL, we derive the expectation of VfMt (Xt)
over the sampling rule of Mt . The episodic memory Mt = M0 for all t is uniformly sampled once
from the random sequence of P , and ER-reservoir iteratively samples the replay memory Mt by the
selection probability P(Mt|Mt-1).
Lemma 1. Let the history of Mt be M[o：t] = (Mo, ∙∙∙ ,Mt). If Mo is uniformly sampled from P,
then both episodic memory and ER-reservoir satisfies
EM[0:t] VfMt(Xt) =Vf(Xt) and EM[0:t] [eMt] =0.	(9)
We provide the detailed proof in Appendix B. Note that taking expectation iteratively with respect
to the history M[o:t] is needed to compute the expected value of gradients for Mt. Since taking the
expectation over the stochasticity of NCCL implies the total expectation
E = EM[o：t] [EIt [EJt [ ∙ |It]] |M[o：t]],
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Nonconvex Continual Learning (NCCL)
Input: Previous task P, K task data stream {Dι, •一DK}, initial model χ0, memory Mo ⊂ P
for k = 1 to K do
for t = 0 to T - 1 do
Uniformly sample a mini-batch It ⊂ Mt with |It | = bf
Uniformly sample a mini-batch Jt ⊂ Dk with |Jt | = bg
Compute learning rates α% and β% with NfIt (Xt) and VgJt (Xt)
Xt+1 - xt - aHt VfIt(xt) - βHt VgJt (xt)
Store Jt into Mt+1 by the rule of replay memory scheme
end for
χ0 J XT-1
P J P ∪ UtJt
M0 J MT-1
end for
we know that E[Bt] is also 0. We note that considering a random choice of M0 allows us to analyze
the convergence on f (X), even if M0 cannot access the all datapoints in P to compute kVf(Xt)k2.
Our first main result is the following lemma that provides the stepwise change of upper bound.
Lemma 2. Suppose that Assumption 1 holds and 0<α% ≤ L2. Thenfor xt updated by Algorithm 1,
we have the following bound
EkVf(Xt)k2 ≤ E	——1^--	(f(xt)- f(xt+1) +	Bt	+Γt)	+ MaHL L、σf	,(10)
αHt (1 一 ɪaHt )	2(1 - ɪ aHt ) _
where the effect of Bt vanishes by E[Bt] = 0.
The proof is presented in Appendix B. Surprisingly, taking the expectation over M0 ⊂ P in Equation
10 to handle the stochasticity of choosing M0 allows us to analyze the convergence of f(X) in Section
3.3. We also note that the individual trial with a randomly given M0 cannot cancel the effect by Bt,
although its total expectation over the whole possible trials E[Bt] is zero. More specifically, the worst
case supM0 kVf(Xt)k2 definitely contains the non-zero value of Bt in the upper bound term. We
discuss the more details of the overfitting to memory in Appendix D.
3.3	Theoretical Convergence Analysis
We now describe our convergence analysis of the stochastic gradient descent by Equation 7. Without
any continual learning scheme, the model Xt eventually converges to M ∪ C, not P ∪ C . A local
optimal point X0 for P at time step t = 0, which we have already trained before the continual
learning step starts, loses the performance on P very quickly over time. This phenomenon is called
catastrophic forgetting. We also note that the overfitting to replay memory is another reason of
degrading performance for each trial when the model trains on samples in M with replacement after
learning on P for previous tasks. We obtain the convergence rate of NCCL to a stationary point of f
as the following theorem.
Theorem 1. Let aH = a = -√T for some 0 < c ≤ 2jT and t ∈ {0, ∙∙∙ ,T — 1}. By Lemma 2, the
iterates of NCCL satisfy
mtin EkVf(Xt)k2 ≤ √at (C (△/ + £ E [「J + 1。'),	(11)
where A = 1/(1 - La/2).
For completeness we present a proof in Appendix A. The loss gap △f and the variance of gradients
σf are fixed values. We first note that there exists the cumulative sum of E[Γt] unlike the convergence
rate of SGD. Most of novelty in our analysis lies in dealing with P E[Γt]. The PT-1 E[Γt]∕√T
5
Under review as a conference paper at ICLR 2022
is not guaranteed to converge to 0. This fact gives rise to catastrophic forgetting in terms of the
nondecreasing upper bound.
Before showing a bound on IFO calls made by NCCL, we present the convergence rate for g(x).
To simplify the proof, We assume that learning rates α%, β% are a same fixed value β = c0/√T.
The assumption is reasonable, because we can observe that RHS of Equation 10 is not perturbed
drastically by the learning rates With small value 0 < αHt , βHt ≤ 2/L 1. Then We have the
folloWing lemma.
Lemma 3. Suppose that It ∩ Jt = 0, and the datapoints d ∈ M ∩ P use the same objective function
gd = fd. Taking expectation over It ⊂ Mt and Jt ⊂ C, we have
min EkVg(xt)k2 ≤
(12)
where ∆g and σg is the version of loss gap and the variance for g on M ∪ C, respectively. In fact,
we note that the convergence rate ofg is on M ∪ C. Despite the convergence rate is not on C, it also
converges to C trivially.
NoW We can get the upper bound of	E[Γt] by Lemma 3. Then We have the folloWing main result.
Corollary 1. The IFO complexity of Algorithm 1 to obtain an -accurate solution is:
IFO calls = O(1/2),
∞,
E[Γt] = O(1),
E[Γt] = O(T).
(13)
The proofs of both Lemma 3 and Corollary 1 are presented in Appendix B. Here, We denote ∞ as the
impossibility of convergence. This result reveals that catastrophic forgetting is inevitable Without
minimizing the cumulative sum P E[Γt]. With replay memory, We knoW that the inner product in Γt
can be negative by the result of A-GEM (Chaudhry et al., 2019a). This observation shoWs that there
exists the chance that Γt becomes negative, and P E[Γt] can converge to a constant O(1) as t → ∞.
NoW We have the proper convergence rate of f (x), and the model can keep the performance on P
after learning on C. On the other hand, When αHt = 0 for all time step, E[Γt] is alWays larger than
0. Intuitively, this case implies SGD on C Without the replay memory M or other supports, and the
cumulative sum P E[Γt] monotonically increases over time. Then, f(xt) diverges as t → ∞. We
emphasize that the reason of catastrophic forgetting can be explained remarkably by the nonconvex
optimization perspective.
The optimization problem of nonconvex continual learning. The local optimal point xt after
learning on C might be different from χ0, because Xt moves towards XP∪c as described in Figure 1.
The above observation motivates the folloWing formulation of continual learning to induce P E[Γt]
to converge as t → ∞ and keep -accuracy of f during T iterations. More specifically, we solve the
following problem over T:
minimize
αHt, βHt
T-1
XΓt
t=0
subject to 0 < αHt, βHt ≤ 2/L for all t < T.
(14)
Recall that we have the condition of two learning rates to prove Theorem 1 and Lemma 3. This
condition needs to be constraints of the above optimization problem.
Finally, we note that our result first shows a theoretical convergence analysis of memory based
nonconvex continual learning in the setting of Corollary 1. More precisely, we can now explain this
method by Problem 14 with theoretical guarantee.
3.4	Adaptive Learning Rates
As discussed in Section 3.3, the minimization problem 14 leads to tight the upper bound of conver-
gence rate in nonconvex setting. The result of convergence analysis provides a simple continual
learning framework that only adjusts two learning rates in Equation 7. First, we review A-GEM
and other methods with fixed learning rate such as GSS (Aljundi et al., 2019) and ER-Reservoir
(Chaudhry et al., 2019b) through our result.
6
Under review as a conference paper at ICLR 2022
A-GEM (Chaudhry et al., 2019a) propose a surrogate of RgJt (Xt) as the following equation to avoid
violating the constraint when NfIt (Xt), RgJt (xt)i ≤ 0:
VgJt (Xt)Y f¾, VgJt 2
RfIt (Xt)
kVfIt (Xt) IL
We can interpret the surrogate as boosting the learning rate αHt to cancel out the negative com-
ponent of VfIt (Xt) on VgJt (Xt). After applying the surrogate, E[Γt] is reduced but still non-
negative, which we show in Appendix C. A-GEM also fails to fully use the advantage of the case
hVfIt(Xt),VgJt(Xt)i >0.
Fixed learning rates imply that building a replay memory is the key to success in continual
learning. It means that this memory setting is an abundant pool to sample It , which satisfies
hVfIt (Xt), VgJt (Xt)i > 0, as much as possible. This can reduce P E[Γt].
However, two above methods cannot enjoy the case hVfIt (Xt), VgJt (Xt)i > 0 enough. As discussed
above,we note that Γt is a quadratic polynomial of βHt where βHt > 0. We can solve the minimum
of the polynomial on βHt when hVfIt (Xt), VgJt (Xt)i > 0. By differentiating on βHt, we can easily
find the minimum Γ J= and the optimal learning rate βHt
β* = (I — αHt L)hVfIt (Xt), VgJt (Xt))	γ* = (I — αHt LWIt (Xt), VgJt (Xt))
βHt =	LkVgJt (Xt)k2	, t =	2L∣∣VgJt (Xt)Il2	.
We integrate the above result to propose a better nonconvex continual learning algorithm with the
theoretical guarantee. More specifically, we propose an adaptive learning rate method that can reduce
P E[Γt] in both cases of hVfIt (Xt), VgJt (Xt)) ≤ 0 and hVfIt (Xt), VgJt (Xt)) > 0. Then, the
proposed adaptive learning rate scheme is as follows.
1 _ anfit(Xt)NgJt(Xtyi ∖
QHt =	Q(I	kVfit (xt)k2 一),
α,
hVfIt(Xt),VgJt(Xt)) ≤ 0
hVfIt (Xt), VgJt (Xt)) > 0
Q,	hVfIt(Xt),VgJt(Xt)) ≤ 0
βHt = (1-αL)LftJ((Xx%gJt(xt)i, EfIt (Xt), VgJt(Xt)i > 0
(15)
(16)
We derive the details of the above result in Appendix C. In addition, our result shows that A-GEM can
be viewed as an adaptive learning rate scheme using samples in Mt directly. This fact implies that
there is no difference between A-GEM and experience replay methods. Figure 1 illustrates intuitively
how scaling learning rates achieve the convergence to a mutual stationary point X=P ∪C as we proved
the theoretical complexity in Corollary 1.
4	Experiments
We evaluate the proposed NCCL and the state of the art online continual learning baselines on the
following benchmarks. We report detailed experimental materials and results in Appendix.
Datasets. We demonstrate the experimental results on standard continual learning benckmarks:
Permuted-MNIST (Kirkpatrick et al., 2017) is a MNIST (LeCun et al., 1998) based dataset, where
each task has a fixed permutation of pixels and transform data points by the permutation to make
each task distribution unrelated. Split-MNIST (Zenke et al., 2017) splits MNIST dataset into five
tasks. Each task consists of two classes, for example (1, 7), (3, 4), and has approximately 12K images.
Split-CIFAR10 and 100 also split CIFAR-10 and 100 datasets (Krizhevsky et al., 2009) into five
tasks and 20 tasks, respectively.
Baselines. We report the experimental evaluation on the online continual setting which implies a
model is trained with a single epoch. We compare with the following continual learning baselines.
Fine-tune is a simple method that a model trains observed data naively without any support, such
as replay memory. Elastic weight consolidation (EWC) is a regularization based method by Fisher
Information (Kirkpatrick et al., 2017). ER-Reservoir chooses samples to store from a data stream
with a probability proportional to the number of observed data points. The replay memory returns
7
Under review as a conference paper at ICLR 2022
a random subset of samples at each iteration for experience replay. ER-Reservoir (Chaudhry et al.,
2019b) shows a powerful performance in continual learning scenario. GEM and A-GEM (Lopez-Paz
& Ranzato, 2017; Chaudhry et al., 2019a) use gradient episodic memory to overcome catastrophic
forgetting. The key idea of GEM is gradient projection with quadratic programming and A-GEM
simplifies this procedure. We also compare with iCarl, MER, ORTHOG-SUBSPACE (Chaudhry
et al., 2020), and stable-SGD (Mirzadeh et al., 2020).
Architecture and Training detail. For a fair comparison, we follow the commonly used model
architecture and hyperparameters of Lee et al. (2020); Chaudhry et al. (2020). For Permuted-MNIST
and Split-MNIST, we use fully-connected neural networks with two hidden layers of [400, 400] or
[256, 256] and ReLU activation. ResNet-18 with the number of filters nf = 64, 20 (He et al., 2016)
is applied for Split CIFAR-10 and 100. All experiments conduct a single-pass over the data stream. It
is also called 1 epoch or 0.2 epoch (in the case of split tasks). We deal both cases with and without the
task identifiers in the results of split-tasks to compare fairly with baselines. Batch sizes of data stream
and memory are both 10. All reported values are the average values of 5 runs with diffrent seeds, and
we also provide standard deviation. Other miscellaneous settings are the same as in Chaudhry et al.
(2020). We implement the baselines and the propose method on Tensorflow 1. For evaluation, we use
an NVIDIA 2080ti GPU along with 3.60 GHz Intel i9-9900K CPU and 64 GB RAM.
4.1	Results
The following tables show our main experimental result, which is averaged over 5 runs. We denote
the number of example per class per task at the top of each column.
Table 1: Multi-headed split-CIFAR100, reduced size Resnet-18 nf = 20. Accuracy and forgetting
results.
Method	memory size	1		5	
	memory	accuracy	forgetting	accuracy	forgetting
EWC	X	42.7(1.89)	0.28 (0.03)	42.7 (1.89)	0.28 (0.03)
Fintune	X	40.4 (2.83)	0.31 (0.02)	40.4 (2.83)	0.31 (0.02)
Stable SGD	X	59.9 (1.81)	0.08 (0.01)	59.9 (1.81)	0.08 (0.01)
A-GEM	✓	50.7 (2.32)	0.19 (0.04)	59.9 (2.64)	0.10 (0.02)
ER-Ring	✓	56.2 (1.93)	0.13 (0.01)	62.6 (1.77)	0.08 (0.02)
ER-Reservoir	✓	46.9 (0.76)	0.21 (0.03)	65.5 (1.99)	0.09 (0.02)
ORTHOG-subspace	✓	58.81 (1.88)	0.12 (0.02)	64.38 (0.95)	0.055 (0.007)
NCCL + Ring	✓	54.63 (0.65)	0.059 (0.01)	61.09 (1.47)	0.02 (0.01)
NCCL + Reservoir	✓	52.18 (0.48)	0.118 (0.01)	63.68 (0.18)	0.028 (0.009)
Table 2: Permuted-MNIST (23 tasks 60000 examples per task), FC-[256,256]. Accuracy and
forgetting results.
Method	memory size	1		5	
	memory	accuracy	forgetting	accuracy	forgetting
multi-task	X	83	-	83	-
Fine-tune	X	53.5 (1.46)	0.29 (0.01)	47.9	0.29 (0.01)
EWC	X	63.1 (1.40)	0.18 (0.01)	63.1 (1.40)	0.18 (0.01)
MER	✓	69.9 (0.40)	0.14 (0.01)	78.3 (0.19)	0.06 (0.01)
A-GEM	✓	62.1 (1.39)	0.21 (0.01)	64.1 (0.74)	0.19 (0.01)
ER-Ring	✓	70.2 (0.56)	0.12 (0.01)	75.8 (0.24)	0.07 (0.01)
ER-Reservoir	✓	68.9 (0.89)	0.15 (0.01)	76.2 (0.38)	0.07 (0.01)
ORHOG-subspace	✓	84.32(1.10)	0.12 (0.01)	84.32 (1.1)	0.11 (0.01)
NCCL + Ring	✓	74.22 (0.75)	0.13 (0.007)	84.41 (0.32)	0.053 (0.002)
NCCL+Reservoir	✓	79.36 (0.73)	0.12 (0.007)	88.22 (0.26)	0.028 (0.003)
8
Under review as a conference paper at ICLR 2022
Overall, NCCL variants outperforms baseline methods especially in the forgetting metric. Our goal
is to demonstrate the usefulness of the adaptive learning rate scheme to reduce the catastrophic
forgetting, and verify the proposed theoretical convergence analysis. We remark that our adaptive
learning rates successfully suppress forgetting by a large margin compared to baselines. Note that
NCCL also outperform A-GEM, which does not maximize transfer g于丸(Xt), VgJt (Xtyi > 0. Now,
we can empirically demonstrate our theoretical guaranteed method by minimizing P Γt is valid.
We clipped βHt to increase the performance. As we discussed earlier, we can prevent forgetting when
hVfIt (Xt), VgJt (Xt)i > 0. However, we observe that kVfIt(Xt)k2 suddenly increases because
of the interference at the previous step t - 1. The very large learning rate βHt by the increased
kVfIt (Xt)k can force the model to fall into an arbitrary point that is likely to increase the loss of f.
Clipping the learning rate reduces this problem and still has the effect of reducing the catastrophic
forgetting term Γt . By the property of quadratic polynomial, the catastrophic forgetting term is
negative because the clipped value is smaller than the original learning rate.
We show that NCCL is a potentially powerful alternative for continual learning. Even with tiny
replay memory, NCCL still performs better than some baselines. We note that NCCL shows the best
performance on the forgetting metric. It implies that NCCL prevent catastrophic forgetting more
efficiently than others by minimizing the catastrophic forgetting term in the proposed optimization
problem. However, the accuracy is slightly lower than other baselines, which include experience
replays. The purpose of our adaptive learning rate scheme is to prevent catastrophic forgetting, so the
performance of current task is slightly lower than ER-Ring, stable-SGD, and ORTHOG-subspace.
This result shows that the plasticity to learn a new task is restricted by NCCL variants with tiny
memory. In particular, we would expect that NCCL would benefit from the additional enhancements
in ORTHOG-SUBSPACE and stable SGD by introducing their techniques. In appendix, we add more
results with larger sizes of memory, which shows that NCCL outperforms on the average accuracy.
We conclude that the transfer effect by the small size of memory for NCCL is less effective.
5	Related Work
We extend the memory based continual learning (Lopez-Paz & Ranzato, 2017; Chaudhry et al.,
2019a; Riemer et al., 2018; Chaudhry et al., 2019b) to the nonconvex optimization problem to provide
the theoretical guarantee of previous methods. Our problem setting is related to the theoretical
convergence analysis of smooth nonconvex optimization. Smooth nonconvex finite-sum optimization
problem has been widely employed to derive the theoretical complexity of computation for stochastic
gradient methods (Ghadimi & Lan, 2013; 2016; Lei et al., 2017; Zaheer et al., 2018; Reddi et al.,
2016a). Unlike the convex optimization, the gradient based algorithms are not expected to converge
to the global minimum but are evaluated by measuring the convergence rate to the stationary points
in the nonconvex case. Prior works have only focused on analyze the problems with two objectives
where the additional term is for regularization (Yao & Kwok, 2016). Here, we instead focuses on
the continual learning problem, which has two objectives for both datapoints of the previous task
and the current task. This setting is interesting since we can observe the convergence of each task
respectively. Otherwise, (Knoblauch et al., 2020) first develops a theoretical approach on continual
learning by set theory, and shows that optimal continual learning algorithms and building a perfect
memory is equivalent. This is analogous to our result that we cannot handle the overfitting bias for
each trial of continual learning.
6	Conclusion
In this paper, we have presented the first generic theoretical convergence analysis of continual learning.
Our proof shows that a training model can circumvent catastrophic forgetting by suppressing the
disturbance term on the convergence of previously learned tasks. We also demonstrate theoretically
and empirically that the performance of past tasks by nonconvex continual learning with replay
memory is degraded by two separate reasons, catastrophic forgetting and overfitting to memory.
To tackle these problems, nonconvex continual learning uses two methods, scaling learning rates
adaptive to mini-batches and sampling mini-batches with small size from the replay memory. Finally,
it is expected the proposed nonconvex framework is helpful to analyze the convergence rate of other
continual learning algorithms.
9
Under review as a conference paper at ICLR 2022
References
Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection
for online continual learning. In Advances in Neural Information Processing Systems, pp. 11816-
11825, 2019.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with A-GEM. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019a.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual
learning. arXiv preprint arXiv:1902.10486, 2019b.
Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual learning in low-rank
orthogonal subspaces. Advances in Neural Information Processing Systems, 33, 2020.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided
continual learning with bayesian neural networks. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Robert M. French and Nick Chater. Using noise to compute error surfaces in connectionist networks:
A novel means of reducing catastrophic forgetting. Neural Computation, 14(7):1755-1769, 2002.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521-3526, 2017.
Jeremias Knoblauch, Hisham Husain, and Tom Diethe. Optimal continual learning has perfect
memory and is np-hard. arXiv preprint arXiv:2006.05188, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with
unlabeled data in the wild. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 312-321, 2019.
Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model
for task-free continual learning. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
scsg methods. In Advances in Neural Information Processing Systems, pp. 2348-2358, 2017.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems, pp. 6467-6476, 2017.
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Under-
standing the role of training regimes in continual learning. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020.
10
Under review as a conference paper at ICLR 2022
Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning.
In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabgs P6czos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp.
314-323,2016a.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, BarnabgS P6czos, and Alexander J. Smola. Stochastic
variance reduction for nonconvex optimization. In Proceedings of the 33nd International Confer-
ence on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 314-323,
2016b. URL http://proceedings.mlr.press/v48/reddi16.html.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference.
In International Conference on Learning Representations, 2018.
Mark B. Ring. Continual learning in reinforcement environments. PhD thesis, University of Texas at
Austin, TX, USA, 1995. URL http://d-nb.info/945690320.
Sebastian Thrun. A lifelong learning perspective for mobile robot control. In Intelligent Robots
and Systems, Selections of the International Conference on Intelligent Robots and Systems
1994, IROS 94, Munich, Germany, 12-16 September 1994, pp. 201-214, 1994. doi: 10.1016/
b978-044482250-5/50015-3. URLhttps://doi.org/10.1016/b978-044482250-5/
50015-3.
Quanming Yao and James Kwok. Efficient learning with a family of nonconvex regularizers by
redistributing nonconvexity. In International Conference on Machine Learning, pp. 2645-2654.
PMLR, 2016.
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in neural information processing systems, pp. 9793-9803,
2018.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
Proceedings of machine learning research, 70:3987, 2017.
Dongruo Zhou and Quanquan Gu. Lower bounds for smooth nonconvex finite-sum optimization.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
7574-7583, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
11
Under review as a conference paper at ICLR 2022
Appendix
A Additional Experimental results and details
A. 1 Hyperparameter grids
We report the hyper-paramters grid we used in our experiments below. Except for the proposed
algorithm, we adopted the hyper-paramters that are reported in the original papers. We used grid
search to find the optimal parameters for each model.
•	finetune - learning rate [0.003, 0.01, 0.03 (CIFAR), 0.1 (MNIST), 0.3, 1.0]
•	EWC- learning rate: [0.003, 0.01, 0.03 (CIFAR), 0.1 (MNIST), 0.3, 1.0] - regularization:
[0.1, 1, 10 (MNIST,CIFAR), 100, 1000]
•	AGEM - learning rate: [0.003, 0.01, 0.03 (CIFAR), 0.1 (MNIST), 0.3, 1.0]
•	ER-Reservoir - learning rate: [0.003, 0.01, 0.03 (CIFAR), 0.1 (MNIST), 0.3, 1.0]
•	ORTHOG-SUBSPACE - learning rate: [0.003, 0.01, 0.03, 0.1 (MNIST), 0.2, 0.4 (CIFAR),
1.0]
•	MER - learning rate: [0.003, 0.01, 0.03 (MNIST, CIFAR), 0.1, 0.3, 1.0] - within batch
meta-learning rate: [0.01, 0.03, 0.1 (MNIST, CIFAR), 0.3, 1.0] - current batch learning rate
multiplier: [1, 2, 5 (CIFAR), 10 (MNIST)]
•	iid-offline and iid-online - learning rate [0.003, 0.01, 0.03 (CIFAR), 0.1 (MNIST), 0.3, 1.0]
•	ER-reservoir - learning rate: [0.003, 0.01, 0.03, 0.1 (MNIST, CIFAR), 0.3, 1.0]
•	NCCL-Ring (default) - learning rate α: [0.003, 0.001(CIFAR), 0.01, 0.03, 0.1, 0.3, 1.0]
•	NCCL-Reservoir - learning rate α: [0.003(CIFAR), 0.001, 0.01, 0.03, 0.1, 0.3, 1.0]
A.2 Additional results
Table 3: Multi-headed split-CIFAR100, full size Resnet-18 nf = 64. Accuracy and forgetting results.
Method	memory size	1		5	
	memory	accuracy	forgetting	accuracy	forgetting
Fintune	X	42.6 (2.72)	0.27 (0.02)	42.6 (2.72)	0.27 (0.02)
EWC	X	43.2 (2.77)	0.26 (0.02)	43.2 (2.77)	0.26 (0.02)
ICRAL	✓	46.4 (1.21)	0.16 (0.01)	-	-
A-GEM	✓	51.3 (3.49)	0.18 (0.03)	60.9 (2.5)	0.11 (0.01)
MER	✓	49.7 (2.97)	0.19 (0.03)	-	-
ER-Ring	✓	59.6 (1.19)	0.14 (0.01)	67.2 (1.72)	0.06 (0.01)
ER-Reservoir	✓	51.5 (2.15)	0.14 (0.09)	62.68 (0.91)	0.06 (0.01)
ORTHOG-subspace	✓	64.3 (0.59)	0.07 (0.01)	67.3 (0.98)	0.05 (0.01)
NCCL + Ring	✓	59.06(1.02)	0.03 (0.02)	66.58 (0.12)	0.004 (0.003)
NCCL + Reservoir	✓	54.7 (0.91)	0.083 (0.01)	66.37 (0.19)	0.004 (0.001)
12
Under review as a conference paper at ICLR 2022
Table 4: permuted-MNIST (23 tasks 10000 examples per task), FC-[256,256]. Accuracy and forgetting results.					
Method	memory size	1		5	
	memory	accuracy	forgetting	accuracy	forgetting
multi-task	X	91.3	-	83	-
Fine-tune	X	50.6 (2.57)	0.29 (0.01)	47.9	0.29 (0.01)
EWC	X	68.4 (0.76)	0.18 (0.01)	63.1 (1.40)	0.18 (0.01)
MER	✓	78.6 (0.84)	0.15 (0.01)	88.34 (0.26)	0.049 (0.003)
A-GEM	✓	78.3 (0.42)	0.21 (0.01)	64.1 (0.74)	0.19 (0.01)
ER-Ring	✓	79.5 (0.31)	0.12 (0.01)	75.8 (0.24)	0.07 (0.01)
ER-Reservoir	✓	68.9 (0.89)	0.15 (0.01)	76.2 (0.38)	0.07 (0.01)
ORHOG-subspace	✓	86.6 (0.91)	0.04 (0.01)	87.04 (0.43)	0.04 (0.003)
NCCL + Ring	✓	74.38 (0.89)	0.05 (0.009)	83.76 (0.21)	0.014 (0.001)
NCCL+Reservoir	✓	76.48 (0.29)	0.1 (0.002)	86.02 (0.06)	0.013 (0.002)
Table 5: Single-headed split-MNIST, FC-[256,256]. Accuracy and forgetting results.
Method	memory size	1		5		50	
	memory	accuracy	forgetting	accuracy	forgetting	accuracy	forgetting
multi-task	X	95.2	-	-	-	-	-
Fine-tune	X	52.52 (5.24)	0.41 (0.06)	-	-	-	-
EWC	X	56.48 (6.46)	0.31 (0.05)	-	-	-	-
A-GEM	✓	34.04 (7.10)	0.23 (0.11)	33.57 (6.32)	0.18 (0.03)	33.35 (4.52)	0.12 (0.04)
ER-Reservoir	✓	34.63 (6.03)	0.79 (0.07)	63.60 (3.11)	0.42 (0.05)	86.17 (0.99)	0.13 (0.016)
NCCL + Ring	✓	34.64 (3.27)	0.55 (0.03)	61.02 (6.21)	0.207 (0.07)	81.35 (8.24)	-0.03 (0.1)
NCCL+Reservoir	✓	37.02 (0.34)	0.509 (0.009)	65.4 (0.7)	0.16 (0.006)	88.9 (0.28)	-0.125 (0.004)
Table 6: Single-headed split-MNIST, FC-[400,400] and mem. size=500(50 / cls.). Accuracy and
forgetting results.
Method	accuracy
multi-task	96.18
Fine-tune	50.9 (5.53)
EWC	55.40 (6.29)
A-GEM	26.49 (5.62)
ER-Reservoir	85.1 (1.02)
CN-DPM	93.23
Gdumb	91.9 (0.5)
NCCL + Reservoir	95.15 (0.91)
Table 7: Single-headed split-CIFAR10, full size Resnet-18 and mem. size=500(50 / cls.). Accuracy
and forgetting results.
Method	accuracy
iid-offline	93.17
iid-online	36.65
Fine-tune	12.68
EWC	53.49 (0.72)
A-GEM	54.28 (3.48)
GSS	33.56
Reservoir Sampling	37.09
CN-DPM	41.78
NCCL + Ring	54.63 (0.76)
NCCL + Reservoir	55.43 (0.32)
13
Under review as a conference paper at ICLR 2022
Table 8: Permuted-MNIST (23 tasks 10000 examples per task), FC-[256,256] and Multi-headed
split-CIFAR100, full size Resnet-18. Accuracies with different clipping rate on NCCL + Ring.
βmax	Permuted-MNIST	Split-CIFAR100
0.001	72.52(0.59)	49.43(0.65)
0.01	72.93(1.38)	56.95(1.02)
0.05	72.18(0.77)	56.35(1.42)
0.1	72.29(1.34)	58.20(0.155)
0.2	74.38(0.89)	57.60(0.36)
0.5	72.95(0.50)	59.06(1.02)
1	72.92(1.07)	57.43(1.33)
5	72.31(1.79)	57.75(0.24)
Table 9: Permuted-MNIST (23 tasks 10000 examples per task), FC-[256,256] and Multi-headed
split-CIFAR100, full size Resnet-18. Training time.
Methods	Training time [s]	
	Permuted-MNIST	Split-CIFAR100
fine-tune	91	92
EWC	95	159
A-GEM	180	760
ER-Ring	109	129
ER-Reservoir	95	113
ORTHOG-SUBSPACE	90	581
NCCL+Ring	167	248
NCCL+Reservoir	168	242
14
Under review as a conference paper at ICLR 2022
Figure 1: Geometric illustration of Non-Convex Continual Learning (NCCL). In the continual
learning setting, the model parameter starts from the moderate local optimal point for the previously
learned tasks XP. Over T iterations, we expect to reach a new optimal point XPuc which has a
good performance on both previously learned and current tasks. In the t-th iteration, the model
parameter xt encounters either Vgjt,pos(xD or VgJtneg(Xt). These two different cases indicates
whether(/几(xt), NgJt (xt)i is positive or not. To prevent χt from escaping the feasible region, i.e.,
catastrophic forgetting, we impose a theoretical condition on learning rates for f and g.
B Theoretical Analysis
In this section, we provide the proofs of the results for nonconvex continual learning. We first start
with the derivation of Equation 5 in Assumption 1.
B.1 Assumption and Additional Lemma
Derivation of Equation 5. Recall that
Ifi(X) — fi5-Wfi(y),χ — vi∖ ≤ Lllχ — yk2.	(17)
Note that fi is differentiable and nonconvex. We define a function g(t) = fi(y + t(x — y)) for
t ∈ [0,1] and an objective function fi. By the fundamental theorem of calculus,
Z g0(t)dt = f(x) — f(y).
Jo
(18)
By the property, we have
Ifi(X) — fi(y) — N fi(y),x — yi∖
=I/ hVfi(y +t(x — y)),x — yidt —〈Vfi(y),x — yi
=J / (Vfi(y +t(x — y))—Vfi(y),x — yidt .
Using the Cauchy-schwartz inequality,
I / (Vfi(y +t(x — y)) — 'Vfi(y),x — yidt
≤ I / IIVfi(y +t(x — y)) — Vfi(y)k ∙ Ilx — y∣∣dt .
Since fi satisfies Equation 4, then we have
Ifi(X) — fi(y) —〈Vfi(y),x — yi∖
1
Llly +t(x — y) — y∣∣∙ Ilx — y∣∣dt
L||x -y∣∣2
L l∣x - y12.
「tdt
Jo
15
Under review as a conference paper at ICLR 2022
Lemma B.1. LetP = [pi, •…PD], q = [qι, ∙∙∙ , q0] be two statistically independent random vectors
with dimension D. Then the expectation of the inner product of two random vectors E[hp, qi] is
PdD=1 E[Pd]E[qd].
Proof. By the property of expectation,
D
E[hP, qi] = E[X Pdqd]
d=1
D
=	E[Pdqd]
d=1
D
= X E[Pd]E[qd].
d=1
B.2 Proof of Main Results
We now show the main results of our work.
Proof of Lemma 1. To clarify the issue of EMt [EIt [et|Mt]] = 0, let us explain the details of
constructing replay-memory as follows. We have considered episodic memory and reservoir sampling
in the paper. We will first show the case of episodic memory by describing the sampling method for
replay memory. We can also derive the case of reservoir sampling by simply applying the result of
episodic memory.
Episodic memory (ring buffer). We divide the entire dataset of continual learning into the previous
task P and the current task C on the time step t = 0. For the previous task P, the data stream of
P is i.i.d., and its sequence is random on every trial (episode). The trial (episode) implies that a
continual learning agent learns from an online data stream with two consecutive data sequences of P
and C. Episodic memory takes the last data points of the given memory size m by the First In First
Out (FIFO) rule, and holds the entire data points until learning on C is finished. Then, we note that
Mt = M0 for all t ≥ 0 and M0 is uniformly sampled from the i.i.d. sequence of P . By the law of
total expectation, We derive Em°⊂P [Eit [Vfit (χt)∣Mo]] for any xt, ∀t ≥ 0 as follows.
EMo⊂P [EIt [ VfIt (Xt)IMθ]] = EMo⊂P [VfMo (Xt)].
We know that M0 was uniformly sampled from P on each trial before training on the current task C .
Then, we take expectation with respect to every trial that implies the expected value over the memory
distribution M0 . We get
EM0⊂P [VfM0(Xt) =Vf(Xt)
for any Xt, ∀t. We can consider VfMt (Xt) as a sample mean of P on every trial for any
Xt , ∀t ≥ 0. Although Xt is constructed iteratively, the expected value of the sample mean for
any Xt, EM0⊂P [VfM0 (Xt)] is also derived as Vf(Xt).
Reservoir sampling. To clarify the notation for reservoir sampling first, we denote the expectation
with respect to the history of replay memory M[o：t] = (Mo,…,Mt) as EM^.t〕. This is the
revised version of EMt . Reservoir sampling is the more tricky case than episodic memory, but
EM[0:t] [EIt [et|Mt]] = 0 still holds. Suppose that M0 is full of the data points from P as the episodic
memory is sampled and the mini-batch size from C is 1 for simplicity. The reservoir sampling
algorithm drops a data point in Mt-1 and replaces the dropped data point with a data point in the
current mini-batch from C with probability P = m/n, where m is the memory size and n is the
number of visited data points so far. The exact pseudo-code for reservoir sampling is described in
[1]. The replacement procedure uniformly chooses the data point which will be dropped. We can
16
Under review as a conference paper at ICLR 2022
also consider the replacement procedure as follows. The memory Mt for P is reduced in size 1 from
Mt-ι, and the replaced data point de from C contributes in terms of Vgdc (Xt) if de is sampled
from the replay memory. Let Mt-ι = [di,…，d∣Mt-ι∣] where | ∙ | denotes the cardinality of the
memory. The sample mean of Mt-1 is given as
VfMt-I (XtT) = |M1 ι∣ X Vfdi(XtT).
(19)
By the rule of reservoir sampling, we assume that the replacement procedure reduces the memory
from Mt-1 to Mt with size |Mt-1| - 1 and the set of remained upcoming data points Ct ∈ C from
the current data stream for online continual learning is reformulated into Ct-1 ∪ [de]. Then, de
can be resampled from Ct-1 ∪ [de] to be composed of the minibatch of reservoir sampling with
the dfferent probability. However, we ignore the probability issue now to focus on the effect of
replay-memory on Vf. Now, we sample Mt from Mt-1, then we get the random vector VfMt (Xt)
as
1	|Mt-1|
VfMt(Xt) =两 X WijVfdj (χt),	(20)
where the index i is uniformly sampled from i 〜[1, ∙∙∙ , ∣Mt-ι∣], and Wij is the indicator function
that Wij is 0 if i = j else 1.
The above description implies the dropping rule, and Mt can be considered as an uniformly sampled
set with size |Mt| from Mt-1. There could also be Mt = Mt-1 with probability 1 - p = 1 - m/n.
Then the expectation of VfMt (Xt) given Mt-1 is derived as
(1	∣Mt-ι∣ ι Mt-ι∣	∖
EMt[VfMt (Xt)IMt-1] = P [ M ]∣ X jMJj X Wij Vfdj (Xt)J + (I-P)(VfMt-ι (Xt))
= VfMt-1 (Xt).
When we consider the mini-batch sampling, we can formally reformulate the above equation as
EMt 〜p(Mt∣Mt-ι) EIt⊂Mt VfIt(Xt)IMt IMt-1 =VfMt-1(Xt).	(21)
Now, we apply the above equation recursively. Then,
EMι-p(Mι∣Mo)[…EMt 〜P(Mt ∣Mt-ι) [EIt⊂Mt [VfIt (Xt)IMt] |Mt-l]…1跖]=VfMO (Xt).
(22)
Similar to episodic memory, M0 is uniformly sampled from P . Therefore, we conclude that
Emo,∙∙∙ ,Mt [VfMt (Xt)] = Vf(Xt)	(23)
by taking expectation over the history M®t] = (M1,M2, ∙∙∙ , Mt).
Note that taking expectation iteratively with respect to the history M[t] is needed to compute the
expected value of gradients for Mt. However, the result Em0,…,Mt [E^ [et∣Mt]] = 0 still holds in
terms of expectation.
Proof of Lemma 2. We analyze the convergence of nonconvex continual learning with replay mem-
ory here. Recall that the gradient update is the following
Xt+1 = Xt - αHt VfIt (Xt) - βHt VgJt (Xt)
17
Under review as a conference paper at ICLR 2022
for all t ∈ {1,2,…,T}. Let et = Vfit (Xt) - Vf (xt). Since we assume that f, g is L-Smooth, We
have the following inequality by applying Equation 5:
f (xt+1) ≤ f (xt) + hVf (Xt), xt+1 - Xti + L kxt+1 - xtk2
=f(xt) - hVf(xt),αHtVfit (xt) + βHt VgJt (xt)i + LkaHt VfIt (xt) + Bh VgJt (xt)k2
=f(Xt) -αHthVf(Xt),Vfit(Xt)i -βHthVf(Xt),VgJt(Xt)i
+ 2 αHt kVfit(Xt)k2 + 2 βH t kVgJt(Xt)k2 + LaHt Bh hV fit(Xt), VgJt (Xty)
= f(Xt) - αHthVf(Xt),Vf(Xt)i - αHt hVf (Xt), eti - βHt hVfit (Xt), VgJt(Xt)i + βHthVgJt(Xt),eti
+ LH kVf(Xt)k2 + LaHt hVf (Xt),eti + LoHt 口”俨 + 粤 kVgJt 3)仔 + LaHt βH Wfit 3), VgJt (Xt))
=f(Xt)- QHt- 2aHt) kVf(Xt)k2 + 2βHtkVgJt(Xt)k2-βHt(1- aHL)hVfit3), VgJt(/t)i
+ (LaHt- aHt) hVf(Xt),eti + βHt NgJt (Xt),eti + 2 aHtket k2.	(24)
To show the proposed theoretical convergence analysis of nonconvex continual learning, we define
the catastrophic forgetting term Γt and the overfitting term Bt as follows:
Bt = (La2Ht - aHt )hVf (Xt), eti + BHt hVgJt (Xt), eti,
B2 L
Γt =号kVgJt(Xt)k2 - βHt(1 - aHtL)hVfit(Xt), VgJt(Xt)).
Then, we can rewrite Equation 24 as
f(Xt+1) ≤ f(Xt) - (aHt - 2aHt) kVf(Xt)k2 +Γt + Bt + 2。%|同匕
(25)
We first note that Bt is dependent of the error term et with the batch It . In the continual learning
step, an training agent cannot access Vf(Xt), then we cannot get the exact value of et. Furthermore,
Γt is dependent of the gradients Vfit (Xt), Vgit (Xt) and the learning rates aHt, BHt.
Taking expectations with respect to It on both sides given Jt, we have
Eit f(Xt+1) ≤ Eit
≤ Eit
f(XD- (aHt- 2aHt) kVf(Xt)k2 + rt + Bt + 2aHtketk2Jt
f(xD - (aHt- 2aHt) kVf(Xt)k2 + 2aHtketk2 + EIthrt + BtlJti.
Now, taking expectations over the whole stochasticity we obtain
E [f(xt+1)] ≤ E f (xt) - "t- 2 aHt) kVf(xt)k2 +Γt + Bt + L 0符同|2
Rearranging the terms and assume that I-LaH /2 > 0, We have
"t - LaHt) EkVf(xt)k2 ≤ E f(xt) - f(xt+1)+Γt + Bt + 2aHtketk2
and
EkVf(Xt)k2≤E
1
aHt (I - 2aHt)
≤E
1
aHt (I - 2aHt)
(f(Xt) - f(Xt+1) + Γt + Bt) + *aHLL、ketk2
2(1 - 2aHt )
(f(Xt) - f(Xt+1) + Γt + Bt) + *aHLL、Of
2(1 - 2aHt )
18
Under review as a conference paper at ICLR 2022
ProofofTheorem 1. Suppose that the learning rate α% is a constant α = c∕√T, for c > 0,
1 - l2ɑ = 1 > 0. Then, by summing Equation 10 from t = 0 to T - 1, we have
1 T-1
mtin EkVf(Xt)k2 ≤ T EEkVf(xt)k2
T t=0
≤ r-⅛α (OT
1
f (x0) - f (XT)+ X (E[Bt + Γt])) + L
l-2
-
1
C
T-1	Lc
δ/ + X(E [Bt + rt])J + 2√τσ2
δ + X E [Bt + rt]) + fσ2).
(26)
We note that a batch It is sampled from a memory Mt ⊂ M which is a random vector whose element
is a datapoint d ∈ P ∪ C. Then, taking expectation over It ⊂ Mt ⊂ P ∪ C implies that E[Bt] = 0.
Therefore, we get the minimum of expected square of the norm of gradients as follows:
minEkVf(Xt)k2 ≤
δ + X Em) + "f).
Proof of Lemma 3. By the assumption, it is equivalent to update on M ∪ C . Then, the non-convex
finite sum optimization is given as
min g(X) = --1TTTT X gi(x),	(27)
x∈Rd	n + |M|
x∈	g	i∈M ∪C
where gi is the same function as fi, and |M| is the number of elements in M. This problem can be
solved by a simple SGD algorithm (Reddi et al., 2016b). Thus, we have
min EkVg(Xt)k2 ≤
(28)
Lemma B.2. Let an upper bound β > βHt > 0. For the worst case, the expectation of summing the
catastrophic forgetting term over iterations T is
T-1
X Γt = O(T).
t=0
Proof. First, we derive the rough upper bound of E[Γt]:
β 2 L
E [Γt] = E [?kVgjt(xt)k2 - βHt(1 - OHtL)hVfit(xt), VgJt(Xt)〉]	(29)
≤ E ] “2 kVgJt (Xt)k2 + βH (I- αHtL)kVfIt (Xt)kkVgJt (Xt)k	(3O)
=O (e]β2LkVgjt(Xt)k2 )	(31)
where kVgJt(Xt)k ≥ kVfIt(Xt)k.
19
Under review as a conference paper at ICLR 2022
In addition, the supremum of the variance of the mini-batch gradient VgJt (Xt) is derived as
SUpEkVgJt(Xt)-Vg(xt)k2 =SUp JgUg ∙ - XkVgj(Xt)-Vg(xt)k2
x	t	x (ng - 1)bg ng j=1
=ng - bg	2
=(ng - l)bg g ,
(32)
(33)
where ng and bg denotes the size of C and minibatch Jt , respectively. The detailed derivation is
shown in [1]. By the triangular inequality, we get
EkgJt(Xt)k2≤EkgJt(Xt)-g(Xt)k2+Ekg(Xt)k2	(34)
≤ Ekg(Xt)k2 + Jg Ibg σg	(35)
(ng - 1)bg g
By plugging the above equation into E[Γt], we conclude that
E[Γt]	OE	β2L kVgjt (Xt)k2	(36)
	OE	|¥kVg(xtT + β2Ln-w σg).	(37)	
The sum of catastrophic forgetting term Γt is corrected as E[Γt]. We use the technique for
summing up in the proof of Theorem 1, then the cumulative sum of catastrophic forgetting term is
derived as
T-1 X E[Γt] t=0	T-1	2L =X β^O t=0	2	(E[kVg(Xt)k2] + ⅛-¾ σg)		(38)
	2LT -1 ≤ β2L X O 2	t=0	(1 [g(Xt) - g(Xt+1)] + L2βσg +	(ng - bg) σ2) Ing- 1)bg 3)	(39)
	<β2LO ( 1δ +TLβσ2 + T(ng - bg)σ2) ≤ 丁 Ou∆g + F σg + (ng - i)bg %)			(40)
	= O β∆g +	M (Lβ3 Jng- bg)β2) τ) σg (F + (ng - 1)bg 产 J		(41)
Rearranging the above equation, we get
TX1 Em = O (σg (Lβ3 + ¾÷⅜Γ) T+βd).	(42)
t=0	2	(ng - 1)bg
Therefore, we can write PtT=-01 E[Γt] = O(T). We note that the rough upper bound of P Γt
increases monotonically with training step as in the previous result in the paper.
Now we provide the rigorous derivation of the convergence rate by the result of Corollary 1 as
follows:
O (σg (LF+M-*) √T+√g )=O(√T).	(43)
This result is obtained by dividing P E[Γt] by √T as in the proof of Thm. 1.
On the other hand, Γt can be negative when hVfIt (Xt), VgJt (Xt)i > 0. It implies that the cumulative
sum of Γt does not increase monotonically. Therefore, for some large number N , we can denote the
cumulative sum of Γt over the finite steps T as follows:
T-1
X Γt ≤ N = O(1).	(44)
t=0
20
Under review as a conference paper at ICLR 2022
Proof of Corollary 1. To formulate the IFO calls, Recall that T ()
T(E) = min {T : min EkVf(xt)∣∣2 ≤ e}.
Additionally, a single IFO call is invested in calculating each step. As seen in Theorem 1, NCCL has
a convergence rate of
O (ρ√Tzt).	,
We note that the convergence rate for the worst case is
O (√T),	(
where the given model diverges on the convergence of f (x). Then, IFO calls are denoted as ∞.
(45)
(46)
For the case of Equation 44, We obtain the convergence rate O(1∕√T). Thus We get O(1∕e2) in this
case.
C Derivation of Equations in Section 3.4
Derivation for A-GEM Let the surrogate RgJt (χt) as
VJt(Xt) = VgJt(Xt)-〈 k"flt(χt)k CgJ(Xt)) kvflt(xt)k,	(47)
where α% = α(1 一 hft!x 可 JtF )i) and β% = α for Equation 7.
Ilv J It (x ) k
Then, We have
β2 L
E[Γt] = E 牛kVgjt(xt)k2 -βHthVfit(xt), VJt(xt)i
=E [ βHL (∣∣Vgjt (xt)k2 - 2 hVfIt (X；)，VgJt !Ri2 + "4号VgJ Lx32) Th EfIt (xt)，VgJt (Xt))
2	t	kVfIt(Xt)k2	kVfIt(Xt)k2	t t	t
=E [βHL (kVgJt (Xt)k2 - hVfIt V)，VgJtl2Xt)i2) -βHt (hVfIt(Xt), VgJt (Xt)i - hVfIt (Xt), VgJt (Xt)i)
2	kVfIt (X )k
=E [ βHL ("(Xt)k2 - hVfIt VfItVgJtI2Xt)i2)].	(48)
Note that this result is smaller than the original E[Γt].
Derivation of optimal ΓJ= and βHt For a fixed learning rate a, we have
0= ∂≡= E [匹]
∂βHt	[∂βκj
=E [βHtLkVgJt (Xt)k-(1- αL)hVfIt (xj), VgJt(xj))].
Thus, we obtain
β* = (1- α&L)hVfIt (Xt), VgJt (Xt)i
βHt =	LkVgJt (Xt)k2	，
γ* =	(1 - αHtL) hVfIt (x'), VgJt (Xt)
t =	2L∣∣VgJt (Xt)k2	.
D Overfitting to replay Memory
In the main text, we discussed a theoretical convergence analysis of continual learning for a smooth
nonconvex finite-sum optimization problems. The practical continual learning tasks have the restric-
tion on full access to the entire data points of previously learned tasks. Unlike taking expectation
21
Under review as a conference paper at ICLR 2022
over It 〜M and M 〜P ∪ C, we have to compute on the given memory in the practical scenario.
Then, we note that E[Bt|M] 6= 0.
Now we rewirte Equation 26 for the worst case as follows.
TSUPk▽/(X)k2 ≤ ~7] T " (δ∕ + X (Bt + Ct) + 5α2σ2)	(49)
α (1 ——αL/ 2) ∖	2 l
SUPk▽/(X)k2 ≤ √T (c (δ∕ + X (Bt+ Ct)) + ɪσ2) .	(5O)
We note that P Bt is a random variable, which is UnPrediCtible, and choosing VfM (t) = Vf (Xt)
over entire period is impossible. Then, the cumulative sum of Bt is increasing over T. Therefore, we
conclude that for the overfitting to memory degrades the convergence rate of NCCL empirically.
22