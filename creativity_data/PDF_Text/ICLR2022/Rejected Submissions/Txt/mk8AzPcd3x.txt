Under review as a conference paper at ICLR 2022
BCDR: Betweenness Centrality-based Dis-
tance Resampling for Graph Shortest Dis-
tance Embedding
Anonymous authors
Paper under double-blind review
Ab stract
Along with unprecedented development in network analysis such as biomedical
structure prediction and social relationship analysis, Shortest Distance Queries
(SDQs) in graphs receive an increasing attention. Approximate algorithms of
SDQs with reduced complexity are of vital importance to complex graph appli-
cations. Among different approaches, embedding-based distance prediction has
made a breakthrough in both efficiency and accuracy, ascribing to the significant
performance of Graph Representation Learning (GRL). Embedding-based dis-
tance prediction usually leverages truncated random walk followed by Pointwise
Mutual Information (PMI)-based optimization to embed local structural features
into a dense vector on each node and integrates with a subsequent predictor for
global extraction of nodes’ mutual shortest distance. It has several shortcomings.
Random walk as an unstrained node sequence possesses a limited distance explo-
ration, failing to take into account remote nodes under graph’s shortest distance
metric, while the PMI-based maximum likelihood optimization of node embed-
dings reflects excessively versatile local similarity, which incurs an adverse im-
pact on the preservation of the exact shortest distance relation during the mapping
from the original graph space to the embedded vector space.
To address these shortcomings, we propose in this paper a novel graph shortest
distance embedding method called Betweenness Centrality-based Distance Re-
sampling (BCDR). First, we prove in a statistical perspective that Betweenness
Centrality(BC)-based random walk can occupy a wider distance range measured
by the intrinsic metric in the graph domain due to its awareness of the path struc-
ture. Second, we perform Distance Resampling (DR) from original walk paths be-
fore maximum likelihood optimization instead of the PMI-based optimization and
prove that this strategy preserves distance relation with respect to any calibrated
node via steering optimization objective to reconstruct a global distance matrix.
Our proposed method possesses a strong theoretical background and shows much
better performance than existing methods when evaluated on a broad class of real-
world graph datasets with large diameters in SDQ problems.
1 Introduction
Shortest Distance Queries (SDQs) in graphs focus on determining the minimum distance between
arbitrary node pairs, defined as sum of related edge weights. Despite the fact that Shortest Path
Queries (SPQs) (Wei, 2012; Sommer, 2014; Wang et al., 2021) are already renowned in graph struc-
ture exploration, SDQs play an essential role in increasing applications, such as social relationship
analysis (Carlton, 2020; Melkonian et al., 2021; Chen et al., 2021; Zaki et al., 2021; Parveen &
Varma, 2021), biomedical structure prediction (YUe et al., 2019; GaloviCova et al., 2021; Sokolowski
& Wasserman, 2021), learning theory (Yang et al., 2021; Yuan et al., 2021), optimization (Melkonian
et al., 2021; Rahmad Syah et al., 2021; Jiang et al., 2021), etc. The key challenge in the SDQ prob-
lem is the prohibitive complexity in very large graphs. e.g., for an undirected dense graph with N
nodes and k queries, the time complexity of A*(Hart et al., 1968) and Dijkstra Algorithm(Thorup &
Zwick, 2004) are up to O(kN 2) and O(kN log N) for unweighted and weighted graph, respectively.
1
Under review as a conference paper at ICLR 2022
Table 1: Overall comparison of approaches to shortest distance query.
Accuracy loss of approximate methods are evaluated on facebook(A.7.2) by mRE(2). N -number of
nodes, T -walk length, K -window size, l-number of landmarks, d-dimension of embedding space,
α, β, C-Constant independent of N.
ObjeCtive Method	Model	Off-Line Time	Space	Time	Accuracy Loss
ExaCt	Compress	-	O(N3)	O(N2)	O(kN)	0 ExaCt	Index	-	O(N2logN)	O(N2)	O(kNlogN)	0 Exact	Cache	-	-	O(N)	O(N)〜O(N2)0 Approximate Oracle	-	O(αN 1+1)	O(αN 1+1) O(kα)	2(α - 1) Approximate Landmark	-	O(lN log N + lN)	O(lN)	O(kl)	0.523 Approximate Embedding Orion	O(αN log N + cN)	O (dN)	O(kd)	0.688 Rigel	O(αNlogN+cN)	O(dN)	O(kd)	0.369 DeepWalk	O(TKNlogN+ (T	+c)N)	O(dN)	O(kd)	0.329 Node2Vec	O(TKNlogN+ (T	+c)N)	O(dN)	O(kd)	0.299 DADL	O(TKNlogN+ (T	+c)N)	O(dN)	O(kd)	0.228 	BCDR(ours.) θ(αTKN log N + (βT + C)N) θ(dN)	θ(kd)	0.177						
Figure 1: DistanCe Confusion of a Conventional model in embedding. (a) random walks rooted at
va have muCh diffiCulty in exploring beyond Current Community to vc . (b): Weight deCay on walks
Causes instability of shortest distanCe sinCe walks rooted at va have large probability to steer Clear
of vb for starters and baCk to vb as the end, whiCh results in a extremely weak Correlation between
va and vb despite the faCt that they have an immediate edge. (c): A suffiCient number of 2-hop links
between vc and va induCe a shorter distanCe in embedding spaCe than that of vb and va . (d): vb
and vc sharing substantial ConneCtion deserve to be mapped Closed to eaCh other even if they have a
large shortest distanCe gap, while the divergenCe of distanCe between vb , vc and va is also plagued
with extraCtion.
(d)
(C)
To address this issue, a surging number of approximate algorithms in a wide range of fields have been
proposed in the past a few years. They Can be Categorized into oraCle-based (Thorup & ZwiCk, 2004;
Baswana & Kavitha, 2006), landmark-based (Sarma et al., 2010; GubiChev et al., 2010), embedding-
based distanCe prediCtion methods (Xiaohan et al., 2010; 2011; Rizi et al., 2018). Among these
Categories, embedding-based methods are of high aCCuraCy with signifiCantly reduCed Complexity
(see Table1), owing muCh to rapid advanCes of representation learning in graphs (Perozzi et al.,
2014; Grover & LeskoveC, 2016). Embedding-based methods taCkle the SDQ problem with two
stages. First, they embed loCal struCtural features into a dense veCtor on eaCh node, whiCh preserves
the essential information for indiCating where the node is. Then, for arbitrary node pairs in the
training set, subsequent prediCtor extraCts mutual shortest distanCe globally via their embedding
tuples and minimizes the mean of square loss between a prediCted value and the real one. As
prediCtors serve as a non-linear metriC in the embedding spaCe, the performanCe of embedding-
based models depends highly on the first stage. These methods usually embed graph struCtures by
leveraging trunCated random walk to serialize node’s neighborhood in a statistiCal perspeCtive and
maximize Co-oCCurrenCe likelihood of nodes in one walk path to refleCt their Correlation, whiCh is
also proved to impliCitly faCtorize a Pointwise Mutual Information (PMI) matrix (Levy & Goldberg,
2014; Shaosheng et al., 2015).
Although existing embedding-based methods have aChieved a great suCCess, they have several short-
Comings. On one side, random walk is an unstrained node sequenCe from the root, possessing a
limited distanCe exploration. This is beCause eaCh transition on nodes is not implied for a speCifiC
direCtion to move towards or beyond the root, espeCially after several walk steps, whiCh is baCk-
breaking to aCCommodate Correlation with remote nodes under graph’s shortest distanCe metriC (see
Figure 1a). On the other side, the PMI-based optimization refleCts exCessively versatile loCal sim-
2
Under review as a conference paper at ICLR 2022
ilarity, which is not guaranteed for shortest distance-preserved mapping from the original graph
space to the embedded vector space. As a matter of fact, it exerts a too-general metric over nodes’
correlation, wherein the more links or paths exist between two nodes, the stronger correlation they
share. That means we have many ways to claim a shorter distance for two nodes (e.g., by adding
mutual edges, deleting links to other nodes) even if some of the operations do not change their actual
shortest path distance (see Figures 1c and 1d).
In this paper, we address the above shortcomings by proposing a novel graph shortest distance em-
bedding method called Betweenness Centrality-based Distance Resampling (BCDR). Here, random
walk paths are simulated by considering nodes’ betweenness centrality on each transition for cov-
ering a wider distance range of nodes. Then, a sampling process of nodes based on their mutual
shortest distances is performed before optimization of co-occurrence likelihood to preserve pair-
wise distance relation during the mapping from graph to the embedding space.
We summarize our major contributions as follows.
•	We propose a Betweenness Centrality (BC)-based random walk for accommodating
correlation with nodes of wider distance range under the intrinsic graph metric (see
Section 3.1). To the best of our knowledge, there is no existing method that combines
betweenness centrality and random walk for the graph representation. We prove in a statis-
tical perspective that the transition driven by nodes’ BC value tends to explore high order
proximity of the root due to the awareness of local path structure.
•	We propose a Distance Resampling (DR) strategy to preserve nodes‘ mutual shortest dis-
tance during the mapping from graph to the embedding space (see Section 3.2). We prove
that by sampling node sequences from original walk paths before maximum likelihood op-
timization instead of PMI-based optimization, the objective can be steered to reconstruct a
global distance matrix, and for any calibrated node, it strictly preserves the shortest distance
relation to other nodes on the graph.
•	We evaluate BCDR and compare it with existing embedding-based methods with a broad
class of real-world graph datasets with divergent diameters. BCDR shows a much bet-
ter empirical performance in solving the SDQ problem than existing embedding-based
methods (see Section 4).
2	Notation & Problem Definition
2.1	Notation
G = (V, E) denotes an undirected graph, with V = {v1, v2, ..., v|V |} being the set of nodes and E
being the set of edges. An edge eij = (vi, vj) represents an undirected edge between nodes vi and
vj. A node vi’s neighborhood Ni is a set of nodes with an edge with vi, i.e., Ni = {vj |(vi, vj) ∈
E}. We use Z∣v∣×d to represent an embedding matrix, where d is the embedding size. For any
matrix B, the symbol Bi represents the i-th row of B, and Bij represents the element at i-th row
and j -th column. A truncated random walk Wi rooted at node vi of length l is a random vector
of hWl, W2,…，Wii, where Wk is a node chosen from the neighborhood of node WkT for
k = 1, ..., l, with Wi0 := vi. Pi is a finite set of walk paths sampled from Wi, and Wi is the multiset
of nodes on the walk paths in Pi .
2.2	Problem Definition
An arbitrary path p of length l ∈ N on graph G is an ordered sequence of nodes (v1, v2, ...vl+1),
where each node except the last one has an edge with the subsequent node, i.e., (vi, vi+1) ∈ E for
1 ≤ i ≤ l. The shortest path PijiS one of the paths With the minimum length l between two nodes
Vi and Vj, with the shortest distance Dij defined as the length of Pij. The global distance matrix D
comprises {Dij}.
An embedding-based model learns two mappings F and G for structure embedding and mutual
distance extraction, respectively, as follows.
F : G → Rd, G : hRd,Rdi →R	(1)
3
Under review as a conference paper at ICLR 2022
A shortest distance query Q can be simply defined as a set of node pairs, i.e., Q =
{(v11, v12), (v21, v22), ..., (v|Q|,1, v|Q|,2)}. For each node pair (vi, vj), the model intends to find
out approximation DD j = G(F(vi), F(Vj)) nearest to real Dij. The commonly used metrics of ap-
proximation quality are mean of Relative Error (mRE) and mean of Absolute Error (mAE). mRE is
defined as the relative loss of the prediction value with respect to the real value, while mAE measures
the absolute gap between the prediction value and the real value, i.e.,
mRE :
ɪ X
1Q1 (Vij∈Q
D ij - DijI
Dij
1
mAE := ∣Q∣	E	IDij- Dij |
(2)
3	Method
Random walk as a serialization strategy of similarity measurement has been widely used in
many graph representation and learning methods, aiming to model long-range dependency of
nodes (Grover & Leskovec, 2016; Zhuang & Ma, 2018). The optimization of maximizing co-
occurrence likelihood of nodes on walk paths also yields an impressive performance in network-
structure extraction via approximating PMI matrix (Levy & Goldberg, 2014). But in terms of the
shortest distance representation of a graph, we contend that this intuitive approach has several lim-
itations on the performance. Consider a walk path P = (va,Vaι ,Va2,…,Val) ∈ Pa sampled from
a general random walk Wa from root node va . As an unstrained sequence of nodes, the distance
measured along the walk path p is not consistent with that on the graph (see Figure 1b), i.e., for
Vai , Vaj ∈ p,
i≤j<Daai ≤ Daaj	(3)
where i and j are indexes of node Vai and Vaj on p and 1 ≤ i, j ≤ l.
Using the aforementioned unstrained walk paths for maximizing co-occurrence likelihood incurs
two problems.
1.	Problem 1: Limited exploration range of walks. Each transition on walks only considers
a local structure of the current node, causing agnostic tendency to move towards or beyond
the root node under the graph’s shortest-distance metric after a few steps (see Figure 1a).
2.	Problem 2: Intractability of shortest distance on paths. Distance measured on walk
paths may not actually reflect the graph’s shortest distance because of the unbalanced num-
ber of links between different nodes (see Figure 1c and 1d).
In this section, we describe in detail our proposed method, which is a decent way of encoding
shortest distances using BC-based random walk plus a distance resampling strategy, and present a
theoretical analysis for its interpretability and efficiency. More specifically, we first present BC-
based random walks and distance resampling, and then an algorithm to integrate them. We provide
an intuitive complexity explanation. Additionally, we discuss the connection to Multidimensional
Scaling (MDS) and graph structure decomposition in Appendix A.1 and A.2, respectively.
3.1 BC-based Random Walk
Definition 1. (Betweenness Centrality) Define G = (V, E) as undirected graph. Vi, Vs , Vt are
arbitrary nodes in the node set V. σst(Vi) represents the number of shortest paths between Vs and
Vt that pass Vi, and σst is the total number of shortest paths between Vs and Vt. Then we say that
BC of Vi is
BC(Vi)
Σ
s6=i6=t
σst(vi)
σst
(4)
To address Problem 1, we propose a BC-based random walk. As defined in Definition 1, BC(Vi)
determines the probability of Vi located on shortest paths of arbitrary node pairs. Thus, we consider
a node with large BC value vitally significant to drive the walk to move away from root node, since
4
Under review as a conference paper at ICLR 2022
it reveals a easy way of traveling to some nodes with minimal cost. And to leverage this property, in
BC-based random walk Wa = hWa1, Wa2, ...Waj, ...i on node va, we prefer choosing nodes with the
largest BC values among their neighborhoods when simulating walk paths, i.e.,
BC(Wj)
P(Wa WaT = Vj-I) = P——⅛7-π, Wa ∈ NT	(5)
vk∈Nj-1 BC(vk)
where Ni is the neighborhood of vi .
The following theorem, proved in Appendix A.3, indicates that a BC-based random walk tends to
transit from Na(h) to Na(h+1), leading to a deeper exploration measured by the intrinsic graph’s
shortest distance. It also reveals that our method performs better when the number of final nodes
fh (va) is larger or there are more links between final nodes fh(va ) to connective nodes eh (va) at
any h-order neighborhood of va.
Theorem 1. Define Na(h) = {vj |Daj = h} as a set of nodes that are h-hops away from va and
Nh(va) = |Na(h) | as the number of nodes in the set. Let the number of the nodes that have con-
nection with nodes in Na(h+1) (called connective nodes) be eh(va) and the number of other nodes
(called final nodes) be fh(va). The BC we use is an approximate value by considering only the
shortest path of nodes within a range of k-hops locally. Let pR(Nh(va) → Nh+1(va)) repre-
sent the probability to transit from nodes of Nh(va) to Nh+1 (va) by a general random walk, and
pB(Nh(va) → Nh+1 (va)) represent that by a BC-based random walk. Let PR(fh(va) → eh(va))
be a transition from fh(va) nodes to eh(va). Then, for any node va in graph G and any h > 1,
PB(Nh(Va)→ NH =1 + B(k)+ C	(6)
PR(Nh(Va) → Nh+l(Va))
lim	B(k) + C = A2 ~ 1 + C > 0
k→E(va)-1-h	A1
A = eh(Va)	a = ___________1_________ ⑺
1	fh(Va) ,	2	PR(fh(Va) → eh(Va))'
where C ≥ 0, and E(Va) is the eccentricity of Va, E(Va) = maxvb∈G Dab.
3.2 Distance Resampling
To address Problem 2, we propose a Distance Resampling (DR) strategy, which is inspired by
Sampling-Importance-Resampling (SIR) method (Smith & Gelfand, 1992). Let vx(V1, V2) be a
random vector of the node tuple. Define P(vx) as the joint distribution reflecting the real shortest
distance of V1, V2 on the graph. We have
max Evx 〜p[f] = E f (vι, V2)p(v1,v2)
v1 ,v2 ∈V
(8)
as a probabilistic objective for representing the shortest distance D12, where f is a normalized
learnable density function determined by nodes’ mutual distance in the embedding space. According
to the rearrangement inequality, the above equation arrives at the maximum when f(V1, V2 ) varies
consistent with P(V1, V2), which means f indicates the shortest distance metric on the graph with
respect to any V1, V2 asP does. Therefore, the critical issue is to extract accurate P(V1, V2) in a graph.
Let q(va) be the joint distribution reflecting distance relation on sampled walks. We try to leverage
q on walk paths to approximate P. Here, we adopt a simple but effective resampling by considering
both their mutual distance D12 as well as BC values.
Without loss of generality, let Vi represent the root node of walks and Vj be the j-th node on a walk
path (i.e., sampled from Wij). Our objective is thus to sample from Wi to generate a new random
. 、八 /、f,1 、八 9	、： J∖ Cl .1 7 El	1	1	IIC 、八 j Fl Tl
vector Wi = hWi1, Wi2, ..., Wil i of length l. Then, each node Vj sampled from Wij can be described
as a weighted resampling in original walk paths Wi based on Di and BC(V), i.e.,
Evj ~p(vj ∣Vi) [f |Vi]
f(Vi, Vj)P(Vj |Vi)
vi,vj ∈V
f(Vi, Vj)q(Vj |Vi)
vi,vj ∈V
P(VjIVi)
q(v八Vi)
f(Vi, Vj)q(Vj|Vi)
vi∈V vj∈Wi
aDij BC(Vj)
Pvk ∈WiαDik BC(Vk)
(9)

5
Under review as a conference paper at ICLR 2022
where α is a hyper-parameter of the weight decay coefficient on paths.
Reminiscent of previous discussions on relating maximizing co-occurrence likelihood with matrix
factorization, we demonstrate the connection between the above intuitive design and graph shortest
distance metric. As we perform distance resampling for walk paths, the objective to learn an optimal
node embedding is therefore interpreted as
arg max Evx 〜P[f(Zi,Zj)]
=X P(Vi)Evj 〜P(Wi)[P(VjIvi)(IOg σ(ZiZT ) + λEVk 〜PN (vk∣Vi)[log σ(-ZiZT 用]	(10)
vi∈V
Thereinto, f(Zi, Zj) = lOgσ(ZiZjT), pN (Vk |Vi) is the distribution of negative sampling. In our
algorithm, pN can be specified as a weighted random sampling over all occurred nodes in Wi (sim-
ulated based on Equation 5) by occurrence frequency, i.e.,
#(Vk occurs in Wi)
PN (vk |vi) = -E—	(11)
where #(•) is a counting function indicating the number of occurrence times of specified nodes, i.e.,
the cardinality of a sampled set.
Proposition 1. Define G = (V,E) as an undirected graph. Z∣v∣×d is the embedding matrix
of V corresponding to maximizing likelihood objective Evx〜p[f (Zi, Zj)] defined in Equation 10.
PN(Vb|Va) is the negative sampling distribution ofVb from Wa simulated by Equation 11. Let the
weight decay coefficient in distance resampling be α, 0 < α < 1. Then, the inner product of em-
bedding matrix corresponds to a global distance matrix, i.e. ZZt = D. For any Va and any Vb
that is n-hops away from Va, the distance between them in the embedding space varies linearly with
respect to distance n, namely,
Dab = n log α - log A	(12)
where A is a constant independent ofVb.
Proposition 1 is proved in Appendix A.4.
Proposition 1 indicates that optimization of Equation 10 conforms to reconstruct a global distance
matrix where nodes far away from each other in the graph under shortest distance metric(i.e., large
Dab) should be mapped with large distance in the embedding space(i.e., large Dab). We can also
conclude that Dab varies linearly with respect to the distance n between two nodes, while A is a
constant independent of Vb but related to Va, which means when we fix the source (or destination)
node as Va, any destination (or source) node Vb’s distance with Va could be compared with each
other (we call that distance is measurable with respect to calibrated node Va).
Consider the preservation of shortest distance relation. Some studies on metric learning (Hermans
et al., 2017; Zeng et al., 2020) have revealed that a tuple of samples (Va, Vb, Vc) being easy to learn
means ifVb shares strong correlation with Va, the distance between Vb and Va in the embedding space
should be shorter than that of Vc and Va . With this property, we have the following theorem, which
indicates that our method is consistent with distance relation under intrinsic graph metric. This will
be used in the prediction task described next.
Theorem 2. Each symbol here follows the definition in Proposition 1. Let D be a global distance
matrix defined on graph G and Dab be graph’s shortest distance between node Va and Vb. Then for
any nodes Va , Vb, Vc ∈ G,
(Dab — Dac)(∣Dab|-|Dac∣) ≥ 0	(13)
The proof is presented in Appendix A.5.
3.3 Algorithm
Our BCDR algorithm is presented in Algorithm 1. It contains three steps. First, pre-computation of
the BC value (line 21) is required for each node on the graph. Second, for each node Vi, we sample
a batch of walk paths Pi guided by the BC value according to Section 3.1 (line 1 to 20). Meanwhile,
6
Under review as a conference paper at ICLR 2022
Table 2: Statistics of graph datasets and their train & validation & test sets.
RoBC-range of BC, mBC-mean of BC, L-landmark nodes, T -train set, V -validation set, E -test set.											
	∩VT^	|E|	|E|/|V|	Diameter	RoBC	mBC	∏LΓ	|T|	|V|		|E|
Cora	2, 708	10, 787	3.9834	21	375.20	2.7174	100	100 ×	|V	|	800 × 30	1, 600 × 10
Facebook	4, 039	176, 437	21.846	8	1, 306.9	0.89931	100	100 ×	|V	|	800 × 50	1, 600 × 20
GrQc	5, 242	30, 042	5.7310	17	148.43	2.7219	100	200 ×	|V	|	800 × 70	1, 600 × 30
distance relation of each node with vi is also recorded for subsequent resampling. Third, for each
node vi , we resample nodes in Wi by their distances with vi and BC values to formulate Pi (line
25 to 34), which is optimized by maximizing pair-wise co-occurrence likelihood (line 37). This
embedding algorithm takes O(woutlo2ut|V | log |V |+(winlin+woutlout)|V |) of time complexity and
O((winlin + woutlout)|V |) of space complexity for any sparse graph (detailed analysis is presented
in Appendix A.6). The above result reveals a feasible way of reducing time complexity by just
taking larger winlin and smaller woutlout in BCDR. This practice is applied in our experiments.
Algorithm 1: BCDR Embedding Algorithm
Input: input graph G = (V, E), embedding size d, input sample size win, output sample size wout, input walk length lin, output walk length lout, distance weight decay α, use
smoothed normalization τ.
1 Def BC-Walk(G,Vi, Di, Win, lin,τ):
2
3
4
5
6
7
8
9
for walk k from 0 to win do
visit sign set Si := {Vi}, cUrrent node Vc := Vi, length ci := 0, real length cir := 0
while ci < lin do
Nck := {Vj |Vj ∈ Nc ∧ Vj ∈/ Si}
if τ is True then
I normalizepc→j- - Softmax(Yj + 1),vj- ∈ Nck
else
21 Pre-ComPUte BC of each node Vi as Yi — BC(Vi).
22 Walk path set P
23 for Vi in G do
normalize Pc→j —
Yj
10
11
12
13
14
15
16
17
18
19
20
vm∈Nck γm
, vj ∈ Nik
end
end
sample next vn from Nck by pc→j
if vn ∈ Di then
Di[vn]
cir = m
i[vn]，cir + 1}
]-1，cir}
24
25
26
27
28
29
30
31
32
33
34
distance map Di := {vi : 0}
Di — BC-Walk (G,Vi,Di,Win,lin,τ)
for vj ∈ Di.keys do
if τ is True then
I Yj — Softmax(Yj + 1)
I p(vj∣Vi):= αDi[vj] ∙ Yj
else
I p(vj∣Vi) := αDi[vj] ∙ Yj
end
end
else
I Di[Vn] = Cr + 1
end
Vc — vn, Si - vn, ci - ci + 1, cr - Cr + 1
35
36 end
end
sample walk paths Pi = {(vi,Vχ1 ,v。？，∙∙∙，v。？，・一)}oflength 1。混 by
p(vxj |vi), vxj ∈ Di.keys for wout times.
append Pi into P
37 maximize Equation 10 by P .
38 return Z
4 Experimental Evaluation
In this section, we evalUate oUr model’s performance with several graph datasets and compare it with
conventional embedding-based methods for SDQ tasks.
4.1	Datasets
We test oUr model Using three real-world graph datasets as well as some simUlated datasets. As
this paper focUses on the shortest distance prediction of connected and Undirected graphs, we make
a trivial change for some directed graphs by simply eliminating the direction of each edge. For
graphs with mUltiple connected components, we repeatedly add one edge between two Unconnected
components Until the whole graph is connected. The graphs we Use are of complex inner connections
and divergent diameters. The basic information of these datasets is oUtlined in Table 2 (colUmns 2
to 7). The detailed information is presented in Appendix A.7.
We define T, V, and E as datasets for train, validation, and test, respectively. Each of them is
composed of (Vi， Vj， Dij). As simUlations of the above sets in hUge graphs have to be of O(|V |) or
less complexity, in oUr experiments, we initially select a groUp S of nodes as soUrces, and for each
soUrce Vi ∈ S, a destination set D with their shortest distances to the soUrce is randomly collected.
Then, |S | × |Di | inpUt node pairs are simUlated. The detailed setUp for each dataset is presented in
Table 2 (colUmns 8 to 11).
4.2	Parameter Setup
We compare oUr method with conventional graph embedding models based on random walk as well
as matrix factorization (see Appendix A.8). We simUlate 40 walks on each node for random walk-
based methods, and each walk is trUncated at a length of 40. Sliding window size and negative
sampling size are fixed at 20 and 5, respectively, for each dataset. Node2Vec takes hyper-parameter
7
Under review as a conference paper at ICLR 2022
Table 3: Performance comparison of embedding-based models
	Cora		Facebook		GrQC	
	mAE	mRE	mAE	mRE	mAE	mRE
LLE	5.6265 ± 0.0490	0.8445 ± 0.0096	1.9921 ± 0.0731	0.6841 ± 0.0029	4.8849 ± 0.1034	0.7105 ± 0.0165
GF	5.6249 ± 0.0876	0.8440 ± 0.0142	1.8743 ± 0.0717	0.6383 ± 0.0284	4.8562 ± 0.0516	0.7125 ± 0.0084
LE	5.6393 ± 0.0782	0.8455 ± 0.0132	2.0312 ± 0.0625	0.6998 ± 0.0248	5.0046 ± 0.0242	0.7366 ± 0.0044
DeepWalk	1.5183 ± 0.0654	0.2425 ± 0.0101	0.9323 ± 0.0272	0.3289 ± 0.0137	2.8002 ± 0.1479	0.4169 ± 0.0227
Node2Vec	1.3072 ± 0.0236	0.2115 ± 0.0038	0.8541 ± 0.0436	0.2993 ± 0.0166	1.5156 ± 0.0691	0.2278 ± 0.0087
DADL	1.1349 ± 0.0180	0.1790 ± 0.0031	0.6150 ± 0.0325	0.2279 ± 0.0135	1.3624 ± 0.1366	0.2033 ± 0.0183
BCDR	0.9768 ± 0.0245	0.1605 ± 0.0043	0.4804 ± 0.0406	0.1770 ± 0.0156	1.0490 ± 0.0634	0.1684 ± 0.0058
p = q = 1. Regularization r of GF is set to 1.0. DADL takes the Hadamard operator for embedding
aggregation. Like previous research (Zhuang & Ma, 2018), linear regression is utilized as a predictor
for general graph embedding models.
For our model, we take the same configuration as previous random walk-based models using input
walk length lin = 40 and input sample size win = 40. In order to keep complexity of parameters
equal to or even less than that of baselines, the multiplication of output length lout and output sample
size wout is fixed at 40, with lout = 10, wout = 160 for Cora and GrQc and lout = 40, wout = 40
for Facebook. Meanwhile, the weight decay coefficient α is set to 0.1 for Cora and GrQc datasets
and 0.98 for Facebook. To avoid dramatically unbalanced values of BC, we also take smoothed
normalization (τ = T rue) in all datasets except Facebook. Finally, all of the above models use the
Adam optimizer with a learning rate r = 1e - 4 for training, and embedding dimension d is fixed
at 16 for every dataset.
4.3	Performance of Prediction Accuracy
To compare embedding-based methods, we train each model on each dataset up to 500 iterations and
save the results every 10 epochs. The best models are selected by their mAE and mRE scores on
validation set V . Each model is evaluated 5 times independently on each dataset, and their average
performance is recorded. The resulting mAE and mRE are reported in Table 3. Some comparisons
with the latest GRL methods and further discussion are presented in Appendix A.9, and run-time
test compared with general random walk is reported in Appendix A.10. We can see from the table
that our model outperforms previous models significantly for all three datasets.
4.4	Result of Exploration Distance
As stated in Section 3.1, exploration distance under intrinsic graph metric plays an indispensable
role in modeling a wider range of node correlation. Here, we compare our BC-based random walk
with existing renowned walk strategies, including DeepWalk (Perozzi et al., 2014; Zhuang & Ma,
2018), Node2Vec (Grover & Leskovec, 2016), and Random Surfing (Cao et al., 2016). We use
T G(20, 1, 3, 10) as a test graph of large diameters. We randomly sample 20 root nodes and, for each
root, simulate 10 walks with length 10 to show how many nodes in different order proximity are
visited. The ideal situation for a batch of walk paths with length l is to cover up to nodes l - hop
away from the current root. The results are shown in Figure 2. From the figure, we can see that our
BC-based walk is much more competitive regarding exploration distance. A further illustration of
diverse graph structures is presented in Appendix A.11.
4.5	Preservation of Distance Relation
We have also evaluated the property of shortest distance preservation during the mapping from the
original graph space into embedded vector space. First, we test distance variation in the embedding
space when adjusting the shortest distance between nodes on the graph, taking the same configu-
ration of simulation dataset as TG(20, 1, 3, 10). Distance in the embedding space is measured by
inner product Zi ZjT for any node vi and vj . We initially train embedding vectors using walks sim-
ulated by each model and randomly sample 20 source nodes with 100 destinations for each source.
The results are shown in Figure 3, which indicates that our model has a better tendency to maintain
a linear distance relation for the mapping.
8
Under review as a conference paper at ICLR 2022

blk Iik
(a)
■■Ik
(b)
Mk Ie
(c)
Mk laι⅝th
(d)



Figure 2:	Exploration distance of different random walk strategies tested on graph TG(20,1, 3,10).
(a): general random walk in DeepWalk. (b): Node2Vec. (c): Random Surfing. (d): BC-based
random walk(ours.).

dwtaat ・，■!■(■ kιteM< Mba an
(a)
dar^at OMbim	Ml_ . Ma
(b)
•lart-t Mlateaa K■< Ita “a・
(d)
(c)

Figure 3:	Distance relation during mappings when taking different random walk strategies. (a):
general random walk in DeepWalk. (b): Node2Vec. (c): Random Surfing. (d): BCDR(ours.).
Second, we try to find out how much the probability distance relation is violated in the embedding
space. We randomly take 10000 node triple (vα,%,vc), and record if they violate Equation 13.
The results are shown in Figure 4. The figure confirms that our model is much more satisfactory in
preserving distance relation during mappings.
(a)
ιa<b trlpla
—nab Irlp Ih
>lι1a⅞MI
• prww<l
“一，■! ιβb »，,，■
1∙ IrIplM
(d)
(b)	(c)


Figure 4:	Distance preservation in the embedding spaces of different models. (a): general random
walk in DeepWalk. (b): Node2Vec. (c): Random Surfing. (d): BCDR(ours.).
5	Conclusion
In this paper, we propose a novel graph shortest distance embedding method called Betweenness
Centrality-based Distance Resampling (BCDR). It improves the graph embedding for the shortest
distance representation with two components we propose in this paper. The first is Betweenness
Centrality-based random walk to accommodate long-distance correlation on graphs by covering a
wider range of nodes under the intrinsic graph metric. The second is a distance resampling strategy
to preserve shortest distances during the mapping from graph to the embedding space via recon-
structing a global distance matrix. The experimental evaluation indicates that BCDR possesses a
better capacity than existing graph embedding methods to extract distance structure from original
graphs. BCDR can be integrated into graph-based learning models (especially in graph neural net-
works), which should improve their performance on graph structure recognition. This will be our
future work.
9
Under review as a conference paper at ICLR 2022
References
Principal Component Analysis and Factor Analysis, pp. 150-166. Springer New York, New York,
NY, 2002. ISBN 978-0-387-22440-4. doi: 10.1007/0-387-22440-8_7. URL https://doi.
org/10.1007/0-387-22440-8_7.
A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-
scale natural graph factorization. In Proceedings of the 22nd international conference on World
Wide Web, pp. 37-48, 2013.
S. Baswana and T. Kavitha. Faster algorithms for approximate distance oracles and all-pairs small
stretch paths. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science
(FOCS’06), pp. 591-602, Oct 2006. doi: 10.1109/FOCS.2006.29.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993-1022, 2003.
Ulrik Brandes and Christian Pich. Centrality Estimation in Large Networks. International Journal
of Bifurcation and Chaos, 17(7):2303, January 2007. doi: 10.1142/S0218127407018403.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.
Jason Carlton. The shortest distance between two people is a story: Storytelling best practices in
digital and social media marketing. Journal of Digital & Social Media Marketing, 8(2):108-115,
2020.
Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos, and Charalampos
Tsourakakis. Node embeddings and exact low-rank representations of complex networks. Ad-
vances in Neural Information Processing Systems, 33, 2020.
Yu Chen, Hanchao Ku, and Mingwu Zhang. Pp-ocq: A distributed privacy-preserving optimal
closeness query scheme for social networks. Computer Standards & Interfaces, 74:103484, 2021.
Maria Ercsey-Ravasz and Zoltan Toroczkai. Centrality scaling in large networks. Physical review
letters, 105(3):038701, 2010.
Robert W Floyd. Algorithm 97: shortest path. Communications of the ACM, 5(6):345, 1962.
Lucia Galovicova, Petra Borotova, Veronika Valkova, Nenad L Vukovic, Milena Vukic, Jana
Stefanikova, Hana Duranova, PrzemysIaW Eukasz Kowalczewski, Natalia Cmikova, and
Miroslava KaCaniova. Thymus vulgaris essential oil and its biological activity. Plants, 10(9):
1959, 2021.
Rainer Gemulla, Erik Nijkamp, Peter J Haas, and Yannis Sismanis. Large-scale matrix factorization
with distributed stochastic gradient descent. In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining, pp. 69-77, 2011.
A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In the 22nd ACM
SIGKDD International Conference, 2016.
A.	Gubichev, S. Bedathur, S. Seufert, and G. Weikum. Fast and accurate estimation of shortest paths
in large graphs. In Proceedings of the 19th ACM Conference on Information and Knowledge
Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010, 2010.
Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination
of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100-107,
1968.
Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-
identification. arXiv preprint arXiv:1703.07737, 2017.
Shijie Jiang, Yang Wang, Guang Lu, and Chuanwen Li. Dlsm: Distance label based subgraph
matching on gpu. In Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM)
Joint International Conference on Web and Big Data, pp. 194-200. Springer, 2021.
10
Under review as a conference paper at ICLR 2022
Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:
//snap.stanford.edu/data, June 2014.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances
in neural information processing systems, 27:2177-2185, 2014.
Vardges Melkonian et al. Mathematical models for a social partitioning problem. American Journal
of Computational Mathematics, 11(01):1, 2021.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013b.
Ruksar Parveen and N Sandeep Varma. Friend’s recommendation on social media using different
algorithms of machine learning. Global Transitions Proceedings, 2021.
B.	Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. Pro-
ceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Min-
ing, 03 2014. doi: 10.1145/2623330.2623732.
Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as
matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the eleventh
ACM international conference on web search and data mining, pp. 459-467, 2018.
Bayu Rahmad Syah, Mahyuddin Nasution, Erna Nababan, and Syahril Efendi. Sensitivity of
shortest distance search in the ant colony algorithm with varying normalized distance formu-
las. TELKOMNIKA Indonesian Journal of Electrical Engineering, 19:1251-1259, 08 2021. doi:
10.12928/TELKOMNIKA.v19i4.18872.
Fatemeh Salehi Rizi, Joerg Schloetterer, and Michael Granitzer. Shortest path distance approxima-
tion using deep learning techniques. In 2018 IEEE/ACM International Conference on Advances
in Social Networks Analysis and Mining (ASONAM), pp. 1007-1014. IEEE, 2018.
S.	T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.
science, 290(5500):2323-2326, 2000.
A. D. Sarma, S. Gollapudi, M. Najork, and R. Panigrahy. A sketch-based distance oracle for web-
scale graphs. In Proceedings of the Third International Conference on Web Search and Web Data
Mining, WSDM 2010, New York, NY, USA, February 4-6, 2010, 2010.
C.	Shaosheng, L. Wei, and X. Qiongkai. Grarep: Learning graph representations with global struc-
tural information. In Proceedings of the 24th ACM international on conference on information
and knowledge management, pp. 891-900, 2015.
A. F. M. Smith and A. E. Gelfand. Bayesian statistics without tears: A sampling-resampling per-
spective. The American Statistician, 46(2):84-88, 1992. doi: 10.1080/00031305.1992.10475856.
URL https://doi.org/10.1080/00031305.1992.10475856.
Marcus Sokolowski and Danuta Wasserman. A candidate biological network formed by genes from
genomic and hypothesis-free scans of suicide. Preventive Medicine, 152:106604, 2021.
Christian Sommer. Shortest-path queries in static networks. ACM Computing Surveys (CSUR), 46
(4):1-31, 2014.
J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear
dimensionality reduction. science, 290(5500):2319-2323, 2000.
M. Thorup and U. Zwick. Approximate distance oracles. Journal of the Acm, 52(1), 2004.
Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Muller. Verse: Versatile graph
embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference,
pp. 539-548, 2018.
11
Under review as a conference paper at ICLR 2022
Ye Wang, Qing Wang, Henning Koehler, and Yu Lin. Query-by-sketch: Scaling shortest path graph
queries on very large networks. In Proceedings of the 2021 International Conference on Manage-
ment ofData ,pp.1946-1958, 2021.
Fang Wei. Tedi: efficient shortest path query answering on graphs. In Graph Data Management:
Techniques and Applications, pp. 214-238. IGI global, 2012.
Z. Xiaohan, A. Sala, C. Wilson, Z. Haitao, and Z. Ben Y. Orion: Shortest path estimation for large
social graphs. In Proceedings of the 3rd Wonference on Online Social Networks, WOSN’10, pp.
9, USA, 2010. USENIX Association.
Z. Xiaohan, A. Sala, Z. Haitao, and Z. Ben. Fast and scalable analysis of massive social graphs.
CoPR, 07 2011.
Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, and Dacheng Tao. Spagan: Shortest path
graph attention network. arXiv preprint arXiv:2101.03464, 2021.
Huilin Yuan, Jianlu Hu, Yufan Song, Yanke Li, and Jie Du. A new exact algorithm for the shortest
path problem: An optimized shortest distance matrix. Computers & Industrial Engineering, 158:
107407, 2021.
Xiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab, Yungui
Huang, Simon M Lin, Wen Zhang, Ping Zhang, and Huan Sun. Graph embedding on biomedical
networks: methods, applications and evaluations. Bioinformatics, 36(4):1241-1251, 10 2019.
ISSN 1367-4803. doi: 10.1093/bioinformatics/btz718. URL https://doi.org/10.1093/
bioinformatics/btz718.
Abeer A Zaki, Nesma A Saleh, and Mahmoud A Mahmoud. Performance comparison of some
centrality measures used in detecting anomalies in directed social networks. Communications in
Statistics-Simulation and Computation, pp. 1-15, 2021.
Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo. Hierarchical clustering with hard-batch
triplet loss for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 13657-13665, 2020.
Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised
classification. In Proceedings of the 2018 World Wide Web Conference, pp. 499-508, 2018.
A	Appendix
A.1 Connections to MDS
Our algorithm also shares some connection with conventional MDS methods in global distance met-
ric. Take a view of original MDS. Input data X distributes in agnostic high-dimensional euclidean
space and mutual distance between samples is extractable as a global distance matrix DX . A ap-
proximate distance matrix DY calculated by embedding matrix Y endeavor to be optimized closest
to DX by Frobenius norm, i.e.,
mYinkDX - DY k2F = kDX-YYTk2F	(14)
Here, we suppose DX and Y have been double-centered for stability, and finally, Y could be de-
duced as the group of top-d leading eigenvectors in DX . Then, as a non-linear version of MDS,
Isomap (Tenenbaum et al., 2000) generalizes DX as distribution on a manifold and utilizes short
hops to measure the mutual distance between samples. Deriving from the above idea, we regard the
global distance matrix DX as a canonical metric on graphs and exert embedding matrix Z∣ v∣×d to
reconstruct it. Different from previous work, we encounter prohibitive complexity to acquire all ele-
ments in DX and leverage an iterative process to approximately minimize the accuracy loss between
DZ and DX.
12
Under review as a conference paper at ICLR 2022
250	500	750	1∞0	1250	1500	1750 2000
iteration
(a)
JoJJ∙UIJOU qoj*
(b)
Figure 5: Converge analysis of LPCA on different datasets (a): LPCA loss. (b): Frobenius norm
error.
Table 4: Converge time and Accuracy Comparison between LPCA and BCDR(ours.).
	Cora		Facebook		GrQc	
	Time	mRE	Time	mRE	Time	mRE
LPCA (Chanpuriya et al., 2020)	82.60s	0.3445	2587s	0.8272	1163s	03952
BCDR(ours.)	113.7s	0.1576	774.0s	0.1599	264.8s	0.1603
A.2 Connections to Graph Structure Decomposition
A newly published work (Chanpuriya et al., 2020) provides an exciting perspective to embed com-
plex sparse graphs perfectly into low-rank representations, which is helpful for downstream ML
tasks. Nevertheless, we need to address the limitation of LPCA in shortest distance prediction as
follows.
First and most importantly, it should be clarified that a perfect representation of graph structure is
not equal to that of graph shortest path structure, since they have a large calculation gap. According
to Floyd-Warshall algorithm (Floyd, 1962), even if each node is aware of all related path structures,
inference of the exact shortest path structure also needs up to O(N3) complexity. Second, LPCA
has poor embedding performance on some relatively dense graphs, despite converging fast on sparse
graphs. We test this method on the three real-world datasets used in this paper, and illustrate the
converge curve in Figure 5. The converge time is reported in Table 4. The results show LPCA meets
a bottleneck on the relative-dense graph (Facebook) and consumes a long time to converge.
Compared to the above method, the motivation of this paper is to directly embed graph shortest
distance matrix with sub-linear time complexity for fast and accurate online queries of shortest
distance. To the best of our knowledge, there is no existing embedding method that could directly
and perfectly represent shortest path structures in linear time. Moreover, our method takes a more
flexible objective to represent shortest distances, since only high-level restrictions are implicitly
exerted on the embedding space by Equation 12 and 13.
A.3 Proof of Theorem 1
Proof. We simplify symbols Nh(va), eh(va), fh(va) as Nh, eh, fh for short.
min{E(va),h+k-1}
Nh = #(	[	{vj|vj ∈ Na(i)})
i=h
h
Nh = #(	[	{vj|vj ∈ Na(i)})
i=max{0,h-k+1}
(15)
13
Under review as a conference paper at ICLR 2022
According to definition in the theorem, we have Nh = eh+fh. Since only eh nodes could travel from
Nh to Nh+1, We firstly consider PR(eh → eh+1|eh → Nh+1) andPB(eh → eh+1|eh → Nh+1).
For general random walk, the choice of destination is based on uniform sampling, thus causing
PR (eh → eh+1 |eh → Nh+1)
eh+1
Nh+1
(16)
For BC-based random walk, we need calculate BC value of eh+1 and fh+1 nodes for starters. Let
BC(eh+1) and BC(fh+1) represent the BC value of nodes in eh+1 and fh+1 respectively, and the
V→	V
correspond legal shortest path counts comes from 4 sources as {Nh → Nh+2}, {Nh → Nh+1},
→
{Nh+ι → Nh+2} and {Nh+ι → Nh+ι}. And We use BC({∙ → ∙}) as the BC gain from the
specified source, then
V→
V→
BC({Nh → Nh+2}) = NhNh+2
BCHNh → Nh+ι}) = Nh(fh+ι ∙ 0 + eh+ιβ(1))
→→
BCHNh+1 → Nh+2}) = Nh+2(fh+1 ∙ 1 + £%+1β，)
BC({Nh+1 → Nh+1}) = Nh2+1βe(2)
(17)
Where βe(1) means average BC gain betWeen nodes in eh+1 and N→h+2, and βe(2) means average BC
gain betWeen nodes both in eh+1, Which are constantly related With G. Therefore, We have
BC(eh+1) = NhNh+2 + (Nh + N h+2)eh+1 βe(1) + N h+2 fh+1 + Nh2+1βe(2)	(18)
LikeWise, We calculate
BC(fh+1) =NhNh+2 ∙ 0 + Nh(fh+1βf1) + eh+1 ∙ 0) + Nh+2(fβf + eh+1 ∙ 0) + Nh+1 瞪2
=(NVh + N→h+2)βf(1) + Nh2+1βf(2)
(19)
To compare the above BC(eh+1) and BC(fh+1),
BC(fh+ι) =_____________(Nh + Nh+2)βf1)+ Nh+ιβf2)____________
Bc(eh+1)	N hN h+2 + (N h + N h+2)eh+1β(1) + N h+2fh+1 + Nh+仍产
→V→
note that for any Nj and N(x,y) = N0 - Nx - Ny Where j, x, y ∈ {0, E(va)},
Nj
Iim	— = lim	e(k) = 0	(21)
k→E (va )-1-h N(x,y)	k→E(va )-1-h
Equation 19 is reduced to
BC(fh+ι) =	2e(k)βf1) + e(k2)βf2)	= 2补1”)
BC(eh+1)	1 + 2e(k)neh+ιβe1) + e(k)fh+ι + e(k2)β(2)	f
Then, We perform Weighted random sampling based on BC and get
pB (eh → eh+1 |eh → Nh+1 )
________eh+1_______
eh+1 + 2fh+1 βfl"(k)
(23)
14
Under review as a conference paper at ICLR 2022
Now, we consider the relation between PR(Nh → eh+1 |Nh → Nh+1) and PB (Nh → eh+1 |Nh →
Nh+1).
PB (Nh → Nh+ι) _ PB (eh)pB (eh → Nh+ι)
PR(Nh → Nh+1)	PR(Ch)PR(eh → Nh+1)
_ Pb (Nh-I → eh) + Pb (Nh-I → fh)PB (fh → eh)
PR(Nh-I → eh) + PR(Nh-I → fh)PR(fh → eh)
=1+ [PB (Nh-I → eh) - PR(Nh-I → eh)][1 - PR(fh → eh)]
PR(Nh-I → eh) + PR(Nh-I → fh)PR(fh → eh)
+PR(fh → eh)	(24)
PR(Nh-I → eh) + PR(Nh-I → fh)PR(fh → eh)
fh [1 - 2fh βf(1) (k)][1 - PR (fh → eh)]
=1 + ----------------τ^τ∖------------
(1 + f )[eh + 2fh Bf ^(k)]PR (fh → eh)
+______________PR(fh → eh)______________
PR(Nh-I → eh) + PR(Nh-I → fh)PR(fh → eh)
Let C = __________TR(fh→eh)e____	B(k)
PR(Nh-1→eh)+PR(Nh-1→ f h)PR(f h→eh)'	( J
C is a non-negative value independent of k, finally we get
PB (Nh → Nh+1)
PR(Nh → Nh+1)
fh[1-2fhβf1'i e(k)][1-pR(fh→eh)]
(I + fh升%+小力”⑹恒山九.⑴
. Since
(25)
1+B(k)+C
where
lim	B(k) + C = A2 ~ 1 + C.
k→E(va)-1-h	A1
□
A.4 Proof of Proposition 1
Proof. Initially, we rewrite the negative sampling item of Equation 10 as
Evk ~PN (vk∣Vi)[log σ(-Zi ZT )])= E PN (vk[vi)[log σ(-ZiZT )]
vk ∈Wi
=Pn(Vj ∣Vi)[logσ(-ZiZT)] +	X	PN(vk∣Vi)[logσ(-ZiZT)]
vk ∈Wi∖{vj}
(26)
(27)
Then, for each pair of vi ∈ V and vj ∈ Wi, we get independent objective by combing similar items
in overall likelihood expression Evx^p[f (Zi, Zj)], and get
Evx~p[f(Zi, Zj)]= E E L(Vi,Vj)
vi∈V vj∈Wi	(28)
L(vi,vj) = P(vi, vj) log σ(-ZiZjT) + λP(vi)PN (vj |vi) logσ(-ZiZjT)
Let DD = ZZT, for each node pair (va, Vb) with mutual shortest distance DDab, consider
T
Dab = ZaZbT = arg max L(Va, Vb)	(29)
Za,Zb
Suppose Vb is n-hops away from Va, and denote BC(Vb) by γb, according to Equation 5,
P(Va,Vb) = P(Va) ∙ P(Vb ∣Va) = P(Va) ∙ αnYb	(30)
Note that PN (Vb |Va) is a negative sampling in walk set Wa(Equation 11) which is selected by BC-
based random walk locally restricted to Va according to Equation 5, we have
PN(Vb|Va) = κ(Va)γb	(31)
15
Under review as a conference paper at ICLR 2022
where κ(va) is related with the neighbor structure of va. Then, Equation 29 could be described as
n
Dab = arg max p(va)αnγb log σ(Dab) + λp(va)κ(va)γb log σ(-Dab)
D ab
Solve the above problem by just let dL(^a,Vb) be equal to zero, i.e.,
∂Dab
∂L(va, vb)
∂DD ab
n
p(va)αnγbσ(1 - Dab) - λp(va)κ(va)γbσ(1 + Dab) = 0
After some simplification, we get
Dab = n log α - log λκ(va)
Let A = -λκ(va) and there holds
DDab = n log α - log A
(32)
(33)
(34)
(35)
□
A.5 Proof of Theorem 2
Proof. Let vb and vc in graph be n and m-hops away from va respectively. In terms of node pair
(va, vb), as proved in Proposition 1, their mutual distance Dab in the embedding space varies linear
with respect to the graph shortest distance n, i.e.,
I A I	Il	1 Λ I	1	. 1 Λ	/Oj≤∖
∣Dab∣ =	|n log α	- log A|	= -n log α	+ log A	(36)
Likewise, we have for (va, vc)
|DDac| = |m log α — log A| = -m log a + log A	(37)
where 0 < α < 1 and A is independent of vb and vc. Then, consider the distance relation of va, vb
and vc, there holds
(Dab - Dbc)(∖Dab∣-∣Dbc|) = (n - m)(m - n) log α = - log α ∙ (n - m)2 ≥ 0	(38)
□
A.6 Complexity Analysis of BCDR Embedding Algorithm
We analyze the complexity of Algorithm 1 as follows.
The first step depends on the algorithm used for BC calculation, wherein some approximation meth-
ods could reduce complexity to O(|V |) or O(|V | log |V |)(Brandes & Pich, 2007; Ercsey-Ravasz
& Toroczkai, 2010). Then, in the second step, for each walk rooted at each node vi(line 23),
we use a loop(line 2 to 20) up to win times for generating nodes on the walk. Normalization of
pi→j (line 5 to 18) is calculated for lin times for each node, and summation of γ is up to O(|E|)
during the whole routine. Therefore, it takes totally O(winlin|V | + |E|) time complexity. Fi-
nally, as for the third step, line 26 to 33 re-weights explored area of every node vi which need
loops up to winlin |V | times. For the sake of resampling wout lout nodes, line 34 also requires nor-
malization by O(woutlout |V |). Line 37 use a maximum likelihood optimization which occupies
O(woutlo2ut|V | log |V | + woutlout|V | + |E|) time complexity(Mikolov et al., 2013a). As for space
complexity, distance map Di and p(vj|vi) are stored temporarily up to O(winlin), while visit sign
set Si is up to O(win). For dumped and resampled walk paths, O((winlin + woutlin)|V |) space is
required. Finally, our embedding algorithm takes O(woutlo2ut|V | log |V |+(winlin+woutlout)|V |+
|E|) time complexity and O((winlin +woutlout)|V |) space complexity. Especially, for sparse graph,
time complexity is reduced to O(woutlo2ut|V | log |V | + (winlin + woutlout)|V |).
A.7 Complementary Information of Datasets
This part provides detailed information about graph datasets we used, and presents some visualiza-
tion result in Figure 6.
16
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)
Figure 6: Visualization of graph datasets. (a): Cora. (b): Facebook. (c): GrQc. (d), (e), (f):
TG(20, 1, 3, 10).
A.7.1 CORA
Cora graph dataset describes the citation relationship of papers, which contains 2708 nodes and
10556 directed edges among them. Each node also has a predefined feature with 1433 dimensions.
A.7.2 Facebook
Facebook dataset(Leskovec & Krevl, 2014) describes the relationship among Facebook users by
their social circles(or friend lists), which is collected from a group of test users. Facebook has also
encoded each user with a reindexed user ID to protect their privacy.
A.7.3 GRQC
Arxiv GR-QC (General Relativity and Quantum Cosmology) collaboration network(Leskovec &
Krevl, 2014) is recorded from the e-print arXiv in the period from January 1993 to April 2003,
which used to represent co-author relationship based on their submission. We suppose an undirected
edge (vi, vj) if an author vi co-authored a paper with another author vj. If one paper is owned by k
authors, a complete graph of k nodes is generated correspondingly.
A.7.4 TEST GRAPH (TG)
We define TG(cls, c, r, n) as a parameterized graph simulation for SDQ tasks. cls is the number
of sub-graphs to represent communities, and n is the number of nodes in each community. c and
r determine the inner and outer connectivity of each community, respectively. TG is guaranteed to
be an undirected and connected graph in our experiment, which is also expected to be sparse and of
large diameter. Some simulation results of TG(20, 1, 3, 10) that we used in Section 4.4 and 4.5 are
presented in Figure 6 (d), (e), (f).
A.7.5 Other Graphs
We also use some other graphs with diverse structures in this paper. The visualization of these graphs
is shown in Figure 7. We describe each graph as follows.
17
Under review as a conference paper at ICLR 2022
Figure 7: Visualization of some synthetic graphs with diverse structure. (a): circle graph. (b):
triangle graph. (c): tri-circle graph. (d): tree graph. (e): spiral graph. (f): net graph.
•	Circle Graph: a graph that contains several circles of different sizes. The simulation of
circle graphs takes an iterative process where for each newly introduced circle, there are a
limited number of nodes (called exit nodes) connected to the previous node set.
•	Triangle Graph: a graph possessing several cliques which are linearly connected mutually.
•	Tri-circle Graph: a graph that combines the properties of circle graphs and triangle graphs.
Here, each circle is simulated by connecting triangle sub-graphs end to end.
•	Tree Graph: a graph that is generated from one root to several leaves recursively. There is
no cycle in tree graphs. To control the tree structure, we define a splitting probability that
is decayed exponentially with current depth.
•	Spiral Graph: a graph shaped like a spiral line. We first simulate a line graph and add edges
between nodes with exponentially increased distances by their indices on the line.
•	Net Graph: a graph containing grid-like connections between nodes. We define a small
probability to drop those edges stochastically.
A.8 Baseline Algorithm
We outline all of the baseline models used in performance comparison as follows.
A.8.1 LLE
Locally Linear Embedding(LLE)(Roweis & Saul, 2000) is an effective method of dimensional re-
duction that preserves the local linear combination property of nodes. In contrast with previous
PCA(Jol, 2002) and LDA(Blei et al., 2003) which are guaranteed to discover an optimal reduction
in euclidean space, LLE that addresses local distance relation gives a more compact representation
for non-linear manifolds.
A.8.2 LE
Analog to Laplace-Beltrami operator on manifolds, Laplacian Eigenmap(LE)(Roweis & Saul, 2000)
leverages graph Laplacian matrix to generate embedding vectors. The objective of LE is to minimize
18
Under review as a conference paper at ICLR 2022
Table 5: Performance comparison with other GRL techniques
	Cora		Facebook		GrQc	
	mAE	mRE	mAE	mRE	mAE	mRE
GraRep (Shaosheng et al., 2015)	2.4036	0.3444	2.7975	170000	4.2263	0.6083
NetMF (Qiu et al., 2018)	4.1388	0.5988	1.5179	0.5477	4.10134	0.6037
VERSE (Tsitsulin et al., 2018)	2.9453	0.4140	1.1553	0.3939	3.3925	0.4761
LPCA (Chanpuriya et al., 2020)	2.3739	0.3445	2.0306	0.8272	2.7971	0.3952
BCDR(ours.)	0.9768 ± 0.0245	0.1605 ± 0.0043	0.4804 ± 0.0406	0.1770 ± 0.0156	1.0490 ± 0.0634	0.1684 ± 0.0058
mutual distance on edges by L2 loss, which maps nodes with more first-order links closer to each
other.
A.8.3 GF
Graph Factorization(GF)(Ahmed et al., 2013) utilize matrix factorization methods(Gemulla et al.,
2011) to deal with graph structure exploration. By approximating the graph adjacency matrix on a
bundle of random nodes, GF is of high efficiency for large graphs.
A.8.4 DeepWalk
DeepWalk(Perozzi et al., 2014) is a random walk-based embedding method of sub-linear complexity
to represent graph structure. This method derives from skip-gram(Mikolov et al., 2013a) and nega-
tive sampling(Mikolov et al., 2013b) to learn each node representation by predicting its context.
A.8.5 Node2Vec
Node2Vec(Grover & Leskovec, 2016) improves DeepWalk by performing a parametrized random
walk. For each transition, Node2Vec considers second-order proximity to decide the next node be
near or far from the previous node, which allows exploration both in local and global scopes.
A.8.6 DADL
DADL(Rizi et al., 2018) leverages conventional GRL methods to embed local structure on each
node, which implies some similarities between the current node and its neighbors. Then, by con-
sidering different binary operations over node embeddings in deep learning techniques, this method
outperforms conventional ones in the shortest distance prediction task.
A.9 Further Comparison in Accuracy
We also evaluate our embedding method with the latest GRL techniques. Parameter setup takes the
same configuration as stated in Section 4.2 except for some trivial modification to match the embed-
ding dimension d = 16. All compared models take default parameters. We list out the comparison
among these methods in Table 5. Moreover, we further illustrate the relation between prediction
accuracy and path length for each method on both sparse graph (GrQc) and relatively-dense graph
(Facebook). The sampling frequency of different path lengths in each dataset is presented in Figure
8. Length-level prediction accuracy is presented in Figure 9.
The experimental results show our method outperforms others mainly benefited by decent represen-
tations of node pair’s similarity in a large range of shortest distance. On the one hand, BC-based
walk occupies a wider range of shortest paths on each node, making remote nodes available before
embedding. On the other hand, the distance resampling strategy discerns nodes by different order
proximity to preserve their mutual shortest distance.
A.10 Run Time of BCDR and General Random Walk
As is stated in Section 4.2, the time complexity of BCDR could be adjusted by sampling parameters
win , wout , lin , lout . To keep the capacity of accomodating large shortest distance, win and lin are
fixed as the previous. Define β as the compressing coefficient of path length, and use lout = 40 × β .
Wout is thus adapted for inputs, i.e., Wout = IinWin = 40. The ComParasion between BCDR
lout	β
19
Under review as a conference paper at ICLR 2022
(a)	(b)
Figure 8: Sampling frequency of different path lengths. (a): Facebook dataset. (b): GrQc dataset.
0 5 0 5 0 5
∙∙∙∙∙∙
aJ8
0.50
0.25
0.00
1.0	1.5	2.0	2.5	3.0	3.5	4.0
path length
(a)
Figure 9: Length-level prediction accuracy (a): Facebook dataset. (b): GrQc dataset.
0 5 0 5 0 5
∙0∙7∙5∙2∙0∙7
aJ8WKE
0.50
0.25
0.00
2	4	6	8	10	12	14	16
path I Cngth
(b)
and general random walk method is provided in Figure 10 and 11 for run time and corresponding
accuracy, respectively.
The experimental results reveal that our method could reduce time complexity with high accuracy
to keep pace with general random walk in sparse graphs, but for relatively-dense graphs(like Face-
book), there is some room for further optimization.
A.11 FURTHER DISCUSSION ON WALKING PATTERNS
We test the performance of BC-based walk and other walking patterns on six synthetic graphs. The
statistics of these graphs we simulated are listed in Table 6. To address the aspect of getting out of
local cliques or circles, we provide another BFS-like searching pattern where each transition tends
to choose the edges that could get out of the local clique by considering up to second-order prox-
imity (with probability Pout ≈ 0.901). Properties and visualization of these graphs are presented in
Appendix A.7.5. And traversal results are illustrated in Figure 12 to Figure 17. The fluctuation of
BC value on each graph is illustrated in Figure 18
The results are analyzed as follows.
• For circle graphs and tri-circle graphs, BC-based walk tends to choose the exit nodes of
each circle since they share a large BC gain by locating on the shortest path between inner
nodes and outer nodes.
20
Under review as a conference paper at ICLR 2022
l-3 w >o Pu∞n-Ξ∙
⑶
∞m∞∞m
l⅛- τC8S----I
li⅛- τC8S----I
(b)	(c)
Figure 10:	Comparison of run time between BCDR and general random walk. (a): Cora dataset (b):
Facebook dataset. (c): GrQc dataset.
⑶
(b)
(c)
Figure 11:	Comparison of accuracy correpsonding to run time presented in Figure 10. (a): Cora
dataset (b): Facebook dataset. (c): GrQc dataset.
•	For triangle graphs, transitions on every triangle clique tend to move forward since the num-
ber of nodes beyond the current clique is often larger than that of inner nodes, contributing
to more shortest paths.
•	For tree graphs, each transition appears to move forward from the root to the leaves, and all
walking patterns show a similar exploration.
•	For spiral graphs, although some exit nodes are located on the circle, BC gains on exit
nodes and inner nodes on circles are usually on par, which misleads the direction choice of
the next transition. Thus, BC-based walk only performs slightly better than others.
•	For net graphs, all walking patterns are limited on exploration distance, since BC values on
different nodes do not change considerably.
To conclude, the above results show that BC-based walk possesses a better all-around performance
to capture remote nodes with large shortest distances.
21
Under review as a conference paper at ICLR 2022
Table 6: Statistics of six synthetic graphs
	Circle	Triangle	Tri-circle	Tree	Spiral	Net
V	--262^^	352	234	207	550	100
|E|	281	853	559	206	585	178
∖E∖∕∖V |	1.0725	2.4233	2.3889	0.9952	1.0636	1.78
avg. Degree	2.1412	4.8438	4.7735	1.9855	2.1255	3.55
diameter	32	100	26	18	101	18
⑶	(b)	(c)	(d)	(e)
Figure 12: Exploration distance of different random walk strategies tested on circle graph. (a):
General random walk (b): Node2Vec. (c): Random Surfing. (d): BFS-like search. (e): BC-based
random walk(ours.).
Figure 13: Exploration distance of different random walk strategies tested on triangle graph. (a):
General random walk (b): Node2Vec. (c): Random Surfing. (d): BFS-like search. (e): BC-based
random walk(ours.).
Mk
M la*dι
Mfe ImVIi
Mfe Im*Ii
Mk
(a)	(b)	(c)	(d)	(e)
Figure 14: Exploration distance of different random walk strategies tested on tri-circle graph. (a):
General random walk (b): Node2Vec. (c): Random Surfing. (d): BFS-like search. (e): BC-based
random walk(ours.).
Figure 15: Exploration distance of different random walk strategies tested on tree graph. (a): Gen-
eral random walk (b): Node2Vec. (c): Random Surfing. (d): BFS-like search. (e): BC-based
random walk(ours.).
22
Under review as a conference paper at ICLR 2022
3‰k hwh"
(a)
Ia*d∣a
(b)
ɪife ImvIi -
(c)
Figure 16: Exploration distance of different random walk strategies tested on spiral graph. (a):
General random walk (b): Node2Vec. (c): Random Surfing. (d): BFS-like search. (e): BC-based
random walk(ours.).

|M«||Ia
(d)
3‰k hwh"
(e)

(a)
M Iavdi
(b)
Mfe ImvUi
(c)
Figure 17: Exploration distance of different random walk strategies tested on net graph. (a): General
random walk (b): Node2Vec. (c): Random Surfing. (d): BFS-like search. (e): BC-based random
walk(ours.).


Mfe Im*1i
(d)
(e)
Q Circle
JXK ⅛- ∙a~! S
nodK
(a)
G_Trιanβ∣o
JXK ⅛- ∙a~! S
no)⅛s
(b)
8_Tri-circl∙
JXK ⅛- ∙a~! S
(c)
JTree
JXK ⅛- ∙a-! S
IM	ISO	2M
r>odw
B Spiral
JXK ⅛- ∙a-! S
(d)	(e)
Figure 18: Fluctuation of BC value on each graph. (a): circle
tri-circle graph. (d): tree graph. (e): spiral graph. (f): net graph.
no)⅛s
(f)
IM
graph. (b): triangle graph. (c):
23