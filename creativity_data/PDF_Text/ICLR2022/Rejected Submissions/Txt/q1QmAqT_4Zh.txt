Under review as a conference paper at ICLR 2022
Koopman Q-learning: Offline Reinforcement
Learning via Symmetries of Dynamics
Anonymous authors
Paper under double-blind review
Ab stract
Offline reinforcement learning leverages large datasets to train policies without
interactions with the environment. The learned policies may then be deployed in
real-world settings where interactions are costly or dangerous. Current algorithms
over-fit to the training dataset and as a consequence perform poorly when deployed
to out-of-distribution generalizations of the environment. We aim to address these
limitations by learning a Koopman latent representation which allows us to infer
symmetries of the system’s underlying dynamic. The latter is then utilized to
extend the otherwise static offline dataset during training; this constitutes a novel
data augmentation framework which reflects the system’s dynamic and is thus to be
interpreted as an exploration of the environments phase space. To obtain the sym-
metries we employ Koopman theory in which nonlinear dynamics are represented
in terms of a linear operator acting on the space of measurement functions of the
system and thus symmetries of the dynamics may be inferred directly. We provide
novel theoretical results on the existence and nature of symmetries relevant for
control systems such as reinforcement learning settings. Moreover, we empirically
evaluate our method on several benchmark offline reinforcement learning tasks
and datasets including D4RL, Metaworld and Robosuite and find that by using
ourframework we consistently improve the state-of-the-art for Q-learning methods.
1	Introduction
The recent impressive advances in reinforcement learning (RL) range from robotics, to strategy games
and recommendation systems (Kalashnikov et al., 2018; Li et al., 2010). Reinforcement learning is
canonically regarded as an active learning process - also referred to as online RL - where the agent
interacts with the environment at each training run. In contrast, offline RL algorithms learn from large,
previously collected static datasets, and thus do not rely on environment interactions (Agarwal et al.,
2020a; Ernst et al., 2005; Fujimoto et al., 2019). Online data collection is performed by simulations
or by means of real world interactions e.g. robotics and in either scenario interactions maybe costly
and/or dangerous.
In principle offline datasets only need to be collected once which alleviates the before-mentioned
short-comings of costly online interactions. Offline datasets are typically collected using behavioral
policies for the specific task ranging from, random policies, or near-optimal policies to human
demonstrations. In particular, being able to leverage the latter is a major advantage of offline RL
over online approaches, and then the learned policies can be deployed or finetuned on the desired
environment. Offline RL has successfully been applied to learn agents that outperform the behavioral
policy used to collect the data (Kumar et al., 2020; Wu et al., 2019; Agarwal et al., 2020b; Ernst et al.,
2005). However algorithms admit major shortcomings in regard to over-fitting and overestimating the
true state-action values of the distribution. One solution was recently propsed by Sinha et al. (2021),
where they tested several data augmentation schemes to improve the performance and generalization
capabilities of the learned policies.
However, despite the recent progress, learning from offline demonstrations is a tedious endeavour as
the dataset typically does not cover the full state-action space. Moreover, offline RL algorithms per
definition do not admit the possibility for further environment exploration to refine their distributions
towards an optimal policy. It was argued previously that is basically impossible for an offline RL
agent to learn an optimal policy as the generalization to near data generically leads to compounding
1
Under review as a conference paper at ICLR 2022
errors such as overestimation bias (Kumar et al., 2020). In this paper, we look at offline RL through
the lens of Koopman spectral theory in which nonlinear dynamics are represented in terms of a
linear operator acting on the space of measurement functions of the system. Through which the
representation the symmetries of the dynamics may be inferred directly, and can then be used to guide
data augmentation strategies see Figure 1. We further provide theoretical results on the existence on
nature of symmetries relevant for control systems such as reinforcement learning. More specifically,
we apply Koopman spectral theory by: first learning symmetries of the system’s underlying dynamic
in a self-supervised fashion from the static dataset, and second employing the latter to extend the
offline dataset at training time by out-of-distribution values. As this reflects the system’s dynamics
the additional data is to be interpreted as an exploration of the environment’s phase space.
Some prior works have explored symmetry of the state-action space in the context of Markov Decision
Processes (MDP’s) (Higgins et al., 2018; Balaraman & Andrew, 2004; van der Pol et al., 2020) since
many control tasks exhibit apparent symmetries e.g. the classic cart-pole task which is symmetric
across y-axis. However, the paradigm we introduce in this work is of a different nature entirely. The
distinction is twofold: first, the symmetries are learned in a self-supervised way and are in general
not apparent to the developer; second: we concern with symmetry transformation from state tuples
(st, st+ι) → (st, St+ι) which leave the action invariant inferred from the dynamics inherited by the
behavioral policy of the underlying offline data. In other words we seek to derive a neighbourhood
around a MDP tuple in the offline dataset in which the behavioral policy is likely to choose the same
action based on its dynamics in the environment. In practice the Koopman latent space representation
is learned in a self-supervised manner by training to predict the next state using a VAE model (Kingma
& Welling, 2013).
To summarize, in this paper, we propose Koopman Forward (Conservative) Q-learning (KFC): a
model-free Q-learning algorithm which uses the symmetries in the dynamics of the environment
to guide data augmentation strategies. We also provide thorough theoretical justifications for KFC.
Finally, we empirically test our approach on several challenging benchmark datasets from D4RL (Fu
et al., 2021), MetaWorld (Yu et al., 2019) and Robosuite (Zhu et al., 2020) and find that by using
KFC we can improve the state-of-the-art on most benchmark offline reinforcement learning tasks.
2	Preliminaries and background
2.1	Offline RL & Conservative Q-learning
Reinforcement learning algorithms train policies to maximize the cumulative reward received by an
agent who interacts with an environment. Formally the setting is given by a Markov decision process
(S, A, ρ, r, γ), with state space S, action space A, and ρ(st+ι |st, at) the transition density function
from the current state and action to the next state. Moreover, γ is the discount factor and r(St) the
reward function. At any discrete time the agent chooses an action at ∈ A according to its underlying
policy ∏θ (at∣st) based on the information of the current state St ∈ S where the policy is parametrized
by θ. We focus on the Actor-Critic methods for continuous control tasks in the following. In deep
RL the parameters θ are the weights in a deep neural network function approximation of the policy
or Actor as well as the state-action value function Q or Critic, respectively, and are optimized by
gradient decent. The agent i.e. the Actor-Critic is trained to maximize the expected γ-discounted
cumulative reward Eπ [PtT=0 γt rπ(st, at)], with respect to the policy network i.e. its parameters θ.
For notational simplicity we omit the explicit dependency of the latter in the remainder of this work.
Furthermore the state-action value function Q(st, at), returns the value of performing a given action
at while being in the state st. The Q-function is trained by minimizing the so called Bellman error as
Q i+1 - arg min E [(rt + γQ^i(st+ι, at+ι) - Qi(st, at))2] ,	(1)
This is commonly referred to as the ith policy evaluation step where the hat denotes the target
Q-function. In offline RL one aims to learn an optimal policy for the given the dataset D =
Sk=1,...,#data-points st, at, rt, st+1 k as the option for exploration of the MDP is not available. The
policy is optimized to maximize the state-action value function via the policy improvement
∏i+ι — arg max Est〜D [Q (st,∏i(at∣st))] .	(2)
π
2
Under review as a conference paper at ICLR 2022
(7e is symmetry <	〉commutes with
randomly symmetry shifted	l. ɪ. . ɪ. ɪ
policy optimisation step
states in e - ball
Figure 1: Overview of Koopman Q-learning. The states of the data point (st, st+1 , at) are shifted along
symmetry trajectories parametrized by 1,2,3 for constant at. Symmetry transformations can be combined to
reach other specific subset of the -ball region.
Note that behavioural policies including sub-optimal or randomized ones may be used to generate the
static dataset D. In that case offline RL algorithms face difficulties in the learning process.
CQL algorithm: CQL is built on top of a Soft-Actor Critic algorithm (SAC) (Haarnoja et al., 2018),
which employs soft-policy iteration of a stochastic policy (Haarnoja et al., 2017). A policy entropy
regularization term is added to the policy improvement step in Eq. (2) as
∏i+ι — arg
maxEst〜D [Q(st,∏i(at∣st)) - αlog∏i(at∣st)].
(3)
where α either is a fixed hyperparameter or may be chosen to be trainable. CQL reduces the
overestimation of state-values - in particular those out-of distribution from D. It achieves this
by regularizing the Q-function in Eq. (1) by a term minimizing its values over out of distribution
randomly sampled action. In the following at+1 is given by the prediction of the policy πi(at+1 |st+1)
and a is a hyperparameter balancing the regulizer term. The policy optimisation step is given by
Qi+ι — arg min	E
Q	st,at,st+ι~D
+ α E
st〜D
*t + YQi(St+1,at+1) - Qi(St,at))	(4)
log V" exp (Qi(St,a)) -	E [Qi(st,,a)]
a—a	a 〜π(st)
2.2 Koopman theory
Historically, the Koopman theoretic perspective of dynamical systems was introduced to describe the
evolution of measurements of Hamiltonian systems (Koopman, 1931; Mezic, 2005). The underlying
dynamic of most modern reinforcement learning tasks is of nonlinear nature, i.e. the agents actions
lead to changes of it state described by a complex non-linear dynamical system. In contrast to linear
systems which are completely characterized by their spectral decomposition non-linear systems lack
such a unified characterisation. The Koopman operator theoretic framework describes nonlinear
dynamics via a linear infinite-dimensional Koopman operator and thus inherits certain tools applicable
to linear control systems (Mauroy et al., 2020; Kaiser et al., 2021). In practice one aims to find
a finite-dimensional representation of the Koopman operator which is equivalent to obtaining a
coordinate transformations in which the nonlinear dynamics are approximately linear. A general
non-affine control system is governed by the system of non-linear ordinary differential equations
(ODEs) as
S = f (S, a)
(5)
where S, is the n-dimensional state vector, a the m-dimensional action vector with (S, a) ∈ S × A
the state-action-space. Moreover, S = ∂tS is the time derivative, and f is some general non-linear - at
least C1-differentiable - vector valued function. For a discrete time system, Eq. (5) takes the form
St+1 = F(St, at)	(6)
where St denotes the state at time t where F is at least C1-differentiable vector valued function.
Definition 1 (Koopman operator) Let K(S × A) be the (Banach) space of all measurement func-
tions (observables). Then the Koopman operator K : K(S × A) → K(S × A) is defined by
Kg(St, at) = g F(St, at), at+1 = g(St+1, at+1) , ∀g ∈ K(S × A)	(7)
where g : S × A → R.
3
Under review as a conference paper at ICLR 2022
Many systems can be modeled by a bilinearisation where the action enters the controlling equations 5
linearly as f(s, a) = f0(s) + Pim fi(s)ai for fi, i = 0, . . . , m i.e. C1-differentiable-vector valued
functions. In that case the action of the Koopman operator takes the simple form
m
g(st+1) = K(a)g(st)	=	K0	+ XKiai	g(st)	,	∀g	∈ K(S)	,	(8)
i
where K(S) is a (Banach) space of measurement functions K0, Ki decompose the Koopman operator
in a the free and forcing-term, respectively. Details on the existence of a representation as in
equation 8 are discussed in Goswami & Paley (2017). Associated with a Koopman operator is its
eigensPectrUm, that is, the eigenvalues λ, and the corresponding eigenfunctions 夕λ(s, a), SUch that
[K2λ](s, a) = λ夕λ(s, a). In practice one derives a finite set of observable g = (gι,..., gN) in which
case the approximation to the Koopman operator admits a finite-dimensional matrix representation.
The N × N matrix representing the Koopman operator may be diagonalized by a matrix U containing
the eigen-vectors of K as columns. In which case the eigen-functions are derived by ~ = U~ and one
infers from Eq. (5) that Si = λiφi , for i = 1,..., N with eigenvalues λi=ι,...,N. These ODES
admit simple solutions for their time-evolution namely the exponential functions exp(λit)..
3 The Koopman forward framework
We initiate our discussion with a focus on symmetries in dynamical control systems in Subsection 3.1
where we additionally present some theoretical results. We then proceed in Subsection 3.2 by
presenting the Koopman forward framework for Q-learning based on CQL. Moreover, we discuss the
algorithm as well as the Koopman Forward model’s deep neural network architecture.
Overview: Our theoretical results culminate in Theorem 3.4 and 3.5, which provides a road-map
on how specific symmetries of the dynamical control system are to be inferred from a VAE forward
prediction model. In particular, Theorem 3.4 guarantees that the procedure leads to symmetries of
the system (at least locally) and Theorem 3.5 that actual new data points can be derived by applying
symmetry transformations to existing ones. Practically the VAE model parametrized by a neural net
is trained data from one of many behavior policies and is thus learning an approximate dynamics.
The theoretical limitations are twofold, firstly the theorems only hold for dynamical systems with
differentiable state-transitions; secondly, we employ a Bilinearisation Ansatz for the Koopman
operator of the system. Practically many RL environments incorporate dynamics with discontinuous
“contact” events where the Bilinearisation Ansatz may not be applicable. However, empirically we
find that our approach nevertheless is successful for environments with “contact” events. The latter
does not affect the performance significantly (see Appendix D.1).
3.1	Symmetries of dynamical control system
In general, a symmetry group Σ of the state space may be any subgroup of the group of isometrics
of the Euclidean space En. We restrict oneself to an Euclidean state space, however in general one
may consider Riemannian manifolds, see e.g. Giannakis (2019). In particular, in our work we also
consider Σ invariant compact subsets of Euclidean space. Moreover, relevant for a system of ODEs
are local Lie symmetries as well as symmetries of ODEs.
Definition 2 (Local Lie Group) A Parametrizedset OftransfOrmatiOns σe : S → S with S → 3(s, e)
for ∈ (low , high ) where low < 0 < high is a one-parameter local Lie group if
1.	σ0 is the identity map i.e fOr e = 0 such that s3(s, 0) = s.
2.	σ1 σ2 = σ2σ1 = σ1+2 fOr every |e1 |, |e2 |	1.
3.	s3(s, e) admits a TaylOr series expansiOn in e, i.e. in a neighbOurhOOd Of s determined by
e = 0 as s3(s, e) = s + e ζ(s) + O(e2).
The points (1) and (2) in definition 2 imply the existence of an inverse element σ -1 = σ- for
|e|	1. Moreover, note that a system of ODEs may be represented by the set of its solutions.
Definition 3 (Symmetries of ODEs) A symmetry Of a system Of ODEs On a lOcally-smOOth structure
(such as En) is a lOcally-defined diffeOmOrphism that maps the set Of all sOlutiOns tO itself.
4
Under review as a conference paper at ICLR 2022
Definition 4 (Equivariant Dynamical System) Consider the dynamical System S = f (S) and let
Σ be a group acting on the state-space S. Then the System is called Σ-equivariant if f (σ ∙ s) =
σ ∙ f (s) , for s ∈ S , ∀σ ∈ Σ. For a discrete time dynamical SyStem st+ι = F(St) one defines
equivariance analogously, namely if F(σ ∙ St) = σ ∙ F(St) , for St ∈ S , ∀σ ∈ Σ.
Note that as a local Lie group may satisfy the group axioms for sufficiently small parameters values it
may not be a group on the entire set. And moreover not every diffeomorphism is also an isometry.
Let us start by introducing symmetries in the simple context of dynamical system without control
(Sinha et al., 2020) given by S = f (s) where We use analog notations as in Eq.(5). The KooPman
operator of equivariant dynamical system is reviewed in the appendix A. Let us next turn to the case
relevant for RL, namely control systems. In the remainder of this section we focus on dynamical
systems given as in Eq. (6) and Eq. (8).
Definition 5 (Action EquiVariant Dynamical System) Let Σ be a group acting on the state-action-
space S × A of a general control system as in Eq. (6) such that it acts as the identity operation on A
i.e. σ ∙ (st,at) = (σ∣s ∙ St,a∕ , ∀σ ∈ Σ . Then the system is called Σ-action-equivariant if
F(σ ∙ (st, at)) = σ∣s ∙ F(st, at) , for (st, at) ∈S ×A , ∀σ ∈ Σ .	(9)
Lemma 3.1 The map 求：Σ X K(S XA) → K(S X A) given by (σ求g)(s, a) -~~→ g(σ-1 ∙ s, a)
defines a group action on the Koopman space of observables K(S × A).
Theorem 3.2 A Koopman operator K ofa Σ-action-equivariant SyStem st+ι = F(st,at) satisfies
[σ求(Kg)](st,at) = [K(σ求g)](st,at) .	(10)
In particular, it is easy to see that the biliniarisation in Eq. (8) is Σ-action-equivariant if fi(σ∣s ∙ s)=
σ∣s ∙ fi(s) , ∀i = 0,...,m. Let us thus turn our focus on the relevant case of a control system
st+ι = F(st, at) which admits a Koopman operator description as
g(st+1) = K(at)g(st) , for at ∈ A , ∀g ∈ K(S) ,	(11)
where {K(a)}a∈A is a family of operators with analytical dependence on a ∈ A. Note that the
bilinearisation in Eq. (8) is a special case of Eq. (11). Furthermore, let {U (a)}a∈A be a family
of invertible operators s.t. U(a) : K(S) → F(S X A) is a mapping to the (Banach) space of
eigenfunctions F(SXA) with eigenfunctions 夕(s,a) := U(a)g(s) which obeys U(a)K(a)U(a)-1 =
Λ(a), with Λ(a)夕(s, a) = λφ(a)φ(s, a) and where λφ(a) : A → R. The existence of such operators
on an infinite dimensional space requires the Koopman operator in Eq. (11) to be self-adjoint or a
finite-dimensional (approximate) matrix representation to be diagonalizable.1
Lemma 3.3 The map φ: Σ X K(S) X {U(a)}a∈A → K(S) g^ven by
(σa⅛O(S)I~~› (U-1(a)(σ 采(U (a)g)))(S)= g(σ-1 ∙ s) ,	(12)
defines a group action on the Koopman space of observables K(S). Where 求 is defined analog to
Lemma 3.1 but acting on F(S X A) instead of K(S X A) by (σ林)(s, a) -~~→ 夕(σ-1 ∙ s, a). We refer
to a control system in Eq. (11) admitting a φ-symmetry as Σ-action-equivariant.
mi. . . . .	4 T	τ-t /	∖ ι	S ，∙	∙	∙	< X ，	∙, 1	，	，∙
Theorem 3.4 Let st+1 = F(st, at) be a Σ-action-equivariant control system with a symmetry action
as in Lemma 3.3 which furthermore admits a Koopman operator representation as
g(st+1) = K(at)g(st) , for at ∈ A, ∀g ∈ K(S) .	(13)
Then
[σat MK(at)g)] (St)= [K(at) (σat 余g)] (St).
(14)
Moreover, a control system obeying equations 13 and 14 is Σ-action-equivariant locally if g-1 exists
r	∙ ιι ι Ir ∙	. ι	f~ι t	∖ f~ι t t	∖ ∖
for a neighborhood of St, ι.e. then σ ∙ F(St, a. = F(σ ∙ (st, a.).
1See Appendix A for the details. We found that, in practice the Koopman operator leaned by the neural nets
is diagonalizable almost everywhere.
5
Under review as a conference paper at ICLR 2022
Let us next provide theoretical statements on how data-points may be shifted by symmetry transfor-
mations of solutions of ODEs. To establish an easier connection to the next section we introduce the
notation Let E : S → K(S) and D : K(S) → S denote the C1-differentiable encoder and decoder to
and from the finite-dimensional KooPman space approximation, respectively, i.e. E◦D = D ◦E = id.
Theorem 3.5 Let st+ι = F(st,at) be a control system as in equation 13 and。.t an operator
obeying equation 14. Then σ1t : (st, st+ι, aj -~~→(3如 st+ι,αt) with
St = D((1 + eσat)^E(st)) , st+ι = D((1 + eσaj余E(st+ι))	(15)
is a one-parameter local Lie symmetry group of ODEs. In other words one can use a symmetry
transformation to shift both st as well as st+1 such that sSt+1 = FS(sSt, at). 2
3.2 KFC algorithm
In the previous section we laid the theoretical foundation for the KFC-algorithm by providing a
raod-map on how to derive symmetries of dynamical control systems based on a Koopman latent
space representation. The goal is to generate new data points for the RL algorithm at training-time as
Eq. (15) in Theorem 3.5. The reward rt is not part of the symmetry shift process and will just remain
unchanged as an assertion.
On the power of using symmetries: Let us emphasize the practical advantage of employing
symmetries to generate augmented data points. It is evident from Theorem 3.5 that a symmetry
transformation shifts both st as well as st+1 which evades the necessity of forecasting states. Thus
the use of an inaccurate fore-cast model is avoided and the accuracy and generalisation capabilities
of the VAE are fully utilized. The magnitude of the induced shift is controlled by the parameter
e 1 such that |s - sS| = O(e) to limit out-of-distribution generalisation errors . Algorithmically
the symmetry maps are derived in two distinct ways which we denote KFC and KFC++. The latter,
constitute a simple starting point to extract symmetries from our setup.3 *
Limitations: Theorems 3.4 and 3.5 imply that if an operator commutes with the Koopman operator
we can find a local symmetry group. However, global conditions may not be easily inferred. Moreover,
in practice one has that D ◦ E ≈ id. Thus all assumptions of theorem 3.5 hold approximately.
The Koopman forward model: The KFC algorithm requires pre-training of a Koopman forward
model F : S × A → S which is closely related to a VAE architecture as
D E(st)	= st
Fc(st, at) =
[D ] (Ko + Pi=ι Ki at,i)E(St)) = st+ι
if c = 0: VAE
if c = 1: forward prediction
(16)
where both of E and D are approximated by Multi-Layer-Perceptrons (MLP’s) and the bilinear
Koopman-space operator approximation are implemented by a single fully connected layer for
Ki , i = 0, . . . , m, respectively. The model is trained on batches of the offline data-set tuples
(st+1, st, at) and optimized via an additive loss-function of the VAE and the forward prediction part
of the model. We refer the reader to Appendix C for details.
We integrate our framework into a specific Q-learning algorithm (CQL). In practice the Koopman
latent space representation is N-dimensional (finite). The Koopman operator and the symmetry
generators admit matrix representations; matrix multiplication replaces the mapping ^. Following in
the footsteps of (Sinha et al., 2021) our approach leaves the policy improvement Eq. (3) unchanged
but modifies the policy optimisation step Eq. (4) as
Qi+1 - arg min
Q
E
st,at,st+ι〜D |_
+ αS	E
St〜D
rt+YQ i (工 t(st+i|st+i),at+i) — Qi (工 t (StIst), at))
logTexp (Qi(St,a)) 一 E [Qi(st,
a	a 〜π(st)
(17)
2No assumptions on the Koopman operator are imposed. Moreover, note that the equivalent theorem holds
when σat → PI=1 I σaIt to be a local N-parameter Lie group.
3More elaborate studies employing the extended literate on Koopman spectral analysis are desirable. See
appendix B for details on numerical errors.
6
Under review as a conference paper at ICLR 2022
The state-space symmetry generating function σa : S → S depends on the normally distributed
random variables . Note that we only modify the Bellman error of Eq. (17) and leave the CQL
specific regulizer untouched. We study two distinct cases which differ on an algorithmic level4
KFC b£: s→ S = D ((1 + eσajE(s))	KFC++ b£ : s→ S = D((1 + b。,(e))E(S)),
where σ:t (e) = Re( U (at) diag(eι,..., e.) U-1(at)) ,	(18)
and U(at) diagonalizes the Koopman operator with eigenvalues λi , i = 1, . . . , N i.e. the latter
can be expressed as K(at) = U(at)diag(λ1, . . . , λN)U-1(at). Note that we abuse our notation as
E(S) ≡ ~g(S) = [g1(S), . . . , gN (S)] i.e. the encoder provides the Koopman space observables. From
theorem 3.5 one infers that in order for Eq. (18) to be a local Lie symmetry group of the approximate
ODEs captured by the neural net Eq. (16) one needs σat to commute with the approximate Koopman
operator in Eq. (16).
For the KFC option in Eq. (18) the symmetry generator matrix σa is obtained by solving the equation
ba, ∙ K(at) — K(at) ∙ ba, = 0 which may be accomplished by employing a Sylvester algorithm (syl).
For KFC++ we compute the eigen-vectors of K(a) which then constitute the columns of U (a). Thus
in particular one infers that [ba, (~e), K(at)] = 0. The latter, is solved by construction of ba(~e) which
commutes with the Koopman operator for all values of the random variables.5 The advantage of KFC
is that it is computationally less expensive than KFC++ , however it provides less freedom to explore
different symmetry directions than the latter. We employ our symmetry shift only with probability
pK ; otherwise we use a random normally distributed state shift.
4	Empirical evaluation
In this section, we will first experiment with the popular D4RL benchmark commonly used for offline
RL (Fu et al., 2021). The benchmark covers various different tasks such as locomotion tasks with
Mujoco Gym (Brockman et al., 2016), tasks that require hierarchical planning such as antmaze, and
other robotics tasks such as kitchen and adroit (Rajeswaran et al., 2017). Furthermore, similar to
S4RL (Sinha et al., 2021), we perform experiments on 6 different challenging robotics tasks from
MetaWorld (Yu et al., 2019) and RoboSuite (Zhu et al., 2020). We compare KFC to the baseline
CQL algorithm (Kumar et al., 2020), and two best performing augmentation variants from S4RL,
S4RL-N and S4RL-adv (Sinha et al., 2021). We use the exact same hyperparameters as proposed in
the respective papers. Furthermore, similar to S4RL, we build KFC on top of CQL (Kumar et al.,
2020) to ensure conservative Q-estimates during for policy evaluation.
4.1	D4RL benchmarks
We present results in the benchmark D4RL test suite and report the normalized return in Table 1.
We see that both KFC and KFC++ consistently outperform both the baseline CQL and S4RL across
multiple tasks and data distributions. Outperforming S4RL-N and S4RL-adv on various different
types of environments suggests that KFC and KFC++ fundamentally improves the data augmentation
strategies discussed in S4RL. KFC-variants also improve the performance of learned agents on
challenging environments such as antmaze: which requires hierarchical planning, kitchen and adroit
tasks: which are sparse reward and have large action spaces. Similarly, KFC-variants also perform
well on difficult data distributions such as “medium-replay”: which is a collected by simply using
all the data that the policy encountered while training base SAC policy, and “-human” which is
collected using human demonstrations on robotic tasks which results in a non-Markovian behaviour
policy (more details can be found in the D4RL manuscript (Fu et al., 2021)). Furthermore, to our
knowledge, the results for KFC++ are state-of-the-art in policy learning from D4RL datasets for most
environments and data distributions.
We do note that S4RL outperforms KFC on the “-random” split of the data distributions, which is
expected as KFC depends on learning a simple dynamics model of the data to use to guide the data
augmentation strategy. Since the “-random” split consists of random actions in the environment, our
simple model is unable to learn a useful dynamics model. For ablation studies see appendix D.
4Eq. (18) holds for both st as well as st+1 thus we have deliberately dropped the subscript. Moreover, note
that in case (II) the symmetry transformations are of the form σa, (~) = PI=1 I σaI, .
5[∙, ∙] denotes the commutator of two matrices. The Koopman operators eigenvalues and eignevectors
generically are C-valued. However, the definition in Eq. (18) ensures that σa ( ~) is a R-valued matrix.
7
Under review as a conference paper at ICLR 2022
Table 1: We experiment with the full set of the D4RL tasks and report the mean normalized episodic returns
over 5 random seeds using the same protocol as Fu et al. (2021). We compare against 3 competitive baselines
including CQL and the two best performing S4RL-data augmentation strategies. We see that KFC and KFC++
consistently outperforms the baselines. We use the baseline numbers reported in Sinha et al. (2021).
Domain	Task Name	CQL	S4RL-(N)	S4RL-(Adv)	KFC	KFC++
	antmaze-umaze	74.0	91.3	94.1	96.9	99.8
	antmaze-umaze-diverse	84.0	87.8	88.0	91.2	91.1
AntMaze	antmaze-medium-play	61.2	61.9	61.6	60.0	63.1
	antmaze-medium-diverse	53.7	78.1	82.3	87.1	90.5
	antmaze-large-play	15.8	24.4	25.1	24.8	25.6
	antmaze-large-diverse	14.9	27.0	26.2	33.1	34.0
	cheetah-random	35.4	52.3	53.9	48.6	49.2
	cheetah-medium	44.4	48.8	48.6	55.9	59.1
	cheetah-medium-replay	42.0	51.4	51.7	58.1	58.9
	cheetah-medium-expert	62.4	79.0	78.1	79.9	79.8
	hopper-random	10.8	10.8	10.7	10.4	10.7
Gym	hopper-medium	58.0	78.9	81.3	90.6	94.2
	hopper-medium-replay	29.5	35.4	36.8	48.6	49.0
	hopper-medium-expert	111.0	113.5	117.9	121.0	125.5
	walker-random	7.0	24.9	25.1	19.1	17.6
	walker-medium	79.2	93.6	93.1	102.1	108.0
	walker-medium-replay	21.1	30.3	35.0	48.0	46.1
	walker-medium-expert	98.7	112.2	107.1	114.0	115.3
	pen-human	37.5	44.4	51.2	61.3	60.0
	pen-cloned	39.2	57.1	58.2	71.3	68.4
	hammer-human	4.4	5.9	6.3	7.0	9.4
Adroit	hammer-cloned	2.1	2.7	2.9	3.0	4.2
	door-human	9.9	27.0	35.3	44.1	46.1
	door-cloned	0.4	2.1	0.8	3.6	5.6
	relocate-human	0.2	0.2	0.2	0.2	0.2
	relocate-cloned	-0.1	-0.1	-0.1	-0.1	-0.1
Franka	kitchen-complete	43.8	77.1	88.1	94.1	94.9
	kitchen-partial	49.8	74.8	83.6	92.3	95.9
KFC++	KFC	CQL+S4RL-Adv CQL+S4RL-N	CQL
(a) MetaWorld Environments
(b) RoboSuite Environments
Figure 2: Results on challenging dexterous robotics environments using data collected using a similar strategy
as S4RL (Sinha et al., 2021). We report the % of goals that the agent is able to reach during evaluation, where
the goal is set by the environments. We see that KFC and KFC++ consistently outperforms both CQL and
the two best performing S4RL variants.
4.2	Metaworld and Robosuite benchmarks
To further test the ability of KFC, we perform additional experiments on challenging robotic tasks.
Following (Sinha et al., 2021), we perform additional experiments with 4 MetaWorld environments
(Yu et al., 2019) and 2 RoboSuite environments (Zhu et al., 2020). We followed the same method to
8
Under review as a conference paper at ICLR 2022
collect the data as described in Appendix F of S4RL (Sinha et al., 2021), and report the mean percent
of goals reached, where the condition of reaching the goal is defined by the environment.
We report the results in Figure 2, where we see that using KFC to guide the data augmentation strategy
for a base CQL agent, we are able to learn an agent that performs significantly better. Furthermore,
we see that for more challenging tasks such as “push” and “door-close” in the MetaWorld, KFC++
outperforms the base CQL algorithm and the S4RL agent by a significant margin. These set of
experiments further highlight the ability of KFC to guide the data augmentation strategy.
5	Related works
The use of data augmentation techniques in Q-learning has been discussed recently (Laskin et al.,
2020b;a; Sinha et al., 2021). In particular, our work shares strong parallels with (Sinha et al.,
2021). Our modification of the policy evaluation step of the CQL algorithm (Kumar et al., 2020) is
analogous to the one in Sinha et al. (2021). However, the latter randomly augments the data while
our augmentation framework is based on symmetry state shifts. Regarding the connection to world
models (Ha & Schmidhuber, 2018). Here a VAE is used to decode the state information while a
recurrent separate neural network predicts future states. Their latent representation is not of Koopman
type. Also no symmetries and data-augmentations are derived.
Algebraic symmetries of the state-action space in Markov Decision Processes (MDP) originate
(Balaraman & Andrew, 2004) an were discussed recently in the context of RL in (van der Pol et al.,
2020). Their goal is to preserve the essential algebraic homomorphism symmetry structure of the
original MDP while finding a more compact representation. The symmetry maps considered in our
work are more general and are utilized in a different way. Symmetry-based representation learning
(Higgins et al., 2018) refers to the study of symmetries of the environment manifested in the latent
representation. The symmetries in our case are derived form the Koopman operator not the latent
representation directly. In (Caselles-DuPre et al., 2019) the authors discuss representation learning of
symmetries (Higgins et al., 2018) allowing for interactions with the environment. A Forward-VAE
model which is similar to our Koopman-Forward VAE model is employed. However, our approach
is based on theoretical results providing a road-map to derive explicit symmetries of the dynamical
systems as well as their utilisation for state-shifts.
In (Sinha et al., 2020) the authors extend the Koopman operator from a local to a global description
using symmetries of the dynamics. They do not discuss action-equivariant dynamical control systems
nor data augmentation. In (Salova et al., 2019) the imprint of known symmetries on the block-
diagonal Koopman space representation for non-control dynamical systems is discussed. This is
close to the spirit of disentanglement (Higgins et al., 2018). Our results are on control setups and
deriving symmetries. On another front, the application of Koopman theory in control or reinforcement
learning has also been discussed recently. For example, Li et al. (2020) propose to use compositional
Koopman operators using graph neural networks to learn dynamics that can quickly adapt to new
environments of unknown physical parameters and produce control signals to achieve a specified
goal. Kaiser et al. (2021) discuss the use of Koopman eigenfunction as a transformation of the state
into a globally linear space where the classical control techniques is applicable. To the best of our
knowledge, this paper is the first to discuss Koopman latent space for data augmentation.
6	Conclusions
In this work we proposed a symmetry-based data augmentation technique derived from a Koopman
latent space representation. It enables a meaningful extension of offline RL datasets describing
dynamical systems, i.e. further "exploration" without additional environment interactions. The
approach is based on our theoretical results on symmetries of dynamical control systems and symmetry
shifts of data. Both hold for systems with differentiable state transitions and with a Bilinearisation
Ansatz for the Koopman operator. However, the empirical results show that the framework is
successfully applicable beyond those limitations. We empirically evaluated our method on several
benchmark offline reinforcement learning tasks D4RL, Metaworld and Robosuite and find that by
using our framework we consistently improve the state-of-the-art of Q-learning algorithms.
9
Under review as a conference paper at ICLR 2022
References
Sylvester algorithm - scipy.org. URL https://docs.scipy.org/doc/scipy-0.15.1/
reference/generated/scipy.linalg.solve_sylvester.html.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning, 2020a.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, 2020b.
Ravindran Balaraman and G. Barto Andrew. Approximate homomorphisms: A framework for non-
exact minimization in markov decision processes. In In International Conference on Knowledge
Based Computer Systems, 2004.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Steven L. Brunton, Marko Budisic, Eurika Kaiser, and J. Nathan Kutz. Modern koopman theory for
dynamical systems, 2021.
Hugo CaSelleS-DUPra Michael Garcia Ortiz, and David Filliat Symmetry-based disentangled
representation learning requires interaction with environments. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6(18):503-556, 2005.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning, 2021.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 2052-2062. PMLR, 09-15 Jun 2019.
Dimitrios Giannakis. Data-driven spectral decomposition and forecasting of ergodic dynamical
systems. Applied and Computational Harmonic Analysis, 47(2):338-396, 2019. ISSN 1063-
5203. doi: https://doi.org/10.1016/j.acha.2017.09.001. URL https://www.sciencedirect.
com/science/article/pii/S1063520317300982.
Debdipta Goswami and Derek A. Paley. Global bilinearization and controllability of control-affine
nonlinear systems: A koopman spectral approach. 2017 IEEE 56th Annual Conference on Decision
and Control (CDC), pp. 6107-6112, 2017.
David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 1352-1361. PMLR, 06-11 Aug 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1861-1870. PMLR, 10-15 Jul 2018.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a definition of disentangled representations, 2018.
10
Under review as a conference paper at ICLR 2022
Eurika Kaiser, J. Nathan Kutz, and Steven L. Brunton. Data-driven discovery of Koopman eigenfunc-
tions for control. Machine Learning: Science and Technology, 2:035023, 2021.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Scalable
deep reinforcement learning for vision-based robotic manipulation. In Aude Billard, Anca Dragan,
Jan Peters, and Jun Morimoto (eds.), Proceedings of The 2nd Conference on Robot Learning,
volume 87 of Proceedings ofMachine Learning Research, pp. 651-673. PMLR, 29-31 Oct 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Bernard Osgood Koopman. Hamiltonian systems and transformation in Hilbert space. Proceedings
of the National Academy of Sciences of the United States of America, 17(5):315-318, 1931.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1179-1191. Curran
Associates, Inc., 2020.
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representa-
tions for reinforcement learning. In Hal Daume In and Aarti Singh (eds.), Proceedings ofthe 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 5639-5650. PMLR, 13-18 Jul 2020a.
Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
19884-19895. Curran Associates, Inc., 2020b.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
World wide web, pp. 661-670, 2010.
Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba. Learning compositional Koopman
operators for model-based control. In Proc. of the 8th Int’l Conf. on Learning Representation
(ICLR’20), 2020.
Alexandre Mauroy, Igor MeziC, and Yoshihiko Susuki. The Koopman Operator in Systems and
Control: Concepts, Methodologies, and Applications. Springer, 2020.
I. MeziC. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear
Dynamics, 41:309-325, 2005.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Icml, 2010.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.
Anastasiya Salova, Jeffrey Emenheiser, Adam Rupe, James P. Crutchfield, and Raissa M. D’Souza.
Koopman operator and its approximations for systems with symmetries. Chaos: An Interdisci-
plinary Journal of Nonlinear Science, 29(9):093128, 2019. doi: 10.1063/1.5099091.
Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4RL: Surprisingly Simple Self-Supervision
for Offline Reinforcement Learning. In Conference on Robot Learning, 2021.
11
Under review as a conference paper at ICLR 2022
Subhrajit Sinha, Sai P. Nandanoori, and Enoch Yeung. Koopman operator methods for global phase
space exploration of equivariant dynamical systems, 2020.
Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homo-
morphic networks: Group symmetries in reinforcement learning. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 4199-4210. Curran Associates, Inc., 2020.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning,
2019.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
arXiv preprint arXiv:1910.10897, 2019.
Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mardn-Mardn. robosuite: A modular
simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.
12
Under review as a conference paper at ICLR 2022
A Koopman theory and S ymmetries
This section provides some additional introductory material on Koopman theory and symmetries of
dynamical systems.
Lemma A.1 The map * : Σ X K(S) → K(S) given by (σ * g)(s) -~~→ g(σ-1 ∙ S) defines a group
action on the Koopman space of observables K(S).
Theorem A.2 LetK be the Koopman operator associated with a Σ-equivariant system st+1 = F (st).
Then
[σ* (Kg)](s) = [K(σ *g)](s) .
(19)
Theorem A.2 states that for a Σ-equivariant system any symmetry transformation commutes with the
Koopman operator. For the proof see (Sinha et al., 2020).
To proceed let us re-evaluate some information from the main text of this work. Let {U (a)}a∈A be
a family of invertible operators s.t. U(a) : K(S) → F(S × A) is a mapping to the (Banach) space
of eigenfunctions 夕(s, a) := U(a)g(s) ∈ F(S × A). Which moreover obeys U(a)K(a)U(a)-1 =
Λ(a), with Λ(a)夕(s, a) = λφ(a)^(s, a) and where λφ(a) : A → R. The existence of such operators
puts further restriction on the Koopman operator in Eq. (11). However, as our algorithm employs the
finite-dimensional approximation of the Koopman operator i.e. its matrix representation this amounts
simple for K(a) to be diagonalizable and U(a) is the matrix containing its eigen-vectors as columns.
To evolve a better understanding on the required criteria for the infinite-dimensional case we employ
an alternative formulation of the so called spectral theorem below.
Theorem A.3 (Spectral Theorem) Let K be a bounded self-adjoint operator on a Hilbert space H.
Then there is a measure space (S, Σ,μ) and a real-valued essentially bounded measurablefunction
λ on S and a unitary operator U : H → Lμ(S) i.e. U*U = UU* = id such that
U A U * = K , with [Aq](s)= λ(s)φ(s) .	(20)
In other words every bounded self-adjoint operator is unitarily equivalent to a multiplication operator.
In contrast to the finite-dimensional case we need to slightly alter our criteria to U (a)K(a)U (a)-1 =
A(a), with A(a)夕(s,a) = λφ(a,s)夕(s,a) and where λ中(s,a) : S × A → R. Concludingly, a
sufficient condition for our criteria to hold in terms of operators on Hilbert spaces is that the Koopman
operator K(a) is self-adjoint i.e. that
K(a) = K(a)* .	(21)
B Proofs
In this section we provide the proofs of the theoretical results of Section 3.
Proof of Lemma A.1 and Theorem A.2 The proofs of the lemma as well as the theorem can be
found in (Sinha et al., 2020).
Proof of Lemma 3.1: We aim to show that map * : Σ × K(S × A) → K(S × A) given by
(σ*g)(s, a) —-→ g(σ-1 ∙ s, a) defines a group action on the Koopman space of observables K(S × A),
where Σ defines the symmetry group of definition 5. Firstly, let g ∈ K(S × A) and σ° ∈ Σ be the
identity element, then we see that it provides existence of an identity element of * by
(σo汞g)(s,a)= g(σ0^1 ∙ s,a) = g(s,a).
Secondly, let σ1,σ2 ∈ Σ and Θ denoting the group operation i.e. σι Θ σ2 = σ3 ∈ Σ. Then
(σ2*(σ1*g)) (s,a)	= σ2*	(g(σ-1	∙ s,a))	=	g(σ-1	∙	(σ-1	∙ s),a)	= g((σ-1	θ σ-1) ∙ s),a)
=) g((σι Θ σ2)-1 ∙ s,a) = ((σι Θ σ2)*g) (s,a) = (σ3*g) (s,a),
where in (I) we have used the invertibility of the group property of Σ. Lastly it follows analogously
that for σ, σ-1
σ*
Thus the existence of an inverse is established which concludes to show the group property of 七
13
Under review as a conference paper at ICLR 2022
Proof of Theorem 3.2: We aim to show that with K being the Koopman operator associated with a
Σ-action-equivariant system st+ι = F (st,at). Then
[σ求(Kg)](s,a)= hK(σ求g)](s,a).
First of all note that by the definition of the Koopman operator of non-affine control systems it obeys
[Kg](st, at) = g(F (st, at), at+1) .
Using the latter one thus infers that
，WKg)] (st,at) = σ 许 g(F (St ,ajat+ι) = g(σ-1 ∙ F (st, aj at+ι) = g(F (σ-1 ∙ st, aj at+ι),
where in (I) We have used that it is a Σ-action-equivariant system. Moreover, one finds that
g(F(σ-1 ∙ st, at), at+ι) = Kg(σ-1 ∙ St, a/ = [κ(σ求g) ] (st,at),
which concludes the proof.
TΓ⅛ i∙ i∙ ɪ	C C	-ɪɪ τ ♦	,	1	.1 ..1	7 XzS CZ∙∕c∖ C T T / ∖ 1	CZ∙ / c∖ ∙
Proof of Lemma 3.3: We aim to show that the map φ : Σ × K(S) × {U (a)}a∈A → K(S) given
by
(σa余g) (S) --→ (U-1(aXσ求(U(aOg))) (S)= g(σ-1 ∙ S),
defines a group action on the Koopman space of observables K(S). Where 汞 is defined analog to
Lemma 3.1 but acting on F(S X A) instead of K(S X A) by (σ求夕)(s, a) —-→ 夕(σ-1 ∙ s, a). First of
all note that
(U-1(a)(σ	* (U(a)g)))(e) =	U-1(a)(σ *	^(s, a))	=	U-1(a)j(σ-1 ∙ s,	a)	=	g(σ-1	∙ s)	(22)
We proceed analogously as in the proof of Lemma 3.1 above. Firstly, let g ∈ K(S) and σ° ∈ Σ be
the identity element then on infers from Eq. (22) that it provides existence of an identity element of φ
by
(σa,0*g)(S) = g(σ-1 ∙ S) = g(s),
where we have used the notation
(σa,i*g)(s)= (u-1(a)(σi¢(U(a)g)))(s), for i = 0,....
Secondly, let σ1,σ2 ∈ Σ and Θ denoting the group operation i.e. σι Θ σ2 = σ3 ∈ Σ. Then
(σa,2*(σa,1余g。(S)	= σa,2*(g(σ-1	∙	S))	=	g(σ-1	∙ (σ-1 ∙	s))	= g((σ-1	θ σ-1)	∙	s)
=g((σi θ σ2)-1 ∙ s) = g(σ-^1 ∙ s) = (σa,3*g) (s),
where in (I) we have used the invertibility of the group property of Σ. Lastly, it follows analogously
that for σ,σ-1 ∈ Σ that
(σ*(σ-1*g))(s) = ((σ-1 Θ σ)⅜g)(s) = (σ0余g)(s) = g(s).
Thus the existence ofan inverse is established which concludes to show the group property of φ.
Proof of Theorem 3.4: We aim to show twofold.
^⇒: Firstly, that with st+ι = F (st,at) be a Σ-actιon-equιvarιant control system with a symmetry
action as in Lemma 3.3 which furthermore admits a Koopman operator representation as
g(st+1) = K(at)g(st) , for at ∈ A , ∀g ∈ K(S) .
Then
Kat £(K(at)g)] (St)= IK(Ot) (σat fg) (St).
14
Under review as a conference paper at ICLR 2022
仝三： Secondly, the converse. Namely, if a control system st+ι = 户(st, at) obeys Eqs. (13) and
(14), then it is Σ-actιon-equivarιant, i.e, σ ∙ F (st, at) = F (σ ∙ (st, at)). For notational simplicity we
drop the subscripts referring to time in the remainder of this proof i.e. St → S and at → a.
Let us start with the first implication i.e. =⇒.
First of all note that by the definition of the Koopman operator one has
[K(a)g](s) = g(F(S,a)).
Using the latter one infers that 卜a余(K(a)g)] (S),	=σ0*g(F(s, a)), =UT(a)(σ采(U(a)g(F(S,a)))), =U-1(a)卜称(F (S, a),a)), =u-1(a)^φ(σ-1 ∙ F(S,a),a))), =u-1(a) (WF (σ-1 ∙ s,a),a))), =g(F(σ-1s,a).
Moreover, one derives that g(F (σ-1s,a))=	K(a)g(σ-1s), K(a) U-1(a) U(a) g(σ-1s), '	V	} =id K(a) U-1(a) .(σ-1s, a), K(a)(u-1(a) (σ郎(s, a))), K(a)(u-1(a) (σ*(u(a)g(S)))), [K(a) (σa 余g)](S),
which concludes the proof of the first part of the theorem. Let us next show the converse implication
i.e. u=. For this case it is practical to use the discrete time-system notation explicitly. Let σ ∈ Σ be
a symmetry of the state-space and be s t g(st+ι)	=σ ∙ St and Gt+1 = σ ∙ st+ι the σ-shifted states. Then =g(F(st, at)), =K(at) g(st), =K(at) g(σ-1 ∙ G t), =K(at)(σa^g(Gt)), =[K(at)(σa余g) (Gt), =	卜a*(K(at)g)] (Gt).
Thus in particular
σ-11g(st+ι)=卜-1*(σ0*(K3)g))i (Gt) Lem=a3.3 [K(at)g] (Gt),
15
Under review as a conference paper at ICLR 2022
where in (I) we have used that the symmetry operator commutes with the Koopman operator. More-
over, one finds that
σ-1*g(st+ι) = g((σ-1)T ∙ St+1)= g(σ ∙ St+1)= g(st+ι),
from which one concludes that
g(st+ι) = [K(αt)g](st) = g(F(St,at)).
Finally, we use the invertibility of the Koopman space observables i.e. g-1(g(s)) = s to infer
st+ι = F(St,at).
However, in general s the Koopman space observables are not invertible globally. The "inverse
function theorem" guarantees the existence of a local inverse if g(s) is C1 differentiable for maps
between manifolds of equal dimensions. However, we assume inevitability locally.
st+1 = σ ∙ st+1 ⇒ F(st,αt)= σ ∙ F(st,at) ⇒ F(σ ∙ st,at)= σ ∙ F(st,at) ,	(23)
which at last concludes our proof of the second part of the theorem.
Extension of Theorem 3.5: Moreover, one may account for practical limitations i.e. an error by
the assumption [σa , K(at)] = a 1. One then finds that sSt+1 = F(sSt, at) + O(a). Thus the error
becomes suppressed when a . The error may be due to practical limitations of capturing the true
dynamics as well as the symmetry map.
Proof of Theorem 3.5: We will show here theorem 3.5 as well as the extension mentioned in the
paragraph prior to this proof. Let st+1 = F(st, at) be a Σ-action-equivariant control system as in
Theorem 3.4. For symmetry maps
St = D((1 + eσat)余E(St)) , St+1 = D((1 + eσat)余E(st+ι)) .	(24)
we aim to show that one can use a symmetry transformation to shift both st as well as st+1
σt: (St,st+ι,at) ——→ (st,st+ι,at), st st+ι = F(St,at).	(25)
From equation 24 and the definition of the Koopman operator one infers that6
St+ι = D( σat余E(st+ι)) = D( σa>K(at)E(st)) ∙	(26)
By using that [σat, K(at)] = 0 and equation 24 one finds that
St+1 = D( K(at)σat第E(St)) = D( K(at) E(D"余E(St)))) = D( K(at)E(St)) ,	(27)
=1
which concludes the proof of the first part of the theorem with σat → (1 + e σat).
Local diffeomorphism. Per definition the maps D, E are differentiable and invertible which implies
that they provide a local diffeomorphism from and to Koopman-space. Also the linear map i.e. a
matrix multiplication by (1 + e σat) is a diffeomorphism. Thus the symmetry map σat i.e. Eq. (15)
constitutes a local diffeomorphism. The above proof culminating in Eq. (24) implies that we have a
local diffeomorphism mapping solutions of the ODEs to solutions, thus a symmetry of the system of
ODEs.
Limitations. However D, E to be invertible is a strong assumption as it implies that the Koopman
space approximation admit the same dimension as the state space. Note that however in practice as
we only require D ◦ E ≈ id one may choose other Koopman space dimensions.
Local Lie group. What is left to show is that the symmetry of the system of ODEs locally is a Lie
group. In definition 2 we need to show points (1)-(3).
1.	For e = 0 one finds that σa0t is the identity map i.e for such that
St(St,0) = D((1 + 0 ∙ σaJE(St)) = D(E(St)) = St	(28)
6For notational simplicity we study the general case (1 + σat) → σat .
16
Under review as a conference paper at ICLR 2022
2.
σe1 σe2	= D((1 + eισθt)(1 + e2σaJE(st))	(29)
=D((I + e1σat + e2σat + O(e1 ∙ e2))E(St))
= D((I + (e1 + e2)σat + O(e1 ∙ e2))E(St))
= σ1+2
for every |e1|, |e2|	1.
3.	St(st, e) admits a Taylor series expansion in e, i.e. in a neighbourhood of S determined by
e = 0. We may Taylor expand D around the point E(St ) ≡ g as
N	∂D
St(St, e) = D(E(St))+ e ɪ3(σat E(St))I ∂g + O(e )	(30)
thus
SSt (St, e) = St + eζ(St) + O(e2)	(31)
for Z(St) = PI=ι(σatE(St))IdDI∙
4.	The existence of an inverse element (σa )-1 = σa- follows from (1) and (2).
Numerical errors. Let us next turn to the second part of the theorem incorporating for practical
numerical errors. We aim to show that under the assumption [σat , K(at)] = ea 1 one finds SSt+1 =
FS(SSt, at) + O(ea). Note that σat → (1 + e σat) admits an expansion in the parameter e. For e 1
one can Taylor expand the differentiable functions D, E to find that
St = St + δ(e) , With δ(e) = e Xgσ∂gJ∣E(st) + O(e2) ,	(32)
where gσ =。。,余E(St) and g = E(St), and with the index I = 1,..., N. The analog expression
holds for t + 1 i.e. SSt+1 = St+1 + O(e). If at = at(St) is differentiable function of the states7 one
may Taylor expand the Koopman operator using equation 32 as8
K(at(SSt)) = K(at(St)) + XX
δn(e)a0in(St)Ki = K(at(St)) + e ∆ + O(e2) .	(33)
in
where and ain(St) = ∂dat- L and n = 1,..., dim(St) and with
st,n
∆ = ∑∑∑gσ∂Dn∣E(stM(St)Ki .	(34)
i n I gI
However, by the definition of the symmetry map at(SSt) = at(St), thus the Koopman operators in
equation 33 match ∀e. By using the assumption on the error one infers from equation 26 that
st+1 = D((I + eσat)余K(at)E(St)) =	D((K(at) + ea 1) (1 + eσat)余E(St))
=D(K(at))(1 + eσat)余E(St) + e。E(St) + O&e))
=	D(K(at)E(SSt) + ea E(St) + O(eae)) ,	(35)
One may Taylor expand D by making use of ea	1 as
St+1 = D(K(at)E(St)) + ea X EI(St)Tj- I	+ O(eae)+ O(ea) .	(36)
一	∂gI ∣K(at)E(st)
7Although neural networks modeling the policies modeling the data-distribution contain non-differentiable
components they are differentiable almost everywhere.
8The reader may wonder about the explicit use of the index δn () expressing the dimension of the state.
it is necessary here although we have simply used st as an implicit vector without explicitly specifying any
components throughout the work.
17
Under review as a conference paper at ICLR 2022
Thus under the assumption that the numerical error is given by a violation of the commutation relation
one finds gι = F(St, at) + O(e。)where
Ca ^X EI (St)Tj-I	〜O(ea) .	(37)
G	∂gι IK (at )E(St)
By comparison with equation 32 one infers that the e-expansion is a good approximation to the real
dynamic if ea e. This concludes the proof of theorem 3.5. Let us note that the proof for state shifts
arising from a sum of symmetry transformations as
NN
st = D((I + XeIσIt%E(St)) , st+1 = D((I + XeIσIt%E(St+1)) .	(38)
I=1	I=1
is completely analogous, and an analogous theorem holds. We choose the number of symmetries to
run over the dimension of the Koopman latent space N as it is relevant for the KFC++ setup.
C Implementation details
The Koopman forward model The KFC algorithm requires pre-training of a Koopman forward
model F : S × A → S as
{ D(E(St)) = St	if c = 0 corresponds to a VAE.
Z	、
D	K0 + Pim=1 Ki at,i E(St) = St+1 if c = 1: forward prediction model.
(39)
where both of E and D are approximated by Multi-Layer-Perceptrons (MLP’s) and the bilinear
Koopman-space operator approximation are implemented by a single fully connected layer for
Ki=0,...,m, respectively. The model is trained on batches of the offline data-set tuples (St+1, St, at)
and optimized via the loss-function
l^2(F 1(st, at) , st+1) + Y2 b(F O(St + se,at) , st + Se) ,	(40)
where S is a normally distributed random sate vector shift with zero mean and a standard deviation
of 6 ∙ 10-2 where the /2-loss is the Huberloss. For training the KooPman forward model in Εq.(16)
we split the dataset in a randomly selected training and validation set with ratios 70%/30%. We then
looP over the dataset to generate the required symmetry transformations and extend the rePlay buffer
dataset with the latter according to Εq. (18) as
symmetries
KFC :	St, St+1, at,rt	7-→	St, St+1, at, rt, σat ,	(41)
KFC++ : (st,st+ι ,at ,rt) symmetries	(st,st+ι,at,rt,U (at),U (at)-1).
It is more economic to comPute the symmetry transformation as a Pre-Processing steP rather than at
runtime of the RL algorithm.
Let us reemPhasize a couPle of Points addressed already in the main text to make this section self-
contained. Note that we have built uPon the imPlementation of CQL (Kumar et al., 2020), which
is based on SAC (Haarnoja et al., 2018). We also use the same hyperparameters for all the
experiments as presented in the CQL and S4RL papers for baselines.
Koopman forward VAE-model: For the encoder as well as the decoder we choose a three layer
MLP architecture, with hidden-dimensions [512, 512, N] and [512, 512, m], resPectively. Where
N = 32 is the KooPman latent sPace dimension and m is the state sPace dimensions following our
notation from section 2. The activation functions are ReLU (Nair & Hinton, 2010). We use an ADAM
optimizer with learning rate 3 ∙ 10-4. We train the model for 75 epochs. The batch size is chosen to
be 256. Lastly, we emPloy a random normally distributed state-augmentation for the VAΕ training i.e.
S → S + Se with Se ∈ N(0, 1 ∙ 10-2).
Hyper-parameters KFC: We take over the hyper-parameter settings from the CQL paper (Kumar
et al., 2020) except for the fact that we do not use automatic entropy tuning of the policy optimisation
step Εq. (3) but instead a fixed value of α = 0.2. The remaining hyper-parameters of the conservative
18
Under review as a conference paper at ICLR 2022
Q-learning algorithm are as follows: Y = 0.99 and T = 5 ∙ 10-3 for the discount factor and target
smoothing coefficient, respectively. Moreover, the policy learning rate is 1 ∙ 10-4 and the value
function learning rate is 3 ∙ 10-4 for the ADAM optimizers. We enable Lagrange training for a with
threshold of 10.0 and learning rate of 3 ∙ 10-4 where the optimisation is done by an ADAM optimizer.
The minimum Q-weight is set to 10.0 and the number of random samples of the Q-minimizer is 10.
For more specifics on these quantities see (Kumar et al., 2020). The model architectures of the policy
as well as the value function are three layer MLP’s with hidden dimension 256 and ReLU -activation.
The batch size is chosen to be 256. Moreover, the algorithm performs behavioral cloning for the first
40k training-steps, i.e. time-steps in an online RL notation.
The KFC specific choices are as follows: the split of random to Koopman-symmetry state shifts
is 20/80 i.e. pK = 80%, whereas the option for the symmetry-type is either "Eigenspace" or
"Sylvester" referring to case (I) and (II) of Eq. (18), respectively. Regarding the random variables in
the symmetry transformation generation process, We choose 6i ∈ N(0,1 ∙ 10-4) for case (II) and
e ∈ N(0,5 ∙ 10-5) for case (I) in Eq. (18) after normalizing o& by its mean. While for the random
shift we use e ∈ N(0, 3 ∙ 10-3) which is the hyper-parameter used in (Sinha et al., 2021).
Computational setup: We performed the empirical experiments on a system with PyTorch 1.9.0a
(Paszke et al., 2019) The hardware was as follows: NVIDIA DGX-2 with 16 V100 GPUs and 96
cores of Intel(R) Xeon(R) Platinum 8168 CPUs and NVIDIA DGX-1 with 8 A100 GPUs with 80
cores of Intel(R) Xeon(R) E5-2698 v4 CPUs. The models are trained on a single V100 or A100 GPU.
D Ablation study
Task Name	S4RL-(N)	KFC	KFC++	KFC++-contact
cheetah-random	523	48.6	49.2	49.4
cheetah-medium	48.8	55.9	59.1	61.4
cheetah-medium-replay	51.4	58.1	58.9	59.3
cheetah-medium-expert	79.0	79.9	79.8	79.8
hopper-random	108	10.4	10.7	10.7
hopper-medium	78.9	90.6	94.2	95.0
hopper-medium-replay	35.4	48.6	49.0	49.1
hopper-medium-expert	113.5	121.0	125.5	129
walker-random	24.9	19.1	17.6	18.3
walker-medium	93.6	102.1	108.0	108.3
walker-medium-replay	30.3	48.0	46.1	48.5
walker-medium-expert	112.2	114.0	115	118.1
Table 2: We study the effect of combining S4RL-N based augmentation training on “contact” event transitions
(state transitions where the agent makes contact with a surface, read D.1 for more details), and KFC++ aug-
mentation training on non-“contact” events. We see that KFC++-contact performs similarly and in most cases,
slightly better than the KFC++ baseline, which is expected. We experiment with the Open AI gym subset of the
D4RL tasks and report the mean normalized returns over 5 random seeds.
D. 1 Training on non contact events
To test the theoretical limitations in a practical setting one interesting experiment is to use the S4RL-
N augmentation scheme when the current state is a “contact” state and to choose KFC++ otherwise.
We define a “contact” state as one where the agent makes contact with another surface; in practice
we looked at the state dimensions which characterize the velocity of the agent parts, and if there is a
sign difference between st and st+1, then contact with a surface must have been made. The details
regarding which dimension maps to agent limb velocities is available in the official implementation
of the Open AI Gym suite (Brockman et al., 2016).
Note that the theoretical limitations are twofold, firstly the theorems 3.4 and 3.5 only hold for
dynamical systems with differentiable state-transitions; secondly, we employ a Bilinearisation Ansatz
for the Koopman operator. Contact events lead to non-continuous state transitions and moreover
19
Under review as a conference paper at ICLR 2022
would require incorporating for external forces in our Koopman description, thus both theoretical
assumptions are practically violated in our main empirical evaluations of KFC and KFC++. By
performing this ablation study which is more suitable for our Ansatz, i.e. the KFC++ part is
only trained on "differentiable" trajectories without contact events. In other words, the Koopman
description is unlikely to give good information regarding st+1 if there is a “contact event”, we
simply use S4RL-N when training the policies, which we expect to give slightly better performance
than the KFC++ baseline.
We report the results in Table 2, where we perform results on the OpenAI Gym subset of the D4RL
benchmark test suite. We denote the proposed scheme as KFC++-contact and compare to both
S4RL-N and KFC++. We see that KFC++-contact performs comparably and slightly better in
some instances, compared to KFC++. This is expected as contact events may be rare, or the data
augmentation given by the KFC++ model during contact events is pseudo-random and itself is
normally distributed, which may suggest that KFC++ behaves similarly to KFC++-contact. However,
we do not say this conclusively.
D.2 Magnitude of KFC augmentation
To further try to understand the effect of using KFC for guiding data augmentation, we measure the
mean L2-norm of the strength of the augmentation during training. We measured the mean L2-norm
between the original state and the augmented state, and found the distance to be 2.6 × 10-4 . For
comparison, S4RL uses a fixed magnitude of 1 × 10-4 (Sinha et al., 2021) for all experiments with
S4RL-Adv. Since S4RL-Adv does a gradient ascent step towards the direction with the highest
change, its reasonable for the magnitude of the step to be smaller in comparison. An adversarial step
too large may be detrimental to training as the step can be too strong. KFC minimizes the need for
making such a hyperparameter search decision.
D.3 Symmetries
In this section we provide a quantitative analysis of the symmetry shifts generated by KFC++ in
comparison to random shifts of the latent states. For the latter we choose the variance such that
absolute state shift
δs := |St - st| + |St+i - st+l∖	(42)
averaged over all sampled data points (D4RL) is comparable between the two setups. Moreover we
simply use the encoder We then compare how well the shifted states are in alignment with the actual
dynamic using the online Mujoco environment .9 To do so we set the environment state to the value
St and use the action at to perform the environment step which We denote by st+ι = M(st, at)10
The performance metric is then given by
∆E := ∖st+1 - M(sSt, at)∖ .	(43)
Note that ∆E is simply the error to the actual dynamic of the environment. We take the model trained
on the hopper-medium-expert dataset from section appendix D.1 to compute the symmetry shift.
Moreover, for the comparison in equation 43 we split the state space into positions and velocities
of the state vector, i.e. the first five elements of st+1 - M(sSt, at) give position while the remaining
ones given velocities. The norm is taken subsequently. For the results see figure D.3.
We conclude that there is a qualitative difference between the distributions obtained. Most notable
the symmetry shift allows for a controlled shift by a larger ∆S while still maintaining a small error
∆E. This amounts to a wide but accurate exploration of the environments state-space. Although we
cannot say this conclusively we expect this to be the reason for the performance gains of KFC++ over
random shifts.
D.4 Prediction Model
As a further ablation study it is of interest to compare the symmetry induced state shifts tuple
(sSt, sSt+1) to one obtained by the forward prediction of our VAE-forward model equation 16.
9Note that this requires a minor modification of the environment code by a function which allows to set states
to specific values.
10M (st , at ) denotes the step done by the active gym environment.
20
Under review as a conference paper at ICLR 2022
(a) Hopper position state shifts
(b) Hopper velocity state shifts
Figure 3: Symmetry shifts vs. random latent space shifts of same magnitude in the mean of ∆S compared by
its accuracy in the online evaluation in the gym Hopper-v3 environment. The first line shows the result where st
and st+1 are shifted by the same N-dimensional random variable. For the ablation study in line two we sample
twice from the random variable for the shifts st of st+1 , respectively.
KFC++prediction: In particular We USe the forward prediction on St obtained by the KFC++ shift,
i.e. st+1 = Fc=1 (st, at). We refer to this setup in the following as KFC++prediction. See table 3
for the results.
Fwd-prediction: Moreover, we study the case where we sSt is obtained by a shift with a normally
distributed random variable N(0, 6 ∙ 10-3) and then simply forward predict as St+1 = Fc=1 (St, at).
This is comparable to conventional model based approaches employing our simple VAE forward
model. This constitutes an ideal systematic comparison as the model, hyper-parameters and training
procedure are identical to the one used in the KFC variants. Moreover, the variance of the random
variable is such that the distance to the augmented states is comparable to the ones in KFC, see
appendix D.2. See table 4 for the results.
We conclude that KFC++prediction falls behind both KFC as well as KFC++. This was expected
as the symmetry shift in the latter alters the original data points by means of the VAE which admits
not only a much higher accuracy but also advanced generalisation capabilities to out of distribution
values.
More interesting however is that Fwd-prediction falls behind both KFC as well as KFC++ as
well as KFC++prediction. This is a strong indicator that the symmetry shifts provide superior
out-of-distribution data for training a Q-learning algorithm.
D.5 Discussion
Although our KFC framework is suited best for the description of environments described by control
dynamical systems the learned (in self-supervised manner) linear Koopman latent space representation
may be applicable to a much wider set of tasks. However, there are notable shortcomings to the
current implementation both conceptually as well as practically. The bilinearisation in Eq. (16) of the
latent space theoretically assumes the dynamical system to be governed by Eq. (8), which is rather
restrictive. Although a general non-affine control system admits a bilinearisation (Brunton et al., 2021)
21
Under review as a conference paper at ICLR 2022
Task Name	KFC	KFC++	KFC++-prediction
cheetah-random	48.6	49.2	46.5
cheetah-medium	55.9	59.1	53.7
cheetah-medium-replay	58.1	58.9	55.3
cheetah-medium-expert	79.9	79.8	76.3
hopper-random	10.4	10.7	10.8
hopper-medium	90.6	94.2	90.5
hopper-medium-replay	48.6	49.0	44.2
hopper-medium-expert	121.0	125.5	121.2
walker-random	19.1	17.6	15.6
walker-medium	102.1	108.0	105.3
walker-medium-replay	48.0	46.1	45.2
walker-medium-expert	114.0	115.6	114.5
Table 3: Results with prediction model KFC++-prediction on the Open AI Gym subset of the D4RL tasks. We
report the mean normalized episodic rewards over 5 random seeds similar to the original D4RL paper Fu et al.
(2021).
Domain	Task Name	KFC KFC++	FWd-Prediction
	antmaze-umaze	96.9	99.8	92.7
	antmaze-umaze-diverse	91.2	91.1	90.1
Ar∣tK∕f fl7P ntaze	antmaze-medium-play	60.0	63.1	60.8
	antmaze-medium-diverse	87.1	90.5	88.0
	antmaze-large-play	24.8	25.6	23.1
	antmaze-large-diverse	33.1	34.0	29.3
	cheetah-random	48.6	49.2	50
	cheetah-medium	55.9	59.1	50.1
	cheetah-medium-replay	58.1	58.9	56.4
	cheetah-medium-expert	79.9	79.8	71.3
	hopper-random	10.4	10.7	10.4
∏∙∖7TΠ	hopper-medium	90.6	94.2	82.3
ym	hopper-medium-replay	48.6	49.0	40.8
	hopper-medium-expert	121.0	125.5	120.3
	walker-random	19.1	17.6	18.4
	walker-medium	102.1	108.0	103.2
	walker-medium-replay	48.0	46.1	41.7
	walker-medium-expert	114.0	115.3	111.8
	pen-human	61.3	60.0	49.4
	pen-cloned	71.3	68.4	50.2
	hammer-human	7.0	9.4	6.1
ʌ z-l-⅛*o-g ÷ Adroit	hammer-cloned	3.0	4.2	4.2
	door-human	44.1	46.1	41.8
	door-cloned	3.6	5.6	1.2
	relocate-human	0.2	0.2	0.2
	relocate-cloned	-0.1	-0.1	-0.1
Prj⅞nl√^5∣	kitchen-complete	94.1	94.9	90.0
rana	kitchen-partial	92.3	95.9	84.6
Table 4: Results with prediction model Fwd-prediction on the D4RL tasks. We report the mean normalized
episodic rewards over 5 random seeds similar to the original D4RL paper Fu et al. (2021).
it generically requires the observables to depend on the action-space variables implicitly. Secondly,
the Koopman operator formalism is theoretically defined by its action on an infinite dimensional
22
Under review as a conference paper at ICLR 2022
observable space. The finite-dimensional approximation i.e. the latent space representation of the
Koopman forward model in Eq. (16) lacks accuracy due to that. On the practical side our formalism
requires data pre-processing which is computationally expensive, i.e. solving the Sylvester or
eigenvalue problem for every data-point. Moreover, the Koopman forward model in Eq. (16) serves as
a function approximation to two distinct tasks. Thus one faces a twofold accuracy vs. over-estimation
problem which needs to be balanced. The systematic error in the VAE directly imprints itself on the
state-data shift in Eqs. (15) and (18) and may thus conceal any potential benefit of the symmetry
considerations. Lastly, the dynamical symmetries do not infer the reward. Thus the underlying
working assumption is that the reward should not vary much in Eq. (15).
A note on the simplicity of the current algorithm: Let us stress a crucial point. Algorithmically
the symmetry maps are derived in two distinct ways KFC and KFC++. The latter, constitute a simple
starting point to extract symmetries from our setup. More elaborate studies employing the extended
literate on Koopman spectral analysis are desirable. Moreover, it is desirable to extend our framework
to more complex latent space descriptions such as e.g. world models. Both on a theoretical as well
as a practical level. It is our opinion that by doing so there is significant room for improvement
both in the accuracy of the derived symmetry transformations as well their induced performance
gains of Q-learning algorithms. Note that currently our VAE model is of very simple nature and the
symmetries are extracted in a rather uneducated way. While the Sylvester algorithm simply converges
to one out of many symmetry transformations for the KFC++ algorithm we omit all the information
of the imaginary part, let alone utilize concrete spectral information.
23