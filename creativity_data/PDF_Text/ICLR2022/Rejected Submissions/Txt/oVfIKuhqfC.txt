Under review as a conference paper at ICLR 2022
Non-Denoising Forward-Time Diffusions
Anonymous authors
Paper under double-blind review
Ab stract
The scope of this paper is generative modeling through diffusion processes. An
approach falling within this paradigm is the work of Song et al. (2021), which
relies on a time-reversal argument to construct a diffusion process targeting the
desired data distribution. We show that the time-reversal argument, common to all
denoising diffusion probabilistic modeling proposals, is not necessary. We obtain
diffusion processes targeting the desired data distribution by taking appropriate
mixtures of diffusion bridges. The resulting transport is exact by construction,
allows for greater flexibility in choosing the dynamics of the underlying diffusion,
and can be approximated by means of a neural network via novel training objectives.
We develop a unifying view of the drift adjustments corresponding to our and
to time-reversal approaches and make use of this representation to inspect the
inner workings of diffusion-based generative models. Finally, we leverage on
scalable simulation and inference techniques common in spatial statistics to move
beyond fully factorial distributions in the underlying diffusion dynamics. The
methodological advances contained in this work contribute toward establishing a
general framework for generative modeling based on diffusion processes.
1	Introduction
Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020;
Song et al., 2021) is a recent generative modeling paradigm exhibiting strong empirical performance.
Consider a dataset of N samples D = {x(n) }nN=1 with empirical distribution PD. The unifying
key steps underlying DDPM approaches are: (i) the definition of a stochastic process with initial
distribution PD , whose forward-time (noising) dynamics progressively transform PD toward a simple
data-independent distribution PZ ; (ii) the derivation of the backward-time (denoising / sampling)
dynamics transforming PZ toward PD ; (iii) the approximation of the backward-time transitions by
means of a neural network. Following the training step (iii), a sample whose distribution approximates
PD is drawn by (iv) simulating from the approximated backward-time transitions starting with a
sample from PZ. Both discrete-time (Ho et al., 2020) and continuous-time (Song et al., 2021)
formulations of DDPM have been pursued. This work focuses on the latter case, to which we refer
as diffusion time-reversal transport (DTRT). As in DTRT, dynamics are specified through diffusion
processes, i.e. solutions to stochastic differential equations (SDE) with associated drift f (∙) and
diffusion g(∙) coefficients. A number of approximations are involved in the aforementioned steps.
Firstly, as the dynamics are defined on a finite time interval, a dependency from PD is retained
through the noising process. Hence, starting with a sample from the data-independent distribution
PZ in (iv) introduces an approximation. Secondly, while the backward-time dynamics of (ii) are
directly available for diffusions, they are approximated by means of a neural network in (iii). Thirdly,
sampling in (iv) is achieved through a discretization on a time-grid, which introduces a discretization
error. De Bortoli et al. (2021, Theorem 1) links these approximations to the total variation distance
between the distribution of the generated samples from (iv) and PD .
In our first methodological contribution we develop a procedure for constructing diffusion processes
targeting PD without relying on time-reversal arguments. The proposed transport (coupling) between
PZ and PD is achieved by: (1) specifying a diffusion process X on [0, τ] starting from a generic x0 ;
(2) conditioning X on hitting a generic xτ at time τ , thus obtaining a diffusion bridge; (3) taking a
bivariate mixture Π0,τ of diffusion bridges over (x0, xτ) with marginals Π0 = PZ and Πτ = PD,
obtaining a mixture process M ; (4) matching the marginal distribution of M over [0, τ] with a
diffusion process, resulting in a diffusion with initial distribution PZ and terminal distribution PD .
1
Under review as a conference paper at ICLR 2022
The realized diffusion bridge mixture transport (DBMT) between PZ and PD is exact by construction.
We thus sidestep the approximation common to all DDPM approaches due to the dependency from
PD retained through the noising process. Moreover, the DBMT can be realized for almost arbitrary
PZ, f (∙) and g( ∙). This increased flexibility is a departure from the DTRT where f (∙) and g(∙)
need to be chosen to obtain convergence toward a simple distribution PZ .
Similarly to the DTRT, achieving the DBMT requires the computation of a drift adjustment term
which depends on D. For a SDE class of interest, we develop a unified and interpretable representation
of DTRT and DBMT drift adjustments as simple transformations of conditional expectations over
D. This novel result provides insights on the target mapping that we aim to approximate and on
the quality of approximation achieved by the trained score models of Song et al. (2021). Having
defined for the DBMT a Fisher divergence objective similarly to Song et al. (2021), we leverage on
this unified representation to define two additional training objectives featuring appealing properties.
In our last methodological contribution we extend the class of SDEs that can be realistically employed
in computer vision applications. Specifically, computational considerations have so far restricted the
transitions of the stochastic processes employed in DDPM to be fully factorials. We view images
at a given resolution as taking values over a 2D lattice which discretizes the continuous coordinate
system [0, 1]2 representing heights and widths. Diffusion processes are viewed as spatio-temporal
processes with spatial support [0, 1]2. Doing so, it is possible to leverage on scalable simulation and
inference techniques from spatial statistics and consider more realistic diffusion transitions.
This paper is structured as follows. In Section 2 we review the DTRT of Song et al. (2021) and in
Section 3 we introduce the DBMT. In order to implement the DTRT and the DBMT it is necessary
to specify the underlying SDE, i.e. the coefficients f (∙) and g(∙). We study a class of interest in
Section 4. The unified view of drift adjustments is introduced in Section 5. Section 6 develops the
training objectives and Section 7 reviews the obtained results and finalizes the DBMT construction.
In Section 8 we establish the connection with spatio-temporal processes. We conclude in Section 9.
Appendices A to D contain the theoretical framework, assumptions, proofs, and additional material.
Notation and conventions: we use uppercase notation for probability distributions (measures, laws)
and lowercase notation for densities; each probability distribution, and corresponding density, is
uniquely identified by its associated letter not by its arguments (which are muted); for example
P (dx) is a distribution, p(x) is its corresponding density; random elements are always uppercase (an
exception is made for times, always lowercase for typographical reasons); if P is the distribution of a
stochastic process, we use subscript notation to refer to its finite dimensional distributions (densities
with p), conditional or not, for some collection of times; for example pt0 |t denotes a transition density,
which is understood to be a function of four arguments pt0|t(y|x) = f(t, t0, x, y); δx is the delta
distribution at X and 0 is used for product distributions; we refer directly to a given SDE instead of
referring to the diffusion process satisfying such SDE when no ambiguity arises; we use [a]i and
>
[ ]i,j for vector and matrix indexing, for matrix transposition.
2	Diffusion Time-Reversal Transport
The starting point of Song et al. (2021) is a diffusion process Y satisfying a generic D-dimensional
time-inhomogenous SDE with initial distribution Y0 〜 PD
dYr = f(Yr,r)dr +g(Yr,r)dWr,	(1)
over noising time r ∈ [0, τ]. Thorough this paper we denote with Q the law of the diffusion solving
(1) and with q the corresponding densities. Thus, let qr0|r(y|x), 0 ≤ r < r0 ≤ τ, be the transition
density of (1), and let q『(y), 0 < r ≤ T, be the marginal density of (1). As Y0 〜 PD, we have
1N
qr(y) = N ]qri0(y|x(n)).	(2)
The dynamics of (1) over the reversed, i.e. sampling, time t = τ - r, t ∈ [0, τ], are given by
(Anderson, 1982; Haussmann & Pardoux, 1986; Millet et al., 1989)
dXt = [-f (Xt, r) + ▽• G(Xt, r) + G(Xt, r) VXt ln qr(Xt)] dt + g(Xt, r)dWt,	(3)
where r = τ - t is the remaining sampling time, G(x, r) = g(x, r)g(x, r)> and the D-dimensional
vector V ∙ G(x, r) is defined by [V ∙ G(x, r)]i = PD=ι Vxj [G(x, r)]i,j. That is the processes Xt
2
Under review as a conference paper at ICLR 2022
and Yr = Yτ-t have the same distribution. Approximating the terminal distribution Qτ of (1), i.e.
the initial distribution of (3), with PZ, X0 is sampled from PZ and (3) is discretized and integrated
over t to produce a sample Xτ approximately distributed as PD .
The computation of the multiplicative drift adjustment Vy ln q『(y) entering (3), i.e. the score of the
marginal density (2), requires in principle O(N) operations. Let sφ(y, r) be a neural network for
which we would like sφ(y, r) ≈ Vy ln qr(y). It remains to find a suitable training objective for which
unbiased gradients with respect to φ can be obtained at O(1) cost with respect to the dataset size N.
As qr(y) has a mixture representation, the identity of Vincent (2011) for Fisher divergences provides
us with the desired objective for a fixed r ∈ (0, τ]
LFD,DTRT(φ, r) = E	VYr lnqr(Yr) - sφ(Yr, r)2
E	VYr ln qr|0(Yr|Y0) - sφ(Yr, r)2 .
(YO ,Yr )〜Q0,r
(4)
The key point is that an unbiased, O(1) with respect to N, mini-batch Monte Carlo (MC) estimator
for the expectation (4) can be trivially obtained by sampling a batch Y0 〜PD, Yr 〜Qr∣o(dyr |Y0),
and evaluating the average loss over the batch. In order to achieve a global approximation over the
whole time interval (0, τ], Song et al. (2021) proposes uniform sampling of time r
LFD,DTRT(φ) =	E	Rr VYr ln qr|0(Yr|Y0) - sφ(Yr, r)2 ,	(5)
fv, c , / / I I I	I v* C v* 1 c , / ) c I
r 〜U(0,τ ],(Yo,Yr)〜Qo,r L 11
where Rr = E[kVYr ln qr|0(Yr |Y0)k2]-1 is a regularization term. A MC estimator for (5) is con-
structed by augmenting the MC estimator for (4) with the additional sampling step r 〜U(0,τ].
3	Diffusion B ridge Mixture Transport
Our starting point is a generic D-dimensional time-inhomogenous SDE which, in contrast to Song
et al. (2021), is directly defined on the sampling time t ∈ [0, τ]
dXt = f (Xt, t)dt + g(Xt, t)dWt.
(6)
We reserve P∙∣0( ∙∣χo) to denote the law of the diffusion solving (6) for a given starting value xo and
P∙∣0( ∙∣χ0) to denote the corresponding densities.
3.1	Diffusion Bridges
Diffusion bridges are central to the proposed methodology, in this Section we cover their basic theory.
A diffusion bridge is a diffusion process starting from a given value which is conditioned on hitting a
terminal value. It is a deep result, and consequence of Doob h-transforms (Sarkka & Solin (2019,
Chapter 7.9), Rogers & Williams (2000, Chapter IV.6.39)), that a diffusion processes pinned down on
both ends is still a diffusion process. In particular the Markov property is preserved. More precisely,
(6) with initial value x0 conditioned on hitting a terminal value xτ at time τ is characterized the
following SDE on [0, τ] with initial value x0 (Sarkka & Solin, 2019, Theorem 7.11)
dXt= f(Xt, t) + G(Xt, t) VXtln PT ∣t(xτ Xt)] dt + g(Xt, t)dW,
(7)
where G(x,t) = g(x,t)g(x,t)>. The multiplicative adjustment factor VxtlnPT∣t(x"xt) forces the
process to hit xτ at time τ and the diffusion process solving (7) is known as the diffusion bridge from
(x0, 0) to (xT, τ). As previously noted, PT|t(xT|xt) in (7) refers to the transition density of (6).
3.2	Diffusion Mixtures
The proposed transport construction relies on a representation result for diffusion mixtures. We
present here an informal version and report the precise statement, the required assumptions, and the
proof in Appendix A.
Theorem 1 (Diffusion mixture representation — informal). Let {Xλ}, λ ∈ Λ be a collection of
diffusions with associated SDEs {dXtλ} and marginal densities {πtλ}. Let L be a mixing distribution
on Λ, πt be the L-mixture of {πtλ}. Then there exists a diffusion process X with marginal πt. X
follows a SDE whose drift and diffusion coefficients are weighted averages of the corresponding
coefficients in {dXtλ}, where the weights are proportional to {πtλ} and to the mixing density.
3
Under review as a conference paper at ICLR 2022
Theorem 1 is first established in Brigo (2002, Corollary 1.3) limitedly to finite mixtures and 1-
dimensional diffusions. The proof of Theorem 1 in Appendix A is more direct and extends the result
to the required multivariate setting. In Section 3.1 we introduced diffusion bridges mapping arbitrary
initial values x0 to arbitrary final values xτ . Let Π0,τ denote a generic bivariate distribution on
RD × RD with marginals Π0 , Πτ . We define the diffusion mixture M as the mixture of diffusion
bridges corresponding to (X0,Xτ)〜∏o,τ. That is, We apply Theorem 1 to the collection of
diffusion bridges (7) indexed by their initial and terminal values, λ = (x0, xτ), Λ = RD × RD, with
mixing distribution L(dλ) = Π0,τ (dx0, dxτ). By Theorem 1 the folloWing SDE on [0, τ] With initial
distribution Π0 has the same marginal distribution as M , in particular its terminal distribution is Πτ
dXt = μ(Xt ,t)dt + g(Xt,t)dWt,
μ(χt,t) = f (χt,t) + G(χt,t) /Nxt lnPT∣t(χτ∣χt)
Pt∣0,τ(xt∣X0,Xτ)
πt(xt)
{z'''∙∕'^^
A(xt,t)
Π0,τ (dx0, dxτ),
---------------}	(8)
πt(xt)
/ Pt∣0,τ (xt∣X0,Xτ )∏0,τ (dxo,dxτ).
In (8), A(xt, t) gives the multiplicative drift adjustment factor for (6). A case of particular interest
occurs When Π0 puts all the mass on a single value x0. In the folloWing We refer to A(xt, t, x0)
in stance of A(χt,t), and to ∏t∣o(xt∣xo) in stance of ∏t(χt), When it is necessary to distinguish
this specific case. We also extend the scope of Π to indicate the laW of M . Indeed, We already
denoted With Π0,τ its initial-terminal distribution, and With πt its marginal density. Accordingly,
A(χt,t) = EXT〜∏(dxτ∣xt)[Vxt lnPτ∣t(Xτ∣xt)]. The transport from PZ to PD is then achieved by
Πτ = PD and PZ = Π0 (PZ can be arbitrarily defined). As PD is an empirical distribution, the
integral in (8) With respect to xτ reduces to averages over D. In summary, the diffusion X solution
of (8) realizes the proposed transport from PZ to PD by matching the marginal distribution of M .
4	SDE CLASS
The starting point of the proposed transport is the unconstrained SDE (6). In this Section We define
SDEs Which are realized through a time-change of simpler SDEs and Which are general enough to
subsume the SDEs introduced in Song et al. (2021). Consider the D-dimensional SDEs
dZt = r1/2dWt,	(9)
dZt = at Ztdt + r1/2dWt,	(10)
Where αt 6= 0 is a scalar function and G(Xt, t) = Γ introduces an arbitrary covariance structure. (9)
is the SDE of a correlated and scaled BroWnian motion and (10) is the SDE of an Ornstein-Uhlenbeck
process driven by a correlated and scaled BroWnian motion. The transition densities of (9) and (10)
are Gaussian (Appendix B). We denote both Withpet0|t, informally (9) is a special case of (10) With
αt = 0. SDE (10) in the time-homogenous case αt = -1/2 has stationary distribution ND (0, Γ). We
noW introduce the time-change. Let βt > 0 be a continuous function on [0, τ]. Then bt = R0t βudu
defines a monotonically (strictly) increasing function bt : [0, τ] → [0, bτ]. The folloWing SDEs on
[0, τ ] represent the class of dynamics for (1) and (6) on Which We focus on the rest of this paper
dXt = PβtΓ1/2 dWt,	(11)
dXt = αtβtXtdt + Petr1/2dWt,	(12)
and G(x, t) = βtΓ. The standard time-change result for diffusions (0ksendal, 2003, Theorem 8.5.1)
establishes that the processes Xt respectively from (11) and (12) are equivalent in laW to their time-
scaled counterparts Zbt from (9) and (10). That is, SDEs (11) and (12) correspond to the evolution of
the simpler SDEs (9) and (10) under a non-linear time Wrapping Where time floWs With instantaneous
intensity βt. For both (11) and (12) the time-change argument yields Pτ∣t(y|x) = Pbτ∣bt (y|x) for the
transition density of (6), and equivalently for the transition density qτ∣t of (1). We thus obtain
Pτ∣t(xτ|xt) = ND(xτ； xta(t,τ), Γv(t,τ))	(13)
for appropriate scalar functions a(t, τ), v(t, τ) With v(t, τ) > 0 (Appendix B). By direct computation
Vxtln PTIt(XTwt) = γ-1( Uy -Xt) a⅛⅛,	(14)
4
Under review as a conference paper at ICLR 2022
Vχτ lnPT∣t(xτ∣Xt) = Γ-1 (xta(t,τ) - x「)。(1 T).	(15)
From Bayes theorem and the Markov property we have
Pt∣0,τ (xt∣X0, Xτ ) = ND (xt； xoabr(0, t,T) + Xτ αbr(0, t,T), Γvbr(0, t,T)) ,	(16)
where once again abr(0,t,τ), αbr(0,t, τ) and vbr(0,t,τ) > 0 are scalar functions given in AP-
pendix B. Finally, by direct computation
-1 xθαbr(0, t,τ) + xτabr(0, t,τ) - Xt
Nxt ln pt∣0,τ (xt |x0, xτ ) ——γ	V (0 t T)	.	(17)
These results Provide all the analytical formulas required for the comPutation of the adjustment
factors A(xt, t), A(xt, t, x0) and of the training objectives used to aPProximate them (Section 6).
4.1	Interpretation of Denoising Time-Reversed SDEs
Song et al. (2021) introduces two sPecifications of (1), named VESDE and VPSDE, which are
resPectively given by
dYr = Pβve7 dWr,	(18)
dYr = - 2βvp,r Yr dr + Pβvp,r dWr ∙	(19)
See APPendix B for the functional form of βve,r and βvp,r. We thus recover (18) and (19) from (11)
and (12) with Γ = I and αt = -1/2. That is, VESDE and VPSDE corresPond to a time change of
the much simPler SDEs for the standard Brownian motion and for the standard Langevin SDE
dZr = dWr ,
dZr = — ；Zr dr + dWr.
5	Unified View of Drift Adjustments
The linearity of SDEs (11) and (12), underlying our and Song et al. (2021) works, has the imPortant
consequence that (14) and (15) are linear in xt . This in turn allow us to derive an alternative
rePresentation for the drift adjustment in (8). Indeed, substituting (14) in (8) gives (APPendix A)
G(X,t)A(χ,t) = βt (ɑɪ) XT 〜∏5 (dxτ∣χ)Xτ]- x) v(tTτ).
Similarly, for the time-reversal drift adjustment term in (3) we have (APPendix A)
G(X,r) Vxln qr (x) =βr 卜((V) Xτ→JE (dxτ∣x)[Xτ]―x) v⅛).
(20)
(21)
The relations (20) and (21) Provide a unified view of the inner workings of the DTRT and of the
DBMT targeting PD . In the following we always refer to samPling time t. Remember that r = T - t
is the remaining samPling time. For ease of exPosition we assume βt =1, as shown in Section 4 the
term βt corresPonds to a time-warPing. The terms a(t, T), a(0, r) are “integrated scalings”. They are
equal to 1 for (11) and the same holds for (12) as r → 0. The terms v(t, T), v(0, r) are “integrated
variances”. They are equal to r for (11) and the same holds for (12) as r → 0. We commonly refer
to E [Xτ |X, t] for exPectation terms in (20) and (21). Both drift adjustments (20) and (21) are thus
essentially of the form (E[X/|x, t] — X)V-I where the term v-1 diverges as r → 0.
The exPectations E [Xτ |X, t] are convex linear combinations of the samPles X(n) from D. ExPlicitly,
E[Xτ∣x,t] = PnN=I ω(x,t)(n)x(n), where the weights ω(x,t)(n) are the Probabilities, under the
distributions Q (time-reversal samPling Process (3)) and Π (mixture of diffusions Process M from
Section 3.2), of reaching each state X(n) at terminal time T from X at time t. By construction, the
initial weights entering exPectation (20) are all equal to 1/ N when X starts from a fixed value X0,
and are so on average when X0 is stochastic. The initial weights entering exPectation (21) are
5
Under review as a conference paper at ICLR 2022
on average approximately equal to 1/N, depending on the quality of the approximation PZ ≈ Qτ .
Thus, E[Xτ IX0,0] is an averaging of many samples x(n). AS time progresses, changes in Xt
correspond to changes in E[Xτ ∣Xt, t] through changes in the weights ω(x,t)(n). Eventually all
mass concentrates on a single weight ω(x, t)(*) corresponding to a dataset sample x(*). Ultimately,
the attractor dynamics implied by (E[Xτ ∣x, t] - x)v-1 drive Xt to x(*). We provide an inspection
in Figure 1, where D(CIFAR) stands for the training portion of the CIFAR10 dataset, and Euler(T)
corresponds to the Euler scheme (Kloeden & Platen, 1992) applied with T discretization steps.
In the VESDE and VPSDE of Song et al. (2021) we have G(x, r) = βrI and reversing (21) gives
E	[Xτ] =
XT 〜Qo∣r(dχτ |x)
v(0, r) Vx ln qr (x) + X
a(0, r)
(22)
where Vy lnqr(y) is the true score. We can thus take a trained score model sφ(x, r) ≈ Vx lnqr(y),
plug it in (22), and verity the extent to which E[Xτ ∣x, t] has been approximated, see Figure 1.
Figure 1: VPSDE model — 2nd cells’ row: evolution of a trajectory of X over sampling time (its
terminal value Xτ is the generated sample) via the Euler(1000) discretization of (3) using the true
score Vy lnqr(y) for D(CIFAR); line-plot: weights’ evolution ω(Xt, t)(n) for all x(n) in D(CIFAR)
for the same X (cyclical palette, many weights cannot be distinguished as they remain close to 0); 1st
cells, row: E[Xτ ∣Xt, t] evolution for the same X; 3rd and 4th cells, rows: same as 1st and 2nd cells’
rows for another trajectory X , using the trained score model; 5th and 6th cells’ rows: same as 3rd and
4th cells, rows for another trajectory X , using Euler(100).
We pause for a moment to review the findings of Figure 1 (see Appendix D for additional related
plots). Firstly, We can classify the dynamics of E[Xτ ∣Xt, t] and of the associated weights in three
stages. In the 1st stage the weights do not move much. During the 2nd stage, roughly t ∈ [0.4, 0.6],
the weights, mass gets distributed over a limited number of samples. Interestingly, the weights,
dynamics are not monotonic. As time progresses the weights, mass shifts between different objects
from different classes. From the beginning of the 3rd stage all mass gets allocated to a single weight,
the terminal image is decided well in advance of the terminal time. These dynamics are suboptimal.
We would like to shorten the 1st stage, but it is associated with large values of βt (i.e. quick time
passing) which are required to decouple Qτ from PD . This is an intrinsic limitation of time-reversal
approaches. Itis also dubious that (partially) sampling multiple objects over t is beneficial for efficient
generative modeling when we make use only of the terminal sample. This issue applies to trained
models as well, as the 3rd row of Figure 1 shows. An interesting open question is how to obtain more
suitable dynamics, where perhaps class transitions happen rarely. Secondly, E[Xτ ∣Xt, t] provides
a denoised representation of Xt across the whole 3rd stage. An alternative to the noise removal
step applied to Xτ in Song et al. (2021) is to consider E[Xτ ∣Xt, t] as the sampling process instead.
Thirdly, Figure 1 makes it clear that lowering the number of discretization steps affects generative
6
Under review as a conference paper at ICLR 2022
sampling in multiple ways. On the one hand the terminal sample Xτ is more noisy. This is not very
surprising: close to T the drift adjustment is approximately (x(*) - Xt)V-1, which is the drift of
a Brownian bridge. Bridge sampling is notoriously problematic (Bladt et al., 2016). On the other
hand larger discretization errors also significantly affect the dynamics of E Xr X ,t] resulting in
less coherent samples. We remark that none of these insights could have been gained by observing
Xt alone, i.e. the even cells’ rows of Figure 1. To conclude, (20) and (21) give an additional meaning
to “denoising”. Neural network approximators need to map from a noisy input Xt to an adjustment
toward a smoother superimposition of samples. The desire to minimize the discrepancy between the
smoothness properties of Xt and that of E[Xτ |Xt, t] motivates the developments of Section 8.
6 Transports Approximation
As in Song et al. (2021), computing the multiplicative drift adjustment A(xt, t) requires O(N)
operations. In this Section we introduce three training objectives for which unbiased and scalable, i.e.
O(1) with respect to N, MC estimators can be immediately derived.
The first training objective applies only to A(xt, t, x0). It relies on the identity (Appendix A)
A(xt,t,xo) = Nxt ln∏t∣o(xt∣xo) - Nxt lnPt∣o(xt∣xo).	(23)
It is advantageous to consider the right-hand side of (23) because from (8) we know that ∏t∣o(χt |xo)
has mixture representation. As in Song et al. (2021), we can rely on Vincent (2011) to obtain a
scalable objective to train a neural network approximator sφ(xt, t) ≈ Nxtln ∏t∣o(χt∣χo), i.e.
LFD,DBMT(φ) =	E	Jt
t~U(0,τ ),Xt~∏t∣o
=E
t 〜U(0,τ ),(Xt,Xτ)〜∏t,
NXtln∏t∣o(Xt∣xo) - Sφ(Xt,t)∣∣2]
τ∣0
[Jt∣∣ NXt ln pt∣0,τ(Xt∣xo,Xτ) - Sφ(Xt,t)∣∣2],	(24)
where Jt = E[kVXtlnPt∣o,τ(Xt∣χo, Xτ)k2]-1 is a regularization term.
The remaining training objectives rely on the identities (20) and (21). The goal is directly approximate
the expectations of (20) and (21) which, as in Section 5, we denote with a generic E[Xτ |x, t]. That is,
we aim to train a neural network approximator sφ(x, t) ≈ E[Xτ |x, t]. As conditional expectations
are mean squared error minimizers, suitable objectives for the expectation terms of (20) and (21) are
LCE,DBMT(φ) =	E	h∣∣Xτ - sφ(Xt,t)∣∣2i,	(25)
t〜U[0,τ),(Xt,Xτ )〜Πt,τ
LCE,DTRT(φ) =	E	h∣∣Y0 - sφ(Yr, r)∣∣2i.	(26)
r 〜U[0,τ ),(Y0,Yr)〜Qo,r
In Table 1 we summarize the operations needed to implement the plain MC estimators for the four
objectives considered in this work. We reference where to find the required quantities for SDEs
(i1) and (12). The MC estimators for the Fisher divergence losses Lfd,* involve multiplications
by Γ-1 (by (15) and (17)). Moreover, computing the drift adjustment at generation time requires
multiplications by Γ. In Section 8 we discuss how to manage the computational burden. An appealing
property of LCE,* is that computing the drift adjustment only requires the application of simple
scalar functions (see (20) and (21)), and that their MC estimators only requires sampling operations.
A further advantage of LCE,* is that no regularization is required. In contrast, in the absence of
regularization terms, LFD,* are divergent for t ≈ τ due to the term vr-1 (Section 5).
L	Sampling (r〜U(o,τ],t〜U[0,τ))	Evaluation
LFD,DTRT LFD,DBMT LCE,DTRT LCE,DBMT	Yo〜Pd, Yr〜Qr∣o(dyr∣Yo)(13) XT ~Pd, (X0=xθ), Xt~Pt∣0,τ (dxt ∣x0,Xτ )(16) Y0 ~Pd, Yr~Qr∣θ(dyr | Yo)(13) XT〜Pd, X0〜∏0∣τ(dx0∣Xτ), Xt〜Pt∣0,τ(dxt∣X0,Xτ)(16)	▽Yr ln qr∣0(Yr ∣K)(15) ▽Xt ln pt∣0,τ (Xt∣X0,Xτ )(17)
Table 1: Sampling and evaluation operations required to implement the proposed MC estimators.
7
Under review as a conference paper at ICLR 2022
7 DBMT Overview and Numerical Experiment
In this section we finalize the DBMT construction, putting together the results of Sections 3, 4 and 6.
The unconstrained SDE follows (11) or (12). It remains to choose the mixing distribution Π0,τ . The
marginal Πτ needs to match PD, but there is flexibility in the choice of Π0∣τ. Song et al. (2021)
derived a random ordinary differential equation (RODE) matching the marginal distribution of a
generative SDE, leading to faster sampling and to likelihood evaluation. RODE-matching requires
Π0 to have density. A natural implementation is given by the factorial distribution Π0,τ = PZ 名)PD
with PZ = ND (0, Γ) and the unconstrained SDE following (12) with αt = * 1/2 which preserves
PZ. If instead the DBMT starts from a fixed value x0, i.e. Π0,τ = δx0 0 PD, We can choose
x0 = 1/N PnN=1 x(n)a(0, τ)-1 to remove the drift adjustment at t = 0 (see (20)) and reduce the
Work required to transport x0 to PD . Finally, the use of non-factorial distributions can lead to a more
efficient implementation, by linking the initial distribution to PD .
The training steps for the simplest objective (25) of Section 6 are reported in Algorithm 1. Batch
size is assumed to be 1 to ease the description. It is also assumed that Π0,τ is factorial, otherWise
the obvious modification applies to line 2 (Algorithm 2 is unaffected) Where the endpoints are
sampled. At line 3 a random central time and the corresponding state are sampled. The function
optimizationstep implements a step of stochastic gradient descent update based on the loss L.
The corresponding sampling algorithm is reported in Algorithm 2 Where the Euler(T) discretization is
assumed in line 6. Pt∣0,τ(dxt ∣X0, XT), a(t, T), V(t, T), βt are defined in Section 4. Section 8 shows
how to sample efficiently from Pt∣0,τ(dxt |X0, Xτ) and ND(0, Γ) in computer vision applications.
Algorithm 1 DBMT training (LCE,DBMT)
Input: Pd, PZ, SDE (11中12), NN sφ(x,t)
Output: trained sφ (x, t)
1:	repeat
2:	Xτ 〜Pd, Xo 〜PZ
3:	t 〜U[0,τ), Xt 〜Pt∣o,τ(dxt|Xo,Xτ)
4:	LJnXT - sφ(Xt,t)Il
5:	φ J optimizationstep(φ,L)
6:	until convergence
Algorithm 2 DBMT sampling (LCE,DBMT)
Input: PZ, SDE (11) or (12), trained Sφ(x,t)
Output: Discretized path X0:T
1: Xo 〜Pz
2: for s = 1, . . . , T do
3: t J— (S — 1)T, X J- Xs-1
4: US J βt ( a(t,τ) sφ (X,t) - X) v(t,T)
5:	Es 〜ND(0, Γ)
6:	Xs J X +(f(x,t)+ uS ) T + g(x, t) p~τf Es
7: end for
We consider a toy numerical example with PZ = PD = 1∕3(δ-2 + δ0 + δ2), D = T = 1. The
unconstrained SDE follows the standard Brownian motion. We consider two mixing distributions:
independent mixing Πo⊥⊥,1 where Xo and X1 are independent and fully dependent mixing Πo=,1 where
Xo = X1 . The results are reported in Figure 2. For both couplings the correct terminal distribution
PD is recovered, as can be seen by taking the row-wise sum of the transition matrices. The initial-
terminal distribution of X solving (8), which realizes the DBMT, is different from the corresponding
mixing distribution Πo,1 which is realized by the mixture process M . Πo⊥⊥,1 results in a transition
matrix of equal entries 1∕9, Πo⊥⊥,1 results in a diagonal transition matrix of equal diagonal entries 1∕3.
3

0.24
0.08
0.01
0.07
0.19
0.08
0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0
0.01
0.08
0.25
-2.0	0.0	2.0
Xo
0.30
0.00
0.03
0.28
0.03
0.00
0.03
0.30
-2.0	0.0	2.0
Xo
2
0
Figure 2: (1st (Πo⊥⊥,1), 2nd (Πo=,1) plots): marginal density of the diffusion mixture M in yellow, which
matches the marginal density of X solving (8), 5 sample paths of X started at 0 in black; (3rd (Πo⊥⊥,1),
4th (Πo=,1) plots): transition matrix ofX from t = 0 to t = 1 estimated from 2000 samples.
8
Under review as a conference paper at ICLR 2022
8 Non-Denoising Diffusions
In computer vision applications, images of resolution H×W corresponds to D = 3HW. The use of
an arbitrary covariance matrix Γ in (11) and (12) requires its Cholesky (or equivalent) decomposition
with cost O(D3). As the resolution increases the computational burden gets intractable very quickly.
Indeed, to the best of the authors’ knowledge, all prior DDPM literature only considers independent
transitions, that is Γ = I. We suggest to view SDEs (11) and (12) as corresponding to the space
discretization on an H×W grid of a spatio-temporal process defined over the spatial domain [0, 1]2.
Consider the EulerdiSCretization of (12): Xt+∆t = Xt+αtβtXt∆t+√∆tEt, where Et 〜ND (0, Γ),
the idea is to adopt a functional perspective: X(t + ∆t, S)= X(t, S) + αtβtX(t, s)∆t + ∖T∆tE(t, S)
where s ∈ [0, 1]2 defines spaCe Coordinates. That is, both X(t) and E(t) at eaCh time t are random
processes over [0, 1]2. We assume the innovations E(t) to be a Gaussian process (GP) for each t.
As the GPs E(t) are defined on a 2D domain we can leverage on scalable inference techniques from
spatial statistics. As an example, we consider the circulant embedding method (CEM) (Wood &
Chan, 1994; Dietrich & Newsam, 1997) which exploits a connection with the fast Fourier transform
(FFT). See Appendix C for a cursory review of the CEM. Consider an H×W uniform grid S of
size S = HW discretizing [0, 1]2, i.e. the support of images. For a stationary covariance function
the CEM samples E(t) on S with cost O(D ln(D)). This is close to the O(D) cost of sampling
from a pure white-noise process, and compares very favorably to the O(D3) cost of a Cholesky
decomposition. One limitation of CEM is that generated samples, while always Gaussian, might not
have the correct covariances. Whether this happens, and in that case the quality of the approximation,
depends on the covariance function. In Appendix C we select and fit an isotropic GP to the microscale
properties of D(CIFAR). For this estimated GP sampling is exact. Figure 3 shows samples from a
pure white-noise GP, i.e. Γ = I, (1st row) and from the fitted GP using CEM (2nd row). As noted in
Section 6, sampling is enough to implement the MC estimators for Lce,* , but the MC estimators and
drift adjustments for Lce,* involve additional matrix multiplications by Γ and Γ-1. The CEM allows
to compute these at the same O(D ln(D)) cost if we define the GP E(t) on a 2D torus (Rue & Held,
2005, Chapter 2.1). This corresponds to introducing dependencies between “opposing” boundaries of
[0, 1]2. Figure 3 (3rd row) shows some samples, in the highlighted patch the opposing-boundaries
dependency is evident. Either way, all samples from the 2nd and 3rd rows of Figure 3 match the
smoothness properties of D(CIFAR).
Figure 3: Spatial GP samples, see the main text for the description.
9 Conclusions
The DBMT construction of Section 3 is exact. The SDE class of Section 4 is tractable as it results in
linear diffusion bridges. The time-space factorization of the diffusion coefficient g(x,t) = √βtΓ1/2
separates modeling concerns: βt corresponds to a time-wrapping, Γ can be efficiently modeled
by fitting the microscale properties of PD . Availability of GPU-accelerated FFT implementations
motivates our focus on the CEM. Alternative scalable approaches abound, from Gaussian Markov
Random Fields (Rue & Tjelmeland, 2002; Rue, 2001) to Karhunen-Loeve expansions (Betz et al.,
2014). It remains to apply the results of this work to perform an empirical benchmarking. Section 6
develops three novel training objectives, two of which with desirable properties compared to the
objective of Song et al. (2021), especially for non-factorial transitions. We remark the simplicity of the
proposed DBMT approach (Algorithms 1 and 2) compared to alternatives grounded in the Schrodinger
bridge problem (De Bortoli et al., 2021; Wang et al., 2021; Vargas et al., 2021). The understanding
of the target mappings ((20) and (21)) can guide the development of neural networks more closely
matching the target structure compared to the U-Net default choice. https://github.com/?
links to the code accompanying this paper which is made available under the MIT license.
9
Under review as a conference paper at ICLR 2022
References
Brian D.O. Anderson. Reverse-Time Diffusion Equation Models. Stochastic Processes and their
Applications,12(3):313-326, May 1982.
Wolfgang Betz, Iason Papaioannou, and Daniel Straub. Numerical Methods for the Discretization
of Random Fields by Means of the KarhUnen-Loeve Expansion. Computer Methods in Applied
Mechanics and Engineering, 271:109-129, April 2014.
Mogens Bladt, Samuel Finch, and Michael S0rensen. Simulation of Multivariate Diffusion Bridges.
Journal of the Royal Statistical Society. Series B (Statistical Methodology), 78(2):343-369, 2016.
Damiano Brigo. The General Mixture-Diffusion SDE and Its Relationship with an Uncertain-Volatility
Option Model with Volatility-Asset Decorrelation, December 2002.
Noel Cressie. Statistics for Spatial Data. John Wiley & Sons, 1993.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrodinger
Bridge with Applications to Score-Based Generative Modeling, June 2021.
C. R. Dietrich and G. N. Newsam. Fast and Exact Simulation of Stationary Gaussian Processes
through Circulant Embedding of the Covariance Matrix. SIAM Journal on Scientific Computing,
18(4):1088-1107, July 1997.
U. G. Haussmann and E. Pardoux. Time Reversal of Diffusions. The Annals of Probability, 14(4):
1188-1205, October 1986.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 6840-6851, 2020.
Ioannis Karatzas and Steven E. Shreve. Brownian Motion and Stochastic Calculus. Number 113
in Graduate Texts in Mathematics. Springer, New York, 2nd ed edition, 1996. ISBN 978-0-387-
97655-6 978-3-540-97655-4.
Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations.
Springer Berlin Heidelberg, Berlin, Heidelberg, 1992. ISBN 978-3-642-08107-1 978-3-662-12616-
5.
NV Krylov. Introduction to the Theory of Diffusion Processes, volume 142. Providence, 1995.
Annie Millet, David Nualart, and Marta Sanz. Integration by Parts and Time Reversal for Diffusion
Processes. The Annals of Probability, pp. 208-238, 1989.
L Chris G Rogers and David Williams. Diffusions, Markov Processes and Martingales: Volume 2:
Ito Calculus, volume 2. Cambridge university press, 2000.
Havard Rue. Fast Sampling of Gaussian Markov Random Fields. Journal of the Royal Statistical
Society. Series B (Statistical Methodology), 63(2):325-338, 2001.
Havard Rue and Leonhard Held. Gaussian Markov Random Fields: Theory and Applications.
Number 104 in Monographs on Statistics and Applied Probability. Chapman & Hall/CRC, Boca
Raton, 2005. ISBN 978-1-58488-432-3.
Haavard Rue and Haakon Tjelmeland. Fitting Gaussian Markov Random Fields to Gaussian Fields.
Scandinavian Journal of Statistics, 29(1):31-49, 2002.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised
Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
2256-2265. PMLR, 2015.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In
International Conference on Learning Representations, 2021.
10
Under review as a conference paper at ICLR 2022
Simo Sarkka and Arno Solin. Applied Stochastic Differential Equations. Cambridge University Press,
first edition, April 2019. ISBN 978-1-108-18673-5 978-1-316-51008-7 978-1-316-64946-6.
Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving Schrodinger
Bridges via Maximum Likelihood. Entropy, 23(9):1134, September 2021.
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
Computation, 23(7):1661-1674, July 2011.
Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep Generative Learning via
Schrodinger Bridge, June 2021.
Andrew T. A. Wood and Grace Chan. Simulation of Stationary Gaussian Processes in [0,1]d. Journal
of Computational and Graphical Statistics, 3(4):409-432, 1994.
B. K. 0ksendal. Stochastic Differential Equations: An Introduction with Applications. Universitext.
Springer, Berlin ; New York, 6th ed. edition, 2003. ISBN 978-3-540-04758-2.
11
Under review as a conference paper at ICLR 2022
A Theoretical Framework
A.1 Assumptions
Assumption 1 (SDE solution). A given D-dimensional SDE(f, g) with associated initial distribution
V0 and integration interval [0, τ] admits a unique strong solution on [0, τ].
Assumption 1 can be checked through the application of the standard existence and uniqueness
theorems for SDE solutions. Of particular relevance to our setting is the formulation of Krylov (1995,
Chapter 5, Theorem 1) that limits the monotonic requirement to, informally speaking, drifts that pull
the process toward infinities.
Assumption 2 (SDE density). A given D-dimensional SDE(f, g) with associated initial distribution
V0 and integration interval [0, τ] admits a marginal / transition density on (0, τ) with respect to the
D-dimensional Lebesgue measure that uniquely satisfies the Fokker-Plank / Kolmogorov-forward
partial differential equation (PDE).
We refer to Sarkka & Solin (2019, Chapter 5) and to Karatzas & Shreve (1996, Chapter 5.7) for
connections between SDEs and PDEs.
All theoretical results of this work rely on simple algebraic manipulations and re-arrangements of
quantities of interest. The main complication stems from the need to justify differentiation and
integration exchanges, i.e. exchange of limits.
Assumption 3 (exchange of limits). We assume that limits exchanges are justified in the steps marked
with (?) and (??).
Similarly, various steps in the derivations involve considering fractional quantities with densities
appearing in the denominators.
Assumption 4 (positivity). For a given stochastic process, all finite-dimensional densities, condi-
tional or not, are strictly positive.
Assumption 4 is easy to verify. We resorted to the practical but somewhat unsatisfactory formulation
of Assumption 3 because in full generality it is complicated to give easy to check conditions. We just
note that when ΠT = PD, the limit exchange marked with (?) is always justified. So are the limits
exchanges marked with (??) when in addition Π0 puts all the mass to a fixed initial value, or (by
direct verification) when Π0 is Gaussian for the SDE class of Section 4. Thorough this paper, both in
the main text and in the proofs that follow, it is supposed that Assumptions 1, 2 and 4 are satisfied by
SDEs (1), (6) and (7). This is the case for the SDE class of Section 4, i.e. (11) and (12), for any Π0
with finite variance.
Remark: For ease of exposition it is assumed thorough this paper that all diffusions take values in the
state space RD . There is no impediment in extending the presented results to the case of diffusions
taking values in a subset X ⊂ RD . The obvious changes to Assumptions 1 to 4 apply, the proofs
carry over without substantial modifications. This extension could be of practical interest as images
are often represented as floating point values in [0, 1].
A.2 S tatement and Proof of Diffusion Mixture Representation Theorem
Theorem 2 (Diffusion mixture representation). Consider the family of D-dimensional SDEs on
t ∈ [0, τ] indexed by λ ∈ Λ
dXλ = μλ(Xλ,t)dt + σλ(χλ,t)dWtλ,
X0 〜Vλ,
(27)
where the initial distributions V0λ and the BMs Wtλ are all independent. Let νtλ, t ∈ (0, τ) denote
the marginal density of Xtλ . For a generic mixing distribution L on Λ, define the mixture marginal
density νtfort ∈ (0, τ) and the mixture initial distribution V0by
νt(x)
νtλ(x)L(dλ),
Λ
V0(dx) =
Λ
V0λ(dx)L(dλ).
(28)
12
Under review as a conference paper at ICLR 2022
Consider the D-dimensional SDE on t ∈ [0, τ] defined by
μ(x, t)
Ja μλ(X,t)v，(X)LmR
σ (X, t)
νt(X)
Ra σλ(X, t)νtλ(X)L(dλ)
νt(X)
(29)
dXt = μ(Xt,t)dt + σ(Xt,t)dWt,
Y0 〜Vo.
It is assumed that all diffusion processes X λ and the diffusion process X solving (29) satisfy the
regularity assumptions Assumptions 1, 2 and 4 and that Assumption 3 holds. Then the marginal
distribution of the diffusion X is νt.
Proof of Theorem 2. We start by establishing that the law of X is indeed given by the solution of
(29). In this proof We make use of the following notation: for f scalar-valued (f )t = + f, for a
VeCtOr-ValUed (Gx = PD=1 悬a for A matrix-valued (A)Xx = PDj=I dχddχjA. ThiS notation
allows for a compact representation of PDEs reminiscent of the 1-dimensional setting. Then, for
0 < t < τ we haVe that
(ν(X, t))t
νλ(X, t)L(dλ)
a
(V λ(x,t))tL(dλ)
(??)
L (μλ(X,t)νλ(X,t))x + 2 (σλ(x,t)νλ(χ,t))χχL(dλ)
1
+ 2
E- V(x,t))
σλ (X, t)ν λ (X, t)
( V(X t)	V(x,t)J L(dλ)
μλ(x,t)νλ(X,t r
——V(Xt)——L(dλ)ν(X,t) I
σ λ (X, t)V λ (X, t)
----V(Xt)——-L(dλ)ν(X,t)J	.	(??)
The second line is an exchange of limits, the third line is the application of the Fokker-Plank PDEs
for the collection of processes Xλ, the fourth line is a rewriting in terms of V(y, t), the last line is
another exchange of limits. The result follows by noticing that the last line giVes the Fokker-Plank
representation of (29).	□
A.3 Drift Adjustments
Limitedly to this section, we lighten the notation by remoVing subscripts from probability measures
and densities. The missing time points can be inferred without ambiguity from the Variables.
A.3.1 Drift Adjustment Identities for Constant Initial Value
First identity:
Vxtln Z PxTJx)Π(dXτ)
t	p(Xτ |X0 )
V VxtP(XTIXt)	P(XTlXt)
=J p(Xτ∣X0) P(Xt|X0)n(dXT)/ J P(⅛)P(Xt|X0)n(dXT)	(?)
=ZVxtln P(XT |Xt)P(XT，：一 ∏(dXτ)/ Z p⅛≠0⅛(dXτ)
t	P(XT lX0)	P(XT lX0)
=	Vxt lnP(XTlXt)P(XtlX0,XT)Π(dXT) π(XtlX0)
= A(Xt, t, X0).
13
Under review as a conference paper at ICLR 2022
Second identity:
V Z P(XTIx)Π(dχτ)
P p(χτ ∣χo)
=Vxtln p P(XTIIxt)p(xt∣xo)∏(dxτ) 一 Vxtlnp(xt∣xo)
t	p(xτ |x0 )	t
= Vxt ln π(Xt∣X0) 一 Vxt lnP(Xt∣X0).
A.3.2 Drift Adjustments as Expectations
To establish (20) notice that from (14) we have
G(Xt, t)A(Xt , t)
-1	Xτ	a2(t,τ)P(Xt∣X0,Xτ)
=βtrr J IatT - xt) V(ttTr}	n(xt)	π0,τ(dx0,dxτ)
1	π(Xt∣X0, Xτ)	a2(t,τ )
=βt (OtT) J XT	π(xt) 一π0,τ(dX0, dxτ)- Xt) MT)
=βt (—1—	E	[Xτ] 一 Xt] a2(t, τ).
a(t,τ) XT〜Π(dxτ∣xt)	v(t,τ)
To establish (21) note that from (15) we have
G(yr,r) Vyr ln q(yr) = Γ
V Vyrlnq(yr∣yo)q(yr⅛)PD(dyo)
q(yr)
-1	a(0, r)y0 一 yr q(yr∣y0)
βr γγ 八	v(0,r)	PD (dy0)
β (α(0,r) Z yo q(yr∣y°)PD(dyo) — yr)	1 ʌ
q(yr )	v(0, r)
Br (a(0,r)	E l	[Y0] — yr) 1 ..
Yo 〜Q(dxo∣yr)	v(0,r)
B SDEs Class Formulas
The transition densities of (9) and (10) are given respectively by
ebm,τ |t (ZTIzt) = ND (zτ ； zt, γ(T 一 t)),
eou,τ∣t(zτIzt) = Nd (zτ； zteαt:T(τ-t), Γ (^e2%"-')
Here We used the notation f t：T = τ-t JtT fudu, i.e. f t：T is the average value of a function f on the
interval [t, T]. The time-homogenous case of (10), where &t：T = ɑ, is thus immediately recovered.
The scalar functions abm(t,τ), aou (t,τ),vbm (t,τ) andvou(t,τ) are given by
abm (t,τ) =1,	vbm (t, τ) = bτ 一 bt,
aou(t,τ) = eabt:bT (bi),	vou(t,τ) = ɪ e2αbt% (bτ-bt) — ɪ.
2αbt	2αbT
The scalar functions vbr(0, t, T), abr(0, t, T) and abr(0, t, T) are given by
vbr(0, t,τ)
abr(O,t,T )
abr(0,t,τ)
v(0,t)v(t,T)
v(0, t)a2(t, t) + v(t,T)
v(t, τ)a(0, t)
v(0,t)a2(t, τ) + v(t, τ)
v(0, t)a(t, τ)
v(0,t)a2(t, τ) + v(t, τ)
14
Under review as a conference paper at ICLR 2022
The scalar functions βve,r and βvp,r are given by
2r
βve,r= σ2lm σmax	2lθg ♦,	βvp,r = (βmm + r (βmaχ - βmin)).
σmin	σmin
The constants σma, σmaχ, βmin, βmax depend in part on the dataset considered, but are consistently
chosen to have βve,r, βvp,r small for r ≈ 0 and large for r ≈ τ.
The approximating distributions PZ in Song et al. (2021) are PZve = ND(0, Iσm2 aχ) for VESDE,
PZvp =ND(0,I) for VPSDE.
C	Additional Material
C.1 Closely Related Work
A work closely related to the present paper is that of Wang et al. (2021) as it similarly avoids the
time-reversal construction. Wang et al. (2021) construct a 2-stages diffusion process from a constant
initial value xo to PD by relying on the theory of Schrodinger bridges. The most notable differences
with respect to the DBMT transport are: (i) the dynamics considered in Wang et al. (2021) are less
general, in our notation they correspond to f (∙) = 0,g( ∙) = σI for a fixed scalar σ; (ii) the transport
proposed in Wang et al. (2021) necessarily starts from x0, the general result of (8) allows for (almost)
arbitrary initial distributions and initial-terminal dependencies. For an initial x0, i.e. for case of
A(xt, tx0) in Section 3.2 and for the more limited dynamics considered in Wang et al. (2021), the
achieved transport is the same. In this sense the DBMT generalizes the first stage diffusion of Wang
et al. (2021). It is interesting to note that in the case of a constant x0 the DBMT can also be obtained
by an application of Doob h-transforms as we show in the following section.
We now review two additional works grounded in the Schrodinger bridge problem: De Bortoli et al.
(2021); Vargas et al. (2021). Both works rely on the Iterative Proportional Fitting (IPF) procedure
to solve the (dynamic) Schrodinger bridge problem. Both works leverage on time-reversal results
to carry out the alternated Schrodinger half-bridge IPF iterations. The main difference between
the two works is that De Bortoli et al. (2021) estimates the optimal SDE drifts via neural network
approximations and score-matching, while Vargas et al. (2021) relies on Gaussian Processes and
maximum likelihood fitting. The work ofDe Bortoli et al. (2021) can be seen as an extension of Song
et al. (2021), and similarly to our work allows the use of shorter time intervals. Compared to our
proposal, it solves a harder problem but also presents additional difficulties. Training is more involved
as all the neural network approximations, one for each IPF iterate, need to converge. Moreover, there
is limited guidance on how to optimally choose the number of integration steps over the number of
IPF iterates.
C.2 Connection with Doob h-transforms
The previously established identity
PT It(XT Ixt)TI
A(Xt , t, x0 ) = V Xt ln /	/ I 、IlT (dxτ) ,
t	PT|0(xT|x0)
shows that the drift adjustment can be equivalently expressed as
μ(xt,t) = f (xt,t) + G(xt,t) Vxt h(xt,t),	h(xt,t) = ln / PTIt(XTIxt) ∏τ(dxτ),
t	PT|0(xT|x0)
as X0 is a constant. It can be verified that the h function satisfies the required space-time regularity
property (Sarkka & Solin, 2019, Eq. (7.73)). As such, it is a genuine Doob h-transform. That
Pth0It(Xt0 IXt) = Pt0It(Xt0 IXt)h(Xt0, t0)/h(Xt, t) is the transition density of the DBMT transport from
δx0 to ∏T follows by direct computation.
C.3 GP MODELLING ON CIFAR10
For simplicity, we assume a factorial distribution over the channels and an isotopic stationary
covariance function. We rely on the semivariogram approach (Cressie, 1993) to compare how
15
Under review as a conference paper at ICLR 2022
different covariance functions fit D(CIFAR). A semivariogram is a measure of dependency across
space. In the case of an isotropic stationary covariance it simplifies to a scalar function of the
Euclidean distance between points: Y(∣∣∆s∣∣) = E[(∆χs)2]∕2 with ∆xs = Xs+∆s - xs. The
rate of decrease of γ(k∆sk) toward 0 as k∆sk → 0 gives a measure of the infinitesimal spatial
dependency, i.e. the smoothness of the spatial process. Semivariograms corresponding to different
covariance functions are here fitted to their empirical counterparts via a weighted minimum-least-
squares procedure (Cressie, 1993). Figure 4 illustrates the exponential and RBF semivariogram
fits for two images of D(CIFAR). The exponential covariance, which corresponds to rougher paths,
provides a much better fit to the shown samples. This result is consistent across D(CIFAR). We
remark that Γ = I corresponds to a pure white-noise process with a perfectly flat semivariogram
which would clearly result in a very poor fit to the empirical semivariograms shown in Figure 4.
Based on these findings, we model the innovations of each image channel as a GP with exponential
covariance function with length-scale θ = 0.205, the median estimated value (Figure 4 (right)). We
match the marginal variance to that of D(CIFAR), σ2 = 0.063.
Figure 4: Empirical semivariograms (dots) and fitted exponential (solid lines) and RBF (doted lines)
semivariograms for the 1st (left) and 2nd (center) image of D(CIFAR); histograms (bins) and medians
(lines) of the distributions of the length-scale parameters in the exponential variogram model over
D(CIFAR) (right); colors represent the RGB channels.
C.4 Circulant Embedding Method
Figure 5: Circulant embedding covariance matrices, see the appendix’s main text for the description.
We start by providing a cursory explanation leading to efficient sampling in the 1D case, before
giving the intuition behind the extension to the 2D case. We refer to Wood & Chan (1994); Dietrich
& Newsam (1997) for a complete explanation and to Rue & Held (2005) for the results underlying
efficient density (likelihood) computation. Let [0, 1] be the spatial domain of interest. Let S =
{si}iM=1 be a uniform grid (regular lattice) discretizing [0, 1], where the points si are assumed to be
ordered. The first key observation is that for a stationary covariance function ρ( ∙, ∙) the covariance
matrix C with entries Ci,j = ρ(sj , sj ) is symmetric and Toeplitz, i.e. with constant-diagonals. See
Figure 5 (leftmost) for an example where M = 9. A property of symmetric Toeplitz matrices is
that they can always be embedded in larger symmetric circulant matrices. A circulant matrix of size
M 0×M 0 is defined by the property that all its rows (and columns) are obtained by cycling through
the same M0-dimensional vector. Circulant matrices correspond to covariance matrices of GPs
defined on a (here 1D) torus (Rue & Held, 2005, Chapter 2.1). The circulant embedding matrix just
introduced corresponds to an artificial enlargement of the spatial domain [0, 1] to a larger interval
leading to a torus. See Figure 5 (2nd from left) for a circulant embedding of C. The second key
observation is that a circulant matrix is diagonalized by the 1D FFT matrix. Having obtained the
eigenvalues of C , efficient sampling on the enlarged domain is achieved by the 1D FFT applied to
16
Under review as a conference paper at ICLR 2022
complex standard random numbers multiplied by the (square root of the) eigenvalues. The real and
imaginary part of the generated samples are independent. The main issue with the CEM is that the
circulant matrix embedding might fail to be positive definite. The issue can be avoided by considering
progressively larger embeddings, see the theoretical and empirical findings of Dietrich & Newsam
(1997). Otherwise, a level of approximation can be accepted by modifying the covariance function or
by truncating the eigenvalues to be positive.
The development of the 2D CEM follows very similar steps. The domain of interest is now [0, 1]2,
the uniform grid is S = {si,j }iM,j=1 and the points si,j are assumed to be lexicographically ordered.
The stationarity of the covariance functions results in a symmetric block-Toeplitz covariance matrix,
as shown in Figure 5 (3rd from left). Again, symmetric block-Toeplitz matrices can be embedded
in symmetric block-circulant matrices, as exemplified by Figure 5 (rightmost, zooming might be
required to see the block structure). Block-circulant matrices can be shown to be diagonalized by the
2D FFT matrix, and efficient sampling follows from similar steps to the ones seen in the 1D case.
D Additional Figures
17
Under review as a conference paper at ICLR 2022
Figure 7: Additional samples from the trained VPSDE model of Song et al. (2021), E[Xτ |Xt, t] and
Xt (interleaved rows) over sampling time t, Euler(1000) (top 16 rows) and Euler(100) (bottom 16
rows).
18
Under review as a conference paper at ICLR 2022
Figure 8: Additional samples from the trained VESDE model of Song et al. (2021), E[Xτ |Xt, t] and
Xt (interleaved rows) over sampling time t, Euler(1000) (top 16 rows) and Euler(100) (bottom 16
rows).

19