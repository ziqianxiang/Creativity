Continuous Deep Q-Learning in
Optimal Control Problems:
Normalized Advantage Functions Analysis
Anonymous authors
Paper under double-blind review
Ab stract
One of the most effective continuous deep reinforcement learning algorithms is
normalized advantage functions (NAF). The main idea of NAF consists in the ap-
proximation of the Q-function by functions quadratic with respect to the action
variable. This idea allows to apply the algorithm to continuous reinforcement
learning problems, but on the other hand, it brings up the question of classes of
problems in which this approximation is acceptable. The presented paper de-
scribes one such class. We consider reinforcement learning problems obtained by
the discretization of certain optimal control problems. Based on the idea of NAF,
we present a new family of quadratic functions and prove its suitable approxi-
mation properties. Taking these properties into account, we provide several ways
to improve NAF. The experimental results confirm the efficiency of our improve-
ments.
1	Introduction
The standard reinforcement learning (RL) setup consists ofan agent interacting with an environment
(Sutton & Barto, 2018). At each step of the interaction, the agent determines an action based on its
policy and its current state, gets a reward, and makes a transition to the next state. An aim of the
agent is to learn the policy that maximizes the sum of rewards.
Q-learning (Watkins & Dayan, 1992) is one of the most widespread algorithms for solving RL prob-
lems. According to this algorithm, the optimal action-value function (Q-function) is being found
as a solution of the Bellman optimality equation. After the learning, the agent can act optimally
by the learned Q-function. Initially, the Q-learning algorithm was applied for solving RL problems
with finite state and action spaces. In this case, the Q-function can be represented by a finite ta-
ble. For the case of a large or continuous state space, Q-learning has recently been extended to
Deep Q-learning algorithm (Mnih et al., 2015) that allows to look for the approximate Q-function
in the class of neural networks by means of the stochastic gradient descent. Deep Q-learning and
its modifications have shown the efficiency for a range of challenging tasks (Wang et al., 2016; van
Hasselt et al., 2016; Schaul et al., 2016; Hessel et al., 2018), however, note that this algorithm can
not be directly applied for solving RL problems with continuous action spaces. The reason is that
Deep Q-learning involves a maximizing of an approximate Q-function by the action variable on each
step of the learning, which is a complex problem for continuous action spaces. Among various ap-
proaches to overcome this problem (Lillicrap et al., 2016; Haarnoja et al., 2017; Kalashnikov et al.,
2018; Lim et al., 2019; Ryu et al., 2020; Lutter et al., 2021), we focus on an idea of the normalized
advantage functions (NAF) algorithm (Gu et al., 2016). This idea consists in the approximation of
the Q-function by functions quadratic with respect to the action variable. It allows to get the max-
imum quite fast and precisely and solve some challenging control problems (Gu et al., 2017; Dong
et al., 2018; Ikemoto & Ushio, 2021), but on the other hand, it brings up the question of classes of
RL problems in which this approximation is acceptable. The presented paper describes one of the
possible answers to this question.
Note that the class of LQR problems (Bradtke et al., 1994) has the Q-functions quadratic with respect
to the action variable. However, this class, being quite special, is not suitable for the description of
complex controlled processes. In the paper, we consider a wider (in some sense) class of RL prob-
1
lems. We consider RL problems which are obtained by the discretization of certain optimal control
problems (Bardi & Dolcetta, 1997). The rationale for the consideration of such RL problems is
that a lot of RL problems with continuous action spaces arise from control problems for mechanical
or robotic systems (Lillicrap et al., 2016; Gu et al., 2016; Haarnoja et al., 2017; Gu et al., 2017;
Kalashnikov et al., 2018), whose dynamic are, in fact, described by ordinary differential equations.
For the considered class of problems, based on the idea of NAF, we present a new family of quadratic
functions and prove that, first, this family is sufficiently rich to approximately solve the Bellman
optimality equation (Theorem 1), and second, any sufficiently accurate solution of the Bellman
optimality equation allows to approximately obtain the optimal policy in the corresponding optimal
control problem (Theorem 2). Moreover, we prove that it is impossible to get the same results for
the original family of functions from Gu et al. (2016) (Theorem 3). From the obtained theoretical
statements, we get some additional knowledge about the Q-function approximation by our family of
quadratic functions and also provide several ways to use this knowledge in order to improve NAF.
The experimental results confirm the efficiency of our improvements.
2	Background
The standard reinforcement learning (RL) setup consists ofan agent interacting with an environment
(Sutton & Barto, 2018). This interaction is described by a Markov Decision Process (MDP), which
is a tuple (S,U, P, R, ρ0, γ), where S is a state space, U is an action space, P(s0|s, u) is a transition
distribution, R(s, u) is a reward function, ρ0 (s) is an initial state distribution, and γ ∈ [0, 1] is a
discount factor. An aim of the agent is to learn its optimal policy μ*(s) that maximizes the value
J(μ) = E V"γiR(Si,Ui) I S0 〜P0(S0)，Ui =仙⑸)，Si+1 〜P(Si+1 ∣Si,Ui),i = 0,1, 2,...
In the general statement of reinforcement learning problems, a policy of the agent can be stochastic,
however, within this paper, we assume that the policy is deterministic.
One of the most effective algorithms for solving RL problems is Q-learning (Watkins & Dayan,
1992). According to this algorithm, the agent explores the environment and looks for the optimal
action-value function (Q-function)
Q*(s, U) = supE
μ
∞
P γiR(si, Ui) I s0 = s, U0 = U,
i=0
Si+1 〜P(si+ι∣Si,Ui), ui+1 = μ(si+ι), i = 0, 1, 2,...
as a solution of the Bellman optimality equation
Q*(s,u) = E[R(s, u) + Y max Q*(s0,u0) ∣ s0 〜P (s0∣s,u)].
u0∈U
In other words, the agent solves the following minimization problem:
sup ∣Q(s, u) — E[R(s, u) + γmaxQ(s0,u0) ∣ s0 〜P(s0∣s, u)] I → inf .	(1)
s∈S,u∈U	u0∈U	Q
If the agent knows the function Q*(s,a), it can act optimally by the greedy policy μ*(s) ∈
argmaxu∈u Q*(s,u).
Initially, the Q-learning algorithm was applied for solving RL problems with finite state and ac-
tion spaces. In this case, the Q-function is represented by a finite table and problem (1) is finite-
dimensional. For the case of a large or continuous state space, Q-learning has recently been extended
to Deep Q-learning algorithm (Mnih et al., 2015) that allows to look for the approximate Q-function
in the class of neural networks Q(x, uIθQ), where θQ is the parameter vector of the neural network.
During the learning, the experiences (si, ui, ri, si+1) are stored in the buffer D and simultaneously
the parameter vector θQ is updated by means of the stochastic gradient descent minimizing the loss
function
L(θQ) = E[(Q(s,u∣θQ) — y)2 ∣ (s,u,r, s0)〜U(D)],	y = r + YmaxQ(s0,u0∣θQ).⑵
u0∈U
2
where U(D) is the uniform distribution on D. Deep Q-learning and its modifications are effective for
a range of challenging tasks (Wang et al., 2016; van Hasselt et al., 2016; Schaul et al., 2016; Hessel
et al., 2018), however, note that this algorithm can not be directly applied for solving RL problems
with continuous action spaces. The reason is that Deep Q-learning involves the maximizing in (2) on
each step of the learning, which is a complex problem for continuous U. Among various approaches
to overcome this problem (Lillicrap et al., 2016; Haarnoja et al., 2017; Kalashnikov et al., 2018;
Lim et al., 2019; Ryu et al., 2020; Lutter et al., 2021), we focus on an idea of the normalized
advantage functions (NAF) algorithm (Gu et al., 2016). This idea consists in the approximation of
the Q-function by the following quadratic with respect to u functions:
Q(s,u∣θQ) = V (S∣θv)+ A(s,u∣θA),
1	(3)
A(s,u∣θA) = --(u - μ(s∣θμ))TP(s∣θP)(u — μ(s∣θμ)),
where V(s∣θv), μ(s∣θμ), and P(s∣θp) are neural networks with parameters θv, θμ, and θP, re-
spectively; P(s∣θp) is a positive-definite square matrix for each S and θp; θA = {θμ, θp} and
θQ = {θA, θv}. Under the condition
μ(s∣θμ) ∈ U,	(4)
it allows to get the maximum and argmaximum values directly by values of V(s∣θv) and μ(s∣θμ):
max Q(s, u∣θQ) = V (s∣θv),	Argmax Q(s, u∣θQ) = μ(s∣θμ),	(5)
u∈U	u∈U
but on the other hand, it brings up the question of classes of RL problems in which quadratic ap-
proximations is acceptable. Below, we describes one such class.
3	Problem statement
In this section, we consider a certain class of optimal control problems and show that discrete ap-
proximations of these problems can be formalized as RL problems.
Consider the following optimal control problem: it is required to maximize the functional
J (u(∙))
σ(x(T))-ZT
0
(q(t, x(t)) + u(t)Tr(t, x(t))u(t))dt,
(6)
over all u(∙), where χ(∙) is the solution (Filippov, 1988, §1) of the differential equation
ddtX(t) = f (t, x(t)) + g(t, x(t))u(t),	t ∈ [0, T],	(7)
under the initial condition
x(0) = z.	(8)
Here t is the time variable, T > 0 is the terminal instant of time, x(t) ∈ Rn is the current state
vector, u(t) ∈ U is the current control action vector forming the measurable function u(∙), U ⊂ Rm
is the nonempty compact set, z ∈ Rn is the fixed initial state vector, f(t, x) ∈ Rn, g(t, x) ∈
Rn×m, q(t, x) ∈ R, r(t, x) ∈ Rm×m, (t, x) ∈ [0, T] × Rn are continuous with respect to t and
continuously differentiable with respect to x functions, r(t, x) is the positive-definite matrix for each
(t, x) ∈ [0, T] × Rn, and σ(x) ∈ R, x ∈ Rn is the continuous function. We assume that there exists
a constant cfg > 0 such that
kf (t, x) + g(t, x)uk ≤ (-+ kxk)cf g,	(t, x) ∈ [0,T] × Rn, u ∈ U.	(9)
Note that, under these conditions, for each function u(∙), there exists a unique solution x(∙) of
equation (7) under the initial condition (8) (Filippov, 1988, §1).
Define the value function in optimal control problem (6), (7) by
V* (t*, x*)
sup
u(∙)
(t*, x*) ∈ [0,T] ×Rn,
(10)
3
where, for each u(∙), χ(∙) is the solution of equation (7) on the interval [t*,T] under the initial
condition x(t*) = x*.
Define the sets
S = {(t,x) ∈ [0,T] X Rn : kXk ≤ (1 + Ilzk)ecfgt - 1}, S(t) = {x ∈ Rn : (t,x) ∈ S}. (11)
Let k ∈ N, ∆tk = T/k, and t = i∆tk, i ∈ 0,k. Consider the corresponding discrete optimal
control problem: it is required to maximize the function
k-1
Jk(u0, u1, . . . uk-1) = σ(xk) - ∆tk	q(ti, xi) + uiTr(ti,xi)ui ,	(12)
i=0
over all Ui ∈ U, i ∈ 0, k 一 1, where (xo, χι,...,χk) is defined by
X0 = z, Xi+1 = Xi + (f (ti, Xi) + g(ti, Xi)Ui)∆tk, i ∈ 0, k ― 1.	(13)
Let us show that problem (12), (13) can be formalized as the RL problem. First, we define the state
and actions spaces, the initial state distribution, and the discount factor as follows:
S = ∪k=o({ti} × S(ti)) ∪ ST, U = U, po(so) = δ(so = (0, z)), Y = 1.	(14)
Here ST is some fictional terminal state, δ is Dirac delta distribution. Next, for every i ∈ 0, k 一 1,
X ∈ S(ti), and U ∈ U, we define the transition distribution and the reward function by
P(S |s = (ti,X),U)= δ(s = (ti+1, x )),	R(S = (ti,χ),u) = 一 (q(ti, X) + U r(ti, X)U) δ% ,
(15)
where X0 = X + (f(ti, X) + g(ti, X)U)∆tk. Taking into account (9) and (11), one can prove the
inclusion (ti+1, X0) ∈ S. Hence, the transition distribution P is well-defined. For i = k, we set
P(S0|S = (tk, X), U) = δ(S0 = ST),	R(S = (tk, X), U) = σ(X), X ∈ S(tk), U ∈ U. (16)
In order to make dynamical processes (13) formally infinite, we put
P(S0|ST, U) = δ(S0 = ST),	R(ST, U) = 0, U ∈ U.	(17)
Thus, we define MDP which describes the RL problem corresponding to problem (12), (13). Next,
we show that such RL problems is suitable for using quadratic approximations of the Q-function.
4	Quadratic approximations of the Q-function
Denote by Q the family of functions Q such that
Q(t, X, U) = V (t, X) + A(t, X, U),	(t, X, U) ∈ [0, T) × Rn × U,
where
A(t, X, u) = -(U — μ(t, X))T P (t, x)(u — μ(t, x))
+ (μ(t, x) — μ(t, x))tP(t, X)(μ(t, x) — μ(t, x))
μ(t, x) ∈ arg min (u0 — μ(t,x))τP(t, x)(u0 — μ(t,x)).
u0∈U
Here V(t, x), μ(t,χ), and P(t, x) are continuous functions; P(t,x) is a positive-definite square
matrix for each (t, X) ∈ [0, T) × Rn.
Note that, functions Q from the family Q satisfy the equalities
max Q(t, x, u) = V(t, x),	Argmax Q(t, x, u) = μ(t, x),	(20)
u∈U	u∈U
as well as (see (5)) quadratic functions from family (3). However, these function families are differ-
ent. The difference is that We do not assume the inclusion μ(t, x) ∈ U as opposed to assumption (4).
The theorems below establish a connection between the optimal control problem (6)-(8) and mini-
mization problems (1) for MDP (14)-(17).
(18)
(19)
4
Theorem 1.	Let the value function 匕(t, x) be continuously differentiable. Then, for every ε > 0,
there exists k* > 0 such that, for every k ≥ k*, the function Q ∈ Q defined by (18) and (i9) where
V (t,x) = K(t,x),	P (t,x) = r(t,x)∆tk, μ(t,x) = 1 r-1(t,x)gT (t,x)VχV^(t,x)
satisfies the inequality
Q(ti, x, u) + q(ti, x) + uTr(ti, x)u ∆tk - maxQ(ti+1, x0, u0) ≤ ε∆tk,
x0 = X + f(ti, x) + g(ti, x)u)∆tk,	U ∈ U, X ∈ S(ti), i ∈ 0,k — 1,
(21)
(22)
where we assume Q(tk, x0, u0) = σ(x0).
Theorem 2.	Let the value function V= (t,χ) be continuously differentiable. Let ε > 0 and k* > 0 be
defined according to Theorem 1. Take k ≥ k* and suppose that a function Q ∈ Q satisfies inequality
(22). Then the following estimate holds:
Jk (u0,u1,...uk-1) ≥ SUP J (Us) — 3Tε,	(23)
u(∙)
where the function Jk(u0, uι,... Uk-ι) is defined by (12) with Ui = μ(ti, Xi), i ∈ 0, k — 1 and the
function μ(t, x) is defined by Q according to (18) and (19).
Thus, Theorem 1 shows that the function family Q is sufficiently rich to contain approximate solu-
tions of problem (1) with a predetermined accuracy and Theorem 2 establishes that all such approx-
imate solutions contained in Q allow to get the policy, which approximately provides the optimal
result in optimal control problem (6)-(8).
Note that the similar results can be obtained for the wider class of optimal control problems (see
Appendix B). However, in this case, we need to consider another form of the function A(t, x, U).
Within the presented paper, we focus on class (6)-(8), because, firstly. this class seems quite general
and important for applications and, secondly, the function A(t, x, U), corresponding to this class,
has the quite simple (quadratic) form.
Now, let us consider the original family of functions from Gu et al. (2016). Denote by QNAF the
family of functions Q such that
Q(t, x,u) = V(t, x) — (u — μ(t, X))TP(t, x)(u — μ(t, x)),	(t, x, U) ∈ [0, T) X Rn X U, (24)
where V(t, x), μ(t, x), and P(t, x) are continuous functions; P(t, x) is a positive-definite square
matrix for each (t, x) ∈ [0,T) X Rn; μ(t, x) ∈ U for each (t, x) ∈ [0, T) X Rn. The theorem below
establishes that ifwe take the family QNAF instant ofQ, then Theorem 1 can not be proved even in
the simplest cases of optimal control problem (6)-(8).
Theorem 3. Let n = m = 1, T = 1, U = [—1, 1], f(t, x) = q(t, x) = 0, g(t, x) = r(t, x) = 1,
σ(x) = —x2, and Z = 2. Then, for every k ≥ 4 and Q ∈ QNAF, there exist i ∈ 0, k — 1, X ∈ S(ti),
and U ∈ U such that
∣Q(ti, x, u) + u2∆tk — max Q(ti+ι, x0, u0) I > ∆tk∕8, x0 = X + u∆tk,
u0∈U
where we assume Q(tk, x0, U0) = —(x0)2.
Proofs of the theorems are given in the Appendix A.
5 Experiments
We consider four examples of optimal control problems (6)-(8) described in Table 1, where
σι(x) =	—x2	— x2,	σ2(x)	= —|xi| —	0.l∣x2∣,	σ3(x) = —|xi	— 4| —	∣x21	—	∣x3 — 0.75π∣
σ3 (x) = —x1 — x2 — (x3 — 2) — (x4 — 2)
Van der Pol oscillator is a famous model of a non-conservative oscillator with non-linear damping.
The aim of the control is to stabilize the oscillator at the terminal time.
5
Table 1: Parameters in the examples of optimal control problems
Name
n m T U	f(t, x)	g(t, x)	q(t, x) r(t, x)	σ z
Van der Pol oscillator	2	1	11	[-1, 1]	(1 - x21 )x2			-0x1	0	0.05	σ1(x)
Pendulum	2	1	5	[-2, 2]	14.7 sin(x1 )			30	0	0.01	σ2(x)
						cos(x3)		0			
Dubins car	3	1	2π	[-0.5, 1]		sin(x3)		0	0	0.05	σ3(x)
						0		1			
						(0	∖		/1 0、			
						0		01			
A target problem	6	2	10	[-1, 1]2		x5 x6		00 00	0	0.001	σ4(x)
						x1 - x3		00			
						x2 - x4		00			
10
0
0
π
Pendulum is a traditional problem for testing control algorithms. The aim of the control is the
stabilization of the pendulum in the top position at the terminal time.
Dubins car is a quite famous model which describes a motion of the point particle moving at a
constant speed on the plane. The problem is to find a control providing the closeness of the motion
with a target point at the terminal time.
A target problem is an optimal control problem presented in Munos (2006). The dynamic system
describes a hand holding a spring to which is attached a mass. It is required to control the hand such
that the mass achieve the target point at the terminal time.
5.1	B ounded NAF
First, we modify NAF algorithm, proposed in Gu et al. (2016), based on the function family Q. Note
that, the considered examples have U = [α,β]m. Denote tanhα,β(V) = α+(1+tanh(ν))(β-α)∕2,
ν ∈ Rm, α, β ∈ Rm, where tanh is the hyperbolic tangent for each coordinate. Then, according to
(18) and (19), we can use the following approximation of the Q-function, within NAF algorithm:
Q(t,x,u∣θQ) = V (t,x∣θv) + A(t, x,u∣θA),
A(t,x,u∣θA) = -(U — μ(t,x∣θμ )/P (t,x∣θP)
十 (tanhα,β(μ(t, x∣θμ)) — μ(t, x∣θμ))2P(t, x∣θp),
where V(t, χ∣θv), μ(t, χ∣θμl), and P(t, χ∣θp) are neural networks with parameters θv, θμ, and θp,
respectively; P(t, χ∣θp) is a positive-definite square matrix for each (t, x) and θp; θA = {θμ, θp}
and θQ = {θA, θV}. To be short, we call this algorithm Bounded NAF (BNAF), because it is
essential for our modification of NAF that the set U is bounded.
Note that, we can also use the function clipα,β (ν) = max{α, min{β, ν}}, ν ∈ Rm, α, β ∈ Rm
instead of the function tanhα,β. According to (19), it is more correct, however our experiments
show that the learning results are slightly better with tanhα,β (see Appendix C). A possible reason
for this is that the function clipα,β is not smooth.
5.2	Reward-based BNAF
If we know the function r(t, x), then, according to Theorem 1, we can use the function r(t, x)∆tk
instead of the neural network P(t,χ∣θp) to reduce the number of learning parameters. This variant
of BNAF is called Reward-based BNAF (RB-BNAF).
6
(a) Van der Pol oscillator
(b) Pendulum
(c) Dubins car
Figure 1: Results of NAF, BNAF, RB-BNAF, GB-BNAF, DDPG algorithms averaged over 5 seeds.
(d) A target problem
5.3	Gradient-based BNAF
If we also know the function g(t, x), then, according to the Theorem 1, we can use the function
μ(t,x∣θv) = 1 r-1(t,x)gT (t,x)VχV (t,x∣θv)
instead of the neural network μ(t, χ∣θμ). It also reduces the number of learning parameters. This
variant of BNAF is called Gradient-based BNAF (GB-BNAF).
5.4	Experimental results
We use the same learning parameters of every our tasks. We apply neural networks with two layers
of 256 and 128 rectified linear units (ReLU) and learn their used ADAM with the learning rate
lr = 5e-4. We use batch size nbs = 256 and smoothing parameter τ = 1e-3. Also we take
∆t = 0.1. All calculations were performed on a personal computer in a standard way.
We compere NAF, BNAF, RB-BNAF, GB-BNAF algorithms, and DDPG algorithm presented in
(Lillicrap et al., 2016). Figure 1 shows learning curves of the algorithms for the considered exam-
ples. One can note that GB-BNAF algorithm is the most stable and gets the best performance in
all examples. RB-BNAF is also capable to get acceptable results in all examples, although it does
not have the same stable learning as GB-BNAF. RB-BNAF algorithm gets worse result than GB-
BNAF and RB-BNAF in Dubins car (c). NAF demonstrate good results in Van der Pol oscillator (a)
and Pendulum (b), however it does not cope with Dubins car (c) and A target problem (d). DDPG
algorithm shows poor performance only in A target problem (d).
Thus, taking the presented experiments into account, we can give the following general recommen-
dations. For solving optimal control problems (6)-(8), it is rational to use bNAf, GB-BNAF, and
7
especially RB-BNAF algorithms along with NAF algorithm. There is a high probability that they
will show better results and more stable learning.
6 Related works
Many different ways to apply reinforcement learning approaches for solving optimal control
problems are investigated (Baird, 1994; Doya, 1995; 2000; Munos, 2006; Tallec et al., 2019;
Jeongho Kim, 2020; Lutter et al., 2021). Among them, a time-discretization is perhaps the most
obvious and widely used tool (Lillicrap et al., 2016; Gu et al., 2016; Haarnoja et al., 2017; Gu et al.,
2017; Kalashnikov et al., 2018). From the theoretical point of view, it is known (Bardi & Dol-
cetta, 1997, p.388) that solutions of time-discrete optimal control problems converge to a solution
of the initial problem as the discretization step tends to zero. In the present paper, we also use the
time-discretization and study approximating solutions of Bellman equations and the corresponding
greedy politics depending on the discretization step (see Theorem 1 and 2). Other studies of depen-
dencies on the discretization step of reinforcement learning methods can be found in Munos (2006);
Tallec et al. (2019).
We focus on the idea from Gu et al. (2016) to expand the Q-learning algorithm to optimal control
and reinforcement learning problems with continuous actions. Other approaches for solving such
problems are investigated in Lillicrap et al. (2016); Haarnoja et al. (2017); Kalashnikov et al. (2018);
Lim et al. (2019); Ryu et al. (2020); Lutter et al. (2021). The paper Lutter et al. (2021) seems the
closest to the presented paper. In this paper, the similar optimal control problem and feedback
control policy are considered, however, another Bellman equation is used.
Let us also note that the family of functions (18), (19) is included, in some sense, to families of
Q-functions considered in Wang et al. (2016); Tallec et al. (2019). It seems expected because more
general classes of problems are considered in these papers. Nevertheless, proposed algorithms and
results of these papers are very different from presented in this paper.
References
Leemon C. Baird. Reinforcement learning in continuous time: Advantage updating. Proceedings of
1994 IEEE International Conference on Neural Networks, 1994.
Martino Bardi and Italo Capuzzo Dolcetta. Optimal Control and Viscosity Solutions of Hamilton-
Jacobi-Bellman Equations. Birkhauser, Boston, 1997.
Steven J. Bradtke, B. Erik Ydstie, and Andrew G. Barto. Adaptive linear quadratic control using
policy iteration. Proceedings of American Control Conference, 1994. doi: 10.1109/ACC.1994.
735224.
Xingping Dong, Jianbing Shen, Wenguan Wang, Yu Liu, Ling Shao, and Fatih Porikli. Hyperpa-
rameter optimization for tracking with continuous deep q-learning. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 518-527, 2018. doi:
10.1109/CVPR.2018.00061.
Kenji Doya. Temporal difference learning in continuous time and space. Proceedings of the 8th
International Conference on Neural Information Processing Systems, pp. 1073-1079, 1995.
Kenji Doya. Reinforcement learning in continuous time and space. Neural Computation, 12:219-
245, 2000. doi: 10.1162/089976600300015961.
Alexey Fedorovich Filippov. Differential Equations with Discontinuous Righthand Sides. Springer,
Berlin, 1988.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
with model-based acceleration. Proceedings of the 33rd International Conference on Machine
Learning, 48:2829-2838, 2016.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning
for robotic manipulation with asynchronous off-policy updates. Proceedings 2017 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), 2017. doi: 10.1109/ICRA.2017.7989385.
8
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. Proceedings of the 34th International Conference on Machine Learn-
ing, 70:1352-1361,2017.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. Proceedings of The Thirty-Second AAAI Conference on Artificial
Intelligence, pp. 3215-3222, 2018.
Junya Ikemoto and Toshimitsu Ushio. Continuous deep q-learning with simulator for stabilization
of uncertain discrete-time systems. arXiv:2101.05640, 2021.
Insoon Yang Jeongho Kim. Hamilton-jacobi-bellman equations for q-learning in continuous time.
Proceedings of the 2nd Conference on Learning for Dynamics and Control, 120:739-748, 2020.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Scalable
deep reinforcement learning for vision-based robotic manipulation. 2nd Conference on Robot
Learning (CoRL 2018), 87:651-673, 2018.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. Interna-
tional Conference on Learning Representations (ICLR), 2016.
Sungsu Lim, Ajin Joseph, Lei Le, Yangchen Pan, and Martha White. Actor-expert: A framework
for using q-learning in continuous action spaces. arXiv:1810.09103, 2019.
Michael Lutter, Shie Mannor, Jan Peters, Dieter Fox, and Animesh Garg. Value iteration in con-
tinuous actions, states and time. Proceedings of the 38th International Conference on Machine
Learning, 139:7224-7234, 2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015. doi: 10.1038/nature14236.
Remi Munos. Policy gradient in continuous time. Journal of Machine Learning Research, 7:
771-791, 2006.
Moonkyung Ryu, Yinlam Chow, Ross Michael Anderson, Christian Tjandraatmadja, and Craig
Boutilier. Caql: Continuous action q-learning. Proceedings of the Eighth International Con-
ference on Learning Representations, 2020.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In-
ternational Conference on Learning Representations (ICLR), 2016.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning An Introduction (Second Edition).
The MIT Press, Cambridge, Massachusetts, 2018.
Corentin Tallec, Leonard Blier, and Yann Ollivier. Making deep q-learning methods robust to time
discretization. Proceedings of the 36th International Conference on Machine Learning, 97:6096-
6104, 2019.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16), 30
(1):2094-2100, 2016.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling network architectures for deep reinforcement learning. Proceedings of The 33rd Interna-
tional Conference on Machine Learning, 48:1995-2003, 2016.
Christopher Watkins and Peter Dayan. Q-learning. Machine Learning, 8:279-292, 1992. doi:
10.1007/BF00992698.
9
A Appendix
Denote
H(t, x, s, u) = hf (t, x) + g(t, x)u, si - q(t, x) - uT r(t, x)u.	(25)
Lemma 1. Let the value function 匕(t,x) be continuously differentiable. Then, the following
Hamilton-Jacobi-Bellman equation holds:
'K', X) + maχH(t,χ, VχK(t,x),u) = 0,	(t,x) ∈ (0,T) X Rn.
∂t	u∈U
The proof can be found, for example, in J. Yong. Differential Games: A Concise Introduction.
World Scientific, University of Central Florida, 238 USA, 2015. doi: 10.1142/9121.
Proof of Theorem 1. Note that, due to (19), (21), and (25), we have
A(t, x, u)∕∆tk = H(t, x, VxK(t, x), U) — maxH(t, x, VχV^(t, x), u0)	(26)
x	u0∈U	x
for any (t, x, u) ∈ [0, T] × Rn × U. Due to (9) and (11), we have
f (t, x) + g(t, x)u ≤ cfg(1 + kzk)ecfgT := αfg,	(t, x) ∈ S, u ∈ U. (27)
Let ε > 0. Since V=(t, x) is continuously differentiable, there exists δ > 0 such that
匕(t0,x0)-K(t,x)-dv(t,x) (t0 —t)-hVxV(t,x),x0 —xi ≤ (|t—t0∣ + kx-x0k)(28)
∂t	1 + αfg
for any (t, x), (t0, x0) ∈ S satisfying |t —10∣ + ∣∣x — x0k ≤ δ. Put k* = (1 + ɑfg)T∕δ.
Let k ≥ k*, i ∈ 0,k — 1, X ∈ S(ti), u ∈ U, and x0 = X + (f (ti, x) + g(ti, x)u)∆tk. Due to (9) and
(11), we have (ti+1, x0) ∈ S. Then, from (27) and (28), we derive
Vi(ti+1, x0) — %(ti, x) — dV*(ti,x)∆tk —(VxVXti, x),f (ti, x)+ g(ti, X)Ui∆tk ≤ ε∆tk, (29)
Next, according to (18), (20), (25), (26), (29), and Lemma 1, we obtain
Q(ti, x, u) + q(ti, x) + uTr(ti, x)u ∆tk — mi0n Q(ti+1, x0, u0)
=∣K(ti,x) + A(ti,x,u) + (q(ti,x) + uτ r(ti,X)U) ∆tk — K(ti+ι,x0)∣
∂K(ti,x)	_ 一 ,	、八，	，	，
≤ ——xi―- + maxH(ti,x, VxK(ti,x),u ) ∆tk + ε∆tk = ε∆tk.
∂t	u0∈U
The theorem is proved.
Proof of Theorem 2. Let ε > 0. Define k* according to Theorem 1. Let k ≥ k*. Let Q ∈ Q satisfy
(22). Let Xi, i ∈ 0, k be defined by (13), where Ui ∈ μ(ti, Xi). Then, due to (12), (20), and (22), We
have
V(0, z) — Jk(u0, . . . , uk-1)
k-1
X(Q(ti,Xi,ui) + (q(ti,Xi) + uTr(ti,Xi)Ui)∆tk — nιaxQ(ti+ι,Xi+ι,u0)) ≤ Tε.
(30)
i=0
Let us define Q* ∈ Q according to (21). Let x*, i ∈ 0, k be defined by (13), where u* ∈
arg maxu∈U Q*(ti, xi, u). Then, due to the Theorem 1 and (20), we have
∣V*(ti,x*) + (q(ti,x*) + (u*)Tr(ti,x*)u*)∆tk — V;(ti+ι,x*+ι)∣ ≤ ε∆tk.	(31)
Note that, according to definition (10) of the value function V*, we have V* (tk, x) = σ(x), x ∈ Rn.
Then, from (20), (22), and (31), we derive
0 = max Q(tk, xk*, u0) — σ(x*k)
u0∈U
k-1
≤ X (mmaχ Q(ti+1,x*+1,u0) — Q(ti,x*,u*) — (q(ti,x*) + (u*)Tr(ti,x*)u*)∆tk)+V(0, z)
i=0 u ∈
10
k-1
+ X (⅛(ti,x"+ (q(ti,x*) + (u*)T r(ti,x"u7)∆tk -匕(友+卜/如))-匕(0,z)
i=0
≤ 2Tε + V(0, z) - K(0,z).
DUe to this inequality and inequality (30), We obtain Jk (uo,..., uk-ι) ≥ V= (0, z) - 3Tε. Taking
into account (6) and (10) we have SuP J(u(∙)) = V=(0, z). Thus, the theorem is proved.
u(∙)
Proof of Theorem 3. Let k ≥ 4 and Q ∈ QNAF. Let us take i = k - 1 and x = 2. For the sake of
brevity, denote V = V(tk-ι, 2), μ = μ(tk-ι, 2), and P = P(tk-ι, 2). Then, in order to prove the
theorem, we have to show that
max IV — (U — μ)2P + u2∆tk + (2 + u∆tk)21 > ∆tk/8.
u∈[-1,1]
Arguing by contradiction assume that
max ∣V — (u — μ)2P + u2∆tk + (2 + u∆tk)21 ≤ ∆tk∕8.
u∈[-1,1]
Then, for u = -1, u = 1, and u = 0, we have
—∆tk∕8 ≤ V - (1 + μ)2P + ∆tk + (2 — ∆tk)2	≤	∆tk∕8,	(32)
—∆tk∕8 ≤ V — (1 — μ)2P + ∆tk + (2 + ∆tk)2	≤	∆tk∕8,	(33)
-∆tk∕8 ≤ -V + μ2P — 4 ≤ ∆tk/8.	(34)
Adding up inequations (32), (33), and twice inequity (34), we derive
— ∆tk∕2 ≤ 2P — 2∆tk — 2∆t2k ≤ ∆tk∕2.	(35)
Adding up twice inequations (32), (34), and inequity (35), we obtain
—∆tk ≤ -4μP — 8∆tk ≤ ∆tk.
From this estimate, taking into account (35) and the inequality k ≥ 4, we conclude
μ ≤- 0 ≤ -—7— ≤- 7 < -1.
产―	4P -	5 + 4∆tk -	6
This inequality contradicts the inclusion μ ∈ U = [-1,1], which should be valid for Q ∈ QNAF.
B Appendix
Consider the following optimal control problem: it is required to maximize the functional
T
J(u(∙)) = σ(x(T)) - / F0(t, x(t), u(t))dt,
0
over all u(∙), where χ(∙) is the solution (Filippov, 1988, §1) of the differential equation
-dx(t) = F(t,x(t),u(t)), t ∈ [0,T],
dt
under the initial condition
x(0) = z.
Here t is the time variable, T > 0 is the terminal instant of time, x(t) ∈ Rn is the current state vector,
u(t) ∈ U is the current control action vector forming the measurable function u(∙), U ⊂ Rm is the
nonempty compact set, z ∈ Rn is the fixed initial state vector, F (t, x, u) ∈ Rn, F0 (t, x, u) ∈ R,
(t, x, u) ∈ [0, T] × Rn × U are continuous with respect to t and continuously differentiable with
respect to x functions, and σ(x) ∈ R, x ∈ Rn is the continuous function. We assume that there
exists a constant cfg > 0 such that
kF (t, x, u)k ≤ (1 + kxk)cfg,	(t, x) ∈ [0,T] × Rn,	u ∈ U.
11
(a) Van der Pol oscillator
episodes
(c) Dubins car
Figure 2: Results of RB-BNAF, GB-BNAF algorithms with clip and tangent functions averaged over
5 seeds.
(b) Pendulum
(d) A target problem
Similar to (25), denote
H(t, x, s, u) = hF (t, x, u)u, si - F0 (t, x, u).
Theorem 4. Let the value function V= (t, x) be continuously differentiable. Then, for every ε > 0,
there exists k= > 0 such that, for every k ≥ k=, the function
Q(t, x, u) = V= (t, x) + A(t, x, u),
A(t, x, U) = H(t, x, VχV=(t, x), u)∆tk — max H(t, x, ▽)匕(t, x), u0)∆tk
(36)
satisfies the inequality
Q(ti, x, u) + F0(ti, x, u)∆tk — max Q(ti+1, x0, u0) ≤ ε∆tk,
u0∈U
x0 = x + F(ti, x, u)∆tk, u ∈ U, x ∈ S(ti), i ∈ 0, k — 1,
where we assume Q(tk, x0, u0) = σ(x0).
Proof of Theorem 4 can be obtained similar to the proof of Theorem 1.
For the family of q-function (36), based on Theorem 4, one can also prove the result similar to
Theorem 2.
C Appendix
Figure 2 shows learning curves of RB-BNAF and GB-BNAF algorithms with the clip and hyperbolic
tangent functions for the considered examples. Note that there are no cases in which algorithms us-
ing the clip functions show better performance comparing with the corresponding algorithms using
the hyperbolic tangent function. Although the difference does not seem very significant.
12