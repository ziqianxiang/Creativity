Under review as a conference paper at ICLR 2022
The Power of Contrast for Feature Learning:
A Theoretical Analysis
Anonymous authors
Paper under double-blind review
Ab stract
Contrastive learning has achieved state-of-the-art performance in various self-
supervised learning tasks and even outperforms its supervised counterpart. De-
spite its empirical success, theoretical understanding of why contrastive learning
works is still limited. In this paper, under linear representation settings, (i) we
provably show that contrastive learning outperforms autoencoder, a classical un-
supervised learning method, for both feature recovery and downstream tasks; (ii)
we also illustrate the role of labeled data in supervised contrastive learning. This
provides theoretical support for recent findings that contrastive learning with la-
bels improves the performance of learned representations in the in-domain down-
stream task, but it can harm the performance in transfer learning. We verify our
theory with numerical experiments.
1	Introduction
Deep supervised learning has achieved great success in various applications, including computer
vision (Krizhevsky et al., 2012), natural language processing (Devlin et al., 2018), and scientific
computing (Han et al., 2018). However, its dependence on manually assigned labels, which is
usually difficult and costly, has motivated research into alternative approaches to exploit unlabeled
data. Self-supervised learning is a promising approach that leverages the unlabeled data itself as
supervision and learns representations that are beneficial to potential downstream tasks.
At a high level, there are two common approaches for feature extraction in self-supervised learning:
generative and contrastive (Liu et al., 2021). Both approaches aim to learn latent representations
of the original data, while the difference is that the generative approach focused on minimizing the
reconstruction error from latent representations, and the contrastive approach targets to decrease the
similarity between the representations of contrastive pairs. Recent works have shown the benefits of
contrastive learning in practice (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b;c). However,
why the contrastive approach outperforms the generative approach remains mysterious.
Additionally, recent works aim to further improve contrastive learning by introducing the label infor-
mation. Specifically, Khosla et al. (2020) proposed the supervised contrastive learning, where the
contrasting procedures are performed across different classes rather than different instances. With
the help of label information, their proposed method outperforms self-supervised contrastive learn-
ing and classical cross entropy based supervised learning. However, despite this improvement on
in-domain downstream tasks, Islam et al. (2021) found that such improvement in transfer learning
is limited and even negative for such supervised contrastive learning. This phenomenon motivates
us to rethink the role of labeled data in the contrastive learning framework.
In this paper, we first compare contrastive learning with a representative method in the generative
approach - the autoencoders. Specifically, We initialize the investigation in the linear representa-
tion setting, which has been widely adopted in theory to shed light upon complex machine learn-
ing phenomena such as in Du et al. (2020); Tripuraneni et al. (2021). We provide a theoretical
analysis of their feature learning performances on the spiked covariance model (Bai & Yao, 2012;
Yao et al., 2015; Zhang et al., 2018) and theoretically justify Why contrastive learning outperforms
autoencoders—contrastive learning is able to remove more noises by constructing contrastive sam-
ples. Then We investigate the role of label information in the contrastive learning frameWork and
provide a theoretical justification of Why labeled data help to gain accuracy in same-domain classi-
fication While can hurt multi-task transfer learning.
1
Under review as a conference paper at ICLR 2022
Related works The idea of contrastive learning was first proposed in Hadsell et al. (2006) as
an effective method to perform dimension reduction. Following this line of research, Dosovitskiy
et al. (2014) proposed to perform instance discrimination by creating surrogate classes for each
instance and Wu et al. (2018) further proposed to preserve a memory bank as a dictionary of negative
samples. Other extensions based on this memory bank approach include He et al. (2020); Misra &
Maaten (2020); Tian et al. (2020); Chen et al. (2020c). Rather than keeping a costly memory bank,
another line of works exploits the benefit of mini-batch training where different samples are treated
as negative to each other Ye et al. (2019); Chen et al. (2020a). Moreover, Khosla et al. (2020)
explores the supervised version of contrastive learning where pairs are generated based on label
information.
Despite its success in practice, theoretical understanding of contrastive learning is still limited. Pre-
vious works provide provable guarantees for contrastive learning under conditional independence
assumption (or its variants) (Arora et al., 2019; Lee et al., 2020; Tosh et al., 2021; Tsai et al., 2020).
Specifically, they assume the two contrastive views are independent conditioned on the label and
show that contrastive learning can provably learn representations beneficial for downstream tasks.
In addition to this line of research, Wang & Isola (2020); Graf et al. (2021) investigated the repre-
sentation geometry of supervised contrastive loss, and HaoChen et al. (2021) provided analysis via a
novel concept of augmentation graph with a new loss function that performs spectral decomposition
on such graph. Moreover, Wen & Li (2021) considered the representation learning under the sparse
coding model and studied the optimization properties on shallow ReLU neural networks. Different
from all previous works, which aim to show that contrastive learning can learn useful representa-
tion, our paper aim to explain why contrastive learning outperforms other representation learning
methods and also shed light on the role of labeled data in contrastive learning framework, which is
under-explored in prior works.
2	Preliminaries
Notations In this paper, We use O, Ω, Θ to hide universal constants and We write ak . bk for
two sequences of positive numbers {ak} and {bk} if and only if there exists an universal constant
C > 0 such that ak < Cbk for any k. We use k ∙ k, k ∙ k2, k ∙ ∣∣f to represent the '2 norm of
vectors, spectral norm of matrices and Frobenius norm of matrices respectively. Let Od,r be a set
of d × r orthogonal matrices. i.e., Od,r , {U ∈ Rd×r : U>U = Ir}. We use |A| to denote
the cardinality of a set A. For any n ∈ N+, let [n] = {1, 2,…，n}. We use ∣∣ sinΘ(U1, U2)∣∣F
to refer to the sine distance betWeen tWo orthogonal matrices U1 , U2 ∈ Od,r, Which is defined by:
∣sin Θ (U1, U2)∣F , U1>⊥U2 F. More properties of sine distance can be found in Section A.1.
We use {ei}id=1 to denote the canonical basis in d-dimensional Euclidean space Rd, that is, ei is
the vector Whose i-th coordinate is 1 and all the other coordinates are 0. Let I{A} be an indicator
function that takes 1 When A is true, otherWise takes 0. We Write a ∨ b and a∧ b to denote max(a, b)
and min(a, b), respectively.
2.1	Setup
Given an input x ∈ Rd , contrastive learning aims to learn a loW dimensional representation h =
f (x; θ) ∈ Rr by contrasting different samples, i.e., maximizing the agreement betWeen positive
pairs, and minimizing the agreement betWeen negative pairs. Suppose We have n data points X =
[χ1,χ2, ∙∙∙ ,xn] ∈ Rd×n from the population distribution D. The contrastive learning task can be
formulated to be an optimization problem:
1n
min L(θ) = min - £'(“, Bpos, BNeg； f (∙,θ)) + λR(θ),	(2.1)
θ	θn
i=1
where '(∙) is a contrastive loss and λR(θ) is a regularization term; Bpos, BNeg are the sets of
positive samples and negative samples corresponding to xi , Which We Will describe in detail beloW.
Losses and Models. We then present the model setup considered in this paper.
(a)	. Linear representation and regularization term. We consider the linear representation function
f(x, W) = Wx, where the parameter θ is a matrix W ∈ Rr×d. Since regularization techniques
2
Under review as a conference paper at ICLR 2022
have been widely adapted in contrastive learning practice (Chen et al., 2020a; He et al., 2020; Grill
et al., 2020), we further consider penalizing the representation by a regularization term R(W) =
kW W > k2F /2 to encourage the orthogonality of W and therefore promote the diversity of wi to
learn different representations.
(b)	. Triplet Contrastive loss. The contrastive loss is set to be the average similarity between positive
pairs minus that between negative pairs:
'(x, BPos, BNeg ,…))= -X	h (X"Pox：")〉+ X	h (XkgNe ,θ)i ,
xPos ∈BPos	xNeg ∈BNeg
(2.2)
where BPos, Bneg are sets of positive samples and negative samples corresponding to x. This loss
has been commonly used in constrastive learning (Hadsell et al., 2006) and metric learning (Schroff
et al., 2015; He et al., 2018). In Khosla et al. (2020), the authors show that it is an approximation
of the NT-Xent contrastive loss, which has been highlighted in recent contrastive learning practice
(Sohn, 2016; Wu et al., 2018; Oord et al., 2018; Chen et al., 2020a).
(c)	. Generation of positive and negative pairs. There are two common approaches to generate such
pairs, depending on whether or not label information is available. When the label information is
not available, the typical strategy is to generate different views of the original data via augmentation
(Hadsell et al., 2006; Chen et al., 2020a). Two views of the same data point serve as positive pair
for each other while those of different data serve as negative pairs.
Definition 2.1 (Augmented pairs generation). Given two augmentation functions g1 , g2 : Rd → Rd
and n training samples B = {xi}i∈[n] , the augmented views are given by: {(g1 (xi), g2(xi))}i∈[n] .
Then for each view gv (xi), v = 1, 2, the corresponding positive samples and negative samples are
defined by： BPvS = {gs(xi) :s ∈ [2] \ {v}} and BNeg = {gs(xj) :s ∈ [2],j ∈ [n] \ {i}}.
The loss function of self-supervised contrastive learning problem can be written as:
LSg(W ) = - 2L XX XX [hWgv (Xi),Wg[2]∖{v}(Xi)i-XXX hWgv (χin,-Wgs(Xj )i -
i=1 v=1	j 6=i s=1
In particular, we adopt the following augmentation in our analysis.
+2 kww >kF.
(2.3)
Definition 2.2 (Random masking augmentation). The two views of the original data are generated
by randomly dividing its dimensions to two sets, that is, g1(xi) = Axi, and g2(xi) = (I - A)xi,
where A = diag(aι, ∙∙∙ ,ad) ∈ Rd×d is the diagonal masking matrix with {ai}d=ι being i.i.d.
random variables sampled from a Bernoulli distribution with mean 1/2.
A similar augmentation was considered in Wen & Li (2021). However, our primary interest lies in
comparing the performance of contrastive learning against autoencoders and analyzing the role of
labeled data, while their work focuses on understanding the training process of neural networks in
contrastive learning.
When the label information is available, Khosla et al. (2020) proposed the following approach to
generate pairs.
Definition 2.3 (Supervised pairs generation). In a K-class classification problem, given nk samples
for each class k ∈ [K]: {xik : i ∈	[nk]}kK=1 and let n	=	PkK=1 nk, the	corresponding	positive
samples and negative samples for xik	are defined by BiP,kos	=	{xjk : j ∈ [nk]	\ i} and BiN,keg	= {xjs :
s ∈ [K] \ k,j ∈ [ns]}. That is, the positive samples are the remaining ones in the same class with
xik and the negative samples are the samples from different classes.
Correspondingly, the loss function of the supervised contrastive learning problem can be written as:
LSupCon (W)
hWxk, Wxk i	X X hWxk, Wxj Y
n -1	j=1 S=k n(K -1) 一
+λ2 kWW >kF.
(2.4)
(d)	. Spiked Covariance Model. We consider the following spiked covariance model (Bai & Yao,
2012; Yao et al., 2015; Zhang et al., 2018) to study the power of contrastive learning:
x = U?z + ξ,	Cov(z) = ν2Ir,	Cov(ξ) = Σ,	(2.5)
3
Under review as a conference paper at ICLR 2022
where z ∈ Rr and ξ ∈ Rd are both zero mean sub-Gaussian random variables. In particular,
U? ∈ Od,r and Σ = diag(σ2,…，σ2). The first term U?z represents the signal of interest residing
in a low-dimensional subspace spanned by the columns of U? . The second term ξ is the dense noise
with heteroskedastic noise. Given that, the ideal low-dimensional representation is to compress the
observed x into a low-dimensional representation spanned by the columns of U?.
In this paper, we aim to learn a good projection W ∈ Rr×d onto a lower-dimensional subspace
from observation x. Since the information of W is invariant with the transformation W J OW
for any invertible matrix O ∈ Rr,r, the essential information of W is contained in the right
eigenvector of W. Thus we quantify the goodness of the representation W by the sine distance
k sin Θ(U, U?)kF, where U is the top-r right eigenspace of W.
3	Self-supervised contrastive learning versus autoencoder
Autoencoder and contrastive learning are two popular approaches for self-supervised learning. Re-
cent experiments have highlighted the improved performance of contrastive learning compared with
autoencoders. In this section, we rigorously demonstrate the advantage of contrastive learning over
autoencoder by investigating the linear representation settings under the spiked covariance model
Eq.(2.5). The investigation is conducted for both feature recovery and downstream tasks.
3.1	Recover features from Noisy Data
Here we focus on the analysis of feature recovery to understand the benefit of contrastive learn-
ing over autoencoders. As mentioned above, our target is to recover the subspace spanned by the
columns of U? , which can further help us obtain information on the unobserved z that is important
for downstream tasks. However, the observed data has covariance matrix of ν2 U?U?> + Σ rather
than the desired ν2U ?U ?>, which brings difficulty to representation learning. We demonstrate that
contrastive learning can better exploit the structures of core features and obtain better estimation
than an autoencoder in this setting.
We start with autoencoders. Formally, an autoencoder consists of an encoder fAE : Rd → Rr and
a decoder gDE : Rr → Rd . While the encoder compresses the original data into low dimensional
features, and the decoder recovers the original data from those features. It can be formalized as the
following optimization problem for samples {xi}in=1 (Ballard, 1987; Fan et al., 2019):
1n
f AminAE n X kxi- gDE (f AE (Xi))k2.
In the linear representation setting, where fAE(x) = WAEx + bAE and gDE (y) = WDEy + bDE,
previous works (Bourlard & Kamp, 1988; Plaut, 2018) have shown that the autoencoder can be
reduced to principal component analysis (PCA). That is, the optimal WAE is given by:
WAE = (UAE ΣAE VA>E)>,	(3.1)
where UAE is the top-r eigenvectors of matrix M := X(I- 1n1n>/n)X>, ΣAE is a diagonal matrix
consists of eigenvalues of M and VAE = [vι, ∙∙∙ ,vn] ∈ Rr×r can be any orthonormal matrix. In
the noiseless case, the covariance matrix is ν2 U? U?> and autoencoder can perfectly recover the core
features. However, in noisy cases, the random noises sometimes perturb the core features, which
make autoencoders fail to learn core features. Such noisy cases are very common in real applications
such as measurement errors and backgrounds in images such as grasses and sky. Interestingly, we
will later show that contrastive learning can better recover U? despite the presence of large noise.
To provide rigorous analysis, We first introduce the incoherent constant (CandeS & Recht, 2009).
Definition 3.1 (Incoherent constant). We define the incoherence constant of U ∈ Od,r as
I(U) = maxei>U2 .	(3.2)
Intuitively, the incoherent constant measures the degree of the incoherence of the distribution of
entries among different coordinates, or loosely speaking, the similarity between U and canonical
4
Under review as a conference paper at ICLR 2022
basis {ei}id=1. For uncorrelated random noise, the covariance matrix is diagonal and its eigenspace
is exactly spanned by the canonical basis {ei}id=1 (if the diagonal entries in Σ are all different),
which attains the maximum value of incoherent constant. On the contrary, core features usually
exhibit certain correlation structures and the corresponding eigenspace of the covariance matrix is
expected to have a lower incoherent constant.
We then introduce a few assumptions where our theoretical results are built on. Recall that in the
spiked covariance model Eq.(2.5), X = U?z + ξ, Cov(Z) = VlIr and Cov(ξ) = diag(σ2,…，σ2).
Assumption 3.1 (Regular covariance condition). The condition number of covariance matrix Σ =
diag(σ2,…，σ2) satisfies K :=。,口值d)< C, where σj represents the j-th largest number
among σ2, ∙ ∙ ∙ ,σ^ and C > 0 is a universal constant.
Assumption 3.2 (Signal to noise ratio condition). Define the signal to noise ratio P := v∕σ(1), We
assume ρ = Θ(1), implying that the covariance of noise is of the same order as that of core features.
Assumption 3.3 (Incoherent condition). The incoherent constant of the core feature matrix U? ∈
Od,r satisfies I(U?) = O(r log d/d). 1
Remark 3.1. Assumption 3.1 implies that the variances of all dimensions are of the same order.
For Assumption 3.2, we focus on a large-noise regime where the noise may hurt the estimation
significantly. Here we assume the ratio lies in a constant range, but our theory can easily adapt
to the case where ρ has a decreasing order. Assumption 3.3 implies a stronger correlation among
coordinates of core features, which is the essential property to distinguish them from random noise.
Now we are ready to present our first result, showing that the autoencoder is unable to recover the
core features in the large-noise regime.
Theorem 3.1 (Recovery ability of autoencoder, lower bound). Consider the spiked covariance
model Eq.(2.5), under Assumption 3.1-3.3 and n > d	r, let WAE be the learned representa-
tion of autoencoder with singular value decomposition WAE = (UAE ΣAE VA>E)> (as in Eq.(3.1)).
Ifwefurtherassume {σ2}d=1 are differentfrom each other and σ(i)∕(σ(r) 一 02丁+1)) < Cσ for some
universal constant Cσ. Then there exist two universal constants Cρ > 0, c ∈ (0, 1), such that when
ρ < Cρ, we have
EksinΘ(U?, Uae)∣∣f ≥ c√r.	(3.3)
Remark 3.2. The additional assumptions {σf}d=1 are different from each other and 021)/。2丁)一
σ(2r+1)) < Cσ for some universal constant Cσ are for technical consideration. We need these condi-
tions to guarantee the uniqueness of UAE. As an extreme example, the top-r eigenspace of identity
matrix can be any r-dimensional subspace and thus not unique. To avoid discussing such arbitrari-
ness of output, we make these assumptions to guarantee the separability of eigenspace.
Then we investigate the feature recovery ability of the self-supervised contrastive learning approach.
Theorem 3.2 (Recovery ability of contrastive learning, upper bound). Under the spiked covariance
model Eq.(2.5), random masking augmentation in Definition 2.2, Assumption 3.1-3.3 and n > d
r, let WCL be any solution that minimizes Eq.(2.3), and denote its singular value decomposition as
WCL = (UCLΣCLVC>L)>, then we have
..	,, 一,	r3/2	I dr
EksinO(U ?,UCL)kF .才 log d + 匕
(3.4)
Remark 3.3. In Eq.(3.4), the first term is due to the shift between the distributions of the augmented
data and the original data. Specifically, the random masking augmentation generates two views with
disjoint non-zero coordinates and thus can mitigate the influence of random noise on the diagonal
entries in the covariance matrix. However, such augmentation slightly hurts the estimation of core
features. This bias, appearing as the first term in Eq.(3.4), is measured by the incoherent constant
Eq.(3.2). The second term corresponds to the estimation error of the population covariance matrix.
Theorem 3.1 and 3.2 characterize the difference of feature recovery ability between autoencoder and
contrastive learning. The autoencoder fails to recover most of the core features in the large-noise
1The order of I(U?) can be chosen to be any function that decreasing to 0 when d → ∞ and one can easily
adapt the later results to this setting. Here we set it to O(r log d/d), the order when U is drawn from a uniform
distribution on Od,r (see the proof in Lemma B.1) for simplicity to obtain an exact order in later results.
5
Under review as a conference paper at ICLR 2022
regime, since ∣∣ sinΘ(U, U?)∣∣f has a trivial upper bound √r. In contrast, with the help of data
augmentation, the contrastive learning approach mitigates the corruption of random noise while
preserving core features. As n and d increase, it yields a consistent estimator of core features and
further leads to better performance in the downstream tasks, as shown in the next section.
3.2	Performance on the Downstream Task
In the previous section, we have seen that contrastive learning can recover the core feature effec-
tively. In practice, we are interested in using the learned features to downstream tasks. He et al.
(2020) experimentally showed the overwhelming performance of linear classifiers trained on repre-
sentations learned with contrastive learning against several supervised learning methods in down-
stream tasks.
Following the recent empirical studies, here we evaluate the downstream performance of simple
predictors, which take a linear transformation of representation as an input. Specifically, we consider
the regression setting with a class of predictors δW,w (x) = w>Wx constructed upon the learned
representations W = WCL and WAE respectively. Given the observation X = U?z + ξ independent
of unsupervised data X, our prediction target is y generated from a linear model y =(Z, w?i/ν + K
where Z 〜N(0, ν2Ir) is the low-dimensional core feature, V is the scale of Z and e is the error
term independent of Z with zero mean and finite variance. w? ∈ Rr is a unit vector of coefficients.
We can interpret this model as a principal component regression model (PCR) (Jolliffe, 1982) under
standard error-in-variables settings2, where we assume that coefficients lie in a low-dimensional
subspace spanned by column vectors of U? . We either estimate or predict the signal based on
observed samples contaminated by the measurement errorξ. For details of PCR in error-in-variables
settings, see, for example, Cevid et al. (2020); Agarwal et al. (2020); Bing et al. (2021).
Now we state our result on the downstream performance in prediction task. For any linear represen-
tation W ∈ Rr×d, let R(W):= EX [infw∈Rr Ey,而['(δw,w)] be the risk of the best linear predictor,
where '(δ) = (y 一 δ(X))2.
Theorem 3.3 (Upper Bound for Downstream Excess Risk of WCL). Suppose conditions in Theorem
3.2 are satisfied. Then we have
R(Wcl) — R(U?>) . r3/2 logd + Tdr.
dn
This result shows that the price of estimating U? by contrastive learning on a downstream prediction
task can be made small in a case where the core feature lies in a relatively low-dimensional subspace,
and the number of samples is relatively large compared to the ostensible dimension of data.
However, the downstream performance of autoencoders is not as good as contrastive learning. We
obtain the following lower bound for the downstream prediction risk with the autoencoder.
Theorem 3.4 (Lower Bound for Downstream Excess Risk of WAE). Suppose the conditions in
Theorem 3.1 hold. Assume r ≤ rc holds for some constant rc > 0. Additionally assume that
ρ = Θ(1) is sufficiently small and n d r. Then,
R(WAE) 一 R(U?>) ≥ c0,
where c0 > 0 is a constant independent of n and d.
Comparing theorems 3.3 and 3.4, We find that even when r/d and ,d/n are small, the downstream
performance of autoencoders is not satisfying and has a much larger downstream task error rate than
that of contrastive learning. We note that the constant lower bound of Theorem 3.4 can be obtained
without the assumption r ≤ rc by assuming a slightly stronger conditions ρ = O(1/ log d) and
n dr. We also illustrate this phenomenon via numerical simulation in Fig.1. As predicted by
Theorem 3.1 and 3.2, the downstream task risk of contrastive learning decreases as d increases (Fig.
1: Left) and as n increases (Fig. 1: Center) while that of autoencoder remains large when n and d
increase.
2In error-in-variables settings, the bias term from the measurement error appears in prediction and estima-
tion risk. Since our focus lies in proving a better performance of contrastive learning against autoencoders, we
ignore the unavoidable bias term here by considering the excess risk.
6
Under review as a conference paper at ICLR 2022
.5.4.32. 1.0
0.0.0.0.0.0.
>∣SΞEEəbsuMoa
0.05
0.04
0.03
0.02
0.01
-5	-3	-1	1	3	5
∣oge(α)
Figure 1: The vertical axes indicate the prediction risk. Left: Comparison of downstreak task
performance between contrastive learning and autoencoders the dimension d. The sample size n
is set as n = 20000. Center: Comparison of downstreak task performance between contrastive
learning and autoencoders the dimension n. The dimension d is set as d = 40. Right: Downstream
task performance in transfer learning against penalty parameter α in log scale. T is the number of
source tasks and r is the dimension of the representation function. We set the number of labeled
data and unlabeled data as m = 1000 and n = 1000 respectively.
Remark 3.4. A similar results of upper and lower bound hold for linear predictors δ(x) =
I{F (w>Wx) > 1/2} under binary classification settings, where label is generated from a binary
response model y|z = Ber(F(〈Z, W?)/v)) with a known function F : R → [0,1]. Our results imply
that the learned representations by contrastive learning are also useful in downstream classification
tasks, compared to an autoencoder, thus supporting the empirical success of contrastive learning. A
detailed results and proofs are deferred to Appendix B.2.
>ls≡EeφtSUΛΛ0α
10	30	50	70	90	110
Dimension d
4	The impact of labeled data in supervised contrastive learning
Recent works have explored adding label information to improve contrastive learning (Khosla et al.,
2020). Empirical results show that label information can significantly improve the accuracy of the
in-domain downstream tasks. However, when domain shift (multiple sources) is considered, the
label information hardly improves and even hurts transferability (Islam et al., 2021). Motivated
by those empirical observations, in this section, we aim to investigate the role of labeled data in
contrastive learning and provide a theoretical foundation for such phenomena.
4.1	Feature mining in multi-class classification
We first demonstrate the role of label in the single-sourced case in contrastive learning. Suppose
our samples are drawn from r + 1 different classes with probability pk for class k ∈ [r + 1], and
Prk+=11 pk = 1. For each class, samples are generated from a class-specific Gaussian distribution:
Xk = μk + ξk,	ξk 〜N(0, Σk), ∀k = 1, 2,…，r + 1.	(4.1)
To be consistent With the spiked covariance model Eq.(2.5), We assume ∣∣μk ∣∣ = √∕rν, ∀k ∈ [r + 1].
We further assume Σk = diag(σ2,k,…,σd,k), denote ,j1)= max1≤i≤d,1≤j ≤r+1 σ2,j∙ and assume
Pk=1 Pkμk = 0, where the last assumption is added to ensure identifiable since the classification
problem (4.1) is invariant under translation. Denote A = Pk=1 Pkμkμ>, we assume rank(A) = r
and C1ν2 < λ(r) (Λ) < λ(1)(Λ) < C2ν2 for two universal constants C1 and C2. We remark that
this model is a labeled version of the spiked covariance model Eq.(2.5) since the core features and
random noise are both sub-Gaussian. We use r +1 classes to ensure that μk,s span an r-dimensional
space, and denote its orthonormal basis as U?. Recall that our target is to recover U?.
As introduced in Definition 2.3, Khosla et al. (2020) proposed a novel approach named supervised
contrastive learning, which allows us to discriminate instances across classes. When we have both
labeled data and unlabeled data, we can perform contrastive learning based on pairs that are gener-
ated separately for the two types of data.
Data Generation Process. Formally, let us consider the case in which we draw n samples as unla-
beled data X = [x1, ∙ ∙ ∙ , Xn] ∈ RdXn from the Gaussian mixture model Eq.(4.1) with p1 = p2 =
…=pr+1. For the labeled data, we draw (r + 1)m samples, i.e., m samples for each of the r + 1
classes in the Gaussian mixture model, and denote them as X = [X1, ∙ ∙ ∙ , X(r+1)m] ∈ Rd×(r+1)m.
We discuss the above case for simplicity. More general versions are considered in Theorem C.2 (in
7
Under review as a conference paper at ICLR 2022
the appendix). We study the following hybrid loss to illustrate how label information helps promote
the performance over the self-supervised contrastive learning:
min L(W) := min LSelfCon (W) + αLSupCon(W),	(4.2)
W ∈Rr×d	W ∈Rr×d
where α > 0 is the ratio between supervised loss and self-supervised contrastive loss.
We first provide a high-level explanation of why label information help learn core features. When
the label information is unavailable, no matter how much (unlabeled) data we have, we can only
take themselves (and their augmented views) as positive samples. In such a scenario, performing
augmentation leads to an unavoidable trade-off between estimation bias and accuracy. However, if
we have additional class information, we can contrast between data in the same class to extract more
beneficial features that help distinguish a particular class from others and therefore reduce the bias.
Theorem 4.1. Suppose the labeled and unlabeled samples are generated as the process mentioned
above. If Assumption 3.1-3.3 hold, n > d	r and let WCL be any solution that minimizes the
supervised contrastive learning problem in Eq.(4.2), and denote its singular value decomposition as
WCL = (UCLΣCLVC>L)>, then we have
Ek sinΘ(UcL,U?)kF .ɪ (r3/2 logd + rdr) + ʌ
1+α d	n 1+α
Moreover, if we have m labeled data for each class and no unlabeled data, then
Ek sin Θ(UCL,
The first bound in Theorem 4.1 demonstrates how the effect of labeled data changes against the
ratio α in the hybrid loss in Eq.(4.2). In addition, compared with Theorem 3.2, when we only have
labeled data (α → ∞), the second bound in Theorem 4.1 indicates that with labeled data being
available, the supervised contrastive learning can yield consistent estimation as m → ∞ while the
self-supervised contrastive learning consists of an irreducible bias term O(r3/2 log d/d). At a high
level, label information help gain accuracy by creating more positive samples for a single anchor
and therefore extract more decisive features. One should notice a caveat that when labeled data
is extremely rare compared with unlabeled data, the estimation of supervised contrastive learning
suffers from high variance. In comparison, self-supervised contrastive learning, which can exploit a
much larger number of samples, may outperform it.
4.2	Information filtering in multi-task transfer Learning
Label information can tell us the beneficial information for the downstream task, and learning with
labeled data will filter out useless information and preserve the decisive parts of core features. How-
ever, in transfer learning, the label information is sometimes found to hurt the performance of con-
trastive learning (Khosla et al., 2020; Islam et al., 2021). In this section, we consider two regimes
of transfer learning - tasks are insufficient/abundant. In both regimes, We provide theories to SUP-
port the empirical observations and further demonstrate how to wisely combine the supervised and
self-supervised contrastive learning to avoid those harms and reach better performance. Specifically,
We consider a transfer learning problem With regression settings. Suppose We have T source tasks
Which share a common data generating model Eq.(2.5). In order to study the case of transfer learn-
ing, the labels are generated in a different Way, that is, for the t-th task, the labels are generated by
yt = hwt, zi∕ν, where Wt ∈ Rr is a unit vector and different across tasks.
To incorporate label information, We maximize the Hilbert-Schmidt Independence Criteria (HSIC)
(Gretton et al., 2005; Barshan et al., 2011), which has been widely used in literature (Song et al.,
2007a;b;c; Barshan et al., 2011). HSIC is defined as HSIC(X, y; W) = X>W>WXHyy>H/(n-
1)2, where W ∈ Rr×d is linear representation to be learned and H = In - (1/n)1n1n> is
the centering matrix. A detailed discussion about its background, motivation and its connec-
tion to the mean squared loss is presented in Appendix A.3. Suppose we have n unlabeled data
X = [xi,…，Xn] ∈ Rd×n and m labeled data for each source task Xt = [X；,…，X"], yt =
[yt,…，ytm], ∀t = 1,...,T where Xi's and Xj's are independently drawn from spiked covariance
model Eq. (2.5), we learn the linear representation via the joint optimization:
T
W ∈Rn×dL(W ):= WmRnXdLSelfCon(W ) - αE HSIC(X t,yt;W),
(4.3)
8
Under review as a conference paper at ICLR 2022
where α > 0 is a pre-specified ratio between the self-supervised contrastive loss and HSIC. (A
more general setting is considered in the appendix, see Section C.2 for details.) We now present a
theorem showing the recoverability of W by minimizing the hybrid loss function (4.3).
Theorem 4.2. Suppose Assumption 3.1-3.3 hold for spiked covariance model Eq.(2.5) andn > d
r, if we further assume that α > C for some constant C, T < r and wt ’s are orthogonal to each
other, and let WCL be any solution that optimizes the problem in Eq.(4.3), and denote its singular
value decomposition as WCL = (UCLΣCL VC>L)>, then we have:
Ek sme(UCL,U ?)kF <√r-τ( M+d++QNm ∧ 1)+√T( Tgq+a 卷+T
d
m
(4.4)
In Theorem 4.2, as a goes to infinity (corresponding to the case where We only use the supervised
loss), Eq.(4.4) is reduced to √r 一 T + T3/2 y/d/m, which is worse than the r3/2 log d/d rate ob-
tained by self-supervised contrastive learning (Theorem 3.2). This implies that when the model
focuses mainly on the supervised loss, the algorithm will extract the information only beneficial for
the source tasks and fail to estimate other parts of core features. As a result, when the target task has
a very different distribution, labeled data will bring extra bias and therefore hurt the transferability.
Additionally, one can minimize the right-hand side of Eq.(4.4) to obtain a sharper rate. Specifically,
we can choose an appropriate α such that the upper bound becomes ,r2(r - T) log d/d (when
n, m → ∞), obtaining a smaller rate than that of the self-supervised contrastive learning. These
facts provide theoretical foundations for the recent empirical observations that smartly combining
supervised and self-supervised contrastive learning achieves significant improvement on transfer-
ability compared with performing each of them individually (Islam et al., 2021).
When the tasks are abundant enough then estimation via labeled data can recover core features
completely. Similar to Theorem 4.2, we have the following result.
Theorem 4.3. Suppose Assumptions 3.1-3.3 hold for spiked covariance model Eq.(2.5) and n >
d r, if we further assume that T > r and λ(r)(PiT=1 wiwi>) > c for some constant c > 0,
suppose WCL is the optimal solution of optimization problem eq.(4.3), and denote its singular
value decomposition as WCL = (UCLΣCLVC>L)>, then we have:
Ek sinΘ(UcL,U?)∣∣f <一r- (r logd +
α+1 d
dr
(4.5)
m
Similar to Theorem 4.2, Theorem 4.3 shows that in the case where tasks are abundant, as α goes
to infinity (corresponding to the case where we use the supervised loss only), Eq.(4.5) is reduced to
Ty/rd/m. This rate can be worse than the √r3^log d/d + ʌʌd/n rate obtained by self-supervised
contrastive learning when m is small. Recall that when the number of tasks is small, labeled data
introduce extra bias term √r 一 T (Theorem 4.2). We note that when the tasks are abundant enough,
the harm of labeled data is mainly due to the variance brought by the labeled data. When m is suf-
ficiently large, supervised learning on source tasks can yield consistent estimation of core features,
whereas self-supervised contrastive learning can not. We also illustrate the different behaviors of
the two regimes via numerical simulations in Fig. 1 right panel. Consistent with our theory, it is
observed that when tasks are not abundant, the transfer performance exhibit a U -shaped curve, and
the best result is achieved by choosing an appropriate α. When tasks are abundant and labeled data
are sufficient, the error remains small when we take large α.
5	Conclusion
In this work, we theoretically prove that contrastive learning, compared with autoencoders, can
obtain a better low-rank representation under the spiked covariance model, which further leads to
better performance in downstream tasks. We also highlight the role of labeled data in supervised
contrastive learning and multi-task transfer learning: labeled data can reduce the domain shift bias in
contrastive learning, but it harms the learned representation in transfer learning. To our knowledge,
our result is the first theoretical result to guarantee the success of contrastive learning by comparing it
with existing representation learning methods. However, in order to get tractable analysis, like many
other theoretical works in representation learning (Du et al., 2020; Lee et al., 2020; Tripuraneni et al.,
2021), our work starts with linear representations, which still provides important insights. Extending
the results to more complex models is an interesting direction of future work.
9
Under review as a conference paper at ICLR 2022
References
Anish Agarwal, Devavrat Shah, and Dennis Shen. On principal component regression in a high-
dimensional error-in-variables setting. arXiv preprint arXiv:2010.14449, 2020.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-
shi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint
arXiv:1902.09229, 2019.
Zhidong Bai and Jianfeng Yao. On sample eigenvalues in a generalized spiked population model.
Journal of Multivariate Analysis ,106:167-177, 2012.
Dana H Ballard. Modular learning in neural networks. In AAAI,, volume 647,pp. 279-284,1987.
Elnaz Barshan, Ali Ghodsi, Zohreh Azimifar, and Mansoor Zolghadri Jahromi. Supervised principal
component analysis: Visualization, classification and regression on subspaces and submanifolds.
Pattern Recognition, 44(7):1357-1371, 2011.
Xin Bing, Florentina Bunea, Seth Strimas-Mackey, and Marten Wegkamp. Prediction under latent
factor regression: Adaptive pcr, interpolating predictors and beyond. Journal of Machine Learn-
ing Research, 22(177):1-50, 2021.
Herve Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological cybernetics, 59(4):291-294, 1988.
Herve Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological Cybernetics, 59:291-294, 2004.
T Tony Cai and Anru Zhang. Rate-optimal perturbation bounds for singular subspaces with appli-
cations to high-dimensional statistics. The Annals of Statistics, 46(1):60-89, 2018.
T Tony Cai, Rungang Han, and Anru R Zhang. On the non-asymptotic concentration of het-
eroskedastic wishart-type matrix. arXiv preprint arXiv:2008.12434, 2020.
Emmanuel J CandeS and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717-772, 2009.
Domagoj Cevid, Peter Buhlmann, and Nicolai Meinshausen. Spectral deconfounding Via perturbed
sparse linear models. Journal of Machine Learning Research, 21:232, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597-1607. PMLR, 2020a.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-
supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020b.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020c.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimi-
native unsupervised feature learning with convolutional neural networks. Advances in neural
information processing systems, 27:766-774, 2014.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Carl Eckart and G. Marion Young. The approximation of one matrix by another of lower rank.
Psychometrika, 1:211-218, 1936.
Jianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning. arXiv preprint
arXiv:1904.05526, 2019.
10
Under review as a conference paper at ICLR 2022
Gene H. Golub and Charles Van Loan. Matrix computations (3rd ed.). 1996.
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised con-
Strastive learning. In International Conference on Machine Learning, pp. 3821-3830. PMLR,
2021.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical de-
pendence with hilbert-schmidt norms. In International conference on algorithmic learning theory,
pp. 63-77. Springer, 2005.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: Anew approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recogni-
tion (CVPR’06), volume 2, pp. 1735-1742. IEEE, 2006.
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510,
2018.
Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai. Triplet-center loss for multi-view
3d object retrieval. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 1945-1954, 2018.
Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky, Richard J. Radke, and
Rogerio Schmidt Feris. A broad study on the transferability of visual representations with con-
trastive learning. ArXiv, abs/2103.13517, 2021.
Ian T Jolliffe. A note on the use of principal components in regression. Journal of the Royal
Statistical Society: Series C (Applied Statistics), 31(3):300-303, 1982.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-
supervised learning: Generative or contrastive. IEEE Transactions on Knowledge and Data En-
gineering, 2021.
George Marsaglia. Choosing a point from the surface ofa sphere. Annals of Mathematical Statistics,
43:645-646, 1972.
Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representa-
tions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 6707-6717, 2020.
11
Under review as a conference paper at ICLR 2022
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Dan Pirjol. The logistic-normal integral and its generalizations. Journal of Computational and
APPIiedMathematics, 237(1):460-469, 2013.
Elad Plaut. From principal subspaces to principal components with linear autoencoders. arXiv
PrePrint arXiv:1804.10253, 2018.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. 2015 IEEE Conference on ComPuter Vision and Pattern Recognition
(CVPR), pp. 815-823, 2015.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NIPS, 2016.
Le Song, Arthur Gretton, Karsten Borgwardt, and Alex Smola. Colored maximum variance unfold-
ing. Advances in Neural Information Processing Systems, 20:1385-1392, 2007a.
Le Song, Alex Smola, Arthur Gretton, and Karsten M Borgwardt. A dependence maximization
view of clustering. In Proceedings of the 24th international conference on Machine learning, pp.
815-822, 2007b.
Le Song, Alex Smola, Arthur Gretton, Karsten M Borgwardt, and Justin Bedo. Supervised feature
selection via dependence estimation. In Proceedings of the 24th international conference on
Machine learning, pp. 823-830, 2007c.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ComPuter
Vision-ECCV2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings,
Part XI 16, pp. 776-794. Springer, 2020.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. In Algorithmic Learning Theory, pp. 1179-1206. PMLR, 2021.
Nilesh Tripuraneni, Chi Jin, and Michael Jordan. Provable meta-learning of linear representations.
In International Conference on Machine Learning, pp. 10434-10443. PMLR, 2021.
Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-
supervised learning from a multi-view perspective. arXiv PrePrint arXiv:2006.05576, 2020.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929-9939. PMLR, 2020.
Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv PrePrint arXiv:2105.15134, 2021.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE conference on comPuter vision
and Pattern recognition, pp. 3733-3742, 2018.
Jianfeng Yao, Shurong Zheng, and ZD Bai. SamPle covariance matrices and high-dimensional data
analysis. Cambridge University Press Cambridge, 2015.
Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via in-
variant and spreading instance feature. In Proceedings of the IEEE/CVF Conference on ComPuter
Vision and Pattern Recognition, pp. 6210-6219, 2019.
Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis-kahan theorem for
statisticians. Biometrika, 102(2):315-323, 2015.
Anru R Zhang, T Tony Cai, and Yihong Wu. Heteroskedastic pca: Algorithm, optimality, and
applications. arXiv PrePrint arXiv:1810.08316, 2018.
12
Under review as a conference paper at ICLR 2022
A Background
A. 1 Distance between subspaces
In this section, we will provide some basic properties of sin Θ distance between subspaces. Recall
that the definition is:
ksinΘ(U1,U2)kF , U1>⊥U2F = U2>⊥U1F.	(A.1)
where U1 , U2 ∈ Od,r are two orthogonal matrices. Similarly, we can also define:
ksinΘ(U1,U2)k2 , U1>⊥U22 = U2>⊥U12.
We first give two equivalent definition of this distance:
Proposition A.1.
ksinΘ(U1,U2)k2F =r-U1>U22F
Proof. Write U = [U1 , U1⊥] ∈ Od,d we have :
r = kU2kF = kU>U2kF =∣∣U⊥U2∣∣F +∣∣U>U2∣∣F ,
then by definition of Sin Θ distance We can obtain the desired equation.	□
Proposition A.2.
ksinθ(u1,U2)kF = 2∣∣U1U> - U2U>kF
Proof. Expand the right hand and use Proposition A.1 We have:
2 kUιU> - U2U> IlF =2(IlUI U> IlF + kU2U> IlF - 2tr(UιU> U2U7))
=1(r + r — 2tr(U>U2U>U1))
=r- IU1>U2I2F = IsinΘ(U1,U2)I2F.
□
With Proposition A.1 and A.2, it,s easy to verify its properties to be a distance function. Obviously,
we have 0 ≤ ∣∣sinΘ (U1,U2)∣∣F ≤ √r and ∣∣sinΘ (U1, U2)∣∣F = ∣∣sinΘ (U2, Ui)∣∣f by definition.
Moreover, We have the folloWing results:
Lemma A.1 (Lemma 1 in Cai & Zhang (2018)). For any U, V ∈ Od,r,
k sinΘ(U,V)∣2 ≤ inf ∣UO - V∣2 ≤ √2∣ sinΘ(U,V)∣2,	(A.2)
O∈Or,r
and
k sinΘ(U,V)∣∣f ≤ inf ∣∣UO — VIIF ≤ √2∣ sinΘ(U,V)∣∣f.	(A.3)
O∈Or,r
Proposition A.3 (Identity of indiscernibles).
IsinΘ(U1,U2)IF = 0⇔ ∃O ∈ Or×r, s.t. U1O = U2
Proof. It’s a straightforward corollary by definition:
IsinΘ(U1,U2)IF =0⇔ U1>⊥U2F =0⇔U2⊥ ⊥ U1
⇔ ∃O ∈ Or×r, s.t. U1O = U2.
□
Proposition A.4 (Triangular inequality).
Isin Θ (U1, U2)IF ≤ Isin Θ (U1, U3)IF + Isin Θ (U2, U3)IF
Proof. By the triangular inequality for Frobenius norm we have:
IU1U1> -U2U2>IF ≤ IU1U1> -U3U3>IF+IU2U2> -U3U3>IF,
then apply Proposition A.2 to replace the Frobenius norm with sin Θ distance we can finish the
proof.	□
13
Under review as a conference paper at ICLR 2022
A.2 Principal component analysis and autoencoders
Autoencoders are popular unsupervised learning methods to perform dimension reduction. Its basic
idea is to learn low dimensional representations for the original data while largely preserving its
salient features. To achieve this goal, autoencoders learn two functions: encoder f : Rd → Rr
and decoder g : Rr → Rd . While the encoder f compresses the high dimensional data to the low
dimensional space, we try to recover the original data based on this low dimensional representation
via the decoder g. Formally, it can be formulated to be the following optimization problem:
minExL(x, g(f(x))).	(A.4)
f,g
Intuitively speaking, optimization problem (A.4) is trying to preserve the most essential features to
recover the original data in the low dimensional representation. In practice, a commonly used form
is to optimize the empirical loss function and choose the loss function to be mean squared error, that
is:
Amin「E — gDE(fAE(χi))k2,	∀i ∈ H	(A.5)
fAE,gDE n	2
where fAE , gDE are usually two neural networks. Now if we take fAE , gDE to be both linear
transformations, i.e.,
fAE(x) = WAEx + bAE, gDE (x) = WDEx + bDE,
where WAE ∈ Rr×d,bAE ∈ Rr,W0E ∈ Rd×r,, bAE ∈ Rd, and denote X = [xi,…，Xn] ∈
Rd×n, the optimization problem (A.5) can be reduced to be:
W Λ min h ~Xx — (WDE (WAE X + bAE1> ) + bDE 1>)kF.
WAE,bAE,WDE,bDE n
It’s shown in Bourlard & Kamp (2004) that the bias term can be reduced by centeralize the data
matrix X, that is, denote x = 1 PZi Xi as the sample mean and X。= X 一 x1> as the centered
sample matrix, then the optimization problem correspond to WAE and WDE can be transfromed to
be:
min	1 ∣∣Xo — WDE WAE XokF ∙
WAE,WDE n
By Theorem 2.4.8 in (Golub & Loan, 1996), the optimal solution is given by the singular value
decomposition of matrix X。，i.e., the eigenspace of X(In 一 ɪ 1n1>)X>, which is actually what
PCA does. This fact indicates that PCA is actually a linear case of autoencoders, which is often
known as undercomplete linear autoencoders. (Bourlard & Kamp, 1988; Plaut, 2018; Fan et al.,
2019)
A.3 Hilbert-Schmidt Independent Criteria
In Gretton et al. (2005), the Hilbert Schmidt Independent Criteria (HSIC) is proposed to measure
the dependence between two random variables by computing the Hilbert-Schmidt norm of the cross-
covariance operator associated with their Reproducing Kernel Hilbert Spaces (RKHSs). Such mea-
surement has been widely used as supervised loss function in feature selection(Song et al., 2007c),
feature extraction(Song et al., 2007a), clustering(Song et al., 2007b) and supervised PCA (Barshan
et al., 2011).
The basic idea behind HSIC is that two random variables, named X and Y, are independent if and
only if any bounded continuous function of the two random variables are uncorrelated. Let F be
a separable RKHS containing all continuous bounded real-valued functions of x from X to R and
G be that of Y, likewise. To each point x ∈ X, there corresponds an element φ(x) ∈ F such that
hφ(x), φ (x0)iF = k (x, x0), where k : X × X → R is a unique positive definite kernel. Likewise,
define the kernel l(∙, ∙) and feature map ψ for G. Denote the joint measure of pχ,y, then the empirical
HSIC is defined to be:
Definition A.1 (Empirical HSIC (Gretton et al., 2005)). Let Z := {(xi, yi) , . . . , (xm, ym)} ⊆
X × Y be a series of m independent observations drawn from pxy . An estimator of HSIC, written
HSIC(Z, F, G), is given by
HSIC(Z, F, G):= (m 一 1)-2 tr(KHLH),
14
Under review as a conference paper at ICLR 2022
where H,K,L ∈ Rm×m, Kij= k (xi, Xj), Lij= l (yi, y) and H = Im - m 1m∕∙
In our problems, we hope to maximize the dependency between learned features WX ∈ Rr×n
and label y ∈ Rn via HSIC, equivalently maximize tr(KHLH) where K is a kernel of WX (e.g.
X>W >W X) and L is a kernel of y (e.g. yy>). Then we obtain our supervised loss corresponding
to parameter W :
HSIC(X,y; W) = --1—2 tr(X>W>WXHyy>H).	(A.6)
(n- 1)
A.3.1 Connection with mean s quared error
In regression tasks, a more commonly used loss function is mean squared error:
1n
Lmse(θ) = - Ekf(Xi,θ) - yikF,	(A.7)
n i=1
where θ is model parameter. In our contrastive learning framework, where we first learn the repre-
sentation via a linear transformation and then perform linear regression to learn a classifier w ∈ Rr
with learned representation. Assuming that both of X and y have been centered and we ignore the
bias term, the model can be viewed as a two layer linear network:
f(X, θ) = w>WX.
Plug this formula back into mean squared error, we have:
LMSE (θ) = — kw>WX - y>kF.
n
Since the label is a scalar, the optimal W will be singular since we perform a 1-dimensional projec-
tion via w . That is, the feature filtering discussed in the main body and if we jointly optimize this
loss with unsupervised contrastive loss, the optimal W should be full rank in general.
On the other hand, the classifier w only involves in the supervised loss function and does not affect
the contrastive loss, thus we can find the optimal solution for w and W sequentially. For any fixed
W such that rank(W X ) = d, which can be achieved via joint optimization, the optimal solution of
w is:
w? = (WXX>W> )-1W Xy.
And the optimal error is:
L(W) =-kX>W>(WXX>W>)-1WXy - ykF
nF
= -(y>y - tr(y>X>W>(WXX>W>)-1WXy)).
Ignoring the constant term y>y and scalar, we can find that the only difference between this loss
function and HSIC (A.6) is the inverse matrix (WXX>W>)-1 which can be viewed as normal-
ization of W . In our contrastive learning framework, this normalization can be achieved by the
regularization term kWW> k2F. Thus we can use the HSIC to replace the standard regression error
which helps us to avoid dealing with singularity and additional parameters in optimization.
B Omitted proofs for Section 3
B.1 Proofs for Section 3.1
In this section, we will prove Theorem 3.1 and 3.2 in Section 3.1. The restatement and proof of them
can be found in B.1 and B.3. Before starting the proof, we first provide a lemma to justify the order
of I(U? ) in the Assumption 3.3.
Lemma B.1 (Expectation of incoherent constant over a uniform distribution).
EU 〜Uniform (Od,r)I (U ?)=Oq log d∙	(B∙D
15
Under review as a conference paper at ICLR 2022
Before starting the proof, we give two technical lemmas to help the proof.
Lemma B.2 (Uniform distribution on the unit ShPere (Marsaglia, 1972)). If x1,x2, ∙ ∙ ∙ ,Xn i.i.d.
〜N(0,1), then (xι//ɪɪi X,…，Xn//ɪɪi X) is uniformly distributed on the unit sphere
Sd = {(xi,…，Xn) ∈ Rn : pn=ι x2 = 1}. 一
Lemma B.3. If xι, x2,…，Xn i.i.d.〜N (0,1) ,then:
E max Xi2 ≤ 2 log(n).
1≤i≤n i
Proof. Denote Y = max1≤i≤n Xi2, then we have:
n
exp(tEY) ≤ E exp(tY) ≤ E ^X exp(tx2) = nE exp(tx2).
i=1
Note that the moment-generating function of chi square distribution with v degrees of freedom is:
MX(t) = (1 - 2t)-v/2 .
Then combine this fact with equation B.1 we have:
exp(tEY) ≤ n(1 — 2t)-2,
which imPlies:
EY≤log(n) - 3,	∀t<1.
一 t	2t ,	2
In particular, take t → ɪ yields:
EY ≤ 2 log(n)
as desired.	□
ProofofLemma B.1. Denote the columns of U as U = [uι, ∙∙∙ , u" ∈ Od,r, we have:
r
EU〜UnifOrm(Od,r)I(U) =EU〜UnifOrm(Od,r) max ɪ3 归JUj l^
i∈[d] j=1
r
≤EU〜Uniform(Od,r)): max lei Uj |
=rEu〜Uniform(Sd) max ∣e>U∣2.
i∈[d]
By Lemma B.2 we can transform this expectation on the uniform sphere distribution into normalized
multivariate Gaussian variables:
2
K	,	、〃m"	maxi∈[d] xi	…
EU~Uniform(Od,r)I(U) = rExι,…，xd	d > ∙	(B.2)
j=1 Xj
where xi, X2,…，Xd are i.i.d. standard normal random variables. Apply Chebyshev,s inequality
we know that:
P(I d X 吟-11 >) ≤ W.
In particular, take = 1 we have:
16
Under review as a conference paper at ICLR 2022
Then take it back into equation B.2 and apply Lemma B.3 we obtain:
2
maxi∈[d] xi2
EU〜Uniform(Od,r)I(U) =rExι,∙∙∙ ,Xd Pd ~
j=1 xj
d
I{Xxj2
i=1
d
I{X
i=1
x2
xj
d
≥ -}
一 2 }
≤rP
2
I rE	maxi∈[d] Xi
十/qxI,…，xd	d 2
j=1 xj2
2r	2
+——-Eχ1 ∙∙∙ XH maxx2
T d	x1, ,xd i∈[d] i
8r	4r log d
― d + d
<
d
2 }
as desired.
□
Lemma B.1 demonstrates the expectation of incoherent constant over a uniform distribution on Od,r
takes the order of O( d log d), thus in the main body We take the order to be the same as it. Again We
need to mention that the order can be chosen to be other functions that decrease to 0 when d → ∞,
and one can easily prove our results under more general assumptions.
NoW, let’s start proving our main results. In the mainbody and section A.2, We have shoWn that in
the linear representation setting, the autoencoder can be deduced to PCA. Here We briefly revieW the
results again, for the autoencoder With an encoder fAE : Rd → Rr and a decoder gDE : Rr → Rd,
it can be formalized as solving the folloWing optimization problem for samples {xi}in=1
1n
fAmin1E n X kxi-g (f	(Xi))k2
In the linear representation setting, Where fAE (X) = WAEX I bAE and gDE(y) = WDEy I bDE,
it has been shoWn that the optimal WAE is given by:
WAE = UAE ΣAE VA>E	,	(B.3)
Where UAE is top-r eigenspace of matrix M := X(I - 1n1n>/n)X>, ΣAE is a diagonal matrix
consists of eigenvalues of M and VAE = [vι,…,vn] ∈ Rr×r can be any orthonormal matrix.
Note that UAE is the top-r left eigenspace of the observed covariance matrix and U? is that of
core feature covariance matrix, and by Assumption 3.2 the observed covariance matrix is dominated
by the covariance of random noise. The Davis-Kahan theorem provides a technique to estimate the
eigenspace distance via estimate the difference betWeen target matrices. We Will adopt this technique
to prove the loWer bound of feature recovery ability of autoencoder in Theorem 3.1.
Theorem B.1 (Restatement of Theorem 3.1). Consider the spiked covariance model Eq.(2.5), under
Assumption 3.1-3.3 and n > d r, let WAE be the learned representation of autoencoder with
singular value decomposition WAE = (UAE ΣAE VA>E)> (as in Eq.(3.1)). If we further assume
{σ2}d=ι are different from each other and σ(i)∕(σ(r)—。，叶]))< Cσ for some universal Constant
Cσ. Then there exist two universal constants Cρ > 0, c ∈ (0, 1), such that when ρ < Cρ, we have
EksinΘ(U?, Uae)∣∣f ≥ c√r.	(B.4)
Proof. Denote M = V2U?U?> to be the target matrix, Xi = U?zi + ξi, i = 1,2,•…n to be
the samples generated from model 2.5 and let X = [xi, ∙ ∙ ∙ ,Xn] ∈ Rd×n, Z = [zι, ∙ ∙ ∙ ,zn] ∈
Rr×n, E = [ξι,…∙，ξn] ∈ Rd×n to be the corresponding matrices. In addition, we write the
column mean matrix X ∈ Rn×d of a matrix X ∈ Rn×d to be X = ɪX 1n1>, that is, each column
of X is the column mean of X. We denote the sum of variance σ2 as σ21m = Pd=ι σ2. As shown
in B.3, autoencoder finds the top-r eigenspace of the following matrix:
11	1	1
M ι = -X (In — -1n1>)X > = TU ?Z + E)(U ?Z + E)> — TU ?Z + E)(U ?Z + E)>.
17
Under review as a conference paper at ICLR 2022
The rest of the proof is divided into three steps for the sake of presentation.
Step 1, bound the difference between M1 and Σ. In this step, we aim to show that the data recovery
of autoencoder is dominated by the random noise term. Note that Σ = Cov(ξ ) = Eξξ >, we just
need to bound the norm of the following matrix:
MI - ς = 1U ?zz >u ?>
n
1
1
1
>

n
n
n
and we will deal with these four terms separately.
(B.5)
1.
For the first term, note that Ezz> =
1U ?ZZ >U ?>
n
ν2Ir, the first term can then be divided into two terms
M + U?(1 ZZ> - Ezz>)U?>.
n
(B.6)
Then apply the concentration inequality of Wishart-type matrices (Lemma D.3) we have:
Ek1ZZ> - Ezz>k2 ≤ (∖/r + r)ν2.
nn
Plug it back into (B.6) we obtain the bound for the first term:
k1UZZ>U>k2 ≤ ∣∣m∣∣2 + kU∣2k1 ZZ> -Ezz>k2kU∣2 ≤ fι +
r + r )ν2.
nn
(B.7)
2.
For the second term, since Z and E are independent, we must have EU ?ZE> = 0, so
apply Lemma D.2 twice we have:
1 EkEZ >U ?k2 =1 Ez [Ee [∣∣EZ >U *∣∣2∣Z]]
n
.1EZ [∣∣Z∣∣2 (σsum + r1∕4√σsum σ(i) + √rσ(1))]
.ɪ Ez [kZ k2 ]√dσ(i)
.L√dσ(i)(r"V + (nr)1/4 V + n1/2V)
√d
.-^σ(i)V.
(B.8)
3.
For the third term, apply Lemma D.3 again yields:
Ek1 EE> - Σ∣
n
2
σ(1).
(B.9)
4.
For the last term, note that each columns of ZE and EE are the same, so we can rewrite is as:
1(U ?Z + E)(U ?Z + E)> = (U ?z + E)(U ?z + ξ)>,
n
where E= 1 Pn=Izi and E
1 Pn= 1 ξi. Since Z and ξ are independent zero mean
sub-Gaussian random variables and Cov(z) = ν2Ir, Cov(ξ ) = Σ, we can conclude that:
Ek 1(U?Z + E)(U?Z + E)>k2 ≤ EkEZ>k2 + 2EkEξ>k2 + Ekfrk2
n
∕ν2 , √d , dσ2i)
.----1 尸 σ(i)ν +	.
nn	n
n
n
n
n
18
Under review as a conference paper at ICLR 2022
To sum up, combine equations (B.7)(B.8)(B.9)(4) together we obtain the upper bound for the 2 norm
expectation of matrix M - Σ:
EkMI- Σk2 . V2 1 +
+ Vdσ(i)ν.	(B.10)
Step 2, bound the sin Θ distance between eigenspaces. As we have shown in step 1, the target matrix
of autoencoder is close to the covariance matrix of random noise, i.e., Σ. Note that Σ is assumed to
be diagonal matrix with different elements, hence its eigenspace only consists of canonical basis ei .
Denote UΣ to be the top-r eigenspace of Σ and {ei}i∈C to be its corresponding basis vectors, apply
the Davis-Kahan Theorem D.1 we can conclude that:
Step 3, obtain the final result by triangular inequality. By Assumption 3.3 we know that the distance
between canonical basis and the eigenspace of core features can be large:
k sinΘ(U*,U∑)kF = kU>⊥U?kF = E ke>U?kF = kU?kF -Eke>U?kF
i∈[d]∕C	i∈C
r2
≥ r — rI(U?) = r — O — log d
d
Then apply the triangular inequality of sin Θ distance (Proposition A.4) we can obtain the lower
bound of autoencoder.
Ek sinΘ(UAE,U?)∣∣f ≥ EIl sinΘ(U?, U∑)∣∣f - EIl sinΘ(UAE, U∑)∣∣f
+ρ
By Assumption 3.2, it implies that when n and d is sufficient large and ρ is sufficient small (smaller
than a given constant Cρ > 0), there exist a universal constant c ∈ (0, 1) such that:
Ek sinΘ(UAE ,U ?)∣∣f ≥ c√r.
□
On the other hand, recall that the optimization problem for self-supervised contrastive learning has
been formulated to be:
min
W ∈Rd×r
1n
-1X
n	i=1
xiPos
∈BiP os
hf (Xi,w ),f(χpos,w )i
∣BPosl
X	hf(xi,θ),f(xNeg ,W )i
Neg	I 女 Negl
xNeg∈BNeg	|Bi |
ii
+λR(W),
(B.11)
where f(x, W) = Wx, R(W) = kWW> k2F /2. To compare contrastive learning with autoencoder,
we now derive the optimal solution of the optimization problem B.11. Let’s start with the general
result for self-supervised contrastive learning with augmented pairs generation (Definition 2.1), and
then turn to the special case for random masking augmentation (Definition 2.2).
Σ
—
Theorem B.2. For two fixed augmentation function g1 , g2 : Rd → Rd, denote the augmented data
matrices as Xi = [gι(xι),…，gι(xn)] ∈ Rd×n and X2 = [g2(x1),…，g2(xn)] ∈ Rd×n, when
the augmented pairs are generated as in Definition 2.1, the optimal solution of contrastive learning
problem (B.11) is given by:
r>
WCL = C X uiσivi>
i=1
19
Under review as a conference paper at ICLR 2022
where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:
X1X2 + x2x> - Xf--------TT(XI + X2)(1r 1> - Ir )(XI + X2)>,	(B.12)
2(n - 1)	r
Ui is the corresponding eigenvector and V = [vι, ∙∙∙ ,vn] ∈ Rr×r Can be any orthonormal matrix.
Proof of Theorem B.2. When augmented pairs generation 2.1 is applied, the contrastive loss can be
written as:
L(W )= 2 kWW >kF -
1n
—TKWtl(Xi),Wt2 (Xii))
n i=1
—
、XihWt^1,(xi∙i) + Wt2(xi), Wtl(Xj ) + Wt2(xi))]
4(n - 1) j6=i
=2 kWW >kF -
1n
—EhWtι(xi), Wt2(xi))
n
i=1
1n
+ 4n占 E EhWtι(xi) + Wt2(xi),Wt1(xj) +Wt2(xi))
n(n - ) i=1 j 6=i
=λ∣∣WW>kF - Ltr(X>W>WX2 + X>W>WXι)
2	F	2n	1	2
+ 4n(n - 1) tr((In1> - In)(XI + X2)>W>W(XI + X2))
=2 ∣WW>kF
-2ntr ((X2X>+X1 x> - 2n⅛ (X1+X2)(1n1> -In)(XI+X2)τ)W >w)
2 λWTW - 2nλ (X2XT + XIXT - 25 - J) (XI + X2)(InIT - In)(XI + X2)T) ||
-2nλ (X2XT + XIXT - 2n⅛(XI + X2)(InIT - In)(XI + X2)T) IF.
2
F
Note that the last term only depends on X , and the first term implies that when WCL is the optimal
solution, AWclWTL is the best rank-r approximation of ⑺匕》XHXt. Then apply Lemma D.4
We can conclude that WCL satisfy the desired conditions.	□
Theorem B.2 shoWs a general result for augmented pairs generation With any augmentation. Specif-
ically, if We apply the random masking augmentation 2.2, We can obtain a more precise result to
characterize the optimal solution. To formally state the result, We need additional notations: for any
square matrix A ∈ Rd×d, We denote D(A) to be A With all off-diagonal entries set to be zero and
∆(A) = A - D(A) to be A With all diagonal entries set to be zero. Then We have the folloWing
corollary for random masking augmentation.
Corollary B.1. Under the same conditions as in Theorem B.2, ifwe use random masking (Definition
2.2) as our augmentation function, then in expectation, the optimal solution of contrastive learning
problem (B.11) is given by:
WCL = C X uiσiviT	,
i=1
where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:
∆(XXτ) - -ɪ- X (1n1> - In)X > ,
n-1 n
(B.13)
Ui is the corresponding eigenvector and V = [vι, ∙∙∙ ,vn] ∈ Rr×r Can be any orthonormal matrix.
20
Under review as a conference paper at ICLR 2022
Proof of Corollary B.1. Following the proof of Theorem B.2, now we only need to compute the
expectation over the augmentation distribution defined in 2.2:
λ	1n
L(W )= 2 kWW >kF - E(t1,t2)” [ -∑^[hWt1(xi),Wt2(xi )i
n i=1
—
4，)、X<Wtι(xi) + Wt2(xi), Wtι(xj) + Wt2(xi)i]]
4(- - 1) j6=i	(B.14)
=2IlWW>kF - E(ti,t2)〜T[2-tr((X2χ> + χιχ>
-大----TT(XI + X2)(1n1> - In)(XI + X2)T)W>W)].
2(- - 1)	n
Note that by the definition of random masking augmentation, we have X1 = AX, X2 = (I - A)X,
which implies X1 + X2 = X. On the other hand, X1 and X2 has disjoint nonzero dimensions,
hence the matrix X1 X2> + X2 X1> only consists of off-diagonal entries and each of the off-diagonal
entry, let’s say xij, appears if and only if ai + aj = 1. Moreover, once it appears, we must have
xij equals to the (i, j) element of XX> . With this result, we can then compute the expectation in
equation (B.14):
L(W ) =2 IlWW >kF - E(tι,t2)〜T [2- tr((χ2χ> + χιχ>
-K-----7Γ(χi + χ2 )(1n1> - In )(XI + X2)T)W >W)]
2(- - 1)	n
=2kWW>kF - Jtr((2∆(XX>) — —ɪ-X(1n1> - In)X>)W>W)
2	2-	2	2(- - 1)
=1 kλW >W -工(∆(XX >) - ɪ X (1n1> - In)X > kF
2	4-λ	- - 1 n
-k 工(∆(XX >) - ɪ X (1n1> - In)X >kF .
4-λ	- - 1 n
By similar argument as in the proof of Theorem B.2, we can conclude that WCL satisfy the desired
conditions.	□
Remark B.1. Note that the two views generated by random masking augmentation have disjoint
non-zero dimensions, hence contrasting such positive pairs yields correlation between different di-
mensions only. That’s why the first term in equation (B.13) appears to be ∆(XXT) where the
diagonal entries are eliminated.
With Theorem B.2 and Corollary B.1 established, we can find that the self-supervised contrastive
learning equipped with augmented pairs generation and random masking augmentation can elimi-
nate the effect of random noise on the diagonal entries of the observed covariance matrix. Since
Cov(ξ) = Σ is a diagonal matrix, and by Assumption 3.3 we know that the diagonal entries
Cov(U ?z) = ν2U ?U ?T only take a small proportion of the total Frobenius norm. Thus contrasting
augmented pairs will preserve the core features while eliminating most of the random noise, and
give a more accurate estimation of core features. To start the proof, we introduce a technical lemma
first.
Lemma B.4 (Lemma 4 in Zhang et al. (2018)). If M ∈ Rp×p is any square matrix and ∆(M) is
the matrix M with diagonal entries set to 0 , then
k∆(M)k2 ≤2kMk2.
Here, the factor ” 2 ” in the statement above cannot be improved.
Then we turn to prove Theorem 3.2.
Theorem B.3 (Restatement of Theorem 3.2). Under the spiked covariance model Eq.(2.5), ran-
dom masking augmentation in Definition 2.2, Assumption 3.1-3.3 and - > d	r, let WCL
be any solution that minimizes Eq.(2.3), and denote its singular value decomposition as WCL =
(UCL ΣCL VCTL )T , then we have
—..	.	__	. ..	_ r3/2 _	_ Zdr
Eksinθ(U , UCL)k F . --j-o log d + y —.	(B.15)
21
Under review as a conference paper at ICLR 2022
Proof. The proof strategy is quite similar to that of Theorem 3.2 and we follow the notation de-
fined in the first paragraph of that proof. As we have shown in Corollary B.1, under our linear
representation setting, the contrastive learning algorithm finds the top-r eigenspace of the following
matrix:
MM2 =— (δ(XX>)-----X(In1> - In)X>)
n	n-1 n
=-∆((U ?Z + E)(U ?Z + E)>)———(U ?Z + E)(U ?Z + E)>
n	n-1
+ / 1 ,、(U ?Z + E)(U ?Z + E)>.
n(n - )
To prove the theorem, first We need to bound the difference between M2 and M. We aim to show that
the contrastive learning algorithm is dominated by the core feature term. Note that Σ = EUzz> U>,
we just need to bound the norm of the following matrix:
M2 - M =(-∆(U?ZZ>U?>) - M) + -∆(U?ZE> + EZ>U?>) + -∆(EE>)
nn	n
——二(U *Z + E)(U *Z + E)> + —-ɪ-(U ? Z + E )(U ?Z + E)>.
n - 1	n(n - 1)
and we will also deal with these five terms separately.
(B.16)
1.	For the first term, we can divide it into two parts:
-∆(U?ZZ>U?>) - M = ∆(-U?ZZ>U?T - M) + ∆(M) - M. (B.17)
nn
Then apply Lemma B.4 and Lemma D.3 we have:
Ek∆(-U?ZZ>U?> - M)k2 ≤ 2Ek-U?ZZ>U?> - M∣∣2 ≤ 2(↑ /r + r)ν2.
n	n	nn
Using the incoherent condition I (U) = O( d log d), we know that:
l∣M - ∆(M)∣2 ≤ V2 max ∣∣e>U*∣2 = V2I(U?) . r logdν2.
i∈[d]	d
Combine the two equations above together we obtain the bound for the first term:
E∣-∆(U?ZZ>U?>) - M∣∣2 ≤ E∣∆(-U?ZZ>U?> - M)∣2 + ∣∣M - ∆(M)∣2
nn
(B.18)
.V 2( d log d+n + ʌ^n).	(B.19)
2.	For the second term, apply equation (B.8) yields:
n	> + EZ>U*>)∣∣2 ≤ nElIEZ>U*>∣∣2 . ^。⑴%	(B.20)
-Ek∆(U ?ZE
3.	For the third term, apply equation (B.9) yields:
Ek-∆(EE>)∣2 = E∣∆(-EE> -Σ)∣2 ≤ 2∣-EE> -Σ∣2 . (∖P + d)b^. (B.21)
n	n	n	nn
4.
For the fourth term, apply equation (4) yields:
Ek二T(U?Z + E)(UZ + E)>∣2 .E∣-(UZ + E)(UZ + E)>∣2
n- 1	n
M √d	dσ21)	(B.22)
.1	σ(i)ν +	.
nn	n
22
Under review as a conference paper at ICLR 2022
5. For the last term, by equations (B.7)(B.8)(B.9) we know:
Ek 1(U ?Z + E)(U ?Z + E)>∣2
n
. ∣Σ∣2
2 d	dd
V +v nσ V + Vn + n
Thus we can conclude that:
Ek n(n1-i)(U ?Z+E)(U ?Z+E)>k2. dσ2i) + rν2.
(B.23)
To sum up, combine equations (B.18)(B.20)(B.21)(B.22)(B.23) together we obtain the upper bound
CjC	.	Γ∙	.	∙ 7l^Γ	Tl ʃ
for the 2 norm expectation of matrix M2 - M :
EkM2- Mk2.ν2 d logd+
+σ21)( n d+n)+σ⑴V
d
n
(B.24)
With the upper bound for ∣∣MM2 - M∣∣2, simply apply Lemma D.1 we can obtain the desired bound
for sin Θ distance:
Ek sinΘ(UcL,U?)kF ≤
一 ..ʌ ..
2√rEkM2 - Mk2
ν2
.√r-12 ν2 (dlog d+
+σ21)( n d+di+σ⑴V
d
n
=√r ( (r log d + ʌ F + r) + ρ-2 ( yd + d ) + PT
d	nn	nn
r3/2
.-log d+
dr
n
Moreover, there exists an orthogonal matrix O ∈ Or×r depending on UCL SUch that:
EkU>UclO - IrkF = EkUCLO-UkF ≤ 2√rEkM2 - Mk2 . r3/2 log d +
V2	d
dr
which finishes the proof.
n
□
In the following, we will show that our results do not change if we applied the same augmentation
(2.2) for autoencoders, which indicates that our comparison is fair. As discussed in Section A.2, we
can ignore the bias term in autoencoders for simplicity, which only serves as centralization of the
data matrix. In that case, we applied random augmentation t1 (x) = Ax and t2 (x) = (I - A)x to
the original data {xi}in=1, and the optimization problem can be formulated as follows:
min	ɪEa[∣AX - WDEWAEAXkF + k(I - A)X - WDEWAE(I - A)XkF]∙ (B.25)
WAE,WDE 2n
Then, similar to Theorem B.2 for contrastive learning, we can also obtain an explicit solution for
this optimization problem.
Theorem B.4. The optimal solution of autoencoders with random masking augmentation (B.25) is
given by:
WAE = WD>E = CXr uiσivi>>,
i=1
where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:
2∆(XX >) + D(XX >),
Ui is the corresponding eigenvector and V = [vι, ∙∙∙ ,vn] ∈ Rr
(B.26)
×r can be any orthonormal matrix.
23
Under review as a conference paper at ICLR 2022
Proof. We first derive the equivalent form for this objective function:
--EA[k AX - WDE WAE AX kF + k(I - A)X - WDE WAE (I - A)XkF ]
2n
=2n Ea[tr(X >A>AX) +tr(X >a>Wde WAE AX) +tr(X >A>W>E WDE WDE WAE AX)
+ tr(X>(I - A)>(I - A)X) + tr(X>(I - A)>WDEWAE(I - A)X)
+ tr(X>(I - A)>W>EW>EWDEWAE(I - A)X)]
= 21n Ea[tr(X >AX) +tr(AXX > a>Wde WAE) +tr(AXX >A>W>E W>E WDE WAE)
+ tr(X>(I - A)X) + tr((I - A)XX>(I - A)>WDEWAE)
+ tr((I - A)XX>(I - A)>W>EW>EWDEWAE)]
=2n EA[tr(X >X ) + tr(MWDE WAE )+tr(MW>E WDE WDE WAE )],
(B.27)
where M := AXX>A> + (I - A)XX> (I - A)>. Note that by Definition 2.2 we have A =
diag(aι,…，ad) and ai follows the Bernoulli distribution, so We have:
EAM = 1∆(XX >) + D(XX >)
(B.28)
Again, by Theorem 2.4.8 in (Golub & Loan, 1996), the optimal solution of B.25 is given by the
eigenvalue decomposition of EAM = 11 ∆(XX>) + D(XXT), up to an orthogonal transformation,
which finishes the proof.	□
With Theorem B.4 established, we can now derive the space distance for autoencoders with random
masking augmentation.
Theorem B.5. Consider the spiked covariance model Eq.(2.5), under Assumption 3.1-3.3 andn >
d	r, let WAE be the learned representation of augmented autoencoder with singular value
decomposition WAE = (UAE ΣAE VA>E)> (i.e., the optimal solution of optimization problem B.25).
Ifwefurtherassume {σ2}d=1 are differentfrom each other and σ1i)∕(σ1r)-。彳叶]))< Cσ for some
universal constant Cσ. Then there exist two universal constants Cρ > 0, c ∈ (0, 1), such that when
ρ < Cρ, we have
EksinΘ(U?, Uae)∣∣f ≥ c√r.	(B.29)
Proof. Step1, similar to the proof of Theorem B.1, we first bound the difference between M :=
∆(XX>) + 2D(XX>) andΣ := Cov(ξξ>). Note that:
kM - Σk2 = kXX> - Σ - 2∆(XX>)k2 ≤ kXX> - Σk2 + 1 k∆(XX> - Σ)k2 + 1 k∆(Σ)k2
(B.30)
Since Σ is a diagonal matrix, then by Lemma B.4 we have:
kM - ∑k2 ≤ 2kXX> - Σk1	(B.31)
Now, directly apply equation (B.7)(B.8)(B.9) we can obtain that:
EkM - Σk2
(B.32)
Step 2, bound the sin Θ distance between eigenspaces. As we have shown in step 1, the target matrix
of autoencoder is close to the covariance matrix of random noise, i.e., Σ. Note that Σ is assumed to
be diagonal matrix with different elements, hence its eigenspace only consists of canonical basis ei.
Denote UΣ to be the top-r eigenspace of Σ and {ei}i∈C to be its corresponding basis vectors, apply
24
Under review as a conference paper at ICLR 2022
the Davis-Kahan Theorem D.1 we can conclude that:
ν
Step 3, obtain the final result by triangular inequality. By Assumption 3.3 we know that the distance
between canonical basis and the eigenspace of core features can be large:
k sinΘ(U*,U∑)kF = kU>⊥U?kF = E ke>U?kF = kU?kF -Eke>U?kF
i∈[d]∕C	i∈C
≥ r — rI(U?) = r — O (号 log d).
Then apply the triangular inequality of sin Θ distance (Proposition A.4) we can obtain the lower
bound of autoencoder.
Ek sinΘ(UAE, U?)∣∣f ≥ Ek sinΘ(U?, U∑)∣∣f - EIl sinΘ(UAE, U∑)∣∣f
≥ √r - o
By Assumption 3.2, it implies that when n and d is sufficient large and ρ is sufficient small (smaller
than a given constant Cρ > 0), there exist a universal constant c ∈ (0, 1) such that:
Ek sinΘ(UAE,U?)kF ≥ c√r.
□
Compared with Theorem 3.1, we can find that random masking augmentation makes no difference
to autoencoders, which justifies the fairness of our comparison between contrastive learning and
autoencoders.
B.2 Proofs for Section 3.2
In this section, we will provide the proof of Theorem 3.3 and 3.4 with both regression and classifi-
cation settings. The statement and detailed proof can be found in Theorem B.6 and B.7.
Before going onto the proof, we clarify our models and assumptions. Let WCL and WAE be the
learned representations based on train data X ∈ Rn×d. We observe a new signal X = U?z + W in-
dependent of X following the spiked covariance model 2.5. For simplicity, assume Z 〜N(0, V2Ir)
and ξ ⊥ z. We consider the two major types of downstream tasks: classification and regression.
For binary classification task, We observe a new supervised sample y following the binary response
model:
y∣Z = Ber(F (<Z,w*>∕ν)),	(B.33)
where F : R → [0, 1] is a known monotone increasing function satisfying 1 - F(u) = F (-u) for
any u ∈ R. Notice that our model B.33 includes logistic models (when F(u) = 1∕(1 + e-u)) and
probit models (when F(u) = Φ(u), where Φ is the cumulative distribution function of standard nor-
mal distribution.) We can also interpret model B.33 as a shallow neural network model with width
r for binary classification. For regression task, we observe a new supervised sample y following the
linear regression model:
y= (Z,w*i∕ν + e,	(B.34)
where e 〜(0, σ2) is independent of Z.
25
Under review as a conference paper at ICLR 2022
In classification setting, We specify 0-1 loss, i.e., 'c(δ)，I{y = δ(X)} for some predictor δ tak-
ing values in {0,1}. For regression task, we employ squared error loss '(δ) ，(y 一 δ(X))2.
Based on some learned representation W, we consider a class of linear predictors, i.e., δw,w (X)，
I{F(w>WX) ≥ 1/2} for classification task and δw,w(X) ，w>WX for regression task, where
w ∈ Rr is a weight vector w ∈ Rr . Note that the learned representation depends only on un-
supervised samples X. Let ED[∙] and EE[∙] the expectations with respect to (X, Z) and (y, x, Z),
respectively.
For notational simplicity, define the prediction risk of predictor δ for classification and regression
tasks as Rc(δ) := ED['c(δ)] and Rr(δ) := ED['r(δ)], respectively. Define Σχ := V2U?U?> + Σ.
Since any representation W can be decomposed into W = V ΣW U> by singular value decompo-
sition, where U ∈ Od,r, {δW,w : w ∈ Rr} = {δU>,w : w ∈ Rr}. Thus we write δU,w for δW,w
with a slight abuse of notation. Our goal as stated above is to bound the prediction risk of predictors
{δW,w : w ∈ Rr } constructed upon the learned representations WCL and WAE, i.e., the quantity
inf w∈Rr EE ['(δWcL,w )] and inf w∈Rr EE ['(δWAE ,w )].
Now we state our result on the downstream task.
Theorem B.6 (Excess Risk for Downstream Task: Upper Bound). Suppose the conditions in Theo-
rem 3.2 hold. Then, for classification task, we have
Ed[ infr EE['c(δw0L,w)] - infr EE['c(δu*,w)]
and for regression task,
ED [ inR r EE ['r (δwcL,w )] - 叫 r EE ['r (δu *,w )]
We obtain the lower bound for the downstream predction risk with autoencoders.
Theorem B.7. Suppose the conditions in Theorem 3.1 hold. Suppose r ≤ rc for some constant
rc > 0. Additionally assume that ρ = Θ(1) is sufficiently small and n d r. For classification
task, assume F is differentiable at 0 and F0(0) > 0. Then,
ED [哪r EE ['c(δuAE,w)] 一 infr EE ['c(δu?,w)] & 1.
For regression task,
ED[ infr Ee['『(6以卫,w)] 一 infr EE['『(δu?,w)] & 1.
For two matrices A and B of the same order, we define A 占 B when A 一 B is positive semi-definite.
The proofs of Theorem B.6 and B.7 relies on Lemma B.8, B.9, B.10, B.11 and B.12 which are
proved later in this section.
Proof of Theorem B.6: Classification Task Part. Lemma B.10 gives for any U ∈ Od,r,
ED Linf r Rc (δu,w) - Uinf r Rc(δu ?,w)]
≤ ((κ(1 + ρ2))3 + κρ2(1 + ρ-2)2 + (KP ∨ 1)-1)Ed[k sinΘ(U, U*)∣∣2].
Substituting U J UAE combined with Assumption 3.2 and K = O(1) concludes the proof. □
Proof of Theorem B.6: Regression Part. Note that under Assumption 3.2 and κ = O(1), (1 +
ρ-2)∕(1 + κ-1ρ-2)2 = O(1). LemmaB.12 gives for any U ∈ Od,r,
Ed[Jnfr Rr(δu,w) - Jnfr Rr(δu?,w)] = O((1 + p-2)Ed[k sinΘ(U,U*)∣∣2]kw*k2).
Theorem 3.2 with substitution U J UAE gives the desired result.
□
26
Under review as a conference paper at ICLR 2022
ProofofTheorem B.7: Classification Part. LemmaB.9 gives that for ci := 1 一 1∕(2κrc) ∈ (0,1),
we can take n d r and sufficiently small ρ > 0 so that ED[k sin Θ(UAE, U?)k2F] ≥ c1r holds.
By Lemma B.11,
ED [winR r Rc(δUAE ,w ) - WinR r Rc ・^ )]
& (1 + P2)3/2 2(	1
& (1 + κρ2)3∕2ρ V+ P
一 κ(r -k sinΘ(UAE,U?)∣∣F)
(1 + P2)3/2 2(	1 r1_ ɔ
(1 + κρ2)3∕2ρ (1+ P2 K c1)r)
≥ (1 + P2)3/2 2(	1	1 ʌ
≥ (1 + κρ2)3/2 p 11+ P 2),
(B.35)
(B.36)
(B.37)
≥
where the last inequality follows since r ≤ rc. If we further take ρ= Θ(1) < 1/2, the right hand
becomes a positive constant. This concludes the proof.	□
Proof of Theorem B.7: Regression Part. From proposition B.1, we have
WinR r Rr(δUAE ,w ) - WinR rRr (δU ? ,w )
=w*>((I + (1∕ν 2)U *>ΣU ?)-1
- U?>UAE(UAEU?U*>Uae + (1∕ν2)U>e∑Uae)-1U>eU?)w?∙
Thus from Lemma B.8,
WinR r Rr (δUAE ,w) - WinR r	*W )
≥ (τ+ρ-2 + P2κ(k sinΘ(UAE, U?)kF - r)) kw?k2.
Using Lemma B.9 and by the same argument in the proof of Theorem B.7: Classification Part, we
conclude the proof.	□
Lemma B.5. For any U ∈ Od,
λmin(ν2U?>U(U>ΣχU)-1U>U?) ≥
ν2
ν 2 +σ2 (1 -k sinΘ(U,U ?)k2).
Proof. Since λmin (AC) ≥ λmin (A)λmin(C) for symmetric positive semi-definite matrices A and
C,
λmin(ν2U ?>U (U >ΣχU )-1U >U ?)
≥ λmin(U>U?U?>U)λmin(ν2(U>ΣχU)-1 )
ν2
≥ λmin(I - (I - UTU*U*TU)) λmaχ(ν2U> U?U?> U + U> ΣU)
ν2
≥ V 2 +%)(1 -k sinΘ(U,U ?)k2),
where We used Weyl's inequality λmin(A + C) ≥ λmin(A) - ∣∣C∣∣2 in the second inequality.	□
Lemma B.6. For any U ∈ Od,r,
ν2
λmax(ν2U?>U(U>ΣχU)-1U>U?) ≤ -ɪ. η，" "*W - 2 .
V 2(1 - k sm0U,U ? )∣2)+ σ2d)
27
Under review as a conference paper at ICLR 2022
Proof. Since IlAC心 ≤ IlAIl2∣∣C∣∣2,
λmax(ν2U*>U[U>ΣχU尸 UTU*) ≤ λmax(νV (UT ΣxU)-1)
2
V
≤
λmin(ν 2U T U *U *t U + U T ΣU)
2
V
≤
λmin(ν2I - V2(I - UTU*U*tU) + UTΣU)
2
V
≤
ν2(1-∣∣ sinΘ(U,U *)∣∣2)+ % ,
where we used WeyrS inequality λmin(A + C) ≥ λmin(A) -IlCll2 and λmin(ν2I + UtΣU) ≥
ν2+ σ2d).	日
Lemma B.7. For any U ∈ Od,r,
∣∣ν2(U*τΣχU*)T - V2U*tU(UτΣχU)-1UtU*∣2
二θ (1 - k sinΘ(U,U *)∣2 + KTP-2 1 + IPIP-2 k sinθ(U, U*)k 2 ).
Proof. Observe that
∣(U*τΣχU*)-1 - U*τU(UτΣχU)-1UτU*∣2
≤ k(U*τΣχU*)T-(UτΣχU)-1∣2 + k(UτΣχU)-1 - U*τU(UτΣχU)-1UτU*∣2
:=(T 1) + (T 2).
For the term (T1),
(T 1) = Il(UTΣχU)-1(UτΣχU)(U*τΣχU*)-1 - (UτΣχU)-1(U*τΣχU*)(U*τΣχU*)-1∣2
≤ I(UTΣχU)T∣∣2∣∣UτΣχU - U*τΣχU*∣∣2∣∣(U*τΣχU*)-1∣∣2.
Note
∣∣UτΣxU - U*τΣxU*∣∣2 = ∣∣V2UτU*U*τU - V2I + UTΣU - U*τΣU*∣∣2
≤ V2Il sinΘ(U, U*)I2 + ∣∣UτΣ(U - U*) + (U - U*)τΣU*心
≤ V21 sinΘ(U,U*)∣∣2+2σ2i)IU - U*∣∣2∙
Also we have λmin(U τΣxU) ≥ v2(1 -∣∣ sinΘ(U, U * )∣∣2) + %)from the proof of Lemma B.6 and
λmin(U*τΣχU*) ≥ V2 + σ2d). Therefore
(T I) ≤ (V 2 + S 2 (1-I s1nΘ(U,U *)I2)+ 呢))(V制 Sin0U,U *)I2+2%)IU - U W
For the term (T 2),
(T2) = I(UτΣχU)-1 - U*τ(U* + (U - U*))(UτΣχU)-1(U* + (U - U*))τU*∣∣2
=Il - U*τ(U - U*)(UτΣxU)-1 - (UτΣxU)-1(U - U*)τU*
-U*τ(U - U*)(UτΣχU)-1(U - U*)τU*∣∣2
≤ V 2(1-I sinΘ(U,U * )I2) + 脸(2IU - U *12 + IU - U *I2).
From Lemma A.1, ∣∣ sinΘ(U, U*)∣∣2 ≤ ∣∣U - U*∣∣2. Finally from these results and ∣∣U - U*∣∣2 ≤
2IU - U*∣2,
∣∣V2(U*τΣχU*)-1 - V2U*τU(UτΣχU)-1UτU*∣2
(____________v2____________V2 + σ2i) iiu *i ʌ
一[ν2(1 - k sin0U, U*)12) + 呢)ν2 + σ2d)	2J .
28
Under review as a conference paper at ICLR 2022
Since LHS does not depend on the orthogonal transformation U J UO where O ∈ Or,r, We obtain
∣∣ν2(U?>ΣχU?)-1 - V2U?>U(U>ΣχU)-1U>U?k2
V2	V 2 + σ2i)
ν2(1-∣ sinΘ(U,U?)∣∣2) + σ2d) V2+¾
o∈1tkUO - U?k2
Combined again with Lemma A.1, we obtain the desired result.
□
Lemma B.8. For any U ∈ Od,r,
λmin(ν2(U*>ΣχU?)-1 - V2U?>U(U>ΣχU)-1U>U?)
22
≥ F——-(r -k sinΘ(U,U?)∣∣F).
V + σ(1)	σ(d)
Proof. Observe
λmin(ν 2(U ?> ΣχU ?)-1 - V 2U ?>U (U >ΣχU )-1U >U ?)
≥ λmin((I + (1/v2)U*>ΣU?)-1) - ∣U?>U(U>U?U?>U + (1/v2)U>ΣU)-1U>U*∣2.
Since U>U?U?>U 占 0, it follows that (U>U?U?>U + (1/v2)U>ΣU)-1 W V2(U>ΣU)-1. Thus
∣∣U ?> U (U >U ?U ?>U + (1/v 2)U >ΣU )-1U >U *∣2
≤ V2λmaχ((U>ΣU)-1)∣U?>Uk2
V2
≤ F∣U?>UkF
σ(d)
V2
=F (r -I sinΘ(U,U ?)∣∣F),
σ(d)
where we used λmaχ((U>ΣU)-1) ≤ 1∕λmin(U>ΣU) ≤ \/%)and ∣∣sinΘ (Ui, U2)kF = r 一
U1>U2 2F from Proposition A.1. Combined with Lemma B.6, we obtain
λmin(v2(U*>ΣχU?)-1 - V2U?>U(U>ΣχU)-1U>U?)
22
≥ 2 , 2	2- (r - k sinθ(U, U ?) IlF).
V + σ(1)	σ(d)
□
Lemma B.9. Suppose the conditions in Theorem 3.1 hold. Fix c1 ∈ (0, 1). There exists a constant
c2 > 0 such that if，r log d∕d ∨ ρ2 ∨ d∕n < c2 ,then
EDk sinΘ(UAE,U?)kF ≥ cir,
where c1 ∈ (0, 1) is a universal constant.
Proof. By Cauchy-Schwartz inequality,
ED k sinΘ(UAE ,U ?)kF - r
≥ (EDk sinΘ(UAE,U?)kF)2 - r
=(EDk sinΘ(UAE,U?)|f -√T)(Edk sEΘ(Uae,U?)∣∣f + √∖
From Theorem 3.1, there exists a constant c3 > 0 such that
ED ksinΘ(U?,Uae)∣f ≥ √r — c3-r^Plogd — c3√r
29
Under review as a conference paper at ICLR 2022
Therefore combined with a trivial bound ∣∣ sinΘ(Uae, U?) ∣∣f ≤ √r,
ED ∣ sinΘ(UAE ,U ? )∣F
where We used p，d/n ≤ ρ2 ∨d/n ≤ ρ2 ∨，d/n since d < n. Thus We can take c2 = 6(1 — c1)∕c3.
This concludes the proof.	□
Lemma B.10. For any U ∈ Od,r,
ED[ if Rc(δu,w) — if Rc(δu?,w)]
≤ ((κ(1 + ρ2))3 + κρP2(1 + ρ-2)2 + (κρ2 ∨ 1)T)ED[k sinΘ(U, U*)∣2].
Proof. Recall that we are considering the class of linear classifiers {δU,w : w ∈ Rr}, where
δu,w (X) = I{F(x>Uw) > 1∕2}. For notational simplicity, write β := Uw and β? := U?w?.
Rc(δu,w) = Pe(δu,w(x) = y) = PE(y = 0, F(X>β) > 1/2) + PE(y = 1, F(X>β) ≤ 1/2).
Since F(0) = 1/2 and F is monotone increasing, the false positive probability becomes
Pe(y = 0, F(X>β) > 1/2) = PE (y = 0, x>β > 0)
=EE [Ee [I{y = 0}∣X,Z]I{X>β > 0}]
=EE[(1 — F(V-1z>U*>β*))I{X>β > 0}].
Write ω := x>β and ω? := V-IzTU*>β*. From assumption, (ω*,ω) jointly follows a normal
distribution with mean 0. Write v?2 := Var(ω?) = w?>w?, v2 := Var(ω) = β> Σxβ, where
Σx := ν2U?U?> + Σ. Let τ := Cor(ω*,ω) = νw*>U*>e/(v*v). By a formula for conditional
normal distribution, we have ω∣ω?〜N(tv“*/v*, v2(1 — T2)). This gives
Pe(y = 0,F(X>β) > 1/2)
=EE[(1 — F(ω*))I{ω> 0}]
=EE[(1 — F3*))Ee[I{ω> 0}∣ω?]]
=Ee[(1 — F(s*))Pe(ω > 0∣ω?)]
E 「/ L/ ?、、E (ω — TVG?/v?	TVG?/v?	?\
=EE (1 — F(ω ))pe —,------2^/2 > --∩----^ι∕2 ω
∖v(1 — τ 2)1/2	v(1 — τ 2)1/2	)
=EE [(1 — FQ?)^(as?/v?)]
=EE[(1 — FQ?)^(as?/v?)I{s? > 0}] + EE[(1 — FQ?)K(as?/v?)I{s? < 0}],
where Φ is cumulative distribution function of N(0,1) and α := t/(1 — T2)1/2. We define Ψf as
Ψf(s2) := 2Eu〜N(o,s2)[F(u)I{u > 0}]. When F(u) = 1/(1 + e-u), Ψf(s2) is called the logistic-
normal integral, whose analytical form is not known (Pirjol (2013)). Since a random variable ω? is
symmetric about mean 0 and F(u) = 1 — F (—u),
Ee[(1 — FQ?)^(as?/v?)I{s? < 0}] = EE[(1 — F(—ω?))(1 — Φ(αω?/v?))I{ω? > 0}]
=Ee[F(ω?)(1 — Φ(αω?/v?))I{ω? > 0}].
Hence
Pe(y = 0,F(X>β) > 1/2)
=EE[(Φ(aω?/v?) + F(ω?) — 2F(ω?湮(as?/v?))I{s? > 0}]
=∣Ψf(v?2) — Ee[(2F(ω?) — 1^(as?/v?)I{s? > 0}].
30
Under review as a conference paper at ICLR 2022
Note that the true negative probability is exactly the same as the false positive probability under our
settings:
Pe(y = 1, F(X>β) ≤ 1/2) = EE [F(X>β*)I{X>β ≤ 0}]
=EE[F(-X>β*)I{X>β ≥ 0}]
=EE[(1 - F(X>β*))I{X>β ≥ 0}]
=Pe(y = 0,F(X>β) > 1/2).
Therefore
Rc(δu,w) = Ψf(v?1 2) - 2Ee[(2F(ω?) - 1)Φ(αω*∕v*)I{ω? > 0}].
Let
Tmaχ,U ：= SUp VW*>U ?> Uw/(w?> W?W> U >ΣχUw)1/2,
Tmɑχ,U ? ：= SUp VW*>w∕(w*> W?W> U ?> ΣχU ?w)1/2.
From Cauchy-Schwartz inequality,
τ2
τmax,U
τ2
max
V 2 w?>U ?>U (U >ΣxU)-1U >U ? w?
w?>w?
v 2 w? > (U ?> ΣxU ?)-1 w?
w?>w?	.
Define αmax,U := τmax,U /(1 - τmax,U ) and αmax,U ? := τmax,U ? /(1 - τmax,U ? ) . Then,
since on the event where ω? > 0, α → Φ(αω?∕v?) is monotone increasing and 2F(w?) 一 1 is
non-negative, we have
JnRr Rc(δu,w) = Ψf(v?2) - 2Ee [(2F(ω?) - 1)Φ(αmaχ,uω*∕v?)I{ω? > 0}]
Jnfr Rc(δu?,w) = Ψf(v?2) - 2Ee[(2F(ω?) - 1)Φ(αmaχ,u?ω*∕v?)I{ω? > 0}].
This yields
Jnfr Rc(δu,w) - Jnfr Rc(δu?,w)
=2Ee[(2F(ω?) - 1)(Φ(αmaχ,u?ω?∕v?) - Φ(αmaχ,uω?∕v*))I{ω? > 0}].
Note that for any a, b ≥ 0,
∣Φ(b) — Φ(a)∣ ≤ φ(a ∧ b)∣b — a∣,
where φ is a density function of standard normal distribution. Observe
Jnfr Rc(δu,w) - Jnfr Rc(δu?,w)
≤ 2Ee[(2F(ω?) - 1)∣Φ(αmax,u?ω*∕v?) - Φ(αmax,uω*∕v?)∣I{ω? > 0}]
.~~~ / (2F (ω* ) - I)Iamax,U ? - αmax,U 1ω*φ((αmax,U ? ∧ αmax,U )ω*∕v* ) -^ ~ d dω*
v? 0	v?
1 ∕∞(2F(ω?) - 1)φ((αmaχ,u? ∧ αmaχ,u)ω*∕v*)dω?
0
.|amax,U? - αmax,U
v?		
Iαmax,U ?	-	αmax,U I
αmax,U ?	∧	αmax,U
Iαmax,U ?	-	αmax,U I
/∞(2F(ω?) - 1)exP(-1〃2(Smax，U? ∧ αmax,U)-2v?2))ω?2)也?
O0	P2π((αmax,U? ∧ αmax,U)~2 v?^ )
(Ψf (((amax,u ? ∧ αmαχ,u ? )-2V?2)) - 1∕2),
αmax,U ? ∧ αmax,U
where we used supu>0 uφ(u) < ∞. Since (a - b) = (a2 - b2)∕(a + b) ≤ (a2 - b2)∕(a ∧ b) for
a, b > 0, and ΨF ≤ 1, we obtain
Iα2	——α2	—I
max,U	max,U
inf Rc(δU,w) - inf Rc(δU?,w) .	2	2	2
w∈Rr	,	w∈Rr	,	α2	U? ∧ α2 U
max,U	max,U
31
Under review as a conference paper at ICLR 2022
When τmax,u? ≥ Tmɑχ,u, since T → T2/(1 — T2) is increasing in τ > 0,
22
inf Rc(δu,w) — inf Rc(δu?,w) . mxu^―maxU
w∈Rr c ,w	w∈Rr c ,w	α2	U
max,U
Tmax,U ? — Tmax,U
(I - Tmax,U? )τmax,U
(B.38)
From Lemma B.5 and B.6, we have
ν2
ν2 + σ2 (I - k sinθ(U,U )k2) ≤ Tmax,U
V2
V 2。≤ T 2 仆
V2 + σ21) — Tmax，U ?
v2 (1 - k sinΘ(U,U ?)k2)+ σ2d)
V 2
V 2+σ2d).
(B.39)
≤
≤
Then, equation B.38 becomes
WinRr Rc(δU,W )- WinR,Rc(δU?,W )
V2 + σ(2d)
V2 +
≤ ν2 + σ2d)
v2(1 — k sinΘ(U,U?)k2)
V2 + σ(21)
(Tmax,U ? — Tmax,U )
≤ (1 +κ
v2(1 — k sinΘ(U,U?)k2)
(κρ2 + 1)(ρ-2 + 1)2
∣∣v2(U *>ΣχU ?)-1 — v2U ?>U (U >ΣχU )-1U >U ?k2
-IP-2)(1-k sinΘ(U,U?)k2)2
k sinΘ(U,U?)k2
κρ2(ρ-2 + 1)2
(1 - k sinΘ(U,U?)k2)2
k sinΘ(U,U?)k2.
where the last inequality follows from Lemma B.7.
On the event where k sin Θ(U, U?)k22 ≤ 1/2,
inRr Rc(δu,w) — inRr Rc(δu*,w) . κρ2(1 + ρ-2)2k sinΘ(U,U*)∣2.
When Tmax,u* < Tmax,u, on the event where k sinΘ(U, U*)k2 ≤ KTP-2/2,
WinRr Rc(δU,W X WinRr	*,W )
V2 + σ(21) V2(1 — k sin Θ(U, U?)k22) + σ(2d)
V2	-V2k sinΘ(U,U?)k2 + σ2d)
(Tmax,U — Tmax,U * )
/ (V + σ2i))2	1
≤ -V2- -V2k sinΘ(U,U?)k2 + σ2d)
× ∣∣v2(U?>ΣχU?)-1 — V2U?>U(U>ΣχU)-1U>U?k2
(1 + P-2 )3
≤ ( - k smΘ(U,+*Pk2)+κ-iρ-2)3 k sin"?)k2
.(κ(1 + ρ2))3k sinΘ(U,U?)k2,
where we used Lemma B.7 again.
In summary, on the event where k sin Θ(U, U?)k2 ≤ κ-1 P-2/2 ∧ 1/2,
infrRc(δU,W) — infrRc(δU*,W)
.((κ(1+ P2))3 + κρ2(1 + ρ-2)2)k sinΘ(U,U?)∣∣2
32
Under review as a conference paper at ICLR 2022
On the other hand, on the event where ∣∣ sinΘ(U, U?)∣∣2 > κ-1ρ-2∕2 ∧ 1/2, We have a trivial
inequality infw∈Rr Rc(δU,w) - infw∈Rr Rc(δU? ,w) ≤ 1. This gives
ED[jnfr Rc(δu,w) - Jnfr Rc(δu?,w)]
.((κ(1 + ρ2))3 + KPQ + p-2)2)Ed[k sinΘ(U, U?)|目
+ PD(k sinΘ(U, U*)∣2 > κ-1ρ-2∕2 ∧ 1/2)
.((κ(1 + ρ2))3 + κρ2(1 + ρ-2)2 + (κρ2 ∨ 1))Ed[k sinΘ(U, U*)∣2],
where the last inequality follows from Markov,s inequality.	□
Lemma B.11. Suppose U ∈ Od,r satisfies 1∕(1 + ρ2) - κ(r - k sin Θ(U, U?)k2F) ≥ 0. Then,
Jnfr Rc(δu,w) - Jnfr Rc®?,w)
& ⅛⅛p^Hi⅛ - κ(r -ksinθ(U,U?)kF)).
Proof. We firstly bound the term τm2 ax,U? - τm2 ax,U. From Lemma B.8,
Tmax,U ? -T1Lχ,U ≥ λmin(ν 2 (U ?> ΣχU ?) - 1 - V2U ?> U (U >ΣχU )-1U >U ?)
ν2
≥ V2 +
ν2
— F (r -k sinΘ(U,U ?)∣∣F).
σ(d)
(B.40)
From assumption, RHS of equation B.40 is non-negative. Then using the inequality a - b = (a2 -
b2)∕(a + b) ≥ (a2 - b2)∕(2a) for a ≥ b ≥ 0,
1
αmax,U ? - αmax,U &
αmax,U ?
(αmax,U ? - αmax,U ? )
≥ (I - τmax,U? ) /	τmax,U? - Tmax,U
τmax,U?	(I - Tmax,U? ) (I - Tmax,U)
From equation B.39 and equation B.40,
αmax,U ? - αmax,U
& VV2+σ2d)
〜[V2
ν2 +
3/2	2
V v⅛
— B (r -k sinΘ(U,U ?)∣∣F))
σ(d)
(1+κ-1ρ-2)1/2(1+P2)3/2 ν⅛
-匚(r -k sinΘ(U,U ?)kF) ).	(B.41)
σ(d)
From the proof of Lemma B.10,
Jnf^ Rc(δu,w) - Jnf^ Rc(δu?,w)
=2Ee[(2F(ω?) - 1)(Φ(amax,u?ω?∕v?) - Φ(αmax,uω*∕v*))I{ω? > 0}].
Note that for any b ≥ a ≥ 0, Φ(b) - Φ(a) ≥ φ(b)(b - a). Since we assume RHS of equation B.40
is positive, αmax,u? ≥ αmax,u. Thus on the event where ω? > 0, αmax,u?ω?∕v? ≥ αmax,uω?∕v?.
Observe
Jnf^ Rc(δu,w) - Jnf Rc(δu?,w)
≥ 2Ee[(2F(ω?) - 1)φ(αmax,u?ω*∕v?)(αmax,u?ω?∕v? - αmax,uω*∕v?)I{ω? > 0}]
=~α (αmax,U? - αmax,U) / (2F(ω*) - 1)ω*-- φ ^φ(αmax,U?ω*∕v*)dω*
v?	0	v?
∞
'	,U ?	,U	(2F(ω?) - 1)ω? exp(-(1∕2)(1 + ɑm^u*)ω*2∕v*2) dω?
v0
'αmaxf* - αmax,U ∕∞(2F ((1 + αmax,u *)T"ω?)- 1)ω? exp(-(1∕2)ω?2) dω?,
1 + αmax,U?	0
33
Under review as a conference paper at ICLR 2022
where in the last equality we transformed w? → (1 + α2max,U? )1/2w?/v?. Since F (u) is differen-
tiable at 0 and F (0) = 1/2,
F(u) - 1/2 = F0(0)u + o(u).
Thus there exists a constant > 0 only depending on F such that 2(F (u) - 1/2) ≥ F0(0)u for all
u ∈ [0, ] since F 0 (0) > 0. This gives
Jnfr Rc(δu,w) - Jnfr Rc(δu?,w)
&
αmax,U? - αmax,U Z70∕∩W1 I C,2	、一1/20,*
-1 + 2-----------F (0)(1 + αmaχ,U ? ) v V
1 + αmax,U ?
× (9+αmax,u?) V 3*2 eχp(-(ι∕2)ω*2) dω*
αmax,U ? - αmax,U	2	-1/2 ?	? 2	? 2	?
& -∏-------------- (1+ αmax,U ? ) 1 v 3 eχp(-(I∕2)ω	dω
1 + αmax,U?	0
&
αmax,U ? - αmax,U	2	-1/2
1 + α2a XU ?	(1 + αmaχ,U ?)
max,U
The last inequality follows since v ? = kw? k = 1 by assumption. It is noted that α2max,U? ≤ ν2∕σ(2d)
from equation B.39. Therefore with equation B.41,
WinRr Rc(δU,w ) - WinRr Rc(SU*,W )
& (1 + Kρ2)3∕2 (I + KTP-2)1∕2(I +『)33/11+ρ-2 - κP2(r -k sinθ(U, U*)kF))
& (Γ⅛22F2Pti⅛ -K(I sinθ(U,U*)kF)).
□
Proposition B.1. For any U ∈ Od,r,
infr Rr(δu,w) = ν2w*>(I - ν2U*>U(V2U>U*U*>U + U>ΣU)-1U>U*)w* + σ2.
>Λ	八 八 C	■ . ■	ɪ-ɪ 1 y- -I	.	1	∙ t -I	/— ― 晨 ―∖Γ∙11	♦	. 1	1 1	. ∙	τ-> .C A
Proofof Proposition B.1. Generate random variables (x, z, ξ, e) following the model equation B.34.
We calculate the prediction risk of δU,W as:
Rr(δu,w) := EE(y - x>Uw)2
=VarE(ν-1 Z>w* + <≡)2 — 2Cove(ν-1Z>w* + <≡, U*Z + ξ)Uw
+ w>U > VarE (U *z + W)Uw
=∣∣w*k2 + σ2 - 2νw*>U*>Uw + WT(V2U>U*U*>U + U>ΣU)w
= (w - A-1b)>A(w - A-1b) - b>A-1b + kw? k2 +σ2,
where A := V2U>U? U?>U + U>ΣU and b := VU>U? w? . From this, we obtain
infr Rr(δu,w) = w*>(l - U*>U(U>U*U*>U + (1∕ν2)U>ΣU)-1U>U*)w* + σ2.
□
Lemma B.12. For any U ∈ Od,r,
ED[Jnfr Rr(δu,w) - Jnfr Rr(δu?W)] = O((1 + ρ-2)Eo[∣ sinΘ(U,U*)k2]∣∣w*k2)∙
Proof of Lemma B.12. From proposition B.1, we have
Jnf^ Rr (δu,w ) - Jnf^ Rr (δu?,w )
=w*>((I + (1∕ν2)U*>ΣU*)-1 - U*>U(U>U*U*>U + (1∕ν2)U>ΣU)-1 U>U*)w*.
34
Under review as a conference paper at ICLR 2022
Note that infw∈Rr Rr(δu,w) — infw∈Rr Rr(δu?,w) ≡ infw∈Rr Rr (δuo,w) — infw∈Rr Rr(δu?,w)
for any orthogonal matrix O ∈ Or,r. Take O ∈ Or,r such that kUO一U*∣∣2 ≤ √2∣∣ sinΘ(U, O)k2
without loss of generality, since we can always take a sequence(Om)m≥1 such that kU Om-U ?k2 ≤
√2k sinΘ(U, O)k2 + 1/m fromLemmaA.1.
Lemma B.7 gives
Jnfr Rr (δu,w ) - Jnfr Rr (δu?,w )
=O( 1 - k sinΘ(U,U?)k2 + κ-1ρ-2 1 +: P1ρ-2 k sinθ(U,U?)k2kw*k2).
On the event where k sin Θ(U, U?)k22 < 1/2,
WinR r Rr (δU,w ) - WinR r Rr(δU ?,w ) = o( (1+ +-ρρ-2)2 k Sinθ(U,U ?)|但心?『).
On the event where k sin Θ(U, U?)k22 ≥ 1/2, we utlize the trivial upper bound
ν2
inf Rr(δu,w) - inf Rr∙(δu?,w) ≤ k(I + ν-2U*>ΣU?)-1||2|m？『≤ 2 ι 2 ∣∣w*k2.
W∈Rr	W∈Rr	ν2 + σ(d)
Combining these results, we have
ED [ if Rr (δu,w ) - inf Rr (δu?,w )]
1	+ ρ-2
./]*+P -2、2ED[k sinΘ(U,U?)k2]kw?k2
(1 + κ-1 ρ-2 )2
+ 1 + J1ρ-2 kw*k2PD(k sinΘ(U,U?)k2 ≥ 1∕√2)
1	+ ρ-2
.“JP .2 ED [k sinΘ(U,U ?)k2]kw?k2,
(1 + κ-1 ρ-2 )2
where the last inequality follows from Markov,s inequality.	□
C Omitted proofs for Section 4
C.1 Proofs for Section 4.1
In this section, we will provide the proof of a generalized version of Theorem 4.1 to cover the
imbalanced setting, the statement and the detailed proof can be found in Theorem C.2. In the main
body, we assume the unlabeled data and labeled data are both balanced for the sake of clarity and
simplicity. Now we allow them to be imbalanced and provide a more general analysis. Suppose
We have n unlabeled data X = [xi,…，Xn] ∈ Rd×n and n labeled data Xk = [xk，…，Xnk] ∈
Rd×nk for class k, the contrastive learning task can be formulated as:
min L(W) := min LSelfCon (W) + LSupCon(W).	(C.1)
W ∈Rr×d	W ∈Rr×d
In addition, we write a generalized version of supervised contrastive loss function to cover the im-
balanced cases:
r+1	nk
LSUpCon(W) = - r+1 X n； XX
k=1 k i=1 j6=i
hWxk,Wχki-Pn= Ps=khWχk,Wxji λkWW>k2,
nk - 1	P —r. ns	2
Σ,
s6=k ns
(C.2)
where αk > 0 is the weight for supervised loss of class k. Again we first provide a theorem to give
the optimal solution of contrastive learning problem.
Theorem C.1. The optimal solution of supervised contrastive learning problem (C.1) is given by :
WCL = C	uiσivi>	,
35
Under review as a conference paper at ICLR 2022
where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:
41n (δ(XXT)-占 X(In1> Tn)X>
r+1
+	1	X Ok
r + 1	nk
k=1 k
JXk(Ink 1 *>k - Ink)X> - P=方Xk1k1>X>
Ui is the corresponding eigenvector and V = [vι, ∙ ∙ ∙ ,vn] ∈ Rr×r Can be any orthonormal matrix.
Proof. Under this setting, combine with the result obtained in Corollary B.1, the contrastive loss
can be rewritten as:
L(W ) = 2 kWW >kF - ； tr(( 1∆(XX >) - 2-^ς^ X (1n1> - In)X >)W >W)
2	2n	2	2(n - 1)
—
r+1	nk
------Ok —
r + 1-nk
k=1 k i=1
J X hWxk ,Wxki - P^
ns
hWxik,Wxjsi .
s6=k j=1
Then we deal with the last term independently, note that:
nk
X
i=1
1	1	ns
「X hWxk ,Wxki-E X XhWxk ,Wxji
nk - 1
nk	nk	ns
X XhWχk, Wxki- P~^ X X XhWxk, Wxsi
i=1 j6=i	t6=k	t i=1 s6=k j =1
-ɪπ tr(Xk(Ink Ink-Ink)X>W>W) - P^— X tr(Xk 1k 1>X>W>W) ∙
nk - 1	t6=k nt s6=k
Thus we have:
L(W) = 2 kWW >kF- 41n tr(g(XX T)-占 X (Inl>- In)X T)W >w
r+1
r+1 X Ok [「tr(Xk (Ink Ink- Ink )X>W>W)
k=1 k k
P^— X tr(Xk 1k 1>X>W >W)].
t6=k nt s6=k
Then by similar argument as in the proof of Theorem B.2, we can conclude that the optimal solution
WCL must satisfy the desired conditions.
With optimal solution obtained in Theorem C.1, we can provide a generalized version of Theorem
4.1 to cover the imbalance cases.
Theorem C.2 (Generalized version of Theorem 4.1). If Assumption 3.1-3.3 hold, n > d r and
let WCL be any solution that minimizes the supervised contrastive learning problem in Eq.(C.1),
and denote its singular value decomposition as WCL = (UCLΣCLVCTL)T, then we have
1
—
—
□
EksinΘ(UCL,U)kF
log d +
dr
nk
+ r+1X Ok [X
k=1	s6=k
1 r+1	T 1 r+1	T	ns
Where T , 4 ∑k=1 piμkμk	+ r+1	∑k = 1 0k (μkμk	- ∑s=k	P=	n
1 (μkμ> + μsμ>)) .
36
Under review as a conference paper at ICLR 2022
Proof. For labeled data X = [xι, ∙ ∙ ∙ ,xrt∖, we write it to be X = M + E, where M = [μι,…，μn]
and E = [ξι,…，ξr] are two matrices consisting of class mean and random noise. To be more
specific, if Xi subject to the k-th cluster, then μ% = μk and ξi 〜N(0, Σk). Since the data is
randomly drawn from each class, μ% follows the multinomial distribution over μ1, ∙…，μr with
probability pi, ∙ ∙ ∙ ,Pr+ι. Thus μ% follows a subgaussian distribution with covariance matrix
N = Pk=I Pk μk μ>.
As shown in Theorem C.1, the optimal solution of contrastive learning is equivalent to PCA of the
following matrix:
r+1
+ r+ι X nk[ nτ-ιXk (Ink Dkfk )X>
-------1
1
Pt=k nt
—
X 2(Xk 1k 1>x> + XS 1s1>XJ)].
s6=k
Again we will deal with these terms separately,
1.	For the first term, as we have discussed, X can be divided into two matrices M and E, each
of them consists of subgaussian columns. Again we can obtain the result as in (B.24) (the
proof is totally same):
Ekng(XX>)- n-1 X(In1>- In)X >)- N k2. ν2( dlog d+ʌ/r)+σ21) 也
(C.3)
2.	For the second term, notice that:
nk
Xk (Ink 1>k - Ink )X> = XX(μk + ξ )(μk + ξj )>
i=1 j6=i
nk	nk	nk
=nk(nk - 1)μk μ> + (nk - 1)μkX ξi )> + (nk - 1)(£ ξk)μ> + XX ξi j,
i=1	i=1	i=1 j6=i
(C.4)
and that:
1
Pt=k nt
Xk1k1s>Xs>
s6=k
nk	ns
P=^ X X (μk+ξi) X(μs+针
1
Pt=k nt
ns
£[nk nsμkμ> + nkμk (E ξj)>
s6=k	j=1
(C.5)
Since ξ 〜N(0, Σk), we can conclude that:
nk	nk	ns
+ ns X 球 μ> + X 6 X 铲.
i=1	i=1	j=1
1 nk	u
Ek 嬴 X ξikk2 ≤t
(C.6)
Moreover, we have
nk
F——"Ek XXξiξjτk2 ≤ F——"EkEkE>k2 + -LI
nk (nk - 1)	i=1 j6=i	nk (nk - 1)	nk - 1
.
nk (1)
Ekξkξk>k2
(C.7)
Take equation (C.6) and (C.7) back into (C.4) we can conclude:
Ek nk (nk- 1) Xk (Ink 1>k - Ink )X> - μkμ>k2 . nd nσ⑴√rν + nσ21). (C⑻
37
Under review as a conference paper at ICLR 2022
On the other hand, by equation (C.6) we know:
Ek
1
Pt=k nt
ns
XXξjsk2
s6=k j=1
≤
s6=k
ns
t6=k nt
Notice that:
Ek ns X ξsk2. X ∑⅛ &σ(1).
(C.9)
Ek
1	1
Pt=k nt nk
nk	ns
XX ξkX ξsTk2 ≤ EkXPnV ξkξs>k2
s6=k i=1 j=1	s6=k t6=k t
≤ XPnsn Ekξkξs>k2 . X
s6=k	t6=k t	s6=k
ns	d
(C.10)
∑t=k nt √nkns,
Thus take equations (C.9) and (C.10) back into equation (C.5) we have:
Ek — P	X XkIk 1>X> - X P^_s	μkμ>k2
nk	t6=k nt s6=k	s s	s6=k	t6=k nt	s
.V nsd
〜W Pt=k nt
(C.11)
(C.12)
Then combine equations (C.3)(C.8)(C.11) together, we can obtain the following result:
Since We have assumed that rank(Pk=1 Pkμkμ>) = r We can find that the top-r eigenspace of
matrix:
1 r+1	1 r+1	n 1
T =4 Epi〃k〃> + r+1∑αk μkμ> -E P~s— x(μkμ> + μsμ>)
4k=1	k r+1k=1	k	s6=k	t6=k nt 2	s	k
is spanned by U?, then apply Lemma D.1 again We have:
ʌ
ν2 I r3/2	∕dr	1 r+1
WI -Γlogd +V√ + FHα>
s6=k
□
Roughly speaking, since ∣∣μ> ∣∣ = √rν and Pk=I p>μ> = 0, approximately we have 入 VN)≈
—.-——1口4 ]. Although we can not obtain the closed-form eigenvalue in general, in a special case,
mink∈[r] [1+αk ]
38
Under review as a conference paper at ICLR 2022
where α = αι = •…=a『+i, m = nι = n = •…=n『+i and ^^ɪ = pi = p2 = •…=pr+ι, it's
easy to find that:
E 5(Mkμ> + μsμk) = -μkμ>,
s6=k
which further implies that:
1 r+1	1 r+1	1	1	1
T = 4 Epk μk μ>	+ r+1 Ea(I	+ r )μk μ>,	λr(T)	= [4	+ a(i + ∕]λ(N).
k=1	rk=1	r	r
and we can obtain the result in Theorem 4.1.
C.2 Proofs for Section 4.2
In this section, we will provide the proof of generalized version of Theorem 4.2 and 4.3 to cover the
imbalanced setting, the statement and detailed proof can be found in Theorem C.4 and C.5. First
we prove a useful lemma to illustrate that supervised loss function only yields estimation along a
1-dimensional space. Consider a single source task, where the data x = U?z + ξ is generated by
spiked covariance model and the label is generated by
y= hw?, zi
suppose We have collect n labeled data from this task, denote the data as X = [χ1,χ2,…，Xn] ∈
Rd×n and the label y = [y1,y2, ∙∙∙ ,yn] ∈ Rn, then we have the following result.
Lemma C.1. Under the conditions similar to Theorem 3.2, we can find an ^vent A such that
P(AC) = O(pd∕n) and:
E --1—^XHyyTHX> - V2U*w*w*>U?>
(n -1)2
F I{A}. vdσ(ι)ν.
(C.13)
The proof strategy is to estimate the difference between the two rank-1 matrices via bounding the
difference of the corresponding the vector component. We first provide a simple lemma to illustrate
the technique:
Lemma C.2. Suppose α, β ∈ Rd are two vectors, then we have:
kαα> - ββ>kF ≤√2(kak2 + kβk2)ka - βk2.
Proof. Denote a = (ai,…，ad), β = (βι,…，βd), then we have:
dd	dd
∣∣aa> - ββ>kF ≤ ^X^X Iaiaj - BiBj∣2 ≤ 2^X^X Iaiaj- aiβj∣2 + Iaiej- βiβj12
i=1 j=1	i=1 j=1
dd
≤2 X X IaiI2Iaj - BjI2 + IBjI2Iai - BiI2 ≤ 2(∣a∣22 + ∣B∣22)∣a - B∣22
i=1 j=1
≤2(∣a∣2+∣B∣2)2∣a-B∣22.
Take square root on both side we can finish the proof.	□
Now we can prove the Lemma C.1.
Proof of Lemma C.1. Clearly, we have:
k /	∖、2 XHyy γHX> - ν2U?w?w?>U*>∣F
(n - 1)2
≤ / J k-12XHyy>HX> - V2U*w*w*>U*>∣F + 2n +12 ∣ν2U*w*w*>U*>∣F
(n -1)2 n2	(n - 1)2
.k -12XHyy>HX> - ν2U*w*w*>U*>∣F + rν2,
n2	n
39
Under review as a conference paper at ICLR 2022
thus We can replace the ⑺匕产 with 1 in equation (C.13) and conclude the proof. Denote N，
n12 XHyyTHX>, note that both of N and Uw?w?> U > are rank-1 matrices. We first bound the
difference between 1 XHy and Uw?:
k1 XHy - VU?w?k =k-1(U?Z + E)HZTw? - VU?w?k
n	nν
≤k-1(U?Z + E)HZT - νU?k2
nV
≤ 1(k1 U?ZZT - V2U?k2 + 1 kEZTk2 + 1 kU*ZZt∣∣2 + 1 kEZτk2).
Vn	n	n	n
(C.14)
We deal with the four terms in (C.14) separately:
1.	For the first term, apply Lemma D.3 we have:
Ek1U?ZZT - ν2U?k2 ≤ Ek1 ZZτ - V2Irk2 ≤ ∣r + Jr)V2.	(C.15)
n	n	nn
2.	For the second term, apply Lemma D.2 twice we have:
1	EkEZ Tk2 =1 Ez [Ee [∣∣EZτk2∣Z]]
nn
.1EZ [∣∣Z∣∣2 (σsum + r1∕4√σsum σ(i) + √rσ(i))]
.1EZ [kZ k2 ]√dσ(i)	(C.16)
.!√dσ(i)(r"V + (nr)1/4 V + n1/2V)
√d
.√nσ(1)"
3.	For the third term and fourth term, from equation (4) we know:
E1 kU*ZZτk2 + EIkEZTk2 ≤ Ek怒Tk2 + EkaTk2 ≤ IV2 + JdVσ(i). (C.17)
n	n	nn
Combine these three equations (C.15)(C.16)(C.17) together we have:
Ek1 XHy - VU?w?k . ʌ∕dσ(i).
nn
With equation (C.18), we can now turn to the difference between N and Uw?w
C.2 we know that:
(C.18)
?TU T. By Lemma
kN - V2U?w?w?TU?TkF . (k1 XHyk + kVU*w*k)k1 XHy - VU?w?|k.
nn
Using Markov’s inequality, we can conclude from (C.18) that:
,“1	+ +“	、	Ek1 XHy - VU?w?k	dd
P(k-XHy - VU?w?k ≥ V) ≤	_--L . ʌ/-.
n	Vn
Then denote A = {ω : k 1 XHy — V2U*w*∣∣2 < V} We have:
EkN - V2U?w?w?TU?TkFI{A} .E(k1 XHyk + ∣∣VU?w?k)k1 XHy - VU?w?|kI{A}
nn
.VEkLXHy - VU?w?k2 . ∖ 色G(1)V.
nn
which finished the proof.
□
40
Under review as a conference paper at ICLR 2022
In the main body, we assume the number of labeled data and the ratio of loss function is both
balanced. Now we will provide a more general result to cover the imbalance occasions. For-
mally, suppose we have n unlabeled data X = [xi, ∙ ∙ ∙ ,xn ∈ Rd×n and n labeled data Si
Xi = [x1,…，xn],yi = [y1, ∙ ∙ ∙ , yin1 ], ∀i = 1,…T for source task , We learn the linear rep-
resentation via joint optimization:
T
WmRn×dL(W ):= WmRn×d Gg(W)- X ai HSIC(X t，yt； W )，
(C.19)
To investigate its feature recovery ability, we first give the following result.
Theorem C.3. For optimization problem C.19, if we apply augmented pairs generation 2.1 with
random masking augmentation 2.2 for unlabeled data, then the optimal solution is given by:
WCL = CXruiσivi>! ,
where C > 0 is a constant, σi is the i-largest eigenvalue of the following matrix:
41n G(XX >)- n^-ι X(In 1>- In)X >)+ X
i=1
αi
(ni - 1)2
XiHniyiyi>HniXi>),
Ui is the corresponding eigenvector V = [vι, ∙ ∙ ∙ ,vr] ∈ Rr×r can be any orthogonal matrix and
Hni = Ini — n: 1电11 is the centering matrix.
Proof. Under this setting, combine with the result obtained in B.1, the loss function can be rewritten
as:
L(W) = 2kWWrkF - 2n tr(Q∆(XX>) - 2n41yX(1n1> - In)X>) W>W
T1
-∑ α (n--1p tr(X> WTWXiHyy H)
λ
2
WW> -去 G(XX>)- n-iX(In1> -In)X>
T
-X
i=1
λ
αi
λ(ni - 1)2
XiHniyiyi>HniXi>)
24nλ
∆(XX> ) - -ɪ-X(1n1> - In)X>
n - 1 n
—
2
F
1
T
+ X λ(ni - 1)2 XiHniyiy>HniXir
2
F
Then by similar argument as in the proof of Theorem B.2, we can conclude that the optimal solution
WCL must satisfy the desired conditions.
□
Then we can give the proofs of Theorem 4.2 and Theorem 4.3 under our generalized setting, one can
easily obtain those under balanced setting by simply setting α = α1
αT and m = n1
•…=nτ, which is consistent with Theorem 4.2 and Theorem 4.3 in the mainbody.
Theorem C.4 (Generalized version of Theorem 4.2). Suppose Assumption 3.1-3.3 hold for spiked
covariance model Eq.(2.5) andn > d r, if we further assume that T < randwt ’s are orthogonal
to each other, and let WCL be any solution that optimizes the problem in Eq.(C.19), and denote its
singular value decomposition as WCL = (UCLΣCLVC>L)>, then we have:
Ek sin(Θ(UcL,U?))kF .(—
mn
√r-r
i∈[T]{αi, 1}
Tt	wr1	, β,
+ mini∈fτ]&)(d log d + ∖n)
+ X(√r-Tαi +mini∈[τ]{αi, 1} +、不久行 +mini∈[τ] a，
i=1	mini∈[τ]{ai, 1}	mi&∈[t] ai
）心
ni
41
Under review as a conference paper at ICLR 2022
Proof. As shown in Theorem C.3, optimizing loss function (C.19) is equivalent to find the top-r
eigenspace of matrix
41n G(XXT)-占X(In1>-In)X>)
T
+X
i=1
XiHniyiyi>HniXi>.
Again denote M2，n(∆(XX>) - ɪX(1n1> - In)X>) and Ni，(niɪpXiHyiy>HX>.
By equation (B.24) we know that:
EpM2 - Mk2 . ν2 d logd+
d
+σ(I)VVd
ByTheoremC.1 we know that for each task Si, we can find an event Ai such that P(Ai) = O(J n):
EkNi- V2U*WiW>U?>kFI{Ai} . Jf(1)V.
The target matrix is N = ν2U?U?> + PT=I αiν2U?wiwTU?>, and we can obtain the upper bound
i' .Λ 1 ∙ i'i'	Λ	^7∖ T	1 TCT
for the difference between N and N :
1T
EkN - Nk2I{∩幺1A,} ≤ -EkMM2 - Mk2 + EaiE∣∣Ni - V2Uw,w>U>∣∣fI{Ai}
i=1
.ν2(d logd+JI+n)+σ2i) V nd+n + σ(1)ν
(C.20)
i=1
ν2
T
We divide the top-r eigenspace UCL of WCL WC>L into two parts: the top-T eigenspace UC(1L) and
top-(T + 1) to top-r eigenspace UC(2L) . Similarly, we also divide the top-r eigenspace U? of N into
two parts: U ?(1) and U?(2). Then apply Lemma D.1 we have we can bound the sin Θ distance for
each parts: on the one hand,
Ek sin (Θ(UC1L) ,U ?⑴))∣∣f
=Ek sin(Θ(UC1L), U?⑴))kFI{∩T=ιAi} + Ek sin(θ(uC1L, U?⑴))||尸I{∪T=MC}
FTEkN- ”2： + √P(∪T=ιAC)
λ(T)(N) - λ(T+1)(N)
.
mini∈[T] αiν2
.W (
mini∈[T] αi
On the other hand,
d logd+π)十 √T XX
i=1
αi +mini∈[τ] αi Λd
mini∈[T] αi
ni
Ek sin(θ(UCL ,U ?(2)))kF
=Ek sin(θ(UC2L, U?(2))) kFI{∩T=1 Ai} + Ek sin(θ(U^, U?(2))) ||尸I{∪T=ιAf }
≤	√-EkN - Nk2I{∩T=ιAi}	+ √7-τP(UT AC)
≤ min{λ(τ )(N) - λ(τ +。(N ),λ(,)(N)} +v	( i=1 i )
.——"- T [ ] 2 (V2 i log d + σ21)
mini∈[T]{αi, 1}V2	d	(1)
..√r - T K(r log d+rd!+VZr—T	. J
mini∈[T]{αi, 1} d	d	i=1 mini∈[T]{αi, 1
42
Under review as a conference paper at ICLR 2022
Note that:
k sin(Θ(UcL,U?))kF
=r- kUC>LU?k2F
≤r- kUC(1L)>U?(1)k2F- kUC(2L)T U ?(2)k2F
≤ T - kuC* 1L>u *(1)kF + (r - T) - kuC2L>u ?(2)kF
≤ k sin(θ(uC1L, U?⑴))kF + k sin(θ(uCL, U?⑴))kF,
and the sin Θ distance has trivial upper bounds:
k sin(θ(uC1L, U?⑴))kF ≤ T, k sin(θ(uC2L, U?⑵))kF ≤ r - T
Thus we can conclude:
Ek sin(Θ(UcL,U?))kF
≤ Ek sin(θ(UC1L, U?⑴))kF + Ek 研。(。浣,U?(2))) ||尸
.√. I ] I r r log d+r!+X E αi+侬隈,{；「} rd! ∧√r=T
mini∈[T]{αi, 1} d	n	i=1	mini∈[T]{αi, 1} ni
+ (	(r log d +ʌ 昌 + X √Tαi + min 闫T] α ʌ /ɪ) ∧ √T.
mini∈[T] αi d	n i=1 mini∈[T] αi ni
□
Theorem C.5 (Generalized version of Theorem 4.3). Suppose Assumptions 3.1-3.3 hold for spiked
covariance model Eq.(2.5) and n > d r, if we further assume that T ≥ r and PiT=1 αiwiwi>
is full rank, suppose WCL is the optimal solution of optimization problem eq.(C.19), and denote its
singular value decomposition as WCL = (UCLΣCLVC>L)>, then we have:
Ek sin(Θ(UcL, U ))kF . --	√T------------- (r log d +
1	+ ν2λ(r)(PiT=1 αiwiwi>) d
T
+ √r X
i=1
___________α____________
1 + ν2λ(r)(PiT=1 αiwiwi>)
Proof. The proof strategy is similar to that of Theorem 4.2, here the difference is that each direc-
tion can be accurately estimated by the labeled data and we don’t need to separate the eigenspace.
Directly applying Lemma D.1 and equation (C.20) we have:
Ek sin(Θ(UcL,U?))kF
=Ek sin(Θ(U", U?))kFI{∩T=ιAi} + Ek sin(Θ(U°L, U?))∣∣fI{∪T=iAC}
.√7EkN -N峭碓1 Ai} +√P(∪乙AC)
λ(r)(N)
ν2 + ν2λ(r)(PiT=1 αiwiwi>)
V2 d log d + σ2i)
----------^T~-------I ʒ log d +
1 + λ(r)(Pi=1 αiwiwi>) d
1 + λ(r) ( i=1 αiwiwi
□
D Useful lemmas
In this section, we list some of the main techniques that have been used in the proof of the main
results.
43
Under review as a conference paper at ICLR 2022
ɪ	■ -¼ Y ∕E1	C ♦ ʌ T .	1 ∕cc< L、、 ɪ . ŋ √^S	_ τπ> n V n 1	.	∙	∙ .1	ι
Lemma D.1 (Theorem 2 in Yu et al. (2015)). Let Σ, Σ ∈ Rp×p be symmetric, with eigenval-
ues λ1 ≥ . . . ≥ λp and λ1 ≥ . . . ≥ λp respectively. Fix 1 ≤ r ≤ s ≤ p and assume that
min (λr-1 - λr, λs - λs+1) > 0 where λ0 := ∞ and λp+1 := -∞. Let d := s - r + 1, and let
V = (Vr ,Vr+ι,...,Vs) ∈ Rp ×d and V = (Vr ,Vr+ι,...,Vs) ∈ Rp×d have orthonormal columns
satisfying Σvj = λjVj and ΣVj = %Vj for j = r,r +1,...,s. Then
.. ,ʌ ...
k sinΘ(V, V)∣∣f ≤
2min (d1/2|E - Σ∣∣2,k∑ - Σ∣∣f
min (λr-i — λr, λs — λs+ι)
Moreover there exists an orthogonal matrix O ∈ Rd×d such that
..ʌ ʌ ..
kV O - V kF ≤
23/2 min (d1/2k£ - Σ∣∣2,k∑ - ∑kF
min (λr-1 - λr ,λs - λs+1)
Lemma D.2 (Lemma 2 in Zhang et al. (2018)). Assume that E ∈ Rp1 ×p2 has independent sub-
Gaussian entries, Var (Eij ) = σi2j , σC2 = maxj i σi2j , σR2 = maxi j σi2j , σ(21) = maxi,j σi2j .
Assume that
kEij/σijkψ2 := maxq-1/2 {E(IEijI Ej)q}1/q ≤ κ
2	q≥1
Let V ∈ Op2,r be a fixed orthogonal matrix. Then
P (kEV k2 ≥ 2 (σC + x)) ≤ 2exp
5r-min{ κ4σχ)σ2,
⅛))
EkEV∣∣2 . σc + κr1/4 (。⑴。。)1/2 + κr1∕2σ(i).
Lemma D.3 (Theorem 6 in Cai et al. (2020)). Suppose Z is a p1 -by- p2 random matrix with inde-
pendent mean-zero sub-Gaussian entries. If there exist σ1 , . . . , σp ≥ 0 such that kZij /σi kψ ≤ CK
for constant CK > 0, then
EIIZZ > - ezz>∣∣2 . X σ2 + qp2 X σ2 ∙ max σi.
Lemma D.4 (The Eckart-Young-Mirsky Theorem (Eckart & Young, 1936)). Suppose that A =
UΣVT is the singular value decomposition ofA. Then the best rank- k approximation of the matrix
A w.r.t the FrobeniUS norm, ∣ ∙ ∣∣f, is given by
k
Ak =	σiuiViT .
i=1
that is, for any matrix B of rank at most k
kA - Ak kF ≤ kA - BkF.
E	Numerical experiments
E.1 Supporting empirical results in related works
In this section, we list some recent empirical evidence that provides sound support to our theory:
Contrastive learning outperforms generative self-supervised learning. In Figure 1 of Chen
et al. (2020a) and Figure 5 of Liu et al. (2021), it’s observed that contrastive learning has superior
performance compared to the generative approach. These results are consistent with our theory
in Theorem 3.1-3.4, where we show that contrastive learning achieves better performance on both
feature recovery and downstream tasks compared with autoencoder, a representative generative self-
supervised learning method.
44
Under review as a conference paper at ICLR 2022
Supervised contrastive learning improves downstream accuracy. In Table 2 of Khosla et al.
(2020) and the first column in Table 4 of Islam et al. (2021), supervised contrastive learning shows
significant improvement with 7%-8% accuracy increase on ImageNet and Mini-ImageNet. This ob-
servation is consistent with our finding in Theorem 4.1, where we prove that supervised contrastive
learning can achieve a better upper bound in feature recovery compared with self-supervised con-
trastive learning.
Label information may hurt transferability in contrastive learning. In Table 4 of Khosla et al.
(2020) and Table 4 of Islam et al. (2021), where the supervised contrastive learning hardly increases
the predictive accuracy compared to the self-supervised contrastive learning (the difference of mean
accuracy is less than 1%) and can harm significantly on some datasets (e.g. 5.5% lower for SUN
397 in Table 3 of Khosla et al. (2020)). These results indicate that some mechanisms in supervised
contrastive learning hurt model transferability since the improvement on source tasks is significant.
Moreover, in Table 4 of Islam et al. (2021), it is observed that combining supervised learning and
self-supervised contrastive learning together achieves the best transfer learning performance com-
pared with each of them individually. These findings are consistent with our theory in Theorem
4.2, where we show that the error can increase as α (the ratio between supervised learning loss and
self-supervised learning loss) grows and the optimal error is achieved when choosing a moderate α,
i.e., combining self-supervised learning and supervised learning together.
E.2 Simulation with synthetic data
To verify our theory, we conducted numerical experiments on the spiked covariance model (2.5)
under a linear representation setting. As we have explicitly formulated the loss function and derive
its equivalent form in the main body and appendix, we simply minimize the corresponding loss
by gradient descent to find the optimal linear representation W. For self-supervised contrastive
learning with random masking augmentation, we independently draw the augmentation function by
Definition 2.2 and apply them to all of the samples in each iteration. To ensure the convergence, we
set the maximum number of iteration for it (typically 10000 or 50000 depends on dimension d).
We report two criteria to evaluate the quality of the representation, downstream error and sine dis-
tance. To obtain the sine distance for a learned representation W, we perform singular value decom-
position to get W = (UΣV >)> and then compute k sin Θ(U, U?)kF. To obtain the downstream
task performance, in the comparison between autoencoder and contrastive learning, we first draw
n labeled data from spiked covariance model (2.5) with labels generated as in Section 3.2, then we
train the model by using the data without labels to obtain the linear representation W, and learn
a linear predictor w using the data with labels and compute the regression error. In the transfer
learning setting, we draw some labeled data on the source tasks and additional unlabeled data. The
number of labeled data is set m = 1000 and the number of unlabeled data is set n = 1000. Then
train with them to obtain the linear representation W, and draw labeled data from a new source task
to learn a linear predictor w to compute the regression error. In particular, we subtract the optimal
regression error obtained by the best representation U?> for each regression error and report the
difference, or more precisely, the excess risk as downstream performance.
The results are reported in Fig. 1, 2 and Table 1, 2. As predicted by Theorem 3.1 and 3.2, the feature
recovery error and downstream task risk of contrastive learning decreases as d increases (Fig. 1:
Left) and as n increases (Fig. 1: Center) while that of autoencoder is insensible to the changes in d
and n. The performance of transfer learning exhibits a U-shape type curve when the number of tasks
is insufficient, which implies that the supervised training may hurt the transferability and we need
to choose an appropriate ratio α to obtain the best performance. When tasks are abundant enough,
the performance of transfer learning becomes better as we increase the weight of supervised loss.
loge(α)
--5	-4	-3	-2	-1	0
12345
T =	8,r = 10 U 0.0242	0.0231	0.0199	0.0141	0.0122	0.0125	0.0184	0.0345	0.0499	0.0535	0.0587
T =	20,r = T0- 0.0223	0.0163	0.0156	0.0096	0.0079	0.0055	0.0064	0.0064	0.0067	0.0070	0.0079
Table 1:	Downstream performance in transfer learning against the penalty parameter α. T is the
number of source tasks.
45
Under review as a conference paper at ICLR 2022
loge(α)
T = 8, r = 10
T = 20, r = 10
-5
-4
-3
-2
-1
2.0373^^2.03712.0228^^19908^^2.0021~~2.0055^^2.0010^^2.0362^^2.0699^^2.0705^^2.0813
2.0352^^2.0292^^2.0030~~1.9871^^1.97401.96901.9766^^1.97021.97901.97141.9672
0
1
2
3
4
5
Table 2:	Feature recovery performance in transfer learning against the penalty parameter α. T is the
number of source tasks.
—T=8,r=10
--T=20,r= 10
*∩ Ol əuuelso3£s
Figure 2: Left: Comparison of learned feature between contrastive learning and autoencoders
against the dimension d. The sample size n is set as n = 20000. Center: Comparison of fea-
ture recovery performance between contrastive learning and autoencoders against the dimension n.
The dimension d is set as d = 40. Right: Feature recovery performance in transfer learning against
penalty parameter α in log scale. T is the number of source tasks. We set the number of labeled data
and unlabeled data as m = 1000 and n = 1000 respectively.
-5	-3	-1	1	3	5
∣oge(α)
46