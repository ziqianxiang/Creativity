Under review as a conference paper at ICLR 2022
Neural Capacitance: A New Perspective of
Neural Network Selection via Edge Dynamics
Anonymous authors
Paper under double-blind review
Ab stract
Efficient model selection for identifying a suitable pre-trained neural network to a
downstream task is a fundamental yet challenging task in deep learning. Current
practice requires expensive computational costs in model training for performance
prediction. In this paper, we propose a novel framework for neural network selec-
tion by analyzing the governing dynamics over synaptic connections (edges) during
training. Our framework is built on the fact that back-propagation during neural
network training is equivalent to the dynamical evolution of synaptic connections.
Therefore, a converged neural network is associated with an equilibrium state of a
networked system composed of those edges. To this end, we construct a network
mapping φ, converting a neural network GA to a directed line graph GB that is
defined on those edges in GA . Next, we derive a neural capacitance metric βeff
as a predictive measure universally capturing the generalization capability of GA
on the downstream task using only a handful of early training results. We carried
out extensive experiments using 17 popular pre-trained ImageNet models and five
benchmark datasets, including CIFAR10, CIFAR100, SVHN, Fashion MNIST
and Birds, to evaluate the fine-tuning performance of our framework. Our neural
capacitance metric is shown to be a powerful indicator for model selection based
only on early training results and is more efficient than state-of-the-art methods.
1 Introduction
Leveraging a pre-trained neural network (i.e., a source model) and fine-tuning it to solve a target
task is a common and effective practice in deep learning, such as transfer learning. Transfer learning
has been widely used to solve complex tasks in text and vision domains. In vision, models trained
on ImageNet are leveraged to solve diverse tasks such as image classification and object detection.
In text, language models that are trained on a large amount of public data comprising of books,
Wikipedia etc are employed to solve tasks such as classification and language generation. Although
such technique can achieve good performance on a target task, a fundamental yet challenging problem
is how to select a suitable pre-trained model from a pool of candidates in an efficient manner. The
naive solution of training each candidate fully with the target data can find the best pre-trained model
but is infeasible due to considerable consumption on time and computation resources. This challenge
motivates the need for an efficient predictive measure to capture the performance of a pre-trained
model on the target task based only on early training results (e.g., predicting final model performance
based on the statistics obtained from first few training epochs).
In order to implement an efficient neural network (NN) model selection, this paper proposes a
novel framework to forecast the predictive ability of a model with its cumulative information in
the early phase of NN training, as practised in learning curve prediction (Domhan et al., 2015;
Chandrashekaran & Lane, 2017; Baker et al., 2017; Wistuba & Pedapati, 2020). Most prior work on
learning curve prediction aims to capture the trajectory of learning curves with a regression function
of models’ validation accuracy. Some of the previous algorithms developed in this field require
training data from additional learning curves to train the predictors (Chandrashekaran & Lane, 2017;
Baker et al., 2017; Wistuba & Pedapati, 2020). On the other hand, our model does not require any
such data. It solely relies on the NN architecture. Ranking models according to their final accuracy
after fine-tuning is a lot more challenging as the learning curves are very similar to each other.
1
Under review as a conference paper at ICLR 2022
The entire NN training process involves iterative updates of the weights of synaptic connections,
according to one particular optimization algorithm, e.g., gradient descent or stochastic gradient
descent (SGD) (Bottou, 2012; LeCun et al., 2015). In essence, many factors contribute to impact
how weights are updated, including the training data, the neural architecture, the loss function, and
the optimization algorithm. Moreover, weights evolving during NN training in many aspects can
be viewed as a discrete dynamical system. The perspective of viewing NN training as a dynamical
system has been studied by the community (Mei et al., 2018; Chang et al., 2018; Banburski et al.,
2019; Dogra, 2020; Tano et al., 2020; Dogra & Redman, 2020; Feng & Tu, 2021), and many attempted
to make some theoretical explanation of the convergence rate and generalization error bounds. In this
paper, we will provide the first attempt in exploring its power in neural model selection.
One limitation of current approaches is that they concentrated on the macroscopic and collective
behavior of the system, but lacks a dedicated examination of the individual interactions between
the trainable weights or synaptic connections, which are crucial in understanding of the dependency
of these weights, and how they co-evolve during training. To fill the gap, we study the system
from a microscopic perspective, build edge dynamics of synaptic connections from SGD in terms of
differential equations, from which we build an associated network as well. The edge dynamics induced
from SGD is nonlinear and highly coupling. It will be very challenging to solve, considering millions
of weights in many convolutional neural networks (CNNs), e.g., 16M weights in MobileNet (Howard
et al., 2017) and 528M in VGG16 (Simonyan & Zisserman, 2014). Gao et al. (2016) proposed a
universal topological metric for the associated network to decouple the system. The metric will
be used for model selection in our approach, and it is shown to be powerful in search of the best
predictive model. We illustrate our proposed framework in Fig.1.
(a) Deep Neural Network Ga
Inputs
HiddenS
Outputs
Synaptic Connections
Φ : GA f GB
Network Mapping
Randomly Initializing
(b)	Freezing Top Layers
一	一 Ga.
Neural Capacitance
Probe (NCP)
Output Layer
t
Wi = f (Wi) + ∑ Pij g(wi, Wj)
(C)
Fine-tuning
Bottom Layers
Transfer
Learning
New Layers
Layer K
Layer 1
Target Data
1TPP1
βeff = 1 Tp 1
Epoch
Learning Curve Prediction
Figure 1: Illustration of our framework. (a) An example multilayer perceptron (MLP) GA is mapped
to a directed line graph GB , which is governed by an edge dynamics B. Each node (dichromatic
square) of GB is associated with a synaptic connection linking two neurons (in different colors) from
different layers of GA . (b) A diagram of transfer learning from the source domain (left stack) to
a target domain (right stack). The pre-trained model is modified by adding additional layers, i.e.
installing a neural capacitance probe (NCP) unit, on top of the bottom layers. The NCP is frozen with
a set of randomly initialized weights, and only the bottom layers are fine-tuned. (c) Observed partial
learning curves (green line segments) of validation accuracy over the early-stage training epochs and
the corresponding neural capacitance metric βeff during fine-tuning. The predicted final accuracy at
βeff → 0 (red dot) is used to select the best one from a set of models. The metric βeff relies on GB ’s
weighted adjacency matrix P, which itself is derived from the reformulation of the training dynamics.
To predict the performance, a lightweight βeff of the NCP is used instead of the heavyweight one
over the entire network on the right stack of (b).
2
Under review as a conference paper at ICLR 2022
The main contributions of our framework can be summarized as follows:
•	View NN training as a dynamical system over synaptic connections, and first time investigate
the interactions of synaptic connections in a microscopic perspective.
•	Propose neural capacitance metric βeff for neural network model selection.
•	Empirical results of 17 pre-trained models on five benchmark datasets show that our βeff
based approach outperforms current learning curve prediction approaches.
•	For rank prediction according to the performance of pre-trained models, our approach im-
proves by 9.1/38.3/12.4/65.3/40.1% on CIFAR10/CIFAR100/SVHN/Fashion MNIST/Birds
over the best baseline with observations from learning curves of length only 5 epochs.
2	Related Work
Learning Curve Prediction. Chandrashekaran & Lane (2017) treated the current learning curve (LC)
as an affine transformation of previous LCs. They built an ensemble of transformations employing
previous LCs and the first few epochs of the current LC to predict the final accuracy of the current LC.
Baker et al. (2017) proposed an SVM based LC predictor using features extracted from previous LCs,
including the architecture information such as number of layers, parameters, and training technique
such as learning rate and learning rate decay. A separate SVM is used to predict the accuracy of an
LC at a particular epoch. Domhan et al. (2015) trained an ensemble of parametric functions that
observe the first few epochs of an LC and extrapolate it. Klein et al. (2017a) devised a Bayesian NN
to model the functions that Domhan formulated to capture the structure of the LCs more effectively.
Wistuba & Pedapati (2020) developed a transfer learning based predictor that was trained on LCs
generated from other datasets. It is a NN based predictor that leverages architecture and dataset
embeddings to capture the similarities between the architectures of various models and also the other
datasets that it was trained on.
Dynamical System View of NNs. There are many efforts to study the dynamics of NN training.
Some prior work on SGD dynamics for NNs generally have a pre-assumption of the input distribution
or how the labels are generated. They obtained global convergence for shallow NNs (Tian, 2017;
Banburski et al., 2019). System identification itself is a complicated task (Haykin, 2010; Lillicrap
et al., 2020). In studying the generalisation phenomenon of deep NNs, Goldt et al. (2019) formulated
SGD with a set of differential equations. But, it is limited to over-parameterised two-layer NNs
under the teacher-student framework. The teacher network determines how the labels are generated.
Also, some interesting phenomena (Frankle et al., 2020) are observed during the early phase ofNN
training, such as trainable sparse sub-networks emerge (Frankle et al., 2019), gradient descent moves
into a small subspace (Gur-Ari et al., 2018), and there exists a critical effective connection between
layers (Achille et al., 2019). Bhardwaj et al. (2021) built a nice connection between architectures
(with concatenation-type skip connections) and the performance, and proposed a new topological
metric to identify NNs with similar accuracy. Many of these studies are built on dynamical system
and network science. It will be a promising direction to study deep learning mechanism.
3	Preliminaries
Dynamical System of a Network. Many real complex systems, e.g., plant-pollinator interac-
tions (Waser & Ollerton, 2006) and the spread of COVID-19 (Thurner et al., 2020), can be described
with networks (Mitchell, 2006; Barabasi & Posfai, 2016). Let G = (V, E) be a network with node
set V and edge set E. Assuming n = |V |, the interactions between nodes can be formulated as a set
of differential equations
Xi = f(xi) + EPijg(xi,Xj),∀i ∈ V,	(1)
j∈V
where xi is the state of node i. In real systems, it could be the abundance of a plant in ecological
network, the infection rate of a person in epidemic network, or the expression level of a gene in
regulatory network. The term P is the adjacency matrix of G, where the entry Pij indicates the
interaction strength between nodes i and j. The functions f (∙) and g(∙, ∙) capture the internal and
external impacts on node i, respectively. Usually, they are nonlinear.
3
Under review as a conference paper at ICLR 2022
Let x = (x1, x2, . . . , xn). Fora small network, given an initial state, one can run a forward simulation
for an equilibrium state x*, such that X* = f (x*) + Pj∈v Pijg(X"xj) = 0. However, When the
size of the system goes up to millions or even billions, it will pose a big challenge to solve the
coupled differential equations. The problem can be efficiently addressed by employing a mean-field
technique (Gao et al., 2016), where a linear operator LP(∙) is introduced to decouple the system. In
specific, the operator depends on the adjacency matrix P and is defined as
LP(z)
IT P Z
IT P1,
(2)
where z ∈ Rn. Let δin = P1 be nodes’ in-degrees and δout = 1TP be nodes’ out-degrees. For a
weighted G, the degrees are weighted as well. Applying LP(∙) to b®, it gives
βeff = LP(δin)
IT P δin = δTutδm
ITδin = ITδin ,
(3)
which proves to be a powerful metric to measure the resilience of networks, and has been applied to
make reliable inferences from incomplete networks (Jiang et al., 2020b;a). We use it to measure the
predictive ability of a NN (see Section 4.3), whose training in essence is a dynamical system. For an
overview of the related technique, the readers are referred to Appendix H.
NN Training is a Dynamical System. Conventionally, training a NN is a nonlinear optimization
problem. Because of the hierarchical structure of NNs, the training procedure is implemented by two
alternate procedures: forward-propagation (FP) and back-propagation (BP), as described in Fig.1(a).
During FP, data goes through the input layer, hidden layers, up to the output layer, which produces
the predictions of the input data. The differences between the outputs and the labels of the input data
are used to define an objective function C, a.k.a training error function. BP proceeds to minimize C,
in a reverse way as did in FP, by propagating the error from the output layer down to the input layer.
The trainable weights of synaptic connections are updated accordingly.
Let GA be a NN, w be the flattened weight vector for GA , and z be the set of activation values. As a
whole, the training of GA can be described with two coupled dynamics: A on GA, and B on GB,
where nodes in GA are neurons, and nodes in GB are the synaptic connections. The coupling relation
arises from the strong inter-dependency between z and w: the states z (activation values or activation
gradients) of GA are the parameters of B, and the states w of GB are the trainable parameters of
GA . If we put the whole training process in the context of networked systems, A denotes a node
dynamics because the states of nodes evolve during FP, and B expresses an edge dynamics because of
the updates of edge weights during BP (Mei et al., 2018; Poggio et al., 2020a;b). Mathematically, we
formulate the node and edge dynamics based on the gradients of C :
(A)	dz/dt ≈ h∕(z,t; W) = -VzC(z(t)),	(4)
(B)	dw/dt ≈ hB(w,t; z) = -VwC(w(t)),	(5)
where t denotes the training step. Let a(') be the pre-activation of node i on layer ', and σg(∙) be the
activation function of layer `. Usually, the output activation function is a softmax. The hierarchical
structure of GA exerts some constraints over z for neighboring layers, i.e., z(') = σ'(a(')),1 ≤ i ≤
n`, ∀1 ≤ ' < L and ZkL) = exp{a(L')}/ Pj exp{ajL)}, 1 ≤ k ≤ nL, where n` is the total number
of neurons on layer `, and GA has L + 1 layers. It also presents a dependency between z and w. For
example, when GA is an MLP without bias, a(') = w(')Tz('-1), which builds an interconnection
from GA to GB. It is obvious, given w, the activation z satisfying all these constraints, is also a fixed
point of A. Meanwhile, an equilibrium state of B provides a set of optimal weights for GA .
4 Our Framework
The metric βeff is a universal metric to characterize different types of networks, including biological
neural networks (Shu et al., 2021) (Section 3). Because of the generality of βeff, we analyze how
it looks on artificial neural networks which are designed to mimic the biological counterparts for
general intelligence. Therefore, we set up an analogue system for the trainable weights. To the end,
we build a line graph for the trainable weights (Section 4.1), and reformulate the training dynamics in
the same form of the general dynamics (Eq. 1) (Section 4.2). The reformulated dynamics reveals
4
Under review as a conference paper at ICLR 2022
a simple yet powerful property regarding βeff (Section 4.3), which is utilized to predict the final
accuracy of GA with a few observations during the early phase of the training (Section 4.4). For a
detailed description of the core idea of our framework, see Appendix I.
4.1	LINE GRAPH GB
We build a mapping scheme φ : GA 7→ GB, from an NN GA to an associated graph GB . The
topology of the synaptic connections (edges) is established as a well-defined line graph proposed by
Nepusz & Vicsek (2012), and nodes of GB are the synaptic connections of GA . More precisely, each
node in GB is associated with a trainable parameter in GA . For an MLP, each synaptic connection is
assigned a trainable weight, the edge set of GA is also the set of synaptic connections of GB. For a
CNN, this one-to-one mapping from neurons on layer ` to layer ` + 1 is replaced by a one-to-more
mapping because of weight-sharing, e.g., a parameter in a convolutional filter is repeatedly used in
FP and associated with multiple pairs of neurons from the two neighboring layers. Since the error
gradients flow in a reversed direction, we reverse the corresponding links of the proposed line graph
for GB. In specific, given any pair of nodes in GB, if they share an associated intersection neuron
in FP propagation routes, a link with a reversed direction will be created for them. In Fig.1(a), we
demonstrate how the mapping is performed on an example MLP. We have the topology of GB in
place, but the weights of links in GB are not yet specified. To make up this missing components, we
reveal the interactions of synaptic connections from SGD, quantify the interaction strengths and then
define the weights of links in GB accordingly. Related technical details are disclosed in next section.
4.2	EDGE DYNAMICS B
In SGD, each time a small batch of samples are chosen to update w, i.e., W J W 一 HwC, where
α > 0 is the learning rate. When desired conditions are met, training is terminated.
We denote the activation gradients as δ(') = [∂C∕∂z('),…，∂C∕∂z('')∖τ ∈ Rn'1 and the derivatives
of activation function σ for layer ' as σ' = [σ'(af)),…，σ'(an'))]τ ∈ Rn', 1 ≤ ' ≤ L. To
understand how the weights W(') affect each other, We explicitly expand δ('):
δ⑶=W ('+I)T (W ('+2)τ (∙ ∙ ∙ (W (LT)T (W(L)T (Z(L)- y)) θ σL-ι )∙∙∙) Θ σ'+2) Θ σ' 十)
where Θ is the Hadamard product. We find that parameters W(') are associated with all accessible
parameters on downstream layers, and such recursive relation defines a high-order hyper-network in-
teraction (Casadiego et al., 2017) between any W(') and the other parameters. The Hadamard product
xΘy has an equivalent matrix multiplication form, i.e. xΘy = Λ(y)x, where Λ(y) is a diagonal ma-
trix consisting of the entries of y on the diagonal. Therefore, we have δ(') = W('+I)TΛ(σ'+1 )δ('+1)
and δ(') = W('+1)TΛ(σ' +1)W('+2)TΛ(σ'十？)…W(LT)TΛ(σL-1)W(L)T(Z(L) — y). For a
ReLU σ'(∙), σ' is binary depending on the sign of the input pre-activation values a(') of layer '. If
a(') ≤ 0, then σ'(a(')) = 0, blocking a BP propagation route of the prediction deviations Z(L) - y
and giving rise to vanishing gradients.
Our purpose is to build direct interactions between synaptic connections. It can be done by identifying
which units provide direct physical interactions to a given unit and appear on the right hand side of its
differential equation B in Eq. 4, and how much such interactions come into play. There are multiple
routes to build up a direct interaction between any pair of network weights from different layers,
as presented by the product terms in δ('). However, the coupled interaction makes it an impossible
task, which is well known as a credit assignment problem (Whittington & Bogacz, 2019; Lillicrap
et al., 2020). We propose a remedy. The impacts of all the other units on W(') is approximated by
direct, local impacts from W('+1), and the others, contribution as a whole is implicitly encoded in
the activation gradient δ('+1).
Moreover, we have the weight gradient (see Appendix A for detailed derivation)
▽w (`) = Λ(σ' 2(')z('T)T = A(σ')W('+I)T A(σ∕12('+1)z('T)T,	(6)
1In some literature δ(') is defined as gradients with respect to a('), which does not affect our analysis.
5
Under review as a conference paper at ICLR 2022
which shows the dependency of W (') on W ('+1), and itself can be viewed as an explicit description
of the dynamical system B in Eq. 4. Put it in terms of a differential equation, we have
dW(')/dt = -Λ(σ')W('+1)TΛ(σ' +1)δ('+1)z('-1)τ，F(W('+1)).	(7)
Because of the mutual dependency of the weights and the activation values, it is hard to make an
exact decomposition of the impacts of different parameters on W('). However, in the gradient Vw(`),
W('+1) presents as an explicit term and contributes the direct impact on W('). To capture such direct
impact and derive the adjacency matrix P for GB, we apply Taylor expansion on VW⑼ and have
P (l,l+1) = ∂2C/∂W (')∂W ('+1),	(8)
which defines the interaction strength between each pair of weights from layer ` + 1 to layer `.
See Appendix B for detailed derivation of P on MLP, and Appendix C on general NNs. Let
w = (w1, w2, . . .) be a flattened vector of all trainable weights of GA. Given a pair of weights wi
and Wj, one from layer 'ι, another from layer '2. If '2 = '1 + 1, the entry Pij is defined according
to Eq. 8, otherwise Pij = 0. Considering the scale of trainable parameters in GA, P is very sparse.
Let W('+1)* be the equilibrium states (Appendix C), the training dynamics Eq. 7 is reformulated
into the form of Eq. 1 and gives the edge dynamics B for GB :
W i = f(wi) + EPij g(wi,Wj),	(9)
j
with f(Wi) = F(Wi*) and g(Wi, Wj) = Wj - Wj*. The value of weights at an equilibrium state {Wj*}
is unknown, but it is a constant and does not affect the computing of βeff .
4.3	Neural Capacitance
According to Eq. 8, we have the weighted adjacency matrix P of GB in place. Now we can quantify
the total impact that a trainable parameter (or synaptic connection) receives from itself and the others,
which corresponds to the weighted in-degrees S® = P1. Applying LP(∙) (see Eq. 2) to b®, we get a
“counterpart” metric βeff = LP (δin) to measure the predictive ability of a neural network GA, as the
resilience metric (see Eq. 3) does to a general network G (see Dynamical System of a Network in
Section 3). If GA is an MLP, we can explicitly write the entries ofP, hence a βeff explicitly
βeff
PL-2[1tZ(J)] × IT[z(J) Θ σ'1] × IT[δ⑶ Θ σ'] × IT[S('+1) Θ σ' +1]
PL=21 [1Tz('-2)] X [S/ X IT[δ⑶ Θ σ']
(10)
For details of how to derive P and βeff of an MLP, see Appendix B. Moreover, we prove in Theorem
1 below that as GA converges, VW) vanishes, and βeff approaches zero (see Appendix D).
Theorem 1. Let ReLU be the activation function of GA. When GA converges, then βeff = 0.
For an MLP GA, it is possible to derive an
analytical form of βeff. However, it becomes
extremely complicated for a deep NN with
multiple convolutional layers. To realize βeff
for deep NNs in any form, we take advantage
of the automatic differentiation implemented
in TensorFlow2. Considering the number of
parameters, it is still computationally expen-
sive, and prohibitive to calculate a βeff for the
entire GA. Because of this, we seek to derive
a surrogate from a partial of GA . As shown
in Section 4.4, we insert a neural capacitance
probe (NCP) unit, i.e., putting additional lay-
Algorithm 1 Implement NCP and Compute βeff
Input: A pre-trained model Fs = {Fs(1), Fs(2)} with
bottom layers Fs(1) and output layer Fs(2), a target
dataset Dt, the maximum number of epochs T
1:	Remove Fs(2) from Fs and add on top of Fs(1) an
NCP unit U with multiple layers (Fig.1b)
2:	Initialize with random weights and freeze U
3:	Train Ft = {Fs(1), U} by fine-tuning Fs(1) on Dt
for epochs of T
4:	Obtain P from U according to Eq. 8
5:	Compute βeff with P according to Eq. 3 or Eq. 10
ers on top of the beheaded GA (excluding the original output layer), and estimate the predictive
ability of the entire GA using βeff of the NCP unit. Therefore, in the context of model selection from
a pool of pre-trained models, if no confusion arises, we call βeff a neural capacitance.
2https://www.tensorflow.org/
6
Under review as a conference paper at ICLR 2022
4.4 MODEL SELECTION WITH βeff
Here we show a novel application of our proposed neural capacitance βeff to model selection. In
specific, we transfer the pre-trained models by (i) removing the output layer, (ii) adding some layers
on top of the remaining layers (Fig.1b), and fine-tune them using a small learning rate. As shown in
Algorithm 1, the newly added layers U on top of the bottom layers of Fs are used as an NCP unit.
The specifics of the NCP unit are detailed in Section 5. The NCP does not involve in fine-tuning, and
is merely used to calculate βeff, then to estimate the performance of GA over the target domain Dt .
According to Theorem 1, when the model converges, βeff → 0. In an indirect way, the predictive
ability of the model can be determined by the relation between the training βeff and the validation
accuracy I . Since both βeff and I are available during fine-tuning, we collect a set of data points of
these two in the early phase as the observations, and fit a regularized linear model I = h(βeff ; θ) with
Bayesian ridge regression (Tipping, 2001), where θ are the associated coefficients (see Appendix E
for technical details). The estimated predictor I = h(βeff; θ*) makes prediction of the final accuracy
of models by setting βeff = 0, i.e., I * = h(0; θ*), see an example in row 3 of Fig.2. For full training
of the best model, one can either retain or remove the NCP and fine-tune the selected model.
5	Experiments and Results
Pre-trained models and datasets. We evaluate 17 pre-trained ImageNet models implemented in
Keras3, including AlexNet, VGGs (VGG16/19), ResNets (ResNet50/50V2/101/101V2/152/152V2),
DenseNets (DenseNet121/169/201), MobileNets (MobileNet and MobileNetV2), Inceptions (In-
ceptionV3, InceptionResNetV2) and Xception, to measure the performance of our approach. Four
benchmark datasets CIFAR10, CIFAR100, SVHN, Fashion MNIST of size 32 × 32 × 3, and one
Kaggle challenge dataset Birds4 of size 224 × 224 × 3 are used, and their original train/test splits are
adopted. In addition, 15K original training samples are set aside as validation set for each dataset.
Experimental setup. To get a well-defined βeff, GA requires at least three hidden layers (see
Appendix C). Also, a batch normalization (Ioffe & Szegedy, 2015) is usually beneficial because it
can stabilize the training by adjusting the magnitude of activations and gradients. To this end, on top
of each pre-trained model, we put a NCP unit composed of (1) a dense layer of size 256, (2) a dense
layer of size 128, each of which follows (3) a batch normalization and is followed by (4) a dropout
layer with a dropout probability of 0.4. Before fine-tuning, we initialize the NCP unit using Kaiming
Normal initialization (He et al., 2015).
We set a batch size of 64 and a learning rate of 0.001, fine-tune each pre-trained model for T = 50
epochs, and repeated it for 20 times. As shown in Fig.2, the pre-trained models are converged after
the fine-tuning on CIFAR10. For each model, we collect the validation accuracy (blue stars in row
1) and βeff on the training set (green squares in row 2) during the early stage of the fine-tuning
as the observations (e.g., green squares in row 3 marked by the green box for 5 epochs), then use
these observations to predict the test accuracy unseen before the fine-tuning terminates. For better
illustration, learning curves are visualized on a log-scale.
Evaluation. We apply the Bayesian ridge regression on the observations to capture the relation
between βeff and the validation accuracy, and to estimate a learning curve predictor I = h(βeff; θ*).
The performance of the model is revealed as I* = h(βe*ff; θ*) with βe*ff = 0. As shown in row 3 of
Fig.2, the blue lines are estimated h(∙; θ), the true test accuracy at T and the predicted accuracy are
marked as red triangles and blue stars, respectively. Both the estimates and predictions are accurate.
We aim to select the best one from a pool of candidates. A relative rank of these candidates matters
more than their exact values of predicted accuracy. To evaluate and compare different approaches,
we choose Spearman’s rank correlation coefficient ρ as the metric, and calculate ρ over the true
test accuracy at epoch T and the predicted accuracy I* of all pre-trained models. In Fig.3(a), we
report the true and predicted accuracy for each model on CIFAR10, as well as the overall ranking
performance measured by ρ. It indicates that our β-based model ranking is reliable with ρ > 0.9. For
the results on all five datasets, see Appendix Fig.F.4.
3https://keras.io/api/applications/
4https://www.kaggle.com/gpiosenka/100-bird-species
7
Under review as a conference paper at ICLR 2022
0.94
0.92
VGG
Epoch t
Train ∖β∖
Train ∖β∖
0.90
Figure 2: Learning curves of five representative pre-trained models w.r.t accuracy (row 1) and βeff
(row 2). A regularized linear model h(∙; θ) (blue curve in row 3) is estimated with Bayesian ridge
regression using a few of observations of βeff on training set and validation accuracy I during early
Train ∖β∖
Train ∖β∖
0.92-	*'βS
0.90-
0,88 7 t0=4,BΓC=45
io-2' " io-1......ioo ''
Train ∖β∖
fine-tuning. The starting epoch t0 of observations affects the fit of h, and is automatically determined
according to BIC, and the true test accuracy at epoch 50 is predicted with I * = h(0; θ*).
(a)
Q 0.96
<.
H 0.94
I
0.92
♦* ♦
* p = 0.92
Spearman Corr.
0.92 0.94 0.96
Pre-tranied
O DenseNet
★ Inception
◊ MobileNet
□ ResNet
ε) VGG
> Xception
Length of Learning Curve
Length of Learning Curve
IOK
15K
25K
30K
35K
Predicted Acc.

Figure 3: (a) Our βeff based prediction of the validation accuracy versus the true test accuracy at
epoch 50 of seven representative pre-trained models. Each shape is associated with one type of
pre-trained models. Distinct models of the same type are marked in different colors. Because the
accuracy of AlexNet is much lower than others, we exclude it for better visualization. Its predicted
accuracy is 0.871, and the true test accuracy is 0.868. If it is included, ρ = 0.93 > 0.92. (b) Impacts
of the starting epoch t0 of the observations and (c) the number of training samples on the ranking
performance of our βeff based approach.
The estimation quality of h determines how well the relation between I and βeff is captured. Besides
the regression method, the starting epoch t0 of the observations also plays a role in the estimation.
As shown in Fig.3(b), we evaluate the impact of t0 on ρ of our approach. It goes as expected, when
the length of learning curves is fixed, a higher t0 usually produces a better ρ. Since our ultimate
goal is to predict with the early observations, t0 should also be constrained to a small value. To
make the comparisons fair, we view t0 as a hyper-parameter, and select it according to the Bayesian
information criterion (BIC) (Friedman et al., 2001), as shown in row 3 of Fig.2.
Impact of size of training set. CIFAR10 has 50K original training and 10K testing samples.
Generally, the 50K samples are further split into 35K for training and 15K for validation. In studying
the dynamics of the NN training, it is essential to understand how varying the training size influences
the effectiveness of our approach. We select the first {10,15,20,25,30}K of the original 50K samples
as the training set of reduced size, and the last 10K samples as the validation set to fine-tune the
pre-trained models for 50 epochs. As shown in Fig.3(c), we can use a training set of size as small
as 25K to achieve similar performance to that uses all 35K training samples. It has an important
implication for efficient NN training, because the size of required training set can be greatly reduced
(around 30% in our experiment) while maintaining similar model ranking performance. To be noted
that the true test accuracy used in computing ρ is the same test accuracy for the model trained from
35K training samples and it’s shared by all the five cases {10,15,20,25,30}K in our analysis.
Ours versus baselines. We select BGRN (Baker et al., 2017) and CL (Chandrashekaran & Lane,
2017) as the baselines, as well as two heuristic rules of using the last seen value (LSV) (Klein et al.,
2017b) or the best seen value (BSV) of a learning curve for extrapolation. 6
6https://github.com/tdomhan/pylearningcurvepredictor
8
Under review as a conference paper at ICLR 2022
Table 1: A comparison between our βeff based approach and the baselines in model ranking. The no-
tation LLC represents the length of the learning curve, and Imprv represents the relative improvement
of our approach to the best baseline. Due to the failure of the supporting package6 of LC, there is a
missing ρ at LLC of 10, which does not affect our conclusions.
Dataset	CIFAR10		CIFAR100		SVHN		Fashion MNIST		Birds	
LLC	5	10	5	10	5	10	5	10	5	10
Ours	0.93	0.98	0.77	0.80	0.84	0.88	0.95	0.89	0.74	0.79
BSV	0.86	0.89	0.55	0.80	0.74	0.78	0.53	0.60	0.52	0.61
LSV	0.85	0.87	0.55	0.80	0.73	0.70	0.49	0.45	0.48	0.45
BGRN	0.74	0.78	0.45	0.60	0.63	0.65	0.57	0.59	0.53	0.52
LC	0.85	0.85	0.50	0.58	0.44	0.10	0.55	0.61	0.50	一
Imprv (%)	9.1	10.2	38.3	-0.9	12.4	13.3	65.3	49.2	40.1	30.6
We compare the performance of ours with the baselines. As shown in Table 1 and Appendix Fig.F.5,
using a few of observations, e.g., only 5 epochs, our approach can achieve 9.1/38.3/12.4/65.3/40.1%
relative improvements over the best baseline on CIFAR10/CIFAR100/SVHN/Fashion MNIST/Birds.
Running time analysis. Our approach is efficient, especially for large and deep NNs. Different
from the training task that involves a full FP and BP, i.e. Ttrain = TFP + TBP , computing βeff only
requires to compute the adjacency matrix P according to Eq. 8 on the NCP unit, Tβeff = TNCP .
Although the computation is complicated, the NCP is lightweight. The computing cost per epoch is
comparable to the training time per epoch (see Appendix Fig.G). Let Tβeff = c × Ttrain. If c > 1,
i.e., Tβeff is higher than Ttrain, vice versa. Considering the required epochs, our approach needs k
observations, and takes Tours = k × Tβeff. To obtain the ground-truth final accuracy by running K
epochs, it takes Tfull = K × Ttrain. If Tfull > Tours, our βeff based prediction is cheaper than “just
training longer”. It indicates that K × Ttrain - k × Tβeff = (K - c × k) × Ttrain > 0, saving us
K - c × k more training epochs.
We perform a running time analysis of the two tasks with 4× NVIDIA Tesla V100 SXM2 32GB, and
visualize the related times in Appendix Fig.G. On average c = Tβeff /Ttrain ≈ 1.3, computing βeff
takes 1.3 times of the training per epoch. But the efforts are paying off, as we can predict the final
accuracy by observing only k = 10 of K = 100 full training epochs, Tours is only 13% of Tfull.
When the observations are used for learning curve prediction, the heuristics LSV and BSV directly
take one observation (last or best) as the predicted value, so they are mostly computationally cheap but
have suboptimal model ranking performances. Relatively, BGRN and CL are more time-consuming
because both require to train a predictor with a set of full learning curves from other models. Our
approach also estimates a predictor, but does not need any external learning curves. Here we assume
that each model is observed for only k = 5 epochs, and conduct a running time analysis of these
approaches over learning curve prediction, including estimating a predictor. As shown in Appendix
Table G.2, our approach applies Bayesian ridge regression to efficiently estimate the predictor
I = h(βeff ; θ), taking comparable time as BGRN, significantly less than CL, but performs best in
model ranking. In contrast, the most expensive CL, does not perform well, sometimes even worst.
6	Conclusion and Discussion
We present a new perspective of NN model selection by directly exploring the dynamical evolution of
synaptic connections during NN training. Our framework reformulates the SGD based NN training
dynamics as an edge dynamics B to capture the mutual interaction and dependency of synaptic
connections. Accordingly, a networked system is built by converting an NN GA to a line graph GB
with the governing dynamics B, which induces a definition of the link weights in GB. Moreover, a
topological property of GB named neural capacitance βeff is developed and shown to be an effective
metric in predicting the ranking of a set of pre-trained models based on early training results.
There are several important directions that we intend to explore in the future, including (i) simplify the
adjacency matrix P to capture the dependency and mutual interaction between synaptic connections,
e.g., approximate gradients using local information (Jaderberg et al., 2017), (ii) extend the proposed
framework to NAS benchmarks (Ying et al., 2019; Dong & Yang, 2020; Dong et al., 2021; Zela et al.,
2020; Li et al., 2021) to select the best subnetwork, and (iii) design an efficient algorithm to directly
optimize NN architectures based on βeff.
9
Under review as a conference paper at ICLR 2022
References
Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep networks.
In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net, 2019.
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture
search using performance prediction. arXiv preprint arXiv:1705.10823, 2017.
Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Fernanda De La Torre, Jack
Hidary, and Tomaso Poggio. Theory III: Dynamics and generalization in deep networks. arXiv
preprint arXiv:1903.04991, 2019.
Albert-Laszlo Barabasi and Marton Posfai. Network Science. Cambridge University Press, 2016.
Kartikeya Bhardwaj, Guihong Li, and Radu Marculescu. How does topology influence gradient
propagation and model performance of deep networks with densenet-type skip connections?
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
13498-13507, 2021.
Leon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks ofthe Trade, pp. 421-436.
Springer, 2012.
Jose Casadiego, Mor Nitzan, Sarah Hallerberg, and Marc Timme. Model-free inference of direct
network interactions from nonlinear collective dynamics. Nature Communications, 8(1):1-10,
2017.
Akshay Chandrashekaran and Ian R Lane. Speeding up hyper-parameter optimization by extrapolation
of learning curves using previous builds. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 477-492. Springer, 2017.
Bo Chang, Minmin Chen, Eldad Haber, and H Chi. AntisymmetricRNN: A dynamical system view
on recurrent neural networks. In International Conference on Learning Representations, 2018.
Akshunna S Dogra. Dynamical systems and neural networks. arXiv preprint arXiv:2004.11826,
2020.
Akshunna S. Dogra and William Redman. Optimizing neural networks via Koopman operator theory.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 2087-2097. Curran Associates, Inc., 2020.
Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter
optimization of deep neural networks by extrapolation of learning curves. In Twenty-fourth
International Joint Conference on Artificial Intelligence, 2015.
Xuanyi Dong and Yi Yang. NAS-Bench-201: Extending the scope of reproducible neural architecture
search. arXiv preprint arXiv:2001.00326, 2020.
Xuanyi Dong, Lu Liu, Katarzyna Musial, and Bogdan Gabrys. NATS-Bench: Benchmarking nas
algorithms for architecture topology and size. IEEE transactions on pattern analysis and machine
intelligence, 2021.
Yu Feng and Yuhai Tu. Phases of learning dynamics in artificial neural networks: in the absence or
presence of mislabeled data. Machine Learning: Science and Technology, 2021.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the
lottery ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019.
Jonathan Frankle, David J Schwab, and Ari S Morcos. The early phase of neural network training.
arXiv preprint arXiv:2002.10365, 2020.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,
volume 1. Springer series in statistics New York, 2001.
10
Under review as a conference paper at ICLR 2022
Jianxi Gao, Baruch Barzel, and Albert-Laszlo Barabasi. Universal resilience patterns in complex
networks. Nature, 530(7590):307-312, 2016.
Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka ZdebOrova. Dynamics
of stochastic gradient descent for two-layer neural networks in the teacher-student setup. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv
preprint arXiv:1812.04754, 2018.
Simon Haykin. Neural Networks and Learning Machines. Pearson Education India, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on ImageNet classification. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 1026-1034, 2015.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456.
PMLR, 2015.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In
International Conference on Machine Learning, pp. 1627-1635. PMLR, 2017.
Chunheng Jiang, Jianxi Gao, and Malik Magdon-Ismail. Inferring degrees from incomplete networks
and nonlinear dynamics. In Proceedings of the Twenty-Ninth International Joint Conference on
Artificial Intelligence, IJCAI-20, pp. 3307-3313. International Joint Conferences on Artificial
Intelligence Organization, 2020a.
Chunheng Jiang, Jianxi Gao, and Malik Magdon-Ismail. True nonlinear dynamics from incomplete
networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
131-138, 2020b.
Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian
optimization of machine learning hyperparameters on large datasets. In Artificial Intelligence and
Statistics, pp. 528-536. PMLR, 2017a.
Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction
with Bayesian neural networks. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
2017b.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue
Wang, and Yingyan Lin. HW-NAS-Bench: Hardware-aware neural architecture search benchmark.
arXiv preprint arXiv:2103.10584, 2021.
Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backprop-
agation and the brain. Nature Reviews Neuroscience, pp. 1-12, 2020.
David JC MacKay. Bayesian interpolation. Neural Computation, 4(3):415-447, 1992.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
11
Under review as a conference paper at ICLR 2022
Melanie Mitchell. Complex systems: Network thinking. Artificial Intelligence, 170(18):1194-1212,
2006.
Tamas Nepusz and Tamas Vicsek. Controlling edge dynamics in complex networks. Nature Physics,
8(7):568-573, 2012.
Tomaso Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks. Proceed-
ings of the National Academy of Sciences, 117(48):30039-30045, 2020a.
Tomaso Poggio, Qianli Liao, and Andrzej Banburski. Complexity control by gradient descent in deep
networks. Nature Communications, 11(1):1-5, 2020b.
Pin Shu, Hong Zhu, Wen Jin, Jie Zhou, Shanbao Tong, and Junfeng Sun. The resilience and
vulnerability of human brain networks across the lifespan. IEEE Transactions on Neural Systems
and Rehabilitation Engineering, 29:1756-1765, 2021.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Mauricio E Tano, Gavin D Portwood, and Jean C Ragusa. Accelerating training in artificial neural
networks with dynamic mode decomposition. arXiv preprint arXiv:2006.14371, 2020.
Stefan Thurner, Peter Klimek, and Rudolf Hanel. A network-based explanation of why most covid-19
infection curves are linear. Proceedings of the National Academy of Sciences, 117(37):22684-
22689, 2020.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In International Conference on Machine
Learning, pp. 3404-3413. PMLR, 2017.
Michael E Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine
Learning Research, 1(Jun):211-244, 2001.
Nickolas M Waser and Jeff Ollerton. Plant-pollinator interactions: from specialization to generaliza-
tion. University of Chicago Press, 2006.
James CR Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends in
Cognitive Sciences, 23(3):235-250, 2019.
Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In International Conference
on Machine Learning, pp. 10303-10312. PMLR, 2020.
Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. NAS-
Bench-101: Towards reproducible neural architecture search. In International Conference on
Machine Learning, pp. 7105-7114. PMLR, 2019.
Arber Zela, Julien Siems, and Frank Hutter. NAS-Bench-1Shot1: Benchmarking and dissecting
one-shot neural architecture search. arXiv preprint arXiv:2001.10422, 2020.
12
Under review as a conference paper at ICLR 2022
A Error Gradients
Let GA be an MLP. To understand the learning mechanism, we take a sample x with label y, and
go through the entire training procedure, including a forward pass FP and a backward pass BP. To
be convenient, we rewrite z(0) = x as the inputs, let z(1) and z(L-1) be the activations of the first
and the last hidden layers, respectively, and let z (L) be the outputs. The MLP is a parameterized
model y^ = Z(L) = Fw(x) with W = (W⑴，W(2),..., W(L)), where W(') is the weight matrix of
synaptic connections from layer ` - 1 to layer `, and 1 ≤ ` ≤ L. Suppose there are n` neurons on
layer ', W(') has the size n` X n`-i. The inputs X are fed into MLP, after a forward pass from layer
1 down to layer L - 1 and layer L. Each neuron receives a cumulative input signal from the previous
layer, and sends an activated signal to a downstream layer. Let σ' be the activation function of layer
',a(') = w(')Tz('-1) be the pre-activation value of neuron i on layer ', we have z(') = σg(a('))
with w(') being the ith row of W('), 1 ≤ i ≤ n`. The output activation function σL is generally a
softmax function, i.e. zi(L) = exp{ai(L)}/ Pj exp{a(jL)}, and z(L) is a probability distribution over
nL classes, i.e. 1Tz(L) = 1.
With the predictions z(L) and the ground truth y, we can calculate the prediction error C(z(L), y),
which is often a cross entropy loss, i.e. C(z(L), y) = - Pi yi log zi(L). To minimize C, BP is applied,
and the weights w are updated backward, from the output layer up to the first hidden layer.
Now we derive the gradients for a close examination. First, we get the derivatives ofC w.r.t z(L) and
W(L). Because zi(L) = exp{ai(L)}/ Pj exp{a(jL)}, we get the gradient of the output zk(L) w.r.t wi(jL):
dzkL/dwj = ZkL (δk - Z(L))ZjL-1),
where δki = 1 if k = i, otherwise δki = 0.
On layer L, we get the derivatives of C w.r.t z(L) and W(L) :
∂C _ 工, ∂ C	X ∂C ∂zkL
∂z(L)	z(L) ; ∂w(L)	∂z(L) ∂w(L)
∂zi	zi	∂wij k ∂zk ∂wij
(zi(L) - yi)zj(L-1)
(11)
Then, we examine layer L - 1. The activation function σL-1 associates to a pair of neurons ui(L-1)
on layer L - 1 and u(jL-2) on layer L - 2 with a unique connection weight wi(jL-1). Since Zi(L) =
exP{a(L)}/Pj eχP{a(L)} and ZiLT) = σL-jaiLT)), Weget dzkL)/dz(L-1) = ZkL)(WkL) 一
Pj ZjL)WjL)) = ZkL)(WkL)-WiL)TZ(L)), and
∂Z(LT)的尸) = ZjL-2)σL-i(a(LT)).
The derivatives of C are
∂C
∂Z(L-T)
i
∂C
∂w(LT)
ij
P ∂C
乙k ∂z(L)
k
∂zkL)
∂z(LT)
■»
∂C	∂z
i
(Li-1)
'i
∂z(LT) ∂w(L-1
=Pk yk (WiL)T Z(L)-WkL)) = w(L )T (Z(L)- y),
w*L )T (Z(L)- y)¥-2)%-1(。(LT)).
On layer ', where 1 ≤ ' ≤ L 一 2, we get ∂Zk'+1)∕∂Z(')
Wk'+1)b'+ι(ak'+1)) and
∂C
i
∂C
R
ij
H	∂r	∂z ('+1)
「	∂C	dzk
乙 k∂Z((5
ki
rlj	t	rlj
∂C dz(2) —	∂C KT) -.0/a('八
KR = XZj	σ'(ai	),
i	ij	i
Pk ≠ΓWk'+1)σ' +ι(ak'+1))
according to the relations Z('+1) = σ'+1(ai'+1)) and Z(') = b`(af)).
Let δ(') = [∂C/∂Z('),…，∂C/∂燔)r ∈ Rne, σ' = [σ'(a,)), ∙ ∙ ∙ ,σ'(an'))]T ∈ Rne, 1 ≤ ' ≤ L.
The gradients can be written in a dense form:
VW(L)	=	(Z(L) - y)Z(LT)T ∈ RnL ×nL-I,
δ(L-1) = W (L)T (Z (L) - y ) ∈ RnL-1,
VW(L-1) = (δ(L-1)	σL0 -1)Z(L-2)T ∈ RnL-1×nL-2,	(12)
δ(') = W('+1)τ(δ('+1) Θ σ' +1) ∈ Rne,
VW(`	=	(δ(') Θ σ')Z('-1)τ ∈ Rne×ne-ι.
13
Under review as a conference paper at ICLR 2022
B WEIGHTED DEGREES OF GB
We examine three hidden layers {' - 1,',' + 1} of GA and three neurons on these layers {j, i, k}.
T . ('+1)	. J	7	〃，一 J	.	〃(')1」	J
Let wki connects the neuron k on layer ' + 1 to the neuron i on layer ', wij be the synaptic
connection weight between the neuron j on layer ' and the neuron i on layer ' - 1, and wj'-1)
connects the neuron j on layer ' - 1 and the neuron m on layer ' - 2.
Now we have a close look at ∂C/∂wj. According to the chain rule, we have
∂C _ ∂C ∂z(')
----:_-    :_:-----:_—
∂w(j) ∂z(( ) ∂w(j)
ij	i	ij
δ(')zj'T)σ'(ai')) = zj'-1)σ'(a(')) X。"熄+皿”加沪).
k
The gradient term δk'+1) = ∂C∕∂zk'+1) is a highly coupled function of all accessible synaptic
(`)	(`)
connection weights of wij on the forward propagation route from zi to the output neurons.
To ease the analysis, we simplify it with a numerical value or a synthetic one with no synaptic
connection weight included. Therefore, the summation term can be viewed as a simple linear
function of all synaptic connection weights wk∕+1) associated with neuron i on layer ', and the
associated coefficient is p{wki+1'),wij'')} = zj'-1)σ'(ai'))δk'+1)σ' +ι(af+1)), which defines the
edge weights from wk'+1) to w(') on GB. Similarly, we have the edge weight from w(') to w(n-'1,
i.e., p{w('),wjm-1)} = zm-2)σ'-ι(aj'-1))δ(')σ'(a(')). Therefore, we are able to calculate the in-
(`)
degree and out-degree of wij , which are defined as the sum of the weights of all in-bound connections
(`)	(`)
to wij and the sum of the weights of all out-bound connections from wij , i.e.
δin(w(')) =zj'-1)σ'(ai'))[X δk'+1)σ' +ι(a")],	(13)
k
δout(Wa =tχ Zm-2)]σ'-κaj'τ) 对')σ' (ai')).	(14)
m
There are several exceptions, including the first hidden (' = 1), the last hidden (' = L - 1) and the
output (' = L) layers. For the output layer, we have
∂ C
∂w(L)
ij
X dC 3#'
k dzkL) dw(L)
zj(L-1) (zi(L) - yi).
(15)
Because σL is softmax, no explicit relation regarding wi(jL) can be built. It implies that no well-defined
in-bound connections to wi(jL), i.e., δin(wi(jL)) = 0. But, we can build the connections from wi(jL) to
wj(Lm-1). It is easy to derive
∂ C	_	∂ C	∂z(L-1)
∂w(L-1) = ∂z(Lτ) ∂w(L-1)
zj(L-2)σL0-1(ai(L-1))X(zk(L) -yk)wk(Li).
k
(16)
From the perspective of wi(jL-1), we get p{wk(Li), wi(jL-1)} = zj(L-2)σL0 -1(ai(L-1))(zk(L) - yk); from
the perspective of wi(jL), we havep{wi(jL), wj(Lm-1)} = zm(L-2)σL0 -1(a(jL-1))(zi(L) - yi). So we have
δout(wi(jL))	= Pm zm(L-2)σL0-1(a(jL-1))(zi(L)-yi),
δin(wi(jL-1))	= zj(L-2)σL0-1(ai(L-1))Pk(zk(L) - yk) = 0,
δout(wi(jL-1))	= Pm zm(L-3)σL0-2(a(jL-2))δi(L-1)σL0-1(ai(L-1)).
The softmax。工(∙)makes the output values sum up to one, i.e., Pk yk = 1, and δin(w(L-1)) = 0.
Now, we examine the first hidden layer. Similar to the output layer, there is no well-defined out-bound
connections for wi(j1), δout(wi(j1)) = 0. Setting ' = 1 in Eq. 13, we can get the in-degree of wi(j1)
δin(wi(j1)) = zj(0)σ10(ai(1))Xδk(2)σ20(a(k2)).
k
14
Under review as a conference paper at ICLR 2022
Based on our definition of the weights of GB, when the number of layers is small, it is trivial that
βeff = 0. To get a non-trivial βeff, We identify the minimum number of hidden layers in Ga. First,
we examine a GA with one hidden layer, i.e. L = 2, whose degrees are
δin(w(1)) = δout (w(ŋ) = δin(w(2)) = 0,
δout(Wg))=	[∑m zm0)]σ1 (αjI))(Zy) —yi).
Since the degrees sum UP to zero, βeff = 0, regardless of how many hidden neurons in Ga.
If GA only has two hidden layers, i.e. L = 3, the in-degrees are
δin(Wy)) = ZjO)σ1 (OiI)) [ Pn= 1 δk2'σ2 (Ok2))],
δin(Wy)) = δin(w(3)) = 0.
The out-degrees are summarized as follows:
δout (Wj)) = 0,
δout(Wg) = [pm=ι Zm)]σ1(αjI))S(2)b2(ai2)),
δout(Wg)) = [pm1=1Zm)]σ2 («(2))(z(3) - yi).
The total degree may be non-zero, but βeff = 0 alway holds.
Therefore, the minimum number of hidden layers required for a well-defined βeff is three, i.e.,
L ≥ 4. We summarize the in-degrees
Sin(W(I))	= ZjO)σ1 (αi1))[∑fc δk2)σ2 (ak2))],
δin(W9) = ZfT)σ'(αf )[∑fc 霓+1)煜+1&'+1))],∀1 < / < L — 1,
δin(W(LT)) =0,
δin(W(L)) = 0.
and the out-degrees
δout (Wj))	= 0,
δout(W ?)=[Pm Zm-2)]*(α 尸))δ(%(αf)), ∀1 </<L — 1,
δout(W(LT))	= [Pm ZmL-3)^-2(^7)^-^-1(^7),
δout(W(L))	= [Pm ZmL-2)]%-i(ajLT))(Z(L) -y‘).
It is easy to derive
旺 δout = PijP1<e<L-1 δin(Wj) )δout(Wj))
=PijPι<e<L-i[Pm Zm-2)]Z尸)σ'-i(α 『))[σ'(αf))]2δf [Pfc 机I)嗡像户1))],
=Pi<'<l-i[1tz('-2)] × 1T[z('T) Θ σ'-i] × 1τ[δ(') Θ σ'2] × 1T[6('+1) Θ σ;十」.
Now, we move forward to compute the total degree
1tδin = Pj Z*σ1 (a(I)) [Pk δk2M(ak2))] + PjPi<<l-i Z尸匕"))[P® 机七十1或‘+1))],
=[1TZ(O)] × [1Tσ1] × 1TB⑵ θ σ2] + P1<e<L-1[1T2('-1)] × [1Tσ'] × it[6('+I) θ σ' +1],
=Pi≤'<l-i[1tz('-1)] × [IT% × 1t[δ('+1) Θ σ J.
The definitions of in-degree and out-degree ensure that 1tb® = 1tδout must hold. Let,s prove it:
Itbout = Pij [P1<'≤L-1Pm Zm-2"-i(α尸))δ(')σ'(αf))+ Pm ZmL-2)bL-i(ajLT))(Z(L) — y,)],
=P1<'≤L-1[1Tz(-2)] × [1Tσ'-1] × 1T[b⑶ θ σ'] = ITbin.
With the fact that σ' = σ' for ReLU, according to Eq. 3, we have
Q	PL:22[1tz('-2)]	× 1t[n('T)	Θ σ'-1]	×	1t[δ⑶ Θ	σ']	× 1t[δ(E+1) Θ	σ' ,1]
βeff = ----------------τ^^i----------------------------------------------------
PL-21[1tn('-2)] × [1tσ'-i] × 1t[δ⑶ Θ σ']
15
Under review as a conference paper at ICLR 2022
C DERIVATION OF ADJACENCY MATRIX P OF GB
The right hand side (RHS) of Eq. 6 is a function of W('+1), and denoted as F(W('+1)).
Here We derive the strength of the impact from W('+1) and other weights W(-`) =
(W⑼，W⑴，...，W(QW('+2),...,W(L)) for building the edge dynamics. Let W =
(W('+1), W(T)) and F(W) = dW(')/dt. We denote W(-')as the current states of W(-')),
W *('+1) as an equilibrium point, and W * = (W *('+1),W (-')). According to the Taylor expansion,
we linearize F at W * and have
dW (')/dt ≈ F (W *) + dF (WdW+：+『-'" (W ('+1) - W *('+1)) + dF (WdW(-'W I)) (W (-') — W (T)).
The last term onthe RHS can be cancelled out when the realizations of W(-') take the current states
of W(-'), i.e. W(-'). The gradient is simplified as
dW (')/dt ≈ F (W *) + dF(W *(II),W( -')(W ('+1) - W *('+1)).
∂W ('+1)
The term ∂F(W*('+1),W(T))/∂W('+1) = ∂2C(W*('+1),W(T))/∂W⑶∂W('+1) effectively
captures the interaction strengths between W(') and W('+1), because it measures how much F
is affected by a unit perturbation on W('+1). Usually, W*('+1) are not available before updat-
ing W('+1) following the update of W('), we use the current states of W('+1) instead. The
system can be viewed as a realization of the general Eq. 1, with linear f (W (')) = F (W *) and
g(W('), W('+1)) = W*('+1) 一 W('+1). Now, we can immediately have the adjacency matrix P of
GB with P(I，l+1) = ∂2C(W('+1),W(-')"∂W(')∂W('+1), ∀1 ≤ ' ≤ L.
D	Proof of Theorem 1
The second order gradient P(l,l+1) = d2C/dW(')∂W('+1) is proposed to measure the interaction
strength between W(') and W('+1), ∀1 ≤ ' ≤ L. Considering an MLP, and assume that each
activation function σ' is ReLU for ' < L, when GA converges, VWW) vanishes, i.e., V(W) =
(δ(') Θ σ0')z('T)T = 0 (Eq. 12in Appendix B). It indicates that (δ(') Θ σ0')izj'T) = 0, i.e., either
(δ(') Θ σ0')i = 0 or zj'T) = 0, ∀(i,j). According to Eq. 10, the numerator involves the product of
terms δ(') Θ σ0' and z('-1), which are zeros7, so βeff = 0.
E Bayesian Ridge Regression
Ridge regression introduces an '2-regularization to linear regression, and solves the problem
arg min(y - Xθ)T(y - Xθ) + λkθk22,
(17)
θ
where X ∈ Rn×d, y ∈ Rn , θ ∈ Rd is the associated set of coefficients, the hyper-parameter λ > 0
controls the impact of the penalty term kθk22 .
Bayesian ridge regression introduces uninformative priors over the hyper-parameters of the model,
and estimates a probabilistic model of the problem in Eq. 17. Usually, the ordinary least squares
method posits the conditional distribution of y tobea Gaussian, i.e., p(y∣X, θ) = N(y|Xθ, σ2Id),
where σ > 0 is a hyper-parameter to be tuned, and Id is a d×d identity matrix. Moreover, ifwe assume
a spherical Gaussian prior θ, i.e., p(θ) = N(θ∣0, T2Id), where τ > 0 is another hyper-parameter to
be estimated from the data at hand. According to Bayes, theorem, p(θ∣X, y) 8 p(θ)p(y |X, θ), the
estimates of the model are made by maximizing the posterior distribution p(θ∣X, y), i.e.,
arg max log p(θ∣X, y) = arg max log N (y|X θ,σ2Id) + log N (θ∣0,τ2Id),
θ
θ
which is a maximum-a-posteriori (MAP) estimation of the ridge regression when λ = σ2 /τ2 . All θ,
λ and τ are estimated jointly during the fit of the model, and σ = τ√λ.
7A small constant ε is added to the denominator of βeff to avoid division by zero.
16
Under review as a conference paper at ICLR 2022
To estimate I = h(βeff; θ), we use scikit-learn8, which is built on the algorithm described in Tipping
(2001) updating the regularization parameters λ and τ according to MacKay (1992).
F Ranking Performance on All Five Datasets
QFARlO	ClFARloo	SVHN	Fashion MNIST	Birds
6 4 2
9 9 9
Ooo
∙8<i ©sh
*♦
p = 0.93
Spearman Corr.
0.92	0.94	0.96
Predicted Acc.
0.8
P = O用
)earman Corr.
0.7
SPe
0.7	0.8
Predicted Acc.
0.96
0.95
p = 0.84
Spearman Corr.
0.95 0.96
Predicted Acc.
0.94
0.93
p = 0.95
Spearman Corr.
0.93	0.94
Predicted Acc.
1.00
0.96
0.92
« p = 0.74
■ Spearman Corr.
0.92	0.96	1.00
Predicted Acc.
V O * O □ «w >
AlexNet
DenseNet
Inception
MobileNet
ResNet
VGG
Xception
Figure F.4: Predictions of the validation accuracy of pre-trained models on all five datasets based
on βeff v.s. true test accuracy of these models after fine-tuning for T = 50 epochs. The Spearman’s
ranking correlation ρ is used to quantify the performance in model selection. Each shape is associated
with one type of pre-trained models. Distinct models of the same type are marked in different colors.
To be noted, each includes AlexNet in computing ρs.
ςχi∙00
加.75
g 0,50
10.25
Figure F.5: A comparison between our βeff based approach and the baselines in model ranking.
G Running Time Analysis
CIFAR10
Cifarioo
SVHN
Fashion MNIST
Birds
Figure G.6: Training time per epoch versus
computing time for βeff per epoch over all 17
pre-trained models and five datasets discussed
in the main text. Each data point is associated
with one pre-trained model over one dataset.
The relative cost of our approach in comput-
ing βeff with respect to training more epochs
can be measured by c = Tβeff /Ttrain. On
average, it is C ≈ 1.3 (slope of the Pink line).
Table G.2: Running time
(in seconds) in learning
curve prediction, includ-
ing the predictor estima-
tion (if necessary, e.g.,
Ours, CL and BGRN).
Dataset	OUrs I BGRN ∣ LSV			BSV	CL
CIFAR10	0.491	0.610	0.059	0.049	3966.128
CIFAR100	0.414	0.628	0.051	0.045	5256.478
SVHN	0.506	0.607	0.074	0.044	4690.507
Fashion MNIST	0.493	0.625	0.057	0.046	4552.194
Birds	0.460	0.636	0.071	0.044	4734.992
8https://scikit- learn.org/stable/modules/generated/sklearn.linear_model.
BayesianRidge.html
17
Under review as a conference paper at ICLR 2022
H Mean-Field Approach
We summarize the main idea of the mean-field approach developed by Gao et al. (2016), and show
how Eq. 3 is obtained (Jiang et al., 2020a;b).
Based on the notations described in Section 3, we consider a vertex i and the interaction term
Pj Pij g(xi, xj) in Eq. 1, where Pij is the influence j has on i. Similarly, i influences j with a
weight Pji . We define the in-degree δiin = Pj Pij and the out-degree δiout = Pj Pji . The interaction
term can be rewritten as
X Pij g(Xi,Xj ) = δin Pj P fi,Xj).	(18)
j	k Pik
Here the in-degrees δin captures the idiosyncratic part, and the average g(∙, ∙) captures the network
effect. The mean-field approximation is to replace local averaging with global averaging, which
approximates the network impact on a vertex as nearly homogeneous. Specifically, we can get
Ej Pij g(Xi ,xj)	Eij Pij g(Xi ,xj ) = IT Pg®, x)
-PPik- ≈ -PkPik- =	ITPι	,	( )
where the vector g(Xi, x) has the jth component g(Xi, Xj). A linear operator
1T P	δ°ut z
LP (z) =	Z =	——	(20)
Pll	ITPι	δout∙ι	v J
is defined for a weighted average of the entries in z. The mean-field approximation gives
Xi = f (Xi) + δinLp[g(xi, x)].	(21)
In the first order linear approximation, we can take the LP -average inside g. The average of external
interactions is approximately the interaction with its average, i.e. LP [g(Xi, x)] ≈ g(Xi, LP (x)) and
Xi= f (Xi) + δing(Xi, Lp(x)),	(22)
where LP(x) is a global state. Let Xav , LP (x). Applying LP to both sides of Eq. 22 gives
Xav = Lp [f(x)]+ Lp [δmg(x,Xav)].
(23)
According to the extensive discussion and tests in (Gao et al., 2016), the in-degrees δin and the
interaction with the external Xav are roughly uncorrelated, so the Lp -average of the product is roughly
the product of Lp -averages. Therefore, Lp [δing(x, Xav)] ≈ Lp(δin)Lp [g(x, Xav)]. Using the first
order linear approximation, we take the Lp -average inside f and g
X av = f (Lp (X)) + Lp (δin)g(Lp (x), Xav ).
(24)
Therefore, we have
Xav
f (Xav ) + βeff g(Xav , Xav ),
where βeff = Lp (δin) is the resilience metric, and its steady-state is the effective network impact
Xeff, SatiSfying Xeff = f (Xeff ) + βeff g(Xeff, Xeff ) = 0.
I Core Procedure
Our framework is built on several different techniques and the related contents are dispersed in
different sections. Here we briefly summarize the core idea of this paper and show how these sections
are organized, see Fig. I.7 for a flowchart of our core procedure.
We view the NN training as a dynamical system, and directly model the evolving of the trainable
weights in the SGD based training as a set of differential equations (Section 3), characterized by a
general dynamics in Eq. 1. Usually, it is convenient to study the dynamics of agents (trainable weights
in our case) on a regular network, where each node represents an agent in the dynamical system and
the interactions of agents are governed by Eq. 1. Many powerful techniques have been developed
in network science and dynamical systems, e.g. the universal metric βeff developed by Gao et al.
(2016) to quantify and categorize various types of networks, including biological neural networks
18
Under review as a conference paper at ICLR 2022
(Shu et al., 2021). Because of the generality of the metric, we analyze how it looks on artificial neural
networks which are designed to mimic the biological counterpart for general intelligence. Therefore,
an analogue system of the trainable weights under the context of the general dynamics is set up in our
framework. To the end, we build a line graph for the trainable weights (Fig. 1a and Section 4.1) and
“rewrite” (Section 4.2 and Appendix C) the training dynamics in the form of Eq. 1, which includes a
self-driving force f (∙), an external driving force g(∙, ∙) and an adjacency matrix P (Eqs. 8 & 9).
The reformulated training dynamics yields a simple
yet powerful property. It is proved that as the neural
network converges, βeff approaches zero (Theorem
1 in Section 4.3, also one of our primary contribu-
tions). As shown in Fig. 1(c) and Section 4.3, we
exploit the property to predict the final accuracy of a
neural network model with a few observations during
the early phase of the training, and apply it to select
the pre-trained models (Algorithm 1 in Section 4.4).
Generally speaking, the metric βeff should be calcu-
lated for the entire neural network. However, because
many state-of-the-art neural network models have
large-scale trainable weights. If all layers are consid-
ered, it will be prohibitive to compute the associated
βeff . We make a compromise, and estimate βeff of
the entire network using βeff of the NCP unit (i.e.,
a partial part of the entire network, see the second
to the last sentence of Section 4.3). It’s confirmed
from our empirical experiments (Section 5) that the
simplified, lightweight version of βeff is still effective
in predicting the final accuracy of the entire network.
Figure I.7: A flowchart of the core procedure
of our framework.
The metric βeff developed by Gao et al. (2016) is universal to characterize different types of networks.
Although our framework utilizes the metric, our application to artificial neural network training
dynamics and the related theoretical results as specified by Theorem 1 are novel. Specifically, it
is applied to study the NN training (Section 3) and predict the final accuracy of an NN with a few
observations during the early phase of the training (Fig. 1c). But βeff relies on the adjacency matrix
P of GA (Eq. 3). To derive P, we resort to a reformulation (Section 4.2 and Appendix C) of the
training dynamics in the same form of the general dynamics (Eq. 1). One issue in calculating βeff is
the complexity if the entire GA is considered. As a resolution, we propose to use the lightweight βeff
of the NCP unit - a partial of GA - to predict the performance of the entire network (Sections 4.3 &
4.4).
19