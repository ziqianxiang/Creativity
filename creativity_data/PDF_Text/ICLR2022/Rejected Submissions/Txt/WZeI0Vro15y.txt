Under review as a conference paper at ICLR 2022
Generative Posterior Networks for Approxi-
mately Bayesian Epistemic Uncertainty Esti-
MATION
Anonymous authors
Paper under double-blind review
Abstract
Ensembles of neural networks are often used to estimate epistemic uncertainty in
high-dimensional problems because of their scalability and ease of use. These
methods, however, are expensive to sample from as each sample requires a new
neural network to be trained from scratch. We propose a new method, Generative
Posterior Networks (GPNs), a generative model that, given a prior distribution
over functions, approximates the posterior distribution directly by regularizing the
network towards samples from the prior. This allows our method to quickly sam-
ple from the posterior and construct confidence bounds. We prove theoretically
that our method indeed approximates the Bayesian posterior and show empirically
that it improves epistemic uncertainty estimation over competing methods.
1	Introduction
Deep learning systems are becoming ubiquitous in many industries, specifically in applications with
large datasets and where prediction mistakes are inexpensive. However, for many other industries,
such as autonomous manufacturing, where datasets are smaller and mistakes can be catastrophic, the
adoption of deep learning techniques has been slow. One of the critical challenges to applying deep
learning models in these safety-critical domains is the lack of uncertainty estimation. If a model
makes a prediction in a safety critical environment, we would like that model to be highly confident
that prediction is correct. Uncertainty prediction can also provide significant benefit in data-poor
domains by guiding exploration towards areas of uncertainty. Most deep learning models, however,
are not able to estimate epistemic uncertainty - uncertainty deriving from lack of data samples. In
this work, we introduce a new method for modeling epistemic uncertainty in neural networks.
We formulate this problem as a Bayesian Inference problem: given a prior distribution over functions
and a set of training data, we want to model the posterior function distribution. There are many
existing approaches to constructing such a posterior, such as Gaussian Processes (GPs) or Variational
Inference (VI) methods, but these methods can require significant engineering to work in practice
and can be expensive in high-dimensional settings. For this reason, many practitioners turn to using
(a) 2D embedding samples
Figure 1: Samples from a Generative Posterior Network using a 2D embedding trained on a simple
sine-function. On the left are samples from the embedding with corresponding posterior samples on
the right. Black x' represent observed data points.
(b) Corresponding posterior samples
1
Under review as a conference paper at ICLR 2022
ensembles of neural networks to approximate epistemic uncertainty for their ease of use in high-
dimensional settings. Pearce et al. (2020) illustrated that, given the correct regularization, these
ensemble members approximate samples from the posterior distribution of functions. However,
using this technique, every new sample of the posterior requires training a new neural network from
scratch, which can be prohibitively expensive for large problems.
To address this challenge, we introduce Generative Posterior Networks (GPNs), a generative neural
network model that directly approximates the posterior distribution by regularizing the output of
the network towards samples from the prior distribution. By learning a low-dimensional latent
representation of the posterior, our method can quickly sample from the posterior and construct
confidence intervals. We prove that our method approximates the Bayesian posterior over functions
and show empirically that our method can improves uncertainty estimation over competing methods.
2	Related Work
There are two categories of uncertainty in the context of probabilistic modeling: aleatoric uncertainty
and epistemic uncertainty. Aleatoric uncertainty refers to uncertainty inherent in a system, even
when the true parameters of the system are known, while epistemic uncertainty refers to modeling
uncertainty that can be reduced through collecting more data (Der Kiureghian & Ditlevsen, 2009;
Gal, 2016). Deep learning models have shown impressive performance in quantifying aleatoric un-
certainty when given enough data. On the other hand, despite many methods seeking to estimate
epistemic uncertainty due to its well-motivated applications in Bayesian Optimization (Snoek et al.,
2012; Springenberg et al., 2016), Reinforcement Learning (Curi* et al., 2020), and adversarial ro-
bustness (Stutz et al., 2020), quantifying it remains a challenging problem for deep learning models.
Gaussian Processes (GPs) remain one of the most effective methods for epistemic uncertainty es-
timation in low-dimensional, low-data settings. While scalability can be improved with interpola-
tion methods (Hensman et al., 2013; Titsias, 2009; Wilson & Nickisch, 2015) and kernel learning
methods (Wilson et al., 2016), GPs tend to perform worse than neural network-based methods on
high-dimensional problems.
Another way to formulate epistemic uncertainty estimation is as a Bayesian Inference problem.
Specifically, given a prior distribution over functions and some labeled data, the goal is to con-
struct a posterior distribution in the Bayesian sense. Because this posterior distribution is usually
intractable to compute exactly, approximate sampling methods are used instead. Markov-Chain
Monte Carlo (MCMC) (Tanner & Wong, 1987) and Variational Inference (VI) (Blei et al., 2017)
are some of the most common methods for posterior sampling. These methods, however, tend to
struggle in high dimensional problems. The computational complexity of MCMC scales poorly with
the dimension of the data and VI methods tend to require restrictive function families with strong
independence assumptions. Bayesian Neural Networks (BNNs) (Blundell et al., 2015) are another
way to perform Bayesian Inference a neural networks by modeling each parameter in the network
with an independent Gaussian distribution. While BNNs perform well in low dimensional settings,
their performance tends to degrade in high-dimensional settings that require large networks.
Gal & Ghahramani (2016) proved that dropout can also be used as a form of Bayesian Inference.
Specifically, if a network is trained using dropout and L2 regularization, posterior samples can be
collected by applying dropout at test-time. This method has surprisingly good out-of-distribution
detection and can be applied to any network. However, it can be hard to tune the dropout parameter
to have good uncertainty prediction without significantly degrading performance.
Neural network ensembling is another commonly used epistemic uncertainty estimation technique,
which is closely related to our proposed method in this work. This technique involves training a
number of completely separate neural networks on the same data to form an ensemble. Because
of the stochasticity of initialization and the training process, each member of the ensemble should
be a slightly different function. Specifically, the ensemble members should produce similar out-
puts in data-dense regions of the state-space and dissimilar outputs in data-sparse regions. Pearce
et al. (2020) proved that, by using the correct regularization, each ensemble member becomes an
approximate sample from the function posterior. Ensembling methods are particularly attractive
to practitioners because they require very little fine-tuning and work surprisingly well in practice.
However, these methods have one main drawback, namely that each new sample from the posterior
2
Under review as a conference paper at ICLR 2022
requires training a new network from scratch. Our method, on the other hand, seeks to construct a
generative posterior model using similar regularization techniques to Pearce et al. (2020), allowing
for quick sampling from the posterior.
3	Problem Statement and Background
We will focus on the problem of Bayesian Inference on the parameters of θ of some function f (∙; θ).
We assume We have some known prior over parameters Pprior(θ)〜N(μprior, ∑prior) as Well as
access to a noisy dataset (xobs, yobs) = {(xo1bs, yo1bs) . . . , (xoNbs, yoNbs)}, where yoibs = f (xiobs; θ) + ,
E 〜N(0, σe) for some unknown parameter θ. We can then define the data-likelihood as
Plike(yobs | θ, xobs) = Y N(yoibs | f (xiobs; θ), σ).	(1)
i
The goal is to approximate samples from the posterior,
Ppost(θ | yobs, xobs) H Plike (yobs | θ, xobs)Pprior (θ) .	(2)
For ease of notation, we will drop the xobs from the likelihood.
For our analysis, it is convenient if the posterior distribution is Gaussian. To show this, we will
use the fact that the product of two Gaussian PDFs is proportional to a Gaussian PDF. We make a
distinction here between the data-likelihood, a function of the observed data yobs, and the parameter-
likelihood, a function of the parameters θ, defined as follows:
Pparam-like (θ; yobs ) = Plike (yobs | θ).	(3)
The parameter-likelihood PDF is equivalent to the data-likelihood PDF for all values of yobs and
θ, but is notably a function of the parameters θ. For a more detailed explanation of the distinction
between the parameter and data-likelihoods, see Definition 1 from Pearce et al. (2020).
When the function f is linear, it can be shown that the parameter-likelihood is also Gaussian,
Pparam-like(θ; yobs) HN3like, Elike).	(4)
While this is not true in general, Pearce et al. (2020) assume that the parameter likelihood is Gaussian
so that the posterior also becomes Gaussian,
Ppost(θ | yobs) H PParam-Iike(仇 yobs)Pprior(θ) HN3like, ςIike)N(μprior, ςPriOr) = N3post, ςPOSt),
where the posterior covariance and mean are given by
(5)
Epost =(£悬 + ςPrior)	, μpost = EpostElikePdike + Epost Eprior μprior∙	(6)
Note, however, that after the reformulation of the problem in Section 4.1, we no longer require this
assumption for our theoretical analysis.
3.1	Randomized MAP sampling (RMS)
Since the values 从毗 and £毗 are intractible to compute in practice, the challenge of computing
the posterior distribution remains. However, Pearce et al. (2020) showed how we can use a method
called Randomized MAP sampling (RMS) to approximately sample from the posterior distribution.
With RMS we first assume access to a mechanism to compute the maximum a posteriori (MAP)
solution for our posterior. Then, by randomly shifting the prior distribution we use in our MAP
solver, we can estimate samples from the posterior.
Specifically, RMS samples “anchor points” θa∩c from some distribution θa∩c 〜 N(μa∩c, ∑a∩c). Each
anchor point defines a shifted prior distribution Panc(θanc) = N(θanc, Eprior). Now we define the
MAP function which finds the MAP solution using this shifted prior:
θMAP(θanc) = arg max Plike (yobs | θ)Panc(θ).	(7)
θ
By sampling different anchor points, we can then construct a probability distribution over MAP
solutions P (θMAP(θanc)), which can be shown tobe Gaussian. Using moment matching, Pearce et al.
(2020) proved, in Theorem 1, that if we choose μa∩c = μprior and ∑a∩c = Σprior + ∑prior∑-e∑prior,
then P (θMAP(θanc)) = Ppost(θ | yobs). Using the approximation Eanc ≈ Eprior, a sample from
posterior, θpost, can be collected by sampling θanɑ 〜N(μanc, ∑anc) and optimizing the following:
θpost = arg max log Pparam-like (θ; yobs) + log Panc(θ).	(8)
θ
3
Under review as a conference paper at ICLR 2022
4	Generative Posterior Networks
While RMS can be used to construct samples from the posterior, every new sample requires solving
a new optimization problem. Instead, we propose learning the MAP function itself.
Let g be a neural network parameterized by some vector, φ, that takes as input a sample from the
anchor distribution, θa∩c 〜 Pα∏c(θa∏c), and outputs Θmap(θa∏c). If We assume that g has enough
expressive power to represent the MAP function, then g(θanc; φ) = θMAP(θanc) if:
φ = arg max Eθ*∩c〜Panc(θanc) log Plike(yobs | g(θanc; Φ)) + log Panc(g(θanc; Φ)).	(9)
φ
In practice, hoWever, this is a very hard optimization problem for tWo main reasons. (1) Optimizing
in parameter space tends to be challenging for gradient-descent optimization. (2) The space of all
parameters θanc is not only very large, but there are also many parameters θanc that map to the same
outputs for all inputs. Moreover, We Would expect the posterior parameters to be highly correlated
and, therefore, need less expressive poWer to be represented.
To address these tWo problems, We Will first change the problem definition slightly to find the poste-
rior in output space as opposed to parameter space. We prove in the next section that, by changing
our loss function, We can still use RMS to approximate the posterior in output space. Next, We shoW
practically hoW We construct a loW-dimensional representation of the anchor distribution.
4.1	Theoretical Results
As mentioned above, optimizing a generative model to output parameters can be challenging. In-
stead, We reformulate the Bayesian Inference problem slightly and consider the posterior in output
space. Concretely, consider some discretization of the input space, xsample = {xs1ample, . . . , xsMample}.
Let Yi = f (xsiample; θ) be a set of sample points and Y = {Y1, . . . , YM} be a transformation of the
random variable θ, Where each element is the output of the function f parameterized by θ evaluated
at points XSamPle. The prior and likelihood for this transformed random variable Y can be expressed
in terms of the prior and likelihood of θ :
P(Y ) = Zθ 1Y i = f(xiiθ)∀iPprior⑻dθ,	(10)
P (yobs | Y) Y / IY i=f(xiamp ι^θ)∀i Plike (yobs | θ)Pprior(θ)dθ.	(11)
Note that samples from the prior of Y can be easily obtained by sampling θ 〜 Piike(θ) and com-
puting Yi = f (XiamPle； θ) for all i. The goal in this reformulated problem is to find the posterior
,^ . ^ . , ^ .
P(Y | yobs) Y P (yobs | Y )P(Y).
As before, We estimate the posterior using RMS and define aneW MAP function in the output space:
YMAP(Yanc) = arg max P (yobs | Y)Panc(Yanc)	(12)
Y
for some anchor distribution Ys∏c 〜PanC(Yanc) = N(μanc, ∑anc). We now need to show that, with
the correct choice of μanc and Σanc, P(WAP(YanC)) = P(Y | yobs).
To simplify the analysis, we assume that the output prior is normally distributed P(Y) =
N(μγ, Σγ). Since the parameters are normally distributed, this approximation becomes exact
when either f is linear or the final layer is infinitely wide.
Next, we need to show that the likelihood function P(y°bs | Y) is roughly normally distributed with
respect to the vector Y . Recall that each observed data point, yoibs, is equal to a noisy output of f
for some parameter θ, yoibs = f(xiobs； θ) + . In the case where Xobs ⊂ Xsample, yoibs is evaluated on
the same points as Y, so the likelihood of P (yobs | θ) becomes equivalent to P (yobs | Y). By the
same reasoning, ifwe choose Xsample to contain all the points in the domain X, P (yobs | θ) becomes
equivalent to P(y°bs | Y).
4
Under review as a conference paper at ICLR 2022
Lemma 1. Let f be some neural network parameterized by θ. Let xobs, yobs correspond to vectors
of observed labeled data. Let xsample be some discretization over the input space of M d points such
that for any X ∈ X there exists an x0 ∈ X sample where ∣∣x 一 x0k2 ≤ M for some constant l. Define
Y := {f (x; θ) : ∀x ∈ xsample} be a VectorofevaluationS of f on all points in Xsample for a given
parameter vector θ.
Then, ifwe take the limit as M → ∞, P(yobs | Y) = P(yobs | θ).
Proof. See Appendix A.1.	□
It turns out that, since the observation noise is additive, the likelihood in parameter space P (yobs | θ)
is also Gaussian as a function of the output vector Y. Thus, if We take ∣Xsampie | → ∞ and apply
Lemma 1, We get that the output likelihood P (yobs | Y) is Gaussian as a function of Y.
NoW, since We have that both the prior and likelihood of Y can be expressed as Gaussian functions of
Y, we also have that the posterior is Gaussian. As before, wejust need to show that the distribution
of MAP solutions P(YMAP(Yanc)) is also Gaussian, then apply moment matching to find the right
values of μanc and ∑anc such that P(YMAP(YanC))) = P(Y | yobs).
Theorem 1. Let f be some neural network parameterized by θ. Let xsample be some vector of inputs
sampled uniformly from XM. Define Y := [f (xi； θ),..., f (XM; θ)] as a transformation of the
random variable θ. Assume the prior of Y follows a multivariate normal distribution, P(Y) =
N(μY^, ςY).
We assume access to some function which takes as input the center of a shifted distribution and
outputs the MAP estimate ofY, YMAP(Yanc).
Ifwe choose the distribution over Y* to be P(Y诋) =N(μanc, Σ诋),where μanc = μγ and
1
Σanc = ςY + ςy ςlike ςy, then limM→∞ P (YMAP (Yanc )) = P (Y | yobs ).
Proof. See Appendix A.2.	□
4.2	Practical Implementation
In practice, as was done in Pearce et al. (2020), we will instead use the approximation Σa∩c ≈ Σγ
to make it tractable to sample from the anchor distribution. Pearce et al. (2020) argue that this
approximation, in general, causes RMS to over-estimate the posterior variance.
With this assumption, the anchor distribution Panc(Yanc) becomes approximately equal to the prior
distribution P (Y). Recall that, to sample from the output prior, we simply sample a set of param-
eters θ 〜PPriOr(θ) and evaluate the function f at all evaluation points XSamPle. Now, if we want
to learn the MAP function in the output space, we can construct a neural network g, parameterized
by some vector φ, that takes a sample from the output anchor distribution, Yanc 〜N(μa∩c, ∑anc),
along with the evaluation points Xsample, and outputs θMAP(θanc). If we assume that g has enough
expressive power to represent the MAP function, then g(Yanc; φ) = YMAP(Yanc), if:
φ = arg max Eθanc〜PpriOr(θanc) log Pθ (yobs | g(Xsample, θanc; φ)) + log Pa∏c(g(Xsample, ®anc； Φ)) (13)
φ
This can be re-written as:
N
φ = argmin Eθanc〜PPriOr(θanc) X 屣65-9(4^, θan3 φ) I 2 + CYW	(14)
φ	i=1
where δj = g(xjample, θanc; φ) 一 f (XjamPle； θanc). See Appendix A.3 for a step-by-step derivation.
In practice, of course, we cannot compute the regularization term, Nσfδτ∑Y1δ as ∣Xsample∣ → ∞.
Instead, we approximate this term using a finite number of sample points from the input space X .
Because of this, for high-dimensional problems, the off-diagonal terms of Σγ are near zero almost
5
Under review as a conference paper at ICLR 2022
1.5
1.0
0.5
0.0
-0.5
-1.0
-1.5
-2.0
Figure 2: Predicted posterior distributions of different methods using the same observed data. Em-
pirical distributions are constructed using 100 samples for all methods except for the Ensemble-10
method, which is only over 10 ensemble members.
always. However, we found that, in practice, assuming that the samples ofY are independent did not
significantly degrade performance. Using this approximation, our optimization problem becomes:
N
φ = argmin Eeanc〜Pprior(θaηc),xsample〜Unif(X) E llyθbs - g(XobS, %c； φXI2 + βkδk2,	(15)
φ	i=1
where δ = g(xsampie, θanc; φ) - f (/sample； θanc) and β is a hyper-parameter.
In order for g to have enough expressive power to represent the full MAP function, it may require
the size of the parameters φ to be much larger than θ. However, after applying the low-dimensional
approximation of θanc described below, in our experiments, we only used roughly double the number
of parameters compared to each member of the ensemble method. And since the ensemble we used
has 10 members, the total number of parameters needed for our method is significantly smaller.
4.3	Low-Dimensional Embedding
As mentioned above, optimizing the generative function g over the the space of all anchor parameters
θanc did not yield great results because the space was too large. Instead, we sought to learn a low-
dimensional representation of the anchor parameters. There are two key reasons why we would
expect to be able to decrease the representational power of the anchor distribution and maintain the
same of fidelity in the posterior estimation. (1) Because of the nature of neural networks, there
are many settings of parameters θ that would result in the same output vector Y. Because We are
focusing on the posterior of Y, we need less representational power to model Y. (2) And, maybe
more importantly, because the posterior parameters are highly correlated, we would expect to need
less expressive power to represent the posterior distribution than the prior distribution.
Thus, we would like to construct a simpler estimate of the prior space using a low-dimensional
embedding vector, z. That is, we want our generative model to take as input the embedding vector
Z 〜N(0, I), instead of θanc, in order to estimate the MAP function. To do this, we need to learn
a mapping from anchor parameters θanc to embedding vectors z. For our experiments, we used a
1-1 embedding scheme where we sample k parameters from the true prior θι,..., θk 〜Pprior(θ)
and k independent samples from our embedding z1 , . . . , zk. We then jointly optimize over φ and
z1 , . . . , zk as follows:
N
arg min Ej∈[1,...,k]	lyoibs -	g(xiobs,	zj	+ ； φ)l22	+βlδl22	+Lreg(z1, . . . , zk),	(16)
φ,z1,...,zk	i=1
where δj = g(χjample, Zj + e； Φ) — f (XjamPle； θj), E 〜 N(0,I) is a noise injection vector that
improves the smoothness of interpolations between embedding vectors, and Lreg(z1, . . . , zk) is a
regularizer to keep Z1, . . . , Zk roughly normally distributed, which allows us to easily sample from
the embedding space.. For our experiments we use the KL divergence between Z and the normal
distribution, Lreg(Zι,∙∙∙, Zk) = DKL(N(z, Sz),N(0,1)). Figure 1 illustrates how we can sample
from this embedding space to construct posterior functions.
6
Under review as a conference paper at ICLR 2022
g-e> Uo-Gunu
Figure 3: Best normalized
sample value for each method
throughout training averaged
across 5 BO problems and 5 tri-
als for each problem.
Out of Dist. Prediction ROC Curve
(a) MNIST
(b) CIFAR-10
Figure 4: ROC curves for out of distribution prediction based
on sample variance from 100 samples.
4.4	Classification
Our method uses the assumption that the prior over Y is approximately normally distributed. While
this approximation is exact, assuming the prior parameters are normally distributed, only when either
the network is linear or the final layer is infinitely wide and does not have an activation function,
for most deep networks, this is a good approximation. However, if we add a soft-max to the final
output, as is usually done for classification tasks, this assumption is violated. To get around this, for
classification tasks, we add the anchor loss to the pre-softmax outputs.
5	Experiments
The goal of our experiments is to illustrate the ability of our method to accurately model epistemic
uncertainty for out of distribution data while retaining high performance on in distribution data. We
will test our method on three problems: (1) a small-scale regression task, (2) Bayesian Optimization
(BO), and (3) Classification with out of distribution examples.
We compare our method against 3 competing Bayesian methods: Gaussian Processes (GPs),
Bayesian Dropout (Gal & Ghahramani, 2016), and anchor-regularized neural-network ensembles
(Pearce et al., 2020) with 10 ensemble members. For our BO experiments we use exact GP with
a static kernel since the problems are sufficiently low-dimensional, but for our classification ex-
periments we need to use approximate GPs. Specifically, we use a grid-interpolated GP with Deep
Kernel Learning (Wilson et al., 2016) implemented with the GPyTorch library (Gardner et al., 2018).
For the anchor-regularized ensembles, since every member requires a unique set of parameters (and
anchor points) the number of ensemble members was limited by the memory capacity of our GPUs.
5.1	Small Scale Regression
Our first experiment is a small 1-dimensional regression problem where we can easily compute the
ground truth. We provide each method with the same 6 observations. For this experiment, we com-
pute 100 samples from the approximate posterior for each method (except for the Ensemble where
we only use 10 samples). For ground truth, we use the Metropolis-Hastings MCMC algorithm.
Figure 2 shows the results.
5.2	Bayesian Optimization
The goal of Bayesian Optimization is to find the maximum of a function in as few samples as
possible. This is commonly done by first constructing a posterior over possible functions given the
samples we have collected so far, then, through various methods, choosing sample points that might
maximize the true function. By using different methods for predicting the posterior in BO, we can
evaluate how well these methods capture epistemic uncertainty.
7
Under review as a conference paper at ICLR 2022
Bayesian Dropout Ensemble 10
GPN (Ours)
Deep GP
o
.	9
In Distribution	g
IZZI Out Of Distribution 7
6
5
4
3
2
(a) MNIST
In Distribution Out Of Distribution
Bayesian Dropout
Deep GP
truck-
ship -
horse -
frog-
dog-
deer-
cat-
bird-
a Utomobile -
airplane-
Ensemble 10 GPN (Ours)
In Distribution Out Of Distribution
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(b) CIFAR-10
Figure 5: Boxplots of 100 posterior samples from every method for 2 test images, one from a class
that the model was trained on (“In Distribution”) and one from a class that the model wasn’t trained
on (“Out Of Distribution”).
Table 1: Classification performance on both the MNIST and CIFAR-10 datasets for In Distribution
and Out of Distribution examples. CI-Any refers to the proportion of data points for which at least
one sample (out of 100) contains the correct label and CI-All refers to the proportion of data points
for which every sample predicts the correct label.
Dataset			Deep GP	Dropout	Ensemble 10	GPN (Ours)
	In Dist.	Accuracy	99.7%	99.2%	99.0%	99.5%
MNIST		CI-All	98.9%	89.8%	0.0%	97.9%
	Out of Dist.	CI-Any	0.0%	9.4%	0.0%	79.1%
	OUt ofDist. ROCAUC		-0.94^^	0.97^^	0.88	0.99
	In Dist.	Accuracy	88.7%	82.3%	87.3%	88.5%
CIFAR-10		CI-All	87.0%	68.5%	0.0%	87.2%
	OUt of Dist.	CI-Any	0.0%	0.0%	0.0%	67.6%
	OutofDist. ROCAUC		0.71	0.64^^	0.60	0.84
For all methods we use Thompson Sampling and implemented the experiments using the BOTorch
library by Balandat et al. (2020). We tested each method on five standard benchmark BO functions:
the Branin, Hartmann-6D, Shekel, Michalewicz-10D, and Powell functions. We normalize the out-
put of each function between a minimum value (found by random sampling) and the optimal value.
To make sure that each method had a large enough sample variance, we chose hyper-parameters
such that 100 prior samples from each method have a range of roughly [-1, 1]. To choose the best
regularization hyper-parameter, we ran each method with 5 different regularization parameters, then
chose the hyper-parameters with the highest average performance across all BO problems.
Figure 3 shows the best normalized function value sampled so far during optimization averaged
across the 5 BO problems each ran for 5 trials. Our method significantly out-performs Random
sampling and even slightly outperforms GPs, one of the best BO techniques for low-dimensional
problems. Even though our method trails slightly behind Bayesian Dropout, these results still illus-
trate the uncertainty estimation power of our method, even on low-dimensional problems.
5.3	Classification
We also look at the problem of image classification. Along with our method having high accuracy,
we are also interested in how well our method (and the baseline methods) is able to capture epistemic
uncertainty. To measure this we split the dataset into two sets based on class labels: the in distri-
bution dataset and the out of distribution dataset. During training, we only provide the labels for
8
Under review as a conference paper at ICLR 2022
the in-distribution dataset. During testing, along with recording the accuracy on the in distribution
dataset, we also sample our models on the out of distribution dataset. For such out of distribution
examples, we expect the models to show high epistemic uncertainty and, thus, the samples for these
out of distribution examples should have high variance.
Note that we still provide the full set of unlabeled training data to our agent. This obviously favors
our method over the baseline methods as our method is the only method that is able to take advantage
of this unlabeled data. However, from a practical perspective this is a reasonable setup as many real-
world datasets have far more unlabeled data than labeled data.
One desired property of an epistemic uncertainty estimator is in constructing confidence sets. For the
classification setting, the confidence interval (CI) we consider is the set of possible class predictions.
We would expect that this confidence interval contains the true class with high probability. We also
expect that, on in-distribution examples, the confidence interval only contains the true label with
high probability. To construct these confidence intervals, we take 100 samples from each model on
every data point in the test data. We define the metrics CI-Any as the proportion of data points for
which at least one sample contains the correct label and CI-All as the proportion of data points for
which every sample predicts the correct label.
Another desired property of an epistemic uncertainty estimator is in predicting when a data point is
outside the training distribution and, thus, any classification prediction made on such a data point
will likely be incorrect. For out of distribution data points, we expect high sample variance from
the posterior model and low sample variance from in distribution data points. Thus, variance can be
used as a metric to predict which test data points are outside the training distribution. To quantify
each method’s ability to perform out of distribution detection using sample variance, we construct
ROC curves for each method and measure the area under the curve (AUC).
We tested all methods on both the MNIST dataset and the CIFAR-10 dataset. For MNIST, the
in distribtuion classes were [0, 1, 2, 3, 4, 5] and the out of distribution classes were [6, 7, 8, 9]. For
CIFAR-10, the in distribtuion classes were [“airplane”, “automobile”, “bird”, “deer”, “dog”, “ship”]
and the out-of-distribution classes were [“cat”, “frog”, “horse”, “truck”]. To make sure that each
method had a large enough sample variance, we chose hyper-parameters such that prior distributions
had near 100% CI-All. To choose the best regularization hyper-parameter, as in the BO experiments,
we ran each method with 5 different regularization parameters, then chose the model with both a
high validation accuracy and CI-All.
For all other methods, we take 100 samples for measuring CI-Any, CI-All, and sample variance.
Table 1 summarizes the performance of each model using the metrics of test accuracy, CI-Any,
CI-All, and the ROC AUC for out of distribution detection. Full ROC curves for each method are
plotted in Figure 4. While all methods are able to achieve high prediction accuracy and high CI-All
on in distribution examples, our method is the only one to achieve high CI-Any on out of distribution
examples, illustrating our method’s ability to construct valid confidence intervals. Additionally, our
method significantly out performs all other methods on out of distribution detection.
Figure 5 shows two informative test images, one from the in distribution dataset and one from the out
of distribution dataset, for both MNIST and CIFAR-10, along with box plots of sampled PMFs from
each of the learned models. While all methods predict the correct label with high precision on the
in distribution example, ours has a uniquely wide prediction distribution on the out of distribution
example, illustrating high predicted epistemic uncertainty.
6	Conclusion
In this paper we introduce Generative Posterior Networks (GPNs), a method to learn a generative
model of the Bayesian posterior distribution by regularizing the outputs of the network towards
samples of the prior. We prove that under mild assumptions, GPNs approximate samples from
the true posterior. We then show empirically that our method is not only competitive with the
best uncertainty predictions techniques on small Bayesian Optimization problems, it significantly
outperforms these competing methods on high-dimensional classification tasks.
9
Under review as a conference paper at ICLR 2022
References
Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-
drew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo
Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. URL
http://arxiv.org/abs/1910.06403.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American statistical Association, 112(518):859-877, 2017.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622. PMLR, 2015.
Sebastian Curi*, Felix Berkenkamp*, and Andreas Krause. Efficient model-based reinforcement
learning through optimistic policy search and planning. In Neural Information Processing Systems
(NeurIPS), 2020. URL https://arxiv.org/abs/2006.08684.
Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural safety,
31(2):105-112, 2009.
Yarin Gal. Uncertainty in deep learning. PhD thesis, University of Cambridge, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059.
PMLR, 2016.
Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson.
Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances
in Neural Information Processing Systems, 2018.
James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. arXiv preprint
arXiv:1309.6835, 2013.
Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in neural networks: Approxi-
mately bayesian ensembling. In International conference on artificial intelligence and statistics,
pp. 234-244. PMLR, 2020.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. Advances in neural information processing systems, 25, 2012.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization
with robust bayesian neural networks. Advances in neural information processing systems, 29:
4134-4142, 2016.
David Stutz, Matthias Hein, and Bernt Schiele. Confidence-calibrated adversarial training: Gen-
eralizing to unseen attacks. In International Conference on Machine Learning, pp. 9155-9166.
PMLR, 2020.
Martin A Tanner and Wing Hung Wong. The calculation of posterior distributions by data augmen-
tation. Journal of the American statistical Association, 82(398):528-540, 1987.
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Artificial
intelligence and statistics, pp. 567-574. PMLR, 2009.
Andrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured gaussian pro-
cesses (kiss-gp). In International Conference on Machine Learning, pp. 1775-1784. PMLR,
2015.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial intelligence and statistics, pp. 370-378. PMLR, 2016.
10
Under review as a conference paper at ICLR 2022
A Proofs
A.1 Proof of Lemma 1
Proof. It is a standard result that linear feed-forward neural networks have bounded Lipschitz con-
stants. We’ll denote L(θ) as the Lipschitz constant of f for a given set of parameters θ.
Recall that this likelihood is defined as follows:
P (yobs 1 Y fX / IY i = f(xSamp2θ)∀i
N (yoibs|f (xiobs; θ), σ)	Pprior(θ)dθ
Let xsample0 ⊂ xsample be the subset of points in our discretization, xsample, that are closest to the
observed points, xobs; in other words, for each element xiobs ∈ xobs, there exists an element xi ∈
xsample 0 where:
x0i = min kxiobs - xk2
x∈xsample
We denote the elements of XsamPIe0 as [χ0i,..., XN]. Let Y0 ⊂ Y be the corresponding elements in
YY; in other words, Y = f (xi; θ) for i ∈ [1,..., N].
By definition of our discretiztaion, We know that for all i ∈ [1,..., N], ∣∣xobs - Xik ≤ M. Using
the Lipschitz-Continuity of f, for any setting of parameters θ, kf (xobs; θ) 一 Yi0 k ≤ M L(θ).
If we take the limit as M → ∞, f (Xobs; θ) = YY and
P (yobs | Y) X Z 1Yi=f(xsampgθ)∀i (γ “(yobs lYi0,σC)) Pprior (θ)dθ
=(Y N (yobs I Yi0,σe)) ]θ 1Yi = f(xsamplea∀iPprior(θ)dθ
=(Y MyobsE 0,σj P (Y)
=(Y MyobsIf(Xobs; θ),σj P(Y)
x P (yobs I θ)
Now we know that P (yobs I Y) X P (yobs I θ). But since both P (yobs I Y) and P (yobs I θ) are both
probability distributions of yobs, then P (yobs ∣ Y) = P (yobs ∣ θ).
□
A.2 Proof of Theorem 1
Proof. We will start by showing that P (Y Iyobs) is normally distributed. By Lemma 1, we know
that limM→∞ P (yobs I Y) = P (yobs I Y). Thus, in the limit as M → ∞,
,^ . ^ . , ^ .
P(Y I yobs) X P (yobs I Y )P(Y)
,^ .
=P (yobs I θ)P(Y)
=(Y MyibsIf(Xobs; θ),σj P(Y)
=(Y Mf(Xobs; θ)Iyibs,σj P(Y)
.-,^-, _____ . . -, ^-, ___ .
=N(Y1μlike, ςIike)N(Y1从Y,ςY)
11
Under review as a conference paper at ICLR 2022
for some mean and covariance μiike, ∑iike. Since the product of two multivariate Gaussians is a
Gaussian, we know that:
P(Y | yobs)= N (μpost, Σpost)	(17)
where
Σpost = (∑-e + Σγ1)- ,	μpost = EpostElikePdike + ∑post∑γ1μγ.	(18)
Now we consider at the distribution P(YMAP(YanC)) where YanC 〜N(μanc, ∑anc). We will show
that, when we set
μanc = μγ,	Eanc = ∑y+∑y ∑-e∑γ
.i	t∙ .	.∙ 丁、∕<y	/VAy ∖ ∖ i	i . .ι	,♦∙>♦,♦[,♦	* r/	b< ∖ EI
the distribution P(YMAP(YanC)) becomes equal to the posterior distribution N(μpost, ∑post). There
are three steps needed to show equality between these distributions:
< CI	. 1 . I -> /W	∕√Ay ∖ ∖	11	1 ∙ . ∙1 . 1 i'	1
1.	Show that P(YMAP(Yanc)) is normally distributed for some mean and variance,
RMS ERMS
μpost，Epost .
2.	Show that μRMtS = "post.
3.	Show that EpRoMstS = Epost.
Using the same reasoning as above, in the limit as M → ∞,
_ ,ʌ- , ^- - _ , ,
P (YMAP(YanC)) = arg max P(yobs∣Y)Panc (Y)
Y
ʌ r∕-¾^τ-1	∖ ʌ r∕√'τ-1	∖
=arg maχN(Ylμlike, Elike)N(Ylμanc, Eanc)
Y
Since the max of a Gaussian is the mean, then
___________________________________ , , . ^ _
FMAP(YanC)= AYanC + b	(19)
where we define:
A = ∑post∑γ1	(20)
b = ∑post∑-eμlike	(21)
We will now show that E[Ymap(Yanc)] = μpost. Because we set μanc = μγ:
_	, ^- 、r	_ r . ^_	_r
E[YMAp(Yanc)] = E[AYanc + b]
=AE[Y anc] + b
=Aμγ + b
=Epost Eγ1μγ + EpostEIike μlike
=μpost
τ-,∙ Ii	∙ ιι ι	. ι . Er rW t-^^τ	∖ 1	ɪ-ι
Finally, we will show that Var[YMAP(Yanc)] = Epost.
-人 ,^ . - ^ -
Var[YMAp(Yanc)] = Var[AYanc + b]
=AVar[Y anc]AT
=(∑post∑Y1)(∑γ + ∑Y ∑-e∑Y )(∑post∑γ1)T
=(Σpost + ∑post∑-e∑γ )(∑γ1∑post)
= EpostEY Epost + EpostElikeEpost
= Epost(EY1 + ςIike) Epost
= Epost
12
Under review as a conference paper at ICLR 2022
So, P(YMap(Yanc))〜N(μRMS, ∑RMtS) where μRMtS = "post and ∑RMs =三刖
Thus, in the limit as M →∞, P(YMAP(Yanc)) = P(Y卜obs).
A.3 Derivation of loss function
We start with
Φ = arg max Eθanc〜PPrior(θanc) log Plike(yobs | g(Xsample, θanc; Φ)) + log Panc(g(Xsample, ®anc； Φ))
φ
If We choose μanc = μγ and ∑anc = Σγ, We can simplify the logarithm of the likelihood and
anchor distributions as follows:
log Plike(yobs | g(xsample, θanc; φ)) = log N (yobs |g(xsample, θanc; φ), σ)
-2σ2 X kyobs - g(XSamPle, θanc; φ)k2
log PanC (g(xsample, θanc; φ)) = log N(g(xsample, θanc; φXμY, ςY )
where δj = g(xjample, %c； φ) - f (xjample；
Putting these together, We get:
φ = argmax Eθanc 〜PPriOr (θanc) — X k yZbs - g (XSamPle，仇耻；φ) k 2 -	^j^ δ
φ	2σ	i	2
argmin Eθanc 〜PPriOr(θanc) E X l^bs - g(XSamPle,仇底；0)k2 + E σl δ ς γ1 δ
φi
13