Under review as a conference paper at ICLR 2022
Kernel Deformed Exponential Families for
Sparse Continuous Attention
Anonymous authors
Paper under double-blind review
Ab stract
Attention mechanisms take an expectation of a data representation with respect
to probability weights. This creates summary statistics that focus on important
features. Recently, Martins et al. (2020; 2021) proposed continuous attention
mechanisms, focusing on unimodal attention densities from the exponential and
deformed exponential families: the latter has sparse support. Farinhas et al. (2021)
extended this to use Gaussian mixture attention densities, which are a flexible class
with dense support. In this paper, we extend this to two general flexible classes:
kernel exponential families (Canu & Smola, 2006) and our new sparse counterpart
kernel deformed exponential families. Theoretically, we show new existence re-
sults for both kernel exponential and deformed exponential families, and that the
deformed case has similar approximation capabilities to kernel exponential fam-
ilies. Experiments show that kernel deformed exponential families can attend to
multiple compact regions of the data domain.
1	Introduction
Attention mechanisms take weighted averages of data representations (Bahdanau et al., 2015), where
the weights are a function of input objects. These are then used as inputs for prediction. Discrete
attention 1) cannot easily handle data where observations are irregularly spaced 2) attention maps
may be scattered, lacking focus. Martins et al. (2020; 2021) extended attention to continuous set-
tings, showing that attention densities maximize the regularized expectation of a function of the data
location (i.e. time). Special case solutions lead to exponential and deformed exponential families:
the latter has sparse support. They form a continuous data representation and take expectations with
respect to attention densities. Using measure theory to unify discrete and continuous approaches,
they show transformer self-attention (Vaswani et al., 2017) is a special case of their formulation.
Martins et al. (2020; 2021) explored unimodal attention densities: these only give high importance
to one region of data. Farinhas et al. (2021) extended this to use multi-modal mixture of Gaussian
attention densities. However 1) mixture of Gaussians do not lie in either the exponential or deformed
exponential families, and are difficult to study in the context of Martins et al. (2020; 2021) 2) they
have dense support. Sparse support can say that certain regions of data do not matter: a region of
time has no effect on class probabilities, or a region of an image is not some object. We would like
to use multimodal exponential and deformed exponential family attention densities, and understand
how Farinhas et al. (2021) relates to the optimization problem of Martins et al. (2020; 2021).
This paper makes three contributions: 1) we introduce kernel deformed exponential families, a
multimodal class of densities with sparse support and apply it along with the multimodal kernel
exponential families (Canu & Smola, 2006) to attention mechanisms. The latter have been used
for density estimation, but not weighting data importance 2) we theoretically analyze normalization
for both kernel exponential and deformed exponential families in terms of a base density and ker-
nel, and show approximation properties for the latter 3) we apply them to real world datasets and
show that kernel deformed exponential families learn flexible continuous attention densities with
sparse support. Approximation properties for the kernel deformed are challenging: similar kernel
exponential family results (Sriperumbudur et al., 2017) relied on standard exponential and logarithm
properties to bound the difference of the log-partition functional at two functions: these do not hold
for deformed analogues. We provide similar bounds via the functional mean value theorem along
with bounding the Frechet derivative of the deformed log-partition functional.
1
Under review as a conference paper at ICLR 2022
The paper is organized as follows: we review continuous attention (Martins et al., 2020; 2021). We
then describe how mixture of Gaussian attention densities, used in Farinhas et al. (2021), solve a
different optimization problem. We next describe kernel exponential families and give novel nor-
malization condition relating the kernel growth to the base density’s tail decay. We then propose
kernel deformed exponential families, which can have support over disjoint regions. We describe
normalization and prove approximation capabilities. Next we describe use of these densities for
continuous attention, including experiments where we show that the kernel deformed case learns
multimodal attention densities with sparse support. We conclude with limitations and future work.
2	Related Work
Attention Mechanisms closely related are Martins et al. (2020; 2021); Farinhas et al. (2021); Tsai
et al. (2019); Shukla & Marlin (2021). Martins et al. (2020; 2021) frame continuous attention as
an expectation of a value function over the domain with respect to a density, where the density
solves an optimization problem. They only used unimodal exponential and deformed exponential
family densities: we extend this to the multimodal setting by leveraging kernel exponential families
and proposing a deformed counterpart. Farinhas et al. (2021) proposed a multi-modal continuous
attention mechanism via a mixture of Gaussians approach. We show that this solves a slightly
different optimization problem from Martins et al. (2020; 2021), and extend to two further general
density classes. Shukla & Marlin (2021) provide an attention mechanism for irregularly sampled
time series by use of a continuous-time kernel regression framework, but do not actually take an
expectation of a data representation over time with respect to a continuous pdf, evaluating the kernel
regression model at a fixed set of time points to obtain a discrete representation. This describes
importance of data at a set of points rather than over regions. Other papers connect attention and
kernels, but focus on the discrete attention setting (Tsai et al., 2019; Choromanski et al., 2020). Also
relevant are temporal transformer papers, including Xu et al. (2019); Li et al. (2019; 2020); Song
et al. (2018). However none have continuous attention densities.
Kernel Exponential Families Canu & Smola (2006) proposed kernel exponential families: Sripe-
rumbudur et al. (2017) analyzed theory for density estimation. Wenliang et al. (2019) parametrized
the kernel with a deep neural network. Other density estimation papers include Arbel & Gretton
(2018); Dai et al. (2019); Sutherland et al. (2018). We apply kernel exponential families as atten-
tion densities to weight a value function which represents the data, rather than to estimate the data
density, and extend similar ideas to kernel deformed exponential families with sparse support.
Wenliang et al. (2019) showed a condition for an unnormalized kernel exponential family density
to have a finite normalizer. However, they used exponential power base densities. We instead relate
kernel growth rates to the base density tail decay, allowing non-symmetric base densities.
To summarize our theoretical contributions: 1) introducing kernel deformed exponential families
with approximation and normalization analysis 2) improved kernel exponential family normalization
results.
3	Continuous Attention Mechanisms
An attention mechanism involves: 1) the value function approximates a data representation. This
may be the original data or a learned representation. 2) the attention density is chosen to be ’similar’
to another data representation, encoding it into a density 3) the context combines the two, taking an
expectation of the value function with respect to the attention density. Formally, the context is
C = ET 〜p[V (T)].	(1)
Here V (t) the value function approximates a data representation, T 〜p(t) is the random variable
or vector for locations (temporal, spatial, etc), and p(t) is the attention density.
To choose the attention density p, one takes a data representation f and finds p ’similar’ to f and thus
to a data representation, but regularizing p. Martins et al. (2020; 2021) did this, providing a rigorous
formulation of attention mechanisms. Given a probability space (S, A, Q), let M1+(S) be the set of
densities with respect to Q. Assume that Q is dominated by a measure ν (i.e. Lebesgue) and that it
has density qo = ddQ with respect to ν. Let S ⊆ RD, F be a function class, and Ω : M∣(S) → R
be a lower semi-continuous, proper, strictly convex functional.
2
Under review as a conference paper at ICLR 2022
Given f ∈ F, an attention density (Martins et al., 2020) p: F → R≥o solves
p[f ] = arg max hp, fgg) - Ω(p).	(2)
p∈M1+(S)
This maximizes regularized L2 similarity between P and a data representation f. If Ω(p) is the
negative differential entropy, the attention density is Boltzmann Gibbs
p[f](t)=exp(f(t)- A(f)),	(3)
where A(f) ensures JSP[f](t)dQ = 1. If f(t) = θτφ(t) for parameters and statistics θ ∈
RM, φ(t) ∈ RM respectively, Eqn. 3 becomes an exponential family density. For f in a repro-
ducing kernel Hilbert space H, it becomes a kernel exponential family density (Canu & Smola,
2006), which we propose to use as an alternative attention density.
One desirable class would be heavy or thin tailed exponential family-like densities. In exponential
families, the support, or non-negative region of the density, is controlled by the measure Q. Letting
Ω(p) be the α-Tsallis negative entropy Ωα(p) (TSaniS, 1988),
Ωα(p)
α0α-i) (RSp(t)αdQ - 1) ,α = 1;
RS p(t) log p(t)dQ, α = 1,
then p[f] for f(t) = θτφ(t) lies in the deformed exponential family (Tsallis, 1988; Naudts, 2004)
pΩa [f ](t) = exP2-α(θTφ(t) - Aɑ(f)),
(4)
where Aα (f) again ensures normalization and the density uses the β-exponential
M ∫[1 + (1 - β)t]J1-β),β = 1；
expβ(t) = exp(t),β= 1.+	(5)
For β < 1, Eqn. 5 and thus deformed exponential family densities for 1 < α ≤ 2 can return 0
values. Values α > 1 (and thus β < 1) give thinner tails than the exponential family, while α < 1
gives fatter tails. Setting β = 0 is called sparsemax (Martins & Astudillo, 2016). In this paper, we
assume 1 < α ≤ 2, which is the sparse case studied in Martins et al. (2020). We again propose to
replace f(t) = θT φ(t) with f ∈ H, which leads to the novel kernel deformed exponential families.
Computing Eqn. 1’s context vector requires parametrizing V (t). Martins et al. (2020) obtain a value
function V : S → RD parametrized by B ∈ RD×N by applying regularized multivariate linear
regression to estimate V (t； B) = BΨ(t), where Ψ = {ψn}nN=1 is a set of basis functions. Let L be
the number of observation locations (times in a temporal setting), O be the observation dimension,
and N be the number of basis functions. This involves regressing the observation matrix H ∈ RO×L
on a matrix F ∈ RN×L of basis functions {ψn}nN=1 evaluated at observation locations {tl}lL=1
B* = argmin ∣∣BF - HkF + λ∣∣B∣∣F.	(6)
B
3.1	Gaussian Mixture Model
Farinhas et al. (2021) used mixture of Gaussian attention densities, but did not relate this to the
optimization definition of attention densities in Martins et al. (2020; 2021). In fact their attention
densities solve a related but different optimization problem. Martins et al. (2020; 2021) show that
exponential family attention densities maximize a regularized linear predictor of the expected suffi-
cient statistics of locations. In contrast, Farinhas et al. (2021) find a joint density over locations and
latent states, and maximize a regularized linear predictor of the expected joint sufficient statistics.
They then take the marginal location densities to be the attention densities.
Let Ω(p) be Shannon entropy and consider two optimization problems:
arg max hθ Ep[Φ(T)])理-Ω(p)
p∈M1+(S)
arg max hθ, Ep[Φ(T, Z)]i12 - Ω(p)
p∈M1+(S)
3
Under review as a conference paper at ICLR 2022
The first is Eqn. 2 with f = θT φ(t) and rewritten to emphasize expected sufficient statistics. If one
solves the second with variables Z, we recover an Exponential family joint density
PΩa [f](t, Z) = exp(θTφ(t, Z)- A(θ)).
This encourages the joint density of T, Z to be similar to a complete data representation θT φ(t, z)
of both location variables T and latent variables Z, instead of encouraging the density of T to be
similar to an observed data representation θT φ(t). The latter optimization is equivalent to
arg max Ω(p)
p∈M1+(S)
s.t.
Ep(T,Z)[φm (T, Z)]= cm,m = 1,…，M.
The constraint terms cm are determined by θ. Thus, this maximizes the joint entropy of Z and T,
subject to constraints on the expected joint sufficient statistics.
To recover EM learned Gaussian mixture densities, one must select φm so that the marginal distribu-
tion of T will be a mixture of Gaussians, and relate cm to the EM algorithm used to learn the mixture
model parameters. For the first, assume that Z is a multinomial random variable taking |Z | possi-
ble values and let φ(t, Z) = (z1,z2,…，z∣z∣-ι,I(Z = 1)t,I(Z = 1)t2,…，I(z = |Z|)t, I(z =
|Z|)t2). These are multinomial sufficient statistics, followed by the sufficient statistics of |Z| Gaus-
sians multiplied by indicators for each Z. Thenp(T|Z) will be Gaussian, p(Z) will be multinomial,
and p(T) will be a Gaussian mixture. For contraints, Farinhas et al. (2021) have
L	|Z|
Ep(T,z)[Φm(T,Z)] = X Wl X Poid(Z∣tι)Φm(tι ,z), m =1,…，m	⑺
l=1	z=1
at each EM iteration. Here pold(Z|xl) is the previous iteration’s latent state density conditional on
the observation value, wl are discrete attention weights, and tl is a discrete attention location. That
EM has this constraint was shown in Wang et al. (2012). Intuitively, this matches the expected
joint sufficient statistics to those implied by discrete attention over locations, taking into account the
dependence between Z and tl given by old model parameters. An alternative is simply to let θ be the
output of a neural network. While the constraints lack the intuition of Eqn. 7, it avoids the need to
select an initialization. We focus on this case and use it for our baselines: both approaches are valid.
4	Kernel Exponential and Deformed Exponential Families
We now use kernel exponential families and a new deformed counterpart to obtain flexible attention
densities solving Eqn. 2 with the same regularizers. We first review kernel exponential families. We
then give a novel theoretical result describing when an unnormalized kernel exponential family den-
sity can be normalized. Next we introduce kernel deformed exponential families, extending kernel
exponential families to have either sparse support or fatter tails: we focus on the former. These can
attend to multiple non-overlapping time intervals or spatial regions. We show similar normaliza-
tion results based on the choice of kernel and base density. Following this we show approximation
theory. We conclude by showing how to compute attention densities in practice.
Kernel exponential families (Canu & Smola, 2006) extend exponential family distributions, replac-
ing f(t) = θTφ(t) with f in a reproducing kernel Hilbert space H (Aronszajn, 1950) with kernel
k : S × S → R. Densities can be written as
p(t) = exp(f (t) - A(f))
= exp(hf,k(∙,t))H)- A(f)),
where the second equality follows from the reproducing property. A challenge is to choose H, Q
so that a normalizing constant exists, i.e., S exp(f (t))dQ < ∞. Kernel exponential family dis-
tributions can approximate any continuous density over a compact domain arbitrarily well in KL
divergence, Hellinger, and Lp distance (Sriperumbudur et al., 2017). However relevant integrals
including the normalizing constant generally require numerical integration.
4
Under review as a conference paper at ICLR 2022
Figure 1: Attention densities for kernel deformed exponential families for the first attention head
of the uWave experiment and all test set participants for three classes. The densities are sparse and
each have support over different non-overlapping time intervals, which cannot be done with either
Gaussian mixtures or exponential families. They also attend to similar regions within each class.
To avoid infinite dimensionality one generally assumes a representation of the form
I
f = X Yik(∙,ti),
i=1
where for density estimation (Sriperumbudur et al., 2017) the ti are the observation locations. How-
ever, this requires using one parameter per observation value. This level of model complexity may
not be necessary, and often one chooses a set of inducing points (Titsias, 2009) {ti }iI=1 where I is
less than the number of observation locations.
For a given pair H, k, how can we choose Q to ensure that the normalization constant exists? We
first give a simple example of H, f and Q where the normalizing constant does not exist.
Example 1. Let Q be the law of a N (0, 1) distribution and S = R. Let H = span{t3, t4} with
k(x, y) = x3y3 + x4y4 andf(t) = t3 + t4 = k(t, 1). Then
e exp(f(t))dQ = / exp(t2 + t3 + t4)dt
where the integral diverges.
(8)
4.1	Theory for Kernel Exponential Families
We provide sufficient conditions for Q and H so that A(f) the log-partition function exists. We
relate H’s kernel growth rate to the tail decay of the random variable or vector TQ with law Q.
Proposition 1. Let p(t) = exp(f(t)) where f ∈ H an RKHS with kernel k. Assume k(t,t) ≤
Lkktkξ2 +	Ck for constants Lk, Ck, ξ > 0. Let Q be the law of a random vector TQ, so that
Q(A) = P(TQ ∈ A). Assume ∀u s.t. kuk2 = 1,
P(|uTTQ| ≥ z) ≤ Cq exp(-vzη)
(9)
for some constants η > 2 ,Cq , v > 0. Then
I p(t)dQ < ∞.
S
Proof. See Appendix A.1
□
Based on k(t, t)’s growth, We can vary What tail decay rate for TQ ensures We can normalize p(t).
Wenliang et al. (2019) also proved normalization conditions, but focused on random variables with
exponential poWer density fora specific groWth rate of k(t, t) rather than relating tail decay to groWth
rate. By focusing on tail decay, our result can be applied to non-symmetric base densities. Specific
kernel bound groWth rate terms ξ lead to alloWing different tail decay rates.
Corollary 1. For ξ = 4, TQ can be any sub-Gaussian random vector. For ξ = 2 it can be any
sub-exponential. For ξ = 0 it can have any density.
Proof. See Appendix A.2
□
5
Under review as a conference paper at ICLR 2022
4.2	Kernel Deformed Exponential Families
We now propose kernel deformed exponential families: flexible sparse non-parametric distributions.
These take deformed exponential families and extend them to use kernels in the deformed exponen-
tial term. This mirrors kernel exponential families. We write
p(t) = exp2-α (f(t) - Aα (f)),
where f ∈ H with kernel k. Fig. 1b shows that they can have support over disjoint intervals.
4.2.1	Normalization Theory
We construct a valid kernel deformed exponential family density from Q and f ∈ H. We first
discuss the deformed log normalizer. In exponential family densities, the log-normalizer is the log
of the normalizer. For deformed exponentials, the following holds.
Lemma 1. Let Z > 0 be a constant. Then for 1 < α ≤ 2,
7 exP2-α(Zα-1f ⑴)=exP2-af ⑴-lθgɑ Z)
Z
where
(t1---1 if t > 0,β = 1；
logβ t =	log(t) ift > 0,β = 1;
I undefined if t ≤ 0.
Proof. See Appendix B.1
□
We describe a normalization sufficient condition analagous to Proposition 1 for the sparse deformed
kernel exponential family. With Lemma 1, we can take an unnormalized exp2-α(f(t)) and derive a
valid normalized kernel deformed exponential family density. We only require that an affine function
of the terms in the deformed-exponential are negative for large magnitude t.
Proposition 2. For 1 < α ≤ 2 assume p(t) = exp2-α(f (t)) With f ∈ H, H is a RKHS with kernel
k. If ∃Ct > 0 s.t. for ∣∣tk2 > Ct, (α 一 1)f(t) + 1 ≤ 0 and k(t,t) ≤ Lkktk2 + Ck for some ξ > 0,
then S exp2-α (f (t))dQ < ∞.
Proof. See Appendix B.2	□
We now construct a valid kernel deformed exponential family density using the finite integral.
Corollary 2. Under the conditions of proposition 2, assume exp2-α (f (t)) > 0 on a set A ⊆ S such
1
that Q(A) > 0, then ∃ constants Z > 0, Aɑ(f) ∈ R Such that for f (t) = Zo-T f (t), the following
holds
S
exp2-α(f(t) 一 Aα(f))dQ
1.
Proof. See Appendix B.3.
□
We thus estimate f(t) = (Z)α-1 f (t) and normalize to obtain a density of the desired form.
4.2.2 Approximation Theory
Under certain kernel conditions, kernel deformed exponential family densities can approximate den-
sities of a similar form where the RKHS function is replaced with a C0(S) 1 function.
1continuous function on domain S vanishing at infinity
6
Under review as a conference paper at ICLR 2022
Proposition 3. Define
P0 ={πf(t) =exp2-α(f(t)-Aα(f)),t∈ S:f ∈ C0(S)}
where S ⊆ Rd. Suppose k(x, ∙) ∈ Co(S),∀x ∈ S and
J J k(x, y)dμ(x)dμ(y) > 0, ∀μ ∈ Mb(S)∖{0}.
(10)
here Mb(S) is the space of bounded measures over S. Then the set of deformed exponential families
is dense in P0 wrt Lr (Q) norm and Hellinger distance.
Proof. See Appendix B.4
□
We apply this to approximate fairly general densities with kernel deformed exponential families.
Theorem 1. Let q0 ∈ C(S), such that q0(t) > 0 for all t ∈ S, where S ⊆ Rd is locally compact
Hausdorff and q0(t) is the density of Q with respect to a dominating measure ν. Suppose there exists
l > 0 such that for any > 0, ∃R > 0 satisfying |p(t) - l| ≤ for any t with ktk2 > R. Define
Pc
{p∈ C(S) : p(t)dQ
S
1, p(t) ≥ 0, ∀t ∈ S and p - l ∈ C0 (S)}.
Suppose k(t, ∙) ∈ Co (S)∀t ∈ S and the kernel integration condition (Eqn. 10) holds. Then ker-
nel deformed exponential families are dense in Pc wrt Lr norm, Hellinger distance and Bregman
divergence for the α-Tsallis negative entropy functional.
Proof. See Appendix B.5.
□
For uniform q0, kernel deformed exponential families can thus approximate continuous densities on
compact domains arbitrarily well. Our Bregman divergence result is analagous to the KL divergence
result in Sriperumbudur et al. (2017). KL divergence is Bregman divergence with the Shannon
entropy functional: we show the same for Tsallis entropy. The Bregman divergence here describes
how close the uncertainty in a density is to its first order approximation evaluated at another density.
Using the Tsallis entropy functional here is appropriate for deformed exponential families: they
maximize it given expected sufficient statistics (Naudts, 2004).
These results extend Sriperumbudur et al. (2017)’s approximation results to the deformed setting,
where standard log and exponential rules cannot be applied. The Bregman divergence case requires
bounding Frechet derivatives and applying the functional mean value theorem.
4.3 Using Kernels for Continuous Attention
We apply kernel exponential and deformed exponential families to attention. The forward pass com-
putes attention densities and the context vector. The backwards pass uses automatic differentiation.
We assume a vector representation v ∈ R|v| computed from the locations we take an expectation
over. For kernel exponential families we compute kernel weights {γi}iI=1 for f(t) = PiI=1 γik(t, ti)
T
γi = wiTv,
and compute Z = JS exp(f (t))dQ numerically. For the deformed case We compute Yi = WT V and
f(t) = (Z)α-1f(t) = Pi=I Yik(t,ti) followed by Z = JS exp2-α(f(t))dQ. Thecontext
C = ET ~p[V (T)] = BEp[Ψ(t)]
requires taking the expectation of Ψ(T) with respect to a (possibly deformed) kernel exponential
family density p. Unlike Martins et al. (2020; 2021), where they obtained closed form expectations,
difficult normalizing constants prevent us from doing so. We thus use numerical integration for
the forward pass and automatic differentiation for the backward pass. Algorithm 1 shows how
to compute a continuous attention mechanism for a kernel deformed exponential family attention
density. The kernel exponential family case is similar.
7
Under review as a conference paper at ICLR 2022
Algorithm 1 Continuous Attention Mechanism via Kernel Deformed Exponential Families
Choose base density q0(t) and kernel k. Inducing point locations {ti}iI=1
Input Vector representation v of input object i.e. document representation
Parameters {Yi}I=ι the weights for f(t) = (Z)α-1f (t) = PI= %k(t, ti), matrix B for basis
weights for value function V (t) = BΨ(t). I is number of inducing points.
Forward Pass
Compute Z = j exp2-α(f (t))dQ(t) to obtain p(t) = 1 exp2-α(f (t)) via numerical integration
Compute ET〜p[Ψ(T)] via numerical integration
Compute C = ET〜p[V(T)] = BEp[Ψ(T)]
Backwards Pass use automatic differentiation
Attention	N=32	N=64	N=128	Mean
Cts Softmax	89.56	90.32	91.08	90.32
Cts Sparsemax	89.50	90.39	89.96	89.95
Kernel Softmax	91.30	91.08	90.44	90.94
Kernel Sparsemax	90.56	90.20	90.41	90.39
Table 1: IMDB sentiment classification dataset accuracy. Continuous softmax uses Gaussian at-
tention, continuous sparsemax truncated parabola, and kernel softmax and sparsemax use kernel
exponential and deformed exponential family with a Gaussian kernel. The latter has α = 2 in
exponential and multiplication terms. N: basis functions, I = 10 inducing points, bandwidth 0.01.
5	Experiments
For document classification, we follow Martins et al. (2020)’s architecture. For the remaining, ar-
chitectures have four parts: 1) an encoder takes a discrete representation ofa time series and outputs
attention density parameters. 2) The value function takes a time series representation (original or
after passing through a neural network) and does (potentially multivariate) linear regression to ob-
tain parameters B for a function V(t; B). These are combined to compute 3) context c = Ep[V(T)],
which is used in a 4) classifier. Fig. 2 in the Appendices visualizes this.
5.1	Document Classification
We extend Martins et al. (2020)’s code2 for the IMDB sentiment classification dataset (Maas et al.,
2011). This starts with a document representation v computed from a convolutional neural network
and uses an LSTM attention model. We use a Gaussian base density and kernel, and divide the inter-
val [0, 1] into I = 10 inducing points where we evaluate the kernel in f(t) = PiI=1 γik(t, ti). We
set the bandwidth to be 0.01 for I = 10. Table 1 shows results. On average, kernel exponential and
deformed exponential family slightly outperforms the continuous softmax and sparsemax, although
the results are essentially the same. The continuous softmax/sparsemax results are from their code.
2Martins et al. (2020)’s repository for this dataset is https://github.com/deep-spin/quati
Attention	N=64	N=128	N=256
Cts Softmax	67.78±1.64 =	67.70± 2.49	68.00± 2.24Z=
Cts Sparsemax	74.20±2.72	74.69±3.78	74.58±4.27
Gaussian Mixture	81.13±1.76	80.99±2.79	79.04±2.33
Kernel Softmax	93.85±0.60	94.26±0.75	93.83±0.60
Kernel Sparsemax	92.32±1.09 一	92.15±0.79	92.14±0.96一
Table 2: Accuracy results on uWave gesture classification dataset for the irregularly sampled case.
All methods use 100 attention heads. Gaussian mixture uses 100 components (and thus 300 param-
eters per head), and kernel methods use 256 inducing points.
8
Under review as a conference paper at ICLR 2022
Attention	Accuracy	F1
Cts Softmax	96.97	83.69
Cts Sparsemax	-9604-	73.71
Gaussian Mixture	-9720-	84.89
Kernel Softmax	-9675-	83.27
Kernel Sparsemax	96.86	84.90
Table 3: Accuracy results on MIT BIH Arrhythmia Classification dataset. For F1 score, kernel soft-
max and continuous softmax have similar results, while kernel sparsemax drastically outperforms
continuous sparsemax. For accuracy, the Gaussian mixture slightly beats other methods.
5.2	uWave Dataset
We analyze uWave (Liu et al., 2009): accelerometer time series with eight gesture classes. We follow
Li & Marlin (2016)’s split into 3,582 training observations and 896 test observations: sequences
have length 945. We do synthetic irregular sampling and uniformly sample 10% of the observations.
Table 2 shows results. Our highest accuracy is 94.26%, the unimodal case’s best is 74.69%, and the
mixture’s best is 81.13%. Since this dataset is small, we report ±1.96 standard deviations from 10
runs. Fig. 1 shows that attention densities have support over non-overlapping time intervals. This
cannot be done with Gaussian mixtures, and the intervals would be the same for each density in the
exponential family case. Appendix C describes additional details.
6	ECG heartb eat classification
We use the MIT Arrhythmia Database’s (Goldberger et al., 2000) Kaggle 3. The task is to detect
abnormal heart beats from ECG signals. The five different classes are {Normal, Supraventricular
premature, Premature ventricular contraction, Fusion of ventricular and normal, Unclassifiable}.
There are 87,553 training samples and 21,891 test samples. Our value function is trained using
the output of repeated convolutional layers: the final layer has 256 dimensions and 23 time points.
Our encoder is a feedforward neural network with the original data as input, and our classifier is a
feedforward network. Table 3 shows results. All accuracies are very similar, but the F 1 score of
kernel sparsemax is drastically higher. Additional details are in Appendix D.
7	Discussion
In this paper we extend continuous attention mechanisms to use kernel exponential and deformed
exponential family attention densities. The latter is a new flexible class of non-parametric densities
with sparse support. We show novel existence properties for both kernel exponential and deformed
exponential families, and prove approximation properties for the latter. We then apply these to the
framework described in Martins et al. (2020; 2021) for continuous attention. We show results on
three datasets: sentiment classification, gesture recognition, and arrhythmia classification. In the
first case performance is similar to unimodal attention, for the second it is drastically better, and in
the third it is similar in the dense case and drastically better in the sparse case.
7.1	Limitations and Future Work
A limitation of this work was the use of numerical integration, which scales poorly with the di-
mensionality of the locations. Because of this we restricted our applications to temporal and text
data. This still allows for multiple observation dimensions at a given location. A future direction
would be to use varianced reduced Monte Carlo to approximate the integral, as well as studying
how to choose the number of basis functions in the value function and how to choose the number of
inducing points.
3https://www.kaggle.com/mondejar/mitbih-database
9
Under review as a conference paper at ICLR 2022
References
Michael Arbel and Arthur Gretton. Kernel conditional exponential family. In International Confer-
ence on Artificial Intelligence and Statistics ,pp.1337-1346. PMLR, 2018.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical
society, 68(3):337-404, 1950.
Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In 3rd International Conference on Learning Representations,
ICLR 2015, 2015.
Stephane CanU and Alex Smola. Kernel methods and the exponential family. Neurocomputing, 69
(7-9):714-720, 2006.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, XingyoU Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz MohiUddin, LUkasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794, 2020.
Bo Dai, HanjUn Dai, ArthUr Gretton, Le Song, Dale SchUUrmans, and Niao He. Kernel exponential
family estimation via doUbly dUal embedding. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 2321-2330. PMLR, 2019.
Antonio Farinhas, Andre FT Martins, and Pedro MQ Aguiar. Multimodal continuous visual attention
mechanisms. arXiv preprint arXiv:2104.03046, 2021.
Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G
Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank,
physiotoolkit, and physionet: components of a new research resource for complex physiologic
signals. circulation, 101(23):e215-e220, 2000.
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan.
Enhancing the locality and breaking the memory bottleneck of transformer on time series fore-
casting. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32, pp. 5243-5253. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/
file/6775a0635c302542da2c32aa19d86be0-Paper.pdf.
Steven Cheng-Xian Li and Benjamin M Marlin. A scalable end-to-end gaussian process adapter for
irregularly sampled time series classification. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29, pp.
1804-1812. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/
paper/2016/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf.
Yikuan Li, Shishir Rao, Jose Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan,
Dexter Canoy, Yajie Zhu, Kazem Rahimi, and Gholamreza Salimi-Khorshidi. Behrt: transformer
for electronic health records. Scientific reports, 10(1):1-12, 2020.
Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan. uwave: Accelerometer-
based personalized gesture recognition and its applications. Pervasive and Mobile Computing, 5
(6):657-675, 2009.
Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies, pp. 142-150, 2011.
Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention
and multi-label classification. In International Conference on Machine Learning, pp. 1614-1623.
PMLR, 2016.
Andre Martins, AntOniO Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and
Mario Figueiredo. Sparse and continuous attention mechanisms. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural In-
formation Processing Systems, volume 33, pp. 20989-21001. Curran Associates,
10
Under review as a conference paper at ICLR 2022
Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f0b76267fbe12b936bd65e203dc675c1- Paper.pdf.
Andre FT Martins, Marcos Treviso, Antonio Farinhas, Pedro MQ Aguiar, Mario AT Figueiredo,
Mathieu Blondel, and Vlad Niculae. Sparse continuous distributions and fenchel-young losses.
arXiv preprint arXiv:2108.01988, 2021.
Jan Naudts. Estimators, escort probabilities, and phi-exponential families in statistical physics. arXiv
preprint math-ph/0402005, 2004.
Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled
time series. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=4c0J6lwQ4_.
Huan Song, Deepta Rajan, Jayaraman Thiagarajan, and Andreas Spanias. Attend and diagnose:
Clinical time series analysis using attention models. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 32, 2018.
Bharath SriPerUmbUdur, Kenji Fukumizu, Arthur Gretton, Aapo Hyvarinen, and Revant Kumar.
Density estimation in infinite dimensional exponential families. The Journal of Machine Learning
Research ,18(1):1830-1888,2017.
Bharath K Sriperumbudur, Kenji Fukumizu, and Gert RG Lanckriet. Universality, characteristic
kernels and rkhs embedding of measures. Journal of Machine Learning Research, 12(7), 2011.
Dougal Sutherland, Heiko Strathmann, Michael Arbel, and Arthur Gretton. Efficient and princi-
pled score estimation with nystrom kernel exponential families. In International Conference on
Artificial Intelligence and Statistics, pp. 652-660. PMLR, 2018.
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Artificial
intelligence and statistics, pp. 567-574. PMLR, 2009.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: A unified understanding of transformer’s attention via
the lens of kernel. arXiv preprint arXiv:1908.11775, 2019.
Constantino Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of statistical
physics, 52(1):479-487, 1988.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In-
ternational Conference on Neural Information Processing Systems, pp. 6000-6010, 2017.
Shaojun Wang, Dale Schuurmans, and Yunxin Zhao. The latent maximum entropy principle. ACM
Transactions on Knowledge Discovery from Data (TKDD), 6(2):1-42, 2012.
Li Wenliang, Dougal Sutherland, Heiko Strathmann, and Arthur Gretton. Learning deep kernels for
exponential family densities. In International Conference on Machine Learning, pp. 6737-6746.
PMLR, 2019.
Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Self-
attention with functional time representation learning. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neu-
ral Information Processing Systems, volume 32, pp. 15915-15925. Curran Associates,
Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
cf34645d98a7630e2bcca98b3e29c8f2- Paper.pdf.
11
Under review as a conference paper at ICLR 2022
A Proof Related to Proposition 1
A.1 Proof of Proposition 1
Proof. This proof has several parts. We first bound the RKHS function f and use the general tail
bound we assumed to give a tail bound for the one dimensional marginals TQd of TQ. Using the
RKHS function bound, we then bound the integral of the unnormalized density in terms of expec-
tations with respect to these finite dimensional marginals. We then express these expectations over
finite dimensional marginals as infinite series of integrals. For each integral within the infinite se-
ries, we use the finite dimensional marginal tail bound to bound it, and then use the ratio test to show
that the infinite series converges. This gives us that the original unnormalized density has a finite
integral.
We first note, following Wenliang et al. (2019), that the bound on the kernel in the assumption allows
us to bound f in terms of two constants and the absolute value of the point at which it is evaluated.
If(t)l = lhf,k(t, ∙)iH∣ reproducing property
≤ kfkH∣∣k(t, ∙)kH CaUchySchWarz
=kfkH PWrT^k(tT)iH
=kf ∣H Pk(S)
≤ ∣f∣H Lk ∣t∣ξ + Ck by assumption
≤ Co + CIktk 同/2 for some Ci, C2 > 0.
We can write TQ = (Tqi,…，Tqd). Let e& be a standard Euclidean basis vector. Then by the
assumption and setting u = ed We have
P(|TQd| ≥ z) ≤ Cq exp(-vzη)
Letting Qd be the marginal law,
/ exp(f(t))dQ ≤ / exp(Co + CIktkξ∕2)dQ
= exp(Co) / exp(Cι∣∣tkξ∕2)dQ
=exp(Co)E exp(CιkTQkξ/2)
≤ exp(Co)Eexp(Cι(√d max ∣TQd∣)ξ/2)
d=1,…，D ~
D
≤ exp(Co) XEexp(C2∣TQd∣ξ/2)
d=1
which will be finite if each E exp(C2 ∣TQd∣ξ/2) < ∞. Now letting Sd be the relevant dimension of
S,
Eexp(C2∣TQd∣ξ/2)= I exp(C2∣td∣S)dQd
Sd
-1 j+1	∞ j+1
≤ ΣS /	exp(C2∣td∣ξ∕2)dQd + £ /	exp(C2∣td∣32)dQd
j=-∞ j	j=0 j
where the inequality follows since Sd ⊆ R, exp is a non-negative function and probability measures
are monotonic. We will show that the second sum converges. Similar techniques can be shown for
the first sum. Note that for j ≥ 0
Qd([j,j+1))=P(Td≥j)-P(Td≥j+1)
≤ P (Td ≥ j)
≤ Cq exp(-vjη) by assumption
12
Under review as a conference paper at ICLR 2022
Then
∞	j+1	∞
X J.	eχp(C2∣td∣"2)dQd ≤ Xeχp(C2 j∣ξ∕2)Qd([j, j +1))
∞
≤ X Cq exp(C2jW2-Vjn)
j=0
Let aj = exp(C2∣j∣ξ/2 - vin). We will use the ratio test to show that the RHS converges. We have
aj+1 =exp(C2((j + 1)ξ/2 - jξ/2) - V[(j + 1)n - jn]).	(11)
aj
We want this ratio to be < 1 for large j . We thus need to select η so that for sufficiently large j , we
have
C1 ((j + 1)S-jS) < [(j + 1)n -jn].
v
Assume that η > 2. Then
(j + 1)n-jn	jn [(1 + 1 )n - 1]
(j + I)ξ/2 - jξ/2 = j-2[(1 + 1 甲2- 1]
≥ jn-ξ/2.
Since the RHS is unbounded for η > 2, we have that Eqn. 11 holds for sufficiently
large j. By the ratio test Eqd(t)exp(C2∣Td∣S) = P-=L∞ Rj+1 exp(C2|td|〃2)dQd +
P∞= 0 Rjj+1 exp(C2∣tdP2)dQd is finite. Thus putting everything together we have
exp(f(t))dQ ≤
exp(C0 + CIktk-2)dQ
D
< exp(C0) X E exp(C2 ∣TQd∣ξ/2)
d=1
<∞
and p(t) can be normalized.
□
A.2 Proof of Corollary 1
Proof. Let ξ = 4. Then η > 2 and
P(|uTT| > t) ≤ P(|uTT| ≥ t) monotonicity
≤ CQ exp(-vtη)
< CQ exp(-vt2).
The second case is similar. For the uniformly bounded kernel,
/ exp(hf,k(∙,t)iH)dQ ≤ exp(kf kHpCk) / dQ
=exp(kfkH VZCk)
< ∞.
The first line follows from Cauchy Schwarz and ξ = 0
□
13
Under review as a conference paper at ICLR 2022
B Proofs Related to Kernel Deformed Exponential Family
B.1 Proof of Lemma 1
Proof. The high level idea is to express a term inside the deformed exponential family that becomes
1/Z once outside.
1
exP2-af ⑴-logɑ(Z)) = [1 + (α - 1)(f ⑴-logα Z)]+-1
Z1-α - 1 ɪ
= [1 + (α - 1)f (t)-(α - 1)-——]+-1
1-α
1
=[1 + (α - 1)f (t)+ Z1-α - 1]+-1
1
=[(α - 1)f(t) + Z 1-α]+-τ
Z α-1	i
= [(α - 1)f(t)+ Z 1-α)]尸
Zα
1	1 ɪ
=R(α-1)f(t) ki + 1]+-1
ZZ
= 1exP2-α(Z α-1f (t))
Z
□
B.2 Proof of Proposition 2
Proof.	JSeXp2-α(f(ty)dQ = j∖1 + (α - 1)f(t)]+-τdQ =1	[1 + (α - 1)f(t)]+-τdQ ktk≤Ct ≤ [	[1 + (α -I)(CO + CIlCtF/2)]+-1 dQ ktk≤Ct <∞
□
B.3	Proof of Corollary 2
Proof. From proposition 2 and the assumption,
eXP2-a(f(t))dQ
Z
S
for some Z > 0. Then
Js Z exP2-α(Zα-1f (t))dQ = 1
exp2-α(f(t) -logαZ)dQ = 1
S
where the second line follows from lemma 1. Set Aα (f) = logα (Z) and we are done.
□
B.4	Proof of Proposition 3
Proof. The kernel integration condition tells us that H is dense in C0(S) with respect to L∞ norm.
This was shown in Sriperumbudur et al. (2011). For the Lr norm, we apply kpf - pg kLr ≤
2Mexpkf - gk∞ from Lemma 5 with f ∈ C0(S), g ∈ H, and f0 = f. L1 convergence implies
Hellinger convergence.	□
14
Under review as a conference paper at ICLR 2022
B.5	Proof of Theorem 1
Proof. For any P ∈ Pc, define pδ = p+. Then
l∣P - Pδ ∣∣r = l+δ kPkr
→0
for 1≤ r ≤ ∞. Thus for any > 0, ∃δ > 0 such that for any 0 < θ < δ, we have lp - pθ lr ≤ ,
where pθ (t) > 0 for all t ∈ S.
Define f =(镣)	log2-a Pθ ⅛+θ. Since P ∈ C(S), So is f. FiX any η > 0 and note that
f(t) ≥ η
1 + θ 1-α	1 + θ
y + θ)	og2-α pθ 1 + θ
≥η
1 + θ
l + θ
α-1
log2-α Pθ
⑷)
≥
η
1+ θ
pθ E
≥ exp2-α ((V+1) )	η
Pθ
l+θ	1+θ α-1
≥ 币"a] g?) η
P-l≥(l+θ)
exp2-a ((l+θ)	〃) - 1)
Thus
A = {t : f(t) ≥ η}
P-l ≥ (l+θ)
卜。2-a ((1+θ)	η) - 1)}
Since P - l ∈ C0(S) the set on the second line is bounded. Thus A is bounded so that f ∈ C0(S).
Further, by Lemma 1
Pθ = exp2-α
f - l* 1+θ
giving us Pθ ∈ P0 . By Proposition 3 there is some Pg in the deformed kernel eXponential family
so that lPθ - PglLr(S) ≤ . Thus lP - Pglr ≤ 2 for any 1 ≤ r ≤ ∞. To show convergence in
Helinger distance, note
H2 (P, Pg)
≤
1∣s(√p-√pg YdQ
2人 (P - 2√ppg + Pg)dQ
2 /(P - 2min(p,pg) + Pg)dQ
2/ |P - PgIdQ
2 IIp - Pg Ili
4actually an equality, see https://www2.cs.uic.edu/ zhangX/teaching/bregman.pdf for proof
15
Under review as a conference paper at ICLR 2022
so that L1 (S) convergence, which we showed, implies Hellinger convergence. Let us consider the
Bregman divergence. Note the generalized triangle inequality4 for Bregman divergence
BΩα(P,Pg ) = BΩα (P,Pθ )+ BQa(P8 ,Pg ) - 5 - Pθ , 8仪(Pθ ) - 8仪(Pg ^2	(12)
'----{z-----} X----{-----} X---------------{---------------}
I	II	III
Term I
BQa(P3 ) = α(α⅛
_	1
α(α - 1)
1
≤ —(π
α(α - 1)
(Pα - Pθα)dQ
S
(Pα - Pθα)dQ
S
(Pα - Pθα)dQ +
S
"Cα(pθ),P - Pθi
-ɪf pPaT(P - Pθ)dQ
α-1 θ
ɪ ∣Pa-1kιkkP - pθ ∣∞
α-1
—
—
The first term on the rhs clearly vanishes as θ → 0. For the second term, we already showed that
kP -Pθk∞ → 0.
Term II
Fix θ. Then term I I converges to 0 by Lemma 5.
Term III
For term III ,
hp - Pθ, VΩα(pθ ) - VΩα(Pg ))2 ≤ IlP - Pθ ∣∣∞kVΩα(pθ ) - VΩα(Pg )∣∣1
Clearly the first term on the rhs converges by Lr convergence. The L1 term for the gradient is given
by
∣VΩa(Pθ)-VΩa(Pg)kι = ;八Pθ(t)a-1 -Pg(tYa-1∖dQ
α-1
≤	(∣Pθ ∣∞ + ∣Pθ - Pg ∣∞)α-2 ∣Pθ - Pg ∣∞dQ Eqn. 17
= (∣Pθ ∣∞ + ∣Pθ - Pg ∣∞ )α- ∣Pθ - Pg ∣∞
so that the inner product terms are bounded as
∖hP — Pθ, VCa(P6 ) - VCa(Pg )i2∣ ≤ (∣∣Pθ k∞ + ∣∣Pθ - Pg ∣∣∞)a-2kPθ - Pg k∞kP - Pθ k∞
□
Lemma 2. (Functional Taylor’s Theorem) Let F : X → R where X is a Banach space. Let
f, g ∈ X and let F be k times Gateaux differentiable. Then we can write
k-1
F(g)=XFi(f)(g-f)i+Fk(f+c(g-f))(g-f)k
i=0
for some c ∈ [0, 1].
Proof. This is simply a consequence of expressing a functional as a function of an ∈ [0, 1], which
restricts the functional to take input functions between two functions. We then apply Taylor’s the-
orem to the function and apply the chain rule for Gateaux derivatives to obtain the resulting Taylor
remainder theorem for functionals.
Let G(η) = F(f + η(g - f)). By Taylor’s theorem we have
G(1) = G(O) + G0(O) + …+ Gk(C)
and applying the chain rule gives us
k-1
F(g)=XFi(f)(g-f)i+Fk(f+c(g-f))(g-f)k
i=0
□
16
Under review as a conference paper at ICLR 2022
Lemma 3. (Functional Mean Value Theorem) Let F : X → R be a functional where f, g ∈ X
some Banach space with norm ∣∣ ∙ ∣∣. Then
|F (f) - F (g)l≤∣F0(h)k op kf - gk
where h = g + c(f 一 g) for some C ∈ [0,1], F0(h) is the Gateaux derivative of F, and ∣∣ ∙ ∣∣op is the
operator norm ∣A∣op = inf {c > 0 : ∣Ax∣ ≤ c∣x∣∀x ∈ X}.
Proof. Consider G(η) = F(g + η(f - g)). Apply the ordinary mean value theorem to obtain
G(1) - G(0) = G0(c),c∈ [0, 1]
=F0(g + c(f 一 g)) Yf - g)
and thus
|F(f)一 F(g)l≤∣F0(h)kopkf - gk
□
Claim 1. Consider P∞ = {pf = exp2-α(f - Aα(f)) : f ∈ L∞ (S)}. Then for pf ∈ P∞,
Aα(f) ≤ ∣f∣∞.
Proof.
pf (t) = exp2-α (f(t) - Aα (f))
≤ exp2-α(∣f∣∞ - Aα(f)) for 1 < α ≤ 2
Spf(t)dQ≤ Sexp2-α(∣f∣∞
- Aα(f))dQ
1 ≤ exp2-α(∣f∣∞ - Aα(f))
log2-α 1 ≤ ∣f∣∞ - Aα(f)
Aα(f) ≤ ∣f∣∞
where for the second line recall that We assumed that throughout the paper 1 < α ≤ 2.	□
Lemma 4. Consider P∞ = {pf = exp2-α(f -Aα(f)) : f ∈ L∞ (S)}. Then the Frechet derivative
of Aα : L∞ → R exists. It is given by the map
A0(f )(g)= Epf-α (g(T))
_ Rpf-α(t)g(t)dQ
=R pf-α(t)dQ
Proof. This proof has several parts. We first derive the Gateaux differential of pf in a direction
ψ ∈ L∞ and as it depends on the Gateaux differential of Aα(f) in that direction, we can rearrange
terms to recover the latter. We then show that it exists for any f, ψ ∈ L∞ . Next we show that the
second Gateaux differential of Aα(f) exists, and use that along with a functional Taylor expansion
to prove that the first Gateaux derivative is in fact a Frechet derivative.
In Martins et al. (2020) they show how to compute the gradient of Aα(θ) for the finite dimensional
case: we extend this to the Gateaux differential. We start by computing the Gateaux differential of
pf.
而Pf+ηψ (t)=而 exP2-α(f (t) + ηψ(t) - Aα(f + ηψ))
=dη [1 + (α - 1)(f (t) + ηψ(t) - Aα(f + ηψ))^α—1
= [1 + (α - 1)(f (t) + ηψ(t) - Aα(f + ηΨ))]+2-α”(αT)
=pf+ηψ(t) (ψ(t) - dηAa(f+ηψ))
ψ(t)- dnAa(f + ηψ)
17
Under review as a conference paper at ICLR 2022
evaluating at η = 0 gives us
dp(f; ψ)(t) = pf2-α (ψ(t) + dAα (f ; ψ))
Note that by claim 1 we have
pf+ηψ(t) = exp2-α(f(t) + ηψ(t) - Aα(f + ηψ(t))
≤ exp2-α(2kfk∞ + 2ηkψk∞)
≤ exp2-α(2(kfk∞ + kψk∞))
We can thus apply the dominated convergence theorem to pull a derivative with respect to η under
an integral. We can then recover the Gateaux diferential of Aα via
d
0=而
pf+ηψ (t)dQ
η=0
dp(f; ψ)(t)dQ
/
Z
pf (t)2-α (ψ(t) - dAα (f; ψ))dQ
dAα(f ； ψ)= Ep2-α (ψ(T))
<∞
where the last line follows as ψ ∈ L∞ . Thus the Gateaux derivative exists in L∞ directions. The
derivative at f maps ψ :→ Ep2-α(ψ(T)) i.e. A∖(f )(ψ) = Ep2-α (ψ(T)). We need to show that
this is a Frechet derivative. To do so, we will take take second derivatives of pf+ηψ (t) with respect
to η in order to obtain second derivatives of Aα (f + ηψ) with respect to η. We will then construct
a functional second order Taylor expansion. By showing that the second order terms converge
sufficiently quickly, We will prove that the map ψ :→ E~2-α (ψ(T)) is a Frechet derivative.
d^2pf+ηΨ⑴=~j∑pf+ηΨ(t)2-α (ψ⑴-d；Aaf + ηψ))
ηη	η
=(ɪpf+ηΨ⑴2-a) (ψ⑴一~LAa(f + ηψ)
ηη
d2
-pf+ηψ(t)2 a dη2Aa(f + ηψ)
(2 - α)pf+ηψ(t)(ψ(t) - dηAa(f + ηψ))dηpf+ηψ(t)
d2
-pf+ηψ(t)	dη2 Aa(f + ηψ)
d	d2
(2 - α)pf+ηψ(ψ(t) - dηAa(f + ηψ)) -pf+ηψ(t) ~a~η2Aa(f + ηψ)
We need to show that we can again pull the second derivative under the integral. We already showed
that we can pull the derivative under once (for the first derivative) and we now need to do it again.
We need to show an integrable function that dominates Pf+ηψ (t)2-a(ψ(t) - E方2-α ψ(T)).
pf+ηψ
∣Pf+ηΨ(t)2-a(Ψ(t) — Epf+ηψΨ(T))1 ≤ Pf+ηψ(t)2-a2kΨk∞
≤ exp2-a(2(kfk∞ + kψk∞))2kψk∞
which is in L1(Q). Now applying the dominated convergence theorem
d2
0=/ d^2pf+eψ (t)dQ
d	d2
=	(2 - α)pf+eψ (ψ(t)	-	TAa(f	+ Eψ))	-	pf+eψ (t)	2Aa Aa(f	+ eψ)	dQ
d	d
18
Under review as a conference paper at ICLR 2022
and rearranging gives
d2
QAa(f + eψ) = (2 - a)
RPf+2ψ(ψ(t)- dAaf + W))2dQ
/Pf+eψ ⑴2-adQ
d2	Rp3f-2a(ψ(t) - Ep2-α[Ψ(T)])2dQ
…―Tpf—
since f, ψ ∈ L∞. For the functional Taylor expansion, we have from Lemma 2
Aa(f + Ψ) = Aaf) + Aa (f )(ψ) + 2 Aa (f + eψ)(ψ)2
for some ∈ [0, 1]. We thus need to show that for ∈ [0, 1],
R pf+a (ψ(t) - Epf+αψ [Ψ(T )])2dQ ψ→0n
(2- a)	R PXa (t)2-adQ	→ 0
It suffices to show that the numerator tends to 0 as ψ → 0.
1
kψk∞
(ψ(t) -
Epf+αψ[ψ(T )])2
K ψ(t)-
kψk∞
*2fψ[ψ(T)]+
Efjψ(T 力
kψk∞
Epf + αψ[ψ(T )]
ψ⑴
kψk∞
ψ(t) - 2Epf+αψ[Ψ(T)]
≤
+ Epf+αψ kψψ(T∞ ∣Epf+αψ[ψ(T)]|
≤ ∣ψ(t) - 2Epf+αψ[Ψ(τ)]∣ + kρ∕+eak2-a ∣Epf+αψ[Ψ(τ)]|
→ 0 as ψ → 0
and plugging this in we obtain the desired result. Thus the Frechet derivative of Aa (f) exists.
□
Lemma 5. Define P∞ = {pf = exp2-a (f - Aa (f)) : f ∈ L∞ (S)} where L∞ (S) is the space of
almost surely bounded measurable functions with domain S. Fix f0 ∈ L∞. Then for any fixed > 0
and Pg,pf ∈ P∞ such that f, g ∈ B∞(fo) the L∞ closed ball around fo, there exists constant
Mexp > 0 depending only on f0 such that
kpf - pg kLr ≤ 2Mexp kf - g k∞
Further
BΩa(Pf ,Pg ) ≤ α~1 kPf - Pg k∞[(kPf k∞ + kPf - Pg k∞尸-1 +exP2-a(2kgk∞X
Proof. This Lemma mirrors Lemma A.1 in Sriperumbudur et al. (2017), but the proof is very dif-
ferent as they rely on the property that exp(x + y) = exp(x) exp(y), which does not hold for
β-exponentials. We thus had to strengthen the assumption to include that f and g lie in a closed
ball, and then use the functional mean value theorem Lemma 3 as the main technique to achieve our
result.
Consider that by functional mean value inequality,
kPf -PgkLr = k expβ(f -Aa(f)) - expβ(g - Aa(g))kLr
≤ k expβ(h - Aa(h))2-ak∞(kf - gk∞ + |Aa(f) - Aa(g)|)	(13)
where h = cf + (1 - c)g for some c ∈ [0, 1]. We need to bound expβ(h - Aa(h)) and kAa(f) -
Aa (g)k∞ .
19
Under review as a conference paper at ICLR 2022
We can show a bound on khk∞
khk∞ = kcf + (1 - c)g - f0 + f0 k∞
≤ kc(f - f0) + (1 - c)(g - f0) + f0 k∞
≤ ckf - f0 k∞ + (1 - c)kg - f0 k∞ + kf0 k∞
≤ + kf0k∞
so that h is bounded. Now We previously showed in claim 1 that ∣Aα(h)∣ ≤ IlhII∞ ≤ E + kfok∞.
Since h, Aα(h) are both bounded expβ (h - Aα(h))2-α is also.
Now note that by Lemma 3,
∣Aα(f) — Aα(g)∣ ≤ kAα(h)kopkf -gk∞
We need to show that ∣∣Aα(h)kop is bounded for f,g ∈ Be(fo). Note that in Lemma 4 we showed
that
∣Aα(f )(g)∣ = ∣Epf-α [g(τ )]∣
≤ Ig I∞
Thus IlAaIlOP = sup{∣Aα(h)(m)∣ ： ∣∣m∣∣∞ = 1} ≤ 1. Let MeXP be the bound on exp.(h 一 Aα(h)).
Then putting everything together we have the desired result
Ipf 一 pg ILr ≤ 2MeXP If 一 g I∞
Now
BΩα(Pf ,Pg ) = Ca(Pf ) - Ca (Pg ) —〈▽Ca(Pg ),Pf 一 Pg i2	(14)
For the inner prodct term, first note that following Martins et al. (2020) the gradient is given by
VΩa(Pg )(t) = Pg(t)a 1
α 一 1
(15)
Thus
KVaa(Pg ),Pf - Pg i2| ≤ IIVCa(Pg ) ∣∣ 1 ∣∣Pf - Pg ∣∣∞
=α-ι/eχp2-a(g(t) 一 A(g))dQkPf - Pg ιι∞
≤ a-yexPz-alzHgHsNPf -Pgll∞
where the second line follows from claim 1.
Further note that by Taylor’s theorem,
ya = xa + αza-1(y 一 x)	(16)
for some z between x and y. Then letting y = Pf (t) and x = Pg(t), we have for some z = h(t)
lying between Pf (t) and Pg (t) that
Pf(t)a = Pg (t)a + αh(t)aT(Pf(t) — Pg(t))
Since f ∈ L∞ then applying Claim 1 we have that each Pf,Pg ∈ L∞ and thus h is. Then
∣Pf(t)a — Pg(t)al = α∣h(t)la-1∣Pf(t) -Pg(t)l
≤ αIhIa∞-1 IPf 一 Pg I∞
≤ αmaχ{∣∣Pf ll∞, MgUaTllPf -Pgll∞
≤ α(IPf I∞ + IPf 一 Pg I∞)a-1 IPf 一 Pg I∞	(17)
so that
∣Ωa(Pf ) 一 Ωa(Pg )| = —八 Pf (t)a — Pg (t)a )dQ
α(α 一 1)
≤ α⅛ι("Pf ∣∣∞ + ∣∣Pf - Pg k厂-」必 一 Pg ιι∞.
20
Under review as a conference paper at ICLR 2022
Figure 2: General architecture for classification using continuous attention mechanisms. The
pipeline is trained end-to-end. The encoder takes a discretized representation of an observation
(i.e. a time series) and outputs parameters for an attention density. The value function takes the
original (potentially irregularly sampled) time series and outputs parameters for a function V (t).
These are then combined in an attention mechanism by computing a context vector c = Ep[V (T)].
For some parametrizations of p and V (t) this can be computed in closed form, while for others it
must be done via numerical integration. The context vector is then fed into a classifier.
Putting it all together we obtain
BΩα (Pf,pg) ≤ α - 1 (kpf k∞ + kpf ― Pgk∞ )α Ikpf - Pgk∞
+ α~1exP2-α(2kgk∞)kPf - Pg k∞
=-ʒ kPf - Pg k∞[(kPf k∞ + kPf - Pg k∞)α-1 +exP2-αPkgk∞)]
α-1
C uWave Experiments: Additional Details
We experiment with N = 64, 128 and 256 basis functions, and use a learning rate of 1e - 4. We use
H = 100 attention mechanisms, or heads. Unlike Vaswani et al. (2017), our use of multiple heads
is slightly different as we use the same value function for each head, and only vary the attention
densities. Additional architectural details are given below.
C.1 Value Function
The value function uses regularized linear regression on the original time series observed at random
observation times (which are not dependent on the data) to obtain an approximation V (t; B) =
BΨ(t) ≈ X (t). The H in Eqn. 6 is the original time series.
C.1.1 Encoder
In the encoder, we use the value function to interpolate the irregularly sampled time series at the
original points. This is then passed through a convolutional layer with 4 filters and filter size 5
followed by a max pooling layer with pool size 2. This is followed by one hidden layer with 256
21
Under review as a conference paper at ICLR 2022
units and an output V of size 256. The attention densities for each head h = 1,∙∙∙ ,H are then
μh = wh,ιv
σh = softplus(whT,2v)
γh = W(h)v
for vectors wh,1,wh,2 and matrices Wh and heads h = 1,…，H
C.1.2 Attention Mechanism
After forming densities and normalizing, We have densities pι(t),…，Ph(t), which We use to com-
pute context scalars
ch = Eph[V (T)]
We compute these expectations using numerical integration to compute basis function expectations
Eph [ψn(T)] and a parametrized value function V (t) = Bψ(t) as described in section 3.
C.1.3 Classifier
The classifier takes as input the concatenated context scalars as a vector. A linear layer is then
folloWed by a softmax activation to output class probabilities.
D MIT BIH: Additional Details
Note that our architecture takes some inspiration for the H that We use in our value function from a
github repository5, although they used tensorfloW and We implemented our method in pytorch.
D. 1 Value Function
The value function regresses the output of repeated convolutional and max pool layers on basis
functions, Where the original time series Was the input to these convolutional/max pooling layers.
All max pool layers have pool size 2. There are multiple sets of tWo convolutional layers folloWed
by a max pooling layer. The first set of convolutional layers has 16 filters and filter size 5. The
second and third each have 32 filters of size 3. The fourth has one With 32 filters and one With 256,
each of size 3. The final output has 256 dimensions of length 23. This is then used as our H matrix
in Eqn 6.
D.2 Encoder
The encoder takes the original time series as input. It has one hidden layer With a ReLU activation
function and 64 hidden units. It outputs the attention density parameters.
D.3 Attention Mechanism
The attention mechanism takes the parameters from the encoder and forms an attention density. It
then computes
c=Ep[V(T)]
(18)
for input to the classifier.
D.4 Classifier
The classifier has tWo hidden layers With ReLU activation and outputs class probabilities. Each
hidden layer has 64 hidden units.
5https://github.com/CVxTz/ECG_Heartbeat_Classification
22