Under review as a conference paper at ICLR 2022
Parallel Deep Neural Networks
Have Zero Duality Gap
Anonymous authors
Paper under double-blind review
Ab stract
Training deep neural networks is a well-known highly non-convex problem. In re-
cent work (Pilanci & Ergen, 2020), it is shown that there is no duality gap for reg-
ularized two-layer neural networks with ReLU activation, which enables global
optimization via convex programs. For multi-layer linear networks with vector
outputs, we formulate convex dual problems and demonstrate that the duality gap
is non-zero for depth three and deeper networks. However, by modifying the deep
networks to more powerful parallel architectures, we show that the duality gap
is exactly zero. Therefore, strong convex duality holds, and hence there exist
equivalent convex programs that enable training deep networks to global optimal-
ity. We also demonstrate that the weight decay regularization in the parameters
explicitly encourages low-rank solutions via closed-form expressions. For three-
layer non-parallel ReLU networks, we show that strong duality holds for rank-1
data matrices, however, the duality gap is non-zero for whitened data matrices.
Similarly, by transforming the neural network architecture into a corresponding
parallel version, the duality gap vanishes.
1	Introduction
Deep neural networks demonstrate outstanding representation and generalization abilities in popular
learning problems ranging from computer vision, natural language processing to recommendation
system. Although the training problem of deep neural networks is a highly non-convex optimization
problem, simple algorithms, such as stochastic gradient descent, can find a solution with good gener-
alization properties. The non-convex and non-linear nature of neural networks render the theoretical
understanding of neural networks extremely challenging.
The Lagrangian dual problem (Boyd et al., 2004) plays an important role in the theory of convex
and non-convex optimization. For convex optimization problems, the convex duality is an important
tool to determine its optimal value and to characterize the optimal solutions. Even for a non-convex
primal problem, the dual problem is a convex optimization problem the can be solved efficiently. As
a result of weak duality, the optimal value of the dual problem serves as a non-trivial lower bound for
the optimal primal objective value. Although the duality gap is non-zero for non-convex problems,
the dual problem provides a convex relaxation of the non-convex primal problem. For example, the
semi-definite programming relaxation of the two-way partitioning problem can be derived from its
dual problem Boyd et al. (2004).
The convex duality also has important applications in machine learning. In (Paternain et al., 2019),
the design problem of an all-encompassing reward can be formulated as a constrained reinforcement
learning problem, which is shown to have zero duality. This property gives a theoretical convergence
guarantee of the primal-dual algorithm for solving this problem. Meanwhile, the minimax generative
adversarial net (GAN) training problem can be tackled using duality (Farnia & Tse, 2018).
In lines of recent works, the convex duality can also be applied for analyzing the optimal layer
weights of two-layer neural networks with linear or ReLU activations (Pilanci & Ergen, 2020; Ergen
& Pilanci, 2020a). Based on the convex duality framework, the training problem of two-layer neural
networks with ReLU activation can be represented in terms of a single convex program in (Pilanci
& Ergen, 2020). Such convex optimization formulations are extended to two-layer and three-layer
convolutional neural network training problems in (Ergen & Pilanci, 2020c). Strong duality also
holds for deep linear neural networks with scalar output (Ergen & Pilanci, 2020b). The convex
1
Under review as a conference paper at ICLR 2022
optimization formulation essentially gives a detailed characterization of the global optimum of the
training problem. This enables us to examine in numerical experiments whether popular optimizers
for neural networks, such as gradient descent or stochastic gradient descent, converge to the global
optimum of the training loss.
Admittedly, the zero duality gap is hard to achieve for deep neural networks, especially for those
with vector outputs. This imposes more difficulty to train deep neural networks. Fortunately, neural
networks with parallel structures (also known as multi-branch architecture) appear to be easier to
train. Practically, the usage of parallel neural networks dates back to AlexNet (Krizhevsky et al.,
2012). Modern neural network architecture including Inception (Szegedy et al., 2017), Xception
(Chollet, 2017) and SqueezeNet (Iandola et al., 2016) utilize the parallel structure. As the “parallel”
version of ResNet (He et al., 2016a;b), ResNeXt (Xie et al., 2017) and Wide ResNet (Zagoruyko &
Komodakis, 2016) exhibit improved performance on many applications. Recently, it was shown that
neural networks with the parallel architecture have smaller duality gaps (Zhang et al., 2019) com-
pared to standard neural networks. On the other hand, it is known that overparameterized parallel
neural networks have benign training landscapes (Haeffele & Vidal, 2017). For training `2 loss with
deep linear networks using Schatten norm regularization, Zhang et al. (2019) show that there is no
duality gap. From another perspective, the standard two-layer network is equivalent to the parallel
two-layer network. This may also explain why there is no duality gap for two-layer neural networks.
1.1	Contributions
Following the convex duality framework introduced in (Ergen & Pilanci, 2020b;a), which showed
the duality gap is zero for two-layer networks, we go beyond two-layer and study the convex duality
for vector-output deep neural networks with linear activation and ReLU activation. Surprisingly, we
prove that three-layer networks may have duality gaps depending on their architecture, unlike
two-layer neural networks which always have zero duality gap. We summarize our contributions
as follows.
•	For training standard vector-output deep linear networks using `2 regularization, we pre-
cisely calculate the optimal value of the primal and dual problems and show that the duality
gap is non-zero, i.e., Lagrangian relaxation is inexact. We also demonstrate that the `2-
regularization on the parameter explicitly forces a tendency toward a low-rank solution,
which is boosted with the depth. However, we show that the optimal solution is available
in closed-form.
•	For parallel deep linear networks, with certain convex regularization, we show that the
duality gap is zero, i.e, Lagrangian relaxation is exact.
•	For training vector-output three-layer ReLU networks with standard architecture using `2
regularization, even when the data is whitened, we show that the duality gap is non-zero.
The gap can be closed by replacing the neural network to one with parallel architecture.
•	For parallel deep ReLU networks of arbitrary depth, with certain convex regularization, we
prove strong duality, i.e., the duality gap is zero. Remarkably, this guarantees that there
is a convex program equivalent to the original deep ReLU neural network problem.
We summarize the duality gaps for parallel/standard neural network in Table 1.
1.2	Notations
We use bold capital letters to represent matrices and bold lowercase letters to represent vectors. De-
note [n] = {1, . . . , n}. For a matrix Wl ∈ Rm1 ×m2, for i ∈ [m1] and j ∈ [m2], we denote wlc,oil
as its i-th column and wlr,ojw as its j-th row. Throughout the paper, X ∈ RN ×d is the data matrix
consisting of d dimensional N samples and Y ∈ RN ×K is the label matrix for a regression/classifi-
cation task with K outputs.
1.3	Motivations and Background
Recently a series of papers (Pilanci & Ergen, 2020; Ergen & Pilanci, 2020b;a) studied two-layer
neural networks via convex duality and proved that strong duality holds for these architectures.
2
Under review as a conference paper at ICLR 2022
Table 1: Duality gaps for L-layer standard and parallel architectures. we compare our duality gap
characterization with previous literature. Each check mark indicates whether a characterization of
the duality gap exists for the corresponding architecture and the number next to it indicates whether
the gap is zero or not.
I	Linear					ReLU		
			L=2	L = 3	L>3	L = 2	L = 3	L > 3
Standard	Ergen & Pilanci (2020a; 2021a)	X	X	X	✓(0)	X	X
	Bach (2017) Pilanci & Ergen (2020)	✓(0)	X	X	✓(0)	X	X
	Ergen & Pilanci (2021b)	X	X	X	✓(0)	X	X
	Ergen & Pilanci (2021c)	✓(0)	X	X	X	X	X
	This paper	✓(0)	✓(= 0)	✓(6= 0)	✓(0)	✓(6= 0)	X
Parallel	Ergen & Pilanci (2021b)	X	X	X	✓(0)	✓(0)	X
	Zhang et al. (2019) Ergen & Pilanci (2021c)	✓(0)	✓(0)	✓(0)	X	X	X
	This paper	✓(0)	✓(0)	✓(0)	✓(0)	✓(0)	✓(0)
Particularly, these prior works consider the following weight decay regularized training framework
for classification/regression tasks. Given a data matrix X ∈ RN ×d consisting of d dimensional N
samples and the corresponding label matrix y ∈ RN, the weight-decay regularized training problem
for a scalar-output neural network with m hidden neurons can be written as follows
1β
P = Vmm 5kφ(XWI)W2- yk2 + HkWikF + I∣w2k2),	⑴
W1 ,w2 2	2
where W1 ∈ Rd×m and W2 ∈ Rm are the layer weights, β > 0 is a regularization parameter, and φ
is the activation function, which can be linear φ(z) = z or ReLU φ(z) = max{z, 0}. Then, one can
take the dual of (1) with respect to W1 and W2 obtain the following dual optimization problem
D := max — 1 kλ — yk2 + 1 kyk2, s.t.	max	|XTφ(XWI)| ≤ 仇	⑵
λ 2	2	w1 :kw1 k2 ≤1
We first note that since the training problem (1) is non-convex, strong duality may not hold, i.e.,
P ≥ D. Surprisingly, as shown in Pilanci & Ergen (2020); Ergen & Pilanci (2020b;a), strong
duality in fact holds, i.e., P = D, for two-layer networks and therefore one can derive exact convex
representations for the non-convex training problem in (1). However, extensions of this approach to
deeper and state-of-the-art architectures are not available in the literature. Based on this observation,
the central question we address in this paper is:
Does strong duality hold for deep neural networks?
Depending on the answer to the question above, an immediate next questions we address is
Can we characterize the duality gap (P-D)? Is there an architecture for which strong duality holds
independent of the depth?
Consequently, throughout the paper, we provide a full characterization of convex duality for deeper
neural networks. Then, based on this characterization, we propose a modified architecture for which
strong duality holds regardless of depth.
1.4	Convex duality for two-layer neural networks
We briefly review the convex duality for two-layer neural networks introduced in (Ergen & Pilanci,
2020b;a). Consider the following weight-decay regularized training problem for a vector-output
neural network architecture with m hidden neurons
1β
Amin 5 kφ(XWI)W2 - YkF + -(kW1kF + kw2kF),	⑶
W1 ,W2 2	2
3
Under review as a conference paper at ICLR 2022
where W1 ∈ Rd×m and W2 ∈ Rm×K are the variables, and β > 0 is a regularization parameter.
Here φ is the activation function, which can be linear φ(z) = z or ReLU φ(z) = max{z, 0}. As
long as the network is sufficiently overparameterized, there exists a feasible solution for such that
φ(XW1)W2 = Y. Then, a minimal norm variant1 of the training problem in (3) is given by
min 1(kWιkF + ∣∣W2kF) s.t. φ(XW1)W2 = Y.	(4)
W1 ,W2 2
As shown in (Pilanci & Ergen, 2020), after a suitable rescaling, this problem can be reformulated as
m
min Xkwr2o,jwk2s.t.φ(XW1)W2=Y,kwc1o,jlk2 ≤1,j∈ [m].	(5)
W1 ,W2	,	,
j=1
where [m] = {1, . . . , m}. Here wr2o,jw represents the j-th row of W2 and w1co,jl denotes the j-th
column of W1 . The rescaling does not change the solution to (4). By taking the dual with respect
to W1 and W2 , the dual problem of (5) with respect to variables is a convex optimization problem
given by
max tr(ΛT Y), s.t.	max kΛT φ(Xu)k2 ≤1,	(6)
Λ	u"∣u∣∣2≤1
where Λ ∈ RN ×K is the dual variable. Provided that m ≥ m*, where m* ≤ N + 1, the strong
duality holds, i.e., the optimal value of the primal problem (5) equals to the optimal value of the dual
problem (6).
1.5	Organization
This paper is organized as follows. In Section 2, we review standard neural networks and intro-
duce parallel architectures. For deep linear networks, we derive primal and dual problems for both
standard and parallel architectures and provide calculations of optimal values of these problems in
Section 3. We derive primal and dual problems for three-layer ReLU networks with standard archi-
tecture and precisely calculate the optimal values for whitened data in Section 4. We also show that
deep ReLU networks with parallel structures have no duality gap.
2	Standard neural networks vs parallel architectures
We start with the L-layer neural network with the standard architecture:
fθ(X) =AL-1WL, Al = φ(Al-1Wl), ∀l ∈ [L- 1], A0 =X,
where φ is the activation function, Wl ∈ Rml-1 ×ml is the weight matrix in the l-th layer and θ =
(W1, . . . , WL) represents the parameter of the neural network. We also denote the input and output
dimensions as m0 = d and mL = K for simplicity and assume that ml ≥ max{d, K}, ∀l ∈ [L-2].
We then introduce the neural network with the parallel architecture:
fθprl(X) = AL-1WL,
Al,j =φ(Al-1,jWl,j),∀j ∈ [m],∀l∈ [L-1],
A0,j = X, ∀j ∈ [m].
Here for l ∈ [L - 1], the l-th layer has m weight matrices Wl,j ∈ Rml-1 ×ml where j ∈ [m].
Specifically, we let mL-1 = 1. In short, we can view the output AL-1 from a parallel neural
network as a concatenation of m scalar-output standard neural work. In Figure 1 and 2, we provide
examples of neural networks with standard and parallel architectures. We shall emphasize that for
L = 2, the standard neural network is identical to the parallel neural network.
1This corresponds to weak regularization, i.e., β → 0 in (3) as considered in Wei et al. (2018).
4
Under review as a conference paper at ICLR 2022
Input Layer 1 Layer 2 Layer 3 Layer 4
Figure 1: Standard Architecture
Input Layer 1 Layer 2 Layer 3 Layer 4
Figure 2: Parallel Architecture
3 Deep linear networks
In this section, we discuss the convex duality of deep linear network with vector output.
3.1 Standard deep linear networks
We first consider the neural network with standard architecture, i.e., fθ(X) = XW1 . . . WL. Con-
sider the following minimal norm optimization problem:
1L
Plin = min	H IWikF, s.t. XWi,..., WL = Y,	⑺
{Wl }lL=1 2 i=1	F
where the variables are W1 , . . . , WL . As shown in the Proposition 3.1 in (Ergen & Pilanci, 2020b),
by introducing a scale parameter t, the problem (7) can be reformulated as
Plin = min L-212 + Plin(t),
where Plin (t) is defined as
K
Plin(t) = {Wmi}nL XkwLro,wjk2,
l l=1 j=1	(8)
s.t. kWi kF ≤ t, i ∈ [L- 2], kwLco-l 1,j k2 ≤ 1, j ∈ [mL-1],
XW1 . . . WL = Y.
The following proposition characterize the dual problem of Plin(t) and its bi-dual, i.e., dual of the
dual problem.
Proposition 1 The dual problem of Plin(t) defined in (8) is a convex optimization problem given by
Dlin(t) = max tr(ΛTY)
Λ
s.t. kΛTXW1...WL-2wL-1k2≤1,	(9)
kWikF ≤ t, ∀i ∈ [L- 2], kwL-1k2 ≤ 1.
There exists m* ≤ KN + 1 such that the dual problem Dlin (t) Can be reformulated as the bi-dual,
i.e.,
*
m
Dlin(t) = WminL X kwLro,wj k2,
{Wl,j }l=1 j=1
m*	(10)
s.t. XW1,j... WL-2,j wLco-l 1,j wLro,wj =Y,
j=1
kWi,j IlF ≤ t,i ∈ [L - 2],j ∈ [m*], IlWL-Ijk2 ≤ 1,j ∈ [mJ
5
Under review as a conference paper at ICLR 2022
Detailed derivation of the dual and the bi-dual are provided in Appendix B.1. The reason why we
do not directly take the dual of Plin is that the objective function in Plin involves the weights of first
L - 1 layer, which prevents obtaining a meaningful dual problem. We note that bi-dual problem is
related to the minimal norm problem of a parallel neural network with aligned weights. Namely, the
Frobenius norm of the weight matrices in each branch has the same upper bound t.
For a matrix A ∈ Rm×n and p > 0, the Schatten-p quasi-norm of A is defined as
(min{m,n}
X	σip(A)
where σi(A) is the i-th largest singular value of A. The optimal value of Plin(t) and Dlin(t) can be
precisely calculated.
Theorem 1 For fixed t > 0, the optimal value of Plin(t) and Dlin(t) are given by
Plin (t) = t-(L-2)kXtYks2∕L ,	(11)
and
Diin(t)= t-(L-2)∣∣χtY∣∣*,	(12)
where Xt is the pseudo inverse of X and ∣∣∙∣∣* represents the nuclear norm.
As a result, the duality gap exists, i.e., P > D, for standard deep linear networks with L ≥ 3, if the
singular values of XtY are not equal to the same value. We note that the optimal scale parameter
t for the primal problem Plin is given by t* = ∣∣W*∣∣S/L. To the best of our knowledge, this
result wasn’t shown previously. We leave the proof in Appendix B.2 and provide conditions on
W1,...,WLto achieve the optimal value.
In the calculation of Plin(t), we utilize the following proposition.
Proposition 2 Suppose that W ∈ Rd×K with rank r is given. Assume that mi ≥ rfor i = 0, . . . , L.
Consider the following optimization problem:
min 1 (kWιkF + …+ HWlHF), s.t. W1W2 …WL = W.	(13)
{Wl}lL=1 2	F	F
Then, the optimal value ofthe problem (13) is given by L k WkS/L ∙
3.2 Parallel deep linear neural networks
For the neural network with parallel structure, we consider the minimal norm optimization problem:
L-1 m
{Wmin=ι 2 (X X kWij kF+ kWLkF),
m
s.t. X XW1,j... WL-2,j wLco-l 1,j wLro,wj =Y.
j=1
(14)
Due to a rescaling to achieve the lower bound of the inequality of arithmetic and geometric means,
we have the following result.
Proposition 3 The problem (14) can be formulated as
min
{Wl,j}l∈[L],j∈[m]
m
t X kwLow k2/L，
m
s.t. XXW1,j...WL-2,jwcLo-l1,jwLro,wj =Y,
j=1
(15)
kWl,j kF ≤	1,	l ∈	[L - 2], j ∈	[m], kwcLo-l	1,j k2 ≤	1,	j ∈	[m].
6
Under review as a conference paper at ICLR 2022
We note that z2/L is a non-convex of z and we cannot take a meaningful dual. The bi-dual problem
of the standard neural network does NOT correspond to the primal problem of the parallel neural
network because the Frobenius norm of each parallel part of the parallel architecture can be different
in formulating (15).
We can consider another regularization, i.e.,
L-1 m
Plpnl = eminL ? XX kWl,j kF + kWL kF,
{Wl,j }lL=1 2 l=1 j=1
m
s.t. X XW1,j... WL-2,j wcLo-l 1,j wrLo,wj =Y.
j=1
Proposition 4 The problem (16) can be formulated as
m
Plpnl = {w min 2 X kwL,wk2,
{Wl,j}l∈[L],j∈[m] 2 j=1
m
s.t. X XW1,j... WL-2,j wcLo-l 1,j wrLo,wj =Y,
j=1
kWl,j kF ≤ 1, l ∈ [L -2], j ∈ [m], kwLco-l 1,j k2 ≤ 1, j ∈ [m].
The dual problem of Plpinrl is a convex problem
(16)
(17)
Dlpinrl = mΛaxtr(ΛTY),
s.t. kΛTXW1...WL-2wL-1k2 ≤ L/2,	(18)
∀kWikF ≤ 1,i ∈ [L - 2], kwL-1k2 ≤ 1.
For the parallel linear network, the strong duality holds.
Theorem 2 There exists m* ≤ KN + 1 such that as long as the number of branches m ≥ m*,
the strong duality holds for the problem (16). Namely, Plpinrl = Dlpinrl. The optimal values are both
L kχtγk* ∙
This implies that there exist equivalent convex problems which achieve the global optimum of the
deep parallel linear network. Comparatively, optimizing deep parallel linear neural networks can be
much easier than optimizing deep standard linear networks.
3.3 General loss functions
Now we consider general loss functions, i.e.,
βL
min '(χwι...WL,Y) + βEkWikF,
{Wl}lL=1	2 i=1	F
where '(Z, Y) is a general loss function and β > 0 is a regularization parameter. According to
Proposition 2, the above problem is equivalent to
mWn'(XW, Y) + βL kWkS2LL.	(19)
The `2 regularization term becomes the Schatten-2/L quasi-norm on W to the power 2/L. Suppose
that there exists W such that l(χW, Y) = 0. With β → 0, asymptotically, the optimal solution to
the problem (19) converges to the optimal solution of
mWn kWkS/LL, s∙t∙'(XW, Y) = 0.	(20)
In other words, the `2 regularization explicitly regularizes the training problem to find a low-rank
solution W.
7
Under review as a conference paper at ICLR 2022
4 Neural networks with ReLU activation
Now, we focus on the three-layer neural network with ReLU activation, i.e., φ(z) = max{z, 0}.
4.1	Standard three-layer ReLU networks
We first focus on the three-layer ReLU network with standard architecture. Consider the minimal
norm problem
13
PReLU = min 5 kW IIWikF, s.t. ((XW1)+W2)+W3 = Y.	(21)
{Wi}i3=1 2 i=1
Here we denote (z)+ = max{z, 0}. Similarly, by introducing a scale parameter t, this problem can
be formulated as
PReLU = min 212 + PReLU(t),
where PReLU(t) is defined as
K
PReLU(t) = min3 X kw3ro,jwk2,
{Wi}i=1 j=1	(22)
s.t. kW1kF ≤ t,kw2co,jlk2 ≤ 1, j ∈ [m2],
((XW1)+W2)+W3 = Y.
The proof is analagous to the proof of Proposition 3.1 in (Ergen & Pilanci, 2020b). For W1 ∈
Rd×m, we define the set
A(W1)={((XW1)+w2)+|kw2k2 ≤ 1}.	(23)
Proposition 5 The dual problem of PReLU(t) defined in (22) is a convex problem defined as
DReLU(t) = max tr(ΛT Y), s.t.kΛTvk2 ≤ 1,v ∈ A(W1),∀kW1kF ≤ t. (24)
Λ
There exists m* ≤ KN +1 such that the dual problem can be reformulated as the bi-dual problem,
i.e.,
K
DReLU(t) =	* min *	…X ∣w30W∣2,
{W1 j}⅛ ,W2∈Rm1×m* ,W3∈Rm*×κ W
'「	j=	(25)
m
s.t. X((XW1,j)+w2co,jl)+w3ro,jw=Y,kW1,jkF≤t,kw2co,jlk2 ≤1.
j=1
For the case where the data matrix is with rank 1 and the neural network is with scalar output, there
is no duality gap.
Theorem 3 For a three-layer scalar-output ReLU network, let X be a rank-one data matrix. Then,
strong duality holds, i.e., PReLu(t) = DReLu(t). Suppose that λ* is the optimal solution to the dual
problem DReLU(t), then the optimal weights for each layer can be formulated as
WI= tsign(I(T)T(C)+| - |(T)T( —C)+DρoPT, w2 = Pi.
Here ρ0 = a0/ka0k2 and ρ1 ∈ R+ml satisfieskρ1k	= 1.
Now, we consider the case where the data matrix is whitened, i.e., XXT = In and Y has orthogonal
columns. We leave the proof of Theorem 4 and the characterization of optimal solutions in Appendix
C.3.
8
Under review as a conference paper at ICLR 2022
Theorem 4 Let {X, Y} be a dataset such that XXT = In and Y has orthogonal columns. Then,
the optimal value of PReLU(t) and DReLU (t) are given by
(K	∖ 3/2
X(k(yj)+k2/3 + k(p )+k2/3))
and
n
DReLU(t)=t-1X(k(yj)+k2+k(-yj)+k2).
j=1
(26)
(27)
Suppose that Y is the one-hot encoding of the label. Then, we note that yj ≥ 0 and k(yj)+ k2 is the
square root of the number of data points in the j -th class. Therefore, we recover the result about the
duality gap of standard three-layer ReLU networks.
4.2 Parallel deep ReLU networks
For the parallel architecture, we show that there is no duality gap for arbitrary deep ReLU network
with large enough number of branches. Consider the following minimal norm problem:
L-1 m
PReLU= min 2 XX kWι,j kF + IWlIIF ,
2 l=1 j=1
m
s.t. X(((XW1,j)+... WL-2,j)+
wLco-l 1,j )+wrLo,wj = Y.
j=1
Proposition 6 The problem (28) can be reformulated as
m
PReLu = min2 X kwL,Wk2,
2 j=1
m
s.t. X(((XW1,j)+... WL-2,j)+
wLco-l 1,j )+wLro,wj = Y,
j=1
kWl,j kF ≤ 1, l ∈ [L - 2], kwLco-l 1,j k2 ≤ 1, j ∈ [m].
The dual problem (29) is a convex problem defined as
DRprelLU = max tr(ΛT Y), s.t.	max	kΛT vk2 ≤ L/2.
ReLU	v=((XW1)+...WL-2)+wL-1)+,	2
kWlkF≤1,l∈[L-2],kwL-1k2≤1
(28)
(29)
(30)
For deep ReLU network with parallel architecture, the strong duality holds.
Theorem 5 The strong duality holds for (29) in the sense that PReLU = DRelLU for m ≥ m*, where
m* is upper bounded by KN + L
Similar to case of parallel deep linear networks, the parallel deep ReLU network also achieves zero-
duality gap. Therefore, to find the global optimum for parallel deep ReLU network is equivalent to
solve a convex program.
5	Conclusion
We present the convex duality framework for standard neural networks, considering both multi-layer
linear networks and three-layer ReLU networks with rank-1 or whitened data. In stark contrast to
the two-layer case, the duality gap can be non-zero for neural networks with depth three or more.
Meanwhile, for neural networks with parallel architecture, with the regularization of L-th power of
Frobenius norm in the parameters, we show that strong duality holds and the duality gap reduces to
zero. A limitation of our work is that we primarily focus on minimal norm interpolation problems.
We expect to generalize our results to general regularized training problems.
9
Under review as a conference paper at ICLR 2022
6	Acknowledgements
This work was partially supported by the National Science Foundation under grants ECCS-2037304,
DMS-2134248, the Army Research Office.
References
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research,18(1):629-681, 2017.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Francois Chollet. XcePtion: Deep learning with depthwise separable convolutions. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 1251-1258, 2017.
Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In International Conference on Artificial Intelligence and Statistics, pp.
4024-4033. PMLR, 2020a.
Tolga Ergen and Mert Pilanci. Convex duality of deep neural networks. arXiv preprint
arXiv:2002.09773, 2020b.
Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex opti-
mization of two-and three-layer networks in polynomial time. arXiv preprint arXiv:2006.14798,
2020c.
Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
Journal of Machine Learning Research, 22(212):1-63, 2021a.
Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks
via convex programs. In International Conference on Machine Learning, pp. 2993-3003. PMLR,
2021b.
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
In International Conference on Machine Learning, pp. 3004-3014. PMLR, 2021c.
Farzan Farnia and David Tse. A convex duality framework for gans. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
831caa1b600f852b7844499430ecac17-Paper.pdf.
Benjamin D Haeffele and Rene Vidal. Global optimality in neural network training. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331-7339, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters andj 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Santiago Paternain, Luiz FO Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained
reinforcement learning has zero duality gap. arXiv preprint arXiv:1910.13393, 2019.
10
Under review as a conference paper at ICLR 2022
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time con-
vex optimization formulations for two-layer networks. arXiv preprint arXiv:2002.10553, 2020.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. L1 regularization in infinite di-
mensional feature spaces. In International Conference on Computational Learning Theory, pp.
544-558. Springer, 2007.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 31, 2017.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. arXiv preprint arXiv:1810.05369, 2018.
Saining Xie, Ross Girshick, Piotr Dollar, ZhUoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 1492-1500, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. arXiv preprint
arXiv:1605.07146, 2016.
Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural netWorks With multi-branch
architectures are intrinsically less non-convex. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 1099-1109. PMLR, 2019.
11
Under review as a conference paper at ICLR 2022
A S tairs of duality gap for standard deep linear networks
Now we consider partially dualizing the non-convex optimization problem by exchanging a subset
of the minimization problems with respect to the hidden layers. Consider the Lagrangian for the
primal problem of standard deep linear network
Plin(t) = min max tr(ΛT Y) - I kΛT XW1... WL-2wL-1k2 ≤ 1 ,
{Wl}lL=-11 Λ	(31)
s.t. kWikF ≤ t, i ∈ [L - 2], kwL-1 k2 ≤ 1.
By changing the order of L - 2 mins and the max in (31), for l = 0, 1, . . . , L - 2, we can define the
l-th partial “dual” problem
Dl(iln)(t) = min max min	tr(ΛT Y) -I kΛT XW1 ... WL-2wL-1 k2 ≤ 1 ,
W1,...Wl Λ Wl+1,...,WL-2	(32)
s.t. kWi kF ≤ t, i ∈ [L - 2], kwL-1 k2 ≤ 1.
For l = 0, Dl(iln) (t) corresponds the primal problem Plin(t), while for l = L - 2, Dl(iln) (t) is the
dual problem Dlin(t). From the following proposition, we illustrate that the dual problem of Dl(iln) (t)
corresponds to a minimal norm problem of a neural network with parallel structure.
Proposition 7 There exists m* ≤ KN + 1 such that the problem D(n (t) is equivalent to the “bi-
dual” problem
m*
minX kwLro,wj k2,
j=1
--*
m
s.t. X XW1... WlWl+1,j... WL-2,j wcLo-l 1,j wrLo,wj =Y,	(33)
j=1
IlWilIF ≤ t,i ∈ [l], kWijkF ≤ t,i = l +1,...,L - 2,j ∈ [m*],
kwL0-1,jk2 ≤ 1,j ∈ [m*],
where the variables are Wi ∈ Rmi-1 ×mi for i ∈ [l], Wi,j ∈ Rmi-1 ×mi for i = l + 1, . . . , L - 2,
j ∈ [m*], WL-1 ∈ RmL-2×m* and WL ∈ Rm* ×mL.
We can interpret the problem (33) as the minimal norm problem of a linear network with parallel
structures in (l + 1)-th to (L - 2)-th layers. This indicates that for l = 0, 1, . . . , L - 2, the bi-dual
formulation of Dl(iln)(t) can be viewed as an interpolation from a network with standard structure to
a network with parallel structure. Now, we calculate the exact value of Dl(iln) (t).
Proposition 8 The optimal value Dl(iln) (t) follows
D(iln(t)= t-(L-2)kχtγks2∕(i+2).	(34)
Suppose that the eigenvalues XtY are not identical to each other. Then, We have
RinS= D(iL-2)(t) > D(L-3)(t) > …> Dl(O)(t) = D(t).	(35)
In Figure 3, We plot Dl(iln)(t) for l = 0, . . . , 5 for an example.
B Proofs of main results for linear networks
B.1 Proof of Proposition 1
Consider the Lagrangian function
K
L(W1,...,WL,Λ)=XkwL,jk2+tr(ΛT(Y-XW1...WL)).
j=1
(36)
12
Under review as a conference paper at ICLR 2022
Figure 3: Example of Dl(iln) (t).
Here Λ ∈ RN ×K is the dual variable. We note that
P(t) = min	maxL(W1,...,WL,Λ),
W1,...,WL Λ
s.t. kWi kF ≤ t, i ∈ [L - 2], kwLco-l 1,j k2 ≤ 1, j ∈ [mL-1],
mL-1
= min	maxtr(ΛTY) - X I kΛTXW1...WL-2wL-1,jk2 ≤ 1
W1,...,WL-1 Λ	,
j=1
s.t. kWi kF ≤ t, i ∈ [L - 2], kwLco-l 1,j k2 ≤ 1, j ∈ [mL-1],
= min	maxtr(ΛT Y) -I kΛT XW1 ...WL-2wL-1k2 ≤ 1
W1,...,WL-2,WL-1 Λ
s.t. kWikF ≤ t, i ∈ [L - 2], kwL-1 k2 ≤ 1.
(37)
Here I(A) is 0 if the statement A is true. Otherwise it is +∞. For fixed W1 , . . . , WL-1, the
constraint on WL is linear so we can exchange the order of maxΛ and minWL in the second line of
(37).
By exchanging the order of min and max, we obtain the dual problem
D(t) =max	min	tr(ΛTY) -IkΛTXW1...WL-2wL-1k2 ≤ 1 ,
Λ W1,...,WL-2
s.t. kWikF ≤ t, i ∈ [L - 2], kwL-1 k2 ≤ 1,
= max tr(ΛT Y)	(38)
s.t. kΛTXW1...WL-2wL-1k2 ≤ 1
∀kWikF ≤t,i ∈ [L - 2], kwL-1k2 ≤ 1.
Now we derive the bi-dual problem. The dual problem can be reformulated as
max tr(ΛT Y),
s.t. kΛTXW1...WL-2wL-1k2 ≤ 1,	(39)
∀(W1,...,WL-2,wL-1) ∈ Θ.
Here the set Θ is defined as
Θ = {(W1, . . . , WL-2, wL-1)|kWikF ≤ t, i ∈ [L - 2], kwL-1 k2 ≤ 1}.	(40)
By writing θ = (W1, . . . , WL-2, wL-1), the dual of the problem (39) is given by
min kμ∣∣TV,
s.t. L∈θ XWi ... WLqWL-idμ (θ) = Y.
(41)
Here μ : Σ → RK is a signed vector measure and Σ is a σ-field of subsets of Θ. The norm kμkτv
is the total variation of μ, which can be calculated by
kμkτv = h sup {/ UT(θ)dμ(θ) =: χ/ Ui(θ)dμi(θ)},
(42)
13
Under review as a conference paper at ICLR 2022
μι
where We write μ =	.	. The formulation in (41) has infinite width in each layer. According to
.
μκ
Theorem 9 in Appendix E, the measure μ in the integral can be represented by finitely many Dirac
delta functions. Therefore, we can rewrite the problem (41) as
m*
minX kwLro,wj k2,
j=1
m*	(43)
s.t.	XW1,j...WL-2,jwcLo-l1,jwLro,wj =Y,
j=1
IIWijlIF ≤ t,i ∈ [L - 2], kwL-i,jk2 ≤ 1,j ∈ [m*].
Here the variables are Wi,j for i ∈ [L 一 2] and j ∈ [m*], Wl-ι and Wl. As the strong duality
holds for the problem (43) and (39), we can reformulate the problem of Dlin (t) as the bi-dual
problem (43).
B.2 Proof of Proposition 2
We restate Proposition 2 with details.
Proposition 9 Suppose that W ∈ Rd×K with rank r is given. Consider the following optimization
problem:
min 2 (kWιkF + …+ ∣Wl∣F), s.t. W1W2 …WL = W,	(44)
in variables Wi ∈ Rmi-1 ×mi. Here m0 = d, mL = K and mi ≥ r for i = 1, . . . , L 一 1. Then,
the optimal value of the problem (44) is given by
L kWkS2L「	(45)
Suppose that W = UΣVT. The optimal value can be achieved when
Wi = Ui-1∑1∕LUT, i = 1,...,N, Uo = U, UL = V.	(46)
Here Ui ∈ Rr×mi satisfies that UiTUi = I.
We start with two lemmas.
Lemma 1 Suppose that A ∈ Sn×n is a positive semi-definite matrix. Then, for any 0 < p < 1, we
have
nn
X Aipi ≥ X λi (A)p .	(47)
Here λi is the i-th largest eigenvalue of A.
Lemma 2 Suppose that P ∈ Rd×d is a projection matrix. Then, for arbitrary W ∈ Rd×K, we have
σi(PW) ≤ σi(W),
where σi(W) represents the i-th largest singular value of W.
Now, we present the proof for Proposition 2. For L = 1, the statement apparently holds. Suppose
that for L = l this statement holds. For L = l + 1, by writing A = W2 . . . Wl+1, we have
min ∣Wι∣F + …+ ∣Wl∣F , s.t. W1W2... W1+1 = W
=minIW1I2F+lIAI22∕∕ll, s.t. W1A = W,	(48)
=mint2 +lIAI22∕∕ll, s.t. W1A = W, IW1IF ≤ t.
Suppose that t is fixed. It is sufficient to consider the following problem:
minIAI22∕∕ll, s.t. W1A = W, IW1IF ≤t.	(49)
14
Under review as a conference paper at ICLR 2022
Suppose that there exists W1 and A such that W = W1A. Then, we have WAtA =
W1AAtA = W. As WAt = W1AAt, according to Lemma 2, kWAtkF ≤ kW1kF ≤ t.
Therefore, (WAt, A) is also feasible for the problem (49). Hence, the problem (49) is equivalent
to
min ∣∣Ak2∕l, s.t. WAtA = W, kWAtkF ≤ t.
(50)
Assume that W is with rank r. Suppose that A = UΣVT, where Σ ∈ Rr0×r0
we have At = VΣ-1UT. We note that
kWAtk2F
= tr(WVΣ-2VT WT)
= tr(VT WT WVΣ-2)
Denote G(V) = VTWTWV. This implies that
r
X σi(A)-2 (G(V))ii ≤ t2.
i=1
. Here r0 ≥ r. Then,
(51)
Therefore, we have
r0
Xσi(A)-2(G(V))ii
0i"
l+1
(G(V))i1i/(l+1)
As WVTV = W, the non-zero eigenvalues of G(V) are exactly the non-zero eigenvalues of
WVVT WT = WWT, i.e., the square of non-zero singular values of W. From Lemma 1, we
have
r0
X (G(V))i1i/(l+1)
i=1
Therefore, we have
r0
≥ £%(G(V))1/(l+1) ≥ ][>i(W产(I+1).
(52)
i=1
i=1
kAk2S/2l/l
r0
X。" ≥ t-2/l
i=1
Xr(l+1)/l
σi(W)")
(53)
r
This also implies that
minkAk22//ll, s.t. W1A = W, kW1kF ≤t
≥t-2∕l(X bi(W)2/(l+1)!(i+i)/i.
Suppose that W = Pir=1 uiσiviT is the SVD of W. We can let
(PrT σ2∕(l+1)y∕2 r
A =巨一~X X Ui。”+1)PT,
t	i=1
r
Wi =-——t-E X P冠/…V.
(Pr=ι O"+1))" W
Here kPik2 = 1 and PiT Pj = 0 for i 6= j. Then, W1A = W and kW1kF ≤ t. We also have
(54)
(55)
kAk2S/2L/L =t-2/l
=t-2/l
r1/l r
σ2∕(l+1)	X σ"l+1)
i=1
(X σ,(W)3["I
15
Under review as a conference paper at ICLR 2022
In summary, we have
min t2 +lkAk2S/2l/l, s.t. W1A = W, kW1kF ≤ t.
min t2 + lt-2/l
t>0
Xr(l+1)/l
bi(W)2/(l+1))
=(l+1)
(XX bi(W)2/(l+1)
(l+1)/2
(56)
=kWk
2/(l+1)
S2/(l+1)
This completes the proof.
B.3 Proof of Theorem 1
We first compute the optimal value P of the primal problem. From Proposition 2, the minimal norm
problem (7) is equivalent to
min LkWk2S/2L/L, s.t.XW=Y,	(57)
in variable W ∈ Rd×K . According to Lemma 2, for any feasible W satisfying XW = Y, because
XtXW = XtY and XtX is a projection matrix, we have
LkWkS/Ll ≥ LkXtYkS/Ll.	(58)
We also note that XXtY = XXtXW = XW = Y. Therefore, XtY is also feasible for the
problem (57). This indicates that Plin = L∣∣XtYkS/^-
On the other hand, for a feasible point (W1, . . . , WL) for Plin(t), we note that
(W1/t, . . . , WL-2/t, WL-1, tL-2WL) is feasible for Plin(1). This implies that tL-2Plin(t) =
Plin(1), or equivalently, Plin(t) = t-(L-2)Plin(1). Recall that
Plin =min L 2 212 +1-(L-2)Plin⑴
=2 (Plin ⑴产.
This implies that Plin(1) = kXtYkS2/L and
Plin(t) =t-(L-2)kXtYkS2/L.
(59)
(60)
For the dual problem Dlin(t) defined in (38), we note that
kΛTXW1...WL-2wL-1k2
≤kΛTXW1...WL-2k2kwL-1k2
L-2
≤kΛT Xk2 Y kWlk2kwL-1k2	(61)
l=1
L-2
≤kΛTXk2YkWlkFkwL-1k2= tL-2kΛTXk2.
l=1
The equality can be achieved when Wl = tululT+1 for l ∈ [L - 2], where kul k2 = 1 for l =
1, . . . , L - 1. Specifically, we set uL-1 = wL-1 and let u0 as right singular vector corresponds to
the largest singular value of ΛT X. Therefore, the constraints on Λ is equivalent to
kΛT Xk2 ≤ t-(L-2).	(62)
Thus, according to the Von Neumann’s trace inequality, it follows
tr(ΛTY) = tr(λτXXtY) ≤ ∣∣λτX∣∣2∣∣XtY∣∣* ≤ t-(L-2)IlXtY∣"	(63)
As a result, We have Dla(t) = t-(L-2)∣∣XtYk* < t-(L-2)|》*丫|层/工=Plin(t). Namely, the
duality gap exists for the standard neural network.
16
Under review as a conference paper at ICLR 2022
B.4 Proof of Proposition 3
For simplicity, we write WL-1,j = wLco-l 1,j and WL,j = wrLo,wj for j ∈ [m]. For the j-th branch of
the parallel network, let Wι,j = aι,jWlj for l ∈ [L]. Here aι,j > 0 for l ∈ [L] and they satisfies
that QlL=1 αl,j = 1 for j ∈ [m]. Therefore, we have
XWι,j …WL-2,jwL-ι,jwL,W = XW ι,j …W Ljj W L-Ij W Lw.	(64)
This implies that {Wl,j}l∈[L],j∈[m] is also feasible for the problem (14). According to the the
inequality of arithmetic and geometric means, the objective function in (14) is lower bounded by
mL
2 XX α2,jkWl,jkF
2 j=1 l=1
mL
≥ X L Y (α2,jLkWljkFZL)	(65)
j=1	l=1
mL
=2 XY kWljkF/L
j=1 l=1
QL kW k1/L
The equality is achieved when al,j = "l=1W_jF for l ∈ [L] and j ∈ [m]. As the scaling
operation does not change QlL=1 kWl,j k2F/L, we can simply let kWl,j kF = 1 and the lower bound
becomes L Pm=IIlWLjkFZL = L Pm=IIlWLw||2/L. This completes the proof.
B.5 Proof of Proposition 4
We first show that the problem (16) is equivalent to (17). The proof is analogous to the proof of
Proposition 3. For simplicity, we write WL-1,j = WcLo-l 1,j and WL,j = WrLo,wj for j ∈ [m]. Let
αl,j > 0 for l ∈ [L] and they satisfies that QlL=1 αl,j = 1 for j ∈ [m]. Consider another parallel
network {Wl,j }l∈[L],j∈[m] whose j-th branch is defined by Wl,j = αl,j Wl,j for l ∈ [L]. As
QlL=1 αl,j = 1, we have
XWIj …Wl-2 jWLo-1 jwL,w = XW1 j ... W Ljj W L-Ij W Lw.	(66)
This implies that {Wlj}l∈[L] j∈m] is also feasible for the problem (16). According to the the
inequality of arithmetic and geometric means, the objective function in (14) is lower bounded by
mL
1XX aLj kWlj kF
mL
≥ X 2 Y (αljk WljkF)
mL
=2 XYkWlj kF.
2 j=1 l=1
(67)
The equality is achieved when αl,j
Ql=IWWljkF for l ∈ [L] and j ∈ [m]. As the scaling
operation does not change QlL=1 kWl,j kF, we can simply let kWl,j kF = 1 and the lower bound
becomes LL Pm=I ∣∣WL,j|f = LL Pm=I IMLwk2. Hence, the problem (16) is equivalent to (17).
For the problem (17), we consider the Lagrangian function
mm
L(Wι,..., WL) = 2 X kWL,w k2 +tr bT (Y - X XWij... WLo-IjWL,w )1 .	(68)
17
Under review as a conference paper at ICLR 2022
The primal problem is equivalent to
Plpi rl =	min	maxL(W1,...,WL,Λ),
n	W1,...,WL Λ
s.t. kWl,j kF ≤ t, j ∈ [ml], l ∈ [L - 2], kwLco-l 1,j k2 ≤ 1, j ∈ [mL-1],
= min	max min L(W1, . . . , WL, Λ),
W1,...,WL-1 Λ WL
s.t. kWl,j kF ≤ 1, l ∈ [L - 2], kwLco-l 1,j k2 ≤ 1,j ∈ [m],
mL
= min	max tr(ΛT Y) - XI kΛT XW1,j ... WL-2,j wLco-l 1,j k2 ≤ L/2 ,
W1 ,...,WL-1 Λ
j=1
s.t. kWl,j kF ≤ 1, l ∈ [L - 2], kwLco-l 1,j k2 ≤ 1,j ∈ [m].
The dual problem follows
Dlpinrl = mΛax tr(ΛT Y),
s.t. kΛT XW1,j... WL-2,j k2 ≤ L/2,
∀kWl,jkF ≤ 1,l∈ [L - 2], kWcLo-l 1,j k2 ≤ 1,j∈ [m]
max tr(ΛT Y),
(69)
(70)
s.t. kΛTXW1...WL-2wL-1k2 ≤ L/2,
∀kWikF ≤ 1,i ∈ [L - 2], kwL-1k2 ≤ 1.
B.6 Proof of Theorem 2
We can rewrite the dual problem as
Dlpinrl = mΛaxtr(ΛTY),
s.t. kΛTXW1...WL-2wL-1k2 ≤ L/2,	(71)
∀(W1,..., WL-2, wL-1) ∈ Θ,
where the set Θ is defined as
Θ={(W1,...,WL-2,wL-1)|kWlkF ≤1,l∈ [L - 2], kwL-1k2 ≤ 1}.	(72)
By writing θ
given by
(W1, . . . , WL-2, wL-1), the bi-dual problem, i.e., the dual problem of (71), is
min kμkτv,
s.t. Jθ∈θ XWl ... WL-2WL-idμ (θ) = Y.
(73)
Here μ : Σ → RK is a signed vector measure, where Σ is a σ -field of subsets of Θ and k μ k TVisits
total variation. τhe formulation in (73) has infinite width in each layer. According to τheorem 9 in
Appendix E, the measure μ in the integral can be represented by finitely many Dirac delta functions.
Therefore, there exists m* < KN + 1 such that We can rewrite the problem (73) as
m*
minX kwrLo,wj k2,
j=1
m*	(74)
s.t. XW1,j... WL-2,j wLco-l 1,j wrLo,wj =Y,
j=1
IIWij ∣∣F ≤ 1,l ∈ [L - 2], kwL0-1,j k2 ≤ 1,j ∈ [m*].
Here the variables are Wlj for l ∈ [L 一 2] and j ∈ [m*], Wl-i and Wl. This is equivalent to
(17). As the strong duality holds for the problem (71) and (73), the primal problem (17) is equivalent
to the dual problem (71) as long as m ≥ m* .
18
Under review as a conference paper at ICLR 2022
Now, we compute the optimal value of Dlpinrl . Similar to the proof of Theorem 1, we can show that
the constraints in the dual problem (71) is equivalent to
kΛT Xk2 ≤ L/2.	(75)
Therefore, we have
tr(ΛTY) ≤ kλτX∣∣2∣XYk* ≤ 2∣∣χtYk*.	(76)
This implies that Plpnl = Dpnl = LL∣∣χtY∣∣*.
B.7 Proof of Proposition 8
We note that
max tr(ΛT Y),
s.t. ∣ΛTXW1...WL-2∣2 ≤ 1,∣Wi∣F ≤t,i=l+1,...,L-2
= max min	tr(ΛT Y), s.t. ∣ΛT XW1 . . . Wl∣2 ≤ t-(L-2-l)
Λ Wj+1,...,WL-2
Therefore, we can rewrite Dl(iln) (t) as
Dl(il) (t) =	min max tr(ΛT Y),
lin	W1,...Wl Λ
s.t. ∣ΛTXW1...Wl∣2 ≤ t-(L-2-l), ∣Wi∣F ≤t,i ∈ [l],
= min maxt-(L-2-l) tr(ΛT Y),
W1,...Wl Λ
s.t. ∣ΛTXW1...Wl∣2 ≤ 1,∣Wi∣F ≤t,i ∈ [l].
From the equation (11), we note that
min max tr(ΛT Y)
W1,...Wj Λ
s.t. ∣ΛTXW1...Wj∣2 ≤ 1,∣Wi∣F ≤t,i ∈ [j],
K
= min ∣wl+2,j ∣2 ,
j=1
s.t. ∣Wi∣F ≤ t,i ∈ [L-2], ∣wL-1,j ∣2 ≤ 1,j ∈ [mL-1],
XW1 . . . Wl+2 = Y
=t-l ∣xtYkS2/(l+2) .
This completes the proof.
C Proofs of main results for ReLU networks
C.1 Proof of Proposition 5
For the problem of P (t), introduce the Lagrangian function
K
L(W1,W2,W3,Λ) =X∣w3ro,jw∣2 - tr(ΛT (((XW1)+W2)+W3 - Y)).
j=1
According to the convex duality of two-layer ReLU network, we have
PReLU(t) =	min	max tr(ΛT Y) - I(∣ΛT((XW1)+w2)+ ∣2 ≤ 1)
kW1kF≤t,kw2k≤1 Λ
= min max min tr(ΛT Y) - I(∣ΛT((XW1)+w2)+∣2 ≤ 1)
kW1 kF≤t Λ kw2k≤1
= min	maxtr(ΛTY), s.t. ∣ΛT v∣2 ≤ 1, ∀v ∈ A(W1).
kW1 kF≤t Λ
(77)
(78)
(79)
(80)
(81)
19
Under review as a conference paper at ICLR 2022
By changing the min and max, we obtain the dual problem.
DReLU(t) = max tr(ΛT Y), s.t. ∣ΛT v∣2 ≤ 1,v ∈A(W1),∀∣W1∣F ≤t.
Λ
The dual of the dual problem writes
min k“∣TV,
s.t.八WikF≤t,kw2k2≤l ((XW1) + w2)+ dμ (W1, w2) = Y.
(82)
(83)
Here μ is a signed vector measure and ∣∣μkτv is its total variation. Similar to the proof of Proposition
1, we can find a finite representation for the optimal measure and transform this problem to
K
min	* K kw3 kw3,j ∣∣2,
{Wι,j}mι ,W2∈Rm1 ×m* ,W3∈Rm*×κ j=1
m*
s.t. X((XW1,j)+w2,j)+w3,j =Y, ∣W1,j∣F ≤ t, ∣w2,j∣2 ≤ 1.
j=1
Here m* ≤ KN +1. This completes the proof.
(84)
C.2 Proof of Theorem 3
For rank-1 data matrix that X = ca0T, suppose that A1 = (XW1)+. It is easy to observe that
A1 = (c)+a1T,+ + (-c)+a1T,-,
Here we let a1,+ = (W1a0)+ and a0,- = (-W1a0)+.
For a three-layer network, i.e., L = 3, suppose that λ* is the optimal solution to the dual problem
DReLU(t). We consider the extreme points
argllw ll max ll「|(T)*((XWI)+ w2)+|.	(85)
kW1 kF ≤t,kw2 k2 ≤1
For fixed W1, because a1T,+a1,- = 0, suppose that
w2 = u1a1,+ + u2a1,- + u3 r,
where rT a1,+ = rT a1,- = 0 and ∣r∣2 = 1. The maximization problem on w2 becomes
arg max (λ*)T(c)+∣a1,+∣22(u1)+ + (λ*)T(-c)+∣aL-2,-∣22(u2)+
s.t. u21∣a1,+∣22 +u22∣a1,+∣22 +u32 ≤ 1.
If (λ*)T (c)+ and (λ*)T (-c)+ have different signs, then the optimal value is
maχ{∣(λ*)τ(c)+∣kaL-2,+ k2, ∣(λ*)T(-c)+∣kaL-2,-k2}.
And the corresponding optimal wl-1 is wl-1	=	aL-2,+∕kaL-2,+ k or wl-i	=
aL-2,-∕kaL-2,-k. Then, the problem becomes
argmaxmax{∣(λ*)T(c)+∣∣aL-2,+ ∣2, ∣(λ*)T(-c)+∣∣∣aL-2,-k2}.
We note that
max{∣aL-2,+ ∣2, ∣aL-2,- ∣2} ≤ ∣W1Ta0∣2 ≤ ∣W1 ∣2 ∣a0∣2 ≤ t* ∣a0∣2.
Thus the optimal W1 follows
Wi = t*sign(∣(λ*)T (c)+∣-∣(λ*)τ (-c)+∣)ρoρT
Here ρ0 = a0∕ka01∣2 and pi ∈ Rml satisfies ∣∣ρι∣ = 1. This also gives the optimal w2 = Pi.
On the other hand, if (λ*)T(c)+ and (λ*)T(-c)+ have same signs, then, the optimal w2 follows
W =	∣(λ*)τ(c)+∣aL-2,+ + ∣(λ*)τ(-c)+∣aL-2,-
2	P((λ*)T(c)+)2∣aL-2,+ k2 + ((λ*)T(-c) + )2kaL-2,-∣2 .
The maximization problem is equivalent to
argmax((λ*)T(c)+)2∣aL-2,+∣22 + ((λ*)T(c)-)2∣aL-2,-∣22
By noting that
∣aL-2,+∣22 + ∣aL-2,+∣22 = ∣WiT a0∣22 ≤ ∣Wi∣22∣a0∣22 ≤(t*)2∣a0∣22,
the optimal Wi follows
Wi= tsign(∣(λ*)T (c)+∣-∣(λ*)τ (-c)+∣)popT.
Here ρ0 = a0∕∣a0∣2 and ρi ∈ R+m1 satisfies ∣ρi ∣ = 1. This also gives the optimal w2 = ρi.
20
Under review as a conference paper at ICLR 2022
C.3 Proof of Theorem 4
We restate Theorem 4 with details as follows.
Theorem 6 Let {X, Y} be a dataset such that XXT = In and Y has orthogonal columns. Then,
the optimal value of PReLU(t) and DReLU (t) are given by
/ K	T"
PReLu(t) = t-1 (X (k(yj)+ k2/3 + k(-yj )+k2∕3) J
and
n
DReLU(t)=t-1X(k(yj)+k2+k(-yj)+k2).
j=1
(86)
(87)
For the bi-dual formulation of DReLU (t) defined in (25), the optimal weight matrices for each layer
can be constructed as
W1，r = kφ⅛ 公 W?’' =&，
for r = 1,..∙, 2K. Here (φ0,2j-1, φ0,2j ) = (XT(yj )+ , XX-Yj ) + )，kφ1,rk = t, φ1,r ∈ 畔1 .
For PReLU(t), the optimal weight matrices for each layer write
2K
WI= X gr ∣φ0j∣2 φT,r , W2 =[的/，…，φ1,2K ] ∙
Here
g2j+1 =____________k，)+k2/3______________,
2'+1	(pj=ι (IM)+k2/3 + k(f )+胪))3/2'
g2 .+2 =_________k(-yj )+k2/3____________
2'+2	(pj=ι (k(yj)+k2/3 + k(-yj)+胪))3/2
We require that kφ1,j k2 = 1forj ∈ [2K] and φ1T,iφ1,j = 0for i 6= j.
Since XXT = In, we can characterize the set as
∪kWιkF≤tA(Wι) = {(z)+∣z ∈ RN, ∣∣z∣∣2 ≤ t}.	(88)
Here A(W1) is defined in (23). For one thing, for z ∈ RN, kzk2 ≤ t, we can let W1 = XTzvT and
w2 = v, where v ∈ R+m1 with kvk2 = 1. Then, kW1kF ≤ t, kw2k2 ≤ 1 and (XW1)+w2)+ =
(z)+. For another, for any kW1kF ≤ t and kw2k ≤ 1, we note that
k(XW1)+w2k2 ≤ k(XW1)+k2 ≤ k(XW1)+kF ≤ kXW1kF ≤ kW1kF ≤ t.
Therefore, ∪kWkF≤tA(W) = {(z)+|z ∈ RN, kzk2 ≤ t}.
Thus, the constraint on Λ in the dual problem (24) is equivalent to say that t∣(Λ*)T(z)+1∣2 ≤ 1 for
all z ∈ RN satisfying kzk2 ≤ 1. As Y has orthogonal columns, we note that
Kn
tr((Λ*)TY) = X(λ*)τ((yj)+ -(-Yj)+) ≤ t-1 X (k(yj)+∣∣2 + k(-yj)+∣∣2).
j=1	j=1
Suppose that (Yj)+ 6= 0 and (-Yj)+ 6= 0 for all j = 1, . . . , K. Then, the dual problem is minimized
by
a* = t-i Γ (y。+	_ (-y。+	(YK)+	_ ZK)+
Ul(yι)+k2	Il(-yι)+k2，	，k(yκ)+l∣2	ll(-yκ)+1反
We can also verify that
tI(Λ*)T(z)+I2 ≤ 1,∀IzI2 ≤ 1.	(89)
21
Under review as a conference paper at ICLR 2022
Suppose that (Z)+ = PK=ι αj,ι fj⅛ + αj,2 k((-jb + r, Where r ∈ RN is orthogonal to
(yj)+ and (-yj)+. Here αj,1, αj,2 ≥ 0. As k(z)+k ≤ kzk2 ≤ 1, we have
K
X(αj2,1 +αj2,2) ≤ 1 - krk22 ≤ 1.
j=1
We note that
KK
k(Λ*)Tt(z)+k2 = X(αj,ι - a”)2 ≤ X(α2,ι + α8) ≤ 1.	(90)
j=1	j=1
Thus, Λ satisfies the constraint (89).
We can characterize the optimal layer Weight to the bi-dual problem as the extreme points that solves
arg max	∣∣(Λ*)t ((XWI)+w2)+∣∣2.
kW1kF≤t,kw2k2≤1
These extreme points correspond to the constraints
(91)
(Λ*)T t(yjU
() ⑹)+k2
2
≤ 1	(λ*)t t(-yj)+
≤ 1, (A ) k(-yj)+k2
≤ 1.
2
In other Words, the dual problem is equivalent to
DReLU(t) =maxtr(ΛTY),
Λ
s.t.
λT t"yjU
k(yj )+k2
2 ≤ 1, Iλt ra+212 ≤ 1,1 ≤j ≤K
(92)
NoW, We consider an arbitrary matrix W1 satisfying ∣W1 ∣F ≤ t. Denote
P(W1) = max tr(ΛT Y), s.t. ∣ΛT v∣2 ≤ 1,∀v ∈ A(W1).	(93)
Λ
Suppose that Λ is the optimal solution. Then, we have
K
tr((Λ*)tY) = X(λ*)T((yj)+ - (-yj)+)
j=1
=X(UT ≡ IM )+k2
-X("T ι⅛)⅛ k(-yj )+k2.
For a vector λ ∈ RN, define
g(λ;Wι)= max ∣λτ((XWI)+W2)+∣.	(94)
kw2 k≤1
For a given u ∈ R+N, we want to estimate
max IuTλ∣ s.t. g(λ; Wι) ≤ 1.	(95)
The following lemma gives an upper bound.
Lemma 3 Suppose that u ∈ RN satisfying ∣u∣2 = 1. Then, for arbitrary λ satisfying g(λ; W1) ≤
1, we have IuTλ∣ ≤ 1∕g(u; Wi).
Denote z2j+1 =限*)+^ and z2j+2 = k((—j]k2. For simplicity, we write g(z; Wi) = g(z). We
note that λj satisfies that g(λ*) ≤ 1. According to Lemma 2, we have
K
P*(Wi) =tr((Λ)TY) ≤ X
j=i
(k(yj )+k2
k g(Z2j+l)
+ MyLkh)
g(Z2j+2))
22
Under review as a conference paper at ICLR 2022
This serves as an upper bound for P * (Wι). We note that for U ∈ RN, We have
g(u)2 = max |uT((XW1)+w2)+|2
kw2k≤1
≤ max |uT(XW1)+w2|2
kw2 k≤1
=kuT((XW1)+k22
≤kuTXW1k22
≤ tr(uuT XW1 W1T XT).
Then, we have
2K
X g(zj)2 ≤ tr
j=1
X ZjZT ) XW1WT X)≤ tr(XW1WT XT )= t2.
According to the Holder’s inequality, we have
(K	∖ 2/3 ∕2K	∖ 1/3
X (IKyj )+k2/g(Z2j+1) + IKyj )+k2/g(Z2j+2)))	(X g(Zj 尸)
K
≥X(k(yj)+k2/3 + k(-yj)+k2/3).
j=1
Thus, it follows
K
X(k(yj)+k2/g(Z2j+1)+k(yj)+k2/g(Z2j+2))
j=1
(∖ 3/2
X(I(yj)+I22/3+I(-yj)+I22/3)
The optimal g(zj ) follows
g(z2j + l) = --------tkjk/3-------------
PjK=1 k(yj)+k22/3+k(-yj)+k22/3
3/2 ,
g(Z2j+2)= (PK=1 (k(yj I；+：f)+k2/3))3/2.
This bound can be achieved for
2K
W1 = Xg(Zj)Zjφ1T,j,
j=1
Where Iφ1,j I2 = 1 forj ∈ [2K] and φ1T,iφ1,j = 0 for i 6= j.
C.4 Proof of Proposition 6
Analogous to the proof of Proposition 4, We can reformulate (28) into (29). The rest of the proof is
analogous to the proof of Proposition 4. For the problem (29), We consider the Lagrangian function
L(W1,...,WL)
m
ΛT(Y-X(((XW1,j)+..........WL-2,j)+
wLco-l 1,j)+wLro,wj )
j=1
Lm
2 X kwLow k2 + tr
(96)
23
Under review as a conference paper at ICLR 2022
The primal problem is equivalent to
prl
PReLU
= min	maxL(W1, . . . , WL, Λ),
W1,...,WL Λ	1	L
s.t. kWl,j kF ≤ t, j ∈ [ml], l ∈ [L - 2], kwcLo-l 1,j k2 ≤ 1,j ∈ [mL-1],
= min	maxminL(W1, . . . , WL, Λ),
W1,...,WL-1 Λ WL
s.t. kWl,j kF ≤ 1, l ∈ [L - 2], kwcLo-l 1,j k2 ≤ 1, j ∈ [m],
m
= min	mΛax tr(ΛT Y) - XI kΛT (((XW1,j)+ ...WL-2,j)+wcLo-l1,j)+k2 ≤ L/2 ,
W1 ,...,WL-1 Λ
j=1
s.t. kWl,j kF ≤ 1, l ∈ [L - 2], kwcLo-l 1,j k2 ≤ 1, j ∈ [m].
By exchanging the order of min and max, the dual problem follows
(97)
Dprl
DReLU
max tr(ΛT Y),
s.t. kΛT (((XW1,j)+... WL-2,j)+
wLco-l 1,j)+k2 ≤ L/2,
∀kWl,jkF ≤ 1,l∈ [L - 2], kWcLo-l 1,j k2 ≤ 1,j∈ [m]
max tr(ΛT Y),
s.t. kΛT(((XW1)+...WL-2)+wL-1)+k2 ≤ L/2,
∀kWikF ≤ 1,i ∈ [L - 2], kwL-1k2 ≤ 1.
(98)
C.5 Proof of Theorem 5
The proof is analogous to the proof of Theorem 2. We can rewrite the dual problem as
DRpreLU = max tr(Λ Y),
Λ
s.t. kΛT(((XW1)+...WL-2)+wL-1)+k2 ≤ L/2,	(99)
∀(W1,..., WL-2, wL-1) ∈ Θ,
where the set Θ is defined as
Θ={(W1,...,WL-2,wL-1)|kWlkF ≤1,l∈ [L-2],kwL-1k2 ≤ 1}.	(100)
By writing θ
given by
(W1, . . . , WL-2, wL-1), the bi-dual problem, i.e., the dual problem of (71), is
min kμkτv,
s.t. Rθ∈θ(((XWi)+ ...WL-2)+WL-1)+dμ(θ) = Y.
(101)
Here μ : Σ → RK is a signed vector measure, where Σ is a σ -field of subsets of Θ and k μ k TVisits
total variation. τhe formulation in (101) has infinite width in each layer. According to τheorem 9 in
Appendix E, the measure μ in the integral can be represented by finitely many Dirac delta functions.
Therefore, there exists m* ≤ KN +1 such that We can rewrite the problem (101) as
m*
minX kwrLo,wj k2,
j=1
m*	(102)
s.t.	(((XW1,j)+...WL-2,j)+wLco-l1,j)+wLro,wj =Y,
j=1
IlWijIIF ≤ 1,l ∈ [L - 2], kwLo-ι,jk2 ≤ 1,j ∈ [ml
Here the variables are Wι,j for l ∈ [L - 2] and j ∈ [m*], Wl-ι and Wl. This is equivalent to (29).
As the strong duality holds for the problem (99) and (101), the primal problem (29) is equivalent to
the dual problem (71) as long as m ≥ m*.
24
Under review as a conference paper at ICLR 2022
D Proofs of auxiliary results
D.1 Proof of Lemma 1
Denote a ∈ Rn such that ai = Aii and denote b ∈ Rn such that bi = λi(A). We can show that a is
majorized by b, i.e., for k ∈ [n - 1], we have
kk
a(i) ≤	b(i),
i=1	i=1
and Pin=1 ai = Pin=1 bi. Here a(i) is the i-th largest entry in a. We first note that
nn
XAii=tr(A)=Xλi(A).
On the other hand, for k ∈ [n - 1], we have
k
a(i) =	max	vTa
i=1	v∈Rn,vi∈{0,1},1T v=k
=	max	tr(diag(v)Adiag(v))
v∈Rn,vi∈{0,1},1T v=k
≤ max	tr(VAVT)
V∈Rk×n,VVT=I
kk
=Xλi(A) = Xb(i).
i=1	i=1
(103)
(104)
Therefore, a is majorized by b. As f(x) = -xp is a convex function, according to the Karamata’s
inequality, we have
nn
f(ai) ≤	f(bi).
This completes the proof.
D.2 Proof of Lemma 2
According to the min-max principle for singular value, we have
σi(W) =	min max kWxk2.
dim(S)=d-i+1 x∈S,kxk2 =1
As P is a projection matrix, for arbitrary x ∈ Rd, we have kPWxk2 ≤ kWxk2. Therefore, we have
max
x∈S,kxk2=1
kPWxk2 ≤
max
x∈S,kxk2=1
kWxk2.
This completes the proof.
D.3 Proof of Lemma 3
Suppose that λ(u) is optimal solution to
argmaxv|uτλ∣ s.t. g(λ; Wι) ≤ 1.	(105)
Denote ne is the number of extreme points of
arg max ∣(λ(u))T v|.
v∈A(W1)
Denote v1 , . . . , vne as extreme points of (105). We note that (105) is the dual problem for a scalar
output three-layer neural network. By applying the previous duality theorem, we can write u =
Pn= ι aiVi, where Vi ∈ A(Wι). Wealsohave IUTλ(u)∣ = Pn=ι |a/. Besides, we note that
ne	ne
1 = I∣uk2 = XaiuTVi ≤ X ∣ai∣g(u; Wι) = IUTλ(u)∣g(u; Wι).
i=1	i=1
This completes the proof.
25
Under review as a conference paper at ICLR 2022
E Caratheodory’s theorem and finite representation
We first review a generalized version of Caratheodory’s theorem introduced in (Rosset et al., 2007).
Theorem 7	Let μ be a positive measure Supported on a bounded subset D ⊆ RN. Then, there
exists a measure ν whose support is a finite subset of D, {z1, . . . , zk}, with k ≤ N + 1 such that
k
/ zdμ(z) =	ZidV(Zi),
D	i=1
(106)
and kμktv = IlVktv.
We can generalize this theorem to signed vector measures.
Theorem 8	Let μ : Σ → RK be a signed vector measure supported on a bounded subset D ⊆ RN.
Here Σ is a σ-field of subsets of D. Then, there exists a measure V whose support is a finite subset
of D, {Z1, . . . , Zk}, with k ≤ KN + 1 such that
k
Z zdμ(z) = XZidV(Zi),	(107)
D	i=1
and ∣∣νktv = kμktv.
Proof Let μ be a signed vector measure supported on a bounded subset D ⊆ RN. Consider the
extended set D = {zuτ|z ∈ D,u ∈ RK, ∣u∣2 = 1}. Then, μ corresponds to a scalar-valued
measure μ on the set D and ∣∣μ∣∣τv = kμkτv. We note that D is also bounded. Therefore, by
applying Theorem 7 to the set D and the measure μ, there exists a measure V whose support is a
finite subset of D, {zιuτ,..., ZkUT}, with k ≤ KN + 1 such that
k
/ Zdμ(Z) = X ZiUTdV(zjUT),
JD	=
(108)
and ∣∣μ∣∣TV = ∣∣∕∣∣tv∙ We can define V as the signed vector measure whose support is a finite subset
r	τ i i /	∖	τ7 ∖ EI H H	H ~ H	ιι ~ ιι	ιι ιι EI ♦	ι .
{zι,...,Zk} and dν(Zi) = Uid(ZiUi). Then, ∣v∣tv = ∣∣M∣tv = ∣∣μ∣∣τy = ∣∣μ∣∣TV∙ ThiS completes
the proof.
Now we are ready to present the theorem about the finite representation of a signed-vector measure.
Theorem 9	Suppose that θ is the parameter with a bounded domain Θ ⊆ Rp and φ(X, θ) :
RN×d × Θ → RN is an embedding of the parameter into the feature space. Consider the following
optimization problem
min ∣∣μ∣TV, s.t. / φ(X, θ)dμ(θ) = Y.
Θ
(109)
Assume that an optimal solution to (109) exists. Then, there exists an optimal solution μ SUPPorted
on at most KN + 1 features in Θ.
Proof Let μ be an optimal solution to (109). We can define a measure P on RN as the push-
forward of μ by P(B) = μ({θ∣φ(X, θ) ∈ B}). Denote D = {φ(X,θ)∣θ ∈ Θ}. We note that P
is supported on D and D is bounded. By applying Theorem (8) to the set D and the measure P,
we can find a measure Q whose support is a finite subset of D, {Z1, . . . , Zk} with k ≤ 2K(n + 1).
For each Zi ∈ D, We can find θi such that φ(X, θi) = Zi. Then, μ = Pk=I δ(θ 一 θi)dQ(Zi) is an
optimal solution to (109) with at most 2C(N + 1) features and ∣∣μ∣∣τv = ∣∣μ∣τv. Here δ(∙) is the
Dirac delta measure.
26