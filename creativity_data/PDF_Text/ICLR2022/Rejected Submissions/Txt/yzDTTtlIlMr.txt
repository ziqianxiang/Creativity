Under review as a conference paper at ICLR 2022
Does momentum Change the Implicit Bias on
separable data?
Anonymous authors
Paper under double-blind review
Ab stract
The momentum acceleration technique is widely adopted in many optimization
algorithms. However, the theoretical understanding on how the momentum affects
the generalization performance of the optimization algorithms is still unknown.
In this paper, we answer this question through analyzing the implicit bias of
momentum-based optimization. We prove that on the linear classification problem
with separable data, both SGD with momentum and Adam (without stochasticity)
converge to the L2 max-margin solution for exponential-tailed loss, which is the
same as vanilla gradient descent. That means, these optimizers with momentum ac-
celeration still converge to a model with low complexity, which provides guarantees
on their generalization. Technically, to overcome the difficulty brought by the error
accumulation in analyzing the momentum, we construct new Lyapunov functions
as a tool to analyze the gap between the model parameter and the max-margin
solution.
1	Introduction
It is widely believed that the optimizers have implicit bias in terms of selecting output parameters
among all the local minima on the landscape (Neyshabur et al., 2015; Keskar et al., 2017; Wilson
et al., 2017). It is shown in the analysis of Adaboost that the coordinate descent would converge to
the L1 max-margin solution for the linear classification task with exponetial-tailed loss ((Schapire
& Freund, 2013; Telgarsky, 2013)). Latter, Soudry et al. (2018) shows that gradient descent would
converge to the L2 max-margin solution under the same setting, which mirrors its good generalization
property in practice. Since then, many efforts have been taken on analyzing the implicit bias of
various local-search optimizers, including stochastic gradient descent (Nacson et al., 2019), steepest
descent (Gunasekar et al., 2018a), AdaGrad (Qian & Qian, 2019) and optimizers for homogeneous
neural networks (Lyu & Li, 2019; Ji & Telgarsky, 2020; Wang et al., 2021).
However, though the momentum acceleration technique is widely adopted in the optimization
algorithms in both convex and non-convex learning tasks (Sutskever et al., 2013; Vaswani et al., 2017;
Tan & Le, 2019), the understanding on how the momentum would affect generalization performance
of the optimization algorithms is still unclear. A natural question is:
Can we theoretically analyze the implicit bias of momentum-based optimizers?
In this paper, we take the first step to analyze the convergence of momentum based optimizers and
unveil their implicit bias. Specifically, we study the classification problem with linear model and
exponential-tailed loss using Stochastic Gradient Descent with Momentum (SGDM) and Adam
optimizers. We consider the optimizers with constant learning rate and constant momentum hyper-
parameters, which are widely adopted in practice, e.g., the default setting in popular machine learning
frameworks (Paszke et al., 2019) and in experiments (Xie et al., 2017). We note that Gradient Descent
with Momentum (GDM) can be viewed as a special case of SGDM, and naturally share the properties
for SGDM. Our main results are summarized in Theorem 1.
Theorem 1 (informal). With linear separable dataset S, for SGDM and Adam (without stochasticity,
abbreviated as w/s latter), the loss converges to 0 with rate O( t) where t is the numberof iterations,
the parameter norm diverges to infinity, and direction of parameters converges to the direction of the
L2 max-margin solution.
1
Under review as a conference paper at ICLR 2022
Theorem 1 states SGDM converges to the L2 max-margin solution, which is the same as SGD,
indicating that momentum does not affect the convergent direction. The good generalization behavior
of the output parameters of SGDM is well validated as the margin of a classifier is positively
correlated with its generalization error (Jiang et al., 2019). This is supported by existing experimental
observations (c.f. Figure 1, (Soudry et al., 2018) and Figure 2, (Nacson et al., 2019)). Similar claims
hold for Adam (w/s), which is also well supported by empirical results in Wang et al. (2021).
Our contributions are significant in terms of the following aspects:
•	We establish the implicit bias of the momentum based optimizers, an open problem since the
initial work Soudry et al. (2018). The momentum based optimizers are widely used in prac-
tice and our theoretical characterization deepens the understanding on their generalization
property, which is important by its own.
•	Technically, we propose a new Lyapunov function to analyze the convergence of SGDM,
which helps to bound the sum of squared gradients along the training trajectory. Compared
to the usual one, the new Lyapunov function depends on a middle variable of an alternative
update rule of SGDM, which helps to capture the historical dependence in the momentum
update. To our knowledge, such a technique has not been exploited ever before and can be
of independent interest for convergence analysis of momentum-based optimizers. We then
construct a new Lyapunov function to bound the difference of learned parameters and the
scaled max-margin solution, which finally leads to the direction convergence. This Lyapunov
function provides a direct way to establish the convergence to the desired direction.
Organization of This Paper. Section 2 collects further related works on the implicit bias of first
order optimizers and convergence of momentum-based optimizers. Section 3 shows basic settings
and assumptions which will be used throughout this paper. Section 4 studies the implicit bias of
GDM as a warm up, while Section 5 and Section 6 explore respectively the implicit bias of SGDM
and Adam (w/s). Discussions of these results are put in Section 7.
2	Further Related Works
Implicit Bias of First-order Optimization Methods. Soudry et al. (2018) prove that gradient
descent on linear classification problem with exponential-tailed loss converges to the direction of the
max L2 margin solution of the corresponding hard-margin Support Vector Machine. Nacson et al.
(2019) extend the results in (Soudry et al., 2018) to the stochastic case, proving that the convergent
direction of SGD is the same as GD almost surely. Qian & Qian (2019) go beyond the vanilla gradient
descent methods and consider the AdaGrad optimizer instead. They prove that the convergent
direction of AdaGrad has a dependency on the optimizing trajectory, which varies according to
the initialization. Ji & Telgarsky (2021) propose a primal-dual analysis framework for the linear
classification models, and prove a faster convergent rate of the margin by increasing the learning rate
according to the loss. Based on (Ji & Telgarsky, 2021), (Ji et al., 2021) design another algorithm
with an even faster convergent rate of margin by applying the Nesterov’s Acceleration Method on the
dual space. However, the corresponding form of the algorithm on the primal space is no longer a
Nesterov’s Acceleration Method nor GDM, which is significantly different from our settings.
On the other hand, there is another line of work trying to extend the result in the linear case to deep
neural networks. Ji & Telgarsky (2018); Gunasekar et al. (2018b) study the deep linear network and
Soudry et al. (2018) study the two-layer neural network with ReLU activation. Lyu & Li (2019)
propose a framework to analyze the asymptotic direction of GD on homogeneous neural networks,
proving that given there exists a time the network achieves 100% training accuracy, GD will converge
to some KKT point of the L2 max-margin problem. Wang et al. (2021) extend the framework of
Lyu & Li (2019) to adaptive optimizers, and prove RMSProp and Adam without momentum have
the same convergent direction as GD, while AdaGrad doesn’t. The results (Lyu & Li, 2019; Wang
et al., 2021) indicate that results in the linear model can be extended to deep homogeneous neural
networks, and suggest that the linear model is a proper start point to study the implicit bias. There are
also works on the implicit bias of regression problems with bounded optimal points and interesting
readers can refer to (Rosasco & Villa, 2015; Lin & Rosasco, 2017; Ali et al., 2020) etc. for details.
Convergence of Momentum-Based Optimization Methods. For convex optimization problems,
the convergence rate of Nesterov’s Acceleration Method has been proved in Nesterov (1983). In
2
Under review as a conference paper at ICLR 2022
contrast, although GDM (Polyak’s Heavy-Ball Method) was proposed in (Polyak, 1964) prior to the
Nesterov’s Acceleration Method, the convergence of GDM on convex loss with Lipschitz gradient
was not solved until Ghadimi et al. (2015) provides an ergodic convergent result for GDM, i.e., the
convergent result for the running average of the iterates. However, the ergodic result is undesired
under many learning scenarios, e.g., in classification tasks, the optimization algorithms usually
output the parameters of the last step. To the best of our knowledge, there is only two works on
the non-ergodic analysis of (S)GDM: Sun et al. (2019) and Tao et al. (2021). Sun et al. (2019)
prove that if the training loss is coercive (the training loss goes to infinity whenever parameter norm
goes to infinity), convex, and globally smooth, then, with constant momentum hyper-parameter, the
training loss converges to minima with rate O(t-1). Tao et al. (2021) analyze a case when momentum
coefficient increases to 1 and the gradient is bounded all over the parameter space, showing SGDM
can improve the convergence rate of SGD by a factor log(t).
There are also works on the gradient norm convergence of SGDM under various settings (Yan et al.,
2018; Yu et al., 2019; Liu et al., 2020) and on the investigation of momentum-based method from the
view point of dynamics (Sarao Mannelli & Urbani, 2021). However, there is no existing work on the
implicit bias of momentum-based optimizers for the classification problem, which is first analyzed by
this paper.
3	Preliminaries
In this paper, we focus on the linear model with exponential-tailed loss. We first derive the results for
binary classification, then we show that the methodology can be easily extended to the multi-class
classification problem.
Problem setting. The dataset used for training is defined as S = (xi, yi)iN=1, where xi ∈ Rd is the
i-th feature, and y% ∈ R is the i-th label (i = 1,2, ∙∙∙ , N). We will use the linear model to fit the
label: for any feature x ∈ Rd and parameter w ∈ Rd, the prediction is given by hw, xi.
For binary classification, given any data zi = (xi, yi) ∈ S, the individual loss for parameter w is
given as '(yi(w, x/). A common setting is to ensemble the feature and label together as Xi = y%x%
(there is a mapping T : (x, y) → yx, and Xi is T((xi, yi))). The individual loss can be rewritten as
'(w, (Xi, yi)) = '(yi(w, Xii) = '((w, Xi)).
The optimization target is defined as the averaged loss:
L(W) = PN=ι'(w, (Xi y/
Without loss of generality, we consider the case with normalized data1, that is, k Xik ≤ 1, ∀i ∈ [N].
Optimizer. Here we will introduce the update rules of SGDM and Adam (w/s). SGDM can be
viewed as a stochastic version of GDM by randomly choosing a subset of the dataset to update.
Specifically, the update rule of SGDM is given as
(SGDM) : w(t + 1) — w(t) = -ηVLB(t)(w(∖) + β(w(t) — w(t — 1)), ∀ ≥ 1,	(1)
where B(t) is a subset of S with size b which is sampled independently and uniformly with replace-
L	βi ~∖
ment, and Lb(。is defined as LB(t)(w) = —z∈B(t)-----. We also define Ft as the sub-sigma field
such that {w(t)}t∞=1 is adapted with respect to Filtration {Ft}t∞=1.
1The proof can be naturally applied to unnormalized data by letting '(x) = '(maxχ∈τ(S) ∣∣X∣∣ ∙ x) and
=_______⅛i______
maχχ∈τ(S) kxk .
3
Under review as a conference paper at ICLR 2022
The Adam (w/s) can be viewed as a variant of GDM in which the preconditioner is adopted, whose
form is characterized as follows:
m(0) = 0, m(t) = βιm(t 一 1) + (1 — βι)VL(w(t)), m(t) = --^tm(t), Vt ≥ 1,
1 - β1
V(0) = 0, ν(t) = β2ν(t — 1) + (1 — β2)(VL(w(t)))2 , ν(t) = -ɪ-V(t), Vt ≥ 1,
1 一 β2
(Adam (w/s)) : w(t)	= w(t	— 1) — η — m( I)	, Vt ≥	1,	(2)
PV(t — 1) + ε1d,	-	,
where 1、==) is called the preconditioner at step t.
ν( ^(i-1)+ε1d
Assumptions: The analysis of this paper are based on three common assumptions in existing literature
(first proposed by (Soudry et al., 2018)), respectively on the separability of the dataset, the individual
loss behaviour at the tail , and the smoothness of the individual loss. We list them as follows:
Assumption 1 (Linearly Separable Dataset). There exists one parameter w ∈ Rd, such that
hw, Xii > 0, Vi ∈ [N].
Assumption 2 (Exponential-tailed Loss). The individual loss ` is exponential-tailed, i.e.,
•	Differentiable and monotonically decreasing to zero, with its derivative also converging to
zero, i.e., limχ→∞ '(x) = lim.→∞ '0(x) = 0, and '0(x)<0 Vx;
•	Close to exponential loss when x is large enough, i.e., there exist positive constants
c, a, K, μ+, μ-,x+, X- and xo, such that,
Vx>x+ : —'0(x)	≤	c(1	+	e-μ+x)e-ax,	(3)
Vx > x- : —'0(x)	≥	c(1	—	e-μ-x)e-ax.	(4)
Assumption 3 (Smooth Loss). Either of the following assumptions holds regarding the case:
(D): (Deterministic Case) The individual loss ` is locally smooth, i.e., for any s0 ∈ R, there exists a
positive real Hs °, such that Vx, y ≥ so, ∣'0(x) — '0(y)∣ ≤ Hs0 |x — y|.
(S): (Stochastic Case) The individual loss ` is globally smooth, i.e., there exists a positive real H,
such that Vx, y ∈ R, ∣'0(x) — 0(y)| ≤ H|x - y|.
We provide explanations of these three assumptions respectively. Based on Assumption 1, we can
formally define the margin and the maximum margin solution of an optimization problem, which is
introduced in Definition 1
Definition 1. Let the margin γ(w) of parameter w defined as the lowest score of the prediction of w
over the dataset S, i.e., Y(W) = minχi∈τ(S) hw, Xii. ByAssumption 1 and the positive homogeneous
ofY, y( YwW)) = 1, and thus we define the maximum margin solution W and the L2 max margin Y of
the dataset S as follows:
4	2	41
w = arg min kwk , γ =-
Y(W)≥1	IlW k
Since ∣∣ ∙ ∣∣2 is strongly convex and set {w : Y(W) ≥ 1} is convex, W is uniquely defined.
Assumption 2 constraints the loss to be exponential-tailed, which is satisfied by many popular choices
of ', including the ('eχp(x) = e-x) and the logistic loss ('功(x) = log(1 + e-x)). Also, as C and a
can be respectively absorbed by resetting the learning rate and data as η = cη and Xi = aXi, without
loss of generality, in this paper we only analyze the case that c = a = 1.
The globally smooth assumption (Assumption 3. (S)) is strictly stronger than the locally smooth
assumption (Assumption 3. (D)). One can easily verify that both the exponential loss and the logistic
loss meet Assumption 3. (D), and the logistic loss also meets Assumption 3. (S).
4
Under review as a conference paper at ICLR 2022
4 Warm up: Implicit Bias of GDM
In this section, we study the implicit bias of GDM as a warm up. The update rule of GDM is
w(t + 1) 一 w(t) = -ηνL(w(t)) + β(w(t) — w(t - 1)), Vt ≥ 1	(5)
which is a determined case of SGDM, with batch size b = N and thus without randomness. Although
the analysis of GDM is essentially easier than that of SGDM, it helps us gain a better understanding
of this problem, and helps us to demonstrate several techniques which will be applied latter. The
formal theorem of the implicit bias of GDM is as follows:
ι-β
H'-1(NL(wi))
Theorem 2. Let Assumptions 1, 2, and 3. (D) hold. Let β ∈ [0, 1) and η < 2
. Then,
for almost every data set S2, with arbitrary initialization point w(1), GDM (Eq. (5)) satisfies that
w(t) — log(t)W is bounded as t → ∞, and limt→∞ |蔡(；)八
W
同'
Before we move on to the proof, we would like to provide some explanations of the results in Theorem
2. To begin with, Theorem 2 adopts an constant momentum hyper-parameter, which agrees with the
practice use (e.g., β is set to be 0.9 (Xie et al., 2017)). Also, Theorem 2 puts no restriction on the
range of β, which allows wider choices of hyper-parameter tuning. Furthermore, Theorem 2 shows
the implicit bias of GDM agrees with GD in linear classification with exponential-tailed loss (c.f.
Soudry et al. (2018) for results on GD), and this consistency can be verified by existing results (c.f.
Section 7 for detailed discussions).
We then present a proof sketch of Theorem 2, which is divided into two parts: we first prove that the
sum of squared gradients is bounded, which indicates both the loss and the norm of gradient converge
to 0; We then show the difference between w(t) and log(t)W is bounded, and therefore, the direction
of W dominates as t → ∞.
Stage I: Bound the sum of squared gradients. The core of Stage I is to select a proper Lyapunov
function ξ (t), which is required to correlated with the training loss L and be non-increasing along
the optimization trajectory. For GD, since L itself is non-increasing with properly chosen learning
rate, we can just pick ξ(t) = L(t). However, as the update of GDM doesn’t align with the direction
of negative gradient, training loss L(t) in GDM is no longer monotonously decreasing, and the
Lyapunov function requires special construction. A choice of ξ(t) is proposed as the follows:
Lemma 1. Let all conditions in Theorem 2 hold. Define ξ(t) 4 L(w(t)) + 得 ∣∣w(t)—
w(t —
1)∣2.
Let C1 be a positive real, s.t., η
2 ι-β
H'-1(N L(wι))
C1 . We then have
ξ(t) ≥ξ(t +1)+ (1 - β)(1 - CD kw(t + 1)- w(t)k2.	(6)
η
Remark 1. Although this Lyapunov function is obtained by (Sun et al., 2019) by directly examining
the Taylor’s expansion at w(t), the proof here is non-trivial as we only requires the loss to be locally
smooth instead of globally smooth in (Sun et al., 2019), and the Taylor’s expansion can only be
applied to w(t+1) if it’s ensured that all parameters on the line αw(t)+(1 -α)w(t+1) (α ∈ (0, 1))
have training loss no larger than L(w(0)).
To prove Lemma 1, we define w(t + α) = αw(t) + (1 - α)w(t + 1) for any t ∈ Z+ and α ∈ (0, 1),
and prove a generalized version of Eq. (6):
ξ(t) ≥ ξ(t + α)+ (I- β)(1-c)α2 IHt +1) - w(t)∣2.	⑺
η
The proof idea is that as long as Eq. (7) holds for all time in [1, t + α), the training loss across
[1, t + α] will be smaller than L(w(1)), and the strict inequality in Eq. (7) holds. Consequently, there
exists some small enough positive real ε, such that Eq. (7) holds for all time in [1, t + α + ε), and we
are able to extend the feasible set where Eq. (7) holds. The formal description of the generalized
lemma and its corresponding proof is deferred to Appendix B.1.1.
2Here "almost everywhere" means the conclusion holds except a zero-measure set in Rd×N.
5
Under review as a conference paper at ICLR 2022
By Lemma 1, we have that ξ(t) is monotonously decreasing by gap
(1-β)(1-c)α2
kw(t + 1) - w(t)k2.
η
As ξ(1) = L(w(1)) is a finite number, we have Pt∞=1 kw(t + 1) - w(t)k2 < ∞, which further
leads to the following corollary by the negative derivative of the individual loss and separable data:
Corollary 1. Let all conditions in Theorem 2 hold. We have, E∞=ι ∣∣VL(w(t))∣∣2 < ∞.
The proof of Corollary 1 can be found in Appendix B.1.1. Resulted from Corollary 1, we have
∣∣VL(w(t))k2 → 0 as t → ∞, which by the exponential-tailed loss assumption further leads to
limt→∞ L(w(t)) = 0.
Stage II. Bound the difference between w(t) and log(t)W. It seems natural to directly bound
∣∣w(t) 一 log(t)Wk across all iterations. SoUdry et al. (2018) and Nacson et al. (2019) indeed follow
this routine by showing the norm of r(t) 4 w(t) — log(t)W — W is bounded, where W is some
constant vector satisfying ehw,xi recovers the coefficient of support vector X in W. However, their
analyses rely on the fact that w(t + 1) 一 w(t) equals -ηVL(w(t)), which has a simple form, i.e.,
For GD ： ∣∣r(t + 1)∣∣2 - ∣r(t)∣2 = ∣∣r(t + 1) — r(t)∣2 — 2(log t++--W + ηVL(w(t)), r(t)〉.
However, it no longer holds for GDM, as w(t +1) — w(t) is a exponentially-decayed sum of gradients
at W(s), s ≤ t. To handle this problem, we propose a novel Lyapunov’s function for the direction
convergence, concluded as the following Lemma.
Lemma 2. Let all conditions in Theorem 2 hold. Then, ∣r(t)∣ is bounded if and only if the function
g(t) is upper bounded, where g ： Z+ → R is defined as
g⑴ 4 1 kr(t)k2 + Tj-β-ξhr⑴，W⑴-w(t - l)i - Tj-β-ξ Xhr(T) - r(T - I), W(T) - W(T - I)i. (8)
2	1-β	1-β
τ=2
Furthermore, we have	t∞=1(g(t + 1) — g(t)) is upper bounded.
The first claim in Lemma 2 provides an alternative approach to verify that ∣r(t)∣ is bounded, while
the second claim shows this approach can be fulfilled. The motivation of this lemma is to construct a
easy-to-verify criterion (i.e., g(t) is upper bounded) of ∣r(t)∣ is upper bounded. To demonstrate how
g(t) is constructed intuitively, we consider the continuous dynamics approximation of GDM, i.e.,
β	d2w(t)
1 — β	dt2
+ dw(t) +
+	dt	+
⅛ VL(w(t))=0.
1 一 β
We then calculate the 2 ∣∣r(t)∣2 — ɪ ∣∣r(1)∣2 by applying the integral of the dynamics as
/1 J d∣∣r(s)∣∣2 ds
Jl 2 ds
η	1	t β	d2w(s)
r(S),-- VL(W(S))-S W〉ds + J1 - WsL -2^ )ds
η1
r(s) , —	VL(w(s)) — —W d ds +
1-β	S
+	dr(S)
dS
ɪ〉ds
where the last equation is due to Integration by part. We denote g(t) = ɪ-e(hr(t), dW(t)〉—
RithdrdSs), dW(s) ids) +1 ∣∣r(t)∣∣2, and it can be verified the derivative of g(t) is(r(t), — IIne VL(w(t))
—1W). By replacing dW(t) as w(t) — w(t — 1), and drdtt) as r(t) — r(t — 1) in g(t), we obtain
g(t), and g(t + 1) — g(t) has the form <r(t), — Ine VL(W(t)) — log t++1 W)，which can be analyzed
following the similar routine as (Soudry et al., 2018). The proof of the second claim is completed.
On the other hand, the core of the proof of Lemma 2 is that 1 ∣∣r(t)∣2 + ɪ-e hr(t), W(t) — w( — 1)i
is bounded if and only if 2∣∣r(t)∣2 is bounded, while hr(τ) — r(τ — 1), W(T) — W(T — 1)i is an
absolute convergent based on Pt∞=1 ∣W(t + 1) — W(t)∣2 < ∞.
5 Tackle the difficulty brought by random sampling
In this section, we analyze the implicit bias of SGDM. The randomness introduced by random
sampling makes the analysis of GDM no longer work for SGDM. As an example, if we want to
6
Under review as a conference paper at ICLR 2022
follow the same routine of the GDM case to show L(w(t)) + 2ηE∣∣ w(t) - w(t - 1)k2 is a Lyapunov
function of SGDM, we can only have
EL(w(t)) + 2βηE∣∣w(t) - w(t - 1)k2 ≥EL(w(t + 1)) + ɪE∣∣E [w(t + 1) - w(t)∣Ft] k2
+ (1 - β)η(1 - c) EkE [w(t + 1) - w(t)∣Ft] k2.
As the squared first momentum is smaller than the second momentum, EL(w(t)) + 符E∣∣w(t)-
w(t - 1)k2 may no longer be monotonously decreasing for every β ∈ [0, 1). Furthermore, if the
margin γ is small, β will be required to be upper bounded by a small number, which contradicts to the
relatively large choice of β in practice, e.g. 0.9 (We defer a detailed discussion to Appendix B.2.3).
To tackle this problem, we propose a new Lyapunov function, with which we derive the following
theorem for the implicit bias of SGDM:
Theorem 3.	LetAssumption 1, 2, and 3. (S) hold. Let β ∈ [0,1) and η < min{ J-HβN , RHeN3β2b }.
bγ2
Then, for almost every data set S, with arbitrary initialization point w(1), SGDM (Eq. (1)) satisfies
w(t) — log(t)W is bounded as t → ∞ and limt→∞ 口*(；卜 = kWj, almost SureIy (a.s.).
To the best of our knowledge, this is the first convergence analysis of SGDM with no restriction on the
gradient norm and with constant learning rates. Also, as both J-N and (12H)NB b are monotonously
bγ2
increasing with respect to batch size b, Theorem 3 also sheds light on the learning rate tuning, i.e.,
the larger the batch size is, the larger the learning rate is. Furthermore, similar to the GDM case,
Theorem 3 shows the implicit bias of SGDM under this setting is consistent with SGD (c.f. (Nacson
et al., 2019) for the implicit bias of SGD). This matches the observations in practice (c.f. Section 7
for details).
Remark 2. In practice, mini-batch SGDM are more widely adopted, whose update rule differs
from SGDM only by the way obtaining B(t). Specifically, within the T-th epoch E(T) = {KT +
1,…，KT + T} (K is the length Ofone epoch), {B(t)}t∈E(τ)is a randomly and uniformly partition
of S. One may wonder whether the same result hold for mini-batch SGDM. The answer is "Yes",
with the a.s. condition removed in this case. We defer the detailed description of the corresponding
theorem together with the proof to Appendix D.1.
The proof sketch follows the same framework as that for GDM by dividing the proof into two stages.
However, the implementations in both stages differ, among which the proof of Stage I is significantly
distinctive, as a new Lyapunov function is proposed.
Stage I: Bound the sum of squared gradients. To obtain the new Lyapunov function, we first
present a lemma to provide an alternative form of update rule of SGDM:
Lemma 3. Define η 4 ɪ-e , and u(1) 4 w(1). The update rule ofSGDM (Eq. (1)) is equivalent to
u(t +1) = -ηPLB(t) (w(t))+ u(t),
w(t + 1) = βw(t) + (1 - β)u(t + 1).	(9)
By a simple rearrangement of the second equation, We have u(t + 1) = w(t) + ɪ-1e (w(t + 1) - w(t)),
which differs from w(t + 1) only by a larger step size updated from w(t). The following lemma then
indicates that EL(u(t)) is a proper selection of Lyapunov function.
Lemma 4. Let all conditions in Theorem 3 hold. Then, we have
E[L(u(t + 1))] ≤ EL(u(t)) - IEkVL(W(t))k2 十 一η
(X βt-1-sE kVL(w(s))k2
and
tη
E[L(u(t + 1))] ≤ L(u(1)) - X 4EkVL(W(S))k2.
s=1
7
Under review as a conference paper at ICLR 2022
EL(u(t)) may be not monotonously decreasing. However, the upper bound of EL(u(t)), i.e.,
L(U⑴)一 PS=i nɪ 'EkVL(w(s))k2, is monotonously decreasing by [EkVL(w(t))k2 at step t,
and leads to the sum of expected squared gradients being finite along the trajectory (which further
indicates the sum of squared gradients is finite, a.s.).
The proof idea of Lemma 4 is the expectation of the first order Taylor’s expansion of L at w(t) has
the form
Ln2
E[L(u(t + 1))∣Ft] ≤ L(u(t)) - nhVL(u(t)), VL(w(t))i + -ɪE[kVLB(t)(w(t))k2|FJ
As L is H smooth, we have VL(u(t)) = VL(w(t)) + O(ku(t) 一 w(t)k), where u(t) 一 w(t) =
，e(w(t) - w(t - 1)) = -n 1ββ PS=I β1tτ-SVLB(S)(W(Sy) is proportional to n. Therefore,
When n is small, -(VL(u(t)), VL(w(t))i is close to -∣∣VL(w(t))k2, and the coefficient of
-kVL(w(t))k2 becomes dominant.
Stage II. Bound the difference between w(t) and log(t)W. The Lyapunov function used by Stage
II of SGDM is the same as the g(t) in the GDM case. As We have proved the sum of squared gradient
along the trajectory is bounded, one can easily obtain that Pt∞=1 kw(t + 1) - w(t)k2 is bounded,
and kr(t)k being bounded is still equivalent to that g(t) is upper bounded. HoWever, When it comes
to the detailed calculation of analyzing g(t + 1) - g(t), it differs from the GDM case due to the
random subset. We defer the proof to Appendix B.2.2.
6 Analyze the effect of preconditioners
When it comes to the analysis of Adam (W/s), the effect of the preconditioner should be taken into
consideration When designing the corresponding Lyapunov functions. To tackle this problem, We
incorporate the preconditioner into the Lyapunov function, and obtain the implicit bias of Adam (W/s)
as folloWs:
Theorem 4.	Let Assumption 1, 2, and 3. (D) hold. Let 1 > β2 > β1 ≥ 0, and the learning rate n is
a small enough constant (The upper bound of learning rate is complex, and we defer it to Appendix
C.1). Then, for almost every data set S, with arbitrary initialization point w(1), Adam (w/s) (Eq. (2))
satisfies that w(t) — log(t)W is bounded as t → ∞, and limt→∞ 口,(；,
W
同'
Remark 3. Existing literature usually assume a time-decaying hyperparameter choice of β1 or β2
(c.f., (Kingma & Ba, 2014; Chen et al., 2018)). To the best of our knowledge, the only work analyzing
Adam with constant β1 and β2 is (Reddi et al., 2019), which assumes β2 > β12 (stronger than our
assumption). Our result indicates that the ranges of β1 and β2 can be broader in hyper-parameter
selection.
We still start the proof sketch by proving the sum of squared gradients is finite in Stage I. Compared
to GDM, the difference is that in Stage I, We Will also prove the loss converges to 0 With rate Θ(t-1).
This property Will be used in Stage II to analyze the effect of preconditioner on implicit bias.
Stage I: Bound the sum of squared gradients. The next lemma can be vieWed as an extension of
Lemma 1 by taking the preconditioner into consideration, and characterizes the one-step loss update
in Adam (W/s).
Lemma 5. Let all conditions in Theorem 4 hold. Then, for any t ≥ 1,
L(t + 1) + 2n(i -ββ))U Pε1d + ν(t) ® (w(t +1) - w(t))∣∣
≤L(w(t)) + --门'1JJIt-ι ∣∣Pε1d + 力(力一 1)©(w(t) 一w(t- 1))∣∣ .	(10)
2cn (1 - β1 ) 1 - (cβ1 )t-1
The difference betWeen the proof of Lemma 5 and Lemma 1 is that We need to handle the
gap betWeen the preconditioners at step t and step t + 1, this leads to a amplifying factor
ofterm
the shrinking factor
Pε1d + ^(t - 1) © (w(t) — w(t — 1))∣∣ , which, however, is smaller than
(1-βt-i) introduced by the gap between the coefficients of momentum
8
Under review as a conference paper at ICLR 2022
m(t) and m(t — 1). As t → ∞, both βt → 0 and (cβι)t → 0, and we can obtain that
ξ(t) 4 L(w(t)) + 2 √cη11-βI) Il P ε1d + ^(t — 1) Θ (w(t) 一 w(t - 1))∣∣ is a Lyapunov function.
Based on Lemma 5, we can prove the following asymptotic rate of L.
Lemma 6. Let all conditions in Theorem 4 hold. Then,
L(w(t)) = Θ (t-1), kw(t)k = Θ(log(t)), and kw(t) - w(t - 1)k = Θ(t-1).
The proof idea of Lemma 6 is that by the exponential-tailed property, when time is large enough, the
training loss can be bounded by the gradient, and further bounded by ∣∣ 4jε1d + ν(t — 1) Θ (w(t) 一
w(t - 1))∣. Consequently, ξ(t + 1) 一 ξ(t) ≤ -O(ξ(t)), andξ(t) = O(t-1).
Stage II. Bound the difference between w(t) and log(t)W. The Lyapunov function for the direction
difference is also modified according to the preconditioner, as introduced in Lemma 7:
Lemma 7. Let all conditions in Theorem 4 hold. Then, ∣r(t)∣ is bounded if and only if g(t) is upper
bounded, where g(t) is defined as follows.
g(t) 4 √2ε kr(t)k2 + T-^ Dr(t), (1 — β1-1)Pε1d + ^(t — 1) Θ (w(t) — w(t — 1)))
—
1 β1 f, Xhr(T)- r(T-1),(I - βτ-1)Pε17+ντ-3)θ (W(T) - W(T- I))i.
1 — β1
1 τ=2
Furthermore, we have s∞=1(g(t + 1) - g(t)) is upper bounded.
The first claim of Lemma 7 is similar to that in GDM and SGDM case. HoWever, When We come to
the second claim, simple calculation of g(t + 1) - g(t) leads to
hr(t),(√ε — (1 — βt)Pε1d + ^(t)) Θ (w(t + 1) — w(t))i +〈r(t),-√εlog(t+1)w —	VL(w(t))i.
t	1-β
This is where we need Lemma 6, which bounds the gap between ,ε1d + ^(t) and √ε by O(t-2)
and makes P∞=ιhr(t), (√ε 一 (1 一 βt)pε1d + V(t)) Θ (w(t + 1) 一 w(t))i an absolute continuous
sequence. The second term hr(t), -√ε log(t++1 )W — ɪ-e VL(w(t))i can be tackled by following
the same routine as the GDM/SGDM case. Consequently, the proof of Lemma 7 is completed.
Till now, we have obtained the implicit bias for GDM, SGDM and Adam (w/s), one may wonder
whether the implicit bias of stochastic Adam can be obtained. Here, we make some discussions here
and put its further investigation for future works. First, our analysis can be extended to prove that the
stochastic adaptive heavy-ball algorithm converges to the L2 max margin solution, which is proposed
by (Tao et al., 2021) and has the following form3:
Wt+1)=w(t)-η VS+β(w(t)-w(t-1)),t ≥1,
where ν(t) is defined as Eq. (2).The proof is a simple combination of the proof techniques in SGDM
and Adam by observing that the stochastic adaptive heavy-ball algorithm has the following equivalent
update rule (u(1) = w(1))
u(t + 1) = —
η	^LB(t)(w(t))
1 - β Pε1d + ν(t)
+ u(t),
w(t + 1) = βw(t) + (1 - β)u(t + 1).
However, Adam is different from the stochastic adaptive heavy-ball algorithm as it combine the
conditioner and momentum as a whole, which makes our constructed Lyapunov function can not be
applied. We will put further investigation on stochastic Adam in future work.
3Compared to (Tao et al., 2021), We put the ε1d inside the square-root and use ^(t) instead of ν(t) to
demonstrate the difference between Adaptive Heavy Ball and Adam, while these changes will not influence the
convergent direction.
9
Under review as a conference paper at ICLR 2022
7	Discussions
Extension to the Multi-Class Classification Problem. As mentioned in the introduction, despite all
the previous analyses are aimed at the binary classification problem, they can be naturally extended
to the analyses multi-class classification problem. Specifically, in the linear multi-class classification
problem, for any (x, y) ∈ RdX ×{1, ∙∙∙ ,C} in the sample set S, the (individual) logistic loss with
parameter W ∈ RC ×dX is denoted as
eWy,x
'(y, Wx) = log —c——.
PiC=1 eWi,x
Correspondingly, dataset S is separable if there exists a parameter W, such that ∀(x, y) ∈ S, we
have Wy, x > Wi, x, ∀i 6= y . The multi-class L2 max-margin problem is then defined as
min kWkF , subject to : Wy,x ≥ Wi,x + 1, ∀(x, y) ∈ S, i 6= y,
where k ∙ ∣∣f denotes the Frobenius norm. Denote W as the L2 max-margin solution, We have
(mini-batch) SGDM and Adam (w/s) still converges to the direction of W, the proof of which is
deferred to the Appendix D.2.
Theorem 5. For linear multi-class classification problem using logistic loss and almost every
separable data, with a small enough learning rate, and 1 > β2 > β14 ≥ 0 (for Adam (w/s)),
(mini-batch) SGDM and Adam (w/s) converge to the multi-class L2 max-margin solution (a.s. for
SGDM).
Consistency with the Existing Experimental Results. Our results stand with existing experimental
observations. Specifically, Soudry et al. (2018) conduct experiments using GD and GDM on the
same linear separable data (c.f., Figure 1 in (Soudry et al., 2018)), and it is observed that the training
behaviors of GD and GDM are quite similar in terms of the direction w(t), the training loss, and the
margin, which supports our Theorem 2. Nacson et al. (2019) extend the experiment to the stochastic
setting (c.f. Figure 2 in (Nacson et al., 2019)), and observe the same similarity between SGD and
SGDM, which agrees with our Theorem 3. Wang et al. (2021) conduct the experiments of SGD,
SGDM, Adam (without momentum) and Adam on MNIST using homogeneous neural networks (c.f.
Appendix F.1.2 in (Wang et al., 2021)), and observe such similarity for deep neural networks. Our
theorems apply to linear models, which is a special case of homogeneous neural network, and meet
their observation.
Gap Between The Linear Model and Deep Neural Networks. While our results only hold for
the linear classification problem, extending the results to the deep neural networks is possible.
Specifically, Lyu & Li (2019) construct a Lyapunov function bounding the rate between loss and
parameter norm of GD on the homogeneous deep neural networks, and prove the parameter direction
converges to some KKT point of the L2 max-margin problem, which extends the result of GD
on linear model. Wang et al. (2021) further show the proof techniques in (Lyu & Li, 2019) can
be generalized, and successfully extend the Lyapunov function to AdaGrad, RMSProp and Adam
(without momentum) on deep homogeneous neural networks. It will be interesting to see whether
such a Lyapunov function can be constructed for GDM and Adam in homogeneous neural networks.
8	Conclusion
In this paper, we study the implicit bias of momentum-based optimizers in linear classification with
exponential-tailed loss. Our results indicates that for SGD and Adam, adding momentum will not
influence the implicit bias, and the direction of parameter converges to the L2 max-margin solution.
Our theoretical results stands with existing experimental observations.
References
Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In International Conference on Machine Learning, pp. 233-244. PMLR,
2020.
10
Under review as a conference paper at ICLR 2022
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In International Conference on Learning Representations,
2018.
Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of
the heavy-ball method for convex optimization. In 2015 European control conference (ECC), pp.
310-315.IEEE, 2015.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp.1832-1841.
PMLR, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018b.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. arXiv
preprint arXiv:2006.06657, 2020.
Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In
Algorithmic Learning Theory, pp. 772-804. PMLR, 2021.
Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In
International Conference on Machine Learning, pp. 4860-4869. PMLR, 2021.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
International Conference on Learning Representations, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. The
Journal of Machine Learning Research, 18(1):3375-3421, 2017.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. Advances in Neural Information Processing Systems, 33, 2020.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 3051-3059. PMLR, 2019.
Y Nesterov. A method for solving a convex programming problem with convergence rate o (1/k2). In
Soviet Mathematics. Doklady, volume 27, pp. 367-372, 1983.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. In ICLR, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
11
Under review as a conference paper at ICLR 2022
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computa-
tional mathematics and mathematical physics, 4(5):1-17, 1964.
Qian Qian and Xiaoyuan Qian. The implicit bias of adagrad on separable data. In Advances in Neural
Information Processing Systems, pp. 7761-7769, 2019.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization. In Proceedings
of the 28th International Conference on Neural Information Processing Systems-Volume 1, pp.
1630-1638, 2015.
Stefano Sarao Mannelli and Pierfrancesco Urbani. Analytical study of momentum-based acceleration
methods in paradigmatic high-dimensional non-convex problems. Advances in Neural Information
Processing Systems, 34, 2021.
Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. Kybernetes, 2013.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, Lei Guan, and Hao Jiang. Non-ergodic
convergence analysis of heavy-ball algorithms. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33, pp. 5033-5040, 2019.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139-1147.
PMLR, 2013.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.
In International Conference on Machine Learning, pp. 6105-6114. PMLR, 2019.
Wei Tao, Sheng Long, Gaowei Wu, and Qing Tao. The role of momentum parameters in the optimal
convergence of adaptive polyak’s heavy-ball methods. arXiv preprint arXiv:2102.07314, 2021.
Matus Telgarsky. Margins, shrinkage, and boosting. In International Conference on Machine
Learning, pp. 307-315. PMLR, 2013.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. The implicit bias for adaptive optimization
algorithms on homogeneous neural networks. In International Conference on Machine Learning,
pp. 10849-10858. PMLR, 2021.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in neural information
processing systems, pp. 4148-4158, 2017.
Saining Xie, Ross Girshick, Piotr Doll念 Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492-1500, 2017.
Y Yan, T Yang, Z Li, Q Lin, and Y Yang. A unified analysis of stochastic momentum methods for
deep learning. In IJCAI International Joint Conference on Artificial Intelligence, 2018.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient
momentum sgd for distributed non-convex optimization. In International Conference on Machine
Learning, pp. 7184-7193. PMLR, 2019.
12
Under review as a conference paper at ICLR 2022
Supplementary materials for
“Does momentum Change the Implicit Bias on separable data?”
A Preparations
This section collect definitions and lemmas which will be used throughout the proofs.
A. 1 Characterization of the Max-margin Solution
This section collects several commonly-used characterization of the max-margin solution from
Nacson et al. (2019) and Soudry et al. (2018).
To start with, we define support vectors and support set, which are two common terms in margin
analysis.
Definition 2 (Support vectors and support set). For any i ∈ [N], Xi is called a support vector of the
dataset S, if
hXi, Wi = 1.
Correspondingly, Xi is called a non-support vector if〈Xi, W〉> L The support set of S is then
defined as
Ss = {(xi, yi) : hyixi, Wi = 1}.
The following lemma delivers W as an linear combination of support vectors.
Lemma 8 (Lemma 12, Soudry et al. (2018)). For almost every datasets S, there exists a unique
vector V = (vι, •一,VN), such that W can be represented as
N
W = X ViXi,	(11)
i=1
where V satisfies Vi = 0 if Xi ∈ T(Ss), and Vi > 0 if Xi ∈ T(Ss). Furthermore, the size of Ss is at
most d.
By Lemma 8, we further have the following corollary:
Corollary 2. For almost every datasets S, the unique V given by Lemma 8 further satisfies that for
any positive constant C?, there exists a non-zero vector W, such that, Xi ∈ T(Ss), we have
C2e-hχi,w i = Vi.	(12)
Proof. For almost every datasets S, any subsets with size d of S is linearly independent. Since Ss
has size no larger than d (by Lemma 8), and Eq. (12) is equivalent to linear equations, the proof is
completed.	口
For the stochastic case, we will also need the following lemma when we calculate the form of
parameter at time t.
Lemma 9 (Lemma 5, Nacson et al. (2019)). Let B(s) be the random subset used in SGD (i.e., the
one used in SGDM). Almost surely, there exists a vector W
N X S X	ViXi=log (Nt) w+n⑴,
s = 1 i∙∙Xi∈T (B(s)∩Ss)	∖ J
where n(t) satisfies kn(t)k = o(t-0.5+ε) for any ε > 0, and kn(t + 1) - n(t)k = O(t-1). If the
B(s) is the random subset used in SGDM (i.e., the one used in mini-batch SGDM), then the a.s.
condition can be removed.
13
Under review as a conference paper at ICLR 2022
A.2 Preparations of the optimization analysis
This section collects technical lemmas which will be used in latter proofs. We begin with a lemma
bounding the smooth constants if the loss is bounded.
Lemma 10. If loss ` satisfies (D) in Assumption 3, then for any w0, if L(w) ≤ L(w0), then we have
L is Hs0 smooth at point w, where s0 = `-1 (N L(w0)). Furthermore, L is globally Hs0 smooth
over the set {w : L(w) ≤ L(w0)}.
Proof. Since ` is positive, we have ∀i ∈ [N],
'(w, Zi) / PN=1 '(w, Z)	r, λ.r,、
—N— < —~~N------------= L(W) ≤ L(W0),
which leads to '(w, Zi) < NL(w0), and ' is Hs° smooth at hw, x”.
Furthermore, since Nw'(w, Zi) = Nw'(hw, Xii) = '(hw, Xii)Xi, for any two parameters wι and
W2 close enough to W,
~, ~, ... ................... .,. ...
IlNw '(W1, Zi)-Nw '(W2, Zi)II = k('(hwι, Xi i) - ' (hw2, Xi i))Xik
≤∣'0(hwι, Xii) - '0(hw2, Xii)∣ ≤ Hsolhwi - W2, Xiil ≤ Hso∣wι - W2∣,
and thus,
1N
∣∣NwL(WI)-NwL(w2)k ≤ N 工 IlNw'(wι, Zi)-Nw'(w2, Zi)Il ≤ Hs°∣∣wι - W2k,
which completes the proof that L is locally Hs0 smooth at W.
Now if Wi and W2 both belong to {w : L(w) ≤ L(w0)}, we have for any Xi ∈ T(S),(wi, Xii >
'-1(NL(wo)), and (w2, Xii > '-1(NL(wo)). Following the same routine as the locally smooth
proof, we complete the second argument.
The proof is completed.	□
Based on Assumption 2, we also have the following lemma characterizing the relationship between
loss ` and its derivative `0 when x is large enough.
Lemma 11. Let loss ` satisfy Assumption 2. Then, there exists an large enough x0 and a positive
real K, such that, ∀x > x0, we have
-ɪ'0(x) ≤ '(x) ≤ -K'0(x).
Proof. By Assumption 2, there exists a large enough x0, such that ∀x > x0, we have
2e-x ≤-'0(x) ≤ 2e-x.	(13)
On the other hand, as limt→∞ '(χ) = 0, we have
Z∞
-`0 (s)ds,
=x
which by Eq. (13) leads to
1 e-x
2
1/	e-sds ≤ '(x) ≤ 2/ e-sds = 2e-x
Therefore, setting K = 4 completes the proof.
□
The following lemma bridges the second moment of NLB(t) with its squared first moment.
14
Under review as a conference paper at ICLR 2022
Lemma 12. Let the dataset S satisfies the separable assumption 1. Let B be a random subset of S
with size b sampled independently and uniformly without replacement. Then, at any point w, we have
kVL(w)k2 ≤ EB [∣∣VLb(w)k2] ≤ YNbkVL(w)k2.
Proof. To start with, notice that
kVL(w)k = kEBLB(w)k ≤EBkLB(w)k.
Therefore, the first inequality can be directly obtained by Cauchy-Schwartz’s inequality. To prove the
second inequality, we first calculate the explicit form of VLB (w).
2
kVLB (w)k2 = 1
VE '(w, Z)
z∈B
2
1
b2
E 'l(hw, Xi)X
X∈T (B)
b2	X	'l(hw,Xi)'I(hw,xi)hX,X'i
x,x0∈τ (B)
1
≤ b2
E '(hw, Xi)'0(hw, X0i).
X,X0∈T (B)
Therefore,
EBkVLB(w)k2
≤EB b2
=EB b2
E	'0(hw, Xi)I(WXOi)
x,x0∈τ (B)
X	'(hw, Xi)'l(hw, Xli)1χ,χo∈τ(B)
(14)
x,x0∈τ (S)
=b2	X	'(〈w, X))'1 (〈W, Xii)Eb 1χ,χo∈τ(B)
x,x0∈τ (S)
=N X '(hw,Xi)2+bΝ(N-i) X	'(hw,Xi)'i(〈w,XIi)
X∈T(S)	v	J x,x0∈τ(S),x=x0
≤NIb(X 'I(〈w, Xi)).
∖x∈T (S)	/
(15)
On the other hand,
kVL(w)k = N
E 'l(hw, Xi)X
X∈T (S)
≥K X '(hw,Xi)X,-k⅛)-
∖x∈T (S)	11 11 /
where Eq. (?) is due to ∀z ∈(X, -Wi ≥ 1 and ' < 0.
Therefore,
N X	'ι(hw, Xi)
X∈T (S)
kVL(w)k2 ≥ N J X '(hw, Xi))
∖x∈T (S)	/
(16)
The proof is completed by putting Eqs. (15) and (16) together.
□
15
Under review as a conference paper at ICLR 2022
In the following lemma, we show the updates of GDM, Adam, and SGDM are all non-zero.
Lemma 13. Regardless of GDM, Adam, or SGDM, the updates of all steps are non-zero, i.e.,
kw(t + 1) - w(t)k > 0, ∀t > 1.
Proof. We start with the alternative forms of the update rule of GDM, Adam, and SGDM using the
gradients along the trajectory respectively. For GDM, by Eq. (5), the update rule can be written as
w(t + 1) - w(t)
-η
βt-sVL(w(s))
(17)
Similarly, the update rule of SGDM can be written as
w(t+ 1) - w(t) = -η X βt-sVLB(s)(w(s)) ,
while the update rule of Adam can be given as
…、小	PS=ι1-βs βt-svL(w(s))
w(t + 1) — w(t) = —η —	1
≠1d + PS=ι⅛ βt-s(VL(w(s)))2
(18)
(19)
On the other hand, by the definition of empirical risk L, the gradient of L at point w can be given as
VL(W)= PNI '0(S，Xii)Xi.	(20)
By Eq. (20) and Eq. (17), we further have for GDM,
w(t + 1) - w(t) = -η (Eβt-sPI '0((w(s)，Xii)Xi! .	(21)
By Assumption 1, there exists a non-zero parameter W, such that,hW, Xii > 0, ∀i. Therefore, by
executing inner product between Eq. (21) and W, we have
kw(t+1)-w(t)kkWk ≥ hw(t+1)-w(t), Wi = -η (XXβt-s EN ’0(hw(N, Xii)hXi, Wi! >) 0,
where Eq. (*) is due to ' < 0. This complete the proof for GDM.
Similarly, for SGDM, we have
kW(t + 1)-W(t)kkW k≥-η (X βt-s P(χ,y)‘B "? yXi)hyX,W i! > 0,
which completes the proof of SGDM.
For Adam, we have
∣∣W(t + 1) - W(t)k W Θ
ε1d + XX Tβ2-s(VL(W(s)))2
s=1 1 - β2
≥ - (W Θ t
1 IX 1-β2 印』▽片 mw	PS=1⅛βt-sVL(W(S))	+
ε1d + 工 S-不β2 (VL(W(S)))2,η /	)
W 1 - β2	≠1d + PS=ι⅛β2-s(VL(W(s)))2 /
(W,ηXX Tβt-sVL(W(S))
tS 1 - β ∕θt-s
-ηmI%
PNI '0((W(S), Xii)hXi, Wi
> 0,
N
which completes the proof of Adam.
The proof is completed.
□
16
Under review as a conference paper at ICLR 2022
B Implicit Bias of GD/S GD with momentum
This section collects the proof of the implicit bias of gradient descent with momentum and stochastic
gradient descent with momentum.
B.1	Implicit Bias of GD with Momentum
This section collects the proof of Theorem 2.
B.1.1	Proof of the sum of squared gradients converges
To begin with, we will prove the sum of squared norm of gradients along the trajectory is finite for
gradient descent with momentum. To see this, we first define the continuous-time update rule as
w(t + α) - w(t) = α(w(t + 1) - w(t)), ∀t ∈ Z+, ∀α ∈ [0, 1].
We then prove a generalized case of Lemma 1 for any w(t + α).
Lemma 14 (Lemma 1, extended). Let all conditions in Theorem 2 hold. We then have
L(w(t)) + β~llw(t) - w(t - 1)k2 ≥L(w(t + α)) + 7-α2kw(t + I)- w(t)k2
2η	2η
+ (I- β)(1- C1)α2IHt + 1) - w(t)k2,
η
where Ci is a positive real such that η = 2 1—β C3
Hs0
ProofofLemma 14. For brevity, We denote so 4 '-1(NL(wι)). We prove this lemma by reduction
to absurdity.
Concretely, let t* be the smallest positive integer time such that there exists an α ∈ [0,1], such that
Eq. (7) doesn,t hold. Let α* = inf {α ∈ [0,1] : Eq. (7) doesn't hold for (t*, α)}. By continuity,
Eq. (7) holds for (t*, α*).
We further divide the proof into two cases depending on the value of α*.
Case 1:	α* = 0: For any t* > t ≥ 1, we have Eq. (7) holds for (t, 1). Specifically, we have
L(w(t)) + β-kw(t)- w(t - 1)k2 ≥ L(w(t +1)) + β~kw(t +1)- w(t)k2,
2η	2η
which further leads to
L(w(1)) = L(w(1)) + β-∣w(1) - w(0)∣2 ≥ L(w(t*)) + ɪ∣∣w(t*) - w(t* - 1)∣2.
2η	2η
Since 4∣∣w(t*) 一 w(t* 一 1)∣2 is non-negative, we have
L(W⑴)≥L(w(t*)).
By Lemma 10, we have L is Hs0 smooth at w(t*). Therefore, by Taylor’s expansion for L at point
w(t*), we have for small enough α > 0
L(w(t* + α))
≤L(w(t*)) + hVL(w(t*)), w(t* + α) — w(t*)i + Hs0 ∣∣w(t* + α) — w(t* )∣2
=L(w(t*)) + α(VL(w(t*)), w(t* + 1) — w(t*)i + Ha l∣w(t* + 1) — w(t*)k2
=)L(w(t*)) + α (1(β(w(t*) — w(t* — 1)) — (w(t* + 1) — w(t*))), w(t* + 1) — w(t*),
+ Hs2α2 kw(t* + 1) — w(t*)k2
17
Under review as a conference paper at ICLR 2022
=L(W"+αηβ h(Wr)- w(t* -I), w(t*+I)-…+( Hs2α2 - η)kw(t*+I)- w(t*)『
≤)L(w(t*)) + αβ k(w(t*) - W(t* - 1)k2 + a k(w(t* + 1) - w(t*)k2
2η	2η
+( Hs2α2 - η)∣Ht*+i)- w(t*)∣∣2
=L(w(t*)) + αβk(w(t*) - w(t* - 1)k2 + (α - α + Hsoα2) k(w(t* + 1) - w(t*)k2
2η	2η	η 2
=L(w(t*)) + ɪk(w(t*) - w(t* - 1)k2 - (1-α)βk(w(t*) - w(t* - 1)k2
2η	2η
+( αβ -α+Hs0α2)k(w(t*+i)- w(t*)k2
2η	η 2
()	β	β
≤L(WD) + 2η∣Ht*) - w(t* - 1)k2 - Va2IHt* + 1) - w(t*)k2
--C1- " + 1)-w(t*)k2,
(22)
where Eq. (*) is due to a simple rearrangement of the update rule of gradient descent with momentum
(Eq. (5)), i.e.,
VL(w(t)) = ɪ(e(w(t) — w(t — 1)) — (w(t + 1) — w(t))),∀t ≥ 1,	(23)
η
Inequality (**) is due to Cauchy Schwarz,s inequality and arithmetic-geometric average inequality,
and Inequality () is due to
(1 — a)β
-2η-
(1 -α)β
2η
≤O(α2)
—
—
")-w(t*- 1)k2+(t- η+Ha)"+1)- w(t*)k2
k(W(t*) - W(t* - 1)k2 + O(α)
2kw(t* + 1) - w(t*)k2 - (1 - β)(1 - C" ∣∣w(t* + 1) - w(t*)k2.
--βα'
2η
η
Here the inequality is due to that -(1-；)e k(w(t*) - w(t* - 1)k2 tend to - M ∣∣(w(t*) - w(t* - 1)k2
as α tend to zero, which is a negative constant by Lemma 13.
Eq. (22) indicates Eq. (7) holds at (t*,α) forα > 0 is small enough, which contradicts to α* = 0.
Case 2:	α* 6= 0: Same as Case 1, we have for any 1 ≤ t < t*,
L(Mt)) + 2ηIHt)- Mt- 1)k2 ≥ L(Mt + 1)) + 2ηkw(t + 1) - w(t)k2,
which further leads to
L(w(1)) ≥ L(w(t*)) + 2ηkw(t*) - w(t* - 1)k2.	(24)
On the other hand, by the definition ofα*, we have for any 0 ≤ α < α*, we have Eq. (7) holds for
(t*, α), which by continuity further leads to Eq. (7) holds for (t*,α*). Therefore, α* < 1, otherwise,
Eq. (7) holds for (t*, α), ∀α ∈ [0, 1] which contradicts the definition oft*.
Combining Eq. (7) with (t*,α) and Eq. (24), we further have
L(W(I)) ≥ L(w(t* + a)) + 2βηa2kw(t + 1) - w(t)k2 + ⅛C1 HS0α2kw(t +1) - w(t)k2,
18
Under review as a conference paper at ICLR 2022
Consequently, for any α ∈ [0,a*]
L(W⑴)≥L(w(t* + α)),
and by Lemma 10, We then have L is Hs° smooth at w(t* + α), which further by Taylor,s expansion
leads to
L(w(t* + α*))
≤L(w(t*)) + (VL(w(t*)), w(t* + α*) - w(t*)i + H0∣∣w(t* + α*) - w(t*)∣∣2
2
(◦)	α* β	a* β
≤L(w(t*)) + Ve∣∣w(t*) - w(t* - 1)∣2 + -2ηβ ∣∣w(t* + 1) - w(t*)∣2
+ (HMo- η )kw(t* + 1) - w(t*)k2
=L(w(t*)) + αβ kw(t*) - w(t* - 1)k2 + (HMOf - α*(2-e)) ∣w(t* + 1) - w(t*)k2
2η	2	2η
= L(w(t*)) + Oβ∣w(t*) - w(t* - 1)∣2 + (上^C1- - U-e)) MP + 1) - w(t*)k2
2η	η	2η
<)L(w(t*)) + ɪ∣∣w(t*) - w(t* - 1)∣2 -亨β∣w(t* + 1) - w(t*)k2
2η	2η
-(I-β)(I- C1①T kw(t* + 1)-w(t*)∣2
η
where Eq. (◦) follows the same routine as Case 1, Eq. (•) is due to the definition ofη and C1, and
Eq. (*) is due to a* < 1, and ∣∣w(t* + 1) - w(t*)∣2 > 0 (given by Lemma 13).
By the continuity of L, for any small enough δ > 0, Eq. (7) holds for (t*, α* + δ), which contradicts
to the definition of a*.
The proof is completed.	□
By Lemma 1, one can easily obtain the sum of the squared norms of the updates across the trajectory
converges.
Corollary 3. Let all conditions in Theorem 2 hold. We have
∞
X∣w(t+ 1) - w(t)∣2 < ∞.	(25)
t=1
Consequentially, we have
∣∣w(t)k= O( √t).
Proof. By Lemma 1, we have
L(W(t)) + ：IHt)- w(t - 1)k2
-(L(W (t + I)) + 2η kw(t + 1) - W(t)k2
≥ 上 C≡-∙^kw(t + 1) - w(t)k2,
η
which by summing over t further leads to
L(w(1)) ≥L(w(1))-(L(w(t +1)) + 2η
kw(t +1) - w(t)k1 ≥ T C1≡-^
t
X ∣w(s+1)-w(s)∣2.
s=1
η
Taking t → ∞ leads to
∞
X ∣W(s+ 1) - W(s)∣2 < ∞.
s=1
19
Under review as a conference paper at ICLR 2022
By triangle inequality, we further have
t
kw(t)k ≤Xkw(s+1) - w(s)k + kw(1)k
s=1
t
(?)
≤t
kw(s + 1) - w(s)k2
+ kw(1)k = O( √t),
where Eq. (?) is due to Cauchy-Schwartz’s inequality.
The proof is completed.
□
By the negative derivative of the loss and the separable data, we can finally prove Corollary 1.
Proof of Corollary 1. By Eq. (21), we have
kw(t + 1)- w(t)k2=η2 XXβt-s P3'"n⑶,Xii)Xi
≥)η2γ2 *w,XXβ- P=。Hw')，Xii)Xi +2
(**) 2
≥ η2γ
t-s P屋 40(hw(s), Xii)
N
≥η2γ2
Pi= 40(hw(t), Xii) !2
2
≥)η2γ2 (∑i=1 '0(hw(t), Xii)kXik
≥η2γ2 ∣Pi=1 '0(hw(t), Xii)Xi 2
=η2γ2 kVL(w(t))k2,
(26)
where Inequality (*) is due to Cauchy-Schwartz's inequality, Inequality (**) is due to [0(s) < 0,
∀s ∈ R and(W, Xii ≥ γ, ∀i ∈ [N], and Inequality (•) is due to ∣∣Xik ≤ 1. By combining Eq. (25)
and Eq. (26), We complete the proof.	口
By the exponential-tailed assumption of the loss (Assumption 2), we further have the following
corollary.
Corollary 4. Let all conditions in Theorem 2 hold. Then, limt→∞ ∣VL(w(t))∣ = 0, and
lim (w(t), Xii = ∞, ∀i.
t→∞
Consequently, there exists an large enough time to, SUCh that, ∀t > to, ∀i, we have (w(t), Xii > 0,
and
-40((w(t), Xii) ≤ (1 + e-μ+hw㈤，xii)e-hw⑴,xii,
-40((w(t), Xii) ≥ (1 — e-μ-hw㈤,χii)e-hw⑴,xii.
20
Under review as a conference paper at ICLR 2022
B.1.2 B ounding the orthogonal part
To prove Theorem 2, We only need to show w(t) - log(t)W (t ≥ 1) has bounded norm for any
iteration t > 0. Letting C2 =(二川 in Corollary 2, We obtain an constant vector W satisfying Eq.
(12). Define
r(t) 4 w(t) — log(t)W — W.	(27)
As W is a constant vector, that w(t) - log(t)W (t ≥ 1) has bounded norm is equivalent to r(t) has
bounded norm. Lemma 2 then propose an equivalent proposition of kr(t)k is bounded, and further
prove this proposition is fulfilled. As the proof is rather complex, We separate it into tWo sub-lemmas.
We first prove kr(t)k is bounded if and only if function g(t) is upper bounded.
Lemma 15 (First argument in Lemma 2). Let all conditions in Theorem 2 hold. Then, kr(t)k is
bounded if and only if function g(t) is upper bounded.
Proof. We start the proof by shoWing that A1(t) =4 Ptτ=2 hr(τ) - r(τ - 1), W(τ) - W(τ - 1)i has
bounded absolute value.
By the definition of r(t) , We have
r(t) - r(t - 1) =w(t) - w(t - 1) - log (—~~[) W，
Which further indicates
Aι(t) = X (w(τ) - W(T - 1) - log (T-I) W, W(T) - W(T - 1)
Therefore, the absolute value of A1(t) can be bounded as
t
∣Aι(t)| = X (W(T) - w(t - 1) - log (τ⅛! ) W，w(t) — w(t - 1)
τ=2
≤ X KW(T) - w(t - 1) - log T-~Σ~[) W, w(t) - w(t - 1)
t
≤	kW(T) -W(T-1)k2+
t
τ=2
t
τ=2
t
W, w(t ) — w(t — 1)
≤	kW(T) - W(T - 1)k2 + log
τ=2
(?) 3 t
≤ 2 EkW(T) — W(T - 1)k
τ=2
(◦)
< ∞,
τ=2
2+
t
T
t — 1
I χiog
τ=2
W ∣∣w(t ) — w(t — 1)k
T⅛ W
2
Where Inequality (?) is due to the Inequality of arithmetic and geometric means, and Inequality (◦) is
due to Corollary 3 and log T-I = O(ɪ).
Therefore, g(t) is upper bounded is then equivalent to 11 ∣∣r(t)∣2 + ɪ-β〈r(t), w(t) - w(t - 1)i is
upper bounded. Now if 11 ∣∣r(t)k2 + I-ehr(t), w(t) - w(t - 1)i is upper bounded, we will prove
∣r(t)∣ is bounded by reduction to absurdity.
Suppose that ∣r(t)∣ has unbounded norm. By Corollary 3, we have limt→∞ ∣W(t) - W(t - 1)∣ = 0,
and there exists a large enough time T, such that ∣W(t) - W(t - 1)∣ < 1 for any t ≥ T. On the
other hand, since r(t) is unbounded from above, there exists an increasing time sequence ki > T,
i ∈ Z+, such that
lim ∣r(ki)∣ = ∞.
i→∞
21
Under review as a conference paper at ICLR 2022
Therefore, we have
1β
Qm xkr(ki)k2 + Tl~~晨r(ki), w(ki) - w(k - 1)i
i→∞ 2	1 - β
≥ Iim 2kr(ki)k2 - 1~β^ kr(ki)k kw(ki) - w(ki - I)k
i→∞ 2	1 - β
≥ Iim 1 kr(ki)k2 - 1~β^ llr(ki)k = ∞,
i→∞ 2	1 - β
which leads to contradictory, and completes the proof of necessity.
On the other hand, if kr(t)k is upper bounded, since kw(t) - w(t - 1)k is also upper bounded,
We have 1 ∣∣r(t)k2 + ι-ββ (r(t), w(t) — w(t - 1)i is upper bounded, which completes the proof of
sufficiency.
The proof is completed.	□
Therefore, the last piece of this puzzle is to prove g(t) is upper bounded ∀t > 0.
Lemma 16 (Second argument in Lemma 2). Let all conditions in Theorem 2 hold. Then, we have
g(t) is upper bounded.
Proof. We start the proof by calculating g(t + 1) - g(t). For any t ≥ 2, we have
g(t +1) — g⑴=Ikr(t +1) — r ⑴ k2+ hr ⑴，r(t +1) — r⑴〉+ √β-Σ hr(t +1), w(t +1) — W⑴〉
2	1-β
一√β-∑ hr(t), WW — w(t — 1)i — √β-∑ hr(t + 1) — r(t), w(t +1) — WWi
1-β	1-β
=Ikr(t +1) — r(t)k2+ hr⑴，r(t + 1) — r⑴〉+ √β-Σ hr⑴，w(t + 1)+ w(t — 1) — 2w(t)i.
2	1-β
On the other hand, by simply rearranging the update rule Eq. (5), we have
Ye^ (w(t + 1) + w(t — 1) — 2w(t)) = - -ɪ- VL(w(t)) — (w(t +1) — w(t)),	(28)
1-β	1-β
which further indicates
g(t + 1) - g(t)
=1 kr(t + 1) — r(t)k2 + hr⑴，r(t + 1) — r⑴〉+ (r⑴，—Yn^VL(W⑴) — (W(t + 1) — W⑴)
2	1-β
=2 kr(t+1)—r(t)k2+(r⑴，一ιog (t+1)W—ɪ-jVL(W⑴)).
Denote A2(t) = kr(t + 1) — r(t)k2, and A3(t) =(r(t),—log( t++1) W —	VL(W(t))). We
then prove respectively Pt∞=1A2(t) and Pt∞=1 A3 (t) are upper bounded.
First of all, by definition of r(t) Eq.(27), we have
X A2⑴=X (kW(t +1) ― W⑴k2 +log (t+1) kWk2 ― 2log (t+1) hW(t +1) ― W⑴,Wi
≤2 X (kW(t+1) -W⑴『+log (t+1
(•)
< ∞,
(29)
where Eq. (•) is due to Lemma 3 and log (t++1) = O( 1).
Then we only need to prove Pt∞=1 A3(t) < ∞.
22
Under review as a conference paper at ICLR 2022
To begin with, by adding one additional term 1 w into A3, we have
A3⑴=(r⑴，1 w - log (t+1) w)+ (r⑴，-1 w - i-βVL(w(t))).
On the one hand, by Corollary 3, ∣∣ w(t) ∣∣ = O(√t), which further leads to
∣∣r(t)∣∣ = ∣w(t)∣ +log(t)∣W ∣ + IIw ∣ = O(√t)
By 1 - log 号=O (表)，we have
Gt), tw - iog (t++τ )w)=。(t⅛) ∙
(30)
On the other hand, by direct calculation of the gradient, we have
(r(t),-；w - T-βVL(w(t)),
=(r(t), -1 w - (1 -ηβ)N X /(〈w(t), Xii)Xi)
=N *r(t)，-1 T-β	X	eTw'f- T-β X ` (〈w，Xii)Xi)
Xi∈T(Ss)	X i=1	/
=N (r(t)，-占出X Ge-MXii + 3, Xii))Xi) - N E 占
where Eq. (*) is due to the definition of w (Eq. (12) with C2 = ι-β).
2(〈w(t), X，)X
Xi∈T (SS)
,
Denote
A4(t)
-〈r(t), I-β	X ”(〈w(t), Xii)Xi),
∖	一 P Xi∈T(SS)	/
and
A5(t) = ∕r(t),-占	X	GeTwXii + /(hw(t), Xii))Xi).
∖	' Xi∈T(SS) '	/	/
We then analysis these two terms respectively. As for A4(t), due to ' < 0, we have
A4⑴ < -ɪɪ (r⑴，	X	'(〈w⑴，Xii)Xi).
∖	Xi/T (Ss),hr(t),Xii>0	/
By Corollary 4, we further have ∀t > t0
-/(hw(t), Xii) < (1 + e-"+hw⑴，Xii)e-〈w(t)，Xii < 2e-hw⑴，Xii,
which further indicates
A4(t) < - ι-β	X	/(〈w(t), XjNr(O, Xii
Xi/T (Ss),hr(t),Xii>0
< -ɪ
一1-β
η
1-β
E	2e-hw㈤,χii (r(t), Xii
Xi ∈T (Ss),hr(t),Xii>0
X	2e-〈r(t)+log tw+w,Xii (r(t), Xii
Xi∈T (Ss),hr(t),Xii>0
<(max e∙wX
E	2e-hr㈤+log tw，Xii <r(t),竭
Xi/T (Ss),hr(t),Xii>0
(◦)
≤
η (maxi eh-w,xii
-1 - β
tθ
E	2e-hr⑴,xii hr(t), Xii
Xi ∈T (Ss),hr(t),Xii>0
(◊)
≤
η (maxi eh-w,Xii
-1 - β
tθ
2N,
23
Under review as a conference paper at ICLR 2022
where θ in Eq. (o) is defined as
θ = min (xi, Wi > 1.
xi∈T (SS)
(31)
As P∞=ι + < ∞, we have
∞
X A4(t) < ∞4 *.
t=1
(32)
For each term <r(t), — ILe (ɪe-hw,xii + 20((w, Wii))历i,(历i ∈ T(SS)) in A5(t), we divide the
analysis into two parts depending on the sign of (r(t), Xi).
Case 1: (r(t), Wii ≥ 0∙ By Corollary 4, we have
r(t),—占(te-hw,xii + 40((w, Wii))Wi
—
η
1-β
1 e
< -ɪ
< 1 — β
(=)η
=T-β
t
1
——e
—1 e
「Mxii + 40(hw, Xii)	hr(t), Xii
-hw,xii + (1 + e-μ+hw(0xii)e-〈w⑴,xii	(r(t), Xii
-hw,xii + (1 + e-*+〈r(t)+logtw+w,xii)e-hr⑴+logtw+wxii ) (r(t) Xii
where Eq. (◊) is due to the definition of r(t) (Eq. (27).
Since (r(t), Xii ≥ 0, we further have
r(t),—占(te-hw,xii + 40((w, ⅛ii∕j ⅛i
< -ɪ
^ 1 — β
(=)η
=T-β
一η
—1 e
—1 e
-hw,xii + (1 + e-μ+hlog tw+wxii)e-(r(t)+log tw+wxii ) (r(t) Xii
-hwxi> + 1(1 + t-μ+ e-"+hwxii)e-Et)+wxii) (r(t), Wii
1 — β t
1 e-hw,xii
—1 + (1+ t-μ+ e-"+hw,xi> )e-hr⑴,xii) hr(t), Xii ,
where Eq. (□) is due to(W, Xii = 1, NXi ∈ T(Ss).
Specifically,
—1 + (1 + t-μ+ e-μ+hw,xii)e-hr⑴,xii
=—1 + e-h"t) ,xi + t-μ+ e-μ+ hw,xiie-hr(D,xii
≤t-μ+ e-μ+ hw,xiie-hr(t),xii
Therefore,
η
1
1 — β Ie
/ η 1
-1 — β ~te
一〈wxi)(—1 + (1+ t-μ+ e-μ+hwxii )e-hr(力xii) (r(t),兔i)
Twxii 卜一μ+ e-μ+ hw,χi*Tr(t),χi)) hr(t), Xii
η
1
≤ (1 — β)et1+μ+
e-(1+μ+)hw,xii = O
1
t1+μ+
4In this paper, for a real series {ri}∞1, we use P∞1 ri < ∞ representing PTI ri is uniformly upper
bounded for any T.
24
Under review as a conference paper at ICLR 2022
Case 2: (r(t), xi) < 0. Similar to Case 1., in this case we have
《⑴，—占	如常十 W 端))当
≤Lj (_；eT血诙〉+ 0	"M⑴晶)eTM')怎](r(t),⅛4
__1_ /_lg一〈疝,凉G I
-+
= η le-<∞.⅜) f-1 + (1 .e-μ-Ht)+logt∞+∞,⅛.Λ e-(r(t),⅛A (r(t),xi')
ɪ _ e-μ-<r(t)⅛log tw+w,xi'Λ ɛ-<r(t)⅛log	\	而步
Specifically, if (r(t).xi) ≥ -t-0^5μ-,
η 1
1 — β te
η 1
1 — β te
77
-(w,χi)(_] + (i — £_*—什⑶+kjg右④+疝怎))g—①⑴岛)〈？⑴)送)
T证岳)+ (1 _ t-μ- e-μ-(r(f)+Λ,i^y-(r(f),ii^	⑴，诵》
1
<
e-(w,xi') lɪ _p ∩ _ ^--μ- e-μ-<r(t)+w,⅛in e~(r(t'),xi)

where Eq. (f) is due to if (r(t∖xi) ≥ -t-0∙5μ-,
Jiln ∣-1+ (1 _ 广e-(τ■⑴+血薮)eTγ■⑴,≤⅛>∣ = 0.
If -2 ≤ (r(t), xi) < -t-0-5μ-, we have
r! ɪ -(⅛,⅛4
l-∕3t
j7 ɪ -(w,⅞4
\— βt
n IC一(疝亚 a)
-1 + ɑ - e-μ-(r(t)+logtw+w^^ ɛ-(r(t)^ɔ (r(t∖xi)
-1 +
-1 +
-μ-(r(t)+w,xi) ∖ e(r(t),xi) \	历分〉
1 -机 £—*―〈疝生〉)eT"t)加i))⑴⑴，Xi).
Therefore, when t is large enough, 1 - ^^e~μ~^w,x^ > 0, WhiChbye一①⑶陋)≥ 1 - (r(t), xi)
leads to
(τ 十(1 - g±e—eTγ∙⑴,a) (r(f),xi)
≤占;eT加端(T +(1-	Ma)(ITT⑴，竭))⑴⑴，竭
≤占;U加弱(—1十(1 一味葭“(血诙)(1+⅛∑))⑴⑴，竭
=	(-Δ- + o( (r(t),iɔ < 0.
1 — β t	\卅5“_ \科小一 J J ' …"
If-2 > (r(t), xi),
rI 1 -(w,⅜)
l-βt
^rl 1 -(w,⅜)
l-βt
-1+ (1 - e-μ-(r(t)+logtw+w,⅛ɔ e-(r(t),⅛ɔ (r(t),Xi')
-1+ 0_e-"-标㈤M>)eTγ>⑴,端)(T⑴荐>
25
Under review as a conference paper at ICLR 2022
η	1 e-hw ,xii
1-βt
For large enough t, 1 一 e-μ- hw(t),xii > 1, and
-1+(1 - e-μ- hw⑴,χii) e-hr㈤,xii) hr(t), Xii
≤1 e-hw,χii (一1+(1 - e-μ- hw⑴,xii) e2) (了⑴，Xii
≤占te-hw,xii (—1 + e2) hr(t), Xii < 0.
Therefore, in Case 2., for large enough t, we have
Gt―占 Qe-" + '0(hw, Xii)) χ) ≤ O (tι+15μ-).
Combining Case 1. and Case 2., we conclude that
A5(t) ≤ O (t1+0.5μ+ ),
which further yields
∞
X A5(t) < ∞.	(33)
t=1
Combining Eq. (32) and Eq. (33), we conclude that Pt∞=1 A3(t) < ∞, which together with Eq. (29)
yields P∞=2 g(t + 1) 一 g(t) < ∞, and completes the proof.	口
We are now ready to prove Theorem 2.
Proof of Theorem 2. By Lemma 16, we have g(t) is upper bounded. Therefore, by Lemma 2, we
have ∣∣r(t)k is bounded, which further indicates ∣∣w(t) 一 log(t)W∣∣ is bounded.
Therefore, the direction of w(t) can be calculated as
w(t)	log(t)W	w(t) — log(t)W	log(t)W	w(t) — log(t)W
IHt)k	l∣w(t)k + IHt)k	Il log⑴w + W⑴一 log⑴W Il +	IHt)k
W	w(t) — log(t)W W (	、
-∣∣1^+Mt)-Og⑴WI 十1心⑴口 → iw Ii aast ∞ ∞.
The proof is completed.
□
B.2 Implicit Bias of SGD with Momentum
This section collects the proof of Theorem 3. Following the same framework as Appendix B.1, we
will first prove that the sum of the squared gradient norms along the trajectory is finite. One may
expect L(w(t)) + M ∣∣w(t) 一 w(t 一 1)∣∣2 is a Lyapunov function of SGDM. However, due to the
randomness of the update rule of SGDM, L(w(t)) + ∣∣w(t) - w(t - 1)∣∣2 may no longer decrease
(we will show this in the end of Appendix B.2, please see Appendix B.2.3 for explanation).
B.2.1	Proof of the sum of gradients along the trajectory is finite
We first provide a proof of Lemma 3.
26
Under review as a conference paper at ICLR 2022
ProofofLemma 3. We denote the parameter of the t-th step in Eq. (9) as W, while that in Eq. (1) as
w(t). With W(I) = W(I), we will prove W(t) = w(t), ∀t > 1 iteratively. Specifically, suppose for
any 1 ≤ k ≤ t w(k) = W(k). Then, we have
u(t + 1) = -NLB (t)(W(t) + u(t) = -NLB(t)(w(t)) + u(t)
=-rjVLB(t)(w(t)) - nVLB(t-i)(w(t - 1)) + u(t - 1)
tt
=…=-XηVLB(k)(w(k)) + u(1) = - XnVLB(k)(w(k)) + w(1).
k=1	k=1
Therefore,
W(t +1) = βW(t) + (1 - β)u(t + 1)= βW(t) + (1- β) (― XnVLB(k)(W(k)) + W(1)),
which by iteration further indicates
W(t+1) = W(1)-n (X (1 - βt+1-k )VLB(k)(W(k)j = w(1)-n (X (1 - βt+1-k )VLB(k)(w(k))
k=1	k=1
w(t) = w(1) 一 η
1 - βt-k)VLB(k)(W(k))	,
and
w(t - 1) = w(1) - η (X(1 - βt-1-k)VLB(k)(w(k)).
On the other hand, by Eq. (1), we have
W(t+ 1)
W(t) - ηVLB(t)(W(t)) + β(W(t) -W(t - 1))
w(1) 一 η
1 - βt-k)VLB(k)(w(k)) - ηVLB(t)(w(t)) + βη
βt-1-k - βt-k)VLB(k)(W(k))
w(1) 一 η
1 - βt+1-k)VLB(k)(w(k))	= W(t + 1).
The proof is completed.
□
Using the alternative form given by Lemma 3, we then prove Lemma 4, which indicates L(u(t)) is a
proper choice of Lyapunov function.
Proof of Lemma 4. We start the proof by applying the Taylor’s expansion of L at the point u(t) to
the point u(t + 1). Concretely, by Assumption 3. (S), we have
L(u(t + 1)) ≤ L(u(t)) + hu(t + 1) - u(t), VL(u(t))i + HH∣∣u(t + 1) - u(t)k2,
which by Eq. (9) leads to
Hn2
L(u(t + 1)) ≤ L(u(t)) - ηhVLB(t)(w(t)), VL(u(t))i + W||VLB(t)(w(t))k2.	(34)
27
Under review as a conference paper at ICLR 2022
Taking the expectation of Eq. (34) with respect to w(t + 1) conditioning on {w(s)}S=ι, we have
Ew-i)[L(u(t + 1))∣{w(s)}S=ι]
=⅛B(t"(t + 1))∣{w(s)}S=ι]
Hn2..	.....C 一 一
≤EB(t) L(U(U)- ηhVLB(t) (w(t)), VL(U(U)) +	2- IlVLB(t)(W⑴州{w(S)}s：
= L(u(t)) - η"L(w(t)), VL(u(t))) + Hη2EB(t) [∣∣VLB(t)(w(t))∣∣2]
(•)	Hn2N
≤L(u(t)) - η"L(w(t)), VL(u(t))) + ʒnɪIIVL(W(t))I2,
2bγ 2
=1
(35)
where Eq. (*) is due to that w(t + 1) is uniquely determined by B(t) given {w(s)}S=> Eq. (o) is
due to u(t) is uniquely determined by {w(s)}S=ι, and Inequality. (•) is due to Lemma 12.
Therefore, we have
Ew-i)[L(u(t + 1))∣{w(s)}S=ι]
Hn2N	C
≤L(u(t)) - η{VL(w(t)), VL(u(t))) + ɪ-IIVL(W(t))∣∣2
Hn2N	C
=m,VL(W⑴))+a VL(W⑴)-Ni) + HYrkVL(W⑴)k2
=L(u(t)) - η 1 -
≤L(u(t))-η ι-
=L(u(t))- η 1
HηN
2bγ2
HηN
2bγ2
(2+
∣∣VL(w(t))∣∣2 + (ηVL(w(t)), VL(w(t)) - S)
IIVL(W(t))k2 + 2 ∣∣ηVL(w(t))k2 + 2 IIVL(W(t)) - VL(u(t))k2
b—) η) kVL(w(t))k2 + 1 IlVL(W(t)) - VL(u(t))k2.
By Assumption 3. (S), ' is H-smooth, which further leads to
2
IIVL(W(t)) -VL(u(t))∣∣2 = X ('0(hX, w(t)))-队&, u(t)))) X
〜
x∈s
≤ (X ∣'0(hX, w(t))) - 40(hX,u(t)))∣l	≤ (H X Kx, w(t))-能 u(t))∣l
∖x∈S	X ∖ x∈S	)
(□) H2 N2 β2
≤H2N2∣∣w(t)- u(t)∣∣2 ≤ 百Tr∣∣w(t)- w(t - 1)∣∣2
(1 - β)
HN^ Xηβt-1-sVLB(s)(W(S))	= H2N2η2β2
2
t-1
X βt-1-s VLB(S)(W(S))
s=1
(≤)「(Xβt-1-s IIVLB(S)(W(S))Il)
自‘IX"-s WLb(s)(w(s))D(xβt-1-s)
≤：：N?；2 (Xβt-1-s IIvlB(S)(W(S))II2),
(36)
where Inequality (□) is due to β(w(t) - w(t - 1)) = (1 - β)(u(t) - w(t)) by Eq. (9), Inequality
(◊) is due to triangular inequality, and Inequality (a)is due to Cauchy-Schwartz Inequality.
28
Under review as a conference paper at ICLR 2022
Combining Eqs. (35) and (36), we have
Ew(t+1)[L(u(t + 1))|{w(s)}ts=1]
≤L(u(t)) -η (1 - (1 + Hγ2})kvL(w(t))k2 + 2HN-ηββ (X*-s IivLB(S)(W(S))II2
which by taking expectation to {w(s)}ts=1 leads to
EFt+1 [L(u(t + 1))] =E{w(s)}ts+=11[L(u(t+1))]
≤EFtL(u(t)) -EFtη (1 - (1 + 2bγ2) η) kvL(w(t))k2+EFt|；2：?；2 (Xβt-1-s ||vLB(s)(w(s))||2)
=EFt L(u(t)) - η (1 - (1 + HN) η) EFtkVL(w(t))k2 + 2 H；：?：2 (X β t-1-sEFs + ι IIvLB(S)(W(S))II2
By Lemma 12 and η <
丐，2H2N3β2
}, we further have
EFt+1[L(u(t+1))] = E{w(S)}ts+=11 [L(u(t + 1))]
≤EFtL(u(t)) - η(1 - (1 +
η EFtkVL(w(t))k2
+ 2 HN-C (X CEFskVL(W(S))k2
≤EFtL(u(t)) - IEFtkVL(w(t))k2 + Lη (Xβ'T-sEFs kvL(w(s))k2
(37)
Summing Eq. (37) with t from 1 to T leads to
E[L(u(T + 1))] = EFT+1 [L(u(T + 1))]
Tη ≤L(u(1)) - X 2EFtkVL(W(t))k2 t=1 Tη =L(u(1)) - X 2EFtkVL(W(t))k2 t=1 Tη ≤L(u(1)) - X 2EFtkVL(W(t))k2 t=1 Tη ≤L(u(1)) - X 4EFtkVL(W(t))k2 t=1 Tη =L(u(1)) - X 4EkVL(W(t))k2.	+ X 一η (XβT-sEFs kVL(W(s))k2) T-1	T + X 中η X βt-1-sEFs kVL(W(s))k2 S=1	t=S+1 T-1 1 + X 4η (EFskVL(W(S))k2) S=1
t=1 The proof is completed.	□
As L(u(1)) is upper bounded, we have the following corollary given by Lemma 4.
Corollary 5. Let all conditions in Theorem 3 hold. Then, we have
∞
XEkvL(W(t))k2 < ∞.	(38)
=1
29
Under review as a conference paper at ICLR 2022
Consequently,
∞
X kVL(w(t))k2 < ∞
t=1
and
.. . 〜
hw(t), Xi → ∞, ∀x ∈ S
hold almost surely.
Proof. By Lemma 4, we have for any T > 1,
Tη
X 4EkVL(W(t))k2 ≤ L(u(1)) - E[L(u(T + 1))] ≤ L(U⑴)< ∞,
t=1
which completes the proof of Eq. (38). The rest of claims follows immediately by Fubini’s Theorem
and Assumption 2.
The proof is completed.	口
B.2.2	B ounding the orthogonal part
Similar to the case of GDM, We define W as the solution of Eq. (12) with C2 =(%),. We also let
n(t) be given by Lemma 9, and define r(t) in this case as
r(t) 4 w(t) — log(t)W — W — n(t).	(39)
As W is a constant vector, and ∣∣n(t)k → 0 as t → ∞, we have w(t) 一 log(t)W has bounded norm
if and only if kr(t)k is upper bounded. Similar to the GDM case, we have the following equivalent
condition of that kr(t)k is bounded.
Lemma 17. Let all conditions in Theorem 3 hold. Then, kr(t)k is bounded almost surely if and only
if function g(t) is upper bounded almost surely, where g : Z+ → R is defined as
g(t)4 2l∣r(t)k2 + i-βhr⑴，W⑴-w(t-1)i-1-β Xhr(T)- r(T-1), W(T)- W(T-1)i. (40)
Proof. To begin with, we prove that almost surely | Ptτ =2 hr(τ) - r(τ - 1), W(τ) - W(τ - 1)i| is
upper bounded for any t. By Corollary 5, we have almost surly
∞
XkVL(W(t))k2<∞.
t=1
On the other hand, for any W, we have
ι∣vLB(t)(w)k = b X	`(hw,χi)χ
X∈T (B(t))
≤	- b X	'(hw, Xi) < 一b X '(hw, Xi)
X∈T (B(t))	X∈T (S)
≤	- b X '(hw,χi)hw,xi ≤ NN N X '(hw,Xi)X kwi
X∈T (S)	X∈T (S)
N
=	bγ ∣vL(w)k .
Therefore, we have almost surely,
∞
X vLB(t)(W(t))2 <∞,
t=1
30
Under review as a conference paper at ICLR 2022
which further leads to almost surely
∞
t
∞
X ∣w(t + 1) 一 w(t)∣2 ≤ η2
t=1
∞
≤η2X
t=1
∞
≤η2X
t=1
≤ η2
≤ K
η2
=口
E Eet-SVLB(S)(W(S))
t=1
S=1
βt-S VLB(S)(w(S))
βt-S VLB(S)(w(S))
βt-S
X	VLB(S)(w(S))2 X βt-S
S=1
∞
t=S
X	VLB(S)(w(S))2 < ∞.
S=1
∞
2
By the definition of r(t) (Eq. (39)), we further have
t
hr(T) 一 r(T 一 1), w(T) 一 w(T 一 1)i
τ=2
t
≤ X Kr(T) 一 r(τ 一 1), w(τ) 一 w(τ 一 1))|
τ=2
=^X J(w(T) 一 w(τ 一 1) 一 log (T + 1) 一 (n(τ) 一 n(τ ― 1)), w(τ) 一 w(τ 一 1)
t
t
≤	kw(τ) - w(τ - 1)k2 +	-log
τ=2
τ=2
T+1
---- I 一 (n(T) 一 n(T - 1)), w(t) 一 w(t - 1)
3t
≤2 Ekw(T) 一 W(T - 1)k2 +
2 τ=2
(?) 3 t
≤ 2 ∑Skw(τ) - w(τ - 1)k2 +
t
2
1 X 1
2 工-log
τ=2
2XOG) <∞,
τ=2
t-+1 ) -(n(T) - n(T -I))
where Inequality (?) is due to ∣∣n(τ) 一 n(τ - 1)k = O( 1) and log τ+1 = O( 1).
Therefore, g(t) is upper bounded almost surely is equivalent to 2 ∣∣r(t)∣2 + 1-e hr(t), w(t)-w(t-1)i
is upper bounded, which can be shown to be equivalent with ∣r(t)∣ is bounded following the same
routine as Lemma 2.
The proof is completed.
□
As the case of GDM, we only need to prove g(t) is upper bounded to complete the proof of Theorem
3.
Lemma 18. Let all conditions in Theorem 3 hold. We have g(t) is upper bounded.
Proof. Following the same routine as Lemma 2, we have
g(t + 1) 一 g(t)
=1 kr(t + 1) 一 r(t)k2 + hr(t), r(t + 1) 一 r(t)i + (r(t), 一 1一βvLB(t)(w(t)) 一 (w(t + 1) 一 w(t))) ,
where Pt∞=1 ∣r(t + 1) 一 r(t)∣2 is upper bounded.
31
Under review as a conference paper at ICLR 2022
On the other hand, by the definition of r(t) (Eq. (39)), we have
r(t + 1) - r(t)
=w(t + 1) - w(t) - log (t+-j- ) W - n(t + 1) + n(t),
while by Lemma 9,
NN1 X	ViXi=IOg (t41 )w+n(t+1) - n(t).
i4i∈T (B(t)∩Ss)	∖	/
Combining the above two equations, we further have
N
r(t + 1) - r(t) = w(t + 1) - w(t) — - E	ViXi,
i4i∈T (B(t)∩Ss)
which further indicates
g(t+1)-g(t) = 1 |lr(t+1)-r(t)k2+(r(t), - 1ɪ vLB(t)(w(t)) - n-	X	ViWi) ∙
2	Ie	bt
∖	L	i4i∈T (B(t)∩Ss)	/
Therefore, we OnlyneedtO prove PlIhr(t),-凸VL(W(t)) - N Pi：XieT(B(t)∩ss) viXii < ∞∙
By directly applying the form of VL(w(t)), we have
(r(t),-1-βVLB(t)(w(t)) - N X	Vmi)
i i∈T (B(s)∩Ss)
(r(t)，- (I -B)b	X	'0(hw(t)，血i)市i- n X
i 疝 i∈T (B(s))	i∙4i∈T (B(s)∩Ss)
(I-ηβ)b (r(t),-	X	'(〈w(t),端后i- N (I-B)I
∖	iai∈T (B(s))
(1—β)b (r(t),	-	X	'(hw(t), Wii)Wi-1
∖	iai∈T (B(s))
ViXi)
Σ
i∙∙Xi∈T (B(s)∩Ss)
ViXj
X	e-hw 岳Mi)
i^i∈T (B(s)∩Ss)	/
η
(1 - β)b
+ η
+ (1 - β)b
Σ
i∙∙Xi∈T(B(s)∩Ss )
X
i:Xi∈T (B(s)∩SC)
-QhW(t), Xii)- te-"i)hr(t),Xii
(r(t), -40(hw(t), XJ)Xi ∙
η
t
Let	A6(t)	=	Pi位i∈T(B(s)∩Ss)	(-，((w(t), Xii)- te-hw(t)岳))(r(t),	Xii,	and	A7(t)=
Pi.i.∈t(B(S)cS。)(r(t), -'0((w(t), Xii)Xii∙ We will investigate these two terms respectively.
As (w(t), Xi → ∞, ∀X ∈ T(S), a.s., we have a.s., there exists a large enough time t0, s.t., ∀t ≥ t0,
∀⅛ ∈ T(S),
-'(hw(t), Xi) ≤ (1 + e-"+hw㈤㈤)e-hw⑴㈤,
-'(hw(t), Xii) ≥ (1 - e-μ-hw㈤㈤)e-hw⑴㈤,
(X, w(t)i > 0∙
32
Under review as a conference paper at ICLR 2022
Therefore,
A7(t) ≤	E	_'(hw(t), Xii)hr(t), Xii1(r(t),Xi>≥0
i∙.Xi∈T (B(s)∩Sc)
≤
E	(1 + e-μ+hw⑴,xi〉)e-hw㈤,即r(t),端口⑴生后。
i∙∙Xi∈T(B(S)∩Sc)
≤2
Σ
i∙Xi∈T (B(s)∩Sf)
eTr(t)+l°g⑴w+w+n㈤,χii mt),端 1——
(?)
≤ 2
Σ
i∙Xi∈T (B(s)∩Sc)
,eTw+n㈤,xi〉e-hr㈤,xi〉(r(t),端口⑴*后。
(t) 2 1
≤-----
一 etθ
=)O
Σ
i：xieT (B(s)∩sc)
ɪ)
tθ	,
e—〈w+n(t),xi〉1/ J
e	〈r(t),xi〉≥0
where Inequality.	(*) is due the definition of θ (Eq. (31)), Inequality. (f) is due to
e—〈r(t),xi〉(r(t),而ii ≤ e—1 , andEq. (o) is due to limt→∞ e—〈w+n(t),xi〉= e—〈w,xi〉. Thus,
∞
X A7(t) < ∞.
t=1
On the other hand, A6 (t) can be rewritten as
4(t)
E (一'(hw(t), Xii)-
i：xiET (B(s)∩Ss)
；e—〈w⑴,xi〉) hr(t), Xi〉1(r(t),x^0
+ E	—'(hMt), Xii) _
i：xieT(b(s)∩ss) '
；e—(w⑴㈤i) hr(t),端 1(r(t),Xi><0.
If <r(t),X力 ≥ 0, we have for ε < 0.5,
-40(hw(t), Xii) _ ；e—(w⑴,xi〉) hr(t), Xi
≤ ((1 + e-μ+ hw⑴,xi〉) e—hw(t),xi〉_ 1 e—hw(t),xi〉) (r(t), Xii
1 + e—*+〈r(t)+iog(t)w+w+n(t),xi〉) e—〈r(t)+iog(t)w+w+n(t),xi〉_ ɪe—〈w(t),xi〉) (r(t) 蝠
((1 + e-*+〈r(t)+log(t)w+w+n(t),xi〉) e—〈r(t)+n(t),xi〉_ 1) 1 (r(t),而iie—〈w(t),xi〉
≤ ((1+___3_e—“+〈w+n(t)/i〉) e—〈r(t)+n(t)岳i〉_ 1) ɪ(r(t), XiieTw㈤,必i〉
1 + O
1 + O ft0⅛ε) ) e—〈r(t),xi〉- 1) 1 hr(t),自ie—〈w(t),xi〉
e—〈r⑴,xi〉_ 1) 1(Nt), XiieTw㈤,xi〉+ 1。
1
∖min{μ+ ,0.5—ε}
e-hr⑴，xii (r(t), Xiie-hwOxi)
≤"
1
∖min{μ+ ,0.5—ε}
e—〈r(t)，xi)(r(t), Xi〉e—〈w(t)，xi)
=)O
1
[min{ 1+μ+, 1.5—ε}
where Eq. (•) is due to n(t) = O(◎1-ε), andEq. (◊) is due to e—hr(t),xi〉(r(t),端 ≤ ɪ.
33
Under review as a conference paper at ICLR 2022
On the other hand, if (r(t),端 < 0, we have
-40(hw(t), Xii)- ；eTw(t)，Xi) hr(t),竭
≤ ((1 - e-μ-h∞(⅛),⅛ii) e-hw(t),xi - 1 e-hw(t),χii) hr(t), Xii
=-eTw,xi> (-1 +(1 - e-μ-h∞(⅛),⅛i) e-hr⑴+n⑴a)) hr⑴，Xi)
Specifically, if hr(t), Xii ≥ -t-0-5min<μ--0.5},
-eTw,xi) (-1 + (1 - e-"-(w(t)，xi〉) e-hr㈤+n㈤,xii) gt), Xii
≤_________1________e-hw,xii I _1 + (1 _ e-μ-h∞(t),χ ii) e—〈r(t)+n(t)，xi)
—11+0.5 min{μ- ,0.5}
(=)O (_________1__________∖
11+0.5min{μ-,0.5}	,
where Eq. (□) is due to that as (w(t), Xii → ∞ and t-0∙5 min{*一,0.5} → 0 as t → ∞, there exists
a large enough time T, s.t., ∀t > T, under the circumstance 0 > (r(t), Xii ≥ -t-0∙5min{μ-,0.5},
e-hr(t)+n(t),χii < 1 and e-μ-hw(t),x i < 1.
If -2 ≤ hr(t), Xii <
e〃-(一〈w,xi〉+4)
-1-0.5min{μ-,0.5}, then, for large enough t, |〈Xi, n(t)i∣ < 2, 1 —
> 0, and
1 e-hw ,xi)
t
=1 e-hw ,xii
t
=1 e~hw ,xi)
t
≤ 1 e~hw ,xi)
~t
≤ 1 e-hw ,x.
t
≤ 1 e-hw ,x.
t
=1 e-hw ,x.
t
=1 e-hw ,x.
t
(-1 +(1 - e-"-(Mt),χii) eTr(t)+n(t)，x") (r(t), Xii
(-1 + (1 - e-*-(r(t)+iog(t)w+w+n(t),χ ii) e-hr(t)+n(t),x ii) (r(t), Xii
(-1 +(1 - J"-μwχ1e-"-(r(t)+n(t),x i) e-hr(t)+n(t)，xi) hr(t), Xii
e	e	eμ-(-hw ,xii+4)、
(-1 +(1---------1_——)e-μ-Et)+n⑴,χi)> hr(t), Xii
e	e	eμ-{-hw ,xii+4) ∖
(-1+11----------tμ———)(1-μ-hr(t)+ n(t), Xii)) (r(t), Xii
6"-(一〈疝,Xi)+4)∖
(-1+(1----------tμ-——)(1 + μ-t-0.5min{μ-，0.5}- μ- hn(t), XiiDhr(t), Xii
(-1+ (1 -t-；'")(1 + μ-t-0.5min{μ-,0.5} + o (t-0.5min{μ-,0.5}))) hr(t), Xii
(-1 + 1 + μ-t-0.5min{"-,0.5} + o (t-0.5min{“-,0.5})) hr(t), Xii < 0.
If -2 > hr(t), dcii, then for large enough time t, e-hr(t)+n(t),x ii ≥ e2, 1 — e-μ-hw(t),x ii ≥ e-2,
and
1 e-hw,x ii (-1 + (1 - e-"-hw㈤,xii) e-hr(t)+n(t),x) (r(t), Xi)
≤∣e-hw,xii (-1 + e)hr(t), Xii < 0.
Conclusively, if hr(t), Xii < 0, for large enough t, we have
(-'0(hw(t), Xii)- ；e-hw(t),xi) hr(t), Xii ≤ O (tι+0.5mi∖μ-,0.5}),
34
Under review as a conference paper at ICLR 2022
which further indicates, for large enough t, we have
1
A6(t) ≤ max O
11+0.5 min{μ- ,0.5}
,O
1
which indicates
∞
A6(t) < ∞.
t=1
Therefore,
∞
X(g(t+1)-g(t))
t=1
=X ( 1 kr(t + 1) - r(t)k2 + ( r(t), -ττη-βvLB(t)(W⑴) - Nt X	Vix))
t=1 ∖	∖	_ P	i∙Xi∈T(B(t)∩Ss)	/ )
=X (2kr(t + 1) - r(t)k2 + 1⅛A6(t) + T⅛A7(t))
t=1 2	1 - β	1 - β
<∞.
The proof is completed.
□
B.2.3 Explanation for Proper Lyapunov Function
Based on the success of applying Lyapunov function L(w(t)) + 号 ∣∣w(t) - w(t - 1)∣2 to analyze
gradient descent with momentum, it is natural to try to extend this routine to analyze stochastic
gradient descent with momentum. However, in this section, we will show such Lyapunov function
is not proper to analyze SGDM as this will put constraints on the range of the momentum rate β.
Specifically, at any step t, since the loss L is H smooth at w(t), we can expand the loss L in the
same way as the GDM case:
L(w(t + 1)) ≤L(w(t)) + hw(t +1) - w(t), VL(w(t))i + HH∣∣w(t + 1) - w(t)k2.
By taking expectation with respect to w(t + 1) conditioning on {w(s)}ts=1 for both sides, we further
obtain
Ew(t+1) L(w(t + 1)) {w(s)}ts=1
H
≤L(W⑴)+ hEw(t+1) [w(t +1) - W⑴ ∣{w(S)}s = ι] , VL(W⑴)i + 工Ew(t+1) [kw(t +1) - W⑴k2 ∣{w(S)}s = J
= L(w(t)) + 1 <Ew(t+i) [w(t + 1) - w(t) ∣{w(s)}S=ι] ,β (w(t) - w(t - 1)) - Ew(t+i) [w(t + 1) - w(t) ∣{w(s)}S=ι]；
H
+ yEw(t+1) [kw(t + 1) - W⑴k2 ∣{w(S)}s=i]
β
=L(W⑴)+η((秒⑴-W(t-1)), Ew(t+i) IW(t+1)- W(t) i{w(s)}s=i]〉
H1
+-ɪ Ew(t+i) [kW(t+1)- W⑴k2 i{w(S)}s=i]- η ∣∣Ew(t+i) [W(t+1)- W⑴ i{w(S)}s=i] Ii
sL(W(t)) + 2η IHt)- W(t- 1)k2 + 2η ∣∣Ew(t+i) [W(t + 1)- W(t)|{W(S)}S=i] ∣∣2
+ HHEw(t+i) [kW(t +1)- W⑴I2 |{w(S)}S=i] -1 ∣∣Ew(t+i) [W(t +1)- W⑴ |{w(S)}S=i] ∣∣2,
2η
where Eq.	(?) is becasue Ew(t+1) [w(t + 1) - w(t) |{w(s)}ts=1 ]
-ηVL(w(t)) +
β (W(t) - W(t - 1)) due to the definition of SGDM (Eq. (1)). Rearranging the above inequal-
35
Under review as a conference paper at ICLR 2022
ity and taking expectations of both sides with respect to {w(s)}ts=1 leads to
Ew(t+1) [L(w(t + I))] + 2-β E{w(s)}t,1 ∣∣Ew(t+1) [w(t + I)- W⑴ |{w(s)}S = 1 ]∣∣2
2η	s=1
H
-~2E{w(s)}S=i [kw(t +I) - W⑴k ]
≤Ew(t)L(W⑴)+ T^-E{w(s)}t ɪ kw(t) - w(t -I)k2.	(41)
2η	s=1
On the other hand, we wish to obtain some positive constant α from Eq. (41), such that (at least),
Ew(t+1) [L(W(t + 1))] + αE{w(s)}ts=1 kW(t + 1) - W(t)k2 .
≤Ew(t)L(W(t)) + αE{w(s)}ts=1 kW(t) - W(t - 1)k2 ,	(42)
which requires to lower bound E{w(s)}t ∣∣Ew(t+1) [W(t + 1) - W(t) |{W(s)}ts=1 ]∣∣2
by E{w(s)}t+1	k W(t + 1) - W(t)k2.	However, in general cases,
E{w(s)}S=1 ∣∣Ew(t+i) [w(t + 1) - w(t) ∣{w(s)}S=ι]∣∣2 is only UPPer bounded by
E{w(s)}t+1 k W(t + 1) - W(t)k2 (Holder’s Inequality), although in our case,
∣∣Ew(t+1) [W(t + 1) - W(t) |{W(s)}ts=1 ]∣∣2 can be bounded as
∣∣Ew(t+1) [W(t + 1) - W(t) ||{W(s)}ts=1 ]∣∣2
=k-ηVL(w(t))+ β (w(t)- w(t - 1))k2
=k-ηVL(w(t))k2 + kβ (w(t) - w(t - 1))k2 + 2βηhw(t) - w(t - Γ),-VL(w(t)∙)i
while Ew(t+1) k W(t + 1) - W(t)k2 |{W(s)}ts=1 can be calculated as
Ew(t+1) kW(t+1)-W(t)k2 ||{W(s)}ts=1
=EB(t) ∣∣-ηVLB(t)(W(t)) + β(W(t) - W(t - 1))∣∣2
=EB(t) ∣∣-ηVLB(t)(W(t))∣∣2 + kβ(W(t)	- W(t	- 1))k2 + 2ηβEB(t)h-VLB(t)(W(t)), W(t) - W(t	- 1)i
=EB(t) ∣∣-ηVLB(t)(W(t))∣∣2 + kβ(W(t)	- W(t	- 1))k2 + 2ηβh-VL(W(t)), W(t) - W(t - 1)i
≤TN2 k-ηVL(w(t))k2 + kβ(w(t) - w(t - 1))k2 +2ηβh-VL(w(t)), w(t) - w(t - 1)〉
bγ2
≤bN (k-ηVL(w(t))k2 + kβ(w(t) - w(t - 1))k2 + 2ηβh-VL(w(t)), w(t) - w(t - 1)〉)
N2
=宁 ∣∣Ew(t+1) [w(t +1) - W⑴ |{w(S)}s = l]∣∣ .
(43)
By Eqs. (41) and (43), we have that to ensure Eq. (42), it is required that
2 - βbγ2 H β
2η NT - "2" ≤ 而，
which Puts constraint on β that
β ≤ 2 JN 寸-Hn
≤ ι + N Y2 .
SPecifically, β <
range.
2 N γ2
1+N γ2
, which aPProaches 0 when γ aPProaches 0, and constrains β in a small
36
Under review as a conference paper at ICLR 2022
C Implicit Bias of Adam
This section collects the proof of the convergent direction of Adam, i.e., Theorem 4. The methodology
of this section bears great similarity with GDM, although the preconditioner of Adam requires specific
treatment for analysis. The proof is still divided into two stages: (1). we first prove the sum of
squared gradients along the trajectory is finite. Additionally, we prove the convergent rate of loss
is O( t )；(2). We prove w(t) - log(t)W has bounded norm. Before We present these two stages of
proof, we will first give the required range of η for which Theorem 3 holds.
C.1 Choice of learning rate
Let Hs0 be the smooth parameter over [s0 , ∞) given by Assumption 3. (D). Let β2 = (cβ1)4 (c > 1).
The "sufficiently small learning rate" in Theorem 3 means
η≤
√ε inf t≥2
(1-βt _ 1-βt-1	1-(cβι)t ʌ
(l-βι c(1-βι) 1-(cβι)t-1 J
H'-1((1-cβι)-1 N L(W(I)))
To ensure η is Well-defined, We need to prove
."1- βt _ 1- βt-1 1 - (cβι)t ʌ , 0
in2 υ - βι c(1- βι)1 - (cβ1)t-1J	,
and We introduce the folloWing technical lemma:
Lemma 19. Define ft(x) = χ(11-x-i), ∀t ∈ Z, t ≥ 2. We have ft(x) is decreasing with respect to
x. Furthermore, for any x ∈ [0, 1), we have
f (x) ≥ PZfPy	(44)
Proof. First of all, by definition,
1-x	1-x	1-x	1
f(x) = X-标=1 + X-标=1 + χ(1-χt-1) =1 + χ(1 + X + …+ χt-2)
is monotonously decreasing as 0 ≤ X < 1. Secondly, Eq. (44) is equivalent to
(1 — xt )4	(1 — x4t)
β4(1 - Xt-1)4 ≥ x4(1 - x4(tT))
(1 - Xt )3	(1 + Xt )(1 + X2t )
Vf ⇒ ---------：-- 〉--------：------------7—.
(1 - xt-1)3 - (1 + xt-1)(1 + x2(tT))
The left side of the above inequality is no smaller than 1, While the right side is no larger than 1,
which completes the proof.	□
We are noW ready to prove η is Well-defined. First of all, for every t, We have
1 - βt	1 - βt-1 1 - (cβι)t
-------- ：---------：-------：--：-： Γ
1 - βι	c(1 - βι) 1 - (cβι)t-1
=βι(1- βt-1) (	1- βt______________1 - (cβι)t	A
=1 - βι	lβι(1- βt-1)	(cβι)(1 - (cβι)t-1 )7
="-βt 1) (ft(βι) - ft(cβι)) > 0,	(45)
1 - β1
where Eq. (?) is by Lemma 19 and cβι = √β2 < 1.
On the other hand, we have
lim (I-M - 1-βt-1 1 - (Mt A =(1-1A ɪ
t→∞11- βι c(1 - βι)1 - (cβι)t-17 I C) 1 - βι
(46)
By Eq. (45) and Eq. (46), we obtain 1-β^
constant across t, and η is well defined.
1--βι) ι--,ββ1M is lower bounded by some positive
37
Under review as a conference paper at ICLR 2022
C.2 Sum of gradients along the trajectory is bounded
We start with the following lemma, which indicates L(w(t)) +1∣ 4Jε1d + ν(t) Θ (w(t) 一 w(t —1)) k2
is a proper Lyapunov function for Adam.
ProofofLemma 5. We start with the case t = 1. To begin with, We have L is h`-i(nl(w(i)))
smooth around w(1). By definition Hx is non-increasing with respect to x, and since `-1 is also
non-increasing, we have
H'-1(NL(w(1))) ≤ H'-1(T-CerNL(w(1))),
which further indicates when α is small enough,
(?)	L 2
L(w(1 + α)) ≤L(w(1)) + αhVL(w(1)), w(2) — w(1)i + — α2∣∣w(2) — w(1)k
=L(W(I))—《VL(W(I)),η pε1⅛w
Θ VL(W(1))	+ o(α2)
≤L(W(I))- 2ηα2 UPε1d + V(I) θ (W(2)一w(t))|| ,
where in Eq. (?) We denote L 4 h`-t(1％ Nl(w(i))), and the last inequality is due to 2ηɑ2
UPε1d + 力⑴ θ (w(2) — w(t))||	= o(α2), and (VL(W(I)),η√^[a) θ VL(W(I)),is
positive.
Now if there exists an α ∈ (0,1), such that Eq. (10) fails, we denote a* = inf{α :
Eq.(10) fails for 1 + α}. We have α* > 0, and the equality in Eq. (10) holds for 1 + α*.
Therefore, We have for any α ∈ (0, α*),
L(w(1 + α)) ≤ L(w(1 + α)) + 2n02 U Pε1d + ^(1) Θ (w(2) — W(t))∣∣ ≤ L(w(1)),
which by Lemma 10 leads to L is h`-i(nl(w(i)))smooth (thus L smooth) over the set {w(1 + α):
α ∈ [0, α*]}, and
L(w(1 + α*))
≤L(w(1)) + α*(VL(W(1)), w(2) — w(1)〉+ 2(α*)2∣∣W(2) — w(1)∣2
=L(w(1)) — α*(1 Pε1d + ^(1) Θ (w(2) — w(1)) , w(2) — w(1)) + 2(α*)2∣∣w(2) — w(1)∣2
=L(W⑴)—α* η UP^ θ (W⑵一W(I))U2+2(α*)2 w≡ θ P^ θ (W⑵一W(I))
2
≤L(w(1)) — α* 1 U P ε1d + V(1) Θ (w(2) — W(I))U2 + 2√f (0*)2 U pɛl+^ Θ (w(2) — W(I))U2
<L(w(1)) — (α*)2； U Pε1d + V(1) Θ (w(2) — W(I))U2 + 2√^9*)2 U Pε〃 + V(1) G (w(2) — W(I))U2
≤L(w(1)) — (α*)22η U Pε1d + ^(1) Θ (w(2) — W(I))U2 ,
(47)
where the second-to-last inequality is due to kW(2) — W(1)k > 0 (by Lemma 13) and α* > (α*)2,
while the last inequality is due to
个 ir1f-∙√1-βt	1-βt-1 1-(cβι)t
√ε inft≥2 ( ι-βT - c(l-βι) 1-(cβι)t-ι
η ≤-----------丁..................
ε 11-β2 _	1-β1 ι-(cβι)2∖
Vε (l-βι	c(1-βι) 1-(cβι) J
L
= : ‹ √ε.
38
Under review as a conference paper at ICLR 2022
Eq. (47) contradicts the fact that the equality in Eq. (10) holds for 1 + α*, which completes the proof
of t = 1.
If t ≥ 2, following the similar routine as t = 1, we also prove Eq. (10) by reduction to
absurdity. If there exist t and α such that Eq. (10) fails. Denote t* as the smallest time
such that there exists an α ∈ [0,1) such that Eq. (10) fails for t* and α. By Lemma 13,
U Pε1d + ^(t* — 1) Θ (w(t*) — w(t* — 1))∣∣ is positive, and strict inequality in Eq. (10) holds for
t and α = 0, which by continuity leads to
1 > a* 4 inf{α ∈ [0, 1] : Eq.(10) fails for 1 + α} > 0.
Then, for any α ∈ [0, α*], we have
L(w(t* + α))
≤L(W(t* + α)) + 2α2"(1 -β])∣∣ Pε1d + ν(t*) θ (W(t* +I) - w(t*))∣∣
1	Qt* ——1	1 (CQ 11* 11	119
≤L(w(t*)) + --,1s -—( R ；*_11∣Pε1d+V① -I) θ (w(t*) -w(t* - 1))∣∣ .
2cη (1 - β1 ) 1 - (cβ1 )t -1
On the other hand, for any time 2 ≤ s ≤ t* - 1, we have
L(W(S+1)) + 1 η∖ J) ∣∣ P ε1d+V(S θ (W(S+1) - w(s))∣∣
≤L(W(S)) + β(IC % / J -(CeILI ∣∣Pε1d+V(S — 1)θ(W(S) - W(S - 1))∣∣ .	(48)
2cη (1 - β1 ) 1 - (cβ1 )s-1
By Eq. (46), we have
1 — β1	1 — β1	_ 1 — β1 1 — (cβι )s+1 1 — (cβι)s
η(1 — βι) > C"(1 — βι)	C"(1 — βι) 1 — (cβι)s 1 — (cβι)s+1
which by ((Tes)) further leads to
L(W(S))+⅛[β⅛⅛⅞β⅛ UP ε1d+4 -1) θ (W(S)-W(S -1))∣∣2
1 1 βs
≥L(W(S + 1))+2 T
Pε1d + V(S) Θ (w(s + 1) — w(s))∣∣
>L(w(s + 1)) + 2 1 - β,)I-(TI)S+1 11-("+ι ∣∣ Pε1d + ^(s) Θ (w(s + 1) — W(S)) ∣∣2
2cη (1 — β1 ) 1 — (cβ1 )s 1 — (cβ1 )s+1
> 1 - (cβι)s
> 1 - (cβι)s+1
L(W(S + 1)) +
1 — β1	1 — (cβι)s+1
2c"(1 — βι) 1 — (cβι)s
Wε1d + V(S) Θ (w(s + 1)—
(49)
On the other hand, for S = 1, we have
L(W(I)) ≥L(W⑵)+ 2"ɪɪ ∣∣ ^1^θ (w(2) - W(I))U2
≥ 1 - (CeI)
≥ 1 - (cβι)2
L(W(2)) +
1 - Bl 1 - (。β1)2
2c"(1 — βι) 1 -(CeI)
4/ε1d + ^(1) Θ (w(2)—
(50)
39
Under review as a conference paper at ICLR 2022
Combining Eqs. (48), (49), and (50), we have
L(w(t* + Q))
≤l(Wr))+X不 J；= BP ε1-…I) θ (Wr) - w-I))『
< II-(C,;I jc(w(t* - 1)) + J - βt ： 1 - (cβ1)tr B Pε1d + V(t*- 2) Θ (w(t* - 1) - w(t* - 2))『
1 — (c,2)t 1 ∖	2cη(1 — ,1) 1 — (cpι)τ 2 Il	Il
<…
J-(CPι)t*
< 1 - (cβ1)2
L(w(2)) +
1 - PI	1 -(CPI)2
2cη(1 — Pi) 1 - (cPι)
4/ε1d + ^(1) Θ (w(2)-
1-(cpι)t*
1 - cpi
L(W(I)) <
1
1 - cpi
L(W(1)).
≤
Therefore, by Lemma 10, L is 4-i(】％ NL(W(I))) smooth (thus L smooth) over the set {w(t* + α):
Q ∈ [0, α*]}, which further leads to
L(w(t* + α*))
≤L(w(t*)) + α*(VL(w(t*)), w(t* + 1) - w(t*))+ ∣(α*)2kw(t* + 1) - w(t*)∣∣2
=)-	α*
=- η(1 - Pi)
<w(t* + 1) - w(t*), (1 - Pt* )pε1d + V(t*) Θ (w(t* + 1) - w(t*))
-P1(1 - Pt*-1)√ε1d + ν(t*) Θ (w(t*) - w(t* - 1)))
+L(w(t*)) + ∣(q*)2∣∣w(t* + 1)- w(t*)∣∣2
=L(w(t*)) + go*)2∣∣w(t* + 1)- w(t*)∣∣2-
Q*(1 - Pt*)
η(1 - P1)
Pε1d + V(I* Θ (w(t* + 1) - w(t*)) I I
C Q
+P1
：*(1 - Pt- 1
η(1 - P1)
) (w(t* + 1) - w(t*), pε1d + ^(t*) Θ (w(t*) - w(t* - 1)),
=L(w(t*)) + ∣(q*)2∣∣w(t* + 1) - w(t*)∣∣2 -	— PI)) I Pε1d + ^(t*) Θ (w(t* + 1) - w(t*))∣∣2
C Q
+P1
：*(1 - Pt*-1
η(1 - P1)
) ( P： 1；+ “I；1) Θ Pε1d + ^(t*) Θ (w(t* + 1) - w(t*)),
∖ V ε1d + ^(t*)
Pε11+	Θ Pε1d + ^(t*- 1) Θ (w(t*) - w(t* - 1)) +
V ε1d + ^(t*)	/
≤L(w(t*)) + ∣(q*)2∣∣w(t* + 1) - w(t*)∣∣2 -	— PI)) I Pε1d + ^(t*) Θ (w(t* + 1) - w(t*))∣∣2
*
+P1
(a*)2(1 - Ptji)	Wε1d + ^(t* - 1)
2
2η(1 - P1)
弋 ε1d + V(t*)
Θ 4Jε1d + ν(t*) Θ (w(t* + 1) - w(t*))
+p (1-Pt*-1)
+P1 2η(1 - P1)
√ε1d + ^(t* - 1)
P ε1d + ^(t*)^^
2
Θ 4Jε1d + ν(t* - 1) Θ (w(t*) - w(t* - 1))
40
Under review as a conference paper at ICLR 2022
(V)L(W(t*)) + L(α*)2∣∣w(t* + 1) - w(t*)k2 - °：1- H P^ + W Θ (w(t* + 1) - w(t*)) ∣∣2
2	η(1 -历)Il	Il
+βι (R211-βt)T) 8 U*") ∣∣ Pε1d + ν(t*) Θ (w(t* + 1) - w(t*))『
2η(1 - βι)	cβι(1 - (cβι)t 1) Il	Il
/rc∕∙* ——1∖r∕C∖**	C
+βι⅛TΓ⅛Γ 8(1-((CKt*-n ∣∣ Pε1d + ν(t*- 1) Θ (w(t*) - W(t* - 1))∣∣
2η(1 - βι) cβι(1 - (cβι)t	1) Il	Il
≤L(w(t*)) + -√= (α*)2∣∣ Pε1d + ^(t*) Θ (w(t* + 1) - w(t*))∣∣2
2y ε
—
；(1 -；「∣∣ Pε1d + ^(t*) Θ (w(t* + 1) - w(t*))∣∣2
Q (a*)2(1 - βt )	1 -(。尸1了 ∣∣ 4Γ~^—I	厂、/ /,* l 1、	/,*"∣2
+β1 2η(1-β)	cβ(1-(cβ)t*-i) U5 + %)θ (W( +1) - W(t DH
+β 上:工 1」-W),I、∣∣ Pε1d + V(t*- 1) Θ (w(t*) - w(t* - 1)) ∣∣2
2η(1 - βι) cβι(1 -(cβι)t 1) Il	Il
吵L(w(t*)) - (OOT-T)B Pε1d + ^(t*) Θ (w(t* + 1) - w(t*))∣∣2
2η(1 —历)Il	Il
+ (； -J -I)) (II-((Cβ1" ∣∣ Pε1d + ν(t*- 1) Θ (w(t*) - w(t* - 1))∣∣2 ,
2η(1 —历)c(1 — (cβι)t 1) Il	Il
where Eq. (•) is due to an alternative form of the Adam,s update rule:
(1 - β1 )P ε1d + 0(t*) θ (w(t* + 1) - w(t* )) - β1(1 - β1 -I)P ε1d + V(t* - I) θ (W(t*) - w(t* - I))
=-η(1 - βι)VL(w(t*)),	(51)
Inequality (◊) is due to

Tε1d + V(t* - 1)
P ε1d + ^(t*)—
ε1d + V(t* - 1)
ε1d + V(t*)	— ∖
ε1d +
ε1d + V(t* - 1)
β2ν(t*-1) + (1-β2)VL(w(t*))2
V 4
1
二S
ε1d + ^(t* - 1)
ε1d + V(t - 1)
ε1d + V(t - 1)
ε1d + e—
u1 I β2(1-βt*-1)V(1*-1)
ε1d +	1-βξ*∙
β2(1-β2*-1 )^(1*-1)i,ι	, β2(1-βt*-1)^(ι*-1)
1-βt*	ε1d +	1-βf
"-；11IJ 1d (all the computings are COmPOnent-WiSely),
4
t
4

≤
4
t
and f(cβι) ≥ 4/f ((cβι)4), and Inequality (□) is due to
L ≤ inf 1≥2 (⅛⅜
2√ε ≤
c(1-βι) 1-(cβι)t"!
2η
1-βt*	1-βt*-1	1-(cβι)t*
---------------- ------ . 一 .
I 1-βT - c(1-βι) 1-(cβι)t*-1
≤ -ʌ-------------------------------
2η
—
i-βt-1 i-(cβι)t
—
α* > (α*)2,and ∣∣pε1d + ^(t*) Θ (w(t* + 1) - w(t*))∣∣2 > 0.
This contradicts to that the equality in Eq. (10) holds for t* + α*.
The proof is completed.
□
As limι→∞ β1 = 0 and limι→∞(cβ1)1 = 0, we have the following corollary based on Lemma 1.
Corollary 6. Let all assumptions in Theorem 4 hold. Then, for large enough t, we have
L(W(t+1)) + 5√=Y~~钎 ∣∣ P ε1d+Vs θ (w(t+1)- w(t))∣∣
24cη(1 - β1) Il	Il
≤L(w(t)) + 2√η(1 - β) ∣∣ Pε1d + ν(t - I) θ (w(t) - w(t - I))II .	(52)
41
Under review as a conference paper at ICLR 2022
Consequently, we have
∞
X kVL(w(t))k2 < ∞.
t=1
(53)
The proof of Corollary 6 requires the following classical lemma on the equivalence between the
convergence of two non-negative sequence. The proof is omitted here and can be found in Wang et al.
(2021).
Lemma 20 (c.f. Lemma 27, Wang et al. (2021)). Let {ai}i∞=1 be a series of non-negative reals, and
ε be a positive real. Then, P∞=ι ai < ∞ is equivalent to Pi∞=ι √ +p； = < ∞.
Proof of Corollary 6. We have
r 1 - βt-1	1-(cβι)t	1	,	1
lim	=<
t→∞ 2cη(1 一 βι) 1 一 (cβι)t-1	2cη(1 一 βι)	2√cη(1 一 βι)
1	1 - βt	1	、	1
lim	=>
t→∞ 2η(1 一 员)2η(1 一 βι)	2√Cη(1 一 βι),
which completes the proof of Eq. (52). Rearranging Eq. (52) leads to
2√ηc1 一\) B Pε1d ”⑴ θ (w(t +1) 一 w(t)) B ≤ 2√Cη(1 — βι) B Pε1d …-I) θ (W⑴一 w(t 一 I))Il
+L(WW) 一 (L(W(t+I))+2 √η(1 一 β0 B p εid+V(U θ (W(t+I) - W(U)B),
which by iteration further leads to that for a large enough time T1
X D 2PZZc 1---TΓ B Pε1d + z>(t) θ (W(t + 1) - W⑴)B
T 2 2 cη(1 一 β1 )
t=T1
≤L(w(Ti)) + 3 ； Q B Pε1d + V(TI- 1) Q (W(TI)- W(TI- 1))B2
2	4 cη(1 一 β1 )
-L(W(T2 + I)) + 2√cη(1 一 β]) B Pε1d + V(C) θ (W(T2 + 1) - W(T2 + I))B
<L(W(TI)) + 2√cη(1 一 β]) B Pε1d + V(TI 一 I) θ (W(TI) - W(TI - I))B .
Consequently, we obtain
三	.4/7 _ 1 II _____________ ∣∣2
X 2√cη(1 - βι) Bpε1d + "⑴ θ (W + 1)一 W(t))B < ∞.
(54)
On the other hand, for any t, we have
B Pε1d + V(t) Θ (W(t + 1) 一 W(t))B BPε1d + ^(t) Θ WB
≥ DPε1d + V(t) Θ (W(t + 1) — W(t)), Pε1d + V(t) Θ WE = DPε1d + V(t) Θ (W(t + 1) — W(t)), WE
=h-ηm(t),Wi = 一η(11--β1) (Xβt-sVL(W(s)), W
=一 η(1 - β1) ɪ [x βt-s X '0(hXi, W(S)i)Xi, W) ≥ -η(1 一 β1) — X βt-s X '0(hXi, W(s)i)
1 一 βt N 1	i,	i,	1 一 βt N 1	i,
'1	∖s=1	Xi∈T (S)	/	' 1	s=1	Xi∈T (S)
≥-F妥N X '《Xi,W(t)i)≥
Pl	Xi∈τ (S)
T-ZeI) kVL(W(t))k,
1 一 β1
42
Under review as a conference paper at ICLR 2022
which by Eq. (54) indicates
X f η(i- βι) Y	kVL(w(t))k2	< ∞
t=11 1-β" ∣∣pεid+^w Θ W『.
As limt→∞
(η1-βt1)) = η2(1 — βι)2, We then obtain
XX	kVL(w(t))k2	≤ XX	kVL(w(t))k2
t=ι《ε + Ps=I kVL(w(t))k2 — t=ι《ε + £：=皿-β)βt-skVL(w(t))k2
kVL(w(t))k2
2 /u J PS = ι(1-β)βt-sgL(w(t))k2
Vε +	ι-βt
≤ driɪe X
4 ε1d +
kVL(w(t))k2
PS = ι(1-β)βt-sVL(w(t))2
1-βt
2
≤rι-β XX
kVL(w(t))k2
P ε1 + V(t) ∣∣2
,一“2	Γ^~ 好	∣∣VL(w(t))k2
≤ dkwk∞√L E ∣	'	< ∞,
V 1-β t=ι ∣∣p ε1d + ^(t) Θ w∣∣
Which by Lemma 20 completes the proof.
□
Based on Corollary 6, We can further prove Lemma 6, characterizing the convergent rate of loss L
directly.
Proof of Lemma 6. To begin With, Eq. (51) indicates
kη(1- βι)VL(w(t))k2
=∣∣(1 一 βt)pε1d + ν(t) Θ (w(t +1) — w(t)) — βι(1 — βt-1)pε1d + ^(t — 1) Θ (w(t) — w(t — 1))∣∣
≤ Il (1 一 βt)p ε1d + ν(t) θ (W(t+I)- Wd))II + ∣∣βι(1 一 βt 1)ρ ε1d + ν(t — 1) θ (W(t)— w(t - I))II
≤ (∣∣pε1d + ^(t) Θ (w(t + 1) - w(t))∣∣ + ∣∣pε1d + ^(t — 1) Θ (w(t) - w(t - 1))∣∣)
≤2 (∣∣Pε1d + ^(t) Θ (w(t + 1) — w(t))∣∣ + ∣∣pε1d + V(t — 1) Θ (w(t) — w(t — 1))∣∣ )	(55)
On the other hand, by Corollary 6,
∞
X ∣VL(w(s))∣2 < ∞,
s=1
Which folloWing the same routine as Corollary 5 leads to
, . ~
hw(t), Xi → ∞, ∀x ∈ S.
Therefore, by Lemma 11, there exists a large enough time T1, such that ∀t ≥ T1,
ɪ'(hw(t), Xi) ≤ -'0(hw(t), Xi) ≤ K'(hw(t), Xi),∀X ∈ T(S),
K
Which by the separable assumption further leads to
KL(w(t)) ≤-N X '0(hw(t), Xi) ≤ N
X∈T (S)

-
≤N X '0(hw(t),Xi)X kYWk = ∣VL(w(t))∣
X∈T (S)
≤-N X '0(hw(t), Xi) ≤ K L(w(t)).
X∈T (S)
Σ
X∈T (S)
'0(hw(t), X))X, YW
(56)
43
Under review as a conference paper at ICLR 2022
Combining Eq. (26) and the above inequality, we have
η( J)，) L(w(t))2 ≤2 0∣pε1d+V⑴ © (w(t+I)- w(t))ii
+
√ε1d + V(t - 1) Θ (w(t) - w(t
(57)
On the other hand, by Eq. (54), we have
X ∣∣ pε1d + V(t) Θ (w(t + 1) - w(t))∣l < ∞.
t=1
Therefore, there exists large enough time T2, such that ∀t > T2,
∣p ε1d + V(t) Θ (w(t + 1) - w(t))∣∣ < 1,
and thus,
Pε1d + V(t) Θ (w(t + 1) — w(t))∣∣ < ∣∣ Pε1d + ^(t) Θ (w(t + 1) — w(t))∣∣ .	(58)
Combining Eq. (57) and Eq. (58), there exists a positive real constant C, such that
L(w(t))2 ≤C (∣∣pε1d + ^(t) Θ (w(t + 1) - w(t))∣∣
+ ∣∣ √ε1d + V(t - 1) Θ (w(t) - w(t
Pε1d + ^(t) Θ (w(t + 1) - w(t))∣∣ ≤C (∣∣pε1d + ^(t) Θ (w(t + 1) - w(t))∣∣
+ ∣∣ √ε1d + V(t - 1) Θ (w(t) - w(t
Rearranging Eq. (52) leads to
茨-1
4√Cη(1 — βι)
(∣∣ Pε1d + V(t — 1) Θ (w(t) - w(t -
1))∣∣∣2
+
4Jε1d + ^(t - 1) Θ (w(t) - w(t
≤L(w(t)) + 4 √ηc1+-1βι) ∣∣ P ε1d+V(t -1) θ (w(t) - w(t - I))II
L(w(t+ 1)) +
石+1
4√η(1- βι)
4Jε1d + ^(t) Θ (w(t + 1)-
44
Under review as a conference paper at ICLR 2022
which further indicates
L(w(t)) +
√c +1
4 √η(i- βι)
4/ε1d + ν(t - 1) Θ (w(t) - w(t
≤2 L(w(t))2 +
茨+1
4√Cη(i - βι)
4/ε1d + ν(t - 1) Θ (w(t) - w(t
≤2C (1+4U+Li) QPε1d "⑴ θ (W(t +1)- w(t川2
+
√ε1d + ν(t - 1) Θ (w(t) - w(t
≤2C 1 +
√c + 1	) 4√η(1 — βι)
4√Cη(1 — βι)J	√c - 1
L(w(t)) +
茨+1
4√η(1 — βι)
Pε1d + ^(t — 1) Θ (w(t) — w(t — 1))U — (L(w(t + 1))
+4 U+-1/" ειd+"⑴ θ (w(t+1)-必川「)).
Denote ξ(t) as
W 4 Lw⑻ + 4"βι)U PIF^ θ …w(t - I))U2.
We then have
ξ(t)2 ≤ 2C 1 +
√c +1	ʌ
4√η(1 - βι)J
4 2Cη(1 - βι)
-√ - 1-
(ξ(t) - ξ(t + 1)),
which leads to
i.e.,
L(w(t)) = O
and
Pε1d + ν(t — 1) Θ (w(t) - w(t - 1))∣∣
DUe to Eq. (56), We further have ∣∣VL(w(t))k = O(t-1), which indicates
t
∣w(t)∣ ≤∣w(1)∣ +X∣w(s+1) -
s=1
t
t
≤∣w(1)∣ +
η
√ε(I- β)
βs-i∣VL(w(i))∣
s=1 i=1
t
≤kw(1)k + √=∩⅛2 X kVL(w(s))k = O(log(t)).
ε(1 - β)	1
Therefore, for any X ∈ T(S), we have hw(t), Xi = O(log(t)), which by ' is exponential-tailed leads
to '(hw(t), Xi) = Ω(t-1), and thus L(w(t)) = Θ(t-1). Also, since L(w(t)) = O(t-1), We have
hw(t), Xi = Ω(log(t)), which further leads to ∣∣w(t)∣ = Ω(log(t)), and thus ∣∣w(t)∣ = Θ(log(t)).
45
Under review as a conference paper at ICLR 2022
Finally, we have
t	t
YXβt-skVL(w(s))k = N Xβt-s	X '0(hw(s), Xi)X
s=1	s=1	X∈T (S)
t
≤-NXβt-s χ 'o(hw(s),Xi)≤
s=1	X∈T (S)
t
s
s=1
X
∖x∈T (S)
'0(hw(s), X))X, γ W
t
s=1
t
kγWk = km(t)k≤ Xβt-skVL(w(s))k,
s=1
which leads to km(t)k = Θ(t-1). Similarly, we have ν(t) = O(t-2), component-wisely. As
limt→∞ β1t = 0 and limt→∞ β2t = 0, we have
kW(t) -W(t- 1)k
m(t)
Pε1d + ^(t)
Θ(t-1).
≤

The proof is completed.
□
C.3 Bounding The Orthogonal Part
By Lemma 8, there exists a solution W as the solution of Eq. (12) with C2 = (i-β√ ∙ Define r(t)
as
r(t) 4 w(t) — log(t)W — W,	(59)
and we only need to prove kr(t)k is bounded over time. We then prove Lemma 7, providing an
equivalent condition of kr(t)k being bounded. As the GDM and SGDM case, we separate the proof
into two sub-lemmas.
Lemma 21. Let all conditions in Theorem 4 hold. Then, kr(t)k is bounded if and only if g(t) is
upper bounded.
Proof. Following the same routine as Lemma 2 and Lemma 17, we only need to prove
lim ∣∣(1 — βt-1)pε1d + ^(t - 1) Θ (w(t) — w(t — 1))∣∣ = 0,	(60)
and
∞
Xkr(T) — r(τ — 1), (1 — β[-1 )pε‰ + ^(τ — 1) Θ (W(T) — w(τ — 1))i∣ < ∞.	(61)
τ=2
As for Eq. (60), by Lemma 6, we have
∣∣(1 — βt 1)p ε1d + ν(t — 1)θ (W(t) — W(t — 1))∣∣
=O(t-1) = o(1).
As for Eq. (60), we have
Dr(T) — r(τ — 1), (1 — β[-1),ε1d + V(T — 1) Θ (W(T) — W(T — 1)))|
=(W(T) — W(T — 1) — log T-1W, (1 — βτ-1)pε1d + V(T — 1) Θ (W(T) — W(T — 1)))|
≤ (log t-JW, (1 — βι-1)pε1d + V(T — 1) Θ (w(t) — w(t — 1)))|
+ (1 — β1 -1) ∣∣ Pε1d + V(T - 1) Θ (w(t) — w(t — 1))∣∣
(=?)O(T-2),
where Eq. (?) is due to Lemma 6 and log(T-ɪ) = Θ(τ-1).
The proof is completed.	□
46
Under review as a conference paper at ICLR 2022
We conclude the proof of Theorem 4 by showing g(t) is upper bounded.
Lemma 22. Let all conditions in Theorem 4 hold. Then, g(t) is upper bounded.
Proof. g(t) is upper bounded is equivalent to P∞=1 g(t + 1) - g(t) < ∞, We then prove this lemma
by calculating g(t + 1) - g(t) directly.
g(t +I) - g(t)
=亨 ιιr(t+ι)k2 + √β⅛- Dr(t+ι), a- βi )Pε1d+νw Q (w(t+ι)- w(t))E
2	1 — βι
-(ɪ ∣∣r(t)k2+1 — β∖ Dr⑴，(1 - βt- I)Pε1d+风力一I) θ (W⑴-w(t - I))E)
-ι β1 & 〈r(t + 1) - r(t), (1 - β1 )√ε1d + ν(t) Θ (w(t + 1) - w(t)))
1	- βι
=√ε ι∣r(t+1) - r(t)|2+√β⅛r Dr⑴，(I - βt )√ε1d+巩t) θ (W(t+1) - W⑴)
2	1 - β ι ∖
-(1 - βt- 1 )√ε1d + ^(t - 1) Θ (w(t) - w(t - 1)),+ √ε(r(t + 1) - r(t), r(t))
=)(r(t), -(1 - βt)√ε1d + V(t) Θ (w(t + 1) - w(t))-占VL(w(t))^
+ √ε llr(t + 1) - r(t)|2 + √εhr(t + 1) - r⑴，r⑴)，
where Eq. (*) is due to a simple rearranging of the update rule of Adam, i.e.,
1 βa ((1 - βι)pε1d + ^(t) Θ (w(t + 1) - w(t)) - (1 - βt-1)pε1d + V(t - 1) Θ (w(t) - w(t - 1)))
1 — βι ∖	/
=----^ς-VL(w(t)) - (1 - β t)pε1d + v(t θ (w(t + I) - w(t)) ∙
1-β
On the one hand, as ∣∣r(t + 1) - r(t)∣ = ∣∣w(t + 1) - w(t) - log 号W∣∣ = O(t- 1),
t=1
< ∞.
On the other hand,
Gt)，Y - βt )√≡y θ (W(t+1) - W(t))-占 VL(Mt))
+ √ε<r(t + 1) - r(t), r(t))
r(t), -(1 - βt)√ε1d + ^(t) Θ (W(t + 1) - W(t)) - -ɪ-VL(W(t))
1-β
+ √ε(W(t + 1) - W(t) - log (t+1) W，r(t),
(r(t)，-(1 - βt)√ε1d + V(t) Θ (W(t + 1) - W(t)) + √ε(W(t + 1) - W(t
+ ∕r(t),-√ε log (t+1) W -
= O(βt + t-2) + ( r(t), -√ε log
η
1-β
t + 1'
VL(w(t))
W - ɪVL(W(t))),
1-β
where Eq. (•) is due to V(t) = O(t-2).
Furthermore, following exactly the same routine as Lemma 16, we have
X(r(t)，-√εlog (t-+1) W - 1-βVL(W(t)),< ∞∙
The proof is completed.
□
47
Under review as a conference paper at ICLR 2022
D Applications&Extensions
D. 1 Application to the mini-batch SGDM
This section provides formal description of the implicit bias of mini-batch SGDM and its corre-
sponding proof. To begin with, we would like to provide a formal definition of mini-batch SGDM.
Mini-batch SGDM differs from SGDM by applying sampling without replacement to obtain B(t) in
Eq. (1). Specifically, let K = N. For any T ≥ 0, we call time series {KT +1,…，KT + K} the
(T + 1)-th epoch, and during the T + 1-th epoch, the dataset S is randomly uniformly divided into K
parts {B(KT +1),…，B(KT + K)}, with UK=TKiZK+ι B(t) = S. The implicit bias of mini-batch
SGDM is then stated as the following theorem:
Theorem 6. Let Assumptions 1, 2, and 3. (S) hold. Let learning rate η be small enough, and
β ∈ [0,1).Then,for almost every dataset S, mini-batch SGDM satisfies w(t) 一 log(t)W is bounded
W
同∙
ast → ∞, andlimt→∞ ∣∣W(t)k
The without-replacement sampling method leads to the direction of every trajectory of mini-SGDM
converge to the max-margin solution, compared to the same conclusion holds for SGDM a.s.. We
prove the theorem following the same framework of GDM, by proceeding with two stages.
Stage I. The following lemma proves L(u(t)) is an Lyapunov function for mini-batch SGDM and
without the a.s. condition.
Lemma 23. Let all conditions in Theorem 6 hold. Then, we have
t
L(u(t + 1)) ≤ L(u(1)) — Ω(η) X ∣∣VL(w(s))k2.
s=1
Proof. By the Taylor Expansion of L(u(t + 1)) at u(t), we have
L(u(KT+T+1))
≤L(u(KT +1)) 一 η
VL(u(KT + 1)), XK VLB(t+KT)(w(t + KT))
Hn2 ^K
+^-2~ X LB(tiKT)(W(t + KT))
t=1
(62)
On the other hand, for any t ∈ {2,…，K}, we have
t KT+s
w(KT +1) - w(KT +1)= n X X βKT+s-'VLb(')(w('))
s=1 ∖ '=1	)
t	KT+s	t KT
=nX	X βkt+s-'vLb(')(w(')) + nX XβKT+s-'vLb(')(w(`))
s = 1 V=KT +1	) s=1 v=1	)
=nX 1 -1+VLB(KT+`)(w(KT +')) + nβ(I -βt) XβKT-'VLB(')(w('))
'=1 P	P '=1
£ 1 - βt-'+1
=*
t
VLB(KT +`) (W(K T + `)) - n X
+n
β(1 一 βt)
KT
EeKT-'vlb(')(w(')) + n£
'=1
'=1
'=1
1-β
1 — βt-'+1
一；---v一vlB(KT+`)(W(KT + I))
1-β
，t-'+1
vlb(kt+`)(W(KT + 1)),
1 - β
t
1 - β
48
Under review as a conference paper at ICLR 2022
which by η is small enough further indicates
IlW(KT +1) - w(KT +1)∣∣
t
≤η X
'=1
1 — βt-'+1	t 1 _ βt-'+1
—1-β- VLB(KT+`)(W(KT + ')) -	-1-β- VLB(KT+')(W(KT + 1))
+η
β(1- βt)
1 - β
KT
X βKTTVLB(')(w('))
2=1
t 1 _ βt-2+1
+ η E —1 _ β— VLB(KT+')(W(KT + 1))
'=1	- β
t	KT
=O(η) X IlW(KT + ') - w(KT + 1)∣∣ + O(η)	XβKT-IIVLB(`)(w(4)) ∣∣
'=2	V=1
+ O(η) ∣∣VL(w(KT +1))∣.
Applying the same analysis to ∣∣w(KT +1 - 1) - w(KT + 1)∣ recursively, we finally obtain
∣∣w(KT +1) - w(KT + 1)∣
≤O(η) (XβKT-'∣∣VLB(e)(w(4))" + O(η) IIVL(W(KT + 1))∣.	(63)
Applying Eq. (63) to the ∣∣VLB(')(w(2))∣∣ in Eq. (63) (∀' ∈ [1, KT]) iterative and choosing η to be
small enough, we further have
∣∣w(KT +1) - w(KT + 1)∣
≤O(η)
,βK(T- ∣∣VLB(K'+1)(w(K' +1))∣∣ + O(η) ∣VL(w(KT + 1))∣
=O(η) (X PKT- ∣∣VLBg+1)(w(K' +1))∣∣
v=0
Therefore,
K
SX VLB(t+KT) (W(t + KT))
t=1
K
X VLB(t+KT) (W ⑴)+ O
t=1
∕βK(T- ∣∣VLB(K'+1)
))
=K VL(w(t)) + O
,βK(TT) ∣∣VLB(K'+1)(w(K' +1))∣∣
(64)
Similarly, one can obtain
VL(u(KT +1))
=VL(w(KT +1)) + O (IHKT +1) - w(KT)k)
=VL(w(KT +1)) + O
,βK(T- ∣∣VLB(K'+1)
(W(K` +1)) ∣ ∣
))
(65)
49
Under review as a conference paper at ICLR 2022
Applying Eq. (64) and Eq. (65) back to the Taylor Expansion (Eq. (62)), we have
L(u(KT+T+1))
≤L(u(KT + 1)) — Ω(η) hVL(w(KT + 1)), VL(w(KT + 1)))
+ O 卜2 (XX PeK(T-' IlVLBg+i)(w(K' + 1))∣∣! j
≤L(u(KT + 1)) — Ω(η) hVL(w(KT + 1)), VL(W(KT + 1)))
+O
PeK(T-' IlVLBg+i)(w(K' +1))∣∣2)).
Summing the above inequality over T and setting η small enough leads to the conclusion.
The proof is completed.
□
D.2 Extension to the multi-class classification problem
Here we use several notations and lemmas from (Soudry et al., 2018). We define w = vec(W),
W = Vec(W), ei ∈ RC (i ∈ {1,…，C}) satisfying ®)j = δj, and Ai = e% 0 IdX, where IdX
is the identity matrix with dimension dX. We still consider the normalized data, i.e., kxk ≤ 1,
∀(x, y) ∈ S. Then, the individual loss of sample (x, y) can be then represented as
ehw,Ay xi
'(y, Wx) = log —c~~	.
C ehw,Aixi
i=1
Furthermore, the gradient of training error at W has the form
1C	1
VL(W) = N XSXPC 」(Aj-Ai)Xi(Ai-Ay)x.
(x,y)∈S i=1	j=1 e
and the Hessian matrix of L can be represented as
1	C PC ehw,(Aj -Ai)xi
HL(W) = N E E√j-------------------------G (Ai- Ay )x(4- 4)x)>,
N (x,y)∈S i=1 PjC=1 ehw,(Aj -Ai)xi
one can then easily verify all absolute value of the eigenvalues of HL(W) is no larger than 2, which
indicates L is 2-globally smooth.
On the other hand, the separable assumption leads tohW, (Ay — Ai)x) > 0, ∀y = i, which further
indicates
(VL(w), Wi > 0.
Let Y = k^k, following the similar routine as the binary case, we have for a random subset of S
sampled uniformly without replacement with size b, we have
2N
kVL(W)k2 ≤ EB(t)kVLβ(t)(W)k2 ≤ γb2kVL(W)k2.	(66)
Similarly, we have for any positive real series {at}tt2=t1,
t2
γ X a(t)kVL(W(t))k ≤
t=t1
t2
X a(t)VL(W(t))
t=t1
t2
≤ X a(t)kVL(W(t))k.
t=t1
(67)
The proofs of Stage I can then be obtained with Lyapunov functions unchanged and by replacing the
corresponding lemmas using Eq. (66) and Eq. (67).
As for the proofs of Stage II, the Lyapunov functions are still the same, while we only need to
prove the sum of(r(t), -IneVL(W(t)) 一 log t++1 W) (for GDM, hr(t),-凸VLβ(t)(W(t))-
50
Under review as a conference paper at ICLR 2022
Nt Pi.Xi∈T(B(t)∩Ss) vixii for SGDM, hr⑴，-√εlog (t++1 ) W - ι-ηβ VL(Mt))i for Adam). For
the multi-class case using GDM, We present the following lemma from (Soudry et al., 2018), while
the other two cases can be proved similarly:
Lemma 24 (Part of the proof of Lemma 20, (Soudry et al., 2018)). If hw(t), (Ay - Ai)xi → ∞ as
t → ∞, ∀(x, y) ∈ S and ∀i = y, we have the sum of hr(t), 一ι-ηβ VL(w(t)) — log t++1 W)is upper
bounded.
The proof of Theorem 5 is then completed.
E Experiments
This section collects several experiments supporting our theoretical results.
E.1 Experiments of linear model
E.1.1 Comparing the training behavior OF GD, GDM, and Adam (w/s)
The experiments in this section is designed to verify Theorem 2, i.e., with proper learning
rates, gradient descent with momentum converges to the max margin solution, which is the
same as gradient descent. We use the synthetic dataset as (Figure 1, (Soudry et al., 2018))
and run GD, GDM and Adam (w/s) over it with different learning rates η = 0.1, 0.01 and
different random seeds (for random initialization and random samples despite the support sets
{((1.5, 0.5), 1), ((0.5, 1.5), 1), ((一1.5, 一0.5), 一1), ((一1.5, 一0.5), 一1)}). Both the angle between
the output parameter and max margin solution and the training accuracy are plotted in Figure 1.
Specifically, we plot the results with learning rate (1). ηGD = ηGDM = ηAdam = 0.001, (2).
ηGD = ηGDM = ηAdam = 0.1. We plot the training accuracy and the angle between the output
parameter and the max margin solution for each setting respectively. The results are shown in Figure
1. The observations can be summarized as follows:
•	When learning rate is small enough (η = 0.1, 0.001), both GD, GDM, and Adam converge
to the max margin solution, which supports our theoretical results;
•	(Similarity between GD and GDM) The training behaviors of GD and GDM are highly
similar.
•	(The acceleration effect of Adam) Adam achieves smaller angle with the max margin
solution under the same number of iterations.
E.1.2 Comparing the training behavior of SGD and SGDM
We also run the stochastic optimizers (SGD and SGDM) on the same synthetic dataset as Figure 1.
The learning rate is same as Figure 1, namely 0.001 and 0.1. The results of the experiment are plotted
in Figure 2. It can be observed that when learning rate is small enough, SGD and SGDM have the
similar training behavior, both converging to the max margin solution.
E.1.3 Adam on ill-posed Dataset in (Soudry et al., 2018)
In Figure 3 of (Soudry et al., 2018), an ill-posed synthetic dataset is proposed to support the argument
"Adam does not converge to max margin solution", which contradicts to the theoretical results of
this paper. We re-conduct the experiment of Figure 3 in (Soudry et al., 2018) with the same ill-posed
synthetic dataset with different learning rates and different random seeds as Figure 3. Figure 3. (f)
is similar to Figure 3 in (Soudry et al., 2018), where with learning rate η = 0.1 and random seed 1,
the angle of GD to the max margin solution is smaller than Adam all the time. However, it can be
observed from the amplified figure that the angle of GD keeps still and above 0 all the time, meaning
that GD doesn’t converge to the max margin solution under this setting. However, the angle of Adam
to the max margin solution still keeps decreasing and it’s unreasonable to claim "Adam doesn’t
converge to the max margin solution" in this case (the same issue exists in Figure 3 in (Soudry et al.,
2018)). Also, as we mentioned at the beginning of this section, this dataset is ill-posed, which is due
to the imbalance between the two components of the data (for all data ((x1, x2), y) in the dataset,
51
Under review as a conference paper at ICLR 2022
(b) Comparison of Angle: η = 0.001, random seed
=1
(c)	Comparison of Accuracy: η = 0.001, random
seed = 2
(d)	Comparison of Angle: η = 0.001, random seed
=2
(e)	Comparison of Accuracy: η = 0.1, random seed
=1
(f)	Comparison of Angle: η = 0.1, random seed
=1
(g)	Comparison of Accuracy: η = 0.1, random seed
=2
(h)	Comparison of Angle: η = 0.1, random seed
=2
Figure 1:	Comparison of GD, GDM, and Adam on the synthetic dataset in (Soudry et al., 2018). In
(a-g). the GD curve coincides with the GDM curve.
52
Under review as a conference paper at ICLR 2022
0.001,
(a) Comparison of Accuracy: η
seed = 1
(b) Comparison of Angle: η = 0.001, random seed
=1
0.001, random
(c) Comparison of Accuracy: η
seed = 2
Iog(t)
(d) Comparison of Angle: η = 0.001, random seed
=2
(e)	Comparison of Accuracy: η = 0.1, random seed
=1
log(t)
(f)	Comparison of Angle: η = 0.1, random seed
=1
(g)	Comparison of Accuracy: η = 0.1, random seed
=2
Figure 2:	Comparison of SGD and SGDM on
(h) Comparison of Angle: η = 0.1, random seed
=2
the synthetic dataset in (Soudry et al., 2018).
53
Under review as a conference paper at ICLR 2022
|x1 | is always smaller than 2, while |x2 | is larger than 10 (and even larger than 30 despite two data in
the dataset)), which requires smaller learning rate. To tackle this problem, we need to tune down the
learning rate. By Figure 3. (b),(d) and Figure 4. (b), after scaling down the learning rate, both GD’s
angle and Adam’s angle keep decreasing.
E.2 Evidence in the deep neural networks
We conduct an experiment on the MNIST dataset using the four layer convolutional networks used
in (Lyu & Li, 2019; Wang et al., 2021) (first proposed by (Madry et al., 2018)) to verify whether
SGD and SGDM still behave similarly in (homogeneous) deep neural networks. The learning rates
of the optimizers are all set to be the default in Pytorch. The results can be seen in Figure 5. It can
be observed that (1). SGDM achieves similar test accuracy compared to SGD while (2). SGDM
converges faster than SGD.
54
Under review as a conference paper at ICLR 2022
(b) Comparison of Angle: η = 0.001, random seed
=1
(a) Comparison of Accuracy: η =
=1
0.1, random seed
AUE」？M
O 2	4	6	8	10
∣og(t)
(c) Comparison of Accuracy: η
seed = 2
(d) Comparison of Angle: η = 0.001, random seed
=2
0.001, random
——GD
---Adam
12	14
(e)	Comparison of Accuracy: η =
=1
(f)	Comparison of Angle: η = 0.1, random seed
=1
0.1, random seed
(g)	Comparison of Accuracy: η = 0.1, random seed
=2
(h)	Comparison of Angle: η = 0.1, random seed
=2
O O
6 4
O O O ŋ O
O 8 642
Figure 3:	Comparison of GD and Adam on the ill-posed synthetic dataset in (Soudry et al., 2018).
55
Under review as a conference paper at ICLR 2022
Figure 4: Comparison of GD and Adam on the ill-posed synthetic dataset in (Soudry et al., 2018)
(continue).
(b) Comparison of Angle: η = 0.001, random seed
=3
(a) Comparison of Training Accuracy
(b) Comparison of Test Accuracy
Figure 5: Comparison of SGD and SGDM on MNIST with four layer CNN.
56