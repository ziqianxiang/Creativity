Under review as a conference paper at ICLR 2022
Deep Q-Network with Proximal Iteration
Anonymous authors
Paper under double-blind review
Ab stract
We employ Proximal Iteration for value-function optimization in reinforcement
learning. Proximal Iteration is a computationally efficient technique that enables
us to bias the optimization procedure towards more desirable solutions. As a con-
crete application of Proximal Iteration in deep reinforcement learning, we endow
the objective function of the Deep Q-Network (DQN) agent with a proximal term
to ensure that the online-network component of DQN remains in the vicinity of
the target network. The resultant agent, which we call DQN with Proximal Itera-
tion, or DQNPro, exhibits significant improvements over the original DQN on the
Atari benchmark. Our results accentuate the power of employing sound optimiza-
tion techniques for deep reinforcement learning.
1	Introduction
A core competency of a reinforcement learning agent is the ability to learn in environments with
large state spaces, which is key to solving important applications such as robotics (Kober et al.,
2013), dialog systems (Williams et al., 2017), and large-scale games (Tesauro, 1994; Silver et al.,
2017). Recent breakthroughs in deep reinforcement learning have demonstrated that existing algo-
rithms such as Q-learning (Watkins and Dayan, 1992; Sutton and Barto, 2018) can surpass human-
level performance on large-scale problems when they are equipped with deep neural networks for
function approximation (Mnih et al., 2015).
Two key components of a gradient-based deep reinforcement learning agent are its objective func-
tion, and an optimization procedure. The optimization procedure takes estimates of the gradient of
the objective with respect to network parameters, and updates the network parameters accordingly.
In DQN (Mnih et al., 2015), for example, the objective function is the empirical expectation of the
one-step temporal difference (TD) error (Sutton, 1988) on a buffered set of environmental interac-
tions (Lin, 1992), and advanced variants of stochastic gradient descent are employed to update the
network to best minimize this objective function.
A fundamental difficulty in minimizing this objective function stems from operating in what is
known as the deadly triad (Tsitsiklis and van Roy, 1997; Sutton and Barto, 2018; van Hasselt et al.,
2018): Updates are performed (1) off-policy and in conjunction with (2) non-linear function approx-
imation and (3) bootstrapping. In the context of reinforcement learning, bootstrapping refers to the
dependence of the target of our gradient updates on the parameters of the neural network, which
is itself continuously updated during training. Employing bootstrapping in reinforcement learn-
ing stands in contrast to typical supervised-learning techniques, or even Monte-Carlo reinforcement
learning (Sutton and Barto, 2018), where the target of our gradient updates does not directly depend
on the parameters of the neural network.
Mnih et al. (2015) proposed a simple and intriguing approach to hedging against issues that arise
when operating in the deadly triad, specifically to use a target network for bootstrapping in value-
function optimization. In this case, the target network is updated periodically, and tracks the online
network with some delay. While this modification constituted a major step towards combating mis-
behavior in Q-learning (Lee and He, 2019; Kim et al., 2019; Zhang et al., 2021), extreme volatility in
the optimization procedure is still prevalent. To ease optimization, we can synchronize the target and
the online network less frequently, but doing so could be thought of as delaying policy improvement,
therefore making the ultimate performance sensitive to the period of synchronization selected (Kim
et al., 2019).
1
Under review as a conference paper at ICLR 2022
Our contribution is to employ Proximal Iteration to improve the stability of the optimization proce-
dure in DQN. In particular, by observing a synergy between DQN and Proximal Iteration, we endow
the objective function in DQN with a proximal term that ensures the parameters of the online-
network component of DQN remain in the vicinity of the parameters of the target network. This
novel combination adds no additional computational cost to the original DQN agent. We present
comprehensive experiments on Atari benchmarks (Bellemare et al., 2013) demonstrating that our
new algorithm, which we refer to as Deep Q-Network with Proximal Iteration or DQNPro, yields
significant improvements over its vanilla DQN counterpart, thus revealing the benefits of combining
deep RL algorithms with Proximal Iteration.
2	Background on Reinforcement Learning
Reinforcement learning (RL) is the study of the interaction between an environment and an artificial
agent that learns to maximize reward through experience. The Markov decision process (Puter-
man, 1994), or MDP, is used to mathematically define the RL problem. An MDP is specified by
the tuple hS, A, R, P, γi, where S is the set of states and A is the set of actions. The functions
R : S × A → R and P : S × A × S → [0, 1] denote the reward and transition dynamics of the
MDP. Finally, by discounting future rewards, γ formalizes the intuitive notion that short-term re-
wards are more valuable than those received later.
The goal in the RL problem is to learn a policy, a mapping from states to a probability distribution
over actions, π : S → P(A), that obtains high sums of future discounted rewards. An important
concept in RL is the state-action value function (Q-function). Formally, it denotes the expected
discounted sum of future rewards when taking a particular action a in state s and following policy π
thereafter:
Qπ (s, a) := ERt+1 + γRt+2 + γ2Rt+3 + . . . St = s, At = a, π .
Define Q? as the optimal value of a state-action pair: Q?(s, a) := maxπ Qπ(s, a). The Bellman
optimality operator T : RlSl×lAl → RlSl×lAl is defined as follows:
T (Q)(s, a) := R(s, a) + Xγ P(s, a, s0) maxQ(s0, a0) .
a
s0∈S
Q?(s, a) is the unique fixed-point of T, meaning Q? := T (Q?). The Bellman operator is at the
heart of many planning and RL algorithms such as Value Iteration (Bellman, 1957), which proceeds
as follows given an arbitrarily initialized Q0 :
Qk+1 - T(Qk) .
This iterative process is guaranteed to converge to the unique fixed point ofT, namely Q?, because
T is a γ-contraction in the infinity norm:
kT (Q) - T (Q0)k∞ ≤γkQ-Q0k∞ .
In large-scale settings, it is not tractable to learn a separate number for each state-action pair.
A common way to address this curse of dimensionality is to learn a representation of the Q
function in the form of a parameterized function approximator, such as a deep neural network:
Q(s, a; w) ≈ Q?(s, a), where w here denotes the parameters of the function approximator. No-
tice that the Bellman operator may no longer be a contraction in the presence of general function
approximation.
3	Theory
Define the Bregman divergence generated by the convex function F as follows:
DF(χ,y) = F(X)- F(y) - VF(y)>(χ - y),
where we assume that F is smooth in the sense that the gradient of F is non-expansive:
||VF (x) -VF(y)|| ≤ ||x -y|| .
2
Under review as a conference paper at ICLR 2022
We define the Bergman-smoothed Bellman operator, TD as follows:
TD (V) = arg min 1 ||T (V) - v0||2 + ɪ Df (v0 ,v).
v0 2	2c
Define v = TD (v) , We have:
V = T (v) + 21c (VF (V)-VF (V)),
We can then shoW:
||TD(V1) - TD(V2)||	=
≤
≤
This implies:
∣∣Vι 一 V2∣∣
||T(vi) + 21c (VF(vi) 一 VF(Vι)) 一 T侬)一 21- (VF侬)一 VF5))||
IIT (vι) — T (V2)∣∣ + ɪ ||VF (vι) — VF (V2)|| + ɪ ||VF (Vι) — VF (V2)∣∣
2c	2c
||T (VI)- T (V2)|| + 1- ||VF (VI)-VF (V2)|| + 1- ||V1 - v2||
2c 1
IITd(vi) - TD(v2)∣∣
≤
≤
≤
IIT(vι) - T(v2)∣∣ + 2ccIIVF(vi)—VF(v2)∣∣
||T (VI) - T (V2)" + 21c IIv1 一 v2 ii
JCYI YIIv1 - V2" + ʒɪf IIV1 一 V2”
2C 一 1	2C 一 1
2cY + 1II	II
ɪɪT M一。2”,
alloWing us to conclude that:
IITd(vi) - TD(v2)II≤ Y-+-^CM—V2II
2c - 1
Therefore, the operator TD is a YγC-2C -contraction so long as c > ɪ-1^ .
4 Proximal Operator
Consider the folloWing optimization problem:
minimizew h(x; w) ,	(1)
Where h is the objective function (e.g., standard temporal difference (TD) error in reinforcement
learning (5)), w denotes the learnable parameters (e.g., the parameters ofa Q-netWork), and x is the
input data (e.g., hs, a, r, s0i). If h is differentiable, then stochastic gradient descent (SGD), Which
proceeds as folloWs:
Wk — Wk-1 - αkVwh(x; Wk-i), k = 1, 2, 3,...	(2)
could be used to solve the optimization problem (1). Here, αk denotes the step size at iteration k,
and W0 is an arbitrary starting point1.
While SGD is Well-behaved asymptotically and under standard assumptions, it can perform reck-
lessly in the interim, leading into a situation Where the iterates get exponentially far from the solu-
tion (Moulines and Bach, 2011; Ryu and Boyd, 2014). This behavior can be particularly disruptive
in the context of RL Where sample efficiency is an important concern.
One simple and effective approach to stabilizing SGD is to augment the objective With a quadratic
regularizer to ensure that the solution at each iteration stays in the vicinity of the previous ones:
Wk = arg min h(w) + ɪ IlW — Wk-Ik2 ∙	(3)
w	2c
1To simplify the notation and Whenever it is clear from the context, We drop x from the notation, i.e.,
h(w) = h(x; w) and 7h(wk-∖) = Vh(x; Wk-ι).
3
Under review as a conference paper at ICLR 2022
The quadratic term is basically a proximal term that ensures the next iterate wk will not stray too far
from the previous iterate wk-1. Having completed iteration k, we replace the previous iterate wk-1
by its new value wk and then proceed to iteration k + 1.
To understand the behavior of this iterative process, called Proximal Iteration (Parikh and Boyd,
2014; Ryu and Boyd, 2014), we define the proximal operator:
Proxh(Wk):= arg min h(w) + ɪ ∣∣w - Wk-ik2 ,
w	2c
Proximal Iteration could be written as:
(4)
Wk J Proxh(Wk-ι) k = 1, 2, 3 ...
From the Banach fixed-Point theorem, we know that iteratively aPPlying Proxh eventually converges
to a unique fixed Point Provided that the oPerator is a contraction. Bauschke et al. (2012) have shown
that, if the function h is μ-strongly convex, then the operator Proxh is indeed a contraction:
∣Proxh(W) - Proxh (W0)∣ ≤
1
1 + μ
∣W - W0 ∣
and therefore iteratively aPPlying the oPerator would guarantee convergence.
What is the fixed-Point of this oPerator? It is Possible to show that it is the minimum of the function
h, i.e., arg minw h(W), the minimum of Problem (1), which we originally sought to solve (Bauschke
and Combettes, 2011).
Therefore, the intuitive way of thinking about the Proximal Iteration algorithm, which is sometimes
referred to as disaPPearing Tikhonov regularization (Parikh and Boyd, 2014), is as a PrinciPled
way to bias the oPtimization Path towards the Previous iterate using the quadratic-norm Penalty
without altering the final answer. In other words, as Proximal Iteration makes Progress, the Previous
iterate Wk-1 gets closer to W? = arg min h(W) in light of the contraction ProPerty, therefore the
contribution of the quadratic term vanishes to zero thus not biasing the asymPtotic solution. The
following remark sheds more light on the relationshiP between Proximal Iteration and trust-region
algorithms:
Remark 1 (Relationship between trust region and Proximal Iteration). Define the trust-region
problem as follows:
minimizew h(W)
s.t. ∣W - Wk-1∣2 ≤ ρ ,
Parikh and Boyd (2014) show that solving the above trust-region formulation is equivalent to a step
of Proximal Iteration under appropriate choices of ρ (above) and c (in prox). Therefore, iteratively
applying the two formulations leads to the same solution. (See their Section 3.4 for more details.)
The second remark highlights the strength of Proximal Iteration relative to SGD:
Remark 2 (The non-asymptotic advantage of Proximal Iteration over SGD). While the asymp-
totic performance of SGD and Proximal Iteration are analogous (Ryu and Boyd, 2014), they may
exhibit qualitatively different behavior in a non-asymptotic sense. More concretely, denote the step-
size of SGD and Proximal Iteration at iteration k by ak = C, and w? := arg minw h(w). For SGD,
we have (Bauschke and Combettes, 2011):
E[∣wk-w*k2 ] ≤O (exp(C^ + 1).
kk
The exponential dependence on C can indeed manifest itself as Ryu and Boyd (2014) demonstrate.
In contrast, for Proximal Iteration we have (Ryu and Boyd, 2014):
k
E ∣Wk - W? ∣22 ≤ ∣W0 - W? ∣22 + X αi .
i=1
While the iterates of SGD can get exponentially far from the optimal answer (Moulines and Bach,
2011; Ryu and Boyd, 2014), Proximal Iteration is fairly well-behaved in the interim. This property
is particularly valuable in the context of reinforcement learning where sample-efficiency, and not
just the asymptotic performance, is of utmost importance.
4
Under review as a conference paper at ICLR 2022
In the context of a parameterized Q-function in RL, commonly used loss functions are not convex;
however, several works have introduced convex loss via linear approximation. Lee and He (2019)
have shown that in linear policy evaluation the loss function is convex in the weights. Serrano
et al. (2021) introduced a convex loss function for control with linear approximation. These works
demonstrate that many useful convex minimization objectives exist for RL.
In the case of deep RL, the loss function is not convex, and few convergence results exist for most
state-of-the-art algorithms. Nevertheless, we follow standard practice and show that Proximal It-
eration achieves significant empirical improvements when applied to standard deep RL algorithms.
Furthermore, recent work provides convergence results for Proximal Iteration in specific non-convex
cases, such as when h(w) can be decomposed into a sum of a smooth non-convex function and non-
smooth convex function (Fukushima and Mine, 1981; J Reddi et al., 2016; Li and Li, 2018). These
recent results open the door for potential future developments toward proving convergence for Prox-
imal Iterations in RL, which would further strengthen its theoretical grounding.
5 Deep Q-Network with Proximal Iteration
In this section, we introduce a new deep RL algorithm by applying Proximal Iteration to DQN.
Recall that the original DQN agent employs two parameterized value functions: an online network
Q(s, a; w) that is updated at each step, and a target network Q(s, a; θ) that is used for bootstrapping
and is synchronized with the online network periodically. More formally, let tuples hs, a, r, s0i
denote the buffered environmental interactions of the RL agent. Define the following objective
function akin to DQN:
7 /	∖	/ -I ∕C∖U
h(w) := (1/2)Ehs,a,r,s0i
[(r + Y max Q(s0
a0; θ) - Q(s, a; w)2i .
(5)
Given a fixed target-network weight θ, in the original DQN, our desire is to find a setting of weights
w that minimizes h. We can do so by applying SGD, but as argued above, it can lead to reckless
updates of w. To hedge against this possibility, we can bias the online weights towards the previous
iterate. The target network is a natural choice for this purpose, motivating the following objective:
12
arg min h(w) + — ∣∣w 一 θ∣b = proxh(θ).
To solve this minimization problem, we can take multiple descent steps using the stochastic gradi-
ents of the objective. Thus, starting from the initial point w = θ, we perform multiple w updates
(specified by the period hyper-parameter) as follows:
w=θ
for i = 1 . . . period:
W — W 一 α(Vh(w) + ɪ(w - θ))	(6)
θ — w.
In the last step, akin to the original DQN, We synchronize the two networks by performing θ J w,
before moving to the following iteration. This iterative optimization proceeds until convergence is
obtained.
Observe that the online-network update (6) can equivalently be written as:
w J(1 一(ɑ∕c)) ∙ w + (α∕c) ∙ θ 一 αVh(w) .	(7)
Notice the intuitively appealing form of the update: We first compute a convex combination of θ
and w, based on the hyper-parameters α and c, then add the gradient term to arrive at the next iterate
of w. If w and θ are close, the convex combination is close to w itself and so this DQNPro update
would in effect perform an update similar to that of the original DQN. However, when the online
weight w strays too far from the previous target-network iterate, taking the convex combination
ensures that the online network gravitates towards the target network by default. In other words, the
gradient signal from minimizing the squared TD error (5) needs to be strong enough to compensate
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Deep Q-Network with Proximal Iteration (DQNPro)
1:	Initialize θ, N, period, replay buffer D, α, and c
2:	S J env.reset(), W J θ, numUpdates J 0
3:	repeat
4:	a 〜e-greedy(Q(s, ∙; W))
5:	s0, r J env.step(s, a)
6:	add hs, a, r, s0i to D
7:	if s0 is terminal then
8:	s J env.reset()
9:	end if
10:	for n in {1, . . . , N} do	. perform N online-network updates
11:	sample a mini-batch B = {hs, a, r, s0i} from D
Vh(W) JVw21B∣	X	(r + YmoaXQ(SO
hs,a,r,s0 i∈B
a0; θ) - Q(S, a; W)2
w J(1 — (α∕c)) ∙ W + (α∕c) ∙ θ — αVw h(w)
numUpdates J numUpdates + 1
12:	if numUpdates % period = 0 then
13:	θ J W
14:	end if
15:	end for
16:	until convergence
. update the target network
for the default gravitation towards θ. Notice also that the update rule includes the original DQN
update as a special case when c → ∞.
The pseudo-code for DQNPro is presented in Algorithm 1. The difference between DQN (Mnih
et al., 2015) and our DQNPro is minimal, and is highlighted in red. Note that our proposed method
adds almost no additional computational cost to the original DQN. While the particular form of
DQNPro presented in Algorithm 1 uses SGD as an optimizer, we note that the general idea be-
hind our method can be applied with other forms of first-order gradient-based optimizers such as
ADAM (Kingma and Ba, 2015), ADAGRAD (Duchi et al., 2011), etc.
6 Experiments
In this section, we first evaluate the DQNPro agent relative to the vanilla DQN counterpart on a
set of Atari-2600 benchmarks (Bellemare et al., 2013), and show that endowing the agent with the
proximal term can lead to significant improvements in overall performance. We next investigate
the utility of our proposed proximal term through ablation analyses and experiments. See also
Appendix A for a complete description of our experimental pipeline, as well as Appendix B, and
Appendix C for more results and analyses.
6.1	Setup
We used 40 games from the Atari-2600 benchmark suite (Bellemare et al., 2013) to conduct our
experimental evaluations. Following Machado et al. (2018) and Castro et al. (2018), our experiments
used sticky actions to inject stochasticity into the otherwise deterministic Atari-2600 emulator.2
In our experiments, we endow DQN (Mnih et al., 2015) and DDQN (van Hasselt et al., 2016) with
the proximal term and compare with the vanilla counterparts. Our training and evaluation protocols
and the hyper-parameter settings closely follow those of the Dopamine3 (Castro et al., 2018) base-
line, which is consistent with the existing literature. To report performance results, we measure the
2When sticky actions are enabled, the emulator will ignore the agent’s current action choice and execute the
agent’s previous action with small probability (Machado et al., 2018).
3https://github.com/google/dopamine
6
Under review as a conference paper at ICLR 2022
+ 500%
+ 100%
+ 10%
-10%
-500%
-100%
-500%
Figure 1: Final performance gain for DQNPro over vanilla DQN. Results are averaged over four
random seeds, and the y-axis is scaled logarithmically.
undiscounted sum of rewards obtained by the learned policy during evaluation. We further report
the learning curve for all experiments averaged across 4 random seeds in Appendix C. We reiterate
that we used the same network architecture as that of Mnih et al. (2015) for all experiments in this
paper and we also used the exact same hyper-parameters for all agents to ensure a sound and fair
comparison. More details are provided in Appendix A.
We emphasize that our DQNPro only adds a single hyper-parameter, namely c, to the original DQN.
We set the hyper-parameter c = 0.2 throughout all experiments with DQNPro and in all games.
We did a minimal random search on four games, namely Asterix, Breakout, Qbert, and Seaquest, to
tune this value. We also observed no need to tune c on a per-game basis, as c = 0.2 worked well
consistently for all games and the two algorithms demonstrating that obtaining good performance
does not hinge on per-game exhaustive search over c. We also observed that any value of c ≥ 0.1
resulted in improvements over vanilla DQN.
6.2	Results
The most important question to answer is whether endowing the DQN agent with the proximal term
can yield significant improvements over the original DQN. To answer this question in the affirmative,
our first result is a large-scale comparison between DQN and DQNPro.
Figure 1 shows a comparison between DQN and DQNPro in terms of the final performance. In
particular, for each game, we compute the performance difference between the final policy learned
by the DQN agent and the uniformly-random policy, i.e., (DQN - random), as well as (DQNPro
- random). Given these two values, we compute the relative improvement of the more performant
agent over the weaker one in percentile. Bars shown in red indicate the games in which we observed
better final performance for DQNPro relative to DQN, and bars in blue indicate the converse case.
The height of a bar denotes the magnitude of this improvement for the corresponding benchmark;
notice that the y-axis is scaled logarithmically.
It is clear that DQNPro dramatically improves upon DQN, and this improvement manifests itself
in all but 6 games tested. This improvement is particularly rewarding in light of the fact that the
implementation of the DQNPro agent is just minimally different than that of the DQN, and that
DQNPro adds almost no additional computational burden to the original DQN agent. We also show
learning curves on 4 Atari games (Asterix, Breakout, Gravitar, and SpaceInvaders) in Figure 2,
which demonstrate that DQNPro typically yields better interim performance as well. We defer to
Figure S4 in the Appendix for full learning curves on all games tested.
Can we combine DQNPro with some of the existing algorithmic improvements of DQN? One in-
teresting idea in this space is to bootstrap based on Q s, arg maxa Q(s, a; w); θ as opposed to
max Q(s, a; θ) during value-function optimization (van Hasselt et al., 2016). Referred to as Double
DQN, or DDQN, doing so can hedge against over-estimation issues that arise in Q-learning in light
of the convexity of the max operator (Thrun and Schwartz, 1993). Recall that DQNPro encourages
the online network to stay in the vicinity of the target network. As such, it would be interesting to
7
Under review as a conference paper at ICLR 2022
Figure 2:	Learning curves for DQN (blue) and DQNPro (red) on four Atari games. X-axis
indicates the number of steps from the environment used in training and Y-axis shows undiscounted
return (sum of rewards). See also Figure S4 for learning curves for all experimental environments.
O 0.5	1.0	1.5	2	2.5	3
Training Iterations ιθ7
Rainbow
---DQNPro (Ours)
——C51
----DQN (Our code)
DQN (Dopamine)
Figure 3:	A comparison between
DQNPro and various deep RL base-
lines. To compare the agents across
all games, we first normalize the per-
formance of all agents so we can av-
erage over all games meaningfully. To
this end, for each combination of al-
gorithm, game, and number of train-
ing iterations, we compute the perfor-
mance relative to the best policy learned
by Rainbow (Hessel et al., 2018). We
then average out this quantity across all
games.
see if we can still fruitfully combine DDQN and DQNPro since a successful combination hinges on
the presence of some discrepancy between the two networks.
In Figure S2, we answer this question affirmatively as well by comparing the final performance
of DDQN and DDQNPro on the same set of games tested in the previous experiment, where we
used the same values for all hyper-parameters including c = 0.2. Notice again, that the value of
c = 0.2 across all games and for both DQNPro and DDQNPro, demonstrating that this parameter
is easy to tune. (See Figure S3 for all learning curves.) Numerous other interesting improvements
for DQN exist in the RL literature (Hessel et al., 2018). Our goal was to show such combinations
are promising, and not to exhaustively test all of them, so we leave further exploration of other
combinations for future work.
Finally, it is valuable to evaluate the performance of DQNPro relative to the more recent base-
lines. In Figure 3, we provide a comparison between DQNPro, our own DQN implementation, and
the Dopamine (Castro et al., 2018) implementation for DQN, C51 (Bellemare et al., 2017), and
Rainbow (Hessel et al., 2018). First, observe that our DQN implementation obtains a performance
analogous to that of the DQN implementation from Dopamine, thus confirming the soundness of
our results. More importantly, our DQNPro algorithm is able to achieve more than 50 percent of
the improvement of Rainbow over the original DQN, and is also performing equally well relative
to C51. We find this result appealing, because DQNPro is remarkably simpler to understand and to
implement relative to baselines such as Rainbow.
6.3 Ablation Experiments and Further analyses
Effect of the proximal term. Notice that the purpose of our endowing the agent with the proximal
term was to keep the online network in the vicinity of the target network, so it would be natural to
ask if this property can be observed in practice. In Figure 4, we answer this question affirmatively
by plotting the magnitude of the update to the target network during synchronization. Notice that
we periodically synchronize online and target networks, so the proximity of the online and target
8
Under review as a conference paper at ICLR 2022
Wizardofwor
0.25	0.50	0.75	1.00
Training Iterations 1e7
0.25	0.50	0.75	1.00
Training Iterations 1e7
0.25	0.50	0.75	1.00
Training Iterations	1e7
0.25	0.50	0.75	1.0(
Training Iterations	1θ7
Figure 4: The effect of the proximal term on the magnitude of updates to the target network.
These results clearly demonstrate that our method has more success than DQN in keeping the online
network close to the target network.
Figure 5: A comparison between DQNPro and DQN with periodic (Top) and Polyak (Bottom)
updates for target network.
network should manifest itself in a low distance between two consecutive target networks. Indeed,
the results demonstrate the success of the proximal term in terms of obtaining the desired proximity
of online and target networks.
Sensitivity to target network update strategy. While using the proximal term leads to significant
improvements when combined with DQN/DDQN, one may still wonder if the advantage of DQNPro
over DQN is merely stemming from a poorly-chosen period hyper-parameter in the original DQN,
as opposed to a truly more stable optimization in DQNPro. To refute this hypothesis, we ran DQN
with various settings of the period hyper-parameter {2000, 4000, 8000, 12000}. This set included
the default value of the hyper-parameter (8000) from the original paper (Mnih et al., 2015), but also
covered a wider set of settings.
Additionally, we tried an alternative update strategy for the target network, referred to as Polyak
averaging, which was popularized in the context of continuous-action RL (Lillicrap et al., 2015),
and which proceeds by updating the target network as follows: θ — TW + (1 - T)θ. For this update
strategy, too, we tried different settings of the τ hyper-parameter, namely {0.05, 0.005, 0.0005},
which includes the value 0.005 used in numerous papers (Lillicrap et al., 2015; Fujimoto et al.,
2018; Asadi et al., 2021).
Figure 5 presents a comparison between DQNPro and DQN with periodic and Polyak target up-
dates for various hyper-parameter settings of period and T. It is clear that DQNPro is consistently
outperforming the two alternatives regardless of the specific values of period and T , thus clearly
demonstrating that the improvement is stemming from a more stable optimization procedure lead-
ing to a better interplay between the two networks.
9
Under review as a conference paper at ICLR 2022
7	Related Work
The introduction of the proximal operator could be traced back to the seminal work of Moreau (1962;
1965), Martinet (1970) and Rockafellar (1976), and the use of the proximal operators and related
algorithms has since expanded into many areas of science such as signal processing (Combettes and
Pesquet, 2009), statistics and machine learning (Beck and Teboulle, 2009; Polson et al., 2015; Reddi
et al., 2015), and convex optimization (Parikh and Boyd, 2014; Bertsekas, 2011b;a).
In the context of RL, Mahadevan et al. (2014) introduced a theory of proximal RL for deriving
convergent off-policy temporal difference algorithms with linear function approximation. One in-
triguing characteristic of their family of algorithms is that they perform updates in primal-dual space,
a property that was recently leveraged in a finite sample complexity analysis (Liu et al., 2020) for
the proximal counterparts of the gradient temporal-difference algorithm (Sutton et al., 2008). Prox-
imal operators also have appeared in recent work in the deep RL literature. For instance, in the
context of meta-learning, Fakoor et al. (2020b) used proximal operators during the adaption phase
to keep the new parameters close to the meta parameters. Similarly, Maggipinto et al. (2020) im-
proved TD3 (Fujimoto et al., 2018) by employing a stochastic proximal-point interpretation and
bootstrapping action value estimates for continuous control.
It is also worthwhile to note that the effect of the proximal term in our work is to ensure that the
online network remains in the vicinity of the target network (see also Remark 1), which is remi-
niscent of the use of trust regions in policy gradient-based methods (Schulman et al., 2015; 2017;
Wang et al., 2019; Fakoor et al., 2020a; Tomar et al., 2021). However, three factors differentiate our
work: we define the proximal term using the value function, not the policy, we enforce the proximal
term in the parameter space, as opposed to the function space, and we use the target network as the
previous iterate in our proximal definition.
Finally, existing work in RL showed the benefits of leveraging the structure of the RL problem dur-
ing optimization. For example, Precup and Sutton (1997) argued that the space of value functions
could better be understood as a manifold with an entropic metric, resulting in the exponentiated TD
algorithm. Mahadevan and Liu (2012) used mirror descent to penalize weights with large p-norms,
a biasing scheme that unfortunately changes the fixed-point of the resultant Bellman equation. Sim-
ilarly, van Seijen et al. (2019) advocated for performing Q-updates in a logarithmic space to combat
issues that arise in problems where the action-gap (Farahmand, 2011) can vary drastically on a state-
by-state basis; however, extra work is necessary in their approach to ensure that negative values can
be represented.
8	Conclusion and Future work
We showed a clear advantage in using Proximal Iteration in the context of value-function optimiza-
tion in deep reinforcement learning. Proximal Iteration ensures that the online network remains in
the vicinity of the target network, and shows robustness with respect to noise and stochasticity in
reinforcement learning. Several improvements to proximal methods exist, such as the acceleration
algorithm (Nesterov, 1983; Li and Lin, 2015), as well as using other proximal terms (Combettes and
Pesquet, 2009), which we leave for future work. Our results demonstrate the rewarding nature of
developing principled optimization techniques for deep reinforcement learning.
10
Under review as a conference paper at ICLR 2022
References
K. Asadi, N. Parikh, R. E. Parr, G. D. Konidaris, and M. L. Littman. Deep radial-basis value
functions for continuous control. In AAAI Conference on Arficial Intelligence, 2021.
H. H. BaUsChke and P L. Combettes. Convex analysis and monotone operator theory in Hilbert
spaces. 2011.
H. H. Bauschke, S. M. Moffat, and X. Wang. Firmly nonexpansive mappings and maximally mono-
tone operators: correspondence and duality. Set-ValUed and Variational AnaIySis, 2012.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-
lems. SIAM JOUrnal on Imaging Sciences, 20O9.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. JOUrnal of ArtifiCiaI Intelligence ReSearch, 2013.
M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning.
In InternatiOnal COnferenCe on MaChine Learning, 2017.
R. E. Bellman. DynamiC Programming. 1957.
D. P. Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimization:
A survey. OPtimizatiOn for MaChine Learning, 2011a.
D. P. Bertsekas. Incremental proximal methods for large scale convex optimization. MathematiCal
Programming, 2011b.
P. S. Castro, S. Moitra, C. Gelada, S. Kumar, and M. G. Bellemare. Dopamine: A Research Frame-
work for Deep Reinforcement Learning. 2018.
P. L. Combettes and J.-C. Pesquet. Proximal Splitting Methods in Signal Processing. Fixed-POint
algorithms for inverse problems in SCienCe and engineering, 2009.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. JOUrnal OfMaChine Learning ReSearch, 2011.
R.	Fakoor, P. Chaudhari, and A. J. Smola. P3O: Policy-on policy-off policy optimization. In
COnferenCe on UnCertainty in ArtifiCial Intelligence, 2020a.
R. Fakoor, P. Chaudhari, S. Soatto, and A. J. Smola. Meta-Q-learning. In InternatiOnal COnferenCe
on Learning RePreSentations, 2020b.
A. Farahmand. Action-gap phenomenon in reinforcement learning. AdVanCeS in NeUral InfOrmatiOn
PrOCeSSing Systems, 2011.
S.	Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic meth-
ods. In InternatiOnal COnferenCe on MaChine Learning, 2018.
M. Fukushima and H. Mine. A generalized proximal point algorithm for certain non-convex mini-
mization problems. International JOUrnal of SyStemS Science, 1981.
M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,
M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. In
AAAI COnferenCe on ArtifiCiaI Intelligence, 2018.
S.	J Reddi, S. Sra, B. Poczos, and A. J. Smola. Proximal stochastic methods for nonsmooth noncon-
vex finite-sum optimization. AdVanCeS in NeUraI InfOrmatiOn PrOCeSSing Systems, 2016.
S.	Kim, K. Asadi, M. Littman, and G. Konidaris. Deepmellow: removing the need for a target
network in deep q-learning. In InternatiOnal Joint COnferenCe on ArtifiCial Intelligence, 2019.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In InternatiOnal COnferenCe
on Learning RePreSentations, 2015.
11
Under review as a conference paper at ICLR 2022
J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. International
JoUrnaI of Robotics Research, 2013.
D. Lee and N. He. Target-based temporal-difference learning. In International Conference on
Machine Learning, 2019.
H. Li and Z. Lin. Accelerated proximal gradient methods for nonconvex programming. Advances
in neural information processing systems, 2015.
Z. Li and J. Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimiza-
tion. In AdvanCeS in NeUral InfOrmatiOn PrOCeSSing Systems, 2018.
T.	P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.
Continuous control with deep reinforcement learning. In InternatiOnal COnferenCe on Learning
RePreSentations, 2015.
L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
MaChine Iearning, 1992.
B.	Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik. Finite-sample analysis of proximal
gradient TD algorithms. In COnferenCe on UnCertainty in ArtifiCial Intelligence, 2020.
M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting
the arcade learning environment: Evaluation protocols and open problems for general agents.
JOUrnal of ArtifiCiaI Intelligence ReSearch, 2018.
M. Maggipinto, G. A. Susto, and P. Chaudhari. Proximal deterministic policy gradient. In
InternatiOnal COnferenCe on Intelligent Robots and Systems, 2020.
S. Mahadevan and B. Liu. Sparse q-learning with mirror descent. In COnferenCe on UnCertainty in
ArtifiCiaI Intelligence, 2012.
S. Mahadevan, B. Liu, P. Thomas, W. Dabney, S. Giguere, N. Jacek, I. Gemp, and J. Liu. Proximal
Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces.
arXiv, 2014.
B. Martinet. Regularisation, d'ineqUatiOnS variationelles par approximations succesives. ReVUe
FranCaiSe d'informatique et de ReCherChe Operationelle, 1970.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 2015.
J. J. Moreau. Fonctions convexes duales et points proximaux dans un espace hilbertien.
COmPteS rendus hebdomadaires des SeanceS de l'ACademie des sciences, 1962.
J. J. Moreau. Proximite et dualite dans un espace hilbertien. Bulletin de la SOCiete MathematiqUe
de France, 1965.
E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for ma-
chine learning. In AdVanCeS in NeUral InfOrmatiOn PrOCeSSing Systems, 2011.
Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence
o (1∕k^ 2). In Doklady an USSR, 1983.
N. Parikh and S. Boyd. proximal algorithms. FOUndatiOnS and TrendS in optimization, 2014.
N. G. Polson, J. G. Scott, and B. T. Willard. Proximal algorithms in statistics and machine learning.
StatiStiCaI Science, 2015.
D. Precup and R. S. Sutton. Exponentiated gradient methods for reinforcement learning. In
InternatiOnal COnferenCe on MaChine Learning, 1997.
M. L. Puterman. MarkOV DeCiSiOn Processes: DiSCrete StOChaStiC DynamiC Programming. 1994.
12
Under review as a conference paper at ICLR 2022
S. Reddi, B. Poczos, and A. Smola. Doubly robust covariate shift correction. In AAAI Conference
on Arficial Intelligence, 2015.
R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM JoUmal on Control
and Optimization, 1976.
E. K. Ryu and S. Boyd. Stochastic proximal iteration: a non-asymptotic improvement upon stochas-
tic gradient descent. 2014.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
International COnferenCe on machine Iearning, 2015.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv, 2017.
J. B. Serrano, S. Curi, A. Krause, and G. Neu. Logistic Q-learning. In International Conference on
ArtifiCial IntelligenCe and StatiStics, 2021.
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 2017.
R. S. Sutton. Learning to predict by the methods of temporal differences. MaChine Iearning, 1988.
R. S. Sutton and A. G. Barto. ReinfOrCement Iearning: An introduction. 2018.
R.	S. Sutton, C. Szepesvari, and H. R. Maei. A convergent O(n) temporal-difference algorithm
for off-policy learning with linear function approximation. In AdvanceS in Neural InfOrmatiOn
PrOCeSSing Systems, 2008.
G. Tesauro. TD-gammon, a self-teaching backgammon program, achieves master-level play. NeUral
computation, 1994.
S.	Thrun and A. Schwartz. Issues in using function approximation for reinforcement learning. In
PrOCeedingS of the FOUrth COnneCtiOniSt MOdelS SUmmer School, 1993.
M. Tomar, L. Shani, Y. Efroni, and M. Ghavamzadeh. Mirror descent policy optimization, 2021.
J. N. Tsitsiklis and B. van Roy. An analysis of temporal-difference learning with function approxi-
mation. IEEE transactions on automatic control, 1997.
H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In
Thirtieth AAAI COnferenCe on ArtifiCial Intelligence, 2016.
H. van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, and J. Modayil. Deep reinforcement
learning and the deadly triad. arXiv, 2018.
H. van Seijen, M. Fatemi, and A. Tavakoli. Using a logarithmic mapping to enable lower discount
factors in reinforcement learning. AdVanCeS in NeUral InfOrmatiOn Processing Systems, 2019.
Y. Wang, H. He, X. Tan, and Y. Gan. Trust region-guided proximal policy optimization. In AdVanceS
in NeUral InfOrmatiOn PrOCeSSing Systems, 2019.
C. J. Watkins and P. Dayan. Q-learning. MaChine learning, 1992.
J. D. Williams, K. A. Atui, and G. Zweig. Hybrid code networks: practical and efficient end-to-
end dialog control with supervised and reinforcement learning. In ASSOCiatiOn for COmPUtatiOnal
LingUiStics, 2017.
S. Zhang, H. Yao, and S. Whiteson. Breaking the deadly triad with a target network. arXiv, 2021.
13
Under review as a conference paper at ICLR 2022
Appendix: Deep Q-Network with Proximal Iteration
A Implementation Details
Table S1 and Table S2 show hyper-parameters, computing infrastructure, and libraries used for the
experiments in this paper for all games tested. Our training and evaluation protocols and the hyper-
parameter settings closely follow those of the Dopamine4 (Castro et al., 2018) baseline. To report
performance results, we measured the undiscounted sum of rewards obtained by the learned policy
during evaluation.
DQN and DDQN hyper-parameters (shared)
Replay Buffer size Target update period Max steps per episode Evaluation frequency Batch Size Update period Number of frame skip Number of episodes to evaluate Update horizon e-greedy (training time) e-greedy (evaluation time) e-greedy decay period Burn-in period / Min replay size Learning rate Discount factor (Y) Total number of iterations Sticky actions Optimizer Network Architecture Random Seeds	200000 8000 27000 10000 64 4 4 2 1 0.01 0.001 250000 20000 10-4 0.99 3 X 107 True Adam (Kingma and Ba, 2015) Nature DQN network (Mnih et al., 2015) 	{0,1,2,3}	
Our DQNPro and DDQNPro hyper-parameter	
_C			0.2	
Table S1: Hyper-parameters used for all methods for all 40 games of Atari-2600 benchmarks.
All results reported in our paper are averages over repeated runs initialized with each of the random
seeds listed above and run for the listed number of episodes.
Computing Infrastructure	
Machine Type	AWS EC2 - p2.16xlarge
GPU Family	Tesla K80
CPU Family	Intel Xeon 2.30GHZ
CUDA Version	11.0
NVIDIA-Driver	450.80.02	
Library Version	
Python	3.8.5
Numpy	1.20.1
Gym	0.18.0
Pytorch		180	
Table S2: Computing infrastructure and software libraries used in all experiments in this pa-
per.
4https://github.com/google/dopamine
14
Under review as a conference paper at ICLR 2022
Figure S1:	Learning curves for DQN with prioritized experience replay (PER) (gary) and
DQNPro (red) on 15 Atari games. X-axis indicates the number of steps from the environment
used in training and Y-axis shows average undiscounted return.

+ 500%-
+ 1OO%-
+ 1O%-
0%
-ιo%-
+ 500%
+ 100%
+ 10%
0%
-10%
Figure S2:	Performance gain for DDQNPro over the original DDQN in term of the final per-
formance. Results are averaged over four random seeds.The y-axis is scaled logarithmically.
B Additional Experiments
Prioritized Experience Replay (PER). In the standard DQN (Mnih et al., 2015), data are uni-
formly sampled from the reply buffer regardless of their importance. To further improve the perfor-
mance of DQN, Schaul et al. (2016) proposed an effective way, called prioritized experience replay,
to prioritize samples that are more conducive to sample-efficient learning. To further evaluate and
situate DQNPro, we compare DQNPro using standard replay buffer against DQN with prioritized
experience replay (called PER) on 15 Atari games. As Figure S1 shows our method outperforms
PER on the majority of these 15 games. These results provide another data point that our method is
a more effective approach for improving the performance of DQN while adding negligible compu-
tation cost relative to PER.
C Learning curves
In this section, we provide all learning curves for the experiments described in Section 6.2 of the
main text. Specifically, Figure S4 shows learning curves for DQN and DQNPro and Figure S2 and
Figure S3 show results comparing DDQNPro against DDQN for 40 Atari-2600 games.
15
Under review as a conference paper at ICLR 2022
Figure S3:	Learning curves for DDQN (green) and DDQNPro (orange) on 40 Atari games. X-
axis indicates the number of steps from the environment used in training and Y-axis shows average
undiscounted return.
16
Under review as a conference paper at ICLR 2022
Figure S4:	Learning curves for DQN (blue) and DQNPro (red) on 40 Atari games. X-axis indi-
cates the number of steps from the environment used in training and Y-axis shows average undis-
counted return
17
Under review as a conference paper at ICLR 2022
▽h(W)J Vw 丽
Figure S5:	A comparison between DQN, DQNPro in parameter space, and DQN in function space.
It would be natural to ask if adding the proximal term in the function space can yield similar improve-
ments. To test this hypothesis, we augment the loss function in DQN by performing the following
update given a batch:
X	(r+γmaxQ(s0, a ; θ)-Q(s, a; w))2 + ɪ (Q(s, a; W)-Q(s, a; θ))2
a0	2c
hs,a,r,s0 i∈B
This will ensure that we still update based on the original loss, but keep the two networks close
in the function space. Results shown in Figure S5 compares this idea with the original DQNPro
performed in the parameter space. We observe that adding the proximal term in the function space is
not nearly as effective as the parameter space. We conjecture that in this case we can only maintain
closeness on samples drawn from the batch. In other words, the function are still allowed to differ
significantly in all areas of the state-action space but those used for training. This ultimately results
in inferior performance. Additional work may be needed to perform proximal gradient steps in the
function space, such as through using natural gradients.
18
Under review as a conference paper at ICLR 2022
Additional References for the Appendix
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations, 2015.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 2015.
T. SchaUL J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In International
Conference on Learning Representations, 2016.
19