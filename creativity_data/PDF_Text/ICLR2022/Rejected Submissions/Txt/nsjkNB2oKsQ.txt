Under review as a conference paper at ICLR 2022
Off-Policy Reinforcement Learning with
Delayed Rewards
Anonymous authors
Paper under double-blind review
Ab stract
We study deep reinforcement learning (RL) algorithms with delayed rewards. In
many real-world tasks, instant rewards are often not readily accessible or even de-
fined immediately after the agent performs actions. In this work, we first formally
define the environment with delayed rewards and discuss the challenges raised due
to the non-Markovian nature of such environments. Then, we introduce a general
off-policy RL framework with a new Q-function formulation that can handle the
delayed rewards with theoretical convergence guarantees. For practical tasks with
high dimensional state spaces, we further introduce the HC-decomposition rule
of the Q-function in our framework which naturally leads to an approximation
scheme that helps boost the training efficiency and stability. We finally conduct
extensive experiments to demonstrate the superior performance of our algorithms
over the existing work and their variants.
1	Introduction
Deep reinforcement learning (RL) aims at maximizing the cumulative reward of a MDP. To apply
RL algorithms, the reward has to be given at every state-action pair in general (i.e., r(s, a)). With
a good and high quality reward function, RL can achieve remarkable performance, e.g. AlphaGo
Zero for Go (Silver et al., 2017), DQN(Mnih et al., 2015) for Atari, SAC (Haarnoja et al., 2018b)
for robot control e.t.c. Recently, RL has been applied in many other real world settings beyond
games and locomotion control. This includes industrial process control (Hein et al., 2017), traffic
optimization (Gong et al., 2019; Lin et al., 2018), molecular design (Olivecrona et al., 2017) and
resource allocation (Xu et al., 2018). However, in many of these real-world scenarios, Markovian
and instant per-step rewards are hard or impossible to obtain or even clearly defined. In practice,
it becomes more reasonable to use a delayed reward of several consecutive steps as feedback. For
example, in traffic congestion reduction (Gong et al., 2019), the amount of decreased congestion
for a single traffic light switch is hard to define in practice while it is more adequate to use the
average routing time consumed for the vehicles as the feedback. The latter is a delayed reward
which can only be obtained after the vehicles have passed the congestion (long after a single switch).
In molecular design, only the molecule formed after the final operation can provide a meaningful
evaluation for the whole process (Olivecrona et al., 2017). In locomotion control where the feedback
is generated by the interaction with the large environment, it is usually the case that the frequency of
the rewards generated from the environment’s sensors is much lower than the frequency of the robot
control, thus the rewards are given only every so often.
Despite the importance and prevalence of the delayed reward setting, very few previous research in
RL has been focusing on problems with non-Markovian properties and delayed rewards. Besides,
current RL algorithms lack theoretical guarantee under non-Markovian rewards and perform unsat-
isfactorily in practice (Gangwani et al., 2020). Thus, in this paper, our goal is to better understand
the properties of RL with delayed rewards and introduce a new algorithm that can handle the de-
layed reward both theoretically and practically. A key to our approach lies in the definition of the
past-invariant delayed reward MDPs. Based on this definition, theoretical properties and algorith-
mic implications are discussed to motivate the design of a new practical algorithm, which explicitly
decompose the value function into components of both historical (H) and current (C) step infor-
mation. We also propose a number of ways to approximate such HC decomposition, which can be
readily incorporated into existing off-policy learning algorithms. Experiments demonstrate that such
approximation can improve training efficiency and robustness when rewards are delayed.
1
Under review as a conference paper at ICLR 2022
2	Problem Formulation
In order to characterize the delayed and non-Markovian reward signals, we introduce the Delayed
Reward Markov Decision Process (DRMDP). In this work, we focus on DRMDP that satisfies the
Past-Invariant (PI) condition, which is satisfied in many real-world settings with delayed rewards.
2.1	Past-Invariant Delayed Reward MDPs
In DRMDP, the transition of the environment is still Markovian and the agent can observe and
interact with the environment instantly. However, rewards may be non-Markovian and delayed and
are observed only once every few steps. More specifically, the time steps are divided into consecutive
signal intervals of random lengths, and the reward signal generated during a signal interval may
depend on the state-action sequence and is observed only at the end of the interval. We formally
define DRMDP as follows.
Definition 1 (DRMDP). A Delayed Reward Markov Decision Process M = (S, A, p, qn , r, γ) is
described by the following parameters.
1.	The state and action spaces are S and A respectively.
2.	The Markov transition function isp(s0|s, a) for each (s, a) ∈ S × A; the initial state distribution
is p(s0).
3.	The signal interval length is distributed according to qn(∙), i.e., for the i-th signal interval, its
length n is independently drawnfrom qn(∙).
4.	The reward function r defines the expected reward generated for each signal interval; suppose
Ti = Tt:t+ni = (S, a)t:t+ni = ((St, at), . . . (st+n-1, at+ni - 1 )) is the State-action Sequence
during the i-th signal interval of length ni, then the expected reward for this interval is r(τi).
5.	The reward discount factor is γ.
In this work, we focus on the infinite-horizon DRMDP. We use (τ, n) = (τi, ni)i∈{1,2,3,... } to
denote a trajectory, where τi is the state-action sequence during the i-th signal interval and ni is
the corresponding length. We also let ti be first time step of the i-th signal interval, i.e., ti =
Pij-=11 nj . Note that the reward r(τi) is revealed at time ti + ni - 1 = ti+1 - 1. We finally define
the discounted cumulative reward of the trajectory (τ, n) by R(τ, n) := Pi∞=1 γti+1-1r(τi). The
objective for DRMDP is to learn a policy π that maximized the expected discounted cumulative
reward J(∏) := E(T,n)〜∏ [R(τ,n)].
Augmented Policy Class. In DRMDP, the Markov policy class {π = ∏(at∣st)} might not achieve
satisfactory performance because of the non-Markovian nature of the reward function. Theoretically
speaking, We need a more general policy class ∏τ = {∏ = n(at|T行：t ◦ st)} (i is the index of the
signal interval that t belongs to). A formal statement is included in Appendix B. Unfortunately,
policy optimization in such an exponentially large space Πτ is hard in practice, especially for high-
dimensional problems. Thus, as a trade-off, we consider the policy class ∏s = {∏ = ∏(at∣st,t -
ti)}, which resembles the traditional Markov policy class, but is augmented with an extra parameter
indicting the relative index of the current time step in the signal interval. Moreover, we focus on the
PI-DRMDP problems, in which our algorithmic framework has theoretical guarantees.
Definition 2 (PI-DRMDP). A Past-Invariant Delayed Reward Markov Decision Process is a DR-
MDP M = (S, A, p, qn , r, γ ) whose reward function r satisfies the following Past-Invariant (PI)
condition: for any two trajectory segments τ1 and τ2 of the same length, and for any two equal-
length trajectory segments τ10 and τ20 such that the concatenated trajectories τa ◦ τb0 are feasible
under the transition dynamics pfor all a, b ∈ {1, 2}, it holds that
r(τι ◦ τ1) >r(τι ◦ τ∖) Q⇒ r(τ2 ◦ τ0) > r(τ ◦ τ'2).
Roughly speaking, in PI-DRMDP, the relative credit for different actions at each St only depends on
the experience in the future and is invariant of the past. This property may relieve the agent from
considering the past experience for decision making. A simple example of r with PI condition is
r(Tt：t+n) = Pt+n-1 r(si, ai), where r is a per-step reward function. This kind of tasks is studied
in many previous work (Zheng et al., 2018; Oh et al., 2018; Klissarov & Precup, 2020). We refer
this kind of reward functions as the sum-form.
2
Under review as a conference paper at ICLR 2022
In the worst case, there exists some bizarre reward design in PI-DRMDPs that still requires the
optimal policy to take history information into consideration (see Appendix B). Despite the sub-
optimality issue, we will show that our algorithmic framework guarantees policy improvement in
Πs, which is sufficient to support the design of actor-critic methods. Consequently, we are able
to put forward a practical RL algorithm that can achieve the SOTA performance in delayed-reward
tasks. The overall contribution of our work is summarized in Table 1 and we leave the optimality of
the DRMDP problems as future works.
Table 1: Contribution of our algorithm in different problem settings. Notice that MDP is a degraded
case of DRMDP, which has signal interval length fixed as 1.
	Policy Improvement	Policy Optimality
MDP	Yes	Yes
PI-DRMDP	Yes	No
DRMDP	No	No
General Reward Function. In Definition 1, we define the reward as a function of the state-action
sequence of its signal interval. In general, we may allow reward functions with longer inputs which
overlap with the previous signal intervals, e.g., maximal overlapping of C steps, r(τti-c-ti+m). The
theoretical analysis in Section 3.1 and the empirical method in Section 3.2 can be directly extended
to this general reward function. We provide detailed discussions in Appendix B for the general
definition while we only consider c = 0 in the main text for the simplicity of the exposition.
2.2	Off-Policy RL in PI-DRMDP
Deep Reinforcement Learning has achieved great success in solving high-dimensional MDPs prob-
lems, among which the off-policy actor-critic algorithms SAC (Haarnoja et al., 2018a) and TD3
(Fujimoto et al., 2018) are the most widely used ones. However, since the rewards are delayed and
non-Markovian in PI-DRMDP, directly applying these SOTA off-policy RL algorithms faces many
challenges that degrade the learning performance. In this subsection, we briefly discuss the problems
that arise in critic learning based on TD3 and similar problems also exist for SAC.
First, in PI-DRMDP, value evaluation with off-policy samples brings in off-policy bias. TD3 mini-
mizes over φ w.r.t.
Lφ = ED [(Rt + γQφ(st+ι, at+ι) — Qφ(st, at))2],	⑴
where (st, at, Rt, st+1) is sampled from the replay buffer D, Rt = r(τi) ift = ti+1 - 1 and i = i(t)
is the index of the reward interval that t belongs to, and Rt = 0 otherwise. Qφ represents the target
value of the next state and a0t+1 is sampled from the smoothed version of the policy π. Furthermore,
in practical implementation, samples in the replay buffer D are not sampled from a single behavior
policy β as in traditional off-policy RL (Sutton & Barto, 2018). Instead, the behavior policy changes
as the policy gets updated. Thus, we assume that samples are collected from a sequence of behavior
policies β = {βk}kK=1.
In the delayed reward setting, since the reward r(τi) depends on the trajectory of the whole signal
interval (i.e., τi) instead of a single step, the function Qφ(st, at) learned via Eq. (1) will also have
to depend on the trajectory in the signal interval UPto time t (i.e., TtiQ rather than the single state-
action pair at time t. Since the samples are collected under a sequence of behavior polices β,
different behavior policy employed at state St may lead to different distribution over Tti：t in Eq.(1).
Consequently, Qφ(st, at) will be affected by this discrepancy and may fail to assign the accurate
expected reward for the current policy. Please refer to Appendix B for detailed discussions.
Second, in addition to the issue resulted from off-policy samples, Qφ(st, at) learned in Eq. (1) with
on-policy samples in D may still be problematic. We formally state this problem via a simple sum-
form PI-DRMDP in Appendix B whose optimal policy is in Πs . In this example, the fix point of
Eq. (1) fails to assign the actual credit and thus misleads policy iteration even when the pre-update
policy is already the optimal. We refer this problem as the fixed point bias.
Last but not least, the critic learning via Eq. (1) would suffer a relative large variance. Since Rt
varies between 0 and r(Ti), minimization of TD-error (Lφ) with a mini-batch of data has large
3
Under review as a conference paper at ICLR 2022
variance. As a result, the approximated critic will be noisy and relative unstable. This will further
effect the policy gradients in Eq. (2).
VθJ(∏θ) = ED VatQφ(st,at)∣	Vθ∏θ(St)	⑵
πθ(st)
To sum up, directly applying SOTA off-policy algorithms in PI-DRMDP will suffer from multiple
problems (off-policy bias, fixed point bias, large traning noise, etc). Indeed, these problems result
in a severe decline in performance even in the simplest task when n = 5 (Gangwani et al., 2020).
3	Method
In this section, we first propose a novel definition of the Q-function (in contrast to the original Q-
function) and accordingly design a new off-policy RL algorithm for PI-DRMDP tasks. This method
has better theoretical guarantees in both critic learning and policy update. We then introduce a HC-
decomposition framework for the proposed Q-function, which leads to easier optimization and better
learning stability in practice.
3.1	THE NEW Q-FUNCTION AND ITS THEORETICAL GUARANTEES
Since the non-Markov rewards make the original definition of Q-function ambiguous, we instead
define the following new Q-function for PI-DRMDP tasks.
∞
Qn (Tti:t+。：= E(τ,n)〜π X γtj + 1-t-1r(Tj )∣入t+1	,	⑶
j=i
The new Q-function is defined over the trajectory segments Tti:t+1, including all previous steps in
st’s signal interval. Besides, the expectation is taken over the distribution of the trajectory (T, n) that
is due to the randomness of the policy, the randomness of signal interval length and the transition
dynamics. Despite the seemingly complex definition of Q, we provide a few of its nice properties
that are useful for PI-DRMDP tasks as follows. The proofs are listed in Appendix B.
First, we consider the following objective function
Lφ := ED h(Rt + γQφ(τtj：t+2)- Qφ(%t+1))2i ,	(4)
where Ttj:t+2 = Tti:t+1 ◦ (st+1, a0t+1) (so that j = i) if t is not the last step of Ti, and Ttj:t+2 =
(sti+1 , a0t ) (so that j = i+ 1) otherwise. Similarly to Eq. (1), in Eq. (4), (Tti:t+1, Rt, st+1) is also
sampled from D and a0t+1 is sampled from π correspondently. We may view Eq. (4) as an extension
of Eq. (1) for Qπ. However, with these new definitions, we are able to prove the following fact 1.
Fact 1. For any distribution D with non-zero measure for any Tti:t+1, Qπ (Tti:t+1) is the unique
fixed point of the MSE problem in Eq. (4). More specifically, when fixing Qφ as the corresponding
Qπ, the solution of the MSE problem is still Qπ.
Consequently , Qπ can be found precisely via the minimization for tabular expressions. This helps
to solve the problems in critic learning for PI-DRMDP. We next introduce Fact 2 which states that
the order of Qπ w.r.t. the actions at any state st is invariant to the choice of Tti:t, thanks to the
PI condition.
Fact 2. For any Tti:t, Tt0i:t and st, at, a0t which both Tti:t ◦ st and Tt0i:t ◦ st are feasible under the
transition dynamics, ∀π ∈ Πs, we have that
Qn (Tti:t ◦ (st, at)) > QK(Tti:t ◦ (st, Ot)) Q⇒ Qn (Tti:t ◦ (st, at)) > Qn(TOi:t ◦ (st, at)).
Fact 2 ensures that the policy iteration on ∏k (at ∣st,t-ti) with an arbitrary Qn (Tti：t ◦ (st, at)) results
in the same πk+1 ∈ Πs . Consequently, we are able to prove the convergence theorem for the policy
iteration with the Q-function.
1The definition of Qn and Fact 1 also holds in DRMDP and for π ∈ ∏τ.
4
Under review as a conference paper at ICLR 2022
Proposition 1. (Policy Improvement Theorem for PI-DRMDP) For any policy πk ∈ Πs, the
policy iteration w.r.t. Qπk produces policy ∏k+ι ∈ ∏s such that ∀τ^t^it+ι, it holds that
Qπk + 1(Tti:t + 1 ) ≥ Qnk (Tti:t +1),
which implies that J (πk+1) ≥ J (πk).
Thus, the off-policy value evaluation with Eq. (4) and the policy iteration in Proposition 1 guarantee
that the objective function is properly optimized and the algorithm converges. Besides, for off-policy
actor critic algorithms in the continuous environment, the policy gradient Vθ J(∏) is changed to
Vθ J (∏θ )= ED Vat QK(Tti:t ◦ (st,at ))1	Vθ ∏θ(st,t - ti) ,	(5)
πθ (st,t-ti)
where (Tti:t, st) is sampled from D. The full algorithm is summarized in Algorithm 1. As a straight-
forward implementation, we approximate Qn(丁益：t+1) with a GRU network (Cho et al., 2014) and
test it on the continuous PI-DRMDP tasks. As shown in Figure 2(a), our prototype algorithm
already outperforms the original counterparts (i.e., SAC) on many tasks.
3.2	The HC-Decomposition Framework
One challenge raised by the Q-function is that it takes a relatively long sequence of states and
actions as input. Directly approximating the Q-function via complex neural networks would suffer
from lower learning speed (e.g., recurrent network may suffer from vanishing or exploding gradients
(Goodfellow et al., 2016)) and computational inefficiency. Furthermore, the inaccurate estimation of
Qπ(Tti：t+i) will result in inaccurate gradients in Eq. (5) which degrades the learning performance.
To improve the practical performance of our algorithm for high dimensional tasks, we propose the
HC-decomposition framework (abbreviation of History-Current) that decouples the approximation
task for the current step from the historical trajectory. More specifically, we introduce Hφ and Cφ
functions and require that
Qφ(τti：t + 1) = Hφ(τtiQ+ Cφ(st ,at),	⑹
Here, we use Cφ(st, at) to approximate the part of the contribu-
tion to Qφ made by the current step, and use Hφ (Tti：t) to approx-
imate the rest part that is due to the historical trajectory in the
signal interval. The key motivation is that, thanks to the Markov
transition dynamics, the current step (st , at ) has more influence
than the past steps on the value of the future trajectories under
the given policy π. Therefore, in Eq. (6), we use Cφ to highlight
this part of the influence. In this way, we believe the architecture
is easier to optimize, especially for Cφ whose input is a single
state-action pair.
Moreover, we also find that the policy gradient will only de-
pend on Cφ(st, at). Indeed, we calculate that the policy gradient
VθJHC(πθ) equals to
Figure 1: Policy gradient vari-
ance averaged over the training
process. All bars show the mean
and one standard deviation of 4
seeds.
ED VatCφ(st,at)	Vθπθ(st,t-ti) ,	(7)
πθ (st,t-ti)
Comparing Eq. (7) with Eq. (5), we note that the gradient on the policy π under HC-decomposition
will not be effected by Tti :t . Thus, for a mini-batch update, our policy gradient has less variance
and the training becomes more efficient. In Figure 1, we visualize and compare the scale of the
gradient variance of our HC-decomposition framework and the straightforward recurrent network
approximation of the Q function, w.r.t. the same policy and the same batch of samples. Clearly, the
result supports our claim that HC-decomposition results in less gradient variance.
Finally, to learn the Hφ and Cφ functions, we minimize the following objective function
LHC =ED [(Rt + Y (Hφ(τtii-.t + 1) + Ccφ(st+1,at + 1)) -(Hφ(τti∙,t)+ Cφ (St, at)))2] + λLreg(Hφ),
(8)
5
Under review as a conference paper at ICLR 2022
(a) Performance on High-Dimensional Tasks
(b) Ablation on Regulation
Figure 2: (a): Performance of different implementations of HC-decomposition framework (Q-HC),
Q-RNN and vanilla SAC. The task is the continuous sum-form PI-DRMDP task and qn is a uniform
distribution between 15 and 20. The results show the mean and standard deviation of 7 seeds each.
(b) Ablation on the Lreg. Each dashed line is the no-regularization version of the corresponding
Q-HC of the same color.
where the first term is the TD error in Eq. (4) and the second term is a regularizer on Hφ . We use
the regularization to stabilize the optimization process and prevent Hφ from taking away too much
information of the credit on the choice of at. In Section 4.1, we will compare several simple choices
of Hφ and Lreg . The search for other designs of Hφ and the regularization term in various settings
is left as future work.
4	Experiment
In this section, we first test and illustrate our algorithmic framework and HC-decomposition on high-
dimensional PI-DRMDP tasks to validate our claims in previous sections. Then, we compare our
algorithm with several previous baselines on sum-form delayed reward tasks. Our algorithm turns
out to be the SOTA algorithm which is most sample-efficient and stable. Finally, we demonstrate
our algorithm via an illustrative example.
4.1	Design Evaluation
We implement the following architectures of Hφ in our experiment. Our algorithm is based on SAC
(Haarnoja et al., 2018a).
Q-HC-RNN. Hφ(τt"t) is approximated With a GRU network.
Q-HC-PairWise-K. Hφ(丁左。is the sum of K+1 networks {cφ}k=0,1,...κ for different pairwise
terms as follows2. K = 1, 3 are included in the experiment.
Hφ (τti :t) =	cφ(Tj ：j + 1 ◦ τj+kj+k+1).
k∈[0=K+1] j∈[ti∙.t-k]
Q-HC-Singleton. H©gi：t) is the Sum of single step terms.
Hφ(TtiI)= E bφ(Tj：j+i).
j∈[ti :t]
In terms of the representation power of these structures, (-RNN) is larger than (-Pairwise) larger than
(-Singleton). With a small representation power, critic’s learning will suffer from large projection
error. In terms of optimization, (-Singleton) is easier than (-Pairwise) and easier than (-RNN). As
discussed in Section 3.2, non-optimized critic will result in inaccurate and unstable policy gradients.
The practical design of Hφ is a trade-off between the efficiency of optimization and the scale of
projection error. For the regularizer in Eq. (8), we choose the following for all implementations.
Lreg(Hφ) = ED (Hφ(Ti) - r(Ti))2 ,	(9)
2 Input for c0φ is thus a single state-action pair.
6
Under review as a conference paper at ICLR 2022
where τi is sampled from the D. As discussed in Section 3.2, the history part Hφ (τti :t) should only
be used to infer the credit of τti :t within the signal interval τi . However, without the regularization,
the approximated Hφ may deprive too much information from Qφ than we have expected. Thus, in
a direct manner, we regularize Hφ(τi) to the same value of r(τi).
In Figure 2, we compare these algorithms on sum-form PI-DRMDP continuous control tasks based
on OpenAI Gym. Rewards are given once every n steps which is uniformly distributed from 15 to
20. The reward is the sum of the standard per-step reward in these tasks. Our method is compared
with vanilla SAC and Q-RNN whose Qπ is approximated by a GRU network. The results verify our
claims made in Section 3 and the necessity of the design of our method.
1.	Algorithms under the algorithmic framework (i.e., with prefix Q) outperform vanilla SAC. Our
framework approximates Qπ while vanilla SAC cannot handle non-Markovian rewards.
2.	Qπ approximated with HC-decomposition architecture (i.e., with prefix Q-HC) is more sample-
efficient and more stable than Q-RNN. Indeed, the HC architecture results in an easier optimiza-
tion in critic learning.
3.	Figure 2(b) shows the importance of regularization, especially when Hφ has complex form (i.e.,
Q-HC-RNN) and thus is more likely to result in ill-optimized C-part.
More Experiments. In Appendix C.1, we show that the empirical results are consistent in a variety
of PI-DRMDP tasks and under different implementations. First, we test in sum-form tasks with
different interval length distributions (qn). As shown in Table 2, our Q-HC methods are scalable to
sum-form tasks with a signal interval length of 60 environmental steps, which is quite a long period
in the Gym benchmark, and perform well in various random length tasks. Then, in non-sum-form
tasks with Max and Square reward functions, we find that Q-HCs still outperform Q-RNN and the
vanilla SAC consistently in all environments (Figure 5). This suggests that the advantage of HC-
decomposition also holds in other PI reward function tasks. Moreover, in Figure 6, we also test
TD3-based Q-HC-Singleton and Q-HC-Pairwise-1. Comparing to TD3-based Q-RNN and vanilla
TD3 (Fujimoto et al., 2018), these two variants outperform the baselines uniformly.
To sum up, based on the empirical evidence, we conclude that our Q-HC is widely applicable in
PI-DRMDPs as a remedy to the unstable optimization of direct Q approximation. As Q-HC is an
implementation of the general algorithm in Section 3.1, it handles the non-Markovian rewards with
theoretical support. In future work, we will explore the boundary of HC-decomposition, and analyze
the effect of the projection error mathematically.
4.2	Comparative Evaluation
Here, we compare our algorithm with previous algorithms in sum-form high-dimensional delayed
reward tasks. The following baselines are included in the comparison.
•	LIRPG (Zheng et al., 2018). It utilizes intrinsic rewards to make policy gradients more efficient.
The intrinsic reward is learnt by meta-gradient from the same objective function. We use the
same code provided by the paper.
•	RUDDER (Arjona-Medina et al., 2019). It decomposes the delayed and sparse reward to a
surrogate per-step reward via regression. Additionally, it utilizes on-policy correction to ensure
the same optimality w.r.t the original problem. We alter the code for the locomotion tasks.
•	SAC-IRCR (Gangwani et al., 2020). It utilizes off-policy data to provide a smoothed guidance
rewards for SAC. As mentioned in the paper, the performance heavily relies on the smoothing
policy. We implement a delayed reward version of IRCR in our setting.
For clarity, we only include Q-HC-Pairwise-1 and Q-HC-RNN in the comparison. As shown in
Figure 3, we find that LIRPG and RUDDER perform sluggishly in all tasks, due to their on-policy
nature (i.e., based on PPO (Schulman et al., 2017)). SAC-IRCR performs well only on some easy
tasks (e.g. Hopper-v2). Unfortunately, SAC-IRCR has a bad performance (e.g. Ant-v2, Walker2d-
v2, Reacher-v2) on other tasks. We suspect in these tasks, the smoothing technique results in a
erroneous guidance reward which biases the agent. Thus, SAC-IRCR is not a safe algorithm in solv-
ing delay reward tasks. In contrast, ours (as well as other implementations shown in Appendix C.3)
perform well on all tasks and surpass the baselines by a large margin. Most surprisingly, our algo-
rithm can achieve the near optimal performance, i.e., comparing with Oracle SAC which is trained
on dense reward environment for 1M steps. Noticing that we use an environment in which rewards
are given only every 20 steps.
7
Under review as a conference paper at ICLR 2022
Figure 3: Comparisons of Q-HC-Pairwise-1, Q-HC-RNN and baselines on 5 sum-form PI-DRMDP
tasks. The dashed line is the value of Oracle SAC. qn is fixed as 20 (Zheng et al., 2018; Oh et al.,
2018). The results show the mean and the standard deviation of 7 runs. All curves are further
smoothed equally for clarity. The last picture shows the relative performance w.r.t the Oracle SAC on
sum-form PI DRMDP tasks with general reward function (Appendix B). Each data point shows the
average of 5 tasks of each algorithm. X-axis refers to the overlapping steps of the reward function.
We also conduct experiments on tasks with general reward functions, in which our algorithmic
framework and the HC-decomposition can be naturally extended to (please refer to Appendix B and
Appendix A). In the General Experiment of Figure 3, we plot the relative average performance w.r.t
Oracle SAC on tasks whose general reward functions have different amount of overlapped steps (c =
5, 10). Clearly, our Q-HC is still the SOTA algorithm for every c. Thus, our method is applicable to
environments with general reward functions. Please refer to Appendix D for experiment details.
4.3	Historical Information
In addition, with a toy example, we explore what kind of information the H-component learns so
that the C-component makes suitable decisions. The toy example is a target-reaching task illustrated
in Figure 4 Left. The point agent is given delayed reward which roughly indicates its distance to
the target area. This mimics the low frequency feedback from the environment. Noticing that the
reward function is not in sum-form. Please refer to Appendix D for more details.
The learning curves are shown in Figure 4 Middle. Clearly, Q-HC-Singleton outperforms the base-
lines by a large margin. For better illustration, we visualize bφ (st , at) in Q-HC-Singleton on the
grid in Figure 4 Right. The pattern highlights the line from the start point to the target area (i.e., the
optimal policy), suggesting that bφ has captured some meaningful patterns to boost training. We also
observe a similar pattern for HC-Singleton without regression (Appendix D). This is an interesting
discovery which shows that the H-component may also possess direct impact on the policy learning
instead of simply function as an approximator.
5	Related Work
Off-Policy RL: Off-policy deep reinforcement learning algorithms TD3 (Fujimoto et al., 2018)
and SAC (Haarnoja et al., 2018a) are the most widely accepted actor-critic algorithms on the robot
locomotion benchmark (Duan et al., 2016) so far. Based on previous work (Degris et al., 2012; Silver
et al., 2014), Fujimoto et al. (2018) puts forward the clipped double Q-learning (CDQ) technique
to address overestimation in critic learning. Haarnoja et al. (2018a) also uses CDQ technique but
instead optimizes the maximum entropy objective. These two methods lead to a more robust policy.
8
Under review as a conference paper at ICLR 2022
Figure 4: Left: 100 × 100 Point-Reach task with additional positional reward (indicated by area
color). Reward is given every 20 steps. Middle: Learning curves of Q-HC-Singleton and several
baselines. Dashed line represents Q-HC-Singleton without regularization. Y-axis shows the number
of steps needed for the point agent to reach the target area (cut after 500 steps). All curves represent
the mean of 10 seeds and are smoothed equally for clarity. Right: Heatmap visualization of bφ in
Q-HC-Singleton.
In our setting, we observe that SAC-based algorithms slightly outperforms TD3-based algorithms
suggesting the benefit of maximum entropy in delayed reward tasks.
Delayed or Episodic Reward RL: Developing RL algorithms for delayed reward or episodic reward
(at the end of the trajectory) has become a popular research area recently. Some theoretical analysis
have been conducted on delayed rewards in MAB and MDP ZhoU et al. (2018; 2019); HelioU et al.
(2020) These tasks are known to be difficult for long-horizon credit assignment (Sutton, 1984). To
address this issUe, Zheng et al. (2018) proposes to Use intrinsic reward to boost the efficiency of
policy gradients. The intrinsic reward is learnt via meta-learning (Finn et al., 2020). Gangwani et al.
(2019) and GUo et al. (2018) Use a discriminator to provide gUidance reward for the policy. The
discriminator is trained jointly with the policy with binary classification loss for self-imitating. In
RUDDER (Arjona-Medina et al., 2019), it Utilizes a recUrrent network to predict a sUrrogate per-step
reward for gUidance. To ensUre the same optimality, the gUidance is then corrected with groUnd trUth
reward. LiU et al. (2019) extends the design and Utilizes a Transformer (Vaswani et al., 2017) network
for better credit assignment on episodic reward tasks. Recently, Klissarov & PrecUp (2020) proposes
to Use GCN (Kipf & Welling, 2016) network to learn a potential fUnction for reward shaping (Ng
et al., 1999). Noticing that all these algorithms are based on PPO (SchUlman et al., 2017) and thUs
are on-policy algorithms while oUrs is an off-policy one.
Most recently, Gangwani et al. (2020) proposes to aUgment off-policy algorithms with a trajectory-
space smoothed reward in episodic reward setting. The design tUrns oUt to be effective in solving
the problem. However, as mentioned by the paper itself, this techniqUe lacks theoretical gUarantee
and heavily relies the choice of the smoothing policy. As shown in Section 4.2, in many cases, this
method becomes extremely spUrioUs. OUr algorithm is derived from a theoretical perspective and
performs well on all tasks.
6	Conclusion
In this paper, we model the seqUential decision problem with delayed rewards as Past-Invariant
Delayed Reward MDPs. As previoUs off-policy RL algorithms sUffer from mUltiple problems in PI-
DRMDP, we pUt forward a novel and general algorithmic framework to solve the PI-DRMDP prob-
lems that has theoretical gUarantees in the tabUlar case. The framework relies on a novelly defined
Q-valUe. However, in high dimensional tasks, it is hard to approximate the Q-valUe directly. To ad-
dress this issUe, we propose to Use the HC-approximation framework for stable and efficient training
in practice. In the experiment, we compare different implementations of the HC framework. They
all perform well and robUstly in continUoUs control PI-DRMDP locomotion tasks based on OpenAI
Gym. Besides, oUr method oUtperforms previoUs baselines on delayed reward tasks remarkably,
sUggesting that oUr algorithm is a SOTA algorithm on these tasks so far.
In terms of fUtUre work, two research directions are worth exploring. One is to develop oUr algorithm
and the HC-approximation scheme to varioUs real world settings mentioned in Section 1, possibly
with offline data (Levine et al., 2020). The other direction is to design efficient and advanced algo-
rithms with theoretical gUarantees for the general DRMDP tasks.
9
Under review as a conference paper at ICLR 2022
References
Mostafa Al-Emran. Hierarchical reinforcement learning: a survey. International journal of comput-
ing and digital systems, 4(02), 2015.
Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brand-
stetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards. In Advances in
Neural Information Processing Systems, volume 32, pp. 13566-13577. Curran Associates, Inc.,
2019.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 1724-1734, Doha, Qatar, October 2014. Associ-
ation for Computational Linguistics. doi: 10.3115/v1/D14-1179.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. In Proceedings of the
29th International Coference on International Conference on Machine Learning, pp. 179-186,
2012.
Esther Derman, Gal Dalal, and Shie Mannor. Acting in delayed environments with non-stationary
markov policies. arXiv preprint arXiv:2101.11992, 2021.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeal. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
2016.
Chelsea Finn, Pieter Abbeal, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In 34th Conference on Neural Information Processing Systems, 2020.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596, 2018.
Tanmay Gangwani, Qiang Liu, and Jian Peng. Learning self-imitating diverse policies. In Interna-
tional Conference on Learning Representations, 2019.
Tanmay Gangwani, Yuan Zhou, and Jian Peng. Learning guidance rewards with trajectory-space
smoothing. In 34th Conference on Neural Information Processing Systems, 2020.
Yaobang Gong, Mohamed Abdel-Aty, Qing Cai, and Md Sharikur Rahman. Decentralized network
level adaptive signal control by multi-agent deep reinforcement learning. Transportation Research
Interdisciplinary Perspectives, 1:100020, 2019.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Brockman Greg, Cheung Vicki, Pettersson Ludwig, Schneider Jonas, Schulman John, Tang Jie, and
Zaremba Wojciech. Openai gym, 2016.
Yijie Guo, Junhyuk Oh, Satinder Singh, and Honglak Lee. Generative adversarial self-imitation
learning. arXiv preprint arXiv:1812.00950, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
ReSearch, pp. 1861-1870, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018a. PMLR.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018b.
10
Under review as a conference paper at ICLR 2022
D. Hein, S. Depeweg, M. Tokic, S. Udluft, A. Hentschel, T. A. Runkler, and V. Sterzing. A bench-
mark environment motivated by industrial control problems. In 2017 IEEE Symposium Series on
ComputationalIntelligence (SSCI),pp.1-8,2017. doi:10.1109/SSCI.2017.8280935.
Amelie Heliou, Panayotis Mertikopoulos, and ZhengyUan Zhou. Gradient-free online learning in
continuous games with delayed rewards. In International Conference on Machine Learning, pp.
4172U181.PMLR, 2020.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Computer Science,
2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. CoRR, abs/1609.02907, 2016.
Martin Klissarov and Doina Precup. Reward propagation using graph convolutional networks mar-
tin. In 34th Conference on Neural Information Processing Systems, 2020.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Kaixiang Lin, Renyu Zhao, Zhe Xu, and Jiayu Zhou. Efficient large-scale fleet management via
multi-agent deep reinforcement learning. In the 24th ACM SIGKDD International Conference,
2018.
Yang Liu, Yunan Luo, Yuanyi Zhong, Xi Chen, Qiang Liu, and Jian Peng. Sequence modeling of
temporal credit assignment for episodic reinforcement learning. arXiv preprint arXiv:1905.13420,
2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
A. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and
application to reward shaping. In International Conference on Machine Learning, 1999.
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In Jennifer
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3878-3887, Stock-
holmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo
design through deep reinforcement learning. Journal of cheminformatics, 9(1):1-14, 2017.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on
International Conference on Machine Learning-Volume 32, pp. I-387, 2014.
David Silver, Julian Schrittwieser, Karen Simonyan, Aj Antonoglou, Ioannis abd Huang, Arthur
Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lill-
icrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis.
Mastering the game of go without human knowledge. Nature, 550, 2017.
Richard S Sutton. Between mdps and semi-mdps: Learning, planning, and representing knowledge
at multiple temporal scales. 1998.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, Depart-
ment of Computer Science, University of Massachusetts at Amherst, 1984.
11
Under review as a conference paper at ICLR 2022
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, N,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In 31th Conference on Neural
Information Processing Systems,pp. 5998-6008, 2017.
Thomas J Walsh, Ali Nouri, Lihong Li, and Michael L Littman. Learning and planning in envi-
ronments with delayed feedback. Autonomous Agents and Multi-Agent Systems, 18(1):83-105,
2009.
Zhe Xu, Zhixin Li, Qingwen Guan, Dingshui Zhang, and Jieping Ye. Large-scale order dispatch in
on-demand ride-hailing platforms: A learning and planning approach. In the 24th ACM SIGKDD
International Conference, 2018.
Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient
methods. In Advances in Neural Information Processing Systems, volume 31, pp. 4644-4654.
Curran Associates, Inc., 2018.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter Glynn, Yinyu Ye, Li-Jia Li,
and Li Fei-Fei. Distributed asynchronous optimization with unbounded delays: How slow can
you go? In International Conference on Machine Learning, pp. 5970-5979. PMLR, 2018.
Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet. Learning in generalized linear contextual bandits
with stochastic delays. Advances in Neural Information Processing Systems, 32:5197-5208, 2019.
12
Under review as a conference paper at ICLR 2022
A Algorithm
We summarize our algorithm for PI-DRMDP with General Reward Function as follows.
Algorithm 1 Algorithm with General Reward Function
1:	Choose Qφ structure: Q-RNN, Q-HC-Pairwise-K, e.t.c
2:	Initialize ∏θ , Qφ and target networks, D Ju
3:	for each env step do
4:	at 〜∏θ(∙∣st,t — ti)
5:	Take at and get st+1, Rt
6:	Add (st, at, Rt, st+1) to D
7:	for each gradient step do
8:	Sample a batch of (Tti -c：t+i, Rt, st+ι) from D
9:	Update Qφ via minimizing Eq.(8) (with τti-c.t) for Q-HC or Eq. (11) for Q-RNN
10:	Update πθ via Eq. (7) for Q-HC or Eq. (12) for Q-RNN
11:	Update the target networks
12:	end for
13:	end for
B Discussions and Formal Proofs
In this part, we provide the proofs for the statements in the main paper. In addition, we also add
detailed discussions on the issues mentioned above. To begin with, we restate the definitions with
general reward functions.
Definition 3 (DRMDP with General Reward Function, DRMDP-c). A DRMDP-c Mc =
(S, A, p, qn , rc , γ) is described by the following parameters.
1.	The state and action spaces are S and A respectively.
2.	The Markov transition function isp(s0|s, a) for each (s, a) ∈ S × A; the initial state distribution
is p(s0).
3.	The signal interval length is distributed according to qn(∙), i.e., for the i-th signal interval, its
length n is independently drawnfrom qn(∙).
4.	The general reward function rc defines the expected reward generated for each signal inter-
Val with a overlap of maximal C steps with previous signal intervals; suppose Ti = ”：右+出 =
(s, a)t：t+ni = ((st, at), . . . (st+n-1, at+ni-1)) is the state-action sequence during the i-th sig-
nal interval of length ni, then the expected reward for this interval is rc(Tt-c：t+ni ). 3
5.	The reward discount factor is γ.
The PI-DRMDP is also extended naturally as follows.
Definition 4 (PI-DRMDP with General Reward Function, PI-DRMDP-c). A Past-Invariant
DRMDP-c is a DRMDP-c Mc = (S, A, p, qn, rc, γ) whose reward function rc satisfies the fol-
lowing Past-Invariant (PI) condition: for any two trajectory segments T1 and T2 of the same length
(no less than c), and for any two equal-length trajectory segments T10 and T20 such that the concate-
nated trajectories Ta ◦ Tb0 are feasible under the transition dynamics pfor all a, b ∈ {1, 2}, it holds
that
r(τι ◦ τ1) > r(τι ◦ τ2) ^⇒ r(τ2 ◦ τ0) > r(τ2 ◦ τ2).
Remark 1. Definition 1 and Definition 2 are the special case (c = 0) of Definition 3 and Definition 4
respectively.
Clearly, we have the following Fact.
Fact 3. ∀c0 < c, DRMDP Mc0 is also a DRMDP Mc and PI-DRMDP Mc0 is also a PI-DRMDP Mc.
3Ift<c, Tt—c：o refers to some zero token paddings which indicates the beginning of the trajectory.
13
Under review as a conference paper at ICLR 2022
Besides, under the general reward function, we extend the Q definition.
∞
Qn(Tti-C：t+i) := E(τ,n)〜∏ XYtj+1-t-1r(τtj-c：tj+i)pti-c：t+i .	(10)
j=i
Correspondingly, Qn (Tti-c：t+i) is optimized via
Lφ := ED h(Rt + YQφ(Ttj-C:t+2) - Qφ(τti-c:t+1))∣2'i ,	(II)
where Ttj-c:t+2 = Tti-c:t+1 ◦ (st+1, a0t+1) (so that j = i) if t is not the last step of Ti, and
Ttj-C:t+2 = Tti+1-C:ti+1 ◦ (sti+1, a0ti+1) (so that j = i + 1) otherwise.
The general policy update is also extended to
Vθ J (∏θ) = ED Rat Qn(Tti-C:t ◦ (st,at))∣	Vθ ∏θ (st,t- ti) .	(12)
πθ (st,t-ti)
Without specification, the following proofs hold for ∀c.
B.1 Proof of Fact 1
Fact 4 (Fact 1 with General Reward Function). For any distribution D with non-zero measure
for any Tti-C:t+1, Qπ (Tti -C:t+1) is the unique fixed point of the MSE problem in Eq. (4). More
specifically, when fixing Qφ as the corresponding Qπ, the solution of the MSE problem is still Qπ.
Proof. Though we state the fact for PI-DRMDP-c, we will prove it for general π ∈ Πτ,C in DRMDP-
c setting. For brevity, we replace t + 1 with t in the proof. By definition in Eq. (10), we have
Q (Tti -C:t ) = qn (ni 6= t - ti |ni ≥ t - ti )
Y E p(st∣St-i,at-i)∏(at∣Tti-ct ◦ St)Qn(Tti-C：t ◦ (St,at))
st,at
+ qn (ni = t - ti |ni ≥ t - ti )
Ir(Tti-C：t) + γ y^p(st∣st-i, at-i)∏(at∣Tt-ut ◦ St)Qπ(Tt-C：t ◦ (st,at)).
st,at
(13)
Clearly, Qn (Tti-C：t) is the solution of the following MSE problem. Noticing that We assume Qφ has
tabular expression over Tti-C:ts.
min E PD(Tti-C：t, Rt-1, St)∏(at∣Ttj∙-ut ◦ St)(Rt-1 + YQn(Ttj-C：t ◦ (st, at)) — QΦ(Tti-C：t)『
Rt-1,st
Here, Ttj-C:t is defined similarly as in the main text and pD refers to the probability of the fragment
of trajectory is sampled from the buffer D. We denote the objective function as £@(丁乜-足).As Lφ
in Eq. (11) is the sum of all Lφ(Tti-ut), Qn (Tti-C：t) is a fixed point of the MSE problem.
The uniqueness is proved similar to the standard MDPs setting. For any Qφ, we denote the optimal
solution of Eq. (4) as Qφ(τti-c：t). Since we assume tabular expression and non-zero probability of
any PD(Tti-C：t) Then, ∀τii.-c.t, we have
∣Qφ(^Tti-ct) - Qn(Tti-C：t)| ≤ Iqnlni = t - ti∣ni ≥ t 一 ti) X P(StISt-ι,at-i)∏(at∣Tti-ut ◦ St)
st,at
∣∣Qφ (Tti-C:t+1 ) - Q (Tti-C:t+1 )∣∣ + Yqn (ni = t - ti Ini ≥ t - ti )
EP(St∣St-i,at-i)∏(at∣Tt-ut ◦ St)
st,at
∣∣Qφ (Tt
-C:t ◦ (St, at)) - Qn(Tt-C:t ◦ (St, at))∣∣
ʌ _ ..
≤ Y kQφ-Qnk∞
14
Under review as a conference paper at ICLR 2022
T	.	.	1	.	.1	∙ r∙ ∙ .	i' .Λ	.	i'	∙ 1	1	1	Λ .	CE	1 ʌ T-I )	.
Last term denotes the infinite norm of the vector of residual value between Qπ and Qφ. Each entry
corresponds to some Tti-C：t feasible in the dynamic. Consequently, by the Y-concentration property,
if Qφ = Qφ after the iteration (i.e., Qφ is a fixed point), then Qφ = Qπ . This completes the proof
of uniqueness.	□
Remark 2. Fact 1 holds as the special case of Fact 4 when c = 0.
B.2	Proof of Fact 2
Fact 5 (Fact 2 with General Reward Function). For any 丁乜—亡匕,TtLcf and st, at, a which both
Tti—c：t ◦ St and Tti-C：t ◦ St arefeasible under the transition dynamics, ∀π ∈ ∏s, we have that
Qn(Tti-c：t ◦ (st, Ot)) > QITti-c：t ◦ (st, at)) ^⇒ Q(Tti-c：t ◦ (st, at)) > Qn(Tti-c：t ◦ (st, at))
Proof. Since π(at∣st,t - ti) ∈ ∏s, for any st, at, the distribution of the remaining trajectories
ρn (Tt:T, nt:T |st, at,ti) is irrelevant with Tti-c:t . Thus, we have
Q	(Tti-C:t	◦ (st,	at))	= X : P	(Tt：g, nt∙,∞ |st, at, ti)	I X : γ	j+1	r(Ttj-c：tj+i ) I
τt∙.∞ ,nt∙.∞	∖^j=i	)
= γ i+1	qn (ni	= ti+1 - ti |ni	≥ t -	ti )	ρ	(Tt:ti+1	◦	sti+1 |st ,	at , ti )
ti+1	Tt：ti+1
•卜(Tti-C:ti+i) + Y E π(ati+ι lsti+ι, O)Qπ (Tti+1-C：ti+1 ◦ (sti+1 ,ati+ι)) I
ati+1
H 〉:	qn (ni = ti+1 - ti lni ≥ t - ti)ρ (Tt:ti+i | st, at, ^ti)r(Tti-C∙.ti+ι~)
ti+1 ,Tt:ti+i
+ K(st, at, ti)
The last term denotes the residual part which is irrelevant with Tti:t. Besides, under the PI condition,
the order of r(Tti-C:ti+1 ) is invariant over Tti-C:t. As a result, the order of Qn(Tti-C:t ◦ (st, a))
among a is also invariant of Ttnt. This completes the proof.	□
Remark 3. Fact 2 holds as then special case of Fact 5 when c = 0.
B.3	Proof of Proposition 1
Proposition 2 (Proposition 1 with General Reward Function). For any policy πk ∈ Πs, the policy
iteration w.rt. Qπk (Eq. (10)) produces policy ∏k+ι ∈ ∏s such that ∀Tt;i-ct+ι, it holds that
Qnk+1(Tti-C:t+1) ≥ Qnk(Tti-C:t+1),
which implies that J (πk+1) ≥ J (πk).
Proof. Under Fact 5, the new policy iteration is defined formally as following
∏k+ι(at∣st,t - ti) = arg max Qπk (Tt-C：t ◦ (st,at)
at
where Tt-C:t is any trajectory segments feasible for st under the dynamics. If there are multiple
maximums, we can assign arbitrary probability among these ats. Following Eq. (13), we have
15
Under review as a conference paper at ICLR 2022
∀Tti-ut
Q k (τti -c:t) = qn (ni 6= t - ti |ni ≥ t - ti)
Y Ep(St∣st-i,at-i)∏k(at∣st,t - ti)Qπk (Tti-C：t ◦ (st,at))
st,at
qn(ni = t -ti|ni ≥ t -ti)
Ir(Tti-C:t) + YE p(st∣st-i,at-i)∏k(at∣st, 0)Qπk(τt-ut ◦ (st,at))
st,at
≤ qn(ni 6= t - ti|ni ≥ t - ti)
Y Ep(St∣St-i,at-i)∏k+ι(at∣St,t - ti)Qπk (Tti-C：t ◦ (st,at))	+
st,at
qn(ni = t -ti|ni ≥ t -ti)
Ir(Tti-C：t) + Y Ep(St∣st-i,at-i)∏k+ι(at∣st, 0)Qπk (τt-c：t ◦ (st,at))	(14)
st,at
Then, we can iteratively extend each Qπk term in Eq. (14) and get ∀Tti -C:t
Qπk (Tti-C:t) ≤ Qπk+1 (Tti-C:t)
Noticing that the inequality is strict if any step of the inequality in the iterations is strict. Further-
more, by definition, J(∏) = Ps0,a0 p(s0)π(a0∣s0,0)Qπ(s0,a0)4,we have
J(πk) ≤	p(S0)πk+1 (a0 |S0, 0)Qπk(S0, a0)
s0,a0
≤	p(S0)πk+1 (a0 |S0, 0)Qπk+1 (S0, a0)
s0,a0
≤ J (πk+1)
This completes the proof.	□
Remark 4. Proposition 1 holds as the special case of Proposition 2 when c = 0.
B.4	Discussion of Optimal Policies
To begin with, we consider the general DRMDP-c. Similarly, we denote ∏τ,c = {∏(at∣τti-ut ◦ st)}
as the extension of Πτ (c = 0). Then, we prove the following Fact.
Fact 6. For any DRMDP-c, there exists an optimal policy π* ∈ ∏τ,c∙ However, there exists some
DRMDP-0 (so as DRMDP-c ∀c) such that all ofits optimal policies π* ∈ ∏s.
In general, any policy belongs to the class 口皿 = {n(at|To：t ◦ st,no：t)}. Namely, the agent's
policy can only base on all the information it has experienced till step t. Besides, in DRMDP-c, the
expected discounted cumulative reward is
J(π) = Xρπ(T,n)R(T,n).
τ,n
where ρπ(T, n) denotes the probability that trajectory (T, n) is generated under policy π ∈ Πall. To
begin with, we first prove the following Lemma.
Lemma 1. For any policy π ∈ Πall which is the optimal, i.e, maximize J (π), we consider two
segments of trajectories (T0a：ta, n0a：ta) and (T0b：tb, nb0：tb) which satisfies that Ttaa-C：ta = Ttbb-C：tb and
tb ≥ ta. Besides, we also consider ∀t ≥ tb, t0 = t -tb+ta and T0b：t = T0b：tb ◦Ttb：t, nb0：t = nb0：tb ◦ntb：t
4Here, we omit specifying the c-step padding.
16
Under review as a conference paper at ICLR 2022
(0)
and state st feasible in the dynamic, we switch the policy π to π0 w.r.t these two trajectories as
following
n1|TOb：t ◦ st,nθ:t) = n(1Ta：ta ◦ Ttb：t ◦ st0 ,na:ta ◦ ntb：t).
Then, π0 ∈ Πall and J (π0) = J (π).
Proof. Obviously, π0 is well-defined and π0 ∈ Πall as the transition is Markovian. Noticing that
J (π0) =	ρπ0 (τ, n)R(τ, n)
τ,n
E	ρπ (τ, n)R(τ, n) +ρπ'(TbB ,n0B)
(T，n)：(Tb：” ,no” )⊂(τ,n)
X--------------------{z--------------------}
denoted by Jπ0 (Tb b ,nb b )
ρπ0(T, n|T0b:tb, nb0:tb)R(T, n)
(T,n)：(Tb：tb ,no” )⊂(τ,n)
J (T0b:tb , nb0:tb ) + ρπ	(T0b:tb , nb0:tb )	X
(T，n)：(Tb” ,no：” )⊂(τ,n)
(∞
X Ytj + Tr(Ttj-C：tj + i)
j=1
i(tb)-1
X Ytj + Tr(Ttj-C：tj + i )
j=1
+ Pπ0(τM,n0：tb) (Y"bʤb)).
We denote (T0:t, n0:t) ⊂ (T, n) if the former is the trajectory prefix of the latter. Qπ0 is defined in
Eq. (3) except that π0 ∈ Πall in this case. By the definition of π0 and the condition that π is optimal,
WehaVe Qπ0 (Ttb-C:tb ) = Qnaa-C:ta ) = QnTbb-C:tb ) and Jn (TO:tb ,n0：tb ) = J 以 T0：tb ,n0：tb ).
Thus, we have
z	z	∕i(.tb)-1	∖
J(π0) = JN(TO：tb,n0：tb) + Pn(TO:t,n0B) [ X γtj+1-1 T(Ttj-C：tj+i)1
+ PN(TM,nθ:tb) (Ytb-1Qπ(Tb-C:tb))
= Jπ	(T0O:tb, nO0:tb) +ρπ	(T0O:tb, nO0:tb)	X	ρπ (T, n|T0O:tb, nO0:tb)
(T,n)：(Tb：tb ,n0：tb )⊂(^r,n)
[X Ytj+1-1r(Ttj-C:tj+1)1
= J(π).
□
We refer it as a policy switch on π w.r.t (T0O:tb, nO0:tb) and (T0a:ta , n0a:ta ) to π0. Then, We prove Fact 6
as folloWs.
Proof of Fact 6. For simplicity, We denote ΠtT,C ⊂ Πall as the policy space that if π ∈ ΠtT,C and
∀tα,tb ≤ t then ∀(Tata ,nθ:ta ),(TO:tb ,n0：tb ) which Taa-C:ta = Ttb-C"b, MlTa俨。Sta ,n")=
∏(∙∣T0°.tb。Stb,n0.tb) for all s feasible. Clearly, ∏T,c = Πall and ΠT∞,C = ΠT,C. Then, starting from
some optimal policy π0 ∈ ΠT0,C, we utilize the policy switch operation to shift it into ΠT .
17
Under review as a conference paper at ICLR 2022
Suppose that πt-1 ∈ Πtτ-,c1, we consider the following two steps of operations.
1.	Step 1. V(To：t,no：t) that ∃(τ0①,nD which t0 < t and 丁%一疗 = τ0o-* (randomly
i:	i c:
choose one if multiple segments exist), We conduct a policy switch on ∏t w.r.t(To：t,no：t)
and (丁0出,no：to). We denote the policy after all these policy switches as ∏0-ι.
2.	Step 2. WedenOte S∏t-ι (Tti-C:t) = {(Tθ:t,nθ:/|Tti-c:t = Tti-c:t and @(T0：to, n0：t0), t <
t which T；o	o = Tti-C：t}. For each S∏1 (Tti-C：t), werandomly choose some (τl‰ n0：J ∈
i-c:
S∏t-ι (Tti-C：t) and conduct policy switch on π0-ι w.r.t all (τ,n) ∈ S∏.∖ (Tti-C：t) and
(T00:t,n00:t). Finally, we get πt.
Since πt-1 ∈ Πtτ-,C1, it is straightforward to check that πt ∈ Πtτ,C. Besides, as all operations are
policy switches, πt is optimal if πt-1 is optimal. Consequently, by induction from the optimal
policy π0, we can prove that ∃π∞ ∈ Πτ,C which is optimal.
For the second statement, we consider the following DRMDP-0.
whose n = 2. a0 , a1 denote two actions from A0, A1 to B respectively and b0 , b1 are two actions
from B to C. The reward is defined as r(a%, bj) = i㊉ j, ∀(i, j) ∈ {0,1}. The initial state distribution
is p(A0) = p(A1) = 0.5. Clearly, for any optimal policy, the agent has to query for its action from
the initial state to B before deciding the action from B to C. This illustrates that ∏* may not be in
∏s.	□
Consequently, learning optimal policies is relatively hard in DRMDP due to the large policy class
to search. In our work on PI-DRMDP, we only focus on the optimization in Πs . However, in some
bizarre cases, the optimal policy ∏* ∈ ∏s. For example, consider the following PI-DRMDP-0 where
n if fixed as 2.
The initial state is uniformly chosen from {Ab, Al} and C+, T are terminal states. The reward
function for the first interval is r(ai, bj) = i × j (Past-Invariant) and r(T) = 5. The optimal policy
at B should be π(∙∣Ab∕ι,B) = b+1∕-1 which is not in ∏s.
We have not covered the potential non-optimal issue in this work and leave developing general
algorithms for acquiring optimal policy for any PI-DRMDP-c or even DRMDP-c as future work. As
mentioned above, most real world reward functions still satisfy that the optimal policy belongs to
Πs . This makes our discussion in Πs still practically meaningful. Additionally, we put forward a
sub-class of PI-DRMDP-c which guarantees that there exists some optimal policy ∏* ∈ ∏s.
Definition 5 (Strong PI-DRMDP-c). A Strong Past-Invariant DRMDP-c is a PI-DRMDP-c whose
reward r satisfies a stronger condition than the Past-Invariant (PI) condition:
∀ trajectory segments T1 and T2 of the same length (no less than c), and for any two equal-length
trajectory segments T10 and T20 such that the concatenated trajectories Ta ◦ Tb0 are feasible under the
transition dynamics p for all a, b ∈ {1, 2}, it holds that
r(T1 ◦ T10) - r(T1 ◦ T20) = r(T2 ◦ T10) - r(T2 ◦ T20).
18
Under review as a conference paper at ICLR 2022
Namely, the discrepancies are invariant of the past history τ1,2 in addition to only the order. A
straightforward example is the weighted linear sum of per-step rewards
t+n-1
r(Tt-c：t+n) = E Wi-tιr(si,ai), 0 ≤ Wi-t ≤ 1
i=t-c
The weight mimics the scenario that the per-step reward for the i-tth step in the reward interval will
be lost with some probability wi-t . Furthermore, Strong PI-DRMDP-c satisfies
Fact 7. For any Strong PI-DRMDP-c, there exists some optimal policy π* ∈ ∏s.
Remark 5. Table 1 summarizes the case when c = 0.
B.5	Discussion of Off-Policy Bias
This following example illustrates the off-policy bias concretely. We consider the case for a sum-
form PI-DRMDP-0.
The signal interval length is fixed as n + 1 and any trajectory ends after one interval. The reward
function is in sum-form over r(s, a). When learning the critic (tabular expression) via Eq. (1), the
Q-values at the last step n are 5
Qφ(Sn,an)= EPe(T0：n|Sn,an)r(Tl)
T0:n
=EPe(T0：n|Sn,an) ^r(Tt：t+1) +r(Sn,On)	(15)
T0：n	∖t=O	)
X-----------------V--------------}
Q (sn,an)
In Eq. (15), ρβ(To：n|sn, an) denotes the probability distribution of To：n collected under the policy
sequence β conditioning on that Tn：n+i = (sn, an). Besides, we denote ρβk(To：n|sn) as the dis-
tribution under policy βk conditioning on sn at the last step. Obviously, Pβk is independent of last
step’s action an as π ∈ Πs . By Bayes rule, we have the following relation
β( I	、	Pk=1 pβk(T0:n|sn)ek(an|sn)	「q
PP (TO:n lsn,an) =	k 1 LK C ,——------------- (16)
k=1 βk (an |sn )
In other word, pβ (∙∣Sn, an) is a weighted sum of {pβk (∙∣Sn)}. The weights are different between an S
since the behavior policies {βk} are different. Consequently, Qβ(sn, an) will also vary between ans
and it is hard to be quantified in practice. Unfortunately, We may want Qφ(sn, an) a r(sn,an) for
unbiased credit assignment the actions at the last step. Thus, updating the policy with approximated
Qφ(sn, an) will suffer from the off-policy bias term Qβ(sn, an).
B.6	Discussion of Fixed Point Bias
Even with on-policy samples in D, critic learning via Eq. (1) can still be erroneous. We formally
state the observation via a toy sum-form PI-DRMDP-0.
5Without loss of generality, the discussion is simplified by assuming sn will only appear at the end of a
reward interval.
19
Under review as a conference paper at ICLR 2022
where for state B, C, D, there is only one action. The agent only needs to decide among a0 , a1 at
state A. n is fixed as 2 and the initial state distribution p0 (A) = p0 (B) = 0.5. The reward has
sum-form over the following per-step reward
r(C, c) = r(D, d) = 0
r(A, a0) = 0.01, r(A, a1) = 1
r(B, b) = -1
Fact 8. For the above PI-DRMDP-0 and any initialization of the policy, value evaluation (Qφ(st, at)
in tabular form) via Eq. (1) with on-policy data and policy iteration w.r.t Qφ (st , at) will converge
to a sub-optimal policy with J (π) = -0.495γ. In contrast, our algorithm can achieve the optimal
policy with J (π) = 0.
Proof. For any policy π, We denote P = π(aι |A). Then, if We minimize Eq. (1) with on-policy
samples, we will have Qφ(C, c) = 0.01, Qφ(A, a0) = 0.01γ. Besides, as initial state distribution
are uniform, We have
Qφ(D, d) = I(P - I), QΦ(A,aι) = 2Y(P - I)
Since Qφ(A, a1) < Qφ(A, a0) for any p, policy iteration Will converges to a sub-optimal policy that
π(ao∣A) = 1.The sub-optimal policy has J(π) = -0.495γ. Noticing that this holds even if the
initial policy is already the optimal.
For our algorithm, We have
Qφ (A, a1 ) =γ Qφ (A, a0, D, d) =γ
For anyγ > 0 and any P, a single policy iteration Will have the optimal policy With but the optimal
has J(∏*)=0.	□
B.7	Discussion of DRMDP Definition
In this section, We compare DRMDPs With other related definitions about MDP delays. To be
specific, We provide brief discussions on the differences betWeen DRMDP (delayed reWard MDP)
With Delay MDP and Semi-MDP.
Semi-MDP and Option Framework: In Semi-MDP, actions are alloWed to execute variable length
of time. In DRMDP, actions are executed and re-planned at each time step While reWards are delayed
for random length and are non-Marvian. Simply solving DRMDPs (or PI-DRMDPs) as Semi-MDPs
Will result in serious sub-optimal problems.
The middle ground betWeen Semi-MDP and MDP is the Option FrameWork (Sutton, 1998), in Which
options are temporally extended action sequences and cen be learned, i.e., option policy, initial state
set, termination state set. Contrarily, in DRMDP, the length of the signal interval (comparing to the
length of the option execution time) is random and is determined by the environment. Furthermore,
in option frameWork, reWards are still provided once very step (Markovian).
Option frameWork is often used on problems With hierarchical structures (Al-Emran, 2015), in Which
the Whole policy is composed of a loW-level option policy over actions and a high-level control policy
over options. HoWever, DRMDPs are formulated for general sequential decision making problems.
Delay MDP: Delay MDPs (Walsh et al., 2009; Derman et al., 2021) are used to characterize ob-
servation delay and action delay. For example, the observation inputs to the controller (e.g., a
computer) is the observation of the remote agent (e.g., a drone) several milliseconds ago and the
action executed at the remote agent is the output of the control policy several steps ahead. We
have to emphasize that, in delay MDP, the reWards are still defined as per-step reWards and are cou-
pled With the corresponding state-action pair, i.e., When the delay is a constant m, the transition
(st, at, rt, st+1)a observed at the remote agent, is also the transition the controller received at step
t + m, i.e., (st+m , at+m , rt+m, st+m+1 )c.
In DRMDP, there is no observation-delay or action-delay and there is no per-step reWards coupled
With each state-action pair.
20
Under review as a conference paper at ICLR 2022
C Additional Results
C.1 Design Evaluation
Here, we show more experiments omitted in Section 4.1. To begin with, we compare different
HC-decomposition algorithms (Q-HC) with Q-RNN and vanilla SAC on several sum-form PI-
DRMDP tasks with different signal interval length distributions (qn). The results are consistent
with our discussions in Section 4.1, despite of the lengths and the distributions.
Table 2: Relative average performance in tasks with different signal interval length distribution qn
over 4 environments: Hopper-v2, HalfCheetah-v2, Walker2d-v2, Ant-v2. δ(x) refers to a fixed
interval length as x and U (a, b) refers a uniform distribution in range [a, b]. All results show the
mean of 6-7 seeds and NA refers to no result due to limited computation resources.
qn	SAC	Q-RNN	Q-HC- Singleton	Q-HC- Pairwise-1	Q-HC- Pairwise-3	Q-HC-RNN
δ(10)	0.62	0.61	0.91	0.89	0.92	0.98
U (10, 20)	0.48	0.52	0.95	0.84	0.95	0.96
U (15, 20)	0.48	0.61	0.89	0.99	0.95	0.93
δ(20)	0.51	0.53	0.91	0.93	0.98	0.99
δ(40)	0.30	0.38	0.90	0.94	0.85	0.87
δ(60)	0.17	N.A.	0.87	0.81	0.88	N.A.
Then, we test the HC-decomposition on non sum-form PI-DRMDP taks based on OpenAI Gym.
Two non sum-form tasks are included: Max and Square. For Max,
r(Tt：t+n) = 10 X max ,{r(si,ai)}
i∈ [t:t+n]
Here, r(si, ai) is the standard per-step reward in each task. For Square, We denote ravg(Tt：t+n)
1 Ei∈[t∙t+n] r(si,ai) and the reward is defined as follows.
r(Tt：t+n) =4 X
r avg (Tt:t+n)
sign(ravg (Tt:t+n ))
X ra2vg (Tt:t+n)
|ravg (Tt:t+n )| < 1.
|ravg(Tt:t+n)| ≥ 1.
Noticing that these two tasks are still PI-DRMDP. The results are shown in Figure 5. Clearly, Q-HC
algorithms outperform Q-RNN and vanilla SAC on all tasks.
Figure 5: Experiments of non sum-form PI-DRMDP tasks. The first line shows the result of Max
form and the second line shows the result of Square form. The learning curves are mean and one
standard deviation of 6-7 seeds.
Moreover, we also implement TD3-based Q-HC-Singleton, Q-HC-Pairwise-1 and compare them
with the vanilla TD3 and TD3-based Q-RNN. The results still match our analysis of HC-
decomposition in Section 3.2. However, we find that SAC-based variants consistently outperform
21
Under review as a conference paper at ICLR 2022
TD3-based variants. We think that the entropy maximization regularization is useful in the delayed
reward environment, which prevents the policy getting trapped in local maximal points.
Figure 6: Learning curves of TD3-based Q-HC-Singleton, Q-HC-Pairwise-1, Q-RNN and vanilla
TD3. All curves show the mean and one standard deviation of 7 seeds. The result is consistent with
that of SAC-based methods.
C.2 Ablation Study
In Figure 7, we show the ablation study on the regulation term Lreg on all 4 tasks. We observe
that in HalfCheetah-v2 and Ant-v2, the regulation plays an important role for the final performance
while the regulation is less necessary for the others. Besides, for Q-HC algorithms with complex
H-component architectures (i.e., Q-HC-RNN, Q-HC-Pairwise-3), the regulation is necessary while
for simple structure like Q-HC-Singleton, regulation is less helpful. We suspect that the simple
structure itself imposes implicit regulation during the learning.
Figure 7: More ablations on Lreg term. The task is a sum-form PI-DRMDP with n uniformly drawn
from 15 to 20. Dashed lines are the no regularization version of the algorithm Q-HC with the same
color. All curves show the mean and a standard deviation of 6-7 seeds.
Furthermore, we ablate the HC-decomposition in Q-HC-Singleton and Q-HC-Pairwise-1. Namely,
we implement Q-Singleton and Q-Pairwise-1 with the similar architecture but not with HC-
decomposition. For Q-Singleton, we have
Q(Tti：t+i) =	ɪ2 bφ(Γj∙.j+ι)
j∈[t"+1]
and for Q-Pairwise-1, we have
Q(Tti：t+1) = ɪ2 cφ (Tj：j+1 ◦ Tj+1：j+2) + ɪ2	bφ (Tjj+1)
j∈[ti :t]	j∈[t"+1]
In Figure 8, we observe that these two ablations perform sluggishly and even worse than Q-RNN
(without any decomposition). This suggests the necessity of HC-decomposition for PI-DRMDP, i.e.,
the separated history part (H) and the current part (C), as the transition of the environment is still
Markovian. Without the separation, a simple decomposition fails to assign the adequate credit to the
last step action in Tti:t+1, thus results in errorneous policy update.
C.3 Comparative Evaluation
We show detailed results of our algorithms Q-HC and previous baselines on sum-form PI-
DRMDP tasks with general reward function. Please refer to Appendix B and Appendix A for
22
Under review as a conference paper at ICLR 2022
Figure 8: Ablation of HC-decomposition in Q-HC-Singleton and Q-HC-Pairwise-1. The task is
a sum-form PI-DRMDP with n fixed as 20. Dashed lines are two ablations Q-Singleton and Q-
Pairwise-1. All curves show the mean and a standard deviation of 6 seeds.
detailed discussions. For environments with maximal c overlapping steps, the reward function is
formally defined as follows.
rc(Tti-C:ti +ni ) =	〉:	r (Si, Qi)
i∈[ti -c:ti +ni -c]
where r(si , ai ) is the standard per-step reward. If i < 0, then r(si , ai ) = 0. In this definition,
the reward is delayed by c steps and thus the signal intervals are overlapped. In Figure 9, we show
the relative average performance (over Reach-v2, Hopper-v2, HalfCheetah-v2, Walker2d-v2, and
Ant-v2) of each algorithm w.r.t the Oracle SAC trained on the dense reward setting. Please refer to
Appendix D for the exact definition of this metric. The results demonstrate the superiority of our
algorithms over the baselines in the General Reward Function experiments. This supports our claim
that our algorithmic framework and the approximation method (HC) can be extended naturally to
the general definition, i.e., PI-DRMDP-c. Furthermore, we show the detailed learning curves of all
algorithms in tasks with different overlapping steps c in Figure 10.
Figure 9: Relative Average Performance of our algorithms and the baselines on PI-DRMDP-c tasks
with general reward functions. Each dot represents the relative performance of the algorithm over
the PI-DRMDP-c task (average over Reach-v2, Hopper-v2, HalfCheetah-v2, Walker2d-v2, Ant-v2).
D Experiment Details
D.1 Implementation Details of Our Algorithm
We re-implement SAC based on OpenAI baselines (Dhariwal et al., 2017). The corresponding
hyperparameters are shown in Table 3. We use a smaller batch size due to limited computation
power.
To convince that the superiority of our algorithms (Q-HC-RNN, Q-HC-Pairwise-K, Q-HC-
Singleton) results from the efficient approximation framework (Section 3.2) and the consistency
23
Under review as a conference paper at ICLR 2022
Figure 10: Learning curves of our algorithms Q-HC and several previous baselines. The first line
refers to the task with c = 0. The second line refers to the task with c = 5. The third line refers to
the task with c = 10. All curves show the mean and one standard deviation of 6-7 seeds.
Table 3: Shared Hyperparameters with SAC
Hyperparameter	SAC	
Qφ , πθ architecture	2 hidden-layer MLPS with 256 units each
non-linearity	ReLU
batch size	128
discount factor γ	0.99
optimizer	Adam (Kingma & Ba, 2014)
learning rate	3 × 10-4
entropy target	-|A|
target smoothing τ	0.005
replay buffer	large enough for 1M samples
target update interval	1
gradient steps	1	
with the theoretical analysis (Section 3.1), rather than from a better choice of hyper-parameters, we
did not tune any hyper-parameter shared with SAC (i.e., in Table 3).
The architecture of different Hφs are shown as follows. For the regularization coefficient λ, we
search λ in {0.0, 0.05, 0.5, 5.0} for each implementation of Q-HC. λ is fixed for all tasks including
the toy example.
-	Q-HC-RNN. Each step's input Tt：t+i is first passed through a fully connected network With 48
hidden units and then fed into the GRU with a 48-unit hidden state. Hφ(τtiQ is the output of
the GRU at the corresponding step. We use λ = 5.0.
-	Q-HC-Pairwise-1. Both c0φ and c1φ are two-layer MLPs with 64 hidden units each. λ = 0.5.
-	Q-HC-Pairwise-3. All ciφ, i = 0, 1, 2, 3 are two-layer MLP with 48 hidden units each. λ = 5.0.
-	Q-HC-Singleton. bφ is a two-layer MLPs with 64 hidden units each. λ = 0.05.
As suggested by the theoretical analysis, we augment the normalized t - ti to the state input.
24
Under review as a conference paper at ICLR 2022
All high dimensional experiments are based on OpenAI Gym (Greg et al., 2016) with MuJoCo200.
All experiments are trained on GeForce GTX 1080 Ti and Intel(R) Xeon(R) CPU E5-2630 v4 @
2.20GHz. Each single run can be completed within 36 hours.
D.2 Implementation Details of Other Baselines
Q-RNN. We use the same shared hyper-parameters as in Table 3. The only exception is the architec-
ture of the GRU critic. Similar to Q-HC-RNN, each state-action pair is first passed through a fully
connected layer with 128 hidden units and ReLU activation. Then it is fed into the GRU network
with 128-Unit hidden state. The Qφ(τti：t+1) is the output of the GRU network at the CorresPond-
ing step. We choose the above architecture to ensure that the number of parameters is roughly the
same with our algorithm for fair comparison. The learning rate for the critic is still 3 × 10-4 after
fine-tuning.
SAC-IRCR. Iterative Relative Credit Refinement (Gangwani et al., 2020) is implemented on
episodic reward tasks. In the delayed reward setting, we replace the smoothing value (i.e., episode
return) with the reward of the reward interval. We find this performs better than using the episode
return.
D.3 Details for Toy Example
Point Reach. As illustrated in Figure 4, the Point Reach PI-DRMDP task consists of a 100x100
grid, the initial position at the bottom left corner, a 10x10 target area adjacent to the middle of the
right edge and a point agent. The observation for the point agent is its (x, y) coordinate only. The
action space is its moving speed (vx , vy) ∈ [-1, 1]2 along the two directions. Since the agent can
not observe the goal directly, it has to infer it from the reward signal. To relieve the agent from
heavy exploration (not our focus), the delayed rewards provide some additional information to the
agent as follows.
The grid is divided into 10 sub-areas of 10x100 along the x-axis. We denote the sub-areas (high-
lighted by one color each in Figure 4) as Si, i = [0 : 10] form left to right. In each sub-area Si, it
provides an extra reward and the reward ri indicates how far is the sub-area to the target (i.e., ri = i).
Clearly, the bigger the reward is, the closer the sub-area is to the target area. Besides, reward interval
length is fixed as 20 and the point agent is rewarded with the maximal ri it has visited in the interval
together with a bonus after reaching the target and a punishment for not reaching. Namely,
9
r(τt.t+n) = max y' ri ∙ 1(sjj+ι ∈ Si) + 10 ∙ (1( reach target) - 1)
je[t:t+n]
i=0
The task ends once the agent reach the target area and also terminates after 500 steps. Clearly. the
optimal policy is to go straightly from the initial point to the target area without hesitation. The
shortest path takes roughly 95 steps. The learning curves in Figure 4 show the mean and half of a
standard deviation of 10 seeds. All curves are smoothed equally for clarity.
Heatmap. In Figure 4 right, the heatmap visualizes the value of bφ(st, at) on the whole grid. We
select the bφs after the training has converged to the optimal policy. To be specific of the visual-
ization, for each of the 10x10 cells on the grid, st in bφ(st, at) is selected as the center of the cell
and at is sampled from ∏(∙∣st, 0). Additionally, in Figure 11, We visualize bφ(st, at) similarly of
Q-HC-Singleton without Lreg. Clearly, Figure 11 shows the similar pattern as Figure 4 right.
D.4 Details for Variance Estimation
The experiment in Figure 1 is conducted in the following manner. First, we train a policy πθ with
Q-HC-Singleton and collect all the samples along the training to the replay buffer D. Second, two
additional critics are trained concurrently with samples uniformly sampled from D. One critic uses
the GRU architecture (i.e., Q-RNN) and the other uses Q-HC-Singleton structure. Most importantly,
since these two critics are not used for policy updates, there is no overestimation bias (Fujimoto et al.,
2018). Thus, instead of using the CDQ method Fujimoto et al. (2018), these two critics are trained
via the method in DDPG (Silver et al., 2014). With a mini-batch from D, we compute the gradients
on the policy’s parameters θ in Eq. (5) and Eq. (7) from the two extra critics respectively. Noticing
that these gradients are not used in the policy training.
25
Under review as a conference paper at ICLR 2022
Figure 11: Visualization of bφ(st, at) of Q-HC-Singleton without regulation term.
Then, we compute the sample variance of the gradient on each parameter θi in the policy network.
The statistics (y-axis in Figure 1) is the sum of variance over all parameters in the final layer (i.e., the
layer that outputs the actions). The statistics is further averaged over the whole training and scaled
uniformly for clarity.
D.5 Details for Relative Average Performance
Here, we formally introduce the Relative Average Performance (RAP) metric in Table 2, Figure 9,
and Figure 3. The relative average performance (RAP) of algorithm A on task set E is defined
as the average of RAP over tasks in E. RAP of each task is the episodic return after 1M steps
training normalized by the episodic return of Oracle SAC at 1M steps (trained on dense reward
environment). The only exception is Reach-v2, in which the episodic returns are added by 50 to
before the normalization.
26