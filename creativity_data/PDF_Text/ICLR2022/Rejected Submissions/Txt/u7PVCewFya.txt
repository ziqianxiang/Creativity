Under review as a conference paper at ICLR 2022
Losing Less: A Loss for
Differentially Private Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Differentially Private Stochastic Gradient Descent, DP-SGD, is the canonical ap-
proach to training deep neural networks with guarantees of Differential Privacy
(DP). However, the modifications DP-SGD introduces to vanilla gradient descent
negatively impact the accuracy of deep neural networks. In this paper, we are
the first to observe that some of this performance can be recovered when train-
ing with a loss tailored to DP-SGD; we challenge cross-entropy as the de facto
loss for deep learning with DP. Specifically, we introduce a loss combining three
terms: the summed squared error, the focal loss, and a regularization penalty. The
first term encourages learning with faster convergence. The second term empha-
sizes hard-to-learn examples in the later stages of training. Both are beneficial
because the privacy cost of learning increases with every step of DP-SGD. The
third term helps control the sensitivity of learning, decreasing the bias introduced
by gradient clipping in DP-SGD. Using our loss function, we achieve new state-
of-the-art tradeoffs between privacy and accuracy on MNIST, FashionMNIST, and
CIFAR10. Most importantly, we improve the accuracy of DP-SGD on CIFAR10
by 4% for a DP guarantee of ε = 3.
1	Introduction
Releasing machine learning (ML) models may risk the privacy of training data as ML models unin-
tentionally leak information about the training data; this is due to limitations of learning, including
overfitting (Song & Shmatikov, 2019) and data memorisation (Fredrikson et al., 2015; Carlini et al.,
2019). The framework of Differential Privacy (DP) by Dwork et al. (2014) is the gold standard for
formalising privacy guarantees. When training ML models, a DP training algorithm provides guar-
antees bounding the leakage of private data that may occur during training. This requires that one
bound the influence of individual training examples on the output of the algorithm (e.g. parameters
or gradients of the model). In addition to these strong theoretical guarantees, DP models are empiri-
cally resistant to various attacks - membership inference attacks (Shokri et al., 2017; Rahman et al.,
2018), training data extraction (Carlini et al., 2019) and data poisoning attack (Ma et al., 2019).
Differentially Private Stochastic Gradient Descent, DP-SGD, trains a ML model under the frame-
work of DP by (1) clipping per-example gradients to a fixed norm to bound their sensitivity and
(2) adding noise to these clipped gradients before they are applied to update the model parame-
ters (Abadi et al., 2016). When done separately and at the level of a minibatch, clipping (Menon
et al., 2019; Zhang et al., 2019) or noising (Foret et al., 2021) effectively regularize learning because
they respectively control the dynamics of iterates and smoothen the loss landscape. However, gradi-
ent clipping and noising are detrimental to model performance in DP-SGD because they are applied
in combination and at the granularity of individual training examples. In addition to this, the accu-
racy of DP-SGD is negatively affected by the limited number of iterations that can be computed:
each iteration of DP-SGD increases the privacy budget expended by learning.
Recent works have explored bounded activation functions (Papernot et al., 2021), publicly available
feature extractors and data (Tramer & Boneh, 2021), randomised smoothing (Wang et al., 2021)
and gradient embedding (Yu et al., 2021) as a means to improve the tradeoffs between privacy and
accuracy of DP-SGD. However, training deep neural networks with strong DP guarantees in DP-
SGD still comes at a significant cost in accuracy for datasets like CIFAR10.
1
Under review as a conference paper at ICLR 2022
In this paper, we show that one key aspect of formulating the optimization problem solved in learning
is left out of studies seeking to improve deep learning with DP: the loss function. All existing
implementations of deep learning with DP we are aware of optimise cross-entropy (which performs
well in the non-privacy-preserving setting with SGD). However, we first observe that optimising
cross-entropy with DP-SGD leads to exploding model weights, layer pre-activations and logit values.
This is true even if the activation functions themselves are bounded (Papernot et al., 2021). This
phenomenon makes it difficult to control the sensitivity of the learning algorithm at a minimal impact
to its correctness. Indeed, clipping and noising large gradients in DP-SGD introduces an information
loss and additional biases. More precisely, the direction of a minibatch of clipped per-example
gradients in DP-SGD is not necessarily aligned with the direction of the original gradients in SGD.
Furthermore, we also note that the slow training convergence of cross-entropy (Allen-Zhu et al.,
2019) negatively impacts the performance of DP-SGD for which a limited number of iterations are
required to achieve strong privacy guarantees.
In this paper, to tackle these two limitations of the cross-entropy loss, we propose to tailor the loss
function to the specifics of DP-SGD. We design a novel loss function that takes into account sensi-
tivity, training convergence and the order in which examples are learned with the overarching goal
of improving the tradeoffs between privacy and accuracy of DP-SGD. We achieve these objectives
with a novel loss function that combines the Sum Squared Error (SSE) between the model logits and
ground-truth labels, the focal loss (Lin et al., 2017), and a regularisation penalty on pre-activations.
The first two penalties impose a curriculum structure in the training to improve privacy-accuracy
tradeoffs of DP-SGD by exploiting the generalisation and fast convergence speed (Wang et al., 2019;
Guo et al., 2018) of curriculum learning (Bengio et al., 2009). We start training with SSE which has
faster convergence (Allen-Zhu et al., 2019) and smaller gradients than cross-entropy. Then, we
gradually shift toward emphasising hard-to-learn examples using the focal loss: this down-weights
gradients assigned to examples that are already learned correctly to focus more on hard-to-learn and
misclassified examples. Finally, our regularisation penalty prevents the weights from exploding to
further reduce the magnitude of per-example gradients prior to clipping. Implementation-wise, our
new loss function can be easily integrated within existing implementations of deep learning with
DP; it requires a single line change to edit the loss function being optimized.
In summary, our main contributions are as follows:
•	We are the first to investigate the loss function in the context of deep learning with DP.
We analyse the effect of the loss function on the sensitivity of DP models by providing
extensive analysis of the norm of activations, weights and gradients in Section 4.
•	We propose a novel loss function tailored to deep learning with DP. In Section 3 and Sec-
tion 4, we analytically and experimentally connect the superior performance of our pro-
posed loss function to the gradient norm and direction, smoothness of the loss surface, and
convergence of DP-SGD. Our proposed loss function better controls the sensitivity of the
learning algorithm by better preconditioning gradients to being clipped and noised.
•	We show in Section 4 that our proposed loss function is compatible with other DP-SGD
improvement strategies, and more importantly allows us to establish new state-of-the-art
tradeoffs between privacy and accuracy on several key benchmarks for deep learning with
DP, including CIFAR10. Finally, we perform an ablation study to analyse the effect of each
component of our proposed loss function on DP learning.
2	Differential privacy, DP-SGD and state-of-the-art approaches
Trained machine learning models memorise and leak information about their training data (Shokri
et al., 2017; Rahman et al., 2018; Song & Shmatikov, 2019; Fredrikson et al., 2015). Differential
privacy (Dwork et al., 2014) is the established gold standard to reason about the privacy guarantees
of learning algorithms. An algorithm is differentially private if its outputs are statistically indistin-
guishable on neighbouring datasets. More formally, a randomised learning algorithm A is (ε, δ)-
differentially private if the following holds for any two neighbouring datasets D and D0 that differ
only in one record and all S ∈ Range(A):
Pr[A(D) ∈ S] ≤ eεPr[A(D0) ∈ S] + δ.	(1)
2
Under review as a conference paper at ICLR 2022
The privacy budget ε upper bounds the privacy leakage in the worst possible case: the smaller the
ε, the tighter the upper bounds (or stronger privacy guarantee). Note that the factor δ is very small
(generally it is chosen to be in the order of the inverse of the dataset size).
In order to train ML models with DP, we can add randomness to the learning algorithm in three
ways: output perturbation (Wu et al., 2017; Zhang et al., 2017), objective perturbation (Chaudhuri
et al., 2011; Iyengar et al., 2019) or gradient perturbation (Bassily et al., 2014; Abadi et al., 2016).
Among these approaches, differentially private stochastic gradient, DP-SGD of Abadi et al. (2016),
has established itself as the de facto strategy because of its versatility. DP-SGD perturbs gradients
computed at each step of stochastic gradient descent using the Gaussian mechanism to achieve
competitive tradeoffs between privacy and accuracy (YU et al., 2020; Tramer & Boneh, 2021). Next,
we describe the process of training a classifier using DP-SGD on a private dataset.
Let X = {xi|i = 1, ..., N} be the training set containing N private examples xi, and
Y = {yi|i = 1, ..., N} be the ground-truth label set where each yi is a D-dimension one-hot en-
coded vector. We consider a D-class classifier containing M layers parameterised with weights
W= {Wm|m = 1,...,M}.
In each DP-SGD iteration, similarly to the non-private SGD one, the classifier receives each training
example xi from a minibatch containing L data points that are sampled randomly from X . The
activation of each intermediate layer m of size dm, aim = f(him) ∈ R1×dm, is computed by applying
a non-linear function, f (∙), on the pre-activation hm = Wmam-1. The pre-aCtivatiOn is a linear
combination of the weight of that layer, Wm, and the activation of the previous layer, aim-1. The
last layer outputs the logit values associated with each class without any activation function as aiM =
hiM = WM hiM -1 ∈ R1×D. The cross-entropy loss, LCE, for each training example xi is computed
as
D	ea(Mi,d)
LCE = - ‰y(i,d) log(p(i,d)),	where p(i,d)= —D—M—
d=1	dD0=1 ea(i,d0)
(2)
The probability that xi belongs to d-th class, p(i,d) ∈ [0, 1], is computed by applying a Softmax
function on the logit value of its corresponding class a(Mi,d) .
The DP-SGD optimiser computes per-example gradient of LCE with respect to W, as opposed to per
minibatch gradient in the SGD optimiser, to bound the influence of each individual training example
on the output of the learning algorithm (i.e. weight gradients). The gradients, Gm, for each training
example xi are computed from the last layer and back-propagated until the first layer as:
GM = (Pi - yi)TaM-1,	GMT = ((Pi - yi)WM)TaM -
GM-2 = f0((Pi -yi)WMWmT)TaM-3.
(3)
One can continue this gradient computation until the first layer. All the per-example gradients are
concatenated as Gi = [Gi1; ...; GiM], where ; denotes the concatenation operation. As there is no
a priori bound on Gi, the l2 norm, k ∙ k2, of each Gi are artificially clipped by C to bound the
influence of each xi on the final gradients, G:
1L	C
G = L(E Gi + N(0,σ2C2I)) where Gi — Gi ∙ mm(1, k—ɪ).
i=1	i 2
(4)
Finally, the classifier weights are updated through the average of the clipped and noisy (scaled by
the noise multiplier σ) per-example gradients of a minibatch.
The accuracy of classifiers trained with DP-SGD is lower than that of classifiers trained by non-
private SGD. Recently, researchers investigated the choice of f (∙) and data representation for im-
proving the accuracy of the classifier trained by DP-SGD. Papernot et al. (2021) demonstrated that
exploiting a bounded family of activation functions instead of the more commonly employed un-
bounded ReLU activation for the choice of f (∙) can decrease the bias introduced by DP-SGD and
improve the tradeoffs between privacy and accuracy of DP-SGD. Their approach though does not
fully bridge the gap between non-private and private learning for datasets like CIFAR10. Tramer
& Boneh (2021) proposed to train the classifier on a representation of data outputted by a public
ScatterNet feature extractor as opposed to using the raw pixels xi . This however requires access to
public data in addition to the private dataset.
3
Under review as a conference paper at ICLR 2022
3 Proposed approach
We propose to design a new loss function tailored to DP-SGD. Our goal is to converge faster with
smaller gradient norms, control better the sensitivity of the learning algorithm, increase tolerance
to noise, and effectively learn both easy and hard examples. Indeed, these address several crucial
differences between DP-SGD and its non-private counterpart SGD:
•	Due to per-example gradient clipping (recall Equation 4), information contained in gradi-
ents whose magnitude is too large is discarded (see Figure 1.c). This cannot be compen-
sated by tuning the training algorithm, e.g., by increasing the model’s learning rate. This is
because clipping is done at the granularity of individual training examples. Together with
the noise injected (recall Equation 4), this biases learning and implies that DP-SGD takes
a different optimisation path compared to SGD.
•	As shown in Equation 3, the model weights1 contribute to the computation of all gradi-
ents. In practice, this leads to model weights exploding in DP-SGD. Therefore, one way
to decrease the magnitude of gradients is to prevent the model weights from exploding,
especially the weight of the last layer which contributes in the gradient of all the layers.
•	The number of training iterations are limited in DP-SGD because each iteration increases
the risk of privacy leakage. Hence, faster convergence is beneficial to DP-SGD as well as
ensuring both easy and hard examples are attended to during the limited training run.
Next, we describe our choice of loss function. We analyse how our new loss function improves
the tradeoffs between privacy and accuracy of DP-SGD. We uncover several effects: our loss limits
the information loss of gradient clipping, speeds up convergence, recovers a generalisation boost
comparable to the one achieved by batch normalisation, and improves learning’s tolerance to noise.
3.1 Our loss
We propose to learn the easy examples at the beginning of the training using the per-example Sum
Squared Error (SSE) between the logit values and one-hot vector labels:
1D
LSSE = 2IIhM -yik2,	where IIhM -yik2 = £(hMd)-y(i,d)产	⑸
d=1
Later in training, we exploit the focal loss LFocal to learn hard-to-learn examples. LFocal modifies the
cross-entropy loss to reduce the loss of easy, well-classified examples, letting the model focus more
on hard, misclassified examples. LFocal multiplies a factor (1 - p(i,t))γ based on the probability of
the ground-truth class p(i,t) to the cross-entropy of each training example xi as:
D
LFocal = -(1 - p(i,t))γ	y(i,d) log(p(i,d)),	(6)
d=1
where the tunable focusing parameter γ adjusts the down-weighting rate: the higher the γ, the
higher the down-weight rate of easy, well-classified examples. Note that the focal loss is equivalent
to cross-entropy loss, when γ = 0.
To further decrease the gradient magnitudes and prevent the explosion of intermediate weights, we
impose a regularisation penalty on the intermediate pre-activations as:
M -1
LReg =
m=1
dL khmk2.
dm
(7)
Finally, our proposed loss function L combines LFocal, LSSE and LReg as:
L = αLFocal + (1 - α)LSSE +
，LReg,
(8)
where we set hyper-parameter α = Sigmoid(ec - et) (current epoch, ec, and threshold epoch, et)
to enable curriculum learning where easy examples are learned in the early training iterations using
LSSE which eases learning of hard examples in the later training iterations using LFocal . We perform
a hyper-parameter search to set the best values for α, β and γ.
1 Note that all the terms except those corresponding to weights are bounded in Equation 3.
4
Under review as a conference paper at ICLR 2022
(a) Logit values
(b) Weights
(c) Gradient
(d) Path divergence
Figure 1: l2 norm of the logit values (a), l2 norm of the last layer weights (b), histogram of per-
sample gradient magnitude in the first epoch (c), and divergence histogram of optimisation path in
aggregated clipped gradients from the optimisation path in unclipped ones (d) when we train the
classifier on CIFAR10 using cross-entropy loss or our proposed loss function. Minimising cross
entropy loss using DP-SGD leads to logit exploding (a) and weight exploding (b). However, our
proposed loss function prevents logit exploding (a) and weight exploding (b). Therefore, per-sample
gradient magnitudes (c) obtained based on our proposed loss function is smaller than per-sample
gradient magnitudes obtained based on cross entropy loss. In addition to this, the per-sample gradi-
ents of our proposed loss function is much more condensed. Finally, direction of the gradients (d)
of our proposed loss function is more aligned with the unclipped gradients.
3.2 An analysis of our loss
Our loss limits information loss from clipping. Figure 1 shows that under the supervision of L,
we are able to control the sensitivity of the learning algorithm. In particular, LReg on the intermediate
pre-activations and LSSE on the logit values both prevent logit values and weights from exploding,
which decreases the magnitude and variance of per-example gradients (see Equation 3). Smaller and
more condensed gradients decrease the negative impact of gradient clipping on the trajectory of the
gradient descent as shown in Figure 1.d.
Our loss yields faster convergence. LSSE improves the tradeoffs between privacy and accuracy of
DP-SGD with strong privacy budgets as LSSE converges exponentially O(e-t), where t is the steps,
to 0 loss, while cross-entropy loss only converges at O(1/t) as shown in Allen-Zhu et al. (2019).
Our loss achieves an effect similar to batch normalisation. The magnitude of gradients com-
puted when optimising LSSE with DP-SGD is smaller than for gradients computed when optimising
cross-entropy, as LSSE is defined on logit values hiM . This prevents logit values and consequently
WM from exploding (per-example logit values are computed based on the weight of the last layer).
Controlling WM can limit the changes of weights in other layers as their gradients are a function
of WM and per layer gradient function is locally lipschitz. Preventing logits from exploding may
also help recover the generalisation boost of batch normalisation (Dauphin & Cubuk, 2021). Indeed,
batch normalisation cannot be used in differentially private learning because the technique shares
information across different training examples contained in a single minibatch. This violates the pri-
vacy analysis of DP-SGD. Therefore, our proposed LSSE penalty may also be beneficial to learning
with DP-SGD because it achieves a similar effect to batch normalisation (Dauphin & Cubuk, 2021).
Our loss improves tolerance to noise. Wang et al. (2021) showed that smoother loss functions
improve the generalisation bounds and accuracy of DP learning by making the loss surface more
tolerant to noise. The most common smoothness notion is based on the Lipschitz constant of the
gradient of a function.
Definition 1 (Smoothness (Nesterov, 2003)) A loss function L is β-smooth if
kVLw - NLwk2 ≤ βkw - w0k2,
where VLw is the gradient of the loss function with respect to the weights and β is the smoothness
constant. Smaller β indicates a smoother function.
We now derive a lemma illustrating how our loss also favours smoothness of the loss being optimised
by DP-SGD.
Lemma 1 When singular values are bounded by σmax, we have for all w and w0 in a simply con-
nected set that kVLw - VLw0 k2 ≤ σmaxkw - w0k2.
5
Under review as a conference paper at ICLR 2022
Proof.
Let Hw be the hessian of the loss function with respect to the weights w.
∣∣VLw - VLwO∣∣2 = k( / Hw+t(w-w0) ∙ (w - w0)dt)k2 ≤ /	∣∣Hw+t(w-w0) ∙ (w - w0)∣∣2dt
00
≤	σmax∣w - w0 ∣2dt ≤ σmax∣w - w0∣2 ,
0
(9)
where σmax is the maximum of the first singular value along the line from w to w0.
Therefore, the existence of local bounds on the singular values results in local bounds on the smooth-
ness. We empirically observe in Figure 2 that the singular values of our loss function are smaller
than cross-entropy loss function. This is particularly true at the beginning of training. This, thus,
suggests that our proposed loss function is smoother and more noise-tolerant than cross-entropy.
4 Validation
We validate the benefit of deploying our proposed loss
function in improving how DP-SGD tradeoffs privacy
and accuracy. To do so, we show our loss further im-
proves the state-of-the-art results of the two approaches
for deep learning with DP described in Section 2-1)
CE-T-CNN (Papernot et al., 2021): using the cross-
entropy loss function together with an end-to-end CNN
classifier equipped with Tanh activation functions; 2)
CE-T-PFE-CNN (Tramer & Boneh, 2021): using the
cross-entropy loss function, this time combining the Scat-
terNet public feature extractor (PFE) with a private CNN
classifier whose internal activations are also Tanh. To en-
sure a fair evaluation, we use the same datasets, MNIST,
FashionMNIST and CIFAR10, and the same classifiers
(described in Appendix A) as these two baseline ap-
Figure 2: Analysing the smoothness of
the landscape of our proposed loss func-
tion and cross-entropy on MNIST. Our
loss function obtains smaller σmax (the
maximum of the first singular value),
making it more smooth than cross en-
tropy.
proaches. In addition to this, we fix the DP-SGD configuration across all approaches to the best
hyperparameter values reported in the hyperparameter search done by Tramer & Boneh (2021) (see
Appendix B). Note that our results would only be improved by additional hyperparameter search on
the DP-SGD configuration. For the hyperparameters introduced by our own loss function, we use
a search on the values as described in detail in Appendix B. We note that none of the prior work
we compare against captures the privacy cost of the hyperparameter search itself when reporting DP
guarantees achieved. The cost of this search should however be moderate, as analysed by Liu &
Talwar (2019).
4.1 Improving the tradeoffs between privacy and accuracy
Figure 3 and 4 compare the privacy-accuracy tradeoffs achieved by our proposed loss function and
prior approaches for DP training. Coloured regions represent the range of model accuracy with
respect to the privacy budget ε needed to achieve this accuracy. Experiments are repeated 5 times;
the minimum, median and maximum of the 5 experiments are each highlighted by a line.
When learning without privacy, end-to-end classifiers achieve a test accuracy on CIFAR10, Fash-
ionMNIST and MNIST of 76.6%, 89.4% and 99.0%, respectively. Instead, when learning with
DP-SGD, the maximum test accuracy (across 5 runs) of our first baseline CE-T-CNN for ε < 3 are
59.8%, 86.9%, 98.2% on CIFAR10, FashionMNIST and MNIST, respectively. This illustrates how
CIFAR10 is the most challenging dataset for DP-SGD training among the datasets we considered:
there is about 20% gap in accuracy between the privacy-preserving and non-privacy-preserving set-
tings. Our proposed loss function reduces this gap significantly, even more so for CIFAR10 than
simpler datasets like FashionMNIST and MNIST. For example, training an end-to-end classifier
using our loss function on CIFAR10 achieves 63.2% test accuracy, instead of 59.8% with the cross-
entropy loss. While our second baseline CE-T-PFEJCNN sees stronger accuracy than the first
6
Under review as a conference paper at ICLR 2022
Privacy budget ε
FashionMNIST
Privacy budget ε
Privacy budget ε
Figure 3:	Deployment of our loss function in DP-SGD training of an end-to-end classifier for 5 runs.
Comparison between the test accuracy of CE-T-CNN (PaPemot et al., 2021)( ) and O-T-CNN,
ours (	), as a function of privacy budget ε.
70
60
50
40
30
CIFAR10
1 1.5 2 2.5 3
Privacy budget ε
FashionMNIST
90
85彳尸
80
/
75
,
70
/
65 ------------->
1 1.5 2 2.5 3
Privacy budget ε
MNIST
100
$
90
85 ------------------≡
1.5 2 2.5 3
Privacy budget ε
Figure 4:	Deployment of our loss function in DP-SGD training of a public feature extractor followed
by a classifier for 5 runs. Comparison between the test accuracy of CE-T-PFE-CNN (Tramer &
Boneh, 2021) ( ) and O-T-PFEjCNN, ours ( ), as a function of privacy budget ε.
baseline CE-T-CNN because the learner additionally has access to public data, our proposed loss
function also improves its ability to tradeoff privacy and accuracy across the entire ε regime.
The improvement of our proposed loss function to privacy-accuracy tradeoffs is strongest in the
initial epochs. These epochs correspond to stronger privacy guarantees (i.e., smaller values of ε)
because the privacy guarantee of each individual step of DP-SGD needs to be composed across the
entire training run; hence each step further increases the privacy budget expended. For example, our
proposed O-T-CNN trains an end-to-end classifier with 55.4% test accuracy for ε < 1.5, compared
to 45.9% only with our first baseline CE-T-CNN. In another example, O-T-PFEJCNN improves the
test accuracy of our second baseline CE-T-PFEJCNN from 64.5% to 66.8% for ε < 2.
4.2 Analysis of Gradients computed by DP-SGD for O-T-CNN
We first analyse the impact of the loss function on the model weights, pre-activations, and logit val-
ues. Figure 5 (first row) shows the impact our loss function has on model weights. We visualise the
average l2 norm of model weights at each epoch. Across the three datasets, optimising CE-T-CNN
with DP-SGD leads to model weight explosion. This is not the case when learning with our pro-
posed loss. In particular, note how O-T-CNN prevents last layer weights from exploding, which
is important because they contribute to the gradients for all layers. Next, Figure 5 (second row)
shows the impact of our loss function on pre-activation and logit norms. Although CE-T-CNN
uses bounded tanh activation functions to prevent activations from exploding, the pre-activations
and logits still explode because model weights are unbounded and explode themselves (as visual-
ized above). Instead, replacing the cross-entropy loss function with our loss function decreases this
phenomenon for both pre-activations and logits. This is explained by our pre-activation regulariser
penalty but also the fact that the sum-squared-error penalty is defined over logits.
Reducing the bias of gradient clipping. We provide the histogram of per-example gradient l2
norms in Figure 6. For better visualisation, we clip the x-axis between [0, 100] and use a log scale
for the y-axis. The gradient l2 norms of O-T-CNN are smaller than their counterpart for CE-T-CNN.
This is a consequence of our observations above: our proposed loss function prevents several terms
including in the gradient computation from exploding. To further demonstrate the importance of
better preconditioning gradients to the clipping performed by DP-SGD, we visualize the divergence
7
Under review as a conference paper at ICLR 2022
mron 2l
Second layer (C)
UnoU-
00
63
1
10 20 30
Training epochs
65432
Second layer (F)
80
60
40
20
0
Last layer (F)
Training epochs
Training epochs
Training epochs
Figure 5:	l2 norm of weights (first row) and pre-activations (second row) in CE-T-CNN (-)
and O-T-CNN (—) using CIFAR10 (C) and FaShiOnMNIST (F). See Appendix C for results of
MNIST and other layers.
Epochl	Epochs
EpochlO	Epoch29
Figure 6: Histogram ofl2 norm of per-example gradient in CE-T-CNN and O-T-CNN on CIFAR10.
Histogram for other epochs and other datasets are presented in Appendix E.
O 20	40 CO βθ IOO O 20	40 CO βθ IOO O 20	40	¢0	80 IOO O 20	40	¢0	80 IOO
2-nornι grads (clipped In IO1IOOD
CIFAR10	FashionMNIST	MNIST
SJUnoD
00000
8642
.4 -.2 0 .2 .4 .6 .8 1
20
10
0 —i::::—
-.4-.2 0 .2 .4 .6 .8 1
Cosine	Cosine	Cosine
Figure 7:	Cosine similarity between aggregated of clipped per-example gradients and aggregated of
unclipped per-example gradients in first epoch of CE-T-CNN (-) and O-T-CNN (-----).
between the path taken by aggregated clipped per-example gradients and aggregated unclipped per-
example gradients in Figure 7. For each minibatch, we compute the cosine similarity between
the average of per-example clipped gradients and the average of per-example unclipped gradients.
The direction of clipped gradients in O-T-CNN is more aligned with the direction of their unclipped
counterparts than is the case for the baseline approach. This suggests that our proposed loss function
decreases the information loss and bias introduced by DP-SGD.
Learning hard vs. easy examples. Finally, we visualise the confusion matrices of per-class train-
ing accuracy for non-private learning with SGD, private learning with the CE-T-CNN baseline, and
private learning with our approach O-T-CNN in Figure 8. Qualitatively speaking, the per-class
training accuracy of O-T-CNN is closer to the non-private one than is the case for the baseline
CE-T-CNN. During the initial epochs of learning, O-T-CNN converges faster especially on easy-
to-learn examples thanks to the sum-squared-error loss function. For example, the training accuracy
of O-T-CNN for “car” and “truck” are 51% and 48%, while CE-T-CNN only achieves 34% and
37%, respectively, on these classes. In addition to this, O-T-CNN deals better with hard-to-learn ex-
amples (e.g. “cat”, “dear” or “dog” classes) in later epochs thanks to the focal loss. For example, the
training accuracy of “cat” class is 49% at epoch 20 of O-T-CNN, but is only 35% for CE-T-CNN.
4.3 Ablation study
Next, we present an ablation study to tease out the contribution of each of the three components
of our proposed loss when it comes to improving privacy-accuracy tradeoffs in deep learning with
8
Under review as a conference paper at ICLR 2022
IM
cat
4ecr
dos
二；
11
722134S19M
34 ɪ 4	2	7 lð ∙ IS U
M 7 S S ∙ 3∙
S «	11	< IS M
S 7	7
4	7
3	。	3
n 2 «
11 1 «
S <	«
ι⅛o
2
3
hone
13
ɪɪ*
MXfe
4
Nnne car M e ðwr 痴 free hone gHp tπxfc
14 ɪ 9 ɪ 9	7 IS U 37
<	3 S S «
S2<f∙Z!K>13
S13O<13X1M
S
4
S
4
M S S S M It
Figure 8:	Confusion matrices that show per-class training accuracy of O-T-CNN (first column),
CE-T-CNN (second columns) and non-private SGD approach (last columns). The first and second
row show confusion matrices of CIFAR10 at epoch 2 and 20, respectively. See Appendix D for the
results of other epochs as well as FashionMNIST and MNIST datasets.
Privacy budget ε
FashionMNIST
Privacy budget ε
Privacy budget ε
Figure 9:	Ablation study of DP-SGD training of an end-to-end classifier (first row) and a public
feature extractor followed by a classifier (second row) for 5 runs. Loss functions under study: CE,
SSE, SSE+CE, SSE+Focal and SSE+Focal+Regulariser.
DP, namely sum-squared-error, focal loss and regularisation penalty. To do that we consider four
combinations of these penalties: only using sum-squared-error, a combination of sum-squared-error
and cross-entropy, a combination of sum-squared-error and focal loss, and finally our proposed loss
function that combines all three-sum-squared-error, focal loss and regulariser. We do not consider
other combinations because they are not meaningful. To ensure a fair evaluation, we perform a
hyperparameter search for the combination of sum-squared-error and cross-entropy as well as the
combination of sum-squared-error and focal loss, similarly to the search performed for our pro-
posed loss function (see Appendix B). The privacy-accuracy tradeoffs of these four combinations is
shown in Figure 9. While the sum-squared-error improves the privacy-accuracy tradeoff in initial
epochs, the focal loss improves it for later epochs. Finally, imposing a regularisation penalty on
pre-activations further improves these privacy-accuracy tradeoffs. Our proposed loss thus achieves
the best possible performance across each of these settings and none of its penalties can be omitted.
5	Conclusion
In this paper, we showed that designing a loss function tailored to the specificities of DP-SGD,
namely per-example gradient clipping and a limited number of iterations, significantly improves
tradeoffs between privacy and accuracy. We reduced the bias introduced by gradient clipping using
a pre-activation regularisation term. In addition to this, we improved the convergence speed by
imposing a curriculum structure in learning with the sum-squared-error and focal loss. As future
work, we will evaluate our loss function for DP-SGD in other domains such as text.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
We submit our code as the supplementary material. Our code is based on the public Opacus library
in PyTorch. We use the publicly available datasets. For the theoretical smoothness result, we provide
clear explanations of lemma and complete proof.
7	Ethics Statement
Our work improves the tradeoffs between privacy and accuracy of training machine learning models
in privacy-preserving setting. Therefore, we expect the broader impact of our work to be generally
positive especially in privacy-sensitive applications such as health-care or language modeling where
we work with sensitive datasets in practice.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the ACM SIGSAC Conference
on Computer and Communications Security (CCS), Vienna, Austria, October 2016.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the International Conference on Machine Learning (ICML),
Long Beach, California, USA, June 2019.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In Proceedings of the IEEE Symposium on Foundations of
Computer Science (FOCS), Washington, USA, October 2014.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the International Conference on Machine Learning (ICML), Montreal Quebec,
Canada, June 2009.
Nicholas Carlini, Chang Liu, UJlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Eval-
uating and testing unintended memorization in neural networks. In Proceedings of the USENIX
Security Symposium, Santa Clara, California, USA, August 2019.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal ofMachine Learning Research (JMLR), 12(3):1069-1109, 2011.
Yann Dauphin and Ekin Dogus Cubuk. Deconstructing the regularization of BatchNorm. In Pro-
ceedings of the International Conference on Learning Representations (ICLR), Virtual, May 2021.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efficiently improving generalization. In Proceedings of the International Conference on
Learning Representations (ICLR), Virtual, May 2021.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the ACM SIGSAC Conference
on Computer and Communications Security (CCS), Denver Colorado, USA, October 2015.
Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R Scott, and
Dinglong Huang. Curriculumnet: Weakly supervised learning from large-scale web images.
In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,
September 2018.
Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang. To-
wards practical differentially private convex optimization. In Proceedings of the IEEE Symposium
on Security and Privacy (SP), San Francisco, California, USA, May 2019.
10
Under review as a conference paper at ICLR 2022
TsUng-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object
detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV),
Venice, Italy, October 2017.
Jingcheng LiU and KUnal Talwar. Private selection from private candidates. In Proceedings of the
ACM SIGACT Symposium on Theory of Computing, Phoenix, Arizona, USA, JUne 2019.
YUzhe Ma, Xiaojin ZhU, and JUstin HsU. Data poisoning against differentially-private learners: At-
tacks and defenses. In Proceedings of the International Joint Conference on Artificial Intelligence
(IJCAI), Macao, China, AUgUst 2019.
Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv KUmar. Can gradient
clipping mitigate label noise? In Proceedings of the International Conference on Learning Rep-
resentations (ICLR), New Orleans, Los Angeles, USA, May 2019.
YUrii Nesterov. Introductory lectures on convex optimization: A basic course, volUme 87. Springer
Science & BUsiness Media, 2003.
Nicolas Papernot, AbhradeeP Thakurta, Shuang Song, Steve Chien, and UJlfar Erlingsson. Tempered
Sigmoid activations for deep learning with differential privacy. In Proceedings of the Association
for the Advancement of Artificial Intelligence (AAAI), Virtual, February 2021.
Md Atiqur Rahman, Tanzila Rahman, Robert Laganiere, Noman Mohammed, and Yang Wang.
Membership inference attack against differentially private deep learning model. Transactions
on Data Privacy,11(1):61-79, 2018.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In Proceedings of the IEEE Symposium on Security and
Privacy (SP), SAN JOSE, California, USA, May 2017.
Congzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining (KDD), Anchorage, Alaska, USA, August 2019.
Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more
data). In Proceedings of the International Conference on Learning Representations (ICLR), Vir-
tual, May 2021.
Wenxiao Wang, Tianhao Wang, Lun Wang, Nanqing Luo, Pan Zhou, Dawn Song, and Ruoxi Jia.
DPlis: Boosting utility of differentially private deep learning via randomized smoothing. Privacy
Enhancing Technologies Symposium (PETS), 2021(4):163-183, 2021.
Yiru Wang, Weihao Gan, Jie Yang, Wei Wu, and Junjie Yan. Dynamic curriculum learning for
imbalanced data classification. In Proceedings of the IEEE International Conference on Computer
Vision (ICCV), Seoul, Korea, October 2019.
Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on
differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the
ACM International Conference on Management of Data, Chicago Illinois, USA, May 2017.
Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Gradient perturbation is underrated
for differentially private convex optimization. In Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI), Yokohama, Japan, July 2020.
Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient
embedding perturbation for private learning. In Proceedings of the International Conference on
Learning Representations (ICLR), Virtual, May 2021.
Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efficient private ERM for smooth ob-
jectives. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),
Melbourne, Australia, August 2017.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In International Conference on Learning Rep-
resentations (ICLR), New Orleans, Los Angeles, USA, May 2019.
11
Under review as a conference paper at ICLR 2022
Table 1: The architecture of end-to-end CNNs for Table 2: The architecture of end-to-end CNNs for
Fashion-/MNIST (PaPemot et al., 2021).	CIFAR10 (PaPemot et al., 2021).
Layer	Parameters
Conv	16 filters of 8x8, stride 2, padding 2
MP	2x2, stride 1
Conv	32 filters of 4x4, stride 2, padding 0
MP	2x2, stride 1
FC	32 units
FC	10 units
Layer	Parameters
Conv x2	32 filters of 3x3, stride 1, padding 1
MP	2x2, stride 2
Conv x2	64 filters of 3x3, stride 1, padding 1
MP	2x2, stride 2
Conv x2	128 filters of 3x3, stride 1, padding 1
MP	2x2, stride 2
FC	128 units
FC	10 units
Table 3: The architecture of CNNs trained on top Table 4: The architecture of CNN trained on
of Public ScatterNet feature extractor for Fashion- toP of Public ScatterNet feature extractor for CI-
ZMNIST (Tramer & Boneh, 2021).____________________ FAR10 (Tramer & Boneh, 2021).
Layer	Parameters
Conv	16 filters of 3x3, stride 2, padding 1
MP	2x2, stride 1
Conv	32 filters of 3x3, stride 1, padding 1
MP	2x2, stride 1
FC	32 units
FC	10 units
Layer	Parameters
Conv x2	64 filters of 3x3, stride 1, padding 1
MP	2x2, stride 2
Conv x2	64 filters of 3x3, stride 1, padding 1
MP	2x2, stride 2
FC	10 units
A Classifiers
Table 1 and Table 2 show the architecture of end-to-end CNNs for MNIST, FashionMNIST and
CIFAR10, suggested by PaPernot et al. (2021). Table 3 and Table 4 show the architecture of CNNs
trained on ScatterNet features for MNIST, FashionMNIST and CIFAR10, suggested by Tramer &
Boneh (2021).
B Hyperparameters
Table 5 rePorts hyPerParameter values of the DP-SGD configuration across all aPProaches, sug-
gested by Tramer & Boneh (2021). Table 6 reports the hyperparameter values of our proposed loss
function, obtained through a hyPerParameter search on values rePorted in Table 7. However, we
evaluate the performance of our loss function for each hyperparameter value using all three datasets
in Figure 10. We can observe that the impact of hyperparameter values on the performance of our
loss function is negligible. Therefore, the improvement of our loss function is not sensitive to the
hyperparameter values.
Table 5: Training hyperparameter values obtained through line search Tramer & Boneh (2021).
Dataset	∣ Learning rate Momentum Batch size Epochs Noise scale Clipping norm	
	End-to-end CNN		
CIFAR10 Fashion-MNIST MNIST		1	0.9	1024	30	1.54	0.1 4	0.9	2048	40	2.15	0.1 0.5	0.9	512	40	1.23	0.1
CNN trained on ScatterNet features	
CIFAR10 Fashion-MNIST MNIST		4	0.9	8192	60	5.67	0.1 4	0.9	2048	40	2.15	0.1 1	0.9	1024	25	1.35	0.1
12
Under review as a conference paper at ICLR 2022
Table 6: The value of the hyperparameter of our loss function.
	CIFAR10	FaShionMNIST	MNIST	
O-T-CNN	et = 7, β = 11, γ = 5	et =0, β = 1, Y = 5	et = 0, β = 1, Y = 5
O-T-PFE-CNN	et = 2, β =1, Y = 2	et = 2, β = 51, Y = 2	et = 2, β = 11, Y = 10
Table 7: Hyperparameters for our proposed loss function.
Parameter	Value
et β		Start=0, End=10, SteP-Size=1 Start=1, End=200, SteP-Size=10
C	l2 NORM OF PRE-ACTIVATION AND WEIGHTS
Figure 11 and Figure 12 show the l2 norm of weights and pre-activation of models trained by DP-
SGD approaches on CIFAR10, FashionMNIST and MNIST.
D Confusion matrices of per-class accuracy
Figure 13 visualises the confusion matrices of per-class training accuracy for non-private learning
with SGD, private learning with the CE-T-CNN baseline, and private learning with our approach
O-T-CNN using CIFAR10, FashionMNIST and MNIST.
E	Per-example gradients
Figure 14	provides the histogram of per-example gradient l2 norms in CE-T-CNN and O-T-CNN
on CIFAR10, FashionMNIST and MNIST.
F Comparing SSE with SAE and Huber
In this section, we analyse the performance of SSE, Sum Absolute Error (SAE) and Huber-Loss in
DP-SGD training.
The SAE is defined as:
D
LSAE = |hiM -yi|,	where	|hiM -yi| = X |(h(Mi,d) -y(i,d))|.
d=1
(10)
The d-th element of Huber loss is defined as:
LHuber(d)
11 (hMd)- y(i,d))2/e
1|(hMd)- y(i,d))|- 2β
|(h(Mi,d) - y(i,d))| < β
otherwise
(11)
Figure 15	shows the tradeoffs between the privacy and accuracy of DP-SGD training using SSE,
SAE and Huber loss. The performance of SSE is better than SAE. The performance of Huber
loss increases as we increase β, activating SSE part and deactivating SAE part of Huber loss in
Equation 11.
13
Under review as a conference paper at ICLR 2022
FashionMNIST
CIFAR10
Figure 10: Test accuracy of O-T-CNN as a function of hyperparameter values of our loss function.
MNIST
8642 6 4 2 6 4
..........
3333
mron 2l mron 2l mron 2l
Second layer
Last layer
01RAFIC
Training epochs
Training epochs
Training epochs
2
Training epochs
Figure 11:	l2 norm of model weights in CE-T-CNN (-) and O-T-CNN (-) using CIFAR10,
FashionMNIST and MNIST.
First layer
mron 2l mron 2l mron 2l
00
32
10 20 30
10 20 30 40
1 10 20 30 40
Training epochs
Second layer
100
80
60
40
Third layer
60
40
20
1 10 20 30 40
Training epochs
1 10 20 30 40
Training epochs
Last layer
15
10
5
1 10 20 30 40
Training epochs
OlXVHID ISIN2工 ISIN2
Figure 12:	Pre-activation l2 norm per layer of CE-T-CNN (-) and O-T-CNN (--) on CIFAR10,
FashionMNIST and MNIST.
14
Under review as a conference paper at ICLR 2022
plane
car
bM
cat
⅛w
do⅞
⅛⅛
horaβ
*⅛>
inκk
13
5
6
3
2
3
21
5
plane ca∣∙
ɪ	1	3	3	3	6
0	112	5	5
6	15	5	23	9	13	8
8
4
8
7
10
11
24
21
7
5
7
23
2
plane-	44	7	2
or-	5	34	ɪ
bM-	ɪɪ	10	7
eβt-	5	9	6
deo∙-	4	5	3
dg-	4	ɪθ	5
⅛⅛- 243
horeβ-	3	9	2
Sh⅜>-	23	9	ɪ
track-	4	14	1
plane ear bH
2	1	3	4	5	19	14
4	2
5	5
11	4
5	7
9	5
5	5
4	6
3	O
3	ɪ
cat	dw
7	10	8	15 lβ
8	34	9
19	26	ɪɪ
26	26	11
6	10
8	22	30
1	4
0	3
2	13
4	1	3	g	16
3	7	IS	lβ	37
d<κ	f⅛w	horeβ	»Hp	track
plane
car
bM
cat
dee<∙
dg
froβ
how
8h⅜)
track
2	3	11
Illl
7	M	8	8
3
4
1
1
2
10
4
plane car
11
4
2
4
11
1
1
1
1
1
1
5
13
a 6
2	6
2
1
5
a) Initial epoch 2 in CIFAR10
α
α
α
α
α
15
α
α
4
α
2
α
3
α
α
α
α
α
α
3
α
12
l≈s
l⅝EU
T-sħlrt-^Q α
l)ŋw	O
Hillaw-	1	S
ESS-	4	1
CMt-	β	β
Sandal -	β	。
SHrt- 17 β
Sneakv- O O
Bag- 。 β
Λn Ideboot-
c) Initial epoch 2 in FashionMNIST
TelM
Trower
Fullqwr
Drvss
CMt
Sandal
SHrt
Sieaker
Bag
AnMeboat
I-*0:1W
l≈s
l⅝EU
T-sħlrt-^^V α
l)ŋw β
Hillaw-	1	S
Otos-	1	S
CMt-	β	α
Sandal -	β	。
SHrt-	9	S
Sneakv-	β	。
Bag-	。	β
AnMeboat-。	。
1	I
β
α
α
*
d)	Late epoch 20 in FashionMNIST
e)	Initial epoch 2 in MNIST
f)	Late epoch 20 in MNIST
Figure 13:	Confusion matrices that show per-class training accuracy of O-T-CNN (first column),
CE-T-CNN (second columns) and non-private SGD approach (last columns) using CIFAR10, Fash-
ionMNIST and MNIST.
15
Under review as a conference paper at ICLR 2022
Epochl	Epochs	EpochlO	E□och29

20	40	¢0	80	100	0	20	40 CO βθ IOO O 20	40 CO 80 IOO O 20	40 CO 80 IOO
2-norm grads {clipped In 10.1OoD
Epochl	Epochs	EpochlO	E□och39
0	20	40 CO «0	100	0	20	40 CO «0	100	0	20	40 CO «0	100	0	20	40 CO «0	100
2-nαrm grads {clipped In [0Λ00D
Epochl	Epochs	EpochlO	Epoch39
0	20	40 CO 80	100	0	20	40 CO «0	100	0	20	40 CO 80 IOO O 20	40 CO 80 IOO
2-normgrads (clipped In I0,100D
Figure 14:	Histogram ofl2 norm of per-example gradient in CE-T-CNN and O-T-CNN on CIFAR10
(top row), FashionMNIST (middle row) and MNIST (bottom row).
ycarucca tse
Figure 15:-DP-SGD training of an end-to-end classifier using SSE , SAE and HUber loss
(β = .1 , β = .2 , β = .3 , β = .4 , β = .5 , β = .6 , β = .7 ,
β = .8	, β = .9----and β =1-------).
16