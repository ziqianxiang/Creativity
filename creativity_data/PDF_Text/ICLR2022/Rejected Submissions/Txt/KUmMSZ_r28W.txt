Under review as a conference paper at ICLR 2022
Particle Based Stochastic Policy Optimiza-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic policy has been widely applied for its good property in exploration
and uncertainty quantification. Modeling policy distribution by joint state-action
distribution within the exponential family has enabled flexibility in exploration and
learning multi-modal policies and also involved the probabilistic perspective of
deep reinforcement learning (RL). The connection between probabilistic inference
and RL makes it possible to leverage the advancements of probabilistic optimiza-
tion tools. However, recent efforts are limited to the minimization of reverse KL
divergence which is confidence-seeking and may fade the merit of a stochastic
policy. To leverage the full potential of stochastic policy and provide more flexible
property, there is a strong motivation to consider different update rules during
policy optimization. In this paper, we propose a particle-based probabilistic pol-
icy optimization framework, ParPI , which enables the usage of a broad family
of divergence or distances, such as f -divergences and the Wasserstein distance
which could serve better probabilistic behavior of the learned stochastic policy.
Experiments in both online and offline settings demonstrate the effectiveness of the
proposed algorithm as well as the characteristics of different discrepancy measures
for policy optimization.
1	Introduction
Deep reinforcement learning (DRL) leverages the power of neural-network function approximators
and has shown great promise in a diverse field, such as control (Lillicrap et al., 2015; Andrychow-
icz et al., 2020), robotics (Haarnoja et al., 2017; Gu et al., 2017; Plappert et al., 2018). Classic
views on the notion of optimality in reinforcement learning state that an optimal policy can be
deterministic (Sutton & Barto, 2011). However, recent studies by Daniel et al. (2012); Ziebart (2010)
demonstrate that deterministic policy may suffer from several drawbacks, e.g., the multi-modal
behaviors have significant applications in real robotic control tasks (Daniel et al., 2012) and inflex-
ibility on exploration (Ostrovski et al., 2017). Particularly in offline RL, the deterministic policy
lacks the structure to manage risk towards uncertainty dynamics. The branch of probabilistic RL
is then introduced where the optimal solution is stochastic. Probabilistic RL not only enables the
stochasticity of the learned policy but also provides a different perspective to reveal the connection
between reinforcement learning and probabilistic inference (Todorov, 2008), where the optimization
procedure of the policy could be considered a statistical divergence minimization procedure towards
distribution in the state-action or trajectory space, which is implied by the corresponding optimal RL
policy.
Besides the different definitions of optimality and above mentioned priorities, another appealing prop-
erty of probabilistic reinforcement learning lies in that advanced techniques in approximate inference
can then be applied in the optimization for RL. There are two widely applied frameworks, i.e., the
pseudo-likelihood framework and maximum entropy (MERL) framework. The pseudo-likelihood de-
fines the optimality by considering a specified positive and bounded function of reward as the density
function. By regressing to the reweighted samples, the pseudo-likelihood based methods not only is
not an explicit probabilistic model but also suffers from high variance and poor risk handling (Levine,
2018). MERL is a prominent choice for policy improvement for stochastic policy which tends to
maximize the expected reward and the expected conditional entropy. It avoids the drawbacks of
pseudo-likelihood that it suffers from high variance and poor risk handling ability (Levine, 2018).
In practice, the MERL is generally implemented with the reverse KL minimization which promotes
1
Under review as a conference paper at ICLR 2022
the mode-seeking behavior and could fade the merits of a stochastic policy. For example, in offline
setting, the mode-seeking policy will result in an overly-optimistic estimation of Q-function (Kumar
et al., 2020) which could be problematic when generalizing in the test scenario.
In this work, we explore the possibility of using a broad class of distribution discrepancies for
stochastic policy improvement, and find it is possible by leveraging particle-based methods. By
providing better distribution matching methods, our proposed framework, ParPI , enjoys the power of
probabilistic RL. Moreover, ParPI is naturally compatible with the offline RL setting. And we show
that with specific discrepancy measure selected, ParPI could conjoin the benefit of different policy
constraint methods. We firstly demonstrate that using a broader class of distribution discrepancies
achieves significant improvements in stabilizing the policy optimization and achieving the state-of-
the-art expected returns on high-dimensional and complex continuous control tasks (Todorov et al.,
2012). Besides, we conduct extensive experiments in the offline setup (Yu et al., 2020) to show that
the ParPI could be more defensive for optimization thus results in more generalizable optimal policy.
2	Preliminaries
2.1	Probabilistic Inference Framework for RL
The reinforcement learning (RL) problem can be modeled as the following infinite-horizon Markov
Decision Process (MDP), which is implied by the tuple hS, A, r,p,p0, γi. We consider the common
cases where the state space S and action space A are assumed to be continuous. p(St+1|St, at) refers
to the transition probability, i.e. the probability density of next state St+1 given the current state
St and action at . when an agent chooses an action a in state S, the environment emits a reward
r(s, a) : S ×A→ [rmin, Tmax] during the transition. The policy distribution π(a∣s) is presented as
a distribution over the action space conditioned on any state. The distribution of state-action pairs
when navigating the environment with policy π could be defined as ρπ(S, a), which is also referred
to as occupancy measure (Ho & Ermon, 2016), and the marginal distribution on the state space is
ρπ(S). With a predefined MDP, the notation trajectory τπ refers to the following Markov sequence
τ∏ = {(s0,a0), •一(ST, aτ)} generated by interacting with environment under policy π. The discount
factor is denoted as γ . Standard reinforcement learning seeks to find the optimal policy that could
maximize the expected return, and the optimal solution could be deterministic (Sutton & Barto, 2011).
However, considering the exploration (Schulman et al., 2015), robustness (Ziebart, 2010) and multi-
modal objectives (Daniel et al., 2012) in real-world application scenarios, a stochastic/multi-modal
policy is desired. To address this desideratum, optimal maximum entropy policy (Ziebart et al., 2008;
Haarnoja et al., 2017) is defined as:
∏* ：= argmaxEρ∏(st,at)[r(st,at) + αH[π(∙∣St)]].
and the soft Bellman optimality equation (Ziebart, 2010) is defined as:
Q* (St, at) = Tt + YEst+1 [V*(St+1)],
(1)
which gives the soft Bellman operator TπQπ(∙) = Qπ(∙) and Tπ∙ := r(h) + γE%,〜p(sθ∣h)∏(aθ∣sθ)[∙]
and also referred to as Maximum Entropy RL(MERL). Ziebart (2010); Haarnoja et al. (2017) and
Haarnoja et al. (2018) show that the target of the policy update is given by the Q-function in an energy-
based form, π(a∣s) 8 exp(-Q(s, a)∕α). Due to the intractability in estimating the normalizing
constant Z(S) = ʃ exp(-Q(s, a)∕α)da for continuous or large discrete action space, the energy-
based form poses difficulties in utilizing the policy such as sampling or density estimation, so various
methods use a standalone parametric policy model ∏φ(a∣s) to distill from the energy function, and
the MERL task is turned into iteratively updating the value model Qθ and the policy model ∏φ(a∣s).
3	Particle-Based Policy Improvement
3.1	Distribution Matching for Policy Improvement
We aim to update a policy model to match a better target p(a|s), which is demonstrated by particles.
We now formalize it as a distribution matching task under a general distribution metric/discrepancy
D[∙k∙]. The first problem is that the policy model parameterizes a conditional distribution ∏φ(a∣s), so
We need to match the targetp(a∣s) by minimizing D(∏φ(a∣s),p(a∣s)), however, it is intractable for
2
Under review as a conference paper at ICLR 2022
any given s. To make the minimization practical, we note that in almost all modern policy update
tasks, we also have an associated state trajectory, e.g., experiences in a replay buffer in off-policy
RL, or simulated trajectories in inverse RL (Finn et al., 2016) and demonstrative trajectories in
imitation learning (Ho & Ermon, 2016). This state trajectory demonstrates a marginal distribution
on the state, ρ(s), which is widely assumed to be stationary among different policies within the
same MDP (Tsitsiklis & Van Roy, 1996; Haarnoja et al., 2017; 2018). Hence we instead minimize
D(P(S)∏φ(a∣s), ρ(s)p(a∣s)). When thejoint distributions match, the conditionals also match. This
idea has been exploited in other fields, such as expectation propagation algorithms (Minka, 2001)
for Bayesian inference. And we further provide discussions in the Appendix F. Along with the state
trajectory, we also have samples/particles from the augmented target, P(s)p(a|s). Particularly for ex-
isting MERL methods, they minimize the reverse KL divergence Dkl(∏φ(∙∣s),p(∙∣s)) averaged over
a state distribution P(s) which can be implemented by conducting sampling from the replay buffer:
Ep(s)[Dkl(∏φ(∙∣s),p(∙∣s)]. This objective can be reformulated as DKL(P(S)∏φ(a∣s),ρ(s)p(a∣s)),
corresponding to our proposed method with D instantiated as the reverse KL.
As stressed, this framework allows matching the distribution by minimizing more powerful met-
rics/discrepancies that do not have a particular propensity, like the reverse KL. To balance utility and
implementability, we consider the f -divergences DF and the Wasserstein distance (WD) DW as two
instances of D[∙∣∣∙] of the proposed framework.
Minimizing f-divergences for policy updates. The f -divergence is defined as DF (q, p) :=
Ep [F (q/p)], where F is a real-valued convex function with F (1) = 0. Although the forward
and reverse KL are also f-divergence instances, we do not consider them here since they would
introduce the respective propensity of the learned model. Nowozin et al. (2016) show a useful lower
bound of an f-divergence that benefits practical optimization:
Df(q,p) > sup (DF(q,P； f) ：= Eq[f (∙)] - Ep[F*(f (∙))])	⑵
where F*(t) := supχ{χt - F(x)} is the Fenchel conjugate of F. If the parametric model is defined
via a reparameterization h 〜qφ(h) : h = gφ(e) with e following a fixed and easy-to-sample
distribution q() like the standard Gaussian, the first term can also be estimated as Eq() [f(gφ())].
By also parameterizing f in some function class, the supremum can be estimated after optimizing
over f, which also tightens the lower bound. Thus minimizing DF well serves as minimizing DF.
Minimizing WD for policy updates. The WD is defined as the minimal cost of transferring from
one distribution to the other by a probabilistic transferring plan. It is shown to have an optimization
utility even when the two distributions do not have overlapping support (Arjovsky et al., 2017). Its
formulation under Kantorovich-Rubinstein duality makes it convenient to optimize:
DW(q, p) = sup Eq [f] -Ep[f]	(3)
kfkL61
For distributions q, p with a bounded expectation on a Polish metric space (Villani, 2008, Particular
Case 5.16) (e.g., the common Euclidean space), where |卜||工 denotes the Lipschitz constant of a
function. To enforce the Lipschitz constant constraint, various implementations are proposed, e.g.
parameter clipping (Arjovsky et al., 2017), gradient penalty (Gulrajani et al., 2017) and spectral norm
/ hinge loss regularizations (Miyato et al., 2018).
Framework 1. The Particle-based Policy Improvement (ParPI) framework,
minφ D(P(S)∏φ(a∣s), ρ(s)p(a∣s)) can be used for the sub-task of updating policy in RL, as
long as the improved policy can be demonstrated by particles. The process is shown in Fig. 1.
Remark 1. The ParPI framework can be applied to policy update in SQL (Ziebart, 2010; Haarnoja
et al., 2017) and SAC (Haarnoja et al., 2018). In SQL, once the optimal Q-function Q* is achieved,
the optimal policy is shown to be π*(a∣s) = exp(Q*(s, a)∕α)∕Z*(S) where Z*(S) is a normalizing
COnStant. In SAC, it is ShOwn that π0(a∣S) := exp(Qπ(s, a)∕α)∕Z(S) is a better POliCy than π
where Qπ is the Q-function under policy π. In both cases, the policy update target is given in an
energy-based form, whose particles can be drawn using MCMC algorithms.
3
Under review as a conference paper at ICLR 2022
3.2	PARPI FOR MERL
The particle-based policy improvement (ParPI)
framework can be applied to any RL task where
a state-action trajectory demonstrates a better pol-
icy. We now show in detail how it helps to up-
date policy in the MERL framework. As intro-
duced, MERL (Ziebart et al., 2008; Ziebart, 2010;
Haarnoja et al., 2017; Liu et al., 2017; Haarnoja
et al., 2018; Zhang et al., 2018) defines the op-
timal policy which also considers the entropy to
cover multiple optimalities and encourage explo-
ration. Based on the definition, a policy update tar-
get is given in the form p(a∣s) H exp(Q(s, a)∕α),
where Q(s, a) is either the optimal Q-function or the
Q-function of the current policy, which can be esti-
mated from simulated trajectories. These methods
update the policy model by minimizing the reverse
KL DKL(P(s)∏φ (a∣s),ρ(s)p(a∣s)). The merit of it is
that estimating the troublesome normalizing constant
is not required in optimization, which is also widely
TD-UPdate
ParPI
Figure 1: The Particle-based Policy Improve-
ment (ParPI) framework. The current Q-
function provides a better policy in an energy-
based form, whose particles can be drawn by
an MCMC algorithm. The policy model is
updated to match the better policy by mini-
mizing a broader class of distribution metrics
using the particles.
exploited in variational inference. However, it has been pointed out (Huszar, 2015; Theis et al., 2016)
that minimizing the reverse KL promotes the mode-seeking behavior in finite optimization steps,
making ρ(s)∏φ(a∣s) concentrate to one mode of the target ρ(s)p(a∣s). This is because a substantial
penalty can be incurred when ρ(s)∏φ(a∣s) puts more mass to where ρ(s)p(a∣s) is dilute, but a regular
loss in the other way. The behavior is also observed in practice, and there are attempts to ameliorate
it in other fields, like variational inference (Hernandez-Lobato et al., 2016; Li & Gal, 2017) As the
goal of MERL is to capture the diversity and multi-modality of policy, such behavior would weaken
the spirit of MERL.
The ParPI framework allows MERL to benefit from more powerful metrics/discrepancies, as long
as the trajectory samples are available. To draw from the target policy, we employ the Langevin
dynamics, which only requires an unnormalized density function of the target distribution.
Sampling with Langevin Dynamics. To minimize the general metrics/discrepancies above, sam-
ples from the target distribution Pn(s)p(a∣s) are required. Fortunately, this can be done by subse-
quently sampling from Pπ(s) and p(a|s), and both can be implemented. Samples from Pπ(s) can
be drawn by simulating the (s, a) trajectory following the current policy π(a∣s) and environment
transition. Sampling from p(a|s) can be done by running dynamics-based MCMC algorithms, which
only require the unnormalized density, i.e. exp(Q(s, a)∕α). From various particle-based inference
algorithms, we employ the Langevin dynamics (Roberts et al., 1996; Roberts & Stramer, 2002;
Welling & Teh, 2011) due to the low computational cost, whose transition is given by:
a(i+I) = &Ci) + εV logp(a⑴ |s) + N(0, 2εI) = a(i) + (ε∕α)VaQ(α(i), S) + N(0, 2εI)	(4)
It has been shown to achieve δ precision in total variance (Dalalyan, 2017), Wasserstein distance (Dur-
mus & Moulines, 2016) and KL divergence (Cheng & Bartlett, 2017) in O(1∕δ2) steps for strongly
log-concave densities. Note that although we can use MCMC to draw samples from the policy target,
the sampling is slow in deploying the policy, where we need to run Markov chains for every state
along the trajectory, so explicitly modeling policy ∏φ(a∣s) is still necessary.
Value Function Update. The proposed policy improvement rule described above only requires a
model for the state-action value function Qθ(s, a), so it can be applied with any value function update
method. The soft-Q learning method (Haarnoja et al., 2017) aims to update the value function towards
optimal by a temporal difference implementation based on the soft Bellman optimality equation 1:
12
JQ (θ) = E(st,at)〜D 2 (Qθ (st, at) - r (st, at) + log Est+ι~p(st+ι ∣st,at) [exp (V (st+1))] 2	(5)
4
Under review as a conference paper at ICLR 2022
The soft actor-critic (SAC) method (Haarnoja et al., 2018) instead updates the state-value function
model towards the state-action value of the current policy which gives a better policy target as
following:
12
JV(ψ) =	Est〜ρ(s)	2 (Vψ (St)	-	Eat〜∏φ	[Qθ	(st, at)	- log πφ	(at1St)D
12
JQle) =	E(st,at)〜D	2	(Qθ	(st, at)	- r (st,	at)	+ γEst+ι〜p(st+ι∣st,at)	[Vψ	(St+1)])
(6)
And we provide convergence analysis of ParPI in Appendix E.
3.3	ParPI for Offline RL
Another important advantage of ParPI lies in that it is naturally compatible with the offline RL setting.
In the offline RL setting, we only have access to some static dataset, i.e. D = {s, a, r, s0}, which is
usually collected by the behavior policy πB . And during the learning procedure, the algorithm could
not interact further with the environment. Thus the key challenge for offline RL algorithms is to
generalize beyond the state and action support of the offline data. There are two mainstream offline RL
algorithms: the value function regularization methods and the policy constrained methods. The value
function regularization methods generally refers to the methods which conservatively regularized the
estimation of value function in either model-free or model based fashion, e.g. MOPO (Yu et al., 2020)
and CQL (Kumar et al., 2020). Such methods do not have requirements on the policy improvement
procedure, thus we could just directly change the vanilla policy optimization algorithm, e.g. entropy
regularized update (Haarnoja et al., 2018), into ParPI as illustrated in Remark 1.
We mainly discuss the relationship between the policy constrained based offline RL algorithm and
ParPI . The policy constrained offline RL algorithm tends to stabilize the training by adding constraints
into the policy improvement procedure. Without loss of generality, the constrained policy objective
could be then formalized as:
Πφ = arg max Es〜D,a〜∏φ(a∣s) [Q(s,a)] s.t. D (∏φ(a∣s),∏β(a∣s)) ≤ ε
(7)
Here D(∙, ∙) stands for some discrepancy measure between two distributions, and ∏β is the behavior
policy. Two main types of policy constraints are distributional constraints and support constraint. The
distribution constraints (Nair et al., 2020; Wang et al., 2020; Peng et al., 2019; Wu et al., 2019) are
generally more strict, which could provide a stable supervision signal yet limit the search space for the
learned policy. And they are usually implemented by the minimization of density-based divergence,
e.g. KL divergence and f -divergence. In comparison, the support constraints are relatively loose and
only require the support of learned distribution to be equal to the behavior distribution. The support
constraint enlarges the search space while it could bring unstable optimization when the support
distance is large. Correspondingly, the support constraint compares samples regardless of density
which is in line with the Integral Probability Metric (IPM), e.g. Maximum Mean Discrepancy and
Wasserstein distance.
We then show that within a specific choose of divergence in ParPI , we could conjoin the benefit of
both the distribution constraints and support constraint. With the D(∙, ∙) as reverse KL divergence,
the Lagrangian duality of Eq. 7 could be formulated as: ∏φ = argmax∏φ Es〜d,。〜∏φ(a∣s) [Q(s, a)] +
αDKL (∏φ(a∣s),∏β(a∣s)) where α is the Lagrangian multiplier. The above formulation has the
closed-form solution ∏φ* (a|s) α ∏β(a|s) exp(Q(s, a)∕α) and we could then use ParPI to optimize
∏φ towards ∏φ*. Particularly, the V logp(a(i)∣s) in Eq. 4 is V∏β(a|s) exp(Q(s, a)∕α) in this case
and the initial state of langevin dynamics is set as the behavior policy πβ . And the Wasserstein
distance is used in particle optimization. Then we have the following fact:
Remark 2. With limited steps of langevin dynamics, ParPI is approximately solving the optimization
problem:
Φk+ι = argminKL(∏φ(a∣s)k nβ(a|s)exp(Q(S,a"α)) +。卬/ (∏φ,∏β) + γW^ (∏φ,∏φQ (8)
φZ
Here Z Standsfor the normalizing Constant and the W2(∙, ∙) is the W2 distance.
5
Under review as a conference paper at ICLR 2022
We leave the formal discussion of Remark. 2 in the Appendix D. The first two terms in Eq. 8
correspond to the distribution constraint and the support constraint. And the last term is a specific
property introduced by ParPI which regularizes the update of policy to be close to the current
policy. The intuition ensembles the trust region methods such as TRPO (Schulman et al., 2015) and
PPO (Schulman et al., 2017) while the regularization of ParPI is conducted according to Wasserstein
distance instead of KL divergence.
3.4	Practical Optimization with ParPI
Estimating and Minimizing the Discrepancy Measure The occupancy measure from the current
policy can be sampled by firstly sampling states from replay buffer and take action under the current
policy ∏φ(a∣s) on each state, We carefully overload the notation and refer to the corresponding
(s, a) distribution as ρφ. As introduced in Eq.4, the occupancy measure implied by the Qθ can
approximately be retained by finite steps in Langevin Dynamics. In practice, We use the samples
from ρφ as the initial state, and the corresponding distribution is referred to as ρQ .
With the samples from ρφ and ρQ available, We parameterize the discriminator(critic) as fω to
compute the objective in Eq.2 and Eq.3. More specifically, the Lipschitz constraint in minimizing
Wasserstein Distance(WD) cases is imposed by spectral normalization folloWing (Miyato et al., 2018).
The weight matrix W in the discriminator network is regularized as WSN ：= σ(Ww)W Where σ(W)
denotes the largest singular value of W . Empirically, the amortization optimization procedure of
finding the appropriate φ of policy model can lead to an intractable problem due to the complexity of
the space S × A which leashes the very purpose of function approximation. To counter this obstacle,
following the recent success in apprenticeship learning and imitation learning (Ho & Ermon, 2016),
we add an entropy regularizer to the objective of the policy(actor) model. Without loss of generality,
we reparameterize the policy model (∏φ(at∣st)) as a neural network transformation:
at = μφ(st) + σφ(sjet, j 〜p(e),
so ∏φ(a∣s) = pe(ca-μφS(S)∕σφ(s). Before each policy update, we first update the discrimina-
tive(critic) function fω for estimating DF or DW using particles of the target policy {(a0t, st)}t, by
maximizing
Jf (ω):=
'Eρ(s)∏ψ(α∣s)[logSigm(fω(s, a))] - EP(S)P(叱)[logSigm(1 - fω(s,a))]
≈ logSigm(fω(st,μφ(st) + σφ(st)ej) - logSigm(1 - fω(St, a0)), for JSD,
EP(S)∏φ(α∣s) [fω (s, a)] - EP(S)P(a|s) [fω (s, a)]
≈ fω(st,μφ(st) + σφ(st)et) - fω(st, a0),
for WD,
(9)
take Jensen-Shannon Divergence(JSD) as an example of f-divergence and Sigm is the sigmoid func-
tion. After updating ω, we update the policy model by minimizing Jn (φ) := -EP(S) [H[∏φ(∙∣s)]] +
D(ρ(s)∏φ(a|s), ρ(s)p(a∣s)) as the final objective.
Squashing Correction and Sampling in Latent Space Following the default setting in (Haarnoja
et al., 2018), we model the action distribution with an unbounded Gaussian. As the experiment
settings always limit the action space in a finite interval, we apply tanh on the samples from Gaussian.
However, if we conduct the Langevin Dynamics(Eq. 4) in the bounded support, e.g. (-1, 1), we
empirically find the method is susceptible to the selection of noise level e and some numerical issues
could raise due to that the produced samples could be out of the boundary. To alleviate the above
issues, we do the MCMC steps in the raw action space before squashing. Here, we slightly overload
the notation by denoting the random variable of the raw output as u which has infinite support and
the variable of corrected action is a = tanh(u). With the stationary distribution in the action space as
exp(Q(S, a)∕α), we could get the corresponding stationary distribution in u space by applying the
change of variable formula. The score Nu logp(u∣s) used in Langevin Dynamics, i.e. Eq. 4, is:
▽u logp(u∣s)=Vu(Q(s,tanh(u))∕α+2 PD=I ▽. log(1-(tanh(ui))
where ui denotes the i-th element of u.
3.5 Discussion and Related Works
The final objective is related to the regularized variants of apprenticeship learning algorithms in
imitation learning (Ho et al., 2016; Syed et al., 2008). While in our setting, the corresponding target
occupancy measure is defined implicitly also changes along with the optimization of Q-function
6
Under review as a conference paper at ICLR 2022
Algorithm 1 ParPI:Particle-Based Policy Improvement
1:	Input: Replay buffer D. Policy model ∏φ(a|s), parameterized Q-function Qθ, state function Vψ
and discriminator(critic) fτ .
2:	Set the step size , the length of MCMC steps K and the total iterations T .
3:	while not converged do
4:	Sample a batch of triple {(s, a, r)t}tm=1 from the replay buffer D.
5:	Conducting update on the Qθ :
6:	θi - θi - λQVθi Jq (θi) fori ∈ {1, 2}	Eq. 5 or Eq. 6
7:	ψ  ψ - λvVψ JV (ψ)	Eq.6# ParPI + SAC
8:	Sample a batch of state {st}tm=1 from D.
9:	Draw action under current policy and get the state-action pairs {st, aπφ}tm=1
10:	Fixing the state, and conduct Langevin dynamics with Qφ following equation 4 to acquire the
updated state-action pairs {st, a∏@ }m=I.
11:	ω  ω — λf Vω Jf (ω)	Eq. 9.
12:	φ  φ — λ∏VφJ∏ (φ).
13:	end while
which indicates a more difficult optimization problem. Liu et al. (2017) consider a particle-based
policy update, but rather than the target policy, their particles represent the posterior of the policy
model parameter, and the update rule is also based on minimizing the reverse KL. Zhang et al.
(2018) also propose a particle-based method for policy update by utilizing the minimal movement
discretization of the Wasserstein gradient flow to minimize, but still the reverse KL. Many recent
methods learn energy-based models especially with deep models (Du & Mordatch, 2019; Song
& Ermon, 2019; Li et al., 2019a), while we instead learn a parametric model to approximate a
distribution defined by an energy function for efficient prediction. We note that there are other
methods to do so like the amortized MCMC methods (Li et al., 2017; 2019b; Feng et al., 2017),
but the stationality-oriented objective may not be efficient for directly matching the target, and we
cover more scenarios where only the particles are available. Our method is also related to the policy
constraint methods in the offline RL scenario. Wu et al. (2019) and Kumar et al. (2019) proposed to
constrained the learned policy with f -divergence or IPM, while their methods differ from us in the
fact that the target occupancy could be directly acquired from the static dataset and the divergence
minimization term is only served as a regularizer.

,:;
# frames (millθn)
12000
10000
0.4	0.6
# frames (million)
Hoρper-v3
0	0.2	0.4	0.6	0.8	1.0
# frames (million)
——SQL
——SAC
---ParPIJSD+SQL
--- ParPI JSD+SAC
--- ParPI WD+SQL
---ParPI WD+SAC
---ParPI RKL+SQL
--- ParPI RKL+SAC

Figure 2: Training curves on continuous control benchmarks (Todorov et al., 2012). Our ParPI
framework improves the performance of base RL algorithms SQL and SAC, and achieves the best
results consistently across all tasks. We average the return over the past 100 episodes, where the solid
lines indicate the mean and shaded areas indicate the standard deviation.
4	Experiments
We conduct extensive experiments on both online scenarios and offline scenarios. Besides, we delve
deeply into how the component would affect the performance of ParPI on task HalfCheetah-v3.
4.1	Online Scenarios
We choose five well-known representative benchmark tasks with continuous control (Ant, Hopper,
Humanoid, HalfCheetah and Walker2d) from Mujoco (Todorov et al., 2012). The method is compared
7
Under review as a conference paper at ICLR 2022
to SQL (Haarnoja et al., 2017) and SAC (Haarnoja et al., 2018), which represents the SoTA model-
free off-policy methods for continuous control tasks. We choose Reverse KL divergence(RKL) and
Jensen-Shannon Divergence(JSD) out of the f -divergence family. For the Wasserstein distance, we
adopt the hinge loss (Miyato et al., 2018). And the Lipschitz constraint is enforced through gradient
penalty (Gulrajani et al., 2017) and spectral normalization (Miyato et al., 2018). The proposed
method is robust, We keep all of ParPI ’s hyper-parameter constant across all tasks in all domains for
simplicity. For hyper-parameters and additional implementation details, please refer to Appendix G.
All our experiments are conducted on machines equipped with Nvidia P100 GPUs using five different
random seeds.
As shown in Figure 2,we see that ParPI yields significant improvements in both performance and
sample efficiency across a wide range of environments. Comparing learning curves of different
methods, ParPI improves both SQL and SAC by a significant boost with almost no hyper-parameter
tuning effort. One observation is that the ParPI with SQL shows more improvement then with SAC.
The reason lies in the fact that the policy implied by the Q function in SAC is also shifted along with
the training, which could be the unstable element for the optimization of ParPI . And such situation
could be alleviated by optimizing the Q network for more steps than the policy network in each
update. Here we only involve the same training configurations for fair comparison. Interestingly,
we found that the Reverse KL divergence with ParPI has still consistently outperformed the original
SQL and SAC which optimized the reverse KL divergence in the primal form. This could be due to
that the intermediate target distribution constructed by MCMC smooths the density ratio estimation
procedure (Rhodes et al., 2020) and favors the dual form optimization. The fact further justifies the
effectiveness of proposed framework.
4.2	Offline Scenarios
To further demonstrate the efficacy of ParPI , we then experiment with the D4RL offline reinforcement
learning benchmark (Fu et al., 2020), including three environments(hopper, walker2d, and halfchee-
tah), which consist of five different logged data-sets types (random, medium, medium-replay, expert,
and medium-expert), in a total of 15 sub-tasks. These datasets are generated by an agent using SAC
in rlkit (Pong, 2020), with each dataset containing 1 million time-steps of environment interaction.
As we discussed in Sec. 3.3, we study the performance of ParPI from the combination with both
value function regularized methods and policy constraint methods. Particularly, we implement ParPI
on top of MOPO (Yu et al., 2020) and BRAC (Wu et al., 2019)(see Appendix H), and the discrepancy
measure in ParPI is set as wasserstein distance. We follow the same schema as the D4RL paper (Fu
et al., 2020) to calculate the normalized return. And the result of 15 sub-tasks is presented in Tabel 1.
Compared to the baseline, our approach achieves better performance in most environments, showing
the power of ParPI .
4.3	Component Analysis and Computational Efficiency
We conduct ablation study of different discrepancy measures and MCMC parameters on the task
HalfCheetah-v3 with the same settings as ParPI + SQL (see 1).
Discrepancy Measure We demonstrate the characteristics of different distribution metrics for policy
optimization including: JSD, original WD and the hinge loss. Here we retain all the hyper-parameters
of our method and the choice of the metric is the only difference. Interesting results can be observed
from Figure 3: while the entropy of learned policy remains almost the same for different metrics, the
standard deviation varies. The large standard deviation indicates that the learned policy tends to be
more certain on frequently observed state regions and does not mislead in other regions. According
to the empirical performance, we provide the following guidelines for the future usage of ParPI
: in general the Wasserstein distance could be a more desirable discrepancy measure and it also
achieves the best performance in most tasks; Jensen-Shannon divergence, though it is reported to
show some tendency of mode seeking (Theis et al., 2015), it also effectively ameliorates the problem
as it consistently outperforms the exclusive KL. Hence, we suggest that ParPI with W-distance could
be the generally recommended setting. When the dimension of action is not large, JSD could also
show good performance.
Langevin Dynamics We examine the choice of the step length K and step size of MCMC steps.
We observe that the sample that have higher Q-value could be fetched along with the MCMC steps,
8
Under review as a conference paper at ICLR 2022
Table 1: Results for D4RL benchmarks. Each number is the normalized score computed as (score -
random policy score) / (expert policy score - random policy score) of the policy at the last iteration of
training, averaged over 5 random seeds, ± standard deviation. Results of MOPO (Yu et al., 2020),
BEAR (Wu et al., 2019) and CQL (Kumar et al., 2020) are reported from their respective papers.
Remaining results are taken from the D4RL white-paper (Fu et al., 2020).
Offline Types	Tasks	ParPIWD+BRAC	ParPIWD +MOPO	MOPO	CQL BEAR	BRACv I	BC
	Walker2d	9.9±3.2	15.2±8.2	13.6±2.6	7	7.3	1.9	1.6
random	HalfCheetah	31.5±1.3	36.2±1.5	35.4±2.5	35.4	25.1	31.2	2.1
	Hopper	10.2±4.7	11.6±0.8	11.7±0.4	10.8	11.4	12.2	9.8
	Walker2d	79.1±7.2	39.4±13.2	17.8±19.3	79.2	59.1	81.1	6.6
medium	HalfCheetah	49.6±2.3	45.8±6.2	42.3±1.6	44.4	41.7	46.3	36.1
	Hopper	89.2±9.5	64.1±9.3	28.0±12.4	58	52.1	31.1	29.0
	Walker2d	41.8±7.9	38.8±12.6	39.0±9.6	26.7	19.2	0.9	11.3
medium-replay	HalfCheetah	39.9±1.7	54.8±3.1	53.1±2.0	46.2	38.6	47.7	38.4
	Hopper	42.1±8.1	64.5±27.1	67.5±24.7	48.6	33.7	0.6	11.8
	Walker2d	105.6±3.4	91.2±11.2	44.6±12.9	98.7	40.1	81.6	6.4
medium-expert	HalfCheetah	93.9±12.2	104.1±8.9	63.3±38.0	62.4	53.4	41.9	35.8
	Hopper	135.2±8.1	78.4±18.1	23.7±6.0	111	96.3	0.8	111.9
	Walker2d	108.0±3.1	99.4±10.1	-	153.9	106.1	0	125.7
expert	HalfCheetah	151.4±6.7	109.4±7.1	-	104.8	108.2	-1.1	107
	Hopper	128.8±2.3	84.1±14.4	-	109.9	110.3	3.7	109
(a) Entropy of learnt policy
Figure 3: Study of component of ParPI on HalfCheetah-v3. Every experiment in ablation study was
conducted using three different seeds.
# frames
(b) The hyper-parameter of Langevin Dynamic analysis.
and a longer MCMC run is or larger step size is, the sample has higher Q value as illustrated by
Figure 3.
T Training Time Efficiency To better understand the trade-off of ParPI on performance and train- ing efficiency, we conduct a case study on walk clock time in Ant as shown in Table. 2. Here seconds to return indicates how much time is needed to gain the corresponding level of re-	able 2: Wall-clock Time Comparison on Ant			
	Seconds to Return	1000	2000	3000
	ParPIWD +SAC SAC	9463 5953	11548 10006	14544 12323
				
turns. ParPI would take slight more time as it takes additional time for MCMC steps. Moreover, the
wall-clock time is usually not the main obstacle comparing to the environment time, i.e., time spent
during interaction with the environment, in online settings and sample efficiency for offline settings.
5	Conclusions and Discussion
We devise a novel algorithms framework for model-free reinforcement learning, ParPI , a particle-
based discrepancy/metric minimization framework for policy improvement, which can leverage the
full potential of stochastic policy by enable the broad family of divergence and discrepancy, such
as f -divergences and IPM-based metric. Our experiments on both online and offline settings show
that the baseline algorithms benefit from the ParPI in that they achieve state-of-the-art performance
with less hyper-parameter tuning efforts. ParPI accumulates many desirable properties: robustness,
stochasticity, and sample efficiency. Furthermore, ParPI exhibits a stabilized training and has been
shown to be time and sample efficient as compared to state-of-the-art approaches. Moreover, the
ParPI is model invariant, allowing it adapt to different RL task settings.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 16),pp. 265-283, 2016.
OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20,
2020.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv preprint
arXiv:1705.09048, 2017.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):
651-676, 2017.
Christian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search. In
Artificial Intelligence and Statistics, pp. 273-281, 2012.
Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In
Advances in Neural Information Processing Systems 32, pp. 3603-3613, 2019.
Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the unadjusted Langevin
algorithm. arXiv preprint arXiv:1605.01559, 2016.
Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized Stein variational
gradient descent. arXiv preprint arXiv:1707.06626, 2017.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International conference on machine learning, pp. 49-58, 2016.
Justin Fu.	rail-berkeley/d4rlevaluations, 2020. URL https://github.com/
rail-berkeley/d4rl_evaluations.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA),
2017 IEEE International Conference on, pp. 3389-3396. IEEE, 2017.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1352-1361. JMLR. org, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.
10
Under review as a conference paper at ICLR 2022
JM Herndndez-Lobato, Y Li, M Rowland, D Herndndez-Lobato, TD Bui, and RE Turner. Black-box
α-divergence minimization. In Proceedings of the 33rd International Conference on Machine
Learning, volume 48, pp. 1511-1520. International Machine Learning Society, 2016.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, pp. 4565-4573, 2016.
Jonathan Ho, Jayesh Gupta, and Stefano Ermon. Model-free imitation learning with policy optimiza-
tion. In International Conference on Machine Learning, pp. 2760-2769, 2016.
Ferenc Huszdr. How (not) to train your generative model: Scheduled sampling, likelihood, adversary?
arXiv preprint arXiv:1511.05101, 2015.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-planck
equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Neural Information Processing Systems (NeurIPS),
2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018.
Chongxuan Li, Chao Du, Kun Xu, Max Welling, Jun Zhu, and Bo Zhang. To relieve your headache
of training an mrf, take advil. In International Conference on Learning Representations, 2019a.
Chunyuan Li, Ke Bai, Jianqiao Li, Guoyin Wang, Changyou Chen, and Lawrence Carin. Adversarial
learning of a sampler based on an unnormalized distribution. In The 22nd International Conference
on Artificial Intelligence and Statistics, pp. 3302-3311. PMLR, 2019b.
Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2052-2061.
JMLR. org, 2017.
Yingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised MCMC.
arXiv preprint arXiv:1702.08343, 2017.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Chang Liu, Jingwei Zhuo, and Jun Zhu. Understanding mcmc dynamics as flows on the wasserstein
space. In International Conference on Machine Learning, pp. 4093-4103. PMLR, 2019.
Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. In 33rd
Conference on Uncertainty in Artificial Intelligence, UAI 2017, 2017.
Thomas Peter Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,
Massachusetts Institute of Technology, Cambridge, 2001.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,
Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed framework
for emerging {AI} applications. In 13th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 18), pp. 561-577, 2018.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
11
Under review as a conference paper at ICLR 2022
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems,
pp. 271-279, 2016.
Georg Ostrovski, Marc G Bellemare, Aaron Oord, and Remi Munos. Count-based exploration with
neural density models. In International conference on machine learning, pp. 2721-2730. PMLR,
2017.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learn-
ing: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464,
2018.
Vitchyr Pong. vitchyr/rlkit, 2020. URL https://github.com/vitchyr/rlkit.
Benjamin Rhodes, Kai Xu, and Michael U Gutmann. Telescoping density-ratio estimation. arXiv
preprint arXiv:2006.12204, 2020.
Gareth O Roberts and Osnat Stramer. Langevin diffusions and Metropolis-Hastings algorithms.
Methodology and computing in applied probability, 4(4):337-357, 2002.
Gareth O Roberts, Richard L Tweedie, et al. Exponential convergence of Langevin distributions and
their discrete approximations. Bernoulli, 2(4):341-363, 1996.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11895-11907, 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2011.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear program-
ming. In Proceedings of the 25th international conference on Machine learning, pp. 1032-1039,
2008.
L Theis, A van den Oord, and M Bethge. A note on the evaluation of generative models. In
International Conference on Learning Representations (ICLR 2016), pp. 1-10, 2016.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.
Emanuel Todorov. General duality between optimal control and estimation. In 2008 47th IEEE
Conference on Decision and Control, pp. 4286-4292. IEEE, 2008.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
JN Tsitsiklis and B Van Roy. An analysis of temporal-difference learning with function approximation-
technical. Report LIDS-P-2322). Laboratory for Information and Decision Systems, Massachusetts
Institute of Technology, Tech. Rep., 1996.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
ZiyU Wang, Alexander Novikov, Konrad Zoina, Jost Tobias SPnngenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020.
12
Under review as a conference paper at ICLR 2022
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings ofthe 28th international conference on machine learning (ICML-11), pp. 681-688,
2011.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint arXiv:2005.13239,
2020.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as wasserstein
gradient flows. In International Conference on Machine Learning, pp. 5737-5746. PMLR, 2018.
Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal
entropy. 2010.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
13
Under review as a conference paper at ICLR 2022
A Particle Improvement with Langevin Dynamics
As in the practical implementation, only the finite-step Langevin dynamics could be conducted.
Fortunately, the property of MCMC guarantees the improvement of the particle distribution. With the
p denotes the unique stationary distribution, e.g., the distribution implied by the Q-function in SQL
and SAC. qt and qt-1 refer to the distribution which is implicitly implied by an initial distribution
and t or t - 1 steps Langevin dynamics, the following monotonic property is satisfied:
DKL(qt||p) ≤ DKL(qt-1||p).	(10)
And qt converges to the stationary distribution p as t → ∞. The above proposition is the direct result
of the following lemma, we also provide the proof here for the completeness.
Lemma 1. Cover & Thomas (2012) Let q and r be two distributions for z0. Let qt and rt be
the corresponded distributions of state zt at time t, induced by the transition kernel K. Then
DKL[qt||rt] ≥ DKL[qt+1||rt+1] for all t ≥ 0.
Proof.
DκL[qt∣∣rt]= Eqt log『
rt(zt)
Eqt(zt)K(zt+1 |zt)
lo qt(zt)K(zt+ι∣Zt)
.°g rt(zt)K(zt+ι∣Zt)
Eqt+1(zt+1)qt+1(zt|zt+1)
'log qt+1(zt+1)q(zt∣zt+1)
.g rt+1(zt+1)r(zt∣zt+1).
DKL[qt+1||rt+1] + Eqt+1DKL[qt+1(zt|zt+1)||rt+1(zt|zt+1)].
□
B	Policy Improvement Theorem for ParPI
Given the objective function as the expected discounted sum of rewards, the policy improvement
theorem corresponds to how policies can be improved monotonically. Similar theorems have been
derived under the maximum entropy framework, i.e., SAC Haarnoja et al. (2018) and SQL Haarnoja
et al. (2017). In this section, we shows that optimization of the policy with ParPI does not hurt the
monotonic property.
B.1	PARPISQL
In the SQL-version of ParPI, considering the original policy improvement theorem of SQL:
Theorem 1. Haarnoja et al. (2017) Given a policy π, define a new policy π as
π(1S) (X eχp(Qπoft(s, •丹,VS
Assume that throughout our computation, Q is bounded and exp(Q(s, a))da is bounded for any s
(for both π and π). Then we have Qnoft (s, a) ≥ Qsoft(s, a)∀s, a.
It should be noticed that there is no constraint on the policy optimization in Theorem. 1, hence we
could directly replace the original reverse KL minimization with ParPI without hurting the policy
improvement theorem.
B.2	PARPISAC
In the situation of ParPISAC, we need to generalize the previous objective:
Jn(φ) = Est〜D [dkl (∏φ (∙∣St) kexp(Qθ1st, •)))
Zθ (St)
to
Jn(φ) = Est〜D [d 卜φ (∙∣St) k exp(Qθ (St, ,)))
Zθ (St)
To make it clear, we follow the proof provided in Haarnoja et al. (2018):
14
Under review as a conference paper at ICLR 2022
Lemma 2. Let πold ∈ Π and let πnew be the optimizer of the minimization problem defined in
J∏(φ) := D(P(S)∏φ(a∣s),ρ(s)p(a∣s)). Then Qnnew (s,a) ≥ Qnold (s,a) for all (s,a) ∈ S XA with
|A| < ∞
Proof. Let πold ∈ Π and let Qnold and V nold be the corresponding soft state-action value and soft
state value, and let πnew be defined as
∏new (∙∣St) = arg min Dkl (∏0 (∙∣St) k exp (Qnold (st, ∙) - log Znold (St)))
n0 ∈Π
=arg min Jnold (π0 (1St))
n0 ∈Π
Note that Jnold (∏new (∙∣st)) ≤ Jnold (nold (∙∣st)), SinCe we can always choose ∏new = ∏οid ∈ Π.
Hence
Eat〜nnew [log ∏new ⑶际)— Qnold ⑸,at) + lθg Z"old (St)] ≤
Eat〜nold [log∏oid ⑶双)一Qnold ⑸,a。+ log Znold (St)]
Applying soft Bellman Equation, we could get the policy improvement for SAC.
(11)
□
Note in ParPISAC, we could only need to guarantee the condition in Eq. 11 is also satisfied. This
directly follows the monotonic property of Langevin dynamics as shown in Eq.10.
C Derivation of Squash Correction
To constrain the action space in a finite interval, the tanh is applied on the samples from the raw
output u. And we also conduct the Langevin dynamics on u space. The change of variable formula
indicates the following equation is satisfied:
log p(a|S) = log p(u|S)
detr
du
-1	D
= log p(u|S) - log 1 - tanh2 (ui)
i=1
The stationary distribution on the a space is exp(Q(s, a)∕α), the corresponding density function on
the u space satisfying that:
D
log p(u|s) = exp(Q(s, tanh(u)α) + 2 log(1 - (tanh(ui))
i=1
We take derivative and get the score function as:
D
Nu logp(u∣s) = Vu(Q(s, tanh(u))∕α + 2 X Nu log(1 - (tanh(ui))
i=1
D Discussion on Remark 2
The langevin dynamic is a special case of wasserstein gradient flow (Liu et al., 2019). At step k + 1,
the particle simulation is to solve the following problem:
(h)	.......... W22 ")
μk+ι = argmmKL(μkp(X)) +-------焉--------
μ	2h
Here μk denotes the sampled distribution after k-th step and p(χ) is the target distribution, i.e.
ne(a|s)eXpZ(Q(S'a"α" in our case. With limited steps of langevin dynamics, according to the triangle
inequality of wasserstein distance we have:
X X W2 (μh+1,μkh))	W22(μk+ι,μ1h))
J»	2h ≥	2h	(12)
i	i=1
The above inequality is to say that if each step the wasserstein distance could be bounded (by Ci),
then the wasserstein distance between the final distribution and the original distribution is also
bounded. And we replace μIh) with ∏β and ■[、with ∏φk, we get the W2 (∏β, ∏φk) is constrained.
Note that we optimize the wasserstein distance between πφ and πφk explicitly, and the wasserstein
distance W22 (πφ, πφk ) is also constrained. Applying the triangle inequality agian, we get the
15
Under review as a conference paper at ICLR 2022
bounded condition on W22 (πφ, πβ) ≤ W22 (πφ, πφk) + W22 (πβ, πφk). Integrating the W22 (πφ, πβ)
and W22 (πφ, πφk) into Eq. 12 with Lagrangian multiplier α and γ, we then get the Remark 2.
E Convergence analysis of ParPI
The convergence property of ParPI is highly correlated with the particle updates in Eq. 4. Note that
Jordan et al. (1998) indicates that with unlimited samples and infinitely small step size, the sample
result could approach some stationary distribution eU(x). Such property provide a good theoretical
intuition for convergence analysis on ParPI :
Proposition 1. With unbiased gradient estimation on the VφW22(∏φ, πsampled), where ∏sampled indi-
cates the empirical distribution acquired from the langevin dynamics. If the sample size M → ∞
and step size a → 0, ∏φ in ParPI would converge to the global minimum ne(a|s)expZQ(s,a"a)).
Note the Proposition 1 is based on the fact the KL(∙∣∣∙) is convex and We leave the detailed derivation
in the Appendix.
To start With, We introduce the folloWing lemma:
Lemma 3. (Jordan et al., 1998) Assume that log p(x) ≤ C1 is infinitely differentiable,
and kV log p(x)k ≤ C2 (1 + C1 - log p(x)) (∀x) for some constants {C1, C2} . Let T =
hK,μo，qo(x), and {μkh)} be the solution of the functional optimization problem: μk+ι =
W22 (μ,μkh))
arg min* KL(μ∣∣p(x)) +------送Il—L, which are restricted to the space with finite second-order
moments. Then i) the problem is COnvex;and ii) μK converges to μτ in the limit of h → 0,
i.e., limh→o μ? = μτ, where μτ is the solution of Fokker-Planck (FP) equation: ∂τμτ =
V ∙ (—μτVU + V ∙ (μτσσ>)) at T.
Note the stationary distribution of the FP Equation is proportional to eU(x)), Lemma 3 shoWs that
limk→∞,h→o μkh) = Zzeu. In our case the stationary distribution refers to ne(a|s)expZQ(s,a)/a)).
Thus Lemma 3 suggests that With sample size M → ∞ and step size α → 0, πφ Would converge to
the global minimum πβ(Ms)吗Q(s，a)/a)).
F	Discussion on Matching the Joint State-Action Distribution
Note that ultimate goal of learning stochastic policy could be formulated as matching the conditional
distribution, i.e., for every value of the state: min& D (qθ (∙ | S)Ilp(∙ | S)), ∀s. While in this situation,
the optimization problems need to be solved independently in different states. For example, if We
Want to optimize JSD or WD, then We Will need many different discriminators or critics for different
states. The corresponding computational complexity is unacceptable. To make the training tractable,
We "amortized" the optimization problem on different states into a joint matching problem. Put it
in another Way, using the joint distribution is obvious When considering the analogy to supervised
learning: the task is to match fθ(x) to the corresponding target y for every x, but a joint distribution
is used in the objective: Ep(x,y) [D (fθ(x), y)]. The analogy is achieved by replacing the model fθ(x)
with qθ(∙ | S) and the target y withp(∙ | s).
G	Experiment Details
All algorithms are implemented in Tensorflow (Abadi et al., 2016) and use a distributed implementa-
tion powered by Ray (Moritz et al., 2018). Following the implementation of TD3 (Fujimoto et al.,
2018), we use two Q-value functions parameterized by a two-layer feed-forward network to fend off
overestimation. A discriminator model sharing the same architecture is introduced for the divergence
minimization purpose. Besides, we do not introduce any reward shaping for all tasks. When collecting
rollouts for evaluations, we simply take the action selected by the policy at every state for every 1000
updates.
Policy, Q-value function & Discriminator architectures for both baselines and our algorithms:
16
Under review as a conference paper at ICLR 2022
S ∈ Rs,a ∈ Ra,z ∈ Ra 〜N(0,I)
Affine Transformation
Dense layer 256, ReLU
Dense layer 256, ReLU
Dense layer 256, Tanh
Table 3:	Policy
s ∈ Rs,a ∈ Ra
Dense layer 256, ReLU
Dense layer 256, ReLU
Dense layer 256, ReLU
dense → 1
Table 4:	(Double)Q Function
s ∈ Rs,a ∈ Ra
Affine Transformation
Dense layer 256, ReLU
Dense layer 256, ReLU
Dense layer 256, Linear
Table 5:	Discriminator
Table 6: Hyper-parameter Settings of ParPI
Parameter	Policy	Q-function	Discriminator
optimizer	Adam	Adam	Adam
Learning rate	3∙10-4	3∙10-4	3∙10-4
discount(γ)	0.99	n/a	n/a
replay buffer size	106	n/a	n/a
entropy target	—dim(A)	n/a	n/a
MCMC steps	n/a	n/a	5
MCMC step size	n/a	n/a	3 ∙10-4
gradient steps	1	1	1
target update interval	1	1	1
H Offline Algorithm Details
For completeness, we list the detailed algorithms for ParPI +MOPO in Algorithm 2. We use rollout
length 5 for all tasks, and the same penalty coefficients reported by Yu et al. (2020).
17
Under review as a conference paper at ICLR 2022
Algorithm 2 ParPI on top of MOPO
1:	Input: reward penalty coefficient λ rollout horizon h, rollout batch size b.
2:	Train on batch data Denv an ensemble of N probabilistic dynamics {Ti(s0,r∣s,a)
N(Ni(S,a0,σiGa))}N=1.
3:	Initialize policy ∏ and empty replay buffer Dmodel J 0.
4:	for epoch 1, 2, . . . do
5:	for 1, 2, . . . , b (in parallel) do
6:	Sample state S1 from Denv for the initialization of the rollout.
7:	for j = 1, 2, . . . , h do
8:	Sample an action aj 〜π(sj).
9:	Randomly pick dynamics T from {Ti}N=ι and sample Sj+ι, rj 〜 T(Sj, aj).
10:	Compute r = rj-λmaxN=ι k∑i(sj,aj)∣∣f.
11:	Add sample (sj-, aj,j Sj+ι) to Dmodel.
12:	end for
13:	end for
14:	Drawing samples from Denv ∪ Dmodel, use ParPI to update π.
15:	end for
For the ParPI +BRAC, following the original implementation (Wu et al., 2019; Fu, 2020), we using
dual form for the behavior policy regulation(with KL) and value penalty with fixed α. Then we use
ParPi for both Value-function and Q-function update. We using (256,256) fully connected network as
the critic in the minimax objective. We also adapt the gradient penalty in KL dual training. we using
Adam for all optimizers.
18