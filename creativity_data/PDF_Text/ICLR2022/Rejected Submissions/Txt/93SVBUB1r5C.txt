Under review as a conference paper at ICLR 2022
Learning with convolution and pooling oper-
ATIONS IN KERNEL METHODS
Anonymous authors
Paper under double-blind review
Ab stract
Recent empirical work has shown that hierarchical convolutional kernels inspired
by convolutional neural networks (CNNs) significantly improve the performance
of kernel methods in image classification tasks. A widely accepted explanation
for the success of these architectures is that they encode hypothesis classes that
are suitable for natural images. However, understanding the precise interplay be-
tween approximation and generalization in convolutional architectures remains a
challenge. In this paper, we consider the stylized setting of covariates (image
pixels) uniformly distributed on the hypercube, and fully characterize the RKHS
of kernels composed of single layers of convolution, pooling, and downsampling
operations. We then study the gain in sample efficiency of kernel methods using
these kernels over standard inner-product kernels. In particular, we show that 1)
the convolution layer breaks the curse of dimensionality by restricting the RKHS
to ‘local’ functions; 2) local pooling biases learning towards low-frequency func-
tions, which are stable by small translations; 3) downsampling may modify the
high-frequency eigenspaces but leaves the low-frequency part approximately un-
changed. Notably, our results quantify how choosing an architecture adapted to
the target function leads to a large improvement in the sample complexity.
1 Introduction
Convolutional neural networks (CNNs) have become essential elements of the deep learning tool-
box, achieving state-of-the-art performance in many computer vision tasks (Krizhevsky et al., 2012;
He et al., 2016). CNNs are constructed by stacking convolution and pooling layers, which were
shown to be paramount to their empirical success (LeCun et al., 2015). A widely accepted hy-
pothesis to explain their favorable properties is that these architectures successfully encode useful
properties of natural images: locality and compositionality of the data, stability by local deforma-
tions, and translation invariance. While some theoretical progress has been made in studying the
approximation and generalization benefits brought by convolution and pooling operations (Cohen &
Shashua, 2016a;b; Bietti, 2021), our mathematical understanding of the interaction between network
architecture, image distribution, and efficient learning remains limited.
Consider x ∈ Rd an input signal, which we can think of as a grayscale pixel representation of
an image. For mathematical convenience, we will consider one-dimensional images with cyclic
convention xd+i := xi, and denote x(k) = (xk, xk+1, . . . , xk+q-1) the k-th patch of the signal x,
k ∈ [d], with patch size q ≤ d. Most of our results can be extended to two-dimensional images.
We further consider a simple convolutional neural network composed of a single convolution layer
followed by local average pooling and downsampling. The network first computes the nonlinear
convolution of N filters w1 , . . . , wN ∈ Rq with the image patches x(k). The outputs of the con-
volution operation σ(hwi, x(k)i) are then averaged locally over segments of length ω (local average
pooling). This pooling operation is followed by downsampling which extracts one out of every
∆ output coordinates (for simplicity, ∆ is assumed to be a divisor of d). Finally, the results are
combined linearly using coefficients (a，ik)i∈[N∖,k∈∖dZ
fcNN(x; a, Θ) = JN∆ωd X X aik X σ (h,wi, X(k∆+s)i) . (CNN-AP-DS)
i∈[N] k∈[d∕∆]	s∈[ω]
1
Under review as a conference paper at ICLR 2022
Note that pooling and downsampling operations are often tied together in the literature. However in
this work we will treat these two operations separately.
In the formula above, different values for q, ω, ∆ lead to different architectures with vastly different
behaviors. For example, when q = ∆ = d and ω = 1, we recover a two-layer fully-connected
neural network fFC(x; a, Θ) = N -1/2 Pi∈[N] aiσ(hwi, xi) which has the universal approxima-
tion property at large N . When ω = ∆ = 1 and q < d, the network is “locally connected”
fLC(x; a, Θ) = N -1/2 Pi∈[N],k∈[d] aikσ(hwi, x(k)i), and not a universal approximator anymore:
however, fLC vastly outperforms fFC in some cases (Li et al., 2020). Forω > 1, local pooling enables
learning functions that are locally invariant by translations more efficiently than without pooling. For
ω = d (global pooling), the network only fits functions fully invariant by cyclic translations.
The aim of this paper is to formalize and quantify the trade-off between the target function class
and the statistical efficiency brought by these different architectures. As a concrete first step in
this direction, we consider kernel models that are naturally associated with the convolutional neural
networks (CNN-AP-DS) through the neural tangent kernel perspective (Daniely et al., 2016; Jacot
et al., 2018). Kernel methods have the advantage of 1) being tractable—leaving the computational
issue of learning CNNs aside; 2) having well-understood approximation and generalization proper-
ties, which depends on the eigendecomposition of the kernel and the alignment between the target
function and associated RKHS (Caponnetto & De Vito, 2007; Wainwright, 2019) (see Appendices
B and C for background). While kernel models only describe neural networks in the lazy training
regime (Chizat et al., 2019; Du et al., 2019b;a; Allen-Zhu et al., 2019; Zou et al., 2018) and miss
important properties of deep learning, such as feature learning, we see that architecture choice is
already crucial to enable efficient learning of ‘image-like’ functions in the fixed-feature regime.
Neural tangent kernels are obtained by linearizing the associated neural networks. Here we consider
the tangent kernel associated to the network fCNN (c.f. Appendix A.2 for a detailed derivation):
Hωκ∆(χ,y) = dω X X Mhχ(k∆+s),y(k4+so)i/q),	(CK-AP-DS)
k∈ [d/∆] s,s0∈ [ω]
where h : R → R is related to the activation function σ in (CNN-AP-DS). As a linearization of
CNNs, the kernel (CK-AP-DS) inherits some of the favorable properties of convolution, pooling,
and downsampling operations. Indeed, a line of work (Mairal et al., 2014; Mairal, 2016; Arora
et al., 2019; Li et al., 2019; Shankar et al., 2020) showed that, though performing slightly worse
than CNNs, such (hierarchical) convolutional kernels have empirically outperformed the former
state-of-the-art kernels. For instance, these kernels achieved test accuracy around 87% - 90% on
CIFAR-10, against 79.6% for the best former unsupervised feature-extraction method (Coates et al.,
2011) (currently, the state-of-the-art CNNs can achieve test accuracy 99%).
In this paper, We will further consider a stylized setting with input signal distribution X 〜 Unif(Qd)
(uniform distribution over Qd := {-1, +1}d the discrete hypercube in d dimensions). This simple
choice allows for a complete characterization of the eigendecomposition of HωC,κ∆, thanks to all
patches having same marginal distribution x(k)〜 Unif(Qq). We will be particularly interested in
four specific choices of (q, ω, ∆) in (CK-AP-DS):
HFC(x, y)	=h(hx, yi/d),	(FC)
HCκ(x, y)	=d X h(hx(k), y(k)i/q), k∈[d]	(CK)
HωCκ(x, y)	=dωω X X	h(hx(k+s), y(k+s0)i/q), k∈[d] s,s0 ∈[ω]	(CK-AP)
HGCPκ(x, y)	=d X h(hx(k), y(ko)i/q).	(CK-GP)
k,k0 ∈[d]
These kernels are respectively the neural tangent kernels of a fully-connected network fFC (FC), a
convolutional network fLC (CK), a convolutional network followed by local average pooling (CK-
AP) and a convolutional network followed by global pooling (CK-GP). We will further be interested
in (CK-GP) with patch size q = d, which we denote HGFPC : this corresponds to a convolutional kernel
with full-size patches q = d, followed by global pooling.
2
Under review as a conference paper at ICLR 2022
In this paper, we first characterize the reproducing kernel Hilbert space (RKHS) of these convo-
lutional kernels, and then investigate their generalization properties in the regression setup. More
specifically, assume {(xi, yi)}i≤n are n i.i.d. samples with Xi 〜Unif(Qd) and yi = f?(Xi) + 已％.
Here f? ∈ L2(Qd) and (εi)i≤n are independent errors with mean zero and variance bounded by
σε2. We will focus on the generalization error of kernel ridge regression (KRR) (see Appendix B.1
for general kernel methods). In particular, given a kernel function H : Qd × Qd → R and a
regularization parameter λ ≥ 0, the KRR estimator is the solution of the tractable convex problem
fλ = arg min { X ® — f (xi))2 + λ∣∣f 隐》,
f∈H	i∈[n]
(KRR)
where H is the RKHS associated to H with RKHS norm k ∙ k h. We denote the test error with square
loss by R(f?, fλ) = Eχ{(f*(x) — fλ(x))2}. We will sometimes consider the expected test error
Eε{R(f?, fλ)}, where expectation is taken with respect to noise ε = (εi)i≤n in the training data.
The generalization properties of the kernels HFC and HGFCP were recently studied in Mei et al. (2021b);
Bietti et al. (2021). In particular, they showed that global pooling (kernel HGFCP) leads to a gain of
a factor d in sample complexity when fitting cyclic invariant functions, but still suffers from the
curse of dimensionality (HGFCP only fits very smooth functions in high-dimension). More precisely,
Mei et al. (2021b) considered the high-dimensional framework of Mei et al. (2021a) and showed the
following: KRR with HFC requires n ≈ d` samples to fit degree-` cyclic polynomials, while KRR
with HGC only needs n ≈ d`-1. To enable milder dependence on the dimension d, further structural
assumptions on the kernel and the target function should be considered (for instance, in this paper,
we use the kernel HCK and consider ‘local’ functions).
1.1	Summary o f main results
Our contributions are two-fold. First, we describe the RKHS associated to the convolutional kernel
(CK-AP-DS) in the stylized setting X 〜Unif(Qd), which provides a fully explicit illustration of
the roles of convolution, pooling and downsampling operations in learning specific classes of func-
tions. Second, based on the eigendecomposition of the kernels, we provide generalization bounds on
kernel ridge regression both in the fixed and high-dimension settings. In particular, we quantify the
gain in sample complexity achieved by different architectures when learning target functions with
corresponding structures.
Let us define the q-local function class L2 (Qd, Locq) and the cyclic q-local function class
L2(Qd, CycLocq) (subspace of L2 (Qd, Locq) consisting of cyclic-invariant functions) as follow:
L2(Qd,Locq) = {f ∈ L2(Qd) : ∃{gk}k∈[d]⊆ L2(Qq),f(X)= X gk(X(k))} , (LOC)
k∈[d]
L(Qdd CycLOcq) = {f ∈ L2(Qd) ： ∃g ∈ LlQq),f(x) = X g(x(k))} . (CYC-LOC)
k∈[d]
We summarize below some of the insights that follow from our results.
One-layer convolutional layer. The RKHS of HCK is constituted of q-local functions
L2 (Qd, Locq). Furthermore, the decay of the eigenvalues of HCK is controlled by a
q-dimensional kernel, instead of a d-dimensional kernel for HFC . In particular, this implies
that the test error of kernel ridge regression decays at rate n-O(1/q) instead of n-O(1/d),
when the target function is in L2 (Qd, Locq). Hence, when q d, kernel methods with
convolutional kernel HCK breaks the curse of dimensionality.
Average pooling. The RKHS of HωCK is still constituted of q-local functions f ∈ L2 (Qd, Locq),
but penalizes differently the frequency components fj(X) by reweighting their eigenspaces
by a factor	Kj,	where	fj(x) =	Pk∈[d]	Pjf (tk ∙ x) with	Pj	=	e2idπj	and tk ∙ X =
(xk+1, . . . , xd, x1, . . . , xk) denotes the k-shift. As ω increases, local pooling penalizes
more and more heavily the high-frequency components (κj 1), while making low-
frequency components statistically easier to learn (κj 1). For global pooling ω = d,
HGCPK only learns cyclic local functions L2(Qd, CycLocq) and enjoy a factor d gain in sta-
tistical complexity compared to HCK .
3
Under review as a conference paper at ICLR 2022
To fit a degree ' polynomial	H FC	HGC	H CK	Hωκ	HCK
Sample complexity	d	d'-1	dq'-1	dq'-1∕ω	~T-τ^
Table 1: Sample size n required to fit a q-local cyclic-invariant polynomial of degree ` using kernel
ridge regression (KRR) with the 5 different kernels of interest in this paper.
Downsampling. When ∆ ≤ ω , downsampling after average pooling leaves the low-frequency
eigenspaces of HωCK stable. In particular, the downsampling operation does not modify
the statistical complexity of learning low-frequency functions in one-layer kernels, while
being potentially beneficial in further layers in deep convolutional kernels.
In Table 1, we report our high-dimensional predictions for the sample complexity of learning q-local
cyclic-invariant polynomials of degree `. These results are proven within the framework of Mei et al.
(2021a) (see Appendix C for background).
The rest of the paper is organized as follows. We discuss related work in Section 1.2. In Section
2, we present our main results on convolutional kernels and describe precisely the roles of convolu-
tion, pooling and downsampling operations. We briefly mention the multilayer case in Section 2.4.
Finally, we present a numerical simulation on synthetic data in Section 3 and conclude in Section 4.
Some details and discussions are deferred to Appendix A.
1.2	Related work
Convolutional kernels have been considered in Mairal et al. (2014); Mairal (2016); Li et al. (2019);
Shankar et al. (2020); Bietti (2021); Thiry et al. (2021). In particular, they showed that these archi-
tectures achieve good results in image classification (90% accuracy on Cifar10) and that pooling and
downsampling were necessary for their good performance (Li et al., 2019).
The generalization error of kernel ridge regression (KRR) has been well-studied in both the fixed
dimension regime (Wainwright, 2019, Chap. 13), Caponnetto & De Vito (2007) and the high-
dimensional regimes El Karoui (2010); Liang et al. (2020); Ghorbani et al. (2020; 2021); Mei et al.
(2021b). These results show that the generalization error depends on the eigenvalues and eigenfunc-
tions of the kernel, and the alignment of the kernel with the target function.
Recently, a few theoretical work have considered the generalization properties of invariant kernels
and convolutional kernels (Scetbon & Harchaoui, 2020; Mei et al., 2021b; Bietti et al., 2021; Favero
et al., 2021). Favero et al. (2021) consider a one-layer convolutional kernel with and without global
pooling and derived asymptotic rates in n, the number of samples, in a student-teacher scenario us-
ing statistical physics heuristics and a Gaussian equivalence conjecture. In particular, they show that
locality rather than translation-invariance breaks the curse of dimensionality. Here, our goal is dif-
ferent: we derive mathematically rigorous quantitative bounds that give separation in generalization
power between different architectures.
See Malach & Shalev-Shwartz (2020); Li et al. (2020) for more theoretical results on the separation
between convolutional and fully connected neural networks, and Boureau et al. (2010); Cohen &
Shashua (2016b) for the inductive bias of pooling operations in convolutional neural networks.
2 Main results
We start by introducing some basic background on functions on the hypercube and eigendecompo-
sition of kernel operators in Section 2.1. We first consider a kernel with a single convolution layer in
Section 2.2, and characterize its eigendecomposition and generalization properties. We then show
how these results are modified when applying local average pooling and downsampling in Section
2.3. Finally, we briefly discuss multilayer convolutional kernels in Section 2.4.
2.1 Functions on the hypercube and eigendecomposition of kernel operators
Recall that we work on the d-dimensional hypercube Qd := {-1, +1}d. Let L2 (Qd) =
L2(Qd, Unif) be the 2d-dimensional vector space of all functions f : Qd → R, with scalar product
4
Under review as a conference paper at ICLR 2022
hf,g) L2 := Ex 〜Unif(Qd)[f (x)g(x)]. Let k ∙ ∣∣L2 be the norm associated with the scaler product. We
introduce the set of Fourier functions {YS(d)(x)}S⊆[d] which forms an orthonormal basis of L2(Qd).
For any subset S ⊆ [d], the Fourier function is defined as YS(d)(x) := Qi∈S xi with the convention
that Yʧd) := 1 (it is easy to verify that hYS(d) ,YS(d)i l2 = 1s=so). We will omit the superscript (d)
which will be clear from context and write YS := YS(d).
Consider a nonnegative definite kernel function H : Qp × Qp → R (p = d or q in this paper)
with associated integral operator H : L2 (Qp) → L2 (Qp) defined as Hf (u) = Ev{h(u, v)f (v)}
with v 〜Unif(Qp). By spectral theorem of compact operators, there exists an orthonormal ba-
sis {ψj}j≥ι of L2(Qp) and nonnegative eigenvalues (λj j≥ι such that H = Pj≥ι λjψjψj (i.e.,
H(u, v) = Pj≥1 λjψj(u)ψj(v) for any u, v ∈ L2 (Qp)).
The most widespread example are inner-product kernels defined as H(u, v) := h(hu, vi/p) for
some function h : R → R. Inner-product kernels have the following simple eigendecomposition in
L2 (Qp) (taking here u, v ∈ Qp):
p
h(〈u, vi/p) = X ξp,'(h)	X	YS (U)YS (v),	(1)
'=0	S⊆[p],∣S∣='
where ξp,'(h) is the '-th Gegenbauer coefficient of h(∙/√p) in dimension p, i.e.,
ξp,'(h) = Eu〜Unif(Qp) [h(〈u, ei∕p)Q'p)(hu, ei)],	⑵
for e ∈ Qp arbitrary and Q'p) the degree-' Gegenbauer polynomial on Qp (see Appendix D for
details). Note that (ξp,')o≤'≤q are non-negative by positive semidefiniteness of the kernel. We will
write ξp,' := ξp,'(h) and use extensively the decomposition identity (1) in the rest of the paper.
2.2	One-layer convolutional kernel
We first consider the convolutional kernel HCK (CK) given by a one-layer convolution layer with
patch size q and inner-product kernel function h : R → R:
1d
HCK(χ, y) = d £ h (hχ(k), y(k)i/q),	⑶
k=1
where we recall that x(k) = (xk, . . . , xk+q-1) ∈ Qq is the k’th patch of the image with size q.
Before stating the eigendecomposition ofHCK, we introduce some notations. For any subset S ⊆ [d],
denote γ(S) the diameter of S with cyclic convention, i.e., γ(S) = max{min{mod(j - i, d) +
1, mod(i - j, d) + 1} : i, j ∈ S} (e.g., γ({2, d}) = 3). For any integer ' ≤ q, consider the set
e` = {S ⊆ [d] : |S| = ', Y(S) ≤ q} of all subsets of [d] of size ' with diameter less or equal to q.
We will assume throughout this paper that q ≤ d/2 to avoid additional overlap between sets.
Proposition 1 (Eigendecomposition of HCK). Let HCK be a convolutional kernel as defined in
Eq. (3). Then HCK admits the following eigendecomposition:
HCK(x, y) = ξq,0 + X X r(Sdξq' ∙ YS(X)YS(y),	(4)
'=1 S∈E'
where r(S) = q + 1 一 Y(S) and ξq,' ≥ 0 is defined in Eq. (2).
Notice that YS with γ(S) > q (monomials with support not contained in a segment of size q) are
in the null space of HCK. Hence (as long as ξq,' > 0 for all 0 ≤ ' ≤ q), the RKHS associated to
HCK exactly contains all the functions in the q-local function class L2(Qd, Locq) (c.f. Eq. (LOC)).
In words, L2 (Qd, Locq) consists of functions that are localized on patches, with no long-range
interactions between different parts of the image. An example of local function with q = 3 is given
by f(x) = x1x2x3 + x4x6 + x5.
On the other hand, the RKHS associated to the fully-connected kernel HFC (FC) typically contains
all the functions in L2 (Qd) (under genericity assumptions on h). The RKHS with convolution
5
Under review as a conference paper at ICLR 2022
dim(L1 2 (Qd, Locq)) = d2q-1 + 1 is significantly smaller than dim(L2 (Qd)) = 2d, which prompts
the following question: what is the statistical advantage of using HCK over H FC when learning
functions in L2 (Qd, Locq)?
We first consider the classical approach to bounding the test error of Caponnetto & De Vito (2007);
Wainwright (2019); Bach (2021) which relies on the following two standard assumptions:
(A1) Capacity condition: We assume N(h, λ) := Tr[h∕(h + λI)-1] ≤ C九λ-Va with1 α > 1.
(A2) Source condition: kh-e/2gkL2 ≤ B with2 β > α-1 and B ≥ 0.
The capacity condition (A1) characterizes the size of the RKHS: for increasing α, the RKHS con-
tains less and less functions. The source condition (A2) characterizes the regularity of the tar-
get function (the ‘source’) with respect to the kernel: increasing β corresponds to smoother and
smoother functions. See Appendix B.2 for more discussions.
Based on these two assumptions, we can apply standard bounds on the KRR test error and obtain:
Theorem 1 (Generalization error of KRR with HCK). Let h : R → R be an inner-product ker-
nel satisfying (A1). Let f? ∈ L2 (Qd, Locq) with f(x) = k∈[d] gk(x(k)) satisfying (A2) with
Pk∈[d] kh-β^2gkk2L2 ≤ B2. Then there exists C1, C2, C3 > 0 constants that only depend on (A1)
and (A2) (and independent of d), such thatfor n ≥ Ci max(kf*∣∣L∞, d) and λ* = C(d/n) ɑβ+1,
αβ
Eε{R(f*,fλ*)} ≤ C (d)αβ+∙	(5)
Note that the exponent βθ+r only depends on the q-dimensional kernel h. Hence, the generalization
bound with respect to (n/d) is independent of the dimension d of the image. Let’s compare to
KRR with inner-product kernel H FC (FC): from Caponnetto & De Vito (2007), we have the minmax
-2-
ʌ	- α,β
rate Eε{R(f?, fλ)} N n αβ+1 where h is now defined in d dimension and verifies (A1) and (A2)
with constants α,β. Typically, if f? is only assumed Lipschitz, then βα = O(1∕d), which leads
to a minmax rate n-O(1/d) for HFC, while for HCK, βα = O(1/q), which leads to a minmax rate
n-O(1/q). Hence, for q d, HCK breaks the curse of dimensionality by restricting the RKHS to
‘local’ functions. Similarly, Favero et al. (2021) derived a decay rates in n that do not depend on dfor
a one-layer convolutional kernel. The key difference between Theorem 1 and Favero et al. (2021) is
that we obtain a non-asymptotic bound that is minmax optimal up to a constant multiplicative factor
in both d and n (this can be showed for example by adapting the proof in Appendix B.6 in Bietti
et al. (2021)) using a rigorous framework of source and capacity condition.
Theorem 1 and results of this type suffers from several limitations: 1) they are tight only in terms of
the exponent of n in a minmax sense; 2) they do not provide comparisons for specific subclasses of
functions; 3) in order to obtain the minmax rate, the regularization parameter λ has to be carefully
tuned to balance the bias and variance terms, which is in contrast to modern practice where often
the model is trained until interpolation. This led several groups to consider instead the test error of
KRR in a high-dimensional limit Ghorbani et al. (2021); Mei et al. (2021a); Canatar et al. (2021)
and derive exact asymptotic predictions correct up to an additive vanishing constant for any f? ∈ L2
(see Appendix C for more details).
Using the general framework in Mei et al. (2021a), we get the following result for q, d large:
Theorem 2 (Generalization error of KRR with HCK in high-dimension (informal)). Let f? ∈
L2(Qd, Locq) and h : R → R verifying some ‘genericity condition’. Then for n = dqs-1+ν
with 0 < ν < 1, and λ= O(1) (in particular λ= 0 works), we have
fλ = PE≤s,ν f? + Oq ⑴,	(6)
where PE≤s,ν is the projection on the span ofYS with either |S| < s and S ∈ E|S| or |S| = s and
Y(S) ≤ q(1 - q-ν)∙
1Here, h is the integral operator and Tr[h∕(h + λI)-1] = Pj≥ι λλ++λ With {λj }j≥ι eigenvalues of h.
2Again, h is the operator with h-κg = Pj≥1 λj-κhf, ψjiψj, where {ψj}j ≥1 are the eigenvectors of h.
6
Under review as a conference paper at ICLR 2022
See Appendix C.1 for a rigorous statement. In words, when dqs-1	n	dqs, KRR with HCK
only learns a degree-s polynomial approximation to f? .
On the other hand, when considering the standard inner-product kernel HFC (FC) we get:
Theorem 3 (Generalization error of KRR with H FC in high-dimension (informal)). Let f? ∈
L2(Qd) and h : R → R with some 'generiCity condition". Thenfor d《n《ds+1 and λ = O⑴,
fλ = P≤sf? + od(1),	(7)
where P≤s is the projection on the subspace of degree-s polynomials.
This theorem was proved in Ghorbani et al. (2021); Mei et al. (2021a). Notice that Eq. (7) does
not depend on the structure of f?. Hence, when f? ∈ L2 (Qd, Locq), Theorems 2 and 3 shows a
clear statistical advantage of HCK over HFC when q d (and therefore of one-layer CNNs over
fully-connected neural networks in the kernel regime).
2.3	Local average pooling and downsampling
In many applications such as object recognition, we expect the target function to depend mildly on
the absolute spatial position of an object and to be stable under small shifts of the input. To take this
local invariance into account, convolution layers are often followed by a pooling operation. Here we
consider local average pooling on a segment of length ω and obtain the kernel
HωK(X，y) = dω X X h (hx(k+s), y(fe+so)i/q).	⑻
k∈[d] s,s0∈[ω]
Define s` = {S ⊆ [q] : |S| = '} as the collection of sets of size '. We further define an equivalence
relation 〜on s`: S 〜S0 if S0 is a translated subset of S in [q] (without cyclic convention). We
denote C the quotient set of a` under the equivalence relation 〜.
Proposition 2 (Eigendecomposition of HωCK). Let HωCK be a convolutional kernel with local average
pooling as defined in Eq. (8). Then HωCK admits the following eigendecomposition:
HωK(x, y) = ωξq,0 + XXX Kr(S)ξq,' ∙ ψj,s(χ)ψj,s(y),	(9)
'=1 S∈C' j∈[d]
where (denoting k + S the translated set S by k positions with cyclic convention in [d])
Kj = 1 + 2 X (1 - k∕ω)cos (2∏jk ), ψj,s (X) =或 X e 2idk Yk+S (x) ∙	(10)
k=1	k=1
First notice that, as long as gcd(ω, d) = 1, the RKHS associated to HCK contains the same set of
functions as the RKHS of HCK, i.e., all local functions L2 (Qd, Locq). (There are gcd(ω, d) - 1
number of zero weights: κj = 0 for allj ∈ [d- 1] such that dis a divisor of jω. See Appendix A.3
for details.) However HCK will penalize different frequency components of the functions differently.
Denote fj (X) thej -th component of the discrete Fourier transform of the function, i.e., fj (X) =
√d Pk∈[d] Pkf (tk ∙x) where Pj = e2iπj7d and tk ∙x = (xk+1,...,xd,x1,...,xk) is the cyclic shift
by k pixels. Then HCK reweights the eigenspaces associated with fj(X) by a factor κj, promoting
low-frequency components (κj > 1) and penalizing the high-frequencies (κj < 1). In words,
pooling biases the learning towards low-frequency functions, which are stable by small shifts.
Let us focus on two special choices here: the pooling parameter ω = 1 and ω = d. When ω = 1,
HωCK reduces to HCK (κj = 1 for allj ∈ [d]) which does not bias towards either low or high
frequency components. When ω = d, we denote such kernel HωCK=d by HGCPK which corresponds to
global average pooling. In this case, we have κd = dand κj = 0 forj < d which enforces exact
invariance under the group of cyclic translations. More precisely, HGCPK has RKHS that contains all
cyclic q-local functions f(X) = Pk∈[d] g(X(k)) ∈ L2 (Qd, CycLocq) (c.f. Eq. (CYC-LOC)).
We obtain a bound on the test error of KRR with HωCK similar to Theorem 1, but with d replaced by
an effective dimension deff .
7
Under review as a conference paper at ICLR 2022
Theorem 4 (Generalization of KRR with average pooling (fixed d, q)). Assume that h : R → R
has ξq,0 = 0 and satisfies (A1). Further assume (A20) that k(HωCK /ω)-β /2 f? kL2 ≤ B. Define
deff = Pj∈[d]κ3>0(κj/ω)^α. Then there exists C1,C2, C3 > 0 constants independent of d, such
thatfor n ≥ Ci max(kf*∣∣L∞, deff) and setting λ* = C2(deff/n) αβ+1, we get
αβ
Eε{R(f*,fλ*)} ≤ C3 (defy"”.	(11)
By Jensen's inequality, We have deff ≤ d∕ω1∕α. In particular, for global pooling, deff = 1 and the
bound (11) does not depend on d at all. Adding average pooling improve by a factor ω1∕α the
upper bound on the sample complexity for fitting loW-frequency functions. Can We confirm this
statistical advantage using the predictions for KRR in high dimension? Consider first the case of
global pooling:
Theorem 5 (Generalization of KRR With HGCPK in high-dimension (informal)). Let f? ∈
L2 (Qd, CycLocq) and h : R → R verifying some ‘genericity condition’. Then for n = qs-i+ν
with 0 < ν < 1, and λ = O(1), we have (PE≤s,ν is defined as in Theorem 2)
fλ = PE≤s,ν f? + Oq ⑴.	(12)
Hence, global average pooling results in an improvement by a factor d in statistical efficiency When
fitting cyclic local functions, compared to HCK . This improvement Was already noticed in Mei et al.
(2021b); Bietti et al. (2021) but in the case of q = d (fully connected neural netWorks).
For ω < d, the asymptotic frameWork Mei et al. (2021a) is more challenging to implement. HoW-
ever, We present in Appendix C.1 a simplified kernel With non-overlapping local pooling Which We
believe captures the statistical behavior of local pooling. In this case, We shoW that Theorem 5 holds
with n = (d∕ω) ∙ qs-1+ν, which interpolates between Theorem 2 (ω = 1) and Theorem 5 (ω = d).
Downsampling: Often pooling is associated With a doWnsampling operation, Which subsample
one every ∆ output coordinates. In Appendix A.4, we characterize the eigendecomposition of HωCK,∆
(Proposition 4) and prove for the popular choice ω = ∆, that downsampling does not modify the
cyclic invariant subspace j = d (Proposition 5). More generally, we conjecture and check nu-
merically that downsampling with ∆ ≤ ω leaves the low-frequency eigenspaces approximately
unchanged. In particular, the statistical complexity of learning low-frequency functions is not modi-
fied by downsampling operation in the one-layer case (while downsampling is potentially beneficial
in further layers).
2.4	Multilayer convolutional kernels
For completeness, we briefly discuss here some intuitions of multilayer convolutional kernels.
The benefit of depth in convolutional kernels has been investigated in Cohen & Shashua (2016b);
Mhaskar & Poggio (2016); Scetbon & Harchaoui (2020); Bietti (2021). In particular, Bietti (2021)
observed that the top layer operation of a two-layers convolutional kernel can be replaced by a
low-degree polynomial without a performance change.
Here, as a concrete example, we consider a two-layers convolutional kernel with ω-local pooling on
first layer, quadratic kernel and global pooling on the second layer (see Appendix A.5 for details):
Hω2CK(x, y)
h
k,k0∈[d] t,t0∈[ω]
u,u0 ∈[q] r,r0 ∈[ω]
(hx(k+u+t), y(k0+u+t0)i
h ( hx(k+u0+r), y(k0+u0+r0)i
(	q
(13)
While we believe techniques contained in this paper could be used to study kernels of the type (13),
we leave it to future work. Here we only comment on the structure of Hω2CK: 1) Including a second
convolutional layer allows interactions between patches: the RKHS of (13) contains functions of
the form f (x) = P∣k-ι∣≤q gki(x(k), x(i)). 2) Pooling on the first layer encourages interactions that
do not rely too much on the relative position of the two patches. 3) Pooling on the second layer
penalizes functions that depend on the absolute position of the interaction. For more layers and
higher degree kernels, one obtain hierarchical interactions of higher-order, with multi-scale absolute
and relative local invariances brought by pooling layers.
8
Under review as a conference paper at ICLR 2022
Figure 1: Learning low-frequency (left) and high-frequency (right) cubic polynomials over the hy-
percube d = 30, using KRR with HFC (FC), HGFCP (FC-GP), HCK (CK), HωCK (CK-LP) and HGCPK
(CK-GP), and regularization parameter λ = 0+ . We report the average and the standard deviation
of the test error over 5 realizations, against the sample size n.
3	Numerical simulations
In order to check our theoretical predictions, we perform a simple numerical experiment on simu-
lated data. We take X 〜Unif(Qd) with d = 30, and consider two target functions:
fLF,3 (X) = -1= X Xixi+1xi+2 ,	fHF,3 (X) =	X (Ty • xixi+1xi+2 .	(14)
d i∈[d]	d i∈[d]
Here fLF,3 is a cyclic-invariant local polynomial (fLF,3 is ‘low-frequency’). The function fHF,3 is a
high-frequency local polynomial, and is orthogonal to the space of cyclic invariant functions. On
these target functions, we compare the test error of kernel ridge regression with 5 different kernels:
a standard inner-product kernel HFC(X, y) = h(hX, yi/d); a cyclic invariant kernel HGFCP (X, y)
(convolutional kernel with global pooling and full-size patches q = d); a convolutional kernel HCK
with patch size q = 10; a convolutional kernel with local pooling HωCK with q = 10 and ω = 5;
and a convolutional kernel with global pooling HGCPK with q = 10. In all these kernels, we choose a
common h(t) = Pi∈[5] 0.2 * ti which is a degree 5-polynomial.
In Figure 1, we report the test errors of fitting fLF,3 (left) and fHF,3 (right) using kernel ridge
regression with these 5 kernels. We choose a small regularization parameter λ = 10-6, and
the noise level σε = 0. The curves are averaged over 5 independent instances and the error
bar stands for the standard deviation of these instances. The results match well our theoreti-
cal predictions. For the function fLF,3, the sample sizes required to achieve vanishing test errors
are ordered as HGCPK < HωCK < HCK < HGFPC < H FC and are around the predicted thresholds
q2 < dq2 /ω < d2 < dq2 < d3 respectively. Next we look at the test error of fitting the high
frequency local function fHF,3. The test errors of HCK and HFC are the same for fHF,3 and fLF,3: this
is because these kernels do not have bias towards either high-frequency or low-frequency functions.
The kernel HωCK perform worse on fHF,3 than on fLF,3 : this is because the eigenspaces of HωCK are
biased towards low-frequency polynomials. The kernels HGCPK and HGFCP do not fit fHF,3 at all (test
error greater than or equal to 1): this is because the RKHS of these two kernels only contain cyclic
polynomials, but fHF,3 is orthogonal to the space of cyclic polynomials.
4	Discussion and Future Work
In this paper, we characterized in a stylized setting how convolution, average pooling and downsam-
pling operations modify the RKHS, by restricting it to q-local functions and then biasing the RKHS
towards low-frequency components. We quantified precisely the gain in statistical efficiency of KRR
using these operations. Beyond illustrating the ‘RKHS engineering’ of image-like function classes,
these results can further provide intuition and a rigorous foundation for convolution and pooling
operations in kernels and CNNs. A natural extension would be to study the multilayer convolutional
kernels in details and consider other pooling operations such as max-pooling. Another important
question is how anisotropy of the data impacts the results of this paper: in particular, it was shown
that pre-processing (whitening of the patches) greatly improves the performance of convolutional
kernels Thiry et al. (2021); Bietti (2021). A more challenging question is to study how training and
feature learning can further improve the performance of CNNs outside the kernel regime.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In Advances in Neural Information Processing Systems, pp. 6676-6688. 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, pp. 8139-8148, 2019.
Francis Bach. Learning Theory from First Principles. 2021.
William Beckner. Inequalities in Fourier analysis. Annals of Mathematics, pp. 159-182, 1975.
William Beckner. Sobolev inequalities, the Poisson semigroup, and analysis on the sphere Sn .
Proceedings of the National Academy of Sciences, 89(11):4816-4819, 1992.
Alberto Bietti. Approximation and learning with deep convolutional models: a kernel perspective.
arXiv preprint arXiv:2102.10032, 2021.
Alberto Bietti, Luca Venturi, and Joan Bruna. On the sample complexity of learning with geometric
stability. arXiv preprint arXiv:2106.07148, 2021.
Aline Bonami. Etude des coefficients de Fourier des fonctions de Lp(G). In Annales de l’institut
Fourier, volume 20, pp. 335-402, 1970.
Stephane Boucheron, Olivier Bousquet, and Gabor Lugosi. Theory of classification: A survey of
some recent advances. ESAIM: probability and statistics, 9:323-375, 2005.
Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in visual
recognition. In Proceedings of the 27th international conference on machine learning (ICML-10),
pp. 111-118, 2010.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment
explain generalization in kernel regression and infinitely wide neural networks. Nature communi-
cations, 12(1):1-12, 2021.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331-368, 2007.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2933-2943, 2019.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dudlk (eds.), Proceedings
of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of
Proceedings of Machine Learning Research, pp. 215-223, Fort Lauderdale, FL, USA, 11-13 Apr
2011. PMLR.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decom-
positions. In International Conference on Machine Learning, pp. 955-963. PMLR, 2016a.
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. arXiv preprint arXiv:1605.06743, 2016b.
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka ZdebOrova. Generalization error rates
in kernel regression: The crossover from the noiseless to noisy regime. arXiv preprint
arXiv:2105.15004, 2021.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances in Neural Information
Processing Systems, pp. 2253-2261, 2016.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 1675-1685. PMLR, 09-
15 Jun 2019a.
10
Under review as a conference paper at ICLR 2022
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):
1-50, 2010.
Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
sionality in convolutional teacher-student scenarios. arXiv preprint arXiv:2106.08619, 2021.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? Advances in Neural Information Processing Systems, 33,
2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. The Annals of Statistics, 49(2):1029-1054, 2021.
Leonard Gross. Logarithmic sobolev inequalities. American Journal of Mathematics, 97(4):1061-
1083, 1975.
Zaid Harchaoui, Francis R Bach, and Eric Moulines. Testing for homogeneity with kernel fisher
discriminant analysis. In NIPS, pp. 609-616. Citeseer, 2007.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, pp. 8580-
8589, 2018.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Kernel
alignment risk estimator: Risk prediction from training data. arXiv preprint arXiv:2006.09796,
2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.
Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efficient than
fully-connected nets? arXiv preprint arXiv:2010.08515, 2020.
Tengyuan Liang, Alexander Rakhlin, et al. Just interpolate: Kernel “ridgeless” regression can gen-
eralize. Annals of Statistics, 48(3):1329-1347, 2020.
Julien Mairal. End-to-end kernel learning with supervised convolutional kernel networks. arXiv
preprint arXiv:1605.06265, 2016.
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
arXiv preprint arXiv:1406.3332, 2014.
Eran Malach and Shai Shalev-Shwartz. Computational separation between convolutional and fully-
connected networks. arXiv preprint arXiv:2010.01369, 2020.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random fea-
tures and kernel methods: hypercontractivity and kernel matrix concentration. arXiv preprint
arXiv:2101.10588, 2021a.
11
Under review as a conference paper at ICLR 2022
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021b.
Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory
perspective. AnalysisandAPPUcations,14(06):829-848, 2016.
Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.
Electronic Communications in Probability, 18, 2013.
Meyer Scetbon and Zaid Harchaoui. Harmonic decompositions of convolutional networks. In Inter-
national Conference on Machine Learning, pp. 8522-8532. PMLR, 2020.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge University Press, 2014.
Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan Ragan-Kelley, Ludwig
Schmidt, and Benjamin Recht. Neural kernels without tangents. In International Conference on
Machine Learning, pp. 8614-8623. PMLR, 2020.
Louis Thiry, Michael Arbel, Eugene Belilovsky, and Edouard Oyallon. The unreasonable effective-
ness of patches in deep convolutional kernels methods. arXiv PrePrint arXiv:2101.07528, 2021.
Martin J Wainwright. High-dimensional statistics: A non-asymPtotic viewPoint, volume 48. Cam-
bridge University Press, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv:1811.08888, 2018.
12
Under review as a conference paper at ICLR 2022
Contents
1	Introduction	1
1.1	Summary of main results ..................................................... 3
1.2	Related work ................................................................ 4
2	Main results	4
2.1	Functions on the hypercube and eigendecomposition of kernel operators ....... 5
2.2	One-layer convolutional kernel .............................................. 5
2.3	Local average pooling and downsampling ...................................... 7
2.4	Multilayer convolutional kernels ............................................ 8
3	Numerical simulations	9
4	Discussion and Future Work	10
A	Details from the main text	14
A.1 Notations ................................................................... 14
A.2 Convolutional neural tangent kernel ......................................... 14
A.3 Local average pooling operation ............................................. 15
A.4 Downsampling operation ...................................................... 16
A.5 Multilayer convolutional kernels ............................................ 20
A.6 Proofs diagonalization of convolutional kernels ............................. 21
A.7 Additional numerical simulations ............................................ 23
B	Generalization error of kernel methods in fixed dimension	25
B.1	Bound on kernel methods using Rademacher complexities ...................... 25
B.2	Generalization error of KRR in the classical regime ........................ 26
C Generalization error of KRR in high dimension	29
C.1	Generalization error of convolutional kernels in high dimension ............ 30
C.2	Checking the assumptions ................................................... 33
C.3	Proof of Proposition 6 ..................................................... 34
C.4	Proof of Theorem 7 ......................................................... 35
C.5	Proof of Theorem 8 ......................................................... 38
C.6	Auxiliary results .......................................................... 40
D Technical background of function spaces on the hypercube	44
D.1	Fourier basis .............................................................. 44
D.2	Hypercubic Gegenbauer ...................................................... 44
D.3	Hermite polynomials ........................................................ 45
13
Under review as a conference paper at ICLR 2022
D.4 Hypercontractivity of uniform distributions on the hypercube
46
A Details from the main text
A. 1 Notations
For a positive integer, we denote by [n] the set {1, 2, . . . , n}. For vectors u, v ∈ Rd, we denote
hu, vi = u1v1 + . . . + udvd their scalar product, and kuk2 = hu, ui1/2 the `2 norm. Given a
matrix A ∈ Rn×m, we denote kAkop = maxkuk2=1 kAuk2 its operator norm and by kAkF =
(Pi j A2j)1/2 its FrobeniUs norm. If A ∈ Rn×n is a square matrix, the trace of A is denoted by
Tr(A) = Pi∈[n] Aii.
We use Od( ∙) (resp. od( ∙)) for the standard big-O (resp. little-o) relations, where the subscript d
emphasizes the asymptotic variable. Furthermore, We write f = Ωd(g) if g(d) = Od(f (d)), and
f = ωd(g) if g(d) = Od(f(d)). Finally, f = Θd(g) if we have both f = Od(g) and f = Ωd(g).
We use Od,p( ∙) (resp. θd,p( ∙)) the big-O (resp. little-o) in probability relations. Namely, for hι(d)
and h2(d) two sequences of random variables, h1(d) = Od,P(h2(d)) if for any ε > 0, there exists
Cε > 0 and dε ∈ Z>0, such that
P(|h1(d)/h2(d)| > Cε) ≤ε,	∀d≥ dε,
and respectively: h1(d) = od,P(h2(d)), if h1(d)/h2(d) converges to 0 in probability. Similarly, we
will denote hι(d) = Ωd,p(h2(d)) if h2(d) = Od,p(hι(d)), and hι(d) = ωd,p(h2(d)) if h2(d)=
od,P(h1 (d)). Finally, h1 (d) = Θd,P(h2(d)) if we have both h1(d) = Od,P(h2(d)) and h1(d) =
Ωd,p(h2(d)).
A.2 Convolutional neural tangent kernel
In this section, we justify the expression of the convolutional neural tangent kernel HwCK,∆ (CK-
AP-DS), obtained as the tangent kernel of a neural network composed of a one convolution layer
followed by local average pooling and downsampling (CNN-AP-DS).
Proposition 3. Let σ ∈ C1 (R) be an activation function. Consider the following one-layer convo-
lutional neural network with ω-local average pooling and ∆-downsampling:
fNNN(x; Θ) = X X aik X σ(hwi, X(k∆+s)i) .	(15)
i∈[N] k∈[d∕∆]	s∈[ω]
Let a0k 〜i.i.d. N(0,1) and √∕qw0	〜i.i.d. Unif(Qq) independently, and Θ0	=
{(a0k )i∈[N ],k∈[d∕∆], (w0)i∈[N ]}. Then there exists h : [—1,1] → R, such that for any x, y ∈ Qd,
we have almost surely
N→∞〈V®fNNN(x; Θ0), V®fNNN(y; Θ0)>/N = Σ Σ h(hχ(k∆+s), y(k∆+so)i∕q). (16)
k∈[d∕∆] s,s0∈[ω]
Proof of Proposition 3. For u, v ∈ Qq, define
h(1)(hu, V∕q = Ew〜Unif(Qq) [σ({u, w'i∕∕q)σ(hv, wi∕√q)],
h⑵(hu, vi∕q) = Ew〜Unif(Qq) [σ0({u, w}/√q)σ'(hv, w}/√q)hu, vi]/q .
The functions h(1), h(2) are well defined (the RHS only depend on the inner product hu, vi) and can
be extended to functions h(1), h(2) : [—1, 1] → R.
Computing the derivative of the convolutional neural network with respect to a =
(ai0k)i∈[N],k∈[d∕∆], we have
N (VafNNN (x; Θ0), VafNNN (y; Θ0))
=XX N X σ(hw0, x(k∆+s)i)σ(hw0, x(k∆+s0)i).
k∈[d∕∆] s,s0∈[ω]	i∈[N]
14
Under review as a conference paper at ICLR 2022
Hence by law of large number, we have almost surely
Nl→∞ NN〈VafNNN(x； Θ0), VafNNN(y; Θ0)) = X X h ⑴(hx(k∆+s), y(k∆+s,)i∕q).
k∈[d∕∆] s,s0∈[ω]
Similarly, computing the derivative with respect to √qW = (√qw0)i∈[N] gives
N(VwfNNN(x; Θ0), VwfNNN(y; Θ0)>
= XX NN X aik aik0 σ'(hwi, x(k∆+s)'i')σ'{hW0 x(k0∆+s0)i) hx(S+S), x”+SO)i .
k,k0∈[d∕∆] s,s0∈[ω]	i∈[N]	q
By law of large number, using that aik and aik0 are independent of mean zero and variance 1, we
get almost surely
Nim∞ N(VWfNNN(x； θ0), VWfNNN(y; θ°)> = X X h(2)(hχ(k∆+S), y(k∆+S0)i∕q) .
k∈[d∕∆] S,S0∈[ω]
Taking h = h(1) + h(2) concludes the proof.	□
A.3 Local average pooling operation
Consider a function f ∈ L2(Qd): we can decompose it as
f (X) = √d X fj(X),
fj(X) = √d X Pj f (tk ∙ X),
d k∈[d]
(17)
(18)
where Pj = e 2iπj and tj ∙ X = (xk+ι,... ,xd,xι,..., Xk) is the cyclic shift of X by k pixels. We
can think about fj (X) as the j-th component of the discrete Fourier transform of the function f(X)
seen as a d-dimensional vector {f (tj ∙ X)}j∈[d] for any X ∈ Qd.
Notice furthermore that if f is a local function, i.e., f can be decomposed as a sum of functions on
patches f(X) = Pk∈[d] gk(X(k)), then we can write
fj(X) =	√d	X	Pjf(tk	∙ X) =	√d	X	Pjgu(X(U+k))	=	√d	X	Pjgj(X(k)),
k∈[d]	k,u∈[d]	k∈[d]
where we denoted (v ∈ Qq)
gj (V)= X P-Ugu(V).
u∈[d]
In particular, decomposing gj in the Fourier basis, We get (denoting CS = hgj, YS〉l2),
fj(X) = √d X Pjgj(X(k))= X CS ∙ √d X PjYk+s(X),
d k∈[d]	S⊆[q]	d k∈[d]
which shows that the j-th frequency component fj is in the span of {Yj,S}S⊆[q] . In particular,
applying average pooling operation in the kernel will reweight this eigenspace by a factor κj .
Let us further comment on the values of κj . First, we have
ω
κj = X (1- k∕ω)Pjk .
k=-ω
In particular, the maximal eigenvalue is attained at j = d with κd = ω, which corresponds to the
subspace of cyclic invariant functions. Furthermore, κj = 0 if and only if d is a divisor of jω for
j ≤ d - 1, i.e., j is a multilple of gcd(ω, d). There are gcd(ω, d) - 1 such zero eigenvalues.
15
Under review as a conference paper at ICLR 2022
In convolutional kernels, a weighted average is often preferred to local average pooling (Mairal
et al., 2014; Mairal, 2016; Bietti, 2021): in that case we consider τ : R → R and obtain the kernel
HCK(x, y) = d X	T(d(s))τ(d(s0))h(hx(k+s), y(k+s，)i/q),
k,s,s0 ∈[d]
where d(S) = min(S, d - S) (the distance between k + S and k on [d] with cyclic convention). Note
that HτCK has the same eigendecomposition as HωCK but with different weights κj .
A popular choice for T is the Gaussian filter T (x) = √∏σ e

x2
2σ2. In Figure 2, We compare the
eigenvalues κj for local average pooling and Gaussian filter with different value of ω and σ2 . Note
that the eigenvalue decay controls how much high-frequencies are penalized: faster decay induces
heavier penalty on the high-frequency components.
lol
O O -
Il
W sn>u山
10~2 二	XX ∖ ∖
O 20	40	60	80 IOO
index (local pooling)
U 1
1010^
10-5
10-6
O 20	40	60	80 IOO
index (Gaussian filter)
Figure 2: Decay of the weights κj for different lenfths ω for local average pooling (on the left) and
bandwidths σ2 for pooling with Gaussian filter (on the right), for d = 101.
A.4 Downsampling operation
As mentioned in the main text, a downsampling operation is often added after pooling. The kernel
is given by
HωK∆(χ,y) = ∆ X X h (hχ(k∆+s), y(k∆+soGlq .	(19)
k∈ [d/∆] s,s0∈ [ω]
Let us introduce the family {Mr}r∈[q] of block-circulant matrices defined by
Mij = ω(q ： - r) ∣{(k, s, s0,t) ∈ 工ω,∆,r : k∆ + S + t ≡ i[d],k∆ + s0 + t ≡ j[d]}∣,	(20)
where we introduced the set of indices
Iω,∆,r = n(k, S, s0,t): k ∈ [d∕∆], s, S0 ∈ [ω], 0 ≤ t ≤ q - r} .	(21)
We can now state the eigendecomposition of HωC,K∆ in terms of the eigenvalues and eigenvectors of
the matrices {M r}r∈[q] .
Proposition 4 (Eigendecomposition of HωC,K∆). Let HωCK,∆ be a convolutional kernel with local av-
erage pooling and downsampling, as defined in Eq. (19). Then HωCK,∆ admits the following eigende-
composition:
HωK(x, y) = ωξq,o+ XXX ξq”,κj ∙ ψj,s(X)ψj,s(y),	(22)
'=1 S∈C' j∈[d]
where ψj∆,S (x) = Pkd=1 vjS,k Yk+S (x) with {κjS, vjS}j∈[d] eigenvalues and eigenvectors of M γ(S).
16
Under review as a conference paper at ICLR 2022
Let us make a few comments on these matrices Mγ(S) . First because they only depend on S
through the diameter γ(S), the eigenvalues and eigenvectors {κjS, vjS}j∈[d] only depend on γ(S).
Second, we see that M(γi(+S∆) )(j+∆) = Miγj(S) and Miγj(S) = 0 if d(i, j) ≥ ω, where d(i, j) =
min(|i - j|, d - |i - j|) (i.e., the distance between i and j on the torus [d]). In words M γ(S) is a
symmetric block-circulant matrix with non-zero elements on a band of size ω - 1 on the left and
right of the diagonal, and on the upper-right and lower-left corners. Furthermore, notice that
Tr(M Y(S)) = dj(S)|n(k, s,t) : k ∈ [d∕∆], s ∈ [ω], 0 ≤ t ≤ q - Y(S )}∣ = 1,
which is independent of ω, ∆, γ(S) and justify the chosen normalization. In particular, this implies
that (for ξq,0 = 0)
τr(HωK∆) := Eχ{HωK∆(χ,X)} = E ξq,' E r(S) = E ξq,'B(Qq; `) = h ⑴,(23)
'∈[q]	S∈C'	'∈[q]
is also independent of the parameters (q, ω, ∆).
Example 1. Take ∆ = 3, ω= 5, q = 11, then
	18 15 11	7	4	0		∖
	15 19 15	11	8	4	0	...
	11 15 18	14 11	7	3	0	
	7	11 14	18 15 11	~1 ~~3^^0^	
M1 = ɪ	4	8	11	15 19 15	11	8	4	...
50	047	11 15 18	14 11 7	
	03			
	0	. . .	. . .	...
	∖			/
and
13	11	8	5	3	0	
11	14	11	8	6	3	0
8	11	13	10	8	5	2	0
5	8	10
,	3
M4 = 一	3	6	8
35	0	3	5
∖
/
Remark A.1. Symmetric block-circulant matrices can be easily diagonalized as follows. Consider
M = CirCUlant(Bι, B2,..., Bm) where Bk ∈ Rδ×δ, BT = Bi and B2+k = Bm-k for
k = 0,..., m — 2. Denote Pj = e2iπj7m and Yj (V) = [v, ρj∙ v, ∙ ∙ ∙ , ρm-1v]∕√m ∈ Rmδ for any
v ∈ R∆. Introduce for j = 0, . . . , m - 1, the matrix Hj ∈ R∆×∆ given by
Hj = B1 + ρjB2 + . . . + ρjm-1Bm .
(24)
The matrix Hj is Hermitian and we denote (λj,s)s∈[∆] and (vj,s)s∈[∆] its eigenvalues and
eigenvectors. Then the eigenvalues and eigenvectors of M are given by {λj,s}j∈[m],s∈[∆] and
{Yj(vj,s)}j∈[m],s∈[∆].
In particular, if∆ = 1 and M = CirCUlant(b1, b2, . . . , bm) is a circulant matrix, then the eigenvalues
are simply given by
λj = b1 + ρjb2 + . . . + ρjm-1bm ,
and eigenvectors Vj = [1,ρj∙, ∙∙∙ , ρm-i]/√m.
Here we will focus on the impact of downsampling for single-layer convolutional kernels. We expect
the downsampling operation to have a much more important role for the next layers: for example,
increasing the scale of interactions or reducing the dimensionality of the pixel space.
17
Under review as a conference paper at ICLR 2022
We will argue below that adding a downsampling operation after local pooling leaves the low-
frequency components approximately unchanged (while potentially modifying the high-frequency
eigenspaces). We consider ∆ ≤ ω: for ∆ > ω, some basis functions YS with S ∈ e` are in the null
space of HωCK,∆, which impact all frequencies.
To emphasize the dependency on ω, ∆, denote Mωr,∆ the matrix (20). We will study the change in
the matrix Mωr,1 when adding downsampling ∆, and consider
Mωr,∆ = Mωr,1 + Arω,∆ ,	(25)
where we denote Arω,∆ = Mωr,∆ - Mωr,1. Notice that Arω,∆ is a symmetric block-circulant ma-
trix. Therefore, from Remark A.1, the eigenvectors of Arω,∆ are given by {γj (vj,s)}j∈[m],s∈[∆]
where d = m∆ and Yj (vj,s) = [vj,s,pm,j Vj,s,..., pm- 1Vj,s] With ρm,j = e 2mπj and (vj,s)s∈[∆]
eigenvectors of Hj (24). The eigenvectors of M£/are given by Ut = [1, pd,t,…，Pd- 1]∕√d with
Pd,t = e2idπt. Notice that
hut,Yj(Vj,s)i = √d= X X PmjIP-tm(Vj,s)u
dm k∈[m] u∈[∆]
=√= ( X P-((UT(V j,s )u) ∙ X (Pmj p-,δ )k 1
dm
m U∈[∆]	k∈[m]
which is 0 except when t ≡ j [m]. Hence, we see that Arω,∆ in Eq. (25) only modify the eigenspaces
of Mωr,1 as follows: the eigendirections {γj (vj,s)}j∈[m],s∈[∆] coming from Hj (24) only modify
the eigenspaces of Mωr,1 spanned by {uam+j}a=0,...,∆-1.
For simplicity, we will focus on the popular choice ∆ = ω. Furthermore, we will only look at the
impact of the eigenvalues H0 on the eigenspace spanned by {uam}a=0,...,∆-1, which contain the
cyclic invariant direction. We show below that H0 = 0 and therefore Arω,ω does not modify the
cyclic invariant eigenspace of Mωr,1:
Proposition 5. Consider d = mω and the symmetric block-circulant matrix Arω,ω = Mωr,ω -Mωr,1.
Denote Arω,ω = Circulant(B1, B2, . . . , Bm) and
H0 = B1 + . . . + Bm .
We have the following properties:
(a)	If q + 1 - r ≡ 0[ω], then Arω ω = 0, and downsampling does not modify the matrix
Mr = Mr 1.	,
ω,ω	ω,1
(b)	We have H0 = 0 and downsampling does not modify the cyclic invariant eigenspace
Arω,ω1 = 0.
Proof of Proposition 5. Let us first start by proving point (a). Consider q + 1 - r = pω. Fix
i ∈ {0, . . . , ∆ - 1} and κ ∈ {0, . . . , ω - 1}. Let us compute the entry (i, i + κ) of the matrix
Mcr,ω: this amounts to counting the number of quadruples (k, s, s0,t) with k ∈ [d∕ω], s, s0 ∈ [ω]
and 0 ≤ t ≤ pω - 1, satisfying (kω + s + t, kω + s0 + t) ≡ (i, i + κ)[d]. Notice that we must have
s0 = s + κ and therefore s ∈ {0, . . . , ω - 1 - κ}. Notice that for each interval uω ≤ t < (u + 1)ω
with u ∈ {0, . . . ,p - 1}, there are exactly ω - κ ways of choosing s and then t and k to satisfy the
equality. We deduce that
(Mωr,ω )i(i+κ) = ω(q +1 - r) p(ω — K) = I — ω =(Mω,J(i+κ).
By symmetry of Mωr,ω , this concludes the proof of point (a).
Consider now point (b). First notice, because Mωr,ω has zero entries for min(|i —j|, d— |i —j|) ≥ ω,
the only non-zero blocks are B1, B2 and Bm. Furthermore, when computing H0, the diagonal
entries only have one contribution from the diagonal elements of B1 . The off-diagonal elements of
H0 have two contribution: one from B1 and one from B2 (if below the diagonal) or Bm (if above
the diagonal), i.e.,
(H0)ii = (B1)ii	(H0)i(i+κ) = (B1)i(i+κ) + (Bm)i(i+κ) .
18
Under review as a conference paper at ICLR 2022
Let us compute first the diagonal elements: we have easily, by a similar argument as above
(Mωr,ω)ii = 1 = (Mωr,1)ii, and therefore H0 has zero zero diagonal entries. For off-diagonal
elements, first notice that (Mωr,ω)i(i+κ-ω) = (Mωr,ω)i(i+ω-κ). Then for q + 1 - r = pω + v, we
can consider each subsegment uω ≤ t < (u + 1)ω separately, and by a simple counting argument,
get(Mωr,ω)
implies H0
li(i+ω-κ) + (Mω,ω )i(i+K) = 1 - ω .Wededucethat (Ho)通+-) = 0, which by symmetry
□
0 and concludes the proof.
From the above result, we conjecture that more generally, for ∆ ≤ ω, the low-frequency eigenspaces
of HωCK remain approximately unchanged when applying a downsampling operation. We verify this
conjecture numerically in several examples. In Figure 3, we plot the eigenvalues κj with and without
downsampling. On the left, we compare κj for fixed ω = 25 and increasing ∆. We notice that the
eigenvalues do not change much for ∆ ≤ ω, and for ∆ > ω, some κj become null, as discussed
above. On the right, we plot κj for ∆ = 1 (continuous line) and ∆ = ω (dashed lines) for several ω .
As conjectured, the top eigenvalues (low-frequency) are left approximately unchanged. In Figure 4,
we plot a heatmap of the eigenvectors ordered vertically from highest associated eigenvalue (bottom)
to lowest (top) for a fixed ω = 25 and increasing downsampling ∆ ∈ {1, 25, 40}. First indeed
check that the top eigenvectors correspond to low-frequency functions and the bottom eigenvectors
correspond to high-frequency functions. Second, most eigenvectors are not much modified between
∆ = 1 and ∆ = ω = 25. For the case, ∆ > ω, the top eigenvectors corresponds still low-frequency
functions.
index	index
Figure 3: Impact of downsampling on the eigenvalues κj . On the left, we fix ω = 25 (d = 200,
q = 30, r = 1) and increase δ from 1 (no downsampling) to 40. On the right, we compare ∆ = 1
(continuous line) and ∆ = ω (dashed lines), with d = 150,q = 20,r = 1.
From these observations, we expect HωCK,∆ to have the same statistical properties as HωCK when learn-
ing low-frequency functions. In Figure 5, we plot the test error of kernel ridge regression for fitting
cyclic q-local polynomials (see Section A.7) on the hypercube of dimension d = 30. We report
the test error of one realization, against the sample size n, and choose regularization λ = 10-6 and
noise σε = 0. We compare kernels with and without downsampling. On the left, we consider q = 10
and ω = ∆ = 5, and compare the test error with HωCK (continous line) and with HωC,K∆ (dashed line)
when learning degree 2, 3 and 4 polynomials. On the right, we fix the target function to be the cubic
local cyclic polynomial and consider the test error of learning with HωCK,∆ for q = 10, ω = 10, and
∆ ∈ {1, 3, 6, 10}. As expected, we observe in both simulations that the test error is almost identical
between the kernels with and without downsampling, when learning cyclic invariant functions.
In Section C.1, we further check that downsampling with ∆ > ω does not improve the high-
dimensional predictions for the test error of KRR.
19
Under review as a conference paper at ICLR 2022
Figure 4: Heatmap of the eigenvectors {vj }j∈[d] ordered from highest associated eigenvalue (bot-
tom) to lowest (top), for d = 200, q = 30, r = 1, ω = 25, and ∆ = 1 (left), ∆ = ω = 25 (middle)
and ∆ = 40 (right).
Figure 5: Test error of kernel ridge regression with and without downsampling. We report the test
error of one realization, against the sample size n. On the left, we consider a unique architecture
q = 10 and ω = ∆ = 5, and compare HωCK (continuous line) versus HωCK,∆ (dashed line) when
learning cyclic q-local polynomials of degree 2, 3 and 4. On the right, we consider a unique cyclic
q-local polynomial of degree 3 for fixed q = 10, ω = 10 and ∆ ∈ {1, 3, 6, 10}.
A.5 Multilayer convolutional kernels
As an example, we will consider a two layers convolutional kernel with patch and local average
pooling sizes (q1, ω1) on the first layer and (q2, ω2) on the second layer. We consider a general
inner-product kernel for the first layer:
h1 hu, vi/q1 = hψ(u), ψ(v)i ,
(26)
where the feature map is given explicitly ψ(u) = {ξqι,∣s∣Ys(u)}s⊆[qι] ∈ R2q1. Following the work
Bietti (2021), we consider a degree-2 polynomial kernel on the second layer, i.e., h2 (hφ, φ0i) =
hφ, φ0i2.
Let us decompose this two-layers convolutional kernel in the Fourier basis. Let Ψ(x) =
{Ψk(x)}k∈[d] be the output of the first layer, with
ψk(X)= X ψ(X(k+S))= {ξqι ,|S| X Yk+s+S (x)
s∈[ω1]
S⊆[q1] ∈R2q1.	(27)
s∈[ω1]
20
Under review as a conference paper at ICLR 2022
Then denoting Ψ(k) (x) = (Ψk+1 (x), . . . , Ψk+q2 (x)), the two-layers convolutional kernel is given
by
Hω2C1,Kω2 (x, y)
=XXhΨ(k+s)(x), Ψ(k+s0)(x)i2
k∈[d] s,s0∈[ω2]
(28)
=XX X X
k∈[d] s,s0 ∈[ω2] u,u0∈[q2] t,t0 ,r,r0 ∈[ω1]
hψ(x(k+s+u+t) ) Z) ψ(x(k+s+u0+r) ), ψ(y(k+s0 +u+t0) ) Z) ψ(y(k+s0+u0+r0) )i .
Let us comment the structure of Hω2C1,Kω2 :
1.	The associated RKHS, which we will denote H2CK, contains all the homogeneous poly-
nomials YS with S = S1 ∪ S2 with S1, S2 contained on segments of size q1, with the
two segments separated by at most q2 + ω2 - 2. In words, the RKHS contains interaction
between patches x(k) and x(k0) that are within some distance.
2.	The eigenvalue associated to a degree-k homogeneous polynomials is still of order q-k in
high-dimension. To learn functions restricted to L2(Q2, Locq), it is statistically more effi-
cient to use HCK (smaller degeneracy of eigenvalues). However H2CK will fit a richer class
of functions with two-patch interactions, while still not being plagued by dimensionality:
dim(H2CK) ≤ q2d22q1 . Hence we still expect H2CK to be much more statistically efficient
than a standard inner-product kernel.
3.	Local pooling on the two layers plays different roles: pooling on the first layer encour-
ages the interactions to not depend strongly on the relative positions of the patches, while
pooling on the second layer penalizes functions that depend on the global position of these
interactions.
We believe that Eq. (28) can be studied in more details, by a careful combinatorial argument and a
2-dimensional Fourier transform on the second layer (see Bietti (2021)). We leave this problem to
future work. Similarly, we can consider instead a degree-k kernel on the second layer (which would
include interactions between k patches), or three layers and deeper networks.
A.6 Proofs diagonalization of convolutional kernels
In this section, we prove the diagonalization of the kernels HCK, HωCK and HωC,K∆ introduced in
Propositions 1, 2 and 4 respectively.
Recall that we can associate to a kernel function H : X × X → R defined on a probability space
(X, τ) (assume x 7→ H(x, x) square integrable), the integral operator H : L2(X, τ) → L2(X, τ)
Hf(x) =
X
H(x, x0)f(x0)τ(dx0) .
(29)
By the spectral theorem of compact operators, there exists an orthonormal basis (ψj)j≥1 ofL2(X,τ)
and eigenvalues (λj)j≥ι, With nonincreasing values λι ≥ λ? ≥ ∙∙∙ ≥ 0 and Pj≥ιNj < ∞, SUch
that
∞∞
H = X λjψjψj,	H(x, x0) = X λjψj (x)ψj (x0).
j=1	j=1
We first prove the diagonalization of HωC,K∆ in Proposition 4. The case of HωCK and HCK then folloWs
by setting ∆ = 1, and ∆ = ω = 1 respectively.
Proof of Proposition 4. Consider the inner-product kernel function h : R → R defined on the hy-
percube Qq. By rotational symmetry (see Section 2.1 and Appendix D), h admits the folloWing
diagonalization: for any u, v ∈ Qq,
q
h (hu, vi/q) = Eξq,' E	YS (U)YS (v) ,	(30)
'=0	S⊆[q],∣S∣='
21
Under review as a conference paper at ICLR 2022
where (YS)s⊆[q] is the Fourier basis on Qq, and ξd,'(h) is the '-th GegenbaUer coefficient of h in
dimension q (see Sections 2.1 or D for background).
Recall that We defined	s`	=	{S	⊆	[q]	: |S| = '}, the equivalence relation S 〜S0	if	S0	is a
translated subset of S in [q] (without cyclic convention), and e` the quotient set of a` by 〜.For each
equivalence class S ∈ e`, consider S the unique subset in S that contains 1. Then the equivalence
class S contains the subsets U + S = {u + k : k ∈ S} ⊆ [q] with U = 0,...,q - γ(S). By a slight
abuse of notations, we will identify S and this subset S. Below we will denote U + S the translated
subset with cyclic convention on [d] (e.g., 2 + {1, 3, d - 1} = {3, 5, 1}).
Using Eq. (30) and that YS(x(k)) = Yk+S (x), we have the following decomposition of HωC,K∆
Fourier basis
HωCK,∆ (x, y)
=一 £ E h (hx(k∆+s), y(k∆+so)”q)
ω
k∈[d∕∆] s,s0∈[ω]
q	f δ	1
= dωξq,0 +∑ξq,`∑ ω E	Yk∆+s+t+S (x)Yk∆+s0+t+S (y )	,
'=1	s∈C' ] ω (k,s,s0 ,t)∈Iω,∆,γ(S)
where we recall the definition of the set of indices
Iω,∆,γ(S) = n(k, s,s0,t) : k ∈ [d/A,s,s0 ∈ [ω], 0 ≤ t ≤ q - Y(S)O .
in the
(31)
(32)
Note that the diagonalization of the kernel H can be obtained by computing the matrix M =
(MSS0)S,S0⊆[d] ∈ R2d×2d with M = Ex,y [YS (x)H (x, y )YS0 (y )]: if λj and vj ∈ R2d are the
eigenvalues and eigenvectors of M, then λj and ψj (x) = PS⊆[d] vj,S YS (x) are the eigenvalues
and eigenvectors of H .
From Eq. (31), we see 1) the basis functions YS with γ(S) > q (subset S not contained in a segment
of size q) are in the null space ofHωCK,∆, 2) for S, S0 ⊆ [d] with S and S0 not translations of each other,
then Ex,y[YS(x)HωC,K∆(x, y)YS0(y)] = 0, and YS and YS0 are contained in orthogonal eigenspaces.
We deduce that it is sufficient to diagonalize HωC,K∆ on each of the (orthogonal) subspaces VS :=
span{Yk+s : k ∈ [d]} for 0 ≤ ' ≤ q and S ∈ e`.
For each S ∈	e`, define MY(S)	∈	Rd×d the matrix with entries Mj(S)	=
r(S)Eχ,y [Yi+S(x)Hωκ∆(x, y)Yj+S(y)]. FrOmEq. (3D, we get
Mj(S) = ωr∆S) |{(k，s，s0，t)∈ Iω,∆,γ(S) ： k∆+ S +1 ≡ i[d],k∆ + s0 + t ≡ j[d]}∣,	(33)
which concludes the proof of Proposition 4.	□
We can now prove Propositions 1 and 2 by taking ω = ∆ = 1 and ∆ = 1 respectively.
Proof of Proposition 1. Set ∆ = ω = 1 in Proposition 4. We get
Mj(S) = r(S)In(At) : k ∈ [d], 0 ≤ t ≤ q - Y(S), k + 1+t ≡ i[d], k + 1+t ≡ j[d] O J
= δij .
In this case, MY(S) is simply equal to identity, which concludes the proof.	□
Proof of Proposition 2. Set ∆ = 1 in Proposition 4. We get
Mj(S) = ωr(S) In(k, s, S0，t) ∈ Iω,∆,γ(S) : k + S + t ≡ i[d],k + s0 + t ≡ j [d] O I
=(1- I),
22
Under review as a conference paper at ICLR 2022
where d(i, j) is the distance between i and j on the torus [d] (i.e., if i > j, d(i, j) = min(i - j, d +
j - i)). Hence, M γ(S) is a circulant matrix independent of γ(S), which has well known explicit
formula for eigenvalues and eigenvectors (See for example Remark A.1).	□
A.7 Additional numerical simulations
Here, We consider a numerical experiment similar to Figure 1. We consider X 〜 Unif(Qd) with
d = 30 and consider three cyclic invariant target functions:
f2(x)=二 X XiXi+ι ,	f3(x)=二 X XiXi+1Xi+2 ,
d i∈[d]	d i∈[d]
f4(X) = √d X
d i∈[d]
xixi+1xi+2xi+3 .
We consider a higher order polynomial kernel h(x) = Pk∈[7] 0.2 ∙ Xk than in Figure 1, which should
lead to higher self-induced regularization. We consider the same kernels as before, With q =10 and
ω = 5.
In Figure 6, we report the test errors of fitting f2 (top), f3 (middle) and f4 (bottom) using kernel
ridge regression with the 5 kernels of interests in the main text. We choose a small regularization
parameter λ =10-6, and the noise level σε = 0. The curves are averaged over 5 independent
instances and the error bar stands for the standard deviation of these instances. The results again
match with our overall theoretical predictions. We report the predicted thresholds for the three
functions:
1.	For f2	target:	q < d < dq∕ω	< dq	< d2 for HCP < HGC < HK < HCK < HFC.
2.	For f3	target:	q2 < dq2∕ω <	d2 <	dq2 < d3 for HGK < HCK < HCK < HGC < HFC.
3.	For f4	target:	q3 < dq3 /ω <	d3 <	dq3 < d4 for HGCPK < HωCK < HCK < HGFCP < HFC .
We see that the kernels, especially for	f4, perform much better than their theoretical high-dimension
predictions: this can be explained by the low-dimensionality of the experiment where q =0.
23
Under review as a conference paper at ICLR 2022
1.4
1.2
1.0
-8-6
O O
」0」」⅛φ
Cyclic degree-2 polynomial, d = 30
Hrc嚼户嘤嘴
0.4-
0.2-
0.0-
1.4
1.4
1.00	1.25	1.50	1.75	2.00	2.25	2.50	2.75
Cyclic degree-3 polynomial, d = 30
1.2-
ι.o
0.8-
0.6-
0.4-
0.2-
0 0	1.00	1.25	1.50	1.75	2.00	2.25	2.50	2.75
Cyclic degree-4 polynomial, d = 30
1.2-
ι.o
0.4-
0.2-
° 0.8-
①
1 0.6-
0 0	1.00	1.25 1.50 1.75 2.00 2.25 2.50 2.75
log(n)∕log(d)
Figure 6: Learning cyclic polynomials of degree 2 (top), 3 (middle) and 4 (bottom) over the hy-
percube d = 30, using KRR with HFC (FC), HGFCP (FC-GP), HCK (CK), HωCK (CK-LP) and HGCPK
(CK-GP),regularization parameter λ = 0+ and h(χ) = Pk∈[7] 0.2 ∙ Xk. We report the average and
the standard deviation of the test error over 5 realizations, against the sample size n.
24
Under review as a conference paper at ICLR 2022
B Generalization error of kernel methods in fixed dimension
B.1 Bound on kernel methods using Rademacher complexities
We first consider the case of a Lipschitz bounded loss and uniform convergence, and make a few
simple remarks on the connection between generalization error and eigendecomposition in kernel
methods.
Consider i.i.d data (xi,yi) ∈ X × R with (x, y)〜P and a loss function ' : R X R → R that We
take 1-Lipschitz w.r.t second argument and bounded by 1. The goal is to minimize the expected loss
_ , O. _	. . , O ,	. . , 一	一--. 一一	一一	一 一一 -一 - 一	.一
L(f) = Ey,x{'(y, f(x))}. Take a RKHS H With kernel function H : X × X → R and consider
following constrained empirical risk minimizer:
fB
arg min
kfkH≤B
'(yi,f(χi))
(34)
The generalization error of fB has the following standard bound on the Rademacher complexity of
the kernel class {f : kfkH ≤ B} (Boucheron et al., 2005; Shalev-ShWartz & Ben-David, 2014):
with probability 1 - δ,
「、	，、	8B I--7——；..-	2 log 2
LjB)- k胖BL(f) ≤ √n HHE + V 丁
(35)
Note that instead of a constraint on the norm in Eq. (34), one might find more convenient to use a
penalty. In that case, there exists an equivalent to the bound (35) (Wainwright, 2019; Bach, 2021),
but we focus here on the constrained formulation for simplicity.
From the bound (35), we see that the generalization error depends crucially on the choice of B. For
simplicity, let us forget about the approximation error and take kf?kH ≤ B where f? = E{y|x}.
Recall that for a kernel H with eigenvalues {λj}j≥1 and eigenvectors {ψj}j≥1, we have
kf k2H = X λj-1 hψj, fi2L2(P) .
j≥1
Consider HωCK,∆ as in Eq. (8) and assume ξq,0 = 0. From the normalization choice of the kernel (see
Eq. (23)), we have
Ex{HωCK,∆(x,x)} = h(1).
Consider now for simplicity ∆ = 1. From the eigendecomposition in Proposition 2, the RKHS
norm of f ∈ L2(Qd, Locq) is given by
kfk2H =
'∈[q] j∈[d] S∈C'
hψj,s ,jL
ξq,'r(S)κj/d
Consider the case where f ∈ L2(Qd, Locq) has a unique non-zero component in its discrete Fourier
transform, i.e., f (x) = √ Pk∈[d] Pkg(x(k)) with E{g(x)} = 0 and Pj = e2iπj7d (see Section
A.3). Note that, denoting cS = hYS, giL2(Qq) :
q	r(S)-1
f(x)=XX	X Pj-ucu+S	ψj,S .
'=1 S∈C' ∖ u=0	)
Hence,
kfkH=X X' ； ≤d
q	r(S)-1
XXX
'=1 S∈C' u=0
cU+S
ξq,'r(S )
≤ d⅛
一Kj
where kgk2h is the RKHS norm associated to the inner-product kernel h : R → R in Qq, i.e.,
kgkh = Ps⊆[q] ξ;cSS∣. From the bound (35), we deduce the first generalization bound using a
convolutional kernel: with probability at least 1 - δ,
L(fB ) -
mm L(f) ≤ 8 (㈤团鼬⑴J + ∖尸2 .
kfkH ≤B	nκj	n
We make the following two remarks on this bound:
25
Under review as a conference paper at ICLR 2022
1.	It depends on kgkh, which is a RKHS norm on Qq instead of Qd, which has potentially
much lower dimension and contain less smooth function for balls of same radius.
2.	There is a factor κj gain in sample complexity when learning functions that have j -th
frequency with κj > 1. In particular, for j = d (cyclic invariant functions), κj = ω, and
we need ω less samples to get the same (upper) bound on the generalization error. On the
contrary, when κj < 1, i.e., high-frequency oscillatory functions, the generalization bound
becomes worse.
B.2 Generalization error of KRR in the classical regime
We consider here the regression setting which allows for finer results. Several works have considered
bounding the generalization error of kernel ridge regression (KRR) Caponnetto & De Vito (2007);
Jacot et al. (2020), (Wainwright, 2019, Theorem 13.17). In this section, we consider the following
fully-explicit upper bound from Bach (2021).
Consider i.i.d data (xi,yi) ∈ X × R with Xi 〜P, and yi = f?(Xi) 十 号％. Assume the noise
E[εi∣Xi] = 0 and E[ε2∣Xi] ≤ σ2, and denote ε = (ει,..., εn).
Let H be a RKHS with reproducing kernel H : X × X → R. The KRR solution with regularization
parameter λ ≥ 0 is given by
arg min
f∈H
-f(Xi))2 + λkf k2H
which has the following analytical formula:
ʌ	.	,	-l
fλ(x) = h(x)(H + λIn)-1y,
where H = (H(Xi, Xj))ij∈[n] is the empirical kernel matrix, h(X) = [H(X, X1), . . . , H(X, Xn)]
and y = (y1, . . . , yn). The risk is taken to be the test error with squared error loss
R(f*,fλ)= Ex{(f*(x)- fλ(x))2} .	(36)
Below, we give an upper bound on the expected risk over the noise ε in the training data, i.e.,
Eε{R(f?, fλ)} (it is also possible to give high probability bounds by concentration arguments, but
we restrict ourselves to bounding the expected risk).
Theorem 6. (Bach, 2021, Theorem 7.2) Assume H(X, X) ≤ R2 almost surely and let the regular-
ization parameter λ ≤ R2. If n ≥ 5RR2(1 + log R^, then
Eε{R(f*, fλ)} ≤ 16σ2N(H, λ) + 16 inf {∣∣f - f*∣∣L2 + λ∣∣f ∣∣H} + 2∣∣∣f*∣∣L∞,	(37)
n	f∈H	n
where N (H, λ) = Tr[(H + λI)-1H].
Let us comment on the upper-bound in Eq. (37). The first term corresponds to an upper bound on
the variance: N(H, λ) is sometimes called the degrees of freedom or the effective dimension of the
kernel H. The second term bounds the bias term and corresponds to an approximation error. In
particular, for any r > 0,
域{kf - f?kL2 + λkfkH} ≤ λrkH-"2f*kL2,
f∈H
(38)
where we recall that H is the integral operator associated to H (see Eq. (29)). The third term can be
removed by a more intricate analysis.
From the above discussion, it is natural to consider the following two assumptions on H and f?, that
are standard in the kernel literature:
(B1)	Capacity condition: N(H, λ) ≤ CHλ-^α with α > 1.
(B2)	Source condition: there exists β > 0 SUchthat |阳-6/2/?||第2 =: Bf < ∞.
26
Under review as a conference paper at ICLR 2022
Intuitively, the capacity condition (B1) characterizes the size of the RKHS: for increasing α, the
RKHS contains less and less functions. It is verified when the eigenvalues λj ’s of H decay at the
rate j-α . For example, taking the Matern kernel of order s > d/2, whose RKHS is the Sobolev
space of order s (i.e., functions with bounded s-order derivatives), we have α = 2s/d (e.g., see
Harchaoui et al. (2007)). The source condition (B2) characterizes the regularity of the target function
(the ‘source’) with respect to the kernel: β = 1 is equivalent to f? ∈ H, while β > 1 corresponds to
f? more smooth (and β < 1 less smooth f?).
Assuming (B1) and (B2) in Theorem 6, we get the bound
Eε{R(f*, fλ)} ≤ 16Chσ2x-1/a + 16B2λβ + 22kf?kL∞
n	?	n2
(39)
αβ
32σf (CHH 厂
+ n kf*kL∞,
α
where in the second line, We balanced the two terms by taking λ* := (BH) ɑβ+1. Note that in
order to use Theorem 6, we need further to constrain n ≥ 5R2(1+log R2). For simplicity, we
will choose r > α-1, so that this condition is verified for n sufficiently large.
Remark B.1. The rate in n in Eq. (39) is minmax optimal over all functions that verify assumptions
(A1) and (A2) Caponnetto & De Vito (2007). However, for large d, the RKHS is composed of very
smooth functions (e.g., Sobolev spaces of order s are RKHS if and only if s > d/2, i.e., if the order
of the bounded derivatives grows with the dimension d) and β will be small, such that βα ≈ κ∕d for
functions with bounded derivatives up to order κ. In that case, the risk decreases at the rate n-O( d):
KRR suffers from the curse of dimensionality when κ does not scale with d. As a consequence, the
bound (39) is vacuous when n does not scale exponentially in d, which led several groups to derive
finer bounds on KRR in the high dimensional regime (see Section C).
Let us now apply Theorem 6 and Eq. (39) to our convolutional kernels to show Theorems 1 and 4.
Proof of Theorem 1. First notice that HCK(x, x) = h(1) =: R2 and we can therefore apply Theo-
rem 6. The effective dimension of HCK is bounded by
N (HCK, λ)
ξq,0	+ XX X	ξq,'r(S)/d
ξq,0 + λ	£l £ ξq,'r (S)∕d + λ
=--_L ∈ ∈ ʊ'
≤
dξq,0	+ X	ξq,'	X r(S)
ξq,0 + d ∙λ	L0 ξq,' + d ∙λ S∈E'
d X B(Qq，') ξq⅛
dN(h,d ∙λ),
where we used that r(S) ≥ 1 in the second line andN(h, λ) is the effective dimension of the inner-
product kernel h on Qq. We deduce from (A1) that N(Hck, λ) ≤ ChdIT∕αλ-1∕α. Furthermore,
from (A2) and the assumption that E{gk (x)} = 0, we have
q	∕r(S)T	∖ 2
k(HCK)-e/2f?kL2 = dβ Xξ-β X X r(S)-β	X hgk-u, Yu+S iL2
'—1	S∈C' k∈[d]	∖ u-0	)
q	r(S)-1
≤ dβ X ξ-β XXr(S)1-β X hgk-u , Yu+S iL2
'-1	S∈C' k∈[d]	u-0
d
≤ dβq1-β X ||h-e/2gk∣∣L2 ≤ dβqB2 ∙
k-1
Injecting the two above bounds in Eq. (39), we deduce that there exists constants C1 , C2 , C3 that
only depends on the constants in (A1) and (A2), and h(1), σε2 (but independent of d), such that
27
Under review as a conference paper at ICLR 2022
taking n ≥ Ci max(kf*∣∣L∞ ,d) and λ* = C (d/n) αβ+1, we get
Eε{R(f*,fλ* )} ≤。3
αβ
d∖ αβ+1
n )
□
Proof of Theorem 4. The proof is similar to the proof of Theorem 1. Notice that HωCK(x, x) ≤ h(1),
and that the effective dimension of HωCK is bounded by
dq
N(HωCK,λ)=XXX
j=1 '=1 S∈C'
dq
ξq,'r(S )κj /d
ξq,'r(S)κj/d + λ
≤	r(S)
j=1 '=1 S∈C'
ξq,'
ξq,' + dλ/Kj
XN(h,dλ∕κj) ≤ Chd7、\g X/相,
j=1
j=1
d
d
where we used condition (A1). Denoting deff
Pd=I(Kj/ω)^α, the rest of the proof follows from
the proof of Theorem 1 with d replaced by deffG1/a and B2 replaced by ωβB2.
□
Remark B.2. Note that the requirement k (H£K/s) -e/2f? k l2 ≤ B is to make the result comparable
to the other theorems when we consider target functions with low-frequencies. For a cyclic invariant
function, WegeteXaCtlyk(H£K/")-e/2f?kL2 = ||(HCK)-e/2f?kL2.
28
Under review as a conference paper at ICLR 2022
C Generalization error of KRR in high dimension
In Section B.2, we considered upper bounds on the test error of KRR using the standard capacity
and source conditions. However, these results suffer from several limitations:
1.	They only provide an upper bound on the test error. While the decay rate with respect
to n is minmax optimal (see Caponnetto & De Vito (2007)), this is not strong enough to
show, for example, a statistical advantage of using local average pooling, which appears as
a prefactor deff, and which would require a lower bound matching the upper bound within
a constant factor.
2.	As mentioned in Remark B.1, the bound is of order n-1/O(d), except when the target
function has smoothness order increasing with d. This bound is non-vacuous only if n =
exp(O(d)) which is impractical in modern image datasets where typically d ≥ 100. This
motivates a new type of question: given n	dα , what is the prediction error achieved by
KRR for a given function?
3.	In order to achieve the bound Eq. (39), one need to carefully balance the bias and the
variance terms by setting the regularization parameter. This is in contrast with modern
practice which usually train until interpolation (which corresponds to setting λ → 0).
Given the above limitations, several recent works have instead considered a high-dimensional set-
ting where the number of samples scales with d, and derived asymptotic test errors, exact up to
a vanishing additive error Ghorbani et al. (2021; 2020); Mei et al. (2021a). In addition to these
works, several papers have derived general estimates for the test error using non-rigorous methods
Jacot et al. (2020); Canatar et al. (2021); Cui et al. (2021) that are believe to be correct in the high
dimensional limit and which show great agreement with numerical experiments. The picture that
emerges in this regime is much more precise than in the classical regime: KRR approximately acts
as a shrinkage operator on the target function (not assumed to be in a particular space anymore),
with shrinkage parameter that scales as a self-induced regularization parameter over the number of
samples.
More precisely, Mei et al. (2021a) shows the following: considers a kernel Hd : Rd × Rd → R with
eigenvalues (λd,j)j≥1 in nonincreasing order and n ≡ n(d) the number of samples. Let m ≡ m(d)
be an integer such that m ≤ n1-δ and
∞
λd,m+1∙ n1+δ ≤ E λd,j,
j =m+1
for some δ > 0. Then, assuming some additional conditions insuring that the kernel Hd is ‘spread-
out’ and well behaved, the KRR solution
fλ=aιmtm{ 1X VL f(xa2+电民}
(40)
is equal up to a vanishing additive L2-error (as d → ∞) to the following effective ridge regression
estimator
fλfff = argmin [kf? - f kL2 + λef kf kHd ] ,	(41)
eff	f∈Hd	n	d
where λeff = λ + Pj∞=m+1 λd,j. The effective estimator (41) amounts to replacing the empirical
risk in Eq. (40) by its population counterpart kf? - f k2L2 = Ex{(f?(x) - f (x))2 }. In words, in
high dimension, KRR with a finite number of samples is the same as KRR with infinite number of
samples but with a larger ridge regularization.
The solution of Eq. (41) admits an explicit solution in terms of a shrinkage operator in the basis
(ψd,j)j≥1 of eigenfunctions of Hd:
∞∞
f?(X) = X Cjψd,j(X)	→	fλfff = X -~~d⅛ ∙ cj∙ %，(X).	(42)
j=1	j=1 λd,j + 丁
29
Under review as a conference paper at ICLR 2022
Hence, KRR will fit better the target function along eigendirections associated to larger eigenvalues
of H. If λd,j》λeff∕n, KRR fits perfectly f? along the eigendireCtion ψd,j, while if λd,j《λef n,
KRR does not fit this eigendirection at all. This phenomena has been referred as the spectral bias
and task-kernel alignment of kernel ridge regression in several works.
Finally, notice from Eq. (42) that the minimum test error is achieved for the regularization parameter
λ = 0, which corresponds to the KRR estimator fitting perfectly the training data. In other words,
the interpolating solution is optimal for kernel ridge regression in high dimension.
C.1 Generalization error of convolutional kernels in high dimension
Consider a sequence of integers {d(q)}q≥1 which corresponds to a sequence of image spaces x ∈
Qd of increasing dimension, and assume d(q)/2 ≥ q ≥ d(q)δ for some constant δ > 0. For ease of
notations, we will keep the dependency on q implicit, i.e., d := d(q). Let {hq}q≥1 be a sequence of
inner-product kernels hq : R → R.
Test error with one-layer convolutional kernel: we first consider a vanilla one-layer convolu-
tional kernel HCK as defined in Eq. (3). We will assume that the kernels {hq}q≥1 verify the following
‘genericity’ condition.
Assumption 1 (Generecity assumption on {hq}q≥1 at level s ∈ N). For {hq}q≥1 a sequence of
inner-product kernels hq : R → R, we assume the following conditions to hold. There exists
s0 ≥ 1∕δ + 2s + 3 where δ > 0 verifies q ≥ dδ and a constant C such that hq (1) ≤ C, and
min qs-1-kξq,kB(q,k)=Ωd⑴，	(43)
k≤s-1
min	ξq,k B(q,k)=Ωd(1),	(44)
k∈{s,s+1,s0}
max qs0-k+1ξq,q-kB(q, q - k) = Od(1).	(45)
k=0,...,s0
Assumption 1 will be verified by standard kernels, e.g., the Gaussian kernel. We discuss this assump-
tion in Section C.2 and present sufficient conditions on the activation function σ for its associated
CNTK to verify Assumption 1.
Recall that we denoted L2(Qd, Locq) the space of local functions, i.e., that can be decomposed as
f (x) = Pk∈[d] fk (x(k)). Denote hq,>' the inner-product kernel hq with its (' +1) -first Gegenbauer
coefficients set to 0, i.e.,
q
hq,>'(hu vi∕q) = X ξq,kB(Qq； k)Qkq)(hu, Vi),	(46)
k='+1
for any u, v ∈ Qq. The following result is a consequence of the general theorem on the generaliza-
tion error of KRR in Mei et al. (2021a).
Theorem 7 (Test error of CK in high dimension). Let {fd ∈ L2 (Qd, Locq)}q≥1 be a sequence
of local functions. Let (xj∈[n(d)]〜i.i.d. Unif(Qd) and yi = fd(xi) + εi with εi 〜i.i.d. N(0,σ2).
Assume d ∙ qs-1+δ ≤ n ≤ d ∙ qs-δ for some δ > 0 and let {hq}q≥ι be a SequenCe of activation
functions satisfying Assumption 1 at level s. Consider {HCK,d}q≥1 the sequence of convolutional
kernels associated to {hq }q≥ι as defined in Eq. (3). Then thefollowing holds for the solution fλ of
KRR with kernels {HCK,d}q≥1.
For any regularization parameter λ ≥ 0, define the effective regularization λeff := λ + hq,>s(1).
Then for any η > 0, we have
∖∖fλ - fλfeff∣∣L2 = Od,p(1) ∙ (kfdkL2+η + σ2).	(47)
The proof of Theorem 7 is deferred to Section C.4.
Let Us expound on the predictions of Theorem 7. First, recall that ff is given explicitly in Eq. (42)
by a shrinkage operator with parameter λeff. From Assumption 1 and taking λ = 0, the shrinkage
30
Under review as a conference paper at ICLR 2022
operator is of order 1
q
λeff= hq,>s(I)= E ξq,'B(Qq； ') = θq(I).
'=s+1
From the eigendecomposition of HCK introduced in Proposition 1, KRR fits perfectly f? along the
eigendireCtion YS with |S| = ' if n ∙ ξd,gr(S)/d》λef, while it does not fit this eigendireCtion at
all if n ∙ ξd,gr(S)/d ≤ λef. Consider n = d ∙ qs-1+α:
•	KRR fits the eigendireCtions Corresponding to the homogeneous polynomials of degree
s - 1 and less, and of degree s for subsets S suCh that γ(S)	q - q1-α.
•	KRR does not fit at all the eigendireCtions Correpsonding to homogeneous polynomials of
degree s + 1 and larger, and degree s for subsets S suCh that γ(S)	q - q1-α.
In words, for d ∙ qs-1《n《d ∙ qs, KRR fits at least a degree-(s 一 1) polynomial approximation to
f? and at most a degree-s polynomial approximation. As n increases from d ∙ qs-1 to d ∙ qs, KRR first
fits degree-s homogeneous polynomials that have smaller diameter γ(S) (i.e., ‘more loCalized’).
Test error of CK with global average pooling: we Consider the kernel HGCPK given by a Convolu-
tional layer followed by global average pooling:
HGK(χ, y) = d X h (hx(k), y(k，)i/q) ,	(48)
k,k0 ∈[d]
In addition to the generiCity Condition, we will assume that the kernels {hq}q≥1 verify the following
differentiability Condition.
Assumption 2 (Differentiability assumption on {hq}q≥1 at level s ∈ N). For {hq}q≥1 a sequence
of inner-product kernels hq : R → R, we assume the following conditions to hold. There exists
V ≥ max(2∕δ, S) where δ > 0 verifies q ≥ dδ such that hq is (v + 1)-differentiable andfor k ≤ V,
sup	h(qv,>+v1) (γ) ≤ Oq(1),
γ∈[-1,1]
h(qk,>)v(0) ≤ Oq(q-(v+1-k)/2),
where we denoted hq,>v the truncated inner-product kernel hq as in Eq. (46).
Assumption 2 is used to extend the following theorem to non-polynomial kernel hq (in partiCular, it
is trivially verified for polynomial kernels by taking V larger than the degree of hq). This assumption
is diffiCult to CheCk in praCtiCe, however we provide some examples where it holds in Appendix C.2.
ReCall that we denoted L2(Qd, CycLocq) the spaCe of funCtions that are given by the Convolution
of a funCtion g : Rq → R with the image x ∈ Qd, i.e., f(x) = Pk∈[d] g(x(k)).
Theorem 8 (Test error of CK with GP in high dimension). Let {fd ∈ L2 (Qd, CycLocq)}q≥1
be a sequence of convolutional functions. Assume qs-1+δ ≤ n ≤ qs-δ for some δ > 0 and let
{hq }q≥1 be a sequence of activation functions satisfying Assumptions 1 and 2 at level s. Consider
{HGCPK,d}q≥1 the sequence of convolutional kernels with global pooling associated to {hq}q≥1 as
defined in Eq. (48). Then the solution fλ of KRR with kernels {HCK,d}q≥ι verifies Eq. (47) with
λeff := λ + hq,>s(1).
The proof of Theorem 8 is deferred to SeCtion C.5.
The prediCtions of Theorem 8 are similar to the ones of Theorem 7 but with a faCtor d gain in statisti-
Cal effiCienCy: this is due to the eigenvalues of HGCPK being a faCtor d larger than for H CK . Therefore,
with global average pooling, for qs-1 n qs, KRR fits at least a degree-(s 一 1) invariant
polynomial approximation to f? and at most a degree-s invariant polynomial approximation. As n
inCreases from qs-1 to qs, KRR first degree-s invariant homogeneous polynomials with inCreasing
diameter γ (S ).
31
Under review as a conference paper at ICLR 2022
Test error of CK with local average pooling: In the case of local average pooling with ω < d, the
eigenvalues are harder to control. Indeed, we have mixing of the eigenvalues between polynomials
of different degree: there exists j,j0 ∈ [d] such that ξq,'Kj 《ξq,'+1κj0. The eigenvalues are
not ordered in increasing degree of their associated eigenfunctions anymore. While this case is
potentially tractable with a more careful analysis, we instead introduce a simplified kernel which we
believe qualitatively captures the statistical behavior of local average pooling.
Assume q ≤ ω∕2 and ω is a divisor of d. Denote x(kω) = (xkω+ι,..., xkω+ω) the k-th segment
of length ω in [d] and x((ik)ω) = (xkω+i , . . . , xkω+q+i ) the patch of size q with cyclic convention in
{kω + 1, . . . , kω +ω}. Consider the following convolutional kernel with ‘non-overlapping’ average
pooling:
HωK,NO(χ,y) = 1 X X hq(hχ(k)ω),y(kω)i∕q),	幽
k∈[d∕ω] i,j∈[ω]
In words, HωCK,NO is the combination of d∕ω non-overlapping convolutional kernels with global
average pooling on images of size ω:
HωK,NO =	X HGP(Mkω), y")
k∈[d∕ω]
q	(50)
=X ξq,' X X ψk, S(x)ψk,S(y) ,
'=0	k∈[d∕ω] S∈C'
where ψk,s(x) = √ω Pi∈[ω] Yi+s(x(kω)) where i + S is the translated set with cyclic convention
in [ω].
Denote L2 (Qd, LocCycLocq) the RKHS associated to HωCK,NO, which contains functions that are
locally convolutions on segments of size ω. For this simplified model, the proof of Theorem 8 can
be easily adapted and we obtain the following result:
Corollary 1 (Test error of CK with NO pooling in high dimension). Let {fd ∈
L2(Qd, LoCCyCLoCq)}q≥ι be a Sequence of local convolutional functions. Assume (d∕ω) ∙
qs-1+δ ≤ n ≤ (d∕ω) ∙ qs-δ for some δ > 0 and let {hq}q≥ι be a SeqUenCe OfaCtiVationfunC-
tions satisfying Assumptions 1 and 2 at level s. Consider {HωCK,NO,d}q≥1 the sequence of convolu-
tional kernels with non-overlapping pooling associated to {hq}q≥1 as defined in Eq. (49). Then the
solution fλ ofKRR with kernels {H£K,NO,d}q≥ι verifies Eq. (47) with λeff := λ + dhq,>s(1).
Corollary 1 shows that HωCK,NO enjoys a factor ω gain in statistical efficiency compared to HCK, due
to a factor ω smaller effective ridge regularization. Therefore, with (non-overlapping) local average
pooling, for (d∕ω) ∙ qs-1《n《(d∕ω) ∙ qs, KRR fits degree-(s 一 1) locally invariant polynomials
and none of the polynomials of degree-(s + 1) and larger. Heuristically, we see that this yields the
same statistical efficiency than HCK for ω = 1 and HGCPK for ω = d, and interpolates between the two
cases for 1 < ω < d.
Test error of convolutional kernels with downsampling: We consider adding a downsampling
operation to the previous kernels. Let ∆ be a constant and a divisor of d and ω and consider the
following ‘downsampled’ kernels:
H∆K(χ, y) = ∆ X h(hχ(k∆), y(k∆)i∕q),	(51)
k∈[d∕∆]
HGP,∆(χ, y) = δδ X	h(hχ(k∆), y(k0∆)i∕q),	(52)
k,k0 ∈ [d∕∆]
HωT(χ, y)= X HCK,∆(χ(kω), y(kω)).	(53)
k∈[d∕ω]
We can easily adapt the proofs of Theorems 7 and 8, and Corollary 1 to these kernels. In particular,
their conclusions do not change (for any constant ∆) and downsampling do not provide a statistical
advantage.
32
Under review as a conference paper at ICLR 2022
C.2 Checking the assumptions
In this section, we discuss Assumptions 1 and 2 and present sufficient conditions for them to be
verified.
Genericity assumption: Recall that the inner-product kernel hq : R → R has the following
eigendecomposition on Qq as
q
hq (hu, vi/q) = Eξq,'	E	YS (U)YS (V).
'=0	S⊆[q],∣S∣='
The genericity assumption amounts to: 1) A universality condition in Eqs. (43) and (44): if
Pkh(h1, ∙i∕q) = 0, then h does not learn degree-k homogeneous polynomials; 2) A constant or-
der scaling of the self-induced regularization hq,>s(1), from hq (1) ≤ C and Eq. (44) with s0, i.e.,
hq,>s(1) ≤ hq(1) = Oq⑴ and hq,>s(1) ≥ ξqgB(q, s0) = Ωq(1); 3) The last eigenvalues decay
sufficiently fast in Eq. (45) in order to avoid pathological cases.
For generic kernels, We have typically ξq,' N q-' (for fix '). For example, if h is smooth, ξq,' =
q-'(h(k)(0)+ Oq (1)) and it is sufficient to have h(k)(0) > 0. See Appendix D.2 in Mei et al. (2021a)
for a proof of Eq. (45) When h is sufficiently smooth.
BeloW, We present instead sufficient conditions on the activation σ such that the induced neural
tangent kernel verifies the ‘genericity’ assumption. More precisely, We display sufficient conditions
on the sequence {σq}q≥1 of activation functions σq : R → R, such that the induced neural tangent
kernels {hq}q≥1 verifies Assumption 1, Where hq Was derived in Section A.2 and is given by (u, v ∈
Qq)
hq(hu, vi/q) := h(q1)(hu, vi/q) + h(q2)(hu,vi/q) ,	(54)
Where
hqi)(hu, vi/q) = Ew〜Unif(Qq)[σq(hu, wi∕√q)σq({v, wi∕√q)] ,	(55)
hq2)(hu, vi∕np.sqrt(q)) = Ew 〜Unif(Qq)区(hu, wi∕√q)σq (hv, w^i∕√q)hu, vi]∕q.	(56)
Assumption 3 (Assumptions on {σq}q≥1 at level s ∈ N). For {σq}q≥1 a sequence of functions
σq : R → R, we assume the following conditions to hold. There exists s0 ≥ 1∕δ + 2s + 3 where
δ > 0 verifies q ≥ dδ, such that
(a)	The function σq is differentiable and there exists c0 > 0 and c1 < 1 independent of q, such
that ∣σq(x)|, ∣σq(x)| ≤ co exp(cιx2∕2).
(b)	We have
min, qs-1-kl∣Pkσq(he, ∙i∕√q)∣∣L2(Qq) =Ωq(1),	(57)
k≤s-1
Imin JPkσq(he, ∙i∕√q)∣L2(Qq)=Ωq(1),	(58)
k∈{s,s+1,s0}
where e ∈ Qq is arbitrary.
(c)	We have for a fixed δ > 0
max 0 qs0-k+1kPkσq(he, ∙i∕√q)∣L2(Qq)=Oq⑴,	(59)
k=0,...,s
rnax qs0-k+1kpkσqIe ∙"√q)kL2(Qq) = Oq⑴.	(60)
k=0,...,s0
Proposition 6. Consider a sequence {σq}q≥1 of activation functions σq : R → R that satisfies
Assumption 3. Let {hq}q≥1 be the sequence of neural tangent kernels associated to {σq}q≥1 as
defined in Eq. (54). Then the sequence {hq}q≥1 satisfies the ‘genericity’ Assumption 1.
Differentiability assumption: As mentioned in the previous section, this condition is required in
our proof technique to extend Theorem 8 to non-polynomial kernel functions. While We believe that
weaker conditions should be sufficient, we leave checking them to future work. Note that Assump-
tion 2 was proved for X 〜 Unif(Sd-1(√d)) and hq(〈x, yi∕q) = Ew{σ(hx, wi)σ(hy, wi)} for
W 〜Unif(Sd-1(1)), given that σ satisfies some differentiability conditions, in Mei et al. (2021b).
33
Under review as a conference paper at ICLR 2022
C.3 Proof of Proposition 6
Proof of Proposition 6. Step 1. Effective activation function.
Let us decompose both functions σq and σq0 in the Gegenbauer polynomial on the hypercube basis:
q
σq(hu, vi∕√q) = XXq,'B(Qq； ')Q'q)(hu, Vi) ,	(61)
'=0
q
σ'q(hu, vi∕√q) = EKq,'B(Qq;')Q'q)(hu, Vi) ,	(62)
'=0
where we recall B(Qq; `) = d` and (for e ∈ Qq arbitrary)
Xq,' (σq ) = Eu 〜Unif(Qq) [σq (hu, ei / √q)Q'q) (hu, ei)],
Kq,'(σ'q ) = Eu 〜Unif(Qq)区((叫 6 / √)Q 孚® e))].
From the definition of h(q1) in Eq. (55) and the eigendecomposition (61), we have
q
hqι)(hu, vi∕q) = £x2,'B(Qq; ')Q'q) (hu, vi).
'=0
Similarly, from the definition of h(q2) in Eq. (56), the eigendecomposition (62) and using Lemma 1
stated below, we get
qq
hq2)(hu, vi/q) = Eκ2,eB(Qq; ')Q'q)(hu, vi)hu, vi/q = EZ2,'B(Qq; ')Q'q)(hu, vi),
'=0	'=0
where
广 2	_ ' / Iq - ' /
Zq,' = qκq,'-1 +	q~κq,'+1.	(63)
We can therefore define ∏q,'
Vzx2,' + Z2,' and σeff,q(h∙, ∙"√q) : Qq × Qq → R by
q
σeff,q (hu, v"√q) = Enq,'B(Qq ； '')Q^ (hu, v)),
'=0
such that the NT kernel (54) can be written as the kernel of the effective activation σeff,q:
hq(hu, vi/q) = Eθ〜Unif(Qd)
σeff,q(hU, θi∕√q)σeff,qIhyk) θi∕√q)]
q
X ∏2,'B(Qq; ')Q'q)(hu, vi).
'=0
(64)
We will show that hq with GegenbaUer coefficients ξq,' := ∏q ` verifies Assumption 1.
Step 2. Decay of the eigenvalues.
Recall that the sequence {σq}q≥1 satisfies Assumption 3 at level s. From Assumption 3.(a) (for
example by adapting the proof of Lemma C.1 in Ghorbani et al. (2021) to the hypercube), there
exists C > 0 such that
hq(1) = kσeff,qk2L2(Qq) = h(q1)(1) + h(q2)(1) = kσqk2L2(Qq) + kσq0 k2L2(Qq) ≤C,
and we deduce that 媚力吟力不彳' =Oq(B(Qq; ')-1). Using that B(Qq;') = ('), we deduce
that for any fixed ', X2,', K2', ∏2,' = Oq(q-'). Furthermore, from Assumption 3.(c), we have for
k = 0, . . . , s0 + 1,
χq2,q-k = B(Qq; q - k)-1 kPq-kσqk2L2(Qq) = Oq(q-s -1) ,
κq2,q-k = B(Qq; q - k)-1kPq-kσq0 k2L2(Qq) = Oq(q-s0-1) ,
34
Under review as a conference paper at ICLR 2022
By Eq. (63) and the definition of n；© We have ∏2,q-k = Od(q-s'_1) for any k ≤ s0, which verifies
Eq. (45) in Assumption 1.
Furthermore, by Assumption 3.(b), using that χq2,k = B(Qq; k)-1kPkσqk2L2(Qq) and ξq2,k ≥ χq2,k,
we get
kmsn1 ξ2,k = αq (q-s+1),
and
ξ2,s = Cq (q S),	ξ2,s+1 = Cq (q SI),	ξ2,'0 = ω (q ' ).
InParticUlar,thisimpliesthat∣∣σeff,d,>skL2(Qq) ≥ |艮，σq∣∣L2(Qq)= Ωq⑴.	□
Lemma 1. Let ` be an integer such that 0 ≤ ` ≤ q. Consider the following Gegenbauer polynomial
defined on the q-dimensional hypercube (see Section D): for x, y ∈ Qq,
Q'q)(hx, yi)
1
B(Qq ；'
YS(x)YS(y),
s⊂[q],lSl='
where we recall the definition of the homogeneous polynomial YS (x) = xS = i∈S xi. We have
Q'q)(hx, yi)hx, yi/q = :Q-I (hx, yi) + q-'Q'+)ι (hx, yi),
with the convention Q(-q1) = Q(qq+)1 = 0.
ProofofLemma 1. Consider 1 ≤ ' ≤ q - 1. We have
Q'q)(hχ, yi)hχ, yi/q
1
qB (Qq ；')
E £ Ys (χ)χi ∙ Ys (y)yi
s⊂[q],lSl='i∈[q]
We haveS(x)xi =YS∪{i}(x) ifi 6∈ S, andYS(x)xi = YS\{i}(x) ifi ∈ S. Hence, the above sUm
contains sets of size `- 1 and `+ 1. For each set S ⊂ [q] with |S| = `- 1, there q + 1 - ` sets
|S| = `, sUch that by removing one element we can obtain S. For each set S ⊂ [q] with |S| = `+ 1,
there `+ 1 sets |S | = `, sUch that by adding one element we can obtain S.
We dedUce that
Q'q)(hχ, yi)hχ, yi/q
=qʃ+1 - ` X	YS (X)YS (y)+ /+1 八 X	YS (X)YS (y).
qB(Qq; `)	qB(Qq; `)
S S S S S⊂[q],∣S∣='-1	S S S S S⊂ [q], | S| ='+1
Using B(Qq;2) = ('), we obtain
Q'q)(hχ, yi)hχ, yi/q = 'Q'q)ι (hx, yi) + q---'Q'+)ι (hx, yi).
The cases ' = 0 and ' = q are straightforward.	□
C.4 Proof of Theorem 7
Let {d(q)}q≥ι be a sequence of integers with 2q ≤ d(q) ≤ q^δ for some δ > 0. We will denote
d = d(q) for simplicity. Consider X 〜Unif(Qd), dqs-1+δ ≤ n ≤ dqs-δ for some δ > 0 and a
sequence of inner-product kernels {hq}q≥1 that satisfies Assumption 1 at level s. We consider the
vanilla one-layer convolutional kernel
1d
H CK,d(χ, y) = d£hq (hx(k)，y(k)i/q).
k=1
Theorem 7 is a consequence of Theorem 4 in Mei et al. (2021a) where we take Xd = Qd, νd =
Unif(Xd) and Dd = L2(Qd, Locq) ⊂ L2(Qd). The proof amounts to checking that {H CK,d}q≥1
verifies the kernel concentration properties and eigenvalue condition (see Section 3.2 in Mei et al.
(2021a)). We borrow some of the notations introduced in Mei et al. (2021a) and we refer the reader
to their Section 2.1.
35
Under review as a conference paper at ICLR 2022
Proof of Theorem 7. Step 1. Diagonalization of the kernel and choosing m = m(q).
From Proposition 1, we have the following diagonalization of HCK,d:
1q
Hd(x, y) := HCK，d(x, y) = E Σ 酊,屋S) ∙ YS(X)YS®)，
where r(0) = d and r(S) = q + 1 - Y(S) for S ⊂ [q] \ {0}, and We recall e` = {S ⊆ [d] : |S|
',γ(S) ≤ q}. Using that B(Qq;') = Θq(q'), ξq,'B(Qq;') ≤ hq⑴ and Assumption 1, we have
min1 ξq,' = Cq (q-s+1),
'≤s-1
ξq,s+1 = Θq (q -s-1),
ξq,s = Θq(q-s),
SUP ξq,' = Oq S--2).
'≥s+2
(65)
Further define E',h
and
{S ∈ e` : Y(S) = h} for h = ',...,q. Itis easy to check that ∣E',h | = d(h-∣)
q
|E' I = ∑∣E',h∣
h='
and therefore |E'| = Θq(d ∙ q'-1).
Denote {λq,j}j≥ι the eigenvalues {ξq,'r(S)∕d}'=o,…^see` in nonincreasing order, and {ψq,j}j≥ι
the reordered eigenfunctions. Set m to be the number of eigenvalues such that λq,j > qξq,s+1 ∕d
(recall qξq,s+1 = Θd(q-s)). Denote α = qξq,s+1 ∕ξq,s. From the bounds (65) on ξq,s+1 and ξq,s,
we have a = Θq(1). Denote α = q + 1 - α and Es,≥α = {S ∈ Es : Y(S) ≥ a} and Es,<α =
Es \ Es,≥α. Using Eq. (65) and that 1 ≤ r(S) ≤ q, we have {λd,j}j∈[m] that contains exactly the
eigenvalues associated to homogeneous polynomials of degree less or equal to s - 1 and of degree s
with S ∈ Es,<α (which corresponds to the sets S such that r(S) > α, i.e., ξq,sr(S) > qξq,s+ι). In
particular, ifα < 1, then {λd,j}j∈[m] contains exactly the eigenvalues associated to all homogeneous
polynomials of degree less or equal to s.
Note that we have
s
m ≤ X |E'| = Oq(dqs-1) = Oq(q-δn).
'=0
(66)
Step 2. Diagonal elements of the truncated kernel.
Define the truncated kernel Hd,>m to be
Hd,>m(x, y) = X λq,jψq,j(x)ψd,j (y)
j≥m+1
ξ	1q
=ξds E r(S) ∙ YS(X)YS(y) + d ∑ ξq,' ∑ r(S) ∙ YS(X)YS(y).
s∈Es,≥α	'=s+1	s∈e'
The diagonal elements of the truncated kernel are given by: for any X ∈ Qd,
ξ	1q
Hd,>m(x, x)=号 E r(S) + d £ ξq,' £ r(S) = Tr(Hd,>m).	(67)
S∈Es,≥α	'=s+1	S∈E'
Notice that
X r(S) = X(q +1 - h)|E',h| = dX(q + 1 - h)(h_2) = d(') = dB(Qq;'),
S∈E'	h='	h='	' ~ 7	' /
X r(S) ≤α	Xq	|Es,h| ≤ dα2qs -- 22) = Od(dqs-2).
S∈Es,≥&	h=q+1-a
Hence using that ξq,s = Od(q-s), we have
ξq
Tr(Hd,>m) =等 E r(S )+£ ξq,'B(Qq ； ') = hq,>s(1) + 0q,p(1),
S∈Es,≥α	'=s+1
36
Under review as a conference paper at ICLR 2022
where hq,>s is the inner-product kernel with the (s + 1)-first Gegenbauer coefficients set to zero,
i.e., hq,>s(hu, v〉/q) = Pq=s+ι ξq,'B(Qq； ')Q'q)(hu, vi), for any u, V ∈ Qq. From Assumption
1 at level s, We have Ωq⑴=ξq,'oB(Qq; '0) ≤ hq,>s⑴ ≤ hq⑴=Oq(1). Hence, Tr(Hd,>m)=
Θd(1).
Similarly,
ξ2	1 q
Eχ0Hd,>m(x, x0)2] = ⅛ E T(S)2 + d E ξ2,' E T(S)2 = Tr(Hd,>m).
S∈Es,≥α	'=s+1	S∈E'
Step 3. Choosing the sequence u = u(d).
Let s0 be chosen as in Assumption 1, i.e., such that ξquB(Qq; s0) = Ωq⑴.We have
ξq,s0 =θq S-S),	SUp ξq,' =Oq S-s T).
'≥s0 + 1
(68)
(69)
Set u = u(d) to be the number of eigenvalues such that λq,j > qξq,s0/d = Θq(q-s'+1∕d). From
Eqs. (65) and (69), and recalling that 1 ≤ r(S) ≤ q, We deduce that {λd,j}j∈[u] must contain all
the eigenvalues associated to homogeneous polynomials of degree less or equal to ` and does not
contain any of the eigenvalues associated to homogeneous polynomials of degree larger or equal to
s0.
We have
Tr(Hd,>U) =	λq,j ≤ Tr(Hd,>m) = Oq(1),
j>U
Tr(Hd,>u) ≥ ξds0 X T(S) = ξq,s0 B (Qq ； s0) = Ωq (1).
S∈Es0
Similarly, We have
Tr(Hd,>U) = X λ2,j ≤ Tr(Hd,>U) ∙ SUp λd,j = qd-1ξq,s0 Tr(Hd,>m)=Oq(d-1q- +1
j>u	j>m
ξ2 0	0
Tr(Hd,>u) ≥ ¾l E T(SY ≥ d-1ξ2,s0B(Qq；s0) = Ωq(d-1q-s ).
S∈Es0
),
Finally,
Tr(Hd,>u) = X λd,j ≤ d 3q3ξ3,s0 Tr(Hd,>m) = Oq (d 3q 3' +3)∙
Step 4. Checking the kernel concentration property at level {(n(q), m(q))}q≥1.
Let us check the kernel concentration property at level (n, m) with the sequence of integers
{u(q)}q≥1 defined in the previous step (Assumption 4 in Mei et al. (2021a)):
(a)	(Hypercontractivity of finite eigenspaces) The subspace spanned by the top eigenvectors
{ψq,j }j ∈[U] is contained in the subspace of polynomials of degree less or equal to s0 - 1
on the hypercube. The hypercontractivity of this subspace is a consequence of a classical
result due to Beckner, Bonami and Gross (see Lemma 4 in Section D).
(b)	(Properly decaying eigenvalues.) From step 3 and recalling that s0 ≥ 1∕δ + 2s + 3 where
δ > 0 verifies q ≥ dδ, We have
T((H2:：=Ωq⑴∙ dL = Ωq⑴∙ d2q2s+1 ≥ n2+δ0,
for δ0 > 0 sufficiently small. Similarly,
Tr(M)2 = Ωq⑴∙ dqs0-3 = Ωq⑴∙ d2q2s ≥ n2+δ0,
for δ0 > 0 chosen sufficiently small.
37
Under review as a conference paper at ICLR 2022
(c)	(Concentration of the diagonal elements of the kernel) From Eqs. (67) and (68), the diago-
nal elements of the kernel are constant and the assumption is automatically verified.
Step 5. Checking the eigenvalue condition at level {(n(q), m(q))}q≥1.
Let us now check the eigenvalue condition at level {(n(q), m(q))}q≥1 which corresponds to As-
sumption 5 in Mei et al. (2021a)):
for δ > 0 sufficiently small. Similarly,
(70)
T(Hd,>m) = Ωq⑴∙ -d- = Ωd(1) ∙ dqS ≥ n1+δ.
λd,m+1	qξd,s+1
(b) This is a direct consequence of Eq. (66).
We can therefore apply Theorem 4 in Mei et al. (2021a), which concludes the proof.	□
C.5 Proof of Theorem 8
Consider qs-1+δ ≤ n ≤ qs-δ for some δ > 0 and a sequence of inner-product kernels {hq}q≥1 that
satisfies Assumptions 1 and 2 at level s. We consider the one-layer convolutional kernel with global
average pooling
HCK,d(x, y) = d
d
E hq(hχ(k),y(ko)"q)∙
k,k0=1
Again, the proof of Theorem 8 will amount to checking that the conditions of Theorem 4 in Mei
et al. (2021a) hold.
For the sake of simplicity, we will further assume that ξq,s > qξq,s+1, which simplifies some of
the computation. This condition can be removed as in Theorem 7, by considering the set Cs,<α =
{S ∈ Cs : Y(S) <α} and showing that the extra terms corresponding to these eigenfunctions are
negligible.
Proof of Theorem 8. Step 1. Diagonalization of the kernel and choosing m = m(q).
From Proposition 2 with ω = d, we have the following diagonalization of Hdd,q :
q
Hd(χ, y) ：= HGP,d(χ, y) = XX ξq”(s) ∙ Ψs(χ)Ψs(y),
where We recall ψs(x)= 嗨 Pk∈[d] Yk+s(x) and that C is the quotient space of e` with the
translation equivalence relation. It is easy to check that |C'| =('-：).
From Assumption 1, We get the same bounds on the Gegenbauer coefficients ξq,' as Eq. (65) in the
proof of Theorem 7. Denote {λq,j}j≥ι the eigenvalues {ξq,'r(S)}'=o,…^see` in nonincreasing
order, and {ψq,j}j≥1 the reordered eigenfunctions. Set m to be the number of eigenvalues such that
38
Under review as a conference paper at ICLR 2022
λq,j > qξq,s+1 (recall qξq,s+1 = Θd(q-s)). From the bounds (65) and our simplifying assump-
tion that ξq,s > qξq,s+1, we have {λd,j}j∈[m] that contains exactly the eigenvalues associated to
homogeneous polynomials of degree less or equal to s.
Note that we have
s
m = X ∣C'I = Oq(qs-1) = Oq(q-δn).	(71)
'=0
Step 2. Diagonal elements of the truncated kernel.
Define the truncated kernel Hd,>m to be
q
Hd,>m (x, y) = E λd,jψd,j(χ)Ψd,j(y) = E £ ξq”(S) ∙ ψs(χ)Ψs(y).
j≥m+1	'=s+1 S∈C'
The diagonal elements of the truncated kernel are given by: for any x ∈ Qd,
q
Hd,>m(x, X)= X ξq,'B(Qq； ')Υ'q)(x),
'=s+1
where
Y'q)(x) = B⅛与 X r(S)ψs(x)2.
'	;S S∈C'
Notice that we have now
X r(S) = X(q +1 - h) (h - 2) = (') = B(Qq； ').
S∈C'	h='	∖	/	∖ /
Therefore Eχ [Υ'q)(x)] = 1 and
q
Tr(Hd,>m) = Eχ[Hd,>m(x, x)]= E ξq,'B(Qq ；') = hq,>s(1).
'=s+1
From Proposition 7 with ` = s, we have
sup Hd,>m(Xi, Xi)- Ex[Hd,>m(x, x)]l = Tr(Hd,>m) ∙ 0d,p(1),
i∈[n]
sup ∣Eχ0 [Hd,>m(Xi, x0)2] - Eχ,χ0 [Hd,>m(x, X0)2] l = Tr(Hd >m) ∙ 0d,p(1).
i∈[n]
(72)
Step 3. Choosing the sequence u = u(d).
Let s0 be chosen as in Assumption 1. Similarly to step 3 in the proof of Theorem 7, take u = u(d)
to be the number of eigenvalues such that λq,j > qξq,e. We get
Tr(Hd,>u) = Θq(1),
Tr(H2d,>u) = Oq(q-s0+1),
Tr(Hd,>u)=Ωq (q-'0),
Tr(H4d,>u) = Oq(q-3s0+3).
Step 4. Checking the kernel concentration property at level {(n(q), m(q))}q≥1.
The kernel concentration property at level (n, m) hold with the sequence {u(q)}q≥1 as defined
in step 3. The hypercontractivity of finite eigenspaces and the properly decaying eigenvalues are
obtained as in step 4 of the proof of Theorem 7, while the concentration of the diagonal elements of
the kernel is given by Eq. (72).
Step 5. Checking the eigenvalue condition at level {(n(q), m(q))}q≥1.
This is obtained similarly as in step 5 of the proof of Theorem 7.
□
39
Under review as a conference paper at ICLR 2022
C.6 Auxiliary results
Proposition 7. Let s ≥ 1 be a fixed integer. Assume that the sequence of inner-product kernels
{hq}q≥1 satisfies Assumptions 1 and 2 at level s. Define Hd>s : Qd × Qd → R as the convolutional
kernel with global average pooling
H>s(x, y) = d X hq,>s(hx(k), y(ko)i/q),
k,k0 ∈[d]
where hq,>s is the inner-product kernel where the s + 1 first Gegenbauer coefficients are set to 0.
Thenfor n =Oq(qp) forsome fixedP, letting (xi)i∈[n]〜 Unif(Qd), we have
sup ∣H>s(xi, Xi)- Eχ[H>s(x, x)]∣ = Eχ[H>s(x, x)] ∙ 0d,p(1),	(73)
i∈[n]
SUp ∣Eχ0 [H>s(xi, x0)2] - Eχ,χ0 [H>s(x, X0)2] ∣ = Eχ,χ0 H>s(x, x0)2] ∙ Od,p(1).	(74)
i∈[n]
Proof of Proposition 7. Step 1. Bounding supi∈[n] ∣∣Hd>s(Xi, Xi) - Ex [Hd>s(X, X)]∣∣.
Recall that we defined
γ'q)(X) = B⅛与 X r(S)ψs(X)2.
;S S∈C'
Following the same proof as Proposition 8 in Mei et al. (2021b), notice that for the integer v in
Assumption 2, by Lemma 2 stated below, we have
sup ∣∣∣Hd>s(Xi, Xi) - Ex [Hd>s(X, X)]∣∣∣
i∈[n]
v
≤ sup	∣H>v(Xi, Xi)- Eχ[H>v(x, x)]∣	+ X ξq,'B(Qq;') ∙ max	∣Υ'd)(Xi)	- Eχ[Υ'd)(X)]∣
i∈[n]	1	1	'=S+l	i∈[n]	1	1
v
sup
i∈[n]
(Xi, Xi)- Eχ[H>v(x, x)]∣ + I E ξq,'B(Qq;') I ∙ Od,p(1).
V=s+1	)
By Assumption 2, there exists C > 0 such that for any γ ∈ [-1, 1],
v
∣hq,>v(γ) - X r!hq>V(0)γr∣ ≤ C ∙∣Y∣v+1,	(75)
r=0
and |h(qr,>) v(0)| ≤ Cq-(v+1-r)/2 for r ≤ v. Moreover, by Hanson-Wright inequality as in Lemma
3, using n = Oq(qp) (at most polynomial in q) and a union bound, we have for any η > 0,
sup sup sup Ih(Xi)(k), (Xi)(l)ir∣ ∙ q-k∕2-η = Oq,p(1),
1≤r≤v+1 k6=l i∈[n]
sup supE [∣(X(k), X(I)ir∣i ∙ q-k/2-n = Oq,p(1).
1≤r≤v+1 k6=l
Therefore, injecting these bounds in Eq. (75), we get
sup sup ∣hq,>v(h(Xi)(k), (Xi)(l)i∕q)∣ = Oq,p(q-(v+1"2+η),
k6=l i∈[n]
supE h∣hq,>v(hX(k), X(i)i∕q)∣i = Oq,p(q-(v+1"2+η).
40
Under review as a conference paper at ICLR 2022
Hence, we deduce that
sup ∣H>v(Xi, Xi)- Ex[H>v(x, x)] ∣
i∈[n]
≤ d X SUPIhq,>v(h(Xi)(k), (Xi)(l)i∕q) - Ex[hq,>v(<X(k), X(l)i∕q)] ∣
Ct k=l∈[d] i∈[n]
≤ d SUP 卜UpJhq,>v(h(Xi)(k), (Xi)(l)i∕q) + E [∣hq,>v(<X(k), X(l)i∕q) ∣i }
=Oq,p(dq-(v+1)/2+n ) = 0d,p(1).
Furthermore, recall that by Assumption 1, we have E[H>e(x, x)] ≥ ξqgB(Qq; s0) = Ωq(1). We
get
sup ∣ H>v(Xi, Xi)- Ex[H>v(x, x)]∣ = E[H>e(x, x)] ∙ 0q,p(1),
i∈[n]
which concludes the proof of the first bound.
Step 2. Bounding supi∈[n] ∣ Ex，[H>s(xi, x0)2] - Ex,x, [H>s(x, x0)2] ∣ .
Notice that we can write,
q
Eχ, [H>s(x, x0)2]=	E ξ2,'Re ∙ Ξ'd)(x),
'=s+1
where we denoted r` = Ps∈c r(S)2 and
ξF)(X) =春 X r(S)2ψs(x)2.
Then, by Lemma 2, we get for any U ≥ s,
sup ∣Eχo [H>s(xi, x0)2] - Exe [H>s(x, x0)2] ∣
i∈ [n] '	'
u
≤ sup ∣ Exo[H>u(xi, x0)2] - ExH[H>u(x, x0)2]∣ + X ξ2eRe ∙ max ∣ 三'")3) - Ex[Ξ'd)(x)]
i∈[n] 1	1 e=+1	i∈[n] 1
u
=SUP ∣ ExO[H>u(xi, x')2] - Ex,x，[H>u(x, x')2]| +	E ξ2,eRe ) ∙ od,p(1).
i∈[n]	∖e=s+ι	)
We conclude following the same argument as in the proof of Proposition 9 in Mei et al. (2021b). □
Lemma 2. Let' ≥ 2 be an integer. Define Yed) : Qd → R and Ξ'd) : Qd → R to be
Yed)(X)= B(W) X TM(x)2,
卜 ; S S∈C'
Eed)(X)=jτ xr(S)2ψs(X)2,
e S∈C'
Where R' = Ps∈Cg MS)2∙
Let n ≤ qp forsome fixedP. Then,for (x)∈[nj削'Unif(Qd), we have
max ∣γ'd)(xi) - Ex[Υ'd)(x)]∣ = 0d,p(1),
i∈[n] I	I
max Ed)(Xi) - Ex[Ξ'd)(x)]∣=Od,p(i),
where Eθ[Υ'd)(θ)] = Ex[Ξ'd)(x)] = 1.
(76)
(77)
(78)
(79)
41
Under review as a conference paper at ICLR 2022
Proof of Lemma 2. Step 1. Boundingmaxi∈[n] ∣Υ'd)(Xi)- Eχ[Υ'd)(x)]∣.
Define f` : Qd → R to be
F'(X) =γ'd)(x) - Eχ[γ'd)(x)] = dB(∖q. `) X r(S) X Yi+S(x)Yj+s(x).	(80)
1	; S S∈C' i=j∈[d]
Notice that F'(x) is a degree 2' polynomial and therefore satisfies the hypercontractivity property.
For any m ≥ 1, there exists C > 0 such that
Ex[F'(x)2m]1/(2m) ≤ C ∙ Eχ[F'(x)2]1/2.
Let us bound the right hand side. We have
(81)
E[F'(x)2] = d2B(Qq ； ')2
r(S)r(S0)	ω(B1,B2,B3,B4),
S,S0∈C'
i,j,i0,j0∈[d]
where B1 = i + S, B2 = j + S, B3 = i0 + S0 and B4 = j 0 + S0 , and we denoted
ω(B1,B2,B3,B4) = ExhYBI (X)YB2 (x)YB3 (x)YB4 (x)i lB1=B2 lB3=B4.
Notice that ω(B1, B2, B3, B4) = 1 if B1∆B2 = B3∆B4 (the symmetric difference) and 0 oth-
erwise. In other words, every elements in B1 ∪ B2 ∪ B3 ∪ B4 appears exactly in 2 or 4 of these
sets.
Let us fix i ∈ [d] and S ∈ C', and bound
r(S0)	ω(B1, B2, B3, B4).
S0∈Cq,'	j,i0=j0∈[d]
(82)
Denote ∣B1∆B2∣ = 2k with 1 ≤ k ≤ '. In order for ω(B1,B2,B3,B4) = 1, B3 must contain
exactly k points in B1 ∆B2 while B4 must contain the remaining k points.
•	Case k < '. There are at most '2 ways of choosing j such that Bi ∩ B2 = 0. Fixing j
(i.e., Bi and B2) and S0, then there are 2k' ways of choosing i0 and 2k' ways of choosing
j0 such that B3 ∩ (B1∆B2) 6= 0 and B4 ∩ (B1∆B2) 6= 0. Hence the contribution of these
terms in Eq. (82) is upper bounded by
`-i
X r(S0) X '2 ∙(2k')2 ≤ 4'7 X r(S0)=4'7B(Qq;').	(83)
S0 ∈C'	k=1	S0∈C'
•	Case k = `. There are at most d ways of choosing j . Furthermore, for j fixed, there
are at most (2e) ways of choosing B3 and B4 such that B3 ∪ B4 = Bi ∪ B2 (note that
Bi ∩B2 = 0 and therefore B3 ∩B4 = 0). Hence the contribution of these terms in Eq. (82)
is upper bounded by
X	Ir(S0) ∙ d ∙ IB3∪B4=B1∪B2 ≤ dq ( `
S0∈C',i0,j0 ∈[d]	' '
where we used that r(S0) ≤ q.
(84)
Combining Eqs. (83) and (84) and using there are dB(Qq; `) choices for i and Si, we get
E[F'(χ)2]≤ d2B⅛F	X	MS)h4'7B(Qq；') + dq(2')i
' i i∈[d],S∈C'	∖ '
=Oq(1) ∙ [d-i + qB(Qq； ')-i] = Oq(q-i),
where We used that' ≥ 2 and B(Qq;') = Ωq(q').
42
Under review as a conference paper at ICLR 2022
Using Eq. (81), we deduce
Ehmax ∣F'(xi)∣i ≤ Ehmax4的严]1/" ≤ n1/(2m)E[43严]1")
≤ Cn1/(2m)E[F'(x)2]1∕2 = n1/m ∙ Oq(q-1/2).
Using Markov’s inequality and taking m sufficiently small yield Eq. (78).
Step 2. Bounding maxi∈[n] ∣Ξ'd)(xi) - Eχ[Ξ'd)(χ)]∣.
The second bound (79) is obtained very similarly. Define g` : Qd → R to be
G'(x) = Ξ'd)(x) - Eχ[Ξ'd)(x)] = -1r X r(S)2 X 匕+s(x)Yj+s(x).
dR' z-":	.
S∈C'	i=j∈[d]
(85)
Then, we have
E[G'(X)2] = dr2 ^X r(S)2r(SO)2	^X	ω(B1, B2, B3, B4).
' S,S0∈c'	i,i0,jj0∈[d]
Further notice that following the same computation as in Eq. (70), we get
r` = X r(S)2 = X(q + 1 - h)2 (h : 2) =Ωq⑴∙ q1+'.
S∈C'	h='	∖	)
Hence, the same computation as for f` in step 1 yields
E[G'(x)2] ≤ 3 X	r(S)2[4'7R' + dq2(2')i
' i∈[d],S∈C'	∖)
=Oq(1) ∙ [d-1 + q2R-1] = Oq(q-1),
where we used that ` ≥ 2. We deduce Eq. (79) similarly to step 1.
□
Lemma 3 (Hanson-Wright inequality). There exists a universal constant c > 0, such that for any
t > 0 and q1∕δ ≥ d ≥ q ∈ N for some δ > 0, when X ∈ Unif(Qd), we have
P I SUp lhx(k), X(i)il∕q>t) ≤ 2q2/ exp{-cq ∙ min(t2,t)},
k6=l∈[d]
where we recall that X(k) = (xk, . . . , xk+q-1).
Proof of Lemma 3. For any k 6= l, denote A = (aij )i,j∈[d] the matrix with a(k+i),(l+i) = 1 for
i = 0, . . . , q - 1 and aij = 0 otherwise, such that hX, AXi = hX(k), X(l)i. Note that we have
kAkF = √q, kAkop ≤ 1 and E[〈x, Ax〉] = 0. By Hanson-Wright inequality of vectors with
independent sub-Gaussian entries (for example, see Theorem 1.1 in Rudelson & Vershynin (2013)),
we have
P (∣hx, Ax〉|/q > t) ≤ 2exp{-cq ∙ min(t2, t)}.
Taking the union bound over k = l concludes the proof.	□
43
Under review as a conference paper at ICLR 2022
D	Technical background of function s paces on the hypercube
Fourier analysis on the hypercube is a well studied subject O’Donnell (2014). The purpose of this
section is to introduce some notations and objects that are useful in the statement and proofs in the
main text.
D.1 Fourier basis
Denote Qd = {-1, +1}d the hypercube in d dimension, and τd to the uniform probability measure
on Qd. All the functions will be assumed to be elements of L2 (Qd, τd) (which contains all the
bounded functions f : Qd → R), with scalar product and norm denoted as h∙, •〉l2 and ∣∣∙ ∣∣l2 :
hf, giL2 ≡
/
Qd
f(x)g(x)τd(dx)
2n X f(X)g(X).
x∈Qd
Notice that L2(Qd, τd) is a 2n dimensional linear space. By analogy with the spherical case we
decompose L2 (Qd, τd) as a direct sum of d + 1 linear spaces obtained from polynomials of degree
` = 0, . . . , d
d
L2(Qd,τd) = M Vd,'.
'=0
For each ' ∈ {0,...,d}, consider the Fourier basis {Y(S)}s⊆[d],∣s∣=' of degree ', where for a set
S ⊆ [d], the basis is given by
Y(S)(X) ≡ xs ≡ Y Xi.
i∈S
It is easy to verify that (notice that xi = xi if k is odd and xik = 1 if k is even)
hY(S),γ(,SoiL2 = E[xS × xS ] = δ',kδS,S0.
Hence {Y(S)}S⊆[d],∣S∣=' form an orthonormal basis of Vd,' and
dim(Vd,') = B(Qd; ')= (d).
We will omit the superscript (d) in Y'(,dS) when clear from the context and write YS := Y'(,dS).
We denote by P' the orthogonal projections to Vd,' in L2(Qd). This can be written in terms of the
Fourier basis as
P'f(X) ≡	X	hf,YSiL2YS(X).	(86)
S⊆[d],∣S∣='
We also define P≤' ≡ Pk=0 Pk, P>' ≡ I - P≤' = Pk∞='+1 Pk, and P<' ≡ P≤'-1, P≥' ≡ P>'-1.
D.2 Hypercubic Gegenbauer
We consider the following family of polynomials {Q('d)}'=0,...,d that we will call hypercubic Gegen-
bauer, or Gegenbauer on the d-dimensional hypercube, defined as
Q('d)(hX, yi)
1
B(Qd;')
X	Y'(,dS) (X)Y'(,dS) (y).
S⊆ [d] ,| S| ='
(87)
Notice that the right hand side only depends on hX, yi and therefore these polynomials are well
defined. In particular,
hQ'd)(h1, ∙i),Qkd)(h1, ∙i)iL2 = B(⅛7kyδ'k.
44
Under review as a conference paper at ICLR 2022
Hence {Q'd)}'=o,…,d form an orthogonal basis of L2({-d, -d +2,...,d - 2,d}, T(I) where T(I is
the distribution of(1, Xi when X 〜t(, i.e., Td 〜2Bin(d, 1/2) - d/2.
It is easy to check more generally that
hQ'%(x,∙i), Qkd)((y,∙i)iL2= B*, k、Qk((χ, yi)δ'k.
B(Q ; k)
Furthermore, Eq. (87) imply that —up to a constant— Q(kd)(hX, yi) is a representation of the pro-
jector onto the subspace of degree-k polynomials
(Pkf)(X) = B(Qd; k)	Q(kd)(hX, yi) f (y) Td(dy).	(88)
Qd
For a function σ(∙/√d) ∈ L2({-d, -d + 2,...,d - 2, d}, τ(), denote its hypercubic Gegenbauer
coefficients ξd,k(σ) to be
ξd,k (σ) = I	σ (X/Vd) Qkd)(X)T(I (dx)
{-d,-d+2,...,d-2,d}
(89)
To any inner-product kernel Hd(xι, X2) = hd((xι, x2i∕d), with hd( ∙ ∕√d) ∈ L2({-d, -d +
2,...,d - 2, d}, Td) We can associate a self adjoint operator Hd : L2(Qd) → L2(Qd) via
Hdf(X) ≡	hd(hX, X1i/d) f (X1 ) Td(dX1 ) .
Qd
(90)
By permutation invariance, the space Vk of homogeneous polynomials of degree k is an eigenspace
of Hd, and we will denote the corresponding eigenvalue by ξd,k(hd). In other words Hdf(X) ≡
Pqk=0 ξd,k(hd)Pkf. The eigenvalues can be computed via
ξd,k (Ad)= I	hd(x/d) Qkd)(X)T(KdX)
{-d,-d+2,...,d-2,d}
(91)
D.3 Hermite polynomials
The Hermite polynomials {Hek}k≥0 form an orthogonal basis of L2 (R, γ), where γ(dX) =
e-x /2dx/√2∏ is the standard Gaussian measure, and Hek has degree k. We will follow the classi-
cal normalization (here and below, expectation is with respect to G 〜N (0,1)):
E{Hej (G)Hek (G)} = k! δj k .	(92)
As a consequence, for any function g ∈ L2 (R, γ), we have the decomposition
g(x) = X μkg) Hek(X),
k=0
μk (g) ≡ E{g(G) Hek (G)}.
(93)
The Hermite polynomials can be obtained as high-dimensional limits of the Gegenbauer polyno-
mials introduced in the previous section. Indeed, the Gegenbauer polynomials (UP to a √d scaling
in domain) are constructed by Gram-Schmidt orthogonalization of the monomials {Xk}k≥0 with
respect to the measure T(I, while Hermite polynomial are obtained by Gram-Schmidt orthogonaliza-
tion with respect to γ. Since T(I ⇒ Y (here ⇒ denotes weak convergence), it is immediate to show
that, for any fixed integer k,
lim Coeff{Qkd)(√dX) B(Qd; k)1/2} = Coeff ʃ ɪ, Hek(X)) ∙
d→∞	(k!)1/2
(94)
Here and below, for P a polynomial, Coeff{P (X)} is the vector of the coefficients of P . As a
consequence, for any fixed integer k, we have
μk(σ) = lim ξd,k(σ)(B(Qd; k)k!)1/2,
d→∞
(95)
where μk(σ) and ξd,k(σ) are given in Eq. (93) and (89).
45
Under review as a conference paper at ICLR 2022
D.4 Hypercontractivity of uniform distributions on the hypercube
By Holder’s inequality, we have kf kLp ≤ kf kLq for any f and any p ≤ q. The reverse inequality
does not hold in general, even up to a constant. However, for some measures, the reverse inequality
will hold for some sufficiently nice functions. These measures satisfy the celebrated hypercontrac-
tivity properties Gross (1975); Bonami (1970); Beckner (1975; 1992).
Lemma 4 (Hypercube hypercontractivity Beckner (1975)). For any ` = {0, . . . , d} and fd ∈
L2 (Qd) to be a degree ` polynomial, then for any integer q ≥ 2, we have
IlfdIlLq (Qd) ≤ (q -I)'• IIfdkL2(Qd).
46