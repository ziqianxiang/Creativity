Under review as a conference paper at ICLR 2022
Exploiting Minimum-Variance Policy Evalua-
tion for Policy Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Off-policy methods are the basis of a large number of effective Policy Optimiza-
tion (PO) algorithms. In this setting, Importance Sampling (IS) is typically em-
ployed as a what-if analysis tool, with the goal of estimating the performance of
a target policy, given samples collected with a different behavioral policy. How-
ever, in Monte Carlo simulation, IS represents a variance minimization approach.
In this field, a suitable behavioral distribution is employed for sampling, allowing
diminishing the variance of the estimator below the one achievable when sam-
pling from the target distribution. In this paper, we analyze IS in these two guises,
showing the connections between the two objectives. We illustrate that variance
minimization can be used as a performance improvement tool, with the advantage,
compared with direct off-policy learning, of implicitly enforcing a trust region.
We make use of these theoretical findings to build a PO algorithm, Policy Op-
timization via Optimal Policy Evaluation (PO2PE), that employs variance mini-
mization as an inner loop. Finally, we present empirical evaluations on continuous
RL benchmarks, with a particular focus on the robustness to small batch sizes.
1	Introduction
Policy Optimization methods (PO, Deisenroth et al., 2013) have been widely exploited in Reinforce-
ment Learning (RL, Sutton & Barto, 2018) with successful results in addressing, to name a few,
continuous-control (e.g., Peters & Schaal, 2008; Lillicrap et al., 2016), robot manipulation (e.g., Gu
et al., 2017; Chatzilygeroudis et al., 2020), and locomotion (e.g., Kohl & Stone, 2004; Duan et al.,
2016). Most of these algorithms employ the notion of trust region (Conn et al., 2000), introduced
ante litteram in the RL literature by the safe RL approaches (Kakade & Langford, 2002; Pirotta
et al., 2013), giving rise to a surge of effective algorithms, having TRPO (Schulman et al., 2015) as
the progenitor. The core of any RL algorithm, being value-based or policy-based, lies in the ability
to employ the samples collected with the current (or behavioral) policy to evaluate the performance
of a candidate (or target) policy (Sutton & Barto, 2018). The skeleton rationale behind the usage of
a trust region is to control the set of candidate policies whose performance can be accurately eval-
uated. Intuition suggests that if the candidate policy is “sufficiently close” to the current one, this
off-policy evaluation problem (Precup et al., 2000) will provide a good estimate for the performance
of the candidate policy. Formally, this idea has been studied in the field of Importance Sampling (IS,
Owen, 2013) and the phenomenon is particularly evident looking at the IS estimator variance, which
grows exponentially with the Renyi divergence (Renyi, 1961) between the behavioral and the target
policy (Metelli et al., 2018; 2020). In this off-policy learning (Off-PL) setting, IS is employed as a
what-if analysis tool (Owen, 2013) and its role is passive, as samples have been already collected
with the current behavioral policy. In this sense, the trust region is an a-posteriori remedy for the
limitations of off-policy evaluation, for controlling the uncertainty injected by the IS procedure.
However, IS originated in the Monte Carlo simulation community (Hesterberg, 1988; Hammers-
ley, 2013) as an active tool for variance minimization (Off-VM). While in Off-PL, the behavioral
policy is fixed and we look for the best target policy, whose performance we aim to estimate, here
the roles are reversed. Indeed, in Off-VM, the target policy is fixed and we search for the behav-
ioral policy (from which to collect samples) that yields an IS estimate with the minimum possible
variance (Hammersley, 2013; Kahn & Marshall, 1953). It might seem surprising, at first, that sam-
pling from a policy, other than the target one, can lead to an estimator with less variance (even zero
in some cases) w.r.t. the on-policy estimate. In this role, IS has been previously employed in RL,
1
Under review as a conference paper at ICLR 2022
mainly to address rare events (Frank et al., 2008; Ciosek & Whiteson, 2017) which naturally lead to
high-variance estimates, when tackled on-policy. The idea of explicitly using IS as a variance reduc-
tion technique, with the goal of finding an optimal behavioral policy, was proposed by (Hanna et al.,
2017) for evaluation and subsequently combined with policy gradient learning (Hanna & Stone,
2018; Hanna et al., 2019). However, in these works, the variance minimization (Off-VM) process
and the off-policy learning (Off-PL) problem are treated separately.
The goal of this paper is to investigate the relation between variance minimization (Off-VM) and
off-policy learning (Off-PL). The core question we address can be summarized as: “Can Off-VM be
employed as a tool for Off-PL, overcoming the need for an explicit trust region?” Intuitively, given a
target policy, when the reward function is positive, one way to reduce the variance of the IS estimator
is to assign larger probability to the trajectories that have a large impact on the mean, i.e., those
with high returns. This provides a first hint about the connection between the minimum-variance
sampling policy and the performance improvement, i.e., between Off-VM and Off-PL. Furthermore,
it suggests that we could repeatedly apply the process of identifying the minimum-variance policy
as a tool for policy improvement. The interesting aspect of such an approach is that, by minimizing
the variance, it implicitly controls the divergence between two consecutive policies. In other words,
it allows enforcing a trust region, without the need for divergence constraints or penalizations.
Outline of the Contributions In this paper, we provide theoretical, algorithmic, and experimen-
tal contributions. After having introduced the background (Section 2), we present the problem of
finding the minimum-variance behavioral distribution (Section 3). Then, we study the properties of
the Off-VM problem in two settings: unconstrained (Section 4) and constrained (Section 5). First,
we assume that there are no restrictions for choosing the behavioral distribution. We show that the
minimum-variance behavioral distribution, besides leading to the zero-variance estimator (Kahn &
Marshall, 1953), is guaranteed to yield a performance improvement, requiring the non-negativity
of the reward only. Furthermore, we prove that this approach allows controlling the divergence be-
tween two consecutive distributions, thus enforcing an implicit trust region. Although this provides
a valuable starting point, the minimum-variance distribution might be unrealizable given the envi-
ronment transition model, i.e., there might be no policy inducing it. For this reason, we move to
the scenario in which the distributions are constrained in a suitable space. In this setting, the zero-
variance estimator could not be achievable. Nevertheless, we prove that such a procedure can lead to
a performance improvement and preserves the trust region enforcement. Based on these theoretical
results, we propose Policy Optimization via Optimal Policy Evaluation (PO2PE), a novel PO algo-
rithm, that we particularize for parametric policy spaces (Section 6). Finally, we provide numerical
simulations on continuous-control benchmarks, in comparison with POIS (Metelli et al., 2018) and
TRPO (Schulman et al., 2015), with a particular focus on the robustness of PO2PE to small batch
sizes (Section 7). The proof of the results presented in the main paper are reported in Appendix A.
2	Preliminaries
In this section, we provide the necessary background that will be employed in the paper.
Mathematical Notation Let X be a set, and let FX be a σ-algebra over X. We denote with PpXq
the space of probability measures over pX,FXq. Let P P PpXq, whenever needed, we assume that
P admits a density function p. For a subset Y J R, we denote with B(X, Y) the space of measurable
functions f: X → Y. Let P,Q P P(X) be two probability measures such that P ! Q, i.e., P is abso-
lutely continuous w.r.t. Q, for every aP [0,8]，we define the α-Renyi divergence as (Renyi, 1961):
Da(P}Q) “ a´l log IXp(x)ɑq(x)1-αdx. In the limit of α → 1, the Renyi divergence reduces to the
KL-divergence DKL(P}Q), while for α→8, it corresponds to ess supx„Q tp(x){q(x)u.
Importance Sampling Let P, QP P(X) with P! Q and let fPB(X,R). Importance Sam-
pling (IS, Owen, 2013) allows estimating the expectation of f under a target distribution P,
i.e., Eχ~p[f (x)S having samples {χi}ip[n] collected with a behavioral distribution Q: pp∕q “
1 ∑iPrns Ppxiqf (xi). The IS estimator is unbiased (Owen, 2013), i.e., Ex-q [μp{q]= Eχ~p[f (x)S,
but it might suffer from large variance, due to the heavy-tailed behavior (Metelli et al., 2018). The
properties of pp∕q and its transformations have been extensively studied in the literature (e.g., Ion-
ides, 2008; Thomas et al., 2015; Papini et al., 2019; Kuzborskij et al., 2021; Metelli et al., 2020).
2
Under review as a conference paper at ICLR 2022
Policy Optimization A Markov Decision Process (MDP, Puterman, 1994) is a 6-tuple M “
(S, A,P,R,γ,Do), where S is the state space, A is the action space, P: S X A→ P(S) is the
transition model, R: S X A→ [0,Rmaχ] is the reward function, YP [0,1] is the discount factor, and
D0 P P(S) is the initial state distribution. The agent’s behavior is modeled by a parametric pol-
icy ∏θ : S→ P(A) belonging to a parametric policy space ∏θ = {∏e: θP Θ JRd}. The interac-
tion between an agent and the MDP generates a trajectory T“(s0,a0,s1,a1,...,sH—i,a，H´i,sh)
where HPN is the trajectory length and so 〜Do, at 〜∏θ(∙∣st), st`i 〜P(∙∣st,at) for all tP
{0,...,H —1}. Given a trajectory T, the return is the discounted sum of the rewards R(T)“
XH´1 YtR(St,at). For a policy ∏θ P ∏θ, We denote with p(∙∣θ) the induced trajectory distribution:
P(T∣θ) “Do(so)皿句1 ∏θ(at∣st)P(st'i∣st,at). In the action-based (AB) setting, an agent aims
at finding a parametrization fulfilling: θ* Pargmaxep& {J(θ)}, where J(θ) = ET〜p(∙∣θ) [R(τ)] is
the expected return. πθ must be stochastic to ensure exploration. Instead, in the parameter-based
(PB) setting, we consider a hyperpolicy νρ, belonging to a parametric hyperpolicy space NP “ tνρ :
ρPPJRlu, from which we sample the parameters θ of the policy. In this case, the policy πθ can
be deterministic since exploration is managed at the hyperpolicy level and the agent goal becomes
to learn a hyperpolicy parametrization maximizing the expected return: ρ* P argmaXρpp {J(ρ)},
where J(ρ) “ Eθ~ν° [J(θ)]. In the paper, we keep the presentation as general as possible, introduc-
ing the results for arbitrary distributions. Then, we will particularize for the parametric PO setting.
3	MINIMUM—Variance BEHAVIORAL Distribution
In this section, we revise Off-VM, i.e., the problem of finding a behavioral distribution QP P(X)
that induces an IS estimate μp∕q with minimum variance, knowing the (fixed) target distribution
PP P(X) and function f PB(X, [0, 8)).1 Furthermore, we do not enforce any restriction on the
possible forms of the behavioral distribution QP P(X). The problem and the corresponding well-
known minimum-variance behavioral distribution Q* are stated in the following (Kahn, 1950):
*	—	q*(x) = Ep(X).Xq], VxPX.	(1)
Ex„P [f (x)]
We observe that the IS estimator pp∕q* is non-stochastic, equal to the quantity we aim to estimate,
i.e., μp∕Q* = Eχ~p[f (x)]. This suggests that the construction of Q* is infeasible as it requires
knowledge of Ex„P [f (X)]. Since Q* generates a non-stochastic estimator, it not only leads to
zero-variance but, clearly, simultaneously minimizes the absolute central moments of any order. A
Q 哪 JVarx„Q 箸 f (X)
second, and most remarkable property, is that Q* is a performance improvement w.r.t. P, i.e., the
expectation of f under Q* is larger than the expectation of f under the target P (Owen, 2013):
Ex~Q* [f(x)S— Ex„P [f(x)] =
Varx„P [f(x)]
Ex„p [f(x)]
(2)
It is worth noting that the magnitude of the improvement is directly related to the reduction in
variance Varx„P [f (X)]. Equation (2) suggests an appealing connection between the problem of
finding the minimum-variance behavioral distribution (Off-VM) and the problem of finding a target
distribution that maximizes the expectation Ex„P [f (X)] (Off-PL). In other words, we could employ
Off-VM as a performance improvement tool, by repeatedly solving the problem in Equation (1).
> 0.
In the following two sections, we will delve into the properties of the repeated construction of
the minimum-variance distribution as a performance improvement tool under two assumptions: (i)
there are no restrictions in the choice of the behavioral distribution QP P(X) (Section 4); (ii) the
behavioral distribution must be chosen within a subset QP QJ P(X) (Section 5). In both cases, we
will address the following three questions: (Q1) Does this procedure always generate a distribution
that is a performance improvement? (Q2) Does this procedure converge to a (global or local)
maximum of f ? (Q3) Can we quantify the divergence between two consecutive distributions, i.e.,
does this procedure enforce a trust region?
Before proceeding, let us map this general setting to PO. In the action-based (AB) setting, X is
the trajectory T, P and Q are trajectory distributions p(τ∣θ) induced by policies ∏θ. Instead, in
the parameter-based (PB) setting, x is the pair (Θ,t), P and Q are joint distributions Vρ(θ)p(T∣θ)
induced by hyperpolicies νρ. In both cases, function f is the trajectory return R(T ).
1We restrict our attention to non-negative functions. From the RL perspective, this choice is w.l.o.g. since
we can always define an equivalent non-negative reward function, by means of a translation of the original one.
3
Under review as a conference paper at ICLR 2022
4	Unconstrained Probability Distribution Space
In Section 3, We have seen that Q* is a performance improvement w.r.t. P. We now generalize this
construction, by composing function f with a non-negative monotonic strictly-increasing function
h: [0,8)→ [0,8). The rationale behind this choice is that if h is strictly-increasing, then ho f has
the same maxima as f .2 We start defining the operator Ihι0f : P(X) → P(X):
(ZhofrPs)(X) =
p(x)h(f(x))
Ex„p [h(f(x))s,
@xPX.
(3)
Thus, Ihof takes as input a target distribution PP P(X), a function ho f PB(X, [0,8)), and out-
puts the minimum-variance behavioral distribution for the IS estimation of Ex„P rh(f (x))s, i.e.,
Q* =Ihof [Ps. Intuitively, looking at Equation (3), by iterating the application ofIhof, we will ob-
tain distributions tending to assign larger probability mass to points xPX with high values of f (x).
Concerning (Q1), the following result, due to Ghosh et al. (2020), generalizes Equation (2) showing
that whenever h is increasing, we can prove that Ihof [Ps is a performance improvement w.r.t. P.
Proposition 4.1 (Proposition 9 of Ghosh et al. (2020)). Let PP P(X), fPB(X, [0, 8)), and h:
[0, 8) → [0, 8) monotonic increasing. Then, Ihof [P s is a performance improvement w.r.t. P:
Eχ~ihf[p Srf(X)s´ Ex„P [f(x)s =
Covx„p [h(f(x)),f(x)s
Ex„p [h(f (x))s
> 0.
Note that, since h is a monotonic increasing function, we have that Covχ~p[h(f (x)),f (x)s >
0 (Cuadras, 2002). The following sections tackle questions (Q2) and (Q3).
4.1	Convergence Properties
We now address question (Q2), analyzing the effect of repeatedly applying operator Ihof. More
formally, let us consider an initial distribution PP P(X), and suppose to iterate the application of
the operator Ihof, generating the sequence of distributions (Qk )kPN, where Q0 =P and for every
kPN50 we have Qk = Ihof [Qk—is = (IhOf )k [Ps. The following result shows that, under certain
conditions, the operator Ihof admits fixed points and the sequence (Qk)kPN converges to a distribu-
tion Q8 that assigns probability only to the global maxima of f, restricted to the support ofP, i.e.,
supp(P ).
Theorem 4.2. Let PP P(X), fPB(X, [0, 8)), and h : [0, 8) → [0, 8) monotonic strictly-
increasing. Then, the following statements hold:
(i)	P is a fixed point ofIhof, i.e., Ihof [P s =P a.s., if and only if Varx„P [f (X)s =0;
(ii)	let X* = arg maxxPsupppP q tf (X)u be the set of maxima of f restricted to the support of P.
If X * is non-empty and measurable then, the repeated application of Ihof converges to
a distribution Qg = limk-g (IhOf) [Ps with Support X*. In particular Ex-Q- [f (x)s =
maxxPsupppP q tf (X)u.
Some remarks are in order. First, both properties are independent of the function h as long as it is
non-negative and monotonic strictly-increasing. This is expected since, ho f admits the same set of
global optima of f. Second, as a corollary to point (i), any deterministic P is a fixed point of Ihof.
Finally, from point (ii), we deduce that if we select P that assigns non-zero probability to all points
in X, i.e., supp(P ) = X, the iterated application of Ihof converges to the distribution Qg such that
Ex„Q8 [f (X)s =maxxPX tf (X)u, i.e., we are performing a global optimization of f.
4.2	Implicit Trust Region
The reader might wonder what are the advantages of casting the optimization of function f as such
an iterative procedure. The reason lies in question (Q3). We now prove that we are able to naturally
control the divergence between two consecutive distributions Qk and Qk+i = IhOfrQks with kPN,
with the effect of enforcing an implicit trust region. The following result shows how it is possible to
obtain a bound on the α-Renyi divergence between two consecutive distributions.
2As we shall see in the following sections, the different choices of h will be useful to control the trust region
of the optimization process.
4
Under review as a conference paper at ICLR 2022
k
k
-→- β = 0.01 -4∙ β = 0.1 ■■■■ β = 0.2	β = 1 β = 2 -*- β =10 -I- β = 100
Figure 1: The Ackley function (left), the expectation of the distribution Qk “ (ZhOf )k [PS (center),
and the KL-divergence (right) between two consecutive distributions Qk´1 and Qk, with h = (∙)β.
Theorem 4.3. Let PPP(X), f PB(X, [0,8)), and h: [0,8)→[0,8)monotonic StrictIy-
increasing. Then, for every αP r0, 8s, it holds that:
Dα (IhOf [P s}P ) “
In particular, for α “ 1 it holds that:
DKL (IhOf [P s}P ) “
1 ι	Ex„p [h(f(x))αS
α- 1 °g Ex„p[h(f(x))Sα
Covx„p [h(f (x)), log h(f (x))S
Eχ~p[h(f(x))S
FOr α =2, we Obtain D2 (Ihof rPs}Pq“ log ExlPrhpf pχqq22 ≤ VarxP'pMχ秒.Thus, the divergence
is large when the variance of h(f (x)) is. The result is particularly remarkable as we are able to
control the Renyi divergences of any order a P [0,81. This is a relevant achievement since the trust
regions commonly used, like KL-divergence (Schulman et al., 2015), are unable to control higher-
order divergences that can still be infinite. We can also appreciate the role of the increasing function
h that works as a regularizer with the effect of controlling the size of the trust region. The following
example shows that the faster h increases, the larger the induced trust region becomes.
Example 4.1. We consider (a slight variation of) the one-dimensional Ackley function (Ackley,
2012): f (x) = —5'20exp(—0.1414|x|)+ exp(0.5(cos(2πx) ' 1))' e, shown in Figure 1 (left) and
the class of increasing functions (h o f )(x) = f (x)β where β20. We consider an initial uniform
distribution P=Uni ([—5, 5s). In Figure 1, we plot the expectation of distribution Qk = (IhOf )k [P s
(center) and the KL-divergence between two consecutive distributions (right), as a function of the
number of applications k, for the different β values. We observe that convergence to the global
optimum (x* = 0 and f (x*) = 15) isfasterfor higher powers that also lead to larger trust regions.
5	Constrained Probability Distribution Space
The approach we have presented in Section 4 can be applied when there are no restrictions on the
class of distributions that can be played, i.e., we can select Q in the whole space P(X). However,
in the action-based PO, we can intervene on the policy ∏θ factors only of the distribution p(τ∣θ)=
Do (so) ∏H01 ∏θ (at∣st)P (st'i∣st,at), leading to a constrained setting. Similarly, in the parameter-
based PO, we can act on the hyperpolicy VP while keeping the trajectory distribution p(τ∣θ) fixed.
More in general, when considering a class of distributions Q J P(X), even if PP Q, the distri-
bution IhOf [Ps might not belong to Q.Furthermore, while IhOf [Ps minimizes all absolute central
α-moments of the IS estimator, as it leads to a non-stochastic estimator (Section 3), there may exist
different distributions in Q minimizing the different absolute central α-moments:
田哄[e,~q Hpxq h(f (x))—Eχ~p [h(f(χ))s 1V	(4)
QPQ〔	LI q(x)	JJ
Apart from α = 2, where the problem in Equation (4) reduces to Equation (1), for general value
of α P [0, 8s, the optimization is not straightforward (e.g., Equation (4) is not differentiable for
ap(0, 2)). The following result shows that performing a moment projection through the α-Renyi
divergence is a reasonable surrogate for minimizing the absolute central α-moments of Equation (4).
Proposition 5.1. Let PP P(X), fPB(X, [0, 8)), and h: [0, 8) → [0, 8) monotonic strictly-
increasing. Then, for any αP (1, 8), it holds that:
5
Under review as a conference paper at ICLR 2022
Ex„Q „| Ppxq hpfpX))- Ex„p [hfpxqqsjk Ex„q „(翳 hf pxqq)} epfDMhf rp s}QqEχ~p [hf pχqqsα.
absolute central α-moment
(non-central) α-moment
n
Thus, having considered the subset of distributions Q J P(Xq, whenever ZhOfrPS R Q, We replace
it with the corresponding moment projection performed through the α-Renyi divergence:
Q: P arg min tDα pIhOf rP s}Qqu.
QPQ
(5)
In the following, we shall address the questions (Q1), (Q2), and (Q3) for the constrained setting.
5.1	Performance Improvement
In Proposition 4.1, we have seen that, whenever h is strictly-increasing, IhOf rPs is a performance
improvement w.r.t. P, evaluated under function f (and also under the composition between f and
any strictly-increasing function). In this section, we address question (Q1), showing that, when
considering a subset of distributions QJ PpXq, the performance improvement cannot be in general
guaranteed for f, but just for a specific monotonic transformation of f, depending on h and α.
Theorem 5.2. Let PP P(Xq, f P B(X, [0, s)), and h: [0,8)→ [0,8) monotonic StrictIy-
increasing. Let QJ P pXq, QP Q, and αP r0, 8s, then, it holds that:
Ex„Q [h(f (xqqαS-Ex„p[h(f (xqqαS ≥ Ex„p[h(f (Xqqsa ´e(i)Da(Ihf rp]}P) _e(a-1)Da(Ihf rP1}Q)).
a 一 1
In particular, for α “ 1, it holds that (Ghosh et al., 2020, Proposition 6):
Ex„q [h(f(Xqqs- Ex„P [h(f (χqqs> EX„P [h(f(Xqqs(D KL (IhOf [p]}p q-D KL(Ihof[ps}Qqq.
The theorem shows that by minimizing the ɑ-moment of the transformed function h o f, we are able
to guarantee a performance improvement on the function (∙qa o h o f. The result holds provided that
Da (Ihof [Ps}Qq ≤ Da (IhOf [Ps}P), which is always guaranteed when PP Q and Q “ Q:, being
Q: defined in Equation (5) as the minimizer of the second divergence term. In particular, ifwe select
h = (.q1/a, the guarantee holds for the function f directly. For all other choices, the performance
improvement can be guaranteed for a monotonic transformation of f only.3
5.2	Convergence Properties
We now turn to (Q2). By using Equation (5) as an iterate Qk'i PargminQPQ {Da(Ihof [Qks}Qq}
to generate a sequence of distributions (Qk qkPN, we are not guaranteed to converge to any fixed-
point distribution Q8, differently form the unconstrained setting (Theorem 4.2). This is because the
minimization might yield multiple solutions. Nevertheless, we are able to provide guarantees on the
final divergence value and on the performance of the distributions Qk.
Theorem 5.3. Let PP P(Xq, fPB(X, [0, 8qq, and h : [0, 8q → [0, 8q monotonic strictly-
increasing. Let Q J P (Xq and suppose that h o f is bounded from above, then, the iterate
Qk'i P argminQPQ {Da(Ihof [Qks}Qq} (where possible ties are broken arbitrarily) satisfies:
(i)	the sequence of divergences Da(IhOf [Qk s}Qk q is convergent;
(ii)	the sequence of expectations Ex„Qk [h(f (Xqqa s is non-decreasing in kPN and converges to
a stationary point of Ex„Q [h(f (Xqqa s w.r.t. QP Q.
The convergence of the sequences Da(IhOf [Qks}Qkq and Ex„Qk [h(f (Xqqa s is derived by the per-
formance improvement of Theorem 5.2. Importantly, Theorem 5.3 shows the convergence to a
stationary point of Ex„q [h(f(xqqas. If Q is a parametric space Qξ = {Qξ P P(Xq: ξP Ξ JRd},
then we are guaranteed to stop when E,~Qξ [Vξ logqξ(χqh(f (Xqqas= 0, like for a general policy
gradient method maximizing h(f (Xqqa (Papini et al., 2018). Compared to the result for the uncon-
strained distribution space (Theorem 4.2), we loose the convergence to a fixed point. This property
can be recovered under the assumption that the iterate in Equation (5) admits a unique solution for
every P. In such a case, we will converge to a distribution Q8 = arg minQPQ tDa (IhOf [Qs}Qqu.
3In Appendix B, we discuss the effects of optimizing a power of f instead of f itself, i.e., when h“(∙)β;
while in Appendix C, we discuss cases in which the performance improvement can be obtained for MDPs.
6
Under review as a conference paper at ICLR 2022
5.3	Implicit Trust Region
In Theorem 4.3, We have proved that the α-Renyi divergence between ZhOf [PS and P is bounded.
In this section, we answer (Q3), wondering whether similar properties hold when we consider a
limited set of distributions Q J P (X). The following result shows that, under a particular form of
convexity (van Erven & Harremoes, 2014) of Q, we are able to control the trust region as well.
Theorem 5.4. Let f PB(X, [0,8)), and h: [0,8)→[0,8)monotonic strictly-increasing. Let
Q ⊂ P (X) be a (1 — α) -convex set (van Erven & Harremoes, 2014, Definition 4), P P Q, Q: P
arg minQPQ tDα (IhOf [P s}Q)u, and αP [0, 8s, then it holds that:
Da(Q：>P)≤ Da (IhOfrPS}P)´ Da (IhOfrP] >Q：).
Therefore, we are always guaranteed that the trust region induced by Q: is tighter compared to the
one induced by Q* “ IhOfrPs computed in Theorem 4.3, i.e., Da (Q:>P) ≤ Da (IhOfrP ]}P).
6	Policy Optimization via Optimal Policy Evaluation
In this section, we build a sample-based Off-PL algorithm, named Policy Optimization via Optimal
Policy Evaluation (PO2PE), which uses Off-VM as an inner loop. For generality, we consider a
parametric distribution space Qξ “ {Qξ P P(X): ξPΞ JRd}, a common setting met in PO.4
The structure of PO2PE consists
of two nested loops. The outer
loop (Optimization) acts on
the target distribution qξi. At
the end of each outer iteration i P
ris, the target distribution qξi”
is updated with the last behav-
ioral distribution produced by the
inner loop qξij ^. Instead, the
inner loop (Evaluation) takes
the target distribution provided by
the outer loop qξi and provides a
new behavioral distribution. At
each inner iteration j P rJs, it col-
lects samples Di,j with the current
Algorithm 1: PO2 PE.
input : α divergence order, h function, f function, QΞ
distribution space, ξ1 initial parameter, n batch size
output: final parameter ξi'i P Ξ
1	for i “ 1, . . . , I do	Optimization
2	ξi,1 “ ξi
3	for j “ 1, . . . , J do	Evaluation
4	I ! Collect Dij “tpxi,fPxι))}iP[n] with Q-6
5	Using pDi,kqkPrjs, perform M steps of gradient
descent on (Obj).
6	end
7	ξi'1 “ ξi,J'1
8	end
behavioral distribution qξ and employs them, together with all the samples collected so far
(Di,k)kpj], to compute the next behavioral distribution qξ 十Ι，with the goal of finding the be-
havioral distribution minimizing the absolute central α-moment of the IS estimator (Equation 4). As
we shall see, the optimization is performed using samples and by resorting to a penalized objective.
SamPle-based Optimization The problem of finding the next behavioral distribution parameter
ξi,j+ι using the samples collected so far (。公院团 is an off-policy learning problem. Let us define
Φi,j “ 1 XkPrjs q-ik as the mixture of the j behavioral distributions experienced so far in the inner
loop. Instead of directly estimating Da (IhOf rQξi s}Qξ )), we refer to the (non-central) α-moment,
which is connected to the original objective through Proposition 5.1. Since we have samples coming
from different behavioral distributions, we can use a multiple IS estimator Veach & Guibas (1995):
Pa (IhOfrQξjQi )$ fc∑ ι∑ φ⅛⅛ 望东
kPrj s lPrns l
h(f (xk,l))a.
(6)
oooon l
(aq	(b)
The (a) factor accounts that we are using samples collected with the mixture Φi,j to estimate an
expectation under qξ, whereas the (b) factor is the actual variable we want to compute the expectation
of, i.e., the α-moment. Itis simple to prove that the expectation of da is indeed the α-moment (Papini
et al., 2019). To minimize Equation (6), we employ a variance correction to mitigate the effect of
finite samples (Metelli et al., 2018), theoretically grounded in the following result.
4In the action-based PO ξ “ θ are the policy parameters and Ξ “ Θ, while in the parameter-based PO ξ “ ρ
are the hyperpolicy parameters and Ξ “ P .
7
Under review as a conference paper at ICLR 2022
CartPole	Inverted Double Pendulum Mountain Car
5000
4000
3000
2000
1000
0
i≡
Swimmer
0 1 2 3 4 5,
×104
Episodes
—∙- AB-PO2PE -4・ AB-POIS ■■■■■■■ TRPO PB-POIS	PB-PO2PE
Figure 2: Average return as a function of the number of episodes for different environments and
algorithms with batch size n “ 100, α “ 2, h “ Id, and J “ 1 (20 runs + 95% bootstrapped c.i.).
Theorem 6.1. Let Qξ / P (X) be a set of parametric distributions and let ξ, ξ P Ξ. If }h o f }g ≤
m, then, ifsamples are independent, for every δ P r0,1S, with probability at least 1 一 δ it holds that:
Ex„ _(qH⅛h(f(X))) _ &d(IhfrQ is}Q ∖φi,j)+mα{^jJXKj(；)q(χX)2(ατ)dx∙
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon
y	-
(Obj)
Some remarks are in order. First, the integral within the square root is an upper bound to the variance
of the α-moment estimator dα (IhOfrQ iS}Q *j). In particular, When “ i, we obtain the ex-
ponentiated Renyi divergence, as illustrated in (Metelli et al., 2020). When all involved distributions
are Guassians, it is possible to provide a closed-form tight bound on this quantity (Appendix D).
Second, unlike the results available in the literature about concentration of IS estimator, without
corrections or transformations, we are able to provide an exponential concentration inequality (de-
pendence on delta of the form log(1{δ) ), instead of a polynomial concentration (dependence of
the form 1{δ). This is due to the fact that we are dealing with random variables that are bounded
to zero from below and they allow applying stronger unilateral Bernstein’s concentration inequal-
ities (Boucheron et al., 2009). The reader might object that to optimize the proposed objective
function, designed to enforce an implicit trust region, we are actually introducing an additional cor-
rection term. This is necessary for theoretical purposes, but, as we shall see in the Section 7, the
need for a penalization or constraint is significantly less relevant than in existing approaches, like
TRPO (Schulman et al., 2015), or POIS (Metelli et al., 2018).
Sample Collection In the action-based setting (AB-PO2PE), we sample n trajectories tτlulPrns
independently with the policy ∏θi, and we build the dataset Dij “ {(τι, R(Tl))}仁〔却.Instead, in the
parameter-based setting (PB-PO2PE), we sample independently n policy parameters tθlulPrns and
for each of them we run policy πθl once to generate trajectory τl. The corresponding dataset is given
by Di,j “ t((θl,τl),R(τl))ulPrns. For the AB case, the correction in Theorem 6.1 is estimated from
samples, as done for the Renyi divergence in (Metelli et al., 2018), since it involves integrals between
trajectory distributions, while the closed form exists for Gaussian distributions (Appendix D).
As noted in Section 5, PO corresponds to a constrained setting and, thus, we are in general unable to
provide a performance improvement guarantee for every h (Theorem 5.2). We show in Appendix C
that performance improvement, independently on h, is ensured for deterministic environments and
we show some (only theoretical) approaches to extend the guarantee to stochastic environments.
7	Experimental Evaluation
In this section, we provide the experimental evaluation of PO2PE on continuous control tasks. We
first compare the learning performance ofPO2PE with POIS (Metelli et al., 2018) and TRPO (Schul-
man et al., 2015) on four benchmarks. Then, deepen two relevant aspects of PO2PE: its robustness
to small batch sizes and the effect of the function h. All experiments are conducted with Gaussian
policies, linear in the state, with fixed variance. The experimental details are reported in Appendix E.
Comparison with POIS and TRPO In Figure 2, we show the average return as a function of
the number of collected episodes, with a batch size n = 100, using ɑ “ 2, h = Id (identity function),
and one inner iteration (J “ 1). In the Cartpole environment, we observe that the performance of
8
Under review as a conference paper at ICLR 2022
nruter egarev
n = 50
5000
4000
3000
2000
1000
0
n= 11
5000
4000
3000
2000
1000
0
nruter egarev
2500
2000
1500
1000
500
0
12345
Episodes ×1
—β = 1	- j β = 0.1
012345	012345
Episodes ×10	Episodes ×1
—AB-PO2PE (J = 1) →-∙ AB-PO2PE (J = 2)
-M-- AB-PO2PE (J = 10)	-4- AB-POIS	TRPO
■■■■— β = 0.5	β = 2 — *■- β = 4
0
Figure 3: Average return as a function of the number
of episodes in the Cartpole environment for different
algorithms, batch-size n and inner iterations J (10 runs
+ 95% bootstrapped c.i.).
Figure 4: Average return as a function
of the number of episodes in the Inverted
Double Pendulum for different choices of
h = (∙)β (5 runs + 95% bootstrappedc.i.).
AB-PO2PE is slightly above that of AB-POIS and PB-PO2PE; while the fastest learning curve is
shown by PB-POIS. Instead, TRPO converges to a suboptimal policy that fails keeping the pole in
the vertical position. In the Inverted Double Pendulum experiment, the gap between AB-PO2PE and
AB-POIS and TRPO is more evident. The PB versions outperform the AB ones with PO2PE slightly
faster than POIS. In the Mountain Car domain, while AB-POIS, TRPO, and PB-PO2PE display a
similar convergence speed, AB-PO2PE and PB-POIS reach the optimal performance faster. Finally,
in the Mujoco Swimmer domain (Todorov et al., 2012), AB-PO2PE and TRPO clearly outperform
AB-POIS, although the fastest learning curves are displayed by the PB versions of POIS and PO2PE.
Robustness to Small Batch Sizes Based on the previous results, we further investigate the prop-
erties of PO2PE in terms of variance control. In the Cartpole domain, we test the robustness to the
reduction of the batch size. In Figure 3, we show the average return as a function of the number
of collected episodes for batch sizes nP t11, 50u and different number of inner iterations J. Also
considering the n“ 100 case (Figure 2), we notice, as expected, that the variance of each setting
increases overall as n decreases. Nevertheless, PO2PE proves to be robust, always succeeding in
reaching the optimal performance. Differently, POIS suffers the reduced batch size, while TRPO
always converging to the same suboptimal policy. The desirable behavior of PO2PE is indeed an
effect of the kind of objective function we employ that explicitly accounts for the variance of the
estimator, trying to minimize it, and, as we have shown in the previous sections, it allows enforcing
an implicit trust region. Finally, a small number of inner iterations J is beneficial for the stability.
Effect of the Function h While previous experiments consider h as the identity function, we now
investigate the effects of using h =(∙)β, i.e., a power function. In Figure 4, We show the learning
curves of the Inverted Double Pendulum for different values of β . We notice that for β close to 1
(0.5, 1, 2) the curves are not very dissimilar, while for too extreme powers (0.1 and 4) the learning
performance degrades. This example shows an interesting phenomenon, i.e., even if we optimize a
power of return, within certain limits, we are still able to converge to a (near-)optimal policy.
8	Discussion and Conclusions
In this paper, we have deepened the study of importance sampling beyond its usage as a passive tool
for off-policy evaluation and learning. We imported the role of IS as a variance reduction active
tool, typical of the Monte Carlo simulation, to the off-policy learning setting. We have illustrated
that by minimizing the absolute central α-moment of the IS estimator we are able to guarantee the
performance improvement for a monotonic transformation of the original objective and eventually
converge, at least, to a stationary point. Interestingly, this approach is able to naturally induce a
trust region, mitigating the need for an explicit penalization or constraint. The experimental evalu-
ation confirmed our theoretical findings. PO2PE is able to outperform POIS and TRPO on several
continuous control tasks. Our algorithm has proved to be remarkably robust to the reduction of the
batch size. This is a consequence of using the minimum-variance behavioral distribution that has
the benefit of inducing an implicit trust region. We believe that this work contributes to shed light
on an appealing facet of off-policy learning with possible new research opportunities. Future works
include an extension of the convergence analysis to the sample-based setting and an experimentation
of PO2PE coupled with more complex policy architectures.
9
Under review as a conference paper at ICLR 2022
References
David Ackley. A connectionist machine for genetic hillclimbing, volume 28. Springer Science &
Business Media, 2012.
StePhane Boucheron, Gabor Lugosi, Pascal Massart, et al. On concentration of self-bounding func-
tions. Electronic Journal OfProbability, 14:1884-1899, 2009.
Konstantinos I. Chatzilygeroudis, Vassilis Vassiliades, Freek StulP, Sylvain Calinon, and Jean-
BaPtiste Mouret. A survey on Policy search algorithms for learning robot controllers in a handful
of trials. IEEE Trans. Robotics, 36(2):328-347, 2020. doi: 10.1109/TRO.2019.2958211. URL
https://doi.org/10.1109/TRO.2019.2958211.
Kamil Andrzej Ciosek and Shimon Whiteson. OFFER: off-environment reinforcement learning. In
Satinder P. Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference
on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, PP. 1819-1825.
AAAI Press, 2017.
Andrew R Conn, Nicholas IM Gould, and PhiliPPe L Toint. Trust region methods. SIAM, 2000.
Carles M Cuadras. On the covariance between functions. Journal of Multivariate Analysis, 81(1):
19-27, 2002.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on Policy search for robotics.
Found. Trends Robotics, 2(1-2):1-142, 2013. doi: 10.1561/2300000021. URL https://doi.
org/10.1561/2300000021.
Prafulla Dhariwal, ChristoPher Hesse, Oleg Klimov, Alex Nichol, Matthias PlaPPert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OPenai baselines. https:
//github.com/openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deeP re-
inforcement learning for continuous control. In Maria-Florina Balcan and Kilian Q. Weinberger
(eds.), Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New
York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceed-
ings, PP. 1329-1338. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/
duan16.html.
Jordan Frank, Shie Mannor, and Doina PrecuP. Reinforcement learning in the Presence of rare
events. In William W. Cohen, Andrew McCallum, and Sam T. Roweis (eds.), Machine Learning,
Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June
5-9, 2008, volume 307 of ACM International Conference Proceeding Series, PP. 336-343. ACM,
2008. doi: 10.1145/1390156.1390199. URL https://doi.org/10.1145/1390156.
1390199.
Dibya Ghosh, Marlos C. Machado, and Nicolas Le Roux. An oPerator view of Policy gradient
methods. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
22eda830d1051274a2581d6466c06e6c- Abstract.html.
Shixiang Gu, Ethan Holly, Timothy P. LillicraP, and Sergey Levine. DeeP reinforcement learn-
ing for robotic maniPulation with asynchronous off-Policy uPdates. In 2017 IEEE Interna-
tional Conference on Robotics and Automation, ICRA 2017, Singapore, Singapore, May 29 -
June 3, 2017, PP. 3389-3396. IEEE, 2017. doi: 10.1109/ICRA.2017.7989385. URL https:
//doi.org/10.1109/ICRA.2017.7989385.
John Hammersley. Monte carlo methods. SPringer Science & Business Media, 2013.
Josiah P. Hanna and Peter Stone. Towards a data efficient off-Policy Policy gradient. In 2018 AAAI
Spring Symposia, Stanford University, Palo Alto, California, USA, March 26-28, 2018. AAAI
Press, 2018. URL https://aaai.org/ocs/index.php/SSS/SSS18/paper/view/
17578.
10
Under review as a conference paper at ICLR 2022
Josiah P. Hanna, Philip S. Thomas, Peter Stone, and Scott Niekum. Data-efficient policy evaluation
through behavior policy search. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the
34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, volume 70 of Proceedings ofMachine Learning Research, pp. 1394-1403. PMLR,
2017.
Josiah Paul Hanna et al. Data efficient reinforcement learning with off-policy and simulated data.
PhD thesis, 2019.
Timothy Classen Hesterberg. Advances in importance sampling. PhD thesis, Citeseer, 1988.
Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statis-
tics, 17(2):295-311, 2008.
H. Kahn and A. W. Marshall. Methods of reducing sample size in monte carlo computations. Oper.
Res., 1(5):263-278, 1953. doi: 10.1287/opre.1.5.263. URL https://doi.org/10.1287/
opre.1.5.263.
Herman Kahn. Random sampling (monte carlo) techniques in neutron attenuation problems. i.
Nucleonics (US) Ceased publication, 6(See also NSA 3-990), 1950.
Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In Claude Sammut and Achim G. Hoffmann (eds.), Machine Learning, Proceedings of the Nine-
teenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia,
July 8-12, 2002, pp. 267-274. Morgan Kaufmann, 2002.
Nate Kohl and Peter Stone. Policy gradient reinforcement learning for fast quadrupedal locomotion.
In Proceedings of the 2004 IEEE International Conference on Robotics and Automation, ICRA
2004, April 26 - May 1, 2004, New Orleans, LA, USA, pp. 2619-2624. IEEE, 2004. doi: 10.1109/
ROBOT.2004.1307456. URL https://doi.org/10.1109/ROBOT.2004.1307456.
Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesvari. Confident off-policy evalu-
ation and selection through self-normalized importance weighting. 130:640-648, 2021.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http:
//arxiv.org/abs/1509.02971.
Andreas Maurer et al. A bound on the deviation probability for sums of non-negative random
variables. J. Inequalities in Pure and Applied Mathematics, 4(1):15, 2003.
Alberto Maria Metelli, Matteo Papini, Francesco Faccio, and Marcello Restelli. Policy optimization
via importance sampling. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grau-
man, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Process-
ing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS
2018, DeCember 3-8,2018, Montreal, Canada,pp. 5447-5459, 2018.
Alberto Maria Metelli, Matteo Papini, Nico Montali, and Marcello Restelli. Importance sampling
techniques for policy optimization. J. Mach. Learn. Res., 21:141:1-141:75, 2020.
Art B Owen. Monte carlo theory, methods and examples, 2013.
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.
Stochastic variance-reduced policy gradient. In Jennifer G. Dy and Andreas Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stock-
holmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine
Learning Research, pp. 4023-4032. PMLR, 2018. URL http://proceedings.mlr.
press/v80/papini18a.html.
11
Under review as a conference paper at ICLR 2022
Matteo Papini, Alberto Maria Metelli, Lorenzo Lupo, and Marcello Restelli. Optimistic policy op-
timization via multiple importance sampling. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-
15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning
Research,pp. 4989-4999. PMLR, 2019.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
Networks, 21(4):682-697, 2008. doi: 10.1016/j.neunet.2008.02.003. URL https://doi.
org/10.1016/j.neunet.2008.02.003.
Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy itera-
tion. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013,
Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceed-
ings, pp. 307-315. JMLR.org, 2013. URL http://proceedings.mlr.press/v28/
pirotta13.html.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In Pat Langley (ed.), Proceedings of the Seventeenth International Conference on
Machine Learning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000,
pp. 759-766. Morgan Kaufmann, 2000.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wi-
ley Series in Probability and Statistics. Wiley, 1994. ISBN 978-0-47161977-2. doi: 10.1002/
9780470316887. URL https://doi.org/10.1002/9780470316887.
Alfred Renyi. On measures of entropy and information. Technical report, Hungarian Academy of
Sciences Budapest Hungary, 1961.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust re-
gion policy optimization. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, vol-
ume 37 of JMLR Workshop and Conference Proceedings, pp. 1889-1897. JMLR.org, 2015. URL
http://proceedings.mlr.press/v37/schulman15.html.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-
policy evaluation. In Blai Bonet and Sven Koenig (eds.), Proceedings of the Twenty-Ninth AAAI
Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, pp. 3000-3006.
AAAI Press, 2015.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS
2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026-5033. IEEE, 2012. doi:
10.1109/IROS.2012.6386109.
Tim van Erven and Peter Harremoes. Renyi divergence and kullback-leibler divergence. IEEE
Trans. Inf. Theory, 60(7):3797-3820, 2014. doi: 10.1109/TIT.2014.2320500. URL https:
//doi.org/10.1109/TIT.2014.2320500.
Eric Veach and Leonidas J. Guibas. Optimally combining sampling techniques for monte carlo
rendering. In Susan G. Mair and Robert Cook (eds.), Proceedings of the 22nd Annual Conference
on Computer Graphics and Interactive Techniques, SIGGRAPH 1995, Los Angeles, CA, USA,
August 6-11, 1995, pp. 419-428. ACM, 1995. URL https://dl.acm.org/citation.
cfm?id=218498.
12
Under review as a conference paper at ICLR 2022
A Proofs and Derivations
In this appendix, we report the proofs and derivations, we have omitted in the main paper.
A.1 Proofs of Section 4
Proposition 4.1 (Proposition 9 of Ghosh et al. (2020)). Let P P PpXq, f PBpX, r0, 8qq, and h:
[0,8)→ [0,8)monotonic increasing. Then, Ihof [PS is a performance improvement w.r.t. P:
Eχ~Zhf [P Srf(X)s´ Ex„P[f (x)s =
Covx„p [h(f(x)),f(x)]
Ex„P [h(f(x))S
> 0.
Proof. Let us consider the following derivation:
Eχ~ιw [P Srf(X)s´ Ex„P [f(x)S= f	p(x)h(fPx)) f (x)dx ´ Ex„p [f(x)S
X Ex„P [h(f (x))s
Ex„p [h(f(x))f(x)S— Ex„p [f(x)]Eχ~P [h(f(x))S
“	Ex„p [h(f(x))S
_CoVx„P [h(f(x)),f(x)S
“Ex„p [h(f(x))S	,
where we have exploited the definition of Ihof and the definition of covariance. The result is ob-
tained by recalling that h is increasing and the covariance between two increasing functions of the
same random variable (i.e., h and the identity function) is non-negative (CUadras, 2002).	□
Theorem 4.2.	Let PP P(X), fPB(X, [0, 8)), and h: [0, 8) → [0, 8) monotonic strictly-
increasing. Then, the following statements hold:
(i)	P is a fixed point ofIhof, i.e., Ihof [P s “P a.s., if and only ifVarx„P[f(X)s “0;
(ii)	let X * “ argmaXχpsupppp){f (x)} be the set of maxima of f restricted to the SuPPort of P.
If X* is non-empty and measurable then, the repeated application of IhOf converges to
a distribution Qg “limk-g (IhOf) [PS with SuPPOrt X*. In particular Eχ~Qs [f (x)S =
maxxPsupppP q tf (X)u.
Proof. We start with (i). First of all, we observe that since h is monotonically strictly-increasing it
holds that Varx„P [f (X)S “0 if and only if Varx„P [h(f (X))S “0. P is a fixed point ofIhOf, i.e.,
P“IhOf [PS a.s. if and only if for all XPX it holds a.s.:
P q_ P(Xqhpf(Xyq
Ppxq_ Ex„p[h(f(χ))S,
that occurs if and only if either P(X)= 0 (x R SuPP(P ))or h(f (x)) “ Eχ~p [h(f (x))S. (^)Whenever
P(X) is not zero, function h(f (X)) is a constant in supp(P ) and, consequently, its variance under
P is zero. (ð) Suppose that Varx„P [h(f (X))S =0, then h(f (X)) =Ex„P [h(f(X))S almost surely
and, consequently 旧似？IhpffxqqqS = P(X) almost surely. Let US now consider (ii). First of all, We can
easily observe that for every kPN:
(IhOf )k [P S(X) =
p(X)f(X)k
Ex„P[f(X)kS.
Let f * = maxχpsuppppq {f (x)}, consider the function gk(x) = P(X) ´ffxq) and the limit:
lim gk (X)= lim p(x) ^8 Y = "p(x) if XP X*
kl-8gk( ) = k-8P( ) ^ f* ) =(0 otherwise .
13
Under review as a conference paper at ICLR 2022
Thus, we have:
Qg = lim (Zhof qk [P](χ)= lim
k→8	k→8
p(x)f(x)k
IXp(X)f(x)k dx
“ lim
k→8
gk(x)
IX gk(x)dx
p	Ppx)
“ √ Sx* Ppxqdx
0
if x p X *
otherwise
Thus, the support of Q8 is given by X*. Consequently, the expectation of f under Q8 is given by:
Ex„Q8 rf(x)s“ X
q8(x)f(x)dx“f*.
□
Theorem 4.3.	Let PP P(X), f PB(X, [0,8)), and h: [0,8)→[0,8) monotonic StrictIy-
increasing. Then, for every αP r0, 8s, it holds that:
Dα (Ihof [Ps}P) “
1 I	Ex„p [h(f (x))αS
a´iog Ex„p[h(f(x))sa.
In particular, for α “ 1 it holds that:
DKL (Ihof [P s}P ) “
Covx-p [h(f (x)), log h(f (x))S
Ex„p [h(f(x))S
Proof. Let us consider the following derivation:
J ：“ IX ((Ihof [P S)(X))a PT” “ L ^ ESf	y p(X)jdX
_Ex„P [h(f(χ))αS
“Ex„P [h(f (χ))Sɑ.
By observing that Da (IhOf [PS}P)“ o´ɪlogJ, We obtain the result. For a“ 1, We provide an
independent derivation:
D (I	[Ps}pq= f	P(Xqhpf(Xyq I	p(χ)h(f(χ)q
DKL(Ihof rPs}Pq“ JX Ex„p[h(f (X))Slog Ex„p[h(f (X))Sp(X)dx
“ Ex„p [h(f (x)) log h(f (x))S— Ex„p[h(f (X))SEx„p [log h(f (x))S
“	Ex„P [h(f(X))S
=Covx „ P [h(f (x)), log h(f (x))S
-	Ex„P [h(f(X))S	,
where we exploited the definition of covariance in the last line.	□
A.2 Proofs of Section 5
Proposition 5.1. Let PP P(X), fPB(X, [0, 8)), and h: [0, 8) → [0, 8) monotonic strictly-
increasing. Then, for any αP (1, 8), it holds that:
Ex„Q „|%h(f(X))-Ex„p[h(f(x))S∣α] ≤Ex„Q [(工h(f(x)))"] “ep—qDa(Ihf rPs}QqEx~p[h(f(X))]a.
∖ ， ∖ √
absolute central α-moment
(non-central) α-moment
Proof. First of all, we observe that since Ex„Q Ippxqh(f (x))∣ = Ex„p [h(f (x))S, for a21, the
absolute central a-moment is smaller or equal than the (non-central) α-moment. Thus, for a21,
14
Under review as a conference paper at ICLR 2022
we have:
α
Ex„Q P) [ hf Pxqq ´ Ex„P rhpf Pxqqsl ] W Ex„Q
qpxq
“ f ^FpPxqhPfPxqq])ɑq(x)1rdxEχ~P[h(fpxqqsɑ
X Ex„P rhPf Pxqqs
“ JX ppih0f rp sqpxqqα q(xq-αdxEχ~p rh(f pxqqsɑ
“exp {p0—ιq o´´ɪiog J ((IhOfrPsqpxqqa qpxqjdxkx-p 囱/Pxqqsα.
By applying the definition of Renyi divergences, We get the result.
□
Theorem 5.2. Let PP P(X), f PBPX, [0,8)), and h: [0,8)→[0,8)monotonic strictly-
increasing. Let Q 三 P (Xq, Q P Q, and a p[0, 8s, then, it holds that:
Ex„q [h(f Pxqqas´ Eχ~p[h(f pxqqas ≥ Ex~Prhpf Pxqqsa (epiqDapIhf rp s}P)_e(a-1)Da(Ihf rP s}Q)).
α 一 1
In particular, for α “ 1, it holds that (Ghosh et al., 2020, Proposition 6):
Ex„Q [h(f (xqqs´ Ex„p [h(f pxqqs > Ex„p [h(f pxqqs(D KL (IhOf [ps}p q´ D KL (IhOf [ps}Qqq.
Proof. Let us consider the folloWing derivation:
Ex„Q[h(f(xqqas “	q(xqh(f(xqqadx
X
“ f ppxq qpxqh(f pxqqadx
X	p(xq
“J ppxqhpfpxqqadx+J ppxq (pp∣q■ — 1) h(f(xqqadx
》[ppxqhpfpxqqadx+
X
h(f (xqqa dx
(7)
“Ex„p[h(fpxqqas+——1- f Ppxqhpfpxqqadx
α ´ 1 JX
´ o´ɪ JX ppxq ^ PXq)	hpf pxqqadx
“Ex„P[h(f pxqqas + Ex„P[h(f pxqqsao´ɪ L ^E*f qqsyppxqFx
´Ex„P[h(fpxqqsaɪ f ^ Ppxqhf(xqq )aqpxq-adx
α ´ 1 JX \Ex„P[h(f pxqqs√
“Ex„P[hpfpxqqas
+Ex„p [h(f pxqqsa
´ Ex„p [h(f pxqqsa
1
ɑ ´ 1
1
ɑ ´ 1
exp (α ´ 1q
exp (α ´ 1q
1
ɑ ´ 1
1
ɑ ´ 1
log
log
f p Ppxqhpfpxqq ʌ
JX [Ex~p [h(f Pxqqs)
f ( Ppxqhpfpxqq )
JX [Eχ~p [h(f Pxqqs)
a
p(xq1-adx
a
q(xq1-adx
)
*
“Ex„p[h(f Pxqqas + Ex„P[hfpxqqsa (e(aT)Da(IS}P)´e(a´1)Da(IS}Q)),
Where line (7) derived from Lemma A.1. The second inequality Was provided in Proposition 6
of (Ghosh et al., 2020).
□
Theorem 5.3. Let PP PpXq, fPBpX, [0, 8qq, and h : [0, 8q → [0, 8q monotonic strictly-
increasing. Let Q 三 P (Xq and suppose that h o f is bounded from above, then, the iterate
Qk'i P argminQPQ {Da(Ihof [Qks}Qq} (where possible ties are broken arbitrarily) satisfies:
15
Under review as a conference paper at ICLR 2022
(i)	the Sequence of divergences DapIhof [Qk ]}Qk) is convergent;
(ii)	the sequence of expectations Ex„Qk rhpf pxqqα s is non-decreasing in kPN and converges to
a stationary point of Ex„Q rhpf px))α s w.r.t. Q P Q.
Proof. Let us consider the sequence of distributions pQk )kPN, generated by the iterate in Equa-
tion (5), where possible ties are broken with an arbitrary (possibly with a tie-breaking rule Tk dif-
ferent for every k). From Theorem 5.2, we have for every kPN:
Eχ~Qk'i[h(f(x))αS- Eχ~Qjh(f(x))a]
) Eχ~Qjh(f(x))]a 卜(a —1)Da(IhOfrQk ]}Qk) ´ e(a — 1)Da(IhOfrQk ]}Qk'l))20
where we simply exploited thatQkPargminQPQ tDα(IhofrQks}Q)u. Thus, Ex„Qk rh(f(x))αs is
a non-decreasing function of k. Since h o f is bounded, it must be that limk—gEx„Qk rh(f(x))as “
μg < 8, that proves convergence.5
Furthermore, being convergent, for k →8 it must be that Ex„Qfc [h(f (x))ɑ S “ Ex〜q2 [h(f (x))a]
and consequently D a (Ihof rQk ]}Qk) “ DapIhOfrQk ]}Qk+ι). Therefore, even if the tie-braking
rule prescribes to select Q—i ‰ Qk We could select Qk instead, since it lead to the same divergence
value. Consequently, being Qk a solution, we can assert that it is a stationary point of the function
Da(Ihof [Qk]}∙) (as well as Qk+i):
0 “ JSDapIhOfgk s} Q)IQ = Qk
“(a T)e(aT)Da(Ihof[QkS}Q)Eχ~Qkrh(f(χ))SVq()JX h(f (X))“ qk(x)aq(X)I—adx|Q=Qk


e(a´1)Da(Ihof rQkS}Q)Ex„Qk [h(f (χ))S JX h(f (x))aqk(x)aq(X)—adx|Q=Qk
epaT)Da(IhofrQks}Q)Ex~Qjh(f(x))S JX h(f (X))adχ.
We observe that the latter expression is zero if and only if the gradient of Ex„Q rh(f(x))as w.r.t. Q
is zero. Indeed:
Vq(.)Ex~Q [h(f (x))as = JX h(f (x))adx.
Thus, the process converges to a stationary point of Ex„Qk rh(f(x))as.
□
Theorem 5.4. Let fPB(X, r0, 8)), and h: r0, 8) → r0, 8) monotonic strictly-increasing. Let
Q 三 P (X) be a (1 — a) -convex set (van Erven & Harremoes, 2014, Definition 4), P P Q, Q: P
arg minQPQ tDa (Ihof rP s}Q)u, and αP r0, 8s, then it holds that:
Da (QlP) ≤ Da (IhOfrPsiP)´ Da (IhOfrP] >Q：).
Proof. The proof is a simple application of Lemma A.2, by taking Q DP, Q* D Qt, and PD
Ihof rP s.
A.3 Proofs of Section 6
Theorem 6.1. Let Qξ / P (X) be a set of parametric distributions and let ξ, ξ P Ξ. If }h o f }g ≤
m, then, ifsamples are independent, for every δ P [O, 1s, with probability at least 1 — δ it holds that:
Ex„ _(qH⅛ h(f (X))) _ & dpa (IhOfrQξis}Qξ ^i,j)+ma {^j L 院(Xqq ^磨-° dx.
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon
y	-
(Obj)
5Notice that the improvement holds also for a V1. Indeed, while it is true that %„呢似；pxqqs V 0, but in
such a case function epa_1qp-q is decreasing in its argument.
16
Under review as a conference paper at ICLR 2022
Proof. We start observing that each addendum of dα pih0f rQ iS}Q [Φi,j) is non negative. Since
all terms are i.i.d., we can apply unilateral Bernstein’s inequality (Maurer et al., 2003) that allows
achieving an exponential concentration. Thus, for every δP r0, 1s, with probability at least 1 ´ δ it
holds that:
α
[(
qξipxq
qξpx)
hpfpx))
Φi,j)
+ {2Varxi~φi,j ”da (IhOfrQξis}Qξ ",jq] log 5.
Thus, it remains to provide a bound on the variance term. We exploit the fact that h(f (x)) ≤ m and
that each addendum represents an i.i.d. random variable:
Varx
i〜φi,j Ipa (IhOfrQξiS}Qξ2j)]≤
τnj)2 Σ Σ Exk,ι~φi,j	i
nj	kPrjs lPrns
m^2a
Wpnjq2 ∑ Ex Exk,ι~φi,j
nj	kPrjs lPrns
^ j⅛ξ‰h …
(_qξi (χk,ι )a	Y
φi,j (χk,ι)qξ(χk,ι)aτ J
了
m2α
qξi(x)a
nj
Φi,j (χ)qξ(χ)ατ
A.4 Technical Lemmas
Lemma A.1. For every X20 and a p(0, 1)y (1,8), it holds that:
x ´1N a´i ^1 ´ I).
Furthermore, for α “ 1, it holds that:
x ´ 1 N log x.
Proof. Consider the auxiliary function gα(χ) “ X — 1 — a´ɪ(1 ´ χ⅛). We are going to prove that
the minimum of ga(x) is zero. Suppose α > 1, then ga (0) “ 8 and g。(8)= 8. Thus, the minimum
must lie in betWeen and since function ga is differentiable, We have:
B
——gα(x) “ 1 ´ X = 0	=^ X = 1.
Bx
Thus, We have gα(1) “0. Suppose now that α< 1, We have gα(0) “ ɪ´a >。and ga(8) = 8. Thus,
again, the minimum must lie in between and with the same calculations as before, we conclude
ga(1) “0. The case a“ 1 is trivial.
Lemma A.2. Let PP P(X) and let a p(0, 8). Let Q 三 P(X) be an (α — 1)-convex (van Erven &
Harremoes, 2014, Definition 4) subset Ofdistributions. Let Q* P Q be the α-momentprojection:
Q*“argmintDa(P}Q)u.
QPQ
If Q* exists, thenfor every Q P Q ifholds that:
Da(P }Q)》Da(P }Q*)+ Da(Q*}Q)∙
□
□
17
Under review as a conference paper at ICLR 2022
Proof. The proof of the result is inspired to (Van Erven & Harremoes, 2014, Theorem 14). Let
λP r0, 1s and let us define Qλ as the (1 一 α, (1 一 λ, λqq-mixture of Q* and Q:
qλ(x) = Z;((1 - λ)q* (x)i ' λq(x)-α) 11α ,
Zλ = ' '(1 一 λ)q*(x)1r ' λq(x)-a) 士 dx.
X
Let us first observe that for λ = 0, we have Qo “ Q* and Zo “，久 q*(x)dx = 1. Since Q is (1 一
αq-convex and Q* is the minimizer over Q, it holds that 昙Da(P}Qλ)∣λ=o20. First of all, we
compute:
f P(Xqaqλ(x)-αdx = za´1 f “(1 一 Xqp(x)aq* (x'q1´a ' λp(x)aq(x)1-a‰ dx
XX
ɪ Zλ = ɪ f ((1 - λ)q*(x)i + λq(x)-a)* 'q(x)1-a — q*(x)1-a) dx.
B λ 1 一 α X
The latter, for λ = 0, becomes: BλZλ∣	= ɪ´a [1xq*(x)aq(x)1—a —1‰. For calculation easiness,
instead of directly operating on Da(P}Qλq, we consider:
-B / p(x)αqλ(x)idx = Za´1 [ “—p(x)aq*(x)1-a + p(x)aq(x)i‰ dx,
XX
+ (α一 1)Za.ɪZλ [ “(1 一λ)p(x)aq*(x)-a + λp(x)aq(x)-a‰ dx.
Bλ X
We now evaluate it at λ “ 0:
ɪ f p(x)αqλ(x)1-αdx
Bλ X
∣ “一J p(x)αq*(x)1-αdx'ʃ P(Xqaq(x)-αdx
一f p(x)αq*(x)1-αdx
X
f q*(x)αq(x)-αdx´ 1 .
X
For a21, We require 昙,χp(x)αqλ(x'q1´αdx∣	>0, to obtain:
f p(x)αq(x)1-αdx》f p(x)αq*(x)1-αdx f q*(x)αq(x)-αdx.
X	XX
By applying both sides the log function and dividing by o´ɪ > 0 We get the result. Symmetrically,
for α V1, we require the converse Bλ ,χp(χ)ɑqλ(χ)1-αdχ∣	≤ 0. Recalling that o´ɪ V0, We
λ=0
obtain the desired result.	□
B	OPTIMIZING MOMENTS OF f
In this appendix, we analyze the effect of optimizing a power of f instead of f .
LemmaB.1. Let PP P(X) and f PB(X,[m,m]. If ap (1,8), it holds that:
0 ≤ Ex„p f(x)as—(Ex„p rf(x)s)a
ma(m 一 Ex„p rf (x)S) + ma(Eχ~p f (x)]— m) — Ex„p f (x)]a (m 一 m)
&	ZZ
m 一 m
In particular for α = 2, we have:
0 ≤ Ex„p “f (x)2‰ tex~p rf (x)s)2 ≤ (m 一 Ex„p rf (x)sq (Ex„p f (诩一 m),
that is the Bhatia-Davis inequality for the variance.
18
Under review as a conference paper at ICLR 2022
Proof. We explicitly consider the optimization problem, for α21 and having denoted μ “
Ex„Prfpxqs:
max
f: X →R
ppxqf pxq
X
αdx
s.t.	p(x)f (x) “μ
X
m ≤ f (x) ≤ m.
Since α21, the optimization problem corresponds to the maximization of a concave function subject
to linear and box constraints. It is simple to prove that the optimal solution must assign extreme
values to function f. Let pP r0, 1s, the linear and box constraints enforce:
ʌ—	m — μ
Pm `pi — p)m = μ	=^ P = ---------
—	m — m
From which, by substitution in the objective function, we have:
p(x)f (x)αdx“Pma ' pi — p)mα “
X	一
mα pm — μq ' mα(μ — m)
m — m
□
Thus, in general, optimizing moments of the function f, leads to different optimal policies compared
to optimizing function f directly. However, from the above results, we see that this discrepancy
reduces when the expectation Eχ~p [f (χ)] approaches the extreme value m (and also m, but this
is less interesting since We are maximizing). The value m can be indeed achieved if We have no
restrictions on the distribution space (Section 4).
C	Performance Improvement for the MDP Setting
As already mentioned in Section 5, When We consider the MDP setting, We do not have the full
control of the trajectory distribution P(T∣θ) = D0(s0) 口匕1 ∏θ(at∣st)P(st+ι∣st,at) as the factors
involving the transition model P and the initial state distribution D0 are out of the control of the
policy and the policy itself πθ is limited due to the parametrization. As a consequence, performing
a step of optimization of the α-moment is able to provide improvement guarantees on J (θ) only
by choosing the transformation function h = (∙) 1 (Theorem 5.2). In this appendix, we prove that
under specific conditions We are able to provide guarantees on the improvement of J (θ). Moreover,
we show some approaches, with a main theoretical interest, to extend the performance improve-
ment guarantees to the case of stochastic MDPs. They must not to be intended as implementation
proposals, but rather as theoretical approaches to cope with this phenomenon.
C.1 Action-based PO
We start with the action-based PO setting.
Deterministic MDPs If the MDP is deterministic (i.e., P and D0 are deterministic), we denote
the next state as st`1 =P(st, at). Thus, the trajectory distribution is governed by the policy πθ only:
p(τ∣θ) = ∏H01 ∏θ(at∣st). In such a case, provided that the policy space Θ is sufficiently powerful,
by minimizing any of the α-moments, we are able to guarantee the performance improvement.
Indeed, in such a case, neglecting the limits of the parametrization Θ, we are in an unconstrained
setting.
Stochastic MDPs For general stochastic MDPs, the trajectory density function depends on the
transition model probabilities. Thus, we need to design an estimator that get rids of these elements.
To this purpose, we denote with p(α∣θ) as the probability of having observed a sequence of actions
a = (ao,aι,..., aH-ι) when playing policy ∏θ in the MDP. If we take f = E [R(τ )|a], i.e., the ex-
pectation of the return conditioned to the sequence of actions, we are again in an unconstrained
19
Under review as a conference paper at ICLR 2022
setting, neglecting the limits of Θ. Therefore, we are able to guarantee the performance improve-
ment on J(θ). Nevertheless, this requires to design an estimator of E [R(τ)[a], that might be a not
easy task.
C.2 Parameter-based PO
We now consider the parameter-based PO setting.
Deterministic MDPs and Policies In the case of deterministic MDPs and deterministic policy πθ,
we have that for every θPΘ, the support of the trajectory distribution is made of one trajectory only.
In such a case, ignoring the limits of the hyperpolicy parameter space P, we are in an unconstrained
setting.
Stochastic MDPs or Policies In the case of a stochastic MDP or a stochastic policy (or both),
we have a trajectory distribution in which we do not have the possibility to intervene on the pol-
icy and transition model factors. Thus, we need to consider an estimators that get rid of these
stochastic elements. If We take f “ E[R∣θ] “ J(θ), i.e., the expected return conditioned to a policy
parametrization θ, we reduce to the unconstrained setting, with the corresponding performance im-
provement guarantee. It is Worth noting that, compared to the action-based setting, estimating Jpθq
is notably simpler compared to the estimation of E [R(τ)|a].
D Closed Form of the Integral for Gaussians
In this appendix, We derive a closed form for the integral involved in the computation of the bound of
Theorem 6.1 in the case that all involved distributions are Gaussians and for α “ 2. Let us introduce
the notation:
μ “ N (μμ, Σμ),	φ “ N (μφ, ∑φ),	V “ N (从”,∑ν).	(8)
We have to compute the folloWing integral:
JX Φ(XqVχX)2 dχ.
Let us start elaborating on the integrand function, denoting for properly sized vector x and matrix
S, }m}s “xτSx and |S| the determinant of S:
μ4(χ)	_	(2n)—2k|£“「2exp (—2}X-"μ}∑"τ)
φ(xqν(xq2 — (2π)i勺Σφf2 exp (—1{2}X ´ μφ}∑φJ (2π)i ∣Σ-∣τ exp (TX ´ 〃“ }∑ν-)
“(舅_；国_1 eχp (—2}x ´ 〃“性”-1 + 1{2}x ´ NΦ}Σφ_ + }x ´ μν}∑ν-).
NoW, We have to deal With the argument of the exponential:
´2}X — μμ}Σμ T + 1{2}x — μφ }Σφ ´1 +}x — μν }∑ν ´1
“ —1 xτ(4ΣμT- ΣφT- 2ΣνT) X + (4ΣμTμμ ´ ΣφTμφ ´ 2ΣνTμν)T X
2 loooooooooooooooooooooooooooooooon loooooooooooooooooooooooooooooooooooooooooooooooon
M	bT
—2 (4μμT 'μ	μμ 一 μφ 'φ	μφ — 2μν, 'ν	μν ) .
V	X
c
We noW proceed completing the square:
XT Mx ´ 2bτ X =(x ´ MTb)T M(x ´ MTb)— bτ M』.
Thus, We have:
—1 (xT Mx ´ 2bT X + C) =— 1 (x ´ MTb)T M(x ´ M』) +1 bT MTb ´
1
2 c.
20
Under review as a conference paper at ICLR 2022
Moreover, we observe that the following expression is the density of a k-variate normal distribution
with mean M—1b and covariance matrix M ´1:
(2π)T∕2M—1|—1/2 exp (´ 1 (X´ MTX)TM(X´ M—1b)}
Thus, its integral is 1. Therefore, coming to the initial expression:
f	μ4(χ)	dχ .(2∏)τ/2∣∑μF
JX φ(x)ν(x)2	^∣Σφ∣T/2∣∑ν∣T
((2n)—k/2|M—111/2)—1 exp(1 bT MTb ´ 1 C)
l∑Φ∣1/2l∑ν∣
∣∑μ ∣2∣M∣1{2
E Experimental Details
In this appendix, we report the experimental details and additional experimental results.
Infrastructure The experiments have been run on two machines:
•	2 x CPUs Intel(R) Xeon(R) CPU E7-8880 v4 @ 2.20GHz (22 cores, 44 thread, 55 MB
cache) and 128 GB RAM;
•	4 x Intel(R) Xeon(R) CPU E5-4610 v2 @ 2.30GHz (8 cores, 16 thread, 16MB cache) and
256 GB RAM.
Environments The environments are the rllab implementations (Duan et al., 2016), MIT li-
cense, https://github.com/rll/rllab. The Swimmer environment belongs to the Mu-
joco suite (Todorov et al., 2012), MuJoCo Personal License, http://www.mujoco.org/.
Algorithms The TRPO implementation is taken from baselines (Dhariwal et al., 2017), MIT li-
cence, https://github.com/openai/baselines. For POIS we use the original imple-
mentation (Metelli et al., 2018), MIT license, https://github.com/T3p/baselines.
Hyperparameters In order to properly compare the algorithms, a set of20 seeds has been chosen.
A subset of 5 seeds, underlined, was used to test the performances during the tuning phase. Once
the optimal hyperparameters were found, the experiments were extended to the other 15 seeds. In
the following, we report the hyperparameter values for PO2PE.
The shift return refers to the need for making the return non-negative in order to perform the opti-
mization of the α-moment in PO2PE. This procedure is carried out independently at each algorithm
iteration by subtracting the minimum return among the ones observed. The variance init hyperpa-
rameter refers to the logarithm of the standard deviation. All experiments have been carried out with
Gaussian policies linear with mean linear in the state variables and constant variance uniform over
the state space.
Cartpole
•	seeds: 0, 3,11,16, 19, 42, 66, 72, 84, 87, 90,123, 222, 343, 404,452, 542, 875, 943, 999
•	max iters: 500
•	policy: linear
•	policy init: zeros
•	capacity: 1
•	inner: 1
•	variance init: -1
•	step size: 1 / gradient norm
•	penalization: True
•	delta: 0.75
21
Under review as a conference paper at ICLR 2022
•	max offline iters: 10
MoUntain Car
•	seeds: 0, 3,11,16, 19, 42, 66, 72, 84, 87, 90,123, 222, 343, 404,452, 542, 875, 943, 999
•	max iters: 500
•	policy: linear
•	policy init: zeros
•	capacity: 1
•	inner: 1
•	variance init: -1
•	step size: 2 / gradient norm
•	penalization: TrUe
•	delta: 0.9
•	max offline iters: 10
•	shift retUrn: TrUe
Inverted DoUble PendUlUm
•	seeds: 0, 3,11,16, 19, 42, 66, 72, 84, 87, 90,123, 222, 343, 404,452, 542, 875, 943, 999
•	max iters: 500
•	policy: linear
•	policy init: zeros
•	capacity: 1
•	inner: 1
•	variance init: -1
•	step size: 2 / gradient norm
•	penalization: TrUe
•	delta: 0.99
•	max offline iters: 10
SWimmer
•	seeds: 0, 3, 11, 16, 19, 42, 66, 72, 84, 87, 90, 123, 222, 343, 404,452, 542, 875, 943, 999
•	max iters: 500
•	policy: linear
•	policy init: zeros
•	capacity: 1
•	inner: 1
•	log-std init: -0.6
•	step size: 1 / gradient norm
•	penalization: TrUe
•	delta: 0.99
•	max offline iters: 10
•	shift retUrn: TrUe
22
Under review as a conference paper at ICLR 2022
nruter egarev
5000
4000
3000
2000
1000
0
100
600
300
200
500
400
σ=1
0 0.5 1 1.5 2
×104
Episodes
PB-POIS - j PB-PO2PE
Figure 5: Learning curves comparing PB-POIS and PB-PO2PE with increasing magnitude of the
noise (20 runs, 95% c.i.).
For POIS (both AB and PB) and TRPO, the same hyperparameter value have been used, except
for the algorithm-specific ones that have been tuned with the same protocol discussed above (δKL P
t0.001, 0.01, 0.1, 1u). In particular, for POIS, we employ the line search procedure presented in
the original paper for setting the step-size. The following table summarizes the algorithm-specific
hyperparameter values for the different algorithms and environments.
Environment / Algorithm	PO2PE (delta)	AB-POIS (delta)	TRPO (max kl)
Cartpole	0.75	0.4	0.01
Mountain Car	0.9	0.9	0.01
Inverted Double Pendulum	0.99	0.1	0.001
Swimmer	0.99	0.8	0.01
Environment / Algorithm	PB-POIS (delta)	PB-PO2PE (delta)
Cartpole	0.4	0.6
Mountain Car	1	0.00001
Inverted Double Pendulum	0.1	0.999999
Swimmer	0.4	0.4
E.1 Noise Robustness
As we have shown in Appendix C, using the trajectory return Rpτq as function f does no longer
allow to provide performance improvement guarantees. Nevertheless, we conjecture that the loss
of this property is compensated by the variance reduction implicit in our approach. In the direction
of empirically showing this aspect, we tested the parameter-based version of PO2PE in the Inverted
Double Pendulum environment, with forced stochasticity in the environment. Specifically, whenever
an action is prescribed by the policy the actual action to be executed is obtained by adding while
Gaussian noise with standard deviation σ. The results are shown in Figure 5. We observe that our
algorithm is overall competitive with PB-POIS and, in the case of σ = 1, significantly outperforms
PB-POIS.
E.2 About Return Translation
Our approach can be employed for non-negative functions f . Since in the PO experimental
evaluation we employ f “ Rpτ q. Under the assumption that the immediate reward is bounded
R(s,a)p [Rmin, RmaXs for all (s,a)p S X A, we can make the return function with a simple transla-
tion and preserving the optimality of policies:
1 ´ YH
R(T )= R(T )´ Rmin ~γ^
1 — γ
loooooooooooon
y -
´Cmin
23
Under review as a conference paper at ICLR 2022
where Rmin 1二 is the minimum achievable return. Of course, We can perform the translation
even by using a constant CeCmin = -Rmin⅛YH and still obtain a translated return that remains
positive. It is worth noting, from Theorem 4.3 that the size of the trust region is larger as the
constant approaches the its minimum possible value.
For instance, we consider α “ 2, f20 , and we apply a further translation with C20. From Theo-
rem 4.3, we have:
D2(I'm[p]}p q= log
Ex„p [(f(x) + c)2s
Ex„p [f(x) + cS2
I	Ex„p [f(x)2S + c2 + 2cEx~p [f(x)S
“ og Ex„p [f(x)S2+ c2 + 2cEx~p [f(x)S.
Since Ex„Pf (x)2]》Ex„Pf (x)S2, we have that this expression is maximized with the smallest
value of C, i.e., C“0.
24