Under review as a conference paper at ICLR 2022
Differentially Private SGD with Sparse Gra-
DIENTS
Anonymous authors
Paper under double-blind review
Ab stract
To protect sensitive training data, differentially private stochastic gradient descent
(DP-SGD) has been adopted in deep learning to provide rigorously defined pri-
vacy. However, DP-SGD requires the injection of an amount of noise that scales
with the number of gradient dimensions, resulting in large performance drops
compared to non-private training. In this work, we propose random freeze which
randomly freezes a progressively increasing subset of parameters and results in
sparse gradient updates while maintaining or increasing accuracy. We theoreti-
cally prove the convergence of random freeze and find that random freeze exhibits
a signal loss and perturbation moderation trade-off in DP-SGD. Applying ran-
dom freeze across various DP-SGD frameworks, we maintain accuracy within
the same number of iterations while achieving up to 70% representation sparsity,
which demonstrates that the trade-off exists in a variety of DP-SGD methods.
We further note that random freeze significantly improves accuracy, in particular
for large networks. Additionally, axis-aligned sparsity induced by random freeze
leads to various advantages for projected DP-SGD or federated learning in terms
of computational cost, memory footprint and communication overhead.
1	Introduction
The success of machine learning, and deep neural networks in particular, combined with ubiqui-
tous edge computation and digital record keeping, has led to a surge in privacy sensitive learning
applications. Internet-scale data promises to accelerate the development of data-driven statistical
approaches, but the need for privacy constrains the amalgamation of such datasets. Private data are
in fact isolated, constraining our ability to build models that learn from a large number of instances.
On the other hand, the information contained in locally stored data can also be exposed through
releasing the model trained on a local dataset (Fredrikson et al., 2015; Shokri et al., 2017), or even
reconstructed when gradients generated during training are shared (Zhu et al., 2019; Geiping et al.,
2020; Zhu & Blaschko, 2021).
To address these issues, many applications of machine learning are expected to be privacy-
preserving. While differential privacy (DP) provides a rigorously defined and measurable privacy
guarantee for database operations (Dwork & Roth, 2014), it also contains intriguing properties, such
as robustness to post-processing and composability, which enables conveniently computing an over-
all privacy guarantee for several DP components. Differential privacy1 defines privacy with respect
to the difficulty of distinguishing the outputs. For a pair of neighboring databases X, X0 ∈ X, i.e.
X can be obtained from X0 by adding or removing an element.
Definition 1. A randomized mechanism M : X → R is (ε, δ)-differentially private, if for any
subset of outputs S ⊆ R it holds that:
Pr[M(X) ∈ S] ≤ eε Pr[M(X0) ∈ S]+δ.
A common paradigm for a randomized mechanism M in deep learning is perturbed gradient descent:
M(X) := f(X)+N(0,Sf2σ2I),	(1)
1In this work we only consider approximate differential privacy which includes the δ term.
1
Under review as a conference paper at ICLR 2022
where f : X → R computes an aggregated gradient given a database X or X0 . The isotropic Gaus-
Sian distributed noise ξ0p 〜N(0, Sf σ2I) is calibrated to f's sensitivity Sf, which is the maximal
`2 distance kf(X) - f(X0)k, i.e. the maximal `2 norm of gradient among all individual exam-
ples. X, X0 could be batches of training data, for instance, in our experiments they are batches of
image-label pairs. The factor σ is a noise multiplier controlling the strength of the privacy guarantee:
higher σ leads to lower privacy loss. Differentially private stochastic gradient descent (DP-SGD) up-
per bounds the certainty of connecting data with arbitrary subset of gradient space using the privacy
budget variables (ε, δ).
Bassily et al. (2014) show that in a convex setting, DP-SGD achieves excess risk of O(√d∕nε) for a
model w ∈ Rd that minimizes the empirical risk Pin=1 `(w, xi), where x1, x2, ..., xn are drawn from
X . While we show that in non-convex general setting, the mean square error (MSE) of perturbed
gradient g = g + ξ0p is between Ω(d) and Ω(d2) by assuming the gradients follow a Gaussian
distribution:
Theorem 1. Assuming that the gradient is drawn from N(Vw, Σ), centered at the true gradient
Vw and with respect to the covariance matrix Σ whose trace goes linearly up with dimension d.
The MSE ofperturbed gradient g = g + ξ0p can be lower bounded by:
MSE ≥ Tr[Σ](1 + dσ2).	(2)
from which we conclude that the lower bound on MSE is between linear and quadratic in d in
practice. For the proof and conclusion, refer to Appendix A. In terms of deep learning, as d is a
large number for modern network architectures, this can lead to a significant increase in error.
The work of Abadi et al. (2016) proposed to clip the gradient of each individual example in `2 norm
to a preset bound C, i.e. g = g ∙ min(1, ^^). They then apply this clipping bound to compute the
variance of Gaussian distributed noise. The Gaussian noise mechanism can be expressed as:
M(D)= f (D)+ N(0,C2 ∙ σ2Id).	⑶
DP-SGD with gradient clipping has been empirically verified to be effective as it constraints the
amount of injected noise by setting a small clipping bound. However, clipping removes the mag-
nitude information of the gradient and therefore results in gradient estimation bias. Setting a small
clipping bound with a deeper network will not result in better performance. The expected MSE of
the perturbed gradient is only constrained to O(d), the impact of perturbation is non-negligible in
practice. The biggest network where this strategy is successfully applied so far is a CNN with Tanh
proposed by Papernot et al. (2021), which reaches 〜66% accuracy on CIFAR10 in a low privacy
regime and is regarded as the state-of-the-art (SOTA) end-to-end network with DP-SGD. To address
this curse of dimensionality, most recent works concentrate on gradient dimension reduction.
1.1	Related works
Abadi et al. (2016) propose to pretrain a network on an auxiliary dataset and then transfer the feature
extraction, so that only a linear classifier will be replaced and trained on the private data. Tramer
& Boneh (2021) adopt ScatterNet (Oyallon et al., 2019) to extract handcrafted features and train a
relatively shallow network based on the features. Both work decrease d by excluding the majority
of parameters during DP learning, which also constrains the learning ability of network.
Inspired by the empirical observation that the optimization trajectory is contained in lower-
dimensional subspace (Vogels et al., 2019; Gooneratne et al., 2020; Li et al., 2020), a line of work
intend to reduce d by exploiting the low-rank property of the gradient while considering privacy.
Several recent works (Zhou et al., 2021; Yu et al., 2021a; Kairouz et al., 2021) project the gradient
into a subspace which is identified by auxiliary data or released historical gradients. In practice,
they use the power method to search for the subspace. The computational cost of running the power
method and projecting gradients as well as the memory footprint of storing the projection matrix
limits the application of such method to large models. Zhang et al. (2021) target an NLP task where
networks are heavily over-parameterized and gradients are extremely sparse, and propose to adopt
DP selection to privately select top-k significant gradients for optimization. Also targeting an NLP
task, Yu et al. (2021b) propose a low-rank reparameterization of weights via released historical
gradients.
2
Under review as a conference paper at ICLR 2022
In addition to the aforementioned works, McMahan et al. (2017); Yang et al. (2019) and others
study how to incorporate differential privacy in collaborative training, e.g. federated learning, in the
interest of protecting the privacy of participants. Federated learning also suffers from large d, as it is
usually deployed on edge devices and local models are periodically synchronized, so communication
cost becomes expensive both in time and power usage (Pathak et al., 2012). Therefore, gradient
dimension reduction can have large benefits in federated learning involving power-restricted edge
devices. A line of work studies how to tackle this issue by utilizing the low-rank property (Shokri &
Shmatikov, 2015; Yang et al., 2019; Liu et al., 2020).
1.2	Our contribution
In this work, we demonstrate an axis-aligned gradient dimension reduction method. Our work is
orthogonal to previous works, we do not extract any characteristic information of the gradient or
model, or approximate the gradient from a subspace. Instead, we randomly zero-out a fraction of
the gradient during training and force the gradient to have a sparse representation. We provide a
theoretical study on this strategy and reveal that random freeze exhibits a trade-off between signal
loss and perturbation moderation in DP-SGD. We remark that our theory and approach do not nec-
essarily rely on the low-rank assumption. To the best of our knowledge, we are the first to study this
trade-off in DP-SGD and provide an effective approach.
We use the benchmark CIFAR10 (Krizhevsky, 2012) which is to date standard in benchmarking
DP learning and show that well implemented random freeze exhibits various advantages and can
be widely applied. More specifically, we maintain accuracy when we adapt projected DP-SGD
with random freeze, while we reduce the computational cost and memory footprint induced by the
power method and projection. Applying it to various frameworks, we achieve a high representation
sparsity of gradient without a loss in performance. Federated learning can take advantage of the
resulting sparse representation to reduce communication costs. We further note that the random
freeze strategy improves the accuracy of large networks, which we demonstrate with the SOTA
End-to-end CNN proposed by Papernot et al. (2021).
2	Analysis of Random Freeze
In this section we theoretically prove that to a certain freeze rate r, DP-SGD with random freeze
will converge if the approach without random freeze can converge. Then we investigate the trade-
off between signal loss and perturbation moderation induced by random freeze. Furthermore, we
empirically demonstrate benefits of applying random freeze from the perspective of the gradient
distribution.
2.1	Convergence rate of DP-SGD with random freeze
Let L be the objective function L(W) := Ex〜X['(w, x)], m ∈ {0,1}d the freeze mask and r the
freeze rate, we randomly draw rp indices and set these positions in the mask to 0 and others to
1 so that P m =(1 一 r)d. We assume an oracle telling Us the true gradient Vw and individual
gradient gt,i = Vw + ξt(χi), where ξt is independent gradient deviation with zero mean. Let Pt
be the distribution of ξt. For random freeze the sparse gradient is gt0,i = Vwt0 + ξt0 (xi), where
Vwt0 := m Vwt, ξt0 (xi) := m ξt (xi). Denote for DP-SGD the averaged clipped gradient of
B samples gt := B Pi gt,i ∙ min(1, ^£-k), and for DP-SGD with random freeze gt := B Pi g[% ∙
min(1, kgC k). We have for DP-SGD with random freeze the following inequality:
Theorem 2. Assume G-Lipschitz smoothness of Vw such that ||Vwt+1 一 Vwt|| ≤ G||wt+1 一 wt ||.
Consider an algorithm with clipping bound C, learning rate γ and choose a symmetric probability
density distributionp(∙) satisfyingpt(ξt) = Pt(-ξt), ∀ξt ∈ Rd. Then ∃ K ≥ 1 — r such that:
TT
T X Pξt 〜Pt (kξtk < Z)h(Vwt)kvwtk ≤ K ( ~T + γ δC + (1 - r)γ δDP - T X Em [bt]), (4)
t=1	γ	t=1
3
Under review as a conference paper at ICLR 2022
where we define bt := JEwt,g1,『 min(1, ɪg^)i(pt(ξt) — Pt(ξt))dξt, Ptt, Pt are corresponding
projected distribution, and define Δl := E[Lι — min. L(w)], ∆c := GC2, Δdp :二。靠宁,
h(ywt) := min(∣Vwtk, 3CC)∙ The proof can be found in Appendix B.
When no freeze r = 0, i.e. m is a vector filled with value 1. Note that κ = 1 for r = 0, we can
obtain the inequality 4 in the following form:
TT
T X Pξt〜Pt(kξtk	< ɪ)h(vwt)kvwtk	≤ -T	+ YδC +	δDP) - τ X bt,	⑸
T t=1	4	γT	T t=1
which describes DP-SGD without random freeze. Chen et al.(2020b) argue that by tweaking Pt it is
possible to bound Pξt〜Pt away from zero which means the l.h.s. is proportional to ∣∣Vwtk or ∣∣Vwtk2
at each iteration, while letting the convergence bias term -bt tend to be small as pt is approximately
symmetric. They then prove that by setting a certain learning rate γ, the r.h.s. diminishes to zero, so
the network under DP-SGD can converge. Adapted from that, we see at the r.h.s. of inequality 4 for
random freeze, -Em [bt] also tends to be small as p0t is expected to be approximately symmetric if
Pt is. Additionally, 1∕κ will not be large as along as r is not extremely close to 1. So with the same
learning rate and γ adapted for DP-SGD without random freeze, the r.h.s. of inequality 4 will also
tend to zero, which proves the convergence of DP-SGD with random freeze.
(a) ∆S
Figure 1: (a): Signal loss of no freeze minus random freeze. (b): MSE of gradient estimation of no
freeze minus random freeze.
2.2	TRADE-OFF B ETWEEN SIGNAL LOSS AND PERTURBATION MODERATION
From Theorem 2, we see that by applying random freeze the injected noise term ΔDP is reduced
by 1-r < 1 while other terms could be increased by 1∕κ. From an operational view, random
freeze moderates perturbation while removing some signal. Therefore, the convergence rate of
random freeze is a trade-off between signal loss and perturbation moderation. Consider that ΔDP
is proportional to the number of parameters d, which is large for neural networks. It might be
worthwhile to remove some signal while moderating perturbation. To investigate this question, we
consider the MSE of gradient estimation of DP-SGD with and without random freeze at iteration t:
MSEDP :=	E[kVwt	—	(g	+ ξDP) k2]	=	E[kVwt	— gk2] +-奇—,	⑹
B2
MSERF := E[∣Vwt - (g + m Θ WDP)k2] = E[∣Vwt - g∣2] + (1 - r)C⅛2d.	⑺
B2
Two statistics of interest are: First, the difference of signal loss ΔS :=E[∣Vwt -训2] - E[∣Vwt -
g∣2]. DP-SGD induces signal noise due to clipping bias while random freeze has additional loss
due to freezing. Second, the difference of MSE ΔMSE := M SEDP - MSERF which implies
the trade-off between signal loss and perturbation moderation. We train the End-to-end CNN with
80 epochs on a privacy budget (ε = 7.53, δ = 10-5) and measure these two statistics with respect
to empirical distribution at the end of three training stages (see Figure 1). The result shows that
although random freeze has more signal loss as the freeze rate increasing, due to a large amount of
injected noise, MSE of gradient estimation with random freeze is clearly lower. The result reflects
4
Under review as a conference paper at ICLR 2022
that injected noise is dominant during optimization. Combining this with Theorem 2, it shows the
possibility that we achieve better convergence by removing some signal while mitigating noise. For
example, in addition to taking advantage of noise mitigation, we can also raise the noise to ξD+P so
that E[km ξD+P k] matches E[kξDP k]. This allows us to run more iterations T on the same privacy
budget, which also implies better convergence by Theorem 2. In section 4 we will demonstrate that
when d is large, which implies large ∆DP, we can improve the performance by applying random
freeze.
2.3	A gradient distribution view of random freeze
We also observe the potential benefit of random freeze from the perspective of the gradient distri-
bution. Compared to no freeze, random freeze leads to less clipping bias and gradient distortion, as
shown in Figure 2. We adopt the same clipping bound for random freeze and no freeze. As fewer
dimensions contribute to the norm computation, random freeze reduces the clipping probability and
therefore alleviates clipping bias (Zhang et al., 2020). We also note that the norm of sparse gradi-
ents are not equally scaled down, weak gradients can spontaneously become larger during training,
which mitigates the distortion of gradients due to perturbation, while the perturbation of random
freeze is already moderated compared to no freeze.
Figure 2: Gradient norm distribution of DP-SGD with or without the random freeze strategy. The
vertical dashed line indicates the clipping bound. With the random freeze strategy, in later epochs
the variance of the norm magnitude decreases. A lower number of high-magnitude gradient norms
implies less clipping bias, while the decrease in low magnitude gradient norms implies a higher
signal-to-noise ratio of the perturbed gradients. The two plots overlap in the subfigure corresponding
to the first epoch as the freeze rate is 0 and the networks are initialized equally. The freeze rate at
the 20th epoch is 0.45 and reaches 0.9 at the 40th epoch. Note that both axes are in log scale.
3	Technical Details
Algorithm 1 outlines our approach. Since the coordinates that have been selected to freeze do not
depend on the dataset, their indices can be exposed or transferred in clear text and do not lead
to additional privacy loss to the dataset. We apply random freeze to optimization with SGD with
or without momentum. In case of non-zero momentum, the velocity is updated with the sparse
gradient as normal. That means, parameters that have been frozen can still be updated as long as
their velocity has not decayed to zero. Next, we discuss the properties and implementation details
of random freeze, compare different variants and provide insight into the strategy,
3.1	Gradual cooling
We find that if we initiate the training with constant freeze rate r, the network converges slowly and
performs poorly when the privacy budget has been fully consumed. The reason is that in the early
stages of training, the network is far from its optimal position, it is better to let all parameters stay
5
Under review as a conference paper at ICLR 2022
Algorithm 1: Random freeze
Input: Initialized parameters: w0 ; Loss function: `; Iterations per epoch: T ; Epochs E; Freeze
rate: r*; Cooling time: e*; Clipping bound: C; Momentum: μ; Learning rate Y.
for e = 0...E-1 do
r(e) = r* ∙ min(e⅛, 1);
Randomly generate a freeze mask m ∈ {0, 1}d subject to P m = d ∙ (1 - r(e));
for t = 0...T-1 do
For each Xi in minibatch of size B, compute gt(xi) = V'(wt,xi);
Partially zero out each gradient gt(xi) = m	gt(xi);
Clip each individual gradient gt(x/ = gt(x/ ∙ min(1, “「认；)」);
Add noise gt = B(Pi gt(x。+ m ΘN(0,C2σ2Id));
Update vt+ι = μ ∙ Vt + gt,wt+ι = Wt — γvt+ι;
end
end
active. So we present gradual cooling, which is inspired by the gradual warm-up of the learning rate
adopted for non-privacy-preserving training (Goyal et al., 2017). Gradual cooling linearly ramps up
the freeze rate from 0 to r* within a predefined cooling time e* and stays at r* for the remaining
training epochs, i.e. r = r* ∙ min(^e^, 1).
3 .2 Per-iteration randomization vs. Per-epoch randomization
Although conducting random freeze leads to negligible additional computational cost and memory
footprint, we find that it is sufficient to generate one freeze mask per epoch, while re-randomizing the
freeze mask at each iteration slightly decreases the performance. As we have shown in Section 2.2,
noise dominates throughout training. Therefore, by per-iteration randomization, parameters have
been perturbed at this round could be frozen during subsequent iterations and stay biased. By con-
trast, per-epoch randomization lets the selected parameters update for one epoch, allowing the noise
of multiple iterations to be averaged out.
Another strength of per-epoch randomization is that for one epoch there are certainly (1 - r)d pa-
rameters updated, which is favorable in collaborative learning schemes as communication cost is
a significant issue, and data are not transmitted every iteration. While for per-iteration random-
ization, the cumulative number of updated parameters depends on the freeze rate r and iterations
between two communication rounds, resulting in higher communication overheads than per-epoch
randomization.
s」①一① LUPJ Pdo⅛
Figure 3: Histogram of the number of parameters versus the number of times a parameter is frozen.
We present non-privately ranked freeze, i.e. ranking after excluding noise, for comparison.
3	.3 Random freeze vs. Ranked freeze
Based on the observation of the low-rank property of gradients (Vogels et al., 2019; Gooneratne et al.,
2020; Li et al., 2020), freezing the parameters with respect to the mean of past perturbed gradients
instead of a random draw might be helpful: First, taking the mean of past perturbed gradients will
6
Under review as a conference paper at ICLR 2022
average out zero-mean noise. Second, the magnitude of the gradients is a diagonal approximation to
the principal components and may be indicative of a useful working subspace. We therefore consider
ranking the dimensions of the gradient by their magnitude and freezing the smallest ones.
However, empirical results do not match these intuitions. Comparing Table 3 with Table 4, ranked
freeze performs similarly to random freeze. We further find that ranked freeze is itself inherently
random as the ranking is dominated by Gaussian noise. To demonstrate this, we run random freeze
and ranked freeze2 on End-to-end CNN for 100 epochs, then statistically analyse the distribution
of how many times a parameter is frozen. The result shows the equivalency between these two
strategies, which implies even averaging the perturbed gradients over a full epoch cannot sufficiently
mitigate the noise added in gradient perturbation (see Figure 3). This result also reflects that even in
a low privacy regime, noise has a significant impact throughout training.
Figure 4: Test accuracy with respect to various clipping bound and momentum pairs. The network
architecture is End-to-end CNN, and the privacy budget is (ε = 3, δ = 10-5). We adjust the clipping
bound C and momentum μ based on their optimal values C = 0.1, μ = 0.9 (Tramer & Boneh,
2021), (a) is adjusted with respect to an inversely proportional scaling rule C = τ⅛⅛ = 1, (b)
1 μ 1 o.9
has momentum fixed μ = 0.9 and varies the clipping bound, (c) has clipping bound fixed to C = 0.1
and varies momentum. These figures demonstrate that the inversely proportional scaling rule can
achieve good performance, otherwise the network performance is degraded. We also observe that
no momentum μ = 0 results in better performance, probably due to less clipping bias induced by
a higher clipping bound. We find out that this rule is relatively general and independent on privacy
level. For experiment on low privacy budget (ε = 7.53, δ = 10-5), refer to Appendix F.
3.4 Inversely proportional scaling rule for adjusting the clipping bound and
MOMENTUM
Previous works commonly adopt first order momentum in the optimization, because momentum can
alleviate oscillation and accelerate gradient descent (Sutskever et al., 2013). As a result, it is believed
to reduce the number of iterations of training and therefore achieve less privacy loss. However, for
privacy-preserving training, momentum will also exaggerate the additive i.i.d. Gaussian noise by
incorporating current and all historical noise. For instance, using the Pytorch (Paszke et al., 2019)
implementation of SGD, the velocity update can be written as: vt+ι = μ ∙ Vt + gt+ι, where v, μ, g
denote perturbed velocity, momentum and perturbed gradients, respectively. Using the expression
of one step noise in Equation 3 and denoting by Vt the velocity after separating the noise, We have
Vt+ι - Vt+ι = (1+ μ+μ2 +...+μt )∙N (0, C2 ∙ σ2 Id). After many iterations, the scalar approximates
a geometric series, i.e. Vt+ι — ^+ι ≈ ι-1μ ∙ N(0, C2 ∙ σ2Id). Pulling the clipping bound C out
and forming the noise as ιCμ ∙ N(0, σ2Id), We present an inversely proportional scaling rule for
adjusting C and 1 — μ, i.e. with other hyperparameters fixed, networks trained with same value of
the ratio ιCμ perform similarly (see Figure 4). Our conjecture is that the inversely proportional
scaling rule ensures the same amount of injected noise.
2For implementation details of ranked freeze, refer to Appendix C.
7
Under review as a conference paper at ICLR 2022
It is worth noticing that the inversely proportional scaling rule is general and amenable to no freeze
and random freeze. Tuning hyperparameters in the context of privacy-perserving training has been
observed to be brittle. This rule helps reduce the tuning workload.
4	Experimental Results
Privacy budget According to Definition 1, we have that each iteration of training is (ε, δ)-
differentially private with respect to a batch of training data, while shuffling and partitioning the
dataset into batches implies that each iteration is (O(qε), qδ)-differentially private with respect to
the full dataset according to the privacy amplification theorem (Balle et al., 2018; Wang et al., 2019),
where q = B/N , B is the batchsize and N is the size of dataset. To track the cumulative privacy
loss over multiple training epochs, We adopt Renyi differential privacy (Mironov, 2017), which is
more operationally convenient and quantitatively accurate.
Advantages of sparsity Projected DP-SGD induces a significant additional computation cost by
running the power method and projecting gradients into and out of subspace. For the power method,
the basic operation is WW|V , W ∈ Rd×s denotes sample gradients, V ∈ Rd×b denotes eigenvec-
tors, the computational cost is O(dbs). Similarly, for projection V |X; X ∈ Rd×n denotes original
gradients, the computational cost is O(dbn). Applying random freeze, a random selection of rows
of X are deleted, while corresponding rows of V, W can be removed as no information of gradient
exits in that subspace. We note that b, s might also be able to be reduced. Overall, the computational
cost is between O(1 - r) and O((1 - r)3). Another issue of projected DP-SGD is the memory
footprint of V . Saving sparse V by random freeze can be achieved by storing non-zero values and
indices of zeros. The cost of indexing is logarithmic of the number of parameters, consider that
log2 109 < 32, we can decrease the memory footprint by removing a single 32 bit float gradient.
Communication overhead can similarly be reduced. We note that random freeze uses the same mask
during one training epoch, which could contain multiple groups of eigenvectors and communication
rounds. Therefore, the cost of indexing is negligible: communication overhead and memory foot-
print are O(1 - r). Further, we define the total density as the total amount of non-zero gradients by
random freeze over the total amount of gradients by the original dense representation to reflect these
advantages of sparsity.
Our experiments are implemented in the Pytorch framework. To compute the gradients of an in-
dividual example in a minibatch, which is required for gradient clipping, we use the BackPACK
package (Dangel et al., 2020). The privacy loss of multiple iterations has been tracked with Opacus.
We use the benchmark CIFAR10, which is to date standard in benchmarking DP learning. Our code
is available for download from http://anonymized.for.review/.
To validate the reliability of random freeze, we conduct experiments on several SOTA networks cross
different frameworks, including End-to-end CNN (Papernot et al., 2021); Handcrafted CNN (Tramer
& Boneh, 2021) which incorporates ScatterNet (Oyallon et al., 2019) as feature extractor; Gradient
Embedding Perturbation (GEP) proposed by Yu et al. (2021a) which injects noise in projected gradi-
ent; DP-Transfer Learning which adopts a pretrained network and replaces the linear classifier layer,
in particular we use SIMCLR v2 (Chen et al., 2020a) pretrained on unlabeled ImageNet (Deng et al.,
2009), which has been benchmarked by Tramer & Boneh (2021).
For a fair comparison, we run every experiment 5 times then compute the average of best accuracy
and the standard error. We do not tune the hyperparameters when adapting the SOTA works with
random freeze, instead we adopt the optimal hyperparameters for DP-SGD without random freeze
provided in the respective works. Random freeze is applied as follows: if the optimal number of
epochs from the previous work is set as e*, we linearly ramp UP the freeze rate r from r = 0 at
epoch e = 0 to r = r* at epoch e = e*, i.e. r = r* ∙ e⅛ι. These straightforward experiments allow
us to demonstrate that random freeze is a safe add-on in a variety of methods. We summarize the
performance results in Table 1, and the corresponding total density by random freeze in Table 2. We
document the hyperparameters in Appendix D.
We find that when the network is large, for instance End-to-end CNN has the most parameters
among all frameworks, random freeze is able to improve the accuracy. To demonstrate this, we
further tune the End-to-end CNN. We still adopt the best hyperparameters from the previous work,
then first adjust the clipping bound and momentum from (C = 0.1, μ = 0.9) to (C = 1, μ = 0)
8
Under review as a conference paper at ICLR 2022
Test Accuracy
Approaches	ε-DP	# of parameters	Baseline	Random freeze	Freeze rate r*
End-to-end CNN	7.53	550K	66.9 ± 0.4	66.7 ± 0.3	0.7
	3.0		60.4 ± 0.2	61.1 ± 0.2	0.7
Handcrafted CNN	3.0	187K	69.4 ± 0.2	69.4 ± 0.2	0.6
GEP	8.0	268K	73.5 ± 0.4	73.4 ± 0.3	0.4
DP-Transfer Learning	2.0	41K	92.6 ± 0.0	92.7 ± 0.0	0.7
Table 1: Test accuracy of SOTA works before and after adopting random freeze. We maintain the
accuracy with high freeze rate. Communication overhead, computational cost and memory footprint
of the projected DP-SGD are accordingly reduced as implied by total density in Table 2.
Total density
End-to-end CNN Handcrafted CNN GEP DP-Transfer Learning
0.65	0.7	0.8	0.65
Table 2: Total representation density of random freeze. This table is aligned with Table 1.
Test Accuracy
Approaches	ε-DP	Baseline	Our adjusted baseline	Random freeze
End-to-end CNN	7.53	66.9 ± 0.4	69.7 ± 0.1	70.2 ± 0.1
	3.0	60.4 ± 0.2	63.1 ± 0.2	64.5 ± 0.3
Table 3: Test accuracy of End-to-end CNN adjusted with respect to inversely proportional scaling
rule and trained with random freeze. Our adjusted baseline performs better than original work while
with random freeze we further improve the accuracy. Altogether we obtain significantly better utility.
with respect to the inversely proportional scaling rule proposed in Section 3.4, which leads to better
accuracy. Secondly, we apply random freeze. Similar to the previous experiment we also linearly
ramp UP the freeze rate to r* but then We extend the training for 20 additional epochs at freeze rate
r*, this help us to improve the performance further. The result is summarized in Table 3. Note that,
after extending the training epochs, the noise multiplier σ has been increased accordingly to ensure
the same privacy budget Will be consumed. As a result, more epochs Without random freeze Will
decrease the performance. Moreover, We achieve loWer total density With higher freeze rate. For
hyperparameters, total density and accuracy as a function of privacy loss, refer to Appendix E.
5	Discussion and Conclusions
In this Work We propose random freeze Which is an axis-aligned random gradient dimension re-
duction method. We provide a fundamental theoretical study on random freeze and investigate the
trade-off betWeen signal loss and perturbation moderation. Although simple to implement, random
freeze can be safely (Without further tuning) applied across different frameWorks and architectures.
Even in conjunction With other gradient space reduction methods performance is maintained. With
the sparse representation of the gradient update, random freeze can reduce the computational cost
and memory footprint of projected DP-SGD. Also, itis able to reduce the total amount of transferred
data in federated learning or other collaborative learning frameWorks. Moreover, We significantly
improve the performance of SOTA End-to-end CNN using the random freeze strategy and inversely
proportional scaling rule that We have proposed. We note that the computational cost of the random
freeze procedure is negligible. Therefore We believe that random freeze can be incorporated as a
general method in DP-SGD. and expect random freeze to exhibit strong improvements as DP-SGD
is adapted to larger and deeper netWorks in the future. We also hope this Work can shed light on
gradient dimension reduction in DP-SGD and motivate further research in this direction.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016.
Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight
analyses via couplings and divergences. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, 2018.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. Proceedings - Annual IEEE Symposium on Foundations of
Computer Science, FOCS, pp. 464-473, 2014.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. Big
self-supervised models are strong semi-supervised learners. In Advances in Neural Information
Processing Systems 33, 2020a.
Xiangyi Chen, Steven Z. Wu, and Mingyi Hong. Understanding gradient clipping in private SGD:
A geometric perspective. In NeurIPS, 2020b.
Felix Dangel, Frederik Kunstner, and Philipp Hennig. Backpack: Packing more into backprop. In
International Conference on Learning Representations, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255, 2009.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations
and Trends® in Theoretical Computer Science, 9:211-407, 2014.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 1322-1333, 2015.
Jonas Geiping, HartmUt Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients
- how easy is it to break privacy in federated learning? In Advances in Neural Information
Processing Systems, pp. 16937-16947, 2020.
Mary Gooneratne, Khe Chai Sim, Petr Zadrazil, Andreas KabeL Francoise Beaufays, and Giovanni
Motta. Low-rank gradient approximation for memory-efficient on-device training of deep neural
network. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 3017-3021, 2020.
Priya Goyal, Piotr Dollar, RoSS Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. 06 2017.
Peter Kairouz, Monica Ribero Diaz, Keith Rush, and Abhradeep Thakurta. (nearly) dimension in-
dependent private erm with adagrad rates
via publicly estimated subspaces. In Proceedings of Thirty Fourth Conference on Learning The-
ory, volume 134, pp. 2717-2746, 2021.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto,
2012.
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based
analysis of sgd for deep nets: Dynamics and generalization. In Proceedings of the 2020 SIAM
International Conference on Data Mining (SDM), pp. 190-198, 2020.
Ruixuan Liu, Yang Cao, Masatoshi Yoshikawa, and Hong Chen. Fedsel: Federated SGD under
local differential privacy with top-k dimension selection. In Yunmook Nah, Bin Cui, Sang-Won
Lee, Jeffrey Xu Yu, Yang-Sae Moon, and Steven Euijong Whang (eds.), Database Systems for
Advanced Applications, 2020.
10
Under review as a conference paper at ICLR 2022
Arak Mathai. Storage capacity of a dam with gamma type inputs. Annals of the Institute of Statistical
Mathematics, pp. 591-597,1982.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceed-
ings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 1273-1282,
2017.
Ilya Mironov. Renyi differential privacy. 2017 IEEE 30th Computer Security Foundations SymPo-
sium (CSF), 2017.
Panagis Moschopoulos. The distribution of the sum of independent gamma random variables. An-
nals of the Institute of Statistical Mathematics, pp. 541-544, 1985.
P.G. Moschopoulos and W.B. Canada. The distribution function of a linear combination of chi-
squares. Computers & Mathematics with Applications, pp. 383-386, 1984.
Opacus. Opacus PyTorch library. Available from opacus.ai.
Edouard Oyallon, Sergey Zagoruyko, Gabriel Huang, Nikos Komodakis, Simon Lacoste-Julien,
Matthew Blaschko, and Eugene Belilovsky. Scattering networks for hybrid representation learn-
ing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.
Nicolas Papernot, Abhradeep Thakurta, ShUang Song, Steve Chien, and UJlfar Erlingsson. Tem-
pered sigmoid activations for deep learning with differential privacy. Proceedings of the AAAI
Conference on Artificial Intelligence, pp. 9312-9321, 2021.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32, pp. 8024-8035. 2019.
Abhinav Pathak, Y. Charlie Hu, and Ming Zhang. Where is the energy spent inside my app? fine
grained energy accounting on smartphones with eprof. In Proceedings of the 7th ACM European
Conference on Computer Systems, EuroSys ’12, pp. 29-42, 2012.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In 2015 53rd Annual Allerton
Conference on Communication, Control, and Computing (Allerton), pp. 909-910, 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In IEEE Symposium on Security and Privacy, 2017.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In Proceedings of the 30th International Conference on
Machine Learning, pp. 1139-1147, 2013.
Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more
data). In International Conference on Learning Representations, 2021.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical Low-Rank Gradi-
ent Compression for Distributed Optimization. 2019.
Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled renyi differential
privacy and analytical moments accountant. In Proceedings of the Twenty-Second International
Conference on Artificial Intelligence and Statistics, 2019.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 2019.
Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient
embedding perturbation for private learning. In International Conference on Learning Represen-
tations, 2021a.
11
Under review as a conference paper at ICLR 2022
Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via
low-rank reparametrization. In International Conference on Machine Learning (ICML), 2021b.
Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential pri-
vacy, 2021.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In International Conference on Learning Rep-
resentations, 2020.
Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private SGD
with gradient subspace identification. In International Conference on Learning Representations,
2021.
Junyi Zhu and Matthew B. Blaschko. R-GAP: Recursive gradient attack on privacy. In International
Conference on Learning Representations, 2021.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems, 2019.
A Mean square error of gradient in DP-SGD
Theorem 1.	Assuming that the gradient is drawn from N(Vw, Σ), centered at the true gradient
Vw and with respect to the covariance matrix Σ whose trace goes linearly up with dimension d.
The MSE ofperturbed gradient g = g + ξ0p can be lower bounded by:
MSE ≥ Tr[Σ](1 + dσ2).	(2)
Proof. Assuming an oracle can tell us the true gradient Vw given a network with parameters w ∈
Rd , while the individual gradient g can be seen as drawn from the Gaussian distribution centered at
Vw with respect to the covariance matrix Σ, i.e. g 〜N(Vw, Σ). Denote ξg as gradient deviation,
i.e. ξg = g 一 Vw, the perturbed gradient g in DP-SGD can be expressed as:
9 = Vw + ξg + ξDP	(8)
and the mean square error of perturbed gradient is:
MSE = E[ξ∣ξg + 2ξ∣ξDP + ξD P ξDp]	(9)
Since ξ°p and ξg are independent and plugging the definition of injected noise. i.e. ξ0p 〜
N(0, max(glg)σ2Id), into Equation 9, We have:
MSE = E[ξ∣ξg ]+ E[ξD P ξDP ]	(10)
= Tr[Σ] + dσ2E[max(glg)]	(11)
= Tr[Σ] + dσ2E[max(ξglξg + 2Vwlξg + VwlVw)]	(12)
≥ Tr[Σ] + dσ2E[max(ξglξg)]	(13)
where equality can be achieved when the network converges to stationary point, i.e. Vw → 0.
Applying a spectral decomposition to Σ, we obtain QlΛQ = Σ, where Q is an orthonormal basis
and Λ is diagonal matrix of eigenvalues of Σ. Denote by z a random vector drawn i.i.d. from a
normal distribution, we can further derive:
ξlξg 〜zlΣz = zlQlΛQz = zlΛz	(14)
As random variable with respect to the same distribution leads to the same expectation, plugging
Equation 14 into Equation 13, we have:
MSE ≥ Tr[Σ] + dσ2E[max(zlΛz)]		(15)
= Tr[Σ] 1 + dσ2E	max (τri∑i zlAz)	(16)
= Tr[Σ] 1 + dσ2E	maχ(τ1Λi zlAz)	(17)
12
Under review as a conference paper at ICLR 2022
Since zlΛz
λιz2 + λ2z2 +-+ λdzp; Zi 〜N(0,1), we have:
τr1Λ] z1λz 〜τr1Λ] X Gamma (1, W).
(18)
So far either the exact PDF of a Gamma distribution summation can only be represented with (an-
other) series of gamma distributions (Moschopoulos, 1985; Mathai, 1982), or its exact CDF has a
series representation (Moschopoulos & Canada, 1984). For the convenience of further derivation
of the expected maximum, we use the Welch-Satterthwaite approximation which provides a certain
Gamma distribution:
-ɪr X Gamma (-, ɪ
Tr[Λ] J	[2, 2λi
≈ -ɪr Gamma
Tr[Λ]
(Tr2 {Λ}	Tr{Λ} ∖
2Tr{Λ2} , 2Tr{Λ2})
Gamma
(Tr2{Λ}	Tr2{Λ} ∖
2Tr{Λ2}, 2Tr{Λ2})
(19)
(20)
Denote Mn = max(x1, x2,…，xn), where Xi is drawn from Gamma (2T{^}, 2Trr{Λ}}) ∙ While
Uo-QuaAl-SU ① P Al=一 qpqoJCL
Figure 5: A series of Gamma distributions with the form Gamma(k, k).
from the Cauchy-Schwarz inequality, we have:
IhId,diag{Λ}i∣2 ≤ hId,Idi∙hdiag{Λ},diag{Λ}i
Tr2 [Λ] ≤ d ∙ Tr[Λ2]
Tr2 {Λ}
2Tr[Λ2]
(21)
(22)
(23)
d
≤ —
一2
Tr2{Λ}
Denote k as 2 口{八}}, Figure 5 illustrates the PDF of Gamma distribution With parameters
Gamma(k, k).
As k becomes large, the distribution becomes more concentrated, and we have the following in-
equality:
Mn(1) = max(x(11), x(21), ..., x(n1)), xi(1)
Mn(2) = max(x(12), x(22), ..., x(n2)), xi(2)
Tr2 {Λ} Tr2{Λ}
〜Gamma(21Y{Λ2}, 2Tr{Λ2})
〜GammaW,勺
EMnD] ≥ EMn2)] ≥ E[- X Xi2)] = 1
i
(24)
13
Under review as a conference paper at ICLR 2022
Substituting inequality 24 back into Equation 17, we have:
MSE ≥ Tr[Σ](1 + dσ2).	(25)
The proof is completed.	□
We note that Tr[Σ] is the trace of a d-dimensional covariance matrix. In the extreme case that
Σ = λI, this will scale linearly in d, while in the opposite extreme of a rank deficient covariance
matrix, the trace is constant in d. Thus, in the former case
MSE ≥ λd + λσ2d2,	(26)
while in the latter we have
MSE ≥ λ + λσ2d,	(27)
which leads to our conclusion that the lower bound on MSE is between linear and quadratic in d in
practice.
B	The convergence rate of DP-SGD with random freeze
Lemma 1. u, v ∈ Rd are too arbitrary vectors, m is a {0, 1}d random mask independent from u, v,
and subject to m = (1 - r)d, where r denotes the freeze rate. We have the following expectation:
E[hmk	u, vi] = (1 - r)E[hu, vi].	(28)
Proof.
E[hmk	u, vi] = E[hm u, vi],	(29)
= X E[mi]E[uivi],	(30)
i
= (1 -r)E[hu,vi].	(31)
The proof is completed.	□
Corollary 1.1. Following the notation of lemma 1, we have the expectation below:
E[hm u, m vi] = (1 - r)E[hu, vi].	(32)
Proof.
E[hm	u,m vi] = E[hm2	u,vi],	(33)
2=8 (1 - r)E[hu, vi].	(34)
The proof is completed.	□
Lemma 2. u ∈ Rd is an arbitrary vector, m is a {0, 1}d random mask independent from u and
subject to m = (1 - r)d, where r denotes the freeze rate. We have the following inequality:
	E[km uk] ≥ (1 - r)kuk.	(35)
Proof.	E[km Θ uk] = II1II E[km Θ Ukkuk], kuk	(36)
	≥ η^^[j^E[km ® uk2], kuk	(37)
	=IAr(I — r)E[hu,ui], kuk	(38)
	= (1 - r)kuk.	(39)
Equality is taken when r	0. The proof is completed.	□
14
Under review as a conference paper at ICLR 2022
Theorem 2.	Assume G-Lipschitz smoothness of Vw such that ∣∣Vwt+ι — Vwt|| ≤ G∣∣wt+ι 一 Wt ||.
Consider an algorithm with clipping bound C, learning rate γ and choose a symmetric probability
density distributionp(∙) satisfyingpt(ξt) = pt(-ξt), ∀ξt ∈ Rd. Then ∃ K ≥ 1 — r such that:
TT
T X Pξt 〜Pt (kξtk < Z)h(Vwt)kvwtk ≤ K (奇 + Y δC + (1 - r)Y δDP - T X Em [bt]), (4)
t=1	γ	t=1
Proof. Denote the injected noise at each iteration ξ0p := Bz, where Z is drawn from
N(0, C2σ2Id), follow from the smoothness assumption, we have:
G
Lt+ι ≤ Lt + hvwt, wt+ι 一 Wti + ^2Ilwt+ι 一 WtIl ,
=Lt — Yhvwt,gt + m Θ ξDPi + G2Y-Ilgt + m Θ ξDP∣∣2,
=Lt — YhVwt,gti — γhVwt, m Θ ξDPi + GY- k^t + m Θ ξ0p ∣∣2,
Taking expectations on both sides and rearranging, we have:
EKVwt, Oti] ≤ —E[Lt — Lt+1] 一 EKVwt, m θ ξDPi] +-^E[kgt + m θ ξDPk2],
Y2
=8 "[Lt — Lt+1] — (1 一 r)EKVwt, ξDPi] + YE-E[kgt + m θ ξDP ||2],
Y2
=—E[Lt — Lt+1] 一 0+ ■-^(E[∣∣gt∣∣2]+ E[km θ ξDPk2] 一 0),
Y2
=YE[Lt - Lt+1] + ~γ((E[k0tk2] + (1 — r) R2 ),
1	GY 2	C2σ2d
≤ γE[Lt - Lt+i] + -2-(C + (1 - r) B2 ).
Now focusing on the l.h.s., we have:
(40)
(41)
(42)
(43)
(44)
(45)
(46)
(47)
EKVwt,^ti] =(Vwt, E[gt]i,
=hVwt, BR X E[m Θ gt,i ∙ min(1,
C
=EKVwt, m Θ gt,i ∙ min(1, ∣mΘ
=EKm Θ Vwt, m Θ gt,i ∙ min(1, ɪ
I
C
=Em Eξt [hVw0,g0,i∙ min(1, I--
Em
Eξ0〜Pt KVwO,g0,i ∙ min(1,
C ∣∣m Θ g	t,iI )]i,		(48) (49)
gt,i I )i],			(50)
C m Θ gt,	iI )i],		(51)
)i|m] I	,		(52)
C -Λ-∏- )i|m]		+ Em [bt],	(53)
where We have defined b := R(Vw0,g0,i ∙ min(1, ∣⅛)i(pt(ξ0) - pt(ξ0))dξ0. Equation52 is
achieved because ξt and m are independent. We note that for any given m, ξt0 is still independent
deviation with zero mean and pt(ξO) = Pt(—ξ0) since projection of symmetric distribution to Sub-
space is symmetric. According to Theorem 2 from Chen et al. (2020b), for a given m we have the
following inequality:
C	C	3C
Eξ0 〜Pt KVwO ,g0,i∙ min(1, ||-II )i|m] ≥ Pξ0 〜Pt (kξ0k < -)min(kVwt k, ɪ)kvw0 k.	(54)
Igt,i I	4	4
15
Under review as a conference paper at ICLR 2022
Back to Inequality 47 and considering the overall T steps, we have:
1 T	1	Gγ	C2σ2d
T XEKVw,0ti] ≤ TEE[L1 -LT] + -2-(C +(I - r) B? ),	(55)
t=1	γ
≤ YTΔl + γ∆c + (1 - r)γΔdp,	(56)
where We have defined Δl := E[Lι - mi□w L(w)], ∆c := g2c2, Δdp := C^dG. Plugging
Inequality 54 into Equation 53 and combining with Inequality 56:
TT
T X Em [Pξ0 〜Pt (kξ0 Il < ɪ)h(vw0 )kVwt k] ≤ -EL + 7δC + (I-Ir)YδDP - T X Em [bt],
t=1	γ	t=1
(57)
where we have defined h(Vwt) = min(∣Vw0k, 34C). Now we look at the l.h.s. and consider each
step with respect to the following cases:
case 1:	∣Vwtk ≤ 4C, then Pm(kVw0k ≤ ∣C) = 1,
CC
Em[Pξ0 F (kξ0 k < jh(Vwt )kVw0 k] ≥ Pξt 〜Pt (kξtk < ;r)Em[h(Vwt) ∣∣Vwt k],	(58)
C
=Pξt〜Pt(kξtk < z)Em[kVw0k2],	(59)
=Pξt〜Pt(kξtk <C4)(1 - r)kVwtk2,	(60)
C
= (IT)Pξt〜Pt(k&k < W)h(Vwt)kVwtk.	(61)
case2:	kVwtk > ∣ C, consider two events Ai： ∣Vw0 k ≤ 3 C; A2: ∣∣Vw0k > ∣ C then we have:
CC
Em[Pξ0 〜Pt (kξ0 k < jh(Vwt )kVwt k] ≥ Pξt 〜Pt (kξtk < jEm[h(Vw0 )kVwt k],	(62)
60	C
≥ Pξt〜Pt(kξtk < N(P(Aι)(1 - r)kVwtk2+	(63)
3
P(A2)4C ∙ Em[kVwtk]),
35	C
≥ Pξt〜Pt(kξtk < F(P(Aι)(1 -r)kVwtk2+	(64)
3
P(A2)4C(1 - r)kVwtk),
C3
≥ Pξt〜Pt(k&k < W)(I- r)4CllVwtk，	(65)
C
= (I-r)Pξt〜Pt(k&k < jh(Vwt)kVwtk,	(66)
from which we conclude that:
CC
Em[Pξ0〜Pt(kξ0k < Cm)kVw0k] ≥ (1 -r)Pξt〜Pt(kξtk < z)h(Vwt)∣∣Vwtk∙	(67)
The inequality above implies that ∃ κ ≥ 1 - r such that:
TT
X Em [Pξ0 〜Pt (kξ0 k < W)h(Vw0)kVw0 k] = K X Pξt 〜Pt (kξtk < Nh(Vwt)kVwtk.	(68)
t=1	t=1
We note that κ = 1 if r = 0. Plugging equation 68 into inequality 57 we obtain:
TT
T X Pξt 〜Pt(k&k < Nh(Vwt)kVwtk ≤ K ( ~T +Y^C + (1-r)2DP - T X Em[bt])∙ (69)
t=1	Y	t=1
The proof is completed.	□
16
Under review as a conference paper at ICLR 2022
It is worthwhile to note that the l.h.s. can also be written as the general form below and the inequality
still holds:
1
TEPξt~pt(kξtk < ZC)min(INwtk, (I-Z)C)kvwtk, ∀z ∈ (OJ)
(70)
t=1
C Ranked freeze
Algorithm 2 describes ranked freeze, the performance is recorded in Table 4. Note that for aggre-
gated gradient estimation we also inject noise to gradient estimation ge of frozen coordinates. If not,
the coordinates get frozen at the first iteration will receive aggregated gradient estimation as 0 and
never get updated for all the remaining iterations, which will degrade the network. Adding noise
to frozen coordinates can give these coordinates a chance to be ranked in higher positions, while in
turn the coordinates in updating but with low magnitude of true gradient may get frozen in the next
iteration.
Algorithm 2: Ranked freeze
Input: Initialized parameters: w°; Loss function: '; Iterations per epoch: T; Epochs E; Freeze
rate: r*; Cooling time: e*; Clipping bound: C; Momentum: μ; Learning rate γ.
for e = 0...E-1 do
ge = {0}d;
r(e) = r* ∙ min(e⅛, 1);
if e is 0 then
I m = {1}d
else
Sort the indices [1,...,d] with respect to corresponding aggregated gradient of the last
epoch ge-1 in ascending order then set the first d ∙ r(e) positions in mask m to 0 and
the rest to 1;
end
for t = 0...T-1 do
For each xi in minibatch of size B, compute gt (xi) = v`(wt, xi);
Partially zero out each gradient gt (xi) = m gt (xi);
Clip each individual gradient gt(xi) = gt(xi) ∙ min(1,州2);
Add noise gt = BB (Pi gt(xi) + m ΘN(0, C2σ2Id));
Update
vt+1 = μ ∙ Vt + gt, wt+1 = wt 一 γvt+ι, ge = ge + -B(Pi gt,i + N(0,C2σ2Id));
end
end
Test Accuracy
Approaches
ε-DP
End-to-end CNN 7.53
3.0
Baseline Our adjusted baseline Random freeze
66.9 ± 0.4
60.4 ± 0.2
69.7 ± 0.1
63.1 ± 0.2
70.0 ± 0.1
64.6 ± 0.1
Table 4: Test accuracy of End-to-end CNN adjusted with respect to inversely proportional scaling
rule and trained with ranked freeze. We adopt hyperparameters recorded in Table 9. Our adjusted
baseline performs better than original work while with ranked freeze we further improve the accu-
racy. However, we note that ranked freeze performs similarly as random freeze (see Table 3).
D Hyperparameters for Table 1
The δ term is 10-5 for all settings. There exist two ways of determining the injected noise scale
per iteration: 1. Set the privacy budget and noise multiplier then stop the training when the privacy
17
Under review as a conference paper at ICLR 2022
budget is fully consumed; 2. Set the privacy budget and iterations of training then compute the noise
multiplier so that the privacy budget will be fully consumed at the last iteration. In this work we refer
to the second method when talking about hyperparameters. Hyperparameters are listed separately
for each framework, see Table 5, 7, 6, 8. There is a minor difference for hyperparameters adopted
from Tramer & Boneh (2021). For instance, they set batchsize=1024 for End-to-end CNN, which
leads to non-exact batch partitions. We set batchsize=1000 to avoid that and achieve equally good
or slightly better baseline performance. Additonally, their hyperparameters of End-to-end CNN are
tuned for ε = 3 and then applied to ε = 7.53, we find out that based on their hyperparameters for
batchsize, clipping bound and momentum, setting the number of epochs to 80 can achieve the best
accuracy.
End-to-end CNN
ε	σ	lr	Batchsize	Epoch	Momentum	Clip
3	1.54	1	1000	40	0.9	0.1
7.53	1.10	1	1000	80	0.9	0.1
Table 5: Hyperparameters of End-to-end CNN adopted from Tramer & Boneh (2021)
DP-Transfer Learning
ε σ lr Batchsize Epoch Momentum Clip Source model Aux. dataset
2 2.30 4	1000	50	0.9	0.1 SIMCLR v2 ImageNet
Table 6:	Hyperparameters of DP-Transfer Learning adopted from Tramer & Boneh (2021)
Handcrafted CNN
ε σ lr Batchsize Epoch Momentum Clip Input norm BN norm Architecture
3	5.65	4	8000	80	0.9	0.1 BN	8 ScatterNet + CNN
Table 7:	Hyperparameters of Handcrafted CNN adopted from Tramer & Boneh (2021)
Gradient Embedding Perturbation
ε lr Batchsize Epoch Momentum Clip0 Clip1 Subspace Sample gradients Aux. dataset
8 0.1	1000	200	0.9	5	2	1000	2000	ImageNet
Table 8:	Hyperparameters of GEP adopted from Yu et al. (2021a)
E Improving the accuracy of End-to-end CNN
Table 9 includes the hyperparameters and total density of ajusted End-to-end CNN with random
freeze. Figure 6 shows the accuracy as a function of privacy loss.
Adjusted End-to-end CNN with random freeze
ε	σ	lr	Batchsize	Epoch	Momentum	Clip	Freeze rate	Cooling time	Total density
3	1.81	1	1000	60	0	1	0.9	40	0.6
7.53	1.18	1	1000	100	0	1	0.9	80	0.58
Table 9: Hyperparameters for adjusted End-to-end CNN with random freeze
18
Under review as a conference paper at ICLR 2022
Method: - Adjusted Baseline - Baseline — Random Freeze
train
Figure 6: Accuracy as a function of privacy loss. We run five experiments and computes the mean
value. The result shows that random freeze outperforms the baseline and adjusted baseline in all
high privacy levels.
F Inversely proportional scaling rule
We include here an additional experiments of inversely proportional scaling rule on low privacy
budget (see Figure 7).
(a) C— = 1	(b) μ = 0.9	(C) C = 0.1
1-μ
Figure 7: Test accuracy with respect to various clipping bound and momentum pairs. The network
arChiteCture is End-to-end CNN, and the privaCy budget is (ε = 7.53, δ = 10-5). We adjust the
0.9 (Tramer &
C _	0.1	_
1-μ = 1-0.9 =
clipping bound C and momentum μ based on their optimal values C = 0.1, μ =
Boneh, 2021), (a) is adjusted with respeCt to an inversely proportional sCaling rule
1, (b) has momentum fixed μ = 0.9 and varies the clipping bound, (c) has clipping bound fixed to
C = 0.1 and varies momentum. These figures demonstrate that the inversely proportional scaling
rule can achieve good performance, otherwise the network performance is degraded. Combining
with Figure 4, we see this rule is relatively general and does not dependent on privacy level.
G A nuance of random freeze
Gradient clipping is usually the first step we need to do after backpropagation in DP-SGD. However,
random freeze first zeros out part of the gradient then clips with respect to the respective norm. We
note that the strategy of first clipping then freezing appears in a recent work (Zhang et al., 2021)
as a baseline for comparison, but shows no improvement in performance. We name this strategy
posterior random freeze for convenience. For DP-SGD with posterior random freeze, denote the
19
Under review as a conference paper at ICLR 2022
gradient gt := B Pi m Θ gt,i ∙ min(1,电%), assume G-LiPschitz smoothness, We have:
43 EKVwt,同i] ≤	—E[Lt γ	—Lt+1] — EKVwt,m Θ ξDPi] + GYE[∣∣gt + m Θ ξ0p∣∣2],		(71)
28	_E[Lt	- Lt+1] -	(I-T)EKVwt, ξDPi] + ^YE[kgt + m θ ξDPk2],	(72)
28	γ —E[Lt	- Lt+1] -	0+ -2γ(E[kgtk2]+ E[km θ ξDP k2] -O),	(73)
32	γ —E[Lt γ	- Lt+1] +	GY (E[∣ 制 2] + (1-r) CB2d)	(74)
Similarly for DP-SGP with random freeze, we have:
EKVwt,gti] ≤ 1 E[Lt - Lt+ι] + GY(E[k^tk2] + (1 - r)C-2^d).	(75)
γ	2	B2
We see both methods moderate the perturbation, while E[∣∣gtk2] ≤ E[∣∣gtk2] but it is complicated
to compare EKVwt,瓦〉]and EKVwt,gti], we leave this for future work. From an operational
view, denote gti = m Θ gt,i ∙ min(1, ^^) and gt,i = m Θ gt,i ∙ min(1, ∣∣m乙匕口), we see that
hgt,i, gt,ii∕kgt,ikk^t,ik = 1 and kgt,ik ≤ kgt,ik ≤ ∣∣gt,ik, i.e. random freeze and posterior random
freeze contain the same direction information of individual gradient, but random freeze retains more
distance information. Therefore, we suppose random freeze to perform better than posterior random
freeze.
20