Under review as a conference paper at ICLR 2022
A Communication-Efficient Distributed Gra-
dient Clipping Algorithm for Training Deep
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
In distributed training of deep neural networks or Federated Learning (FL), peo-
ple usually run Stochastic Gradient Descent (SGD) or its variants on each ma-
chine and communicate with other machines periodically. However, SGD might
converge slowly in training some deep neural networks (e.g., RNN, LSTM) be-
cause of the exploding gradient issue. Gradient clipping is usually employed to
address this issue in the single machine setting, but exploring this technique in the
FL setting is still in its infancy: it remains mysterious whether the gradient clip-
ping scheme can take advantage of multiple machines to enjoy parallel speedup in
the FL setting. The main technical difficulty lies at dealing with nonconvex loss
function, non-Lipschitz continuous gradient, and skipping communication rounds
simultaneously. In this paper, we explore a relaxed-smoothness assumption of the
loss landscape which LSTM was shown to satisfy in previous works, and design
a communication-efficient gradient clipping algorithm. This algorithm can be run
on multiple machines, where each machine employs a gradient clipping scheme
and communicate with other machines after multiple steps of gradient-based up-
dates. Our algorithm is proved to have O (n^) iteration complexity for finding
an -stationary point, where N is the number of machines. This indicates that our
algorithm enjoys linear speedup. Our experiments on several benchmark datasets
and various scenarios demonstrate that our algorithm indeed exhibits fast conver-
gence speed in practice and validate our theory.
1	Introduction
Deep learning has achieved tremendous successes in many domains including computer vi-
sion (Krizhevsky et al., 2012; He et al., 2016) and natural language processing (Devlin et al., 2018),
game (Silver et al., 2016). To obtain good empirical performance, people usually need to train large
models on a huge amount of data, and it is usually very computationally expensive. To speedup the
training process, distributed training becomes indispensable (Dean et al., 2012). For example, Goyal
et al. (2017) trained a ResNet-50 on ImageNet dataset by distributed SGD with minibatch size 8192
on 256 GPUs in only one hour, which not only matches the small minibatch accuracy but also enjoys
parallel speedup, and hence improves the running time. Recently, there is an increasing interest in
an variant of distributed learning, namely Federated Learning (FL) (McMahan et al., 2017), which
focuses on the cases where the training data is non-i.i.d. across devices and only limited communi-
cation is allowed. McMahan et al. (2017) proposed an algorithm named Federated Averaging, which
runs multiple steps of SGD on each clients before communicating with other clients.
Despite the empirical success of distributed SGD and its variants (e.g., Federated Averaging) in
deep learning, they may not exhibit good performance when training some neural networks (e.g.,
Recurrent Neural Networks, LSTMs), due to the exploding gradient problem (Pascanu et al., 2012;
2013). To address this issue, Pascanu et al. (2013) proposed to use the gradient clipping strategy,
and it has become a standard technique when training language models (Gehring et al., 2017; Peters
et al., 2018; Merity et al., 2018). There are some recent works trying to theoretically explain gra-
dient clipping from nonconvex optimization’s perspective (Zhang et al., 2019; 2020). These works
are built upon an important observation made in (Zhang et al., 2019): for certain neural networks
such as LSTM, the gradient does not vary uniformly over the loss landscape (i.e., the gradient is
not Lipschitz continuous with a uniform constant), and the gradient Lipschitz constant can scale
linearly with respect to the gradient norm. This is referred to as the relaxed smoothness condition
(i.e., (L0, L1 )-smoothness defined in Definition 2), which generalizes but strictly relaxes the usual
smoothness condition (i.e., L-smoothness defined in Definition 1). Under the relaxed smoothness
1
Under review as a conference paper at ICLR 2022
Table 1: Comparison of Iteration and Communication Complexity of Different Algorithms for find-
ing a point whose gradient’s magnitude is smaller than (i.e., -stationary point defined in Defini-
tion 3), N is the number of machines, the meaning of other constants can be found in Assumption 1.
Algorithm	Setting	Iteration Complexity	Communication Complexity
SGD (Ghadimi & Lan, 2013)	Single1	O (∆(Lo + LiM)e-2 + ∆(Lo + L∖M)σ2 *e-4)	N/A
Clipped SGD (Zhang et al., 2019)	Single	O ((△ + (Lo + Lισ)σ2 + σL2∕Lι)2)e-4)	N/A
Clipping Framework (Zhang et al., 2020)	Single	O (∆Loσ2e-4)	N/A
Naive Parallel of (Zhang et al., 2020)	Distributed2	O (∆L°σ2∕(Ne4))	O (∆L°σ2∕(Ne4))
Ours (this work)	FL	O(∆Lo(σ + κ)2∕(Ne4))	O (∆Lo(σ + κ)2e-3)
condition, Zhang et al. (2019; 2020) proved that gradient clipping enjoys polynomial-time iteration
complexity for finding the first-order stationary point in the single machine setting, and it can be
arbitrarily faster than fix-step gradient descent.
In practice, both distributed learning (or FL) and gradient clipping are important techniques to accel-
erate neural network training. However, the theoretical analysis of gradient clipping is only restricted
to the single machine setting (Zhang et al., 2019; 2020). Hence it naturally motivates us to consider
the following question:
Is it possible that the gradient clipping scheme can take advantage of multiple machines to
enjoy parallel speedup in training deep neural networks, with data heterogeneity across ma-
chines and limited communication?
In this paper, we give an affirmative answer to the above question. Built upon the relaxed smoothness
condition as in (Zhang et al., 2019; 2020), we design a communication-efficient distributed gradient
clipping algorithm. The key characteristics of our algorithm are: (i) unlike naive parallel gradient
clipping algorithm which requires averaging model weights and gradients from all machines for
every iteration, our algorithm only aggregates weights with other machines after certain number of
local updates on each machine; (ii) our algorithm clips the gradient according to the norm of the
local gradient on each machine, instead of the norm of the averaged gradients across machines as in
the naive parallel version. These key features make our algorithm amenable to the FL setting and it
is nontrivial to establish desired theoretical guarantees (e.g., linear speedup, reduced communication
complexity). The main difficulty in the analysis lies at dealing with nonconvex objective function,
non-Lipschitz continuous gradient, and skipping communication rounds simultaneously. Our main
contribution is summarized as the following:
•	We design a novel communication-efficient distributed stochastic local gradient clipping al-
gorithm, namely CELGC, for solving a nonconvex optimization problem under the relaxed
smoothness condition. The algorithm only needs to clip the gradient according to the local
gradient’s magnitude and globally averages the weights on all machines periodically. To the
best of our knowledge, this is the first work proposing communication-efficient distributed
stochastic gradient clipping algorithms under the relaxed smoothness condition.
•	Under the relaxed smoothness condition, we prove iteration and communication complex-
ity results of our algorithm for finding an -stationary point. First, comparing with (Zhang
et al., 2020), we prove that our algorithm enjoys linear speedup, which means that the iter-
ation complexity of our algorithm is reduced by a factor of N (the number of machines).
Second, comparing with naive parallel verion of the algorithm of (Zhang et al., 2020), we
1In this setting, we assume the gradient norm is upper bounded by M such that the gradient is (L0 + L1M)-
Lipschitz. However, we want to emphasize that the original paper of (Ghadimi & Lan, 2013) does not require
bounded gradient assumption, instead they require L-Lipschitz gradient and bounded variance σ2 . Under their
assumption, their complexity result is O ∆L-2 + ∆Lσ2-4 .
2Naive Parallel of (Zhang et al. 2020) is different from our algorithm (CELGC) with I = 1 in that the naive
version requires averaging gradients across all machines to update the model while CELGC only updates the
model using local gradients computed in that machine. This also means that in each iteration, naive version
clips the gradient based on the globally averaged gradient while ours only bases on the local gradient.
2
Under review as a conference paper at ICLR 2022
prove that our algorithm enjoys better communication complexity. Specifically, our algo-
rithm’s communication complexity is smaller than naive parallel clipping algorithm if the
number of machines is not too large (i.e., N ≤ O(1/)). The detailed comparison over
existing algorithms under the same relaxed smoothness condition is described in Table 1.
Please refer to (Koloskova et al., 2020) for local SGD complexity results for L-smooth
functions.
•	We empirically verify our theoretical results by conducting experiments on different neural
network architectures on benchmark datasets and on various scenarios including small to
large batch-sizes, homogeneous and heterogeneous data distributions, and partial participa-
tion of machines. The experimental results demonstrate that our proposed algorithm indeed
exhibit speedup in practice.
2	Related Work
Gradient Clipping/Normalization Algorithms In deep learning literature, gradient clipping
(normalization) technique was initially proposed by (Pascanu et al., 2013) to address the issue of
exploding gradient problem in (Pascanu et al., 2012), and it has become a standard technique when
training language models (Gehring et al., 2017; Peters et al., 2018; Merity et al., 2018). Menon
et al. (2019) showed that gradient clipping is robust and can mitigate label noise. Recently gradient
normalization techniques (You et al., 2017; 2019) were applied to train deep neural networks on
the very large batch setting. For example, You et al. (2017) designed LARS algorithm to train a
ResNet50 on ImageNet with batch size 32k, which utilized different learning rate according to the
norm of the weights and the norm of the gradient.
In optimization literature, gradient clipping (normalization) was used in early days in the field of
convex optimization (Ermoliev, 1988; Alber et al., 1998; Shor, 2012). Nesterov (1984) and Hazan
et al. (2015) considered normalized gradient descent for quasi-convex functions in deterministic
and stochastic cases respectively. Gorbunov et al. (2020) designed an accelerated gradient clipping
method to solve convex optimization problem with heavy-tailed noise in stochastic gradients. Mai
& Johansson (2021) established the stability and convergence of stochastic gradient clipping algo-
rithms for convex and weakly convex functions. In nonconvex optimization, Levy (2016) showed
that normalized gradient descent can escape from saddle points. Cutkosky & Mehta (2020) showed
that adding a momentum provably improves the normalized SGD in nonconvex optimization. Zhang
et al. (2019) and Zhang et al. (2020) analyzed the gradient clipping for nonconvex optimization under
the relaxed smoothness condition rather than the traditional L-smoothness condition in nonconvex
optimization (Ghadimi & Lan, 2013).
However, all of them only consider the algorithm in the single machine setting or the naive parallel
setting, and none of them can apply to FL setting where data on different nodes is heterogeneous
and only limited communication is allowed.
Communication-Efficient Algorithms in Distributed and Federated Learning In large-scale
machine learning, people usually train their model using first-order methods on multiple machines
and these machines communicate and aggregate their model parameters periodically. When the
function is convex, there is scheme named one-shot averaging (Zinkevich et al., 2010; McDonald
et al., 2010; Zhang et al., 2013; Shamir & Srebro, 2014), in which every machine runs an stochas-
tic approximation algorithm and averages the model weights across machines only at the very last
iteration. One-shot averaging scheme is communication-efficient and enjoys statistical convergence
with one pass of the data (Zhang et al., 2013; Shamir & Srebro, 2014; Jain et al., 2017; Koloskova
et al., 2019), but the training error may not converge in practice. McMahan et al. (2017) consid-
ered the Federated Learning setting where the data is decentralized and might be non-i.i.d. across
devices and communication is expensive. McMahan et al. (2017) designed the very first algorithm
for FL (a.k.a., FedAvg), which is communication-efficient since every node communicates with
other nodes infrequently. Stich (2018) considered a concrete case of FedAvg, namely local SGD,
which runs SGD independently in parallel on different works and averages the model parameters
only once in a while. Stich (2018) also showed that local SGD enjoys linear speedup for strongly-
convex objective function. There are also some works analyzing local SGD and its variants on
convex (Dieuleveut & Patel, 2019; Khaled et al., 2020; Karimireddy et al., 2020; Woodworth et al.,
2020a;b; Gorbunov et al., 2021; Yuan et al., 2021) and nonconvex smooth functions (Zhou & Cong,
2017; Yu et al., 2019a;b; Jiang & Agrawal, 2018; Wang & Joshi, 2018; Lin et al., 2018; Basu et al.,
3
Under review as a conference paper at ICLR 2022
2019; Haddadpour et al., 2019; Karimireddy et al., 2020). Recently, Woodworth et al. (2020a;b)
analyzed advantages and drawbacks of local SGD compared with minibatch SGD for convex objec-
tives. Woodworth et al. (2021) proved hardness results for distributed stochastic convex optimiza-
tion. Reddi et al. (2021) introduced a general framework of federated optimization and designed
several federated versions of adaptive optimizers. Zhang et al. (2021) considered to employ gradient
clipping to optimize L-smooth functions and achieve differential privacy. Due to a vast amount of
literature of FL and limited space, we refer readers to (Kairouz et al., 2019) and references therein.
However, all of these works either assume the objective function is convex or L-smooth. To the best
of our knowledge, our algorithm is the first communication-efficient algorithm which does not rely
on these assumptions but still enjoys linear speedup.
3	Preliminaries, Notations and Problem Setup
Preliminaries and Notations Denote k ∙ k by the Euclidean norm. We denote f : Rd → R as the
overall loss function, and fi : Rd → R as the loss function on i-th machine, where i = 1, . . . , N .
Denote Vh(x) as the gradient of h evaluated at the point x, and denote Vh(x; ξ) as the stochastic
gradient of h calculated based on sample ξ.
Definition 1 (L-smoothness). A function h is L-smooth if kVh(x) - Vh(y)k ≤ Lkx - yk for all
x, y ∈ Rd.
Definition 2 ((L0, L1)-smoothness). A second order differentiable function h is (L0, L1)-smooth if
kV2h(x)k ≤ L0 + L1 kVh(x)k for any x ∈ Rd.
Definition 3 (-stationary point). x ∈ Rd is an -stationary point of the function h if kVh(x)k ≤ .
Remark: From definitions, we know that the (L0, L1)-smoothness is strictly weaker than L-
smoothness. To see this, first, we know that L-smooth functions is (L0, L1)-smooth with L0 = L
and L1 = 0. However the reverse is not true. For example, consider the function h(x) = x4, we
know that the gradient is not Lipschitz continuous and hence is not L-smooth, but |h00(x)| = 12x2 ≤
12 + 3 × 4|x|3 = 12 + 3|h0(x)|, so h(x) = x4 is (12, 3)-smooth. Zhang et al. (2019) empirically
showed that the (L0, L1)-smoothness holds for the AWD-LSTM (Merity et al., 2018). In nonconvex
optimization literature (Ghadimi & Lan, 2013; Zhang et al., 2020), the goal is to find an -stationary
point since it is NP-hard to find a global optimal solution for a general nonconvex function.
Problem Setup In this paper, we consider the following optimization problem:
minf (X) = W X fi(X),	⑴
x∈Rd	N
i=1
where N is the number of nodes and each f (x) := Eξi^Di [Fi (x; ξi)] isa nonconvex function where
Di can be possibly different for different i. This formulation has broad applications in distributed
deep learning and FL. For example, in FL setting, fi stands for the loss function on i-th machine,
Di represents the data distribution on i-th machine, and N machines want to jointly optimize the
objective function f .
We make the following assumptions throughout the paper.
Assumption 1.	(i) Each function fi(x) is (L0, L1)-smooth, i.e., kV2fi(x)k ≤ L0 +
L1 kVfi(x)k, for ∀x ∈ Rd and i =1, . . . , N.
(ii)	There exists ∆ > 0 such that f (xo) 一 f ≤ ∆, where f is the global optimal value of f.
(iii)	For all X ∈ Rd, Eξi^Di [VFi(x; ξi)] = Vfi(X), and ∣∣VFi(x; ξ) — Vfi(X)k ≤ σ almost
surely.
(iv)	N PILI ∣Vfi(x) -Vf (x)k ≤ K.
Remark: The Assumption 1 (i) means that that the loss function defined on each machine satis-
fies the relaxed-smoothness condition, and it holds when we want to train a language model using
LSTMs. Assumption 1 (ii) and (iii) are standard assumptions in nonconvex optimization (Ghadimi
& Lan, 2013; Zhang et al., 2019). Note that it is usually assumed that the stochastic gradient is
unbiased and has bounded variance (Ghadimi & Lan, 2013), but we follow (Zhang et al., 2019) to
assume we have unbiased stochastic gradient with almost surely bounded deviation σ. This is an
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Communication Efficient Local Gradient Clipping (CELGC)
1:	for t = 0,..., T do
2:	Each node i samples its stochastic gradient VFi(xt; ξt), where ξ 〜 Di.
3:	Each node i updates it local solution in parallel:
xt+ι = xi - min (η, kVFi(XtSk ) VFi(xi； ξi)	⑵
4:	if t is a multiple of I then
5:	Each worker resets the local solution as the averaged solution across nodes:
1N
Xt = X := N AXt	∀i ∈ {1,...,N}	⑶
6:	end if
7:	end for
stronger assumption than the bounded variance, but it is a normal assumption when encountering
relaxed smoothness. Assumption 1 (iv) quantifies the averaged heterogeneity across nodes, which is
frequently used in the FL literature (e.g., Yu et al. (2019a)).
4	Algorithm and Theoretical Analysis
4.1	Main difficulty and the algorithm design
We briefly present the main difficulty in extending the single machine setting (Zhang et al., 2020) to
the FL setting. In (Zhang et al., 2020), they split the contribution of decreasing objective value by
considering two cases: clipping large gradients and keeping small gradients. If communication is
allowed at every iteration, then we can aggregate gradients on each machine and determine whether
we should clip or keep the averaged gradient or not. However, in FL setting, communicating with
other machines at every iteration is not allowed. This would lead to the following difficulties: (i) the
averaged gradient may not be available to the algorithm if communication is limited, so it is hard
to determine whether clipping operation should be performed or not; (ii) the model weight on every
machine may not be the same when communication does not happen at the current iteration; (iii) the
loss function is not L-smooth, so the usual local SGD analysis for L-smooth functions cannot be
applied in this case.
To address this issue, we design a new algorithm, namely Communication-Efficient Local Gradi-
ent Clipping (CELGC), which is presented in Algorithm 1. The algorithm calculates a stochastic
gradient and then performs multiple local gradient clipping steps on each machine in parallel, and
aggregates model parameters on all machines after every I steps of local updates. Note that the
naive version of the parallel gradient clipping algorithm in (Zhang et al., 2020) needs to aggregates
model parameters and gradients across all machines at every iteration, and perform one step of gra-
dient clipping operation based on the aggregated gradient. Conceptually speaking, our algorithm is
expected to have better performance. The reason is that our algorithm is able to skip communica-
tion rounds, and does not need to transmit gradient information across machines (note that it only
averages the weights). The remaining issue is that the non-asymptotic convergence guarantees are
not established yet. In other words, we aim to establish iteration and communication complexity for
Algorithm 1 for finding an -stationary point. We present our main theoretical results as below.
4.2	Main Results
Theorem 1. Suppose Assumption 1 holds and σ+κ ≥ 1. Take e ≤ min( BAL0,0.1) be a small enough
constant and N ≤ min(1, 54BLLl). In Algorithm 1, choose I ≤ ɪ^, Y ≤ ?&(N;二)min{ ^L-, BL-}
and the fixed ratio γ = 5(σ + K), where A ≥ 1 and B ≥ 1 are constants which will be specified in
the proof, and runAlgorithm 1for T = O (NLO) iterations. Then we have
1T
T EEkVf(Xt)k ≤ 4e.
t=1
5
Under review as a conference paper at ICLR 2022
Remark: We have some implications of Theorem 1. When the number of machines is not large
(i.e., N ≤ O(1/)) and the number of skipped communications is not large (i.e., I ≤ O(1/N)),
then with proper setting of learning rate, we have following observations. First, our algorithm
enjoys linear speedup, since the number of iterations we need to find an -stationary point is di-
vided by the number of machines N when comparing the single machine algorithm in (Zhang
et al., 2020). Second, our algorithm is communication-efficient, since the communication com-
plexity is T/I = O ∆L0 (σ + κ)2-3 , which provably improves the naive parallel gradient clip-
Ping algorithm of (Zhang et al., 2020) with O(∆Loσ2∕(Ne4)) communication complexity when
N ≤ O(1/).
Another important fact is that both iteration complexity and communication complexity only depend
on L0, independent of L1 and the gradient upper bound M . This indicates that our algorithm does
not suffer from slow convergence even if these quantities are large. In addition, local gradient clip-
ping is a good mechanism to alleviate the bad effects brought by a rapidly changing loss landscape
(e.g., some language models such as LSTM).
4.3	S ketch of the Proof of Theorem 1
In this section, we present the sketch of our proof of Theorem 1 and the detailed proof can be
found in Appendix B. The key idea in our proof is to establish the descent property of the sequence
{f (Xt)}T=o in the FL setting under the relaxed smoothness condition, where Xt = N Pt=1 Xt is the
averaged weight across all machines at t-th iteration. The main challenge is that the descent property
of (L0, L1)-smooth function in the FL setting does not naturally hold, which is in sharp contrast to
the usual local SGD proof for L-smooth functions. To address this challenge, we need to carefully
study whether the algorithm is able to decrease the objective function in different situations. Our
main technical innovations in the proof are listed as the following.
First, we monitor the algorithm’s progress in decreasing the objective value according to some novel
measures. The measures we use are the magnitude of the gradient evaluated at the averaged weight
and the magnitude of local gradients evaluated at the individual weights on every machine. Please
note that our algorithm does not have access to the gradient evaluated at the averaged weight, but
it can be served as a proxy in our proof even if we do not have knowledge about it. To this end,
we introduce Lemma 2, whose goal is to carefully inspect how much progress the algorithm makes,
according to the magnitude of local gradients calculated on each machine. The reason is that the
local gradient’s magnitude is an indicator of whether the clipping operation happens or not. For each
fixed iteration t, we define J (t) = {i ∈ {1 : N} : kVFi (xt,ξi)k ≥ γ∕η} and J(t) = {1 : N }\J(t).
Briefly speaking, J(t) contains all machines that perform clipping operation at iteration t and J(t)
is the set of machines that do not perform clip operation at iteration t. In Lemma 2, we perform
one-step analysis and consider all machines with different clipping behaviors at the iteration t. By
considering all cases together and taking the telescoping sum over t = 0, . . . , T, we can get an upper
bound of the gradient in the ergodic sense.
Second, Zhang et al. (2020) inspect their algorithm’s progress by considering the magnitude of
gradient at different iterations, so they treat every iteration differently. However, this approach does
not work in FL setting since one cannot get access to the averaged gradient across machines at every
iteration. Instead, we treat every iteration of the algorithm as the same but consider the progress
made by each machine.
Third, by properly choosing hyperparameters (η, γ, I) and using an amortized analysis, we prove that
our algorithm can decrease the objective value by an sufficient amount, and the sufficient decrease is
mainly due to the case where the gradient is not too large (i.e., clipping operations do not happen).
This important insight allows us to better characterize the training dynamics without worrying too
much about the case that where gradient is large(i.e., the clipping operation is performed).
With the idea mentioned above, now we present how to proceed with the proof in detail.
Lemma 1 characterizes the `2 error between averaged weight and individual weights at t-th iteration.
Intuitively, the `2 error scales linearly in terms of the length of node synchronization interval I .
Lemma 1. UnderAssumption 1,for any i and t, Algorithm 1 ensures ∣∣Xt 一 Xtk ≤ 2γI holds almost
surely.
Lemma 4 and Lemma 5 (included in Appendix A) are some properties of (L0, L1)-smooth functions
and we need to use them frequently in our paper. To make sure they work, we need 2γI ≤ c/L1 for
6
Under review as a conference paper at ICLR 2022
some c > 0. This inequality follows from the choice of parameters in Theorem 1 and details will be
shown in Appendix B. We denote A = 1 + ec - ec-1 and B = ec-1.
Let J(t) be the index set of i such that ∣∣VFi(xt, ξi)k ≥ Y at fixed iteration t, i.e., J(t) = {i ∈
[1,...,N] | ∣∣VFi(xt,ξi)∣ ≥ γ}. Lemma 2 characterizes how much progress We can get in one
iteration of Algorithm 1, and the progress is decomposed into contributions from every machine
(note that J(t) ∪ J(t) = {1,..., N} for every t).
Lemma 2. Let J(t) be the set defined as above. If 2γI ≤ c/L1 for some c > 0,If AL0η ≤ 1/2,
then we have
E[f(xt+ι) - f(Xt)]
≤ɪE X [-2γ∣Vf(Xt)k- 3γ2 + 7γ∣VFi(xi;ξi)-Vf(Xt)k + AL0γ2 + BLIY-V/xt)k + ALy'
N	5	5η	5	2	N
i∈J (t)
+EN X [-2∣Vf(Xt)k2 + 4γ212A2L2η + 4γ2ι2B2LlnkVf(xt)k2 + ALN2σ2 + BLιγ2kVf(xt)k ,
i∈J(t) L	」
where A = 1 + ec - ec-1 and B = ec-1.
cc
Lemma 3 quantifies an upper bound of the averaged `2 error between local gradient evaluated at the
local weight and the gradient evaluated at the averaged weight. The upper bound contains the noise
term in stochastic gradient σ, the data heterogeneity κ, and another error term which scales linearly
with the length of node synchronization interval I.
Lemma 3. Suppose Assumption 1 holds. If 2γI ≤ c/L1 for some c > 0, then we obtain
1N
N EllVFi(Xt; ξJ-Vf(xt)∣∣ ≤ σ + K + 2γI (ALo + BLIkVf(Xt)∣)	almost surely,
i=1
where A = 1 + ec - ec-1 and B = ec-1.
cc
Putting all together Suppose our algorithm runs T iterations. Taking summation on both
sides of Lemma 2 over all t = 0, . . . , T - 1, we are able to get an upper bound of
PT=o1 E [f (xt+1) - f (xt)] = E [f (xτ) - f (xo)]. Note that E [f (xτ) - f (x。)] ≥ -∆ due to
Assumption 1, so we are able to get a upper bound of gradient norm. For details, please refer to the
proof of Theorem 1 in Appendix B.
5	experiments
We conduct extensive experiments to validate the merits of our algorithm in realistic settings and find
the distributed clipping algorithm indeed consistently exhibits substantial speedup compared with
the baseline, which is the naive parallel version of the algorithm in (Zhang et al., 2020). We want
to re-emphasize that the major difference is that the baseline algorithm needs to average the model
weights and local gradients at every iteration while ours only requires averaging the model weights
after I iterations and does not need to average the gradients at all. This immediately suggests that
our algorithm will gain substantial speedup in terms of the wall clock time, which is also supported
by our empirical experiments in this section.
We conduct each experiment in two nodes with 4 Nvidia-V100 GPUs for each node. In our experi-
ments, one "machine" corresponds to one GPU, and we use the word "GPU" and "machine" in this
section interchangeably. We compared our algorithm with the baseline across three deep learning
benchmarks: CIFAR-10 image classification with ResNet, Penn Treebank language modeling with
LSTM, and Wikitext-2 language modeling with LSTM. All algorithms and the the training frame-
work are implemented in Pytorch 1.4. Due to limited computational resources, we choose the same
hyperparameters (learning rates, clipping thresholds) according to the best-tuned baselines unless
otherwise specified. For more results including small and large batch-sizes, heterogeneous data
distribution, and partial participation of machines, we kindly refer readers to the Appendix C and E.
5.1	Effects of skipping communication
We focus on one feature of our algorithm: skipping communication. Theorem 1 says that our
algorithm enjoys reduced communication complexity since every node only communicates with
7
Under review as a conference paper at ICLR 2022
Q.5Q,5Q
ZLL 6 6
SSol 6u'E-(σ4L
200	400	600	800 1000
Wallclock time (second)
(a) Over epoch
Figure 2: Algorithm 1 with different I : Training loss and validation perplexity v.s. (Left) epoch and
(right) wall clock time on training an AWD-LSTM to do language modeling on Penn Treebank.
(a) Over epoch	(b) Over wall clock time
Figure 1: Algorithm 1 with different I : Training loss and test accuracy v.s. (Left) epoch and (right)
wall clock time on training a 56 layer Resnet to do image classification on CIFAR10.
Wallclock time (second)	Wallclock time (second)
(b) Over Wall clock time
---Ours I = 2
—Ours I = 4
---Ours I = 8
---Ours I = 16
——Ours I = 32
---Naive Parallel SGDCIip
200
2Xə一e-əd UoneP=e>
180
160
140
---Ours I = 2
—Ours I = 4
---Ours I = 8
---Ours I = 16
——Ours I = 32
---Naive Parallel SGDCIip
0	100	200
Epoch
6°-o
IOO 200
Epoch
Wallclock time (second)
Wallclock time (second)
.k
(a) Over epoch	(b) Over Wall clock time
Figure 3: Algorithm 1 with different I : Training loss and validation perplexity v.s. (Left) epoch and
(right) wall clock time on training an AWD-LSTM to do language modeling on Wikitext-2.
other nodes periodically with node synchronization interval length I . To study how communication
skipping affects the convergence of Algorithm 1, we run it with I ∈ {2, 4, 8, 16, 32}.
CIFAR-10 classification with ResNet-56. We train the standard 56-layer ResNet (He et al., 2016)
architecture on CIFAR-10. We use SGD with clipping as the baseline algorithm with a stagewise
decaying learning rate schedule, following the widely adopted fashion on training the ResNet archi-
tecture. Specifically, we use the initial learning rate η = 0.3, the clipping threshold γ = 1.0, and
decrease the learning rate by a factor of 10 at epoch 80 and 120. The local batch size at each GPU
is 64. These parameter settings follow that of Yu et al. (2019a).
The results are illustrated in Figure 1. Figure 1a shows the convergence of training loss and test
accuracy v.s. the number of epochs that are jointly accessed by all GPUs. This means that, if the
x-axis value is 8, then each GPU runs 1 epoch of training data. The same convention applied to all
other figures for multiple GPU training in this paper. Figure 1b verifies our algorithm’s advantage
of skipping communication by plotting the convergence of training loss and test accuracy v.s. the
wall clock time. Overall, we can clearly see that our algorithm matches the baseline epoch-wise but
greatly speeds up wall-clock-wise.
Language modeling with LSTM on Penn Treebank. We adopt the 3-layer AWD-LSTM (Merity
et al., 2018) to do language modeling on Penn Treebank (PTB) dataset (Marcus et al., 1993)(word
8
Under review as a conference paper at ICLR 2022
Figure 4: Performance v.s. # of iterations each
GPU runs on training ResNet-56 on CIFAR-10
showing the parallel speedup.
——Naive Parallel SGDCIip
—Single GPU SGDCIip
50	100	150	200
Epoch
(a) ResNet-56
而 M 2.0
O.0.0.S
6u-dd-PM-0 suo≡odo.ld
(b) Penn Treebank
Figure 5: Proportions of iterations in each epoch
in which clipping is triggered v.s. epochs show-
ing clipping is very frequent.
level). We use SGD with clipping as the baseline algorithm with the initial learning rate η = 30 and
the clipping threshold γ = 7.5. The local batch size at each GPU is 3. These parameter settings
follow that of Zhang et al. (2020).
We report the results in Figure 2. Though we slightly fall behind the baseline epoch-wise in terms of
validation perplexity, we do better in training, and gains substantial speedup (2x faster for I = 16)
wall-clock-wise.
Language modeling with LSTM on Wikitext-2. We again adopt the 3-layer AWD-LSTM (Merity
et al., 2018) to do language modeling on Wikitext-2 dataset (Marcus et al., 1993)(word level). We
use SGD with clipping as the baseline algorithm with the initial learning rate η = 30 and the clipping
threshold γ = 7.5. The local batch size at each GPU is 10. These parameter settings follow that
of Merity et al. (2018).
We report the results in Figure 3. We can match the baseline in both training loss and validation
perplexity epoch wise, but we again obtain large speedup (2.5x faster for I = 16) wall-clock-
wise. This, together with the above two experiments, clearly show our algorithm’s effectiveness in
speeding up the training in distributed settings. Another observation is that Algorithm 1 can allow
relatively large I without hurting the convergence behavior.
5.2	Verifying Parallel Speedup
Figure 4 shows the training loss and test accuracy v.s. the number of iterations. In the distributed
setting, one iteration means running one step of Algorithm 1 on all machine; while in the single
machine setting, one iteration means running one step of SGD with clipping. In our experiment,
we use minibatch size 64 on every GPU in distributed setting to run Algorithm 1, while we also
use 64 minibatch size on the single GPU to run SGD with clipping. In Figure 4, we can clearly
find that even with I > 1, our algorithm still enjoys parallel speedup, since our algorithm requires
less number of iterations to converge to the same targets (e.g., training loss, test accuracy). This
observation is consistent with our iteration complexity results in Theorem 1.
5.3	Clipping operation happens frequently
Figure 5 reports the proportion of iterations in each epoch that clipping is triggered. We observe that
for our algorithm, clipping happens more frequently than the baseline, especially for NLP tasks. We
conjecture that this is because we only used local gradients in each GPU to do the clipping without
averaging them across all machines like the baseline did. This leads to more stochasticity of the
norm of the gradient in our algorithm than the baseline, and thus causes more clippings to happen.
This observation highlights the importance of studying clipping algorithms in the distributed setting.
Another interesting observation is that clipping happens much more frequently when training lan-
guage models than image classification models. Hence this algorithm is presumably more effective
in training deep models in NLP tasks.
6 Conclusion
In this paper, we design a communication-efficient distributed stochastic local gradient clipping al-
gorithm to train deep neural networks. By exploring the relaxed smoothness condition which was
shown to be satisfied for certain neural networks, we theoretically prove both the linear speedup
property and the improved communication complexity. Our empirical studies show that our algo-
rithm indeed enjoys parallel speedup and greatly improves the runtime performance due to skipping
communication rounds in various scenarios.
9
Under review as a conference paper at ICLR 2022
References
Ya I Alber, Alfredo N. Iusem, and Mikhail V. Solodov. On the projected subgradient method for
nonsmooth convex optimization in a hilbert space. Mathematical Programming, 81(1):23-35,
1998.
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd
with quantization, sparsification and local computations. In Advances in Neural Information
Processing Systems, pp. 14668-14679, 2019.
Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International Confer-
ence on Machine Learning, pp. 2260-2268. PMLR, 2020.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In
Advances in neural information processing systems, pp. 1223-1231, 2012.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Aymeric Dieuleveut and Kumar Kshitij Patel. Communication trade-offs for local-sgd with large
step size. Advances in Neural Information Processing Systems, 32:13601-13612, 2019.
Yuri Ermoliev. Stochastic quasigradient methods. numerical techniques for stochastic optimization.
Springer Series in Computational Mathematics, (10):141-185, 1988.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
sequence to sequence learning. In International Conference on Machine Learning, pp. 1243-
1252. PMLR, 2017.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-
tailed noise via accelerated gradient clipping. arXiv preprint arXiv:2005.10785, 2020.
Eduard Gorbunov, Filip Hanzely, and Peter Rich也rik. Local sgd: Unified theory and new efficient
methods. In International Conference on Artificial Intelligence and Statistics, pp. 3556-3564.
PMLR, 2021.
Priya Goyal, Piotr Doll狂 Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Lo-
cal sgd with periodic averaging: Tighter analysis and adaptive synchronization. In Advances in
Neural Information Processing Systems, pp. 11080-11092, 2019.
Elad Hazan, Kfir Y Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. arXiv preprint arXiv:1507.02030, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paralleliz-
ing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model
misspecification. Journal of Machine Learning Research, 18:223-1, 2017.
Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In Advances in Neural Information Processing Systems, pp. 2525-
2536, 2018.
10
Under review as a conference paper at ICLR 2022
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtdrik. Tighter theory for local Sgd on identi-
cal and heterogeneous data. In International Conference on Artificial Intelligence and Statistics,
pp. 4519-4529. PMLR, 2020.
Anastasia Koloskova, Sebastian U. Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. In Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
pp. 3478-3487, 2019.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized sgd with changing topology and local updates. In International Confer-
ence on Machine Learning, pp. 5381-5393. PMLR, 2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Kfir Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint
arXiv:1611.04831, 2016.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018.
Vien V Mai and Mikael Johansson. Stability and convergence of stochastic gradient clipping: Be-
yond lipschitz continuity and smoothness. arXiv preprint arXiv:2102.06489, 2021.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Comput. Linguist., 19(2):313-330, June 1993. ISSN
0891-2017.
Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured
perceptron. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, pp. 456-464. Association for
Computational Linguistics, 2010.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. AISTATS, 2017.
Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient
clipping mitigate label noise? In International Conference on Learning Representations, 2019.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM
language models. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=SyyGPP0TZ.
Yurii E Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions.
Matekon, 29:519-531, 1984.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient prob-
lem. corr abs/1211.5063 (2012). arXiv preprint arXiv:1211.5063, 2012.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International conference on machine learning, pp. 1310-1318. PMLR, 2013.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,
2018.
11
Under review as a conference paper at ICLR 2022
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konevcny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. ICLR, 2021.
Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd
Annual AUerton Conference on Communication, Control, and Computing (Allerton), pp. 850-
857. IEEE, 2014.
Naum Zuselevich Shor. Minimization methods for non-differentiable functions, volume 3. Springer
Science & Business Media, 2012.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint
arXiv:1805.09767, 2018.
Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heteroge-
neous distributed learning. arXiv preprint arXiv:2006.04735, 2020a.
Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcma-
han, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International
Conference on Machine Learning, pp. 10334-10343. PMLR, 2020b.
Blake Woodworth, Brian Bullins, Ohad Shamir, and Nathan Srebro. The min-max complexity
of distributed stochastic convex optimization with intermittent communication. arXiv preprint
arXiv:2102.01583, 2021.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 6:12, 2017.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient mo-
mentum SGD for distributed non-convex optimization. In Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
pp. 7184-7193, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019b.
Honglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated composite optimization. In Interna-
tional Conference on Machine Learning, pp. 12253-12266. PMLR, 2021.
Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms
for non-convex optimization. arXiv preprint arXiv:2010.02519, 2020.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881, 2019.
Xinwei Zhang, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu, and Jinfeng Yi. Understanding
clipping for federated learning: Convergence and client-level differential privacy. arXiv preprint
arXiv:2106.13673, 2021.
Yuchen Zhang, John C Duchi, and Martin J Wainwright. Communication-efficient algorithms for
statistical optimization. The Journal of Machine Learning Research, 14(1):3321-3363, 2013.
Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradi-
ent descent algorithm for nonconvex optimization. arXiv preprint arXiv:1708.01012, 2017.
12
Under review as a conference paper at ICLR 2022
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient
descent. In Advances in neural information processing Systems, pp. 2595-2603, 2010.
13
Under review as a conference paper at ICLR 2022
Appendix
A PROPERTIES OF (L0, L1) FUNCTIONS AND USEFUL LEMMAS
Lemma 4 (Descent inequality in Zhang et al. (2020)). Let f be (L0, L1)-smooth, and c > 0 be a
constant. For any xk and xk+1, as long as kxk - xk+1 k ≤ c/L1, we have
f(xk+ι) ≤ f(xk) + (Vf(xk),Xk+1 - Xki + AL0 + BLIkVf(Xk)k kxk+1 - Xkk2,	(4)
where A = 1 + ec - ec-1 and B = ec-1.
cc
Lemma 5 (Zhang et al. (2020)). Let f be (L0, L1)-smooth, and c > 0 be a constant. For any xk
and xk+1, as long as kxk - xk+1k ≤ c/L1, we have
kVf (xk+1) - Vf (xk)k ≤ (AL0 + BL1kVf(xk)k) kxk+1 - xkk,	(5)
where A = 1 + ec - ec-1 and B = ec-1.
cc
Lemma 6 (Zhang et al. (2020)). Let μ ≥ 0 be a real constant. For any vector U and V,
-h；vVi ≤ -μkuk - (I - μ)kvk +(I + μ)kv - uk.	⑹
B	Proof of Theorem 1
During the proof of Theorem 1, we need the following simple fact for our algorithm.
Fact Recall the definition of Xt = 焉 PN=I Xt for a fixed iteration t, Algorithm 1 immediately
gives us Xt+1 — Xt = - N PN=I min (η, gFi(Xi忐)k) VFi(Xt; ξi),andhence kxt+1 - xtk2 ≤ γ2
holds for any t.
B.1	Proof of Lemma 1
Lemma 1 restated Under Assumption 1, Algorithm 1 ensures
IlXt - XiIl ≤ 2γI,∀i,∀t almost surely.	(7)
Proof. Fix t ≥ 1 and i. Note that Algorithm 1 calculates the node average every I iterations. Let
to ≤ t to be the largest multiple of I. Then We know Xt° = Xt for all i. (Note that t 一 to ≤ I). We
further notice that
Xt = Xt0 - χ1 min (η, IVFi(Xt,ξi)k ) VFi(Xt,ξi).
By the definition of Xt, we have
Xt = X to
t-1	N
-X N X min
τ =t0	j=1
η, IVFj(xj,ξj)k
VFj(Xtj,ξtj)
γ
14
Under review as a conference paper at ICLR 2022
Thus, we have
kxt - xik2
2
t-1	γ
T=t0min V, kVFi(Xt,ξi)k
t-1	N
VFi(Xi,ξt)- X NN Xmin
τ=t0	j=1
Y
η, kVFj (xj,ξj)k
VFi(xtj,ξtj)
≤2
t-1	γ
自min (η, kVFi(Xt,ξi)k
VFi(xit,ξti)
2
+2
τ=t0	j=1
2
+
Y
η, kVFj(xj,ξj)k
VFj(xtj,ξtj)
t-1
≤ 2(t - t0) X
τ=t0
kVFi (Xi ,ξi)k )VFi(Xt,劭
2(t — to)
N2
t-1
X
NY
min η,-----------:——:-
j=1	kVFj (Xj ,ξj )k
VFj(xtj,ξtj)
t-1
≤2Y2(t-t0)X
τ=t0
VFi(Xt,ξi)
kVFi(xt,ξi)k
2(t — to)
N2
t-1 N
NXX
τ=t0 j=1
Y2
VFj (xj,ξj)
kVFi(xj,ξj)k
2
+
2
2
≤ 2γ212 + 2^ X X γ2
τ=t0 +1 j=1
≤ 4Y2I2.
(8)
The technique we mainly use in this proof is the inequality k PjK=1 zj k2 ≤ K Pj=IkZj k2 for any
K vectors zι,..., ZK. Taking the square root for both sides，then We obtain the desired result. □
B.2	Proof of Lemma 2
Notethat J (t) = {i ∈ [1,...,N ] | ∣∣VF (xt,ξi)k ≥ Y}.
Lemma 2 restated If ALoη ≤ 1/2 and 2YI ≤ c/L1 for some c > 0, then We have
E[f (Xt+ι) - f (Xt)]
≤ɪE X [-2YkVf(Xt)k- 3Y2 + 7γkVFi(xi;ξi)-Vf(Xt)k + ALoY2 + BLIY2k：f(Xt)k
N	5	5η 5	2
i∈J (t)
+ N X E [-2kVf(Xt)k2 + 4y2I2A2L2η + 4Y212B2L2ηkVf(Xt)k2 + ALN+ BLIYk}(Xt)"
i∈J(t) L
where A =1 + ec- ec-1 andB = ec-.
Proof. By the (Lo, LI)-SmoothneSS and invoking Lemma 4 and noting that kXt+1 一 Xtk ≤ Y for
any t, we have
E [f (Xt+ι) - f (Xt)]
≤E(Vf(Xt),Xt+ι - Xti + (ALO + BLIkVf(Xt)k)E∣∣Xt+ι - Xtk2
≤-YE * NX m； ξi)k,Vf(X t)+-ηE*:X VFi(Xi&Vf(X t)+	⑼
∖	i∈J(t)	i' t''t	i∈Jr(t)	/
+ ALOEkxt+1-Xtk2 + BLIkVf(Xt)kY2.
For each iteration t, we first consider the case where i ∈ J(t), i.e. ∣∣VFi(xt, ξi)k ≥ γ. We use
Lemma 6 with μ = 2 to obtain the following result
〈VFi(Xt,ξi), Vf(Xt)〉/	2y	3y2	7y	i 小
-Y	kVFi(xt; ξi)k	≤ -后kVf(Xt)k- 而 + 可kVFi(xt； ξt) - Vf(Xt)%	(10)
15
Under review as a conference paper at ICLR 2022
Taking the expectation for both sides of above inequality, then we obtain
-γE(飞 X	, Vf(Xt)) ≤ ɪE X [-胃 W(Xt)k- 3γ2 + 7γNFi(Xt; ξi)-Vf(Xt)Il
∖n i∈JOkVFi(Xt,ξi)k	/ n i∈⅛[ 5	5η 5
(11)
Next, we consider the case in which i ∈ J(t), i.e. ∣∣VFi(Xi, ξi)k ≤ γ at fixed iteration t. Denote
ξ[t-1] by the σ-algebra generated by all random variables until (t - 1)-th iteration, i.e., ξ[t-1] =
σ(ξ0, ...,ξN ,ξι,...,ξN,..., ξt- ι, ...,ξN ι). We notice that
-ηE(N X VFi(Xt,ξi), E ":用 Vf(Xt)+ = -ηE ]e(N X VFi(Xt,ξi), E "Nt)口 Vf(Xt)
∖ i∈J(t)	/	|_ ∖ i∈J(t)
ξ[tτj
N X VFi(Xt,ξi) ξ[tT
i∈J(t)
-ηE
EJ≡ Vf(Xt)I
1 N
N X
, i=1
,• , . —...
VFi(Xi,ξi)I(i ∈ J(t))
ξ[t-1]
E
E J(t)∣]	,、\]
JN型Vf(Xt)，
(a)	1 G V7f∕ i' E [∣J(t)∣-]
=-ηE	N 工 PNfKXi) —N- Vf(Xt) /
(b)	ηκ l^ E [∣J (t)l]	、, 1 V7f∕ i∖
=-2E —N— Vf(X t)	+ NTPiVfi(Xt)
Ni=I
1 G y i、	E [∣J(t)∣]	『
NTPiVfi(Xt)--------N- Vf(X t) I I
—
=-"N?"2 E kVf (Xt)k2 - 2E	1N N EPiVfi(Xt)
	i=i
≤ -η(E2N?I)2E kVf (Xt)k2 - 2E	1N nn EPiVfi(Xt) i=i
2
+ 2 E
1N
N EPi(Vf(Xt)-Vfi(Xi))
i=1
2
+ 2Jlɪ NE X II(Vfi(Xt)- Vfi(Xi))II
i=1
2
≤ -η(E2N2)I)2EkVf(Xt)k2 - 2E N XPiVfi(Xt)	+ 2 (EIN2)"E [47212 (AL0 + BLikVf(Xt)II)2]
i= i=1
2
≤ -η(E2N：)“E kVf (Xt)k2 - 2E N XPiVfi(Xt) + 2 (EIN2)I)2E [2γ212 (2A2L2 + 2B2L2kVf(Xt)k2)]
i=i
N 2
≤ - "N?”? E kVf (X t)k2 - 2 E N X PiVfi(Xt)	+ (E%02 E [4γ212A2L0η + 4γ212B2L2η∣Vf (Xt)∣2 ],
i=i
(12)
where (a) holds due to Lemma 8 with Ai = VFi(Xt; ξi), Bi = I(i ∈ J(t)), 0 ≤ Pi ≤ 1 is the
corresponding constant defined in Lemma 8 and PN=i Pi = E [∣ J(t) ∣]; (b) holds by using the basic
identityhXi, X2)= ∣ (IlXIk2 + ∣∣X2k2 一 I∣xi 一 X2∣∣2) for any two vectors Xi, X2; (c) holds due to
Jensen,s inequality and PN=IPi = E∣ J(t)∣. (d) holds due to Lemma 1 and Lemma 5, (e) holds
since (a + b)2 ≤ 2a2 + 2b2.
16
Under review as a conference paper at ICLR 2022
Next, We compute Ekxt+ι - Xtk2. By definition, We have
EkXt+i - Xtk2 = E
N E
≤ 2γ2E
≤ 2Y2 E
≤ N2
1N
N Emink
i=i
2
n, kVFi(Xt;ξi)k ) VFi(Xt,ξi)
2U
kVFi(Xt;ξi)k )VFi(Xi,ξi)
U2
ɪ X VFi(Xt,ξi)
N i∈Jt) kVFi(Xt; ξi)k
2
N
2
N E
∖ X kV≡ + η ,∑「VFi(Xtt； ξti)
i∈J (t)	i∈J (t)
1
+ 2η2E NN EVFi(Xt,ξi)I (i ∈ J(t))
i=1
2
X VFi(Xi ,ξi)
kVF kVFi(Xi；ξi)k
i∈J(t)
N
1 N
+ 2η2E NN EVFi(Xi,ξi)I (i ∈ J(t))-
E
1N
E N ∑>Fi(Xt,ξt)I (i ∈ J(t))
i=i
i=i
2
1N
N EVFi(Xt,ξi)I (i ∈ J(t))
N i=i
2
(≤) 2γ2(E∣J(t)|)2
≤
N
N2
i
+ 2η2E NN X
i=1
N
2
VFi(Xit,ξti)-E
2Y2(EIJ(t)1)2+宇 + 2n2 E
N2
(b) 2γ2(E∣J(t)|)2	2n2σ2
=	+ -ɪ
N2
NX VFi(Xt,ξi) ] +2η2 E
i=1
1N
N EVFi(Xt,ξi)I (i ∈ J(t))
2
1N
nn EVFi(Xi,ξi)I (i ∈ J(t))
i=i
i=i
+ 2η2 E	XN piVfi(Xit)!
(13)
where (a) holds due to Lemma 7 with Ai = VFiX; ξi), Bi = I (i ∈ J(t)); (b) holds due to
Lemma 8 with Ai = VFi(Xt; ξi), Bi = I(i ∈ J(t)), 0 ≤ p ≤ 1 is the corresponding constant
defined in Lemma 8 and PN=Ipi = E [∣ J(t)∣].
Substituting (11), (12) and (13) into (9) yields
E[f (xt+ι) - f (Xt)]
≤ ɪ E X .
一N 乙
i∈J (t)
-Y^ iVf(Xt)k -
5
—
2 E≡ E
+
1N
NEpiVfi(Xt)
ALoY2(E∣J (t)|)
N2
≤ ɪ E X -
一N 乙
i∈J (t)
+NE
35n2 + 7MVFi(Xt； ξi)-Vf(Xt)U
2
i=i
∣2 ALoη2σ2
一 + -N-
—
nEN" EkVf (Xt)k2
+ EJN≡E [4γ212A2L0η + 4γ212B2LfnkVf (Xt)k2]
+
AL0η2 E XN piVfi(Xit)	+
BLiY 2EkVf(Xt)k
2
-2γ kVf(X t)k- 3n+7γ ∣vFi(Xi； ξi)- Vf(Xt)k+AL0γ2+
BLiY 2kVf (X t)k	ALon2
+ N
2
σ2
X ]-2kVf(Xt)k2 + 4γ212A2L2n + 4γ212B2L2n∣∣Vf(Xt)k2 + ALN2σ2 +
i∈J(t) L
BLiY2IIVf(Xt)II]
where the last inequality holds if AL0η ≤ 2 and EkXk2 ≥ ∣∣EXk2.
2
(14)
□
17
Under review as a conference paper at ICLR 2022
B.3 Proof of Lemma 3
Lemma 3 restated Suppose Assumption 1 holds. If 2γI ≤ c/L1 for some c > 0, then we obtain
1N
N EkVFi(xt; ξi)-Vf(Xt)k ≤ σ + K + 2γI (ALo + BLIkVf(Xt)k) almost surely,
i=1
Where A = 1 + ec — U and B = ec-
Proof. By using triangular inequality and Lemma 5, We obtain
kVFi(xt; ξt)-Vf(Xt)k
≤ kVFi(xt；ξi) - Vfi(xi)k + kVfi(xt) - Vf(Xt)k + kVf (xi) - Vf(Xt)k
≤ σ + kVfi(xt) - Vf(Xt)k + 2γI(ALo + BLIkf(Xt)k).
Summing over i = 1 to N and by Assumption 1, We have
1N
N E kVFi(xt; ξi) - Vf (xt)k ≤ σ + K + 2]I(ALO + BLIkf(Xt)k).
N i=1
□
B.4 Main Proof of Theorem 1
Proof. We consider 0 < e ≤ min(BL0, 0.1) be a small enough constant and choose N ≤
min{1, 54A1⅛}, Y ≤ 28(N+κ) min{ AL0, b⅛7}, ratio γ∕η = 5(σ + K) and I ≤ 选，we first
check several important conditions Which Were used in previous Lemmas and We Will use in our
later proof.
Ne	e S)	e ㈤ e 1
0η ≤	0 5(σ + K) ∙ 28(σ + K) AL0 ≤ 140(σ + κ)2 ≤ 140 ≤ 2,
where (a) holds because of Ne ≤ 1 and (b) is true due to σ + K ≥ 1. Note that Ne ≤ 1， σ + K ≥ 1，
A ≥ 1， B ≥ 1， and so we have
Ne 1	1	1 c
2γI ≤ 2≤=,
2 — 28(σ + K) BLi 2Ne — 28Lι	LJ
where C = 28. Recall the descent inequality (Lemma 4), we can explicitly define
A=1+e1/28-
e1/28 - 1
1/28
B
e1/28 - 1
1/28
We can show that our choice of e guarantees that AL ≤ BL-. Then, the upper bound of the
N2
parameter Y becomes 28ALN(σ+κ) ∙ Based on Lemma 2 and Lemma 3, we take summation over t
18
Under review as a conference paper at ICLR 2022
and obtain
-T-1	-
E X f(xt+1) — f(xt)
_t=0	_
(a)	1 T-1	L
≤NEEE [Pi+P2∣∣vf(χt)k]
N t=0 i∈J (t)
1 T-1
+ N XXE
t=0 i∈J(t)
-2 kVf(Xt)k2 +4γ212A2L2η + 4γ212B2L2η∣∣Vf (Xt)/+	+ BL^^Vf(Xt)" 一
(b) 1 T-1
≤ N X E X [P1 + P2kVf(Xt)k]
N t=0 i∈J (t)
+ N X X E I-4IlVf(Xt)Il2 +4Y212A2L0η + AL卢 + BL1γ2kVf(Xt)k
t=0 i∈J(t)	L
where (a) holds due to Lemma 2, and
(15)
P1 =-等 + 7γ (σ + κ) +了也 + AL0 γ2 + AL粤2
5η	5	5	N
2γ	14γ 2IBLr	BL1γ2
P2 =-彳 + ―5— + ^^,
(b)	is derived by computing
4γ212B2L2η ≤
4Ν 2e412B2L1 η
282(σ + κ)2A2L0
4B2L2 e4η	282A2L0	1	η
≤ 282(σ + κ)2A2L2 25B2L1e2	4N2e2	≤ 4
0 `^^} S
bound of N2 bound of I2
(16)
with N ≤ min{ ⅛, 1⅛B}, Y ≤ 28(⅛ min{念,B⅛ } and 1 ≤ &.
Now we give the upper bound of P1 and P2.
From the choice of N ≤ min{ I, IBw }, Y ≤ 28AN⅞+κ), 1 ≤ 选 and the fixed ratio Y
5(σ + κ), and noting that σ + K ≥ 1, we have
3	7/	、 e e ∖	3 z 、	AL0η2σ2
P1 ≤(-5 ∙ 5(σ + κ) + 5(σ + κ) + 20 + 28y∣ γ ≤-10γ(σ + κ) +
21 e	1
P2 ≤ - 5 + 20 +I0j Y ≤-10 Y
(17)
Plugging (17) back to (15), we obtain
1 T-1	1 T-1
-∆ ≤ -(f (X0) - f*) ≤ Ef (XT) - f (X0) ≤ ET X X U(Xt) + - X X V(Xt)
t=0 i∈J(t)	t=0 i∈J(t)
where
U(Xt) = -.YIVf(Xt)I - 130Y(σ + K) + ALN2σ2
V (Xt) = - 4 IVf (Xt )k2 +4y2I 2A2L0η+ ALNL + BLIY(Xt)"
≤ -ηeIVf(Xt)I + BLIY2kVf(Xt)k + ■ +4Y212A2L0η + AL0Nσ2
≤ -η4eIVf(Xt)I + η42 +4y2I2A2L0η + AL。Nσ2.
19
Under review as a conference paper at ICLR 2022
SScn 6u⊂一B=
Naive Parallel SGDCIιp
250 500 750 1000 1250 1500	0	250 500 750 1000 1250 1500
Wallclock time (second)	Wallclock time (second)
(b) Over wall clock time
Figure 6: Training loss and test accuracy v.s. (Left) epoch and (right) wall clock time on training a
32 layer Resnet to do image classification on CIFAR10.
(a) Over epoch
Inequality (a) follows by using the standard inequality x2 ≥ 2x - 2 and (b) is true because
吧 + BLi Y2
^2 + —2—
To check (18), we compute
≤τ
BLIY2 ≤ BLI的(σ + K) 28Al0 (σ + K)
5BL1N2
=η 18ALΓ
(18)
(19)
≤拳
where the last inequality holds because We assume N ≤ 5BAL°
It’s clear that U(x) ≤ V (x). Then we obtain
T-1	T-1
-∆ ≤-(Ef(Xo) - f*) ≤ E X V(Xt) ≤ X
t=0	t=0
Then we rearrange above inequality and get
22
-:EkVf (X t) k + η4- + 4γ212 A2 L2 η + ALo Nσ2
1T
T 工EkVf(Xt)k≤
t=1
4∆ + £ + 4ALoησ2 + 16γ212 A2 L0
< 4,
as long as T ≥ 560:沈。+")2 and i ≤ ɪ .
N	2N
□
C Experiments details and other results
C.1 CIFAR-10 classification with ResNet-32
Apart from the 56 layer Resnet used in Section 5, we also trained the 32 layer Resnet on CIFAR-10.
Here, we used a smaller minibatch size of 16 per GPU. We also used SGD with clipping for the
baseline algorithm with a stagewise decaying learning rate schedule, We set the initial learning rate
η = 0.1 and the clipping parameter Y = 1.0. We decrease both η and Y by a factor of 10 at epoch
80 and 120. These parameter settings follow that of Zhang et al. (2020).
Results reported in Figure 6 again shows that our algorithm can not only match the baseline in both
training loss and test accuracy epoch-wise, but is way better in terms of wall clock time for moderate
values of I .
D Useful Lemmas
Lemma 7. Suppose {Ai }iN=1 is an sequence of vector-valued independent random variables,
{Bi}iN=1 is a sequence of independent random variables where Bi ∈ {0, 1} for every i. Suppose
EkAi k2 < +∞ for every i. Then we have
1 N
E N ɪ2 AiBi- E
i=1
i=1
20
Under review as a conference paper at ICLR 2022
Proof. It suffices to show that E kAiBi - E [AiBi]k2 ≤ E kAi - E [Ai]k2, since both {AiBi -
E [AiBi]}iN=1 and {Ai - E [Ai]}iN=1 are sequences of independent random vectors with zero mean.
Define Ci = 1 - Bi, and hence BiCi = 0 with probablity 1. Without loss of generality, we can
assume E [Ai] = 0. Note that
EkAi-E[Ai]k2 = E kAiBi + AiCik2 =EkAiBik2+EkAiCik2+2EhAiBi,AiCii
=EkAiBik2+EkAiCik2 ≥EkAiBik2 =EkAiBi-E[AiBi]k2+kE(AiBi)k2
≥EkAiBi-E[AiBi]k2.
It completes the proof.	□
Lemma 8. Suppose {Ai }iN=1 is an sequence of vector-valued independent random variables,
{Bi}iN=1 is a sequence of independent random variables where Bi ∈ {0, 1} for every i. Ai and
Bj are independent of each other if i 6= j. Then there exists (p1, . . . , pN) where 0 ≤ pi ≤ 1 and
PiN=1 pi = E hPiN=1 Bii such that
NN
E X AiBi = X piEAi .	(20)
i=1	i=1
Proof. Without loss of generality, we can assume the sequence {Bi }iN=1 is sorted in a decreasing
order, i.e., B1 ≥ B2 ≥ . . . ≥ BN , since otherwise we can switch the order of summation in both
LHS and RHS of the Equation (20).
Define Te = sup{1 ≤ i ≤ N : Bi = 1}, and define FS = σ(Aι, Bι,...,As,Bs), where σ(∙)
denotes σ-algebra generated by random variables in the argument. Since {Te = s} ∈ Fs for all s,
we know that T is a stopping time, and T ≤ N almost surely. Hence we have
N
E X AiBi
i=1
Te
XAi
i=1
+∞
E X AiI(Te ≥ i)
i=1
+∞
XE E AiI(Te ≥ i)|Ft-1 .
i=1
E
ʌ τ , ,ι , τr f rr∖ ∙ ∖ i ττ / rri ， ∙ -ι ∖ _ -r^	i Λ ∙ ∙ i	i , r∙ -r-	ι	. ι ,
Note that I(T ≥ i) = 1 - I(T ≤ i - 1) ∈ Fi-1, and Ai is independent of Fi-1, we know that
+∞	+∞	+∞	N
XE E	AiI(Te	≥ i)|Ft-1	=XE	I(Te	≥	i)E [Ai]	=XE[Ai]Pr(Te	≥ i)	=XE[Ai]Pr(Te≥ i).
i=1	i=1	i=1	i=1
Define pi = Pr(Te ≥ i), we have 0 ≤ pi ≤ 1 and PiN=1 pi = E(Te) = E PiN=1 Bi . Hence (20) is
proved.	□
E	More experiment results
E.1 Parallel Speedup on NLP Tasks
In Section 5.2 we have shown the parallel speedup effects of our algorithm on training a Resnet
model on the CIFAR10 dataset. Here we present the same phenomena but on training AWD-LSTMs
on doing language modeling on Penn Treebank and Wikitext-2 in Figure 7.
Again, note that in the distributed setting, one iteration means running one step of Algorithm 1 on
all machine; while in the single machine setting, one iteration means running one step of SGD with
clipping. For Penn Treebank, we use minibatch size 3 on every GPU in distributed setting to run
Algorithm 1, while we also use 3 minibatch size on the single GPU to run SGD with clipping. For
Wikitext-2, the corresponding minibatch size is 10.
Figure 7 again clearly shows that even with I > 1, our algorithm still enjoys parallel speedup,
since our algorithm requires less number of iterations to converge to the same targets (e.g., training
loss, validation perplexity). This observation is consistent with our iteration complexity results in
Theorem 1.
21
Under review as a conference paper at ICLR 2022
(a) Penn Treebank
.5,0,5,0,5,0,5
6.6.5,5.4.4.3.
sso—I6u∙≡q=
(b) Wikitext-2
0	200k 400k 600k 800k Im
Iterations
Figure 7:	Performance v.s. # of iterations each GPU runs on training AWD-LSTMs to do language
modeling on Penn Treebank (left) and Wikitext-2 (right) showing the parallel speedup.
10~1
E」。N N-IUSPQJd
----Ours I = 2
O
50 IOO 150	200
Epoch
Figure 8:	`2 norm of gradients over epoch on training ResNet-56 on CIFAR-10.
E.2 Gradient Norm Dynamics
The careful reader might notice that in Theorem 1 We require N ≤ O (ɪ) where e is the desired '2
norm of the gradient when converging and wonder if this is too strict and impractical.
Therefore, we draw the dynamics of the `2 norm of gradients over epochs for training the Resnet-56
model to do image classification on CIFAR10 in Figure 8. It can be clearly seen that the gradient
`2 norm decreases to less than 0.1 after epoch 100 and eventually reaches around 0.06 at the epoch
200. Since 1/0.06 ≈ 17, this means that the N = 8 setting we used in our experiments is consistent
with our theory.
E.3 Heterogeneous Setting
In the previous experiments, all machines can access the whole dataset. Yet, in prac-
tice, each machine might only access a subset of the whole dataset and the data
might not be evenly distributed across machines. Therefore, we conduct an experi-
ment on this heterogeneous setting. Specifically, we take the Penn Treebank training set
which is a text file and divide it into 8 non-overlapping consecutive parts which contain
{10.64%, 11.17%, 11.70%, 12.23%, 12.77%, 13.30%, 13.83%, 14.36%} of the whole text file re-
spectively. We then train the AWD-LSTM of the same structure as in Section 5.1 to do language
modeling. Considering that the heterogeneous setting introduces big difference compared with the
setting we adopted in Section 5.1, we finely tuned the initial learning rate for both the baseline and
our algorithm and picked the one that gives the smallest training loss.
The results are reported in Figure 9. Note that, as the size of training data in each machine is
different, we draw the training and testing performance curves over the number of iterations each
machine runs instead of epochs. This means that, if the x-axis value is 1000, then each GPU runs
1000 iterations. We can see that, though the unbalancing and heterogeneity of the training data
deteriorates the performance for both the baseline and our algorithm as compared with Figure 2a,
our algorithm is still able to surpass the baseline in training losses though it falls slightly behind
in testing; meanwhile, our algorithm obtains significant speedup in terms of wall-clock time. This
suggests that our algorithm is robust to the heterogeneous setting.
22
Under review as a conference paper at ICLR 2022
5.5.4.4.3.
SSol 6u-UE
Ok 20k 40k 60k 80k 100k 120k
Iteration
Ooooooo
0 8 6 4 2 0 8
2 11111
至 XθBφd Uon
Ok 20k 40k 60k 80k 100k 120k
Iteration
3.5
0 3k 6k 9k 12k 15k 18k
Wallclock time (second)
Wallclock time (second)
AEXqdSd UOAeP=e>
(a)	Over epoch
(b)	Over Wall clock time
Figure 9: Training an AWD-LSTM to do language modeling on Penn Treebank in a heterogeneous
setting where each node accesses only a different subset of the whole dataset.
(a) CIFAR10
6.5
Naive Parallel SGDCIιp
10000 15000 20000
Communication rounds
10000 15000 20000
Communication rounds
(b) Penn Treebank
AaX①-d」① d UoAeP=(D>
200
(c) Wikitext-2
Figure 10: Performance v.s. # of communication rounds each GPU conducts on training (a) Resnet-
56 on CIFAR10 (b) AWD-LSTM on Penn Treebank (c) AWD-LSTM on Wikitext-2.
E.4 Performance vs. Communication Rounds
The reader might be interested in how the training and testing performance changes w.r.t. the number
of communications each machine does for both the Naive Parallel SGDClip and our CELGC with
different I. Thus, we show such results in Figure 10. The results are very close to that w.r.t. the
wall-clock time which is as expected.
E.5	Partial Participation
In the previous experiments, when communication between nodes occurs, all nodes send their model
and/or gradients for averaging and update accordingly. Yet, in practice, it might be that only a (dif-
ferent) subset of nodes participate in communication each time. Therefore, we conduct an exper-
iment on this setting. Specifically, we take the Penn Treebank dataset and train the AWD-LSTM
of the same structure as in Section 5.1 to do language modeling. However, when the algorithm
requires a communication across nodes, we randomly select 6 out of the total 8 nodes to commu-
nicate and average the model and/or gradient and update those 6 nodes only; the rest 2 nodes will
be left untouched. Considering that this setting introduces big difference compared with the setting
we adopted in Section 5.1, we finely tuned the initial learning rate for both the baseline and our
algorithm and picked the one that gives the smallest training loss.
The results are reported in Figure 11. We can see that, though the partial participation setting slightly
deteriorates the performance for both the baseline and our algorithm as compared with Figure 2a,
our algorithm is still able to closely match the baseline for both the training loss and the validation
23
Under review as a conference paper at ICLR 2022
(a) Over epoch
(b) Over wall clock time
Figure 11: Train an AWD-LSTM to do language modeling on Penn Treebank where only a subset
of nodes participate in each communication.
(a) Over epoch	(b) Over wall clock time
(c) Over # of iterations each GPU runs
Figure 12: Algorithm 1 with different I: Training loss and test accuracy v.s. (a) epoch, (b) wall clock
time, and (c) # of iterations each GPU runs on training a 56 layer Resnet to do image classification
on CIFAR10.
perplexity epoch-wise, but also note that our algorithm greatly saves wall-clock time. This indicates
that our algorithm is robust in the partial participation setting.
E.6 Large Mini-batch
The distributed learning paradigm is especially desirable with training using large mini-batches. To
this end, we trained a Resnet-56 model on CIFAR10 with batch-size 256 on each node which sums
up to a batch-size of 2048 globally. Compared with the experment in Section 5.1, considering the
large batch-size, we trained for 300 epochs instead of 200, decrease the learning rate by a factor of
10 at epoch 150 and 250, and finely tuned the initial learning rate for each variant and picked the
one that gives the smallest training loss. Results are shown in Figure 12. Similar to small batch case
reported in Figure 1, our algorithm with skipped communication is able to match the naive parallel
SGDClip in terms of epochs but greatly speeds up wall-clock-wise. Also, Figure 12c again verifies
the parallel speed-up property of our algorithm.
24