Under review as a conference paper at ICLR 2022
Understanding Overfitting in Reweighting
Algorithms for Worst-group Performance
Anonymous authors
Paper under double-blind review
Ab stract
Prior work has proposed various reweighting algorithms to improve the worst-
group performance of machine learning models for fairness. However, Sagawa
et al. (2020a) empirically found that these algorithms overfit easily in practice un-
der the overparameterized setting, where the number of model parameters is much
greater than the number of samples. In this work, we provide a theoretical back-
ing to the empirical results above, and prove the pessimistic result that reweighting
algorithms always overfit. Specifically we prove that with reweighting, an over-
parameterized model always converges to the same ERM interpolator that fits all
training samples, and consequently its worst-group test performance will drop to
the same level as ERM in the long run. That is, we cannot hope for reweighting
algorithms to converge to a different interpolator than ERM with potentially better
worst-group performance. Then, we analyze whether adding regularization helps
fix the issue, and we prove that for regularization to work, it must be large enough
to prevent the model from achieving small training error. Our results suggest that
large regularization (or early stopping) and data augmentation are necessary for
reweighting algorithms to achieve high worst-group test performance.
1	Introduction
It has been well established by prior work that overparameterized models, whose number of pa-
rameters is much larger than the number of training samples, can empirically achieve high test
performance on a variety of tasks, in contrast to the theory that models with too many parameters
could have large generalization error.
This high performance however is on average; a large body of prior work (Hovy & S0gaard, 2015;
Blodgett et al., 2016; Tatman, 2017) showed that these models tend to learn spurious features, such
as learning the background in image classification instead of the object, and learning keywords like
“not” in language sentiment analysis instead of really understanding the sentences. Consequently,
these models are unfair, i.e. they fail on certain minority groups (such as positive sentences con-
taining “not”) while still having high average-case performance. To solve this problem, people have
proposed various reweighting algorithms to improve the model’s worst-group performance, such as
upweighting the minority groups or using distributionally robust optimization (DRO) based methods
(Shimodaira, 2000; Hashimoto et al., 2018; Duchi & Namkoong, 2018; Sagawa et al., 2020a).
While reweighting algorithms in principle can improve the worst-group performance compared to
vanilla empirical risk minimization (ERM), previous work empirically found that when applied to
modern overparameterized models, these methods could overfit very easily, so that they have poor
test worst-group performance. For example, Sagawa et al. (2020a) studied a reweighting algorithm
called group DRO. They found that compared to ERM, group DRO does improve the worst-group
test accuracy by a large margin at the early stage of training. However, if no regularization is
applied, then as training goes on, the worst-group test accuracy of group DRO will drop signifi-
cantly and eventually to a level almost the same as ERM. Some previous work tried to explain why
reweighting algorithms can overfit so easily. For instance, Sagawa et al. (2020b) argued that with
these algorithms, an overparameterized model would typically memorize all training samples in the
minority groups while still learning the spurious features from the majority groups.
In this work, we aim to understand the overfitting phenomenon in reweighting algorithms by study-
ing their implicit biases. Specifically, we prove for a family of overparameterized neural networks
1
Under review as a conference paper at ICLR 2022
that for almost all reweighting algorithms, the model always converges to the same interpolator that
fits all training samples, no matter the reweighting. Since ERM is a special case of such reweight-
ing algorithms (where each sample receives the same weight), this means that the implicit biases
of all reweighting algorithms are equivalent to that of ERM. Consequently, the model trained by
any reweighting algorithm always overfits to the ERM interpolator, so we cannot hope for its worst-
group test performance to be better than ERM. In short, reweighting algorithms always overfit.
Given this pessimistic result, we analyze whether regularization can help mitigate overfitting, as pro-
posed by Sagawa et al. (2020a). We find that a necessary condition for regularization to work is that
it considerably lowers the training performance. Specifically, we prove that if the overparameterized
model trained by a reweighting algorithm with regularization can still perform almost perfectly on
the training set, then overfitting is still inevitable. This explains why in practice we need very large
regularization that prevents the model from achieving nearly zero training error to avoid overfitting.
Our results have two important consequences for practice: (i) We should always use large regular-
ization or early stopping when optimizing for worst-group performance; (ii) We should always try
to obtain more training samples, e.g. with strong data augmentation or semi-supervised learning.
1.1	Related work
Group fairness. Group fairness in machine learning was first studied in Hardt et al. (2016) and
Zafar et al. (2017), where they required the model to perform equally well over all groups. Later,
Hashimoto et al. (2018) studied another type of group fairness called Rawlsian max-min fairness
(Rawls, 2001), which does not require equal performance but rather requires high performance on
the worst-off group. The problem we study in this paper is most closely related to Rawlsian max-min
fairness. A large body of recent work in machine learning have studied how to improve this worst-
group performance (Duchi & Namkoong, 2018; Oren et al., 2019; Xu et al., 2020; Liu et al., 2021;
Zhai et al., 2021). Recent work however observe that these approaches, when used with modern
overparameterized models, easily overfit (Sagawa et al., 2020a;b). Apart from group fairness, there
are also other notions of fairness, such as individual fairness (Dwork et al., 2012; Zemel et al., 2013)
and counterfactual fairness (Kusner et al., 2017), which we do not study in this work.
Implicit bias under the overparameterized setting. For overparameterized models, there could
be many model parameters which all minimize the training loss. In such cases, it is of interest to
study the implicit bias of specific optimization algorithms such as gradient descent i.e. to what train-
ing loss minimizer the model parameters will converge to (Du et al., 2019; Allen-Zhu et al., 2019).
Our results use the NTK formulation of wide neural networks (Jacot et al., 2018), and specifically
we use linearized neural networks to approximate such wide neural networks following Lee et al.
(2019). There is some criticism of this line of work, e.g. Chizat et al. (2019) argued that infinitely
wide neural networks fall in the “lazy training” regime and results might not be transferable to gen-
eral neural networks. Nonetheless such wide neural networks are being widely studied in recent
years, since they provide considerable insights into the behavior of more general neural networks,
which are typically intractable to analyze otherwise.
2	Preliminaries
Consider a data domain X × Y ⊆ Rd × R that consists of K groups (subdomains)1, where each
data point belongs to one of the groups2. We assume that the input space X is a subset of the unit
ball of Rd, such that any x ∈ X satisfies kxk2 ≤ 1. We are given a training set {(xi, yi)}in=1 i.i.d.
sampled from some underlying distribution P over XXY. Let the K groups be Di,…，DK where
each Di is a subset of X × Y. Let Pk(z) = P (z|z ∈ Dk) be the conditional data distribution
over Dk, where Z = (x, y). Denote X = (xi,…,Xn) ∈ Rd×n, and Y = (yi,…,yn) ∈ Rn;
for any function g : X → R, we overload notation and use g(X) = (g(xι),…，g(xn)) . Let
the loss function be ` : Y × Y → [0, 1]. In vanilla training, the goal is to minimize the expected
risk denoted by R(f; P) = Ez〜P['(f (x),y)], which is done by minimizing the empirical risk
R(f ) = 1 Pn=i '(f(χi),yi).
1We prove our results for Y ⊆ R, but our results can be easily extended to the multi-class scenario Y ⊆ Rm.
2This is the non-overlapping setting. There is also the overlapping setting where groups can overlap with
each other. We focus on the non-overlapping setting in this paper.
2
Under review as a conference paper at ICLR 2022
For tasks requiring high worst-group performance, the goal is to train a model f : X → Y that
performs well over every Pk, which can be achieved by minimizing the worst-group risk defined as
Rmax(f; P )= max R(f ； Pk )= max Ez~p ['(f(x),y)∣z ∈ Dk ]	(1)
k=1,…，K	k = 1,…，K
2.1	Reweighting Algorithms
Most existing methods that minimize the worst-group risk are reweighting algorithms that assign
each sample with a weight during training and minimize the weighted average risk. At time t, we
assign a weight qi(t) to sample zi, and minimize the weighted empirical risk:
n
R q(t)(f )= X q(t)'(f (Xi),yi)	(2)
i=1
where q⑴ =(q(t),…，q^) and q(t +---------+ q?) = 1.
A static reweighting algorithm assigns to each zi = (xi, yi) a fixed weight qi that does not change
during training, i.e. qi(t) ≡ qi . A famous example is Importance Weighting (IW, Shimodaira (2000)),
in which if zi ∈ Dk and the size of Dk is nk, then qi = (Knk)-1. Under IW, each group has the
same weight, and the reweighted empirical risk is a simple (unweighted) average of the empirical
risk over each group, so that each group has an equal contribution to the overall risk objective. Note
that ERM is also a special case of static reweighting algorithms: by assigning qι = •…=qnr = 1/n.
On the other hand, in a dynamic reweighting algorithm, q(t) changes with t. Specifically, it up-
weights samples over which the model has a high risk in order to help the model learn “hard”
samples. A popular dynamic reweighting algorithm is Group DRO (Sagawa et al., 2020a). De-
note the empirical risk over group k by Rk (f), and the model at time t by f (t). Group DRO sets
qi(t) = gk(t)/nk for all zi ∈ Dk where gk(t) is the group weight that is updated by
gkt) Y gkt-1) exp (VRk(f (t-1)))	(∀k = 1,…，K)	(3)
for some ν > 0, and then normalized so that q(t) + …+ ql) = 1. Sagawa et al. (2020a) proved a
convergence rate theorem (their Proposition 2) showing that in the convex setting, the worst-group
training risk of Group DRO converges to the global minimum with the rate O(t-1/2).
There are many other reweighting algorithms. Particularly, all variants of DRO and DRO-based
methods like CVaR and χ2-DRO are reweighting algorithms. See Appendix A for more examples.
2.2	Reweighting algorithms can easily overfit
In this section, we will empirically demonstrate that while IW and Group DRO can achieve higher
worst-group test performances than ERM at the early stage of training, they can easily overfit after
a number of training epochs.
Following Sagawa et al. (2020a), we conduct the experiment on two datasets: Waterbirds and
CelebA. Each dataset contains a binary confounding variable a and a binary target variable y, di-
viding the dataset into four groups (four combinations of (a, y)). In Waterbirds y is the type of the
bird and a is the background; In CelebA y is whether the person has blond hair and a is whether the
person is male. On each dataset, a model trained by ERM always exhibits a very strong empirical
correlation between y and a, so its performance on one of the groups is extremely poor. The goal
is to make the model perform well on every group. See Appendix C.1 of Sagawa et al. (2020a) for
detailed information of these datasets.
On each dataset, we use the ResNet18 model as the classifier and optimize it with momentum SGD.
We run each of the three algorithms: ERM, IW and group DRO (GDRO), for 500 epochs on Wa-
terbirds and 200 epochs on CelebA, and plot the average training/test and worst-group (WG) train-
ing/test accuracy curves throughout training in Figure 1. From the plots we can conclude that:
• All algorithms can achieve and maintain high average training/test accuracy throughout
training, i.e. there is almost no overfitting in the average test accuracy.
3
Under review as a conference paper at ICLR 2022
Aoe,lnooα 6≡u5,ll φrora∙-φ><
ERM
IW
GDRO
0s
e-n。OV01 φrora∙-φ><
(b) Avg test acc.	(c) WG train acc.	(d) WG test acc.
(a) Avg train acc.
300	400
Epochs
Aoe-n。OV 6U-U_e」l φrora∙-φ><
(e) Avg train acc.	(f) Avg test acc.	(g) WG train acc.	(h) WG test acc.
Figure 1: Performances of ERM, IW and Group DRO. First row: Waterbirds. Second row: CelebA.
• Regarding the worst-group test accuracy, while the two reweighting algorithms outperform
ERM by a large margin at the early epochs, they overfit very quickly. On CelebA after
roughly 100 epochs, the worst-group test accuracies of the two reweighting algorithms
become the same as ERM. On Waterbirds, the worst-group test performances of IW and
Group DRO drop significantly after around 30 epochs though they are still better than ERM.
3 Implicit biases of reweighting algorithms
In the previous section, we empirically demonstrated that the worst-group test performances of
reweighting algorithms converge to the same level as ERM. To theoretically understand why this
happens in practice, we analyze the implicit biases of reweighting algorithms. Our main theorem
(Theorem 6) states that almost all reweighting algorithms (including ERM) have equivalent implicit
biases, in the sense that they converge to the same interpolator. Meanwhile, it is observed in practice
that the ERM interpolator has a poor worst-group test performance. This leads to the pessimistic
result that reweighting algorithms always overfit. All proofs can be found in Appendix B.
3.1	Linear models
We first demonstrate this pessimistic result on simple linear models to provide our readers with a
key intuition, and later we will apply this same intuition to neural networks. Let the linear model be
f(x) = hθ, xi, where θ ∈ Rd. In the overparameterized setting, we have d > n. Consider using the
squared loss '(y, y) = 2 (y- y)2, and minimizing the weighted empirical risk with gradient descent:
n
θ(t+1) = θ(t) - η X q(t)Vθ'(f (t)(xi), y)	(4)
i=1
where η > 0 is the learning rate. For a linear model with the squared loss, the update rule is
n
θ(t+1) = θ(t) - ηXqi(t)xi(f(t)(xi) - yi)	(5)
i=1
It is a well known result that under the overparameterization setting where d > n,if xι, ∙∙∙ , Xn are
linearly independent, then with a sufficiently small η, a linear model trained by ERM can always
converge to an interpolator which fits all training samples (i.e. θ(t) → θ* such that (θ*, Xii = yi
for all i). Here the linear independence is necessary, because otherwise in the extreme case where
X1 = X2 but y1 6= y2, the model cannot fit (X1, y1) and (X2, y2) simultaneously.
In this section, we aim to extend this ERM convergence analysis to general reweighting algorithms.
Our results require the following assumption:
Assumption 1. There exist constants qι, ∙∙∙ ,qn such that for all i, qi(t → qir as t → ∞. And
mini qi = q* > 0.
4
Under review as a conference paper at ICLR 2022
This assumption avoids the scenario where there is some i such that qi(t) ≈ 0 for all t, in which case
the model could never fit zi . Assumption 1 empirically holds for Group DRO on Waterbirds and
CelebA (see Appendix D.2). Under this assumption, we can prove that the model always converges
to an interpolator:
Theorem 1.	For any reweighting algorithm satisfying Assumption 1, if xι, ∙∙∙ , Xn are linearly
independent, then there exists an η0 > 0 such that for any η ≤ η0, as t → ∞, θ(t) converges to
some interpolator θ* such that forall i, (θ*, Xii = yi.
We now make the following key observation regarding the update rule (5): θ(t+1) - θ(t) is a lin-
ear combination of xι, ∙∙∙ , Xn for all t, and thus θ(t) - θ(0) always lies in the linear subspace
SPan(Xι,…,Xn). Note that this is an n-dimenSionaI linear subspace if xι, ∙∙∙ , Xn are linearly in-
dependent, and by Cramer,s rule, there is exactly one θ in this subspace such that(石+ θ(0), Xii = yi
for all i, which implies that θ* = θ + θ(0) is unique. Together with Theorem 1, this leads to:
Theorem 2.	If Xi, ∙∙∙ , Xn are linearly independent, then there exists no > 0 such that for any
reweighting algorithm satisfying Assumption 1, and any η ≤ η0, θ(t) converges to the same interpo-
lator θ* that does not depend on q(t.
Note that ERM is also a reweighting algorithm satisfying Assumption 1. Therefore, we have essen-
tially proved the following result: The implicit bias of any reweighting algorithm satisfying Assump-
tion 1 is equivalent to ERM, so reweighting algorithms always overfit3.
The key intuition here is that no matter what reweighting algorithm we use, θ(t) - θ(0) always lies
in a low-dimensional subspace, in which the interpolator is unique. Therefore, as long as a model
trained by the algorithm converges to some interpolator, it must converge to that unique interpolator,
which means that the implicit bias of the algorithm is equivalent to ERM.
3.2 Linearized neural networks
Now we prove the same result for neural networks. Of course it would be very hard to prove it
for all neural networks. However, we can prove the result for a family of overparameterized neural
networks that can be approximated by their linearized counterparts Lee et al. (2019). Denote the
neural network at time t by f(t) (X) = f(X; θ(t)) which is parameterized by θ(t) ∈ Rp where p is
the number of parameters. The linearized neural network of f(t) (X) is defined as
flin)(X) = f ⑼(x) + hθ(t) - θ(0), Vθf (0)(X)i	(6)
where we use the shorthand Vθf (O)(X) := Vθf (x; θ)∣θ=θθ. Consider training f(n) (x) via gradient
descent on the reweighted risk (as in (4)) using the squared loss. Given a training set {(Xi, yi)}in=1,
we can construct a new training set { Vθf(0) (Xi), yi - f(0) (Xi) }in=1, so that training a linearized
neural network on the original training set is equivalent to training a linear model on the new training
set. Based on this observation, we have the following corollary of Theorem 2:
Corollary 3. If Vθf(O)(xi),…，Vθf (O)(Xn) are linearly independent, then there exists no > 0
such that for any reweighting algorithm satisfying Assumption 1, and any η ≤ η0, θ(t) converges to
the same interpolator θ* that does not depend on q%.
Here we are still using the key intuition: θ(t) - θ(O) always lies in the n-dimensional linear subspace
span (Vθf (O)(Xi),…,V f (O)(Xn)). By Cramer,s rule, there is a unique interpolator θ* such that
θ* - θ(0) ∈ span (Vθf(O)(xi),…，Vθf (O)(Xn)), and θ(t) always converges to that θ*. Thus, We
have essentially proved that for linearized neural networks, reweighting algorithms always overfit.
Now let us delve deeper into the training dynamics of a linearized neural network. Note that
Vθfl(int)(X) = Vθf(O) (X) ∈ Rp×n, so the change in the training function value vector is
f(t+I)(X) - f1(t)(x ) = -n Vθ f (O)(X )>Vθ f (O)(X )Q(t) Vy'(fIin)(X), Y)	⑺
3By overfit, we are saying that the training error of the model trained by the reweighting algorithm will
converge to zero, but the worst-group test performance will converge to the same low level as ERM.
5
Under review as a conference paper at ICLR 2022
where Q(t) = diag(q(t),…,q(t)). The function value vector moves along the kernel gradient with
respect to Θq0t) = Vθf(0)(X)>Vθf(0)(X)Q(t). Meanwhile, the neural tangent kernel (NTK,
Jacot et al. (2018)) is Θ(0)(x, x0) = Vθf(0)(x)>Vθf(0)(x0) , and the Gram matrix is Θ(0) =
Θ(0) (X, X), so Θ(q0()t) = Θ(0)Q(t). We can thus extend our result for gradient descent on linearized
neural networks to a kernel gradient descent algorithm as above.
3.3 Wide fully-connected neural networks
Now we prove the result for sufficiently wide fully-connected neural networks, which can be ap-
proximated by the linearized neural networks. First we define a fully-connected neural network
with L hidden layers (we always assume L ≥ 1 so there is at least one hidden layer). Let hl and
xl be the pre- and post-activation outputs of layer l, and dl be the width of layer l. Let x0 = x and
d0 = d. Define the neural network as
hl+1 = W xl + βbl
√dl
xl+1 = σ(hl+1)
(l = 0,…，L)
(8)
where σ is a non-linear activation function, Wl ∈ Rdl+1 ×dl and WL ∈ R1×dL. The parameters θ
consist of W0, ∙ ∙ ∙ , W L and b0, ∙ ∙ ∙ ,bL (θ is the concatenation of all flattened weights and biases).
The final output of the neural network is f(x) = hL+1. And let the neural network be initialized as
(Wilj0)~N(0,1)
[bj(0) 〜N(0,1)
(l = 0,…，L — 1)
and
WiLj(0) = 0
i,j
IbL ⑼〜N(0,1)
(9)
We also need the following assumption for our approximation theorem:
Assumption 2. σ is differentiable everywhere, and both σ and σ are Lipschitz.4
Difference from Jacot et al. (2018). Our initialization (9) is different from the original one in
Jacot et al. (2018) in the last (output) layer. For the output layer, we use the zero initialization
WL(0) = 0 instead of the Gaussian initialization WL(0) 〜N(0,1). This modification enables US
to accurately approximate the neural network with its linearized counterpart (6), as we notice that
the proofs in Lee et al. (2019) (particularly the proofs of their Theorem 2.1 and their Lemma 1 in
Appendix G) are flawed. In Appendix C we will explain what goes wrong in their proofs and how
we manage to fix the proofs with our modification.
For our new initialization, we still have the following NTK theorem:
Theorem 4. If σ is Lipschitz and dl → ∞ for l = 1, ∙ ∙ ∙ ,L sequentially, then Θ(0) (x, x0) converges
in probability to a non-degenerated5 deterministic limiting kernel Θ(x, x0).
The kernel Gram matrix Θ = Θ(X, X) ∈ Rn×n is a positive semi-definite symmetric matrix.
Denote its largest and smallest eigenvalues by λmax and λmin . Note that Θ is non-degenerated,
so we assume that λmin > 0 (which holds almost surely in the overparameterized setting where
dL n). Then, we can prove the following approximation theorem:
Theorem 5 (Approximation Theorem). Let η* = (λmin + λmax)-1. For a fully-connected neural
network f(t) that satisfies Assumption 2 and is trained by any reweighting algorithm satisfying
Assumption 1, let fl(int) be its linearized neural network which is trained by the same reweighting
(t)
algorithm (ι.e.	N%,t,	qi)are the same for both networks).	If di	=	d?	= •…=dL	= d and
λmin > 0, thenfor any δ > 0, there exists D > 0 and a COnStant C such that as long as η ≤ η* and
d ≥ D, for any test point X ∈ Rd such that ∣∣xk2 ≤ 1, with probability at least 1 一 δ over random
initialization,
SupWn)(X)- f(t)(x)∣ ≤ CdT/4	(10)
4f is Lipschitz if there exists a constant L > 0 such that for any x1, x2, |f(x1) - f(x2)| ≤ L kx1 - x2 k2.
5Non-degenerated means that Θ(x, x0) depends on x and x0 and is not a constant.
6
Under review as a conference paper at ICLR 2022
Remark. We can easily extend this theorem to the case where there exists αl > 0 for each of
l = 2, ∙∙∙ ,L such that dι∕dι → αι and di → ∞.
Combining all the above results altogether, we achieve our main theorem:
Theorem 6. Under the conditions of Theorem 5, there exists an η1 > 0 such that if η ≤ η1 and
Vθ f (0)(xι), •…,Vθ f (O)(Xn) are linearly independent, then as d → ∞ ,for any test point X ∈ Rd
such that kxk2 ≤ 1, with probability close to 1 over random initialization,
Iimsuplf(t)(x) - ∕ERm(x)∣ = O(d-1/4) → 0	(11)
t→∞
where f(t) is trained by the reweighting algorithm and fE(tR)M is trained by ERM.
The main theorem shows that at any test point X, the gap between the function values of the two
models converges to an infinitely small term, so the worst-group test performance of the reweighting
algorithm will converge to the same level as ERM. Therefore, we have proved that for sufficiently
wide fully-connected neural networks, reweighting algorithms always overfit.
Our key intuition tells us that the change in the model parameters always lies in an n-dimensional
subspace. Thus, one possible way to improve the worst-group test performance is to enlarge this
subspace by adding more training samples, e.g. via data augmentation or semi-supervised learning.
However, even if we have more training samples, as long as the model is still overparameterized,
and all Vθf(0) (Xi) are linearly independent, then our result still says that no reweighting algorithm
can do better than ERM in the long run (though the performance of ERM itself might be improved).
Moreover, our theoretical results can explain the surprising empirical observation in Sagawa et al.
(2020b) that removing some samples from the majority groups to match the group sizes can some-
times achieve even higher worst-group test performance than reweighting even though it wastes lots
of data (see their Section 6). When training samples are removed, the model will converge to an in-
terpolator of the smaller training set which is different from the interpolator of the original training
set, so there is a chance that the performance of the new interpolator is actually higher.
4 Does regularization really help?
In the previous section, we proved the pessimistic result that reweighting algorithms always overfit,
i.e. in the long run their worst-group test performances always drop to the same level as ERM. And
even if we use strong data augmentation or semi-supervised learning, reweighting algorithms still
cannot outperform ERM if the training set is not sufficiently enlarged.
Sagawa et al. (2020a) proposed to tackle the overfitting problem of reweighting algorithms via regu-
larization. In particular, they empirically demonstrated with experiments that large regularization is
required to prevent reweighting algorithms such as group DRO from overfitting. With a large regu-
larization, the model can maintain a high test worst-group performance, but it cannot obtain perfect
training accuracy, in contrast to the case where no regularization is applied.
In this section, we study the necessary conditions for regularization to maintain high worst-group
test performance. Specifically, we will show that regularization will not work if it is not large enough
to prevent the model from obtaining nearly zero training error. In other words, lowering the training
performance is the key to keeping a high worst-group test performance. Note that the results in this
section do not require Assumption 1, so the results hold for all reweighting algorithms.
4.1	Theoretical analysis
Consider a reweighting algorithm with sample weights qi(t). Following Sagawa et al. (2020a), we
consider adding L2 penalty to the weighted empirical risk (2):
n2
R ;(t (f) = X qft'(f(xi),yi) + 2∣∣θ - θ(0) ∣∣2	(12)
i=1
Given that sufficiently wide neural networks can be approximated by linearized ones, we first focus
on linearized neural networks. We will use the subscript “reg” to refer to a regularized model (which
7
Under review as a conference paper at ICLR 2022
is trained trained by minimizing the regularized risk (12)). Let fl(int)reg be a regularized linearized
neural network trained by some reweighting algorithm, and fl(int)ERM be an unregularized linearized
neural network trained by ERM. As before, we consider training the models with gradient descent
under the squared loss '(y, y) = 1 (y - y)2. The following result shows that these two models are
very close if fl(int)reg can achieve low training error:
Theorem 7. If there is a constant Mo > 0 such that ∣∣ Vθ f (0)(x)^2 ≤ Mo for all kxk2 ≤ 1,
Vθf (0)(xι),…,Vθf (O)(Xn) are linearly independent, and the empirical training risk of f(Reg
satisfies
limsupR(端eg) < e,	(13)
t→∞
for some > 0, then for any test point x such that kxk2 ≤ 1 we have
lim sup fl(in!eg(X)- flinERM(X)I= O(W)∙	(14)
The proof of this theorem also follows the key intuition: we can show that even with the L2 penalty
added, θ(t) - θ(o) is still limited in a low-dimensional subspace. And although we cannot prove
that θ(t) always converges to the ERM interpolator, we can prove that it can get very close to that
interpolator if its training error is very low, so the resulting model is very close to the ERM model.
Then, we can extend this result to sufficiently wide fully-connected neural networks:
Theorem 8. If λmin > 0 andμ > 0, then let η* = (μ + λmin + λmax)-1. Forawidefully-Connected
neural network fr(etg) defined by (8) and (9) and satisfying Assumption 2, and any reweighting algo-
rithm, if di = d，2 =…=dL = d, η ≤ η*, Vθ f (0)(xι),…，Vθ f (O)(Xn) are linearly independent,
and the empirical training risk of fr(etg) satisfies
limsupτR(fretg)) < E	(15)
t→∞
for some E > 0, then as d → ∞, with probability close to 1 over random initialization, for any test
point X such that kXk2 ≤ 1 we have
limsup Ifreg (x) — fERM(x)∣ = O(d-1∕4 + √E) → O(√E)	(16)
t→∞
The result shows that a regularized model trained by any reweighting algorithm will get very close
to an unregularized ERM model at any test point X if the training error of the former is nearly zero.
Thus, regularization only helps when it is large enough to keep the training error of the model away
from zero by a margin.
Our results explain the empirical observation of Sagawa et al. (2020a) that by using large regular-
ization, the model can maintain a high worst-group test performance, but it cannot achieve perfect
training accuracy. If smaller regularization is applied and the model can achieve nearly perfect
training accuracy, then its worst-group test performance will still significantly drop.
4.2	Empirical study
In this section, we validate our theoretical results above with experiments on Waterbirds and CelebA.
We run ERM, IW and group DRO under different levels of weight decay for 500 epochs on Water-
birds and 250 epochs on CelebA. Note that we do not strictly follow our L2 penalty formulation
(12), but we study the L2 weight decay regularization which is most widely used in practice. We re-
peat each experiment five times with different random seeds and report the 95% confidence interval
of the mean average training and worst-group test accuracies of the last 10 training epochs in Table
1. To compare with early stopping, we also report the mean accuracies of epochs 11-20 with no
regularization in blue. Moreover, we plot the average training and worst-group test accuracy curves
throughout training for IW and Group DRO with one of the random seeds in Figure 2.
On both datasets, early stopping achieve the best performances. Particularly, on Waterbirds, there
is no clear sign that regularization could help prevent overfitting. When the regularization is small,
8
Under review as a conference paper at ICLR 2022
Table 1: Mean average training accuracy and worst-group test accuracy (%) of the last 10 training
epochs of ERM, IW and Group DRO under different levels of weight decay (WD). Each entry is
Average training accuracy / Worst-group test accuracy. Blue entries are mean accuracies of epochs
11-20 with no weight decay. Each experiment is repeated five times with different random seeds.
Dataset WD	ERM	IW	Group DRO________
0	100.0 ± 0.0/56.3 ± 1.8^^100.0 ± 0.0/67.6 ± 1.1 ^^100.0 ± 0.0/64.5 ± 1.6
	(11-20)	(Early stopping)	92.4 ± 0.4/83.7 ± 0.6	92.9 ± 0.4/79.9 ± 2.1
Waterbirds	0.05		100.0 ± 0.0/71.0 ± 1.9	100.0 ± 0.0/63.5 ± 2.6
	0.1		100.0 ± 0.0/67.7 ± 0.7	100.0 ± 0.0/54.7 ± 2.7
	0.15		99.0 ± 0.7/53.7 ± 2.7	99.4 ± 0.6/52.5 ± 2.5
	0.2		91.6 ± 2.0/35.9 ± 6.9	94.8 ± 0.9/38.0 ± 7.5
0
(11-20)
CelebA 0.01
99.0 ± 0.2/40.2 ± 5.6
(Early stopping)
99.4 ± 0.1/42.7 ± 1.7	99.4 ±	0.1/49.5 ± 1.9
92.1 ± 0.3/78.2 ± 3.2	90.5 ±	0.5/85.2 ± 1.7
97.9 ± 0.2/50.0 ± 2.8	96.5 ±	0.5/67.2 ± 1.7
95.0 ± 0.2/62.8 ± 2.4	88.9 ±	1.1/83.1 ± 2.2
89.4 ± 2.0/76.0 ± 2.4	75.1 ±	9.5/50.6 ± 15.9
(a) IW: Avg Train Acc.
(b) IW: WG Test Acc.
(c) GDRO: Avg Train Acc. (d) GDRO: WG Test Acc.
Figure 2: Average training accuracy and worst-group (WG) test accuracy of IW and Group DRO
(GDRO) under different L2 weight decay levels on CelebA.
the training accuracy is still 100% and the algorithm continues to overfit. However, when the regu-
larization is large enough to lower the training accuracy, the worst-group test accuracy drops more
because the model cannot learn the samples well under such a large regularization. Thus, perhaps
not surprisingly, a lower training performance is only a necessary condition but not sufficient.
On CelebA, regularization does help mitigate overfitting, but a useful regularization must be large
enough to lower the training accuracy. We observe that Group DRO overfits more slowly than IW,
as it still has over 70% worst-group test accuracy after 70 epochs. However, as Figure 2d clearly
shows, its worst-group test accuracy will still drop to the ERM level at 200 epochs. We also notice
that Group DRO requires a smaller regularization than IW: for IW we need the weight decay level
to be as large as 0.1 to achieve a similar performance as early stopping, but for Group DRO it only
needs to be 0.01, and using 0.1 is actually harmful.
Overall, we find that early stopping achieves a markedly better performance. On the other hand,
using large regularization could result in training instability, as well as a loss in overall performance,
and there may or may not be a small band for the regularization parameter where the worst-group
test performance is better.
5 Conclusion
In this work, we theoretically studied why reweighting algorithms overfit in practice by analyzing
their implicit biases. Specifically, we proved the pessimistic result that reweighting algorithms al-
ways overfit. Our proof was based on the key intuition that the change in model parameters always
lies in a low-dimensional subspace, so that even with reweighting, the model still converges to the
same unique interpolator. When regularization is applied, we proved that the regularization must
be large enough to keep the model from achieving nearly zero training error in order to prevent
overfitting. We empirically validated our theoretical results on real datasets, and our results can
also explain the empirical observations in previous work. Our results are especially important for
large-scale machine learning tasks, where early stopping is not always possible in order to achieve
high performances. Practitioners shooting for high worst-group performances in those tasks must
be very careful about to what extent overfitting affects reweighting algorithms.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement
To guarantee the reproducibility of all our empirical results, in all our experiments we use a fixed
set of random seeds, and we run some of the experiments twice with the same random seed to
make sure that the outputs are the same. See Appendix D.1 for experiment details. After this
paper is deanonymized, we will provide a GitHub repository that contains all the codes, datasets,
hyperparameters, random seeds, machine speculations and anaconda environment speculations that
are sufficient to exactly reproduce our empirical results.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Su Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social
media: A case study of African-American English. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing, pp. 1119-1130, Austin, Texas, November
2016. Association for Computational Linguistics. doi: 10.18653/v1/D16- 1120.
Lenalc ChizaL EdoUard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volUme 32. CUrran Associates, Inc., 2019.
Simon DU, Jason Lee, HaochUan Li, Liwei Wang, and XiyU Zhai. Gradient descent finds global
minima of deep neUral networks. In International Conference on Machine Learning, pp. 1675-
1685. PMLR, 2019.
John DUchi and Hongseok Namkoong. Learning models with Uniform performance via distribUtion-
ally robUst optimization. arXiv preprint arXiv:1810.08750, 2018.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
throUgh awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214-226, 2012.
Moritz Hardt, Eric Price, and Nati Srebro. EqUality of opportUnity in sUpervised learning. In D. Lee,
M. SUgiyama, U. LUxbUrg, I. GUyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volUme 29, pp. 3315-3323. CUrran Associates, Inc., 2016.
TatsUnori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness with-
oUt demographics in repeated loss minimization. In Jennifer Dy and Andreas KraUse (eds.),
International Conference on Machine Learning, volUme 80 of Proceedings of Machine Learning
Research, pp. 1929-1938, StoCkhoImsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Dirk Hovy and Anders S0gaard. Tagging performance correlates with author age. In Proceedings
of the 53rd annual meeting of the Association for Computational Linguistics and the 7th interna-
tional joint conference on natural language processing (volume 2: Short papers), pp. 483-488,
2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31.
Curran Associates, Inc., 2018.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances
in neural information processing systems, pp. 4066-4076, 2017.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572-8583, 2019.
Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,
Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pp. 6781-6792. PMLR,
2021.
10
Under review as a conference paper at ICLR 2022
Yonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. Distributionally robust lan-
guage modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 4227-4237, Hong Kong, China, November 2019. Association for Com-
putational Linguistics. doi: 10.18653/v1/D19-1432.
John Rawls. Justice as fairness: A restatement. Harvard University Press, 2001.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. In International Conference on Learning Representations, 2020a.
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why over-
Parameterization exacerbates spurious correlations. In Hal DaUme In and Aarti Singh (eds.),
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, pp. 8346-8356. PMLR, 13-18 Jul 2020b.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Rachael Tatman. Gender and dialect bias in youtube’s automatic captions. In Proceedings of the
First ACL Workshop on Ethics in Natural Language Processing, pp. 53-59, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Ziyu Xu, Chen Dan, Justin Khim, and Pradeep Ravikumar. Class-weighted classification: Trade-
offs and robust approaches. In Hal Daume In and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 10544-10554. PMLR, 13-18 Jul 2020.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171-
1180, 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Runtian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. Doro: Distributional and outlier ro-
bust optimization. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pp. 12345-12355. PMLR, 18-24 Jul 2021.
11
Under review as a conference paper at ICLR 2022
A Other reweighting algorithms
In this section, we will review some other previously proposed reweighting algorithms. First, we
will look at DRO-based methods, where DRO stands for Distributionally Robust Optimization.
DRO is designed for tasks with distributional shift, where the training distribution and the test distri-
bution are different, and there are some constraints on the distance between these two distributions
(typically described by a divergence function D). Since the real test distribution is unknown, DRO
minimizes the model’s risk over the worst distribution that satisfies the distance constraints, which
is an upper bound of the model’s real test error. Formally speaking, given a training distribution P,
DRO minimizes the expected risk over the worst-case distribution Q in a ball w.r.t. divergence D
around the training distribution P . For group shift problems which require high worst-group per-
formance, Q also needs to be absolutely continuous with respect to P , i.e. Q P . Overall, DRO
minimizes the following expected DRO risk:
Rd,p(Θ; P)=标Up{Eq['(Θ; Z)] : D(QIl P) ≤ ρ}	(17)
The expected DRO risk is typically minimized in the following way: for each epoch t, we first find
the worst Q that maximizes EQ [`(θ; Z)] and satisfies D(Q I P) ≤ ρ, Q P, and then minimize
the model’s expected risk over this Q with gradient descent. The rationale behind this algorithm is
the famous Danskin’s Theorem, which says that if F (x) is the maximum of a family of functions,
then its gradient at point x is equal to the gradient of the function that attains the maximum value at
x.
Note that in practice We only have a finite set of training samples {zι, •…，Zn}, so P is always
chosen as the empirical distribution, i.e. uniform distribution over zι,…，Zn. Then, note that
Q P, which implies that the support of Q must be a subset of the support of P, which is
{zι, ∙ ∙ ∙ , Zn}. This means that Q must be a distribution over zι, ∙ ∙ ∙ , zn i.e. it is a reweighting
over the training samples. Thus, we have essentially showed that DRO is a reweighting algorithm,
and in fact almost all methods based on DRO are reweighting algorithms.
Two widely used variants of DRO are CVaR (Conditional Value at Risk) and χ2-DRO. In CVaR, for
a fixed α ∈ (0,1), we let D(Q ∣∣ P) = sup log 骼 and P 二 一 log a. As a result, suppose that an
is an integer, then CVaR will assign weight On to an training samples that incur the highest losses,
and weight 0 to the rest of the samples, so we can easily see that CVaR is a reweighting algorithm.
χ2-DRO was first used in Hashimoto et al. (2018) to deal with fairness tasks where the group labels
are unknown, where D(Q ∣∣ P) = 2 R(dQ/dP _ 1)2dP and P = 2(1 一 1)2. χ2-DRO is also a
reweighting algorithm.
There are many other previously proposed methods of maximizing the worst-group performance
that are also reweighting algorithms. For instance, Xu et al. (2020) studied the imbalanced class
problem where a standard trained model always has high performance over classes with many train-
ing samples and low performance over minority classes. They proposed to balance the classes with
Label CVaR, which is based on DRO and is a reweighting algorithm.
Liu et al. (2021) proposed a two-stage training process called JTT: in the first identification stage
they trained a model with ERM to identify training samples that are hard to learn, and in the second
upweighting stage they trained a new model with the hard samples upweighted, so that the model
could learn all samples equally well. As the process itself suggests, JTT is a reweighting algorithm.
Finally, Zhai et al. (2021) argued that DRO-based methods are very sensitive to outliers in the
training set because they upweight training samples with high losses and outliers tend to incur high
losses. They proposed the DORO algorithm which at each iteration removes the samples with the
highest losses, and then performs DRO on the rest of the samples. DORO is a reweighting algorithm.
B Proofs
Notations. In all of the proofs, for a matrix A, we will use ∣A∣2 to denote its spectral norm and
∣A∣F to denote its Frobenius norm.
12
Under review as a conference paper at ICLR 2022
B.1	Proof of Theorem 1
To help our readers understand the proof more easily, we will first prove the result for static reweight-
ing algorithms where qi(t) = qi for all t, and then we will prove the result for dynamic reweighting
algorithms that satisfy qi(t) → qi as t → ∞.
B.1.1	Static reweighting algorithms
We first prove the result for all static reweighting algorithms such that mini qi = q* > 0.
We will use a standard optimization proof technique called smoothness. Denote A = Pin=1 kxi k22 .
The empirical risk of the linear model f (x) = hθ, xi is
n
F(θ) = X qi(xi>θ - yi)2	(18)
i=1
whose Hessian is
n
V2 F (θ)=2 X qiXiX>	(19)
i=1
So for any unit vector v ∈ Rd, we have (since qi ∈ [0, 1])
nn
v>Vθ2F(θ)v = 2Xqi(xi>v)2 ≤ 2Xqi kxik22 ≤ 2A	(20)
i=1	i=1
which implies that F (θ) is 2A-smooth. Thus, we have the following upper quadratic bound: for any
θ1,θ2 ∈Rd,
F(θ2) ≤ F(θ1)+ hVθF(θ1),θ2 -θ1i+Akθ2-θ1k22	(21)
Denote g(θ(t)) = √Q(X>θ(t) — Y) ∈ Rn where √Q = diag(√q1,…，√qn). We can see that
∣∣g(θ(t))∣∣2 = F (θ(t)), so that VF (θ(t)) = 2X √Qg(θ(t)). The update rule of a static reweighting
algorithm with gradient descent and the squared loss is:
n
θ(t+1) = θ⑴-ηXqiXi(f㈤(Xi) -yi) = θ㈤-ηXPQg(θ⑴)	(22)
i=1
Substituting θ1 and θ2 in (21) with θ(t) and θ(t+1) yields
F(θ(t+1)) ≤ F(θ⑴)-2ηg(θ㈤)>PQ>XtxPQg(θ㈤)+ A∣∣ηXPQg(θ㈤)∣∣j	(23)
Since xι, ∙ ∙∙ , Xn are linearly independent, XtX is a positive definite matrix. Denote the smallest
eigenvalue of XtX by λmin > 0. And ∣∣√Qg(θ(t))∣∣2 ≥ √q* ∣∣g(θ(t))∣∣2 = PqF(θ⑶),so we
have g(θ⑴)t√Q>XtX√Qg(θ㈤)≥ q*λminF(θ⑴).Thus,
F(θ(t+1)) ≤ F(θ㈤)—2ηq*λminF(θ⑴)+ An2 ∣∣xPQ∣∣2 ∣∣g(θ㈤)∣∣2
≤ F(θ㈤)-2nq*λminF(θ⑴)+ An2 ∣∣XPQ∣∣2 F(θ(t))
≤ F(θ(t)) - 2ηq*λminF (θ(t)) +Aη2 kXk2F F(θ(t))
= (1 - 2nq*λmin + A2n2)F (θ(t))
(24)
Let n0 = 9：，”. For any n ≤ n0, we have F(θ(t+1)) ≤ (1 一 nq*λmin)F(θ⑴)for all t, which
implies that limt→∞ F(θ⑴)=0. Moreover, pF(θ(t+1)) ≤ (1 一 "Q：，”)pF(θ⑴)due to
√T-^x ≤ 1 一 x/2.
13
Under review as a conference paper at ICLR 2022
The convergence in F (θ) implies the convergence in θ. This is because
Mt+1) -叫2 = η2 ∣∣XpQg(θ(t))∣∣2 ≤ η2 ∣∣XPQdg(θ(t))∣∣2
≤η2kXk2Fg(θ(t))2=Aη2F(θ(t))
which implies that for any η ≤ η0 ,
∞	∞
----	2A Irg、
X ∣Wt+1) - θ(R2 ≤ pAη2X √F(θt)) ≤ qjλ-√F(θ(T))
(25)
(26)
Therefore, limT →∞ Pt∞=T θ(t+1) - θ(t) 2 = 0, which means that θ(t) converges, and it converges
to some interpolator.
B.1.2 Dynamic reweighting algorithms
Now we prove the result for all dynamic reweighting algorithms satisfying Assumption1. By As-
sumption 1, for any > 0, there exists t such that for all t ≥ t and all i,
qi(t) ∈ (qi - , qi + )	(27)
This is because for all i, there exists ti such that for all t ≥ ti, qi(t) ∈ (qi - , qi + ). Then,
We can define te = max{tι, ∙∙∙ ,tn}. Denote the largest and smallest eigenvalues of X>X by
λmax and λmin, and because X is full-rank, we have λmin > 0. Select and a fix an such that
0 < e < max{ q3-, (q2λmax2 }, and then te is also fixed.
We still denote Q = diag(qι,…，qn). When t ≥ te, the update rule of a dynamic reweighting
algorithm With gradient descent and the squared loss is:
θ(t+1) = θ(t) - ηXQ(t) (X> θ(t) - Y )	(28)
where Q(t) = Q(t), and we use the subscript to indicate that Q(t) - Q < . Then, note that
we can rewrite Qst) as Qst) = QQ3^ ∙ √Q for all e < q*/3. This is because q% + e < ∙p(q^+^3e)q^
and qi - e > (/i-i - 3e)qi for all e < qj3, and qi ≥ q*. Thus, we have
θ(t+1) = θ㈤-ηXy∕Q^g(θ(t')) where QSt) = ∖/Q ∙ PQ	(29)
Again, substituting θ1 and θ2 in (21) with θ(t) and θ(t+1) yields
F(θ(t+1)) ≤ F(θ(t)) - 2ηg(θ(t))>PQ>x>xyQ3t)g(θ(t)) + A ηX√Q3Sg(θ⑴)(30)
2
Then, note that
g(θ(t))>pQ>X > X (qQi) - PQ) g(θ(t))
≤ ∣pq>x >x (qQ3ty - ρQ)∣∣2 …2	(31)
≤ llp⅛X>XWqQty - ρ⅛W"2
≤λmax√3^F (θ(t))
where the last step comes from the following fact: for all < qi /3,
Pqi + 3c — √qi ≤ √3e and √qi — Pqi 一 3c ≤ √3e	(32)
14
Under review as a conference paper at ICLR 2022
And as proved before, we also have
g(θ⑴)>PQ>X>XPQg(θ(t)) ≥ q*λminF(θ⑴)	(33)
Since e ≤ (；；：：：：2?, We have
g(θ㈤)>PQ>X>XVzQ3t)g(θ(t)) ≥ (q*λmin - λmax√3i) F(θ⑴)≥ 1 q*λminF(θ⑴)(34)
Thus,
F (θ(t+1)) ≤ F (θ㈤)-ηq*λminF (θ㈤)+ An2 X q∕Q(t)∣∣ ∣∣g(θ(t))∣∣2
≤ (1 - ηq*λmin + A2η2(1 + 3))F (θ(t))
≤ (1 - ηq*λmin + 2A2η2)F(θ(t))
(35)
for all C < 1/3. Let no = q4A2n. For any n ≤ no, we have F(θ(t+1)) ≤ (1-nq*λmin∕2)F(θ(t)) for
all t≥ t, which implies that limt→∞ F(θ(t)) = 0. As before, we can prove that the convergence
in F(θ) implies the convergence in θ. Thus, θ converges to some interpolator.	口
B.2 Proof of Theorem 4
Note that the first l layers (except the output layer) of the original NTK formulation and our new
formulation are the same, so we still have the following proposition:
Proposition 9 (Proposition 1 in Jacot et al. (2018)). If σ is LiPschitz and d → ∞ for l = 1,…，L
sequentially, then for all l = 1,…，L, the distribution of a single element of hl converges in
probability to a zero-mean Gaussian process of covariance Σl that is defined recursively by:
Σ1(x, x0) = -J-x>x0 + β2
do
Σl(x, x0) = Ef[σ(f(x))σ(f(x0))] + β2
where f is sampled from a zero-mean Gaussian process of covariance Σ(l-1).
(36)
Now we show that for an infinitely wide neural network with L ≥ 1 hidden layers, Θ(o) converges
in probability to the following non-degenerated deterministic limiting kernel
Θ = Ef ~∑l [σ(f (x))σ(f (x0))]+ β2	(37)
Consider the output layer hL+1 = w√Lσ(hL) + βbL
We can see that for any parameter θi before
the output layer,
WL>
vθi hL+1 = diag(σ(hL)) K vθi hL = 0
(38)
And for W L and bL, we have
VW L hL+1 = ɪ σ(hL)	and	VbL hL+1 = β	(39)
dL
Then we can achieve (37) by the law of large numbers.	口
B.3 Proof of Theorem 5
We will use the following short-hand in the proof:
g(θ(t))=f(t)(X)-Y
J(θ(t)) = vθf(X; θ(t)) ∈ Rp×n	(40)
Θ(t) = J(θ(t))>J(θ(t))
15
Under review as a conference paper at ICLR 2022
For any > 0, there exists t such that for all t ≥ t and all i, qi(t) ∈ (qi - , qi + ). Like what we
have done in (29), We can rewrite Q(t) = Q(t) = Jq3? ∙ √Q, where Q = diag(qι,…，qn).
The update rule of a reweighting algorithm with gradient descent and the squared loss for the wide
neural network is:
θ(t+1) = θ(t) - ηJ(θ(t))Q(t)g(θ(t))	(41)
and for t ≥ t, it can be rewritten as
θ(t+1) = θ(t) - ηJ(θ(t))/θf hPQg(θ(t))i	(42)
First, we will prove the following theorem:
Theorem 10. There exist constants M > 0 and e0 > 0 such that for all E ∈ (0, e0], η ≤ η* and
any δ > 0, there exist R0 > 0, D > 0 and B > 1 such that for any d ≥ D, the following (i) and
(ii) hold with probability at least (1 - δ) over random initialization when applying gradient descent
with learning rate η:
(i)	For all t ≤ t, there is
g(θ(t))	≤ BtR0	(43)
XXMj)-θjT∣ ≤ ηMR0 XX Bj-1 < MB-R0	(44)
j=1	2	j=1	-
(ii)	For all t ≥ t, we have
∣∣PQg(θ(t))∣∣2 ≤(1 - ηqɪλmin)t -t Bt，Ro	(45)
XX W-θ(jτ)∣L ≤ ”EMBt，Ro XX(1 - η⅛minj
j=t， +1	j=t， +1	(46)
3√1+3lMBt≡ Ro
< q* λmin
Proof. The proof is based on the following lemma:
Lemma 11 (Local Lipschitzness of the Jacobian). Under Assumption 2, there is a constant M > 0
such that for any Co > 0 and any δ > 0, there exists a D such that: If d ≥ D, then with probability
at least (1 - δ) over random initialization, for any x such that kxk2 ≤ 1,
Vθf (x； θ) -Vθf (x；研∣2
l∣Vθ f(x； θ)k2
∣∣J ⑻ -J ⑻ ∣∣f
lJ(θ)lF
M
≤ --
.P
≤M
M
≤
.P
≤M
∀θ,θ ∈ B(θ⑼,Co)	(47)
θ -矶2
θ -矶2
where B(θ(o), R) = {θ : ∣θ - θ(o)∣2 < R}.
The proof can be found in Appendix B.4. Note that for any x, f(o) (x) = βbL where bL is sampled
from the standard Gaussian distribution. Thus, for any δ > 0, there exists a constant Ro such that
with probability at least (1 一 δ∕3) over random initialization,
∣∣∣g(θ(o))∣∣∣ < Ro	(48)
A 1 1 EI	A .1	*,7^Λ7 C	< . < . Γ∙	7、 7^Λ	∙ . 1	1 1 ∙ 1 ∙ .	. 1	. / 1 Γ / ∏ ∖
And by Theorem 4, there exists D2 ≥ 0 such that for any d ≥ D2, with probability at least (1 一 δ∕3),
Θ-Θ(o)∣∣F≤
q*λmin
-3-
(49)
16
Under review as a conference paper at ICLR 2022
Let M be the constant in Lemma 11. Let e° = (108”4) . Let B = 1 + η*M2, and Co = MB-R +
3√1+IlMBt^R∙. By Lemma 11, there exists Di > 0 such that with probability at least (1 - δ∕3),
for any d ≥ Di, (47) is true for all θ,θ ∈ B(θ(0), Co).
By union bound, with probability at least (1 - δ), (47), (48) and (49) are all true. Now we assume
that all of them are true, and prove (43) and (44) by induction. (43) is true for t = 0 due to (48), and
(44) is always true for t = 0. Suppose (43) and (44) are true for t, then for t + 1 we have
θ(t+i)-θ(t)	≤ηJ(θ(t))Q(t)	g(θ(t))	≤ηJ(θ(t))Q(t)	g(θ(t))
≤ηJ(θ(t))	g(θ(t))	≤ MηBtRo
(50)
So (44) is also true for t + 1. And we also have
g(θ(t+i))	= g(θ(t+i)) - g(θ(t)) + g(θ(t))
=J (θ(t'))τ (θ(t+1) - θ(t)) + g(θ(t))∣∣2
=∣∣-ηJ (θ ⑴)>J (θ(t))Q(t)g(θ(t)) + g(θ(t))∣∣2
≤ ∣∣I - nJ(θ(ty)τJ(θ(t))Q(t)∣∣2∣∣g(θ(t))∣∣2	(51)
≤ (ι + ∣∣nJ俨))>J(θ(t))Q(t)∣∣J ∣∣g(θ(t))∣∣2
≤(1+n ∣∣j(巧UJ Gt))UF) ∣s∣∣2
≤ (1 + n*M2)∣∣g(θ(t))∣∣2 ≤ Bt+1Ro
Therefore, (43) and (44) are true for all t ≤ te, which implies that ∣∣√Qg(θ(te))∣∣2 ≤ ∣∣g(θ(te))∣∣2 ≤
BtRo, so (45) is true for t = tl. And (46) is obviously true for t = tl. Now, let us prove (ii) by
induction. Note that when t ≥ tl, we have the alternative update rule (42). If (45) and (46) are true
for t, then for t + 1, there is
θ(t+i) - θ(t) ∣∣ ≤ η
J (θ(t))qQIt)∣∣j∣pQg(θ(t))∣∣2 ≤ n
J (θ(t))qQIt)∣∣^∣∣pQg(θ(t) )∣∣2
≤ n√T+3i∣∣j(θ(t))∣∣F ∣∣√Qg(θ(t))∣∣2 ≤ Mn√T+3i (ι
nq*λmin
-3-
t-t
BtRo
(52)
—
So (46) is true for t + 1. And we also have
∣∣pQg(θ(t+1))∣∣2 = ∣∣pQg(θ(t+1)) - pQg(θ(t)) + pQg(θ(t))∣∣
=∣∣Pqj(θ(t))>(θ(t+1) -θ(t)) + PQg(θ(t))∣∣
2
2
=∣∣-n √QJ (θ(t))> J (θ(t))Q(t)g(θ(t)) + √Qg(θ(t)) ∣∣2
(53)
≤ 1-nPQJ(θw)>J(θ(t))√Q3te)∣∣2 ∣∣PQg(θ(t))
2
I-η
≤
2(1-「)tRo
where θ(t) is some linear interpolation between θ(t) and θ(t+1). Now We prove that
I - nPQJ(6t))>J(θ(t))qθty∣∣2 ≤ 1 - nqɪ3-	(54)
17
Under review as a conference paper at ICLR 2022
For any unit vector v ∈ Rn, we have
v>(i - η√Qθ√Q)v = 1 - ηv>√Qθ√Qv
(55)
∣∣√Qv∣∣2 ∈ [√q*, 1], so for any η ≤ η*, v>(I - η√QΘ√Q)v ∈ [0,1 - ηλminq*], which implies
that IlI - η√Qθ√Q∣∣2 ≤ 1 - ηλminq*. Thus,
∣∣i-η√Qj M))TJ (θ(t))√Q∣∣2
≤ ∣∣I - η√Qθ√Q∣∣2 + η ∣∣√Q(Θ - θ ⑼)√Q1 + η ∣∣√Q(J (θ ⑼)> J (θ(O))- J (θ㈤)> J (θ(t)))√Q∣∣2
≤1 - ηλminq* + η∣∣√Q(Θ - Θ(0))√Q∣∣^ + η ∣∣√Q(J(θ⑼)>J(θ(O))- J(θ(t))>J(θ㈤))√Q∣l
≤1 - ηλminq* + η∣∣Θ - Θ ⑼ ∣∣ + η ∣∣J (θ ⑼)> J (θ(O)) - J (θθ(t )> J (θ(t))∣∣
≤1 - ηλminq*+ηq*F+ηM (∣∣θ(t) - θ(0) ∣∣2+M)- θ(0) ∣∣J ≤ 1 - *
(56)
for all d ≥ max {d1, D2, (IqMmCO) }, which implies that
I - η√QJ(θ(t))>J(θ(t))qQ3ty∣∣2
≤1- ηq*λmn + η√QJ0t))>J(θ㈤)(qQ3ty - √q)[	(57)
≤1 - ηq*λmn + ηM2√3i ≤ 1 - ▼ (due to (32)j
for all ≤ O. Thus, (45) is also true for t + 1. In conclusion, (45) and (46) are true with probability
at least (1 - δ) for all d ≥ DD = max ∣D1,D2, (IqMmC0) ).	□
Returning back to the proof of Theorem 5. Choose and fix an such that <
2
-∣ /	* λmιn ʌ 2
min{6o, 1 ( 3λmqχ++q*λmin ) }, where e° is defined by Theorem 10. Then, te is also fixed. There
exists D ≥ 0 such that for any d ≥ D, with probability at least (1 - δ), Theorem 10 and Lemma 11
are true and
* min
Θ - θ(o)∣∣f ≤ q^-
which immediately implies that
∣∣Θ(O)∣∣2 ≤kθ∣∣2 + ∣∣Θ - Θ (O)IIF ≤ λmax + q*λmn
(58)
(59)
We still denote B = 1 + η*M2 and Co = MBt-R + 3√1+3λMBtɪR. Theorem 10 ensures that for
all t, θ(t) ∈ B(θ(O), CO). Then we have
I - η√QΘ(0)√Q∣∣2 ≤ ∣∣I - η√QΘ√Q∣∣2 + η∣∣√Q(Θ - Θ(O))√Q∣∣?
≤ 1 - ηλminq* +
ηq*λmin
3
2ηq*λmin
1	3
(60)
so it follows that
I-η
I - η√QΘ(0)√Q∣∣2 + η√QΘ(0)
‰z7* ∖ min	c* ∖min
≤ 1 - 2ηq-^~ + η(λmax + ∖)√3^
—
2 (61)
)
3
3
18
Under review as a conference paper at ICLR 2022
[/	* ʌ min ∖ 2
Thus, for all e < 1 (3λmqx+q*λmn) , there is
I - ηpQθ(0)√Q(⅛i)∣2
≤1-
ηq*λmm
-3-
(62)
The update rule of the reweighting algorithm for the linearized neural network is:
θl(itn+1) = θl(itn) - ηJ(θ(0))Q(t)glin(θ(t))	(63)
where we use the subscript “lin” to denote the linearized neural network, and with a slight abuse of
notion denote glin(θ(t)) = g(θl(itn)).
First, let us consider the training data X. Denote ∆t = glin(θ(t)) - g(θ(t)). We have
(glin(θ(t+1)) - glin(θ(t)) = -ηJ(θ(0))>J(θ(0))Q(t)glin(θ(t))
[	g(θ(t+1)) - g(θ(t)) = -ηJ(θ(t))>J(θ⑴)Q⑴g(θ㈤)
where θ⑴ is some linear interpolation between θ(t) and θ(t+1). Thus,
∆t+ι - ∆t =η [j(θ(t))>J(θ㈤)-J(θ⑼)>j(θ⑼)]Q㈤g(θ⑴)
- ηJ(θ(0))>J(θ(0))Q(t)∆t
(64)
(65)
By Lemma 11, we have
∣∣j (θ(t))>j (θ(t)) - j (θ ⑼)>j (θ (O))L
≤ (J(θ(t)) - j(θ⑼))>j(θ㈤)+1j(θ⑼)> (J(θ(t)) - j(θ(%)∣∣	(66)
FF
≤2M 2C0dτ∕4
which implies that for all t < t,
k∆t+1k2 ≤ ∣∣[i - nJ(θ(0))>J(θ⑼)Q(t)i ∆t∣∣2 + ∣∣η [j(θ(t))>J(θ(t)) - J(θ⑼)>j(θ⑼)]Q⑴g(θ⑴)(
≤ ∣∣i - nJ (θ(0))>J (θ(%Q⑴||尸 k∆tk2 + n ∣∣J (θ(t))>J (θ(t)) - J (θ ⑼)>J (θ(%∣∣F ∣∣g(θ(t))∣∣2
≤ (1 + nM2) k∆tk2 + 2nM2C0BtROd-1/4
≤ B k∆tk2 +2nM2COBtROd-14
(67)
Therefore, we have
B-(t+1) k∆t+1k2 ≤ B-t k∆tk2 + 2nM2CoB-1Rod-1/4	(68)
Since ∆O = 0, it follows that for all t ≤ t,
∣∣∆tk2 ≤ 2tηM2CoBt-1Rod-1/4	(69)
and particularly we have
|| PQ∆te∣∣2 ≤ ∣∆t J2 ≤ 2teηM2CoBt-RodT/4	(70)
For t ≥ t, we have the alternative update rule (42). Thus,
PQ∆t+1 - PQ∆t =ηPQ [j(θ⑴)>J(θ㈤)-J(θ(o))>J(θ(O))] qQt [PQg(θ(t))]
-npQJ (θ(o))>J ©^qQ^ [pQ∆t]
(71)
19
Under review as a conference paper at ICLR 2022
Let A = I - η√QJ (θ(0))>J (θ⑼)yθ% = I - η√QΘ(0) ∕q3). Then, we have
pQ∆t+ι = ApQ∆t+ηpQ hJ(θ㈤)> J(θ⑴)-J(θ⑼)> J(θ⑼)i qQ3) (pQgM)) (72)
* ʌ min
Let Y = 1 - ηq-3j—— < 1. Combining with Theorem 10 and (62), the above leads to
∣√Q∆t+ι∣2 ≤ kA∣∣2 ∣∣√Q∆JL + η pQ [j(科)>J(θ㈤)-J(θ⑼)>j(θ⑼)i qQ3^)∣J∣√Qg(θ(t))∣2
≤ γ ∣∣√Q∆t∣∣2 + η ∣∣j (θ ⑴)>j (θ⑴)-j (θ⑼)> j (θ(0))∣∣F √τ+3iγ j Bte Ro
≤ Y ∣∣√Q∆t∣∣2 + 2ηM2C0√T+3lγt-teBteRod,-1/4
2	(73)
This implies that
γ-(t+1) ∣∣√Q∆t+ι∣∣2 ≤ γ-t ∣∣√Q∆t∣∣2 + 2ηM2Co√1 + 3eγ-1-teBteRo(i-1/4	(74)
Combining with (70), it implies that for all t ≥ t,
∣∣√Q∆t∣∣2 ≤ 2γt-teηM2CoBteRo ∖teB-1 + √1 + 3eγ-1(t - te)] d-1/4	(75)
Next, we consider an arbitrary test point x such that kxk2 ≤ 1. Denote δt = fl(int) (x) - f (t) (x).
Then we have
(fl(n+1)(x) - flin)(x) = -ηVθf(x; θ(O))TJ(θ(0))Q(t)glin(θ(t))
If(t+1)(X)- f ⑴(X) = -ηVθf (x; θ(t))>J(θ㈤)Q㈤g(θ⑴)
which yields
δt+ι - δt =η [Vθf (x; θ(t'))τJ(θ㈤)一Vθf (x; θ(O))T J(θ(0))i Q(t)g(θ(t))
- ηVθf(X; θ(0))>J(θ(0))Q(t)∆t
For t ≤ t, we have
t-1
kδtk2	≤ηX	∣∣	Vθf(x;	θ,(s))TJ(θ(s)) - Vθf(x; θ(0))TJ(θ(0))	Q(s)	∣∣	∣∣g(θ(s))∣∣
s=0	2	2
t-1
+ η X ∣∣Vθf(x;
s=0
θ(0))TJ(θ(0))Q(s) ∣∣
k∆sk2
≤η X∣∣Vθ f (x; θ(s))τJ (θ(S))-Vθ f (x; θ(O))TJ (θ(O))∣∣ ∣∣g(θ(S))∣∣
s=0	F	2
t-1
+ηX ∣∣Vθf (x; θ(0))∣∣2 ∣∣J(θ(0))∣∣Fk∆Sk2
t-1	t-1
≤2ηM2C0d-1∕4 X BsR0 + ηM2 X(2sηM2C0Bs-1R0d-1/4)
S=0	S=0
(76)
(77)
(78)
20
Under review as a conference paper at ICLR 2022
So We can see that there exists a constant Ci such that ||6尢 |卜 ≤ Cιd-1∕4. Then, for t > te, We have
t-1
kδtk2-kδtk2 ≤ηX
s=t
[vθ f(χ; θ(s))>j (θ(s))-Vθ f(χ; θ(0))>j (θ(0))i qQF]j∣PQg(θ(s))∣L
t-1
+η
X vθf(χ;θ(0))>j©0))qQF||	∣∣pQ∆s∣∣
s=t	2
s=t
2
2
t-1
≤2ηM2C0dτ∕4√1+^ X YSf BteR0
s=t
t-1
+ ηM2√1+3i X (2γs-teηM2CoBteRo 艮BT + √1+3lγ-1(s - te)]殷1/4)
s=te
(79)
Note that Pt∞=o tγt is finite as long as γ ∈ (0, 1). Therefore, there is a constant C such that for any
t, ∣∣δt∣∣2 ≤ Cd-1/4 with probability at least (1 - δ) for any d, ≥ D.	□
B.4 Proof of Lemma 11
We Will use the folloWing theorem regarding the eigenvalues of random Gaussian matrices:
Theorem 12 (Corollary 5.35 in Vershynin (2010)). If A ∈ Rp×q is a random matrix whose entries
are independent standard normal random variables, then for every t ≥ 0, with probability at least
1 - 2 exp(-t2/2),
√p-√q -1 ≤ λmin(A) ≤ λmax(A) ≤√p + √q +1	(80)
By this theorem, and also note that WL is a vector, we can see that for any δ, there exist DD > 0 and
Mi > 0 such that if d ≥ DJ, then with probability at least (1 一 δ), for all θ ∈ B(θ(0), Co), we have
∣∣Wl∣∣2 ≤ 3pj	(∀0 ≤ l ≤ L — 1) and ∣∣WL∣∣2 ≤ C ≤ 3p (81)
as well as
∣∣β bl∣∣2 ≤ Mi VZj	(∀l = 0,∙∙∙,L)	(82)
Now we assume that (81) and (82) are true. Then, for any x such that kxk2 ≤ 1,
∣hi∣2
∣hl+i∣2
√1= W 0x + β b0
2 ≤ √d^ ∣∣w 0∣∣2 kxk2+ ∣∣βb0∣∣2 ≤(√d^+ MI)P
ɪ W lxl + βbl
Vd
≤ 3∣∣Wl∣∣2∣∣xl∣∣2 + ∣∣βbl∣∣2
(∀l ≥ 1)
(83)
∣∣xl∣∣2 = ∣∣σ(hl) - σ(0l) + σ(0l)∣∣2 ≤ Lo ∣∣hl∣∣2 + σ(0)√j	(∀l ≥ 1)
where Lo is the Lipschitz constant of σ and σ(0l) = (σ(0), ∙∙∙ , σ(0)) ∈ Rdl. By induction, there
exists an M2 > 0 such that ∣∣xl∣∣2 ≤ M2 vQ and ∣∣hl∣∣2 ≤ M2 Vd for all l = 1,…，L.
Denote al = Vhlf (x) = VhlhL+1. For all l = 1,…，L, we have al = diag(σ(hl))Wl>αl+1
v d
where σ(x) ≤ Lo for all X ∈
Iiag(WhL))WT∣∣2 ≤ 畲L
R since σ is Lo-Lipschitz, αL+i = 1 and ∣αL ∣2 =
Then, we can easily prove by induction that there exists an
M3 > 1 such that ∣∣al∣∣2 ≤ M3/4/dl for all l = 1,…，L (note that this is not true for L + 1
because αL+i = 1).
For I = 0, VW 0f (X) = √⅛ χoaι>, so kvwlf(X)k2 ≤ √do ∣∣xo∣∣2 ∣∣α1∣∣2 ≤ √do M3/ ∙4∕d. And
for any l = 1,…，L, VWlf(X) = √∣xlαl+1, so ∣∣Vw l f (x)k2 ≤ -√∣ ∣∣xl∣∣2 ∣∣αl+1∣∣2 ≤ M2 M3.
21
Under review as a conference paper at ICLR 2022
(Note that if M3 > 1, then ∣∣αL+1∣∣2 ≤ M3； and since d ≥ 1, there is IlalII2 ≤ M3 for l ≤ L)
Moreover, for l = 0,…，L, Pbl f(x) = βal+1, so ∣∣Vbif (x)k2 ≤ βM3. Thus, if (81) and (82) are
true, then there exists an M4 > 0, such that ∣Vθf (x)∣∣2 ≤ M4∕√n. And since Ilxik2 ≤ 1 for all i,
so ∣J(θ)∣F ≤ M4.
Next, We consider the difference in Vθf (x) between θ and θ. Let f, W, b, x, h, α be the function
and the values corresponding to θ. There is
h1 - h 1∣∣2 =	√1=(W0 - W0)x + β(b0 - b0)
∣∣w0- W0∣∣∕xk2 + β∣∣b0
-叫 2 ≤(√⅛+β ∣∣θ - θ!
∣∣hl+1 - hl+1∣∣ = ɪWl(xl - xl) + ɪ(wl - Wl)xl + β(bl - Bl)
11	ll2 Vd	dJd
≤ pp∣∣W l∣2∣xl - xl∣∣2+p⅛∣∣Wl-W l
≤3∣∣xl -xBl∣∣2+(M2+β) ∣∣∣θ - θB∣∣∣	(∀l ≥ 1)
∣∣xl	-xBl∣∣2 =	∣∣∣σ(hl) - σ(hB l)∣∣∣	≤ L0	∣∣∣hl -hBl∣∣∣	(∀l	≥ 1)
2
∣∣∣2 ∣∣xBl∣∣2 + β ∣∣∣bl - bBl∣∣∣2
(84)
By induction, there exists an	M5	> 0 such that ∣∣xl - xBl ∣∣2 ≤	M5	∣∣θ -	θB∣∣	for all	l.
For al, we have aL+1 = aB L+1 = 1, and for all l ≥ 1,
∣∣al- a l∣∣2 =Fiag(σ(hl)) WJ'
∣∣	Wl>
≤ ∣∣diag(σ(hl))-pB (
B l>
αl+1- diag(σ(h l)) F a l+1
al+1
-al+1)	+ 卜ag(σ(hl)) (Wl-ZWl)> al+1
2
2
+ ∣∣diag((σ(hl) - σ(hl)))Wp=Bal+1
≤ 3L0 ∣∣al+1 -aBl+1∣∣2+ M3L0dB-1/2+3M3M5L1dB-1/4 ∣∣∣θ - θB∣∣∣
2
(85)
where L∖ is the Lipschitz constant of σ. Particularly, for l = L, though aL+1 = 1, since ∣∣ WL
3(i1/4, (85) is still true. By induction, there exists an Mg > 0 such that ∣∣αl - a l∣∣ ≤ M6
2	7 d
for all l ≥ 1 (note that this is also true for l = L + 1).
≤
2
〜
θ-θ∣∣2
Thus, if (81) and (82) are true, then for all θ, θB ∈ B(θ(0), C0), any x such that kxk2 ≤ 1, we have
VW0f (x) - VW0f(x)∣∣2
√d= ∣∣
d0
√d= ∣∣
d0
xa1> - xaB 1> ∣∣2
a1 - aB 1 ∣2
(86)
1	M6
θ - θB∣∣2
≤
≤
√d0 1
and for l = 1,…，L, we have
22
Under review as a conference paper at ICLR 2022
VWιf(x) - VW厅(x)∣∣ = ɪ I∣xlαl+1> - Xlal+1>∣∣2
2d
≤ P (∣∣χl∣∣2∣∣al+1- al+1∣∣2 + ∣∣Xl- xl∣∣2∣∣al”)⑻)
M2M6
≤
Vd
M5M3
+^√dτ
2
Moreover, for any l = 0, ∙∙∙ ,L, there is
f(X) - Vblf(X)∣∣2 = β ∣∣al+1 - al+1∣∣2 ≤ βM6∣θ - θ∣2
(88)
Overall, We can see that there exists a constant M7 > 0 such that IlVθf (x) - jf(x)
〜
M7
√n∙ √
θ - θ∣∣2,sothat∣∣J(θ) - J(Θ)∣∣f
≤ ⅜∣∣θ - θ∣∣2

≤
2
□
(89)
(90)
(91)
□
(92)
B.5	Proof of Theorem 6
Let ηι = min{η0,η*}, where no is defined in Corollary 3 and η* is defined in Theorem 5. Let
fl(int)(X) and fl(int)ERM(X) be the linearized neural netWorks of f(t) (X) and fE(tR)M(X), respectively. By
Theorem 5, for any δ > 0, there exists D > 0 and a constant C such that
SupMn)(X)- f(t)(x)∣ ≤ CdT/4
SUplfiinERM(X)- fERM(X)I ≤ CdT/4
t≥0
By Corollary 3, we have
tl→im∞ ∣∣fl(in) (X) - fl(in)ERM (X)∣∣ = 0
Summing the above yields
limsup ∣f(t)(X) - ∕ERm(x)∣ ≤ 2Cd-1/4
t→∞
which is the result we want.
B.6	Proof of Theorem 7
To minimize the regularized risk (12) with gradient descent, the update rule is
n
θ(t+1) = θ㈤-n X q(t)Vθ'(f ⑴(Xi),yi) - nμ(θ㈤-θ(O))
i=1
We can see that under the new rule, θ(t) - θ(0) ∈ SPan(Vθf (O)(X1),…，Vθf (O)(Xn)) is still true
for all t. Let θ* be the interpolator in SPan(Vθf (O)(XI), .…，Vθf(O)(Xn)), then the empirical risk
of θ is 2n Pn=Ig - θ*, Vθf(O)(Xi)i = 2n∣∣Vθf(O)(X)>(θ - θ*)∣∣2. Thus, there exists T > 0
such that for any t ≥ T,
∣∣Vθf(O)(X)>(θ㈤-θ*)∣∣2 ≤ 2ne	(93)
Let the smallest singular value of √nVθf (O)(X) be smin, and we have Smm > 0. Note that the
column space of Vθf (O)(X) is exactly span(Vθf (O)(X1), ∙∙∙ , Vθf (O)(Xn)). Define H ∈ Rp×n
23
Under review as a conference paper at ICLR 2022
such that its columns form an orthonormal basis of this subspace, then there exists G ∈ Rn×n such
that Vθf (O)(X) = HG, and the smallest singular value of √1nG is also smin. Since θ(t) - θ(0)
is also in this subspace, there exists V ∈ Rn such that θ(t) - θ* = Hv. Then We have √2ne ≥
∣∣G>H>Hv∣∣2 = ∣∣G>v∣∣2. Thus, ∣∣v∣∣2 ≤ SmI, which implies
Mt)-θI ≤ √1	(94)
By Corollary 3, if we minimize the unregularized risk with ERM, then θ always converges to the
interpolator θ*. So for any t ≥ T and any test point X such that |罔卜 ≤ 1,
lfl(2eg(x) - f‰(x)l = lhθ⑴-θ*, Vθf⑼(x)i∣ ≤ M√p	(95)
which implies (14).	□
B.7 Proof of Theorem 8
First of all, with some simple linear algebra analysis, we can prove the following proposition:
Proposition 13. For any positive definite symmetric matrix H ∈ Rn×n, denote its largest and
smallest eigenvalues by λmax and λmin. Then, for any q ∈ R' and Q = diag(qι,…,qn), HQ
has npositive eigenvalues that are all in [mini qi ∙ λmin, maxi qi ∙ λmax].
Proof. H is a positive definite symmetric matrix, so there exists A ∈ Rn×n such that H = A>A,
and A is full-rank. First, any eigenvalue of AQA> is also an eigenvalue of A>AQ and vice
versa, because for any eigenvalue λ of AQA> we have some v 6= 0 such that AQA>v = λv.
Multiplying both sides by A> on the left yields A>AQ(A>v) = λ(A>v) which implies that λ
is also an eigenvalue of A>AQ because A>v 6= 0 as λv 6= 0. We can prove the other direction
similarly.
Second, by condition we know that the eigenvalues ofA>A are all in [λmin, λmax] where λmin > 0,
which implies for any unit vector v, v>A>Av ∈ [λmin, λmax], which is equivalent to kAvk2 ∈
[√λmin, √λmax]. Thus, we have v> A>QAv ∈ [λmin mini q%, λmax maxi q/, which implies that
the eigenvalues of A>QA are all in [λmin mini qi, λmax maxi qi].
Thus, the eigenvalues of HQ = A>AQ are all in [λmin mini qi, λmax maxi q/.	□
Now return back to the proof of Theorem 8. We still use the shorthand (40). With L2 penalty, the
update rule of the reweighting algorithm for the neural network is:
θ(t+1) = θ⑶-nJ(θ(t))Q(t)g(θ(t)) - ημ(θ(t) - θ(0))	(96)
And the update rule for the linearized neural network is:
θ(in+1) = θl(i? - nJ(θ(0))Q(t)glin(θ㈤)-nμ(θ(i? - θ⑼)	(97)
First, we need to prove that there exists Do such that for all d ≥ D0, supt≥0 ∣∣θ(t) - θ(0)∣∣2 is
bounded with high probability. Denote at = θ(t) - θ(0). By (96) we have
at+1 =(1 - nμ)at - n[J(θ㈤)-J(θ⑼)]Q⑴g(θ㈤)
-nJ(θ(0))Q(t)[g(θ(t))-g(θ(0))]-nJ(θ(0))Q(t)g(θ(0))
which implies
kat+1k2 ≤ ∣∣(1 - nμ)i - nJ(θ⑼)Q㈤ J(θ⑴)>(1%1卜
+n∣∣∣J(θ(t))-J(θ(0))∣∣∣	∣∣∣g(θ(t))∣∣∣	+n∣∣∣J(θ(0))∣∣∣	∣∣∣g(θ(0))∣∣∣
(98)
(99)
where θ(t is some linear interpolation between θ(t) and θ(0). Our choice of n ensures that nμ < 1.
Similar to (48), we can show that for any δ > 0, there exists a constant R0 > 0 such that with
24
Under review as a conference paper at ICLR 2022
probability at least (1 - δ∕3), ^g(θ(0))„2 < Ro. Let M be as defined in Lemma 11. Denote
A = ηMRo, and let Co = 4ηA in Lemma 116. By Lemma 11, there exists Di such that for all
d ≥ Di, with probability at least (1 - δ∕3), (47) is true.
Now we prove by induction that kat k2 < Co . It is true for t = 0, so we need to prove that if
katk2 < Co,thenkat+ik2 < Co.
For the first term on the right-hand side of (99), we have
∣∣(1 - ημ)i - nJ(θ⑼)Q(tJ(θw)>∣∣ ≤(1 - ημ) ∣∣I - T^J(θ⑼)Q(t)J(θ(0))>
U	112	∣∣	1 - nμ	2 (100)
+n∣J (θ(0))∣U∣J (θ(t)) - J (θ(0))∣∣p
Like what we have done before, we can show that all non-zero eigenvalues of J (θ(o))Q(t)J (θ(o))>
are eigenvalues of J(θ(o))> J(θ(o))Q(t). This is because for any λ 6= 0, if J(θ(o))Q(t) J(θ(o))>v =
λv,	then	J(θ(o))> J(θ(o))Q(t) (J (θ(o))>v)	=	λ(J(θ(o))>v),	and	J(θ(o))>v	6=	0 since	λv	6=
0, so λ is also an eigenvalue of J(θ(o))> J(θ(o))Q(t). On the other hand, by Theorem 4,
J(θ(o))>J(θ(o))Q(t) converges in probability to ΘQ(t) whose eigenvalues are all in [0, λmax]
by Proposition 13. So there exists D2 such that for all d ≥ D2, with probability at least
(1 — δ∕3), the eigenvalues of J(θ(0))Q(t) J(θ(0))> are all in [0,λmax + λmin] for all t. Since
n/(1 - nμ) ≤ (λmin + λmax)-i by our choice of n, We have
I-----n—J (θ(0) )Q(t)J (θ(0))>	≤ 1	(101)
1 - nμ	2
On the other hand, we can use (47) since ∣∣atk2 < C0, so ∣∣ J(θ(0))∣∣F ∣∣ J(θ(t)) - J(θ(0))∣∣	≤
M^Co. Therefore, there exists D3 such that for all d ≥ D3,
(1 - nμ)i - nJ(θ(0))Q(t)J(θ(t))>∣∣2 ≤ 1
nμ
——
2
(102)
For the second term, we have
∣∣g(θ(t))∣∣2 ≤ ∣∣g(θ(t)) - g(θ(o))∣∣2 + ∣∣g(θ(o))∣∣2
≤ ∣∣J 0t))∣∣2 ∣∣θ(t) - θ(0)∣∣2 + Ro ≤ MCo + Ro
(103)
And for the third term, we have
n∣∣∣J(θ(o))∣∣∣	∣∣∣g(θ(o))∣∣∣ ≤nMRo=A	(104)
Thus, we have
kat+1k2 ≤ T) kat∣2 + nM(M"RO)+ A	(105)
v 2j	√ d
So there exists D4 such that for all d ≥ D4, ∣at+ιk2 ≤(1 - ημ) IlatIl2 + 2A. This shows that if
∣at∣2 < Co is true, then ∣at+i∣2 < Co will also be true.
In conclusion, by union bound, we have proved that for any δ > 0, with probability at least (1 - δ)
for all d, ≥ Do = max{Dι,D2, D3, D4}, ∣∣θ(t) - θ(o) ∣∣2 < Co is true for all t. This also implies
that for Ci = MCo + Ro, we have ∣∣g(θ(t))∣∣2 ≤ Ci for all t by (103).
6 Note that Lemma 11 only depends on the network structure and does not depend on the update rule, so we
can use this lemma here.
25
Under review as a conference paper at ICLR 2022
Second, let ∆t = θl(itn) - θ(t). Then we have
∆t+1- ∆t = η(J(θ㈤)Q⑴g(θ⑴)-J(θ⑼)Q⑴glin(θ⑴)-μ∆t)	(106)
which implies
∆t+ι= h(1 - ημ)I - nJ(θ⑼)Q㈤ J(θ(t))>i ∆t + η(J(θ㈤)-J(θ(O)))Q㈤g(θ⑴)(107)
1	ʌ ∕<CC∖	∙ . 1	F F ∙1 • .	. 1	.	/ 1	ς∙∖	1'	Il T、 TA	1
By (102), with probability at least (1 - δ) for all d ≥ D0, we have
k∆t+1k2 ≤ ∣∣(1 - ημ)I - nJ (θ(0))Q(t)J (θ⑴)>(IA k2 + η IIJ (。㈤)- J(。(O))IUlg(/)(
≤(1- ημ) k∆tk2 + ηPC0C1
2	2 j	4J d
(108)
Again, as ∆O = 0, we can prove by induction that for all t,
k∆tk2 < 2MC0C1 d-1/4	(109)
μ
For any test point x such that kxk2 ≤ 1, we have
If⅛) (X)-咸eg(X) I = f (X θ(t)) - flin(x； θl⅛))
≤	f(X； θ(t))	-	flin(X；	θ(t))	+	flin(x;	θ(t))	-	flin(X；。马|
(110)
≤ |f (x； θ(t)) - flin (x； θ(t))∣ + ∣∣vθ f (x； θ(0))∣∣J∣θ(t) - θι^)∣∣2
≤ IIIf(X； θ(t)) - flin(X； θ(t))III + M k∆tk2
For the first term, note that
f	f (X； θ*-f (X； θ⑼)=Vθ f (x； θ(t))(θ(t) - θ(O))
[flin(x； θ⑴)-fiin(x； θ(O)) = Vθf (x； θ(0))(θ(t) - θ(0))
(111)
where θ(t is some linear interpolation between θ(t) and θ(O). Since f(X； θ(O)) = flin(X； θ(O)),
f (X； θ(t)) - flin(x； θ ⑶)| ≤ ∣∣Vθ f (X； θ(t)) -Vθf (X； θ(O))(Mt) -θ(O) IL ≤ M C
EI	1	1	.1 . Γ∙	Il T、 7^Λ	∙ . 1	F F ∙ 1 ∙ .	. 1	. / 1	Γ ∖ Γ∙	11 >	1 11
Thus, we have shown that for all d ≥ DO, with probability at least (1 - δ) for all t and all X,
Ifret)(X)-咸eg(x)∣ ≤ (mc2 + 2MCOCI) d-1/4 = O(d-1/4)
Given that R(fι(Reg) < E for sufficiently large t, this also implies that
∣R(fι(Reg)- R(f⅛))∣ = O(d-"√E+d-1/2)
(112)
(113)
(114)
So for a fixed E, there exists D > 0 such that for all d ≥ D, for sufficiently large t,
R(f揄 <E ⇒R(虑eg) <2e	(115)
By Theorem 5, we have
SUp |f(tERM(X)- fERM(X)I = o(d-1/4)	(116)
t≥O
Combining Theorem 7 with (113) and (116) derives
limsup Ifreg)(X)- fERM(x)∣ = O(d-1/4 + √e)	(117)
t→∞
T . . ∙	7	1	1 . .1	1 .	1	I-I
Letting d → ∞ leads to the result We need.	□
26
Under review as a conference paper at ICLR 2022
C A note on the proofs in Lee et al. (2019)
We have mentioned that the proofs in Lee et al. (2019), particularly the proofs of their Theorem 2.1
and Lemma 1 in their Appendix G, are flawed. In order to fix their proof, we change the network
initialization to (9). In this section, we will demonstrate what goes wrong in the proofs in Lee et al.
(2019), and how we manage to fix the proof. For clarity, we are referring to the following version of
the paper: https://arxiv.org/pdf/1902.06720v4.pdf.
To avoid confusion, in this section we will still use the notations used in our paper.
C.1 Their problems
Lee et al. (2019) claimed in their Theorem 2.1 that under the conditions of our Theorem 5, for any
δ > 0, there exist D > 0 and a constant C such that for any d ≥ D, with probability at least (1 - δ),
the gap between the output of a sufficiently wide fully-connected neural network and the output of
its linearized neural network at any test point x can be uniformly bounded by
Supf ⑴(X)- ∕ι(t)(x)∣ ≤ CdT/	(claimed)	(118)
where they used the original NTK formulation and initialization in Jacot et al. (2018):
hl+1 = W xl + βbl
dl
xl+1 = σ(hl+1)
(Wilj0)~N(0,1)
[b* ~N(0,1)
(∀l = 0,…，L)
(119)
where x0 = x andf(x) = hL+1. However, in their proof in their Appendix G, they did not directly
prove their result for the NTK formulation, but instead they proved another result for the following
formulation which they called the standard formulation:
hl+1 = Wlxl + βbl
xl+1 = σ(hl+1)
and
Wilj0) ~ N(0, ɪ)
ij	dl	(∀l = 0,…，L)	(120)
b* ~N(0,1)
See their Appendix F for the definition of their standard formulation. In the original formulation,
they also included two constants σw and σb for standard deviations, and for simplicity we omit
these constants here. Note that the outputs of the NTK formulation and the standard formulation at
initialization are actually the same. The only difference is that the norm of the weight Wl and the
gradient of the model output with respect to Wl are different for all l.
In their Appendix G, they claimed that if a network with the standard formulation is trained by
minimizing the squared loss with gradient descent and learning rate W = η∕d, where η is our
learning rate in Theorem 5 and also their learning rate in their Theorem 2.1, then (118) is true for
this network, so it is also true for a network with the NTK formulation because the two formulations
have the same network output. And then they claimed in their equation (S37) that applying learning
rate η0 to the standard formulation is equivalent to applying the following learning rates
nW = dd— η	and ηb = dɪ η	(121)
dmax	dmax
to Wl and bl of the NTK formulation, where dmax = max{d0,…，Ql}.
To avoid confusion, in the following discussions we will still use the NTK formulation and initial-
ization if not stated otherwise.
Problem 1. Claim (121) is true, but it leads to two problems. The first problem is that ηbl =
O(d-m1ax) since η = O(1), while their Theorem 2.1 needs the learning rate to be O(1). Nevertheless,
this problem can be simply fixed by modifying their standard formulation as hl+1 = Wlxl +β√d∣bl
where b，0) ~ N(0,d-1). The real problem that is non-trivial to fix is that by (121), there is
27
Under review as a conference paper at ICLR 2022
nW =抖一η. However, note that do is a constant since it is the dimension of the input space, while
dmax
dmax goes to infinity. With that being said, in (121) they were essentially using a very small learning
rate for the first layer W0 but a normal learning rate for the rest of the layers, which definitely does
not match with their claim in their Theorem 2.1.
Problem 2. Another big problem is that the proof of their Lemma 1 in their Appendix G is erro-
neous, and consequently their Theorem 2.1 is unsound as it heavily depends on their Lemma 1. In
their Lemma 1, they claimed that for some constant M > 0, for any two models with the parameters
θ and θ such that θ, θ ∈ B(θ(o),Co) for some constant Co, there is
J (θ)- J (")∣∣F ≤ p∣ ∣∣θ-1∣∣2	(CIaimed)
(122)
Note that the original claim in their paper was ∣∣ J (θ) - J (θ)∣∣ ≤ M p∕<i ∣∣θ - θ*∣∣ . This is because
they were proving this result for their standard formulation. Compared to the standard formulation,
in the NTK formulation θ is Vdl times larger, while the Jacobian J(θ) is Vdl times smaller. This
is also why here we have θ,θ ∈ B(θ(0), Co) instead of θ,θ ∈ B(θ⑼,Cd-1/2) for the NTK
formulation. Therefore, equivalently they were claiming (122) for the NTK formulation.
However, their proof of (122) in incorrect. Specifically, the right-hand side of their inequality (S86)
is incorrect. Using the notations in our Appendix B.4, their (S86) essentially claimed that
∣∣αl-αl∣∣2 ≤ pM∣∣∣θ-θ]∣2	(CIaimed)
(123)
for any θ,θ ∈ B(θ(0),Co), where al = NhhL+1 and αl is the same gradient for the second
model. Note that their (S86) does not have the ∖∕ll in the denominator which appears in (123). This
is because for their standard formulation, θ is yTd times smaller than the original NTK formulation,
while ∣αl ∣2 has the same order in the two formulations because all hl are the same.
However, it is actually impossible to prove (123). Consider the following counterexample: Since θ
choose them such that they only differ in bl1 for some 1 ≤ l < L.
can see that hl+1 and hl+1 only differ in the first element, and
and θ are arbitrarily chosen, we can
∣2 = M - b11∣. We
Then, 11 θ — θ
∣h1+1 — h 1+1 = ∣β(b1 — b：) ∣. Moreover, we have Wl+1 = Wl+1, so there is
W l+1>	Wl l+1>
αl+1 - αl+1 =diag(σ(hl+1))——αl+2 - diag(σ(hl+1))——7o-oll+2
dl	dl
=hdiag(σ(i))- diag(σ(hl+1))i Wppd>αl+2
l+1>
+ diag(σ(hl+1))—7=^(αl+2 - αl+2)
dl
Then we can lower bound ∣∣αl+1 - αl+1∣∣2 by
∣∣αl+1 - ɑl+1∣∣2 ≥ ∣∣[diag(σ(hl+1)) - diag(σ(hl+1))] Wp^ αl+2
2
-Idiag(σ(hl+1)) Wpp∣ (αl+2 - αl+2)
(124)
(125)
2
The first term on the right-hand side is equal to ∣ [σ(h1+1) - σ(h1+1)](W：+1/Pd al+2)∣ where
Wl+1 is the first row of Wl+1. We know that ∣∣ Wl+1∣∣2 = Θ (Pd) with high probability as its
28
Under review as a conference paper at ICLR 2022
elements are sampled from N (0, 1), and in their (S85) they claimed that αl+2 2 = O(1), which
is true. In addition, they assumed that σ is Lipschitz. Hence, We can see that
'iag(σ(hl+1))- diag(σ(i))i WpFαl+2
=O
2
O θ - θZ2
(126)
1 Z ι+ι
—
On the other hand, suppose that claim (123) is true, then αl+2 - αZ l+2
Then We can see that the second term on the right-hand side is O
Il Wl+1∣∣2 = O(，d) and σ(x) is bounded by a constant as σ is Lipschitz. Thus, for a very large d,
the second-term is an infinitely small term compared to the first term, so We can only prove that
∣∣αl+1 - αZ l+1∣∣2 = O ∣∣θ - θZ∣∣2	(127)
Which is different from (123) because it lacks a critical dZ-1/2 and thus leads to a contradiction.
Hence, we cannot prove (123) with the d-1/2 factor, and consequently we cannot prove (122) with
the d/d in the denominator on the right-hand side. As a result, their Lemma 1 and Theorem 2.1
cannot be proved without this critical dZ-1/2 . Similarly, we can also construct a counterexample
where θ and θZ only differ in the first row of some Wl .
C.2 Our fixes
Regarding Problem 1, we can still use an O(1) learning rate for the first layer in the NTK formulation
given that kxk2 ≤ 1. This is because for the first layer, we have
V w o f (x) = √^~ x0 a1，= √^ xa1，	(128)
For all l ≥ 1, we have ∣xl ∣2 = O(dZ1/2). However, for l = 0, we instead have ∣x0 ∣2 = O(1).
Thus, we can prove that the norm of VW0f(x) has the same order as the gradient with respect to
any other layer, so there is no need to use a smaller learning rate for the first layer.
Regarding Problem 2, in our formulation (8) and initialization (9), the initialization of the last layer
of the NTK formulation is changed from the Gaussian initialization WL(O)〜N(0,1) to the zero
initialization WiL,j(0) = 0. Now we show how this modification solves Problem 2.
The main consequence of changing the initialization of the last layer is that (81) becomes different:
instead of ∣∣ Wl∣∣2 ≤ 3pd, we now have ∣∣ Wl∣∣2 ≤ Co ≤ 3pɪ In fact, for any r ∈ (0,1/2), we
〜
〜
can prove that ∣W L ∣
2 ≤ 3dr for sufficiently large d. In our proof we choose r = 1/4.
Consequently, instead of ∣αl ∣2 ≤ M3, we can now prove that ∣αl ∣2 ≤ M3dZr-1/2 for all l ≤ L by
induction. So now we can prove θ - θZ∣∣ instead of O ∣∣θ - θZ∣∣	,
because
•	For l < L, we now have ∣αl+1 ∣2 = O(dZr-1/2) instead of O(1), so we can have the
additional dZr-1/2 factor in the bound.
•	For l = L, although ∣αL+1 ∣2 = 1, note that ∣W L∣2 now becomes O(dZr) instead of
O(dZ1/2), so again we can decrease the bound by a factor of dZr-1/2.
Then, with this critical dZr-1/2, we can prove the approximation theorem with the form
Supf ㈤(X)- fl(t)(x)∣ ≤ CdrT/2	(129)
for any r ∈ (0, 1/2), though we cannot really prove the O(dZ-1/2 ) bound as originally claimed in
(118). So this is how we solve Problem 2.
29
Under review as a conference paper at ICLR 2022
One caveat of changing the initialization to zero initialization is whether we can still safely assume
that λmin > 0 where λmin is the smallest eigenvalue of Θ, the kernel matrix of our new formulation.
The answer is yes. In fact, in our Theorem 4 we proved that Θ is non-degenerated (which means
that Θ(x, x0) still depends on x and x0), and under the overparameterized setting where dL n,
chances are high that Θ is full-rank. Hence, we can still assume that λmin > 0.
As a final remark, one key reason why we need to initialize WL as zero is that the dimension of the
output space (i.e. the dimension of hL+1) is finite, and in our case it is 1. Suppose we allow the
dimension of hL+1 to be d which goes to infinity, then using the same proof techniques, for the NTK
formulation we can prove that supt hL+1(t) - hlLin+1(t) ≤ C, i.e. the gap between two vectors
of infinite dimension is always bounded by a finite constant. This is the approximation theorem we
need for the infinite-dimensional output space. However, when the dimension of the output space is
finite, supt hL+1(t) - hlLin+1(t) ≤ C no longer suffices, so we need to decrease the order of the
norm of W L in order to obtain a smaller bound.
D	Experiment details and additional experiments
D. 1 Experiment details
All experiments are conducted on a Ubuntu 18.04.6 machine with NVIDIA Geforce GTX 1080ti
GPUs. Each model is trained with one GPU. On each of Waterbirds and CelebA, we use a ResNet18
as the model. The model is trained with SGD with momentum = 0.9. On Waterbirds the learning
rate is 10-4, and on CelebA it is 10-3. For Group DRO, ν is selected as 0.01 (see the definition of
ν in (3)). The batch size used for Waterbirds is 128, and for CelebA it is 400. Data augmentation
including random cropping, random horizontal flip and normalization is performed on both datasets.
D.2 Sample weights converge in Group DRO
The results in Section 3 require Assumption 1 which states that each sample weight qi(t) converges
to some positive value as t → ∞. Our readers might wonder how strong this assumption is, and
whether reweighting algorithms satisfy this assumption in practice. In this section we empirically
demonstrate that for Group DRO, the dynamic reweighting algorithm we experiment on, this as-
sumption is satisfied on Waterbirds and CelebA.
Recall that in Section 2.2 we empirically showed that reweighting algorithms could easily overfit
without regularization. Here using the same experimental settings, we keep track of the weight of
each group gk during training, and we plot the group weight curves in Figure 3. We also train the
models longer (1000 epochs on Waterbirds and 300 epochs on CelebA). Clearly we can see that as
the training accuracy converges to 100%, the group weights also converge to an equilibrium. Note
that qi(t) = gk(t)/nk for all zi ∈ Dk , so the sample weights also converge.
(b) CelebA.
Figure 3: Weights of each group in Group DRO on Waterbirds and CelebA. The four curves corre-
spond to the four groups.
30