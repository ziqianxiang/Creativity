Under review as a conference paper at ICLR 2022
Multi-Objective Online Learning
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a systematic study of multi-objective online learning. We
first formulate the framework of Multi-Objective Online Convex Optimization,
which encompasses a novel multi-objective dynamic regret in the unconstrained
max-min form. We show that it is equivalent to the regret commonly used in the
zero-order multi-objective bandit setting and overcomes the problem that the lat-
ter is hard to optimize via first-order gradient-based methods. Then we propose
the Online Mirror Multiple Descent algorithm with two variants, which computes
the composite gradient using either the vanilla min-norm solver or a newly de-
signed L1-regularized min-norm solver. We further derive regret bounds of both
variants and show that the L1 -regularized variant enjoys a lower bound. Extensive
experiments demonstrate the effectiveness of the proposed algorithm and verify
the theoretical advantage of the L1 -regularized variant.
1	Introduction
Traditional optimization methods for machine learning are usually designed to optimize a single
objective. However, in many real-world machine learning applications, we are often required to
optimize multiple correlated objectives simultaneously. For example, in autonomous driving (Huang
et al., 2019; Lu et al., 2019b), the self-driving vehicle needs to learn to solve multiple tasks such as
self-localization and object identification at the same time. In online advertising (Ma et al., 2018a;b),
the advertiser aims to choose the exposure of items to different users so that both the Click-Through
Rate (CTR) and the Post-Click Conversion Rate (CVR) are maximized simultaneously. In many
multi-objective scenarios, the objectives may conflict with each other (Kendall et al., 2018). Hence,
there may not exist any single solution that optimizes all the objectives simultaneously. For example,
in the online advertising scenario, merely optimizing either CVR or CTR will incur the degradation
of performance of the other (Ma et al., 2018a;b).
Multi-objective optimization (MOO) (Marler and Arora, 2004; Deb, 2014) is concerned with opti-
mizing multiple conflicting objectives simultaneously. Many different approaches for MOO have
been proposed, which include evolutionary methods (Murata et al., 1995; Zitzler and Thiele, 1999),
Scalarization methods (FIiege and Svaiter, 2000) and gradient-based iterative methods (Desideri,
2012). Recently, due to the great success of training multi-task deep neural networks, the first-order
gradient-based iterative methods, i.e., Multiple Gradient Descent Algorithm (MGDA) and its vari-
ants, have regained a significant amount of research interest (Sener and Koltun, 2018; Lin et al.,
2019; Yu et al., 2020). These methods compute a composite gradient based on the gradient in-
formation of all the individual objectives and then apply the composite gradient to model update.
The determination of the composite gradient is based on a min-norm solver (Desideri, 2012) which
yields the common descent direction of all the objectives.
However, compared to the increasingly wide application prospect, the first-order gradient-based
iterative algorithms are relatively understudied, especially for the online learning setting. Multi-
objective online learning is very important due to reasons in two main aspects. First, due to the data
explosion in many real-world scenarios such as various web applications, making in-time prediction
requires to perform online learning. Second, the theoretical investigation will lay a solid foundation
for the design of new optimizers of multi-task deep neural networks, such as multi-objective Adam.
In this paper, we conduct a systematic study of multi-objective online learning. To begin with,
we first formulate the framework of Multi-Objective Online Convex Optimization (MO-OCO). The
biggest challenge in the design of MO-OCO is the derivation of an appropriate regret definition in
1
Under review as a conference paper at ICLR 2022
the multi-objective setting. Since the multiple objectives form a vector space, we need a discrepancy
metric to scalarize the loss vector. Specifically, we adopt the Pareto suboptimality gap (PSG), which
is a distance-based discrepancy metric extensively used in multi-objective bandits (Turgay et al.,
2018; Lu et al., 2019a). Then analogously to the single-objective online setting, we define the
multi-objective static regret and the multi-objective dynamic regret. However, since PSG is a metric
motivated purely from the geometric view, it is intrinsically difficult to directly optimize using first-
order gradient-based iterative methods. To remedy this problem, for the multi-objective dynamic
regret, via a highly non-trivial transformation, we derive its equivalent regret in the unconstrained
max-min form, which is much easier to optimize. Unfortunately, we further show that such an
equivalence does not hold for the multi-objective static regret since PSG always yields non-negative
measurements. Hence, in this paper, we mainly focus on studying the multi-objective dynamic regret
and leave the complete treatment of its static counterpart as an open problem.
Based on the proposed MO-OCO framework, we develop the Online Mirror Multiple Descent
(OMMD) algorithm. The key module of OMMD is the gradient composition scheme, which uti-
lizes the information of all the individual gradients to compute a composite gradient that tends to
descend all the losses simultaneously. By directly applying the min-norm solver (Desideri, 2012) in
the offline setting to determine the composition weights, we give the first variant of OMMD termed
as OMMD-I. However, the min-norm solver only uses the instantaneous gradients and ignores the
historical information, which can be very unstable in the online setting where the losses and the gra-
dients at different rounds can vary wildly. To make the learning process more stable, we introduce a
carefully designed L1-regularizer to the vanilla min-norm solver, which results in the second variant
of OMMD, namely OMMD-II.
We then give the theoretical analysis of the proposed OMMD algorithm. Specifically, we derive a
non-trivial dynamic regret bound of OMMD-II, which includes that of OMMD-I as a special case.
The dynamic regret bound is in the same order O(VT1/3T 2/3), where T is the time horizon and VT
is the temporal variability at T, as that of its single-objective counterpart(Zhang et al., 2018). We
further show that the regret bound of OMMD-II is lower than that of OMMD-I.
To evaluate the effectiveness of our proposed algorithm, we conduct extensive experiments using
both simulation datasets and real-world datasets. Specifically, we first design simulation experi-
ments to verify the capability of OMMD-II to track the dynamic online data streams. Then we suc-
cessfully realize adaptive regularization for online learning using our MO-OCO formalism, which
demonstrates the effectiveness of OMMD-II in the convex setting. We further conduct online multi-
task learning experiments with deep neural networks, the results of which show that both OMMD-I
and OMMD-II are effective in the non-convex setting. Moreover, in both simulation and multi-task
deep experiments, OMMD-II yields better performance than OMMD-I, which verifies the theoreti-
cal superiority of the regularized min-norm method over the vanilla min-norm method.
In summary, in this paper, we give the first systematic study of multi-objective online learning,
which includes novel framework, algorithm design and theoretical analysis. We believe that our
paper paves the way for future research on multiple-objective optimization and multi-task learning.
2	Preliminaries
In this section, we briefly review the necessary background knowledge of online convex optimization
and multi-objective optimization.
2.1	Online Convex Optimization
Online Convex Optimization (OCO) (Zinkevich, 2003; Hazan, 2019) is the most commonly
adopted framework for designing online learning algorithms. It can be viewed as a structured re-
peated game between a learner and an adversary. At each round t ∈ {1, . . . , T}, the learner is
required to generate a decision xt from a convex compact set X ⊂ Rn . Then the adversary replies
the learner with a convex function ft : X → R and the learner suffers the loss ft (xt). The goal of
the learner is to minimize the regret with respect to the best fixed decision in hindsight, i.e.,
TT
RS(T) = X ft(xt) - χm∈χ X ft(x*).
2
Under review as a conference paper at ICLR 2022
Note that the above regret is the static regret (Hall and Willett, 2013), which compares the learner’s
cumulative loss with that of a fixed decision. There is another version of regret, namely the dynamic
regret (Hall and Willett, 2013; Zhang et al., 2018), which compares the learner’s cumulative loss
with that of a sequence of changing decisions, i.e.,
T
T
Rd(T) = Eft(Xt)-E minft(X*)∙
t=1	t=1 χt∈x
Any meaningful regret is required to be sublinear in T, i.e., limT →∞ RS/D (T)/T = 0, which
implies that when T is large enough, the learner can perform as well as the best fixed decision in
hindsight (for static regret) or the changing optimal decisions at each round (for dynamic regret).
Online Mirror Descent (OMD) (Hazan, 2019) is a classic first-order online learning algorithm. At
each round t ∈ {1, . . . , T}, OMD yields its decision using the following formula
Xt+1 = arg min /▽♦(Xt),x〉+ Br(x,x∕
x∈X
where η is the step size, R : X → R is the regularization function, and BR (X, X0) = R(X) 一
R(x0) 一 hVR(X0), X 一 X0i is the Bregman divergence induced from R. As a generic algorithm, by
instantiating different regularization functions, OMD can induce two important algorithms, i.e., On-
line Gradient Descent (Zinkevich, 2003) and Online Exponentiated Gradient (Hazan, 2019). Several
papers work on the dynamic regret of online mirror descent (Jadbabaie et al., 2015; Shahrampour
and Jadbabaie, 2017).
2.2	Multi-Objective Optimization
Multiple-objective optimization (MOO) is concerned with solving the problems of optimizing
multiple objective functions simultaneously (Zitzler and Thiele, 1999; Sener and Koltun, 2018). In
general, since different objectives may conflict with each other, there is no single solution that can
optimize all the objectives at the same time. Instead, MOO seeks to find solutions that achieve Pareto
optimality. In the following, we exposit Pareto optimality and related definitions more formally
using a vector-valued loss H = (h1, . . . , hm)> as objectives, where m ≥ 2 and hi : K → R,
i ∈ {1, . . . , m}, K ⊂ R, is the i-th loss function.
Definition 1 (Pareto optimality). (a) For any two solutions x, x0 ∈ K, we say that x dominates
x0, denoted as X Y x0 or x0 * X, if hi(x) ≤ hi(x0) for all i, and there exists one i Such that
hi(x) < hi(x0); otherwise, we say that X does not dominate x0, denoted as X 幺 x0 or x0 5 X.
(b) A solution X* ∈ K is called Pareto optimal if it is not dominated by any other solution in K.
There may exist multiple Pareto optimal solutions. For example, itis easy to show that the optimizer
of any single objective, say, x； ∈ argminχ∈κ h1(x), is Pareto optimal. Different Pareto optimal
solutions reflect different trade-offs among the objectives (Sener and Koltun, 2018; Lin et al., 2019).
Definition 2 (Pareto front). (a) All Pareto optimal solutions form the Pareto set, denoted as PK(H).
(b) The image of PK(H) constitutes the Pareto front, denoted as P(H) = {H (x) | x ∈ PK (H)}.
Now that we’ve established the notion of optimality in MOO, we proceed to introduce the metrics
that measure the discrepancy of an arbitrary solution x ∈ K from being optimal. Recall that, in the
single-objective setting with merely one loss function h : Q → R, where Q ⊂ R, for any z ∈ Q,
the loss gap h(z) - minz00 ∈Q h(z00) is directly the discrepancy measure. However, in MOO with
more than one loss, for any x ∈ K, the loss gap H(x) - H(x00), where x00 ∈ PK (H), is a vector.
Intuitionally, the desired discrepancy metric shall scalarize the vector-valued loss gap and yield
the value 0 for any Pareto optimal solution. In general, there are two commonly used discrepancy
metrics in MOO, namely Pareto suboptimality gap (PSG) (Turgay et al., 2018) and Hypervolume
(HV) (Bradstreet, 2011). As HV is a volume-based metric, it is very difficult to optimize or analyze
via iterative algorithms (Zhang and Golovin, 2020). Hence in this paper, we adopt PSG, which has
been extensively used in multi-objective bandits (Turgay et al., 2018; Lu et al., 2019a).
Definition 3 (Pareto suboptimality gap). For any x ∈ K, the Pareto suboptimality gap to a given
comparator set K* ⊂ K, denoted as ∆(x; K*, H), is defined as the minimal scalar e ≥ 0 that needs
to be Subtractedfrom all entries of H (x), such that H (x) 一 el is not dominated by any point in K*,
where 1 denotes the all-one vector in Rm, i.e.,
∆(x; K* ,H) = inf e s.t. ∀x00 ∈ K*, ∃i ∈ {1,. .. ,m}, hi(x) 一 e < hi(x00).
≥0
3
Under review as a conference paper at ICLR 2022
Clearly, PSG is a distance-based discrepancy metric that motivated from a purely geometric view-
point. In practice, the comparator set K is often set to be the Pareto set PK (H) (TUrgay et al., 2018).
Then for any x ∈ K, its PSG is always non-negative and equals to zero if and only if x ∈ PK (H).
Multiple Gradient Descent Algorithm (MGDA) is an offline first-order algorithm for MOO
(Fliege and Svaiter, 2000; Desideri, 2012). At each iteration l ∈ {1,..., L}, where L is the maxi-
mum number of iterations, it first computes the gradient Vhi(xi) for each objective i ∈ {1,..., m},
and then derive the composite gradient gι = Pm=I λiVhi(χι) as the convex combination of these
multiple gradients; it applies the composite gradient to execute the gradient descent step to update
the decision, i.e., xι+1 = xι - ηgι , where η is the step size. The core module of MGDA is the
determination of the weights λι = (λι1, . . . , λιm ) for the gradient composition, which is given as
m
λι = arg min k XλιiVhi(xι)k22,
λl∈∆m	i=1
where ∆m	=	{λ	∈	Rm	|	λi	≥ 0, i ∈	{1,	. . . , m}, and Pim=1	λi	= 1} denotes the probabilistic
simplex in Rm . This is a min-norm solver which finds the weights in the simplex that yields the
minimum L2 norm of the composite gradient. Thus MGDA is also called the min-norm method.
Existing works (Desideri, 2012; Sener and Koltun, 2018) have shown that MGDA is guaranteed
to decrease all the objectives simultaneously until it reaches a Pareto optimal decision (under the
convex setting where all hi are convex functions).
3	Multi-Objective Online Convex Optimization
In this section, we formally formulate the framework of multi-objective optimization in the online
setting, termed Multi-Objective Online Convex Optimization (MO-OCO).
We tailor the famous online convex optimization (OCO) framework to the multi-objective setting,
which can be viewed as a repeated game between an online learner and the adversarial environment.
At each round t ∈ {1, . . . , T }, the learner generates a decision xt from a given convex compact
decision set X ⊂ Rn. Then the adversary replies the decision with a vectoral loss function Ft(x) :
X → Rm , where its i-th component fti (x) : X → R belongs to the i-th objective, and the learner
suffers the loss Ft(xt) ∈ Rm . The goal of the learner is to generate a sequence of decisions {xt |
1 ≤ t ≤ T} so that the cumulative loss PtT=1 Ft(xt) can be optimized.
Recall that, in the single-objective setting, the performance metric R(T) = PT=I ft(xt) — ft(x⅛),
i.e., the regret, compares the actual decisions Xt with some comparator XJ= ∈ X at each round t ∈
{1, . . . , T}. In general, there are two common types of regret which differ in the comparators, i.e.,
the static regret and the dynamic regret. For the static regret, all comparators Xt=, ∀t ∈ {1, . . . , T}
are identically set as the fixed optimal decision X= w.r.t. all losses in hindsight, i.e., Xt= ≡ X= ∈
arg minx∈X PtT=1 ft(X). For the dynamic regret, the comparator Xt= at each round t is selected as
the optimal decision w.r.t. the instantaneous loss ft at that round, i.e., Xt= ∈ arg minx∈X ft(X).
In analogy, in the multi-objective setting, we define the regret as R(T) = PtT=1 ∆t, where the
quantity ∆t at each round t compares the actual decisions Xt with some comparator Xt= ∈ X .
However, in general, no single decision can optimize all the objectives at the same time. Hence, it
is reasonable to compare Xt with all the Pareto optimal decisions that constitute a comparator set
Xt= ⊂ X. Specifically, we introduce the Pareto suboptimality gap (PSG) (Turgay et al., 2018), i.e.,
∆(xt; X；, Ft) = inf e s.t. ∀x00 ∈ Xj, ∃i ∈{1,..., m}, fJ(xt) — e < ft(X0).	(1)
t	≥0	t	t	t
Then the multi-objective regret can be defined as R(T) = PtT=1 ∆(Xt; Xt=, Ft). Given the above
definitions, we can formulate the multi-objective variants of the static and dynamic regret respec-
tively, by using different comparator set Xt= at each step. Specifically, if we set all Xt= to be the
Pareto set of the cumulative loss PtT=1 Ft, then we can formulate a regret metric termed the multi-
objective static regret (recall that PX(F) denotes the Pareto set of F)
TT
RMOS(T):= X ∆(xt; X*, Ft), where X* = PX(X Ft).
4
Under review as a conference paper at ICLR 2022
Alternatively, if We set Xj to be the Pareto set of the instantaneous loss Ft at each round t, then We
can give a regret metric termed the multi-objective dynamic regret
T
RMOD(T) := X ∆(xt; X*,Ft),	where X* = PX (Ft), ∀t ∈ {1,...,T}.
t=1
Recall that, PSG is a zero-order metric motivated in a purely geometric sense, namely, its calculation
can be vieWed as a constrained minimization problem (1) With an unknoWn boundary fti(x00), ∀x00 ∈
Xt* . Hence, it is intrinsically complex to design a first-order algorithm to optimize PSG given its
unknoWn constraints, not to mention the regret analysis.
Surprisingly, specific to the multi-objective dynamic regret RMOD, We can transform it into an un-
constrained max-min form as folloWs. The derivation utilizes Pareto optimality of Xt* and is highly
non-trivial, Which is given in the appendix due to the space limit. The equivalent form is closely
related to the dynamic regret, as it recovers the dynamic regret defined for λt>Ft, t ∈ {1, . . . , T}
if We determine λt at each round t ∈ {1, . . . , T} beforehand. Moreover, it exactly reduces to the
standard dynamic regret RD in the single-objective setting.
Proposition 1. The multi-objective dynamic regret has an equivalent form, i.e.,
T
RMOD(T)=	sup	inf	X(λt*>Ft(xt) - λt*>Ft(xt*)),	(2)
X*∈X,*,1≤t≤T λ；…,N∈∆m y
t t	t=1
where ∆m represents the probabilistic simplex in Rm .
Remark. In the single-objective setting Where m = 1, the probabilistic simplex ∆m collapses into
a single point {1}. Moreover, the Pareto set Xt* of the scalar loss function Ft : X → R reduces to
the optimal set arg minx∈X Ft(x). Hence We have RMOD (T) = PtT=1(Ft(xt) - minx∈X Ft(x)),
Which is exactly the dynamic regret RD (T) in the standard online setting.
Unfortunately, for the multi-objective static regret RMOS, such a correspondence does not exist. Here
is the reason. In RMOS, the comparator set X* is the Pareto set of the cumulative loss PtT=1 Ft rather
than the instantaneous loss Ft . Hence, at some specific round t, the actual decision xt may Pareto
dominate all decisions in X* W.r.t. the instantaneous Ft, and so We expect the discrepancy metric
∆t to give a negative measurement. HoWever, PSG (as Well as other commonly used discrepancy
metrics such as Hypervolume) is alWays non-negative, so the induced RMOD is not aligned With RS .
For example, When m = 1, We have RMOS (T) = supx*∈X* PtT=1 max{Ft(xt) -Ft(x*), 0}, Which
can be much looser than the static regret RS (T) = supx*∈X* PtT=1 Ft(xt) - Ft(x*). Therefore, the
analysis of RMOD is intrinsically complex if We only use existing discrepancy metrics; its analysis
may require a completely neW discrepancy metric ∆t that alloWs to give a negative measurement.
Given the limits of existing discrepancy metrics to characterize RMOS, in this paper, We mainly focus
on the dynamic variant RMOD. Indeed, the algorithm design and theoretical analysis W.r.t. RMOD are
already highly non-trivial. We Will leave the analysis of RMOS for future Works.
4	Online Mirror Multiple Descent
In this section, We first present the Online Mirror Multiple Descent (OMMD) algorithm, then provide
theoretical analysis for it.
4.1	The Algorithm
The protocol of OMMD is given in Algorithm 1. At each round t, the learner computes the gradient
for each loss Vfi(xt), then determines the weights for the composition of these multiple gradients,
and finally executes an online mirror descent step using the composite gradient. For simplicity, We
define the matrix form of the multiple gradients as VFt(xt) = [Vft1(xt), . . . , Vftm(xt)] ∈ Rn×m.
The core module of OMMD is the composition of the multiple gradients. In intuition, the composite
gradient gt shall elaborately utilize the information of all the individual gradient. As illustrated in
Preliminary (see the MGDA part), in the offline setting, there is one simple yet effective scheme,
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Online Mirror Multiple Descent (OMMD)
1:	Input: Convex set X, time horizon T, regularization parameter a, learning rate η, regulariza-
tion function R.
2:	Initialize: x1 ∈ X, λ0 ∈ ∆m.
3:	for t = 1, . . . , T do
4:	Predict with xt and receive the vector loss function Ft : X → Rm .
5:	Compute the multiple gradients VFt(xt) = [▽/，(xt),..., Vfj,m(χt)] ∈ Rn×m.
6:	Determine the weights for the gradient composition
λt = arg min kVFt(xt)λk22;	(OMMD-I)	(3)
λ∈∆m
λt = argminkVFt(xt)λk22 + αkλ - λt-1k1.	(OMMD-II)	(4)
λ∈∆m
7:	Compute the composite gradient gt = VFt(xt)λt.
8:	Perform online mirror descent using the composite gradient
xt+1 = arg min ηhgt, xi + BR(x,xt).	(5)
x∈X
9:	end for
i	.e., the min-norm method (Desideri, 2012; Sener and Koltun, 2018), which computes a common
descent direction that can descend all the losses simultaneously. We directly apply it to the online
setting, which results in the first variant of OMMD, i.e., OMMD-I, as shown in Algorithm 1.
Despite its simplicity, OMMD-I may not yield optimal decisions in the online setting, since the
min-norm method is designed in the offline setting, which does not capture the characteristics of
the online setting. Specifically, the composition weights λt given by the min-norm solver are deter-
mined solely by the gradients at the instantaneous round t, regardless of the historical information.
Indeed, this makes sense in the offline setting, where the optimized loss function does not change
too much. In the online setting, however, the losses at different rounds can vary wildly and so are
the gradients consequently. Hence, directly applying the min-norm method will yield very different
λts at different rounds, which makes the learning process unstable and even fail to converge.
To fix this issue, we propose to add a regularizer r(λ, λt-1) to the min-norm solver when deter-
mining the composition weights λt, where λt-1 denotes the composition weights at the precedent
round. Such a regularizer ensures that the new weights λt will not move too far away from the
precedent weights λt-1. In principle, r(λ, λt-1) can take many forms such as L1-norm, L2-norm
and KL divergence etc. Here we use the L1 norm since it aligns well with the simplex constraint of
λ. This L1 -regularized min-norm method results in the second variant of OMMD, i.e., OMMD-II,
as shown in Algorithm 1. Later we will show that OMMD-II attains a lower theoretical regret bound
than OMMD-I, and it is also much more robust in experiments.
4.2	Analysis
We first provide a general bound for OMMD, which is agnostic of the choice of λt at each round.
Assumption 1 (Bregman divergence). The regularization function R is 1-strongly convex with
respect to a norm k ∙ k∙ In addition, the Bregman divergence is Y-Lipschitz continuous, i.e.,
BR (x, z) - BR(y, z) ≤ γkx - yk, ∀x, y, z ∈ domR, where domR denotes the domain of the
regularization function R and satisfies X ⊂ domR ⊂ Rn.
Lemma 1. Suppose the diameter of the decision set X is bounded by D. Assume Ft is bounded,
i.e., |fti(x)| ≤ F for any x ∈ X, t ∈ {1, . . . , T}, i ∈ {1, . . . , m}. Then for any δ ∈ {1, . . . , T},
OMMD-I or OMMD-II with composition weights λt at each round t attains the following regret
T-1	T-1
RMOD(T) ≤2δX SUp ∣fi(x) - fi+ι(x)∣ + 4δFT X |内-λt+4∣ι
t=1 x∈X	t=1
T
+η χ kvFt(Xt)λtk2+王 d^Fe.
2t1	2 η δ
6
Under review as a conference paper at ICLR 2022
The bound can be further simplified under the assumptions of temporal variability and Lipschitz
continuity, which are commonly used in dynamic regret analysis (Besbes et al., 2015; Yang et al.,
2016; Campolongo and Orabona, 2021).
Assumption 2 (Temporal variability). For each i ∈ {1, . . . , m}, there exists some positive and
finite VT such that PtT=-11 supx∈X |fti(x) - fti+1(x)| ≤ VT.
Assumption 3 (Lipschitz continuity). For each i ∈ {1, . . . , m}, there exists some positive and
finite G such that, the i-th loss ft at each round t ∈ {1,...,T} is G-Lipschitz continuous w.r.t. k ∙ k,
i.e., |fti(x) - fti(x0)| ≤ Gkx - x0k.
In the convex setting, Lipschitz continuity guarantees bounded gradients, i.e., kVfii(χ)k* ≤ G for
anyt ∈ {1, . . . , T}, i ∈ {1, . . . , m}, x ∈ X. We can now derive a general regret bound for OMMD.
Theorem 1. Assume the step size η satisfies GVT ≤ η ≤ 4VT. Then OMMD-I or OMMD-II with
weights λt at each round t attains the following regret
RMOD(T) ≤ η, T + η X(IlVFt(Xt)λtk2 + --γf kλ kλt - λt-ιkI) +
2	2	VT
t=1	T
4γDVT
η2G2 .
Remark. The above theorem shows the theoretical superiority of OMMD-II over OMMD-I or
naive linear scalarization that always uses fixed composition weights λt ≡ λ at each round. In
particular, with the proper choice of α = Y , OMMD-II adaptively selects λt to minimize the
term kVFt(xt)λtk22+ αkλt - λt-1 k1. This term will become larger with any other choice of λt.
Finally, specific to OMMD-II, we have the following regret bound. Note that, this bound actually
relies on the assumption that Ω(1) ≤ VT ≤ o(T), which is implicitly assumed in other works for
dynamic online learning (Besbes et al., 2015; Yang et al., 2016; Campolongo and Orabona, 2021).
We also extend the bound to arbitrary value of VT ≥ 0 (see Corollary 2 in the appendix).
Corollary 1. With η = G (YDTT- )1/3 and α
8FG2T2, OMMD-IIaChieveS the following regret
RMOD (T) ≤ O(VT1/3T 2/3).
5 Experiments
In this section, we conduct extensive simulation and real-world experiments to evaluate the effec-
tiveness of our proposed algorithm. In our experiments, we mainly compare our proposed OMMD-II
with two baseline algorithms: (i) linear optimal (lin-opt) performs single-objective online learning
using the linearized loss λ>F at each round t, where the linearization weights λ is fixed and decided
by a grid search on ∆m ; note that, it is equivalent to using a fixed composition weight λt = λ in
OMMD. (ii) min-norm extends the famous min-norm method (Desideri, 2012; Sener and Koltun,
2018) to the multi-objective online setting, which has been described as OMMD-I in Algorithm 1.
5.1	Simulation Experiments: Tracking the Pareto Front
In the simulation setup, the goal is to track two points ξt1, ξt2 moving on the plane R2. The two points
cycle along a circle with a ratius of 1, i.e., C = {ξ ∈ R2 | kξk2 = 1}, namely, for each i ∈ {1, 2},
ξti = (cos θti, sin θti) is determined by some angle θti. For each i ∈ {1, 2}, we set a positive integer P i
as the rotating period, which is unknown to the learner. The two points are initialized by θ11 = 0 and
θ2 = n/2, and iteratively generated as follows: at each round t, for each i ∈ {1, 2}, the adversary
independently samples an angle δqi from a Gaussian distribution N(2∏∕Pi, 1∕√Pi), then generates
the next point ξi+ι via θi+ι = θli - δi. Note that, We have Eθi+ι = θ∖ + 2∏t∕Pi, which implies
that in average ξti rotates clockwise uniformly along the circle C, with a period of Pi .
At each round t, the learner generates a decision xt from X = {x ∈ R2 | kxk2 ≤ 2}, which is a L2-
norm ball with a radius of 2. Then it acquires the positions of ξt1 , ξt2 and suffer two losses fti (xt) =
kxt - ξtik2∕2 for i ∈ {1, 2}. In this problem, the Pareto set of the vectoral loss ft = (ft1, ft2) at each
round t is exactly the line segment linking ξ1 and ξ2, i.e., Xj = {λξ1 + (1 - λ)ξ2 | λ ∈ [0,1]}.
For any decision xt ∈ X, its PSG exactly equals to the squared distance between xt and the line
segment. The setup and PSG measurement are summarized in Figure 1 (left plot).
7
Under review as a conference paper at ICLR 2022
(a) simulation setup
(b) runtime performance
Figure 1: Simulation setup and results. Left: Tar-
get points ξt1 , ξt2 cycle along the circle. At each
round t, the Pareto set is the line segment [ξt1 , ξt2],
and PSG measures the distance between xt and
the line segment. Right: Performance compari-
son of OMMD-II and baselines.
Figure 2: Results to verify the effectiveness of
adaptive regularization using OMMD-II in real-
world online classification tasks. The two plots
compare the performance of adaptive regulariza-
tion (OMMD-II) and fixed regularization (lin-
opt) on protein and covtype datasets.
In our experiments, we set T = 10, 000. To simulate the adversarial dynamic change, we further
set P 1 = 10, P2 = 20 for the first T1 = 3, 000 rounds, and P 1 = 20, P2 = 10 for the remaining
T2 = 7, 000 rounds. As our goal is the track the Pareto front, we adopt the average PSG, namely
Pt∈[τ] ∆t(χt)∕T, as the performance metric. Accompanying the lin-opt baseline, We also consider
its two variants: lin-1 which sets λ as the optimal weights for the first T1 rounds, and lin-2 which
set λ to be optimal regarding the last T2 rounds. The learning rates η in all algorithms and the
regularization parameter α in OMMD-II are set as What the corresponding theories suggest.
Figure 1 (right plot) shoWs the performance of all the examined algorithms. From the results, We
observe that OMMD-II achieves the loWest PSG, shoWing its ability to track the Pareto front; mean-
While, min-norm appears very unstable in the dynamic setting, even Worse than linear scalarization.
In particular, by comparing OMMD-II and lin-1, We find that linear scalarization With carefully
selected Weights may attain performance comparable to our algorithm during some certain phase;
hoWever, in the dynamic setting Where the pattern drifts over time, the performance of linear scalar-
ization may become very unstable or even drop severely, While our algorithm is much more robust.
5.2	Online Convex Experiments: An Application to Adaptive Regularization
In many real-World online applications, regularization is often applied to avoid overfitting. A most
common Way is to add a regularization term r(x) to the loss ft (x) in each round, and optimize the
regularized loss ft(x) + σr(x) instead (McMahan, 2011). The strength σ of regularization is often
treated as a hyperparameter that needs to be decided carefully beforehand, e.g., via a grid search.
The formalism of multi-objective online learning provides an another Way to realize regularization.
Since r(x) often measures the complexity of x, it can be vieWed as the second objective to be
optimized. Specifically, by constructing a vectoral loss Ft(x) = (ft(x), r(x)) in each round, We can
cast the regularized online learning into a tWo-objective online optimization problem. Compared to
the previous approach With a fixed strength σ, in our approach, the strength is implied by the Weights
in gradient composition, namely σt = λt2∕λt1, Which is adaptive at each round t.
We run experiments on tWo online benchmark datasets. (i) protein: A bioinformatics dataset for
protein type classification (Wang, 2002), Which has 17 thousand instances With 357 features. (ii)
covtype: A biological dataset collected from a non-stationary environment, Whose goal is to predict
the cover type of forests at a particular location (Blackard and Dean, 1999), Which has 50 thousand
instances With 54 features. For both classification tasks, We use the logistic loss as the first objective,
and the squared L2-norm of model parameters (i.e., the L2-regularizer) as the second objective.
In the experiments, We adopt L2-norm balls as the decision set and set the diameter K = 10. For
OMMD-II, the parameter α is simply set as 0.2. For fixed regularization, the strength σ = λt2∕λt1 is
determined by a grid search over (λt1, λt2) ∈ ∆2, and We denote this method as lin-opt. Moreover, for
both OMMD-II and lin-opt, learning rates η are decided via a grid search over {0.1, 0.2, . . . , 2.0}.
Since the ultimate goal of regularization is to enhance predictive performance, We adopt the average
of cumulative loss, namely Pt∈[T] lt(xt)∕T Where lt(xt) is the classification loss at round t, as the
performance metric. The performance of both algorithms are reported in Figure 2. The results shoW
that OMMD-II attains loWer loss than lin-opt in all the examined tasks, Which shoWs the superiority
of adaptive regularization using our online MOO technique over fixed regularization.
8
Under review as a conference paper at ICLR 2022
(a) Task L, Average Cumulative Loss
(d) TaskR, Average Cumulative Loss
(e) TaskR, Training Loss
Figure 3: Results to verify the effectiveness of OMMD-II for online deep learning. The plots show
the average cumulative loss, training loss and test loss for both tasks (task L/R) on MultiMNIST.
5.3 Online Non-Convex Experiments: Multi-Task Deep Learning
Finally, we evaluate OMMD-II in the online non-convex setting. We choose to use the MultiMNIST
dataset (Sabour et al., 2017), which is a multi-objective version of the famous MNIST dataset for
image classification and commonly used in deep multi-task learning (Sener and Koltun, 2018; Lin
et al., 2019). In MultiMNIST, each sample is constructed by putting a random image at the top-left
and another image at the bottom-right. Our goal to classify the digit on the top-left (task L) and to
classify the digit on the bottom-right (task R) at the same time.
We follow Sener and Koltun (2018)’s setup and adopt the LeNet architecture. For linear scalariza-
tion, we consider two choices of weights, namely (0.5, 0.5) and (0.75, 0.25). For all the examined
algorithms, the learning rates η are selected via a grid search over {0.0001, 0.001, 0.01, 0.1}. For
OMMD-II, the parameter α is set according to our theory. Note that, in online experiments, the sam-
ple arrives one after another in a sequential manner, which is different from stochastic optimization
where sample batches are randomly sampled from the whole training set (Sener and Koltun, 2018).
Figure 3 compares the average cumulative loss, training loss and test loss of all the examined al-
gorithms for both tasks. Note that, the first metric is typically used in online experiments and the
last two are commonly used in stochastic experiments (Reddi et al., 2018). The results show that
OMMD-II outperforms OMMD-I (min-norm) and linear scalarization (lin) in all metrics, which
shows that our proposed algorithm is also effective in the online non-convex setting.
6	Conclusions
In this paper, we conduct a systematic study of multi-objective optimization in the online setting. We
first formulate the framework of Multi-Objective Online Convex Optimization. Then we devise the
Online Mirror Multiple Descent algorithm, which is the first gradient-based multi-objective online
learning algorithm and has a special design when tailoring multiple gradient algorithm to online
learning, namely regularized min-norm solver. Theoretically, we provide the first paradigm of regret
analysis for multi-objective online convex optimization. We finally conduct extensive experiments
to demonstrate the effectiveness of our proposed algorithm. Future works may include developing a
framework based on the Hypervolume metric, or giving an analysis of multi-objective static regret.
7	Ethics Statement
As a study on a general learning problem, our work will not incur ethical issues by itself. However,
ethical issues may arise if our learning method is improperly applied to some application fields - just
as any other general learning method if it is misused.
9
Under review as a conference paper at ICLR 2022
8	Reproducibility Statement
For every theoretical statement in our paper (including proposition, lemma, theorem and corollary),
we give a detailed proof in Appendix C. The codes to reproduce our empirical results are provided
in the supplementary materials.
References
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations
research, 63(5):1227-1244, 2015.
Jock A Blackard and Denis J Dean. Comparative accuracies of artificial neural networks and dis-
criminant analysis in predicting forest cover types from cartographic variables. Computers and
electronics in agriculture, 24(3):131-151, 1999.
Lucas Bradstreet. The hypervolume indicator for multi-objective optimisation: calculation and use.
University of Western Australia Perth, 2011.
Nicolo Campolongo and Francesco Orabona. A closer look at temporal variability in dynamic online
learning. arXiv preprint arXiv:2102.07666, 2021.
Nicolo Cesa-Bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed
share (and feels no regret). arXiv preprint arXiv:1202.3323, 2012.
Kalyanmoy Deb. Multi-objective optimization. In Search methodologies, pages 403-449. Springer,
2014.
Jean-Antoine Desideri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization.
Comptes Rendus Mathematique, 350(5-6):313-318, 2012.
Jorg Fliege and Benar FUx Svaiter. Steepest descent methods for multicriteria optimization. Mathe-
matical Methods of Operations Research, 51(3):479-494, 2000.
Eric Hall and Rebecca Willett. Dynamical models and tracking regret in online convex program-
ming. In International Conference on Machine Learning, pages 579-587. PMLR, 2013.
Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.
Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, and Ruigang Yang. The
apolloscape open dataset for autonomous driving and its application. IEEE transactions on pat-
tern analysis and machine intelligence, 42(10):2702-2719, 2019.
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimiza-
tion: Competing with dynamic comparators. In Artificial Intelligence and Statistics, pages 398-
406, 2015.
Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 7482-7491, 2018.
Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, and Sam Kwong. Pareto multi-task learning. In
Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019), 2019.
Shiyin Lu, Guanghui Wang, Yao Hu, and Lijun Zhang. Multi-objective generalized linear bandits.
arXiv preprint arXiv:1905.12879, 2019a.
Weixin Lu, Yao Zhou, Guowei Wan, Shenhua Hou, and Shiyu Song. L3-net: Towards learning
based lidar localization for autonomous driving. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 6389-6398, 2019b.
10
Under review as a conference paper at ICLR 2022
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relation-
ships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1930-1939,
2018a.
Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. Entire
space multi-task model: An effective approach for estimating post-click conversion rate. In The
41st International ACM SIGIR Conference on Research & Development in Information Retrieval,
pages 1137-1140, 2018b.
R Timothy Marler and Jasbir S Arora. Survey of multi-objective optimization methods for engineer-
ing. Structural and multidisciplinary optimization, 26(6):369-395, 2004.
Brendan McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems and
l1 regularization. In Proceedings of the Fourteenth International Conference on Artificial Intelli-
gence and Statistics, pages 525-533. JMLR Workshop and Conference Proceedings, 2011.
Tadahiko Murata, Hisao Ishibuchi, et al. Moga: multi-objective genetic algorithms. In IEEE inter-
national conference on evolutionary computation, volume 1, pages 289-294, 1995.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. arXiv
preprint arXiv:1710.09829, 2017.
Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In Proceedings
of the 32nd International Conference on Neural Information Processing Systems, pages 525-536,
2018.
Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments
using mirror descent. IEEE Transactions on Automatic Control, 63(3):714-725, 2017.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent.
arXiv preprint arXiv:1107.4080, 2011.
Eralp Turgay, Doruk Oner, and Cem Tekin. Multi-objective contextual bandit problem with sim-
ilarity information. In International Conference on Artificial Intelligence and Statistics, pages
1673-1681. PMLR, 2018.
Jung-Ying Wang. Application of support vector machines in bioinformatics. Taipei: Department of
Computer Science and Information Engineering, National Taiwan University, 2002.
Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal
dynamic regret of online learning with true and noisy gradient. In International Conference on
Machine Learning, pages 449-457. PMLR, 2016.
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems,
33, 2020.
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments.
arXiv preprint arXiv:1810.10815, 2018.
Richard Zhang and Daniel Golovin. Random hypervolume scalarizations for provable multi-
objective black box optimization. In International Conference on Machine Learning, pages
11096-11105. PMLR, 2020.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th international conference on machine learning (icml-03), pages 928-936,
2003.
Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: a comparative case study
and the strength pareto approach. IEEE transactions on Evolutionary Computation, 3(4):257-
271, 1999.
11
Under review as a conference paper at ICLR 2022
Appendix
The appendix is organized as follows. Appendix A provides more details of OMMD, including a
discussion on the lazy version of mirror descent and how to efficiently compute the composition
weights for the regularized min-norm solver. Appendix B supplements the theoretical results with
more general regret bounds for arbitrary temporal variability VT ∈ [0, ∞); such a general setting
is often not considered in a typical analysis of dynamic online learning (Besbes et al., 2015; Zhang
et al., 2018), but we believe that adding such general bounds will make the analysis more self-
contained. Appendix C provides detailed proofs of all theoretical claims in this work, which have
been omitted in our main paper due to the space limit.
A	More Details of The Algorithm
In this section, we provide more details to help better understand the design of OMMD.
First, we note that, our proposed OMMD is actually based on the agile version of online mirror
descent (Hazan, 2019), where the updated model directly moves to its projecting point onto the
decision set at each round. However, we can easily make an analogy and devise a lazy version of
OMMD. Specifically, we only need to use a lazy projection operation in the mirror descent step with
the composite gradient instead of the agile projection operation (line 7 in Algorithm 1). Note that,
the analysis of the lazy version is very similar to that of the agile version (Hazan, 2019).
Next, we show that, when calculating the weights λt for gradient composition, our regularized min-
norm solver only need very light computation, just as the original min-norm solver in stochastic
optimization (Sener and Koltun, 2018; Lin et al., 2019). Specifically, similar to (Sener and Koltun,
2018), we first consider the setting of two objectives, namely m = 2. In this case, when λ, λt-1 ∈
∆2, the L1 regularizer ∣∣λ - λt-1k1 equals to 2∣λ1 - λ1-∕. Then the optimization problem on λ at
round t reduces to (since there are only two objectives, the superscripts of λ1 and λt1-1 are omitted)
min ∣∣λgι + (1 - λ)g2∣∣2 + 2α∣λ - λ~ι∣.
λ∈[0,1]
Interestingly, we find that, the above problem also has an closed-form solution as below.
Proposition 2. Setλι = (g>(g2 -gι) + α)∕∣g2 -gι∣2, and Xr = (g>(g2 -gι) - α)∕∣g2 -gι∣2.
Then the above optimization problem has a closed-form solution, i.e.,
{max{0,λL},	Xl ≤ λ-ι;
min{1, λR},	λR ≥ λt-1;
Xt-1,	otherwise.
Proof. We solve the following two quadratic sub-problems respectively, namely,
min ∣Xg1 + (1 - X)g2 ∣2 + 2α(Xt-1 - X),
λ∈[0,λt-1]
as well as
min ∣Xg1 + (1 - X)g2∣2 + 2α(X - Xt-1).
λ∈[λt-1,1]
The former problem produces X = max{min{XL, Xt-1}, 0}, and the latter one gives X =
max{min{XR, 1}, Xt-1}. Therefore, the solution to the original optimization problem can then be
derived by comparing the optimal values of the two sub-problems, which gives the desired solution
in the proposition.
Now that we have derived the closed-form solution to the regularized min-norm solver with any
two gradients, in priciple, we can apply Sener and Koltun (2018)’s technique to efficiently compute
the solution to the solver with more than two gradients. Specifically, we can adopt the Frank-Wolfe
method, namely, iteratively choose a pair of gradients and update the composition weights with these
two gradients.
12
Under review as a conference paper at ICLR 2022
B More Details of Theoretical Analysis
In this section, we provide more details and highlight technical lemmas in theoretical analysis, which
are omitted in our main paper due to space limitation.
B.1	A More Detailed Regret Bound for OMMD-II
In Corollary 1, we only give the order of OMMD-II’s regret bound w.r.t. the core factors m, T and
VT . In the following theorem, we supplement a more detailed regret bound for OMMD-II which
explicates the dependency of other factors on the regret. The proof is presented in Appendix C.
Theorem 2.	Set η = G (YDTT- )1/3 and a = 'F，；TT. Then OMMD-II attains thefollowing regret
RMOD(T)≤( gdd )1/3 (VT)1/3 xXλ∈f(kvFt(χt)λ 上+
+ 2(γDG2)1∕3(Vr )1/3 T 2/3.
8FGT2 kλ 一 λt-1k1)
Note that, as we have discussed in our main paper, just like its simplified version Corollary 1, the
above theorem is derived under the assumption of “regular" temporal variability, namely Ω(1) ≤
VT ≤ o(T), which is implicitly assumed in other works for dynamic online learning (Besbes et al.,
2015; Yang et al., 2016; Campolongo and Orabona, 2021). To give a more general analysis, in the
following subsection, we also provide strict regret bounds that are valid for arbitrary VT ∈ [0, ∞).
B.2 Regret Bounds for OMMD-II Under Arbitrary Temporal Variab ility
We here provide a more strict regret bound for OMMD-II under arbitrary temporal variability VT ∈
[0, ∞). Specifically, we first give a detailed regret bound in analogy to Theorem 2. The proof of this
theorem is given in Appendix C.
Theorem 3.	In OMMD-II, set η = min{max{G (YGTT )1/3, GVT}, 4VL} and a = 'FGTT. Then
it attains the regret bound
Rmod(T) ≤ max{(Gl)1/3(V^ ^λf (8FGT2kλ - λt-1k1 + |~£(々冈，
+ 2(γDG2 )1∕3(VT )1/3T 2/3, 6VT,	3G (2γD)1∕2T 1/2}.
The above bound can be rewritten into a simpler form, if we are only interested in its order w.r.t.
m, T and VT , just as in Corollary 1.
Corollary 2. In OMMD-II, set η = min{max{G(YDTT )1/3, GVVT}, 4VL} and a = 'FGTT. Then
it attains the following regret
RMOD(T) ≤ max{O(VT 1/3T 2/3), O(VT), O(T 1/2)}.
C Omitted Proofs
In this section, we provide the detailed proofs of Proposition 1, Lemma 1, Theorem 1 and Corollary
1 in our main paper as well as Theorem 2, Theorem 3 and Corollary 2 in the appendix.
C.1 Proof of Proposition 1
Proof. We first analyze the PSG measurement ∆t(xt) at each round t ∈ {1, . . . , T}. Specifically,
for any comparator x ∈ X, we first define the pair-wise suboptimality gap between decisions xt
and x, namely,
δt (xt; x) = inf { | Ft (xt) - 1	Ft (x)}.
13
Under review as a conference paper at ICLR 2022
Then ∆t (xt) can be evaluated via the pair-wise suboptimality gap as ∆t (xt)	=
supx∈PX(Ft) δt(xt, x).
We then focus on the pair-wise gap δt(xt； x) w.r.t. any Pareto optimal decision X ∈ Xj ≡ PX(Ft).
Since x is a Pareto optimal decision of Ft, from the definition of Pareto optimality, there must exist
some i ∈ {1, . . . , m} such that fti (ii)(xt) - fti(x) ≥ 0.
The pair-wise suboptimality gap has an equivalent expression, namely,
δt(Xt； X)=	min	{(fk(χt) - fk(X))+}
k∈{1,...,m}
where (l)+ = max{l, 0}, l ∈ R is the truncation operator. Denote Um = {ek | 1 ≤ k ≤ m} as the
set of all unit vector in Rm, then we equivalently have
δt (Xt； X) = min λ> (Ft(Xt) - Ft (X))+ .
λ∈Um
Note that, we here slightly abuse the truncation operator (l)+ to allow l to be a vector in Rm, which
represents the vector whose i-th coordinate equals to max{li, 0} for any i ∈ {1, . . . , m}.
Now the calculation of δt (Xt； X) becomes a minimization problem over λ ∈ Um. Since Um is
a discrete set, we can apply a linear relaxation trick. Specifically, we now turn to minimize the
quantity p(λ) = λ> (Ft (Xt) - Ft(X))+ over the convex curvature of Um, which is exactly the
probability simplex ∆m = {λ ∈ Rm | λ 0, kλk1 = 1}. Note that, Um contains all vertexes of
△m. Since infλ∈∆m p(λ) is a linear optimization problem, the minimal point λ^τ must be a vertex
of the simplex, i.e., λt> ∈ Um. Thus the relaxed problem is equivalent to the original problem,
namely,
min λ> (Ft(Xt) - Ft(X))+ = inf λ> (Ft(Xt) - Ft(X))+.
λ∈Um	λ∈∆m
For now, we have transformed the calculation of pair-wise suboptimality gap δt (Xt； X) into a opti-
mization problem of finding the minimal linear scalarization of (Ft(Xt) - Ft(X))+. Hence, the PSG
at each round t can be expressed as
△t(Xt) = sup inf λ> (Ft(Xt) - Ft (X))+ .
X∈X* λ∈∆m
In the above expression, the existence of truncation operator (∙)+ will incur irregularity (i.e., non-
linearity) when we try to optimize the loss Ft . Suprisingly, we find that such operator can be
dropped when we compute ∆t (Xt) w.r.t. the Pareto set Xj, as shown in the following.
Lemma 2. Let Xt be the Pareto set of Ft : X → Rm. Thenfor any Xt ∈ X, it holds that
sup inf λ> (Ft(Xt) - Ft(X))+ = sup inf λ> (Ft(Xt) - Ft(X)).
X∈X * λ∈∆m	X∈X* λ∈∆m
Proof. Define the alternative ”gap” metric without the truncation operator as δt0 (Xt； X) =
inf λ∈∆m λ> (Ft (Xt) - Ft(X)). Moreover, define the supremum of δt0 (Xt； X) over X ∈ Xtt as
△t(Xt) = supχ∈χ* δ0(Xt； x). Then from the definition of truncation operator (∙)+, we have
δ(Xt;χ) ≥ δ0(xt; x) and ∆t(Xt) ≥ ∆t(χt).
It then suffices to prove that, for any given Xt ∈ X, there exists some certain Xtt ∈ Xtt such that the
value of δ0(xt; Xt) can be as large as ∆t(χt). Indeed, if this is the case, then ∆t(Xt) ≤ ∆t(χt), and
hence the two quantities △t(Xt) and △0t(Xt) are equal. We consider the following two cases:
(i) Xt is already a Pareto optimal point of Ft, i.e., Xt ∈ Xtt . Then from the definition of PSG, we
directly have △t (Xt) = 0. Notice that δ0(Xt; Xt) = 0, and hence △t (Xt) = supx∈X * δ0(Xt; X) ≥
δ0(Xt; Xt) = 0. Consequently, the relation △t(Xt) ≤ △0t(Xt) holds in this case.
(ii) Xt is not a Pareto optimal point ofFt, i.e., Xt ∈/ Xtt. Then we have △t(Xt) > 0. Set = △t(Xt),
and denote Xtt ∈ arg maxx∈X * δt(Xt; X), then δt(Xt; Xtt) = > 0. Therefore, from the definition of
δt(Xt; Xtt) we know that, for any i ∈ {1, . . . , m}, we have fti(Xt) - fti(Xtt) ≥ . Thus all entries of
Ft(Xt) - Ft(Xt) are positive, and we have (Ft(Xt) - Ft(Xt))+ = Ft(Xt) - Ft(Xt). Consequently,
we have △0t(Xt) = supx∈X * δt0 (Xt; X) ≥ δ0(Xt; Xtt) = δ(Xt; Xtt) = △t(Xt).
14
Under review as a conference paper at ICLR 2022
From the above lemma, the PSG measurement at round t has an equivalent form as
∆t(xt) = sup inf λ> (Ft(xt) - Ft(x)),
X∈X* λ∈∆m
and correspondingly, the multi-objective dynamic regret becomes
TT
RMOD(T) = X∆t(xt) = X sup inf λ>(Ft(xt) - Ft(x)).
t=1	t=1 χ∈χt* λ∈"
Since the time horizon T is finite, we can first swap the summation over t and the supremum over x,
then swap the summation over t and the infimum over λ. Then the multi-objective dynamic regret
further equals to
T
RMOD(T )=	sup	inf	χ(λ 厂 Ft(Xt)- λ > FtH)),
x—」"”.23
which proves the proposition.
C.2 Proof of Lemma 1
Proof. For OMMD whose the composition weights are λt at round t, before analyzing its regret
bound, we first introduce two useful lemmas.
Lemma 3. For OMD with stepsize η and loss λt>Ft(x), we have the following recursion
λt>Ft(Xt)- λt> Ft(Xt) ≤ — (BR (xt; Xt)- BR(Xt; xt+1)) + Jk▽£(Xt)λt∣ 艮	⑹
η	2
for any t ∈ {1, . . . , T}.
Proof. Our proof is similar to the analysis of OMD in the single-objective setting (Srebro et al.,
2011; Cesa-Bianchi et al., 2012). Specifically, fix ft = λt>Ft and gt = λt>Ft(Xt). From the
convexity of ft, we have
ft(Xt) - ft(Xtt ) ≤ gt> (Xt - Xtt) = gt> (Xt+1 - Xtt) + gt> (Xt - Xt+1).
From the first-order optimal condition of Xt+1, for any X0 ∈ X, we have
(NFt(Xt)λt + VR(Xt+ι) - VR(Xt))>(x0 - Xt+ι) ≥ 0.
We set X0 = Xtt in the above inequality, and consequently derive
ft(Xt)- ft(Xt) ≤ —(VR(Xt+ι) - VR(Xt))>(X； - Xt+1) + g>(Xt- Xt+1).
η
Recall the definition of Bregman divergence BR. We can check that (also see (Beck and Teboulle,
2003))
BR(Xtt, Xt) - BR(Xtt, Xt+1) - BR(Xt+1, Xt) = (VR(Xt+1) - VR(Xt))>(Xtt - Xt+1).
Since R is 1-strongly convex, we have BR(Xt+1, Xt) ≥ kXt+1 - Xtk2/2. Hence
ft (Xt) - ft (Xt) ≤ -(BR(Xt, Xt) - BR(Xt,Xt+1) - BllXt+1 - Xt『) + gt (Xt - Xt+1).
η2
Moreover, from the Cauchy-Schwartz inequality we have
g>(Xt - Xt+i) ≤ 2IlgtIl2 + 2ηIIXt - Xt+ik2.
Combining the above two inequalities, we can prove the lemma.
Lemma 4. For an arbitrary comparator sequence u1 , . . . , uT ∈ X, we have
T	T-1	T
Xλt>(Ft(Xt) - Ft(ut)) ≤ Y X kut - ut+ιk + 1 Br(ui,xi) + η- X ∣VFt(Xt)λt∣2
t=1	η t=1	η	2 t=1
15
Under review as a conference paper at ICLR 2022
Proof. Summing the inequality in Lemma 3 over t ∈ {1, . . . , T }, we have
TT
X λt>(Ft(Xt)- Ft(Ut)) ≤ X 1(BR(ut, Xt)- BR(ut, xt+ι)) + In ∣∣VFt(xt)λt k2
1T-1	1	η
≤ η ɪ2(BR(Ut+1,χt+1)- BR(Ut,xt+ι))+ηBR(uι,χι)+~2 ∣∣vFt(χt)λtk*.
Recall the assumption that BR(X, z) - BR(y, z) ≤ γkX - yk, ∀X, y, z ∈ X. We then obtain
T	1T-1	1	η
^X %> (Ft(Xt)- Ft(Ut)) ≤ n ^X YkUt- ut+ιk+ηBR (uι, xι)+2 kvFt(xt)λt1艮
which proves the lemma.
We can now return to prove Lemma 1. Notice that, for an arbitrary comparator sequence
U1, . . . , UT ∈ X, the Pareto tracking regret can be decomposed as
TT
T
TT
T
]Tλt>Ft(xt) - £%>Ft(x；) = £%>Ft(Xt)- £%>Ft(Ut) + £ λt>Ft(Ut) - fλt>F(Xt).
t=1	t=1
We now instantiate the comparator sequence to be a piece-wise stationary sequence, i.e.,
{u1 ,..., UT} = ∖	wiι,..., wIι,...,	wI	τ	,..., wI	τ	, wI	τ	,..., wI	τ
I、 一 {Z J	d ∆ e-1	d ∆ e-1	d ∆ e	d ∆ e
I . V	X______________________/ X_______________/
I ∆ times	a {z	E ;
I	∆ times	(1+ T —「T])△ times
which starts with w1t and only changes for every ∆ steps (∆ is an integer such that ∆ ≤ T). More
specifically, for any i ∈ {1, . . . , dT /∆e}, denote pi = (i - 1)∆ + 1 and qi = i∆, and then
Ii = [pi, qi] is exactly the i-th stationary piece of the comparator sequence.
For any piece i ∈ {1, . . . , dT /∆e}, we set all Ut, t ∈ Ii to be the best fixed decision X?I regarding
the cumulative linearized losses during interval Ii, i.e., Ut ≡ X?I = arg minx∈X Pt∈I λt>Ft(X),
for any t ∈ Ii, i ∈ {1, . . . , dT /∆e}. Then we can apply Lemma 4 to such comparator sequence and
bound the term A as
dT /△e-1	1	T
A ≤ Y X	∣UIi-UIi+ιk + — BR(UII,Xi) + n X kVFt(Xt)λt∣t
η	i=1	i i+1 η	1	2 t=1	t
TT
≤ Y(d δ e- 1)D+~η~+2 X kvFt(Xt)xtkt = γ~ d δ e+2 X kvFt(Xt )λtkt.
Notice that, the second term B measures the difference between the cumulative linearized loss of
the best decisions XtI regarding each interval Ii and that of the comparators {Xtt }. To analyze this
term, we consider such difference Bi restricted to any interval Ii, i ∈ {1, . . . , dT /∆e}:
Bi = X λt>Ft(Ut) - Xλt>Ft(Xtt)
t∈Ii	t∈Ii
= X	λt>Ft(Ut) -	X λpi>Fpi(Xtpi)	+ X	λpi>Fpi(Xtpi)	- X	λt>Ft(Xtt).
t∈Ii	t∈Ii	t∈Ii	t∈Ii
Recall our definition of Ut and Xtt, then we have Pt∈I λt>Ft(Ut) ≤ Pt∈I λt>Ft(Xpti ) and
λpi > Fpi (Xtpi) ≤ λpi>Fpi(Xtt). Hence we further have
Bi≤ Xλt>Ft(Xpti)-Xλpi>Fpi(Xpti)+Xλpi>Fpi(Xtt)-Xλt>Ft(Xtt).
t∈Ii	t∈Ii	t∈Ii	t∈Ii
16
Under review as a conference paper at ICLR 2022
Moreover, for any t ∈Ii,χ ∈ X, we have
t-1	qi
∣λJFt(x)-λpJ Fpi(X)I = I ^X (λk + i>Fk+1(x)-λk>Fk (X))I ≤ ^X SUP ∣λJFk (x)-λk+1 > Fk + 1 (x) ∣.
k=Pi	k=pi x∈X
Recall that Zi has at most ∆ elements, we further have
qi
Bi ≤ 2∆ V sup ∣λfcτFfc(x) - λfc+1τFfc+1(x)∣.
L x∈x
k=Pi
Hence the term B can be bounded as
∣^T∕∆]	∣^T∕∆] qi
B = X Bi ≤ 2∆ X X sup ∣λkτFk(x) - λιmTFk+ι(x)∣.
i=1	i=1 k=pi x∈X
From the definition of Pi and %，we further have
T-1
B ≤ 2δ ^X sup ∣λtTFt(X)- λt+1TFt+1(X) ∣
T-1
≤ 2δ X sup(∣λtT(Ft(X)- Ft+1(x))∣ + ∣(λt - λt+1)TFt+1(x)∣).
Combining the above two inequalities on terms A and B, we finally derive
dτ∕∆]-1	1	T
RMOD(T) ≤ - X IluIi- uIi+ι k + BR(UII ,x1) + X X INFt(Xt)λt∣∣2
i=1	+ η	2 =
T-1
+ 2∆ X sup(∣λtT(Ft(x) - Ft+1(x))∣ + ∣(λt - %+1)TFt+1(x)D.
M χ∈x
Assume that the diameter of the decision domain X is upper bounded by some D > 0, then ∣∣uji -
UIi十ι k ≤ D and BR(UZI ,xι) ≤ YIluII - xι∣∣ ≤ γD. Hence we have
「3-1
一 X	IIuIi - uIi+1 I + -BR(UII ,x1) ≤ —(I^t∕δ] - 1) + — = —rʌe.
η	+ η	η	η η ∆
Further assume that each loss function Ft is coordinate-wise G-Lipschitz continuous, i.e., ∣fi(y)-
fi(x)∣ ≤ GIly - x∣∣,Vx,y ∈ X, i ∈ {1,...,m}. Without loss of generality, we suppose 0 ∈ X
and fi(0) = 0, Vt ∈ {1,..., T}, i ∈ {1,..., m}. Then for any X ∈ X, ∣f (x)∣ ≤ GD, and
consequently
T-1	T-1	T-1
X Kλt - λt+I)TFt+1(x)∣ ≤ X ∣∣λt - λt+1∣∣1 IlFt+1(x)∣∣∞ ≤ 2F X llʌt - λt+1∣∣1.
t=1	t=1	t=1
It then remains to tackle the term PT-LI supχ∈χ ∣λT(Ft(X) - Ft+1(x))∣. To this end, we introduce
the following lemma.
Lemma 5. FOr OMMD with composition weights λt ∈ ∆m at each round t, it holds that
T-1	T-1
X sup ∣λtT(Ft(X)- Ft+1(X))I ≤ 2F(T - 1) X llʌt - λt+1ll1 + VT.
t=1 x∈X	t=1
17
Under review as a conference paper at ICLR 2022
Proof. Denote λ
T-1
X
t=1
T-1
PkTI λk. We then decompose PT-LISUPx∈χ ∣λt>(Ft(x) - Ft+1(x))∣ as
sup ∣λt>(Ft(x) - Ft+1(x))∣
x∈X
^X sup ∣(λt - λ)丁(Ft(X)- Ft+1(x)) + λ (Ft(X)- Ft+1(x))∣
M x∈X
T-1
T-1
≤ ^X sup ∣(λt - λ)丁(Ft(X)- Ft+1(X))I + ^X sup ∣λ (Ft(X)- Ft+1(X))∣.
=χ∈x	/ χ∈x
X--------------------------/ X---------------------/
^^{^^≡
termA
^^{^^™
term B
Since we have assumed that ∣毋(x) ∣ ≤ F for any x ∈ X,t ∈ {1,..., T}, i ∈ {1,..., m}, it holds
that ∣ft(χ) - f+1(χ) ∣ ≤ 2F. Consequently, we can bound the term A as
T-1	m
termA = X SUP ∣ X(K - (λ)i)(fi(χ) -ft+1(X))I
t=1 x∈X i=1
T-1	m
≤ X sup X ∣λt - (λ)i∣∙∣ft(χ) - ft+1(x)∣
t=1 x∈X i=1
T-1 m
≤ XX ∣λt - (λ)i∣∙ sup Ifti(X)- ft+1(x)∣
t=1 i=1	x∈X
T-1 m	T-1
≤ XX ∣λt - (λ)i∣∙ 2F = X 2F kλt - λk1,
t=1 i=1	t=1
where λi and (λ)i represents the i-th entry of λt and λ, respectively. Also notice that for any
t ∈ {1,…，T},
kλt - λk1 = kPTT^ - λtk1 = k PT=I(Tit) k1
=k PT=I Ps=T(λs-λs+1) k1 ≤ X X k *⅜λ+1 k1
k=1s=k
T T-1 ʌ ʌ	T-1 ʌ ʌ	T-1
≤ XX k 勺+1 k1 = T X k 勺+1 k1 = X kλs - λs+1k1.
k=1 s=1	s = 1	s = 1
Therefore, we have
T-1	T-1	T-1
termA ≤ X 2F X |人-λs+1k1 = 2F (T - 1) X	-λt+1∣∣1∙
t=1	s=1	t=1
As for term B, we have
T-1	T-1 m
termB = X SUP K(Ft(X)- Ft+1(X))I ≤ X SUP X囚'∙ IA(X)- ft+1(X)I
M x∈X	M x∈X =
m	T — 1	m
≤ X(W ∙XSUP ∣fi(X)- ft+1 (X)I ≤ X(λ)i ∙ VT = VT,
where the last inequality is derived from the assumption of temporal variability, and the last equation
comes from the fact that Pm=1(λ)i = T P襄1 P" λs = T P" P黑1 λ = 1. Combining the
above bounds for term A andterm B, we finally prove the lemma.	■
18
Under review as a conference paper at ICLR 2022
Plugging the above terms into the above bound for RMOD (T ), and replace the quantity ∆ ∈
{1, . . . , T} by δ, we have
T-1	T	D T
RMOD(T) ≤2δVT +4δFT N kλt - λt+1k1 + 2	kyFt(xt)λtk! + T d犷]，
for any δ ∈ {1,...,T}. Note that PT=Isupχ∈χ(∣λt>(Ft(x) - Ft+ι(x))∣ + ∣(λt -
λt+ι)>Ft+ι(χ)∣) = PT]1 supχ∈χ(∣λt>(Ft(x) - Ft+ι(χ))∣ + ∣(λt - λt+1)>Ft+1(x)∣), because
the regret does not depend on FT+1, λT+1. We thus prove the lemma.
C.3 Proof of Theorem 1
Proof. This theorem can be directly derived from Lemma 1.
Specifically, when GGVT ≤ η ≤ 若,We can set δ = nGT, which satisfies 1 ≤ δ ≤ T. Plugging it
into Lemma 1 and rearranging the inequality, we can directly derive the theorem.
C.4 Proof of Theorem 2
Proof. Since this theorem is a special case of its following Theorem 3 when Ω(1) ≤ VT ≤ o(T), it
can be proved as we derive Theorem 3 in the following subsection.
In fact, as we assume Ω(1) ≤ VT ≤ o(T), in the following derivation of Theorem 3, we are
always in the case of (i). In addition, when Ω(1) ≤ VT ≤ o(T) in Theorem 3 we exactly have
η = G(YDTT-)1/3. Hence this theorem can be directly derived from Theorem 3.	■
C.5 Proof of Theorem 3
Proof. Denote ηo = G(YDTT-)1/3. We consider the following three cases:
(i)	When GGVT ≤ ηo ≤ 4GVf, we can directly apply the above lemma.
(ii)	When η0 < GVT, or equivalently VT > (γD)1/2GT, we have η = GGvT. Set δ = 1 in Lemma 1,
then it can be verified that
2V T	γDG2T2
RMOD(T) ≤ 2VT + G2T y?(αkλt - λt-1k1 + ∣∣VFt(xt)λtk*) +	4V
t=1	T
T
≤ 4Vt + GT X ∣∣VFt(xt)λt-ιk;
t=1
T
≤ 4VT + G2T X G2 = 6VT.
t=1
(iii)	When ηo > 4GVf, or equivalently VT < (YD)1/2G, we have η = 4GVf. Set δ = T in Lemma 1,
then it can be verified that
RPT(T) ≤ 2TVT + iT^X X(αkλt - λt-1k1 + ∣∣vFt(xt)λtk2) + γτr
G	4VT
t=1	T
T
≤ G(2γD)1∕2T1/2 + 2GT X ∣VFt(xt)λt-1∣2
t=1
T
≤ G(2γD炉T1/2 + 2 T X G2 ≤ 3G (27D)1/2T1/2.
t=1	2
Combining (i)-(iii), we prove the theorem.	■
19
Under review as a conference paper at ICLR 2022
C.6 Proof of Corollary 1
Proof. Since this corollary is a special case of its following Corollary 2 when Ω(1) ≤ VT ≤ o(T),
it can be directly derived from Corollary 2.
Specifically, since VT ≥ Ω(1), We have V1/3T2/3 ≥ O(T1/2). Moreover, since VT ≤ o(T), We
have V 1/3T2/3 ≥ o(VT). Therefore, the dominating term in the bound of Corollary 2 is V1/3T2/3,
which proves the corollary.
C.7 Proof of Corollary 2
Proof. We start from the general bound derived in Theorem 3. Specifically, in the first regret term,
since λt is selected to minimize a∣∣λt 一 λt-1k1 + ∣∣VFt(χt)λt∣∣g at each step t, we further have
min {αkλ - λ1kι + ∣∣VFt(xt)λ∣图 ≤ kVFt(xt)λ1k2 ≤ (kλt-1k1∣∣VFt(xt)k∞)2 ≤ G2.
λ∈∆m
Plugging it into the bound in Theorem 3 directly proves the corollary.
D More details in Multi-Objective Static Regret
As we have discussed before, the multi-objective static regret cannot be formulated using most
existing discrepancy metrics, since these metrics always give non-negative measurements, and the
static regret based on any non-negative metric will fail to reduce to the standard static regret RS (T)
in the single-objective setting. In this section, we give a possible form of multi-objective static regret
and provide an analysis for it.
The static regret is enlightened by the equivalent form of dynamic regret derived in Proposition 1.
Recall that in Proposition 1, the comparator XJ= at each round t is selected from the Pareto set Xj
of the instantaneous loss Ft; in addition, the weights λJ at each round t is generated separately. To
derive a static version, we can use a fixed comparator x= from the Pareto set X = of the cumulative
loss Pt Ft and fixed weights λ= at all rounds. Now the static regret takes
RMOS (T) :
sup inf
X*∈X* λ*∈∆m
TT
λ=>(XFt(xt)-XFt(x=)),
where X = = PX (PtT=1 Ft). Note that when m = 1, the probabilistic simplex ∆m reduces to
a single point {1}, and the Pareto optimal set X = coincides with the optimal set of the cumula-
tive scalar loss PtT=1 Ft, i.e., X = = arg minx∈X Ft(x). Hence the multi-objective static regret
takesRMOS(T)=supx∈X*(PtT=1Ft(xt)-PtT=1Ft(x=))=PtT=1Ft(xt)-minx∈XPtT=1Ft(x),
which exactly reduces to the standard static regret RS (T) in the single-objective setting.
Surprisingly, with proper choices of η and α, our proposed OMMD-II algorithm can still achieve
a sublinear bound w.r.t. T for the static regret, as shown in the following theorem. Note that the
derived regret bound for OMMD-II is tight w.r.t. T, since it matches the lower bound O(√T) of the
standard static regret in the single-objective setting (Hazan, 2019).
Theorem 4. OMMD-II with composition weights λt attains the following static regret
T-1	T
RMOS(T) ≤ 2FT X ∣λt - λt+ι∣ι + 1 Br(x*,xi) + η- X ∣VFt(xt)λt∣2.	(7)
t=1	η	2 t=1	=
By setting η = -√Da and α = 4^T, the regret bound reduces to O(√T), which is Sublinear w.r.t. T.
20
Under review as a conference paper at ICLR 2022
Proof. We start from the definition of RMOS (T ). Specifically, for any λ ∈ ∆m and λ1 . . . , λT ∈
∆m , it holds that
TT
RMOS(T) = SUp inf Xλ*>(Ft(xt)- Ft(x*)) ≤ SUp Xλ>(Ft(xt) - £(x*))
x*∈X ∙2m =	x*∈X * =
T
=^X((1>Ft(Xt) - λt>Ft(Xt)) + λt>(Ft(Xt)- Ft(X*)) + (λt>Ft(χ*) - λ>Ft(X)))
t=1
T
T
T
≤ Fkλ-λtk1+	λt> (Ft(Xt)- Ft(X*)) + Σ^Fkλ - λtk1
TT
=2FX kλ - λt∣∣ι + Xλt>(Ft(Xt) - FtQ).
To tackle the second term in the above inequality, we set uι = u = ... = UT = x* in Lemma 4,
which results in
T	1T
X λt>(Ft(xt) - Ft (x*)) ≤ - Br (x* ,x1) + 2 X ∣∣VFt(xt)λt∣∣2∙
To analyze the static regret for OMMD-II, we relate the first term PtT=1 kλ - λt k1 with the L1
regularization in the regularized min-norm solver at each round. Specifically, we denote the average
weights as 入=PL λt∕T and set λ = λ in the first term. Then for any t ∈ {1,..., T}, We have
kʌ - λt∣k=k PT=I(Ti-λt) kι=k PT=I Pk=Tλk-") I.
T t-1	T T -1
≤ XX k λk-Tλk+1 kι ≤ XX k λk-Tλk+1 kι
i=1 k=i	i=1 k=1
T -1	T -1
=T X kλk-Tλk+1 kι = X kλt - λt+1k1.
k=1	t=1
Consequently, the static regret can be bounded as
T T -1	1	T
RMOS (T) ≤ 2F XX
kλt - λt+1k 1 + br(x*, X1) + 2 X IlVFt(Xt)λtk*
t=1 t=1	η	2 t=1
T -1	T
=2ft ^X kλt- λt+ιkι + ηBR(χ*,xι) + 2 ^X IlVFt(Xt)λtk*,
which proves the full static regret bound. Now we prove the reduced bound. From the full bound,
we equivalently have
T
RMOS(T) ≤ -br(x*, X1) + X X(k VFt(Xt)λtk2 +----------kλt - λt-11∣1).
η	2 t=1	* η
We now set λ = 4FnT in OMMD-II, and specify λt to be the composition weights generated by the
algorithm at round t. Then we know that λt ∈ argminλ∈∆t kVFt(xt)λ∣∣2 + 平∣∣λ - λt-ι∣∣ι. In
particular, we have IlVFt(Xt)λt∣∣2 + 4FT∣∣λt - λt-1∣∣1 ≤ IlVFt(Xt)λt-ιk*. Therefore, it holds
that
1	ηT
Rmos(T) ≤ 一Br(x*,xi) + W EkVFt(Xt)λ1k2.
η	2 t=1
Utilize Assumption 1 and Assumption 3, and set η = ∣√;, then we have
1	T
Rmos(T) ≤ 一Br(x*,xi) + η V ∣VFt(xt)λt-ι∣* ≤ G√2DT,
η	2 t=1	*
which proves the reduced bound.
21
Under review as a conference paper at ICLR 2022
Figure 4: Illustration of an example in which the projection-based metric ∆tproj fails to measure the
Pareto optimality of the generated decision xt . The light blue area and the orange area represent the
decision set X and its image set Ft(X), respectively. The blue line segments constitute the Pareto
optimal set Xj. The red line segments constitute the Pareto front Pt. As shown in the plot, in this
example, the projection-based metric ∆tproj compares Ft (xt) with the farthest point Ft (xproj) in
the Pareto front; the comparator xproj does not even dominate xt. In contrast, PSG compares Ft(xt)
with the nearest point Ft(xPSG) in the Pareto front; the comparator xPSG indeed dominates xt.
Note that the newly introduced static regret is no longer based on PSG. It can be understood as in-
duced from a new discrepancy metric δ(xt; xt, Ft, λt) = (λt)>(Ft(xt) -Ft(xt)), where λt ∈ ∆m.
We thus have RMOS(T) = supχ*∈χ * inf 入* ∈∆m PT=I δ(xt; χt,Ft, λt). Such a metric compares the
generated decision xt at each round with the fixed comparator xt ∈ Xt . Hence, in particular, it is
able to produce a negative value when xt dominates xt regarding the instantaneous loss Ft, making
it a general extension of PSG. Although this property is desired in the definition of static regret, it is
rarely utilized in the discrepancy metrics in multi-objective optimization. Thus it looks a bit strange
as a Pareto suboptimality metric and hence its physical meaning needs to be justified further. Be-
sides, there are undoubtedly many other possible ways to define such metrics. Therefore, regarding
the multi-objective static regret, much work remains to be done in the future. We hope our initial
attempt paves the way for future research.
E	Discussion on An Alternative Metric Based on Projection
In this section, we discuss an alternative discrepancy metric based on projection onto the Pareto
optimal set. Then we explain why it is unsuitable to measure the Pareto suboptimality in some
cases.
The metric is formulated as follows, which we term ∆tproj. At each round t, we project the generated
decision xt onto the Pareto optimal set Xtt, namely xproj ∈ arg minx0 ∈X* kxt -x0k2, and then mea-
sure the Euclidean distance between Ft (xt) and Ft (xproj), i.e., ∆tproj (xt) = kFt(xt)-Ft(xproj)k2.
Different from the PSG metric ∆t that directly measures the distance between the actual loss Ft(xt)
and the entire Pareto front Ptt in the loss space, the projection-based metric ∆tproj only compares
the actual loss Ft(xt) with the loss Ft(xproj) evaluated at a single comparator xproj.
In intuition, ∆tproj ignores the landscape of the Pareto front, since the choice of xproj only depends
on the Pareto set, whose structure in the decision domain does not necessarily align with the land-
scape of the Pareto front in the loss space. Therefore, the comparator xproj may not be a good point
to measure the Pareto suboptimality of xt in the loss space. In comparison, PSG directly compares
22
Under review as a conference paper at ICLR 2022
Ft (xt) with the entire Pareto front, thus is always able to measure the Pareto suboptimality of the
generated decision xt .
In the following, we provide an example in the two-objective convex setting to corroborate this
point. In our example, the loss Ft(xproj) induced by the projected decision xproj is actually the
most remote point from Ft(Xt) in the Pareto front Pt. Moreover, for the second objective, We have
ft2(xproj) > ft2(xt). Therefore, xproj is not a proper point to measure the Pareto suboptimality
of xt regarding Ptt . Notably, measuring the Euclidean distance betWeen Ft(xproj) and Ft (xt) is
meaningless, since xproj performs even Worse than xt in the second objective.
Concretely, We consider the decision set X = {(x1, x2) | -1 ≤ x1 ≤ 0, -1 ≤ x2 ≤ 0}, Which is a
rectangle in R2 . We assume there are tWo objectives, and at round t, the loss function Ft : X → R2
takes Ft(χ1,χ2) = (2x -X , 2x -X ). Since Ft is a linear transformation, it is convex. The decision
set X and the image set Ft (X ) are shoWn in Figure. It is easy to verify that, the Pareto optimal
set Xtt = {(-1, x2) | -1 ≤ x2 ≤ 0} ∪ {(x1, -1) | -1 ≤ x1 ≤ 0}, Which consists of tWo
line segments. We assume the generated decision xt = (0, 0) at this round t, Which incurs the
loss Ft(xt) = (0, 0). Then the projection of xt on the Pareto set is xproj = (-1, 0) or (0, -1).
Due to symmetry, We only consider xproj = (-1, 0), then the loss evaluated at the comparator is
Ft(Xproj) = (- 2,1). From the plot in Figure, it is obvious that Ft(Xproj) is the most remote point
from Ft(xt) in the Pareto front. Moreover, regarding the second objective, ft2(xproj) > ft2 (xt),
Which means Xproj does not even dominate Xt .
As a comparison, We also investigate PSG in the above example. Recall that, in Proposition 1, PSG
△t(Xt) equals sup,∈χt* inf λ∈∆2 λ> (Ft (xt) - Ft (x)),or equivalently sup,∈χt* mini∈{i,2}(fi(xt)-
ft(χ)). It is easy to verify that ∆t (Xt) = ɪ and the comparator Xt attaining the supremum is
(-1, -1), which we denote as XPSG. Then the compared loss Ft(XPSG) = (-3, -1), which
is exactly the closest point to Ft (Xt) in the Pareto front. Moreover, XPSG dominates Xt. Hence,
compared to the projection-based metric △tproj that compares Ft(Xt) with the most remote point
in the Pareto front, PSG is obviously more reasonable to measure the Pareto suboptimality in the
multi-objective setting.
23