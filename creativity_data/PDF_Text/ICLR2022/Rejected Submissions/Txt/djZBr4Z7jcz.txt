Under review as a conference paper at ICLR 2022
On the regularization landscape for the lin-
EAR RECOMMENDATION MODELS
Anonymous authors
Paper under double-blind review
Ab stract
Recently, a wide range of recommendation algorithms inspired by deep learning
techniques have emerged as the performance leaders on several standard recommen-
dation benchmarks. While these algorithms were built on different deep learning
techniques (e.g., dropouts, autoencoder), they have similar performance and even
similar cost functions. This paper studies whether the models’ comparable per-
formance are sheer coincidence, or they can be unified under a single framework.
We find that all competitive linear models effectively add only a nuclear-norm
regularizer, or a Frobenius-norm regularizer. The former ones possess a (surprising)
rigid structure that limits the models’ predictive power but their solutions are low
rank and have closed-form. The latter ones are more expressive so they have better
performance. However, the performance leaders EASE and EDLAE have only
full-rank closed-form solutions. Their low-rank counterparts can be solved only
by hard-to-tune numeric procedures such as Alternating Directions Method of
Multipliers (ADMM). Along this line of findings, we further propose two low-
rank, closed-form solutions, derived from carefully generalizing Frobenius-norm
regularization techniques.
1	Introduction
Research progress on algorithms for recommendation has escalated in recent years, partially fueled
by the adoption of deep learning (DL) techniques. However, recent studies have found that many
new deep learning recommendation models have shown sub-par performance against simpler linear
recommendation models (Dacrema et al., 2019; Rendle et al., 2019). Although some studies are
available to analyze linear vs non-linear models (Dacrema et al., 2019), it remains puzzling why
these seemingly different techniques all result in models with similar performance or even similar
cost functions. In the latest study, Jin et al. (2021) examine the relationship between the widely
used matrix factorization (MF), such as ALS (Hu et al., 2008), and the linear autoencoder (LAE)
which encompasses the recent performance leaders, such as EASE (Steck, 2019) and EDLAE (Steck,
2020). They considered two basic regularization forms (see Eqs. (1) and (6)) and found that the
optimal (closed-form) solutions of both models recover the directions of principal components, while
shrinking the corresponding singular values differently. They suggest that the difference may enable
LAE to utilize a larger number of latent dimensions to improve recommendation accuracy. This
finding highlights the similarity and difference between LAE and MF.
In this paper, we go much beyond the two basic models studied in (Jin et al., 2021), to analyze a large
number of recent performance leaders of (linear) recommendation algorithms, and aim to provide an
in-depth understanding of various regularized objective functions and towards unifying them through
their closed-form solutions. In return, the closed-forms serve as the barebones engine to help reveal
what drives the performance improvement for the recent recommendation algorithms.
We examine three open and closely related questions: (i) Characterization of models: Recent
recommendation models are built upon a diverse set of techniques, including dropouts (Srivastava
et al., 2014), variational methods (Kingma & Welling, 2014), and matrix denoising (Tipping &
Bishop, 1999b). We aim to identify a unified regularization framework to (re)-interpret different
models and understand how they are related. We are specifically interested in how models based
on variational autoencoder (VAE, a natural generalization of recent approaches) relate to other
models (Liang et al., 2018). (ii) Weighted generalization of regularizers: A key idea in a recent
performance leader EDLAE (Steck, 2020) is to utilize of weighted/non-uniform regularizers on the
1
Under review as a conference paper at ICLR 2022
parameters’ Frobenius norm. Applying dropouts(Cavazza et al., 2018) is shown to be equivalent to
re-weighting the exponents in the regularizers, such as designing the weighted sum of regularizers
based on other norms or tuning the exponent weights. We are also interested in determining what
circumstance closed-form solutions still exist. (iii) Low-rank closed-form solutions for EDLAE.
While most linear recommendation models are shown to have closed-form solutions, the low-rank
version of recent performance leader EDLAE (Steck, 2020) remains an exception. Fitting this
model requires using ADMM (Steck, 2020). Although its performance may not be better than its
full-rank counterpart, low-rank solutions are easier to interpret, use less storage, and can be more
scalable with respect to the number of items. In addition, a closed-form solution disentangles key
performance drivers from nuances (e.g., need to tune learning rate or deal with local optimal), and
can help reveal the key driven factors comparing with other closed-form solutions. More importantly,
such solutions are significantly easier to implement and be tested using a generic matrix computation
platform (without specialized recommendation library). Can we approximate low-rank EDLAE with
closed-form solutions?
Our investigation led to the following discovery and resolves the above questions:
Regularizer dichotomy (Section 2): We found that all of the leading (linear) recommendation
models can be categorized into those that implement possibly weighted nuclear-norm regularizers, or
those that implement Frobenius-norm regularizers. Specifically, we characterize the Variational Linear
Autoencoders (VLAE) as a form of the weighted nuclear-norm regularization problem, in which the
weights possess a specific combinatorial structure. We observe that it is not matrix factorization or
LAE that determines the shrinkage structure (as Jin et al. (2021) suggested), but instead it is the form
of regularization. Thus, our paper provides a more complete and accurate characterization of a linear
recommendation model’s performance under different regularizations.
Rigidity of nuclear-norm regularizers (Section 3): With the dichotomy result, we next aim to
understand whether the weighted sum idea for Frobenius-norm regularizers (Steck, 2020; Jin et al.,
2021) can be generalized to nuclear-norm regularizers, and whether tuning exponent weights can
be beneficial. First, while VLAE is equivalent to nuclear-norm regularization and is easier to
optimize (weighted nuclear norms are non-smooth but VLAE’s objectives are smooth), its closed-
form solution possesses a (surprisingly) rigid structure, i.e., the weighted regularization will lead to
an auto-sorting singular value reduction and the larger single values tend to receive smaller reduction
(non-ascending reduction). Second, it has been shown that the solution structure for a model with
a squared nuclear-norm regularizer ∣∣ W∣∣2 (i.e., dropout's equivalence) is strikingly similar to that
for using ∣∣ W∣*-regularizer (GU et al., 2014). We generalize the result to show that the solution
structures for ∣∣ W∣∣p are highly similar for all P ≥ 1 (Regularization invariant/rigidity with respect to
p). But when p = 1, 2, the solution and hyperparameters possess favorable properties which make
hyperparameter searches easier. This also partially explains why only p = 1, 2 have been extensively
considered. These rigidity properties severely limit the search space and explains why models that
use only nuclear-norm regularizers share the same performance ceiling even when hyperparameters
are extensively searched.
Closed-form solution for low-rank EDLAE (Section 4): The (weighted) Frobenius-norm reg-
ularizers ∣ΛW ∣2F (Λ is the hyperparameter diagonal matrix) are implemented in EASE (Steck,
2019) and EDLAE (Steck, 2020). These models produce closed-form full-rank estimators; and if
the zero-diagonal constraint on W is enforced, their singular vectors will no longer coincide with
those of the data, and will deliver (slightly) better performance. However, no closed-form solution
for the low-rank estimator is known and existing approaches rely on ADMM or Alternating Least
Square (Steck, 2020). In this paper, we propose two low-rank, closed-form estimators that deliver
comparable results to the full-rank models (EASE and full-rank EDLAE) as well as the ADMM-based
solutions (Steck, 2020), and thus resolve the aforementioned third open question.
2	Background and Regularizer Dichotomy
This section first explains the background, and then give an overview of the dichotomy results. Some
key theorems are deferred to Section 3 and Appendices.
Background. Recommendation algorithms can be categorized into explicit ones that aim to predict
unseen ratings between a user and an item and implicit ones that aim to predict actions, such as user
click or add-cart (Steck, 2019; Dacrema et al., 2019; Zhang et al., 2019). We focus on the implicit
2
Under review as a conference paper at ICLR 2022
Table 1: Investigating the Closed/analytic solutions of linear models. dMat(∙) denotes a diagonal
matrix, diag(X) is the vector on the diagonal of X. Λ is the (hyperparameter) diagonal matrix as a
coefficient of the regularization term. W * (or P *, Q*, etc) is the optimal solution for corresponding
case, except for cases 9-12, where Wc is the low-rank closed-form solution.
Model		Regularization	Solution
Nuclear norm	1. Regularized PCA (Zheng et al., 2018)	黑|因-PQIlF + λ ∙(∣∣P∣∣F + IIQIlF)	X S= UΣVT Ω = √(% - λ)+ P * = Uk	Q* = ΩVT
	2. MF dropout (Cavazza et al., 2018)	min IIX - PQIlF + d - -X I∣Pk∣l2 ∙∣IQTII2 P,Q,d	p	k=1 mγnIIX - YIIF + 亍IIYII2	X S=VD UΣVT Y * = P * ∙ Q* =U ∙ Sμ(Σ)∙ VT
	3. WLAE (Bao et al., 2020)	min kX — XW1W2kF + kW1Λ2kF + ∣∣Λ1 W2∣∣F, W1,W2	W* = V(I - λs-2)2 pT W* = P(I - ΛS-2)2 VT
	4. VLAE (this paper)	min IIX - PQIIF + IIΛ1∕2QIIF + IIP Λ1∕2IIF mA,iBn IIX - XABII2F + IIΛBII2F + IIX AII2F min IIX-WII2F+2IIWIIw,* rank(W)≤k	X S=VD UΣVT P * = Uk ∙ diag( Jσι - λ(k),..., Jσk -入⑴)∙ Ω Q* = QT ∙ diag( Jσ1 - λ(k),…，Jσk - λ(1)) ∙ VT A* = XtP *Λ1 B* = Λ-1 Q*
Frobenius norm	5. EASE (full rank) (Steck, 2019)	mWn IIX - XWIIF + λ ∙IIW IIF s.t. diag(W) = 0	C= (XTX + λI)-1 W* = I - C ∙ dM at(diag(1 C))
	6. DLAE(full rank) (Steck, 2020)	mWn IIX - XWIIF + IIΛ1/2 ∙ WIIF Λ = ^^dMat(diag(XT X))	W* = (XTX + Λ)-1XTX
	7. EDLAE(full rank) (Steck, 2020)	mWn IIX - XWIIF + IIΛ1/2 ∙ WIIF Λ = ^^dMat(diag(XT X)) 1 - p s.t. diag(W) = 0	C= (XTX + Λ)-1 W* = I - C ∙ dM at(diag(1 C))
	8. LRR (Jin et al., 2021)	min IIX -XWII2F+ IIΓWII2F rank(W )≤k	Y* = XW* S=D UΣV Wc = (XTX + ΓTΓ)-1XTX(VkVkT)
	9. EDLAE-ADMM (Steck, 2020)	min IIX - X(AB - dMat(diag(AB)))II2F + IIΛ1/2 ∙ (AB - dMat(diag(AB))IIF	ADMM update A, B
	10. LR-DLAE (this paper)	min	IIX - XWIIF + IIΛ1/2 ∙ WiiF rank(W )≤k Λ = ^^dMat(diag(XT X)) 1 - p	W* = (XTX + Λ)-1XTX Y* = XW * S=D UΣVT Wc = W *(VkVkT)
	11. LR-EDLAE-1 (this paper)	min- i∣x -XWiiF + ∣∣Λ1/2 ∙ W∣∣F rank(W 0)≤k W = W0 - dM at(diag(W 0)) Λ = ^^dMat(diag(XT X)) 1 - p	C= (XTX + Λ)-1 W* = I - C ∙ dM at(diag(1 C)) Y * = XW * S=D UΣV t Wc = W *(VkVkT)
	12. LR-EDLAE-2 (this paper)	min- i∣x -XW∣∣F + ∣∣Λ1/2 ∙ W∣∣F rank(W 0)≤k W = W0 - dM at(diag(W 0)) Λ = ^^dMat(diag(XT X)) 1 - p	C= (XTX + Λ)-1 W* = I - C ∙ dM at(diag(1 C)) W* S=VD UΣVT Wc = UkΣkVkT
problem because it is more economically relevant. Here, let n be the number of items and m be
the number of users. We are given a binary matrix X ∈ {0, 1}m×n that represents the interaction
between users and items so far, i.e., Xi,j = 1 iff user i has purchased or made a rating on item j.
Our goal is to produce a real-valued matrix X, which We evaluate against future interactions using
information retrieval (Top-k) metrics such as Recall or nDCG.
The problem is closely related to matrix completion (MC) because Xi,j = 0 can be viewed as
“missing a data point”. But MC’s evaluation metric is mean-squared error and is different from
ours (CandeS & Tao, 2010). The connection between two problems results in models with similar
objectives (Zheng et al., 2018). A technique developed for one problem often finds its counterpart
for the other. Nevertheless, because of the difference in evaluation, efficacy of an algorithm for
one problem does not imply its performance guarantee for the other. Thus, the non-overlapping
component between two problems remains substantial. We also remark that our structural results on
weighted nuclear-norm is new and applicable to MC.
3
Under review as a conference paper at ICLR 2022
2.1	Dichotomy of the models
This section explains that (i) All linear recommendation models can be categorizeed into those that
use nuclear-norm regularizers and those that use Frobenius norm (see also Table 1), and (ii) The
form of regularization, instead of whether the problem shall be cast as matrix factorization or LAE,
determines the shrinkage structure (Proposition 1).
Nuclear-norm regularizers. Let X ∈ Rm×n be a matrix of rank at most k with k leading singular
values being σι(X) ≥ σ2(X) ≥ …≥ σk(X). Let ω = (ωι,...,ωk) ∈ (R+)k. The weighted
nuclear norm of X with respect to ω is defined as ∣∣X∣∣ω,* = Pk=I ω% ∙ σi(X). This is a natural
generalization of the weighted nuclear norm for low-rank matrices (Gu et al., 2014). Despite its name,
the weighted nuclear norm is neither convex nor differentiable unless ω^s are sorted in descending
order (Chen et al., 2013; Iglesias et al., 2020).
Nuclear-norm regularizers perform '1-shrinkage over the estimator,s singular values, which resembles
performing '1-shrinkage for coefficients in a linear model in LASSO (Tibshirani, 1996). Therefore,
nuclear-norm regularizers also promote sparsity over the solution’s singular values (i.e., the solution
is usually low rank). Algorithms below effectively add only a nuclear-norm regularizer to MF.
A1. Regularized PCA (Udell et al., 2016; Zheng et al., 2018) aims to solve
min∣X-PQ∣2F+λ∣P∣2F+λ∣Q∣2F	(1)
P,Q
where λ is the hyperparameter. It has been known that Eq. (1) is equivalent to minχ ∣∣X 一 X∣∣F +
2λ∣Xk*. To solve Eq. (1), one can use factored gradient descent (Bhojanapalli et al., 2016) or directly
derive its closed-form solution (Kunin et al., 2019), which involves computation of SVD of X (see
case 1 in Table 1).
A2. Matrix Factorization with dropouts. This approach uses PQ (P ∈ Rm×k, Q ∈ Rk×n) to
approximate X . A standard dropout technique is utilized when we train P and Q. Cavazza et al.
(2018) show that optimization with dropout is equivalent to solving minχ ∣∣X 一 X∣∣F + λ∣X∣∣2, and
the closed-form solution is obtained by shrinking all singular values of X by the same magnitude of
μ, which depends on the data X and the choice of hyperparameter λ. See details in Appendix E.3.
The next two approaches leverage techniques from (variational) autoencoder. We show that they also
effectively add variants of nuclear-norm regularizers although this may not be clear at the first glance.
A3. Weighted Linear Autoencoder (WLAE). Consider the following (non-uniform) weighted '2-
regularization (Bao et al., 2020):
IIx - XWIW21F + ∣∣wiλ2IIF + IlA2 W2∣∣F,	(2)
where Λ is a hyperparameter diagonal matrix and W1, W2 denote the encoder and the decoder
network respectively. Bao et al. (2020) have shown a closed-form solution for Eq. (2) with a specific
of diagonal matrix Λ = dMat(λι, λ2,…，λk) when the weight is non-descending: λι ≤ λ2 ≤
…≤ λk. It remains unclear if a closed-form solution exists for an arbitrary weight order.
A4. Variational Linear Autoencoders (VLAE). While not explicitly studied before, it is also natural to
consider linear simplification of Variational Autoencoders, such as Multi-VAE (Liang et al., 2018),
which has shown to exhibit strong performance for recommendation. To optimize VLAE, we need to
find the maximum likelihood estimation (MLE) for the probabilistic model
P(X | Z)= N (WZ + μ, σ21) andp(z | x) = N(V(X — μ),D)	(3)
where V , W denote the encoder and the decoder networks, σ is a parameter, and D is a diagonal
covariance matrix (for the data X ∈ Rm×n). We have the following observation.
Lemma 1. Consider optimizing the ELBO (Evidence Lower Bound) (Kingma & Welling, 2014) for
the above LVAE model (Eq. (3)). When μ = 0, this optimization problem is equivalent to minimizing
L = ∣X — XVT W TkF + m∣√DW T kF + σ2∣∣XV TkF + g(D,σ)	(4)
where g(D, σ) = —σ2m( log |D| — tr(D) + k — nlog(2πσ2)).
4
Under review as a conference paper at ICLR 2022
Here, We set μ = 0 for simplicity and following standard practices in recommendations. The
proof can be found in Appendix D.1. We may further “clear up” Eq. (4) and obtain the following
optimization problem (see Appendix D.3):
min kX-XABk2F + kXAk2F + kΛBk2F,	(5)
A∈Rn×k,B∈Rk×n
in which decision variables are A and B, and the hyperparameter is a diagonal matrix Λ ∈ Rk×k.
In Section 3, we show that solving Eq. (5) (A4) is equivalent to solving kX - W k2F + λkW kω,*
subject to rank(W) ≤ k, where ω consists of Λ's diagonal values, sorted in non-descending order.
Furthermore, we characterize the optimal solution for ∣∣X - W∣∣F + λ∣∣W∣∣p for any P ≥ 1. We
shall show that regardless the choice of p, the optimal solution uniformly shrinks all X’s singular
values by a constant magnitude μ (and to 0 if a singular value is already less than μ). Finally, for all
nuclear-norm-based approaches discussed above, the estimators always keep singular vectors of X
and shrink its singular values. Therefore, the solution space offered by nuclear-norm regularizers is
quite constrained, which limits their predictive power.
Frobenius-norm regularizers. Most algorithms below were originally motivated by the design of
(denoising) autoencoders. It has been shown that they effectively add a Frobenius-norm regularizer.
A5. EASE (Steck, 2019) aims to optimize minw ||X - XW ||F + λ ∙ ||W ||F subject to the constraint
that diag(W) = 0. A closed-form solution exists for this problem (case 5 in Table 1).
A6. DLAE (Steck, 2020) adds a weighted Frobenius-norm regularizer so the objective becomes
minw ||X — XW∣∣F + ∣∣Λ1/2 ∙ W∣∣F, where Λ = ɪ-pdMαt(diag(XTX)), dMat(∙) denotes a
diagonal matrix, diag(X) is the vector on the diagonal of X. The closed-form solution also exists
(case 6 in Table 1).
A7. EDLAE (Steck, 2020) integrates weighted Frobenius norm in DLAE with EASE’s diagonal
constraint so its objective is the same as DLAE but it requires diag(W) = 0. A closed-form solution
exists for this problem (case 7 in Table 1). For the low-rank EDLAE (case 9 in Table 1), an ADMM
algorithm is used.
A8. Tikhonov regularization/Low Rank Regression (LRR) (Jin et al., 2021). Let Vk be the k leading
right singular vectors of X. Jin et al. (2021) find an estimator that solves
W= arg min ∣X -XW∣2F+ ∣ΓW ∣2F,	(6)
rank(W)≤k
where Γ = Λ 1 VfT and Λ = diag(λ∖,…，λk) is a hyperparameter. The closed-form solution is
W * = Vk ∙ dMat( -^σ-0-,..., -^σ2- )Vt	(7)
σ1 + λ1	σk + λk
We first remark that A5 and A6 produce full-rank estimators. A7 can produce either full-rank or
low-rank estimator (via ADMM) and has the best performance (among all approaches we discussed).
The estimator from A8 is low-rank and has a closed-form solution but it has to keep singular vectors
of X so its predictive power is also limited. Nevertheless, A8 is conceptually interesting because it
uses Frobenius-norm regularizers but its solution space cover the solution space offered in A4 (and
thus also A1-A4).
Proposition 1. For any regularized instances in the form of Eq. (5) with regularization parameter
Λ such thatσi (X) ≥ λ(k-i) for all i, there is a corresponding Tikhonov regularized instance with
Γ = Λ 1 VT that provides the same regularization effect.
The proof is in Appendix D.2. A major implication of Proposition 1 is that we can focus on designing
Frobenius-norm regularizers because it also gets the value from using nuclear-norm regularizers.
As mentioned earlier, no closed-form solution exists for the low-rank EDLAE (case 9 in Table 1)
so existing approaches rely on ADMM or Alternating Least Square algorithms (Steck, 2020). In
Section 4, we will introduce two low-rank models based on Frobenius-norm regularizers that have
both competitive performance and closed-form solutions.
5
Under review as a conference paper at ICLR 2022
3 Rigidity of Nuclear-norm regularizers
This section presents two results. (i) Eq. (5) (A4) is equivalent to a model with weighted nuclear-norm,
where the weights are diagonal values of Λ arranged in non-descending order. Because weighted
nuclear norms are non-differentiable in general so Eq. (5) compiles non-differentiable objectives into
differentiable ones, which are easier to optimize. Also, the auto-sorting property restricts Eq. (5)
from expressing an arbitrary weight sequence in ∣∣ W∣∣ω,*, which limits its predictive power. (ii) We
give a closed-form solution for models with regularizer ∣∣ W∣∣p for all P ≥ 1. The solutions share the
same structure so tuning p will not improve a model’s predictive power.
Rigidity of closed-form solutions. We first analyze Eq. (5) (A4). We start with the problem:
min ∣∣X-PQkF +∣∣Λ1 QkF+∣PΛ2 ||F, or equivalently, min ∣∣X-PQkF +∣∣ΛQ∣∣F +∣P∣∣F, (8)
Note that when we let P0 = XA* and Q0 = B*, where A* and B* are an optimal solution of Eq. (5),
the syntax of Eq. (8) matches with that of Eq. (5). This implies that the solution for Eq. (8) is a lower
bound of that for Eq. (5). These two solutions coincide only when the columns in the optimal P* in
Eq. (8) are spanned by the columns of X .
We next find a closed-form solution (P*, Q*) for Eq. (8), and show that indeed the column space of
P* is in the column space of X .
Proposition 2. Let f : Rm×n → R+ be any cost function. Let P ∈ Rm×k and Q ∈ Rk×n. Let Λ ∈
Rk×k be a diagonal matrix such that λi = Λii ≥ 0 (i ∈ [k]). Let also ω = (λπ(1), λπ(2), . . . , λπ(k)),
where π is a permutation on [k] such that λ∏(i) ≤ λ∏(2) ≤ ∙∙∙ ≤ λ∏(k). The following two
optimization problems have the same optimal values
OPT 1 ： minp,Q	f (PQ) + M2 QkF + ∣P Λ 2 ∣F.
OPT2 : minW	f(W)+2∣W∣ω,*
subject to rank(W) ≤ k.
In addition, if (P*, Q*) is an optimal solution for OPT1, then W* = P*Q* is an optimal solution
for OPT2. If W* is an optimal solution for OPT2, then there exists an optimal solution (P*, Q*)
for OPT1 such that W* = P *Q*.
See Appendix D.4 and Appendix D.5 for the full analysis. We note that diagonals of Λ do not have to
be sorted in ascending order as stated in (Bao et al., 2020) because any permutation of the diagonals
will be equivalent to OPT2. In addition, Proposition 2 is applicable to A & A4. f (∙) in A3 & A4 is
the reconstruction error, in which case closed-form solutions exist:
Corollary 1. Let X ∈ Rm×n (m ≥ n) be a full rank matrix. Let Λ be a diagonal matrix with
Λii ≥ 0 for all i. Let P ∈ Rm×k and Q ∈ Rk×n. Consider the optimization problems
min ∣χ - PQkF + ∣λ 2 QkF + ∣p λ 2 kF
P,Q
minkX-XABk2F+ kΛBk2F + kXAk2F.
A,B
(9)
(10)
Let the SVD of X be U ΣV T, and let Σk ∈ Rk×k be a matrix comprising the k largest singular
values of X, and Uk and Vk be the corresponding singular vectors. Let λ(i) ≥ λ(2) ≥ ∙∙∙ ≥ 入⑹ be
the sorted sequence of the diagonal values from Λ. Eq. (9) has a closed-form solution:
P * = Uk diagQ (σι - λg) +),..., J (σk -入⑴)+ )Ω,
Q*=ωTdiag(q (σι - x(k))+),..., q (σ - 1⑴)+)VkT,
(11)
where Ω is a unitary matrix that corresponds to the permutation π such that λ∏(i) ≤ ∙∙∙ ≤ λ∏(k).
In addition, Eq. (10) has a closed-form solution: A* = X*P*Λ 1 and B* = Λ-2Q*, where X* is
the pseudo-inverse of X.
RigidityofkW ∣∣p In (Cavazza et al., 2018), it was observed that when we use a neural net X = PQ
(with learnable parameters being P and Q) to train a model and a standard dropout is used, the
6
Under review as a conference paper at ICLR 2022
objective is equivalent to solving minw ∣∣X 一 W ∣∣F + λ∣∣W ^. Whiletheregularizer ∣∣W ∣∣2 deviates
from the standard one ∣∣W∣*, the optimal solution here is W = USμ(Σ)VT, where UΣVT is SVD
of X, and Sμ (Σ) is a diagonal matrix Such that its (i, i)-th element is (∑i,i 一 μ)+, in which μ depends
on the data X and λ. In other words, the optimal solutions for regularizers ∣∣ W∣∣2 and ∣∣ W∣* are
strikingly similar. Thus, we are interested in how regularizers with different exponents are connected.
Our main observation is that for any regularizer ∣∣ W∣∣p (P ≥ 1), the optimal solution has the same
structure.
Lemma 2. Let X ∈ Rm×n, where m ≥ n. Let the d leading SVDs of X be Ud and Vd respectively.
Let σ1 , . . . , σn be the singular values of X. Consider the optimization problem:
mWn∣∣X - W∣F + λ∣W∣P.	(12)
Let μk = 1 Pi≤k σi(X), η(μk) be the positive root Ofthefunction Z + λkζp-1 — kμk and d be the
largest value such that σd(X) — λ(η(μd))p-1 ≥ 0. Let μ = λ(η(μd))p-1. The optimal solution of
W is Ud ∙ dMαt(σι — μ,σ2 — μ,...,σd — μ) ∙ VT.
Lemma 2 implies that regularizers ∣∣ W ∣∣p with different p's differ in how the shrinkage variable μ is
obtained. Observing that μ is a hyperparameter that requires extensive tuning, all regularizers ∣∣ W∣∣p
provide the same learning power. As noted earlier, μ is a function of the data X unless p = 1; λ
needs to be rescaled when X is scaled by a constant factor unless p = 2. This implies it could be
easier to tune μ when p = 1, 2, and explains why only p = 1, 2 have been extensively considered.
4	Low-Rank Frobenius-norm regularizations
This section presents two closed-form low-rank estimators with competitive performance.
Approximate low-rank DLAE and EDLAE. Recall that (full-rank) DLAE solves:
min ||X — XW||F + ∣∣Λ1W||F s.t. Λ = -^dMat(diag(XTX))	(13)
W	F	F	1—p
and EDLAE with the additional diag(W) = 0 constraint. While both the regularizations use nuclear-
norm and the full-rank DLAE/EDLAE solutions all have closed-form solutions, such solution is
unknown for the low-rank DLAE and EDLAE, which still require ADMM (Steck, 2020). The
closed-form solutions will help both better understand and compare these models, and determine the
hyperparameters such as learning rates, which is usually difficult for the ADMM type solutions.
Low-rank DLAE is a special case of Tikhonov regularization (Eq. (6)) so it has a closed-form solution,
namely LR-DLAE (Jin et al., 2021). See case 10 in Table 1. However, for EDLAE, it enforces
the zero-diagonal constraint, which makes the exact solution difficult to express. Recall that that
low-rank EDLAE aims to solve :
W = arg min	||X — X(W — dMat(diag(W))∣∣F + ∣∣Λ2(W — dMat(diag(W))∣∣F (14)
rank(W)≤k	F	F
Here, we present an approximate low-rank closed-form solution of Eq. (14) by decomposing the
optimization problem into two subproblems, which is similar to (Jin et al., 2021). For detailed
analysis and process, please refer to Appendix B. We first consider the full-rank closed-form solution
for EDLAE (Eq. (13) with zero-diagonal constraint), which is given by:
W * = I — C ∙ dMat(1 0 diag(C)), where C = (X T X + Λ)-1
Then, we consider two (closed-form) approaches to produce low-rank matrix approximation of W* :
Method 1 (LR-EDLAE-1): Selecting Wc to
the zero-diagonal constraint:
best approximate the performance of W * without
Wc =arg	min	||XW *
rank(W)≤k
—XW ||F
arg min ||XW * — XW ||F + ∣∣Λ2(W * - W )||F
rank(W)≤k
(15)
where X = ʌ 1 . Noting, in the full-rank problem Eq. (13), it forces the diagonal of derived matrix
Λ2
(W*) to be zero. Here, we relax the zero-diagonal constraint. This relaxation corresponds to lower
7
Under review as a conference paper at ICLR 2022
bounding an approximate low-rank EDLAE objective function when rank k is sufficiently large.
(Details in Appendix B). The closed-from solution of Eq. (15) is given by:
C = W *(QkQT)I	(16)
where Qk takes the first k rows of matrix Q: XW* S=D PΣQT.
Method 2 (LR-EDLAE-2): SVD approximation of W*: The alternative solution is to simply
perform SVD: W* S=VD UΣVT, and which gives a low-rank estimation of W*: Wc = UkΣkVkT.
This solution corresponds to upper bounding the minimization objective in Eq. (15) (Details in
Appendix B).
Both approaches relax the hard zoer-diagonal constraint on Wc. In the next section, the experimental
results show both approaches can provide comparable or better performance compared with the
ADMM solution, and also very close to the full rank EDLAE solution.
5	Experimental Results
This section experiments different regularizations for linear recommendation models, with a goal to
validate the efficacy of various regularizations (all can be categorized under nuclear or Frobenius
norms) together with their closed-form solutions. We answer three questions: Q1. How do the
closed-form solutions of low-rank Frobenius-norm perform compared with the ADMM solutions
(Section 4) and how does the weighted nuclear-norm regularizer for matrix factorization (Proposition 2
and Corollary 1) perform? Q2. What is the tradeoff between the number of factors (rank k) and
the recommendation accuracy? Q3. How does the ordering of weights from small to large (non-
descending) for adjusting the singular values (Corollary 1) affect the recommendation performance?
Experimental Setup: We use three commonly used datasets for recommendation studies: MovieLens
20 Mil. (ML-20M) (Harper & Konstan, 2015), Netflix Prize (Netflix) (Bennett et al., 2007), and the
Million Song Data (MSD)(Bertin-Mahieux et al., 2011). We obtained the datasets and all benchmarks
from authors of EASE (Steck, 2019), EDLAE (Steck, 2020), and Mult-VAE (Liang et al., 2018).
Similar to the latest study in EASE (Steck, 2019), and EDLAE (Steck, 2020), we consider the follow-
ing state-of-the-art recommendation models: ALS (WMF) (Hu et al., 2008) for matrix factorization
approaches, SLIM (Ning & Karypis, 2011), EASE (Steck, 2019), and EDLAE (Steck, 2020) for
linear autoendoers, CDAE (Wu et al., 2016), Mult-DAE and Mult-VAE (Liang et al., 2018) for
deep learning models. The experiment settings for these baseline are the same as (Liang et al.,
2018; Steck, 2019; 2020). Also we follow their practice (Liang et al., 2018; Steck, 2019; 2020) for
the strong generalization by splitting the users into training, validation and tests group, and report
performance metrics Recall@20, Recall@50 and nDC G@100. Finally, we note that our code are
openly available (see Appendix E.1).
Q1: Low-Rank Frobenius-Norm and (Weighted) Nuclear-Norm Regularization: We evaluate the
low-rank Frobenius-norm-based regularization and the nuclear-norm-based regularization (Eq. (1))
for the matrix factorization. Here EDLAE-ADMM, LRR, LR-DLAE, LR-EDLAE-1, LR-EDLAE-2,
MF dropout and VLAE are listed in Table 1. To determine the non-descending order of weights λi for
the closed-form solution in Eq. (11), we follow the practice in weighted nuclear-norm regularization
in (GU et al., 2014) as well as the optimized PPCA weight (Lucas et al., 2019b). Let λi = C where
σi
C is a hyperparameter, and we perform grid-search to find the optimal one.
Table 2 shows that the weighted nuclear-norm regularization (VLAE) based matrix factorization
performs worse than the constant weighted version (Regularized PCA). The latter demonstrates strong
performance comparing against the WFM/ALS (one of the most popular implicit matrix factorization
algorithms). The closed-form solutions (LR-DLAE, LR-EDLAE-1 and LR-EDLAE-2) all perform
comparable with the ADMM-based low-rank solution and the full-rank DLAE and EDLAE solutions.
Q2: nDCG vs Rank k for low-rank Frobenius norm: In this experiment, we focus on evaluating
the recommendation accuracy (using nDCG) against the rank k. Specifically, we vary k from around
1K to around 10K, and we tune and compare four different methods, including EDLAE-ADMM,
LR-DLAE, LR-EDLAE-1, and LR-EDLAE-2. We have the following observations from Fig. 1: 1) As
k increases, the recommendation accuracy also increases in general; however, most of them reach a
plateau around similar K, and for different datasets, the saturating point varies. 2) LR-DLAE performs
8
Under review as a conference paper at ICLR 2022
Table 2: The performance comparison between different regularizations. We highlight the best results
in bold and underline the 2nd best results for each metric. See Table 1 for the notation.
Model			ML-20M			Netflix			MSD		
			Recall@20	ReCall@50	nDCG@100	ReCall@20	Recall@50	nDCG@100	Recall@20	ReCall@50	nDCG@100
FrobeniUs Norm	full rank	EASE	-0391	-0521	0.420	-0362	-0445	0393	-0333	-0.428	0.389
		DLAE	-0392	-0527	0.424	-0.362	-0.446	0395	-0.329	-0.426	0387
		EDLAE	-0393	-0523	0.424	-0.366	-0.449	0.398	-0.334	-0.429	0.392
	low rank	EDLAE-ADMM	-0392	-0524	0.424	-0365	-0448	0396	-0330	-0.424	0.386
		LRR	-0376	-0511	0.408	-0348	-0431	0380	-0248	-0335	0301
		LR DLAE	-0392	-0527	0.424	-0.362	-0445	0395	-0306	-0.403	0.363
		LR-EDLAE-1-	-0392	-0523	0.424	-0365	-0.449	0.398	-0327	-0.423	0.384
		LR-EDLAE-2-	-0392	-0523	0.424	-0365	-0.449	0.398	-0325	-0.421	0.382
Nuclear Norm		MF dropout	-0367	-0501	0.393	-0.334	-0.418	0365	-0270	-0367	0328
		Regularized PCA	-0364	-0501	0.392	-0331	-0.417	0365	-0.229	-0.313	0.279
		VLAE	-0348	-0474	0.378	-0325	-0405	0357	-0205	-0.254	0.286
Baseline	WMF/ALS		-0360	-0498	0.386	-0.316	-0404	0351	-0.211	-0.312	0.257
	SLIM		-0370	-0495	0.401	-0347	-0428	0379	no results in (Ning & Karypis, 2011)		
	CDAE		-0391	-0523	0.418	-0.343	-0428	0376	0.188	0.283	0.237
	MULT-DAE		-0387	-0524	0.419	-0.344	-0.438	0380	-0.266	-0363	0.313
	MULT-VAE		-0395	-0.537	0.426	-0351	-0444	0.386	-0.266	-0.364	0.316
# items			20108			17769			41140		
# users			136677			463435			571353		
# interactions			10million			57million			34million		
(a)
(b)	(c)
Figure 1: Low-rank models nDCG@100 on test data for 3 datasets.
worse than EDLAE-based approaches in two out of three datasets. This partially demonstrates the
benefits of zero-diagonal constraint. 3) The closed-form solution of EDLAE performs comparable
or even slightly better than ADMM methods as K grows; but when K is relatively small, ADMM
method performs slightly better. But none-the-less, for most of the reasonable choices of k when low
rank approximates full rank, the closed-form solution performs comparable or better.
Q3: Impact of Weight Ordering: Finally, we study how the ordering of weights from small to
large (non-descending) for adjusting the singular values ( Corollary 1) affects the recommendation
performance using matrix factorization (closed-form solution in Eq. (11)). Our results are in Table 3.
Here, we obtain the searched optimal weight parameters from weighted Tikhonov regularization
(following the approach in (Jin et al., 2021)), and map it back to the parameters in the closed-form
solution ( Proposition 1). Then we sort the parameters in the non-descending order, and then report
their results in the second row of Table 3. We can see that the recommendation performance becomes
significant worst. This help confirm our conjecture that the strict ordering of weight on matrix
factorization and other regularizations can be an inherent limitation for those approaches.
6 Conclusion and Discussion
This work provides a complete analysis on the recently proposed linear models for recommendation
systems. Despite that models leverage different deep learning techniques, they achieve similar perfor-
mance. We find that this is not coincident: all the models add either a nuclear-norm-based (Lemma 1)
or a Frobenius-norm-based regularizer. The nuclear-norm-based approach results in estimators that
keep X’s singular vectors and shrink its singular values in a quite rigid way ( Proposition 2 and
Lemma 2), which limit their prediction power. The Frobenius-norm models are more expressive
( Proposition 1) and effective but their estimators are either full-rank or do not have closed-form solu-
tions. To get the best of both nuclear and Frobenius worlds, we propose two low-rank and closed-form
estimators ( Section 4) based on carefully generalizing Frobenius-norm-based regularizers. These
estimators have competitive performance against linear performance leaders, and thus concisely pack
all the benefits obtained by a recent long line of research and abstract out all the computation nuance.
Table 3: Investigating the weight ordering of Matrix Factorization
Model	ML-20M			Netflix			MSD		
	Recall@20	ReCan@50	nDCG@100	Recall@20	Recall@50	nDCG@100	Recall@20	ReCan@50	nDCG@100
MFZLRR Weighted	-0.3806-	-0.5175-	0.4102	-0.3484-	-0.4320-	0.3797	-0.2508-	-0.3390-	0:3037
MF sorted	0.3017	0.4507	0.3361 —	0.2860	0.3801	0.3265 —	0.2288	0.3148	0.2802 —
9
Under review as a conference paper at ICLR 2022
References
Charu C. Aggarwal. Recommender Systems: The Textbook. Springer, 1st edition, 2016. ISBN
3319296574.
Josh Alman and Virginia Vassilevska Williams. A refined laser method and faster matrix multipli-
cation. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pp.
522-539. SIAM, 2021.
Xuchan Bao, James Lucas, Sushant Sachdeva, and Roger B. Grosse. Regularized linear autoencoders
recover the principal components, eventually. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
James Bennett, Charles Elkan, Bing Liu, Padhraic Smyth, and Domonkos Tikk. Kdd cup and
workshop 2007. 2007. doi: 10.1145/1345448.1345459. URL https://doi.org/10.1145/
1345448.1345459.
Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song
dataset. 2011.
Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster
semi-definite optimization. In Conference on Learning Theory, pp. 530-582. PMLR, 2016.
Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.
Jacopo Cavazza, Pietro Morerio, Benjamin Haeffele, Connor Lane, Vittorio Murino, and Rene Vidal.
Dropout as a low-rank regularizer for matrix factorization. In Proceedings of the Twenty-First
International Conference on Artificial Intelligence and Statistics. PMLR, 2018.
Kun Chen, Hongbo Dong, and Kung-Sik Chan. Reduced rank regression via adaptive nuclear norm
penalization. Biometrika, 100(4):901-920, 2013.
Evangelia Christakopoulou and George Karypis. Hoslim: Higher-order sparse linear method for
top-n recommender systems. In Advances in Knowledge Discovery and Data Mining, 2014.
Maurizio Ferrari Dacrema, P. Cremonesi, and D. Jannach. Are we really making much progress? a
worrying analysis of recent neural recommendation approaches. In RecSys’19, 2019.
Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM Trans.
Inf. Syst., 2004.
C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,
1(3):211-218, 1936.
Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization
with application to image denoising. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 2862-2869, 2014.
F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM Trans.
Interact. Intell. Syst., 2015. doi: 10.1145/2827872.
Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for implicit feedback datasets. In ICDM’08,
2008.
Jose Pedro Iglesias, Carl Olsson, and Marcus Valtonen Ornhag. Accurate optimization of weighted
nuclear norm for non-rigid structure from motion. arXiv preprint arXiv:2003.10281, 2020.
Ruoming Jin, Dong Li, Jing Gao, Zhi Liu, Li Chen, and Yang Zhou. Towards a better understanding
of linear recommendation models. In KDD’21, 2021. URL https://arxiv.org/abs/
2105.12937.
Santosh Kabbur, Xia Ning, and George Karypis. Fism: Factored item similarity models for top-n
recommender systems. KDD ’13, 2013.
10
Under review as a conference paper at ICLR 2022
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Yehuda Koren. Factorization meets the neighborhood: A multifaceted collaborative filtering model.
In KDD’08, 2008.
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30-37, August 2009.
Daniel Kunin, Jonathan Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of regularized
linear autoencoders. In Proceedings of the 36th International Conference on Machine Learning,
Proceedings of Machine Learning Research, pp. 3560-3569. PMLR, 09-15 Jun 2019.
Xiaopeng Li and James She. Collaborative variational autoencoder for recommender systems. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’17, pp. 305-314, 2017.
Dawen Liang, R. G. Krishnan, M. D. Hoffman, and T. Jebara. Variational autoencoders for collabora-
tive filtering. In WWW’18, 2018.
James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don't blame the elbo!
a linear vae perspective on posterior collapse. In Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019a.
James Lucas, George Tucker, Roger B. Grosse, and Mohammad Norouzi. Don’t blame the elbo!
A linear VAE perspective on posterior collapse. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d'Alch6-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
9403-9413, 2019b. URL https://proceedings.neurips.cc/paper/2019/hash/
7e3315fe390974fcf25e44a9445bd821-Abstract.html.
Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and
high-dimensional scaling. The Annals of Statistics, pp. 1069-1097, 2011.
Xia Ning and George Karypis. Slim: Sparse linear methods for top-n recommender systems. ICDM
’11, 2011.
Steffen Rendle, Li Zhang, and Yehuda Koren. On the difficulty of evaluating baselines: A study on
recommender systems, 2019.
Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Darius Braziunas. On the effectiveness of
linear models for one-class collaborative filtering. AAAI’16, 2016.
Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I. Nikolenko. Recvae:
A new variational autoencoder for top-n recommendations with implicit feedback. In Proceedings
of the 13th International Conference on Web Search and Data Mining, WSDM ’20, pp. 528-536,
2020.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Andreas Stathopoulos and James R McCombs. Primme: Preconditioned iterative multimethod
eigensolver—methods and software description. ACM Transactions on Mathematical Software
(TOMS), 37(2):1-30, 2010.
Harald Steck. Embarrassingly shallow autoencoders for sparse data. WWW’19, 2019.
Harald Steck. Autoencoders that don’t overfit towards the identity. In NIPS, 2020.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267-288, 1996.
11
Under review as a conference paper at ICLR 2022
Michael E. Tipping and Chris M. Bishop. Probabilistic principal component analysis. JOURNAL OF
THE ROYAL STATISTICAL SOCIETY, SERIES B, 61(3):611-622, 1999a.
Michael E. Tipping and Christopher M. Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611-622, 1999b. doi:
https://doi.org/10.1111/1467- 9868.00196. URL https://rss.onlinelibrary.wiley.
com/doi/abs/10.1111/1467-9868.00196.
Madeleine Udell, Corinne Horn, Reza Zadeh, and Stephen Boyd. Generalized low rank models.
Found. Trends Mach. Learn., 9(1):1-118, June 2016.
Wessel N. van Wieringen. Lecture notes on ridge regression, 2020.
Lingfei Wu, Eloy Romero, and Andreas Stathopoulos. Primme_svds: A high-performance precondi-
tioned svd solver for accurate large-scale computations. SIAM Journal on Scientific Computing, 39
(5):S248-S271, 2017.
Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. Collaborative denoising auto-
encoders for top-n recommender systems. WSDM ’16, 2016.
Man-Chung Yue. A matrix generalization of the hardy-littlewood-p\’olya rearrangement inequality
and its applications. arXiv preprint arXiv:2006.08144, 2020.
Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system: A survey
and new perspectives. ACM Comput. Surv., 2019.
Shuai Zheng, Chris Ding, and Feiping Nie. Regularized singular value decomposition and application
to recommender system, 2018.
12
Under review as a conference paper at ICLR 2022
A Related Work
There have been extensive researches on recommendation (Aggarwal, 2016). Besides the basic
user-based and item-based collaborative filtering (Deshpande & Karypis, 2004), the full-rank lin-
ear autoencoder approaches include SLIM (Ning & Karypis, 2011), HOLISM (Christakopoulou
& Karypis, 2014), EASE (Steck, 2019), DLAE (Steck, 2020), whereas low-rank approaches in-
clude (Kabbur et al., 2013; Sedhain et al., 2016; Steck, 2020). All the customized recommendation
models have been enforcing zero diagonal constraints for generalization purpose, whereas we show an
approximate closed-form solution for a two-term Tikhonov regularization without the zero diagonal
constraint can be as effective as these models.
Matrix factorization has been been widely studied in practice, partially due to Netflix competition (Ko-
ren et al., 2009). Methods like SVD++ (Koren, 2008) and implicit Alternating Least Square (ALS)
method (Hu et al., 2008) (also weighted matrix factorization) have been very influential. Jin et al.
(2021) shows the relationship between linear autoencoders and matrix factorization, and pointed
out a potential advantage of linear autoencoders. In this work, we take a step further to reveal a
deeper relationship between Tikhonov regularized linear autoencoders and a few other regularizations
including matrix factorization, and show the potential limitation of the class of regularization. We also
utilize the variational linear autoencoders (VLAE) to study how the deep VAE based recommendation
approaches (Li & She, 2017; Liang et al., 2018; Shenbin et al., 2020) relate to linear autoencoders
and matrix factorization.
Outside recommendation, there have been a few recent studies on regularization landscapes of linear
(variational) autoencoders (Kunin et al., 2019; Bao et al., 2020; Lucas et al., 2019a). They do not
provide the general weighted `2 regularization and thus did not find the inherent limitation on the
regularization (for MF). Our VLAE inspired regularization is also never studied before.
Nuclear-norm regularizers can recover low-rank matrices in the vector regression setting (Negahban
& Wainwright, 2011). Its weighted generalization can be applied in the area of image processing (Gu
et al., 2014). Because weighted nuclear-norm is usually not convex or differentiable, finding optimal
solutions is difficult except for a few special cases (Chen et al., 2013).
B Approximate Low rank EDLAE
B.1 Low-rank closed-form solution for DLAE
Recall that full-rank DLAE problem:
min ||X - XW||F + ∣∣Λ 1 W||F s.t. Λ = TJdMat(diag(XTX))	(17)
W	1-p
And now consider the following basic low-rank DLAE model:
arg min ||X — XW ||F + ∣∣Λ 1 W ||F s.t. Λ = —p—dMat(diag(X t X))	(18)
rank(W)≤k	F	F	1 - p
We leverage the strategy used in (Jin et al., 2021) to derive the low-rank closed-form solution. We
formalize the low-rank problem as a standard regression problem:
一	「X]	一 「X]
Y = X X = λ2	(19)
Equiped with this, the above regression problem can be formulated as:
min ||Y — XW ||F
rank(W)≤k
IIY — XW」F + ||XW * — XW ∣∣F,
min
rank(W)≤k
where W * = arg min ∣∣Y — XW * ∣∣F, is the optimal solution for the full-rank regression problem
Eq. (17). The primary loss function ∣∣Y — XW||F then can be decomposed into two parts: ∣∣Y —
XW*∣∣F (full-rankregression problem), and ∣∣XW* — XW∣∣F (low-rank approximation problem).
13
Under review as a conference paper at ICLR 2022
Since W * is the optimum, the vector (Y - XW *)is orthogonal to XW * - XW = X (W * - W)
(van Wieringen, 2020), the above equation holds.
Thus the primary low-rank regression problem Eq. (18) can be broken into two subproblems:
(Subproblem 1:) full-rank closed-from solution:
W * = arg min ||Y - XW *||F
= (XTX+Λ)-1XTX
(Subproblem 2:) low-rank approximation solution:
Wc
arg min
rank(W)≤k
∣∣XW* -Xw||F
arg min ||XW* - XW||F + ∣∣Λ2(W* - W)||F
rank(W)≤k
Denote Y * = XW *, the best rank k approximation of Y * in Frobenius norm can be derived from
SVD (Eckart & Young, 1936):
Y * s=d P ΣQt
Y *(k) = Pk∑k QT
where Qk takes the first k rows of matrix Q, as well as P and Σ. Since, Q are orthogonal matrices,
we notice that:
Y *(k) = Pk Σk QT = P ∑Qτ (Qk QT)
=XW *(Qk QT ) = XW
Imediately, We have
C = W *(QkQT)
The complete loW-rank closed-form solution for Eq. (18) is:
C =(X T X + Λ)-1X T X (Qk QT)
B.2 Approximate Low-rank closed-form solution for EDLAE (LR-EDLAE- 1)
Recall that that loW-rank EDLAE aims to solve
W = arg min ||X — X(W — dMat(diag(W))∣∣F + ∣∣Λ2(W — dMat(diag(W))∣∣F (20)
rank(W)≤k	F	F
When the rank k of the loW-rank model is sufficiently large (for instance k ≈ 1000), its objective
function can be approximate as (Steck, 2020):
W = arg min ||X - XW||F + ∣∣Λ2 W||F
rank(W)≤k
s.t. diag(W) = 0
(21)
Note that for most of our experimental studies, the rank k in the loW-rank model is typically fairly
large (over 1K), and thus the above approximation is reasonable. Compared to Eq. (18), loW-rank
EDLAE imposes only an additional zero-diagonal constraint. Our goal is to mimic the strategy
developed in Appendix B.1 to build a tWo-staged algorithm to produce a loW-rank approximation.
We have the folloWing key Lemma.
Lemma 3. Let W * be a (full rank) solution to
ɪwin kY - XW*kF
diag(W*) = 0.
14
Under review as a conference paper at ICLR 2022
where,
X
Λ 2
X
0
Y
X
Let W be any matrix such that diag(W) = 0. We have
hY — XW *,XW * — XW i = 0.
Proof. First, observe that W * can be found by solving a sequence of independent linear regressions,
each of which correpsonds to a column of Y. Let X(-i)be the matrix that removes the i-th column
of X. Then the i-th column of W*, namely W* can be found using two steps:
•	Step 1. Wi = (XT-i)X(-1))-1XT-i)Y.
•	Step 2. Construct W* by inserting a zero between the (i 一 1)-st and i-th entries of Wi.
This is because enforcing W*i = 0 is the same as removing the i-th column in X to fit 匕.
Next, we move to show that for any i,
hYi 一 XW*,XW* - XWii = 0.
Observing that W* is the optimal solution for ∣∣Yi ― XW* k2, Yi ― XW* is orthogonal to the
column space of X(_» On the other hand, XWi* - XWi = X(Wi* 一 Wi) and (Wi* 一 Wj = 0.
This implies that XW* -XWi is in the column space of X(->Therefore, Yi 一XW* is orthogonal
to XWi* - XWi, which directly implies the lemma.	□
Using Lemma 3, we see that
min
rank(W)≤k
l∣Y - XW ||F
s.t.	diag(W) = 0
min	∣∣Y — XW*∣∣F + ||XW* — XW||F,
rank(W)≤k
s.t.	diag(W *) = 0 diag(W) = 0
(22)
≥	minJ∣Y - XW *∣∣F + ||XW * — XW ||F,
rank(W)≤k
s.t. diag(W *) = 0
The last inequality holds because one constraint diag(W) = 0 is removed so the objective will only
improve. With the above analysis, we are ready to describe our algorithm.
(Subproblem 1:) full-rank closed-from solution:
W * =argmin ||Y 一 XW * ||F
= argmn||x-XW *1|F + iiλ1 W *1|F
s.t.	diag(W*) = 0
(23)
The closed-form solution is:
C= (XTX + Λ)-1
W * = I 一 C ∙ dMat(1 0 diag(C))
15
Under review as a conference paper at ICLR 2022
Table 4: Time complexity analysis for the models listed in Table 1. X ∈ Rm×n. Assume m > n.
And we treat XTX as precomputed.
Models	Complexity Bounding Operation	Complexity
1. Regularized PCA	Singular Value Decomposition	O(kn2 + knm)
2. MF dropout	Eigen Decomposition	O(m2n + mn2)
4. VLAE	k leading Eigen Decomposition	O(km2 + kn2)
5. EASE 6. DLAE 7. EDLAE	Multiplication	θ(n2.373)
9. EDLAE-ADMM	ADMM	θ(t22.373)一
3. WLAE_8. LRR 10. LR-DLAE 11. LR-EDLAE-1	12. LR-EDLAE-2		k leading Eigen Decomposition	O(kn2 + n2∙373)
(Subproblem 2:) low-rank matrix approximation:
Wc
arg min
rank(W)≤k
∣∣Xw* -XW||F
arg min ||XW* - XW||F + ∣∣Λ2(W* - W)||F
rank(W)≤k
(24)
Note, in the full-rank subproblem Eq. (23), it forces the diagonal of derived matrix (W*) to be zero.
Here, we relax the zero-diagonal constraint
have to be strictly zero.
- the diagonal of low rank approximate matrix Wc doesn’t
Given this, the closed-from solution (LR-EDLAE-1) of Eq. (24) is given by:
Wc = W* (QkQkT)
where Qk comes from: XW* S= PΣQT.
Intuition on the efficacy of the proposed approximation. We believe that because the full-rank
W * performs well for recommendation accuracy and better than the low-rank solutions (Steck,
2020; Jin et al., 2021), when W’s estimation is closer to W* with respect to the final recovery of
Y , XW ≈ XW*, it has a comparable performance against XW*. In other words, it may not be
essential for the diagonal of low-rank approximate matrix Wc to be
zero.
B.3 SVD OF W* (LR-EDLAE-2)
Furthermore, if we consider the inequality
min	||XW* - XW||F
rank(W)≤k
or a tighter one
min< J|X ||F||W *
rank(W)≤k
- W||2F,
min	||XW* - XW||F
rank(W)≤k
≤	mn<JIX||2||W*
rank(W)≤k
- W||2F
Both suggest if we simply perform SVD on W*, we can obtain an upper bound error between XW*
and XW.
Simply, the other closed-form solution Wc (LR-EDLAE-2) is:
W* S=VD UΣVT
≤O^Z?	____ _ _F
Wc =UkΣkVkT
C Time Complexity Analysis
The running time of all linear models discussed in this paper is determined by one or more of the
following factors: (i) Computation of matrix multiplication and inversion. The best known theoretical
bound is O(n2.373)(Alman & Williams, 2021) but to the authors’ knowledge the algorithm is not
implemented in any mainstream libraries. (ii) Computation of k leading singular vectors. The running
≤
16
Under review as a conference paper at ICLR 2022
time is O(kn2) for an n × n matrix. But recent trends believe that the constant terms in the big-O
notation matter so the asymptotic analysis becomes less fashionable (Stathopoulos & McCombs,
2010; Wu et al., 2017). (iii) Convergence rate ofa gradient-based algorithm. Except for special cases,
when an objective is not convex (e.g., when W is decomposed into W = PQ), the convergence rate
of an algorithm is remarkably difficult to analyze and often has substantial discrepancies with its
practical performance (Bhojanapalli et al., 2016).
We remark that all three factors exhibit significant gaps between theory and practice, i.e., the best
theoretical algorithms are not implemented, constants in big-O matter, and accurate characterization
of convergence rate is difficult. Therefore:
In theory, (i) the running time for full-rank Frobenius-norm models (cases 5. EASE, 6. DLAE, and 7.
EDLAE in Table 1) is O(n2.373) because they involve merely matrix inversion and multiplication
operations; (ii) the running time for most low-rank models (cases 3. WLAE, 8. LRR, 10. LR-DLAE,
11. LR-EDLAE-1, and 12. LR-EDLAE-2 in Table 1) are O(n2.373 + kn2) because they require
computation of k leading singular vectors or eigenvectors; (iii) the nuclear-norm models require
the multiplication with X ∈ Rm×n . So the running time for cases 1 (Regularized PCA), 2 (MF
dropout), and 4 (VLAE) are O(kn2 + mnk), O(m2n + mn2), O(km2 + kn2) respectively; and (iv)
The running time ADMM-based model (case 9. EDLAE-ADMM in Table 1) is O(tn2.373), where t
is the number of iteration needed to converge. We summarize the results in Table 4
In practice, solving low-rank SVD and computing matrix multiplication require similar time especially
after carefully using HPC techniques. So our proposed algorithms LR-EDLAE-1 and LR-EDLAE-2
(Section 4) are neither slower nor faster than existing ones that do not use gradient algorithms (e.g.
full-rank EDLAE, etc.). The clock time for gradient-based (e.g. ADMM) algorithms is usually worse
than that in theory because determination of hyper-parameters such as learning rate is usually a
guesswork. A rule of thumb is that LR-EDLAE-1 and LR-EDLAE-2 are faster for small k and slower
(than e.g., ADMM-based methods) for large k.
Finally, we would like to address again on the advantage of having any closed-form solutions: they
are easy to implement and use, and can be quickly tested using any standard matrix computation
platform without the specialized recommendation library. They are more robust to error and bug, and
can serve as the competitive baseline for recommendation evaluation tasks.
D Proofs
D.1 Proof of Lemma 1
Consider X ∈ Rm×n . Suppose latent variables z ∈ Rk and generate data x ∈ Rn . The Variational
Linear Autoencoder (VLAE) is defined in the same way as (Lucas et al., 2019b):
p(x | Z) = N (Wz + μ,σ2I)
q(z | x) = N(V(X - μ), D)
For simplification, We set μ = 0 in following context. And the ELBO of VLAE is known as:
Lx = - KL(q(z|x)||p(z)) + Eq(z|x) [logp(x|z)]
KL(q(z |x)||p(z)) = - log |D| + xTVTVx + tr(D) - k
Eq(z∣x)[logp(x∣z)] = —	2 (tr(WDWT) + xτVTWTWVx _ 2xTWVx + xτx)
(25)
(26)
2
n
——log 2πσ
Again, the (maximizing) ELBO can be written as:
17
Under review as a conference paper at ICLR 2022
Lx = -1 ( - log |D| + XTVTVx + tr(D) - k) - 2 log 2πσ2
-S (ibr(WDWT) + xτVTWTWVx - 2xTWVx + XTx)
=-1 ∣∣Vx∣∣2 - 2⅛ (∣∣W √D∣∣F + ∣∣x - wvx∣∣2) + f(D,σ)
where f(D, σ) = 2 log |D| - 2tr(D) + 2 - q log 2πσ2, X ∈ Rn and Z ∈ Rk.
For whole data, it is equivalent to minimize:
L = ||XT - WVXTIlF + m||W√D∣∣F + σ2∣∣VXT||F + g(D,σ)
=IIX - XVTWTIlF + m∣∣√DWTIlF + σ2∣∣XVT||F + g(D,σ)
(27)
(28)
where g(D, σ) = -σ2m log IDI - tr(D) + k - nlog 2πσ2).
D.2 Proof of Proposition 1
Note that when σi ≤ λ(k-i), the new singular value shrinks to zero, and can be removed. Basically,
for any λ(i) ≥ ∙∙∙ ≥ λ(k), we can build the corresponding Tikhonov regularized instance by setting
σ2
σ+λi
σi - λ(I) “ ∖0
,i∙e∙,λk
σi
σi3
σi - λ(k-i)
- σi2 .
(29)
Discussion of Proposition 1: Further, the same observation holds true for the regularization (Eq. (2)),
and the weighted-nuclear norm regularization in when the weights are in the non-ascending order.
This observation suggests a potentially limitation of the earlier regularization as they will always try to
maintain the larger singular values: when a singular value is large, the shrinkage will be small. Such
regularization has shown to work well in the areas such as image processing (Gu et al., 2014). But it
has not been studied or confirmed if it will work for the recommendation. In Section 5, we report our
experimental study which shows such regularization could be too restrictive for recommendation.
D.3 “Clear Up” Eq. (4)
Since our objective is for recommendation (not purely on recovering the low rank factors of the data),
we treat the covariance matrix D as hyperparameters. Thus, the term g(D, σ) becomes a constant,
and let A = σVT, B = 1∕σWT, Λ = σ√mD.
Ifwe consider D as an optimization parameter, then the optimal solution of VLAE is equivalent to that
of PPCA (Tipping & Bishop, 1999a). In this case, D = σ2(Σ2∕m)-1, where Σ2 = diag(σf) are
σ2
the eigen-values of the covariance matrix of X, and σ2 = £^ Y^n=k+ι mj ∙ Further, the closed-form
solutions of V (A) and W (B) are characterized. However, for recommendation, the matrix D can be
considered as a hyperparameter (to be tuned); in this case, the closed-form solution is not studied yet.
It is easy to see we can “clear up” Eq. (4) and obtain the following optimization problem :
A∈Rn×mk,iBn∈Rk×n kX-XABk2F+ kXAk2F + kΛBk2F,	(30)
in which decision variables are A and B, and the hyperparameter is a diagonal matrix Λ ∈ Rk×k .
D.4 Intuition for Proving Proposition 2
Here, We explain the intuition for proving Appendix D.5 Proposition 2 (see Appendix D.5 for the
full analysis). Consider OPT1 and let W = P Q. Our goal is to characterize the behaviors of P
and Q with the presence of the regularizers when W = PQ is known (fixed). Let the SVD of W
be UW∑wVTw. Because two regularizers ∣∣Λ2QkF and ∣∣PΛ1 ∣∣F are symmetric, we could “guess”
P = UW∑WΩ and Q = Ωt∑WVW, where Ω is a unitary matrix. Now we have
18
Under review as a conference paper at ICLR 2022
kΛ1 QkF + kP Λ 1 kF = M1ΩΣ1 ViW kF + kUw ∑W ΩΛ 2 kF = 2kΛ2 Ω∑W kF.
Now the question of finding P and Q when W is known boils down to finding a unitary matrix Ω
that minimizes ∣∣Λ2Ω∑W∣∣F, where diagonal matrices Λ and ∑w are given. Recall that λ% = Λɑ
and let σ% = (∑w)近.Note that λ%'s could be unsorted and, and that σ^s are sorted in descending
order.
If we restrict Ω to be only a permutation matrix, then we aim to find a permutation ∏ ∈ [k]
that minimizes Pi≤k λπ(i) σi. Using a rearrangement inequality (Yue, 2020), we can see that
the minimal is achieved when λ∏(i) ≤ λ∏(2) ≤ ∙∙∙ ≤ λ∏(k). In this case, we indeed have
minΩ a permutation IIλ 2 ωςW kF = INwks,*, Where ω = (λ∏(i),..., λ∏(k)).
Note that because PQ = PΩΩTQ for any unitary matrix Ω, it is always beneficial to use Ω to shuffle
the rows and columns of P and Q so that the largest σi is mapped to the smallest λi , etc. This “degree
of freedom” from Ω also explains why ordering the values along Λ's diagonal is irrelevant.
Appendix D.5 shows that even when Ω is allowed to be any unitary matrix, the optimal one is still a
permutation matrix. This conclusion can be viewed as a matrix version of re-arrangement inequality.
D.5 Proof of Proposition 2
By slightly abusing the notation, we shall let OPT1 (OPT 2) be the value of the optimal solution for
OPT 1 (OP T 2). We need to show that OPT 1 = OPT 2. We need two directions.
OPT2 ≥ OPT1: LetW* be an optimal solution for OP T 2. LettheSVDofW* be U*Σ*(V *)T.
Recall that W* needs to satisfy the rank constraint rank(W*) ≤ k so U* ∈ Rm×k, Σ* ∈ Rk×k,
and V * ∈ Rn×k. Let∏ be a permutation on [k] such that λ∏(i) ≤ X冗⑵ ≤ ∙∙∙ ≤ λ∏(k). Let also Ω
be the corresponding permutation matrix. Specifically, Ω ∈ {0,1}k×k and there is exactly one entry
in each row of Ω is 1:
Ωi,j
ifj = π(i).
otherwise.
1
0
For example, consider a case in which λι > λ2 > •…> λk. Then we set ∏ = (k, k 一 1,..., 1), and
correspondingly,
0 ... 0 1
C 0 ... 1 0 I
Ω =	.
1 .... 0 0
Next, let P = U *(Σ*) 2 Ω and Q = Ωt(Σ* ) 2 (V * )t We have W * = PQ and f (W *) = f (PQ).
In addition,
kΛ1 QkF + kP Λ 1 kF = 2kΛ2 Ω(Σ*)2 kF = 2 X 入.(严=2kW *kω,*,
i≤k
where σi is the i-th largest singular value of W*. In other words, we have found a (P, Q) pair such
that
f (PQ) + kΛ2 QkF + kP Λ2 kF = f (W *) + 2kW *kω,* =OPT 2,
which shows that OPT1 ≤ OPT2.
OPT2 ≤ OPT1. Let P* and Q* be an optimal solution for OPT1. Let the singular values of P*
be σι(P*) ≥ σ2(P*) ≥ ∙∙∙ ≥ σk(P*) and those of Q* be σι(Q*) ≥ σ2(Q*) ≥∙∙∙≥ σk(Q*). Let
also σ* ≥ ∙∙∙ ≥ σk be the singular values of P*Q*.
We shall find a lower bound of kΛ2QkF + kPΛ 1 kF expressed in terms of σ*'s. In fact, we shall
s ow a	kΛ2QkF + kPΛ2kF ≥ 2kP*Q*kω,*.	(31)
19
Under review as a conference paper at ICLR 2022
One can see that if Eq. (31) were true, we have
OPT2 ≤ f(P*Q*) +2∣∣P*Q*kω,* ≤ f(P*Q*) + kΛ1 Q*∣∣F + ∣∣P*Λ1 kF = OPT 1.
Thus, it remains to prove Eq. (31). Let 入⑴ ≥ λ(2) ≥ ∙∙∙ ≥ λ(k) be a sorted sequence of λi's i.e.,
λ(k) = λπ(1), λ(k-1) = λπ(2),.. .,λ(1) = λπ(k).
First, we show that kPΛ21∣F ≥ Pk=I λ(k-i+i)×σ2(P*) and ∣∣Λ 1 QkF ≥ Pk=I X(i+i)×σ2(Q*).
We need the following Lemma (see e.g., Theorem 2 in (Yue, 2020)):
Lemma 4. Let A and B be two positive definite matrices in Rk×k. Then it holds that
k
Xσi(A)σk-i+ι(B) ≤ tr(B2AB2).	(32)
i=1
Let the SVD of P* be UP* ΣP* VPT* and that of Q* be Uq* Σq* VQT*. We have
∣∣P*Λ1 kF = ∣∣Up * Σp * VP* Λ1 kF = k∑p * VPT* Λ1 kF = tr(Σp* VP* ΛVP * Σp *).	(33)
We now apply Lemma 4 by setting A = VPT* ΛVP* and B = Σ2P* , and obtain that
k
kP*Λ2 kF = tr(∑p* VP*ΛVP* Σp*) ≥ X λ(k+i-i) × σ2(P*).	(34)
i=1
We may similarly prove that kΛ1Q* kF ≥ Pik=1 λ(k-i+1) × σi2 (Q*). Therefore,
k
kΛ2Q* kF + kP*Λ2 kF ≥ Xλ(k+i-i) × (σ2(P*) + σ2(Q*))	(35)
i=1
provides a lower bound of ∣∣Λ1 Q*kF + ∣∣P*Λ1 kF in terms of σi(P*) and σ%(Q*). We next aim to
express the lower bound in terms of σ*'s (singular values of P*Q*) directly.
The following program gives a lower bound for ∣∣Λ 1 Q* kF + kP*Λ2 kF:
min: M2 Q*kF + ∣P*Λ2 kF	(36)
subject to W = P*Q*
σi(W) = σi* for i ≤ k.
Write the SVD of W be UW ∑w VW. Also, let P = UW P * and Q = Q*Vw . Noting that the
columns in P* are in the column space of W and the rows in Q* are in the row space of W, we
have (i) σi(P*) = σi(P) and σi(Q*) = σi(Q) for i ≤ k, and (ii) ∣Λ2Q*∣F + l∣P*Λ1 kF =
∣Λ1 QkF + ∣PΛ2 kF.
Therefore, Eq. (36) can be equivalently written as
min： kΛ2 QkF + kPΛ2 kF	(37)
subject to ΣW = PQ
(ΣW )i,i = σi* for i ≤ k.
Now PQ is positive definite. Using a similar technique developed in (Bao et al., 2020) (Theorem 1),
one can see that P = QT. See also Lemma 5. This implies that P = ΣWΩ for some unitary matrix
Ω and σi(P*) = σi(P) = σi(Q*) = σi(Q) = ∙λ∕σ* for i ≤ k. Together with Eq. (35), we have
kΛ2Q*kF + kP*Λ2kF ≥ 2Xλ(k+i-i) × σ2(P*) =2Xλ(k+j) × σ* = 2∣P*Q*∣ω,*.
i≤k	i≤k
20
Under review as a conference paper at ICLR 2022
D.6 Proof of Corollary 1
We first find an optimal solution for Eq. (9). Let the SVD of X be X = UX ΣX VXT , where
UX ∈ Rm×n, Σχ ∈ Rn×n, and Vχ ∈ Rn×n. Let UX be an arbitrary basis for the subspace that is
orthogonal to X's column space so UX ∈ Rm×(m-n) and [Uχ, UX] form a basis for Rm. We have
kX - PQkF + kP Λ1 kF + kΛ1 QkF
UX)XVX -( UX ) PQVX
F+K UX k λ 1
2 + kΛ 2 QVx IlF.
F
~
Let P
P and Q = QVX. Then our objective becomes
min
P,Q
0 ςx )-PQ
0(m-n)×n
2
+ kPΛ2 kF + M2 QkF.
F
(38)
~ ~ ~
〜
〜
Let W = PQ and the singular values of W be σj ≥ σg ≥ ∙∙∙ ≥ σ屋 Let also Σ
Recall also that σi is the i-th largest singular value of X. We next show that
2
k
n
ςX	- P Q
k∑ - PQkF ≥ E(σi-σf + E σ2.
i=1
i=k+1
F
Note first that
〜
〜
2
F
k∑ - Wk
Next, we have ((Zheng et al., 2018)):
k∑ kF + IWkF - 2h∑ ,W i.
(39)
k
lh∑ ,W il = ∣tr(∑ W T)k ≤ ∣tr(∑∑W )1 = X σiσi.
i=1
Therefore, hΣ, Wi is maximized when
Wij =	% ifithj≤ek
When we plug in this optimized W to Eq. (39), we get
k
n
k∑ - WkF ≥ ∑(σi - σ*)2 + E σ.
i=1	i=k+1
Next, from Proposition 2, we have
k
kPV2kF + kΛ2QkF ≥ Xλ(fc-i+υσ*.
i=1
Therefore, We can find a lower bound for Eq. (5) in terms of σ*'s:
k
k
m
L(σl, . . . ,σk) = y^(σi	-	σi)2 +2E λ(k-i+1)σi	+ E	σ2	(σl ≥∙∙∙≥	σk	≥	O).	(4O)
i=1
i=1
i=k+1
We next find a minimal value of L (by treating σ* 's as decision variables). This will give US a lower
bound (and is independent of σ*) on our optimization problem. We then show that this lower bound
can be achieved by carefully constructing W (as well as P and Q). This means such W is optimal.
21
Under review as a conference paper at ICLR 2022
Specifically, we need to find an optimal solution for the following program:
minimizeσf,…,σk L(σ"...,σk)	(41)
subject to:	σi ≥ 0
。；≤ σ^ ≤ ∙∙∙ ≤ σk (Ordering constraint)
We shall first find an optimal solution for
minimizeσ*,...,σ*	L(σ:, ...,σ^)	(42)
subject to:	σ* ≥ 0
Note here, the ordering constraint is removed so the optimal value for Eq. (42) should be no more
than that for Eq. (41). We shall see that the optimal solution for Eq. (41) also satisfies the ordering
constraint so indeed optimal solutions for Eq. (41) and Eq. (42) are the same.
The problem Eq. (42) boils down to finding
k
m in(σi - σi)2 + 2 X λ(k-i+i)σi.
σ 干 ≥0
i	i=1
We note that σ厂S do not interact with each other so We can optimize each σ厂S independently. We get
σi = (σi - λ(k-i+1) ) .
We can check that σj ≥ ∙∙∙ ≥ σkξ. Therefore, the optimal value for Eq. (41) is
kkn
(σi - (σi - λ(k-i+1))+)2 + 2	λ(k-i+1)(σi - λ(k-i+1))+ +	σi2.
i=1	i=1	i=k+1
This is also a lower bound for Eq. (9). One can check that when we set P and Q as
P * = Uk diag(J(σι- λ(k))+),..., J(σk - 入⑴)+)Ω,	(43)
Q* = Ωτdiag(,(σι- λ(k))+),..., q(σk - λ(1))+)V^,
the lower bound is achieved so Eq. (43) gives an optimal solution. Here, Uk and Vk are leading left
and right singular vectors of X .
Now we move to analyze Eq. (10). Our goal is to reduce Eq. (10) to Eq. (9). Let
P = XAΛ- 1	Q = Λ 2 B.
Then Eq. (10) becomes
minimizep,Q	∣∣X - PQkF + ∣∣PΛ1 ∣∣F + ∣∣Λ2QkF	(44)
subject to P = XAΛ-2 (Constraint P)
Q = Λ 2 B (Constraint Q).
Here, X and Λ are given, whereas P, Q, A, and B are decision variables. The (Constraint P) says that
each column of P needs to be in a column space of X (it is a necessary and sufficient condition for A
to exist). The (Constraint Q) simply says Q and B are linearly related and does not have tangible
impact to the optimization problem.
But we note that when we put aside the constraints, an optimal (P, Q) is specified by Eq. (43). The
columns of the optimal P indeed is in the column space of X. So (P, Q) is also an optimal solution
for Eq. (44). We may find the corresponding A and B :
A* = XtP*Λ1	and B* =Λ-2Q*,
22
Under review as a conference paper at ICLR 2022
D.7 Symmetric lemma
Lemma 5. Let P,Q ∈ Rk ×k befull rank, Λ be a diagonal matrix, and ∑w be a diagonal matrix so
that (∑w )i,i = σ*, where σ*，s are sorted in descending order. Consider the optimization problem:
min : ∣∣Λ2Q∣∣F + ∣∣PΛ2 ∣∣F	(45)
subject to ΣW = P Q
(∑w)i,i = σ* for i ≤ k.
There is an optimal solution such that P = QT
1	1 1 . 1∖	六 A1	t 人	K - IK EI	Fl T-I ///-、♦	∙ 1 ..
Proof. Let P = PΛ 2 and Q = Λ 2 Q. The problem Eq. (45) is equivalent to
min： ∣ΛQ kF + ∣P∣F	(46)
subject to ΣW = PQ
(∑w)i,i = σ* for i ≤ k.
Let the SVD of Q be UQΣ^VT so QT = VqΣ-1 UT. We can also see that P = ∑wQ-1.
Therefore, the objective term becomes
MUQ ∑q VTkF + k∑w v^ ∑Q1uT kF = MUQ ∑q kF + k∑w v^ ∑Q1kF.
Let US consider the stationary points U^ and V^ when Σ八 is fixed. We can see that they need to be
QQ	Q
permutation matrices to minimize both terms in the objective (using the rearrangement inequality
again). Therefore, we can see Q = ∑i∑q∑2 for two permutation matrices ∑ι and ∑2. ThiS implies
AICCC
that Q = Λ 2 ∑ι ∑q∑2, i,e., each row (column) of Q has exactly one non-zero entry. Wemay similarly
show that each row (column) of P has exactly one non-zero entry. In addition, the locations of
non-zero entries of P and QT are identical because PQ is a diagonal matrix. We may thus write
~ 一 一 一	-T 一m 一 一m
P = ς(i)ςP ς(2) Q = ς(2)ς(Q),(1)，
where (Σp)i,i = σi(P) and (夕&))y =。丁(i)(Q), where T is a permutation on [k]. The set of
(possibly unsorted) singular values for PQ thus is σi(P)στ(i)(Q). Thus, we can see that there exists
a permutation ∏ such that
kΛ2 QkF + kPΛ2 kF = X S2(P*)λ∏(i) + σT(i)(Q*)λ∏(i))
i≤k
≥ X2σi(P"T(i)(Q)λ∏(i)
i≤k
≥ 2∣∣P*Q*kω,*.
One can see that we can set P = QT to make all inequality becomes equality so there is an optimal
solution such that P = Qt.	□
E Experimental Details
E.1 Anonymous Code
https://anonymous.4open.science/r/ICLR-2022-Anonymous-Demo-Code-B9FF/
README.md
23
Under review as a conference paper at ICLR 2022
E.2 DLAE and Hyperparameter Tuning
This section presents the hyperparameter tuning process on the validation data over three (ML-20M,
Netflix, MSD) datasets for the full-rank DLAE formula (case 6 in Table 1), which was introduced by
Steck (2020) yet not investigated:
mWn l∣X - XWIlF + l∣Λ1/2 ∙ W||F
Λ = TJdMat(diag(X T X))
1-p
Wc =(XT X + Λ)-1XT X
In practical, l2 regularization (hyperparameter λ) is also imposed:
Wc =(XT X + Λ + λ)-1XT X
The Tables 5 to 7 show the results of nDCG@100 over three datasets respectively. And the optimal
parameters are highlighted.
Table 5: ml-20m, DLAE full rank, parameter tuning on validation dataset by nDCG@100
			λ						
		800	900	1000	1100	1200	1300
P	0.1	0.42024	0.42063	0.42073	0.42102	0.42131	0.4212
	^02^	0.43132	0.43139	0.43154	0.4314	0.43147	0.43136
	^03^	0.43203	0.43211	0.43214	0.43206	0.43203	0.43196
	^01^	0.43001	0.43001	0.42995	0.42996	0.42984	0.42978
	~0Γ^	0.42754	0.42745	0.42729	0.42718	0.42715	0.42704
Table 6: netflix, DLAE full rank, parameter tuning on validation dataset by nDCG@100
			 λ								
		800	900	1000	1100	1200	1300	1400
P	0.2	0.3904	0.3904	0.39027	0.39024	0.3902	0.3903	0.39018
	0.25	0.39247	0.39252	0.39248	0.39249	0.3925	0.3925	0.39256
	0.3	0.39359	0.39359	0.39366	0.39358	0.39362	0.39368	0.39369
	0.35	0.39402	0.39403	0.394	0.39405	0.39399	0.39403	0.39397
	0.4	0.39399	0.39393	0.39395	0.39393	0.39389	0.39388	0.39387
	0.45	0.39346	0.3935	0.39343	0.39344	0.39338	0.3933	0.39329
	0.5	0.39249	0.39241	0.39247	0.39241	0.39242	0.3923	0.39224
E.3 Matrix Factorization with Dropout and Hyperparameter Tuning
Cavazza et al. (2018) show that optimization with dropout (allowing rank d to be optimized) is
equivalent to solving a matrix approximation problem with nuclear norm:
1d
min IIX - PQIlF + d - ∙ £ |典||2 ∙∣∣QTII2
P,Q,d	F	p k=1	2 k 2	(47)
mYn IIX - YIlF + — I∣Y∣∣2
Table 7: msd, DLAE full rank, parameter tuning on validation dataset by nDCG@100
			λ						
		10	20	30	40	50	60
P	0.3	0.38514	0.38515	0.38517	0.38505	0.38492	0.38474
	^04^	0.38596	0.38599	0.38602	0.386	0.38597	0.38592
	^0^	0.38556	0.38555	0.38553	0.38557	0.38553	0.38549
	~Q6~	0.38382	0.3838	0.38381	0.38374	0.38373	0.38366
24
Under review as a conference paper at ICLR 2022
and the solution is given by:
X S=VD UΣVT
Y * = P * ∙ Q*
=U ∙ Sμ(Σ) ∙ VT
Sμ(σ) = max(σ 一 μ, 0)
μ = ∣1ι P 浦Xσi(X)
p + (1 - p)d i=1
where d denotes the largest integer such that:
σd(X) > P +(1 -p)d
d
Xσi(X)
i=1
Hence, there is only one parameter p to tuning. We present the tuning process on the validation set
below, see Tables 8 to 10. Optimal parameter P as well as induced rank d is highlighted.
Table 8: ml-20m, matrix factorization with dropout, hyper parameter tuning by nDCG@100 on
validation dataset and its induced rank .____________________________________________
P	0.9	0.99	0.995	0.996	0.997
induced rank d	10	-200-	-385-	-467-	-602-
nDCG@100~	0.29723	0.39369	0.40045	0.40046	0.39925
Table 9: netflix, matrix factorization with dropout, hyper parameter tuning by nDCG@100 on
validation dataset and its induced rank .____________________________________________
P	0.9	0.99	0.996	0.997	0.998
induced rank d	9	-209-	-524-	-653-	-883-
nDCG@100~	0.26026	0.35462	0.36453	0.36495	0.36406
Table 10: msd, matrix factorization with dropout, hyper parameter tuning by nDCG@100 on
validation dataset and its induced rank .____________________________________________
P	0.99	0.999	0.9995	0.9999	0.99995
induced rank d	-249-	-2054-	3783	11380	19308
nDCG@100~	0.18986	0.28532	0.307	0.32634	0.30995
E.4 Resources
Our code are mainly implemented in Numpy 1.19, Pytorch 1.7.1 on CUDA 11.0. Our experiments
are performed on nodes with two sockets, each containing a 24-core Intel(R) Xeon(R) Platinum 8268
CPU @ 2.90GHz and 4 GeForce RTX 3090 24GB memory GPU.
25