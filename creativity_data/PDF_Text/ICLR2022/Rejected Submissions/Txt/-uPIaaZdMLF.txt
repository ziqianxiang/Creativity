Under review as a conference paper at ICLR 2022
Attentional meta-learners for
few-shot polythetic classification
Anonymous authors
Paper under double-blind review
Ab stract
Polythetic classifications, based on shared patterns of features that need neither be universal
nor constant among members of a class, are common in the natural world and greatly
outnumber monothetic classifications over a set of features. We show that threshold
meta-learners, such as Prototypical Networks, require an embedding dimension that is
exponential in the number of task-relevant features to emulate these functions. In contrast,
attentional classifiers, such as Matching Networks, are polythetic by default and able
to solve these problems with a linear embedding dimension. However, we find that in
the presence of task-irrelevant features, inherent to meta-learning problems, attentional
models are susceptible to misclassification. To address this challenge, we propose a self-
attention feature-selection mechanism that adaptively dilutes non-discriminative features.
We demonstrate the effectiveness of our approach in meta-learning Boolean functions, and
synthetic and real-world few-shot learning tasks.
1	Introduction
Classification meta-learning is typically approached from the perspective of few-shot learning: Can we train
a model that generalises to unseen ‘natural’ classes at test time? For example, in the Omniglot task (Lake
et al., 2011) we have access to a labelled set of handwritten characters during training and we are tasked
with distinguishing new characters, from unseen writing systems, at test time. From this perspective each
example is associated with a consistent class and members of that class share a common set of properties
(e.g. all handwritten characters have the shape of the underlying character class). Alternatively, we may
consider meta-learning over unseen ways of categorising: Can we train a model on character recognition that
generalises to alphabet recognition? or to distinguishing upper from lower case letters? In this setting, features
need to be understood in relation to a given classification: For instance, when tasked with distinguishing
equids (horses, zebras, donkeys) from big cats, the presence of stripes on both zebra and tigers is irrelevant,
and potentially misleading. On the other hand, stripes are the key to distinguishing horses from zebra.
Understanding features in the context of a classification is central to the concepts of monothetic and polythetic
classes recognised in the fields of taxonomy and knowledge organisation. Monothetic classifications are
based on universal attributes: there is at least one necessary and sufficient attribute for class membership.
Polythetic classifications are instead based on combinations of attributes, none of which are sufficient in
isolation to indicate membership and, potentially, none of which are necessary. Carl Linneaus, inventor of
the binomial nomenclature for species and “father of taxonomy,” recognised that natural orders could not
be defined monothetically, lacking features that were unique and constant over families and that, until such
features could be found, such classifications were necessarily Polythetic (Hj0rland, 2017; Stevens, 1998).
Figure 1 illustrates Linnaeus’ system for classifying plants, which relies on polythetic classifications and is
still in use today. Consider, for examPle, that we can distinguish A from B by the number of filaments, but not
1
Under review as a conference paper at ICLR 2022
B from C; we can distinguish B from C based on whether there are split anthers (the ends), but not C from A,
and so on. To recognise classes it is necessary to consider their attributes in the context of other attributes.
Figure 1: Section of Methodus Plantarum Sexualis,
Georg Ehret, illustrating Linnaeus’s Systema Naturae,
1735. Class defining attributes need not be exclusive or
universal, and useful classifications may be contextual.
Threshold functions are a related concept in Boolean
algebra. A threshold function evaluates positively
if a weighted sum of binary inputs crosses some
threshold. Lacking a preferred feature basis, we can
identify monothetic classifications with threshold
functions, as threshold functions that are polythetic
in one basis, such as logical OR(x, y), may be re-
cast in a basis where they are monothetic e.g. bi-
nary OR(x, y) evaluates as the unary MIN(x+y, 1).
We can compare the frequency of monothetic and
polythetic classifications in this general case. The
number of binary inputs of length n is 2n and the
total number of Boolean functions is the number of
binary labellings of these inputs, 22n , whereas the
number of threshold functions grows only singly ex-
ponentially (≤ 2n2), and therefore monothetic clas-
sifications represent a vanishingly small proportion
of the total (see Gruzling (2007); Irmatov (1993)).
This work explores meta-learning for polythetic classification. Specifically, we
•	consider the limitations of widely used threshold classifiers, such as Prototypical Networks (Snell
et al., 2017), and how they are able to learn and approximate non-threshold functions in practice;
•	show that simple alternatives based on attention, such as Matching Networks (Vinyals et al., 2016),
are polythetic by default but susceptible to misclassification due to excessive sensitivity;
•	characterise the challenge of spurious correlations in irrelevant features for attentional classifiers;
•	and propose a simple solution to this challenge that is non-parametric and based on self-attention.
Throughout, we evidence these findings and the effectiveness of our proposals with experiments on synthetic
and real-world few-shot learning tasks.
2	Background
Problem formulation. We are interested in few-shot classification: provided with a small number of
labelled points S = {xi , yi}i∈IS, the support set, with feature vectors xi ∈ Rn and labels yi ∈ {1, . . . , K},
we want to predict the labels of the query set Q = {xj }j∈IQ . Sk denotes the set of support elements with
label k and I? is the index set of the subscript e.g. IS is the index set of the support. The label space is
arbitrary and potentially unique to a task (also referred to as an episode) — both the number of classes, k, and
assigned labels may vary over tasks — and, importantly, examples that share labels under one categorisation
will not necessarily share labels under another. We will refer to classification functions over n features simply
as classifications, and in meta-learning we are often interested in classifications that only depend on some
features, α, and not the remainder, β = n - α.
Classifiers. Deep neural models for problems of this kind are usually equipped with either a threshold
classifier or an attentional classifier. Threshold classifiers are based, as the name suggests, on using thresholds
to partition a space into regions associated with each class. Prototypical Networks (Snell et al., 2017) are an
example of such a model. This model embeds examples using a learned neural function, fφ, finds the average
embedding for each class in the support, ck, and classifies queries based on their proximity (measured with a
2
Under review as a conference paper at ICLR 2022
Figure 2: Confidence heatmaps and decision boundaries of prototype and attention classifiers on 2-
variable Boolean functions. The attention classifier shown uses a temperature of 1, lower temperatures
‘harden’ the classification and produce decision boundaries more closely aligned with the axes (see
Appendix B). As XOR(x, y) is not a threshold function, simple prototypes fail to produce a correct
classification scheme, in this case the prototypes are equal (= (0, 0)) and there is no decision boundary.
distance function d) to these class prototypes:
CLL X 断(X)	∙	Plh(U	=	k|x)	= eχpJdfφ(X), Ck))
ck=∣Sk | 2 fφ(Xi)	;	pφ(y	=	k1x)=	PkO exp(-d(fφ(x), Ck 吊	⑴
i∈ISk
The key advantage of such an approach is that salient class features are preserved when forming the prototype
while irrelevant aspects of particular examples are washed out. Attentional classifiers instead use a similarity
function to directly compare queries with each example in the support. For example, Matching Networks
(Vinyals et al., 2016) also learn embedding functions, but each query is compared with every member of
the support to weight a sum over their labels, which We can write simply as y = Piiels a(X, Xi)yi. In the
popular terminology of transformers we may write the embedded queries as Q, the embedded support as keys
K and their labels as values V, and dot-product attention classification with temperature τ -1 as
DotAttn(Q, K, V,τ) = softmax(τQKT)V ∈ RlQl×k.	(2)
Attentional classifiers are more sensitive to variations within a class at the cost of additional computation.
Boolean tasks. In comparing these classifiers we make repeated use of tasks based on Boolean functions,
and on exclusive-OR (XOR) in particular. For a binary feature vector X ∈ {-1, 1}n, the number χA(X) =
i∈A xi is the parity function or exclusive-or (XOR) over the bits (xi)i∈A. We write a parity function of α
bits XORα. The set of parity functions over n bits form a linearly independent basis (O’Donnell, 2014) and,
as such, being able to model the partition functions guarantees that one can model any other Boolean function
over that domain. Put another way, the decision boundaries of XORα are at least as complicated as those for
any other α-variable Boolean function: there is one between every possible pair of feature vectors. For these
reasons, XOR is our polythetic function of choice in derivations, examples, and experiments.
3 Challenges in meta-learning polythetic classifications
Threshold and attentional classifiers have their own strengths and, in meta-learning polythetic classifications,
their own weaknesses. Threshold classifiers are insufficiently flexible and attentional classifiers are prone to
misclassification.
Threshold classifiers. Figure 2 shows the decision boundaries formed by a prototypical threshold classifier
and an attentional classifier for a selection of 2-variable Boolean functions, highlighting the problem with
using threshold classifiers for polythetic classification: logical XOR(x, y) is not a threshold function and the
prototypes fail to produce a useful decision boundary. This is the perceptron problem identified by Minsky &
3
Under review as a conference paper at ICLR 2022
Prototypical networks accuracy for XOR2
,94
90
0
,91
0.90 0
,90 0
,91
,95 0,94 0.
95
0
,94
0,94 0
,94 0
,98 0,98 0,
98
0
,97
0,98 0
,98 0
,98
,99 0,99 0,
1,00 0,99 1
,00 1
99 0
,00 0,
,99 0,99 0
99 0
,99 1
,00 1
-1,00 1,00 1,00 1,00 1,00 1,00 1,00 1,00 1,00 1,00
1	4	9	16	25	36	49	64	81	100
Embedding dimension
,99 0
,99
,00 1
,00
Figure 3:	A non-threshold function of 2-variables, the pseudo-variable solution, and Prototypical
Network performance on the XOR2 problem. (a) XOR(x, y), which does not have a threshold solution
in 2 dimensions. (b) Appending the pseudo-variable XOR(x, y) gives a 3-dimensional embedding in
which all 2-variable Boolean functions have threshold solutions. XOR(x, y) is a pseudo-variable in
that it is determined by the other variables and cannot freely vary, for example the hatched circle at
(1, 0, 0) cannot occur. Right: Accuracy of prototypical networks for the XOR2 problem over sequence
length and embedding dimension. Mean over 1000 tasks, |S| = 40. See Appendix F for details.
Papert (1969). However, deep networks using threshold classifiers can learn XOR. This is possible because
the network can learn additional pseudo-features for the non-threshold functions it observes. Figure 3 shows
an example for 2-variables: one can embed the corners of the square at the corners of a tetrahedron with
coordinates x, y, XOR(x, y) and produce linear thresholds solutions for every 2-variable Boolean function.
There are two problems with this approach, both of which are exacerbated in the meta-learning context: i)
the required number of pseudo-variables grows as ⑴ 〜 O(na) to account for all combinations of α active
components, and ii) the method does not generalise to unseen non-threshold functions (see Appendix A). The
right plot in Figure 3 demonstrates these shortcomings for XOR2 : the required number of pseudo-variables is
C)〜O(n2) and initially we find that a quadratic embedding is able to maintain performance, but for longer
sequences the number of threshold-functions unseen in training grows and performance degrades.
Attentional classifiers. Attentional classifiers avoid these problems — the required embedding dimension
is linear in the number of features and the classifier generalises to unseen classifications — but suffer from
over-sensitivity to irrelevant features. This results in misclassification, which we quantify in the case of
Boolean functions to understand the scaling properties of this problem generally.
Consider classifications over binary feature vectors x ∈ {-1, 1}n, with the class determined by XORα over
α elements with the remaining β = n - α being irrelevant. Assume each of the 2α variations of the active
elements are present with equal frequency, r, for a support set S of size |S| = r2α, and that the remaining
β elements follow a Bernoulli distribution with probability p. Using an attention classifier of the form
y = Pi∈s softmaxi (a(X, Xi)) yi, where a(x, Xi) is a measure of the similarity of X and xi, how likely is it
that we misclassify a query drawn from the same distribution as S?
Without loss of generality, we can focus on the positive examples (with label = 1) for which a positive output
gives the correct classification. Using dot-product attention, the mean and variance of the classifier output are,
with P = p2 + (1 — p)2 and q= 1 — p,
μ =	r(e — e-1)α	](pe + qe-1)"	= r(e — e-1)α[cβ],	(3)
σ2 = r(e2 +	e-2)α	](pe2	+ qe-2^ —	Qe + qe-1) "	= r(e2 + e-2)α[dβ	— c2β],	(4)
4
Under review as a conference paper at ICLR 2022
introducing c and d for compactness. The mean is positive, as desired, but we are interested in the rate of
misclassification, which may be interpreted using the scale-free and dimensionless coefficient-of-variation (the
ratio of the standard deviation to the mean) where greater variation indicates a greater rate of misclassification.
From Equations 3 and 4, we have
(5)
Starting with the leftmost term, increasing the number of repetitions, r, reduces the relative variability. This
aligns with intuition and limits, where an empty support set, r = 0, provides no basis on which to make
predictions and at the other extreme, r 2β, the support set is likely to span the input domain reducing
classification to look-up. The term raised to α is approximately 3.2, and so the variability increases with the
number of active elements. An intuitive explanation is that the number of immediate neighbours of each
point grows as αn = α and this reduces the confidence with which the point is classified, so the barrier to
misclassification is reduced. Finally, d > c2 and so the rightmost term is positive and grows exponentially in
β, meaning that misclassification increases with the number of irrelevant features. A full derivation, including
alternative attention functions, is provided in Appendix C.
The problem of misclassification due to over-sensitivity in attentional classifiers was recognised in the work
of Luong et al. (2015) on sequence processing. There the problem was addressed by attending only to a
subset of elements within some distance of the target position. However, sets do not have such an ordering
and so we instead propose a feature-selection method to resolve the problem more generally.
4 Attentional feature selection
A key challenge of meta-learning is that not all features are relevant in all tasks and that the support is unlikely
to span the input domain. The model must choose, using incomplete information, what to focus on and what
to ignore by detecting the salient features within and between classes. For monothetic classifications this is
straightforward: by definition, averaging highlights necessary features whilst diminishing irrelevant features.
Prototype methods rely on this process. In the polythetic case, attentional classifiers have the advantage
of being able to learn non-threshold functions without the need for pseudo-variables but do not benefit, as
prototypes do, from ‘washing-out’ irrelevant features through averaging. Indeed, attentional classifiers are
susceptible even in the monothetic setting to misclassifying on the basis of closely matching features that
Figure 4:	Feature values and attention coefficients during the feature-selection self-attention (n =
{0, 5, 10, 25}) within a class of XOR3 . Nodes depict examples of the support set: the colour of the
left halves represents the active features; the right halves represents the magnitude of the irrelevant
features. Edge width and opacity indicate the attention strength between a pair of nodes. The red,
green, blue and white groups, different variants, automatically segregate which preserves their active
features while the irrelevant features converge, as shown in the feature scores beneath each plot. In
this way, the active features identify themselves.
5
Under review as a conference paper at ICLR 2022
are not relevant to the problem (putting a zebra with the big-cats because its stripes match those of a tiger
in the support set, for example). Misclassification occurs when irrelevant features overwhelm the signal
from the active elements. As this is a problem of highlighting the salient patterns within a set, we propose a
self-attention based mechanism for feature selection, presented in Algorithm 1 and illustrated in Figure 4.
Intuitively, the process exploits the over-representation of patterns within features that are relevant to the
classification as compared to patterns within the irrelevant features. We first standardise the features to prevent
those common to the entire support, which are not discriminating, from dominating (Line 1) and stabilise, , to
prevent weakly activated features from being excessively scaled-up. We then repeatedly self-attend within each
separate class k of the support set, using dot product attention with scale T, Xk — DOtAttn(Xk, Xk, Xk, τ).
Self-attention maps elements of a set of vectors to the interior of their convex hull. If every member of a class
has some feature in common, the convex hull in that dimension is a point and the features do not change. In
the polythetic case it is patterns of features that matter, and by attending more strongly between elements of
the support set with such feature-patterns, these too are preserved. Figure 4, for example, shows polythetic
variations within a class of XOR3 with three active and three irrelevant features (α = β = 3). The patterns
in the active features self-reinforce, forming cliques of strongly connected elements, whilst the irrelevant
features decay. Finally, features are scored by their dispersion over the support set (Line 5) which indicates
how well they have been preserved through the self-attention iterations, and thus how relevant they are.
Algorithm 1: Self-attention feature scoring. Scores can be used for rescaling or masking. Note that the
z-normalisation is over the entire support set whilst the self-attention is within classes. The choice of
dispersion measure is of secondary importance and discussed in the main text.
Input : Support set S = {xi, yi}i∈IS with class labels yi ∈ {1, . . . , K} and features xi ∈ RF , Sk denoting the
subset of S containing all samples with yi = k and Xk ∈ RISk l×F an arbitrarily ordered matrix of feature
vectors belonging to Sk; small numerical constant, ; attention temperature, τ-1; repetitions, R.
Output : Feature scores, f ∈ RF.
ι Xi J (Xi — μχ)∕(σχ + E) ;	// standardise
2	repeat R times
3	for kJ 1 to K do	// for each class
4	IXk J softmax(τXkXT)Xk ;	// softmax is row-wise
5	f J dispersion({Xi}i∈Si) ;	// mean-absolute-deviation, std. dev etc.
The scores can be used directly to rescale features across the support and query sets before applying the
classifier, as in Figure 5, or in top-k selection. We focus on rescaling as the method that makes the fewest
assumptions about the underlying classification, but using top-k is highly effective when the number of active
elements is known, as shown in Appendix D.
5	Experiments
We compare the proposed method (FS) with Prototypical Networks (PN) (Snell et al., 2017), a threshold
classifier, and Matching Networks (MN) (Vinyals et al., 2016), an attentional classifier without feature-
selection, in a sequence of increasingly complex synthetic and real-world few-shot learning problems. As
our approach is non-parametric and operates directly on high-level features, it is agnostic to the choice of
feature extractor, and we are free to choose as appropriate e.g. a convolutional neural network for images or
a multi-layer perceptron for tabular data, and in all experiments we use the same embedding model for all
methods (see experimental details in Appendix F).
Binary strings. We consider meta-learning Boolean functions of n = α + β variables. Labels for inputs
x ∈ {—1, 1}n are generated by computing the XOR of a random subset of components of size α. Each
variation in the active elements occurs 5 times in the support, |S| = 5 ∙ 2α. The subset of active components
6
Under review as a conference paper at ICLR 2022
-1.0
with soft feature selection
0.9
-0.8
-0.7
-0.6
0.5
123456789 10
1.0
0.9
0.8
0.7
0.6
Variant examples, r
∆accuracy
0.5
Figure 5: Attention classification of the XOR4 problem over variant frequency, r, and number
of inactive components β. Increasing r assists feature selection, in agreement with the derived
misclassification distribution. Soft feature-selection rescales features according to their scores, as
determined by the proposed self-attention procedure. This greatly improves performance even at low
repetitions, for example at 2 repetitions and 3 inactive components the change in accuracy is +38pp.
Neither method is effective at high β with low r .
Table 1: Binary strings. Accuracy by embedding dimension for sequences of length n = 5 and n = 10.
Mean and standard error calculated over 1000 tasks.
Model	Emb.	n=5			n=10		
		XOR2	XOR3	XOR4	XOR2	XOR3	XOR4
	1	57.6 ± 0.5	55.3 ± 0.5	60.8 ± 0.7	51.7 ± 0.4	50.2 ± 0.2	50.1 ± 0.2
	n	73.6 ± 0.5	70.3 ± 0.7	91.4 ± 0.4	56.7 ± 0.4	50.1 ± 0.2	50.4 ± 0.2
PN	n2	90.4 ± 0.3	77.8 ± 0.7	100.0 ± 0.0	62.1 ± 0.4	50.3 ± 0.2	50.6 ± 0.2
FS+MN	n	99.6 ± 0.2	100.0 ± 0.0	100.0 ± 0.0	75.9 ± 1.3	82.6 ± 1.1	96.3 ± 0.5
is unknown to the meta-learner. Appendix I shows a concrete example of an XOR2 task. Table 1 summarises
the performances of Prototypical Networks and our approach. PN accuracy decreases sharply with sequence
length n and the number of embedding units required to effectively solve this problem grows rapidly with n,
as shown previously in Figure 3. This suggests that PN are indeed learning pseudo-variables and demonstrates
the limitations of threshold classifiers in solving polythetic problems.
Polythetic MNIST. We evaluate the ability of the models to jointly extract high-level features and identify
polythetic patterns. We build tasks (episodes) using MNIST digits (LeCun et al., 2010), where an example
consists of 4 coloured digits (RGB). An example task and further details are provided in Appendix J. For
monothetic tasks, a single high-level feature (e.g. colour of the top-right digit) distinguishes classes. For
polythetic tasks, class membership derives from XOR interactions over a subset of features, and the remainder
are problem-irrelevant. Table 2 shows the performances on three versions of the polythetic MNIST dataset:
clean (excluding non-discriminative digits), colourless (task-irrelevant digits but no colour), and full (both task-
irrelevant digits and colour). The models are trained on monothetic tasks and evaluated both on monothetic
and polythetic tasks. Protonets excel at identifying monothetic features and ignoring non-discriminative
features, but have a close to random performance on polythetic tasks. Conversely, matching networks, which
are polythetic classifiers by default, are highly sensitive to task-irrelevant features. The proposed approach
(FS) can simultaneously detect salient features and perform polythetic classifications. Furthermore, as shown
in Figure 6, we found our classifier to be robust to the rate of polythetic tasks seen during training in a second
experiment.
Omniglot. The Omniglot dataset (Lake et al., 2011) consists of handwritten characters from 50 writing
systems with 20 hand drawn examples of each character. Training tasks are formed using examples from 30
of the alphabets and test tasks draw from the other 20. We compare our method to PN, MN, Infinite Mixture
7
Under review as a conference paper at ICLR 2022
Table 2: Polythetic MNIST. Evaluation accuracy on monothetic and polythetic tasks in three settings.
Mean and standard error calculated over 1000 tasks.
Model	Clean	Colourless	Full Monothetic Polythetic Monothetic Polythetic Monothetic Polythetic
PN MN	97.9 ± 0.1	50.6 ± 0.3	92.8 ± 0.3	49.9 ± 0.3	94.5 ± 0.3	49.8 ± 0.2 79.6 ± 0.6	57.6 ± 0.4	69.7 ± 0.4	61.0 ± 0.5	70.1 ± 0.7	56.6 ± 0.5
FS+MN	96.8 ± 0.1	98.3 ± 0.0 94.5 ± 0.2 98.0 ± 0.0 75.0 ± 0.7 60.4 ± 0.7
Figure 6: Polythetic MNIST (colourless) by polythetic proportion
during training. FS matches or outperforms the other models at
all training proportions and is far less affected by the training mix.
Mean and standard deviation over 1000 tasks at each proportion.
Table 3: Omniglot. 20-way, 5-shot
characters; 3-way alphabets. Mean
and standard error on 1000 tasks.
	Characters	Alphabets
PN	98.6 ± 0.0	83.4 ± 0.3
MN	91.1 ±0.1	78.4 ± 0.3
FS+MN	96.2 ± 0.0	94.2 ± 0.2
IMP	98.6 ± 0.0	96.0 ± 0.2
MAML	94.0 ± 0.1	89.9 ± 0.3
MN*	97.9 ± 0.1	81.3 ± 0.3
NN*	98.3 ± 0.0	95.7 ± 0.3
FS+MN*	98.1 ± 0.0	96.0 ± 0.2
FS+NN*	98.3 ± 0.0	96.0 ± 0.2
Prototypes (IMP) (Allen et al., 2019), and MAML (Finn et al., 2017) with a threshold classifier (Triantafillou
et al., 2020). We train the models for character recognition, and additionally evaluate performance on
3-way alphabet recognition (inherently polythetic). Our approach can be used in conjunction with most
few-shot learning approaches - We specifically apply FS prior to MN and Single-nearest-neighbours (NN),
corresponding to MN with softmax converging to argmax (see Appendix B). Table 3 shows the results of
this experiment. The end-to-end trained model (FS+MN) is competitive with PN and IMP, while performing
better than MN and MAML in character recognition. In alphabet recognition, FS+MN performs better
than other methods, while being competitive with IMP. We further evaluate the performance when using
a pre-trained feature extractor (methods marked with *), obtained by training a PN threshold classifier on
character recognition. FS+MN* improves over MN* in both tasks. Compared to PN, the accuracy of FS+MN*
and FS+NN* in character recognition is reduced by at most 0.5pp; yet improved in alphabet recognition by
12.6pp, while performing similarly to the more complex IMP.
TieredImageNet. TieredImageNet (Ren et al., 2018) is a subset of ILSVRC-12 (Russakovsky et al., 2015)
with polythetic characteristics, with classes grouped into categories corresponding to higher-level nodes in
the ImageNet hierarchy. There are 34 categories of 10 to 30 classes each. We compare our method (FS) to
Table 4: TieredImageNet. Model accu-	Table 5: TieredImageNet. Head-to-head comparison
racy by categories (C) and groups (G)	over 500 tasks by categories (C) and subgroups (G).
over 500 tasks.	Bold indicates significance at the p < 0.001 level.
G	C=2	C = 4	C=8	C = 2	C = 4	C=8
FS	83.5 ± 0.3	66.4 ± 0.3	48.9 ± 0.2	G X Y	X/Y	(tie)	X/Y (tie)	X / Y (tie)
			∕1Q ClCC			
5 MN	82.7 ± 0.4	65.4 ± 0.3	48.3 ± 0.2			
PN	81.4 ± 0.3	64.6 ± 0.3	49.4 ± 0.1	5 FS MN 230 / 162 (108)	329/ 101	(70)	343 / 113	(44)
FS	83.6 ± 0.3	65.8 ± 0.3	48.7 ± 0.1	5 FS PN 319 / 136	(45)	339 / 142 (19)	239/247 (14)
						
10 MN	82.9 ± 0.3	64.9 ± 0.3	48.0 ± 0.1	10 FS MN 258 / 185	(57)	357 / 108 (35)	413/ 68 (19)
PN	81.1 ± 0.3	63.3 ± 0.3	48.2 ± 0.1	10 FS PN 374 / 101	(25)	396 / 99	(5)	296/ 191	(13)
8
Under review as a conference paper at ICLR 2022
PN and MN classifiers. We use a publicly available pre-trained ResNet-12 (Zhang et al., 2020), pre-trained
using the training classes in TieredImageNet, as the feature extractor for all models. Table 4 presents the
aggregate accuracy while Table 5 shows the head-to-head results of this experiment. FS leads to significant
improvements in performance (except C=8/G=5, where the difference between PN and FS is not significant)
in this full scale, naturally polythetic problem, particularly in the “more polythetic” case with 10 subgroups.
6	Related work
We characterise meta-learning approaches for few-shot classification. In addition to evaluating their ability to
generalise to unseen classes, we investigate how well they can adapt to polythetic tasks (generalisation to
unseen ways of cateogorising). Adaptability in few-shot settings has been studied through different paradigms
such as fast weights (Ba et al., 2016), learnable plasticity (Miconi et al., 2018) and meta-learning. Recent
work on meta-learning for few-shot classification includes approaches that are able to quickly adapt through
various mechanisms such as recurrent architectures (Mishra et al., 2018; Santoro et al., 2016) for learning
parameter updates (Ravi & Larochelle, 2017). Other more general optimisation-based approaches (Finn
et al., 2017; Nichol et al., 2018; Rusu et al., 2019), tackle these tasks by explicitly optimising the model’s
parameters. These, however, are typically model-agnostic and commonly used in conjunction with threshold
classifiers (Triantafillou et al., 2020), inheriting their limitations in a polythetic scenarios.
Our work aligns more closely with metric-learning approaches for few-shot classification (Chen et al., 2019)
that apply distance functions between queries and the support in a common embedding space (Allen et al.,
2019; Oreshkin et al., 2018; Snell et al., 2017; Sung et al., 2018; Vinyals et al., 2016; Zhang et al., 2020).
Methods that construct class-wise prototypes from the support (Allen et al., 2019; Ren et al., 2018; Snell
et al., 2017) can successfully tackle monothetic tasks, but can struggle with task-adaptiveness in a polythetic
context. Attentional meta-classifiers (Hou et al., 2019; Jiang et al., 2020; Kim et al., 2019; Vinyals et al.,
2016) adapt to polythetic tasks but lack crucial mechanisms for focusing exclusively on relevant features.
Attending over datapoints has been considered previously. Luong et al. (2015) introduced dot-product
attention in the context of attending over sequences. Vinyals et al. (2016) considered y = Pi∈∕, a(X, Xi)yi
and provided the conditions under which such a model carries out kernel density estimation or k-nearest-
neighbours classification. Vaswani et al. (2017) used explicit scaling in hidden attention layers, but scaling a
classifying softmax by a temperature parameter dates to the work of Boltzmann and later Gibbs (1902). Plotz
& Roth (2018) note that their neural-nearest-neighbours (N3) block recovers a soft-attention weighting when
the number of neighbours is set to 1, and deploy their model on an outlier-detection set-reasoning task.
7	Conclusion
In this work we have articulated the difference between monothetic and polythetic classifications and
considered the limitations of standard meta-learning classifiers in the polythetic case. We have shown that
threshold classifiers require an embedding space that is exponential in the number of active features and
that attentional classifiers are overly sensitive and susceptible to misclassification. To address this, we have
proposed an attention based method for feature-selection and demonstrated the effectiveness of our approach
in several synthetic and real-world few-shot learning problems. Our approach is simple and can be used
in conjunction with most few-shot meta-learners. We expect polythetic meta-learners to find real-world
application in domains where data is typically scarce and complex, such as healthcare or bioinformatics. For
example, We envision a use in classifying rare diseases - there are around 7000 rare diseases, affecting 〜1/17
of the worldwide population — from DNA sequences, where mutations often lead to different phenotypes. In
such scenarios, few-shot learning approaches able to generalise over unseen combinations of mutations (i.e.
ways of categorising), in a similar vein to our binary strings experiment, may lead to better performance in
diagnosing rare diseases and shed new insights into their molecular mechanisms.
9
Under review as a conference paper at ICLR 2022
Reproducib ility s tatement
In Section 3 we discuss the challenges of polythetic classification and the limitations of current work on
meta-classifiers. Our proposed self-attention feature scoring algorithm is described in detail in Section 4, and
in particular, Algorithm 1 (and Appendix H). To ensure the work is readily reproducible, besides descriptions
of the experimental setup provided in Section 5 and the supplementary material (Appendices F, G, I, J) - we
also provide code used for producing the results in the paper.
References
Kelsey R. Allen, Evan Shelhamer, Hanul Shin, and Joshua B. Tenenbaum. Infinite mixture prototypes for
few-shot learning. In PrOceedings of the 36th International COnference on Machine Learning,ICmL 2019,
9-15 JUne 2019, Long Beach, California, USA, volume 97 of Proceedings OfMachine Learning Research,
pp. 232-241. PMLR, 2019.
Jimmy Ba, Geoffrey E. Hinton, Volodymyr Mnih, Joel Z. Leibo, and Catalin Ionescu. Using fast weights to
attend to the recent past. In NIPS, pp. 43314339, 2016.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at
few-shot classification. In 7th International COnference on Learning RePresentations, ICLR 2019, NeW
Orleans,LA, USA, May 6-9, 2019, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Doina Precup and Yee Whye Teh (eds.), PrOceedingS of the 34th IntematiOnal COnference
on Machine Learning, volume 70 of PrOceedingS OfMachine Learning ReSearch, pp. 1126-1135. PMLR,
06-11 Aug 2017.
J Willard Gibbs. Elementary PrinciPleS in StatiStical mechanics. Charles Scribner,s Sons, 1902.
Nicolle Gruzling. Linear separability of the vertices of an n-dimensional hypercube. 2007. doi: 10.24124/
2007/bpgub464.
Birger Hj0rland. Classification. Knowledge Organization, 44⑵:97-128, 2017. URL http://www.isko.
org/cyclo/classification.
Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Cross attention network for
few-shot classification. In AdVanceS in Neural InfOrmatiOn PrOceSSing SyStemS 32: Annual COnference
on Neural InfOrmatiOn PrOceSSing SyStemS 2019, NeUrIPS 2019, December 8-14, 2019, VncOUVer, BC,
Canada, pp. 40054016, 2019.
Anwar A Irmatov. On the number of threshold functions. DiScrete MathematicS and APPlications, 3(4):
429^32, 1993.
Zihang Jiang, Bingyi Kang, Kuangqi Zhou, and Jiashi Feng. Few-shot classification via adaptive attention.
CoRR, abs/2008.02465, 2020.
Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals,
and Yee Whye Teh. Attentive neural processes. In International COnference on Learning RePreSentations,
2019. URL https://openreview.net/forum?id=SkE6PjC9KX.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple
visual concepts. In PrOceedingS of the annual meeting of the cognitive Science society, volume 33, 2011.
10
Under review as a conference paper at ICLR 2022
Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT LabS [Online].
Available: http:〃yann.lecun.com/exdb/mnist, 2, 2010.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural ma-
chine translation. In Llu^s Marquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (eds.),
Proceedings of the 2015 Conference on EmpiricaI MethOdS in NatUral LangUage PrOceSsing, EMNLP
2015, LiSbon, PortUgal, September 17-21, 2015, pp. 1412-1421. The Association for Computational Lin-
guistics, 2015. doi:10.18653/v1/d15-1166. URL https://doi.org/10.18653/v1/d15-1166.
Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic neural networks
with backpropagation. In Jennifer Dy and Andreas Krause (eds.), PrOCeedingS of the 35th International
COnferenCe on MaChine Learning, volume 80 of Proceedings of MaChine Learning ReSearch, pp. 3559-
3568. PMLR, 10-15 Jul 2018.
Marvin Minsky and Seymour Papert. Perceptrons. 1969.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In
International COnferenCe on Learning RePreSentations, 2018.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms, 2018.
Ryan O,Donnell. AnalySiS OfBoolean functions. Cambridge University Press, 2014.
Boris N. Oreshkin, Pau Rodriguez L6pez, and Alexandre Lacoste. TADAM: task dependent adaptive
metric for improved few-shot learning. In AdVanCeS in NeUral InfOrmatiOn Processing SyStemS 31:
AnnUal COnferenCe on Neural InfOrmatiOn PrOCeSSing SyStemS 2018, NeUrIPS 2018, December 3-8,2018,
MOntreal, Canada, pp. 719-729, 2018.
Tobias Plotz and Stefan Roth. Neural nearest neighbors networks. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), AdVanceS in NeUral InfOrmatiOn PrOCeSSing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/
2018/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf.
Sachin Ravi and H. Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo
Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classification. In 6th
International COnferenCe on Learning RePreSentations, ICLR 2018, VnCOUVer, BC, Canada, April 30 -
May 3, 2018, COnferenCe TraCk Proceedings, 2018.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale
visual recognition challenge. International JOUrnaI of COmPUter ViSion, 115(3):211-252, Dec 2015. ISSN
1573-1405.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and
Raia Hadsell. Meta-learning with latent embedding optimization. In International COnference on Learning
RePreSentations, 2019.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning
with memory-augmented neural networks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.),
Proceedings of The 33rd International COnferenCe on MaChine Learning, volume 48 of PrOCeedingS of
MaChine Learning ReSearch, pp. 1842-1850, New York, New York, USA, 20-22 Jun 2016. PMLR.
11
Under review as a conference paper at ICLR 2022
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learn-
ing. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), AdvanceS in NeUral Information Processing Systems, volume 30. Curran
Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2 017/file/
cb8da6767461f2812ae4290eac7cbc42-Paper.pdf.
P.F. Stevens. Linnaeus, Carl von (1707-78).	1998. doi: 10.4324/9780415249126-Q059-1.
URL	https://www.rep.routledge.com/articles/biographical/
linnaeus-carl-von- 1707-78/v-1/sections/the-natural-method.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning
to compare: Relation network for few-shot learning. In 2018 IEEE/CVF Conference on ComPUter ViSion
and Pattern Recognition, pp. 1199-1208, 2018.
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,
Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of
datasets for learning to learn from few examples. In International ConferenCe on Learning RePreSentations,
2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕 ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), AdVanceS in NeUral Information ProCeSSing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), AdVanCeS in NeUral Information Processing Systems, volume 29. Curran
Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2 016/file/
90e1357833654983612fb05e3ec9148c-Paper.pdf.
Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with
differentiable earth mover,s distance and structured classifiers. In IEEE/CVF Conference on ComPUter
ViSion and Pattern ReCognition (CVPR), June 2020.
12
Under review as a conference paper at ICLR 2022
A Partitioned XOR performances
We hypothesise that, in order to solve polythetic tasks, prototypical networks need to create pseudo-variables
in the embedding space (e.g. XOR for each pair of components for binary strings tasks). To test this, we train
a prototypical network on binary strings tasks (where pairs of active components are always chosen from the
5 first components) and evaluate the performance on unseen combinations of components (i.e. combinations
of the 5 first components not seen during training). Table 6 shows the accuracies for sequences of length
n = 5 + β (where β is the number of variables that are always inactive). We attribute the better-than-chance
performances (i.e. for 100-dimensional embeddings) to the high-dimensionality of the embeddings - by
chance, large numbers of non-linear features (e.g. output by a random feature extractor) will include some
features that are useful for the task of interest (i.e. similar to extreme learning machines). Overall, these
results highlight the inability of prototypical networks to generalise to unseen combinations and support our
hypothesis.
Table 6: Accuracy of prototypical networks on unseen non-threshold functions by embedding di-
mension. We use sequences of length n = 5 + β, where β is the number of components that are
always inactive. The labels are derived from XOR2 combinations over the first 5 components and
the sets of combinations seen at train and test times are disjoint. In other words, the combinations
of variables (0, 1), (0, 2), (1, 3), (2, 4), (3, 4) are only seen active at train time, whereas, the combina-
tions (0, 3), (0, 4), (1, 2), (1, 4), (2, 3) are only seen at test time, with r = 5 repetitions for each XOR
combination. Each of the 5 first variables has the same expected frequency of being active at train and
test times. The inability to generalise in this scenario suggests that protonets need pseudo-variables
(i.e. XOR functions applied to each pair of components) to solve the binary strings task.
Emb.	β=0	β = 1	β=2
1	48.741 ± 0.276	48.855 ± 0.282	49.558 ± 0.284
10	50.350 ± 0.282	49.397 ± 0.286	49.479 ± 0.265
20	50.705 ± 0.271	51.057 ± 0.247	50.144 ± 0.255
100	52.246 ± 0.256	52.390 ± 0.265	52.573 ± 0.258
B Temperature in attention classification
The softmax in attention mechanisms permits a temperature scaling that interpolates between argmax and
uniform (and argmin.) This is controlled by T = β, as
softmaxi (x, β)
exp(βxi)
Pj exp(βxj)
with softmax converging to argmax as β → ∞, and to uniform (i.e. all elements equal to the reciprocal of the
length of the vector) as β → 0.
For attention classifiers, a decrease in temperature increases the model confidence and can cause decision
boundaries to move. As β → ∞ and the softmax converges to argmax, the classifier tends to the single
nearest neighbour classification scheme; for β = 0 the classifier returns the support set class balance. For
Boolean functions, changes in temperature effect the degree to which decision boundaries are axis-aligned.
For example, Figure 2 shows the decision boundary for f(x, y) = AND(x, y) using a softmax temperature of
13
Under review as a conference paper at ICLR 2022
T =1	T =1/2	T =1/3	T =1/5
Figure 7: Changes in confidence and decision boundary of the attention classifier with temperature for
AND(x, y).
1 at y = - 2 ln (tanhx), which We derive as
p(class 1) = p(class 0)
exp (β(x + y)) _ exp (β(-x - y)) +exp (β(-x + y)) +exp (β(x - y))
P exp(...)	P exp(...)
exp β(x +y) - exp -β(x +y) = exp β(-x +y) +exp β(x - y)
sinh(β(x + y)) = cosh(β(x - y))
sinh(βx) cosh(βy) + sinh(βy) = cosh(βx) cosh(βy) - sinh(βy)
tanh(βx) = COsMey)+ sinMey)
(βx)= cosh(βy) - sinh(βy)
tanh(βx) = exp(-2βy)
y = - 2β ln (tanh(βx))∙
The effect of decreasing the temperature on the decision boundary is shown in Figure 7.
C Misclassification distribution full derivation
We first present a more detailed derivation for the case of dot-product attention. We will make repeated use of
the Binomial theorem
(x+y)n = X nkxn-kyk = X nkxkyn-k
k=0	k=0
(6)
Recall that we consider classifications over binary feature vectors x ∈ {-1,1}n, with the class determined
by XORα over α elements with the remaining β = n - α being irrelevant. Assume each of the 2α variations
of the active elements are present with equal frequency, r, for a support set S of size |S| = r2α, and that the
remaining β elements follow a Bernoulli distribution with probability p. There are αδ strings over the active
elements that differ from a given example in δ positions. XOR flips the classification with each difference, so
if δ is even the class is the same, if δ is odd the class is different (parity).
14
Under review as a conference paper at ICLR 2022
The class is determined by the sign of the sum of contributions at an even distance subtract those at an odd
distance. Putting those into a single sum we have
class = sign (Xr(-1)'(；) exp(α - 2δ)).
(7)
We can factor out the number of repetitions, r, and as it is positive it doesn’t change the sign. We can then
rearrange to match the Binomial theorem form and recover the form in the main text:
sign
+.
(8)
Next We give the Binomial distribution of the irrelevant feature contribution as 2B(β,p) - β with P =
p2 - (1 - p)2 = 2p2 - 2p + 1. Something that is important to the behaviour of the mean and variance later,
and breaks the usual symmetry of p and q =(1 — p), is that P ≥ 0.5 and is quadratic in p defined by the
three points (p,/)={(0,1), (0.5,0.5), (1,1)}. q has the usual definition 1 - p), so q ≤ 0.5 and so on. The
contribution at a difference δ is then X(δ)〜exp (α - 2δ + 2B(β,历-β). The expectation is computed in
the usual way
β
E[X(δ)] =	P(X = xi)xi =	P(B(β,pq) = b) exp (α - 2δ+ 2b -β)
i	b=0
= exp (α - 2δ) X βb pqbqq(β-b) exp (2b - β)
= exp (α - 2δ) X b (pqe)b(qqe-1)β-b = exp (α - 2δ) pqe + qqe-1
b=0
Finding the variance in the traditional way, Var[X(δ)] = E[X(δ)2] - E[X(δ)]2, first E[X(δ)2] following the
derivation for E[X(δ)]:
β
E[X(δ)2] =	P(X = xi)xi2 =	P(B(β, pq) = b) exp (2α -4δ+4b - 2β)
i
= exp (2α - 4δ)
= exp (2α - 4δ)
(9)
(10)
(11)
From this we can write the variance
Var[X(δ)] = exp (2α - 4δ)	pqe2 + qqe-2	- pqe + qqe-1	.	(12)
Next we want to find the expectation of the sum of the contributions at each difference δ. As pointed out there
are (a)many strings at a difference of δ, We apply E[Pi Xi] = Pi E[Xi] and Var[X - Y] = Var[X]+ Var[Y],
15
Under review as a conference paper at ICLR 2022
and, remembering to change the sign with each increase in δ,
and
E hPi∈S(-1)δiX(δi)i =Xα r(-1)δαδE[X(δ)]
δ=0	δ
=r (Pe2 + qe-2)β X (”" G j
δ=0
=r (Pe + 7e-2Y3 (e — e-1) ,
Var [Pi∈s( —1产X®] = Var [P^ X(δ∕ = XX r^ Var[X(δ)]
r ((^pe2 + qe-2')β — (Pe + qe-1)产)(e2 + e-2)α.
(13)
(14)
(15)
(16)
(17)
(18)
C.1 Different attention mechanisms
For dot-product and cosine-similarity attention, ‘angular’ difference mechanisms, we use (+, —) to encode
the input variables. This is because we want the score to be greater when the variables are the same and lesser
when they are opposed (if we were to use (1, 0) we’d have 0 × 0 6= 1 × 1 and 0 × 1 = 0 × 0). With these
methods we get
fdot(δ) = α — 2δ,	(19)
fcos(δ) = 1------.	QO)
α
The change for cosine-similarity introduces a factor of exp(1∕α) to the mean and exp (2∕α) to the variance,
and the overall picture doesn’t change
E [Pi∈s( —1产Xcos(δi)] = r (pe2 + "-2)” (e1/a — e TBa	(21)
Var [Pi∈s( —1)δiXcos(δi)] = r ((pe2 + ^e-2) β — (pe + 强-1)2]卜2/a + e -%广	(22)
For squared Euclidean distance attention (which coincides with the ‘Laplace attention, L1 norm in this
encoding), if We encode the inputs as (1,0) We get fL2(δ) = —(√δ)2 = —δ, introducing a factor of
exp (—a) to the mean and exp (-2a) to the variance, which also doesn,t change the overall picture.
16
Under review as a conference paper at ICLR 2022
D Top-k feature selection
Top-k feature selection masks out all but the k highest scoring features. That is X J m Θ X With
(1	if rank(s)i ≤ k
mi	0 otherWise
(23)
As compared to soft feature selection Which rescales as X J s Θ X, or X J s0 Θ X With normalised s0 .
-1.0
-1.0
0βr2u①UOdmOo ①A-oojUI
Directly on inputs
123456789 10
-0.9
-0.8
-0.7
-0.6
with top-k selection (k = 4)
0.90
0.89
0.93
0.94
0.96
0.97
0.93
0.96
0.97
0.99
0.99
L 0.5
0 cstju①UOdmOo ①A-0&UI
∞ -0T0T0T0T
0.93
0.97
0.99
0.99
0.99 1.00
0.93
0.96
0.99
0.99
1.00 1.00 1.00
9 - 0.63 I 0.80 0.92
0.97
1.00
1.00 1.00 1.00 1.00 1.00
0.95 0.99
1.00 1.00 1.00 1.00 1.00 1.00
0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00
0.96 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
-0.81	0.98	1.00 1.00 1.00	1.00	1.00 1.00 1.00	1.00
-0.89	0.96	1.00 1.00 1.00	1.00	1.00 1.00 1.00	1.00
ιιιιιιιιιι
23456789 10
1.0
0.9
0.8
0.7
0.6
0.5
O
I~~I
I~~I
with soft feature selection
1.00 1.00 1.00
1.00 1.00 1.00
-0.9
0.98 0.99 1.00
0.98 1.00 1.00 1.00
1.00 1.00 1.00
1.00
1.00
1.00
1.00 1.00 1.00 1.00 1.00 1.00
1.00 1.00 1.00 1.00 1.00 1.00
1.00 1.00 1.00 1.00 1.00 1.00
1.00 1.00 1.00
123456789 10
∆(top-k, soft-feature)
O - 0.071 0.10 0.16 0.19 0.24 0.20 0.21 0.18 0.17 0.15
23456789 10
-0.8
-0.7
-0.6
0.5
-0.20
-0.15
o
I~~I
II
II
1
1
Variant examples, r
Variant examples, r
Figure 8: Comparing soft and top-k feature selection in the same setting as Figure 5: classification
of XOR4 over variant frequency, r, and number of inactive components β . Examples of a variant
satisfy the XOR in the same Way, i.e. the active components are equal, but may not have the same
inactive components. The top-k version uses a binary mask to leave the k highest scoring features
unchanged and zeroing the rest. This produces significant improvements over even the soft feature
selection method at high values of β, but requires knoWledge of the number of active elements.
17
Under review as a conference paper at ICLR 2022
E Convergence of feature vectors across self-attention iterations
Figure 9 illustrates how feature vectors converge on the XOR2 task across self-attention iterations of the
feature selection mechanism.
Figure 9: Feature vectors converging under iterated self-attention on a XOR2 classification of vectors
uniformly sampled over the sphere, in 3D (top) and down the z-axis (bottom). Colours indicate classes.
The vectors quickly align by xy-quadrant and the variation in z is ‘washed-out,’ also seen in the feature
selection scores (mean-absolute-deviation) [x, y, z].
18
Under review as a conference paper at ICLR 2022
F Experimental details
We use the same feature extractor architecture and train loop for all the baselines.
Feature extractor. We leverage a convolutional neural network with 4 blocks as a feature extractor. Each
block consists of a convolutional layer (64 output channels and 3 × 3 filters), followed by batch normalisation
(momentum 0.01), a ReLU activation, and 2 × 2 max pooling:
Conv2d(64, 3 × 3) → BN → ReLU → MaxPool(2 × 2)
Then, we flatten the output and apply a linear layer to map the data into a 64-dimensional embedding
space (unless otherwise stated). As explained in the main manuscript, each method then manipulates these
embeddings in different way.
Train loop. We train all models in an episodic manner. At each training iteration, we follow these steps:
•	First, we sample task-specific support and query sets. For polythetic MNIST, the support set has
96 samples (2 classes, 2 groups per class, and 24 group-specific examples per group). The query
set consists of 32 samples (2 classes, 2 groups per class, and 8 group-specific examples per group).
For Omniglot, the support set consists of 5 examples for 20 classes (20-way, 5 shot). The query set
consists of 15 examples per class.
•	Second, we compute embeddings using the feature extractor and produce class probabilities for
the query points. The way in which these probabilities are computed depends on the method (e.g.
attentional classification for matching networks or softmax over prototype distances for prototypical
networks).
•	Finally, we compute the cross entropy for the query examples and optimise the feature extractor via
gradient descent. We employ an Adam optimise with learning rate 0.001.
We train the models for 10,000 iterations (i.e. tasks) for all experiments, except for full polythetic MNIST
(100,000 tasks). We then compute the performances on a held-out dataset and average the results across 1,000
tasks.
G	Multi-categorical pre-training
In this experiment we first train a classifier in the multi-categorical setting for the full polythetic MNIST
task. In this case there are no task-irrelevant digits or colours as the label describes all four digits with
their colours: there are four 10-way labels for the digits (∈ R4×10) and four 3-way labels for the colours
(∈ R4×3) for a combined multi-hot output vector ∈ R52 . The model architectures match that used in the
other experiments, see Appendix F, other than using a variation of the MLP head that takes the flattened
output from the convolutional network. The variation has two hidden layers with 512 units each and ReLU
activations, before linear layers with softmaxes for each of the label heads. The model is pre-trained over 800
batches of 16 examples drawn at random from the label combinations. In the multi-categorical pre-training,
the model achieved a validation accuracy of 95.6% on the digit labels and 100% on the colour labels over
1600 validation examples.
The pre-trained model was then used with prototypical and attentional classifiers in the polythetic MNIST
few-shot classification setting discussed in Section 5 and detailed further in Appendix F. We compared
performance using the multi-headed softmax activations the model was pre-trained with and an elementwise
sigmoid for both the monothetic and polythetic settings. The results are presented in Table 7, and conform
to the trend we see in other experiments: threshold classification has the advantage in monothetic tasks but
perform no better than chance for polythetic tasks. Attentional classifiers are weaker in the monothetic setting,
but more than make up for this defeict in the polythetic setting.
19
Under review as a conference paper at ICLR 2022
Table 7: Polythetic MNIST problem with multicategorical pre-training. Mean over 1000 tasks.
Model	Softmax	Sigmoid Monothetic Polythetic Monothetic Polythetic
Proto. Attn.	97.95	50.18	94.64	50.21 93.48	93.40	88.16	86.24
H An illustrative example of the method
Figure 10 provides a more comprehensive overview of the proposed method. In this example, we are interested
in distinguishing between big-cats and equids (horses, donkeys, and in the case zebra). We first extract
features from all samples in the support and query sets. Here, we imagine these features as corresponding
to some general ‘cat’ properties, patterns (such as stripes, dots etc.) and general ‘equid’ properties. Next
(as presented in lines 2 : 4 in Algorithm 1), we perform repeated self-attention with respect to the separate
classes of the support set, which yields updated features for each support sample.
We then aggregate the resulting support features with an appropriate dispersion metric, e.g. mean absolute
deviation or standard deviation, (line 5 in Algorithm 1) to obtain a vector of feature scores. These scores
quantify the relevance of each feature in a given task. Next, we rescale both the query and (initial) support
features, i.e. multiplying by the feature scores, to dilute the task-irrelevant and (potentially) misleading
features. In this particular example, this would correspond to diluting the ‘patterns’ feature, since it is
irrelevant when distinguishing between cats and equids. Finally, we produce class probabilities via an
attentional classifier.
Figure 10: Diagram of the proposed approach. Note how the misleading features of stripes and spots,
which may cause misclassification, are diluted through the feature selection process.
20
Under review as a conference paper at ICLR 2022
I Construction of b inary strings tasks
For binary strings tasks, we construct training tasks by sampling |S| = 5 ∙ 2ɑ support examples and |Q| = 5 ∙ 2α
query examples. Each example consists of α + β bits. The location of the α active bits is completely random
and consistent between the support and query sets within the same task. The labels are computed as the XOR
over the α active components. The remaining β noisy components are randomly sampled from a Bernoulli
distribution with protability 0.5. Figure 11 shows an example of a binary strings task with α = 2 active
components, β = 3 noisy bits, and r = 1 repetitions.
Support set	Query set
---+	-	→		++---	→	
+---	+	→	+	-+-++	→	+
+++-	-	→	+	-+++-	→	+
-++-	+	→		--+++	→	
Figure 11:	Example of an XORα task with α = 2 active components (3rd and 5th bits), β = 3 noisy
bits, and r = 1 repetitions for each combination of active components. The support and query sets
contain r2α examples each.
J Construction of polythetic MNIST tasks
For polythetic MNIST tasks, the support set consists of 96 samples, with 48 samples for class 0 and 48
samples for class 1. Each class is further divided into 2 groups of 24 samples and each group is defined by a
specific set of traits. The groups are complementary between classes, e.g. red ones and blue zeros for class 0;
and blue ones and red zeros for class 1. The set of traits is sampled randomly for each task. The query set is
sampled in the same manner, with 2 groups per class and 8 samples per group. Tables 8 and 9 summarise the
details of the support and query sets and Figures 13, 14, and 15 show the whole support set for the clean,
colourless, and full versions of polythetic MNIST.
Class: 0 Class: 0
Class: 0 Class: 0 Class: 0 Class: 0 Class: 1
Class: 1 Class: 1 Class: 1 Class: 1 Class: 1
∂4∣t?
“I 7
Class: 0 Class： 0
IlII I 」
Class: 0 Class: 0 Class: 0 Class: 0 Class: 1
3 Iq+ I
)八:	7
Class: 1 Class: 1 Class: 1 Class: 1 Class: 1
/I /
Iql / I s
ol 6∣ 八。()I

Figure 12:	Example of an MNIST polythetic task. Examples from class 0 have either digit 1 (in any
colour) in the bottom-left corner with a red digit in the bottom-right corner (top row); or digit 6 in the
bottom-left corner with a green digit in the bottom-right (bottom row). Examples from class 1 can
have either digit 1 in the bottom-left with a green digit in the bottom-right (top row); or digit 6 in the
bottom-left with a red digit in the bottom-right corner (bottom row).
21
Under review as a conference paper at ICLR 2022
Support set. Polythetic task
Class: O	Class: O	Class: O	Class: O	Class: O	Class: O	Class: O	Class: O
Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O
Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O
Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O
Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O	Class： O
3
Class: 1	Class: 1	Class: 1	Class: 1	Class: 1	Class: 1	Class: 1	Class: 1
ʒ
3
Class: 1	Class: 1	Class: 1	Class: 1	Class: 1	Class: 1	Class: 1	Class: 1
3
Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1
Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1
Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1
Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1	Class： 1
Figure 13: Example of the support set for a polythetic MNIST task (clean).
Under review as a conference paper at ICLR 2022
Figure 14: Example of the support set for a polythetic MNIST task (colourless).
Under review as a conference paper at ICLR 2022
Figure 15: Example of the support set for a polythetic MNIST task (full).
Under review as a conference paper at ICLR 2022
|S| = 96	Examples	Groups	Examples per group	Example of groups
Class 0	48	2	24	green ones and blue threes
Class 1	48	2	24	blue ones and green threes
Table 8: Support set details
|Q| =32	Examples	Groups	Examples per group	Example of groups
Class 0	16	2	8	green ones and blue threes
Class 1	16	2	8	blue ones and green threes
Table 9: Query set details
25