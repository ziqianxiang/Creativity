Under review as a conference paper at ICLR 2022
p-LAPLACIAN BASED GRAPH NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
Graph neural networks (GNNs) have demonstrated superior performance for semi-
supervised node classification on graphs, as a result of their ability to exploit node
features and topological information simultaneously. However, most GNNs im-
plicitly assume that the labels of nodes and their neighbors in a graph are the same
or consistent, which does not hold in heterophilic graphs, where the labels of
linked nodes are likely to differ. Moreover, when the topology is non-informative
for label prediction, ordinary GNNs may work significantly worse than simply
applying multi-layer perceptrons (MLPs) on each node. To tackle the above prob-
lem, we propose a new p-Laplacian based GNN model, termed as pGNN, whose
message passing mechanism is derived from a discrete regularization framework
and could be theoretically explained as an approximation of a polynomial graph
filter defined on the spectral domain of p-Laplacians. The spectral analysis shows
that the new message passing mechanism works simultaneously as low-pass and
high-pass filters, thus making pGNNs are effective on both homophilic and het-
erophilic graphs. Empirical studies on real-world and synthetic datasets validate
our findings and demonstrate that pGNNs significantly outperform several state-
of-the-art GNN architectures on heterophilic benchmarks while achieving compet-
itive performance on homophilic benchmarks. Moreover, pGNNs can adaptively
learn aggregation weights and are robust to noisy edges.
1 Introduction
In this paper, we explore the usage of graph neural networks (GNNs) for semi-supervised node
classification on graphs, especially when the graphs admit strong heterophily or noisy edges. Semi-
supervised learning problems on graphs are ubiquitous in a lot of real-world scenarios, such as user
classification in social media (Kipf & Welling, 2017), protein classification in biology (Velickovic
et al., 2018), molecular property prediction in chemistry (Duvenaud et al., 2015), and many oth-
ers (Marcheggiani & Titov, 2017; Satorras & Estrach, 2018). Recently, GNNs are becoming the de
facto choice for processing graph structured data. They can exploit the node features and the graph
topology by propagating and transforming the features over the topology in each layer and thereby
learn refined node representations. A series of GNN architectures have been proposed, including
graph convolutional networks (Bruna et al., 2014; Henaff et al., 2015; Defferrard et al., 2016; Kipf
& Welling, 2017; Wu et al., 2019), graph attention networks (Velickovic et al., 2018; Thekumpara-
mpil et al., 2018), and other representatives (Hamilton et al., 2017; Xu et al., 2018; Pei et al., 2020).
Most of the existing GNN architectures work under the homophily assumption, i.e. the labels of
nodes and their neighbors in a graph are the same or consistent, which is also commonly used in
graph clustering (Bach & Jordan, 2004; von Luxburg, 2007; Liu & Han, 2013) and semi-supervised
learning on graphs (Belkin et al., 2004; Hein, 2006; Nadler et al., 2009). However, recent stud-
ies (Zhu et al., 2020; 2021; Chien et al., 2021) show that in contrast to their success on homophilic
graphs, most GNNs fail to work well on heterophilic graphs, in which linked nodes are more likely
to have distinct labels. Moreover, GNNs could even fail on graphs where their topology is not help-
ful for label prediction. In these cases, propagating and transforming node features over the graph
topology could lead to worse performance than simply applying multi-layer perceptrons (MLPs) on
each of the nodes independently. Several recent works were proposed to deal with the heterophily is-
sues of GNNs. Zhu et al. (2020) finds that heuristically combining ego-, neighbor, and higher-order
embeddings improves GNN performance on heterophilic graphs. Zhu et al. (2021) uses a compat-
ibility matrix to model the graph homophily or heterphily level. Chien et al. (2021) incorporates
1
Under review as a conference paper at ICLR 2022
the generalized PageRank algorithm with graph convolutions so as to jointly optimize node feature
and topological information extraction for both homophilic and heterophilic graphs. However, the
problem of GNNs on graphs with non-informative topologies (or noisy edges) remains open.
Unlike previous works, we tackle the above issues of GNNs by proposing the discrete p-Laplacian
based message passing scheme, termed as p-Laplacian message passing. It is derived from a discrete
regularization framework and is theoretically verified as an approximation of a polynomial graph
filter defined on the spectral domain of the p-Laplacian. Spectral analysis of p-Laplacian message
passing shows that it works simultaneously as low-pass and high-pass filters 1 and thus is applicable
to both homophilic and heterophilic graphs. Moreover, when p 6= 2, our theoretical results indicate
that it can adaptively learn aggregation weights in terms of the variation of node embeddings on
edges (measured by the graph gradient (Amghibech, 2003; ZhoU & Scholkopf, 2005; LUo et al.,
2010)), and work as low-pass or both low-pass and high-pass filters on a node according to the local
variation of node embeddings aroUnd the node (measUred by the norm of graph gradients).
Based on p-Laplacian message passing, we propose a new GNN architectUre, called pGNN, to en-
able GNNs to work with heterophilic graphs and graphs with non-informative topologies. Several
existing GNN architectUres, inclUding SGC (WU et al., 2019), APPNP (Klicpera et al., 2019) and
GPRGNN (Chien et al., 2021), can be shown to be analogical to pGNN with p = 2. OUr empiri-
cal stUdies on real-world benchmark datasets (homophilic and heterophilic datasets) and synthetic
datasets (cSBM (Deshpande et al., 2018)) demonstrate that pGNNs obtain the best performance
on heterophilic graphs and competitive performance on homophilic graphs against state-of-the-art
GNNs. Moreover, experimental resUlts on graphs with different levels of noisy edges show that
p GNNs work mUch more robUstly than GNN baselines and even as well as MLPs on graphs with
completely random edges. Additional experiments (reported in Appendix F.5) illUstrate that inter-
grating pGNNs with existing GNN architectUres (i.e. GCN (Kipf & Welling, 2017), JKNet (XU
et al., 2018)) can significantly improve their performance on heterophilic graphs. In conclUsion, oUr
contribUtions can be sUmmarized as below:
(1) New methodologies. We propose p-Laplacian message passing and pGNN to adapt GNNs to
heterophilic graphs and graphs where the topology is non-informative for label prediction. (2) Supe-
rior performance. We empirically demonstrate that pGNNs is sUperior on heterophilic graphs and
competitive on homophilic graphs against state-of-the-art GNNs. Moreover, pGNNs work robUstly
on graphs with noisy edges or non-informative topologies. (3) Theoretical justification. We theo-
retically demonstrate that p-Laplacian message passing works as both low-pass and high-pass filters
and the message passing iteration is gUarantee to converge with proper settings. (4) New paradigm
of designing GNN architectures. We bridge the gap between discrete regUlarization framework and
GNNs, which coUld fUrther inspire researchers to develop new graph convolUtions or message pass-
ing schemes Using other regUlarization techniqUes with explicit assUmptions on graphs. DUe to space
limit, we defer the discUssions on related work and fUtUre work and all proofs to the Appendix.
2 Preliminaries and Background
Notation. Let G = (V, E, W) be an Undirected graph, where V = {1, 2, . . . , N} is the set of nodes,
E ⊆ V × V is the set of edges, W ∈ RN×N is the adjacency matrix and Wi,j = Wj,i, Wi,j > 0
for [i, j] ∈ E, Wi,j = 0, otherwise. Ni = {j}[i,j]∈E denotes the set of neighbors of node i,
D ∈ RN×N = diag(D1,1, . . . DN,N) denotes the diagonal degree matrix with Di,i = PjN=1 Wi,j,
for i = 1, . . . , N . f : V → R and g : E → R are fUnctions defined on the vertices and edges of G,
respectively. FV denotes the Hilbert space of fUnctions endowed with the inner prodUct hf, fiFV :=
._.	~，.、 ：，.、  ............ 一	. 一	一 ”一r . .	一、.  _ _ 一
Pi∈v f(i)f(i). Similarly define FE. We also denote by [K] = {1, 2,..., K},∀K ∈ N and we use
kxk = kxk2 = (Pid=1 xi2)1/2, ∀x ∈ Rd to denote the FrobeniUs norm of a vector.
Problem Formulation. Given a graph G = (V, E, W), each node i ∈ V has a feature vector Xi,:
which is the i-th row of X and a subset of nodes in G have labels from a label set L = {1, . . . , L}.
The goal of semi-supervised node classification on G is to learn a mapping M : V → L and predict
the labels of unlabeled nodes.
1 Note that if the low frequencies and high frequencies dominate the middle frequencies (the frequencies
that are around the cutoff frequency), we say that the filter works both as low-pass and high-pass filters.
2
Under review as a conference paper at ICLR 2022
Homophily and Heterophily. The homophily or heterophily of a graph is used to describe the
relation of labels between linked nodes in the graphs. The level of homophily of a graph can be
measured by H(G) = Ei∈V {j}j∈Ni,yi=yj /|Ni| (Pei et al., 2020; Chien et al., 2021), where
{j}j∈Ni,yi=yj denotes the number of neighbors of i ∈ V that share the same label as i and
H(G) → 1 corresponds to strong homophily while H(G) → 0 indicates strong heterophily. We
say that a graph is a homophilic (heterophilic) graph if it has strong homophily (heterophily).
Graph Gradient. The graph gradient of an edge [i, j], i, j ∈ V is defined to be a measurement of
the variation of a function f2 : V → R on the edge [i, j].
Definition 1 (Graph Gradient). Given a graph G = (V, E) and a function f : V → R, the graph
gradient is an operator V : FV → Fe defined by
(Vf)(LjD = JWj f(j)-JWj f(i), for all [i,j ] ∈E ∙	⑴
Dj,j	Di,i
For [i, j] ∈/ E, (Vf)([i, j]) := 0. The graph gradient of a function f at vertex i is defined to
be Vf (i) := ((Vf)([i, 1]), . . . , (Vf)([i, N])) and its Frobenius norm is given by kVf (i)k2 :=
(PjN=1(Vf)2([i,j]))1/2, which measures the variation off around node i. We measure the variation
of f over the whole graph G by Sp(f) where it is defined to be
Sp (f )：=1 XX XXk(Vf )([i,j])kp =2 XX XXI yWIj f(j)- WWif f(i)	,for P ≥ 1,⑵
Note that the definition of Sp here is different with the P-Dirichlet form in ZhoU & ScholkoPf (2005).
Graph Divergence. The graph divergence is defined to be the adjoint of the graph gradient:
Definition 2 (GraPh Divergence). Given a graph G = (V, E), and functions f : V → R, g : E → R,
the graph divergence is an operator div : FE → FV which satisfies
hVf,gi=hf,-divgi.	(3)
The graph divergence can be computed by
(diVg)⑶=X jwW,j (g([i,j]) - g([j,i])).
(4)
Fig. 4 in APPendix E.1 gives a tiny examPle of illUstration of graPh gradient and graPh divergence.
Graph P-Laplacian Operator. By the definitions of graPh gradient and graPh divergence, we reach
the definition of graPh P-LaPlacian oPerator as below.
Definition 3 (GraPh P-LaPlacian3). Given a graph G = (V, E) and a function f : V → R, the graph
P-Laplacian is an operator ∆p : FV → FV defined by
∆pf := - 1div(kVf kp-2Vf), for P ≥ 1.	(5)
where IHIp-2 is element-wise, i.e. kVf (i)kp-2 = (k(Vf )([i, 1])kp-2,..., k(Vf )([i,N Dkp-2).
SUbstitUting Eq. (1) and Eq. (4) into Eq. (5), we obtain
Cpf)(i)=XSWlk(Vf)(j,i])kp-2 (s⅞f⑶-s¾f⑺!⑹
The graPh P-LaPlacian is semi-definite: hf, ∆pfi = Sp(f) ≥ 0 and we have
驾 I = P(∆pf )(i).	⑺
∂ f i
When P = 2, ∆2 is refered as the ordinary LaPlacian oPerator and ∆2 = I - D-1/2WD-1/2
and whenP = 1, ∆1 is refered as the Curvature operator and ∆1f := 一 1 div(k VfkTVf). Note
that LaPlacian ∆2 is a linear oPerator, while in general for P 6= 2, P-LaPlacian is nonlinear since
∆p(af) 6= a∆p(f) fora ∈ R.
2 f can be a vector function: f : V → Rc for some c ∈ N and here we use f : V → R for better illustration.
3Note that the definition adopted is slightly different with the one used in Zhou & Scholkopf (2005) where
k ∙ ∣∣p-2 is not element-wise and the one used in some literature such as Amghibech (2003); Buhler & Hein
(2009), Where (∆pf)(i) = PN=I Wj |f (i) - f (j)∣p-2 (f ⑶—f (j)) for p> 1 and P =1 is not allowed.
3
Under review as a conference paper at ICLR 2022
3	p-LAPLACIAN BASED GRAPH NEURAL NETWORKS
In this section, we derive the p-Laplacian message passing scheme from a p-Laplacian regularization
framework and present pGNN, a new GNN architecture developed upon the new message passing
scheme. We theoretically characterize how p-Laplacian message passing adaptively learns aggre-
gation weights and profits pGNN for being effective on both homophilic and heterophilic graphs.
3.1	p-LAPLACIAN REGULARIZATION FRAMEWORK
Given an undirected graph G = (V, E) and a signal function with c (c ∈ N) channels f : V → Rc,
let X = (X1>,:, . . . , XN>,:)> ∈ RN×c with Xi,: ∈ R1×c, i ∈ [N] denoting the node features of G and
F = (F1>,:, . . . , FN>,:)> ∈ RN×c be a matrix whose ith row vector Fi,: ∈ R1×c, i ∈ [N] represents
the function value of f at the i-th vertex in G . We present a p-Laplacian regularization problem
whose cost function is defined to be
N
F* = arg min L(F) := arg min Sp(F) + μ X ||Fi,： - Xi∕∣2,	(8)
F	F	i=1
where μ ∈ (0, ∞). The first term of the right-hand side in Eq. (8) is a measurement of variation of
the signal over the graph based on p-Laplacian. As we will discuss later, different choices ofp result
in different smoothness constraint on the signals. The second term is the constraint that the optimal
signals F* should not change too much from the input signal X, and μ provides a trade-off between
these two constraints.
Regularization with P = 2. When P = 2, the solution ofEq. (8) satisfies ∆2F* + μ(F* 一 X) = 0
and We can obtain the closed form (Zhou et al., 2003; Zhou & Scholkopf, 2005)
f* = μ02 + μIN )-1χ.	(9)
Then, we could use the following iteration algorithm to get an approximation of Eq. (9):
F(k+1) = aD-1/2WDT/2F(k) + gχ,	(10)
where k represents the iteration index, α = ι++μ and β = j+μμ = 1 一 α. The iteration converges to
a closed-form solution as k goes to infinity (Zhou et al., 2003; Zhou & Scholkopf, 2005). We could
relate the the result here with the personalized PageRank (PPR) (Page et al., 1999; Klicpera et al.,
2019) algorithm (proof defered to Appendix D.1):
Theorem 1 (Relation to personalized PageRank (Klicpera et al., 2019)). μ(∆2 + μIχ)-1 in the
closed form solution of Eq. (9) is equivalent to the personalized PageRank matrix.
Regularization with p > 1. For p > 1, the solution of Eq. (8) satisfies p∆pF* + 2μ(F* 一 X) = 0.
By Eq. (6) we have that, for all i ∈ [N],
NW
X pwji k(vf *)(j,i])kp-2
Based on which we can construct a similar iterative algorithm to obtain a solution (Zhou &
Scholkopf, 2005):
F(k+1) = α(ki) X	Mj— Fjk) + β(,k)Xi,:, for all i ∈ [N],	(11)
j=1	Di,i Dj,j
with M(k) ∈ RN×N, α(k) = diag(α(1k,1), . . . , α(Nk,)N), β(k) = diag(β1(k,1), . . . , βN(k,)N) updated by
p-2
for all i,j ∈ [N],
(12)
α(? = 1/ (X	~Dj	+	^μ) ,	β(,k)	=	^μαi,i,	for all i	∈	[N],	(13)
j=1 Di,i	P	P
Note that in Eq. (12), when ∣∣ JWjF(? ― JWjF(? ∣∣ = 0, we set Mikj = 0. It is easy to see
that Eq. (10) is the special cases of Eq. (14) with P = 2.
4
Under review as a conference paper at ICLR 2022
Remark 1 (Discussion on p = 1). Forp = 1, when f is a real-valued function (c = 1), ∆1f is a
step function, which could make the stationary condition of the objective function Eq. (8) become
problematic. Additionally,∆f is not continuous at ∣∣(Vf)([i,j])k = 0. Therefore, P = 1 is not
allowed when f is a real value function. On the other hand, note that there is a Frobenius norm
in ∆pf. When f is a vector-valued function (c > 1), the step function in ∆1f only exists on the
axes. The stationary condition will be fine if the node embeddings F are not a matrix of vectors that
has only one non-zero element, which is true for many graphs. p = 1 may work for these graphs.
Overall, we suggest to use p > 1 in practice but p = 1 may work for graphs with multiple channel
signals as well. We conduct experiments for p > 1 (e.g., p = 1.5, 2, 2.5) andp = 1 in Sec. 5.
3.2	p-LAPLACIAN MESSAGE PASSING AND pGNN ARCHITECTURE
P-Lapladan Message Passing. Rewrite Eq. (11) in a matrix form We obtain
F(k+I) = a(k)D-1/2M(k)D-1/2F(k) + @(k)x.
(14)
Eq. (14) provides a new message passing mechanism, named P-Laplacian message passing.
Remark 2. aD-1/2MD-1/2 in Eq. (14) can be regarded as the learned aggregation weights at
each step for message passing. It suggests that P-Laplacian message passing could adaptively tune
the aggregation weights during the course of learning, which will be demonstrated theoretically and
empirically in the sequel of this paper. βX in Eq. (14) can be regarded as a residual unit, which
helps the model escape from the oversmoothing issue (Chien et al., 2021).
We present the following theorem to show the shrinking property of P-Laplacian message passing.
Theorem 2 (Shrinking Property of P-LaPlacian Message Passing). Given a graph G = (V, E, W)
with node features X, β(k), F(k), M(k),α(k) are updated accordingly to Equations (11) to (13) for
k = 0,1,...,K and F(0) = X. Then there exist some positive real value μ > 0 which depends on
X, G,p andP > 1 such that
Lp(FD) ≤ Lp(F(k)).
Proof see Appendix D.2. Thm. 2 shows that with some proper positive real value μ and p > 1,
the loss of the objective function Eq. (8) is guaranteed to decline after taking one step P-Laplacian
message passing. Thm. 2 also demonstrates that the iteration Equations (11) to (13) is guaranteed to
converge for p > 1 with some proper μ which is chosen depends on the input graph and p.
pGNN Architecture. We design the architecture of pGNNs using p-Laplacian message passing.
Given node features X ∈ RN×c, the number of node labels L, the number of hidden units h, the
maximum number of iterations K, and M, α, and β updated by Equations (12) and (13) respec-
tively, we give the pGNN architecture as following:
F(O) = ReLU(X㊀⑴)，	(15)
F(k+1) = α⑹DT/2M⑹DT/2F⑹ + β⑹F(O),	k = 0,1,...,K — 1,	(16)
Z = SOftmax(F(K) Θ ⑵),	(17)
where Z ∈ RN ×L is the output propbability matrix with Zi,j is the estimated probability that the
label at node i ∈ [N] is j ∈ [L] given the features X and the graph G, Θ(1) ∈ Rc×h and Θ(2) ∈
Rh×L are the first- and the second-layer parameters of the neural network, respectively.
Remark 3 (Connection to existing GNN variants). The message passing scheme of p GNNs is dif-
ferent from that of several GNN variants (say, GCN, GAT, and GraphSage), which repeatedly stack
message passing layers. In contrast, it is similar with SGC (Wu et al., 2019), APPNP (Klicpera
et al., 2019), and GPRGNN (Chien et al., 2021). SGC is an approximation to the closed-form in
Eq. (9) (Fu et al., 2020). By Thm. 1, it is easy to see that APPNP, which uses PPR to propagate the
node embeddings, is analogical to pGNN with P = 2, termed as 2.OGNN. APPNP and 2.OGNN work
analogically and effectively on homophilic graphs. 2.OGNN can also work effectively on heterophilic
graphs by letting Θ(2) be negative. However, APPNP fails on heterophilic graphs as its PPR weights
5
Under review as a conference paper at ICLR 2022
are fixed (Chien et al., 2021). Unlike APPNP, GPRGNN, which adaptively learn the generalized
PageRank (GPR) weights, works similarly to 2.0GNN on both homophilic and heterophilic graphs.
However, GPRGNN needs more supervised information in order to learn optimal GPR weights. On
the contrary, p GNNs need less supervised information to obtain similar results because Θ(2) acts
like a hyperplane for classification. p GNNs could work better under weak supervised information.
Our analysis is also verified by the experimental results in Sec. 5.
We also provide an upper-bounding risk of pGNNs by Thm. 4 in Appendix C.1 to study the effect of
the hyperparameter μ on the performance of pGNNs. Thm. 4 shows that the risk of PGNNs is UPPer-
bounded by the sum of three terms: the risk of label prediction using only the original node features
X, the norm of P-LaP山Cian diffusion on X, and the magnitude of the noise in X. μ controls the
trade-off between these three terms. The smaller μ, the more weights on the P-LaP山Cian diffusion
term and the noise term and the less weights on the the other term and vice versa.
4	SPECTRAL VIEWS OF p-LAPLACIAN MESSAGE PASSING
In this section, we theoretically demonstrate that P-Laplacian message passing is an approximation
of a polynomial graph filter defined on the spectral domain of P-Laplacian. We show by spectral
analysis that P-Laplacian message passing works simultaneously as low-pass and high-pass filters.
P-Eigenvalues and P-Eigenvectors of the Graph P-Laplacian. We first introduce the defini-
tions of P-eigenvalues and P-eigenvectors of P-Laplacian. Let φp : R → R defined as φp (u) =
kukp-2u, for u ∈ R, u 6= 0. Note that φ2(u) = u. For notational simplicity, we denote by
φp(u) = (φp(uι),...,φp(uN ))τ for U ∈ RN and Φp(U) = (φp(U.),…，φp(U,N)) for
U ∈ RN×N and U:,i ∈ RN is the ith column vector of U.
Definition 4 (P-Eigenvector and P-Eigenvalue). A vector U ∈ RN is a P-eigenvector of ∆p if it
satisfies the equation
(∆pU)i = λφp(ui), for all i ∈ [N],
where λ ∈ R is a real value referred as a P-eigenvalue of ∆p associated with the P-eigenvector U.
Definition 5 (P-Orthogonal (Luo, Huang, Ding, and Nie, 2010)). Given two vectors U, v ∈ RN with
U, v 6= 0, we call that U and v is P-orthogonal if φp(U)τφp(v) = PiN=1 φp(ui)φp(vi) = 0.
Luo et al. (2010) demonstrated that the P-eigenvectors of ∆p are P-orthogonal to each other (see
Thm. 5 in Appendix C.2 for details). Therefore, the space spanned by the multiple P-eigenvectors
of ∆p is P-orthogonal. Additionally, we demonstrate that the P-eigen-decomposition of ∆p is given
by: ∆p = Φp (U)ΛΦp(U)τ (see Thm. 6 in Appendix C.3 for details), where U is a matrix of
P-eigenvectors of ∆p and Λ is a diagonal matrix in which the diagonal is the P-eigenvalues of ∆p .
Graph Convolutions based on P-Laplacian. Based on Thm. 5, the graph Fourier Transform f of
any function f on the vertices ofG can be defined as the expansion off in terms of Φ(U) where U is
the matrix of p-eigenvectors of △?: f = Φp(U)τf. Similarly, the inverse graph Fourier transform
is then given by: f = Φp(U)f. Therefore, a signal X ∈ RN×c being filtered by a spectral filter
gθ can be expressed formally as: gθ ? X = Φp(U)gθ(Λ)Φp(U)τX, where Λ denotes a diagonal
matrix in which the diagonal corresponds to the p-eigenvalues {λι}i=o,...,n-i of ∆p and gθ(Λ)
denotes a diagonal matrix in which the diagonal corresponds to spectral filter coefficients. Let gθ be
a polynomial filter defined as gθ = PK=o1 θkλ, where the parameter θ = [θo,..., θκ-ι]τ ∈ RK
is a vector of polynomial coefficients. By the p-eigen-decomposition of p-Laplacian, we have
K -1	K -1
gθ ? X ≈ X θkΦp(U)ΛkΦp(U)τX = X θk∆pX.	(18)
k=0	k=0
Theorem 3. The K-step P-Laplacian message passing is a K-order polynomial approximation to
the graph filter given by Eq. (18).
Proof see Appendix D.3. Thm. 3 indicates that p-Laplacian message passing mechanism is implicitly
a polynomial spectral filter defined on the spectral domain of p-Laplacian.
6
Under review as a conference paper at ICLR 2022
Spectral Analysis of p-Laplacian Message Passing. Here, we analyze the spectral propecties of
p-Laplacian message passing. We can approximately view p-Laplacian message pasing as a filter of
a linear combination of K spectral filters g(Λ)(0), g(Λ)(1), . . . , g(Λ)(K-1) with each spectral filter
defined to be g(Λ)(k) := (aD-1/2MDT/2)k where m^ = WijkqWIjFi,： - qWjFj,：kp-2
for i, j ∈ [N] and F is the matrix of node embeddings. We can study the properties of p-Laplacian
message passing by studying the spectral properties of aD-1/2MDT/2 as given below.
Proposition 1. Given a connected graph G = (V, E, W) with node embeddings F and the P-
Laplacian ∆p with its P-eigenvectors {u(l)}ι=o,ι,…,n-i and the P-eigenvalues {λ1}1=0,1,…,n-i.
Let gp(λi-ι) := αi,i Pj D-J2MijD-;/2 for i ∈ [N ] be the filters defined on the spectral domain
of ∆p, where Mij = WijIl Vf([i,j])kp-2, (▽/)([i,j]) is the graph gradient of the edge between
node i and j and ∣∣Vf (i)k is the norm of graph gradient at i. Ni denotes the number of edges
connected to i, Nmin = min{Nj}j∈[N], and k = argmaXj({∣uj" ∣/PDJ}j∈[N]；i=o,…,n-1), then
1.	When P = 2, gp(λi-ι) works as both low-pass and high-pass filters.
2.	When P > 2, if ∣Vf (i)∣ ≤ 2(p-1)/(p-2), gp(λi-ι) works as both low-pass and high-pass
filters on node i and gp(λi-ι) works as low-pass filters on i when ∣∣Vf (i)∣ ≥ 2(p-1)/(p-2).
3.	When 1 ≤ p < 2, if 0 ≤ ∣∣Vf (i)∣ ≤ 2(2√N)1/(p-2), gp(λi-ι) works as low-pass
filters on node i and gp(λi-ι) works as both low-pass and high-pass filters on i when
∣∣Vf (i)∣ ≥ 2 (2√Nk)1/(P 2). Specifically, whenP = 1, Nk can be replaced by Nmin.
Proof see Appendix D.7. Proposition 1 shows that when P 6= 2, P-Laplacian message passing
adaptively works as low-pass or both low-pass and high-pass filters on node i in terms of the degree
of local node embedding variation around i, i.e. the norm of the graph gradient ∣Vf (i)∣ at node
i. When P = 2, P-Laplacian message passing works as both low-pass and high-pass filters on node
i regardless of the value of ∣Vf (i)∣. When P > 2, P-Laplacian message passing works as low-
pass filters on node i for large ∣Vf (i)∣ and works as both low-pass and high-pass filters for small
∣Vf (i)∣. Therefore, pGNNs with P > 2 can work very effectively on graphs with strong homophily.
When 1 ≤ P < 2, P-Laplacian message passing works as low-pass filters for small ∣Vf (i)∣ and
works as both low-pass and high-pass filters for large ∣Vf (i)∣. Thus, pGNNs with 1 ≤ P < 2 can
work effectively on graphs with low homophily, i.e. heterophilic graphs. The results here confirms
our analysis of the aggregation weights of P-Laplacian message passing presented in Thm. 2.
5 Empirical Studies
In this section, we empirically study the effectiveness ofpGNNs for semi-supervised node classifica-
tion using and real-world benchmark and synthetic datasets with heterophily and strong homophily.
The experimental results are also used to validate our theoretical findings presented previously.
Datasets and Experimental Setup. We use seven homophilic benchmark datasets: citation graphs
Cora, CiteSeer, PubMed (Sen et al., 2008), Amazon co-purchase graphs Computers, Photo, coauthor
graphs CS, Physics (Shchur et al., 2018), and six heterophilic benchmark datasets: Wikipedia graphs
Chameleon, Squirrel (Rozemberczki et al., 2021), the Actor co-occurrence graph, webpage graphs
Wisconsin, Texas, Cornell (Pei et al., 2020). The node classification tasks are conducted in the
transductive setting. Following Chien et al. (2021), we use the sparse splitting (2.5%/2.5%/95%)
and the dense splitting (60%/20%/20%) to randomly split the homophilic and heterophilic graphs
into training/validation/testing sets, respectively. Dataset statistics and their levels of homophily
are presented in Appendix E.
Baselines. We compare pGNN with seven models, including MLP, GCN (Kipf & Welling, 2017),
SGC (Wu et al., 2019), GAT (Velickovic et al., 2018), JKNet (Xu et al., 2018), APPNP (Klicpera
et al., 2019), GPRGNN (Chien et al., 2021). We use the Pytorch Geometric library (Fey & Lenssen,
2019) to implement all baselines except GPRGNN. For GPRGNN, we use the code released by the
authors4. The details of hyperparameter settings are deferred to Appendix E.3.
4https://github.com/jianhao2016/GPRGNN
7
Under review as a conference paper at ICLR 2022
Table 1: Heterophilious results. Averaged accuracy (%) for 100 runs. Best results outlined in bold
and the results within 95% confidence interval of the best results are outlined in underlined bold.
Method Chameleon Squirrel Actor Wisconsin Texas Cornell
MLP	48.02±1.72	33.8O±1.05	39.68±1.43	93.56±3.14	79.50±10.62	80.30±11.38
GCN	34.54±2.78	25.28±1.55	31.28±2.04	61.93±3.00	56.54±17.02	51.36±4.59
SGC	34.76±4.55	25.49±1.63	30.98±3.80	66.94±2.58	59.99±9.95	44.39±5.88
GAT	45.16±2.10	31.41±0.98	34.11±1.28	65.64±6.29	56.41±13.01	43.94±7.33
JKNet	33.28±3.59	25.82±1.58	29.77±2.61	61.08±3.71	59.65±12.62	55.34±4.43
APPNP	36.18±2.81	26.85±1.48	31.26±2.52	64.59±3.49	82.90±5.08	66.47±9.34
GPRGNN	43.67±2.27	31.27±1.76	36.63±1.22	88.54±4.94	80.74±6.76	78.95±8.52
48.86±i.95 33.75±ι 5°
48.74±1 62 33.33±i.45
48.77±1 87 33.60±i.47
4880±1.77 3379±1.45
40.62±1.25 95.37±2.06
4O.35±I.35 95.24±2.01
40.07±1.17 91.15±2.76
39.80±1.31 87.08±2.69
84.06±7.41 82.16±8.62
84.46±7.79 78.47±6.87
87.96±6.27 72.04±8.22
83.01±6.80 70.31±8.84
NN
NN
GGG
Superior Performance on Real-World Heterophilic Datasets. The results on homophilic bench-
mark datasets are deferred to Appendix F.1, which show that PGNNs obtains competitive perfor-
mance against state-of-the-art GNNs on homophilic datasets. Table 1 summarizes the results on
heterophilic benchmark datasets. Table 1 shows that PGNNs significantly dominate the baselines
and 1.0GNN obtains the best performance on all heterophilic graphs except the Texas dataset. For
Texas, 2.0GNN is the best. We also observe that MLP works very well and significantly outperforms
most GNN baselines, which indicates that the graph topology is not informative for label predic-
tion on these heterophilic graphs. Therefore, propagating and transforming node features over the
graph topology could lead to worse performance than MLP. Unlike ordinary GNNs, PGNNs can
adaptively learn aggregation weights and ignore edges that are not informative for label prediction
and thus could work better. It confirms our theoretical findings presented in previous sections. Note
that GAT can also learn aggregation weights, i.e. the attention weights. However, the aggregation
weights learned by GAT are significantly distinct from that of PGNNs, as we will show following.
uniform
w001---------------
Computers
LsGNN(Acc: 85.3%)	. . 2.5GNN(Acc: 83.68%)
GEACC 82.73%)
səpoub'
3000
ZWO
LOGNN(Acc: 85.24%)
entropy
O 2	4 β S 10 12 14
entropy
0
OZ «6*1。
entropy
175
2	4 β S 10 12 14
entropy
Wisconsin
⅛l 111 ≡
OZ 4 β S 10 12 14
entropy
səpoub'
entropy
-i β
entropy
-10	12
entropy
-10	12
entropy
Figure 1: Aggregation weight entropy distribution of graphs. Low entropy means high degree of
concentration, vice versa. An entropy of zero means all aggregation weights are on one source node.
Interpretability of the Learned Aggregation Weights of pGNNs. We showcase the interpretabil-
ity of the learned aggregation weights ai,iD^-,^/2Mi,jD-j/ of PGNNs by studying its entropy
distribution, along with the attention weights of GAT on real-world datasets. Denote {Ai,j}j∈Ni
as the aggregation weights of node i and its neighbors. For GAT, {Ai,j }j∈Ni are referred as the
attention weights (in the first layer) and for PGNNs are αi,iDlIi2M%,jD-,1/2. For any node i,
{Ai,j}j∈Ni forms a discrete probability distribution over all its neighbors with the entropy given by
H({Ai,,},∈Ni) = - P, ∈N Ai,, log(Ai,, ). Low entropy means high degree of concentration and
vice versa. An entropy of zero means all aggregation weights or attentions are on one source node.
The uniform distribution has the highest entropy of log(Di,i). Fig. 1 reports the results on Com-
puters, Wisconsin and we defer more results on other datasets to Appendix F.2 due to space limit.
Fig. 1 shows that the aggregation weight entropy distributions of GAT and PGNNs on Computers
(homophily) are both similar to the uniform case. It indicates the original graph topology of Com-
puters is very helpful for label prediction and therefore GNNs could work very well on Computers.
However, for Wisconsin (heterophily), the entropy distribution of PGNNs is significantly different
from that of GAT and the uniform case. Most entropy of PGNNs is around zero, which means
that most aggregation weights are on one source node. It states that the original graph topology of
Wisconsin is not helpful for label prediction, which explains why MLP works well on Wisconsin.
8
Under review as a conference paper at ICLR 2022
Oqqoqqoooq
0987654321
(Aoe.lnoo<
Computers
0.25	0.5	1
-	* -MLP
-	K-GCN
-	* -SGC
-	• - GAT
-+ -JKNet
----APPNP
----GPRGNN
—1.0-GNN
—1.5-GNN
—*—2.0-GNN
—«—2.5-GNN
WiSConSin
Ooooooo
0 9 8 7 6 5 4
邕 Aoe.lnoo<
0.25	0.5	1
-	* -MLP
-	K -GCN
-	* -SGC
-	♦ -GAT
-+ -JKNet
----APPNP
----GPRGNN
—1.0-GNN
—1.5-GNN
—*—2.0-GNN
→^2.5-GNN
Figure 3: Averaged accuracy (%) on graphs with noisy edges for 20 runs. Best view in colors.
On the contrary, the entropy distribution of GAT is similar to the uniform case and therefore GAT
works similarly to GCN and is significantly worse than PGNNS on Wisconsin. Similar results can
be observed on the experiments on more datasets in Appendix F.2.
Results on CSBM Datasets. We exam the
performance of pGNNs on heterophilic graphs
whose topology is informative for label pre-
diction using synthetic graphs generated by
CSBM (Deshpande et al., 2018) with φ ∈
{-1, -0.75,..., 1}. We use the same settings
of cSBM used in Chien et al. (2021). Due to
the space limit, we refer the readers to Chien
et al. (2021) for more details of CSBM dataset.
Fig. 2 reports the results on CSBM using sparse
splitting (for results on CSBM with dense split-
ting see Appendix F.3). Fig. 2 shows that
when φ ≤ -0.5 (heterophilic graphs), 2.0GNN
obtains the best performance and pGNNs and
GPRGNN significantly dominate the others. It
validates the effectiveness of pGNNs on het-
CSBM sparse split
40	--------------------------------------------------------
-1	-0.75	-0.5	-0.25	0	0.25	0.5	0.75	1
Φ
MLP
- GCN
-♦ - SGC
-■ - GAT
JKNet
--∙-- APPNP
GPRGNN
—•— 1.0-GNN
―•— 1.5-GNN
―•— 2.0-GNN
―•— 2.5-GNN
Figure 2: Averaged accuracy (%) on CSBM
(sparse split) for 20 runs. Best view in colors.
erophilic graphs. Moreover, 2.0GNN works better than GPRGNN and it again confirms that
2.0GnN is more superior under weak supervision (2.5% training rate), as stated in Remark 3.
Note that 1.0GNN and 1.5GNN are not better than 2.0GNN, the reason could be the iteration al-
gorithms Eq. (11) with P = 1,1.5 are not as stable as the one with P = 2. When the graph topology
is almost non-informative for label prediction (φ = -0.25,0), The performance of PGNNS is close
to MLP and they outperform the other baselines. Again, it validates that pGNNs can erase non-
informative edges and work as well as MLP and confirms the statements in Thm. 4. When the graph
is homophilic (φ ≥ 0.25), 1.5GNN is the best on weak homophilic graphs (φ = 0.25, 0.5) and
pGNNs work competitively with all GNN baselines on strong homophilic graphs (φ ≥ 0.75).
Results on Datasets with Noisy Edges. We conduct experiments to evaluate the performance of
pGNNs on graphs with noisy edges by randomly adding edges to the graphs and randomly remove
the same number of original edges. We define the random edge rate as r := #黑；黑：".The exper-
iments are conducted on 4 homophilic datasets (Computers, Photo, CS, Physics) and 2 heterophilic
datasets (Wisconsin, Texas) with r = 0.25, 0.5, 1. Fig. 3 reports the results on Computers, Wiscon-
sin and we defer more results to Appendix F.4. Fig. 3 shows that pGNNs significantly outperform
all baselines. Specifically, 1.5GNN obtains the best performance on Computers, and 1.5GNN and
2.0GNN even work as well as MLP on Computers with completely random edges (r = 1). For
Wisconsin, 1.0GNN is the best, and 1.0GNN and 1.5GNN significantly dominate the others. We also
observed that APPNP and GPRGNN, whose architectures are analogical to 2.0GNN, also work bet-
ter than other GNNs. Nevertheless, they are significantly outperformed by pGNNs overall. Similar
results can be observed in the experiments conducted on more datasets as presented in Appendix F.4.
Conclusion. We have addressed the problem of generalizing GNNs to heterophilic graphs
and graphs with noisy edges. To this end, we derived a novel P-Laplacian message passing scheme
from a discrete regularization framework and proposed a new pGNN architecture. We theoretically
demonstrate our method works as low-pass and high-pass filters and thereby applicable to both
homophilic and heterophilic graphs. We empirically validate our theoretical results and show the
advantages of our methods on heterophilic graphs and graphs with non-informative topologies.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
In order to ensure reproducibility, we have made the efforts in the following respects: (1) Provide a
sampled code as the supplementary material; (2) Provide self-contained proofs of the main claims
in Appendices C and D; (3) Provide more details on experimental configurations in Appendix E and
experimental results in Appendix F. All the datasets are publicly available as described in the main
text. We will fully release our training and evaluation code, as well as our train/validation/test splits.
References
Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A. Alemi. Watch your step:
Learning node embeddings via graph attention. In Samy Bengio, Hanna M. Wallach, Hugo
Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 31, NeurIPS 2018, December 3-8, 2018, Montreal, Canada,
pp.9198-9208,2018.
Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-GCN: multi-scale graph
convolution for semi-supervised node classification. In Amir Globerson and Ricardo Silva (eds.),
Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019,
Tel Aviv, Israel, July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pp.
841-851. AUAI Press, 2019.
S. Amghibech. Eigenvalues of the discrete p-laplacian for graphs. Ars Combinatoria, 67:283-302,
2003.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Daniel D. Lee,
Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 29, NeurIPS 2016, December 5-10, 2016, Barcelona,
Spain, pp. 1993-2001, 2016.
Francis Bach and Michael Jordan. Learning spectral clustering. Advances in Neural Information
Processing Systems, NIPS 2004, 16(2):305-312, 2004.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, VinIciUs Flores
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,
Caglar GUlCehre, H. Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish
Vaswani, Kelsey R. Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan
Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu.
Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261, 2018.
Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for laplacian-based manifold
methods. Journal of Computer and System Sciences, 74(8):1289-1308, 2008.
Mikhail Belkin, Irina Matveeva, and Partha Niyogi. Regularization and semi-supervised learning on
large graphs. In The 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada,
July 1-4, 2004, volume 3120, pp. 624-638, 2004.
Misha Belkin, Partha Niyogi, and Vikas Sindhwani. On manifold regularization. In Robert G. Cow-
ell and Zoubin Ghahramani (eds.), Proceedings of the Tenth International Workshop on Artificial
Intelligence and Statistics, AISTATS 2005, Bridgetown, Barbados, January 6-8, 2005. Society for
Artificial Intelligence and Statistics, 2005.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally con-
nected networks on graphs. In The 2nd International Conference on Learning Representations,
ICLR 2014, Banff, AB, Canada, April 14-16, 2014, 2014.
Thomas BUhler and Matthias Hein. Spectral clustering based on the graph P-laplacian. In An-
drea Pohoreckyj Danyluk, Leon Bottou, and Michael L. Littman (eds.), Proceedings of the 26th
Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada,
June 14-18, 2009, volume 382 of ACM International Conference Proceeding Series, pp. 81-88.
ACM, 2009.
10
Under review as a conference paper at ICLR 2022
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems 29, NeurIPS 2016, December 5-10, 2016, Barcelona, Spain, pp. 3837-3845, 2016.
Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual stochastic
block models. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, NicoIo
Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
31, NeurIPS 2018, December 3-8, 2018, Montreal, Canada,pp. 8590-8602, 2018.
David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gomez-Bombarelli, Tim-
othy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular fingerprints. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi
Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28,
NeurIPS 2015 December 7-12, 2015, Montreal, Quebec, Canada, pp. 2224-2232, 2015.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
CoRR, abs/1903.02428, 2019.
Guoji Fu, Yifan Hou, Jian Zhang, Kaili Ma, Barakeel Fanseu Kamhoua, and James Cheng. Under-
standing graph neural networks from graph signal denoising perspectives. CoRR, abs/2006.04386,
2020.
Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks.
IEEE Transactions on Signal Processing, 68:5680-5695, 2020.
Vikas K. Garg, Stefanie Jegelka, and Tommi S. Jaakkola. Generalization and representational lim-
its of graph neural networks. In Proceedings of the 37th International Conference on Machine
Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine
Learning Research, pp. 3419-3430. PMLR, 2020.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad-
vances in Neural Information Processing Systems 17, NIPS 2004, December 13-18, 2004, Van-
couver, British Columbia, Canada], pp. 529-536, 2004.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30, NeurIPS 2017, 4-9 December
2017, Long Beach, CA, USA, pp. 1024-1034, 2017.
Matthias Hein. Uniform convergence of adaptive graph-based regularization. In The 19th Annual
Conference on Learning Theory, COLT 2006, Pittsburgh, PA, USA, June 22-25, 2006, volume
4005, pp. 50-64, 2006.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. CoRR, abs/1506.05163, 2015.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin
(eds.), Advances in Neural Information Processing Systems 33, NeurIPS 2020, December 6-12,
2020, virtual, 2020.
Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label
propagation and simple models out-performs graph neural networks. In 9th International Confer-
ence on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-
view.net, 2021.
11
Under review as a conference paper at ICLR 2022
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and StePhan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Guohao Li, Matthias Muller, Ali K. ThabeL and Bernard Ghanem. Deepgcns: Can gcns go as deep
as cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
Korea (South), OCtober 27 - November 2, 2019, pp. 9266-9275. IEEE, 2019.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Thirty-SeCond AAAI ConferenCe on ArtifiCial IntelligenCe, AAAI
2018, New Orleans, Louisiana, USA, February 2-7, 2018, pp. 3538-3545, 2018.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S. Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In 7th International ConferenCe on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Fei Liu, Sounak Chakraborty, Fan Li, Yan Liu, and Aurelie C Lozano. Bayesian regularization via
graph laplacian. Bayesian Analysis, 9(2):449-474, 2014.
Jialu Liu and Jiawei Han. Spectral clustering. In Charu C. Aggarwal and Chandan K. Reddy (eds.),
Data Clustering: Algorithms and AppliCations, pp. 177-200. CRC Press, 2013.
Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, and Jiliang Tang. Elastic
graph neural networks. In Marina Meila and Tong Zhang (eds.), ProCeedings of the 38th Inter-
national ConferenCe on MaChine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume
139 of ProCeedings of MaChine Learning ResearCh, pp. 6837-6849. PMLR, 2021.
Andreas Loukas. What graph neural networks cannot learn: depth vs width. In 8th International
ConferenCe on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net, 2020.
Dijun Luo, Heng Huang, Chris H. Q. Ding, and Feiping Nie. On the eigenvectors of p-laplacian.
MaChine Learning, 81(1):37-51, 2010.
Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for
semantic role labeling. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), ProCeedings
of the 2017 ConferenCe on EmpiriCal Methods in Natural Language ProCessing, EMNLP 2017,
Copenhagen, Denmark, September 9-11, 2017, pp. 1506-1515. Association for Computational
Linguistics, 2017.
Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Semi-supervised learning with the graph laplacian:
The limit of infinite unlabelled data. AdvanCes in Neural Information ProCessing Systems, NIPS
2009, 22:1330-1338, 2009.
Partha Niyogi. Manifold regularization and semi-supervised learning: some theoretical analyses.
Journal of MaChine Learning ResearCh, 14(1):1229-1250, 2013.
Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
CoRR, abs/1905.09550, 2019.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In 8th International ConferenCe on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford InfoLab, 1999.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In 8th International ConferenCe on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
12
Under review as a conference paper at ICLR 2022
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2), 2021.
Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In
6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
Collective classification in network data. AIMagazine, 29(3):93-106, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. CoRR, abs/1811.05868, 2018.
Vikas Sindhwani, Partha Niyogi, Mikhail Belkin, and Sathiya Keerthi. Linear manifold regulariza-
tion for large scale semi-suPervised learning. In Proceedings of the 22nd ICML Workshop on
Learning with Partially Classified Training Data, volume 28, 2005.
Dejan SlePcev and Matthew ThorPe. Analysis of p-laPlacian regularization in semi-suPervised learn-
ing. CoRR, abs/1707.06213, 2017.
Alexander J. Smola and Risi Kondor. Kernels and regularization on graphs. In Bernhard Scholkopf
and Manfred K. Warmuth (eds.), Computational Learning Theory and Kernel Machines, 16th
Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel
2003, Washington, DC, USA, August 24-27, 2003, Proceedings, volume 2777 of Lecture Notes in
Computer Science, pp. 144-158. Springer, 2003.
Kiran Koshy Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph
neural network for semi-supervised learning. CoRR, abs/1803.03735, 2018.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 9(86):2579-2605, 2008.
Jesper E. van Engelen and Holger H. Hoos. A survey on semi-supervised learning. Machine Learn-
ing, 109(2):373-440, 2020.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, 2018.
Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R. Devon
Hjelm. Deep graph infomax. In 7th International Conference on Learning Representations, ICLR
2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural net-
works. In The 25th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 1539-1548, 2019.
Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395-416,
2007.
Hongwei Wang and Jure Leskovec. Unifying graph convolutional neural networks and label propa-
gation. CoRR, abs/2002.06755, 2020.
Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 6861-
6871, 2019.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning System, 32(1):4-24, 2021.
Zhang Xinyi and Lihui Chen. Capsule graph neural network. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,
2019.
13
Under review as a conference paper at ICLR 2022
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings of
the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stock-
holm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp.
5449-5458. pmlr, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural net-
works? In 7th International Conference on Learning Representations, ICLR 2017 New Orleans,
LA, USA, May 6-9, 2019, Conference Track Proceedings, 2019.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Samy Bengio, Hanna M.
Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 31, NeurIPS 2018, December 3-8, 2018,
Montreal, Canada,pp. 4805T815, 2018.
Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. In Advances in Neural Information Process-
ing Systems 32, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 9240-9251,
2019.
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. prasanna.
Graphsaint: Graph sampling based inductive learning method. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net, 2020.
Dengyong Zhou and Bernhard Scholkopf. Regularization on discrete spaces. In The 27th DAGM
Symposium, Vienna, Austria, August 31 - September 2, 2005, volume 3663, pp. 361-368, 2005.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf.
Learning with local and global consistency. In Sebastian Thrun, Lawrence K. Saul, and Bernhard
Scholkopf (eds.), Advances in Neural Information Processing Systems 16, NIPS 2003, December
8-13, 2003, Vancouver and Whistler, British Columbia, Canada], pp. 321-328. MIT press, 2003.
Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applica-
tions. AI Open, 1:57-81, 2020.
Xueyuan Zhou and Mikhail Belkin. Semi-supervised learning by higher order regularization. In
Geoffrey J. Gordon, David B. Dunson, and Miroslav Dudlk (eds.), Proceedings ofthe Fourteenth
International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale,
USA, April 11-13, 2011, volume 15 of JMLR Proceedings, pp. 892-900. JMLR.org, 2011.
Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. In Advances in
Neural Information Processing Systems 33, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Jiong Zhu, Ryan A. Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K. Ahmed, and Danai
Koutra. Graph neural networks with heterophily. In 35th AAAI Conference on Artificial Intelli-
gence, AAAI 2021, Virtual Event, February 2-9, 2021, pp. 11168-11176. AAAI press, 2021.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaus-
sian fields and harmonic functions. In Tom Fawcett and Nina Mishra (eds.), Machine Learning,
Proceedings of the Twentieth International Conference, ICML 2003, August 21-24, 2003, Wash-
ington, DC, USA, pp. 912-919. AAAI press, 2003.
Marinka Zitnik and Jure Leskovec. predicting multicellular function through multi-layer tissue
networks. Bioinform., 33(14):i190-i198, 2017.
14
Under review as a conference paper at ICLR 2022
Appendix
Contents
A	Related Work	16
B	Discussions and Future Work	17
C	Additional Theorems	18
C.1	Theorem 4 (Upper-Bounding Risk ofpGNN) ................................. 18
C.2	Theorem 5 (p-Orthogonal Theorem (Luo et al., 2010)) .................... 18
C.3	Theorem 6 (p-Eigen-Decomposition of ∆p) ................................ 18
C.4	Theorem 7 (Bounds of p-Eigenvalues) .................................... 19
D	Proof of Theorems	19
D.1 Proof of Theorem 1 ....................................................... 19
D.2	Proof of Theorem 2 ..................................................... 20
D.3	Proof of Theorem 3 ..................................................... 22
D.4	Proof of Theorem 4 ..................................................... 24
D.5	Proof of Theorem 6 ..................................................... 25
D.6	Proof of Theorem 7 ..................................................... 25
D.7	Proof of Proposition 1 ................................................. 27
E	Dataset Statistics and Hyperparameters	29
E.1	Illustration of Graph Gradient and Graph Divergence .................... 29
E.2	Dataset Statistics ..................................................... 29
E.3	Hyperparameter Settings ................................................ 29
F	Additional Experiments	31
F.1	Experimental Results on Homophilic Benchmark Datasets .................. 31
F.2	Experimental Results of Aggregation Weight Entropy Distribution ........ 31
F.3	Experimental Results on cSBM ........................................... 34
F.4	Experimental Results on Graphs with Noisy Edges ........................ 34
F.5	Experimental Results of Intergrating pGNNs with GCN and JKNet .......... 36
F.6	Experimental Results of pGNNs on PPI Dataset for Inductive Learning .... 37
F.7	Experimental Results of pGNNs on OGBN arXiv Dataset .................... 37
F.8	Running Time of pGNNs .................................................. 38
F.9	Experimental Results on Benchmark Datasets for 64 Hidden Units ......... 38
F.10	Training Curves for p = 1 .............................................. 39
F.11 Visualization Results of Node Embeddings ................................ 40
15
Under review as a conference paper at ICLR 2022
A Related Work
Graph Neural Networks. Graph neural networks (GNNs) are a variant of neural networks for
graph-structured data, which can propagate and transform the node features over the graph topol-
ogy and exploit the information in the graphs. Graph convolutional networks (GCNs) are one type
of GNNs whose graph convolution mechanisms or the message passing schemes were mainly in-
spired by the field of graph signal processing. Bruna et al. (2014) defined a nonparametric graph
filter using the Fourier coefficients. Defferrard et al. (2016) introduced Chebyshev polynomial to
avoid computational expensive eigen-decomposition of Laplacian and obtain localized spectral fil-
ters. GCN (Kipf & Welling, 2017) used the first-order approximation and reparameterized trick to
simplify the spectral filters and obtain the layer-wise graph convolution. SGC (Wu et al., 2019)
further simplify GCN by removing non-linear transition functions between each layer. Chen et al.
(2018) propose importance sampling to design an efficient variant of GCN. Xu et al. (2018) explored
a jumping knowledge architecture that flexibly leverages different neighborhood ranges for each
node to enable better structure-aware representation. Atwood & Towsley (2016); Liao et al. (2019);
Abu-El-Haija et al. (2019) exploited multi-scale information by diffusing multi-hop neighbor in-
formation over the graph topology. Wang & Leskovec (2020) used label propagation to improve
GCNs. Klicpera et al. (2019) incorporated personalized PageRank with GCNs. Liu et al. (2021)
introduced a l1 norm-based graph smoothing term to enhance the local smoothnesss adaptivity of
GNNs. Hamilton et al. (2017); Zeng et al. (2020) proposed sampling and aggregation frameworks
to extent GCNs to inductive learning settings. Another variant of GNNs is graph attention net-
works (Velickovic et al., 2018; Thekumparampil et al., 2018; Abu-El-Haija et al., 2018), which use
attention mechanisms to adaptively learn aggregation weights based on the nodes features. There
are many other works on GNNs (Pei et al., 2020) (Ying et al., 2018; Xinyi & Chen, 2019; Velick-
ovic et al., 2019; Zeng et al., 2020), we refer to Zhou et al. (2020); Battaglia et al. (2018); Wu et al.
(2021) for a comprehensive review. Most GNN models implicitly assume that the labels of nodes
and their neighbors should be the same or consistent, while it does not hold for heterophilic graphs.
Zhu et al. (2020) investigated the issues of GNNs on heterophilic graphs and proposed to separately
learn the embeddings of ego-node and its neighborhood. Zhu et al. (2021) proposed a framework to
model the heterophily or homophily levels of graphs. Chien et al. (2021) incorporated generalized
PageRank with graph convolution to adapt GNNs to heterophilic graphs.
There are also some works on the interpretability of GNNs proposed recently. Li et al. (2018); Ying
et al. (2019); Fu et al. (2020) showed that spectral graph convolutions work as conducting Laplacian
smoothing on the graph signals and Wu et al. (2019); NT & Maehara (2019) demonstrated that
GCN, SGC work as low-pass filters. Gama et al. (2020) studied the stability properties of GNNs.
Xu et al. (2019); Oono & Suzuki (2020); Loukas (2020) studied the expressiveness of GNNs. Verma
& Zhang (2019); Garg et al. (2020) work on the generalization and representation power of GNNs.
Graph based Semi-supervised Learning. Graph-based semi-supervised learning works under the
assumption that the labels of a node and its neighbors shall be the same or consistent. Many meth-
ods have been proposed in the last decade, such as Smola & Kondor (2003); Zhou et al. (2003);
Belkin et al. (2004) use Laplacian regularization techniques to force the labels of linked nodes to
be the same or consistent. ZhoU & ScholkoPf (2005) introduce discrete regularization techniques to
impose different regularizations on the node features based on p-Laplacian. Lable propagation (Zhu
et al., 2003) recursively ProPagates the labels of labeled nodes over the graPh toPology and use
the convergence results to make predictions. To mention but a few, We refer to Zhou & Scholkopf
(2005); van Engelen & Hoos (2020) for a more comPrehensive review.
16
Under review as a conference paper at ICLR 2022
B Discussions and Future Work
In this section, we discuss the future work of pGNNs. Our theoretical results and experimental
results could lead to several potential extensions of pGNNs.
New Paradigm of Designing GNN Architectures. We bridge the gap between discrete regulariza-
tion framework, graph-based semi-supervised learning, and GNNs, which provides a new paradigm
of designing new GNN architectures. Following the new paradigm, researchers could introduce
more regularization techniques, e.g., Laplacian regularization (Smola & Kondor, 2003; Belkin et al.,
2004), manifold regularization (Sindhwani et al., 2005; Belkin et al., 2005; Niyogi, 2013), high-
order regularization (Zhou & Belkin, 2011), Bayesian regularization (Liu et al., 2014), entropy reg-
ularization (Grandvalet & Bengio, 2004), and consider more explicit assumptions on graphs, e.g.
the homophily assumption, the low-density region assumption (i.e. the decision boundary is likely
to lie in a low data density region), manifold assumption (i.e. the high dimensional data lies on
a low-dimensional manifold), to develop new graph convolutions or message passing schemes for
graphs with specific properties and generalize GNNs to a much broader range of graphs. Moreover,
the paradigm also enables us to explicitly study the behaviors of the designed graph convolutions or
message passing schemes from the theory of regularization (Belkin & Niyogi, 2008; Niyogi, 2013;
Slepcev & Thorpe, 2017).
Applications of p GNNs to learn on graphs with noisy topologies. The empirical results (as shown
in Fig. 3 and Tables 6 and 7) on graphs with noisy edges show that pGNNs are very robust to noisy
edges, which suggests the applications of p-Laplacian message passing and pGNNs on the graph
learning scenarios where the graph topology could potentially be seriously intervened.
Integrating with existing GNN architectures. As shown in Table 9, the experimental results on
heterophilic benchmark datasets illustrate that integrating GCN, JKNet with pGNNs can signifi-
cantly improve their performance on heterophilic graphs. It shows that pGNN could be used as a
plug-and-play component to be integrated into existing GNN architectures and improve their perfor-
mance on real-world applications.
Inductive learning for pGNNs. pGNNs are shown to be very effective for inductive learning on
PPI datasets as reported in Table 10. pGNNs even outperforms GAT on PPI, while using much
fewer parameters than GAT. It suggests the promising extensions of p GNNs to inductive learning on
graphs.
17
Under review as a conference paper at ICLR 2022
C Additional Theorems
C.1 Theorem 4 (Upper-Bounding Risk of pGNN)
Theorem 4 (Upper-bounding risks of pGNNs). Given a graph G = (V, E, W) with N nodes, let
X ∈ RN×c be the node features and y ∈ RN be the node labels and M(k), α(k), βk, Fk are
updated accordingly by Equations (12) to (14) for k = 0, 1, . . . , K - 1 and F(0) = X, K ∈ N.
Assume that G is d-regular and the ground-truth node features X* = X + G where e ∈ RN×c
represents the noise in the node features and there exists a L-Lipschitz function σ : RN ×c → RN
such that σ(X*) = y. let y(k+1) = α⑹DT/2M(k)DT/2σ(F(k)) + β(k)σ (F(O)), we have
1N	1N
N	|yi - yi| ≤ N ΣS β(,i D lyi-σ(Xi,J|
τ N	K—2 K—2 / N M(l) ∖
+	N X α(K-1) ∆PKT)F(K T + XY (X Mj I ∆Pk)Xi,:
i=1	k=0 l=k j=1
N
+	N X(1 -βif D) llɛi,:k .
i=1
Proof see Appendix D.4. Thm. 4 shows that the risk of pGNNs is upper-bounded by the sum of three
terms: The first term of the r.h.s in the above inequation represents the risk of label prediction using
only the original node features X, the second term is the norm of p-Laplacian diffusion on the node
features X, and the third term is the magnitude of the noise in the node features. αi,i and βi,i control
the trade-offbetween these three terms and they are related to the hyperparameter μ in Eq. (10). The
smaller μ, the smaller βi,i and larger αi,i, thus the more important of the P-LaP山Cian diffusion term
but also the more effect from the noise. Therefore, for graphs whose topological information is not
helpful for label prediction, we could impose more weights on the first term by using a large μ so
that pGNNs work more like MLPs which simply learn on node features. While for graphs whose
topological information is helpful for label prediction, we could impose more weights on the second
term by using a small μ so that PGNNS can benefit from p-Laplacian smoothing on node features.
In practice, to choose a proper value of μ one may first simply apply MLPS on the node features to
have a glance at the helpfulness of the node features. If MLPs work very well, there is not much
space for the graph’s topological information to further improve the prediction performance and
we may choose a large μ. Otherwise, there could be a large chance for the graph,s topological
information to further improve the performance and we should choose a small μ.
C.2 THEOREM 5 (p-ORTHOGONAL THEOREM (LUO ET AL., 20 10))
Theorem 5 (p-Orthogonal Theorem (Luo et al., 2010)). If u(l) and u(r) are two eigenvectors of
p-Laplacian ∆p associated with two different non-zero eigenvalues λl and λr, W is symmetric and
p ≥ 1, then u(l) and u(r) are p-orthogonal up to the second order Taylor expansion.
Thm. 5 implies that φp(u)(l)>φp(u(r)) ≈ 0, for all l, r = 0, . . . ,N - 1 and λl 6= λr. Therefore, the
space spanned by the multiple eigenvectors of the graph p-Laplacian is p-orthogonal.
C.3 THEOREM 6 (p-EIGEN-DECOMPOSITION OF ∆p)
Theorem 6 (p-Eigen-Decomposition of ∆p). Given the p-eigenvalues {λl ∈ R}l=0,1,...,N-1, and
the p-eigenvectors {u(l) ∈ RN}l=0,1,...,N -1of p-Laplacian ∆p and lu(l) lp = (PiN=1(ui(l))p)1/p =
1, let U be a matrix of p-eigenvectors with U = (u(0) , u(1), . . . , u(N -1)) and Λ be a diagonal
matrix with Λ = diag(λ0, λ1, . . . , λN-1), then the p-eigen-decomposition of p-Laplacian ∆p is
given by
∆p = Φp(U)ΛΦp(U)>.
When p = 2, it reduces to the standard eigen-decomposition of the Laplacian matrix.
Proof see Appendix D.5.
18
Under review as a conference paper at ICLR 2022
C.4 THEOREM 7 (BOUNDS OF p-EIGENVALUES)
Theorem 7 (Bounds of p-Eigenvalues). Given a graph G = (V, E, W), if G is connected and λ
is a p-eigenvalue associated with the p-eigenvector u of ∆p, let Ni denotes the number of edges
connected to node i, Nmin = min{M}i=1,2,…,n, and k = argmax({∣Ui∣∕pDiJ}i=1,2,…,n ) ,then
1.	forp ≥ 2, 0 ≤ λ ≤ 2p-1;
2.	for 1 <p< 2, 0 ≤ λ ≤ 2p-1√Nk;
3.	for P = 1, 0 ≤ λ ≤ √Nmin.
Proof see Appendix D.6.
D Proof of Theorems
D.1 Proof of Theorem 1
Proof. Let i be the one-hot indicator vector whose i-th element is one and the other elements are
zero. Then, we can obtain the personalized PageRank on node i, denoted as πPPR(i), by using the
recurrent equation (Klicpera et al., 2019):
∏PP+1)(i) = aD-1/2 WDT/2nPPR(i) + e口
where k is the iteration step, 0 < α < 1 and β = (1 - α) represents the restart probability. Without
loss of generality, suppose πP(0P)R(i) = i. Then we have,
∏PPR(i) = αD-1∕2WDT∕2πPPRI) (i) + βi
=aD-1/2WDT/2 ,DT/2 WDT/2nPpR 2)(i) + g0 + gi
=(aD-1/2WDT/2)2 ∏ppR2)(i) + eaD-1/2WDT/2i + /£
k	k-1	t
=1口-1/2WDT/2) ∏PpR(i) + β X [d-"WDT/2) i
t=0
=(aD-1/2 WDT/2) k i + β χ (aD-1/2WDT/2) t £
t=0
Since 0 < α < 1 and the eigenvalues of D-1/2WD-1/2 in [-1, 1], we have
lim (aDT∕2WDT∕2)k =0,
k→∞
and we also have
Jim X 触-1/2WDT/2)t =卜N — aD-1/2WDT/2)-1 .
k→∞ t=0
Therefore,
∏PPR(i) = lim ∏PPR(i) = β (IN - aD-1/2WDT/2)- i
k→∞
=β(α∆2+(1-α)IN)-1i
=Mδ2 + μIN ) 'i,
where we let ɑ = ι+μ and β = ι+μ, μ > 0. Then the fully personalized PageRank matrix can be
obtained by substituting i with IN :
∏PPR = μ(∆2 + μIN )-1.
□
19
Under review as a conference paper at ICLR 2022
D.2 Proof of Theorem 2
Proof. By the definition of Lp(f) in Eq.(8), We have for some positive real value μ,μ > 0
2X XI 而 Fi
Lp(F)
Wij F
Djj Fj,：
+ μ X kFi,：- Xi,：k2.
i=1
—
and by Eq. (12),
Mi(,kj) := Wi,j
—
Wj
Djj
p-2
Then, We have
∂Lp(F(k))
∂F(j)
—
2μ(F(?- Xi,：)
P
αii
(k) X	Mij)	F(k) J R(k)χ
αi,ij=1 PD^jj Fj，： + βi,i Xi，：
P
而
k)	F(k+1)
: - i,:
Which indicates that
(k)
k) - F(k+1) = αi,i
:	i,:	p
∂Lp(FH))
∂F(j)
For all i, j ∈ [N], v ∈ R1×c, denote by
N
j=1
+1) := α0i(,ik) X
20
Under review as a conference paper at ICLR 2022
Then
J⅛ (Fik)+V-Fiik+1))
P
Ok
J。1
—
—
≤ Pvk+Ia (Fik)-Fiik)
P
αi,i
GM +
_____p_ F0(k+1) + _P_ F(k + 1)
0(k)Fi,:	+	(k)Fi,:
αi,i	αi,i
I N	M 0(k)	N
⅛) kvk + P ∣(∑ ⅛- - X
αiii	I	j=1	iii	j=1
II / N	M0(k)	N
⅛ kvk+p ∣∣(χ ⅛- - χ
αiii	I	j=1	iii	j=1
/ N M (k)	∖	N
(PX MS + 2μ) kvk + P∑
P住Ms
j=1
j=1
Di,i
Di,i
Mi0,(jk)
N
Fi(,k:k - X
j=1
N
Fi(,k:k - X
j=1
Msk
DDDjDjj
Msk
DDDjJjj
F(k) - 2μx + XX	MS)	F(k) + 2μx
Fj,:	P Xi,: + j=l PDiDSFj: + p X
F(k) + XX	Miij	F(k)
S + j=1 PDTDJ S
Di,i
Mi(,kjk kvk + P
Mi0,(jkk
Mi0,(jk)
j=1
Di,i
D DDDjj
+ 孑 +o (P, v, x, G )1 kvk.
—
—
—
N
—
—
Therefore, there exists some real positive value μ ∈ o (p, v, X, G) > 0 such that
N	(k)
∣∣dLp(Fik)+V)-叫(Fik))∣∣ ≤ P(X -Ds ")同=ɪ mi.
j=1	iii	P	αiii
(19)
Let γ = (γ1 , . . . , γN )> ∈ RN and η ∈ RN ×c. By Taylor’s theorem, we have:
Lp(Fi(,k:) +γiηi,:)
1
=Lp(F(k)) + Yi / hdLp(F(k) + eγiηi,j, ni,:ide
0
1
=Lp(F(k)) + Yihηi,,∂Lp(F(k)i + Yi	h∂Lp(Fik) + eγim,∙) - ∂Lp(F(?), wJde
0
≤ Lp(F(k)) + Yihni,:,dLp(FW)i + Yi / k∂Lp(F(? + eγiηi,.) - »£「(吸||帆,』也
0
≤ Lp (F(k)) + Yihni,:, dLp(F(k) )i +	Pky γ2kni,: k2
2αi,i
21
Under review as a conference paper at ICLR 2022
Let η = -VLp(F(k)) and choose some positive real value μ which depends on X, G,p andP > 1,
i.e. μ ∈ o (p, X, G). By Eq. (19), We have for all i ∈ [N],
Lp(ER-]∂Lp(Ea) ≤ Lp(F(?) -hγi∂Lp(F(k)),∂Lp(F(k))i + Gγ2∣∣∂Lp(F(k))k2
— Y2	k∂Lp(Ei,：)k2
LN- 2α⅛) [ ⅛ɪ -Q- g! ] MLp(F(k))『.
(k)
Then for all i ∈ [N], when 0 ≤ Yi ≤ αiγ-, we have Sp(Fik) - YidSp(Ftk))) ≤ Sp(F(k)) and
Yi =及 minimizes Sp(Ftk) — YidSp(Fik))). Therefore,
Lp(F(k+1)) = Lp(F(k) - 1 ∙ α(k)VLp(F(k)) ≤ Lp(F(k)).
□
D.3 Proof of Theorem 3
Proof. Without loss of generality, suppose F(O) = X. Denote M(k) = DT/2M(k)D-1/2, by
Eq. (14), we have for K ≥ 2,
F(K) = a(K-1)D-1/2M(K-1)D-1/2F(K-1) + βtKT)X
=a(KT)M KTF(KT) + β(K-1)X
=a(KT)MKT (α(K-2)MK-2F(K-2) + e(K-2)X)+ β(KT)X
=a(KT)a(K-2)M KTM K-2F(K-2) + a(KT)M KTe(K-2)X + β(K-1)X
K-1	K-1	K-1	K-1
Y a(k)	Y M(k)	F(0)	+	X	Y α(l)M(I)	e(K-1-k)X +	e(KT)X
k=0	k=0	k=1	l=K-k
K-1	K-1	K-1	K-1
Y a(k)	Y M(k)	X	+ X	Y a(I)M(I)	e(K-1-k)X + β(K-1)X.
k=0	k=0	k=1	l=K-k
(20)
Recall Equations (12) and (13), we have
and
, 2, . . . , N.
, foralli,j = 1,2, . . . ,N,
Note that the eigenvalues of M are not infinity and 0 <αi,i < 1 for all i = 1, . . . , N. Then we have
K-1
and
lim
K→∞
lim
K→∞
α(k)
k=0
0,
K-1	K-1
Y a(k)	Y	M(k)
k=0	k=0
0.
22
Under review as a conference paper at ICLR 2022
Therefore,
lim F(K) = lim
K→∞	K→∞
Y a(I)MI(I)) β(KTi)X + β(KT)X).
l=K -k
(21)
By Equations (6) and (12), we have
NW	N	W
∆pf (i) = X Wj k(Vf )([j,i])kp-2f (i) - X -7=ij= k(Vf )([j,i])kp-2f(j)
j=1 Di,i	j=1	Di,i Dj,j
=X Mij f ⑴-X	Mij— f (j)
=j=1 DJi & PDTDj f (j).
By Eq. (13), we have
(22)
G Mg) _
Di^~ =而-万
j=1 i,i	αi,i	p
Equations (22) and (23) show that
(23)
which indicates
-1
—
-MI (k),
(24)
α(k)M⅛(k) = IN - 2μα(k) - α(k)∆Pk).
(25)
Eq. (25) shows that a(k)Ml(k) is linear w.r.t ∆p and therefore can be expressed by a linear Combi-
nation in terms of ∆p:
a(k) MI (k) = θ0(k)∆p,
where θ0 = diag(θ00 , θ10 , . . . , θN0 -1) are the parameters. Therefore, we have
(26)
lim
K→∞
lim
K→∞
lim F(K) = lim
K→∞	K→∞
lim
K→∞
K -1
Y α(I)M(I)	β(KTi)X + β(KT)X
l=K -k
Y θ0(l)∆p β(K -1-k)X + β(K -1)X
l=K -k
X- β(K -1-k)	Y- θ0(l) ∆kpX + β(K -1)X
k=1	l=K -k
K -1
X θ00(k)∆pkX,
k=0
where θ00(k) = diag(θ100(k), θ200(k), . . . , θN00(k)) defined as θ00(0) = β(K -1) and
K -1
θi00(k) = βi(,Ki -1-k) Y θi0(l), fork= 1,2,...,K- 1.
l=K -k
Let θ = (θ0, θ1, . . . , θK-1) defined as θk = PiN=1 θi00(k) for all k = 0, 1, . . . , K - 1, then
lim F(K)
K→∞
lim
K→∞
θk∆pkX+θ0X
K -1
lim X θk∆kpX.
K→∞
k=0
Therefore complete the proof.
□
23
Under review as a conference paper at ICLR 2022
D.4 Proof of Theorem 4
Proof. The first-order Taylor expansion with Peano,s form of remainder for σ at X*,. is given by:
σ(FjKT)) = σ(X*,.) +	(FjKT)- X*,)' +。(帆：T)- X*,.k).
Note that in general the output non-linear layer σ(∙) is simple. Here we assume that it can be well
approximated by the first-order Taylor expansion and we can ignore the Peano,s form of remainder.
For all i = 1,...,N, Di,i = Dj,j = d, we have om PN=ι D-J2MiKT)D-y + βiKT = 1.
Then
N M (K-1)
…L X p⅛σ (FKT) TT)σ X)
B(K-DQ (X)	(K-1)	X	Mi(K	I)	(Cr(X*	) + dσ(X*，：)(F(KT)	x*「
y-βM	σ (xi,：)- %	∑. pD=D== 1σ (Xi，J + ^^(Fj,.	- XiJ
(K-1)X	Mi(KI)O	l3(K-1)	(X )	(K-1)X	Mi(KI) (dσ(X*,:)	(F(KT)	x*
…,⅛ √D=D= yi-βi,i	”" 一 ",，⅛ √D=D==l ^^(Fj,：	-X
z3(K-1)/ σ(χ ))	(K-1)X Mi(KI) (dσ(X*,:) (F(K-I) X	「、
βi,i	((U-MX:)) - αi,i	⅛ pD=Dj= (^^(Fj, .	- X: - ei,J )
≤ MKT
≤ MKT
≤ MKT
=β(K T)
∣yi-σ(Xi,.)∣ + α(K I)
X Mi(K 1) dσ(X*,：)(F(KT)	X Aτ
⅛ √d=d= F^(Fj,.	-Xij
+ (1-MKT))
∂σ(X*,.) T
∂x q,：
l(i -σ(Xi, ：)1 + 喈-I) IMI H PDS⅛ (FjKT)-Xi, ：)	+ (1 -βi(KT))
"W1 V …j,j
l (i-σ(Xi, ：) ∣+ O(KT)L
F(jK -1) - Xi,
j,:	,
+ (1-βi(K I))L llei,：k
N M(K-1)
l(i - σ(Xy) ∣ +磁T)L X Mr- (FjKT)-F(KT + WT-Xi,：)
∂σ(X*,J
∂X
怕,：k
+ (1-β(KT))L 怕,：|
N M(K-1)	N M(K-2)
卷T) ∣(i - σ(Xi,：)| + 喈T)L ∆pF(K-1) + X ^j- ∙ X ^d- (FjK-2) - Xi,：)
j'=1	j'=1
+ (1-e(KT))L 怕,：k
卷T) ∣(i-σ(Xi,J∣ +喈T)L
卷T) ∣(i-σ(Xi,J∣ +喈T)L
K-2 K-2 / N M(l)∖
△PKT)F(K T)+ X ∏ X-f- △",,： +(1-βi(KT))L 怕,：k
k=0 l = k ∖j∙ = 1	J
K-2 K-2 / N M(l) ∖
△PKT)F(K T)+ X ∏ X-f- △",,： +(1-βi(KT))L 怕,：k
k=0 l = k ∖j = 1	)
24
Under review as a conference paper at ICLR 2022
Therefore,
1N	1N
N ΣS lyi - yi| ≤ N ΣS β(,i D lyi-σ(Xi,J|
N
+ N X ɑ(K-1)
i=1
△PKT)F(K T
K-2 K-2 N
+ XY(X
k=0 l=k	j=1
∆(pk)Xi,:
N
+N x(ii(KT
i=1
) ki,: k
□
D.5 Proof of Theorem 6
Proof. Note that
N	N	NN
φp(u)>u =	Xφp(ui)ui = X kuikp-2ui2 = X kuikp =	X	|ui|p =	kukpp =	1,
i=1	i=1	i=1	i=1
then we have
∆pU = Φp(U)Λ = Φp(U)ΛΦ(U)>U.
Therefore, ∆p = Φp(U)ΛΦp(U)>.
When P = 2,by Φ2(U) = U, We get ∆2 = Φ2(U)ΛΦ2(U)> = UΛU>.	□
D.6 Proof of Theorem 7
Proof. By the definition of graph p-Laplacian, We have for all i = 1, 2, . . . , N,
λφp(ui).
Wij Wij	uj
Di,i	pDi,iDj,j ui
N
X
j=1
N
X
j=1
Wij Wij	Uj
Di,i	PDi,iDjj ui
25
Under review as a conference paper at ICLR 2022
Let l = arg max{kui k}i=1,2,...,N, the above equation holds for all i = 1, 2, . . . , N, then
N
λ=X
j=1
N
≥X
j=1
N
≥X
j=1
≥ 0.
Wlj - ʃlj Uj
Dl,l	PDTDjj Ul
Wlj -	ylj
Dl,l	PDjDjj
Wlj -	ylj
Dl,l	PDTDjj
Uj
ul
Wlj - Wj Uj
Dl,l	V Dj,j Ul
p-2
Wlj -	Wj Uj
Dl,l V Dj,j Ul
1---- p-2
Wlj - Wj Uj
Dl,l V DjjUl
p-2
When p = 1,
λ = X (Wj - ，j	Uj
j=1 ∖ Di,i	PDTDjj Ui
Wij
Dii
Wij uj
DjjUi
-1
—
≤X 再 ≤t
N
Ni X W=pNi,
where the last inequality holds by using the Cauchy-Schwarz inequality. The above inequality holds
for all i = 1, 2, . . . , N , therefore,
λ ≤ Xt /Wij ≤ pNmin.
j=1	Di,i
When p > 1, we have for i = 1, 2, . . . , N,
N
λ=X
j=1
N
≤X
j=1
N
≤X
j=1
N
Wij
Dii
Wi,j	uj
VzDiDjj Ui
p-1
Uj
Ui
p-1
Wij Uj
Djj Ui
p-2
Wj Y
Di,i /
1+√Dk∣Uj
Dj,j Ui
p-1
Without loss of generality, let k = arg max({∣Ui ∣/^Di,i}i=1,2,...,N). Because the above inequality
holds for all i = 1, 2, . . . , N, then we have
p-1
For p ≥ 2,
N
λ ≤ 2p-1 X
2p-1
26
Under review as a conference paper at ICLR 2022
For 1 < p < 2,
N
λ ≤ 2p-1 X
j=1
Wj! p ≤ 2p-1 X SWj ≤ 2p-ιt
N
Nk X Wkj = 2p-1pNk.
j=1 Dk,k
□
D.7 Proof of Proposition 1
Proof. We proof Proposition 1 based on the bounds of p-eigenvalues as demonstrated in Thm. 7.
By Eq. (6) and Eq. (12), we have
NW	N
△pf (i) = X Wj k(vf )([j, i])∣r2f (i) - X
j=1 i,i	j=1
NM	N
=X ML f (i) - X
i,i
j=1	,
j=1
M fj
pDi,iDj,j
By Eq. (13), we have
Equations (27) and (28) show that
z⅞- k(Vf )([j,i])kp-f(j)
Di,iDj,j
N M(k)
X Mij = _1_ - 2μ
Di Dii	Q(k)	P
j=1 i,i	αi,i	p
-1
—
- D-1/2M(k)D-1/2,
which indicates
a(k)D-1/2M(k)D-1∕2 = IN - 2μa(k) - α(k)∆Pk).
For i = 1,2,..., N, let α := (α1,..., aN), αi := 1/ PN=I Mj ,then
(27)
(28)
(29)
(30)
•N	Mij	_ 2	2μ ∖	ʌ
°i,iG PDOTj = (I- Wi - αi'iλi
(31)
1. When P = 2, for all i = 1,...,N, δ⅛ = 1 and 0 ≤ λi-ι ≤ 2, g2(λi-1i works as both
low-pass and high-pass filters.
2. WhenP > 2, by Thm. 7 we have for all i = 1,...,N, 0 ≤ λi-ι ≤ 2p-1. If 0 ≤ α ≤ 21-p,
then 0 ≤ 1 - aiλi ≤ 1, which indicates that gp(λi-ι) works as a low-pass filter; If
27
Under review as a conference paper at ICLR 2022
α > 21-p, then gp(λi-ι) works as both low-pass and high-pass filters. Since
X Mij
j=1 Di，i
N
X
j=1
Wijk(▽/)([i,j])kp-2
Di,i
N W 2N
X	Wj	X k(vf)([i,j])k2(p-2)
Di,i
j=1	,	j=1
uN
∖ Xk(vf)([i,j])k2(p-2)
j=1
kVf (i)kp-2,
which indicates that α⅛ ≥ ∣∣Vf(i)k2-p. 0 ≤ α⅛ ≤ 21-p directly implies that 0 ≤
∣∣Vf(i)k2-p ≤ 21-p, i.e. ∣Vf(i)k ≥ 2(PT)/(p-2) and when ∣∣Vf(i)∣∣2-p ≥ 21-p, i.e.
∣∣Vf (i)∣∣ ≤ 2(P-1)/(D, α ≥ 21-p always holds. Therefore, if ∣Vf (i)∣ ≤ 2(p-1"(P-2),
gp(λi-1) works as both low-pass and high-pass filters on node i; If gp(λi-1) works as a
low-pass filter, ∣Vf (i)∣ ≥ 2(P-1)/(P-2).
3. When 1 ≤ p < 2, by Thm. 7 We have for all i = 1,...,N, 0 ≤ λi-ι ≤ 2p-1√Nk. If
0 ≤ α ≤ 21-p∕√Nk, 0 ≤ 1 一 (⅛λi ≤ 1, which indicates that gp(λi-ι) work as low-pass
filters; If α ≥ 21-p∕√Nk, gp(λi-ι) work as both low-pass and high-pass filters. By
N
X
j=1
N
X
j=1
Wi,j
E k(Vf )([i,j])kp-2
Wij
Di,i k(Vf)([i,j])kp-2
/ ∑N=ι Wij k(Vf)([i,j])kp-2
NW
• X Wj k(Vf )([i,j])kp-2
j=1 Di,i
1
k(Vf )([i,j ])kp-2
1,
∙k(Vf )([i,j])kp-2
≤
≤
≤
∖
1
1
1
we have
1	_	1
PL Mj = Pj= Wj k(Vf )([i,j])kp-2
N
≤X
j=1
Wij	1
D7 k(Vf )([i,j ])kp-2
NW
=X ~Wij k(Vf )([i,j ])∣2-p
j=1 Di,i
≤ kVf(i)k2-p.
α% ≥ 21-p∕√Nk directly implies that ∣Vf(i)k2-p ≥ 21-p∕√Nk, i.e. ∣∣Vf(i)∣ ≥
2(2√Nk)1/(P-2) and when 0 ≤ ∣∣Vf(i)∣∣2-p ≤ 21-p∕√Nk, i.e. 0 ≤ ∣∣Vf(i)∣∣ ≤
2(2√Nk)1∕(p-' 0 ≤ α ≤ 21-p∕√Nk always holds. Therefore, if 0 ≤ ∣∣Vf(i)∣∣ ≤
2(2√Nk)1∕(p-2), gp(λi-ι) work as low-pass filters; If gp(λi-ι) work as both low-pass
and high-pass filters, ∣Vf (i)∣ ≥ 2 (2√Nk)1/(p-2).
Specifically, when P = 1, by Thm. 7 we have for all i = 1,...,N, 0 ≤ λi-ι ≤
2p-1√Nmin. Following the same derivation above we attain if 0 ≤ ∣Vf(i)∣ ≤
2(2√Nmin)1∕(p-2), gp(λi-ι) work as low-pass filters; If gp(λi-ι) work as both low-pass
and high-pass filters, ∣Vf (i)∣ ≥ 2 (2√Nmn)1∕(p-2).
□
28
Under review as a conference paper at ICLR 2022
E DATASET Statistics and HYPERPARAMETERS
E.1 illustration of Graph Gradient and Graph divergence
Figure 4: A tiny example of illustration of graph gradient and graph divergence. Best view in colors.
(divg)⑴
四
®i
E.2 dataset Statistics
Table 2 summarizes the dataset statistics and the levels of homophily H(G) of all benchmark
datasets. Note that the homophily scores here is different with the scores reported by Chien et al.
(2021). There is a bug in their code when computing the homophily scores (doing division with
torch integers) which caused their homophily scores to be smaller.
Table 2: statistics of datasets.
Dataset	#Class #Feature #Node #Edge				Training Validation		Testing H(G)	
cora	7	1433	2708	5278	2.5%	2.5%	95%	0.825
citeseer	6	3703	3327	4552	2.5%	2.5%	95%	0.717
PubMed	3	500	19717	44324	2.5%	2.5%	95%	0.792
computers	10	767	13381	245778	2.5%	2.5%	95%	0.802
Photo	8	745	7487	119043	2.5%	2.5%	95%	0.849
cs	15	6805	18333	81894	2.5%	2.5%	95%	0.832
Physics	5	8415	34493	247962	2.5%	2.5%	95%	0.915
chameleon	5	2325	2277	31371	60%	20%	20%	0.247
squirrel	5	2089	5201	198353	60%	20%	20%	0.216
Actor	5	932	7600	26659	60%	20%	20%	0.221
Wisconsin	5	251	499	1703	60%	20%	20%	0.150
Texas	5	1703	183	279	60%	20%	20%	0.097
cornell	5	1703	183	277	60%	20%	20%	0.386
E.3 Hyperparameter Settings
We set the number of layers as 2, the maximum number of epochs as 1000, the number for early
stopping as 200, the weight decay as 0 or 0.0005 for all models. The other hyperparameters for each
model are listed as below:
• 1.0 GNN, 1.5 GNN, 2.0 GNN, 2.5 GNN:
-	Number of hidden units: 16
-	Learning rate: {0.001,0.01,0.05}
-	Dropout rate: {0, 0.5}
-	μ: {0.01, 0.1, 0.2,1,10}
-	K: 4, 6, 8
29
Under review as a conference paper at ICLR 2022
• MLP:
-	Number of hidden units: 16
-	Learning rate: {0.001,0.01}
-	Dropout rate: {0, 0.5}
• GCN:
-	Number of hidden units: 16
-	Learning rate: {0.001, 0.01}
-	Dropout rate: {0, 0.5}
• SGC:
-	Number of hidden units: 16
-	Learning rate: {0.2, 0.01}
-	Dropout rate: {0, 0.5}
-	K: 2
• GAT:
-	Number of hidden units: 8
-	Number of attention heads: 8
-	Learning rate: {0.001, 0.005}
-	Dropout rate: {0, 0.6}
• JKNet:
-	Number of hidden units: 16
-	Learning rate: {0.001, 0.01}
-	Dropout rate: {0, 0.5}
-	K: 10
-	α: {0.1, 0.5, 0.7, 1}
-	The number of GCN based layers: 2
-	The layer aggregation: LSTM with 16 channels and 4 layers
• APPNP:
-	Number of hidden units: 16
-	Learning rate: {0.001, 0.01}
-	Dropout rate: {0, 0.5}
-	K: 10
-	α: {0.1, 0.5, 0.7, 1}
• GPRGNN:
-	Number of hidden units: 16
-	Learning rate: {0.001, 0.01, 0.05}
-	Dropout rate: {0, 0.5}
-	K: 10
-	α: {0, 0.1, 0.2, 0.5, 0.7, 0.9, 1}
-	dprate: {0, 0.5, 0.7}
30
Under review as a conference paper at ICLR 2022
F Additional Experiments
F.1 Experimental Results on Homophilic Benchmark Datasets
Competitive Performance on Real-World Homophilic Datasets. Table 3 summarizes the aver-
aged accuracy (the micro-F1 score) and standard deviation of semi-supervised node classification
on homophilic benchmark datasets. Table 3 shows that the performance of pGNN is very close to
APPNP, JKNet, GCN on Cora, CiteSeer, PubMed datasets and slightly outperforms all baselines on
Computers, Photo, CS, Physics datasets. Moreover, we observe that pGNNs outperform GPRGNN
on all homophilic datasets, which confirms that p GNNs work better under weak supervised infor-
mation (2.5% training rate) as discussed in Remark 3. We also see that all GNN models work
significantly better than MLP on all homophilic datasets. It illustrates that the graph topological in-
formation is helpful for the label prediction tasks. Notably, 1.0GNN is slightly worse than the other
pGNNs with larger p, which suggests to use p ≈ 2 for homophilic graphs. Overall, the results of
Table 3 indicates that pGNNs obtain competitive performance against all baselines on homophilic
datasets.
Table 3: Results on homophilic benchmark datasets. Averaged accuracy (%) for 100 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold. OOM denotes out of memory.
Method	Cora	CiteSeer	PubMed	Computers	Photo	CS	Physics
MLP	43.47±3.82	46.95±2.15	78.95±0.49	66.11±2.70	76.44±2.83	86.24±1.43	92.58±0.83
GCN	76.23±0.79	62.43±0.81	83.72±0.27	84.17±0.59	90.46±0.48	90.33±0.36	94.46±0.08
SGC	77.19±1.47	64.10±1.36	79.26±0.69	84.32±0.59	89.81±0.57	91.06±0.05	OOM
GAT	75.62±1.01	61.28±1.09	83.60±0.22	82.72±1.29	90.48±0.57	89.96±0.27	93.96±0.21
JKNet	77.19±0.98	63.32±0.95	82.54±0.43	79.94±2.47	88.29±1.64	89.69±0.66	93.92±0.32
APPNP	79.58±0.59	63.02±1.10	84.80±0.22	83.32±1.11	90.42±0.53	91.54±0.24	94.93±0.06
GPRGNN	76.10±1.30	61.60±1.69	83.16±0.84	82.78±i.87	89.81±0.66	90.59±0.38	94.72±0.16
1.0GNN	77.59±0.69	63.19±0.98	83.21±0.30	84.46±0.89	9O69±0,6	91.46±0.50	9472±o.37
1.5GNN	78.86±0.75	63.80±o.79	83.65±0.17	85.03±0.90	90.91±0.50	92J2±o.40	949O±0.i6
2.0GNN	78.93±0.60	63.65±1.08	84.19±0.22	84.39±0.85	90.40±0.63	92.28±0.47	94.93±0.14
2.5GNN	78.87±0.57	63.28±0.97	84.45±0.18	83.85±o.87	89.82±0.64	91.94±0.40	9487±o.ιι
F.2 Experimental Results of Aggregation Weight Entropy Distribution
Here we present the visualization results of the learned aggregation weight entropy distribution
of p GNNs and GAT on all benchmark datasets. Fig. 5 and Fig. 6 show the results obtained on
homophilic and heterophilic benchmark datasets, respectively.
We observe from Fig. 5 that the aggregation weight entropy distributions learned by pGNNs and
GAT on homophilic benchmark datasets are similar to the uniform cases, which indicates that ag-
gregating and transforming node features over the original graph topology is very helpful for label
prediction. It explains why pGNNs and GNN baselines obtained similar performance on homophilic
benchmark datasets and all GNN models significantly outperform MLP.
Contradict to the results on homophilic graphs shown in Fig. 5, Fig. 6 shows that the aggregation
weight entropy distributions of p GNNs on heterophilic benchmark datasets are very different from
that of GAT and the uniform cases. We observe from Fig. 6 that the entropy of most of the aggrega-
tion weights learned by p GNNs are around zero, which means that most aggregation weights are on
one source node. It indicates that the graph topological information in these heterophilic benchmark
graphs is not helpful for label prediction. Therefore, propagating and transforming node features
over the graph topology could lead to worse performance than MLPs, which validates the results in
Table 3 that the performance of MLP is significantly better most GNN baselines on all heterophilic
graphs and closed to pGNNs.
31
Under review as a conference paper at ICLR 2022
UnifOrm
iQGNN (Acc: 78.43%)
Cora
15GNN (Acc: 78.58%)
",φpou J。#
1000
=GNN (Acc: 78.39%)
GAT (Acc: 73.86%)
024β8024e8024e8024β8024eβ
entropy	entropy	entropy	entropy	entropy
Citeseer
Pubmed
uniform	10GNN (Acc: 83.2%)	15GNN (Acc: 83.73%) 2-5GNN (Acc: 84.35%)	GAT (Acc: 83.85%)
140CX>-	-	-	-	-
entropy
«	0	2	4	6
entropy
entropy
«02468024
entropy	entropy
Computers
uniform
10GNN (Acc: 85.24%)
*GNN (Acc: 85.3%)	2 5GNN (Acc: 83.68%)
GAΓ (Acc: 82.73%)
0	2	4 e 8	10 12 14
entropy
02	4	68	10 12U
entropy
LQGNN (Acc: 91.33%)
0	2	4	6	8	10 12 14
entropy
Photo
15GNN (Acc: 9L28%)
0	2	4 e 8	10 12 14
entropy
2 5g N N (Acc: 90.25%)
0	24	68	10 12U
entropy
GAr (Acc: 90.29%)
",φpou ⅛ #
4 e 8	10 12 U
entropy
entropy
0	2	4 e 8	10 12 14
entropy
entropy
uniform
2 5GNN (Acc: 92.11%)
GAr (Acc: 89.8%)
10GNN (Acc:
",φpou ⅛ #
2	3	4
entropy
5	6 -1	0	1	2	3	4	5	6 -1	0	1	2	3	4	5	6 -1	0	123456 -1	0123456
entropy	entropy	entropy	entropy
Physics
10GNN (Acc: 94.73%) 15GNN (Acc: 94.94%)	2 5GNN (Acc: 94.77%)	GtfT (Acc: 94.46%)
ZKel-----------y⅛τ-
K
uniform
4	6 β 10
entropy
0	2	4	6 β 10
entropy
0	2	4	6 β 10
entropy
4	6 β 10
entropy
0	2	4	6 β 10
entropy
Figure 5: Aggregation weight entropy distribution of homophilic benchmark graphs. Low entropy
means high degree of concentration, vice versa. An entropy of zero means all aggregation weights
are on one source node.
32
Under review as a conference paper at ICLR 2022
Squirrel
uniform	10GNN (Acc: 32.08%) 15GNN (Acc: 28.15%)	2 5GNN (Acc: 33.33%)	GAT (Acc: 28.72%)
",φpou ⅛ #
Actor
H
uniform
LOGNN (Acc: 41.21%)	15GNN (Acc: 40.63%)	2 5GNN (Acc: 39.46%)
Gtfr (Acc: 31.12%)
5000
4000-
30∞-
# 2000
10∞
entropy
entropy
entropy
entropy
entropy
Wisconsin
Texas
UnifOrm
1QGNN (Acc: 86.25%)	15GNN (Acc: 83.75%)	2.5GNN (Acc: 86.25%)
Gʌr (Acc: 48.75%)
O 1	2	3	4	5 -1 O 1	2	3	4	5 -1 O 1	2	3	4	5 -1 O 1	2	3	4	5 -1 O 1	2	3	4	5
entropy	entropy	entropy	entropy	entropy
Cornell
",φpou J。#
Figure 6: Aggregation weight entropy distribution of heterophilic benchmark graphs. Low entropy
means high degree of concentration and vice versa. An entropy of zero means all aggregation
weights are on one source node.
33
Under review as a conference paper at ICLR 2022
F.3 Experimental Results on cSBM
In this section we present the experimental results on cSBM using sparse splitting and dense split-
ting, respectively. We used the same settings in Chien et al. (2021) in which the number of nodes
n = 5000, the number of features f = 2000,	= 3.25 for all experiments. Table 4 reports the
results on cSBM with sparse splitting setting, which also are presented in Fig. 2 and discussed in
Sec. 5. Table 5 reports the results on cSBM with dense splitting settings.
Table 4: Results on cSBM with sparse splitting setting. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold.
Method	φ = -1	φ = -0.75	φ = -0.5	φ = -0.25	φ=0	φ = 0.25	φ = 0.5	φ = 0.75	φ=1
MLP	49.72±0.36	51.42±1.83	59.21±1.01	61.57±0.38	61∙70±0.30	59.92±1.88	57.20±0.62	54.48±0.48	50.09±0.51
GCN	57.24±1.15	58.19±1.46	57.30±1.30	51.97±0.44	54.45±1.38	64.70±2.38	82.45±1.35	91.31±0.54	76.07±3.30
SGC	55.98±1.48	58.56±1.40	56.97±0.54	51.54±0.22	52.69±2.36	64.14±1.05	79.88±1.57	90.37±0.09	75.94±0.92
GAT	59.72±2.23	60.20±2.14	55.38±1.96	50.15±0.55	53.05±1.40	64.00±2.03	81.04±1.71	90.37±1.33	78.24±1.95
JKNet	49.70±0.39	49.75±0.79	49.65±0.52	48.93±0.48	52.36±2.09	62.76±2.54	87.10±1.52	97.43±0.36	97.69±0.52
APPNP	48.45±0.98	49.65±0.46	53.31±0.89	56.58±0.58	60.10±0.65	65.02±2.23	82.95±1.38	95.49±0.43	89.85±0.60
GPRGNN	97.26±0.66	94.81±0.91	82.14±0.47	61.15±2.55	60.20±o.76	62.90±2.22	83.61±1.28	96.96±0.41	98.01±0.71
1.0GNN	95.75±1.21	93.06±1.13	77.39±4.21	61.38±0.39	61.80±0.29	65.73±2.11	85.85±3.24	96.80±0.87	97.40±1.10
1.5GNN	95.90±3.01	94.10±4.57	73.08±2.59	61.44±0.3O	61.77±0.35	66.01±1.88	90.57±0.71	97.38±0.43	97.76±0.86
2.0GNN	98.37±0.78	96.32±1.50	84.93±0.39	61.13±0.51	61.79±0.34	63.55±1.73	88.55±1.05	97.56±0.16	97.94±0.39
2.5GNN	97.74±0.99	96.78±0.44	83.21±2.12	61.30±O.41	61.74±0.34	62.88±2.31	79.64±2.15	95.71±0.34	97.25±0.58
Table 5: Results on cSBM with dense splitting setting. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold.
Method	φ=-1	φ = -0.75	φ = -0.5	φ = -0.25	φ=0	φ = 0.25	φ = 0.5	φ	= 0.75		φ=1
MLP	50.37±0.60	65.22±0.92	75.82±0.65	81.18±0.55	79.86±0.69	79.97±0.57	75.03±0.89	67.	53±0.68	51	.96±0.69
GCN	83.14±0.49	82.59±0.48	77.17±0.59	58.58±0.41	61.18±1.06	82.59±0.50	92.20±0.27	97.	21±0.27	97	.10±0.12
SGC	78.35±0.36	82.13±0.09	77.76±0.12	59.14±0.57	60.31±0.63	81.96±0.34	91.68±0.13	96.	56±0.09	96	.87±0.05
GAT	92.99±0.86	90.89±0.60	87.02±0.80	68.40±1.60	61.98±1.16	82.92±0.51	92.05±0.73	97.	28±0.25	98	.04±0.46
JKNet	68.95±9.05	79.21±7.67	67.97±5.22	56.12±4.10	58.33±1.70	80.15±0.80	91.21±0.50	97	62±0 25	98	.32±o 21
APPNP	49.86±0.39	50.47±0.89	65.28±0.68	73.98±0.64	79.37±0.66	86.60±0.73	92.45±0.39	97	. 67±0.14	97	. .65±0.49
GPRGNN	99∙06±0.25	97.14±0.31	94.59±0.32	83.84±0.69	78.81±1.30	85.85±1.01	92.08±0.81	97	49±0.22	98	.46±0.15
1.0GNN	98.19±0.28	94.38±0.44	86.40±1.00	80.57±0.43	80.21 ±0 42	87.32±0.47	92.42±0 62	97	52±0 33	98	37±0 26
1.5GNN	98.88±0 16	95.62±0.21	86.87±1.22	80.70±0.71	. 80.28±0.31	86.29±0.43	. 92.40±0 24	97	. 56±0 25	98	. 24±0 32
2.0GNN	. 99.21±0.09	96.91±0 16	92.96±0.31	80.83±0.61	80.04±o 49	84.96±0.60	. 91.18±0.27	97	. 41±0 14	98	. 45 ±0 14
2.5GNN	99.21±0.14	. 96.94±0.I6	93.28±0.37	80.93±0.44	. 80.28±0.38	83.83±0.70	86.10±0.39	96	. 28±0.43	97	. .76±0.18
Table 5 shows that pGNNs obtain the best performance on weak homophilic graphs (φ = 0, 0.25)
while competitive performance against GPRGNN on strong heterophilic graphs (φ = -0.75, -1)
and competitive performance with state-of-the-art GNNs on strong homophilic graphs (φ = 0.75, 1).
We also observe that GPRGNN is slightly better than p GNNs on weak heterophilic graphs (φ =
-0.25, -0.5), which suggests that GPRGNN could work very well using strong supervised infor-
mation (60% training rate and 20% validation rate). However, as shown in Table 4, p GNNs work
better than GPRGNN under weak supervised information (2.5% training rate and 2.5%) on all het-
erophilic graphs. The result is reasonable, as discussed in Remark 3 in Sec. 3.2, GPRGNN can
adaptively learn the generalized PageRank (GPR) weights and it works similarly to 2.0GNN on both
homophilic and heterophilic graphs. However, it needs more supervised information in order to
learn optimal GPR weights. On the contrary, pGNNs need less supervised information to obtain
similar results because Θ(2) acts like a hyperplane for classification. Therefore, pGNNs can work
better under weak supervised information.
F.4 Experimental Results on Graphs with Noisy Edges
Here we present more experimental results on graph with noisy edges. Table 6 reports the results on
homophilic graphs (Computers, Photo, CS, Physics) and Table 7 reports the results on heterophilic
graphs (Wisconsin Texas). We observe from Tables 6 and 7 that pGNNs dominate all baselines.
Moreover, p GNNs even slightly better than MLP when the graph topologies are completely random,
i.e. the noisy edge rate r = 1. We also observe that the performance of GCN, SGC, JKNet on
homophilic graphs dramatically degrades as the noisy edge rate r increases while they do not change
34
Under review as a conference paper at ICLR 2022
a lot for the cases on heterophilic graphs. It is reasonable since the original graph topological
information is very helpful for label prediction on these homophilic graphs. Adding noisy edges and
remove the same number of original edges could significantly degrade the performance of ordinary
GNNs. On the other hand, since we find that the original graph topological information in Wisconsin
and Texas is not helpful for label prediction. Therefore, adding noisy edges and removing original
edges on these heterophilic graphs would not affect too much their performance.
Table 6: Results on homophilic graphs with random edges. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold. OOM denotes out of memory.
Method	Computers			r = 0.25	Photo	
	r=0.25	r=0.5	r=1		r = 0.5	r=1
MLP	66.11±2.70	66.11±2.70	66.11±2.70	76.44±2.83	76.44±2.83	76.44±2.83
GCN	74.70±1.72	62.16±2.76	8.95±6.90	81.43±0.76	75.52±3.59	12.78±5.20
SGC	75.15±1.08	66.96±1.05	15.79±7.47	82.22±0.36	77.80±0.49	13.57±3.63
GAT	76.44±1.81	68.34±2.61	11.58±7.70	82.70±1.31	77.20±2.10	13.74±5.14
JKNet	56.74±6.48	46.11±8.43	12.50±6.56	73.46±6.74	64.18±4.06	15.66±6.10
APPNP	78.23±1.84	74.57±2.25	66.67±2.68	87.63±1.05	86.22±1.73	75.55±1.72
GPRGNN	77.30±2.24	77.11±1.80	66.85±i.65	85.95±1.05	85.64±1.22	77.46±1.44
1.0GNN	75.14±14.95	63.26±20.67	41.60±i6.i7	87.97±o.70	84.47±3.05	41.17±18.15
1.5GNN	81.79±1.33	78.12±2.08	66.04±2.73	88.09±1.18	86.20±1.61	68.78±8.97
2.0GNN	80.34±1.07	76.90±1.93	67.17±1.63	87.65±0.94	87.06±1.50	77.07±i.83
2.5GNN	79.14±1.51	75.49±1.25	64.95±2.27	87.38±0.85	86.11±1.10	76.65±1.46
Method	r=0.25	CS r=0.5	r=1	r = 0.25	Physics r = 0.5	r=1
MLP	86.24±1.43	86.24±1.43	86.24±1.43	92.58±0.83	92.58±0.83	92.58±0.83
GCN	81.05±0.59	68.37±0.85	7.72±2.39	89.02±0.16	80.45±0.34	19.78±3.94
SGC	83.41±0.01	71.98±0.12	8.00±1.43	OOM	OOM	OOM
GAT	80.11±0.67	68.66±1.42	8.49±2.39	88.72±0.61	82.05±1.86	22.39±5.04
JKNet	81.35±0.74	71.30±2.14	11.43±1.18	87.98±0.97	81.90±2.27	26.38±5.80
APPNP	88.63±0.68	87.56±0.51	76.90±0.96	93.46±0.12	92.81±0.24	90.49±0.33
GPRGNN	85.77±0.81	83.89±1.54	72.79±2.24	92.18±0.29	90.96±0.48	91.77±0.41
1.0GNN	90.27±0.86	89.56±0.81	86.60±1.22	94.35±0.39	94.23±0.27	92.97±0.36
1.5GNN	91.27±0.40	90.50±0.71	84.40±1.84	94.34±0.21	93.77±0.29	92.51±0.35
2.0GNN	90.97±0.49	89.98±0.50	80.84±1.48	94.14±0.18	93.30±0.31	91.72±0.44
2.5GNN	89.90±0.45	89.00±0.59	76.82±2.ii	93.61±0.30	92.77±0.26	91.16±0.47
Table 7: Results on heterophilic graphs with random edges. Average accuracy (%) for 20 runs. Best
results are outlined in bold and the results within 95% confidence interval of the best results are
outlined in underlined bold.
Method	r= 0.25	Wisconsin		Texas		
		r = 0.5	r=1	r=0.25	r = 0.5	r=1
MLP	93.56±3.14	93.56±3.14	93.56±3.14	79.50±10.62	79.50±10.62	79.50±10.62
GCN	62.31±8.12	59.44±5.76	64.21±4.49	41.56±8.89	44.69±23.05	40.31±18.26
SGC	64.68±7.34	62.36±2.64	51.81±2.63	42.50±5.49	40.94±18.34	23.81±14.54
GAT	65.37±9.04	60.05±9.12	60.05±7.46	39.50±9.29	34.88±21.59	29.38±11.53
JKNet	64.91±13.07	51.39±10.36	57.41±2.57	47.75±7.30	46.62±23.23	40.69±13.57
APPNP	70.19±9.04	60.32±4.70	72.64±4.73	66.69±13.46	63.25±9.87	69.81±7.76
GPRGNN	90.97±3.83	87.50±3.86	87.55±2.97	74.25±7.25	76.75±14.05	80.69±5.87
1.0GNN	94.91±2.73	95.97±2.00	95.97±2.27	81.50±9.24	82.12±11.09	81.81±5.67
1.5GNN	9458±i.25	95.19±2.18	94.95±2.79	82.50±6.39	78.12±5.30	78.50±7.98
2.0GNN	90.46±2.79	90.97±4.22	91.44±2.27	86.06±5.17	69.38±11.47	63.50±8.90
2.5GNN	82.45±3.93	88.24±2.79	84.40±i.98	80.00±10.83	56.62±10.01	52.31±10.58
35
Under review as a conference paper at ICLR 2022
F.5 Experimental Results of Intergrating pGNNS with GCN and JKNet
Here we further conduct experiments to study whether p GNNs can be intergrated into existing GNN
architectures and improve their performance on heterophilic graphs. We use two popular GNN
architectures: GCN (Kipf & Welling, 2017) and JKNet (Xu et al., 2018).
To incorporatepGNNs with GCN, we use the p GNN layers as the first layer of the combined models,
termed as pGNN + GCN, and GCN layer as the second layer. Specifically, we use the aggregation
weights aD-1/2MDT/2 iearæd by the PGNN in the first layer as the input edge weights of GCN
layer in the second layer. To combine pGNN with JKNet, we use the pGNN layer as the GNN layers
in the JKNet framework, termed as pGNN + JKNet. Tables 8 and 9 report the experimental results
on homophilic and heterophilic benchmark datasets, respectively.
Table 8: The results of pGNNs + GCN and pGNNs + JKNet on homophilic benchmark dataset.
Averaged accuracy (%) for 20 runs. Best results are outlined in bold and the results within 95%
confidence interval of the best results are outlined in underlined bold.
Method	Cora	CiteSeer	PubMed	Computers	Photo	CS	Physics
GCN	76.23±0.79	62.43±0.81	83.72±o.27	84.17±0.59	9O.46±0.48	9O.33±0.36	94.46±0.08
1.0GNN + GCN	72.37±1.35	60.56±1.59	82.14±0.31	83.75±1.05	9024±1.12	89.60±0.46	9459±0.33
1.5GNN +GCN	72.72±1.39	60.23±1.80	82.21±0.22	83.89±0.74	90.00±0.68	89.48±0.45	94.7O±0.18
2.0GNN + GCN	72.39±1.55	60.19±1.60	82.24±0.23	83.92 ±1.09	90.17±0.83	89.60±0.71	94.51±0.39
2.5GNN +GCN	72.85±1.19	59.68±1.85	82.23±o.34	83.69±0.92	90.02±1.09	89.53±0.68	9458±o.3i
JKNet	77.19±0.98	63.32±0.95	8254±0.43	79.94±2.47	88.29±1.64	89.69±0.66	93.92±0.32
1.0GNN+JKNet	75.67±1.54	60.38±1.65	81.68±0.44	83.19±1.36	89.71±1.05	90.26±0.72	94.27±0.69
1.5GNN+JKNet	76.40±1.59	60.67±1.93	8242±o.35	82.78±2.09	9O.25±1.03	9O.76±0.75	94.82±0.34
2.0GNN+JKNet	76.75±1.26	61.05±1.48	82.5O±0.53	82.36±2.39	89.31±1.39	90.33±0.63	947O±0.33
2.5GNN+JKNet	76.48±1.28	60.97±0.97	82.56±i.04	81.45±1.55	89.21±1.10	89.66±0.68	94.29±0.59
Table 9: The results of pGNNs + GCN and pGNNs + JKNet on heterophilic benchmark dataset.
Averaged accuracy (%) for 20 runs. Best results are outlined in bold and the results within 95%
confidence interval of the best results are outlined in underlined bold.
Method	Chameleon	Squirrel Actor	Wisconsin Texas Cornell
GCN	34.54±2.78	25.28±i.55	31.28±2.04	61.93±3.oo	56.54±i7.02	51.36±4.59
1∙0GNN + GCN	48.52±i.89	34∙78±ι.n	32.37±3.i2	68.52士3.75	67.94±i2.6o	67.81
±7.61
1.5GNN + GCN	48.85±2.i3	34.61±i.ii	32.37±2.48	66.25±3.95	65.62±ii.99	64.88±9.19
2.0GNN + GCN	48.71±2.24	35.06±1.18	32.72±2.02	66.34±4.51	65.94±7.63	68.62±6.55
2.5GNN + GCN	49.53±2.19	34.40±1.60	32.40±3.23	67.18±3.50	68.31±9.18	66.06±9.56
JKNet
1.0GNN + JKNet
1.5GNN + JKNet
2.0GNN + JKNet
2.5GNN + JKNet
33.28±3.59
49OO±2.09
48.77±2.22
48.88±1.63
49.04±1.95
25.82±1.58
35.56±1.34
35.98±0.93
35.77	±1.73
35.78	±1.87
29.77±2.61
40.74±0.98
40.22±1.27
40.16±1.31
40.00±1.12
61.08±3.71
95.23±2.43
94.86±2.00
88.84±2.78
85.42±3.86
59.65±12.62
80.25±6.87
80.38±9.79
86.12±5.59
79.06±7.60
55.34±4.43
78.38±8.14
72.25±9.83
74.75±7.81
76.81±7.66
We observe from Table 8 that intergrating p GNNs with GCN and JKNet does not improve their
performance on homophilic graphs. The performance of GCN slightly degrade after incorporating
pGNNs. The performance of JKNet also slightly degrade on Cora, CiteSeer, and PubMed but is
improved on Computers, Photo, CS, Physics. It is reasonable since GCN and JKNet can predict
well on these homophilic benchmark datasets based on their original graph topology.
However, for heterophilic benchmark datasets, Table 9 shows that there are significant improvements
over GCN, and JKNet after intergrating with pGNNs. Moreover, pGNNs + JKNet obtain advanced
performance on all heterophilic benchmark datasets and even better than pGNNs on Squirrel. The
results of Table 9 demonstrate that intergrating pGNNs with GCN and JKNet can sigificantly im-
prove their performance on heterophilic graphs.
36
Under review as a conference paper at ICLR 2022
F.6 Experimental Results of pGNNS on PPI Dataset for Inductive Learning
Additionally, we conduct comparison experiments of pGNNs against GAT on PPI dataset (Zitnik &
Leskovec, 2017) using the inductive learning settings as in Velickovic et al. (2018) (20 graphs for
training, 2 graphs for validation, 2 graphs for testing). We use three layers of GAT architecture with
256 hidden units, use 1 attention head for GAT (1 head) and 4 attention heads for GAT (4 heads).
We use three PGNN layers and a MLP layer as the first layer for pGNNs, set μ = 0.01, K = 1, and
use 256 hidden units for pGNN-256 and 512 hidden units for pGNN-512. The experimental results
are reported in Table 10.
Table 10: Results on PPI datasets. Averaged micro-F1 scores for 10 runs. Best results are outlined
in bold.
Method	PPI
GAT (1 head)	0.917 ± 0.041
GAT (4 heads)	0.972 ± 0.002
1.0GNN-256	0.961 ± 0.003
1.5GNN-256	0.967 ± 0.008
2.0GNN-256	0.968 ± 0.006
2.5GNN-256	0.973 ± 0.002
1.0GNN-512	0.978 ± 0.005
1.5GNN-512	0.977 ± 0.008
2.0GNN-512	0.981 ± 0.006
2.5GNN-512	0.978 ± 0.005
From Appendix F.6 we observe that the results of 2.5GNN on PPI slightly better than GAT with 4
attention heads and other p GNNs are very close to it. Moreover, all results of pGNNs significantly
outperform GAT with one attention head. The results of p GNNs on PPI is impressive. p GNNs have
much less parameters than GAT with 4 attention heads while obtain very completitive performance
on PPI. When we use more hidden units, 512 hidden units, pGNNs-512 significantly outperform
GAT, while pGNNs-512 still have less parameters. It illustrates the superior potential of applying
pGNNs to inducting learning on graphs.
F.7 Experimental Results of pGNNS on OGBN arXiv Dataset
Table 11: Results on OGBN arXiv dataset. Average accuracy (%) for 10 runs. Best results are
outlined in bold.
Method	OGBN arXiv
MLP	55.50 ± 0.23
GCN	71.74 ± 0.29
JKNet (GCN-based)	72.19 ± 0.21
DeepGCN	71.92 ± 0.16
GCN + residual (6 layers)	72.86 ± 0.16
GCN + residual (8 layers) + C&S	72.97 ± 0.22
GCN + residual (8 layers) + C&S v2	73.13 ± 0.17
1GNN	72.40 ± 0.19
2GNN	72.45 ± 0.20
3GNN	72.58 ± 0.23
1 GNN + residual (6 layers) + C&S	72.96 ± 0.22
2 GNN + residual (6 layers) + C&S	73.13 ± 0.20
3 GNN + residual (6 layers) + C&S	73.23 ± 0.16
Here we present the experimental of pGNNs on OGBN arXiv dataset (Hu et al., 2020). We use the
official data split setting of OGBN arXiv. We use three layers pGNN architecture and 256 hidden
units with μ = 0.5, K = 2. We also combine PGNNS with correct and smooth model (C&S) (Huang
et al., 2021) and introduce residual units. The results of MLP, GCN, JKNet, DeepGCN (Li et al.,
37
Under review as a conference paper at ICLR 2022
2019), GCN with residual units, C&S model are extracted from the leaderboard for OGBN arXiv.
dataset5. Table 11 summaries the results of pGNNs against the baselines.
We observe from Table 11 that pGNNs outperform MLP, GCN, JKNet, and DeepGCN. The perfor-
mance of pGNNs can be further improved by combining it with C&S model and residual units and
3GNN + residual (6 layers) + C&S obtains the best performance against the baselines.
F.8 Running Time of pGNNS
Tables 12 and 13 report the averaged running time of p GNNs and baselines on homophilic and
heterophilic benchmark datasets, respectively.
Table 12: Efficiency on homophilic benchmark datasests. Averaged running time per epoch (ms) /
averaged total running time (s). OOM denotes out of memory.
Method	Cora	CiteSeer	PubMed	Computers	Photo	CS	Physics
MLP	7.7 ms / 5.27s	8.1 ms / 5.37s	7.8 ms / 5.52s	8.8 ms/ 5.45s	8.4 ms / 5.34s	10.5 ms / 8.18s	14.6 ms / 12.78s
GCN	82.2 ms / 6.1s	84.2 ms / 6.1s	85 ms / 6.13s	85.2 ms / 7.07s	83.6 ms / 6.08s	85 ms / 9.68s	90 ms / 13.8s
SGC	89.5 ms / 4.96s	74.7 ms / 4.86s	80.6 ms/ 5.28s	109 ms / 5.21s	85.9 ms / 4.96s	213.6 ms / 8.01s	OOM
GAT	534.8 ms/ 13.06s	313.6 ms / 13.36s	314.6 ms/ 13.97s	441.3 ms / 24.62s	309.8 ms/ 15.96s	454 ms / 21.87s	436.9 ms/ 40.9s
JKNet	95.4 ms / 20.07s	101.1 ms / 19.58s	105.4 ms / 20.8s	106.1 ms / 29.72s	97.9 ms / 21.18s	102.7 ms / 24.94s	119.2 ms / 40.83s
APPNP	86.7 ms / 11.6s	86.3 ms/ 11.98s	85.5 ms/ 11.97s	92.1 ms/ 15.75s	86 ms / 12.19s	90.5 ms/ 17.36s	99.6 ms/ 25.89s
GPRGNN	86.5 ms / 12.42s	195.8 ms/ 12.6s	88.6 ms/ 12.59s	93.3 ms/ 15.98s	86.7 ms / 12.65s	92 ms/ 17.8s	217.1 ms / 26.33s
1.0GNN	96 ms / 20.12s	98.1 ms / 19.81s	100.2 ms/ 21.74s	151.4 ms / 64.08s	121.3 ms / 34.07s	109.7 ms / 25.03s	122.9 ms / 49.59s
1.5GNN	98.2 ms / 20.19s	97 ms / 20.26s	100.2 ms / 22.6s	140.3 ms / 64.08s	120 ms / 34.22s	112.3 ms/ 25.11s	127.9 ms / 49.54s
2.0GNN	98.1 ms / 20.11s	96.3 ms / 19.97s	99.3 ms / 22.17s	141 ms / 64.04s	129.3 ms / 34.14s	104.7 ms / 24.93s	124.6 ms / 49.35s
2.5GNN	96.6 ms / 20.12s	92.9 ms/ 20.16s	103 ms / 22.17s	141.6 ms / 64.01s	128.1 ms / 34.22s	110.8 ms/ 25.07s	124ms/ 49.39s
Table 13: Efficiency on heterophilic benchmark datasests. Averaged running time per epoch (ms) /
averaged total running time (s).
Method	Chameleon	Squirrel	Actor	Wisconsin	Texas	Cornell
MLP	7.7 ms / 5.29s	8 ms / 5.44s	8.6 ms / 5.4s	7.7ms/ 5.16s	7.9 ms/ 5.22s	7.6 ms/ 5.19s
GCN	83.4 ms / 6.1s	83.2 ms / 6.2s	90.7 ms / 6.07s	83.5 ms / 5.94s	80.7 ms/ 5.96s	87.1 ms/ 5.92s
SGC	78.1 ms / 4.93s	110.9 ms / 5.21s	77.1 ms / 4.71s	73.2 ms / 4.52s	74.2 ms / 4.79s	71.3 ms/ 4.8s
GAT	374.9 ms/ 13.49s	324.2 ms/ 17.15s	420 ms / 13.82s	317.5 ms/ 12.68s	357.9 ms / 12.38s	383.3 ms/ 12.45s
JKNet	102.4 ms/ 21.15s	101 ms / 22.84s	97.2 ms/ 21.24s	98.5 ms/ 21.07s	103.6 ms / 20.92s	102.2 ms / 20.79s
APPNP	87.1 ms / 12.12s	98.8 ms / 12.41s	87.2 ms/ 11.81s	84.2 ms / 11.83s	86 ms / 11.9s	83.1 ms / 11.94s
GPRGNN	93 ms / 12.98s	86.1 ms / 13.01s	94.2 ms / 13.01s	84.3 ms / 12.66s	92 ms/ 12.64s	89.1 ms / 12.6s
1.0GNN	107.3 ms/ 22.43s	116.3 ms/ 30.92s	117.8 ms / 23.6s	94.5 ms/ 18.47s	92 ms/ 18.83s	92.7 ms / 18.97s
1.5GNN	97.2 ms / 22.54s	115 ms / 31.04s	119.2 ms/ 23.47s	93.3 ms / 18.64s	90.8 ms / 19.09s	94.9 ms / 18.88s
2.0GNN	98.7 ms / 22.37s	114.8 ms/ 31.14s	100.8 ms/ 23.73s	92.2 ms/ 19.09s	92.5 ms/ 18.72s	98ms/ 18.64s
2.5GNN	97.9 ms / 22.38s	115.9 ms/ 31.09s	97.3 ms/ 23.77s	92.8 ms / 19.03s	91 ms / 18.84s	90.7 ms / 18.83s
F.9 Experimental Results on Benchmark Datasets for 64 Hidden Units
Table 14: Results on heterophilic benchmark datasets for 64 hidden units. Averaged accuracy (%)
for 20 runs. Best results outlined in bold and the results within 95% confidence interval of the best
results are outlined in underlined bold.
Method	Chameleon	Squirrel	Actor	Wisconsin	Texas	Cornell
MLP	46.55±0.90	33.83±0.59	38.40±o.76	93.91±2.47	87.51±8.53	86.75±8.22
GCN	34.74±2.62	25.68±1.17	30.86±1.51	65.93±5.47	58.56±13.28	46.81±4.28
SGC	34.57±4.71	24.39±1.54	35.50±2.09	62.87±8.92	50.62±5.60	29.44±14.83
GAT	43.33±1.53	30.07±0.99	33.44±2.45	66.57±4.69	50.69±12.89	42.62±13.37
JKNet	32.69±4.47	27.18±0.76	25.72±2.75	66.57±10.53	43.88±17.10	47.69±3.25
APPNP	35.09±3.18	28.15±0.93	32.28±1.75	66.30±1.60	69.00±4.53	54.88±3.85
GPRGNN	34.65±2.86	28.56±1.35	34.58±i.45	93.70±3.12	86.50±6.04	84.75±8.38
1.0GNN	49.51±1.32	32.67±1.00	40.70±o.88	95.23±1.60	84.12±7.39	82.56±6.97
1.5GNN	49.52±1.15	33.14±1.10	39.82±1.54	94.03±2.26	86.94±6.99	86.89±6.63
2.0GNN	49.19±0.81	33.78±0.87	39.75±1.26	94.49±1.81	87.62±6.64	85.56±7.25
2.5GNN	48.93±0.74	33.31±1.27	39.47±i.20	92.13±2.16	87.25±5.57	80.56±5.28
5https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxiv
38
Under review as a conference paper at ICLR 2022
Table 15: Results on homophilic benchmark datasets for 64 hidden units. Averaged accuracy (%)
for 20 runs. Best results are outlined in bold and the results within 95% confidence interval of the
best results are outlined in underlined bold. OOM denotes out of memory.
Method	Cora	CiteSeer	PubMed	Computers	Photo	CS	Physics
MLP	49.05±0.82	50.67±1.25	80.32±0.40	70.58±0.82	79.44±0.79	89.48±0.50	92.84±0.62
GCN	77.65±0.42	64.72±0.52	84.13±0.12	84.56±0.79	90.16±0.88	91.14±0.10	94.75±0.04
SGC	70.32±1.87	65.77±0.99	76.27±0.94	83.24±0.81	89.43±1.03	91.11±0.10	OOM
GAT	76.97±1.18	61.28±1.62	83.57±0.23	83.84±1.93	9054±o.56	89.68±0.42	93.91±0.20
JKNet	78.77±0.79	64.62±0.80	82.82±0.16	82.22±1.32	88.43±0.53	90.48±0.13	93.75±0.32
APPNP	79.95±0.72	65.56±o.64	84.00±0.22	83.83±0.78	905O±0.59	91.90±0.12	9484±o.08
GPRGNN	78.17±1.31	61.26±2.14	84.54±0.24	83.77±i.06	89.86±0.63	91.34±0.25	94.63±0.26
1.0GNN	77.11±0.39	63.17±0.89	83.14±0.46	82.64±o.98	89.60±0.69	92.53±0.22	94.86±0.24
1.5GNN	78.69±0.43	63.14±0.93	83.97±0.04	84.64±1.42	90.67±0.67	92.93±0.14	9493±o.i2
2.0GNN	79.06±0.41	63.92±1.14	84.24±0.27	84.57±0.96	90.17±0.88	9274±0.26	95.05±0.09
2.5GNN	79.15±0.39	63.16±1.25	84.88±0.09	83.84±o.71	89.05±0.85	92.31±0.19	94.92±0.10
F.10 TRAINING CURVES FOR p = 1
ComPUterS
——Training Less -Testing Accuracy
100
60
Zg
L7S
LM
L2S
LβΦ
ɑτs
αso
ɑzs
α∞
S.O-
28
L7S
_________PUbrneCl
——Training Loss-Testing Accuracy
100
80
60
40
M-
Λ3ejnu3v
ω",σl
Λ3ejnu3v
αso-
aas-
α∞-
■ — Testing Accuracy
ω",σl
100
Λ3ejnu3v
6040
100
60
60 g
-40 U
-20
80
60 g
40 U
20
actor
S.0-
2.S-
2.0-
1ls
LO-
e∙s-
Wisconsin ,nn
100
80
60 g
40 a
τmιnιng less ---- Testing Accuracy
ɑo-
100
CorneIl
——Training Less--Testing Accuracy
60
Λ32n84
Λ3ejnu3v
Figure 7:	The curves of training loss and testing accuracy for p = 1.
39
Under review as a conference paper at ICLR 2022
F.11 Visualization Results of NODE EMBEDDINGS
cora
Y。 y -aɪ Q a> «0	«
cora
cora
y。 -«	-» a 20 颠
⑶ 1∙0GNN.	(b) 1∙5GNN.	(c) 2∙0GNN.
cora
cora
a 颠
(d) 2∙5GNN.	(e) GCN.
Figure 8:	Visualization of node embeddings for Cora dataset using t-SNE (van der Maaten & Hinton,
2008)
(a) 1∙0GNN.
(b) 1∙5GNN.
(c) 2.0GNN.
CDmouters
CDmouters
15
≈
Q
-≡
-SB
-τs
-TS Sβ -25 D 2S
75
-uo -τs -s»	-≡ o ≈ Sa n ια>
(d) 2.5GNN.	(e) GCN.
Figure 9: Visualization of node embeddings for Computers dataset using t-SNE.

40
Under review as a conference paper at ICLR 2022
(b) 1∙5GNN.
(c) 2∙0GNN.
chameleon
(d) 2∙5GNN.	(e) GCN.
Figure 10: Visualization of node embeddings for Chameleon dataset using t-SNE.
(a) 1∙0GNN.	(b) 1∙5GNN.	(c) 2∙0GNN.
WiSCOnSin	Wisconsin
—>--<---1---1---1---1_	-15 ,	.
-IOTD	SlalS	-IS-IOTDSlD 15	20
(d) 2∙5GNN.	(e) GCN.
Figure 11: Visualization of node embeddings for Wisconsin Dataset using t-SNE.
41