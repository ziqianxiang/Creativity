Under review as a conference paper at ICLR 2022
Blur Is an Ensemble:
Spatial Smoothings to Improve
Accuracy, Uncertainty, and Robustness
Anonymous authors
Paper under double-blind review
Ab stract
1	Bayesian neural networks (BNNs) have shown success in the areas of uncertainty
2	estimation and robustness. However, a crucial challenge prohibits their use in
3	practice. Bayesian NNs require a large number of predictions to produce reliable
4	results, leading to a significant increase in computational cost. To alleviate this
5	issue, we propose spatial smoothing, a method that ensembles neighboring feature
6	map points of CNNs. By simply adding a few blur layers to the models, we
7	empirically show that spatial smoothing improves accuracy, uncertainty estimation,
8	and robustness of BNNs across a whole range of ensemble sizes. In particular,
9	BNNs incorporating spatial smoothing achieve high predictive performance merely
10	with a handful of ensembles. Moreover, this method also can be applied to canonical
11	deterministic neural networks to improve the performances. A number of evidences
12	suggest that the improvements can be attributed to the stabilized feature maps
13	and the flattening of the loss landscape. In addition, we provide a fundamental
14	explanation for prior works—namely, global average pooling, pre-activation, and
15	ReLU6—by addressing them as special cases of spatial smoothing. These not
16	only enhance accuracy, but also improve uncertainty estimation and robustness by
17	making the loss landscape smoother in the same manner as spatial smoothing.
18 1 Introduction
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
In a real-world environment where many unexpected
events occur, machine learning systems cannot be guar-
anteed to always produce accurate predictions. In or-
der to handle this issue, we make system decisions
more reliable by considering estimated uncertainties,
in addition to predictions. Uncertainty quantification is
particularly crucial in building a trustworthy system in
the field of safety-critical applications, including med-
ical analysis and autonomous vehicle control. However,
canonical deep neural networks (NNs)—or determinis-
tic NNs—cannot produce reliable estimations of uncer-
tainties (Guo et al., 2017), and their accuracy is often
severely compromised by natural data corruptions from
noise, blur, and weather changes (Engstrom et al., 2019;
Azulay & Weiss, 2019).
Bayesian neural networks (BNNs), such as Monte Carlo
(MC) dropout (Gal & Ghahramani, 2016), provide a
probabilistic representation of NN weights. They com-
bine a number of models selected based on weight prob-
ability to make predictions of desired results. Thanks to
Accuracy (%)
Figure 1: Spatial smoothing improves
both accuracy and uncertainty (NLL).
Smooth means spatial smoothing. Down-
ward from left to the right (&) means bet-
ter accuracy and uncertainty.
this feature, BNNs have been widely used in the areas of uncertainty estimation (Kendall & Gal, 2017)
and robustness (Ovadia et al., 2019). They are also promising in other fields like out-of-distribution
detection (Malinin & Gales, 2018) and meta-learning (Yoon et al., 2018).
1
Under review as a conference paper at ICLR 2022
Input
Prediction
Importance
Observed
data
Temporally proximate
data stream
一 人 一
<----->
t=0	t=-1 t=-2
Spatially proximate
data points
BNN
Temporal smoothing
Spatial smoothing

Figure 2: Comparison of three different Bayesian neural network inferences: canonical BNN
inference, temporal smoothing (Park et al., 2021), and spatial smoothing (ours). In this figure, x0 is
observed data, Pi is predictions p(y|xo, Wi) or p(y∣Xi, Wi), ∏i is importances ∏(x∕xo), and N is
ensemble size.
42
43
44
45
46
47
48
49
50
51
52
53
Nevertheless, there remains a significant challenge that prohibits their use in practice. BNNs require
an ensemble size of up to fifty to achieve high predictive performance, which results in a fiftyfold
increase in computational cost (Kendall & Gal, 2017; Loquercio et al., 2020). Therefore, if BNNs
can achieve high predictive performance merely with a handful of ensembles, they could be applied
to a much wider range of areas.
1.1 Preliminary
We would first like to discuss BNN inference in detail, then move on to Vector-Quantized BNN
(VQ-BNN) inference (Park et al., 2021), an efficient approximated BNN inference.
BNN inference. Suppose we have access to posterior probability ofNN weight p(W|D) for training
dataset D. The predictive result of BNN is given by the following predictive distribution:
p(y|x0, D) =	p(y|x0, W) p(W|D) dW	(1)
where x0 is observed input data vector, y is output vector, and p(y|x, W) is the probabilistic prediction
parameterized by the result of NN for an input x and weight W. In most cases, the integral cannot be
solved analytically. Thus, we use the MC estimator to approximate it as follows:
N-1 1
p(y∣χ0, D) ` E NP(y∣χo, Wi)	(2)
i=0
where Wi 〜P(W |D) and N is the number of the samples. The equation indicates that BNN inference
is ensemble average of NN predictions for one observed data point as shown on the left of Fig. 2.
Using N neural networks in the ensemble would requires N times more computational complexity
than one NN execution.
2
Under review as a conference paper at ICLR 2022
D Deterministic D Deterministic + Smooth -Q-- MC dropout ■ MC dropout + Smooth
Figure 3: Spatial smoothing improves both accuracy and uncertainty across a whole range of
ensemble sizes. We report the predictive performance of ResNet-18 on CIFAR-100.
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
Data-complemented BNN inference. To reduce the computational cost of BNN inference, VQ-
BNN (Park et al., 2021) executes NN for an observed data only once and complements the result
with previously calculated predictions for other data. If we have access to previous predictions, the
computational performance of VQ-BNN becomes comparable to that of one NN execution. To be
specific, VQ-BNN inference is:
N-1
p(y∣x0, D) ` En(Xi|xo)p(y∣Xi, Wi)	(3)
i=0
where ∏(xi∣xo) is the importance of data Xi with respect to the observed data x°, and it is defined as a
similarity between Xi and xo. p(y∣xo, wo) is the newly calculated prediction, and {p(y∣xι, wι),…}
are previously calculated predictions. To accurately infer the results, the previous predictions should
consist of predictions for “data similar to the observed data”.
Thanks to the temporal consistency of real-world data streams, aggregating predictions for similar
data in data streams is straightforward. Since temporally proximate data sequences tend to be similar,
we can memorize recent predictions and calculates their average using exponentially decreasing
importance. In other words, VQ-BNN inference for data streams is simply temporal smoothing of
recent predictions as shown in the middle of Fig. 2.
VQ-BNN has two limitations, although it may be a promising approach to obtain reliable results
in an efficient way. First, it was only applicable to data streams such as video sequences. Applying
VQ-BNN to images is challenging because it is impossible to memorize all similar images in advance.
Second, Park et al. (2021) used VQ-BNN only in the testing phase, not in the training phase. We find
that ensembling predictions for similar data helps in NN training by smoothing the loss landscape.
1.2 Main Contribution
0 Spatially neighboring points in visual imagery tend to be similar, as do feature maps of convolu-
tional neural networks (CNNS). By exploiting this spatial consistency, we propose spatial smoothing
as a method of ensembling nearby feature maps to improve the efficiency of ensemble size in BNN
inference. The right side of Fig. 2 visualizes spatial smoothing aggregating neighboring feature maps.
。We empirically demonstrate that spatial smoothing improves the efficiency in vision tasks, such
as image classification on CIFAR (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015),
without any additional training parameters. Figure 3 shows that negative log-likelihood (NLL) of
“MC dropout + spatial smoothing” with an ensemble size of two is comparable to that of vanilla MC
dropout with an ensemble size of fifty. We also demonstrate that spatial smoothing improves accuracy,
uncertainty, and robustness all at the same time. Figure 1 shows that spatial smoothing improves both
the accuracy and uncertainty of various deterministic and Bayesian NNs with an ensemble size of
fifty on CIFAR-100.
命 Global average pooling (GAP) (Lin et al., 2014; Zhou et al., 2016), pre-activation (He et al.,
2016b), and ReLU6 (Krizhevsky & Hinton, 2010; Sandler et al., 2018) have been widely used in vision
tasks. However, their motives are largely justified by the experiments. We provide an explanation for
these methods by addressing them as special cases of spatial smoothing. Experiments support the
claim by showing that the methods improve not only accuracy but also uncertainty and robustness.
3
Under review as a conference paper at ICLR 2022
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
2	Probabilistic Spatial Smoothing
To improve the computational perfor-
mance of BNN inference, VQ-BNN
(Park et al., 2021) executes NN pre-
diction only once and complements
the result with previously calculated
predictions. The key to the success
of this approach largely depends on
the collection of previous predictions
for proximate data. Gathering tempo-
rally proximate data and their predic-
tions from data streams is easy be-
cause recent data and predictions can
be aggregated using temporal consis-
tency. On the other hand, gathering
Wh∕2xW∕2x2C
Figure 4: Stages of CNNs such as ResNet (left) and the
stages incorporating spatial smoothing layer (right).
time-independent proximate data, e.g. images, is more difficult because they lack such consistency.
2.1	Module Architecture for Ensembling Neighboring Feature Map Points
So instead of temporal consistency, we use spatial consistency—where neighboring pixels of images
are similar—for real-world images. Under this hypothesis, we take the feature maps as predictions
and aggregate neighboring feature maps.
Most CNN architectures, including ResNet, consist of multiple stages that begin with increasing the
number of channels while reducing the spatial dimension of the input volume. We decompose an
entire BNN inference into several steps by rewriting each stage in a recurrence relation as follows:
p(zi+1|zi, D)
p(zi+1 |zi, wi) p(wi |D) dwi
(4)
where zi is input volume of the i-th stage, and the first and the last volume are input data and
output. wi and p(wi|D) are NN weight in the i-th stage and its probability. p(zi+1 |zi, wi) is output
probability of zi+1 with respect to the input volume zi . To derive the probability from the output
feature map, we transform each point of the feature map into a Bernoulli distribution. To do so, a
composition of tanh and ReLU, a function from value of range [-∞, ∞] into probability, is added
after each stage. Put shortly, we use neural networks for point-wise binary feature classification.
Since Eq. (4) is a kind of BNN inference, it can be approximated using Eq. (3). In other words, each
stage predicts feature map points only once and complements predictions with similar feature maps.
Under spatial consistency, it averages probabilities of spatially neighboring feature map points, which
is well known as blur operation in image processing. For the sake of implementation simplicity,
average pooling with a kernel size of 2 and a stride of 1 is used as a box blur. This operation ensembles
four neighboring probabilities with the same importances.
In summary, as shown in Fig. 4, we propose the following probabilistic spatial smoothing layer:
Smooth(z) = Blur ◦ Prob (z)
(5)
where Prob(∙) is a point-wise function from a feature map to probability, and Blur(∙) is importance-
weighted average for ensembling spatially neighboring probabilities from feature maps. Smooth layer
is added after each stage. Prob and Blur are further elaborated below.
Prob: Feature map to probability. Prob is a function that transforms a real-valued feature map
into probability. We use tanh-ReLU composition for this purpose. However, tanh is commonly
known to suffer from the vanishing gradient problem. To alleviate this issue, we propose the following
temperature-scaled tanh:
tanhτ(Z) = T tanh (z/τ)
(6)
where τ is a hyperparameter called temperature. τ is 1 in conventional tanh and ∞ in identity
function. tanhτ imposes an upper bound on a value, but does not limit the upper bound to 1.
4
Under review as a conference paper at ICLR 2022
əɔuBmA ⅛s XmBaH
o.ιoo-
0.075-
0.050-
0.025-
c3
s3 c4 s4
Blocks
0.000	'	. C C
cl si c2 s2
g 0.20
目 0.10
⅛ 0.15
Blocks
a
5 0.05-
⅛
<υ
H °∙0° :	:	o	'o
cl si c2 s2
c3
(a)	Model uncertainty
(b)	Data uncertainty
MC- MC dropout
• MC dropout + Smooth
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
Figure 5: Spatial smoothing layers reduce feature map variances, suggesting that they ensemble
feature map points. We provide standard deviation of feature maps by block depth with ResNet-50 on
CIFAR-100. c1 to c4 and s1 to s4 each stand for stages and spatial smoothing layers, respectively.
Model uncertainty is represented by the average standard deviation of several feature maps obtained
from multiple NN executions. Data uncertainty is represented by the standard deviation of feature
map points obtained from one NN execution.
An unnormalized probability, ranging from 0 to τ , is allowed as the output of Prob. Then, thanks to
the linearity of integration, we obtain an unnormalized predictive distribution accordingly. Taking
this into account, we propose the following Prob:
Prob(z) = ReLU ◦ tanhτ (z)	(7)
where τ > 1. We empirically determine τ to minimize NLL, a metric that measures both accuracy
and uncertainty. See Fig. B.3 for more detailed ablation studies. In addition, we expect upper-bounded
functions, e.g., ReLU6(z) = ReLU ◦ min(z, 6) and feature map scaling z∕τ with τ > 1 which
is BatchNorm, to be able to replace tanhτ in Prob; and as expected, these alternatives improve
uncertainty estimation in addition to accuracy. See Appendix C.2 and Appendix C.3 for detailed
discussions on activation (ReLU ◦ BatchNorm) and ReLU6 as Prob.
Blur: Averaging neighboring probabilities. Blur averages the probabilities from feature maps.
We primarily use the average pool with a kernel size of 2 and a stride of 1 as the implementation
of Blur for the sake of simplicity. Nevertheless, we could generalize Blur by using the following
depth-wise convolution, which acts on each input channel separately, with non-trainable kernel
K=k ㊈ k>	⑻
where k is a 1D matrix, e.g., k ∈ {(1) , (1, 1) , (1, 2, 1) , (1, 4, 6, 4, 1)}. Different ks derive different
importances for neighboring feature maps. We empirically show that most Blurs improve the
predictive performance and that optimal K varies by model. For more ablation studies, see Table B.2.
2.2	How Does Spatial Smoothing Help Optimization?
We present theoretical and empirical aspects to show that spatial smoothing ensembles feature maps.
Feature map variance. BNNs have two types of uncertainties: One is model uncertainty and the
other is data uncertainty (Park et al., 2021). These randomnesses increase the variance of the feature
maps. To demonstrate that spatial smoothing is an ensemble, we use the following proposition:
Proposition 1. Ensembles reduce the variance of predictions.
We omit the proof since it is straightforward. In our context, predictions are output feature maps of a
stage. We investigate model and data uncertainties of the predictions along NN layers to show that
spatial smoothing reduces the randomnesses and ensembles feature maps. Figure 5 shows the model
uncertainty and data uncertainty of Bayesian ResNet including MC dropout layers. In this figure, the
5
Under review as a conference paper at ICLR 2022
(a) Frequency mask
Frequency	Noise frequency
(b) Log amplitude at c1 (c) Robustness for noise frequency
Deterministic --------- Deterministic + Smooth
----MC dropout ----------- MC dropout + Smooth
Figure 6: MC dropout adds high-frequency noises, and spatial smoothing filters high-frequency
signals. In these experiments, we use ResNet-50 for ImageNet. Left: Frequency mask Mf with
w = 0.1π. Middle: Diagonal components of Fourier transformed feature maps at the end of the
stage 1. Right: The accuracy against frequency-based random noise. ResNets are vulnerable to
high-frequency noises. Spatial smoothing improves the robustness against high-frequency noises.
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
uncertainty of MC dropout’s feature map only accumulates, and almost monotonically increases in
every NN layer. In contrast, the uncertainty of “MC dropout + spatial smoothing”’s feature map is
significantly decreases at the end of stages, suggesting that the smoothing layers ensemble the feature
map. In other words, they make the feature map more accurate and stabilized input volumes for the
next stages. In addition, consistently, the spatial smoothing layer close to the last layer significantly
improves performance because it reduces the uncertainty of predictions largely. See Fig. B.5 for more
detailed results. Deterministic NNs do not have model uncertainty but data uncertainty. Therefore,
spatial smoothing improves the performance of deterministic NNs as well as Bayesian NNs.
Fourier analysis. We also analyze spatial smoothing through the lens of Fourier transform:
Proposition 2. Ensembles filter high-frequency signals.
The proof is provided in Eqs. (16) to (17). Figure 6b shows the 2D Fourier transformed output
feature map at the end of the stage 1. This figure reveals that MC dropout almost does not affect
low-frequency (< 0.3π) ranges, and it adds high-frequency (≥ 0.3π) noises. Since spatial smoothing
is a low-pass filter, it effectively filters high-frequency signals, including the noises caused by MC
dropout.
We also find that CNNs are particularly vulnerable to high-frequency noises. To demonstrate this
claim, following Shao et al. (2021), we measure accuracy with respect to data with frequency-based
random noise Xnoise = xo + F-1 (F(δ) Θ Mf), where xo is clean data, F(∙) and F-1(∙) are Fourier
transform and inverse Fourier transform, δ is random noise, and Mf is frequency mask as shown
in Fig. 6a. Figure 6c exhibits the results. In sum, high-frequency noises, including those caused by
MC dropout, significantly impair accuracy. Spatial smoothing improves the robustness by effectively
removing high-frequency noises.
Loss landscape. Lastly, we show that the randomness hinders NN training as follows:
Proposition 3. Randomness of predictions sharpens the loss landscape, and ensembles flatten it.
The proof is provided in Eqs. (18) to (25). Since a sharp loss function disturbs NN optimization
(Keskar et al., 2017; Santurkar et al., 2018; Foret et al., 2020), reducing the uncertainty helps NN
learn strong representations. For example, training phase NN ensemble averages out the randomness,
and it flattens the loss function. In consequence, an ensemble of BNN outputs in training phase
significantly improves the predictive performance. See Fig. D.4 for numerical results. However, we
do not use training phase ensemble because it significantly increases the training time. Instead, we
use spatial smoothing as a method that ensembles feature maps without sacrificing training time.
We visualizes the loss landscapes (Li et al., 2018), the contours of NLL on training dataset. Figure 8b
shows that the loss landscapes of MC dropout fluctuate and have irregular surfaces due to the
6
Under review as a conference paper at ICLR 2022
(a) MLP classifier
(b) GAP classifier (vanilla)
(c) GAP classifier + Smooth
Figure 8: Both GAP and spatial smoothing smoothen the loss landscapes. To demonstrate this,
we present the loss landscape visualizations of ResNet-18 models with MC dropout on CIFAR-100.
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
randomness. As Li et al. (2018); Foret et al. (2020) pointed out,
this may lead to poor generalization and predictive performance.
Spatial smoothing reduces randomness as discussed above, and
spatial smoothing aids in optimization by stabilizing and flattening
the loss landscape of BNN as shown in Fig. 8c.
Furthermore, we use Hessian to quantitatively represent the sharp-
ness of the loss landscapes. Figure 7 shows the Hessian max eigen-
value spectra of the models in Fig. 8 with a batch size of 128, which
reveals that spatial smoothing reduces the magnitude of Hessian
eigenvalues and suppresses outliers. Since large Hessian eigenval-
ues disturb NN training (Ghorbani et al., 2019), we come to the
same conclusion that spatial smoothing helps NN optimization. See
Appendix C.1 for a more detailed description of the configurations
of the Hessian max eigenvalue spectra. In addition, from these
observations, we propose the conjecture that the flatter the loss
landscape, the better the uncertainty estimation, and vice versa.
Max Eigenvlaue
Figure 7: Both GAP and spa-
tial smoothing suppress large
Hessian eigenvalue outliers,
i.e., they flatten the loss land-
scapes. Compare with Fig. 8.
2.3	Revisiting Global Average Pooling
The success of GAP classifier in image classification is
indisputable. The initial motivation and the most widely
accepted explanation for this success is that GAP prevents
overfitting by using far fewer parameters than multi-layer
perceptron (MLP) (Lin et al., 2014). However, we discover
that the explanation is poorly supported. We compares
GAP with other classifiers including MLP. Contrary to
popular belief, Table 1 suggests that MLP does not overfit
the training dataset. MLP underfits or gives comparable
performance to GAP on the training dataset. On the test
Table 1: MLP does not overfit the
training dataset. We report train-
ing NLL (NLLtrain) and testing NLL
(NLLtest) of ResNet-50 on CIFAR-100.
Classifier	NLLtrain	NLLtest
GAP	0.0061	0.822
MLP	0.0071	1.029
dataset, GAP provides better results compared with MLP. See Table C.1 for more detailed results.
Our argument is that GAP is an extreme case of spatial smoothing. In other words, GAP is successful
because it ensembles feature maps and smoothens the loss landscape to help optimization. To support
this claim, we visualizes the loss landscape of MLP as shown in Fig. 8a. It is chaotic compared to
that of GAP as shown in Fig. 8b. Hessian shows the consistent results as demonstrated by Fig. 7.
3	Experiments
This section presents two experiments. The first experiment is image classification through which
we show that spatial smoothing not only improves the ensemble efficiency, but also the accuracy,
uncertainty, and robustness of both deterministic NN and MC dropout. The second experiment is
semantic segmentation on data streams through which we show that spatial smoothing and temporal
smoothing (Park et al., 2021) are complementary. See Appendix A for more detailed configurations.
7
Under review as a conference paper at ICLR 2022
77.0
75.5
76.0
Deterministic φ Deterministic + Smooth -G-- MC dropout ■ MC dropout + Smooth
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
Figure 9: Spatial smoothing also improves predictive performance on large datasets. We report
predictive performance of ResNet-50 on ImageNet.
Three metrics are measured in these experiments: NLL (11), accuracy (↑), and expected calibration
error (ECE, 1) (GUo et al., 2017). NLL represents both accuracy and uncertainty, and is the most
widely used as a proper scoring rule. ECE measures discrepancy between accuracy and confidence.
3.1	Image Classification
This section mainly discuss ResNet (He et al., 2016a). Table E.1 also discuss other settings that
show the same trend: e.g., VGG (Simonyan & Zisserman, 2015), ResNeXt (Xie et al., 2017),
and pre-activation models (He et al., 2016a). Spatial smoothing also improves deep ensemble
(Lakshminarayanan et al., 2017), another non-Bayesian probabilistic NN method. See Fig. E.1.
Performance. Fig. 3 and Fig. 9 show the predictive performances of ResNet-18 on CIFAR-100
and ResNet-50 on ImageNet, respectively. The results indicate that spatial smoothing improves both
accuracy and uncertainty in many respects. Let us be more specific. First, spatial smoothing improves
the efficiency of ensemble size. In these examples, the NLL of “MC dropout + spatial smoothing”
with an ensemble size of 2 is comparable to or even better than that of MC dropout with an ensemble
size of 50. In other words, “MC dropout + spatial smoothing” is 25× faster than MC dropout with
a similar predictive performance. Second, the predictive performance of “MC dropout + spatial
smoothing” is better than that of MC dropout, at an ensemble size of 50. Third, spatial smoothing
improves the predictive performance of deterministic NN, as well as MC dropout.
Robustness. To evaluate robustness against data corruption, we
measure predictive performance of ResNet-18 on CIFAR-100-
C (Hendrycks & Dietterich, 2019). This dataset consists of data
corrupted by 15 different types, each with 5 levels of intensity
each. We use mean corruption NLL (mCNLL, 1), the averages
of NLL over intensities and corruption types, to summarize the
performance of corrupted data in a single value. See Eq. (32) for
a more rigorous definition. Figure 10 shows that spatial smoothing
not only improves the efficiency but also corruption robustness
across a whole range of ensemble size. See Fig. E.3 for more
details. Spatial smoothing also improves adversarial robustness
and perturbation consistency (↑) (Hendrycks & Dietterich, 2019;
Zhang, 2019a), shift-transformation invariance. See Table E.2,
Table E.3, and Fig. E.4 for more details.
3.2	Semantic Segmentation
V 5
7 6
0) TlNOE
60
IO0	IO1
Ensemble Size
Figure 10: Spatial smoothing
improves the robustness. See
Fig. E.3 for more details.
Table 2 summarizes the result of semantic segmentation on CamVid dataset (Brostow et al., 2008)
that consists of real-world 360×480 pixels videos. The table shows that spatial smoothing improves
predictive performance, which is consistent with the image classification experiment. Moreover, the
result reveals that spatial smoothing and temporal smoothing (Park et al., 2021) are complementary.
See Table E.4 for more results.
1We use arrows to indicate which direction is better.
8
Under review as a conference paper at ICLR 2022
Table 2: Spatial smoothing and temporal smoothing are complementary. We provide predictive
performance of MC dropout in semantic segmentation. Spat and Temp each stand for spatial
smoothing and temporal smoothing. Acc and Cons stand for accuracy and consistency. The numbers
in brackets denote the performance improvements over the baseline.
Spat	Temp	NLL	Acc (%)	ECE (%)	Cons (%)
•	•	0.298 (-0.000)	92.5 (+0.0)	4.20 (-0.00)	95.4 (+0.0)
X	•	0.284 (-0.014)	92.6 (+0.1)	3.96 (-0.24)	95.6 (+0.2)
•	X	0.273 (-0.025)	92.6 (+0.1)	3.23 (-0.97)	96.4 (+1.0)
X	X	0.260 (-0.038)	92.6 (+0.1)	2.71 (-1.49)	96.5 (+1.1)
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
4	Related Work
Spatial smoothing can be compared with prior works in the following areas.
Anti-aliased CNNs. Local means (Zhang, 2019a; Zou et al., 2020; Vasconcelos et al., 2020; Sinha
et al., 2020) were introduced for the shift-invariance of deterministic CNNs in image classification.
They were motivated to prevent the aliasing effect of subsampling. Although the local filtering can
result in a loss of information, Zhang (2019a) experimentally observed an increase in accuracy that
was beyond expectation. We provide a fundamental explanation for this phenomenon: Local means
are a spatial ensemble. An ensemble not only improves accuracy, but also uncertainty and robustness
of deterministic and Bayesian NNs. In Fig. F.1, we also show that the predictive performance
improvement is not due to anti-aliasing of local mean. See Appendix F for more discussion on local
means. For a discussion on non-local means (Wang et al., 2018) and self-attention (Dosovitskiy et al.,
2021), see Section 5.
Sampling-free BNNs. Sampling-free BNNs (Herngndez-Lobato & Adams, 2015; Wang et al.,
2016; Wu et al., 2019) predict results based on a single or couple of NN executions. To this end, it is
assumed that posterior and feature maps follow Gaussian distributions. However, the discrepancy
between reality and assumption accumulates in every NN layer. Consequently, to the best of our
knowledge, most of the sampling-free BNNs could only be applied to shallow models, such as LeNet,
and were tested on small datasets. Postels et al. (2019) applied sampling-free BNNs to SegNet;
nonetheless, Park et al. (2021) argued that they do not predict well-calibrated results.
Efficient deep ensembles. Deep ensemble (Lakshminarayanan et al., 2017; Fort et al., 2019) is
another probabilistic NN approach for predicting reliable results. BatchEnsemble (Wen et al., 2020;
Dusenberry et al., 2020) ensembles over a low-rank subspace to make deep ensemble more efficient.
Depth uncertainty network (Antoran et al., 2020) aggregates feature maps from different depths of
a single NN to predict results efficiently. Despite being robust against data corruption, it provides
weaker predictive performance compared to deterministic NN and MC dropout.
5	Discussion
We propose spatial smoothing, a simple yet efficient module to improve BNN. Three different per-
spectives, namely, feature map variance, Fourier analysis, and loss landscape, suggest that spatial
smoothing ensembles feature maps. The limitation of spatial smoothing is that designing its compo-
nents requires inductive bias. In other words, the optimal shape of the blur kernel is model-dependent.
We believe this problem can be solved by introducing self-attention (Vaswani et al., 2017). Self-
attentions for computer vision (Dosovitskiy et al., 2021; Touvron et al., 2021; Carion et al., 2020)
can be deemed as trainable importance-weighted ensembles of feature maps. The observation that
Transformers are more robust than expected (Bhojanapalli et al., 2021; Shao et al., 2021) supports this
claim. Therefore, using self-attentions to generalize spatial smoothing would be a promising future
work because it not only expands our work, but also helps deepen our understanding of self-attention.
9
Under review as a conference paper at ICLR 2022
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
Reproducibility S tatement
To ensure reproducibility, we provide comprehensive resources, such as code and experimental details.
The codebase will be released as open source under the Apache License 2.0. See the supplemental
material for the code. Appendix A provides the specifications of all models used in this work. Detailed
experimental setup including hyperparameters and ablation study are also available in Appendix A
and Appendix B. De-facto image datasets are used for all experiments as described in Appendix A.
References
Javier Antoran, James Allingham, and Jose MigUel Herndndez-Lobato. Depth uncertainty in neural
networks. Advances in Neural Information Processing Systems, 2020.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? Journal of Machine Learning Research, 2019.
Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and
Andreas Veit. Understanding robustness of transformers for image classification. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2021.
Gabriel J Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recog-
nition using structure from motion point clouds. In European Conference on Computer Vision.
Springer, 2008.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer
Vision. Springer, 2020.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations, 2021.
Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji
Lakshminarayanan, and Dustin Tran. Efficient and scalable bayesian neural nets with rank-1
factors. In International Conference on Machine Learning. PMLR, 2020.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Exploring
the landscape of spatial robustness. In International Conference on Machine Learning. PMLR,
2019.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. In International Conference on Learning Representations,
2020.
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspec-
tive. arXiv preprint arXiv:1912.02757, 2019.
Jonathan Frankle, David J Schwab, and Ari S Morcos. Training batchnorm and only batchnorm:
On the expressive power of random features in cnns. In International Conference on Learning
Representations, 2021.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning. PMLR, 2016.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. In International Conference on Learning Representations, 2019.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. In International Conference on Machine Learning. PMLR, 2019.
10
Under review as a conference paper at ICLR 2022
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. International Conference on Learning Representations, 2015.
Priya Goyal, Piotr Dolldr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, AaPo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning. PMLR, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity maPPings in deeP residual
networks. In European Conference on Computer Vision. SPringer, 2016b.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruPtions and Perturbations. In International Conference on Learning Representations, 2019.
Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning
of bayesian neural networks. In International Conference on Machine Learning. PMLR, 2015.
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment
your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020.
A Kendall, V Badrinarayanan, and R Cipolla. Bayesian segnet: Model uncertainty in deep convolu-
tional encoder-decoder architectures for scene understanding. In BMVC, 2017.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? Advances in Neural Information Processing Systems, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
International Conference on Learning Representations, 2017.
Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks on cifar-10. Unpublished
manuscript, 2010.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, 2017.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. In Advances in Neural Information Processing Systems, 2018.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In International Conference on
Learning Representations, 2014.
Antonio Loquercio, Mattia Segu, and Davide Scaramuzza. A general framework for uncertainty
estimation in deep learning. IEEE Robotics and Automation Letters, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
A Malinin and M Gales. Predictive uncertainty estimation via prior networks. In Advances in Neural
Information Processing Systems. Curran Associates, Inc., 2018.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing
Systems, 2019.
11
Under review as a conference paper at ICLR 2022
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
Namuk Park, Taekyu Lee, and Songkuk Kim. Vector quantized bayesian neural network inference
for data streams. In AAAI Conference on Artificial Intelligence, 2021.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems,
2019.
Janis Postels, Francesco Ferroni, Huseyin Coskun, Nassir Navab, and Federico Tombari. Sampling-
free epistemic uncertainty estimation using approximated variance propagation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, 2019.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention. Springer, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal ofComputer Vision, pp. 211-252,
2015.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? Advances in Neural Information Processing Systems, 2018.
Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robustness
of visual transformers. arXiv preprint arXiv:2103.15670, 2021.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations, 2015.
Samarth Sinha, Animesh Garg, and Hugo Larochelle. Curriculum by smoothing. Advances in Neural
Information Processing Systems, 2020.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve
Jegou. Training data-effiCient image transformers & distillation through attention. In International
Conference on Machine Learning. PMLR, 2021.
Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin, Nicolas Le Roux, and Ross Goroshin.
An effective anti-aliasing approach for residual networks. arXiv preprint arXiv:2011.10675, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, 2017.
Hao Wang, Xingjian Shi, and Dit-Yan Yeung. Natural-parameter networks: A class of probabilistic
neural networks. Advances in Neural Information Processing Systems, 2016.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.
Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient
ensemble and lifelong learning. In International Conference on Learning Representations, 2020.
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, Jose Miguel Herndndez-Lobato,
and Alexander L Gaunt. Deterministic variational inference for robust bayesian neural networks.
In International Conference on Learning Representations, 2019.
Saining Xie, Ross Girshick, Piotr Dolldr, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2017.
12
Under review as a conference paper at ICLR 2022
425 Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks
426 through the lens of the hessian. In 2020 IEEE International Conference on Big Data (Big Data).
427 IEEE, 2020.
428 Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
429 Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems,
430	2018.
431 Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.
432 Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
433 Theoretically principled trade-off between robustness and accuracy. In International Conference
434	on Machine Learning. PMLR, 2019.
435 Richard Zhang. Making convolutional networks shift-invariant again. In International Conference on
436	Machine Learning. PMLR, 2019a.
437 Richard Zhang. Official meta-review of making convolutional networks shift-invariant again, 2019b.
438	URL https://openreview.net/forum?id=SklVEnR5K7&noteId=rklZnFS-gN.
439 Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
440 features for discriminative localization. In Proceedings of the IEEE Conference on Computer
441	Vision and Pattern Recognition, 2016.
442 Xueyan Zou, Fanyi Xiao, Zhiding Yu, and Yong Jae Lee. Delving deeper into anti-aliasing in convnets.
443	In BMVC, 2020.
13
Under review as a conference paper at ICLR 2022
MC dropout (N=1)	--∙o∙-- MC dropout (N=50)	—∙— MC dropout + Smooth (N=1)	—∙— MC dropout + Smooth (N=50)
Figure A.1: Spatial smoothing improves predictive performance at all dropout rates. As the
dropout rate increases, both accuracy and ECE decrease. The performance is optimized when
accuracy and uncertainty are balanced.
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
A	Experimental Setup and Datasets
We obtain the main experimental results with the Intel Xeon W-2123 Processor, 32GB memory, and
a single GeForce RTX 2080 Ti for CIFAR (Krizhevsky et al., 2009) and CamVid (Brostow et al.,
2008). For ImageNet (Russakovsky et al., 2015), we use AMD Ryzen Threadripper 3960X 24-Core
Processor, 256GB memory, and four GeForce RTX 2080 Ti. We conduct ablation studies with four
Intel Intel Broadwell CPUs, 15GB memory, and a single NVIDIA T4. Models are implemented
in PyTorch(Paszke et al., 2019). The detailed configurations of image classification and semantic
segmentation are as follows.
A. 1 Image Classification
We use VGG (Simonyan & Zisserman, 2015), ResNet (He et al., 2016a), pre-activation ResNet (He
et al., 2016a), and ResNeXt (Xie et al., 2017) in image classification. According to the structure
suggested by Zagoruyko & Komodakis (2016), each block of Bayesian NNs contains one MC dropout
layer.
NNs are trained using categorical cross-entropy loss and SGD optimizer with initial learning rate of
0.1, momentum of 0.9, and weight decay of 5 × 10-4. We also use multi-step learning rate scheduler
with milestones at 60, 130, and 160, and gamma of 0.2 on CIFAR, and with milestones at 30, 60,
and 80, and gamma of 0.2 on ImageNet. We train NNs for 200 epochs with batch size of 128 on
CIFAR, and for 90 epochs with batch size of 256 on ImageNet. We start training with gradual warmup
(Goyal et al., 2017) for 1 epoch on CIFAR. Basic data augmentations, namely random cropping and
horizontal flipping, are used. One exception is the training of ResNeXt on ImageNet. In this case, we
use the batch size of 128 and learning rate of 0.05 because of memory limitation.
We use hyperparameters that minimizes NLL of ResNet. Table A.1 provides hyperparameters for
deterministic and Bayesian NNs. For fair comparison, models with and without spatial smoothing
share hyperparameters such as MC dropout rate. However, Fig. A.1 shows that spatial smoothing
improves predictive performance of ResNet-18 at all dropout rates on CIFAR-100. The default
ensemble size of MC dropout is 50. We report averages of three evaluations, and error bars in figures
represent min and max values. Standard deviations are omitted from tables for better visualization.
See the source code released on GitHub for other details.
A.2 Semantic S egmentation
We use U-Net (Ronneberger et al., 2015) in semantic segmentation. Following Bayesian SegNet
(Kendall et al., 2017), Bayesian U-Net contains six MC dropout layers. We add spatial smoothing
before each subsampling layer in U-Net encoder. We use 5 previous predictions and decay rate of
e-0.8 for temporal smoothing.
14
Under review as a conference paper at ICLR 2022
Table A.1: Hyperparameters of models for image classification.
Dataset	Model	MC dropout rate (%)	|k|	Temperature
	VGG	• 30 •	• • 2	• • 10
		30	2	10
	ResNet	• 30 •	• • 2	• • 10
CIFAR-10		30	2	10
& CIFAR-100	Preact-ResNet	• 30 •	• • 2	• • 10
		30	2	10
	ResNeXt	• 30 •	• • 2	• • 10
		30	2	10
	ResNet	• 5 •	• • 2	• • 10
ImageNet		5	2	10
	ResNeXt	• 5 •	• • 2	• • 10
				
		5	2	10
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
CamVid consists of 720×960 pixels road scene video sequences. We resize the image bilinearly to
360×480 pixels. We use a list reduced to 11 labels by following previous works, e.g. (Kendall & Gal,
2017).
NNs are trained using categorical cross-entropy loss and Adam optimizer with initial learning rate of
0.001 and β1 of 0.9, and β2 of 0.999. We train NN for 130 epoch with batch size of 3. The learning
rate decreases to 0.0002 at the 100 epoch. Random cropping and horizontal flipping are used for
data augmentation. Median frequency balancing is used to mitigate dataset imbalance. Other details
follow Park et al. (2021).
B Ablation Study
The probabilistic spatial smoothing proposed in this
paper consists of two components: Prob and Blur. This
section explores several candidates for each component
and their properties.
B.1 Prob: Feature maps to probabilities
We define Prob as a composition of an upper-
bounded function and ReLU, a function that imposes
the lower bound of zero. Fig. B.1 shows widely used
upper-bounded functions: tanhτ (x) = T tanh(x∕τ),
ReLU6(x) = min(max(x, 6), 0), and constant scaling
which is x∕τ.
Table B.1 shows the predictive performance improve-
ment by Prob with various upper-bounded functions on
Figure B.1: Upper-bounded functions as
a candidates of Prob.
15
Under review as a conference paper at ICLR 2022
Table B.1: We use tanh as the default for Prob based on the predictive performance ofMC dropout
for CIFAR-100 with various Probs.
Model	Smooth	NLL	Acc (%)	ECE (%)
	•	1.133 (-0.000)	68.8 (+0.0)	3.66 (+0.00)
	ReLU ◦ tanh	1.064 (-0.069)	70.4 (+1.6)	2.99 (-0.67)
	ReLU ◦ ReLU6	1.093 (-0.040)	69.8 (+1.0)	4.26 (+0.60)
	ReLU ◦ Constant	0.995 (-0.138)	72.5 (+3.7)	2.11 (-1.55)
VGG-16	Blur	0.985 (-0.000)	72.4 (+0.0)	1.77 (+0.00)
	Blur ◦ ReLU ◦ tanh	0.984 (-0.001)	72.7 (+0.3)	2.07 (+0.30)
	Blur ◦ ReLU ◦ ReLU6	0.982 (-0.003)	72.5 (+0.1)	1.84 (+0.07)
	Blur ◦ ReLU ◦ Constant	0.991 (+0.005)	72.9 (+0.5)	1.03 (-0.74)
	•	1.215 (-0.000)	67.3 (+0.0)	6.37 (+0.00)
	ReLU ◦ tanh	1.131 (-0.084)	69.2 (+1.9)	5.23 (-1.14)
	ReLU ◦ ReLU6	1.166 (-0.049)	68.3 (+1.0)	6.44 (-0.06)
	ReLU ◦ Constant	0.997 (-0.218)	72.5 (+5.2)	1.09 (-5.29)
VGG-19	Blur	1.039 (-0.000)	71.1 (+0.0)	3.12 (+0.00)
	Blur ◦ ReLU ◦ tanh	1.034 (-0.005)	71.3 (+0.2)	3.31 (+0.19)
	Blur ◦ ReLU ◦ ReLU6	1.038 (-0.002)	71.3 (+0.2)	3.84 (+0.72)
	Blur ◦ ReLU ◦ Constant	0.995 (-0.045)	72.3 (+1.2)	1.41 (-1.71)
	•	0.848 (-0.000)	77.3 (+0.0)	3.01 (+0.00)
	ReLU ◦ tanh	0.838 (-0.010)	77.7 (+0.4)	2.92 (-0.08)
	ReLU ◦ ReLU6	0.844 (-0.004)	77.4 (+0.1)	2.74 (-0.27)
	ReLU ◦ Constant	0.825 (-0.023)	77.7 (+0.4)	1.87 (-1.14)
ResNet-18	Blur	0.806 (-0.000)	78.6 (+0.0)	2.56 (+0.00)
	Blur ◦ ReLU ◦ tanh	0.801 (-0.005)	78.9 (+0.3)	2.56 (-0.01)
	Blur ◦ ReLU ◦ ReLU6	0.805 (-0.001)	78.9 (+0.2)	2.59 (+0.03)
	Blur ◦ ReLU ◦ Constant	0.811 (+0.005)	78.5 (-0.2)	1.84 (-0.72)
	•	0.822 (-0.000)	79.1 (+0.0)	6.63 (+0.00)
	ReLU ◦ tanh	0.812 (-0.010)	79.3 (+0.2)	6.74 (+0.11)
	ReLU ◦ ReLU6	0.799 (-0.023)	79.4 (+0.3)	6.71 (+0.08)
	ReLU ◦ Constant	0.788 (-0.034)	79.6 (+0.5)	5.22 (-1.41)
ResNet-50	Blur	0.798 (-0.000)	80.0 (+0.0)	7.21 (+0.00)
	Blur ◦ ReLU ◦ tanh	0.800 (+0.002)	80.1 (+0.1)	7.25 (+0.04)
	Blur ◦ ReLU ◦ ReLU6	0.800 (+0.002)	80.2 (+0.2)	7.30 (+0.09)
	Blur ◦ ReLU ◦ Constant	0.779 (-0.019)	80.4 (+0.4)	5.81 (-1.40)
16
Under review as a conference paper at ICLR 2022
I---ReLU	I
Figure B.2:	Temperature-scaled tanhs (left) and their first derivatives (right) for different tem-
peratures.
MC dropout + Smooth (N=1)	■ MC dropout + Smooth (N=50)
Figure B.3:	The temperature controls the trade-off between accuracy and uncertainty. The
accuracy increases as the temperature increases, but predictions become more overconfident.
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
CIFAR-100. In this experiment, we use models with MC dropout, and τ = 5 for constant scaling. The
results indicate that upper-bounded functions with ReLU tend to improve accuracy and uncertainty
at the same time. In addition, they show that Prob and Blur are complementary. The best results
are obtained when using both Prob and Blur. For the main experiments, we use the composition
of tanhτ and ReLU as Prob, because the hyperparameter of constant scaling is highly dependent on
dataset and model.
Temperature. The characteristics of temperature-scaled tanh depends on τ. Figure B.2 plots
tanhτ and their first derivatives with various temperatures. As shown in this figure, tanhτ has a
couple of useful properties. First, tanhτ has an upper bound of τ . Second, the first derivative of
tanhτ at x = 0 does not depend on τ .
Fig. B.3 shows the predictive performance of ResNet-18 with MC dropout and spatial smoothing for
the temperature on CIFAR-100. In this figure, the accuracy increases as the temperature increases. In
terms of ECE, NN predicts more underconfident results as τ decreases. It is a misinterpretation that
the result is overconfident at low τ because ECE is high. By definition, ECE relies on the absolute
value of the difference between confidence and accuracy. In this example, at low τ , the accuracy is
greater than the confidence, which leads to a high ECE. Moreover, at τ = 0.2, ECE with N = 50 is
greater than that with N = 1, which means that the result is severely underconfident. NLL, a metric
representing both accuracy and uncertainty, is minimized when the accuracy and the uncertainty are
balanced. In conclusion, we set the default value of τ to 10.
17
Under review as a conference paper at ICLR 2022
0.06	0.12	0.06
0.12	0.25	0.12
0.06	0.12	0.06
(a)k= (1)	(b)k=(1,1)	(c)k= (1, 2, 1)
(d)k= (1,4,6,4,1)
Figure B.4:	Kernels for Blur. Brighter background indicates higher importance.
Table B.2: The optimal shape of the blur kernel is model-dependent. We measure the predictive
performance ofMC dropout using spatial smoothing with various size of Blur kernels on CIFAR-100.
Model	|k|	NLL	Acc (%)	ECE (%)
	1	1.087 (-0.000)	69.8 (+0.0)	3.43 (-0.00)
VGG-16	2	1.034 (-0.053)	71.4 (+1.6)	1.06 (-2.37)
	3	0.986 (-0.101)	72.7 (+2.9)	1.03 (-2.40)
	5	1.018 (-0.069)	72.0 (+2.2)	1.32 (-2.11)
	1	1.096 (-0.000)	69.8 (+0.0)	4.74 (-0.00)
VGG-19	2	1.071 (-0.025)	70.4 (+0.6)	2.15 (-2.59)
	3	1.026 (-0.070)	71.9 (+2.1)	2.56 (-2.18)
	5	1.032 (-0.064)	71.6 (+1.8)	2.16 (-2.58)
	1	0.840 (-0.000)	77.6 (+0.0)	2.63 (-0.00)
ResNet-18	2	0.801 (-0.039)	78.9 (+1.4)	2.56 (-0.07)
	3	0.822 (-0.018)	78.7 (+1.1)	2.86 (-0.23)
	5	0.837 (-0.003)	78.4 (+0.8)	3.05 (-0.42)
	1	0.814 (-0.000)	79.5 (+0.0)	6.56 (-0.00)
ResNet-50	2	0.806 (-0.008)	80.0 (+0.5)	7.35 (+0.79)
	3	0.796 (-0.019)	79.9 (+0.4)	7.38 (+0.82)
	5	0.816 (+0.001)	79.4 (-0.1)	7.38 (+0.82)
18
Under review as a conference paper at ICLR 2022
8 7 6
7 7 7
(我)AɔE-rnɔɔv
I O
- 1
5
7
101
Ensemble Size
(故)Wow
2.0-
10o
101
Ensemble Size
-θ- None —9— si Φ s2 —9— s3 O s4
Figure B.5: Spatial smoothing close to the last layer (s3) significantly improves performance.
We report predictive performance of ResNet-18 with one spatial smoothing after each stage on
CIFAR-100. None indicates vanilla MC dropout.
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
B.2	Blur: Averaging neighboring probabilities
Blur is a depth-wise convolution with a kernel. The kernel given by Eq. (8) is derived from various
ks such as k ∈ {(1) , (1, 1) , (1, 2, 1) , (1, 4, 6, 4, 1)}. In these examples, if |k| is 1, Blur is identity.
If |k| is 2, Blur is a box blur, which is used in the main experiments. If |k| is 3 or 5, Blur is an
approximated Gaussian blur.
Table B.2 shows predictive performance of models using spatial smoothing with the kernels on
CIFAR-100. This results show that most kernels improve both accuracy and uncertainty. However,
the most effective kernel size depends on the model.
B.3	Position of s patial smoothing.
As shown in Fig. 5, the magnitude of uncertainty tends to increase as the depth increases. Therefore,
we expect that spatial smoothing close to the output layer will mainly drive performance improvement.
We investigate the predictive performance of models with MC dropout using only one spatial smooth-
ing layer. Figure B.5 shows the predictive performance of ResNet-18 with one spatial smoothing after
each stage on CIFAR-100. The results suggest that spatial smoothing after s3 is the most important
for improving performance. Surprisingly, spatial smoothing after s4 is the least important. This is
because GAP, the most extreme case of spatial smoothing, already exists there.
C	Revisiting Prior Works
As mentioned in Section 2, prior works—namely, GAP, pre-activation, and ReLU6—are spacial cases
of spatial smoothing. This section discusses them in detail.
C.1 Global Average Pooling
The composition of GAP and a fully connected layer is the most popular classifier in classification
tasks. The original motivation and the most widely accepted explanation for the success is that GAP
classifier prevents overfitting because it uses significantly fewer parameters than MLP (Lin et al.,
2014). To verify this claim, we measure the predictive performance of MLP, GAP, and global max
pooling (GMaxP), a classifier that uses the same number of parameters as GAP, on training dataset.
Predictive performance. Table C.1 shows the experimental results on the training and the test
dataset of CIFAR-100, suggesting that the explanation is poorly supported. On both the training
and the test dataset, most predictive performance of MLP is worse than that of GAP. It is a counter-
intuitive result meaning that MLP do not overfit the training dataset. In addition, the performance
improvement by GAP is remarkable in VGG, which has irregular loss landscape. The predictive
19
Under review as a conference paper at ICLR 2022
Table C.1: MLP classifier does not overfit training dataset, i.e., GAP does not regularize NNs. We
provide predictive performance of MC dropout with various classifiers on CIFAR-100. Err is error.
Model	Classifier	Train			Test		
		NLL	ERR (%)	ECE (%)	NLL	Acc (%)	ECE (%)
	GAP	0.0852	0.461	6.75	1.030	72.3	3.24
VGG-16	MLP	0.5492	13.1	13.8	1.133	68.8	3.66
	GMaxP	0.0846	0.470	6.67	1.050	72.2	3.60
	GMedP	0.0867	0.501	6.80	1.042	72.2	3.35
	GAP	0.1825	2.50	10.4	1.035	71.9	1.46
VGG-19	MLP	0.7144	17.7	14.8	1.215	67.3	6.37
	GMaxP	0.1939	2.85	10.6	1.063	71.5	2.10
	GMedP	0.1938	2.80	10.6	1.051	71.7	1.70
	GAP	0.0124	0.0287	1.19	0.841	77.5	2.92
ResNet-18	MLP	0.0076	0.0347	7.22	1.040	74.8	9.55
	GMaxP	0.0113	0.0233	1.41	0.905	76.3	5.23
	GMedP	0.0156	0.0347	1.46	0.889	76.4	5.03
	GAP	0.0061	0.0220	0.48	0.822	79.1	6.63
ResNet-50	MLP	0.0071	0.0370	8.53	1.029	76.9	11.8
	GMaxP	0.0074	0.0313	1.09	0.887	77.2	5.67
	GMedP	0.0053	0.0287	0.47	0.849	78.5	6.29
Intensity	Intensity	Intensity
GAP GAP	MLP	GMaxP	GMedP
O
1
(坎)TlNoɪɪɪ
101
Ensemble Size
(故)WOWO6
1
40
100	101
Ensemble Size
mLp MLP
—»— GMaxP	—∙— GMedP
—GAP
Figure C.1: GAP classifier improves not only the predictive performance on clean dataset but
also the robustness. We measure the predictive performance of ResNet-18 using MC dropout with
classifiers on CIFAR-100-C.
20
Under review as a conference paper at ICLR 2022
(a) MLP classifier
(b) GAP classifier
(c) GAP classifier + Smooth
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
Figure C.2: GAP and spatial smoothing flatten the loss landscapes. We visualize the loss landscape
sequences of ResNet-18 with MC dropout on CIFAR-100. Although each sequence shares the bases,
it fluctuates due to the randomness of the MC dropout.
performance of GMaxP is better than that of MLP, but worse than that of GAP. This shows that using
fewer parameters partially helps to improve predictive performance; however, it is insufficient to
explain the predictive performance improvement by GAP. Finally, global median pooling (GMedP)
provides better predictive performance than GMaxP. It implies that using other noise reduction
methods instead of average pooling helps to improve predictive performance.
Robustness. To evaluate the robustness of the classifiers, we measure the predictive performance of
ResNet-18 using MC dropout with the classifiers on CIFAR-100-C. Figure C.1 shows the experimental
results. This figure suggests that MLP is not robust against data corruption, as we would expect. In
terms of accuracy, the robustness of GMaxP and GMedP is relatively comparable to that of GAP;
however, in terms of uncertainty, GAP is the most robust. These are consistent results with other
spatial smoothing experiments.
Loss landscape visualization. To understand the mechanism of GAP performance improvement,
we investigate the loss landscape. Figure C.2 shows the loss landscape sequences of ResNet with
MC dropout. In this figure, each sequence shares the bases, but they fluctuate due to the randomness
of the MC dropout. Figure C.2a is the loss landscape of the model using MLP classifier instead of
GAP classifier. The loss landscape is chaotic and irregular, resulting in hindering and destabilizing
NN optimization. Fig. C.2b is loss landscape sequence of ResNet with GAP classifier. Since GAP
ensembles all of the feature map points at the last stage, it flattens and stabilizes the loss landscape.
Likewise, as shown in Fig. C.2c, spatial smoothing layers at the end of all stages also flattens and
stabilizes the loss landscape.
Hessian eigenvalue spectra. To evaluate the smoothness of the loss landscapes quantitatively,
we also investigate their Hessians at the optimized weights. In particular, we calculate Hessian
eigenvalue spectra (Ghorbani et al., 2019), distributions of Hessian eigenvalues, to show how spatial
smoothing helps NN optimization. To this end, we try to use stochastic Lanczos quadrature algorithm
21
Under review as a conference paper at ICLR 2022
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
GAP
---- GAP + Smooth
implemented by Yao et al. (2020). However, the problem is that the model with MLP classifier
requires a lot of memory while the algorithm is memory inefficient.
In the training phase, we calculate the mean gradients with respect to mini-batches, rather than the
entire dataset. Therefore, it may be reasonable to investigate the properties of the Hessian “mini-
batch-wisely”. For that purpose, we propose a method, Hessian max eigenvalue spectra, that evaluates
the distribution of “Hessian’s maximum eigenvalues for one mini-batch”. We use power iteration to
produce only the greatest eigenvalue of the Hessian. This algorithm is easy to implement and requires
significantly less memory and computational cost, compared with stochastic Lanczos quadrature
with respect to entire dataset. With this method, we can investigate the Hessian of NNs with MLP
classifiers, which would require a lot of GPU memory.
Figure 7 shows the Hessian max eigenvalue spectra of GAP classi-
fier models with and without spatial smoothing layers. As Li et al.
(2018); Foret et al. (2020) and Appendix D.3 pointed out, Hessian
eigenvalue outliers disturb NN training. This figure explicitly show
that the GAP and spatial smoothing reduce the magnitude of the
Hessian eigenvalues and suppress the outliers, which leads to the
same result as the previous visualizations: GAP as well as spatial
smoothing smoothen the loss landscape. In conclusion, averag-
ing feature map points tends to help neural network optimization
by smoothing, flattening, and stabilizing the loss landscape. We
observe a similar phenomenon for deterministic NNs. We also
evaluate the Hesse eigenvalue spectrum as shown in Fig. C.3, and
it leads to the same conclusion.
In these experiments, we use MLP incorporating dropout layers
with a rate of 50% as the classifier. Since the dropout is one of
the factors that makes MLP underfit the training dataset, we also
evaluate MLP using dropouts with a rate of 0%. Nevertheless, the
results still shows that the predictive performance of MLP is worse
than that of GAP on the training dataset. Moreover, it severely degrades predictive performance of
ResNet on the test dataset.
C.2 Pre-activation
He et al. (2016b) experimentally showed that the pre-activation arrangement, in which the activation
ReLU ◦ BatchNorm is placed before the convolution, improves the accuracy of ResNet. Since γs of
most BatchNorms in CNNs are near-zero (Frankle et al., 2021), BatchNorms reduce the magnitude
of feature maps. As shown in Fig. B.1, constant scaling is a non-trainable BatchNorm with no
bias, and it also reduces the magnitude of feature map. In Table B.1, we show that constant scaling
improves predictive performance. Considering the similarity between Prob with constant scaling and
conventional activation, i.e.,the similarity between ReLUOConstantScaling and ReLUoBatchNorm,
we find that the pre-activation arrangement improves uncertainty as well as accuracy, because
convolutions act as a Blur.
To show this, we change the post-activation of all layers to pre-activation, and measure the predictive
performance. For ResNet, we follow the original paper by He et al. (2016b). Table C.2 shows
the predictive performance of models with pre-activation. The results suggests that pre-activation
improves both accuracy and uncertainty in most cases. For deterministic VGG-19, pre-activation
significantly degrades accuracy but improves NLL. In conclusion, they imply that pre-activation is a
special case of spatial smoothing.
Santurkar et al. (2018) argued that BatchNorm helps in optimization by flattening the loss landscape.
We show that spatial smoothing flattens and smoothens the loss landscape, which is a consistent
explanation. It will be interesting to investigate if BatchNorm helps in ensembling feature maps.
C.3 RELU6
ReLU6 was experimentally introduced to improve predictive performance (Krizhevsky & Hinton,
2010). Sandler et al. (2018) used “ReLU6 as the non linearity because of its robustness when used
O	10	20
Eigenvlaue
Figure C.3: Spatial smoothing
suppress eigenvalue outliers.
We provide Hessian eigenvalue
spectra of ResNet-18 with MC
dropout on CIFAR-100. See also
Fig. 7.
22
Under review as a conference paper at ICLR 2022
Table C.2: Pre-activation arrangement improves uncertainty as well as accuracy. We measure
the predictive performance of models with pre-activation arrangement on CIFAR-100.
Model	MC dropout	Pre-act	NLL	Acc (%)	ECE (%)
	•	•	2.047 (-0.000)	71.6 (+0.0)	19.2 (-0.0)
VGG-16	•	X	1.827 (-0.219)	72.5 (+0.9)	19.8 (+0.6)
	X	•	1.133 (-0.000)	68.8 (+0.0)	3.66 (-0.00)
	X	X	1.036 (-0.096)	71.7 (+2.9)	3.55 (-0.11)
	•	•	2.016 (-0.000)	67.6 (+0.0)	21.2 (-0.0)
VGG-19	•	X	1.799 (-0.217)	64.4 (-3.2)	17.2 (-4.0)
	X	•	1.215 (-0.000)	67.3 (+0.0)	6.37 (-0.00)
	X	X	1.084 (-0.131)	70.1 (+3.7)	4.23 (-2.14)
	•	•	0.983 (-0.000)	77.1 (+0.0)	7.75 (-0.00)
ResNet-18	•	X	0.934 (-0.049)	77.6 (+0.5)	8.04 (+0.29)
	X	•	0.937 (-0.000)	76.9 (+0.0)	5.11 (-0.00)
	X	X	0.872 (-0.065)	77.6 (+0.7)	5.53 (+0.42)
	•	•	0.880 (-0.000)	79.0 (+0.0)	8.35 (-0.00)
ResNet-50	•	X	0.870 (-0.010)	79.4 (+0.4)	8.27 (-0.08)
	X	•	0.831 (-0.000)	78.6 (+0.0)	6.06 (-0.00)
	X	X	0.819 (-0.012)	79.5 (+0.9)	6.29 (+0.23)
624
625
626
627
628
629
630
631
632
633
634
635
636
with low-precision computation”. In Table B.1, we show that ReLU6s at the end of stages helps to
ensemble spatial information by transforming the feature map to Bernoulli distributions. Since spatial
smoothing improves robustness against data corruption, it seems reasonable that ReLU6 is robust to
low-precision computation. A more abundant investigation into this topic is promising future works.
We measure the predictive performance of NNs using all activations as ReLU6 instead of ReLU.
However, in contrast to the results in Table B.1, the results are not consistent. We speculate that the
reason is that a lot of ReLU6s overly regularize NNs.
D Extended Analysis of How spatial smoothing Works
This section provides further explanation of the analysis in Section 2.2.
D.1 Neighboring Feature Maps in CNNs Are Similar
This work exploits the spatial consistency of feature maps, i.e., neighboring feature maps in CNNs
are similar. Below, we theoretically and empirically prove the spatial consistency. Moreover, this
spatial consistency of feature maps holds even if the input data is spatially inconsistent.
Consider a single-layer convolutional neural network with one channel:
yi = [w * χ]i
k
=	wlxi-l+1
l=1
(9)
(10)
23
Under review as a conference paper at ICLR 2022
(a) single-layer CNN
(b) five-layer CNN with ReLU
Figure D.1: Neighboring feature map points in CNNs are similar, even if input values are iid.
We provide covariances of feature map points with respect to the center feature map (in the red
square). Input values are Gaussian random noise. Left: A single convolutional layer correlates the
target feature map with another feature map that is 3 pixels away, since the kernel size is 3×3. Right:
A deep CNN more strongly correlates neighboring feature maps.
637
638
639
640
641
642
643
644
645
646
647
where * is convolution operator with a kernel of size k, y is feature map output, W is kernel weight,
and x is input random variable. Then, the covariance of two neighboring feature maps is:
kk
Cov(yi, yi+1) = Cov(	wlxi-l+1,	wmxi-m+2)	(11)
kk
=	wlwm Cov(xi-l+1 , xi-m+2)	(12)
l=1 m=1
k-1
=Ewlwι+ι σ2(xi-i+2) +------ (13)
l=1
where σ2 (xi-l+1) is the variance of xi-l+1. Therefore, Cov(yi, yi+1) is non-zero for randomly
initialized weights. If x is iid, i.e., Cov(xi, xj) = δijσ2(xi) where δij is the Kronecker delta, the
remainders in Eq. (13) vanish.
For example, the covariance of two neighboring feature map points in a CNN with a kernel size of 3
is:
Cov(y1, y2) = w1w1 Cov(x1, x2) + w1w2 Cov(x1, x3) + w1w3 Cov(x1, x4)
+	w2w1 Cov(x2, x2)	+ w2w2	Cov(x2, x3)	+	w2w3 Cov(x2, x4)	(14)
+	w3w1 Cov(x3, x2)	+ w3w2	Cov(x3, x3)	+	w3w3 Cov(x3, x4)
When xi is iid, the covariance is:
Cov(y1, y2) = w1w2 σ2(x2) + w2w3 σ2(x3)	(15)
Since it is non-zero, the neighboring feature maps y1 and y2 are correlated.
Experiment. To demonstrate the spatial consistency of feature maps empirically, we provide
feature map covariances of randomly initialized single-layer CNN and five-layer CNN with ReLU
non-linearity. In this experiment, the input values are Gaussian random noises. As shown in Fig. D.1a,
one convolutional layer correlates neighboring feature map points. Fig. D.1b shows that multiple
convolutional layers correlate one feature map with distant feature maps. Moreover, the feature maps
in deep CNNs have a stronger relationship with neighboring feature maps.
D.2 Ensemble Filters High-Frequency Signals
Following the notation of Eq. (3), the ensemble is convolution of importance π and prediction p:
π *p	(16)
24
Under review as a conference paper at ICLR 2022
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
where ∏i,j = ∏(xi∣Xj) and Pi = p(y∣xi, wi). To show that this ensemble is low-pass filter, We
apply the convolution N times:
π * …* π *p	(17)
'----{z----}
N times
Since π is probability, i.e., Pi ∏i,j = 1, π *∙∙∙* π is the probability for the sum of N random
variables from π, i.e, φ + •一+ φ 〜π *∙∙∙* π where φ 〜π. By definition, an operator is
low-pass filter if and only if the high frequency component vanishes when the operator is applied
infinitely. Therefore, ensemble with π is low-pass filter because Var(φ +----+ φ) = N Var(φ) and
F [π * …* π * p] = F [π * …* π] F [p] where F is Fourier transform.
Experiment. Since blur filter is low-pass filter, probabilistic spatial smoothing is also low-pass
filter. In Section 2.2, at the end of the stage 1, we show that MC dropout adds high-frequency noise to
feature maps, and spatial smoothing effectively removes it. As shown in Fig. D.2, we observe the
same phenomena at other stages.
In addition, Fig. 6c shows that CNNs are vulnerable to high-frequency random noise. Interestingly, it
also shows that CNNs are robust against noise with frequencies from 0.6π to 0.8π, corresponding to
approximately 3 pixel periods. Since the receptive fields of convolutions are 3×3, the noise with a
period smaller than the size is averaged out by convolutions. For the same reason, convolutions are
particularly vulnerable against the noise with a frequency of 0.3π, corresponding to a period of 6
pixel.
D.3 Randomness Sharpens Loss Landscape, and Ensemble Smoothens It
Ws show that the randomness of BNNs hinder and destabilize NN training because it causes the loss
landscape and its gradient to fluctuate from moment to moment. In other words, the randomness,
such as dropout, sharpens the loss landscape.
To show the claim theoretically, we use Foret et al. (2020)’s definition of sharpness with respect to
training dataset D:
sharpness = max LD(w + δw) - LD(w)	(18)
ρ δw≤ρ
where LD is NLL loss, w is NN weight, δw is small weight perturbation, and ρ is neighborhood
radius. Therefore, as dropout rate■—and the magnitude of δw—increases, the sharpness increases.
We next calculate the sharpness more rigorously. Let pi ∈ (0, 1] be a confidence of one NN prediction,
and P(N) be a confidence of N ensemble, i.e., P(N) = N PIN=IPi. Then, the variance of the NLL
loss is:
V [L]= V D X -log P(N)
=∣⅛ V h-log P(N )i
'ɪ V [- log μ +(1- PN
∣D∣ L	∖ μ
1 V [Pi]
Nμ2∣D∣
1 σpred
Nμ2∣D∣
(19)
(20)
(21)
(22)
(23)
(24)
where μ = P(∞) and σ2red is predictive variance of confidence. We use the formula V
[N∙ PrN= ξ]
NV [ξ] for arbitrary random variable ξ, and we take the first-order Taylor expansion with an assump-
25
Under review as a conference paper at ICLR 2022
(a) Deterministic
(b) Deterministic + Smooth
(c) MC dropout
(d) MC dropout + Smooth
0,0π	0.5π	1.0π
Frequency
0.0π	0.5 冗	l,0π
Frequency
VPmndUIB Mo-M
0.0π	0.5π	1.0π
Frequency
.0.0
2 1
əpmuum 80u
0.0π	0.5 兀	1.0π
Frequency
Deterministic --------- Deterministic + Smooth ---------MC dropout ------------- MC dropout + Smooth
(e) Diagonal components
Figure D.2: Spatial smoothing filters high-frequency signals including MC dropout noise. We
present average feature maps of ResNet-50 on ImageNet in frequency space by using Fourier
transform. Each column corresponds to feature maps at stage 1 to 4.
26
Under review as a conference paper at ICLR 2022
A4∙su3pbooα
-"suəɑ
O	IO	20
Eigenvlaue
O 200	400	600
Max Eigenvlaue
----0%	---- 10%	--- 20%	----- 30%
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
Figure D.3: Randomness due to MC dropout sharpens the loss function. We provide Hessian
eigenvalue (left) and Hessian max eigenvalue spectra (right) of ResNet-18 on CIFAR-100.
tion P(N) ` μ in Eq. (21). Therefore, the approximated sharpness is:
2
SharpneSSP ` N*	(25)
In conclusion, the variance of NLL, (the square of) the sharpness, is proportional to the variance of
predictions σp2red and inversely proportional to the ensemble size N. As the ensemble size increases
in the training phase, the loss landscape becomes smoother. Flat loss landscape results in better
predictive performance and generalization (Foret et al., 2020).
Here, we only consider model uncertainty for the sake of simplicity. Extending the formulations to
data uncertainty is straightforward. The predictive distribution of data-complemented BNN inference
(Park et al., 2021) is:
p(y|S, D) =	p(y|x, w)p(x|S)p(w|D)dxdw	(26)
=	p(y|z)p(z|S, D)dz	(27)
where S is proximate data distribution, z = (x, w), andp(z|S,D) = p(x|S) p(w|D). This equation
clearly shows that w and x are symmetric. Therefore, we obtain the formulas including both model
and data uncertainty by replacing w with joint random variable of x and w, i.e. w → z = (w, x).
Experiment. Above, we claim two statements. First, the higher the dropout rate, the sharper the
loss landscape. Second, the variance of the loss is inversely proportional to the ensemble size.
To demonstrate the former claim quantitatively, we compare the Hessian eigenvalue spectra and the
Hessian max eigenvalue spectra of MC dropout with various dropout rates. In these experiments,
we use ensemble size of one for MC dropout. For detailed explanation of Hessian max eigenvalue
spectrum, see Appendix C.1.
Fig. D.3 represents the spectra, which reveals that as the randomness of the model increases, the
number of Hessian eigenvalue outliers increases. Since outliers are detrimental to the optimization
process (Ghorbani et al., 2019), dropout disturb NN optimization.
To show the latter claim, we evaluate the variance of NLL loss for ensemble size Ntrain as shown in
Fig. D.4a. As we would expect, the variance of the NLL loss—the sharpness of the loss landscape—is
inversely proportional to the ensemble size for large Ntrain.
D.4 Training Phase Ensemble Leads to Better Performance
Appendix D.3 raises an immediate question: Is there a performance difference between ‘training
with prediction ensemble’ and ‘training with a low MC dropout rate, instead of no ensemble’? Note
27
Under review as a conference paper at ICLR 2022
Ntrain
(b) NLL for ensemble size on test dataset
(a) V [L] for ensemble size on training dataset
Figure D.4: Training phase ensemble helps NN learn strong representation. Left: The variance
of NLL (V [L]) on training dataset is inversely proportional to the ensemble size for large Ntrain. See
Eq. (24). Right: Training phase ensemble improves the predictive performance on test dataset.
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
that both methods reduce the sharpness of the loss landscape. This section answers the question
by providing theoretical and experimental explanations that the ensemble in the training phase can
improve predictive performance.
According to Gal & Ghahramani (2016), the total predictive variance (in regression tasks) is:
σ2	σ2	+ σ2
σpred = σmodel + σsample
(28)
where σm2 odel is model precision and σs2ample is NN prediction variance. Therefore, the model precision
is the lower bound of the predictive variance, i.e.:
σp2red ≥ σm2odel	(29)
The model precision depends only on the model architecture. For example, in the case of MC dropout,
σm2 odel is proportional to the dropout rate (Gal & Ghahramani, 2016) as follows:
σ22odei α dropout rate
(30)
These suggest that model precision dominate predictive variance if the MC dropout rate is large
enough, i.e., even if the number of ensembles is increased in the training phase, the predictive variance
is almost the same. In contrast, decreasing the MC dropout rate reduces prediction diversity, and it
obviously leads to performance degradation. Therefore, in the training phase, it is better to ensemble
predictions than to lower the MC dropout rate. We believe that the training phase ensemble is strongly
correlated with Batch Augmentation (Hoffer et al., 2020). We leave concrete analysis for future work.
Experiment. The experiments below support the theoretical analysis. We train MC dropout by
using training-phase ensemble method with various ensemble sizes Ntrain.
As we would expect, Fig. D.4b shows that training phase ensemble significantly improves the
predictive performance. In this experiment, we use MC dropout rate of 30%. As shown in Fig. A.1, it
provides the best predictive performance. We use ensemble size Ntest = 50 in test phase.
We also measure the predictive variances of NLL. The predictive variances of the model with
Ntrain = 1 and with Ntrain = 3 are V [L] = 0.0169 and V [L] = 0.0179, respectively. Since the
predictive variances of the two models are almost the same, we infer that there exists a lower bound.
E Extended Informations of Experiments
This section provides additional information on the experiments in Section 3.
E.1 Image Classification
We present numerical comparisons in the image classification experiment and discuss the results in
detail.
28
Under review as a conference paper at ICLR 2022
Table E.1: Spatial smoothing improves both accuracy and uncertainty at the same time. Predic-
tive performance of models with spatial smoothing in image classification on CIFAR-10, CIFAR-100,
and ImageNet.
Model & Dataset	MC dropout	Smooth	NLL	Acc (%)	ECE (%)
	•	•	0.401 (-0.000)	93.1 (+0.0)	3.80 (-0.00)
VGG-19 &	•	X	0.376 (-0.002)	93.2 (+0.1)	5.49 (+1.69)
CIFAR-10	X	•	0.238 (-0.000)	92.6 (+0.0)	3.55 (-0.00)
	X	X	0.197 (-0.041)	93.3 (+0.7)	0.68 (-2.86)
	•	•	0.182 (-0.000)	95.2 (+0.0)	2.75 (-0.00)
ResNet-18 &	•	X	0.173 (-0.009)	95.4 (+0.2)	2.31 (-0.44)
CIFAR-10	X	•	0.157 (-0.000)	95.2 (+0.0)	1.14 (-0.00)
	X	X	0.144 (-0.014)	95.5 (+0.2)	1.04 (-0.10)
	•	•	2.047 (-0.000)	71.6 (+0.0)	19.2 (-0.0)
VGG-16 &	•	X	1.878 (-0.169)	72.2 (+0.6)	20.5 (+1.3)
CIFAR-100	X	•	1.133 (-0.000)	68.8 (+0.0)	3.66 (-0.00)
	X	X	1.034 (-0.099)	71.4 (+2.6)	1.06 (-2.60)
	•	•	2.016 (-0.000)	67.6 (+0.0)	21.2 (-0.0)
VGG-19 &	•	X	1.851 (-0.165)	71.7 (+4.0)	20.2 (-1.0)
CIFAR-100	X	•	1.215 (-0.000)	67.3 (+0.0)	6.37 (-0.00)
	X	X	1.071 (-0.144)	70.4 (+3.0)	2.15 (-4.22)
	•	•	0.886 (-0.000)	77.9 (+0.0)	4.97 (-0.00)
ResNet-18 &	•	X	0.863 (-0.023)	78.9 (+1.0)	4.40 (-0.57)
CIFAR-100	X	•	0.848 (-0.000)	77.3 (+0.0)	3.01 (-0.00)
	X	X	0.801 (-0.047)	78.9 (+1.6)	2.56 (-0.45)
	•	•	0.835 (-0.000)	79.9 (+0.0)	8.88 (-0.00)
ResNet-50 &	•	X	0.834 (-0.002)	80.7 (+0.8)	9.29 (+0.42)
CIFAR-100	X	•	0.822 (-0.000)	79.1 (+0.0)	6.63 (-0.00)
	X	X	0.800 (-0.022)	80.1 (+1.0)	7.25 (+0.62)
	•	•	0.804 (-0.000)	80.6 (+0.0)	8.23 (-0.00)
ResNeXt-50 &	•	X	0.825 (+0.022)	80.8 (+0.3)	9.41 (+1.18)
CIFAR-100	X	•	0.762 (-0.000)	80.5 (+0.0)	5.67 (-0.00)
	X	X	0.759 (-0.002)	80.7 (+0.2)	6.62 (+0.94)
	•	•	1.210 (-0.000)	70.3 (+0.0)	1.62 (-0.00)
ResNet-18 &	•	X	1.183 (-0.027)	70.6 (+0.3)	1.22 (-0.40)
ImageNet	X	•	1.215 (-0.000)	70.0 (+0.0)	1.39 (-0.00)
	X	X	1.190 (-0.032)	70.6 (+0.6)	2.25 (+0.86)
	•	•	0.949 (-0.000)	76.0 (+0.0)	2.97 (-0.00)
ResNet-50 &	•	X	0.916 (-0.033)	76.9 (+0.9)	3.46 (+0.49)
ImageNet	X	•	0.945 (-0.000)	76.0 (+0.0)	1.89 (-0.00)
	X	X	0.905 (-0.040)	77.0 (+1.0)	2.49 (+0.60)
	•	•	0.919 (-0.000)	77.7 (+0.0)	3.63 (-0.00)
ResNeXt-50 &	•	X	0.907 (-0.012)	78.0 (+0.3)	4.60 (+0.97)
ImageNet	X	•	0.895 (-0.000)	77.7 (+0.0)	2.53 (-0.00)
	X	X	0.887 (-0.008)	78.1 (+0.4)	3.28 (+0.75)
29
Under review as a conference paper at ICLR 2022
Ensemble Size
(a) VGG-19 on CIFAR-10
Ensemble Size
(b) ResNet-18 on CIFAR-10
6.0-
4.0-
2.0-
100	101
Ensemble Size
3.0-
2.0-
1.0-
100	101
Ensemble Size
8.0
6.0
4.0
100	101
Ensemble Size
(c) ResNet-18 on CIFAR-100
Ensemble Size
(d) ResNet-50 on CIFAR-100
Ensemble Size
(e) ResNet-50 on ImageNet
Ensemble Size
Deep ensemble	-⅜- Deep ensemble + Smooth -Q- MC dropout ■ MC dropout + Smooth
Figure E.1: Spatial smoothing improves both accuracy and uncertainty across a whole range of
ensemble sizes.
30
Under review as a conference paper at ICLR 2022
Test 1	2	3	4	5
Intensity
Intensity
Detennimstic	Detenninistic + Smooth	MC dropout	MC dropout + Smooth
r1°
0 5 0
7 6 6
(坎)ατN?
101
Ensemble SiZe
85.0-
100
90.0-
87.5-
101
Ensemble Size
100	101
Ensemble Size
D Deterministic ♦ Deterministic + Smooth -G— MC dropout ' MC dropout + Smooth
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
Figure E.3: Spatial smoothing improves corruption robustness. We measure the predictive per-
formance of ResNet-18 on CIFAR-100-C. In the top row, we use an ensemble size of fifty for MC
dropout with and without spatial smoothing.
Computational performance. The throughput of MC dropout and “MC dropout + spatial smooth-
ing” is 755 and 675 image/sec, respectively, in training phase on ImageNet. As mentioned in lines
Section 3.1, NLL of “MC dropout + spatial smoothing” with ensemble size of 2 is comparable
to or even better than that of MC dropout with ensemble size of 50. Therefore, “MC dropout +
spatial smoothing” is 22× faster than MC dropout with similar predictive performance, in terms of
throughput.
Predictive performance on test dataset. Fig. E.2 repre-
sents the reliability diagram of ResNet-18 on CIFAR-100,
which shows that spatial smoothing improves the uncertainty
of both deterministic and Bayesian NNs. Numerical compar-
isons are provided below.
Table E.1 shows the predictive performance of various deter-
ministic and Bayesian NNs with and without spatial smooth-
ing on CIFAR-10, CIFAR-100, and ImageNet. This table
suggests the following: First, spatial smoothing improves
both accuracy and uncertainty in most cases. In particular, it
improves the predictive performance of all models with MC
dropouts. Second, spatial smoothing significantly improves
the predictive performance of VGG compared with ResNet.
VGG has a chaotic loss landscape, which results in poor pre-
dictive performance (Li et al., 2018), and spatial smoothing
smoothens its loss landscape effectively. Third, as the depth
increases, the performance improvement decreases. Deeper
NNs provide more overconfident results (Guo et al., 2017),
but the number of spatial smoothing layers calibrating uncer-
tainty is fixed. Last, the performance improvement of ResNeXt, which includes an ensemble in its
internal structure, is relatively marginal.
Fig. E.1 shows predictive performance of MC dropout and deep ensemble for ensemble size. A
deep ensemble with an ensemble size of 1 is a deterministic NN. This figure shows that spatial
smoothing improves efficiency of ensemble size and the predictive performance at ensemble size of
Confidence (%)
-¼- Deterministic	-O— MC dropout
. Deterministic + Smooth . MC dropout + Smooth
Figure E.2: Spatial smoothing cal-
ibrates predictions. We present re-
liability diagram of ResNet-18 on
CIFAR-100.
31
Under review as a conference paper at ICLR 2022
Table E.2: Spatial smoothing improves adversarial robustness. We measure the accuracy (ACC)
and the Attack Success Rate (ASR) of ResNet-50 against adversarial attacks on ImageNet.
Attack	MC dropout	Smooth	Acc (%)	ASR (%)
	•	•	28.3 (+0.0)	62.9 (-0.0)
FGSM	•	X	30.3 (+2.0)	60.5 (-2.4)
	X	•	30.3 (+0.0)	59.8 (-0.0)
	X	X	32.6 (+2.3)	57.4 (-2.4)
	•	•	7.5 (+0.0)	90.1 (-0.0)
PGD	•	X	9.0 (+1.4)	88.2 (-1.9)
	X	•	12.2 (+0.0)	83.7 (-0.0)
	X	X	13.7 (+1.5)	82.1 (-1.6)
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
50. In addition, spatial smoothing stabilizes NN training. It reduces the variance of the performance,
especially in VGG.
A peculiarity of the results on ImageNet is that spatial smoothing degrades ECE of ResNet-50. It
is because spatial smoothing significantly improves the accuracy in this case, and there tends to be
a trade-off between accuracy and ECE, e.g. as shown in (Guo et al., 2017), Fig. A.1, and Fig. B.3.
Instead, spatial smoothing shows the improvement in NLL, another uncertainty metric.
Predictive performance on training datasets. Note that spatial smoothing helps NN learn strong
representations. In other words, spatial smoothing does not regularize NNs. For example, NLL
ResNet-18 with MC dropout on CIFAR-100 training dataset is 2.20 × 10-2. The NLL of the ResNet
with spatial smoothing is 1.94 × 10-2. In conclusion, spatial smoothing reduces the training loss.
Corruption robustness. We measure predictive performance on CIFAR-100-C (Hendrycks &
Dietterich, 2019) in order to evaluate the robustness of the models against 5 intensities and 15 types
of data corruption. The top row of Fig. E.3 shows the results as a box plot. The box plot shows the
median, interquartile range (IQR), minimum, and maximum of predictive performance for types.
They reveal that spatial smoothing improves predictive performance for corrupted data. In particular,
spatial smoothing undoubtedly helps in predicting reliable uncertainty.
To summarize the performance of corrupted data in a single value, Hendrycks & Dietterich (2019)
introduced a corruption error (CE) for quantitative comparison. CEcf, which is CE for corruption type
c and model f, is as follows:
CEcf =X5 Eif,c!X5 EiA,clexNet!	(31)
where Eif,c is top-1 error of f for corruption type c and intensity i, and EiA,clexNet is the error of AlexNet.
Mean CE or mCE summarizes CEcf by averaging them over 15 corruption types such as Gaussian
noise, brightness, and show. Likewise, to evaluate robustness in terms of uncertainty, we introduce
corruption NLL (CNLL, 1) and corruption ECE (CECE, 1) as follows:
and
32
Under review as a conference paper at ICLR 2022
Table E.3: Spatial smoothing improves the consistency, robustness against shift-perturbation.
We measure the consistency of ResNet-18 on CIFAR-10-P. Deterministic NN with N = 5 means
deep ensemble.
MC dropout	Smooth	N	Cons (%)	CEC (×10-2)
•	•	1	97.9 (+0.0)	1.03 (-0.00)
•	X	1	98.2 (+0.3)	1.16 (+0.13)
•	•	5	98.7 (+0.0)	1.22 (-0.00)
•	X	5	98.9 (+0.2)	1.33 (+0.11)
X	•	50	98.2 (+0.0)	1.29 (-0.00)
X	X	50	98.4 (+0.2)	1.34 (+0.05)
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
where NLLif,c and ECEif,c are NLL and ECE of f for c and i, respectively. mCNLL and mCECE
are averages over corruption types. Experimental results show that spatial smoothing improves the
robustness against data corruption. See Fig. E.3 for the results.
The bottom row of Fig. E.3 shows mCNLL, mCE, and mCECE for ensemble size. They indicates that
spatial smoothing improves not only the efficiency but corruption robustness across a whole range of
ensemble size.
Adversarial robustness. We show that spatial smoothing also improves adversarial robustness.
First, we measure the robustness, in terms of accuracy and attack success rate (ASR), of ResNet-
50 on ImageNet against popular adversarial attacks, namely FGSM (Goodfellow et al., 2015) and
PGD (Madry et al., 2018). Table E.2 indicate that both MC dropout and spatial smoothing improve
robustness against adversarial attacks.
Next, we find out how spatial smoothing improves adversarial robustness. To this end, similar
to Section 2.2, we measure the accuracy on the test datasets with frequency-based adversarial
perturbations. In this experiment, we use FGSM attack. This experimental result shows that spatial
smoothing is particularly robust against high frequency (≥ 0.3π) adversarial attacks. This is because
spatial smoothing is a low-pass filter, as we mentioned in Section 2.2. Since the ResNet is vulnerable
against high frequency adversarial attack, an ef-
fective defense of spatial smoothing against high
frequency attacks significantly improves the robust-
ness.
Consistency. To evaluate the translation invari-
ance of models, we use consistency (Hendrycks &
Dietterich, 2019; Zhang, 2019a), a metric represent-
ing translation consistency for shift-translated data
sequences S = {xι,…,XM +ι}, as follows:
1M
Consistency = M £l(g(xi) = g(xi+ι))
M i=1
(34)
Translation (PX)
where g(x) = arg max p(y|x, D). Table E.3 pro-
vides consistency of ResNet-18 on CIFAR-10-P
(Hendrycks & Dietterich, 2019). The results shows
that MC dropout and deep ensemble improve con-
sistency, and spatial smoothing improves consis-
tency of both deterministic and Bayesian NNs.
Prior works (Zhang, 2019a; Azulay & Weiss, 2019)
investigated the fluctuation of predictive confidence
—0— Deterministic	MC- MC dropout
—D- Deterministic + Smooth	—∙— MC dropout + Smooth
Figure E.4: Spatial smoothing improves the
confidence when the predictions are incorrect.
We define relative confidence (See Eq. (36)),
and measure the metric of ResNet-18 on CIFAR-
10-P.
33
Under review as a conference paper at ICLR 2022
Table E.4: Spatial smoothing and temporal smoothing are complementary. We provide predictive
performance of MC dropout in semantic segmentation on CamVid for each method. Spat and Temp
each stand for spatial smoothing and temporal smoothing. Cons stands for consistency.
MC dropout	Spat	Temp	N	NLL	Acc (%)	ECE (%)	Cons (%)
•	•	•	1	0.354 (+0.000)	92.3 (+0.0)	4.95 (+0.00)	95.1 (+0.0)
•	X	•	1	0.318 (+0.036)	92.4 (+0.1)	4.54 (+0.41)	95.5 (+0.4)
•	•	X	1	0.290 (+0.064)	92.5 (+0.2)	3.18 (+1.77)	96.3 (+1.2)
•	X	X	1	0.278 (+0.076)	92.5 (+0.2)	3.03 (+1.92)	96.6 (+1.5)
X	•	•	50	0.298 (+0.000)	92.5 (+0.0)	4.20 (+0.00)	95.4 (+0.0)
X	X	•	50	0.284 (+0.014)	92.6 (+0.1)	3.96 (+0.24)	95.6 (+0.2)
X	•	X	1	0.273 (+0.025)	92.6 (+0.1)	3.23 (+0.97)	96.4 (+1.0)
X	X	X	1	0.260 (+0.038)	92.6 (+0.1)	2.71 (+1.49)	96.5 (+1.1)
on shift-translated data sequence. However, surprisingly, we find that confidence fluctuation has
little to do with consistency. To demonstrate this claim, we introduce cross-entropy consistency
(CEC, 1), a metric that represents the fluctuation of confidence on a shift-translated data sequence
S = {xι, ∙∙∙ , XM+ι}, as follows:
1M
cec = -MEf(Xi) ∙ log(f(Xi+1))
i=1
(35)
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
where f(X) = p(y|X, D). In Table E.3, high consistency does not mean low CEC; conversely, high
consistency tends to be high CEC. Canonical NNs predict overconfident probabilities, and their
confidence sometimes changes drastically from near-zero to near-one. Correspondingly, it results in
low consistency but low CEC. On the contrary, well-calibrated NNs such as MC dropout provide
confidence that oscillates between zero and one, which results in high CEC.
To represent the NN reliability properly, we propose relative confidence (↑) as follows:
Relative confidence = p(ytrue|X, D) maxp(y|X, D)	(36)
where maxp(y|X, D) is confidence of predictive result and p(ytrue|X, D) is probability of the result
for true label. It is 1 when NN classifies the image correctly, and less than 1 when NN classifies it
incorrectly. Therefore, relative confidence is a metric that indicates the overconfidence of a prediction
when NN’s prediction is incorrect.
Figure E.4 shows a qualitative example of consistency on CIFAR-10-P by using relative confidence.
This figure suggests that spatial smoothing improves consistency of both deterministic and Bayesian
NN.
E.2 Semantic Segmentation
Table E.4 shows the performance of U-Net on the CamVid dataset. This table indicates that spatial
smoothing improves accuracy, uncertainty, and consistency of deterministic and Bayesian NNs.
This is consistent with the results in image classification. In addition, temporal smoothing leads
to significant improvement in efficiency of ensemble size, accuracy, uncertainty, and consistency
by exploiting temporal information. Moreover, temporal smoothing requires only one ensemble to
achieve high predictive performance, since it cooperates with the temporally previous predictions. We
obtain the best predictive and computational performance by using both temporal smoothing and
spatial smoothing.
34
Under review as a conference paper at ICLR 2022
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
F Comparison With Anti-aliased CNN
As we mentioned in Section 4, local means (Blur), also known as anti-aliased CNN (Zhang, 2019a),
improve accuracy. Nevertheless, our work (Prob + Blur) has novelties in three respects: different
motivation, improved uncertainty estimation, and analysis of how spatial smoothing works.
Different motivation. The motivation of local means was to mitigate the aliasing effect of subsam-
pling and to improve shift invariance. In contrast, our spatial smoothing is introduced to aggregate
and ensemble nearby feature map points.
Improved uncertainty estimation. We demonstrate that spatial smoothing improves not only
accuracy, but also uncertainty estimation and robustness against natural corruptions and adversarial
attacks all at the same time. Moreover, we show that spatial smoothing significantly enhances the
performance of MC dropout. Since there typically tends to be a trade-off between accuracy and
“uncertainty + robustness”—e.g. as shown in (Guo et al., 2017; Zhang et al., 2019; Geirhos et al.,
2019; Zhang, 2019b), Fig. A.1, and Fig. B.3—in NN modeling, we believe our simple yet effective
method makes major inroads into the uncertainty quantification and generalization.
Analysis of how spatial smoothing improves performance. We find that the predictive perfor-
mance improvement is not due to the anti-aliasing effect of local means.
•	Prob + Blur—our probabilistic spatial smoothing—improves the performance of pre-
activation CNNs, but Blur alone—local mean or anti-aliased CNN—does not. In fact,
contrary to (Zhang, 2019a), local mean degrades the predictive performance since it results
in loss of information. It suggests that Prob plays an key role in prediction. For more details,
see Appendix F.1.
•	Although the local filtering can result in loss of information, Zhang (2019a) experimentally
observed an increase in both shift-invariance (as expected) and accuracy (which was be-
yond expectation). However, “there exist a fundamental trade-off between ‘shift-invariance
plus anti-aliasing’ and performance” (Zhang, 2019b). Moreover, it is difficult to relate
anti-aliasing to improved uncertainty and robustness. Zhang (2019a) did not provide an
explanation for these phenomena. As discussed in Appendix E.1, spatial smoothing helps
NNs learn strong representations, not regularizes NNs.
•	Spatial smoothing is, surprisingly, robust against blur corruptions.
We analyze how spatial smoothing improves predictive performance, by using loss landscape vi-
sualization, Hessian eigenvalue spectra, and Fourier analysis. These analyzes draw the following
conclusions:
•	Loss landscape visualization: Spatial smoothing stabilizes loss landscape fluc-
tuations, caused by e.g. MC dropout. This results in stabilizing NN training
and improving performance as well as generalization. See Figs. 8 and C.2.
See also code/resources/losslandscapes/resnet_mcdo_18.gif and
code/resources/losslandscapes/resnet_mcdo_smoothing_18.gif in the
supplementary material.
•	Hessian eigenvalue spectra: Spatial smoothing suppresses outliers of Hessian eigenvalues,
which disrupt NN training. See Figs. 7 and C.3.
•	Fourier analysis: Spatial smoothing effectively removes high frequency signals, including
noise due to MC dropout. We also show that CNNs are vulnerable to high frequency noise
and high frequency adversarial attacks. See Figs. 6 and D.2.
We also provide theoretical analysis of how spatial smoothing works. We prove that dropout sharpens
the loss landscape, and ensemble smoothens it. Since the spatial smoothing is a spatial ensemble, it
significantly enhances the performance of MC dropout. See Appendix D.3 for more details. Further-
more, we also show that training-phase ensemble significantly improves the predictive performance
because it smoothens the loss landscape without loss of prediction diversity. Therefore, the spatial
smoothing, which ensembles feature map points at training time, improves the performance effectively.
See Appendix D.4.
35
Under review as a conference paper at ICLR 2022
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
F.1 Prob Plays an Important Role in Spatial Smoothing
As discussed in Section 2.1, we take the perspective that each point in feature map is a prediction for
binary classification by deriving the Bernoulli distributions from the feature map by using Prob. It is
in contrast to previous works known as sampling-free BNNs (Herngndez-Lobato & Adams, 2015;
Wang et al., 2016; Wu et al., 2019) attempting to approximate the distribution of feature map with
one Gaussian distribution. We do not use any assumptions on the distribution of feature map, and
exactly represent the Bernoulli distributions and their averages. However, sampling-free BNNs are
error-prone because there is no guarantee that feature maps will follow a Gaussian distribution.
This Prob plays an important role in spatial smoothing. CNNs such as VGG, ResNet, and ResNeXt
generally use post-activation arrangement. In other words, their stages end with BatchNorm and
ReLU. Therefore, spatial smoothing layers Smooth(z) = Blur ◦ Prob(z) in CNNs cooperates with
BatchNorm and ReLU as follows:
Prob(z) = ReLU ◦ tanhτ ◦ ReLU ◦ BatchNorm (z)	(37)
= ReLU ◦ tanhτ ◦ BatchNorm (z)	(38)
since ReLU and tanhτ are commutative, and ReLU ◦ ReLU is ReLU. This Prob is trainable and is a
general form of Eq. (7). If We only use Blur as spatial smoothing, the activations BatchNorm-ReLU
play the role of Prob.
In order to analyze the roles of Prob and Blur
more precisely, we measure the predictive perfor-
mance of the model that does not use the post-
activation. Figure F.1 shows NLL of pre-activation
VGG-16 on CIFAR-100. The result shows that
Blur with Prob improves the performance, but
Blur alone does not. In fact, contrary to (Zhang,
2019a), blur degrades the predictive performance
since it results in loss of information. We also
measure the performance of VGG-19, ResNet-18,
ResNet-50, and BlurPool (Zhang, 2019a) with pre-
activation, and observe the same phenomenon. In
addition, BatchNorm-ReLU in front of GAP signif-
icantly improves the performance of pre-activation
ResNet.
As mentioned in Appendix C.2, pre-activation is
a special case of spatial smoothing. Therefore, the
performance improvement of pre-activation by spa-
tial smoothing is marginal compared to that of post-
activation.
O	Deterministic	MC- MC dropout
D Deterministic + BlUr	MC dropout + BlUr
D	Deterministic + Prob + Blur '	MC dropout + Prob + Blur
Figure F.1: Blur alone harms the predic-
tive performance, although Prob + Blur im-
proves it. We provide NLL of pre-activation
VGG-16 on CIFAR-100.
36