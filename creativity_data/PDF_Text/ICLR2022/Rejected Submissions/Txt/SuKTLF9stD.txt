Under review as a conference paper at ICLR 2022
Data-efficient Augmentation for Training
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Data augmentation is essential to achieve state-of-the-art performance in many deep
learning applications. However, modern data augmentation techniques become
computationally prohibitive for large datasets. To address this, we propose a
rigorous technique to select subsets of data points that when augmented, closely
capture the training dynamics of full data augmentation. We first show that data
augmentation, modeled as additive perturbations, speeds up learning by enlarging
the smaller singular values of the network Jacobian. Then, we propose a framework
to iteratively extract small subsets of training data that when augmented, closely
capture the alignment of the fully augmented Jacobian with label/residual vector.
We prove that stochastic gradient descent applied to augmented subsets found by
our approach have similar training dynamics to that of fully augmented data. Our
experiments demonstrate that our method outperforms state-of-the-art max-loss
strategy by 7.7% on CIFAR10 while achieving 6.3x speedup, and by 4.7% on
SVHN while achieving 2.2x speedup, using 10% and 30% subsets, respectively.
1 Introduction
Data augmentation expands the training data by applying transformations, such as rotations or crops
for images, to the original training examples. Due to its effectiveness, data augmentation is a key
component in achieving nearly all state-of-the-art results in deep learning applications (Shorten &
Khoshgoftaar, 2019). The most effective data augmentation techniques often search over a (possibly
large) space of transformations to find sequences of transformations that speeds up training the
most (Cubuk et al., 2019; 2020; Luo et al., 2020; Wu et al., 2020). In addition, multiple augmented
examples are usually generated for a single data point to obtain better results, increasing the size of
the training data by orders of magnitude. As a result, state-of-the-art data augmentation techniques
become computationally prohibitive for large real-world problems (c.f. Fig. 1).
To make data augmentation more efficient and scalable, an effective approach is to carefully select a
small subset of the training data such that augmenting only the subset have similar training dynamics
to that of full data augmentation. If such a subset can be quickly found, it would directly lead to a
significant reduction in storage and training costs, and lower costs incurred from selecting and tuning
the optimal set of transformations to apply. Despite the efficiency and scalability that it can provide,
this direction has remained largely unexplored. Existing studies are limited to fully training a network
and subsampling data points based on its loss or influence for augmentation in subsequent training
runs (Kuchnik & Smith, 2018). However, this method is prohibitive for large datasets, provides
q 5 q 5 q.5
1512.10.z5.2
<Λ-lnoq)ΦUJ,≡6uc∙≡e -£QL
(a) CIFAR10/ResNet20
015105
(SJnos4>UJ,C6ucφ匕一 £QL
training time
Accuracy qain
(b) SVHN/Resnet32
u-e6 >U23UU<
Figure 1: Speedup/Accuracy of augmenting 30% coresets compared to original max-loss policy for
(a) ResNet20 trained on CIFAR10 and (b) ResNet32 trained on SVHN.
1
Under review as a conference paper at ICLR 2022
a marginal improvement over augmenting random subsets, and does not provide any theoretical
guarantee for the performance of the network trained on the augmented subsets.
A major challenge in finding the most effective data points for augmentation is to theoretically under-
stand how data augmentation affects the optimization and generalization of neural networks. Existing
theoretical results are mainly limited to simple linear classifiers and analyze data augmentation as
enlarging the span of the training data (Wu et al., 2020), providing a regularization effect (Bishop,
1995; Dao et al., 2019; Wager et al., 2013; Wu et al., 2020), enlarging the margin of a linear classifier
(Rajput et al., 2019), or having a variance reduction effect (Chen et al., 2019). However, such tools
do not provide insights on the effect of data augmentation on training deep neural networks.
Here, we study the effect of label invariant data augmentation modeled by small additive perturbations
(Rajput et al., 2019) on training dynamics of overparameterized neural networks. In particular, we
rely on recent results that characterize training dynamics of neural networks based on the alignment
of the labels/residuals with the singular subspace of the network’s Jacobian matrix containing all its
first-order partial derivatives (Arora et al., 2019). We show that label invariant data augmentation
enlarges smaller singular values of the information space, and prove that this will speed up training.
Next, we develop a rigorous method to iteratively find small weighted subsets (coresets) that when
augmented, closely capture the alignment of the full augmented data with the label/residual, at every
point during the training. Augmenting the coresets guarantees similar training dynamics to that of
full data augmentation. Our key observation is that early in training, this alignment is best captured
by data points that are highly representative of their classes. However towards the end of training
when the network converges, data points with maximum loss best capture this alignment. Data
augmentation has been empirically shown to mainly affect the initial phase of training (Golatkar
et al., 2019), which crucially determines the final basin of convergence (Fort et al., 2020). Better
selection of points by our method during this initial phase explains the superior accuracy improvement
resulted by augmenting our coresets vs. the max-loss and the better final generalization performance.
Importantly, we show that our coresets can be provably trained on even in absence of full data, and
even when a high fraction of labels are noisy.
We demonstrate the effectiveness of our approach applied to training ResNet20, ResNet32, Wide-
ResNet on CIFAR10, CIFAR10-IB and SVHN, compared to random and max-loss baselines (Kuchnik
& Smith, 2018). We show that augmenting coresets found by our approach outperforms the state-
of-the-art even in absence of the full data. For instance, when small augmented subsets of size 30%
found by our approach are appended to CIFAR10, we attain 75% of the improvement in test accuracy
compared to augmenting the full dataset while enjoying a 3.4x speedup in training time. Even in the
absence of full data, training and augmenting tiny coresets of size 1% can achieve 74.7% accuracy on
CIFAR10/ResNet20 while providing a 39x speedup compared to using the full dataset. On CIFAR10
with 50% noisy labels, augmenting 50% of the training data outperforms full data augmentation.
2	Problem Formulation
We begin by formally describing the problem of learning from augmented data. Consider a
dataset Dtrain = (Xtrain, ytrain), Where Xtrain = (xι,…,Xn) ∈ Rd×n is the Set of n
normalized data points Xi ∈ [0,1]d, from the index set V, i.e., i ∈ V = {1, ∙∙∙ ,n}, and
ytrain = (jι,…,Jn) ∈ {y ∈ {ν1,ν2,…，u。}} With {νj}cc=i ∈ [0,1]. Assume at every step t
during training, We have a set of augmented examples Datug generated by a set of label-invariant
transformations. In particular, folloWing (Rajput et al., 2019) We model data augmentation as an
arbitrary bounded additive perturbation , With kk≤ 0 . Formally, for a given upper bound 0 and the
set of all possible transformations A, We study the transformations selected from S ⊆ A satisfying
S = {Ti ∈ A| kTi(x) - xk≤ S ∀x ∈ X train}.	(1)
Under the smoothness constraint of images, Where adjacent pixels have close intensities, such as small
translations, crops, rotations, and for other pixel-Wise augmentation methods such as sharpening,
blurring, and color distortions (Cubuk et al., 2020), 0 is small. This model is also especially suitable
for modelling state-of-the-art augmentation techniques such as structured adversarial perturbation
(Luo et al., 2020), in Which pixel intensities are changed minimally. While in our analysis We focus
on small perturbations ε0, our experiments use variety of strong and Weak augmentations. Although
other transformations such as data synthesis (Baluja & Fischer, 2017; Mirza & Osindero, 2014),
2
Under review as a conference paper at ICLR 2022
semantic augmentation (Wang et al., 2019), large translations, crops, rotations, and flips are still valid
under the additive perturbation model, they can be more effectively modelled using matrices of linear
transforms (Wu et al., 2020), as we analyze in Appendix B.3.
In practice, multiple augmentations are generated for each example xi , and each augmented data
point can be a combination of multiple transformations, e.g. random cropping and rotating followed
by horizontal flipping. The set of augmentations at iteration t generating r augmented examples per
data point can be specified, with abuse of notation, as Dat ug = {Sir=1 (Tit (Xtrain), ytrain)}, where
|Dat ug |= rn and Tit(Xtrain) transforms all the training data points with the set of transformations
i	.	aug = {Sir=1Tit(X
train)} and yat ug = {Si=1 ytrain }.
Let f(W, x) be an arbitrary neural network with m vectorized (trainable) parameters W ∈ Rm. We
assume that the network is trained using (stochastic) gradient descent with learning rate η to minimize
the squared loss L over the original and augmented training examples Dt = {Dtrain ∪ Datug} with
associated index set V t , at every iteration t:
L(Wt, X):=2 X Li(Wt, Xi) := 2 X	kf(Wt, Xi)-yik2.	(2)
i∈Vt	(xi,yi)∈Dt
The gradient update at iteration t is given by
Wt+1 = Wt - ηVL(Wt, X),	VL(Wt, X) = JT(Wt, X)(f (Wt, X) - y),	(3)
where Xt = {Xtrain ∪ Xatug} and yt = {ytrain ∪ yatug} are the set of original and augmented
examples and their labels, J(W, X) ∈ Rn×m is the Jacobian matrix associated with f, and
rt = f(Wt, X) - y is the residual. We further assume that J is smooth with Lipschitz constant L:
kJ(W,xi) - J(W,xj)k≤ Lk xi - xjk	∀xi,xj ∈ X.	(4)
This trivially holds for linear models, and when W is bounded, it holds for deep ReLU, and generally
for networks with any activation φ with bounded derivatives φ0 and φ00 (Jordan & Dimakis, 2020).
Under this assumption, augmentation as defined in Eq. (1) results in bounded perturbations to the
Jacobian matrix. I.e., for any transformation Tj ∈ S, we have kJ(W, xi) - J(W, Tj (xi))k≤ L0.
Using the shorthand notations J = J(W, Xtrain) and J = J(W, Tj (Xtrain)), we obtain
J = J + E, where E is the perturbation matrix With ∣∣E∣∣2≤ IIEIIF≤ √nLe0.
3 Data Augmentation Speeds Up Learning
In this section, we analyze the effect of data augmentation on training dynamics of neural networks,
and show that data augmentation can provably speed up learning. To do so, we leverage the recent
results that characterize the training dynamics based on properties of neural network Jacobian
matrix and the corresponding Neural Tangent Kernel (NTK) (Jacot et al., 2018) defined as Θ =
J(W, X)J(W, X)T. Formally (Arora et al., 2019):
rt = X(1 - ηλi)(uiuiT)rt-1 = X(1 - ηλi)t(uiuiT)r0,	(5)
i=1	i=1
where Θ = UΛUT = Pi=1 λiuiuiT is the eigendecomposition of the NTK. Although the constant
NTK assumption holds only in the infinite width limit, Lee et al. (2019) found close empirical
agreement between the NTK dynamics and the true dynamics for wide but practical networks, such
as wide ResNet architectures (Zagoruyko & Komodakis, 2016). Eq. (5) shows that training dynamics
depend on the alignment of the NTK with the residual vector at every iteration t. In particular,
shrinkage of residuals along the directions associated with larger eigenvalues of the NTK is fast
and happens early during the training, while learning along the space associated with the small
eigenvalues is slow and happens later. In the following, we prove that for small perturbations 0,
data augmentation speeds up training by enlarging smaller eigenvalues of the NTK, while decreasing
larger eigenvalues with a high probability. Intuitively, this can be characterized as decreasing the
learning rate for dimensions with larger gradient and slightly increasing the learning in dimensions
with smaller gradients, and having a regularization effect by slightly perturbing the eigenvectors.
3
Under review as a conference paper at ICLR 2022
(a) MNIST/MLP Epoch 15 (b) C10/Res20 Epoch 15 (c) SVHN/Res32 Epoch 15 (d) MNIST/MLP Epoch 0
Figure 2: Histogram of the singular values of the Jacobian matrix for augmented vs. original (a)
MNIST/1 hidden-layer MLP, (b) CIFAR10/ResNet20, and (c) SVHN/ResNet32. (d) Effect of strong
vs. weak augmentation on MNIST/MLP at initialization. We sampled 2 and 4 transformations from
rotation, translation, contrast, brightness, etc. for normal vs. strong augmentation.
3.1 Effect of Data Augmentation on the Eigenvalues of the NTK
We first investigate the effect of data augmentation on the singular values of the Jacobian, and use
this result to bound the change in the eigenvalues of the NTK. To characterise the effect of data
augmentation on singular values of the perturbed Jacobian J , we rely on Weyl’s theorem (Weyl,
1912) stating that under bounded perturbations E, no singular value can move more than the norm of
the perturbations. Formally, ∣σi - σi ∣≤ ∣∣E ∣∣2, where σi and σi are the singular values of the perturbed
and original Jacobian respectively. Crucially, data augmentation affects larger and smaller singular
values differently. Let P be orthogonal projection onto the column space ofJT, and P⊥ = I - P be
the projection onto its orthogonal complement subspace. Then, the singular values of the perturbed
Jacobian JT are σ2 = S + μi)2 + Z2, where ∣μi∣≤ ∣∣PE∣∣2, and σmin(P⊥E) ≤ Zi ≤ ∣∣P⊥E∣2,
σmin the smallest singular value of JT (Stewart, 1979). Since the eigenvalues of the projection
matrix P are either 0 or 1, as the number of dimensions m grows, for bounded perturbations we
get that on average μ2 = O⑴ and Zf = O(m). Thus, the second term dominates and increase of
small singular values under perturbation is proportional to √m. However, for larger singular values,
first term dominates and hence σi — σi = μi. Thus in general, small singular values can become
significantly and proportionally larger, while larger singular values remain relatively unchanged.
Empirically, we observed that largest singular values often decrease by data augmentation.
Fig. 2 shows the effect of data augmentation on singular values of the Jacobian matrix for a 1 hidden
layer MLP trained on MNIST, ResNet32 trained on SVHN, and ResNet20 trained on CIFAR10. As
calculating the entire Jacobian spectrum is computationally prohibitive, data is subsampled from 3
classes. For all datasets, we used the data augmentation techniques described in the Experiments
section, such as rotation, translation, contrast, brightness. It can be observed that for less diverse
datasets such as MNIST that have smaller singular values in general, data augmentation can easily
enlarge the singular values. On the other hand, for more diverse datasets such as CIFAR10 that
generally have larger singular values, augmentation cannot change the singular values considerably.
In both cases largest singular values became smaller.
The following Lemma characterizes the expected change to the eignvalues of the NTK.
Lemma 3.1 Data augmentation as additive perturbations bounded by small 0 results in the follow-
ing expected change to the eigenvalues of the NTK:
Eni] = E[σf] = σi + σi(1 - 2p,)∣∣E∣∣ + ∣IE∣∣2/3	⑹
where Pi := P(σi — σi < 0) is the probability that σi decreases as a result of data augmentation.
All the proofs can be found in the Appendix.
From Lemma 3.1 and Eq. (5) we see that augmentation can speed up learning at every epoch by
σi(1 — 2pi)∣E∣∣ + ∣∣E∣∣2/3 along dimensions with smaller singular values for whichPi ≤ 0.5, and
by ∣∣E∣∣2/3 along dimensions with for which singular values do not change (pi ≈ 0.5). It can
also speed up learning along dimensions for which singular values decrease (Pi > 0.5) as long as
∣E∣≥ 3σi(2pi - 1). This is mainly relevant for smaller singular values under strong augmentation.
3.2	Effect of Data Augmentation on the Eigenvectors of the NTK
Next, we focus on characterizing the effect of data augmentation on the eigenspace of the NTK.
Let the singular subspace decomposition of the Jacobian be J = U ΣV T. Then for the NTK, we
4
Under review as a conference paper at ICLR 2022
have Θ = JJT = UΣVTVΣUT = UΣ2UT (since VTV = I). Hence, the perturbation of the
eigenspace of the NTK is the same as perturbation of the left singular subspace of the Jacobian J .
Suppose σi are singular values of the Jacobian. Let the perturbed Jacobian be J = J + E, and
denote the eigengap γo = min{σi -。2+1 : i = 1, ∙∙∙ ,r} where。丁+1 := 0. Assuming γo ≥ 2kEk2,
a combination of Wedin’s theorem (Wedin, 1972) and Mirsky’s inequality (Mirsky, 1960) (the
counterpart of Weyl’s inequality (Weyl, 1912) for singular values) implies that
kui- Uik≤ 2√2kEk∕γo	⑺
This result provides an upper-bound on the change of every left singular vectors of the Jacobian.
However as we discuss below, data augmentation affects larger and smaller singular directions
differently. To see the effect of data augmentation on every singular vectors of the Jacobian, let
the subspace decomposition of Jacobian be J = UΣV T = Us ΣsVsT + Un Σn VnT , where Us
associated with nonzero singular values, spans the column space of J , which is also called the
signal subspace, and Un , associated with zero singular values (Σn = 0), spans the orthogonal space
of Us, which is also called the noise subspace. Similarly, let the subspace decomposition of the
perturbed JacObian be J= UΣVT = US ∑s VT + Un∑nVT, and US = US + ∆Us, where
∆Us is the perturbation of the singular vectors that span the signal subspace. Then the following
general first-order expression for the perturbation of the orthogonal subspace due to perturbations of
the Jacobian characterize the change of the singular directions: ∆US = UnUnTEVSΣS-1 (Li et al.,
1993). We see that while singular vectors associated to larger singular values are more robust to data
augmentation, singular vectors associated with small singular values are more affected. We also note
that singular vectors are more robust to perturbations than singular values.
3.3	Augmentation Improves Training and Generalization
As discussed, data augmentation increases the smaller eigenvalues of the NTK with a high probability,
and is likely to slightly decrease the largest eigenvalues. Besides, eigenvectors (in particular those as-
sociated with larger eigenvalues) are generally more robust than eigenvalues. The following Theorem
characterizes the expected improvement in the training dynamics resulted by data augmentation.
Theorem 3.2 Gradient descent with learning rate η applied to a neural network with constant NTK
and Lipschitz constant L, and data points Daug augmented with r additive perturbations bounded by
0 as defined in Eq. (1) results in the following training dynamics:
E[ky - f(Xaug, Wt)k2] ≤
∖
n	2t
nX(1 - η (rσ2 + √Tσi(1 - 2pi)kEk + kE∣∣2∕3) )	(8)
i=1
where E with ∣∣E∣∣≤ √nLeo is the perturbation to the Jacobian, and Pi := P ((Ji — σi < 0) is the
probability that σi decreases as a result of data augmentation.
Corollary 3.3 Under the same assumptions as in Theorem 3.2 let σmin be the minimum singular
value of Jacobian J associated with training data Xtrain, then probability 1 - δ generalization error
of the network trained with gradient descent on augmented data Xaug enjoys the following bound:
/ 2
V (σmin + √nLeo)2
+O
(9)
4 Most Effective Subsets for Data Augmentation
In Sec. 3 we discussed the effect of data augmentation on changing the alignment of the NTK with
the residual, and how it improves training. Here, we focus on identifying the most effective subsets
for data augmentation. Our key idea is to find subsets of data points that when augmented, closely
capture the alignment of the NTK (or equivalently the Jacobian) corresponding to the full augmented
data with the residual vector, J(Wt, Xatug)Tratug. If such subsets can be found, augmenting only
the subsets will change the NTK and its alignment with the residual in a similar way as that of
full data augmentation, and will result in similar improved training dynamics. Effectively, such
augmented subsets can be trained on along with the full non-augmented data, or without it to further
5
Under review as a conference paper at ICLR 2022
Algorithm 1 CORESETS FOR EFFICIENT DATA AUGMENTATION
Input: The dataset D = {(xi, yi)}in=1, number of iterations T.
Output: Output model parameters W T .
1: for t = 1, ∙∙∙ ,T do
2：	St = 0, Xaug = O.
3:	for C ∈ {1,…，C} do
4:	Sct = greedy(Vc)	. Extract a coreset from class c by solving Eq. (11)
5:	γj = Pi∈VcI[j=argminj0∈SkJT(Wt, xi)ri-JT(Wt, xj0)rj0 k] . Coreset weights
6:	Xatug = {Xaug ∪ {∪ir=1Tit(XSct)}}	. Augment the coreset
7:	ρtj = γjt /r
8:	Update the parameters Wt using weighted gradient descent on Xatug or {Xtrain ∪ Xatug}.
speed up training. However, generating the full set of transformations Xatug is often very expensive.
This is exacerbated in the case of strong augmentations. Hence, generating the transformations, and
then extracting the subsets may not provide a considerable overall speedup.
In the following, we show that weighted subsets (coresets) S that closely estimate the alignment of the
Jacobian associated to the original data with the residual vector JT (Wt, Xtrain)rtrain can closely
estimate the alignment of the Jacobian of the full augmented data and the corresponding residual
JT (Wt, Xatug)ratug. Thus, the most effective subsets for augmentation can be directly found from
the training data. Formally, subsets St weighted by YS that capture the alignment of the full Jacobian
with residual by and error of at most ξ can be found by solving the following optimization problem:
St = argmin∣S | s.t.	∣∣J T (Wt, X t)rt - diag(γS )J T (Wt, XS )rS ∣∣≤ ξ. (10)
S⊆V
Solving the above optimization problem is NP-hard. However, as we discuss in the Appendix A.6, a
near optimal subset can be found by minimizing the Frobenius norm of a matrix GS, in which the ith
row contains the euclidean distance between data point i and its closest element in the subset S, in
the gradient space. Formally, [GS]i. = minj0∈S ∣J T (W t, xi)ri - JT(Wt, xj0 )rj0 ∣. Intuitively,
such subsets contains the set of medoids of the dataset in the gradient space, where the medoids of
a dataset are defined as the most centrally located elements in the dataset (Kaufman et al., 1987).
The weight of every element j ∈ S is the number of data points closest to it in the gradient space,
i.e., γj = Pi∈V I[j = argminj0∈S∣JT(Wt, xi)ri - JT(Wt, xj0)rj0 ∣]. The set of medoids can
be found by solving the following submodular1 cover problem:
Stt = argmins⊆v∣S|	s.t. C -IlGSIIF≥ C - ξ,	(11)
where C ≥ ∣GS ∣F is a constant. The classical greedy algorithm provides a logarithmic approxima-
tion for the above Submodular maximization problem, i.e., ∣S∣≤ (1 + Ln(n)). It starts with the empty
set S0 = 0, and at each iteration τ, it selects the training example e ∈ V that maximizes the marginal
gain F(e∣Sτ) = F(ST ∪ {e}) 一 F(ST). Formally, ST = Sτ-1 ∪ {argmaXe∈vF(e∣Sτ-ι)}. The
O(nk) computational complexity of the greedy algorithm can be reduced to O(n) using randomized
methods (Mirzasoleiman et al., 2015), and further improved using lazy evaluation (Minoux, 1978) and
distributed implementations (Mirzasoleiman et al., 2013). The rows of the matrix G can be efficiently
upper-bounded using the gradient of the loss w.r.t. the input to the last layer of the network, which
has been shown to capture the variation of the gradient norms closely (Katharopoulos & Fleuret,
2018). The above upper-bound is only marginally more expensive than calculating the value of the
loss, and hence the subset can be found efficiently. Better approximations can also be obtained by
considering earlier layers in addition to the last two, at the expense of greater computational cost.
At every iteration t during training, we select a coreset from every class c ∈ [C] separately, and apply
the set of transformations {Tit}ir=1 only to the elements of the coresets, i.e., Xatug = {∪ir=1Tit(XSt)}.
We divide the weight of every element j in the coreset equally among its transformations, i.e. the
final weight ρtj = γjt /r if j ∈ St . We apply the gradient descent updates in Eq. (3) to the weighted
Jacobian matrix of Xt = Xatug or Xt = {Xtrain ∪ Xatug} (viewing ρt as ρt ∈ Rn) as follows:
Wt+1 = Wt 一 η (diag(Pt) J(Wt, Xt))T rt.	(12)
1Asetfunction F : 2v → R+ is submodularif F(S∪{e})- F(S) ≥ F(T∪{e})-F(T), for any S ⊆ T ⊆ V
and e ∈ V \ T. F is monotone if F(e|S) ≥ 0 for any e∈ V\S and S ⊆ V.
6
Under review as a conference paper at ICLR 2022
(a) Loss intersection
(b) Improvement
Figure 3: Training ResNet20 on full data and augmented coresets extracted from CIFAR10. (a)
Intersection between elements of coresets of size 30% and maximum loss subsets of the same size.
The intersection increases after the initial phase of training, (b) Accuracy improvement for training
on full data and augmented coresets over training on full data and augmented max-loss points. (c)
Accuracy vs. fraction of data selected for augmentation during training Resnet20 on CIFAR10.
(c) Coresets vs random
The pseudo code is given in Alg. 1. The following Lemma upper-bounds the difference between the
alignment of the Jacobian and residual for augmented coreset vs. full augmented data.
Lemma 4.1 Let S be a coreset that captures the alignment of the full data NTK with residual
with an error of at most ξ as in Eq. (4). Augmenting the coreset with perturbations bounded by
€o ≤ 4一 captures the alignment ofthefully augmented data with the residual by an error ofat most
n 2 Ll i
kJ T (W t, Xaug )r - diag (Pt)J t(Wt, XSaug )rs k≤ ξ + O (√L) .	(13)
Importantly, since the augmented coresets capture the alignment of the Jacobian of a fully augmented
data with residual, they can be trained on in absence of the full data to gain more speedup. This is not
the case for existing methods such as max-loss. Besides, the gradient of noisy labeled data points
do not cluster in the gradient space (Mirzasoleiman et al., 2020b). Thus, only clean data points are
selected for augmentation. This results in an improved performance, as we will show in Sec. 5.1.
4.1	Coreset vs. Max-loss S trategy for Data Augmentation
In the initial phase of training the NTK goes through rapid changes, which determines the final basin
of convergence and network’s final performance (Fort et al., 2020). Regularizing deep networks
by weight decay or data augmentation mainly affects this initial phase and matters little afterwards
(Golatkar et al., 2019). Crucially, augmenting coresets that closely capture the alignment of the NTK
with the residual during this initial phase results in a considerable increase in the speed of convergence
and generalization performance. On the other hand, augmenting points with maximum loss early
in training decreases the alignment between the NTK and the label vector and slows down learning
and convergence. Fig. 3b illustrates the superior performance of coreset vs max-loss augmentation.
After this initial phase (highlighted in green in Fig. 3a) when the network have a good prediction
performance, the gradients for the majority of data point becomes very small. Here, the alignment is
mainly captured by the elements with the maximum loss. Hence, as training proceeds, the intersection
between the elements of the coresets and examples with maximum loss increases (c.f. Fig. 3a).
The following Theorem characterizes the training dynamics of training on the full data and the
augmented coresets, using the additive perturbation model in Eq. (1).
Theorem 4.2 Let Li be β-smooth, L be λ-smooth and satisfy the α-PL condition, that is for α > 0,
∣∣VL(W )k2 ≥ αL( W) for all weights W. Let f be Lipschitz in X with constant L0 ,and L =
max{L, L0}. Let G0 be the gradient at initializaion, σmax the maximum singular value of the coreset
Jacobian at initialization. Choosing e0 ≤ --√j^ and running SGD onfull data with augmented
coreset using constant step size η =热,we get thefollowing convergence bound:
E[kVLf+caug(Wt)k] ≤ √α(1 - an)t(2G0 + ξ + O (σ√l)).
(14)
7
Under review as a conference paper at ICLR 2022
Table 1: Accuracy improvement by augmenting subsets found by our method vs. max-loss and
random, over improvement of full data augmentation (F.A.) compared to no augmentation (N.A.). The
table shows the results for training on CIFAR10 with ResNet20 (C10/R20), SVHN with ResNet32
(SVHN/R32), and CIFAR10-Imbalanced with ResNet32 (C10-IB/R32), with R = 20.
Dataset	N.A.	F.A.	Random			Max-loss			Ours		
	Acc	Acc	5%	10%	30%	5%	10%	30%	5%	10%	30%
C10 / R20	89.46	93.50	21.8%	39.9%	65.6%	32.9%	47.8%	73.5%	34.9%	51.5%	75.0%
C10-IB / R32	87.08	92.48	25.9%	45.2%	74.6%	31.3%	39.6%	74.6%	37.4%	49.4%	74.8%
SVHN / R32	95.68	97.07	5.8%	36.7%	64.1%	35.3%	49.7%	76.4%	31.7%	48.3%	80.0%
The above Theorem shows that training on full data and augmented coresets converges with the same
rate as of training on the fully augmented data, to a close neighborhood of the optimal solution. The
size of the neighborhood depends on (1) the error of the coreset ξ in Eq. (4), and (2) the error in
capturing the alignment of the full augmented data with the residual derived in Lemma 4.1. The first
term decrease as the size of the coreset grows, and the second term depends on the network structure.
We also analyze convergence of training only on the augmented coresets, and augmentations modelled
as arbitrary linear transformations using a linear model (Wu et al., 2020) in Appendix B.1, B.3.
5 Experiments
Setup and baselines. We evaluate the performance of our approach in two different settings. First,
we consider the case where full data is small enough to be trained on. Here, we study the effect of
augmenting coresets on the generalization performance of clean and noisy labeled data. Next, we
focus on the case where the full training data is too large to be trained on. In this setting, we train only
on the augmented coresets. In both cases we use subsets with maximum loss, and random subsets as
baselines. For all methods, we select a new augmentation subset every R epochs. We note that while
the original method of (Kuchnik & Smith, 2018) selects points based on loss of a fully trained model,
to maximize fairness, our max-loss baseline selects a new subset at every subset selection step.
Data and augmentation. We apply our method to training ResNet20 and Wide-Resnet-28-10 on
CIFAR10, and ResNet32 on CIFAR10-IMB (Long-Tailed CIFAR10 with Imbalance factor of 100
following (Kim & Kim, 2020)) and SVHN datasets. All models are trained for 200 epochs using
SGD with 0.9 momentum and learning rate warm-up. We use the strong augmentation proposed by
(Wu et al., 2020) to generate 4 distinct augmented examples by randomly sampling 2 augmentations
from the same set used by (Cubuk et al., 2019; 2020) to apply to each example. A new set of default
augmentations (random crop and horizontal flip) are also applied every R epochs to the original
data. We individually measure average time taken for subset selection, gradient descent, and subset
augmentation per epoch to compute total training time and speedups. All results are averaged over 5
runs using an Nvidia A40 GPU, and full results are reported in the Appendix.
5.1	Training on Full Data and Augmented Subsets
Table 1 demonstrates the accuracy improvement resulted by augmenting subsets of size 5%, 10%
and 30% selected by our method vs. max-loss and random over full data augmentation. We observe
that augmenting coresets effectively improves generalization, and outperforms augmenting random
201510
-Iot5ej dnp<uads
(a) CIFAR10/ResNet20 (b) CIFAR10/ResNet20 (c) SVHN/ResNet32 (d) SVHN/ResNet32
Figure 4: Accuracy improvement and speedups by augmenting subsets found by our method vs. max-
loss and random, over improvement of full data augmentation (F.A.) compared to no augmentation
(N.A.). The Figure shows the results for CIFAR10 with ResNet20 in terms of (a) speedup and (b)
accuracy, and results for SVHN with ResNet32 in terms of (c) speedup and (d) acuracy.
8
Under review as a conference paper at ICLR 2022
Table 2: Training ResNet20 on CIFAR10 with 50% label noise, with R = 20. Accuracy without
augmentation is 70.72 ± 0.20 and the accuracy of full data augmentation is 75.87 ± 0.77. Note that
augmenting 50% subsets outperforms augmenting the entire data (marked as **).
Subset	Random	Loss	Ours
10%	72.32 ± 0.14	71.83 ± 0.13	73.02 ± 1.06
30%	74.46 ± 0.27	72.45 ± 0.48	74.67 ± 0.15
50%	75.36 ± 0.05	73.23 ± 0.72	76.20 ± 0.75**
Table 3: Training ResNet20 and WideResnet-28-10 on CIFAR10 using small subsets, with R = 1.
Training and augmenting subsets selected by max-loss performed poorly and did not converge.
Model	Subset	Random			Ours			Loss
		No Aug.	Aug.	Improv.	No Aug.	Aug.	Improv.	Aug.
	0.1%	31.7 ± 3.2	33.5 ± 2.7	5.7%	29.6 ± 3.8	37.8 ± 4.5	27.7%	< 15%
	0.2%	35.9 ± 2.1	42.7 ± 3.9	18.9%	33.6 ± 3.2	45.1 ± 2.3	34.2%	< 15%
ResNet20	0.5%	51.1 ± 2.3	58.7 ± 1.3	14.9%	55.8 ± 3.1	63.9 ± 2.1	14.5%	< 15%
	1%	66.2 ± 1.0	74.4 ± 0.8	12.4%	65.9 ± 4.0	74.7 ± 1.1	13.4%	< 15%
WRN-28-10	1%	61.3 ± 2.4	57.7 ± 0.8	5.9%	59.9 ± 2.4	62.1 ± 3.1	3.7%	< 15%
and max-loss subsets across different models and datasets. Fig. 5 shows the speedup vs. test
accuracy of augmenting subsets of different size selected from CIFAR10 and SVHN by our method
vs baselines. We see that our method outperforms the baseline while providing a similar speedup as
that of max-loss. On SVHN, we achieve 80.0% of the augmentation accuracy gain by augmenting
only 30% size subsets while providing 2.2x speedup, while max-loss only achieves 76.4% with the
same speedup. In the Appendix, we also show that our approach is effective when trained with full
data even for small augmentation subset sizes of 0.2% and 0.5% selected every epoch.
Augmenting noisy labeled data. Table 2 shows the result of augmenting coresets vs. max-loss and
random subsets of different sizes selected from CIFAR10 with 50% label noise for training ResNet20.
Note that augmenting coresets selected by our method not only outperforms max-loss and random,
but provides a superior performance over full data augmentation. This clearly shows the effectiveness
of the coresets in capturing the alignment of the NTK with the residual of clean data points.
5.2 Training only on augmented subsets in absence of full data
We now evaluate the effectiveness of our approach for training only on the augmented coresets.
Our main goal here is to show the accuracy improvement resulted from augmenting the coresets. In
particular instead of focusing on the quality of the coresets, we aim at showing the effectiveness of the
coresets for augmentation. Table 3 shows the test accuracy for training ResNet20 and Wide-ResNet
on CIFAR10 when we only train on small augmented coresets of size 0.1% to 1% selected at every
epoch (R = 1). We see that while the non-augmented coreset performs worse than random in most
cases, the augmented coresets outperform augmented random subsets by a large margin. That is,
the improvement from augmenting the coresets is significantly larger than random data points. This
clearly shows the effectiveness of augmenting the coresets, and the importance of capturing the
alignment of the NTK with the residual for data augmentation. Note that training on the augmented
max-loss points did not even converge in absence of full data. Finally, Fig. 3c shows the fraction of
data selected for augmentation during training of ResNet20 on CIFAR10. We see that the coresets
achieve a superior performance by carefully selecting certain data points more often than others.
6	Conclusion
We showed that data augmentation improves training and generalization by enlarging the smaller
singular values of the neural network Jacobian. Then, we proposed a framework to iteratively
extract small subsets of training data that when augmented, closely capture the alignment of the fully
augmented Jacobian with the label/residual vector. We showed the effectiveness of augmenting the
coresets selected by our method to provide a superior generalization performance when added to the
full data, in presence of noisy labels, or as a standalone subset.
9
Under review as a conference paper at ICLR 2022
7	Reproducibility Statement
Our experimental setup along with training details are introduced in Section 5. Full results containing
standard deviations can be found in Appendix C. All code use for our empirical results are also
available in the supplementary material, with along with instructions on how to run them.
All proofs for non-trivial theoretical results in the main paper can be found in Appendix A, while
additional theoretical results and their respective proofs can be found in Appendix B.
References
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
Shumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adver-
sarial examples. arXiv preprint arXiv:1703.09387, 2017.
Raef Bassily, Mikhail Belkin, and Siyuan Ma. On exponential convergence of sgd in non-convex
over-parametrized learning. arXiv preprint arXiv:1811.02564, 2018.
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation, 7
(1):108-116, 1995.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. Invariance reduces variance: Understanding data
augmentation in deep learning and beyond. arXiv preprint arXiv:1907.10905, 2019.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113-123, 2019.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020.
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher R2 A kernel
theory of modern data augmentation. In International Conference on Machine Learning, pp.
1528-1537. PMLR, 2019.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. Advances in Neural Information
Processing Systems, 33, 2020.
Aditya Sharad Golatkar, Alessandro Achille, and Stefano Soatto. Time matters in regularizing deep
networks: Weight decay and data augmentation affect early learning dynamics, matter little near
convergence. Advances in Neural Information Processing Systems, 32:10678-10688, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Matt Jordan and Alexandros G Dimakis. Exactly computing the local lipschitz constant of relu
networks. arXiv preprint arXiv:2003.01219, 2020.
Angelos Katharopoulos and FrangoiS Fleuret. Not all samples are created equal: Deep learning with
importance sampling. In International conference on machine learning, pp. 2525-2534. PMLR,
2018.
L Kaufman, PJ Rousseeuw, and Y Dodge. Clustering by means of medoids in statistical data analysis
based on the, 1987.
Byungju Kim and Junmo Kim. Adjusting decision boundary for class imbalanced learning. IEEE
Access, 8:81674-81685, 2020.
10
Under review as a conference paper at ICLR 2022
Michael Kuchnik and Virginia Smith. Efficient augmentation via data subsampling. In International
Conference on Learning Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In NeurIPS, 2019.
Fu Li, Hui Liu, and Richard J Vaccaro. Performance analysis for doa estimation algorithms: unifica-
tion, simplification, and observations. IEEE Transactions on Aerospace and Electronic Systems,
29(4):1170-1184,1993.
Calvin Luo, Hossein Mobahi, and Samy Bengio. Data augmentation via structured adversarial
perturbations. arXiv preprint arXiv:2011.03010, 2020.
Michel Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In
Optimization techniques, pp. 234-243. Springer, 1978.
Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The quarterly journal of
mathematics, 11(1):50-59, 1960.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed submodular
maximization: Identifying representative elements in massive data. In Advances in Neural
Information Processing Systems, pp. 2049-2057, 2013.
Baharan Mirzasoleiman, AshWinkUmar Badanidiyuru, Amin Karbasi, Jan Vondrdk, and Andreas
Krause. Lazier than lazy greedy. In Twenty-Ninth AAAI Conference on Artificial Intelligence,
2015.
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of
machine learning models. In International Conference on Machine Learning, pp. 6950-6960.
PMLR, 2020a.
Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec. Coresets for robust training of deep neural
netWorks against noisy labels. Advances in Neural Information Processing Systems, 33, 2020b.
Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does
data augmentation lead to positive margin? In International Conference on Machine Learning, pp.
5321-5330. PMLR, 2019.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):1-48, 2019.
GW SteWart. A note on the perturbation of singular values. Linear Algebra and Its Applications, 28:
213-216, 1979.
Stefan Wager, Sida Wang, and Percy Liang. Dropout training as adaptive regularization. arXiv
preprint arXiv:1307.1493, 2013.
Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Gao Huang, and Cheng Wu. Implicit semantic
data augmentation for deep netWorks. Advances in Neural Information Processing Systems, 32:
12635-12644, 2019.
Per-Ake Wedin. Perturbation bounds in connection with singular value decomposition. BITNumerical
Mathematics, 12(1):99-111, 1972.
Hermann Weyl. The asymptotic distribution law of the eigenvalues of linear partial differential
equations (with an application to the theory of cavity radiation). mathematical annals, 71(4):
441-479, 1912.
Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher R6. On the generalization effects of
linear transformations in data augmentation. In International Conference on Machine Learning,
pp. 10410-10420. PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference 2016. British Machine Vision Association, 2016.
12
Under review as a conference paper at ICLR 2022
S upplementary Material:
Data-Efficient Augmentation for Training Neural
Networks
A Proof of Main Results
A.1 Proof for Lemma 3.1
Proof Let δi := σi - σi, where P(δi < 0) = pi. Assuming uniform probability between -∣∣Ek to 0, and between 0 to kEk, we have pdf ρi(x) for δi:		
	f 雀J, if - kEk≤ x < 0	
Pi(X) = 1k 1Ei,	0 ≤ X ≤ kEk		(15)
	(0,	otherwise	
Taking expectation,	∞	
E(σi - σi)=	E(δi)=Z Xρi(X)dX	(16)
	-∞ 0	pi	kEk	1 - pi =J X 画dx + J0	X IErdx -kEk	(17)
	—kEkPi	(1 - Pi)kE k =—-厂 + ―2—	(18)
	= (1- 2Pi)kΕk 2	(19)
We also have	∞	
E(δi2) =	Z X2ρi(X)dX	(20)
=	-∞ I χ2 2 dχ + [旧]1-竺 dχ χ X 画dx + J0	X IErdx -kEk	(21)
=	kEk2Pi , (1 -Pi)kEk2 3	+	3	(22)
=	kEk2 3	(23)
Thus, we have		
,~ . E(λi)	=Ε(σ2)	(24)
	= E((σi , δi)2)	(25)
	= E(σi2 , 2σiδi , δi2)	(26)
	= σi2 , 2σi E[δi] , E[δi2]	(27)
	2	(1 - 2pi)kEk	kEk2 =σi+2σi	2	, 丁	(28)
	=σi + σi(1 - 2pi)kEk + ⅛	(29)
13
Under review as a conference paper at ICLR 2022
A.2 Proof of Theorem 3.2
Using Jensen’s inequality, we have
E [ky - f(Xaug, Wt)k2] = E λ XX(1 - ηλi)2t(uTy)2 ±e
i=1
t e[ ∑(1 - ηλi)2t(uT y)2
n
X E [(1 - ηλi)2t(n)]
i=1
n XX (1 - ηE [引产
i=1
n XX (1 - ηE [引产
i=1
k + σi(1-2Pi)kEk + 呼)]
(30)
(31)
(32)
(33)
(34)
(35)
A.3 Proof of Corollary 3.3
Under the assumptions of Theorem 5.1 of Arora et al. (2019), i.e. where the minimum eigenvalue
of the NTK is λmin(JJT) ≥ λ0 for a constant λ0 > 0, and training data X of size n sampled
i.i.d. from distribution D and I-LiPschitz loss L, We have that with probability δ∕3, training the
over-parameterized neural network with gradient descent for t ≥ Ω (比 log 蓑)iterations results in
the following population loss LD (generalization error)
LD (Wt, X) ≤ JyT (JJ T)—y + O (logF ),
(36)
with high probability of at least 1 - δ over random initialization and training samples.
Hence, using λmin , σmin to denote minimum eigen and singular value respectively of the NTK
corresponding to full data, we get
LD
train
(Wt,Xt
(37)
(38)
≤
≤
≤
∖
∖
∖
For augmented dataset Xaug, we have * ≤ σi + √nLeo, hence the improvement in the generalization
error is at most
LDaug(Wt' Xaug) ≤ y (σmin + √nLe0p + O (log j) .	(39)
Combining these two results, we obtain Corollary 3.3.
14
Under review as a conference paper at ICLR 2022
A.4 Proof of Lemma 4.1
Proof
kJT(Wt,Xaug)r-diag(ρt)Jt(Wt,XSaug)rSk	(40)
= k(JT(Wt,X)+E)r-(diag(ρt)Jt(Wt,XS)+ES)rSk	(41)
≤ k(JT(Wt,X)r-diag(ρt)Jt(Wt,XS)rS)+Er-ESrSk	(42)
≤ k(J T (W t, X)r - diag(ρt)J t(W t, XS)rS)k+kErk+kESrSk (43)
Applying definition of coresets, we obtain
k(J T (W t, X)r-diag(ρt)J t(W t, XS)rS)k+kErk+kESrSk	(44)
≤ ξ + kErk+kES rS k	(45)
3
≤ ξ + 2n2 LCo	(46)
≤ ξ + 2√L	(47)
A.5 Proof of Theorem 4.2
Proof In this proof, as shorthand notation, we useXf and Xtrain interchangeably. We further use
Xc to represent the coreset selected from the full data, and Xcaug to represent the augmented coreset.
By Theorem 1 of Bassilyet al. (2018), under the α-PL assumption for L and interpolation assump-
tion (i.e. for every sequence W1, W2, . . . such that limt→∞ L(Wt, X) = 0, we have that the loss
for each data point limt→∞ L(Wt, xi) = 0), the convergence of SGD with constant step size is
given by
E[∣∣VL(Wt, Xf+caug)k2] ≤(1 - αη)tL(W0, Xf+caug)	(48)
≤ 1(1- αη)t kVL(W0, Xf+caug)k2	(49)
15
Under review as a conference paper at ICLR 2022
Using Jensen’s inequality, we have
E[kVL(W0, Xf+caug)k] ≤ ,E[kVL(Wt,Xf+caug)k2] ≤√1α(1 - α2η)t kVL(W0,Xf+caug)k ≤√1α(1 - α2η)t (kVL(W0,Xf)k+kVL(W0,Xcaug)k) ≤√1α(1 - α2η)t (G0 + k(J(W0,Xc) + E)(y - f(W0,Xc + e))k) ≤√1α (1-α2η) 2 (G0 + k(J(W0, Xc) + E)τ(y - f(W0, Xc)- Vxf(W0, Xc)τe - O(eτe)k)	(50) (51) (52) (53) (54) (55) (56)
√1α(1 - α2η) t (Go + ∣∣VL(W0, Xc)-(J(W0, Xc)T(Vxf(W0, Xc)Te + O(eτe)) +
(57)
E(y-f(W0,Xc+))k)	(58)
≤ √1α(1 - α2η) t (G0 + kVL(W 0, Xc)-(J (W 0, Xc)τ (Vxf(W 0, Xc )τ e + O(eτ e))k +
(59)
(60)
√2kEk)
(G0 + kVL(W0, Xc)k+σmaχL√ne0 + σmaχO(ne0)) + 加工£0)	(61)
(GO + kVL(W 0, Xf )k+ξ + σmaχL √ne0 + σmaxO(ne0)) + √2nL £0)
(62)
≤ √α (1---2) (2G0 + ξ + σmaXL √ne0 +
σmaxO(ne2)) + √2nLt0)
(63)
A.6 Finding Subsets
Let S be a subset of training data points. Furthermore, assume that there is a mapping πw,S : V → S
that for every W assigns every data point i ∈ V to its closest element j ∈ S, i.e. j = πw,S (i) =
arg maxj0∈S sij0 (W), where sij(W) = C - kJτ(Wt, xi)ri - Jτ(Wt, xj)rj k is the similarity
between gradients of i and j, and C ≥ maxij sij (W) is a constant. Consider a matrix Gπw,S ∈
Rn×m, in which every row i contains gradient of πw(i), i.e., [Gπw,S]i. = J τ (W t, xπw,S(i))rπw,S(i).
The Frobenius norm of the matrix Gπw provides an upper-bound on the error of the weighted subset S
in capturing the alignment of the residuals of the full training data with the Jacobian matrix. Formally,
kJ τ (W t, Xtrain)rttrain - γSt J τ (W t, [Xtrain].St)rSt k≤ kGπw,S kF,	(64)
where the weight vector γSt ∈ R|S| contains the number of elements that are mapped to every
element j ∈ S by mapping πw,S, i.e. γj = Pi∈V 1[πw,S (i) = j]. Hence, the set of training points
that closely estimate the projection of the residuals of the full training data on the Jacobian spectrum
can be obtained by finding a subset S that minimizes the Frobenius norm of matrix Gπw,S.
16
Under review as a conference paper at ICLR 2022
B Additional Theoretical Results
B.1 Convergence analysis for training on the coreset and its augmentation
Theorem B.1 Let Li be β-smooth, L be λ-smooth and satisfy the α-PL condition, that is for α > 0,
∣∣VL(W, X)k2 ≥ αL(W, X) for all weights W. Let ξ upper-bound the normed difference in
gradients between the weighted coreset and full dataset. Assume that the network f(W, X) is
Lipschitz in W, X with Lipschitz constant L and L respectively, and L = max{L, L0}. Let Go
the gradient over the full dataset at initialization, σmax the maximum Jacobian singular value at
initialization. Choosing perturbation bound co ≤ -------1√^ where σmaχ is the maximum singular
σmax Ln
value of the coreset Jacobian and n is the size of the original dataset, running SGD on the coreset
and its augmentation using constant step size η 二篇,we get the following convergence bound:
E[kVL(W t，Xc+Caug )k] ≤√lα (1 - 等 P (2G0 + 2ξ + θ (S ))，	(65)
where Xc+caug represents the dataset containing the (weighted) coreset and its augmentation.
Proof As in the proof for Theorem 4.2, we begin with the following inequality
E[kVL(Wt, Xc+caug)k2] ≤ (1 - α2η)tL(W0，Xc+caug)	(66)
≤ 1 (1 - α2η)t kVL(W0, Xc+caug)k2	(67)
Thus, we can write
E[kVL(W0, Xc+caug)k]	(68)
≤ qE[kVL(Wt, Xc+caug)k2]	(69)
≤√α	(1	-	a2η )t	kVL(W 0，Xc+caug )k	(70)
≤ √α	(1	-	OZ)t	(kVL(W0，Xc)k + kVL(W0,	XCaUg)k)	(71)
≤ √α	(1	-	α2η )t	(G0 + ξ + k(J (W 0, Xc)	+	E )(-f(W 0, Xc + e))k)	(72)
The rest of the proof is similar to that of Theorem 4.2.	■
B.2	Lemma for eigenvalues of coreset
The following Lemma characterizes the sum of eigenvalues of the NTK associated with the coreset.
Lemma B.2 Let ξ be an upper bound of the normed difference in gradient of the weighted coreset
and the original dataset, i.e. for full data X and its corresponding coreset XS with weights γS,
and respective residuals r, rS, we have the bound kJT (Wt, X)rt - γSJT (Wt, XS)rSt k≤ ξ. Let
{λi }ik=1 be the eigenvalues of the NTK associated with the coreset. Then we have that
X λ > IkJ T(Wt, X )rtk-ξ∣
i=1λi ≥	两
Proof Let singular values of coreset Jacobian be σi. Let JT(Wt, X)rt = γSJT (Wt, XS)rSt +ξS
where kξS k≤ ξ.
17
Under review as a conference paper at ICLR 2022
Taking Frobenius norm, we get
kγSJT(Wt,XS)rStk= kJT(Wt,X)rt-ξSk
⇒kγSJT(Wt,XS)kkrStk≥ kJT(Wt,X)rt-ξSk
⇒kγSJT(Wt,XS)k≥
JT(Wt X)rt-ξsk
krS Il
JT(Wt, X)rt-ξsk
krS k
kJT(Wt, X)rt-ξsk
krS Il
J T(Wt, X )rtk-ξ∣
krS Il
by reverse triangle inequality
(73)
(74)
(75)
(76)
(77)
(78)
We can make the following observations: For overparameterized networks, with bounded activation
functions and labels, e.g. softmax and one-hot encoding, the norm of the residual vector is bounded,
and the gradient norm is likely to be much larger than residual, especially when dimension of gradient
is large. In this case, the Jacobian matrix associated with small weighted coresets found by solving
Eq. (11), have large singular values.
B.3	Augmentation as Linear Transformation: Linear Model Analysis
We introduce a simplified linear model to extend our theoretical analysis to augmentations modelled
as linear transformation matrices F applied to the original training data. These augmentations are
also originally studied by Wu et al. (2020). In this section, we specifically study the effect of these
augmentations using a linear model when applied to coresets.
B.4	Linear case (1 input layer, 1 output layer)
Lemma B.3 (Augmented coreset gradient bounds: Linear) Let f be a simple linear model with
weights W ∈ Rd×C where f (W, xi) = WTxi, trained on mean squared loss function L. Let
F ∈ Rd×d be a common linear augmentation matrix with norm kF k with augmentation xiaug given
by F xi. Let coreset be of size k and full dataset be of size n. Further assume that the predicted
label of xi and its augmentation Fxi are sufficiently close, i.e. there exists ω such that WT(Fxi)
= WTxi + zi, kzi k≤ ω ∀i. Let ξ upper-bound the normed difference in gradients between the
weighted coreset and full dataset. Then, the normed difference between the gradient of the augmented
full data and augmented coreset is given by
k
kX VL(W, XaUg) - X Ysj VL(W, Xaug)k≤kFk(ξ + √dnω)
i∈V	j=1
for some (small) constant ξ.
Proof By our assumption, we can begin with,
k
kXVL(W,Xi)-XγsjVL(W,Xsj)k≤ξ	(79)
i∈V	j=1
Furthermore, by Mirzasoleiman et al. (2020a), we know that sum of the coreset weights γsj is given
by Pjk==11 γsj ≤ n.
18
Under review as a conference paper at ICLR 2022
Hence,
(80)
k
kX VL(W, XaUg) - X Ysj VL(W, Xaug)k
i∈V	j=1
i∈V
k
-yi] -Xγsj(J(W,XsajUg))T[WT(FXsj)-ysj]k (81)
j=1
k
= kXFXi[WT(FXi) -yi] - XγsjFXsj[WT(FXsj) - ysj]k	(82)
k
= kFXXi(WTXi-yi)-FXγsj Xsj (W	Xsj	+ zi	-	ysj )k	(83)
kk
= kFXVL(W,Xi)-FXγsjVL(W,Xsj)-FXγsjXsjzsjk	(84)
kk
≤ kFkkXVL(W,Xi)-XγsjVL(W,Xsj)k+kFkkXγsjXsjzsjk	(85)
i∈V	j=1	j=1
≤ kFkξ + √dkFknω	(86)
=kF k(ξ + √dnω)	(87)
Corollary B.4 In the simplified linear case above, the difference in gradients of the full training
data with its augmentations (VL(W, Xf +aug)) and gradients of the coreset with its augmentations
(VL(W, Xc+caug)) can be bounded by
kVL(W, Xf+aug) - VL(W, Xc+caug)k≤ (kFk + 1)ξ + √dkFknω
Proof Applying Eq. (79) and Lemma B.3, we obtain
kVL(W,Xf+aug)-VL(W,Xc+caug)k	(88)
= k(VL(W,Xf)+VL(W,Xaug))-(VL(W,Xc)+VL(W,Xcaug))k	(89)
= k(VL(W,Xf)-VL(W,Xc))+(VL(W,Xaug)-VL(W,Xcaug))k	(90)
≤ k(VL(W, Xf) -VL(W, Xc))k + k(VL(W, Xaug) -VL(W, Xcaug))k	(91)
≤ ξ + kFk(ξ + √dnω)	(92)
= (kF k+1)ξ + √dkF knω	(93)
Theorem B.5 (Convergence of linear model) Let f be a linear model with weights W and aug-
mentation be represented by the common linear transformation F. Let Li be β-smooth, L be
λ-smooth and satisfy the α-PL condition, that is for α > 0, kVL(W, X)k2 ≥ αL(W, X) for all
weights W. Let ξ upper-bound the normed difference in gradients between the weighted coreset and
full dataset and ω bound WT(FXi) = WTXi + zi, kzik≤ ω ∀i. Let G00 be the gradient over the
full dataset and its augmentations at initialization. Then, running SGD on the size k coreset with its
augmentation using constant SteP size η = λβ, we get thefollowing convergence bound:
t	_t
E[kVL(Wt, Xc+caug)k] ≤ √α(1 - α2η) 2(G0 + (kFk+1)ξ + √dkF∣∣nω)
19
Under review as a conference paper at ICLR 2022
Proof From Bassilyet al. (2018), we have
E[∣∣VL(Wt, Xc+caug)k2] ≤(1 - α2η)tL(W0, Xc+caug)	(94)
≤ 1(1- α2η)t kVL(W0, Xc+caug)k2	(95)
(96)
Using Jensen’s inequality, we have
E[kVL(Wt, Xc+caug)k]	(97)
≤ ,E[kVL(Wt, Xc+caug)k2]	(98)
≤√α (l- a2η)t kVL(W0,Xc+caug)k	(99)
≤√1ɑ(1 - α2η) t(G0 + (kFk+1)ξ + √dkFknω)	(100)
where the last inequality follows from applying Corollary B.4.	■
C Appendix C: Extra experiments
C.1 Full Results for Table 1
This section contains full experiment results including standard deviations and the full augmentation
benchmark for Table 1. Augmenting coresets of size 10% achieves 51% of the improvement obtained
from augmentation of the full data and further enjoys a 6x speedup in training time on CIFAR10.
This speedup becomes more significant when using strong augmentation techniques which are mostly
computationally demanding, especially when applied to the entire dataset.
Table 4: Supplementary table for Table 1 - Test accuracy on CIFAR10 + ResNet20, SVHN +
ResNet32, CIFAR10-Imbalanced + ResNet32 including standard deviation errors and full dataset
augmentation accuracy.
Method	Size	CIFAR10	CIFARI0-IMB	SVHN
None	0%	89.46 ± 0.17%	87.08 ± 0.50%	95.676 ± 0.108%
	5%	90.34 ± 0.18%	88.48 ± 0.25%	95.760 ± 0.084%
Random	10%	91.07 ± 0.13%	89.52 ± 0.15%	96.187 ± 0.112%
	30%	92.11 ± 0.12%	91.11 ± 0.18%	96.569 ± 0.073%
	5%	90.79 ± 0.19%	88.77 ± 0.35%	96.165 ± 0.108%
Max-Loss	10%	91.39 ± 0.08%	89.22 ± 0.48%	96.370 ± 0.076%
	30%	92.43 ± 0.07%	91.11 ± 0.25%	96.735 ± 0.068%
	5%	90.87 ± 0.05%	89.10 ± 0.41%	96.121 ± 0.055%
Coreset	10%	91.54 ± 0.19%	89.75 ± 0.52%	96.354 ± 0.091%
	30%	92.49 ± 0.15%	91.12 ± 0.26%	96.791 ± 0.051%
All	100%	93.50 ± 0.25%	92.48 ± 0.34%	97.068 ± 0.030%
C.2 Speed-up measurements
We measure the improvement in training time in the case of training on full data and augmenting
subsets of various sizes. While our method yields similar or slightly lower speed-up to the max-loss
policy and random approach respectively, our resulting accuracy outperforms these two approaches.
For example, for SVHN/Resnet32 using 30% coresets, we sacrifice 10% of the speed-up to obtain an
additional 24.8% of the gain in accuracy from full data augmentation when compared to a random
subset of the same size.
20
Under review as a conference paper at ICLR 2022
Table 5: Speedup on CIFAR10 + ResNet20 (C10/R20), SVHN + ResNet32 (SVHN/R32).
Dataset	Full Aug. 100%	Ours						Max loss.	Random.
		5%	10%	15%	20%	25%	30%	30%	30%
C10 / R20	1x	7.93x	6.31x	4.46x	4.27x	3.41x	3.43x	3.48x	4.03x
SVHN / R32	1x	5.35x	3.93x	3.40x	2.80x	2.49x	2.18x	2.21x	2.43x
C.3 Training on full data and augmenting small subsets re-selected every
EPOCH
We apply our proposed method to select a new subset for augmentation every epoch (i.e. using
R = 1) and compare our results with other approaches using accuracy and percentage of data not
selected (NS). We see that while the max-loss policy selects a small fraction of data points over
and over and random uniformly selects all the data points, our approach successfully finds the
smallest subset of data points that are the most crucial for data augmentation. Hence, it can achieve
a superior accuracy than max-loss policy, while augmenting only slightly more examples. This
confirms the data-efficiency of our approach. This is especially evident when using coresets of size
0.2%. Furthermore, despite the random baseline using a significantly larger percentage of data, it is
outperformed by our approach in both data-efficiency and accuracy. We emphasize that results in this
table is different from that of Table 4, as default augmentations on the full training data are performed
once every R = 1 epochs instead of every R = 20 epochs. Since selecting subsets at every epoch
can be computationally expensive, we only perform these experiments on small coresets and hence
still enjoy good speedups compared to full data augmentation. This shows that our approach is still
effective at very small subset sizes, hence can be computationally efficient even when subsets are
re-selected every epoch.
Table 6: Training on full data and selecting a new subset for augmentation every epoch (R = 1).
Subset	Random	Max-loss Policy	Ours
Acc	NS (%)	Acc	NS (%)	Acc	NS (%)
0%	91.96 ± 0.12	- 0.2%	92.22 ± 0.22	67.03 ± 0.04 0.5%	92.06 ± 0.17 36.70 ± 0.18	91.96 ± 0.12	-	91.96 ± 0.12	- 91.94 ± 0.12	86.70 ± 0.15	92.26 ± 0.13	79.19 ± 1.10 92.20 ± 0.13	76.80 ± 0.31	92.27 ± 0.08	63.23 ± 0.35
C.4 Additional visualizations for training on coresets and its augmentations -
Measuring training dynamics over time
We include additional visualizations in Figure 5 for training on coresets and its augmentations as
supplementary plots to Figure 3c and Table 3. We plot metrics obtained during each point (epoch)
of the training process based on percentage of data selected/used and test accuracy achieved. All
metrics are averaged over 5 runs and obtained using R = 1. These plots demonstrate that coreset
augmentation approaches outperform random augmentation baselines throughout the training process.
Furthermore, they show that augmentation of coresets result in a larger increase in test accuracy
compared to augmentation of randomly selected training examples, especially for small subset sizes.
21
Under review as a conference paper at ICLR 2022
O 2500	5000 7500 IOOOO 12500 15000
Top n selections
Figure 6: Intersection between max-loss and coresets in the top N points selected aggregated across
the entire training process. Here, we show the increasing overlap between max-loss and coreset
points as N grows.
QOQOOO
6 5 4 3 2 1
AOe.InUOV
Figure 7: Qualitative evaluation of coreset and max-loss points.
(c) CIFAR10 - 0.5%
- 0.2%
(a) CIFAR10 - 0.1%	(b) CIFAR10
80604020
AuB.lnu3v
(d) CIFAR10 - 1%
O 20	40	60	80 IOO
% data selected
(e) CIFAR10 - 5%
Figure 5: Supplementary plots for Figure 3c: Training on coreset and its augmentation compared to
random baseline, measured using test accuracy against percentage of data used on CIFAR10 dataset
across various subset sizes. Accuracy and percentage of data used are measured at every epoch and
averaged over 5 runs.
C.5 Intersection of max-loss policy and coresets
Figure 3a depicts the increase in intersection between max-loss subsets and coresets over time. In
addition, we also aggregate 30% subsets selected every R = 20 epochs using both approaches over
the entire training process to compute intersection between the top N selected data points. Our plots
in Figure 6 suggest that a similar pattern holds in this setting. We also qualitatively visualize this in
Figure 7.
22