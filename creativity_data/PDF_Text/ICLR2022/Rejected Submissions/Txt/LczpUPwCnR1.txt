Under review as a conference paper at ICLR 2022
ES-Based Jacobian Enables Faster Bilevel Op-
TIMIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Bilevel optimization (BO) has arisen as a powerful tool for solving many modern
machine learning problems. However, due to the nested structure of BO, exist-
ing gradient-based methods require second-order derivative approximations via
Jacobian- or/and Hessian-vector computations, which can be very costly in prac-
tice, especially with large neural network models. In this work, we propose a novel
BO algorithm, which adopts Evolution Strategies (ES) based method to approx-
imate the response Jacobian matrix in the hypergradient of BO, and hence fully
eliminates all second-order computations. We call our algorithm as ESJ (which
stands for the ES-based Jacobian method) and further extend it to the stochas-
tic setting as ESJ-S. Theoretically, we characterize the convergence guarantee and
computational complexity for our algorithms. Experimentally, we demonstrate the
superiority of our proposed algorithms compared to the state of the art methods
on various bilevel problems. Particularly, in our experiment in the few-shot meta-
learning problem, we meta-learn the twelve millions parameters of a ResNet-12
network over the miniImageNet dataset, which evidently demonstrates the scala-
bility of our ES-based bilevel approach and its feasibility in the large-scale setting.
1	Introduction
Bilevel optimization has recently arisen as a powerful tool to capture various modern machine learn-
ing problems, including meta-learning (Bertinetto et al., 2018; Franceschi et al., 2018; Rajeswaran
et al., 2019; Ji et al., 2020a; Liu et al., 2021a), hyperparamater optimization (Franceschi et al., 2018;
Shaban et al., 2019), neural architecture search (Liu et al., 2018a; Zhang et al., 2021), etc. Bilevel
optimization generally takes the following mathematical form:
min Φ(x) := f (x,y*(x)) s.t. y*(x) = argming(x,y),
x∈Rp	y∈Rd
(1)
where the outer and inner objectives f : Rp × Rd → R and g : Rp × Rd → R are both continuously
differentiable with respect to (w.r.t.) the inner and outer variables x ∈ Rp and y ∈ Rd. In many
machine learning problems, the loss functions f and g take a finite-sum form over a set of given data
Dn,m = {ξi , ζj , i = 1, ..., n, j = 1, ..., m} as follows:
f(X, y) = n Pn=ι F(X, y; ξi), g(x, y) = m1 Pm=IG(X, y; Zi)
(2)
where the sample sizes n and m are typically very large. In this work, we consider a popular setting
where the total objective function Φ(X) in eq. (1) is possibly nonconvex w.r.t. X and the inner function
g is strongly convex w.r.t. y.
Gradient-based methods have served as a popular tool for solving bilevel optimization problems.
Two types of approaches have been widely used: the iterative differentiation (ITD) method (Domke,
2012; Maclaurin et al., 2015; Finn et al., 2017; Franceschi et al., 2017; Shaban et al., 2019; Ra-
jeswaran et al., 2019; Liu et al., 2020a) and the approximate iterative differentiation (AID) method
(Domke, 2012; Pedregosa, 2016; Gould et al., 2016; Liao et al., 2018; Lorraine et al., 2020). In
the ITD method, the objective value f (x, y(x)) is first computed using an approximation point y(x)
close to y* (χ), and the hypergradient VΦ(x) = d"x∂c (X)) is then approximated by "f'x∂XXx) using
either reverse or forward mode automatic differentiation. Alternatively, the AID method leverages
1
Under review as a conference paper at ICLR 2022
the Implicit Function Theorem (IFT) to establish an implicit expression for the hypergradient. Due
to the bilevel structure of the problem, these gradient-based ITD and AID approaches typically in-
volve second-order matrix computations. Existing efficient implementations of these methods often
adopt Jacobian- or/and Hessian-vector computations, which can still be very costly in practice for
high-dimensional problems with deep neural networks.
To overcome the computational challenge of current gradient-based methods, Evolution Strategies
(ES) can be a good candidate to eliminate the second-order computations, which uses function val-
ues for estimating its gradient in blackbox optimization (Nesterov & Spokoiny, 2017). An ES-based
bilevel optimizer has recently been proposed in Gu et al. (2021) for hyperparameter optimization,
which uses the ES method to approximate the hypergradient of Φ(x), treating Φ(x) fully as a black-
box objective. However, the hypergradient in bilevel optimization has a much more involved struc-
ture (see eq. (3)) than the gradient in standard minimization problems, and hence direct use of the
ES method without exploiting the structure of the hypergradient can encounter a large estimation
bias and result in poor performance in practice, especially in deep learning as demonstrated in our
experiments in Section 4. This hence motivates the following intriguing question:
Can we design a better hypergradient estimator based on the ES method by leveraging the analytical
structure of the hypergradient, and then use such an estimator to construct more efficient ES-based
bilevel optimizers, which are scalable for high-dimensional problems?
Furthermore, since the hypergradient in bilevel optimization is much more challenging to estimate
than the gradient in standard minimization problems, it is unclear whether the ES-based estimator
can yield a provably convergent algorithm for bilevel optimization. So far the ES-based bilevel
optimizer in Gu et al. (2021) did not come with a polynomial-time complexity guarantee. Our
empirical experiments in Section 4 show that such an algorithm fail to converge even with shallow
neural networks. This thus motivates the second question we ask:
Can the ES method lead to bilevel optimizers with theoretical finite-time convergence guarantee?
This paper provides the affirmative answers to both questions.
1.1	Main Contributions
Novel ES-based Jacobian bilevel optimizer. We propose a novel bilevel optimizer with the ES-
based Jacobian estimation, which we call as ESJ. In contrast to the existing ES-based optimizer in
Gu et al. (2021), which estimates the hypergradient by treating the outer-level objective Φ(x) fully
as a blackbox, ESJ estimates only the respone Jacobian matrix (i.e., gradient of y* (x) with respect
to x), which is the major computational bottleneck, and then leverages the analytical structure of the
hypergradient to construct a much more accurate estimator. Further, ESJ has only gradient compu-
tations, and is computationally much more efficient than existing AID and ITD bilevel optimizers
that require Hessian- and/or Jacobian-vector product computations. We further propose a stochastic
bilevel optimizer ESJ-S to allow the algorithm scalable over large dataset.
Convergence guarantee. Theoretically, we characterize the convergence rate and their computa-
tional complexity for both ESJ and ESJ-S to achieve an -accurate solution. Technically, in contrast
to the standard analysis of ES-based methods on the smoothed blackbox function values, we de-
velop tools to analyze the ES estimator on the output of the execution of an inner optimizer. Such
an analysis can be of independent interest for ES-based bilevel optimization.
Superior performance. Experimentally, our algorithms ESJ and ESJ-S achieve superior perfor-
mance compared to the current state of the art bilevel optimizers. Our approach is not only efficient
and scalable, but also stable and robust across various bilevel problems. In particular, on the few-shot
meta-learning problem, we meta-learn the twelve millions weights of a ResNet-12 network. Previ-
ous applications of other bilevel algorithms to meta-learning were limited only to settings where the
backbone network has only up to few hundreds of thousands of parameters.
1.2	Related Work
Bilevel optimization. Bilevel optimization has been studied for decades since Bracken & McGill
(1973). A variety of bilevel optimization algorithms were then developed, including constraint-
based approaches (Hansen et al., 1992; Shi et al., 2005; Moore, 2010), approximate implicit differ-
2
Under review as a conference paper at ICLR 2022
entiation (AID) approaches (Domke, 2012; Pedregosa, 2016; Gould et al., 2016; Liao et al., 2018;
Lorraine et al., 2020) and iterative differentiation (ITD) approaches (Domke, 2012; Maclaurin et al.,
2015; Franceschi et al., 2017; Finn et al., 2017; Shaban et al., 2019; Rajeswaran et al., 2019; Liu
et al., 2020a). These methods often suffer from expensive computations of second-order information
(e.g., Hessian-vector products). Hessian-free algorithms were recently proposed based on interior-
point method (Liu et al., 2021b) and Evolution Strategies (ES) (Gu et al., 2021). This paper proposes
a more efficient ES-based approach via exploiting the benign structure of the hypergradient.
Recently, the convergence rate has been established for gradient-based (Hessian involved) bilevel
algorithms (Grazzi et al., 2020; Ji et al., 2021; Rajeswaran et al., 2019; Ji & Liang, 2021). This
paper provides the convergence analysis for the proposed Hessian-free ES-based approach.
Stochastic bilevel optimization. Ghadimi & Wang (2018); Ji et al. (2021); Hong et al. (2020) pro-
posed stochastic gradient descent (SGD) type bilevel optimization algorithms by employing Neu-
mann series for Hessian-inverse-vector approximation. Recent works (Khanduri et al., 2021a;b;
Chen et al., 2021; Guo et al., 2021; Guo & Yang, 2021; Yang et al., 2021) then leveraged momentum-
based variance reduction to further reduce the computational complexity of existing SGD-type
bilevel optimizers. In this paper, we propose a stochastic ES-based method, which eliminate the
computation of second-order information required by all aforementioned stochastic methods.
Please see Appendix B for more works about applications of bilevel optimization and ES methods.
2	Proposed Algorithms
We describe our proposed deterministic and stochastic algorithms for bilevel optimization.
2.1	Hypergradients
The key step in popular gradient-based bilevel optimizers is the estimation of the hypergradient (i.e.,
the gradient of the objective with respect to the outer variable x), which takes the following form:
VΦ(x) = Vχf (x,y*(x)) + J*(x)>Vy f(x,y*(x))	(3)
where the Jacobian matrix /(χ) = dyd(X) ∈ Rd×p. Following Lorraine et al. (2020), it can be
seen that VΦ(χ) contains two components: the direct gradient Vxf (x, y* (x)) and the indirect gra-
dient /(χ)>Vyf (x, y*(χ)). The direct component can be efficiently computed using the existing
automatic differentiation techniques. The indirect component, however, is computationally much
more complex, because /(χ) takes the form of J (x) = 一 [V：g (x, y*(x))] VxVyg (x, y*(χ))
(if Vyg (x,y*(x)) is invertible), which contains the Hessian inverse and the second-order mixed
derivative. Some approaches mitigate the issue by designing Jacobian-vector and Hessian-vector
products (Pedregosa, 2016; Franceschi et al., 2017; Grazzi et al., 2020) to replace second-order
computations. But these algorithms still have poor scalability to modern large-scale bilevel prob-
lems with high-dimensional variables such as neural network parameters. We next introduce the ES
approach which is at the core of our idea for designing scalable Hessian-free bilevel optimizers.
2.2	Evolution S trategies (ES) Method
Evolution Strategies (ES) is a powerful technique to estimate the gradient of a function based on
function values, when it is not feasible (such as in black-box problems) or computationally costly to
evaluate the gradient. The idea of the ES method in Nesterov & Spokoiny (2017) is to approximate
the gradient of a general black-box function h : Rn → R using the following oracle based only on
the function values (i.e., the zeroth-order information)
Vh(x; U)= h(X + μu)-h(X) U	(4)
μ
where U ∈ Rn is a Gaussian random vector and μ > 0 is the smoothing parameter. Such an oracle
can be shown to be an unbiased estimator of the gradient of the smoothed function Eu [h(x + μu)].
2.3	ES-based Jacobian and its Enabled Bilevel Optimizers
Drawback of an existing ES-based bilevel optimizer. In Gu et al. (2021), the ES method was
directly applied to estimate the hypergradient of Φ(x), treating Φ(x) as a blackbox objective. Since
3
Under review as a conference paper at ICLR 2022
the hypergradient has a complex form as in eq. (3) and can be very sensitive to both inner and outer
functions, such an estimation will likely have a large bias error. As our experiments in Section 4
demonstrate, such a method is not robust and even fails to converge in complex bilevel optimization
problems (such as with neural network parameters).
Our key idea is to exploit the analytical structure of the hypergradient in eq. (3), where the derivatives
Vχf (x, y* (x)) and Ny f (x, y*(χ)) can be computed efficiently and accurately, and then use the ES
method only to estimate the Jacobian J (x), which is the major term posing computational difficulty.
In this way, our estimation of the hypergradient can be much more accurate and reliable.
We propose the following novel ES-based Jacobian estimator, which contains two ingredients: (i)
for a given x, apply an algorithm to solve the inner optimization problem and use the output as an
approximation of y*(x); for example, the output yN(x) of N gradient descent steps of the inner
problem can serve as an estimate for y* (x). Then JN(x) = %Ix) serves as an estimate of J*(x);
and (ii) construct an ES-based Jacobian estimator JN (x; U) ∈ Rd×p for JN (x) as
Jn (x; U) = yN(x + ""-N(X) u>	⑸
μ
where u ∈ Rp is a Gaussian vector with independent and identically distributed (i.i.d.) entries.
Then for any vector v ∈ Rd, the Jacobian-vector product can be efficiently computed using only
vector-vector dot product JN(x; u)>v = <δ(x; u), Vi u, where δ(x; U) = y (x+μu)-y (X) ∈ Rd.
ESJ: A novel bilevel optimizer with ES Jacobian oracles. We design a bilevel optimizer (see
Algorithm 1) using the ES-based Jacobian oracle in eq. (5), which we call as the ESJ algorithm. At
each step k of the algorithm, ESJ runs an N -step full GD to approximate ykN (xk). ESJ then samples
Q Gaussian vectors {uk,j ∈ N(0, I),j = 1, ..., Q}, and for each sample Uk,j, runs an N -step full GD
to approximate yN (Xk + μuk,j), and then computes the Jacobian estimator JN (x; Ukj) as in eq. (5).
Then the sample average over the Q estimators is used for constructing the following hypergradient
estimator for updating the outer variable x.
V φ(xk) = Vxf (Xk ,yN) + 吉 PQ=I JN (Xk; uk,j )Vyf (Xk ,yN)
=Vxf (Xk ,yN ) + Qq PQ=I <δ (Xk ; Ukj ), Vyf (Xk ,yN )〉Uk,j.	3 * * (6)
Computationally, in contrast to the existing AID and ITD based bilevel optimizers (Pedregosa
(2016), Franceschi et al. (2018), Grazzi et al. (2020)) that contains the complex Hessian- and/or
Jacobian-vector product computations, ESJ has only gradient computations, and hence is computa-
tionally much more efficient as demonstrated in our experiments.
ESJ-S: A stochastic bilevel optimizer with ES Jacobian oracles. For the finite-sum problem with
the objective functions given in eq. (2), we design a stochastic bilevel optimizer (see Algorithm 2
in Appendix A) based on ES Jacobian oracles, which we call as ESJ-S.
Differently from Algorithm 1, which applies GD updates to find yN(xk), ESJ-S adopts N stochas-
tic gradient descent (SGD) steps to find {YkN, YkN,1, ..., YkN,Q} to the inner problem, each with the
outer variable set to be Xk + μUk,j. Note that all SGD runs follow the same batch sampling path
{S0, ..., SN-1}. The Jacobian estimator JN(xk; Uk,Q) can then be computed as in eq. (5). At the
outer level, ESJ-S samples a new batch DF independently from the inner batches {S0, ..., SN-1}
to evaluate the stochastic gradients VxF(Xk,y£； DF) and NyF(χk,YNN; DF). The hypergradient is
then estimated as follows.
VΦ(xk) = VxF(Xk, YN; DF) + Q1 Pj=1 (δ(xk; Uk,j), VyF(Xk, YN; DF)〉Uk,q.	⑺
3 Convergence Analysis
3.1 Technical Assumptions
In this work, we focus on the following types of loss functions.
Assumption 1. The innerfUnction g(x, y) is μg-strongly convex with respect to the variable y. For
the finite-sum case, the same assumption holds for VG(X, y; ζ).
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Bilevel optimizer via ES Jacobians (ESJ)
1:	Input: lower- and upper-level stepsizes α, β > 0, initializations x0 ∈ Rp and y0 ∈ Rd, inner and outer
iterations numbers K and N , and number of Gaussian vectors Q.
2:	fork = 0, 1,2, ...,Kdo
3:	Set yk0 = y0,	yk0,j = y0, j = 1, ..., Q
4:	for t = 1, 2, ..., N do
5:	Update yk = yk-1 - αVyg(xk,y『)
6:	end for
7:	for j = 1, ..., Q do
8:	Generate uk,j = N (0, I) ∈ Rp
9:	for t = 1, 2, ..., N do
10:	Update ykt,j = ykt-,j* 1 - αVyg xk + μuk,j ,yk,j )
11:	end for
12:	Compute δj = yk,j-yk
13:	end for
14：	Compute VΦ(xk) = Vxf(Xk,yN) + Q Pj=I ©, Vyf(xk,yN))uk,j
15:	Update xk+1 = xk - βVΦ(xk )
16:	end for
We take the following assumptions on the inner and outer loss functions g(x, y) and f(x, y), as also
adopted in Ghadimi & Wang (2018); Ji et al. (2021); Yang et al. (2021).
Assumption 2. Let W = (x, y) ∈ Rd+p. The gradient Vg(W) is Lg-Lipschitz continuous, i.e., for
any w1,w2 ∈ Rd+p JVg(WI) — Vg(w2)∣∣ ≤ LgllWI — W2 ^； further, the derivatives Vyg(w) and
VxVyg(W) are ρ- and τ -Lipschitz continuous, i.e, Vy2g(W1) - Vy2 g (W2)F ≤ ρW1 - W2 and
lVxVyg(W1) — VxVyg(W2)lF ≤ τ lW1 — W2 l. For the finite-sum case, the same assumptions
hold for G(W; ζ).
Assumption 3. Let W = (x, y) ∈ Rd+p. The objective f(W) and its gradient Vf (W) are M- and
Lf -Lipschitz continuous, i.e., for any W1, W2 ∈ Rd+p,
f (W1) —	f (W2)	≤ M llW1	—	W2 ll, llVf (W1)	— Vf(W2)ll ≤	Lf llW1	— W2 ll.
For the finite-sum case, the same assumptions hold for F(W; ξ).
For the finite-sum setting, we take the standard bounded-variance assumption on VG(W; ζ).
Assumption 4. Gradient VG(W; ζ) has a bounded variance, i.e., Eζ kVG(W; ζ) — Vg(W)k2 ≤ σ2
for some constant σ ≥ 0.
3.2 Convergence Analysis for ESJ
Different from the standard zeroth-order analysis for a blackbox function, here we develop new
techniques to analyze the ES-based estimator that depend on the entire inner optimization trajectory,
which is specific to bilevel optimization. We first establish the following essential proposition which
characterizes the LiPshitzness property of the approximate Jacobian matrix JN (x) = dyJX).
Proposition 1. Suppose that Assumptions 1 and 2 hold. Let LJ =(1 + 乎)(μL + PL), with
L = max{Lf, Lg}. Then, the Jacobian JN (x) is Lipschitz continuous with constant LJ:
llJN (x1) — JN (x2)llF ≤ LJ llx1 — x2ll	∀x1, x2 ∈ Rp.	(8)
We next provide an upper-bound on the hypergradient estimation error for ESJ.
Proposition 2. Suppose that Assumptions 1, 2, and 3 hold. Consider the ESJ algorithm. Its hyper-
gradient estimation error can be upper-bounded as:
E∣∣V Φ(xk )-VΦ(xk )∣∣2 ≤O ((1 — αμg )n + Q + μ2dp3 + 啜4)	(9)
where E[∙] is Conditioned on Xk and taken over the Gaussian vectors {uk,j ： j = 1,…,Q}.
5
Under review as a conference paper at ICLR 2022
By using the smoothness property in Proposition 1 and the upper-bound in Proposition 2, we provide
the following characterization of the convergence rate for ESJ.
Theorem 1 (Convergence of ESJ). Suppose that Assumptions 1, 2, and 3 hold. Choose the inner-
and outer-loop stepsizes respectively as α ≤ L and β = 4L^, where Lφ = L + "+TMM +
PLM+L2+TML + ρL 3M. Then, the iterates Xk for k = 0,..., K — 1 of ESJ in Algorithm 1 satisfy:
μg	μg
K PK-01 EllVΦ(χk)∣∣2 ≤ 16Lφ(Φχo)-φ*) +∆ι	(10)
with Φ* = infχ Φ(x) and ∆ι = O ((1 一 αμg)N + Q + μQp + μ2dp3).
Further, choose K, N, μ, and Q at the order of O( ɪ), O (log ɪ), O(min{ PJdp, 1 Jdp}) and
O(P). Then ESJ achieves an E-accurate stationary point with O(g log ɪ) computations of the
gradient Ny g(x, y) and O( ɪ) computations of the gradients PXf (x, y) and Ny f (x, y).
Theorem 1 characterizes the sublinear convergence of ESJ with respect to the number K of outer
iterations due to the nonconvexity of the outer objective. The convergence error in ∆1 is due to
the estimation error of the Jacobian matrix J via our ES oracle, which captures three types of
errors: (a) the approximation error between JN and J via inner-loop gradient descent, which
decreases exponentially w.r.t. the number N of inner iterations due to the strong convexity of the
inner objective; (b) the error between our ES oracle and the Jacobian J of the smoothed output
EuyN (Xk + μu), which decreases sublinearly w.r.t. the batch size Q, and (c) the error between the
Jacobians JN and Jμ, which can be controlled by the smoothness parameter μ.
Theorem 1 also indicates that ESJ requires only gradient computations to converge, and eliminates
Hessian- and Jacobian-vector products required by the existing AID and ITD based bilevel optimiz-
ers (Pedregosa (2016), Franceschi et al. (2018), Grazzi et al. (2020)). Thus, ESJ is computationally
much more efficient particularly for high-dimensional problems.
3.3 Convergence Analysis for ESJ-S
In this section, we apply the stochastic algorithm ESJ-S to the finite-sum objective in eq. (2) and an-
alyze its convergence rate. We first note that the Lipschitzness property established in Proposition 1
is general and can also be used here. The following proposition establishes an upper bound on the
estimation error of Jacobian J= by JN = dY},where YN is the output of N inner SGD updates.
Proposition 3. Suppose that Assumptions 1, 2, and 4 hold. Consider the ESJ-S algorithm. Choose
the inner-loop stepsize as ɑ = L+μ-, where L = max{Lf, Lg}. Then, we have:
E∣∣JN -JjIF ≤cN LJ +
11	11F Y μg
λ(L + μg )2(1 — αμg )CN 1	Γ
(L + μg)2(1 一 αμg) 一(L 一 μg)2	1 - CY
where λ, Γ, and Cγ < 1 are constants (see appendix G for their forms).
I-V T	.	∙ t	F	t	.<	CjI	t∙ . Λ Γ≥7 T / ∖ ♦	∕r∖
We next provide an upper-bound on the estimation error of the hypergradient by NΦ(Xk) in eq. (7).
Proposition 4. Suppose that Assumptions 1, 2, 3, and 4 hold. Consider the ESJ-S algorithm. Set
the inner-loop stepsize as ɑ = L+μ- where L = max{Lf, Lg}. Then, we have:
EllNb Φ(Xk) 一 NΦ(Xk)ll2
≤O
((I- αμg)N+S+Df+Q+μ2dp3+μQ-)
where S and Df are the sizes of the inner and outer mini-batches, respectively.
Based on Propositions 1, 3, and 4, we characterize the convergence rate for ESJ-S.
Theorem 2 (Convergence of ESJ-S). Suppose that Assumptions 1, 2, 3, and 4 hold. Set the inner-
and outer-loop stepsizes respectively as ɑ = LJμ- and β = 4L^, where L = max{Lf, Lg} and
the constant LΦ are defined in Theorem 1. Then, the iterates Xk, k = 0, ..., K 一 1 of ESJ-S satisfy:
KPK=01 E∣∣NΦ(xk)∣∣2 ≤ 16&(X0K-®*)L' +∆2,	(11)
6
Under review as a conference paper at ICLR 2022
where △[ = O ((1 — αμg)N + 1 + DIy + Qp + μ Qp—+ μ2dp3).
For ESJ-S to attain an -accurate stationary point, it suffices to select K, S, and Df at the order
of O( 1), and N, μ, and Q respectively at the order of O (log ɪ), O(min{ p^'d^p, P ↑fdp}) and
O(P), which thus amount to O(g log ɪ) computations of the gradient NyG(x, y, ξ) and O(表)
computations of the gradients NxF (x, y, ξ) and Ny F (x, y, ξ).
Comparing to the convergence error ∆1 in Theorem 1 for the deterministic algorithm ESJ, Theo-
rem 2 for the stochastic algorithm ESJ-S captures two more sublinearly decreasing error terms 1
and Dy respectively due to the sampling of inner and outer batches to estimate the objectives.
To compare between ESJ-S and ESJ on the finite-sum objective in eq. (2), Theorem 1 suggests that
ESJ attains an e-accurate stationary point of eq. (2) with O(臂 log ɪ) computations of NG and
O(n) computations of NF. As a comparison, Theorem 2 suggests that ESJ-S achieves O(g log ɪ)
computations of NG and O(1)computations of NF, which outperforms ESJ in the large sample
regime where the sample sizes n,m> ɪ. This also reflects the advantage of SGD over GD.
4 Experiments
We validate our algorithms over three bilevel problems: shallow hyper-representation (HR) with
linear/2-layer net embedding model on synthetic data, deep HR with LeNet network (LeCun et al.,
1998) on MNIST dataset, and few-shot meta-learning with ResNet-12 on miniImageNet dataset.
We run all models using a single NVIDIA Tesla P100 GPU. Hyperparamter Optimization (HO)
experiments can be found in Appendix D. All running time are in seconds.
(a) d = 128
045	0.1«	«.15	02«	«25	«30	«3S	«40
RlJnnIngtIme
(d) Outer loss
ESJ
HOZOG
——AID-CG
AID-FP
U
(b) d = 256
so-^æbo
Ol 02	«3 0Λ OS «4	07	04
Running time
(c) ESJ-N: N inner GD steps
0.1«	«20	血
Running time
(f) ESJ-N: N inner GD steps
——ESJ
HOZOG
——AID-CG
——AID-FP -
---ΓTD-R
Running time
(e) Hypergradient norm
ESJ
HOZOG
——AIG-CG
AID-FP
ΓTD-R
Running time
: 1
ssq-,no
^as Eo is m^
Runnlngtlme
Figure 1: First row: HR with linear embedding model. Second row: HR with two-layer net.
4.1	Shallow Hyper-Representation on Synthetic Data
The hyper-representation (HR) problem (Franceschi et al., 2018; Grazzi et al., 2020) searches for a
regression (or classification) model following a two-phased optimization process. The inner-level
identifies the optimal linear regressor parameters w, and the outer level solves for the optimal em-
bedding model (i.e., representation) parameters λ. Mathematically, the problem can be modeled by
the following bilevel optimization:
min f(λ) = ɪ kT(Xi； λ)w*(λ) - Y1k2 s.t. w*(λ) = argminɪ ∣∣T(X2; λ)w - Y2『+ Y ||w『(12)
λ∈Rp	2n1	w∈Rd 2n2	2
where X2 ∈ Rn2 ×m and X1 ∈ Rn1 ×m are matrices of synthesized training and validation data,
and Y2 ∈ Rn2, Y1 ∈ Rn1 are the corresponding response vectors. In the case of shallow HR, the
embedding function T(∙; λ) is either a linear transformation or a two-layer network. We generate
data matrices X1, X2 and labels Y1, Y1 following the same process in Grazzi et al. (2020).
We compare our ESJ algorithm with the baseline bilevel optimizers AID-FP, AID-CG, ITD-R, and
HOZOG (see Appendix C.1 for details about the baseline algorithms and hyperparameters used).
7
Under review as a conference paper at ICLR 2022
Figure 1 show the performance comparison among the algorithms under linear and two-layer net
embedding models. It can be observed that for both cases, our proposed method ESJ converges
faster than all the other approaches, and the advantage of ESJ becomes more significant in Figure 1
(d), which is under a higher-dimensional model of a two-layer net. In particular, ESJ outperforms
the existing ES-based algorithm HOZOG. This is because HOZOG uses the ES technique to ap-
proximate the entire hypergradient, which likely incurs a large estimation error. In contrast, our ESJ
exploits the structure of the hypergradient and uses ES only to estimate the response Jacobian so that
the estimation of hypergradient is more accurate. Such an advantage is more evident under a two-
layer net model, where HOZOG does not converge as shown in Figure 1 (d). This can be explained
by the flat hypergradient norm as shown in Figure 1 (e), which indicates that the hypergradient esti-
mator in HOZOG fails to provide a good descent direction for the outer optimizer. Figure 1 (c) and
(f) further show that the convergence of ESJ does not change substantially with the number N of
inner GD steps, and hence tuning of N in practice is not costly.
4.2	Deep Hyper-Representation on MNIST Dataset
In order to demonstrate the advantage of our proposed algorithms in large neural net models, we
perform deep hyper-representation to classify MNIST images by learning an entire LeNet network.
The corresponding bilevel problem is given by
min LOUt(λ) := IDIutI	∑	L(w*(λ)f(xi; λ),yi)
(xi ,yi )∈Dout
s.t.	w*(λ)= argmin Lm(w,λ):= ∣D-j	P	(L(Wf(Xi； λ),yi) + β ∣H∣2).
w∈Rc×p	n (xi,yi)∈Din
(13)
where f (xi； λ) ∈ Rp corresponds to features extracted from data point Xi, L(∙, ∙) is the Cross-
entropy loss function, c = 10 is the number of categories, and Din and DoUt are data used to
compute respectively inner and outer loss functions. Since the sizes of DoUt and Din are large in the
case of MNIST dataset, we apply the more efficient stochastic algorithm ESJ-S in Algorithm 2 with
a minibatch size B = 256 to estimate the inner and outer losses Lin and LoUt.
(a) Accuracy on outer data
(b) Outer loss value
Figure 2: Deep HR on the MNIST dataset.
(c) ESJ-S-N: N inner SGD steps
Figure 2 compares the classification accuracy on the outer dataset DoUt among the different methods.
Our algorithm ESJ-S converges with the fastest rate and attains the best accuracy with the lowest
variance among all algorithms. Note that ESJ-S is able to attain the same accuracy of 0.98+ obtained
by the standard training of all parameters with one-phased optimization on the MNIST dataset using
the same backbone network. All other methods fail to recover such a level of performance, and
instead saturate around an accuracy of 0.93. Further, Figure 2(c) indicates that the convergence of
ESJ-S does not change substantially with the number N of inner SGD steps. This demonstrates the
robustness of our method when applied to complex function geometries such as deep nets.
4.3 Few- S hot Meta-Learning over MiniImageNet
To study our algorithms over larger neural nets, we study the few-shot image recognition problem,
where classification tasks Ti, i = 1, ..., m are sampled over a distribution PT . In particular, we
consider the ANIL meta-learning method (Raghu et al., 2019; Ji et al., 2020a), where all tasks share
common embedding features parameterized by φ, and each task Ti has its task-specific parameter
wi for i = 1, ..., m. More specifically, we set φ to be the parameters of the convolutional part of a
deep CNN model (e.g., ResNet-12 network) and w includes the parameters of the last classification
layer. All model parameters (φ, w) are trained following a bilevel procedure. In the inner-loop,
the base learner of each task Ti fixes φ and minimizes its loss function over a training set Si to
8
Under review as a conference paper at ICLR 2022
obtain its adapted parameters w*. At the outer stage, the meta-learner computes the test loss for
each task T using the parameters (φ, w*) over a test set Di, and optimizes the parameters φ of the
common embedding function by minimizing the meta-objective Lmeta over all classification tasks.
The problem can be expressed as the following bilevel optimization
min Lmeta(φ, we*) :=/ Pm=I LDi 9, Wi)
φm
s.t.	泊* = argmin LadaPt(。,泊)：= 2Pm=I LSi(φ,w%),
we
(14)
where we collect all task-specific parameters into we = (w1, ..., wm) and the corresponding mini-
mizers into W* = (w*,…,Wm). The functions LSi (φ, Wi)= 高 Pζ∈Sa (L(Φ, Wi； Z) + R(Wi))
and LDi(O,Wi)=尚 Pξ∈DiL (φ, Wi* ; ξ ) correspond respectively to the training and test loss
functions for task Ti , with R a strongly-convex regularizer and L a classification loss function. In
our setting, since the task-specific parameters correspond to the weights of the last linear layer, the
inner-level objective LadaPt(φ, We) is strongly convex with respect to We = (W1, ..., Wm). We note
that the problem studied in Section 4.2 can be seen as single-task instances of the more general
multi-task learning problem in eq. (14). However, in contrast to the problem in Section 4.2, the sizes
of the datasets Di and Si are usually small in few-shot learning and full GD can be applied here.
Hence, we use ESJ (Algorithm 1) here. Also since the number m of tasks in few-shot classification
datasets is often very large, it is preferable to sample a minibatch of i.i.d. tasks by PT at each meta
(a) Test accuracy (ResNet-12) (b) Test accuracy (CNN4)
(c) Time to reach 69% (ResNet-12)
Figure 3: 5way-5shot image classification on the miniImageNet dataset on single GPU.
We conduct few-shot meta-learning on the miniImageNet dataset (Vinyals et al., 2016) using two
different backbone networks for feature extraction: ResNet-12 and CNN4 (Vinyals et al., 2016).
The dataset description and hyperparameter details can be found in Appendix C.4. We compare our
algorithm ESJ with four baseline methods for few-shot meta-learning MAML (Finn et al., 2017),
ANIL (Raghu et al., 2019), MetaOptNet (Lee et al., 2019), and ProtoNet (Snell et al., 2017). We
run their efficient Pytorch Lightning implementations available at the learn2learn repository (Arnold
et al., 2019).
Figure 3(a) and (b) show that our algorithm ESJ converges faster than the other baseline methods.
Also, Comparing Figure 3(a) and (b), the advantage of our method over the baselines MAML and
ANIL becomes more significant as the size of the network increases. Further, Figure 3(c) shows
that MetaOptNet did not reach 69% accuracy after 20 hours of training with ResNet-12 network. In
comparison, our ESJ is able to attain 69% in less than three hours, which is about 1.5 times less than
the time taken for ProtoNet to reach the same performance level. Both ESJ and ProtoNet saturate
around 70% accuracy after 10 hours of training.
5 Conclusion
In this paper, we propose a novel ES-based approach for bilevel optimization, which eliminates all
second-order computations in the gradient-based approaches. Compared to the existing ES-based
algorithm, our approach explores the analytical structure of the hypergradient, and hence leads to
much more efficient and accurate hypergradient estimation. Thus, our algorithm outperforms the
existing algorithms in the experiments, particularly in the high-dimensional applications. We also
characterize the convergence rate for our proposed algorithms and show that the polynomial-time
complexity can be achieved. We anticipate that our approach will be useful for accelerating bilevel
algorithms in various machine learning problems.
9
Under review as a conference paper at ICLR 2022
6 Reproducibility Checklist
To ensure reproducibility, we use the Machine Learning Reproducibility Checklist v2.0, Apr. 7
2020 (Pineau et al., 2021). An earlier version of this checklist (v1.2) was used for NeurIPS
2019 (Pineau et al., 2021).
• For all models and algorithms presented,
-	A clear description of the mathematical settings, algorithm, and/or model. We
clearly describe all of the settings, formulations, and algorithms in Section 2.
-	A clear explanation of any assumptions. All assumptions are stated in Section 3.1
and details are clearly explained in Section 3.1.
-	An analysis of the complexity (time, space, sample size) of any algorithm. We
provide the time/computational complexity analysis for our algorithms in Theorems 1
and 2.
•	For any theoretical claim,
-	A clear statement of the claim. A clear statement of theoretical claims are made in
Section 3.
-	A complete proof of the claim. The complete proofs of all claims are available in
Appendix E, Appendix F, and Appendix G.
•	For all datasets used, check if you include:
-	The relevant statistics, such as number of examples. We use widely adopted
datasets MNIST and miniImageNet in Section 4. The related statistics can be seen
at http://yann.lecun.com/exdb/mnist/.
-	The details of train/validation/test splits. We give this information in the Supple-
mentary Appendix C.
-	An explanation of any data that were excluded, and all pre-processing step. We
did not exclude any data or perform any pre-processing.
-	For new data collected,a complete description of the data collection process, such
as instructions to annotators and methods for quality control. We do not collect
or release new datasets.
• For all shared code related to this work, check if you include:
-	Training code. The training code is available in our code in supplementary material.
-	Evaluation code. The evaluation code is available in our code in supplementary ma-
terial.
-	(Pre-)trained model(s). We do not release any pre-trained models.
•	For all reported experimental results, check if you include:
-	The range of hyper-parameters considered, method to select the best hyper-
parameter configuration, and specification of all hyper-parameters used to gen-
erate results. We provide all details of the hyper-parameter tuning in Supplemen-
tary Appendix C.
-	A clear definition of the specific measure or statistics used to report results. We
use the classification accuracy on test-set and the loss on the train-set.
-	A description of results with central tendency (e.g. mean) & variation (e.g. error
bars). We do not report the mean and standard deviation for experiments.
-	The average runtime for each result, or estimated energy cost. We report the
running time of the algorithms in Section 4.
-	A description of the computing infrastructure used. All detailed descriptions are
presented in Section 4.
10
Under review as a conference paper at ICLR 2022
References
Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, and Ian Bunner. learn2learn, 2019.
https://github.com/learnables/learn2learn.
Juhan Bae and Roger B. Grosse. Delta-stn: Efficient bilevel optimization for neural networks using
structured response jacobians. CoRR, abs/2010.13514, 2020.
Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. In International Conference on Learning Representations (ICLR),
2018.
Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the
constraints. Operations Research,21(1):37-44,1973.
Tianyi Chen, Yuejiao Sun, and Wotao Yin. A single-timescale stochastic bilevel optimization
method. arXiv preprint arXiv:2102.04671, 2021.
Justin Domke. Generic methods for optimization-based modeling. International Conference on
Artificial Intelligence and Statistics (AISTATS), pp. 318-326, 2012.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proc. International Conference on Machine Learning (ICML), pp. 1126-1135,
2017.
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In International Conference on Machine Learning
(ICML), pp. 1165-1173, 2017.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference
on Machine Learning (ICML), pp. 1568-1577, 2018.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018.
Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison
Guo. On differentiating parameterized argmin and argmax problems with application to bi-level
optimization. arXiv preprint arXiv:1607.05447, 2016.
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration com-
plexity of hypergradient computation. International Conference on Machine Learning (ICML)),
2020.
Bin Gu, Guodong Liu, Yanfu Zhang, Xiang Geng, and Heng Huang. Optimizing large-scale hy-
perparameters via automated learning algorithm. CoRR, 2021. URL https://arxiv.org/
abs/2102.09026.
Zhishuai Guo and Tianbao Yang. Randomized stochastic variance-reduced methods for stochastic
bilevel optimization. arXiv preprint arXiv:2105.02266, 2021.
Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. On stochastic moving-average
estimators for non-convex optimization. arXiv preprint arXiv:2104.14840, 2021.
Nikolaus Hansen. The cma evolution strategy: a comparing review. Towards a new evolutionary
computation, pp. 75-102, 2006.
Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel
programming. SIAM Journal on Scientific and Statistical Computing, 13(5):1194-1217, 1992.
11
Under review as a conference paper at ICLR 2022
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020.
Kaiyi Ji and Yingbin Liang. Lower bounds and accelerated algorithms for bilevel optimization.
arXiv preprint arXiv:2102.03926, 2021.
Kaiyi Ji, Zhe Wang, Yi Zhou, and Yingbin Liang. Improved zeroth-order variance reduced algo-
rithms and analysis for nonconvex optimization. In International Conference on Machine Learn-
ing (ICML),pp. 3100-3109, 2019.
Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with
task-specific adaptation over partial parameter. In Advances in Neural Information Processing
Systems (NeurIPS), 2020a.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-step model-agnostic meta-learning: Convergence
and improved algorithms. arXiv preprint arXiv:2002.07836, 2020b.
Kayi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced
design. International Conference on Machine Learning (ICML)), 2021.
Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A
momentum-assisted single-timescale stochastic approximation algorithm for bilevel optimization.
arXiv e-prints, pp. arXiv-2102, 2021a.
Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A
near-optimal algorithm for stochastic bilevel optimization via double-momentum. arXiv preprint
arXiv:2102.07367, 2021b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations (ICLR), 2014.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10657-10665, 2019.
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, Xaq Yoon, KiJung Pitkow, Raquel Urtasun,
and Richard Zemel. Reviving and improving recurrent back-propagation. International Confer-
ence on Machine Learning (ICML)), 2018.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055, 2018a.
Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic first-order al-
gorithmic framework for bi-level programming beyond lower-level singleton. In International
Conference on Machine Learning (ICML), 2020a.
Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level opti-
mization for learning and vision from a unified perspective: A survey and beyond. arXiv preprint
arXiv:2101.11517, 2021a.
Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A value-function-based
interior-point method for non-convex bi-level optimization. In International Conference on Ma-
chine Learning (ICML), 2021b.
Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zeroth-
order stochastic variance reduction for nonconvex optimization. In Advances in Neural Informa-
tion Processing Systems (NeurIPS), pp. 3731-3741, 2018b.
12
Under review as a conference paper at ICLR 2022
Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Mingyi Hong,
and Una-May O’Reilly. Min-max optimization without gradients: Convergence and applications
to black-box evasion and poisoning attacks. In International Conference on Machine Learning
(ICML), pp. 6282-6293, 2020b.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. International Conference on Artificial Intelligence and Statistics (AIS-
TATS), pp. 1540-1552, 2020.
Matthew Mackay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger Grosse. Self-tuning
networks: Bilevel optimization of hyperparameters using structured best-response functions. In
International Conference on Learning Representations (ICLR), 2018.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning (ICML), pp.
2113-2122, 2015.
Gregory M Moore. Bilevel programming algorithms for machine learning model selection. Rensse-
laer Polytechnic Institute, 2010.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, pp. 527-566, 2017.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Con-
ference on Machine Learning (ICML), pp. 737-746, 2016.
Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer,
Florence d,Alche Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine
learning research. Journal of Machine Learning Research, 22:1-20, 2021.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of MAML. International Conference on Learning Rep-
resentations (ICLR), 2019.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with im-
plicit gradients. In Advances in Neural Information Processing Systems (NeurIPS), pp. 113-124,
2019.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-
Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision,
3(115):211-252, 2015.
Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation
for bilevel optimization. In International Conference on Artificial Intelligence and Statistics (AIS-
TATS), pp. 1723-1732, 2019.
Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended kuhn-tucker approach for linear bilevel
programming. Applied Mathematics and Computation, 162(1):51-63, 2005.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems (NIPS), 2017.
Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and Yunhao
Tang. Es-maml: Simple hessian-free meta learning. In International Conference on Learning
Representations (ICLR), 2019.
Konstantinos Varelas, Anne Auger, Dimo Brockhoff, Nikolaus Hansen, Ouassim Ait ElHara, Yann
Semet, Rami Kassab, and Frederic Barbaresco. A comparative study of large-scale variants of
cma-es. In International Conference on Parallel Problem Solving from Nature, pp. 3-15. Springer,
2018.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, and Daan Wierstra. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems (NIPS), 2016.
13
Under review as a conference paper at ICLR 2022
Tengyu Xu, Zhe Wang, Yingbin Liang, and H Vincent Poor. Gradient free minimax optimization:
Variance reduction and faster convergence. arXiv preprint arXiv:2006.09361, 2020.
Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. arXiv
preprint arXiv:2106.04692, 2021.
Miao Zhang, Steven Su, Shirui Pan, Xiaojun Chang, Ehsan Abbasnejad, and Reza Haffari.
idarts: Differentiable architecture search with stochastic implicit gradients. arXiv preprint
arXiv:2106.10784, 2021.
Daniel Zugner and StePhan Gunnemann. Adversarial attacks on graph neural networks via meta
learning. In International Conference on Learning Representations (ICLR), 2019.
14
Under review as a conference paper at ICLR 2022
Supplementary Material
A Stochastic Bilevel Optimizer Based on ES Method
In the following, we present the algorithm specification for our proposed stochastic bilevel optimizer
based on ES method, which we call ESJ-S.
Algorithm 2 Stochastic bilevel optimizer via ES Jacobians (ESJ-S)
1: Input: lower- and upper-level stepsizes α, β > 0, initializations x0 ∈ Rp and y0 ∈ Rd, inner
and outer iterations numbers K and N , and number of Gaussian vectors Q.
2 3 4 5 6 7 8 9 10 11 12 13 14	: for k = 0, 1, ..., K do :	Set Yk0 = y0 ,	Yk0,j = y0 , j = 1, ..., Q : Generate uk,j = N(0, I) ∈ Rp,	j = 1, ..., Q : for t = 1, 2, ..., N do :	Draw a sample batch St-1 Update Yk = YtT- αVyG(xfc,理-1; S-) :	for j = 1, ..., Q do UPdate Ykj= YkJ 1 - aVy G (χk + μuk,j ,Yk-1 ； St-I) :	end for : end for YN -YN Compute δj = k,j. k , j = 1,..., Q : Draw a sample batch DF Compute V Φ(χk) = Vx F (Xk ,YfN ； DF) + Q pQ=1 (δj, Vy F(Xk ,YfN ； DF )〉uk,j
15 16	:	Update xk+1 = xk - βVb Φ(xk) : end for
B Additional Related Work
In this section, we provide further related work on the applications of bilevel optimization and ES
methods.
Bilevel optimization applications. Bilevel optimization has been employed in various applications
such as few-shot meta-learning (Snell et al., 2017; Franceschi et al., 2018; Rajeswaran et al., 2019;
Zugner & Gunnemann, 2019; Ji et al., 2020a;b), hyperparameter optimization (Franceschi et al.,
2017; Mackay et al., 2018; Shaban et al., 2019), neural architecture search (Liu et al., 2018a; Zhang
et al., 2021), etc. This paper demonstrates the superior performance of the proposed ES-based
bilevel optimizer in meta-learning and hyperparameter optimization.
ES applications. Evolution Strategies (ES) method has been studied for more than two decades (see
review papers (Hansen, 2006; Varelas et al., 2018)). In particular, Nesterov & Spokoiny (2017) pro-
posed a simple and effective ES-gradient via Gaussian smoothing, which was further extended to the
stochastic setting by Ghadimi & Lan (2013). Such an ES technique has exhibited great effectiveness
in various applications including meta-reinforcement learning (Song et al., 2019), hyperparameter
optimization (Gu et al., 2021), adversarial machine learning (Ji et al., 2019; Liu et al., 2018b), min-
imax optimization (Liu et al., 2020b; Xu et al., 2020), etc. This paper proposes a novel ES-based
Jacobian estimator for accelerating bilevel optimization. The ES method has also been used for
studying hyperparameter optimization problems. For example, Mackay et al. (2018) models the
response function itself as a neural network (where each layer involves an affine transformation of
hyperparameters) using the Self-Tuning Networks (STNs). An improved and more stable version
of STNs was further proposed in Bae & Grosse (2020), which focused on accurately approximating
the response Jacobian rather than the response function itself.
15
Under review as a conference paper at ICLR 2022
C	Further Specifications for Experiments in Section 4
We note that the smoothing parameter μ (in Algorithms 1 and 2) was easy to set and a value of 0.1
or 0.01 yields a good starting point across all our experiments. The batch size Q (in Algorithms 1
and 2) is fixed to 1 (i.e., we use one Jacobian oracle) in all our experiments.
C.1 Specifications on Baseline Bilevel Approaches in Section 4.1
We compare our algorithm ESJ with the following baseline methods:
•	HOZOG (Gu et al., 2021): a hyperparameter optimization algorithm that uses evolution strate-
gies to estimate the entire hypergradient (both the direct and indirect component). We use our
own implementation for this method.
•	AID-CG (Grazzi et al., 2020; Rajeswaran et al., 2019): approximate implicit differentiation
with conjugate gradient. We use its implementation provided at https://github.com/
prolearner/hypertorch
•	AID-FP (Grazzi et al., 2020): approximate implicit differentiation with fixed-point. We exper-
imented with its implementation at the repositery https://github.com/prolearner/
hypertorch
•	ITD-R (REVERSE) (Franceschi et al., 2017): an iterative differentiation method that computes
hypergradients using reverse mode automatic differention (RMAD). We use its implementation
provided at https://github.com/prolearner/hypertorch.
C.2 Hyperparameters details for shallow HR experiments in Section 4.1
For the linear embedding case, We set the smoothing parameter μ to be 0.01 for ESJ and HOZOG.
We use the following hyperparameters for all compared methods. The number of inner GD steps
is fixed to N = 20 with the learning rate of α = 0.001. For the outer optimizer, we use Adam
(Kingma & Ba, 2014) with a learning rate of 0.05. The value of γ in eq. (12) is set to be 0.1. For
the two-layer net case, we use μ = 0.1 for ESJ and HOZOG. For all methods, we set N = 10,
α = 0.001, β = 0.001, and use Adam with a learning rate of 0.01 as the outer optimizer.
C.3 Specifications on Baseline Stochastic Algorithms in Section 4.2
We compare our stochastic algorithm ESJ-S with the following baseline stochastic bilevel algo-
rithms.
•	stocBiO: an approximate implicit differentiation method that uses Neumann Series to esti-
mate the Hessian inverse. We use its implementation available at https://github.com/
JunjieYang97/StocBio.
•	AID-CG-S and AID-FP-S: stochastic versions of AID-CG and AID-FP, respectively. We
use their implementations in the repository https://github.com/prolearner/
hypertorch.
C.4 Specifications for Few-shot Meta-Learning in Section 4.3
The miniImageNet dataset (Vinyals et al., 2016) is a large-scale benchmark for few-shot learning
generated from ImageNet (Russakovsky et al., 2015) Russakovsky. The dataset consists of 100
classes with each class containing 600 images of size 84 × 84. Following (Arnold et al., 2019), we
split the classes into 64 classes for meta-training, 16 classes for meta-validation, and 20 classes for
meta-testing. More specfically, we use 20000 tasks for meta-training, 600 tasks for meta-validation,
and 600 tasks for meta-testing. We normalize all image pixels by their means and standard deviations
over RGB channels and do not perform any additional data augmentation. At each meta-iteration,
we sample a batch of 16 training tasks and update the parameters based on these tasks. We set the
smoothness parameter to be μ = 0.1 and use N = 30 inner steps. We use SGD with a learning rate
ofα = 0.01 as inner optimizer and Adam with a learning rate ofβ = 0.01 as outer (meta) optimizer.
16
Under review as a conference paper at ICLR 2022
AUeJnUUe uo=ep=e>
0.50-
0.2	0.4	0.6	0.8	1.0	1.2
Running time
Figure 4: Classification results on 20 Newsgroup dataset. Left: number of inner GD step N = 5.
Right: number of inner GD steps N = 10. Running time is in seconds.
AUeJnUUe uo=ep=e>
HOZOG
—AID-CG
—AID-FP
-REVERSE
0.60
0.25	0.50	0.75
1.00	1.25	1.50	1.75	2.00	2.25
Running time
D Experiments on Hyperparameter Optimization
Hyperparameter optimization (HO) is the problem of finding the set of the best hyperparamters (ei-
ther representational or regularization parameters) that yield the optimal value of some criterion of
model quality (usually a validation loss on unseen data). HO can be posed as a bilevel optimization
problem in which the inner problem corresponds to finding the model parameters by minimizing a
training loss (usually regularized) for the given hyperparameters and then the outer problem mini-
mizes over the hyperparameters. Hence, HO can be mathematically expressed as follows
mλin Lval(X) = ∣D1aι∣ Pξ∈Dvai L (w*(λ); ξ)
s.t.	w*(λ) = arg min Ltr(w,λ):=点 Pζ∈Dtr (L(w,λ Z) + R(w,λ)),
wr
where L is a loss function (e.g., logistic loss), R(w, λ) is a regularizer, and Dtr and Dval are respec-
tively training and validation data. Note that the loss function used to identify hyperparameters must
be different from the one used to find model parameters; otherwise models with higher complexities
would be always favored. This is usually achieved in HO by using different data splits (here Dval
and Dtr) to compute validation and training losses, and by adding a regularizer term on the training
loss.
Following (Franceschi et al., 2017; Grazzi et al., 2020), we perform classification on the 20 News-
group dataset, where the classifier is modeled by an affine transformation, the cost function L is
the cross-entrpy loss, and R(w, λ) is a strongly-convex regularizer. We set one l2-regularization
hyperparameter for each weight in w, so that λ and w have the same size.
For ESJ and HOZOG, we use GD with a learning rate of 100 and a momentum of 0.9 to perform the
inner updates. The outer learning rate is set to be 0.02. We set the smoothing parameter (μ in Algo-
rithm 1) tobe 0.01. For AID-FP, AID-CG, and REVERSE we use the suggested hyperparameters
in their implementations accompanying the paper Grazzi et al. (2020).
It can be seen from Figure 4, our method ESJ slightly outperforms HOZOG and converges faster
than the other AID and ITD based approaches. We note that the similar performance for ESJ and
HOZOG can be explained by the fact that in HO, the hypergradient expession in eq. (6) contains
only the second term (the first term is zero), which is very close to the approximation in HOZOG
method. However, as we have seen in the other experiments (not for HO), ESJ is a more robust
and stable bilevel optimizer than HOZOG, and it achieves good performance across many bilevel
problems.
E	S upporting Technical Lemmas
In this section, we provide auxiliary lemmas that are used for proving the convergence results for
the algorithms ESJ and ESJ-S.
In the following proofs, We let L = max{Lg, Lf } and D be such that ∣∣y*(x)k ≤ D.
17
Under review as a conference paper at ICLR 2022
First we recall that for any two matrices A ∈ Rm×r and B ∈ Rr×n , we have the following upper-
bound on the Frobenius norm of their product,
ABF ≤ ABF.	(16)
The following lemma follows directly from the Lipschitz properties in Assumptions 2 and 3.
Lemma 1. Suppose that Assumptions 2 and 3 hold. Then, the stochastic derivatives VF(x, y; ξ),
VxVyG(x, y; ξ), and VyG(x, y; ξ) have bounded variances, i.e.,forany (x, y) and ξ we have:
•	Eξ VF(x, y; ξ) - Vf(x,y)2 ≤ M2;
•	EξVx Vy G(x, y; ξ) - Vx Vy g(x, y)2F ≤ L2;
•	EξV2y G(x, y; ξ) - V2y g(x, y)2F ≤ L2.
Using Lemma 2.2 in Ghadimi & Wang (2018), the following lemma characterizes the Lipschitz
property of the gradient of the total objective Φ(χ) = f (x, y* (x)).
Lemma 2. Suppose that Assumptions 1, 2, and 3 hold. Then, we have:
VΦ(x2) -VΦ(x1) ≤ LΦx2-x1	∀x1 ∈ Rp, x2 ∈Rp,
where the constant Lφ = L + 2L2+τM2 + PLM+l3+tml + PLM.
μg	μg	μ g
We next provide some essential properties of the ES-based (i.e., zeroth-order) gradient oracle in
eq. (4), due to Nesterov & Spokoiny (2017).
Lemma 3. Let h : Rn → R be a differentiable function with L-Lipsctitz gradient. Define its
Gaussian smooth approximation hμ(x) = Eu [h(x + μu)], where μ > 0 and U ∈ Rn is a standard
Gaussian random vector. Then, hμ is differentiable and we have:
• The gradient of hμ takes theform
Vhμ(x) = Eu h(x + μu)-h(X) U.
μ
•	For any x ∈ Rn,
..	..“2	…
∣∣Vhμ(x) -Vh(x)∣∣ ≤ μ2L(n + 3)3/2.
•	For any x ∈ Rn,
Eu∣∣ 心 + J(X)u∣∣2 ≤ 4(n + 4)∣∣Vhμ(x)∣∣2 +2μ2L2(n + 5)3.
Note the first item in Lemma 3 implies that the oracle in eq. (4) is in indeed an unbiased estimator
of the gradient of the smoothed function hμ.
Lemma 4. Suppose thatAssumptions 1 and 2 hold. The Jacobian J* = %F) has bounded norm:
∣∣j*∣∣f ≤ -.	(17)
μg
Proof of Lemma 4. From the first order optimality condition of y* (x), we have Vy g (x, y*(x)) = 0.
Hence, the Implicit Function Theorem implies:
J* = - V2y g (x,y*(x))-1 Vx Vy g (x, y*(x)) .	(18)
Taking norms and applying eq. (16) together with Assumptions 1 and 2 yield the desired result
∣∣ j*∣∣f ≤ ∣∣VXVyg(χ,y*(X)) ∣∣f∣∣ [Vyyg(χ,y*(X))]-11∣ ≤ —.	(19)
μg
□
18
Under review as a conference paper at ICLR 2022
Lemma 5. Suppose that AssumPtions 1 and 2 hold. The Jacobian JN = dk has bounded norm:
Il JNI∣F ≤ ~.
μg
(20)
Proof of Lemma 5. The inner loop gradient descent updates writes
yk = yk	- αVyg (xk, yk ɪ) ,	t = 1,..∙, N.
Taking derivatives w.r.t. xk yields
Jt = Jt-I- αVMg (xk, yk-1) - αJt-K'g 由,y『1)
=Jt-I (I — αvyg (xk,yk-1)) — aVxVyg (Xk,yk-1).
Telescoping over t from 1 to N yields
N-1	N-1	N-1
JN = Jo Y (I - aVyg (Xk,yk)) - α X VXVyg (Xk,yk) Y (I - ɑv2g (xk,ym))
t=0	t=0	m=t+1
N-1	N-1
=-α X VXVyg (xk,yk) Y (I- aVyg (χk,ym)).	QI)
t=0	m=t+1
Hence, we have
N-1	N-1
IlJNIIF ≤α XllVXVyg (xk,yk) ∣lFl∣ γ (I-aVyg(Xk,ym)) ι∣
t=0	m=t+1
(i)	N-1	N-1
≤ α L	llI - αV2yg(Xk,ykm) ll
t=0	m=t+1
(ii)	N-1
≤ αL X (1 - αμg)n-1-t
t=0
N-1	L
=ɑL): (1 - αμg F ≤ —
=	一μg
where (i) follows from Assumption 2 and (ii) applies the strong-convexity of function g(x, ∙). This
completes the proof.	□
Lemma 6. Suppose that Assumptions 1 and 2 hold. Then, the Jacobian in the stochastic algorithm
∂YN
ESJ-S Jn = ∂^k- has bounded norm, as shown below.
Il JNIIF ≤ g
μg
(22)
Proof of Lemma 6. The proof follows similarly to Lemma 5.
□
E.1 Proof of Proposition 1
Proposition 1 (Re-stated). Suppose that Assumptions 1 and 2 hold. Define the constant
LJ =(1 + -)(二 + pL) .	(23)
∖	μg) ∖μg	μg√
Then, the Jacobian JN(x) = X) is LJ-Lipschitz with respect to X under the Frobenious norm:
IIJN (X1) - JN (X2)IIF ≤ LJ IIX1 - X2II ∀X1 ∈ Rp, X2 ∈ Rp.	(24)
19
Under review as a conference paper at ICLR 2022
Proofof Proposition 1. Using eq. (21), we have
N-1	N-1
Jn(X) = -α X VxVyg (x,yt(x)) Y (I - aVyg (x,ym(x))), X ∈ Rp
t=0	m=t+1
Hence, for xι ∈ Rp and X2 ∈ Rp, we have
Il JN(xι) - Jn(x2)∣∣F
N-1	N-1
=α∣∣ X VxVyg (χ1,yt(χ1)) Y (I-aVyg(X1,ym(XI)))
t=0	m=t+1
N-1	N-1
-X VxVy g (X2,yt(X2 )) Y (I - aVy g (X2,ym(X2 ))) || 尸
t=0	m=t+1
N-1	N-1
≤α X ||VxVyg (X1,yt(X1)) Y (i - aVyg(X1,ym(XI)))
t=0	m=t+1
N-1
-VxVyg (X2,yt(X2)) Y (I- aVyg (X2,ym(X2))) llF
m=t+1
(i) N-
≤α E l l VxVyg (X1,yt(X1)) l l F l l At(X1) - AtS)〔1
t=0
N-1
+ α X llAt(X2)llllVxVyg (九城(XI)) — VxVyg (X2)yt(X2)) Hf
t=0
(25)
where we define At(X) = "；=：+1 (I - aVyg (x, ym(X))) and (i) follows from eq. (16).
Next we upper bound the quantity At(X1) - At(X2) , as shown below.
llAt(XI)- At(x2)H ≤llα (Vyg (x2,yt+1(x2)) - Vyg (x2,yt+1(x2))) At+1(x1)
+ (i - aVyg (x2, yt+1(x2))) (At+1(XI)- At+1(x2)) ll
≤α∣∣At+1(x1) l l l l Vyg(X1,yt+1(x1)) - Vyg (x2,yt+1(x2)) H
÷ llI - aVyg (x2, yt+1(x2)) ll ll At+1 (XI)- At+1(x2)ll
≤(1 - αμg) l l At+1(XI) - At+1(x2)l l
÷ αρ(l ÷----)(1 - αμg)N-t-2 llx1 - x2H,	(26)
μg
where the last inequality follows from Lemma 5 and Assumptions 1 and 2.
Telescoping eq. (26) over t yields
lAt(XI)- At(X2)H ≤(1 - αMg )N-t-2 l AN-2(XI)- AN-2(x2) H
N-t-3	T
÷ ^X aP(1 ÷-----)(1 - αμg)N t 2 m(1 - αμg)m ||x1 - x2 ll
m=0	μg
=(1 -αμg)n-t-2llV2g (x1,yN-1(x1)) -Vyg (x2,yN-1(x2)) H
N -t-3
÷ ^X QP(IH------)(1 - αμ3)N t 2∣∣x1 - x2H
m=0	μg
(i)	L
≤αρ(1 ÷----)(1 - αμ3)N t 2 l l x1 - x2 l l
μg
÷ (N — t — 2)ɑρ(1 ÷--)(1 — αμg)N t 2 ||x1 — X2ll
20
Under review as a conference paper at ICLR 2022
=αρ(1 +----)(N - t - I)(I - αμg)N - 2 ∣∣xι - χ2 ∣∣,	(27)
μg
where (i) follows from Lemma 5 and Assumption 2. Replacing eq. (27) in eq. (25) and using
Assumption 2, we have
∣JN(x1)-JN(x2)∣F
N -1	L
≤ α	Lαρ(1 +---)(N — t — 1)(1 — αμg)n-t-2∣∣xι — x21∣
t=0	μg
N -1	L
+ α ^X T(I+---)(1 - αμg)N - 1∣χι - x2∣∣
M	μg
≤ α2Lp(1 +—)∣∣xι-x2∣∣ ^X m(I - αμg)m-1 +—(1 +—)∣∣xι- x2∣∣
μg	m=0	μg	μg
≤ P2 (1+	~)∣∣x1- x2∣∣+ ɪ (1+	~)∣∣x1-	x2∣∣	(28)
μg	μg	μg	μg
where We use PN-1 mxm-1 ≤ 0^2 in eq. (28), which can be obtained by taking derivatives for
the expression PNm-=10 xm with respect to x. Hence, rearranging and using the definition of LJ in
eq. (23) finishes the proof.	□
Lemma 7. Suppose that Assumptions 1 and 2 hold. Define the constant
LJ=(1+k)(+Pf).
Then, the Jacobian JN(x) = dYdXx;" is LJ-Lipschitz with respect to X under the Frobenius norm:
∣∣ Jn(xι; ∙) -Jn(x2; ∙)∣∣F ≤ LJ∣∣xι — x2∣∣	∀xι ∈ Rp,x2 ∈ Rp.
ProofofLemma 7. The proof follows similarly to that for Proposition 1.	□
F Proofs for Deterministic B ilevel Optimization
For notation convenience, we define the following quantities:
f
分 分 /一 一、
JNj = JN (xk, uj ) 一
∖
JN
∂yN
∂xk
J*
∂yk
∂xk
(29)
where Uj ∈ Rp, j = 1,...,Q are standard Gaussian vectors. Let y^(xk) be the Gaussian smooth
approximation of yN(Xk). We collect yNμ(xk) for i = 1,...,d together as a vector y『(Xk), which
is the Gaussian approximation of the vector yN(Xk). If μ > 0, y/(Xk) is differentiable and we let
Jμ be the Jocobian given by
(30)
We approximate 舞^ using the average ES estimator given by JN = = Pj=I JN,j. The hypergra-
dient is then approximated as
v Φ(xk) = Vxf (Xk ,yN) + JNNyf(Xk ,yN)
1Q
=vxf (Xk ,yN)+ Q EJNjVyf (Xk ,yN).
Q j =1
(31)
21
Under review as a conference paper at ICLR 2022
Let δj = y (xk+μuj)-y (Xk), and let δi,j be the i-th component of δj. Hence, We have
(δι,jU> ʌ
?	_	δ2,j u>
JNj=	.	.
.
.
δd,juj>
JN,jNlyf (Xk,yk ) = ( δ1,juj δ2,juj	... δd,juj ) Nlyf (Xk,yk )
=<δj, Ny f (Xk, yk )〉uj.
Using eq. (31) and eq. (32), the estimator for the hypergradient can thus be computed as
1Q
N φ(xk) = Nxf(Xk,yN) + Q £〈%, Ny f(xk,yN )〉uj.
Q j=1
(32)
F.1 Proof of Proposition 2
Proposition 2 (Re-stated). Suppose that Assumptions 1, 2, and3 hold. Then, the expected estimation
error can be upper-bounded as follows:
2	L4	L2M2
ElNφ(χk) - Nφ(Xk)Il ≤2L D (I - αμg) + 4—D (I - αμg) + 24(4p + 15)	2
μg	Qμg
+ QLJM2dP4(p) + 24L2MJ；2- %)2N + 6μ2L2σM2d(p + 3)3
+ 48M2(τμ + Lρ)2 (1-α )N-1D2,	(33)
μg
where the expectation E[∙] is Conditioned on Xk and yN.
Proof of Proposition 2. Based on the definitions of NΦ(Xk) and Nb Φ(Xk) and conditioning on Xk
and ykN, We have
EIINbΦ(Xk) - NΦ(Xk)II2
≤2∣∣Nxf (Xk, yN) - Nxf(Xk, yk)∣∣2 + 2E∖∖JNNy f (x® , yNN) - J> Ny fg, yk)∣∣2
≤2L2∖∖yN - y1∖∖2 +4∣∣J*∣∣F∣∣Nyf (xk,yNN) - Nyf (xk,yk)∣∣2
+ 4E∣∣Jn -J*∣∣F∣∣Nyf(xk,yN)∣∣2
≤ 2L2D2 (I- a〃g )N+4 ⅜^∣∣yN - y⅛∣∣2+4M 2E∣∣ JN - JJIF
μg
(≤)2L2D2(1 - αμg)N + 4L24D2(1 - αμg)N + 4M2E∣∣ JN - J ∣∣F	(34)
Where (i) folloWs from Lemma 4 and Assumption 3, and (i) and (ii) also use the folloWing result
for full GD (When applied to a strongly-convex function).
∣∣yN-y"≤ (1-αμg)Nd2.
Next, we upper-bound the last term E∣∣JN -J=IIF at the last line of eq. (34). First note that
E∣∣Jn -J;∣∣F ≤ 3E∣∣Jn -Jμ∣∣F + 3∣∣JN -J*∣∣F + 3∣∣Jμ -Jn∣∣F.	(35)
22
Under review as a conference paper at ICLR 2022
We then upper-bound each term of the right hand side of eq. (35). For the first term, we have
Q
EJN-JμUF =Eu Q X JNj-RF
Q j=1
=Q Ell X (JNj- Jμ) IlF
Q	j=1
=Q2 E (χil JNj - JμIlF + 2 XDJN,i - Jμ, JNj-
Q	j=1	i<j
Q
=Q2 XEIIJNj-JμllF
Q j =1
=万EIIJNj-Jμll F, j ∈{1,..., Q}.
Q
We next upper-bound the term ElJNj -Ju 修 in eq. (36).
ElIJN,j-JμIlF =ElIJNjIlF -IlJμIlF
≤) X(4(p + 4) Il Vy黑Il2 + 3μ2Lj(P + 5)3)- £ Il VyNJ2
≤ X ((4p + 15)IIVyNU『+ 2μ2LJ(P +5)3),
(36)
(37)
(38)
where (i) follows by applying Lemma 3 to the components of vector yN (xk) which have Lipschitz
gradients by Proposition 1. Then, noting that 11 VyNμ 112'≤ 2ll VyN『+1 μ2LJ(p+3)3 and replacing
in eq. (38), we have
d
Ell JN,j - JμIlF ≤ X(2(4P +15)ll VyNII2 + μ2LJP4(p))
i=1
≤2(4p + 15)∣∣Jn∣∣F + μ2LJ dP4(p)
(i)	L2
≤2(4p + 15) μ + μ2LJ dp4(P),	(39)
where (i) follows from Lemma 5 and P4 is a polynomial of degree 4 in P. Combining eq. (36) and
eq. (38) yields
22
EIIJN - Jμ∣∣F ≤2(4p + 15) 75^^2 + TTLJ dP4 (P) .	(4O)
Qμg	Q
We next upper-bound the second term at the right hand side of eq. (35), which can be upper-bounded
using eq. (41) in Ji et al. (2021), as shown below.
Il Jn -J*∣∣2 ≤ 2L2(1-2αμg)2N + 4(τμg + LPY (ι - °〃g)n-1d2.	(41)
μ2	μg
We finally upper-bound the last term at the right hand side of eq. (35) using Lemma 3.
d
IlJμ-JNlF = X IlwN, -Il * * VyNIl2
i=1
2
≤ ^2^ LJ d(P + 3)3.	(42)
23
Under review as a conference paper at ICLR 2022
Substituting eq. (40), eq. (41) and eq. (42) into eq. (35) yields
EIIJN - J*∣∣F ≤6(4p + 15)QLμ2 + QLJdP4(P) + 6L (1 μJψμ)
+ 12(τμg + LLP (1 - aμg)n-1d2 + 3μ2LJd(p + 3)3.	(43)
μg	2
Finally, the bound for the expected estimation error in eq. (34) becomes
L4	L2M 2
e∣∣▽ φ(xk) - Vφ(xk) II ≤2L2D2(I - aμg)N + 4 —2D2(1 - αμg)N + 24(4p + 15)	?
μ2g	Qμ2g
μ2	24L2M2(1 - αμ )2N
+μrLj M 2dP4(p)+---------=_αμg-
Q	μg
+ 48M 2(τμ + Lρ)2	)N-1D2.
μg4
+ 6μ2L2JM2d(p+3)3
(44)
This completes the proof.
□
F.2 Proof of Theorem 1
Theorem 1 (Re-stated). Suppose that Assumptions 1, 2, and 3 hold. Choose the inner- and outer-
loop stepsizes respectively as α ≤ L and β = τ1-, where Lφ = L + 2L2+τMM + PLM+L2+TML +
L	4lΦ	μg	μ g
PL M. Then, the iterates Xk for k = 0,…，K 一 1 ofESJin Algorithm 1 satisfy:
μg
1 K-1
K X E∣∣vφ(χk)∣∣2≤
k=0
16Lφ(Φ(xo) — Φ*)
K
+ 3D
(45)
with Φ* = infχ Φ(x) and D is the upper-bound established in Proposition 2 and is g^ven by
L2M2
D =2L2D2(1 — αμg )N + 4	1 — αμg )N + 24(4p + 15) ——~~2-
Qμg
+ μ2LJM2dP4(p) + 24L2M2(12- 0μg)2N + 6μ2LjM2d(p + 3)3
Q	μg
+ 48M 2(τμg+LL)2	)N-1 D2.
μ4g
(46)
Proof of Theorem 1. Using the Lipschitzness of function Φ(xk), we have
Φ(xk+1) ≤Φ(xk) + hVΦ(xk),Xk+ι — Xki + ^2φ∣∣Xk+l — Xk∣∣2
≤φ(χk) — βhVΦ(χk), V Φ(χk )i + L β2∣∣v Φ(χk )∣∣2
≤Φ(xk) — βhVΦ(xk), V Φ(xk) — VΦ(xk )i — β∣∣VΦ(χk )∣∣2
+ Lφβ2 (∣∣VΦ(χk)∣∣2 + ∣∣VΦ(χk) — VΦ(χk)∣∣2)
≤Φ(xk) —	(2 —	Lφβ 2)∣∣VΦ(xk)∣∣2 +	(2+ Lφβ2)∣∣V Φ(xk) — VΦ(xk )∣∣2.	(47)
Let Ek [∙] =	Euk,i:Q [∙∣Xk	,yN ]	be the expectation over the Gaussian vectors uk,ι,..., Uk,Q	condi-
tioned on Xk and yN. Applying the expectation Ek [∙] to eq. (47) yields
Ek φ(xk+1) ≤φ(xk )	— (2	一 Lφβ2 )HV©Xk ) ∣∣	+(2 + Lφβ 2)Ek ∣∣V φ(xk) — V©xk	) ∣∣
≤Φ(xk)	— (β	— Lφβ2 )∣∣VΦ(xk )∣∣2	+ (2 + Lφβ 2)D,
(48)
24
Under review as a conference paper at ICLR 2022
where D represent the upper-bound established for the estimation error in Proposition 2. Now taking
total expectation over Uk = {u1,1:Q , . . . , uk,1:Q}, we have
Ek+1 ≤Ek - (2 - Lφβ2)EUk Il Vφ(xk)∣∣ +(2 + Lφβ2')D	(49)
where Ek = EUk-1 Φ(xk). Summing up the inequalities in eq. (49) for k = 0, . . . , K - 1 yields
Ek ≤E0 -(2 - LΦβ2) X EUk ∣Wφ(xk) ∣∣2 +(2 + Lφβ2)KD.
k=0
Setting β = 4l1^, denoting by Φ* = infχ Φ(x), and rearranging eq. (50), We have
K KXLEUk Wφ(χk)∣∣2 ≤ 16Lφ(φK0)- φ*) + 3D.
k=0
Hence, the proof is finished.
(50)
(51)
□
G Proofs for Stochastic B ilevel Optimization
Define the folloWing quantities
f
分	分 /一 一、
JN,j = JN (Xk,uj) 一
∂YkN
J N	n ,
∂xk
∂碇
∂Xk
J
Where uj ∈ Rp, j = 1, . . . , Q are standard Gaussian vectors and YkN is the output of SGD obtained
With the minibatches {S0, ..., SN-L}.
Conditioning on xk and YkN and taking expectation over uj yields
Y YIN (xk+μuj ；S)-YlN (xk；S) > \
μ	Uj
ʌ
Euj JN,j= Euj	.
l YN (xk+μuj;S)-YdN (xk；S) > I
μ	μ	Uj /
(v>Y£(xk； s)\
.
=.
.
∖ v>YNμ(χk; S) )
=Jμ(S)
where YN(χk； S) is the i-th component of vector YN(xk； S), which is the entry-wise Gaussian
smooth approximation of vector Y N (xk； S). Let Ek [∙] = EHxk ,YN ] = EDF ,皿玛 be the expecta-
tion over the Gaussian vectors and the sample minibatch DF conditioned on xk and YkN .
G. 1 Proof of Proposition 3
Proposition 3 (Re-stated). Suppose that Assumptions 1, 2, and 4 hold. Choose the inner-loop
StePsize as α =工;μ . Define the constants
CY =(1 - αμg) (1 - ɑμg + Y + γμ~^ , Cxy = α (α + Y(I - αμg) + α7) , Cy = 7Cxy
σ2	L2
γ =2(τ Cxy + P Cy) .- + 2W(Cxy + Cy ),	λ = 2(丁 Cxy + P Cy )D ,
μg LS	S
(52)
25
Under review as a conference paper at ICLR 2022
where Y is such that Y ≥ L+μ-. Then, we have:
Ng
ElIJN -JjIF ≤cv 与 +
11	11F Y μg
λ(L + μg )2(1 — αμg )CN 1	Γ
(L + μg)2(1 - αμg) - (L - μg)2	1 - CY
Proofof Proposition 3. Based on the SGD updates, we have
Yk = y；T- αVyG (跳,Yt-1; St-1) ,	t = 1,...,N
Taking the derivatives w.r.t. Xk yields
Jt =Jt-I- aVxVyG (χk, Yk 1; St-I) - αJt-ι^yg (Xk, Yk 1; St-I)
Jt -J =Jt-I- J - aVxVyG (xk, Yk 1; St-I) - αJt-1 VyG (xk, Yk 1; St-I)
+ α (VxVyg (Xk,碇)+ j≠v2g(Xk, yk))
=Jt-I-J - α (VxVyG (xk, Yt-1; St-1) - VxVyg (Xk,碇))
-α (Jt-I-J*)VyG(Xk, Yk1； St-I)
+ αJ (V2g (Xk,城)-vyG (XQ Ykt-1; St-1)).
Hence, using the triangle inequality, we have
(i)
IlJt -	JjIF	≤ Ii (Jt-I-J*)(I-	VyG	g,γk1；	St-I))	∣∣f
+	α ∣ ∣ VxVyG (Xk,Yt-1; St-1) -VxVyg (Xk,碇)∣∣F
+	α ∣ ∣ J (V2G (Xk,Yt-1; St-1) -Vyg (Xk,碇))||尸，
where (i) follows from Assumption 1. We then further have
∣∣"-J*∣∣F ≤
(1 - αμg )2|| Jt-1 - J*llF + α2 Il VxVy G (XQYt 1; St-1) - VxVyg (XQyN)MF
+ α2LjllVyG (xkMT; St-1) - vyg (Xk,成)∣∣F
+ 2α(1 - αμg) ∣∣ Jt-I- J*∣∣FIIVXVyG(Xk,^-1; St-ι) - VXVyg (Xk,成)∣∣F
V-----------------------------------V------------------------}
P1
+ 2α(1 - αμg) — ∣∣Jt-ι - J*∣∣FIIVyG(Xk,丫厂;St-ι) - Vyg(Xk, yk) ∣∣F
μg、▼/
P2
+2α2 — ∣∣Vy G(Xk ,Yk-1; St-1) -Vy g (Xk ,琼)∣∣FIIVXVyG(Xk,Yt「St-1) -VXVy g (Xk,yk)∣∣F .
μg、▼/
P3
The terms P1, P2 and P⅛ in the above inequality can be transformed as follows using the Peter-Paul
version of Young,s inequality.
P1 ≤21γ∣∣Jt-1 - J*∣∣F + 2∣∣VxVyG (Xk,Ykt-1;St-1) - VxVyg (Xk,求)||；, Y > 0
P2 ≤2γ∣∣Jt-1-j*∣∣f+2∣ιVyG(Xk,Yk-1;StT) -Vyg(Xk,碇)∣∣f, γ >0
P3 ≤ 2∣∣vy G (XkM-1; St-1) -vy g (Xk ,yk)∣∣F
+ 1 IIVxVy G (Xk ,Yt-1; St-1) - VxVy g (Xk, yk) ||F
Note that the trade-off constant Y controls the contraction coefficient (factor in front of ∣ ∣ Jt-1 -
/1∣F). Hence, we have
IIJt-J」|F
26
Under review as a conference paper at ICLR 2022
≤
((1 - 3Ng )2 + Y (1 - 3Ng) + γμ (1 - 3Ng)) IlJt-I - J* IIf
+ (a2 + αγ(1 - aμg) + α27) ∣∣ VxVyG (Xk,γAt-1; St-I) - VxVyg (Xk, y*) b
+ (α2μ + αγμ(I - αμg) + ɑ2T) ∣∣vyG (χfe, Yt-1; St-I) - Vyg (XMy*) 1F-
Let Et-ι[∙] = E[∙∖χk,Yt-1]. Conditioning on Xk and YtT and taking expectations yield
Eι∣∣jt -j*∣∣F ≤
Cγ∣∣Jt-ι -J*∣∣F + CxyEt-illVxVyG (XkX-1;St-ι) -VxVyg (xfc,y*)∣∣F
+ CyEt-i∣∣VyG (XMYt-1; St-I)-Vyg (Xfc,y*)∣∣F,	(53)
where CY, Cxy and Cy are defined as follows
CY = (1 — αμg) (1 — αμg + Y + 纱),Cxy = Q (Q + γ(1 — αμg) + Q7) , Cy
f
μg
xy ∙
Conditioning on Xk and Yt-i, we have
Et-11lVxVyG 由X-1; St-i) - VxVyg (x®, y*) ||；
≤2Et-ι∣∣VxVyg (x®,YtT) - VxVyg (Xfc,y*)∣∣F
+ 2Et-ι∣∣VxVyG (Xfc,Yt-1;St-ι) -VxVyg 由,Yt-i) ∣∣F
(i) Γ2
≤2LS- +2τ2∣/t-i - y*∣∣2,	(54)
where (i) follows from Lemma 1 and Assumption 2. Similarly we can derive
“C ，	2 2	、	2	..“2	L2	n H Xl . ll2
Et-11 I VyG 由,Yt-1; St-i) - Vyg (Xk,y*) ∣ ∣ F ≤27 + 2ρ2 11 Yt-1 - y* ∣ ∣ .	(55)
Combining eq. (53), eq. (54), and eq. (55) we obtain
Et-IIIJt -JJIF ≤cy11 Jt-I-J*llF + 2(T2Cxy + PCy)∣∣Yt-1 - y*∣∣2
L2 _	_
+ 2W(Cxy + Cy).	(56)
S
Unconditioning on X k and Y；-1 and taking total expectations of eq. (56) yield
e I I Jt - j*IIF ≤cyEIIJtT-J*iiF + 2(T2Cxy + ρ2Cy)EIIYt-1 - y*『+ 2Sr(Cxy + Cy)
S
ZCEII Jt-I-RF + 2(T2Cxy + P2Cy) ((S)2(T D2 + μg⅛)
l2
+ 2q(Cxy + Cy),
S
where (i) follows from the analysis of SGD for a strongly-convex function. Let Γ = 2(τ2Cxy +
P2Cy) J；S + 2L(Cxy + Cy) and λ = 2(τ2Cxy + ρ2Cy)D2. Then, we have
E I I Jt -J* IIF ≤Cγ EIIJt-1 -J*IIF + λ (L-)(	)+Γ.	(57)
∖l + μg∕
Telescoping eq. (57) over t from N down to 1 yields
N-1 /r	、2t	N-1
eII Jn -j*IIF ≤cNEHJO-J*IIF +λ X (Liμg)	CN-1-t + γ X CY	(58)
t=0 ' + μ"	t=0
27
Under review as a conference paper at ICLR 2022
which, in conjunction with (L+jg
2	/ r	.	L+μr, …，, T	♦一
≤ 1	一 αμg	and Y ≥ ],g	such that CY ≤ 1	一 αμg, yields
EIIJN -	J*IlF ≤cn-++λCNT	X ((L+(；-；"-	W))	+1 -C
μg	t = 0 ∖	(L + μg) (1 一	αμg) J 1 一 CY
≤c N L2 +	λL + μg)2(I — αμg)CN T	+ γ
Y μg	(L + μg)2(1 一 αμg) 一 (L 一 μg)2	1 一 CY
The proof is then complete.
Lemma 8. Suppose that Assumptions 1, 2, 3, and 4 hold. Set the inner-loop stepsize as α
Then, we have
ElIENΦ(xk) -VΦ(xk)∣∣2
(59)
□
2
L+μg .
<8M2 (CN L2	I λ(L + μg产(I- αμg)CN	1	+ γ , μ2 L2 d(0 +	3)3
≤8M fY 福	+(L + μg )2(1-αμ) 一(L 一	μ, )2 +K + T LJ d(p +	3)
+ 2L2 1 +
L -μg YN D2 + σ2 ]
L + μg)	+ μg LS ,,
(60)
where the expectation Ek [∙] is conditioned on Xk and YN.
Proof of Lemma 8. Conditioning on xk and YkN, we have
EkV Φ(xk ) = Vxf(Xk, YkN) + Jf> Vy f (Xk ,YfN).
Recall VΦ(xk) = Vxf(xk, yk*) + J*>Vyf(xk, yk*). Thus, we have
∣∣Ek V Φ(xk-VΦ(xk)∣∣2
≤2∣∣Vxf(xk,YN) - Vxf(Xk,yk)∣∣2 + 2∣∣J>Vyf(Xk,YN) - J;>Vyf(Xk,yk)∣∣2
≤2L2∣∣YN - yk∣∣2 +4∣∣J;>Vyf(Xk,YN) - J>Vyf(Xk,γlN)∣∣2
+ 4∣∣J:>Vyf(Xk,Yk) - J*>Vyf(Xk,y*)∣∣2
(i)	L2
≤2L2∣∣Yn —ykU2 +4M 2∣∣Jμ —J*UF +4 -2∣∣Vy f (Xk,YkN)—Vyf 由,y*)∣∣2
≤2L2∣∣YN - yk『+ 8M 2∣∣Jμ -外修 + 8M 2∣∣jn -j*∣∣F + 4 -2 ∣∣YkN - yk『.
where (i) applies Lemma 4 and Assumption 3. Taking expectation of the above inequality yields
E∣∣EkVΦ(Xk) - VΦ(Xk)∣∣2 ≤2L2(1 + 2L2) E∣∣YN - y*∣∣2 + 8M2E∣∣ Jn — J*UF
+ 8M 2E∣∣Jμ —JnHF .	(61)
Using the fact that YrN(Xk; ∙) has LiPSChitz gradient (Proposition 1) and Lemma 3, the last term at
the right hand side of eq. (61) can be directly upper-bounded as
d2
∣∣Jμ -JnUF = X∣∣VY^(Xk; S)-VYN(Xk；S)∣∣2 ≤ μLJd(p + 3)3,	(62)
i=1	2
where LJ is the Lipschitz constant of the Jacobian JN (and also of its rows VYiN (Xk; S)) as defined
in Proposition 1. Combining eq. (61), eq. (62), Proposition 3, and SGD analysis (as in eq. (60) in Ji
et al. (2021)) yields
E∣∣EkVbΦ(Xk) 一 VΦ(Xk)∣∣2
28
Under review as a conference paper at ICLR 2022
V8M 2 (C N L2 + λ(L + μg )2(1 - αμg )CN 1	I γ _l μ2 L d +3)3
≤8M (CY 尾 + (L + μg)2(1-αμg) - (L - μ,) + T-CY + ɪLJd(P + 3)
+ 2L2 1 +
L-μg YN D2 +
L + μg)
σ2
μg LSJ'
(63)
This finishes the proof.
□
G.2 Proof of Proposition 4
Proposition 4 (Re-stated). Suppose that Assumptions 1, 2, 3, and4 hold. Set the inner-loop stepsize
as α = -rv-. Then, we have:
L+μg	,
EwΦ(xk) - VΦ(xk)∣∣2 ≤ △ + B1	(64)
where δ = 8M2 ((1 + D1f ) 4p+15 + Df ) L +2MMf + (1 + D1f ) 4MM-μ2dLJp4(P) + 4Df μ2dLJP3(P)
and B1 respresents the upper bound established in Lemma 8.
Proof of Proposition 4. We have, conditioning on xk and YkN
EkwΦ(xk) - VΦ(xk)∣∣2 =EkWΦ(xk) - ENΦ(xk)∣∣2 + IlENΦ(xk) - VΦ(xk)∣∣2. (65)
Our next step is to upper-bound the first term in eq. (65).
Ek ∣∣VbΦ(xk)-EkVbΦ(xk)∣∣2
≤2Ek∣∣VχF (Xk ,YN; DF )-Vχf(xk,YN )∣∣2
+ 2Ek∣∣J>Vy F (Xk ,YN; DF)-J>Vyf (Xk ,YN )∣∣2
≤2 Dy+4EkIIVy F (Xk, γlN; Df )∣∣2∣∣1JN- Jμ∣∣F
+ 4Ek∣∣Jμ∣∣F∣∣VyF(xk,YN;DF) - Vyf(Xk,YN)∣∣2
≤2 D+4M2 (1+D) EkIIJN- Jμ∣∣F+4 Dτ∣∣ Jμ∣∣F,	(66)
where the last two steps follows from Lemma 1.
Next, We upper-bound the term Ek11JN - Jμ∣∣F.
EkIIJN -jμ∣∣F =万 EkIIJNj-Jμ∣∣ F, j ∈ {1,''.,Q}
Q
≤ Q (EkIIJN,j∣∣F- ∣∣Jμ∣∣F)
≤ɪXX(Ek∣∣YN(Xk+叫;S)-YN(Xk;S)Uj∣∣2-∣∣VYNμ(Xk；S)∣∣2). (67)
Recall that for a function h With L-Lipschitz gradient, We have
Eu∣ h(X +「(X)u∣∣2 ≤ 4(P + 4)∣∣Vhμ(X)∣∣2 + 3μ2L2(p + 5)3.	(68)
Then, applying eq. (68) to function 匕N(∙; S) yields
Euj ∣∣ YN(Xk + μuj ； S) - YN(Xk; S)uj∣∣2
≤ 4(P + 4)∣∣VYiμ(Xk； S)∣∣2 + 3μ2LJ(p + 5)3.
29
Under review as a conference paper at ICLR 2022
Hence, eq. (67) becomes
EkJN - Jμ∣∣2 ≤4p+15 XX IlVY›k; S)『+ 3μ2dLJ (P + 5)3
Q i=1	2Q
≤ 2(4pQ+152 XX Il VYN (Xk ； S )ll2 + μ2QJ P4(p)
Q	i=1	Q
≤ 空产 I∣Jn∣∣F + μdLJ P4 (P),	(69)
QQ
where P4 is a polynomial of degree 4 in p. Combining eq. (66), eq. (69) and Lemma 3 yields
Ek∣∣VΦ(xk)-EkVΦ(xk)∣∣2 ≤2M +4M2(l + D)(2(4pJ5)∣∣jN∣∣F + μ2QJP4(p))
+ -Dy (2IIJNIlF + μ2dLJP3(P))
≤ 8M2((1+D) 4p+5+D *+2 M
+ (1 + D) 4MM2 〃2北% P4(p) + 4M2 μ2dLJ P3 (p)
= ∆,	(70)
where △	=	SM2(。+ 应)4p+15 + Df) L2 + 2M +。+ D)萼"2dLJP，(P)+
4M μ2dLJ P3(p).
Taking total expectations of both eq. (65) and eq. (70) and combining, we have
E∣∣VbΦ(xk) -VΦ(xk)∣∣2=E∣∣VbΦ(xk) -EkVbΦ(xk)∣∣2+E∣∣EkVbΦ(xk) -VΦ(xk)∣∣2
≤∆+B1	(71)
where Bi representing the upper-bound established in eq. (63). The proof is then complete. □
G.3 Proof of Theorem 2
Theorem 2 (Re-stated). Suppose that Assumptions 1, 2, 3, and4 hold. Set the inner- and outer-loop
stepsizes respectively as ɑ =工二μ and β =工^, where L = max{ Lf, Lg} and the constant Lφ
is defined as in Theorem 1. Then, the iterates xk, k = 0, ..., K - 1 of the ESJ-S algorithm satisfy
-	K PK-01 E∣∣VΦ(xk)∣∣2 ≤ 16(φ(x嚎φ*)Lφ +3Bi +∆,	(72)
where ∆ and B1 have the following forms
δ =8M2 ((1 + D1f) 4pQ15 + D1f) L2+ 2M + (1 + D1f) 4MFdLJP4(p)
2
-	μ2 dLJ P3(p)
4m12l2
C N L2 λ(L + μg )2(1 - αμg )CN T	Γ μ2 L2 d(	, 3、3
Y	0 + (L + μg)2(1-αμg) - (L - μg)2 + T-C； + T J(P + )
L-μg )2N D2 + -2^ !
L + μg)	μg LS)
and the constants Γ, λ, and Cγ are defined in Proposition 3.
Proof of Theorem 2. Using the Lipschitzness of function Φ(xk), we have
Φ(xk+i) ≤Φ(xk) + hVΦ(χk),Xk+i — Xki + Lφ∣∣Xk+ι — Xk∣∣2
30
Under review as a conference paper at ICLR 2022
≤Φ(xk) - βhVΦ(xk), VΦ(xk)i + Lβ2∣∣VΦ(xk) - VΦ(xk) + VΦ(xk)∣∣2
≤Φ(Xk) - βhVΦ(Xk), VbΦ(Xk)i + Lφβ2 ∣∣VΦ(Xk)∣∣2+ ∣∣VbΦ(Xk) -VΦ(Xk)∣∣2
Hence, taking expectation over the above inequality yields
EΦ(Xk+1) ≤EΦ(Xk) -βEhVΦ(Xk),VbΦ(Xk)i+Lφβ2E∣∣VΦ(Xk)∣∣2
+ Lφβ2E∣∣VbΦ(Xk) - VΦ(Xk)∣∣2.
(73)
Also, We have
-EhVΦ(xk), VΦ(xk)i
= - EhVΦ(Xk), VbΦ(Xk) - VΦ(Xk)i -E∣∣VΦ(Xk)∣∣2
=E h-hVΦ(Xk),EkVbΦ(Xk) - VΦ(Xk)ii -E∣∣VΦ(Xk)∣∣2
≤E (1∣∣VΦ(xk)∣∣2 + 2∣∣EkVΦ(xk) - VΦ(xk)∣∣2) - E∣∣VΦ(xk)∣∣2
=1 E∣∣EkVΦ(xk) - VΦ(xk)∣∣2 - 2E∣∣VΦ(xk)∣∣2,
Which, in conjunction With eq. (73), yields
EΦ(xk+ι) ≤EΦ(xk) + 2EIlEkVΦ(xk) - VΦ(xk)∣∣2
+ Lφβ2EVbΦ(Xk) -VΦ(Xk)2.
2 -Lφβ2) E∣∣VΦ(xk)∣∣2
(74)
—
Setting β = 4l1^ and using the bounds established in Lemma 8 and Proposition 4, We have
β
Eφ(Xk+1) ≤eφ(Xk) + 2BI-
β
Summing up the above inequality over k from k = 0 to k = K - 1 yields
EΦ(xk) ≤EΦ(xo) + βKBi - β X E∣∣VΦ(xk)∣∣2 + βK(Bι + ∆).
k=0
Rearranging the above inequality yields
1 K-1
KK X EIIVφ(χk)||2
k=0
≤ 4(EΦ(xo) - EΦ(xk))
βK
≤16 (φ(XO)- φ*) LΦ
+ 3B1 + ∆
+ 3B1 + ∆.
K
The proof is then complete.
□
31