Under review as a conference paper at ICLR 2022
Hypergraph Convolutional Networks via
Equivalency between Hypergraphs and Undi-
rected Graphs
Anonymous authors
Paper under double-blind review
Ab stract
As a powerful tool for modeling the complex relationships, hypergraphs are gain-
ing popularity from the graph learning community. However, commonly used
algorithms in deep hypergraph learning were not specifically designed for hy-
pergraphs with edge-dependent vertex weights (EDVWs). To fill this gap, we
build the equivalency condition between EDVW-hypergraphs and undirected sim-
ple graphs, which enables utilizing existing undirected graph neural networks as
subroutines to learn high-order interactions induced by EDVWs of hypergraphs.
Specifically, we define a generalized hypergraph with vertex weights by proposing
a unified random walk framework, under which we present the equivalency condi-
tion between generalized hypergraphs and undigraphs. Guided by the equivalency
results, we propose a Generalized Hypergraph Convolutional Network (GHCN)
architecture for deep hypergraph learning. Furthermore, to improve the long-
range interactions and alleviate the over-smoothing issue, we further propose the
Simple Hypergraph Spectral Convolution (SHSC) model by constructing the Dis-
counted Markov Diffusion Kernel from our random walk framework. Extensive
experiments from various domains including social network analysis, visual ob-
jective classification, and protein fold classification demonstrate that the proposed
approaches outperform state-of-the-art spectral methods with a large margin.
1 Introduction
Hypergraphs, whose edges link to the arbitrary number of vertices to model high-order relation-
ships, attract much attention recently from researchers in the graph learning community (Feng et al.,
2019; Jiang et al., 2019; Zhang et al., 2021). The topology of hypergraph can be considered to
be embedded in vertex weights, which represents the connection strength between hyperedges and
vertices (Chitra & Raphael, 2019). The vertex weights can be divided into two categories, edge-
independent and edge-dependent (Chitra & Raphael, 2019; Hayashi et al., 2020), depending on
whether the vertex weights are related to the incident hyperedges or not (EDVW-hypergraph implies
that each vertex v is assigned a weight qe(v) ∈ R for each incident hyperedge e). Many hypergraph
neural networks have been proposed (Feng et al., 2019; Jiang et al., 2019; Yadati, 2020; Huang
& Yang, 2021) to handle hypergraphs with edge-independent vertex weights (EIVW-hypergraph).
However, rigorous study for hypergraphs with edge-dependent vertex weights (EDVW-hypergraph)
is still lacked, though they enjoy stronger expressive power (Ding & Yilmaz, 2010; Huang et al.,
2010; Li et al., 2018a; Zhang et al., 2018).
In this work, we are devoted to spectral-based methods, which are considered to be robust and to
allow for simple model property analysis (Wu et al., 2020). Unfortunately, the spectral theory of
EDVW-hypergraph has not been well established. A recent work (Chitra & Raphael, 2019) provides
the spectral theory for EDVW-hypergraphs, and indicates that the defined EDVW-hypergraphs is
equivalent to digraphs. Though digraph convolutional networks can be directly used for EDVW-
hypergraphs, most existing convolutional algorithms for digraphs are more or less related to undi-
rected graphs, e.g., (Monti et al., 2018; Li et al., 2020; Maet al., 2019; Tong et al., 2020). Essentially,
they transform digraphs to undigraphs with various techniques due to the challenges of directed
graph Laplacian. In addition, key theoretical analysis of digraph convolutional networks properties,
1
Under review as a conference paper at ICLR 2022
Figure 1: An example of hypergraph and its equivalent weighted undirected graph (WC = (WC)>), where
qi(∙) = Qi(∙,e),i ∈ {1, 2}. Here, Qi = Q2 are both edge-dependent. The characteristics of the hypergraph
are encoded in the weighted incidence matrices (i.e. vertex weights) Qi, Q2. WC := Q2D-1 Q> denotes
the edge-weight matrix of the clique graph and can be viewed as the embedding of high-order relationships.
4 O-
10	6
7	3
3	3.
such as expressive power (XU et al., 2019) or the performance of models in deep layers (Li et al.,
2018b), yet remains open.
An arguably more succinct way for EDVW-hypergraph learning is to use undigraph convolutional
networks. Compared with digraphs, the undigraph convolutional networks have been extensively
studied and there exist various effective strategies for analyzing their properties, to name a few (Kipf
& Welling, 2017; Chen et al., 2020; Zhu & Koniusz, 2021). Furthermore, one can take established
techniques on undigraphs as a subroutine for hypergraph learning, which would largely ease hyper-
graph learning in real-world scenarios. It remains unclear, however, whether EDVW-hypergraphs
can be made equivalent to undigraphs in a principled way, thereby hindering this promising prospect.
In this paper, we focus on constructing the equivalency condition between hypergraphs and undi-
graphs. This makes it possible to utilize undigraphs as low-order encoders of hypergraphs, and thus
enabling hypergraph learning directly from the equivalent undirected graphs by means of undigraph
convolutional networks. Along this route, we have made the following contributions:
1)	We define a generalized hypergraph capturing EDVWs via designing a two-step unified random
walk framework. In addition, we construct the equivalency conditions to fill the gap between the
generalized hypergraph and corresponding undigraph.
2)	We present a unified Laplacian from the equivalency conditions and propose two spectral con-
volution models (GHCN, SHSC) for EDVW-hypergraphs learning. We analyze the over-smoothing
issues (Li et al., 2018b) based on their spectral properties deduced from equivalency conditions.
3)	Extensive experiments across various domains (social network analysis, visual objective classi-
fication and protein modeling) demonstrate the effectiveness of the proposed methods (GHCN and
SHSC) for both EIVW-hypergraph and EDVW-hypergraph learning tasks. Notably, we are first to
adopt EDVW-hypergraphs for protein structures and obtain significant performance boost.
Due to space limit, more discussions on related works are deferred to Appendix A.
2 The Theory of Equivalency
Figure 2: Hypergraph Random
Walk. ① denotes the first step
and ② represents the second.
Notations. I ∈ RnXn denotes the identity matrix, and 1 ∈ Rn
represents the vector with ones in each component. We use boldface
letter X ∈ Rn to indicate an n-dimensional vector, where x(i) is the
ith entry of x. We use a boldface capital letter A ∈ Rmxn to denote
an m by n matrix and use A(i,j) to denote its ijth entry.
Let G (V, E, ω) be a graph with vertex set V, edge set E, and edge
weights ω. Let H(V, E, W, Q) be the hypergraph with vertex
set V, edge set E. Let w(e) denote the weight of hyperedge e.
W ∈ RlEl×lEl is the edge-weights diagonal matrix with entries
W(e, e) = W(e). Q denotes edge-vertex-weights matrix with en-
tries Q(u, e) = qe(U) ∈ R if U ∈ e and 0 if u ∈ e. Q is said to
be edge-independent if qe (u) = q(u) ∈ R for all e 3 u, and called
edge-dependent otherwise (Chitra & Raphael, 2019). If qe (u) equals to 1 for all linked U and e ,Q
would reduce to the binary incident matrix H, which is widely used to represent the structure of
hypergraph (Zhou et al., 2006; Carletti et al., 2021). The matrix Q can also be viewed as a weighted
incident matrix. Note that a graph is a special case of a hypergraph with hyperedge degree |e| = 2.
We say a hypergraph is connected when its unweighted clique graph (Chitra & Raphael, 2019) is
2
Under review as a conference paper at ICLR 2022
connected and we assume hypergraphs are connected. Throughout this paper, equivalency refers to
equivalency between hypergraphs and undigraphs if not otherwise specified.
2.1	Unified Random Walk and Generalized Hypergraphs
Traditionally, hypergraph random walk is defined by a two-step manner (Zhou et al., 2006; Ducour-
nau & Bretto, 2014) (Fig. 2). Then, Chitra & Raphael (2019) raise a new random walk involving
the edge-dependent vertex weights into the second step and build the equivalency between EDVW-
hypergraphs and digraphs. However, due to the limitation of their hypergraph definition, they fail
to answer whether the EDVW-hypergraphs can be equal to undigraphs. So in this part, we integrate
the existing two-step random walk methods (Chitra & Raphael, 2019; Carletti et al., 2020; 2021) to
obtain a comprehensive unified random walk framework with the vertex weights added into the first
step, based on which we further define a generalized hypergraph.
Definition 1 (Unified Random Walk on Hypergraphs) The unified random walk on a hypergraph
H is defined in a two-step manner: Given the current vertex u,
Step I: choose an arbitrary hyperedge e incident to u, with the probability
=	w(e) Σv∈V Q2(V, e)ρ(∑v∈V Q2(V, e))Q1(u, e)
p1 = Pe∈E w(e) Pv∈v Q2(v, e)ρ(Pv∈v Q2(v, e))Qι(u, e);
Step II: choose an arbitrary vertex v from e, with the probability
Q2 (v,e)
Pv∈V Q2(v,e) ,
(1)
(2)
where w(e) denotes the hyperedge weight; Q1(u, e) denotes the contribution of vertex u to hyper-
edge e in the first step while Q2 (v, e) represents the contribution of vertex v to hyperedge e in the
second step; and ρ(∙) is a real-valuedfunction, for example, a power function (∙)σ.
Suppose δ(e) :=	v∈V Q2(v, e) is the degree of hyperedge, and d(v) :=
Pe∈E w(e)δ(e)ρ(δ(e))Q1 (v, e) is the degree of vertex v. The transition probability of our
unified random walk on a hypergraph from vertex u to vertex v is:
P(u, v) =
e∈E
w(e)Q1(u, e)Q2 (v, e)ρ(δ(e))
d(u)
(3)
Here, P(u, v) can be written in a |V| × |V| matrix form: P = Dv-1Q1Wρ(De)Q2>, where Dv,
De are diagonal matrices with entries Dv(v, v) = d(v) and De (e, e) = δ(e), respectively. ρ(De)
represents the function ρ acting on each element of De . Notably, the ρ does not always work
properly. We present the failure conditions in the Appendix G.4. More detailed intuition of our
purpose to design P (u, v) can be found in Appendix B.2.
Under our framework, the existing hypergraph random walk (Zhou et al., 2006; Chitra & Raphael,
2019; Carletti et al., 2020; 2021) can be seen as special cases with specific p1, p2 (see Appendix B.3).
The definition of our framework is lazy since it allows self-loops (P (v, v) > 0). The non-lazy
version is provided in Appendix C. Based on the framework, we define the generalized hypergraph
for constructing equivalency between EDVW-hypergraphs and undigraphs as follows.
Definition 2 (Generalized hypergraph) A generalized hypergraph is a hypergraph associated
with the unified random walk in Definition 1, denoted as H(V, E, W, Q1, Q2). Here, Q1, Q2 are
vertex weights matrices, each of which can be edge-independent or edge-dependent.
2.2	The Equivalency Between Hypergraphs and undigraphs
In this part, we provide the condition under which the generalized hypergraph is equivalent to
weighted undirected clique graph. By establishing the equivalency, we can view graphs as low-
order encoders of EDVW-hypergraphs, so any undigraph technique can be used as a subroutine for
hypergraph learning.
The clique graph of H(V, E, W, Q1, Q2) is denoted as GC, which is a unweighted graph with
vertices V and edge set {(u, v) : u, v ∈ e, e ∈ E}. Actually, GC turns all hyperedges into cliques.
We first present the definition of the equivalency in connection with Markov chains.
3
Under review as a conference paper at ICLR 2022
Definition 3 (Chitra & Raphael (2019)) Let M1 ,M2 be the Markov chains with the same finite
state space, and let PMI and PM2 be their probability transition matrices, respectively. M1, M2
are equivalent if PMv = PMv for all states U and v.
The equivalency between digraph and hypergraph is
straightforward and we defer to Appendix E.On the other
hand, it is easy to understand that any simple undigraph can
be constructed as a special generalized hypergraph, while
the converse is not necessarily true. We provide a coun-
terexample in Appendix G.4. This result implies that undi-
graph convolutional networks can not trivially be extended
to generalized hypergraphs. So, we give one of the implicit
equations for formulating the equivalency problem to ex-
plore the explicit equivalency conditions.
LemmaI Let H(V, E, W, Q1, Q2) denote the general-
ized hypergraph in Definition 2. Let F(q1 ,q2) (u,v):=
Pe∈Ew(e)ρ(δ(e))Q1(u,e)Q2(v,e) and T(Q)(u) :=
e∈E w(e)δ(e)ρ(δ(e))Q(u, e). When Q1, Q2 satisfies the
following equation
Edge-independent ∩ (QI = kQ2)	Edge-independent ∩ (QI / kQ2)
Edge-dependent	Edge-dependent
∩ (QI = kQ2)	∩ (QI + kQ2)
	
Figure 3: The division of Equivalency
problem (undigraph). The blue box indi-
cates condition (1) in Thm. 1 and red box
indicates condition (2). The dark blue part
is still open yet.
T(Q2) (U)T(Qι) (V)F(QI,Q2) (U,v) = T(Q2) (V)T(Qι) (U)F(QI,Q2) (V, U), ∀u,v ∈ V	(4)
there exists a weighted undirected clique graph G C such that a random walk on H is equivalent to a
random walk on GC with edge weights ω(U, V) = T(Q2) (U)F(Q1,Q2) (U, V)/T(Q1) (U) if T(Q1) (U) 6=
0 and 0 otherwise.
The proof can be found in Appendix G.2. Next, we derive the explicit condition under which random
walks on hypergraphs are equivalent to undigraphs.
Theorem 1 (Equivalency between generalized hypergraph and weighted undigraph) Let
H(V, E, W, Q1, Q2) denote the generalized hypergraph in Definition 2. When H satisfies any of
the condition bellow:
Condition (1) Q1 and Q2 are both edge-independent; Condition (2) Q1 = kQ2 (k ∈ R),
there exists a weighted undirected clique graph GC such that a random walk on H is equivalent to
a random walk on G C.
It is easy to verify that conditions (1) and (2) satisfy equation 4 (proof in Appendix G.3). This
theorem brings insights from three aspects: i) Hypergraph foundation. The equivalency itself is a
fundamental problem and remains open before this work (Agarwal et al., 2006; Chitra & Raphael,
2019). Our conclusion provides more adaptive and explores more essential conditions thanks to the
unity of our random walk and the generalization of hypergraph defined (Figure 3). Notably, Chitra
& Raphael (2019) shows that the equivalency condition is: Q1 = H and Q2 is edge-independent,
which can be viewed as a special case of the condition (1), while Theorem 1 implies the equiva-
lency is not only related to the dependent relationships between vertex weights and edges, but also
to whether the vertex weights used in the first and second step of the unified random walk are pro-
portional. Furthermore, thanks to the introduced Q1 in the generalized hypergraph, we can obtain
the equivalency between EDVW-hypergraph and undigraph (i.e. condition (2) ), filling an important
gap in the equivalency theory of EDVW-hypergraph. ii) Providing a theoretical basis for hyper-
graph applications. The condition (2), containing both the edge-independent vertex weights and
edge-dependent vertex weights cases which match different hypergraph applications (Ding & Yil-
maz, 2010; Zeng et al., 2016; Zhang et al., 2018), provides those with theoretical explanations. The
condition (2) also reveals that the existing random walks (Carletti et al., 2020; 2021) on hypergraphs
are equivalent to undigraph. iii) Hypergraph learning. The equivalent undigraphs can be viewed as
the lower-order encoders of hypergraphs. Thus, one can obtain hypergraph representations directly
by exploiting the undigraph learning methods. Especially, the condition (2), which implies that an
EDVW-hypergraph can be equivalent to an undigraph, gives the theoretical guarantee for learning
hypergraphs without losing edge-dependent vertex weights via undigraph neural networks. Next,
we provide the spectral theory for the generalized hypergraphs.
4
Under review as a conference paper at ICLR 2022
2.3 Generalized Hypergraph Laplacian and Its Spectral Properties
To build a bridge between spectral convolution and equivalency, we derive the equivalent undirected
weighted graph Laplacian based on Lemma 1 and Thm 1. Then the graph Laplacian can be consid-
ered as the Laplacian of generalized hypergraph. Starting from the deduced Laplacian, it is direct
and convenient to construct the spectral convolution for hypergraphs. Formally, we have
Corollary 1 Let H(V, E, W, Qi, Q2) be the generalized hypergraph in Definition 2. Let DV be a
|V| × |V| diagonal matrix with entries Dv (v, v) := d(v) := e∈E w(e)δ(e)ρ(δ(e))Q2(v, e). No
matter H satisfies condition (1) or condition (2) in Thm. 1, it obtains the unified explicit form of
stationary distribution π and Laplacian matrix L as:
π
1>D V
1> D V1
and L = I - D-1/2Q2 WP(De)Q>D-1/2.
(5)
There are two observations from Corollary 1: (i) This stationary distribution is different from the
classical π(v) = d(v)/PV d(v) (Zhou et al., 2006), which depends on the vertex degree d(v). Our
π(v) is related to d(v), which means that we cannot trivially extend from classical theory. The de-
tailed proof is deferred to Appendix G.6; (ii) Both π and L are only related to Q2, independent of
Q1, implying possible information loss of Q1 under the setting required an asymmetrical two-steps
random walk(i.e. Q1 6= Q2) to construct a hypergraph. Meanwhile, L can be viewed as a normal-
ization Laplacian led by the equivalent undigraph GC with adjacency matrix Q2WP(De)Q2>, based
on which we can design the EDVW-hypergraph spectral convolutions.
Corollary 1 implies that no matter Q1, Q2 are both edge-independent or Q1 = Q2, we can directly
use the Laplacian matrix L to analyze the spectral properties. The spectral properties of Lapla-
cian (Chung & Graham, 1997) are vital in the research of graph neural networks to design a stable
and effective convolution operator. Therefore, we deduce two important conclusions concerning
eigenvalues of L as the basic theory of Laplacian application under the equivalency conditions in
Thm. 1. One claims the eigenvalues range of L is [0, 2] (details in Appendix G.7), and the other
describes the rate of convergence of our unified random walk listed bellow (see Appendix G.8)
Corollary 2 Let H(V, E, W, Q1, Q2) be the generalized hypergraph in Definition 1. When H satis-
fies any of two conditions in Thm. 1, let L andπ be the hypergraph Laplacian matrix and stationary
distribution from Corollary 1. Let λH denote the smallest nonzero eigenvalue of L. Assume an
initial distribution f with f(i) = 1 (f(j) = 0, ∀ j 6= i) which means the corresponding walk starts
from vertex vi. Let p(k) = fPk be the probability distribution after k steps unified random walk
where P denotes the transition matrix, then p(k) (j) denotes the probability of finding the walker in
vertex vj after k steps. We have:
∖p(k)(j) - ∏(j)∖≤ ｛符-λH)k.
(6)
For hypergraph learning, Corollary 2 implies that the spectral convolution deduced from the Lapla-
cian in Corollary 1 might suffer from the over-smoothing issue (Zhu & Koniusz, 2021). Appendix
F.4 gives a theoretical perspective for analyzing the over-smoothing issue based on this Corollary,
which further inspires the design of convolutions in Section 3 to alleviate the issue.
3 Two S pectral Convolutions for Hypergraphs
Intuitively, there are two possible routes for designing EDVW-hypergraph neural networks: 1. mes-
sage passing; 2. spectral-based methods. Message passing has proved to be a powerful tool for
extracting information from hypergraphs or graphs. However, on the one hand, researchers usually
adopt heuristic ideas to design message-passing models, leading to a lack of theoretical guaran-
tees (Huang & Yang, 2021). This makes it difficult to analyze properties of corresponding neural
networks directly, such as over-smoothing issue (Li et al., 2018b). On the other hand, spectral-based
methods not only have a solid foundation in graph signal processing (Kipf & Welling, 2017; Feng
et al., 2019), but also effectively inspires the design of message passing techniques (Xu et al., 2019).
5
Under review as a conference paper at ICLR 2022
Here, we design two hypergraph spectral convolutions for hypergraph learning based on equivalency
conditions in Thm. 1. As the Laplacian L enjoys the same formula under any of the two equiva-
lency conditions, algorithms deduce by L would be adaptive to any generalized hypergraph satisfied
Thm. 1, including EIVW-hypergraph and EDVW-hypergraph.
3.1	Generalized Spectral Hypergraph Convolution
We denote K as Q2Wρ(De)Q2> for expression simplicity. Then the unified hypergraph Lapla-
Cian matrix L in Corollary 1 can be expressed as L = I - DD-1/2KD-1/2. Similar to Defferrard
et al. (2016), we approximate the convolutional kernel with Chebyshev polynomial to avoid eigen-
value decomposition of L. Finally, the following Generalized Hypergraph Convolutional Network
(GHCN) is obtained by introducing the re-normalization technique (Kipf & Welling, 2017):
X(I+1) = ψ(T X(I)Θ),	(7)
-1/2	-1/2
where T := Dv KDv , and K = K + I can be regarded as the weighted adjacency matrix of
the equivalent undigraph with self-loops, and D(v, v) = u∈V K(v, u). Θ is a learnable parameter
matrix and ψ(∙) denotes an activation function. More details of derivation can be found in Appendix
F.1. Most of existing hypergraph Laplacians (Zhou et al., 2006; Carletti et al., 2020; 2021) can be
viewed as special forms of the Laplacian matrix L and can be derived the special convolutions of
GHCN. The biggest advantage of GHCN is that it can handle both EIVW-hypergraphs and EDVW-
hypergrpahs. Besides, the unity of random walk makes it possible to aggregate more fine-grained
information. Note that HGNN (Feng et al., 2019) can be viewed as a specific case of GHCN without
the re-normalization trick (ρ(∙) = (∙)-1, Q = H).
However, Corollary 2 indicates that initial signals or features intend to converge to a certain vector,
causing vertices hard to distinguish between each other. This implies that GHCN also suffers from
over-smoothing issue (Li et al., 2018b), which limits the depth of the convolution model and hinders
the aggregation of long-range information. In fact, the analysis also suggests that HGNN (Feng et al.,
2019) suffers from the over-smoothing issue. Next, we would propose a new spectral convolution to
alleviate this issue.
3.2	A Simple Spectral Convolution Based on Markov Diffusion Kernel
In order to aggregate long-range information, we propose the discounted Markov Diffusion process
to aggregate the information of vertices and propose a new convolutional architecture for hyper-
graphs.
Discounted Markov Diffusion Kernel for Hypergraphs. A diffusion process on a generalized
hypergraph can be understood as a process of information extracting among vertices. To generate
the underlying feature map (Z(t)), we first utilize the transition probabilities of the Markov chain in
Definition 1 to design a discounted average visiting rate: Vik (t) = t PT =1 ατ Pr(S(T) = k∣s(0)=
i), where t ∈ Z++. Notably, we introduce a discount factor α ∈ (0, 1] to exponentially weaken
the long-step transition probabilities from vertex i to vertex k. Specially, We set Vik (0) = 1 if and
only if i = k, otherwise Vik(0) = 0. Assume that Vi(t) = (Vi1(t),…，Vi∣v∣ (t))>. Then, we adopt
the diffusion distance dij(t) = ∣∣Vi(t) - Vj(t)k2 = ∣∣Z>(t)(Vi(0) - Vj(0))∣∣2 proposed in Pons &
Latapy (2005) to generate our discounted Markov diffusion kernel KMD (t) = Z(t)Z> (t), where
Z(t) = PT=1 竽 PT can be viewed as the weighted sum of the transition matrix (derivation in
Appendix F.2). The intuition behind this diffusion distance is that if two vertices diffuse similarly
based on hypergraph random walk, we are supposed to take them similar. Then, the influence of the
two vertices to other vertices in the hypergraph is considered in a similar manner.
Simple Hypergraph Spectral Convolution (SHSC). Inspired by the diffusion kernel-based GC-
NNs from Zhu & Koniusz (2021), based on our discounted Markov diffusion kernel, we propose a
Simple Hypergraph Spectral Convolution (SHSC) as follows:
Y = ψ ( (β Xx KTk + (1 - β)l) Xθ) .	(8)
6
Under review as a conference paper at ICLR 2022
Recall T is the transition matrix We used m equation 7, ψ(∙) is the activation function to map the
node representation of output, and Θ is the parameter of filter to be learned during training. SHSC is
able to balance the global information aggregation and the vertex’s oWn information by introducing
a hyperparameter β ∈ [0,1] to control the self-loops (T)0 = I. Through the discounted Markov
Diffusion process, our SHSC could gain stronger local and Weaker global information, thereby im-
proving the expressive poWer in the deep layers. On the other hand, SHSC uses only one linear
layer, Which greatly improves the computational efficiency and effectively alleviates the overfitting
problem. Actually, SHSC is a spatial-based model While We can analyze it from a spectral-based
perspective, as SHSC can be vieWed as a polynomial filter With specific coefficients (see Appendix
F.3). Finally, We study Why SHSC can relieve over-smoothing. Specifically, We utilize Corollary 2
to generate a loWer-bounded quantity of the defined over-smoothing energy, based on Which We
shoW that SHSC can relieve the over-smoothing issue compared to multi-layer GHCNs (details in
Appendix F.4).
Table 1: Summary of classificaiton accuracy(%) results. We report the average test accuracy and its standard
deviation over 10 train-test splits. The number in parentheses corresponds to the number of layers of the model.
(OOM: our of memory)
Dataset	Architecture	Cora (co-authorship)	DBLP (co-authorship)	Cora (co-citation)	Pubmed (co-citation)	Citeseer (co-citation)
MLP	-	52.02±1.7	78.72±0.6	52.02±1.7	69.86±1.6	55.03±1.3
HyperGCN	spectral-based	60.66±10.8	84.82±9.7	62.35±9.3	68.12±9.7	56.94±6.3
HGNN	spectral-based	69.23±1.6	88.55±0.18	55.60±1.8	46.41±0.7	38.98±1.1
HNHN	message-passing	63.95±2.4	84.43 ±0.3	41.59±3.1	41.94±4.7	33.60±2.1
HGAT	message-passing	65.42±1.5	OOM	52.21±3.5	46.28±0.53	38.32±1.5
UniGNN	message-passing	75.30±1.2	88.80±0.2	70.10±1.4	74.40±1.0	63.60±1.3
SSGC	spectral-based	72.04±1.2	88.61±0.16	68.79±2.1	74.49±1.3	60.52±1.7
GHCN (ours)	spectral-based	74.79±0.91	89.04±0.19	69.45±2.0	75.37±1.2	62.67±1.2
SHSC (ours)	spectral-based	76.05±0.75(6)	89.17±0.21(16)	70.64±1.8 (32)	75.08±1.1(4)	65.14±0.97(32)
4 Empirical Studies
In this section, We evaluate our proposed meth-
ods on four tasks: citation netWork classification,
visual object classification, protein quality assess-
ment (regression), and fold classification. For sim-
plicity, we set ρ(∙) to be a power function (∙)σ
(σ is a hyper-parameter) and additional experi-
ments to compare the performance with different
ρ is deferred to Appendix I.10. The weight ma-
trix of edges, we set, to be an identity matrix by
default in our GHCN and SHSC models. No-
tably, although GHCN and SHSC are designed for
EDVW-hypergraph learning, they work for EIVW-
hypergraph as well thanks to the unified Laplacian
in Corollary 1. In our experiments, citation net-
work classification belongs to EIVW-hypergraph
Table 2: Classification accuracy (%) on Model-
Net40. The embedding means the output represen-
tation of MVCNN+GVCNN Extractor.
Methods	input	Accuracy
MVCNN (Feng etal., 2018)	image	90.1
PointNet (Qi et al., 2017a)	point	89.2
PointNet++ (Qi et al., 2017b)	point	90.1
DGCNN (Wang et al., 2019)	point	92.2
InterpCNN (Mao et al., 2019)	point	93.0
SimpleView (Uy et al., 2019)	image	93.6
pAConv (XU et al., 2021)	point	93.9
HGAT (Ding et al., 2020)	embedding	96.4
UniGNN (Huang & Yang, 2021)	embedding	96.7
HGNN	embedding	97.2
GHCN(ours)	embedding	97.3
SHSC(ours)	embedding	97.7
learning tasks, and visual object classification and protein learning are EDVW-hypergraph learn-
ing tasks. For baselines designed for EIVW-hypergraph, we replace Q1, Q2 with H.
Due to space limitations, we defer additional experimental results on Over-smoothing Analysis,
Hyper-parameter Sensitivity Analysis, and Ablation Analysis to Appendix I.6, I.7, I.9, respectively.
The insights are: 1) SHSC can efficiently alleviate over-smoothing; 2) The hyper-parameters σ, β, α
are sensitive at a relatively large range but insensitive in a small range of the performance optimum;
3) The re-normalization trick and edge-dependent vertex weights are both effective. In addition, we
provide the computational complexity analysis for our models in Appendix I.11.
4.1	Citation Network Classification
This is a semi-supervised node classification task. The datasets include co-authorship and co-citation
datasets: PubMed, Citeseer, Cora (Sen et al., 2008), and DBLP (Rossi & Ahmed, 2015). We adopt
the same public datasets (https://github.com/malllabiisc/HyperGCN) and train-test
7
Under review as a conference paper at ICLR 2022
splits of Yadati et al. (2019). Note these datasets satisfy the condition that Q1 = Q2 = H. So
this experiment can be regarded as a special case of the applications of our models. For baselines,
we include Multi-layer perceptron(MLP), HNHN (Dong et al., 2020), HGAT (Ding et al., 2020),
UniGNN (Huang & Yang, 2021), and two recent spectral-based hypergraph convolutional neural
networks (HGCNNs): HGNN (Feng et al., 2019) and HyperGCN (Yadati et al., 2019).
Comparison with
SOTAs. As shown in
Table 19, the results
successfully verify
the effectiveness
of our models and
achieve a new SOTA
performance across
all five datasets. No-
tably, both GHCN
and SHSC have an
Table 3: Test accuracy on visual object classification. Each model we ran
10 random seeds and report the mean ± standard deviation. BOTH means
GVCNN+MVCNN, which represents combining the features or structures to gen-
erate multi-modal data.
Datasets	Feature	Structure	HGNN	UniGNN	HGAT	GHCN(ours)	SHSC(ours)
	MVCNN	MVCNN	80.11±0.38	75.25±0.17	80.40±0.47	81.37±0.63	82.56±0.39
NTU	GVCNN	GVCNN	84.26±0.30	84.63± 0.21	84.45±0.12	85.15±0.34	83.35±0.30
	BOTH	BOTH	83.54±0.50	84.45±0.40	84.05±0.36	84.45±0.40	85.12±0.25
Model- Net40	MVCNN	MVCNN	91.28±0.11	90.36±0.10	91.29±0.15	91.99±0.16	92.01±0.08
	GVCNN	GVCNN	92.53±0.06	92.88±0.10	92.44±0.11	92.66±0.10	92.69±0.06
	BOTH	BOTH	97.15±0.14	96.69±0.07	96.44±0.15	97.28±0.15	97.78±0.03
average performance improvement of 4.88% and 5.83% over the previous spectral-based methods,
respectively. On the other hand, SHSC achieves the best result at a deep layer and gains better
performance than GHCN on most datasets, which demonstrates the benefits of deep model and
the long-range information in hypergraphs. It’s worth noting that our methods gain superior
performance on disconnected datasets compared to HGNN. HGNN shows poor performance on
disconnected datasets, mainly due to the row in the adjacency matrix of equivalent undigraph
corresponding to an isolated point is 0, resulting direct loss of its vertex information (an example
can see Fig. 4 in appendix). And our methods utilize the renormalization trick, which can
maintain the features of isolated vertices during aggregation. Another important observation is that
HyperGCN has a high standard deviation, revealing its poor generalization. Conversely, SHSC has
lower bias and standard deviation than others, showing better generalization.
4.2	Visual Object Classification
This experiment is about semi-supervised learning. We employ two public benchmarks: Prince-
ton ModelNet40 dataset (Wu et al., 2015) and the National Taiwan University (NTU) 3D model
dataset (Chen et al., 2003) to evaluate our methods. We follow HGNN (Feng et al., 2019) to pre-
process the data by MVCNN (Su et al., 2015) and GVCNN (Feng et al., 2018). Finally, we use
the datasets provided by its public Code (https://github.com/iMoonLab/HGNN). More
details can be found in Appendix I.4.
Results. Table 2 depicts that our methods significantly outperform the image-input or point-input
methods. These results demonstrate that our methods can capture the similarity of objects in the
feature space to improve the performance of the classification task. Table 20 compares our methods
with HGNN and uniGNN on NTU and ModelNet40. From the results, we can see that our methods
outperform HGNN on both single modality and multi-modality (BOTH) datasets and SHSC achieves
much better performance on multi-modality compared with others. These results reveal that our
SHSC has the advantage of combining such multi-modal information through concatenating the
weighted incidence matrices (Q) of hypergraphs, which means merging the multi-level hyperedges.
4.3	Protein Quality Assessment (QA) and Fold Classification
Protein QA and fold classification are vital for mining the property of proteins. However, to the
best of our knowledge, most researchers represented a protein as a sequence or a simple graph (Bal-
dassarre et al., 2020; Hermosilla et al., 2021) in the deep learning area. So we’d like to investigate
whether hypergraphs are better than simple graphs for protein modeling, and the following results
confirm this conjecture.
Protein hypergraphs. A protein is a chain of amino acids (residues) that will fold to a 3D structure.
To simultaneously model protein sequence and spatial structure information, we build sequence
hyperedges and distance hyperedges: We choose T consecutive amino acids (vi, vi+ι, ∙∙∙ , vi+τ) to
form a sequence hyperedge, and choose amino acids whose spatial Euclidean distance is less than
8
Under review as a conference paper at ICLR 2022
the threshold > 0 to form a spatial hyperedge, where Vi(i = 1, ∙∙∙ , |S|) represents the i-th amino
acids in the sequence. Appendix I.5 contains more details.
Protein Quality Assessment (QA). Protein QA is used to estimate the quality of computational
protein models in terms of divergence from their native structure. It is a regression task to predict
how close the decoy is to the unknown, native structure.
Inspired by Baldassarre et al. (2020), we
train our models on Global Distance Test
Score (Zemla, 2003), which is the global-
level score, and the Local Distance Differ-
ence Test (Mariani et al., 2013), an amino-
acids-level score. The loss function of QA
is defined as the Mean Squared Error (MSE):
Lg = MSE(Ppred — GDT-TS), Ll =
P|iS=|1 MSE(Pplredi - LDDTi) where Ppgred
and Ppl red denote predicted global and lo-
cal score, respectively.We jointly learn node
and graph embeddings and the losses are
Weighted as Ltotal = μLl + (I ― μ)Lg,
Table 4: Comparison of our method to others on pro-
tein Quality Assessment task (CASP12). At the residue
level, we report Pearson correlation across all residues
of all decoys of all targets (R) and Pearson correlation
across all residues of per decoys and then average all de-
coys (Rdecoy) with LDDT scores. At the global level,
we report Pearson correlation across all decoys of all tar-
gets (R) and Pearson correlation per target and then aver-
age over all targets (Rtarget) with GDT-TS scores.
Methods	GDT-TS R	Rtarget		LDDT R	RdeCOy	
Olechnovivc & Venclovas (2017)	-	0.557	-	-
Zhang & Zhang (2010)	-	0.313	-	-
Derevyanko et al. (2018)	-	0.607	-	-
Conover et al. (2019)	0.651	0.439	-	-
HGNN	0.667	0.582	0.632	0.319
GHCN (ours)	0.737	0.609	0.656	0.340
SHSC (ours)	0.760	0.554	0.678	0.449
where μ is a hyper-parameter. We use the
data from past years’ editions of CASP, in-
cluding CASP10-13. CASP10,11,13 are used
for training and validation, and CASP12 is
used for testing. Results. Table 4 shows that our methods outperform most of SOTAs, which demon-
strates our methods can learn protein more efficiently. The outstanding performance of SHSC indi-
cates that it is able to jointly learn better the node and graph level representation. But SHSC trained
to predict only global scores obtains R = 0.712, which suggests that the local information can help
the assessment of the global quality.
Protein Fold Classification. This task is critical for studying the relationship	Table 5: Comparison of our methods to others on fold classifica- tion. We report the mean accuracy (%) of all proteins.					
between protein structure and func- tion, and protein evolution. We use the well-known SCOPe 1.75 dataset (Hou et al., 2018) and the cross-entropy loss. Results. Table 5 depicts that our proposed GHCN outperforms all others. It is manifest in the table that HGCNN-based methods GHCN						
	Methods	Architecture	#params	Fold Super. Fam.		
	Hou et al. (2018) Rao et al.(2019)* Bepler & Berger (2018)* Strodthoff et al. (2020)* Kipf & Welling (2017) Diehl (2019) Gligorijevic et al. (2020)* Baldassarre et al. (2020)	-1D ReSNet 1D Transformer LSTM LSTM GCNN GCNN LSTM+GCNN GCNN	41.7M 38.4M 31.7M 22.7M 1.0M 1.0M 6.2M 1.3M	17.0 21.0 17.0 14.9 16.8 12.9 15.3 23.7	31.0 34.0 20.0 21.5 21.3 16.3 20.6 32.5	77.0 88.0 79.0 83.6 82.8 72.5 73.2 84.4
and HGNN achieve much better per- formance compared to GCNN-based. However, it is worth noting that our proposed SHSC, does not seem to of-	HGNN SHSC (ours) GHCN (ours)	-HGCNN HGCNN HGCNN	2.1M 0.75M 2.1 M	24l∑^ 21.4 25.0	34.4 23.0 36.3	90.0 78.4 91.6
	* Pre-trained unsupervised	on 10-31 million protein sequences.				
fer significant advantages over GHCN and HGNN. This can be understood in the following lens:
SGHC is used to alleviate the over-smoothing problem, which has a large impact on the node-level
task, so our SHSC is more suitable for node-level tasks, while for the pure graph-level tasks, SHSC
is too simple to offer good performance.
5 Discussions and Future Work.
Despite the good results, there are some limitations which worth further explorations in the future:
1) We just provide sufficient conditions for the equivalency to undigraphs, but the necessary condi-
tion is still unclear. 2) With the equivalency between hypergraphs and undigraphs, we have extended
the undigraph-based GCNNs to hypergraph learning in this paper. Meanwhile, the equivalency be-
tween hypergraphs and digraphs would allow one to extend digraph-based GCNNs to potentially
learn richer information in hypergraphs. 3) One could use the proposed unified random walk on hy-
pergraphs to devise new clustering algorithms for hypergraph partitioning. 4) We make every effort
to include a thorough experimental study. Further results on other possible datasets and baselines
will be added as a future work.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement and Broader Impact
In the work, we proposed a unified random walk framework on hypergraphs, based on which we
study the equivalence between hypergraphs and graphs, and show that the unified random walk on
EDVW-hypergraphs can not be equivalent to undigraphs without some specific conditions. Mean-
while, we lead to the Laplacian of our framework and analyze the spectral properties of it, especially
focusing on the condition equivalent to an undigraph. Furthermore, we develop a Generalized Hy-
pergrpah Convolutional Network and a Simple Hypergraph Spectral Convolution for hypergraph
learning, based on the condition equivalent to undigraph.
Our research of the equivalency conditions between hypergraph and graph opens up many more pos-
sibilities in designing more comprehensive graph-based methods to solve the hypergraph problem.
The experimental tasks used in our work including social networks and academic networks analysis,
visual object classification, and protein learning, suggest that GHCN and SHSC can be applied for
beneficial purposes, such as Rumour Detection, Crime Identification, Protein Function Prediction,
etc.
Meanwhile, we have to be aware of possible negative impacts, such as using our model for telecom
fraud on social networks, placing a high degree of trust in our model and obtain an incorrect inter-
pretation of the results, etc. In addition, we should be vigilant about the potential unemployment
issue due to the reduced amount of the need of labeling by human beings.
Reproducibility S tatement
All the datasets are publicly available as described in the main text. Our efforts to ensure repro-
ducibility include the following aspects: 1) A sampled code is provided in the supplementary mate-
rial. 2) The proofs and derivations are provided in Appendix G, F. 3) More details of experimental
configurations are provided in Appendix I.
References
Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In
Proceedings ofthe 23rd international conference on Machine learning, pp. 17-24, 2006.
Sinan G Aksoy, Cliff Joslyn, Carlos Ortiz Marrero, Brenda Praggastis, and Emilie Purvine. Hyper-
network science via high-order hypergraph walks. EPJ Data Science, 9(1):16, 2020.
F Baldassarre, DM Hurtado, A Elofsson, and H Azizpour. Graphqa: Protein model quality assess-
ment using graph convolutional networks. Bioinformatics (Oxford, England), 2020.
Tristan Bepler and Bonnie Berger. Learning protein sequence embeddings using information from
structure. In International Conference on Learning Representations, 2018.
Timoteo Carletti, Federico Battiston, Giulia Cencetti, and Duccio Fanelli. Random walks on hyper-
graphs. Physical Review E, 101(2):022308, 2020.
Timoteo Carletti, Duccio Fanelli, and Renaud Lambiotte. Random walks and community detection
in hypergraphs. Journal of Physics: Complexity, 2021.
T-H Hubert Chan and Zhibin Liang. Generalizing the hypergraph laplacian via a diffusion process
with mediators. Theoretical Computer Science, 806:416-428, 2020.
T-H Hubert Chan, Zhihao Gavin Tang, Xiaowei Wu, and Chenzi Zhang. Diffusion operator and
spectral analysis for directed hypergraph laplacian. Theoretical Computer Science, 784:46-64,
2019.
Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d
model retrieval. In Computer graphics forum, volume 22, pp. 223-232. Wiley Online Library,
2003.
10
Under review as a conference paper at ICLR 2022
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph con-
VolUtional networks. In International Conference on Machine Learning, pp. 1725-1735. PMLR,
2020.
Uthsav Chitra and Benjamin Raphael. Random walks on hypergraphs with edge-dependent vertex
weights. In International Conference on Machine Learning, pp. 1172-1181. PMLR, 2019.
Fan ChUng. The laplacian of a hypergraph. Expanding graphs (DIMACS series), pp. 21-36, 1993.
Fan ChUng. Laplacians and the cheeger ineqUality for directed graphs. Annals of Combinatorics, 9
(1):1-19, 2005.
Fan RK ChUng and Fan ChUng Graham. Spectral graph theory. NUmber 92. American Mathematical
Soc., 1997.
Matthew Conover, Max Staples, Dong Si, Miao SUn, and Renzhi Cao. AngUlarqa: protein model
qUality assessment with lstm networks. Computational and Mathematical Biophysics, 7(1):1-9,
2019.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. arXiv preprint arXiv:1606.09375, 2016.
Georgy Derevyanko, Sergei Grudinin, Yoshua Bengio, and Guillaume Lamoureux. Deep convolu-
tional networks for quality assessment of protein folds. Bioinformatics, 34(23):4046-4053, 2018.
Frederik Diehl. Edge contraction pooling for graph neural networks. CoRR, 2019.
Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, and Huan Liu. Be more with less: Hyper-
graph attention networks for inductive text classification. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pp. 4927-4936, 2020.
Lei Ding and Alper Yilmaz. Interactive image segmentation using probabilistic hypergraphs. Pattern
Recognition, 43(5):1863-1873, 2010.
Yihe Dong, Will Sawin, and Yoshua Bengio. Hnhn: Hypergraph networks with hyperedge neurons.
arXiv preprint arXiv:2006.12278, 2020.
Aurelien Ducournau and Alain Bretto. Random walks in directed hypergraphs and application to
semi-supervised image segmentation. Computer Vision and Image Understanding, 120:91-102,
2014.
Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue Gao. Gvcnn: Group-view convo-
lutional neural networks for 3d shape recognition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 264-272, 2018.
Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3558-3565,
2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1263-1272. PMLR, 2017.
Vladimir Gligorijevic, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Beren-
berg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, IanM Fisk, Hera Vlamakis, et al. Structure-
based function prediction using graph convolutional networks. bioRxiv, pp. 786236, 2020.
David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
Koby Hayashi, Sinan G Aksoy, Cheong Hee Park, and Haesun Park. Hypergraph random walks,
laplacians, and clustering. In Proceedings of the 29th ACM International Conference on Informa-
tion & Knowledge Management, pp. 495-504, 2020.
11
Under review as a conference paper at ICLR 2022
Pedro Hermosilla, Marco Schafer, Matej Lang, Gloria Fackelmann, Pere PaU Vazquez, Barbora
Kozllkova, Michael Krone, Tobias RitscheL and Max Ropinski, Timo. Intrinsic-extrinsic Con-
volution and pooling for learning on 3d protein structures. In Proceedings of the International
Conference on Learning Representations, 2021.
Roger A Horn. Topics in matrix analysis, 1986.
Jie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping
protein sequences to folds. Bioinformatics, 34(8):1295-1303, 2018.
Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks.
In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21,
2021.
Yuchi Huang, Qingshan Liu, Shaoting Zhang, and Dimitris N Metaxas. Image retrieval via proba-
bilistic hypergraph ranking. In 2010 IEEE computer society conference on computer vision and
pattern recognition, pp. 3376-3383. IEEE, 2010.
Jianwen Jiang, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Gao. Dynamic hypergraph neural
networks. In IJCAI, pp. 2635-2641, 2019.
Taisong Jin, Liujuan Cao, Baochang Zhang, Xiaoshuai Sun, Cheng Deng, and Rongrong Ji. Hyper-
graph induced convolutional manifold networks. In IJCAI, pp. 2670-2676, 2019.
Wolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: pattern recog-
nition of hydrogen-bonded and geometrical features. Biopolymers: Original Research on
Biomolecules, 22(12):2577-2637, 1983.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In Proceedings of the International Conference on Learning Representations, 2017.
Johannes Klicpera, Stefan WeiBenberger, and Stephan GUnnemann. Diffusion improves graph learn-
ing. arXiv preprint arXiv:1911.05485, 2019.
Chensheng Li, Xiaowei Qin, Xiaodong Xu, Dujia Yang, and Guo Wei. Scalable graph convolutional
networks with fast localized spectral filter for directed graphs. IEEE Access, 8:105634-105644,
2020.
Jianbo Li, Jingrui He, and Yada Zhu. E-tail product return prediction via hypergraph-based local
graph cut. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 519-527, 2018a.
Pan Li and Olgica Milenkovic. Inhomogeneous hypergraph clustering with applications. arXiv
preprint arXiv:1709.01249, 2017.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018b.
Linyuan Lu and Xing Peng. High-ordered random walks and generalized laplacians on hypergraphs.
In International Workshop on Algorithms and Models for the Web-Graph, pp. 14-25. Springer,
2011.
Yi Ma, Jianye Hao, Yaodong Yang, Han Li, Junqi Jin, and Guangyong Chen. Spectral-based graph
convolutional network for directed graphs. arXiv preprint arXiv:1907.08990, 2019.
Jiageng Mao, Xiaogang Wang, and Hongsheng Li. Interpolated convolutional networks for 3d point
cloud understanding. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 1578-1587, 2019.
Valerio Mariani, Marco Biasini, Alessandro Barbato, and Torsten Schwede. lddt: a local
superposition-free score for comparing protein structures and models using distance difference
tests. Bioinformatics, 29(21):2722-2728, 2013.
12
Under review as a conference paper at ICLR 2022
Osamu Maruyama, Takayoshi Shoudai, Emiko Furuichi, Satoru Kuhara, and Satoru Miyano. Learn-
ing conformation rules. In International Conference on Discovery Science, pp. 243-257. Springer,
2001.
Naoki Masuda, Mason A Porter, and Renaud Lambiotte. Random walks and diffusion on networks.
Physics reports, 716:1-58, 2017.
Federico Monti, Karl Otness, and Michael M Bronstein. Motifnet: a motif-based graph convolu-
tional network for directed graphs. In 2018 IEEE Data Science Workshop (DSW), pp. 225-228.
IEEE, 2018.
Alexey G Murzin, Steven E Brenner, Tim Hubbard, and Cyrus Chothia. Scop: a structural classifi-
cation of proteins database for the investigation of sequences and structures. Journal of molecular
biology, 247(4):536-540, 1995.
Kliment Olechnovivc and vCeslovas Venclovas. Voromqa: Assessment of protein structure quality
using interatomic contact areas. Proteins: Structure, Function, and Bioinformatics, 85(6):1131-
1145, 2017.
Pascal Pons and Matthieu Latapy. Computing communities in large networks using random walks.
In International symposium on computer and information sciences, pp. 284-293. Springer, 2005.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 652-660, 2017a.
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical
feature learning on point sets in a metric space. In NIPS, 2017b.
Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel,
and Yun S Song. Evaluating protein transfer learning with tape. Advances in Neural Information
Processing Systems, 32:9689, 2019.
Ryan Rossi and Nesreen Ahmed. The network data repository with interactive graph analytics and
visualization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek. Udsmprot: universal deep
sequence models for protein classification. Bioinformatics, 36(8):2401-2409, 2020.
Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convo-
lutional neural networks for 3d shape recognition. In Proceedings of the IEEE international
conference on computer vision, pp. 945-953, 2015.
Zekun Tong, Yuxuan Liang, Changsheng Sun, David S Rosenblum, and Andrew Lim. Directed
graph convolutional network. arXiv preprint arXiv:2004.13970, 2020.
Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revis-
iting point cloud classification: A new benchmark dataset and classification model on real-world
data. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1588-1597.
IEEE Computer Society, 2019.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.
Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):
1-12, 2019.
Chris Wendler, Markus PuscheL and Dan Alistarh. Powerset convolutional neural networks. Ad-
vances in Neural Information Processing Systems 32, 2:929-940, 2020.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1912-1920, 2015.
13
Under review as a conference paper at ICLR 2022
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems, 32(1):4-24, 2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=ryGs6iA5Km.
Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan Qi. Paconv: Position adaptive convolu-
tion with dynamic kernel assembling on point clouds. arXiv preprint arXiv:2103.14635, 2021.
Naganand Yadati. Neural message passing for multi-relational ordered and recursive hypergraphs.
Advances in Neural Information Processing Systems, 33, 2020.
Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha
Talukdar. Hypergcn: A new method for training graph convolutional networks on hypergraphs.
In Advances in Neural Information Processing Systems, pp. 1511-1522, 2019.
Adam Zemla. Lga: a method for finding 3d similarities in protein structures. Nucleic Acids Research,
31(13):3370, 2003.
Kaiman Zeng, Nansong Wu, Arman Sargolzaei, and Kang Yen. Learn to rank images: A unified
probabilistic hypergraph model for visual search. Mathematical Problems in Engineering, 2016,
2016.
Chenzi Zhang, Shuguang Hu, Zhihao Gavin Tang, and TH Hubert Chan. Re-revisiting learning
on hypergraphs: confidence interval and subgradient method. In International Conference on
Machine Learning, pp. 4026-4034. PMLR, 2017.
Jian Zhang and Yang Zhang. A novel side-chain orientation dependent potential derived from
random-walk reference state for protein fold selection and structure prediction. PloS one, 5(10):
e15386, 2010.
Jiying Zhang, Yuzhao Chen, Xi Xiao, Runiu Lu, and Shu-Tao Xia. Learnable hypergraph laplacian
for hypergraph learning. arXiv preprint arXiv:2106.05701, 2021.
Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-sagnn: a self-attention based graph neural network
for hypergraphs. arXiv preprint arXiv:1911.02613, 2019.
Yubo Zhang, Nan Wang, Yufeng Chen, Changqing Zou, Hai Wan, Xinbin Zhao, and Yue Gao.
Hypergraph label propagation network. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 6885-6892, 2020.
Zizhao Zhang, Haojie Lin, Yue Gao, and KLISS BNRist. Dynamic hypergraph structure learning.
In IJCAI, pp. 3162-3169, 2018.
Dengyong Zhou, JiayUan Huang, and Bernhard Scholkopf. Learning with hypergraphs: Clustering,
classification, and embedding. Advances in neural information processing systems, 19:1601-
1608, 2006.
Hao Zhu and Piotr Koniusz. Simple spectral graph convolution. In International Conference on
Learning Representations, 2021.
14
Under review as a conference paper at ICLR 2022
Appendix
Contents
A	Related Work	16
A.1 Simplified Related Work ................................................... 16
A.2 Detailed Related Work ..................................................... 17
B	Relations with Previous Random Walks on Hypergraph	18
B.1	A Classical Random walk on Hypergraph .................................... 18
B.2	Intuition and Analysis of The Unified Random Walk on Hypergraph ......... 18
B.3	Special Cases of The Unified Random Walk Framework and Laplacian ........ 19
C	Unified Non-lazy Random Walks on Hypergraph	19
D	Typical Hypergraph Convolution (HGNN) and Its Spectral Analysis	21
D.1 Typical Spectral Hypergraph Convolution ................................... 22
D.2 Spectral Analysis of HGNN ................................................. 22
E The Relationship between Generalized Hypergraphs and Digraphs	23
F Derivation and Analysis of GHCN	and	SHSC	23
F.1	Derivation of GHCN ....................................................... 23
F.2	Derivation of The Discounted Markov Diffusion Kernel ..................... 24
F.3	Derivation of SHSC ....................................................... 24
F.4 Over-smoothing Analysis of GHCN and SHSC .................................. 25
G Details of Theories and Proofs	26
G.1 Proof of Theorem 3 ........................................................ 26
G.2 Proof of Lemma 1 .......................................................... 27
G.3 Proof of Theorem 1 ........................................................ 28
G.4 Nonequivalent Condition. .................................................. 30
G.5 Failure Condition of Function ρ............................................ 30
G.6 Proof of Corollary 1....................................................... 32
G.7 Spectral Range of Laplacian Matrix ........................................ 33
G.8 Proof of Corollary 2 ...................................................... 34
H	Generalized Hypergraph Partition	35
I	Details of Experiments	35
I.1	Hyper-parameter Strategy ................................................. 36
15
Under review as a conference paper at ICLR 2022
I.2	Baselines of Spectral Convolutions ............................................ 37
I.3	Citation Network Classification ............................................... 37
I.4	Visual Object Classification .................................................. 39
I.5	Protein Quality Assessment and Fold Classification ............................ 39
I.6	Over-Smoothing Analysis........................................................ 41
I.7	Sensitivity Analysis .......................................................... 42
I.8	The Spectrum Analysis of SHSC ................................................. 42
I.9	Ablation Analysis ............................................................. 43
I.10	Examples of ρ ................................................................. 43
I.11	Running Time and Computational Complexity ..................................... 44
I.12	The edge-dependent vertex weights visualization ............................... 44
I.13	Additional Experiments of Baselines. .......................................... 45
A Related Work
A. 1 S implified Related Work
Deep learning for hypergraphs. For spectral-based methods, Feng et al. (2019) introduce the first
hypergraph spectral convolution architecture HGNN, based on the hypergraph Laplacian proposed
by Zhou et al. (2006). Yadati et al. (2019) use a non-linear Laplacian operator (Chan & Liang, 2020)
to convert hypergraphs to simple graphs by reducing hyperedges into simple edges with mediators,
and take advantage of graph neural networks to learning hypergraphs. Meanwhile, for spatial-based
methods. Dong et al. (2020) propose a message-passing method that is divided into two processes
from vertices to edges and from edges to vertices. Wendler et al. (2020) utilize signal processing
theory to define the convolutional neural network on set functions. Yadati (2020) and Huang & Yang
(2021) generalize the current graph message passing methods to hypergraphs and obtain several
generalized models. Even though hypergraph neural networks have been well studied to process the
the edge-independent vertex weights, to the best of our knowledge, we are the first to attempt on
edge-dependent vertex weights.
Hypergraph random walk and spectral theory. In machine learning applications, Laplacian is
an important tool to represent graphs or hypergraphs. While random walk-based graph Laplacian
has a well-studied spectral theory, the spectral methods of hypergraphs are surprisingly lagged. A
two-step random walk Laplaciain was proposed by Zhou et al. (2006) for the general hypergraph,
and was improved by Chitra & Raphael (2019) and Carletti et al. (2020; 2021). On the other hand,
Lu & Peng (2011) and Aksoy et al. (2020) define a s-th Laplacian through random s-walks on
hypergraphs. More recently, non-linear Laplacian, as an important tool to represent hypergraphs,
has been extended to several settings, including directed hypergraphs (Zhang et al., 2017; Chan
et al., 2019), submodular hypergraphs (Li & Milenkovic, 2017), etc.
Equivalency between hypergraphs and undigraphs. A fundamental problem in the spectral the-
oretical study of hypergraphs is the equivalency with undigraphs. Once equipped with equivalency,
it is possible to represent hypergraphs with lower-order graphs, thus reducing the difficulty of hy-
pergraph learning. So far, equivalence is established from two perspectives: spectrum of Lapla-
cian (Agarwal et al., 2006; Hayashi et al., 2020) or random walk (Chitra & Raphael, 2019).
Hypergraph with edge-dependent vertex weights (EDVW-hypergraph)
EDVW-hypergraphs have been widely adopted in machine learning applications, including 3D ob-
ject classification (Zhang et al., 2018), image segmentation (Ding & Yilmaz, 2010), e-commerce (Li
et al., 2018a), image search (Huang et al., 2010), hypergraph clustering (Hayashi et al., 2020) etc.
Unfortunately, to the best of our knowledge, existing learning algorithms for processing EDVW-
hypergraphs do not involve deep learning methods.
16
Under review as a conference paper at ICLR 2022
A.2 Detailed Related Work
Deep learning for hypergraphs Feng et al. (2019) introduce the first hypergraph architecture
hypergraph neural network(HGNN), based on the hypergraph Laplacian proposed by Zhou et al.
(2006), however, a flaw in the theory makes the learning of unconnected networks very ineffi-
cient (details can see Appendix D.1). Yadati et al. (2019) use the non-linear Laplacian opera-
tors Chan & Liang (2020) to convert hypergraphs to simple graphs by reducing a hyperedge to a
subgraph with edge weights related only to its degree, which causes the information loss of hyper-
graphs and limited the generalization. Jiang et al. (2019) proposed a dynamic hypergraph neural
networks to update hypergraph structure during training. Wendler et al. (2020) utilize signal pro-
cessing theory to define the convolutional neural network on set functions. Zhang et al. (2020)
combined hypergraph label propagation with deep learning and introduced a Hypergraph Label
Propagation Network to optimize the hypegraph learning. Yadati (2020) proposed a generalised-
MPNN on hypergraph which unifies the existing MPNNs Gilmer et al. (2017) on simple graph and
also raised a MPNN-Recursive framework for recursively-structured data processing, but it performs
poorly for other high-order relationships. Huang & Yang (2021) generalized the current graph mes-
sage passing methods to hypergraphs and obtain a UniGCN with self-loop and a deepen hypergrpah
network UniGCNII. However, it does not give reasons why self-loop can assist UniGCN to gain
the improvement of performance in the citation network benchmark. Moreover, its experimental
results with various depths may show that the HGCNNs have over-smoothing issue, but theoretical
analysis to explain this phenomenon is lacked. Hypergraph deep learning has also been applied to
many areas, such as NLP (Ding et al., 2020), computer vision (Jin et al., 2019), recommendation
system (Zhang et al., 2019), etc
Hypergraph random walk and spectral theory. In machine learning applications, Laplacian is
an important tool to represent graphs or hypergraphs. While random walk-based graph Laplacian
has a well-studied spectral theory, the spectral methods of hypergraphs are surprisingly lagged.
The research of hypergraph Laplacian can probably be traced back to Chung (1993), which defines
the Laplacian of k-uniform hypergraph (each hyperedge contains the same number of vertices).
Then Zhou et al. (2006) defined a two-step random walk-based Laplacian for general hypergraphs.
Based on it, many works try to design a more comprehensive random walk on hypergraphs. Chitra
& Raphael (2019) designed a random walk which considered edge-dependent vertex weights of
hypergraphs in the second step to replace the edge-independent weights in Zhou et al. (2006).
Actually, the edge-dependent vertex weights could model the contribution of vertex v to hyperedge
e and were widely used in machine learning such as 3D object classification (Zhang et al., 2018; Feng
et al., 2019) and image segmentation (Ding & Yilmaz, 2010) etc., but without spectral guarantees.
On the other hand, Carletti et al. (2020; 2021) takes another perspective to gain more fine-grained
information from a hypergraph by taking into account the degree of hyperedges to measure the im-
portance between vertices in the first step. However, a comprehensive random walk should consider
the above two aspects at the same time. Furthermore, different from the two-step random walk
Laplacian of Zhou et al. (2006), Lu & Peng (2011) and Aksoy et al. (2020) define a s-th Laplacian
through random s-walk on hypergraphs. More recently, non-linear Laplacian, as an important tool to
represent hypergraphs, has been extended to several settings, including directed hypergraphs (Zhang
et al., 2017; Chan et al., 2019), submodular hypergraph (Li & Milenkovic, 2017) etc. Meanwhile,
liner Laplacian has also been developed. For example, Chitra & Raphael (2019) use a two-step ran-
dom walk to develop a spectral theory for hypergraphs with edge-dependent vertex weights, and a
two-step random walk on hypergraph proposed by Carletti et al. (2020; 2021) also designed a two-
step random walk by involving the degree of hyperedges in the first step and lead a linear Laplacian.
In this work, we design a unified two-step random walk framework to study the linear Laplacians.
Equivalency between hypergraphs and undigraphs. A core problem in the spectral theoreti-
cal study of hypergraphs is the equivalency between hypergraphs and graphs. Once equipped with
equivalency, it is possible to represent hypergraphs with lower-order graphs, thus reducing the diffi-
culty of hypergraph learning. So far, equivalence can be established from two perspectives: spectrum
of Laplacian or random walks. For the aspect of Laplacian spectrum, Agarwal et al. (2006) firstly
showed the Laplacian of Zhou et al. (2006) is equivalent to the Laplacian of the corresponding star
expansion graph (having the same eigenvalue problem). Next, Hayashi et al. (2020) claims that the
Laplacian defined by Chitra & Raphael (2019) is equal to an undigraph Laplacian, but the undi-
17
Under review as a conference paper at ICLR 2022
graph Laplacian needs to be constructed by invoking the stationary distribution, which restricts its
application. Meanwhile, from the random walk perspective, Chitra & Raphael (2019) claims that
hypergraph is equivalent to its clique graph (undirected) when the vertex weight of hypergraph is
edge-independent. However, despite the edge-dependent weights were widely used (Ding & Yil-
maz, 2010; Zeng et al., 2016; Zhang et al., 2018), the equivalency problem remains open when the
vertex weight is edge-dependent.
Protein learning. Proteins are biological macromolecules with spatial structures formed by fold-
ing chains of amino acids, which have an important role in biology. A protein has a three-level
structure: primary, secondary, and tertiary, where primary (sequence of amino acids) and tertiary (3D
spatial) structures are often used to model proteins for representation learning. Based on amino acids
sequence, there exist many related works of protein learning, such as Rao et al. (2019); Bepler &
Berger (2018); Strodthoff et al. (2020); Conover et al. (2019), etc. Meanwhile, many 3D struc-
ture based models are also raised for protein learning, such as Olechnovivc & Venclovas (2017);
Derevyanko et al. (2018); Baldassarre et al. (2020); Diehl (2019); Baldassarre et al. (2020); Gligori-
jevic et al. (2020); Hermosilla et al. (2021), etc. However, to the best of our knowledge, despite the
hypergraphs have been developed to model proteins (Maruyama et al., 2001), there is still no related
work to design the EDVW-hypergraph-based protein learning algorithm.
B	Relations with Previous Random Walks on Hypergraph
B.1 A Classical Random walk on Hypergraph
It can be traced back to Zhou et al. (2006) in which they have given a view of random-walk to
analyze the hypergraph normalized cut. The transition probability of Zhou et al. (2006) from current
vertex u to next vertex v is denoted as:
P(U V) = X w(e)H(U,e) H(V,e)	⑼
P(U) = 2w(e) d(u) δ(e) .	(9)
It is easy to find P(U, V) means: (i) choose an arbitrary hyperedge e incident with U with probability
w(e)/d(U) where d(U) = Pe∈E w(e)H(U, e); (ii) Then choose an arbitrary vertex V ∈ e with
probability 1∕δ(e) where δ(e) = Pv∈v H(v, e), which means to select vertex randomly in the
hyperedge.
B.2 Intuition and Analysis of The Unified Random Walk on Hypergraph
In this subsection, we would like to explain our intuition of developing the unified random walk
framework in Definition 1 from a view of two-step random walk on a hypergraph. To generalize the
notation of hypergraph random walk in the seminal paper Zhou et al. (2006), in this work, we tend
to explain the process from another perspective. The fact in Zhou et al. (2006) that
d(U) = X w(e)H (U,e) = XX w(e) H (U,e)H(v,e)= e∈E	e∈E v∈V	δ(e) and	=XX	Wτ4	W δ(e) v∈V e∈E(u,v)
w (e)H (U, e) H(V, e)	1	w(e) e∈E ^u	δ(eτ =丽e∈E(u,v) δeτ	P	w(e) 2	7(e) =e∈E(u,V)-rτ	(11) P	P	w(e) 乙	乙	7(e) b∈V e∈E(u,b)
show that the probability P(U, V) also means a normalized weighted adjacency relationship between
U and V (Here, E(U, V) denotes the hyperedges contained vertices U and V.). Furthermore, the
relationship is actually measured by the sum of We) for all e ∈ E concluding pair {u, v}, which
demonstrates that a hyperedge e with larger degree (δ(e)) linked to {U, V} contribute less to the
transition probability between U and V .
From Carletti et al. (2021), which proposed a random walk on hypergraphs (a specital case of our
unified random walk with Qi = Q2 = H and ρ(∙) = (∙)σ), We further explain the intuition of our
18
Under review as a conference paper at ICLR 2022
purpose to design the P(u, v) of our unified random walk. When Q1 = Q2 = H, P(u, v) in Eq. 3
can be expressed as:
w(e)ρ(δ(e))
P ((UV)= Py w(e)ρ(δ(e))
b∈V e∈E(u,b)
That is to say, the influence of hyperedge degree should not be limited to an inverse relationship by
introducing the function ρ(∙). Thus, by evolving more fine-grained vertexes weights Qι(u, e) and
Q2(v, e) to replace H(v, e) and H(u, e), we obtain the final transition probability of our unified
hypergraph random walk in equation 3.
B.3 Special Cases of The Unified Random Walk Framework and Laplacian
We show the existing random walks and Laplacians are the special cases of our unified random
walks and our unified Laplacians, respectively. Under our framework, the existing random walk
models can be seen as a special case with specific p1, p2. It is easy to construct the relations of our
framework to previous works as follows:
Remark 1 (ZhouetaL (2006)) When we choose ρ(∙) = (∙)T and Qi = Q2 = H in our
framework, the probabilities
—W(E)H(U，e)— and .
Pe∈E w(e)H(u,e) and p2
of unified
H(v,e)
Pv∈V H(v,e) .
random walk on hypergraph are reformulated as p1 =
. As the special case of ours satisfying Q1 = Q2 = H
and ρ(∙) = (∙)-i, Zhou et al. (2006)'s random walk satisfies both condition (1) and condi-
tion (2) in Thm. 1. So we obtain from Corollary 1 that ∏zhou(V) = P d(vd(u) and Lzhou =
I - Dv-1/2HWDe-1H>Dv-1/2 where Dv(u,u) = d(u) = Pe∈E w(e)H(u, e) and De(e, e) =
δ(e) = v∈eH(V, e). The forms of πzhou and Lzhou are exactly the same in Zhou et al. (2006).
Remark 2 (Carletti et al. (2020; 2021)) When we select ρ(∙)
(∙)σ and Qi
Q2
H, the probabilities of unified random walk on hypergraph are reduced to pi =
P ⅜⅜⅛⅜⅝vH⅛⅜⅞σ+ι and P2 = P WIve. As the special case ofours satisfying
Qi = Q2 = H and ρ(∙) = (∙)σ, Carletti et al. (2021)'s random walk satisfies both condi-
tion (1) and condition (2) in Thm. 1. So we obtain from Corollary 1 that ∏car (V) = P d(vd(u)
and Lcar = I - D-i/2HWD；H>D-i/2 where Dv(u, u) = d(u) = Pe∈E w(e)δ(e)σ+iH(u, e)
and De(e, e) = δ(e) = v∈eH(V, e). The forms of πcar and Lcar are exactly the same in Carletti
et al. (2021). Note that the random walk proposed by Carletti et al. (2020) is a special case of
Carletti et al. (2021) with σ = 1, the π and L are easy to obtain from our conclusions.
Remark 3 (Chitra & Raphael (2019)) When we set ρ(∙) = (∙)-i and Qi = H, the probabilities
of unified random walk on hypergraph become pi = P w(e)H(Ue)_- and p2 = P Q2'，?一V With
e∈E w(e)H (u,e)	v∈V Q2 (v,e)
an edge-dependent vertex weights Q2 in the second steps. Actually, as the special case of ours
satisfying Qi = H and ρ(∙) = (∙)-i, Chitra & Raphael (2019)'s random walk on hypergraph
generates the Laplacian Lchi which could build up a simple relation with our Lrw in equation 20 as
Lchi =中1/2Lrw Φ1/2 = Φ — "P +2P ”
which means that Lrw denotes a form of symmetrization on Lchi. Further, as the vertex weights is
edge-independent in Chitra & Raphael (2019) which means they satisfies condition(1) in Thm. 1,
the same π as Chitra & Raphael (2019) can be obtained from Corollary 1, i.e. π(ν) = £丸).
C	Unified Non-lazy Random Walks on Hypergraph
The non-lazy version is nontrivial to factor and analyze. To ease the studies, we first give the
following definition and lemma.
19
Under review as a conference paper at ICLR 2022
Definition 4 (Reversible Markov chain) Let M be a Markov chain with state space X and tran-
sition probabilities P (u, v), for u, v ∈ X. We say M is reversible if there exists a probability
distribution π over X such that
π(u)P (u, v) = π(v)P (v, u).	(12)
Lemma 2 (Chitra & Raphael (2019)) Let M be an irreducible Markov chain with finite state
space S and transition probabilities P (u, v) for u, v ∈ S. M is reversible if and only if there
exists a weighted, undirected graph G with vertex set S such that a random walk on G and M are
equivalent.
Proof. Let π be the stationary distribution of M (suppose that is irreducible). Note that π(u) 6= 0
due to the irreducibility of M . Let G be a graph with vertices S. Different from Chitra & Raphael
(2019), we set the edge weights of G to be
ω(u, v) = cπ(u)P (u, v),	∀ u, v ∈ S
(13)
where c > 0 is a constant for normalizing π. With reversibility, G is well-defined (i.e. ω(u, v) =
ω(v, u)). In a random walk on G, the transition probability from u to v in one time-step is:
ω(u,v)
Pv∈V ω(Sv)
cπ(u)P (u,v)
Pv∈V cπ(u)P (u,v)
P(u, v), since	w∈S P(u, w) = 1. Thus, if M is reversible,
recall Definition 3, the stated claim holds. The other direction follows from the fact that a random
walk on an undirected graph is always reversible (Aldous & Fill,[70]).
Next, we generalize the non-lazy random walk of Carletti et al. (2021) to our unified non-lazy
random walk with vertex weights Q1, Q2.
Definition 5 (Unified Non-lazy Random Walk on Hypergraphs) Suppose	δ(e)	:=
Pv∈V Q2(v, e) is the degree of hyperedge, and dnl(v) := Pe∈E w(e)(δ(e) - Q2(v, e))ρ(δ(e) -
Q2(v, e))Q1 (v, e) is the degree of vertex v. The unified non-lazy random walk on a hypergraph H
is defined in a two-step manner: Given the current vertex u,
Step I: choose a hyperedge e incident to u, with probability
w(e)(δ(e) — Q(u, e))ρ(δ(e) — Q2(u,e))Q1(u,e)
dnl (u)
(14)
Step II: choose an arbitrary vertex v 6= u from e, with probability
=Q2 (V,e)
p2 = δ(e) — Q2(u,e),
(15)
Thus, the transition probability of our unified non-lazy random walk from vertex u to v is:
Pnl(u, v)
e∈E
w(e)ρ(δ(e)-Q2(u,e))Q1(u,e)Q2(v,e)
dnl (u)
0
if u 6= v,
if u = v
(16)
Thus, the transition matrix of unified non-lazy random walk can be expressed as:
Pnl = Dn-l1(Q1 ρ(1De — Q2))WQ2>,	(17)
where Dnl, De are diagonal matrices with entries Dnl(v, v) = dnl(v) and De(e, e) = δ(e), respec-
tively. denotes the Hadamard product and 1 denotes a |V | × |E | ones matrix(i.e. all elements equal
to 1). ρ(1De — Q2)) represents the function ρ acting on each element of (1De — Q2).
Following Chitra & Raphael (2019), the modified version of the clique graph without self-loops is
defined below:
Definition 6 (Chitra & Raphael (2019)) Let H(V, E, Q) be a hypergraph associated with the uni-
fied non-lazy random walk in Definition 5. The clique graph of H without self-loops, GnCl, is a
weighted, undirected graph with vertex set V , and edges E0 defined by
E0 = {(v, w) ∈ V × V : v, w ∈ e for some e ∈ E, and v 6= w}	(18)
20
Under review as a conference paper at ICLR 2022
Different from the lazy random walk, a unified non-lazy random walk on a hypergraph with con-
dition (1) or condition (2) in Thm. 1 is not guaranteed to satisfy reversibility (see Definition 4).
However, if Q1 = Q2 = H, then reversibility holds, and we obtain the result below.
Theorem 2 Let H(V, E, Q) be a hypergraph associated with the unified non-lazy random walk in
Definition 5. Let Q1 = Q2 = H, i.e. Qi(v, e) = 1, i ∈ {1, 2} for all vertices v incident hyperedges
e. Then, there exist weights ω(u, v) on the clique graph without self-loops GnCl such that a non-lazy
random walk on H is equivalent to a random walk on GnCl.
Proof. We first prove that the unified non-lazy random walk on H is reversible. It is easy to
verify the stationary distribution of the unified non-lazy random walk on H with Q1 = Q2 = H
is π(V) = P Cdnl(V)I(V). Let P(u, V) be the probability of going from UtoVina unified non-lazy
random walk on H, where u 6= v . Then,
π(U)P(U,v)=Pud册ι (U)
1
u∈V dnl(U)
1
Pu∈V dnl (U)
1
u∈V dnl(u)
= π(V)P(V, u)
X w(e)ρ(δ(e) — Q2(u, e))Q1 (u, e)Q2(v, e)
e∈E	dn(U)
(w(e)ρ(δ(e) -Q2(U,e))Q1(U,e)Q2(V,e))
e∈E
X(w(e)ρ(δ(e) -H(U,e))H(U,e)H(V,e))
e∈E
X(w(e)ρ(δ(e) -H(V,e))H(V,e)H(U,e))
e∈E
So the unified non-lazy random walk is reversible. Thus, by Lemma 2, there exists a graph G with
vertex set V and edge weights
ω (u, v) = dni (U)P (u, v)
such that a random walk on G and the unified non-lazy random walk on H is equivalent. The
equivalence of the random walks implies that P(u, v) > 0 if and only if ω(u, v) > 0, so it follows
that G is the clique graph of H without self-loops.
Figure 4: An example of the disconnected hypergpraph and its equivalent weighted undirected graph. C is an
isolated vertex. The characteristics of hypergraph are encoded in the weight incidence matrix Q. The elements
in WC := QD-1 Q> denotes the edge-weight matrix of the clique graph.
0
146
1J
O-
3.
D Typical HYPERGRAPH Convolution (HGNN) and Its Spectral
Analysis
Recall Remark 1, the random walk and Laplaican proposed by Zhou et al. (2006) are special
cases (ρ(∙) = (∙)-1, Q1 = Q2 = H) of our unified random walk and unified Laplacian, respec-
tively. Thus, now the degree of a vertex v ∈ V is d(v) = e∈E w(e)H(v, e) and for a hyperedge
e ∈ E, its degree is δ(e) = v∈VH (v, e). The edge-degree matrix and vertex-degree matrix can
be denoted as diagonal matrices De ∈ RlEl×lEl and Dv ∈ RlVl×lVl respectively.
21
Under review as a conference paper at ICLR 2022
D.1 Typical Spectral Hypergraph Convolution
From the seminal paper Zhou et al. (2006), a classical symmetric normalized hypergraph
Laplacian is defined from the perspective of spectral hypergraph partition: L = I -
Dv-1/2HWDe-1H>Dv-1/2. Based on it, Feng et al. (2019) follow Kipf & Welling (2017) to deduce
the Hypergraph Neural Network HGNN. Specifically, they first perform an eigenvalue decompo-
sition of L: L = UΛU>, where Λ is a diagonal matrix of eigenvalues and U is consisted of the
corresponding regularized eigenvectors. Combining the hypergraph Laplacian with Fourier Trans-
form of graph signal processing, they derive the primary form of hypergraph spectral convolution:
g * X = U((U>g) © (U>x)) = Ug(Λ)U>x where g(Λ) = diag(g(λι), g(λ2),…,g(λχ)) is
a function of the eigenvalues of L. To avoid expensive computation of eigen decomposition (Def-
ferrard et al., 2016), they use the truncated Chebyshev polynomials Tk(x) to approximated g(Λ),
which is recursively deduced by Tk(x) = 2xTk-1 (x) - Tk-2 (x) with T0(x) = 1 and T1(x) = x.
g * x ≈ PK=-01 θkTk (Λ)x, with scaled Laplacian Λ = y^- A — I limiting the eigenvalues in [-1,1]
to guarantee requirement of the Chebyshev polynomial (Hammond et al., 2011). Since the parame-
ters in the neural network could adapt to the scaled constant (λmax ≈ 2), the convolution operation
is further simplified to beg * X ≈ θ0x - θιD-2 HWD-IH>D-2 x.
However, in order to obtain the final convolutional layer similar to Kipf & Welling (2017) pro-
posed, Feng et al. (2019) used a specific matrix to fit the scalar parameter θ0, causing a flaw in
the derivation. In the end, they have managed to obtain a seemingly correct hypergraph spectral
convolution (HGNN):
Y = DT/2HWD-1H> D-1/2X®,	(19)
where Θ is the parameter to be learned during the training process. There is no doubt that HGNN can
work properly, but it could have low efficiency when the hypergraph of the datasets is disconnected.
Our experiments in subsection 4.1 and Figure 4 verify our conclusion.
D.2 Spectral Analysis of HGNN
We give the definition of Rayleigh quotient to study the spectrum of Laplacian.
Lemma 3 ( Rayleigh Quotient (Horn, 1986)) Assume that M ∈ Rn×n is a real symmetric matrix
and X ∈ Rn is an arbitrary nonzero vector. Let λι ≤ λ? ≤ ∙ ∙ ∙ ≤ λn denote the eigenvalues of M
in order. Then the Rayleigh quotient satisfies the property:
X>MX
λι ≤ R(M, x) := —>— ≤ λn.
X> X
The equality holds that R(M, u1) = λ1 and R(M, un) = λn where ui is the eigenvector related to
λi,i∈{1,n}.
The Lemma 3 is mainly used to get all exact or approximated eigenvalues and eigenvectors of M.
In this part, we utilize it to prove that the smallest eigenvalue λmin of L = I -
-1	_1 › -1	1
Dv 2HWDe-1H>Dv 2 is 0 and figure out the corresponding eigenvector umin is Dv2 1.
Specifically, let g denote an arbitrary column vector which assigns to each vertex v ∈ V a real value
g(v). We demonstrate R(L, g) as below in order to describe our conclusion distinctly:
g>Lg
λmin ≤ g>g
g>(I - D-2HWD-1H>D-2)g
g>g
1	_1	_1	1
(Dv f )>(I - D-2HWD-IH>D-2)(DV2f)
1	1
(DV2 f )>(DV2 f)
f>(Dv - HWD-1H>)f
f>Dv f
Pe Pv Pu w(e)h¾ejh(u⑻(f (u) - f(v))2
2 Pvf(Vyd(V)
22
Under review as a conference paper at ICLR 2022
where g = DV/2f and f ∈ RlVl×1. It is easy to see the fact: R(L, g) ≥ 0, and R(L, g) = 0 if and
only iff = 1 (i.e. g = D1v/21). Then with lemma 3 we get λmin = min R(L, g) = R(L, D1v/21) =
g
1
0 and Umin = D^21.
Actually, since the Laplacian of Zhou et al. (2006) is our unified Laplacian in Corollary 1, the
property above can be directly led by Thm. 5.
E The Relationship between Generalized Hypergraphs and
Digraphs
In this part, we would like to illustrate that our unified random walk on a hypergraph is always
equivalent to a random walk on the weighted directed clique graph, As long as the equivalent condi-
tions are established, the graphs can be viewed as the low-order encoders of hypergraphs, and many
machine learning methods of digraphs could be easily generalized to hypergraphs.
Chitra & Raphael (2019), whose random walk is a special case of our framework with ρ(∙) =
(∙)-1, Qi = H, claims that the hypergraph with edge-dependent weights (i.e. Q2 is edge-
dependent) is equivalent to a directed graph. Here we extend this conclusion to a more general
version by the above definition.
Theorem 3 (Equivalency between hypergraph and weighted digraph) Let H(V, E, W, Qi, Q2)
denote the generalized hypergraph in Definition 2. There exists a weighted directed clique graph
GC such that the random walk on H is equivalent to a random walk on GC.
Thm. 3 indicates that any hypergraph is equivalent to a digraph under our framework (Fig. 1) and
implies that the existing hypergraph random walks are modeling pairwise relationships (proof in
Appendix G.1 ).
Adopting results of Thm. 3, we design the Laplacian for our unified random walk framework and
explore its spectral properties. Thm. 3 says that the unified random walk framework is equivalent to
directed graph, so we extend the definition of directed graph Laplacian (Chung, 2005) to obtain the
normalized Laplacian matrix as follows.
Corollary 3 (Random Walk-based Hypergraph Laplacian) Let H(V, E, W, Qi, Q2) be the gener-
alized hypergraph Let P be the transition matrix of our random walk framework on H with station-
ary distribution π (suppose that exists). Let Φ be a |V| × |V| diagonal matrix with Φ(v, v) = π(v).
The unified random walk-based hypergraph Laplacian matrix Lrw is defined as:
L ɪ	φV2pφ-V2 + φ-V2p> φ1/2
Following Chung (2005), it is easy to study many properties based on Rayleigh quotient of Lrw ,
such as positive semi-definite, Cheeger inequality, etc. The Laplacian Lrw has a great expressive
power thanks to the comprehensive random walk, which unifies most of existing random walk-
based Laplacian variants due to the generalization ability of the framework (see Appendix B.3).
Note that the distribution π is not easy to obtain and do not admit a unitized expression that limits
the application of Lrw in equation 20. In this paper, we focus on the case when hypergraph is
equivalent to undigraph (Thm. 1) and deduce the explicit expression of Lrw (Corollary 1).
Based on our equivalency Theorem and Lapalacian, we can design directed graph neural networks
for generalized hypergraph learning, which we will do as a future work.
F Derivation and Analysis of GHCN and SHSC
F.1 Derivation of GHCN
We define K as Q2Wρ(De)Q2>. Then the unified hypergraph Laplacian matrix L in Thm. 1 can be
denoted as
L = I - D-1/2KD-1/2,	(21)
23
Under review as a conference paper at ICLR 2022
Similar to GCN (Kipf & Welling, 2017) and Appendix D.1, we perform eigenvalue decomposition
of L, then obtain the convolution kernel by using Fourier transform, and approximate it with the
truncated Chebyshev polynomial to get the convolution form:
g ? X = θ0X - ΘiDd -1/2KD-1/2x.	(22)
Then we adopt the same method proposed by Kipf & Welling (2017) to set the parameter θ = θ0 =
-θ1 and obtain the following expression:
g ? x = θ(I + D-1/2KD-1/2) x.	(23)
According to Theorem 5, it is easy to know the eigenvalues of I + D-1/2KD-1/2 range in [0,2],
which may cause the unstable numerical instabilities and gradient explosion problem. So we use the
renormalization trick (Kipf & Welling, 2017):
I + D-1/2KD-1/2 → D-1/2KD-1/2，T	(24)
+v v → v v	,	()
where K = K + I can be regarded as the weighted adjacency matrix of the equivalent weighted
graph with self-loops, and D(v, v) = u K(v, u) is a |V| × |V| diagonal matrix. This self-loops
makes the methods more robust to process disconnected hypergraphs datasets. Finally, we drive the
generalized hypergraph convolutional network(GHCN):
X(I+1) = ψ(T X(I)Θ)	(25)
Θ is a learnable parameter and ψ(∙) denotes an activation function.
F.2 Derivation of The Discounted Markov Diffusion Kernel
Discounted Markov Diffusion Kernel for hypergraph Recall that the discounted average visit-
ing rate:
1t	1t
Vik(t) = - EaT Pr(S(T) = k∣s(0) = i) = ~ E ατ(PT)ik = ZOik	(26)
τ=1	τ=1
i.e. Z(t) = PT=ι αtτPτ.
Then, Vi(t) = (Vii(t),…，Vin(t))> denotes as:
Vi(t) = (e>Z(t))> = Z>(t)Vi(0)
Recall the definition of Vi(0) in subsection 3.2: Vik (0) = 1 if and only if i = k, otherwise Vik (0)=
0. Then, the discounted Markov diffusion kernel KMD can be derived from the l2-norm diffusion
distance as follows:
di,- (t) = kVi(t) - Vj (t)k2 = Hz」—)-Vj mil；
=(Vi(0) — Vj(0))>Z(t)Z>(t)(Vi(0) - Vj(0))
= (ei - ej) KMD (ei - ej)
F.3 Derivation of SHSC
Inspired by the diffusion kernel-based methods from Zhu & Koniusz (2021) and Klicpera et al.
(2019), and based on the discounted Markov diffusion kernel defined in subsection 3.2, the hyper-
graph convolution designed in our work is shown as below general form:
Y=ψ
KTk xθ，
(27)
where T is the generalized transition matrix in equation 24, Θ is the parameter matrix to be learned
during training, and ψ(∙) denotes an activation function. Since Tk can be viewed as a k-steps
transition matrix, in order to weaken the neighbor vertices in long diffusion steps, we introduce
24
Under review as a conference paper at ICLR 2022
a discounting factor K for Tk, which can be deduced from the underlying feature map Z(t) of
discounted Markov diffusion kernel defined in subsection 3.2. We use a well-known trick (Zhu &
Koniusz, 2021; Chen et al., 2020) to balance the global information aggregation and the vertex’s own
information by introducing a hyper-parameter β ∈ [0,1] to control the self-loop T0 = I. Finally,
we proposed a Simple Hypergraph Spectral Convolution (SHSC) as:
Y = ψ((β XX aTk + (1- β)l)xθ)	(28)
Actually, SHSC is a spatial-based model while we can analyze it from a spectral-based view. Let
L = I - T denote the normalized Laplacian matrix. Then the SHSC can be viewed as a special
spectral-based polynomial filter through PI=0 ξiLi = PK=0 θkTk where θk = OK. Finally, We
can deduce the coefficients of the special polynomial filter as ξi = (-1)i PK=i (：) OK showing the
strong relationships between SHSC and spectral-based model. The explicit form ofξicanbe derived
as:
X θkTk	= X θk(I-L)k = X θk XXX	C)(-1)iLi = X	(X(-1)i (k) Kk) Li
k=0	k=0	k=0 i=0	i=0 k=i
F.4 Over-smoothing Analysis of GHCN and SHSC
For real-world datasets, we always use non-Euclidean graphs with finite-dimensional features of
vertices. It is difficult to use the absolute diffusion distance (Masuda et al., 2017) which measures
the distance walked from starting vertex in the diffusion process. Therefore, we use the reciprocal
of the mean l1 -norm error between t-step transition probability Pt and stationary distribution π
as the measure of over-smoothing in arbitrary starting vertex. Formally, we name the measure by
over-smoothing energy as follows:
e(i,t)= N f⑺ Pt- ∏kι
where i denotes the sign of starting vertex i and t denotes the number of diffusion step. As mentioned
in Corollary. 2,f(i) is an initial distribution which means the walker diffuse starting from vertex
i(i.e.f(i)i = 1 andf(i)j = 0, ∀j 6=i ). It is easy to observe that if the limit of e(i, t) with respect
to t converges to infinity, it indicates the model based on P undergo severe over-smoothing issue.
Recall Corollary 2, we get a low-bounded quantity of e(i, t) as elow (i, t):
e(i, t) ≥
P良(I- λH)t √g
(1 - λH)tφ(H)
elow (i, t)
N
where 夕(H) = PN=1 ʌ/ dj can be seen as a constant associated with H which is independent
with i or t. Recall Corollary 2 that λH is smallest nonzero eigenvalue of L.
Over-smoothing analysis of GHCN. As shown in Table 14, we can see our proposed GHCN
model also face with the over-smoothing phenomenon. Assume that we get a K -layers GHCN
model with the form: X(l+1) = ReLU(TX(I)Θ). To get a simple mathematical form from above
incremental convolution formula (remove the activation layer), we could get approximated K -layers
GHCN as:
X(K) = T K X(O)Θ
Intuitively, from the simple form we find the over-smoothing energy of GHCN in vertex i can be rep-
resented by elow(i, K). It is easy to analyze that as K → ∞, elow(i, K) converge to infinity which
implies that the over-smoothing energy e(i, t) converge to infinity. Furthermore, the convergence of
over-smoothing energy to infinity means the error between initial signal andπ converge to 0 as the
number of layers increasing, which describes the reason of over-smoothing underwent by GHCN
from a quantitative perspective. HGNN (Feng et al., 2019) also suffers from over-smoothing as
shown in Table 14. Meanwhile, the theoretical analysis of over-smoothing on HGNNcan be covered
by the analysis of GHCN due to HGNN is a special case of GHCN.
25
Under review as a conference paper at ICLR 2022
Over-smoothing analysis of SHSC. Similarly, from the form of SHSC, we use
-K PK=0 αkeiow(i, k) to represent SHSC's over-smoothing energy. In order to explain the
reason why SHSC could relieve over-smoothing, we provide the idea of equivalent infinitesimals
to compare the over-smoothing energy between muti-layers GHCN and SHSC as follows: As
K → ∞:
eSHSC(i, K)	% PK=0 αkelow(i, k)
lim ------ .	= lim -------- -------
K →∞ eGHCN (i, K)	K →∞	elow (i, K)
1.	(1-λH)K pKK=o( 1⅛)k
=lim	H——
K →∞	K
(I- λH)K+1 - ɑK+1
= lim ----TTK--------ʌ——(suppose 1 -入口 - α = 0)
K→∞	K(1 - λH - α)
= 0	(29)
where the last equation depends on α ∈ (0, 1] and λH ∈ (0, 2] proved by Theorem 5. If 1-λH -α =
0, 1 - λH = α < 1. Thus, the equation 29 also holds. The result describes why SHSC can
relieve over-smoothing compared to multi-layer GHCN that the over-smoothing energy of multi-
layers GHCN has been sparse by the designed diffusion kernel of SHSC as K growing.
Actually, we can directly analyse the over-smoothing energy of SHSC:
lim ɪ X αkeiow(i,k)
K→∞ K
k=0
lim X	ak
K→∞ K
k=0
N^(i)
O-AHW(H)
NVzMi)	1 - λH	α
lim ——L-：—	——：---------：----：----------
k→∞ 中(H)	K (1	— Ah	— α)	K (1	— Ah — a)
(30)
If α ≤ |1 - AH | and 1 - AH - α 6= 0, equation 30 equals to 0; If 1 - AH - α = 0, equation 30
equals to N^Hi ； Otherwise, equation 30 converges to infinity. Notably, this result shows that
the over-smoothing property of SHSC is related to the magnitude of α and AH. Moreover, this
analysis further reveals that the introduced hyperparameter α is meaningful for SHSC to alleviate
over-smoothing.
G Details of Theories and Proofs
G.1 Proof of Theorem 3
Theorem 3 (Equivalency between hypergraph and weighted digraph) Let H(V, E, W, Q1 , Q2 )
denote the generalized hypergraph in Definition 2. There exists a weighted directed clique graph
GC such that the random walk on H is equivalent to a random walk on GC.
Proof. Let ω(u, v) be the weight of edge (u, v) in GC. We set ω(u, v) = d(u)P (u, v) in Defini-
tion 1, then the weighted clique graph is a directed graph, where P(u, v) is the transition probabili-
ties in Eq equation 3. The random walk on the directed weighted G C with transition probabilities
ω(u, v)	d(u)P (u, v)
Pv∈V ω(u,v) = Pv∈v d(u)P(u,v)
= P(u, v)
is equivalent to H with transition probabilities P(u, v). The matrix form of edge weight of GC is :
WC = Q1Wρ(De)Q2>
Suppose W = I and let ρ(∙) = (∙)-1, We can obtain the equivalent weighted clique graph showing
in Figure 5.
26
Under review as a conference paper at ICLR 2022
Figure 5: An example of hypergraph and its equivalent weighted directed graph, where qi(∙) = Qi(∙, e),i ∈
{1, 2}. Here, Qi is edge-dependent and Q2 is edge-independent. The characteristics of the hypergraph are
encoded in the weighted incidence matrices (i.e. vertex weights) Qi, Q2. WC := QiD-1 Q> denotes the
edge-weight matrix of the clique graph and can be viewed as the embedding of high-order relationships.
O-


G.2 Proof of Lemma 1
Lemma 1 Let H(V, E, W, Q1 , Q2) denote the generalized hypergraph in Defini-
tion 2. Let F(Q1,Q2)(u,v)	:=	Pe∈E w(e)ρ(δ(e))Q1(u, e)Q2(v, e) and T(Q)(u)	:=
e∈E w(e)δ(e)ρ(δ(e))Q(u, e). When Q1, Q2 satisfies the following equation
T(Q2) (U)T(Q1) (V)F(QI,Q2) (U,v) = T(Q2) (V)T(Qι) (U)F(QI ,Q2) (V, U), ∀u,v ∈ V	(4)
there exists a weighted undirected clique graph G C such that a random walk on H is equivalent to a
random walk on GC with edge weights ω(u, v) = T(Q2) (u)F(Q1 ,Q2) (u, v)/T(Q1) (u) if T(Q1) (u) 6=
0 and 0 otherwise.
And the symmetrical Laplacian of GC is
L(UN) = I{u=v} - T(Q2) (U)-1∕2ω(U,v)T(Q2) (V厂1/2
where I{∙} denotes the indicator function.
(31)
Proof. Form Lemma 2, we just need to prove Markov chain with state space V is reversible. The
transition probabilities of the unified random walk on H are:
P(U,V) =
e∈E
w(e)ρ(δ(e))Q1(u,e)Q2(v,e)	F(Q1,Q2)(u,v)
d(u)
T(Q1)(u)
By equation 4, we have:
f(Qi ,Q2)(U V)	T	心 f(Qi ,Q2) (v，u)
"(U)	T(Q1) (U)	= T(Q2) (V)	T(Q1) (V)
i.e.
T(Q2)(U)P(U,V) = T(Q2)(V)P(V,U),
Comparing with equation 12, we normalize G(Q2) to define the stationary distribution of M:
π(U) ：= P T(Q2)(U)	, ∀u ∈ V,
u∈V T(Q2) (U)
(32)
which can be easy to verify by	u π(u)P (u, v) = π(v). Thus, π(u)P (u, v) = π(v)P (v, u), which
suggests that M is reversible. By Lemma 2, the edge weight ω(u, v) of GC is
ω(u, v) = cπ(u)P (u, v) = T(Q2) (u)P (u, v) =
T(Q2) (U)
T(Q1)(u)
F(Q1,Q2)(u,v)
To gain the Laplacian of GC, we calculate the sum of row of edge weights matrix,
X ω(U，v) = T(Q2)(U) X F(Qι,Q2)(U,v) = T(Q2)(U)
v∈V	T(Q1) (U) v∈V
Then we get the symmetrical Laplacian of GC :
L(u,v) = I{u=v} -T(Q2) (U)-2ω(u,v)T(Q2) (V厂1
(33)
27
Under review as a conference paper at ICLR 2022
G.3 Proof of Theorem 1
Theorem 1 (Equivalency between generalized hypergraph and weighted undigraph) Let
H(V, E, W, Q1, Q2) denote the generalized hypergraph in Definition 2. When H satisfies any of
the condition bellow:
Condition (1) Q1 and Q2 are both edge-independent;	Condition (2) Q1 = kQ2 (k ∈ R),
there exists a weighted undirected clique graph G C such that a random walk on H is equivalent to
a random walk on GC.
Proof.
1) Proof based on Lemma 1 For condition (1),
F(Q1,Q2) (u, v) =	w(e)ρ(δ(e))Q1(u, e)Q2(v, e) = q1(u)q2(v)	w(e)ρ(δ(e))H(u, e)H(v, e)
e∈E	e∈E
T(Qi)(u) = X w(e)δ(e)ρ(δ(e))Qi(u, e) = qi(u) X w(e)δ(e)ρ(δ(e))H(u, e), i = {1, 2}
e∈E	e∈E
(34)
Substituting the above equations into equation 4, then equation 4 holds.
For condition (2),
F(Q1,Q2) (u, v) =	w(e)ρ(δ(e))Q1(u, e)Q2(v, e) = k	w(e)ρ(δ(e))Q2(u, e)Q2(v, e)
e∈E	e∈E
T(Q1)(u) = X w(e)δ(e)ρ(δ(e))Q1(u, e) = k X w(e)δ(e)ρ(δ(e))Q2(u, e) = kT(Q2)(u)
e∈E	e∈E
(35)
Similarly, substituting the above equations into equation 4, then equation 4 holds.
2 ) Proof based on Lemma 2 I) For Condition (1):
Because Q1 and Q2 are both edge-independent, we set Q1 (u, e) = q1 (u) and Q2(v, e) = q2(v)
for all hyperedge e. From Lemma 2, the key is to prove the unified hypergraph random walk on H
under condition (1) is reversible (see Definition 4). It is hard to find the explicit form of π before we
get Corollary 4. Fortunately, by Kolmogorov’s criterion, that is equal to prove:
pVl,V2 pV2,V3 ^ ^ ^ pVn,Vl = pVl,Vn pVn,Vn-1 .** pV2,Vl
for arbitrary subset {vι, ∙∙∙ ,vn} ⊆ V. The transition probabilities of the generalized random walk
on H are:
P(UM= X W⑹P((He))QI(U,e通2(V，e) = *)q2(V) X w(e)ρ(δ(e))H(u,e)H(v,e)
e∈E	d(u)	d(u)	e∈E
=q⅛^ X w(e)ρ(δ(e))
d(U)
28
Under review as a conference paper at ICLR 2022
where E(u, v) = {e ∈ E : u ∈ e, v ∈ e} to be the set of hyperedges incident to both v and u. Then
we have:
pvi ,V2 ∙∙∙ Pvn ,V1
也警等	X	w(e)ρ(δ(e))
d(v1)
e∈E(v1,v2)
q⅛F	X	w("δ(e))
d(vn)	e∈E(vn,v1)
YY /)；2)(Vi)) ∙ YY X	w(e)ρ(δ(e)) , Whereset v» = v1
=1	i	=1 e∈E(vi,vi+1)
X	w(e)P(δ(e))
d(v1 )
e∈E (vn,v1)
叫4X	w(e)ρ(δ(e))
d(v2)	e∈E(v1,v2)
pv1,vn …pv2,v1
So the unified random Walk on H under condition (1) is reversible.
As a matter of fact, We can succinctly obtain the reversibility via the stationary distribution. Specif-
ically, We can verify the stationary distribution is
e e = Ee∈E w(e)δ(e)ρ(δ(e))Q2(u, e)
π(U)= Pv∈V Pe∈E w(e)δ(e)ρ(δ(e))Q2(ν,e)
(36)
by u π(u)P(u,v) = π(v). Then With π(u)P (u, v) = π(v)P (v, u) holding, We knoW our unified
random Walk on H under condition (1) is reversible. Since H is connected, the unified random on
H is irreducible. From Lemma.2, We knoW that the current unified random Walk on H is equivalent
to a random Walk on an undirected graph G With vertex set V . By equation 13, the edge Weights of
G are:
ω(u,v) = cπ(u)P (u,v) =	w(e)ρ(δ(e))Q2 (u, e)Q2 (v, e)
e∈E
(37)
Where c =	v∈V e∈E w(e)δ(e)ρ(δ(e))Q2 (v, e). Note that P (u,v) > 0 if and only if ω(u, v) >
0, so G is the clique graph of H.
II) For condition (2) that Q1 = kQ2,
We have P = kDv-1Q2Wρ(De )Q2>. Thanks to the underlying symmetry adjacency matrix of
P (i.e. Q2Wρ(De)Q2 is symmetrical), we have: 1>DDvP = 1>DD>. Thus, the stationary distribu-
tion is
dU _	Pe∈E w(e)δ(e)ρ(δ(e))Q(u,e)
Pv 才(V) = Pv∈v Pe∈E w(e)δ(e)ρ(δ(e))Q(ν,e)
(38)
Further, we would prove that the unified random walk on H under current condition (2) is reversible.
^ >
π(U)P(U,v) = P / X
v0 d(V0) e∈E
d(V)
=POdv) NE
w(e)ρ(δ(e))Q(u, e)Q(V, e)
d(u)
w(e)ρ(δ(e))Q(V, e)Q(u, e)
d(V )
π(V)p(V, u)
So the unified hypergraph random walk on H under condition (2) is reversible. Since H is connected,
the unified random on H is irreducible. From Lemma.2, We know that the current unified random
walk is equivalent to a random walk on a weighted, undirected graph G with vertex set V. By
equation 13, the edge weights of G are:
ω(U, V) = cπ(U)P (U, V) =	w(e)ρ(δ(e))Q(U, e)Q(V, e)	(39)
e∈E
29
Under review as a conference paper at ICLR 2022
where c =	v0 d(v0) = v∈V	e∈E w(e)δ(e)ρ(δ(e))Q(v, e). Note that P(u, v) > 0 if and only
if ω(u, v) > 0, so G is the clique graph ofH.
On the whole, we get the specific equivalent weighted undigraphs under our condition (1) and con-
dition (2). Note that they have the same edge weights expression ( equation 37,equation 39):
WC = Q2 Wρ(De )Q>
which further suggests the connection between condition (1) and condition (2) stated in Corollary 4
and Thm. 1.
Remark: Let ρ(∙) = (∙)-1 and W = I, we can get the undigraph shown in Figure 1.
1 0
1 1
1 1
.0 1
,0 3 3 3-
4 7 7 3
410106
-4 4 4
Figure 6: An example of hypergraph and its equivalent weighted directed graph, where qi(∙) = Qi(∙, e),i ∈
{1, 2}. Here, Qi is edge-independent and Q2 is edge-dependent. The characteristics of the hypergraph are
encoded in the weighted incidence matrices (i.e. vertex weights) Qi, Q2. WC := QiD-1 Q> denotes the
edge-weight matrix of the clique graph and can be viewed as the embedding of high-order relationships.
G.4 NONEQUrVALENT CONDITION.
Theorem 4 There at least exists one generalized hypergraph H(V, E, W, Q1, Q2) (Definition 2)
with edge-dependent weights and Q1 6= kQ2, such that a random walk on H is not equivalent to a
random walk on its undirected clique graph GC for any choice of edge weights on GC.
Remark. Theorem 4 tells that the random walk on a hypergraph can not be always equivalent to a
random walk on an undigraph, and open up the possibility to construct a hypergraph is not equivalent
to an undigraph, which may inspire researcher to further study for higher-order interactions and other
insurmountable bottlenecks on hypergraphs.
Proof. When the vertex weights are edge-dependent, the reversibility is not always satisfied in
our unified random walk on a hypergraph. By Lemma 2, if the unified random walk is not time-
reversible, it could not equivalent to random walks on digraphs for any choice of edge weights.
A irreversible example can see Figure 6. The probability transition matrix of the unified random
walk on the left hypergraph (with W = I,ρ(∙) = (∙)-1) is given below:
P=Dv-1Q1Wρ(De)Q2> =Dv-1Q1De-1Q2> =
I I
O 1-81-8 1-3
1-3 7 一247 一241-3
1-35125121-2
1-31-61-6 O
4

and the stationary distribution of the unified random walk on hypergraph with P is:
3752
π = [17, 17,17,17]
(41)
We verify π(1)P0(1, 2) = , X 3 = , X 6 = π(2)P(2,1), which indicates that the Markov chain
represented by the unified random walk in Figure 6 with transition probabilities P is irreversible.
G.5 FAILURE CONDITION OF FUNCTION ρ.
As a matter of fact, the ρ in our random walk framework can not always work and we give its failure
condition as follows.
30
Under review as a conference paper at ICLR 2022
Proposition 1 Let H(V, E, W, Q) associated with the unified random walk in Definition 1. ρ(∙)
fails to work in our unified random walk if the hyperedge degrees are edge-independent (i.e. δ(e) =
δ(e0), ∀e0 ∈ E).
Proof. Given δ(e) are edge-independent, let us suppose δ(e) = k, then
w(e)Q1(u, e)Q2(v, e)ρ(δ(e))
u,v) =〉 ----------------------------
邑	d(u)
Σ
e∈E
w(e)Ql(u, e)Q2(v, e)ρ(δ(e))
Pe∈E W(C)δ(C)P(6(C))QI(V, e)
Σ
e∈E
w(e)Q1(u, e)Q2(v, e)ρ(k)
Pe∈E w(e)δ(e)P(k)Q1(v,e)
Σ
e∈E
ρ(k)w(e)Qι(u, e)Q2(v, e)
P(k) pe∈E W(C)δ(C)QI(V,e)
Σ
e∈E
W(C)Ql(ULe)Q2(V1C
Pe∈E W(C)δ(C)QI(V,c)
which indicates P(u, v) is independent to ρ(∙), i,e. ρ(∙) fails to work.
It is obvious that the k-uniform hypergraph has edge-independent hyperedge degrees. Formally, we
have the following proposition.
Proposition 2 Let H(V, E, W, Q1, Q2 ) denote the generalized hypergraph in Definition 2 with
ρ(∙) = (∙)σ and Qi = Q2 = H , then H is a k-uniform hypergraph(i.e each hyperedge contains the
same number of vertices) if and only if P(u, V) in equation 3 is independent with σ.
Proof. For Q = H, then δ(C) =	v∈e h(V, C) = k.
So, for all u, V ∈ V, we have:
K(σ) (u, V) := X W(C)(δ(C))σqe(u)qe(V) = kσ X W(C)H (u, C)H (V, C)	(42)
e∈E	e∈E
Then we get the entries of the transition matrix P(σ) :
K(σ)(u, v)	_	kσ Pe∈E w(c)H(u, c)H(v, c)
Pb K (σ)(u,b) = Pb∈v kσ Pe∈E W(C)H (u,c)H (b,c)
Pe∈E W(C)H(u, C)H(v, C)
Pb∈V Pe∈E W(C)H(u,c)H(b,c)
(43)
(44)
It is easy to see that P(σ) (u, V) is independent with σ.
The other direction, we find when P (σ)(u, V) in is independent with σ,then:
∂P (σ)(u,v) = * K (σ)(u,v) - dK(∂σuvl d(u)	,45∖
∂σ =	d2(u)	( 5)
_ Pb Pe Pe0 W(C)W(C0)h(u, c)h(u, c0)h(v, c0)h(b, c)δ(c)σδ(c0)σ(ln(δ(c)) - ln(δ(c0)))
=	d2(u)	()
= 0,∀u,V ∈V	(47)
It is equal to ∃k ∈ Z++ s.t.:
δ(c) = δ(c0 ) = k for all c ∈ E
.i.e. the hypergraph is k-uniform. With the condition that Qi = Q2 = H, our unified random
walk on hypergraph reduces to a random walk on hypergraph that leaving only incident relation-
ships between hyperedges and vertices. Actually, the reduced random walk mentioned is a common
structure in real-world applications. The proposition is to say when the hypergraph based on the
reduced random walk is uniform, the function ρ(∙) does not impact the importance between vertices
any more.
31
Under review as a conference paper at ICLR 2022
G.6 Proof of Corollary 1.
Before proving the Corollary 1, we further illustrate the fact that Lemma 4 states.
Lemma 4 Let H(V, E, W, Q1 , Q2) be the generalized hypergraph in Definition 2. Given a H
satisfying condition (1) in Thm. 1, there exists a H0 satisfying the condition (2) such that the random
walk on H is equivalent to that on H0.
Proof of Lemma 4. When the condition (2) in Thm. 1 holds (i.e. Q1 = Q2), the transition
probabilities of the unified random walk on H are
Pc2(	)= Σe∈E w(e)P(δ(e))Q2(u, e)Q2(v, e)
(u, V)=	Ρe∈E w(e)ρ(δ(e)δ(e)Q2(u,e)
(48)
Meanwhile, if the condition (1) in Thm. 1 holds (i.e. Q1 , Q2 are both edge-independent), for
all hyperedge e, we represent Q1 (u, e), Q2(v, e) as q1 (u), q2(v) respectively, and the transition
probabilities are
Pc1 (u, v)
Σ2e∈E w(e)ρ(δ(e))Q1(u, e)Q2(v, e) _ q1(u)q2(v) fe∈E w(e)ρ(δ(e))H(u, e)H(v, e)
e∈E w(e)ρ(δ(e)δ(e)Q1 (u, e)
q1 (u)	e∈E w(e)ρ(δ(e)δ(e)H(u, e)
q2(u)q2(v) Ee∈e w(e)ρ(δ(e))H(u, e)H(v, e) _ Ee∈e w(e)ρ(δ(e))Q2(u, e)Q2(v, e)
q2(u) Pe∈E w(e)ρ(δ(e)δ(e)H(u, e)	一 Pe∈E w(e)ρ(δ(e)δ(e)Q2(u, e)
(49)
It can be observed that Pc1(u, v) = Pc2(u, v), ∀u, v ∈ V. By definition 3, the Lemma 4 holds.
Next, based on Lemma 4, we prove the following main conclusion.
Corollary 1 Let H(V, E, W, Qi, Q2) be the generalized hypergraph in Definition 2. Let D)V be a
|V| × |V| diagonal matrix with entries Dv (v, v) := d(v) := e∈E w(e)δ(e)ρ(δ(e))Q2(v, e). No
matter H satisfies condition (1) or condition (2) in Thm. 1, it obtains the unified explicit form of
stationary distribution π and Laplacian matrix L as:
π
1>D V
1> D V1
and L = I - D-1/2Q2 Wρ(De)Q>D-1/2.
(5)
Proof of Corollary 1.
1)	Proof based on Lemma 1 When the generalized hypergraph satisfies Theorem 1, we obtain
the equation 34 and equation 35 in Appendix G.2. substituting the equations into equation 33 and
equation 32, we can obtain the Corollary 1.
2)	Proof based on Lemma 4 We know whether H satisfying condition (1) or condition (2) in
Theorem 1, there exists a weighted undirected clique graph GC equivalent to H. Recall Lemma 4,
when H satisfies any of the two conditions, the probability transition matrix P of H can be denoted
as:
P(u, v)
Ee∈E We)P(SQ))Q2(U, e)Q2(V, e)
Pe∈E w(e)ρ(δ(e)δ(e)Q2(u, e)
Then, we obtain the unified form of probability transition matrix P under any of the two conditions:
P = D-1Q2Wρ(De)Q>
(50)
where DV is a |V| × |V| diagonal matrix with entries DV (v, v) = e∈E w(e)δ(e)ρ(δ(e))Q2(v, e)
Remark the P is also the probability transition matrix of the equivalent undigraph in Theorem 1. Due
to the symmetric adjacency matrix underlying P (i.e. Q2Wρ(De)Q2> is symmetrical), it is easy to
derive the unified stationary distribution π by 1>Dv P = 1>Dv. Thus, the stationary distribution
is:
1>DV	/()= d(u) =	Pe∈E w(e)δ(e)P(δ(e))Q2(u, e)
1>Dv 1	∖ U	PV d(v)	PV Pe∈E w(e)δ(e)P(δ(e))Q2(v,e)
(51)
32
Under review as a conference paper at ICLR 2022
Substitute P and π into equation 20, we finally get the unified hypergraph Laplacian L as:
L = I — D-1/2Q2Wp(De)Q>D-1/2
Remark. The stationary distribution with edge-independent vertex weights in Chitra & Raphael
(2019) can be seen as a special case of π in equation 51 by setting Q1 = H, Q2 (U, e) = γ(U) and
ρ(∙) = (∙)-1:
Y(U) Ee∈E W⑹H(U,e)
γ(u)dCh(u)
πCh (U)
v γ(v) e∈E w(e)H(v, e)	v γ(v)dCh(v)
where dCh(U) =	e∈E w(e)h(U, e) is the definition of vertex degree in Chitra & Raphael (2019).
G.7 Spectral Range of Laplacian Matrix
The following Theorem proves the upper bound for eigenvalues of L which meets the requirement
of Chebyshev polynomials to enable the derivation for our proposed GHCN in Appendix F.1 and
leads to Corollary . 2 in the paper.
Theorem 5 Let L = I 一 D-1/2Q2 Wp(De)Q>DD-1/2 denote the unified hypergraph Laplacian
matrix in Corollary 1 and λ1 ≤ λ2 ≤ ∙∙∙ ≤ λn denote the eigenvalues of L in order.
1 •一	一 一
∖	∖	(^> 7	ɪʌ 互 T/,? ♦	，	∙ . 1 ∙ ∖ X
λmin = λι = 0 and uι = Dv 1 (the eigenvector associated With λι)
For k = 2, 3,…，n, we have
1)
2)
3)
Proof:
Pe,u,v β(e, U, V)(f(U) 一 f(V))2
λk = f ⊥Din S …-----Pvf(Vyd(V)----------,
f ⊥Dv Sk-1
here, Sk-1 is the subspace SPanned by eigenvectors {u1, •一，Uk-1} where Ui related to
λi and β(e, U, V) = w(e)ρ(δ(e))Q2 (V, e)Q2(U, e).
λmax = λn ≤ 2
To analyze the generalized Laplacian proposed in our work, we use the Rayleigh quotient
mentioned in the Lemma 3 to give out some conclusions about the eigenvalues and eigenvectors of
the Laplacian.
1)	Deduce the Rayleigh quotient of L2:
g>Lg _ g>(I- D-1 Q2Wρ(De)Q>D-1 )g
g>g
g>g
(D V f )>(I- D -1 Q2Wp(De)Q>D - 2)(D V2 f)
(D V2 f )>(D V2 f)
f>(Dv - QWP(De)Q>)f
一-I- ^	~
f>D V f
Pu Pv Pe w(e)ρ(δ(e))Q2(v, e)Q2(U, e)(f (U) - f (v))2
2 Pv f(v)2d(v)
where g = Dv/2f and f ∈ RlVl×1. It is easy to see the fact: R(L,g) ≥ 0 and R(L,g) = 0 if
and only if f = 1 (i.e. g = Dv/21), Then with Lemma 3 we get λ1 = λmin = min R(L, g)=
g
R(L, DV/21) = 0 and u1 = DV2 L
2)	From Courant-Fischer theorem (Horn, 1986), We have:
λk = inf R(L, g) = inf g Jg
k g⊥Sk-1	,	g⊥Sk-1 g>g
inf
1
f ⊥Dv2 Sk-1
Pu Pv Pe w(e)ρ(δ(e))Q2(v, e)Q2(U, e)(f(U) - f(v))2
-^
2 Pvf(Vyd(V)
33
Under review as a conference paper at ICLR 2022
3)	We have the fact:
(f(u) - f(v))* 2 ≤ 2(f2(u) + f2(v))
And from Lemma 3, we get:
λn
λmax
R(L,
umax )
inf
1
f ⊥Dv2 Sn-1
≤ inf
f ⊥D2 Sn-1
Pu Pv Pe w(e)ρ(δ(e))Q2(u, e)Q2(v, e)(f (u) - f (v))2
-^
2 Pv f(v)2d(v)
Pu Pe w(e)ρ(δ(e))δ(e)Q2(u, e)f (u)2 + Pv Pe w(e)ρ(δ(e))δ(e)Q2(v, e)f (v)2
_______________________________ _ ^
2 Pvf(Vyd(V)
≤ inf 2Pu d(υ)f2(υ)
~D⊥D 1 Sn-1 Pv ⅜)f2(v)
2
G.8 Proof of Corollary 2
Corollary 2 LetH(V, E, W, Q1, Q2) be the generalized hypergraph in Definition 1. When H satis-
fies any of two conditions in Thm. 1, let L and π be the hypergraph Laplacian matrix and stationary
distribution from Corollary 1. Let λH denote the smallest nonzero eigenvalue of L. Assume an
initial distribution f with f(i) = 1 (f(j) = 0, ∀ j 6= i) which means the corresponding walk starts
from vertex Vi. Let p(k) = fPk be the probability distribution after k steps unified random walk
where P denotes the transition matrix, then p(k) (j) denotes the probability of finding the walker in
vertex Vj after k steps. We have:
*(j) - ∏(j)l≤ √fy(1 - 5.
(6)
Proof: From equation 50:
P = D-1QWP(De)QT = D-1 (I - L)Dv
Then we have:
ITD V P = ITD V
Then the stationary distribution π ∈ R1×lVl of P can be denoted as:
ITD v
π =------
c
(52)
1	i ^T ɪʌ i	K ~ʌ	1/ ∖ , ZA ∙ ,1	CT-T ɪʌ ɪ , ʌ ,	,	/ 、 F , , 1
,where C = 11 DV1 = 52v∈v d(v) > 0 is the sum of 11 Dv. Let λι ≤ λ2 ≤ ∙∙∙ ≤ λn denote the
ʌ _1	n
eigenvalues of L in order. Then We assume that fDV 2 = P aiU> ∈ R1×lVl, where Ui ∈ RlVl×1
i=1
denotes the l2-norm orthonormal eigenvector related with 入” From Thm .5, we know U1 = 湍% =
-1
D√c1. Then a` can be computed as:
U T(fD -1 )τ =屯 T CfD - 2 ))T = (D n )τ(fD-1 )τ = _1
u T ∙ u 1	= 1 ( V ) =	√	= √
34
Under review as a conference paper at ICLR 2022
with the fact that f ∈ R1×lVl is a probability distribution(i.e. fl = 1).
Then,
>
∣Pj (k)-∏j I = ∣(fPk)j - ∏j∣= (fPk - -Dv )j =∣(fPk - aιU >D V )j∣
∖(fD - 2 (I - L)k D V - aιU >D V )j∣ = (X ai U > (I - L)kD V - aιU >D V )j
∣ i=1
n
(X aiU>(1 - λi)kDV - aιU>DV)j
i=1
(1 -
≤ (1 - λH)k 西 ∣(X aiU> )j
∣ i=1
H Generalized Hypergraph Partition
For arbitrary vertex subset S ⊂ V, Sc denotes the compliment of S. We define the hyperedge
boundary ∂S as ∂S := {e ∈ E | e ∩ S = 0, e ∩ SC = 0}. We will generalize the hypergraph par-
tition problem in Zhou et al. (2006) to our unified hypergraph framework. Hypergraph partition is
to cut vertex set V into two parts S and SC . As shown in Theorem 3, we are inspired to adopt the
custom of directed graph normalized cut to formulate the hypergraph partition in a mathematical
expression. We need to reuse the definition of vol(S) and vol(∂S) for directed graphs as:
vol(S ) =	π(u), vol(∂S ) =ΣΣπ(u)P (u, v)
u∈S	u∈S V∈Sc
And the objective function of the hypergraph partition can be expressed as:
argmin c(S)
0=S⊂V
Vol dSQ⅛ + V0I⅛C)).
(53)
As we need the explicit form of stationary distribution to formulate this problem, we set the unified
random walk on a hypergraph that satisfies with the conditions in Thm. 1. Then we have:
.7/ . ∖
Vol(S) = ⅛Vul,
vol(dS) = volɪ ) X X X w(e)p(S(e))Q2(u,e)Q2(v,e)
e∈∂S u∈e∩S V∈e∩Sc
=	:V) X w(e)ρ(δ(e))m(e ∩ S)m(e ∩ SC)
Vol(V) e∈∂S
where m(e∩ S) = Pu∈e∩S Q2(u, e) is a measure on the subset of V which explicitly demonstrates
the difference between Zhou et al. (2006) (where m(e ∩ S) = Pu∈e∩S H(u, e)) and our work.
We considerate more fine-grained information on vertices when we establish a measure on arbitrary
subset of Vand involve the degree of hyperedges to impact the corresponding hyperedge weights
rather than use the a priori hyperedge weights.
I Details of Experiments
Note that our GHCN and SHSC are both based on the Laplacian led by the equivalent condition
in Thm .1, which means the vertex weights we use in our experimental datasets should satisfy the
condition (1) or condition (2) in Thm. 1.
35
Under review as a conference paper at ICLR 2022
I.1	Hyper-parameter Strategy
We use grid search strategies to adjust the hyper-parameters of our GHCN and SHSC. The range of
hyper-parameters listed in Table 6, Table 7, Table 8, and Table 9.
Table 6: Hyper-parameter search range for citation network classification and visual object classification.
Methods	Hyper-parameter	Range
GHCN	σ Y(ViSUal object classification) Learning rate Hidden dimension Layers Weight decay Dropout rate Optimizer Epoch Early stopping patience GPU	{-2,-1,-0.5,0,0.5,1,2} {0.1,0.2,0.4, 0.5,0.8,1.0} {0.001,0.005, 0.01} {64,128} {2,4} {1e-3,1e-4,5e-4,1e-5} {0.1,0.2,0.3,0.4, 0.5} Adam 1000 100 GTX 2080Ti
SHSC	ɑ β γ(visual object classification) Learning rate Hidden dimension Layers Weight decay Dropout rate Optimizer Epochs Early stopping patience GPU	{1,0.98,0.95,0.93,0.92,0.9,0.85,0.8} {1,0.95,0.9,0.85,0.80, 0.75} {0.1,0.2, 0.4, 0.5,0.8,1.0} {0.001,0.005, 0.01} {64,128} {2,4,6,8,16,32,64 } {1e-3,1e-4, 1e-5} {0.1,0.2,0.3,0.4, 0.5} Adam 1000 100 GTX 2080Ti
Table 7: Hyper-parameter search range for GHCN and SHSC model in fold classification.
Methods ∣ Hyper-parameter	∣ Range	best
GHCN	Amino-acids type input size Secondary-structure input size Readout layer input size Layers L Batch size Weight decay Learning rate Dropout rate σ Y Optimizer Epochs Early stopping patience GPU	{32,64,128}	64 {32,64,128}	64 {256,512,1024}	1024 {2,3,4,5,6}	4 {64,128,256}	128 {1e-3,1e-4, 1e-5}	1e-3 {0.0001,0.001,0.005,0.01}	0.0001 {0.1,0.2,0.3,0.4,0.5}	0.2 {-1}	-1 {0.1,0.2,,0.4,0.5,0.8,1.0}	0.1 Adam	- 300	- 30	- GTX 2080Ti	-
SHSC	Amino-acids type input size Secondary-structure input size Readout layer input size Layers L Batch size Weight decay Learning rate Dropout rate σ Y α β Optimizer Epochs Early stopping patience GPU	{32,64,128}	64 {32,64,128}	64 {256,512,1024}	512 {2,4,6,8,12,16,32}	12 {64,128,256}	64 {1e-3,1e-4, 1e-5}	1e-5 {0.0001,0.001,0.005,0.01}	0.0001 {0.1,0.2,0.3,0.4,0.5}	0.3 {-1}	-1 {0.1,0.2,,0.4,0.5,0.8,1.0}	0.4 {i,0.97,0.95,0.9,0.85,0.8,0.75,0.7,0.65}	0.97 { 1,0.95,0.90,0.85,0.8 }	0.85 Adam	- 300	- 30	- GTX 2080Ti	-
Extra parameter settings for Visual Object Classification include k = 6.
36
Under review as a conference paper at ICLR 2022
Table 8: Hyper-parameter search range for GHCN and SHSC model in protein Quality Assess-
ment (CASP10,CASP11,CASP13 for training, CASP12 for testing).
Methods	Hyper-parameter	Range	best
	Amino-acids type input size	{128,256,512}	512
	Secondary-structure input size	{32,64,128}	64
	Readout layer input size	{256,512,1024}	1024
	Layers L	{2,3,4,5,6}	4
	Batch size	{64,128,256}	64
	Weight decay	{1e-3,1e-4, 1e-5}	1e-5
GHCN	Learning rate	{0.0001,0.001,0.005, 0.01}	0.0001
	Dropout rate	{0.1,0.2,0.3,0.4, 0.5}	0.5
	σ	{-1}	-1
	Y	{0.1,0.2,,0.4,0.5,0.8,1.0}	0.1
	Optimizer	Adam	-
	Epochs	200	-
	Early stopping patience	20	-
	GPU	Tesla P40	-
	Amino-acids type input size	{128,256,512}	512
	Secondary-structure input size	{32,64,128}	128
	Readout layer input size	{256,512,1024}	256
	Layers L	{ 2,4,6,8,12,16,18,32}	18
	Batch size	{64,128,256}	128
	Weight decay	{1e-3,1e-4, 1e-5}	1e-5
	Learning rate	{0.0001,0.001,0.005, 0.01}	0.0005
SHSC	Dropout rate	{0.1,0.2,0.3,0.4, 0.5}	0.3
	σ	{-1}	-1
	Y	{0.1,0.2,,0.4,0.5,0.8,1.0}	0.2
	α	{i,0.97,0.95,0.9,0.85,0.8,0.75,0.7,0.65,0.6}	0.65
	β	{ 1,0.95,0.90,0.85,0.8 }	0.95
	Optimizer	Adam	-
	Epochs	200	-
	Early stopping patience	20	-
	GPU	Tesla P40	-
I.2	Baselines of Spectral Convolutions
For HyperGCN (Yadati et al., 2019), we reproduce it by public code with the data already split (10
splits). But the result in its original paper is via running 1000 times with randomly split to get the
best 100 times, which is not very common. Therefore, despite the HyperGCN is the SOTA method
on Table 10, it can not achieve a competitive performance under the 10-splits.
For HGNN (Feng et al., 2019), in addition to the flaw during the derivation (i.e. use a specific
matrix to fit the scalar parameter θ0 ), when doing the Visual Object Classification experiment in
their public code, it also takes Gaussian kernel as the incidence matrix for hypergraph learning,
which lacks theoretical guarantee. So in our experiment, we just use the incidence matrix H which
only consists of elements {0, 1} as the incidence matrix using in HGNN.
HGNN and HyperGCN we use the same grid search strategy as our method GHCN.
I.3	Citation Network Classification
Datasets. The datasets we use for citation network classification include co-authorship and co-
citation datasets: PubMed, Citeseer, Cora (Sen et al., 2008) and DBLP (Rossi & Ahmed, 2015). We
adopt the hypergraph version of those datasets directly from Yadati et al. (2019), where hypergraphs
are created on these datasets by assigning each document as a node and each hyperedge represents
(a) all documents co-authored by an author in co-authorship dataset and (b) all documents cited
together by a document in co-citation dataset. The initial features of each document (vertex) are
represented by bag-of-words features. The details about vertices, hyperedges and features are shown
in Table 10.
37
Under review as a conference paper at ICLR 2022
Table 9: Hyper-parameter search range for GHCN model in protein Quality Assessment (CASP10-12 for train-
ing, and CASP13 for testing).
Methods ∣ Hyper-parameter	∣ Range	best
GHCN	Amino-acids type input size Secondary-structure input size Readout layer input size Layers L Batch size Weight decay Learning rate Dropout rate σ Y Optimizer Epochs Early stopping patience GPU	{128,256,512}	128 {32,64,128}	64 {256,512,1024}	1024 {2,3,4,5,6}	4 {64,128,256}	1024 {1e-3,1e-4, 1e-5}	1e-5 {o.0001,0.001, 0.005, 0.01}	0.001 {o.1,0.2,0.3, 0.4, 0.5,0.6,0.7}	0.7 {-1}	-1 {0.1,0.2,,0.4,0.5,0.8,1.0}	0.1 Adam	- 200	- 20	- Tesla P40	-
SHSC	Amino-acids type input size Secondary-structure input size Readout layer input size Layers L Batch size Weight decay Learning rate Dropout rate σ Y α β Optimizer Epochs Early stopping patience GPU	{128,256,512}	128 {32,64,128}	64 {256,512,1024}	1024 {2,4,6,8,12,16,18,32}	8 {64,128,256}	1024 {1e-3,1e-4, 1e-5}	1e-5 {o.0001,0.001, 0.005, 0.01}	0.001 {0.1,0.2,0.3, 0.4, 0.5}	0.2 {-1}	-1 {0.1,0.2,,0.4,0.5,0.8,1.0}	0.2 {i,0.97,0.95,0.9,0.85,0.8,0.75,0.7,0.65,0.6}	0.65 { 1,0.95,0.90,0.85,0.8 }	0.95 Adam	- 200	- 20	- Tesla P40	-
Table 10: Real-world hypergraph datasets used in our citation network classification task.
Dataset	# vertices	# Hyperedges	# Features	# Classes	# isolated vertices
Cora (co-authorship)	2708	1072	1433	7	320(11.8%)
DBLP (co-authorship)	43413	22535	1425	6	0 (0.0%)
Pubmed (co-citation)	19717	7963	500	3	15877 (80.5%)
Cora (co-citation)	2708	1579	1433	7	1274 (47.0%)
Citeseer (co-citation)	3312	1079	3703	6	1854 (55.9%)
Settings and baselines. We adopt the same dataset and train-test splits (10 splits) as provided by
in their publically available implementation1. Note that this dataset just has the edge-independent
vertex weights H, which is also called the incidence matrix. So this experiment can be regarded
as a special case of the specific application of our model (i.e. Q = H). For baselines, we include
Multi-layer perceptron (MLP) which can be considered as the model without hypergraph struc-
ture, current state-of-the-art message-passing hypergraph framework UniGNN (Huang & Yang,
2021) and two recent spectral-based hypergraph convolutonal networks: Hypergraph neural net-
works(HGNN) (Feng et al., 2019) and HyperGCN (Yadati et al., 2019). For UniGNN, we reuse
the results of UniGCN reported in (Huang & Yang, 2021), and for HGNN and HyperGCN, we
reproduce them following Appendix I.2.
We use cross-entropy loss and Adam SGD optimizer with early stopping with patience of 100 epochs
to train GHCN and SHSC. For other hyper-parameters, we use the grid search strategy. It is worth
noting that the layer number of our model is K in equation 8. More details of hyper-parameters can
be found in Table 6.
38
Under review as a conference paper at ICLR 2022
Table 11: summary of the ModelNet40 and NTU datasets
Dataset ∣ ModelNet40 ∣ NTU
Objects	12311	2012
MVCNN Feature	4096	4096
GVCNN Feature	2048	2048
Training node	9843	1639
Testing node	2468	373
Classes	40	67
I.4	Visual Object Classification
Datasets and Settings. We employ two public benchmarks: Princeton ModelNet40 dataset (Wu
et al., 2015) and the National Taiwan University (NTU) 3D model dataset (Chen et al., 2003), as
shown in Table 11.
In this experiment, each 3D object is represented by the feature vectors which are extracted by
Multi-view Convolutional Neural Network (MVCNN) (Su et al., 2015) or Group-View Convolu-
tional Neural Network (GVCNN) (Feng et al., 2018). The features generated by different methods
can be considered as multi-modality features. The hypergraph structure we designed is similar to
Zhang et al. (2018)( but they did not give spectral guarantees for supporting the rationality of their
practices). We represent the hypergraph structure as a edge-dependent vertex weight Q to satisfy the
condition (2) in Thm. 1 (i.e. Q1 = Q2 = Q). Specifically, we firstly generate hyperedges by k-NN
approach, i.e. each time one object can be selected as centroid and its k nearest neighbors are used
to generate one hyperedge including the centroid itself (in our experiment, we set k = 10). Then,
given the features of data, the vertex-weight matrix Q is defined as
Q(v,e)=(exP(-(F))，ifV ∈ °
0,	otherwise,
(54)
where d(v, vc ) is the euclidean distance of features between an object v and the centroid object vc
in the hyperedge and d is the average distance between objects. γ is a hyper-parameter to control
the flatness. Because we have two-modality features generated by MVCNN and GVCNN, we can
obtain the matrix Q{i} which corresponds to the data of the i-th modality (i ∈ {1, 2}). After all
the hypergraphs from different features have been generated, these matrices Q{i} can be concate-
nated to build the multi-modality hypergraph matrix Q = [Q{1}, Q{2}]. The features generated by
GVCNN or MVCNN can be singly used, or concatenated to a multi-modal feature for constructing
the hypergraphs.
For baselines, we just compare with HGNN method for multi-modality learning, following the set-
tings of Appendix I.2. We also compare our methods using two-modality features to recent SOTA
methods on ModelNet40 dataset. And we use the datasets provided by its public Code 1 2. We use
cross-entropy loss and Adam SGD optimizer with early stopping with patience of 100 epochs to
train GHCN and SHSC. It is worth noting that the layer number of our model is K in equation 8.
More details of hyper-parameters can be found in Table 6.
I.5	Protein Quality Assessment and Fold Classification
Protein hypergraph modeling. At the high level, a protein is a chain of amino acids (residues)
that will form 3D structure by spatial folding. In order to simultaneously consider protein
sequence and spatial structure information, we build sequence hyperedge and distance hyper-
edge. Specifically, given a protein with |S | amino acids, we choose τ consecutive amino acids
(vi, Vi+ι,…,Vi+τ) to connect to form a sequence hyperedge and choose amino acids whose spa-
tial Euclidean distance is less than a threshold > 0 to connect to form a spatial hyperedge, where
Vi(i = 1,…，|S|) represent the i-th amino acid in the sequence. Let Es and Ee denote sequence
hyperedge and spatial hyperedge, respectively. Then, we design an edge-dependent vertex-weight
matrix Q for capturing the more granular high-order relationships of proteins below (actually, our
1https://github.com/malllabiisc/HyperGCN, Apache License
2https://github.com/iMoonLab/HGNN, MIT License
39
Under review as a conference paper at ICLR 2022
Figure 7: An expample of edge-dependent vertex weight matrix Q of protein.
models GHCN and SHSC allow one to design a more comprehensive Q for learning proteins better):
1,
Q(v,e)=	exP( -d⅛vc)),
O, C
if v ∈ e and e ∈ Es
if v ∈ e and e ∈ Ee
otherwise
(55)
where d(v, vc) < is the euclidean distance between an amino acid v and the centroid amino acid
Vc in the hyperedge and dvc is the average distance between Vc and the other amino acids {vi}i=c. Y
is a hyper-parameter to control the flatness. Here we just design an edge-dependent vertex-weights
matrix Q for satisfying the condition (2) in Thm. 1 (i.e. Q1 = Q2 = Q). Note that this Q matrix
pays more attention to the information of the sequence (Figure 7).
Experimental settings In our experiment, we set τ = 6 and = 8A. The initial node features,
following Baldassarre et al. (2020), are composed of amino acid types and 3D spatial features includ-
ing dihedral angles, surface accessibility and secondary structure type generated by DSSP (Kabsch
& Sander, 1983). Note that both Quality Assessment and Fold Classification include graph-level
tasks, which means we should add global pooling layers to readout the node representations from
our GHCN or SHSC.
Here, for the sake of simplicity, we adopt the permutation-invariant operator mean pooling and a
single layer MLP as Readout layers to obtain a global hypergraph embedding and add the softmax
(classification) or sigmoid (regression) activation function before output.
Table 12: Comparison of our method to others on protein Quality Assessment tasks. At the residue level,
We report Pearson correlation across all residues of all decoys of all targets (R) and Pearson correlation all
residues of per decoys and then average all decoys (Rdecoy) with LDDT scores. At the global level, we report
Pearson correlation across all decoys of all targets (R) and Pearson correlation per target and then average
over all targets (Rtarget) with GDT_TS scores.
Test set	Methods	GDT TS		LDDT	
		R	Rtarget	R	Rmodel
	HGNN	0.714	0.622	0.651	0.337
CASP13	GHCN	0.718	0.620	0.657	0.352
	SHSC	0.628	0.565	0.654	0.435
	VoroMQA	-	0.557	-	-
	RWplus	-	0.313	-	-
	3D CNN	-	0.607	-	-
CASP12	AngularQA	0.651	0.439	-	-
	HGNN	0.667	0.582	0.632	0.319
	GHCN (ours)	0.737	0.609	0.656	0.340
	SHSC (ours)	0.760	0.554	0.678	0.449
Protein QA is used to estimate the quality of computational protein models in terms of divergence
from their native structure. It is a regression task aiming to predict how close the decoys to the
unknown, native structure. Inspired by Baldassarre et al. (2020), we train our models on Global Dis-
tance Test Score (Zemla, 2003), which is the global-level score, and the Local Distance Difference
Test (Mariani et al., 2013), an amino-acids-level score. The loss function of QA is defined as the
40
Under review as a conference paper at ICLR 2022
Mean Squared Error (MSE) losses:
|S|
Lg = MSE(Ppred- GDT-TS)	Ll = X MSE(Ppredi- LDDTi)	(56)
i=1
where Ppgred and Pplred denote predicted score of global and local respectively.
I.5.1	Protein Quality Assessment (QA)
Table 13: summary of CASP datasets
Dataset ∣ Targets Decoys ∣ Usage
CASP 10	103	26254	Train
CASP 11	85	12563	Train
CASP 12	40	6924	Test
CASP 13	82	12336	Train
Dataset and settings We use the data from past years’ editions of CASP, including CASP10-
13. We randomly split the CASP10, CASP11, CASP13 for training and validation, with ratio
training: validation = 9:1. CASP 12 is set aside for testing against other methods. More details
about the datasets can be found in Table 13. For the baseline, we compare our methods with
other start-of-the-art methods, including random walk-based methods : RWplus (Zhang & Zhang,
2010), sequence-based methods: AngularQA (Conover et al., 2019), and 3D structrue-based meth-
ods: VOroMQA (Olechnovivc & Venclovas, 2017), 3DCNN (Derevyanko et al., 2018). The results
of these baselines we reused from Baldassarre et al. (2020) reports. Another baseline HGNN (Feng
et al., 2019) is reproduced by us with same training strategy as our methods.
Because our hypergraph based methods can jointly learn node and graph embeddings, the losses in
equation 56 can be weighted as Ltotal = μLl + (1-μ)Lg and co-optimized by Adam Optimizer with
L2 regularization, where μ = 0.5 in our experiment. We use grid search to select hyper-parameters
and more details can be found in Table 8.
In addition, we use CASP10-12 for training and valuation, CASP 13 for testing to further evaluate
the efficiency of our methods (the range of hyper-parameters can see Table 9), and the results are
shown in Table 12.
I.5.2	Protein Fold Classification
Datasets and settings The dataset that we used for training, validation and test is SCOPe 1.75
data set of Hou et al. (2018). This dataset includes 16,712 proteins covering 7 structural classes with
total of 1195 folds. The 3D structures of proteins are obtained from SCOpe 1.75 database (Murzin
et al., 1995), in which each protein save in a PDB file. The datasets have three test sets: 1) Fold,
where proteins from the same superfamily do not appear in the training set; 2) Family, in which
proteins from the same family are not present in the training set; 3) Family, where proteins from the
same family that are present in the training set.
For baselines, we include sequence-based methods pre-trained unsupervised on millions of protein
sequences: Rao et al. (2019); Bepler & Berger (2018); Strodthoff et al. (2020), 3D structure based
model: Kipf & Welling (2017); Diehl (2019); Baldassarre et al. (2020) and Gligorijevic et al. (2020),
who process the sequence with LSTM first and then apply GCNN. The accuracy of the above base-
lines is reused from Hermosilla et al. (2021) reported. We adopt Adam optimizer to minimize the
cross-entropy loss of our methods. Hyper-parameters search range can see Table 7.
I.6	Over-Smoothing Analysis.
Table 14 shows that our proposed SHSC significantly alleviates the performance descending with
the increase of layers. Given the same layers, it can be observed that our SHSC almost outperforms
the other methods for all cases, especially at a deep layer, which demonstrates the benefits of deep
model and the long-range information around hypergraph. Furthermore, It should be noted that the
optimal layer numbers K of our SHSC are generally larger than other models, due to the only one
41
Under review as a conference paper at ICLR 2022
linear layer avoids the over-fitting problem. These results also reveal that HyperGCN, HGNN and
our GHCN all suffer from severe over-smoothing issue, limiting the power of neural network to
capture high-order relationships.
Table 14: Summary of classification accuracy (%) results with various depths. In our SHSC, the number of
layers is equivalent to K in equation 8. We report mean test accuracy over 10 train-test splits.
Dataset	Method	2	4	Lay 8	ers 16	32	64
	HyperGCN	60.66	57.50	31.09	31.10	30.09	31.09
Cora (co-authorship)	HGNN	69.23	67.23	60.17	29.28	27.15	26.62
	GHCN (ours)	74.79	72.86	63.99	31.03	30.46	31.09
	SHSC (ours)	74.60	75.78	75.70	75.04	75.26	74.79
	HyperGCN	84.82	54.65	22.37	23.96	23.04	24.13
DBLP (co-authorship)	HGNN	88.55	88.28	85.38	27.64	27.62	27.56
	GHCN (ours)	89.04	88.90	85.15	27.61	27.61	27.62
	SHSC (ours)	86.63	88.26	89.00	89.17	89.05	88.60
	HyperGCN	62.35	58.29	31.09	31.17	31.09	29.68
Cora (co-citation)	HGNN	55.60	55.72	42.10	26.16	24.40	24.43
	GHCN (ours)	69.03	69.45	57.37	28.21	26.27	26.95
	SHSC (ours)	62.21	64.57	67.59	68.96	69.37	68.15
	HyperGCN	68.12	63.59	39.99	39.97	40.01	40.02
Pubmed (co-citation)	HGNN	46.41	47.16	40.93	40.24	40.30	40.29
	GHCN (ours)	75.37	74.76	60.65	40.38	40.31	40.42
	SHSC (ours)	74.39	74.91	74.41	73.90	72.79	71.49
	HyperGCN	56.94	36.75	20.72	20.41	20.16	18.95
Citeseer (co-citation)	HGNN	39.93	38.98	36.67	19.91	19.86	19.79
	GHCN (ours)	62.67	61.50	49.94	21.95	21.84	21.93
	SHSC (ours)	61.63	62.75	63.86	64.62	65.14	65.10
I.7	Sensitivity Analysis
The proposed SHSC model performance on co-authorship Cora, co-citation cora and co-citation
Pubmed with different σ, β and α is reported in Figure 8. For σ, we can see that the best choice
will vary depending on the data set but mainly concentrated around -0.5, which verify that the
effect of hyperedges degree is various and it has a negative effect in most cases. For β, with the
growth of β, the performance of SHSC stably increases, which means that the diffusion kernel
is useful for information aggregation. Moreover, the fact that the performance remains stable on
cora (coauthorship) and pubmed (cocitation) when β is at 0.8-1.0 suggests we can only adjust this
hyper-parameter at a range of large value. For α, the tendency of it is similar to β , so we can also
adjust this hyper-parameter at a range of large value to obtain a satisfying performance.
Figure 8: Test accuracy by varying the hyper-parameters σ (left), β (middle) and α (right) of SHSC.
I.8	The Spectrum Analysis of SHSC
We calculate the eigenvalues of (β PK=I OKkTk + (1 - β)I)With various (α,β) on NTU2012
dataset (|V| = 2012) and count the number of eigenvalues in different size ranges, which is shown
42
Under review as a conference paper at ICLR 2022
in the Table 15. The table suggests that our SHSC is able to capture both low and high frequency
information of the graph signal, depending on the selection of the appropriate hyper-parameters
(α, β).
Table 15: The number of eigenvalues of SHSC in different size ranges on NTU2012 dataset.
α, β	λ ≥ 0.0	λ≥0.1	λ≥0.2	λ≥0.3	λ ≥ 0.4	λ ≥ 0.5	λ ≥ 0.6	λ ≥ 0.7	λ ≥ 0.8	λ ≥ 0.9
(1,1)	2012	364	163	108	82	66	52	38	26	13
(1,0.8)	2012	327	140	90	71	51	33	19	1	0
(0.8,1)	2012	161	40	0	0	0	0	0	0	0
(0.8,0.8)	2012	139	8	0	0	0	0	0	0	0
I.9	Ablation Analysis
In order to verify the effectiveness of our proposed edge-dependent vertex weight Q and the neces-
sity of re-normalization trick, we conduct an ablation analysis and report the results in Table 16.
Specially, w/o Q is a variant of our methods that replaces the edge-dependent vertex weight Q with
the edge-independent vertex weight matrix H. w/o renormalization represents the variants without
re-normalization trick. Form the reported results we can learn that both Q and renormalization are
efficient for hypergpraph learning, respectively. Moreover, on Cora and Pubmed, the performance
gap between w/o renormalization and with both GHCN reveals the advantage of renormalization
on disconnected hypergraph dataset. This phenomenon is mainly caused by the row in the adjacent
matrix corresponding to an isolated point is 0 (Figure 4), resulting in a direct loss of its vertex in-
formation (Figure 4). And the renormalization trick adding the self-loop to matrix K can maintain
the features of isolated vertices during aggregation. Compared with GHCN, the performance impact
of renormalization on SHSC seems to be smaller. This is because we add initial features to the
information gathered by the neighbors, which reduces the information loss of isolated vertices.
Table 16: Test accuracy (%) of our methods for ablation analysis. We report mean ±standard deviation. Cora
and Pubmed are the datasets that do not contain Q, so we just report the w/o renormalization results. NTU and
ModelNet40 constructed by Feng et al. (2019) are both connected hypergraph networks in this work.
methods	-	Cora (co-authorship)	Pubmed (co-citation)	NTU	ModelNet40
	with both	74.79±0.91	75.37±1.2	85.15±0.34	97.28±0.15
GHCN	w/o Q	-	-	84.93±0.31	97.18±0.20
	w/o renormalization	69.23±1.6	46.41±0.70	84.85±0.30	97.20±0.15
	w/o both	-	-	84.21±0.25	97.15±0.14
	with both	75.91±0.75	74.60±1.4	83.35±0.30	97.74±0.05
SHSC	w/o Q	-	-	82.84±0.40	97.65±0.08
	w/o renormalization	75.76±0.69	73.83±2.1	82.92±0.47	97.74±0.05
	w/o both	-	-	82.55±0.46	97.69±0.07
I.10	EXAMPLES OF ρ
We give many examples for ρ, including random, x-1, Sigmoid(X) ,√√1-exp(- (x-χ) ), log(x)
,exp(x) and exp(-x), and the experimental results on citation networks can be seen the table 17 .
Regarding learnable ρ, we will do as a future work.
Table 17: classifificaiton accuracy(standard deviation) with different ρ
dataset	model	random	XT	SigmOid(X)	√2σ exp(-⅛x)2)	IOg(X)	exp(x)	exp(-x)
pubmed	GHCN	0.74(±0.01)	0.75(±0.01)	0.74(±0.02)	0.74(±0.01)	0.74(±0.02)	0.73(±0.02)	0.73(±0.01)
pubmed	SHSC	0.75(±0.01)	0.75(±0.01)	0.75(±0.01)	0.75(±0.01)	0.75(±0.01)	0.74(±0.01)	0.74(±0.01)
cora	GHCN	0.72(±0.02)	0.73(±0.01)	0.72(±0.02)	0.72(±0.01)	0.71(±0.02)	0.71(±0.01)	0.60(±0.02)
cora	SHSC	0.72(±0.01)	0.73(±0.01)	0.72(±0.01)	0.73(±0.01)	0.72(±0.01)	0.72(±0.01)	0.64(±0.01)
43
Under review as a conference paper at ICLR 2022
I.11	Running Time and Computational Complexity
Firstly, we analyze the theoretical computational complexity of our GHCN and SHSC: For GHCN,
the computational cost is the O(|E|d), where |E| is the total edge count in equivalent undigraph.
Each sparse matrix multiplication TX costs |E |d. And the computational of HGNN is the same
as GHCN. For SHSC, the computational cost is the O(K|E|d + K |V |d), which includes K sparse
matrix multiplication and K summation over filters(|V |d is the cost of adding features X). Then, we
compare the running time between our GHCN and SHSC with existing models in Table 18 and the
results illustrate that our methods are of the same order of magnitude as SOTA’s approach UniGNN
and outperform the HyperGCN and HGAT.
Table 18: The average training time per epoch with different methods on citation network classification task is
shown below and timings are measured in seconds. The float in parentheses is the standard deviation.
Methods ∣	cora coauthorshiP	I dblp coauthorship	I Cora cocitation	I pubmed cocitation ∣	citeseer cocitation
HyPerGCN	0.150(±0.058)	1.181(±0.071)	0.151(±0.029)	1.203(±0.104)	0.130(±0.029)
HGNN	0.005(±0.002)	0.081(±0.006)	0.005(±0.040)	0.008(±0.002)	0.005(±0.002)
UniGNN	0.014(±0.044)	0.042(±0.040)	0.014(±0.042)	0.023(±0.043)	0.0168(±0.043)
HNHN	0.001(±0.0026)	0.007(±0.014)	0.001(0±0.004)	0.009(±0.006)	0.001(±0.003)
ChebNet	0.027(±0.005)	0.063(±0.001)	0.073(±0.008)	0.050(±0.019)	0.067(±0.0198)
SSGC	0.055(±0.001)	0.291(±0.001)	0.205(±0.003)	0.135(±0.056)	0.193(±0.057)
HGAT	0.381(±0.080)	OOM	0.279(±0.083)	1.329(±0.016)	0.286(±0.087)
GHCN(ours)	0.005(±0.036)	0.020(±0.079)	0.005(±0.039)	0.011(±0.075)	0.081(±0.091)
SHSC(ours)	0.016(±0.010)	0.072(±0.001)	0.038(±0.005)	0.011(±0.002)	0.027(±0.001)
I.12 The edge-dependent vertex weights visualization
I.12.1 Visual Object.
Figure 9: The edge-dependent vertex weights of NTU2012 dataset (MVCNN feature + MVCNN structure). The
figure on the left represents the distance-based vertex weights matrix Q used in our GHCN and SHSC. The
middle (Q1 ) and right (Q2) denote the node-level and edge-level attention coefficient matrix of HGAT (Ding
et al., 2020), respectively.
44
Under review as a conference paper at ICLR 2022
I.13 Additional Experiments of Baselines.
Table 19: Summary of classificaiton accuracy(%) results. We report the average test accuracy and its standard
deviation over 10 train-test splits. The number in parentheses corresponds to the number of layers of the model.
(OOM: our of memory)
Dataset	Architecture	Cora (co-authorship)	DBLP (co-authorship)	Cora (co-citation)	Pubmed (co-citation)	Citeseer (co-citation)
MLP	-	52.02±1.7	78.72±0.6	52.02±1.7	69.86±1.6	55.03±1.3
HyperGCN	spectral-based	60.66±10.8	84.82±9.7	62.35±9.3	68.12±9.7	56.94±6.3
HGNN	spectral-based	69.23±1.6	88.55±0.18	55.60±1.8	46.41±0.7	38.98±1.1
HNHN	message-passing	63.95±2.4	84.43±0.3	41.59±3.1	41.94±4.7	33.60±2.1
HGAT	message-passing	65.42±1.5	OOM	52.21±3.5	46.28±0.53	38.32±1.5
UniGNN	message-passing	75.30±1.2	88.80±0.2	70.10±1.4	74.40±1.0	63.60±1.3
ChebNet	spectral-based	42.27±2.2	85.81±5.22	45.55±2.37	65.73±1.3	48.44±0.91
APPNP	spectral-based	68.25±1.9	86.99±0.33	64.50±1.7	72.02±1.4	55.05±2.0
SSGC	spectral-based	72.04±1.2	88.61±0.16	68.79±2.1	74.49±1.3	60.52±1.7
GHCN (ours)	spectral-based	74.79±0.91	89.04±0.19	69.45±2.0	75.37±1.2	62.67±1.2
SHSC (ours)	spectral-based	76.05±0.75(6)	89.17±0.21(16)	70.64±1.8 (32)	75.08±1.1(4)	65.14±0.97(32)
Table 20: Test accuracy on visual object classification. Each model we ran 10 random seeds and report the
mean ± standard deviation. BOTH means GVCNN+MVCNN, which represents combining the features or
structures to generate multi-modal data.
Datasets	Feature Structure	HGNN	UniGNN	HGAT	ChebNet	SSGC GHCN(ours) SHSC(ours)
NTU	MVCNN MVCNN GVCNN GVCNN BOTH BOTH	80.11±0.38	75.25±0.17	80.40±0.47	72.17±3.0	81.23±0.24	81.37±0.63	82.56±0.39 84.26±0.30	84.63± 0.21	84.45±0.12	82.52±1.0	84.26±0.12	85.15±0.34	83.35±0.30 83.54±0.50	84.45±0.40	84.05±0.36	79.17±1.8	84.13±0.34	84.45±0.40	85.12±0.25
Model- Net40	MVCNN MVCNN GVCNN GVCNN BOTH BOTH	91.28±0.11	90.36±0.10	91.29±0.15	85.81±5.2	91.21±0.11	91.99±0.16	92.01±0.08 92.53±0.06	92.88±0.10	92.44±0.11	92.08±0.17	92.74±0.04	92.66±0.10	92.69±0.06 97.15±0.14	96.69±0.07	96.44±0.15	85.98±17	97.07±0.07	97.28±0.15	97.78±0.03
45