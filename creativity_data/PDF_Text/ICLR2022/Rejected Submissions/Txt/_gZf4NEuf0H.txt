Under review as a conference paper at ICLR 2022
Towards Understanding the Condensation of
Neural Networks at Initial Training
Anonymous authors
Paper under double-blind review
Ab stract
Implicit regularization is important for understanding the learning of neural net-
works (NNs). Empirical works show that input weights of hidden neurons (the
input weight of a hidden neuron consists of the weight from its input layer to the
hidden neuron and its bias term) condense on isolated orientations with a small
initialization. The condensation dynamics implies that the training implicitly reg-
ularizes a NN towards one with much smaller effective size. In this work, we
utilize multilayer networks to show that the maximal number of condensed orien-
tations in the initial training stage is twice the multiplicity of the activation func-
tion, where “multiplicity” is multiple roots of activation function at origin. Our
theoretical analysis confirms experiments for two cases, one is for the activation
function of multiplicity one with arbitrary dimension input, which contains many
common activation functions, and the other is for the layer with one-dimensional
input and arbitrary multiplicity. This work makes a step towards understanding
how small initialization implicitly leads NNs to condensation at initial training
stage, which lays a foundation for the future study of the nonlinear dynamics of
NNs and its implicit regularization effect at a later stage of training.
1	Introduction
Over-parameterized neural networks often show good generalization performance on real-world
problems by minimizing loss functions without explicit regularization (Breiman, 1995; Zhang et al.,
2017). For over-parameterized NNs, there are infinite possible sets of training parameters that can
reach a satisfying training loss. However, their generalization performances can be very different.
It is important to study what implicit regularization is imposed aside to the loss function during the
training that leads the NN to a specific type of solutions.
Empirical works suggest that NNs may learn the data from simple to complex patterns (Arpit et al.,
2017; Xu et al., 2019; Rahaman et al., 2019; Xu et al., 2020; Jin et al., 2020; Kalimeris et al.,
2019). For example, an implicit bias of frequency principle is widely observed that NNs often learn
the target function from low to high frequency (Xu et al., 2019; Rahaman et al., 2019; Xu et al.,
2020), which has been utilized to understand various phenomena (Ma et al., 2020; Xu & Zhou,
2021) and inspiring algorithm design Liu et al. (2020). The NN output, either simple or complex,
is a collective result of all neurons. The study of how neuron weights evolve during the training is
central to understanding the collective behavior, including the complexity, of the NN output.
Luo et al. (2021) establish a phase diagram to study the effect of initialization on weight evolution
for two-layer ReLU NNs at the infinite-width limit and find three distinct regimes in the phase
diagram, i.e., linear regime, critical regime and condensed regime. The non-linear regime, a largely
unexplored non-linear regime, is named as condensed regime because the input weights of hidden
neurons (the input weight or the feature of a hidden neuron consists of the weight from its input
layer to the hidden neuron and its bias term) condense on isolated orientations during the training
(Luo et al., 2021). The three regimes are identified based on the relative change of input weights as
the width approaches infinity, which tends to 0, O(1) and +∞, respectively.
The condensation is a feature learning process, which is important to the learning of DNNs. Note
that in the following, condensation is accompanied by a default assumption of small initializa-
tion or large relative change of input weights during training. For practical networks, such as
resnet18-like (He et al., 2016) in learning CIFAR10, as shown in Fig. 1(a) and Table 1, we find that
1
Under review as a conference paper at ICLR 2022
Table 1: Comparison of common (Glorot & Bengio, 2010) and condensed Gaussian initializations
onresnet18. m = (min + mout)∕2. min: in-layer width. mout: out-layer width.
common				condensed		
	Glorot.uniform	Glorot_normal	N (0, m)	N (0, m4-) mout	N (0, mh) mout	N (0, (m )2)
Test 1	0.8807	0.8777	0.8816	0.8847	0.8824	0.8826
Test 2	0.8857	0.8849	0.8806	0.8785	0.8813	0.8807
Test 3	0.8809	0.8860	0.8761	0.8824	0.8861	0.8800
the performance of networks with initialization in the condensed regime is very similar to the com-
mon initialization methods. However, the condensation phenomenon provides an intuitive explana-
tion of the good performance as follows, which may lead to a quantitative theoretical explanation
in future work. The condensation transforms a large network to a network of only a few effective
neurons, leading to an output function with low complexity. Since the complexity bounds the gen-
eralization error (Bartlett & Mendelson, 2002), the study of condensation could provide insight to
how NNs are implicitly regularized to achieve good generalization performance in practice.
For two-layer ReLU NN, Maennel et al. (2018) prove that, as the initialization of parameters goes
to zero, the features of hidden neurons condense at finite number of orientations depending on the
input data; when performing a linearly separable classification task with infinite data, Pellegrini &
Biroli (2020) show that at mean-field limit, a two-layer infinite-width ReLU NN is effectively equal
to a NN of one hidden neuron, i.e., condensation on a single orientation. Both works (Maennel et al.,
2018; Pellegrini & Biroli, 2020) study the condensation behavior for ReLU-NNs at an initial training
stage in which the magnitudes of NN parameters are far smaller from well-fitting an O(1) target
function. However, it still remains unclear that for NNs of more general activation functions, how
the condensation emerges at the initial training stage.
Figure 1: The test accuracy in (a) and condensation in (b, c) of networks on CIFAR10. Each network
consists of the convolution part of resnet18 and fully-connected (FC) layers with size 1024-1024-10
and softmax. The color in (b, c) indicates the inner product of normalized input weights of two
neurons in the first FC layer, whose indexes are indicated by the abscissa and the ordinate. We
discard about 55% of the hidden neurons, in which the L2-norm of each input weight is smaller
than 0.001, while remaining ones bigger than 0.05 in (b). The convolution part is equipped with
ReLU activation and initialized by Glorot normal distribution (Glorot & Bengio, 2010). For FC
layers in (a), the activation is ReLU and they are initialized by three common methods (red) and
three condensed ones (green) as indicated in Table 1. The learning rate is 10-3 for epoch 1-60 and
10-4 for epoch 61-100. For (b, c), the learning rate is 5 × 10-6 for visualization and FC layers
are initialized by N(0, -4—) and equipped with ReLU in (b) and X tanh(x) in (C) as activation
mout
functions. Adam optimizer with cross-entropy loss and batch size 128 are used for all experiments.
In this work, we show that the condensation at the initial stage is closely related to the multiplicity
p at x = 0, which means the derivative of activation at x = 0 is zero up to the (p - 1)th-order
and is non-zero for the p-th order. To verify their relation, we use the common activation func-
tion sigmoid(x), softplus(x), tanh(x), which are multiplicity one, and variants of tanh(x),i.e.
x tanh(x) and x2 tanh(x) with multiplicity two and three, for our experiments. For comparison, we
also show the initial condensation of ReLU(x), which is studied previously (Maennel et al., 2018)
and has totally different properties at origin compared with tanh(x). Our experiments suggest that
2
Under review as a conference paper at ICLR 2022
the maximal number of condensed orientations is twice the multiplicity of the activation function
used in general NNs. For finite-width two-layer NNs with small initialization at the initial training
stage, each hidden neuron’s output in a finite domain around 0 can be approximated by a p-th order
polynomial and so is the NN output function. Based on the p-th order approximation, we show
a preliminary theoretical support for condensation by a theoretical analysis for two cases, one is
for the activation function of multiplicity one with arbitrary dimension input, which contains many
common activation functions, and the other is for the layer with one-dimensional input and arbitrary
multiplicity. Therefore, small initialization imposes an implicit regularization that restricts the NN
to be effectively much narrower neural network at the initial training stage.As commonly used acti-
vation functions, such as tanh(x), sigmoid(x), softplus(x), etc., are all multiplicity one, our study
of initial training behavior lays an important basis for the further study of implicit regularization
throughout the training.
2	Related works
A research line studies how initialization affects the weight evolution of NNs with a sufficiently large
or infinite width. For example, with an initialization in the neural tangent kernel (NTK) regime or
lazy training regime (weights change slightly during the training), the gradient flow of infinite-width
NN, can be approximated by a linear dynamics of random feature model (Jacot et al., 2018; Arora
et al., 2019; Zhang et al., 2020; E et al., 2020; Chizat & Bach, 2019), whereas for the initialization
in the mean-field regime (weights change significantly during the training), the gradient flow of
infinite-width NN exhibits highly nonlinear dynamics (Mei et al., 2019; Rotskoff & Vanden-Eijnden,
2018; Chizat & Bach, 2018; Sirignano & Spiliopoulos, 2020). Pellegrini & Biroli (2020) analyze
how the dynamics of each parameter transforms from a lazy regime (NTK initialization) to a rich
regime (mean-field initialization) for an two-layer infinite-width ReLU NN to perform a linearly
separable classification task with infinite data. Luo et al. (2021) systematically study the effect of
initialization for two-layer ReLU NN with infinite width by establishing a phase diagram, which
shows three distinct regimes, i.e., linear regime (similar to the lazy regime), critical regime and
condensed regime (similar to the rich regime), based on the relative change of input weights as the
width approaches infinity, which tends to 0, O(1) and +∞, respectively. Luo et al. (2021) also
empirically find that, in the condensed regime, the features of hidden neurons (orientation of the
input weight) condense in several isolated orientations, which is a strong feature learning behavior,
an important characteristic of deep learning, however, in Luo et al. (2021), it is not clear how general
of the condensation when other activation functions are used and why there is condensation.
3	Preliminary: Neural networks and initial stage
A two-layer NN is
m
fθ(x) = Eajσ(wj ∙ x),	(1)
j=1
where σ(∙) is the activation function, Wj = (Wj, bj) ∈ Rd+1 is the neuron feature including the
input weight and bias terms, and X = (x, 1) ∈ Rd+1 is combination of the input sample and scalar
1, θ is the set of all parameters, i.e., {aj, wj}jm=1. For simplicity, we call wj as input weight or
weight and x as input sample.
A L-layer NN can be recursively defined by feeding the output of the previous layer as the input to
the current hidden layer i.e.,
x[0] = (x, 1), x[1] = (σ(W[1]x[0]), 1),	x[i] = (σ(W [i]x[i-1]), 1), forl ∈ {2, 3, ..., L}
1	(2)
f(θ, X) = -alx[L]，fθ (x),
α
where W[l] = (W[l], b[l]) ∈ Rmι×(ml-1+1), and mi represents the dimension of the l-th hidden
layer. For simplicity, we also call each row of W[l] as input weight or weight and x[l-1] as input
3
Under review as a conference paper at ICLR 2022
neurons. The target function is denoted as f * (x). The training loss function is mean squared error
1n
RS(θ) = 2n ∑(fθ(Xi) - f*(χi))2.	⑶
n i=1
Without loss of generality, we assume that the output is one-dimensional for theoretical analysis,
because for high-dimensional cases, we only need to sum the components directly. For summation,
it does not affect the results of our theories. We consider the gradient flow training
θ = -Vθ RS (θ).	(4)
For convenience, we characterize the activation function by the following definition.
Definition 1 (multiplicity p). Suppose that σ(x) satisfies the following condition, there exists a
P ∈ N and P ≥ 1, such that the k-th order derivative σ(k) (0) = 0 for k = 1, 2, ∙∙∙ ,p 一 1, and
σ(p)(0) 6= 0, then we say σ has multiplicity p.
In the experiments, we study the condensation at the initial stage of training. For a fixed loss, the
step we need to achieve it is highly related to the size of learning rate. Therefore, we propose a
definition of the initial stage of training by the size of loss in this article, that is the stage before the
value of loss function decays to 70% of its initial value. Such a definition is reasonable, for generally
a loss could decay to 1% of its initial value or even lower. The loss of the all experiments in the
article can be found in Appendix A.3, and they do meet the definition of the initial stage here.
4	Initial condensation of input weights
It is intuitively believed that NNs are powerful at learning data features, which should be an impor-
tant reason behind the success of deep learning. A simple way to define a learned feature ofa neuron
is by the orientation of its input weights. Previous work in Luo et al. (2021) show that there is a
condensed regime, where the neuron features condense on isolated orientations during the training
for two-layer ReLU NNs. The condensation implies that although there are many more neurons
than samples, the number of effective neurons, i.e., the number of different used features in fitting,
is often much smaller than the number of samples. Therefore, the condensation provides a potential
mechanism that helps over-parameterized NNs avoid overfitting. However, it is still unclear how the
condensation, for general NNs with small initialization, emerges during the training. In this section,
we would empirically show how the condensation differs among NNs with activation functions of
different multiplicities, followed by theoretical analysis in the next section.
4.1	Experimental setup
For Synthetic dataset and MNIST: Throughout this work, we use fully-connected neural network
with size, d-m---m-dout. The input dimension d is determined by the training data. The output
dimension is dout = 1 for synthetic data and dout = 10 for MNIST. The number of hidden neu-
rons m is specified in each experiment. All parameters are initialized by a Gaussian distribution
N (0, var). The total data size is n. The training method is Adam with full batch, learning rate lr
and MSE loss. For synthetic data, we sample the training data uniformly from a sub-domain ofRd.
For CIFAR10 and CIFAR100 dataset: We use Resnet18-like neural network, which has been de-
scribe in Fig. 1 thoroughly. The input dimension d is determined by the training data. The output
dimension is dout = 10 for CIFAR10 and dout = 100 for CIFAR100. All parameters are initialized
by a Gaussian distribution N(0, var). The total data size is n. The training method is Adam with
batch size 128, learning rate lr and Cross-entropy loss.
4.2	Multidimensional data
We first show the condensation at initial training stage in fitting multidimensional dataset. Since
the input is a multidimensional vector, the direction is also multidimensional. To characterize the
condensation, we use D(u, v) to denote the inner product of the normalized vectors of two input
weights, i.e., D(u, v) = u|v.
4
Under review as a conference paper at ICLR 2022
(a) tanh(x)	(b) x tanh(x)	(c) x2 tanh(x)	(d) ReLU(x)	(e) sigmoid(x) (f) softplus(x)
Figure 2: Condensation of two-layer NNs. The color indicates D(u, v) of two hidden neurons’ input
weights at epoch 100, whose indexes are indicated by the abscissa and the ordinate, respectively.
If neurons are in the same beige block, D(u,v) 〜 1 (navy-blue block, D(u,v) 〜 一1), their
input weights have the same (opposite) direction. The activation functions are indicated by the
sub-captions. The training data is 80 points sampled from P5k=1 3.5 sin(5xk + 1), where each
xk is uniformly sampled from	[-4, 2].	n =	80,	d =	5,	m =	50,	dout = 1, var =	0.0052.
lr = 10-3, 8 × 10-4, 2.5 × 10-4 for (a-d), (e) and (f), respectively.
We use a two-layer fully-connected NN with size 5-50-1 to fit n = 80 training data sampled from a
5-dimensional function Pk=1 3.5 sin(5xk + 1), where X = (x1, x2, ∙ ∙ ∙ , x5)| ∈ R5 and each Xk is
uniformly sampled from [-4, 2]. As shown in Fig. 2(a), for activation function tanh(x), the color
indicates D(u, v) of two hidden neurons’ weights at epoch 100, whose indexes are indicated by the
abscissa and the ordinate. If the neurons are in the same beige block, D(u, V)〜1 (navy-blue block,
D(u, v)〜一1), their input weights have the same (opposite) direction. Obviously, input weights
of hidden neurons condense at two opposite directions, i.e., one line. As the multiplicity increasing,
NNs with x tanh(x) (Fig. 2(b)) and x2 tanh x (Fig. 2(c)) condense at two and three different lines,
respectively. For activation function sigmoid(x) in Fig. 2(d) and softplus(x) in Fig. 2(e), which are
frequently used and have multiplicity one, NNs also condense at two opposite directions. For ReLU
in Fig. 2(f), for which the multiplicity definition cannot apply, the NN condenses at three directions,
in which two are opposite. Through these experiments, we conjecture that the maximal number of
condensed orientations is twice the multiplicity of the activation function used at initial training.
(a) layer 1
(b) layer 2
(c) layer 3
(d) layer 4
(e) layer 5
Figure 3: Condensation of six-layer NNs with residual connections. The activation functions for
hidden layer 1 to hidden layer 5 are x2 tanh(x), x tanh(x), sigmoid(x), tanh(x) and softplus(x),
respectively. The numbers of steps selected in the sub-pictures are epoch 1000, epoch 900, epoch
900, epoch 1400 and epoch 1400, respectively, while the NN is only trained once. The color in-
dicates D(u, v) of two hidden neurons’ input weights, whose indexes are indicated by the abscissa
and the ordinate, respectively. The training data is 80 points sampled from a 3-dimensional function
P3k=1 4 sin(12xk + 1), where each xk is uniformly sampled from [-4, 2]. n = 80, d = 3, m = 18,
dout = 1, var = 0.012, lr = 4 × 10-5.
For multilayer NNs with different activation functions, we show that the condensation for all hidden
layers is similar to the two-layer NNs. In deep networks, residual connection is often introduced
to overcome the vanishing of gradient. To show the generality of condensation, we perform an
experiment of six-layer NNs with residual connections. To show the difference of various activa-
tion functions, we set the activation functions for hidden layer 1 to hidden layer 5 as x2 tanh(x),
x tanh(x), sigmoid(x), tanh(x) and softplus(x), respectively. The structure of the residual is
hl+1(x) = σ(Wlhl(x) + bl) + hl(x), where hl(x) is the output of the l-th layer. As shown in
Fig. 3, input weights condense at three, two, one, one and one lines for hidden layer 1 to hidden
layer 5, respectively. Note that residual connections are not necessary. We show an experiment of
the same structure as in Fig. 3 but without residual connections in Appendix A.6. To show the
universality of condensation, we train resnet18-like neural networks to learn CIFAR10. We study
5
Under review as a conference paper at ICLR 2022
the condensation of the first fully connected layer of the network, using ReLU and x tanh (x) as the
activation functions and initialization distribution N(0, (m15)2). As shown in Fig. 1 (b) and (c), the
condensations for activation ReLU(x) and x tanh (x) are consistent with Fig. 2 and our conjucture.
More experiments on dataset CIFAR10 and CIFAR100 can be found in Appendix A.5.
We also find that when the training data is less oscillated, the NN may condense at fewer directions.
For example, as shown in Fig. 4(a), compared with the high frequency function in Fig. 2, we only
change the target function to be a lower-frequency function, i.e., P5k=1 3.5 sin(2xk + 1). In this
case, the NN with x2 tanh(x) only condenses at three directions, in which two are opposite. For
MNIST data in Fig. 4(b), we find that, the NN with x2 tanh(x) condenses at one line, which may
suggest that the function for fitting MNIST dataset is a low-frequency function. For CIFAR100 data
in Fig. 4(c), we find that input weights of the first FC layer with x tanh(x) condense at only one line,
which implies that features extracted by the convolution part of the NN may own low complexity.
(a) P5k=1 3.5 sin(2xk + 1)
(b) MNIST
(c) CIFAR100


Figure 4: Condensation of low-frequency functions with two-layer NNs in (a,b) and condensation of
the first FC layer of the Resnet18-like network on CIFAR100 in (c). The color indicates D(u, v) of
two hidden neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate. For
(a,b), two-layer NN at epoch: 100 with activation function: x2 tanh(x). For (a), we discard about
15% of hidden neurons, in which the L2-norm of each input weight is smaller than 0.04, while
remaining those bigger than 0.4. The mean magnitude here for each parameter is (0.42/785)0.5
〜0.01, which should also be quite small. All settings in (a) are the same as Fig. 2, except for the
lower frequency target function. Parameters for (b) are n = 60000, d = 784, m = 30, dout = 10,
var = 0.0012. lr = 5 × 10-5. The structure and parameters of the Resnet18-like neural network
for (c) is the same as Fig. 1(b), except for the data set CIFAR100 and learning rate lr = 1 × 10-6.
To understand the mechanism of the initial condensation, we turn to experiments of 1-d input and
two-layer NN, which can be clearly visualized in the next subsection.
4.3	1-d input and two-layer NN
For 1-d data, we visualize the evolution of the two-layer NN output and each weight, which confirms
the connection between the condensation and the multiplicity of the activation function.
(a) tanh(x)
(b) x tanh(x)
(c) x2 tanh(x)	(d) ReLU(x)
Figure 5: The outputs of two-layer NNs at epoch 1000 with activation functions tanh(x), x tanh(x),
x2 tanh(x), and ReLU(x) are displayed, respectively. The training data is 40 points uniformly
sampled from sin(3x) + sin(6x)/2 with x ∈ [-1, 1.5], illustrated by green dots. The blue solid
lines are the NN outputs at test points, while the red dashed auxiliary lines are the first, second,
third and first order polynomial fittings of the test points for (a, b, c, d), respectively. Parameters are
n = 40, d = 1, m = 100, dout = 1, var = 0.0052, lr = 5 × 10-4.
6
Under review as a conference paper at ICLR 2022
We display the outputs at initial training, epoch 1000, in Fig. 5. Due to the small magnitude of
parameters, an activation function with multiplicity p can be well approximated by a p-th order
polynominal, thus, the NN output can also be approximated by a p-th order polynominal. As shown
in Fig. 5(a-c), the NN outputs with activation functions tanh(x), x tanh(x) and x2 tanh(x) overlap
well with the auxiliary of a linear, a quadratic and a cubic polynominal curve, respectively. In Fig.
5(d), the NN output with ReLU activation function deviates from a linear function (red auxiliary
line). Particularly, the NN output has several sharp turning points. This experiment, although simple,
but convincingly shows that NN does not always learn a linear function at the initial training stage
and the complexity of such learning depends on the activation function.
Figure 6: The direction field for input weight w := (w, b) of the dynamics in (4.3) at epoch 200.
All settings are the same as Fig. 5. Around the original point, the field has one, two, three stables
lines, on which an input weight would keep its direction, for tanh(x), x tanh(x), and x2 tanh(x),
respectively. We also display the value of each weight by the green dots and the corresponding
directions by the orange arrows.
We visualize the direction field for input weight wj := (wj, bj), following the gradient flow,
n
Wj = -^j Eeiσ<Wj ∙ Xi)xi,
n i=1
where ei := fθ(Xi) - f * (xi). Since We only care about the direction of Wj and aj is a scalar at each
epoch, we can visualize Wj by Wj/aj. For simplicity, we do not distinguish Wj/ɑj and Wj if there
is no ambiguity. When we compute Wj- for different j,s, eiXi for (i = 1, ∙∙∙ ,n) is independent with
j. Then, at each epoch, for a set of {ei, xi}in=1, we can consider the following direction field
1n
ω = -—Eeixiσ (ω ∙ Xi).
i=1
When ω is set as Wj, we can obtain Wj∙. As shown in Fig. 6, around the original point, the field has
one, two, three stables lines, on which a neuron would keep its direction, for tanh(x), x tanh(x),
and x2 tanh(x), respectively. We also display the input weight of each neuron on the field by
the green dots and their corresponding velocity directions by the orange arrows. Similarly to the
high-dimensional cases, NNs with multiplicity p activation functions condense at p different lines
for p = 1, 2, 3. Therefore, It is reasonable to conjecture that the maximal number of condensed
orientations is twice the multiplicity of the activation function used. As shown in Fig. 6(d), the field
and the condensation for the NN with ReLU(x) is much more complex.
Taken together, we have empirically shown that the multiplicity of the activation function is a key
factor that determines the complexity of the initial output and condensation. In figures of this sec-
tion, we only show the results of the final step of condensation. To facilitate the understanding of
evolution of condensation in the initial stage, we show several steps during the initial stage of each
example in Appendix A.4. From the evolution, we could directly observe how condensation occurs.
5	Analysis of the initial condensation of input weights
In this section, we would present a preliminary analysis to understand how the multiplicity of the
activation function affects the initial condensation. At each training step, we consider the velocity
field of weights in each hidden layer of a neural networks.
Considering a network with L hidden layers, we use row vector Wj[k] to represent the weight from
the (k-1)-th layer to the j-th neuron in the k-th layer. For each k andj, Wj[k] satisfies the following
7
Under review as a conference paper at ICLR 2022
dynamics, (see Appendix A.2)
W - (W ∙ U)U	/C
r = u ∙ W, U= -----------------.	(5)
r
where W can be Wj[k]| for all k’s and j’s, r = kWk2 is the amplitude, and U = W/r.
Suppose the activation function has multiplicity p, i.e., σ(k)(0) = 0 for k = 1, 2, ∙∙∙ ,p - 1, and
σ(P)(O) = 0. For convenience, We define an operator P satisfying PW := W — U(W ∙ u). Conden-
sation refers to that the weight evolves towards a direction that will not change in the direction field
and is defined as folloWs,
U = 0 ⇔ PW := W — U(W ∙ u) = 0.
Since W ∙ U is a scalar, W is parallel with u. U is a unit vector, therefore, we have U = W/∣∣W∣∣2. In
this work, we consider NNs with sufficiently small parameters. For example, suppose r = ∣∣w∣2 〜
O(e), E is a small quantity. Dynamics (5) shows that O(r)〜 O(W) and O(U)〜 O(r)∕O(e).
Therefore, the orientation U would moves much more quickly than the amplitude r . In the following,
we study the case of (i) p = 1 and (ii) ml = 1.
5.1	CASE 1:p= 1
Since we have (see Appendix A.2),
n
W| = W[k] = - - X (f(θ, Xi) -yi)[diag{σ0(W[k]xikτ])}(E[k+LL]a)jxikτ]1,
n
i=1
where we use El = W[l]| diag{σ0(W [l]x[l-1])}, for l ∈ {2, 3, ..., L}, E [q:p] = EqEq+1...Ep.
For a fixed step, we only consider the gradient w.r.t. W[k]. Suppose σ0(0) = 0 and parameters are
small. Denote ei := (f (θ, xi) - yi). By Taylor expansion,
n
PW Ieadi≈order Qw := --(diag{σ0(0)}(E[k+LL]a)j X呼71]
n	i=1
+ (-(diag{σ0(0)}(E[k+1:L]a)j X eiχik-1] ∙ U)U = 0,
where operator Q is the leading-order approximation of operator P, and here E[k+1:L] is indepen-
dent with i because diag{σ0(W [l] x[l-1] )} ≈ diag{σ0(0)}. Since diag{σ0 (0)} = I, and, WLOG,
we assume a 6= 0, then
n
QW = 0 ⇔ X ei x[ik-1]
i=1
(X eiχik-1] ∙ u)u∙
We have
Pn e x[k-1]	Pn e x[k-1]
u = " eixi 八 or u = - UI eiXi	.
kPn=1 eixi	]∣2	kPn=1 ^1^2
This calculation shows that for layer k, the input weights for any hidden neuron j have the same
two stable directions. Therefore, when parameters are sufficiently small, which implies that the
orientation U would moves much more quickly than the amplitude r, all input weights towards
converging to the same direction or the opposite direction, i.e., condensation on a line.
5.2	CASE 2: ml = 1
By the definition of the multiplicity p, we have
σ0(w ∙ Xi) = σ-(O)(W ∙ Xi)p-1 + o((w ∙ Xi)p-1).
(p- 1)!
8
Under review as a conference paper at ICLR 2022
where (∙)p-1 and σp(∙) operate on component here. Then UP to the leading order in terms of the
magnitude of θ, we have (see Appendix A.2)
leading order
Pw	≈ Qw :
—
(1 XX eixikτ](wlxikτ])pT)[diag{ σ-0)! }(E[k"L]a)]j
+((1 XXeixikτ](wlxikτ])pT)[diag{σp)(0!}(E[k+1:L]a)]j ∙ u)u.
n i=1	(p-1)!
WLOG, we also assUme a 6= 0. And by definition, w = ru, we have
Qw = 0 ⇔ U= n1 Pn=I enxikT(uTxikT)PT
k n Pnn=1 en xik-1](uTxikT])P-1k2
or u
1 Pn=I eiXikT](uTχikτ])pT
k 1 P乙 enxik-1](u∣xik-1])P-1k2
—
Since d + 1 = 2, we denote u = (u1, u2)| ∈ R2 and x[ik-1] = ((x[ik-1] )1, (x[ik-1])2)| ∈ R2, then,
pn=ι(uι(xikT)I + u2(xik-1]DPTei(XikT)I = ul , U
pn=i(ui(xik-1])l +U2(xikT])2)P-1ei(xikT])2 — u2 一 .
We obtain the equation for U,
nn
X(U(Xik-1])ι + (xik-1])2)p-1ei(xik-1])ι = U X(U(Xik-1])ι + (xik-1])2)p-1ei(xik-1])2.
i=1	i=1
Since it is an univariate p-th order equation, U = UU1 has at most P complex roots. Because U is a
unit vector, u at most has p pairs of values, in which each pair are opposite.
Taken together, our theoretical analysis is consistent with our experiments, that is, the maximal num-
ber of condensed orientations is twice the multiplicity of the activation function used when parame-
ters are small. Besides, because commonly used activation functions, such as tanh(x), sigmoid(x),
softplus(x), etc., are all multiplicity one, the theoretical analysis sheds light on practical training.
6	Discussion
In this work, we have shown that the characteristic of the activation function, i.e., multiplicity, is
a key factor to understanding the complexity of NN output and the weight condensation at initial
training. The condensation restricts the NN to be effectively low-capacity at the initial training stage,
even for finite-width NNs. During the training, the NN increases its capacity to better fit the data,
leading to a potential explanation for their good generalization in practical problems. This work also
serves as a starting point for further studying the condensation for multiple-layer neural networks
throughout the training process.
As the scale of parameter initialization becomes larger, the condensation becomes weaker. The un-
derstanding from a complete condensation would benefit our understanding of the training process in
common initialization, that is there is an effect, although not so strong, that can limit the complexity
of the neural network in the initial training.
How small the initialization should be in order to see a clear condensation is studied in Luo et al.
(2021) for two-layer ReLU NNs with infinite width. For general activation functions, the regime of
the initialization for condensation depends on the NN width. A further study of the phase diagram
for finite width NNs would be important.
For general multiplicity with high-dimensional input data, the theoretical analysis for the initial
condensation is a very difficult problem, which is equivalent to count the number of the roots of a
high-order high-dimensional polynomial with a special structure originated from NNs.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances
in NeUral Information Processing Systems 32: AnnUal Conference on NeUral Information
Processing Systems 2019, NeUrIPS 2019, December 8-14, 2019, VancoUver, BC, Canada, pp.
8139-8148, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
dbc4d84bfcfe2284ba11beffb853a8c4- Abstract.html.
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David KrUeger, EmmanUel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. CoUrville, YoshUa Bengio, and Simon
Lacoste-JUlien. A closer look at memorization in deep networks. In Doina PrecUp and Yee Whye
Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
Sydney, NSW, AUstralia, 6-11 AUgUst 2017, VolUme 70 of Proceedings of Machine Learning
Research, pp. 233-242. PMLR, 2017. URL http://ProceedingS.mlr.press/v7 0/
arpit17a.html.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaUssian complexities: Risk boUnds and
strUctUral resUlts. JoUrnaI ofMachine Learning Research, 3(Nov):463U82, 2002.
Leo Breiman. Reflections after refereeing papers for nips. The Mathematics of Generalization, XX:
11-15, 1995.
LenaIc Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models Using optimal transport. In Proceedings of the 32nd International
Conference on NeUral Information Processing Systems, pp. 3040-3050, 2018.
Lenaic Chizat and Francis Bach. A note on lazy training in sUpervised differentiable programming.
In 32nd Conf. NeUraI Information Processing Systems (NeUrIPS 2018), 2019.
Weinan E, Chao Ma, and Lei WU. A comparative analysis of optimization and generalization prop-
erties of two-layer neUral network and random featUre models Under gradient descent dynamics.
Science China Mathematics, pp. 1-24, 2020.
Xavier Glorot and YoshUa Bengio. Understanding the difficUlty of training deep feedforward neUral
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image recog-
nition. In Proceedings of the IEEE conference on compUter Vision and Pattern recognition, pp.
770-778, 2016.
ArthUr Jacot, Clement Hongler, and Franck Gabriel. NeUral tangent kernel: Convergence
and generalization in neUral networks. In Samy Bengio, Hanna M. Wallach, HUgo
Larochelle, Kristen GraUman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), AdVances
in NeUral Information Processing Systems 31: AnnUal Conference on NeUral Information
Processing Systems 2018, NeUrIPS 2018, December 3-8, 2018, Montreal, Canada, pp.
8580-8589, 2018. URL https://proceedings.neurips.cc/paper/2 018/hash/
5a4be1fa34e62bb8a6ec6b91d2462f5a- Abstract.html.
Pengzhan Jin, LU LU, Yifa Tang, and George Em Karniadakis. QUantifying the generalization error
in deep learning in terms of data distribUtion and neUral network smoothness. NeUral Networks,
130:85-99, 2020.
Dimitris Kalimeris, Gal KaplUn, PreetUm Nakkiran, Benjamin L. Edelman, Tristan Yang,
Boaz Barak, and Haofeng Zhang. SGD on neUral networks learns fUnctions of in-
creasing complexity. In Hanna M. Wallach, HUgo Larochelle, Alina Beygelzimer, Flo-
rence d,Alche-BUc, Emily B. Fox, and Roman Garnett (eds.), AdVances in NeUraI
Information Processing Systems 32: AnnUaI Conference on NeUraI Information Processing
Systems 2019, NeUrIPS 2019, December 8-14, 2019, VancoUver, BC, Canada, pp.
3491-3501, 2019. URL https://proceedings.neurips.cc/paper/2 019/hash/
b432f34c5a997c8e7c806a895ecc5e25- Abstract.html.
10
Under review as a conference paper at ICLR 2022
Ziqi Liu, Wei Cai, and Zhi-Qin John Xu. Multi-scale deep neural network (mscalednn) for solving
Poisson-boltzmann equation in complex domains. CommUnications in Computational Physics,
28(5):1970-2001,2020.
Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu neural
networks at infinite-width limit. JoUrnal OfMachine Learning Research, 22(71):1-47, 2021.
Chao Ma, Lei Wu, and E Weinan. The slow deterioration of the generalization error of the random
feature model. In Mathematical and Scientific Machine Learning, pp. 373-389. PMLR, 2020.
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network
features. arXiv PrePrint arXiv:1803.08367, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. In Alina Beygelzimer and Daniel Hsu (eds.),
Conference on Learning Theory, COLT 2019, 25-28 JUne 2019, Phoenix, AZ, USA, volume 99
of Proceedings of Machine Learning Research, pp. 2388-2464. PMLR, 2019. URL http://
proceedings.mlr.press/v99/mei19a.html.
Franco Pellegrini and Giulio Biroli. An analytic theory of shallow networks dynamics for hinge loss
classification. Advances in NeUral Information Processing Systems, 33, 2020.
Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht,
Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. International
Conference on Machine Learning, 2019.
Grant M. Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time con-
vergence and asymptotic error scaling of neural networks. In Samy Bengio, Hanna M. Wallach,
Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), Advances
in NeUral Information Processing Systems 31: AnnUal Conference on NeUral Information
Processing Systems 2018, NeUrIPS 2018, December 3-8, 2018, Montreal, Canada, pp.
7146-7155, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
196f5641aa9dc87067da4ff90fd81e7b- Abstract.html.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central
limit theorem. Stochastic Processes and their APPlications, 130(3):1820-1852, 2020.
Zhi-Qin J Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in
frequency domain. InternationaI Conference on NeUraI Information Processing, pp. 264-274,
2019.
Zhi-Qin John Xu and Hanxu Zhou. Deep frequency principle towards understanding why deeper
learning is faster. In Proceedings of the AAAI Conference on ArtificiaI Intelligence, volume 35,
2021.
Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle:
Fourier analysis sheds light on deep neural networks. CommUnications in ComPUtational Physics,
28(5):1746-1767, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
RePresentations, ICLR 2017, Toulon, France, APril 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced
by initialization in deep neural networks. In MathematicaI and Scientific Machine Learning, pp.
144-164. PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Basic definitions
In this study, we first consider the neural network with 2 hidden layers,
A two-layer NN is
m
fθ(x) = E ajσ(wj ∙ x),	(6)
j=1
where σ(∙) is the activation function, Wj = (Wj, bj) ∈ Rd+1 is the neuron feature including the
input weight and bias terms, and X = (x, 1) ∈ Rd+1 is combination of the input sample and scalar
1, θ is the set of all parameters, i.e., {aj, wj}jm=1. For simplicity, we call wj as input weight or
weight and x as input sample.
Then, we consider the neural network with l hidden layers,
x[0] = (x, 1), x[1] = (σ(W[1]x[0]), 1),	x[l] = (σ(W [l]x[l-1]), 1), forl ∈ {2, 3, ..., L}
f(θ, X) = -alx[L]，fθ(x),	()
α
where W[l] = (W[l],b[l]) ∈ R(ml×ml-ι), andml represents the dimension of the l-th hidden layer.
The initialization of WklkO, l ∈ {1,2,3,...,L} and ak obey normal distribution WklkO 〜N(0, β2)
for l ∈ {1, 2, 3,…,L} and ak 〜N(0, βL+J.
The loss function is mean squared error given below,
1n
Rs(θ) = 2n ɪ^(fθ (Xi)- yi)2 .	(8)
For simplification, we denote fθ(X) as f in following.
A.2 Derivations for concerned quantities
A.2.1 Neural networks with three hidden layers
In order to better understand the gradient of the parameter matrix of the multi-layer neural network,
we first consider the case of the three-layer neural network,
fθ (X) := alσ(W [2]σ(W [1] X)),	(9)
with the mean squared error as the loss function,
1n
Rs(θ) = 2^∑(fθ (Xi)- yi)2.	(10)
ni=1
We calculate dW⅛y and dWfτy respectively, using differential form,
df = tr(( ∂f )l df).	(11)
We consider dfy first,
12
Under review as a conference paper at ICLR 2022
df = tr{d(alσ(W [2]x[1]))}
= tr{ald(σ(W [2]x[1]))}
=tr{alσ0(W[2]x[1]) Θ dW[2]x[1]}
= tr{(a Θ σ0(W[2]x[1])ldW[2]x[1]}
= tr{x[1](a Θ σ0(W [2]x[1])|dW [2]}
= tr{((a Θ σ0(W[2]x[1]))x[1]l)ldW[2]},
(12)
where Θ is Hadamard Product, and it is the multiplication of matrix elements of the same position.
Hence,
	HWT = (a θ σ0(W [2]x[1]))x[1]1	(13) = diag{σ0 (W [2]x[1])}ax[1]|.
Then, We consider ^{1],
	df = tr{(a Θ σ (W[2]x[1]))lW[2]dσ(W[1]x)} =tr{(W[2]l(a Θ σ0(W⑵x[1])))lσ0(W[1]x) Θ d(W[1]x)} =tr{((W[2]l(a Θ σ0(W[2]x[1])) Θ σ0(W[1]x))ld(W[1]x)}	(M) = tr{[((W [2]|(a Θ σ0 (W [2]x[1])) Θ σ0 (W [1]x))x|]|d(W [1])}.
Hence, we have,
	df = ((W[2]l(a Θ σ (W[2]x[1]))) Θ σ (W[1]x))xl	(15) =diag{σ0 (W [1]x)}W [2]ldiag{σ0 (W [2]x[1])}axl.
Through the chain rule, We can get the evolution equation of W[1] and W[2],
dW [1]	dRs (θ)
—
dt	-	τπ- dW [1] =-1 X(f (θ, Xi)- yi)dWw	(16) i=1 1n	0	|	0 =--£(f (θ, Xi) - yi)diag{σ (W[1]Xi)}W[2] diag{σ (W[2]χi ])}ax|, n i=1
and	dW [2]	dRs(θ) 	=	TTT- dt	dW[2] n =-n X(f(θ, Xi)- yi)dWW	(17) i=1 1n	| =——£(f (θ, Xi) - yi)diag{σ'(W[2]xi1])}axi1]τ. n i=1
A.2.2 L HIDDEN LAYERS CONDITION
And, We consider the neural netWork With L hidden layers,
13
Under review as a conference paper at ICLR 2022
df = tr{daldσ(W [L] x[L-1])}
= tr{(a σ0(W [L]x[L-1]))|dW [L]σ(W [L-1]x[L-2])}
=tr{(W[L]lΛL)lσ0(W[L-1]x[L-2]) Θ dW[L-1]σ(W[L-2]x[L-3])}
=tr{((W[L]lΛL) Θ σ (W[L-1]x[L-2]))|W[L-1]dσ(W[L-2]x[L-3])}
=(W [L-1]lΛL-i)ldσ(W [L-2]χ[L-3])
tr{ΛldW [k]x[k-1]}
tr{(Λk x[k-1]1 )ldW [k]},
where Λl，(W[l+1]lΛl+1) Θ σ (W[l]x[l-1]) for l = k,k + 1...L — 1 and ΛL
σ0 (W [L]x[L-1]).
(18)
aΘ
Hence, we get,
df
dW [k]
(19)
Through the chain rule, we can get the evolution equation of W[k] ,
	dW [k]	dRs(θ) 	：	=	TTT- dt	dW [k] 1 n	df =-n X(f(θ,Xi)- yi)dW[kτ	(20) 1n	| =—n ∑(f (θ, Xi) — yi)Λk Xik-1]τ.
Through a Θ σ0(Wx) = diag{σ0 (W x)}a,
Finally, the dynamic system can be obtained:
da a =正二	1n =-n EXiL] 5θ, xi) - yi), n i=1
W [L] = dWL] : dt	1n =--]Tdiag{σ0(W[L]χiL-1])}aχiL-1]1 (f(θ, Xi) — yi), n i=1
W [k] =必; dt	1n	| =—n X diag{σ0(W[k]Xik-1])}E区+1:%第「-1] (f(θ, Xi) — y) ∀i ∈ [1 : L — 1], i=1	(21)
where we use El(x) = W [l]| diag{σ0(W [l]x[l-1])}. And E[q:p] = EqEq+1...Ep.
Let rk,j = kWj[k] k2. We have
	dt F I2 = dt kWj[k]k2.	(22)
Then we obtain	r-k,jTk,j = W[k] ∙ Wj[k].	(23)
Finally, we get	K =亨=Wj[k] ∙ W[k]∕rk,j ,j	dt	j	j	,j	(24) =Wj[k] ∙ Uk,j,
14
Under review as a conference paper at ICLR 2022
W[k]
where uk,j = -j— is a unit vector. Then We have,
U kj=M=d H!
-Wj[k]rk,j - W[k]rk,j
W[k]rk,j- W[k](Wjuk,j)
(25)
rk,j
Wj[k]-Uk,j (Wj[k]∙Uk,j )
rk,j
To conclude, the quantities we concern are summarized as follows,
1n
a = -- XL XiL] (f (θ, Xi)- yi)
n
i=1
1n
W[L] = --]Tdiag{σ0(W[L]xiL-1])}axiL-1]| (f(θ, Xi)-y),
n i=1
n
W[k] = -_ Xdiag{σ0(W[k]xik-1])}E[k+1:L]axik-1]| (f (θ, Xi)-纳)∀i ∈ [1 : L
n
i=1
rk,j = W[k] ∙ Ukj
.	Wj[k] - Ukj(Wj[k]∙Ukj)
U kj =	元 ,
(26)
(27)
1] (28)
(29)
(30)
where we use El(X) = W[l]| diag{σ0(W [l]X[l-1] )}. And E[q:p] = EqEq+1...Ep.
A.2.3 PROVE FOR Pw IN 5.2
leading order
We calculate Pw	≈	Qw as following,
15
Under review as a conference paper at ICLR 2022
1n
Pw≈Qw:=—	eix[ik-1] [diag{σ0(W [k]x[ik-1])}(E[k+1:L]a)]j
n i=1
+
n
(1 X eixik-1] [diag{σ0( W[k]χik-1] )}(E[k+1:L]a)]j ∙ u)u
n i=1
-n Xeixik-1][diag{(σp-0)! Θ (W[k]xikτ])pT}(E[k"L]a)]j
+ (1 XeiXik-1] [diag{σp)(0! Θ (W[k]χik-1])p-1}(E[k+1:L]a)]j ∙ u)u
n i=1	(p - 1)!
-1 X eixik-1][diag{(W[k]xik-1])p-1} diag{^(p)≡}(E[k+1:L]a)]j
n i 1	(p-1)!
+ (1 XeiXikT [diag{(W[k]xik-1])p-1} diag{(σp-0)!}(E[k+1:L]a)]j ∙ u)u
=-1 X eixik-1] [diag{(W[k]xik-1])p-1}]j diag{(σ(p)(0!}(E[k+1:L]a)
n i=1	(p - 1)!
+ (1 XeiXikT][diag{(W[k]xikτ])pT}]jdiag{σp)(0!}(E[k+1:L]a) ∙ u)u
n i=1	(p- 1)!
=-(1 X eixikτ](Wj[k]xikτ])pT)[diag{ σ-0)! }(E[k+1：L]a)]j
+ ((n Xeixikτ](W[k]xikτ])pT)[diag{σ-0)!}(E[k+1:L]a)]j ∙ u)u,
i=1	(31)
where (∙)p-1 and σp(∙) operate on component here.
16
Under review as a conference paper at ICLR 2022
A.3 The Verification of the initial stage
We put the loss of the experiments in the main text here to show that they are indeed in the initial
stage of training by the definition.
As is shown in Fig.7 and Fig.8, at the number of steps I drew the graphs in the article, loss satisfies
the definition of the initial stage, so we consider that they are in the initial stage of training.
Only the epoch number could hardly reflect the initial stage, which also depends on the learning
rate. Therefore, we use the loss to indicate the initial stage. Learning rate is not a sensitive to
the appearance of condensation. However, a small learning rate enables us to clearly observe the
condensation process in the initial stage under a gradient flow training. For example, when the
learning rate is relatively small, the initial stage of training may be relatively long, while when the
learning rate is relatively large, the initial stage of training may be relatively small.
We empirically find that to ensure the training process follows a gradient follow, where the loss
decays monotonically, we have to select a smaller learning rate for large multiplicity p. Therefore, it
looks like we have a longer training in our experiments with large p. Note that for a small learning
rate in the experiments of small p, we can observe similar phenomena.
In all subsequent experiments in the Appendix, we will no longer show the loss graph of each
experiment one by one, but we make sure that they are indeed in the initial stage of training.
17
Under review as a conference paper at ICLR 2022
Figure 7: Losses from Fig. 1 to Fig.4. The original pictures and the numbers of steps corresponding
to each sub-picture are written in the sub-captions.
18
Under review as a conference paper at ICLR 2022
Figure 8: Losses from Fig. 5 to Fig.6. The original pictures and the numbers of steps corresponding
to each sub-picture are written in the sub-captions.
19
Under review as a conference paper at ICLR 2022
A.4 Several steps during the evolution of condensation at the initial stage
In the article, we only give the results of the last step of each condense, while the details of the
evolution of condensation are lacking, which may provide us a better understanding. Therefore, we
show these details in Fig. 9, Fig. 10, Fig. 11 and Fig. 12, which also further illustrate the rationality
of the experimental results and facilitate the understanding of the evolution of condensation in the
initial stage.
(a) Step 1
(b) Step 2

(c) Step 4	(d) Step 6	(e) Step 8
(j) Step 61
(f) Step 30	(g) Step 51	(h) Step 54	(i) Step 58
(k) Step 150	(l) Step 215	(m) Step 220	(n) Step 225	(o) Step 250
Figure 9: Evolution of condensation from Fig. 1(b) to Fig. 2(b) and Fig. 1(c). The evolution from
the first row to the fifth row are corresponding to the Fig. 1(b), Fig. 1(c), Fig. 4(c), Fig. 2(a), Fig.
2(b). The numbers of evolutionary steps are shown in the sub-captions, where sub-figures in the last
row are the epochs in the article.
20
Under review as a conference paper at ICLR 2022
(m) Step 40
(n) Step 60
(o) Step 100
Figure 10: Evolution of condensation from Fig. 2(c) to 2(f) and Fig. 4(a). The evolution from the
first row to the fifth row are corresponding to the Fig. 2(c), Fig. 2(d), Fig. 2(e), Fig. 2(f), Fig. 4(a).
The numbers of evolutionary steps are shown in the sub-captions, where sub-figures in the last row
are the epochs in the article.
21
Under review as a conference paper at ICLR 2022
(u) Step 100
(v) Step 500
Figure 11: Evolution of condensation from Fig. 3(a) to 3(e). The evolution from the first row to the
fifth row are corresponding to the Fig. 3(a), Fig. 3(b), Fig. 3(c), Fig. 3(d), Fig. 3(e). The numbers
of evolutionary steps are shown in the sub-captions, where sub-figures in the last row are the epochs
in the article.
22
Under review as a conference paper at ICLR 2022
(a) Step 20	(b) Step 40
(f) Step 40	(g) Step 80	(h) Step 120	(i) Step 160	(j) Step 200
(k) Step 40
(l) Step 80
(p) Step 40
(q) Step 80
(m) Step 120
(n) Step 160
(o) Step 200
(u) Step 40
(v) Step 80
(w) Step 120
Figure 12: Evolution of condensation from Fig. 6(a) to 6(d) and 4(b). The evolution from the first
row to the fifth row are corresponding to the Fig. 4(b), Fig. 6(a), Fig. 6(b), Fig. 6(c), Fig. 6(d). The
numbers of evolutionary steps are shown in the sub-captions, where sub-figures in the last row are
the epochs in the article.
(r) Step 120	(s) Step 160	(t) Step 200
(x) Step 160
(y) Step 200
23
Under review as a conference paper at ICLR 2022
A.5 The experiments on CIFAR 10 and CIFAR 100 with Resnet18-like neural
NETWORK
The condensation of the Resnet18-like neural network on CIFAR10 and CIFAR100 is shown in Fig.
13 and Fig. 14, whose activation functions for fully-connected (FC) layers are tanh(x), ReLU(x),
sigmoid(x), softplus(x) and x tanh(x), indicated by the corresponding sub-captions, respectively.
(a) tanh(x)	(b) ReLU(x)	(c) sigmoid(x)	(d) softplus(x)	(e) x tanh(x)
Figure 13: Condensation of Resnet18-like neural networks on CIFAR10. Each network consists
of the convolution part of resnet18 and fully-connected (FC) layers with size 1024-1024-10 and
softmax. The color in figures indicates the inner product of normalized input weights of two neurons
in the first FC layer, whose indexes are indicated by the abscissa and the ordinate, respectively. We
discard the hidden neurons, in which the L2-norm of each input weight is smaller than 0.001, while
remaining ones bigger than 0.05 in (b). The convolution part is equipped with ReLU activation
and initialized by Glorot normal distribution (Glorot & Bengio, 2010). The activation functions are
tanh(x), ReLU(x), sigmoid(x), softplus(x) and x tanh(x) for FC layers in (a), (b), (c), (d) and
(e), separately. The numbers of steps selected in the sub-pictures are epoch 20, epoch 8, epoch 30,
epoch 30 and epoch 61, respectively. The learning rate is 3 × 10-8, 5 × 10-6, 1 × 10-8, 1 × 10-8
and 5 X 10-6, separately .The FC layers are initialized by N(0, --3-), and Adam optimizer with
mout
cross-entropy loss and batch size 128 are used for all experiments.
Figure 14: Condensation of Resnet18-like neural networks on CIFAR100. Each network consists
of the convolution part of resnet18 and fully-connected (FC) layers with size 1024-1024-10 and
softmax. The color in figures indicates the inner product of normalized input weights of two neurons
in the first FC layer, whose indexes are indicated by the abscissa and the ordinate, respectively. We
discard about 15% of the hidden neurons, in which the L2-norm of each input weight is smaller
than 0.0001, while remaining ones bigger than 0.0025 in (b). The convolution part is equipped
with ReLU activation and initialized by Glorot normal distribution (Glorot & Bengio, 2010). The
activation functions are tanh(x), ReLU(x), sigmoid(x), softplus(x) and x tanh(x) for FC layers
in (a), (b), (c), (d) and (e), separately. The numbers of steps selected in the sub-pictures are epoch 10,
epoch 10, epoch 10, epoch 10 and epoch 250, respectively. The learning rate is 1 × 10-7, 1 × 10-7,
3 × 10-8, 3 × 10-8 and 1 × 10-6, separately. The FC layers are initialized by N(0, -4—), and
mout
Adam optimizer with cross-entropy loss and batch size 128 are used for all experiments.
24
Under review as a conference paper at ICLR 2022
A.6 Multilayer experimental
The condensation of the six layer without residual connections is shown in 15, whose activation
functions for hidden layer 1 to hidden layer 5 are x2 tanh(x), x tanh(x), sigmoid(x), tanh(x) and
softplus(x), respectively.
The condensation of the three layer without residual connections is shown in 16, whose activation
functions are same for each layer indicated by the corresponding sub-captions.
The condensation of the five layer without residual connections is shown in 17, whose activation
functions are same for each layer indicated by the corresponding sub-captions.
The condensation of the five layer with residual connections is shown in 18, whose activation func-
tions are same for each layer indicated by the corresponding sub-captions.
Figure 15: Condensation of six-layer NNs without residual connections. The activation functions for
hidden layer 1 to hidden layer 5 are x2 tanh(x), x tanh(x), sigmoid(x), tanh(x) and softplus(x),
respectively.The numbers of steps selected in the sub-pictures are epoch 6800, epoch 6800, epoch
6800, epoch 6800 and epoch 6300, respectively, while the NN is only trained once. The color
indicates D(u, v) of two hidden neurons’ input weights, whose indexes are indicated by the abscissa
and the ordinate, respectively. The training data is 80 points sampled from a 3-dimensional function
P3k=1 4 sin(12xk + 1), where each xk is uniformly sampled from [-4, 2]. n = 80, d = 3, m = 18,
dout = 1, var = 0.0082, lr = 5 × 10-5.
(a) tanh(x)
(b) x tanh(x)
(c) x2 tanh(x)
(g) tanh(x)
(h) x tanh(x)
(i) x2 tanh(x)
(j) ReLU(x)
(k) sigmoid(x)
(l) softplus(x)
Figure 16:	Three-layer NN at epoch 700. (a-f) are for the input weights of the first hidden layer and
(g-l) are for the input weights of the second hidden layer. The color indicates D(u, v) of two hidden
neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate, respectively.
The training data is 80 points sampled from a 5-dimensional function P5k=1 3 sin(8xk + 1), where
each xk is uniformly sampled from [-4, 2]. n = 80, d = 5, m = 50, dout = 1, var = 0.0052.
lr = 10-4, 2 × 10-5, 1.4 × 10-5 for (a-d), (e) and (f), respectively. For (d) and (j), we discard
hidden neurons, whose L2-norm of its input weight is smaller than 0.1.
25
Under review as a conference paper at ICLR 2022
(a) tanh(x)
(b) tanh(x)
(c) tanh(x)
(d) tanh(x)
(i) x2 tanh(x)
(j) x2 tanh(x)
D(u, V)
(k) x2 tanh(x)
(l) x2 tanh(x)
(m) sigmoid(x)
(n) sigmoid(x)
D(u,⑺
Ii
iLII
O Neu index 17
(o) sigmoid(x)
(p) sigmoid(x)
0I
O Neu index 17
(q) sof tplus(x)
Neu index
π1∙0
□I；
(r)	sof tplus(x)
O Neu index 17
(s)	sof tplus(x)
(t) sof tplus(x)






D(u, 0


Figure 17:	Five-layer NN. The first to fourth columns of each row are for the input weights of
neurons from the first to the fourth hidden layers, respectively. The color indicates D(u, v) of two
hidden neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate, respec-
tively. The training data is 80 points sampled from a 5-dimensional function P3k=1 3 sin(10xk + 1),
where each xk is uniformly sampled from [-4, 2]. n = 80, d = 5, m = 18, dout = 1,
var = 0.0082. lr = 1.5 × 10-5, 1.5 × 10-5, 1.5 × 10-5, 1.5 × 10-5, 1.5 × 10-6 and
epoch is 10000, 10000, 26000, 10000, 20000 for tanh(x), xtanh(x), x2tanh(x), sigmoid(x),
softplus(x), respectively.
26
Under review as a conference paper at ICLR 2022
(a) tanh(x)
(b) tanh(x)
(c) tanh(x)
(d) tanh(x)
(f) x tanh(x)


(e) x tanh(x)
(g) x tanh(x)
(h) x tanh(x)
(q) sof tplus(x)
(r) sof tplus(x)
(s) sof tplus(x)
(t) sof tplus(x)
Figure 18: Five-layer NN. The first to fourth columns of each row are for the input weights of
neurons from the first to the fourth hidden layers, respectively. The color indicates D(u, v) of two
hidden neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate, respec-
tively. The training data is 80 points sampled from a 5-dimensional function P3k=1 3 sin(10xk + 1),
where each xk is uniformly sampled from [-4, 2]. n = 80, d = 5, m = 18, dout = 1, var = 0.0082.
lr = 1×10-4, 1×10-4, 1×10-4, 5×10-5, 5×10-5 and epoch is 400, 400, 400, 3000, 360, 400
for tanh(x), xtanh(x), x2tanh(x), x2tanh(x), sigmoid(x), sof tplus(x), respectively.
(n) sigmoid(x)


(m) sigmoid(x)
(o) sigmoid(x)
(p) sigmoid(x)


27