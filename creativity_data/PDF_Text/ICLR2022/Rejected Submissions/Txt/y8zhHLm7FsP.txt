Under review as a conference paper at ICLR 2022
Ensemble Kalman Filter (EnKF)
for Reinforcement Learning (RL)
Anonymous authors
Paper under double-blind review
Ab stract
This paper is concerned with representing and learning the optimal control law
for the linear quadratic Gaussian (LQG) optimal control problem. In recent years,
there is a growing interest in re-visiting this classical problem, in part due to the
successes of reinforcement learning (RL). The main question of this body of re-
search (and also of our paper) is to approximate the optimal control law without
explicitly solving the Riccati equation. For this purpose, a novel simulation-based
algorithm, namely an ensemble Kalman filter (EnKF), is introduced in this paper.
The algorithm is used to obtain formulae for optimal control, expressed entirely in
terms of the EnKF particles. For the general partially observed LQG problem, the
proposed EnKF is combined with a standard EnKF (for the estimation problem)
to obtain the optimal control input based on the use of the separation principle.
The theoretical results and algorithms are illustrated with numerical experiments.
1	Introduction
This paper is concerned with the problem of reinforcement learning (RL) in continuous-time and
continuous (Euclidean) state-space settings. A special case is the linear quadratic Gaussian (LQG)
problem where the dynamic model is a linear system, the cost terms are quadratic, and the distribu-
tions - of the random initial condition and the noise - are Gaussian.
The LQG problem has a rich and storied history in modern control theory going back to the very
origin of the subject (Kalman, 1960). To obtain the optimal control, the bottleneck is to solve the
Riccati equation - the differential Riccati equation (DRE) in finite-horizon settings or the algebraic
Riccati equation (ARE) in the infinite-horizon settings of the problem. There is a body of literature
devoted to the study of these equations (Bittanti et al., 2012; Lancaster & Rodman, 1995) with
specialized numerical techniques to compute the solution (Laub, 1991; Benner & BujanoviC, 2016).
There are two issues which makes the LQG and related problems a topic of recent research interest:
(i) In high-dimensions, the matrix-valued nature of the DRE or ARE means that any algorithm is
O(d2) in the dimension d of the state-space; and (ii) the model parameters may not be explicitly
available to write down the DRE (or the ARE) let alone solve it. The latter is a concern, e.g., when
the model exists only in the form of a black-box numerical simulator.
These two issues have motivated the recent research on the infinite-horizon linear quadratic regulator
(LQR) problem (Fazel et al., 2018; Mohammadi et al., 2021b; Tu & Recht, 2019). In LQR, the
bottleneck is to solve an ARE. The algorithms studied in the recent papers seek to bypass solving
an ARE by directly searching over the space of stabilizing gain matrices. Global convergence rate
estimates are established for both discrete-time (Fazel et al., 2018; Dean et al., 2020; Malik et al.,
2020; Mohammadi et al., 2021a) and continuous-time (Mohammadi et al., 2020a;b; 2019; 2021b)
settings of the LQR problem. Extensions to the H∞ regularized LQR (Zhang et al., 2020) and
Markov jump linear systems (Jansch-Porto et al., 2020) have also been carried out.
The motivation and goals of the present work are related to these recent papers, albeit the proposed
solution approach is very different. Our inspiration comes from data assimilation (Reich & Cotter,
2015), where in practical applications, e.g., in weather prediction, (i) only simulation-based models
are available, and (ii) these are very high-dimensional. The ensemble Kalman filter (EnKF) is an
efficient simulation-based algorithm to assimilate sensor data in these applications without the need
to explicitly solve a DRE (Evensen, 1994; 2006; Houtekamer & Mitchell, 2001).
1
Under review as a conference paper at ICLR 2022
Our contribution: A novel extension of the EnKF algorithm is proposed to learn the optimal
control law for the general (stochastic and partially observed) setting of the LQG optimal control
problem. The proposed algorithm is simulation-based (using particles) and, in particular, avoids the
need to solve the DRE. Assuming full-state feedback (e.g, setting of the LQR problem), the optimal
control law is directly obtained. For the partially observed case, the proposed EnKF is combined
with a standard EnKF (for the estimation problem) to obtain the optimal control input, based on the
use of the separation principle.
The algorithm is shown to be exact in its mean-field (when the number of particles N = ∞) limit
(Prop. 2). For the finite-N approximation, an error bound is obtained (Eq. (14)). An extensive
discussion is included to provide an intuitive explanation of the algorithm, situate the algorithm in
the RL landscape (see discussion after Eq. (12) in Sec. 2.2), and to compare and contrast the algo-
rithm with the state-of-the-art (Sec. 2.5). For this purpose, numerical comparisons with competing
algorithms are also included (Fig. 2 (c)).
The outline of the remainder of this paper is as follows. The LQG optimal control problem and
its simulation-based solution is described in Sec. 2. The algorithms are illustrated with numerical
examples in Sec. 3 where comparisons with the state-of-the-art algorithms are also described.
2	Linear Quadratic Gaussian (LQG) problem
Problem: The partially observed linear Gaussian model is expressed using the Ito-Stochastic dif-
ferential equations (SDE) as
dXt =	AXt dt + BUt dt + dξt, Xo 〜N (mo, ∑o)	(1a)
dZt =	HXt dt + dζt, Z0 = 0	(1b)
where	X	=	{Xt ∈ Rd : t ≥	0} is the hidden state process, U = {Ut ∈ Rm	:	t	≥	0}	is	the	control
input, and Z	=	{Zt	∈ Rp : t	≥ 0} is the observation process. The model parameters A, B,	H are
matrices of appropriate dimensions, the process noise ξ = {ξt ∈ Rd : t ≥ 0} and the observation
noise ζ = {ζt ∈ Rp : t ≥ 0} are Wiener processes (w.p.) with covariance Q and R, respectively. It
is assumed that Xo, ξ and ζ are mutually independent and R 0.
The LQG optimal control objective is to minimize
J(U) = E (/T 2∣CXt∣2 + 1 UIRUt dt + 2X>PTXT)	(2)
Itis assumed that (A, B) is controllable, (A, C) and (A, H) are observable, and the matrices PT 0
and R 0. In the fully observed settings, Ut is allowed to be a function of Xt . In the partially
observed settings, Ut is a function of the past observations {Zs : 0 ≤ s ≤ t}. For this purpose, it is
convenient to denote Zt := σ({Zs : 0 ≤ s ≤ t}) as the sigma-field generated by the observations,
and consider control inputs U which are adapted to the filtration Z := {Zt : t ≥ 0}.
Classical solution: Using the seperation principle1 the LQG controller is obtained in three steps:
Step 1. Filter design: The objective of the filter design step is to compute the causal estimate
ʌ ʌ
Xt := E(Xt|Zt) for t ≥ 0. The evolution equation for {Xt : t ≥ 0} is the Kalman-Bucy filter.
Notably, the optimal gain matrix for the filter is obtained by solving a forward (in time) DRE.
Step 2. Control design: The objective of the control design step is to compute the optimal feed-
back control law {ut(x) : 0 ≤ t ≤ T, x ∈ Rd} for the fully observed LQG problem. It is well
known to be of the linear form
ut(x) = Ktx,	0 ≤ t ≤ T
where the optimal gain matrix Kt is obtained by solving a backward (in time) DRE.
1The separation principle hinges on the assumption that the control input Ut does not change the observation
sigma-field Zt . This is valid under very mild assumptions on the control policy, e.g., Lipschitz with respect to
estimate Xt (Van Handel, 2007, Sec. 7.3), (Georgiou & Lindquist, 2013).
2
Under review as a conference paper at ICLR 2022
Step 3. Certainty equivalence: The optimal control input is obtained by combining the results
from steps 1 and 2:
ʌ ʌ
Ut = Ut(Xt) = KtXt,	0 ≤ t ≤ T
The bottleneck is to solve the DREs - the forward DRE for the optimal filter gain and the backward
DRE for the optimal control gain2 *. In the following three sections, we describe a simulation-based
algorithm for these three steps which avoids the need to explicitly solve the DREs.
2.1	Step 1. Filter design using EnKF
The filter design objective is to compute the causal estimate Xt = E(XJZt). In the linear Gaussian
settings, the conditional distribution of the Xt is Gaussian whose mean and variance are denoted as
mt and Σt , respectively. These evolve according to the Kalman-Bucy filter:
dmt = Amt dt + BUt dt + Lt( dZt - Hmt dt),	m0 = E(X0)	(3a)
康Σt = A∑t + ∑tA> + Q - ∑tH>R-1 HΣt, Σo = Var(X0)	(3b)
where Lt = ΣtH>R-1 is the Kalman gain. Note that (3b) is a forward (in time) DRE and its
solution Σt is used to compute the optimal Kalman gain Lt .
The EnKF is a simulation-based algorithm to approximate the Kalman filter, that does not require
an explicit solution of the DRE (3b). The design of an EnKF proceeds in two steps:
1.	Construct a stochastic process, denoted by X := {Xt ∈ Rd : t ≥ 0}, such that the conditional
distribution (given Zt) of Xt is equal to the conditional distribution of Xt;
2.	Simulate N stochastic processes, denoted by {Xti : t ≥ 0, 1 ≤ i ≤ N}, to empirically approxi-
mate the distribution of Xt.
The process X is referred to as the mean-field process. The N processes in the step 2 are referred to
as particles. The construction ensures that the EnKF is exact in the mean-field (N = ∞) limit. That
is, for any bounded and continuous function f,
N
Ef (Xt)∣Zt] St= 1 Ef(Xt)|Zt] s≈2 N X f (Xi)
exactness condition	i=1
The details of the two steps are as follows:
Mean-field process: The mean-field process is constructed as
一 一	H H	HX + Hm,+	-
dXt = AXt dt + BUt dt + dξt + Lt( dZt-------------2-------dt), X。〜N(mo, ∑o)	(4)
where ξ := {8 : t ≥ 0} is an independent copy of the process noise ξ, Lt ：= ∑tH>R-1 is the
Kalman gain and
mt ：= E[Xt∣Zt], Σt ：= E[(Xt - mt)(Xt - mt)>∣Zt]
are the conditional mean and the conditional covariance, respectively, of Xt. The right-hand side
of (4) depends upon both the process (Xt) as well as the statistics of the process (mt, ∑t). Such an
SDE is an example ofa McKean-Vlasov SDE. The proof of the following proposition is included in
the Appendix A (see also Taghvaei & Mehta (2020, Theorem 1)).
Proposition 1 (Exactness of EnKF). Consider the mean-field EnKF (4) initialized with a Gaussian
initial condition X。〜N(mo, ∑o). Suppose the control input U is a Z-adapted stochastic process.
Then its solution Xt is a Gaussian random variable whose conditional mean and variance
Tmt	mt,	∑t ∑t, a.s., t > 0
evolve the same as the Kalman filter (3).
2In the steady-state or infinite horizon settings (as T → ∞), one may replace the optimal filter gain and the
optimal control gain by their steady-state values. These are directly obtained by solving the respective AREs.
3
Under review as a conference paper at ICLR 2022
Finite-N approximation: The mean-field process is simulated as an interacting particle system:
dXti = AXtidt+BUtdt+ dξti+L(tN)(dZt
HXi +FXN) dt),	X0 ⅛d N(mo, ∑o) (5)
'---------------------------------------------------------}
z
|
i-th copy of model (1a)
data assimilation step
z
where X(N) := N PN=I Xi is the empirical mean and
N
LtN) = N-I X(Xi)(HXi- HXt(N))>R-1
(6)
is the empirical approximation of the optimal Kalman gain matrix. The system (5) is referred to
as the square root form of the EnKF (Bergemann & Reich, 2012, Eq (3.3)). Note that the gain is
approximated entirely in terms of particles without solving the DRE (3b).
The EnKF (5) is an example of a simulation-based algorithm in the sense that N copies of the
model (1a) are simulated in parallel. The simulations are coupled through a term which is referred
to as the data assimilation step. This term has a gain times error feedback control structure.
2.2 Step 2. Control design using EnKF
This section contains the main contribution of this paper. Assuming full-state feedback, the optimal
control law for the LQG problem (1a)-(2) is
ut(x) = Ktx where Kt = -R-1B>Pt
is the optimal gain matrix and Pt is a solution of the backward (in time) DRE
-dPPt = A>Pt + PtA + C>C - PtBR-1 B^Pt,	PT (given)
(7)
It is known that Pt	0 for 0 ≤ t ≤ T whenever PT	0 (Brockett, 2015, Sec. 24). Therefore,
-1
St = Pt-1 is well-defined. It is straightforward to verify that St also solves a backward DRE
dSSt = ASt + StA> - BR-1B> + StC>CSt,	ST = PTI
(8)
The bottleneck is to solve the DRE (7). In the following, an EnKF is proposed to obtain a simulation-
based approximation of the optimal control law ut. As before, the construction proceeds in two
steps: (i) definition of an exact mean-field process and (ii) its finite-N approximation.
Mean-field process: The objective is to construct a stochastic process, denoted 匕 ∈ Rd at time
t, whose variance equals Pt, the solution of the DRE (7). This is done by constructing Y = {Yt ∈
Rd : 0 ≤ t ≤ T} as a solution of the following backward (in time) McKean-Vlasov SDE:
一	.一 __	1一_丁一 一、	一	、
dYt = AYt dt + B dηt + 2StC>(CYt + Cnt) dt, YT 〜N(0,St)
(9)
and defining
Yt ：= ST1 (Yt- nt)
where η = {η ∈ Rm : t ≥ 0} is a w.p. with covariance matrix R-1, dη in (9) denotes the
backward Ito-integral (Nualart & PardOux, 1988, Sec. 4.2), and
nt ：= E[Yt],	St = E[(Yt- nt)(Yt- nt)>]
(10)
The proof of the following proposition is included in Appendix B.
Proposition 2. Consider the mean-field EnKF (9) initialized with a Gaussian initial condition YT 〜
N(0, ST). Then its solution Yt is a Gaussian random variable whose mean and variance
nt = 0,	St = St,	0 ≤ t ≤ T
一
4
Under review as a conference paper at ICLR 2022
Consequently, Yt is also a Gaussian random variable with mean and variance
E(匕)=0,	E(匕Y>)= Pt,	0 ≤ t ≤ T
and therefore (i) If the matrix B is explicitly known then the optimal gain matrix
Kt = -R-1B>E(YtY>)
or else (when it is not) then (ii) define the Hamiltonian3 (or the Q-function) (Mehta & Meyn, 2009)
H(x, a,t) := 2|Cx|2 + 2a>Ra + x>E(YtY>)(Ax + Ba)
from which the optimal control law is obtained as ut(x) = arg mina∈Rm H(x, a, t).
Finite-N approximation: The mean-field process is empirically approximated by simulating a
system of interacting particles {Yti ∈ Rd : 0 ≤ t ≤ T, i = 1, . . . , N} according to
dYi = AYi dt + B d 万 i + SltN )C > (
'''^^^^^^^{^^^^^^^"'r
i-th copy of model (1a)
Yti = (St(N))-1(Yti -nt(N))
where ηi is a copy of η, and
N
nt(N) =N-1XYti,	St(N)
i=1
CYi+CntN))dt,	γτ ⅛dN(o,P-i)
2
—一一	J
^^{^^™
RL step
N
N-I X(Yi -n(N))(Yi - n(N))>
i=1
(11)
(12)
|
}
Relationship to RL: The following intuitive explanation is included to situate the proposed
simulation-based algorithm in the RL landscape:
Representation: In designing any RL algorithm, the first issue is representation - how does one rep-
resent the unknown value function (Pt in the linear case)? Our novel idea - the first key innovation
of this paper - is to represent Pt is in terms of statistics (variance) of the particles. Such a represen-
tation is fundamentally distinct from representing the value function, or its proxies, such as the Q
function, in terms of a set of basis functions (Bradtke et al., 1994; Devraj et al., 2020; Maei et al.,
2010; Fujimoto et al., 2018; Melo et al., 2008).
The algorithm is entirely simulation based: N copies of the model (1a) are simulated in parallel
where the terms on the righthand-side of (11) have the following intuitive explanation:
Dynamics: The first term “AYti dt” on the right-hand side of (11) is simply a copy of uncontrolled
dynamics in the model (1a).
Control: The second term is the control input “BUt dt” implemented as “B dηi”. That is, the control
input U for the i-th particle is a white noise process with covariance R-1. One may interpret this as
an exploration step whereby the cheaper control directions are explored more.
In summary, for the i-th particle, the dynamics and control are the same as any RL algorithm (white
noise is used for exploration). The difference arises due to the third term (which is the second key
innovation of this paper).
RL step: The third term indicated as the RL step engenders a particle flow that effectively imple-
ments the value iteration step of the RL. There are several points to be made:
1.	The RL step is a function of the state cost term in (2). This is most easily seen by writing
the equation for the empirical mean
dn(N) = (A + S(NNC>C)ntN) dt + B dτ?(?	n(N) = 0
where ηt(N) = N -1 Pi ηti. Noting that the state cost term is x>C>Cx, the RL step imple-
ments a gradient with St(N)C> as the gain matrix.
3The Hamiltonian H(x, a, t) is in the form of an oracle because (Ax + Ba) is the right-hand side of the
simulation model (1a).
5
Under review as a conference paper at ICLR 2022
2.	The RL step has a linear feedback control structure and serves to couple the particles.
Without the RL step, the particles are independent of each other.
3.	This form of the RL step is possible only if one has access to a simulator and an ability
to add additional terms outside the control channel (same as the data assimilation step in
estimation). For example, this is not possible when the system exists only as an experiment.
Arrow ofsimulation time: The particles are simulated backward - from terminal time t = T to initial
time t = 0. This is consistent with the dynamic programming (DP) equation which also proceeds
backward in time. However, because the focus of the RL is on the infinite-horizon problem, this
important property of DP is not considered in algorithm design. It is noted that the infinite-horizon
problem is easily handled in our formulation by taking T as suitably large (which is how the infinite-
horizon limit is defined in DP).
It remains to use the finite-N system to approximate the optimal control law.
Optimal control: There are two cases to consider: (i) If the matrix B is explicitly known then4
N
K(N) = -N-IX R-1(B>Yti)(Yti)>	(13)
i=1
or else (when it is not) then (ii) approximate the Hamiltonian as
1N
H(N)(x,a,t) := 1 |Cx|2 + 1 a>Ra + ——E(X>γi)(γi)> (Ax + Ba)
i=1	' 一寸一 }
i=1	model (1a)
from which the optimal control law is obtained as
ut(N ) (x) = arg min H(N ) (x, a, t)
a∈Rm
There are several zeroth-order approaches to solve the minimization problem, e.g., by constructing
2-point estimators for the gradient. Since the objective function is quadratic and the matrix R is
known, m queries of H(N) (x, ∙,t) are sufficient to compute U(N) (x).
2.3	EnKF-based algorithm for the LQG controller
By combining the result of steps 1 and 2, the optimal control input
J KtN)X(N)	if B is known
t	Iu(N)(X(N)) o.w.
The overall algorithm including steps 1, 2 and 3 is tabulated in the Appendix C where additional
remarks on numerical approximation of backward SDEs also appear.
2.4	Convergence and error analysis
The mean-field process (9) represents the mean-field limit of the finite-N system (11), as the number
of particles N → ∞. The convergence analysis is a delicate matter based on the propagation of
chaos (Bishop & Del Moral, 2018). In Appendix D, under certain additional assumptions on system
matrices, the following error bound is derived:
E[ks(N) - StkF] ≤ 3kNFe-t4μ-N)(T-t) + cN,	0 ≤ t ≤ T	(14)
where μ and Cvar are positive constants and IlTlF is the Frobenius (matrix) norm.
The computational complexity ofan ENKF is O(N d). EnKF is a workhorse in applications such as
weather prediction where models are simulation-based (Evensen, 2006; Reich & Cotter, 2015). In
these applications, the number of simulations N << d.
4Eq. (13) for the optimal control gain matrix Kt(N) is dual to the eq. (6) for optimal filter gain matrix L(tN).
6
Under review as a conference paper at ICLR 2022
2.5	Comparison with related work
Classical RL algorithms for the LQR problem are based on a linear function approximation, us-
ing quadratic basis functions, of the value function or its surrogate the Q-function (Bradtke et al.,
1994). Convergence guarantees typically require (i) a persistence of excitation condition, and (ii)
use of the on policy methods whereby the parameters are learned for a given fixed policy (which is
subsequently improved). For the deterministic LQR problem, the persistence of excitation condi-
tion is difficult to justify theoretically, and in practice can lead to poor performance related to slow
convergence rates. These limitations have spurred recent research on the LQR problem.
Our algorithm is compared to the model-free policy optimization based methods of Mohammadi
et al. (2021a) and Fazel et al. (2018). In policy optimization, the value J(U) is minimized over
the search space of stabilizing gain matrices, using a gradient-descent approach, starting from a
stabilizing controller (which is not straightforward to obtain in a model-free setting). For each
perturbation of the gain matrix, the gradient is estimated by simulating N particles (trajectories)
over a time-horizon [0, T]. Multiple iterations are needed to converge to the minimizer.
In contrast, using the EnKF, the matrix Pt is approximated by simulating N particles once over the
time-horizon [0, T]. The optimal control is obtained from an application of the minimum principle
that only requires m evaluations at each t (the minimization of Hamiltonian is an online calculation).
The trade-off between the two approaches is as follows: While policy optimization methods require
multiple iterations with a small number of particles, the EnKF requires only a single iteration with
relatively larger number of particles. As illustrated with the aid of numerical examples in Sec. 3,
this can lead to an order of magnitude gain in the computation time.
As a final point, our paper provides a simulation-based algorithm for the most general class of linear
quadratic problem: fully or partially observed, finite or infinite-horizon, stochastic or deterministic,
and furthermore does not require an initial stabilizing controller.
Duality: In Appendix E, it is shown that the Gaussian density of the random variable 匕 equals a
log transformation of the value function (at time t) of the LQG optimal control problem. Additional
remarks are also included to expound the duality roots of the proposed algorithm, including a survey
of the relevant literature on this beautiful subject.
3	Numerical Simulations
In all the following three numerical examples, we consider the infinite-horizon fully observed deter-
ministic LQR problem. Although our algorithms are more generally applied, such a choice allows
us to compare and contrast with the recent RL work on the LQR problem. For the LQR problem,
the solution of the ARE is denoted P∞ and the associated optimal feedback gain is denoted K∞.
In each of the three numerical examples, a time-horizon T is fixed. Over the time-horizon, the finite-
N EnKF algorithm (11) is run to obtain an empirical approximation {Pt(N) ∈ Rd×d : 0 ≤ t ≤ T}.
For the sake of comparison, the exact {Pt ∈ Rd×d : 0 ≤ t ≤ T} is obtained by numerically solving
the backward DRE (7). Typically, we chose PT = I, the identity matrix.
3.1	Exponential convergence
An attractive feature of the proposed EnKF algorithm is that the mean-field limit is exact. This means
the EnKF recovers the properties of the DRE in the limit. One such property is the exponential
convergence: For any fixed t, Pt → P∞ as T → ∞, starting from any initialization PT (Ocone
& Pardoux, 1996, Remark 2.1). Moreover, because of the error formula (14), Pt(N) → P∞ with a
small additional bias that decays as O(N).
For a d = 2 dimensional system, Fig. 1 depicts this exponential convergence of the four entries
of the symmetric Pt(N) matrix (using N = 100 particles). As part of Appendix F.1, additional
figures are included to depict exponential convergence of the 100 entries of the 10 × 10 matrix Pt(N)
for a d = 10 dimensional system (using N = 1000 particles). In these evaluations, the system
(A, B ) is chosen to be in its controllable canonical form where the appropriate entries (last row)
of the A matrix are randomly generated. Because these are randomly generated, the matrix A is
7
Under review as a conference paper at ICLR 2022
Figure 1: The d = 2 example: (a) Open-loop (OL) and closed-loop (CL) eigenvalues; (b)-(d)
Convergence of PtN : Plot of the entries of the matrix Pt(N) shows exponential convergence, with
a small (random) error, to the ARE limit P∞ which is also depicted together with the exact DRE
solution. (Note the x-axis of the plots (b)-(d) is T - t so convergence is easy to see).
often unstable (see also Fig. 1 (a)) which does not pose any problem with our algorithm. Additional
details on the numerics appear as part of Appendix F.1.
3.2	Evaluation and comparison with literature: Mass spring damper system
In order to evaluate and compare the performance of the proposed algorithm, we describe next the
results for the coupled mass-spring system studied in Mohammadi et al. (2019). For each mass,
there are two states, position and velocity. A system with d/2 masses is d-dimensional. To evaluate
the performance of the finite-N algorithm, we consider the following error metric:
MSE :
kPt- PiN ⅛F
kRkF
dt
The expectation is approximated empirically by averaging over 100 simulation runs. See Appendix
F.2 for details on modeling, parameter values, and the numerical discretization. Fig. 2 depicts the
results of the numerical experiments showing the O(N) decay ofMSE as N increases (for d fixed).
Part (c) of the figure depicts a comparison with the two state-of-the-art algorithms, namely Moham-
madi et al. (2021b) and Fazel et al. (2018). In the figure, computation time is plotted against the
relative error in approximating the LQR gain K∞ (plots are qualitatively similar using other error
metrics). The computation time is obtained using the Python Process_time() function from
the time library for measuring execution time. The result depicted in the figure is for d = 10 but
qualitatively similar results are also obtained for other values of d. See Appendix F.3 for details
on numerical implementation of the algorithms. We also implemented the classical RL algorithm
in Bradtke et al. (1994) but its convergence was not reliable because of the persistence of excitation
issues.
Relative error in gain
Number of particles (N)	Dimensions (d)
Figure 2:	(a)-(b) Plot of MSE as a function of the number of particles N and system dimension d;
(c) Comparison with algorithms in Fazel et al. (2018) (labeled [F18]) and Mohammadi et al. (2021b)
(labeled [M21]). The comparisons depict the computation time (in a Python implementation) as a
function of the relative error in approximating the LQR gain K∞ .
8
Under review as a conference paper at ICLR 2022
Based on the preliminary analysis, the main reason for the order of magnitude improvement in
computational time is as follows: An EnKF requires only a single iteration over a fixed time-horizon
[0, T]. Although the number of particles (N) is an order of magnitude larger for the EnKF algorithm,
certain vectorisation features of numpy yield significant gains in computational time. Because the
other algorithms require multiple iterations which must necessarily be carried out serially, these
computations are slower. In our comparisons, we used the same time-horizon T and discretization
time-step δt for all the algorithms. It is certainly possible that some of these parameters can be
optimized to improve the performance of the other algorithms. In particular, one may consider
shorter or longer time-horizon T or use parallelization to speed up the gradient calculation.
3.3 Evaluation of EnkF for a nonlinear cart-pole system
It is a time-honored practice to design the optimal control law for a (linearized) LQR model and
then implement this control law on a nonlinear system. By the principle of linearization, the control
law works well (in theory) for small perturbations and often (in practice) for even large ones. We
consider the nonlinear conservative cart pole model (Tedrake; Rawlik et al., 2013). The control acts
as external force applied to the cart. The four-dimensional state for the system is (θ, x, ω, v), where
θ ∈ S1 (the circle) is the angle of the pole (pendulum) as measured from the stable equilibrium,
X ∈ R is the displacement of cart along the horizontal, and (ω, V) := (θ, X) ∈ R2 is the velocity
vector. The control objective is to balance the pole - stabilize the system at the inverted equilibrium
(π, 0, 0, 0), assuming full state feedback. For the purposes of control design, the nonlinear system
is first linearized at the desired equilibrium and an LQR problem is formulated (see Appendix F.4
for details). For the purposes of evaluation, the optimal control is applied to the nonlinear model.
Figure 3 depict the results of numerical experiments for three different choices of N . It is seen that
as few as N = 10 particles are sufficient to stabilize the equilibrium. Using N = 1000 particles, the
closed-loop trajectories are virtually indistinguishable from the DRE-based solution.
Figure 3:	Trajectories of the closed-loop nonlinear cart pole system.
4 Conclusions
In recent years, there has been a concerted effort to revisit the LQR problem in the context of
RL (Fazel et al., 2018; Mohammadi et al., 2021b; Tu & Recht, 2019; Dean et al., 2020; Malik et al.,
2020; Zhang et al., 2020; Jansch-Porto et al., 2020). The effort is largely in response to the well
known issues that arise when the state and action space are continuous (Lu et al., 2021).
In this paper, we present a new paradigm for RL. There are two key innovations: (i) the representa-
tion of the unknown value function in terms of the statistics (variance) of the particles; and (ii) design
of interactions between simulations to solve the optimal control problem. For the LQR problem, this
is shown to yield a learning rate that closely approximates the exponential rate of convergence of the
solution of the DRE. In numerical examples, this property is shown to lead to an order of magnitude
better performance than the state-of-the-art algorithms. Given the enormous success of EnKF in
data assimilation (Evensen, 2006; Reich & Cotter, 2015), the contributions of this paper potentially
open up new opportunities for RL. It is our hope that the paper will engender new synergies between
the data assimilation and the RL communities.
9
Under review as a conference paper at ICLR 2022
5 Reproducibility statement
The implementable pseudo-code of the algorithm is tabulated in Appendix C. For the three exam-
ples in Sec. 3, the numerical parameters are tabulated in Appendix F. The codes for all numerical
simulations are provided as part of the supplementary material. All the codes have a README file
associated with it. Table 1 provides a list of the Appendix containing the simulation parameters and
location of codes for each numerical example.
Table 1: Location of supplementary material
Section	Appendix	Code Directory
3.1	F.1	Section 3-1
3.2	F.2, F.3	Section 3-2
3.3	F.4	Section 3-3
References
P. Benner and Z. BujanoviC. On the solution of large-scale algebraic Riccati equations by using
low-dimensional invariant subspaces. Linear Algebra and itsApplications, 488:430-459, 2016.
K. Bergemann and S. Reich. An ensemble Kalman-Bucy filter for continuous data assimilation.
Meteorologische Zeitschrift, 21(3):213-219, 2012. doi: 10.1127/0941-2948/2012/0307.
A. N. Bishop and P. Del Moral. On the stability of matrix-valued Riccati diffusions. arXiv preprint
arXiv:1808.00235, 2018. URL https://arxiv.org/abs/1808.00235.
A. N. Bishop, P. Del Moral, K. Kamatani, B. Remillard, et al. On one-dimensional riccati diffusions.
Annals of Applied Probability, 29(2):1127-1187, 2019.
Sergio Bittanti, Alan J Laub, and Jan C Willems. The Riccati Equation. Springer Science & Business
Media, 2012.
S.J. Bradtke, B.E. Ydstie, and A.G. Barto. Adaptive linear quadratic control using policy iteration.
In Proceedings of 1994 American Control Conference -ACC ’94, volume 3, pp. 3475-3479 vol.3,
1994. doi: 10.1109/ACC.1994.735224.
R.	W Brockett. Finite dimensional linear systems. SIAM, 2015.
S.	Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the Sample Complexity of the
Linear Quadratic Regulator. Found Comput Math, 20(4):633-679, August 2020. ISSN
1615-3383. doi: 10.1007/s10208-019-09426-y. URL https://doi.org/10.1007/
s10208-019-09426-y.
P. Del Moral and J. Tugaut. On the stability and the uniform propagation of chaos proper-
ties of ensemble KalmanBucy filters. Ann. Appl. Probab., 28(2):790-850, 04 2018. doi:
10.1214/17-AAP1317. URL https://doi.org/10.1214/17-AAP1317.
A. M. Devraj, A. Busic, and S. Meyn. Fundamental design principles for reinforcement learning
algorithms. In K. G. Vamvoudakis, Y.Wan, F. L. Lewis, and D. Cansever (eds.), Handbook on
Reinforcement Learning and Control. Springer, 2020.
G. Evensen. Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte
Carlo methods to forecast error statistics. Journal of Geophysical Research: Oceans, 99(C5):
10143-10162, 1994. doi: 10.1029/94JC00572.
G. Evensen. Data Assimilation. The Ensemble Kalman Filter. Springer-Verlag, New York, 2006.
M. Fazel, R. Ge, S. Kakade, and M. Mesbahi. Global Convergence of Policy Gradient Methods for
the Linear Quadratic Regulator. In International Conference on Machine Learning, pp. 1467-
1476. PMLR, July 2018. URL http://proceedings.mlr.press/v80/fazel18a.
html. ISSN: 2640-3498.
10
Under review as a conference paper at ICLR 2022
W. H. Fleming. Exit probabilities and optimal stochastic control. Applied Mathematics and Opti-
mization ,4(1):329-346,1977.
W. H. Fleming and S. K. Mitter. Optimal Control and Nonlinear Filtering for Nondegenerate
Diffusion Processes. Stochastics, 8(1):63-77, January 1982. ISSN 0090-9491. doi: 10.1080/
17442508208833228. URL https://doi.org/10.1080/17442508208833228.
S.	Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pp. 1587-1596. PMLR, 10-15 Jul 2018. URL http://proceedings.mlr.press/v80/
fujimoto18a.html.
T.	T. Georgiou and A. Lindquist. The separation principle in stochastic control, redux. IEEE Trans-
actions on Automatic Control, 58(10):2481-2494, 2013.
C.	Hartmann and C. Schutte. Efficient rare event simulation by optimal nonequilibrium forcing.
Journal of Statistical Mechanics: Theory and Experiment, 2012(11):P11004, 2012.
C.	Hoffmann and P. Rostalski. Linear optimal control on factor graphs a message passing per-
spective . IFAC-PapersOnLine, 50(1):6314-6319, 2017. ISSN 2405-8963. doi: https://doi.
org/10.1016/j.ifacol.2017.08.914. URL https://www.sciencedirect.com/science/
article/pii/S2405896317313800. 20th IFAC World Congress.
P.	L. Houtekamer and H. L. Mitchell. A sequential ensemble Kalman filter for atmospheric data
assimilation. Monthly Weather Review, 129(1):123-137, 2001.
J. P. Jansch-Porto, B. Hu, and G. E. Dullerud. Convergence guarantees of policy optimization
methods for Markovian jump linear systems. In 2020 American Control Conference (ACC), pp.
2882-2887. IEEE, 2020.
R. E. Kalman. Contributions to the theory of optimal control. Boletin de la Sociedad Matematica
Mexicana (2), 5:102-109, 1960.
H. J. Kappen and H. C. Ruiz. Adaptive importance sampling for control and inference. Journal of
Statistical Physics, 162(5):1244-1266, 2016.
H. J. Kappen, V. Gomez, and M. Opper. Optimal control as a graphical model inference
problem. Machine Learning, 87(2):159-182, May 2012. ISSN 1573-0565. doi: 10.1007/
s10994-012-5278-7. URL https://doi.org/10.1007/s10994-012-5278-7.
J. W. Kim and P. G. Mehta. An optimal control derivation of nonlinear smoothing equations. In
Proceedings of the Workshop on Dynamics, Optimization and Computation held in honor of the
60th birthday of Michael Dellnitz, pp. 295-311. Springer, 2020.
Peter E Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations.
Springer, Applications of Mathematics, 1999.
Huibert Kwakernaak and Raphael Sivan. Linear optimal control systems. Wiley Interscience, New
York, 1972. ISBN 0471511102.
P. Lancaster and L. Rodman. Algebraic Riccati Equations. Clarendon press, 1995.
A. J. Laub. Invariant subspace methods for the numerical solution of Riccati equations. In The
Riccati Equation, pp. 163-196. Springer, 1991.
Daniel Liberzon. Calculus of Variations and Optimal Control Theory. Princeton University Press,
Princeton, NJ, 2012. ISBN 978-0-691-15187-8. A concise introduction.
F. Lu, P. G. Mehta, S. P. Meyn, and G. Neu. Convex q-learning. In 2021 American Control Confer-
ence (ACC), pp. 4749-4756, 2021. doi: 10.23919/ACC50511.2021.9483244.
11
Under review as a conference paper at ICLR 2022
H. R. Maei, C. Szepesvari, S. Bhatnagar, and R. S. Sutton. Toward off-policy learning control with
function approximation. In Proceedings of the 27th International Conference on International
Conference on Machine Learning, ICML’10, pp. 719726, Madison, WI, USA, 2010. Omnipress.
ISBN 9781605589077.
D. Malik, A.n Pananjady, K. Bhatia, K. Khamaru, P. L. Bartlett, and M. J. Wainwright. Derivative-
Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems. Journal of
Machine Learning Research ,21(21):1-51,2020.1SSN 1533-7928. URL http://jmlr.org/
papers/v21/19-198.html.
P. G. Mehta and S. P. Meyn. Q-learning and Pontryagin’s minimum principle. In Proceedings of
the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese
Control Conference, pp. 3598-3605. IEEE, 2009.
F. S. Melo, S. P. Meyn, and M. I. Ribeiro. An analysis of reinforcement learning with func-
tion approximation. In Proceedings of the 25th International Conference on Machine Learn-
ing, ICML ’08, pp. 664671, New York, NY, USA, 2008. Association for Computing Machin-
ery. ISBN 9781605582054. doi: 10.1145/1390156.1390240. URL https://doi.org/10.
1145/1390156.1390240.
S. K. Mitter and N. J. Newton. A variational approach to nonlinear estimation. SIAM journal on
control and optimization, 42(5):1813-1833, 2003.
H. Mohammadi, A. Zare, M. Soltanolkotabi, and M. R. Jovanovic. Global exponential convergence
of gradient methods over the nonconvex landscape of the linear quadratic regulator. In 2019
IEEE 58th Conference on Decision and Control (CDC), pp. 7474-7479, December 2019. doi:
10.1109/CDC40024.2019.9029985. ISSN: 2576-2370.
H. Mohammadi, M. R. Jovanovic, and M. Soltanolkotabi. Learning the model-free linear quadratic
regulator via random search. In Learning for Dynamics and Control, pp. 531-539. PMLR, July
2020a. URL http://proceedings.mlr.press/v120/mohammadi20a.html. ISSN:
2640-3498.
H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovic. Random search for learning the linear
quadratic regulator. In 2020 American Control Conference (ACC), pp. 4798-4803, July 2020b.
doi: 10.23919/ACC45564.2020.9147749. ISSN: 2378-5861.
H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovic. On the Linear Convergence of Random
Search for Discrete-Time LQR. IEEE Control Systems Letters, 5(3):989-994, July 2021a. ISSN
2475-1456. doi: 10.1109/LCSYS.2020.3006256. Conference Name: IEEE Control Systems
Letters.
Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R. Jovanovic. Conver-
gence and sample complexity of gradient methods for the model-free linear quadratic regulator
problem. IEEE Transactions on Automatic Control, pp. 1-1, 2021b. doi: 10.1109/TAC.2021.
3087455.
Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R. Jovanovi. Conver-
gence and sample complexity of gradient methods for the model-free linear quadratic regula-
tor problem. arXiv:1912.11899 [physics], March 2021c. URL http://arxiv.org/abs/
1912.11899. arXiv: 1912.11899.
D. Nualart and E. Pardoux. Stochastic calculus with anticipating integrands. Probability Theory and
Related Fields, 78(4):535-581, 1988.
D. Ocone and E. Pardoux. Asymptotic stability of the optimal filter with respect to its initial
condition. SIAM Journal on Control and Optimization, 34(1):226-243, 1996. doi: 10.1137/
s0363012993256617.
K. Rawlik, M. Toussaint, and S. Vijayakumar. On stochastic optimal control and reinforcement
learning by approximate inference. In Twenty-third international joint conference on artificial
intelligence, 2013.
12
Under review as a conference paper at ICLR 2022
S.	Reich and C. Cotter. Probabilistic forecasting and Bayesian data assimilation. Cambridge Uni-
versity Press, 2015.
C. Schutte, S. Winkelmann, and C. Hartmann. Optimal control of molecular dynamics using markov
state models. Mathematical programming,134(1):259-282, 2012.
T.	Sutter, A. Ganguly, and H. Koeppl. A variational approach to path estimation and parameter
inference of hidden diffusion processes. Journal of Machine Learning Research, 17:6544-80,
2016.
A. Taghvaei and P. G. Mehta. An optimal transport formulation of the ensemble Kalman filter. IEEE
Transactions on Automatic Control, pp. 1-1, 2020. doi: 10.1109/TAC.2020.3015410.
Amirhossein Taghvaei. Design and analysis of particle-based algorithms for nonlinear filtering and
sampling. PhD thesis, University of Illinois at Urbana-Champaign, 2019.
Amirhossein Taghvaei and Prashant G Mehta. Error analysis for the linear feedback particle filter. In
2018 Annual American Control Conference (ACC), pp. 4261-4266. IEEE, 2018. doi: 10.23919/
ACC.2018.8430867.
R.	Tedrake. Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and
Manipulation (course Notes for MIT 6.832). URL http://underactuated.mit.edu/.
Last accessed on 16 May 2021.
E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforce-
ment learning. The Journal of Machine Learning Research, 11:3137-3181, 2010.
E. Todorov. General duality between optimal control and estimation. In 2008 47th IEEE Conference
on Decision and Control, pp. 4286-4292, Dec 2008.
E. Todorov. Efficient computation of optimal actions. Proceedings of the national academy of
sciences, 106(28):11478-11483, 2009.
M. Toussaint and A. Storkey. Probabilistic inference for solving discrete and continuous state
markov decision processes. ICML ’06, pp. 945952, New York, NY, USA, 2006. Associa-
tion for Computing Machinery. ISBN 1595933832. doi: 10.1145/1143844.1143963. URL
https://doi.org/10.1145/1143844.1143963.
S.	Tu and B. Recht. The Gap Between Model-Based and Model-Free Methods on the Linear
Quadratic Regulator: An Asymptotic Viewpoint. In Conference on Learning Theory, pp. 3036-
3083. PMLR, June 2019. URL http://proceedings.mlr.press/v99/tu19a.html.
ISSN: 2640-3498.
R. Van Handel. Filtering, stability, and robustness. PhD thesis, California Institute of Technology,
2006.
R. Van Handel. Stochastic calculus, filtering, and stochastic control. Course notes., URL http://www.
princeton. edu/rvan/acm217/ACM217. pdf, 14, 2007.
J. Watson and J. Peters. Advancing trajectory optimization with approximate inference: Exploration,
covariance control and adaptive risk. In 2021 American Control Conference (ACC), pp. 1231-
1236, 2021. doi: 10.23919/ACC50511.2021.9482657.
J. Watson, H. Abdulsamad, and J. Peters. Stochastic optimal control as approximate input inference.
In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), Proceedings of the Confer-
ence on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pp. 697-
716. PMLR, 30 Oct-01 Nov 2020. URL https://proceedings.mlr.press/v100/
watson20a.html.
J. Watson, H. Abdulsamad, R. Findeisen, and J. Peters. Stochastic control through approximate
bayesian input inference, 2021.
13
Under review as a conference paper at ICLR 2022
K. Zhang, B. Hu, and T. Basar. On the Stability and Convergence of Robust Adversarial Reinforce-
ment Learning: A Case Study on Linear Quadratic Systems. Advances in Neural Information
Processing Systems, 33:22056-22068, 2020. URL https://proceedings.neurips.cc/
/paper/2020/hash/fb2e203234df6dee15934e448ee88971-Abstract.html.
APPENDIX
A	Proof of Prop. 1
It is first shown that the conditional mean Tmt and the conditional covariance Σt evolve according
to the Kalman filter equations (3). The SDE for the mean is obtained by taking the conditional
expectation of (4),
dmt = Amt dt + BUt dt + Lt(dZt — HTmt dt)
where we used the fact that Ut is assumed to be Zt measurable.
To obtain the equation for the covariance, define the error process et = Xt — Tmt which evolves
according to
det = (A - 2L tH )et dt + dξt
Then, upon an application of the Ito rule
d(ete>) = (A - 2 L tH )(et e>) dt + (ete>)(A — 2 LtH )> dt + Q dt + dξte> + et df>
and taking the conditional expectation, the equation for the conditional covariance Σt = E[ete> |Zt]
is obtained as
-Γ7∑ t = A∑ t + ∑ tA> + Q — ∑ tH >H∑ t
dt
where We used the definition Lt = ∑tH>.
The equation for the conditional covariance Σt is identical to the DRE (3b). Therefore, if the initial
condition Σo = ∑o then Σt = ∑t for all t > 0. This also implies Lt = Lt, which in turn implies
that the SDE for the conditional mean Tmt is identical to Kalman filter equation for mt. If mo = mo
then mt = mt for all t > 0.
By replacing the mean-field terms mt and Σt by exogenous processes mt and ∑t, the McKean-
Vlasov SDE (4) simplifies to an Ornstein-Uhlenbeck SDE. Because the distribution of the initial
condition Xo is Gaussian, the distribution of Xt is also Gaussian and given by N(mt, ∑t).
B Proof of Prop. 2
The proof is similar to the proof of Prop. 1. The equation for the mean n is obtained by taking the
expectation of SDE (9),
dnt = (A + StC >C )nt dt
Because nτ = 0, We have n = 0 for all t ∈ [0, T].
The equation for the covariance St is obtained by writing the SDE for the error et := Yt — nt：
det = (A + 2StC>C)et dt + B dηt,
Using the Ito rule for ete>,
d(ete>) = (A+2 StC >c )(ete>) dt+(ete> )(A+2 StC > C )>—BR-1B>+B dηte> +et(B dηt)>
The Ito correction term appears with a negative sign because the SDE involves a backward Wiener
J
process 办(Nualart & Pardoux, 1988, Sec. 4.2). Taking an expectation yields the following equation
for St：
dtS
2 s
2
> — BR-1B>
14
Under review as a conference paper at ICLR 2022
The SDE is identical to the SDE for St. Because ST = ST, We have St = St for all t ∈ [0, T]. The
conclusion that Yt is Gaussian follows from the fact that with n = n and St = St, the SDE for Yt
is an Ornstein-Uhlenbeck SDE With a Gaussian terminal condition.
The proof for the rest of proposition is straightforward. By definition,
E[Yt] = E[S-1(Yt - nt)] = S-1(nt - nt) = 0
E∖YtYtτ] = E[S-1(Yt - nt)(Yt - nt)>S-1] = S-1 = S-1 = Pt
Since EpPtYτ] = Pt, the optimal gain matrixK = -R-IBTEpPtYtτ]. For a given X ∈ Rd, the
optimal control ut(x) = Ktx = -RTBTEpPt匕τ]x is the unique minimizer of the Hamiltonian.
This is referred to as the minimum principle of optimal control.
C Details of the algorithms to solve the partially observed
LQG problem in Sec. 2
In this section, the implementation details of the EnKF-based algorithms are presented to numeri-
cally solve the partially observed LQG problem (1)-(2) introduced in the main body of the paper.
For better readability, the overall algorithm is broken down into three separate algorithms:
1.	Algorithm 1 is an offline algorithm. It is based on the finite-N approximation of the (con-
trol) EnKF as described in Sec. 2.2 of the main body of the paper. The algorithm is run
offline to obtain an approximation of the {Pt : 0 ≤ t ≤ T}.
2.	Algorithm 2 is an online algorithm. It is based on the finite-N approximation of the (filter)
EnKF as described in Sec. 2.1 of the main body of the paper. The algorithm is run online in
a real-time manner. At each time step, it processes the sensor measurements from the true
system (plant) and computes the optimal control input by calling Algorithm 3.
3.	Algorithm 3 computes the optimal control input. It is based upon minimization of the
Hamiltonian function. In an online implementation, the optimal control input is applied to
the plant at each time step.
The input structure of each of the three algorithms is clearly delineated. In particular, the algorithms
require only the simulator in the form of the function evaluator f(x, u) = Ax+Bu for the dynamics,
c(x) = Cx for the cost function, and h(x) = Hx for the observation function. The algorithms do
not require solution of the DRE.
Remark 1. In a numerical implementation of step 1 and step 2, there are two sources of error:
(i) error on account of finite-N approximation; and (ii) error on account of time discretization
which depends upon the step size ∆t. The finite-N approximation error has been studied in the
EnKF literature where it is shown that the error in approximating kPt - Pt(N) k (in step 1) and the
error in approximating the gain the mean ∣∣Xt — XtN)k (in SteP 2) converges to zero with the rate
O(√=) Del Moral & Tugaut (2018); Bishop et al. (2019); Bishop & Del Moral (2018); Taghvaei
& Mehta (2018); Taghvaei (2019). The time discretization error is also expected to be bounded and
converges to zero with the rate O(∆t) when the system is stable Kloeden & Platen (1999).
15
Under review as a conference paper at ICLR 2022
Algorithm 1 [offline] EnKF algorithm to approximate {Pt : 0 ≤ t ≤ T}
Input: Simulation time T, simulation step-size ∆t, number of particles N, simulator f (x, u)
Ax + Bu, terminal cost PT, cost function c(x) = Cx, and control cost matrix R.
Output: {Pz(N) : k = 0,1, 2,..., ∆Tt}.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
TF = ∆
PT(NF)=PT
Initialize {YTrFF }N=ι 对 N(0, PT-* 1)
calculate n(TN) = N-1 PiN=1 YTi
for k = TF to 1 do
Calculate Ck) = N T P3 c(Yk)
Calculate MkN) = (N - 1)-1 PLlyk-))(C(Yk) - ^kN))>
for i = 1 : N do
△nk% N (0,击 RT)
∆Yk = f (Yk, ∆nk )∆t + 2 MkN )(c(Yk) + CkN )Ν
Yki-1 = Yki - △Yki
end for
Calculate n(kN-)1 = N-1PiN=1Yki-1
CalculateSk(N-)1=(N-1)-1PiN=1(Yki-1-n(kN-)1)(Yki-1-n(kN-)1)>
P(N)	(S(N))-1
Pk-1 = (Sk-1)
end for
Algorithm 2 [online] EnKF algorithm to approximate state estimate Xt and optimal control Ut
Input: Simulation time T, simulation step-size ∆t, number of particles N, simulator f (x, u) =
Ax + Bu, initial distribution N(m0, Σ0), process noise covariance R, observation function
h(x) = Hx, {p(N) : k = 0,1,2,..., ∆ } from the offline algorithm F.1.
Output: estimate {X[N) : k = 0,1,2,..., ∆} and optimal control input {u'N) : k =
1
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
0,1,2,..∙, ∆ - 1}.
Define TF := ∆
Initialize particles {X0}= i% N(m0, ∑o)
for k = 0 to TF - 1 do
Calculate X(N) = N PlN=I Xk
Calculate UkN) = argmina H(X(N), Pk(N)X(N), a) from algorithm 3
Apply control u(kN) to the true system and obtain the observation ∆Zk = Z(k+1)∆t - Zk∆t
Calculate hkN) = N pN=ι h(Xk)
Calculate LkN) = N-T P3(X, X(NNXh(Xii) - h(N))>R-1
for i = 1 to N do
∆ξk⅛i Ν(0,Q∆t)	N
∆Xk = f(Xk,UkN))∆t + ∆ξk + LkN)(∆Zk - h(xki)+hkN)∆t)
Xki +1=Xki+∆Xki
end for
end for
16
Under review as a conference paper at ICLR 2022
Algorithm 3 Computation of optimal control
Input: state x, momentum y, control cost matrix R,
Hamiltonian H(x,y, α) = yT(a(x) + b(x)α) + 2∣c(x)∣2 + 1 α>Rα.
Output: optimal control u = arg minα H(x, y, α).
1:	if b(x) is known then
2:	u = -R-1b(x)> y
3:	else
4:	for k = 1 to m do
5:	ek = [0,...,0,	|{1z} ,0,...,0] ∈ Rm
6:	Uk = H(x, y, RTek)P- H(x, y, 0) - 1 (R-1)kk
7:	end for
8:	end if
D Error analysis
The objective of the error analysis is to study the convergence of the empirical variance of the
particles S(N) to its mean-field limit St as N → ∞. According to the Proposition 2, the mean-field
limit St = St, hence it evolves backward in time according to
-dSt = ASt + StA> + StC>CSt- BRTB>,	ST = PT	(15)
dt
In order to study the error S(N) - St, We obtain a stochastic differential equation for the evolution
of St(N).
Lemma 1. The empirical covariance matrix St(N), defined in (12), evolves according to the stochas-
tic differential equation
dSt(N) = ASt(N) + St(N)A> +St(N)C>CSt(N) - BR-1B> dt+ dMt,	(16)
where Mt is a Martingale given by
N
dMt = N—1 X et(B 丽t)> + B 丽t(ei)>,	4.= Yt - n(N)
N -1 i=1
with quadratic variation
dhMit = -^~t hTr(BR-1B>)s(n) + BRTBTTr(StN)) + BRTB>S(N) + S(N)BR-1B>]
Proof. The evolution for the error process e* it := Yti - n(tN) is obtained by subtracting the evolution
for the empirical mean n(tN) from (11):
1N
de； = Aet + BM t - N X B dηj)+ SiiN )Ceit dt
j=1
The evolution for the empirical covariance S(N) = N-i PN=I et(et)T is obtained by application
of the Ito rule.	■
Note that the first term in the evolution of S(N) in (16) is exactly identical to that of St in (15), while
the second term is an additional stochastic term that converges to zero as N → ∞. Even though
the fluctuations scale as O(N-2), the analysis is challenging as has been noted in literature (see
the remark after Theorem 3.1 in (Del Moral & Tugaut, 2018)). Error analysis of the (stochastic)
ensemble Kalman filter has been carried out in the literature (Del Moral & Tugaut, 2018; Bishop &
Del Moral, 2018) under additional assumptions which we also make below:
Assumption 1. The matrix C>C is identity. And the control matrix B is full rank.
17
Under review as a conference paper at ICLR 2022
The assumption regarding the cost matrix C can be relaxed to CTC being non-singular by a change
of coordinates. The assumption regarding the control matrix is strong. It is an open problem in
the literature regarding stability of EnKF to carry out the error analysis under the more natural
assumption that the system is controllable and observable.
Under assumption 1, we can prove the main result stated in the following proposition.
Proposition 3. Let St be the mean-field covariance defined in (10) and SN be the empirical co-
variance of the particles defined in(16). Then, under Assumption 1, the error between StN) and St
satisfies the upper-bound
E[kStN) - StkF] ≤ 3cμkpTkFe-(4"∞-N)(TTt + Car,	0 ≤ t ≤ T
where μ∞, cμ, c var are positive constants that are independent of time t.
(17)
In the remainder of this section, we prove the proposition 3. To simplify the presentation, we use the
time-reversed quantitative ΩtN) := STN-t and Ωt := ST-t which evolve forward in time according
to
dΩt = (-AΩt - ΩtA> - ΩtC>CΩΩ + BRTB~τ')dt
dΩtN) = (-AΩ(n) - ω(n)A> - ΩtN)C>Cω(n) + BRTB>)dt + dMt
(18)
(19)
where Mt is defined in Lemma 1.
First, we show some basic results regarding the asymptotic properties of the Ricatti equation that
governs Ωt. These results are straightforward application of the classical stability theory of the
Ricatti equation.
Lemma 2. Consider the Ricatti equation (18) that governs Ωt. Then,
1.	There exists a positive definite matrix solution Ω∞ to the ARE
0 = -AΩ∞ - Ω∞A> - Ω∞C>CΩ∞ + BR-1B>
such that -A — Ω∞C>C is Hurwitz.
2.	Ωt converges to Ω∞ exponentially fast
3.	Let μt denote the minimum eigenvalue for the matrix Ft := A + 2C> CΩt. Then, there
exists a positive constant μ∞ > 0 and cμ such that
e- Rt μτ dτ < cμe~μ∞t,	∀t ≥ 0	(20)
Proof. Under Assumption 1, the pair (A, BR- 2) is controllable and the pair (A, C) is observable.
Part (1) follows from (Kwakernaak & Sivan, 1972, Theorem 3.7) and part (2) follows from (Ocone
& Pardoux, 1996, Lemma 2.2 and Theorem 2.3). The proof of part (3) follows from the following
two facts (i) (-A - 2Ω∞C>C) is Hurwitz because
-BR-1 B> = (-A - 2ω∞C>C)Ω∞ + Ω∞(-A - 2ω∞C>C)>
is a Lyapunov equation, with BR-1B> positive definite since B is assumed to be full rank; and
(ii) Ωt converges to Ω∞ exponentially fast. Therefore, there exists a time τι > 0 such that (-A -
1 ΩtC>C is Hurwitz for t ≥ τι. As a result, the minimum eigenvalue μt of Ft = A + 2ΩtC>C is
lower-bounded μt ≥ μ∞ > 0 for t ≥ τι. This concludes the bound (20) where cμ = e- ʃθ1 μ dτ ■
Next, we obtain a bound for E[Tr(Ω(N))]. Upon taking the expectation and trace of (19)
4E[tr(ΩtN))] = -tr(AE[ΩtN)]) -tr(E[Ω(N)]A>) - tr(E[Ω(N)C>CΩlf)]) + tr(BR-1B>)
≤ -tr(AE[ΩtN)]) - tr(E[Ω(N)]A>) - tr(E[Ω(N)]C>CE[ΩtN)]) + tr(BR-1 B>)
18
Under review as a conference paper at ICLR 2022
where we used E[Tr(X >X)] ≥ Tr(E[X ]E[X ]>) for X = Ω,N )C › to conclude the inequality. Then,
by inspecting the equation for Tr(Ωt), it follows that
E[Tr(Ω(N))] ≤ Tr(Ωt) ≤ Eo	(21)
where Eo is a uniformly in time bounded constant, because Ωt converges to Ω∞.
Next, We obtain the bound for the error Ω(N) - Ωh Subtracting (18) from (19) yields
d(Ω(N) - Ωt) = (-A(ω(n) - Ωt) - (ω(n) - Ωt)A> - (ω(n) - Ωt)C>C(ω(n) - Ωt)
-(ω(n) - Ωt)C>CΩt - ΩtC>C(ω(n) - Ωt))dt + dMt
Then, by application of the Ito rule to (Ω(N) - Ωt)>(Ω(N) - Ωt) = (Ω(N) - Ωt)2 and taking the
trace,
dTr((ΩtN) - Ωt)2) = 2tr((Ω(N) - Ωt)2)(-A - A> - C>CΩt - ΩtC>C))dt
-2tr(C>C(ω(n) - Ωt)3))dt + tr((dMt + dM>)(Ω(N) - Ωt))
+ ɪ (tr(Ω(N)BR-1B>) + tr(Ω(N))tr(BRTB>)) dt
Upon taking the expectation and defining Θt = E[tr((Ω(N) - Ωt)2)],
-dΘt ≤ 2E [tr(-A - A> - C>CΩt)(Ω(N) - Ωt)2)] - 2E [tr(C>Cω(n)(Ω(N) - Ωt)2)]
+ N^(θt + 2tr((BR-1B >)2) + tr(Ω2) + 2tr(BR-1B>)tr(Ωt))
where We used the bound (21), and Tr(XY) ≤ 2Tr(X>X) + 1 Tr(Y>Y) for X = Ω(N) - Ωt and
Y= BR-1B>.
In order to continue with proving the bound, we use the assumption C>C = I to conclude
dtθt ≤ 2Ehtr(-A - A> - Qt)(C(N) - Q)2)]
+ NF (θt + 2tr((BR-1B>)2) + 2tr(Ωt)tr(BR-1B>) + tr(Ω2))
because tr(ΩtN)(Ω(N) - Ωt)2) ≥ 0. Morevoer, due to exponential convergence of Ωt to Ω∞,
tr(Ω2) ≤ Ei uniformly. Thus,
^tΘt ≤ 4μtΘt + NF(Θt + 2tr((BR-1B>)2) + 2Eotr(BR-1B>) + Ei)
where μt is the minimum eigenvalue for A + ɪΩ> Therefore,
Θt ≤ Θoe-4 R0 μto dt0+Nt + Cvar
≤ ΘoCμe-4μ∞t+ NNt + cNar
where cvar = 2tr((BR-iB>)2) + 2Eotr(BR-iB>) + Ei and we used (20). The proof of the
proposition follows by substitution STN-t - St = Ω(N) - Ωt and the bound Θo ≤ 3kPNkF .
E Duality roots of the proposed algorithm
For t ∈ (0, T), the LQG value function is defined as follows
vt(x) :
min
{Us<≤s≤T }
(1 |CXs|2 + 2U>RUs) ds + XTPTXT Xt
(22)
From the DP optimality principle, it is easily shown Liberzon (2012) that
•	The value function vt(x) = 1 χ>PtX + (constant) is quadratic where
•	The matrix-valued process {Pt ∈ Rd×d : 0 ≤ t ≤ T} solves the DRE with terminal
condition PT at time t = T .
19
Under review as a conference paper at ICLR 2022
Log transformation: The log transformation is a manifestation of the duality between optimal
control and optimal estimation Mitter & Newton (2003); Todorov (2008). Define the probability
density Pt(X) 8 e-vt(x) Fleming & Mitter (1982); Fleming (1977). For the LQG problem, Vt is
quadratic and therefore, pt is a Gaussian density with variance Pt-1 = St . Indeed, the Gaussian
density of the random variable 匕 equals pt. The following proposition gives the precise connection:
Proposition 4. Suppose Pt H e-vt where Vt is the value function defined in (22). Let pt be the
density of the random variable Yt defined in (9). Then, provided PT = PT,
Pt(X) = pt(χ),	∀χ ∈ Rd, 0 ≤ t ≤ T
Proof. By Proposition 2, Yt is a Gaussian random variable N(0, St) where St = P-1 by definition.
Therefore,
一 log(夕 t(x)) = 2 x> Ptx + (constant) = Vt(X) + (constant) = 一 log(Pt(x)) + (constant)
Therefore, sincePt(X) andPt(X) are both normalized to one, the constant is zero andPt(X) = Pt(x).
In the past, duality has been used to:
(i)	Obtain linearly solvable sampling algorithms to solve optimal control problems Kappen & Ruiz
(20i6); Rawlik et al. (2013); Theodorou et al. (2010); Schutte et al. (2012); Todorov (2009); and
(ii)	Set up optimal control problems for the purposes of estimation and simulation Sutter et al.
(2016); Hartmann & Schutte (2012); Van Handel (2006); Kim & Mehta (2020).
(iii)	A parallel approach to duality involves posing optimal control problems as problems of prob-
abilistic inference, and using methods like expectation maximisation (Toussaint & Storkey, 2006),
message passing (Hoffmann & Rostalski, 2017; Watson & Peters, 2021; Watson et al., 2021) or
minimisation ofKL divergence (Kappen et al., 2012; Rawlik et al., 2013; Watson et al., 2020).
Although novel and distinct from the algorithms described in these earlier works - which instead rely
on the use of a Feynman-Kac representation - the proposed algorithm is an example of a sampling
algorithm to solve the optimal control problem. In a related work, we have extended the results
presented here to a class of nonlinear optimal control problems. These nonlinear generalization more
clearly reveals the duality roots of the proposed algorithm. The EnKF algorithm arises as a special
case in the LQG settings. The nonlinear generalization will be a subject of future publication.
F Details of the numerical examples in Sec. 3
Notation: In ∈ Rn×n denotes the identity matrix and 1d ∈ Rd denotes a vector with all entries
equal to 1.
F.1 Exponential convergence
The EnKF algorithm is implemented for the linear system (1) to approximate the optimal gain ma-
trix. The model matrices A and B are in the control canonical form
■ 0	10	0
0010
A =.
.
.
a1 a2 a3 a4
where the entries (a1, . . . , ad) ∈ Rd is selected randomly from N(1.651d, Id). Additional model
and simulation parameters are summarized in Table 2 and Table 3.
The numerical result for a single realization of the algorithm, for d = 2, is depicted in Figure 1.
Part-(a) depicts the spectral properties of the open-loop (X = AX) and the closed-loop (X = (A +
BK0(N))X) dynamics (recall that due to the backward in time nature of the EnKF, K0 is the terminal
20
Under review as a conference paper at ICLR 2022
gain which the algorithm yields). It is observed that the algorithm learns the gain matrix that make
the closed-loop dynamics stable, while the open loop dynamics A is unstable.
The entry-wise convergence of the matrix Pt(N) as t goes from T to 0 is depicted in part (b)-(c)-(d).
The result is compared with the exact solution to the DRE Pt and the asymptomatic limit, as the
time horizon T → ∞, which is the solution to the ARE.
Figure 4: Open-loop (o) and closed loop eigenvalues (x) for the 10 dimensional system.
Additional numerical results, for the case where d = 10, are depicted in Fig. 4 for the open and
closed-loop eigenvalues and in Fig. 5 for the convergence of the 100 entries of the 10 × 10 matrix
Pt(N).
Table 2: Model parameters for the LQR With random dynamics
Model parameter Numerical value
m0	0d×1
Σ0	0.1Id
C for d = 2	√5id
C for d > 2	Id
R	1.0
PT	Id
Table 3: Simulation parameters for LQR with random dynamics
Simulation parameter name	Symbol	Numerical	value
Simulation time	T	10
Step size	∆t	0.02
21
Under review as a conference paper at ICLR 2022
Figure 5: Convergence of the 100 entries of the matrix Pt(N) for d = 10 dimensional system.
Solution of the ARE is depicted as the red dashed line. As in Fig. 1 for the 2-dimensional system,
these plots are depicted with respect to T - t. The initialization is PT = I , the identity matrix.
F.2 Coupled mass spring damper system
This system is taken from Mohammadi et al. (2019). The matrices A and B are as follows:
A
0ds×ds
-T
Ids
-T
0ds×ds
Ids
B
where ⅛ = d is the number of masses and T ∈ Rds × ds is a ToePlitz matrix with 2 on the main
diagonal and -1 on the first sub-diagonal and first super-diagonal. Additional model and simulation
Parameters are listed in Table 4 and 5, resPectively.
Table 4: Model parameters for the coupled mass spring damper system
Model parameter	Numerical value
m0	0d×1
Σ0	0.1Id
C for d = 2	√5id
C for d > 2	Id
R	Ids
PT	Id
Table 5: Simulation parameters for the coupled mass spring damper system
Simulation parameter name	Symbol	Numerical value
Simulation time	T	10
Step size	∆t	0.02
22
Under review as a conference paper at ICLR 2022
F.3 Comparison between EnKF and policy-gradient methods
The performance of the EnKF algorithm is compared with policy gradient algorithms presented
in Mohammadi et al. (2021c) (denoted as [M21]) and Fazel et al. (2018) (denoted as [F18]). These
two algorithms are based on gradient free optimization for the gain matrix, with the difference that
[M21] is for continuous-time systems while [F18] is for discrete-time systems. We used the dis-
cretized linear system xt+∆t = (I + A∆t)xt + But∆t to implement [F18]. The two algorithms
involve similar set of hyper-parameters that are selected optimally for each experiment for fair com-
parison.
The three algorithms, EnKF, [M21], and [F18] are implemented for the mass-spring system of Ap-
pendix F.2 for d = 2, 4, 10. The comparison is made by studying the relationship between the
computational time and the error in approximating the optimal gain matrix. The error is measured
with the formula
error
kK0N)-K∞kF .
kK∞kF	;
kκTN)-K∞kF .
kK∞kF	;
for EnKF
for [M21] and [F18]
where K0(N) is the terminal gain output by EnKF, KT(N) is the terminal gain output by the [M21]
and [F18] algorithms, and K∞ is the analytical gain obtained through the ARE.
In order to study the error vs computational time relationship for EnKF algorithm, the number of
particles N is varied, while for [M21] and [F18] the number of iterations is varied. The simulation
time horizon T = 10, and the step-size ∆t = 0.01 is the same for all algorithms. The hyper-
parameters of the [M21], and [F18] algorithms are presented in Table 6. The initial guess K0 = 0,
initial distribution D0 = N(0,Id), and gradient descent step α = 0.0001 for both [M21] and [F18].
The values of the other hyper parameters are in Table 7. The numerical results for d = 10 are
depicted in Figure 2 and for d = 2, 4 in Figure 6. Additionally, Figure 7 shows comparison when
the error is measured as
error
c(0N)
(N)
T
c(0N)
-c∞
-c∞
-c∞
-c∞
for EnKF
for [M21] and [F18]
c
c
;
;
where c(0N) is the optimal control cost corresponding to the terminal gainas per EnKF, and by the
initial gain as per [M21] and [F18] algorithms; and c(TN) is the optimal control cost corresponding
to the initial gain as per EnKF, and terminal gain as per the [M21] and [F18] algorithms; and c∞ is
the optimal control cost achieved by analytical gain obtained through the ARE.
The simulations are implemented in Python 3 on a Intel Xeon E3-1240 V2 3.40 Ghz CPU, and the
Process_time() function from the time module is used to evaluate the execution time.
Table 6: Hyper-parameters for policy gradient
Hyper-parameter	Symbol
Initial guess	K0
Distribution of initial state	D0
Smoothing parameter	r
Gradient descent step	α
Simulation horizon	T
Averaging for gradient	N
23
Under review as a conference paper at ICLR 2022
—∙- EnKF	→- [M21]	→- [F18]
0.05	0.10	0.02	0.04	0.06	0.08
Relative error in gain
Figure 6:	ComParisonbetWeenEnKF, [M21], [F18]
—EnKF - [M21]	[F18]
(S)①£H∙dluoo
0.000	0.002	0.004	0.006	0.008	0.010	0.000	0.002	0.004	0.006	0.008	0.000	0.002	0.004	0.006
Error in cost
Figure 7:	Comparison between EnKF, [M21], [F18]
F.4 Cart-pole system
The nonlinear model is taken from (Tedrake, Chapter 3.2.1):
θ = ω
ω = (^j-----. 2⑹)(-Fcos(θ) — mlω2 cos(θ) sin(θ) — (m +
x=v
V = ττ--1.2g∖ (F + msin(θ)(lω2 + gcos(θ)))
M + m sin2(θ)
Table 7: Hyper-parameter values for policy gradient
Hyper-param.	[M21]			[F18]		
d	2	4	10	2	4	10
r	10-1	10-1	10-3	10-1	10-1	10-1
N	2	4	10	2	4	10
24
Under review as a conference paper at ICLR 2022
For the LQG control design, we linearize the equations about the desired inverted equilibrium
(π, 0, 0, 0). The associated A and B matrices are as follows:
	0	0	1	0		0
	0	0	0	1		1
A=	(M+m)g -MI-	0	0	0	,B =	Ml 0
	mg L M	0	0	0		1 , M .
The model parameters are listed in Table 8 and the simulation parameters are in Table 9.
Table 8: Model parameters for the cart-pole system
Model parameter name	Symbol	Numerical value
Mass of ball	m	0.08
Mass of cart	M	1
Length of rod	l	0.7
Gravity	g	9.81
Unstable equilibrium	W,x,ω,v)	(π,0,0,0)
Initial condition	(θ(0),x(0),ω(0),v(0))	(1.25π, -0.1, 0, 0)
	C	diag([10, 10, 1, 1])
LQG parameters	R	10
	PT	I4
Table 9: Simulation parameters for the cart-pole system
Simulation parameter name	Symbol	Numerical value
Simulation time	T	10
Step size	∆t	0.0002
25