Under review as a conference paper at ICLR 2022
Marginal Tail-Adaptive Normalizing Flows
Anonymous authors
Paper under double-blind review
Ab stract
Learning the tail behavior of a distribution is a notoriously difficult problem. The
number of samples from the tail is small, and deep generative models, such as
normalizing flows, tend to concentrate on learning the body of the distribution.
In this paper, we focus on improving the ability of normalizing flows to correctly
capture the tail behavior and, thus, form more accurate models. We prove that the
marginal tailedness of a triangular flow can be controlled via the tailedness of the
marginals of the base distribution of the normalizing flow. This theoretical insight
leads us to a novel type of triangular flows based on learnable base distributions
and data-driven permutations. Since the proposed flows preserve marginal tailed-
ness, we call them marginal tail-adaptive flows (mTAFs). An empirical analysis
on synthetic data shows that mTAF improves on the robustness and efficiency
of vanilla flows and—motivated by our theory—allows to successfully generate
tail samples from the distributions. More generally, our experiments affirm that a
careful choice of the base distribution is an effective way to introducing inductive
biases to normalizing flows.
1 Introduction
Heavy-tailed distributions are known to occur in various applications in biology, finance, social
sciences, and more. Examples for such observations include the length of protein sequences in
genomes (Koonin et al., 2006), returns of stocks (Gabaix et al., 2003), or the size of cities (Gabaix,
1999). Applications that are tightly connected to typical deep learning applications include the
frequency of class examples in image classification (Horn & Perona, 2017) and the frequency of
words (Zipf, 1949) in natural language processing. From a theoretical point of view, this is not
surprising since heavy-tailed distributions emerge from several circumstances, including the limiting
distribution in the generalized central limit theorem, of a multiplicative process, or as the limit of
an extremal process (Nair et al., 2013). Given the frequency of occurrence, developing generative
models that allow to learn heavy-tailed distributions is essential.
Normalizing Flows (NFs (Rippel & Adams, 2013; Tabak & Turner, 2013; Dinh et al., 2015; Rezende
& Mohamed, 2015)) are a popular class of deep generative models. Despite their success in learning
tractable distributions where both sampling and density evaluation can be efficient and exact, their
ability to model heavy tailed distributions is known to be limited. Jaini et al. (2020) identified the
problem that a range of NFs (e.g. vanilla triangular flows with a Gaussian base distribution) are
unable to map a light-tailed distribution to a heavy-tailed distribution. They propose to solve this
issue by replacing the Gaussian base distribution by a multivariate t-distribution with one learnable
degree of freedom. While this allows to model distributions with a heavy-tailed euclidean norm, we
show that modeling multivariate distributions, where some of the marginals are heavy- and some are
light-tailed, still poses a problem.
Contributions Our contributions in this work, that extend the results of Jaini et al. (2020), are the
following. First, we prove that a triangular affine NF using a base distribution with solely heavy-
tailed marginals is only able to provide a target distribution with just heavy-tailed marginals as well.
Consequently, such a NF is not capable of learning distributions with mixed marginal tail behavior.
Second, we derive a result that states conditions under which the marginal tailedness of the base
distribution can be preserved. Third, based on these theoretical findings, we propose a novel kind
of triangular NF that allows to learn distributions with heavy- and light-tailed marginals. The new
model is called marginally Tail-Adaptive Flows (mTAFs), and as illustrated in Figure 1, combines
1
Under review as a conference paper at ICLR 2022
Step 1: Estimate the
Marginal Tail Indices
Step 2: Defining the
Step 3: Flow-Layer with a Data-
Driven Permutation Scheme
Figure 1: An Overview of mTAF. In a first step, we apply estimators from extreme value theory to
classify the marginals as heavy- or light-tailed. This classification defines a flexible base distribution
consisting of marginal Gaussians and marginal t-distributions with trainable degree of freedom, as
illustrated by Step 2 of this figure. Further, we rearrange the marginals such that the first d marginals
are light-tailed, whereas the remaining marginals are heavy-tailed. mTAF is then constructed using
several flow-layers as visualized in Step 3: we employ a triangular mapping, followed by a 2-group
permutation scheme, i.e. we just permute within the set of light-tailed marginals and within the
set of heavy-tailed marginals. At the end, we restore the original ordering using the inverse of
the permutation employed in Step 2. Using Theorem 3, we prove that mTAFs are marginally tail-
adaptive (Corollary 1).
•	estimators from extreme value theory (Hill, 1975; Dekkers et al., 1989; Csorgo et al., 1985)
to initially assess heavy-tailedness of the target’s marginals;
•	a flexible and trainable base distribution based on the estimated tail behavior of the target
distribution;
•	a new permutation-scheme between flow-layers that ensures the correct tail behavior of the
estimated target distribution.
Finally, we conduct an experimental analysis demonstrating the superior performance of the pro-
posed mTAFs in comparison to other flow models. Furthermore, we present a new sample genera-
tion scheme, motivated by our theory, which successfully generates joint samples that are from the
tails of a specified marginal.
Notational Conventions In the following, we will denote random variables by bold letters, such as
x, and its realisations by non-bold letters, x. We use this notation for multivariate and for univariate
random variables. Further, we denote the jth component of x by xj , and x≤j or x<j are the first
j or j - 1 components of x, respectively. We denote the random variable representing the base
distribution by z and the random variable representing the target distribution by x. Further, for
notational convenience, we denote the probability density functions (PDFs) of x and z by p and q,
whereas marginal PDFs are denoted by pj and qj , respectively. Finally, we assume that both random
variables x and z have continuous and positive density on RD, i.e p(x), q(z) > 0 for all x, z ∈ RD,
where D is the dimensionality of x and z.
2
Under review as a conference paper at ICLR 2022
2 Background
In this section, we give a brief introduction to heavy-tailed distributions and present needed back-
ground knowledge about normalizing flows.
2.1 Heavy-Tailed Distributions
Heavy-tailed distributions are distributions that have heavier tails (i.e. decay slower) than the ex-
ponential distribution. Loosely speaking, slowly decaying tails allow to model distributions that
generate samples, which differ by a large magnitude from the rest of the samples. For a univariate
random variable x we define heavy-tailedness via its moment-generating function1:
Definition 1 (Heavy-Tailed Random Variables). Consider a random variable x ∈ R with PDF p.
Then, we say that x is heavy-tailed if and only if
∀λ > 0 : Ex eλx = ∞ .
The function mp (λ) := Ex [exp(λx)] is known as the moment-generating function of x. Random
variables that are not heavy-tailed are said to be light-tailed.
Note that this definition is, strictly speaking, merely a definition for heavy right tails. We say a
random variable x ∈ R has heavy left tails if -x has heavy tails according to Definition 1. For
simplicity of derivations and w.l.o.g., we proceed with this definition but the derived results can
analogously be applied to left tails.
We can assess the degree of tailedness of a distribution. While there are many equivalent notions of
the so called tail index, the most straight-forward definition is via the existence of moments:
Definition 2 (Tail Index). A random variable x ∈ R with PDF p is said to have tail index2 α if it
holds that
Ex [∣χ∣β ] [< ∞ , if. β<α,
= ∞ , if β > α .
Since the tail index is tightly related to the decay rate of the PDF, it enables us to assess the degree of
heavy-tailedness of a random variable. Therefore, estimation of the tail index became an important
objective in extreme value theory and statistical risk assessment (see e.g. Embrechts et al., 2013).
Since the existence of the moment does not depend on the “body” of χ but only on the tails of
χ (see Proposition 3 in Section A.1 in the Appendix), estimating the tail index by fitting a full
parametric model to all data e.g. via likelihood maximization leads to a biased estimator. Instead,
semi-parametric estimators have been developed, which aim to fit a distribution only on the tails.
Popular methods for tail estimation include the Hill estimator (Hill, 1975), the moment estimator
(Dekkers et al., 1989), and kernel-based estimators (Csorgo et al., 1985). In Section B.1 of the
Appendix, we discuss these tail estimators and review some practical issues with these.
An example of a heavy tailed distribution is the standardized t-distribution, which has parameter
ν > 0 referred to as the degree of freedom and a density function given by
Γ( ν+1)	χ2	v + 1
p(x) := √VπΓ⅛(1 + W)2 ,x ∈ R,
where Γ is the Gamma function. It is known that the t-distribution has tail index ν (see e.g. Kirkby
et al. (2019) for a detailed reference).
In the multivariate setting, there exist various definitions of heavy-tailedness. For instance Resnick
(2004) make use of a definition based on multivariate regular variation. Jaini et al. (2020) define a
multivariate random variable X to be heavy-tailed if the '2-norm is heavy-tailed, a property which
we refer to as `2 -heavy tailed, and which is formally defined as follows:
1One can readily show that this definition is equivalent to the definition, which compares the tails of x to
the tails of an exponential distribution. See Section 1 in Nair et al. (2013).
2Notice that the notion of a tail index is only valid for regularly-varying random variables, which are a
subclass of heavy-tailed random variables. For the purpose of this work, it is sufficient to consider regularly
varying random variables. More details can be found in Nair et al. (2013).
3
Under review as a conference paper at ICLR 2022
Definition 3 ('2-Heavy-Tailed). Let x ∈ RD be a multivariate random variable. Then, we call x
'2-heavy-tailed ifit holds that kXkis UniVariately heavy-tailed according to Definition 1, where ∣∣ ∙ ∣∣
denotes the '2-norm. Otherwise, we call X '2 -light-tailed.
2.2 Normalizing Flows
The fundamental idea behind NFs is based on the change-of-variables formula for probability den-
sity functions (PDFs) given in the following theorem.
Theorem 1 (Change-of-Variables). Consider random variables X, z ∈ RD and a diffeomorphic
map T : RD → RD such that X = T (z). Then, it holds that the PDF ofX satisfies
p(x) = q(T-1(x)) Idet JT-1 (x) ∣ ∀x ∈ RD ,	(1)
where JT-1 (x) is the Jacobian ofT-1 evaluated at x ∈ RD.
This formula allows us to evaluate the possibly intractable PDF of X if we can evaluate both, the
PDF of z and T-1 (x), and efficiently calculate the Jacobian-determinant det JT-1 (x). As T maps
z to X, we denote the distribution of z and X as the base and the target distribution, respectively.
To model the PDF of X using NFs, it is common to set the base distribution to a standard normal
distribution (i.e., Z 〜N(0,I)) and to employ likelihood maximization to learn a parameterized
transformation
Tθ := T(L)。…。T((I),
which, yet, remains tractable and diffeomorphic. Masked autoregressive flows (MAFs (Papamakar-
ios et al., 2017)) are one popular architecture, which employ transformations T = (T1, . . . , TD)>
of the form
Tj (z) := μj (z<j )+exp(σj (z<j ))zj for j ∈{1,...,D},	(2)
where μj and σj- are neural networks, which obtain the first j -1 components of Z as input and output
a scalar. Composing several transformations of the form (2), we obtain the MAF. The autoregressive
form in (2) allows us to efficiently evaluate the Jacobian-Determinant due to the diagonal form
of JT (x). A crucial issue of such autoregressive models is that a component Xj only depends
on the previous outputs X<j and, therefore, they cannot model a causal relationship in which Xj
causes Xi if i < j . This issue can be solved by applying a permutation before each transformation
Tθ(1), . . . , Tθ(L). This is usually a random permutation or the one that reverses the ordering of the
components. Therefore, in summary a MAF consists of multiple consecutive layers Tθ(l) 。 P (l),
where P(l) ∈ RD×D is a permutation. MAFs belong to the class of triangular flows, which are
defined as flows that consist of diffeomorphisms whose jth output only depends on z≤j . Other
examples for triangular flows include RealNVP (Dinh et al., 2017), NAF (Huang et al., 2018), and
SOS (Jaini et al., 2019). If the triangular maps are affine linear (such as in (2)), we call the resulting
flow a triangular affine flow. Further types of NFs include invertible ResNets (Jacobsen et al., 2018;
Behrmann et al., 2019; Chen et al., 2019), continuous flows (Chen et al., 2018; Grathwohl et al.,
2019), and many more (Kobyzev et al., 2020).
Tail-Adaptive Flows. Jaini et al. (2020) investigated the ability of triangular flows to learn heavy-
tailed distributions. The authors have shown that if a triangular affine flow transforms a '2-light-
tailed distribution, such as the multivariate Gaussian distribution, to a '2-heavy-tailed target distri-
bution, then Tθ cannot be Lipschitz continuous. And more explicitly, it holds the following.
Theorem 2. (Jaini et al., 2020) Let z be a '2-light-tailed random variable and T be an affine
triangular flow such that Tj (z≤j) = μj (z<j) + σj (z<j )zj for all j. If σj is bounded above and μj
is Lipschitz for all j, then the transformed variable X is also '2 -light-tailed.
Furthermore, the authors prove that any triangular mapping from an elliptical distribution to a
heavier-tailed elliptical distribution must have an unbounded Jacobian-determinant. Clearly, these
results illuminate that learning a heavy-tailed distribution using NFs leads to non-Lipschitz trans-
formations and unbounded Jacobians, which inevitably affects training robustness (Behrmann et al.,
2021). Motivated by these result, Jaini et al. (2020) propose Tail-Adaptive Flows (TAF), which re-
place the Gaussian base distribution by a multivariate t-distribution with one learnable degree of
freedom.
4
Under review as a conference paper at ICLR 2022
3	Learning the correct marginal Tail B ehavior with mTAF
In this section, we present a simple extension to triangular affine flows that allows to model distri-
butions with a flexible tail behavior. We start by presenting our theoretical results in Section 3.1.
Motivated by these results, we propose marginally Tail-Adaptive Flow (mTAF) in Section 3.2.
3.1	The Necessity of a flexible Base Distribution
In this work, we investigate the tailedness of NFs more thoroughly through the lense of marginal
tailedness, i.e. we consider the univariate tailedness of the marginal distributions of xj .Therefore,
we introduce the following definitions:
Definition 4 (j -Heavy-Tailed, Mixed-Tailed, Fully Heavy-Tailed, Equal Tail Behavior). We call a
random variable x ∈ RD j-heavy-tailed if its jth marginal xj is heavy-tailed according to Defini-
tion 1. Otherwise, we call x j-light-tailed. x is said to be mixed-tailed if there exists j1,j2 such that
x is j1-heavy-tailed and j2-light-tailed. Further, we say that x is fully heavy-tailed if x is j -heavy-
tailed for all j ∈ {1, . . . , D}. We define two random variables x and z to have equal tail behavior
if it holds for all j that
x is j-heavy tailed ⇔ z is j-heavy tailed .
We found the following relation to Definition 3.
Proposition 1 (j-Heavy-Tailedness induces '2-Heavy-Tailedness). Assume that X is j-heavy-tailed
for any j. Then, X is also '2 -heavy-tailed.
The proof can be found in Section A.1 in the Appendix. The proposition shows that j-heavy-
tailedness is a more general notion of multivariate heavy-tailedness than '2-heavy-tailedness, which
allows a narrow inspection of the tail behavior. More precisely, the new notion allows us to differ-
entiate between fully heavy-tailed random variables and mixed-tailed random variables, which are
both '2-heavy-tailed. One can now wonder how the tails, described by the novel notation, behave for
NFs, which we answer by the following two results. The first result states that, under mild technical
conditions, fully heavy-tailedness of the base distribution is preserved by triangular affine maps.
Proposition 2 (Triangular Affine Maps preserve Fully Heavy-Tailedness). Let z be a fully heavy-
tailed random variable that satisfies Assumption 13 and let T be a a triangular affine map, that is,
Tj (zj,z<j) = μj (z<j)+ σj(z<j)zj with σ7- > 0. Then, it holds that T(Z) is alsofully heavy-tailed.
A formal proof can be found in Section A.2 of the Appendix. Assumption 1 is a mild condition on
the decay rate of the copula density of z. We explain this condition in more detail and give various
examples in Section A.3 of the Appendix.
It is clear that permuting the marginals does not change the heavy-tailedness. Hence, by iterative
application of Proposition 2, we deduce that affine triangular flows with a fully heavy-tailed base
distribution are unable to model mixed tailed distributions. Implicitly, Proposition 2 states that a
Lipschitz normalizing flow as proposed by Jaini et al. (2020) is not able to model mixed-tailed
distributions. However, the following Theorem that we consider as our main result guides us towards
a flow architecture that is able to model target distributions that are mixed-tailed in a marginally tail-
adaptive way.
Theorem 3 (Learning the correct Tail Behavior). Consider a random-variable z that is j -light-
tailed for j ∈ {1, . . . , d} for some d < D and j-heavy-tailed for j ∈ {d + 1, . . . , D}. Then, under
the same conditions as in Theorem 2 and Proposition 2, it holds that z and T(z) have the same tail
behavior.
Proof. Since the result combines Theorem 2 and Proposition 2 in an evident fashion, we just quickly
present a sketch of the proof. First, let us consider j ≤ d. Then it holds for the moment-generating
function of Xj that
mxj (λ)
f
RD
eλTj (z≤j)q(z)dz =
Rj
eλTj(z≤j)p≤j (z≤j)dz≤j
3This Assumption can be found in Section A.2 in the Appendix.
5
Under review as a conference paper at ICLR 2022
which has been shown to be bounded for some λ > 0 (see the proof of Theorem 2 in Jaini et al.
(2020)). Therefore, x is j -light-tailed for all j ≤ d. In the case j > d, we notice4 that the proof for
heavy-tailedness of Tj (z≤j) involves just the heavy-tailedness of zj and not of any other component
of z<j. Hence, if zj is heavy-tailed, then xj = Tj (z≤j) is also heavy-tailed, regardless of z<j.
Therefore, x is j -heavy-tailed for all j > d, which completes the proof. Note that in general we
cannot deduce the latter conclusion for light-tailed marginals, i.e. if zj is light-tailed, this does not
mean that Xj is also light-tailed. This is only the case, if all z<j are light-tailed as well.	口
3.2	Marginally Tail-Adaptive Flow (mTAF)
Our main result, Theorem 3, prompts that if we maintain an ordering of the marginals such that
the first marginals are light-tailed and the following are heavy-tailed in each flow step, we retain
the marginal tail behavior of the base distribution in the estimated target distribution. This finding
motivates the novel NF proposed in this paper. The proposed approach combines research findings
from extreme value theory (Embrechts et al., 2013; Nair et al., 2013), recent findings about nor-
malizing flows (Jaini et al., 2020; Alexanderson & Henter, 2020; Laszkiewicz et al., 2021), and the
results presented herein. The proposed mTAFs consists out of three steps depicted in Figure 1 and
described in the following:
Step 1: Estimating the marginal tail indices and defining the marginal distributions. For each
marginal, i.e. for the marginal distribution qj of each Xj, j = 1, . . . , D, we use the moments double-
bootstrap estimator (Draisma et al., 1999) and the kernel-type double-bootstrap estimator (Groene-
boom et al., 2003) to assess heavy-tailedness of the data distribution. If both estimators predict a
light-tailed distribution, we set the corresponding marginal base distribution qj to be standard nor-
mal distributed, i.e. Zj 〜N(0,1). Otherwise we set the marginal to the standardized t-distribution
with the estimated degree of freedom, i.e. Zj 〜t^j., where Vj is the Hill double-bootstrap estimator
(Danielsson et al., 2001; Qi, 2008). In Section B.1 of the Appendix, we present more details about
the tail-assessment scheme.
Step 2: Defining the base distribution. We construct the base distribution as the mean-field ap-
proximation of the marginals, i.e. Z has the density q(z) := QjD=1 qj(zj) with marginal densities
qj defined in step 1. Further, to satisfy the assumptions of Theorem 3, we need to permute the
marginals such that it holds Zj 〜N(0,1) for j ≤ d and Zj 〜t^j for j > d. We apply the same
permutation to restructure our data according to the base components. To account for tail index esti-
mation errors and for more flexible learning, we make the tail indices (i.e. the degrees of freedom of
each t-distribution) learnable. That is, we initialize the degree of freedom of the j th marginal with
Vj but adapt the parameter together with the network parameters throughout training.
Step 3: A data-driven permutation scheme. Recall, that vanilla triangular flows employ a permu-
tation step after each transformation to enhance the mixing of variables. However, purely random
permutations might lead to a violation on the ordering of marginals, which is necessary to ensure
Theorem 3. Therefore, we permute only within the set of heavy-tailed marginals and within the set
of light-tailed marginals, to ensure the validity of Theorem 3. Within these groups one can choose
any permutation scheme.
Without loss of generality, we assume that the first d components of Z are light-tailed and the re-
maining D - d components are heavy-tailed5. Then, the training objective is to optimize for flow
parameters θ and degrees of freedom V = [^d+ι,..., VD] to maximize the log-likelihood
D
π
1
L(θ,V; X)
i=d+1
j
d
D
1
+
-log det Jτθ(χ(j)) j>
)-log det Jτ^(xj))
j
i=d+1
where X := (X⑴，...X(N)) is the data, and ∏ and t^ are the PDF of the standard normal distribution
and the standard t-distribution with V degrees of freedom, respectively.
4For details, we refer to the proof of Proposition 2 in the Appendix.
5Otherwise we permute the marginals as described in Step 2.
6
Under review as a conference paper at ICLR 2022
When applying our theoretical results presented in the previous section to the proposed mTAF, we
can show that it fulfills the desired tail-preserving property, as formalized by the following corollary:
Corollary 1 (Marginal Tail-Adaptive). Under the same assumptions as in Theorem 2 and in Propo-
sition 2, mTAFs are marginally tail-adaptive, that is, z and x = T (z) have the same tail behavior.
4	Experimental Analysis on Synthetic Data
To investigate the benefits of mTAF we perform an empirical analysis in which we conduct ex-
periments on synthetic heavy-tailed data. We construct synthetic 16-dimensional data sets and in-
vestigate eight different settings: The marginals are chosen to contain h ∈ {1, 2, 4, 8} mixtures of
two t-distributions, four Gaussian distributions, two mixtures of three Gaussian distributions, and
10 - h mixtures of two Gaussian distributions. The degree of freedom of all t-distributions is either
ν = 2 or ν = 3. In all mixtures all mixture components are weighted equally, means are uniformly
sampled from [4, 4], and standard-deviations are sampled from [1, 2]. Using a Gaussian Copula, we
construct a complex joint distribution with mixed-tailed marginals. A detailed description of the
data set generation can be found in Section B.2 in the Appendix. Details about hyperparameters can
be found in Section B.4 in the Appendix. We provide a PyTorch implementation and the code for
all experiments along the submission.
4.1	Model Accuracy
In our first experiment, we compare the performance of 5 different MAFs: a vanilla flow with
Z 〜N(0, I), the TAF, a variation of TAF (TAF(^ι,..., VD) in which all marginals have their own
independent degree of freedom (i.e. Zj 〜t^j (0,1) with trainable degrees of freedom Vj), and mTAF
as described in Section 3.2 with learnable and fixed tail indices, where the letter is denoted as mTAF
(fixed ^).
Table 1 summarizes the test losses archived by the five models in the eight different settings. The
results demonstrate that mTAF can efficiently learn a multivariate heavy-tailed distribution, whereas
the vanilla approach, i.e. fixing Z 〜N(0, I), is prone to optimization instabilities (as can be seen
by high standard deviations) and leads to higher values of the negative log-likelihood. Moreover,
TAF performs worse than mTAF (and is also less stable) which could be contributed to having only
one joint degree of freedom to model the tailedness of all marginals. Using independent degrees
of freedom however only leads to slight improvements, illustrating the necessity of the proposed
permutation scheme of mTAF. The performance increase is especially severe for marginals with
small tails index, i.e., when the degree of freedom of the marginals of the data-generating distribution
is ν = 2. Fixing the tail indices to the estimated values already leads to great results, which can
further be increased by adapting the parameters during training.
4.2	Generating Tail Events
An often-used property of NFs is the diffeomorphic relation between base and target distribution,
which can be used, for instance, for image interpolation6 or for constructing counterfactuals7. In
this work, we present another practical advantage, which we motivate by the derived theory, namely
the generation of tail samples. One generic way to generate tail samples in the target space is to
generate tail samples in the base space, which are then pushed towards the target space. However, in
this work we propose to solve a more specific problem, which is the generation of samples that are
from the j-tails, i.e. samples from the joint distribution of x with the jth component xj being from
the tail of the jth marginal. Recall that in the proof of Proposition 2 we show that if Zj is heavy-
tailed, then Tj (Z≤j) resulting from applying one affine triangular flow layer is also heavy-tailed,
independently of Z<j . Inspired by this result, we hypothesize that we can generate multivariate
samples x with heavy j-tails by sampling tail events of Zj , complementing them with remaining
components z6=j sampled from Z6=j , and pushing the resulting z through the flow, which retains the
j -heavy-tailedness. However, since we apply a permutation after each layer, to sample from the
6by interpolating in the base distribution and pushing them forward using the NF (Kingma & Dhariwal,
2018).
7by applying gradient ascent in the base space (Dombrowski et al., 2021).
7
Under review as a conference paper at ICLR 2022
Table 1: Average test loss (lower is better) on the synthetic data in various settings over 10 trials.
The error bars represent one standard deviation and the numbers in brackets denote the number of
crashed runs. Models that are significantly better than the other models (according to a one-sided
t-test) are highlighted.
number of heavy-tailed components h
	1	2	4	8
ν=2				
Vanilla	35.68 ± 1.92	36.68 ± 1.77	38.19 ± 1.71	45.78 ± 13.91
TAF	36.29 ± 0.61 (1)	35.54 ± 1.08 (2)	36.04± 0.36	38.33 ± 0.40
TAF(Vi,...,^d )	35.23 ± 0.76	35.69 ± 0.44 (1)	36.44 ± 0.58	38.63 ± 0.36
mTAF (fixed ^)	33.48 ± 0.04	34.17 ± 0.07	35.46 ± 0.09	38.52 ± 0.36
mTAF	33.48 ± 0.04	34.11 ± 0.06	35.39 ± 0.13	38.44 ± 0.28
ν=3				
Vanilla	33.47 ± 0.20	33.80 ± 0.19	34.43 ± 0.22	36.09 ± 0.13
TAF	33.81 ± 0.37	34.16 ± 0.45	34.48 ± 0.20	35.75 ± 0.09
TAF(Vi,...,^d )	33.54 ± 0.17	33.83 ± 0.19	34.63 ± 0.58	35.80 ± 0.15
mTAF (fixed ^)	33.34 ± 0.04	33.66 ± 0.04	34.28 ± 0.07	35.80 ± 0.09
mTAF	33.31 ± 0.05	33.63 ± 0.04	34.21 ± 0.04	35.63 ± 0.08
Figure 2: Visualizing the 16th marginal of samples resulting from the tail generation procedure
from Section 4.2 in orange in the settings h ∈ {1, 2, 4, 8} and ν = 2 (from left to right). In blue, we
visualize samples from the true distribution.
jth marginal of the target distribution, we need to employ the above procedure with tail samples of
zk-1 (j), where k : {1, . . . , D} → {1, . . . , D} is the permutation mapping resulting from applying
all permutation layers, and k-1 is its inverse.
For all models trained on the data sets with ν = 2, we generate 1 000 samples of the heavy-tailed
variable zk-1 (16) and consider the 50 with largest absolute value as tail samples. In Figure 2 we
visualize the results for mTAF: We depict the 16th component x16 of the samples generated by
the described procedure for the distributions with different amounts of heavy-tailed components. It
becomes clear that the sampling procedure indeed results only in samples from the joint distribution
where the 16th component stems from the tails of the corresponding marginal. Surprisingly, even
in the case in which 8 heavy-tailed components are permuted (i.e. h = 8) the sampling procedure
succeeds by ignoring the body of the distribution and generating exclusively tail samples in the
16th component. As a further proof of concept, we demonstrate in Figure 5 in Section B.3 in
the Appendix that sampling tail events from other marginals than zk-1 (16) does not result in tail
events in the 16th component of x. Hence, this result substantiates our intuition that zk-1 (j) is the
“responsible” factor for generating j -tail samples. Furthermore, we rerun the sampling procedure
with a vanilla flow and a TAF, which we visualize in Figure 3. While TAF is able to generate
8
Under review as a conference paper at ICLR 2022
(a) ν = 2 and h = 4
Figure 3: Applying the tail sample generation procedure form Section 4.2 with a vanilla flow and
a TAF in two different settings. Samples from the true distribution are visualized in blue and flow
samples are in orange.
(b) ν = 2 and h = 8.
reasonable tail samples comparable to mTAF, the vanilla flow is generating extreme outliers, and is
thus, not able to efficiently sample from the tails.
5	Conclusion and Future Research Directions
In this work, we deepen the mathematical understanding of the tail behavior of triangular flows.
We note that the distribution we want model may have heavy- as well as light-tailed marginals, and
show how the marginal tail behavior of the target distribution of the flow relates to the tail behavior
of its base distribution. Based on these theoretical findings we propose a new algorithm, which we
refer to as mTAF. In particular, we initialize a trainable base distribution based on statistical tail
estimates of the target, and employ a data-driven permutation scheme that guarantees the correct tail
behavior of the target distribution. An in-depth empirical analysis on synthetic data with heavy- and
light-tailed marginals shows that mTAF benefits in terms of estimation performance and robustness
of the training. Moreover, we provide a sampling strategy inspired by out theory that allows to
generate joint samples from the target distribution with a customizable tail behavior. In summary,
we introduce a novel way to impose an inductive bias in normalizing flows that allows to model the
tails of the target distribution.
Yet, we think that there exists much potential for interesting follow-up works. For instance, we are
aware that most SOTA architectures are indeed non-affine, such as NSF (Durkan et al., 2019), which
are based on monotone rational-quadratic splines, or SOS (Jaini et al., 2019), which are based on
strictly increasing polynomials. However, similarly to (Jaini et al., 2020), we hypothesize that the
trade-off between a flexible base distributions and the complexity of the flow leads to simpler and
more efficient models. This, and the investigation on the impact of mTAF on real world data remains
subject of further future studies. Furthermore, the attentive might have noticed that the proposed
mTAF employs a restricted set of permutations, which limits its generality. One possible solution
for this issue is to employ more expressive base distributions that introduce marginal dependencies,
which extends the generality of mTAF, while preserving its ability to be marginally tail-adaptive. For
modeling these more expressive base distributions it is evident to study copula theory, which allows
a grounded treatment of various kinds of multivariate dependency concepts, even beyond the scope
of pearson correlation, such as tail dependencies (Joe, 2014). In addition, we believe that mTAFs,
and further improvements of them, bear lots of potential for sampling methods, which benefit from
the whole range of samples from the target distribution, including tail samples. Generation of such
samples might improve methods for simulation-based inference (Cranmer et al., 2020) such as VAEs
and are essential for the analysis of extremes such as in weather forecasting.
Last but not least, we want to highlight that our theory is not limited to maximum likelihood training
and one could think of various other training paradigms, for instance based on f -divergences. Us-
ing alternative optimization metrics, we can potentially put more emphasize on the learning of the
correct tail behavior.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement This work concentrates on the mathematical understanding of nor-
malizing flows, and in doing so, includes technical proofs, which might rely on abstract assumptions.
Nonetheless, we put our best effort in making all the proofs and the derived theory in Section A.1
and A.2 precise, accessible, and correct. To avoid misconceptions, we dedicate Section A.3 to the
explanation and clarification of Assumption 1. We provide documented source code including code
execution instructions, which allows the reproducibility of our empirical results. Additionally, all
algorithms and hyperparameters are presented in Section B.
References
Simon Alexanderson and Gustav Eje Henter. Robust model training and generalisation with studen-
tising flows. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit
Likelihood Models, 2020.
Jens Behrmann, Will GrathWohL Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573-582.
PMLR, 2019.
Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and Joern-Henrik Jacobsen. Under-
standing and mitigating exploding inverses in invertible neural networks. In Proceedings of The
24th International Conference on Artificial Intelligence and Statistics, volume 130, pp. 1792-
1800, Virtual Event, 2021.
Ricky T. Q. Chen, Jens Behrmann, David K Duvenaud, and Joern-Henrik Jacobsen. Residual flows
for invertible generative modeling. In Advances in Neural Information Processing Systems, vol-
ume 32, pp. 9913-9923, Vancouver, BC, Canada, 2019.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differ-
ential equations. In Advances in Neural Information Processing Systems, volume 31, Montreal,
Canada, 2018.
Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference.
Proceedings of the National Academy of Sciences, 117(48):30055-30062, 2020.
Sandor Csorgo, Paul Deheuvels, and David Mason. Kernel Estimates of the Tail Index of a Distri-
bution. The Annals of Statistics, 13(3):1050 - 1077, 1985.
J. Danielsson, L. de Haan, L. Peng, and C.G. de Vries. Using a bootstrap method to choose the
sample fraction in tail index estimation. Journal of Multivariate Analysis, 76(2):226-248, 2001.
A. L. M. Dekkers, J. H. J. Einmahl, and L. De Haan. A moment estimator for the index of an
extreme-value distribution. The Annals of Statistics, 17(4):1833-1855, 1989.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components es-
timation. In 3rd International Conference on Learning Representations, ICLR Workshop Track
Proceedings, San Diego, CA, USA, 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In
International Conference on Learning Representations, 2017.
Ann-Kathrin Dombrowski, Jan E Gerken, and Pan Kessel. Diffeomorphic explanations with normal-
izing flows. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit
Likelihood Models, 2021.
Gerrit Draisma, Laurens de Haan, Liang Peng, and T Themido Pereira. A bootstrap-based method
to achieve optimality in estimating the extreme-value index. Extremes, 2(4):367-404, 1999.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Ad-
vances in Neural Information Processing Systems, 32:7511-7522, 2019.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. nflows: normalizing flows
in PyTorch, 2020.
10
Under review as a conference paper at ICLR 2022
Paul Embrechts, ClaUdia KluPPelberg, and Thomas Mikosch. Modelling extremal events: for insur-
ance and finance, volume 33. Springer Science & Business Media, 2013.
Xavier Gabaix. ZiPf’s law for cities: An exPlanation. The Quarterly Journal of Economics, 114(3):
739-767,1999.
Xavier Gabaix, Parameswaran GoPikrishnan, Vasiliki Plerou, and H Eugene Stanley. A theory of
Power-law distributions in financial market fluctuations. Nature, 423(6937):267-270, 2003.
Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. In International Con-
ference on Learning Representations, New Orleans, LA, USA, 2019.
P. Groeneboom, H.P. Lopuhaa, and PE de Wolf. Kernel-type estimators for the extreme value index.
The Annals of Statistics, 31(6):1956 - 1995, 2003.
Bruce M Hill. A simple general approach to inference about the tail of a distribution. The annals of
statistics, pp. 1163-1174, 1975.
Marius Hofert, Ivan Kojadinovic, Martin Maechler, and Jun Yan. Elements of Copula Modeling with
R. Springer Use R! Series, 2018.
Grant Van Horn and Pietro Perona. The devil is in the tails: Fine-grained classification in the wild.
arXiv preprint arXiv:1709.01450, 2017.
Maxim (https://math.stackexchange.com/users/491644/maxim). Asymptotics of inverse of normal
cdf. Mathematics Stack Exchange. URL:https://math.stackexchange.com/q/2966269 (version:
2018-10-22).
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. In International Conference on Machine Learning, pp. 2078-2087. PMLR, 2018.
Jorn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks.
In International Conference on Learning Representations, Vancouver, Canada, 2018.
Priyank Jaini, Kira A Selby, and Yaoliang Yu. Sum-of-squares polynomial flow. In International
Conference on Machine Learning, pp. 3009-3018. PMLR, 2019.
Priyank Jaini, Ivan Kobyzev, Yaoliang Yu, and Marcus Brubaker. Tails of Lipschitz triangular
flows. In Proceedings of the 37th International Conference on Machine Learning, volume 119,
pp. 4673-4681, Virtual Event, 2020.
Harry Joe. Dependence modeling with copulas. CRC press, 2014.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, volume 31, 2018.
J. Lars Kirkby, Dang Hai Nguyen, and Duy Nguyen. Moments of student’s t-distribution: A unified
approach. Computation Theory eJournal, 2019.
Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: An introduction and review
of current methods. In IEEE Transactions on Pattern Analysis and Machine Intelligence. IEEE,
2020.
Eugene Koonin, Yuri Wolf, and Georgy Karev. Power Laws, Scale-Free Networks and Genome
Biology. 01 2006.
Mike Laszkiewicz, Johannes Lederer, and Asja Fischer. Copula-based normalizing flows. In ICML
Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models,
2021.
John Liu, Seymour Lipschutz, and Murray Spiegel. Schaum’s Outline of Mathematical Handbook
of Formulas and Tables, 4th Edition. McGraw-Hill, 2012.
11
Under review as a conference paper at ICLR 2022
Jayakrishnan Nair, Adam Wierman, and Bert Zwart. The fundamentals of heavy-tails: Properties,
emergence, and identification. In Proceedings of the ACM SIGMETRICS/International Confer-
ence on Measurement and Modeling of Computer Systems, pp. 387-388, New York, NY, USA,
2013.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In Advances in Neural Information Processing Systems, volume 30, pp. 2338-2347,
Long Beach, CA, USA, 2017.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran
Associates, Inc., 2019.
Yongcheng Qi. Bootstrap and empirical likelihood methods in extremes. Extremes, 11:81-97, 03
2008.
Sidney Resnick. On the foundations of multivariate heavy-tail analysis. Journal of Applied Proba-
bility, 41:191-212, 2004.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional conference on machine learning, pp. 1530-1538. PMLR, 2015.
Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density
models. CoRR, abs/1302.5125, 2013.
E. G. Tabak and Cristina V. Turner. A family of nonparametric density estimation algorithms. Com-
munications on Pure and Applied Mathematics, 66(2):145-164, 2013.
Ivan Voitalov, Pim van der Hoorn, Remco van der Hofstad, and Dmitri Krioukov. Scale-free net-
works well done. Phys. Rev. Research, 1:033034, Oct 2019.
George Kingsley Zipf. Human behavior and the principle of least effort: An introd. to human
ecology. Addison-Wesley Press, 1949.
A Theory and Proofs
In this Section, we derive and proof all of our theoretical results. We start to present some prelimi-
nary results in Section A.1, which help us providing the proof of our main result in Section A.2. In
Section A.3, we illuminate the technical Assumption 1 and provide examples and intuitions.
A. 1 Preliminary Theoretical Results
Proposition 3. Let x ∈ R be a random variable. Then, it holds that x has tail index less or equal
than α iff. for any β > α and any C > 0 it is
I	∖x∖βp(x)dx = ∞ .
|x|>C
Proof. Let us first assume that x has tail index at most α. Then, according to Definition 2, we know
that for any β > α and C > 0
∞ = Ex [∖x∖β] =	∖x∖β p(x)dx =	∖x∖β p(x)dx +	∖x∖β p(x)dx
JR	√∣χ∣≤C	√∣χ∣>C
≤ Cβ	p(x)dx +	∖x∖β p(x)dx .
12
Under review as a conference paper at ICLR 2022
Since we assume p to be continuous, we can bound p on the compact interval [-C, C], and hence,
the first above integral must be bounded. Therefore, it is
I	∖x∖βp(x)dx = ∞ .
|x|>C
To prove the back-direction, let us consider a β > α and C > 0. Very similarly to the forward-proof,
we can now see that
∞ =	∖x∖β p(x)dx
|x|>C
= Cβ	p(x)dx +	∖x∖β p(x)dx
= Ex ∖x∖β ,
where the second equality follows from the finiteness of the integral. Since this follows for any
β > α, x must have tail index α or less.
□
This simple result demonstrates that the tail index, as indicated by the name, depends on the tail
of the distribution, i.e. on the PDFs behavior for large values ∖x∖ > C. This fact motivates why
maximum likelihood estimations of the tail index, which depend on the whole distribution are biased.
For more elaborate details, we refer to Section 9 in Nair et al. (2013).
In a similar fashion to the previous result, the next technical lemma states that unboundedness of
the moment-generating function is due to the unboundedness of the integrand for tail events, i.e. for
z > z*. This little lemma turns out to be useful in the proof of Proposition 2.
Lemma 1. Let Z ∈ R be heavy-tailed. Then it holdsfor any z* ∈ R and λ > 0 that
/
z»z*
eλz pz (z)dz = ∞ .
Proof. Since z ∈ R is heavy-tailed, we know that for all λ > 0
∞ = mz(λ) = eλz
R
pz(z)dz
/
z≤z
eλzpz(z)dz +
z>z
eλz pz (z)dz
*
≤ Fz(z*)eλz* +
z>z
eλzpz (z)dz ,
*
*
where Fz is the CDF of z. The last inequality follows from the fact that exp(λz) is monotonic
increasing in z. Since the first summand is bounded, it follows that the second summand must be
unbounded. This completes the proof.	□
Recall that in Proposition 1 We state that j-heavy-tailedness induces '2-heavy-tailedness. In the
following, we provide a formal proof of this result.
Proof of Proposition 1. For this proof, We employ the equivalent definition of heavy-tailedness of
xj via the decay rate of its distribution function (see e.g. Lemma 1.1. in Nair et al. (2013)), i.e.
lim sup --j(Xj) = ∞ for all λ > 0 ,	(3)
xj →∞	e j
Where Fj is the CDF ofxj. Since xj ≤ kxk for all x ∈ RD, We can conclude that Fj (a) ≥ Fkxk (a)
for a ∈ R. Therefore,
1 - Fkxk (O) ≥ 1 - FXj ⑷
e-λa	e-λa
for a → ∞ .
According to the equivalent definition in (3), kx∣∣ is heavy-tailed, which proves that X is '2-heavy-
tailed.	□
13
Under review as a conference paper at ICLR 2022
The following is a well-known implication of the change of variables formula and the integration
rule by substitution, which we are going to apply in the subsequent proofs.
Lemma 2 (Substitution in the Moment-Generating Function). Let T be a diffeomorphism such that
T (z) = x for some random variables x, z ∈ RD. Then, we can rewrite
/ eλxp(x)dx = /
eλT(z)q(z)dz .
For completeness, we give a brief proof of this result.
Proof. Using the change of variables formula, see (1), we can write
/ eλxp(x)dx = / eλxq(T-1(x)) Idet JT-i(X)Idx .
Now, we can rewrite exp(λx) = exp(λT T-1(x) and substitute z = T-1. Integration by substi-
tution completes the proof.	口
Next, we present how we can use copulae to reformulate a multivariate PDF.
Definition 5 (Copula). A copula is a multivariate distribution with cumulative distribution function
(CDF) C : [0, 1]D → [0, 1] that has standard uniform marginals, i.e. the marginals Cj of C satisfy
Cj ~ U[0,1].
Theorem 4 (Sklar’s Theorem). Taken from Hofert et al. (2018).
1.	For any D-dimensional CDF F with marginal CDFs F1, . . . , FD, there exists a copula C
such that
F(Z)= C (F1(z1),...,FD (ZD))	(4)
for all z ∈ RD. The copula is uniquely defined on U := QjD=1 Im(Fj), where Im(Fj) is
the image of Fj . For all u ∈ U it is given by
C(U) = F (Fr(UI),...,FDr(UD )),
where Fr are the right-inverses of Fj.
2.	Conversely, given any D-dimensional copula C and marginal CDFs F1 , . . . FD, a function
F as defined in (4) is a D-dimensional CDF with marginals F1 , . . . , FD.
Therefore, if F is absolutely continuous, we can differentiate (4) to obtain the PDF of z
D
q(Z) = c F1(Z1), . . . , FD(ZD)	qj (Zj) ,
j=1
where c denotes the PDF of the copula C .
Lastly, we present the following asymptotic behavior of the inverse CDF of a standard Gaussian
distribution, which we use in Section A.3 to explain Assumption 1.
Lemma 3 (Asymptotic Behavior8 of Φ-1(1 - y)). Denote by Φ the CDF ofa standard Gaussian
distribution. Then, it holds for the inverse of Φ that
Φ-1(1 - y) ~ p-2log(y) for y → 0 .
Proof. First, we note that
φ(x) = 2 + 3f(√) ~ 1 -ɪe-x2/2 ,
2	2	2	x2
8The idea of the proof is due to (https://math.stackexchange.com/users/491644/maxim)
14
Under review as a conference paper at ICLR 2022
which is a well-known asymptotic (Liu et al., 2012). Here, erf denotes the error-function. Rearrang-
ing terms gives
2
x
—〜
2
—
—
2
x
— as X → ∞ .
Finally, we can invert the above asymptotic equation to obtain
Φ-1(y) = √-2log(1 - y) for y → 1
or equivalently
Φ-1(1 - y) = √-2log(y)	for y → 0 .
□
A.2 Proof of the Main Result
The proof of Proposition 2 relies on lower-bounding the moment-generating function of each
marginal xj . In order to derive such a bound of a multivariate integral, we rewrite the joint dis-
tributions q≤j using their copula densities:
q≤j (Z ≤j ) = Cj (FI(ZI),..., Fj (Zj )) ɪɪ qi(zi)
i<j
for any j ∈ {1, . . . , D} and for corresponding copula density cj. Our proof relies on the following
technical condition on the decay rate of the copula densities.
Assumption 1 (Bounding the Marginal Decay Rate of the Copula Densities). For all j ∈
{1, . . . , D} and λ > 0 there exists a compact set S ⊂ Rj-1 with positive (Lebesgue-)mass, a
constant Zj > 0, a scaling constant s > 0, and afunction f (z<j∙) < λσ(z<j∙) for z<j∙ ∈ S such that
Cj (FI(Zι),..., Fj(Zj)) ≥ se-f(z<j)zj	for Zj > Zj and z<j ∈ S ,	(5)
where cj is the copula density of q≤j
This assumption sets a bound on the decay rate of the copula density with respect to Zj. We clarify
this assumption in Section A.3 with additional examples.
Now, we set all preliminaries to prove Proposition 2.
Proofof Proposition 2. We start by considering the case j = 1. In this case it is xι = μ + σzι and
therefore
mx1(λ)=
R
=Z
R
eλx1p1(x1)dx1
eλ(μι+σ1z1) q1(Z1)dZ1 (Lemma2)
eλμ1 [ eλσ1z1 q1(Z1)dZ1 .
R
Defining λ0 := λσ1 > 0, we can see that the last integral is unbounded due to the heavy-tailedness
of z1, see Definition 1. Therefore, mx1 (λ) = ∞ for all λ > 0, which proves the heavy-tailedness
of x1 .
Next, we consider the case j > 1. Again, we examine the moment-generating function ofxj. Define
the jth canonical basis vector vj := (0, . . . , 0, 1, 0, . . . , 0)>. Then,9
mxj (λ) = mvj>x =
=Z
RD
=Z
Rj
eλvj>xp(x)dx (LOTUS)
eλTj(zj,z<j)q(Z)dZ (Lemma 2)
eλμ(Z<. )+λσ(z<j)Zj q≤j. (Z≤j. )dZ≤j.
/	eλμ(z<j) / eλσ(z<j)zjq≤j(Z≤j)dZjdZ<j .
(6)
9Note that for the sake of clarity, We leave out the index j in μj and σj.
15
Under review as a conference paper at ICLR 2022
Using Sklar’s Theorem (Theorem 4), we can write any joint PDF as the product of marginals and a
copula density cj such that
q≤j (Z≤j ) = Cj (FI(ZI),...,Fj (Zj )) ɪɪ qi(zi ) .	⑺
i<j
We plug (7) into (6) to obtain
mχj (λ) = Leλ"(z<j )q<j(Z<j) ɪ eλσ(z<j )zj Cj (FI(Z1), …，Fj (Zj ))qj (Zj )dzj dz<j
≥ e eλμ(z<j )q<j (Z<j) I	eλσ(z<j )zj Cj (F1(Z1),…，Fj (ZjIqj (Zj)dZjd,Z<j ,⑻
S S	J zj>zj
since all quantities within the integral are positive. Using Assumption 1, we can bound the inner
integral of the above equation, which we denote by A(Z<j), and get
A(Z<j) ≥ s
Jzj>zj
e(λσ(z<j)-f(z<j))zjqj(Zj)dZj
s	eλ0zjqj(Zj)dZj
J zj>zj
(define λ0 := λ - σ(Z<j) - f(Z<j) )
∞ for all Z<j ∈ S ,
due to the heavy-tailedness of Zj and Lemma 1. Since S is compact, μ and q are both continuous,
and q is positive, We deduce that exp(λμ(Z<j))q<j(z‹) is lower-bounded (by a constant larger
than 0) in S. Therefore, employing (8) and using that S has positive mass, we can lower-bound
the moment-generating function by ∞, which proves the heavy-tailedness of xj . In summary, x is
j-heavy-tailed for all j ∈ {1,..., D}.	□
A.3 Notes on Assumption 1
Assumption 1 might look troublesome at first sight, but we will illustrate in this section that the
condition is indeed very reasonable. We will show how to verify it in simple examples, and we will
introduce a simpler, more intuitive sufficient condition for it.
First of all, let us present a restricted but more intuitive version of Assumption 1.
Assumption 2 (Simplification of Assumption 1). For all j ∈ {1, . . . , D} and λ > 0 it holds for
S := [a, b]j-1 that there exist constants Zj and s > 0 such that
C(FI(Zι),..., Fj (Zj)) ≥ se-(λσ-ε)zj	for Zj > Zj and Z<j ∈ S ,	(9)
where Cj is the copula density ofq≤j, λσ is a lower bound of λσ(Z<j), ε > 0 is small such that
λσ - ε > 0.
Let us summarize the simplifications that we make in Assumption 2. First of all, we restricted S
to be a closed cube [a, b]j-1, which is obviously a specific instant of a compact set with positive
mass. Further, we assumed σ to be continuous, and thus, λσ(Z<j ) must be lower-bounded in S.
This allows us to replace the function f by the constant λσ - ε for arbitrary small ε > 0.
After giving this simplified sufficient condition, we provide some intuition by presenting some ex-
amples where Assumption 2 holds true.
Example 1 (Independent Variables). Consider a random variable z with independent components,
i.e. q(Z) = QjD=1 qj (Zj ). Then, the associated copula is the independence copula (Figure 4a),
which is a uniform random distribution on [0, 1]D. Therefore it is C F1(Z1), . . . , FD(ZD) = 1 for
all Z ∈ RD and Assumption 2 follows immediately since s exp(-(λσ - ε)Zj) → 0for Zj → ∞.
Example 2 (Bounded Copula Density). Consider a lower-bounded copula density, i.e. there exists
a lower bound a > 0 such that
C(u1, . . . , uD) ≥ a for all u ∈ [0, 1]D .
Again, the validity of Assumption 2 in this setting is clear.
16
Under review as a conference paper at ICLR 2022
1.0
0.8
0.6
0.4-
0.2-
0.0
0.00	0.25	0.50	0.75	1.00
(a) 2-dimensional Independence copula: (b) 2-dimensional Gaussian copula with cor-
c(z1 , z2 ) = 1.	relation ρ12 = 0.7 .
Furthermore, this assumption is obviously not limited to bounded copula densities but also holds for
copula densities that converge to 0 but whose decay rate in zj is lower-bounded by (9). To visualize
the intuition, consider the 2-dimensional copula density of a Gaussian copula in Figure 4b. Imagine
fixing S such that F1(z1) ∈ [0.5, 0.75], which is compact for continuous F1. Then (9) bounds the
decay rate within the “tube” [0.5, 0.75] if we consider z2 → ∞, i.e. if F2(z2) → 1. Next, we show
how we can formally prove the assumption for Gaussian copulae.
Example 3 (Gaussian Copula). The Gaussian copula with correlation matrix R ∈ RD×D has
density function
c(u)
√=exp( -1 (φT(UI),...φT(UD))(R-1
det R 2
-1 )(Φ-1(uι),...φT(uD ))>
(10)
where I ∈ RD×D is the identity matrix, Φ-1 is the inverse CDF of the univariate standard Gaussian
distribution, and U ∈ [0,1]D. In the following, we consider Assumption 2 forj = D.
Note that S is assumed to be a compact set, therefore Fj (Zj ) are all upper and lower-bounded by
some value for Z<j ∈ S. This makes all polynomials of them also bounded. Hence, we can find
constants a0 , b0 , c0 such that we can lower-bound the term within the exponential in (10) by
—2(a0Φ-1 (Fd(zd))2 + b0Φ-1(FD(ZD)) + C .
Plugging the above into (10) gives
c(Fι(zι), ∙∙∙, FD(ZD)) ≥ ~d=tR exp
H exp( —aΦ-1 (Fd(ZD))2 + bΦ-1 (Fd(XD)) ) (for some a, b)
≥ exp( -∣a∣Φ-1 (Fd(ZD))2 + M∣Φ-1(Fd(ZD))
≥ exp( -∣a∣Φ-1 (Fd(ZD))2
(11)
where the last line applies if Φ-1 (Fd (xd)) ≥ 0, which is satisfied if z* is large enough10.
Next, we use the asymptotic relation from Lemma 3
φ-1(FD(xd))〜q-2log(l — FD(zD)).
10for instance if z* is larger than the median of ZD
17
Under review as a conference paper at ICLR 2022
Hence,for each ε > 0 there exists a z* large enough such that
φ-1(FD (ZD))	1 .
- 1 < ε ,
-2 log 1 - FD(zD)
which can be rearranged to
Φ-1(Fd(ZD)) < J-2log(1 - FD(ZD))(1 + ε) .
Plugging the above into (11), we obtain
c(Fι(zι),..., FD (ZD)) ≥ exp(2|a|(1 + ε)2 log(1 - FD (ZD)))
=(1- FD(zd))2a ,	(12)
where we define α := 2|a|(1 + ε)2. Hence, we are left to lower-bound (12), which we can do for a
range of heavy-tailed marginal distributions such as:
1.
Pareto distribution: The Pareto distribution with shape parameter α has CDF
F(Z) = 1 -：.
Therefore,
(1 - FD(ZD))2a
1 2∣α∣α
=----
ZD
= exp -2αaα log(ZD)
≥ exp -2αaαZD
for ZD ≥ e.
2.	Scale invariant distributions: Following the same argument as above, each distribution
with CDF
F(Z) = 1 - bZ-α forZ > Z*	(13)
for constants b, α, Z * > 0 satisfies the bound
(1 — FD(zd))2a ≥ bexp(-2αaZD).
Each distribution with CDF as in (13) is a scale-invariant distribution (see e.g. Theorem 2.1
in Nair et al. (2013)).
3.	Exponentially decaying distributions: Every distribution that satisfies the bound
F(Z) ≤ 1 - exp(Z)-α
for Z > Z * and α > 0. In this case, we can again show that Assumption 1 is valid:
(1 - Fd(zd))2a ≥ exp(-2aaZD).
Lastly, we want to emphasize that Corollary 1 is derived by an iterative application of Theo-
rem 3. Therefore, Assumption 1 must hold for all “flow steps"，i.e. if T = T(L) ◦ ∙∙∙ ◦ T(1),
we need to ensure validity of Assumption 1 for z(0) := z, z(1) := T(1) (z), z(2) := T(2) ◦
T⑴(z), ..., Z(LT) := T(LT) ◦…◦ T(I)(Z). InExample 1,we show that this assumption holds
true for Z(0) since we define our base distribution under the mean-field assumption. Furthermore,
we conjecture that if we apply a Lipschitz-continuous diffeomorphism on a random variable with
bounded copula density, then the transformed random variable must also have a bounded copula den-
sity. Hence, Assumption 1 would be valid for all “flow steps” (see Example 2) when starting with
a mean-field base distribution q(Z) = QjD=1 qj (Zj ). However, this conjecture needs to be studied in
further research.
18
Under review as a conference paper at ICLR 2022
B	Algorithms and Computational Details
B.1 Tail Estimation
Many heavy-tailed distributions can be characterized by their tail index, which include the set of
regularly varying distributions,11 such as the t-distribution, the Pareto distribution, and many more.
However, as already shown in Section 2.1, the tail index does not depend on the body of the distri-
bution, and hence, non-tail samples must typically be discarded for tail index estimation. Although
a variety of estimators for the tail index exist, such as the Hill estimator (Hill, 1975), the moment
estimator (Dekkers et al., 1989), and kernel-based estimators (Csorgo et al., 1985), none of them is
considered to be as superior in all settings. A major issue of all mentioned estimators is that they
are based on a threshold defining the tail, i.e. the user needs to input statements of the form “the k
largest samples are considered to be tail events”. Even though there exist some strategies to find k,
there is none working robustly in all settings. In fact, one can construct simple counter examples for
all estimators that lead to failures of tail estimation. This is due to undesired properties of the esti-
mators, such as the lack of translation invariance of the Hills estimator (while the tail index clearly
is location invariant). We refer to Section 9 in Nair et al. (2013) for a detailed text book treatment of
tail index estimation. In summary, robust tail estimation is still considered as an unsolved problem,
which forces practitioners to consider multiple estimators to make a well-founded decision. Fur-
thermore, we note that the Hills estimator can only be applied for regularly varying distributions,
which excludes the application of the Hills estimator to classify light-tailed distributions. In con-
trast, the moments and the kernel estimator can both be applied to identify heavy-tailed marginals
and to assess a tail index.
To implement the tail assessment scheme, see Step 1 of the proposed method in Section 3.2, we
found that Algorithm 1 works fine in classifying the correct tail behavior and giving a decent ini-
tialization for the tail indices. We reused the code by Voitalov et al. (2019), which implements
all tail estimation procedures12 from our Algorithm. Notice that we clip the tail index by 20, i.e.
the algorithm classifies marginals with a tail index larger than 20 as light-tailed, which prevents a
too restrictive set of allowed permutations, see Step 3 in Section 3.2. For illustration, consider the
following simple example. Assume that we estimate all except of one marginal to be heavy-tailed.
Then, the first component of the flow is never allowed to permute with other components, since they
are classified as heavy-tailed. Hence, the mixing of the first component would be severely restricted.
Further, since large tail indices indicate a less heavy-tailed distribution, it is reasonable to clip the
tail index at some threshold.
Algorithm 1 Marginal Tail Estimation
Require: Data_val
tail.est J []
for j in {1, . . . , D} do
marginal J Data_val[:,j]
moments J moments_est[|marginal|]	. 0 if ∣marginal∣ is estimated to be light-tailed
kernel J kernel_est[∣marginal|]	. 0 if ∣marginal∣ is estimated to be light-tailed
if moments==kernel==0 then
tail_est.append(0)	. light-tailed if moments and kernel estimate a light-tailed marginal
else
hill J hills_est( | marginal |)
if hill> 20 then
tail_est.append(0)	. light-tailed if hills estimator predicts high tail index
else
tail_est.append(hill)
end if
end if
end for
return tail.est
11see Section 2 in Nair et al. (2013) for further details
12including the hyperparameter selection (Danielsson et al., 2001; Qi, 2008; Draisma et al., 1999; Groene-
boom et al., 2003)
19
Under review as a conference paper at ICLR 2022
0.175
0.150

0.125
0.075
0.025
0.000
-10	0
10	-10	0	10	-10	0
10	-10	0
10
Figure 5: Applying the sample generation procedure from Section 4.2 but with tail samples from
other base marginals and visualizing the 16th target marginal. Samples from the true distribution are
visualized in blue and flow samples are in orange. We sample from the 3rd, 7th, 11th, and the 15th
base marginal, respectively (from left to right). The visualized setting is ν = 2 and h = 8.
B.2	Synthetic Data Generation
Our generation of the synthetic distribution consists of 3 steps: 1. Generating the marginal distri-
butions, 2. Defining a copula distribution, 3. Combining the marginal and the copula to obtain a
multivariate joint distribution.
Generating the marginal distributions. The first four marginals are defined to be Gaussians. The
following marginals are two 2-mixtures of two Gaussians and two mixtures of three Gaussians. The
last h ∈ {1, 2, 4, 8} components are a mixture of two t-distributions and the remaining marginals are
again mixtures of two Gaussians. All mixtures have equal weight for each mixture component and
all means and standard-deviations are randomized. Means are constructed by uniformly sampling
from [-4, 4], whereas standard-deviations are sampled from [1, 2].
Defining a copula distribution. Recall, a Gaussian copula (10) is parameterized by a correlation
matrix R. To generate R, we randomly sample 16 different pairs (i, j) ∈ {1, . . . , 16}2 with i 6= j
and set the corresponding entry of the correlation matrix
Ri,j := 0.25 .
Obtaining a joint distribution. Lastly, we combine the marginals with the Gaussian copula using
Sklar’s Theorem 4. This gives us a multivariate distribution with specified and complex marginals
with a dependency structure given by the copula, see Joe (2014) for more details on the induced
dependencies.
To construct the training, test, and validation sets 15.000, 5 000, and 5 000 samples from this distri-
bution are sampled, respectively.
B.3	Sampling marginally Tail Events
In the main text, we have seen that the proposed tail generation procedure is successful in generating
samples that are marginally in the tail of some specified marginal (Figure 2). In Figure 5, we present
the 16th marginal when repeating the same experiment but with marginal tail samples from other
marginals zj than the proposed zk-1 (16) .
B.4	Hyperparameters and Experimental Details
In all our experiments, we constructed a model using L = 5 MAF flow layers, where each emerging
neural networks contains 1 hidden layer and 200 hidden neurons. The tail index in TAF is initially
set to ^ = 10. As a permutation scheme, We employ random permutations in the vanilla flow and
20
Under review as a conference paper at ICLR 2022
TAF, and random permutations within the sets of light- and heavy-tailed marginals, respectively,
in mTAF. In addition, we applied batch-normalization after each triangular transformation. For
optimization, we use an Adam optimizer with a learning rate of 1e - 5 and a weight-decay of 1e - 6.
The learning rates of the tail indices are set to 0.1 for TAF and for mTAF. We train until 200 epochs
or use early stopping if training gives no improvement after 30 epochs. We use a batch size of 256.
Our code is built with PyTorch (Paszke et al., 2019) and we make extensive use of the package nflows
(Durkan et al., 2020) to implement mTAF. All code is provided along the submission.
21