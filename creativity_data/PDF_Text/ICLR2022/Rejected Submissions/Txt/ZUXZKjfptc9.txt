Under review as a conference paper at ICLR 2022
Bit-aware Randomized Response for Local Dif-
ferential Privacy in Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we develop BitRand, a bit-aware randomized response algorithm, to
preserve local differential privacy (LDP) in federated learning (FL). We encode
embedded features extracted from clients’ local data into binary encoding bits, in
which different bits have different impacts on the embedded features. Based upon
that, we randomize all the bits to preserve LDP with three key advantages: (1)
Bit-aware: Bits with a more substantial influence on the model utility have smaller
randomization probabilities, and vice-versa, under the same privacy protection;
(2) Dimension-elastic: Increasing the dimensions of embedded features, gradients,
model outcomes, and training rounds marginally affect the randomization proba-
bilities of binary encoding bits under the same privacy protection; and (3) LDP
protection is achieved for both embedded features and labels with tight privacy
loss and expected error bounds ensuring high model utility. Extensive theoretical
and experimental results show that our BitRand significantly outperforms various
baseline approaches in text and image classification.
1 Introduction
Recent data privacy and security regulations (GDPR, 2018; Regulation, 2018; Cybersecurity Law,
2016) pose major challenges in collecting and using personally sensitive data in different places for
machine learning (ML) applications. Federated Learning (FL) (McMahan et al., 2017; Kairouz et al.,
2019) is a promising way to address these challenges, enabling clients to jointly train ML models by
sharing and aggregating gradients computed from clients’ local data through a coordinating server
for model updates. However, recent attacks (Zhu et al., 2019; Y. et al., 2021; Zhao et al., 2020a) have
shown that clients’ training samples, each of which includes an input x and a ground-truth label yx ,
can be extracted from the shared gradients. These attacks underscore the implicit privacy risk in FL.
Our main goal is to provide a strong guarantee that the shared gradients protect the privacy of clients’
local data without undue sacrifice in model utility. Local differential privacy (LDP) has emerged as a
crucial component in various FL applications (Yang et al., 2019; Kairouz et al., 2019). To achieve
our goal, we focus on preserving LDP in cross-device FL, i.e., in which clients jointly train an FL
model (Kairouz et al., 2019).
In cross-device FL, existing LDP-preserving approaches can be categorized into three lines: Clients
(1) add noise into local gradients derived from their local training samples, e.g., using Gaussian
mechanism (Abadi et al., 2016), to protect membership information at the training sample level with
DP guarantees (Zheng et al., 2021; Dong et al., 2019; Malekzadeh et al., 2021; Geyer et al., 2017;
Huang et al., 2020), (2) add noise to local gradients using Randomized Response (RR) mechanisms
to protect the values of the local gradients with LDP guarantees (Sun et al., 2021; Liu et al., 2020;
Zhao et al., 2020b; Wang et al., 2019a), and (3) add noise into each training sample, e.g., embedded
features and labels, using RR mechanisms to protect the value of each training sample with LDP
guarantees (Arachchige et al., 2019; Lyu et al., 2020a), then the clients use LDP-preserved training
samples to derive local gradients. For all three approaches, in each training round, clients send DP or
LDP-preserved local gradients to the coordinating server for model updates, which will be sent back
to the clients for the next training round.
In this paper, we focus on protecting clients’ training data at the value level with LDP guarantees.
Existing RR mechanisms to preserve LDP in FL suffer from the curse of privacy composition, in
which excessive privacy budgets are consumed proportionally to the large dimensions of input or
embedded features (Arachchige et al., 2019), gradients (Zhao et al., 2020b; Wang et al., 2019a), and
training rounds (Zhao et al., 2020b; Wang et al., 2019a), causing loose privacy protection or inferior
model accuracy (Wagh et al., 2021).
1
Under review as a conference paper at ICLR 2022
Addressing the curse of privacy composition is non-trivial. Existing approaches, such as anonymizers
(assumed to be trusted), i.e., shuffler (Erlingsson et al., 2019; Sun et al., 2021; L. et al., 2020; Wang
et al., 2019b; Cheu et al., 2019; Balle et al., 2019) or anonymity approaches (e.g., faking source IP,
VPN, Proxy, etc. (Sun et al., 2021; Cormode et al., 2018)), and dimension reduction (Liu et al., 2020;
Zhao et al., 2020b; Shin et al., 2018; Xu et al., 2019), mitigate the problem but also have limitations.
In the real world, it is possible that the anonymizers can either be compromised or collude with the
coordinating server to extract sensitive information from observing LDP-preserved local gradients
(Erlingsson et al., 2019). Meanwhile, applying RR mechanisms on reduced sets of embedded features
or gradients using dimension reduction techniques can work well with lightweight models, such as
logistic regression and SVM (Liu et al., 2020; Zhao et al., 2020b; Wang et al., 2019a). However, it
is challenging for these techniques to achieve good model utility under tight LDP guarantees given
complex models and tasks, such as DNNs, since the dimensions of reduced embedded features,
gradients, and training rounds still need to be sufficiently large (Zhao et al., 2020b; Liu et al., 2020).
Hence, the curse of privacy composition in preserving LDP by applying RR mechanisms in FL
remains a largely open problem. Orthogonal to this, preserving LDP to protect ground-truth labels
yx in FL has not been well-studied. Two known approaches for centralized training are 1) injecting
Laplace noise into the labels (Phan et al., 2020) and 2) applying RR mechanisms on the labels to
achieve DP at the label level (Ghazi et al., 2021). However, centralized training in Ghazi et al. (2021)
has not been designed for FL with LDP guarantees since they require centralized and trusted databases.
The model utility in Phan et al. (2020) is notably affected by the number of model outcomes.
Key Contributions. To mitigate the curse of privacy composition and optimize the trade-off between
privacy and model utility, our paper is structured around the following contributions:
1)	We propose BitRand, which is a combination of a novel bit-aware f-RRmechanism and label-RR
mechanism, to preserve LDP at both levels of embedded features and labels in FL. In f -RR, we encode
embedded features (extracted from x) into a binary-bit string, which will be adaptively randomized
such that bits with a more substantial impact on model utility will have smaller randomization
probabilities and vice-versa under the same privacy budget. To preserve LDP on yx , we develop a
generalized randomization, in which the probability of randomizing label yx from one to another
class is a function of the number of model outcomes C . By doing that, we can optimize the trade-off
between model utility and privacy loss with significantly tighter expected error bounds.
2)	By incorporating sensitivities of binary encoding bits into a generalized privacy loss bound, we
show that increasing the dimensions of embedded features r, encoding bits l, and model outcomes C
marginally affect the randomization probabilities in BitRand under the same privacy budget. This
dimension-elastic property is crucial to evade the curse of privacy composition by retaining a high
value of data transmitted correctly through our randomization given large dimensions of r, l, and C .
3) These bit-aware and dimension-elastic properties allow us to work with complex models and
tasks with formal LDP guarantees for training samples (x, yx) while retaining high model utility.
Extensive theoretical analysis and experimental results conducted on fundamental FL tasks, i.e.,
text and image classification, using benchmark datasets and our collected Security and Exchange
Commission financial contract dataset show that our BitRand significantly outperforms a variety of
baseline approaches in terms of model utility under the same privacy budget.
2	Background
LDP-preserving mechanisms (Erlingsson et al., 2014; Duchi et al., 2018; Wang et al., 2017; Acharya
et al., 2019; Bassily & Smith, 2015) generally build on the ideas of randomized response (Warner,
1965), which was initially introduced to allow survey respondents to provide their correct inputs
while maintaining their confidentiality. -LDP is presented as follows:
Definition 1. -LDP. A randomized algorithm M fulfills -LDP, if for any two inputs x and x0, and
for all possible outputs O ∈ Range(M), we have: P r[M(x) = O] ≤ eP r[M(x0) = O], where
is a privacy budget and Range(M) denotes every possible output of the algorithm M.
The privacy budget controls the amount by which the distributions induced by inputs x and x0
may differ. A smaller enforces a stronger privacy guarantee. We revisit RR mechanisms for
LDP preservation in Appendix A. Our approach is a binary encoding-based approach, similar to
(Arachchige et al., 2019; Lyu et al., 2020a), since it has the potential to overcome the curse of privacy
composition. In binary encoding, x is converted into an l-bit vector v consisting of 1 sign bit, m bits
for the integer part, and l-m-1 bits for the fraction part (Figure 1), as follows:
∀i ∈ [0, l - 1] : vi = b2i-m|x|c mod 2	(1)
2
Under review as a conference paper at ICLR 2022
Each bit in v is randomized by applying a RR mechanism,
e.g., (Erlingsson et al., 2014; Bassily & Smith, 2015; Wang
et al., 2017), to generate a perturbed l-bit vector v0, which
preserves LDP. However, in our theoretical reassessment
(Appendices I and J), directly applying RR mechanisms
on binary encoded vectors as in existing mechanisms, i.e.,
l	Integer	Fraction
—0	1	2 I m m + 1	] I - 1
o I 1 I 6 I ... I i I 0 I 0 I ... I [
2∏ι+ι 2m-ɪ 2τn-2
Sign Highest
bit Integer bit
2。	2-ɪ 2-2
Lowest	Highest
Integer bit Fraction bit
2-(i-l—m)
Lowest
Fraction bit
Figure 1: Binary encoding.
LATENT (Arachchige et al., 2019) and OME (Lyu et al., 2020a), consumes huge privacy budgets
since each binary encoding bit cannot be treated as a bit in a hash. Each binary encoding bit i has a
different sensitivity, i.e., ∆i = 2m-i for the integer and fraction parts or ∆i = 2m+1 for the sign bit
(Lemma 1), compared with a bit Bi in a hash, i.e., ∀Bi : ∆Bi = 1. Our mechanism does not suffer
from this problem, thanks to our bit-aware randomization in binary encoding (Theorem 2).
3	BitRand Algorithm
Let us now present our FL setting, threat
model, and BitRand algorithm (Figure 2 and
Alg. 1, Appendix B), and privacy guaran-
tees. Then, we will show that our algorithm
is dimension-elastic and the ability to opti-
Figure 2: Federated Learning with BitRand Algorithm.
mize the randomization probabilities with expected error bounds in our theoretical analysis.
Federated Learning Given N clients, each client u ∈ [1, N] has a set of training samples Du =
{(x, yx )}nu where x ∈ Rd is the input features, and its associated ground-truth label yx ∈ ZC is
one-hot encoded with C categorical model outcomes {yx,1 , . . . , yx,C}, and nu is the number of
training samples. In a pre-processing step, each client u extracts r numerical embedded features from
x, denoted ex, by using a pre-trained model fpre. In practice, fpre could be trained on large-scale
and publicly available datasets without introducing any extra privacy cost to the clients’ local data
(He et al., 2016; Devlin et al., 2018). N clients jointly train the model fθ : Rr → RC by minimizing
a loss function L fθ (ex), yx on their local training samples in Du that penalizes mismatching
between the prediction fθ (ex) and the ground-truth label yx, given the model’s parameters θ. In each
training round t, each client u receives the most updated model parameters θt from the coordinating
server and then computes local gradients 5θu = (v ,y )∈D 5θt L(f (ex), yx)/nu, which are sent
to the coordinating server for aggregating and model updating: θt+1 = θt - ηt u∈[1,N] 5θu /N.
Threat Model. The coordinating server strictly follows the training procedure but curious about
the training data Du . This is a practical threat model in the real world since service providers
always aim at providing the best services to the clients (Haeberlen et al., 2011; Truex et al., 2019;
Lyu et al., 2020b). Given the observed gradients 5θu , the coordinating server can extract the
clients’ data {ex, yx}nu by using recently developed attacks (Carlini et al., 2020; Fredrikson et al.,
2015). In a defense-free environment, {ex , yx }nu can be used to infer the sensitive training data
Du = {(x, yx)}nu using the pre-trained model fpre, since fpre (x) = ex (Song & Raghunathan,
2020). This poses a severe privacy risk to the sensitive data Du .
BitRand Algorithm. To protect the sensitive training data Du against the threat model, in our
algorithm, we preserve LDP on both embedded features ex and labels yx .
Each of the r embedded features in ex is encoded into l binary bits following Eq. 1. Binary encoded
features are concatenated together into a vector vx consisting of rl binary bits to represent the
embedded features ex (Alg. 1, line 16). Each bit i ∈ [0, rl - 1] in vx is randomized by our f-RR
mechanism (Alg. 1, line 17) With a bit-aware term 苧 optimizing randomization probabilities:
pX =
(f -RR mechanism) ∀i ∈ [0, rl - 1] : P (vx0 (i) = 1) =
qX =
^一.	77%1	ʌ, if Vx ⑴ =1
1 + α exp( i%l eχ)
α exp( i% eχ)	⑵
li%l、, if Vx ⑴=0
1 + a exp(岑 eχ)
where vx(i) ∈ {0, 1} is the value of vx at the bit i, vx0 is the perturbed vector created by randomizing
all the bits in vx , X is a privacy budget, and α is a parameter bounded in Theorem 2. From Eq. 2,
we also have that P (vx0 (i) = 0) = 1 - pX if vx(i) = 1, and P (vx0 (i) = 0) = 1 - qX if vx(i) = 0.
We use the bit-aware term i%%l to indicate the location of bit i, which is associated with the sensitivity
of the bit at that location, in its l-bit binary encoded vector among rl concatenated binary bits.
3
Under review as a conference paper at ICLR 2022
One of the key differences between our mechanism and existing works (Sun et al., 2021; Zhao
et al., 2020b; Wang et al., 2019a; Liu et al., 2020; AraChChige et al., 2019; Lyu et al., 2020a) is the
bit-aware randomization probabilities. By introducing the bit-aware term 军,we are able to: 1)
Derive signifiCantly tighter privaCy loss and expeCted error bounds Compared with existing approaChes
(Sections 4 and 5); and 2) Adaptively control the randomization probabilities across bits, such that
bits with a stronger influence on the model utility, e.g., sign bits and integer bits, have smaller
randomization probabilities qX , and vice-versa (Section 5). These advantages are crucial to evade
the curse of privacy composition enabling us to work with complex tasks and models (Section 6).
In addition, inspired by the LabelDP (Ghazi et al., 2021), we randomize yx using the following
label-RR mechanism (Alg. 1, line 18):
(label-RRmeChanism) P(yX = yx)
二 exP(e)
PY = 1 + exp(β),
if yx = yx
qY = (1+exp(β))(C - 1)
(3)
,if yx = yχ,yχ ∈ ZC
where yjχ is one-hot encoded, and β is a parameter bounded in Theorem 3 under a privacy budget EY.
Randomizing the label yx provides a complete LDP protection to each local training sample (ex , yx).
All the perturbed training samples (e0x, yx0 ) are included in a local dataset Du0 , which will be used to
train the model fθ : Rr → RC, i.e., e； = E(Vx) where E(∙) is a decoding function, (Alg. 1, lines
20-22). The training will never access the original data Du . All other operations remain the same
with our aforementioned FL setting.
4	Privacy Guarantees of B itRand
In this section, we focus on bounding formal privacy loss of BitRand for input and label protection.
To achieve our goal, we need to bound α and β in Eqs. 2 and 3 such that our algorithm preserves LDP
for each training sample (x, yx) ∈ Du given (vx0 , yx0 ) ∈ Du0 . Note that the decoding e0x = E (vx0 ) does
not incur any extra privacy risk following the post-processing property (Dwork & Roth, 2014). Given
vx and vex can be different at any bit, for the LDP condition to hold, the ratio of two probabilities
P(f-RR(Vx)=Vz) rɪppf]e tc Hp Hohtiγ!pγ1 Hv PYnrFVI With Qf U RCrn∩ρ( f-RR)
P (f RR(e )=V ) neeus Io Ue UounLIeLI Uy exp(tχ ) vvitii Uz ∈ a Lange(J IXlX).
Given a feature a, i.e., one of the r features, let us first consider a bit i in the encoding vector va .
Intuitively, when we apply our f-RR only on the bit i in va (i.e., all the other bits remain the same),
denoted as f-RR(va, i), the l1-sensitivity of the bit i is 1. This is because, given two neighboring
vectors va and va|i that differs only at the bit i, we have that ∀i ∈ va : arg maxVa kf -RR(va, i) -
f-RR(va|i, i)k1 = 1. However, the coordinating server can infer the decoded features E (f -RR(va))
instead of just the intermediate result f-RR(va). Thus, we need to quantify the l1-sensitivity of
a single encoding bit i ∈ va to determine just how accurately we can return the decoded features
E (f -RR(va)) through our randomization f-RR applied on va. Following (Dwork & Roth, 2014), the
sensitivity ∆i of a bit i can be quatified as follows:
Definition 2. Bit l1-sensitivity. Given two neighboring vectors va and va|i that differs only at a bit
i, the sensitivity ∆i captures the magnitude by which the bit i can change the decoding function
E (f -RR (∙)) in the worst case, as follows:
∀i ∈ [0, rl - 1] : ∆i = maxkE f -RR(va) - E f -RR(va|i) k1
Va
Based on Eq. 4, sensitivities ∆i of all the bits in vx are bounded in the following lemma.
Lemma 1. The l1-sensitivity ofa single binary encoding bit is bounded as follows:
2m+1,	ifi is a sign bit
2m-i%l, ifi is an integer/fraction bit
∀i ∈ [0, rl - 1] : ∆i =
(4)
(5)
All the proofs are in Appendix. Unlike existing RR mechanisms, we incorporate the l1-sensitivity
∆i into the privacy loss bound of f-RR by ensuring that the privacy loss in randomizing a bit i is
bounded by the privacy loss in the embedded feature space. By doing so, we can derive tighter privacy
loss bounds as discussed next.
Given ∆i, there always exists a Laplace noise injected into the feature a, i.e., M(va, i) = E(va) +
Lap(∆i∕ei), to achieve Ei-LDP in the embedded feature space (DWOrk & Roth, 2014). In other
words, P(M(Va,i)=z∖ ≤ exp( "|E(Va)-E(Va|i)|) and Z ∈ Range(M). To ensure that the privacy loss
P (M(Va|i,i)=z)	∆i
4
Under review as a conference paper at ICLR 2022
in randomizing the bit i is bounded by the privacy loss in the embedded feature space is to find α in Eq.
2 such that: pP(f RRR,；?==^))≤ exp('ilE(va∆E(va|i)|). However, randomizing a binary encoding
bit i given va and va|i results in a smaller l1-distance |E (f -RR(va, i)) - E (f -RR(va|i, i))| compared
with |E(va) - E(va|i)|; since |E(f-RR(va, i)) - E(f-RR(va|i,i))| ≤ |E(va) - E(va|i)|. Thus, we
can derive a tighter bound by replacing |E(va) - E (va|i)| with |E (f -RR(va, i)) - E (f -RR(va|i, i))|.
Given va (i) is the bit i in va, we have
P (f-RR(Va ,i = Vz) = P (f-RR(Va(i)) = Vz(i)) × Y	P (Va (j) = Vzj))
Pf -RR(VaIi ,i) = Vz)	Pf -RR(Va|i(i))= vz(i))	0	P(VaIij)= Vzj))
j6=i,j ∈[0,l-1]
_ P (f -RR(Va (i))= Vz(i)) V	，ei|E (f -RR(Va,i)) -E (f -RR(Va|i, i))|
=P(f-RR(Va∣i(i))= Vz(i)) ≤exp(	∆	)	(6)
However, finding a closed-form solution of α for the tight privacy bound in Eq. 6 is non-trivial, since
6i is intractable. To address this, We consider two cases: (1) pp(f-RRvva(ii)==z (i)))≥ 1 below, and (2)
0 < PPf-RRR，： (ii)==vz (ii))< 1 in Appendix D. Since ∆i captures the magnitude by which the bit i
can change the decoding function E(∙) in the worst case, we have ∣? f -RR(V i))，E f -RR(V「办)| ≥ 1.
As a result, we have
∆
P (f-RR(Va(i)) = Vz(i)) ≤ ( P (f-RR(Va (i)) = Vz(i)) λ |E (f-RR(v： ,i))-E (f -RR(Va ∣, ,i)) |
P (f -RR(Va∣i(i)) = Vz(i)) — VP (f -RR(Va∣i(i)) = Vz(i))
≤ exp(i)
(7)
Eq. 7 enables us to quantify a generalized privacy loss bound of a RR mechanism, in which different
bits have different sensitivities by randomizing all the bits in vx independently in Theorem 1.
Theorem 1. Generalized privacy loss bound. The privacy loss for randomizing a binary encoding
vector vx is bounded as follows:
P (f -RR (vx ) = Vz)
P (f -RR (ex )= Vz)
≤ Y1 ( P(f-RR(Vx(i)) = Vz(i))
≤ U IP(f-RR(Vx∣i(i)) = vz(i))
|E(f -rr (vx,i))-£(f -rr (vχ∣i,i))1
rl-1
≤ exp(	i)
i=0
(8)
∆
Now, we enforce the condition Pir=l-01 i = X to ensure that the total privacy budget will be bounded
by X . Based upon that, we derive a closed-form solution showing that there always exists an upper
bound of α so that vx0 preserves X -LDP given vx in Theorem 2 (Alg. 1, line 17).
TheOrem 2, ∀α ： O <α ≤ √irpgXPiXi%)，
the f-RR mechanism satisfies X -LDP:
PfR(ex)=Z) < ex，where Vx and Vx Can be different at any bit, and Z ∈ Range(f-RR).
Regarding to the ground-truth label yx, given a privacy budget Y , we show that there always exists
an upper bound of β so that yx0 preserves eY -LDP given yx .
TheOrem 3, ∀β : β ≤ eY - ln(C - 1), the label-RR mechanism (Alg. 1, line 18) satisfies eY -LDP:
P(Iabel 1RR(yx)=Z|yx)	≤ exn(e、/)	oiven	anr distinct labels and τ/	and Z ∈ RanCIe(label-RR)
p(label RR(e )_z|e )	≤ exp'eγ ,,	given	any sbinct t-ac/et-s	yx ancι yx,	ancι Z ∈ ɑnge`,ɑe/e".	.
From Theorems 2 and 3, our BitRand preserves eX -LDP for the vector vx and eY -LDP for label
yx . The gradients 5θu preserve (eX , eY )-LDP in any training rounds t for any training samples
(x, yx) ∈ Du and for all clients u ∈ [1, N]; since, ∀u, t: 5θu are computed from the randomized
training samples {(vx0 , yx0 )}nu without accessing any further information from {(x, yx)}nu.
5	Privacy and Utility Trade-off
As shown in Theorem 1, we can derive a tighter pri-
vacy loss bound. Therefore, we focus on understand-
ing how BitRand can address the privacy-utility trade-
off in comparison with existing approaches by theo-
retically studying: (1) the utility of f-RR regarding
the expected error bound (TheOrem 4), and (2) the
trade-off between privacy budget and randomization
probabilities. All statistical tests are 2-tail t-tests.
Expected ErrOr BOunds, We analyze the data utility
through expected error, denoted as ξa , measuring the
Figure 3: Expected error bound comparison
for an embedded feature a with r = 1, 000,
l = 10, and m = 5.
5
Under review as a conference paper at ICLR 2022
expected change of each embedded feature a after applying f -RR: ξa = E|E (f -RR(va)) - E(va)|.
The smaller expected error is, the better data utility the randomization mechanism achieves. The
expected error ξa is bounded by Pi∈[0,l-1] qXi × ∆i.
Theorem 4. The f-RR expected error bound is quantified by ξa = E|E (f -RR(va)) - E(va)| =
Pi∈[0,l-1] qXi × ∆i.
Theorem 4 can be directly applied to quantify the expected error bounds of f-RR without the bit-
aware term i%l/l, corrected LATENT (Appendix I), and corrected OME (Appendix J). Regarding
existing mechanisms applied on the embedded feature a, including Duchi mechanism (DM) (Duchi
et al., 2013), Piecewise mechanism (PM) (Wang et al., 2019a), Hybrid mechanism (HM) (Wang
et al., 2019a), Suboptimal mechanism (PM-SUB) (Zhao et al., 2020b), Gaussian and Laplace (Dwork
& Roth, 2014), we derive a general form of expected error bounds ξa for these mechanisms as:
ξa = E|M(a) - a| u 1/r Pa∈[1,r] |M(a) - a| where M is an X -LDP preserving mechanism,
since limr→∞ E|M(a) - a| = 1/r Pa∈[1,r] |M(a) - a|.
Figure 3 illustrates the expected error bound of each algorithm as a function of X . It is obvious that
f-RR has significantly tighter expected error bounds compared with baseline approaches under a
wide range of privacy budgets given reasonable large numbers of embedded features and encoding
bits (p = 0.02); thus indicating that our mechanism achieves better data utility compared with the
baselines. The improvement of our f-RR over existing mechanisms is larger when X increases.
To profoundly study the data utility, we take a depth look into bit-level analysis by relaxing Theorem
4 into (1) an expected error bound ξi = qXi × ∆i for each bit i ∈ [0, l - 1], and (2) an average top-k
expected error bound ξtop-k = 1/k Pik=0 ξi, ∀k ∈ [0, l - 1]. Figure 11 (Appendix K) show that, at
the bit-level, f-RR achieves smaller values of ξi for most important bits, especially the sign bit and
integer bits, and comparable ξi for least important bits, i.e., fraction bits. The gap between f-RR
and the baselines are significantly larger given the ξtop-k. We obtain smaller values of ξtop-k for all
k ∈ [0, l - 1], under a tight privacy budget X = 0.1. Importantly, when increasing privacy budget
X (Figures 11b,c), the gap between f-RR and the baselines is larger. In fact, the expected error
bounds in f-RR is reduced while the expected error bounds in the baselines are remained the same.
Privacy budget and randomization probabilities trade-off. Compared with existing RR mech-
anisms (Lyu et al., 2020a; Arachchige et al., 2019; Sun et al., 2021; Zhao et al., 2020b; Duchi &
Rogers, 2019; Wang et al., 2019a; Liu et al., 2020), in our algorithm, bits with a stronger influence on
the model utility, e.g., sign bits and integer bits, have smaller randomization probabilities qX, and
vice-versa. This is because We consider the influence of each bit i through the term 军 in modeling
the randomization probabilities qX (and pX ). This unique property of our algorithm enables us to
better optimize the trade-off betWeen privacy loss and model utility.
To demonstrate this, We examine the behavior of qX across bi-
nary encoding bits under a Wide range of X ∈ {0.1, 1, 2}, given
reasonable values of r and l, i.e., r = 1, 000 and l = 10 (Fig-
ures 4 and 12, Appendix K). Our mechanism achieves a smaller
randomization probability qX than the corrected LATENT in all
cases (p = 3.7e - 9), especially more significant bits, such as
sign bits and integer bits. The gap is more prominent When X
increases. This observation is less obvious When comparing our
mechanism with the corrected OME, given an uneven randomiza-
Figure 4: Randomization proba- tion probability qX across bits in the corrected OME. To better
bility qX and qtop-k, given l = 10, show the comparison, we draw an average top-k measure curve:
r = 1, 000, and X = 1.	∀k ∈ [0, l - 1] : qtop-k = 1/k Pik=0 qi, where qi is the bit i’s qX,
to evaluate the average qX across bits. The smaller qtop-k is, the better the randomization probability
qX is. Given a tight privacy budget X = 0.1, our mechanism and the corrected OME have a similar
qtop-k. However, when X increases, i.e., X ∈ {1, 2}, our mechanism achieves significantly smaller
values of qtop-k than the corrected OME (p = 5.1e - 3).
6	Dimension-Elastic Analysis
In existing RR mechanisms, the curse of privacy composition is rooted in the privacy composition
across bits l and features r (Duchi et al., 2013; Lyu et al., 2020a; Arachchige et al., 2019), gradients
5θu (Zhao et al., 2020b; Wang et al., 2019a), and training rounds T (Zhao et al., 2020b; Wang et al.,
bit24 23 22 21 20 2~1 2~2 2~3 2~4
6
Under review as a conference paper at ICLR 2022
2019a). Thus, increasing the dimensions of l, r, 5θu , and T either significantly increases the privacy
budget (i.e., resulting in loose privacy protection) (Wang et al., 2019a; 2017) or notably affects the
randomization probabilities (i.e., the value transmitted correctly through the randomization becomes
smaller resulting in poor utility (Wang et al., 2016a)) (Lyu et al., 2020a; Arachchige et al., 2019).
In Theorem 1, the privacy composition across bits and features is unavoidable. However, the values of
embedded features transmitted correctly through our randomization mechanism is minimally affected
when increasing the dimensions of r, l, 5θu , T, and C under the same X and Y . That enable us to
evade the curse of privacy composition when working with complex models and tasks under rigorous
LDP protection. To shed light into this property, we conduct a theoretical analysis to examine: (1)
How the dimensions of r, l, gradients 5θu , and training rounds T impact the privacy budget X and
the randomization probabilities qX and pX (= 1 - qX); and (2) How the number of model outcomes
C impacts the privacy budget eγ and the randomization probabilities qγ and PY. We select the upper
bounds of a = √(eχ + ri)∕(2r Pi=0 eχp(2∈χ l)) and β = eγ - ln(C - 1) (Theorems 2, 3) in our analysis.
Dimensions of gradients 5θu and training rounds T . In Bi-
tRand, the clients only use the perturbed samples (vx0 , yx0 ) to train
their local models without accessing any further information from
(x, yx ). Thus, the privacy budgets X and Y are independent of
the dimension of gradients 5θu and the training rounds T , i.e.,
following the post-processing property in DP (Dwork & Roth,
2014). Also, the size of 5θu and T do not affect the random-
ization probabilities, since 5θu and T are not used to model qX,
pX, qY, and pY as in Eqs. 2 and 3.
Dimensions of embedded features r and encoding bits l.
Varying the dimensions of r and l does not affect the privacy
budget X ∈ R+ , since there always exists an α for Theorem 2
Figure 5: Randomization proba-
bility qX as a function of r with
fixed l = 10 and X = 1.
to hold. However, it is necessary to understand the influence of varying r and l on the privacy-utility
trade-off. We theoretically analyze the impacts of r and l on the randomization probabilities qX and
pX , given fixed values of X . A model is expected to achieve higher model utility given smaller
values of qX (higher values of pX) under the same privacy budget. We examine qX and pX in the
following experiments: (i) Fixing l, then varying r ; and (ii) Fixing r, then varying l.
Figures 5 and 13 (Appendix K) illustrate qX as a function of r,
under l ∈ {5, 20, 100, 1, 000} and X ∈ {0.1, 1, 2}. Varying r
does not affect the randomization probability qX (and pX ) for
all the bits in vx . To explain this, we take a deeper look into the
a's bound, which is the only factor affecting qχ and PX. Given
fixed eχ and l, P=0 exp(2eχ/) is a constant, denoted C. So,
we have α = PQX，rl)∕(2rC) = p(手 + l)∕(2C) U √l∕(2C),
since * U 0.0 in practice. Thus, with fixed eχ and l, qχ and
PX are r-elastic, given ɑ approximately is a constant ,l∕(2C).
Now, we fix r = 1, 000, which is a decent number of embedded
features, and show qX as a function of l under a wide range
of r ∈ [10, 10, 000] and X ∈ {0.1, 1, 2} (Figures 6 and 14,
Figure 6: Randomization proba-
bility qX as a function of l with
fixed r = 1, 000 and X = 1.
Appendix K). Given a tight privacy budget X = 0.1, varying l does not affect the randomization
probability qX, i.e., m = bl/2c in our analysis covering most values of embedded features in practice.
However, with higher values of X ∈ {1.0, 2.0}, using more encoding bits l lowers qX for most
important bits, i.e., sign and integer bits; while increasing qX for least important bits, i.e., fraction
bits. When l is large enough (l ≥ 20), the randomization probability qX is l-elastic since the impact
of l becomes marginal to all the bits. This is also true for PX .
Number of model outcomes C. Similar to X , from Eq. 3 and Theorem 3, the privacy budget
Y is not directly affected by the number of model outcomes C, since ∀C, X : ∃β for Theorem
3 to hold. However, C may impact the trade-off between privacy and model utility by affecting
the randomization probabilities qY and PY . Figure 7 shows that, when C is sufficiently large, i.e.,
C ≥ 100, the randomization probability qY is C-elastic since the impact of C on qY is marginal
given a wide range of the privacy budget Y . This is also true for PY . When Y increases, the
randomization probability qY becomes smaller (PY becomes larger). This is a reasonable observation.
7
Under review as a conference paper at ICLR 2022
Thanks to the bit-aware and dimension-elastic properties, our BitRand can achieve high data utility,
especially under tight privacy budgets and expected error bounds.
7	Experimental Results
We have conducted an extensive experiment on benchmark
datasets under two fundamental FL tasks, text and image classi-
fication, to shed light on understanding 1) the interplay among
privacy budget and model utility in BitRand, 2) the effectiveness
of the dimension-elastic and bit-aware properties on model utility,
and 3) different settings of applying RR to preserve LDP.
Baseline Approaches. We consider a variety of LDP-preserving
mechanisms as baseline approaches: (1) Binary encoding ap-
Iof
5	10	100	500	1,000	2,000
C
Figure 7: RandomizationProba-
bility qY with varying Y and C .
Proaches, including the corrected LATENT (Arachchige et al., 2019) and the corrected OME (Lyu
et al., 2020a) (APPendices I and J); (2) LDP-FL (Sun et al., 2021); (3) Duchi mechanism (DM)
(Duchi et al., 2013); (4) Piecewise mechanism (PM) (Wang et al., 2019a); (5) Hybrid mechanism
(HM) (Wang et al., 2019a); (6) Three-Outputs mechanism (Zhao et al., 2020b); (7) SuboPtimal
mechanism (PM-SUB) (Zhao et al., 2020b); and (8) Label-Laplace (Phan et al., 2020). Each base-
line aPProach is aPPlied to randomize (when aPPlicable): (i) Embedded features ex ; (ii) Gradients
5θu ; and (iii) Gradients 5θu with a recent anonymizer (Sun et al., 2021) to reduce the Privacy budget
consumPtion. In addition, Label-LaPlace is used as a baseline to Protect the ground-truth labels
yx . Note that, in our exPeriment, we use the uPPer bound of β resulting in the same randomizing
Probabilities as in LabelDP. Therefore, we do not include LabelDP in comParison. More details
about the difference of label-RR and LabelDP are in APPendix G. These settings are widely accePted
to Preserve LDP in FL; thus, offering a comPrehensive view of Preserving LDP in FL. Note that
LATENT, OME, and BitRand can only be aPPlied on embedded features ex ; while LDP-FL can only
be aPPlied on gradients 5θu with and without the anonymizer (Sun et al., 2021). We include the
Noiseless FL model trained on the original data Du to show uPPer-bounds and a Random guess
model to understanding model utility better.
Datasets, Metrics, and Model Configuration. The comPlete details of the datasets, metrics, and
model configuration are in APPendix L. We carried out our exPeriments on two textual datasets and
two image datasets, including the AG dataset (Gulli et al., 2012), our collected Security and Exchange
Commission (SEC) financial contract dataset, the large-scale celebFaces attributes (CelebA) dataset
(Liu et al., 2015), and the Federated Extended MNIST (FEMNIST) dataset (Caldas et al., 2018). We
use the test accuracy and the test area under the curve (AUC) as evaluation metrics. Models with
higher values of test accuracy and AUC are better. We use the BERT-Base (Uncased) Pre-trained
model (ber; Devlin et al., 2018) to extract embedded features in the AG and SEC datasets. In the
CelebA and FEMNIST datasets, we use the ResNet-18 Pre-trained model (img; He et al., 2016). For
text and image classification tasks, we use two fully connected layers on toP of embedded features,
each of which consists of 1, 500 hidden neurons and uses a ReLU activation function.
Evaluation Results. ComPrehensive results show that BitRand offers stronger Privacy Protection
with higher model utility, comPared with all baseline aPProaches, as discussed next.
LDP-preserving approaches applied on the embedded features ex. Baseline aPProaches do not
work well when they are aPPlied on embedded features ex (Figures 15 and 19, Table 3, APPendix L).
In SEC, AG, and FEMNIST datasets, BitRand achieves the highest model utility comPared with the
best baseline aPProach, which is the corrected OME, under a tight Privacy budget X = 1. In terms
of accuracy and AUC values, BitRand (Y = ∞) achieves an imProvement of 46.03% and 38.51% in
the AG dataset (p = 2.7e - 22), 13.69% and 13.79% in the SEC dataset (p = 4.1e - 12), and 21.62%
and 13.42% in the FEMNIST dataset (p = 5.6e - 11) resPectively. In the CelebA dataset (Table
3, APPendix L), BitRand outPerforms the best baseline aPProach, i.e., PM-SUB, with an average
imProvement of 1.66% across all 40 attributes in terms of AUC measure (p = 1.2e - 2). Since the
CelebA dataset is highly imbalanced, we use the AUC measure instead of the model accuracy. The
gaPs between BitRand and the baseline aPProaches are significantly wider when the Privacy budget
X is larger. In addition to X = 1, with a tight Privacy budget for the class labels Y ∈ {1, 2.5},
BitRand still outPerforms baseline aPProaches in most of cases, offering stronger Privacy Protection
with better model utility, i.e., LDP at both embedded feature and label levels instead of only LDP on
the embedded features as in baseline aPProaches.
8
Under review as a conference paper at ICLR 2022
DM [vgt & Anon]
PM [φg, &Anon]
HM [竭 & Anon]
Three-outputs [v⅛ & Anon]
PM-SUB [V⅛ & Anon]
LDP-FL [vgt & Anonl
—	∙- corrected LATENT [e3,]
—	corrected OME [ex]
BitRand [ex] (εr= ≡>)
BitRand [ex] [εγ= 1.0)
123456789 10
εχ
(b) FEMNIST
*	*- BitRand [e»] (εr= 2.5)
-	T- BitRand [ex] (fr= 5.0)
BitRand [ex] (fr=10.0)
Random ―•— Noiseless
Figure 8: AUC values of each algorithm applied on the gradients 5θu with the anonymizer.
The key reason is that, in the baseline approaches, the model utility is significantly affected by the
size of the embedded features. Thanks to the dimension-elastic and bit-aware properties, BitRand can
achieve high model accuracy and AUC values, especially under tight privacy budgets. In addition,
BitRand achieves the highest improvement in the AG dataset, since it is a balanced dataset compared
with the highly imbalanced CelebA dataset, in which BitRand achieves the least improvement.
Addressing imbalanced data in FL under DP (Huang et al., 2020) is out-of-scope of our study.
LDP-preserving approaches applied on on gradients 5θu without and with the anonymizer
(Sun et al., 2021). We observe the same phenomenon when baseline approaches are applied on
gradients 5θu without and with the anonymizer (Sun et al., 2021), even though the gaps between
BitRand and baseline approaches are (marginally) smaller (Figures 8, 16-17, 20, and Tables 4, 5,
Appendix L). Without using the anonymizer in the baseline approaches, in terms of accuracy and
AUC, compared with the best baseline approach PM-SUB, BitRand (X = 1, Y = 1) achieves an
improvement of 44.95% and 37.52% in the AG dataset (p = 3.9e - 20), 12.82% and 12.92% in the
SEC dataset (p = 1.3e - 11), 24.17% and 13.40% in the FEMNIST dataset (p = 4.1e - 11). When
the anonymizer is applied in the baseline approaches, in terms of accuracy and AUC, compared with
the best baseline approache HM, our BitRand achieves an improvement of 39.38% and 32.75% in the
AG dataset (p = 1.2e - 16), 13.59% and 13.69% in the SEC dataset (p = 2.1e - 10), and 23.12%
and 37.02% in the FEMNIST dataset (p = 2.8e - 11). Regarding the CelebA dataset, BitRand
outperforms the best baseline approaches, which are Three-outputs and HM, with improvements of
1.28% and 0.3%, in the cases of with and without the anonymizer, across all 40 attributes in terms of
AUC (p = 3.1e - 2). The model utility in baselines is affected by the size of gradients and the training
rounds (when the anonymizer is not applied) and their finite numbers of randomization outputs of the
gradients (Zhao et al., 2020b). Thanks to the bit-aware and dimension-agnostic properties, BitRand
can achieve high model utility under rigorous LDP protection.
LDP-preserving labels. Figures 18, 21, and Table 3 (Appendix L) present the model utility of
BitRand as a function of the privacy budgets X and Y , in which label-RR is replaced by the
Label-Laplace, denoted as f-RR & Label-Laplace. label-RR outperforms the Label-Laplace in all
values of X, Y , and datasets. Under rigorous LDP protection Y = 1 given X ∈ [1, 10], there is
an average improvement of 9.65% accuracy and 7.92% AUC in the AG dataset (7.3e - 6), 7.84%
accuracy and 7.35% AUC in the SEC dataset (1.2e - 5), and 9.91% accuracy and 10.55% AUC in
the FEMNIST dataset (p = 9.8e - 5), and 2.04% AUC in the CelebA dataset (p = 1.6e - 2). The
gaps are delicately smaller in the AG and SEC datasets, and substantially larger in the FEMNIST and
CelebA datasets when Y is increased. Given Y = 2.5, there is an average improvement of 1.71%
accuracy and 1.37% AUC in the AG dataset (8.5e - 2), 4.65% accuracy and 4.73% AUC in the SEC
dataset (6.9e - 3), and 14.43% accuracy and 12.02% AUC in the FEMNIST dataset (p = 1.9e - 4),
and 4.24% AUC in the CelebA dataset (p = 2.3e - 2). The reason is that Label-Laplace injects
Laplace noise across C classes in a label yx causing more noisy model outcomes compared with
label-RR, in which only one of the C model outcomes is selected as the result of the randomization.
8 Conclusion
In this paper, we introduced a bit-aware algorithm, called BitRand, providing rigorous LDP protection
to both embedded features and labels in FL via binary encoding. In BitRand, the trade-off between
the privacy budget consumption and randomization probabilities is dimension-elastic to the numbers
of embedded features, encoding bits, gradients, model outcomes, and training rounds, enabling us
to work with complex models and FL tasks. We further optimize the randomization probabilities
by having smaller randomization probabilities assigned to more critical bits and vice-versa under
the same privacy budgets. Theoretical analysis and extensive experiments showed that our BitRand
outperforms baseline approaches in text and image classification.
9
Under review as a conference paper at ICLR 2022
References
Cometomyhead academic news search engine. http://newsengine.di.unipi.it.
Google ai. pre-trained bert model. https://bert-as-service.readthedocs.io/en/
latest/section/get-start.html#installation.
Pre-trained resnet-18 model. https://github.com/christiansafka/img2vec.
Induction proofs. https://www.purplemath.com/modules/inductn3.htm.
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep
learning with differential privacy. In ACM SIGSAC Conference on Computer and Communications
Security,pp. 308-318,2016.
J. Acharya, Z. Sun, and H. Zhang. Hadamard response: Estimating distributions privately, efficiently,
and with little communication. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1120-1129, 2019.
M. Alvim, K. Chatzikokolakis, C. Palamidessi, and A. Pazii. Local differential privacy on metric
spaces: optimizing the trade-off with utility. In 2018 IEEE 31st Computer Security Foundations
Symposium, pp. 262-267, 2018.
P. C. M. Arachchige, P. Bertok, I. Khalil, D. Liu, S. Camtepe, and M. Atiquzzaman. Local differential
privacy for deep learning. IEEE Internet of Things Journal, 7(7):5827-5842, 2019.
B. Balle, J. Bell, A. Gascon, and K. Nissim. The privacy blanket of the shuffle model. In Annual
International Cryptology Conference, pp. 638-667, 2019.
R. Bassily and A. Smith. Local, private, efficient protocols for succinct histograms. In ACM
Symposium on Theory of Computing, pp. 127-135, 2015.
R.	Bassily, K. Nissim, U. Stemmer, and A. Thakurta. Practical locally private heavy hitters. arXiv
preprint arXiv:1707.04982, 2017.
Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protec-
tion against reconstruction and its applications in private federated learning. arXiv preprint
arXiv:1812.00984, 2018.
Robert Istvan Busa-Fekete, Umar Syed, Sergei Vassilvitskii, et al. On the pitfalls of label differential
privacy. In NeurIPS 2021 Workshop LatinX in AI, 2021.
S.	Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Konecny, H. B. McMahan, V. Smith, and A. Talwalkar.
Leaf: A benchmark for federated settings. arXiv preprint:1812.01097, 2018.
N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown,
D. Song, U. Erlingsson, et al. Extracting training data from large language models. arXiv preprint
arXiv:2012.07805, 2020.
A. Cheu, A. Smith, J. Ullman, D. Zeber, and M. Zhilyaev. Distributed differential privacy via
shuffling. In EUROCRYPT, pp. 375-403, 2019.
G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. EMNIST: Extending MNIST to handwritten
letters. In International Joint Conference on Neural Networks, pp. 2921-2926, 2017.
G. Cormode, T. Kulkarni, and D. Srivastava. Marginal release under local differential privacy. In
ACM SIGMOD, pp. 131-146, 2018.
J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
J. Dong, A. Roth, and W. J. Su. Gaussian differential privacy. arXiv preprint arXiv:1905.02383,
2019.
J. Duchi and R.n Rogers. Lower bounds for locally private estimation via communication complexity.
In COLT, pp. 1161-1191, 2019.
10
Under review as a conference paper at ICLR 2022
J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and statistical minimax rates. In IEEE
Protocolsfor secure computations, pp. 429-438, 2013.
J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Minimax optimal procedures for locally private
estimation. Journal of the American Statistical Association, 113(521):182-201, 2018.
C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Found. Trends Theor.
Comput. Sci., 9(3&#8211;4):211-407, 2014. ISSN 1551-305X.
U. Erlingsson, V. Pihur, and A. Korolova. Rappor: Randomized aggregatable privacy-preserving
ordinal response. In Proceedings of the 2014 ACM SIGSAC CCS, pp. 1054-1067, 2014.
U. Erlingsson, V. Feldman, I. Mironov, A. Raghunathan, K. Talwar, and A. Thakurta. Amplification
by shuffling: From local to central differential privacy via anonymity. In ACM-SIAM SODA, pp.
2468-2479, 2019.
G. Fanti, V. Pihur, and UL Erlingsson. Building a rappor with the unknown: Privacy-preserving
learning of associations and data dictionaries. arXiv preprint arXiv:1503.01214, 2015.
M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information
and basic countermeasures. In ACM SIGSAC CCS, pp. 1322-1333, 2015.
GDPR. The european data protection regulation. https://gdpr-info.eu/, 2018.
R. C. Geyer, T. Klein, and M. Nabi. Differentially private federated learning: A client level perspective.
arXiv preprint arXiv:1712.07557, 2017.
B. Ghazi, N. Golowich, R. Kumar, P. Manurangsi, and C. Zhang. On deep learning with label
differential privacy. arXiv preprint arXiv:2102.06062, 2021.
M. E. Gursoy, A. Tamersoy, S. Truex, W. Wei, and L. Liu. Secure and utility-aware data collection with
condensed local differential privacy. IEEE Transactions on Dependable and Secure Computing,
2019.
A. Haeberlen, B. C. Pierce, and A. Narayan. Differential privacy under fire. In USENIX Security
Symposium, volume 33, 2011.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
X. Huang, Y. Ding, Z. L. Jiang, S. Qi, X. Wang, and Q. Liao. DP-FL: a novel differentially private
federated learning framework for the unbalanced data. World Wide Web, 23(4):2529-2545, 2020.
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. arXiv
preprint arXiv:1912.04977, 2019.
S. Kim, H. Shin, C. Baek, S. Kim, and J. Shin. Learning new words from keystroke data with local
differential privacy. IEEE TKDE, pp. 479-491, 2018.
Ruixuan L., Yang C., Hong C., Ruoyang G., and Masatoshi Y. FLAME: differentially private
federated learning in the shuffle model. CoRR, abs/2009.08063, 2020.
R. Liu, Y. Cao, M. Yoshikawa, and H. Chen. Fedsel: Federated sgd under local differential privacy
with top-k dimension selection. In International Conference on Database Systems for Advanced
Applications, pp. 485-501, 2020.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In ICCV, December
2015.
L. Lyu, Y. Li, X. He, and T. Xiao. Towards differentially private text representations. In Proceedings
of the 43rd International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 1813-1816, 2020a.
11
Under review as a conference paper at ICLR 2022
L.	Lyu, H. Yu, and Q. Yang. Threats to federated learning: A survey. arXiv preprint arXiv:2003.02133,
2020b.
M.	Malekzadeh, B. Hasircioglu, N. Mital, K. Katarya, M. E. Ozfatura, and D. Gunduz. Dopamine:
Differentially private federated learning on medical data. arXiv preprint arXiv:2101.11693, 2021.
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient
learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pp.
1273-1282, 2017.
Ilya Mironov. On significance of the least significant bits for differential privacy. In Proceedings of
the 2012 ACM conference on Computer and communications security, pp. 650-661, 2012.
N.	H. Phan, M. T. Thai, H. Hu, R. Jin, T. Sun, and D. Dou. Scalable differential privacy with certified
robustness in adversarial learning. In ICML, pp. 7683-7694, 2020.
Protection Regulation. General data protection regulation. Intouch, 2018.
X. Ren, C. M. Yu, W. Yu, S. Yang, X. Yang, J. A. McCann, and S. Y. Philip. Lopub: High-dimensional
crowdsourced data publication with local differential privacy. IEEE Transactions on Information
Forensics and Security, 13(9):2151-2166, 2018.
H. Shin, S. Kim, J. Shin, and X. Xiao. Privacy enhanced matrix factorization for recommendation
with local differential privacy. IEEE Transactions on Knowledge and Data Engineering, 30(9):
1770-1782, 2018.
C. Song and A. Raghunathan. Information leakage in embedding models. In ACM SIGSAC CCS, pp.
377-390, 2020.
L. Sun, J. Qian, and X. Chen. LDP-FL: Practical private aggregation in federated learning with local
differential privacy. IJCAI, 2021.
Cybersecurity Law.	Cybersecurity law of the people’s republic of china. https:
//en.wikipedia.org/wiki/Cybersecurity_Law_of_the_People%27s_
Republic_of_China, 2016.
GullietaL Ag's corpus of news articles. http://groups.di.unipi.it/~gulli/AG_
corpus_of_news_articles.html, 2012.
S. Truex, N. Baracaldo, A. Anwar, T. Steinke, H. Ludwig, R. Zhang, and Y. Zhou. A hybrid approach
to privacy-preserving federated learning. In Proceedings of the 12th ACM Workshop on Artificial
Intelligence and Security, pp. 1-11, 2019.
S. Wagh, X. He, A. Machanavajjhala, and P. Mittal. Dp-cryptography: marrying differential privacy
and cryptography in emerging applications. Communications of the ACM, 64(2):84-93, 2021.
D. Wang and J. Xu. On sparse linear regression in the local differential privacy model. In ICML, pp.
6628-6637, 2019.
N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, and G. Yu. Collecting and analyzing
multidimensional data with local differential privacy. In IEEE ICDE, pp. 638-649, 2019a.
S.	Wang, L. Huang, P. Wang, H. Deng, H. Xu, and W. Yang. Private weighted histogram aggregation
in crowdsourcing. In International Conference on Wireless Algorithms, Systems, and Applications,
pp. 250-261, 2016a.
S.	Wang, L. Huang, P. Wang, Y. Nie, H. Xu, W. Yang, X. Li, and C. Qiao. Mutual information
optimally local private discrete distribution estimation. arXiv preprint:1607.08025, 2016b.
T.	Wang, J. Blocki, N. Li, and S. Jha. Locally differentially private protocols for frequency estimation.
In 26th USENIX Security Symposium, pp. 729-745, 2017.
T. Wang, B. Ding, M. Xu, Z. Huang, C. Hong, J. Zhou, N. Li, and S. Jha. Improving utility and
security of the shuffler-based differential privacy. arXiv preprint arXiv:1908.11515, 2019b.
12
Under review as a conference paper at ICLR 2022
T. Wang, B. Ding, M. Xu, et al. Murs: practical and robust privacy amplification with multi-party
differential privacy. In ACSAC, 2019c.
S. L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal
OftheAmerican Statistical Association, 60(309):63-69,1965.
X. Xiong, S. Liu, D. Li, J. Wang, and X. Niu. Locally differentially private continuous location
sharing with randomized response. International Journal of Distributed Sensor Networks, 15(8):
1550147719870379, 2019.
C. Xu, J. Ren, L. She, Y. Zhang, Z. Qin, and K. Ren. Edgesanitizer: Locally differentially private deep
inference at the edge for mobile data analytics. IEEE Internet of Things Journal, 6(3):5140-5151,
2019.
Hongxu Y., Arun M., Arash V., Jose M. A., Jan K., and Pavlo M. See through gradients: Image batch
recovery via gradinversion, 2021.
Q. Yang, Y. Liu, T. Chen, and Y. Tong. Federated machine learning: Concept and applications. ACM
Transactions on Intelligent Systems and Technology, 10(2):1-19, 2019.
B. Zhao, K. R. Mopuri, and H. Bilen. idlg: Improved deep leakage from gradients. arXiv preprint
arXiv:2001.02610, 2020a.
X. Zhao, Y. Li, Y. Yuan, X. Bi, and G. Wang. Ldpart: effective location-record data publication via
local differential privacy. IEEE Access, 2019.
Y. Zhao, J. Zhao, M. Yang, T. Wang, N. Wang, L. Lyu, D. Niyato, and K. Y. Lam. Local differential
privacy based federated learning for internet of things. IEEE Internet of Things Journal, 2020b.
K.	Zheng, W. Mou, and L. Wang. Collect at once, use effectively: Making non-interactive locally
private learning possible. In ICML, pp. 4130-4139, 2017.
Q. Zheng, S. Chen, Q. Long, and W. Su. Federated f-differential privacy. In International Conference
on Artificial Intelligence and Statistics, volume 130, pp. 2251-2259, 2021.
L.	Zhu, Z. Liu, and S. Han. Deep leakage from gradients. In NeurIPS, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
60a6c4002cc7b29142def8871531281a- Paper.pdf.
13
Under review as a conference paper at ICLR 2022
Appendix
A Revisiting Randomized Response Mechanisms for LDP
To preserve LDP given the client’s input x, we can apply existing RR mechanisms (Wang et al.,
2016b; Fanti et al., 2015; Bassily et al., 2017; Kim et al., 2018; Ren et al., 2018; Zheng et al., 2017;
Wang &Xu, 2019; Zhao et al., 2019; Gursoy et al., 2019; Alvim et al., 2018; Xiong et al., 2019),
such as unary encoding-based approaches (Wang et al., 2017; Erlingsson et al., 2014), hash-based
approaches (Wang et al., 2017; Bassily & Smith, 2015; Acharya et al., 2019; Wang et al., 2019c),
binary encoding-based approaches (Arachchige et al., 2019; Lyu et al., 2020a), etc. For instance,
hash-based approaches such as those of Google RAPPOR (Erlingsson et al., 2014) and OLH (Wang
et al., 2017) hash the client’s input x onto a bloom filter B of size k using h hash functions. Then, for
each client’s input x and a bit i ∈ B, RAPPOR creates a perturbed binary value Bi0 from Bi with the
following randomization probability:
{1,	with probability p/2
0,	with probability p/2	(9)
Bi , with probability 1 - p
where p is a hyper-parameter. This B0 is reused as the basis for all future analysis, learning, and
reports on this distinct input x. This approach achieves X -LDP, where EX = 2hln((1 - P)/p),
given that the sensitivity of every bit Bi is ∆Bi = 1 (Erlingsson et al., 2014).
To deal with numerical inputs, e.g., embedded features, generalized RR mechanisms such as Duchi
(Duchi & Rogers, 2019; Bhowmick et al., 2018), Piece-wise (Wang et al., 2019a), Hybrid (Wang
et al., 2019a), Three-outputs (Zhao et al., 2020b), Suboptimal (Zhao et al., 2020b), LDP-FL (Sun
et al., 2021), LATENT (Arachchige et al., 2019), and OME (Lyu et al., 2020a) can be applied.
Asymmetric version of RAPPOR (e.g., (Wang et al., 2017)) designs different randomization prob-
abilities for different inputs. The technique is well-applied in the context of frequency estimation
and successfully reduce the communication cost from O(d) to O(logn) (d is data dimension and n
is the number of samples). However, simply applying the mechanism (Wang et al., 2017) does not
optimize the model utility and the privacy-utility trade-off when working with machine learning or
deep learning models.
Another line of work in LDP is Mironov (2012), which addresses the floating-point arithmetic in
implementation of DP applications. The inconsistency between mathematical abstraction of Laplace
mechanism with sampling “uniform” floating-point numbers can be exploited to carry out privacy
attacks. Floating-point arithmetic is a leaky abstraction, which is ubiquitous in computer systems
and is difficult to argue about formally and hard to get right in applications, including all the RR
mechanisms.
However, different from the asymmetric version of RAPPOR and the floating-point arithmetic, our
proposed f-RR mechanism focuses on mitigating the privacy-utility trade-off. To achieve that,
besides the asymmetric nature of the randomization probabilities, our designed f-RR consists of
two key components: 1) The bit-aware term i%l/l, which indicates the location of the bit i in each
embedded feature associated with the sensitivity of the bit at that location; and 2) The adjustable
but bounded α, which takes into account the correlation between privacy loss and the sensitivity of
embedded features to mitigate the privacy-utility trade-off and the curse of privacy composition.
The bit-aware property refers to the bits with a more substantial influence on the model utility
have smaller randomization probabilities, and vice-versa, under the same privacy protection. By
incorporating sensitivities of binary encoding bits into a generalized privacy loss bound, we show
that increasing the dimensions of embedded features r, encoding bits l, and model outcomes C
marginally affect the randomization probabilities in BitRand under the same privacy budget. This
dimension-elastic property is crucial to mitigate the curse of privacy composition by retaining a high
value of data transmitted correctly through our randomization given large dimensions of r, l, and C .
Besides the f-RR for protecting the data, we also include the label-RR for protecting the label in our
proposed BitRand mechanism, that provides a complete protection for every data sample.
14
Under review as a conference paper at ICLR 2022
B BitRand Algorithm Pseudo-Code
1:	Input: Privacy budget X and Y , number of
training iterations T, learning rate ηt, binary
encoding parameters (l and m)
2:	At server side:
3:	Initialize model θ0
4:	Send the pre-trained model fpre to clients
5:	for t ∈ T do
6:	Distribute model parameter θt to each client
7:	for each client u do
8：	5Ut - CHent-Update(θt)
9:	end for
10：	θt+1 =θt-ηt
Pu∈[1,N] 5Ut /N
11： end for
12： Output: (X, Y )-LDP θ
13： At client side u ∈ [1, N]:
14： for each data sample (x, yx) ∈ Du do
15： Extracting embedded features: eχ — fPre(X)
16：	Vx — BinaryEncoding(eχ) # using Eq. 1
17：	Randomizing vx: Vx — f-RR(Vx) With
α = . / ι eχ +rl	. # using Eq. 2
V 2r Pi=1 exp(2f)	S H
18:	Randomizing the label yx: yx — label-RR(yx)
With β = Y - ln(C - 1) # using Eq. 3
19： end for
20:	Client-Update(θt):
21:	5ut = P(Vx,yx)∈Du 5θtL(f (e'x),y'x)/nu
22： return 5θut
Algorithm 1: BitRand Algorithm in Federated Learning
C Proof of Lemma 1
Proof. Without loss of generality, let us study the l1-sensitivity of binary encoding bits of a feature’s
value a in ex. It is obvious that f -RR(va, i) and f-RR(va|i, i) differs at the bit i in the Worst case.
The decoded feature of f-RR(va, i) is a0 = E (f -RR(va, i)).
Let us denote b0b1 . . . bl-1 as the binary representation f-RR(va, i) of a0, Where b0 is the value of
a sign bit (i.e., b0 = 1 if a0 >= 0 and b0 = 0 if a0 < 0), and {bi}li-=11 is the value of integer bits
and fraction bits. We have a decoding function： E (f-RR(va, i)) = (2b0 - 1) Pli-=11 bi × 2m-i. This
decoding function is also applicable to E(f-RR(va|i, i)). Let us denote {b0i}li-=10 as the value of the
bit i in f -RR(va|i, i). FolloWing Eq. 4, the l1-sensitivity of the bit bi is computed as folloWs：
•	If i is an integer or a fraction bit (i ∈ [1, l - 1]), We have： ∆i = maxva	k
E(f-RR(Va,i))-E(f-RR(Va∣i,i)) kι=maxva k(2b0-1)(pj=1,j=i b2m-j+bi2m-i-(2b0-
1) Plj-=11,j6=i bj 2m-j +b0i2m-ik1 = maxva k(2b0 - 1)(bi -b0i)2m-ik1 ≤ maxva |(2b0 -
1)|kbi	-	b0ik12m-i	. The	∆i	is maximized When	bi	and	b0i	are different, and	|2b0	- 1| =	1.
Therefore, ∆i = 2m-i .
•	If i is the sign bit (i =	0), We have： ∆i = maxva k E (f -RR(Va, 0)) -
E(f-RR(Va|i,0)) kι= maxvak(2bo - 1) P；=1 bi2m-i - (2b0 — 1) Pi-I bi2m-ikι =
maxva k(2b0 - 2b00) Pli-=11 bi2m-ik1 ≤ maxva |2b0 - 2b00|k Pli-=11 bi2m-ik1. The ∆i is maxi-
mized When b0 and b00 are different and all {bi}li-=11 = 1. Then, ∆i ≤ maxva|0 2 Pli-=11 2m-i . Since
21 + . . . + 2l-2 = 2l-1 - 2 (mat), We have： 2Pli-=11 2m-i = 2m+1-(l-1)(2l-1 - 1) < 2m+1. As
a result, ∆i = 2m+1.
From the aforementioned ∆i of a feature’s value in ex, it is easily expanded to the entire ex. Since ex
is the concatenation of all l bits of r values in ex, all bits With the same value of i%l have the same
l1-sensitivity across rl bits. Therefore, the ∆i of the sign bit and the integer/fraction bits are 2m+1
and 2m-i%l, respectively. Consequently, Lemma 1 hold.	□
D Proof of Theorem 1
Let us consider a bit i belonging to a feature a, i.e., one of the r features. We denote f-RR(Va, i)
as vector Va With only the bit i randomized by our f -RR, i.e., all the bits different from i are kept
the same in Va ; that is, We only protect the bit i When using the notation f-RR(Va, i). Given the
l1 -sensitivity ∆i, there alWays exists a Laplace noise injected into the embedded feature a, i.e.,
15
Under review as a conference paper at ICLR 2022
M(va, i) = E(Va) + Lap(∆i/∈i), to achieve Ei-LDP in the embedded feature space (DWork & Roth,
2014). In other words, P((M(V：；?)=) ≤ exp("IE(Va∆E(VaIi)I) where va|i is the vector that differs
from va only at the bit i and z ∈ Range(M).
To ensure that the privacy loss in randomizing the bit i, we need to bound α in Eq. 2 such that
j¾fRRS:?=Vz))≤ exp( EE(Va)-E(VaIi)I). However, randomizing a binary encoding bit i given
va and vaIi results in a smaller l1-distance |E (f -RR(va, i)) - E (f -RR(vaIi, i))| compared with
|E(va) - E(vaIi)|; since |E (f -RR(va, i)) - E (f -RR(vaIi, i))| ≤ |E(va) - E(vaIi)|. Thus, we can
derive a tighter bound by replacing |E(va) - E(vaIi)| with |E (f -RR(va, i)) - E (f -RR(vaIi, i))|.
Given va(i) is the bit i in va, we have
P (f -RR(Va,i) = Vz ) = P (f -RR(Va(i)) = Vz(i))	Y	P (Vaj) = Vzj))
P (f -RR(Va∣i, i) = Vz )	P (f -RR(VaIi (i)) = Vz(i)) * .,. U i P (VaIij ) = Vzj))
j 6=i,j∈[0,l-1]
P(f-RR(Va(i)) = Vz(i)) ,	Ei|E(f-RR(Va,i))-E(f-RR(VaIi, i))|
P(f-RR(VaIi(i)) = Vz(i)) ≤ exp(	∆
(10)
However, finding a closed-form solution of α for the tight privacy bound in Eq. 10 is non-trivial,
since ei is intractable. To address this problem, we consider two cases: (1) Pf -RR(Va(Z))=：(i)) ≥ 1
P (f -RR(VaIi (i))=Vz (i))
and (2) 0 < P(f -RR(Va(*”(；)) < 1. Also, since ∆i captures the magnitude by which a bit i can
P (f -RR(VaIi (i))=Vz (i))	,	i
change the decoding function E() in the worst case, we have 卢f-RR(V i))∆Ef -RR(Vl ∙ i))I ≥ 1.
In the first case, the privacy loss for a bit i can be bounded as follows:
P(f -RR(Va(i))= Vz (i)) ≤ Z P(f -RR(Va(i))= Vz (i)) ʌ
|E(f-RR(Va,i))-E(f-RR(Va ∣i,i)) |
P(f-RR(Va|i(i))= Vz(i)) ≤ IP(f-RR(Va|i(i))= Vz(i)))
≤ exp(i)
(11)
Since the RR mechanism is independently applied on each bit i in Va and on every Va of the r features,
Eq. 11 enables us to quantify a generalized privacy loss bound of a RR mechanism, in which different
bits have different sensitivities to the randomized outcome as follows.
P(f-RR(Vx) = Vz) _ rY1 P(f-RR(Vχ(i)) = Vz(i))
P(f-RR(eχ)= Vz) = L P(f-RR(VxIi(i)) = Vz(i))
rl-1
≤Y
i=0
(P(f-RR(Vx(i)) = Vz(i)))
1P(f -RR(VxIi(i)) = Vz(i)) J
_____________δw_____________
IEf-RR(Vx ,i))-Ef-RR(Vx ∣i,iD |
rl-1	rl-1
≤	exp(Ei) = exp(	Ei)
i=0	i=0
(12)
In the second case, the privacy loss for a bit i can be bounded as follows:
P(f-RR(Va(i))= Vz(i)) ≤ P(f-RR(VaIi(i))= Vz(i))
P (f -RR(VaIi (i)) = Vz (i)) — P (f -RR(Va(i)) = Vz (i))
P (f -RR(VaIi(i)) =Vz (i)) λ ∣E(f-RR(va,i))-E(f-RR(Va∣i,i))∣
P(f-RR(Va(i))= Vz(i)))
≤ exp(Ei)
(13)
Similarly, we obtain the same result with Eq. 12 in the second case:
P(f -RR(Vx) = Vz) = rY1 P(f -RR(Vx(i))= Vz(i)) < rY1 P(f -RR(Vx忆⑶)=Vz (i))
P(f-RR(ex) = Vz) = U P(f-RR(VxIi(i)) = Vz(i)) ≤ U P(f-RR(Vx(i)) = Vz(i))
rl-1
≤Y
i=0
P(f -RR(VxIi(i)) = Vz (i)) λ ∣E(f-RR(vx,i))-E(f-RR(vx∣i,i))∣
P(f-RR(Vx(i)) = Vz(i)) J
rl-1	rl-1
≤	exp(Ei) = exp(	Ei)
i=0	i=0
Consequently, Theorem 1 holds.
≤
(14)
16
Under review as a conference paper at ICLR 2022
E Proof of Theorem 2
To ensure that the privacy loss in randomizing the bit i is bounded by the privacy loss in the em-
bedded space and to take into account different sensitivities of different bits (Theorem 1), we need
to find α in Eq. 2 to solve Eq. 12. However, solving Eq. 12 is not straightforward since the
privacy budget i and the actual l1-distance |E (f -RR(vx, i)) - E (f -RR(vx|i, i))| are intractable.
To address this problem, from Eq. 12, we first consider the bit i in all the r embedded fea-
hιreς oς fnllnw ς∙ Q	P (f -RR(Va Ci))=VzCi)) V PXr√P	ei|E(f -RR(Va,i"-E (f -RR(Vazi))I)_
tures, as follows: ɪ la∈[ι,r] P(f-RR(Va∣i(i))=vz(i)) ≤ exp(乙a∈[1,r]	∆i	) =
exp(∆i Pa∈[i,r] |E(f-RR(Va,i)) - E(f-RR(Va∣i, i))|). Note that all the bits i, e.g., sign
bits, in all the features a ∈ [1, r] consume the same privacy budget i with the same
sensitivity ∆i.	The term Pa∈[1,r] |E (f -RR(Va, i)) - E(f-RR(Va|i, i))| can be unbiasedly
replaced with r × E|E (f -RR(Va, i)) - E (f -RR(Va|i, i))| where E is the expectation of
|E(f-RR(Va, i)) - E(f-RR(Va|i,i))|, since limr→∞ E|E(f-RR(Va, i)) - E (f -RR(Va|i, i))| =
Pa∈[1 r] |E(f-RR(Va,i))-E(f-RR(Va|i,i))|
—∈[-,-]-----------r------------------. Hence, we have that
Y	P (f -RR(Va(i)) = Vζ(i))
H P (f -RR(Va∣i(i)) = Vz (i))
a∈[1,r]
≤ eχp(Fei × E|E(f-RR(Va,i)) - E(f-RR(Va∣i, i))|)
⇔ Y ( P(f-RR(Va⑼=vz(i))
ɪɪ PP (f -RR(Va∣i(i)) = Vz(i))
a∈[1,r]
△i
E|E(f-RR(va,i))-E(f-RR(Va |i，i))| ≤ exp"/)
(15)
Note that, in our work, we consider the worst case is the case that all the bits of two neighboring
vectors can be different. The expectation E|E (f -RR(Va, i)) - E (f -RR(Va|i, i))| in Eq. 15 is used to
quantify the difference of every two extreme vectors at bit i. Then for the whole vector, it is the sum
over the expectation E(∙) of all bits i, as follows: Pa∈[i 灯 |E(f-RR(Va, i)) - E(f -RR(Va∣i, i))| =
r × E|E(f-RR(Va, i)) - E(f-RR(Va|i, i))|. Therefore, the expectation in Eq. 15 does not imply the
average-case scenario.
The sum over the expectation E(∙) of all bits i, i.e., Pa∈[ir] Pi∈[01-1] |E(f-RR(Va,i))-
E (f -RR(Va|i, i))| = r × Pi∈[0,l-1] E|E (f -RR(Va, i)) - E (f -RR(Va|i, i))|, is used to bound the
privacy loss as follows:
P (f -RR(Vx) = Vz )	Y Y / P(f -RR(Va(i)) = Vz (i))
P(f -RR(ex)= Vz ) ≤ aTi∈[0,L] "(f -RR(VaIi(i))= Vz(i))
△i
ElE(f-RR(va,i)) - E(f-RR(va ∣i,i)) |
≤ exp(
a∈[1,r] i∈[0,l-1]
i|E(f-RR(Va,i)) -E(f-RR(Va|i,i))|
)
= exp(
i∈[0,l-1]
q Q × E|E (f -RR(Va, i))- E (f -RR(Va|i, i)) |)
∆i
)
≤ exp(r	i)
i∈[0,l-1]
rl-1
⇔ Y(
i=0
P(f-RR(Vx (i)) = Vz(i))
P (f -RR(Vx|i (i)) = Vz (i))
______________△i______________
E∣E(f-RR(vχ,i))-E(f -RR(vχ∣i,i))∣
rl-1
≤ exp(	i )
i=0
(16)
Now, we need to bound the generalized privacy loss by discovering closed-form solutions of α
given the privacy budgets X (Theorem 2). In other words, we need to solve Eq. 16 for discov-
ering the closed-form solution of α. To solve it, first we need to calculate E|E (f -RR(Vx, i)) -
E (f -RR(Vx|i, i))|. To be more precise, let us denote pXi and qXi as pX and qX in Eq. 2for a
particular bit i, respectively. Given the worse case of Vx and Vx|i , there are four possible cases of
|E(f-RR(Vx,i))-E(f-RR(Vx|i,i))|:
•	If f-RR(Vx(i)) = 1 and f -RR(Vx|i (i)) = 1, then |E(f-RR(Vx, i)) - E (f -RR(Vx|i, i))| = 0.
17
Under review as a conference paper at ICLR 2022
•	If f -RR(Vx (i)) =0 and f -RR(Vx∣i(i)) = 0, then |E (f -RR(vx, i)) - E (f -RR(Vx∣i, i))| = 0.
•	If f -RR(Vx (i)) = 1 and f -RR(Vx∣i(i)) = 0, then |E (f -RR(Vx,i)) -E (f -RR(Vx∣i,i))∣ = ∆
This happens with the probability P(f-RR(Vx(i)) = 1, f-RR(Vx∣i(i)) = 0). To compute this
probability, we use marginal probability and Bayes, theorem, as follows:
P(f-RR(Vx(i)) = 1, f-RR(Vx∣g))= 0)
=P (f -RR(Vx ⑴)=1, f-RR(VxIMi)) = 0, Vx ⑴=1, %∣i⑺=0)
+ P (f -RR(Vx(i)) = 1,f -RR(Vx∣i(i)) = 0, Vx(i) = 0, Vx∣i(i) = 1)
=P (f -RR(Vx(i)) = 1|f -RR(VxIKi))= 0,Vx(i) =	⑺=0)
× P(f-RR(VxIi⑴)=0|Vx(i) = 1, Vx〔i⑴=0)
× P(Vx⑺=1|Vx1i⑴=0) × P(VxIi(i)= 0)
+ P (f -RR(Vx(i)) = 1|f-RR(Vx|i(i))=0,vx(i)=0,vx|i(i) = 1)
× P(f-RR(VxIi⑴)=0|Vx(i) = 0, Vx〔i⑴=1)
× P(Vx⑺=0|VxIi⑴=1) × P(VxIi(i) = 1)
=P (f -RR(Vx(i)) = 1|Vx(i) = 1)
×	P(f-RR(VxIi⑴)=0|VxIi⑴=0) × P(VxIi(i) = 0)
+	P (f -RR(Vx(i)) = 1|Vx(i)=0)
×	P(f-RR(VxIi⑴)=0|VxIi⑴=1) × P(VxIi(i) = 1)
=PXiP (VxIi⑴= 0) + qX iP (VxIi(i) = 1)	(17)
• If f-RR(Vx(i)) = 0 and f-RR(Vx∣i(i)) = 1,then |E(f-RR(Vx(i))) -E(f-RR(Vx∣i(i)))[ = ∆
This happens with the probability P(f-RR(Vx(i)) = 0, f-RR(Vx∣i(i)) = 1). To compute this
probability, we use marginal probability and Bayes, theorem, as follows:
P(f-RR(Vx(i)) = 0, f-RR(Vx∣i(i)) = 1)
=P (f -RR(Vx(i)) = 0, f-RR(Vx∣i(i)) = 1, Vx(i) = 1, VxIi(i) = 0)
+ P (f -RR(Vx(i)) = 0,f -RR(Vx∣i(i)) = 1,Vx(i)=0,Vx∣i(i) = 1)
=qX iP(VxIi(i) = 0) + PXiP(Vx∣i(i) = 1)	(18)
Consequently, the expectation E|E(f-RR(Vx, i)) - E(f-RR(Vx∣i, i))| is computed as follows:
E|E(f-RR(Vx,i)) -E(f-RR(Vx&i))|
=(PXiP(VxIi(i)=0) + qX iP (VxIi(i) = 1))∆i
+ (qX iP(VxIi(i) = 0) + PXiP(Vx∣i(i) = 1))∆i
= (PXi + qX i)∆i	(19)
From Eqs. 12, 16, and 19, we have that
18
Under review as a conference paper at ICLR 2022
P(f -RR(Vx)= Vz)
P(f -RR(eχ) = Vz)
rl-1
≤Y
i=0
(P(f-RR(Vx(i)) = vz(i)) λ
PP (f -RR(vx∣i(i)) = Vz (i)))
EIEf-RR(VX,i))- Ef-RR(Vx ∣i,iDl
rl-1
Y
i=0
(P (f -RR(VKi)) = 0|Vx ⑴=I)P (f -RR(VKi)) = 1|Vx ⑴=O) λ pXi + qXi
∖p (f -RR(Vx(i)) = l∣Vx(i) = 1)P (f -RR(Vx(i)) = 0∣Vx(i) = 0) J ""
l-1
■	______r_____
Y 卜2exp(2eχ l)) pχi+(1-pXi)2
i=0	l
(20)
Taking the natural logarithm of two sides of Eq. 20:
l P(f-RR(Vx) = Vz)
n P(f-RR(ex) = Vz)
l-1	.	r
≤ X ln 卜2exp(2eχ l)) pχ+(1-pχi)2
i=0
l-1	r	i
X (PXi + (1-PXiyln (α2exp(2eχI)))
(21)
Let us bound the summation in Eq. 21 using the following inequality:
ln(a) ≤ a - 1 fora > 0	(22)
The proof of Eq. 22 is as follows. Let a > 0, we define h(a) = ln(a) - a + 1. We have:
h0(a) = 1 - 1=0 ⇔ a =1, and since h00(a)=-* < 0, ∀a > 0, We get the maximal point
at a = 1. We also have: lima→0+ h(a) = -∞ = lima→∞ h(a). Therefore, a = 1 is the global
maximal point and than ∀a > 0, h(a) ≤ h(a = 1) = 0, so ln(a) - a + 1 ≤ 0. Therefore, Eq. 22
does hold.
Note that, to simultaneously satisfy the randomization probabilities PX = ι+α^X⅛%T7) ≥ 0
and qχ = ι ααXXp(⅞^) ≥ 0 in Eq. 2, we need to have: (i) 1 + α exp(军 EX) ≥ 0 and (ii)
α exp(军 EX) ≥ 0. Since exp( % EX) ≥ 0 is always true, from (i), α ≥ - exp(-军 EX), and from
(ii), α ≥ 0. Therefore, α ≥ 0 is necessary to satisfy the condition PX ≥ 0 and qX ≥ 0. To apply
Eq. 22 into Eq. 21, we need to have a = α2 exp(2EX ∣) > 0 ⇒ α = 0. As a result, we have that
α > 0	(23)
Applying Eq. 22 into Eq. 21 where a = α2 exp(2EX∣):
l P(f -RR(Vx)= Vz)
n P(f-RR(ex) = Vz)
l-1
≤ X (PXi + (：-PXi)2ln (α2 exp(2EXl)))
≤ X rα2 exp(2EX∣)
一⅛0 PXi + (I - PXi)2
l-1
-X
i=0
r
PXi +(1 - PXi)2
(24)
∆
To bound the logarithm in Eq. 24, we use: PXi + qXi = PXi + (1 - PXiy =(———⅛%—T)2 +
∖ 1 + a exP( -γ- eχ ) /
1+ αe exp( i% CX ))
(l + α exp( i% CX ))
(a exp( i% CX ) )2
∖1 + α exp(竽 CX ))
≤ 1, and pXi + (1 - PXiy ≥ (PXi+12—pXi)2 = 1 (In fact,
19
Under review as a conference paper at ICLR 2022
∀a, b :	a2 + b2 ≥	(a+b)	⇔ (a - b)2	≥ 0, which	is true). Note that, from Eq. 19, We have that
———-----7---、 Q” ,----7---rrτ =	~1~≥ 1
EIE (f -RR(Va,i))-E(f-RR(Va|i,i))|	Pχi + qX i —'
Applying these inequalities in Eq. 24, we obtain:
l P(f -RR(Vx)= Vz) /	rα2 exp(2eχ l)	r
n P(f-RR(ex) = vz) ≤ = pXi + (i- PXiy - = pXi + (i- PXiy
l-1	i	l-1	l-1	i
< ^X 2ra2 exp(2eχɪ)一 Xr= X2ra2 exp(2eχɪ) — rl ≤ EX	(25)
i=0	i=0	i=0
By solving Eq. 25, we have that
⇔ |a| ≤ J
α2 ≤
EX + rl
2r Pi=0 eχp(2EXi)
EX + rl
2r Pi=0 eχp(2EXi)
(26)
Therefore, from Eqs. 23 and 26, we have that ∀α : 0 < α ≤
mechanism satisfies EX -LDP. Consequently, Theorem 2 holds.
卜 Pi-KhXi), the f-RR
F Proof of Theorem 3
Proof. We have:
P(label-RR(yχ) = z∣yχ) V max P(label-RR(yx)=z∣yx)
P(label-RR(eχ) = z∣eχ) — min P(label-RR(eχ) = z∣eχ)
exp(β)
1+exp(β)
1
(1 + exp(β))(C-1)
ln(C - 1)) ≤ eχp(EY ) ⇔ β ≤ EY - ln(C - 1). Consequently, Theorem 3 does hold.
eχp(β +
□
G label-RR AND LABELDP COMPARISON
Although our label-RR is inspired by the randomizing probability (Eq. 1, (Ghazi et al., 2021)) in the
LabelDP showcased by (Ghazi et al., 2021), there are two major differences between our label-RR
and LabelDP discussed next.
(1)	We combine f-RR and label-RR to completely protect a data sample. As pointed out in (Busa-
Fekete et al., 2021), only protecting the label as in the LabelDP offers a weaker privacy protection
than it appears, as the features are sufficiently predictive of the label, obscuring the label is not
enough, as a classifier can still be trained on such noisy data; hence, a user experiences privacy loss
due to both the public release of the features and the private release of the label.
(2)	The RRWithPrior algorithm that is used to guarantee LabelDP requiring publicly available priors
and the multi-stage training (LP-MST) illustrating RRWithPrior algorithm cannot be straightforwardly
applied to federated learning. In LP-MST algorithm, the dataset is partitioned into subsets, then based
on the prior probability to randomize the label and add the data with that randomized label to the
training data. These steps are presently applied on centralized training and it has not been show how
to be effectively applied in federated learning.
Using the upper bound of β (Theorem 3) results in the same randomizing probabilities between label-
RR and LabelDP For instance in En 3	— exP(e) — eχP(eγ-In(C-I)) — eχp(eγ) And
RRandLablDEFormst3nCGm Eq.3, PY = ι+eχp(β) = l+eχp(eγ-ln(C-l)) = C-1⅛eχp(€γ) and
qγ = (1+eχp(β))(C-1) = C-1+1χp(eγ), WhiCh is equivalent to Eq. 1 (Ghazi et al., 2021). Therefore,
we did not include LabelDP (Ghazi et al., 2021) in comparison.
H Proof of Theorem 4
Proof. We have ξa	= E|E(f-RR(va)) - E(va)|	= Pi∈[o,ι-i](PXi	X 0 + qXi	X ∆i)=
Pi∈[o ι-i] qXi x ∆i.	Therefore, Theorem 4 hold.	□
20
Under review as a conference paper at ICLR 2022
I Corrected Privacy Budget Bounds in LATENT (Arachchige et al., 2019)
In this section, we aim at providing corrected privacy budget bounds for LATENT (Arachchige et al.,
2019). LATENT first encodes embedded features ex into an rl-bit binary vector vx . Then, each bit
i ∈ [0, rl - 1] is randomized by a RR mechanism (i.e., the MOUE algorithm for high sensitivities in
Theorem 3.3 (Arachchige et al., 2019)), denoted f -LT, as follows:
∀i ∈ [0, rl - 1] : P (vx0 (i) = 1)
PX = ɪɪ,
1+α
qX = 1 + α exp(篝),
ifvx(i) =1
ifvx(i) =0
(27)
From Eq. 27, we also have that P(Vx(i) = 0) = 1 - PX = ι+α^ if vχ(i) = 1, and P(Vx(i) = 0)=
1 — qχ = :exp(7⅜τ if vx(i) = 0.
YX	1+a exp( -ɪ) S)
Theorem 5. LATENT with the randomization probabilities as in Eq. 27 preserves corrected-LDP,
wufirfi F	— (I+α)α+α exp( -X))F
where Ecorrected = -α(1+exp(瓷))EX'
Proof. Similar to the analysis in Appendix E, we obtain the following inequality:
Pf -LT(Vx) = Vz) ≤ Y1( Pf -LT(Vx(i)) = Vz(i)
Pf-LT(ex) = Vz) ≤ i=0 PP(f-LT(Vx∣i(i)) = Vz(i
△i
EIEf-LT(Vx ,i))- Ef-LT(V χ∣i,i))1	(	∖
x|i ≤ exp(EX)
(28)
and the expectation E|E(f-LT(Vx, i)) - E (f -LT(Vx|i, i))| is computed as follows:
E|E(f-LT(Vx,i)) -E(f-LT(Vx|i,i))|
= P (f -LT(Vx, i) = 1|Vx(i) = 1)
× P (f -LT(Vx|i, i) = 0|Vx|i(i) = 0) × P (Vx|i (i) = 0)
+P(f-LT(Vx,i) = 1|Vx(i) =0)
× P (f -LT(Vx|i, i) = 0|Vx|i(i) = 1) × P (Vx|i (i) = 1)∆i
+ P(f-LT(Vx,i) = 0|Vx(i) = 1)
× P (f -LT(Vx|i, i) = 1|Vx|i(i) = 0) × P (Vx|i (i) = 0)
+P(f-LT(Vx,i) = 0|Vx(i) =0)
× P (f -LT(Vx|i, i) = 1|Vx|i(i) = 1) × P (Vx|i (i) = 1)∆i
= PXi(1 - qX i)P (Vx|i (i) = 0) + qXi(1 - PX i)P (Vx|i (i) = 1)
+ (1 - PXi)qXiP(Vx|i(i) = 0) + (1 - qXi)PXiP(Vx|i(i) = 1) ∆i
= PXi(1 - qXi) + qXi(1 - PXi)∆i	(29)
Furthermore, we have:
α(1 + eχp(弩))
PXmi) + qXi(1 - PXi) = (1 + α)(1 + α eXp(篝))
(30)
21
Under review as a conference paper at ICLR 2022
From Eqs. 28-30, we have that
P (f-LT(vχ) = Vz)
P (f-LT(eχ) = Vz)
rl-1
≤Y
i=0
(P(f-LT(Vx(D) = Vz⑺)∖
1P(f -LT(Vx∣i(i)) = vz(i)))
______________δW______________
E∣Ef-LT(vχ,i))-Ef-LT(vχ∣i,i))∣
rl-1
Y
i=0
(P (f-LT(Vx(i)) = 1∣Vx(i) = 1) λ (pχi(1-qχi)+qχi(1-pχi)')∆i
∖p (f-LT(Vx(i))=0∣Vx(i) = 1)J
rl-1
Y
i=0
(P(f-LT(Vx(i)) = 0∣Vx(i)=0))(px7(i-qx7)+qx7(i-p^i))ʌi
1P(f-LT(Vx(i)) = 1∣Vx(i)=0))
rl-1	___i__
Y (exp(手)―
(31)
×
Then, from Eq. 31, we have:
ecorrected = ln (∏M1 (exp(^χ)) PXia-qXi)+qXia-PXi))=
(1 + α)(1 + α exp(篝))
α(1 + eχp( 宵))
(32)
Consequently, Theorem 5 holds.
□
From Theorem 5, we show the proportion ecorrected/eX as a function of r in Figure 9a and as a
function ofl in Figure 9b. Following the experiment settings in LATENT (Arachchige et al., 2019),
with the commonly used α = 7, when changing r ∈ {10, 100, 1, 000, 10, 000} with a fixed l = 10
(Figure 9a), or when changing l ∈ {5, 10, 20, 100, 1, 000} with a fixed r = 1, 000 (Figure 9b) and
under a tight privacy budget eX = 0.1, the proportion ecorrected/eX moderately changes among
[4.57, 4.75]. In other words, the ecorrected is remarkably larger than eX , for most r andl values
in practice. Unlike LATENT, our mechanism does not suffer from this problem, i.e., in BitRand,
ecorrected/eX = 1, thanks to our bit-aware randomization probabilities for LDP in binary encoding
(Theorem 2).
J Corrected Privacy Budget Bounds in OME (Lyu et al., 2020a)
In this section, we aim at providing corrected privacy budget bounds for OME. OME first encodes
embedded features ex into an rl-bit binary vector Vx. Then, each bit i ∈ [0, rl - 1] is randomized by
the following f -OME mechanism:
'Pιx = τ-^,	if i ∈ 2j,vχ(i) = 1
1 + α
∀i ∈ [0,rl - 1] : P(Vx(i) = 1) = < P2X = 1+α3 ,	if i ∈ 2j + 1,vχ ⑺=1
qχ = 7--1 LX、, if Vx ⑺=0
1+α eχp( ^rf)
(33)
From Eq. 33, We also have that P(Vx (i) = 0) = 1 -pιχ = j+1α if Vx (i) = 1 and i ∈ 2j, P(Vx (i)=
0) = 1 - P2χ = ι+⅛ if Vx⑴=1 and i ∈ 2j + 1, and P(Vx ⑺= 0) = 1 - qχ = ι+αχxP(XX))if
Vx⑴=0.
Theorem 6. OME with the randomization probabilities as in Eq. 33 preserves ecorrected-LDP, where
Ecorrected = (Tr——Jr- )ln(α) + ^X + ^X in Which Qi = OL- α exp( jτ) + -------1	--L-
Corrected 'Q1 Q2)	\ 2 1 2Q1	1 2Q2	*1	1+α 1+α exp( -X)	1+α exp( -X-) 1+α
and Q =」_ _α exp( -XL)+__________i_____a_
卬2	1+α3 1+α exp(瓷)	1+α exp(瓷)1+«3 '
Proof. Similar to the analysis in Appendix E and Appendix I, We obtain:
22
Under review as a conference paper at ICLR 2022
(b) Fixed X = 0.1, r = 1, 000
(a) Fixed X = 0.1, l = 10
5O5O.5.O.5.O
443.3.NNLL
IN山IVI UlX3∕pφbφ±0u3
5050.5.0.5.0
443.3.NNLL
IN山IVI UlX3∕pφbφ±0u3
Figure 9: Impacts of r and l on Corrected/X in LATENT (Arachchige et al., 2019) and OME (Lyu
et al., 2020a).
Pf -OME(Vx) = Vz)
Pf -OME@x) = Vz)
rl-1
≤Y
i=0
P(f -OME(Vx(i)) =Vz (i)) ∖ E∣E(f-ΟME(vχ,i))-E(f-ΟME(vχ∣i,i))∣
Pf -OME(Vx∣i(i)) = vz(i)))
≤ exp(X)
(34)
and the expectation E|E (f -OME(Vx, i)) - E (f -OME(Vx|i, i))| is computed as follows:
E|E(f-OME(Vx,i)) -E(f-OME(Vx|i,i))|
ʃ (pιxi(1 — qxi)	+ qxi(1 — Pιxi))∆i	=	Qι∆i, if i ∈	2j
[(p2Xi(1 — qxi)	+ qxi(1 — P2Xi))∆i	=	Q2∆i,if i ∈	2j	+ 1
(35)
where Qi = pιχi(1 — qxi) + qχi(1 — pιχi) = α~ ~∣ αeχp( rχ、+ --------1 , 咏、1 1 ,
1	IXi	Xi Xi	1Xii	i + α i + a exp( -X)	1 + α exp(-X) 1+α,
1	a exp( -X)	,	1	a3
and Q2 = p2xi(I - qXi) + qXi(I - p2Xi) = 1+03 1+Oexprrf) + 1+α exp( -X) 1+03.
From Eqs. 34 and 35, we have:
P(f -OME(Vx) = Vz)
P(f -OME(ex) = Vz)
rl-1
≤Y
i=0
P(f -OME(Vx(i)) =Vz (i)) ∖ E∣E(f-OME(vχ,i))-E(f-OME(vχ∣i,i))∣
P(f-OME(Vx∣i(i)) = Vz (i)))
π
i∈2j
(P(f-OME(Vx(i)) = 1∣Vx(i) = 1)P(f-OME(Vx(i)) = 0∣Vx(i) = 0)、鼻
P(f -OME(Vx(i)) = 1∣Vx(i) = 0)P(f-OME(Vx(i)) = 0∣Vx(i) = 1))
π
i∈2j+1
(P(f-OME(Vx(i)) = 1∣Vx(i) = 1)P(f-OME(Vx(i)) = 0∣Vx(i) = 0)、Q∆
P(f -OME(Vx(i)) = 1∣Vx(i) = 0)P(f-OME(Vx(i)) = 0∣Vx(i) = 1))
注-旦 ,€X	€X .
α Q1 Q2 eχp⅛? + 2Q2)
(36)
Then, from Eq. 36, we have:
Wcorrected = ln (αQI-Qr2 exP(含 + 前))=(—	) ln(α) + 前 + 前 (37)
2Q1	2Q2 Q1	Q2	2Q1	2Q2
Consequently, Theorem 6 does hold.
×
□
23
Under review as a conference paper at ICLR 2022
From Theorem 6, we show the proportion corrected/X as a function of r in Figure 9a and as a
function of l in Figure 9b. Following the experiment settings in OME (Lyu et al., 2020a), with
the commonly used α = 100, when changing r ∈ {10, 100, 1, 000, 10, 000} with a fixed l = 10
(Figure 9a), or when changing l ∈ {5, 10, 20, 100, 1, 000} with a fixed r = 1, 000 (Figure 9b) and
under a tight privacy budget X = 0.1, the proportion corrected/X significantly changes among
[4.6e + 6, 4.6e + 9]. In other words, the corrected is extremely larger than X, for most r and l
values in practice. Since α = 100 causes the extreme privacy exaggeration, in our experiment, to
compare with OME, we use α = 1. This value is used in OME (Lyu et al., 2020a) and generates
corrected/X ≈ 2, which offers a reasonable range to apply OME in practice. Unlike OME, our
mechanism does not suffer from this problem, i.e., in BitRand, corrected/X = 1, thanks to our
bit-aware randomization probabilities for LDP in binary encoding (Theorem 2).
K Supplementary theoretical results
Setting for Gaussian and Laplace mechanisms.
The Gaussian and Laplace mechanisms naturally ap-
ply an addition operation, which add noise into the
data or embedded features. Therefore, in our anal-
ysis of expected error bound comparison for an em-
bedded feature (Figure 3), we add noise into the em-
bedded feature following the Gaussian and Laplace
mechanisms. The sensitivity captures the magnitude
by which an embedded feature can change in the
worst case. In our experiment and analysis, we use
Figure 10: RMSE error comparison as a func-
tion of X .
--Three-outputs
PM_5UB
corrected LATENT
corrected OME
l = 10 bits in which 1 sign bit, 5 bits for the
integer, and 4 bits for the fraction part. Therefore, the maximum the embedded feature can be change,
i.e., the sensitivity, is 2 Pi4=-4 2i. Note that, we multiply Pi4=-4 2i by 2 since when we flip the sign
bit, it significantly changes the value of the embedded feature from -a to a in which a = Pi4=-4 2i.
RMSE error comparison in mean estimation. To investigate how our proposed approach f-RR
works with statistical query, we study our f-RR and other baselines with a mean estimation. We
created a synthetic data that consists of N = 1, 000 data samples {xi}iN=1, each of them has d = 768
dimensions. The mean estimation is calculated over each dimension as fj (D) = N P= Xij for
j ∈ [1, d]. Root mean square error (RMSE) is used to evaluate the error between the original vector
and the randomized/estimated vector. The binary-encoding-based approaches (i.e., f -RR, corrected
LATENT, and corrected OME) achieve a significantly small error compared with others. As can
be seen in Figure 10, f-RR obtains the smallest error, which further shows the effectiveness of our
proposed mechanism.
No bit-aware (ξ∣)
corrected LATENT [ξ∣)
■•••■ corrected OME (ɛ)
Figure 11: Expected error bound as a function of X with fixed r and l.
V- corrected OME (ξtοp-k)
-T-「RR (WtOP-k)
Y- No bit-aware (ξa)
corrected LATENT (ξa)
(c) X=2.0
(b) X =1.0
(a) X =0.1
Figure 12: Randomization probability qX and qtop-k, given l = 10 and r = 1, 000.
24
Under review as a conference paper at ICLR 2022
■ Sign bit
■ Highest Integer bit
■ Lowest Integer bit
■ Highest Fraction bit
■ Lowest Fraction bib
■ Sign bit
Y - Highest Integer bit
K Lowest Integer bit
⅛∙ Highest Fraction bit
♦ ■ Lowest Fraction bit
Sign bit
< ■ Highest Integer bit
+ Lowest Integer bit
⅛- Highest Fraction bit
♦ ■ Lowest Fraction bit
5 and X
20 and X
100 and X
Slgn bit
Highest Integer bit
Lowest Integer bit
Highest Fraction bit
Lowest Fraction bit
(d) l = 20 and X
20 and X = 2.0
(a)	l = 5 and X = 0.1
(b)	l = 5 and X
■	Sign bit
—	‹ ■ Highest Integer bit
Lowest Integer bit
-	A- Highest Fraction bit
♦ ■ Lowest Fraction bitn
T - Highest Integer bit
+∙. Lowest Integer-bit4
⅛∙ Highest Fraction bit
♦ ■ Lowest Fraction bit
Higtiest lritegerblr
+ Lowest Integer bit
■A- Highest Fraction bit
♦ ■ Lowest Fraction bit
Y - Highest Integer bit
-T- Lowest Integer bit
-A- Highest Fraction bit
T∙ Highest Integer bit
—- Lowest Integer-bit^
-A- Highest Fraction bit
♦ ■ Lowest Fraction bit
(g)	l = 100 and X
(h)	l = 100 and X
—< " Highest Integer bit
-T- Lowest Integer bit
r⅛∙ Highest Fraction bit
■ Sign bit
—‹ ■ Highest Integer bit
- Lowest Integer-bit^
-A- Highest Fraction bit
♦ ■ Lowest Fraction bit
Signjpit....
< " Highest Integer bit
+ Lowest Integer bit
■A- Highest Fraction bit
♦ ■ Lowest Fraction bit
(j)	l = 1, 000 and X = 0.1
(k)	l = 1, 000 and X = 1.0
(l)	l = 1, 000 and X = 2.0
Figure 13: Randomization probability q (p = 1 - q) as a function of r with fixed l and .
25
Under review as a conference paper at ICLR 2022
0.6
0.6
0.6
0.6
■ Sign bit
—‹ ■ Highest Integer bit
Lowest Integer bit
-A- Highest Fraction bit
---♦ ■ Lowest Fraction bite
■ Sign bit
—‹ ■ Highest Integer bit
∙⅜owest Integer-bit^
-A- Highest Fraction bit
■ ♦ ■ Lowest Fraction bit
‘ign bit
< ■ Highest Integer bit
Lowest Integer bit
⅛∙ Highest Fraction bit
♦ ■ Lowest Fraction bit
1, 000 and X
Figure 14: Randomization probability qX (pX
0.4
r = 10, 000 and X
(j) r = 10, 000 and X = 0.1
(e) r = 100 and X
Sign bit
W∙ Highest Integer bit
▼- Lowest Integer bit
▲ " Highest Fraction bit
♦ ■ Lowest Fraction bit
(l) r = 10, 000 and X
(h) r = 1, 000 and X
0.5
q×
0.5
q×
0.5
q×
0.5
qχ
(a) r = 10 and X
(b) r = 10 and X
(c) r = 10 and X
T - Highest Integer bit
Lowest Integer bit
-A- Highest Fraction bit
■ ♦ ■ Lowest Fraction bi⅛
(d) r = 100 and X
T - Highest Integer bit
Lowest Integer bit
-A- Highest Fraction bit
■ ♦ ■ Lowest Fraction bi⅛
(g) r = 1, 000 and X
T - Highest Integer bit
Lowest Integer bit
-A- Highest Fraction bit
---♦ ■ Lowest Fraction bite
■	Sign bit
■	Highest Integer bit
-⅜owest Integer-bit*
■ Highest Fraction bit
■ Lowest Fraction bit
■ Sign
■ Highest Integer bit
■■ ∙⅜owest Integer-bit*
■ Highest Fraction bit
■ Lowest Fraction bit
Sign
Highest Integer bit
⅜owest lnteger-b⅛*
Highest Fraction bit
Lowest Fraction bit
?gn bit
Y∙ Highest Integer bit
Lowest Integer bit
⅛∙ Highest Fraction bit
♦ ■ Lowest Fraction bit
100 and X = 2.0
Sign bit
W∙ Highest Integer bit
▼- Lowest Integer bit
▲ " Highest Fraction bit
Lowest Fraction bit
1 - qX) as a function of l with fixed r and .
26
Under review as a conference paper at ICLR 2022
L S upplementary Experimental Results
Datasets and Data Processing. We carried out our experiments on two textual datasets and two
image datasets, including the AG dataset (Gulli et al., 2012), our collected Security and Exchange
Commission (SEC) financial contract dataset, the large-scale celebFaces attributes (CelebA) dataset
(Liu et al., 2015), and the Federated Extended MNIST (FEMNIST) dataset (Caldas et al., 2018). The
AG dataset is a collection of news articles gathered from more than 2, 000 news sources by (Com).
It is categorized into four classes: world, sport, business, and science/technology classes. Our SEC
dataset consists of over 1, 000 contract clauses collected from contracts submitted in SEC filings1.
The CelebA dataset consists of more than 200, 000 celebrity images, each with 40 attributes, e.g.,
attractive face, big lips, big noses, black hair, etc., which are used as binary classes. The FEMNIST
dataset is built by partitioning the images in Extended MNIST (Cohen et al., 2017) based on the
writer of the handwritten digits and characters. For data preprocessing, we changed all words in the
AG and SEC datasets to lower-case and removed punctuation marks. The breakdown of the datasets
is in Table 1.
Table 1: Dataset breakdown.
Dataset	Train	Test	Samples/client	# classes
	# samples	# samples	(Average)	
AG	120,000	-7,600-	43	4
SEC	-17021-	134	3	2
-CelebA-	155, 529	-19, 962-	20	40 (binary)
FEMNIST	734,033	83, 818	227	62	一
Model Configuration. We use the test accuracy and the test area under the curve (AUC) as evaluation
metrics. Models with higher values of test accuracy and AUC are better. We use the BERT-Base
(Uncased) pre-trained model (ber; Devlin et al., 2018) to extract embedded features in the AG and
SEC datasets. In the CelebA and FEMNIST datasets, we use the ResNet-18 pre-trained model (img;
He et al., 2016). Dimension of the extracted embedded features in the AG and SEC datasets is
r = 768, and in the CelebA and FEMNIST datasets is r = 512. For text and image classification
tasks, we use two fully connected layers on top of embedded features, each of which consists of
1, 500 hidden neurons and uses a ReLU activation function. The output dimension is corresponding
to the number of classes, i.e., 4, 2, 40, and 62 in the AG, SEC, CelebA, and FEMNIST datasets. SGD
optimizer with the learning rate is 0.01 in the AG and SEC datasets, 0.1 in the FEMNIST and CelebA
datasets.
Experimental setting for anonymization (Sun et al., 2021).
In LDP-FL (Sun et al., 2021), they design a LDP mechanism to perturb the weights at the local
client, then each local client applies a split and shuffle mechanism on the weights of local model
and sends each weight through an anonymous mechanism to the cloud. The purpose of the shuffling
mechanism is to break the linkage among the model weight updates from the same clients and to mix
them among updates from other clients, making it harder for the cloud to combine more than one
piece of updates to infer more information about any client. Therefore, the key idea of the shuffle
mechanism in LDP-FL is to mitigate the privacy degradation by high data dimension and many
training/query iterations. In other words, the client anonymity is preserved, and the privacy budget
will not accumulate.
When comparing with LDP-FL, we maintain their mechanism’ spirits of no privacy accumulation.
In the submission, we consider there is no privacy accumulation over the training iterations. It
is equivalent to the shuffling step that breaks the linkage among the model weight updates with
associated clients. We also used the same randomized response mechanism in the paper, which is
Eq. 2 (Sun et al., 2021), to perturb the weight.
In the revision, we added an experiment that do not consider the privacy accumulation over data
dimension and training/query iterations. This completely follows the gist of LDP-FL. In addition,
1 https://www.sec.gov/edgar.shtml
27
Under review as a conference paper at ICLR 2022
the weights we used for LDP-FL. without actual shuffling or splitting can be considered a lossless
process, therefore the results we reported here can be counted as an upper-bound result for LDP-FL.
As shown in Table 2, we obtained the slightly higher accuracy of LDP-FL compared with f-RR
on the AG dataset. The key component of LDP-FL that helps to reduce the privacy accumulation
issue is the shuffling mechanism. However, as pointed out in (Erlingsson et al., 2019), in the real
world, it is possible that the anonymizers (i.e., shuffler) can either be compromised or collude with
the coordinating server to extract sensitive information. Even though there is a marginally lower
trade-off between privacy loss and model utility compared with LDP-FL, the advantage of f-RR is
that it perturbs the data only once, then used the perturbed data for training process without facing an
extra privacy risk potentially caused by the compromised or colluded anonymizer.
Table 2: Results of LDP-FL without privacy accumulation.
∈X	1	2	3	4	5	6	7	8	9	10
LDP-FL	78.73	82.21	83.25	84.19	84.36	84.22	84.64	84.43	84.31	84.72
f -RR	一	72.46	79.72	81.42	79.35	81.11	79.84	79.50	80.05	81.20	82.72
Noiseless model		87.59										
DM [ex]
■■∙+••• PM [ex]
•- HM [ex]
Three-outputs [ex]
-×- PM-SUB [ex]
—∙- corrected LATENT [ex]
—corrected OME [ex]
BitRand [eκ] (εγ=∞)
--- BitRand [eκ] (εγ= 1.0)
-*- BitRand [ex] (εy=2.5)
-Y- BitRand [ex] (εy= 5.0)
BitRand [ex] (εγ= 10.0)
—∙- Random-→- Noiseless
(c) SEC
Figure 15: Accuracy of LDP algorithms applied on the embedded features ex in the AG, SEC, and
FEMNIST datasets.
(b) FEMNIST
Privacy budget ε×
(b) FEMNIST
■ Three-outputs [vθ ]
PM-SUB [V⅛]
corrected LATENT [ex]
corrected OME [ex]
BitRand [ex] [εγ=∞)
BitRand [ex] (εy= 1.0)
BitRand [ex] (εy = 2.5)
BitRand [ex] (εy= 5.0)
BitRand [ex] (εy=10.0)
Random →- Noiseless
Figure 16: Accuracy of LDP algorithms applied on the gradients 5θu in the AG, SEC, and FEMNIST
datasets.
杆谛帝基不沁7；沁K蒜布茄境启T
123456789 10
Privacy budget εχ
(b) FEMNIST
23456789
Privacy budget εχ
(c) SEC
DM [ygt & Anon]
•••+-• PMHK & Anon]
•■■■■- HMMt & Anon]
Three-outputs [v⅛ & Anon]
-×∙∙ PM-SUB [Vβt & Anon]
LDP-FL [V⅛ & Anon]
corrected LATENT [ex]
—corrected OME [ex]
-4- BitRand [ex] (εy= ∞)
BitRand [ex] (εy= 1.0)
10 -*- BitRand [ex] (εy=2.5)
-→- BitRand [ex] (εy= 5.0)
BitRand [ex] (ε×= 10.0)
-→- Random Noiseless
Figure 17: Accuracy of LDP algorithms applied on the gradients 5θu with the anonymizer (Sun et al.,
2021).	t
28
Under review as a conference paper at ICLR 2022
90
123456789 10
Privacy budget εχ
(c) SEC
「RR & Label-Laplace (εγ = 1.0)
-	<-「RR & Label-Laplace (εγ = 2.5)
—	「RR & Label-Laplace (εγ = 5.0)
「RR & Label-Laplace (εγ = 10.0)
∙∙*∙∙ BitRand (εγ= 1.0)
BitRand (εy = 2.5)
—	BitRand (εy= 5.0)
-	4- BitRand (εy= 10.0)
→— Random
—Noiseless
100
90
80
U
< 70
60
50
40
123456789 10
Privacy budget εχ
123456789 10
Privacy budget εχ
(b) FEMNIST
90
80
U
?70
60
50
40
,♦--+-J--+--，/''、
• ■••• DM [ex]
PM [ex]
HM [ex]
Three-outputs [ex]
∙∙∙×∙∙ PM-SUB [ex]
—∙- corrected LATENT [ex]
一v- corrected OME [ex]
-BitRand [ex] (εγ= ∞)
-*-■ BitRand [ex] (εγ= 1.0)
BitRand [ex] (εγ=2.5)
123456789 10 -^*" BitRand [eχ](£y= 5.0)
(a) AG
Privacy budget EX
(c) SEC
BitRand [ex] (εγ= 10.0)
Random Noiseless
Figure 19: AUC values of LDP algorithms applied on the embedded features ex in the AG, SEC, and
FEMNIST dataset.
Privacy budget EX
(b) FEMNIST
Figure 20: AUC values of LDP algorithms applied on the gradients 5θu in the AG, SEC, and
FEMNIST datasets.	t
Three-outputs [vθt]
PM-SUB [v⅛]
corrected LATENT [ex]
corrected OME [ex]
BitRand [ex] (εγ= ∞)
BitRand [ex] (εy= 1.0)
BitRand [ex] (εy = 2.5)
BitRand [ex] (εy= 5.0)
BitRand [ex] (εy= 10.0)
Random →- Noiseless
Privacy budget ε×
(b) FEMNIST
「RR & Label-Laplace (εγ= 1.0)
「RR & Label-Laplace (εγ = 2.5)
「RR & Label-Laplace (εγ = 5.0)
「RR & Label-Laplace (εγ= 10.0)
BitRand (εγ= 1.0)
BitRand (εy = 2.5)
BitRand (εy= 5.0)
BitRand (εy=10.0)
Random
Noiseless
Figure 21: AUC values of each mechanism applied on labels in the AG, SEC, and FEMNIST datasets.
29
Under review as a conference paper at ICLR 2022
Table 3: AUC values of each algorithm applied on ex in the CelebA dataset. Average is the average
of all 40 attributes.
Attribute Algorithm [5θut with Anon]		AttraCtive	Heavy Makeup	High Cheekbones	Male	Mouth Slightly Open	Smiling	LipstiCk	Average
Noiseless		eχ = ∞		-7805-	85.47	76.53	92.77	-7291-	-79.15-	-88.85-	-68.09-
DM	eχ = 1	—	-5196-	50.00	509	50.12	-5154-	-51.77-	-50.63-	-5017-
	eχ = 5	-52.48-	50.00	5104	50.14	-5164-	-52.08-	-50.94-	-50.21-
		eχ = 10		-52.08-	50.00	50:87	50.20	-5191-	-52.03-	-50.40-	-50.19-
PM	eχ = 1	—	-5183-	50.00	5113	50.09	-5222-	-5195-	-50.58-	-5020-
	eχ = 5	-5167-	50.00	50.93	50.39	-5231-	-51.63-	-50.59-	-50.19-
	eχ = 10	-52.62-	50.00	50.98	50.16	-5221-	-52.02-	-50.49-	-50.21-
HM	eχ = 1	—	-52.55-	50.00	50.56	50.13	-5190-	-51.53-	-50.68-	-5018-
	eχ = 5	-52.07-	50.00	50.72	50.16	-51.65—	-5199-	-50.61-	-50.18-
	eχ = 10	-5186-	50.00	50.56	50.09	-52.26-	-52.19-	-50.63-	-50.19-
Three outputs	eχ = 1	—	-52.57-	50.00	50.59	50.19	-5180-	-51.83-	-50.62-	-50.19-
	eχ = 5	-52.07-	50.00	50.75	50.12	-52.14-	-51.66-	-50.60-	-50.T8-
	eχ = 10	-5194-	50.00	50.54	50.19	-5144-	-52.30-	-50.38-	-50.17-
PM-SUB	eχ = 1	—	-52.44-	50.01	5110	50.13	-5202-	-51.66-	-5074-	-5020-
	eχ = 5	-52.49-	50.00	50:87	50.15	-52.18-	-51.64	-50.51-	-50.20-
	eχ = 10	-5194-	50.00	50.30	50.19	-52.02-	-52.28-	-50.85—	-5019-
f-RR and Label -LaplaCe	eχ =1, eγ = 1	-50.00-	50.81	50.00	50.03	-50.00-	-50.00-	-50.00-	-5012-
	eχ = 1, eγ = 2.5	-50.00-	50.00	50.00	51.39	-50.00-	-50.00-	-50.00-	-50.20-
	eχ =1, eγ =5	-5192-	50.06	50.31	51.41	-50.00-	-50.01-	-50.00-	-50.22-
	eχ =5, eγ = 1	-50.64-	50.00	50.07	50.01	-50.00-	-50.39-	-50.00-	-50.21-
	eχ =5, eγ = 2.5	-50.00-	50.23	50.00	49.99	-50.00-	-50.00-	-50.00-	-5010-
	eχ =5, eγ =5	-5187-	52.40	50.00	50.00	-50.00-	-50.00-	-50.00-	-50.21-
	eχ = 10, eγ = 1 一	-5170-	50.00	50.00	50.00	-50.00-	-5103-	-50.00-	-5012-
	eχ = 10, eγ = 2.5	-50.01-	50.12	50.00	50.00	-50.00-	-50.05—	-51.12-	-5016-
	eχ = 10, eγ =5	-50.00-	50.00	50.00	50.13	-50.01-	-50.00-	-50.00-	-50.21-
BitRand	eχ = 1, eγ = ∞	59.8	60.65	567	64.57	-54.56-	-56.44	-64.93-	-51.86-
	eχ =1, eγ = 1	-54.52-	55.03	53.37	56.99	-52.63-	-53.34	-57.43-	-50.91-
	eχ = 1, eγ = 2.5	-59.08-	58.20	55.36	62.02	-54.29-	-55.86-	-61.91-	-51.52-
	eχ =1, eγ =5	-59.98-	60.15	56.28	63.67	-5414-	-56.91-	-63.98-	-51.77-
	eχ = 5, eγ = ∞	-67.15-	71.81	60.65	77.91	-58.16-	-61.82-	-76.15—	-54.99-
	eχ =5, eγ = 1	-58.04-	59.87	5424	62.99	-54.05-	-55.67-	-62.35-	-51.82-
	eχ =5, eγ = 2.5	-64.61-	67.70	58.78	73.56	-56.2-	-60.04	-72.09-	-53.50-
	eχ =5, eγ =5	-67.51-	71.41	60.94	77.96	-57.82-	-6157-	-761-	-54.82-
	eχ = 10, eγ = ∞	-75.09-	81.96	70:14	90.22	-65.26-	-71.76-	-86.68-	-63.31-
	eχ = 10, eγ = 1 一	-6187-	64.88	59.04	68.82	-56.8-	-60.33-	-67.20-	-53.84-
	eχ = 10, eγ = 2.5	-7135-	76.69	66:88	84.48	-62.74-	-68.33-	-81.52-	-5816-
	eχ = 10, eγ =5	74.45	82.28	69.23 —	89.88	64.81	71.47	86.38	62.25
30
Under review as a conference paper at ICLR 2022
Table 4: AUC values of each algorithm applied on the gradients 5θu with the anonymizer (Sun et al.,
2021) in the CelebA dataset.
Attribute Algorithm [5θut with Anon]		AttraCtive	Heavy Makeup	High Cheekbones	Male	Mouth Slightly Open	Smiling	LipstiCk	Average
Noiseless		eχ = ∞		-7805-	85.47	76.53	92.77	-72.91-	-79.15~	-88.85-	-68.09-
DM	eχ = 1	—	-43.44-	55.86	49.63	65.42	-48.86-	-4333~	-60.50-	-49.85-
	eχ = 5	-46.94-	44.25	49.00	49.68	-49.97-	-51.73-	-59.24-	-51.00-
	ex = 10	-38.60-	40.81	5187	64.81	-49.55-	-49.05-	-48.58-	-50.25-
PM	ex = 1	一	-47.48-	62.64	53.94	49.60	-50.88-	-49.07-	-52.04-	-50.81-
	ex = 5	-53.65-	67.99	47.72	44.75	-50.13-	-52.34	-43.20-	-49.72-
		eχ = 10		-56.85-	45.04	52.46	48.42	-50.15-	-49.16-	-49.81-	-50.30-
HM		eχ = 1		-4961-	63.99	52.06	47.27	-5190-	-48.43-	-3825-	-50.05-
	eχ = 5	-5339-	42.84	47.26	59.54	-53.23-	-55.21-	-67.67-	-51.54-
	eχ = 10	-4402-	68.26	51.07	54.43	-50.17-	-49.91-	-44.04-	-50.42-
Three outputs	eχ = 1	—	-5471-	43.89	5570	55.37	-48.46-	-54.93-	-43.11-	-49.99-
	eχ = 5	-3545-	65.80	50.13	42.53	-50.21-	-45.62-	-50.36-	-51.54-
	eχ = 10	-48.53-	46.76	50.03	51.96	-53.64-	-49.57-	-31.48-	-49.21-
PM-SUB	eχ = 1	—	-41.65-	51.61	50.05	63.01	-5035-	-49.02-	-51.05-	-5029-
	eχ = 5	-47.62-	43.88	50.86	54.82	-51.76-	-53.90-	-46.81-	-51.79-
	eχ = 10	-40.82-	74.82	49.86	55.00	-54.19-	-52.39-	-45.04-	-51.34-
LDP-FL	eχ = 1	—	-41.51-	49.43	5024	33.03	-49.01-	-49.40-	-5376-	-487-
	eχ = 5	-50.35-	53.27	5T.99	56.76	-49.71-	-50.02-	-56.31-	-51.74-
	eχ = 10	-4671-	46.90	50.11	52.04	-47.74-	-48.50-	-49.42-	-50.28-
f-RR and Label -LaplaCe	eχ = 1, eγ = 1	-50.00-	50.81	50.00	50.03	-50.00-	-50.00-	-50.00-	-50.12-
	eχ = 1, eγ = 2.5	-50.00-	50.00	50.00	51.39	-50.00-	-50.00-	-50.00-	-50.20-
	eχ = 1, eγ = 5	-51.92-	50.06	50.31	51.41	-50.00-	-50.01-	-50.00-	-50.22-
	eχ = 5, eγ = 1	-50.64-	50.00	50.07	50.01	-50.00-	-50.39-	-50.00-	-50.21-
	eχ = 5, eγ = 2.5	-50.00-	50.23	50.00	49.99	-50.00-	-50.00-	-50.00-	-50.10-
	eχ = 5, eγ = 5	-5187-	52.40	50.00	50.00	-50.00-	-50.00-	-50.00-	-50.21-
	eχ = 10, eγ = 1 一	-51.70-	50.00	50.00	50.00	-50.00-	-51.03-	-50.00-	-50.12-
	eχ = 10, eγ = 2.5	-50.01-	50.12	50.00	50.00	-50.00-	-50.05—	-5112-	-50.16-
	eχ = 10, eγ =5	-50.00-	50.00	50.00	50.13	-50.01-	-50.00-	-50.00-	-50.21-
BitRand	eχ = 1, eγ = ∞	59.8	60.65	567	64.57	-54.56-	-56.44	-64.93-	-5186-
	eχ =1, eγ = 1	-54.52-	55.03	53.37	56.99	-52.63-	-53.34	-57.43-	-50.91-
	eχ = 1, eγ = 2.5	-59.08-	58.20	55.36	62.02	-54.29-	-55.86-	-61.91-	-51.52-
	eχ =1, eγ =5	-59.98-	60.15	56.28	63.67	-54.14-	-56.91-	-63.98-	-51.77-
	eχ = 5, eγ = ∞	-67.15-	71.81	60.65	77.91	-58.16-	-61.82-	-76.15—	-54.99-
	eχ =5, eγ = 1	-58.04-	59.87	54:24	62.99	-54.05-	-55.67-	-62.35-	-5182-
	eχ =5, eγ = 2.5	-64.61-	67.70	58.78	73.56	-562-	-60.04	-72.09-	-53.50-
	eχ =5, eγ =5	-67.51-	71.41	60.94	77.96	-57.82-	-61.57-	-76.11-	-54.82-
	eχ = 10, eγ = ∞	-75.09-	81.96	70.14	90.22	-65.26-	-71.76-	-86.68-	-63.31-
	eχ = 10, eγ = 1 一	-6187-	64.88	59.04	68.82	-56.8-	-60.33-	-67.20-	-53.84-
	eχ = 10, eγ = 2.5	-71.35-	76.69	66.88	84.48	-62.74-	-68.33-	-81.52-	-58.16-
	eχ = 10, eγ =5	74.45	82.28	69.23 —	89.88	64.81	71.47	86.38	62.25
31
Under review as a conference paper at ICLR 2022
Table 5: AUC values of each algorithm applied on 5θu in the CelebA dataset. Average is the average
of all 40 attributes.
Attribute Algorithm [5θut with Anon]		AttraCtive	Heavy Makeup	High Cheekbones	Male	Mouth Slightly Open	Smiling	LipstiCk	Average
Noiseless		eχ = ∞		-7805-	85.47	7653	92.77	-72.91-	-79.15-	-88.85-	-6809-
DM	eχ = 1	—	-5796-	29.57	4645	61.43	-50.58-	-50.9-	-47.61-	-47.96-
	eχ = 5	-59.85-	39.72	51.01	53.36	-51.15-	-49.97-	-32.64-	-48.62-
		eχ = 10		-5633-	44.01	50:05	55.19	-49.8-	-52.05-	-30.79-	-50.00-
PM	eχ = 1	—	-51.87-	39.38	493	73.24	-49.41-	-489-	-52.99-	-49.66-
	eχ = 5	-4842-	40.86	46.84	72.35	-50.55—	-49.52-	-47.68-	-50.66-
	eχ = 10	-4902-	36.59	49:03	59.12	-50.39-	-50.46-	-53.36-	-49.22-
HM	eχ = 1	—	-463-	33.84	5246	42.45	-4833-	-50.98-	-4823-	-50.41-
	eχ = 5	-5239-	34.32	5249	29.79	-50.02-	-48.04	-49.90-	-49.37-
	eχ = 10	533	49.65	5223	35.33	-50.82-	-47.69-	-47.19-	-50.33-
Three outputs	eχ = 1	—	-45.91-	63.71	50.16	61.60	-5135-	-4539-	-61.21-	-50.58-
	eχ = 5	-35.89-	55.43	4895	55.22	-47.60-	-46.30-	-5841-	-50.39-
	eχ = 10	-35.03-	60.51	49.85	60.69	-47.70-	-43.97-	-54.05-	-49.88-
PM-SUB	eχ = 1	—	-61.56-	63.02	4678	38.77	-5223-	-5038-	-48.60-	-48.90-
	eχ = 5	-5859-	57.14	4893	31.43	-50.54-	-53.93-	-51.75-	-47.93-
	eχ = 10	-6509-	55.82	47.10	33.13	-49.70-	-5m-	-59.87-	-49.61-
f-RR and Label -LaplaCe	eχ =1, eγ = 1	-5000-	50.81	50:00	50.03	-50.00-	-50.00-	-50.00-	-50.12-
	eχ = 1, eγ = 2.5	-5000-	50.00	50:00	51.39	-50.00-	-50.00-	-50.00-	-50.20-
	eχ =1, eγ =5	-51.92-	50.06	503	51.41	-50.00-	-50.01-	-50.00-	-50.22-
	eχ =5, eγ = 1	-50.64-	50.00	50:07	50.01	-50.00-	-50.39-	-50.00-	-50.21-
	eχ =5, eγ = 2.5	-50.00-	50.23	50:00	49.99	-50.00-	-50.00-	-50.00-	-50.10-
	eχ =5, eγ =5	-51.87-	52.40	50:00	50.00	-50.00-	-50.00-	-50.00-	-50.21-
	eχ = 10, eγ = 1 一	-51.70-	50.00	50:00	50.00	-50.00-	-51.03-	-50.00-	-50.12-
	eχ = 10, eγ = 2.5	-50.01-	50.12	50:00	50.00	-50.00-	-50.05—	-51.12-	-50.16-
	eχ = 10, eγ =5	-50.00-	50.00	50:00	50.13	-50.01-	-50.00-	-50.00-	-50.21-
BitRand	eχ = 1, eγ = ∞	59.8	60.65	567	64.57	-54.56-	-56.44	-64.93-	-5186-
	eχ =1, eγ = 1	-54.52-	55.03	53:37	56.99	-52.63-	-53.34	-57.43-	-50.91-
	eχ = 1, eγ = 2.5	-59.08-	58.20	55:36	62.02	-54.29-	-55.86-	-61.91-	-51.52-
	eχ =1, eγ =5	-59.98-	60.15	56.28	63.67	-54.14-	-56.91-	-63.98-	-51.77-
	eχ = 5, eγ = ∞	-67.15-	71.81	60.65	77.91	-5816-	-61.82-	-76.15—	-5499-
	eχ =5, eγ = 1	-58.04-	59.87	54:24	62.99	-54.05-	-55.67-	-62.35-	-51.82-
	eχ =5, eγ = 2.5	-64.61-	67.70	5878	73.56	-56.2-	-60.04	-7209-	-53.50-
	eχ =5, eγ =5	-6751-	71.41	60.94	77.96	-57.82-	-61.57-	-76.T1-	-54.82-
	eχ = 10, eγ = ∞	-7509-	81.96	70.14	90.22	-65.26-	-71.76-	-86.68-	-63.31-
	eχ = 10, eγ = 1 一	-61.87-	64.88	59.04	68.82	-56.8-	-60.33-	-67.20-	-53.84-
	eχ = 10, eγ = 2.5	-71.35-	76.69	66.88	84.48	-62.74-	-68.33-	-81.52-	-58.16-
	eχ = 10, eγ =5	74.45	82.28	69.23 —	89.88	64.81	71.47	86.38	62.25
32