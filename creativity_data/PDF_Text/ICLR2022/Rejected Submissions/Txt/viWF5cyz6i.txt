Under review as a conference paper at ICLR 2022
An Efficient and Reliable Tolerance-Based
Algorithm for Principal Component Analysis
Anonymous authors
Paper under double-blind review
Ab stract
Principal component analysis (PCA) is an important method for dimensionality
reduction in data science and machine learning. But, it is expensive for large
matrices when only a few principal components are needed. Existing fast PCA al-
gorithms typically assume the user will supply the number of components needed,
but in practice, they may not know this number beforehand. Thus, it is important
to have fast PCA algorithms depending on a tolerance. For m × n matrices where
a few principal components explain most of the variance in the data, we develop
one such algorithm that runs in O(mnl) time, where l min(m, n) is a small
multiple of the number of principal components. We provide approximation error
bounds that are within a constant factor away from optimal and demonstrate its
utility with data from a variety of applications.
1	Introduction
1.1	The truncated SVD and PCA
Let A be an m × n real matrix and A = UΣV T its singular value decomposition (SVD). The rank-k
truncated SVD (rank-k TSVD) of A is the matrix Ak := UkΣkVKT, where Uk and Vk are the first
k columns of U and V , respectively, and Σk is the leading k × k block of Σ. The columns uj and
vj of Uk and Vk are the left and right singular vectors of A, respectively, and the diagonal entries
σι(A) ≥ ∙ ∙ ∙ ≥ σk(A) ≥ 0 are the singular values of A.
One common use of TSVD is in principal component analysis (PCA). This is a dimensionality
reduction technique that aims to find the directions in which the data varies the most. It turns out
that these directions are given by the top k right singular vectors vj , 1 ≤ j ≤ k. Projecting the
original data onto these directions then transforms the data from a high-dimensional space into a
lower-dimensional one. Typically, k is chosen so that these principal directions explain a certain
amount of variance in the data. For example, if the user wanted to explain 99% of the variance, they
would choose k so that Pik=1 σi(A)2/ Pin=1 σi(A)2 ≥ 0.99. PCA is used to compress data in a
variety of settings such as images, training data for machine learning algorithms, and even neural
network weight matrices (Xue et al., 2013).
In this work, we will be interested in a slightly different version of PCA. Let ε be a user-prescribed
tolerance, and instead choose k so that σk(A) ≥ ε ≥ σk+1(A). Since the ith principal component
explains σi(A)2/ Pin=1 σi(A)2 fraction of the total variance, we are essentially ignoring any com-
ponents that explain less than ε2/ Pin=1 σi(A)2 fraction of the total variance. This can be interpreted
as discarding principal components corresponding to noise, where ε describes the size of the noise.
Despite its utility and importance, the main drawback of using TSVD for PCA is that it is expen-
sive, especially when the user needs only the top few singular values/vectors. Thus, a large body
of research has been devoted to finding faster ways of computing it without sacrificing too much
accuracy.
1.2	Prior Work
The literature on fast, approximate TSVD algorithms typically assume the user knows what rank k
to use. Recent work uses randomization to reduce the run time while still maintaining a high level of
1
Under review as a conference paper at ICLR 2022
accuracy. See, for example, Rokhlin et al. (2010) and Halko et al. (2010) and the references therein
for typical examples of these types of algorithms. In Musco & Musco (2015), the authors present a
randomized algorithm based on block Krylov subspace methods to compute an approximate TSVD.
For a matrix A, rank k, and tolerance ε, the algorithm produces a matrix Z whose columns approxi-
mate the top k left singular vectors of A and such that A - ZZTA2 ≤ (1 + ε) kA - Akk2. They
also prove stronger bounds on the quality of the singular vectors, which is important for PCA. This
algorithm is especially suited to sparse matrices, which can be multiplied quickly.
In some cases, the user may not know ahead of time what k to use, so it is useful to consider
algorithms which accept a desired precision ε as input rather than rank. Algorithms in this vein
incrementally build a matrix Q with orthogonal columns and another matrix B until kA - QB k < ε.
Typically, the number of columns of Q (or the number of rows of B) is quite small so that QB is a
compact approximation of the original data A. An approximate TSVD of A can then be produced
from the SVD of B. Recent work again uses randomization to reduce the run time. We present a
prototypical example of this style of algorithm in Algorithm 1 (Yu et al., 2018). See Halko et al.
(2010) and Martinsson & Voronin (2016) for more examples. While these algorithms guarantee a
small approximation error, there are no guarantees on the accuracy of the singular values or vectors.
We will compare the accuracy for Algorithm 1 to the proposed algorithm in Experiments.
Algorithm 1 The randQB_EI algorithm for the fixed-precision problem
Input: an m × n matrix A; desired accuracy tolerance ε; block size b
Output: Q, B such that kA - QB kF < ε
Q = []; B = [];
E= kAk2F
for i = 1, 2, 3, . . . do
Ωi = randn(n, b)
Qi = Orth(AΩi - Q(BΩi))
Qi = orth(Qi - Q(QTQi))
Bi = QiTA
Q= [Q, Qi]
E=E- kBik2F
if E < ε2 then stop
end for
1.3	Our Work
In this work, we propose an algorithm that, for a matrix A, accuracy tolerance δ, and singular value
tolerance ε, produces an approximate TSVD Ak satisfying:
1.	The rank k of Ak does not exceed the true rank of A, determined by the tolerance ε as
described above,
,≈ , , .. .
2.	σj(Ak) ≥ (1 - δ)σj(A) for 1 ≤ j ≤ k,
3.	∣∣A — AklL ≤ 1+δε ≈ (1 + 2δ)ε, and
4.	If k coincides with the true rank, then ∣∣A - Ak∣∣ ≤ (1+δ)σk+1(A) = (1+δ) ∣∣A - Ak |卜，
i.e. the truncation error is a factor of 1 + δ from optimal.
These properties are verified in the Appendix. The algorithm thus yields a high-quality approxi-
mation to TSVD and can be used in applications as an approximate PCA. The algorithm is fast for
matrices whose singular values decay quickly when ε is set so that k will be relatively small.
2
Under review as a conference paper at ICLR 2022
2	Preliminaries
Unless otherwise stated, we will consider matrices with more rows than columns. For a matrix with
more columns than rows, apply the algorithm to its transpose.
2.1	Flip-Flop Spectrum Revealing QR
The proposed algorithm is essentially a tolerance-based version of Flip-Flop Spectrum Revealing
QR (FFQR) (Feng et al., 2019). For a matrix A and integers k ≤ l, FFQR produces an approximation
to the rank-k TSVD Ak whose accuracy depends on the ratio σk+1(A)∕σ1+1(A). Thus, if A has
rapidly decaying singular values, FFQR will be close to TSVD.
FFQR is computed as follows. Let A be an m × n matrix (m ≥ n) and k ≤ l. Perform l steps of
Randomized QR with Column Pivoting (RQRCP) (Duersch & Gu, 2017) to get the factorization
AΠ = QR = Q
R12
R22
where Π is an n × n permutation matrix, Q is an m × m orthogonal matrix, R is m × n, and R11 is
an l × l upper triangular matrix.
The next phase of FFQR involves performing extra “spectrum-revealing” column swaps on R
and using Givens rotations to restore its upper trapezoidal form. These swaps ensure kR22 k2 =
O(σl(A)). See Xiao et al. (2017) for more details.
Next, perform l steps ofQR on RT to get
RT =PLT =(P1 P2) LL1211 L022T,
where P is an n × n orthogonal matrix, P1 is its leading l columns, L is an m × n matrix, and L11
is l × l lower triangular. Putting the above together yields
A=QRΠT = Q LL1211 L022PTΠT.
Discard L22 (as in truncated QRCP) and approximate (L11) With its rank-k TSVD Uk Σk VkT:
Q (L；1	L(L) PTnT ≈ Q (L21) PT ∏t ≈ Q(Uk ∑ k VT 间 ∏t
Setting Uk := QUJk, Σk := Σk, Vk := ∏P1Vk gives the rank-k approximation A ≈ Uk∑k VT.
In Feng et al. (2019), the authors prove the following bounds for FFQR. Given ε > 0 and g > 1,
there are matrix-dependent quantities gι ≤ 11-y+∣, g2 ≤ g, T ≤ gιg2P(T+Γ)(n-T), and T ≤
g1g2 ʌ/l(n - l) such that for 1 ≤ j ≤ k,
and
σj (Σk) ≥
σj(A)
A - Uk ςk Vkl2 ≤ σk+1(A) f1 + 2τ4 ( E )
2.2	The QLP Decomposition
The basis of FFQR is the QLP decomposition (Stewart, 1999). Let Abe an m × n matrix. Perform
QRCP on A to obtain AΠ = QR and then perform QRCP on RT to get RTΠ1 = PLT , where
3
Under review as a conference paper at ICLR 2022
L is lower triangular. Putting these together yields A = QΠ1LPTΠT. This is the pivoted QLP
decomposition of A. Stewart observed that the diagonal entries Lii of L closely track the singular
values of A.
For the proposed algorithm, we choose not to pivot when factoring RT . In this case, Lii will not
track σi(A) as well. We will discuss a partial remedy for this below. The advantage of not pivoting
is that, just as in FFQR, we do not have to finish computing R before computing L. Once we have
performed l steps of QRCP on A, we can compute the first l rows of L and thus have access to its
first l diagonal entries.
3	A Fast, Approximate, Tolerance-Based PCA Algorithm
3.1	Blocked FFQR
Since we do not know what l is beforehand, we compute R and L incrementally in blocks. Select a
block size b and perform b steps of RQRCP to get
AΠ1 = Q1 R01(b1) RR12((bb22))! ,
where R1(b1) is b × b upper triangular. The first b rows of R are essentially done since subsequent
steps of RQRCP will only permute the columns of R1(b2). Perform QR on them (to keep the notation
simple, we write this as an LQ factorization):
R(1b1) R(1b2) = (L11 0)P1T,
where L11 is b × b lower triangular, and P1 is orthogonal. We have just computed the first b rows
of L and know the first b diagonal entries. For the next block, continue RQRCP for another b steps.
The permutation matrix Π2 in this block will affect only columns b + 1 through n, leaving the first
b columns untouched. Thus, Π2 can be written in block form as Π2 = Ib	0 , where Ib is the
0 Π2
b × b identity matrix and Π2 is an (n - b) × (n - b) permutation matrix. We now have
	R(1b1)	R、12)n2、
A∏i∏2 = Q2Q1	0 0	p(2b) ^^p(2b)- r11	r12 0	R22b)
where R1(21b) is b × b upper triangular. Since the first b rows have changed, we must account for this
in the previous LQ:
(R(I) R(M) = (R(I) R(2)”2 = (Lii 0) PT ∏2.
Now apply the matrix Π2T P1 to the newly completed rows 0 R(121b) R(122b) and perform LQ on
the last n - b columns to get
(0 R(121b) R(122b) ) Π2T P1 = (L21 L22 0)P2T,
where L22 is b × b lower triangular.
The orthogonal matrix P2 affects only the last n - b columns and can therefore be written in block
form as P? =	P ). Hence, (Lιι 0) = (Lιι 0) PT and
R11)	R12)ππ2	! _ fL11	0	0A PT PT∏
0 IR(Ib) Rl2b) ) = 51 L22 0) P2 PI π2,
showing that we have computed the first 2b rows of L. We can continue this procedure, computing
b rows of L at a time. Once we decide to stop, we finish the remaining rows of L by applying the
4
Under review as a conference paper at ICLR 2022
orthogonal matrices from all previous LQ factorizations to the last rows of R. For example, if we
wanted to stop after 2 blocks, apply Π2TP1P2 to 0 0 R(222b) to get
0 0 R2(22b) Π2TP1P2 = (L31 L32 L33)
and the partial QLP decomposition
(R11)	R(2)∏2 、ʌ	/L11	0	0 ∖
A∏i∏2 =	Q2Q1	0	R11b)	R(2b)	I	= Q2Q1 L21	L22	0 PTPT∏2.
∖	0	0	R22b))	'L31	L32	L33，
Afterwards, spectrum-revealing swaps can be performed if desired. For each swap and upper-
trapezoidal restoration, some nonzero entries will appear above the diagonal in L. These are easily
eliminated with Givens rotations.
3.2	DETERMINING l
We now derive a criterion to determine when to stop factoring in blocked FFQR and to find l. Let
ε be the tolerance parameter, and define the rank k by σk+1 (A) ≤ ε ≤ σk(A). One could use the
bounds derived in Feng et al. (2019), using the diagonal entries of L to estimate the singular values
of A and stopping when σ1+1(A)∕σk+1(A) is sufficiently small. However, the dimension-dependent
bounds for T and T are impractical, so We will use a different bound.
In Feng et al. (2019), the authors prove that σj (A)4 ≤ σj(Σk)4+2 kR22 k24 , 1 ≤ j ≤ k. Rearranging
this inequality gives
σj (∑k) ≥ σj (A) ^1-2≡2AI,	1 ≤ j ≤ k.
They also prove the following bound on the truncation error:
A - Uk ς k Vkl2 ≤ σk+ι(A) +1 + 2 σ≡⅛.
(1)
These bounds hold even without spectrum-revealing swaps. So, ifkR22k2 /σk+1(A), is small, then
the leading k singular values of A will be revealed up to a certain number of digits and UkΣkVkT will
be a nearly optimal rank-k approximation. In practice, the above two bounds are sufficient because
kR22k2 = O(σl(A)) already, without extra swaps. The earlier bounds still have theoretical value in
that they show the algorithm works well when Ahas rapidly decaying singular values.
The factors ʌ/1- 2k¾k[ and ʌ/1 + 2^22k24 are equal to 1 - ɪ k¾k4 and 1 + ɪ「叱臬,
σj (A)4	σk+1 (A)4	2 σj (A)4	2 σk+1 (A)4
respectively, UP to first order. Introduce an accuracy parameter δ, and say we have 2 σkR22A)4 ≤ δ.
Then we have σj(Σk) ≥ σj(A)(1 - δ), 1 ≤ j ≤ k, and l∣A - Uk∑kVrh ≤ σk+ι(A)(1 + δ) up to
first order. This means that ≈ - log δ digits of the top k singular values of Aand optimal truncation
error have been computed correctly. We can rewrite 1 σkR22Aj4 ≤ δ as ∣∣R22∣∣2 ≤ σk+ι(A)√2δ.
This is the tolerance-based criterion to determine l.
3.2.1	ESTIMATING σk+1 AND kR22k2
To estimate kR22k2 and σk+1(A) accurately, we use Stewart’s observation that the diagonal entries
Lii of L closely track the singular values σi(A) of A. As stated above, Lii will not track σi(A) as
well because we are not pivoting when factoring RT . A partial remedy is simply to sort the Lii ’s.
We show below that the resulting tracking behavior is similar in quality to that of fully pivoted QLP.
Let Lj) be the j-th largest diagonal entry of L in magnitude, i.e. ∣ L(1) ∣ ≥ ∣L(2)∣ ≥ ∙∙∙ ≥ ∣L(n)∣. In
light of Stewart’s observation, we will assume that there are constants α and β such that α ∣L(j)∣ ≤
σj(A) ≤ β ∣L(j)∣, 1 ≤j ≤ n. The values of α and β will be estimated empirically below. A simple
5
Under review as a conference paper at ICLR 2022
way to interpret these inequalities is that for each diagonal entry Ljj , there is a singular value of
A in the interval [α |Ljj | , β |Ljj |]. Consider {Ljj : β |Ljj| ≤ ε}. This just corresponds to all the
intervals [α |Ljj| , β |Ljj|] contained in (-∞, ε]. For each Ljj in the set, there is a singular value
σi(A) in the corresponding interval. Thus σi(A) ≤ ε. Since σk+1(A) is the largest singular value of
A less than or equal to ε, we must have σi(A) ≤ σk+1(A), which then implies α |Ljj | ≤ σk+1 (A).
This yields a lower bound on σk+1 (A), namely max{α |Ljj | : β |Ljj | ≤ ε}.
Since we will not know all the Ljj ’s, we can obtain only a sub-optimal lower bound sk+1 on
σk+1(A). Initialize sk+1 = 0. After i blocks of blocked FFQR, update sk+1 = max{α |Ljj| :
β |Ljj | ≤ ε and j ≤ ib}.
To estimate kR22 k2 = σ1 (R22), we will use the first diagonal entry L11 of L in the fully pivoted
QLP factorization. First, consider a general matrix A, and perform QRCP: AΠ = QR. Then in
fully pivoted QLP, |L11| is just the largest row norm of R. As noted in Stewart (1999), the largest
row of R is usually among the first few rows. Thus we can estimate kAk2 using max kR(i, :)k2,
2	1≤i≤q	2
for some small integer q. This requires only q steps of QRCP.
We can apply this idea to estimate kR22 k2 . After i steps of QRCP on A, the R factor has the form
0RR(1i)(2i2)
, where R(1i) is i × n upper triangular. The R factor in the QRCP factorization of R(2i2) is
just R1(n) (i+1 : m, i+1 : n). Thus, QRCP-factoring A automatically yields the QRCP factorizations
of all the trailing blocks R(2i2) . Using the 2-norm estimation scheme in the previous paragraph, after
j steps of QRCP, we have the upper bound R2(i2) ≤ β≤m+qJRj%, W=βM2L for
1 ≤i≤j-q+1.
Putting these estimates together will give us the final stopping criterion. After each block, we first
update sk+ι with the newly computed Ljj's and then check if ∣∣R22)∣∣	≤ 1 sk+ι √2δ for some i.
The smallest i for which this inequality holds will be l + 1.
Algorithm 2 Approximate, tolerance-based PCA
Inputs: A, tolerance ε, accuracy δ, block size b, number of rows q, oversampling size p for
RQRCP
Outputs: Rank k, Uk, Σk, 女
C - 0, sk+1 - 0
while c < n do
Perform steps c + 1 to c + b of RQRCP on A; update Q, R, and Π
Compute rows c + 1 to c + b of L; update P
for j = c + 1 : c + b do
if |Ljj | ≤ ε∕β and a |Ljj | ≥ sk+ι then
sk+1 J α ILjj |
end if
end for
for i = 1 : c + b - q + 1 do
if ∣∣R22)∣∣c ° ≤ 1sk+ι √2δ then
lJi-1
exit while loop
end if
end for
cJc+b
end while
Compute rows c + b + 1 to m of L.
Compute TSVD U∑k匕 ofL(:, 1 : l), where k satisfies σk(L(:, 1 : l)) ≥ ε ≥ σk+ι(L(:, 1 : l)).
Uk J QUk, ∑k J ∑k, Vk J ∏PιV4
6
Under review as a conference paper at ICLR 2022
Figure 1: Singular value distributions of the test matrices
Table 1: α and β values for the test matrices under the three schemes.
	UnPivoted		Sorted		Pivoted	
	α	β	α	β	α	β
Random	0.710	1.39	0.742	1.33	0.745	lɪ
Data	0.523	2.75	0.793	1.17	0.817	1.17
Video	0.321	2.85	0.838	1.85	0.840	1.58
Kernel	0.635	1.60	0.802	1.32	0.817	1.31
4	Experiments
We use the following test matrices for our experiments:
1.	A random 3000 × 3000 matrix with singular values decaying geometrically from 1 down to
10-12 . We generate a 3000 × 3000 matrix with entries from a standard normal distribution,
compute its SVD UΣV T , and replace the diagonal of Σ with the desired singular value
distribution.
2.	A 2003 × 2003 data matrix (bcsstk13) from the SuiteSparse matrix collection (Davis & Hu,
2011). This matrix arises from a computational fluid dynamics problem. Its singular values
decay from ≈ 1012 down to ≈ 102.
3.	A 19200 × 5322 matrix generated from a video from the UCF-Crime dataset (Sultani et al.,
2018). The original video was a 240 × 320 RGB video consisting of 5322 frames. We
resized the video by half to 120 × 160, converted it to grayscale, flattened each frame into
a 19200 × 1 column vector, and then stacked these horizontally to form the final matrix.
4.	A 5000 × 5000 kernel matrix generated from 5000 data points from the MNIST handwritten
digits dataset (Lecun et al., 1998). We used the kernel function k(x, x0) = e-γkx-x0k ,
where γ = 1/(median of pairwise distances between data points)2.
The singular values of each test matrix are plotted in Figure 1.
4.1	ESTIMATING α AND β
For each matrix A, we tested three singular value estimation schemes. See Table 1. For the first two
columns (“Unpivoted”), we ran RQRCP to get AΠ = QR and then QR-factored RT = PL. We
recorded the minimum (α) and maximum (β) values of σi(A)/ |Lii|. For the second two (“Sorted”),
we recorded the minimum and maximum values of σi(A)/ L(i) , where L(i) is the ith largest diago-
nal entry of L in magnitude. For the last two columns (“Pivoted”), we QRCP-factored RTΠ1 = PL
and then recorded the minimum and maximum values of σi(A)/ |Lii|.
We observed that the tracking behavior can break down when σi (A) is smaller than machine preci-
sion and thus ignored ratios corresponding to such σi (A) when computing the minimum and max-
imum values. Therefore, it is recommended that the tolerance ε be set at least a small factor above
machine epsilon.
7
Under review as a conference paper at ICLR 2022
Figure 2: Relative singular value errors for the test matrices. Our algorithm is the bold line,
randQB_EI is the thin one.
“Sorted” and “Pivoted” have similar α and β values, with the latter slightly better overall, while
“Unpivoted” tends to be worse than the other two. Based on the middle two columns, it seems that
α ≈ 0.7 and β ≈ 2 are reasonable values.
4.2	Comparison to TSVD and RANDQB_EI
Here We compare the proposed algorithm to TSVD and randQB_EI. Tests were coded in Fortran and
run on a laptop with a 2.00 GHz Intel i7-4510U CPU with 16.0 GB of RAM.
First, we compare the proposed algorithm to TSVD. To compute the latter, the LAPACK routine
dgesdd routine is used to compute the full SVD, which is then truncated based on the tolerance ε.
For the random, data, and kernel matrices, tolerances corresponding to 99% explained variance are
chosen. For the video matrix, we choose one corresponding to 99.9% explained variance because
the first principal component already accounts for 99% of the variance.
It is not so simple to translate a Frobenius norm tolerance to a corresponding 2-norm tolerance,
but we find that for matrices with geometrically decaying singular values, 99% explained variance
roughly corresponds to a tolerance of 0.1 kAk2. The 2-norm of A can be estimated after the first
block of FFQR. For experimental purposes, we computed the SVD of each matrix and then selected
the tolerance.
We set δ = 10-4 for all test matrices because machine learning algorithms typically only need a
few digits of accuracy. But the larger singular values are computed with more accuracy because the
accuracy of the jth singular value depends on kR22k2 /σj (A). Finally, for all matrices, we set the
block size b = 64, number of rows q = 5, and oversampling size p = 5.
The results are listed in Table 2. The proposed algorithm detects the rank k correctly for each test
matrix and is much faster than dgesdd. The column REoTE contains the relative error in the optimal
.	」	kA-Akk。，
truncation error -‰-2-— — 1
kA-Akk2
This is always bounded above by δ, but we see that in practice this
relative error is much smaller.
Figure 2 plots the relative errors in the approximate singular values ∣1 — 2(*) ∣, 1 ≤ j ≤ k for
each of the test matrices. These are again bounded by δ. As expected, the larger singular values
are computed more accurately. We also plot the relative errors in the singular values computed by
randQB_EI, using block size 64; ε = √0.01 ∣∣A∣∣f for the random, data, and kernel matrices; and
ε = √0.001 IIAkF for the video matrix. The authors of randQB_EI also include a power parameter
P in their implementation. We set P = 1 as in their paper.
Although we did not analyze the error in the singular vectors (in general, the error depends on
the gap between the singular values), we compute the angles θ(vj ,Vj) between the right singular
vectors and their approximations for both our algorithm and randQB_EI, and plot them in Figure
3. For our algorithm, the angles are all quite small, so the proposed algorithm finds good-quality
approximations to the true singular vectors/principal directions.
Accuracy-wise, our algorithm performs better than randQB_EI, except on the kernel matrix. We
found that setting P = 0 causes the accuracy of randQB_EI to drop below ours. The first few
8
Under review as a conference paper at ICLR 2022
Figure 3: Angle between the top k right singular vectors Vj and their approximations Vj for the test
matrices. Our algorithm is the bold line, randQB_EI is the thin one.
Table 2: Comparison of the proposed algorithm to TSVD.
		dgesdd		Proposed algorithm				
Matrix	ε	k	Time(s)	l	k	Time (s)	REOTE	Speed-up
Random	1 × 10-1	^250	14.9	804	250	2.76	1.11 × 10-16	-^539×-
Data	1 X 1011	128	4.41	759	128	1.38	2.22 × 10-16	3.19×
Video	5.6 × 103	36	161	1692	36	50.2	9.55 × 10-15	3.21×
Kernel	7.68 × 101	7	72.7	370	7	2.75	1.11 × 10-15	26.4×
singular values of the kernel matrix are extremely large compared to the rest; thus, performing even
just one power iteration effectively enhances the accuracy of randQB_EI on this matrix.
5	Conclusion
In this work, we developed an efficient algorithm for computing an approximate truncated SVD.
In contrast to much of the literature, this algorithm truncates according to a tolerance rather than
a fixed rank. We have also demonstrated that it provides high-quality approximations to both the
singular values and vectors of the original matrix, thus making it suitable for use in applications as
an approximate PCA.
Acknowledgments
We would like to thank Jed Duersch for providing his code for RQRCP.
References
Timothy A. Davis and Yifan Hu. The university of florida sparse matrix collection. ACM Transac-
tions on Mathematical Software, 38(1):1-25, 2011.
Jed A. Duersch and Ming Gu. Randomized qr with column pivoting. SIAM Journal on Scientific
Computing, 39(4):C263-C291, 2017.
Yuehua Feng, Jianwei Xiao, and Ming Gu. Low-rank matrix approximations with flip-flop spectrum-
revealing qr factorization. Electronic Transactions on Numerical Analysis, 51:469-494, 2019.
Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions, 2010.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
Per-Gunnar Martinsson and Sergey Voronin. A randomized blocked algorithm for efficiently com-
puting rank-revealing factorizations of matrices. SIAM Journal on Scientific Computing, 38(5):
S485-S507, 2016.
9
Under review as a conference paper at ICLR 2022
Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster
approximate singular value decomposition. In Corinna Cortes, Neil D. Lawrence, Daniel D.
Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec Canada ,pp.1396-1404, 2015.
Vladimir Rokhlin, Arthur Szlam, and Mark Tygert. A randomized algorithm for principal compo-
nent analysis. SIAM Journal on Matrix Analysis and Applications, 31(3):1100-1124, 2010.
G. W. Stewart. The qlp approximation to the singular value decomposition. SIAM Journal on
Scientific Computing, 20(4):1336-1348, 1999.
Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance
videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Jianwei Xiao, Ming Gu, and Julien Langou. Fast parallel randomized qr with column pivoting algo-
rithms for reliable low-rank matrix approximations. In 2017 IEEE 24th International Conference
on High Performance Computing (HiPC), pp. 233-242, 2017. doi: 10.1109/HiPC.2017.00035.
Jian Xue, Jinyu Li, and Yifan Gong. Restructuring of deep neural network acoustic models with
singular value decomposition. In Interspeech, January 2013.
Wenjian Yu, Yu Gu, and Yaohang Li. Efficient randomized algorithms for the fixed-precision low-
rank matrix approximation. SIAM Journal on Matrix Analysis and Applications, 39(3):1339-
1359, 2018.
A Appendix
A.1 Verification of Properties 1-4
T .1 ∙ 1	1 . . 1	1 7	.1	1 1 . . 1 1	1 ∙ . 1	7	1
In this subsection, we denote the true rank as ktrue, the rank detected by our algorithm as k, and
the matrix output by our algorithm AQ Recall 展…is defined by o^rue+1(A) ≤ ε ≤ σktrue (A).
The detected rank k is determined as follows. We run Blocked FFQR until the trailing block of R
is small enough and then take l to be the smallest integer such that kR(l + 1 : m, l + 1 : n)k2 ≤
σktrue+ι (A) 42δ. Denote R(l + 1 : m,l + 1 : n) by R22 for short. Afterwards, compute the SVD
of Li := L(:, 1 : l) and define k by σ^ι(Lι) ≤ ε ≤ σ%(Li).
First, note that k ≤ ktrue. To see this, first observe that ktrue = #{j : σj (A) > ε}. By the Cauchy
Interlacing Theorem, σj(Li) ≤ σj (A), 1 ≤ j ≤ l. Thus we can only shift the singular values of A
downward, which will not increase the size of the above set. This proves Property 1.
For Property 2, it follows from computations in Feng et al. (2019) that σj (A)4 ≤ σj4(Li)+2 kR22 k42,
or
(T 、、	∕4x 4 ʌ i7kR22∣∣2 〜 Z41 ∣∣r22∣∣2∖ Iw ∙ W ]
σj(LI) ≥ σj(A)VI-2σjw 〜σj(A) C- 2oTW), 1 ≤j ≤ l∙
Plugging in ∣∣R22k2 ≤。卜…+ι(A)42δ gives σj(Li) ≥ σj(A)(1 - δ) for 1 ≤ j ≤ 厩语 + 1 and
♦	. ∙	1 r∙ T ,	∙	, 7 EI ∙	∙	1 ʌ	, C
in particular for 1 ≤ j ≤ k . This is Property 2.
For the last two properties, we refer to Equation 1. In the notation for this section, it reads:
A -闻 L ≤ *1(A) +1+2 4r⅛ ≈ 唳+1(A)
Λ + 1 lai；
1+2 σm(A)4
Again, plugging in ∣∣R22∣∣2 ≤ σktrue+ι(A)42δ and using the fact that k + 1 ≤ k^e + 1 gives
∣∣A - A小 ≤ σ^+ι(A)(1 + δ). We see that if k = kt”, then this inequality is Property 4.
10
Under review as a conference paper at ICLR 2022
Finally, from the proof ofProperty 2 above, we have σ^+j(Lι) ≥ σ 工+ι(A)(1-δ). Thus, σ 工+ι(A) ≤
1⅛ σfe+ι(Lι) and IIA - AfL ≤ 皆 σfe+ι(Lι) ≤ τ⅛δ ε, WhiChisPrOPerty 3.
Note that k < ktrue only when there are singular values slightly above the toleranCe. The toleranCe-
based Criterion ensures that up to first order σj(L1) ≥ σj(A)(1 - δ). So only singular values
satisfying σj (A) ≥ ε ≥ σj (A)(1 - δ) Can be perturbed below ε and deCrease the rank.
11