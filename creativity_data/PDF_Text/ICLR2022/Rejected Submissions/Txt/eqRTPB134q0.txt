Under review as a conference paper at ICLR 2022
Invariance in Policy Optimisation and Partial
Identifiability in Reward Learning
Anonymous authors
Paper under double-blind review
Ab stract
Designing reward functions for complex, real-world tasks is challenging. Reward
learning lets one instead infer reward functions from data. However, multiple re-
ward functions often fit the data equally well, even in the infinite-data limit. Prior
work often considers reward functions to be uniquely recoverable, by imposing
additional assumptions on data sources. By contrast, we formally characterise the
partial identifiability of popular data sources, including demonstrations and tra-
jectory preferences, under multiple standard sets of assumptions. We analyse the
impact of this partial identifiability on downstream tasks such as policy optimisa-
tion, including under shifts in environment dynamics. We unify our results in a
framework for comparing data sources and downstream tasks by their invariances,
with implications for the design and selection of data sources for reward learning.
1	Introduction
A wide range of problems can be represented as sequential decision-making tasks, where the goal
is to maximise some numerical reward (Sutton & Barto, 2018). However, designing an appropriate
reward function remains a challenge in complex real-world tasks (Amodei et al., 2016; Leike et al.,
2018; Dulac-Arnold et al., 2019). Reward learning algorithms infer task reward functions from data
sources such as expert demonstrations (Ng & Russell, 2000), preferences over trajectories (Chris-
tiano et al., 2017), and many others (Jeon et al., 2020). This approach has extended the applicability
of sequential decision-making techniques to more complex tasks (e.g. Abbeel et al., 2010; Christiano
et al., 2017; Singh et al., 2019; Stiennon et al., 2020).
Multiple reward functions are often consistent with the data source, even in the infinite-data limit.
For most data sources, this fundamental ambiguity has been acknowledged, but its extent has not
been characterised. In section 3, we formally characterise the ambiguity of several popular data
sources including expert demonstrations and trajectory preferences. These infinite-data limits bound
the information recoverable from finite data sets using any algorithm, so they are useful for evaluat-
ing algorithms relative to their limits, and data sources relative to each other.
Uniquely identifying a reward function is unnecessary when all plausible reward functions lead to
the same downstream outcome in a given application, such as policy optimisation. Characterising
this ambiguity tolerance for various applications allow us to evaluate the ambiguity of a data source
relative to a given application. Learnt reward functions are often used for policy optimisation,
for example via reinforcement learning (RL)1. In section 3 we formally characterise the ambiguity
tolerance of policy optimisation under arbitrary dynamics.
Ambiguity and ambiguity tolerance are formally related. Both concern invariances - of data sources
or downstream outcomes - to reward function transformations. Thus, our main contribution is to
catalogue the invariances of various mathematical objects derived from the reward function. In
section 4, we explore a partial order on these invariances, and its implications for the selection and
evaluation of data sources, addressing an open problem in reward learning (Leike et al., 2018, §3.1).
1We focus on RL applications. Further applications arise in other fields, where reward functions are used in
models to understand and predict the behaviour of humans, animals, and other systems (see, e.g., Schoemaker,
1982; Dennett, 1989; Rust, 1994; Howes et al., 2014; Peterson et al., 2021; Collins & Shenhav, 2021).
1
Under review as a conference paper at ICLR 2022
R
increasing
ambiguity
(tolerance)
πβ
∏<
」-- 一一 <z'、
/	--斗
Mξ^^
,β ,et CmPl
2 M<∕
k Yξ
>*
、
、-
(b)
(a)
Q
δ
Figure 1: (a) The infinite-data ambiguity of reward learning data sources, and the ambiguity toler-
ance of downstream applications of a learnt reward function, are both invariances of objects derived
from reward functions (sections 1.2 and 3). These invariances have a partial order (section 4): here,
X → Y means that Y can be derived from X, or equivalently that Y is at least as ambiguous as
X . The objects are: the reward function itself (R); Q-functions (Q); Maximum Entropy (β) and
supportive optimal policies (<) and their induced trajectory distributions (∏β, ∆g and ∏<, △*); the
return function restricted to partial and full trajectories (Gζ, Gξ); Boltzmann-distributed (β) and
noiseless (<) comparisons between these trajectories (We, We and W<, W<). (b) Several basic fami-
lies of reward transformations form the basis for our main results (section 2). These transformations
exist in a related hierarchy, within (shown here) and across tasks (section 4).
1.1	Related work
Inverse reinforcement learning (IRL; Russell, 1998) is the prototypical example of reward learning.
IRL infers a reward function from the behavioural data of a task expert by inverting a model of the
expert’s planning algorithm (Armstrong & Mindermann, 2017; Shah et al., 2019). Existing work
partially characterises the inherent ambiguity of behaviour for certain planning algorithms (Ng &
Russell, 2000; Cao et al., 2021) and classes of tasks (Dvijotham & Todorov, 2010; Kim et al., 2021).
We extend these results to more planning algorithms and arbitrary time-unbounded, stochastic tasks,
using a more expressive space of reward functions that reveals novel ambiguity.
Reward learning models have been proposed for many other data sources (Jeon et al., 2020). A
popular and effective data source is preferences over behavioural trajectories (Akrour et al., 2012;
Christiano et al., 2017). Unlike for IRL, the ambiguity arising from these data sources has not been
formally characterised. We contribute a formal characterisation of the ambiguity for central models
of evaluative feedback including trajectory preferences.
Several studies have explored learning from expert behaviour and preferences (Ibarz et al., 2018;
Palan et al., 2019; Biyik et al., 2020; Koppol et al., 2020), or other multi-modal data sources (Tung
et al., 2018; Jeon et al., 2020). One motivation is that different data sources may provide comple-
mentary reward information (Koppol et al., 2020), eliminating some ambiguity. Similarly, Amin
et al. (2017) and Cao et al. (2021) observe reduced ambiguity by combining behavioural data across
multiple tasks. Our partial order provides a general framework for understanding these results.
Computing an optimal behavioural policy is a primary application of learnt reward functions (Abbeel
& Ng, 2004; Wirth et al., 2017). Ng et al. (1999) proved that potential shaping transformations
always preserve the set of optimal policies, and so are always tolerable for this application. We
extend this result, characterising the full set of transformations that preserve optimal policies in each
task, including for additional policy optimisation techniques such as maximum entropy RL.
Ambiguity corresponds to the partial identifiability (Lewbel, 2019) of the reward function modelled
as a latent parameter. The prevailing response to partial identifiability in reward learning has been to
impose additional constraints or assumptions until the data identifies the reward function uniquely
(or, at least, sufficiently for policy optimisation). Following Manski (1995; 2003) and Tamer (2010),
we instead describe ambiguity given various constraints and assumptions. This gives practitioners
results appropriate for their real data (and the ambiguity tolerance of their actual application).
2
Under review as a conference paper at ICLR 2022
IRL is related to dynamic discrete choice (Rust, 1994; Aguirregabiria & Mira, 2010), a problem
where identifiability has been extensively studied (e.g., Aguirregabiria, 2005; Srisuma, 2015; Ar-
cidiacono & Miller, 2020). We study a simpler setting with known tasks. IRL also relates to prefer-
ence elicitation (Rothkopf & Dimitrakakis, 2011) and inverse optimal control (Ab Azar et al., 2020).
Preferences over sequential trajectories are not typically considered as a data source in other fields.
1.2	Preliminaries
We consider an idealised setting with finite, observable, infinite-horizon sequential decision-making
environments, formalised as Markov Decision Processes (MDPs; Sutton & Barto, 2018, §3). An
MDP is a tuple (S, A, τ, μo, R, Y) where S and A are finite sets of environment states and agent ac-
tions ;T : S XA → ∆(S) encodes the transition distributions governing the environment dynamics;
μo P ∆(S) is an initial state distribution; R : S XAXS → R is a deterministic reward function 2;
and γ P (0,1) is a reward discount rate. We distinguish states in the support of μo as initial states,
and states s with τ (s|s, a) “ 1 and R(s, a, s) “ 0 for all a as terminal states.
We represent the transition from state s to state s1 using action a as the tuple x “ (s, a, s1). We clas-
sify (s, a, s1) as possible in an MDP ifs1 is in the support ofτ(s, a), otherwise it is impossible. A tra-
jectory is an infinite sequence of concatenate transitions ξ “ (s0, a0, s1, a1, s2, . . .), anda trajectory
fragment of length n is a finite sequence ofn concatenate transitions ζ “ (s0, a0, s1, . . . , an´1, sn).
A trajectory or fragment is possible if all of its transitions are possible, and is impossible otherwise.
A trajectory or fragment is initial if its first state is initial. A state or transition is reachable if it is
part of some possible and initial trajectory.
Given an MDP, we define the return function G as the cumulative discounted reward of entire tra-
jectories and trajectory fragments: G(Z) “ Xn´1 YtR(st, at, st`i) for a trajectory fragment Z of
length n, and similarly for trajectories. We primarily consider this return function with various
restricted domains, such as only possible or initial trajectories or trajectory fragments.
A policy π : S → ∆(A) encodes an agent’s behaviour as a state-conditional action distribution.
Together with an MDP’s transition distribution τ, a policy π induces a distribution of trajectories
starting from each state. We denote such a trajectory starting from s with the random variable Ξs,
and its remaining components with random variables A0 , S1 , A1 , S2, and so on.
Given an MDP and a policy π, and the value function encodes the expected return from states,
Vn(s) “ Eξs~∏,t[G(Ξs)]; and the Q-function of ∏ encodes the expected return given an initial
action, Q∏ (s, a) “ Eξs~∏,t [G(Ξs) ∣ Ao “ a]. Q∏ and V∏ satisfy a Bellman equation:
Q∏(s,a) = ES1„T (s,a)[R(s,a,s1) + YV∏ (S1)‰,	V∏ (S) = EA~∏(s)[Q∏(s, A)‰,	⑴
for all s P S and a P A. Their difference, Aπ (s, a) “ Qπ (s, a) ´ Vπ (s), is the advantage function.
We further define a policy evaluation function, J , encoding the expected return from following a
particular policy in an MDP, J(π) = Es0~μo [V∏ (So)]. J induces an order over policies. A policy
maximising J is an optimal policy, denoted ∏<. Similarly, Q<, V<, and A< denote the Q-, value, and
advantage functions of an optimal policy. Since J may be multimodal, we often discuss the set of
optimal policies. However, Q‹, V‹, and A‹ are unique.
Moreover, we consider several policies resulting from alternative planning algorithms. Given a base
policy ∏o, and an inverse temperature parameter β > 0, we define the Boltzmann policy with respect
to πo, denoted πβπ0, using the softmax function:
πβπ0 (a|S) =
exp(βA∏o (s, a))
ΣaipA exP'βA∏o (s,a1))
(2)
The Boltzmann-rational policy, ∏β, is the Boltzmann policy with respect to optimal policies, as used
for IRL by Ramachandran & Amir (2007). The popular Maximum Entropy policy, πβ, is defined
using a modified policy evaluation function with an entropy regularisation term (Haarnoja et al.,
2017), and solves the recurrence πβ = πβπβ (i.e., πβ is a Boltzmann policy with respect to itself).
2Notably, we consider deterministic reward functions that may depend on a transition’s successor state. Al-
ternative spaces of reward functions are often considered (such as functions from S or S ^ A, or distributions).
The chosen space has straightforward consequences for invariances, which we discuss in appendix C.
3
Under review as a conference paper at ICLR 2022
2	Reward Function Transformations
In this section, we discuss how invariance to reward function transformations relates to infinite-data
ambiguity in reward learning and ambiguity tolerance in applications.
Definition 2.1 (Transformations and invariances). A transformation is a map between reward func-
tions. The invariances of an object X derived from reward R via function f are all transformations t
that preserve f: X “ fpRq “ f ptpRqq for all R. We say that X determines R up to its invariances.
A set of transformations carves out a partition of the space of reward functions by grouping together
those reward functions reachable from one another using the transformations. The partition carved
out by the invariances of an object is the equivalence kernel of the object's derivation function -
grouping the reward functions from which identical objects are derived into partition blocks.
Given a reward learning data source, consider the object encoding the information available from the
data source in the infinite-data limit (Lewbel, 2019, §3.1). The invariances of this object represent
the infinite-data ambiguity of the data source - it is impossible to recover the reward function beyond
the corresponding partition block, as the remaining functions imply indistinguishable data.
Similarly, consider a downstream application of learnt reward functions involving the computation
ofan object. The object’s invariances capture the ambiguity tolerance of this computation, as by def-
inition all reward functions in each block of the corresponding partition lead to identical outcomes.
2.1	Fundamental Reward Transformations
We introduce several fundamental sets of reward function transformations, forming a basis for the
invariances we study in section 3. Before considering several novel transformations, we recall po-
tential shaping, introduced by Ng et al. (1999) and widely known to preserve optimal policies in all
MDPs. We further distinguish special potential shaping transformations with constant potential over
an MDP’s initial states. We explore some properties of potential shaping in appendix A.
Definition 2.2 (Potential Shaping). A potential function is a function Φ : S → R, where Φ(s) “ 0
if s is a terminal state. If Φpsq “ k for all initial states then we say that Φ is k-initial. Let R and R1
be reward functions. Given a discount γ, we say R1 is produced by (k-initial) potential shaping of
R if R1(s, a, s1q “ R(s, a, s1)+ Y ∙ Φ(s1) 一 Φ(s) for some (k-initial) potential function Φ.
Definition 2.3 (S1-Redistribution). Let R and R1 be reward functions. Given transition dynamics τ ,
say R1 is produced by S1 -redistribution of R if Eg~τ(s,a)[R(s, a, S')] = Eg~τ(s,a) [R1(s, a, S1)].
S1-redistribution allows changing R arbitrarily for impossible transitions. Moreover, if at least two
states s11, s12 are in the support of τ(s, a) then S1-redistribution lets us increase R(s, a, s11) and
decrease R(s, a, s12) by a proportionate amount. Note that S1-redistribution depends crucially on the
reward function’s dependence on the successor state. This set of transformations collapses to the
identity for simpler spaces of reward functions, as we explore in appendix C.
Definition 2.4 (Monotonic Transformations). Let Rand R1 be reward functions. Say R1 is produced
by a zero-preserving monotonic transformation of R if for all pairs of transitions x, x1 P S XAXS,
R(x) ≤ R(x1) if and only if R1(x) ≤ R1(x1), and R(x) “ 0 if and only if R1(x) = 0. Moreover,
say R1 is produced by positive linear scaling of R if R1 “ C ∙ R for some positive constant c.
A zero-preserving monotonic transformation is simply a monotonic transformation that maps zero
to itself. Positive linear scaling is a special case.
Definition 2.5 (Optimality-Preserving Transformation). Let R and R1 be reward functions. Given a
function O : S → P(A) — {0}, transition dynamics T, and discount rate γ, We say R1 is produced
from R by an optimality-preserving transformation with O if there is a function Ψ : S → R such
that Esι~τ(s,a) [R1(s, a, S1) ' Y ∙ Ψ(S1)] ≤ Ψ(s) for all s, a, with equality if and only if a P O(s).
This transformation gives the reward functions with optimal actions from O (Ψ determines the
new value function). Note that in practice an implicit dependence on R is introduced through the
definition of O. Also, we can reach arbitrary R1 if O is unconstrained (in practice, we constrain O).
Finally, we consider transformations allowing the reward to vary freely for a given set of transitions.
Definition 2.6 (Masking). Let R and R1 be reward functions. Given a transition set X J S XAXS,
say R1 is produced by a mask ofX from R if R(x) “ R1(x) for all x R X.
4
Under review as a conference paper at ICLR 2022
3	Invariances of Reward-Related Objects
In this section we catalogue the invariances of various central objects derived from reward functions,
including expert trajectory distributions, the ranking of trajectories induced by the return function,
and the set of optimal policies. Some of these objects correspond to the information available in
the infinite-data limit of a reward learning data source, while others correspond to the outcome of a
downstream application.
If an object X can be derived from another object Y without further reference to the reward function,
then X inherits Y ’s invariances. For example, the optimal Q-function’s invariances are inherited by
various expert policies. Accordingly, we organise this section by incrementally deriving our objects
of interest starting from the reward function, cataloguing the invariances introduced in each step.
This also mirrors the structure of figure 1a. We defer all proofs until appendix B.
3.1	Invariances of Expert Behaviour
Inverse reinforcement learning (IRL) algorithms infer a task’s reward function from the behaviour of
task experts. Formally, this behaviour is represented as an expert’s policy or a sample of trajectories.
To characterise the corresponding invariances, We begin with Q-functions - instrumental to deriving
many policies. Q-functions are invariant to S1-redistribution since they are defined as an expectation
over the successor state S1, We show that this is the only invariance for Q-functions.
Theorem 3.1.	Given an MDP and a policy π, the Q-function for π, Qπ, determines R up to S1-
redistribution. The optimal Q-function, Q‹, has precisely the same invariances.
This invariance is inherited by any object that can be derived from a Q-function. However, note that
S1-redistribution vanishes in simpler spaces of reward functions, as we explore in appendix C.
We now turn to policies derived using various planning algorithms. These policies are instrumental
in constructing the trajectories studied in IRL. For example, Ramachandran & Amir (2007) and
Ziebart et al. (2008) assume that expert behaviour is drawn from a Boltzmann-rational policy, and
Ziebart et al. (2010) assume a Maximum Entropy policy We catalogue the invariances of arbitrary
Boltzmann policies, of which these other policies are special cases. As these policies can be derived
from Q-functions, they inherit invariance to S1-redistribution. We show they are also invariant to
potential shaping, but not to any other transformations.
Theorem 3.2.	Given an MDP, an inverse temperature parameter β, and a base policy π0, the
Boltzmann policy πβπ0 determines R up to S1 -redistribution and potential shaping. The Boltzmann-
rational policy, π<, and the Maximum Entropy policy, ∏β, have precisely the same invariances.
By contrast, Ng & Russell (2000) and Abbeel & Ng (2004) assume that experts follow an optimal
policy. Optimal policies inherit S1-redistribution invariance from the optimal Q-function, and are
also known tobe invariant to potential shaping (Ng et al., 1999). Under an additional assumption that
a given policy is maximally supportive, in that it takes all optimal actions with positive probability,
we show that these invariances and any additional invariances are captured in a class of optimality-
preserving transformations (Definition 2.5) based on the set of optimal actions in each state.
Theorem 3.3.	Given an MDP, let Opsq “ arg maxaA‹ps, aq. A maximally supportive optimal
policy determines R up to optimality-preserving transformations with O.
Additional invariances arise if we consider optimal policies that may lack support for optimal ac-
tions. As a well-known example, the zero-reward is consistent with any policy in this sense.
In the infinite-data limit, a data source of trajectories sampled from a policy reveals the distribution
of trajectories induced by the policy, and therefore the policy itself for all states reachable via its
supported actions. A Boltzmann policy supports all actions, so in the infinite data limit, samples
of trajectories determine the policy for all reachable states. We show this introduces invariance
precisely to changes in the reward of unreachable transitions.
Theorem 3.4.	Given an MDP, an inverse temperature parameter β, and a base policy π0, the dis-
tribution of trajectories induced by the Boltzmann policy πβπ0 from all initial states determines R
up to S1-redistribution, potential shaping, and a mask of unreachable transitions. The distributions
of trajectories induced by the Boltzmann-rational policy, π<, and the Maximum Entropy policy, ∏β,
from all initial states, have precisely the same invariances.
5
Under review as a conference paper at ICLR 2022
Similarly, trajectories sampled from an optimal policy reveal the policy in those states that its actions
reach. This again introduces additional invariance to transformations of reward in other states.
Theorem 3.5.	Given an MDP, consider the distribution of trajectories induced by a maximally
supportive optimal policy. Let S be the set of states in supported trajectories. Let O be the set
of functions O defined on S such that Opsq “ arg maxaA‹ps, aq for all s P S. The induced
distribution of trajectories determines R up to optimality-preserving transformations with O P O.
Note that a mask of the complement of S is not permitted. However, the fact that O is uncon-
strained outside S leaves reward effectively unconstrained in those states, except that the reward of
transitions out of S may have to “compensate” for the value of their successor states, to prevent new
actions that lead out of S from becoming optimal.
3.2 Invariances of Trajectory Evaluation
The return function, capturing the reward accumulated over a trajectory, is instrumental in deriv-
ing data for evaluative feedback such as reward labels and trajectory preference comparisons. We
consider the invariances of the return function for various restricted domains.
Theorem 3.6.	Given an MDP, the return function restricted to possible trajectory fragments, Gζ,
determines R up to a mask of impossible transitions;
Theorem 3.7.	Given an MDP, the return function restricted to possible and initial trajectories, Gξ,
determines R up to zero-initial potential shaping and a mask of unreachable transitions.
The limited invariance of the return of fragments arises because this restricted domain still includes
individual (possible) transitions. Additional invariances will arise from additional restrictions, such
as a minimum or maximum fragment length, or a restriction to initial trajectory fragments.
Pairwise comparisons between trajectories are studied as a data source for reward learning (Akrour
et al., 2012; Christiano et al., 2017). Itis common to model the comparisons as based on the return of
trajectories, but with accompanying decision noise following a Boltzmann distribution. Under this
assumption, in the limit of infinite noisy comparisons for each pair of trajectories, the data source
reveals the Boltzmann distributions. Boltzmann noise encodes relative cardinal information about
the return of trajectories and fragments, so little invariance is introduced.
Formally, given an MDP and an inverse temperature parameter β > 0, let We be a distribution over
each pair of possible trajectory fragments, ζ1, ζ2, such that
PPZι Me Z2)“
exp(βG(ζ2))______
exp(βG(Zι))+ exp(βG(Z2))
and let We be the analogous distribution over each pair of possible and initial trajectories.
Theorem 3.8.	Given an MDP the distribution ofcomparisons ofpossible trajectory fragments, We,
determines R up to a mask of impossible transitions.
Theorem 3.9.	Given an MDP, the distribution of comparisons of possible and initial trajectories,
We, determines R up to k-initial potential shaping and a mask of unreachable transitions.
The limited invariance of Boltzmann comparisons of fragments arises from the very flexible com-
parisons permitted, including, for example, comparisons between individual transitions and empty
trajectories. Additional invariances will arise from additional restrictions, such as permitting com-
parisons only between fragments of a fixed length. Moreover, it is worth reiterating that these
invariances rely heavily on the precise structure of the decision noise revealing cardinal information
in the infinite-data limit.
Itis also possible to model trajectory comparisons as noiseless comparisons based on the return. The
infinite data limit then corresponds to the order induced by the return functions. Formally, define the
noiseless order ofpossible trajectory fragments as a relation, W<, on possible trajectory fragments:
Zi M< Z2 Q G(Zιq ≤ G(Z2).
Similarly, define the noiseless order of possible and initial trajectories as the analogous relation,
Wξ, for pairs of possible and initial trajectories. These relations omit cardinal information about
pairwise comparisons, and so invariances to certain monotonic transformations are introduced. The
precise monotonic invariances depend on the MDP (for example, see the proof in appendix B.3).
6
Under review as a conference paper at ICLR 2022
Theorem 3.10.	We have the following bounds on the invariances of the noiseless order of possible
trajectory fragments, W<. InallMDPs:
(1)	WS is invariant to positive linear scaling and a mask ofimpossible transitions; and
(2)	W< is not invariant to transformations other than zero-preserving monotonic transforma-
tions or masks of impossible transitions.
Moreover, there exist MDPs attaining each of these bounds.
We give a lower bound on the invariances of the noiseless order of possible and initial trajectories,
Wξ. Since wξ can be derived from We, it inherits the latter,s invariances. Moreover, like w<, w< is
always invariant to positive linear scaling.
Theorem 3.11.	Given an MDP, the noiseless order ofpossible and initial trajectories, W S, is invari-
ant to k-initial potential shaping, positive linear scaling, and a mask of unreachable transitions.
3.3 Invariances of Policy Optimisation
The primary application of learnt reward functions is to compute optimal policies, using techniques
such as RL. Policy optimisation procedures typically compute a single optimal policy. However,
in terms of invariances, one may desire to preserve the whole set of optimal policies, so as not to
tolerate any sub-optimal policies becoming optimal through a reward transformation.
The set of optimal policies inherits S1-redistribution invariance from the optimal Q-function, and
is also known to be invariant to potential shaping (Ng et al., 1999). In fact, because a maximally
supportive optimal policy can be derived from the set of optimal policies and vice versa, the set
shares the same invariances as a maximally supportive optimal policy (Theorem 3.3).
Theorem 3.12.	Given an MDP, let Opsq “ arg maxaASps, aq. Then the set of optimal policies
determines R up to optimality-preserving transformations with O.
Moreover, if one uses an algorithm not guaranteed to find a globally optimal policy, one may desire
to preserve the entire order induced on the space of policies by the policy evaluation function, rather
than just the set of maximising policies. Future work could investigate the invariances of the ordinal
information in the policy evaluation function. Note that since the set of optimal policies can be
derived from this order, the order has at most the invariances of the set of optimal policies.
Finally, we sketch some bounds on the invariances of the set of optimal policies across all MDPs.
Potential shaping and linear scaling preserve optimal policies in each MDP, and hence in all MDPs.
S1-redistribution and optimality-preserving transformations for a given MDP might not. Moreover,
Theorem 3.12 implies that any transformation that is not an optimality-preserving transformation in
a given MDP cannot preserve optimal policies in that MDP, let alone all MDPs.
4	Implications for Reward Learning
So far we have catalogued the invariances of transformations to the reward function of various
reward function derived objects. These invariances characterise the infinite-data ambiguity of several
reward learning data sources, and the ambiguity tolerance of policy optimisation. In this section, we
discuss the implications for the practical evaluation of reward learning data sources.
We begin by defining a mathematical framework for comparisons between data sources and applica-
tions in terms of their ambiguity. The characterisation of ambiguity and tolerance as invariances to
reward transformations suggests a natural partial order on data sources and applications. Recall that
the invariances of an object correspond to a partition of the space of reward functions (section 2).
We lift the refinement relation for partitions (Aigner, 1979, §I.2.B) to data sources and applications
as follows.
Definition 4.1 (Ambiguity refinement). Consider two reward learning data sources (or applica-
tions), X and Y . Let ΠX and ΠY be the partitions of the space of reward functions corresponding to
their respective invariances (definition 2.1). If Πχ is a partition refinement of Πγ, We write X W Y,
and we say X is no more ambiguous than Y (or X is tolerablefor application Y). If X W Y but not
Y W X, then we write X Y Y and say X is (strictly) less ambiguous than Y.
7
Under review as a conference paper at ICLR 2022
Given two data sources X and Y, X W Y corresponds to X conflating no additional reward func-
tions compared to Y in the infinite-data limit. This is the sense in which we say X is no more
ambiguous than Y. Moreover, given a downstream application Z, X W Z is precisely the condi-
tion of Z tolerating the infinite-data ambiguity of data source X: X W Z if and only if the reward
functions conflated by X in the infinite-data limit all lead to the same outcome in Z.
Of our fundamental reward functions transformations, there are several clear instances of ambi-
guity refinement in a given MDP, as summarised in figure 1b. Invariance to k-initial potential
shaping (k-Φ) corresponds to less ambiguity than general potential shaping (Φ). Likewise positive
linear scaling (PLS) is less ambiguous than zero-preserving monotonic transformations (ZPMT),
and a mask of impossible transitions is less ambiguous than S’-redistribution (S1-R). All of these
transformations other than zero-preserving monotonic transformations are less ambiguous than the
optimality-preserving transformations we have encountered.
More concretely, we can compare the ambiguity of specific data sources. Some of these comparisons
are indicated in figure 1a. For example, the ambiguity tolerance of the set of optimal policies is a
class of optimality-preserving transformations. Each of the data sources that are less ambiguous
than this tolerance are sufficient for policy optimisation.
Notably, this excludes noiseless comparisons between trajectory fragments in some MDPs. Specif-
ically, policy optimisation does not, in general, tolerate zero-preserving monotonic transformations
(ZPMT), while noiseless comparisons are invariant to this transformation in some MDPs (Theo-
rem 3.10). Policy optimisation also does not tolerate data sources based on possible and initial
trajectories, which are invariant to a mask of unreachable transitions. However, these sources are
tolerable if the application only requires optimal behaviour in reachable states.
Moreover, we can compare data sources drawn from one MDP to applications in another MDP,
such as under a shift in transition dynamics or initial state distribution. This captures the common
sim-to-real setting where learning takes place in a simulated or otherwise restricted environment that
differs from the final deployment environment. The simplest transformations to consider are masks
of possible or reachable transitions. These are parametrised by transition dynamics. In general, the
ambiguity corresponding to a mask of X is less than for a mask of X1 ɔ X. For example, if the
new dynamics supports transitions that were previously impossible, then sources with invariance to
a mask from the original MDP may not be tolerable for applications in the new MDP.
A similar results holds for S1-redistribution, which involves an expectation over MDP dynamics.
As an extreme example, we prove that when the transition dynamics are changed for every state
and action, S1-redistribution under the original dynamics permits an arbitrary Q-function under the
new dynamics. Naturally, data sources derived from Q-functions may also be affected by shifts
in dynamics. Note that this strong result relies on the formulation of rewards as depending on the
successor-state (cf. appendix C).
Theorem 4.1.	Consider an MDP pS, A, τ, μo, R, Y), a policy π, and alternative transition dynamics
T1 with T(s, a) ‰ T'(s, a) for all S P S, a P A. Given a function Q1 : S XA → R, there exists a
reward function R1, produced from R by S 1 -redistribution under τ, such that Q1 is the Q-function
for π under R1 and T1.
Ambiguity refinement is a partial order, and some data sources are indeed incomparable. In conso-
lation, we observe that such incomparable ambiguity is complementary ambiguity, in that by com-
bining the associated data sources, we reduce overall ambiguity about the latent reward.
Theorem 4.2.	Given data sources X and Y, let pX, Y) denote the combined data source formed
from X and Y. If X and Y are incomparable, then (X, Y) Y X and (X, Y) Y Y.
This perspective highlights promising directions for the design of reward learning data sources. In
particular, this suggests developing reward learning algorithms for mixtures of data sources with
complementary ambiguity. Unfortunately, most popular data sources actually appear to have similar
kinds of ambiguity given one MDP. However, ambiguity could be reduced by incorporating data
from multiple MDPs, along the lines of Amin et al. (2017) and Cao et al. (2021).
8
Under review as a conference paper at ICLR 2022
5	Limitations and Future Work
Our results give an upper bound on the amount of information that can be extracted from a given data
source. However, in practice, these bounds may not be reached. In particular, our results are for the
limit of infinite data. But in practice data sets are finite and, when data collection is expensive, may
be fairly small. An important direction for future work is to characterise how much information is
contained in data sets of varying sizes and data sources. This would enable practitioners to determine
the most sample efficient data source for a fixed data collection budget.
Furthermore, our results rely on the data being generated according to the process assumed by the
reward learning algorithm. However, most popular approaches are a poor fit for human data (Orsini
et al., 2021). For example, human demonstrations are rarely perfectly optimal or Boltzmann-
rational. Moreover, there is often a trade-off between how informative a data source is and how
easy it is for a user to provide data. As an extreme example, a user directly specifying the target
reward function is maximally informative - if users could complete such a task correctly. We expect
the maximum informativeness of a data source to be a useful metric, but it should be considered
alongside the cost and tractability of collecting different kinds of data.
6	Conclusion
Substantial effort has been invested to develop reward learning algorithms for a variety of data
sources. A fundamental question to ask is how effective are these algorithms relative to an optimal
algorithm for that data source? Our results characterise the information available in different data
sources, enabling algorithms to be compared to this theoretical upper bound.
Moreover, our framework enables direct comparisons between different data sources. We find that
some data sources are strictly less informative than others, such as noiseless preference comparisons
vs. return labels. By contrast, others are incomparable and have complementary ambiguity, such as
Q-values (invariant to S1 -redistribution but not potential shaping) vs. episode return Gξ (invariant
to some potential shaping, but not to S1-redistribution).
In particular, we have characterised the invariances of various reward-related objects to transfor-
mations such as potential shaping. We have shown that these objects form a partial order under
ambiguity refinement. These results, summarised in figure 1, allow us to predict the ambiguity of
data sources generated from these objects. While practitioners could simply collect data from the
least ambiguous source, this might be very expensive. Our framework also identifies the ambiguity
tolerance of downstream applications (such as policy optimisation) that need to compute these ob-
jects. This enables practitioners to identify reward learning data sources with low ambiguity in the
areas their application is sensitive to, enabling higher performance without unnecessary costs.
Ethics S tatement
It is important that AI systems are aligned with the interests of users and other stakeholders. In open-
ended problems, directly specifying how the AI system should behave is intractable. Prior work has
identified reward learning as an essential building block for AI systems that can cooperate with
humans (Dafoe et al., 2020, §4.1.3), especially for powerful AI systems (Bostrom, 2014, chapter
12). We hope that our work provides greater clarity on both the limits and potential of various
reward learning data sources. However, given the importance of the domain, we should stress that
our work provides only one useful angle by which to evaluate data sources. In particular, we do not
consider sample efficiency, robustness to misspecification, or the cost of data collection.
Moreover, even if a theoretically optimal and practically robust reward learning algorithm were to
be developed, there would still remain important normative questions. In particular, what kinds of
values we are aligning the AI system to - stated preferences, revealed preferences, instructions, or
something else (Gabriel, 2020)? Additionally, it is important that all relevant stakeholders are able
to provide input into the system. This may constrain the kinds of data we can collect. For example,
while only task experts might be able to provide demonstrations, a wider variety of stakeholders
might be able to provide preference comparisons. While a thorough evaluation of these consid-
erations are beyond the scope of this paper, we would encourage practitioners to evaluate reward
learning data sources holistically, including but not wholly relying on our results.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
Our results are all theoretical in nature. We introduce notation and other background material in
sections 1.2 and 2. Necessary assumptions are listed there and in each theorem statement. Proofs
for some fundamental lemmas are provided in appendix A and all other proofs are in appendix B.
References
Nematollah Ab Azar, Aref Shahmansoorian, and Mohsen Davoudi. From inverse optimal control
to inverse reinforcement learning: A historical review. Annual Reviews in Control, 50:119-138,
2020.
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.
Pieter Abbeel, Adam Coates, and Andrew Y Ng. Autonomous helicopter aerobatics through ap-
prenticeship learning. The International Journal of Robotics Research, 29(13):1608-1639, 2010.
Victor Aguirregabiria. Nonparametric identification of behavioral responses to counterfactual policy
interventions in dynamic discrete decision processes. Economics Letters, 87(3):393-398, 2005.
Victor Aguirregabiria and Pedro Mira. Dynamic discrete choice structural models: A survey. Journal
of Econometrics, 156(1):38-67, 2010.
Martin Aigner. Combinatorial Theory. Die Grundlehren der mathematischen Wissenschaften : a se-
ries of comprehensive studies in mathematics. Springer New York, 1979. ISBN 9783540903765.
Riad Akrour, Marc Schoenauer, and Michele Sebag. April: Active preference learning-based rein-
forcement learning. In Joint European Conference on Machine Learning and Knowledge Discov-
ery in Databases, pp. 116-131. Springer, 2012.
Kareem Amin, Nan Jiang, and Satinder Singh. Repeated inverse reinforcement learning. In NIPS,
2017.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Con-
crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Peter Arcidiacono and Robert A Miller. Identifying dynamic discrete choice models off short panels.
Journal of Econometrics, 215(2):473-485, 2020.
Stuart Armstrong and Soren Mindermann. Occam,s razor is insufficient to infer the preferences of
irrational agents. arXiv preprint arXiv:1712.05812, 2017.
Erdem Bιyιk, Dylan P Losey, Malayandi Palan, Nicholas C Landolfi, Gleb Shevchuk, and Dorsa
Sadigh. Learning reward functions from diverse sources of human feedback: Optimally integrat-
ing demonstrations and preferences. arXiv preprint arXiv:2006.14091, 2020.
Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014. ISBN
9780199678112.
Haoyang Cao, Samuel N. Cohen, and Lukasz Szpruch. Identifiability in inverse reinforcement learn-
ing, 2021.
Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. arXiv preprint arXiv:1706.03741, 2017.
Anne GE Collins and Amitai Shenhav. Advances in modeling learning and decision-making in
neuroscience. Neuropsychopharmacology, pp. 1-15, 2021.
Will Dabney, Mark Rowland, Marc G Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
10
Under review as a conference paper at ICLR 2022
Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z. Leibo,
Kate Larson, and Thore Graepel. Open problems in cooperative ai, 2020.
Daniel C Dennett. The intentional stance. MIT press, 1989.
Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement
learning. arXiv preprint arXiv:1904.12901, 2019.
Krishnamurthy Dvijotham and Emanuel Todorov. Inverse optimal control with linearly-solvable
mdps. In ICML, 2010.
Iason Gabriel. Artificial intelligence, values, and alignment. Minds and Machines, 30(3):411—437,
2020.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352-1361.
PMLR, 2017.
Andrew Howes, Richard L Lewis, and Satinder Singh. Utility maximization and bounds on human
information processing. Topics in cognitive science, 6(2):198-203, 2014.
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
learning from human preferences and demonstrations in atari. arXiv preprint arXiv:1811.06521,
2018.
Hong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying
formalism for reward learning. In NeurIPS, volume 33, pp. 4415-4426, 2020.
Kuno Kim, Shivam Garg, Kirankumar Shiragur, and Stefano Ermon. Reward identification in in-
verse reinforcement learning. In International Conference on Machine Learning, pp. 5496-5505.
PMLR, 2021.
Pallavi Koppol, Henny Admoni, and Reid Simmons. Iterative interactive reward learning. In Par-
ticipatory Approaches to Machine Learning Workshop at ICML 2020, 2020.
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable
agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871,
2018.
Arthur Lewbel. The identification zoo: Meanings of identification in econometrics. Journal of
Economic Literature, 57(4):835-903, 2019.
Charles F Manski. Identification problems in the social sciences. Harvard University Press, 1995.
Charles F Manski. Partial identification of probability distributions. Springer Science & Business
Media, 2003.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.
Nonparametric return distribution approximation for reinforcement learning. In ICML, 2010a.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.
Parametric return density estimation for reinforcement learning. In Proceedings of the Twenty-
Sixth Conference on Uncertainty in Artificial Intelligence, pp. 368-375, 2010b.
Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In 17th Interna-
tional Conference on Machine Learning, volume 1, pp. 663-670, 2000.
Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
theory and application to reward shaping. In NIPS, 1999.
Manu Orsini, Anton Raichuk, Leonard Hussenot, Damien Vincent, Robert Dadashi, Sertan Girgin,
Matthieu Geist, Olivier Bachem, Olivier Pietquin, and Marcin Andrychowicz. What matters for
adversarial imitation learning?, 2021.
11
Under review as a conference paper at ICLR 2022
Malayandi Palan, Nicholas C Landolfi, Gleb Shevchuk, and Dorsa Sadigh. Learning reward func-
tions by integrating human demonstrations and preferences. arXiv preprint arXiv:1906.08928,
2019.
Joshua C Peterson, David D Bourgin, Mayank Agrawal, Daniel Reichman, and Thomas L Grif-
fiths. Using large-scale experiments and machine learning to discover theories of human decision-
making. Science, 372(6547):1209-1214, 2021.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, vol-
ume 7, pp. 2586-2591, 2007.
Constantin A Rothkopf and Christos Dimitrakakis. Preference elicitation and inverse reinforce-
ment learning. In Joint European conference on machine learning and knowledge discovery in
databases, pp. 34-48. Springer, 2011.
Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual
conference on Computational learning theory, pp. 101-103, 1998.
Stuart Russell and Peter Norvig. Artificial intelligence: a modern approach. Prentice Hall, third
edition, 2009.
John Rust. Structural estimation of markov decision processes. Handbook of econometrics, 4:
3081-3143, 1994.
Paul JH Schoemaker. The expected utility model: Its variants, purposes, evidence and limitations.
Journal of economic literature, pp. 529-563, 1982.
Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather
than assuming, human biases for reward inference. In International Conference on Machine
Learning, pp. 5670-5679. PMLR, 2019.
Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine. End-to-end robotic
reinforcement learning without reward engineering. arXiv preprint arXiv:1904.07854, 2019.
Sorawoot Srisuma. Identification in discrete markov decision models. Econometric Theory, 31(3):
521-538, 2015.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. arXiv preprint
arXiv:2009.01325, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Elie Tamer. Partial identification in econometrics. Annu. Rev. Econ., 2(1):167-195, 2010.
Hsiao-Yu Tung, Adam W Harley, Liang-Kang Huang, and Katerina Fragkiadaki. Reward learning
from narrated demonstrations. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7004-7013, 2018.
Eric Wiewiora, Garrison W Cottrell, and Charles Elkan. Principled methods for advising reinforce-
ment learning agents. In Proceedings of the 20th International Conference on Machine Learning
(ICML-03), pp. 792-799, 2003.
Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes Furnkranz, et al. A survey of Preference-
based reinforcement learning methods. Journal of Machine Learning Research, 18(136):1-46,
2017.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entroPy inverse
reinforcement learning. In Aaai, volume 8, PP. 1433-1438. Chicago, IL, USA, 2008.
Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the PrinciPle of
maximum causal entroPy. In ICML, 2010.
12
Under review as a conference paper at ICLR 2022
A	Properties of Fundamental Reward Transformations
We begin with some supporting results concerning the basic reward transformations, used in sec-
tion 3 to characterise the invariances of various objects derived from the reward function.
The following result captures how potential shaping affects various reward-related functions.
Lemma A.1. Consider M and M1, two MDPs differing only in their reward functions, respectively
R and R1. Denote the return, Q-, value, and policy evaluation functions of M1 by G1, Q1π, Vπ1, and
J1. If R1 is produced by potential shaping of R with a potential function Φ, then:
(1)	fora trajectory fragment ζ “ ps0, a0, s1, . . . , snq, G1pζq “ Gpζq ` γnΦpsnq ´ Φps0q;
(2)	for a trajectory ξ “ ps0, a0, . . .q, G1pξq “ Gpξq ´ Φps0q;
(3)	for a state s P S, and action a P A, Q1π ps, aq “ Qπ ps, aq ´ Φpsq;
(4)	for a state s P S, Vπ1 psq “ Vπ psq ´ Φpsq; and
(5)	for a policy ∏, J1(∏) “ J(∏) — Es。5[Φ(So)‰.
Proof. (1) is given by straightforward induction on the length of ζ. For (2), take the limit as the
length of a prefix goes to infinity, whereupon γnΦ(sn) goes to zero (Y V 1 by definition, and Φ(sn)
is bounded since its domain is finite). (3) and (4) were proved for optimal policies by Ng et al.
(1999), and they also observed that the extension to arbitrary policies is straightforward (following
immediately from (2), for example). (5) is immediate from (4).	口
The following results explain how k-initial potential shaping and linear scaling of R correspond to
affine transformations of G.
Lemma A.2. Let (S, A, τ, μo, R, Y) be an MDP R a reward function, and k P R a ConStant. Then
we have that G1(ξq “ G(ξq — kfor allpossible and initial trajectories ξ, if and only ifR1 is produced
from R by k-initial potential Shaping and a maSk of unreachable tranSitionS.
Proof. The converse follows from Lemma A.1 and that varying the reward for unreachable transi-
tions does not affect the return of any possible, initial trajectories, by definition.
In the forward direction, consider an arbitrary reachable state s P S. Let ζ be a possible, initial,
length n trajectory fragment ending in s (at least one exists, since s is reachable). Let ξs be some
possible trajectory starting in s, and let ∆ξs “ G(ξs) — G1(ξs). Let ζ ` ξs denote the concatenation
ofζ and ξs. Since ζ ` ξs is an initial trajectory, we have by assumption G(ζ ` ξs) — G1(ζ ` ξs) “ k.
Moreover, note that G(ζ ` ξs) “ G(ζ) ` YnG(ξs), (and likewise for G1), by definition of return.
Then (recalling that We have defined Y > 0),
∆ξs “ G(ξs) — G1 (ξs)
“ G(Z + ξs) — G(Z) ´ G1 (Z + ξs) — G1(Z)
γn	Yn
k ´ G(Z) + G1(Z)
=-------------------.
Yn
Therefore ∆ξs is independent of ξs, except for possible dependence on ξs ’s starting state s. We
associate a unique P (s) = ∆ξs with each reachable s.
Now consider a reachable transition (s, a, s1). Further straightforward algebraic manipulation shows
that R(s, a, s1) = R(s, a, s1) + Y ∙ P(s1) — P(s). Moreover, from the definition of terminal states
we must have that P(s) = ∆ξs = 0 for terminal s, and by assumption we have that P(s) =
∆ξs = k if s is initial. So R1 is given by k-initial potential shaping of R with Φ(s) = P(s) for
reachable transitions. Any variation in reward for unreachable transitions can be accounted for by
the mask.	□
Lemma A.3. Let (S, A, τ, μo, R, Y) be an MDP, R a rewardfunction, and C P R a constant. Then
G1 (ξ) = c ∙ G(ξ) for all possible initial trajectories ξ, if and only if R is ProduCedfrom R by Zero-
initial potential Shaping, linear Scaling by a factor of c, and a maSk of all unreachable tranSitionS.
13
Under review as a conference paper at ICLR 2022
Proof. From Lemma A.1, we have that 0-initial potential shaping leaves G unchanged. Likewise,
a mask of unreachable transitions does not affect G for possible initial trajectories. Moreover, it is
straightforward that linear scaling of R by a factor of c leads to a linear scaling of G by the same
factor of c. Hence the converse is established.
For the forward direction, consider an arbitrary reachable state s P S. Let ξs be some possible
trajectory starting in s, and let ∆ξs “ c ∙ G(ξs) — G1(ξs). Let Z be a possible, initial, length n
trajectory fragment ending in s. Since the concatenation Z + ξs is a possible initial trajectory, we
have that
→
→
→
G1 (Z + ξs) = c ∙ G(Z + ξs)
G1(Z) + YnG1(ξs) “ c ∙(G(Z) + YnG(ξs))
G1(Z) “ c ∙ G(Z) + γn(c ∙ G(ξs) — G1(ξs))
“ c ∙ G(Z) + γn∆ξs
人	GI(Z) — c ∙ G(Z)
飞 “----------Yn-------
Therefore ∆ξs is independent of ξs, except for possible dependence on ξs ’s starting state s. We
associate a unique Ppsq “ ∆ξs with each reachable s.
Now consider a reachable transition ps, a, s1q. Further straightforward algebraic manipulation shows
that R1(s, a, s1) “ C ∙ R(s, a, s1)+ Y ∙ P(s1) — P(s). Moreover, from the definition of terminal states
we must have that Ppsq “ ∆s “ 0 for terminal s, and by assumption we have that Ppsq “ 0 if s
is initial. Thus R1 is given by first potential shaping R with Φ(s) “ CP(s), and then linear scaling
□
with c.
14
Under review as a conference paper at ICLR 2022
B Proofs
We provide proofs for the theoretical results presented in the main paper.
B.1 Proofs for Section 3.1 Results Not Concerning Optimal Policies
We provide proofs for some of the results in section 3.1. The proofs relating to optimal policies
(theorems 3.3 and 3.5) are given in the next subsection (appendix B.2).
Theorem 3.1.	Given an MDP and a policy π, the Q-function for π, Qπ, determines R up to S1-
redistribution. The optimal Q-function, Q‹, has precisely the same invariances.
Proof. Qπ satisfies the Bellman equation for all s P S , a P A:
Q∏(s, a) = Esi〜T(s,a),Ai~∏(si) [R(s, a, S1)+ Y ∙ Q∏(S1, A1)].
This equation can be rewritten as
ES1„Tps,a) [R(s,a,S1)‰ = Qn (s,a)´ Y ∙ Es，„t(s,a),4~∏(Si) [Qπ (S 1,A1)‰ .
From this, we can see that Qπ is invariant to S1-redistribution of R, and no other transformations.
Q< = Q∏< where ∏< is any optimal policy derived from Q<, so the invariances of the optimal
Q-function follow as a special case.	□
Theorem 3.2.	Given an MDP, an inverse temperature parameter β, and a base policy π0, the
Boltzmann policy πβπ0 determines R up to S1 -redistribution and potential shaping. The Boltzmann-
rational policy, π<, and the Maximum Entropy policy, ∏β, have precisely the same invariances.
Proof. By equation (2), πβπ0 can be derived from Aπ0. Aπ0 itself can be derived from Qπ0, given
∏o (by equation (1), A∏° (s,a) = Q∏ (s,a) ´ EA〜∏o(s)[Q∏o (s,A)]). ThUS ∏∏0 is invariant to SJ
redistribution by Theorem 3.1. Moreover, by Lemma A.1, potential shaping causes a state-dependent
shift of Qπ0 . This shift does not affect Aπ0. Therefore πβπ0 is also invariant to potential shaping.
Conversely, recall (or see Lemma B.1, below) that the softmax function is invariant only to constant
shifts. This means that πβπ0 is invariant to all and only those transformations ofR that produce state-
dependent shifts in Qn. Let B : S → R, and suppose R and R1 are two reward functions such that
the corresponding Q-functions (for π) satisfy Q1(s, a) = Q(s, a) ` B(s). Then
E[R1(s, a, S 1)S = E[Q1(s, a) ´ Y ∙ maXaipAQ1(S1, a1)]
“E[Q(s, a) ' B(S) ´ Y ∙ maXaιpA(Q(S1, a1) ' B(SI))S
“ErB(S) — γ ∙ B(S1) ' R(s, a, S1)]
where the expectations are over S1 〜 T(s, a). Now set Φ(s) = ´B(s), and We can see that the
difference between R and R1 is described by potential shaping and S1 -redistribution.
The Boltzmann-rational policy determines its own base policy (it is enough to determine the optimal
Q-function up to a state-dependent shift), and the Maximum Entropy policy is its own base policy,
so the invariances of these policies follow as a special case of this result.	□
Lemma B.1. Consider two functions f : X → R and g : X → R defined on a finite set X. Then g
is constant if, for all x P X,
exp(f (x) + g(x))	=	exp(f(x))
Σx1pX exPf (x1) + g(x1))	ΣχipX exPf (x1)).
Proof. This is an elementary property of the softmax function. It can be seen as follows:
exp(f (x) + g(x))	Σx1PX exp(f(x1) + g(x1))
--→	------------------- ------------------ 	_ .-
exPf (X))------------------------------------------------ςx1pX exPf (x1))
Σx1PX exp(f(x1) + g(x1))∖
-	M= R	ς"pX exp(f(x1)))
which is constant in x. (The converse is true, but we do not make use of it, so we omit a proof.) □
15
Under review as a conference paper at ICLR 2022
Theorem 3.4. Given an MDP, an inverse temperature parameter β, and a base policy π0, the dis-
tribution of trajectories induced by the Boltzmann policy πβπ0 from all initial states determines R
up to S1-redistribution, potential shaping, and a mask of unreachable transitions. The distributions
of trajectories induced by the Boltzmann-rational policy, π<, and the Maximum Entropy policy, ∏β,
from all initial states, have precisely the same invariances.
Proof. That the distribution is invariant to S1-redistribution and potential shaping follows from The-
orem 3.2. The distribution is also invariant to changes in the reward for transitions out of unreachable
states, since these rewards cannot affect the policy for reachable states. As a result the distribution
is additionally invariant to a mask of unreachable transitions.
The trajectory distribution can be factored into the separate distributions πβπ0 psq P ∆pAq for each
reachable state s, by conditioning on a supported prefix trajectory fragment that leads to s and
marginalising over subsequent states and actions. Via a similar argument to the proof of Theo-
rem 3.2, the distribution determines the reward function for transitions (out of these reachable states)
up to potential shaping and transformations and S1-redistribution (as they affect reachable states).
As for Theorem 3.2, the invariances for the Boltzmann-rational policy and the Maximum Entropy
policy arise as special cases.	□
B.2 Proofs for Section 3.1 Results Concerning Optimal Policies
Our results concerning the invariance of optimal policies and their trajectories follow from the fol-
lowing general result:
Lemma B.2. Given an MDP M, suppose we have access to the set of optimal actions for each
state in a subset of states S 三 S. Assume that this subset is closed under optimal actions given
the transition dynamics of M (that is, taking optimal actions from states in S never leads to states
outside of S). Let O be the set of functions O defined on S such that Opsq “ arg maxaPAA‹ps, aq
for all s P S (but O is unconstrained for states outside S). Then these optimal action sets determine
R up to optimality-preserving transformations with O P O.
Proof. Suppose R1 is obtained from M’s reward R via an optimality-preserving transformation with
some O P O. Let Ψ be the corresponding value-bounding function, that is, a function Ψ : S → R
satisfying, for all s P S and a P A,
Esk(s,a) [R1(s, a,S1) + Y ∙ Ψ(S1)‰ ≤ Ψ(s),	⑶
with equality if and only ifa P Opsq. Since Opsq is non-empty (by definition), we have for all s P S
Ψ(s) “ maχ (Esι~τ3°)[R1(s,a, S1) + Y ∙ Ψ(S1)‰).
aPA
This recursive condition on Ψ is the Bellman optimality equation for the unique optimal value func-
tion, V‹1, of the MDP with transformed reward R1. Therefore, Ψ(s) “ V‹1(s) for all s P S, and we
can rewrite equation (3) as
Esι~τ (s,aq[Rf(s,α,S)+ Y ∙ H(S1)‰ W 匕(S),	(4)
with equality only for a P O(s).
Now, consider a state s P S. By assumption, for this s, O(s) “ arg maxaPAA‹(s, a). Then for this
state, the actions that attain the optimal value bound in equation (4) are these same optimal actions.
Therefore, R1 induces the same sets of optimal actions from states in S.
Conversely, consider a second MDP M1, differing from M only in its reward function, R1. Assume
the set of optimal actions in states in S agrees with the optimal actions in M for those states. Let V‹1
and A1‹ denote the optimal value and advantage functions for M1 . The Bellman optimality equation
for M1 ensures that, for s P S,
匕⑶ “ maχ (Es"(s,ɑq [R1(s, a, S1) + Y ∙匕(S1)‰)	(5)
aPA
with the maximum attained precisely by the actions a P arg maxaPA (A1‹(s, a)). Setting O(s) “
arg maxaPA (A1‹(s, a)), equation (5) can be rewritten as
Esι~τ(s,a)[R1(s,α,S1) + γ ∙ K(S1)‰ & K(S)	(6)
16
Under review as a conference paper at ICLR 2022
for all s P S and a P A, with equality if and only if a P Opsq.
Now, for s P S, we have arg maxaPApA1‹ps, aqq “ arg maxaPApA‹ps, aqq, because M and M1 have
matching sets of optimal actions for these states (by assumption). Then equation (6) shows that R1 is
produced from R by an optimality-preserving transformation with Opsq “ arg maxaPApA1‹ps, aqq
(and Ψ(s) “ W(s)).	□
We are now in a position to prove the results from the main text:
Theorem 3.3.	Given an MDP, let O(sq “ arg maxaA‹ (s, aq. A maximally supportive optimal
policy determines R up to optimality-preserving transformations with O.
Proof. An arbitrary maximally supportive optimal policy determines the set of optimal actions from
all states in the MDP. Its invariances follow as a special case of Lemma B.2, with S “ S.	口
Theorem 3.5.	Given an MDP, consider the distribution of trajectories induced by a maximally
supportive optimal policy. Let S be the set of states in supported trajectories. Let O be the set
of functions O defined on S such that O(sq “ arg maxaA‹ (s, aq for all s P S. The induced
distribution of trajectories determines R up to optimality-preserving transformations with O P O.
Proof. The distribution of trajectories can be factored into separate distributions ∏*(s) P ∆(A)
for each state s P S (in a manner similar to Theorem 3.4, as proved above). These individual
distributions determine the set of optimal actions within those states.
Noting that S is clearly closed under optimal actions in the MPD since all optimal actions are
supported by the policy, the invariance result follows from Lemma B.2.	口
B.3 Proofs for Section 3.2 Results
Theorem 3.6.	Given an MDP, the return function restricted to possible trajectory fragments, Gζ,
determines R up to a mask of impossible transitions;
Proof. The result is immediate, since the restricted domain still includes all possible transitions (as
length one trajectory fragments with return equal to the reward of the transition), and no fragments
with impossible transitions.	口
Theorem 3.7.	Given an MDP, the return function restricted to possible and initial trajectories, Gξ,
determines R up to zero-initial potential shaping and a mask of unreachable transitions.
Proof. The result follows from Lemma A.2 with k = 0.	口
Theorem 3.8.	Given an MDP the distribution Ofcomparisons OfPossible trajectory fragments, We,
determines R up to a mask of impossible transitions.
Proof. Since We can be derived from G《,it is invariant to a mask of impossible transitions by
Theorem 3.6. Conversely, We determines R for all possible transitions. This is because R(s, a, s1)
is encoded in the Boltzmann distribution of comparisons between the length zero trajectory fragment
ζ0 “ (sq and the length one trajectory fragment ζ1 “ (s, a, s1q, and can be recovered as follows:
→
P(Z YZ Z q =	eχp(βG(ζl))
(ZO —β ζ1q — exp(βG(Zoqq ' exp(βG(Zιqq
曾	1、	1 , P p(Zo Me Ziq ʌ
R(S,a,sq=β - ln(-P(ZO Me Ziq 卜
exp(βR(s, a, s1qq
exp(β ∙ 0q ' exp(βR(s, a, s1qq
Therefore We is invariant to precisely a mask of impossible transitions.
□
17
Under review as a conference paper at ICLR 2022
Theorem 3.9.	Given an MDP, the distribution of comparisons of possible and initial trajectories,
We, determines R UP to k-initial potential shaping and a mask of unreachable transitions.
Proof. Note that as We can be derived from Gξ, by Theorem 3.7, We is invariant to 0-initial potential
shaping and a mask of unreachable transitions. It is additionally invariant to k-initial potential
shaping for arbitrary constants k P R, and no other transformations: Gξ can be recovered from Ve
up to a constant (we can compare all possible initial trajectories to an arbitrary reference trajectory
and recover their relative return using a similar manipulation as above, but we can’t determine the
return of the reference trajectory). From there, the precise invariance follows from Lemma A.2. □
Theorem 3.10.	We have the following bounds on the invariances of the noiseless order of possible
trajectory fragments, W<. InaUMDPs:
(1)	W < is invariant to positive linear scaling and a mask OfimPoSSible transitions; and
(2)	W< is not invariant to transformations other than zero-preserving monotonic transforma-
tions or masks of impossible transitions.
Moreover, there exist MDPs attaining each of these bounds.
Proof. For (1), positive linear scaling of reward by a constant c leads to the same scaling of the
return of each trajectory fragment, and this always preserves the relation W<, since for any C > 0,
C - G(Zi) ≤ c ∙ G(Z2) 0 G(Zi) ≤ G(Z2) for all pairs of trajectory fragments Zι, Q. Moreover, w<
inherits invariance to a mask of impossible transitions from Gζ (Theorem 3.6).
For (2), let R1 be produced from R via some transformation that is neither a mask of impossi-
ble transitions nor a zero-preserving monotonic transformation. It must be that either R1 fails to
preserve the ordinal comparison of two possible transitions, or that it fails to preserve the set of
zero-reward possible transitions, compared to R. In the first case, consider two possible transitions
whose rewards are not preserved, xi and x2. Without loss of generality suppose R(xi) ≤ R(χ2)
but R1(χ1) > R1 (χ2). This corresponds to a change in w<'s comparison of the length one trajec-
tories formed from xi and x2, namely xi W< χ2 from true to false. Similarly, in the second case,
the comparisons between the transition whose reward became or ceased to be zero and a length one
trajectory (with return 0) will have changed. Therefore, W< is not invariant to such transformations.
The bound (1) is attained by the following MDP invariant precisely to positive linear scaling and a
mask of impossible transitions. Let S “ tsu, A “ tai, a2u, R(s, ai, s) “ 1, and R(s, a2, s) “
1 ` γ. Since R(s, a2, s) “ R(s, ai, s) ` γR(s, ai, s), the corresponding order relation will contain
both (s,a2,s) w< (s,ai,s,ai,s) and (s,ai,s,ai,s) W< (s,a2,s). This property requires that
R(s, ai, s) “ (1 ' γ) ∙ R(s, a2, s), which is preserved only by linear scaling of R. (Non-positive
linear scaling is already ruled out by (2)).
The bound (2) is attained by the following MDP invariant to arbitrary zero-preserving monotonic
transformations. Let S “ tsi, s2u, A “ tau, with possible transitions (si, a, s2) and (s2, a, s2),
and R(si,a, s2) > R(s2,a,s2) “ 0. Any zero-preserving monotonic transformation of R preserves
the ordering of all possible trajectory fragments, namely that all nonempty trajectories starting in si
have positive return and all other possible trajectories have zero return.	□
Theorem 3.11.	Given an MDP, the noiseless order ofpossible and initial trajectories, W<, is invari-
ant to k-initial potential shaping, positive linear scaling, and a mask of unreachable transitions.
Proof. The pairwise Boltzmann distributions of We can be used to derive the noiseless comparisons
of Wξ. Therefore, W< is invariant to k-initial potential shaping and a mask of unreachable transitions.
That Wξ is also invariant to positive linear scaling follows from a similar argument as for the first
bound in Theorem 3.10, proved above.	□
18
Under review as a conference paper at ICLR 2022
B.4 Proofs for Section 3.3 Results
Theorem 3.12.	Given an MDP, let Opsq “ arg maxaA‹ps, aq. Then the set of optimal policies
determines R up to optimality-preserving transformations with O.
Proof. The set of all optimal policies determines a maximally supportive policy, for example by
constructing a policy that supports all actions supported by any policy in the set. Likewise, a maxi-
mally supportive policy determines the set of optimal policies, namely as the set of all policies whose
support is a subset of the maximally supportive policy. Therefore, these objects share precisely the
same invariances.	□
B.5 Proofs for Section 4 Results
Theorem 4.1.	Consider an MDP (S, A, τ, μo, R, Y), a policy π, andalternative transition dynamics
T1 with T(s, a) ‰ T 1(s, a) for all S P S, a P A. Given a function Q1 : S XA → R, there exists a
reward function R1, produced from R by S 1 -redistribution under τ, such that Q1 is the Q-function
for π under R1 and T1.
Proof. Per definition 2.3, that R1 is produced from R by S1-redistribution under T requires, for all
s P S and a P A,
Esι~τ(s,a) [R1(s, a, S1)‰ “ Esι~τ(s,a) “R(s, a S 1)‰ .	⑺
Per equation (1), that Q1 is the Q-function for π under R1 and T1 requires, for all s P S and a P A,
Q1(s, a)“ Esι~τi(s,a),Ai~∏(si) [R1(s, a, S1) ' γQ1(S1, A1)‰ ,
or, equivalently,
Esι~τ ιps,a)[R(s, a, S1)‰ “ Q1(s, a) ´ Y E'~τ ι(s,a),4~π(Si) [Q1(S1, A1)‰ .	(8)
For brevity, denote the right-hand sides of equations (7) and (8) as R(s, a) and Q(s, a) respectively.
Let s P S and a P A. By assumption, T1 (s, a) ‰ T(s, a). Therefore, for at least two s1, denoted
si and s2, We have T 1(s1∣s, a) V T(s1|s, a) and T 1(s2∣s, a) > T(s2∣s, a). For the remaining S, set
R(s, a, s1) “ 0. This reduces equations (7) and (8) to the following:
T(si|s, a) ∙ R1(s, a, s；) ' T(s2∣s, a) ∙ R1(s, a, s$) “ R(s, a)
T 1(s1∣s, a) ∙ R(s, a, s；) ' T 1(s2∣s, a) ∙ R(s, a, s$) “ Q(s, a)
This system has a solution as its determinant is T 1(s2∣s, a) ∙ T(s；|s, a) — T(s2∣s, a) ∙ T 1(s1∣s, a) > 0.
□
Theorem 4.2.	Given data sources X and Y , let (X, Y ) denote the combined data source formed
from X and Y. If X and Y are incomparable, then (X, Y) Y X and (X, Y) Y Y.
Proof. Transformations that preserve (X, Y) necessarily preserve X, therefore (X, Y) W X. But
since X and Y are incomparable, there is some transformation that preserves X and not Y. This
transformation does not preserve (X, Y). Therefore, (X, Y) Y X. Similarly, (X, Y) Y Y.	□
We note that the above result is also an elementary consequence of the lattice structure of the partial
order of partition refinement (Aigner, 1979, §I.2.B), since the combined data source corresponds to
the meet of the original data sources.
19
Under review as a conference paper at ICLR 2022
C Other Spaces of Reward Functions
Hitherto We have assumed reward functions are members of SXAXS → R. That is, they are
deterministic functions of transitions, depending on the state, action and the successor state. In this
appendix, we discuss several alternative spaces of reward functions, and their implications for the
invariance properties of various objects derived from the reward function.
C.1 Restricted-domain Reward Functions
It is common in both reinforcement learning and reward learning to consider less expressive spaces
of reward functions. In particular, the domain of the reward function is often restricted to S or S ^ A.
When modelling a task, the choice of reward function domain is usually a formality: An MDP taking
full advantage of the domain S XAXS has an ‘equivalent, MDP with a restricted domain, and some
added auxiliary states (Russell & Norvig, 2009, §17). Conversely, reward functions with restricted
domains can be viewed as a special case of functions from S XAXS where the functions are constant
in the final argument(s). Restricting the domain can be an appealing simplification when modelling
a task, hence the popularity of these formulations.
When modelling a data source, this equivalence may not apply: We may not have access to data
regarding auxiliary states, and so assuming a restricted domain effectively assumes the latent reward
is indeed constant with respect to the successor state (and possibly the action) of each transition.
This assumption may or may not be warranted.
If a restricted domain of S or S XA is preferred, then our invariance results can be adapted in a
straightforward manner. In general, since we are effectively considering a subspace of candidate
reward functions for transformations, ambiguity can only decrease. In particular, these restrictions
have two main consequences.
Firstly, the reward function transformation of S1-redistribution vanishes to the identity transforma-
tion, since it allows variation only in the successor state argument of the reward function, which
is now impossible. This reduces the effective ambiguity of the Q-function and all derivative ob-
jects. Notably, the Q-function uniquely identifies the reward function, and Boltzmann policies have
the same invariances as Boltzmann comparisons between trajectories. Restricting the domain to S
means the (state) value function for an arbitrary known policy also uniquely identifies the reward
function, but doesn’t otherwise alter the invariances we have explored.
Secondly, for most MDPs, the available potential shaping transformations are restricted, but not
eliminated. The function added in a potential shaping transformation (Y ∙ Φ(s1) — Φ(s)) nominally
depends on the successor state of the transition. Some transformed reward functions may rely on
this dependence, falling outside of the restricted domain. However, some non-zero transformations
will usually remain. For example, in a discounted MDP without terminal states, a non-zero constant
potential function Φ(sq “ k does not effectively depend on s, and the reward transformation of
adding Y ∙ Φ(s1) — Φ(s) “ (γ — 1)∙ k to a reward function does not introduce a dependence on S. In
general, the set of remaining potential shaping transformations will depend on the network structure
of the MDP. At the extreme, in a deterministic MDP with state-action rewards, all potential shaping
transformations are permitted, since a dependence on s1 can be satisfied by a.
C.2 Stochastic Reward Functions
Certain tasks are naturally modelled as providing rewards drawn stochastically from some distribu-
tion upon each transition. An even more expressive space of reward functions than we consider is the
space of transition-conditional reward distributions3. Identifying the reward function in this case is
more challenging in general because the latent parameter contains a full distribution of information
for each input, rather than a single point. In the spirit of this paper, we sketch a characterisation of
this additional ambiguity.
A deterministic reward function can be viewed as the conditional expectation of a reward distribution
function. Taking the expectation of the reward distribution for each transition introduces invariance,
3Of course, it’s also possible to consider reward to be distributed conditionally on only the state or state-
action components of a transition, and not the full transition.
20
Under review as a conference paper at ICLR 2022
since the expectation operation is not injective (except in certain restricted cases such as for para-
metric families of distributions that can be parametrised by their mean). The invariance introduced
is akin to S1-redistribution, but with an expectation over the support of the reward distribution rather
than the successor state of each transition.
In the extension of the RL formalism to account for stochastic rewards, this expectation is effectively
the first step in the derivation of each of the objects we have studied. Therefore, all of these objects
inherit this new invariance.
As a consequence, all data sources are effectively more ambiguous with respect to this new latent
parameter. For example, if optimal comparisons between trajectories are understood to be per-
formed based on the pairwise comparison of the expected return of each individual trajectory, then
these comparisons are also invariant to transformations of the reward distributions that preserve their
means.
Fortunately, much of reinforcement learning also focuses on expected return and reward in appli-
cation. Accordingly, most downstream tasks are tolerant to any ambiguity in the exact distribution
of stochastic rewards, beyond identifying the mean. Since this is the same kind of ambiguity that is
introduced by considering the latent parameter of reward learning as a conditional distribution rather
than a deterministic function, our results are still informative for these situations.
C.3 Further Spaces and Future Work
For certain applications, including risk-sensitive RL where non-mean objectives are pur-
sued (Morimura et al., 2010a;b; Dabney et al., 2018), the distribution of stochastic rewards can be
consequential. Moreover, the introduction of stochastic rewards suggests considering data sources
based on samples rather than expectations, such as a data source of trajectory comparisons based on
sampled trajectory returns. Characterising the invariances of these objectives to transformations of
the reward distribution, and thereby their ambiguity tolerance, is left to future work.
In future extensions of this work to handle continuous MDPs, there will be an opportunity to study
the effect of restricting to various parametrised spaces of reward functions. For example, it is com-
mon in reinforcement learning and reward learning to study MDPs with reward functions that are
linear in a feature vector associated with each transition. This kind of restriction may reduce the
available reward transformations compared to those available to a non-parametric reward function,
in a similar manner to restricting the domain of a finite reward function as discussed above.
The relaxation of the Markovian assumption also introduces a broader space of reward functions,
and with it, new dimensions for transformations and invariance. As one example, related to potential
shaping, the non-Markovian additive transformations studied by Wiewiora et al. (2003) will amount
to new invariances of the optimal policy and other related objects.
21