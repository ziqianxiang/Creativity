Under review as a conference paper at ICLR 2022
Gradient play in stochastic games: stationary
points, convergence, and sample complexity
Anonymous authors
Paper under double-blind review
Ab stract
We study the performance of the gradient play algorithm for stochastic games
(SGs), where each agent tries to maximize its own total discounted reward by
making decisions independently based on current state information which is shared
between agents. Policies are directly parameterized by the probability of choosing
a certain action at a given state. We show that Nash equilibria (NEs) and first-order
stationary policies are equivalent in this setting, and give a local convergence
rate around strict NEs. Further, for a subclass of SGs called Markov potential
games (which includes the cooperative setting with identical rewards among agents
as an important special case), we design a sample-based reinforcement learning
algorithm and give a non-asymptotic global convergence rate analysis for both
exact gradient play and our sample-based learning algorithm. Our result shows that
the number of iterations to reach an -NE scales linearly, instead of exponentially,
with the number of agents. Local geometry and local stability are also considered,
where we prove that strict NEs are local maxima of the total potential function and
fully-mixed NEs are saddle points.
1	Introduction
Multi-agent systems find applications in a wide range of societal systems, e.g. electric grid, traffic
networks, smart building and smart cities etc. Given the complexity of these systems, multi-agent
reinforcement learning (MARL) has gained increasing attention in recent years (Daneshfar & Bevrani,
2010; Shalev-Shwartz et al., 2016; Vidhate & Kulkarni, 2017; Xu et al., 2020). Among MARL
algorithms, policy gradient-type methods are highly popular because of their flexibility and capability
to incorporate structured state and action spaces. However, while many recent works (Zhang
et al., 2018; Chen et al., 2018; Wai et al., 2018; Li et al., 2019; Qu et al., 2020) have studied
the sample complexity of multi-agent policy gradient algorithms, due to a lack of understanding
of the optimization landscape in these multi-agent learning problems, most works can only show
convergence to a first-order stationary point. Deeper understanding of the quality of these stationary
points is missing even in the simple identical-reward multi-agent RL setting.
In this paper, we examine this problem from a game-theoretic perspective. We model the multi-agent
system as a stochastic game (SG) where agents can have different reward functions, and study the
dynamical behavior of first-order (gradient-based) learning methods. The study of SGs dates back to
as early as the 1950s by Shapley (1953) with a series of followup works on developing NE-seeking
algorithms, especially in the RL setting (e.g. (Littman, 1994; Bowling & Veloso, 2000; Shoham et al.,
2003; BUSoniU et al., 2010; Lanctot et al., 2017; Zhang et al., 2019a) and citations within). While
well-known classical algorithms for solving SGs are mostly value-based, such as Nash-Q learning
(HU & Wellman, 2003), Hyper-Q learning (TesaUro, 2003), and WoLF-PHC (Bowling & Veloso,
2001), gradient-based algorithms have also started to gain popUlarity in recent years dUe to their
advantages as mentioned earlier (e.g. (Abdallah & Lesser, 2008; Zhang & Lesser, 2010; Foerster
et al., 2017)). In this work, we aim to gain a deeper Understanding of the strUctUre and qUality of
first-order stationary points for these gradient-based methods, with a particUlar focUs on answering
the following qUestions: 1) How do the first-order stationary points relate to the NEs of the Underlying
game?, 2) Do gradient-based algorithms gUarantee convergence to a NE?, 3) What is the stability of
the individUal NEs?, and 4) How shoUld agents learn Using local samples from the environment?.
These qUestions have already been widely discUssed in other settings, e.g., one-shot (stateless) finite-
action games (Shapley, 1964; Crawford, 1985; Jordan, 1993; Krishna & Sjostrom, 1998; Shamma &
1
Under review as a conference paper at ICLR 2022
Arslan, 2005; Kohlberg & Mertens, 1986; Van Damme, 1991), one-shot continuous games (Mazumdar
et al., 2020), zero-sum linear quadratic (LQ) games Zhang et al. (2019b), etc. There are both negative
and positive results depending on the settings. For one-shot continuous games, (Mazumdar et al.,
2020) proved a negative result suggesting that gradient flow has stationary points (even local maxima)
that are not necessarily NEs. Conversely, Zhang et al. (2019b) designed projected nested-gradient
methods that provably converge to NEs in zero-sum LQ games. However, much less is known in the
tabular setting of SGs with finite state-action spaces.
Contributions. In our paper, we consider the gradient play algorithm for the infinite time-discounted
reward SGs where an agent’s individual policy is directly parameterized by the probability of choosing
an action from the agent’s own action space at a given state. We focus on the tabular setting where
state and action spaces are finite. Through generalizing the gradient domination property in (Agarwal
et al., 2020) to the multi-agent setting, we first establish the equivalence of first-order stationary
policies and Nash equilibria. We then show that strict NEs are locally asymptotically stable under
gradient play and provide a local convergence rate analysis.
Additionally, we study the global convergence for a special class of SGs called Markov potential
games (MPGs) (Gonzalez-Sanchez & Hernandez-Lerma, 2013; Macua et al., 2018; Leonardos et al.,
2021), which includes identical reward multi-agent RL (Tan, 1993; Claus & Boutilier, 1998; Panait &
Luke, 2005) as an important special case. In this setting, we first show that exact gradient play can
find an E-NaSh equilibrium within O (ISnPiIAii) steps. Then, We design a sample-based gradient
play algorithm and show that it can find an -Nash equilibrium with high probability in a fully-
decentralized manner using O (∣6poly (ι--γ, |S|, maxi |4|)) samples, where |S|, |Ai| denote the
size of the state space and action space of agent i respectively. The convergence rate shows that the
number of iterations to reach an E-NE scales linearly with the number of agents. In the sample-based
learning, agents only need to observe the state, their own actions, and their own rewards. The key
enabler of the learning is the existence of an underlying averaged MDP for each agent when other
agents’ policies are fixed. Our learning method can be viewed as a model-based policy evaluation
method with respect to agents’ averaged MDPs. This averaged MDP concept could be applied to
design many other MARL algorithms, especially policy-evaluation-based methods. We also study
the local geometry around some special classes of equilibrium points, showing that strict NEs are
local maxima of the total potential function and that fully mixed NEs are saddle points. Lastly, all the
algorithms studied in this paper have been numerical tested and results are provided in Appendix A.
Comparison to other MARL algorithms: For MPGs with continuous state and action spaces,
there are studies about learning either the open-loop (Gonzalez-Sanchez & Hernandez-Lerma, 2013;
Zazo et al., 2016) or the closed-loop (Macua et al., 2018) NEs for MPGs. These works generally
assume full model information and solve the problem via optimal control. There are two recent
arXiv preprints (Mguni, 2020; Leonardos et al., 2021; Mguni et al., 2021) studying MPGs that are
similar to our MPG setting. In particular, Leonardos et al. (2021) also studies gradient play for MPG.
Both of our papers share similar results on MPGs but the sample-based methods are designed from
different perspectives.1 In addition, our papers studies general SGs besides MPG. Moreover, our
concept of “averaged” MDPs could also serve as a useful tool for the design and analysis of other
MARL algorithms. Beyond these MPG works, decentralized Q-learning introduced in Arslan &
Yuksel (2016) might be the closest to the setting considered in this paper. They consider the identical
interest case and only show asymptotic convergence to the set of NEs. There are other recent works
that also study learning for general-sum or zero-sum stochastic games. However the settings they
consider are different from our setting, for example, Daskalakis et al. (2021) considers convergence
to NE for two player zero-sum games, while Song et al. (2021) considers convergence to correlated
equilibrium for finite time horizon general-sum games. On the other hand, Zhang et al. (2018); Li
et al. (2019); Qu et al. (2019) consider slightly different MARL settings, where agents collaboratively
maximize the summation of agents’ reward with either full or partial state observation. They also
require communication between neighboring agents for a better global coordination.
1Leonardos et al. (2021) considers Monte Carlo, model-free gradient estimation. The sample complexity is
derived under the condition that the estimation is unbiased, which is difficult to hold in general. Interestingly, both
sample complexities are O(*). It is an interesting question to study whether such dependence is fundamental
or not. We also remark that Leonardos et al. (2021) and this work are done in parallel.
2
Under review as a conference paper at ICLR 2022
2	Problem setting and preliminaries
We consider a stochastic game (SG) M = (N, S, A = Ai ×∙∙∙× An, P, r = (ri,..., rn), γ, P)
with n agents (Shapley, 1953) which is specified by an agent set N = {1, 2, . . . , n}, a finite
state space S, a finite action space Ai for each agent i ∈ N, a transition model P where
P (s0 |s, a) = P (s0 |s, ai, . . . , an) is the probability of transitioning into state s0 upon taking ac-
tion a := (ai, . . . , an) in state s where ai ∈ Ai is action of agent i, agent i’s reward function
ri : S × A → [0, 1], a discount factor γ ∈ [0, 1), and an initial state distribution ρ over S.
A stochastic policy π : S → ∆(A) (where ∆(A) is the probability simplex over A) specifies a
strategy in which agents choose their actions jointly based on the current state in a stochastic fashion,
i.e. Pr(at∣st) = π(a∕st). A distributed stochastic policy is a special subclass of stochastic policies,
with π = πi × . . . × πn, where πi : S → ∆(Ai). For distributed stochastic policies, each agent takes
its action based on the current state s independently of other agents’ choices of actions, i.e.:
n
Pr(at∣st) = ∏(at∣st) = ɪɪ ∏i(ai,t∣St),	at = (ai,t,..., an,t).
i=i
For notational simplicity, We define: ∏i(aι|s) := Qi∈∕ ∏i(a∕s), where I ⊆ N is an index set.
Further, we use the notation -i to denote the index set N \{i}.
We consider direct distributed policy parameterization, where agent i’s policy is parameterized by θi :
πi,θi (ai |s) = θi,(s,ai), i = 1, 2, . . . , n.	(1)
For notational simplicity, we abbreviate πi,θi (ai|s) as πθi (ai|s), and θi,(s,ai) as θs,ai. Here θi ∈
∆(Ai)lSl, i.e. θi is subject to the constraints θsg ≥ 0 and Paa∈a. θsg = 1 for all S ∈ S.
The global joint policy is given by: ∏(a|s) = Qn=i ∏θi(a∕s) = Qn=I。§3. We use Xi :=
△(Ai)|SI, X := Xi ×∙∙∙×Xn to denote the feasible region of θi and θ.
Agent i’s value function Viθ : S → R, i ∈ N is defined as the discounted sum of future rewards
starting at state s via executing πθ, i.e.
∞
Viθ(s) := E	γtri(st,at) πθ, s0 = s ,
t=0
where the expectation is with respect to the random trajectory T = (st, at, ri,t)∞=o where at 〜
∏θ(∙∣st), st+i = P(∙∣st, at). We denote agent i's total reward starting from initial state so 〜P as:
Ji(θ) = Ji(θι,...,θn) ：= Eso〜ρViθ(so).
In the game setting, Nash equilibrium is often used to characterize the performance of agents’ policies.
Definition 1. (Nash equilibrium) A policy θ* = (θɪ ,...,θnτ) is called a Nash equilibrium (NE) if
Ji(θi,θ-i) ≥ Ji(θ0,θ-i),	∀θi ∈Xi,	i ∈ N
The equilibrium is called a strict NE if the inequality holds strictly for all θi0 ∈ Xi and i ∈ N. The
equilibrium is called a pure NE if θ* corresponds to a deterministic policy. The equilibrium is called
a mixed NE ifit is not pure. Further, the equilibrium is called a fully mixed NE ifevery entry of θ* is
strictly positive, i.e.: 8；& > 0, ∀ ai ∈ Ai, ∀ S ∈ S, i ∈ N
We define the discounted state visitation distribution dθ of a policy πθ given an initial state distribution
P as:	∞
dθ (s) := EsO 〜P(I - Y) XγtPrθ(st = s|so),	(2)
t=o
where Prθ(st = s|so) is the state visitation probability that st = s when executing πθ starting at state
so . Throughout the paper, we make the following assumption on the SGs we study.
Assumption 1. The stochastic game M satisfies: dθ (s) > 0, ∀s ∈ S, ∀θ ∈ X.
Assumption 1 requires that every state is visited with positive probability, which is a standard
assumption for convergence proofs in the RL literature (e.g. (Agarwal et al., 2020; Mei et al., 2020)).
3
Under review as a conference paper at ICLR 2022
Similar to centralized RL, we define agent i’s Q-function Qiθ : S × A → R and its advantage function
Aiθ : S × A → R as:
∞
Qiθ(s, a) := E	γtri(st, at) πθ, s0 = s,a0 = a ,	Aiθ(s,a) := Qiθ(s, a) - Viθ(s).
t=0
'Averaged' Markov decision process (MDP): We further define agent i's 'averaged' Q-function
Qiθ : S × Ai → R and ‘averaged’ advantage-function Aiθ : S × Ai → R as:
Qθ (s,ai):= E∏θ-i (a-i∣s)Qθ (s,ai,a-i), Aθ (s,ai):= E∏θ-i (a-i∣s)Aθ (s,ai,a-i). (3)
Similarly, we define agent i’s 'averaged' transition probability distribution Piθ : S × S × Ai → R,
and 'averaged' reward riθ : S × Ai → R as:
Pi(s0∣s, ai) ：= E∏θ-i(a-i|s)P(s0∣s,ai,a-i),	rθ(s,ai):= T∏θτ(a-i∣s)ri(s, ai,a-i)
a-i	a-i
From its definition, the averaged Q-function satisfies the following Bellman equation:
Lemma 1. Qθ satisfies: Qθ(s,a，)= r?(s,ai)+ Y X ∏θi(ai∣s0)Piθ(s0∣s,ai)Qθ(s0,ai)	(4)
s0,a0i
Lemma 1 suggests that the averaged Q-function Qiθ is indeed the Q-function for the MDP defined on
action space Ai , with riθ , Piθ as its stage reward and transition probability respectively. We define
this MDP as the 'averaged' MDP of agent i, i.e., Miθ = (S, Ai, Piθ, riθ, γ, ρ). The notion of an
‘averaged’ MDP will serve as an important intuition when designing the sample-based algorithm.
Note that the ‘averaged’ MDP is only well-defined when the policies of the other agents θ-i are kept
fixed. When this is indeed the case, agent i can be treated as an independent learner with respect to
its own ‘averaged’ MDP. Thus, various classical policy evaluation RL algorithms can then be applied.
3	Exact Gradient Play for General Stochastic Games
Under direct distributed parameterization, the gradient play algorithm is given by:
Exact Gradient Play:	θ(t+1) = ProjXi (θ(t) + ηVθi Ji(θ(t))),	η > 0.	(5)
Gradient play can be viewed as a ‘better response’ strategy, where agents update their own parameters
by gradient ascent with respect to their own rewards. A first-order stationary point is defined as such:
Definition 2. (First-order stationary policy) A policy θ* = (θɪ,..., θ^) is called a first-order
stationary policy if (θ0 — θ*)>Vθi Ji (θ*) ≤ 0, ∀θi ∈ Xi, i ∈ N.
It is not hard to verify that θ* is a first-order stationary policy if and only if it is a fixed point under
gradient play (Equation (5)). Comparing Definition 1 (of NE) and Definition 2, we know that NEs are
first-order stationary policies, but not necessarily vice versa. For each agent i, first-order stationarity
does not imply that θi is optimal among all possible θ% given θ-%. However, interestingly, We will
show that NEs are equivalent to first-order stationary policies due to a gradient domination property
that we will show later. Before that, we first calculate the explicit form of the gradient Vθi Ji.
Policy gradient theorem (Sutton et al., 1999) gives an efficient formula for the gradient:
VθEso〜ρViθ(so) = ι--γEs〜dθEa〜∏θ(∙∣s)[Vθ log ∏θ(a∣s)Qθ(s,a)],	i ∈ N. (6)
Applying Equation (6), the gradient Vθi Ji can be written explicitly as follows:
Lemma 2. (Proof in Appendix D) For direct distributed parameterization (Equation (1)),
kJ)= ；	dθ (S)Qi(s, ai)	⑺
∂θs,ai	1 — γ	i
4
Under review as a conference paper at ICLR 2022
Gradient domination and the equivalence between NE and first-order stationary policy.
Lemma 4.1 in Agarwal et al. (2020) established gradient domination for centralized tabular MDP
under direct parameterization. We can show that a similar property still holds for stochastic games.
Lemma 3. (Gradient domination, proof in Appendix E.) For direct distributed parameterization
(Equation (1)), we have that for any θ = (θ1, . . . , θn) ∈ X:
Ji(θi0,θ-i)-Ji(θi,θ-i) ≤
dθ0
dθ
max (θi - θi)>Vθi Ji(θ),	∀θ0 ∈ Xi, i ∈ N (8)
∞ θi∈Xi
where ∣∣ d⅛k ：= maxs ^s, and θ0 = (θi,θ-i).
For the single-agent case (n = 1), Equation (8) is consistent with the result in Agarwal et al. (2020),
i.e.: J(θ0) 一 J(θ) ≤ Uddθ0∣∣ maxθ∈χ(θ 一 θ)>VJ(θ). However, when there are multiple agents,
the condition is much weaker because the inequality requires θ-i to be fixed. When n = 1, gradient
domination rules out the existence of stationary points that are not global optima. For the multi-agent
case, the property can no longer guarantee the equivalence between first-order stationarity and global
optimality; instead, it links the stationary points with NEs as shown in the next theorem whose proof
is in Appendix E.
Theorem 1. Under Assumption 1, first-order stationary policies and NEs are equivalent.
Local convergence for strict NEs Although the equivalence of NEs and stationary points under
gradient play has been established, it is in fact difficult to show that gradient play converges to
these stationary points. Even in the simpler static (stateless) game setup, gradient play might fail to
converge (Shapley, 1964; Crawford, 1§85; Jordan, 1993; Krishna & Sjostrom, 1998). One major
difficulty is that the vector field {Vθi Ji(θ)}in=1 is not a conservative vector field. Accordingly, its
dynamics may display complicated behavior. Thus, as a preliminary study, instead of looking at
global convergence, we focus on the local convergence and restrict our study to a special subset of
NEs - the strict NEs. We begin by giving the following characterization of strict NEs:
Lemma 4. Given a stochastic game M, any StrictNE θ* is pure, meaning that for each i and S,
there exist one a*(S) such that θsa = 1{ai = a*(s)}. Additionally,
i) a*(s)	= arg max Aθ*	(s,ai),	ii) Aθ* (s,a*(s)) = 0;	iii)	Aθ*	(s,ai)	<	0,	∀ ɑi	= a*(s)	(9)
ai
Based on this lemma, we define the following for studying the local convergence of a strict NE θ* :
∆θ (s) := min ∣Aθ* (s,ai)∣ , ∆θ* := minmin	—dθ*(s)∆θ* (S) > 0.	(10)
ai = a*(s) I	I	is 1 一 Y
Theorem 2. (Local finite time convergence around strict NE) Define the metric of policy parameters
as: D(θ∣∣θ0) := maxι≤i≤n maxs∈s |以,§ 一 θi,skι, where ∣∣∙∣∣ι denote the '1 - norm. Suppose θ*
is a strict Nash equilibrium, then for any θ(0) such that
D(θ ⑼"θ*) ≤ 8n|T(PjAiI) , running
gradient play (Equation (5)) will guarantee D(θ(t+1)∣∣θ*) ≤ max {D(θ(tl∣θ*) 一 *一,θ}
means that gradient play is going to converge within d 2D* θ[lθ ) [ steps.
which
Proofs of Lemma 4 and Theorem 2 are provided in Appendix F. The convergence only requires a
finite number of steps and the stepsize η can be chosen arbitrarily large so that exact convergence can
happen in even just one step. However, the caveat is that we need to assume that the initial policy is
sufficiently close to θ*. For numerical stability considerations, one should pick reasonable stepsizes
to run the algorithm to accommodate random initializations. Theorem 2 also shows that the radius of
region of attraction for strict NEs is at least
∆θ* (i-γ)3
8n∣S∣(Pn=ι IAiI) ,
and thus θ* with a larger ∆θ* , i.e., a
larger value gap between the optimal action and other actions, will have a larger region of attraction.
We would like to further remark that Theorem 2 only focuses on the local convergence property, the
way to interpret the theorem is that, if there exists a strict NE, then it is locally asymptotic stable
under gradient play. However, it does not claim to solve the global existence or convergence of the
strict NEs.
5
Under review as a conference paper at ICLR 2022
4	Gradient play for Markov potential games
We have discussed that the main problem for the global convergence of gradient play for general
SGs is that the vector field {^ Ji(θ)}n=ι is not conservative. Thus, in this section, We restrict our
analysis to a special subclass where the vector field is conservative, which in turn enjoys global
convergence. This subclass is generally referred to as a Markov potential game (MPG) in the literature.
Definition 3. (Markov potential game (Macua et al., 2018)) A stochastic game M is called a Markov
potential game ifthere exists a potential function φ : S ×Aι X …X An → R such thatfor any agent
i and any pair of policy parameters (θi0, θ-i), (θi, θ-i) :
∞
E Xγtri(st, at)π = (θi0, θ-i), s0 = s
t=0
∞
- E	γtri(st,at)π = (θi, θ-i), s0 = s
t=0
∞
=E Xγtφ(st,at)π = (θi0, θ-i), s0 = s
t=0
∞
-E	γtφ(st,at)π = (θi,θ-i),s0 = s
t=0
∀ s.
As shoWn in the definition, the condition of a MPG is admittedly rather strong and difficult to verify
for general SGs. MaCUa et al. (2018); Gonzalez-Sanchez & Hernandez-Lerma (2013) found that
continuous MPGs can model applications such as the great fish War (Levhari & Mirman, 1980), the
stochastic lake game (Dechert & O’Donnell, 2006), medium access control (Macua et al., 2018) etc.
There are also efforts attempting to identify conditions such that a SG is a MPG, e.g., Macua et al.
(2018); Leonardos et al. (2021); Mguni (2020). In Appendix B, We provide a more detailed discussion
on MPGs, including a necessary condition (Lemma 5) of MPG, counterexamples of stage-Wise
potential games that are not MPG, sufficient conditions for a SG to be a MPG, and application
examples of MPG. Nevertheless, identifying sufficient and necessary conditions and broadening the
applications of MPG are important furture directions.
Given a policy θ, We define the 'total PotentiaIfUnCtion' Φ(θ):=旧§。〜p(.)[P∞=0 γtφ(st, at)∣ ∏θ]
for a MPG. The folloWing proposition guarantees a MPG has at least one NE and it is a pure NE.
Proposition 1. (Proof in Appendix G) For a Markov potential game, there is at least one global
maximum θ* of the total potentialfunction Φ, i.e.: θ* ∈ arg maxθ∈χ Φ(θ) that is a pure NE.
From the definition of the total potential function We obtain the folloWing relationship
Ji(θi0,θ-i) -Ji(θi,θ-i) =Φ(θi0,θ-i) - Φ(θi, θ-i).	(11)
Thus,
Vθi Ji(θ) = Vθi Φ(θ),
Which means that gradient play (Equation (5)) is equivalent to running projected gradient ascent With
respect to the total potential function Φ, i.e.: θ(t+1) = P rojX (θ(t) + ηVθΦ(θi(t))), η > 0.
To measure the convergence to a NE, We define an -Nash equilibrium as folloWs:
Definition 4. (-Nash equilibrium) Define the 'NE-gap' ofa policy θ as:
NE-gapi (θ) := max Ji (θi0, θ-i) - Ji(θi, θ-i); NE-gap(θ) := maxNE-gapi(θ).
θi0 ∈Xi	i	i
A policy θ is an -Nash equilibrium if: NE-gap(θ) ≤ .
We further assume that the MPG satisfies the folloWing assumption.
Assumption 2. For θ ∈ X, the total potential function Φ(θ) is bounded by: Φmin ≤ Φ(θ) ≤ Φmax.
4.1	Exact gradient play - global convergence and local geometry
In this section, We first focus on the global convergence for exact gradient play (5), Where the gradient
Vθi Ji can be calculated exactly by agent i. The convergence result is given as folloWs:
Theorem 3.	(Global convergence to Nash equilibria, proof in Appendix H.) Given a MPG with
potential function φ(s, a), suppose the total potential function Φ satisfies Assumption 2. Then with
6
Under review as a conference paper at ICLR 2022
stepsize η = 2 P-Y)A ,∣, θ(t) asymptotically converge to a NE under gradient play (Equation (5)),
i.e., limt→∞ NE-gap(θ(t)) = 0. Further, we have:
T X NE-gap(θ⑴)2 ≤ e2, whenever T ≥ 64M2&max ；：；?? Pn=llAil,	(12)
T 1≤t≤T	(1 - γ )
where M := maxθ,θo∈χ ∣∣ ddθ ∣∣ (by Assumption 1, we know that this quantity is well-defined).
The factor M is also known as the distribution mismatch coefficient that characterizes how the state
visitation varies with the policies. Given an initial state distribution ρ that has positive measure on
every state, M can be at least bounded by M ≤ γ-γ maxθ ∣∣ 野 11	≤ ɪ-^ 巾也1雇§). Also note that
Inequality (12) on the average term T1 Pι≤t≤τ NE-gap(θ(t))2 could be translated to a constant
probability guarantee on single NE-gap(θ(t)). For instance, if we randomly pick one θ(t) from
1 ≤ t ≤ T, then it guarantees that NE-gap(θ(t))2 ≤ 3 ∙ €2 with probability at least 22.2 As a
comparison with centralized learning, if we parameterize the policy in a centralized way, the size of the
action space will be |A| = Qn=ι lAi l and the projected gradient ascent would need O (|S| ";=1 |AiI)
steps to find an €-oPtimal policy (Agarwal et al., 2020); whereas we only need O (|S| ":=1 |AiI) steps
to find an €-NE, which scales linearly with n. However, centralized parameterization can provably
find a global optimum, while distributed parameterization can only find a NE.
Though gradient play is guaranteed to converge to a NE, the exact NE which it converges to is
uncertain, and depends on the initial point as well as the local geometry around the various NEs. As a
preliminary study, we have the following characterization for two special types of NEs. More future
investigation is needed for general NEs.
Theorem 4.	For Markov potential game with Φmin < Φmax (i.e., Φ is not a constant function):
•	A strict NE θ* is equivalent to a strict local maximum ofthe total potential function Φ, i.e.:
∃ δ, s.t. ∀ θ = θ* that satisfies ∣∣θ 一 θ*k ≤ δ, θ ∈ X, we have Φ(θ) < Φ(θ*).
•	Any fully mixed NE θ* is a saddle point with regard to the total potential function Φ,
i.e.: ∀ δ> 0, ∃ θ, s.t. ∣∣θ — θ*∣∣≤ δ and Φ(θ) > Φ(θ*).
Theorem 4 implies that strict NEs are asymptotically stable under first-order methods such as gradient
play; while the fully mixed NEs are not stable under gradient play. We remark that the conclusion
about strict NE in Theorem 4 does not hold for settings other than tabular MPG; for instance, for
continuous games, one can use quadratic functions to construct simple counterexamples (Mazumdar
et al., 2020). Also, similar to the remark after Theorem 2, this theorem focuses on the local geometry
of the NEs but does not claim the global existence or convergence of either strict NEs or fully mixed
NEs.
4.2 Sample-based learning: algorithm design and sample complexity
In this section, we no longer assume access to the exact gradient, but instead estimate it via samples.
Throughout the section, we make the following additional assumption on MPGs:
Assumption 3. ((τ, σS)-Sufficient exploration on states) There exist a positive integer τ and a
σS ∈ (0, 1) such that for any policy θ and any initial state-action pair (s, ai), ∀i, we have
Pr(sτ ls0 = s, a0 = a) ≥ σS, ∀sτ.	(13)
It says that every state has a positive probability of being visited after some time. This assumption is
common in proving finite time convergence (e.g. (Qu et al., 2019; Srikant & Ying, 2019)).
We further introduction the state transition probability under θ PSθ : S × S → R as:
PS (Sls)= X ∏θ (a∣s)P (s0∣s,a).
a
We consider fully decentralized learning, where agent i’s observation only includes state st, its
own action ai,t, and its own reward ri,t:= ri(st, at) at time t. Such fully decentralized learning is
2Here 3, 2 could be replaced by ι-p, P where P ∈ (0,1) is a probability.
7
Under review as a conference paper at ICLR 2022
plausible due to the fact that when θ-i is fixed, agent i can be treated as an independent learner with
the underlying MDP being the ‘averaged’ MDP described in Section 2. With this key observation, we
design a two-timescale ‘model-based’ on-policy learning algorithm, where agents perform policy
evaluation in the inner loop and gradient ascent at the outer loop. The algorithm is provided in
Algorithm 1. Roughly, it consists of three main steps: 1) (Inner loop) Estimate the averaged transition
probability and reward using on-policy samples Piθ , riθ , PSθ . 2) (Inner loop) Calculate averaged
Q-function Qiθ and discounted state visitation distribution dθ , and compute the estimated gradient
accordingly, 3) (Outer loop) Running projected gradient ascent with estimated gradients. Before
discussing our algorithm in more detail, we highlight that the idea of using the “averaged” MDP can
be used to design other learning methods including model-free methods, e.g., using the temporal
difference methods to perform policy evaluation. One caveat is that the “averaged” MDP is only well-
defined when all the other agents use fixed policies. This makes it difficult to extend the two-timescale
framework to single-timescale settings, which is an interesting future direction.
Algorithm 1 Sample-based learning
Require: learning rate η, greedy parameter a, sample trajectory length Tj, total iteration steps TG
For each agent i
for k = 0, 1 . . . , TG - 1 do
for t = 0, 1, . . . , TJ do
Implement policy θ(k) and collect trajectory D(k): D(k)-D(k)∪{st,ai,t,ri,t}, ai,t 〜∏θ(k)(∙∣s)
end for
Estimate Pciθ, rbiθ, PcSθ, Mdiθ by Equation (14), Equation (15), Equation (16) respectively.
Calculate Qiθ, dbθ by Equation (17), Equation (18) respectively.
Estimate the gradient by Equation (19):
Run projected gradient ascent as in Equation (20)
end for
Step 1: empirical estimation of Piθ, riθ, PSθ: Given a sequence {st, ai,t, ri,t}tT=J0 generated by a
θ~^
policy θ := (θi, θ-i), the empirical estimation Piθ of Piθ is given by:
Pi(S ls,ai)
PPt=O 1{st + 1 =s ,st=s,ai,t = ai}
Pt=1	1{st =s,ai,t =ai }
1{S0 = S},
PtT=J1-1 1{St = S, ai,t = ai} ≥ 1
Pt=1 1{St = S, ai,t = ai} = 0
(14)
Here we separately treat the special case where the state and action pair is not visited through the
whole trajectory, i.e., PtT=J1-1 1{St = S, ai,t = ai} = 0 to make Piθ well-defined.
θθ
Similarly, the estimates riθ , PSθ
of riθ , PSθ are given by:
rbiθ(S, ai) :=	I Pt=O 1{st = s,ai,t=ai }ri,t Pt=J0 1{st =s,ai,t =ai} 10,	PtT=J1-11{St PtT=J1-11{St	S, ai,t = S, ai,t =	ai} ≥ 1 ai} = 0	(15)
	(PJ-I 1{st+1=s0,st = s}	PtT=J1-1 1{St = PtT=J1-1 1{St =	S} ≥ 1 S} = 0		
: . PS(SlS):=	P	PJ-I 1{st=s}	, I 1{s0 = s},				(16)
Step 2: estimation of Qiθ, dθ: We slightly abuse notation and use Qiθ, riθ ∈ R|S||Ai| to also denote
the vectors corresponding to the averaged Q-function and reward function of agent i. Similarly,
ρ, dθ ∈ R|S| are used to denote the vectors for P(S) and dθ(s). Define Mi ∈ RlsllAil×lsllAil:
Mθ(s,ai)→(s0,ai) := πθi (ai|sO)Piθ (s0 |s,ai).
8
Under review as a conference paper at ICLR 2022
Then from Lemma 1, Qiθ is given by: (I - γMiθ)Qiθ = riθ
Qθ = (I- γMθ)-1rθ.
-------r
The estimated averaged Q-function Qiθ is given by:3
----:	. ^	-"~--r
Qiθ = (I - γMiθ)-1riθ, where Miθ(s,
0)：= πθi(ailsO)Piθ(s0|s, Oi)
i
(17)
Similarly, from Equation (2), we have that dθ and dθ are given by (derivation see Appendix C):
dθ = (I-Y) (I- YPS>) ρ,	dθ ：= (I-Y) (I-YC ) ρ.
(18)
Then accordingly, the estimated gradient is computed as:
bθs,ai JMk)) =占 dθ (S)C (S，ai).
(19)
Step 3: Projected gradient ascent onto the set of α-greedy policies: Let Un = [ɪ，...，ɪ] ∈ ∆(n)
denote the n dimensional uniform distribution. Define ∆ɑ(n)={θ∣ ∃θ0∈∆(n), s.t. θ = (1-α)θ0+αUn}.
We use Xiα=∆α(∣Ai∣)lsl，Xα:=Xa × X2α X …XXna to denote the set of the a-greedy policies for
θi and θ respectively. Every step after doing gradient ascent, the parameter θ will further be projected
onto Xα, i.e.:
θ(k+1)= ProjXa (θ(k) + nVθi Ji(θ(k)))
(20)
The reason of projecting onto Xα instead ofX is to make sure that every action has positive possibility
of being selected in order to get a relatively accurate estimation of averaged Q-function. Intuitively,
a larger α introduces a larger additional error in the NE-gap; however, a smaller α requires more
samples to estimate the gradient. Thus the choice of α is the tradeoff between the two effects.
Theorem 5. (Sample complexity) Assume that the MPG satisfies Assumption 3. Let M ：=
maxθ,θ0∈x Il 爵|| ∙ In Algorithm 1,for n ≤ 4(P^γAi∣, α = (16M)e, and
206976τnM4∣S∣3 maxi ∣Ai∣31	(16tTg∣S∣2 Pi∣Ai∣λ	、648M2(Φmaχ-Φmm)∣S∣
TJ ≥--------(1 - γ)8e4σS--------log(---------δ--------尸	TG ≥-----------谓------------
with probability at least 1 - δ, we have that:/ PTGI NE-gap(θ(k) )2 ≤ 心
That is, the algorithm can find an -NE with probability at least 1 - δ with
TJTG 〜O (∣6poly (ɪ-"，|S|，max 14))	(21)
samples, where O hides logfactors∙
Comparison with centralized learning: The best known sample complexity bound for single-
agent/centralized MDP is O QSYA(Sidford et al., 2018). Compared with Equation (21), the
centralized bound scales better with respect to g |S|，|41，ɪ:1^. However, as argued in the previous
subsection, the total action space |A| = Qin=1 |Ai| in the centralized bound scales exponentially with
the number of agent n, while our complexity bound only scales linearly. Here, we briefly state the
fundamental difficulties of learning in the SG setting compared with centralized learning, which
also explains why our bound scales worse with respect to the factors g |S|，|Ai |，11γ. 1) Firstly,
the optimization landscape in the SG setting is more complicated. For centralized learning, the
gradient domination property is stronger and accelerated gradient methods (e.g. via natural policy
gradient or entropy regularization) can speed up the convergence of exact gradient from O(£) to
O(ɪ) (Agarwal et al., 2020), or even O(log(ɪ)) (Mei et al., 2020). In contrast, for multi-agent
settings, due to the more complicated optimization landscape, these methods can no longer improve
the dependency on e, which thus makes the outer loop complexity TG larger. 2) Secondly, the behavior
of other agents makes the environment non-stationary, i.e., the averaged Q-function Qiθ as well as the
averaged transition probability distribution Piθ depends on the policy of other agent θ-i. Thus, unlike
centralized learning, where the state transition probability matrix can be estimated in an off-policy or
3
3From the Perron-Frobenius theorem, we know that the absolute values of the eigenvalues of Miθ are upper
bounded by 1, which guarantees that the matrix I - γMiθ is invertible.
9
Under review as a conference paper at ICLR 2022
even offline manner, i.e. using data samples from different policies, Piθ can only be estimated in a
online manner, using samples generated by exactly the same policy θ, which increases the inner loop
complexity TJ. 3) Thirdly, the complicated interactions amongst agents necessitate more care during
the learning process. Algorithms designed for centralized learning that achieve near-optimal sample
complexity are generally Q-learning type algorithms. However, in SGs, it can be shown that having
each agent maximize its own averaged Q-function may actually lead to non-convergent behavior.
Thus, we need to consider algorithms that update in a less aggressive manner, e.g. soft Q-learning, or
policy gradient (which is considered in this paper).
Numerical Results. Due to the space limit, numerical performance for algorithms studied in this
paper are deferred to Appendix A.
5 Conclusion and Discussion
This paper studies the optimization landscape and convergence of gradient play for SGs. For general
SGs, we establish some local convergence results; for MPGs, we establish the global convergence
and design a sample-based method. There are many future directions. Firstly, the assumption of
MPGs is relatively strong compared with the notion of potential games in the one-shot setup, which
might restrict its application to broader settings. More effort would be needed to come up with other
special types of SGs which facilitate learning. It would also be meaningful to combine the insight
from learning in stochastic games to investigate real-life applications, such as dynamic congestion
and routing. Secondly, the sample-based learning algorithm proposed in this paper is only one choice.
Other methods, such as actor-critic, natural policy gradient, Gauss-Newton methods, could also be
considered, which might improve the sample complexity. We note in passing that it is an interesting
and important question to characterize the fundamental complexity of MARL. Thirdly, this paper
only considers direct policy parameterization. To broaden the application of MARL and to strengthen
the results, other types of parameterization such as softmax parameterization and general nonlinear
parameterization should be investigated.
References
Sherief Abdallah and Victor Lesser. A multiagent reinforcement learning algorithm with non-linear
dynamics. Journal OfArtificial Intelligence Research, 33:521-549, 2008.
Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift, 2020.
Gurdal Arslan and Serdar YukSeL Decentralized q-learning for stochastic teams and games. IEEE
Transactions on Automatic Control, 62(4):1545-1558, 2016.
Kazuoki Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical
Journal, Second Series, 19(3):357-367, 1967.
Nathalie Bertrand, Nicolas Markey, Suman Sadhukhan, and Ocan Sankur. Dynamic network conges-
tion games. arXiv preprint arXiv:2009.13632, 2020.
Michael Bowling and Manuela Veloso. An analysis of stochastic game theory for multiagent
reinforcement learning. Technical report, Carnegie-Mellon Univ Pittsburgh Pa School of Computer
Science, 2000.
Michael Bowling and Manuela Veloso. Rational and convergent learning in stochastic games. In
International joint conference on artificial intelligence, volume 17, pp. 1021-1026. Citeseer, 2001.
Lucian BuSoniu, Robert Babuska, and Bart De Schutter. Multi-agent reinforcement learning: An
overview. Innovations in multi-agent systems and applications-1, pp. 183-221, 2010.
Tianyi Chen, Kaiqing Zhang, Georgios B Giannakis, and Tamer Bayar. Communication-efficient
policy gradient methods for distributed reinforcement learning. arXiv preprint arXiv:1812.03239,
2018.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent
systems. AAAI/IAAI, 1998(746-752):2, 1998.
10
Under review as a conference paper at ICLR 2022
Vincent P Crawford. Learning behavior and mixed-strategy nash equilibria. Journal of Economic
Behavior & Organization, 6(1):69-78,1985.
Fatheme Daneshfar and Hassan Bevrani. Load-frequency control: a ga-based multi-agent reinforce-
ment learning. IET generation, transmission & distribution, 4(1):13-26, 2010.
Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. arXiv preprint arXiv:2101.04233, 2021.
W Davis Dechert and SI O’Donnell. The stochastic lake game: A numerical solution. Journal of
Economic Dynamics and Control, 30(9-10):1569-1587, 2006.
Jakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor
Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017.
David GOnzalez-SanChez and Onesimo Hernandez-Lerma. Discrete-time stochastic control and
dynamic potential games: the Euler-Equation approach. Springer Science & Business Media,
2013.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The collected
works of Wassily Hoeffding, pp. 409-426. Springer, 1994.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039-1069, 2003.
James S Jordan. Three problems in learning mixed-strategy nash equilibria. Games and Economic
Behavior, 5(3):368-386, 1993.
Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
Claude Sammut and Achim G. Hoffmann (eds.), Machine Learning, Proceedings of the Nineteenth
International Conference (ICML 2002), University of New South Wales, Sydney, Australia, July
8-12, 2002, pp. 267-274. Morgan Kaufmann, 2002.
Elon Kohlberg and Jean-Francois Mertens. On the strategic stability of equilibria. Econometrica:
Journal of the Econometric Society, pp. 1003-1037, 1986.
Vijay Krishna and Tomas Sjostrom. On the convergence of fictitious play. Mathematics OfOPeratiOnS
Research, 23(2):479-511, 1998.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat,
David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement
learning. arXiv preprint arXiv:1711.00832, 2017.
Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence
of multi-agent policy gradient in markov potential games. arXiv preprint arXiv:2106.01969, 2021.
David Levhari and Leonard Mirman. The great fish war: An example using a dynamic cournot-nash
solution. Bell Journal of Economics, 11(1):322-334, 1980.
Yingying Li, Yujie Tang, Runyu Zhang, and Na Li. Distributed reinforcement learning for decentral-
ized linear quadratic control: A derivative-free policy optimization approach, 2019.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.
Sergio Valcarcel Macua, Javier Zazo, and Santiago Zazo. Learning parametric closed-loop policies
for markov potential games. CoRR, abs/1802.00899, 2018. URL http://arxiv.org/abs/
1802.00899.
Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. On gradient-based learning in continuous
games. SIAM Journal on Mathematics of Data Science, 2(1):103-131, 2020.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In Hal DaUme In and Aarti Singh (eds.), Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 6820-6829. PMLR, 13-18 Jul 2020.
11
Under review as a conference paper at ICLR 2022
David Mguni. Stochastic potential games. arXiv preprint arXiv:2005.13527, 2020.
David Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li, Ying Wen, Joel Jennings,
and Jun Wang. Learning in nonzero-sum stochastic games with potentials. arXiv preprint
arXiv:2103.09284, 2021.
Dov Monderer and Lloyd S Shapley. Potential games. Games and economic behavior, 14(1):124-143,
1996.
Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous
agents and multi-agent systems, 11(3):387-434, 2005.
Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for
multi-agent networked systems, 2019.
Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for
multi-agent networked systems. In Learning for Dynamics and Control, pp. 256-266. PMLR,
2020.
S. Shalev-Shwartz, Shaked Shammah, and A. Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving. ArXiv, abs/1610.03295, 2016.
J. S. Shamma and G. Arslan. Dynamic fictitious play, dynamic gradient play, and distributed
convergence to nash equilibria. IEEE Transactions on Automatic Control, 50(3):312-327, 2005.
doi: 10.1109/TAC.2005.843878.
Lloyd Shapley. Some topics in two-person games. Advances in game theory, 52:1-29, 1964.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095-1100, 1953.
Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: a critical
survey. Technical report, Technical report, Stanford University, 2003.
Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time
and sample complexities for solving markov decision processes with a generative model.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
bb03e43ffe34eeb242a2ee4a4f125e56- Paper.pdf.
Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum markov games with a large
number of players sample-efficiently?, 2021.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd
learning. In Conference on Learning Theory, pp. 2803-2830. PMLR, 2019.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford
Book, Cambridge, MA, USA, 2018. ISBN 0262039249.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057-
1063. Citeseer, 1999.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pp. 330-337, 1993.
Gerald Tesauro. Extending q-learning to general adaptive multi-agent systems. Advances in neural
information processing systems, 16:871-878, 2003.
Eric Van Damme. Stability and perfection of Nash equilibria, volume 339. Springer, 1991.
Deepak A Vidhate and Parag Kulkarni. Cooperative multi-agent reinforcement learning models
(cmrlm) for intelligent traffic control. In 2017 1st International Conference on Intelligent Systems
and Information Management (ICISIM), pp. 325-331. IEEE, 2017.
12
Under review as a conference paper at ICLR 2022
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning
via double averaging primal-dual optimization. NIPS'18,pp. 9672-9683, Red Hook, NY, USA,
2018. Curran Associates Inc.
Weiran Wang and MigUel A. Carreira-Perpinan. Projection onto the probability simplex: An
efficient algorithm with a simple proof, and an application. CoRR, abs/1309.1541, 2013. URL
http://arxiv.org/abs/1309.1541.
Xu Xu, Youwei Jia, Yan Xu, Zhao Xu, Songjian Chai, and Chun Sing Lai. A multi-agent reinforcement
learning-based data-driven method for home energy management. IEEE Transactions on Smart
Grid, 11(4):3201-3211, 2020.
Santiago Zazo, Sergio Valcarcel Macua, Matilde Sanchez-Fernandez, and Javier Zazo. Dynamic
potential games with constraints: Fundamentals and applications in communications. IEEE
Transactions on Signal Processing, 64(14):3806-3821, 2016.
Chongjie Zhang and Victor Lesser. Multi-agent learning with policy prediction. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 24, 2010.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning, pp. 5872-5881. PMLR, 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer Bayar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019a.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Policy optimization provably converges to nash
equilibria in zero-sum linear quadratic games. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/
paper/2019/file/5446f217e9504bc593ad9dcf2ec88dda-Paper.pdf.
A Numerical examples
	&2 = 1	&2 = 2
aι = 1	(-1,-1)	(-3,0)
aι = 2	(0,-3)	(-2,-2)
Table 1: Reward table for Game 1
	s2 = 1	s2 = 2
si = 1	2	0
si = 2	0	1
Table 2: Reward table for Game 2
Game 1: multi-stage prisoner’s dilemma The first example — multi-stage prisoner’s dilemma
model(Arslan & YUksel, 2016) 一 studies exact gradient play for general SG settings. It is a 2-agent
MDP, with S = A1 = A2 = {1, 2}. Assume that the reward for each agent ri(s, a1, a2), i ∈ {1, 2}
is independent of state s and is given by Table 1. The state transition probability is determined by
agents’ previous actions:
P (st+1 = 1|(a1,t, a2,t) = (1, 1)) = 1 - , P (st+1 = 1|(a1,t, a2,t) 6= (1, 1)) =
Here action ai = 1 means that agent i choose to cooperate and ai = 2 means betray. The state s
serves as a noisy indicator, with accuracy 1 - , of whether both agents cooperated (st = 1) or not
(st = 2) in the previous stage t - 1.
The single-stage game corresponds to the famous Prisoner’s Dilemma, and it is well-known that
there is a unique NE (a1, a2) = (2, 2), where both agent decide to betray. The dilemma arises from
the fact that there exists a joint non-NE strategy (1, 1) such that both players obtain a higher reward
than what they get under the NE. However, in the multi-stage case, the introduction of an additional
state s allows agents to make decisions based on whether they have cooperated before. Intuitively,
given that both agents cooperated at the previous stage, it is more beneficial to keep this cooperation
rather than destroy it by betraying. It turns out that cooperation can be achieved in this manner given
that the two agents are patient (i.e., γ is close to 1) and that the indicator s is accurate enough (i.e.
is close to 0). Apart from the fully betray strategy, where both agents will betray regardless of s,
13
Under review as a conference paper at ICLR 2022
there is another strict NE θ* that is 0；=ig=1 = 1, θ*=2,a.=1 = 0, where agents Win cooperate
given that they have cooperated in previous stage, and betray otherwise.
We simulate gradient play for this model and mainly focus on the convergence to the cooperative
equilibrium θ*. The initial policy is set as: 0(=)i@=1 = 1 -0.4δi, θ(=)2g=ι = 0, where the δi’s
are uniformly sampled from [0, 1]. The initialization implies that at the beginning, both agents are
willing to cooperate to some extent given that they cooperated at the previous stage. Figure 1 shows
a trial converging to the NE starting from a randomly initialized policy. The size of the region of
attraction for θ* can be reflected by the ratio of convergence ( ##黑；%赛瑞黑)for multiple trials
with different initial points. An empirical estimate of the volume of the region is the convergence
ratio times the volume of the uniform sampling area; the larger the ratio, the larger the region of
attraction. Table 3 demonstrates the change of the ratio with regard to different values of indicator
error . Intuitively speaking, the more accurately the state s represents the cooperation situation of
the agents, the less incentive agents will have for betraying when observing s = 1, that is, the larger
∆θ*(s = 1) will become, and thus the larger the convergence ratio will be. This intuition matches the
simulation result as well as the theoretical guarantees on the local convergence around a strict NE in
Theorem 2.
Figure 1: (Game 1:) Convergence
to the cooperative NE
€	∆θ*(s = 1)	ratio (mean ± std)%
-0^	-433.3-	(47.8± 5.1)%
^00T	-979.3-	(66.3± 4.3)%
^wτ	2498.6	(77.4± 2.8)%
Table 3: (Game 1:) Relationship of convergence ratio and .
Here we fix γ = 0.95. Convergence ratio is calculated by
#TriaIS thatconverge to θ	H	lrnlnt	ti i	Inntri
#Totalnumberoftrials . Here We calculate one ratio using 100 tri
als and the mean and standard deviation (std) are calculated by
computing the ratio 10 times using different trials. ∆ (S = 1) is
calculated using Equation (10)
Figure 2: (Game 2:) Starting from
a close neighborhood of a fully
mixed NE
Figure 4: (Game 2:) NE-gap for
multiple runs
Figure 3: (Game 2:) Total reward
for multiple runs
Game 2: coordination game Our second numerical example studies the empirical performance of
exact gradient play for an identical-reward game which is a special class of Markov potential game.
Consider a 2-agent identical reward coordination game problem with state space S = S1 × S2 and
action space A = A1 × A2, where S1 = S2 = A1 = A2 = {1, 2}. The state transition probability
is given by:
P (si,t+1 = 1|ai,t = 1) = 1 - , P (si,t+1 = 1|ai,t = 2) = , i = 1, 2.
The reward table is given by Table 2, where agents will only be rewarded if they are in the same state,
and state 1 has a higher reward than state 2. Coordination games can be used to model the network
effect in economics, where an agent reaps benefits from being in the same network as other agents.
For this specific example, there are two networks with different utilities. Agents can observe the
occupancy of each network, and take actions to join one of the networks based on the observation.
There is at least one fully-mixed NE where both agents join network 1 with probability 3(1--2E)
regardless of the current occupancy of networks, and there are 13 different strict NEs that can be
verified numerically (computation of the NEs as well as detailed settings can be found in Appendix
N). Figure 2 shows a gradient play trajectory whose initial point lies in a close neighborhood of the
14
Under review as a conference paper at ICLR 2022
mixed NE. As the algorithm progresses, we see that the trajectory in Figure 2 diverges from the
mixed-NE, indicating that the fully-mixed NE is indeed a saddle point. This corroborates our finding
in Theorem 4. Figure 3 shows the evolution of total reward J(θ(t)) for gradient play for different
random initial points θ(0). Different initial points converge to one of 13 different strict NEs each with
a different total reward (some strict NEs with relatively small region of attraction are omitted in the
figure). We see the total reward is monotonically increasing for each initial point, which makes sense
since gradient play runs projected gradient ascent on the total reward function J. While the total
rewards are different, as shown in Figure 4, we see that the NE-gap of each trajectory (corresponding
to same initial points in Figure 3) converges to 0. This suggests that the algorithm is indeed able to
converge to a NE. Notice that the NE-gaps do not decrease monotonically.
Figure 5: (Game 3:) State-based coordination game, rewards are nonzero
if both players locate at the same shaded grids
Figure 6: Total reward J (θ(t) )
keeps increasing
*a	%b	NC
Player 1
I 0.2501	0.0465	0.0145
0.0457	0.0363	0.0744
0.0143	0.0734	0.4450
ya Vb yc
Player 1
Figure 8: NE-gap converges
to a value close to zero. Here
the NE-gap is measured by
FigUre 7： MarginaldistribUtiOn dθ (si,x ,s2,χ)and dy (sι,y ,s2,y)	maxi maχ
(s,ai)Aiθ (s,ai)
Game 3: state-based coordination game OUr third nUmerical example stUdies the empirical
performance of the sample-based learning algorithm, Algorithm 1. Here we consider a generalization
of coordination game (Game 2) where the two players now try to coordinate on a 2D grid. The
two-player state-based coordination game on a 3 × 3 grid is defined as follows： the state space is
given by S = S1 × S2, S1 = S2 = Sx × Sy = {xa,xb,xc} × {ya,yb,yc}, action space is given by
A = A1 × A2, A1 = A2 = {Stay, Left, Right, Up, Down}, i.e., agent can choose to stay at cUrrent
grid or move left/right/Up/down to its neighboring grids. We assUme that there is random noise dUring
the transition; for example, if agent 1 is staying at the middle grid (xb, yb), then：
P (s1,x =	xa |s1,x	= xb, a1	= Stay) = P (s1,x	=	xc |s1,x	=	xb, a1 =	Stay) = ,
P (s1,x =	xb |s1,x	= xb, a1	= Stay) = 1 - 2,
P (s1,y =	ya |s1,y	= xb, a1	= Stay) = P (s1,y	=	yc |s1,y	=	xb, a1	=	Stay) = ,
P (s1,y =	yb |s1,y	= yb, a1	= Stay) = 1 - 2.
The reward is given by：
r(s1,s2) = 1{s1 = s2 = {xa,ya} or {xc, yc}},
i.e. the two agents are only rewarded if they stay at the Upper-left or lower-right corner at the same
time.
For nUmerical simUlation, we take TG = 300, TJ = 10000, α = 0.1, η = 10, = 0.1; the nUmerical
resUlts are as displayed in FigUre 6 - 8. FigUre 6 shows that total reward increases as the nUmber of
iterations increase, and FigUre 8 shows that the NE-gap converges to a valUe close to zero. However,
becaUse we project the policy to the α-greedy set Xα, the NE-gap cannot converge to exactly zero.
15
Under review as a conference paper at ICLR 2022
Figure 7 visualizes the discounted state visitation distribution. To make the visualization more
intuitive, we look at the marginalized discounted state visitation distribution dθx , dθy defined below:
∞
dx(si,x,S2,x)= £ dθ (si,x,Sl,y ,S2,x,S2,y) = Es0~ρ(l-γ) fγ t Pr(si,χ,t = Sl,x,S2,x,t = S2,x∣S0,∏θ)
s1,y,s2,y	t=0
∞
dy (S1,y ,s2,y )= £ dθ (S1,x,s1,y ,s2,x,s2,y ) = Es°~ρ(I-Y) EYt Pr(S1,y,t = s1,y ,s2,y,t = s2,y ls0,πθ )
s1,x,s2,x	t=0
From Figure 7 we can see that most of the probability measure concentrates on {(xa, xa), (xc, xc)},
{(ya, ya), (yc, yc)}, indicating that the two agents are able to coordinate most of the time.
B	More ab out Markov potential games
This section is dedicated to a more thorough understanding of Markov potential games, which
includes necessary or sufficient conditions for MPG and a few (counter)examples.
B.1.	A necessary condition and counterexamples
Definition 5. (Monderer & Shapley (1996)) Define the path in the parameter space as τ =
(θ(0) , θ(1) , . . . , θ(N)), where θt, θt+1 differ only one component it, i.e. θ(t+1) = (θi(t+1) , θ-(ti) ).
A closed path is a path such that θ(0) = θ(N). Define:
N
I(T) := X Jit (θ(t)) - Jit (θ(t-1))
t=1
The following theorem is a direct generalization of Theorem 2.8 in Monderer & Shapley (1996) to
MPG setting:
Lemma 5. For Markov potential games, I(τ) = 0 for any finite closed path τ.
Proof. The proof is quite straightforward from the definition of MPG
N
I(τ) =XJit(θ(t))-Jit(θ(t-1))
t=1
N
= X Φ(θ(t)) - Φ(θ(t-1))
t=1
= Φ(θ(N)) - Φ(θ(0))
= 0.
□
Although the proof of Lemma 5 is straightforward, it serves as a useful tool in proving that a game
is not a MPG. For example, applying the theorem we can get that the following conditions are not
sufficient conditions for MPG.
Proposition 2. None of the following conditions on a SG necessarily imply that it is a MPG:
(1)	There exists φ(s,	a)	such that at each	s,	ri(s, a0i, a-i)	- ri(s,	ai,	a-i)	=	φ(s, a0i, a-i)	-
φ(s, ai, a-i);
(2)	There exists φ(s, a) such thatfor every s, s, ri(s, ai, a-i) — ri(S, a%, a-i) = φ(s, ai, a—) 一
φ(S,ai,a-i);
(3)	Rewards ri are independent of s, and they have a potential function, i.e., ri (ai, a-i) -
ri (a0i, a-i) = φ(ai, a-i) - φ(a0i, a-i).
16
Under review as a conference paper at ICLR 2022
Proof. (of Proposition 2) A simple counterexample showing that the conditions in Equation (2) are
not sufficient is the multi-stage prisoner’s dilemma (Game 1) introduced in the numerical section
(Appendix A). Since the reward table for multi-stage prisoner’s dilemma is the same as the one-shot
prisoner’s dilemma (which is known to be a potential game), Game 1 satisfies condition (3) in
Proposition 2, which implies condition (2), which in turn implies condition (1). In the following we
are going to use Lemma 5 to show that Game 1 is not a MPG. We define the following individual
policies:
Let:
θ(0)
θ Defect :	Defect θs=1,ai =1	0,	Defect θs=2,ai =1 =	0
θCoop :	Coop θs=1,ai =1	1,	Coop θs=2,ai =1 =	0
θAlways∕oop ：	QAlWayS_coop _ θs=1,ai =1	1,	QAlWayS_coop θs=2,ai =1	=1
(θDefect
θ(3)
(θDefect θAlways-cooρ
θ(2) = (θCoop θAlways-coop)
and define a path τ by:
τ=(θ(0),θ(1),θ(2),θ(3),θ(4)), θ(4) = θ(0).
For the sake of easy calculation, we set = 0 and set initial state as s0 = 1 in Game 1. In this
example, it is not hard to see that Jit (θ(t)) - Jit (θ(t-1)) > 0 for each t ∈ {1, 2, 3, 4}, implying that
I(τ) > 0. This indicates that although Game 1 satisfies condition (3) (as well as conditions (1) and
(2)), it is still notaMPG.4	口
B.2.	A sufficient condition
Proposition 2 suggests that MPG is a quite restrictive assumption. Even if the reward table for a
SG is the same as the reward table of a one-shot potential game, the SG may still not be a MPG.
Nevertheless, we can show that the following condition is sufficient for a stochastic game to be a
MPG:4 5
Lemma 6. A stochastic game is a MPG if condition (1) in Proposition 2 is satisfied and that
P (s0 |s, a) = P(s0|s).
Proof. P (s0|s, a) = P(s0|s) implies that the discounted state visitation distribution dθ does not
depend on θ, and thus we denote it as d(s) instead. Condition (1) implies that φ(s, ai, a-i) -
ri(s, ai, a-i) only depends on s and a-i but not ai, and so we denote the difference as δi(s, a-i),
i.e.,
φ(s, ai, a-i) - ri(s, ai, a-i) = δi(s, a-i).
The total reward of agent i can be written as:
Ji(θ) = Ed(s') E∏θ(a∣s)ri(s,a)
=Ed(S) ^%⑷1S)Eπθ-i (a-i|s)r(s, ai, a-i)
s	ai	a-i
Similarly, total potential function can be written as:
Φ(θ) = Ed(S) E∏θ(a∣s)φ(s,a)
sa
=Ed(S) £% (ai|s) Eπθ-a (a-iis)φ(s,ai,a-i)
4We can use this counterexample to show that some propositions (e.g., Proposition 3) in the arXiv preprint
(Mguni, 2020) would need more consideration since the multi-stage prisoner’s dilemma satisfies the conditions
given in (Mguni, 2020).
5The recent arXiv preprint (Leonardos et al., 2021) has a similar characterization.
17
Under review as a conference paper at ICLR 2022
Thus,
Φ(θ) - Ji(θ) = Ed(S) E∏θi(ai|s) E∏θ-i(a-i∣s)(φ(s,ai,a-i) - r(s,ai,a-i))
=Ed(S)	(ai|S)	(a-i|S)&(S,aT)
=Ed(S) E∏θ-i(a-i∣S)δi(S, a—i),
which does not depend on parameter θi, i.e.,
Φ(θi0,θ-i) -Ji(θi0,θ-i) = Φ(θi, θ-i) -Ji(θi,θ-i), ∀(θi0,θ-i),(θi,θ-i) ∈ X,
which completes the proof.	□
B.3.	MPG with local states and an application example
From Proposition 2, we see that it is difficult for a SG to be a MPG even if the game is a potential
game at each state. Lemma 6 only presents a very special case where the action does not affect the
state, meaning that this MPG is merely a collection of potential games. To provide a MPG which is
beyond the identical-interest case and the case in Lemma 6, inspired by the setting in Qu et al. (2019)
and MaCUa et al. (2018), here we consider a special multi-agent setting where S = Si ×∙∙∙×Sn
and Si is the local state space of agent i. In addition, the transition probability takes the decomposed
form, P(S0|S, a) = Qin=1 P(S0i|Si, ai). The rest of the SG setting is the same as the SG in Section 2.
Deviating slightly from the main text, we consider the localized policy where each agent take actions
based on its own state,
n
∏θ (at ∣St) = ∏∏θi (ai,t∣Si,t)
i=1
with the localized direct parameterization:
πθi (ai,t|Si,t) = θ(si,ai),	θi ∈ δ(Ai)ISiI
We use Xilocal := ∆(Ai)lSil to denote the feasibility region of θ%, and the feasibility region of θ is
denoted as Xlocal := Xlocal ×∙∙∙× Xiocal.
in
Lemma 7. If there is a function φ(S, a) such that for every agent i, ri(Si, S-i, ai, a-i) =
φ(Si, S-i, ai, a-i) + ψi (S-i, a-i) where ψi only depends on S-i, a-i, then this SG is a MPG,
i.e., for any parameters (θi0, θ-i), (θi, θ-i) ∈ Xlocal, the equation in Definition 3 is satisfied.
The proof is straightforward given the local structure of the MDP and the localized policies. This
MPG enjoys nontrivial multi-agent application examples such as medium access control (Macua
et al., 2018), dynamic congestion control (Bertrand et al., 2020), etc. Below we provide medium
access control as one of the examples.
Real application - medium access control. We consider the discretized version of the dynamic
medium access control game introduced in (Macua et al., 2018), where each agent is a user that
tries to transmit data via a single transmission medium by injecting power to the wireless network.
Each user’s goal is to maximize its data rate and battery lifespan. If multiple users transmit at the
same time, they will interfere with each other and decrease their data rate. Here user i’s state is
Si ∈ Si = {0, 1, . . . , Bi,max}, which denotes its own battery level, where Bi,max is its initial battery
level. We use δi to denote its discharging factor. Its action ai ∈ Ai = {0, 1, . . . , Pi,max} denotes the
power injected to the network at each time step, where Pi,max is the maximum allowed power. The
state transition is deterministic, describing the discharging process of the battery proportional to the
transmission power, which is given by:
Si,t+1 = Si,t - δiai,t.
The stage reward of user i is given by:
ri(S, a) = log (l + ι	PhiI ai ∣2 ) + αSi,
1 +	j6=i |hj|2aj
18
Under review as a conference paper at ICLR 2022
where hi is the random fading channel coefficient for user i.
By noticing that ri(s, a) = log (1 + Pn=1 |hi|2ai) + α Pi si - log (1 + Pj=i|hj|2aj)-
α Pj 6=i sj , we can apply Lemma 7 to verify that the medium access control problem is indeed
a MPG and that the potential function φ is given as:
nn
1 +	|hi|2ai +α	si.
i=1	i=1
C DERIVATION OF EQUATION (18) (CALCULATION OF dθ)
From the definition of dθ :
∞
dθ(s) = Esο〜ρ(1 - Y) X γtPrθ(st = S∣sο),
t=0
we have that:
dθ = (1 - Y) (P + YPS > ρ + CYPS P +—)
2τ^2
=(1 - γ)(I + YPS + γ2PS …)>P
=(1 - Y)(I-YPθ)->P
D Proof of Lemma 2
Proof. According to policy gradient theorem Equation (6):
∂∂≡ = τ-- XXdθ(s0)πθ(a0ls0)
∂ s,ai 1 - Y s0 a0
∂ log ∏θ (a0∣s0)
∂θs,ai
Qiθ(s,a)
Since for direct parameterization:
∂log∏θ(a0∣s0) _ ∂log∏θi(ai∣s0)
∂θsa
s,ai
∂θsa
s,ai
1{a0i = ai , s0
1{a0i = ai , s0
s}:
s,ai
s}
πθi (ai |s)
Thus we have that:
∂Ji (θ)	1
-------=--------
∂θsa	1 - Y
s,ai
_	1
1 - Y
ΣΣdθ (s0)πθ (a0 |s0)1{a0i = ai, s0 = s}
s0 a0
Xdθ(S)顶(ails)πθ-i(a-i|s) Jl、
—	∏Θ (ails)
a
-i
Ji、Qθ (s,a)
∏θ (ails)
Qiθ (s, ai, a0-i)
dθ(S) Xπθ-i(a-ils)Qθ(s, ai, a0-i)
1 - Y	a0-i
1	/ ∖TΓ7T∕ 、
d----dθ (s)Qi Gai)
1-Y
□
E	Proofs of Lemma 3 and Theorem 1
Before giving proofs for Lemma 3 and Theorem 1, we first introduce the well-known the performance
difference lemma (Kakade & Langford, 2002) in RL literature, which is helpful throughout. A proof
is also provided for completeness.
19
Under review as a conference paper at ICLR 2022
Lemma 8. (Performance difference lemma) For policies πθ , πθ0,
Ji(θ0) - Ji(θ) = 1 - YEs~dg0Ea~∏θ0 Aθ(s, a),	i = 1, 2,...,n	(22)
Proof. (of performance difference lemma) From Bellman’s equation we have that:
Jie)- Ji(θ) = Eso~ρVf (so) - Eso~ρViθ(so)
=(Es~ρEa~∏θo(∙∣s)Qθ(s,a) - Es~ρ%θ(S)) +(ES~pViθ0(S)- Es^ρEa^∏θo(∙∣s)Qθ(s,
=Es~ρEa~∏θ,(∙∣s)Aθ (s,a)+ γEso~ρ,s~Prθ0 (S1=∙∣S0) h%θ0(s) - Viθ (s)i
=Es-PEa-∏θ0 (∙∣s)Aθ (S, a) + YEso~ρ,s~Prθ0 ⑶=∙∣s0 )Ea~∏θ, (∙∣s) Aθ (S, a)
+ γ2Es0~ρ,s~Prθ0 (s2 = ∙∣so) hViθ (S)- Viθ(S)i
=♦♦♦
∞
=X Es0~ρ,s~Prθ0 (st = ∙∣s0)Ea~πθ0YtAθ (S，a)
t=o
=ι - γEs~dgoEa~∏θ0 Aθ(s, a)
□
Proof. (Lemma 3) According to performance difference lemma Equation (22):
Ji(θ0 ,θ-i) - Ji(θi,θ-i) = -1-
i	1-Y
__	1
1 - Y
_	1
1 - Y
£d60 (s)∏θo (a∣S)Aθ (S,a)
s,a
£d60(s)∏θ0(ai|s) E∏θ-i(a-i∣S)Aθ(s, ai, a—)
(23)
s,ai
a-i
£d60(s)∏θ0 (ai∣s)Aθ(s, ai).
s,ai
According to the definition of ‘averaged’ advantage function:
which implies:
E∏θi (ai∣S)Aθ (s, ai) =0,	∀s ∈ S
ai
max Aθ(S,ai) ≥ 0,
ai∈Ai i
thus we have that:
Ji(θ0, θ-i) - Ji(θi, θ-i) = 1 -	X dθ0(s)∏θ0 (ai∣S)Aθ(s, ai)
Y s,ai
≤ γ-^-- X dθ0(s) max Aθ(S,ai)
1 - Y s	ai∈Ai
=X dθ0(4 dθ (s) max Aθ Ga#
1 - Y s dθ (S)	ai∈Ai
(24)
dθ0
dθ
dθ (S) max Aiθ (S, ai).
∞ s	ai∈Ai
1
≤-----
—1 - Y
20
Under review as a conference paper at ICLR 2022
We can rewrite 击 Ps dθ(S) maXai∈Ai Aθ(s, ai) as:
1 1 X dθ(s) max Aθ(s, a，i) =	- max X dθ(s)∏θ∙ (a∕s)Aθ(s, ai)
1 - Y V	ai∈Ai	1 - Y θi∈Xi M	i
=max X(∏θ∙(ai|s) - ∏θi(a/s))[1 dθ(s)Aθ(s, ai)
θi∈Xi M	i	1 - γ	(”、
s,ai	(25)
=max y^(∏θ.(ai|s) - ∏θi(a∕s));------dθ(s)Qθ(s, a，i)
θi∈Xi M	i	1 - Y
=max(θi - θi)>Vθi Ji(θ).
θieXi
Substituting this into Equation (24), we may conclude that
Ji(θ'0,θ-i) - Ji(θi,θ-i) ≤ 半	max(θi - θi)>Vθi Ji(θ)
dθ ∞ θi∈Xi
and this completes the proof.	口
Proof. (Theorem 1) The definition of a Nash equilibrium naturally implies first-order stationarity,
because for any θi ∈ Xi :
Ji((1 - η 闻 + ηθi,θ-i) - Ji(θi ,θ-i) = η(θi - θ)>Vθi Ji(θ*) +。⑺属一θ"∣) ≤ 0,	∀ η> 0
Letting η → 0 gives the first-order stationary condition:
(θi - θ)>V%Ji(θ*) ≤ 0, ∀θi ∈Xi,
It remains to be shown that all first-order stationary policies are Nash equilibria. From Assumption 1
we know that for any pair of parameters θ0, θ*,
dθ0
dθ*
< +∞.
∞
Take θ0 = (θi, θ-j θ* = (θi,θ-i). According to Lemma 3, we have that for any first-order
stationary policy θ*,
Ji(θi ,θ-i) - Ji(θi,θ-i) ≤
dθ0
dθ*
max (θi - θ*)>Vθi Ji(θ*) ≤ 0,
∞ θi∈Xi
which completes the proof.
□
F Proof of Theorem 2 and Lemma 4
Proof. (Lemma 4) For a given strict NE θ* randomly set:
ai* (s) ∈ arg max Aiθ* (s, ai),
ai
and set θi be:
θs,ai = 1{ai = ai* (s)}.
And set θ := (θi, θ-* i) From performance difference lemma Equation (22):
Ji(θi,θ-i)- Ji(θ*,θ-i) = X dθ(S)πθi(S,ai)Aθ*(S, ai)
s,ai
=dθ(s) max A?*(s,ai) ≥ 0
ai i
s
Because θ * is a strict NE, thus the inequality above forces θi* = θ, and that maxaiAθ* (s,ai) = 0.
The uniqueness of θ* also implies uniqueness of ai*(S), and thus,
Aθ* (s, ai) < 0, ∀ ai = a*(s),
which completes the proof of the lemma.
□
21
Under review as a conference paper at ICLR 2022
The proof of Theorem 2 relies on the following auxiliary lemma, whose proof we defer to Appendix
M.
Lemma 9. Let X denote the probability simplex of dimension n. Suppose θ ∈ X, g ∈ Rn and that
there exists i* ∈ {1, 2,...,n} and ∆ > 0 such that:
θi* ≥ θi,	∀i =泊
gi* ≥ gi + ∆,	∀i = i*.
Let
θ0 = P rojX (θ + g),
then:
θ0* ≥ min{ 1,θi* + "2}
Proof. (Theorem 2) For a fixed agent i and state s, the gradient play (Equation (5)) update rule of
policy θi,s is given by:
θ(t+1) = Proj∆(∣Ai∣)(θ(tS + T-^dθ(t)(s)Qfy(s, ∙)),	(26)
where ∆(∣Ai|) denotes the probability simplex in ∣A4∣-th dimension and Qθ(t (s, ∙)) is a ∣Ai∣-th
dimensional vector with ai-th element equals to Qiθ(t) (s, ai)). We will show that this update rule
satisfies the conditions in Lemma 9, which will then allow us to prove that
D(θ(t+1)∣∣θ*) ≤ max{0,D(θ㈤∣∣θ*) - η∆θ-}.
Letting ai* (s) be the same definition as Equation (9), we have that:
ɪdθ(t) (s)QΓ (s,a* (S))- ɪdθ(t) (s)QΓ (s,&)
1-γ	1-γ
1	...... 1
≥I— dθ*(s)Qθ* (s,a* (S))- I— dθ*(s)Qθ* (s,ai)
1-γ	1-γ
1		 1	^TTr.......
-「dθ*(s)Qθ* (s,a*(S))-「dθ(t) (s)Qθ(t) (s,a*(S))
—
1	. .-TTT	1	. .	777 V .
1-γdθ* (s)Qθ (1 * * * s, ai) - γ-γdθ(t) (S)QF) (s, ai)
≥十dθ*(S) (Ar(S,a*(S)- AF(S,ai))) - 2∣∣V为 Ji(θ(t)) - V% Ji(θ*)k
"-(K4γ)3 (XX 14|)*-叫
≥∆θ* - ɪ (XX 4)XX X kθ(ts-θ*,s)kι
≥∆θ* - (T-^n|S| (XX |Ai|) D(θ⑴l∣θ*),
(27)
(28)
where Equation (27) to Equation (28) uses smoothness property in Lemma 18.
We use proof of induction as supposed for ` ≤ t - 1, we have:
D(θ('+1)∣∣θ*) ≤ max{D(θ⑶∣∣θ*) - η∆θ-,0},
thus
D(θ⑴∣∣θ*) ≤ D(θ⑼∣∣θ*)
<	∆θ* (1-γ)3
≤ 8n∣S∣(P乙 IAiI).
22
Under review as a conference paper at ICLR 2022
Then we can further conclude that:
(I- γ)dθ(t) (s)Qθ()(s, ai (S))-(I- Y)dθ(t) (S)Qθ()(s, ai)
≥∆θ* - (r4γ)3n∣S∣ (XX Al) D(θ(t)∣∣θ*)
≥ -2-,	∀ ai = ai(s)
Additionally, for D(θ(t)∣∣θ*) ≤ 所燃；PI-Y)； D, we may conclude:
θ(,a乂S) ≥ 1/2 ≥ θ(,ai ∀ai=a*(s),
then by applying Lemma 9 to Equation (26) we have:
.，、eʌθ*
*1(S) ≥ mm{1Y* (S) + ητ~}
=⇒ kθ(t+1)-%kι = 2 (1-θ(,+1(S))
≤ max{0, kθ(,tS - θi,s kι - η^2-},	∀ S ∈ S, i = 1, 2,...,n
=⇒ D(θ(t+1)∣∣θ*) ≤ max{0,D(θ⑴∣∣θ*) - η∆θ.},
which completes the proof.
□
G Proof of Proposition 1
Proof. First of all, from the definition of NE, the global maximum of the potential function is a NE.
We now show that this global maximum is a deterministic policy. From classical results (e.g. (Sutton
& Barto, 2018)) we know that there is an optimal deterministic centralized policy
∏*(a = (a1,..., an)∣s) = 1{a = a*(s) = (aɪ(s), ..., an(s))}
that maximizes:
∞
π* = arg max E VJ γtφ(st,at) ∣π,s° = S .
π :S →∆(A)	t=0
We now show that this centralized policy can also be represented by direct distributed policy parame-
terization. Set θ* as:
∏θ* (ai|s) = 1{ai = W(S)},
then:
n
n*(a|S)= γ πθ* (ai|S)
i=1
Since ∏ globally maximizes the discounted summation of potential function φ among centralized
policies, which includes all possible direct distributedly parameterized policies, θ* also maximizes
the total potential function Φ globally among all direct distributed parameterization, which completes
the proof.	□
H Proof of Theorem 3
H. 1 Useful Optimization Lemmas
Lemma 10. Let Φ(θ) be β-smooth in θ, define gradient mapping:
Gn(θ) := 1 (Projχ(θ + ηVΦ(θ)) - θ).
η
23
Under review as a conference paper at ICLR 2022
The update rulefor projected gradient is:
θ+ = θ + ηGrη (θ) = Projχ (θ + ηkΦ(θ)).
Then:
(θ0 - θ+)τVΦ(θ+) ≤ (1+ ηβ)kGr(θ)k∣∣θ, - θ+k ∀θ0 ∈ X.
Proof. By a standard property of Euclidean projections onto a convex set, we get that
(θ + ηVΦ(θ) - θ+)τ(θ0 - θ+) ≤ 0
=⇒ ηVΦ(θ)τ(θ0 - θ+) + (θ - θ+)τ(θ0 - θ+) ≤ 0
=⇒ ηVΦ(θ)τ(θ0 - θ+) - ηGr(θ)τ(θ0 - θ+) ≤ 0
=⇒ VΦ(θ)τ(θ0 - θ+) ≤ ∣∣Gr(θ)∣∣θ0 - θ+∣∣
=⇒ VΦ(θ+)τ(θ0 - θ+) ≤ IlGr(θ)∣∣θ, - θ+k + (VΦ(θ+) - VΦ(θ))τ(θ0 - θ+)k
≤ IGr(θ)∣∣θ'-θ+k + βkθ+ -θ∣M-θ+k
= (1 + ηβ)∣Gr (θ)∣∣θ'-θ+k
□
Lemma 11. (Sufficient ascent) Suppose Φ(θ) is β-smooth. Let θ+ = ProjX(θ + ηVΦ(θ)). Then
for η ≤ 1,
Φ(θ+)- Φ(θ) ≥ 2IIGr(θ)∣2
Proof. From the smoothness property we have that:
Φ(θ+) - Φ(θ) ≥ VθΦ(θ)τ(θ+ - θ) - β∣∣θ+ - θ∣2	(29)
Since θ+ = ProjX(θ + ηVΦ(θ)), we have that:
(θ + ηVΦ(θ) - θ+)τ(θ0 - θ+) ≤ 0, ∀ θ0 ∈ X
take θ0 = θ, we get:
VΦ(θ)τ(θ+ - θ) ≥ 1∣θ+ - θ∣2.
η
Thus:
Φ(θ+) - Φ(θ) ≥ VθΦ(θ)τ(θ+ - θ) - β∣∣θ+ - θ∣2
≥ (1 - β)∣θ+ -θk2
η 2
≥ ɪ∣θ+-θ∣2
2η
=2 ∣Gr (θ)∣2,
which completes the proof.	□
Lemma 12. (Corollary ofLemma 11) For Φ(θ) that is β smooth and bounded Φmin ≤ Φ(θ) ≤ Φmax,
running projected gradient ascent:
θ(t+1) = ProjX (θ + ηVΦ(θ⑴))
with η = 1, will guarantee that:
lim KGr(θ⑴)∣ =0.
t→十∞
Further, we have that:
T-1
t=0
≤
2β(φmax - φmin)
T
(30)
24
Under review as a conference paper at ICLR 2022
Proof. From Lemma 11 we have:
Φ(θ(t+1)) - Φ(θ⑴)≥ ɪIlGη(θ㈤)k2 ≥ 0
2β
Thus Φ(θ(t)) is non decreasing, and since it is bounded, we know that Φ(θ(t)) asymptotically
convergence to some value Φ*, and thus show that
lim IGη(θ(t))I = 0.
t→∞
Additionally, from Equation (30),
φ(θ(T))- Φ(θ⑼)≥ ∑ 21βkGn(θ(t))k2
T-1
T X kGn(θ(t))k2 ≤ 2β(φmaτ- φmin),
T t=0	T
which completes the proof.
□
H.2 Proof of Theorem 3
Proof. Recall the definition of gradient mapping:
Gn(θ) = 1 (ProjX(θ + ηVΦ(θ)) — θ).
η
From gradient domination property Equation (8) we have that:
NE-gapi(θ(t+1)) = m0 axJi(θi0.θ-(ti+1))-Ji(θi(t+1),θ-(ti+1))
θ0i ∈Xi
≤ max
θ0i∈Xi
d(θi0.θ-(ti+1) )
d(θi(t+1) ,θ-(ti+1) )
m∈aXi (θi- θ")>	Ji(θ(t+1))
∞
≤ M max (θi - θ(t+1))> VθiΦ(θ(t+1))
θi∈xi '	)	'
≤ M(1+ ηβ)max ∣θi - θ(t+1)kkGn(θ⑴)k
瓦∈Xi
≤ M(1+ ηβ)2p⅜Gn(θ(t))k
=4M PMGn (θ(t))∣
where the last step follows as ∣∣θi — θ(t+1) ∣∣ ≤ 2P∣S∣. Thus
NE-gap(θ(t+1)) ≤ 4MP∖S∖∣∖Gn(θ㈤)∣
Then from Lemma 12 we have that:
lim IGη(θ(t))I = 0
t→∞
lim NE-gap(θ(t)) = 0,
t→∞
and that:
t=1
we can get our required bound of if we set:
32βM 2∣S∣(Φmaχ
T
- Φmin) ≤ 2,
25
Under review as a conference paper at ICLR 2022
or equivalently
T ≥ 32M2e®max - φmE)ISI
一	€2
=64M2(Φmaχ- Φmin)|S| £, |Ai|
=	€2 (1 - γ)3	,
which completes the proof.	□
I	Proof of Theorem 4
Proof. (of the first claim) The proof requires knowledge of Lemma 4 in Section 3 thus we would
recommend readers to first go through Lemma 4 first. The lemma immediately leads to the conclusion
that a strict NE θ* should be deterministic. Let a*(s), ∆θ*(s), ∆θ* be the same definition as
Equation (9) Equation (10) respectively.
For any θ ∈ X , Taylor expansion suggests that:
Φ(θ) - Φ(θ*) = (θ - θ*)>VΦ(θ*) + o(kθ - θ*k)
=X(Oi-OhNθi Ji(θ*)+ o(kθ -θ*k)
i
=ɪ-- XXX dθ*(SH (s,ai)(θs,ai - θS,ai ) + θ(kθ - θ*k)
γ i s ai
≤-占 XXdθ*(s)∆θ* (s) I X (θs,ai,aj + θ(kθ-θ*k)
Y i S	∖ai=a*(s)	)
=-占 XXdθ*(s)∆θ*(s)1 kθi,s-θ卷kι + o(kθ -θ*k)
γi s
∆θ*
≤----2 XX kθi,s - θi,sk1 + o(kθ - θ*k)
∆θ*
≤- F kθ-θ*k+ o(kθ-θ*k)∙
Thus for ∣∣θ 一 θ*k sufficiently small,
Φ(θ) - Φ(θ*) < 0 holds,
this suggests that strict NEs are strict local maxima. We now show that this also holds vice versa.
Strict local maxima satisfy first-order stationarity by definition, and thus by Theorem 1 they are also
NEs, we only need to show that they are strict. We prove by contradiction, suppose that there exists a
local maximum θ* such that it is non-strict NE, i.e., there exists θ0 ∈ X,, θ0 = θi such that:
Ji(θ, ,θ-i) = Ji(θi,θ-i)
According to Equation (25) and first-order stationarity of θ*:
T^ X dθ* (s) max A?* (s, a,) = ^ax (θ, — θ*)>Vθ-J,(θ*) ≤ 0.
1 - Y L	ai∈Ai	?处工
Since maxai∈Ai Aiθ(s, ai) ≥ 0 for all O, we may conclude:
maxAT(s,ai)=0, ∀ s ∈S.
ai∈Ai i
We denote O0 := (Oi0, O-i* ), according to Equation (23)
0 = Ji(θ0 ,θ-i) - Ji(θt,θ-i) =	X dθ0 (s)πθ0 (ai|s)A?* (s,ai) ≤ 0.
1-Y	i
s,ai
26
Under review as a conference paper at ICLR 2022
Since dθ0 (s) > 0, ∀ s, this further implies that
X ∏θ0 (a∕s)Aθ* (s,ai)=0, ∀ S ∈ S,
ai
i.e., ∏θ0 (a∕s) is nonzero only if A?* (s, a# = 0. Define θ? := ηθ0 + (1 - η)θ*, then
X ∏θη (ai∣s)Aθ* (s,ai)=0, ∀ S ∈ S.
ai
Thus let θη := (θi,θ-i)
Ji⑹,θ-i) - Ji(θi,θ-i) = ； X dθη (s)∏θη(ai∣s)Af (s,ai)=0.
1-γ	i
s,ai
Since kθ? - θ"∣ → 0 as η → 0, this contradicts the assumption that θ* is a strict local maximum.
This suggests that all strict local maxima are strict NEs, which completes the proof.	口
Proof. (of the second claim) First, we define the corresponding value function, Q-function and
advantage function for potential function φ.
∞
Vφθ(S) := E Xγtφ(St, at) π = θ, S0 = S
t=0
∞
Qθφ(S, a) :=	γtφ(St, at) π = θ, S0 = S, a0 = a
t=0
Aθφ(S,a) := Qθφ(S,a) - Vφθ(S).
For an index setI ⊆ {1, 2, . . . , n} we define the following averaged advantage potential function of
index set I as:	____
Aφ,I(S,aI)= EAφ(S,aι ,a-I).
a-I
We choose an index set I ⊆ {1,2,...,n} such that there exists s*,aI such that:
Aφ*ι(s*,aI) > O,	(31)
and that for any other index set I0 with smaller cardinality:
Aφ*Io(s, aI0) ≤ 0, ∀ s, aI0, ∀ |I0∣ < |I|.	(32)
Because Φ is not a constant, this guarantees the existence of such an index set I. Further, since
Xπθ*0(aI，|s)A*：I，(s,aI0) =0, ∀s,
aI0
and that θ* is fully-mixed, We have that:
Aφ*∣0(s,aI0) = 0, ∀ s,aI0, ∀∣I0I < |I|.	(33)
We set θ := (θI, θ-I), where θI is a convex combination of θI, θI ∈ X:
θI = (1 - η)θI + ηθI, η > 0.
According to performance difference lemma Equation (22) we have:
(I-Y) (φ(θI ,θ-I ) - φ(θI ,θ-I )) = X dθ (S)πθι(aIIS)Aφ,I (S,aI)
s,aI
=E d?(S) ∏ ((I- η)πθ* (ai|S) + ηπθ0 (ai|S)) Aφ,I (S,aI)
s,aI	i∈I
=X d?(S)((I-η)πθ*0	(aio |S)	+ ηπθ00	(aio IS))	Y	((1-η)πθ* (电⑸十	ηπθ0	(aiIS))	Aφ*I(S,aI),	(∀ io	∈	I)
s,aι	i∈I∖{io}
27
Under review as a conference paper at ICLR 2022
= (I-η)Ede(S) ∏ (Q-η)πθi(ai|S)+ ηπθ0(ai|S))Aφ,ι∖{i0}(s,aι∖{io})
SqI	i∈ι∖{io}
+ nɪ^dθ(s)πθ00(aioIS) ∏	((1 -η)πθ*(ai|S) + ηπθ0(ailS)) Aφ,ι(s,aι).
s,aI	i∈I \{i0 }
According to Equation (33), we know that:
Aφ,I∖iio} (S，aI∖{i0}) = 0，
thus
(1 - Y)(Φ(θι ,θ-ɪ) — Φ(θI ,θ-ɪ))=
Infde(S)πθ0JaioIS)	∏	((1 — η)πθ*(ai|S) + ηπθ0(电国)Aφ,ι(S,az)∙
s,aI	i∈I∖{i0}
Applying similar procedures recursively and using the fact that:
A：1\{讣(5，。1\{讣)=0, ∀ i ∈1，
we get:
n∣ι∣ L	L	_____
Φ(θι,θ-ɪ) — Φ(θI,θ-ɪ) = 1n- Σdθ(s) ∏ Πθi (ai∣S)Aφ,ι(s, aι)∙
γ s,aI	i∈I
Set πe0 (aiIS) as:
πθi(。小*)= { 0 oaherWise
∏θ0 (ai∣S) = ∏θ* (ai∣S), S = s*,
where s*, a* are defined in Equation (31). Then:
n∣ι∣	___
Φ(θι,θ-ɪ) — Φ(θI,θ-ɪ) = 1-γdθ(s*)Aφ,ι(s*, aI) > 0,
which completes the proof.	□
J B ounding the gradient estimation error of Algorithm 1
The accuracy of gradient estimation is essential in the sample-based algorithm 1. In this section, we
will give a high probability bound of the estimation error, which is stated in the following theorem:
Theorem 6.	(Error bound for gradient estimation) Assume that the stochastic game that satisfies
Assumption 3. In Algorithm 1, for
TJ ≥
32τ(1 + α)2∣S∣3 PiIAiI max，∣Ai∣2 l	( 16tTg∣S∣2 £/4|、
(i-γ)6eg aσ	log[	δ	+ +1
with probability at least 1 — δ, we have:
kvΦ(θ(k)) -VΦ(θ(k))k2 ≤ eg, ∀ 0 ≤ k ≤ TG — 1.
The proof of the theorem includes bounding the estimation error of Qie (J.1) and de (J.2). Let’s first
introduce the definition of ‘sufficient exploration’ which is going to play an important role in this
section.
In the main text Assumption 3 we have introduced (τ, σS)-sufficient exploration on states. In this
section we introduce a similar definition (τ, σ)-sufficient exploration:
Definition 6. ((τ, σ)-Sufficient Exploration) A stochastic game and a policy θ is said to satisfy
(τ, σ)-sufficient exploration condition if there exists positive integer τ and σ∈ (0, 1) such that for
policy θ and any initial state-action pair (S, ai), ∀i, we have
Pr(Sτ , ai,τ IS0 = S, a0 = a) ≥ σ, ∀Sτ , ai,τ
Note that '(t, σ)-sufficient exploration' is a stronger condition compared with '(t, σs)-sufficient
exploration on states’. Additionally it is not hard to verify that for any stochastic game that satisfies
(τ, σs)-sufficient exploration on states, and any θ ∈ Xα, it will also satisfy (τ, maXσS4.∣)-sufficient
exploration condition.
28
Under review as a conference paper at ICLR 2022
J.1 Bounding THE estimation error of THE AVERAGED-Q function
We first state the main theorem in this subsection:
Theorem 7.	(Estimation error ofaveraged Q-functions) Assume that the stochastic game with policy
θ satisfies (σ, T) -sufficient exploration condition (Definition 6), thenfor a fixed i, running Algorithm
1 will guarantee that:
Pr (∣∣Qθ - Qθ∣∣∞ ≥ e) ≤ 4τ∣S∣2∣Ai∣ exp (-
(1-Y)4e2σ2b T C
32∣S∣2
further:
Pr (Ilc - QTk∞ ≥ E, ∃ i) ≤ 8τ∣S∣2 (X ∣Ai∣卜XP (-
(1-Y)4e2σ2b T C
32∣S∣2
i.e., when
G	32τ ∣S∣2	1
TJ ≥ ∩—log
(1 — Y )4E2σ2
----------r -----
with probability at least 1 — δ, ∣∣Qθ — Qθ∣∣∞ ≤ e, V i
In the following, we will introduce some lemmas which will play an important role in bounding the
estimation error of the averaged-Q function:
Lemma 13. Assume that the stochastic game with policy θ satisfies (σ, T)-sufficient exploration
condition (Definition 6), then fix s0, s, ai ,for E ≤ 1,
Pr (P/(s0∣s,αi) - Pθ(s0∣s,电)] ≥ E) ≤ 4τexp 卜
F T C
32
Proof. According to the definition of Pi, we have that
∣s,αi) - Pi(s0∣s,αi) ≥ e}
∪
l{st = s,ai,t = ai} = 0}
st+1 = s , st = s, ai,t = ai} - (Pi (s ∣s, ai) + E)I{st = s,
8 ∣s∣2l Pi∣4∣
δ
+ T
{[ T-1-m j	'
〉:	(1{ skτ+m+1 = S , skτ+m	= Sl ai,kτ+m	=	ai} - (Pi (S	∣s, ai)	+ E)I{skτ+m	= Sl	ai,kτ+m =	ai}) ≥ 0
k=0
TUI(
m=0
T-⅛-m J
l{skτ+m = s, ai,kτ+m = ai} = 0
k=0
Let:
bT — 1 — ?m j
A
ʃɪm
E ( l{skτ+m+1 = S , skτ+m = s, ai,kτ+m = ai} - (Pi (s ∣s, ai) + E) l{skτ+m = S,ai,kτ+m =电}) ≥ 0
k=0
T-IFm j	)
Cm .	、
l{skτ+m = s, ai,kτ+m = ai} = 0
k=0
Xk,m := l{skτ+m+1 = s , skτ+m = s, ai,kτ+m = ai} - (Pi(s ∣s, ai) + E) l{skτ+m = s, ai,kτ+m = ai}
^Xk,m :	l{skτ+m = s, ai,kτ+m = ai}
29
Under review as a conference paper at ICLR 2022
Y3= Xk,m - E[Xk,m∣F(k-1)τ+m]
Ykm := Xk,m - E[Xk,m∣F(k-1)τ+m]
I T — 1 — rm∙ I
Then {Yk,m}k=0T	」is a martingale difference sequence. Because E ≤ 1, it is easy to verify that
∣Xk,m∣ ≤ 2, ∣Xk,m∣ ≤ 1. We have that:
∣K,m∣ ≤ ∣Xk,m ∣ + E[∣Xk,m∣∣F(k-1)τ+m] ≤ 4, ∣Yk,m∣ ≤ ∣Xk,m∣ + E[∣Xk,m ∣∣F(k-1)τ+m] ≤ 2.
Further,
E[Xk,m∣F(k-1)τ+m] = E[l{skτ+m = s,ai,kτ+m = αi}∣F(k-1)τ+m] ≥ σ,
and that
E[Xk,m∣F(k-1)τ+m]
=E[l{skτ+m+1 = S , skτ+m = s, ai,kτ+m = ai} - (Pi (S ∣s, ai) + e)1{skτ+m = s, ai,kτ+m = ai}∣F(k-1)τ十m]
(34)
=-eE[1{skτ+m = s,ai,kT+m = °i}∣F(k-i)τ+m] ≤ -eσ.	(35)
To move from equation (34) to equation (35), we used the fact that:
E[1{st+1 = S, st = s,ai,t+m = αi}∣氏-1] = P(SISt-1, at-1) £"6(电，。—∣s)P(s0∣s,电,α-i)
a — i
=P(SISt-ι,αt-i)∏θi(αi∣s) £"—i(α-i∣s)P(s0∣s,αi,α-i)
a — i
=P(SISt-1, αt-i)∏θi(αi∣S)Piθ(s0∣s,电)
=E[p"(s ∣s, ai)1{st = s, ai,t+m = ai}∣Ft-1]
and the inequality in equation (35) is derived directly from Definition 6.
According to Azuma-Hoeffding inequality (Hoeffding, 1994; Azuma, 1967):
八 T—÷m J	'
Pr(Am) = Pr I X	Xk,m ≥ 0
∖	k=0
/L T—÷m J
Pr I X Ykm
∖ k=0
≥
—
(L T — 1 — ?m J
X	Yk,m
k=0
≤ exp -
"[T]
32
L T-—m J
E	E[Xk,m∣F(k-1)τ+m]
k=0
eσ
T — 1 — m + τ
≥
T
Similarly, from Azuma-Hoeffding inequality,
八 T——m J	'
Pr(Am) = Pr I X	Xk,m = 0
∖	k=0
/L T—÷m J
Pr I X Ykm
∖ k=0
L
—
T-—m J
E E[Xk,m∣F(k-1)τ+m]
k=0
≤ Pr I
T — 1 — ?m
T
」
E	Yk
k=0
：,m ≤
—
T — 1 — m + τ
T
σ
30
Under review as a conference paper at ICLR 2022
Thus
Similarly
≤ exp -
上
8
(36)
T — 1
Pr (C(s0∣s, ai)- P(s0∣s, ai) ≥ e) ≤ X Pr(Am) + Pr(Am)
≤ τexp —
E2。2 [T]
32
m=0
+ T exp (- W! ≤ 2τ exp (-
%2 [T]
32
Pr (Pi(s0∣s,ai) - Pi(s0∣s, ai) ≤ -E) ≤ 2τexp -
%2 [T]
32
=⇒ Pr ("θ(s0∣s, ai) - Pi(s0∣s,ai)∣ ≥ E) ≤ 4τexp (-
%2 [T]
32
which completes the proof.
□
Lemma 14. Assume that the StOChaStiC game with policy θ satisfies (σ, T)-sufficient exploration
condition (Definition 6), then fix s0, s, ai ,for E ≤ 1,
Pr (∣rθ(s,ai) - rθ(s,ai) ∣ ≥ e) ≤ 4τ exp (-
e2T T C
32
Proof. The proof is similar to Lemma 13.
{rθ(S,ai)—ri(S, ai) ≥E}
⊆
U
l{st = s, ai,t = ai}ri (St, at) - (ri (Slai) + E)I{st = s, ai,t = ai} ≥ 0}
l{st = s, ai,t = ai} = 0}
—1 j b T-m J	_
⊆ uj V 1{skτ+m = s, ai,kτ+m = ai}ri(skτ+m, akτ+m) - (ri (S, ai) + E) 1{skτ+m = s, ai,kτ+m = ai} ≥ 0
m=0
k=0
τu1(
m=0
T-Fm J
l{skτ+m = s, ai,kτ+m = ai} = 0
k=0
Let:
A
ʃɪm
b T - m J
〉:l{skτ+m	=	s, ai,kτ+m	=ai}ri(SkT+m, akτ+m) -	(ri	(s,ai) + E)I{skτ十m	=	s, ai,kτ+m	=ai} ≥ 0
k=0
T-Fm J
Cm .	、
l{skτ+m = s, ai,kτ+m = ai} = 0
k=0
Xk,m	.= l{skτ+m	= s, ai,kτ+m =ai}ri(SkT+m, akτ+m) - (ri	(S) ai )	+ E)I{skτ+m	=	s, ai,kτ+m	=ai }
Yk,m .= Xk,m - E[Xk,m∣F(k-1)T+m]
I T — 1 — ?m I
Then {Yk,m}k=0T 」is a martingale difference sequence. Because E ≤ 1, it is easy to verify that
∣Xk,m ∣ ≤ 2. We have that:
∣K,m∣ ≤ ∣Xk,m∣ + E[∣Xk,m∣∣F(k-1)T+m] ≤ 4.
31
Under review as a conference paper at ICLR 2022
Further,
E[Xk,m∣F(k-1)τ+m]
=E[l{skτ+m = s, ai,kτ+m = ai}ri(skτ+m, akτ+m) — (rf (s, ai) + 6)l{skτ+m = s, ai,kτ+m = ai}∣F(k-1)τ+m]
=—EE[l{skτ+m = s, ai,kτ+m = ai}∣F(k-1)τ+m∖ ≤ —eσ
the second line to the third line of the equation is derived by the fact that:
E[1{st = s, αςt+m = αi}ri(st, αt)∣Ft-i] = P(s∣st-i, at-1) E∏θ(αi,α-i∣s)ri(s,期，。-0
a-i
=P (s∣st-i,0t-i)∏θi (αi∣s)rθ (醺电)
=EIri(s, ai)1{st = s, ai,t = at}∣Ft-1]
and the inequality in the third line is derived directly from Definition 6.
According to Azuma-Hoeffding inequality:
(L 丁 J	ʌ
Pr(Am) = Pr X Xk,m ≥ 0
Pr
k=0
L 丁 J
X	Yk
k=0
L 丁 J
X	Yk
：,m
：,m
≤ exp —
k=0
[T]
32
Same as Equation (36), we have that:
Thus
L T - m j
一	EXk,m∣F(k-1)τ+m]
k=0
Pr(Am) ≤ exP —
eσ
ʌ	_	T-1	/
Pr 卜?(s, ai) — rθ(s,oɔ	≥ E) ≤ X	Pr(Am)	+ Pr(Am)	≤ 2τ exp	(一
m=0	∖
工[T]
32
Similarly
Pr (rθ(S) ai) — rθ(s,电)≤ 一e) ≤ 2τexp —
e” [ T]
32
=⇒ Pr Qrθ(s, ai) — rθ(s,αi)∣ ≥ E) ≤ 4τexp (一
e” [ T]
32
which completes the proof.
Lemma 13 and 14 lead to the following corollary:
Corollary 1.
r 」	.( E2σ2Rl∖
Pr(IlMd - Mik∞ ≥ e) ≤ 4τ∣S∣2∣4∣ exp -	(37)
∖	32∣s∣ )
Cr	( E2σ2 I T I ʌ
Pr(IIri Yk∞ ≥ e) ≤ 4τ∣S∣∣Ai∣ exp ——3^	(38)
≤ Pr
〉
〉
T — m + T
T
σ2 m
8
□
32
Under review as a conference paper at ICLR 2022
Proof. We first prove Equation (37)
∣Md -丽k∞ = max X ∏θi(ai∖S0)∣ C(sz∣S,ai) - Pθ(小,出)
(Sg)(S0,ai)
=max X∣ Pi(SZ∖s,ai)- Pi(SZ∖s,a^ ∣
(SS)So I	1
Thus,
{向-闻k∞ ≥ e}
Then according to Lemma 13,
Pr (∣d - Mθ∣∞ ≥e) ≤
U
(s,ɑi)
Σ
(s0,s,ai)
UU
(s,ai) s0
ls, ai) - Pi (s1s, ai )1 ≥ 由]
1 ISIJ
s∖s,ai)- Pi (SZBai)I ≥ S)
s0∣s, ai) - Pi(s0∖s,
Now we prove Equation (38). Since
≤ 4τ∖s∖2∖4exp 卜e⅛r
(SU){ Irθ (S,ai) -ri (S,ai) I ≥ e},
according to Lemma 14,
Pr (krθ - rθ ∣∣∞ ≥ e) ≤ X Pr (H(S,ai) - rθ(S, a) ∣ ≥ e)
(s,ai)
≤ 4τ∖S∖∖4∖ exp -
e2。2 [T]
32
⊆
which completes the proof of the corollary.
□
We are now ready to prove Theorem 7.
「一	一 一 一 . .	_ ʃɔ --7Γ
Proof. (of Theorem 7) From the definition of Q2, Q2,
Qθ=(I-询)-ιrθ,
Qc = (I-YMd )-1rθ,
we have that
∣Qθ - QT ∣∞ = II(I- Yd )-ιrθ -(I-YM )-ιrθ L
= Il(I- yM2)-ι(rτ -rθ)+((I-Yd尸-(I - yMt尸)rθk
≤ ∣∣(i - yMt )-1(rθ - rf)L + 卜(I-YMT )-1(Md - M MI-Yd 尸用匚
Because both Mi and Mi are transition probability matrices, thus:
IlMiTx∣∞ ≤ ιιx∣∞
..-.. ....
IIdχ∣∣∞ ≤ ∣∣χ∣∣∞
—:T	1
∣(I-YMiθ)Tχ∣∣∞ ≤ L∣∣x∣∣∞
33
Under review as a conference paper at ICLR 2022
-:1	1
k(I - γMd )xk∞ ≤ 尸同 ∞
Thus,
kc - Qθk∞ ≤ ∣∣(I - γMθ)-1(∕ -用L + 卜(I - γMθ)-1(Md -MMI-Yd尸用匚
1	Y
≤ I-忱-rk∞ + 万―IMθ - Mf 限忱k∞
1 - Y	(I - Iy
≤ 占向-rk∞ +	kMd - Mk∞
Thus if
krf Tk∞ ≤ (1 - Y)*	IMθ - Mf ∣∣∞ ≤ (1 - γ)2e,
we have that:
'	-77
IQf - Qf k∞ ≤ e,
ThUS from Corollary 1,
Pr (IlC - Qf ∣∣∞ ≥e
≤ Pr (借-讨k∞ ≥ (1 - Y)%) + Pr (kd - Mk∞ ≥ (1 - Y)%)
≤ 4τ SIAilexp -(I-Y)LTC +4τ |S F |Ailexp -(I-M『TC
\	32	/	\	32 | S | 2
≤ 8τ S2 |4 exp (- (I-Y鼠2bTC ),
\	32 | s | 2	J
which completes the proof.
□
J.2 BOUNDING THE ESTIMATION ERROR OF dθ
We first state our main result:
Theorem 8. (Estimation error of df) Under Assumption 3,
i.e., when
Pr (∖∖dθ - df ∣∣ι ≥ E) ≤ 4τ| S| 2 exp -
(1-Y)%2σS 里
32y 21 S | 2
32τ | S | 2
T ≥------------
-(1 - Y)2e2σ2
log (牛)+1,
∙.ι	1	1 -1 ■.	. 1	. -I CIl 7'	ι Il ,
with probability at least 1 一 δ, ∣df — df ∣∣ι ≤ e .
Similar to the previous section, the proof of the theorem begins by bounding the estimation error
^	-K
| PS (SI S)- PS (Sl S) | .
Lemma 15. Under Assumption 3, fix s0, s, ai,for E ≤ 1,
Pr (IPS(Sl S)-pS(Sl S)I ≥ E) ≤4τexp (—E
Proof. According to the definition of PS, we have that
Sl S) - Pf (SI S) ≥e}
⊆
St+1 = S, St = s} - (PS(Sl s) + E)1{St = S
U {χ i{st=s}=0}
34
Under review as a conference paper at ICLR 2022
Let:
{L T — 1 — ?m J
):(1{skτ+m+1 = S , skτ+m = s} - (PS(s |s) + ^)1{skτ+m = s}) ≥ 0
k=0
T-IfL T^J	]
∪ <	1	1{skτ+m = s} = 0)
m=0 I	δ—y	I
k=0
A
ʃɪm
L T-T-m J	_
1 1	(1{skτ+m+1 = S , skτ+m = s} - (PS(s ∣s) + ^)1{skτ+m = s}) ≥ 0
k=0
L T-⅛-m
E	i"+
k=0
m = s} = 0
Xk,m
Xk ,m
Ykm
Ykm
1(
1(
kτ+m+1 = S , skτ+m = s} - (PS (S ∣s) + 6)1{skτ+m = s}
kτ+m = s}
=Xk,m - E[Xk,m∣F(k-1)τ+m]
=Xk ,m - E[Xk ,mF(k-1)τ+m]
A0 ■
Gm ■
S
S
J
I T — 1 — ?m I
Then {Yk,m}k=0T	」is a martingale difference sequence. Because E ≤ 1, it is easy to verify that
∣Xk,m∣ ≤ 2, ∣Xk,m∣ ≤ 1. We have that:
∣Yk,m∣ ≤ ∣Xk,m ∣ + E[∣Xk,m∣∣F(k-1)τ+m] ≤ 4, M,m∣ ≤ M,m∣ + E[∣Xk,m ∣∣F(k-1)τ+m] ≤ 2.
Further,
E[Xk,mlF(k-1)T+m] = E[1{skτ+m = s}∣F(k-1)τ+m] ≥ QS,
and that
E[Xk,m∣F(k-1)τ+m]
=E[1{skτ+m+1 = S , skτ+m = S}- (PS (SlS) + e)1{skτ+m = s}∣F(k-1)τ+m]
=-eE[1{skτ+m = s}∣F(k-1)τ+m] ≤ -eσS
the second line to the third line of the equation is derived by the fact that:
E[1{st+1 = S, St = s}∣Ft-i] = P(SISt-I,αt-1) E∏θ(α∣s)P(s0∣s,a)
a
=P (SISt-ι,αt-i)PS (SlS)
=E[PS(s1s)1{st = sHFt-ι]
and the inequality in the third line is derived directly from Assumption 3.
According to Azuma-Hoeffding inequality:
八 T-T-m J	∖
Pr(Am) = Pr X	Xk,m ≥ 0
∖ k=0
/L T-T-m J
=Pr I X Yk,m
∖ k=0
(L T — 1 — ?m J
X Yk,m
k=0
≤ exP J P:
L T-τ-m J	'
E	E[Xk,m∣F(k-1)τ+m]
k=0
〉
—
T — 1 — m + τ
〉
e°s
T
35
Under review as a conference paper at ICLR 2022
Similarly, from Azuma-Hoeffding inequality,
八 T-⅛-m J	'
Pr(Am ) = Pr	X	Xkm = 0
p/
≤ Pr
k=0
T-Fm J
X Km
k=0
T — 1 — ?m j
L T-Fm J	,
E	E[Xk,m∣F(k-1)T+m]
k=0
Thus
Similarly
Σ Km ≤-
k=0
≤ exp - Q≡
QS
T — 1
Pr (PI(SlS)- Pf(SlS) ≥ e) ≤ X Pr(Am) + Pr(Am)
m=0
≤ T exp - m + T exp -
Q≡	≤ 2τ exp -
F [ T1
32
Pr (PS(s0∣s) - Pj(s0∣s) ≤ -€)≤ 2τexp -
e%S用
32
=⇒ Pr ( I Pis(SIS)- Pj(SIS) ∣ ≥ E) ≤ 4τ exp (-
F [ T]
32
which completes the proof.
Corollary 2.
Pr (庐-PC ≥e)≤ 4τ∣S∣2 exp (- ⅞H
(39)
Proof.
假-PiL
Thus,
｛庐-可L
max
S
C u∪
"}= U
|s)- P(s0∣s)≥ 百}
SlS)- PS(
Then according to Lemma 15,
≥ e) ≤ X Pr
(s',s)
SlS)- PI (SlS)I ≥ 向)
Pr (k Ic WL
—
T — 1 — m + T
T
□
≤4τ ∣s∣2exp (-eWF
which completes the proof of the corollary.
□
36
Under review as a conference paper at ICLR 2022
Proof. (Proof of Theorem 8)
Thus
..^ ..
∣∣<⅛ — dθ ∣∣ι = (1 - Y)Il
Pki
□
J.3 Proof OF Theorem 6
Proof. Since the stochastic game satisfies (τ,σs)-sufficient exploration on states, then for any
θ ∈ Xα, we know that it satisfies (τ, mM"∣)-sufficient exploration. Substitute this into Theorem 7,
we have that for
Tj ≥
32τ(1 + α)2∣S∣3 Pi |4| maxi |4|2
log
+ 1,
(40)
(I-Y)6Eg ɑ2σS
16tTg∣S∣2 Pi|4|
δ
with probability at least 1 - 2^,
才-[∣∞ ≤ (1 + 3S)Pi4.
Similarly, applying Theorem 8, we have that with probability at least 1 - 2^,
∣∣dθ(k)-dθdIii ≤J： PY)^Pa ∣/ I.
(1 + a) V|S| ∑i |Ai|
Since:
[VΦ(Θ) - VΦ(θ)](s ⑹
1	/ ʌ -7j∙,	、
d----dθ(s)Qθ(s, ai)-
1 - Y
1
1 - Y
1	_ , , TT ,	"ɔ , 一
------dθ(S)(Q2(s,ai) - Qθ(s,ai)) +
1 - Y

Qθ(s,ai)(dθ(s) - dθ(S))
≤ 1-ʒ;|Qf (s,αi) - Qc(s,ai)| + (1 -1 y)2 |dθ(S) - dθ(S)I
________Eg	+__________Eg a
(1 + a)，6| PjIAjI	(1 + a)，|s| PjAjI
=______Eg______
，|s|Pj|Aj |
Thus, with probability 1 - δ
2
∣∣VΦ(Θ(k) -
V Φ(θ(k))∣2 =∑∑∑l[vφ(θ) - VΦ(θ)]
i Sai	( , i)
≤ Eg,
V 1 ≤ k ≤ Tg
□
37
Under review as a conference paper at ICLR 2022
K Proof of Theorem 5
Notations: We define the following variables that will be useful in the analysis:
Gn(θ) ：= 1 (ProjX(θ + nVΦ(θ)) - θ)
G>α(θ) ：= 1 (ProjXα(θ + nVΦ(θ)) - θ).
K. 1 Optimization Lemmas
+
Lemma 16. (Sufficient ascent) Suppose Φ(θ) is β-smooth. Let θ+ = ProjXα (θ + nVΦ(θ)). Then
for n ≤ 2β，
φ(θ+)-①⑹ ≥ 4kGbn,α(θ)k2 - 2 ∣∣vφ(θ) - VΦ(θ)∣∣2
Proof. From the smoothness property we have that:
Φ(θ+) - Φ(θ) ≥ VθΦ(θ)>(θ+ - θ) - βkθ+ - θk2
r. ∙	八_L	ɪ ʌ	.	/八.	τ / /W ∖	i	.ι
Since θ+ = ProjXα (θ + nVΦ(θ)), we have that:
(θ + nVbΦ(θ) - θ+)>(θ0 - θ+) ≤ 0, ∀θ0 ∈ Xα
take θ0 = θ, we get:
VΦ(θ)>(θ+ - θ) ≥ 1 kθ+ -θk2.
n
Thus:
VΦ(θ)> (θ+ - θ) = (VΦ(θ) - VbΦ(θ)> (θ+ - θ) + Vb Φ(θ)>(θ+ - θ)
≥ -n ∣∣VΦ(θ) - VΦ(θ)∣∣2 - ɪ ∣∣θ+ - θ∣∣2 + VΦ(θ)>(θ+ - θ)
≥ -2 ∣∣vφ(θ) - VΦ(θ)∣∣2 - ɪ ∣∣θ+ - θ∣∣2 +1 kθ+ - θk2
=2- kθ+ - θk2 - 2∣∣VΦ(θ)-V Φ(θ)∣∣2
Thus from equation (29):
Φ(θ+)- Φ(θ) ≥ (21- - 2)kθ+ - θk2 - 2
2-	2	2
VΦ(θ) - Vb Φ(θ)
≥ 4-kθ+ - θk2 - 2∣∣VΦ(θ)-VΦ(θ)∣∣2
=4kGbn,α(θ)k2 - 2∣∣VΦ(θ)-VΦ(θ)∣∣2
which completes the proof.
□
Lemma 16 immediately results in the following corollary:
Corollary 3. (of Lemma 16) In Algorithm 1, suppose kVb Φ(θ(k)) - VΦ(θ(k))k∞ ≤ g holds for
every 0 ≤ k ≤ TG - 1, then running algorithm 1 will guarantee that:
1 TG -1
X kG>α(θE)k2 ≤
TG
G k=0
4(φmax - φmin)
-TG
+ 2g2
38
Under review as a conference paper at ICLR 2022
Proof. From Lemma 16 we have that:
Φ(θ(k+1)) - Φ(θ(k)) ≥ η∣∣<bη,α(θ(fc))k2 - η ∣∣VΦ(θ(fc)) - VΦ(θ(fc))∣∣2
≥ 4∣Gη,α(θ(fc))k2 - 2竟∙
Thus
1 TG-I
X ∣3α(θE)k2 ≤
G k=0
4(Φ(θ(0)) - Φ(θ(TG)))
≤ 4(Φmax
ηTG
-φmin)
+闿
ηTσ
+ M
□
Lemma 17. (First-order stationarity and ∣Gη,α(θ)∣) Suppose Φ(θ) is β-smooth. Let θ+ =
PrOjXα(θ + ηVΦ(θ)). Then:
VθΦ(θ+)τ(θ0-θ+) ≤ [(1 + ηβ)∣∣G%α(θ)k + ∣∣VΦ(θ) - VΦ(θ)k] ∣∣θ,-θ+k,	∀θ0 ∈ Xα. (41)
Further:
max V%Φ(θ+)τ(θi -θ+) ≤ 2√∣S∣ [(1+ ηβ)∣Gη,α(θ)∣ + ∣VΦ(θ) - VΦ(θ)∣] +-^a- (42)
θi ∈Xi	L	J 1 - Y
Proof. Since θ+ = PrOjXα (θ + ηVΦ(θ)), we have:
(θ + ηVΦ(θ) - θ+)τ(θ0 -	θ+) ≤	0 ∀ θ0 ∈ Xα
=⇒ ηVΦ(θ)τ(θ0 -	θ+) ≤	(θ - θ+)τ(θ0	-	θ+)
=⇒ ηVΦ(θ)τ(θ0 -	θ+) ≤	(θ - θ+)τ(θ0	-	θ+)	+	η(VΦ(θ)	-	VΦ(θ))τ(θ0 - θ+)
=⇒ ηVΦ(θ+)τ(θ0 -	θ+) ≤	(θ - θ+)τ(θ0	-	θ+)	+	η(VΦ(θ)	-	VΦ(θ))τ(θ0 - θ+)
+ η(VΦ(θ+) - VΦ(θ))τ(θ0 - θ+)
=⇒ ηVΦ(θ+)τ(θ0 - θ+) ≤ (∣θ - θ+∣ + η∣VΦ(θ) - VΦ(θ)∣ + η∣VΦ(θ+) - VΦ(θ)∣)∣θ,- θ+∣
≤ (∣θ - θ+∣ + η∣VΦ(θ) - VΦ(θ)∣ + ηβ∣θ+ - θk)∣θ0 - θ+∣
=[(1+ ηβ)∣θ -θ+k + η∣VΦ(θ)-VΦ(θ)k] kθ0-θ+k
=⇒ VΦ(θ+)τ(θ0 - θ+) ≤ [(1 + ηβ)∣Gnα(θ)k + ∣VΦ(θ) - VΦ(θ)k] ∣θ0 - θ+k,
which proves equation (41). We now prove equation (42). For any θi s ∈ ∆(∣41), we know that
(1 - α)θi,s + αU∣Ai∣ ∈ ∆α(∣Ai∣). Let Ui = [U∣4∣,..., U∣4∣],thenforany θ0 ∈ Xi, (1 - α)θi +
X----------V-----------}
∣S∣ times
αUi ∈ Xi0.
Thus:
V为Φ(θ+)>(θ0 - θ+) ≤ V%Φ(θ+)τ((1 - 0)θ'i + αUi - θ+) + V%Φ(θ+)>(θ - (1 - α)θ'i - αUi)
≤ [(1 + ηβ)kGη,α(θ)k + kVΦ(θ) - VΦ(θ)k] k(1 - α)θ'i + αUi - θ+k
+ VθiΦ(θ+)τ (θi - (1 - α)θ'i - αUi)
≤ 2p|S1 [(1 + ηβ)kGη,α(θ)k + IlVΦ(θ) - VΦ(θ)k] + αVθiΦ(θ+)τ(θi - Ui)
Since
VθiΦ(θ+)τ(θi - Ui) = Xdθ(s)Qξτ(θi,s - U∣Ai∣)
S
39
Under review as a conference paper at ICLR 2022
≤ Edθ(s)kQξk∞悯,s- U∣Ai∣k1
S
≤ X dθ(s)
S
2
2
1 - Y ≤ 1 — Y,
we have that:
VθiΦ(θ+)τ(θi - θi) ≤ 2√∣S∣ [(1+ ηβ)∣∣Gn,α(θ)k + IlVΦ(θ) - VΦ(θ)k] + ɪ
□
K.2 Proof OF Theorem 5
Proof. Recall that Φ is β-smooth with β = (ITY)3 (Pn=I ∣4∣)∙ The step size η in Theorem 5
satisfies η ≤
(i-γ)3
4 Pn=I 4
1
2β .
Recall from gradient domination property:
NE-gapi(θ(k+1))=maχ Ji(θi, θ⅛+I))-Ji(个+1), C+1))
θ0 ∈Xi
≤ Mmax(θi -θ(k+I))TV%Φ(θ(k+1))
θi∈Xi
Suppose ∣∣VΦ(θ(k)) - VΦ(θ(k))k∞ ≤ Cg, ∀0 ≤ k ≤ TG - 1, recall from Lemma 17,
NE-gap(θ(k+1)) ≤ maxNE-gapi(θ(k+1)) ≤ Mmax max(θ0 — θ(k+1))τVθ-Φ(θ(k+1))
i	i θi∈Xi	i	i
≤ 2M√∣S∣ [(1+ ηβ)∣Gn。(淤))k + €g] +2αMY
Thus,
1 TgT	1 Tg-I
T- X NE-gap(/+1))2 ≤ -L X 3 ×
G k=0	G k=0
4M 2∣S∣(1 + ηβ)2∣∣G%α(θ(k))∣∣2 + 4M 2∣S尾 + 产 M\
-	(I-Y )2」
12M2∣S∣eg + IlaM2 + 12M2∣S∣(1 + ηβ)2 (左 £,陪""(呼))『)
From Corollary 3, we have that
1 TGT
ψ- X NE-gap(θ(k+1))2 ≤ 12M2∣S∣eg +
TG W
≤ 66M2∣S∣eg +
12a2M 2
O-T)2
12a2M 2
+ 12M 2∣S∣(1 + ηβ)2
4(φmax - φmin)
ηTG
108M2∣S∣(φmax - φmin)
ηTG
(43)
(T-^)2 +
+
Substitute
ɑ =	6M , €g = 2√33Mp∣S∣
into the above inequality we get that:
and TG ≥
648M 2(Φmax
ηe
1- φmin)∣S∣
2
1 Tg-1
ɪ X NE-gap(MD)2 ≤
TG
G k=0
€2
€2	€2
€2
(1 — Y)E
€
ι+ y + 7
Substitute the value of α, Eg in Equation (43) into Theorem 6 will give us:
Tj ≥
206976τnM4 ∣S∣3 max^ ∣4∣3
(1-Y)8E4W
log
16tTg ∣S∣2 Pi∣Ai∣
δ
which completes the proof.
□
+ 1
40
Under review as a conference paper at ICLR 2022
L Smoothness
Lemma 18. (Smoothness for Direct Distributed Parameterization) Assume that 0 ≤ ri(s,α) ≤
1, Vs, a, i = 1, 2,...,n, then:
kg(θ0) - g(θ)k ≤ (ɪɪ^ (X |4|)忸-矶
(44)
where g(θ) = {V%Ji(θ)}.
The proof of Lemma 18 depends on the following lemma:
Lemma 19.
Mi JiW)-v% Ji (θ)k ≤
n
√∣A∣ E 西网T k
j=i
(45)
2
o-ʒ^
Lemma 18 is a simple corollary of Lemma 19.
Proof. (Proof of Lemma 18)
n
IIgW)- g(θ)∣∣2 = E ∣∣V% Ji(θ0) - V% Ji(θ)∣∣2
i=1
≤ ((T⅛)2E∣Ai∣ (E西阿-%j
≤ (o⅛ )2 eia⅛AJ (E 呵-嫌
which completes the proof.
□
Lemma 19 is equivalent to the following lemma:
Lemma 20.
Ji® +	i)-必"。"_)I ≤	√μi∣E/网-%k, v∏ = 1
da	Ia=U	(1 - Y)	v
(46)
Proof. (Lemma 20) Define:
∏i,α(αi∣s):= πθi + aui (ai 1 s) = θs,ai + aua⅛,s
πi,a3lS)= πθ i + aui (ai|s) = θSg + ɑuai,s
∏a(a∣s) := ∏θi+aui(ai∣s)∏θ-i(a-i∣s)
∏a(a∣s) ：= ∏θ0+aui(ai∣s)∏θ-i(a-i∣s)
Qa(S, a) ：= Q(θi +aui ,θ - i ) (S, a)
d'a(S) ：= d(θ0 + aui,θ-i)(s)
According to cost difference lemma,
∂Ji(θi0 + αui, θ-i) - ∂Ji(θi + aui, θ-i) i
∂α	1 a=0
41
Under review as a conference paper at ICLR 2022
1	d Es,a da(s)K(a|s)Aa(s,a) ∣
1 — Y	∂α	∣α=0
_	1
1 — Y
1
<-----
—1 — Y
d Es,a da(s)(K(a|s) 一 πα(0js)) Qa(S,a) ∣
∂α	∣ α=0
/
X dθ (S)区⑷S)—加" ⑷ S) ∣	Qθ (s,α)
∂α	la=0
s,a
|-----------------V---------------}
∖	Part A
+ X dθ(s)(πθ(a\s) 一 π(α∣s))町a(Sa) ∣
∂α	∂α lα=0
s,α
X-----------------------V------------------------
Part B
+ X dda(s) ∣	(π'θ(a\s) 一 π(a\S))Qθ(Sa)
∂—∂	∂α la=0
s,a
V---------------------V---------------------}
Part C
Thus:
PartA = X dθ(s)况(小)丁鹏亦)∣	q^,。)
∂α	∂α	lα=0
s,a
=∣ Edθ (S)Uai,s(πθ-i (α-i∖s) — πθ-i (a-i\s))Qθ (S,a)
∣ s,a
<	ɪɪ- ∣X dθ (S) X ∖Uai,s∖X∣∏θ-i (a-i∣S) — ∏θ-i (a-i∣S) ∣ ∣
∣ s	ai	a - i
<	— (maxX ∖uai,s∖) X dθ (S)2dTV(πθ-。(∙∖s)∖∖πθ-i (∙∖S))
<	± maxE ∖uai,s∖ Edθ(S)E2dTV (πθ< diaigj (∙\s))
一Y ∖ Sai	) S	j=i
=— ( maxX %i,s∖ X dθ (S) X 网,s— θU1
Y ∖	ai	) s	j=i
<	1 —	P∖Ai∖ X dθ (S) X J∖4 ∖kθj ,s — θj,sk
—Y	s	j=i
<	占 √1Ai X西 ʌ IX dθ, (S)2 X 网,s-θj,sk2
Y	j=i	V s	V s
=	1-^ √A∏ X q 当 ∖ jX dθ0 (SR Ii θj—θj k
<	ɪ--√∖Ai∖ X J∖Aj ∖kθj -% Il
I-Y 寸
(47)
(48)
(49)
(50)
(51)
(52)
(53)
<
1	n
1—τ √∣a^ X
j=ι
42
Under review as a conference paper at ICLR 2022
where Equation (47) to Equation (48) is derived from the fact that ∣Qθ(s,a)∣ ≤ I-Y. Equation (49)
to Equation (50) relies on the property of total variation distance:
dTV(πθ-i(IS)llπθ-i(IS)) ≤ EdTV(即(IS)Mπθj-(IS))
j6=i
Equation (51) to Equation (52) is derived from:
maxE∣Uai,s∣≤vW, H≤ 1
ai
kθj,s-θj,skl ≤ q∣Ajlkθj,s-θj,s k
which can be immediately verified by applying Cauchy-Schwarz inequality.
Before looking into Part B, we first define P(α) as the state-action under πα:
[PS)] 、，，，、= πɑds0)P(SlS,a)
(s,a)→(s0,a0)
Then we have that:
∂P(α) I
∂α α=0
For an arbitrary vector x:
x
α=0
Thus:
=Uai,s0 ∏θ-i (a-i∣S0)P (S0∣S,a)
(s,a)→(s0,a0)
=	ua0i,s0πθ-i (a0-iiS0)P(S0iS, a)xs0,a0
(s,a)	s0,a0
≤ kxk∞	iua0i,s0iπθ-i(a0-iiS0)P(S0iS,a)
s0,a0
= kxk∞XP(S0iS,a)Xiua0i,s0iXπθ-i(a0-iiS0)
s0
a0i
a0-i
≤ llxk∞ EP (SlS,a) VZiAiE πθ-i (。-小0)
s0
≤ P∣Aiikxk∞
a0-i
∂Pe(α) I
-a^Iα=0x
≤ VWkxk∞
(s,a)
∞
Similarly we can define P (α)0 as the state-action under πα0 , and can easily check that
~	-l
∂Pe(α)0 I
∂α Iα=0x
(s,a)
≤ √iAiikxk∞
∞
Define:	M(α) := I - γPe(α)-1, M(α)0 := I - γPe(α)0-1.
Because:	-1	∞ M(α) = I - γPe(α)	=XγnPe(α), n=0
which implies that every entry of M(α) is nonnegative and M(α)1 = ι-1γ 1, this implies:
	∣∣M(α)xk∞ ≤ 1-γkxk∞,
43
Under review as a conference paper at ICLR 2022
and similarly
∣∣M(α)0χ∣∣∞ ≤	—IIxlι∞.
1 - Y
Now we are ready to bound Part B. Because:
Qa(S,α) = e>s,a)M (a)ri
_ _ , . _ ~ , .
∂Qa(s,a)	T ∂M(α)	T M、")
= '(W) ^^ri=Ye(s，a)MS) -10Γm⑹r

dQa(S,α)
∂α
一、∂P(Q) 7l ʃ,、
≤Y M(Q)kM(Q)ri
∞
≤ (1 -Y)2 PA1
Thus,
PartB = X d(s)(∏θ(α∣s) - πθ(a∣s))dQi (Sa) ∣
∂α	∂Q la=0
s,a
≤ Xde(S)∣πθ(a∣s)-πθ(a∣s)∣ dQh(Sa) I	n
z—∂	∂Q	la=0
s,a
≤ (T-^PA∣ Xdθ(s)2dTV(πe,(∙∣s)∣∣∏e(∙∣s))
≤ (ɪPA∣Xdθ(S)X2dTγ(πe< (∙∣s)∣∣∏θj (∙∣s))
= (r⅛ ρA∣ X de (s) X 网,s-θuι
n
≤ 7t⅛ pa∣ X de (s) X 不呵，s -θj,sk
(	Y)	S	j=1
≤ (1⅛ pA∣ X 西 SX de (s)2 SX kθj,s -θj,sk2
n
≤ ρA∣X 西网-% k
Now let,s look at Part C:
da(s) = (1 - Y) X ρ(s') X Tna (a'∣s')e>f0,α0) M(Qy X e(s,a00)
s0	a0	a”
/
)
X P(S0)X d¾E) e屋a，) M (Q)0 X e(s，a，，)
、------------------V-------------------}
∖	v1
∖
+ X P(s') X Na (alCe葭a，)7，)X e(s，a，，)
s，	a，	a，，
X-------V--------Z
v1	)
(1 - Y)
VTM(q)0 + Yv>M(q)0dPO)M(Q)) X e(s，a，，)
44
Under review as a conference paper at ICLR 2022
Note that vι, V are constant vectors that are independent of the choice of s. Additionally:
MkI = XP(S)X dπ⅛∣s) e(s,a)
s	a	1
=X P(S)XT2
sa
=EP1 * * * (S)E luαi,s∣ πθ-i (a-i∣s)
sa
≤ XP(S)X κ,s∣≤ √μi∣
sa
kv2k1 = Il X P(S) X πα (a∣s)e(s,a)k1
sa
=XP(S) XNCl(a∣s) = 1
sa
Thus:
PartC = X dda(s) I	(πθ (a∣s) — πθ (a∣s))Qθ (s,a)
z—∂	∂α	lα=0
s,a
=(1 — Y)
1	( ) 2	2	( )	ʃ"1 ∣y	I C () )XX e(s,a0)(πθ (a∣s) — πθ (a∣s))Qθ (s,a)
∂a	I α=0	/
s,a a0
{z
V3
,
≤ (1—γ)(十MkIkv3k∞ + (ɪɪ√A⅛2k1Mk∞)
≤ √¾3k∞
1 一 Y
Additionally:
∣[v3](s0,a0) I = E(πθ0 (a∣so) — πθ(a∣s0))Qθ(s0,a)
a
≤	X ∣∏θ0(a∣S0) — ∏θ(a∣S0)∣
1 — Y V
=τ------2dTV(∏θ0 (∙∣s0 )∣∣∏θ (∙∣s0))
1 一 Y
1 n
≤ 1—∑22dTV(πθ< (∙∣s0X∣πθj (∙∣s0))
1 ʌ ,
=L X 网,s- θj,sk1
1	n
≤ L X 西网,s- θj,sk
1	n
≤ L X西网-%k
Combining the above inequalities we get:
PartC ≤ √≡kv3k∞ ≤ 7√¾ X 西I% - %k
I-Y	(I-Y) j=1
45
Under review as a conference paper at ICLR 2022
Sum up Part A-C we get:
∂Ji(θi0+αui,θ-0 i) - ∂Ji(θi + αui, θ-i)
∂α
α=0
≤
≤
--—(Part A + Part B + Part C)
1-γ
2	n I---
(T-YF pιAi∣X √∣Ai ∣kθj - θi k,
which completes the proof.
□
M Auxiliary
We recall Lemma 9.
Lemma 9. Let X denote the probability simplex of dimension n. Suppose θ ∈ X, g ∈ Rn and that
there exists i* ∈ {1, 2,...,n} and ∆ > 0 such that:
θi* ≥ θi , ∀i 6=
*
i
gi* ≥ gi + ∆,	∀i = i*.
Let
θ0 = P rojX (θ + g),
then:
∆
θi* ≥ min{1, θi* + ^2}
Proof. Let y = θ + g, without loss of generality, assume that i* = 1 and that:
yι >y ≥ y ≥ …≥ yn.
Using KKT condition, one can derive an efficient algorithm for solving P rojX (y) (Wang & Carreira-
Perpinan, 2013), which consists of the following steps:
1.
Find P ：= max{1 ≤ j ≤ n : yj + j(1 - Pi=I y) > 0};
2.
3.
Set λ := 1 (1 - Pp=ι yj
Set θi0 = max{yi + λ, 0}.
From the algorithm, we have that:
λ=P卜
—
ρ
X yi
1-
ρ
(θi + gi)
i=1
Ifρ≥2,
Ifρ=1,
Thus:
P1
—
i=1
ρ	1ρ
X θi)- PX gi
1ρ
≥- ρ S gi.
θ10 = max{y1 + λ, 0} ≥ y1 + λ ≥ θ1 + g1
1ρ
-1X gi
ρ
ρ i=1
≥ θι + (1 - -)gι - - X(gι — ∆) = θι + P--∆ ≥ θι + 餐.
P P i=2	P	2
θ10 = y1 + λ = y1 + (1 - y1) = 1.
which completes the proof.
θ1 ≥ min{1, θ1 + ^2},
□
46
Under review as a conference paper at ICLR 2022
N Numerical Simulation Details
Verification of the fully mixed NE in Game 2 We now verify that joining network 1 with proba-
bility 3i-⅛) ,i∙ι
1-3
πθi(ai = IIs)= 3(1 - 2e), ∀s ∈ S, i = 1, 2,
is indeed a NE. First, observe that
θ	1 - 3
Pr (si,t+1	) =(3(1 - 2e)
—(1 — 3e
=3(1- 2e)
P (si,t+1 = 1|ai,t = 1) + 1 -
1 - 3
(i+ 11 - 3(1- 2e)卜
1	- 3
3(1 - 2)) P(si,t+1 = 1|ai，t =2)
1
:一
3
Thus,
∞	2Y
V(S) = T(S)+ J2EstY T(St) = r(S)+ 3(1 - Y),
Qθ(s, ai) = t(s) + y X P(Siai, a-i)∏θ-i(a-i∣S)V(s0)
s0,a-i
r(S) + Y X (P(SiIai)Ρrθ(S-i = 1)r(si, s-i = 1) + P(SiIai)Prθ(S-i = 2)r(si, s-i = 2)) + ʒ∣γ~~Γ
s0i∈{1,2}	i	i	3(1-γ)
12
=r(S) + YP(Si = 1|ai) (3r(si = 1, s-i = 1) + 3T(Si = 1,s-i = 2)
+ YP(Si = 2|ai) (3(Si = 2, S-i = 1) + 3T(S'i = 2, S-i = 2)) +
r ⑶ + 3Y + 3(f⅛ = T(S) + 3(¾ = V(S)，
3(1-γ)
which implies that:
(θi - θi)>Vθi Ji(θ) = 0,	∀θi ∈Xi,	i = 1,2,
i.e. θ satisfies first-order stationarity. Since dθ(S) > 0 holds for any valid θ, by Theorem 1, θ is a NE.
Computation of strict NEs in Game 2 The computation of strict NEs is done numerically, using
the criterion in Lemma 4. We enumerate over all 28 possible deterministic policies and check whether
the conditions in Lemma 4 hold. For = 0.1, Y = 0.95, and an initial distribution set as:
ρ(S1 = i, S2 = j) = 1/4, i,j ∈ {1, 2},
the numerical calculation shows there exist 13 different strict NEs.
47