Under review as a conference paper at ICLR 2022
Implicit Bias of Linear Equivariant Networks
Anonymous authors
Paper under double-blind review
Abstract
Group equivariant convolutional neural networks (G-CNNs) are generalizations of
convolutional neural networks (CNNs) which excel in a wide range of scientific
and technical applications by explicitly encoding group symmetries, such as rota-
tions and permutations, in their architectures. Although the success of G-CNNs is
driven by the explicit symmetry bias of their convolutional architecture, a recent
line of work has proposed that the implicit bias of training algorithms on a partic-
ular parameterization (or architecture) is key to understanding generalization for
overparameterized neural nets. In this context, we show that Z-Iayer full-width
linear G-CNNs trained via gradient descent in a binary classification task con-
verge to solutions with low-rank Fourier matrix coefficients, regularized by the
2/Z-Schatten matrix norm. Our work strictly generalizes previous analysis on the
implicit bias of linear CNNs to linear G-CNNs over all finite groups, including the
challenging setting of non-commutative symmetry groups (such as permutations),
as well as band-limited G-CNNs over infinite groups. We validate our theorems
via experiments on a variety of groups and empirically explore more realistic non-
linear networks, which locally capture similar regularization patterns. Finally, we
provide intuitive interpretations of our Fourier space implicit regularization results
in real space via uncertainty principles.
1 Introduction
Modem deep learning algorithms typically have many more parameters than data points, and ⅛eir
ability to achieve good generalization in this overparameterized setting is largely unexplained by
current theory. Classic generalization bounds, which bound generalization errors when models are
not overly "complex,“ are vacuous for neural networks that can perfectly fit random training labels
(Zhang et al., 2017). More recent work analyzes the complexity of deep learning algorithms by in-
stead characterizing the properties of the functions they output. Notably, prior work has shown
that training via gradient descent implicitly regularizes towards certain hypothesis classes with
low complexity, which may generalize better as a result. For example, in Underdetermined least
squares regression, gradient descent converges to the 0口Om ɪninimizer, while a poiɪitwise-square
reparametrization converges to the ^ɪ-nonn minimizer (Gunasekar et al., 2018a). Such phenom-
ena are consistent with certain linear neural networks, e,g.t Gunasekar et al. (2018b) showed that
learned linear fully-connected and convolutional networks implicitly regularize the h norm and a
depth-dependent norm in the Fourier space respectively.
From a more applied perspective, a large body of work imposes structured inductive biases on deep
learning algorithms to exploit symmetry patterns (Kondor, 2007; Reisert9 2008; Cohen & λVelling,
2016b). One prominent method parameterizes models over functions that are equivariant with re-
spect to a symmetry group (z.e., outputs transfoπn predictably in response to input transformation).
In fact, Kondor & Trivedi (2018) showed that any group equivariant network can be expressed as
a series of group convolutional layers interwoven with poiɪitwise nonlinearities, demonstrating that
group convolutional neural networks (G-CNNs) are the most general family of equivariant networks.
Our Contributions: The explicit inductive bias of G-CNNs is the main reason for their usage. Yet,
to the best of our knowledge, the implicit bias imposed by equivariant architectures has not been
explored. Here, we greatly generalize the results of Yun et al. (2020) and Gunasekar et al. (2018b)
to linear G-CNNs whose hidden layers perform group equivariant transformations. We show, sur-
prisingly, that L-Iayer G-CNNs are implicitly regularized by the 2/L-Schatten norm, which is the
2/L norm of a matrix,s singular values, over the irreducible representations in the Fourier basis. As
1
Under review as a conference paper at ICLR 2022
0	50	100	150	200	250	300	350	400
epoch
(a) Fourier space norm of network linearization β
Figure 1: Training via gradient descent of linear G-CNNs (with linearization /3) implicitly biases
towards sparse singular values for Fourier matrix coefficients of β (see Theorem 1). Figure la traces
the D8 Fourier space sparsity over the course of training three architectures in an overparameterized
classification task. The G-CNN converges to a Fourier-sparse linearization in contrast with fully-
connected and convolutional networks. Figure Ib illustrates the uncertainty principles discussed in
Section 6 which show that sparseness in (group) Fourier space necessitates being “dense” in real
space, and vice versa. See Appendix D for more visualizations of the implicit bias.
6.0
0	50	100	150	200	250	300	350	400
epoch
(b) Real space norm of network linearization β
a result, convergence is biased towards sparse solutions in the Fourier regime, as summarized below
and illustrated in Figure la, as well as further experiments in Section 6.
Theorem 1 (main result; informal). Let NN刀(∙) denote an L layer linear group convolutional neural
network, in which each hidden layer performs group cross-correlation over the group G With a
full-yvidth kernel and the final layer is a fully connected layer. When learning linearly separable
data {xi, yi}i=1 using the exponential loss function, the network converges in direction to a linear
function NNLQ) Y βτx, where β is given by the following minimization problem:
mn I同[I s.t. yix{β ≥ 1 ∀z ∈ [∏∖	(1)
where ∣∣^∣∣^ denotes the ςl∣L-Schatten norm of the matrix Fourier transform of β (see Defini-
tion 4.1) equivalent to
r	-1 L/2
IlCrulgI晨r	, I
Lp∈ 言	-
where G is a complete set of unitary irreducible representations of G and dp is the dimension of
irreducible representation p.
We note that both sparsity (for vectors) and low-rankness (for matrices) are desirable properties for
many applications, not least because such predictors are efficient to store and manipulate, thus po-
tentially expanding the scope of application of G-CNNs to areas where sparsity and low-rankness
are explicitdesiderata.Connecting our findings to research on uncertainty theorems (Wigderson &
Wigderson, 2021) we also show that the implicit regularization towards sparseness (or low rank irre-
ducible representations) in the Fourier regime necessarily implies that solutions in the real regime are
“dense,“ as illustrated in Figure lb. These results provide a more intuitive and practical perspective
into the inductive bias of G-CNNs and the types of functions that they leam.
We proceed as follows. In Section 2, we discuss related works and their relation to our contribu-
tions. In Section 3, we define notation. Section 4 provides a basic background in the group theory
and Fourier analysis necessary to understand our results. Our main results are stated in Section 5,
with the main proof ideas for the abelian (or commutative) and non-abelian (or non-commutative)
cases given in Subsection 5.1 and Subsection 5.2, respectively (complete proofs can be found in
Appendix A). Section 6 validates our theoretical results with synthetic experiments on a variety
of groups and exploratory experiments validating our theory on non-linear networks. Finally, we
discuss these results and future questions in Section 7.
2
Under review as a conference paper at ICLR 2022
2	Related Work
Enforcing equivariance and symmetries via parameter sharing schemes was introduced in the group
theoretic setting in Cohen & Welling (2016a) and Gens & Domingos (2014). Despite considerable
interest in equivariant learning, to the best of our knowledge, no works have explored the implicit
regularization of gradient descent on equivariant convolutional neural networks.We show that the
tensor formulation of neural networks in Yiin et al. (2020) and the proofs in Gunasekar et al. (2018b)
encompass G-CNNs for which the underlying group is cyclic and naturally extend to G-CNNs over
any commutative group (see Subsection 5.1). However, ttιese works do not cover the case of convo-
lutions with respect to non-commutative groups, such as three-dimensional rotations and permuta-
tions, which incidentally include some of the most compelling applications of group equivariance in
practice (Zaheer et al., 2017; Anderson et al., 2019; Esteves et al., 2018). As such, articulating the
implicit bias in the more general non-abelian case is important for understanding many of the current
group equivariant architectures (Zaheer et al., 2017; Kondor et al., 2018; Esteves et al., 2018; Weiler
& Cesa, 2019). However, non-abelian convolutions require more structure to theoretically analyze
compared to abelian convolutions: the former are merely pointwise multiplications in Fourier space,
whereas the latter requires matrix multiplication over irreducible representations which cannot be
expressed in the tensor language of Yiin et al. (2020). Instead, we use standard optimization tools
and comparable convergence assumptions from the work of Gunasekar et al. (2018b) to explicitly
characterize the stationary points of convergence for non-abelian G-CNNs.
We also note that our results are consistent with the results of Razin & Cohen (2020) showing that
implicit generalization is often captured by measures of complexity which are quasi-norms such
as tensor rank. Our results prove ⅛at linear G-CNNs are biased towards low rank solutions in the
Fourier regime, via regularization of the 2/L-Schatten norms over Fourier matrix coefficients (also
a quasi-norm). Lastly, there is a line of work focusing on understanding the expressivity (Kon-
dor & Trivedi, 2018; Cohen et al., 2019; Yarotsky, 2021) and generalization (Sannai & Imaizumi,
2019; Lyle et al., 2020; Bulusu et al., 2021; Elesedy & Zaidi, 2021) of equivariant networks but not
specifically the effects of implicit regularization.
To analyze bounded-width filters, which are more commonly used in practice, a recent work by Ja-
gadeesan et al. (2021) shows that the implicit regularization for an arbitrary filter width K is unlikely
to admit a closed-form solution. Separate from calculating the exact form of implicit regularization,
there is a rich line of work that details the trade-offs between restricting a function in its real ver-
sus Fourier regimes via uncertainty principles (Meshulam, 1992; Wigderson & Wigderson, 2021).
While the connection between uncertainty theorems and bounded-width convolutional neural net-
works has not been thoroughly explored, Caro et al. (2021) and Nicola & Trapasso (2021) highlight
the importance of uncertainty principles for understanding the behaviour of modem CNNs.
3	Notation
Throughout this text, we denote scalars in lowercase script (a), vectors in bold lowercase script (α),
matrices in either bold uppercase script (A) or lowercase script hat (a) when vectors are transformed
into the Fourier regime (see Definition 4.1)t and tensors in bold non-italic uppercase script (A). For
f a function with range in C, we overload notation slightly and let f denote the function with an
element-wise complex conjugate applied, i.e. /(N) = /(N). If ʃ is defined on a group G, let
f~ (g) = f(g~1). For a vector α ∈ Cn or a matrix A ∈ Crnxn9 we denote its conjugate transpose
as at and respectively. We use (■, ■) to denote the standard vector inner product between two
vectors and 卜，)M to denote the inner product between matrices defined as {A, B}m =
We use Il * ∣∣p to denote the vector p-norm (p = 2 when subscript is hidden) and ∣∣ ∙ ∣肥 to denote the
p-Schatten norm1 of a matrix (equivalent to the p-vector noɪm of the singular values of the matrix).
We denote groups by uppercase letters G. We denote an irreducible representation (irrep) of a
group G by p or pi and a complete set of irreps as G, so every Unitary irrep p is equivalent (up to
isomorphism) to exactly one element of G. The dimension of a given irrep pis dp.
1Despite our terminology, p-vector and p-Schatten norms are technically quasi-norms fbrp < 1.
3
Under review as a conference paper at ICLR 2022
Figure 2: Cross-correlation of two functions over a group is equivalent to matrix multiplication over
irreps (shown as blocks of a larger matrix here) in Fourier space.
4	Background in group theory and Group-Equivariant CNNs
In this study, we analyze linear G-CNNs in a binary classification setting, where hidden layers
perform equivariant operations over a finite group G9 and networks have no nonlinear activation
function after their linear group operations. Inputs 电 are vectors of dimension ∣G∣ (i.e.f vectorized
group functions æ : G —> R), and targets % are scalars taking the value of either ÷1 or —1. Hidden
layers in our G-CNNs perform cross-correlation over a group G, defined as
(g*∕ι) (U) = Eg(UM(u)	(3)
veG
Note, that the above is equivariant to the left action of the group, i.e,, if w ∈ G and g,w[u) = g(wu)i
then [g,w ★ ft)(ω) = (g ★ ft)(wu). The final layer of our G-CNN is a fully connected layer mapping
vectors of length ∖G∖ to scalars. We note that this final layer in general will not construct functions
that are symmetric to the group operations, as strictly enforcing symmetries in this linear setting will
result in trivial outputs (only scalings of the average value of the input). Nonetheless, this model
still CaPtureS the composed convolutions of practical G-CNNs, and exhibits non-trivial implicit bias.
Analogous to the discrete Fourier transform, there exists a group Fourier transform mapping a func-
tion into the Fourier basis over the irreps of G.
Definition 4.1 (Group Fourier transform). Let ʃ : G → C. Given a fixed ordering of Gf let eu
be the standard basis vector in 肽IGl 比或 is 1 at the location of u and 0 elsewhere. Then, f =
EUeG f(u)eu is the vectorized function f. Given G a complete set of unitary irreps of Gf IetPSG
be a given irrep of dimension dp, ρ : G ——> GL (dp, C)2. The group Fourier transform of /,
于：GTCata representation p is defined as (Terras, 1999)
F(P) = ∑ An)P(U)∙	(4)
u∈G
By choosing afixed ordering of Gf one can similarly construct f as a block-diagonal matrix version
of f (as in Figure 2). We define Tm t。be the matrix Fourier transform that takes f to f:
F=万Mf =㊉ F(P)©嗫 ∈GL(∣G∣,C).	(5)
p∈G
f or TmJ are shortened notation for the complete Fourier transform. Furthermore, by vectorizing
the matrix f, there is a unitary matrix T taking f to f, analogous to the standard discrete Fourier
matrix, We use the following explicit construction of T: denoting 包夕区引 as the column-major
vectorized basis for element Pij in the group Fourier transform, then we canform the matrix
卜=工工-7^f P(u^3e[p^j]eu∙	(6)
ueGpEG V lθr∣ ij=ι
Intuitively, for each group element gf the matrix 尸 contains all the irrep images p(g) "flattened" into
a single column. See Appendix Bforfurther exposition.
Convolution and cross-correlation are equivalent, up to scaling, to matrix operations after Fourier
transformation. For example, in the case of cross-correlation (Equation 3), (g * ⅛)(p) = g(p)h(p)∖
This simple fact, illustrated in Figure 2, is the basis for the proofs of our impHcit bias results.
2Note that for an abelian group, dp = 1 Vp. For standard Fourier analysis over the cyclic group, each pis a
complex sinusoid at some frequency.
4
Under review as a conference paper at ICLR 2022
5	Main Results
We consider linear group-convolutional networks for classification analogous to those of Yun et al.
(2020) and Gunasekar et al. (2018b). A linear G-CNN is composed of several group cross-
correlation layers followed by a fully connected layer. The input is formalized as a function on
the group (according to some pre-defined ordering of elements), æ ： G → R, and the output is a
scalar. Explicitly, let G be a finite group with æ, wɪ,..., w∑j real-valued functions from G to R.
The network output is NN(je) = (æ ★ wɪ * ∙ ∙ ∙ * wɪ,-i, = ｛x^β). Let W = [wɪ … Wl∖
be the concatenation of all network parameters, and β = P(W) = wɪ, ★ wl-i * * * * * wι be the
“end-to-end“ linear predictor consisting of composed cross-correlation operations. One can check
(as we do in Lemma A.l) that β = FMB =助…向.Networks are trained via gradient descent
over the exponential loss function on linearly separable data ｛也逅 统｝也ι∙ Iterates take the form
M'+D =	-%VwAQ(W))	(7)
Where 2(Q向⑼= exp(-(x,β) - y) and £(/3) = ∑⅛ι 2(〈瓯/3),统).
5.1	Abelian
Similar to ordinary (cyclic) convolution3, the commutative property of abelian groups implies that
convolutions in real space are equivalently pointwise multiplication of irreps in Fourier space since
all irreps are one-dimensional for commutative groups (dp = IVp). To start, recall the key definition
of Yiin et al. (2020), determining which network architectures fall within the purview of their results:
Proposition 5.1 (paraphrased from Yun et al. (2020)). Let M(O:) be a map from data æ ∈ Rd to
a data tensor M(x) ∈ Rfcιxfc2x,"xfcz,. The input into an L-Iayer tensorized neural network can
be written as an orthogonally decomposable data tensor if there exists a full column rank matrix
S ∈ CTrtXd an^semi-unitary matrices 5,…？ UL ∈ CMXTn ^]τere d <m < miι⅛ kg such that:
m
MQ) = £ [S 项([5]./ ® [U2].,j 0∙∙∙Θ [C∕l],j∙) ,	(8)
J=I
such that the network output is the tensor multiplication between M(C) and each layer's parameters:
d d
NN(a?; Θ) = M(:r) ∙ (wɪ,..., wɪ,)=Σ ∙∙∙ΣM(右)£1 …(∞l)i1...............(wi)ii.
心 1=1	i∑,=l
Indeed, a linear G-CNN over an abelian group can be expressed iɪi a way that satisfies Proposi-
tion A.3 for an appropriate choice of S and tʃɪ,..., Ul9 as stated in the following proposition.
Proposition 5.2. Let M(H) be an orthogonally decomposable data tensor with associated matrices
S, UI,..., Ul as in Proposition 5.1. Given a finite abelian group G, let d = m = k£ = ∖G∖ and
F ∈ CiXd be the group Fourier transform of G (see Definition 4.1). With S = d~^~E unitary
matrices Ug = ʃ-1 = R, and the data tensor M (a?) defined correspondingly, the output of a
G-CNN with real-valued filters wɪ,..., Wl is a tensor operation:
M(ic) ∙ (wι,... wl) = (æ* wɪ ★ ∙ ∙ ∙ ★ w√y-ι, wɪ,)
The proof is deferred to Appendix A. Fundamentally, the result requires not only that T is unitary,
which holds for all finite groups, but also that cross-correlation is pointwise multiplication (up to a
conjugate transpose) in Fourier space, i.e. ʃ(æ * w) = (ʃæ) Θ (尸ML This property only holds
for commutative groups, as matrix multiplication is pointwise multiplication only for matrices of
dimension dp = 1. Given Proposition 5.2, we apply the implicit bias statement of Yun et al. (2020).
Theorem 5.3 (Implicit regularization of linear G-CNNs for G an abelian group). Suppose there
exists λ > 0 such that the initial directions tuɪ,... ? Wl of the network parameters satisfy
3In fact, we include a canonical result in the appendix, Theorem A.4, demonstrating that all finite abelian
groups are direct products of cyclic groups, i.e. multidimensional translational symmetries.
5
Under review as a conference paper at ICLR 2022
∣[J7w^]j∣2 — ∣[.77wl]j∙∣2 ≥ Xforallt ∈ [L — 1] and j ∈ [m], Le. if the Fourier transform magnitudes
of the initial directions look sufficiently different pointwise (which occurs mth high probability for
e.g. a Gaussian random initialization). Then, β = P([wι,..., Wl]) converges in a direction that
aligns with a stationary point NOO Ofthefollowing optimization program:
jnmι ∣∣J72∣∣2∕l s.t. yi(xi,z) ≥ l,∀i ∈ [n]	(9)
As noted in Theorem A.4 of the Appendix, all finite abelian groups can be expressed as a direct
product of cyclic groups. In contrast, many groups (rotations, subgroups of permutations, etc.) with
much richer structure are non-commutative, and we now turn our attention to the non-abelian case.
5.2	Non-Abelian
In Fourier space, non-abelian convolution consists of matrix multiplication over irreps, and do not
fit the pointwise multiplication structure of Proposition 5.2. We instead build upon the results of
Gunasekar et al. (2018b), and directly analyze the stationary points of the proposed optimization
program to prove the following:
Theorem 5.4 (Non-abelian; see also Theorem A,5). Consider a classification task with ground-truth
linear predictor β, trained via a linear G-CNN architecture with L > 2 layers under the exponential
loss. For almost all β-separable datasets {5, %}C=ι, any bounded sequence of step sizes ηtf and
almost all initializations: if the loss converges to Oj the gradients converge in direction, and the
iterates themselves all converge in direction to a classifier with positive margin, then the resultant
predictor is a scaling of a first order stationary point of the optimization problem:
ɪɪnn ∣∣^∣∣2% s.t. y∏,yn {xn,β) ≥ 1∙	(10)
To prove the above statement, we show that linear G-CNNs converge to stationary points of the KKT
conditions above, which is also the high-level method of Gunasekar et al. (2018b). However, our
proof differs in several key ways from ⅛at of Gunasekar et al. (2018b). First, we redefine operations
of the G-CNN as a series of inner products and cross-correlations over the space of the matrix
Fourier transform of Definition 4.1. Second, in this Fourier space, we analyze the subdifferential
of the Schatten norms to ultimately show that the KKT conditions of Equation 10 are satisfied. In
contrast, Gunasekar et al. (2018b) analyze the subdifferential of a different objective, the ordinary
2∕I∕-vector norm. The fact that the irreps of a group are only unique up to isomorphism (e.g.f
conjugation by a unitary matrix) aids in identifying this Schatten norm as the correct regularizer,
since Schatten norms are invariant to unitary matrix conjugation. These features are specific to the
non-abelian case. More specifically, the proof of this result follows the outline below:
1.	First, by applying a general result of Gunasekar et al. (2018b), Theorem A.6, we char-
acterize the implicit regularization in the full space of parameters, W (in contrast to the
end-to-end linear predictor /3), as a (scaled) stationary point Wo° of the following opti-
mization problem in W:
v≡np ∖∖W∖∖l s.t. yn,yn(xn,-p(W))≥l	(H)
2.	Separately, we define a distinct optimization problem, Equation 10, in β to show that sta-
tionary points OfEquation 11 are a subset of ⅛ose of Equation 10, up to scaling.
3.	The necessary KKT conditions for Equation 11 characterize its stationary points:
≡{¾ : oιn ≥ 0}^L1 s.t α⅛ = 0 if yn(xn,P(W°°)) > 1
w∞ = vwip(vr∞) £ anyn*^n = ▽他«("),£a∏y?Iir Ti)
, n	J	n
From here, we show that the sufficient KKT conditions for Equation 10 are also satisfied by
the corresponding end-to-end predictor. In particular, we calculate the set of subgradients4
∂0^β∖∖^ for p = ɪ < 1, and then use Equation 12 to derive recurrences demonstrating
that a positive scaling of Erl anynxn is a member of this set.
4∂o is the local subgradient of Clarke (1975): ∂of(β) — conv{limi->00 ▽/(6 + hi) : ⅛ → 0}
6
Under review as a conference paper at ICLR 2022
Remark. For abelian groups where all irreps are one-dimensional, β in Theorem 5.4 is a diagonal
matrix. Thus, the p-Schatten norm coincides with the p-vector norm of the diagonal entries, recov-
ering results in Subsection 5.2. However, Theorem 5.4 requires stronger convergence assumptions.
Infinite dimensional groups: Theorem 5.4 applies to all finite groups, but G-CNNs have extensive
applications for infinite groups, where outputs of convolutions are infinite-dimensional. Here, it is
common to assume sparsity in the Fourier coefficients and “band-limit“ filters over a set of low-
frequency irreps (under some natural group-specific ordering) that form a finite dimensional linear
subspace (we denote the representation of a function in this band-limited Fourier space as w.) Here,
G-CNNs with band-limited filters take precisely the form of the finite G-CNNs from Theorem 5.4.
Thus, slightly modifications yield the following for infinite groups (see Appendix A.2.1 for details).
Corollary 5.5 (Infinite-dimensional groups with band-limited functions; see also Theorem A.ll).
ʌ ʌ _
Let G be a compact Lie group wth irreps Gr and let B C G mth ∖B∖ < ∞.5 Proceed fully in
Fourier space, in the subspace corresponding to B: consider a linearly separable classification task
with ground-truth linear predictor β and inputs xj both real-valued and supported only on irreps in
B, and proceed by gradient descent on the band-limited Fourierspace filters. Under near-identical
conditions as Theorem 5.4, the resultant predictor is a scaling of a first order stationary point of:
噌1 陋L： sλ' YMrl 炸"、a ≥∖.	(13)
6	Experiments
Here, we first experimentally confirm our theory in a simple setting illustrating the effects of implicit
regularization. Then, we relax the crucial assumption of linearity in our setup to empirically show
that our results may hold locally even in nonlinear settings. Note that the results for nonlinear
networks in Subsection 6.2 are only empirical in nature, and Theorems 5.3 and A.5 do not necessarily
hold in the more general nonlinear setting. For all binary classification tasks, we use three-layer
networks with inputs and convolution weights in R∣gL Since we are interested in the resulting
implicit bias alone, we only analyze loss on data in the training set. A complete description of our
experimental setup can be found in Appendix E.
Throughout this section, we plot norms in the Fourier and real regimes side-by-side to highlight un-
avoidable trade-offs in implicit regularization between the two conjugate regimes. These trade-offs,
which have a rich history of study in physics and group theory, are commonly termed uncertainty
principles (Wigderson & Wigderson, 2021). One especially relevant uncertainty theorem states that
sparseness in the real or Fourier regime necessarily implies dense support in the conjugate regime.
Theorem 6.1 (Meshulam uncertainty theorem (Meshulam, 1992)). Given a finite group G and f :
G —> C, let G be the set of irreps of G and f be the Vectorizedfunction (see Definition 4.1). Then
lsɪɪpp(ʃ)l rank(ʃ) = ∣supp(∕)∣ EdPraiIk(Fs)) ≥ IGl	(14)
p∈g
Other related uncertainty principles are detailed in Appendix C.
6.1	Empirical Conhrmation of Theory
Here, we trace the regularization through (training) epochs via analysis over three groups which are
all trained to classify data with ±1 labels:
•	D8 (Figure 1): The dihedral group D8 is a simple non-abelian group that captures the
geometry of a square6. Inputs are vectors with elements drawn i.i.d. from 7V(0,1).
•	(C⅛ x C⅛) x Qs (Figure 3): A non-abelian group that has irreducible representations of
up to dimension 8 displays implicit regularization over a more elaborate group structure.
Inputs are vectors with elements drawn i,i.d. from the standard Normal distribution.
5For example, G = SO(3) and B indexes all Wigner d-matrices with	≤ L (Kondor et al., 2018).
6We use the convention that Dn is the dihedral group of order n.
7
Under review as a conference paper at ICLR 2022
•	(Cr28 ×。28)× Ds (Figure 4): A non-abelian group which acts on images (the digits 1 and
5) from the MNIST dataset.
In the three settings above, we compare the behaviors of G-CNN, traditional CNN7, and fully-
connected (FC) network architectures with similar instantiations. We plot the the real space vector
norm and Fourier space Schatten norm of the network linearization over training epochs. All mod-
els perfectly fit the data in this overparameterized setting, and convergence to a given regularized
solution corresponds to convergence in the loss to zero.
Consistent with theory, G-CNN architectures shown in Figures 1, 3, and 4 have the smallest Fourier
space Schatten norms among the architectures. FC networks exhibit no group Fourier regime regu-
larization while standard CNNs exhibit some regularization since they share some irreducible rep-
resentations with the G-CNN. The differing behaviors of CNNs and FC networks show that implicit
regularization is a consequence of the choice of architecture and not inherent to the task itself.
(a) Fourier space norm of network linearization β
epoch
(b) Real space norm of network linearization β
400000
380000
300000
280000
360000
z-7
⅛ 340000
〈旦
=320000
(a) Fourier space norm of network linearization β
Figure 3: Norms of the linearizations of three different linear architectures for the non-abelian group
G = (C5 × C5) × Q8 trained on a binary classification task with 10 isotropic Gaussian data points.
Figure 4: Norms of the linearizations of three different linear architectures for the non-abelian group
G = (Cr28 × Cf2s) × D8 trained using the digits 1 and 5 from the MNIST dataset.
6.2	Assessment of Theory on nonlinear Networks
Here we introduce rectified linear unit (ReLU) nonlinearities between hidden layers to analyze im-
plicit bias in the presence of nonlinearity. Our theoretical results do not necessarily hold in this
case, so we are exploring, rather than confirming, their validity for nonlinear networks. Accord-
ingly, given a G-CNN with ReLU activations, we wish to calculate the Schatten norm of the Fourier
matrix coefficients of the network linearization β. However, networks can no longer be collapsed
into linear functions due to the nonlinearities. Instead, we construct local linear approximations at
each data point via a first-order Taylor expansion, calculate the norms of interest according to this
linearization, and average the results across the dataset to get a single aggregate value. We evaluate
the implicit bias of an invariant ReLU G-CNN (with final pooling layer) with respect to translations,
7We generically use "CNN” to refer to a G-CNN over the cyclic group of size equal to the size of the input.
8
Under review as a conference paper at ICLR 2022
rotations, and flips on MNIST digits, and a nonlinear G-CNN (with linear final layer) on the dihe-
dral group Dqq with synthetic data. Note that we use linear approximations only to analyze implicit
regularization, and not to further interpret the outputs of the locally linear neural network, as such
analysis can give rise to misleading or fragile feature importance maps (Ghorbani et al., 2019).
Remarkably, as shown in Figures 5a and 5b, our results remain valid in this nonlinear setting. While
this does not guarantee that our implicit bias characterization will hold in more general settings, it
is an encouraging sign that our theoretical results handle the violation of the linearity assumption, at
least numerically. Additional figures detailing the real-space behaviour are provided in Appendix E.
Figure 5: Group Fourier norms for nonlinear architectures with ReLU activations show that non-
linear G-CNNs seem to implicitly regularize locally. Both figures track the mean norm of the per-
sample local linearizations of each network. The networks in Figure 5a use a pooling layer after the
convolutional layers to maintain invariance. Figure 5a evaluates a binary classification task using
the MNIST digits 0 and 5. Figure 5b is obtained on networks with a final linear layer, and evaluates
a binary classification task with 10 isotropic Gaussian data points.
7	Discussion
In this work, we have shown that L-Iayer linear G-CNNs with full width kernels are biased towards
sparse solutions in the Fourier regime regularized by the 2/L-Schatten norm over Fourier matrix
coefficients. Our analysis applies to linear G-CNNs, over finite groups (or infinite groups with
band-limited inputs), trained in the task of binary classification. In advancing our results on implicit
regularization, we highlight some limitations of our work and important future directions:
•	Nonlinearities: Adding nonlinearities to the G-CNNs studied here expands the space of
functions which the G-CNNs can express, but implicit regularization in this nonlinear set-
ting may be challenging to characterize as G-CNNs are no longer linear predictors. Local
analysis may be possible in special cases, e.g., in the infinite width limit (Lee et al., 2017).
•	Bounded width kernels: Our results apply to full-width kernels with support on the whole
space of group operations. Expanding results to bounded-width kernels, i.e., those with
sparse support, is an obvious future direction, though prior work has indicated that closed
form solutions may not exist for these more special cases (Jagadeesan et al., 2021).
•	Different loss function and learning settings: The loss function studied here is the ex-
ponential loss function used in binary classification tasks. Beyond binary classification,
it is an open question how the implicit regularization changes for classification over more
than two classes, not only for G-CNNs but for other re-parametrizations like CNNs and
fully-connected networks as well (Gunasekar et al., 2018b).
Although concise implicit regularization measures are challenging to analyze for realistic, nonlinear
architectures, linear networks provide an instructive case study with precise analytic results. In this
work, by proving that linear group equivariant CNNs trained with gradient descent are regularized
towards low-rank matrices in Fourier space, we hope to advance the broader agenda of understanding
generalization, and in particular how and why networks with diverse architectures — particularly
those with built-in symmetries — learn.
9
Under review as a conference paper at ICLR 2022
References
Brandon Anderson, Truong-Son Hy, and Risi Kondor. Cormorant: Covariant Molecular Neural
Networks. Curran Associates Inc., Red Hook, NY, USA, 2019.
Srinath Bulusu, Matteo Favoni, Andreas Ipp, David I Miiller, and Daniel Schuh. Generalization ca-
pabilities of translationally equivariant neural networks. arXiv preprint arXiv:2103.14686, 2021.
Josue Ortega Caro, Yilong Ju, Ryan Pyle, Sourav Deys Wieland Brendel, Fabio Anselmi, and Ankit
Patel. Local convolutions cause an implicit bias towards high frequency adversarial examples,
2021.
Frank H Clarke. Generalized gradients and applications. Transactions of the American Mathemati-
cal Society, 205:247—262,1975.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of the
33rd International Conference on International Conference on Machine Learning - Volume 48i
ICML,16, pp. 2990-2999. JMLR.org, 2016a.
Taco S Cohen and Max Welling. Group Equivariant Convolutional Networks. arXiv, 2016b.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on ho-
mogeneous spaces. In H. Wallach, H. Larochelle, A. Beygelzimer, E d'Alche-Buc, E. Fox, and
R. Gamett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https : / /proceedings . neurips . cc/pa.per/2019/file/
b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf.
David L Donoho and Philip B Stark. Uncertainty principles and signal recovery. SIAM Journal on
Applied Mathematics, 49(3):906-931, 1989.
David Steven Dummit and Richard M Foote. Abstract Algebra, volume 3. Wiley Hoboken, 2004.
Bryn Elesedy and Sheheryar Zaidi. Provably strict generalisation benefit for equivariant models.
arXiv preprint arXiv:2102,103339 2021.
Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning so(3)
equivariant representations with spherical cnns. In Proceedings of the European Cortference on
Computer Vision (ECCV), September 2018.
Robert Gens and Pedro M Domingos. Deep symmetry networks. Advances in neural information
processing systems, 27:2537-2545, 2014.
Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In
Proceedings of the AAAI Conference on Artificial Intelligence^ volume 33, pp. 3681-3688, 2019.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias
in terms of optimization geometry. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research9pp. 1832-1841. PMLR, 10-15 Jul 2018a. URL http: //proceedings .
mlr.press∕v80∕gunasekarl8a.html.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient de-
scent on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Graumanj
N. Cesa-Bianchi, and R. Gamett (eds.), Advances in Neural Information ProCCSSing Systems,
volume 31. Curran Associates, Inc., 2018b. URL https : //proceedings . neurips . cc∕
paper∕2018∕file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf.
Meena Jagadeesan, Ilya P. Razenshteyn, and Suriya Gunasekar. Inductive bias of multi-channel
linear convolutional networks with bounded weight norm. CoRR, abs/2102.12238, 2021. URL
https ://arxiv.org/abs/2102.12238.
Risi Kondor. A novel set of rotationally and translationally invariant features for images based on
the non-commutative bispectrum. arXivpreprint cs/0701127, 2007.
10
Under review as a conference paper at ICLR 2022
Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups, 2018.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical
convolutional neural network. Advances in Neural Information Processing Systems, 31:10117-
10126, 2018.
Jaeh∞n Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:l711.00165,
2017.
Clare Lylej Mark van der Wilk, Marta Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. On
the benefits of invariance in neural networks. arXiv preprint arXiv:2005.OOl 78, 2020.
Ewa Matusiak, Murad Ozaydin, and Tomasz Przebinda. The donoho-stark uncertainty principle for
a finite abelian group. Acta Math. Univ. Comenianaei 73(2):155-160, 2004.
Roy Meshulam. An uncertainty inequality for groups of order pq. European journal of combina-
torics, 13(5):401^07, 1992.
Fabio Nicola and S. Ivan Trapasso. On the stability of deep convolutional neural networks under
irregular or random deformations, 2021.
Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable
by norms. In H. Larochelle, M. Ranzato9 R. Hadsell, M. E Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21174-21187. Curran As-
sociates, Inc., 2020. URL https : //proceedings , neurips . cc∕paper∕2 02 0∕file/
f21e255f89e0f258accbe4e984eef486-Paper.pdf.
Marco Reisert. Group integration techniques in pattern analysis-a kernel view. 2008.
Akiyoshi Sannai and Masaaki Imaizumi. Improved generalization bound of group invari-
ant/equivariant deep networks via quotient feature space. arXiv preprint arXiv:1910.06552,2019.
Audrey Terras. Fourier analysis on finite groups and applications. Number 43. Cambridge Univer-
sity Press, 1999.
G Alistair Watson. Characterization of the subdifferential of some matrix norms. Linear algebra
and its applications, 170(0):33—45, 1992.
Maurice Weiler and Gabriele Cesa. General E(2)-Equivariant Steerable CNNs. In Conference on
Neural !formation Processing Systems (NeurIPS)9 2019.
Avi Wigderson and Yuval Wigderson. The uncertainty principle: variations on a theme. Bulletin of
the American Mathematical Society, 58(2):225-261, 2021.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive
Approximation, pp. 1-68, 2021.
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training
linear neural networks. CoRR, abs/2010.02501, 2020. URL https : //arxiv.org/abs/
2010.02501.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Bamabas Poczos, Russ R Salakhutdmov, and
Alexander J Smola. Deep sets. Advances in Neural Information Processing Systems, 30, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. 2017. URL https : //arxiv.org/abs/
1611.03530.
11