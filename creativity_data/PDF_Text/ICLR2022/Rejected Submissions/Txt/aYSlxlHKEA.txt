Under review as a conference paper at ICLR 2022
Fully Decentralized Model-based Policy Op-
timization with Networked Agents
Anonymous authors
Paper under double-blind review
Ab stract
Model-based RL is an effective approach for reducing sample complexity. How-
ever, when it comes to multi-agent setting where the number of agent is large,
the model estimation can be problematic due to the exponential increased interac-
tions. In this paper, we propose a decentralized model-based reinforcement learn-
ing algorithm for networked multi-agent systems, where agents are cooperative
and communicate locally with their neighbors. We analyze our algorithm theo-
retically and derive an upper bound of performance discrepancy caused by model
usage, and provide a sufficient condition of monotonic policy improvement. In our
experiments, we compare our algorithm against other strong multi-agent baselines
and demonstrate that our algorithm not only matches the asymptotic performance
of model-free methods but also largely increases its sample efficiency.
1	Introduction
Many real world problems, such as autonomous driving, wireless communications, multi-player
games can be modeled as multi-agent RL problems, where multiple autonomous agents coexist in a
common environment, aiming to maximize its individual or team reward in the long term by inter-
acting with the environment and other agents. Unlike single-agent tasks, multi-agent tasks are more
challenging, due to partial observations and unstable environments when agents update their policies
simultaneously. Therefore, there are hardly any one-fits-all solutions for MARL problems. Exam-
ples include networked systems control (NSC) (Chu et al., 2020), in which agents are connected via
a stationary network. They perform decentralized control based on its local observations and mes-
sages from connected neighbors. Examples of networked systems include connected vehicle control
(Jin & Orosz, 2014), traffic signal control (Chu et al., 2020), etc.
Despite the success of multi-agent reinforcement (RL) algorithms, their performance relies on a
massive amount of model usage. Typically, a multi-agent RL algorithm needs millions of inter-
action with the environment to converge. On the other hand, model-based reinforcement learning
(MBRL) algorithms, which utilize predictive models of the environment to help data collection, are
empirically more data-efficient than model-free approaches. Although model inaccuracy performs
as a bottleneck of policy quality in model-based algorithms, we can still learn a good policy with an
imperfect model (Luo et al., 2019), especially combined with the trick of branched rollout (Janner
et al., 2019) to limit model usage. Experimentally, MuZero (Schrittwieser et al., 2020), a model-
based RL algorithm, succeeded in matching the performance of AlphaZero on Go, chess and shogi,
and becomes state-of-the-art on Atari games. Model-based MARL is not fully investigated. Exist-
ing MB-MARL algorithms either limit their field of research on specific scenario, e.g. two-player
zero-sum Markov game (Zhang et al., 2020) or pursuit evasion game (BoUzy & Metivier, 2θ07),
or use tabular RL method (Bargiacchi et al., 2021). MB-MARL for multi-agent MDPs is still an
open problem to be solved (Zhang et al., 2019), with profound challenges such as scalability issues
caused by large state-action space and incomplete information of other agents’ state or actions.
In this paper, we develop decentralized model-based algorithms on networked systems, where agents
are cooperative, and able to communicate with each other. We use localized models to predict future
states, and use communication to broadcast their predictions. To address the issue of model error,
we adopt branched rollout (Janner et al., 2019) to limit the rollout length of model trajectories. In
the policy optimization part, we use decentralizd PPO (Schulman et al., 2017) with a extended value
function. At last, we analyze these algorithms theoretically to bound the performance discrepancy
1
Under review as a conference paper at ICLR 2022
between our method and its model-free, centralized counterpart. At last, we run these algorithms
in traffic control environments (Chu et al., 2020; Vinitsky et al., 2018) to test the performance of
our algorithm. We show that our algorithm increases sample efficiency, and matches the asymptotic
performance of model-free methods.
In summary, our contributions are three-fold. Firstly, we propose an algorithmic framework,
which is a fully decentralized model-based reinforcement learning algorithm, which is named as
Decentralized Model-based Policy Optimization (DMPO). Secondly, we analyze the theoretical
performance of our algorithm. Lastly, empirical results on traffic control environments demonstrate
the effectiveness of DMPO in reducing sample complexities and achieving similar asymptotic per-
formance of model-free methods.
2	Related Work
Model-based methods are known for their data efficiency (Kaelbling et al., 1996), especially com-
pared with model-free algorithms. There is a vast literature on the theoretical analysis of model-
based reinforcement learning. In a single-agent scenario, monotonic improvement of policy op-
timization has been achieved (Luo et al., 2019; Sun et al., 2018), and a later work improved the
performance of model-based algorithms by limiting model usage (Janner et al., 2019). But these
analysis is restricted to single-agent scenarios, whereas ours addresses multi-agent problems.
On the other hand, Networked System Control (NSC) (Chu et al., 2020) is a challenging setting for
MARL algorithm to take effect. Some multi-agent algorithms falls into centralized training decen-
tralized execution (CTDE) framework. For example, QMIX (Rashid et al., 2018) and COMA (Foer-
ster et al., 2018) all use a centralized critic. In a large network, however, centralized training might
not scale. In many scenarios, only fully decentralized algorithms can be used. Zhang et al. (2018)
proposed an algorithm of NSC that can be proven to converge under linear approximation. Qu et al.
(2020a) proposed truncated policy gradient, to optimize local policies with limited communication.
Baking in the idea of truncated Q-learning in (Qu et al., 2020a), we generalize their algorithm to
deep RL, rather than tabular RL. Factoring environmental transition into marginal transitions can
be seen as factored MDP. Guestrin et al. (2001) used Dynamic Bayesian Network to predict system
transition. Simao & Spaan (2019) proposed a tabular RL algorithm to ensure policy improvement at
each step. However, our algorithm is a deep RL algorithm, enabling better performance in general
tasks.
There are some works on applying model-based methods in MARL settings. A line of research
focuses on model-based RL for two-player games. For example, Brafman & Tennenholtz (2000)
solved single-controller-stochastic games, which is a certain type of two-player zero-sum game;
BoUzy & Metivier (2007) performed MB-MARL in the pursuit evasion game; Zhang et al. (2020)
proved that model-based method can be nearly optimally sample efficient in two-player zero-sum
Markov games. Bargiacchi et al. (2021) extended the concept of prioritized sweeping into a MARL
scenario. However, this is a tabular reinforcement algorithm, thus unable to deal with cases where
state and action spaces are relatively large, or even continuous. In contrast to existing works, our
algorithm is not only applicable to more general multi-agent problems, but is also the first fully
decentralized model-based reinforcement learning algorithm.
3	Problem Setup
In this section, we introduce multi-agent networked MDP and model-based networked system con-
trol.
Networked MDP We consider environments with a graph structure. Specifically, n agents coexist
in an underlying undirected and stationary graph G = (V, E). Agents are represented as a node
in the graph, therefore V = {1, ..., n} is the set of agents. E ⊂ V × V comprises the edges that
represent the connectivity of agents. Agents are able to communicate along the edges with their
neighbors. Let Ni denote every neighbor of agent i, and Ni = Ni ∪{i}. Furthermore, let NK denote
the κ-hop neighborhood of i, i.e. the nodes whose graph distance to i is less than or equal to κ. For
the simplicity of notation, we also define N-κi = V \ Niκ.
2
Under review as a conference paper at ICLR 2022
The corresponding networked MDP is defined as (G, {Si, Ai}i∈V , p, r). Each agent i have their
local state si ∈ Si, and perform action ai ∈ Ai . The global state is the concatenation of all local
states: s = (s1, ..., sn) ∈ S := S1 × ... × Sn. Similarly, the global action is a = (a1, ..., an) ∈ A :=
A1 × ... × An . For the simplicity of notation, we define sNi to be the local states of every agent in
Ni, that is, given Ni = {j1, ..., jc}, then sNi = (sj1 , ..., sjc). aNi , sNκ , aNκ are defined similarly.
The transition function is defined as: p(s0|s, a) : S × A → S. Each agent possess a localized policy
∏θi (a∕sNi) that is parameterized by θi ∈ Θi, meaning the local policy is dependent only on states
of its neighbors and itself. We use θ = (θ1, ..., θn) to denote the tuple of localized policy parameters,
and ∏θ(a|s) = Qn=I ∏θi (a/sNj denote the joint policy. We also assume that reward functions is
only dependent on local state and action: ri(si, ai), and the global reward function is defined to be
the average reward r(s, a) = n Pn=I ri(si, &i).
πθ = arg max η[πθ] = arg max
πθ	πθ
The goal of reinforcement learning is to maximize the expected sum of discounted rewards, denoted
by η :
E∏θ hχ γt ∙1X ri(st,at)i,	⑴
t=0	n i=1
where γ ∈ (0, 1) is the temporal discount factor. We define the stationary distribution under policy
π to be dπ (s).
Independent Networked System Networked system may have some extent of locality, meaning
in some cases, local states and actions do not affect the states of distant agents. In such systems,
environmental transitions can be factorized, and agents are able to maintain local models to predict
future local states. We define Independent Networked System (INS) as follows:
Definition 1. An environment is an Independent Networked System (INS) if:
n
p(s0∣s, a) = ɪɪpi(siIsNi, ai),∀s0, S ∈ S,a ∈A.
i=1
INS might be an assumption that is too strong to hold. However, for the dynamics that cannot be
factorized, we can still use an INS to approximate it. Let DT V denote the total variation distance
between distributions, we have the following definition:
Definition 2. (ξ-dependent) Assume there exists an Independent Networked System P such that
夕(s0∣s,a) = ∩n=ι Pi(si∣sNi, ai). An environment is ξ-dependent, if:
SUp DTV (p(s0∣s,a)kp(s0∣s, a)) = sup 1 X ∣p(s0∣s,a) - p(s0∣s,a)∣ ≤ ξ.
s,a	s,a 2 s0∈S
To explain the intuition behind this definition, we point out that ξ is actually the lower bound of
model error when We use local models p(sN, ai). Recall that p(s0∣s, a) is the real environment
transition, P = Qnn=ι Pi (si I sN, ai) is the product of marginal environment transitions, and p(s, a)=
Qn=ι Pi(si∣sNi, ai) is the product of model transitions. Then the universal model error D(Pkp) can
be divided into two parts: dependency bias D(pkp) and model error D(Pkp):
D(pkp) ≤ D(PkP) + D(DkP).
Then for a ξ-dependent system, when models become very accurate, meaning D(Pkp) ≈ 0,
sup D(Pkp) ≈ sup D(PkP) = ξ. While D can be any appropriate distance metric, We use the
TV-distance hereafter for the ease of presentation. In the following, we develop theory under both
INS and ξ-dependent scenarios.
4 Decentralized Model-based Policy Optimization
In this section, we formally present Decentralized Model-based Policy Optimization (DMPO),
which is a fully decentralized model-based reinforcement learning algorithm. Compared with inde-
pendent multi-agent PPO, DMPO is augmented in three ways: localized model, policy with one-step
communication, and extended value function. We introduce the detail of localized model in 4.1. Pol-
icy and value functions are introduced in 4.2. The illustration of our algorithm is given in Figure
1. All the components mentioned above are analyzed in Section 5. We argue that under certain
conditions, our algorithm ensures monotonic policy improvement.
3
Under review as a conference paper at ICLR 2022
(a) Neighborhood ⑹ Value function
(c) Graph convolutional model
Figure 1: (a) presents the concept of neighborhood. If agent i is the node in purple, then purple and
orange is Ni, and combination of purple, orange and green is Ni. (b) explains that extended value
function takes sN κ as input, here κ = 3. (c) is the illustration of graph convolutional model.
4.1	Decentralized Predictive Model
To perform decentralized model-based learning, we let each agent maintain a localized model. The
localized model can observe the state of 1-hop neighbor and the action of itself, and the goal of a
localized model is to predict the information of the next timestep, including state, reward and done.
This process is denoted by Pi (Si, r0, di ∣s7y. ,ai).
We implement a localized model with graph convolutional networks (GCN). Recall that agents are
situated in a graph G = (V, E). In the first step, a node-level encoder encodes local state into node
embedding,
hi0 = fiencode (si).	(2)
Then we perform one step of graph convolution as follows,
h(i,j)=f(eid,jg)e(hi0,hj0), hi1 = finode( X h(i,j),ai).	(3)
e=(i,j)
In this way, h1 is dependent only on SNi and ai. Finally, a node-level decoder generates the predic-
tion of state, reward and done from hi1 as follows:
S0i = fistate(hi1) + Si, ri0 =fireward(hi1),d0i =fidone(hi1).	(4)
Note that we predict the next state with a skip connection, because empirically, it’s more efficient to
predict the change of the state rather than the state itself.
In practice, the data are all stored locally by each agent. Data that are collected in the environment
by agent i is denoted as Dienv , and those generated by predictive model is denoted as Dimodel .
Scaling model-based methods into real tasks can result in decreased performance, even if the model
is relatively accurate. One main reason is the compound modeling error when long model rollouts
are used, and model error compound along the rollout trajectory, making the trajectory ultimately in-
accurate. To reduce the negative effect of model error, we adopt a branched rollout scheme proposed
in (Janner et al., 2019). In branched rollout, model rollout starts not from an initial state, but from a
state that was randomly selected from the most recent environmental trajectory τ . Additionally, the
model rollout length is fixed to be T . This scheme is shown to be effective in reducing the negative
influence of model error both theoretically and empirically.
To deal with the bias of model trajectories, at each model rollout, we allow the algorithm to fall back
to the real trajectory with probability 1 - q0 , where q0 is a hyperparameter. We describe the detailed
framework of model usage and experiment storage in Algorithm 1.
4.2	Proximal Policy Optimization with Extended Value Function
To optimize the policies, we need to adopt an algorithm that can exploit network structure, whilst
remaining decentralized. Independent RL algorithms that observes only local state are fully decen-
tralized, but they often fail to learn an optimal policy. Centralized algorithms that utilize centralized
4
Under review as a conference paper at ICLR 2022
Algorithm 1: Decentralized Model-based Policy Optimization (DMPO) for MARL
Input: hyperparameters: rollout length T, truncation radius K
1:	Initialize the model piψi , actor πiθi and critic Viφi .
2:	Initialize replay buffers Dienv and Dimodel .
3:	for M iterations do
4:	Perform environmental rollout together, and each agent i collect trajectory information τi .
5:	for i in N agents do
6:	Dienv = Dienv ∪ {τi }.
7:	Train piψi on Dienv .
8:	DmOda = 0.
9:	for B inner iterations: do
10:	Generate a random number q 〜 U(0,1).
11:	if q > q0 then
12:	DimOdel = τi. {Fall back to real trajectory with probability 1 - q0.}
13:	else
14:	for R rollouts, s ∈ τ do
15:	Perform T-step model rollout starting from S using policy pψ*, append to Dmodel.
16:	for G steps, i = 0, ..., n - 1 do
17:	Take a step along the gradient to update ∏θi and critic Viφi on Dmodel
critics often achieve better performance than decentralized algorithms, but they might not scale to
large environments where communication costs are expensive.
We propose Proximal Policy Optimization with extended value function, which is defined as
Vi(SNκ) = EsNK 〜dπ[P∞=o rt∣sNK = SNK],i ∈ V. The intuition behind extended value func-
tion comes from (Qu et al., 2020a), where truncated Q-function Q(sNκ , aNκ ) is initially proposed.
In 5.3, we prove that Vi(SNK) is a good approximation of Vi(S), with a difference decreasing expo-
nentially with κ.
To generate the objective for extended value function, or return Ri, we use reward-to-go technique.
However, because model rollout is short, standard reward-to-go returns would get a biased estima-
tion of Vi. To resolve this issue, we add the value estimation of the last state to the return. In this
way, with a local trajectory τi = {(Sit, ait, rit, (S0)it, dit, logπit), t = 0, 1, ..., T - 1}, the objective of
Vit(SNiK) is
T -t-1
Rit = X γlrit+l +Viφi(S0)TN-iK1,	(5)
l=0
and the loss of value function is defined as Lvalue = -1 PmæDmodei ∖Vφi (SmK) 一 Rm] 2. In policy
training, extended value functions Vi are reduced via communication to their κ-hop neighbors to
generate an estimation of global value function,
Vt = I X Vt,
nj
j∈NiK
(6)
and advantages Ai are computed on Vi with generalized advantage estimation (GAE) (Schulman
et al., 2015) for policy gradient update. The surrogate loss function of a DMPO agent is defined as
LOoiCy = m X min Π ∏θ kX Ai(SVK ),g(e,Ai(SVK))),	⑺
m∈Dmodel	'πi" (ai|SNi)	)
similar to PPO-Clip loss.
The communication of κ step might seem costly, yet information of Niκ is only used in the training
phase. We argue that in the training phase, algorithms are less sensitive with latency than execu-
tion. Furthermore, since model-based learning can effectively increase sample efficiency, we might
tolerate more communication.
5
Under review as a conference paper at ICLR 2022
5	Theoretical Analysis
In this section, we analyze DMPO theoretically. In 5.2, we derive a bound between the true returns
and the returns under a model P ina networked system. In 5.3, We prove that extended value function
Vi(sNκ) is a good approximation of Vi(s), and with extended value function, the true policy gradient
can also be approximated.
5.1	Background: Monotonic Model-based Policy Optimization
Let η[∏] denote the returns of the policy in the true environment, η[∏] denote the returns of the
policy under the approximated model. To analyze the difference between η[∏] and η[∏], we need to
construct a bound
ηp[π] ≥ ηp[π] - C(p,p,π,πD),	⑻
where C is a non-negative function, and πD is the data-collecting policy. According to equation 8,
if every policy update ensures an improvement of η[∏] by at least C, η[∏] will improve monotoni-
cally. This inequality was first presented in single agent domain (Janner et al., 2019). In this work,
we extend this to the multi-agent networked system, aiming to achieve monotonic team reward im-
provement.
In this work, we let ∏ indicate a collective policy ∏ = [∏ι,…,∏n], and the model P be an INS
P(S0|s, a) = Qn=ιPi(s]sNi, ai) that approximating the true MDP. In DMPO, each agent learns a
localized model ∏i, policy ∏i(|SNk), critic Vi(SNK), making it never a trivial extension. We give the
detailed analysis in 5.2.
5.2	Analysis of Returns B ound
In model-based learning, different rollout schemes can be chosen. The vanilla rollout assumes
that models are used in an infinite horizon. The branched rollout performs a rollout from a state
sampled by a state distribution of previous policy ∏d, and runs T steps in ∏ according to ∏. Based
on different rollout schemes, we can construct two lower bounds. Under vanilla rollout, real return
and model return can be bounded by model error and policy divergence. Formal results are presented
in Theorem 1. The detailed proof is deferred to Appendix C.
Theorem 1.	Consider an independent networked system. Denote local model errors as mi =
maXsN ,ai DTV[Pi(si∣SNi, 0i)kPi(si∣SNi, ai)], and divergences between the data-Colkcting policy
and evaluated policy as Eni = maxsj7 DTV [∏d (ai∣SNi)k∏(ai∣SNi)] ∙ Assume the upper bound of
rewards of all agents is rmax. Let ηp[π1, ..., πn] denote the real returns in the environment. Also, let
ηp[∏ι,..., πn] denote the returns estimated in the model trajectories, and the states and actions are
collected with πD. Then we have:
I pr	1 p[	1∣	2rmax 弋 Γ e∏i	(	∞	∞ S k+1 |Ni1]
[∏p [π1, ...,πn ] - η [π1, ..., πn]1 ≤ 1-- ʌ,	+ (Cmi + 2e∏i ) ∙ ʌ, γ -J ∙
γ i=1 n	k=0	n
Intuitively, the term P∞= 0 γk+1 INP would be in the same magnitude as ɪ--Y, which might be huge
given the choice of γ, making the bound too loose to be effective. To make tighter the discrepancy
bound in Theorem 1, we adopt the branched rollout scheme. The branched rollout enables a effec-
tive combination of model-based and model-free rollouts. For each rollout, we begin from a state
sample from d∏D, and run T steps in each localized ∏i. When branched rollout is applied in an INS,
Theorem 2 gives the returns bound.
Theorem 2.	Consider an independent networked system. Denote local model errors as Cmi =
maXsN ,ai DTV[Pi(si∣SNi, 0i)kPi(si∣SNi, ai)], and divergences between the data-Collecting policy
and evaluated policy as Cni = maxsj7 DTV [∏d (ai∣SNi)k∏(ai∣SNi)] ∙ Assume the upper bound of
rewards of all agents is rmax. Let ηp[π1, ..., πn] denote the real returns in the environment. Also, let
ηbranch [π1, ..., πn] denote the returns estimated via T -step branched rollout scheme. Then we have:
∣ηP[∏ι,…,∏n]-ηb…[∏ι,…,∏n]∣≤ 汇 XX [j X-1 Y k+1 少)+/(£ γk+1 少)]
γ i=1	k=0	n	k=T	n
6
Under review as a conference paper at ICLR 2022
Comparing the results in Theorem 1 and 2, we can see that branched rollout scheme reduced the co-
efficient before £mi from P∞=ο γk+1 INP ≤ τ-γγ to PT-O Yk+1 1NP ≤ PT=O1 Yk+1 = Y(I=T).
This reduction explains that empirically, branched rollout brings better asymptotic performance.
Also, if we set T = 0, this bound turn into a model-free bound. This indicates that when mi is
lower than πi allowed by our algorithm, a model might increase the performance.
In reality, not every system satisfies the definition of INS. Yet we can generalize Theorem 2 into a
ξ-dependent system.
Corollary 1. Consider an ξ-dependent networked system. Denote local model errors as mi =
maXsN ,ai DTv[pi(si∣sα, ai)kPi(siIsNi, ai)], and divergences between the data-collecting policy
and evaluated policy as Eni = maxsj7 DTV [∏d (。小电)|忻(。/£此)]∙ Assume the upper bound of
rewards of all agents is rmax. Let ηp[π1, ..., πn] denote the real returns in the environment. Also, let
ηbranch [π1, ..., πn] denote the returns estimated via T -step branched rollout scheme. Then we have:
Iηp[π1,..., πn] - ηbranch[π1,..., πn]I
n
2rmaxγ
≤ (Tfξ+
2rmax
1-γ
i=1
k=O
T-1
X K (X Yk
i
∞
(X
k=T
The proof can also be found in Appendix C. Compared to Theorem 2, Corollary 1 is more general, as
it is applicable to the multi-agent systems that are not fully independent. Intuitively, if a networked
system seems nearly independent, local models will be effective enough. The bound indicates that
when the policy in optimized in a trust region where D(π, πD) ≤ Eπi, the bound would also be
restricted, making monotonic update more achievable.
5.3 Extended Value Function
In this section, we analyze the effect of extended value function. The idea of extended value function
Vi(sNκ) comes from truncated Q-function Qi(sNκ, aNκ) proposed in (Qu et al., 2020a). We prove
that extended value function is an approximation of the real value function. The detailed proof of
Theorem 3 is deferred to Appendix C.
Theorem 3.	Define Vi(sNK) = Es	〜dπ[P∞=o rt∣sNK = sNK], and ¼(s) = E[P∞=o rt∣s0 = s],
i	-i	=	i	i	=
then:
r
∣¾(s) - %(sNiK )∣≤ τmaγγκ.
From Theorem 3, it is straightforward that the global value function can be approximated with
the average of all extended value functions: ∣V(s) - n P2ι Vi(sNK)∣ ≤ rjm-aχγκ. In ΡΡO, value
functions are used for calculating advantages A(t) = r(t) + γV(s(t+1)) - V(s(t)), and We have
proven that V(s) can be estimated with the average of extended value functions ɪ P2ι Vi(sNK).
In practice, an agent might not get the value function of distant agents. HoWever, We can prove
that Vi = n Ej∈nκ Vj(sNκ) is already very accurate for calculating the policy gradient for agent
i. Theorem 4 justifies that the policy gradients computed based on the sum of the nearby extended
value functions is a close approximation of true policy gradients.
Theorem 4. Let At = r(t) +γV(s(t+1))-V(s(t)) be the TD residual, andgi = E[AVθJog∏i(a∣s)]
be the policy gradient. If At and gi are the TD residual and policy gradient when value junction
1
V(s) is replaced by Vi(s) = n 5Σj∈NK Vj (sNK), we have:
Yκ-1	2 Niκ
|gi	9i∖ ≤ λ [1	(I Y )	]rmaxgmax,
1- Y	n
where rmax and gmax denote the upper bound of the absolute value of reward and gradient, respec-
tively.
7
Under review as a conference paper at ICLR 2022
6 Experiments
6.1	Environments
We test our algorithm in four environments, namely Figure Eight, Ring Attenuation (Wu et al.,
2017a), CACC Catchup, and CACC Slowdown (Chu et al., 2020). Detailed description and visual-
ization of these environments is deferred to Appendix A.
Cooperative Adaptive Cruise Control The objective of CACC is to adaptively coordinate a pla-
toon of 8 vehicles to minimize the car-following headway and speed perturbations based on real-time
vehicle-to-vehicle communication. CACC consists of two scenarios: Catch-up and Slow-down. In
CACC Catch-up, vehicles need to catch up to the first car. In CACC Slow-down, every vehicle is
faster than the optimal speed, and they need to slow down without causing any collision. The agents
receives a negative reward if the headway or the speed is not optimal. Also, whenever a collision
happens, a huge negative reward of -1000 is given.
Flow environments This task consists of Figure Eight and Ring Attenuation. The objective of
these environments is letting the automated vehicles achieve a target average speed inside the road
network while avoiding collisions. The state of each vehicle is its velocity and position, and the
action is the acceleration of itself. In Ring Attenuation, the objective is to achieve a high speed,
while avoiding stop-and-go loops. Vehicles are rewarded with their speed, but also punished for their
accelerations. In the perspective of a networked system, we assume that the vehicles are connected
with the preceding and succeeding vehicle, thus resulting in a loop-structured graph.
6.2	Baselines
We describe the following algorithms for performance comparison:
•	CPPO: Centralized PPO learns a centralized critic Vi (s). This baseline aims to analyze the
performance when κ is set to be arbitrarily huge, and is used in (Vinitsky et al., 2018) as a
benchmark algorithm for networked system control.
•	IC3Net (Singh et al., 2018): A communication-based multi-agent RL algorithm. The agents
maintain their local hidden states with a LSTM kernel, and actively determines the com-
munication target. Compared with DPPO, IC3Net uses hidden state and continuous com-
munication, whereas DPPO agents directly observe the states of their neighbors.
•	DPPO: Decentralized PPO learns an independent actor and critic for each agent. We im-
plement it by using neighbor’s state for extended value estimation.
•	DMPO (our method): DMPO is a decentralized and model-based algorithm based on
DPPO. On top of it, we use decentralized graph convolutional kernel as predictive model.
6.3	Results
Figure 2 shows the episode reward v.s. number of training samples curves of the algorithms. We
address that in CACC environments, DMPO uses decentralized SAC as base algorithm. Similar with
DPPO, decentralized SAC uses extended Q-function Qi (sNκ , aNκ) for its policy gradient. From the
results, we conclude that our algorithm matches the asymptotic performance of model-free methods.
It also learns the policy faster, resulting in increased sample efficiency.
The comparison between DMPO and DPPO can be viewed as an ablation study of model usage. In
figure eight, DMPO increases sample efficiency at the beginning, but as the task becomes difficult,
the sample efficiency of our method decreased. In a relatively easy task, ring attenuation, our method
increased sample efficiency massively, compared with its model-free counterpart.
The comparison between the asymptotic performance of CPPO and DMPO or DPPO can be viewed
as an ablation study of extended value function. From the result in four environments, we ob-
serve that the asymptotic performance of CPPO does not exceed that of the algorithms that uses
extended value function. In this way, we conclude that by using extended value function, a cen-
tralized algorithm can be decomposed into decentralized algorithm, but the performance would not
drop significantly.
8
Under review as a conference paper at ICLR 2022
(a) Figure Eight
Figure 3 shows the accuracy of our model in predicting the reward and state during training. The er-
ror is defined as the ratio of MSE loss to variance. From the figures, we conclude that neighborhood
information is accurate enough for a model to predict the next state in these environments. However,
in CACC Slow-down, local models might fail to learn the reward. We observe that the errors may
increase as the agents explore new regions in the state space.
(c) CACC Catch-up
(b) Ring Attenuation
(d) CACC Slow-down
Figure 2: Training curves on multi-agent environments. Solid curves depict the mean of five trails,
and shaded region correspond to standard deviation.
(a) State Error	(b) Reward Error
Figure 3: Figures of state and reward error. Both state error and reward error < 10% in every
environment.
7 Conclusions
In this paper, We propose algorithm DMPO, a model-based and decentralized multi-agent RL algo-
rithm. We then give a theoretical analysis on the algorithm to analyze its performance discrepancy,
compared with a model-free algorithm. By experiments in several tasks in networked systems,
We show that although our algorithm is decentralized and model-based, it matches the asymptotic
performance of some state-of-art multi-agent algorithms. From the results, we also conclude that
using extended value function instead of centralized value function did not sacrifice performance
massively, yet it makes our algorithm scalable.
9
Under review as a conference paper at ICLR 2022
References
Masako Bando, Katsuya Hasebe, Akihiro Nakayama, Akihiro Shibata, and Yuki Sugiyama. Dy-
namical model of traffic congestion and numerical simulation. Physical review E, 51(2):1035,
1995.
Eugenio Bargiacchi, Timothy Verstraeten, and Diederik M. Roijers. Cooperative prioritized sweep-
ing. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021),
pp. 160-168. IFAAMAS, 2021.
BrUno BoUzy and Marc Metivier. Multi-agent model-based reinforcement learning experiments in
the pursuit evasion game. 2007.
Ronen I Brafman and Moshe Tennenholtz. A near-optimal polynomial time algorithm for learning
in certain classes of stochastic games. Artificial Intelligence, 121(1-2):31-47, 2000.
TianshU ChU, Sandeep Chinchali, and Sachin Katti. MUlti-agent reinforcement learning for net-
worked system control. In International Conference on Learning Representations (ICLR), 2020.
URL https://openreview.net/forum?id=Syx7A3NFvH.
Jakob Foerster, Gregory FarqUhar, Triantafyllos AfoUras, Nantas Nardelli, and Shimon Whiteson.
CoUnterfactUal mUlti-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volUme 32, 2018.
Carlos GUestrin, Daphne Koller, and Ronald Parr. MUltiagent planning with factored mdps. In
Advances in Neural Information Processing Systems (NeurIPS), volUme 1, pp. 1523-1530, 2001.
Michael Janner, JUstin FU, Marvin Zhang, and Sergey Levine. When to trUst yoUr model: Model-
based policy optimization. In Advances in Neural Information Processing Systems (NeurIPS),
2019.
I Ge Jin and Gabor Orosz. Dynamics of connected vehicle systems with delayed acceleration feed-
back. Transportation Research Part C: Emerging Technologies, 46:46-64, 2014.
Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A
sUrvey. Journal of artificial intelligence research, 4:237-285, 1996.
YUping LUo, HUazhe XU, YUanzhi Li, YUandong Tian, Trevor Darrell, and TengyU Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical gUarantees. In Interna-
tional Conference on Learning Representations (ICLR), 2019.
GUannan QU, Yiheng Lin, Adam Wierman, and Na Li. Scalable mUlti-agent reinforcement learning
for networked systems with average reward. Advances in Neural Information Processing Systems
(NeurIPS), 33, 2020a.
GUannan QU, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for
mUlti-agent networked systems. In Learning for Dynamics and Control (L4DC), pp. 256-266.
PMLR, 2020b.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory FarqUhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic valUe fUnction factorisation for deep mUlti-agent reinforce-
ment learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
JUlian Schrittwieser, Ioannis AntonogloU, Thomas HUbert, Karen Simonyan, LaUrent Sifre, Simon
Schmitt, ArthUr GUez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.
John SchUlman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continUoUs control Using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015.
John SchUlman, Filip Wolski, PrafUlla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
10
Under review as a conference paper at ICLR 2022
Thiago D Simao and Matthijs TJ Spaan. Safe policy improvement with baseline bootstrapping in
factored environments. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
volume 33,pp. 4967-4974, 2019.
Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale
in multiagent cooperative and competitive tasks. arXiv preprint arXiv:1812.09755, 2018.
Wen Sun, Geoffrey J Gordon, Byron Boots, and J Bagnell. Dual policy iteration. Advances in Neural
Information Processing Systems (NeurIPS), 31:7059-7069, 2018.
Eugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Cathy Wu,
Fangyu Wu, Richard Liaw, Eric Liang, and Alexandre M Bayen. Benchmarks for reinforce-
ment learning in mixed-autonomy traffic. In Conference on robot learning, pp. 399-409. PMLR,
2018.
Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, and Alexandre M Bayen. Flow:
Architecture and benchmarking for reinforcement learning in traffic control. arXiv preprint
arXiv:1710.05465, 10, 2017a.
Cathy Wu, Aboudy Kreidieh, Eugene Vinitsky, and Alexandre M Bayen. Emergent behaviors in
mixed-autonomy traffic. In Conference on Robot Learning, pp. 398-407. PMLR, 2017b.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning (ICML), pp. 5872-5881, 2018.
Kaiqing Zhang, ZhUoran Yang, and Tamer Bayar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms, 2019.
Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. Advances in Neural Information Processing
Systems (NeurIPS), 33, 2020.
11
Under review as a conference paper at ICLR 2022
A Experiment Details
The code of our algorithm can be found at https://anonymous.4open.science/r/
RL-algorithms-0E72.
A.1 Environment Description
The objective of CACC is to adaptively coordinate a line of vehicles to minimize the car-following
headway and speed perturbations based on real-time vehicle-to-vehicle communication. We conduct
experiments on two scenarios: CACC catch-up, CACC slow-down. The local observation of each
agent consists of headway h, velocity v, acceleration a , and is shared to neighbors within two steps.
The action of each agent is to choose appropriate hyper-parameters (a^,β^) for each OVM con-
troller Bando et al. (1995), selected from four levels {(0,0),(0.5,0),(0,0.5),(0.5,0.5)}, where a°,β°
denotes headway gain and relative gain for OVM controller respectively. The reward function is
defined as (h%,t - h*)2 + (vi,t - Vt)2 + 0.1a∣ t to punish the gap between the current state to target
state and speed perturbations, where the target headway and velocity profile are ht = 20m and vj,
respectively. Whenever a collision happens (hi,t < 1m), a large penalty of -1000 is assigned to each
agent and the state becomes absorbing. An additional cost 5 (2hst - hi,t)2+ is provided in training
for potential collisions. In catch-up scenario, initial headway of the first vehicle is larger than the
target headway, thus the following agents learn how to catch up with the first vehicle, where target
speed vtt = 15m/s and initial headway h1,0 > hi,0, ∀i 6= 1. In slow-down scenario, target speed vtt
linearly decreases to 15m/s during the first 30s and then stays at constant, thus agents learn how to
slow down speed cooperatively,where initial headway hi,0 = ht .
Flow environments consists of Figure Eight and Ring Attenuation. The objective of these environ-
ments is letting the automated vehicles achieve a target average speed inside the road network while
avoiding collisions.
The figure eight network, previously presented in (Wu et al., 2017b), acts as a closed representation
ofan intersection. In a figure eight network containing a total of 14 vehicles, we witness the forma-
tion of queues resulting from vehicles arriving simultaneously at the intersection and slowing down
to obey right-of-way rules. This behavior significantly reduces the average speed of vehicles in the
network. The state consists of velocity and position for the vehicle. The action is the acceleration
of the vehicle a ∈ R[amin,amax]. The objective of the learning agent is to achieve high speeds while
penalizing collisions. Accordingly, the local reward function is defined as ri = vdes - |vdes - vi |,
where vdes is an arbitrary target velocity.
In Ring Attenuation, the objective is to achieve a high speed, while avoiding acceleration-
deceleration loops. To achieve this, vehicles are rewarded with their speed and punished for their
accelerations. The state and action of each vehicle is the same as Figure Eight. In the perspective of
a networked system, we assume that the vehicles are connected with the preceding and succeeding
vehicle, thus resulting in a loop-structured graph.
Figure 4: Visualization of CACC and Flow environments. (a) A line of vehicles that need to keep
a stable velocity and desired headway. (b) Vehicles travel in a figure eight shaped road section to
learn the behavior at an intersection .(c) Vehicles travel in a ring to reduce stop-and-go waves.
12
Under review as a conference paper at ICLR 2022
B Hyperparameters
We list some of the key hyperparameters for DMPO and DPPO in Table 1 and 2.
Scenario HyperParameter一…―	Catch Up	Slow Down	Figure Eight	Ring Attenuation
Learning rate of critic	-3e-4-	3e-4	5e-5	5e-4
Learning rate of ∏	-3e-4-	3e-4	5e-5	5e-4
Learning rate of Model	3e-4	3e-4 一	5e-4	5e-4
CHp	-	-	0.15	02
GAE λ	1	1	一	0.5	0.5
KL target	-	-	7.5e-3	0.01
Model usage probability	1	1	0.5	05
Rollout Length	1	1	25	25
K	1	1	一	3	3
Table 1: Hyperparameters for DMPO.
^	———ScenariO HyPerParameter''''~一…—	Catch Up	Slow Down	Figure Eight	Ring Attenuation
Learning rate of V	-7e-4-	5e-4	5e-4	1e-3
Learning rate of ∏	-5e-5-	5e-5	5e-5	1e-3
CnP	02	02	0.2	02
GAE λ	0.5	03	0.5	05
KL target	-0:01-	0.01	0.01	0.01
κ	2	2	一	3	3
Table 2: Hyperparameters for DPPO.
C Proof of Theorems
C.1 Remarks on Proofs of Lemmas and Theorems
To bound the returns, we use TV distance to assess the distance between si , ai at every timestep,
based on the fact that ∖Esiai^pιr(si, aQ - Esiai^p2r(si, ai)| ≤ rmax ∙ DTV(p1∣∣P2). To achieve
this, we argue that in an INS, siT only depends on sTN-1 1, which is the 1-hop neighbors’ states at
previous timestep. Then, inductively, siT is only dependent on sTN-1 1, sTN-2 2, ..., sTN-k k, and ultimately,
s0NT . Then we consider a chain of states xt = sNN-t, t = 0, 1, ..., N in Lemma 4 to analyze how
sit depends on model errors and policy divergences in a step-by-step manner. Insipred by Janner
et al., by summing the divergence at each timestep t = 0, 1, ..., we can generate a overall discounted
returns bound of branched rollout in Lemma 5. Lemma 6 is a simple corollary of Lemma 5, which
handles normal rollout scenario.
If the returns are bounded, we can show that if every optimization step increase the return by C,
the real return would also almost monotonically increase. Specifically, if η[∏k+1] ≥ η[∏k] + C,
combined with η[∏k+1] ≥ η[∏k] - C, We have η[∏k+1] ≥ η[∏k]. But note that η[∏k] increases
monotonically, and it,s a lower bound of η[∏k+1]. This means that η[∏k+1] would almost monoton-
ically increase.
C.2 Proof of Theorem 1
Proof ηp[∏ι,…,∏n] and ηp[∏ι,…,∏n] differs in two ways. First, they are estimated with different
transition dynamics. Second, the latter sample their states and actions with another policy, namely
πD. To deal with these divergences separately, we use an intermediate return η[πD], which is the
13
Under review as a conference paper at ICLR 2022
return of πD in the environment. Then the difference can be bounded by:
lηp[π1,…，πn] - ηp[π1,…，πn]1 ≤ lηp[π1,…，πn] - ηp[πD]| + lηp[πD] - ηp[π1, ∙∙∙,πn]1 .⑼
'-------------------------------V----------}	'-------V--------}
L1	L2
To bound L1, we apply Lemma 6 with mi = 0. Then:
2∞
Li ≤ 2rmx &i + X Yk+1 X e∏j].	(⑼
γ	k=0	j∈Nik
In L2, both the dynamics and policies and different, therefore:
2r	∞
L2 ≤ J - Y [e∏i + X Y + X (Emj + Enj)].	(II)
γ	k=0	j∈Nik
Putting Equation 10 and 11 yields:
2r	∞
lηp[π1,∙∙∙,πn] - ηp[π1, ...,πn]1 ≤ ] - [2Eni + EYk+ E (Emj + 2Enj )] .	(12)
Y	k=0	j∈Nk
And because the binary relation of k-hop neighbor is symmetric, we have:
I	V IVjI
nX X Ej = T -n-j
i=1 j∈Nik j=1
(13)
Then, by averaging Equation 12, we have:
Iηp[∏1, ...,∏n] - ηp[∏i, ...,∏n]∣
1n p	p
≤ n	|ni[πι, ...,πn] -ηi[ni, …,nn]|
n∞	n
≤ > n X Eni + X γk+i 蔡 X jXk (Emj +2G	(14)
= 2rmx [ n X Eni + X Yk+1 X ≡ (Emi +2e∏J]
Y	i=1	k=0	i=1
= 2rmx X [亍 + (Emi +2Eni) ∙ X Y^ 斗]
Y i=1	k=0
□
C.3 Proof of Theorem 2
Proof. To bound Iηp[π1, ..., πn] - ηbranch [π1, ..., πn]I, we need to analyze how do they differ from
each other. ηp denote the real returns of these policies in the environment. ηbranch is the returns esti-
mated in the branched rollout scheme. To explicitly illustrate this point, we describe their difference
in Table 3:
By Lemma 5, we have:
2	T-1	∞
∣ηP[∏1,…，∏n]-ηbranch[∏1,…，∏n]∣≤ 2Jrmx 乏 γk+1 X Em- X γk+1 X E%] (15)
Y k=0	j∈Nik	k=T	j∈Nik
14
Under review as a conference paper at ICLR 2022
branch point	before		after	
	dynamics	policies	dynamics	policies
ηp	P	π	P	π
ηbranch	P	∏D	P	π
Table 3: The difference between ηp and ηbranch
And because the binary relation of k-hop neighbor is symmetric, we have:
1 XXX L XX 吧 j
n	j	nj
i=1 j∈Nik	j=1
(16)
Then, by averaging Equation 15, we have:
∣ηp[∏ι,…，∏n] - ηbranch[∏ι,…，∏n]∣
1n
≤ -	∣ηp[∏ι, ...,∏n] -ηbranch[∏ι,…,∏n]l
ni=1
2	T-1	1 n
≤ ⅛ [ Yk + 1∙ 1 XX Smj
k=0	i=1 j ∈N k
∞n
+ X Yk+1∙ 1ΣΣ S∏j]
k=T	i=1 j∈Nik
(17)
2rmax Γ T 1γk + 1∙ XX INkI (S
1 - γ	n
k=0	i=1
+ XX Yk+i∙ XX ≡e∏i
ni
k=T	i=1
2r
max
1 - γ
n	T -1
X [Sm, ∙ (X Yk+1
=1	k=0
mi
+ Sni •
∞
(X
k=T
γk+1 lNkl)]
n
□
C.4 Proof of Corollary 1
Proof. Although p(s0∣s, a) might not satisfy the criteria of an INS, we can construct another
transition dynamic P = QZiPi(si∣sN,,ai), which is the product of marginal transitions, thus
being an INS. Recall that by definition, if p is a transition dynamic of a ξ-dependent system,
sups,a DTV(p(s0∣s, a)kP(s0∣s, a)) ≤ ξ. Then We divide the difference into two parts:
ηp[π1,...,πn] - ηbranch[π1,..., πn]
=ηp[∏ι, ∙∙∙,∏n] - ηp[∏ι, ∙∙∙,∏n] + ηp[∏ι,…，∏n] - ηbranch[∏ι, ∙∙∙,∏n]	(18)
'------------{z-----------} '-----------------V----------------}
L1	L2
The first part is the difference of policy returns, estimated in two environments. Since there are no
policy divergences, and the transition dynamics only differs in a global manner, utilizing Lemma 2
yields that:
DTV (pt(s,a)kPt(s, a)) ≤ tξ∙	(19)
Then, we have:
Li = ηp[∏1,∙∙∙,∏n] - ηp[∏1,∙∙∙,∏n]
∞
≤ 2rmaχ X YtDTV (pt(s, a)kpt(s,a))
t=0
∞	(20)
≤ 2rmax	Yttξ
t=0
=2rmaχ	Y ξ
1 — γ 1 — γ
15
Under review as a conference paper at ICLR 2022
As for L2, because P is an INS, We can directly apply Theorem 2:
L ≤	1-aχ	XX	[emi	∙	( X Yk+1 乎)+	e∏i	∙	(XX γk+1 号)]	(21)
γi=1	k=0	k=T
Summing Equation 20 and Equation 21 completes the proof.	□
C.5 Proof of Theorem 3
Proof. In (Qu et al., 2020b), it Was proven that if sNκ and aNκ are fixed, then no matter hoW other
states and actions changes, Q-function Will not change significantly:
∣Q(snk , aNκ, SN-κi, aN-κj — Q(SNK, aNκ
sNκ
, a0Nκ )∣ ≤
rmax γK
1 -γ
(22)
i	i
As value function is the expectation of Q-function
Vi(s) = Ea 〜∏ Q(s,a)
Vi(SNK ) = Ea〜πQ(SNκ , aNκ ),
We have,
|Vi(S)- Vi(SNK )| = IEa 〜π Q(S, a) — Ea 〜π Q(SNK , aNκ )|
≤ Ea〜π |Q(s, a) — Q(SNK , aNK ) 1
≤ rmaX γK
-1-γ ,
Which concludes the proof.
(23)
(24)
□
C.6 Proof of Theorem 4
Proof. The difference of the gradients can be Written as
gi - gi =E(A - A)Vθi log∏i(ai∣SNi)
=nE[ X Aj]vθi log ni(ai|SNi) + £E[ X (Aj- Aj )]vθi log ni(ai|STVi )
n j∈NK	U j∈NK
=1 E[ X Crj(Sj,aj) + YVj(SO)- Vj(S))]vθi logni(ai|SNi)
j∈NK
+1E X [(rj(Sj,aj) + YVj(S0)- Vj(Sy)- (r (Sj,aj) + YVj(SNK) - Vj(SNK))]vθi logπ(ai|SNi)
n j∈NiK
=L1 + L2.
(25)
Because for any function b(S) that depends only on s, E[b(S) log πθi (ai| S2Ni)] = 0. Therefore, L2 in
Equation 25 becomes:
lL2l ≤1E X Y IVj (s0) - Vj IsNK )]Hvθi log ∏ (adsNi)1
j∈NiK
|N κ | Yκ+1
≤3 Y—rmaχgmaχ.
n 1 - Y
(26)
For L1, note thatrj(Sj,aj) + YVj(S0) - Vj(S) = -Vj(S) +rj(Sj,aj) + Ptκ=-12EYtrj(Stj,atj) +
Yκ-1Vj(Sκ-1). And in an INS, Stj, atj, t = 0, 1, ..., κ - 2 is not affected by policy πi ifj ∈/ Niκ, We
|L1| ≤ 1E X ∣Yκ-1Vj(SKT)∣∣Vθi log∏i(ai∣SNi)∣
j∈NK
Niκ Yκ-1
≤(1 -	) ~∖	rmaxgmaχ.
n 1 - Y
(27)
16
Under review as a conference paper at ICLR 2022
Put Equation 26 and 27 together, we have
|gi - gi| ≤lL1l + |L2|
≤NKI γκ+1
n
γκ-1
1 - Y
1-γ
Niκ γκ-1
rmaxgmax + (I -	) ~∖	rmaxgmax
n 1-γ
Nκ
[1 - (I - Y )-MrmaXgmax.
n
(28)
□
D Useful Lemmas
Lemma 1. (TVD of Joint Distribution) Consider two distributions p1(x, y) = p1 (x)p1 (y|x) and
p2 (x, y) = p2 (x)p2 (y|x). The total variation distance between them can be bounded as:
DTV(p1(x)y)∣∣P2(x,y)) ≤ DTV(p1(x)∣∣P2(x)) + Ex〜p2 [Dtv(p1(y∣x)∣∣P2(y∣x))]
≤ DTVp1(x)kp2(x) + maxDTVp1(y|x)kp2(y |x)
Proof.
DTV(PI(X,y)l∣p2(X,y)) = 2 X IpI(X,y) -P2(X,y)|
x,y
=2 E IpI(X)pi(y|x) -P2(x)P2(y|x)|
x,y
=2 E ∣pι(X)pι(y∣X) -p2(X)p1(y∣X) + p2(X)p1(y∣X) -p2(X)p2(y∣X)∣
x,y
≤ 2 E IpI(X) -p2(X)Ipi(y|X)+ 2 £p2(X)Ipi(y|X)-p2(y|X)|
=1 X IpI(X)- p2(X)i + X p2 (X)DTV (P1®X)IIp2(y〔X))
xx
=DTV (pi(x)∣∣P2(x)) + Ex〜P2 [Dtv (pi(yIX)||p2®X))]
≤ DTV (p1(x)∣∣p2(x)) + max DTV (p1(yIx)∣∣p2 (y〔x))
□
Lemma 2. Suppose there are two chains of distributions {Xt1, t ≥ 0}, {Xt2, t ≥ 0}. At time t, the
states of both chains share an identical state space Xt1, Xt2 ∈ Xt. Suppose these two chains satisfy
a Markov-like property:pt+1(Xt+1 IXt, ..., X1, X0) =pt+1(Xt+1IXt). Then, the TVD of distributions
of two chains at time t can be decomposed as:
T
DTV (PT (XT )kpT (XT )) ≤ DTV (p1(x0)kp0 (x0))+X Est-I 〜p2-i DTV (p1(xtIxtT)kp2(XtIXtT)).
t=1
Proof. We prove this lemma by induction.
When T = 0, it’s easy to see that this lemma is true.
17
Under review as a conference paper at ICLR 2022
Assume it is true for T = k. We have:
∣pk+1(χk+1) -pk+1(χk+1)l
=∣X[pk+1(χk+1 Ixk)p1 (Xk) - pk+1(χk+1 Ixk)pk(Xk)]∣
xk
≤X ∣pk+1(χk+1∣χk)pk(Xk) -pk+1 (χk+1∣χk)pk(Xk)I
xk
=X ∣pk+1(χk+1∣χk)pk(Xk) -pk+1 (χk+1∣χk)pk(Xk)
xk
+ pk+1(Xk+1∣Xk)pk(Xk) -pk+1(Xk+1∣Xk)pk(Xk)∣
≤X [pk(Xk)∣pk+1(Xk+1∣Xk) -pk+1(Xk+1∣Xk)∣ + pk+1(Xk+1∣Xk)∣pk(Xk) -Pk(Xk)∣]
xk
=Exk〜潴[∣pk+1(Xk+1∣Xk) -pk+1(Xk+1∣Xk)∣] + Xpk+1(Xk+1∣Xk)∣pk(Xk) -Pk(Xk)∣
xk
Dtv (pk+1(Xk+1)M+1(Xk+1))
=2 X ∣pk+1(Xk+1) -pk+1(Xk+1 )∣
xk+1
≤2 X (Exk〜Pk[∣pk+1(Xk+1∣Xk) -pk+1(Xk+1∣Xk)∣]
xk+1
+ Xpk+1(Xk+1∣Xk)∣pk(Xk) -Pk(Xk)∣)
=ESk 〜PkDTV (pk+1(Xk+1 ∣Xk )∣∣pk+1(Xk+1∣Xk)) + Dtv (Pk (Xk )kpk(Xk))
k+1
≤Dtv (p0(X0)∣∣pg(X0)) + X ES -〜PL Dtv (PI(XIXtT)|欣(斓川-1))
t=1
Then this theorem holds for all n ∈ N.	□
Lemma 3. Consider two distributions withpdf/pmfP(X) and ⅛(x), where X = (xi, ..., Xn) ∈ Rn.
Suppose p and q can befactorized as: p(x) = ∩n=1 Pi(Xi),q(x) = Iln=I qi(xi). Then we have:
n
Dtv[p(x)kq(x)] ≤ EDTV[pi(Xi)Ilqi(Xi)].
i=1
Also, if the distance is measured by KL-divergence, we have:
n
DκL[p(x)∣q(x)] = EDKL[pi(Xi )∣qi(xi)].
i=1
18
Under review as a conference paper at ICLR 2022
Proof. We prove this result for discrete distributions, yet by replacing sum with integration, this
result stays true in continuous case.
Dtv[p(χ)kq(x)] = 2 X Ip(X) - q(X)I
x
1n	n
=2 E | Πpk (Xk)- ∏9k (Xk)I
x1,...,xn k=1	k=1
n	i-1	n
=2 x ∣x[ypk (Xk) Y qk (Xk)(pi(Xi) - qi(Xi))
x1,...,xn i=1 k=1	k=i+1
n	i-1	n
≤2 X X[Ypk(Xk) Y qk(Xk)Ipi(Xi) - qi(Xi)I
x1,...,xn i=1 k=1	k=i+1
1n
=2 £ EIpi(Xi)-qi(Xi)|
i=1 xi
n
=	DTV[pi(Xi)kqi(Xi)].
i=1
In KL-divergence case, we have:
DκL[p(X)kq(X)] = X p(x) log p^
x	q(X)
=x	[p(XI，…，Xn) Xlog pi(Xi)i
x1 ,...,xn	i=1	i i
nn
=X X hYpk(Xk)log*i
i=1 x1,...,xn k=1	i i
=X X Pi(Xi)log p≡
i=1 xi
n
=	DKL(pi(Xi)kqi(Xi)).
i=1
□
Lemma 4. (N -step distribution distance) Suppose the expected TVD between two dynam-
ics transitions is bounded as Emi = maXsN.a DTV [p(si∣SNi，ai)kp(s]sNi，ai)] and Eni =
maxs DTV [∏i(ai∣SNi)k∏i(ai∣SNi)]. If N ≤ K, the N-step distribution distance is bounded as:
N-1
DTV [pN (Si)kpN (Si)] ≤ X X (E∏j+ Emj)
t=0 j∈Nit
Thus,
N-1
DTV [pN(Si，ai)kpN(si,ai)] ≤ Eni + X X (EΠj + Emj)
t=0 j∈Nit
Proof. Consider a chain of regional state Xt = SNNT. TWo chains of distributions pt(Xt)，79(Xt)
denote the distributions of Xt under the environment and our model, respectively. First, because of
the property of an INS, these tWo chains’ transition dynamics can be decomposed as:
19
Under review as a conference paper at ICLR 2022
p(x |x ) = p(sN N -t |sN N -t+1 )
=	pj (s0j |sVj).
j∈NiN-t
And because pj (s0j |sVj ) = Pa pj (s0j |sVj , aj)πj (aj |sVj), by the property of TVD, we know that:
DTV [pj (SjIsVj)kpj (SjIsVJ] ≤ e∏j + emj .
With Lemma 3, we have:
DTV [p(xt∣xtT)kP(xt∣xtT)] ≤ X (e∏j + ∈mj).
j∈NiN-t
Then, by Lemma 2, we know that
DTV [pN(Si)kpN(Si)] = DTV [pN(XN)kpN(XN)]
N
≤	(πj +mj)
t=1 j∈NiN-t
N-1
=	(πj +mj),
t=0 j∈Nit
which completes the proof of the first part. And by Lemma 1, the second part holds true. □
Lemma 5. (Returns bound measured in branched rollout) Consider two MDPs p(s) andP(S). Sup-
pose they both adopt T -branched rollout scheme. Before the branch, suppose the dynamics dis-
tributions are bounded as maxs即 @ DTV (Ppre(SiISNi, ai)kPpre(SiISNi,。力 = Eme, and policy
divergences are bounded as maxs^ DTV (∏pre(ai∣SNi)∣∣∏Pre(ai∣SNi)) = Enre. After the branch,
Epmost and	Epπost are defined similarly.	Then the (local)	T -step	returns are bounded	as:
2r	T-1	∞
∣ηi	-	ηi∣≤	Tmx [e∏o st + X	Y k+1 X (Emjst	+ Enost)	+ x γk+1	x (Eme + Enre)]
γ	k=0	j∈Nik	k=T	j∈Nik
Proof. We prove this by estimating the state-action distribution divergence at each timestep. For
notation simplicity, we denote EpNrke = EpNrke = Pj∈Nk (Epmrje + Epnrje), and EpNokst analogously. If we
divide the chains into two parts: pre-branch and post-branch, and apply Lemma 4 on both parts, we
have:
For t ≤ T :
t-1
Dtv [pt(Si, ai)kPi(Si, ai)] ≤ Enost + X ENgt,
k=0	i
and for t > T:
T-1	t-1
DTV [pi(Si,ai)kPi(Si,ai)] ≤ Enost + X ENr + X ENe.
k=0
k=T
20
Under review as a conference paper at ICLR 2022
Then, the difference of discounted distribution can be bounded as:
∞
DTV [pi(si, qi)kPi(si,电)]≤ (1 - Y) E DTV [pt(⅛i,电)|属(方，电)]
t=0
T	t-1
≤ (1-Y)X ItHo st + X *)+
t=0	k=0
∞	T-1	t-1
(	1 - Y) X Yt (嘴st + X %S + X 喷)
t=T+1	k=0	k=T
T-1
=	pπoist + (1 - Y) X pNokst(Yk+1 + Yk+2 +...)+
i
k=0
∞
(	1 - Y) X pNrke(Yk+1 + Yk+2 +...)
k=T
T-1	∞
=	pπoist + X pNokstYk+1 + X pNrkeYk+1.
ii
k=0	k=T
We can convert this bound into the returns bound:
∞
|ni - ηl ≤ X Ytlri(si,αi) -ri⑶,说)|
t=0
2r	∞
≤ ------(I - Y) EDTV [ps(,si,ai)kps(^si,ai)]
1 - Y	t=0
2r
=1 - Y DTV [pi(si,αi)kpi(si,αi)]
2	T-1	∞
=2rmax [嘴st+X 嘿tYk+1+X * k+1]
1-Y	i	i
k=0	k=T
□
Lemma 6. (Returns bound measured in full length rollout) Consider two MDPs p(s) and P(S).
Suppose they both run their rollouts until the end of every trajectory, and the dynamic distributions
are bounded as maxs% 3 DTV (Pi(SiISNi, αi)kpi (si∣s^i, aj) = Emi, While policy divergences are
bounded as max§% DTV(∏i(ai∣SM)k∏i(ai∣SNj) = eπi. Then the (local) returns are bounded as:
2∞
[∏i - 7⅛ l≤ J -	[e∏i + X Y + X (Emj + Enj)]
Y	k=0	j∈Nik
Proof. We can think it as a special case of branched rollout, where Epost = Epre = E for every
subscript, and T = 0. From this perspective, applying the result of Lemma 5 completes the proof.
□
21