Under review as a conference paper at ICLR 2022
Training data size induced double descent for
DENOISING NEURAL NETWORKS AND THE ROLE OF
training noise level.
Anonymous authors
Paper under double-blind review
Abstract
When training a denoising neural network, we show that more data isn’t more
beneficial. In fact the generalization error versus number of of training data points
is a double descent curve.
Training a network to denoise noisy inputs is the most widely used technique for
pre-training deep neural networks. Hence one important question is the effect of
scaling the number of training data points. We formalize the question of how many
data points should be used by looking at the generalization error for denoising
noisy test data. Prior work on computing the generalization error focus on adding
noise to target outputs. However, adding noise to the input is more in line with
current pre-training practices. In the linear (in the inputs) regime, we provide
an asymptotically exact formula for the generalization error for rank 1 data and
an approximation for the generalization error for rank r data. We show using
our formulas, that the generalization error versus number of data points follows a
double descent curve. From this, we derive a formula for the amount of noise that
needs to be added to the training data to minimize the denoising error and see that
this follows a double descent curve as well.
1 Introduction
(a) Denoising Setup
(b) Supervised Learning Setup
Figure 1: Figure showing the difference in the noise placement between the traditional supervised
learning set up for which empirical and theoretical double descent curves have been found versus our
denoising set up for which we recover double descent curves.
Denoising noisy training data is a widely used technique for pretraining networks to learn good
representations of the data. Two extremely common examples of pretraining via denoising are
Masked Language Modelling (MLM) (Devlin et al., 2019) and Stacked Denoising Autoencoders
(SDAE) (Vincent et al., 2010). For many modern problem, we work at large scales in terms of the
number of parameters and the number of training samples. Recently there has been significant work
in understanding the effect of scaling the number of parameters in the neural network. This resulted
1
Under review as a conference paper at ICLR 2022
in the discovery of the much celebrated double descent phenomena (Belkin et al., 2019). However,
we do not have as good of an understanding of the effect of scaling the number of data points.
Works such as Nakkiran et al. (2020); Nakkiran (2020); d’Ascoli et al. (2020); Adlam & Pennington
(2020) show either empirically or via theoretical analysis that sample wise double descent exists.
However, these were in the regime of supervised learning. On the other hand, our motivation comes
from understanding denoising autoencoders. For MLM and SDAEs the denoising is a pretraining
procedure, in which case the generalization error would depend on the downstream task. We shall
instead look at the generalization error with respect to denoising test data. The difference between
prior supervised learning set up and our denoising set up can be seen in Figure 1.
In an attempt to theoretically understand the denoising setting, we look at the simplest setting.
Specifically we look at the case, when our network is a linear (with respect to the inputs) network
and we are denoising data that lies on a line embedded in high dimensional space. In this setting, we
derive the exact asymptotics for the generalization error. We see that in this case, the generalization
error spikes at the interpolation threshold (Figure 5a) and the amount of noise that we want to add
also spikes at the interpolation threshold (Figure 5b). From the theoretical analysis, we see that the
spike occurs due to the variance of the model increasing.
Contributions. The main contributions of our works are as follows.
1.	We empirically show that when denoising data using a feedforward network, the curve for
the generalization error versus the number of training data points as well the curve for the
ratio of the test data SNR to the optimal training data SNR has double descent. Further
changing the training data SNR can mitigate the double descent in the generalization error
curve.
2.	Assuming we have mean 0, rotational invariant noise, we derive an analytical formula
for the expected mean squared generalization error for denoising rank 1 data by a linear
network. Further, we use the same method to derive an approximation for higher rank data
and experimentally verify the accuracy of the formula for general low rank data.
3.	Using our formula, we show that even in this simple model, we see that the double descent
exists for the generalization error and for the amount of noise that should be added versus
the number of training data points.
Related work. Understanding deep neural networks is current active area of research with many
exciting theoretical results. The discovery that fixed depth infinite width neural networks can be
thought of as kernel regression (Jacot et al., 2018) and the discovery of double descent for neural
networks (Belkin et al., 2019) has sparked significant research into understanding the generalization
in the linear regime (in parameters not inputs). The exact asymptotic for generalization loss were first
understood for ridge regression (Bartlett et al., 2020; Hastie et al., 2019; Belkin et al., 2020; Advani
& Saxe, 2020). This was further generalized to understand the situation for the Random Features
model and the Neural Tangent Kernel (NTK) model (Mei & Montanari, 2019; Ghorbani et al., 2019;
Adlam & Pennington, 2020). A partial list of recent work for supervised learning includes Jacot et al.
(2020); Mel & Ganguli (2021); Derezinski et al. (2020); d’Ascoli et al. (2020); Dobriban & Wager
(2015); Geiger et al. (2019); Lampinen & Ganguli (2019); Liang et al. (2020); Muthukumar et al.
(2019); Loureiro et al. (2021). However, there has been no work, to our knowledge, that looks at the
problem either empirically or theoretically for the denoising set up.
The idea of adding noise to improve generalization has been seen before. One popular strategy is to
use Dropout (Hinton et al., 2012; Wan et al., 2013; Srivastava et al., 2014), where we randomly zero
out either neurons or connections. Another idea that is commonly used is data augmentation. In a
revolutionary paper, Krizhevsky et al. (2012) showed that augmenting the dataset with noisy versions
of the images, greatly improved the accuracy. Another area where noise is useful is adversarial
learning. Dong et al. (2021) shows epoch wise double descent for adversarial training.
In terms of recent theoretical work related to SDAEs, Pretorius et al. (2018) derived the learning
dynamics of a linear autoencoder in the presence of noise. They also establish some relationships
between the noise added and weight decay. However, they do not look at the generalization error or
quantify the optimal amount of noise that should be added. Gnansambandam & Chan (2020) looked
at the problem of what is the optimal amount of noise that should be added. However, they studied
this from the perspective of minimizing the variance of the performance.
2
Under review as a conference paper at ICLR 2022
2 Set-Up
Let U ∈ RM ×r be our feature matrix. For ease of notation, we assume that the columns of U have
unit norm and are pairwise orthogonal. Then to generate N data points, we sample our latent variables
V ∈ Rr×N and Σ ∈ Rr+×r such that V has columns that have unit norm and are pairwise orthogonal
and Σ is a diagonal matrix such that kΣkF = 1. Then a data matrix X is given by X = UΣVT. For
us, we have two matrices Xtrn and Xtst that correspond to the train and test data sets. Hence we have
corresponding VtTrn ∈ Rr×Ntrn, VtTst ∈ Rr×Ntst, and Σtrn, Σtst. We make no other assumptions on
U, Vtrn , Σtrn Vtst , Σtst except that they are given and fixed. Finally, let θtst , θtrn ∈ R+ be scalars
that will scale the singular values of Xtrn , Xtst so that we can control the SNR. We also assume
that θtst is fixed and that we have control over θtrn . Let c = M/Ntrn and let Atrn , Atst be noise
matrices that are added to the training and the test data. Let W be the linear autoencoder that is the
solution to the following problem
minimizeW	∣∣θtrnXtrn - W (θtrnXtrn + Atrn )kF ∙	(1)
Ytrn
Then, the expected mean squared error, is given by
R(A Q k V V 、∙— U? kθtstXtst - W(9tstXtst + Atst)IlF	/ɔʌ
R(θtrn, θtst, c, Ltrn,，tst) ： = E	NT	.	(2)
Ntst
2.1	Assumptions about the noise
We assume that each entry of the noise matrix A has mean 0, variance 1/M and that the entries of
A are pairwise uncorrelated. Additionally, we shall assume that A is rotationally bi-invariant. That
is, if Q is an M by M (N by N) orthogonal matrix, then QA (AQ) has the same distribution as A.
Another way to phrase this is if A = UA ΣA VAT is the SVD, then UA and VA are uniformly random
orthogonal matrices and are independent from ΣA and each other. Finally, we shall assume that A
has full rank with probability 1 and that the limiting distribution of the eigenvalues of ATA converge
to the Marchenko-Pastur distribution. While such assumptions on the noise may seem restrictive.
This encompasses a large family of noise distributions.
Proposition 1. If B is a random matrix that has full rank with probability 1 and its entries are
independent, have mean 0, have variance 1/M, and bounded fourth moment, and P, Q are uniformly
random orthogonal matrices. Then A = PBQ satisfies all of our noise assumptions.
2.2	Signal to Noise Ratio (SNR)
A quantity of interest to us will be the SNR, given by ∣X ∣F /∣A∣F . Hence, we need to normalize
everything by ∣A∣F. In this case, due to our assumptions, we have that E[∣A∣2F] = N . Hence, for
any variables and constants, if it has a hat, then that refers to that variable or constant normalized by
√N. For example, given θtrn Xtrn, and Atm, then we have that
Il θtrnXtrn kF/k AtrnkF = θtrn/k Atrn kF ≈ θtrn/ V Ntrn =： θtrn∙
3	Empirical Double Descent
We first show that sample wise double descent occurs for denoising neural networks empirically.
Figure 2 shows that if we train a feedforward network to denoise data such that the training data
signal to noise ratio (SNR) θtrn is the same SNR as that of the test data set (θtst), then the curve
for the denoising generalization error vs the number of training samples has the shape of a double
descent curve. Thus, together with prior work, this suggests that the double descent with respect to
the number of data points is a universal phenomena. However, unlike other hyperparameters, such as
number of features and number of training epochs, we cannot arbitrarily change the number of data
points as we are limited by the data set that we have. Hence it could be the case, that the maximum
number of data points that we have corresponds to the peak of the generalization error curve.
However, we can look at the amount of noise that we add to the training data. To see the effect of
the noise, for a variety of different θtrn/θtst , we compute the denoising generalization error versus
3
Under review as a conference paper at ICLR 2022
(a) Rank 1 Data
(b) Non Linear Synthetic Data
(c) MNIST
(d) Rank 1 Data
(e) Non Linear Synthetic Data
(f) MNIST
Figure 2: Figure showing the double descent phenomena for the generalization error versus the
number of the training data points. The top row is for a linear network and the bottom row is for a 3
layer ReLU network. Here the training data SNR and the test data SNR both equal θ.
(a) MNIST
(b) CIFAR
Figure 3: Figure showing the denoising generalization error for a 3 layer neural network trained
for various different values of θtrn /θtst and number of training data points. Each neural network
was trained for 1500 epochs, using gradient descent with a learning rate of 10-3. For MNIST, we
averaged over 20 trials and for CIFAR10 we averaged over 5 trials.
the number of data points curve. We do this for MNIST and CIFAR dataset. We create test data
sets by taking the test data for each and then adding Gaussian noise. We fix the test SNR to be 1
for both datasets. Hence we know the test data SNR. We then take various different fractions of the
training data and train a 3 layer ReLU neural network (without bias) for various levels of training
noise. For each of pair of parameters (number of training data points and the level of training noise),
we compute the generalization error averaged over 20 trials for MNIST and 5 trials for CIFAR. Here
the test noise and training noise is resampled for each trial. The plots for the generalization error
can be seen in Figures 3a (MNIST) and 3b (CIFAR10). The first thing we notice is that for most
ratios for the test SNR to training SNR we see sample wise double descent. Further, we see that the
optimal denoising error does not occur when the train SNR is equal to the test SNR. This is very
surprising as it contradicts standard thought that training data distribution should be the same as the
test data distribution. Interestingly, as seen in Figures 4 and 4b, we see that the optimal ratio depends
on the number of data points and the shape of the curve for the values θtrn /θtst that results in the
best generalization error versus the number data points also has the shape of a double descent curve.
4
Under review as a conference paper at ICLR 2022
(a) MNIST
0 5 0 5 0 5
20.17.15,12.10,7.
HNfSM-HNS ura4
(b) CIFAR
Figure 4: Figure showing the sample wise double descent for the optimal amount of training noise.
4	Theoretical Results and Consequences
In this paper, we want to theoretically understand the phenomena seen in Section 3. The main
theoretical result of the paper is summarized below in Theorem 1.
Theorem 1. Let σitrn, σitst be entries of Σtrn, Σtst and letr = 1. Let c = M/Ntrn be fixed. Suppose
θtrn is O(,Ntrn) and θtst is O( VNtst) Then, if C < 1, we have that
R(θtrn, θtst, c, Σtrn, Σtst)
(θtstσtst)2	C2 ((θtrnσtrn)2 + (θtrnσtrn)4)
Ntst(1 + (θtrnσtτy2c)2 + M(1+(θtrnσtrn)2c)2(1 - C)
and if c > 1, we have that
+ o(1)
(3)
R(θtrn, θtst, C, Σtrn, Σtst)
(θtstσtst)2
Ntst(1 + (θtrnσ Rnyy
+ M (1+XrnJn)2)(c- 1) + o ⑴.⑷
The o(1) error term goes to 0 as Ntrn, M → ∞.
We could imagine that the rank r version is the same as the above but with a summation over the
rank as shown in the equations below. But, we shall see in Section 5 that this turns out to only be an
approximation. 1
n, θtst, C, Σtrn ,
Σtst) ≈
i=1
(θtstσtst)2
Ntst(1 + (θtrnσtrn)2c)2
+ C2((θtrnσtrn)2 + (θtrnσtrn )4) +
+ M (1 + (θtrnσtrn)2c)2(1 — c)+ ( 1
(5)
R(θ,-一	θ,	. C	Σ"一	Σ,	,)	≈ X _______(θtstσtst)2______,________c(θtrnσtrn)2________, o(1)
(trn,	tst,	, trn,	tst)	i=ι	Ntst(1 +	(θtrnσtrn)2)2	+ M(1	+	(θtrnσtrn)2)(c- 1) + ().
(6)
4.1	Optimal Amount of Noise
First, if we ignore the error term, we can differentiate the formula to get the following formula for the
optimal training SNR.
θ2pt-trn = ImaX (0, NNttsS⅛ (1 - 2-C) - M (C-C) ) c < 1	⑺
Ntrn	ImaX(0,2Nfe(C-1) - Nrn)	c> 1
We already see the surprising result that the optimal training SNR and the test SNR are not equal.
This is surprising, as traditional philosophy is that the training data should be drawn from the same
distribution as the test data. Here instead we see that the optimal training distribution actually depends
c. Further, the formulas in Equation 7 also describe a double descent curve for θopt-trn/Ntrn versus
C curve as shown in Figure 5b.
1Derivation and exact assumptions for when the formulas are accurate in the appendix.
5
Under review as a conference paper at ICLR 2022
(a) Rank 1 Theory Generalization Error (b) Rank 1 Theory Test SNR / Optimal training
SNR
Figure 5: Plot showing the double descent curves for the generalization error as well as for the ratio
of the test SNR to the optimal training SNR. Here M = 1000 and θtst = 1 and c was changed by
changing Ntrn .
4.2	Double Descent Curves
We have already seen that the optimal amount of training noise follows a double decent curve. This
is due to the double descent seen in the asymptotics for the generalization error. To understand this
phenomenon, we first note that the bias of our model is given by the first term in formula in Theorem
1 and the variance is given by the second term. That is, we have that the variance given by
Pr	C2((θtrnσtrn)2 + (θtrnσtrn)4)
乙i=1 M(1+(θtrnσtrn)2 c)2 (1-c)
Pr	c(θtrnσtrn)2
乙 i=1 M (1 + (θtrnσtrn)2)(c-1)
c<1
c>1
From these formulas, we can see that as c → 1 these formulas have a singularity. Since we have a
linear model, c = 1 is the interpolation threshold (i.e., the point after which we have 0 training error).
Hence as with previous models for double descent, we see that as we approach the interpolation
threshold, the variance of model increases, resulting in an increase in the generalization error.
If we fix the number of features M and change c by varying Ntrn , and also scale θtrn as θtrn =
θtrn,Ntrn, then We see that as C → 1, the variance of the model increases. Once We have enough
data points so that c < 1, we have the variance of the model starts decreasing. Additionally, we
see that as We increase the number of data points, the bias decreases until We hit the interpolation
threshold, after this point, the bias is constant. Similarly, if We fixed Ntrn and changed c by changing
M then after the interpolation threshold, the inductive bias of the model kicks in. Here We see that
the variance terms corresponds to kW k2F. Hence We see that as c → ∞, We have that this implicitly
regularize the Weights of the netWork and get the second descent in the generalization error. That is,
the variance of the model decreases as c → ∞. Additionally, We see that as We increase the number
of parameters, the bias of the model of the model decreases and then after the interpolation threshold
it becomes constant. Note that this value is non-zero and depends on the training SNR.
In previous Work (such as Mei & Montanari (2019)) on double descent curves for ridge regression, We
see that optimal ridge regularization results in the vanishing of the double descent phenomena. This
is also seen empirically for L2 regularization for classification in Nakkiran et al. (2020). HoWever, in
our theoretical model even if We optimally pick the amount of training noise, We still have double
descent. This is in contrast to results seen With a deep netWork on real data in Figure 3.
5 Proof of Theorem 1
We prove Theorem 1, via the steps shoWn in Figure 6. The proofs for all of the lemmas have been
moved to the appendix. Here We present a proof sketch that details the high-level ideas.
6
Under review as a conference paper at ICLR 2022
Decompose into Bias and Variance
Derive a formula for W
% E"U∣Xt 城- WXtMl 图 +Ea MIW4m∣ 图
E[7>∙(∕(AAT)%)] -
Matrix Theory
trace terms
物蚂ɪu左7疝
θ^‰∖2
-^ɪ-jr(/iʃh) + 2
τ2
Figure 6: Figure showing the major steps used to derive the formula for the generalization error.
5.1	Step 1: Decompose the error into bias and variance terms
First, we decompose the error into its bias and variance.
Lemma 1. If Atst has mean 0 entries and Atst is independent of Xtst and W, then
EAtst [kθtstXtst - WYtstk2F] = θt2stEAtst [kXtst - WXtstk2F] + EAtst [kWAtstk2F] .	(8)
、------------{-------------} 、--------{-------}
Bias	V ariance
5.2	STEP 2: FORMULA FOR W
In our current setup, we know that W is the solution to a least-squares problem. Hence W =
XtrnYt二 Expanding this out, We get the following formula for W. Let h = VTrnA；”, k = Ajmu,
S =(I - AtrnAtrn)u, t = Vtrn(I - AiLnAtrn), β = 1 + θtrnvTrnAjtrnu, τ1 = θ2rn kt k 2 k kk 2 + β2,
and τ2 = θt2rnksk2khk2 +β2.
Proposition 2. If β 6= 0 and Atrn has full rank then
θtrnβuh + θ2rnktk2UkTAtrn C < 1
W = σ1	τ1	trn
θσ θtσσnβuh + θtrnkh. UsT	C > 1
For Gaussian noise Atrn has full rank with probability 1 and β is a random variable whose expected
value is equal to 1, and the distribution is highly concentrated. Thus, Proposition 2 applies when
Atrn is isotropic Gaussian noise. Here we restricted ourselves to rank 1, as using Meyer (1973), we
can expand formulas of the form (A + xyT)t where x, y are vectors. For the higher rank case, we
apply the formula form Meyer (1973) iteratively. This is the main difficulty of the method. Previous
work on deriving asypmtotics for the generalization error had the noise on the output. Hence would
take the pseudoinverse of a matrix that only depended on the data. However, in our case, we are
taking the pseudoinverse of matrix that depends on the noise.
5.3	Step 3:Decompose the terms into sum of various trace terms.
Let us first look at the bias term.
Lemma 2. If W is the solution to Equation 1, then
Xtst-WXtst=(τ1 Jf c< 1
Let us now look at the second or the variance term.
Lemma 3. If the entries of Atst are independent with mean 0, and variance 1/M, then we have that
EAtst [k WAtstk2] = NMt kW k2 ∙
7
Under review as a conference paper at ICLR 2022
Note that this did not need any assumptions on W or Xtst. All that was needed were the assumptions
on Atst. Thus, this holds more generally. This decomposition also follows from Bishop (1995). In
light of Lemmas 1, 2, 3, and the fact that kXtst k2F = θt2st, we see that the expected mean squared
generalization error is given by,
EAtst
kθtstXtst- WYtstllF
_	NtSt
ι β2
NtstTi
θist + MM kW kF,
where τi depends on whether c < 1 or c > 1. Finally, let us look at the kWk term.
Lemma 4. If β 6= 0 and Atrn has full rank, then we have that if c < 1,
kw kF = θ2rnβi Tr(h h) + 2 够nT2tk2β Tr(h kT AJrn) + θ4nf^ Tr((AJrn)T kkT AJrn)
and if c > 1, then we have that
∣W kF = θ⅜βi Tr(hr h) + 2 限nkh k2β Tr (hτ ST) + θ4nk2^ Tr(SsT).
F	τ22	τ22	τ22
5.4	Step 4: Estimate using random matrix theory.
While the formula given by Lemmas 1, 3, and 4 is correct, we need a simpler formula to analyze the
situation. Using ideas from random matrix theory, we can simplify the expression for kW k2F. To do
so, we first need to prove Lemmas 5 and 6.
The main idea behind Lemmas 5 and 6 is that due to the rotational invariance of Atrn , the expectation
of the trace of products of various matrices derived from Atrn is determined by the expected value of
some function χ of the eigenvalues of Atrn . However, instead of directly computing this expected
value, we note that for any matrix A, that satisfies the noise assumptions, if we let M, N → ∞,
with M/N → c, then the eigenvalue distribution converges to the Marchenko - Pastur distribution
(Marcenko & PastUr,1967; Gotze & Tikhomirov, 2011; 2003; 2004; 2005; Bai et al., 2003). Gotze &
Tikhomirov (2004) showed that the distribution of the eigenvalues converged almost surely with a
rate of at least O(N-1/2+) for any > 0. Thus, we can use the expected value of the χ(λ) for λ
sampled from the Marchenko - Pastur distribution as an approximation.
For space reasons, we provide only one instance of the lemmas in the main text. The complete
versions can be found in the appendix.
Lemma 5. Suppose A is an p by q matrix such that the entries of A are independent and have mean
0, variance 1/q, and bounded fourth moment. Let Wp = AAT and let Wq = ATA. Let C = p/q.
Suppose λp, λq are a random eigenvalue of Wp, Wq. Then
1.	If P < q, then E [f ] = i—c + o(1).
Lemma 6. Suppose A is an p by q matrix that satisfies the noise assumptions. Let x, y be unit vectors
in p and q dimensions. Let C = p/q. Then
1. E[Tr(xT (AAT)J x)] =
J 1—1C +。⑴
Ip TC-LI + o(1)
p<q
p>q
Using these technical lemmas, we can now deal with all of the terms in the expressions in Lemma 4.
First, let us look at the non-trace terms.
Lemma 7. If Atrn satisfies the noise assumptions, then we have that
1.	E[β∕θtrn] = 1∕θtrn +。⑴ and NaMeIetrn)= (maχ(M,Ntrn) 11 -c|)) +。(1)∙
c2	c3(2 + c)
2.	If C < 1, then E[∣∣h∣∣2] =  --+ o(1) and Var(∣∣hk2) = —―-：-------+ o(1).
1 - c	Ntrn(1 - c)3
3.	If c > 1, then E[∣∣h∣∣2] = cτ + o(1) and Var(khk2) = : ()~^ + o(1).
c - 1	Ntrn(c - 1)3 4
4. E[kkk2] = ɪ +。⑴ and Var(kkk2) = :f + ；1 +。⑴.
1 - c	M (1 - c)
8
Under review as a conference paper at ICLR 2022
c1	1
5.	E[ksk2] = 丁 + o(1) and Var(||s『)=2Mc + o(1)
6.	E[ktk2] = 1 - c +。⑴,Var(ktk2) = 2Nc- + o(1).
Lemma 8. Under the noise assumptions, we have that
E[Tr(hτkτAjrn)] = 0 and Var(Tr(hTkTAJm)) = χ3(c)∕Ntrn,
where χ3(c) = E[1∕λ3], λ is an eigenvaluefor AAT and A is as in Lemma 6.
Lemma 9. Under the noise assumptions, we have that
c2	3	1	c4
Tr((Atrn)Tkk Atrn) = (1 - c)3 +o⑴ and VaMTr((Atrn)Tkk Atrn)) = μX4(c)-M (1 - c)6
where χ4(c) = E[1∕λ4], λ is an eigenvalue for AAT and A is as in Lemma 6.
Lemma 10. Under the same assumptions as Proposition 2, we have that Tr(hT sT) = 0.
Lemmas 7, 8, 9, and 10 tell Us that all of the terms are highly concentrated. Thus, even though such
terms may not be uncorrelated, We can use the fact that ∣E[XY] - EX]E[Y]| < PVar(X)Var(Y),
to treat the terms as if they are uncorrelated. Since these variances have now been shown to
be o(1), We have that for each of these terms E[XY] = E[X]E[Y] + o(1). For example, since
τ1 = β2 + θt2rnktk2kkk2 + o(1), using Lemmas 1, 4, and 6, We have that E[τ1] = 1 + θt2rnc + o(1).
Similarly, E[τ2] = 1 + θt2rn + o(1). Finally, using these lemmas, We can simplify the expressions in
Lemma 4 to get the formulas for the expected generalization error shoWn in Equations 3 and 4.
(a) LoW SNR
(b) High SNR
Figure 7: Figure shoWing the relative error for our formula. M = 2500 and c is changed by changing
Ntrn. For loW rank, We average over 10 trials and for high rank, We average over 100 trials.
6	Accuracy of Approximation
In this section We experimentally verify the accuracy of our formula for general rank r data. Here for
loW SNR (θtrn, θtst are O(1)), We sample σitrn, σitst I.I.D. from the squared standard Gaussian and
for high SNR (θtrn,θtst are Θ(√Ntrn), Θ(√NtSt)), We multiply this by √Ntrn, √Ntst. AS We can
from Figure 7, We see that our formula is better for loW SNR and loW rank data.
7	Conclusion
In this paper, We sWitch focus from supervised set up to the unsupervised set up. Specifically, We
look at the problem of denoising data. We empirically shoW that sample Wise double descent exists
for the generalization error. Further, We shoW that the optimal amount of training noise is not the
same as the test noise. In fact, We see sample Wise double descent for the ratio for the test SNR to the
optimal training noise. To understand this phenomena, We study the simplest model, denoising rank 1
data using a linear model. Here We derive the exact asymptotics for the generalization error.
9
Under review as a conference paper at ICLR 2022
References
Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent
and a multi-scale theory of generalization. In ICML, 2020.
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural
networks. Neural Networks,132:428 -446, 2020.
Z. Bai, B. Miao, and J. Yao. Convergence rates of spectral distributions of large sample covariance
matrices. SIAM J. Matrix Anal. Appl., 25:105-127, 2003.
P. Bartlett, Philip M. Long, G. Lugosi, and Alexander Tsigler. Benign overfitting in linear regression.
Proceedings of the National Academy of Sciences, 117:30063 - 30070, 2020.
Mikhail Belkin, Daniel J. Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116:15849 - 15854, 2019.
Mikhail Belkin, Daniel J. Hsu, and Ji Xu. Two models of double descent for weak features. SIAM J.
Math. Data Sci., 2:1167-1180, 2020.
Chris M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural Comput., 7
(1):108-116, January 1995. ISSN 0899-7667. doi: 10.1162/neco.1995.7.1.108. URL https:
//doi.org/10.1162/neco.1995.7.1.108.
Stephane d'Ascoli, Levent Sagun, and G. Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? ArXiv, abs/2006.03509, 2020.
Michal Derezinski, Feynman T. Liang, and Michael W. Mahoney. Exact expressions for double
descent and implicit regularization via surrogate random design. ArXiv, abs/1912.04533, 2020.
J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, 2019.
E.	Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression and
classification. arXiv: Statistics Theory, 2015.
Chengyu Dong, Liyuan Liu, and Jingbo Shang. Double descent in adversarial training: An implicit
label noise perspective. ArXiv, abs/2110.03135, 2021.
M. Geiger, Arthur Jacot, S. Spigler, Franck Gabriel, Levent Sagun, StePhane d'Ascoli, G. Biroli,
Clement Hongler, and M. Wyart. Scaling description of generalization with number of parameters
in deep learning. ArXiv, abs/1901.01608, 2019.
B. Ghorbani, Song Mei, Theodor Misiakiewicz, and A. Montanari. Linearized two-layers neural
networks in high dimension. ArXiv, abs/1904.12191, 2019.
Abhiram Gnansambandam and S. Chan. One size fits all: Can we train one denoiser for all noise
levels? In ICML, 2020.
F.	Gotze and A. Tikhomirov. Rate of convergence to the semi-circular law. Probability Theory and
Related Fields, 127:228-276, 2003.
F. Gotze and A. Tikhomirov. Rate of convergence in probability to the marchenko-pastur law.
Bernoulli, 10:503-548, 2004.
F. Gotze and A. Tikhomirov. The rate of convergence for spectra of gue and lue matrix ensembles.
Central European Journal of Mathematics, 3:666-704, 2005.
F. Gotze and A. Tikhomirov. On the rate of convergence to the marchenko-pastur distribution. arXiv:
Probability, 2011.
T. Hastie, A. Montanari, S. Rosset, and R. Tibshirani. Surprises in high-dimensional ridgeless least
squares interpolation. ArXiv, abs/1903.08560, 2019.
10
Under review as a conference paper at ICLR 2022
Geoffrey E. Hinton, Nitish Srivastava, A. Krizhevsky, Ilya Sutskever, and R. Salakhutdinov. Improving
neural networks by preventing co-adaptation of feature detectors. ArXiv, abs/1207.0580, 2012.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and
generalization in neural networks (invited paper). Proceedings of the 53rd Annual ACM SIGACT
Symposium on Theory of Computing, 2018.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In Hal DaUme In and Aarti Singh (eds.), Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 4631-4640. PMLR, 13-18 Jul 2020. URL https://Proceedings.
mlr.press/v119/jacot20a.html.
A. Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolu-
tional neural networks. Communications of the ACM, 60:84 - 90, 2012.
A. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning in
deep linear networks. ArXiv, abs/1809.10374, 2019.
Tengyuan Liang, A. Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants
and restricted lower isometry of kernels. In COLT, 2020.
Bruno Loureiro, Gabriele Sicuro, CedriC Gerbelot, Alessandro Pacco, Florent Krzakala, and Lenka
Zdeborova. Learning gaussian mixtures with generalised linear models: Precise asymptotics in
high-dimensions, 2021.
V. Marcenko and L. Pastur. Distribution of eigenvalues for some sets of random matrices. Mathematics
of The Ussr-sbornik, 1:457-483, 1967.
Song Mei and A. Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv: Statistics Theory, 2019.
Gabriel Mel and Surya Ganguli. A theory of high dimensional regression with arbitrary correlations
between input features and target functions: sample complexity, multiple descent curves and
a hierarchy of phase transitions. In Marina Meila and Tong Zhang (eds.), Proceedings of the
38th International Conference on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pp. 7578-7587. PMLR, 18-24 Jul 2021. URL https://proceedings.
mlr.press/v139/mel21a.html.
Carl D. Meyer, Jr. Generalized inversion of modified matrices. SIAM Journal on Applied Mathe-
matics, 24(3):315-323, 1973. doi: 10.1137/0124033. URL https://doi.org/10.1137/
0124033.
V. Muthukumar, Kailas Vodrahalli, and A. Sahai. Harmless interpolation of noisy data in regression.
2019 IEEE International Symposium on Information Theory (ISIT), pp. 2299-2303, 2019.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent, 2020.
Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=7R7fAoUygoa.
Arnu Pretorius, S. Kroon, and H. Kamper. Learning dynamics of linear denoising autoencoders.
ArXiv, abs/1806.05413, 2018.
N. R. Rao and A. Edelman. The polynomial method for random matrices. Foundations of Computa-
tional Mathematics, 8:649-702, 2008.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine
Learning Research, 15(56):1929-1958, 2014. URL http://jmlr.org/papers/v15/
srivastava14a.html.
11
Under review as a conference paper at ICLR 2022
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. J. Mach. Learn. Res.,11:3371-3408, December 2010. ISSN 1532-4435.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of
the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pp. 1058-1066, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL
http://proceedings.mlr.press/v28/wan13.html.
12
Under review as a conference paper at ICLR 2022
In this section we present all of the proofs for the results in the main text. Here we present the proofs
in the same order they appear in the text.
A Noise Assumptions
Proposition 1. If B is a random matrix that has full rank with probability 1 and its entries are
independent, have mean 0, and have variance 1/M and P, Q are uniformly random orthogonal
matrices. Then A = PBQ satisfies all of our noise assumptions.
Proof. Since P, Q are a uniformly random orthogonal matrices, and A = PBQ, then it is clear that
A is rotationally bi-invariant and has full rank.
Since each entry of B has mean 0 and each entry of A is a linear combination of entries of B where
the coefficients (i.e., the entries from P, Q are independent of B), we have that each entry of B have
mean 0. Due to the orthogonal nature of P, Q, we have the variance for an entry of A is the same as
the variance of entry in B .
Thus, the only thing left to prove is that the entries of A are uncorrelated. To do this, we note that
NM
aij =	pilblkqkj .
Consider two entries ai1j1 and ai2j2. Then we have that
NM	NM
E[ai1j1ai2j2] =E	pi1lblkqkj1	pi2lblkqkj2
k=1 l=1	k=1 l=1
NM
=	E[pi1lpi2l]E[bl2k]E[qkj1qkj2]
k=1 l=1
ME
M
pi1lpi2l
l=1
N
qkj1qkj2
k=1
E
The second inequality follows from the fact that P, Q, B are independent from each other, and that
fact that the entries of B are independent and have mean 0. Hence the cross terms have expectation 0.
If we have that i1 = i2 and j1 6= j2 , then we have that since Q is an orthogonal matrix
N
N
k=1
E[qkj1 qkj2] = E
qkj1 qkj2
k=1
0.
Thus, the entries are uncorrelated. Similarly when i1 6= i2 since P is orthogonal matrix, we get that
□
the entries are uncorrelated.
Convergence to Marchenko-Pastur. If we strengthened the uncorrelated condition, to the entries
being independent. Then due to the mean and variance assumptions (along with an assumption
that the fourth moment is bounded), we would have convergence to Marchenko-Pastur distribution.
However, the independence along with the bi-invariance would then force our noise distribution to be
i.i.d. Gaussian.
In general however, with relaxed assumption of the entries only being uncorrelated, convergence is
not known. However, in our case, we have a much simpler proof for matrices formed by Proposition
1. In our case, the noise matrices B satisfy the standard assumptions for convergence. We then
multiply B by orthogonal matrices that are independent to B . Hence this has no effect on the
eigenvalue distribution. Thus, the eigenvalues distribution for these matrices also converge to the
Marchenko-Pastur distribution.
B Proofs
Due to our data generation assumptions that kΣtrn kF = kΣtst kF = 1 for rank 1 data, we have that
σ1trn = σ1tst = 1.
13
Under review as a conference paper at ICLR 2022
B.1 STEP ??: DECOMPOSE INTO BIAS AND VARAINCE
Lemma 1. If Atst has mean 0 entries and Atst is independent of Xtst and W, then
EAtst [kXtst - WYtstk2F] = EAtst [kXtst - WXtstk2F] + EAtst [kWAtstk2F] .
'-----------{z-----------} '--------{---------}
Bias	V ariance
Proof. Using the fact that for any two matrices kG - H k2F = kGk2F + kH k2F - 2Tr(GT H), we get
that
kXtst - W Ytst k2 = kXtst - W Xtst - W Atst k2F
= kXtst - W Xtst k2F + kW Atst k2 - 2Tr((Xtst - WXtst)TWAtst).
Then since the trace is linear, and Xtst, W are independent of Atst, and Atst has mean 0 entries, we
see that
EAtst [Tr((Xtst - WXtst)TWA
tSt)] = 0.
Thus, We have the needed result.	口
B.2 STEP ??: FORMULA FOR Wopt
Proposition 2. Let h = VTnAtrn, k = AiLnu, S = (I - AtrnAiLn)u, t = Vtrn (I - AtmAtrn),
β = 1 + θtrn VTrnAtrnu, Ti =。晨|用|2|向|2 + β2 ,and「2 =。鼠|囱|2|向|2 + β2 ∙ If β = 0 and
Atrn has full rank then
(θtτnβUh + θ2rτktk2 UkTAtrn c < 1
lθtτnβ Uh+θ¾hf UST	c>1.
Proof∙ Let us first proof the case When c > 1. Here We knoW that U is arbitrary. Here We have that
Atrn has full rank. Thus, since c > 1, We have that M > Ntrn, thus Atrn has rank Ntrn. Thus, the
roWs of Atrn span the Whole space. Thus, Vtrn lives in the range of AtTrn . Finally, since β 6= 0, We
Want Theorem 5 from Meyer (1973).
Here let us further define
P2 = — θ2nksk2 AtrnhT- θtrnk and q2 = - ^=^ST - h
ββ
and finally τ2 = θt2rnkSk2khk2 + β2. Then We have from Meyer (1973) that
(Atrn + θtrnuvTrn)t = Atrn + ^rnAtrnhTST - 2-p2qT
trn	trn β trn	τ2	2
In our case, We only care about θtrnUVtTrn (Atrn + θtrn UVtTrn)t. Thus let us multiply this through and
see What We get.
θtrnuvtrn(Atrn + θtrnuv4n户=θtrnuvtrn (Alrn +	A- AthTST -	p2q2 )
trn	trn	trn trn β	τ2	2
=θtrnUh + θ2*UST + 铝Uvrn (9AtrnhT + θtrnk) qT
=θtrnUh + θ2rnkhk2 UST + ^^^2"""2 UqT + θ2nβUhUqT
β	τ2	2	τ2	2
Then We have that
θ3rnkSk2khk2 T θ4rnkSk2kh"4 T 。鼠1网2向产
	cq2 =	Z	US	
τ2---------------------------------------------τ2β-τ2
(9)
14
Under review as a conference paper at ICLR 2022
and
θtrnβuhuqτ = - θ3rnkhk uhusτ — %rn' uhuh.
τ2	τ2	τ2
Using that β — 1 = θtrnVTnA[打凶=θtrnhu, We get that
UUhUq = — θ2rnkhHβ T) UST - θtrnβ(β -I) 3.
τ2	2	τ2	τ2
(10)
(11)
Substituting back in and collecting like terms We get that
θ ST " ,θ ST 寸—θ J1 θ2rnksk2khk2 β(β - 1)",
θtrnuvtrn (Atrn + θtrnuVtrn) = θtrnu I 1 -	-	I h+
τ2	τ2
%nu (>f - θ2rnksk2khk4 - kh∣∣2(β - 1) ) sτ
We can then simplify the constants as folloWs.
1 - θ2rnksk2khk2 - β(β -1) = T -θ2rnksk2khk2- β2 + β = 2
τ2	τ2	τ2	τ2
and
khk2 - θ2rnksk2Mk4	khk2(β - 1) = IIhll2(T2 Fn 1网2向|2- β(β -1) = ⅛ =些
β	T2β	T	βτ2	βτ T
This gives us the result for c < 1.
If c > 1, then We have that M < Ntrn . Thus, the rank of Atrn is M the range of Atrn is the Whole
space. Thus, u lives in the range of Atrn . In this case, We then Want Theorem 3 from Meyer (1973).
In this case, We define
P1 = - θrnkkf tτ - k and qT = - θr*kτArn - h.
Then in this case, We have that
(Atrn + θtrnuvTn)) = Atrn + ^rn t kA[n - ^^~pIqT ∙
β	τ1
Then We simplify the equation as We did before!
□
B.3 STEP ??: EXPAND INTO TRACE TERMS
Lemma 3. If the entries of Atst are independent with mean 0, and variance 1/M, then we have that
EAtst [k WAtstk2] = NMt kW k2.
Proof. To see this, We note if We look at AtstAtTst, then this is a M by M, for Which the expected
value of the off diagonal entries is equal to 0, While the expected value of each diagonal entry is
Ntst/M. Thatis, EAtst [AtstATst] = NTIm.
Then note that
kWAtstk2 = Tr(AtTstWTWAtst) = Tr(WTWAtstAtTst) = Tr(WTWAtstAtTst).
Using the fact that the trace is linear again, We see that
EAtst [Tr(WT WAtstATst)] = Tr(W T W EAtst [AtstATst]) = NMt Tr(W T W) = NM ∣W kF.
□
15
Under review as a conference paper at ICLR 2022
Lemma 2. If W is the solution to Equation 1, then
Xtst- WXtst = ʃ βXtst
(T2 tst
if c < 1
ifc>1
Proof. To see this, we have the following calculation for when Ntrn > M .
Xtst- WXtst = Xtst- θrnθtsβuhuvTt - θ2rnθtstktk2ukτA%uvTt
τ1	s	τ1	rn s
_V	θtrn Btste …T λ↑ T,τ	θtrnθtstktk2 LTzIt …T
=Xtst	UvtrnAtrnUvtst	uk AtrnUvtst.
τ1	τ1
First, we note that e = 1 + θtrnvtTrnAttrnu. Thus, we have that θvtTrnAttrnu = e - 1. Thus,
substituting this into the second term, we get that
Xtst - WXtst = Xtst - θtstβ(e - 1) UvTst - θ2rnθtstktk2 UkTAlnUvTst.
τ1	τ1
For the third term, we note that k = AttrnU. Thus, we have that kTAttrnU = kTk = kkk2 .
Substituting this into the expression, we get that
Xtst - W Xtst = Xtst -
θtste (e - 1) T
-------------UVtst
τ1
Θ2rnθtst ktk2kkk2, ”T
Uvtst.
τ1
Noting that Xtst = θtstUvtTst, we get that
Xtst - W Xtst = Xtst	1 —B (B—] — —tτn 11—ʊ—ʊ—ʊ— ''''∖
τ1	τ1
To simplify the constants, we note that τ1 = θt2rnktk2kkk2 + e2. Thus, we get that
T1 + e - β2 - θt2rnktk2kkk2 e
-----------------------------=—
τ1	τ1
For the case when Ntrn < M, we note that the first term of W is the same (modulo replacing τ1 for
τ2) as it is for the case when c > 1. Thus, we just need to deal with the last term. Here we see that
the last term is
θtrnθtstkhk2 USTUvT
τ2
Here we note that s = (I - AtrnAttrn)U. Thus, in particular, s is the projection of U onto the kernel
of Arn. Thus, We have that U = S + s, where S ⊥ S. This then tells Us that sτ U = ∣∣sk2. Thus, for
this term, we get that it is equal to
θ2khk2ksk2
T
Xtst.
For this term we note that τ2 = e2 + θ2 ∣h∣2 ∣U∣2. Thus, doing the same simplification as before, we
see that for the case when Ntrn < M, we have that
e
Xtst - WXtst = -Xtst∙
τ2
□
In light of Lemma 2 and the fact that ∣Xtst∣2F = θt2st. We see that if we look at the expected MSE,
we have that,
EAtst
IlXtst - W(Xtst + Atst)k
_	Nst	_
e
NtstTi
琮st + MM ∣W kF,
—
16
Under review as a conference paper at ICLR 2022
where τi depends on whether c < 1 or c > 1.
Finally, let us look at the kW k term.
Lemma 4. If β 6= 0 and Atrn has full rank, then we have that if c < 1,
k W kF = θ2rnβ2 Tr (hτ h) + 2 够nTF2β Tr(h kτ AL) + θtrnf^ Tr((AZn)T kkT AZn)
and if c > 1, then we have that
k W kF = θ2rnβ2 Tr (hτ h) + 2 θ3rnkhk2β Tr (hτ sτ) + “晨沙户 Tr(SsT).
F	τ21 2	τ22	τ22
Proof. To deal with the term Tr(W τ W ) we are again going to have to look at whether Ntrn is bigger
than or smaller than M . First, let us start by looking at the case when Ntrn > M. Here we have that
kW k2F = Tr(WτW)
=Tr ((θtrnβuh + θ2rnktk2ukτAlrn)T (θtrnβuh + θ2rnktk2ukτAir，)
=θ2rnβ2 Tr(hτ uτ uh) + 2 θ3rnktk2β Tr(hτ uτ ukτ Alrn) + θ44j≠ Tr((AZn)T kuτ ukτ Alrn)
τ12	τ12	τ12
=θtτnβ2 Tr(hτ h) + 2 θ‰τtkβ Tr(hτ kτ A%) + θ4r⅛ Tr((Alrn)T kk A↑m).
Where the last inequality is true due to the fact that kuk2 = 1. How about when Ntrn < M. Then
we have the following string of equalities instead.
kW k2F = Tr(W τ W )
=Tr ((θtrnβuh + θ2rnkhk2usτ)T (θtrnβuh + θ2rnkhk2usτ)!
τ2	τ2	τ2	τ2
=θ2nβ2 Tr(hτ uτ uh) + 2 θ3rnkh k2β Tr(hτ uτ usτ) + θM2h^Tr(suτ usτ)
τ22	τ22	τ12
=θ2nβ2 Tr(hτ h) + 2 θ3rnkhk2β Tr(hτ sτ) + θM2h^ Tr(ssτ).
τ22	τ22	τ22
□
B.4 STEP ??: ESTIMATE USING RANDOM MATRIX THEORY.
Lemma 5. Suppose A is an p by q matrix such that the entries of A are independent and have mean
0, variance 1/q, and bounded fourth moment. Let Wp = AAτ and let Wq = AτA. Let C = p/q.
Suppose λp, λq are a random eigenvalue of Wp, Wq. Then
1. Ifp < q, then E
2. If p < q, then E
3. If p < q, then E
4. If p < q, then E
5. If p > q, then E
6. If p > q, then E
I-1C + O(I) •
(1-1C)3 4 + O(I) •
(1-1C)5 6 + O(I) •
C⅛⅞+1 + o(1)∙
ICC-I +。⑴•
(1-C-1)3 + O(1).
17
Under review as a conference paper at ICLR 2022
7.	If P	>	q ,then E	寺	=C(I-C-C)5 ) + o ⑴.
8.	If p	>	q, then E	=	=	C-(CI-C-IC~~+1) + o(1).
Proof. Suppose A is an p by q matrix such that the entries of A are independent and have mean 0,
variance 1/q, and bounded fourth moment. Then we know that Wp = AAT is an p by p Wishart
matrix with c = C. If we send p, q to infinity such that p/q remains constant, then we have the
eigenvalue distribution Fp converges to the Marchenko Pastur distribution F in probability.
From Rao & Edelman (2008), we know there exists a bi variate polynomial L(m, z) = czm2 - (1 -
c - z)m + 1 such that the zeros of L(m, z) given by L(m(z), z) are such that
m(Z)=/ λ⅛ dF ㈤=Eλ
1
λ — z
For the Marchenko-Pastur distribution, we have that for z = 0, we get that m(z) = 1/(1 -c). Thus,
for λp is an eigenvalue value of Wp , we have that
E [ λp ] =i⅛ + o ⑴.
For Eλ [(λ⅛]
we need to calculate m0(0). Using the implicit function theorem, we know that
m0(z) = —1
∂L(m(z),z)) dL(m(z),z)
Here We can see that ∂L∕∂m = 2czm + C + Z — 1. Thus, at (1/(1 — c), 0), this is equal to C - 1.
Also ∂L∕∂z = Cm + m. Again at (1/(1 — c), 0) this is equal to .二产 + 占=(二产.Thus, We
have that
m0(0) = (T⅛.
Similarly, using the implicit function formulation, We can calculate m00(0) and m000(0).
On the other hand if q < p, then Wq := AT A is not a Wishart matrix here, because it is scaled by
the Wrong constant. HoWever, multiplying it by 1/C gives us the correct scaling. Thus, AT A/C is a
Wishart matrix With C = 1/C Thus, for λq is an eigenvalue value of Wq , We have that
1	C-1
E [λqj = T-C-T+o⑴.
We can obtain the rest in a similar manner from the previous results.
□
Lemma 6. Suppose A is an p by q matrix that satsifies the standard noise assumptions. Let x, y be
unit vectors in p and q dimensions. LetC = p/q. Then
1.
2.
3.
4.
E[ Tr (XT (AAT )*x)]=
T T-C + o(1)
IP T⅛ + o(1)
p<q
p>q
E[Tr(XT(AAT)t(AAT户叫=[-^—+^ T)⑴
U (1-C-1)3 + o(1)
E[ Tr (yT(Ar A)ty)] = ( pP +"R	p<q.
[ɪ-C-T +。⑴	p>q
P p 1
E[Tr(yT(ATA)t(ATA)ty)]={q(C-C)3
1(1-CT)3
+ o(1)
+ o(1)
p<q
p>q
p<q
.
p>q
18
Under review as a conference paper at ICLR 2022
Proof. Let A = UΣVT be the SVD. Then We have that (AAT广=U(Σ2)*UT. Then since A is
bi-unitary invariant, we have that U is a uniformly random unitary matrix. Thus, a = xT U is a
uniformly random unit vector. Note With probability 1, the rank of A is full and that the non-zero
eigenvalues of ATA and AAT are the same.
If p < q, then We have that
p1
E[Tr(xT(AAT)*x)] = ɪ2 a2-2 ∙
i=1	σi
Using Lemma 5, We have that E[1/i2] = 1/(1 - C) + o(1). Thus, We have that
p1	1
E[Τr(xT (AAT )tχ)] = E--- +。⑴.
i=1p1-C
On the other hand, ifp > q, from Lemma 5, We have that E[-/i2] = C-1/(- - C-1) + o(-). Thus,
q - C-1
E[Τr(xT (AAT )tχ)] = EBrrC-I + o(-)∙
i=1 p - - C
Similarly, if we had we looking at Τr(xT (AAT)^ (AAT)*x), we would have a 1/4 term instead. Thus,
if p < q, We Would have that
E[Tr(χT (AAT WAAT )7)] = —+。⑴.
(1 - C )
A similar calculation holds for the others.	□
Now we have the following Lemma in the main text. However, here instead of having one big proof,
we will separate each term out into its own lemma.
Lemma 7. If Atrn satisfies the standard noise assumptions, then we have that
L E网=1 +。⑴ and Var(β) = (max(M⅞rn)∣1-c∣)) +。⑴.
c2	c3 (2 + c)
2.	If C < 1, then E[∣∣h∣∣2] =  -+。⑴ and Var(kh∣∣2) = —— ------3 + o(1).
1 - c	Ntrn(1 - c)3
3.	If c > 1, then E[∣∣h∣∣2] = c— + o(1) and Var(kh∣∣2) = C (+ o(1).
c - 1	Ntrn(c - 1)3
4.	E[kkk2] = ɪ + o(1) and Var(kkk2) = c (2 + c)3 + o(1).
1	- C	M(1 - C)3
C1	1
5.	E[ksk2] = -C- + o(1) and Var(ksk2) =2 Mc + o(1)
6.	E[∣∣tk2] = 1 - c + o(1), Var(ktk2) = 2看 + o(1).
Lemma 11. β term.
Proof. First, we calculate the expected value of β. To do so, let Atrn = UΣVT be the SVD. Then
since Atrn is bi-unitarily invariant, we have that U, V are uniformly random unitary matrices. Since
u, vtrn are fixed. We have that a := vtTrnV ∈ RNtrn and b := UTu ∈ RM are uniformly random
unit vectors. In particular, we have that E[ai] = 0, E[bi] = 0, Var(ai) = 1/Ntrn, Var(bi) = 1/M .
Thus, if σi are the singular values for Atrn , then we have that
min(M,Ntrn) 1
β =1+ θtrn	V	-aibi.
σi
i=1	i
Thus, if you take the expectation you get that
E[β] = 1.
19
Under review as a conference paper at ICLR 2022
On the other hand, lets look at the variance. For the variance, we need to compute E[β2]. Now if we
let T := θtrnWTnAtrnu. Then We have that
β2 = 1 + T2 + 2T.
Thus, again if We take the expectation, We get that
E[β2] = 1 + E[T2].
Again due to the fact that a, b are independent have have mean 0 entries, the cross terms in E[T2].
Thus, We have that
		min(M,Ntrn) X	" a2b2 i=1	i	21 = θtrn2	——E MNtrn	min(M,Ntrn) X	σ12 i=1	σi
E[T2]	θtrn2E			
NoW We need to case on Whether M > Ntrn or M < Ntrn . NoW to use Lemma 5, We note that
q = M and p = Ntrn.
Suppose We have that M > Ntrn , then in this case, We have that q > p. Thus, We have that
1
1 - C
+ o(1),
Where C = p/q = Ntrn/M = 1/c. Thus, We have that
E ⅛2] = 1⅛ +。⑴
C-I +。⑴.
Thus, We have that
Thus, We have
E[T2] = θ2rn M(C - 1) + o (m,
VaHe) = θ2rn M(Cc-I) + o (M
On the other hand, if M < Ntrn . Then We have that q < p. Thus, We have that
+ o(1),
Where C = p/q = Ntrn/M = 1/C. Thus, We have that
E 周=T⅛ +。⑴.
Thus, We have that
C T
1 - C T
Thus, We have
+ o(1)
Var(β) = θt2rn
C
Ntrn(I - c)
□
Lemma 12. khk2 term.
Proof. We Want to do a calculation similar to that in Lemma 1. Here We have that
khk2 = Tr(hT h) = Tr((Attrn)T vtrnvtTrnAttrn) = Tr(vtTrnAttrn(Attrn)T vtrn) = Tr(vtTrn(AtTrnAtrn)tvtrn).
20
Under review as a conference paper at ICLR 2022
To use Lemma 6, we note that A = AtTrn, q = M, p = Ntrn . Let us now suppose that M < Ntrn .
Then again taking the expectation, we see that
M c	c2
E[khk2] = L I— + o(1) = I— + o(1).
Ntrn 1 - c	1 - c
For the expectation of |向|4, let Atrn = U∑VT be the svd. Then h = VrTrnVΣ*UT. Let a = VrTrnV
and note that a is a uniformly random unit vector. Thus, we have that
M1
khk = X Fai.
i=1 σi
For the expectation of khk4 , we note that
MM	M
khk4 = XX 3 aj X j4 a4 + X j2 j2 a2a2.
i=1 j=1 σ2σ2	= σ4	= σ2 σ2
Taking the expectation of the first term, we get
M 1	4	3M	c2	c3
X E hd E[ai] = Nrn ((T-Ψ + o⑴)=3 Ntrn(I-C)3 + o⑴.
Taking the expectation of the second term, we get
1 2	1 c2	c4	c3
M (M-1)E [σ2j E[ai] = M (M-1) Nrn GT-ψ +。⑴)=(Γ-ψ - Ntrn(I-Cy +o ⑴.
Thus, we have that
E[khk4]
(1
C4
-C2
+	c3(2 + C)
Ntrn(I - c)3
+ o(1).
Thus, the variance is
Var(khk2)
c3 (2 + C)
Ntrn(I-C)3
+ o(1).
For M > Ntrn , we instead have that
E[khk2] = Ntrn (C-I+O(I)) = C-T + o⑴.
For the expectation of khk4 , we note that
Nt
rn Nt
rn
khk4 = X X σ⅛a2a2
Ntrn
X σ4 a4 + X ^σ σ aa%∙
i=1 σi	i6=j σi σj
Taking the expectation of the first term, we get
Ntrn
XE 周E[a4]
C3	C3
G-Iy + o(1))=3 Ntrn(C - 1)3 + O(1).
Taking the expectation of the second term, we get
Ntrn(Ntrn -I)E [向[E[a2 F = Ntrn(Ntrn - 1
C2
Nrn
C2
C2
FP+o ⑴
(C-1)2	Ntrn(C - 1)2
+ o(1).
—
Thus, we have that
E[khk4]
CC . + 3 _C3__.___C2__ + O(I)
(C - 1)2 +3 Ntrn(C - 1)3	Ntrn(C - 1)2 +()
C2	c2 (2c - 1)
(C-可 + Ntrn(C- 1)3 + O().
Thus, the variance is
Var(khk2)
C2 (2c - 1)
Ntrn (c - 1)3
+ O(1).
□
21
Under review as a conference paper at ICLR 2022
Lemma 13. kk k2 term.
Proof. First note that k only appears in the formula when c < 1. Thus, we can focus on this case. As
with h, we have that
kkk2 = Tr(UT (Alrn)T Arnu) = Tr(UT (AtrnATjtu).
Again using Lemma 6, with q = M, p = Ntrn , A = Atrn , y = u. Thus, since we have q = M <
Ntrn = p, we get that
E[kkk2] = ɪ + o(1).
1-c
To calculate the variance, we need to calculate the expectation of kkk4. Here be again let A = UΣVT
be the SVD. Then let b := UTu. Then we have that
M1
kkk2=X σσr 陈
Thus, we see that
kkk4 = X -b b4 + X 2-2-b b2b2.
i=1 σi	i6=j σi σj
Taking the expectation of the first term we get
M c2	_	3c2
3 M2 (1 - c)3 = M(1 - c)3 .
Taking the expectation of the second term we get
M(M - 1)	c2	_	c2	c2
-M2(1 - c)2 = (1 - c)2 - M(1 - c)2 .
Thus, we have that
E[kkk4] = 7τJ + : (2 + c)3 + o(1).
(1 - c)2	M(1 - c)3
Thus, we have that
Var(kkk2) = M⅛∣ + o(1).
□
Lemma 14. ksk2 term.
Proof. First, we note that s only appears when M > Ntrn . Thus, we only need to deal with that
case. For this term, we note that (I - AtrnAttrn) is a projection matrix onto a uniformly random
M - Ntrn dimensional subspace. Here be again let A = UΣVT be the SVD. Then let b := UTu.
E[ksk2] = E[uT u - uT AtrnAttrnu] = E 1 - bT IN0trn
Similarly, we have that
Ntrn	2
ksk4 =	1-Xbi2
Ntrn	Ntrn
1+ Xbi2	-2Xbi2
i=1	i=1
Ntrn	Ntrn	Ntrn
1+Xbi4+Xbi2bj2-2Xbi2
i=1 i6=j	i=1
22
Under review as a conference paper at ICLR 2022
Taking the expectation, we get that
i=1
1 + c⅛ +
1 + c⅛ +
2
i6=j
Ntrn (Ntrn - 1)
M2
CM - 2C
i=1
-21
c
2
+ --
+ cM
—
Thus, we have that
Var(ksk2) = 2 cM
□
Lemma 15. ktk2 term.
Proof. First, we note that t only appears when M < Ntrn. Thus, we only need to deal with that
case. For this term, We note that (I - AjrnAtrn) is a projection matrix onto a uniformly random
Ntrn - M dimensional subspace. Then similar to ksk2, we have that
E[ktk2] = E[vtTrnvtrn - vtTrn
AtrnAtrnvtrn] = E 1 - a
IM
0
M1
-X -- = 1 - C
Ntrn
i=1
0
0
a
1
Similarly, we have that
M2
ktk4 =	1-Xai2
i=1
1+XiM=1ai2!
M
M
- 2 X ai2
i=1
1 +	ai4 +	ai2aj2
i=1	i6=j
M
- 2 X ai2
i=1
2
M
Taking the expectation, we get that
MM	M
E[ktk4] = 1 + 3 X N2~ + X n2----------------2 X
i=1 trn i6=j	trn i=1
1
Ntrn
Thus, we have that
1+Ntrn+
Ntrn(Ntrn
1+NCn+c2-
(I-C)2+ CMf
M2
c
- 1) - 2c
-......2c
Ntrn
Var(ktk2)
2 -c-
Ntrn
□
Now We could just use the the fact that ∣E[XY ] - E[X ]E[Y ]| <，Var(X )Var(Y). Another way to
do this is via using big O in probability. Which is defined as follows:
23
Under review as a conference paper at ICLR 2022
Definition 1. We save that a sequence of random variables Xn is OP (an), if there exists an N such
that for all > 0, there exists a constant L such that for all n ≥ N, we have that Pr[|Xn| > Lan] < .
Then the trace terms.
Lemma 8. Under standard noise assumptions, we have that
E[ Tr (hT kT AL)]=。
and
Var( Tr (hT kT A(Ln)) = X3(c)/Ntrn,
where χ3(c) = E[1∕λ3], λ is an eigenvaluefor AAT and A is as in Lemma 6.
Proof. First we note that
Tr(h k Atrn) = Tr((Atrn) vtrnu (Atrn) Atrn) = u (Atrn) (AtrnAtrn) vtrn).
Again let Atrn = UΣV T be the SVD. Then, we have the middle terms depending on Atrn simplifies
to
(Ajrn)T Ajrn(Ajrn)T = U (∑t)T ∑t(∑t)T V T
Thus, again letting b = uT U and a = VTvtrn . We see that
M1
TMhT kT Atrn) = Eaibi —.
i=1	σi
Now if take the expectation, since a, b are independent and mean 0, we see that
EAtrn [Tr(hT kT Attrn)] =0.
Let us also compute the variance. Here we have that
M1
E[Τr(hTkTAtrn)2] = EE-6 E[a2]E[b2] + 0.
i=1 σi
Now for the Marchenko Pastur distribution we have that the expectation of 1∕λ3 = χ3(c). where χ3
is some function. Thus, we have that
E[Τr(hT kT Atrn)2] = ； X3(c) +。⑴.
Ntrn
□
Lemma 9. Under standard noise assumptions, we have that
c2
Tr((Atrn)T kk Atrn) = ∩-----A3 +。⑴
(1 - c)
and
3	1	c4
VaHTr((Atrn)Tkk Atrn))=Μ乂4(C) - M (1 - c)6
where χ4(c) = E[1∕λ4], λ is an eigenvalue for AAT and A is as in Lemma 6.
Proof. Now using Lemma 6, we see that
EAtrn[Tr((Attrn)TkkTAttrn)]
c2
(1-c)3 .
Similar to proofs before, we have that
M 3	1	c4
EAtrnrrr((Atrn)TkkTAtrn)2] = X MX4(c) + X M (T-^ +。(1).
i=1	i6=j	( - c)
24
Under review as a conference paper at ICLR 2022
Where χ4(c) = E[1∕λ4] for the Marchenko PastUr distribution. Thus, We have that
3	1	c4
Var(Tr((Atrn)Tkk Atrn))=Μ乂4 (C) + M (T-CP +。⑴.
□
Lemma 10. Under the same assumptions as Proposition 2, we have that Tr(hT sT) = 0.
Proof. Here We note that hT =(Atrn)T vtrn and ST = uτ(I - AtrnAjrn)T. Thus, we have that
Tr(hT sT) = Tr((Attrn)T vtrnuT - (Attrn)T vtrnuT (AtrnAttrn)T)
= Tr(vtrnAtrnu) - Tr(u (AtrnAtrn) (Atrn) vtrn)
= Tr(vtrnAtrnu) - Tr(vtrnAtrnAtrnAtrnu)
= Tr(vtrnAtrnu) - Tr(vtrnAtrnu)
=0
□
As we can see that if we take the expectation of kW k over Atrn, since the variance of each of the
terms is small, we can approximate E[XY] with E[X]E[Y]. Then we get the following.
If M < Ntrn , we have that
2	θt2rn	C2	θt4rn(1 - C)2	C2
Atrn[k	k ]= (1 + θ2rnC)2 (T-C) +0+(1 + θ2rnC)2 (T-Ψ
=c2	θ2rn + θ4rn
(1 + θ2rnc)2(1 - eV
On the other hand, M > Ntrn , we have that
EAtrJlW k2] = (i+⅛j2 C⅛ + (i+⅛n)2 (C⅛ 一
=C θ2rn(1+ θ2rn)
=C - 1 (1+ θ2rn)2
=θ2rn	C
1 + θtrn C- 1
Now combining everything together, we get that
IlXtst- W (Xtst + Atst)Il
_	Nst	_
θ2st,	+」2	θ2rn + θ4rn
Ntst(I+ θ2rnC)2 + M C (1 + θ2rnC)2(1-c)
______θtst"____i_ ɪ θtrn C
Ntst(I+ θ2rnC)2 + M 1+θ2rn c-1
C<1
C>1
B.5	Proof of Theorem
We can see that the main text has how to put all of the pieces together to prove the main Theorem.
We don’t replicate that here.
B.6	FORMULA FOR θopt-trn
As stated in the main text, we only need to take the derivative. So, we don’t present that calculation
here as it is fairly straightforward.
C Generalizations
In this section we discuss some possible generalizations of the method.
25
Under review as a conference paper at ICLR 2022
C.1 Higher rank
Let us present some heuristics for the higher rank formula. To do so we shall need some notation.
Let Xtrn = Pir=1 σitrnui(vitrn)T. Let A be the noise matrix. Then for 1 ≤ j ≤ r, define
Aj = A + Xj-1 σitrnui(vitrn)T
We shall now make some assumptions. Specifically, we assume that uj , vjtrn, and Aj are all such that
for i1 6= i2, and for all j we have that
E[uTιAj Aj ui2 ] = E[(vtrn)T Aj Aj Vtrn]=。.
Additionally, We assume that for all iι, i2,j We have that E[(vtjn)TAjui2] = 0. We also assume that
the variance of these terms goes to 0 as Ntrn , M go to infinity.
Lemma 16. With the given assumptions, we have that for all i < j,
σtrnUi(vtrn)T Ab σtrnui(vtrn)τ Aj-i ≈ σtrnUi(vtrn)T Aj. ≈ ... ≈ σtrnUi(vtrn)T A3
Proof. Write Aj = Aj-1 + σjtrnuj (vktrn)T and the use Meyer (1973) to expand the pseudoinverse
of Aj. When we do this, we see that due to the assumption all terms expect σtrnui(vtrn)τAj-i are
small.	□
Define hj = (Vjrn)TAj, kj = σjrnAjuj∙, tj∙ = (Vjrn)T(I - AjAj), Sj = σjrn(I - AjA；)Uj,
βj = 1 + σjrn(vjrn)TA;Uj, τ(j) = IltjlI2∣Ikjll2 + β2, τ2j) = Ilsjk2∣Ihj∣∣2 + βj2, and similarly
p(ij) , p(2j ) , qi(j ) , and q2(j) . Now, we can write
Xtrn +A= σrtrnUr (Vrtrn)T + Ar-i
Then we have that
r
W = X (σrrnUr (vrrn)T + Ar )t = X σtrnUi(vtrn)T (σ^rnUr (vrrn)T + Ar )t
i=i
Expanding and using the lemma, we get that
r
W ≈ X σtrnUi(vtrn)TA；+1
i=i
Pr=1 审 Uihi +
Pr=1 σt⅛ Uihi +
τ2
c<1
c>1
Where the second equality comes from the rank 1 results.
Now that we have an approximation for W (given our assumptions), we can now approximate the
variance and bias terms again. Let Wi denote the ith factor (corresponding to Ui ) of W . First, for the
bias, due to the orthogonality of the U’s we get that
r
IXtst - W Xtst I2F = X
i=i
r
σitstUi(Vitst)T-WiXσitstUi(Vitst)T
j=i
2
F
Again, using our assumptions, we see that the terms in the j summation dropout besides when j = i.
Then again using our rank 1 result, we get that
IXtst- WXtstkF = X (条σtst!
i=i τidx
For the variance, we again estimate the norm of W by expanding the trace. Here we see that the cross
terms are 0 due to factors of UiT Ui2 . For the diagonal terms, we again use the rank 1 results and get
that
kWkF =X ITr(hThi) + 2-i≡「储碣AD + T<Tr((ADTk思AD
26
Under review as a conference paper at ICLR 2022
and if c > 1, then we have that
k W kF = XX ； Tr(hT hi) + 2(σ≡h⅛ Tr(hT ST) +
i=1 (τ2 )	(τ2 )
(σtrn)4khik4
(τ2(i)
Tr(sisiT).
The final step would be to estimate each of these terms using random matrix theory. However,
unfortunately the Aj may not satisfy all of the needed conditions. However, we know that Aj is a
perturbation of A and A satisfies all of the needed conditions. Hence, if the perturbation is small,
we can replace Aj with A and hopefully not incur too much cost. Note this is also the reason why
the previous assumptions might be reasonable. If we replace Aj’s with A use our estimates from the
rank 1 result. We then get our estimate for the generalization error for general rank r data.
r
R(θtrn, θtst, c, Σtrn, Σtst) =
i=1
and if c > 1, we have that
(θtstσtst)2	+ c2((θtrnσtrn)2 + (θtrnσtrn)4) + ⑴
NtSt(1 + (θtrnσtrn)2c)2 + M(1 + (θtrnσirn)2c)2(1 - c)+ ( )
(12)
R(θ+	θ…C Σ Σ ) = X___________________(HtSt0产¥__________,_________c(θtrnσtrn)2_________+ 0
R(θtrn, θtst,c, "rn, MSt)	= Ntst(ι + (θtrnσtrn)2)2 十 M(1 + (θtrnσtrn)2)(c - 1)+ O(I).
(13)
In the experimental section, we see that for small values of r for c bounded away from 1. This seems
to be good estimate for the generalization error.
C.2 Random Features Model
Here we assumed that our data is given X = UΣVT. One generalization of this that we have a G
who entries i.i.d Gaussian, or whose columns are uniformly distributed on the unit sphere. Then for
non linear function σ, we assume that X = σ(GUΣV T). If we assume that σ is linear, the we have
that as L → ∞, GTG → IM. Thus, we have that GU approximately satisfies our assumptions of
orthogonal columns. Hence we expect our formula to still be a reasonable approximation.
D	Experiments
Please see accompanying notebook for code to produce the data for all of the figures.
D.1 LOW SNR AND HIGH SNRDATA
For low SNR data, we sample the θ times singular values from a squared standard Gaussian. We do
this independently for all 2r singular values. We call this the low SNR region because θ is not being
scaled with the number of data points. Hence as Ntrn , NtSt → ∞, the SNR goes to 0.
For the high rank data, We sample θ times singular values from a squared Gaussian and then multiply
by √Ntrn, √Ntst. Hence here the SNR does not go to 0 as Ntrn, NtSt → ∞.
27
Under review as a conference paper at ICLR 2022
E Generalization Error versus Training noise level plots
E.1 More Tests for Rank 1
Here we provide more examples of c and how our theoretical formula matches the experimental
performance exactly.
Each empirical point is the average over 50 trials. These were run on a laptop with 8gb of RAM and
an i3 processors. The average time to produce any of these plots is about 10 to 30 minutes.
(d) c = 2
(e) c = 10
ʌ
(f) C = 2, θtst = 0.01
Figure 8: Figures (a) - (e) showing the accuracy of the formula for the expected mean squared error
for c = 0.1, 0.5, 0.9, 2, 10 for fixed value of θtst. Figure (f) empirically verifies the existence of a
regime where training on pure noise is optimal. Here the red and green lines represent E[%] and
E[θ2rn] respectively. Each empirical data point is averaged over at least 50 trials.
E.2 Rank 2 Data
Let us now demonstrate that the double descent shaped curve exists beyond rank 1 data and linear
autoencoders. We will do this by gradually making the set up more complicated until we can no
longer recreate this phenomena. First, we consider rank 2 data is of the following form. Let Wdata
be some fixed matrix, then our data is generated by
X = relu(Wdatarelu(uvT).
Where a different v is sampled for the training and test data. the results for this can be seen in Figure
9. As we can from the figure, we have the exact same qualitative trend for c that we saw before. That
is, as c goes from 0 to 1, we have that θtrn goes from θtst to 0, and then as c → ∞, we have that θtrn
goes to infinity as well.
E.3 MNIST Data
We now look at the linear network with MNIST data.
E.3.1 Non-linear Network
Here, we trained each network for 1500 epochs. During each epoch we computed a gradient using
the whole data set. We used Adam as the optimizer with the code written in Pytorch. Each data point
was generated over 20 trials.
28
Under review as a conference paper at ICLR 2022
(a)c = 0.3	(b)c = 0.5	(c) c= 0.9
(d) c = 2	(e) c = 10
Figure 9: Rank 2
(d) c = 1.12
(e) c = 3.92
(f) c = 39.2
Figure 10: MNIST
These experiments take a little bit more time to run and the one with bigger amounts of data can take
upto 5 hours on a google cloud instance with 16gb RAM. Here we used a Telse P4 gpu.
LRL - is a model with a reLU at the end of the first layer only.
29
Under review as a conference paper at ICLR 2022
(a) c = 0.261
(c) c = 1.12
(b) c
0.784
Figure 11: MNIST - LRL model
(d) c = 1.57
(e) c = 3.92
30