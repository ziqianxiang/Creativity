Under review as a conference paper at ICLR 2022
FedLite: A Scalable Approach for
Federated Learning on Resource-constrained
Clients
Anonymous authors
Paper under double-blind review
Ab stract
In classical federated learning, the clients contribute to the overall training by
communicating local updates for the underlying model on their private data to
a coordinating server. However, updating and communicating the entire model
becomes prohibitively expensive when resource-constrained clients collectively
aim to train a large machine learning model. Split learning provides a natural
solution in such a setting, where only a (small) part of the model is stored and
trained on clients while the remaining (large) part of the model only stays at
the servers. Unfortunately, the model partitioning employed in split learning
significantly increases the communication cost compared to the classical federated
learning algorithms. This paper addresses this issue by proposing an end-to-end
training framework that relies on a novel vector quantization scheme accompanied
by a gradient correction method to reduce the additional communication cost
associated with split learning. An extensive empirical evaluation on standard
image and text benchmarks shows that the proposed method can achieve up to
490× communication cost reduction with minimal drop in accuracy, and enables a
desirable performance vs. communication trade-off.
1 Introduction
Federated learning (FL) is an emerging field that collaboratively trains machine learning models on
decentralized data (Li et al., 2019; Kairouz et al., 2019; Wang et al., 2021). One major advantage ofFL
is that it does not require clients to upload their data which may contain sensitive personal information.
Instead, clients separately train local models on their private datasets, and the resulting locally trained
model parameters are infrequently synchronized with the help of a coordinating server (McMahan
et al., 2017). While the FL framework helps alleviate data-privacy concerns for distributed training,
most of existing FL algorithms critically assume that the clients have enough compute and storage
resources to perform local updates on the entire machine learning model. However, this assumption
does not necessarily hold in many modern applications. For example, classification problems with
an extremely large number of classes (often in millions and billions) commonly arise in the context
of recommender systems (Covington et al., 2016), information retrieval (Agrawal et al., 2013), and
language modeling (Levy & Goldberg, 2014). Here, the classification layer of a neural network itself
is large enough that a typical FL client, e.g., a mobile or IoT device, cannot even store and locally
update this single layer, let alone the entire neural network.
Split learning (SL) is a recently proposed technique (Vepakomma et al., 2018; Thapa et al., 2020)
that naturally addresses the above issue of FL. It splits the underlying model between the clients and
server such that the first few layers are shared across the clients and the server, while the remaining
layers are only stored at the server. The reduction of resource requirement at the clients is particularly
pronounced when the last few dense layers constitute a large portion of the entire model. For instance,
in a convolutional neural network (Krizhevsky, 2014), the last two fully connected layers take 95%
parameters of the entire model. In this case, if we allocate the last two layers to the server, then
the client-side memory usage can be reduced by 20×. Nonetheless, one major limitation of SL is
that the underlying model partitioning leads to an increased communication cost for the resulting
framework. Specifically, to train the split neural network, the activations and gradients at the layer
where the model is split (referred to as cut layer) need to be communicated between the server and
1
Under review as a conference paper at ICLR 2022
^∙>help
Synchronization w/
of the server
Client-side model Wc
A mini-batch
of client data
Clients
Codebook + codewords (+ labels)
Quantizer
Grad.
Corrector
Decoder
Gradient w.r.t.
quantized activation
Server-side model Ws
Loss
Server
/~*~~I
H
l Quantization Layer
Figure 1: Overview of the proposed algorithm. In order to reduce the additional communication
between the clients and the server, we propose to cluster similar client activations and send the cluster
centroids to the server instead. This is equivalent to adding a vector quantization layer in split neural
network training. The success of our method relies on a novel variant of product quantizer and a
gradient correction technique. We refer the reader to Section 4 for further details.
clients at each iteration. The additional message size is in proportion to the mini-batch size as well as
the activation size. As a result, the communication cost for the model training can become prohibitive
whenever the mini-batch size and the activation size of the cut layer are large (e.g. in one of our
experiments, the additional message size can be 10 times larger than that of the client-side model).
In this paper, we aim to make the split neural network training communication-efficient and enable
its widespread adoption for FL in resource-constrained settings. Our proposed solution is based on
the critical observation that, given a mini-batch of data, the client does not need to communicate
per-example activation vectors if the activation vectors (at the cut layer) for different examples in the
mini-batch exhibit enough similarity. Thus, we propose a training framework that performs clustering
of the activation vectors and only communicates the cluster centroids to the server. Interestingly,
this is equivalent to adding a vector quantization layer in the middle of the split neural network (see
Figure 1 for an overview of our proposed method). Our main contributions are as follows.
•	We propose an end-to-end communication-efficient neural network splitting-based FL approach
for resource-constrained clients (cf. Section 4). The approach employs a novel compression
scheme that leverages product quantization to effectively compress the activations communicated
between the clients and server.
•	After applying the activation quantization, the clients can only receive possibly noisy gradients
from the server. The inaccurate client-side model updates lead to significant accuracy drops in our
experiments. In order to mitigate this problem, we propose a gradient correction scheme for the
backward pass, which plays a critical role in achieving a high compression ratio with minimal
accuracy loss.
•	We empirically evaluate the performance of our approach on three standard FL datasets (cf. Sec-
tion 5). Remarkably, we show that our approach allows for up to 490× communication reduction
without significant accuracy loss.
•	We present a convergence analysis for the proposed method (cf. Section 4.3), which reveals a
trade-off between communication reduction and convergence speed. The analysis further helps
explain why the proposed gradient correction technique is beneficial.
Here we note that our proposed method has potential applications beyond FL with resource-
constrained clients as it can be applied in any learning framework that can benefit from a quantization
layer. For instance, it can be used to reduce the communication overhead in two-party vertical
FL (Romanini et al., 2021), where the model is naturally split across two institutions and the data
labels are generated on the server. It is also worth mentioning that the proposed method does not
expose any additional client-side information to the server than vanilla split neural network training
approach, such as SplitFed (Thapa et al., 2020). Thus, our method can also leverage existing
privacy preserving mechanisms such as differential privacy (Wei et al., 2020) or instance-hiding
schemes (Huang et al., 2020) to provide formal privacy guarantees.
2
Under review as a conference paper at ICLR 2022
2	Background and Related Works
Federated Learning (FL). Suppose we have a collection of M clients I = {1, 2, . . . , M}.
Each client i ∈ I has a local dataset Di and a corresponding empirical loss function Fi (w) =
Pξ∈D. f (w; ξ)∕%, where W denotes the model parameters, and n = ∣D∕ denotes the number of
samples of the local dataset. The goal of FL is to find a shared model w that can minimize the
averaged loss over all clients, defined as follows:
M
F(w) = X piFi(w)	(1)
i=1
where pi = ni∕ PiM=1 ni is the relative weight of local loss Fi. Motivated by the data privacy concerns,
under a FL framework, the clients perform local training and only communicate the resulting models
to a coordinating server as opposed to sharing their raw local data with the server (McMahan et al.,
2017). In current FL algorithms, the model size is limited by the resource constraints of clients.
Consequently, these algorithms are not feasible when dealing with large machine learning models that
cannot fit in clients memory or require large compute. Compared to these vanilla FL algorithms, split
learning-based approaches enables learning on resource-constrained clients at a cost of additionally
exposing the label information of clients to the server.
Large Model Training in FL. To deploy large machine learning models on resource-constrained
clients, a few recent works proposed methods to reduce the effective model size on clients. For
example, Diao et al. (2020) varied the layer width on clients depending on their computational
capacities; and Horvath et al. (2021) proposed ordered dropout to randomly removing neurons in
the neural network. These methods are effective in reducing the width of the intermediate layers;
however, they still place the full classification layer (which is in proportion to the size of the output
space) on each client to compute the the client update. This may not be feasible for many clients
as the parameters in the classification layer of a model can dominate the total number of model
parameters (Krizhevsky, 2014).
Split Learning (SL). Split learning (SL) is another way of minimizing (1) without explicitly sharing
local data on clients (Vepakomma et al., 2018). In particular, SL splits the neural network model
into two parts on a layer basis. The first few layers (called client-side model parameterized by
wc) are shared across clients and the server, while the remaining layers (called server-side model
parameterized by ws) are only stored and trained on the server. Under this splitting, the original loss
function for a data sample ξ can be re-written as follows:
f(w; ξ) = h(Ws； u(Wc； ξ)),	∀ξ ∈ Di, ∀i ∈ I	⑵
where u is the client-side function mapping the input data to the activation space and h is the server-
side function mapping the activation to a scalar loss value. Training both the client- and server-side
models requires communicating client activations (i.e., the output of the client-side model, also called
smashed data in some literature) between the clients and the server. We further elaborate on a concrete
SL algorithm as a baseline in Section 3.
Communication-efficient SL and Model Parallel Training. Recently, He et al. (2020); Han et al.
(2021) proposed to add a classification layer on the client-side model in SL so that each client can
locally update its parameters. This can effectively reduce the communication frequency between
the clients and the server. However, this kind of method is not suitable for the settings where the
classification layer dominates the entire model size such that the clients may not have enough memory
space to store the entire classification layer.
Another relevant approach is reducing communication cost in model parallel (MP) training (Dean
et al., 2012). In MP training the worker nodes (dedicated machines instead of resource-constrained
client devices) also need to transfer activations of intermediate layers with each other. Gupta et al.
(2020) proposed to sparsify the activations according to the magnitude of each element and achieved
up to 20× compression ratio (uncompressed/compressed). However, in practice it might be tricky
to set the threshold for sparsification. Our proposed method explores an orthogonal dimension of
sparsification by clustering similar activations within the mini-batches and communicating only
the cluster centroids. Empirical results show that the proposed method can achieve up to 490×
compression ratio without a significant drop in the model performance.
3
Under review as a conference paper at ICLR 2022
Product Quantization. Product quantization (PQ) is a compression method and has been widely
used in approximate nearest neighbor search (Jegou et al., 2010; Ge et al., 2013b). Given a batch of
vectors, PQ divides each vector into multiple chunks and performs K-means clustering separately on
each chunk. Now, the resulting cluster centroids construct the codebook and the closest codeword
will be used to represent each vector. Instead of directly computing the distance between two high-
dimensional vectors, computing the distance between two codewords is orders of magnitude more
efficient and faster. Note that the previous works used PQ to compress either learned features after
training (see, e.g., Ge et al., 2013a) or the parameters of a neural network (see, e.g., Chen et al., 2020).
In this paper, we show that PQ is also effective in compressing the output of intermediate layers and
present a simple technique to enable backpropagation through the corresponding quantization layer.
3	Baseline: the S plitFed Algorithm
In this section, we review a representative FL algorithm based on split learning, namely
SplitFed (Thapa et al., 2020), that provides a baseline training approach in resource-constrained
setting. Each iteration of SplitFed contains four steps detailed as follows. Wherever it simplifies
the presentation, we assume that the mini-batch size B on each client is 1 as the derivations hold true
for arbitrary batch sizes.
1.	Client Forward Pass: Let S be a randomly selected subset of clients. For i ∈ S, compute the
output of the client-side model zi = u(wc; ξ), where ξ ∈ Di is a randomly chosen training sample,
and zi ∈ Rd denotes the activations of the last layer of the client-side model. Then, each selected
client sends zi (together with its corresponding label if necessary) to the server.
2.	Server Update: The server treats all the activations (also referred to as smashed data)
{zi }i∈S as inputs to perform one step of gradient descent on the server-side model: ∆ws =
ηs Pi∈sPiNwsh(ws； Zi)/Pi∈s pi, where η denotes the server learning rate.1 In addition, the
server also computes the gradient with respect to the activations Nzi h(ws； Zi) and sends it back
to the corresponding client i.
3.	Client Backward Pass: Each selected client computes the gradient with respect to the client-side
model using the chain rule: gi , Nwcf = Nzi h(ws; Zi)Nwcu(wc; ξ).
4.	Client-side Model Synchronization: At last, the client-side model is updated by synchronizing
client gradients ∆wc = ηc Pi∈S pigi/ Pi∈S pi, where ηc denotes the learning rate for the
client-side model.
One can easily validate that, with the above four steps, SplitFed is equivalent to mini-batch
stochastic gradient descent (SGD) with a total batch size of B × |S |. No matter how the network is
split, the performance of the algorithm remains unchanged and has the same iteration complexity as
mini-batch SGD.
Besides, note that, a selected client needs to upload both the activations during “Client Forward
Pass” and the gradients of the client-side model during “Client-side Model Synchronization”. Thus,
the communication size per client per iteration can be written as |wc | + Bd. Since the up-link
communication bandwidth is quite limited for clients in federated learning, when the mini-batch size
B or the dimension of the activation d gets large, the up-link communication may become the main
bottleneck that limits the scalability of SplitFed. It is critical to compress the uploaded message
from clients to the server (Reisizadeh et al., 2019; Li et al., 2019). Next, we present a novel vector
quantization-based FL training framework that addresses this limitation of SplitFed to enable
communication-efficient training in resources-constrained settings.
4	Proposed Method: FedLite
We now introduce our proposed method, FEDerated spLIT learning with vEctor quantization
(FedLite), which can drastically reduce the up-link communication cost on clients in the S plitFed
algorithm. The key idea of our method is to compress the redundant information in the client
activations for each mini-batch. For instance, if we assume that all the activation vectors within a
1The server learning rate here has a different definition from previous FL literature, e.g., Reddi et al. (2020).
4
Under review as a conference paper at ICLR 2022
(i) Divide each vector
into q subvectors
Figure 2: Illustration of our proposed quantizer. Given a mini-batch of activations Z ∈ Rd×B, there
are three steps: (i) divide each activation vector into q subvectors; (ii) stack subvectors into R groups
based on their indices; (iii) perform K-means clustering to get L centroids for each group. Each
subvector can be represented by the index of the closest centroid in its corresponding group. On the
server, by simply rearranging centroids, we can get the quantized activations Z.
Original
activations Z
(ii) Stack subvectors
into R groups
qB/R
Quantized
(iii) Assign each subvector
to a cluster centroid
mini-batch are nearly identical to each other, then instead of sending all B activation vectors, the
client needs to send only one representative vector to the server, thereby, reducing the communication
cost by a factor of B .
Building on the above observation, we let each client input their activations into a clustering algorithm
to get L centroids. Then, each activation vector is represented by the index of its closest centroid.
Instead of sending a mini-batch of B vectors, only L centroids and the cluster assignments are
transmitted to the server. This procedure is equivalent to vector quantization. Here, a simple choice
of the clustering algorithm is K-means (Krishna & Murty, 1999). However, as we observe in Figure 3,
vanilla K-means clustering leads to a high quantization error with very limited compression ratio. It
is non-trivial to design a proper clustering algorithm that can minimize the communication between
clients and server, while maintaining a low quantization error. In Section 4.1, we design a novel
variant of product quantizer that can flexibly operate this trade-off.
While the above approach is appealing, it also introduces new challenges. First, note that, in the
backward pass, due to the additional quantization layer, the server no longer knows the true client
activations; thus, constraining the server to compute the gradients with respect to the (possibly noisy)
quantized activations. Furthermore, in order to update the client-side model parameters, the clients
need to receive the gradients with respect to the original activations, which is not available any more.
In Section 4.2, we introduce a gradient correction technique to mitigate this gradient mismatch, which
is critical for our method to achieve a high compression ratio with minimal accuracy loss.
In the following sections, we present the key components of our proposed method and highlight
how they address the aforementioned challenges. In addition, a convergence analysis is provided in
Section 4.3. Unless otherwise stated, for the ease of exposition, we will focus on the operations on a
specific client i and omit the client index. However, it is important to keep in mind that the client-side
operations happen in parallel on all selected clients.
4.1	Forward Pass: Compressing Activations with Product Quantization
In this subsection, we first present our proposed qunatizer and then elaborate on its advantages. A
visual illustration is provided in Figure 2.
The Proposed Compression Scheme. Suppose a client has computed a batch of activations
Z = [z(1), . . . , z(B)] ∈ Rd×B using the client-side model wc, where d is the dimension of
each activation and B denotes the mini-batch size on each client. Our proposed quantizer first
divides each activation vector z(j) , j ∈ [1, B] into q subvectors with equal dimension d/q. We
5
Under review as a conference paper at ICLR 2022
Figure 3: Quantization error (lower is better) versus compression ratio (larger is better). Our proposed
quantizer achieves a better quantization error vs. compression ratio trade-off as compared to vanilla
product quantization (PQ) and K-means. In this example, the activation size is d = 9216, and the
mini-batch size is B = 20. The activations are trained via a two-layer CNN on Federated EMNIST. In
each curve, we vary the number of clusters L. For green curves, the number of subvectors q takes value
in {288, 1152, 4608}; for red curves, the number of groups R takes value in {2304, 1152, 384, 1},
and q is fixed as 4608.
denote the s-th subvector of z(j) as z(j,s) ∈ Rd/q. Then, the quantizer stacks all subvectors into
R groups based on their indices. For example, the first group G(1) contains all subvectors with
indices (j, 1), (j, 2), . . . (j, q/R), ∀j ∈ [1, B]; the second group G(2) contains all subvectors with
indices (j, q/R + 1), (j, q/R + 2), . . . , (j, 2q/R), ∀j ∈ [1, B], and so forth. At last, K-means clus-
tering is performed on subvectors within each group G(r) , ∀r ∈ [1, R] to find L cluster centroids
C(r) = {c(r,1), . . . , c(r,L)} ⊂ Rd/q. Now each subvector z(j,s) can be approximated by its closest
centroid in the corresponding group. Formally, if z(j,s) ∈ G(r), then
e(j,s)，c(r,lj,S)), where l(j,s) = arg min kz(j,s) - c(r,l)k2	(3)
l∈[1,L]
In the above equation, ze(j,s) represents the quantized version of z(j,s). By concatenating all quantized
subvectors, server can get the quantized activations Z and use it as input to update the server-side
model. In order to obtain the quantized subvectors, each client needs to send all cluster centroids
{C(r)}r∈[i,R] (referred to as the codebook) and the cluster assignments {li(j,s'i}j∈[i,B],s∈[i,q] (referred
to as the codewords) to the server. Assuming that each floating-point number occupies φ bits2, the
transmitted message size per client is reduced to φdRL∕q + Bq log? L from φdB.
The Benefit of Subvector Division: Reduced Quantization Error. The first key component of our
proposed quantizer is subvector division, i.e., setting the number of subvectors q to be greater than 1.
When q = 1, the quantizer reduces to vanilla K-means. In this case, the codebook size is φdL and
each activation vector have L possible choices. When we set q = R > 1, the codebook size is still
φdL. However, each activation vector now has Lq possible choices, as each of q subvectors has L
choices. Thus, the number of quantization levels becomes exponentially larger without any increase
in memory usage and computation complexity. As illustrated by the green lines in Figure 3, having
more quantization levels or effective centroid choices can significantly lower the quantization error.
The Benefit of Subvector Grouping: Improved Compression Ratio. By using subvector division,
although the quantization error can be reduced, the codebook size φdL is still pretty large. This is
because subvectors within one vector are quantized separately using different codebooks. To further
reduce the communication size, we propose subvector grouping and force subvectors within one
group to share the same codebook. As a result, the total codebook size can be reduced to φdLR∕q.
When q R ≥ 1, there can be an order of magnitude increase in the compression ratio, as illustrated
by the red lines in Figure 3. One can also observe that, by changing the value of R, our proposed
quantizer (red lines) achieves a much better error-versus-compression trade-off than K-means and
vanilla production quantization scheme.
Why not Reuse the Codebooks from Previous Iterations? In our proposed scheme, the codebook
is reconstructed at each iteration on clients. This is necessary, since in FL, only few stateless clients are
selected to participate training in at each iteration and they have non-IID data distributions (Kairouz
et al., 2019). Previous codebooks can be stale and other clients’ codebooks may not be suitable.
2When computing the compression ratio in this paper, we always assume φ = 64.
6
Under review as a conference paper at ICLR 2022
Now that we have introduced our product quantization-based compression scheme, we next address
the issue of performing backpropagation in the presence of the additional quantization layer.
4.2	Backward Pass: Gradient Correction
As mentioned at the beginning of Section 4, there is a gradient mismatch problem in the client
backward pass. While client i needs Nzih(ws； Zi) to update the client-side models, it can only
receive RZi h(ws； z) from the server. A naive solution is to treat RZi h as an approximation of Nzi h
and update the client-side model by NZih(ws； ei)Vwcu(wc； ξ) = (∂f∕∂ei)(∂zi∕∂Wc). However,
due to gradient mismatch, this approach can lead to a significant performance drop, as observed in our
experiments (cf. Section 5). In order to address this issue, we propose a gradient correction technique.
In particular, we approximate the gradient Nzih(ws; zi) by its first-order Taylor series expansion:
NZih(ws； zi) + NZah(ws； zi) ∙ (zi 一 "i). While the higher-order derivative may be expensive to
compute, we use a scalar parameter λI > 0 to replace N2zZ h(ws; ze)i. Consequently, the gradient of
the client-side model is defined as follows:
e，[、包 + Hz,-zi)]驾WW.	(4)
∂zi	∂wc
In Section 4.3, we will provide a convergence analysis that can help explain the effects of gradient
correction; in Section 5, we will empirically show that setting a strictly positive λ is crucial for the
success of the proposed method. In the following discussion, we provide an intuitive explanation of
the correction to further motivate our approach.
The Effect of Regularization. We show that the client-side gradient in (4) can be considered as a
gradient of the following surrogate loss (proof is provided in the Appendix):
Ilzi - zik2 + 2 Ilzi 一 Zill2,	(5)
where Zi = u(wc； ξ) is the activation of the client-side model. Besides, Zi，Zi 一 NZi h(ws； zi)∕2
and the quantized activation zei are fixed vectors that do not have derivatives when computing the
gradient. Setting λ > 0 is equivalent to having a regularization term, as per (5). The regularizer
encourages the client-side model wc to move in a direction that can decrease the quantization error
k Zi — zi k. Interestingly, using a larger value of λ may let clients put more emphasis on minimizing the
quantization error and lead to quantization-friendly activations. However, one cannot set a arbitrarily
large value for λ because the client-side model may output the same activation for all inputs and fail
to minimize the original loss function.
4.3	Convergence Analysis
In this subsection, we provide a convergence analysis for FedLite. The analysis will highlight how
the quantization error influences the convergence and how the gradient correction technique helps. In
particular, the analysis is conducted under standard assumptions for mini-batch SGD. We assume
the objective function F (w) = F([wc; ws]) is L-Lipschitz smooth (i.e., INF (w) 一 NF (v)I ≤
L Iw 一 vI), and stochastic gradient g(w) has bounded variance EIg(w) 一 NF (w)I2 ≤ σ2∕BS,
where B is the mini-batch size per client and S is the number of selected clients per iteration. Under
these assumptions, we have the following theorem.
Theorem 1 (Convergence of FedLite). Ifthe client-side model and server-side model update using
the same learning rate a = BBS/T where T is the number of total iterations, then the expected
gradient norm of the global function mint∈[0,T -1] E NF(w(t))2 can be bounded by
BbTt
|---------------
Opt. error of mini-batch SGD
4(F (w(0) ) 一 Finf)
|
(6)
Addnl. error caused by quantization
}
where κ is the maximal quantization error max IZ 一 ZzI, λ is the tunable parameter in our gradient
correction scheme, Finf is the lower bound of function value, and constants Λ1 , Λ2 , Λ3 are the largest
eigenvalues ofmatrices ∂2h(ws; z)∕∂ZdWs, ∂2h(ws; z)∕∂Z2,∂u(wC； ξ)∕wc, respectively.
7
Under review as a conference paper at ICLR 2022
Theorem 1 guarantees that the proposed algorithm FedLite converges to a neighbourhood around
the stationary point of the global function. And the size of this neighborhood is in proportion to the
maximal quantization error κ during the training. When there is no quantization, the error bound (6)
recovers that of mini-batch SGD. Moreover, one can observe that setting a positive λ can help to
reduce the additional error caused by adding the quantization layer.
5	Experiments
We implement the proposed method FedLite using FedJAX (Ro et al., 2021) and evaluate its
effectiveness on three standard federated datasets provided by the Tensorflow Federated (TFF)
package (TFF, 2021): (i) image classification on FEMNIST, (ii) tag prediction on StackOverflow
(referred to as SO Tag) and (iii) next word prediction on StackOverflow (referred to as SO NWP). On
FEMNIST, the same model architecture as (Reddi et al., 2020) is adopted. We place two convolutional
layers and two dense layers on the clients and the server, respectively. With this splitting, the client-
side model only has about 1.6% trainable parameters of the entire model. Therefore, the client-side
resource requirement is significantly reduced. On SO Tag, both the client- and server-side models
contains only one dense layer. On SO NWP, we place one LSTM layer and one dense layer on the
clients, and another dense layer on the server. The ratios between the client-side model size and entire
model size are 83% and 79% on SO Tag and SO NWP, respectively. Here, we note that even though
the client-side model sizes for SO Tag and SO NWP do not correspond to an ideal setting for the split
learning-based approaches, we include these two datasets to showcase the utility of our proposed
method for language-based tasks.
For each task, we select the learning rate that is best for the baseline SplitFed algorithm. Although
separately tuning the learning rate for our proposed method can further improve its performance,
using the same learning rate as SplitFed already demonstrates the advantages of our method. Unless
otherwise stated, the number of groups R in our proposed quantizer is set to 1, as it exhibits the best
trade-off in Figure 3 and experiments.
Main Results: Effectiveness of FedLite. As discussed in Section 4.1, there is a trade-off between
the final performance and compression ratio. Although the additional quantization layer helps reduce
the communication cost, it also causes performance drop. While utilizing the quantization layer, it is
important to understand the range of the compression ratio that does not degrade the final performance
too much. To this end, we vary the number of clusters L and number of subvectors q in our proposed
method and report the resulting accuracy vs. compression trade-off in Figure 4. Note that we define
the compression ratio to be the ratio between the original activation size and compressed message
(codebook + codewords) size.
One can observe that the proposed method can achieve at least 10× compression ratio with almost
no accuracy loss. Furthermore, if we allow for 5% relative loss compared to SPLITFED, then our
method can achieve a 490× compression ratio on FEMNIST. This suggests that 99.8% information
is redundant and can be dropped during the up-link communication. Surprisingly, on SO Tag, the
Recall@5 even improves when adding the quantization layer. We conjecture that compressing the
outputs of a intermediate layer might have similar effects to dropout.
Overall Communication and Computation Efficiencies. Here, we provide a concrete example
showing how the proposed method improves the communication and computation efficiencies over
previous works. On FEMNIST, setting q = 1152 and L = 2 amounts to 490× compression ratio,
which amounts to a 490× reduction in the additional communication introduced by the network
splitting. Compared to SplitFed, the overall up-link commutation cost (including the client-side
gradients synchronization) is about 10× smaller. Compared to FEDAVG, the up-link communication
cost per round is reduced by 62×, with 64× less trainable parameters on the clients. We further
compare the training curves with respect to total communication costs of FedLite against SplitFed
and FedAvg in Figure 6 in Appendix. Besides, the overall wall-clock time saving may depend on the
characteristics of the training system. One can get an estimate using the analytical model in (Agarwal
et al., 2021).
Effectiveness of the Gradient Correction. From Figure 4, it is easy to observe that the gradient
correction technique (i.e., λ > 0) is crucial for improving performance. Without correction, the
algorithm can even diverge in the high compression ratio regime. While λ is separately tuned for
8
Under review as a conference paper at ICLR 2022
(a) FEMNIST
(b) SO Tag
ιoo ioɪ ιo2
Compression ratio
(c) SO NWP
Figure 4:	Trade-off between the accuracy and compression ratio. Our proposed method can achieve
up to 490×, 247×, and 51× communication reduction with less than 5% accuracy drop on FEMNSIT,
SO Tag, and SO NWP, respectively. Each curve in the figures corresponds to one value of q (number
of subvectors); and each point on a curve corresponds to a specific value of L (number of clusters).
For our method, the number of groups R is fixed as 1.
SJOZPα,>qns #
.84
-82
80
78
-76
-74
-72
-70
76.732
//F染城 y
SJOtJφ>qns ⅛ H σ
Accuracy
L = # Clusters
74.2 63.0	5.1
16	8	4
L = # Clusters
5.1
2
84
82
80
78
76
-74
-72
-70
(a)λ=5 × 10-5	(b)λ=0	(c) Quantizer comparison
Figure 5:	Ablation studies on FEMNIST. (a) and (b): Validation accuracy on FEMNIST when fixing
λ and varying q and L. Setting a small positive value for λ improves accuracy for almost all (q, L)
pairs; (c) Thanks to subvector grouping, our proposed quantizer can achieve an order of magnitude
larger compression ratio as compared to vanilla product quantization scheme.
each point (i.e., each (q, L) pair) in Figure 4, we found that fixing λ for all (q, L) pairs still leads
to significant improvements. In Figure 5, we report the performance on FEMNIST for various
choices of q and L. In particular, with q = 288, the accuracy improvement ranges from 3 - 72%.
Furthermore, we found that a small λ ranging from 10-5 to 10-3 works well on all training tasks in
our experiments. Setting a larger λ leads to near-zero quantization error. But the model may have
poor performance, as it tends to output the same activation for all inputs and becomes meaningless.
Effectiveness of the Proposed Quantizer. In Section 4.1, we already show that stacking subvectors
into groups (i.e., setting R < q) is critical to reach a high compression ratio. But how does it affect
the model performance? To answer the question, we run the proposed method with R = q > 1
(vanilla PQ), and report the results in Figure 5c. Observe that the proposed method significantly
improves the compression ratio with minimal loss of accuracy.
6 Conclusion
In this paper, we studied the problem of training large machine learning models in federated learning
setting with resource-constrained clients. Due to limitations on storage and/or compute capacity,
the clients cannot locally optimize the entire model. This prohibits the usage of previous federated
learning algorithms. Split neural network is a promising approach to address this issue, as it only
assigns the first few (small) layers of a neural network to the clients. However, the network splitting
incurs additional communication. In order to make split neural network training to be communication-
efficient, we propose an end-to-end training framework FedLite that can compress the additional
communication by up to 490× with minimal loss of accuracy. The success of our method relies
on a variant of product quantization scheme and a gradient correction technique. We perform
theoretical analysis as well as extensive experiments on both vision and language tasks to validate its
effectiveness.
9
Under review as a conference paper at ICLR 2022
References
Saurabh Agarwal, Hongyi Wang, Shivaram Venkataraman, and Dimitris Papailiopoulos. On the
utility of gradient compression in distributed training systems. arXiv preprint arXiv:2103.00543,
2021.
Rahul Agrawal, Archit Gupta, Yashoteja Prabhu, and Manik Varma. Multi-label learning with
millions of labels: Recommending advertiser bid phrases for web pages. In Proceedings of the
22nd international conference on World Wide Web, pp. 13-24, 2013.
Ting Chen, Lala Li, and Yizhou Sun. Differentiable product quantization for end-to-end embedding
compression. In International Conference on Machine Learning, pp. 1617-1626. PMLR, 2020.
Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations.
In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in
neural information processing systems, pp. 1223-1231, 2012.
Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient feder-
ated learning for heterogeneous clients. In International Conference on Learning Representations,
2020.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate
nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2946-2953, 2013a.
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization. IEEE transactions
on pattern analysis and machine intelligence, 36(4):744-755, 2013b.
Vipul Gupta, Dhruv Choudhary, Ping Tak Peter Tang, Xiaohan Wei, Xing Wang, Yuzhen Huang,
Arun Kejariwal, Kannan Ramchandran, and Michael W Mahoney. Fast distributed training of deep
neural networks: Dynamic communication thresholding for model and data parallelism. arXiv
preprint arXiv:2010.08899, 2020.
Dong-Jun Han, Hasnain Irshad Bhatti, Jungmoon Lee, and Jaekyun Moon. Accelerating federated
learning with split learning on locally generated losses. ICML’21 workshop on federated learning
for User Privacy and Data Confidentiality, 2021.
Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group knowledge transfer: Federated
learning of large cnns at the edge. Advances in Neural Information Processing Systems 33 (NeurIPS
2020), 2020.
Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos I Venieris, and
Nicholas D Lane. Fjord: Fair and accurate federated learning under heterogeneous targets with
ordered dropout. arXiv preprint arXiv:2102.13451, 2021.
Yangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes for
private distributed learning. In International Conference on Machine Learning, pp. 4507-4518.
PMLR, 2020.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search.
IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2010.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
10
Under review as a conference paper at ICLR 2022
K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems,
Man, and Cybernetics, PartB (Cybernetics), 29(3):433-439,1999.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint
arXiv:1404.5997, 2014.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances
in neural information processing systems, 27:2177-2185, 2014.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. CoRR, 2019.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. In Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics, 2017.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quanti-
zation. arXiv preprint arXiv:1909.13014, 2019.
Jae Hun Ro, Ananda Theertha Suresh, and Ke Wu. Fedjax: Federated learning simulation with jax.
arXiv preprint arXiv:2108.02117, 2021.
Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe, Abbas Ismail, Tudor
Cebere, Robert Sandmann, Robin Roehm, and Michael A Hoeh. Pyvertical: A vertical federated
learning framework for multi-headed splitnn. arXiv preprint arXiv:2104.00489, 2021.
TFF. TensorFlow Federated: machine learning on decentralized data. https://www.
tensorflow.org/federated, 2021. Accessed: 30 September 2021.
Chandra Thapa, Mahawaga Arachchige Pathum Chamikara, and Seyit Camtepe. Splitfed: When
federated learning meets split learning. arXiv preprint arXiv:2004.12088, 2020.
Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health:
Distributed deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564,
2018.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021.
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek,
and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance
analysis. IEEE Transactions on Information Forensics and Security, 15:3454-3469, 2020.
11
Under review as a conference paper at ICLR 2022
A Detailed Explanation for the Effects of Regularization
In Section 4.2, we argue that the client-side gradient is a gradient of the following surrogate loss
function
S(Wc) = l∣Zi - Zik2 + 2 l∣Zi - eik2	(7)
where Zi，Zi 一 Rzih(ws； Zi)/2 and the quantized activation Zi are fixed vectors that do not have
derivatives when computing the gradient. Here, we are going to provide a formal proof to support
this argument. Directly taking the derivative, we have
∂s(Wc)
∂Wc
=2(Zi- Zi) ∂Wc+ λ(zi- ei) ∂Wc
(Vzih(ws; Zi) + λ(Zi - Zi)) dZi-.
∂wc
(8)
(9)
The above equation is exactly the same as the client-side gradient we defined in (4).
B Proof of Theorem 1
B.1 Preliminaries
Without loss of generality, we define w = [wc ; ws]. Then, the gradient of the original loss function
can be decomposed into two parts:
VF (w) = [Vwc F (w); Vws F (w)]	(10)
Similarly, we denote the stochastic gradients as follows:
g(w) = [gc(w); gs(w)]	(11)
where E[gc(w)] = VwcF (w) and E[gs(w)] = Vws F (w). In addition, we assume the stochastic
gradients with a mini-batch size of BS have bounded variance:
Ekg(w) -VF(w)k2 ≤ 黑.	(12)
BS
That is equivalent to
Ekgc(W)- VwCF (w)k2+Ekgs(W)- VwsF (w)k2 ≤ M	(13)
BS
Then, we denote the client- and server-side gradients in the presence of the quantization layer as
follows:
gZc(W) = gc(W) + δc(W)	(14)
gZs (W) = gs (W) + δs(W).	(15)
The concrete expressions of δc and δs will be derived later.
B.2 Main Proof
With the above notation, according to the Lipschitz smoothness of the loss function, we have
E[F(W(t+1))] -F(W(t)) ≤ - α DVwc F (W(t)), E[gZc(W(t))]E - α DVws F (W(t)), E[gZs(W(t))]E
+ α2LE Mc(w(t))∣2 + α2LE Ms(W('))『	(16)
For the first term on the right-hand side of (16), we have
-DVwcF(W(t)),VwcF(W(t))+E[δc(W(t))]E
≤ -IlVwCF(W⑴)『+ ɪ IlVwCF(W⑴)『+ I 型[δc(w㈤)]『	(17)
≤- (1 - 2I) IVwCF(W(t))∣2 + IE Mc(W(t))||2	(18)
=-1 ∣∣VwcF(W㈤)∣∣2 + 1E ∣∣δc(W㈤)∣∣2	(19)
12
Under review as a conference paper at ICLR 2022
where (17) uses Young’s Inequality, and > 0 is an arbitrary positive number. We set = 1 right
after (17). For the third term on the right-hand side of (16), we have
E gc(w(t)) + δc(w(t))2 ≤2E gc(w(t))2 + 2E δc(w(t))2	(20)
For the server-side gradients ges, we can repeat the same process. Combining them together, it follows
that
E[F(w(t+1))] - F(W㈤)≤ - α ∣∣VF(w(t))∣∣2 + α2LE ∣∣g(w(t))∣∣2
+ 2α2 L(χ2 + χ2) + 2(X2 + χ2)
where
χc2 =maxE kδc(w)k2 ,
χ2s =maxE kδs(w)k2 .
Using the assumption that the stochastic gradient has bounded variance, we obtain that
E[F(w(t+1))] - F(w(t)) ≤- α(1 - αL) ∣∣VF(W㈤)∣∣2 + ^^
+ 2a2L(X2 + χ2) + 2(X2 + χ2)∙
When αL ≤ 1/4, we have
E[F (w(t+1))] — F (w(t)) ≤-4 ∣∣ VF (W㈤)∣∣2 + α2Lσ2 + 2α2L(χ2 + Xs)
+ 2(Xc + χ2)∙
With minor rearranging and taking the total expectation, it follows that
E ∣∣∣VF (W(t))∣∣∣2
4E[F(w⑶)—F (w(t+1))]
≤	α	~"
+	俞—+(2 + 4αL)(X2 + X2).
BS
Taking the average from t = 0 to t = T - 1, obtain that
1 T-IMILj W∖∣∣2	4(F(W(O)) - F(w*))	4αLσ2	, Z 2	2、
T £ E ∣∣ VF(w(t))∣∣ ≤ —-ατ ~^ + ~BS~ + (2 + 40L)(X2 + XS).
T t=0	αT BS
(21)
(22)
(23)
(24)
(25)
(26)
(27)
If α = YBSlT ,then
T X1E ∣∣ VF(w(t))∣∣2 ≤4；(S) + √St + (T + 2) (X2 + XSS
Next we are going to show how Xc , XS relate to the quantization error. Suppose that Bi(t) denotes the
randomly sampled mini-batch on client i. Then according to the definition of server-side gradient, we
have
1 X 1 X ∂h(ws; Q(U(Wc； ξ)))
|S(t)|	|B(t)|	∂Ws
i∈S(t) |Bi | ξ∈B(t)	Ws
=gS (W) +
∣s (t)l
Σ
i∈S(t)
ɪ X
∣B(t)l 乙
ξ∈Bi(t)
∂h(Ws; Q(u(Wc; ξ)))	∂h(Ws; u(Wc; ξ))
--------:-------------------:-------
∂Ws	∂Ws
(30)
1
13
Under review as a conference paper at ICLR 2022
where Q represents the quantizer and maps the original activation to its quantized version. As a
consequence,
and
δs(w)
1
∣s(t)i
∂h(ws; Q(U(wc； ξ)))
∂Ws
∂h(ws; u(wc; ξ))
∂Ws
(31)
kδs(w)k2 ≤ max
ξ∈Di,i∈I
∂h(ws; Q(u(wc; ξ)))	∂h(ws; u(wc; ξ)) 2
-------:-----------	:--------
∂ws--------------∂ws
(32)
Σ
g，
For the ease of writing, we denote z = u(wc; ξ) and ze = Q(u(wc; ξ)). Using mean value theorem,
we have
I∣δs(w)k2 ≤ max d hws U) (e - Z)	(33)
∂u∂ws
≤Λ12κ2	(34)
where u = tz + (1 - t)ze for some 0 < t < 1, constant Λ1 is the largest eigenvalue of matrix
∂2h(ws； u)∕∂u∂ws, and K = max ∣∣e — Zk denotes the maximal quantization error.
Using the same technique, for the client-side gradients, we have
δ ( ) = 1 X 1 X ∂ ∂h(ws; Q(U(wc； ξ))) _ ∂h(ws; u(wc$) ∖ ∂u(wc[ξ)
c w	IS㈤L-⅛) ∖B(t')∖	(t) ∖	dQ(U(Wc； ξ))	du(wc； ξ))	)	dwc
i∈S(t)	i ξ∈B(t)
+ ⅛ X / X λ(U(Wξ)-Q(u(wc;ξ)))du∂W&
i∈S(t) ∖Bi ∖ ξ∈B(t)	c
Accordingly,
kδs(w)k2 ≤max J (dh(weJ2 - ^Wz
+λ(z - zɔ) ∂w
max
Il ( ∂2h(ws; U)
Il I ∂u2
-λ)(e -Z) ∂wc
≤(Λ2 - λ)2 Λ32 κ2
(35)
(36)
(37)
(38)
2
2
where Λ2 and Λ3 are the largest eigenvalues of matrices ∂2h(ws; U)∕∂U2 and ∂Z∕∂wc, respectively.
Substituting the above bounds (34) and (38) on δc , δs into (28), we have
ɪ X1E JVF(W(W ≤4(F(W(02-F(W*)) +≠t
T t=0 Il ll -	√BST	√BST
+ (	√T	+2)。2	+	(λ2	-	χ)2λ3) max Hz -	ek2.	(39)
Here we complete the proof.
C More Experimental Details
C.1 Additional Results
We additionally report the training curves of FedAvg, FedLite and SplitFed on EMNIST in Figure 6.
It can be observed that in terms of communication cost, FedLite is significantly faster than other two
baselines. We also need to highlight that FedLite and SplitFed have lower memory and computation
requirements on clients than FedAvg.
14
Under review as a conference paper at ICLR 2022
Figure 6: Training curves for different algorithms on FEMNIST. Although split learning-based
approaches (SplitFed and ours FedLite) communicate at every iteration, they communicate much less
than FedAvg.
C.2 Hyper-parameter Choices
Here, we summarize all the hyper-parameters on three training tasks.
FEMNIST
•	Best learning rate: 10-1.5
•	Optimizer: SGD
•	Mini-batch size per client B : 20
•	Activation size d: 9216
•	Number of clients per iteration: 10
•	Ranges of q: {4608, 2304, 1152, 576, 288, 144}
•	Ranges of L: {32, 16, 8, 4, 2}
•	Ranges of λ: {0, 10-5, 5 × 10-5, 10-4, 5 × 10-4}
•	Client-side model: Same as the first five layers of the neural network used in (Reddi et al.,
2020): Conv2d + Conv2d + MaxPool2d + Dropout + Flatten. Model size: 18, 816 × 64 bits.
•	Server-side model: Same as the last three layers of the neural network used in (Reddi et al.,
2020): Dense + Dropout + Dense. Model size: 1, 187, 774 × 64 bits.
SO NWP
•	Best learning rate: 0.01
•	Optimizer: Adam (Kingma & Ba, 2014)
•	Mini-batch size per client B: 128. Each sample contains 30 words. So the effective batch
size is 3840.
•	Activation size d: 96
•	Number of clients per iteration: 50
•	Ranges of q: {48, 24, 12, 6, 3}
•	Ranges of L: {960, 480, 240, 120, 60, 30}
•	Ranges of λ: {0,5 × 10-4,10-3,5 × 10-3, 10-2}
•	Client-side model: Same as the first three layers of the neural network used in (Reddi et al.,
2020): Embedding + LSTM + Dense. Model size: 3, 680, 360 × 64 bits.
•	Server-side model: Same as the last layer of the neural network used in (Reddi et al., 2020):
Dense. Model size: 970, 388 × 64 bits.
SO Tag
•	Best learning rate: 10-0.5
15
Under review as a conference paper at ICLR 2022
•	Optimizer: AdaGrad (Duchi et al., 2011)
•	Mini-batch size per client B: 100.
•	Activation size d: 2000
•	Number of clients per iteration: 10
•	Ranges of q: {1000, 500, 250, 200, 125, 25}
•	Ranges of L: {100, 60, 40, 20, 10}
•	Ranges of λ: {0, 10-3, 5 × 10-3, 10-2 , 5 × 10-2}
•	Client-side model: One dense layer. Model size: 5000 × 2000 × 64 bits.
•	Server-side model: One dense layer. Model size: 2000 × 1000 × 64 bits.
16