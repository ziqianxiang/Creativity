Under review as a conference paper at ICLR 2022
Adversarially Robust Models may not Trans-
fer Better: Sufficient Conditions for Domain
Transferability from the View of Regulariza-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Machine learning (ML) robustness and generalization are fundamentally corre-
lated: they essentially concern about data distribution shift under adversarial and
natural settings, respectively. Thus, it is critical to uncover their underlying con-
nections to tackle one based on the other. On one hand, recent studies show that
more robust (adversarially trained) models are more generalizable to other do-
mains. On the other hand, there lacks of theoretical understanding of such phe-
nomenon, and it is not clear whether there are counterexamples. In this paper, we
aim to provide sufficient conditions for this phenomenon considering different fac-
tors that could affect both, such as norm of the last layer, Jacobian norm, and data
augmentations (DA). In particular, we propose a general theoretical framework
indicating factors that can be reformed as a function class regularization process,
which could lead to improvements of domain generalization. Our analysis, for the
first time, shows that “robustness” is actually not the causation for domain gener-
alization; rather, robustness induced by adversarial training is a by-product of such
function class regularization. We then discuss in details about different properties
of DA and we prove that under certain conditions, DA can be viewed as regular-
ization and therefore improve generalization. We conduct extensive experiments
to verify our theoretical findings and show several counterexamples where robust-
ness and generalization are negatively correlated when the sufficient conditions
are not satisfied.
1 Introduction
Domain generalization (or transferability) is the task of training machine learning models with
data from one or more source domains that can be adapted to a target domain, often via low-
cost fine-tuning. Thus, domain generalization refers to approaches designed to address the
natural data distribution Shift problem (MUandet et al., 2013; Rosenfeld et al., 2021). A wide array of
approaches have been proposed to address domain transferability, including fine-tuning the last layer
of DNNs (Huang et al., 2018), invariant feature optimization (Muandet et al., 2013), efficient model
selection for fine-tuning (You et al., 2019), and optimal transport based domain adaptation (Courty
et al., 2016). Improving domain generalization has emerged as an important task in the machine
learning community: for instance, it is among the key technologies to enable an autonomous driving
vehicle trained in city scenarios to make correct decisions in the countryside as well.
On the other hand, robust machine learning aims to tackle the problem of
adversarial data distribution shift. Both empirical and certified robust learning approaches
have been proposed, such as empirical adversarial training (Madry et al., 2018) and certified
defenses based on deterministic and probabilistic approaches (Cohen et al., 2019).
As domain transferability and robust machine learning tackle different kinds of data distribution
shifts, this work seeks to uncover their underlying connections and tradeoffs. For instance, recent
studies suggest that adversarially robust models are more domain transferable (Salman et al., 2020),
which, in turn, provides new insights on improving domain generalization. However, a theoretical
analysis of their relationship is still lacking, and it is unclear whether such positive correlations al-
1
Under review as a conference paper at ICLR 2022
SUffident conditions for domain transferability
(a) Data augmentations (b) Jacobian norm (c) Last-Layer norm
KH■小、什 H
Source
Domain
Feature Extractor
Last Layer
Adversarially robust models may
not transfer better
(b)
Figure 1: Illustration of robustness and domain transferability in different conditions.
ways hold. In this paper, we take the first steps towards formally analyzing the relationship between
model robustness and domain transferability to answer the following questions: What are sufficient
conditions for domain transferability? Is model robustness the cause of domain transferability? Can
robustness and domain transferability be negatively correlated?
To answer the above questions and uncover the underlying relationship between robustness and
domain transferability, we propose a general theoretical framework that characterizes sufficient con-
ditions for domain transferability from the view of function class regularization. Our analysis shows
that if the function class of feature extractors is more regularized, the model based on a feature
extractor trained from the function class, composed with a fine-tuned last layer, can be more trans-
ferable. Formally, we prove that there is a monotone relation between the regularization strength
and a tight upper bound on the relative domain transfer loss.
Under the proposed framework, we analyze several common factors for model training, including
the Jacobian norm, the last layer norm, data augmentation, and adversarial training as shown in
Fig. 1. In particular, controlling the Jacobian norm and last layer norm can be viewed as function-
class regularization, thus can be analyzed in our framework. We also analyze how other common
regularization operations can be mapped to function class regularization. For instance, we consider
noise-dependent and independent data augmentation procedures based on feature average and loss
average aggregation algorithms.
We conduct extensive experiments on ImageNet (CIFAR-10 as target domain) and CIFAR-10
(SVHN as target domain) based on different models to verify our analysis. We show that regulariza-
tion can control domain transferability, and robustness and domain transferability can be negatively
correlated, which are counter-examples against Salman et al. (2020). Taken together, this indicates
that robustness is not a cause of transferability.
Technical Contributions. We aim to uncover the underlying relationship between robustness and
domain transferability and lay out the sufficient conditions for transferability from the view of reg-
ularization. We make both theoretical and empirical contributions.
•	We propose a theoretical framework to analyze the sufficient conditions for domain transferability
from the view of function class regularization. We provably show that stronger regularization
on the feature extractor implies a decreased tight upper bound on the relative transferability loss;
while model robustness could be arbitrary.
•	We prove the tightness of our transferability upper bound, and provide the generalization bound
of the relative transferability loss from the view of regularization.
•	We analyze several factors such as different data augmentations (e.g., rotation and Gaussian) under
the framework, and show how they can be mapped to function class regularization and therefore
affect transferability.
•	We conduct extensive experiments on different datasets and model architectures to verify our theo-
retical claims. We also show several counterexamples that indicate significant negative correlation
between robustness and the relative domain transferability.
2	Related Work
Domain Transferability has been analyzed in different settings. Muandet et al. present a gener-
alization bound for classification task based on the properties of the assumed prior over training
environments. Rosenfeld et al. model domain transferability/generalization as an online game and
show that generalizing beyond the convex hull of training environments is NP-hard, and Zhang et al.
provides a generalization bound for distributions with sufficiently small H-divergence. Given the
2
Under review as a conference paper at ICLR 2022
complexity of domain transferability analysis, recent empirical studies show that adversarially ro-
bust models transfer better (Salman et al., 2020). In this paper, we aim to relax the assumptions and
focus on understanding the domain transferability from the view of regularization and theoretically
show whether “robustness” is indeed a causation for transferability or not.
Model Robustness is an important topic given recent diverse adversarial attacks (Goodfellow et al.,
2014; Carlini & Wagner, 2017). These attacks may be launched without access to model param-
eters (Tu et al., 2019) or even with the model predicted label alone (Chen et al., 2020). Different
approaches have been proposed to improve model robustness against adversarial attack. Adversar-
ial training has been shown to be effective empirically (Madry et al., 2018; Zhang et al., 2019a;
Miyato et al., 2018). Some studies have shown that robustness is property related to other model
characteristics, such as transferability and invertibility (Engstrom et al., 2019).
3	Sufficient Conditions for Domain Transferability
In this section, we theoretically analyze the problem of domain transferability from the view of
regularization and discuss some sufficient conditions for good transferability. All of the proofs are
provided in Section A in the appendix.
Notations. We denote the input space as X ; the feature space as Z and the output space as Y. Let
the fine-tuning function class be g ∈ G . Given a feature extractor f : X → Z and a fine-tuning
function g : Z → Y, the full model is g ◦ f : X → Y. We denote PX ×Y as the set of distributions
on X × Y. The loss function is denoted by ` : Y × Y → R+ , and the population loss based on data
distribution D ∈ PX ×Y and a model g ◦ f is defined as
'D(g ◦ f) ：= E(x,y)〜D ['(g ◦ f (x), y)].
Before diving into the details, we first provide the following example to illustrate why one might
investigate domain transferability from the view of regularization.
3.1	Example: Robustness and Transferability are Independent
In this subsection, we construct a simple example where domain transferability depends on regular-
ization, yet domain transferability and robustness are independent. Moreover, this example serves
as motivation to consider domain transferability from the view of regularization.
Given the source and target distributions DS, DT ∈ PX×Y, we denote their marginal distributions
on the input space X as DSX and DTX , respectively. We consider the case that X ⊂ Rm being a
low-dimensional manifold in Rm, and Y = Rd. Given an input x ∈ X, the ground truth target for
the source domain is yS(x) generated by a function yS : Rm → Rd. Similarly, we define yT for
the target domain. In this example, for simplicity, we neglect the fine-tuning process but directly
consider learning a function f : Rm → Rd with a norm ∣∣ ∙ ∣∣ on Rd. For the source domain We have
the population loss:
'Ds(f)=Ex 〜DX[∣f(x) - yS(x)∣].
A distribution D ∈ PX on the input space X, defines a norm of a function f : Rm → Rd as
kf∣D ：= Exs[kf(x)∣],
where we view two functions f1 , f2 as the same if ∣f1 - f2 ∣D = 0. Therefore, we can define the
source domain loss 'd, (f) = ∣∣f 一 ys∣∣dx and the target domain loss 'd『(f) = ∣∣f 一 yτ∣∣dx .
For the sake of illustration, we consider the simple case where the input distributions DSX , DTX are
the same, and hence we denote D = DSX = DTX. Note that yS and yT are different.
Denoting a function space F = {f : Rm → Rd | ∣f ∣D < ∞}, we assume that yS, yT ∈ F and we
can compare f, yS , yT in the same space. Therefore, given c > 0 as a regularization parameter, the
domain transferability problem can be defined as:
Learning a source model:	fDS ∈ arg min 'ds (f),	s.t. IlfkD ≤ c;	(1)
f∈F
Testing on a target domain:	'd『(fDS),
3
Under review as a conference paper at ICLR 2022
where the minimizer fDS := ys min{1, jyc^}, the source domain loss is 'ds(f) = ∣∣f - ys∣∣d,
and the target domain loss is '。丁 (f) = ∣∣f - yτ ∣∣d. We prove in Proposition 3.1 that fDS is indeed
a minimizer of (1) and provide an intuitive illustration in Figure 2.
We show that the robustness can be independent to domain transferability as follows. Consider the
adversarial robustness of fDS on an input X ∈ X (e.g., maxδ)∣δk2≤e '(fDS (X + δ),ys(x))). Since
the transferred loss 'd『(fDS) only evaluates fDS on X which is a low-dimensional manifold in
Rm , an adversarial perturbation δ ∈ Rm could make X + δ ∈ Rm\X when the loss function is
sufficiently big outside the manifold X . Therefore, the robustness could be arbitrarily bad without
changing the value of 'd『(fDS), i.e., the performance of the source model on the target domain.
As We can see, the robustness is independent to domain transferability in this example. On the
contrary, if We change the perspective to consider the the regularization parameter c, We have the
following interesting finding. An illustration of the finding is shown in Figure 2, and a more formal
statement is provided in Proposition 3.1.
Figure 2: The left figure illustrates the example in the function space F given a regularization parameter c.
The right figure shows the relations between domain transferability and the c. In this example, the weaker the
regularization effect (greater c) is, the greater the relative domain transferability loss (violet arrow becomes).
Different C
Source LoSS = l^s(f^s~)
Transferred Loss = 1。丁(产 S)
Relative Domain
Transferability Loss =
i” (心)-ι¾ɑ为
Source Training Function Class SiZe C
Proposition 3.1. Given the problem defined above, fcDS is a minimizer of equation 1. Ifc ≥ c0 ≥ 0,
then the relative domain transferability loss 'd『(fDS)一 'ds (fDS ) ≥ 'DT (fDS) - 'ds (fDS).
We can see that robustness is not sufficient to characterize domain transferability. However, there is
a monotone relation between the regularization strength and the relative transferability loss, where
adversarial robustness could be arbitrary. Similar behavior is also observed in our experiments, as
we will discuss in Section 4. Naturally, these findings motivate the study of the connections between
the regularization of the training process and domain transferability in general, as we consider next.
3.2	Upper B ound of the Relative Domain Transferability
In this subsection, we consider the general transferability problem with fine-tuning. We prove that
there is a monotone relationship between the regularization strength and relative domain transfer-
ability loss. We also present a tight upper bound on the relative domain transferability loss. Denote
the training algorithm as A that takes a data distribution D and outputs a feature extractor fAD ∈ FA
chosen from a function class FA and a fine-tuning function gAD ∈ G . Next we formally define
relative domain transferability.
Definition 1 (Relative Domain Transferability Loss). Given the training algorithm A and a pair of
distributions DS, DT ∈ PX ×Y, the relative domain transferability loss between DS, DT is defined
to be the difference of fine-tuned losses, i.e.,
T(A; Ds, Dt) ：= inf 'dt(g ◦ fDs) - 'ds(gDS ◦ fDS).
g∈G
Note that the training algorithm A is not required to be optimal, i.e., it could be the case that
'ds(gDS ◦ fDS) > infg∈G,f∈Fa 'ds(g ◦ f). As we can see, the smaller T(A; DS, DT) is, the
better the model’s relative performance becomes on the target domain.
Another perspective ofDefinition 1 is that inf g∈g 'd『(g ◦ fDs) = 'ds (gDS ◦fDS)+ T (A; DS, DT).
From this perspective, the transferred loss is the source loss plus an additional term to be upper
bounded by a certain distance metric between the source and target distributions - as is common
in the literature of domain adaptation (e.g., Ben-David et al. (2007); Zhao et al. (2019)). The key
question of the “distance metric” remains unanswered. To this end, we propose the following.
4
Under review as a conference paper at ICLR 2022
Definition 2 ((G, F)-pseudometric). Given a fine-tuning function class G, a feature extractor func-
tion class F and distributions DS, DT ∈ PX×Y, the (G, F)-pseudometric between DS, DT is
dg,F①S, DT) ：= SUp | inf 'dsIgo f) - inf 'dt(g ◦ f )|.
f∈F g∈G	g∈G
Since the fine-tuning function class is usually simple and fixed, we will use dF as an abbreviation in
the context where G is clear.
It can be easily verified that dG,F is a pseudometric that measures the distance between two distri-
butions, as shown in the following proposition.
Proposition 3.2. dg,F(∙, ∙) : Pχ×γ X Pχ×γ → R+ satisfies the following properties; (Symme-
try)	dG,F(DS,DT)	=	dG,F(DT, DS);	(Triangle Inequality) ∀D0	∈	PX×Y	:	dG,F(DS, DT)	≤
dG,F(DS, D0) + dG,F(D0, Dt)； (Weak Zero Property) ∀D ∈ Pχ×γ : dg,F(D, D) = 0.
In this section, we consider a fixed fine-tuning function class G and feature extractor function class
FA given by the training algorithm A. Thus, we denote dG,F as dFA for the remainder of the paper.
With the definition of dFA , we can derive the following result.
Theorem 3.1.	Given a training algorithm A, for ∀DS, DT ∈ PX×Y we have
T(A; Ds, DT) ≤ dFA (DS, DT),
or equivalently,	inf 'd『(g	◦	fDS)	≤	'»(gDS	◦	fDS)	+ d^ (DS,	DT).
g∈G
Interpretation: As we can see, the above theorem provides sufficient conditions for good domain
transferability. There is a monotone relation between the regularization strength and dFA (DS, DT),
i.e., the upper bound on the relative domain transferability loss τ(A; DS, DT). More explicitly, if
a training algorithm A0 has FA0 ⊆ FA, then dFA0 (DS, DT) ≤ dFA (DS, DT). Moreover, small
dFA (DS , DT ) implies good relative domain transferability. From this perspective, we can see that
We need both small(1了人(DS, DT) and small source loss 'DS (gDS ◦ fDS) to guarantee good ab-
solute domain transferability. Note that there is a possible trade-off, i.e., with FA being smaller,
ʤ (Ds, DT) decreases but possibly 'DS (gDS ◦ fDS) increases due to the limited power of Fa.
On the other hand, there may not be such trade-off if DS and DT are close enough such that
dFA (DS , DT ) is small.
To make the upper bound more meaningful, we need to study the tightness of it.
Theorem 3.2.	Given any source distribution DS ∈ PX×Rd, any fine-tuning function class G where
G includes the zero function, and any training algorithm A, denote
e := 'DS (gDS ◦ fDS) -	inf. 'DS (go f).
g∈G,f∈FA
We assume some properties of the loss function ' : Rd × Rd → R+: it is differentiable and strictly
convex w.r.t. its first argument; '(y, y) = 0for any y ∈ Rd; and limr→∞ infy:kyk2=r '(~0, y) = ∞,
where ~0 is the zero vector. Then, for any distribution DX on X, there exist some distributions
DT ∈ PX ×Rd with its marginal on X being DX such that
T(A; DS, Dt) ≤ dFA (Ds, DT) ≤ T(A; DS, DT) + e,
or equivalently dFA(DS,DT) - ≤ τ(A; DS, DT) ≤ dFA(DS,DT).
Interpretation: In the above theorem, we show that given any A, DS, and the marginal DX, there
exists some conditional distributions of y|x such that by composing it with the given DX we have
a distribution DT to make the bound in Theorem 3.1 e-tight. Note that e is the difference between
the source loss and the its infimum, i.e., with a good enough algorithm A, the e could be arbitrarily
small.
3.3 Generalization Upper Bound of the Relative Domain Transferability
Here we investigate the proposed theory on relative transferability with finite samples. For a distri-
bution D ∈ PX ×Y, we denote its empirical distribution with n samples as Dbn . That being said,
1n
'Dn (g θ f) = E(x,y)〜Dn ['(g。/(x)，")] = "£'(。° /(""),
n i=1
5
Under review as a conference paper at ICLR 2022
where (xi, yi) are i.i.d. samples from D. Therefore, given two distributions DS, DT ∈ PX×Y, the
empirical (G, F)-pseudometric between them is dG,F (DSn , DTn ).
Note that dG ,F is not only a pseudometric of distributions, but also a complexity measure, and we
will first connect it with the Rademacher complexity.
Definition 3 (Empirical Rademacher Complexity (Bartlett & Mendelson, 2002; Koltchinskii,
2001)). Denote the loss function class induced by G , F as
LG,F := {hg,f : X × Y → R+ | g ∈ G,f ∈ F}, where hg,f (x, y) := `(g ◦ f(x),y).
Given an empirical distribution Dbn (i.e., n data samples), the Rademacher complexity of it is
RadDn (Lg,f):= n Eξ	sup
h∈LG,F
n
ξih(xi, yi) ,
i=1
where ξ ∈ Rn are Rademacher variables, i.e., each ξi is i.i.d. uniformly distributed on {-1, 1}.
We can see that if there is a F0 ⊆ F, then RadDbn (LG,F0) ≤ RadDbn (LG,F). With the above defini-
tions, we have the following lemma connecting the (G, F)-pseudometric to Rademacher complexity.
Lemma 3.1. Assuming the individual loss function ` : Y × Y → [0, c], given any distribution
D ∈ PX×Y and ∀δ > 0, with probability ≥ 1 - δ we have
d,,F(D, Dn) ≤ 2RadDbn(Lg,f) + 3c
Jln(8∕δ)
V	2n
Therefore, denoting again dFA as dG,FA, the empirical version of Theorem 3.1 is as follows.
Theorem 3.3.	Assuming the individual loss function ` : Y × Y → [0, c], given ∀DS, DT ∈ PX×Y,
for ∀δ > 0 with probability ≥ 1 - δ we have
,.o„ _	_	o„、	_	_	、	_	_	、
τ(A; DbS, DT) ≤ dFA (DbS, DbT) + 2RadDbn (LG,FA) + 4RadDbn (LG,FA) + 9c
We can see that a smaller feature extractor function class FA implies both a smaller dFA and the
Rademacher complexity. Therefore, the monotone relation between the regularization strength and
the upper bound on the relative domain transferability loss also holds for the empirical settings.
Other than direct regularization, empirically we find that the transferability is also related to the use
of data augmentation. Can we explain such phenomena from the view of regularization again? We
discuss this question next.
3.4 When can Data augmentation be viewed as Regularization ?
In this subsection, we discuss the connections between data augmentation (DA) and regularization.
We present the results and their interpretation in this subsection, while deferring the detailed discus-
sion to the Section B in the appendix.
Empirical research has shown evidence of the regularization effect of DA (Hernandez-GarcIa &
Konig, 2018a;b). However, there is a lack of theoretical understanding on when can data augmen-
tation be viewed as regularization in general. In an attempt to address this question, we consider a
general DA setting of affine transformation (Perez & Wang, 2017) with parameters (W?, b?) whose
distribution represents specific DA.
General Settings. We consider the fine-tuning function g : Rd → R as a linear layer, which will
be concatenated to the feature extractor f : Rm → Rd. Given a model g ◦ f, we use the squared
loss `(g ◦ f (x), y) = (g ◦ f(x) - y)2, and accordingly apply second-order Taylor expansion to the
objective function to study the effect of data augmentation.
DA categories. We discuss two categories of DA, feature-level DA and data-level DA. Feature-level
DA (Wong et al., 2016; DeVries & Taylor, 2017) requires the transformation to be performed in the
learned feature space: given a data sample x ∈ Rm and a feature extractor f, the augmented feature
is W?f(x) + b? where W? ∈ Rd×d, b? ∈ Rd are sampled from a distribution. On the other hand,
6
Under review as a conference paper at ICLR 2022
data-level DA requires the transformation to be performed in the input space: given a data sample x,
the augmented sample is W?x + b? where W? ∈ Rm×m , b? ∈ Rm are sampled from a distribution.
Intuition on sufficient conditions. For either the feature-level or the data-level DA, the intuitions
given by our analysis are similar. Our results (Theorem B.1&B.2) suggest that the following condi-
tions indicate the regularization effects of a data augmentation: 1) EW? [W?] = I; 2) Eb? [b?] = ~0;
3) W? and b? are independent, where I is the identity matrix and ~0 is the zero vector; 4) W? is not a
constant if it is the feature-level DA; 5) DA is of a small magnitude if it is the data-level DA.
Empirical verification. Combining with Theorem 3.3, it suggests that DA satisfying the conditions
above may improve the relative domain transferability. In fact, it matches the empirical observa-
tions in Section 4. Concretely, 1) Gaussian noise satisfies the four conditions, and empirically the
Gaussian noise improves domain transferability while robustness decreases a bit (Figure 5); 2) Ro-
tation, which rotates input image with a predefined fixed angle with predefined fixed probability,
violates EW? [W?] = I, and empirically the rotation barely affects domain transferability (Figure 14
in Appendix D.3); 3) Translation, which moves the input image for a predefined distance along a
pre-selected axis with fixed probability, violates Eb? [b?] = ~0, and empirically the translation dis-
tance barely co-relates to the domain transferability (Figure 14 in Appendix D.3).
4 Evaluation
4.1	Experimental Setting
Source model training We train our model on two source domains: CIFAR-10 and ImageNet.
Unless specified, we will use the training setting as follows1 . For CIFAR-10, we train the model
with 200 epochs using the momentum SGD optimizer with momentum 0.9, weight decay 0.0005,
an initial learning rate 0.1 which decays by a factor of 10 at the 100-th and 150-th epoch. For
ImageNet, we train the model with 90 epochs using the momentum SGD optimizer with momentum
0.9, weight decay 0.0001, an initial learning rate 0.1 which decays by a factor of 10 at the 30-th and
60-th epoch. We use the standard cross entropy loss denote as LCE(hs, x, y), where hs = gs ◦ f
is the trained model and x, y are the input and label respectively. To evaluate the robustness on the
source domain, we follow the evaluation setting in Ilyas et al. (2019) and perform the PGD attack
with 20 steps using = 0.25. We also evaluate the robustness using AutoAttack in Appendix D.4.
For both tasks we use ResNet-18 as the model structure. We provide results of other model structures
in appendix D.2.
Domain Transferability We evaluate the transferability from CIFAR-10 to SVHN and from Im-
ageNet to CIFAR-10. For the ImageNet transferability, we focus on CIFAR as the target domain,
since it is the domain that is the most positively correlated with robustness as shown in Salman et al.
(2020). We evaluate the fixed-feature transfer where only the last fully-connected layer is fine-tuned
following our theoretical framework. We fine-tune the last layer with 40 epochs using momentum
SGD optimizer with momentum 0.9, weight decay 0.0005, an initial learning rate 0.01 which de-
cays by a factor of 10 at the 20-th and 30-th epoch. To mitigate the impact of benign accuracy,
we evaluate the relative domain transfer accuracy (DT Acc) as follows. Let accsrc and acctgt be
the accuracy of the a fine-tuned model on source and target domain, and accsvrc and acctvgt be the
accuracy of vanilla model (i.e., models trained with standard setting) on source and target domain,
then the relative DT accuracy is defined as:
DT Acc = (acctgt - accsrc) - (acctvgt - accsvrc)
We also provide the results of absolute DT accuracy in Appendix D.1.
4.2	Relationship between Robustness and Transferability Under
Controllable Conditions
We train the model under different controllable conditions to validate our analysis. In particular,
we train the methods by controlling different regularization or data augmentations to evaluate the
change of model robustness and transferability.
1These settings are inherited from the standard training algorithms for CIFAR-10 (https://github.
com/kuangliu/pytorch-cifar) and ImageNet (https://github.com/pytorch/examples/
tree/master/imagenet).
7
Under review as a conference paper at ICLR 2022
求 uu< HQo>4-,uα
LLRM/)
• -0.1
- 0
• 0.1
•	∙ 1.0
LLR(λ∕)
- -0.01
• 0
• 0.01
• 0.1
LLOT(∣∣gs∣∣2)
• 0.01
• 0.1
• 1.0
• 10.0
LLOT(∣∣gs∣∣2)
♦ 0.5
・1
• 2
• 5
R: -0.782
R: -0.918
R: -0.916
R: -0.954
10	20	30	18	20	22	24	10	20	30	20	30	40
Robust Acc (%)	Robust Acc (%)	Robust Acc (%)	Robust Acc (%)
Figure 3:	Relationship between robustness and transferability under different norms of last layer,
via training with last-layer regularization (LLR) and last-layer orthogonalization (LLOT).
Controlling the norm of last layer As shown in our framework, domain transferability is related
with the regularization on the feature extractor. Here we regularize the transferability by controlling
the norm of last linear layer gs. Intuitively, when we force the norm ofgs to be large during training,
the corresponding norm of f will be regularized to be small. We use two approaches to control the
last layer norm:
•	Last-layer regularization (LLR): we impose a strong l2-regularizer with parameter λl specifically
on the weight of gs and therefore our training loss becomes: LLLR(hs, x, y) = LCE (hs , x, y) +
λι ∙ ∣∣gs∣∣F, where ||gs || f is the frobenius norm of the weight matrix of gs.
•	Last-layer orthogonal training (LLOT): we directly control the l2-norm of gs with orthogonal
training (Huang et al. (2020)). The orthogonal training will enforce the weight to become a 1-
norm matrix and we multiply a constant to obtain the desired norm ||gs ||2.
The result of LLR and LLOT are shown in Figure 3. We observe that when we regularize the norm
of last layer to be large (i.e. smaller λ in LLR and larger ||gs||2 in LLOT), the domain transferability
will increase while the model robustness will decrease (their negative correlation is significant with
Pearson’s coefficient around -0.9). This is because larger last layer norm will produce a feature
extractor f with smaller norm, which, according to our analysis, leads to a better domain transfer-
ability. On the other hand, the model gs ◦ f will have a larger norm and therefore becomes less
robust under adversarial attacks.
Controlling the norm of feature extractor We directly regularize the feature extractor f and
check the impact on domain transferability. We implement two regularization as follows:
•	Jacobian regularization (JR): we follow the approach in Hoffman et al. (2019) to apply JR on
the feature extractor. Given model hs = gs ◦ f, the training loss becomes: LJR(hs , x, y) =
LCE (hs,x,y) + λj ∙∣∣ J (f, χ)∣∣F, where J (f, x) denotes the Jacobian matrix of f on X and ∣∣∙∣∣f
is the frobenius norm.
•	Weight Decay (WD): we impose a strong weight decay with factor λw on the feature extractor
f during training. This is equivalent to imposing a strong l2-regularizer with factor λw on the
feature extractor (excluding the last layer).
The results under JR and WD are shown in Figure 4. We observe that with larger regularization on
the feature extractor, the model shows higher domain transferability, which matches our analysis.
Meanwhile, the robustness decreases significantly with large regularizer. This is because a large
regularization will harm the model performance on source domain and lead to low model robustness.
CIFAR10 -> SVHN
ImageNet > ClFAR10
CIFAR10 -> SVHN
ImageNet-> CIFAR10
求 uu< HQ φ>-⅛-φtf
15
10
5
JRM/)
- 0
• 100
• 1000
0 ■
R: 0.313
20 30 40 50 60 70
Robust Acc (%)
40
30
20
10
0	∙	R: 0.105
15	25	35	45	55
Robust Acc (%)
JRMy)
• 0
• 0.01
• 0.1
4∩ ∙	WD(Aw)
∙ 0.0005
• 0.001
• 0.005
•	∙ 0.01
20
0 R： -0.734	∙
10	20
Robust Acc (%)
•	WD(Aw)
15	∙ 0.0001
• 0.0005
• 0.001
10	∙
0	■
R: -0.984
10	15	20
Robust Acc (%)
5
Figure 4:	Relationship between robustness and transferability when we regularize the feature
tractor with Jacobian Regularization (JR) and weight decay (WD).
ex-
8
Under review as a conference paper at ICLR 2022
CIFAR10 -> SVHN
60
ImageNet -> CIFAR10
CIFAR10 -> SVHN
ImageNet -> CIFAR10
东40
< 30
φ 20
Gauss(σ)
• 0
• 0.05
• 0.25
• 1.0
40
Gauss(σ)
♦ 0
• 0.05
• 0.25
10
Pos(b)
- 1
• 4
• 8
Pos(b}
• 1
• 4
• 8
20
R: -0.384
R: 0.168
20 30 40 50 60 70
Robust Acc (%)
0	.
20
30
R: 0.566
Robust Acc (%)
20
30
40	50
Robust Acc (%)
R: 0.880
20	25
Robust Acc (%)
30
3 10
ω
0
4
5
0
2
0
Figure 5:	Relationship between robustness and transferability when we use Gaussian noise (Gauss)
and posterize (Pos) as data augmentations.
80
0
σs
Z 60
<
a 40
ω
Res Netl 8
ResNeti8
⅛ 20
ω
al
0
Resca le{m}
♦ 1
・2
• 4
• 8
15
10
)R: -0.917	∙
0	10	20
Robust Acc (%)
R: -0.842
15
Bl□r(k}
♦ 1
• 5
• 11
20
Robust Acc (%)
80
60
40
20
Wide Res Net-50
R: -0.931
10
WideResNet-50
Resca le{m}
• 1
• 2
• 4
• 8
20
Robust Acc (%)
10
R: -0.852
22
B∣□r(k}
• 1
- 5
• 11
24
Robust Acc (%)
5
5
0
0
0
0
Figure 6:	Relationship between robustness and transferability on ImageNet when we use rescale and
blur as data augmentations.
Data augmentation As shown in Section 3.4, data augmentation can be viewed as a type of reg-
ularization during training and thus affects the domain transferability. Here we consider both noise
dependent and independent data augmentations.
Noise-dependent data augmentation We include two noise-dependent augmentations:
•	Gaussian Noise data augmentation (Gauss): we add zero-mean Gaussian noise with variance σ2
to the input image.
•	Posterize (Pos): we truncate each channel of one pixel value into b bits (originally they are 8 bits).
The results of Gauss and Pos are shown in Figure 5. We observe that the domain transferability
of trained model keeps improving with larger data augmentation, which matches our theory. The
robustness also benefits from a small data augmentation, but decreases when it becomes large.
Resolution-related (noise-independent) data augmentation. Specifically, for ImageNet to
CIFAR-10 transferability, we consider two resolution-related data augmentations. The intuition is
that when the target domain has a lower resolution than the source domain (ImageNet is 224 × 224
while CIFAR-10 is 32 × 32), the data augmentations that down-sample the inputs during the training
on source domain will help transferability. We consider the below resolution-related augmentations:
•	Rescale: we rescale the input to be m times smaller (i.e., shape ImageNet as (224/m) × (224/m))
and then rescale them back to the original size.
•	Blur: we apply Gaussian blurring with kernel size k on the input. The Gaussian kernel is created
with a standard deviation randomly sampled from [0.1, 2.0].
We show the results of rescaling and blurring in Figure 6. The experiments are evaluated only
for ImageNet to CIFAR-10, and we include the results of both ResNet18 (the default model) and
WideResNet50. We can see that these data augmentations help with transferability to target domain,
although the robustness on the source domain decreases since these augmentations do not include
any robustness-related operations.
5 Conclusions
In this work, we theoretically analyze the sufficient conditions for domain transferability based on
the view of function class regularization. We also conduct experiments to verify our claims and
observe some counterexamples that shows a negative correlation between robustness and domain
transferability. These results are helpful in the domain generalization of machine learning models.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Our work focuses on theoretically and empirically studying the domain transferability of a machine
learning model. All the datasets and packages we use are open-sourced. We do not have ethical
concerns in our paper.
Reproducibility S tatement
We have tried our best to provide training details to facilitate reproducing our results. In Section 4.1
we provide detailed results on how to train the model and how to transfer the trained model to target
domain, as well as how we evaluate our model. We also upload the zip file of our code with the
submission. We will open-source our code once accepted.
References
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations
for domain adaptation. Advances in neural information processing systems, 19:137, 2007.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient
decision-based attack. In 2020 ieee symposium on security and privacy (sp), pp. 1277-1294.
IEEE, 2020.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Nicolas Courty, Remi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for do-
main adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-
1865, 2016.
Terrance DeVries and Graham W Taylor. Dataset augmentation in feature space. arXiv preprint
arXiv:1702.05538, 2017.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint
arXiv:1906.00945, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Alex Hemandez-GarcIa and Peter Konig. Data augmentation instead of explicit regularization. arXiv
preprint arXiv:1806.03852, 2018a.
Alex Hemandez-GarcIa and Peter KOnig. Further advantages of data augmentation on convolutional
neural networks. In International Conference on Artificial Neural Networks, pp. 95-103. Springer,
2018b.
Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with jacobian regularization.
arXiv preprint arXiv:1908.02729, 2019.
Haoshuo Huang, Qixing Huang, and Philipp Krahenbuhl. Domain transfer through deep activation
matching. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 590-605,
2018.
Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, and Ling Shao. Controllable orthog-
onalization in training dnns. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 6429-6438, 2020.
10
Under review as a conference paper at ICLR 2022
Andrew Ilyas, Shibani Santurkar, Logan Engstrom, Brandon Tran, and Aleksander Madry. Ad-
versarial examples are not bugs, they are features. Advances in neural information processing
systems, 32, 2019.
Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions
on Information Theory, 47(5):1902-1914, 2001.
Clare Lyle, Marta Kwiatkowksa, and Yarin Gal. An analysis of the effect of invariance on gen-
eralization in neural networks. In International conference on machine learning Workshop on
Understanding and Improving Generalization in Deep Learning, volume 1, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pp. 10-18. PMLR,
2013.
Luis Perez and Jason Wang. The effectiveness of data augmentation in image classification using
deep learning. arXiv preprint arXiv:1712.04621, 2017.
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to interpo-
lation and extrapolation in domain generalization. arXiv preprint arXiv:2102.13128, 2021.
Kevin Roth, Yannic Kilcher, and Thomas Hofmann. Adversarial training is a form of data-dependent
operator norm regularization. arXiv preprint arXiv:1906.01527, 2020.
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adver-
sarially robust imagenet models transfer better? arXiv preprint arXiv:2007.08489, 2020.
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and
Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attack-
ing black-box neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 742-749, 2019.
Sebastien C Wong, Adam Gatt, Victor Stamatescu, and Mark D McDonnell. Understanding data
augmentation for classification: when to warp? In 2016 international conference on digital image
computing: techniques and applications (DICTA), pp. 1-6. IEEE, 2016.
Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selec-
tion in deep unsupervised domain adaptation. In International Conference on Machine Learning,
pp. 7124-7133. PMLR, 2019.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472-7482. PMLR, 2019a.
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup
help with robustness and generalization? arXiv preprint arXiv:2010.04819, 2020.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for
domain adaptation. In International Conference on Machine Learning, pp. 7404-7413. PMLR,
2019b.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532. PMLR, 2019.
11
Under review as a conference paper at ICLR 2022
Appendix
A Proofs
Proposition A.1 (Proposition 3.1 Restated). Given the problem defined in subsection 3.1, fcDS is a
minimizer ofequation 1. Moreover, if C ≥ C ≥ 0, then the relative domain transfer loss 'd『(fDS) 一
QDS (fDS ) ≥ 'Dτ(fDS ) - QDS (fDS ).
Proof. Recall that 'ds(f) = kf 一 ys∣∣d, 'dt(f) = kf 一 yτ∣∣d and fDS ：= ys min{1, ky∣%}.
First, let’s verify that
fcDS ∈ arg min QDS (f), s.t. ∣f∣D ≤ c.
f∈F
Ifc ≥ ∣yS∣D, then fcDS := yS achieves the minimum. Ifc < ∣yS∣D, then for any f ∈ F : ∣f∣D ≤
c, we have
QDS (f) = ∣f 一 yS ∣D ≥ ∣yS ∣D 一 ∣f∣D ≥ ∣yS ∣D 一 c
=k(I 一 kyCkD)ysl∣D = kys 一 kysckDysl∣D = qDs(fDS).
Therefore, fcDS indeed achieves the minimum.
Now, let’s prove the proposition. For any c ≥ ∣yS∣D, we have QDS (fcDS ) = 0 and QDT (fcDS )
is a constant. Therefore, there is no difference for all c ≥ ∣yS ∣D , and the proposition holds for
c ≥ c0 ≥ ∣yS ∣D . Then, We only need to verify the case for ∣yS ∣ ≥ c ≥ c0 :
qDs (ZDS) 一 qDs (fDS) = C 一 C = k kySckDys 一 ky∣jDys∣∣d
=kfDs — fDs Id = kfDS - yτ + 期T - fDs Id
≥ IkfDS-yTId -kyT - fDsIdI
≥ ∣fcD0 s 一 yT∣D 一 ∣yT 一 fcDs ∣D
='Dτ(fDS )-'。7(/4).
Rearranging the above inequality gives the proposition.	口
Proposition A.2 (Proposition 3.2 Restated). dg,F(∙,∙): Pχ×γ X Pχ×γ → R+ satisfies thefollow-
ing three properties.
1.	(Symmetry) dG,F(Ds, DT) = dG,F(DT, Ds).
2.	(Triangle Inequality) For ∀D0 ∈ PX×Y: dG,F(Ds,DT) ≤ dG,F(Ds, D0) + dG,F (D0, DT).
3.	(Weak Zero Property) For ∀D ∈ PX×Y: dG,F(D, D) = 0.
Proof. Recall that
d，G,F①S, DT):= sup I inf 'ds(g ◦ f) - inf 'dt(g ◦ f)I∙
f∈F g∈G	g∈G
We can see that the symmetry and weak zero property are obvious. For triangle inequality, given
∀D0 ∈ PX×Y:
dG,F(DS, DT) = SUp I inf 'ds(g ◦ f) 一 inf 'dt(g ◦ f)I
f∈F g∈G	g∈G
=	sup I inf qD (g ◦ f) 一 inf qD0(g ◦ f) + inf qD0(g ◦ f) 一 inf qD (g ◦ f)I
sT
f∈F g∈G	g∈G	g∈G	g∈G
≤	sup(I inf qD (g ◦ f) 一 infqD0(g ◦ f)I + I inf qD0(g ◦ f) 一 inf qD (g ◦ f)I)
sT
f∈F	g∈G	g∈G	g∈G	g∈G
≤	sup	I inf qD (g ◦	f)	一 inf qD0(g	◦ f)I + sup I infqD0(g ◦ f) 一 inf qD	(g ◦	f)I
sT
f∈F g∈G	g∈G	f∈F g∈G	g∈G
□
12
Under review as a conference paper at ICLR 2022
Proposition A.3. Denote the function class
LgF = {hg,f : X × Y → R+ | g ∈ G,f ∈ F}, where hg,f (x, y) := '(g o f(x), y).
Let d : X ×Y → R+ be a metric on X ×Y, and assume Vh ∈ LGF is L-Lipschitz continuous with
respect to the metric d. Then, we have
dFA (DS, DT) ≤ L ∙ W(DS, DT),
where W (DS, DT) is the Wasserstein distance:
W(DS, DT) =	sup	E(x,y)〜% [φ(x, y)] - E3∕〜DT [φ(x, y)] s.t. φ is 1-Lipschitz.
φ:X×Y→R
Proof. Recall that
dg,F (DS, Dt ) := SUP | inf tds (g o f) - inf CDTIgO f )|.
f∈f g∈G	g∈G
By the definition of inf, for Ve > 0 there exist gS,e, gτ,∈ ∈ G such that
inf tds (g O fe) ≥ 'ds (gS,e ◦ fe) - e
g∈G
inf 'dt (g o fe) ≥ 0Dτ(gτ,e ◦ fe) - e.
g∈G
By the definition of sup, there exists fe ∈ F such that
dg,F (DS, Dt ) ≤ | inf 他(go fe) - inf 小。fe) | + e
g∈G	g∈G
=max{ inf 'ds (g。fe) 一 inf 'dt (g。fe), inf 'dt (g。fe) 一 inf 'ds (g。fe)} + e
S	e	T	e	T	e	S	e∕J
g∈G	g∈G	g∈G	g∈G
≤ max{'Ds (gτ,e。fe) - inf 0式9。fe), 'dt (gS,e。fe) - inf 'ds (g。fe)} + e
g∈G	g∈G
≤ max{'Ds (gτ,e O fe) - 'Dt (gτ,e。fe), 'DT ①—。fe) - 'Ds (.9S,e。fe)} +2e.
(2)
Let,s first consider the first term in the max{∙, ∙} above.
CDs (gτ,e。fe) - 'Dτ(gT,e。fe)
=L ∙ (^LIDS (gT,e。fe) - LCDt (gT,e。fe))
=L ∙ (E(χ,y)〜Ds [ L C(gT,e。fe (X), y)] - E(x,y)〜DT [ L '(gT,e。fe (X), y)])
≤ L ∙ W(DS, Dt),
where the inequality is due to that both LC(gS,e。fe(x), y) and L'(gτ,e。fe(x), y) are 1-Lipschitz
w.r.t. (x, y) and the metric d.
Similarly, we also have
'Dt (gS,e。fe) - 'Ds(gS,e。fe) ≤ L ∙ W (DS, DT).
Therefore, equation 2 implies
dg,F(DS, DT) ≤ L ∙ W(DS, DT) + 2e.
Letting e → 0 completes the proof.
□
Proposition A.4. Consider multi-class classification where Y = [k] for some k ≥ 2. Define the
loss function C as
'(g。f (x),y) = 1{argmaχ(g。f(x))j = y}
j∈[k]
Let δτv (DS, DT) denote the total variation distance. Then we have
d^A (DS, DT) ≤ δτv (DS, DT)
13
Under review as a conference paper at ICLR 2022
Proof. Fix f ∈ F. By the definition of inf, there exists gT, such that
| inf 'ds(g ◦ f) - inf 'dt(g ◦ f)|
g∈G	g∈G
≤ | inf 'ds(g ◦ f) — 'dt(gτ,e ◦ f )| + e
g∈G
≤ 1'Ds (gT,e ◦ f) - 'Dt (gT,e ◦ f )| + E
=|E(x,y)〜Ds ['(gT,e ◦ f (X), J)] - E(x,y)〜DT ['(gT,e ◦ f (X), y)]| + E
=∣P(x,y)〜Ds[1{argmaχ(gτ,e ◦ f (x))j = y}] - P(x,y)〜DT [1 {argmaχ(gτ,e ◦ f(x))j = y}]| + E
j∈[k]	j∈[k]
(3)
Let A be the event such that A = {(x, y) : arg maxj ∈[k] (gT, ◦ f (x))j 6= y}. Then we can write
equation 3 as
⑶=lP(x,y)〜DS [A] - P(x,y)〜DT [A] | + E
≤ SUP |P(x,y)〜Ds [B] - P(x,y)〜DT [B] | + E
B
= δTV (DS, DT) + E
Send E → 0. Noting that f ∈ F was arbitrary, apply suP to both sides gives us the desired inequality.
□
Theorem 3.1 can be proved easily by definition.
Theorem A.1 (Theorem 3.1 Restated). Given a training algorithm A, for ∀DS, DT ∈ PX×Y we
have
T(A; Ds, DT) ≤ dFA (DS, DT),
or equivalently,	inf 'DT (g	◦	fADs)	≤	'Ds (gADs	◦ fADs) +	dFA(DS,DT).
g∈G
Proof. By definition,
T (A; DS, DT)
≤
≤
inf 'Dt (g ◦ fDS )-'ds (gD ◦ fDS)
g∈G
inf 'dt (g ◦ fDS) - inf 'ds (g ◦ fAS)
g∈G	g∈G
| inf 'dt(g◦ fDS) - inf 'ds(g◦ fDS)∣
g∈G	g∈G
≤ fsχlgnG 'dt (g ◦f)- gnG'DS(g ◦f )1
= dFA (DS, DT).
□
To prove Theorem 3.2, we first prove the following interesting lemma.
Lemma A.1. Let Srd-1 := {y ∈ Rd | kyk2 = r} denotes the (d- 1)-dimensional sphere in Rd with
radius r > 0. If a function h : Srd-1 → Rd satisfies
∀y ∈ Srd-1 :	hh(y), yi <0,	(4)
then we have
~0 ∈ conv(h(Srd-1)),
i.e., ~0 is in the convex hull of {h(y) | y ∈ Srd-1}.
Proof. We assume that ~0 ∈/ conv(h(Srd-1)) and prove by contradiction. Since ~0 ∈/ conv(h(Srd-1)),
we can find a hyperplane that separates ~0 and the convex set conv(h(Srd-1)). By the separating
hyperplane theorem there exists a nonzero vector v ∈ Rd and c ≥ 0 such that
∀z ∈ conv(h(Srd-1)) :	hz, vi ≥ c ≥ 0.	(5)
14
Under review as a conference paper at ICLR 2022
We choose y = rv/kvk2 and observe that h(y) ∈ conv(h(Srd-1)). Hence, by equation 5 we have
hh(y), yi ≥ 0,
which contradicts to the condition of equation 4. Therefore, it must be that ~0 ∈ conv(h(Srd-1)).
□
Theorem A.2 (Theorem 3.2 Restated). Given any source distribution DS ∈ PX×Rd, any fine-tuning
function class G where G includes the zero function, and any training algorithm A, denote
e ：= 'ds(gAS ◦ fDS) —	'ds(g ◦ f).
g∈G,f∈FA
We assume some properties of the sample individual loss function ` : Rd × Rd → R+: it is
differentiable and strictly convex w.r.t. its first argument; '(y,y) = 0 for any y ∈ Rd; and
limr→∞ inf#：|©[2=丁 '(~,y) = ∞. Then, for any distribution DX on X, there exist some distri-
butions DT ∈ PX ×Y with its marginal on X being DX such that
τ(A; DS, DT ) ≤ dFA (DS, DT) ≤ τ(A; DS, DT) + .
Proof. The τ(A; DS, DT ) ≤ dFA (DS, DT ) is proved by Theorem 3.1, we only need to prove that
there exists some DT ∈ PX ×Y with its marginal on X being DX such that
dFA (DS, DT) ≤ T(A; DS, DT) + e = inf 'dt (g ◦ fAS) - jnj j (g ◦ f).
g∈G	g∈G,f∈FA
We begin by observing that limr→∞ infyRy∣∣2=r '(~, y) = ∞, and thus there exists r > 0 such that
∀y ∈ Sr-1:	'(~,y) ≥ 'Ds(~) = E(x,y)〜Ds['(~,y)],	(6)
where Srd-1 := {y ∈ Rd | kyk2 = r} denotes the (d - 1)-dimensional sphere with radius r. Note
the we abuse the notion a bit to let ~0 also denotes the zero function (i.e., maps all input to zero).
Now, let us define at the following set
V := {V1'(~,y) | y ∈Sd-1},
where Vi is taking the gradient w.r.t. the first argument of '(∙, ∙). By the strict convexity of '(∙, y),
we have
'(y,y) — '(~,y) > hvι'(~,y),yi.
Noting that '(y, y) = 0 is the unique minimum of '(∙, y), we have '(~, y) > 0. Accordingly,
∀y ∈ Sd-1 :	0 > -'(~,y) > hVi'(~,y),yi.
Having the above property, and noting that Vι'(~, ∙) : SdT → Rd, we can invoke Lemma A.1 to
see that
~0 ∈ conv(V).
Therefore, there exists n points {yi}in=1 ⊂ Srd-1 such that
n
~= X CiVi'(~,yi),	(7)
i=1
where ci > 0 and	in=1 ci = 1.
Therefore, we can define the target distribution DT as the following. Given any X 〜 DX, the
distribution of y conditioned on x is: y = yi with probability ci . Now we verify the distribution DT
indeed makes the bound -tight. Denote a strictly convex function h : Rd → R+ as the following
n
h(∙) := Eci'(∙,yi).
i=1
15
Under review as a conference paper at ICLR 2022
Since h is strictly convex and Vh(~) = ~ (equation 7), We can see that h(~) achieves the unique
global minimum of h on Rd.
Therefore, given the DT , for any ∀f ∈ FA We have
inf 'dt (g ◦ f) = infE(x,y)〜DT ['(g ◦ f(x),y)]
g∈G	g∈G
inf Ex〜DX
g∈G
n
ECi'(g。f(χ),yi)
=1
inf Ex〜DX [h(g ◦ f(x))]
g∈G
h(~0)	(G contains the zero function)
n
X Ci'(~,yi).
=1
(8)
Recall that dʃʌ (DS, DT) = SuPf ∈Fa | infg∈G 'df(g。f) — infg∈G 'ds (g。f )∣,we can see that
n
dFA(DS, DT) = SuP | X a'(~,yi) — inf 'ds(g。f )|	⑼
f∈FA i=1	g∈G
By equation 6, for ∀f ∈ FA, we have
n
XCi'(~0,
=1
yi) ≥ 'ds(~) = 'ds(~。f) ≥ inf 'ds(g。f).
g∈G
Hence, we can continue as
nn
XCi	'(~0, yi ) — inf'Ds(g。 f) = XCi	'(~0, yi ) — inf	'Ds (g 。 f)
i=1	g∈G	i=1	g∈G,f∈FA
=inf 'dt (g。fDS) —	inf. 'ds (g。f)	(by equation 8)
g∈G	g∈G,f∈FA
gnG'DT(g。fDS ) - 'DS 您S。fDS )+ 'DS 忘。fDS ) - g∈Gi,n∈FA 'DS	f'
=T (A; DS, Dt )+ e.
Therefore, it holds that dFA (DS, DT) ≤ τ(A; DS, DT) + , and thus the theorem.
□
Lemma A.2 (Lemma 3.1 Restated). Assuming the individual loss function ' : Y × Y → [0, C], given
any distribution D ∈ PX ×Y and ∀δ > 0, with probability ≥ 1 — δ we have
d,G,F(D, Dn) ≤ 2RadDn(Lg,f) + 3cV^n2^^
Proof. Given any δ > 0, f ∈ F, g ∈ G, D ∈ PX ×Y, and taking any hg,f ∈ LG,F (Definition 3),
with probability ≥ 1 — δ we have
1n
'd(g。f) -'Dn(g。f) = E(x,y)〜D[hg,f(x,y)] — hPfh9,f(xi,yi)
ni=1
≤ 2RadDbn (LG,F) + 3C
Jln(2∕δ)
V	2n
(10)
where the inequality is by the well-known Rademacher complexity uniform bound. Similarly,
1n
'Dn (g。f ) — 'd (g。f ) = E(χ,y)〜D [—hg,f (X,y)] 一 n Σ —hg,f (xi , yi )
≤ 2RadDbn (—LG,F) +3C
Jln(2∕δ)
V	2n
2RadDbn (LG,F) + 3C
/ln(2∕δ)
V 2n
(11)
16
Under review as a conference paper at ICLR 2022
The probability that both events equation 10 and equation 11 happen can be upper bounded by union
bound, i.e.,
Pr((10) ∧ (11)) = 1 - Pr((10)c ∨ (11)c) ≥ 1 - (Pr((10)c) + Pr((11)c)) ≥ 1 - 2δ.
Therefore, combining the above with probability ≥ 1 - δ we have
i'd (g ◦ f) - 'Dbn (g ◦ f )| ≤ 2RadDn (LG,F)+ 3c \1'*27
(12)
With equation 12, we can prove the lemma as the following. Given ∀ > 0, by the definition of
infimum there exists a g ∈ G such that
'd(ge ◦ f) < inf'd(g ◦ f) + e.
g∈G
By equation 12, with probability ≥ 1 - δ we have
'Dbn (ge ◦ f) ≤ 'd(ge ◦ f)+ 2RadDn (LG,f) + 3c\l”„
Moreover, by definition
inf 'Dn (gθ f) ≤ 'Dn (ge ◦ f).
g∈G
Combining the above three inequalities we have
inf 'Dn (g ◦ f) < inf'd (g ◦ f) + e + 2RadDn (Lg,f) + 3c-∖/吗")
g∈G	g∈G	2n
Letting → 0, we can see that
inf 'bn (g ◦ f) ≤ inf'd (g ◦ f) + 2Radbn (LG F) + 3cι jn("".
g∈G D…g ∈~ DeQ D !	Dn GFl V 2n
Similarly, we can derive the above inequality again but with D and Dbn switched. Therefore,
| inf 'Dn (g ◦ f) - inf'd(g ◦ f )| ≤ 2RadDn(Lg,f)+3c∖∕ln(4Zδ).
D∈G	D∈G	2n
Since the above inequality holds for ∀f ∈ F, taking the supremum over f ∈ F gives the lemma. □
Lemma A.3. Assuming the individual loss function ' : Y × Y → [0, c], given any distributions
DS , DT ∈ PX ×Y and ∀δ> 0, with probability ≥ 1 - δwe have
d，FA (DS, Dt) ≤ dFA (DS, DT) + 2(RadDn H0川) + RadDn ①0^)) + 6,/n^
Proof. By Proposition 3.2, we apply the triangle inequality to derive
∙^∙-- ^..
dFA(DS,DT) ≤dFA(DS,DbTn)+dFA(DbTn,DT)
≤ dFA(DbSn,DbTn)+dFA(DbTn,DT)+dFA(DbSn,DS).
By Lemma 3.1, we can apply the union bound argument (e.g., see the proof of Lemma 3.1) to bound
- ,^.. _ 一一 , ^.. _ . 一 一. .一 . 一. _ .一 一 一一 一 一. 一
dFA (DbTn, DT) and dFA (DbSn, DS). That being said, ∀δ0 > 0 with probability ≥ 1 - 2δ0 we have
dF (Dn,Ds) ≤ 2RadDn(LG,FA) + 3c∖∕ln^)
S	2n
Jln(4∕δ0)
V	2n
- ,_ ，一 . _
dFA (DbT, DT) ≤ 2RadDbn (LG,FA ) + 3c
Therefore,
Jln(4∕δ0)
V	2n
- ,_ 一 ， ，一 . _ ，一 、 、 _
dFA (DbT, DT) + dFA (DbS, DS) ≤ 2(RadDbn (LG,FA) + RadDbn (LG,FA)) + 6c
Denoting δ= 2δ0 gives the lemma.
□
17
Under review as a conference paper at ICLR 2022
Theorem A.3 (Theorem 3.3 Restated). Given ∀DS, DT ∈ PX×Y, for ∀δ > 0 with probability
≥ 1 - δ we have
τ(A; DbS, DT) ≤ dFA (DbS, DbT) + 2RadDbn (LG,FA) + 4RadDbn (LG,FA) + 9c
Jln(8∕δ)
2	2n
Proof. For ∀δ > 0, from the proof of Lemma A.3 we can see that with probability ≥ 1 - δ:
dFA (Dn, DS) ≤ 2RadDn SA) + 3c Jln(^
S	2n
dFA (DT, DT) ≤ 2RadDbn (LgFa ) + 3cʌ/ln^,
(13)
and Lemma A.3 holds. Therefore
T (A; Dbn, Dt )
n	nn
gnG `dt (g ◦ fDS) - 'Dn (gDS ◦ fDS)
Dbn
inf 'dt (g ◦ fADS)
g∈G
Dbn
inf 'dt (g ◦ fADS)
g∈G
≤
—
—
Dbn
gW'Dn (g ◦ fDS)
bn	bn
inf 'ds (g ◦ f DS) + inf 'ds (g ◦ f DS) - inf
g∈G DS A g∈G DS	A	g∈G
Dbn
QDS (g ◦ fDS)
≤
dFA(DS,DT)+dFA(DSn,DS)
≤
dFA (DS, DT) + 2RadDn (lg,Fa ) + 3c∖l"n2F
≤
dFA (Dbn, DbT) + 2RadDn (Lg,Fa) + 4Rad1bn (Lg,Fa ) + 9c\/皿辔
T	S	2n
where the first inequality is by definition of infimum, the second inequality is by the Definition 2,
the third inequality is by equation 13 and the last inequality is by Lemma A.3.
□
B	Data augmentation (DA) as Regularization
In this section, we discuss data augmentation (DA) as a concrete example of regularization for train-
ing feature extractor fADS , and explore its impact on the function class FA discussed in Section 3.
Empirical research has shown evidence of the regularization effect of DA (Hernandez-GarcIa &
Konig, 2018a;b). However, there is a lack of theoretical analysis, and thus We aim to construct a
theoretical framework to understand under what sufficient conditions DA can be viewed as regu-
larization on the feature extractor function class Fa. We categorize DA into feature-level DA and
data-level DA, and for each category, we analyze different DA algorithms to characterize the suf-
ficient conditions under which DA regularizes the function class Fa. Combined with analysis in
Theorem 3.3, we also provide concrete sufficient conditions to tighten the upper bound of relative
transferability τ(A; Dbn, DT).
General Settings. For the following discussion we apply a general DA setting of affine transfor-
mation (Perez & Wang, 2017), taking the form of x? = W?>x + b?, where (x, x?) is a pair of the
original and augmented samples, (W?, b?) are parameters representing specific DA policies. We set
g : Rd → R as the linear layer corresponding to the weight matrix Wg, which will be composed with
the feature extractor f : Rm → Rd. We use squared loss for ' : R X R → R, and let 'Dbn 八(.9 ◦ f)
be the objective function given by training algorithm A from Theorem 3.3.
B.1	FEATURE-LEVEL DA (AFL)
Feature-level DA (Wong et al., 2016; DeVries & Taylor, 2017) requires the transformation to be
performed in the learned feature space, which gives us an augmented feature W?f(x) + b?. We
use Loss-Averaging algorithm where we take an average of the loss over augmented features for
18
Under review as a conference paper at ICLR 2022
training. Denote the training algorithm based on feature-level DA as AFL, the objective function is
as below.
1n
'DnAFL (g ◦ f) = — X Ew*,b* '(g。(W*f(Xi) + b*),yi).
i=1
Theorem B.1. Apply feature-level DA with affine transformation parameters (W? , b? ) s.t. 1)
EW* [W? ] = Im; 2)W? 6≡ Im (i.e., W? is not an identity matrix); 3) Eb* [b? ] = ~0m; 4)W? and
b? are independent. Set ' : R × R → R as squared loss; Define ∆W* :=W? - Im, then we have
'Dn ,AFL (g。f) = 'Dbn,A(g。f) + ΩaFL ,
where Ωafl = 1 Pn=IEW* [∣∣f(xi)>∆w* Wg |同 + Eb* [∣∣b>Wg∣∣2i.
Proof. '00(Wg> 。 f(xi)) = 2 for ' as squared loss. Apply Taylor expansion to '(g。(W*f(Xi) +
b? ), yi around f(xi), all higher-than-two order terms will vanish:
Ew*,b* ]'(g。(W*f(Xi) + b*),yj]
=Ew*,b*]'(W>。f(χi),yj + W>(∆w*f (Xi) + b*)'0(W>。f (xi),yi)+
1 W>(∆w*f (Xi) + b*)(∆w*f(xi) + b*)>'00(W>。f (Xi), yi)Wg
='(w>。f (Xi),y) + Ew*,b*[wj(∆w*f (Xi) + b*)(∆w*f (Xi) + b*)>Wgi
='(w>。f (Xi), yj + Ew* [f (Xi)>∆w* Wg∣∣2] + Eb* [∣∣b>Wg |同；
The second equality holds since	E∆W	=	EW* [W?	- Im]	=	0(m,m) and	Eb*	=	~0m;	The third
equality holds since Wi and b are independent. Therefore, 'DnAFL (g°f) ：= n Pn=1 Ew*,b*'(g。
(W*f (Xi) + b*),yj] = 'Dn,A(g。f ) + Ωafl.	口
Interpretation. Ωafl is composed of two segments: 1) l2 regularization to an f -dependent scalar
averaged over W* and Xi ; 2) l2 regularization to an f -independent scalar averaged over b* . Due to
the regularization effect on f from the first segment of Ωafl , we can reasonably expect the function
class FA0 enabled by AFL to be a subset of that enabled by a general training algorithm A.
Sufficient conditions. Combined with Theorem 3.3, the sufficient conditions to tighten the upper
bound dFA (DbSn, DbTn) for the relative transferability τ(A; DbS, DT) are: feature-level DA (AFL) with
parameters satisfying: 1) EW* [W*] = Im; 2) W* 6≡ Im; 3) Eb* [b*] = ~0m; 4) W* and b* are
independent.
B.2	DATA-LEVEL DA (ADL)
Data-level DA requires that the transformation to be performed in the input space to generate aug-
mented samples W*X + b* . We cover analysis on two ubiquitous algorithms for data-level DA
training: Prediction-Averaging (APDL) (Lyle et al., 2019) and Loss-Averaging (ALDL) (Wong et al.,
2016). The difference between APDL and ALDL lies in whether we take the average of the prediction
or the losses:
1n
'Dn ADL (g。f) := — E'(Ew*,b*[g。f (W*Xi + b*)],yi);	(14)
,P	n
i=1
1n
'DbnADL (g。f) := - EEW*,b*['(go f (W*Xi + b*),yi)].
n i=1
19
Under review as a conference paper at ICLR 2022
Theorem B.2. Define the data-level deviation caused by data-level DA ADL ∈ {APDL , ALDL } with
parameters (W?, b?) from the original data sample as ∆xi := (W? - Im)xi + b?, and define ∆3x :=
Exi,W?,b?
∆xi 32 . Suppose we apply data-level DA s.t. 1) EW? [W?]
Im; 2) Eb? [b?] = ~0m; 3)
O(∆jx) ≈ 0, ∀j ∈ N+, j ≥ 3; 4) W? and b? are independent. Define ∆W? := W? - Im ∈ Rm×m,
∆ybi := Wg>f(xi) - yi ∈ R. Let Wg(k) ∈ R be the kth dimension component of Wg and then define
wi,(k) := Wg(k)∆ybi ∈ R; Denote the Hessian matrix of the kth dimension component in f(xi) as
Hfk),i; Let Vf be the Jacobian matrix of f, then we have
'Dn,ADL (g ◦ f) = 'Dn,A(g。f) + ΩADL + O(∆X),
where ωADL = 1 Pi=1 Pk=I wi,(k) [tr (eW*δXi∆>i]Hfk)，i)], where δXi = (W*-I)>xi + b?；
Ωadl = Ωadl + n Pi=JEW*∣∣x>∆w* Vf(Xi)Wg∣∣2 + Eb*∣∣b>Vf (Xi)Wg∣∣2i.
Proof. Let ∆fi,APDL := EW*,b* f (W?> xi + b?) - f(xi), then
∆fi,APDL :=EW*,b*f(W?>Xi +b?) - f(Xi)
=Ew*,b* [Vf(Xi)>(∆χi)i + 1 Ew*,b* [∆>iHFi(Xi)∆χiid + O(Ew*,b*k∆χik2)
=1 Ew*,b*[∆>i Hfk),i(xi)∆χJd + O(Ew*,b* k∆χi k2),	(15)
where [∙(k)]d denotes a d-dimensional vector and k denotes the kth dimension element. Since '
is squared loss, the third-and-higher derivative are 0, therefore, the third-and-higher order terms in
Taylor expansion to ` EW*,b* g 。 f(W?Xi + b?) , yi around f(Xi) will vanish:
'(Ew*,b* [g。f (W?Xi + b?)], y，
='(g。f (Xi), y，+ Wgr (δ fiADL )'0(g。f (Xi), y，+
2 Wgr Ofi,ADL )Qfi,ADL )>Wg'"(g。f (Xi), yi)
='(g。f (Xi), yi) + WJ(∆fi,ADL )'0(g。f (Xi),yi) + O(Ew*,b* ∣∣∆χik2)	(16)
Substitute Eq. (15) into the first-order term in Eq. (16), we have
W；(∆fi,ADL)'0(g。f (Xi),yi) =W；Ew*,b* [∆>iHfk),i∆χJd∆bi + O(Ew*,b*k∆χik2)
d
=∆ybi X Wg(k)EW*,b* ∆XriH(fk),i∆Xi + O(EW*,b*k∆Xik23)
k=1
d
=X Wi,(k)tr(Ew*,b* [∆χi∆>i]Hfk),i) + 0州卬*队|心以2).
k=1
(17)
Substitute Eq. (17) into Eq. (16), we have
d
'(Ew*,b* [g。f (W?Xi + b?)], yi) ='(g。f (Xi), yi) + Xwi,(k)tr(Ew*,b* [δXi∆>i]Hfk),i)+
k=1
O(EW*,b*k∆Xik32).	(18)
Substitute Eq. (18) into Eq. (14) which is the definition of '^n ADL (g。f), and recall that ∆X :=
EXi,W*,b* ∣∣∆Xi∣∣32 , we have
1n
'd ADL (g。f) := — $^'(EW b* [g。f (W?Xi + b?)], yi) = 'Dn A(g。f) + ωADL + O(A).
,P	n	,	P
i=1
(19)
20
Under review as a conference paper at ICLR 2022
Let ∆fi,ADL := f(W>xi + b?) - f(xi) = Vf(xi)>(∆w?xi + b?) + O(k∆χik2).
Applying Taylor expansion to EW?,b? ` g ◦ f(W?xi + b?), yi around f(xi) will give us
Ew*,b* ['(g ◦ f (W?xi + b?), y，] ='(g ◦ f (Xi), yi) + WgrEw*,b* Dfi,ADL]'0(g ◦ f (Xi), yi) +
1 WJEW*,b*[(∆fi,ADL )(∆fi,ADL )>]Wg '00(g ◦ f (Xi),yi)
(20)
Since EW*,b*∆f ,ADL = ∆f ,ADL, the first-order term in Eq. (20) is exactly Eq. (17):
W>Ew*,b* Dfi,ADL]'0(g ◦ f(χi),yi)
= Wgr ZiAPL '0(g ◦ f (Xi), yi)
d
=X Wi,(k)tr(Ew*,b*[∆χi∆>i]Hfk),i) + O(Ew*,b*k∆χik3)	(21)
k=1
The second-order term in Eq. (20) is
2WJEW***[(△3ADL)(∆fi,ApL)>]Wg'00(g ◦ f (xi),yi)
=WJEW*,b*[( Vf (xi)>(∆w* Xi + b*)(∆w* Xi + b?)>Vf (xi)] Wg + O(Ew* 也忸"[2)
=EWjXir ∆W* Vf (Xi) Wg ∣∣2 + Eb* ∣∣b> Vf (Xi)Wg ∣∣2 + O(Ew*,b* k∆χik4)	(22)
Substituting Eq. (21) and Eq. (22) into Eq. (20), we have
EW*,b* ['(g ◦ f (W?Xi + b?),yi)]
d
='(g ◦ f (Xi), yi) + X wi,(k)tr(EW*,b* [△xi ^>i ]Hfk),i)+
k=1
Ew*∣∣x> ∆w* Vf (Xi)Wg∣∣2 + Eb* ∣∣b>Vf (Xi)Wg ∣∣2 + O(EW*,b* k∆χik4)	(23)
Substitute Eq. (23) into the definition of '^n ADL (g ◦ f), then
1n
'Dn,ADL (g ◦ f) ：= - EEW*,b*['(g ◦ f(W*Xi + b*),yi)] = 'Dbn,A(g ◦ f) + ΩaDL + O(∆^)
n i=1
(24)
The proof is complete by Eq. (19) and Eq. (24).	□
Interpretation. Ωadl and Ωadl turn out to be: 1) Ωadl is a weighted trace expectation dependent
on the Hessian matrix of f; 2) Ωadl is equivalent to Ωadl together with the summation of two
norm expectations dependent on Vf. Therefore, the data-level DA algorithms APDL and ALDL are
expected to regularize f so that the f function class FADL enabled by ADL ∈ {APDL , ALDL} would
be reasonably expected as a subset ofFA enabled by general training algorithm A.
Sufficient conditions. Combined with Theorem 3.3, the sufficient conditions indicated here to
tighten the upper bound dFA (DSn, DTn ) of the relative transferability τ(A; DS, DT) are: data-
level DA (ADL) with DA parameters satisfying that 1) EW* [W?] = Im; 2) Eb* [b?] = ~0m; 3)
O(∆χ) ≈ 0, ∀j ∈ N+,j ≥ 3; 4) W? and b? are independent.
Empirical verification. We further provide empirical verification in Section 4 for the sufficient
conditions above, investigating the concrete cases of DA methods: 1) Gaussian noise SatiSfieS the
sufficient conditions, then we empirically show that Gaussian noise improves domain transferability
while robustness decreases abit (Figure 5); 2) Rotation, which rotates input image with a predefined
fixed angle with predefined fixed probability, violates Ew* [W?] = Im, and We empirically show that
rotation barely affect domain transferability (Figure 14 in Appendix D.3); Translation, which moves
the input image for a predefined distance along a pre-selected axis with fixed probability, violates
Eb* [b?] = ~m (Figure 14 in Appendix D.3).
21
Under review as a conference paper at ICLR 2022
Corollary B.2.1. If the neural network in Theorem B.2 is activated by Relu or Max-pooling, then
Theorem B.2 becomes
'Dn,ADL (g ◦ f) = 'Dn,A(g。f) + Ω ADL + O (∆^ ),
where ωADL =0； ωADL = 1 Pn=IhEW*∣∣ x>δ W? Vf(Xi)Wg ∣∣2 + Eb*∣∣b>Vf (Xi)Wg 俏].
Proof. Denote an L-Iayer NN g。f(x) := Wgr ∙ z[L-1], where z[l] := σ[l-1](W[>-1] ∙ z[l-1]),
l = 1, 2,3,…，L 一 1; Define that σ[0] (W>] ∙ z[0]) := x, then V2 (g。f (x)) = 0 (B.2 of Zhang et al.
(2020)). Since V2 (g。f (x)) = Wgr ∙ V2 f (x), We have V2 f (x) = 0.
Combine this with Theorem B.2, we have
nd
ω ADL = n XX wi,(k) [tr (EW*di ∆>i]Hfk)，i) ] = 0;
i=1 k=1
n
Ωadl = Ωadl + n X [Ew?∣∣x>Δw* Vf(Xi)Wg∣∣2 + Eb*∣∣b>Vf (Xi)Wg ∣∣2]
i=1
n
=n X 忸W* ∣∣x>Δw* Vf(Xi)Wg∣∣2 + Eb* ∣∣b>Vf (xi)Wg∣∣2].
i=1
□
Remark. Corollary B.2.1 analyzes special cases (Relu/ Max-pooling activation) of Theorem B.2,
giving notably different regularization effect: in these cases the APDL (average the prediction) fails as
a regularizer, therefore, doesn’t fulfill our sufficient conditions for improving domain transferability
(Theorem 3.3); ALDL (average the loss) only reserves the regularization on Vf -dependent norms,
but no longer regularizes Hf (x). Since ALDL still induces regularization, the induced sufficient
conditions analyzed after Theorem B.2 for promoting domain transferability won’t be affected.
C Adversarial Training as a Regularizer
In this section, we show, under certain conditions, why adversarial training may improve domain
generalization by viewing adversarial training as a function class regularizer.
We first provide some notation. Let
F = {fθ(∙) = WLφL-1(WLTφL-2(...) + bL-1) + bL}
where φj are activations, Wj , bj are weight matrix and bias vector, θ is the collection of parameters
(i.e. θ = (W1, b1, . . . , WL, bL). For the rest of the article, assume that φj are just ReLUs.
Now fix x ∈ X . Define the preactivation as
xe1 := W1x + b1
xej := Wjφj-1(xej-1) +bj, j ≥ 2
Define the activation pattern φx := (φ1x, . . . , φxL-1) ∈ {0, 1}m such that for each j ∈[L - 1]
φjx = 1(xej >0)
where 1 is applied elementwise.
Now, given an activation pattern φ ∈ {0, 1}m, we define the preimage X(φ) := {x ∈ Rd : φx = φ}
Theorem C.1. (In the proof of theorem 1 in (Roth et al., 2020))
Let > 0 s.t. Bp(x) ⊂ X(φx) where Bp(x) denotes the lp ball centered at x with radius . Let
P = {1, 2, ∞} and q be the Holder conjugate of P (i.e. P + 1 = 1). Then
E(x,y)〜P [l(y, f (X)) + λ mapx、kf (X)- f (X*)kq] = E(x,y)〜P [l(y, f (X)) + λ ∙ e max 一 Jf(X)V∣∣ /
x*∈BP(x)	v* : ∣∣v* kp≤1	q
22
Under review as a conference paper at ICLR 2022
Interpretation: This theorem provides an equivalence between the objective functions for adversar-
ial training (left term) and jacobian regularization (right term). We give some intuition on the size of
. Let us first consider a shallow 2 layer network f(x) = W2φ(W1x+b1). Suppose W2 ∈ Rm2×m1
and W1 ∈ Rm1 ×d. Given a matrix M, let Mj denote the jth row of M. We study the activation
pattern φx which equals
/ i{wlx + bl} ∖
φχ = (φX) =	.	I
1{wm11x+ b1m1}
We wish to compute the largest radius such that the activation pattern φx is constant within B2(x).
This is simply the distance from x to the closest hyperplane of the form HW1,b1 = {x ∈ Rd :
wj1x + bj1 = 0} where j = 0, . . . , m1 (i.e. = minj dist(x, HW1,b1 )). In particular, if w1 = Id×d
and b1 = 0, = minj ∈d |xj |.
Furthermore, we note that is nondecreasing as a function of the number of layers. However, it has
been observed empirically in (Roth et al., 2020) that approximate correspondence holds in a much
larger ball.
Definition 4. (source and target function class) LetGS, GT be fine tuning function classes for source
and target domains, respectively. We define the class of source models as
HS = GS ◦F = {gS ◦ fθ ： gs ∈G S ,fθ ∈F}
and the class of target models as
HT = GT ◦F = {gT ◦ fθ ： gT ∈G T ,fθ ∈F}
Definition 5. (empirical training objective with jacobian regularization) Let λ, > 0. Take any
hypothesis hθ = gS ◦ fθ ∈ HS .Let R(hθ) = -1 En=I '(hθ (Xi), yi) denote the empirical risk where
l(y, y) = IIy — yk2. We define the empirical training objective with jacobian regularization as
λ-
ObjA(hθ ) = RR(hθ ) + ~n X Il Jhθ (Xi)k2
i=1
Theorem C.2. Fix regularization strength λ > 0. Define
FA = {fA ∈ F : ∃gS ∈ GS s.t. ObjA(gS ◦ fA) ≤ ObjA(0)}
where 0 denotes the zero function (i.e. the class of feature extractors that outperform the zero
function). Suppose (X, y) ∈ X × Y is bounded such that max (IXI∞ , IyI2) ≤ R. Fix δ > 0.
Suppose we additionally restrict our fine tuning class models to linear models where
GS = {w : w ∈ Rd×- ,n≥ 1, min Iwj I2 ≥ δ}
(where wj is the jth column of w) and
GT = {w : w ∈ Rd×- ,n≥ 1}
(Here we are abusing notation to let gS ∈ GS to denote the last linear layer as well as the fine
tuning function).
Then for 0 ≤ λ1 < λ2
FλA2 ( FλA1 ( F
(where ( denotes proper subset). In particular, if HλA,T = GT ◦ FλA, we have
A,T
( HλA1,T ( HT
Interpretation:
At the high level, this theorem captures the idea that minimizing the empirical risk with jacobian
regularization puts a constraint on the set of feature extractors. In particular, FλA represents the
potential class of feature extractors we select after training with jacobian regularization. Therefore,
23
Under review as a conference paper at ICLR 2022
the class of fine tuned models HλA,T with feature extractors trained with jacobian regularization for
the target domain is smaller than the class of fine tuned models H with feature extractors trained
without any regularization. Furthermore, we show that the space of feature extractors shrinks as we
increase the regularization stength λ. Since we showed in section 3.3 that smaller function classes
have smaller dFA , this theorem shows that jacobian regularization reduces dFA . To connect back to
adversarial training, if satisfies the hypothesis in theorem C.1, we have that
E(x,y)〜P [l * * *(y,f (x)) + λ max k kf (X)- f (x*)kq ] = E(x,y)〜P [l(y,f (x)) + λ ∙ ɛ max - Jf(X)v||q]
x*∈BP(x)	v* : ∣∣v* kp≤1	q
Therefore, minimizing the training objective with jacobian regularization is equivalent to minimizing
the adversarial training objective. Using this connection, this theorem essentially shows that, given
sufficient number of samples, adversarial training reduces the class of feature extractors which in
turn reduces dFA .
Finally, we comment on the assumption that gS > δ. Since δ > 0 is arbitrary, we can make it as
small as we like and thus we are essentially excluding the 0 last layer which is hardly a constraint on
the function class. This assumption is necessary as we are considering regularization on the whole
model g ◦ f as opposed to regularization onjust the feature extractor. Thus, this assumption prevents
the scenario where only the last linear layer is regularized.
Proof. We first show that if0 ≤ λ1 < λ2, we have that
FλA2 ( FλA1 ( F
We first prove the following lemma
Lemma C.1. Suppose the conditions of theorem C.2 are satisfied. Suppose additionally we have
that y := n Pd=ι yi = 0 (note this occurs with probability 1 if marginal distribution over Y is
continuous). Thenfor every λ ≥ 0, there exists afunction fθ ∈ FA and a fine tuning layer g* ∈ GS
such that
ObjλA(g* ◦ fθ) = g∈inGfS ObjλA(g ◦ fθ) = ObjλA(0)
Choose another λ0 ≥ 0 (can equal λ). Then there exists a g*0 ∈ GS be the fine tuning layer such
that infg∈GS ObjλA0(g ◦ fθ) = ObjλA0(g*0 ◦ fθ) and
1n
n EJg*ofθ Exh > o
i=1
Proof. Fix α ≥ 0 and c > α
• R. Set biases
(c\
0
b1
bj=0,j≥2
0
and weights
W1
...0∖
...	0
...
..Wj
..
.. 0.
(1
0
0	. . .	0\
0	...	0
..,j ≥ 2
..
. 0.
0
0
Define xi,j be the jth entry of the data point xi . Define
αi := α • xi,1
α:
1d
1X
n
i=1
1d
1X
n
i=1
αi
y ：
yi
24
Under review as a conference paper at ICLR 2022
Now we observe that for a fixed λ ≥ 0 and any g ∈ GS, we have that
ObjA(g ◦ fθ) = R(g ◦ fθ) + λ ∙ E
1n
n EkJgm (χi)k2
1 n
n∑	(αxi,1+ C)
i=1
i=1
g9ιι ʌ
.I -yi
gd1
1n
+λ∙EnX
i=1
0∖
0
n
2
α
0
0
0
2
Therefore,
1n
-	X k(ai + c)g1
n i=1
1n
n X k(ai + c)gι
i=1
1n
-	yik2 + λ • E- y？a kg1k2
n i=1
-	yik2 + λ ∙ Ea Ilg1k2
inf objA(g ◦ fθ)
g∈GS
is equivalent to solving
g
inf
n
∑∣∣(ai + C)W - yik2 + λ
22
• Ea kwk2
(25)
Utilizing lagrange multipliers, we find the minimizer is
K y
W = δ •百
kyk
(26)
when C ≥ kyk2.
Now consider the function
1n
S(c, a)=—	k(a • xi,1 + C)g1 - yik22 + λ • Ea kg1k2
i=1
Note that this function is continuous with respect to the input (C, a). Now fix a = o,c=kyk2. set
w = δ •备.Then we have that
S(呼,0) = n X ky- yik2 < n X kyik2 =ObjA(0)
δ	n i=1	n i=1
The inequality comes from the fact that we assumed y = 0 and noting that y is the minimizer of the
functionp(z) = n ∣∣z 一 yi∣2. Continuity of S ensures that there exists ɑ0 > 0 such that
S(呼,αo) <1 X kyik2 = ObjA(O)
i=1
Now consider U(t) = S((1 +1)ky^, (1 + t)ao) for t ≥ 0. Note that U is continuous with respect
to t. Furthermore, we note that t → ∞ implies U(t) → ∞ which implies there exists some time
t = Tf such that U(Tf) > ObjλA(0). Therefore, by the intermediate value theorem, there exists a
time t = T such that U(T)=ObjA(0). Finally, set C = (T + 1)kyk2, α = (T + 1)a0, and g* as
the matrix where g； = δ •备 and 0 for the other columns. By equation 25 and equation 26 We have
objA(g* ◦ fθ)= gi∈nfs ObjA(g ◦ fθ) = U(T) = ObjA(0)
Furthermore, if We choose another λ0 ≥ 0, since C = (T + 1) ky^ > kyk- by equation 26, We have
that
ObjAO (g" ◦ fθ )= gi⅛ ObjAO (g ◦ fθ )
25
Under review as a conference paper at ICLR 2022
and
1n
—EkJg*0 fθ (xi)k2 = α ∣∣g*,l∣2 = αδ
ni=1
which is nonzero as δ > 0 and α = (T + 1)α0 > 0.
□
Clearly, we have FλA ⊂ FλA . If we can show that fθ1 6∈ FλA then we have FλA ( FλA .
Using lemma C.1 we can find fθ1 ∈ FλA such that
g∈nfs ObjAi (g ◦ f'I)=ObjAI ⑼
In addition lemma C.1 guarantees minimizers gɪ and g2 such that
ObjAi @ ◦ fθi)
1n
inGfs ObjAI (g ◦ M and n∑SllJg*°fθ (Xi) ll 2
g∈G
>0
1n
ObjA2 (g2 ◦ fθI)= infS ObjA2 (g ◦ fθI) and — EllJg*fθ1 (Xi) ll2 > 0
g∈GS ni=1
Thus, we have that
1n
ObjAi (g2 ◦ fθi) = R(g2 ◦ fθi)+ λ ∙ e—£ lJg*fθi (Xi )ll2
n i=1
1n
> RR(g2 ◦ fθi ) + λ1 ∙ e—	ll Jg*ofθi (Xi )l2	since λ2 > λl
n i=1
1n
≥ RR(g1 ◦ fθi ) + λ1 ∙ en E H Jg*ofθι (Xi )12	defof g1
n i=1
= ObjλA (0)	lemma C.1
Thus fθi 6∈ FλA which implies FλA ( FλA . It remains to show for λ1 ≥ 0, we have that FλA ( F .
Consider any g ∈ GS. For j ∈ [L], define Wj as the weight matrix where Wj = Id×d (identity
matrix) for j ∈ [L - 1] and let the final weight WL = B ∙ Id×d for some constant B > 0. Set the
bias vectors bj = 0 for j ≥ 2. Let the first bias equal b1 = R ∙ 1 where 1 is the vector of all 1's and
R is the upper bound such that kXk∞ ≤ R. Set θ = (W1, b1, . . . , WL, bL) and let hθ = g ◦ fθ
We compute
1n
ObjAl (hθ) = R(hθ) + λι ∙ e- EkJhθ(Xi)k2
n i=1
nn
=n X kB(Xi + Ri) - yik2 + n X k J%。(Xi)k2
1n
=—E llB(Xi + RI)- y，『+ B kgk2
n
i=1
1n
≥ n ɪs kB(Xi + RI)- yik2 + Bδ
We note that sending B → ∞ we get ObjλAi (hθ) → ∞ which implies that there exists a B = B0
such that ObjλAi (hθ) > ObjλAi (0). Setting B = B0 implies fθ 6∈ FλA .
□
26
Under review as a conference paper at ICLR 2022
D Extra Experiment Results
D.1 Absolute Transferability
We show the results with absolute transferability in Figure 7,8,9 and 10 respectively.
Domain Transfer Acc (%) 3	3	4 O	Ui	O	CIFAR10 -> SVHN	ImageNet-> ClFAR10			CIFAR10	-> SVHN		ImageNet > CIFAR10	
	LLRa/) • -0.1 - 0 • 0.1 .	∙ 1.0	76 74	•	LLR(λ∕) - -0.01 - 0 •	∙ 0.01 • 0.1	40 35	• ∙	LLOT(∣∣gs∣∣2) • 0.01 • 0.1 • 1.0 ・ 10.0	76	* * *.	LLOT(∣∣gs∣∣2) • 0.5 • 1 • 2 • 5
	∙β ∙ R: -0.798	∙	72 70	• R: -0.922	∙	30 25 20	R: -0.904	•	74 72	R: -0.929	•
10	20	30	18	20	22	24	10	20	30	20	30	40
Robust Acc (%)	Robust Acc (%)	Robust Acc (%)	Robust Acc (%)
Figure 7:	Robustness and absolute transferability when we control the norm of last layer with last-
layer regularization (LLR) and last-layer orthogonal training (LLOT) with different parameters.
CIFAR10 -> SVHN
R: -0.619
§35
u
u
<
φ 30
右
c
ro
H 25
c
-ra
E
§20
20 30 40 50 60 70
Robust Acc (%)
JR(A7)
- 0
• 10
• 100	∙ ∙
• 1000
90
85
80
75
ImageNet-> ClFAR10
JRW7)
- 0
• 0.001	∙ ∙∙
• 0.01	∙ 1
• 1.0	∙
35
CIFAR10 -> SVHN
ImageNet -> CIFAR10
75 WD(Aw)
• 0.0001 .	*
•	0.0005 ∙ ∙	∙
•	0.001
70
70 R： 0.902
15	25	35	45	55
Robust Acc (%)
30 .
R: 0.889
10
WDaW)
■ 0.0005
• 0.001
• 0.005
・ 0.01
20
Robust Acc (%)
65 R： 0.909
10	15	20
Robust Acc (%)
Figure 8:	Robustness and absolute transferability when we regularize the feature extractor with
Jacobian Regularization (JR) and weight decay (WD) with different parameters.
λ CIFARIO -> SVHN
左 λe Gauss(σ)
U	∙ 0	. ∙
⅛	∙ 0.05	：
J ,c	∙ 0.25	.	∙
2 40 ∙ ι,o	∙∙
S
C
(O
H 35 ■
c
'S
I 30	β
Q R: 0.768
20 30 40 50 60 70
Robust Acc (%)
ImageNet -> CIFAR10
•	Gauss(σ)
• 0
• ∙ 0.05
80	∙ 0.25
CIFAR10 -> SVHN
ImageNet > ClFAR10
45
40
75
R: -0.306
20	30
Robust Acc (%)
35
Pos(b)
♦ 1
• 4
• 8
77
75
73
R: 0.629
20	30	40	50
Robust Acc (%)
Pos(b}
• 1
• 4
• 8
R: 0.963
20	25
Robust Acc (%)
30
Figure 9:	Robustness and absolute transferability when we use Gaussian noise (Gauss) and posterize
(Pos) as data augmentations with different parameters.
D.2 Results of Other Model Structures
To further validate our evaluation results, we evaluate the experiments on another model structure.
We use a simpler CNN model for CIFAR-10 to SVHN and a more complicated WideResNet-50 for
ImageNet to CIFAR-10. The CNN model consists of four convolutional layer with 3 × 3 kernels and
32,32,64,64 channels respectively, followed by two hidden layer with size 256. A 2 × 2 max pooling
is calculated after the second and fourth layer. Other settings are the same as in the main text. Note
that in some settings the new model cannot converge, and therefore we will omit the result. In
addition, Jacobian regularization cannot be applied on WideResNet-50 because of the large memory
cost, so we do not include it in the figures. The results are shown in Figure 11, 12 and 13.
27
Under review as a conference paper at ICLR 2022
(东)0u< c-reE0Ω
90
80
ImageNet-> ClFAR10
Rescale(m)
• 1
• 2
• 4
• 8
ImageNet -> ClFARlO
80
Blur(k}
• 1
• 5
* 11
R: -0.557
10	20
Robust Acc (%)
15	20
Robust Acc (%)
Figure 10:	Robustness and absolute transferability when we use rescale and blur as data augmenta-
tions with different parameters.
求 uu< HQo>4-,uα
C∣FAR1O -> SVHN	ImageNet-> C∣FAR1O
LLR(λ∕)
.	■	∙	-0.01
0	∙	.	∙	0
C	∙	0.01
0 LLR(λ∕)	♦	∙ 0.1
- -0.1
- 0
• 0.1	.	∙
• ^5 ∙
^5 ∙ .
R: -0.468	R: -0.890
16	18	20	22	24	28	32
Robust Acc (%)	Robust Acc (%)
C∣FAR10 -> SVHN
5	LLOT{∣∣gs∣∣2)
♦ 0.01
•	∙ 0.1
o ・	∙ io
-5
-10	,
R: -0.994
15	25	35
Robust Acc (%)
ImageNet-> ClFAR10
LLOT(∣∣gs∣∣2)
•	♦	1
•	・2
*	∙	5
0 R: -0.977
25	27
Robust Acc (%)
Figure 11:	Robustness and transferability for the other model structure when we control the norm
of last layer with last-layer regularization (LLR) and last-layer orthogonal training (LLOT) with
different parameters.
D.3 Data Augmentations that Violate Sufficient Condition
We study rotation and translation, the two data augmentations that violate the sufficient condition
for regularization. The result is shown in Figure 14. We observe that these augmentations do not
have an obvious impact on domain transferability.
D.4 Robustness Evaluation with AutoAttack
Besides PGD attack, we also evaluate the model robustness using the stronger AutoAttack. We
use APGD-CE, APGD-T and FAB-T as the sub-attacks in AutoAttack with 100 steps. Since the
accuracy will decrease after the stronger attack, we use a slightly smaller = 0.2 to better visualize
the trend. The results are shown in Fig. 15. We can observe that the trend is similar with what we
observed before when we used the PGD attack - domain generalization is an effect of regularization
and data augmentation, and it is sometimes negatively correlated with model robustness. Also,
求 uu< HQo>4-,uα
C∣FAR10 -> SVHN
0 .	JrW
υ	♦ 0
• 10
• 100
-10	∙ 1000
-20	∙	∙
R： -0.832	∙ ∙ ∙ ∙
20	40	60
Robust Acc (%)
30 ∙
20
10
C∣FAR10 -> SVHN
WDaW)
• 0.0005
• 0.001
• 0.005
• 0.01
0	.	.
R: -0.607
10	20
Robust Acc (%)
10
5
0
ImageNet > ClFARlO
■	WDaW)
• 0.0001
• 0.0005
• 0.001
R： -0.941
10	15	20	25
Robust Acc (%)
Figure 12:	Robustness and transferability for the other model structure when we regularize the fea-
ture extractor with Jacobian Regularization (JR) and weight decay (WD) with different parameters.
28
Under review as a conference paper at ICLR 2022
5 0 5 0
1 1
(求)uu< HQ 3>4e⑥α
Cifario -> SVHN	ImageNet -> Cifario
60
C∣FAR10 -> SVHN
ImageNet-> C∣FAR1O
Gauss(σ)
♦ 0
• 0.05
• 0.25
• 1.0
40
Gauss(σ)
• 0
• 0.05
• 0.25
Pos(b)
• 1
• 4
• 8
R: 0.358
30	40	50
RobustAcc (%)
20
R: -0.505
O
20
30	40
Robust Acc (%)
R: 0.776
Pos(b)
• 1
• 4
• 8
R： 0.472
20	30	40
Robust Acc (%)
20	30	40
Robust Acc (%)
Figure 13:	Robustness and transferability for the other model structure when we use Gaussian noise
(Gauss) and posterize (Pos) as data augmentations with different parameters.
5 O
(求)y<HQ φ>-⅛-φtf
C∣FAR10 -> SVHN		ImageNet-> C∣FAR1O		C∣FAR10 -> SVHN		ImageNet-> ClFAR10
Rotate(") ♦ 0 • 15 •	∙ 45	5	Rotate(β) • 0 • 15 • 45	0		3	・
• * R: 0.012	0	・• R: 0.978	-3 -6	• ? e Translate(d) ♦ 0 • 0.05 • 0.1 n n -	∙ 0.25 R: 0.716	0 -3	■・ • Translate(d) • 0 • 0.05 • 0.1 R: 0.238	* 0 25
20	25 Robust Acc (%)		20	25 Robust Acc (%)		10	15	20 Robust Acc (%)		17	20	23 Robust Acc (%)
Figure 14:	Relationship between robustness and transferability when we use rotation and translation
as data augmentations.
augmentations like rotation and translation, which violates the sufficient condition, do not improve
the domain generalization.
29
Under review as a conference paper at ICLR 2022
求 uu< HQo>4-,uα
CIFAR10 -> SVHN
• LLR(λ∕)
• 0.1
• 1.0
O
-5
-10
CIFAR10 -> SVHN
.	LLOT(∣∣gs∣∣2)
♦ 0.01
• 0.1
• 1.0
・ 10.0
R: -0.846
5
O
-5
-IO
10	15	20
RobustAcc (%)
CIFAR10 -> SVHN
§ 40
u
< 30
R: -0.947	.
10	15	20
Robust Acc (%)
C∣FAR10 -> SVHN
IK JM)
15	. o
•	IOO
•	IOOO
10
9
5
O ∙
R: 0.316
20 30 40 50 60 70
Robust Acc (%)
CIFAR10 -> SVHN
40	∙	WD(Aw)
∙ 0.0005
• 0.001
• 0.005
•	∙ 0.01
20
O R: -0.724
5	10
Robust Acc (%)
Gauss(σ)
• O
• 0.05
• 0.25
• 1.0
CIFAR10 -> SVHN
Pos(b}
10	∙ 1
• 4	,
• 8
5
CIFAR10 -> SVHN
CIFAR10 -> SVHN
Rotate(")
- 0
• 15
• 45
0
-3
⅛ 10
ω
O
20 30 40 50 60 70
Robust Acc (%)
R: 0.146
R: 0.579
20	30	40	50
Robust Acc (%)
0
R: 0.439
15	20
Robust Acc (%)
Translated)
•	0
•	0.05
•	0.1
• 0.25
-6
R: 0.680
8	10	12
Robust Acc (%)
Figure 15: Relationship between robustness and transferability on CIFAR-10 when we use AutoAt-
tack to evaluate model robustness.
30