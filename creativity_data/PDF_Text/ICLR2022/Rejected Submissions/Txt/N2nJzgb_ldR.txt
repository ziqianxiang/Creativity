Under review as a conference paper at ICLR 2022
FastRPB: a Scalable Relative Positional En-
coding for Long Sequence Tasks
Anonymous authors
Paper under double-blind review
Ab stract
Transformers achieve remarkable performance in various domains, including
NLP, CV, audio processing, and graph analysis. However, they do not scale well
on long sequence tasks due to their quadratic complexity w.r.t. the input’s length.
Linear Transformers were proposed to address this limitation. However, these
models have shown weaker performance on the long sequence tasks comparing to
the original one. In this paper, we explore Linear Transformer models, rethink-
ing their two core components. Firstly, we improved Linear Transformer with
Shift-Invariant Kernel Function SIKF, which achieve higher accuracy without
loss in speed. Secondly, we introduce FastRPB which stands for Fast Relative
Positional Bias, which efficiently adds positional information to self-attention us-
ing Fast Fourier Transformation. FastRPB is independent of the self-attention
mechanism and can be combined with an original self-attention and all its ef-
ficient variants. FastRPB has O(N log N) computational complexity, requiring
O(N) memory w.r.t. input sequence length N.
We compared introduced modifications with recent Linear Transformers in dif-
ferent settings: text classification, document retrieval, and image classification.
Extensive experiments with FastRPB and SIKF demonstrate that our model sig-
nificantly outperforms another efficient positional encodings method in accuracy,
having up to x1.5 times higher speed and requiring up to x10 times less memory
than the original Transformer.
1	Introduction
Transformer architecture (Vaswani et al., 2017) originally proposed for machine translation tasks
has shown impressive results in a wide range of domains, including natural language processing,
image recognition, audio captioning, graph analysis, and bioinformatics (Lin et al., 2021). How-
ever, in applications that require processing long sequences, the benefits of transformers are often
accompanied by high consumption of computational and memory resources. The main bottleneck
is the transformer’s core component, the self-attention mechanism. Self-attention computes similar-
ity scores for all pairs of tokens in the input sequence, and therefore, it has a quadratic complexity
O(N1 2) in computations and memory relative to the length of the input sequence N1.
Recently, several approaches have been introduced to reduce the computational complexity and
memory footprint of self-attention. Some works utilize the sparsity of the attention map (Belt-
agy et al., 2020), others express self-attention as a linear dot-product of kernel feature maps
φ(∙) (KatharoPoUlos et al., 2020), or utilize random feature vectors (Choromanski et al., 2020). Pro-
posed approaches reduce the computational complexity to O(N)2. One of the promising variants of
a transformer is the Linear Transformer (KatharoPoulos et al., 2020) since, along with linear com-
Plexity, it requires constant O(1) memory in auto-regressive language modeling. ExPeriments with
the long sequence benchmark Long Range Arena (LRA) (Tay et al., 2020)3 have indeed shown that
1The full comPlexity of self-attention also dePends on attention head size D. For the original self-attention,
comPlexity is O(N 2D)
2In contrast, for Linear Transformer (KatharoPoulos et al., 2020; Choromanski et al., 2020), the comPlex-
ity of linear self-attention is O(N D2). In long sentences, N is assumed to be around thousands of tokens.
Therefore, switching to linear self-attention aPPears beneficial.
3In benchmark sequences ranging from 1K to 16K tokens
1
Under review as a conference paper at ICLR 2022
XW-4
-1.4
-1.2
-3600	-2800	-2000	-1200	-400	400	1200	2000	2800	3600	-0.01
H-0.00
-1800	-1400	-1000	-600	-200	200	600	1000	1400	1800	--0.01
-0.02
-	1.0
-	0.8
-	0.6
-	0.4
-	0.2
-900	-700	-500	-300	-100	100	300	500	700	900
Relative distance between tokens, i
Figure 1:	Learned weights wi assigned to pair-
wise distances between tokens i in FastRPB 1D
for different text LRA tasks.
Figure 2:	Learned FastRPB 2D weights assigned
to distances from pixel (12, 10) to each other
pixel in MNIST 28 × 28 image classification.
the Linear Transformer is 5x times faster than the vanilla Transformer in training speed. However,
the drawback of this architecture is lower performance compared to the original Transformer.
One way to reduce the performance gap between the Linear Transformer and the original one is
to select a more suitable kernel function φ(∙) in linear attention (Choromanski et al., 2020; SChlag
et al., 2021). The poor performance of efficient transformers on LRA can also be attributed to
the model’s ability to capture positional information. The original Transformer model utilizes only
absolute positional information, which is added through positional embeddings to contextual em-
beddings of the tokens. Other approaches, which enrich self-attention with additional information
about relative distances between tokens, have recently shown visible improvements in performance.
Some of them directly add a matrix of relative distances to the attention map (Shaw et al., 2018),
others compute separate attention scores between positional embeddings (He et al., 2020). We hy-
pothesize that adding relative positional information could improve efficient transformers. However,
most of the current implementations possess quadratic computational complexity, which neutralizes
all efficiency of the Linear Transformer. To deal with this problem, a linear complexity stochastic
positional encoding (SPE) was proposed (Liutkus et al., 2021). Despite linear asymptotic, SPE re-
mains relatively inefficient in training time due to its stochastic nature, while the improvement in
accuracy it brings is relatively small on several LRA tasks.
The contribution of this paper is two-fold. At first, we propose the Shift-Invariant Kernel Function
(SIKF). It could be used as a kernel for the Linear Transformer model and holds the shift-invariance
property of softmax in the original attention. Second, we propose Fast Relative Positional Bias
(FastRPB) — Fast Fourier Transform-based bias for self-attention that represents relative posi-
tional information within sequences, has O(N log N) complexity and requires only O(N) memory.
FastRPB is orthogonal to the self-attention mechanism and can be combined with both efficient and
original implementations.
We observed that SIKF is comparable to more complex kernels (Choromanski et al., 2020; Schlag
et al., 2021) while being as fast as the original one (Katharopoulos et al., 2020). We also evaluated
FastRPB under different long-context scenarios, such as image classification and Long Range Arena
tasks. Through a comprehensive study, we showed that the proposed technique outperforms the
prior fast positional encoding method (Liutkus et al., 2021) by a significant margin without adding
a substantial computational footprint.
2	Recent works
2.1	Attention Mechanism
The core component of Transformer (Vaswani et al., 2017) is the attention layer, which computes
attention weights Am,n, measuring how important the role of n-th key word is in shaping the mean-
ing of m-th output word. Using Am,n we can construct attention matrix A ∈ RM ×N , and rewrite
2
Under review as a conference paper at ICLR 2022
gqe6δ
500	1000	1500	2000	2500
Num pixels
(a) Evaluation memory consumption.
puoua,s.la,d sa,-dujes
256	576	1024	1600	2304 2500
Num pixels
(b) Evaluation time.
Figure 3: Evaluation time and memory for various type of transformer on Nvidia A100 with respect
to number of pixels in the input image. For memory consumption y-axis is log-scaled.
the equation using matrix notation. The output of the attention layer Y is defined based on three
matrices Q ∈ RM×D, K ∈ RN×D and V ∈ RN×D (Queries, Keys, and Values) as follows:
Y = AV = Softmax(A)V = Softmax(QK T/√D)V	(1)
In vanilla Transformer, the attention matrix A is computed explicitly, which leads to a O(MND)
complexity, and O(MN) memory to store the matrix4.
2.2	Efficient Attention Mechanism
Linear Transformer variants (Katharopoulos et al., 2020; Choromanski et al., 2020) are a way to
reduce the complexity of attention from quadratic to linear using the associative property of matrix
products and kernel reformulation of attention.
By substituting the Softmax function in Equation 1, we obtain m-th row ym of matrix Y:
=Pn exp(qmkn∕√D)Vn = Pn sim(qm, kn)Vn
ym — Pn exp(qmkn∕√D) — TnlimqmXr
(2)
where exp(qlknj√D) is generalized by any arbitrary defined similarity function sim(qm,, kn).
The core idea of Linear Transformer is to replace sim(qm, kn) with a dot-product using kernel
function φ(∙) and then use an associative property of matrix products as follows:
_ Pn Φ(qm)TΦ(kn)Vn _ Φ(Qm)T Pn Φ(kn)Vn
ym = Pn φ(qm )T φ(kn) =	Φ9m)T Pn φ(kn)
(3)
Original attention has O(N 2D) time complexity, where N represents the sequence length, and
O(N 2) for the memory footprint, while linear attention has time and memory complexity O(N D2),
which scales linearly with sequence length N .
2.3	Kernel function variants
An open question is selecting an appropriate kernel function for Linear Transformer since different
kernel functions dramatically affect trained model accuracy and speed.
ELU + 1. Originaly proposed kernel is an element-wise ELU(∙) + 1 (Katharopoulos et al., 2020):
φ(x) = ELU(x) + 1 = x + 1, x > 0	(4)
exp(x), x ≤ 0
The choice of ELU(∙) + 1 over ReLU(∙) was prompted by its non-zero gradients for negative values.
4In case of self-attention, M equals to N, and thus the complexity is O(N 2D) and memory requirements
is O(N 2).
3
Under review as a conference paper at ICLR 2022
Performer. The core idea is to approximate the softmax on average using random features (Choro-
manski et al., 2020). The kernel function is evaluated as:
Here
φ(X)=hx e eχp(Rx)
φ(x) = √m [exp(-Rx)
,where h(X) = √12exp (-2kxk
(5)
exp(RX)
exp(-RX)
states for concatenation of vectors exp(RX) and exp(-RX) along feature di-
mension, each row r ∈ RD of matrix R ∈ RR×D is sampled from normal distribution N (0, ID),
and dimension size R is a hyperparameter.
The main drawback of Performer is that sampling of matrix R requires extra computations and
introduces variance into the model’s output.
DPFP. Deterministic parameter-free projection is an alternative approach (Schlag et al., 2021). The
kernel function, designed to facilitate orthogonality in the projected space RDproj , is described as:
φi∙ν(x) = ReLU ( -X) ReLU ( -X)	, where φ : RD → RDprOj	(6)
here i ∙ V indicates the index of vector φ(x), i ∈ {1, 2,…,2D} is an index and V ∈ {1,2,…,2D -1}
is a hyperparameter, controlling the capacity of kernel function φ(∙). Linear Transformer with
DPFP model outperforms model with default kernel and Performer, even if Dproj is relatively small.
Also, DPFP showed to be faster than models utilizing random features, but still slightly slower than
ELU + 1.
2.4	Positional Information
Attention is permutation-invariant, which means that the attention layer does not make use of the
order of the sequence. There exist different ways to encode positional information in the attention:
Absolute Positional Encoding (APE) proposed in original Transformer architecture uses real-
valued vector pi ∈ RD assigned to each positions i. Some approaches, such as vanilla Trans-
former (Vaswani et al., 2017), use predefined vectors, while others employ learnable vectors, e.g.,
in BERT (Devlin et al., 2018).
Relative Positional Encoding (RPE) is complement to the absolute positional encoding, which
explicitly adds relative positional information between vectors (Shaw et al., 2018) to the model.
Raffel et al. (2019) proposed to directly embed positional information to the matrix A (see the
Equation 1). This approach was then improved by separating semantic correlation of words and
their positions correlation by Ke et al. (2020). The component Am,n of matrix A then calculated as:
Am,n = √DqTTkn + √D (UQpm)T(UKPn)	⑺
where pn and pm are embeddings of corresponding positions n and m, and UQ, UK ∈ RD×D are
learnable projection matrices for the positional embedding.
By design, these approaches have quadratic computational complexity. Thus their usage with Linear
Transformer is challenging since the naive application will neutralize all effectiveness of linear
computation time.
Stochastic Positional Encoding (SPE) proposed by Liutkus et al. (2021) is, to the best of our
knowledge, currently the only positional encoding method compatible with Linear Transformer
variants due to its linear complexity. The key idea for SPE is to represent the attention relative
distances matrix as a covariance. Following the notation from equation 1, we can express Am,n as:
D
Am,n = EQm,d ∙ Pd(m,n) ∙ K%d∕√D, where Pd(m,n)=旧瓦(Μ)∙ kd(n)]	(8)
here Qm,d and Kn,d are components of matrices Q and K respectively. qd(m) and kd(n) are
two real and zero-mean random variables such that their covariance function matches Pd. Varying
the structure of matrices Pd authors designed two variants of SPE: sinSPE and convSPE. The first
4
Under review as a conference paper at ICLR 2022
Positional Encoding
	None	FastRPB	sinSPE	convSPE	RPE
Original	OOM	OOM	OOM	OOM	OOM
N Linear, DPFP	61.01 ±0.79	64.79 ± 1.52	61.53 ± 0.75	63.52 ± 0.71	N/A
< Linear, SIKF	59.51 ±0.3	67.19 ± 1.64	62.0 ± 0.36	58.93 ± 1.65	N/A
V Linear, ReLU	58.78 ± 0.93	64.94±1.6	62.39 ± 0.59	61.00 ± 1.34	N/A
Performer	59.84±1.46	66.65 ±0.91	60.00 ± 1.20	57.22	N/A
Original	14.43 ± 4.73	14.6±4.14	-	-	OOM
区 Linear, DPFP	20.67 ± 3.95	17.97±11.68	17.57 ± 0.18	16.17 ± 5.89	N/A
2 Linear, SIKF	12.55±3.8	11.47±4.79	15.25 ± 8.97	17.8 ± 0.0	N/A
J Linear, ReLU	17.58±1.01	17.67 ± 0.59	17.80 ± 0.00	9.50 ± 1.17	N/A
Performer	17.80±0.00	17.75 ± 0.39	17.43 ± 0.32	17.80	N/A
Original	41.88±0.48	39.02±0.22	-	-	N/A
X Linear, DPFP	41.79±0.27	38.73 ± 0.09	41.97 ± 1.24	41.33 ± 0.84	N/A
L Linear, SIKF	41.96±0.47	38.89 ± 0.15	40.73 ± 0.58	42.94 ± 0.51	N/A
D Linear, ReLU	42.25 ± 0.01	38.44 ± 0.38	41.21 ± 1.18	39.96 ± 1.31	N/A
Performer	41.81 ± 1.16	32.26 ± 9.53	41.12 ± 1.70	40.06	N/A
Original	62.27 ±0.8	62.02 ± 2.02	-	-	55.7 ± 1.94
Linear, DPFP	62.78 ± 0.48	63.05 ± 0.62	62.76 ± 0.21	62.78 ± 0.48	N/A
E Linear, SIKF	61.64±0.82	62.35 ± 0.24	63.37 ±1.4	62.24 ± 0.56	N/A
Linear, ReLU	58.78 ± 0.93	63.95 ± 0.16	62.39 ±0.59	61.00 ± 1.34	N/A
Performer	59.84±1.46	62.66 ± 0.11	60.00 ± 1.20	57.22	N/A
Table 1: Experiments on Long Range Arena benchmark, the best model is boldface, the double
underline is a top-2 result. Results for Performer and Linear Transformer (ReLU) are copied from
SPE (Liutkus et al., 2021), except experiments with FastRPB. We mark experiments that failed due
to memory limitations as OOM (Out of Memory). Since RPE is compatible only with the Original
Transformer, we marked other experiments as Not Applicable (N/A). RPE is N/A for CIFAR since
plain RPE is designed for 1D sequences. We marked experiments that were too long to train as"-”.
one yields periodic covariance functions, which showed to be beneficial in such tasks as music
generation. The second utilizes vanishing covariance functions, a promising concept introduced in
Wang et al. (2020), which yields notably smaller validation losses in some SPE experiments.
Although SPE was beneficial in some music generation tasks, it still requires many computations due
to its stochastic nature. In practice, it could be dozens of times slower than the original Transformer,
as we will show feather.
3	Approach
3.1	Shift-invariant Kernel Function (SIKF)
We hypothesize that the shift-invariance property of softmax function (i.e., the fact that
softmaxi (x + c) = softmaxi (x), where x is some vector, c is a constant, which is added to ev-
ery component of x) is an important property which makes original Transformer perform better
than Linear Transformer with an arbitrary kernel. Based on this assumption, we propose SIKF as
φ(x) = exp (x), which satisfies the property of shift-invariance. Ifwe substitute this function in the
linear attention from Equation 3, then for every real-valued constants c and d we will get:
φ(qm + C)T Pn φ(kn + d)Vn _ ecΦ(Qm)T Pn edφ(kn)Vn _ Φ(qm)T Pn Φ(kn)Vn
φ(qm + C)T Pm φ(kn + d) = ecΦ(qm)T Pn edΦ(kn) = Φ(qm)T Pn Φ(%)	()
Thus, attention in Linear Transformer with exp(∙) kernel function holds the same shift-invariance
property as plain softmax.
Based on our experiments, we conclude that SIKF is faster than Performer and DPFP, simultaneously
has comparable accuracy, and does not provide an extra memory footprint, which is essential for
scaling Linear Transformer on extremely long sequences.
5
Under review as a conference paper at ICLR 2022
Model	w/o FastRPB	w/ FastRPB
Original	97.34 土 0.23	98.27 ± 0.19
Linear, DPFP	97.09 ±0.19	97.66 ±0.22
Linear, SIKF	96.49 土 0.20	97.37 土 0.35
Linear, ELU + 1	94.01 ±0.31	96.71 ±0.40
Performer	96.6 ± 0.29	97.52±0.26
Table 2: MNIST F1 score. All the experiments run on 4 Nvidia Tesla T4.
3.2	Fast Relative Positional Bias (FastRPB)
Although adding positional information in the attention mechanism is beneficial for model accu-
racy, current approaches are relatively inefficient in long sequences in terms of speed and memory
footprint. In this context, there is a desire to design an approach that will add relative positional in-
formation to attention efficiently and simultaneously be compatible with various efficient attention
modifications. To achieve this goal, we propose FastRPB5 as a separate term for attention.
More formally, output matrix Y of attention layer with FastRPB is defined as:
Y = AttentionVariant(Q, K, V ) + WV	(10)
here matrix W ∈ RM×N consists of learnable weights Wm,n representing relative distances be-
tween m and n embedding vectors from matrix V . Note that Equation 10 is invariant of choosing
a specific attention mechanism and could be used with both vanilla attention and its linear variants
(Equations 1 and 3 respectively).
One can think of the matrix W as a bias term to the usual attention matrix A from equation 1,
correcting the attention weights according to the relative distance between corresponding tokens.
However, adding positional bias term in the Equation 10 still requires O(NMD) computations due
to the matrix product and O(NM) memory to store the bias matrix W6. In this regard, in the
following two subsections, we will construct FastRPB positional bias terms matrices W1d and W2d
for different types of sequences, that can be efficiently multiplied by V . W1d utilized in the case of
1D sequences (e.g., natural language texts), and its coefficients will correspond to distances between
words in 1D sequences. For 2D sequences we will utilize W2d , which coefficients will represent
distances between elements of 2D sequences (i.e., pixels). We will show that these specific matrices
W1d and W2d could be multiplied with V using only O(DN log N) computations, and requiring
only O(N) memory.
Further, we work with self-attention — a variant of attention mechanism, where input and output
sequences lengths are the same, i.e. N = M . In the general case of attention, when we have an
input sequence of length N and an output sequence of length M, we can pad the longer one to make
the input and output lengths match.
3.2.1	1D sequence case
Suppose we have a 1D sequence with N tokens. In such a sequence, there are exactly 2N - 1
relative distances between tokens7 . Let’s assign a learnable parameter wi ∈ R for each relative
distance i ∈ {-N + 1, ..., -1, 0, 1, ..., N - 1}. We then will obtain a set of parameters:
{w-N+1, ..., w-1, w0, w1, ..., wN-1}	(11)
Next we will construct matrix W1d using parameters {wi}iN=--1N+1. The basic intuition is to make
(n, m)-th element of matrix W1d to be assigned to the relative distance between from m-th token to
5There was a desire to name FastRPB as FastRPE to represent that it is like a faster RPE. However, we
change one letter to emphasize that FastRPB is orthogonal to the selection of an attention algorithm and could
be seen as a separate bias term to the attention map.
6O(N 2D) and O(N 2) respectively in the case of self-attention
7Relative distance from m-th token to n-th token is m - n, which can have both positive and negative
values, in this regard we have exactly 2N - 1 learnable parameters
6
Under review as a conference paper at ICLR 2022
n-th token, i.e., wm-n. Therefore matrix W1d will have the following structure:
	w0	wi	W2	∙	•	WN-i∖	
	w-i	w0	Wi	∙	•	WN-2	
Wid =	. .	. .	.. .	. .	(12)
	. w-N +i	. w-N+2	. W-N+3	∙	• •	W0	
By definition, Wid is a ToePlitz matrix (Gray, 2001). A naive way to calculate product Wid ∙ V
requires O(N 2D) computations in case of self-attention8. It turns out that it can be efficiently
multiplied by a matrix V according to the following proposition:
Proposition 3.1 For every Toeplitz matrix Wid ∈ RN×N and for every matrix V ∈ RN×D, matrix
product Wid ∙ V requires O(DN log N) operations and O(N) memory. Here N is length of the
input sequence.
Using proposition 3.1 we can claim that FastRPB for 1D sequence will require O(DN log N) com-
putational operations. Moreover, we only need O(N) memory for storing parameters {wi}iN=--iN+i,
generating Toeplitz matrix Wid. For the proof and a more detailed explanation of proposed proper-
ties see Appendix B.2.
3.2.2	2D sequence case
In the case of 2D sequences (e.g., images), a similar matrix to Wid could be defined. We will call
this matrix W2d, and it will consist of learnable weights assigned to pairwise distances from each
pixel of the image to the rest of the pixels. Here we will consider only the case of square images of
size N × N 9.
The natural way to process images of the size N × Nin the Transformer model is to flatten them into
a vector of size N2. In this regard, matrix of pairwise distances W2d needs to be of size N2 × N2.
For simplicity we will assume images to be presented as a N × N matrix, and W2d will be expressed
as a tensor W2d of size (N × N) × (N × N), which (n, m, l, k) component represents distance from
pixel (l, k) to pixel (n, m).
We will assume the distance between two pixels to be a sum of two distances: the vertical and
horizontal10. In this regard, tensor W2d can be decomposed on vertical and horizontal tensor terms
X and Y, respectively, as W2d = X + Y. Similar to the 1D case, we will assign shared learnable
parameters {wi}iN=--iN+i for horizontal and vertical distances. Then to compute a matrix product of
tensor W2d of size N2 × N2 with matrix V of size N2 × D we will simply flatten tensors X and Y,
and obtain matrices Xflat and Yflat of shape N2 × N2, and compute W2dV as XflatV + YflatV.
It turns out that the structure of matrices Xf lat and Yf lat is very similar to Toeplitz matrices from
the Section 3.2.1. In this regard, W2d can be multiplied by V efficiently, according to the following
proposition:
Proposition 3.2 Product of matrix W2d ∈ RN2×N2 with matrix V ∈ RN2×D using matrices Xflat
and Yflat requires O(DN log N) and O(N) memory.
Using the above proposition 3.2 we conclude that FastRPB 2D will require O(DN log N) com-
putational operations. Moreover, it is not essential to store whole tensors X and Y to compute the
product, we only need O(N) of memory for parameters {wi}iN=--iN+i generating this tensors. (See
Appendix B.3 for the proof).
4	Experiments
Long Range Arena. We evaluate proposed methods in the Long Range Arena (Tay et al., 2020),
a benchmark for efficient Transformers with several text and image long sequence tasks. The
8Matrix V has size N × D, where D is a hidden size
9If we work with non-square images of size N × M , we can simply pad them with zeros to make it square.
10If we consider two pixels p1 = (3, 2) and p2 = (0, 1) of image of size 4 × 4, the horizontal relative
distance from pixel p2 to pixel p1 then will be 2 - 1, and the vertical will be 3 - 0
7
Under review as a conference paper at ICLR 2022
Training time (hours)	∣	Peak Memory Usage (GB)
	None	FastRPB	sinSPE	convSPE	RPE	None	FastRPB	sinSPE	convSPE	RPE
Original	OOM	OOM	OOM	OOM	OOM	5.81	6.03	一	—	9.69
N Linear, DPFP	0.36	0.43	2.45	15.41	N/A	0.31	0.57	0.78	0.87	N/A
V Linear, SIKF	0.36	0.45	1.22	9.12	N/A	0.31	0.57	0.78	0.83	N/A
V Linear, ReLU	0.36	0.45	1.26	9.13	N/A	0.31	0.57	0.78	0.83	N/A
Performer	0.6	0.79	1.6	10.52	N/A	0.54	0.68	0.77	0.87	N/A
Original	0.74	0.85	一	—	OOM	3.25	3.49	一	—	3.66
& Linear, DPFP	0.26	0.36	2.3	13.8	N/A	0.68	0.85	1.32	1.33	N/A
L Linear, SIKF	024	0.34	0.95	6.85	N/A	0.68	0.85	1.32	1.33	N/A
L Linear, ReLU	0.24	0.34	0.98	6.87	N/A	068	0.85	1.32	1.33	N/A
Performer	0.38	0.48	1.06	8.7	N/A	067	0.9	1.03	1.32	N/A
Original	1.94	1.97	一	—	N/A	12.36	12.39	一	—	N/A
L Linear, DPFP	1.94	1.97	2.07	2.44	N/A	12.36	12.39	12.57	12.58	N/A
W Linear, SIKF	1.94	1.96	2.06	2.43	N/A	12.36	12.39	12.57	12.58	N/A
D Linear, ReLU	1.94	1.97	2.06	2.44	N/A	12.36	12.39	12.57	12.58	N/A
Performer	1.94	1.96	2.07	2.44	N/A	12.36	12.39	12.57	12.58	N/A
Original	1.81	2.24	一	—	6.58	0.52	0.52	一	—	0.78
Linear, DPFP	1.48	1.58	4.25	13.56	N/A	0.23	0.23	0.33	0.43	N/A
局 Linear, SIKF	1.56	1.62	3.52	9.85	N/A	0.23	0.23	0.33	0.43	N/A
Linear, ReLU	1.46	1.62	3.54	9.85	N/A	0.23	0.23	0.33	0.43	N/A
Performer	1.93	2.38	5.81	40.09	N/A	0.33	0.41	0.33	0.47	N/A
Table 3: Benchmark results on LRA with experiment setup proposed in SPE (Liutkus et al., 2021).
All the above experiments were conducted using a single Nvidia A100 GPU. The best model is
boldface, the double underline is a top-2 result, and the single underline is a top-3 result. We mark
experiments that failed due to limited memory as OOM (Out of Memory). We did not run Original
Transformer with sinSPE and convSPE since they require too much time to train.
main challenge of these tasks is dictated by the large sequence lengths, which average varies from
1K to 16K token11. In our experiments, we used the following tasks from this benchmark: (1)
ListOps, which test if a model is capable of parsing hierarchical expressions (Nangia & Bowman,
2018); (2) TC, which consist of movie review sentiment analysis on the IMDB corpus (Maas et al.,
2011); (3) All About NLP (AAN), which evaluates the model performance in matching and retrieval
tasks (Radev et al., 2013); and (4) CIFAR10, image classification dataset (Krizhevsky, 2009).
We compared Vanilla Transformer, Linear Transformer with all kernels observed in section 2.3 with
SIKF, combined with different positional encodings, namely: sineSPE, convSPE, FastRPB. We also
report results of experiments w/o adding any relative positional information. All models also used
trainable Absolute Positional Encodings.
All experiments and hyperparameters were conducted following instructions for the LRA dataset.
We also used LRA tasks to measure memory during the evaluation and computational footprints
during the training.
MNIST. Due to the fact that in LRA CIFAR10 experiment only single layer transformer is used, we
conducted another image recognition experiment with larger networks. We evaluated all the above
models w/ and w/o FastRPB on classical image classification dataset MNIST (Lecun et al., 1998). In
this experiment, we did not compare FastPRE with other positional encoding methods since, as we
observed in LRA, they require dozens of times more training time in experiments with multi-layer
transformers with large size of hidden states.
For all experiments, we used a model with 8 layers, 8 attention heads, a hidden size equal to 256,
and batch equal to 160. We trained models using AdamW optimizer and made 20 runs of Bayesian
hyperparameter search to find the optimal learning rate and than trained all models for 25 epochs.
Parameters are presented in Appendix A. We linearly decayed the learning rate to 0 during the
training. Final results are averaged over 10 runs with different values of random seed.
11We do not include another synthetic image classification task, Pathfinder, since we were unable to repro-
duce the results, obtained in the original paper (Tay et al., 2020).
8
Under review as a conference paper at ICLR 2022
5	Results
Long Range Arena. See Table 1 for the evaluation results. Linear Transformer with SIKF kernel
comes out in the top two results for every dataset, except CIFAR10, which we will discuss separately.
Memory footprint (see Table 3) of SIKF is indeed very tight to the ReLU, DPFP, and Performer,
while DPFP and Performer constantly performed slower (up to 1.4x times).
Linear Transformer equipped with FastRPB showed significantly higher results on ANN and TC
both in memory, speed, and accuracy, achieving even better results than Original Transformer. More-
over, architectures with FastRPB, confirming the above propositions 3.1 and 3.2, proved to have
memory and computation consumption comparable with the default architectures. For the ListOps
dataset, the best performance was obtained by the model without any relative positional encoding.
We attribute this result to the fact that relative distances can be confusing in parse hierarchical struc-
ture, such as expressions for ListOps or source code (e.g., the distance between if and else in
source code can be pretty large, however, these statements are inwardly connected). We measured
memory and computational footprint of models (see Table 3), according to which FastRPB requires
up to 30x less time to train than convSPE, and up to 3x less then sinSPE. Simultaneously, FastRPB
has 1.5x smaller memory footprint on evaluation then sinSPE and convSPE. Thus we conclude that
FastRPB is the fastest and the most accurate method among others.
The learned weights of FastRPB for different text tasks sequence is presented in Figure 1, where
x-axis represents relative distance m - n from n-th token to m-th token, and color denotes the value
of (W1d)n,m = wm-n. In the AAN task FastRPB forces the model to attend more to the very end of
the text. Such observation can be attributed to the fact that AAN mainly consists of scientific texts,
which final part usually contains a conclusion. As for ListOps, learned FastRPB weights are usually
relatively small, which supports the hypothesis that relative positional encodings in such tasks should
be designed using the text hierarchy information. In the TC task, learned weights mainly draw the
model’s focus forward and backward to enable the model to capture long-range dependencies.
In experiments with CIFAR, usage of FastRPB was shown to decrease the model’s performance, and
the best result was obtained with convSPE, which outperformed others by a significant margin. We
attributed this to the experiment setup, where a single-layer network is utilized. In this regard, we
conduct the experiments with a more extensive network on the MNIST dataset.
MNIST. In this task, each of the above models equipped with FastRPB showed superior perfor-
mance (see Table 2), requiring quite a small amount of additional memory and computational time
(see Figures 3a and 3b respectively). As can be observed from the plots, of Linear Transformer
with FastRPB is up to 10x times less than the original ones. Moreover, in terms of speed Linear
Transformer with FastRPB is 5x time faster in evaluation time compared to original Transformer.
The detailed view of trained FastRPB could be found in Figure 2, where we learned slice
(W2d):,:,12,10, which represents pairwise distances from pixel (12, 10) to every other pixel of28 × 28
MNIST image. We observed that FastRPB enforced the model to look at more distinct pixels rather
than to close ones.
6	Conclusion
We presented two novel approaches aimed to increase the accuracy of the Linear Transformer model
without additional memory footprint and significant loss in speed. Contribution is two-fold: we first
make linear attention shift-invariance and then add a bias term to attention scores, representing
pairwise distances between tokens of the sequence. We computed this bias term efficiently and
achieved O(N log N) complexity and O(N) memory w.r.t. sequence length.
We demonstrate the superiority of our approach among others using four long sequence tasks from
the Long Range Arena benchmark and on the MNIST dataset. Our model performs significantly
better than previous approaches, obtaining the best accuracy on several tasks while being almost as
efficient in terms of speed and memory as plain Linear Transformer.
We believe that the principles presented in this work can serve as a basis for future research on the
role of positional information encoding in transformer architectures. To this end, we open-source all
the code and trained models.
9
Under review as a conference paper at ICLR 2022
References
Bassam Bamieh. Discovering transforms: A tutorial on circulant matrices, circular convolution, and
the discrete fourier transform, 2020.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150, 2020.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Robert Gray. Toeplitz and circulant matrices: A review. Foundations and Trends® in Communica-
tions and Information Theory, 2, 10 2001. doi: 10.1561/0100000006.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.
Angelos Katharopoulos, APoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International Conference on Ma-
chine Learning, pp. 5156-5165. PMLR, 2020.
Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. arXiv
preprint arXiv:2006.15595, 2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers, 2021.
Antoine Liutkus, Ondrej CIfka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan Yang, and Gael Richard.
Relative positional encoding for transformers with linear complexity. In International Conference
on Machine Learning, pp. 7067-7079. PMLR, 2021.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv
preprint arXiv:1804.06028, 2018.
Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl
anthology network corpus. Language Resources and Evaluation, 47(4):919-944, 2013.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
Andreas Rosowski. On fast computation of a circulant matrix-vector product. arXiv preprint
arXiv:2103.02605, 2021.
Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight
memory systems. arXiv preprint arXiv:2102.11174, 2021.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-
tions. arXiv preprint arXiv:1803.02155, 2018.
10
Under review as a conference paper at ICLR 2022
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. arXiv preprint arXiv:2011.04006, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems,pp. 5998-6008, 2017.
Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen.
Encoding word order in complex embeddings, 2020.
A MNIST hyperparamterts search
Model	w/o FaStRPB	w/ FastRPB
Original	1.05 ∙ 10-4	1.35 ∙ 10-4
Linear, DPFP	1.3 ∙ 10-4	0.7 ∙ 10-4
Linear, SIKF	1.25 ∙ 10-4	1.45 ∙ 10-4
Linear, ELU + 1	1.3 ∙ 10-4	1.25 ∙ 10-4
Performer	1.2 ∙ 10-4	1.0 ∙ 10-4
Table 4: Best Learning Rate values obtained from 20 runs of Bayesian hyperparameters search
B Proposition proofs
B.1 Circulant matrices
To design a more efficient positional encoding method, we leveraged circulant matrices - a subclass
of matrices with some special properties due to their relation to the Fast Fourier Transform (FFT)
and circular convolution Bamieh (2020). Here we will only focus on the property that allows per-
forming matrix-vector product fast and efficient in terms of speed and memory. Given an vector
c = (c0, c1, ..., cn-1) we will define the associated n × n circulant matrix C = circ(c) which first
column is exactly c, and each subsequent column is obtained by a circular shift of the previous
column:
c0
ci
C2
C
Cn-1	Cn-2	…。1、
C0	Cn-1	C2
Ci C0	C3
(13)
∖Cn-1	Cn-I2	Cn-3	…	)
It can be shown that for every vector x of size n matrix-vector product Cx requires only O(n log n)
computation Rosowski (2021). Moreover, to compute the above product, it is not necessary to
store the whole matrix C in memory; it is enough to only keep in memory O(n) parameters of
vector c. Further, we will prove that FastRPB and FastRPB 2D can be expressed through circular
matrices, and hence relative positional information can be embedded efficiently in the self-attention
mechanism.
B.2 Proposition 1
In 3.2.1 we introduced a Toeplitz matrix Wid of shape N × N:
	w0	wi	W2	∙	•	WN-i∖	
	w-i	w0	wi	∙	•	WN-2	
Wid =	w-2 . .	w-i	w0 ..	WN-3 . ..	(14)
	. w-N +i	w-N+2	. W-N+3	∙	.. • •	W0	
11
Under review as a conference paper at ICLR 2022
N
A
A
iw,thgiew dengissA
Relative distance between tokens, i
-0.06
0.04
0.02
0.00
-0.02
(a)	Learned in AAN task weights wi assigned to pairwise distances between tokens i in
FastRPB 1D.
w,thgiew dengiss
ListOps
-0.00
-0.02
-0.04
Relative distance between tokens, i
(b)	Learned in ListOps task weights wi assigned to pairwise distances between tokens i
in FastRPB 1D.
TC
iw,thgiew dengissA
0.01
0.00
-0.01
Relative distance between tokens, i
(c) Learned in TC task weights wi assigned to pairwise distances between
FastRPB 1D.
tokens i in
12
Under review as a conference paper at ICLR 2022
Our goal is to efficiently multiply W1d by arbitrary matrix V of shape N × D. As was stated in
section Circulant matrices, a special class of matrices, namely circulant matrices, can be multiplied
by a vector efficiently in O(N log N) operations and requires O(N) memory. We will extend ma-
trix W1d with additional rows and columns and thus obtain circulant matrix W1edxt. Then we will
introduce V ext, a modified version of matrix V , which W1edxt will be multiplied by. Finally, we will
select a slice from the product W；dt ∙ Vext, which will be exactly Wid ∙ V.
The first step is to define W1edxt:
	W-N+i	W-N +2	∙ ∙ ∙	W0	Wi	W2	•••	WN-i	
	WN-i	W-N+i ∙ ∙ ∙	W-i	W0	Wi	•••	WN-2	
	WN-2	wN-i	…	W-2	W-i	W0		WN-3	
WiXt =	. . . Wi .	. . . W2	… .	W-N+i	W-N+2 .	. . . ••• .	. . . W-i .	. . . W0 .	(15)
	. . W-N+2	. . W-N+3	…	• ∙ ∙	. . •••	. . •••	. . WN-i	. . W-N+i	
As can be seen, constructed matrix Wiedxt is indeed circulant. Moreover, the right upper corner
of Wiedxt is essentially Wid. Hence, Wid can be expressed as a slice of Wiedxt following numpy
notation: WId= (WIdt) N: , N+1∙
NoW We want to calculate matrix product Wid ∙ V using matrix WIXt of size (2N - 1) X (2N - 1).
Due to this fact we will need to multiply Wiedxt with appropriate matrix Vext of size 2N - 1 × D .
Since, as we seen, Wid is a slice of Wiedxt, we only will need first N rows of the resulting product
Wed ∙ Vext. More formally we need to find such matrix Vext that: Wid ∙ V = (Wedt ∙ Vext) N+]..
To achieve with we will basically pad V with N - 1 additional rows filled with zeros as follows:
	0 0 .	0 0 .	•••	0 •••	0 ..	
Vext =	. . 0	. . 0	.. .. •••	0	(16)
	v0,0	v0,i	• • •	v0,D-i	
	vi,0 .	vi,i .	• • •	vi,D-i ..	
	. . vN-i,0	. . vN-i,i	.. .. • • • vN -i,D-i	
To complete the proof lets explicitly show that Wid ∙ V = (Wedt ∙ Vext).N , 1 .. Lets assume
:N+i , :
D = 1, generalization for bigger dimensions can be done using similar operations:
w-N +i
WN-1
(Wixt∙ V ext)∙N+1 : =	WN-2
..
.
Wi
0
0
W0	Wi	•	• • WN-i		.	
W-i	W0	•	• • WN-2		. . 0 v0	
W-2	W-i .	WN-3 .			=(17)
		..		vi	
W-N+i	W-N+2	•	・	W0 /		. .	
vN-i
W0	Wi
W-i	W0
w-2	W-1
:	..
..
W-N+i W-N+2
WN-i
WN-2
WN-3
.
.
.
W0
v0
vi
vN-i
Wid ∙ V
(18)
The last thing we have to do is to calculate the complexity of matrix product of circulant matrix
Wiedxt with Vext. Since matrix Wiedxt is circulant of size (2N - 1) × (2N - 1), according to the
13
Under review as a conference paper at ICLR 2022
section Circulant matrices, it requires O(N) memory to it, and O(N log N) operations to perform
a matrix-vector product with vector if size 2N - 1. In this regard to compute matrix product with
matrix V of size N × D, we will need to perform D times operations more, i.e. O(DN log N)
operations.
B.3 Proposition 2
In the following sections, we will be considering an example of 3 × 3 image. In section Structure
of pairwise distances tensors we will study the general structure of tensors X and Y, which were
introduced in section . In section Flattening of the tensors we will reshape these tensors and obtain a
new pair of tensors Xflat and Yflat, which will be then efficiently multiplied by V matrix in the final
section Efficient matrix product.
B.3.1	S tructure of pairwise distances tensors
First of all, to gain a deeper understanding of structure of tensors X and Y, we will explicitly write
down components of this tensors for an image of size 3 × 3. Consider X1,1,:,: and Y1,1,:,: which
contains weights assigned to vertical and horizontal relative distances from pixel (1, 1) to all other
pixels:	w-1 w-1 w-1	w-1 w0	w1
X1,1,:,: =	w0	w0	w0	, Y1,1,:,:	= w-1	w0	w1	(19) w1	w1	w1	w-1	w0	w1
One can observe, that Xn,m,:,: and Yn,m,:,: have the property of symmetry, through which it can be
proven that:
Proposition B.1 Xn,i,:,: = Xn,j,:,: and Yi,n,:,: = Yj,n,:,: for every n, i, j ∈ {0, ..., N - 1}, and
this property holds for image of every size.
Taking advantage of the proposition B.1 we can write out explicit form of tensors X and Y in case
of 3 × 3 images. We will introduce the following notation:
(20)
w-2	w-2	w-2	w-1	w-1	w-1	w0
w-1	w-1 w-1	, B = w0 w0 w0	, C =	w1
w0	w0 w0	w1	w1	w1	w2
w0	w0
w1	w1
w2	w2
It can be seen that different slices of tensor X can be expressed using matrices A, B , C :
X0,0,:,: = X0,1,:,: = X0,2,:,:	=C	(21)
X1,0,:,: = X1,1,:,: = X1,2,:,:	=B	(22)
X2,0,:,: = X2,1,:,: = X2,2,:,:	=A	(23)
Moreover, are matrices A, B , C also applicable for tensor Y:		
Y0,0,:,: = Y0,1,:,: = Y0,2,:,: =	CT	(24)
Y1,0,:,: = Y1,1,:,: = Y1,2,:,: =	BT	(25)
Y2,0,:,: = Y2,1,:,: = Y2,2,:,: =	AT	(26)
B.3.2 Flattening of the tensors
In transformer architecture before processing the image of size N × N , it is usually flattened into
1-dimensional vector of size N2 . To define flatten operation, consider an arbitrary matrix M of
shape 3 × 3, than its flattened version mflat will have the following structure:
m0,0	m0,1
M = m1,0 m1,1
m2,0	m2,1
m0,2
m1,2
m2,2
flattening
--------→ mflat
m0,0
mo,ι
m0,2
m1,0
m1,1
m1,2
m2,0
m2,1
m2,2
(27)
14
Under review as a conference paper at ICLR 2022
Due to the flatten of images in transformer, matrix V in will have shape N2 × D and hence it is
essential to reshape tensors X and Y from size (N × N) × (N × N) to size N2 × N2 . We will
denoted reshaped versions of tensors X and Y as Xflat and Yflat respectively. Reshaping of the above
tensors can be decomposed into two consecutive flatten operations firstly applied to last two dims
of tensors X and Y and then to the first two ones. Flattening of the last dimensions is equivalent to
flattening of each of the matrix A, B , C , after this operation in case of 3 × 3 images we will obtain
the following three vectors:
	w0	w-i	W-2
	W0	w-i	W-2
	W0	W-1	W-2
	wi	W0	w-1
aflat =	w1	, bflat =	w0	, Cflat =	w-1	(28)
	Wi	W0	w-i
	W2	Wi	W0
	W2	wi	W0
	w2	Wi	W0
Than after next flattening operation we will get:
		W0	w0	w0	wi	wi	wi	w2	w2	w2	
		W0	w0	w0	wi	wi	wi	w2	w2	w2	
		W0	w0	w0	wi	wi	wi	w2	w2	w2	
		w-i	w-i	w-i	w0	w0	w0	wi	wi	wi	
Xflat	X.reshape(N 2 , N2) =	w-i	w-i	w-i	w0	w0	w0	wi	wi	wi	(29)
		w-i	w-i	w-i	w0	w0	w0	wi	wi	wi	
		w-2	w-2	w-2	w-i	w-i	w-	i	w0	w0	w0	
		w-2	w-2	w-2	w-i	w-i	w-	i	w0	w0	w0	
		w-2	w-2	w-2	w-i	w-i	w-	i	w0	w0	w0	
		(wo	wi	w2	w0	wi	w2	w0	wi	w2∖	
		w-i	w0	wi	w-i	w0	wi	w-i	w0	wi	
		w-2	w-i	w0	w-2	w-i	w0	w-2	w-i	wo	
		w0	wi	w2	w0	wi	w2	w0	wi	w2	
Yflat =	二 Y.reshape(N2, N2)=	w-i	w0	wi	w-i	w0	wi	w-i	w0	wi	(30)
		w-2	w-i	w0	w-2	w-i	w0	w-2	w-i	wo	
		w0	wi	w2	w0	wi	w2	w0	wi	w2	
		w-i	w0	wi	w-i	w0	wi	w-i	w0	wi	
		w-2	w-i	w0	w-2	w-i	w0	w-2	w-i	wo	
Note that rows of the Xflat and Yflat are aflat , bflat and cflat.
Now we have flattened representation Xflat and Yflat and the last step we have to take is to compute
matrix product Xflat ∙ V and Yflat ∙ V.
B.3.3 Efficient matrix product
Calculation of flt ∙ V
In case of 3 × 3 images it is easy to see that the matrix Yflat, presented in formula 30 consists of 9
identical blocks, we will denote this blocks as R:
Yfl
RRR
RRR
RRR
where R
w0
w-1
w-2
w1	w2
w0	w1
w-1	w0
(31)
=
Note that matrix R is just a block of Y , not its element. Also a very important fact is that can
matrix R is a Toeplitz, which means that it can be efficiently multiplied by a vector, according
to Proposition 1. Our goal is to efficiently multiply Yflat with a matrix V of shape N2 × D. Since V
is basically D times stacked vectors of size N2 it will be enough to consider matrix-vector product
15
Under review as a conference paper at ICLR 2022
of Yflat with a vector v of shape N2 . The product can be expressed as:
at
Yfl
1234
vvvvv
—
!
RRR
RRR
RRR
V6
V7
v8
R
+
!
345
vvv
∕z(∖
♦
R
+
!
012
vvv
∕z(∖
/
(32)
∖
/	(V0 + V3 + V6、∖
R ∙ V1 + V4 + V7
∖V2 + V5 + V8∕
(33)
..
Lets introduce an additional notation, we will denote as Vm a tensor of size N × N × D, which was
obtained as a reshape of matrix V of size N2 × D. Vm will have the following structure:
v1 v2
v4 v5	= V .reshape(N, N, D)
v7 v8
(34)
Then:
v0 + v3 + v6
VmT.sum(1) = v1 + v4 + v7	(35)
v2 + v5 + v8
Finally using notation 34 and result of product 33, we can conclude that:
'R ∙ (VT.sum(1))∖
Yflat ∙ V =	.	(36)
.
.
..
Note that the summation by dim = 1, not by dim = -1, since in general case, when D is not 1, the
latter dimension refers to hidden size.
R is a Topleitz matrix we worked with in the previous paragraph B.2, that is, the product of matrix
R withVmT.sum(1) can be efficiently computed using the properties of Toeplitz matrix. Finally
We can conclude that for N X N image computation of product flt ∙ V require O(DNlog N)
computations and O(N) memory to store a vector, generating matrix R.
Calculation of Xflat ∙ V
Here We Will apply the folloWing trick: by rearranging the columns of matrix Xflat We Will construct
matrix Xfl0at, and accordingly construct matrix U by rearranging the roWs of matrix V in such aWay
that the equality holds Xflat ∙ V = Xflat ∙ U. We still will work with the D = 1 case since, as was
noted previously, it can be easily generalized for higher dimensions.
	w0	w0	w0	w1	w1	w1	w2	w2	w2		v0
	w0	w0	w0	w1	w1	w1	w2	w2	w2		v1
	w0	w0	w0	w1	w1	w1	w2	w2	w2		v2
	w-1	w-1	w-1	w0	w0	w0	w1	w1	w1		v3
Xflat ∙ V =	w-1	w-1	w-1	w0	w0	w0	w1	w1	w1	•	v4
	w-1	w-1	w-1	w0	w0	w0	w1	w1	w1		v5
	w-2	w-2	w-2	w-1	w-1	w-1	w0	w0	w0		v6
	w-2	w-2	w-2	w-1	w-1	w-1	w0	w0	w0		v7
	w-2	w-2	w-2	w-1	w-1	w-1	w0	w0	w0		v8
(37)
16
Under review as a conference paper at ICLR 2022
(W0	W1	W2	W0	W1	W2	W0	W1	W2		(V0、	α∖
W0	W1	W2	W0	W1	W2	W0	W1	W2		V3	I a I
W0	Wi	W2	W0	W1	W2	W0	W1	W2		v6	Ia∣
w-1	W0	W1	W-1	W0	W1	W-1	W0	W1		V1	∣β∣
w-1	W0	W1	W-1	W0	W1	W-1	W0	W1	∙	V4	=Xflat ∙ U = I β I (38) IIβ
w-1	W0	W1	W-1	W0	W1	W-1	W0	W1		V	
W-2	W-1	W0	W-2	W-1	W0	W-2	W-1	W0		V2	I 7 I
W-2	W-1	W0	W-2	W-1	W0	W-2	W-1	W0		V5	I 7 J
∖W-2	W-1	W0	W-2	W-1	W0	W-2	W-1	W0		∖v8∕	∖7∕
W1	W2∖	仅 0	+ V1	+	V2
W0	W1	V3	+ V4	+	V5
W-1	W0∕	Vβ	+ V7	+	V8
Now lets use already introduced matrix Vm∖
(39)
And finally:
W W0
R ∙ (Vm .sum(1)) = w-ι
∖W-2
∕α∖
α
α
β
β
β
Y
Y
∖7∕
(40)
α ɑ∖
β β I .reshape(N2)=
Y 7/
(41)
Xfl at ∙ U
(R ∙ (Vm.sum(1)) .........) .reshape(N2)
(42)
B.3.4 Conclusion
As we proved in the previous section, products Xflat ∙ V and Yflat ∙ V can be computed using just
two products: R ∙ (vm,.sum⑴)and R ∙ (VT.sum⑴)respectively. Moreover, matrix R utilized
in the above products is a Toeplitz, and in this regard this products can be computed efficiently ac-
cording to Proposition 1. Summarizing it all we get that FastRBP 2D will require be O(DN log N)
computations and O(N) memory.
17