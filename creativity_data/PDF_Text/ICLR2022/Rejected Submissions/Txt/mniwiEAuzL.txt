Under review as a conference paper at ICLR 2022
Sample-efficient actor-critic algorithms
with an etiquette for zero-sum Markov games
Anonymous authors
Paper under double-blind review
Ab stract
We introduce algorithms based on natural actor-critic and analyze their sample
complexity for solving two player zero-sum Markov games in the tabular case.
Our results improve the best-known sample complexities of policy gradient/actor-
critic methods for convergence to Nash equilibrium in the multi-agent setting.
We use the error propagation scheme in approximate dynamic programming, re-
cent advances for global convergence of policy gradient methods, temporal differ-
ence learning, and techniques from stochastic primal-dual optimization. Our algo-
rithms feature two stages, requiring agents to agree on an etiquette before starting
their interactions, which is feasible for instance in self-play. However, the agents
only access to joint reward and joint next state and not to each other’s actions or
policies. Our complexity results match the best-known results for global conver-
gence of policy gradient algorithms for single agent RL. We provide numerical
verification of our methods for a two player bandit environment and a two player
game, Alesia. We observe improved empirical performance as compared to the
recently proposed optimistic gradient descent-ascent variant for Markov games.
1	Introduction
We study two-player zero-sum Markov game framework which is a key model with broad applica-
tions in competitive reinforcement learning (RL), robust RL, and many others Zhang et al. (2019a;
2021). This framework is introduced by Shapley (1953) as stochastic games and popularized in RL
with Littman (1994). In its basic form, two agents with competing interests interact in an environ-
ment where the reward and the state transition depend on the actions of both players. Even with this
simplicity, such systems achieved impressive success in game-playing and robotics (Kober et al.,
2013; Silver et al., 2017; Mnih et al., 2015; Vinyals et al., 2019; Brown & Sandholm, 2019).
While value-based methods offer near-optimal guarantees (Sidford et al., 2020; Bai et al., 2020; Bai
& Jin, 2020; Xie et al., 2020; Tian et al., 2020), policy gradient (PG) methods, including actor-critic
(AC) and their natural counterparts natural PG (NPG) (Kakade, 2001) and natural AC (NAC) (Pe-
ters & Schaal, 2008), only have limited guarantees, despite their model-free and easy-to-implement
structure, flexibility and generality (Schulman et al., 2015; 2017; Wang et al., 2016).
The PG methods (Kakade, 2001; Sutton et al., 2000) directly optimize the value function in the
policy space— a non-convex optimization problem even in the basic single agent, tabular setting.
Intriguingly, recent results demonstrate globally optimal convergence of PG methods by identifying
a hidden convexity structure for single agent RL (Agarwal et al., 2020; Cen et al., 2020; Mei et al.,
2020; Bhandari & Russo, 2019; 2021; Xu et al., 2020b; Lan, 2021; Khodadadian et al., 2021b; Hong
et al., 2020; Xu et al., 2020a; Khodadadian et al., 2021b), and multi-agent RL (MARL) (Daskalakis
et al., 2020; Wei et al., 2021; Zhao et al., 2021).
The existing results on PG methods for tabular two-player zero-sum Markov games mostly focus
on decentralized algorithms with sample complexities O(E-12.5) (Daskalakis et al., 2020), O(E-8),
and even O(e-4), yet with some limitations (Wei et al., 2021); see Section 1.1 for the details. With
function approximation, Zhao et al. (2021) obtains O(e-6) sample complexity when given access to
unbiased sampling oracles of the value functions.
1
Under review as a conference paper at ICLR 2022
On the other hand, the best-known sample complexity for global optimality for single agent problem
is O(-2) in the tabular case (Lan, 2021). As this complexity is achieved by value-based/model-
based methods in the multi-agent setting (Sidford et al., 2020; Zhang et al., 2020), one expects a
similar complexity for policy-based methods. Our work precisely bridges this gap and develops
policy gradient methods whose performance for MARL is closer to their single agent counterparts.
Contributions. We propose algorithms based on natural actor-critic (NAC) framework for solving
two-player zero-sum Markov games in the tabular case. Our sample complexity results match the
best-known ones for global optimality in the single agent setting (Lan, 2021; Khodadadian et al.,
2021b; Hong et al., 2020; Xu et al., 2020b). In particular, we show that by using inner loops for
policy evaluation and a carefully designed algorithm, the sample complexity to get an -approximate
Nash equilibrium, is O(E-2), by assuming a uniform lower bound on the policies. Without this
assumption, We show (O(e-4) complexity.* 1
Surprisingly, we achieve these results—to our knowledge, for the first time with policy gradient
methods—mostly by a careful adaptation of the recent results for policy gradient methods in single
agent setting, temporal difference learning, two-stage error propagation framework of policy itera-
tion (Perolat et al., 2015), and by employing techniques from stochastic primal-dual optimization.
These developments require a careful algorithm design and analysis. In particular, two-stage nature
of the algorithm incurs biases between the stages that we have to control carefully. Obtaining O(E-2 )
complexity requires a tighter analysis for both stages of the algorithm, with strict control on the
aforementioned bias. Therefore, it requires more advanced techniques and algorithms, inspired
from the stochastic primal-dual optimization literature. We explicitly highlight our important new
techniques as insights in the sequel. The full proofs are included in the appendices.
1.1	Related works
Policy gradient methods. There is growing interest in global convergence of PG methods in the
single agent setting. Several works showed convergence rates of natural policy gradient (NPG) in
the tabular setting by assuming access to exact value function oracle (Agarwal et al., 2020; Cen
et al., 2020; Mei et al., 2020; Bhandari & Russo, 2019; 2021) or when value functions are estimated
from the data (Shani et al., 2020; Xu et al., 2020b; Lan, 2021; Khodadadian et al., 2021b; Hong
et al., 2020; Xu et al., 2020a; Khodadadian et al., 2021b). To our knowledge, the best sample
complexity for NPG methods with inner loop for policy evaluation (which we refer to as NAC)
is O (E-2) and is due to (Lan, 2021). For single loop NAC with online policy evaluation, the best
sample complexity is O(E-4) as obtained in (Khodadadian et al., 2021b; Hong et al., 2020; Xu et al.,
2020b) (see (Khodadadian et al., 2021a, Table 1)). For a general overview of results in MARL we
refer to Zhang et al. (2019a).
Policy gradient methods for two-player zero-sum Markov games. With the positive results on
global convergence of PG methods, translating these results to the competitive MARL has been the
goal of many recent works. In particular, independent policy gradient methods with the agents inter-
acting symmetrically has been considered in Daskalakis et al. (2020); Wei et al. (2021). The work
of Daskalakis et al. (2020) built on Agarwal et al. (2020) by using REINFORCE estimator (Williams,
1992) and obtained sample complexity of O(ET2.5) for reaching to one-sided Nash equilibrium.
The algorithm of Wei et al. (2021) built on optimistic gradient descent-ascent (OGDA) method
combined with a running estimate of the value function, obtaining O(e-8) sample complexity for
finding a policy pair with small duality gap. In addition, Wei et al. (2021) showed improved com-
plexity O(e-4) when restricted to Euclidean projections onto the simplex with metric subregularity
assumption. There are two subtleties about this result: First, as pointed out in Daskalakis et al.
(2020), metric subregularity constant can be arbitrarily small, resulting in degradation of the rate.
Second, as also pointed out by Wei et al. (2021), this result is limited to Euclidean setting and cannot
be extended to the NPG with softmax policy update, which requires projection with KL divergence.
The algorithm can be seen similar to the gradient ascent algorithm in Agarwal et al. (2020). As
14
1In Appendix E, we design an algorithm based on single loop NAC with the complexity of O(-4) (and
O(-7) without assuming lower bounded policies). Our results on this algorithm is, to our knowledge, the first
finite-sample analysis of single loop NAC-based methods for two-player zero-sum Markov games.
2
Under review as a conference paper at ICLR 2022
	Assumption	Complexity
Daskalakis et al. (2020)	Zs,a,b ≥ Z> 0.* ,	O-
Wei et al. (2021)	maxx,y,s,s0 τs→s,=	=-where μ > 0J	O(e-8), O(e-4)^ μ
Section 3.1	Assumption 1	O(e-4)
Section 3.1	Assumption 1, 2	O(e-2)
Table 1: *In a game With finite steps, ζs,a,b is the probability that the game will end at state s, after taking
actions a, b (Daskalakis et al., 2020, Section 2). *TX→s is the time that it takes to go from state S to state
s0 by using policy pair x, y (Wei et al., 2021, Assumption 1). JhiS O(c-4) complexity by Wei et al. (2021)
requires using Euclidean projections onto the simplex instead of softmax updates and depends on the metric
subregularity constant. Hence it is not applicable to NPG.
shown in Agarwal et al. (2020) for single agent problems, NPG methods have much better conver-
gence properties than Euclidean projected gradient ascent methods. For comparison with the works
in Daskalakis et al. (2020); Wei et al. (2021), we also refer to Remark 2.1 and Table 1.
Another very related work to ours is by Zhao et al. (2021) which considered (i) tabular setting
with exact value functions and (ii) online setting with function approximation, also using the error
propagation scheme of Perolat et al. (2015). Building on Agarwal et al. (2020), this work showed
O(-6) sample complexity with function approximation, with access to unbiased samples of the
value functions. In contrast, we focus on the tabular setting and we do not assume access to unbiased
value function oracles. Indeed, lack of unbiased samples for value functions required us to use new
insights described in the sequel, to derive the tighter complexities O(e-2) and O(e-4).
Policy gradient methods for linear quadratic regulator (LQR). For zero-sum LQR, Zhang et al.
(2019b); Bu et al. (2019) showed global convergence ofPG with exact value function oracles. These
methods have a nested structure where one player computes best-response and the other does policy
gradient updates. Recently, Zhang et al. (2021) built on Zhang et al. (2019b) to derive sample
complexities when value functions are estimated from data.
2	Preliminaries
Notation. We consider the tabular setting with finite state and action spaces denoted by S, A, B and
the discount factor γ < 1. The policy of the min agent is x and the max agent is y, with action sets
A, B, respectively. At state s, both agents take actions independent of each other: a 〜χ(∙∣s) and
b 〜y(∙∣s). Based on the actions, the environment transitions to the next state s0 〜 P(∙∣s, a, b) and
the agents receive reward |r(s, a, b)| ≤ 1. Given a policy pair x, y, we denote the induced steady-
state distribution as ρx,y . Let U denote the uniform distribution for states that we also take as the
initial state distribution for simplicity. We denote the probability simplex as ∆. Given a policy x,
we sometimes use the notation Xs for x(∙∣s) in the proofs. We use e(st) ∈ R|S| to denote the vector
such that e(s) = 1 if s = st and e(s) = 0, if s 6= st. We use the same notation for e(st, at). The
value function for state s is defined as
∞
V x,y (s) = Ex,y X γtr(st, at, bt)|s0 = s ,
t=0
where Eχ,y is over random variables st,at,b for all t ≥ 0 as at 〜 x(∙∣st), bt 〜 y(∙∣st)
and st+ι 〜 P(∙∣st,at,bt). Similarly, the action value function is defined as Qxy(s,a,b) =
Ex,y [Pt∞=0 γtr(st, at, bt)|s0 = s, a0 = a, b0 = b]. With these definitions, we can state the formal
problem. For all s ∈ S, we aim to solve
min max V x,y (s).
x(∙∣s)∈∆ y(∙∣s)∈∆
We denote the information needed in algorithms as oracles. We provide the background on NPG,
NAC, TD(0) in Appendix A.
Nash equilibrium. We assume the existence of a pair of policies x? , y? that are Nash equilibrium,
namely, for all s, V x?,y (s) ≤ V ?(s) := V x?,y? (s) ≤ V x,y? (s). We are interested in finding a one-
sided Nash equilibrium, similar to Daskalakis et al. (2020); Zhao et al. (2021); Zhang et al. (2019b);
3
Under review as a conference paper at ICLR 2022
Bu et al. (2019). As mentioned in Daskalakis et al. (2020), for the other player, one can re-run the
algorithm by switching roles to have the guarantee for both players. In particular, for the initial state
distribution U , we seek for xout such that
Eso~u[max V(so) - V?(so)] ≤ e.
y
It is easy to prove that this quantity on the LHS is 0 if and only if xout is a Nash equilibrium.
Interaction procedure. We use the interactions of the agents with the environment to estimate the
value functions and related oracles for the running of the algorithm. At each interaction, agents have
access to (si, ai, r(si, ai, bi), si+1) and (si, bi, r(si, ai, bi), si+1), respectively. In terms of access
of agents, our oracle model is similar to Daskalakis et al. (2020); Wei et al. (2021). However, one
difference is that we require a game etiquette: Our algorithms have two stages where the agents have
to behave differently. As long as this etiquette is respected by the agents (for example embedded to
players in the beginning of the game), they do not need further communication.
Softmax update rule/NAC. Given Kullback-Leibler divergence KL and action-value function Qxt,
xt+1 (∙∣s) = PKL(Xt(∙∣s), Qxt (s, ∙)) := arg 印爪(Qxt (s, ∙),x(∙∣s)i + KL(x(∙∣s), xt(∙∣s)), (1)
x(∙∣s)∈∆
is known as NPG with softmax parameterization (Agarwal et al., 2020, Lemma 5.1). When there
is a critic estimating Qxt , along with actor updating xt with NPG, this algorithmic framework is
called natural actor-critic, in short, NAC. We focus on KL divergence for simplicity and its wide
use. Our developments also hold for more general Bregman divergences as Zhan et al. (2021).
Assumption 1. There exists P such that, for any policy iterate pair xt, yt, for any state S, it holds
that ρxt,yt (S) ≥ ρ > 0, where ρxt,yt is the stationary state distribution induced by the policy pair
Assumption2. There exist x, y such that, for any policy iterate pair xt, yt, for any state action tuple
s, a, b, it holds that Xt(a∣s) ≥ x > 0, yt(b∣s) ≥ y > 0.
Our rationale on the assumptions. Assumption 1 and 2 essentially mean positive definiteness of
the sampling matrices in policy evaluation (see eqs. (30), (34) and (42)). To our knowledge, some
form of this assumption is required in most of the existing work on temporal difference (TD) (in-
cluding TD(0)) methods for policy evaluation (Bhandari et al., 2018; Xu et al., 2020b; Khodadadian
et al., 2021a; Lan, 2021; Hong et al., 2020; Xu et al., 2020a; Wu et al., 2020; Zou et al., 2019) (see
App. A). The complexity O(e-2) requires Assumption 2 even for single agent problems (see (Lan,
2021, Rem. 1, Sec. 5.2)).
Remark 2.1. As summarized in Table 1, similar assumptions to Assumption 1 are used in Daskalakis
et al. (2020); Wei et al. (2021). In particular, each of these assumptions ensure that all action-state
pairs are observed with nonzero probability throughout the game. Moreover, by additionally requir-
ing Assumption 2, we can obtain the COmPlexity O(e-2), matching the single-agent counterpart.
Markovian bias. For simplicity, we assume that we sample from the steady state distribution of a
given policy pair. During normal interaction with the environment, this is not the case and we obtain
a single stream of data. Hence, TD(0) update is biased—commonly referred to as the Markovian
bias. A large body of literature in the single agent literature showed that the effect of this bias in
TD(0) update is essentially additive and can be handled by assuming uniform mixing of the induced
Markov chain (Wu et al., 2020; Bhandari et al., 2018; Zou et al., 2019; Khodadadian et al., 2021b;
Xu et al., 2020b;a). These analyses apply to our policy evaluation routines, extending them to the
Markovian setting. For simplicity, we show our techniques with i.i.d. assumption and then illustrate
how the extension with Markovian data follows with the uniform mixing assumption in Appendix D.
Error propagation for approximate dynamic programming. Perolat et al. (2015) proposed error
propagation analysis for approximate version of generalized policy iteration for zero-sum Markov
games (see Appendix C). The authors showed that the following two-stage algorithm converges:
• Stage 1: Given a fixed value function Vk-1, find the policy pair which is an e-equilibrium.
min max	x(a|s)y(a|s)Qk-1(s, a, b) =: xsQsk-1ys,	(2)
xs∈∆ ys∈∆	-
a,b
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Reflected NAC with a game etiquette and ζ-greedy exploration
Require: PKL defined in (1) in Sec. 2. Exploration parameter ζ ≥ 0 (with equality if Assump-
tion 2 holds). Subroutines Policy-Eval-V , Policy-Eval-θ, Policy-Eval-ν (see
Alg. 2, 3, 4 and note βnν, βnθ, βnω are potential step sizes for this routine). Initial policies
xo, yo, yo. Xk, yk denote outer, xt, yt denote inner loop's iterate.
1:
2:
3:
4:
5:
for k = 0, 1, . . . do
Stage 1 // Approximately solve a matrix game
for t = 0, 1, . . . , T - 1 do
∖Vk-ι, VVy-ι] = [Policy-Eval-V (xk-1, yk-1, N, βω), POlicy-Eval-V (xk-1, yk-1, N,以)]
.O
%,
/ / ɪʌ ,1 1	, ,1	, ∙ , ∙	1' T Γ	F , 1	T^T 7∙	F T^Tlt
// Both players compute their own estimations of Vk-1, denoted as V x and Vy
Gy+ι] = [policy-Eval-θ(Xt,yt,N, VV-ι,βnn), POlicy-Eval-θ(Xt,yt,N, V-)]
6:	xt+1(IS) = PKL Gt(IS),η (2θx+ι(S, ∙) - θχ(S, ∙)))
7:	yt+ι(∙∣s) = PKL (yt(∙∣s), -η (2θy+1(s, ∙) - θy(S, ∙)j)
8： Output Xk = T PT=I Xt.
9:	Stage 2 // Approximately find best response
10:	for t = 0, 1, . . . , T - 1 do
11：	^t+ι = Policy-Eval-V (Xk,yjt,N,β = βn)
12：	yt+ι(∙l8) = P KL(yt(∙lS), -nVt+i(S, ∙))
13:	Output yk = y^, where t ∈ [T] is selected uniformly at random.
Algorithm 2 VN = Policy-Eval-V (x, y, N, β)
Require: Policy pair X, y, iteration counter N, step size β, initial value function estimate V0 .
X(IS) = (I - Z)X(IS) +1⅛, y(1S) = (I - z)y(1S) + 备.
1:	for n = 0, 1, . . . , N - 1 do
2:	Sample Sn	〜px,y(∙),	an	〜X(∙∣Sn), bn	〜y(∙∣Sn),	Sn+1	〜P (∙∣Sn,	a，n, bn).
3:	Vn+1 = Vn - βne(Sn) (Vn(Sn) - r(Sn, an, bn) - YVn(Sn+1))
where Qk-1(S, a, b) = r(S, a, b) + γ s0 P(S0lS, a, b)Vk-1(S0). When it is clear from the context,
we drop the subscript ofQk-1. This is a matrix game and is the sample-complexity bottleneck (Per-
olat et al., 2015). Let g denote the accuracy and Xk the output of this stage at iteration k:
E[Es〜U[max(Xk)sQk-ιys - min max XsQk-ιys]] = Ck,	(3)
ys ∈∆	-	xs ∈∆ys ∈∆	-
where the outer expectation is over the randomness of the algorithm used to generate Xk.
• Stage 2: This step finds an approximate best response. The fixed policy (Xk), can be viewed as
a part of the environment. Denote yk as the approximate best-response computed in this stage, at
iteration k. The resulting value function Vk = V xk ,yk is fed to stage 1 in the next iteration. Let Ce
be the accuracy for this stage, y k the output of this stage:
E[Es〜U [max Vxk,y (s) - VXRy (s)]] = e5,	(4)
y
where the outer expectation is over the randomness of the algorithm used to generate yk. Then, Pero-
lat et al. (2015, Theorem 1), Zhao et al. (2021) show that the following holds (see also Appendix C).
E[Es〜u[max VxK，y(s) - V?(s)]] ≤ SKO( SUp ek + sup ef) + θ(⅛⅛K∖	(5)
y∈∆	1 - γ	k≤K 1	k≤K 2	1 - γ
3 Reflected NAC algorithm with a game etiquette
Our approach. We introduce NAC-based algorithms (Konda & Tsitsiklis, 2000; Peters & Schaal,
2008) to solve these two stages in an alternating fashion to obtain an approximate Nash equilib-
rium, in view of (5). We leverage primal-dual algorithms to solve the matrix game in Stage 1
5
Under review as a conference paper at ICLR 2022
A 1	♦八	/"» Λ	-r，	一 r/l，	T^r C、 L	r
Algorithm 3 θN = Policy-Eval-θ(x, y, N, V , β) for player y
Require: X(IS) = (I - Z)X(IS) + |Aai , y3S) = (I - Z)y(1S) + ∣Bb∣ .
1:	for n = 0, 1, . . . , N - 1 do
2:	Sample Sn 〜ρx,y(∙), an 〜X(∙∣Sn), bn 〜y(∙∣Sn), Sn+1 〜P (∙∣Sn, «n, bn ).
3:	θn+1 = θn - βne(Sn, bn) θn (Sn, bn) - r(Sn, an, bn) - γV (Sn+1)
Algorithm 4 VN = Policy-Eval-V (x, y, N, β) for player y
Require: x(1s) = (I - Z)X(IS) + ∣a∣ , y(1S)= (i- Z)y(1S)+ |B.
1:	for n = 0, 1, . . . , N - 1 do
2:	Sample Sn 〜ρx,y(∙), an 〜X(∙∣Sn), bn 〜y(∙∣Sn), Sn+1 〜P (∙∣Sn, an, bn), bn+1 〜y(∙∣Sn+l).
3:	"n+1 = Vn ― β ne (Sn, bn ) (Vn(S n, bn) ― r(Sn, αn, bn ) ― γνn (Sn+1, bn+1 ))
efficiently (Malitsky & Tam, 2020; Nemirovski et al., 2009) and NPG for the single agent problem
in Stage 2. For estimating value functions that are used as oracles in these algorithms, we employ
TD(0) (Tsitsiklis & Van Roy, 1997; Bhandari et al., 2018; Sutton, 1988). To our knowledge, the best
complexities in the single agent setting are obtained with this approach (Lan, 2021).
Stage 1. In this step, at iteration k, we compute an approximate equilibrium of the matrix game (2).
As Vk-1 is fixed, this is a standard matrix game and throughout this loop, we omit the dependence
of Q on k and leave it implicit. Our discussion here is for X player and it would be symmetric
for the y player. Due to its simplicity and generality, we use forward-reflected-backward (FoRB)
algorithm (Malitsky & Tam, 2θ2θ).2 FoRB takes a reflected step rather than using Eb〜ytQ(s, ∙, b)
directly, which is the case in gradient descent-ascent (GDA). The FoRB update is
xt+ι(∙∣S) = PKL(Xt(∙∣S),η (2Eb〜ytQ(S, ∙, b) - Eb〜yt-1 Q(s,∙, b))).
In standard matrix game notation, We can view θχ t = Eb〜yt Q(s, ∙, b) as the matrix-vector multipli-
cation between the dual vector yt and game matrix Q. To get this oracle, we have to solve a linear
equation by sampling the policies Xt, yt. The linear equation is obtained by using the definition of Q,
given after (2). However, the difficulty is that we do not have access to Vk-1. Given that Vk-1 is the
value function of policies Xk-1, yk-1, in Step 4, we use TD(0) to learn this value function and obtain
a biased estimation V∕k-ι (See Algorithm 2). Using this estimation, we can then proceed to solve
the linear equation by sampling the policies xt, yt to find an estimate for θxt = Eb〜ytQ(s, ∙, b), in
Step 5 (see Algorithm 3). This step is similar to stochastic approximation/SGD approaches (Ne-
mirovski et al., 2009; Lan, 2021). Using the oracle, we perform one FoRB step for each player, in
Steps 6 and 7.
As we detail in App. F, for Steps 4, 5 and corresponding policy evaluation routine Alg. 3, X agent
only accesses St, r(St, at, bt), St+1 and its own action at to form the stochastic oracle andy accesses
St, r(St, at, bt), St+1 and its own action bt. We have additional bias coming from the approximation
of Vk-ι by Vk-1, the estimation of which is important for getting our complexity results. We take
special care for the stochastic dependency to make sure to decompose bias and variance of %-ι
estimate (See Insight 1). Markovian data would bring additional bias as mentioned before.
Remark 3.1. For the best complexity, we use fresh estimates of Vk-1 at every iteration (see Algo-
rithm 1 and Insight 4). This gives a tight bound for the bias to get the O(e-2) complexity. This
insight is in contrast to the black box view of Perolat et al. (2015), which uses an estimate of Vk-1
from the stage 2 within the stage 1. Our analysis behooves both agents to remember the output
policies of stage 2 instead, so that they can recompute Vk-1 with a lower bias in the stage 1.
Stage 2. In this step, at iteration k, X player fixes its policy and y computes an approximate best
response by solving the single agent problem in (4) by using NPG in (Lan, 2021; Agarwal et al.,
2020). Value function Vk-1 that was used in stage 1 is precisely the value function of the policies
outputted at this stage in iteration k - 1. For NPG update, y player needs Ea〜χk(.∣s)Qx ,yt(∙, a, ∙).
2In principle, this part can be replaced with mirror-prox (Nemirovski, 2004) or OGDA.
6
Under review as a conference paper at ICLR 2022
This is the joint Q function after taking the expectation over the actions of player x. Since x player’s
policy is fixed, We only use policy Xk in this loop, whereas y player continue to update its policy yt.
We can write Bellman equation for learning this oracle and then use TD(0) as in Bhandari et al.
(2018) (see Algorithm 4), which is similar to learning a Q-function. In particular, as long as
st , at , bt , st+1 are sampled using the interaction procedure described earlier, there is no need for
yt update to see the actions or policy of xk. A similar formulation for policy evaluation in MARL is
considered in Perolat et al. (2018) in a slightly different setting.
Greedy exploration. If Assumption 2 does not hold, one way to lower bound the policies is
to use ζ-greedy exploration which we incorporate into our algorithm. The idea of the analysis
will be to pick the exploration parameters depending on the final accuracy. Since the bounds have
inverse dependence with this parameter, using greedy exploration will result in a worse complexity
compared to what we get when Assumption 1 holds. In the former case, we can take ζ = 0.
3.1 Main Result
Theorem 3.2. (Overall performance bound with Assumption 1, 2) For Alg. 1 with ζ = 0,
E[Es0~u [max V xk,y (SC- V *w]]≤O
k∣s∣2(∣a∣∨∣b∣)	( ι
N(1 - γ)5(ρ(1 - γ))2 I(Pmin{χ,y})2
1
v (Py(I-Y))2
+ O (YK).
In particular, the overall complexity is 0(∣S∣3(∣A∣ ∨ ∣B∣)e-2(1 - γ)-12 ρ-6 y-2(min{x, y})-2).
In this bound, the first term is for solving the outer loops in stages 1 and 2. The second term is due
to the bias and the variance of the stochastic oracles that we got by sampling the policies. The final
term is due to approximating generalized policy iteration in the outermost loop (Perolat et al., 2015).
Theorem 3.3. (Overall performance bound with Assumption 1, see App. G.2.2) Let Assumption 1
hold. Let Z = ∣^-YBI in Alg. 1.
E[Eso~u[maxVχk,y(so) - V?(so)]] ≤ O(T(KSYy)+O(KSι3-m6)+o (yK).
Consequently, the SamPIe complexity is O(∣S∣4(∣A∣ ∨ ∣B∣)6(1 — Y)-15e-4ρ-2).
As summarized in Table 1, Theorem 3.3 matches the best-known existing complexity from Wei
et al. (2021), without using the metric subregularity constant and without restricting to Euclidean
projections. For natural policy gradient, this result improves O(e-8) in Wei et al. (2021). We refer
to Remark G.17 for a more detailed comparison. Moreover, Theorem 3.2 matches the best-known
complexity in single agent RL under Assumption 1, 2.
3.2 Convergence analysis
Proof sketch. Next, we show how to obtain the required bounds for this final guarantee, in view
of eq. (3), (4), (5). Our strategy is to characterize the error of each stage by using the outer/inner
structure given in the algorithm. The innermost algorithms (see Alg. 2) are estimating the required
oracles depending on value functions, by sampling the policies, and applying either SGD or TD(0).
As per (5), the next lemma will characterize the error of stage 1 (see (3)): solving the matrix game.
A critical point to derive the fastest rate as observed by Lan (2021) in the single agent setting is to
characterize the bias and variance separately. As the algorithm in Lan (2021) is akin to gradient
descent, we extend the ideas there to the more complicated FoRB algorithm.
Insight 1. The existing analyses for stochastic FoRB are not suitable for us. In the stochastic
variant in Malitsky & Tam (2020), deterministic oracle is computed at each iteration. Bohm et al.
(2020) uses unbiased oracles with bounded variance and decreasing SteP size. In our case, we
7
Under review as a conference paper at ICLR 2022
have biased oracles and we use inner loops to decrease bias and variance. Next, we develop an
analysis with constant SteP size and characterization of the bias and variance explicitly.
Lemma 3.4. (Boundfor stage 1) Let Assumption 1, 2 hold. Denote Xout = T PT=I Xt and yout =
T1 PT=I yt and let η = 1-γ. Then, it holds that
E Es〜U
mayxxSUtQW-XsQsyJ] = O (：) + O GXE∣∣E[θt+ιE] -θ*,tk
+ O (T X Ekθt+1 - θ*,t∣∣2 + Ekθt - θ*,t-1k2) + TnE[Es〜U ZmaX) X[E1,t(z) + E2,t(Z)]]∙
Remark 3.5. When bias and variance are 0, this reduced to 1/T rate as in Zhao et al. (2021).
Bounding 1k (see (3)) is via bounding LHS of Lem. 3.4. This allows to bound the suboptimality
thanks to (5). To this end, we bound the second term in RHS of Lem. 3.4 in Lem. 3.7 and the third
term in Lem. 3.6.
We drop the superscripts from θ, Vk-ι (See Alg. 1) as estimations are symmetric. For a free variable
z = (x,y),define Eι,t(z)+ E2,t(z)= nhθt+ι(∙∣s)-E[θt+1(∙∣s)∣xt],x(∙∣s)-xt(∙∣s)i-nhθy+1(∙∣s)-
EW+ι(∙∣s)∣yt],y(∙∣s)-yt(∙∣s)i.
Insight 2. The last error term in the lemma involving Eι,t, E2,t is due to the coupling between the
free variables xs,ys and randomness ofthe algorithm. For this error, we adapt the “ghost iterate”
trickfrom Nemirovski et al. (2009) for StOChaStiCprimal-dual algorithms (see Lemma F.6).
This lemma analyzes the behavior of FoRB for solving the matrix game with biased oracles. The
bound therefore reflects the bias and variance of these oracles. For simplicity, we suppress some
dependencies in the following bounds, however we include them in Thm. 3.2 and in the appendices.
Next is the the variance estimation, which is similar to Lan (2021), except handling the error term
coming from V；—i as in Insight 3. Apart from that subtlety, this part is similar to SGD-type analysis
with a biased oracle, where we measure the squared distance of the iterate to the solution.
Lemma 3.6. (Variance estimation for step 5) Let Assumption 1, 2 hold. Let βg = P mE{. 2}(n+ncι)
for no ≥ 1. Then,forAlgorithms1 and 3,
EkθN - θ*,tk2 ≤ °(版 + 彳------rʒ----T72 (+ + EIlVk-I - Vk-Ik∞)).
N2	(ρmin{y,x})2 ∖N	J )
Insight 3. Different from the standard critic analyses (Hong et al., 2020; Khodadadian et al.,
2021b), we accountfor the additional bias comingfrom having Vk-I insteadofreal Vk-I (see (2),
Step 4). We exploit structure ofthe underlying problem to make sure the error term appears as
EkVk-I — Vk-ιk∞ in the bound instead of EkVk-I — Vk-ιk∞, which would deteriorate the rate.
The next estimation is critical for obtaining the complexity result. In particular, we bound the bias
of θt+ι. Since in Lemma 3.4, we need a tight bound for ∣∣E[θt+ι∣χt] - θ*,tk = ∣∣E[Θn|xt] - θ*,tk,
We have to be careful with the additional bias from Vk-1. This part is similar to SGD-type analysis
with a biased oracle, where we measure the distance of the expectation of the iterate to the solution.
Lemma 3.7. (Bias estimation for step 5)) Let Assumption 1, 2 hold, βg = P ma{χ ；}(二十一0), no =
O (Smin{χ,y})2). ForAIgOrthmS 1 and 3
kE[θNlχt] - θ*,tk2 ≤O (ɪ + (P mm{χ,y}HE[Vk-"χt] - Vk-"Q .⑹
Insight 4. The reason to use fresh estimates for Vk-I at each t as in Algorithm 1 is the result of
this lemma (see Remark 3.1). Since the bias term in the algorithm's analysis is ∣∣E[Θn |xt] — θ*,tk
in Lemma 3.4, we take the square root ofthe result of Lemma 3.7. If Vk-I is estimated before
Xt, then we will have EkVk-I — Vk-I k in the bound of Lemma 3.4, which will have the rate
8
Under review as a conference paper at ICLR 2022
O(1∕√N). On the other hand, ifwe estimate Vk-I freshly as in Algorithm 1, then we will be able
,	.i ∙	7 I ∙	1	1 11 τπ Γτ`z^	I 1 τ r	11 J ∕rx / -ι I ^∖τ∖	♦ ,τ	, τ
to use the improved bias bound ∣∣E[Vfe-ι∣xt] 一 Vk-ιk ≤ O(1∕N) as in the next lemma.
The next lemma is for the estimation of the value function Vk-1 using the policies xk-1 , yk-1 with
TD(0). Therefore, this is an analysis for TD(0), similar to Lan (2021).
Lemma 3.8. (Variance/Bias estimation for step 4)) Let Assumption 1, 2 hold and βnω =
ρ(i-γ)jn+no), with no = O Q(i-γ))2). The variance and bias of Vk-L computed as in Algo-
rithm 1 satisfies
kE[Vk-ι∣xt,yt] - Vk-ιk2 ≤O (N-2),	EkWI- Vk-ιk2 ≤O (N-1(ρ(1 - γ))-2).
Unlike stage 1, the stage 2 (finding the best response) mirrors the single agent analysis closely. Due
to space constraints, we defer the details to App. F. Combining Lemma 3.4 with the result of stage 2
(which is of the same order) in (5) gives Theorem 3.2. The main idea of Thm. 3.3 is to use ζ-greedy
exploration to replace the policy values x, y (See Theorem 3.2, Lemmas 3.6, 3.7, 3.8), which might
be 0 without Assumption 2 (see App.G.2.2).
4 Numerical verification
We validate our algorithm in tabular domains comparing against OGDA (Wei et al., 2021) and
REINFORCE (Daskalakis et al., 2020). More results , implementation details and environment
description are given in Appendix H. We emphasize that our main contribution is theoretical; the
preliminary computational results are for verification purposes.
1
Co-Uum -QEnaOs-=qβq2d
ɑw-
≡laos-
W QB-
Vo*-
OT «.<»-
ɑoo-
K	ReflectedNAC
——OCDA
∖∖	——REINFORCE
O 2S ∞ TS IW 12S ISO ITS 2W	0 2sg * g IKUOlK 28	OR 3 8aol8	0 20 40 W SO IW
Environment steps (x 100)	Environment steps (x 100)	Environment steps (x 7000)	Environment steps (x 7000)
(a) Bandits x player	(b) Bandits y player	(c) Alesia x player	(d) Alesia y player
Figure 1: Bandits We plot the probability of action a* for the X policy and b* for the y policy. Results are
averaged over 10 seeds. Alesia: Experiments in a Alesia with length L = 3 and coin budget C = 6. The
suboptimality gap on the vertical axis is maxy V x,y (s0) - V * (s0) for x and | minx V x,y (s0) - V *(s0)| for y
where s0 is the initial state that is deterministic in Alesia. Results are averaged over 5 seeds.
Observations. Both the domains challenge theoretical assumptions. Therefore, our best complexity
result O(e-2) from Theorem 3.2 does not apply in this setting. Similarly, the assumptions in Table 1
do not hold. The assumption of (Wei et al., 2021) and our Assumption 1 do not hold because in
Alesia the players can only lose coins. Therefore the initial state cannot be reached in finite time
from any state. Finally the assumption of Daskalakis et al. (2020) does not hold since in Alesia the
game ends only when either one player wins or both players finish their coin budget. The game ends
with probability 0 in all other cases. Thus, it follows that we cannot lower bound the termination
probability at any state. Nevertheless, we observe that all the algorithms converge. Figures 1a,1b
shows the value of the bandit player policies evaluated at the NE actions (a*,b*), i.e. χ(a*) and
y(b*). Reflected NAC converges faster than OGDA (Wei et al., 2021) and REINFORCE (Daskalakis
et al., 2020). Similar conclusions arise from Figures 1c, 1d where we plot the suboptimality gap in
Alesia.
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
pp. 64-66. PMLR, 2020.
9
Under review as a conference paper at ICLR 2022
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In Inter-
national Conference on Machine Learning,pp. 551-560. PMLR, 2020.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances
in Neural Information Processing Systems, 33, 2020.
Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator theory in
Hilbert spaces, volume 408. Springer, 2011.
Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv
preprint arXiv:1906.01786, 2019.
Jalaj Bhandari and Daniel Russo. On the linear convergence of policy gradient methods for fi-
nite mdps. In International Conference on Artificial Intelligence and Statistics, pp. 2386-2394.
PMLR, 2021.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Conference On Learning Theory, pp. 1691-1692.
PMLR, 2018.
Axel Bohm, Michael Sedlmayer, Erno Robert Csetnek, and Radu Ioan Bo]. TWo steps at a time-
taking gan training in stride with tseng’s method. arXiv preprint arXiv:2006.09033, 2020.
Noam BroWn and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885-890, 2019.
Jingjing Bu, Lillian J Ratliff, and Mehran Mesbahi. Global convergence of policy gradient for
sequential zero-sum linear quadratic dynamic games. arXiv preprint arXiv:1911.04672, 2019.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods With entropy regularization. arXiv preprint arXiv:2007.06558,
2020.
Constantinos Daskalakis, Dylan J Foster, and Noah GoloWich. Independent policy gradient methods
for competitive reinforcement learning. Advances in Neural Information Processing Systems, 33,
2020.
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A tWo-timescale frameWork
for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,
14, 2001.
Sajad Khodadadian, ZaiWei Chen, and Siva Theja Maguluri. Finite-sample analysis of off-policy
natural actor-critic algorithm. arXiv preprint arXiv:2102.09318, 2021a.
Sajad Khodadadian, Thinh T Doan, Siva Theja Maguluri, and Justin Romberg. Finite sample anal-
ysis of tWo-time-scale natural actor-critic algorithm. arXiv preprint arXiv:2101.10506, 2021b.
Jens Kober, J AndreW Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014. Citeseer, 2000.
Galina M Korpelevich. The extragradient method for finding saddle points and other problems.
Matecon, 12:747-756, 1976.
Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, neW sampling
complexity, and generalized problem classes. arXiv preprint arXiv:2102.00135, 2021.
10
Under review as a conference paper at ICLR 2022
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp.157-163. Elsevier,1994.
Yura Malitsky and Matthew K Tam. A forward-backward splitting method for monotone inclusions
without cocoercivity. SIAM Journal on Optimization, 30(2):1451-1472, 2020.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820-6829. PMLR, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. Journal of Machine Learning Research,
21(105):1-49, 2020.
Remi Munos. Error bounds for approximate policy iteration. In Proceedings of the Twentieth
International Conference on International Conference on Machine Learning, pp. 560-567, 2003.
Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAM Journal on Optimization, 15(1):229-251, 2004.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-
1609, 2009.
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum markov games. In International Conference on Machine Learning, pp.
1321-1329. PMLR, 2015.
Julien Perolat, Bilal Piot, and Olivier Pietquin. Actor-critic fictitious play in simultaneous move
multistage games. In International Conference on Artificial Intelligence and Statistics, pp. 919-
928. PMLR, 2018.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180-1190, 2008.
Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, and Matthieu Geist. Approximate
modified policy iteration. In Proceedings of the 29th International Coference on International
Conference on Machine Learning, pp. 1889-1896, 2012.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 5668-5675, 2020.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095-1100, 1953.
Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player
games with near-optimal time and sample complexity. In International Conference on Artificial
Intelligence and Statistics, pp. 2992-3002. PMLR, 2020.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
11
Under review as a conference paper at ICLR 2022
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44,1988.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. Advances in Neural information
Processing Systems, 12:1057-1063, 2000.
Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Provably efficient online agnostic learning
in markov games. arXiv preprint arXiv:2010.15020, 2020.
Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted
to SIAM Journal on Optimization, 1, 2008.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674-690, 1997.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint
arXiv:1611.01224, 2016.
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of
decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games.
arXiv preprint arXiv:2102.04540, 2021.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite time analysis of two time-scale actor
critic methods. arXiv preprint arXiv:2005.01350, 2020.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory, pp. 3674-3682. PMLR, 2020.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for actor-critic
algorithms. arXiv preprint arXiv:2004.12956, 2020a.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale
(natural) actor-critic algorithms. arXiv preprint arXiv:2005.03557, 2020b.
Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason D Lee, and Yuejie Chi. Policy
mirror descent for regularized reinforcement learning: A generalized framework with linear con-
vergence. arXiv preprint arXiv:2105.11066, 2021.
Kaiqing Zhang, Zhuoran Yang, and Tamer BaSar Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019a.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Policy optimization provably converges to nash
equilibria in zero-sum linear quadratic games. Advances in Neural Information Processing Sys-
tems, 32, 2019b.
Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. Advances in Neural Information Processing
Systems, 33, 2020.
Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang. Provably efficient actor-critic for risk-sensitive
and robust adversarial rl: A linear-quadratic case. In International Conference on Artificial Intel-
ligence and Statistics, pp. 2764-2772. PMLR, 2021.
Yulai Zhao, Yuandong Tian, Jason D Lee, and Simon S Du. Provably efficient policy gradient
methods for two-player zero-sum markov games. arXiv preprint arXiv:2102.08903, 2021.
12
Under review as a conference paper at ICLR 2022
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function
approximation. In Advances in Neural Information Processing Systems, volume 32, 2019.
13
Under review as a conference paper at ICLR 2022
Contents
1	Introduction	1
1.1	Related works ................................................................. 2
2	Preliminaries	3
3	Reflected NAC algorithm with a game etiquette	5
3.1	Main Result ................................................................... 7
3.2	Convergence analysis .......................................................... 7
4	Numerical verification	9
A	Background on NPG and NAC	16
B	Basic results on RL and optimization	17
C	Error propagation framework	19
D	Markovian bias	20
E	Single loop NAC with etiquette	22
E.1 Proofs for stage 1 of single loop NAC with etiquette in ....................... 22
E.1.1	Formulation ........................................................... 22
E.1.2	Proofs ................................................................ 24
E.2 Proofs for stage 2 of single loop NAC with etiquette .......................... 27
E.2.1	Formulation ........................................................... 27
E.2.2	Theoretical results ................................................... 28
F Proofs for Reflected NAC with a game etiquette	31
F.1 Proofs for stage 1 of Reflected NAC with a game etiquette ..................... 31
F.1.1	Formulation ........................................................... 31
F.1.2	Theoretical results ................................................... 32
F.2 Proofs for stage 2 of Reflected NAC with a game etiquette ..................... 39
G Greedy exploration to remove Assumption 2	42
G.1 Single loop NAC with a game etiquette and ζ-greedy exploration ................ 42
G.1.1 Stage 1 of single loop NAC with a game etiquette and ζ-greedy exploration
(See Algorithm 8) ..................................................... 42
G.1.2 Stage 2 of single loop NAC with a game etiquette and ζ-greedy exploration
(See Algorithm 8) ..................................................... 47
G.2 Reflected NAC with a game etiquette and ζ-greedy exploration .................. 52
G.2.1 Single agent result with ζ-greedy exploration .......................... 52
14
Under review as a conference paper at ICLR 2022
G.2.2 Reflected NAC with a game etiquette and ζ-greedy exploration in Algo-
rithm 10 .................................................................. 54
H Additional Experiments	59
H.1	Environments description ................................................. 59
H.2	Hyperparameters selection. ............................................... 59
15
Under review as a conference paper at ICLR 2022
A	Background on NPG and NAC
Natural policy gradient and natural actor-critic. As we work in the tabular setting, in this
paper, we focused on the natural policy gradient (Kakade, 2001) in softmax parameterization which
admits a simple update rule. In particular, the update rule for NPG in single agent setting is (Agarwal
et al., 2020, Lemma 5.1)
∏t+ι(∙∣s) X ∏t(∙∣s) exp(ηQπt(s, ∙)),
which is the closed form solution of the update in (1). To get a sample-based version of this algo-
rithm, one needs to learn Qπt typically in an inner policy evaluation loop as in Lan (2021). This
is also called natural actor-critic (NAC) since the actor updates the policy πt+1 and critic learns the
value function Qπt .
Note that the update rule in (Agarwal et al., 2020, Lemma 5.1) is written with the advantage function,
however, due to softmax parameterization, it is equivalent to the form we give.
We can also generalize (1), by using Bregman distances instead of the KL divergence
Xt+ι(∙∣s) = P(xt(∙∣s),Qxt(s, ∙)) = arg 邛也乂Qxt(s, ∙),x(∙∣s)i + D(x(∙∣s),xt(∙∣s)),⑺
x(∙∣s)∈∆
Finally, for the formal setup of Bregman distances, we refer to Tseng (2008); Nemirovski et al.
(2009). Throughout the paper, we focus on the case when D is KL divergence, so that the update
rule corresponds to NPG rule. This choice corresponds to the distance generating function of D
being strongly convex in `1 norm which gives the standard inequality
D(x,y) ≥ 2kχ - yk2,	⑻
that is used frequently throughout the proofs.
Single loop NAC. Unlike the previous case, single loop NAC (Hong et al., 2020; Khodadadian
et al., 2021b) does not have an inner loop for computing Qπt at iteration t. In contrast, single
loop NAC keeps a running estimate for this oracle (which corresponds to one iteration of policy
evaluation) and due to its two time-scale nature, still converges. In the tabular case, the simplest
single loop NAC update takes the form
θt+1 = θt - βt e(st, at) (θt(st, at) - r(st, at) - γθt(st+1, at+1))	(9)
∏t+ι(∙∣s) X ∏t(∙∣s)exp(ηtθt+ι(s, ∙)),
with properly selected βt,ηt, generally with ηt∕βt → 0.
Temporal difference learning. For constructing state or action value functions from samples, we
will use temporal difference learning and in particular TD(0) (Sutton, 1988; Bhandari et al., 2018;
Tsitsiklis & Van Roy, 1997). This algorithm can be seen as a stochastic approximation scheme for
solving a linear equation (Tsitsiklis & Van Roy, 1997; Lan, 2021). In particular, by denoting the
stationary state distribution under π as ρπ , we define
Fπ(θ)(s, a) = ρπ(s)π(a∣s) (θ(s, a) — r(s, a) — Y ^X P(s0∣s, a)π(a0∣s0)θ(s0, a0)).
s0,a0
First, we note that F π(θ?) = 0 where θ? = Qπ. Under Assumption 1, 2 Fπ is strongly monotone
(see (Bhandari et al., 2018, Lemma 3), (Lan, 2021, Section 5.2). The main tools to show this are As-
sumption 1, 2 and Bellman operator being γ-contraction. Then, one can use for example (Bauschke
et al., 2011, Example 22.6, Example 20.7).
One can sample St 〜ρπ, at 〜∏(∙∣st) and st+ι 〜P(∙∣st, at), at+ι 〜π(∙∣st+ι) and one step of
TD(0) corresponds to (9). Note that under i.i.d. assumption, the update in (9) is an unbiased estimate
of the update that we would get by using the true operator Fπ. The results for TD(0) can be extended
to Markovian setting without the i.i.d. assumption by using a uniform mixing assumption (Bhandari
et al., 2018) (see also Appendix D).
16
Under review as a conference paper at ICLR 2022
1 - γ,
PwA
1 - Y
1
1 -Y,
3
1 -Y,
2
B Basic results on RL and optimization
Some notation. We say that an operator θ 7→ F(θ) is λmin-strongly monotone if hF(θ1) -
F(θ2),θ1 -θ2i ≥ λminkθ1 -θ2k22 and λmax-Lipschitz if kF (θ1) -F(θ2)k2 ≤ λmaxkθ1 -θ2k2.
These conditions can be defined with other norms, but we stick to `2 -norm for simplicity.
Lemma B.1. Define θt recursively as θt+1 = θt - βtF (θt, ξt) where r(s, a, b) ≤ 1 and F (θt, ξt) =
e(s0, a0)(θt(s0, a0) - r(s, a, b) - γθt(s00, a00)) and recall the definition of Qk (s, a, b) = r(s, a, b) +
γ s0 P(s0|s, a, b)Vk (s0). Then, it follows for any t, k
kθt k∞ ≤	1
kθtk2 ≤
M-I k∞ ≤
..~ . ...
kF(θt,ξt)k2 ≤
kQk(s,a,b)k∞ ≤ ,
1-γ
Proof. The first inequality is proven by induction, for example see (Khodadadian et al., 2021b,
Lemma C.10). Following inequalities are either basic consequences of the first inequality or directly
follow from definition.	白
A classical result that we use frequently in the proofs is performance difference lemma by Kakade
& Langford (2002). The statement of the lemma is slightly different due to multi agent setting, but
since one policy is held fixed while changing the other one, the original proof of the lemma extends
straightforwardly. The proof for this case is given in Daskalakis et al. (2020).
Lemma B.2 (Performance difference lemma. See Kakade & Langford (2002); Daskalakis et al.
(2020)). For any policies x, y1 , y2 and any state s0
Vx,y1(so)- Vx,y2(so) =占Es〜dx,yι hEa〜x(∙∣s)Qx,y2(s,a, ∙),yι(∙∣s) - y2(∙∣s)i
A standard result that we use is Lipschitzness of y 7→ V x,y(s0). For example, see (Hong et al.,
2020, Lemma 7). We provide proofs as we use the precise constants and use them slightly differently
than Hong et al. (2020).
Lemma B.3. For any policies x, y1 , y2,
kVx,y1 - Vx,y2 k∞ ≤ -A- max ky1(∙∣s)-y2(∙∣s)kι.
(1 - γ)2 s
Proof. By the performance difference lemma Kakade & Langford (2002) and Cauchy-Schwarz in-
equality, for any s0,
Vx,y1(s0) - Vx,y2(so)=占Es〜dx,yι hEa〜χ(∙∣s)Qx,y2(s,a, ∙),yι(∙∣s) - y2(∙∣s)i
≤ 1-γEs〜dχ0y1 kEa〜χ(∙∣s)Qx,y2(S,a, ∙)k∞kyι(∙∣s) - y2(∙∣s)kι.
Next, we are going to further upper bound the right hand side using Lemma B.1
Vχ,y1(s0)- Vχ,y2(so) ≤ 十 max |旧“〜χ(∙∣s)Qχ,y2(s,a, ∙)k∞kyι(∙∣s) - y2(∙∣s)k1
2
≤ (1 - γ)2 maχl∣yι(∙∣s) -y2(∙∣s)kι.
We take maximum over so to conclude.	□
17
Under review as a conference paper at ICLR 2022
Lemma B.4. We have
kQx,y1 - Qx,y2 k∞ ≤ (1¾ max kyi(忖-y2(忖 kι.
Proof. We note that by the definition of Qx,y1 it follows that for all s, a, b
|Qx,y1 (s, a, b) - Qx,y2 (s, a, b)| = γ XP(s0|s,a,b)(Vx,y1(s0)- Vx,y2(s0))
s0
Jensen’s inequality, and the previous lemma gives the result.
Lemma B.5. Let zt be defined as for all s,
zt+ι(IS) = P(ZrIS),η∕t+ιG •)),
where P is defined in (1). In particular,
P(zt(∙∣s),θt+ι(s, ∙)) = arg ,m. hηtθt+ι(s, ∙),z(∙∣s)i + D(z(∙∣s), zt(∙∣s)).
z(∙∣s)∈∆
Then, it holds that for all S,
Ilzt+ι(IS)-Zt(IS)kι ≤ ηtkθt+ιk∞.
□
Proof. By the update rule of zt, it holds for all z that (for example, see (Tseng, 2008, Property 1))
〈▽D(Zt+ι(∙∣S),zt(IS)) + ηtθt+ι(S, XlS)- zt+ι(∙IS)i ≥ 0.
By plugging in z = zt and using three point identity gives
D(Zt+i(」s), Zt(IS)) + D(zt(・|S),zt+i(・|S)) ≤ ηthθt+ι(s, ∙),zt(IS)- zt+i(・|S)i.
By (8) and Cauchy-Schwarz inequality gives
kzt+ι(IS)-Zt(IS)IlI ≤ ηtkθt+ι(S,∙)∣∣∞.
The result follows by ∣∣θt+ι(S, ∙)k∞ ≤ ∣∣θt+ι∣∣∞.	口
Lemma B.6. We have that maxy V xk,y - V x?,y? = 0 iff xk is in the set of Nash equilibrium points.
Proof. Recall that we say that (x?, y?) is a Nash equilibrium if for any x, y
x?Qx?，yy ≤ χ*Qx*,y?y? ≤ χQx'y?y?.
In particular, it is true when we plug in x = xk , y = yk .
Next, by definition of min operation, one can bound minx xQx,yk yk and by the definition of max
operation, xk Qxk,y? y? ≤ maxy xkQxk,yy. In sum, we have
min xQx,yk yk ≤ x?Qx?,ykyk ≤ x?Qx?,y?y? ≤ xkQxk,y?y? ≤ maxxkQxk,yy	(10)
Of course, by definition, for any x, y, V x,y = xQx,y y.
This is easy to see: let xk = x? for any x? which is a Nash equilibrium, then maxy V x?,y = V x?,y? .
Now assume that maxy V xk,y = V x?,y?, then by (10) it must be that maxy V xk,y = V xk,y? =
Vx?,y?. HenceXk = x?.	口
18
Under review as a conference paper at ICLR 2022
C Error propagation framework
Error propagation of generalized policy iteration for Markov games is given in Perolat et al. (2015).
Notation. We define the Bellman operators following Perolat et al. (2015)
Tx,yV(s) = X x(a|s)y(b|s)r(s, a, b) + γ X x(a|s)y(b|s)P(s0|s, a, b)V(s0)
a,b	s0,a,b
TxV(s) = maxTx,yV(s)
TV(s) = min max Tx,yV(s).
It is easy to derive that these operators are contractions in '∞ norm with constant γ. See also Perolat
et al. (2015); Zhao et al. (2021).
We find the one-sided Nash equilibrium as Daskalakis et al. (2020) and Zhao et al. (2021).
max Vx,y (s) - V?(s) ≤ .
y
This is in contrast to Wei et al. (2021) that shows the rate in the duality gap. Two phases are
characterized in Perolat et al. (2015) as
•	Phase 1: Txk Vk-1 ≈ T Vk-1. By using the definitions of T, Tx this corresponds to
myaxTxk,yVk-1 - mxin myax Tx,yVk-1,
(11)
where
Tx,yVk-1(S) =	x(a|S)y(b|S)r(S, a, b) + γ	x(a|S)y(b|S)P(S0|S, a, b)Vk-1(S0).
a,b	s0,a,b
As Vk-1 is fixed, this will give a standard matrix game for all S. By using a stochastic algorithm, we
are going to make the output xk to be an approximate solution in expectation, therefore we write
E max xskQsk-1ys - min max xsQsk-1ys = E1k (S),
y	xy
where the expectation is over the randomness of the specific algorithm used to generate xk .
In our analysis, we bound the stronger quantity, which is called duality gap
(12)
E max xskQsk-1ys - minxsQsk-1yks ≥ E1k(S),
yx
by the definition of a Nash equilibrium since minx xsQsk-1yks ≤ minx maxy xsQsk-1ys.
•	Phase 2: Vk ≈ (Txk)mVk-1. Since Txk is a contraction Perolat et al. (2015); Zhao et al. (2021),
as m → ∞, for any V, TxkV → maxy Vxk,y. Let us denote the best response as yk and the
approximate best response as yk . We want to bound
Vxk,yk (S) - EVXk,yk(s) = Ek(s),	l
where the expectation is over the randomness of the algorithm used to generate yk .
Then (Perolat et al., 2015, Theorem 1) states that (where maxy xskQsk-1ys
⅛
minx maxy xsQsk-1ys = E0k (S), Vxk,yk (S) - Vxk,yk (S) = Ek (S))
(13)
—
μΓμ(m max Vxk,y (S) - V?(s)
sy
2(γ-γk)
(1-γ)
C∞1,k,0j∈[s0u,kp-1]kjk21,σ
≤
2
+ (1 - Yk)C∞k,0 sup kejhσ + 2γLc∞k+1,0 min(kd0k1,σ,|仇||中),
(1 - γ)	j∈[1,k]	1 - γ
where
(1 - γ)
2 k-1 ∞
γl - γk
γjcq(j+d)
i=l j=i
19
Under review as a conference paper at ICLR 2022
with
cq(j) =	sup
x1,y1,...,xj,yj
d(μPχι,yι ... Px j ,yj )
dσ
q,σ
For given state distributions μ, σ, let Us define the concentrability coefficient Perolat et al. (2015):
sup sup
j x1,y1,...,xj,yj
μPx1,y1 ... Pxj ,yj
σ
：Cμ,σ < +∞.
∞
In particular, by upper bounding Cq(j) ≤ Cμ^ for simplicity (one can also use the tighter bounds;
we use the loose upper bounds for simplicity as they only affect the final bound slightly), the bound
becomes
£〃(s)(maxVxk,y(S) - V?(S)) ≤
sy
2kCμ,σ	H ∣∣2
1 Z	Sup	1匕 k1,σ
1 - γ j∈[0,k-1]
+
kCμ,σ
Tl—不 Sup
(1 - γ) j∈[1,k]
k0j k1,σ +
1-γ
where we also used an estimation from (Zhao et al., 2021, Lemma 2).
One important point here is that we will be making sure the inequalities in these stages hold in
expectation. This is also pointed out in Zhao et al. (2021) with a short explanation. We describe
here the details needed to ensure that these bounds hold in expectation. For this, we have to track
the analysis in Perolat et al. (2015) and in Scherrer et al. (2012) where the derivations in Perolat
et al. (2015) build on. In particular, the relations in (Perolat et al., 2015, Lemma 1) are linear and
therefore, would also hold in expectation. Then, in derivation of (Perolat et al., 2015, Theorem 1),
the arguments in (Scherrer et al., 2012, Lemma 2, Lemma 3) are used. Tracking (Scherrer et al.,
2012, Lemma 3), we see that after taking the total expectation, the bounds become
X μ(s) (E max VXk,y (S)- V? (S)) ≤ 2kCμσ sup	归代。
s	y	1-γ j∈[0,k-1]	1 1,σ
JCμ,σ	HjIl ɪ 2γkCμ,σ	....
+ D j∈Upk]ke2 k1,σ + ^-V,()
where g,j and e,j are as defined in (12), (13).
Remark C.1. For simplicity, throughout the paper we take σ, μ to be the uniform distribution and
hence replaced Cμ,σ by its worst case value |S|. As mentioned in MUnos (2003), this value can be
much smaller in general.
D Markovian bias
As mentioned before, in this setting, the Markovian error is essentially additive in our arguments for
policy evaluation steps, and can be bounded by using uniform mixing assumption. In particular, this
assumption holds when the induced Markov Chain over the states, for any policy pair is aperiodic
and irreducible (Lan, 2021; Khodadadian et al., 2021b). In this chapter, we give an informal expla-
nation to illustrate how Markovian sampling can be incorporated into our proofs with the uniform
mixing assumption. The main references for this kind of analysis is Lan (2021); Bhandari et al.
(2018) for Algorithm 1 and Khodadadian et al. (2021b); Zou et al. (2019) for single loop NAC.
We are going to sketch the arguments for Appendix F.1 which will be applicable also to other policy
evaluation routines.
Recall that by using the oracle for stage 1, we can write
F(θn)(s, a) = ρxt,yt(s)xt(a∣s) (θn(s, a) - X yt(b∣s)r(s, a, b)
b
- γ X yt(b|S)P(S0|S, a, b)Vk-1(S0), (15)
s0,b
20
Under review as a conference paper at ICLR 2022
and
F (θn , ξn ) = e(sn , an ) θn (sn , an ) - r(sn , an , bn ) - γ Vk-1 (sn+1 ) .
This time we have a Markovian data stream and we denote ξn = (sn, an, bn, sn+1). Unlike the i.i.d.
Case Eξn F(θn,ξn) = F (θn).
Now we inspeCt the plaCe in the proof of Lemma 3.6 where we used this estimation. ReCall that the
term we have
— hF(°n, ξn), θn - θ*i,
where we take Conditional expeCtation in Lemma 3.6. As we Can no longer Compute the expeCtation,
we are going to identify the error term
.~ , _ , _ . . 一 , _ , _ . .二.- , 一 ,_ . _ .
-hF7(θn, ξn),θn - θ*i = -hF(θn),θn -。?)一〈F(%, ξn) - F(θn),θn - θ*i .
、---------------{----------------}
err(n)
We now separate the error of %-ι and identify the Markovian error, since We sep-
.IIIlI ,1	1	TV	∙ T	C j	T	1 r∙	X / 八 X ∖
arately handled the error due to Vk-1 in Lemma 3.6. Let us define F(θn, ξn)	=
e(sn , an ) (θn (sn , an ) - r(sn , an , bn ) - γVk-1 (sn+1 ))
err(n) = -hF^(θn, ξn) - F(θn),θn - θ*i-hF(θn, ξn) - F(θn, ξn), θn - θ*i∙
X-----------------------{-------------}
ζ(θn,ξn)
We notice that the last term in the above bound is simply -γhe(sn, an)(Vk-1(sn+1) -
Vk-1(sn+1)), θn - θ?) which can be bounded as in Lemma 3.6. Therefore we focus on the Marko-
vian error which is defined as ζ(θn, ξn).
We will argue as in Bhandari et al. (2018). First, it is easy to see that as in Bhandari et al. (2018),
θn 7→ F(θn, ξn), θn 7→ F(θn) and θ 7→ θn - θ? are all Lipschitz. Therefore, it follows for some
constant C1 that
lζ (θn ,ξn) - ζ(θn-τ ,ξn^ ≤ C1kθn - θn-τ∣∣2∙
By triangle inequality and using the update rule θn+1 = θn - βnF(θn, ξn) along with Lemma B.1
give
3C	n-1
Z (θn,ξn) ≤ Z (θn-τ ,ξn) + ɪ-ɪ E βi∙
γ i=n-τ
Next, we bound EZ(θn-τ, ξn) as in Lan (2021). Let Fn-1 be the filtration generated by ξ0, . . . , ξn-1
and note that θn depends on the same randomness as Fn-1 for all n. In particular, by tower property,
EZ(θn-τ, ξn) = EhF(θn-τ) - F7(θn-τ, ξn), θn-τ - θ*i
—.—,_	、	— - X,-	> . —	r_	_ .
=EhF(θn-τ) - E[F7(θn-τ, ξn) |Fn-T-1], θn-τ - θ?
一^	-J.	. .
≤ 2E∣∣F(θn-τ) - E[F(θn-τ,ξn)∣Fn-τ-1]k
≤ Cρτ,
for some C, where the last bound can be derived the same as (Lan, 2021, Lemma 16) under the
assumption that the induced Markov chain is aperiodic and irreducible.
Then, one can use the arguments in Bhandari et al. (2018) by picking T 〜Tmix 〜 器(：/；) if
n > τmix and τ = n ifn ≤ τmix and using the step size rule of βn which decays as 1/n, the same
as Bhandari et al. (2018); Lan (2021) which will only add logarithmic spurious terms to the final
complexity.
In the case of single loop NAC, the arguments are slightly more involved, however, they are well-
studied. In particular, Zou et al. (2019) introduced the technique to handle Markovian noise for
SARSA. These arguments are used for single loop NAC in Khodadadian et al. (2021b); Wu et al.
(2020); Xu et al. (2020a), which also applies in our setting for Algorithm 5, similar to how the above
arguments apply in our setting for Algorithm 1.
21
Under review as a conference paper at ICLR 2022
Algorithm 5 Single loop NAC with a game etiquette
Require: V⅞ such that ∣∣V⅞ - Vx0,y01∣∞ ≤ E
for k = 1, 2, . . . do
Stage 1
for t = 0, 1, . . . , T - 1 do
Sample (st, at, bt, st+1), with policy pair xt, yt observe st, bt, r(st, at, bt), st+1
θx+1 = θt - Bte(st, at)(θx(st, at) - r(st, at, bt) - YVk-I(St+1)))
θy+1 = θy - βte(st,bt)(θχ (st,bt) - r(st,at,bt) - YVk-I(St+1)))
[xt+ι"s), yt+«|s)] = [P(χt(∙ls),ηθχ+ι(S,∙)), P(yt(∙ls),-ηθy+ι(s,∙))]
Output Xk τ P t=____ι Xt.
Stage 2
fort = 0, 1, . . .,T - 1 do
Sample (st, at, bt, st+1,bt+1) With policy pair Xk ,yt, observe st, bt,r(st, at, bt), st+ι, bt+1
νt+1 = νt - βtνe(St, bt) (νt(St, bt) - r(St, at, bt) - Yνt(St+1, bt+1))
ωt+1 = ωt - βtω e(St) (ωt(St) - r(St, at, bt) - Yωt (St+1))
yt+ι (∙ls) = P (yt(∙ls), -ηθt+ι(s,^ ∙))
Output yk = y^, Vk = ω^+> where i ∈ [T] is selected uniformly at random and Vk = VXk ,yk.
E S ingle loop NAC with etiquette
Theorem E.1. Let Assumption 1, 2 hold and μ be a state distribution. For Algorithm 5, for the
output of X-player
EEso〜“[max Vxk,y(so)] - V*(s0) ≤ *口	ʃ Ρ∣1≡E2≡ +	_
y	T1/4(I-Y)I (I-Y )2	λmin(I-Y)
+ |S ∣ΡRFR + Ρ∣s∣(∣A∣∨∣B I) + ∣s ∣PRFR
(I-Y)λmin	λminλmin(I-Y) (I-Y )34^
+ PRR + ι + PRRI + O ( Yk Cμ,σ ∖
+ (1 - Y)2 + λmin(1 - Y)2 + (1 - Y)4J + k 1 - Y )
which gives O ( 4门CGS(λθAl∨ωBlλV~)4) SampIe complexity.
-γ	min min min
Proof. Inserting the results of Lemmas E.7, E.11 andE.14 to (14) gives the result.	□
E.1	Proofs for stage 1 of single loop NAC with etiquette in
In this part, we are going to formulate and present the results for solving stage 1 with single loop
NAC. Unlike Zhao et al. (2021), we do not assume to have an unbiased access to Vk-1. Therefore,
we have a stochastic oracle involving Vk-ι which is an estimate of Vk-1. We characterize the error
from this term and note that the goal of stage 2 will be to provide this oracle Vk-I with small error.
Therefore, the results in this part will contain the error term EkVk-I - Vk-1k∞
E.1.1 Formulation
Notation. The problem is for all s
min max	X(a|s)y(b|s)Qk-1 (s, a,
x(∙ls) y(∙ls) ^b
b) =: XsQsk-1ys,
where Qk-1(s, a, b) = r(s, a, b) + Y	P(s0|s, a, b)Vk-1(s0),
s0
where we also defined Xs , ys , Qs .
22
Under review as a conference paper at ICLR 2022
Here, Vk-1 is fixed (independent of x, y and iteration counter t), therefore the problem is stan-
dard matrix game, with a restricted access to game matrix Qk-1 (s, a, b). In particular, Vk-1 ≈
V xk-1,yk-1. For all s, the equilibrium condition is for all x, y
xs?Qsk-1ys ≤ xs?Qsk-1y?s ≤ xsQsk-1y?s.
For lighter notation, we refer to Qk-1 as Q since it is fixed during the loop.
At iteration t of solving this matrix game, We will need the oracles Eb〜产小⑼Q(s,a,b) and
Ea〜χt(∙∣s)Q(s, a, b) for the X player and y player, respectively. This part is symmetric.
Let us write the oracle for x variable by using the definition of Q(s, a, b) = Qk-1 (s, a, b):
θχ,t(s, a) = Eb〜yt(∙∣s)Q(s, a, b)	(16)
=X yt(b|S)r(s, a, b) + Y X yt(b|S)P(Sls,a, b)Vk-I(SO).
b	s0,b
Given the sampling matrix diag(x∕ 0 diag(ρxt,yt) as in (Lan, 2021, Sec. 5.2), where Pxt,yt is the
steady state distribution under xt, yt; for the critic, define the operator
Ftx(θ)(s, a) = ρxt,yt (s)xt(a∣s) [θ(s, a) - X yt(b|S)r(S, a, b)
b
- γ X yt(b|S)P(S0|S, a, b)Vk-1(S0)i, (17)
s0,b
which, by Assumption 1, 2 is strongly monotone with 1%由 and we would like to find θ*,t such that
Fχ(θx,t) = 0. By (16), θx,t(s, a) = Eb〜yt(.∣s)Q(s, a, b). Since sampling matrix is positive definite
due to abovementioned assumptions, the solution of (17) is unique.
As we do not have access to true Vk-ι, we have the stochastic operator with the estimate Vk-ι and
by sampling ξ = (st,at,bt, st+ι), St 〜PXXyt, at,〜xt(∙∣st), bt 〜yt(∙∣st), st+ι 〜P(∙∣st, at, bt)
Ft(θt,ξt) = e(St,at) (θt(St,at) - r(St,at,bt) - YVk-I(St+ι)).
By assuming we can sample from the stationary state distributions ρxt,yt,
Eξt hFt(°t, ξt) + e(St, at)γ (Vk-I(St+1)- Vk-I(St+1))]
=Ft(θt) + X PXjnt (S)xt(a∣S)yt(b∣S)P(S0∣S,a,b)e(S,a) (Vk-I(S0) — Vk-I(S0))
s,a,b,s0
=: F(θt) + YPXt,yt (Vk-I- Vk-I) = F(θt) + δv,t, (18)
where we defined the matrix Pxt,yt .
Above, the first equality is due to
Eξt [e(St, at) (θt(St, at) - r(St, at, bt) - YVk-1(St+1))]
=	Pr(S, a, b, S0)e(S, a) (θt(S, a) - r(S, a, b) - YVk-1(S0))
s,a,b,s0
= X PXt,yt (S)xt(a|S)yt(b|S)P(S0|S, a, b)e(S, a) (θt(S, a) - r(S, a, b) - YVk-1(S0))
s,a,b,s0
= X PXt,yt (S)xt(a|S)e(S, a)(θt(S, a)-X yt(b|S)r(S, a, b)-Y X yt(b|S)P(S0|S, a, b)Vk-1(S0)),
s,a	b	s0,b
where the last line is due to Pb,s0 yt(b|S)P(S0|S, a, b)θt(S, a) = θt(S, a) Pb,s0 Pr(S0, b|S, a) =
θt(S, a) and
Pb,s0 yt(b|S)P(S0|S, a, b)r(S, a, b) = Pb yt(b|S)r(S, a, b) Ps0 P(S0|S, a, b) = Pb yt(b|S)r(S, a, b).
23
Under review as a conference paper at ICLR 2022
E.1.2 Proofs
We will drop the superscript on θx , θy as the algorithms are symmetric, so we will only analyze one
case.
Lemma E.2. Under Assumption 1, 2, let βt =λ@ ；/2 and ηt =第/工.Thenfor the critic computed
in stage 1 of Algorithm 5,
T
-XEkθt+1	- θ*tk2 ≤ —	∣∣θι	- θ*tk2	+	4 ( +	Og)L
T 士 11 t+1	*,t"2 - √Tl1 1	*,t"2	(λmin)2(l	- γ)2√T
16∣S ∣∣A∣(l + log T)
-√T(1 - γ)4-
+
+ ¾sA^EkVk-I- Vk-1k∞.
(λmin)
Corollary E.3. Extracting only the dependence on λmin, |S|, |A|, γ, T gives
1T
τ EEkθt+1- θ*,t ι∣2 ≤
t=1
√T {o(W)+o⅛⅛^ )+o(W)}
+ EkVl- Vk-1k∞O (” J
Remark E.4. Different from the standard critic analyses, we have to account for the additional bias
coming from only having Vk-1 instead of real Vk-1.
Remark E.5. Important point here is to exploit strong monotonicity of Ft defined in (17), to make
the error term EkVk-I — Vk-1k∞ appear instead ofthe worse term EkVk-I — Vk-1k∞ which would
deteriorate the complexity.
Remark E.6. With some extra work, we can obtain a step size βt not depending on λθmin, similar
to Khodadadian et al. (2021b). We do not pursue this for brevity and keeping the analysis simple.
Proof. Let Us recall θ*,t(s, a) = Eb〜y".|s)Q(s, a, b) (16) and that Ft(θ*,t) = 0 by the definition of
Ft in (17). Moreover, θt+1 = θt - βtFt(θt, ξt). Analyzing the Update rUle of critic in the standard
way (for example see (Hong et al., 2020, Proof of Thm. 3)), gives
kkι-θ*,tk2 = kθt-θ*,tk2 - 2软瓦@,&),%-。?"〉+ β2kFt(θt,ξt)k2∙	(19)
We will take expectation w.r.t. to the sample ξt = (st, at, bt, st+1), conditioned on θt, xt, yt, and
therefore on θ*,t and Use St 〜ρxt,yt. We also note (18) to separate the error due to Vk-I and derive
_ 一 . ɪ , _ . . _ _ 一 , , _ _ _ _ _
-βtEξt hFt(θt, ξt),θt - θ*,ti = -8thFt(θt),θt - θ*,ti - βthδv,t, θt - θ*,ti
≤ -βthFt(θt),θt - θ*,ti + MkPχt,yt(Vk-I- Vk-1)k2 + βtλminkθt - θ*,tk2,
2λmin	2
where Pχt,yt is the matrix denoting the probability matrix multiplying Vk-I - Vk-I in (18) and
δv,t = γPχt,yt (Vk-1 - Vk-I) and We used CaUchy-SchWarz and Young,s inequalities. We can use
standard inequalities to estimate kPxt,yt (Vk-1 - Vk-1)k22 ≤ |S||A|kPxt,yt(Vk-1 - Vk-1)k2∞ ≤
∣S∣∣A∣kVk-1 - Vk-1k∞ and take Eξt in (19) by using the two estimations above to get
%k%+1- θ*,t∣∣2 =嗅/% - θ?,t∣∣2- 2也〈Ft(θt), θt- θ*,ti +
β⅛≡kVk-1 -
λmin
Vk-1k2∞
+ βtλmmkθt-θ*,tk2 + β2EξtkFt(θt,ξt)k2. (20)
For the inner product, we would use Ft(θ*,t) = 0 and strong monotonicity of Ft (an estimation
similar to (Bhandari et al., 2018, Lemma 3)) to get
2βthFt(θt),θt -	θ*,ti =	2βthFt(θt)	-	Ft(θ*,jθt	-	θ*,ti	≥	2βtλmm∣∣θt	-	θ*,tk2∙	(21)
24
Under review as a conference paper at ICLR 2022
Using this estimate, taking total expectation in (20) and using Young’s inequality on the term in-
Volving ∣∣θt - θ*,tk2 * gives,
Ekθt+ι - θ*,t∣∣2 ≤ (I + α) (1 - βt篇E)Ekθt - θ*,t-ιk2
+ (1 + 1∕α)(1-βtλmm)E∣∣θ*,t-θ*,t-i∣∣2+ β2E∣Wt(θt,ξt)k2 + 济：!$ 11A1 EkVk-I-Vk-1∣∞.
λmin
βtλθ	1
Picking α = 2(1-emin.)With ensuring βt ≤ 十 due to βt choice, and using ∣θ*,t - θ*,t-ι∣∣2 =
k Pb(yt(b∣s) - yt-mnIS))Q(S,a,b)∣2 ≤ p⅛ik Pb(yt(b∣s) - yt-i(b∣s))Q(s,a,b)k∞ ≤
2p∣SIIAI/(1 - Y)maxs kyt(∙∣s) - yt-1(∙∣s)k1 WithLemmaB.1 give
Ekθt+1 - θ*,tk2 ≤ (1 - βtλmn) Ekθt - θ*,t-ιk2 + β2EkFt(θt,ξt)k2
+ (1 S?3θ E (max kyt(∙∣s) -yt-i(∙∣s)kι)2 + βγjS≡EkVk-I- Vk-ik∞.
(1 - γ)2βtλθmin	s	λθmin
We only need to show that E (max§ ∣∣yt(∙∣s) - yt-ι(∙∣s)kι)2 is SmalL This is easy since we use
small step sizes ηt for the policy update. In particular, the update rule of xt Will give by Lemma B.5
kxt+ι(∙∣s) - χt(∙∣s)lk ≤ ηtkθt+ιk∞,
and by the symmetrical update for y t, it holds that k yt+ι (∙∣ s) - yt (∙∣ s) k 1 ≤ ηt k θt+ιk∞ . ThiS is a de-
terministic inequality holding for all S, so we can take its maximum over S, square it, use Lemma B.1
and plug it into the main inequality.
Ekθt+1 - θ*,tk2 ≤ (1 - βtλmn) Ekθt - θ*,t-ιk2 + β2E∣Ft(θt,ξt)k2
8∣s∣∣A∣η2kθt+ιk∞ 4 βtY∣S∣∣ALll^	v ll2
+ βt(1 -YFin	+	EkVkT- VkTk∞.
We plug in the bounds from Lemma B.1
Flm θ I” (1	βtλmin) Flm	θ ll2 ,	2β2	,	8∣s∣∣A∣η2
网佐+1-纵,” ≤ (1- 丁产kθt-θ*,t-1k +(ɪɪ^ + βt(1-γ)4λmm
+ βtγλθS∣∣A∣EkVk-I- Vk-ιk∞.
λmin
Picking ηt = t3∕4 and βt = λθ~⅛∕∑
min
Therefore the final bound is
the recursion will be ut+ι ≤ (1 - cο∕√t)ut + c1 Ci + √C2.
T
-XEkθt+ι - θ*tk2 ≤ 十∣∣θι - θ? 0k2 +	. ( + og Tr-
τt= 111+1	*,t" - √t" 1	*,°"	(λmin)2(1 -γ)2√τ
16∣S∣∣A∣(1 + lοgT)	4γ2∣S∣∣A∣ ll^	ʊ ll2
+	√T(1-Y)4	+ E产EkVkT- VkTk∞.
□
Lemma E.7. Denote Xou = T PT=I Xt and yout = T PT=I yt. Let Assumption 1, 2 hold and
ηt = t3∕4. For the actor computed in stage 1 of Algorithm 5,
E max XosutQsys - XQsyosut
s,z=(x,y)
T
log∣A∣∣B∣	4	2 XE
≤ T1/4	+(1 - γ)2T3/4 + T 乙Ekθt+1 - θ*,tk∞
2T
+ τEEkθy+ι-θy,tk∞. (22)
t=1
25
Under review as a conference paper at ICLR 2022
Corollary E.8. By plugging in the bound for T PT1 E∣∣θt+ι — θ*,tk∞ from Lemma E.2 after
Jensen’s inequality and by noting θtx and θty admit the same bounds, and by extracting only the
dependence on λmin, |S|, |A|, γ, T gives
E [ max xs“Qsys — XQSysUJ ≤ 'O (PlSl(lAl I?1^)∖ + ɪ O (——1——)
[s,z=(x,y) outQy	Q y0ut∖ ≤ T1/4	I (1 — γ)2	+ + T1/4	<λmin(1 — γ))
^q^^Y YpSWW)
+ VEkVk-I — Vk-ιk∞O I ------------λ---------..(23)
Remark E.9. We make sure EkVk-1 — Vk-1k∞ is small by the estimation of Vk-1 in stage 2,
in Lemma E.14, Lemma E.11.
Proof. Recall the notation xsQsys =	a,b x(a|s)y(b|s)Q(s, a, b). First by definition of xout and
the standard arrangement for duality gap, it holds for all s
1T	1T
XoUtQ y - XQ yOUt = T ɪ^hEa〜Xt(∙∣S)Q(S, a, )y(Is)i — T y^hEb~yt(.|s)Q(s，∙, b)，x(Is)i
=T ^X [hEa〜xt(∙∣S)Q(S, a, )y(Is)- yt(Is)i — hEb〜yt(∙∣S)Q(S, ∙,b), X(Is)- Xt(Is)i] ∙	(24)
t=1
We are going to bound the inner products in RHS. From the update rule of Xt+ι, for all s, x(∙∣s) ∈ ∆,
it holds that (see (Tseng, 2008, Property 1))
hVD(Xt+ι(∙∣s),Xt(∙∣s)) + ηtθt+ι(s, ∙),x(∙∣s) — Xt+ι(∙∣s)i ≥ 0.
By three point identity and using the notation
D(X(Is),Xt+i(Is)) ≤ D(X("s),/t(ls)) + ηthθ*,t(S, ∙),x(∙∣s)—勺+i(Is)i
+ ηthθt+ι(s, ∙) — θ*,t(s, ∙),x(∙Is)-Xt+i(，|s)i — D(Xt+ι(∙∣s),Xt(，ls)). (25)
We bound the inner products using Cauchy-Schwarz and Young’s inequalities and Equation (8),
since x(∙∣s) ∈ ∆),
ηthθt+1(s, ∙) — θ*,t(s, ∙),x(1s) — Xt+1(，|s)i ≤ 2ηtkθt+1 — θ*,tk∞,
ηthθ*,t(s, ∙),x(Is) — Xt+i(.|s)i = ηthθ*,t(S, ∙),x(Is) — Xt(Is)i + ηthθ*,t(S, ∙),Xt(Is) — Xt+i(.|s)i
2
≤ ηthθ*,t(s, ∙),x(∙Is)-Xt (1s)i + 与 kθ*,tk∞ + D(Xt+i(・|s),Xt(・Is))
Using these estimations in (25) gives with θ*,t(s, ∙) = Eb〜y".|s)Q(s, ∙, b) gives
hEb 〜yt(∙∣S)Q(S, ∙,b),Xt(Is)- X(Is)i+η D(X(Is),々+I(Is)) ≤ η D(X(Is),Xt(Is))
+2kθt+ι - θ*,tk∞+ηt kθ*,tk∞.
We sum the inequality, use LemmaB.1 and maxχ,χt D(X(∙∣s),Xt(∙∣s)) ≤ log ∣A∣.
T
T XhEb〜yt(∙∣S)Q(S, ∙, b),Xt(∙|s) — X(Is)i ≤	D(X(Is)，XI(Is)) + T1/4
t=1	η1	T
+T X 2kθt+1 - θ*,tk∞+2(P-γηtT.
We use the same estimation for the other player, since it is symmetric, to bound the RHS of (24).
Then, we take maximum of both sides w.r.t. s, z, take expectation and bring back superscripts of
X,y to θt since We will have error from both players.	□
26
Under review as a conference paper at ICLR 2022
E.2 Proofs for stage 2 of single loop NAC with etiquette
Remark E.10. Stage 2 is asymmetric for both players. As we are computing best response to xk, x
player’s policy remains fixed in this phase, it only computes Vt-1 to be used in its next stage 1 step.
First, we are going to show that while running this step, y-player can construct its stochastic oracle
without access to policy or actions of x-player. Then, as the best response problem is essentially
a single agent problem where the other player is part of the environment, our proofs are similar to
the results for single agent setting (Khodadadian et al., 2021b; Hong et al., 2020). Let us denote the
approximate best response as y^. Main goal in this step is that We have to characterize explicitly the
error ∣∣V^k - VXk,y^∣ as it is used in the stage 2.
Note that this is generally not done in single agent setting as the goal is to compute a policy. HoW-
ever, here our main goal is to have access to an oracle approximation Vk = Vxk,yt, rather than the
output policy y^, therefore, we keep track of ωt that tracks this value function with an explicit error
estimate (see Lemma E.11).
E.2. 1 Formulation
Notation. Here, the problem is to compute best response where the other player fixes its strategy.
Let us fix Xk and denote the best response as yk. Here, since Xk is fixed, it is a part of the environment
for y -player and single agent MDP analyses will go through. We only need to be careful to make
sure the “gradient” for yt updates can be calculated by not knowing policy or actions of Xk.
For NPG updates, we will need to compute at iteration t, v?,t(s, b) = Ea~χk(.∣s)Qxk,yt (s, a, b).
Writing the Bellman equation and using the definition of value functions
Qxk,yt (s,a,b) = r(s,a,b) + Y X P(SlS,a, b)V xk，yt (s0)
s0
=r(s, a, b) + Y XP(S0|s, a, b) X Xk(a0∣s0)yt(b0∣s0)Qxk,yt (s0, a0, b0)
s0	a0,b0
We note ν*,t(s0, b0) = Eαo^χk(∙∣s0)Qxk,yt (s0, a0, b0) and take expectation of previous equality with
a ~ Xk(∙∣s),
ν*,t(s,b) = £xk(a|s)r(s,a,b) + YE P(s0∣s,a, b)xk(a|s)yt(b0|s0)v?,t(s0,b0)
α	s0,α,b0
We use the sampling matrix (as (Lan, 2021, Sec. 5.2)) diag(ρxk,yt) 0 diag(yt) and define the
operator
FtV(Vt)(s, b) = Pxk,yt(s)yt(b∣s) [νt(s, b) - X Xk(a∣s)r(s, a, b)
α
-Y X Xk(a∣s)P(s0∣s,a, b)yt(b0∣s0)νt(s0,b0)i,
s0,α,b0
such that Ft"(v?,t)	=	0.	Strong monotonicity of Ft	with	constant 入窘由 follows
from Assumption	1, 2,	and	that the operator Txk ν (S, b)	=	Pα Xk(a|S)r(S, a, b) +
γ Ps0 αbo Xk(a∣s)P(s0∣s,a,b)yt(b0∣s0)ν(b0,s0) is Y contraction in '∞ norm, (Zhao et al., 2021,
Lemma 1) (Bauschke et al., 2011, Example 22.6 and 20.7). We define the stochastic opera-
tor after sampling	ξt	=	(St,at,bt,	st+1,bt+1) with	St	~	Pxk,yt,	at ~	Xk(∙∣st),	b ~	yt(∙∣st),
St+1 ~ P(∙∣St, at, bt), bt+1 ~ yt(∙∣St+ι),
ν
Ftν(νt,ξt) = e(St, bt) (νt(St, bt) - r(St, at, bt) - Yνt(St+1, bt+1)) ,
and as we assume we can sample St 〜ρxk,yt, Eξt[Fν(νt,ξt)] = FtV(Vt). In particular, we see that
as long as St,at,bt, St+ι, bt+ι are estimated in the prescribed way, there is no need for yt update to
see the actions or policy ofXk for FtV(Vt, ξt) to be unbiased estimate of FtV(Vt). It only needs to see
27
Under review as a conference paper at ICLR 2022
its own actions bt, bt+1, r(st, at, bt) and st+1.
EξtFtν(νt,ξt)
=	Pr(st	= s,	at	= a, bt	= b,	st+1	=	s0, bt+1 =	b0)e(s, b)	[νt(s, b)	- r(s, a, b)	- γνt(s0,	b0)]
s,a,b,s0,b0
= X	PXkyt (s)xk (a∣s)yt(b∣s)P(s0∣s, a, b)yt(b0∣s0)e(s, b)[νt(s, b) -r(s, a, b) - IVt(S, b0)]
s,a,b,s0,b0
=X Pxk,yt (s)yt(b∣s)e(s,b) [%(s,b) - X Xk(a∣s)r(s, a, b)
s,b	a
-Y X Xk(a∖s)P(s0∣s,a,b)yt(b0∣s0)νt(s0,b0)i∙ (26)
s0,a,b0
The same estimations as Lemma E.2, without the bias from V⅛-ι, as We have unbiased samples will
give Lemma E.11.
Let us define the corresponding operator for learning state-value function
Vxk,yt (s) = X Xk(a∖s)yt(b∖s)r(s,a, b) + X Xk(a∖s)yt(b∖s)P(s0∖s, a, b)Vxk,yt (s0).
a,b	s0,a,b
Similar to the Q function, we can define ω*,t = Vxk,yt and the operator
Ftω(ωt)(s) = ρxk,yt (s) (ωt(s) - XXk(a∖s)yt(b∖s)r(s, a, b)
a,b
-Y X Xk(a∖s)yt(b∖s)P(s0∖s,a,b)ωt(s0)).
s0,a,b
By Assumption 1, 2, this operator is strongly monotone with λωmin, the justification of which is
the same as the Ft operator defined above. We also note that Ft (ω*,t) = 0. The corresponding
stochastic operator is defined as
ω
Ftω(ωt,ξt) = e(st) (ωt(st) - r(st, at, bt) - Yωt(st+1)) ,
where St 〜PXk,yt, at 〜Xk(∙∖s), bt 〜yt(∙∖s), st+ι 〜P(∙∖s, a, b) and as we assume we can sample
St 〜ρxk ,yt,
-〜一. .- ..,
Eξt [Ft (ωt,ξt)]= Ft (ωt).
E.2.2 Theoretical results
First, we characterize the critic of stage 2 in Algorithm 5, denoted by νt, ωt for action value function
and state value function, respectively.
Lemma E.11. Let Assumption 1, 2 hold. Let	βt	=穴	1加/2,	βt	=	λω	1" and	ηt	=	t374.	Then
min	min
for the critic computed by stage 2 of Algorithm 5,
1 X 耽 “ M_ 1 JO CS ∖∖B ∖ ʌ +O (	1 ʌ +O ( ∖S∖∖B∖ N
T t=ιEkVt+1- V?，tk = √T IO (Ly 尸 O l(λmm)2(i-γ)2 尸 O Inf 月.
T X Ekωt+1 - ω,tk2 = √T {O ((⅛)+ O ((λmm)21ι-γ)2)+ O (a⅞)}.
Remark E.12. Using the same SamPles for θt and Vt does not seem to cause a problem since
the analysis in Lemma E.2 only takes conditional expectations conditioning on θt and uses that
maxs ∣∣Xt(∙∖s) — Xt+1(∙∖s)k2 is small directly by small step sizes η.
Remark E.13. Vt is not used in the stage 2, but it is estimated to be used in the stage 1 and also
to make the bound of Lemma E.7 Small since the bound implies E∣ω^+1 — V xk ,y^∣2 = EkVk-I —
%-1k2 ≤O(1/TS).
28
Under review as a conference paper at ICLR 2022
Proof. Let us recall ν*,t(s,b) = Ea 〜#%(«必以 ,yt(s,a,b) and ω*,t = V xk,yt.
We expand the squared norm
l∣νt+ι- ν*,tk2 = kνt- ν*,tk2- 2βthFt (Vt,ξt),νt- ν*,ti + β2kFtν(Vt,ξt)k2.
We take expectation w.r.t. the randomness of ξt = (st, at, bt, st+1, bt+1) and use from (26) that
一	-二	，	.、r	一	,
Eξt [FV (νt ,ξt)] = Ft (νt).
Eξt kνt+ι- ν*,t∣∣2 = IlVt- ν*,t∣∣2 — 2 *风(FtV (Vt), νt — V?,ti + e2% kFV(Vt, ξt )k2
By strong monotonicity and Ft (V?,t) = 0, it follows that
2βt hFtV (Vt), Vt — V?,ti = 2βt hFtV (Vt) — FtV (V?,t), Vt — V?,t i
≥ 2βtλVmin lVt — V?,t l22 .
We use this estimation and then Young’s inequality to obtain
Eξt kVt+i — V?,tk2 ≤ (I- 2βtλmE) kVt - V?,tk2+β2 嘎」|FV(Vt,ξt )k2
2
≤ (I — βtλmin) kVt — V?,t-1k2 + βλV- kV?，t — V?,t-1k2 + βt Eξt k Ft (Vt, ξt)k2 (27)
βt λmin
We now have to bound the second term on RHS. For this, we will use Lemma B.4, but first we have
to transform the term into the form of Lemma B.4. We recall V?,t(s, b) = Ea 〜Xk(∙∣s)Qxk,yt (s,a, b)
kV?,t - V?,t-1k2 ≤ PiSnBIkV?,t - V?,t-1 k∞
=PMBI max IEa〜Xk(∙∣s) (Qxk,yt (s, a, b) — Qxk,yt-1 (s, a, b)) ∣
≤ PMBI maxEa〜xk(∙∣s) ∣Qxk,yt (s, a, b) — Qxk,yt-1 (s, a, b) ∣
≤ PMBJkQxk,yt — Qxk,yt-1 k∞
≤ 2γVZISIiB|mαχkyt(1s) — yt-ι("s)kι
(1 — γ)2	s
where the second inequality is by Jensen and the last inequality is by Lemma B.4.
As used in the proof of Lemma E.2, update rule of yt(∙∣s) gives kyt(∙∣s) — yt-ι(∙∣s)kι ≤ ηtkVt+ιk∞
by Lemma B.5. Using these estimates in (27) after taking total expectation gives
EkVt+1 — V?,tk2 ≤ (1 — βtλmin) EkVt — V?,t-ik2 + 8|怨?；对；+1瞑
+ β2EkFν(Vt,ξt)k2.
We get the result by using the same argument as the end of the proof of Lemma E.2, by also us-
ing LemmaB.1 to bound kFif (Vt,ξt)k2 and kVt+ιk∞.
The proof of the second inequality is exactly the same except that in (27), instead of kV?,t — V?,t-1k
We will have kω*,t — ω*,t-ι k and We will therefore use Lemma B.3.	□
.	∙ -∣ -∣	F	1 ττ^m, 77, ττ^m, ，，手 < ∙	<	∙	-∣	.	Fi	.r∙∙>∙>
Next, we will upper bound Vxk ,yt — Vxk ,yk which is a single agent problem as xk is fixed and we
showed in this section how to do the policy evaluation without knowing the actions of xk . The next
lemma will be proven similar to single agent settings (Hong et al., 2020; Khodadadian et al., 2021b).
Lemma E.14. Let Assumption 1, 2 hold. Let βV = yγ 1t"，β = * 1ti∕2 and ηt = t3/4. Then
min	min
for stage 2 of Algorithm 5,
T
T X EEs0” (Vxk底(SO) - Vxk,yt (S0)) ≤ (I 力 1/4 + (T-
2T
+ T(1 - Y) 4EkVt+1 — "*,tk∞
λmin(1-Y)2
+ O
where the second inequality follows by using the results in Lemma E.11.
29
Under review as a conference paper at ICLR 2022
Proof. By the update rule of yt+ι for all s, y(∙∣s) ∈ ∆, (see (Tseng, 2008, Property 1))
D(y(∙∣s),yt+ι(∙∣s)) ≤ D(y(∙∣s),yt(∙∣s)) — ηthνt+ι(s, ∙),y(∙∣s) — yt+ι(∙∣s)i — D(%+ι(∙∣s), %(∙∣s))
=D(y(∙∣s),yt(∙∣s)) 一 ηthν*,t(s, ∙),y(∙∣s) ― yt+ι(∙∣s)i
―IlthVt+ι(S, ∙) ― ν*,t(s, ∙),y(∙∣s) ― %+ι(∙∣s)i ― D(%+ι(∙∣s),yt(∙∣s))
We estimate the inner products by Cauchy-Schwarz, Young,s inequalities and Equation (8)
IthVt+1(S, ∙) ― ν*,t(s, ∙),y(∙∣s) ― yt+ι(∙∣s)i ≤ 2ηtkνt+ι ― ν*,t∣∣∞,
and
一ηthν*,t(S) ∙),y(∙∣s) — yt+ι(∙∣s)i = —ηthν*,t(S, ∙),y(IS) — yt(∙∣s)i — ηthν*,t(S) ∙),yt(∙∣s) — yt+ι(∙∣s)i
≤ —ηhν*,t(S))^卜国一yt(1S)〉+ ηt " jt"∞ + 0^+136),^36)).
Consequently, by using the definition of ν*,t(S, b) = Ea〜方μ.|§)Qxk,yt (s, a, b)
hEa〜xk(∙∣s)Qxk,yt(s,α,b),y(∙∣s) — yt(∙∣s)i + — D(y(∙∣s),yt+ι(∙∣S)) ≤ — D(y(∙∣s),yt(∙∣S))
ηt	ηt
+ 2kνt+ι — ν*,t∣∣∞ + "W2""' . (28)
We sum the inequality and use maxyι,y2 D(yι(∙∣S), y2(∙∣S)) ≤ log ∣B∣ with KL divergence, to get
1 二,	一................. 1	........ 1
T EhEa〜Xk(TS) Q k,yt (S,生内小⑸—yt("S)〉≤ ^^03(16),01(1 S)) + T1/4 log ∣B∣
t=1	ηι
2 T	spτ
+ T X Ilνt+1 - ν*,t∣∣∞ + 2T(17— Y)2 .	(29)
Let us recall that 碇 is a best response policy. By the performance difference lemma
一 一 *	一 .-	1	一 L.........................
VXk,yk (S0) — Vxk,yt(so) = kES〜得虱 hEa〜Xk(∙∣S)QXk,yt(S, a, ∙),0^(∙∣s) — %(∙∣s)).
As x k and y ^ are independent of t and fixed throughout the loop, in (29) we plug in y = ya and take
ES 〜<k,yQ	口
30
Under review as a conference paper at ICLR 2022
Algorithm 6 Reflected NAC with a game etiquette. (See Algorithm 1)
Require: Subroutine Policy-Eval (see Algorithm 2, Algorithm 3, Algorithm 4 ). Initial policies
χo,yo,yo
for k = 0, 1, . . . do
Stage 1
for t = 0, 1, . . . , T - 1 do
[Vkx-1, Vk-1] = [Policy-Eval(xk-1, yk-1, N, βnω), Policy-Eval(xk-1, yk-1, N, βnω)]
[θχ+ι, θy+ι] = [policy-Eval(Xt,yt,N, Vk-ι,βn), POlicy-Eval(Xt,yt,N, Vk-ι,βn)]
χt+ι3S) = P(Xt(IS),η (2θχ+ιG ∙) - θx(s, •)))
yt+ι(IS) = P(yt(∙ls), -η (2θy+ι(S, ∙) - θy(S, ∙)))
Output Xk = 1 Pt=1 Xt.
Stage 2
for t = 0, 1, . . . , T - 1 do
Vt+1 = POlicy-Eval(Xk,yt,N,β = βn)
yt+ι (IS) = P 仞《园-ηνt+ι(S, ∙))
Output yk = y^, where t ∈ [T] is selected uniformly at random.
Algorithm 7 POlicy-Eval (See Algorithm 2, Algorithm 3, Algorithm 4)
Require: Policy pair x, y, iteration counter N, oracle Vfe_1, step size β
for n = 0, 1, . . . , N - 1 do
Sample Sn 〜ρx,y (∙), an 〜X(∙∣Sn), bn 〜y(∙∣Sn), Sn+1 〜P (∙∣Sn, &n, bn).
if β = βnω then
F(φn, ξn) = e(Sn) (φn(Sn) - r(Sn, an, bn) - γφn(Sn+l))
else if β = βnθ then
F(φn, ξn ) = e( Sn, bn ) (φn (Sn, bn) - Ir(Sn, an, bn ) - YVk-I(Sn+1 ))
else if β = βnν then
Sample also bn+ι 〜y(∙∣Sn+ι).
F(φn, ξn ) = e( Sn, bn ) (φn (Sn, bn) - r(Sn, an, bn ) - γφn (Sn+1, bfnj+1. ))
ι	ι Q π/ι λ 、
φn+1 = φn - βnF(φn , ξn )
Output: φN
F Proofs for Reflected NAC with a game etiquette
F.1 Proofs for stage 1 of Reflected NAC with a game etiquette
F.1.1 Formulation
For single loop actor-critic in the previous section, it was acceptable to do rough analysis since we
used small step sizes. With inner-outer structure, as Lan (2021), we can do tighter analysis with
constant step sizes for the outer loops (updates of X, y). Therefore, we can no longer use GDA
that we used for single loop NAC, and the techniques from Lan (2021) are not sufficient as the
algorithm therein would correspond to GDA in min-max setting. We will have to use a convergent
algorithm for the matrix game solver, such as Mirror Prox (Nemirovski, 2004; Korpelevich, 1976)
or FoRB ((Malitsky & Tam, 2020)) and extend the ideas from Lan (2021) to these more advanced
algorithms to characterize the bias and variance separately. Also, we would have to do a tighter
analysis for Vk estimation.
31
Under review as a conference paper at ICLR 2022
To obtain the desired oracle θ*,t(s, b) = Ea~χt(∙∣s)Q(s, a, b), as in (17),
Ft(θ)(s, b) = PXtM (s)yt(b∣s) [θ(s, b) - X xt(a∣s)r(s, a, b)
a
- γX xt(a|s)P(s0|s, a, b)Vk-1(s0)i. (30)
s0,a
We recall that Ft is strongly monotone with λθmin under Assumption 1, 2. Moreover Ft is Lipschitz
with λmax. We refer to Appendix E.1.1 for how the oracles in the algorithm can be computed
without accessing to other agent’s policy or actions. Moreover, we do not put subscripts θX , θy as
the estimations will be symmetric again.
F.1.2 Theoretical results
Theorem 3.2. Let Assumption 1, 2 hold. For Algorithm 6, for the output of x-player
EEso~μ[max Vxk,y(so) - V?(so)] ≤ Cμ,σk-θ[	1	+
0 y	(1 - γ)	T (1 - γ)2	λθmin(1 - γ)2N
+ ∣s∣2(∣A∣2 ∨∣B∣2) +	∣s∣∣A∣
Kn)2N2(1 - γ)2	(λmm)2(λmm)2(1 - γ)2N
|S||BI +	1	+ PSPi ] + O	(Cμ,σYkʌ
(1 - γ)4N2 +	N(1-Y )4(λmm)2 + (1 - Y )2 N + +	U1 - Y)J	,
which gives 0(
C2,σ ∣s∣(∣A∣∨∣B∣)
e2(1-γ)8(λminλminλmin)2
) sample complexity.
We used Remark C.1 to bound Cμ,σ for the result in the main text.
Our theoretical results here bring together ideas from single agent NPG analysis of Lan (2021)
and stochastic primal-dual optimization techniques from Malitsky & Tam (2020); Nemirovski et al.
(2009). In particular, we will be using ideas from Malitsky & Tam (2020); Nemirovski et al. (2009)
in the analysis we develop for extending ideas of Lan (2021) to the stage 1.
We first analyze the policy evaluation routine in Algorithm 6. In particular, we will bound the
variance and bias of θt+ι as an estimate of θ*,t(s, b) = Ea~χt(∙∣s)Qk-ι(s, a, b). AS this routine is
in an inner loop (indexed by n), the policies we sample, consequentlyFt is fixed, therefore we drop
the subscript. The proofs of these lemmas will be similar to Lan (2021), except the additional bias
We have due to VVk-ι.
Lemma 3.6. Let Assumption 1, 2 hold. Let βnθ
and 7,
λθ—(：+η)for no ≥ 1. Then, for Algorithms 6
EkθN - θ*,tk2 ≤ O (θ¾ + N (λmJ(1 - γ)2 + U EkVl- Vik∞).
Proof. Throughout this proof, EH will stand for conditional expectation EHxt]. By the definition
of θn,
kθn+1-θ*,tk2 = kθn -θ*,t k2 - 2βnhF(θn,ξn ),θn - θ*,Q + β2 k Fθ (θn,ξn)∣∣2∙
We will take expectation Eξn where ξn = (sn, an, bn, sn+1) is the sample at iteration n of Algo-
rithm 7
— ɪ , _ , _ , _ _ _______________ ʌ 、
EξnF(θn,ξn) = F(θn)+ γPχt,yt (Vk-1 - Vk-1),
as in (18) where Pxt,yt was also defined. As we stated, we omit the dependence of Ft to t as t is
fixed throughout this loop. Thus,
Eξn kθn+1 - θ*,tk2 = kθn - θ*,t k2 - 2βn hF(θn), θn - θ*,ti
-2βnYhPχt,yt (Vk-1 -匕-l),θn - θ*,ti + βn ∣∣F(θn,ξn)∣∣2.
32
Under review as a conference paper at ICLR 2022
We use strong monotonicity (with F(θ*,t) = 0) for the first inner product and CaUchy-SchWarz and
Young’s inequalities for the second inner product (exactly as in the proofs for policy evaluation with
single loop NAC) to get
Eξnkθn+1 - θ*,tk2 ≤ (1 - 2βnλmin) llθn - θ*,tk2 + j/ IIPxt,yt (Vk-I- Vk-I) k 2
min
+ βnλmmkθn-θ*,tk2+ βn EξnkF(θn,ξn)k2
=(1 - βnλmm) kθn - θ*,tk2 + β⅛PA⅛Vk-1 - Vk-lk∞ + βnEξnkF(θn,ξn)k2, (31)
λmin
where we estimated ||。方亡跖(Vk-I 一 Vk-ι)k2 as in LemmaE.2. We will use Lemma B.1 to upper
bound ∣∣F(θn,ξn)k2 ≤ (i-γ)2. We define Θn such that Θn(1- βnλma) ≤ Θn-ι with Θ0 = Θι =
1. We multiply both sides of the inequality with Θn after taking total expectation, to get
ΘnE∣θn+ι - θ*,t∣∣2 ≤ Θn-iE∣θn - θ*,t ∣2 + 8 q/*1 EkVfc-I- Vk-11∞ + -2^^ .
λmin	(1 - γ)
Summing the inequality gives
Θne∣Θn +1 -θ*,tk2 ≤ Θ0kθ1 -θ*,tk2 + XX θnβλγ21S11A1 EkVk-1 - Vk-ιk∞ + XX -2θnβn2
n=1	λmin	n=1 (1 - γ)
Using the definition of βn and setting Θn(1 - βnλmin) = Θn-ι gives Θn = Θι(n+n0)(n++ι-1
Let us use Θ0 = Θ1 = 1 and bounds from Lemma B.1 for ∣∣θ1 - θ*,t∣∣2,
e∣Θn - θ*,tk2 ≤
2no (no + 1)∣S∣∣A∣	__________________8N__________________
(1- γγ )2 (N + no)(N + no- 1) + (N + n°)(N + no - 1)(1 - γ)2(λmm)2
+
FEkVj- Vk-ιk∞.
□
We now analyze the bias for θt+ι in Algorithm 6. Let us remark that the bias analysis for Vk-I
in the next lemma is critical and it is the main reason that we get fresh estimates for Vk-ι in this
algorithm, in contrast to the stale estimation in the single loop NAC variant.
Lemma 3.7. Let βn
gorithm 7
λθm□⅛ whereno
6λ2
,max. Then, forAlgorithm 6 and its SubroutineAl-
λmin
kE[5]-≤ O (οjs‰+irn^EiVkTE]-VkTk∞).	(32)
RemarkF.1. Since the bias term in the algorithm's analysis will be (LemmaF.2) ∣∣E[Θn |xt] — θ*,tk,
we will have to take the square root of the result of this lemma. If Vk-I is estimated before Xt, then
we will have in the main analysis EkVk-I — Vk-Ik which will have the rate √N. On the other hand,
if we estimate Vk-I freshly as in Algorithm 6, then we will be able to use the better bias bound
kE[Vk-1 |xt] - Vk-1k = O(1/N) which seems to be enough for our bound.
Proof. We are going to take expectation of the recursion
_ _ _ ~ , _ 、
θn+1 = θn - βnF(θn, ξn),
first w.r.t. sample ξn, where ξn is as in the proof of Lemma 3.6
Eξnθn+1 = θn - βnEξnF(θn,ξn)
_ _ _ , _ . _ _	, ʌ	_ _ 、
=θn - BnF(θn) - βnYPxt,yt (Vk-I- Vk-1),
where we used (18) where Pxt ,yt was also defined.
33
Under review as a conference paper at ICLR 2022
We will now take expectation E[∙∣χt]. We note F and PXt,yt are linear
E[θn+1lxt] = E[θn lxt] - βnFt(EUxtD - βnγPxt,yt (E[Vk-1|xt] - Vk-1).
We denote Hn = E[θn∣xt] and S = γPχt,yt (E[Vk-ι∣xt] - Vk-ι) in the above equality which makes
the recursion θr+ι = Sn - βnF(Sn) - βnδ. We then have
kθn+1 - θ*,t∣∣2 = IlSn - θ*,t∣∣2 - 2βnhF(θn), Sn - θ*,ti - 2βnhδ, Sn - θ*,ti
3β2
+ 3βn kF (θn )k2 + 3βn kδk2, (33)
where we also used Young’s inequality to split the term βn2 IF(θSn) + δSI2.
By strong monotonicity and Lipschitzness of F along with F(θ*,t) = 0,
2βnhF(θn), Sn - θ*,ti = 2βnhF(Sn)- F(θ*,t), Sn - θ*,ti ≥ 2βnλθmiτ∣θn - θ*,tk2,
βnkF(θn)∣2=βnkF(Sn)- F(θ*,t)∣∣2 ≤ βnxmaxi® - θ*m∣2.
β λθ
By Cauchy-Schwarz and Young S inequalities, it follows that 2βnhδ, θn 一 θ?,/ ≤ P 2min ∣∣θn -
θ*,tk2 + 就-kSk2. Using these three inequalities in (33) gives
∣∣S	2 l∣2j(l 3 R ∖θ	I 3 oi2 ʌ 2 ʌ	∣∣S Q ∣∣2 ∣	2βn	IlSll 2 ∣ ozς2 IlSIl 2
kθn+1 -	θ*,t∣∣2	≤ 1-	- $ βnλmin	+ 2 βnλmax J	kθn	- θ*,t∣∣2 +	∣∣δ∣∣2 + MnlgM
We now use n0
(λθmaX2 and βn
(λmin)
λm-(⅛+n)toestimate
Therefore, the recursion is
∣∣S	2 Il 2 r /1 θ ʌθ ʌ IlS 2 Il 2 _i_	2βn	IlSll2 _i_	2∕^2∣∣S∣∣2
kθn+1 -	θ*,t∣∣2 ≤ (1	- βnλmin) ∣∣θn	- θ*,t∣∣2 +	Xq -	∣∣δ∣∣ +	3lβn Hδk2∙
min
This recursion is similar to (31), in particular, by noting βn ≤ ^~, and bounding kSk2 Simi-
min
ʌ	∙-	∖..C	.... ʌ	∙-	. . i-∖
lar to LemmaE2 kPxt,yt(E[Vk-i|xt] - Vk-1)∣∣2 ≤ IS||A|kPxt,yt(E[Vk-i|xt] - vk-ι)k∞ ≤
∣S∣∣A∣kE[V4-ι∣xt] - Vk-ιk∞
kθn+1 - θ*,t∣∣2 ≤ (1 - fnXmmin) Mn - θ*,t∣∣2 +
5βn∣S∣∣∣A∣
λmmin
kE[Vk-ι∣xt] - Vk-ιk∞.
We finally define Θn as in the end of the proof of Lemma 3.6, in particular, Θn(1-βnλmin) = Θn-1
gives Θn = Θι 5+:0)^-10T), where Θo = Θι = 1. We multiply both sides of the inequality
with Θn and sum to get the result.	□
We now have to estimate the bias and variance of the estimation of Vk-I in Algorithm 6, very similar
to Lan (2021). Unlike Lan (2021) that derived O(1/N 3) bound for the bias, we are going to derive
a O(1/N 2) bound which will be sufficient. Let us also note that the previous two lemmas had addi-
tional bias not present in Lan (2021), however the next result does not have this bias and therefore
the arguments in Lan (2021) would be enough. We provide a brief proof to be self-contained.
Let us recall that Vk-ι = V χk-1,yk-1 and by sampling Sn 〜PXkfyk-∖ o：〜xk-ι(∙∣Sn), bn 〜
yk-ι(∙∣Sn), Sn+1 〜P(∙∣Sn,a：,bn),theoracle
ω
F (ωn , ξn ) = e(sn ) (ωn (sn ) - r(sn , an , bn ) - γωn (sn+1 )) ,
satisfies EξnFω (ωn, ξn) = Fω (ωn), where Fω is defined as
Fkω-1(ω)(s) = ρxk-1,yk-1 (s)ω(s) - X xk-1(a|s)yk-1(b|s)r(s, a,b)
a,b
- γ X xk-1(a|s)yk-1(b|s)P (s0|s, a, b)ω(s0), (34)
s0,a,b
where Fkω-1(Vk-1) = 0 and also as before Fkω-1 is strongly monotone with λωmin. We will drop the
subscript of Fω since k is fixed in this loop.
34
Under review as a conference paper at ICLR 2022
2	6λ2
Lemma 3.8. Let Assumptionl, 2 hold and βn = λω~(：十册),With no = 川 χ2. The variance and
bias of Vk-1, computed as in Algorithm 1 satisfies
EkωN - VkTk2 ≤O ( (1 !SYAN2 + N(1 -；小),
kE[ωN |xt] - %-ik2 ≤O ((T-YAN2).
Proof. For the variance, we have by taking expectation w.r.t. ξn
Eξnkωn+1 - VkTk2 = kωn - Vk-1k2 - 2βnhEξn[Fω (ωn, ξn)],ωn - Vk-Ii + β2 Eξn kFω (ωn,ξn)∣∣2.
By Eξn F ω(ωn, ξn) = F ω(ωn), F ω(Vk-1) = 0, and strong monotonicity of F ω, similar to our
previous proofs for policy evaluation,
Ekωn+1 - Vk-ik2 = (1 - 2βnλmm)E∣∣ωn - Vk-1 k2 + βnEkFω(ωn,ξn)k2.
The end of the proof is the same as Lemma 3.6, except that we do not have here the additional bias
term in Lemma 3.6. Therefore, the result follows.
For the bias, we will argue as in Lemma 3.7. Taking expectation of the recursion w.r.t. ξn gives
Eξn ωn+1 = ωn - βn F (ωn ).
We now unroll the expectation until xt and use linearity of F ω
E[ωn+1∣Xt] = E[ωn∣Xt] — βnFω (E[ωn∣xt]).
Denoting ωn = E[ωn∣xt] gives the recursion cDn+ι = ωn 一 βnFω@n), and therefore
kωn+1 - Vk-1k2 = kωn - Vk-1k2 - 2βnhFω-i(ωn),ωn - Vk-Ii + βn2 k『(ωn)∣∣2∙
We will now use Lipschitzness and strong monotonicity of Fω and that Fω(Vk-1) = 0 and similar
to Lemma 3.7, we obtain the recursion
kωn+1 - Vk-1k2 = (1 - 2βnλmin + βn2\Lax) kωn - Vk-1k2.
By the choice of n0 and βn, similar to Lemma 3.7, it holds that 2βnλmin - βn2 λ2max ≥ βnλmin. By
defining Θn the same way as Lemma 3.7 and summing the inequality gives the result.	□
Now we analyze the outer algorithm for solving the matrix game in stage 1. The algorithm is based
on FoRB from Malitsky & Tam (2020). The choice of this algorithm is due to its simple update
with one projection and one oracle computation. We note that the existing analyses for stochastic
versions of this algorithm are not suitable for us. In particular, in the stochastic variant in Malitsky &
Tam (2020), deterministic oracle is also computed at each iteration. On the other hand, the analysis
in Bohm et al. (2020) uses unbiased oracles with bounded variance and a decreasing step size. In
our case, we will have biased samples and we will use inner loops to decrease bias and variance of
this oracle. Therefore, we need to develop an analysis with constant step size and that characterizes
the bias and variance explicitly.
Similar to Malitsky & Tam (2020), let us define the “Lyapunov-like” function
φs+ι = D(χ(∙∣s),χt+ι(∙∣s)) + ηhθ*,t+ι(s, ∙) - θt+ι(s, ∙),χ(∙∣s) - χt+ι(∙∣s)i
+ 2 D(χt+ι(∙∣s),χt(∙∣s)). (35)
We call this “Lyapunov-like” since it is not non-decreasing. Moreover, unlike Malitsky & Tam
(2020), Φt is not necessarily nonnegative. However, it is sufficient for our purposes as it is bounded.
Note that we will also use the following error functions
eι,t = ηhθt+ι(∙∣s) -E[θt+ι(∙∣s)∣χt],χ(∙∣s) -xt(∙∣s)i
e2,t = ηhθy+ι(∙∣s) - E[θy+ι(∙∣s)∣yt],yt(∙∣s) - y(∙∣s)i.
35
Under review as a conference paper at ICLR 2022
Lemma F.2. [See Lemma 3.4] Let Assumption 1, 2 hold. Denote Xout = T PT=I Xt and yout
T PPt=ι yt and let η = ɪɪɪ
EEs”
max XoUtQSyS - xsQsyout
xs ,ys
Φ0 - ΦT
ηT
+ o (T XEIIE[θt+"xt] - θ*,t∣∣
∖ t=1
O
∕1 T 、 I T
+ O TEηE∣θt+ι - θ*,tk2 + E∣θt - θ*,t-ik2 + T~EEs~σ max]T[e1,t + e2,t]).
∖ t=1	)	t=1
Remark F.3. When D is KL divergence, we have maxy1,y2 D(y1,y2) ≤ log |B| and equivalently
for the other player. Therefore, we have the bound
Φ0 - ΦT + Φ0,y - ΦT,y ≤ O (log |A| + log |B| + ɪɪ
Corollary F.4. We use Lemmas 3.6, 3.7 and F.6,
o (t(⅛ )+o
+ O
VWi q VWiUWg	, 1 v ll
(1 - Y)N +	ξF— ||E[Vk-i|xt] - Vk-ι∣∣∞
’∣s I∣a∣	+	1	+ ∣s I∣a∣'
∖(1-Y)2N2 + N(λmin)2(1 - Y)2 + (λmin)2
1
E∣∣%-ι - Vk-ι∣∣∞
We add the bound from Lemma 3.8
+O
+O
∣s I∣a∣
(1 - γ)2N2
√∣s∣W i √m J√mB
-Y)N
(1 - Y)N
1
(1 — Y)N
+ N(λmin)2(1 - Y)2 +
|S I∣a∣
∣s∣∣a∣
(ʌmin)2 L(1 - Y)2N
2 + n(1-y )2(λmm)21 J.
〜
O
+
1
We now refine the bound by only including the dominant terms
O	)+O (「N
+O
∣s I2∣a∣2
∣s I∣a∣
N 2(1 - γ )2 (λ^in)2 + (λmin)2(λmin)2(1-γ)2N
Remark F.5. ^y Lemma 3.6 and Lemma 3.7, the second and third term will bring
O (N + E∣∣E[%-ι∣xt] — Vk-Ik). We will see in the next lemma how to handle error terms e1,e2
1 ∙ιι	. 1 1	1 1 ∙ 1	1 - r τm 11 τm Γt'λ I 1 τ τ^ 11
and will use the bound derived earlier for E∣E[Vk-ι∣xt] — Vk-1∣.
Proof. By the update rule, it follows for all S and χ(∙∣s) ∈ ∆,
(VD(xt+ι(∙∣s),xt(∙∣s)) + η(2%+ι(s, ∙) - θt(s, ∙)),x(∙∣s) - xt+1 (∙∣s)) ≥ 0.
By three point identity,
D(x(∙∣s),xt+ι(∙∣s)) ≤ D(x(∙∣s),xt(∙∣s)) - D(xt+ι(∙∣s),xt(∙∣s))
+ η^2θt+ι(S) ∙) - θt(S, -)),x(-|s) - χt+ι(IS)〉. (36)
We now manipulate the inner product by adding and subtracting θ*,t+ι
ηh(2θt+1(s, ∙) - θt(S,，))，x(IS) - xt+1 (IS)〉= ηhθt+1 - θ*,t+1 (S, ∙b X(IS) - xt+1 (IS))
+ ηhθt+1(S, ∙) - θt(S, ∙) + %t+1(S, ∙),x(∙[s) - xt+1(・|S))
=ηhθt+ι(S, ∙) — θ*,t+ι(s,∙),x"s) — xt+i3S)〉+ ηhθt+ι(S, ∙) — θt(S, ∙),x(S,∙) — Xt(IS))
+ ηhθt+ι(S, ∙) - θt(S, ∙),xt(IS)- xt+i(・|s)〉+ ηhθ*,t+ι(S, ∙),x(1s) - xt+i(・|s)〉. (37)
36
Under review as a conference paper at ICLR 2022
The first two inner products in the final inequality will telescope if We can replace θt+ι with θ*,t in
the second one. For this we have to be careful with bias and variance. Let us take the second inner
product
ηhθt+ι(s, ∙) - θt(s, )x(Is) - Xt(Is)i = ηhθ*,t(SL) - θt(s, )χ(Is) - Xt(Is)i
+ηhθt+1(s, ∙) - θ*,t(s, B,x(∙Is) - Xt(Is)i.
Now in this estimation, we will add and subtract terms involving E[θt+ι(s, ∙)∣xt] to obtain
ηhθt+ι(s, ∙) - θt(s, ∙),χ(Is) - Xt(Is)i = ηhθ*,t(S, ∙) - θt(S, ∙),χ(Is) - Xt(Is)i
+ ηhE[θt+ι(s, ∙)∣Xt] - θ*,t,x(∙∣s) - Xt(∙∣s)i + ηhθt+ι(s, ∙) - E[θt+ι(s, ∙)∣Xt],X(∙∣s) - Xt(∙∣s)i
≤ ηhθ*,t(s, ∙) - θt(s, ∙),x(Is)- Xt(Is)i + 2ηkE[θt+ι|Xt] - θ*,t∣∣∞ + eι,t, (38)
where the inequality is due to Cauchy-Schwarz and we use the definition of e1,t for the last term.
Next, we use Cauchy-Schwarz and Young’s inequalities for the third inner product in RHS of (37)
to derive
ηhθt+ι(s, ∙) -%(1s),Xt(。Is)-勺+1卜国〉≤ η2∣∣θt+ι(s, ∙) -θt(Is)k∞ + 4kXt(Is)-Xt+i(Is)k2
≤ 4η2 [kθt+1(s, ∙) - θ*,t(S, ∙)k∞ + kθ*,t(s, ∙) - θ*,t-l(s, ∙)∣∣∞ + kθ*,t-l(s, ∙) - θt(s, ∙)k∞]
+ 4kXt(Is)- Xt+i(Is)k2.	(39)
As θ*,t(s, a) = Eb〜yt(∙∣s)Q(s, a, b), we have
kθ*,t(s, ∙) - θ*,t-i(s, ∙)k∞ ≤ max ∣Q(s, a, b)∣kyt(∙∣s) - yt-i(∙∣s)kι
2
≤ ι-^kyt(Is) - yt-ι(Is)kι,	(4O)
where the second inequality is by Lemma B.1 and the first by Jensen. We join (38), (39), and (40)
in (37)
ηh(2θt+1(s, ∙) - θt(s, ∙)),x(Is) - Xt+1 (Is)i ≤ ηhθt+1(s, ∙) - θ*,t+l(s, ∙),x(Is) - Xt+1 (Is)i
ηhθ*,t(s, ∙) - θt(s, ∙),x(∙Is)- Xt(Is)i + 2ηkE[θt+ι|Xt] - θ*,tll∞ + eι,t
+ 4η2 [kθt+1 - θ*,tk∞ + kθ*,t-1 - θtk∞] + (1 _；)2 kyt(Is) - yt-1 (Is) k2
+ 4 kXt (Is)- Xt+1 (Is) k2 + ηhθ*,t+1 (S, ∙),x(Is)- Xt+1(∙|s)i. (41)
We note that by Equation (8), 4 ∣∣Xt(∙∣s) - Xt+ι(∙∣s)k2 ≤ 2D(Xt+ι(∙∣s),Xt(∙∣s)) and similarly for
the term involving difference of yt and yt-1.
We insert (41) into (36) by using the definition of Φt
ηhθ*,t+l(s, ∙), Xt+1(,Is)- /(ls)i + φs+1 ≤ φs + e1,t + 2nkE[0t+i|Xt] - θ*,tk∞
+ 4η2 [kθt+1 - θ*,tk∞ + kθ*,t-1 - θtk∞]
+ (132⅛ D(yt(∙Is),yt-ι (∙Is)) - 2 D(Xt(∙∣s),Xt-ι(∙∣s)).
We sum this inequality and use the definition of θ*,t+ι to obtain
T-1	s s	T-1
T X hEb〜yt+ι(∙∣S)Q(S,	∙, b),Xt+1(Is)	- X(Is)i	≤	0	T T	+ T X e1,t
t=0	t=0
1 T-1	4 2 T
+ T X 2nkE[%+1|Xt] - θ*,t∣∣∞ + -T~ X [kθt+1 - θ*,tk∞ + kθ*,t-1 - θtk∞]
t=0	t=1
+ TXπ32⅛D(yt(∙∣s),yt-1 (∙∣s)) - 1 D(Xt(∙∣s),Xt-ι(∙∣s)).
T t=0 (1 - γ)	2
37
Under review as a conference paper at ICLR 2022
We have to estimate the error terms in the last line. The terms in the second line will be the bias
and variance arising from using θt+1 instead of the true oracle. First, by the symmetric estima-
tion on the y player, we can obtain the similar inequality. For making the comparison, we will
denote the corresponding oracle as θy (θ in the previous estimations correspond to θx). In particular
θy,t+ι(s, b) = Ea〜xt+ι(∙∣s)Q(s, a, b), and the corresponding Lyapunov-like function as ΦS,y
T-1
T X hEa〜Xt+ι(∙∣S)Q(S, a, )y(Is)- yt+1 (Is)i ≤
T t=0
T
1 T-1
+ τ X
t=0
e2,t
1 T-1	4 2 T
+ T X 2ηkE[θy+ιE] - θy,tk∞ + 4T X [kθy+ι - θy,tk∞ + key- - θy k∞]
t=0	t=1
+ T X 7132⅛D(χt(∙∣s),χt-ι(∙∣s)) - 1 D(yt(∙∣s),yt-ι(∙∣s)).
T t=0 (1 - γ)	2
After summing up the two inequalities and recalling that we bound the RHS of (24), we pick η ≤
1-γ to cancel the last terms in the last lines of the estimations. Since We estimate θt and θ? in the
same way, their bounds as we derived in Lemma 3.7, Lemma 3.6 will be the same, therefore in the
bound We do not include both and simply put them under big-Oh. Next, We take maximum over x, y,
take expectation W.r.t. state distribution σ and total expectation W.r.t. randomness in the algorithm
and use the definitions of x°ut and yout to conclude the result.	□
For the error terms e1,t, e2,t, We Will use the technique to change the order of maximum and expec-
tation from the literature of stochastic primal-dual methods (Nemirovski et al., 2009, Lemma 3.1,
Lemma 6.1). Let us recall their definitions:
eι,t = ηhθt+ι(∙ls) - E[θt+ι(∙ls)lxt],χ(∙ls) - χt(∙ls)i
e2,t = ηhθy+ι(∙ls) -因修+人⑸加沙小⑸-y(Js)i
We Will derive the bound or e1,t and the bound for e2,t is symmetrical.
Lemma F.6. We have
TT
亍EEs〜σ maxX eι,t ≤ -ɪ-------+ 齐 X4η2Ekθt+ι - θ*,tk∞∙
T	x	TT
t=1	t=1
Proof. First note that (θt+ι(∙∣s) - E[θt+ι(∙∣s)∣χt], xt(∙∣s)i does not depend on X and by the tower
property of conditional expectation,
T
XEEs”ηhθt+ι(∙∣s) - E[θt+ι(∙∣s)∣χt],χt(∙∣s)i
t=1
=EEs〜σηhE[θt+ι(∙∣s)∣χt] - E[θt+ι(∙∣s)∣xt],xt(∙∣s)i = 0.
Therefore, we have to estimate
T
EEs” maxXηhθt+ι(∙∣s) - E[θt+ι(∙∣s)∣xt],x(∙∣s)i∙
x
t=1
Let nt(s, ∙) = -η(θt+ι(∙∣s) - E[θt+ι(∙∣s)∣xt]). First, we note that E[nt(s, ∙)∣xt] = 0. Next, we
define the auxiliary “ghost” process
Xt+ι(∙∣s) = argminhnt(s, ∙),x(∙∣s)i + D(x(∙∣s),Xt(∙∣s)).
x
Note that Xt and Xt depend on the same randomness by definition of Xt, therefore conditioned on
xt, Xt is deterministic. Standard mirror descent analysis gives for any X
hnt(S, )X(Is)i ≤ D(X(Is),Xt(・Is))- D(X(Is),还+心Is)) + hnt(s, ・)，Xt(-Is)i + ∣∣nt(∙Is)Il2.
38
Under review as a conference paper at ICLR 2022
We sum the inequality take maximum and then expectation
TT
EEs~σ maxS^h-nt(s, ∙),x(∙∣s)i ≤ Es~σD(x(∙∣s),Xι(∙∣s)) + EEEs~σh-nt(s, ∙),Xt(∙∣s)i
x t=1	t=1
T
+ X EEs~ρknt(∙∣s)k∞.
t=1
By the tower property and that Xt is deterministic conditioned on Xt, We have
PT=IEhnt(s, ∙),Xt(∙∣s)i = Pt=IEhE[nt(s, ∙)∣Xt],Xt(∙∣s)i = 0.
Recall the definition of nt and use Young’s inequality with Jensen’s inequality to get
Eknt(s, ∙)k2 = Eη2kθt+ι(∙∣s) - E[θt+ι(∙∣s)∣xt]k∞
≤ 2Eη2kθt+ι(∙∣s) - θ*,tk∞ + 2Eη2kθ*,t - E[θt+ι(•⑸山]k∞
≤ 4Eη2k%+ι(∙∣s)-θ*,tk∞.
□
F.2 Proofs for stage 2 of Reflected NAC with a game etiquette
Similar to single loop NAC variant, this part mirror closely the analyses for single agent setting, as
the best response step is like a single agent problem where the other agent (fixed) can be seen as part
of the environment. Therefore, the development in this part will be similar to Lan (2021). Let us
restate that the main concern in this part was to make sure that yjt updates do not require seeing the
policy Xk or the actions of X-player. As we showed that it is the case (in Appendix E.2.1), we will
only provide the proofs here, with mostly using the arguments of Lan (2021). Therefore, the proofs
in this part are included for being self-contained and for easy navigation. Therefore, they will be
brief.
First, we will prove the bias and variance of the estimate νt, similar to Lemma 3.8. Let us recall the
Bellman operator for the oracle v?,t = Ea~χk(∙∣s)Qxk,yt (∙, a, ∙) that the update is using:
ν*,t(s,b) = £xk(a|s)r(s,a,b) + YE P(s0∣s,a,b)xk(a|s)yt(b0|s0)v?,t(s0,b0)
a	s0,a,b0
We use the sampling matrix (as (Lan, 2021, Sec. 5.2)) diag(ρxk,yt) 0 diag(yt) and define the
operator
FtV(Vt)(s, b) = Pxk,ytyt(b∣s) [νt(s, b) - X Xk(a∣s)r(s, a, b)
a
-Y X Xk(a∣s)P(s0∣s,a,b)yt(b0∣s0)νt(s0,b0)i, (42)
s0,a,b0
such that Ft"(v?,t) = 0. Strong monotonicity of Ft follows from Assumption 1, 2 and that the
operator TV (s, b) = Pa Xk (a∣s)r(s, a, b)+ Y Ps，,” Xk (a∣s)P(s0∣s, a, b)yt(b0∣s0)ν(b0, s0) being Y
contraction in '∞ norm (Bauschke et al., 2011, Example 22.6 and 20.7). We define the stochastic
operator after sampling St ~ Pxk,yt, at ~ Xk(∙∣st), b ~ yt(∙∣st), st+ι ~ P(∙∣st,at,bt), bt+ι ~
yt("st+l)
ν
Ftν(Vt,ξt) = e(st, bt) (Vt(st, bt) - r(st, at, bt) - YVt(st+1, bt+1)) ,
and as we assume we can sample St 〜PXkyt, EξJFν(νt,ξt)] = Ft(%). In particular, as long as
st,at,bt, st+1 are estimated in the prescribed way, there is no need for yjt update to see the actions
or policy ofXk for Ftν(Vt, ξt) to be unbiased estimate of Ftν(Vt).
We note that unlike the NAC case, we are having an inner loop to estimate V?,t. At the point of view
of this loop (runs from n = 0, ∙∙∙ ,N - 1), v?,t is fixed.
Lemma F.7. Let Assumption 1, 2 hold and βn = λν^~(：十册)stage 2 in Algorithm 6 satisfies
EkVN - ν*,tk2 ≤ O ((i ∣SYBN2 + N(i-γ)2(λmm)2),
39
Under review as a conference paper at ICLR 2022
kE[νN∣yt]-ν*,tk2 ≤OBN2).
Proof. For the variance, we have by taking expectation w.r.t. ξn = (sn, an, bn, sn+1 , bn+1)
Eξn kνn+1 - ν*,tk2 = IlVn - ν*,tk2 - 2βnhEξn[FV(Vn, ξn儿 Vn - ν*,ti + β2Eξn kF^t (Vn, ξn)l2∙
By EξnFtV(Vn, ξn) = Ft(Vn), Ft(ν*,t) = 0, and strong monotonicity of FV,
E∣Vn+1 - V*,tk2 = (1 - 2βnλmin)E∣Vn — V*,t∣∣2 + βnlEkFtV(Vn,ξn)∣2.
The end of the proof is the same as Lemma 3.6, except that we do not have here the additional bias
term in Lemma 3.6. Therefore, the result follows.
For the bias, we will argue as in Lemma 3.7. Taking expectation of the recursion w.r.t. ξn gives
Eξn Vn+1 = Vn - βn Ft (Vn ).
We now unroll the expectation until yt and use linearity of FtV
E[Vn+1∣yt] = E[Vn∣yt] — βnFν (E[Vn∣yt]).
Denoting Vn = E[νn∣yt] gives
kνn+1 - ν*,t∣∣2 = ∣% - ν*,t∣∣2 - 2丛(FtV (Vn), Vn - ν*,ti + βn2 IlFtV (νn)k2.
We will now use Lipschitzness and strong monotonicity of FtV and that FtV (V?,t) = 0 and similar
to Lemma 3.7, we obtain the recursion
kνn+1 — ν*,t∣∣2 = (1 - 2βnλmin + β2\Lax) IlVn- ν*,t∣∣2.
By the choice of n0 and βn, similar to Lemma 3.7, it holds that 2βnλVmin - βn2 λ2max ≥ βnλVmin. By
defining Θn the same way as Lemma 3.7 and summing the inequality gives the result.	□
We will now give a proof similar to (Lan, 2021, Theorem 2), (Agarwal et al., 2020) regarding the
NPG algorithm for finding the best response.
Theorem F.8. Let Assumption 1, 2 hold and η > 0. For the stage 2 of Algorithm 6.
1T
T∑VXk,yk(so)—Vxk,yt(S0)≤ (ι-ηγ)TE
Es 〜dχk,y∕(yk(∙∣s),yι(∙∣s))-V xk,y1 (s)+v xk,yt+1(s)
η 1T	2	1T
+ 2(1 ――)2 T X EkV?,t - %+ιk∞ + ι -	T X EkE[Vt+i|yt] - V?,tk∞
γ	t=1	γ	t=1
Corollary F.9. We use the bound from Lemma F.7 to obtain
o (	1	) + o (	IS||B|	+________ι_______)+o P PS^
IT(1 - γ)27 + l(1-γ)4N2 + N(1-γ)4(λmm)2J +	1(1-γ)2N
(43)
Proof. By the update rule of yVt+1, it follows for any s, yV (Tseng, 2008, Property 1)
D(y(?|s),yt+i(ds)) ≤ D(yi?|s),yt(ds))-D(yt+i(ds),yt(1s))-hnVt+i(s, ∙),y(∙ls)-yt+ι(∙ls)i.
(44)
We manipulate the inner product
-nhVt+i(s, ∙),y(∙ls)-yt+ι(∙ls)i = -nhVt+i(s, •),y(∙|s)-yt(∙|s)i-nhVt+i(s, ∙),yt(∙ls)-yt+ι(∙ls)i
=-nhV?,t(s, ∙),y(1s) - yt3s)i - InhVt+ι(S, ∙),yt(Is)- yt+ι3s)i
-nh%+ι(s, ∙) - V*,t(s, ∙),y(1s) - yt(1s)i. (45)
40
Under review as a conference paper at ICLR 2022
By the performance difference lemma and using the definition of ν*,t = Ea〜XkQxk,yt (∙, a, ∙).
VXk,yt+1(so)- VXk岳(S0)= ɪES〜产,yt+1 hEa〜χk(∙∣s)Qxk岳(s,a, ∙),yt+1(∙∣s) - %(忖)
1 - Y S〜ctS0
=；	ES〜dxk,yt+ι hν*,t(s, ∙),yt+1(∙∣s) - yt(∙∣s)i
1 — Y S u,s0
=1 - YES〜dxk,yt+1 卜νt+ι(S, ∙),yt+ι(∙∣s) - yM∙∣s)i
+ hν*,t(S, ∙) - νt+1(s, ∙),yt+1(∙∣s) - yt(∙∣s)i
1	η	2
≥ I	ES〜dxk,yt+ι d+1(S, ∙),yt+ι(∙∣s) - yt(∙∣s)i - ηl∣ν*,t(S, ∙) - %+ι(S, ∙)ll∞
1	— Y S u,s0	|_	2
-；kyt+i(IS) - yt(∙∣s)kι ,	(46)
2η
where the last step uses Cauchy-Schwarz and Young,s inequalities.
Plugging in y = y in (44) and using Equation (8) gives
-ηhνt+ι(S, ∙),yM∙∣s) - yt+ι(∙∣s) ≥ D⅛7t(∙∣s),yt+ι(∙∣S)) + D(Ot+ι(IS),yM∙∣s))
≥ll%+ι(∙∣s)- yt(∙∣s)∣∣2,
which implies that (νt+1(s, ∙),yt+ι(∙∣s) - %(∙∣s)) -薪∣∣yt+ι(∙∣s) - yt(∙∣s) ≥ 0.
Recall that dχ,yt+1(s) = (1 - Y) P∞=o YtPrxk,yt+1 (st = s∣so), therefore 1 - Y ≤ dχ⅛,yt+1(so) ≤
1. Using the two previous inequalities in (46) gives
%1(S, ∙)"(∙∣S)- yt(∙∣s)i ≤ Vxk,yt+1(S)- Vxk,yt(s)+ 2⅛帧+1(∙∣S)- yt(∙∣s)∣2
+ 2(1 - Y) "*,t - νt+ι∣∞∙
We use the final inequality, (45), and (8) in (44) to get
ηhν*,t(s, ∙),y(∙∣s) - %(∙∣s)i + D(y(∙∣s),yt+ι(∙∣S))- ηVxk,yt+1 (s) ≤ D(y(∙∣s),yt(∙∣S))
2
-ηVXk,yt(s) + x∩----v∣ν*,t - νt+ι∣∞ + ηhν*,t(s, ∙) - νt+ι(s, ∙),y(∙∣s) - yt(∙∣s)i∙ (47)
2(I - Y)
In view of the definition ν*,t = Ea〜Xk(∙∣S)QXk,yt (∙, a, ∙), performance difference lemma gives (1 -
,*,	,	__	—	、、	_ .	,,	,.	,.,.	.
Y)(VXk,yk (so) - VXk,yt (so)) = ES〜产,yk <ν*,t(s, ∙),yk(∙∣ s) - yt(∙∣s)i. Plugging in y =碇 in (47)
and taking E	%,y* of both sides give
S 〜ds0 k
η(1 - Y)(VXk ,yk (so) - VXk,yt (so))+ ES 〜dχr* [d(求(∙ ∣ s),%+ι(∙∣ s)) -ηVXk,yt+1 (s)]
2
≤ ES〜a** [D(yk(∙∣s), yt(∙∣ s)) - ηVXk,yt(s) + 品三||内-"ι∣∣∞
+ ηhν*,t(s, ∙) - νt+ι(s, ∙), yk(∙∣s) - yt(∙∣ s)i] ∙	(48)
Now we take expectation w.r.t. the randomness in the algorithm, use tower property, the fact that
conditioned on y, y^(∙ ∣s) - yt(∙∣ s) is deterministic, Cauchy-Schwarz inequality, y(∙∣ s) ∈ ∆ for any
*
y, s. After those steps, we note that d§k,yk does not depend on t and sum the inequality over t to get
the result.	□
41
Under review as a conference paper at ICLR 2022
Algorithm 8 Single loop NAC with a game etiquette and ζ-greedy exploration
Require: Vθ such that kV∕0 - Vx0,y01∣∞ ≤ E
for k = 1, 2, . . . do
Stage 1
for t = 0, 1, . . . , T - 1 do
Sample (st,at,b, st+ι), With policy pair Xt, yt observe st,bt,r(st, at,bt), st+ι
ʌ
O_.	ʌ	_，	. / O_. ,	.	,	_ 、	ʌ ,	. . ∖
%+1 = θt - βte(st, at) (θt (St, at) - r(St, at, bt) - YVk-I(St+1)))
^t+1 = θt - βte(* st,bt)(θχ(st,bt) - r(st,at,bt) - YVk-I(St+1)))
E+ι"s), yt+ι(∙ls)] = [P(χt(∙ls),ηθχ+ι(S,", P(yt(∙ls), -ηθθιy+ι(s, ∙))]
Output Xk = 1 PT=I xt.
Stage 2
for t = 0, 1, . . . , T - 1 do
Sample (St, at, bt, St+1,bt+1) with policy pair Xk,认,observe St, bt,r(St, at, bt), St+ι, bt+ι
ν^t+ι = Vt - βνe(St, bt) (Ot(St, bt) - r(St, at, bt) - TVt(St+1, bt+ι))
ωt+1 = ωt - βωe(St) (ωt(St) - Y(St, at, bt) - Yωt(st+l))
yt+i(lS)= P (yt(・|S), -ηνt+ι(S, ∙)
^ ^	- ʌ
Output yk = y^, Vk = ω^+ι, where t ∈ [T] is selected uniformly at random and Vk = VXk ,yk.
G Greedy exploration to remove Assumption 2
G.1 SINGLE LOOP NAC WITH A GAME ETIQUETTE AND ζ -GREEDY EXPLORATION
Remark G.1. In this section, we see how to avoid Assumption 2 as mentioned in the main text.
Essentially the idea is similar to Khodadadian et al. (2021b) and (Lan, 2021, Remark 1) in the
single agent case. We are going to use ζ greedy to avoid Assumption 2.
Let us define the modified policies with greedy exploration
xt(a|S) = (I - ζ)xt(a|S) + |Aa| , yt(blS) = (I - ζ)yt(blS) + 卷.
Now we are going to sample with the Xt, yt and the algorithm will read as Algorithm 8.
G.1.1	STAGE 1 OF SINGLE LOOP NAC WITH A GAME ETIQUETTE AND ζ-GREEDY
exploration (See Algorithm 8)
Let us recall the notation and introduce more notations. At iteration k, we solve
Qs(a, b) = Q(S, a, b) = r(S, a,b) + YXP(S0|S,a, b)Vxk-1,yk-1 (S0).
s0
The problem is to find Xout , yout such that
E max XosutQsys - XsQsyosut .
xs,ys
Now we define
Qs(a, b) = Q(s, a, b) = r(S, a, b) + γ X P(s0∣s, a, b)Vxk-1,yk-1 (s0),
s0
as we sample with greedy policies, at the stage 2 we will learn VXk-I ,yk-1. So we will have the
oracle Vk-I SUCh that k Vk-I - Vxk-1,yk-1 k is small.
In this step, with the abovementioned oracle, we expect to learn
Ct(S, a) = Eb〜ysQs(a, b),	θy,t(s, b) = Ea〜XsQs(a, b).
42
Under review as a conference paper at ICLR 2022
Let us continue with x-player and drop the superscript,
s
θ*,t(S, a)= Eb〜yt(∙∣s)Q Gb)
=X ysQs(a,b)
b
=X yt(b∣s)r(s, a, b) + Y X yt(b∣s)P(s0|s, a, b)Vxkτ,yk-1 (s0).
b	b,s0
Therefore the operator is
Ft(θt)(s,a) = P t,yt (s)xt(a∣s) (θt(s,a) - fyt(b∣s)r(s, a,b)
b
—Y ^Xi)t(b∣s)P(s0∣s, a, b)VXkT,yk-1 (s)) . (49)
s0,b
As Xt(∙∣s) ≥ 备 and yt(∙∣s) ≥ 曲 by definition, We have that Ft is strongly monotone with P备
where P is the lower bound given in Assumption 1, which holds when the induced Markov chain is
aperiodic and irreducible. Let US call this *%由=ρ高.
We state the main result which follows by the results we will prove afterwards.
Theorem G.2. Let Assumption 1 hold. By combining Lemma G.11, Lemma G.4 and their corre-
SPonding corollaries, we can show O(E-7) sample Complexityfor Algorithm 8.
Proof. Insert Corollary G.6 and lemma G.11 into (14) to get the result.	□
Remark G.3. In single agent setting, Khodadadian et al. (2021b) obtains O(E-6) with greedy
exploration, to avoid Assumption 2. Our estimate is an E factor away from this rate.
Below, we analyze the actor in the stage 1 of Algorithm 8.
Lemma G.4. Let Assumption 1 hold and η = 伯+1)6/7, Z = ψ1∕y. Then, for the stage 1 of Algo-
rithm 8
TT
Emax T XhEb〜ys Q(s, ∙,b),xs - Xsi ≤ OTnl + TnT + 2T X Ekθs+1
,4ξ∣B∣ , 8γξ(∣A∣∨∣B∣) l
+ (1-Y)+	(1 - Y )2	+
ʌ _
—^i,tk∞
P= nt
2T(1 - γ)2 .
Corollary G.5. By using ηt , ζ, we get the bound
O ((1∣-∣Y∨2∣T1∕7+t6/7+T X kθt+1 - ”,t k∞!.
Corollary G.6. We can plug in the bound of E∣∣θt+ι — θ*,t∣∣2 (see Corollary G.8) and use the same
bound for the other player to get
EmaxXoutQsys-XsQsyout ≤ O (( |A| ∨2lB1∕7 + ^SP + T2∕7SF1 )2 + ^I~~J +
xs,ys	(1 - Y)2T1/7	T 1.5/7	T2/7(1 - Y)2	T1/7(1 - Y)
O(√∣S∣∣A∣T1/7)EkVk-I- V"k-1,yk-1 k∞.	(50)
Plugging in the bound for the final term from Lemma G.10
E	Qs s_ sQs	≤O( ∣A∣∨∣B∣	+ PSPi +	|S||A|	+	1 ʌ +
m,ys xoutQ y	XQyout≤	((i-γ)2τI/7	+	T 1.5/7	+	T2∕7(1-	γ)2	+ T 1∕7(1- γ))	+
O ((1⅛⅜! , (51)
which gives the O(E-7) complexity for stage 1.
43
Under review as a conference paper at ICLR 2022
ProofofLemma G.4. Let Us denote Xs = xt(∙∣s) and similarly for variables y,Xt,yt, θ. Let Us
recall (24) with our new notations
1T	1T
XoutQsys -XsQsyout = T EhEa~χsQG a, ∙),ysi- T EhEb~ysQls一 b), Xsi
t=1	t=1
=T X [hEa~xs Q(s, a, ∙),ys - ysi -hEb~ys Q(s, ∙,b),xs - xsi] .	(52)
t=1
5 TI	.	.	.< ∙	.	. 1	1	.1	.	∙	∙ ʌ
We have to convert this to the game when the matrix is Q
[
XoutQsys -XsQsySut = T X [hEa~χsQSa ∙), ys -ys - hEb~ysQs(∙, b), Xs - χsi]
t=1
1T	s	s	s s	s	s	s s
+ T J2hEa~xs	[Q	(a, ∙) - Q	(a, ∙)],y	-	yt i	-	hEb-ys	[Q	(∙, b)-Q	(a,	•)], X	- Xti.
t=1
For the error terms note
. - 一. ʌ „ , . , 一 一. .. „ , ʌ „ , ,,..
hEa~xs [Q (a, ∙) - Q (a, ∙)],y - yt i ≤ 2kEa~χs [Q (a, ∙) - Q (a, ∙)]∣∣∞
≤ 2∣∣Qs- Qsk∞
≤ 2γmax | P(s0∣s, a, b)(Vxk-1,yk-1 (s0) — V^τ,yk-1 (s
a,b
s0
≤ 2γkVxkτ,yk-1 - Vafck-1,ykτk∞
0))|
≤ 4yZ(∣a∣∨∣b∣)
≤	(1-γ )2
(53)
where the last step is dUe to the Lipschitzness of the valUe fUnction dUe to performance difference
lemma and that the policies Xk-1,yk-1 and Xk-1,yk-1 differ at most by Z.
With the similar estimation for the other error term, we have
i
XoutQsys - XsQsyout = T X [hEa~χsQs(a, ∙),ys - y；i -hEb~ysQs(∙,b),Xs - Xsi]
t=1
,8Z(∣a∣∨∣b∣)
+ (i-γ )2
(54)
Let Us denote
-一 一 <∙ „ ,
θ?,t = Eb~ys Q s(∙,b).	(55)
Note that this notation is not consistent with previous sections. Here by θ*,t We mean the quantity
we need to Upper boUnd (54).
By the update rule of Xt , for any Xs
. , , -. . ʌ _ _ _
D(Xs,Xs) ≤ D(xs, Xs+ι) - D(Xs+ι, Xs) + ηthθs+ι,Xs - Xs+ιi.
Let us estimate the inner product
.ʌ _ _ _ . ʌ _ _ _. . ʌ _ _ _
ηthθs+ι,χs - χs+ιi = ηthθs+ι,χs - χsi + ηthθs+ι,χs - χs+ιi
. _ _ -. . ʌ _ _ _ - . . ʌ _ _ _
=ηthθj,t, χs - χsi + ηth^s+ι - e?,t, χs - χsi + ηthθs+ι,χs - χs+ιi
A	.11 F	.	11	1	♦	.11 i' .Λ	∙	1	.	.1	1
AS we will be actually learning θj,t, we will further manipulate the inner product
.ʌ _ _ _ . _ _.	. ʌ _	ʌ _ _ _. . ʌ _ _ _ _.
ηthθs+ι,χs - χs+ιi = ηthθ=,t,χs -Xsi + n^+i - θs,t,χs -Xsi + 限号广隆,"-Xsi
+ ηt hθt+1, Xt - Xt+1i.
44
Under review as a conference paper at ICLR 2022
Let us estimate the third inner product on RHS
.ʌ _ _ _ _ . .. ʌ _ _
ηthθS,t - θS,t,^s -χSi ≤ 2ntke?,t—?,th
=2% max | X(yt(b∣s) - Ot(b∣s))Qs(a, b)|
a
b
≤ ι4ηt- X [yt(bls) - yt(MS)I
γb
= 14-tγ XIZ (1/|B|- yt(b∣s)) I
≤ 4ηt∣B∣Z
—1 - Y
We use this estimation and Cauchy-Schwarz and Young’s inequalities to derive
(56)
ηthθS+ι,χs - χS+ιi ≤ ηthθS,t,χs - Xsi + 2ηtk/+ι - θ"∣∞ + 4^
+η ∣∣θt+ιk∞+2 kxs - χs+ιk2.
We use this estimate in the main inequality with strong convexity of Bregman distance to get
ηthθS,t,χS -XSi + D(χs,χS) ≤ D(χs,χS+ι) + 2ηt∣θS+ι -出,tk∞ + 4⅛F + 2(1 η2γ)2.
We divide both sides by ηt to get
1	1	4ζ IB I	ηt
hθh,xS - xsi	+ -D(xs,xS)	≤ -D(xs,xS+ι) +	2k^S+1	- θi,tk∞ +	二	+ 2(1 - ")2 .
ηt	ηt	γ	γ
We sum this inequality and use (55) to get
Tt
1	X λ/e	^sS ( b、 S	s∖ V log |A| I log |A| I 2 1 X λ IMS	θs II
T 2^hEb〜ysQ (∙, b),xt - X i ≤ Tm	+ TnT + 2T 匚 kθt+ι - θ*,tH∞
+ 4ζ |B| + PT=I nt
+ (1-γ) +2T(1-γ)2 .
□
We next analyze the critic in stage 1 of Algorithm 8.
Lemma G.7. LetAssumPtionI hold and n = e+1”々,Z = T1/7, βt =(—)；^θ— ∙ Then, for the
critic of stage 1 of Algorithm 8	mn
T
TX Ei®-加』2 ≤o( yy)+ο
ISI2IAI2
T4/7(1 - γ)4
+O
1
+ 8SAikVk-I- Vfik-1,yk-1 k∞.	(57)
Corollary G.8. Recall that Z = T177 and λmin = ρZ∕∖A∖,
T
TXEkθt-G*,t-ιk2 ≤O(T/Ar) + O
ISI2IAI2
T4/7(1 - γ)4
+ O (T2/7(1-γ)2
..... ctr.；	^	^	..c
+ 8∖S ∖∖A∖T 2A7kV⅛-ι - V "I，"-1 k∞.	(58)
45
Under review as a conference paper at ICLR 2022
>Λ	Z- Z- r	CfA	1 ∙	∙ .) ʌ ʌ	1 1	♦	%/	∖	ττn	ʌ e / τ ∖
ProofofLemma G.7. As We are sampling With xt, yt and learning θ*,t(s, a) = Eb〜ysQs(a, b), We
have that (see (49))
ʌ , O 、
Ft(θ*,t) = 0.
We also denote
ʌ
π/ n 尸、	/	\/八 /	∖	/	ι ∖	τ λ ʌ
F (θt,ξt) = e(st, at)(θt(st, at) - r(st, at, bt) - γVk-1),
where st,at,bt, st+ι are sampled according to Xt,%.
Let us derive
..ʌ	ʌ ..c	.. ʌ	ʌ ..c	. ~ ʌ	ʌ	ʌ .	rɪ .. ~ ʌ	...c
kθt+ι - ^*,tk2 = kθt - θ*,tk2 - 2βthF(θt,ξt),θt - θ*,ti + β2kF(θt,ξt)k2.
一 ..一	一	ʌ ʌ	一 _	~ , O	ʌ , O .	- ,ʌ	__ ʌ ʌ .
Now, by i.i.d. sampling St 〜ρ人,yt, we have Eξt Ft(θt,ξt) = Ft(θt)+ YPxt缶(Vk-ι-V^k-1,yk-1)
as in (18). We have by strong monotonicity and Ft(θ*,t) = 0,
.~ ʌ ʌ ʌ . ʌ ʌ . ʌ ʌ . ʌ ʌ . ʌ ʌ . ʌ ʌ
2βt Ea (FR, ξt), θt- θ*,ti = 2βthF(θt),θt - θ?,/ = 2βthF(θt) - Ft(θ*,jθt - θ*Q
≥ 2βtλminkθt - θ*,tk2∙
The recursion becomes
Eξtkθt+ι- θ*,t∣ι2 ≤ (1- 2风猖山)kθt- θ*,t∣∣2
, ʌ	ʌ	ʌ	C	. . ~ ʌ	...C
-2γβt hP^t,yt (Vk-I- Vxkτ,ykτ ),θt - θ*,ti + β2Eξt kF(θt, ξt) k2.
By Cauchy-Schwarz and Young’s inequalities for the inner product
ʌ . ʌ .
. ,ʌ ʌ ʌ ʌ ʌ .. , ʌ ʌ ʌ .............................................ʌ ʌ
-2γβt(P^t,yt(Vk-1 - Vxk-1,yk-1 ),θt - θ*,ti ≤ 2γβtkP^t,yt(Vk-1 - Vxk-1 ,yk-1 )k2kθt - θ*,tk2
≤ βtλminkθt - θ*,tk2 + βt8γθlS||A| 位-1 - vxkτ,ykτk∞.
2	λmin
The recursion then becomes
Eξtkθt+ι - θ*,tll2 ≤ (1- 3风工山/2) kθt - θ*,tll2
βt8γ2∣S∣∣A∣
+
^L
..ʌ	ʌ	ʌ	..C	C	. . ~ ʌ	...C
kVk-1 - Vxkτ,ykτk∞ + β2EξtkF(θt,ξt)k2.
By Young’s inequality,
Eξt kθt+1 - θ*,tk2 ≤ (1 - βtλmin) ∣∣θ*t - θ*,t-1k2 + —ɪθ — kθ*,t - θ*,t-1k2
βtλθmin
βt8γ2∣S∣∣A∣
第in
+
..ʌ	ʌ	ʌ	..C	C	. . ~ ʌ	...C
∣V4-1 - Vxk-1,ykτ∣∣∞ + β2Eξt∣∣F(θt,ξt)k2.
Now we have to bound ∣θ*,t - θ*,t-ι∣∣2:
kθ*,t - θ*,t-1∣∣2 ≤，|S∣lAlkθ*,t - θ*,t-1k∞
=VZiSnAj^max |Eb〜ysQ (a, b) - Eb〜ys_ Q (a, b)|
a,s
≤ 2 vW[
_	1 - Y
max Ilys - ys-1k1
s
≤ 2 vWi
_	1 - Y
max ∣yts -yts-1∣1,
s
where we used % = (1-Z)yt + Z∕∣B∣. To show that maxs ∣∣yf -yS_1k1 is small, we use LemmaB.5
and Lemma B.1 and have the final bound for this quantity
kθ*,t- ^*,t-ιk2 ≤ 4≡;η2.
46
Under review as a conference paper at ICLR 2022
By also bounding ∣∣F(θt,ξt)k2 by Lemma B.1 and taking total expectation, the main recursion
becomes
Ekθt+1 - θ*,tk2 ≤ (1 - βt*min)Ekθ*,t - θ*,t-lk2 + β2λθST-iγ1 + (12β⅛
+ 8β^lSMIEk0-1 - VM1,yk-1 k∞.
λmin
This inequality gives
TT
斤 X Ekθt - %t-1k2 ≤ 不 X ~^~^θ— (kθt - θ*,t-1k2 -k°t+1 - θ*,tk)
T t=1	T t=1 βtλθmin
1 X	24|s|2|A|2n2	+ ɪX	i28t
T=β2(λmm)2(i-γ)4	T=MT
+8SW1位T- v XkT ,yk-1k∞
(λmin)
(59)
Recall that η = 伯十；”/? and βt = 估+7)4/7^θ . For the summations, We have PT=I η2∕β2 =
O(T3/7(鸿由)2)), P：=1 βt = O(T3/7/鸿山).By using βt+ι ≤ βt, and denoting for simplicity
at = ∣∣θt - θ*,t-ι k2 so 0 ≤ at ≤ (SlIA2 simple estimation gives
T1	T1
Xβ (kθt- θ*,t-ι∣2- kθt+ι- θ*,t∣2) = Xβ (at- at+ι)
t=1 t	t=1 t
T
X
t=1
11
R at - β+1at+1 +
1	1
Rt+1	Rt
<X P 1 上 ISIIAI
≤ ⅛ (瓦at - β+1at+1 + LA
_ 1	1	JSIIAI X( 1	1、
=瓦 a1- bT+1 aτ+1 + Gf N (β+Γ - A)
<1“.	IS mi	_O I IS IIAIT4/7 *min!
≤ Ba1 +(1-Y)2βτ +1 =Ol (1-γ)2	)
Plugging these estimations into (59)
τ
TX E∣θt- G*,t-ι∣2 ≤O( yy)+O
ISI2IAI2
T4/7(1 - γ)4
+O
1
+ 8SA∙Mkτ- VM1,ykτk∞. (60)
(λmin)
□
G.1.2 STAGE 2 OF SINGLE LOOP NAC WITH A GAME ETIQUETTE AND ζ-GREEDY
exploration (See Algorithm 8)
We are now going to estimate k%-1 - V^τ,yk-11∞. First, we need the following lemma for
numerical sequences from Mokhtari et al. (2020).
Lemma G.9. (Lemma 19 in Mokhtari et al. (2020)) Let b ≥ 0, c > 1. Let φt be a sequence of real
numbers satisfying
cb
φt ≤ 1 —  --— φt-ι +  --^~,	(61)
1(t + to )ɑ)	1	(t + to)2α,
for some α ∈ [0, 1] and t0 ≥ 0. Then φt converges to zero with the rate
φt ≤
max(φo(to + 1)α, b∕(c - 1))
(t + to + 1)α
47
Under review as a conference paper at ICLR 2022
1	3	^
For the stage 2, let η = e+1”々 and βt = 2伯+3)4/7。and let Us analyze the critic for %-ι.
1	3
Lemma G.10. LetAssumPtionI hold. Let η = 伯十^尸片 and βt = 2。伯+7)4/7 and recall ω^+1 = Vk
and ω*,t = VXk,yt and that we take as output yk = y^ in Algorithm 8
EkVi - Lk ,yk k2 = T XX 园取+i- ω,tk2 ≤ O ((-)% 4/7).	(62)
Proof. Let Us develop similar recUrsion to previoUs lemma
l∣ωt+ι- ω*,tk2 = kωt- ω*,tk2- 2βthFω E, ξt), ωt- ω*,ti + β2kFtω (ωt, ξt )k2.
ɪʌ . 1 ∙	.	∙	^	^ι ^ .	1	TTTl T-lf.1 /人 A ∖	∙A∕" /人、 IA / ∖ ∙
By taking expectation, using that St 〜P k,yt, we have EξtF『(ω^t,ξt) = F『3) and Ft(ωt) is
strongly monotone With P as it only requires Assumption 1. We also have Ftω (ω*,t) = 0. The main
recursion becomes
Eξtkωt+ι- ω*,tk2 ≤ (1- 2βtρ) kωt- ω*,tk2 + β2EξtkFtω(ωt,ξt)k2.
By Young’s inequality for α > 0
(I-2风P) kωt - ω*,t∣∣2 ≤ (I-2风P)(I + 句|® - ω*,t-ik2 + (I-2仇P)(I+ 1/a)k",t-i — ω*,tk2.
Let us set α such that (1 - 2βtP)(1 + α) = (1 - βtP), which gives the choice α =1 ：；P , with the
1 2β tρ
requirement 1 - 2βtP > 0. Recall that since βt = 2p(t：7)4/7, we have 2βtP < 1 and t ≥ 0. Note
also that (1 - 2/右夕)(1 + 1∕a) ≤ ɪ with out choice of a.
一	βtP
The recursion now becomes
2
Ekωt+1 - ω*,t∣∣2 ≤ (1 - etP)Ekωt - ω*,t-1 k2 + β-PEkω*,t-1 - ω*,t∣∣2 + βt EkF7t (ωt, ξt)k2.
Let us estimate the second term on RHS
kω*,t-ι - ω*,t∣l2 ≤ ISllIω*,t-ι - ω*,t∣∣∞
∣S∣∣Vxk,yt -
V ^k ,y^t-ι
l∞
≤	4∣S∣
一 (1-γ)2
maχUys -ys-ιk∞,
s
where the last inequality is by Lemma B.3. To further upper bound this quantity, use the update
rule of yt, Lemma B.5 and Lemma B.1 to get for all S Hys - ys-1U1 ≤ ɪ-Y. We also use that
ys(b) = (1 - ζ)ys(b) + ζ∕∣BI
kω*,t-ι - ω*,tk2 ≤
2∣s∣ηt
(1 - γ)3
Therefore, in the main inequality we have (after also bounding the last term by Lemma B.1)
Ekωt+ι - ω*,t∣∣2 ≤ (1 - 8向 Ekωt - ω*,t-ιk2 + βs([ -γ)3 + (1 — Y)2.	(63)
Note that by the definitions of βt , ηt, we have
βP =——3~-.	ηt2	=—2^—,	β2	=-------9——-.
'也	2(t + 7)4/7,	βt 3(t + 7)8/7 ,	Pt 4P2(t + 7)8/7
Therefore, we have
Enωt+1-ω*,tk2 ≤ (1 - 2(t + 7)4/7 ) Enωt-ω*,tτn2 + 3(1 - γ)3(t + 7)4/7 +4P2(1 - γ)2(t + 7)8/7 .
48
Under review as a conference paper at ICLR 2022
Now, we use Lemma G.9 with
φt = Ekωt+1 - ω*,tk2
8|S|	18
b = maxU-Y3, 4ρ2n^F ) ≥0
c=2 >1
t0 = 7
α = 4/7,
gives
Ekωt+1 - ω*,tk2 ≤ O ((1 - γ)3P2(t + 7)4/7
Sum the inequality to get
1T
τ EEkωt+ι- ω*,tk2
t=1
1T
≤ T x O
t=1
∣s∣
(1 - γ)3ρ2(t + 7)4/7
1T
≤ T x O
t=1
∣S∣T 3/7
(I-
≤O
|S|

□
The accuracy for the stage 2 will follow a similar proof to single agent settings. We would like to
bound
Vxk,yk - vχk,yt
by sampling Xk(a) = (1 - Z)xk(a) + Z∕∣A∣ and yS(b) = (1 - Z)yS(b) + Z∕∣B∣.
Let us define the oracles and the operators
ν*,t(s,b) = Ea〜^kQxk,yt (s,a, b).
~	. ,	ʌ A , . ʌ , . .	.	......
Ft(Vt,ξt)(s,b) = P k,yt(S)yt(b|s)e(s,b)(Vt(s,b) -r(s,a,b) -γνt(s,b)),
where S 〜ρxk,yt, a 〜Xk, b 〜yf, s0 〜P(∙∣s,a,b), b0 〜yS0, and
Ft(Vt)(s, b) = Pxkyt (s)yt(b∣s)(Vt(s, b) - X Xk(a∣s)r(s, a, b)
a
-Y X P (s0∣s,a, b)Xk (a∣s)yt(b0∣s0)^t(s0,b0)),
s0,a,b0
and therefore Ft(^*,t) = 0 and Ft is strongly monotone With ρZ∕∣B∣ =入m皿
Lemma G.11. Let Assumption 1 hold. Let Us set η = 伯十；”/7, and Z = T1j7. Thenfor the actor
of the stage 2 in Algorithm 8,
ET X VXk屐(so) - Vxk,yt (so) ≤ O ((I-Y)T 1/7 ) + O (±EkVt+1 - ν*,tk∞)
+ O (Q¾) + O (T6/7(1 - Y)3 )
Corollary G.12. By using Lemma G.13, we get the OveraU rate O(1∕T1/7).
49
Under review as a conference paper at ICLR 2022
Proof. By the update rule of the algorithm,
D(ys,ys+1) ≤ Dm)- D(ys+ι,ys) - ηt(νf+1,ys - ys+ιi
=D(ys,ys) - D(y；+i，y；) - ",t,y - y；+J- η"- ν^t,ys - y；+)
We now estimate the inner products by Cauchy-Schwarz, Young,s inequalities and Lemma B.1
-小网,t,y - yt+ιi =-小8,t,y - ys〉一 ηt(νs,t,yf - ys+ιi
η2	CI	C
≤	-ηt亿,t,y - yt〉+ ɪkν*,tk∞ + 2llyt - yt+ιkι
八	八	η2	CI	C
=	-ηt亿,t,y - yt〉- ηthν*,t, yt - yt〉+ ɪI以,tk∞ + 2llyt - yt+ιkι
≤	-ntk,t,us - yti+ ι 一Y Ilys - yskι+ 2(］一 γ)2 + 2lys - ys+ιk2
Since yf(b) = (1 - Z)yS(b) + ζ∕∖B∖, ∣yS - yS∣ι ≤ O(∣B∣Z). We join everyting in the main
inequality to get
ηthν*,t, ys -翦 + D(ys,ys+1) ≤ D(ys,yS) + 2ηt∣^t+ι - ^S,tl∞ + 占O(∖B∖ζ) + 2(1尸
We recall the definition of ^*,t(s, b) = Ea〜XsQ^fc,yt (s, a, b). By the performance difference lemma
VEyk(so) - VXkR(so) = ɪE 产睦优,t,y£ - ys)，
1 — Y S 〜ds0
where y% is the best response to Xk. Note that by Lemma B.3, Vxk,yk (so) - V^k,yk (so) ≤(；庠2.
Therefore, we have
ET X VXk期(so) - Vxk,yt (so) ≤ O ((1-Y)Tητ) + O (占园除1 -喇∞)
+ O ( IBk ) + O (—1—
+ 1(1 -Y)" + IT 6/7(1 -Y)3
Finally due to Lemma B.3, V xk,yt (so) -VXk ,yt (so) ≤ O (Z((A-；% |)J, therefore the final inequality
is
1工 *	一	〜/	1
ET Evxk,y*(so) - Vxk,yt(so) ≤ O ^(1 - Y)T 1/7
+ O (± ElIVS+ι- ν=,tl∞)
+ O ( IBk ) + O (—1—
+ k(1 - y)2J + IT 6/7(1 - y)3
□
We finally analyze the critic for stage 2,
Lemma G.13. LetAssumptionl hold. For the critic in stage 2 in Algorithm 8, let βt
and recall λmin = ρZ∕∖B ∖
2
λmin(t+7)4/7
-T	∕r∖	Z	1,-,111-.1	X	/	_	∖
T X EkVt - ν*,tTk2 ≤ O (t3a) + O ((1 - Y )3T 4/7 ) + O ((1 - Y)2T 2/7 )	(64)
Proof. By the same steps in Lemma G.10, we can get a similar inequality to (63)
EkVt+1 - "*'t"2 ≤ (1 - βtλmin) EWt- ν*,tτ12 + βtλ"l)3 + (⅛ ∙	(65)
50
Under review as a conference paper at ICLR 2022
Equivalently,
T X EkVt- ν*,t-1k2 ≤ O (T3/7) + T X
t=1	T	t=1
4∣B∣∣s∣η2	,	2βt
C 八-	- I ʌ	-
β2Cmin)2(1-γ)3	^min(1-γ)2
(66)
We can estimate other terms
T X EkVt- ν*,jk2 ≤ O (T1/7) + O ( (I-SY)BT4/7 ) + O ((1-Y
(67)
□
51
Under review as a conference paper at ICLR 2022
G.2 REFLECTED NAC WITH A GAME ETIQUETTE AND ζ-GREEDY EXPLORATION
In this section, we give a version of reflected NAC without Assumption 2, in a similar way described
in (Lan, 2021, Remark 1). This algorithm will be Algorithm 1 with nonzero ζ. Since this result is
not derived in Lan (2021) for single agent MDP, we first derive it with single agent MDP.
G.2.1 SINGLE AGENT RESULT WITH ζ-GREEDY EXPLORATION
The algorithm will be Algorithm 9.
Algorithm 9 Single player case
Require: Vθ such that kV∕0 - Vx0,y01∣∞ ≤ C
for t = 0, 1, . . . , T - 1 do
Set ytWs) = (I- ζ)yt(b* l * * *s) + Z/|B|
^t+ι = Policy-Eval(yt, N)
yt+ι(Is) = P (yt(Is), -ηνt+ι(S, ∙))
Theorem G.14. Let Assumption 1 hold. For η > 0, the iterates of Algorithm 9 satisfies
T
T Xv ?(so) - E[Vyt (so)]] ≤ η(r-γT Jdy? hD(y*,s,yS)i
T
+ T(T-YEEs~dy? [VyT+1(s) - Vy1 (s)]+(⅛T X Ek* - pt+ιk∞
+	XEkE[^t+ι∣yt] - ^*,tk∞ + 4η^A2 + π4¾ (68)
1 - γT t=1	(1 - γ)6	(1 - γ)3
Proof. Recall the definitions
ν*,t = Qyt, ν*,t = Qyt.
Recall that the update rule of yt+ι implies D(ys,yS+ι) ≤ D(ys, yS) - D(yS+ι,yS) - ηh^S+ι, ys -
yts+1i and by plugging iny = yt, we get
D(yS,yS+ι)+ D(yS+ι,yS) ≤ -ηh‰ι,yS -成+1∙
By (8),
0 ≤ -hνt+ι, yt - ys+ιi - ^kys+ι - ysk2 ≤ -hνs+ι,ys - ys+ιi - 2-llys+ι - yski.	(69)
η	2η
By performance difference lemma and Young’s inequality
Vyt+1(so) - Vyt (so)=十Es~dyt+1 hQyt,s,yS+ι -ySi
=ɪɪEs~dyt+1 H,t,碗+ι- ysi
=二Es^dyt+1	hhνs+1,ys+1	- ySi	+	h&- ^S+ι, y；+i	- y；i]
≥ ι-γEs~dyt+1	hhVs+ι,ys+ι	- ysi	-	2kνs,t - νs+ιk∞	- 2^llys+ι	- ysk2i∙
Therefore, we get by using (69) and 1 - γ≤dsy0t+1(so) ≤1
hVt+1,yt+1 -	ySi	≤	Vyt+1(s)	- Vyt(S)	+ ɪky；+i	-	ySk2 + ʒɪ-kν*,t	-	^t+ιk∞.	(70)
2η	2(1 - γ)	∞
52
Under review as a conference paper at ICLR 2022
We use again the definition of yt+1
D(ys,yS+ι) ≤ D(ys,yS) - D(yS+ι,yS) - ηh^S+ι,ys -建+1
=D(ys,yS) - D(yS+ι,yS) - ηh^S+ι,ys - y；i— ηh^S+ι,yS - y；+〉	(71)
Let us use (70) and
-ηhνts+ι,ys - y；i = -ηh啧,ys - y；i- ηh^s+ι - v?,t, ys - ySi
on (71) to get (along with (8))
ηhνS,t,ys - y；i + D(ys,yt+ι) ≤ D(ys,yS) + η(Vyt+1(s)- Vyt(S))
2
+ 2(1 - Y) kν*,t - z⅞+ιk∞ - ηhνt+ι - νs,t,ys - ysi	(72)
Since by performance difference lemma, V?(so) - Vyt (so) = ɪ-1YEs〜dy? W?,t+、,y卡,-y；i，We
plug in y = y? and take expectation in (72) w.r.t. dsy0?,
η(i - Y)(V?(so) - Vyt (so)) ≤ Esyy? [D(ys,yS) - D(ys,yt+ι)] + ηEs〜dy? [Vyt+1(s) - Vyt (s)]
η2
+ 2(1 - Y) kν*,t - z⅞+ι∣∣∞ - Es〜dy? [ηh%+ι - ν*,t,y - yt]i
(73)
Next, We take expectation W.r.t. randomness in the algorithm and use toWer property to get
η(1 - γ)(V*(so) - E[Vyt(so)]) ≤ EEs〜dy? [D(y*,s,ys) - Dw,喷+1)]
2
+ ηEEs〜dy? [Vyt+1(s) - Vyt (s)] + 2(1η-^)Ekν*,t - νt+ιk∞
-EEs”？ [ηhE[^s+∕yt] - v?,t,ys - y%]).	(74)
We also note that by CaUChy-SChWarz inequality -ηhE[^s+∕yt] - v?,t, ys - y；]) ≤ 2∣∣E[Vt+ι |yt]-
ν*,t k∞.
By triangle inequality and Young’s inequality
22
2(1 - γ) kν*,t - νt+ιk∞ + 2ηkE[z>t+ιlyt] - ν*,t∣∣∞ ≤ 1 - γ∣∣ν*,t - Ot+ι∣∣∞
2
+ ；--kν*,t - ν*,t∣∣∞ + 2ηkE[z>t+ιlyt] - ν*,t∣∣∞ + 2ηkν*,t - ν*,t∣∣∞
1-Y
≤ 1 - Y kθ*,t - z⅞+ι∣∣∞ + (1 ；)5 (max llys - yskι)2
+ 2ηkE[νt+ιlyt]- o*,t∣ι∞ + ∩一η~2 max ∣∣ys - yskι,
(1 - Y)2 s t t
where the last step used Lemma B.4 and definitions v?,t = Qyt, ^*,t = Qyt. We use the estimation
as in (56), to get kyt - ytkι ≤ |B|Z. With these estimations, (74) becomes
V*(so) - E[Vyt(so)] ≤ η1-γEEs”？ [D(y*,s,yS) - D(^^s,yt+ι)]
1	η2
+「EEsyy? [Vyt+1 (s) - Vyt (s)] + (T-^Ek^*,t -^t+ιk∞ + 「EkE[^t+ι|yt] -^*,tk∞
,4η∣B∣2Z2 ,	4∣b∣Z (75)
+ (i-γ)6 +(i-γ)3 ()
We sum over t to conclude.
□
53
Under review as a conference paper at ICLR 2022
Note that We will learn ^*,t by sampling with yt. As in Appendix G.1.1, the strong monotonicity
constant of the operator
Fyt (V )(s,b) = Pyt (s)yt(b∣s)((s,b) -r(s,b) - Y X P(SlS, b)yt(b0∣s0)ν (s0,b0)
Λ	,	.......... ..	— ,
is ʌmin = (1 - γ)ρZ∕(∣B∣) (see also Lan (2021)). Moreover, Fyt(v?,t) = 0.
Lemma G.15. Let Assumption 1 hold and βn = ^——2------------ stage 2 in Algorithm 9 satisfies
λνmin(n+n0)
Ek^N - V*,tk2 ≤O
(∣s∣∣B∣	,	1
I(I-Y)2N 2	N(1-γ)2*m)2
kE[^⅛|yt] -ν*,tk2 ≤O (U)BN2J
>λ ∕' -I-VT	τ	Tnr	ʌ ʌ J/	ʌ J/	∙>∙PS∕ 八	/ T ∖	/ T ∖	/	/	1 ∕∖
Proof. WecanUSeLemmaF.7With V = ^,入窘也 = 入窘由 and F (ν,ξ) = V (s,b) - r(s,b) - YV (s0,b0)
where S 〜Pyt, b 〜yt(∙∣s), s0 〜P(∙∣s,b), b0 〜yt(∙∣s0) to get the result.	□
Corollary G.16. Let Assumption 1 hold. Let ζ = (1 - Y)3/|B|. The sample complexity of Algo-
rithm 9 for obtaining globally optimal policy in single agent MDP is O(|B|4|S |(1 — Y )-13ρ-2e-4).
Proof.
T
T XV ?(S0)- E[Vyt (S0)]] ≤ ητ-γT Es 〜dy? hD(y?,s,yS)i
T
+ w;-VEEs〜d“？ PyT+1(S)- Vy1(s)] + 7T‰2ɪ XEkV*,t - ^t+ιk∞
T (1 — Y)	s〜dso	(1 — γ)2 T
2 1 Λκllκr , 1 ʌ l, q4η∣B∣2Z2	4∣B∣Z
+ 1-" T NEkE[%+llyt] - “*,tk∞ + (1 - y)6 + (1 - Y )3	(76)
By using Lemma G.15 in the previous lemma with Xm® = (1 - y)pZ∕∣B∣ gives
ɪ X [V ?(so) - E[Vyt (so)]] ≤ O (T(J )2) + 4η^A2 + ∩4¾
T t=1	T(1 - Y)2	(1 - Y)6	(1 - Y)3
+ O( η ( |S l∣B I +	∣b∣2 ʌ + PMR
+ ((1- Y )2 1(1- y)2n 2 + N (1 - Y )4p2Z 2J + (1-Y)N
(77)
By picking η = (1-Y) and Z = (1-Y)3e∕∣B∣, we get the complexity O(∣B∣4∣S|(1-Y)-13ρ-2e-4).
- □
G.2.2 REFLECTED NAC WITH A GAME ETIQUETTE AND ζ-GREEDY EXPLORATION
in Algorithm 10
We first state the main result in the next corollary. As most of the results depend on previous
sections, we will make heavy use of those estimations and provide a brief proof by highlighting the
differences and extracting the error due to greedy exploration. Algorithm 1 is repeated here for
convenience.
Theorem 3.3. Let Assumption 1 hold. Use the parameter choices from Corollaries G.20 and G.21,
use Remark C.1 and use the estimates in these corollaries with (14). Then, the complexity for Algo-
rithm 10 is O(∣S∣4(∣A∣ ∨ ∣B∣)6(1 — y)-15E-4P2).
54
Under review as a conference paper at ICLR 2022
Algorithm 10 Reflected NAC with a game etiquette and ζ-greedy exploration
Require: PKL defined in (1) in Section 1. Exploration parameter ζ ≥ 0 (equal to 0 if Assumption 2
holds). Subroutine Policy-Eval (see Algorithm 2, Algorithm 3, Algorithm 4). Initial policies
χo,yo,yo
for k = 0, 1, . . . do
Stage 1
for t = 0, 1, . . . , T - 1 do
Define χk-1(als) = (I - Z)Xk-I(aIs) + ɪ, yk-1(bIs) = (I - ζ)yk-1(b|s) + ∣B and
correspondingly for Xk,t, yk,t.
^ ^
[V^kx-1, Vzk-ι] = [Policy-Eval(xk-1,yk-1,N,βω), Policy-Eval(xk-ι,yk-ι, N,βnω)]
[θχ+ι, "+J = policy-Eval(Xk,t,yk,t,N, V⅞-ι,βnn), Policy-Eval(χk,t,yk,t,N,v-ι,βnn)]
χk,t+ι(∙) = PKL (χk,t(∙),η (2θχ+1(s, ∙) - θχ(s, •)))
yS,t+ι(∙) = PKL (yS,t(∙), -η 改+小,∙) - θ(s, ∙)j)
Output Xk = T PT=1 Xk,t.
Stage 2
for t = 0, 1, . . . , T - 1 do
Xk(O) = (I- Z)Xk(G + i⅛, ys,t(b) = (I- ζ)ys,t(b) + ∣Bb∣
^t+ι = Policy-Eval(Xk,yt, N, β = βη)
ys,t+ι(∙) =PκL(yk,t (∙), -ηνt+ι(s, ∙))
Output yk = yk ^, where £ ∈ [T] is selected uniformly at random.
Theorem 3.3. Extended form Let Assumption 1 hold. Use the parameter choices from Corollar-
ies G.20 and G.21, use Remark C.1 and use the estimates in these corollaries with (14). Then the
suboptimality gap is bounded as follows:
EEk0~“max Vxk,y(so) - v?(so)] ≤ (C-Y)o{ W-Yɪ
+ ∣S∣2(∣A∣∨∣B∣)6 + ∣S∣∣B∣ +	∣B∣2	+ PMR ] + O (Cμ,σYk A
十(1 - γ)5 * * * * *ρ2e2N +(1 - Y)3N2 + N(1 - 7)11ρ2e2 +(1 - Y)Nj +	((1 - Y))
Then, the Complexityfor Algorithm 10 is 0(∣S∣4(∣A∣ ∨ ∣B∣)6(1 - Y)-15e-4ρ2).
Remark G.17. This result does not require Assumption 2, which makes our set of assumptions
similar to Wei et al. (2021) which also used greedy exploration. Comparing with (Wei et al., 2021,
Corollary 5), our result has the same e-4 dependence, a better dependence on (1 - Y). We note that
even though our dependence with IAI ∨ IBI seems worse compared to (Wei et al., 2021, Corollary 5),
the result in (Wei et al., 2021, Corollary 5) has dependence C-10 where C is the metric subregularity
constant. This constant implicitly depends on dimensions of variables X, y, therefore it depends on
IAI, IBI, ISI. More importantly, this result in Wei et al. (2021) does not cover NPG since it requires
Euclidean projections on the simplex. As a result, the comparison with (Wei et al., 2021, Corollary
5) in terms of IAI, IBI is not possible. On the other hand, the result of (Wei et al., 2021, Corollary 4)
without metric subregularity applies to NPG, and it can be compared with our result, in which case
our sample complexity scales as e-4 whereas (Wei et al., 2021, Corollary 4) as e-8.
Stage 1 of Reflected NAC with a game etiquette and Z-greedy exploration in Algorithm 10 We
first recall the discussion in Appendix G.1.1 and include it here for easy reference. Let us recall the
notation and introduce more notations. At iteration k, we solve
Qk(a, b) = Q(s, a, b) = r(s, a, b) + Y X P(s0Is, a, b)V xk-1,yk-1 (s0).
k0
The problem is to find Xout , yout such that
E max Xkout Qkyk - XkQkyokut .
xs,ys
55
Under review as a conference paper at ICLR 2022
Now we define
Q s(a, b) = Q(s,a,b) = r(s,a, b) + Y X P(s0∣s,a, b)V xk-1,yk-1 (s0),
s0
as We sample with policies with greedy exploration, at stage 2 We will learn Vxk-ι,yk-1. So We will
have the oracle Vk-I such that kVk-I - V^-1,ykτ k is small.
In this step, with the abovementioned oracle, we expect to learn
%,t(s,a)= Eb〜ysQ (a, b),	θf,∕s,b)= Ea〜XsQ (a, b).
Let us continue with x-player and drop the superscript,
s
θ*,t(S,a)= Eb〜yt(.∣s)Q Gb)
=X ysQs(a,b)
b
=X yt(b∣s)r(s, a, b) + Y X yt(b∣s)P(s0|s, a, b)Vxkτ,yk-1 (s0).
b	b,s0
Therefore the operator is
Ft(θt) = P t,yt(s)xt(a∣s) (θt(s,a) - £yt(b∣s)r(s,a,b)
b
-Y Xyt(b∣s)P(s0∣s, a, b)Vxk-1,yk-1 (s∕). (78)
s0,b
AS xt(.|s) ≥ τAτ and yt(.|s)≥ 后, we have that Ft is strongly monotone with P 备 where P is the
lower bound of the stationary state distribution. Let US call this Tm® = P看.
As in Lemma F.2, we introduce notation similar to Malitsky & Tam (2020), let us define Φ, which
is slightly different this time.
1
ΦS+1 = D(Xs,xS+ι)+ ηhθS,t+ι - θs+ι,xs - xS+ιi + XD(Xs+ι,xS).	(79)
We also use the following error functions and recall the definitions
,ʌ , . .	_ r ʌ	,... , ,.. ,..,
eι,t = ηhθt+ι(ds)-胤仇+人⑸也卜必⑸-χt(1s)i
e2,t = ηhθy+ι(∙ls) -E[Gy+i(1s)|yt],yt("s) -y(1s)i,
s
θ*,t(s, a) = Eb〜ytq (a, ∙)
θ*,t(s, a) = Eb〜ytQ (a, ∙).
Lemma G.18. Let Assumption 1 hold. Let η = 1-γ. Denote X。加=T PT=I Xt and yout =
T1 PT=I yt. For Stage 1 in Algorithm 10
EEs〜σ max XOUtQSyS -"可期。毋
xs ,ys
+ O (T XEkθt+1 - θ*,tk2 + Ekθt - θ*,t-ik2) + TnEEs〜σ maxX[eι,t + e2,t])
1 4Z(∣A∣∨ |B|) +4Z∣B∣
+ (I-Y)2 + 1 - γ.
Remark G.19. The idea of this lemma is to extract the error due to greedy exploration and make
the errors due to policy evaluation depending on the policies with greedy exploration. In particular,
θt+ι learns θ*,t, by sampling Xt,yt. Therefore, we can utilize the results we derived earlier for
policy evaluation steps.
56
Under review as a conference paper at ICLR 2022
Proof. We start from the quantity we would like to bound. Recall from (24)
1T	1T
XoutQsys -XsQsysut = T EhEa〜XsQs(a, ∙),ysi- T EhEb〜ysQs(∙,b),Xsi
t=1	t=1
=T XX [hEa〜XsQs(a, ∙),ys - ysi -hEb〜ysQs(∙, b),χs - xsi].
t=1
=T XX [hθy,t, ys - ysi-hθX,t, Xs - Xsi]. (80)
t=1
We will drop the subscript of θ to denote θX for lighter notation and derive the part of X-player. The
y-player case will be symmetrical.
By the definition of Xt+1,
, „ , , …	ʌ „	ʌ „ 一^
D(Xs,Xs+ι) ≤ D(XF)- D(Xs+ι,Xs) + ηh2θs+ι - ΘS,xs - Cs+)	(81)
We manipulate the inner products as Lemma F.2
. ʌ _ ʌ _ _	. ʌ _	_ _ _	. ʌ _	ʌ _ _ . _ _
h2θt+ι - θs,Xs - Xs+ιi =(。；+1 - θ"ι,Xs - Xs+ιi + hθt+ι - θ↑,Xs - Xs+ιi + hθ"ι,Xs - Xs+J
.ʌ _	_ _ _	. ʌ _	ʌ _ _ _. . ʌ _ ʌ _ _ _
=hθs+ι - θ"ι,Xs - Xs+ιi + hθs+ι -毋,Xs -Xsi + hθt+ι - θf,Xs - Xs+ιi
+ hθs,t+ι,Xs - Xs+ιi
.ʌ _	_ _ _ . ʌ _ _. . ʌ _ _ _
=hθs+ι - θ∖t+ι,Xs - Xs+ιi + hθ:,t - θs,Xs - Xsi + h%ι-%, Xs - Xsi
.ʌ _ ʌ _ _ . _ _
+ h璋+1-璋,Xs - Xs+ιi + (θ"ι, Xs - Xs+ιi.
Note that the first two terms will be used while forming Φts and Φts+1, final term will be what we
bound for (80). The third and fourth terms are the error terms. We first analyze the third term
hθt+ι - θs,t, Xs	- Xsi = hE[θs+ι∣Xt]	ft, Xs -	Xsi	+ hθt+ι - E[θs+ι∣Xt], Xs	-Xsi
=hE[4+1|Xt]	- θ*,t, X -	Xti	+ e1,t ≤ 2kE[θt+1 |Xt] -	θ*,t∣∣∞	+ e1,t.
Next, by Cauchy-Schwarz and Young’s inequalities
hθs+ι- θs ,Xs- Xs+ii ≤ ηkθt+ι- θtk∞+4^ iiXs- Xs+i k2
≤ 2ηkθt+ι- θ*,t∣∣∞+4ηkθ*,t- θ*,t-ιk∞+4ηkθ*,t-ι- θtk∞+4η kXs- Xs+ik2
≤ 2ηkθt+ι- θ*,t∣ι∞	+ ∩—η-2 kyt- yt-ιk∞	+ 4ηkθ*,t-ι- θtk∞	+ ʒ-D(Xs+ι,Xs).
∞	(1 - γ)2	∞	∞	2η
where the last inequality is similar to (40) when θ, yt are replaced by θ, yt and We also used ∣∣y^ -
ys-1k1 ≤ ||y； - y；-i k ι along with (8). We collect all the estimations in (81)
1
ηhθi,t+ι,Xs-Xs+ιi + D(Xs,Xs+ι)+ ηhθi,t+ι-碎+1,Xs-Xs+1i≤ D(Xs,Xs)- XD(Xs+ι,Xs)
+ ηhθs,t - ds,Xs - Xsi + eι,t + 2kEWt+i|Xt] - θ*,t∣∣∞ + (] _；)2 Ilys - ys-1k2
+ 2η2kθt+ι - θ*,tll∞ + 4η2kθt - d*,t-ιk∞∙
We use (8) and definition of Φts to get
ηhθi,t+ι,Xs - Xs+ii + Φs+1 ≤ Φs + 2∣∣E[θt+ι∣Xt] - θ*,tk∞
+ 2η2kθt+1 - θ*,t∣∣∞ +仞2||瓦一θ*,t-1k∞ - 2 D(Xs,Xs-i) + (] _；)2 D(ys,ys-i)∙
57
Under review as a conference paper at ICLR 2022
We finally estimate the bias term
_____ △	, r	_	..	__. r	O ..	.. O	_	..
kE[θt+ιlxt] - θ*,t∣∣∞ ≤ ∣∣E[θt+ι|xt] - θ*,t∣∣∞ + ∣∣θ*,t - θ*,t∣∣∞
__”个	.	,	ʌ	..	  一 C	_	2 …
=kE[θt+1∣xt] - θ*,tk∞ + ∣∣Eb~ys Qt - Eb~ys Qt k∞
__”个	.	,	ʌ	..	  一 C	_	2 …	  2-	_	ʌ _
=kE[θt+1 |xt] - θ*,t ∣∣∞	+	∣∣Eb~ys Qt	- Eb~ys Qt ∣∞ + ∣∣Eb~ys Qt	- Eb~ys Qt ∣∞
。取旧	I	1	^	II	j2Z(∣A∣∨∣B∣)	2Z |B|
≤ kE[θt+ι∣xt] - θ*,t∣∣∞ + Zj------72--+ ---------,
(1 - γ)2	1 - γ
where the last step is by (53) and (56).
Inserting this estimate leads to
hθs,t+ι,xs - xs+ii ≤ η (φs - φs+j+2kE[θt+ιιxt] - θ*,t∣∣∞+2ηkθt+ι - θ*,t∣∣∞
+ 4ηkθt - θ*,t-ιk∞ - ɪD(FxL) + 4D(ys,ys-ι) + 4ζ(≡^ +	.
∞	2η	t	t-1	(1 - γ)2	t	t-1	(1 - γ)2	1 - γ
As in Lemma F.2, We Can derive the symmetrical inequality for the y-player and picking η = 1-γ
will make the terms (I-2^(D(ys,ys-ι) + D(xs,xs-ι)) disappear.
We sum the inequality along with its y-player counterpart and insert into RHS of (80) to conclude.
□
Corollary G.20. Set Z = ∣^-YBI and step sizes asfrom Lemmas 3.6 to 3.8 with *%由=ρZ∕(∣A∣ ∨
|B|)，ʌmin = ρ, Z= ∣A∣∨γ)∣. Then, the complexity for stage 1 of Algorithm 10 is O(∣S∣2(∣A∣ ∨
∣B∣)6(1 - γ)-5e-4ρ2).
Proof. First, we notice that both variance and bias terms in Lemma G.18, now we learn θ*,t, θ^t. by
sampling xt, yt. Therefore, from the point of view of policy evaluation, we can directly apply the
results we derived earlier with strong monotonicity constants *%由=ρζ∕(∣A∣ ∨ |B|),富*山=ρ.
The bound in Lemma G.18 becomes
EEs~σ mayxxoutQsys - xsQsyout = O (^-^T ) + O G XX EkE[Mlxt] - θ*,tk)
+ O (T XX Ekθt+1 - θ*,tk2 + Ekθt - θ*,t-ik2) + Tη EEs~σ max XX [ei,t + e2,t])
[ 4ξ(∣A∣∨ |B|) + 4ξ∣B∣
+ (i-γ)2 +ι-γ.
By using Lemma 3.6, we can derive the variance of θ by sampling with xt, yt
e∣Θn - θ*,tk2 ≤o
∣s ∣∣A∣
(1 - γ)2N2
1
+ N(λmin)2(1-γ)2 +
7sA2EkVLI- Vfik-1,yk-1 k∞
(λmin)
For the final term in this bound, we use Lemma 3.8 to estimate Vxk-1,yk-1 by sampling with xk-ι
and yk-i.
ʌ
..ʌ	ʌ	ʌ	. . C
EkVk-I - VXk-1,yk-1k2 ≤ O
(∣s∣∣A∣	,	1
U1-Y)2N2	N(i -γ)2(λmin)2
For the bias of θt+i , we use Lemma 3.7
kE[θt + 1∣xt] - θ*,tk2 ≤O
(1 5N2 + ɪf kE[vk-ι∣xt] - Vxk-1,yk-1 k∞
( min )
(82)
58
Under review as a conference paper at ICLR 2022
On this bound, we use Lemma 3.8
kE[Vk-ι∣xt] - Vxk-1,yk-1 k2 ≤ O ((1ISYAN2).
Finally for the error terms e1,t + e2,t, we use Lemma F.6
TT
TEEs〜σ max X e1,t ≤ 'gT-+ T X 4η2Ekθt+1 - θ*,tk∞∙
t=1	t=1
Inserting these estimates and using Tm® = ρZ∕(∣A∣ ∨ ∣B∣),富*也 = ρ, Z = ∣A∣∨γB∣, RemarkF.3
give
EEs〜σ [maχXoutQsys	-XsQsyoJ	≤ O (TJ J	+ 8e	+ O (lSl2(A""),
χs,ys	_|	∖J. (1 — Y))	(1 (1 — γ)5ρ2e2N J
which gives the final result.	□
Stage 2 of Reflected NAC with a game etiquette and ζ-greedy exploration in Algorithm 10
Recall that in Appendix F.2, we extended the result of Lan (2021) on single agent setting to the
stage 2 of our algorithm. Since in the stage 2, Xk is fixed, it is part of the environment and therefore
single agent analysis extends ina straightforward fashion. In this section, similarly we use our results
in Appendix G.2.1 for stage 2 of the algorithm. Note that our results in Appendix G.2.1 are similar
to single agent results of Lan (2021), with the difference that we use ζ-greedy to avoid Assumption
2. Also recall that the single agent analysis in Lan (2021) requires Assumption 2 (see (Lan, 2021,
Remark 1)).
For brevity, we do not repeat the arguments in Appendix F.2 to extend Appendix G.2.1 for stage 2
in this case. We summarize the result in the next corollary.
Corollary G.21. LetAssumPtionI hold. Let Z = |^-^"： and step sizefrom Lemma F.7 with λmmE =
(1 — γ)ρZ∕∣B∣ (see also Corollary G.16). The overall sample Complexityfor stage 2 OfAlgorithm 10
is O((∣A∣∨∣B∣)4∣S|(1 -γ)T3ρ-2L4).
Proof. It is straightforward to use the arguments in Appendix F.2 to extend Appendix G.2.1 for this
result.
□
H Additional Experiments
H.1 Environments description
Bandit environment. We first consider a two player bandits problem with 100 arms. All the arms
give zero reward except a* that has r(a*) = 1 and b with r(b*) = 1. The reward of the two player
game is r(a, b) = r(a) + r(b). In this environment there exists a pure Nash Equilibrium given by
X = 1{a = a*}, y = 1{b = b*}.
Alesia environment. We also test our algorithms on Alesia(L, C) (Perolat et al., 2015). Alesia is
a simultaneous move game where the two opponents try to move the wrestler to their extremity of
the board of length L. At each turn, the “wrestler” moves one step in the direction of the player that
bets the higher amount of coins from their budget of size C . The game ends when either one of the
wrestler reaches extremity of the board or both players finish their budget.
H.2 Hyperparameters selection.
In the following we report the hyperparameters chosen for the experiments to ensure reproducibil-
ity. In addition, we include an additional experiment on Alesia(L,C) with L = 3 and C = 7
in Figure 2. Since the eigenvalues λθmin, λωmin , λνmin are unknown, we cannot compute the values
59
Under review as a conference paper at ICLR 2022
of learning rates βnθ , βnω used in the theoretical results. Therefore, we simply set the learning rate
as Cn suggested by the lemmas and fine tune the hyperparameter C by grid search. Similarly for
OGDA, we use grid search to replace the optimal learning rate given by (Wei et al., 2021, Theorem
2). Appendix H reports the chosen parameters. Table 2, Table 3, Table 4 and Table 5 summarize
the best hyperparameters found for Reflected NAC and OGDA in the two environments: bandit and
Alesia.
REINFORCE (Daskalakis et al., 2020) only needs two hyperparameters. We chose the exploration
parameter = 0.1 and the step size η = 0.0001. We observe that it was critical to choose small step
size to tackle the high variance coming from the REINFORCE estimator.
Hyperparameter	Value
βω	0.01/n
βnθx	0.8/n
βnθy	0.1/n
η	0.023
βnν	0.01/n
K	100
N	10
T	10
ζ	0
Table 2: Hyperparametets for Reflected NAC in the two players bandit environment
Hyperparameter
η
αt
T
L
Value
0.005
0.01/t
1100
50
0.4
Table 3: Hyperparametets for OGDA in the two players bandit environment. The notation used for
the hyperparameters matches the original OGDA formulation in Wei et al. (2021)
Hyperparameter	Value
βnω βnθ	0.01/n
	0.1/n
η	0.05
βnν	0.01/n
K	500
N	70
T	10
ζ	0 for y and 0.2 for X
Table 4: Hyperparametets for Reflected NAC in Alesia(L, C) with L = 3 and C = 6, 7.
60
Under review as a conference paper at ICLR 2022
Hyperparameter	Value
η	0.005
αt	0.01/t
T	1000
L	3500
	0.4
Table 5: Hyperparametets for OGDA in Alesia. The notation used for the hyperparameters matches
the original OGDA formulation in Wei et al. (2021)
Figure 2: left: x player, right: y player. Experiments in a Alesia with length L = 3 and coin budget C = 7.
The suboptimality gap on the y-axis is maxy Vx,y (so) — V*(so) for X and as | minx Vx,y (so) — V*(so)| for
y where s0 is the initial state that is deterministic in Alesia. Results are averaged over 5 seeds.
61