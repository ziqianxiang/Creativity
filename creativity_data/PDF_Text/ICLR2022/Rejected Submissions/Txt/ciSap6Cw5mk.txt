Under review as a conference paper at ICLR 2022
MANDERA: Malicious Node Detection in
Federated Learning via Ranking
Ab stract
Federated Learning is a distributed learning paradigm which seeks to preserve the
privacy of each participating node’s data. However, federated learning is vulner-
able to attacks, specifically to our interest, model integrity attacks. In this pa-
per, we propose a novel method for malicious node detection called MANDERA.
By transferring the original message matrix into a ranking matrix whose column
shows the relative rankings of all local nodes along different parameter dimen-
sions, our approach seeks to distinguish the malicious nodes from the benign ones
with high efficiency based on key characteristics of the rank domain. We have
proved, under mild conditions, that MANDERA is guaranteed to detect all mali-
cious nodes under typical Byzantine attacks with no prior knowledge or history
about the participating nodes. The effectiveness of the proposed approach is fur-
ther confirmed by experiments on three classic datasets, CIFAR-10, FASHION-
MNIST and MNIST. Compared to the state-of-art methods in the literature for
defending Byzantine attacks, MANDERA is unique in its way to identify the ma-
licious nodes by ranking and its robustness to effectively defense a wide range of
attacks.
1 Introduction
Federated learning (FL) has observed a steady rise in use across a plethora of applications. FL
departs from conventional centralized learning by allowing multiple participating nodes to learn
on a local collection of training data, before each respective node’s updates are sent to a global
coordinator for aggregation. The global model collectively learns from each of these individual
nodes before relaying the updated global update back to the participating nodes. With an aggregation
of multiple nodes, the resulting model observes greater performance than if each node was to learn
on their local subset only. FL presents two key advantages, increased privacy for the contributing
node as local data is not communicated to the global coordinator, and a reduction in computation by
the global node as the computation is offloaded to contributing nodes.
However, the presence of malicious actors in the collaborative process may seek to poison the per-
formance of the global model, to reduce the output performance of the model (Chen et al., 2017;
Fang et al., 2020; Tolpegin et al., 2020b), or to embed hidden back-doors within the model (Bag-
dasaryan et al., 2020). Byzantine attack aims to devastate the performance of the global model by
manipulating the gradient values of malicious nodes in a certain fashion. As these attacks emerged,
researchers seek to defend FL from the negative impacts of these attacks.
In the literature, there are two typical defense strategies: malicious node detection and robust learn-
ing. Malicious node detection defenses by detecting malicious nodes and removing them from the
aggregation (Blanchard et al., 2017; Guerraoui et al., 2018; Li et al., 2020; So et al., 2021). Robust
learning (Blanchard et al., 2017; Yin et al., 2018; Guerraoui et al., 2018; Fang et al., 2020; Cao et al.,
2020), however, withstands a proportion of malicious nodes and defenses by reducing the negative
impacts of the malicious nodes via various robust learning methods (Wu et al., 2020b; Xie et al.,
2019; 2020; Cao et al., 2021).
In this paper, we focus on defensing Byzantine attacks via malicious node detection. In the literature,
there have been a collection of efforts along this research line. Blanchard et al. (2017) propose a
defense referred to as Krum that treats local nodes whose update vector is too far away from the
aggregated barycenter as malicious nodes and precludes them from the downstream aggregation.
Guerraoui et al. (2018) propose Bulyan, a process that performs aggregation on subsets of node
1
Under review as a conference paper at ICLR 2022
updates (by iteratively leaving each node out) to find a set of nodes with the most aligned updates
given an aggregation rule. Xie et al. (2019) compute a Stochastic Descendant Score (SDS) based
on the estimated descendant of the loss function, and the magnitude of the update submitted to the
global node, and only include a predefined number of nodes with the highest SDS in the aggregation.
On the other hand, Chen et al. (2021) propose a zero-knowledge approach to detect and remove
malicious nodes by solving a weighted clustering problem. The resulting clusters update the model
individually and accuracy against a validation set are checked. All nodes in a cluster with significant
negative accuracy impact are rejected and removed from the aggregation step.
Although the aforementioned methods try to detect malicious nodes in different ways, they all share
a common nature: the detection is based on the gradient updates directly. However, it is usually
the case that different dimensions of the gradients remain quite different in the range of values and
follow very different distributions. This phenomena makes it very challenging to precisely detect
malicious nodes directly based on the node updates, as a few dimensions often dominate the final
result. Although the weighted clustering method proposed by Chen et al. (2021) could avoid this
problem partially by re-weighting different update dimensions, it is often not trivial to determine the
weights in a principled way.
In this paper, we propose to resolve this critical problem from a novel perspective. Instead of work-
ing on the node updates directly, we propose to extract information about malicious nodes indirectly
by transforming the node updates from numeric gradient values to the rank domain. Compared to the
original numeric gradient values, whose distribution is difficult to model, the ranks are much easier
to handle both theoretically and practically. Moreover, as ranks are scale-free, we no longer need to
worry about the scale difference across different dimensions. We proved under mild conditions that
the first two moments of the transformed rank vectors carry key information to detect the malicious
nodes under a wide range of Byzantine attacks. Based on these theoretical results, a highly efficient
method called MANDERA is proposed to separate the malicious nodes from the benign ones by
clustering all local nodes into two groups based on the moments of their rank vectors. With the
assumption that malicious nodes are the minority in the node pool, we can simply treat all nodes in
the smaller cluster as malicious nodes and remove them from the aggregation.
Global
ModeI	产
Reject
Malicious
Compute .■■■■■■ Find and reject nodes
Rank Mean	O**** in malicious cluster
& SD Q
ω
. Aggregate and
then,	∙ ∙ Update Global Model
CIuSter	Mean with benign cluster
Figure 1: An Overview of MANDERA
The contributions of this work are as follows. (1) We propose the first algorithm leveraging the
rank domain of model updates to detect malicious nodes (Figure 1). (2) We provide theoretical
guarantee for the detection of malicious nodes based on the rank domain under Byzantine attacks.
(3) Our method does not assume knowledge on the number of malicious nodes, which is required
in the learning process of prior methods. (4) We experimentally demonstrate the effectiveness and
robustness of our defense on Byzantine attacks, including Gaussian attack, Sign Flipping attack and
Zero Gradient attack, in addition to a more subtle Label Flipping data poisoning attack. (5) An
experimental comparison between MANDERA and a collection of robust aggregation techniques
are provided. The computation times are also compared, demonstrating gains of MANDERA by
operating in the rank domain.
2 Defense Formalization
2.1	Notations
Suppose there are n local nodes in the federated learning framework, where n1 nodes are benign
nodes whose indices are denoted by Ib and the other n0 = n - n1 nodes are malicious nodes
whose indices are denoted by Im. The training model is denoted by f(θ, D), where θ ∈ Rp×1
is a p-dimensional parameter vector and D is a data matrix. Denote the message matrix received
from all local nodes by the central server as M ∈ Rn×p, where Mi,: denotes the message received
from node i. For a benign node i, let Di be the data matrix on it with Ni as the sample size, we
have Mi,： = f∂'D ∙ A malicious node j ∈ ‰, however, tends to attack the learning system by
2
Under review as a conference paper at ICLR 2022
manipulating Mj,： in some way. Hereinafter, We denote N* = min({Ni}i∈ib) to be the minimal
sample size of the benign nodes.
Given a vector of real numbers a ∈ Rp×1, define its ranking vector as b = Rank(a) ∈
perm{1, ∙ ∙ ∙ ,p}, where the ranking operator Rank maps the vector a to its permutation
space perm{1, ∙∙∙ ,p} which is the set of all the permutations of {1, ∙∙∙ ,p}. For example,
Rank(1.1, -2, 3.2) = (2, 3, 1). We adopt average ranking, when there are ties. With the Rank
operator, we can transfer the message matrix M to a ranking matrix R by replacing its column
M:,j by the corresponding ranking vector R:,j = Rank(M:,j). Further define
1p	1p
ei , P ΣS Ri,j	and	Vi , P ΣS(Ri,j - ei)2
to be the mean and variance of Ri,:, respectively. As it is shown in later subsections, we can judge
whether node i is a malicious node based on (ei, vi) under various attack types. In the following, we
will highlight the behaviour of the benign nodes first, and then discuss the behaviour of malicious
nodes and their interactions with the benign nodes under various Byzantine attacks respectively.
2.2	Behaviour of benign nodes
As the behaviour of benign nodes does not depend on the type of Byzantine attack, we can study the
statistical properties of (ei, vi) for a benign node i ∈ Ib before the specification ofa concrete attack
type. For any benign node i, the message generated for j th parameter is
M 1 X ∂f (θ, Di,ι)
Mi,j = Ni⅛
(1)
where Di,l denotes the lth sample on it. Throughout this paper, we always assume that Di,ls are
independent and identically distributed (IID) samples drawn from a data distribution D. Under the
independent data assumption, since Equation 1 tells us that Mi,j is the sample mean of IID random
variables, i.e., { f∂Di,l }Ni1, directly applying the Strong Law of Large Numbers (SLLN) and
Central Limit Theorem (CLT) leads to the lemma below immediately.
Lemma 1. Under the independent data assumption, further denote μj = E( "f∂D"l) and σj =
Var( f∂Diill) < ∞, with Ni going to infinity we have for ∀ j ∈ {1, ∙∙∙ ,p}
Mi,j → μj a.s. and Mij →N 1μj,σj/N6 .	(2)
2.3	Behaviour of malicious node under the Gaussian attack
Definition 1 (Gaussian attack). In a Gaussian attack, the attacker manipulates malicious nodes
to send Gaussian random messages to the global coordinator, i.e., {Mi,.}i∈ιm are independent
random samples from Gaussian distribution MVN(mb,:, Σ) , where mb,: =n1ι ∑i∈ib Mi,: and
Σ is the covariance matrix determined by the attacker.
Considering that Mij → μj almost surely (a.s.) with Ni going to infinity for all i ∈ I based on
Lemma 1, it is straightforward to see that IimN*→∞ m∙bj = μj a.s., and the distribution of Mi,j for
each i ∈ Zm converges to the Gaussian distribution centered at μj. Lemma 2 provides the details.
Lemma 2. Under the same assumption as in Lemma 1, with N* going to infinity, we have for each
malicious node i ∈ Im under the Gaussian attack that
Mi,j → N (μj, ∑j,j), 1 ≤ j ≤ p.	⑶
Lemma 1 and Lemma 2 tell us that for each parameter dimension j, {Mi,j }in=1 are independent
Gaussian random variables with the same mean (i.e, μj) but different variances (i.e., σj7Ni or £j,j)
under the Gaussian attack. Due to the symmetry of Gaussian distribution, it is straightforward to see
n+1
E(Rij) = —2—, 1 ≤ i ≤ n, 1 ≤ j ≤ p.
3
Under review as a conference paper at ICLR 2022
Moreover, the exchangeability of benign nodes and the exchangeability of malicious nodes when
N* is reasonably large tell us: for each parameter dimension j, there exist two positive constants
sb2,j and s2m,j such that
Var(Ri,j) = sb2,j, ∀ i ∈ Ib,	and	Var(Ri,j) = s2m,j, ∀i ∈ Im,
where both sb2,j and s2m,j are complex functions of σj2, Σj,j and {Ni}i∈Ib . Further assume that
Ri,j’s are independent of each other, thus ei = P Pp =1 Ri,j is the sum of independent random
variables with a common mean. Thus, according to the Kolmogorov Strong Law of Large Numbers
(KSLLN), we know that ei converges to a constant almost surely, which in turn indicates that vi also
converge some constant almost surely. The Theorem 1 summarizes the results formally, with the
detailed proof provided in Appendix C.
Theorem 1. Assuming {R：,j }ι≤j≤p are independent of each other, under the GaUSSian attack, we
have for each local node i that
n+1
lim lim ei =------- a.s.,
N*→∞ p→∞	2
lim lim (Vi — S2 ∙ I(i ∈ Ib) — Sm ∙ I(i ∈ Zm)) = 0 a.s.,
N*→∞ p→∞ '	,
where I(∙) Standsfor the indicator function, s2 , P Pp=ι s2,j and sS2m ,P pρ=ι sm,j.
(4)
(5)
Considering that 就 =Sm if and only if Σj,j ’s fall into a lower dimensional manifold whose mea-
surement is zero under the LebeSgUe measure, we have P(s2 = Sm) = 0 if the attacker specifies
the Gaussian variance Σj,j ’s arbitrarily in the Gaussian attack. Thus, Theorem 1 in fact suggests
that the benign nodes and the malicious nodes are different on the value of vi, and therefore pro-
vides a guideline to detect the malicious nodes. Although the We do need N* and P to go to infinity
for getting the theoretical results in Theorem 1, in practice the malicious node detection algorithm
based on the theorem typically works very well when N* and p are reasonably large and Ni’s are
not dramatically far away from each other.
The independent rank assumption in Theorem 1, which assumes that {R:,j }1≤j≤p are independent
of each other, may look restrictive. However, in fact it is a mild condition that can be easily satis-
fied in practice due to the following reasons. First, for a benign node i ∈ Ib, Mi,j and Mi,k are
often nearly independent, as the correlation between two model parameters θj and θk is often very
weak in a larger deep neural network with a huge number of parameters. To verify the statement,
we implemented independence tests for 100,000 column pairs randomly chosen from the message
matrix M generated from the FASHION-MNIST data. Distribution of the p-values of these tests
are demonstrated in Figure 2 via a histogram, which is very close to a uniform distribution, indi-
cating that Mi,j and Mi,k are indeed nearly independent in practice. Second, even some M:,j and
M:,k shows strong correlation, magnitude of the correlation would be reduced greatly during the
transformation from M to R, as the final ranking Ri,j also depends on many other factors. Actu-
ally, the independent rank assumption could be relaxed to be uncorrelated rank assumption which
assumes the ranks are uncorrelated with each other. Adopting the weaker assumption will result in a
change of convergence type of our theorems from the “almost surely convergence” to “convergence
in probability”, but with no essential influence to the our algorithm below.
Figure 2: Independence tests for 100,000 column pairs randomly chosen from message matrix M
generated from FASHION-MNIST data supports the independence assumption made in Theorem 1.
2.4 Malicious node detection for sign flipping attack
Definition 2 (Sign flipping attack). Sign flipping attack aims to generate the gradient values of
malicious nodes by flipping the sign of the average of all the benign nodes’ gradient at each epoch,
i.e., specifying Mi,： = —rmb,： for any i ∈ Im, where r > 0, mb =在 £破& Mk,：.
4
Under review as a conference paper at ICLR 2022
Based on the above definition, the update message of a malicious node i under the sign flipping
attack is
Mi,: = -rmb,: = — - ^X Mk,:.	(6)
1	k∈Ib
For fixed {Mk,： }痣丸,Mi,： is also a fixed vector without randomness, as it is a deterministic function
of {MkJk∈ib. On the other hand, however, we can also treat Mi,： as a random vector, since the
randomness of {Mk,： }云或 can be transferred to Mi,： via the link function in equation 6. In fact, for
any parameter dimension j, considering that Mk,j → N 1μj,σj∕Nk) for any k ∈ I according to
Lemma 1, it is straightforward to see that Mi,j = - Pk∈ι Mk,j Can also be well approximated
by a Gaussian distribution. The lemma 3 summarizes the result formally.
Lemma 3. Under the sign flipping attack, for each malicious node i ∈ Im and any parameter
dimension j, we have Mi,j 二 一木 ∑k∈ιb Mk,j is a deterministic function of {Mk,j }k∈τb, whose
limiting distribution when N * goes to infinity is
Mi,j →N(〃j(r),σ2(r)), 1 ≤ j ≤ p,
(7)
Μ∙σ2	_
where μj (r) = -rμj, σj (r)= 嬴Tj, and Nb = P
n1 ι is the harmonic mean of {Nk}k∈ib.
k∈Ib Nk
Lemma 1 and Lemma 3 tell us that for each parameter dimension j, the distribution of {Mi,j}in=1 is
a mixture of Gaussian components {N (μj, σj/Ni) }/或 centered at μj plus a point mass located
at μj(r) = -rμj. If Ni's are reasonably large, variances σj∕Nis would be very close to zero,
and the probability mass of the mixture distribution would concentrate to two local centers μj and
μj (r) = -rμj, one for the benign nodes and the other one for the malicious nodes. This intuition
provides us the guidance to identify the malicious nodes in this attack pattern. Transforming to
the rank domain, the above intuition leads to different behavior patterns of the benign nodes and
the malicious nodes in the rank matrix R, which in turn result in different limiting behavior of
(ei, vi ) for the benign and malicious nodes. The theorem 2 summarizes the results formally, with
the detailed proof provided in Appendix D.
Theorem 2.	With the same independent rank assumption as posed in Theorem 1, under the sign
flipping attack, we have for each local node i that
lim	lim ei	=	"b ∙ I(i	∈	Ib) +	μm ∙ I(i ∈ Im) a.s.,	(8)
N * →∞ p→∞
lim	lim Vi	=	s2 ∙ I(i	∈	Ib) +	Sm ∙ I(i ∈ Im) a.s.,	(9)
N* →∞ p→∞
where μb = n+n20+1 — n0ρ, μm = nip + n0+1, P = limp→∞ "j=11(μj >0) , Mm and s2 are both
quadratic functions ofρ whose concrete form also depends on n0 and n1 .
Considering that μb = "m, if and only if P = ɪ, and s2 = Mm if and only if P is the solution of a
quadratic function, the probability of (μb, s2) = (μm,, Mm) is zero as P → ∞. Such a phenomenon
suggests that we can detect the malicious nodes based on the moments (ei, vi) to defense the sign
flipping attack as well. Noticeably, we note that the limit behaviour of ei and vi does not dependent
on the specification of r, which defines the sign flipping attack. Although such a fact looks a bit
abnormal at the first glance, it is totally understandable once we realize that with the variance of
Mi,j shrinks to zero with Ni goes to infinity for each benign node i, any different between μj and
μj (r) would result in the same rank vector R：,j in the rank domain.
2.5	Malicious node detection for zero gradient attack
Definition 3 (Zero gradient attack). Zero gradient attack aims to make the aggregated message to
be zero, i.e., En=I Mi,： = 0, at each epoch, by specifying Mi,： = — n1 mb,： for all i ∈ Im.
Apparently, the zero gradient attack defined above is a special case of sign flipping attack by speci-
fying r = n^. Since the conclusions of Theorem 2 keep unchanged for different specifications of r
as we have discussed, we have the following corollary for zero gradient attack.
Corollary 1. Under the zero gradient attack, ei ’s and vi ’s follow exactly the same limiting be-
haviours as described in Theorem 2.
5
Under review as a conference paper at ICLR 2022
2.6	MANDERA
Theorem 1, 2 and Corollary 1 imply that, under these three attacks (Gaussian attack, zero gradient
attack and sign flipping attack), the first two moments of Ri,:, i.e., (ei, vi), converge to two different
limits for the benign nodes and the malicious nodes, respectively. Thus, for a real dataset where
Ni’s and p are all finite but reasonably large numbers, the scatter plot of {(ei, vi)}1≤i≤n would
demonstrate a clustering structure: one cluster for the benign nodes and the other cluster for the
malicious nodes. Figure 3 illustrates such a scatter plot for the 100 local nodes in a typical epoch
of training the FASHION-MNIST dataset under different FL settings (to keep the two dimensions
of the scatter plot to the same scale, We replaced Vi by its square root Si = √vi instead). Clearly, a
simple clustering procedure would detect the malicious nodes from the scatter plot. Based on this
intuition, We propose MAlicious Node DEtection via RAnking (MANDERA) to detect the malicious
nodes, Whose WorkfloW is detailed in Algorithm 1.
Algorithm 1 Malicious node detection via ranking (MANDERA)
Input: The message matrix M.
1:	Convert the message matrix M to the ranking matrix R by applying Rank operator.
2:	Compute mean and standard deviation of roWs in R, i.e., {(ei , si)}1≤i≤n.
3:	Run the clustering algorithm K-means to {(ei, si)}1≤i≤n With K = 2, and predict the set of
benign nodes With the lager cluster denoted by Ib .
Output: The predicted benign node set Ib.
Remark. MANDERA can be applied to either a single epoch or multiple epochs. For a single-epoch
mode, the input data M is the message matrix received from a single epoch. For multiple-epoch
mode, the data M is the column-concatenation of the message matrices from multiple epochs. By
default, the experiments below all use single epoch to detect the malicious nodes.
The predicted benign nodes Ib obtained by MANDERA naturally leads to an aggregated message
r^b，： = #(：)Pi∈^ Mi,：. The theorem 3 shows that Ib and mb lead to consistent estimations of
Ib and mb respectively, indicating that MANDERA enjoys robustness guarantee (Steinhardt, 2018)
for typical Byzantine attacks.
Theorem 3.	Under the three typical Byzantine attacks, i.e., Gaussian attack, sign flipping attack
and zero gradient attack, we have:
lim lim P(Ib = Ib) = I and lim lim E||mb ： — mb ： ∣∣2 = 0.	(10)
N*→∞ p→∞	N*→∞ p→∞	,	,
The proof of Theorem 3 can be found in Appendix E.
3	Experiments
We evaluate the efficacy in detecting malicious nodes within the federated learning framework with
the use of three Datasets. The first is the FASHION-MNIST dataset (Xiao et al., 2017), a dataset of
60,000 and 10,000 training and testing samples respectively divided into 10 classes of apparel. The
second is CIFAR-10 (Krizhevsky et al., 2009), a dataset of 60,000 small object images also con-
taining 10 object classes. The third is MNIST (Deng, 2012) dataset which appears in Appendix H.
In these experiments we mainly adopt implementations of Byzantine attacks released by (Wu et al.,
2020b;a) and the label flipping attack (Tolpegin et al., 2020b;a). The label flipping attack is a data
poisoning attack that alters one or more labels of training data to an attacker’s pre-determined target
label. For example, in CIFAR-10’s object labels, an attacker may change the labels of their local
cat images to be labelled as dogs. We use the Label Flipping attack as a comparative poisoning
attack that achieves its objective in a more subtle manner. In our experiments, we set Σ = 30I for
the Gaussian attack and r = 3 for the sign flipping attack, where I is the identity matrix. For all
experiments we fix n = 100 participating nodes, of which a variable number of nodes are poisoned
|n0 | ∈ {5, 10, 15, 20, 25, 30}. The training process is run until 25 epochs have elapsed. We have
described the structure of these networks in Appendix A.
3.1	Illustration of the average ranking and standard deviation of ranking
Section 2 speculated that the distribution of parameter ranks differ sufficiently for the detection of
malicious and benign nodes. We validate this hypothesis in Figure 3 by illustrating the difference
6
UnderreVieW as a ConferenCe PaPersICLR 2022
SD
222333 2 3 3 4	2
789⊂>ι2 5 O 5 O 5
Mean
Figure W The SCatter plots Of (Qr-for the IOO IlOdeS UlIder four types Of attack as≡ustmtive ex—
ames demonstrating ranking mean and VarianCe from the ISt epoch Of training for the FASHlON—
MST dataset∙
Score
OOOO 1。。。	O	-ʌ O O	O	。1。。	。。1
o M ⅛ --j o o k> tπ o o kɪ cn ⊂> o Kj in -'j o
0 505 O O Ui O	Ui	O O cπ	O	5005	050
Λoeιnooy
Mefro'ACCUraCy EB ReCa= E^Prec-S-On Fl
ιι≡oθy
O1-1_01_52_0 2_53-0ɑl-lb 1_52_02_53_0O1-1_01_52_02_5 3_0
NUmber Of ma=c~ous nodes
Aoainoov
HIH"
∙-4U-^
≡HP→
-4J}→
-4ZU-
-Q-
GA ZG SF
Mefro'ACCUraCy EB ReCa= PreC-S-on FI
(a) CIFAR—1。 (b) FASHlON—MNIST
Figure 舟 ClaSSifiCation PerfOnnaIICe Of OUr ProPOSed approach MANDERA (AIgOrithm 1) UIlder
four types Of attack for CIFAR—10 and FARHlON—MNIST data. GaUSSiaIl AttaCk (GA)； zero—
Gmdient (ZG)； SigiI—ippilIg (SF)； and LabeFlipping (LFThe boxot bounds the 25th (Ql)
and 75th (Q3) PerCelItiI尸 With the Centra IHIle representing the 5Oth quantile (medianThe end
Pots Ofthe WlIiSkerrePreSentthe Q1IL5(QQ1) and Q3+L5(QQ1) respectively，
between the benign IlOdeS and malicious IIOdeSiIl terms Ofthe mean Of gradient-rank5'gs and the
Standard deviation Of gradientrank
It Can be ObSerVed from Figure 3 thaLUnder GaUSSiaIl and LabeI flipping attackthe average rank—
ings Of malicious IIodeS are Of a similar distributIItO benign IlOdeIt is PrObIematiCfOr diSt—
guishg between the two types OflIodeif 0y average ranking information is used，On the Other
handy Figure 3 displays a Iarger SeParation Of distributions for the Standard deviation Of rankinIt is
IIOted that all 4 attacks ObSerVe a COnVergenCe Ofthe distributionS as the IIUmber OfmaliCus IIOdeS
increasincreasing the difficulty of defense for both MANDERA and all Other defenseHow—
every the IikeHhOod Of an attacker COIltrHng increasingly Iarge IIUmberS Of malicious IlOdeS also
decrease.
Under review as a conference paper at ICLR 2022
3.2	Malicious node detection by MANDERA
We test the performance of MANDERA on the update gradients of a model under attacks. In this
section, MANDERA acts as an observer without intervening in the learning process to identify
malicious nodes with a set of gradients from a single epoch. Each configuration of 25 training
epochs, with a given number of malicious nodes was repeated 20 times. Figure 4 demonstrates the
classification performance (Metrics defined in Appendix B) of MANDERA with different settings
of participating malicious nodes and the four poisoning attacks of Guassian Attack (GA), Zero
Gradient attack (ZG), Sign Flipping attack (SF) and the Label Flipping attack (LF).
While we have formally demonstrated the efficacy of MANDERA in accurately detecting poten-
tially malicious nodes participating in the federated learning process. In practice, to leverage an
unsupervised K-means clustering algorithm, we must also identify the correct group of nodes as the
malicious group. Our strategy is to identify the group with the most exact gradients, or otherwise
the smaller group (we regard a system with over 50% of their nodes compromised as having larger
issues than just poisoning attacks) 1. We also test other clustering algorithms, such as hierarchical
clustering and Gaussian mixture models (Fraley & Raftery, 2002). It turns out that the performance
of MANDERA is quite robust with different choices of clustering methods. Detailed results can be
found in Appendix F.
From Figure 4, it is immediately evident that the recall of the malicious nodes for the Byzantine at-
tacks is exceptional. However, occasionally benign nodes have also been misclassified as malicious
under a SF, and to a lesser extent the ZG attack for both datasets. On all attacks, in the presence of
more malicious nodes, the recall of malicious nodes trends down. As for the data poisoning attack
of LF, itis consistently more difficult to detect, however we note that the LF attack has a more subtle
influence on the model in contrast to the impact of Byzantine attacks.
3.3	MANDERA for defending against poisoning attacks
In this section, we encapsulate MANDERA into a module prior to the the aggregation step, MAN-
DERA has the sole objective of identifying malicious nodes, and excluding their updates from the
global aggregation step. Each configuration of 25 training epochs, a given poisoning attack, defense
method, and a given number of malicious nodes was repeated 10 times. We compare MANDERA
against 5 other robust aggregation defense methods, Krum (Blanchard et al., 2017), Bulyan (Guer-
raoui et al., 2018), Trimmed Mean (Yin et al., 2018), Median (Yin et al., 2018) and FLTrust (Cao
et al., 2020). Of which the first 2 requires an assumed number of malicious nodes, and the latter 3
only aggregate robustly.
Defence
—KrUm
--NO-attack
BUlyan
Median
Trim-mean …∙ FLTrUst
MANDERA
Defence
KrUm — BUlyan — Trim-mean …∙ FLTrUst
NO-attack ■■■- Median — MANDERA
0 5 10 1520 250 5 10 15 20250 5 10 1520250 5 10 15 20 250 5 10 1520250 5 10 1520 25
Number of Epoch
(a) CIFAR-10 Dataset	(b) FASHION-MNIST dataset
Figure 5: Model Accuracy at each epoch of training, each line of the curve represents a different
defense against the poisoning attacks.
From Figure 5, it is observed that MANDERA performs about the same as the best performing
defense mechanisms, close to the performance of a model not under attack. MANDERA’s accuracy
1More informed approaches to selecting the malicious cluster can be tested in future work. E.g. Figure 3
displays less variation of rank variance in malicious cluster compared to benign nodes. This could robust
selection of the malicious group, and enabling selection of malicious groups larger than 50%.
8
Under review as a conference paper at ICLR 2022
is observed to vary slightly under the LF attack on fashion data with 30 malicious nodes, this is
consistent with the larger accuracy ranges previously observed in Figure 4b. Interestingly, FLTrust
as a standalone defense is weak in protecting against the most extreme Byzantine attacks. However,
we highlight that FLtrust is a robust aggregation method against targeted attacks that may thwart
defences like Krum, Trimmed mean. We see FLTrust as a complementary defence that relies on a
base method of defence against Byzantine attacks, but expands the protection coverage of the FL
system against adaptive attacks.
3.4 Computational Efficiency
We have previously been able to observe that MANDERA can perform at par with the current
highest performing poisoning attack defenses. Another benefit arises with the simplification of
the mitigation strategy with the introduction of ranking at the core of the algorithm. Sorting and
Ranking algorithms are fast. Additionally, we only apply clustering on the two dimensions of rank
mean and standard deviation, in contrast to other works that seek to cluster on the entire node
update (Chen et al., 2021). The times in Table 1 for MANDERA, Krum and Bulyan do not include
the parameter/gradient aggregation step. These times were computed on 1 core of a Dual Xeon
14-core E5-2690, with 8 Gb of system RAM and a single Nvidia Tesla P100. Table 1 demonstrates
that MANDERA is able to achieve a faster speed than that of single Krum 2 (by more than half) and
Bulyan (by an order of magnitude).
Table 1: Mean and standard deviation of computational times for defense function given the same
set of gradients from 100 nodes, of which 30 were malicious. Each function was repeated 100 times.
Defense (Detection)	Mean ± SD (ms)	Defense (Aggregation)	Mean ± SD (ms)
MANDERA	643 ± 8.646	Trimmed Mean	-3.96±0.41-
KrUm (Single)	1352 ± 10.09	Median	-9.81±3.88-
BUlyan	27209 ± 2334-	FLTrust	361±4.07~~
4 Discussion and Conclusion
If attackers create more adaptive attacks unlike Definition 1, 2 and 3, they may evade MANDERA
and achieve model poisoning. In this work, we have configured our Federated Learner to use all 100
nodes in the learning process at every round, we acknowledge FL framework may learn the global
model only using subset of nodes at each round. In these settings MANDERA would still function,
as we would rank and cluster on the parameters of the participating nodes, without assuming any
number of poisoned nodes. In Algorithm 1, performance could be improved by incorporating higher
order moments. MANDERA is unable to function when gradients are securely aggregated in its
current form. However, malicious nodes can be identified and excluded from the secure aggregation
step, while still protecting the privacy of participating nodes by performing MANDERA through
secure ranking (Zhang et al., 2013; Lin & Tzeng, 2005) (recall that MANDERA only requires the
ranking matrix to detect poisoned nodes). It remains to be seen the effectiveness of MANDERA on
more advanced poisoning techniques like adversarial poisoning or Evasion attacks.
In conclusion, we have provided theoretical guarantees and experimentally shown efficacy in the
use of ranking algorithms for the detection of malicious nodes performing poisoning attacks against
federated learning. Our proposed method MANDERA, is able to achieve high detection accuracy
and maintain a model accuracy on par with other seminal, high performing defense mechanisms, but
with three notable advantages. First, provable guarantees for the use of ranking to detect Gaussian,
Zero Gradient and Sign Flipping attacks. Next, faster detection with the use of ranking algorithms.
Finally, the MANDERA defense does not need a prior estimation of the number of poisoned nodes.
In this work we demonstrate how the rank domain can be useful in applications to defend against
malicious actors.
Ethics Statement
The core objective of our research is to provide an additional means of defense against poisoning
nodes that target Federated Learning. To test our defense we have implemented different attacks
against the Federated Learning framework. Attackers may adopt our defense strategy to design
new poisoning attacks. Fortunately, these poisoning attacks can not be leveraged to leak private
information from Federated learning models, instead only impact its performance.
2The use of multi-krum would have yielded better protection (c.f. Section 3) at the behest of speed.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
To ensure reproducible research, we have supplemented our proposal for MANDERA, by supplying
both R and Python implementations of MANDERA used in this paper, uploaded with the remainder
of the experiment code. The three datasets featured in this paper is CIFAR-10 (Krizhevsky et al.,
2009), Fasion-MNIST (Xiao et al., 2017), and MNIST (Deng, 2012); we have used each of these
dataset unaltered from their respective sources. We have stated the assumptions in our theorems
and their proofs can be found in the Appendix. But to explain our assumptions in simple terms, (1)
The data samples on each local node are independently drawn from the same distribution. (2) The
gradient value for each parameter is independent to each other.
References
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938-2948. PMLR, 2020.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Ma-
chine learning with adversaries: Byzantine tolerant gradient descent. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf.
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust feder-
ated learning via trust bootstrapping. arXiv preprint arXiv:2012.13995, 2020.
Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Provably secure federated learning against
malicious clients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pp. 6885-6893, 2021.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial set-
tings: Byzantine gradient descent. Proc. ACM Meas. Anal. Comput. Syst., 1(2), December 2017.
doi: 10.1145/3154503. URL https://doi.org/10.1145/3154503.
Zheyi Chen, Pu Tian, Weixian Liao, and Wei Yu. Zero knowledge clustering based adversarial
mitigation in heterogeneous federated learning. IEEE Transactions on Network Science and En-
gineering, 8(2):1070-1083, 2021. doi: 10.1109/TNSE.2020.3002796.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
Signal Processing Magazine, 29(6):141-142, 2012.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to
byzantine-robust federated learning. In 29th {USENIX} Security Symposium ({USENIX} Se-
curity 20), pp. 1605-1622, 2020.
Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density
estimation. Journal of the American statistical Association, 97(458):611-631, 2002.
Rachid Guerraoui, Sebastien Rouault, et al. The hidden vulnerability of distributed learning in
byzantium. In International Conference on Machine Learning, pp. 3521-3530. PMLR, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients
for robust federated learning. arXiv preprint arXiv:2002.00211, 2020.
Hsiao-Ying Lin and Wen-Guey Tzeng. An efficient solution to the millionaires’ problem based on
homomorphic encryption. In International Conference on Applied Cryptography and Network
Security, pp. 456-466. Springer, 2005.
10
Under review as a conference paper at ICLR 2022
JinhyUn So, BaSak Guler, and A. Salman Avestimehr. Byzantine-resilient secure federated learning.
IEEE Journal on Selected Areas in Communications, 39(7):2168-2181, 2021. doi: 10.1109/
JSAC.2020.3041404.
Jacob Steinhardt. Robust learning: Information theory and algorithms. PhD thesis, Stanford Uni-
versity, 2018.
Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against
federated learning systems - github. https://github.com/git-disl/DataPoisoning_FL, 2020a.
Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against
federated learning systems. In European Symposium on Research in Computer Security, pp. 480-
501. Springer, 2020b.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Byrd-saga - github.
https://github.com/MrFive5555/Byrd-SAGA, 2020a.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variance-reduced
stochastic gradient descent with robustness to byzantine attacks. IEEE Transactions on Signal
Processing, 68:4583-4596, 2020b.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. 2017.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In International Conference on Machine Learning, pp. 6893-
6901. PMLR, 2019.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd. In Interna-
tional Conference on Machine Learning, pp. 10495-10503. PMLR, 2020.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650-5659. PMLR, 2018.
Lan Zhang, Xiang-Yang Li, Yunhao Liu, and Taeho Jung. Verifiable private multi-party computa-
tion: ranging and ranking. In 2013 Proceedings IEEE INFOCOM, pp. 605-609. IEEE, 2013.
A Neural Network configurations
We train these models with a batch size of 10, an SGD optimizer operates with a learning rate of
0.01, and 0.5 momentum for 25 epochs. The accuracy of the model is evaluated on a holdout set of
1000 samples.
A.1 FASHION-MNIST and MNIST
•	Layer 1: 1 * 16 * 5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•	Layer 2: 16 * 32* 5,2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•	Output: 10 Classes, Linear.
A.2 CIFAR- 1 0
•	Layer 1: 1 * 32 * 3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•	Layer 2: 32*32*3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•	Output: 10 Classes, Linear.
11
Under review as a conference paper at ICLR 2022
B Metrics
The metrics observed in Section 3 to evaluate the performance of the defense mechanisms are de-
fined as follows:
Precision
Accuracy
Recall
F1 = 2 ×
_ TP
=TP+FP,
_ TP+TN
=TP+FP+FN+TN,
TP
TP+FN,
Precision × Recall
Precision+Recall .
C Proof of Theorem 1
Proof. Because {Ri,j}1≤j≤p are independent random variables with a finite upper bound (since n
is fixes) as assumed, direct application of KSLLN leads to
1p
lim 一 ɪ2 (Rij — E(Rij)) = 0 a.s.,
p→∞ p j =1
1p
lim — ^X [(Rij- E(Rij)) - Var(Rij)] = 0 a.s..
p→∞ p j=1
(11)
(12)
To prove Theorem 1 based on Equation 11 and 12, we need to derive the concrete form of E(Ri,j)
and Var(Ri,j).
Fortunately, because Mij → N (μj, Σjj for ∀ i ∈ Zm and Mij → N (μj, σj-∕Ni) for ∀ i ∈I
when N* → ∞, it is straightforward to see due to the symmetry of Gaussian distribution that
n+1
lim E(Rij) = ~^~, 1 ≤ i ≤ n, 1 ≤ j ≤ p.
N*→∞	2
(13)
Moreover, assuming that the sample sizes of different benign nodes approach to each other with N*
going to infinity, i.e.,
lim —ɪ- max |Ni — Nk | = 0,
N* →∞ N* i,k∈Ib	i k
(14)
for each parameter dimension j, {Mi,j }i∈Ib would converge to the same Gaussian distribution
N(μj,σj∕N*) with the increase of N*. Thus, due to the exchangeability of {Mi,j}i∈4 and
{Mi,j}i∈Im, it is easy to see that there exist two positive constants sb2 and s2m, such that
lim Var(Rij) = s2 ∙ I(i ∈ Ib) + Smj ∙ I(i ∈ Im),
N* →∞	m,
(15)
where sb2,j and s2m,j are both complex functions of n0, n1, σj2, Σj,j and N*, and sb2,j = s2m,j if and
only if σj∕N * = Σjj.	'	'
Combining Equation 11 and 13, we have
1 p	n+1
lim lim ei = lim lim 一	Rij =-------a.s.,
N*→∞ p→∞	N*→∞ p→∞ p	,	2
j=1
i.e., Equation 4, which further indicates that ei and E(Ri,j) share the same limit when both p and
N* go to infinity. Thus, we have
lim lim vi
N*→∞ p→∞
1p
lim lim -	(Rij — £i/
N* →∞ p→∞ P —*, \	,	,
j=1
1p	2
lim lim 一E(Rij- E(Rij))2 a.s..
p	p j=1
(16)
12
Under review as a conference paper at ICLR 2022
Combining Equation 12, 15, and 16, we have
lim lim (Vi — S2 ∙ I(i ∈ Zb) — Sm ∙ I(i ∈ Zm)) = 0 a.s.,
N*→∞ p→∞ '	,
i.e., Equation 5. Thus, the proof is complete.	口
D Proof of Theorem 2
Proof. It is straightforward to see that equation 11 also holds for sign flipping attack under the
assumptions of Theorem 2. But, we need to re-calculate E(Ri,j) for benign and malicious nodes
under the new setting.
Under the sign flipping attack, because Mi,j → N(μj(r),σj(r)∖ for ∀ i ∈ Zm and Mi,j →
N (μj ,σj∕Ni) for ∀ i ∈ Zb when N * → ∞, and
lim (σj/Ni)= lim σ2(r)=0,
N *→∞ j	N *一∞ j
it is straightforward to see that
lim P(Mi,j > Mkj) = I(μj > 0), ∀ i ∈Zb,∀ k ∈ Zm,
N*→∞
which further indicates that
l*im E(Ri,j)
N*→∞
lim E(Ri,j)
N*→∞
nι + 1
-2-
no + 1
-2-
・ I(i ∈ Zb) +
n + n1 + 1
•	I(i ∈ Zm ) +
2
n+ n0 + 1
•	I(i ∈ Zm) if μj > 0,
•	I(i ∈ Zb) if μj < 0；
(17)
where S[2a,b]
lim
N* →∞
lim
N* →∞
1
E(R2,j) =	S2ι,nι]	• I(i ∈	Zb) + S2nι + 1,n] •	I(i ∈ Zm)	if	〃j	> 0,
E(R2,j) =	S2l,no]	• I(i ∈	Zm) + S[2no + 1,n]	• I(i ∈ Zb)	'if	〃j	< 0,
(18)
b—a+1
Pbk=ak2.
2
Combining Equation 11 and 17, we have
ρ P • n+n2l + 1 +(1 — P) • n0+1 = μm a.s., if i ∈ Zm,
lim lim ei =
N*→∞ p→∞ P P • n2+1 + (1 — ρ) • n+n20 + 1 = μb a.s., if i ∈ Zb,
where ρ = limp→∞
Pp= I(μj>0)
, i.e., Equation 8.
p
Define μi = μm • I(i ∈ Zm) + μb • I(i ∈ Zb). Based on KSLLN, We have:
1p
lim —〉： [(Ri,j — "i)2 — E(Ri,j — "i)2] = 0 a.s..
p→∞ p j=1
As we have proved in Equation 8 that
lim lim ei
N* →∞ p→∞
μi a.s.,
we have
1p
lim lim 一	(Rij — eQ2
N* →∞ p→∞ p	,
—E(Rij- μi )2] = 0 a.s.,
which implies that
1p	1p
lim lim Vi = lim lim 一 (R(Rij — ei)2 = lim lim -	E(Rij — μi)2
N* →∞ p→∞	N* →∞ p→∞ p	,	N* →∞ p→∞ p	,
j=1	j=1
a.s..
13
Under review as a conference paper at ICLR 2022
Considering that
1p
lim lim -E(Rij —也)2
N* →∞ p→∞ PjJ ,j
1p
=pl→∞ N*im∞ P X (E(R2,j) — 2μiE(Ri,j) + (也)2)
p j=1
=[τm —	(μm)2]	∙ I(i	∈	Im) +	[τb	—	(μb)2]	∙ I(i ∈ Ib),
where
Tb = P ∙ S[1,nι] 十 (I - P) • S[n0+1,n] ,
Tm = P ∙ S2nι+1,n] + (I - P)，S2i,no].
we have
lim lim Vi = ITm — ("m)2] ∙ I(i ∈ Im) + ITb — (μ⅛)2] ∙ I(i ∈ Ib).
N* →∞ p→∞
It completes the proof of Equation 9 by specifying s2 = Tb — (“b)2 and 或=Tm — (μm,)2.	□
E Proof of Theorem 3
Proof. According to Theorem 1, 2 and Corollary 1, when both N* and P are large enough, with
probability 1 there exist (eb, vb), (em, vm) and δ > 0 such that ||(eb, vb) — (em, vm)||2 > δ, and
∣∣(ei,Vi) — (eb,Vb)∣∣2 ≤ 2 for ∀ i ∈ Ib and ∣∣(ei,Vi) — (em,Vm)∣∣2 ≤ | for ∀ i ∈ Im.
Therefore, with a reasonable clustering algorithm such as K-mean with K = 2, we would expect
Ib = Ib with probability 1.
Because we can always find a ∆ > 0 such that ||Mi,: — Mj,: ||2 ≤ ∆ for any node pair (i, j) in a
fixed dataset with a finite number of nodes, and mb,： = mb,： when Ib = Ib, We have
E||mb,： — mb,：||2 ≤ ∆ ∙ P(Ib = Ib),
and thus
lim lim E||mb,： — mb,：||2 = 0.
N* →∞ p→∞
It completes the proof.	□
F MANDERA performance with different clustering algorithms
In this section, Figure 6 demonstrate that the discriminating performance of MANDERA when
hierarchical clustering and Gaussian mixture models are used in-place of K-means for FASHION-
MNIST data set remain robust.
G Model Losses on CIFAR-10 and FASHION-MNIST data
Figure 7 presents the model loss to accompany the model prediction performance of Figure 5 previ-
ously seen in Section 3.
H Model performance on MNIST data
In this section we replicate experiments that were previously performed in Section 3 on the
MNIST (Deng, 2012) dataset. The MNIST dataset is a dataset of 60,000 and 10,000 training and
testing samples respectively divided into 10 classes of handwritten digits from multiple authors.
Figure 8 contains the performance characteristics of MANDERA’s defense against the four attacks,
whilst Figure 9 contains the comparative accuracy and loss when the different defenses are applied.
Generally speaking, the observations previously observed continue to hold for this dataset.
14
UlIderreVieW as a COnferenCe PaPer at ICLR 2022
--00
075
O5。
O25
0-00
10。
075
O5。
0.25
oreOoo
Sc--00
0.75
O5。
O25
0-00
--00
075
O5。
O25
0-00
5 1015202530 5 10 15202530 5 10 15202530 5 10 1520253。
NUmber Of ma=C-OUS nodes
(a) GaUSSian miXrUre modeL
Mefr-C≡ACCUraCy>ReCa=-TD-PreC-S-On>F」
Me≡c≡ACCUraCy≡ReCa=>Precision>2

uoisi□θjc∣ ∣∣e□θy Λ□ej∏□□∀
NUmber Ofma=CioUS nodes
(b) HierarChiCaI CIUSSr5'g∙
Figure 6 csrssificat5'n PerfOnnanCe Of OUr PrOPOSed approach MANDERA (AlgOrithm 1) With
Other CIUSterilIg algorithms UlIderfOUrtyPeS Of attack for FASHlON—MNIST datGAiGaUSSian
attack; NQ- ZerO—gradient attack; SF SiglI—flipping; and LF Labenipping，The boxplot bounds the
25th (Ql) and 75th (Q3) PerCentilWith the Central Hne representing the 5Oth quantile (median).
The end Pots Ofthe WhiSker represen 二 he Q1IL5(QQ1) and Q3+L5(QQ1) respectively
15
Under review as a conference paper at ICLR 2022
—Krum — BUIyan — Trim-mean …∙ FLTrust
DefenCe
NO-attack Median — MANDERA
0 5 101520250 5 10 1520250 5 101520250 5 101520250 5 101520250 5 10152025
Number of Epoch
(a) CIFAR-10
— Krum — BUIyan — Trim-mean ■ ■ ■ ■ FLTrust
DefenCe
NO-attack Median — MANDERA
2.25
2.00
1.75
1.50
1.25
0 5 101520250 5 101520250 5 101520250 5 101520250 5 101520250 5 10152025
Number of Epoch
(b) FASHION-MNIST
Figure 7: Model Loss at each epoch of training, each line of the curve represents a different defense
against the attacks (GA: Gaussian attack; ZG: Zero-gradient attack; SF: Sign-flipping; and LF:
Label-flipping).
16
UlIderreVieW as a COnferenCe PaPer at ICLR 2022
Score
Mef「5 ACCUraCy ReCa= PreCisiOnmF一
o
o
I
ooroσι^ιoobjσι-^j
ooσιoσιoocnoσι
SF
1;
uoιspΘJc] neɔə^ Λoejn∞v
0∙75 —
950 —
0∙25 —
0.00— ------ ------ ------
Cn-≤--5 2.0 2.53.0Cn-≤--5 2.02.5 3.0Cn-≤--5 2.0 2.5 3.0Cn-≤--5 2.0 2.5 3・0
NUmbSr Ofma--C-OUS nodes
FigUre∞QaSS≡ca 一o∙n PerfOrmanee Of OUr PrOPOSed approach MANDERA (AIgOri 一 hm I) under
fou二ypes Of attack for MNISTda一a∙ GA- GaUSSian attack; ZG- ZerO—gradienr attack; SF- sign—
Ripping; and LF- LabeI⅛ipping∙ The boXPor bounds rhe 25一h (QI) and 75一h (Q3) percent!^Widl
一 he Central Hne representing rhe 52h quan≡e (median)，The end p2.nrs Ofrhe WhiSkerrePreSen 二 he
Q1—L5(Q3—Q1) and Q3+L5(Q3O1) TeSPeCriVely.
17
Under review as a conference paper at ICLR 2022
Aoaιnoo∖/
—Krum — BUIyan — Trim-mean …∙ FLTrUSt
Defence
NO-attack ■■■■ Median — MANDERA
0 5 10 15 20 250 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25
Number of Epoch
(a)	Model prediction accuracy after defense
(SSol)6O_
—KrUm — BUIyan — Trim-mean …∙ FLTrUSt
Defence
NO-attack ■■■■ Median — MANDERA
0 5 10 15 20 250 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25
Number of Epoch
(b)	Model prediction loss after defense
Figure 9: Model Accuracy and Loss for MNIST data at each epoch of training, each line of the curve
represents a different defense against the attacks (GA: Gaussian attack; ZG: Zero-gradient attack;
SF: Sign-flipping; and LF: Label-flipping).
18