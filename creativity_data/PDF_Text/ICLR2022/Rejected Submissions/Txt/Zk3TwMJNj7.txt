Under review as a conference paper at ICLR 2022
Directional Bias Helps Stochastic Gradi-
ent Descent to Generalize in Nonparametric
Model
Anonymous authors
Paper under double-blind review
Ab stract
This paper studies the Stochastic Gradient Descent (SGD) algorithm in kernel re-
gression. The main finding is that SGD with moderate and annealing step size
converges in the direction of the eigenvector that corresponds to the largest eigen-
value of the gram matrix. On the contrary, the Gradient Descent (GD) with a
moderate or small step size converges along the direction that corresponds to the
smallest eigenvalue. For a general squared risk minimization problem, we show
that directional bias towards a larger eigenvalue of the Hessian (which is the gram
matrix in our case) results in an estimator that is closer to the ground truth. Adopt-
ing this result to kernel regression, the directional bias helps the SGD estimator
generalize better. This result gives one way to explain how noise helps in gen-
eralization when learning with a nontrivial step size, which may be useful for
promoting further understanding of stochastic algorithms in deep learning. The
correctness of our theory is supported by simulations and experiments of Neural
Network on the FashionMNIST dataset.
1	Introduction
In this paper, we study the Stochastic Gradient Descent (SGD) algorithm in a nonparametric re-
gression model. Among nonparametric models, one popular choice in both statistics and machine
learning communities is the kernel model that is generated by a Reproducing Kernel Hilbert Space
(RKHS). When fitting kernel models to the data, SGD is computationally efficient as compared to
Gradient Descent (GD) (Ma et al., 2018). This motivates us to analyze the properties of SGD in the
kernel model, especially for SGD with a nontrivial step size for a practical reason. In particular, we
aim to provide a fundamental explanation of why SGD estimators generalize well.
Our work is inspired by Wu et al. (2021), which shows that directional bias has a significant impact
on the generalization property in the linear regression model. We adopt a similar concept of the
directional bias, but generalize them to the nonparametric model. Our result is a non-trivial extension
of their approach, due to the difference in our problem setting and our SGD algorithm design. To
the best of our knowledge, we are the first to show the directional bias phenomenon of SGD and
analyze how it helps generalization in nonparametric regression.
Our contributions are two folded. First, we study the directional bias of (S)GD in a nonparametric
regression model. On the one hand, nonparametric regression is well studied in both statistics and
machine learning. On the other hand, the directional bias is a relatively new concept (Wu et al.,
2021) of an algorithm that affects the statistics properties, and there is no thorough understanding
of the directional bias of (S)GD algorithms for the nonparametric regression model. Note that our
result is closely related to those in Belkin et al. (2018); Liang & Rakhlin (2020): they prove that
SGD and GD algorithms both converge to the minimum norm interpolate, where the same properties
are discovered for SGD and GD; whereas we investigate the solution paths before their convergence,
and show that SGD and GD have different solution paths that lead to different properties. Our result
helps to explain why the SGD generalizes better than GD.
Our second contribution is to unify the conditions to show the directional bias of (S)GD sequences
in nonparametric models. The main condition is the diagonal dominant gram matrix, which covers
a large class of kernel functions and allows us to study their properties. Moreover, our SGD is
1
Under review as a conference paper at ICLR 2022
different from those in Wu et al. (2021): they define SGD in epochs while we define SGD in steps.
The fundamental difference in the SGD algorithm requires us to develop different techniques for
analyzing the SGD sequence and showing its directional bias, which has not yet been covered in the
current literature.
Main Theorems of this paper can be divided into two parts, briefly summarized as follows:
First is the directional bias of SGD. Theorem 5 shows that for a two-stage SGD with a moderate
step size in the first stage and a small step size in the second stage, an early-stopped estimator has
a directional bias towards the eigenvector that corresponds to the largest eigenvalue of the gram
matrix. Later we refer to this direction as the direction towards the largest eigenvector to simplify
the statement.
As a comparison, Theorem 7 shows that GD with both moderate and small step sizes has a directional
bias towards the eigenvector that corresponds to the smallest eigenvalue of the gram matrix (denote
it as the direction towards the smallest eigenvector). From which, we conclude that SGD and GD
have different direction biases in kernel regression.
Second is the implication of directional bias. The implication is very useful since it quantifies the
effect of the directional bias on the generalization error. Theorem 9 considers a general problem
of quadratic loss. It shows that the estimator biased towards the largest eigenvector of the Hes-
sian (which is the gram matrix in our nonparametric regression) can have the smallest parameter
estimation error, when compared with other estimators of the same loss.
With this high-level idea of directional bias helps generalization, Theorem 11 compares the gener-
alization error of SGD and GD in our problem setting. In particular, it upper bounds the generaliza-
tion error of SGD and lower bounds the generalization error of GD. By directly comparing the error
bounds, we guarantee that the generalization error of the SGD estimator is smaller than that of the
GD estimator with high probability.
We also point out that our result might shed new light on deep learning (Belkin et al., 2018). By
the state-of-the-art mathematical theory of Neural Networks (NN), kernel and/or nonparametric
methods can approximate the functional space of neural networks, see for example the NTK theory
(Jacot et al., 2018), and the Radon bounded variance space description for ReLU NN (Parhi &
Nowak, 2021). Our technique might allow one to characterize the SGD solution path and show the
generalization property in those problem settings.
Paper organization. The rest of the paper is as follows: In Section 2, we review some rele-
vant literature; In Section 3, we give the formulation of the nonparametric regression and define the
optimization problem. We also formalize the algorithm that is considered in this work and make
assumptions to analyze the algorithms; In Section 4, we state our main theory on the directional
bias of SGD/GD in nonparametric regression, where we include both the directional bias result and
the implication of the directional bias for generalization. Experiments are conducted to support our
theory; In Section 5, we discuss the finding in this paper, and propose some future research topic.
All the proof, experiment details, and more experiments are deferred to the appendix due to page
limits.
2	Literature Review
In this section, we review some relevant works. For better understanding, we split into two subsec-
tions: Subsection 2.1 reviews the background of RKHS; Subsection 2.2 presents the state-of-the-art
technique for analyzing the directional bias of the (S)GD algorithm.
2.1	RKHS
Kernel methods are among the core algorithms in machine learning and statistics (Bartlett et al.,
2021). As proposed by Wahba (1990), the kernel method and RKHS serve as a unified framework
of a group of nonparametric models, which extends the spline method. Later, kernel models become
an important component in nonparametric models. In machine learning, the kernel-based method is
always referred to as the “kernel trick”. By lifting the x variable to a high dimensional space via
the kernel method, we can explore possibly nonlinear relationships between variables. Moreover, to
2
Under review as a conference paper at ICLR 2022
play the kernel trick, one can directly calculate the kernel function using original features. This is
computationally efficient since we avoid calculating high dimension or infinite dimension features.
For applications of kernel method in machine learning, one can see kernel regression for image
processing (Takeda et al., 2007), for text mining (Greene & Cunningham, 2006), and for tasks in
bioinformatics (Saigo et al., 2004).
Regarding deep learning, the kernel method is also important because it has implications for deep
learning models. On the one hand, the kernel methods have similar benign overfitting behavior
to neural network due to the implicit regularization and/or implicit bias phenomenon that we will
review in the next subsection (Belkin et al., 2018). On the other hand, the RKHS itself is closely
related to Neural Networks via the Neural Tangent Kernel theory (Jacot et al., 2018). This all
indicates that to understand deep learning, one should first study kernel methods.
2.2	Directional Bias
This paper analyzes the directional bias of SGD for the nonparametric regression. Directional bias,
also referred to as implicit bias, of an algorithm refers to that its solution path is biased towards a
certain direction. It works as that the algorithm prefers some directions over the others even though
they may have the same objective function value. Since the algorithm selects a direction by itself,
instead of explicitly required by any constraint, people use the term “implicit”. It is worth noting
that the implicit regularization is related to implicit bias. The implicit regularization refers to that the
converged point of an algorithm is like a regularized estimator, even if the objective function is not
explicitly regularized. One can also interpret implicit regularization as the “final result” of implicit
bias. In the recent work by Derezinski et al. (2020), implicit regularization is used to develop an
exact bound for double descent in linear regression. In this way, implicit regularization/bias serves
as a way to explain some deep learning phenomenons that could not be addressed by the classical
empirical risk minimization (ERM) framework. Therefore, it is important to study directional bias.
State-of-the-art study on the directional bias of first-order algorithms can be divided into two cate-
gories by the technique they use:
The first category is the (stochastic) gradient flow method, by taking an infinitesimal step size in
(S)GD, the parameter dynamic follows a (stochastic) differential equation. Studying the solution
path and the stationary point of the underlying differential equation helps to reveal the property of
the parameter estimation. We list some works that use the first method to show the directional bias
result of the (stochastic) gradient descent. Liu et al. (2018) analyze the Momentum SGD (MSGD)
with infinitesimal step size, and show that the solution path escapes from the saddle for a nonconvex
objective function. It is worth noting that in their case, the associated stochastic differential equation
defines a complicated stochastic process, thus they replace it with an appropriate diffusion process,
and the analysis is done based on such diffusion approximation. If one analyzes a stochastic gradient
flow and finds it intractable, one may consider using the technique of diffusion approximation. Ali
et al. (2020) shows the stochastic gradient flow for the linear regression problem minw kXw - yk22
has a solution path close to the solution path of Ridge regression; Blanc et al. (2020) shows the
stochastic gradient flow for a general loss function has a solution path close to the solution path of
gradient flow on the objective function plus some extra penalty terms, and they explicitly identify
the penalty terms; Smith et al. (2021) go one more step from the infinitesimal step size to small step
size, and characterize the effect of small step size as an extra penalty term in the gradient flow.
Another category is analyzing the discrete (S)GD sequence. This technique in general just requires
a moderate step size such that the algorithm converges (or nearly converges), thus it is more mean-
ingful from a practical perspective. We also find some directional bias work that is based on this
technique. Vaskevicius et al. (2019); Zhao et al. (2019); Fan et al. (2021) analyze Hadamard repa-
rameterized GD in sparse regression. They divide the true parameter into strong, weak, and 0 parts,
and for each part, they carefully develop the stepwise error bound for each step of GD. They finally
show that an early-stopped estimator along the solution path achieves the minimax optimal error rate
for sparse regression, which indicates that the solution path is in the direction that biased towards
a sparse solution. Recently, Wu et al. (2021) show that for overparameterized linear regression,
SGD with moderate step size converges to the minimum norm interpolant in the direction that cor-
responds to the largest eigenvalue of the design matrix, while GD converges in the direction that
corresponds to the smallest eigenvalue. For Neural Networks in the ‘lazy training’ regime, ? shows
3
Under review as a conference paper at ICLR 2022
that GD also converges in the direction of the smallest eigenvalue of the Neural Tangent Kernel.
Their result further reveal the mechanism of the directional bias as: GD fits the direction of a larger
eigenvalue faster at the beginning of the training, left the smaller eigenvalue direction unfitted; later
the direction of smaller eigenvalue is fitted, resulting in that the estimator goes in this direction.
3	Problem Formulation
We give our problem formulation in this section. In Subsection 3.1 we define the kernel regression
model and objective function; in Subsection 3.2, we give the SGD and GD algorithms; in Subsection
3.3, we state our assumption for later analysis. Due to the page limit, details of the nonparametric
regression, RKHS and justifications for the assumption are deferred to Appendix A and B.
3.1	Kernel Regression
Suppose we are given n data pairs {xi, yi}in=1 generated from an unknown model y = f (x), where
xi ∈ X ⊂ Rp and yi ∈ R. The goal is to estimate the unknown model f from the data. To achieve
the goal, one way is to find an f that minimizes the empirical risk function
1n
min	'(yi,f(xi))	(1)
f n i=1
where ' is the loss function. For the regression task, we use the squared loss '(y, x) = 1 (y-f (x))2.
One can see that problem (1) is not well-defined, as there are infinitely many solutions to
∀i : f(xi) = yi, and some of them do not generalize for a new test data. One way to fix it is to
restrict f ∈ H and add regularization term in kfkH to problem (1) for smoothness, where H is a
RKHS with reproducing kernel K(∙, ∙) and IlTlH is the Hilbert norm. Adding these restrictions and
applying Representer Theorem, problem (1) with the squared loss becomes
1n
min	ιX(yi — KT a)2
α∈Rnn 2n 乙'yi	i 1
i=1
=1-ky - Kak2
2n
(2)
where KiT is the ith row of K := K(X, X) = (K(xi, xj))i,j. For a parameter α, the correspond-
ing estimator in H is f (∙) = Pn=ι αiK(xi, ∙) := ατK(∙, X).
Now when K is invertible, it is trivial that any algorithm on objective function (2) (if it con-
verges) converges at the unique minimal, that is, α = K(X, X)-1y, result in the RKHS functional
estimator
f(x) = K (χ,x )T K (X,X )-1y	⑶
where K(x, X)T = (K(x, x1), . . . , K(x, xn )). Estimator (3) is the minimum norm interpolant as
given by following problem:
arg min{If IH : f(xi) = yi, i =1, . . . , n}
f∈H
And its property has been studied in Liang & Rakhlin (2020).
In this paper, We compare the convergence direction of SGD and GD to a. Specifically, We
consider a two-stage SGD with a phase transition from a larger step size to a decreased step size.
Note that this matches the training scheme people always use in practice for SGD algorithms:
decreasing the step size after training for a few epochs. For that purpose, in the following sections,
we define the one-step SGD/GD update and state our assumptions and notations for analysis.
3.2	One step S GD/GD update
In this paper, we consider the SGD algorithm as follows. For objective function (2), denote the
parameter estimation at tth step as αt, then SGD update αt+1 as
at+ι = at - ηt(KTat - yQ ∙ Kit	(4)
4
Under review as a conference paper at ICLR 2022
where it is uniformly random sampled from {1, . . . , n}.
The GD update αt+1 as
at+ι = at — ： K T (K at — y) = at — ? K (K at — y)	(5)
3.3	Assumptions and Notations
We state our assumption on the gram matrix in a unified format. Later we show in Appendix B that
some popular kernel families satisfy our assumption.
Assumption 1 (Diagonal Dominant gram matrix). Denote by K = K(X, X) the gram matrix, we
assume that K is diagonal dominant. Specifically, suppose w.l.o.g. that K1,1 ≥ K2,2 ≥ . . . ≥
Kn,n > 0, then we have for a small value τ that
|Ki,j | ≤ τ	Kn,n, ∀i 6= j
Remark 2. Diagonal dominant gram matrix is common in kernel learning. Mathematically, one
can justify that a gram matrix is diagonal dominant by imposing proper assumptions on the kernel
function K (∙, ∙) and the data distribution. Thefollowing proposition shows diagonal dominance for
bilinear kernel. We can check for some other popular kernel to be diagonal dominant, which we
defer to Appendix B due to the page limit.
Remark 3. Think of the kernel function as the inner product of high-dimensional features, the
resulting gram matrix is diagonal dominant when the high-dimension features are sparse. This
happens for a lot of practical problems (ScholkOpfet al., 2002; Weston et al., 2003), for example,
when linear or string kernels are applied to text data (Greene & Cunningham, 2006), when domain-
specific kernels are applied to image retrieval (Tao et al., 2004) and bioinformatics (Saigo et al.,
2004), and when the Global Alignment kernel is applied to most datasets (Cuturi et al., 2007; Cuturi,
2011).
Proposition 4 (Lemma 1 in Wu et al. (2021)). Consider the bilinear kernel K(x, x0) := hx, x0i.
Assume the data xi, i = 1, . . . , n are i.i.d. uniformly distributed on the unit sphere Sd-1, where
d》n. When d ≥ 4 log(2n2∕δ) for some δ ∈ (0,1). Then with probability at least 1 — δ, we have
IKijI = Ihxi, XjiI <T ：= O( √2)《Kn,n = 1,∀i = j.
It is meaningful to note that the diagonal dominance is undesired in classification and clustering
tasks. It indicates that the data pieces are dissimilar to each other as measured by the kernel
function, and thus generates very little information for classification/clustering. One may find a
lot of works on solving the issue of diagonal dominance in these cases, for example, Greene &
Cunningham (2006); Kandola et al. (2003). But for the regression task, the diagonal dominance,
in other words, the dissimilarity of data points, may have benefits. One can find similar conditions
such as Restricted Isometry Property and s—goodness that describes linearly dissimilar features
in a huge regression literature as Candes & Tao (2007); Candes (2008); Chen & Donoho (1994).
Such conditions are required for proving minimax optimality or exact recovery of a sparse signal
in sparse settings. In our case, we adopt the dissimilarity concept and apply it to data points in
high-dimensional nonlinear feature space. Later we will see that in the existence of diagonal
dominance, the directional bias drives SGD to select a good solution that generalizes well among
all solutions of the same level of empirical loss. In this way, our SGD estimator benefits from the
diagonal dominance.
Notations. We use the following notations throughout the remaining of this work. For the gram
matrix K, denote Ki,j be the element at ith row jth column of K. Denote λi = Ki,i = K(xi, xi),
and assume w.l.o.g. that λ1 ≥ λ2 ≥ . . . ≥ λn . Denote the ith column of K as Ki, let
K-1 = [K2 , . . . , Kn]. Assume K is full rank, denote P-1 the projection onto column space of
K-1, and P1 = I — P-1. And denote γ1 ≥ . . . ≥ γn > 0 eigenvalues of K in non-increasing order.
4	Main res ult
The main results are presented in two subsections: Subsection 4.1 states the different directional
bias result of SGD and GD estimators; Subsection 4.2 shows that directional bias towards a certain
5
Under review as a conference paper at ICLR 2022
directional leads to good generalization performance, and further applies this result to show that
SGD generalizes better than GD.
4.1	Directional bias
Since we assumed that K is full rank, then SGD and GD algorithm on objective function (2) con-
verges to α = KTy (when they converge). We are interested in the direction at which a converges
to α, i.e. the quantity
bt := at - α
With assumption 1 that the gram matrix is diagonal dominant, we prove that a two-stage SGD has
bt converge in the direction of the largest eigenvector of K.
Theorem 5 (Direction bias of SGD estimator). Assume Assumption 1 holds, run a two-stage SGD
with a fixed step size for each stage: stage 1 with step size η1 for steps 1, . . . , k1 , stage 2 with step
size η2 for steps k1 + 1, . . . , k2, such that
2	2
λ2 - Cι√nτ < η1 < λ2 + C2√nτ
1
η2 < λ2 + C3√nτ
where C1 , C2 , C3 are constants that are specified in the Appendix E. For a small > 0 such that
nτ < poly (e) there exists kι = O (log ɪ) and k2 such that
(1 - 2)γ1 ≤
E [kK bSGD k2]
E[kbSGD k2]
≤ γ1
That is, bkSGD is close to the direction of the largest eigenvector of K.
Remark 6. One should assume τ in Assumption 1 to be small enough for to be very small if
one would like the resulting estimator bkSGD to have the direction that corresponds to the largest
eigenvalue of K. Later we will see that if one only wants different directional bias of SGD and GD
estimators, a moderate is allowed and then the assumption on τ is not that strong.
The proof of Theorem 5 is in Appendix E. Next, we see that GD has bt converge in the direction of
the smallest eigenvector of K, which contrasts with the directional bias of SGD.
Theorem 7 (Direction bias of GD estimator). Assume Assumption 1 hold, run GD with a fixed step
size:
n
η < (λι + nτ)2 ,
for a small e0 > 0, run k = O (log ')steps ofGD, we have
IlK bGD k2	√--
Yn ≤ IbGDTr ≤ G7 Yn
That is, bt is close to the direction of the smallest eigenvector of K.
Remark 8. The assumption (on τ) is mild for differentiating the directional bias of SGD and GD.
Comparing Theorem 5 and 7, we see that as long as Yn < (1 - 2e)Y1, by taking k large enough we
always have
kκ琮D加 < EkKbSGDk2
kbGD k2	EkbSGD k2
That is, one may expect bkSGD to be in the direction of larger eigenvalue compared with bkGD. In
the following subsection, we see that the directional bias towards a larger eigenvalue of the kernel
is good for generalization, which leads to our title that directional bias helps SGD to generalize in
kernel regression.
The proof of Theorem 7 is in Appendix F. Although there is assumption 1 in Theorem 7, it is just
used to bound the step size so that GD converges; the diagonal dominant structure of K is not
required for the directional bias for GD to hold. Moreover, the choice of e0 is independent of the
assumption on τ, then for an arbitrarily small e0 > 0, we can always run GD long enough such that
the theorem holds and the estimator bkGD is arbitrarily close to the smallest eigenvector.
6
Under review as a conference paper at ICLR 2022
4.2 Effect of directional bias
In this subsection, the estimator that has a directional bias towards the largest eigenvalue of the
Hessian is shown to give the best parameter estimation error among all estimators that have the
same squared in-sample loss, see Theorem 9. Later we define a realizable problem setting of kernel
regression where the generalization error depends on a term similar to the parameter estimation
error, and in this way, the directional bias helps SGD to generalize.
Theorem 9. Consider approximately minimizing the quadratic loss
L(w) = kAw - yk22
Assume there is a ground truth w* such that y = Aw*, for a fixed level of the quadratic loss, the
parameter estimation error ∣∣w 一 w*k2 has a lower bound
∀w ∈ {w : L(w) = a} : kw - w* k22 ≥
a
P¾
Moreover, the equality is obtained when w 一 w* is in the direction of the eigenvector that corre-
sponds to the largest eigenvalue of ATA.
Remark 10. Theorem 9 implies that the directional bias towards the largest eigenvalue is good
for parameter estimation. As discussed in Remark 8, the SGD estimator is biased towards a larger
eigenvalue compared to the GD estimator, then by Theorem 9 the SGD estimator better estimates the
true parameter and thus generalizes better. We formalize this statement in the following paragraphs.
The proof of Theorem 9 is in Appendix G.1.
Suppose ∃f* ∈ H such that y = f*(x). Consider the generalization error LD(f) := ∣f 一 f* ∣2H,
for an algorithm output falg , we decompose its generalization error as:
LD(falg) 一
inf LD(f) = LD(falg) 一 inf LD(f) + inf LD(f) 一 inf LD(f)
f∈H D	D	f∈Hs D	f∈Hs D	f∈H D
'-----------------{z-----------------} |
:=△(f alg), estimation error
{^^^^^^^™
approximation error
where Hs is the hypothesis class that the output of the algorithm is restricted to. By formulation (2),
we have our hypothesis class as
Hs = {f ∈ H ： f = ατK(∙,X), α ∈ Rn}
We define the a-level set of training loss:
Va = {f ∈ Hs ： f = ατ K (∙,X), 1- ∣∣K α 一 y∣2 = a},
2n
denote ∆*a := inff∈νa ∆(f).
Note that the approximation error can not be improved by choice of algorithm unless we
change the hypothesis class, which is, changing the problem formulation in our case. So we just
minimize the estimation error for estimators that are in the a-level set. As shown in Appendix G.2,
one can check the estimation error is given by
f∈Hs : ∆(f) = bτKb
where b = α — α .By similar reasoning as Theorem 9, the estimation error is minimized when b
is in the direction of the largest eigenvalue of K, so the directional bias towards a larger eigenvalue
helps to generalize in kernel regression. We formalize the statement for comparing the estimation
error of SGD and GD in the following theorem.
Theorem 11 (Generalization performance). Follow Theorems 5 and 7, we have the following:
• The output of SGD has E[∆"(fSGD)] ≤ (1 + 4e)(∆a)1/2, where a is such that
E[∣KαSGD - y∣2]2 = 2na and could be any positive small constant;
• The output of GD has ∆(fGD) ≥ M∆*a, where a is the training loss ofGD estimator, and
M = Yγ1 (1 — e0) > 1 is a large constant.
7
Under review as a conference paper at ICLR 2022
2 1
L L
9 8 7 6
IUə-lono⅛∙αj>eQ;
876543210
SSol 6u-u-e±
GD, moderate step size
-- GD, small step size
一 SGD, moderate step size
一 SGD, small step size
0	200 400 600 800 1000	0	200 400 600 800 1000	0	200 400 600 800 1000
# iterations	# iterations	# iterations
Figure 1: Kernel regression on synthetic data. We simulate data from a nonlinear regression model
with Gaussian additive noise and fit kernel regression using a polynomial kernel. We run both SGD
and GD for two step size schemes, see details in Appendix H.1. In the first plot, we show the
directional bias by Rayleigh Quotient(RQ):= kKb22 (same as Theorem 5 and 7). The SGD indeed
kbk2
converges in the direction of a larger RQ, which matches our Theorems 5 and 7. In the third plot we
show the prediction error of the solution paths, and the SGD does have lower prediction error than
GD, even GD has smaller training loss than SGD. This supports Theorem 11.
Remark 12. One read from the theorem that the SGD estimator is (8 + 162)-optimum, while
GD estimator is (M - 1)-sub-optimum. Combine with Theorem 7 that 0 k-→→∞ 0, we can take in
Theorem 5 such that (1 + 4e)2 < γι/Yn to have ∆(f SGD) < ∆(f GD) with high probability. This
finishes our claim that the SGD estimator generalize better than the GD estimator.
The detailed proof of Theorem 11 is left to Appendix G.2.
Numeric Study. Figure 1 shows the simulation results of kernel regression, Figure 2 shows the
results of a small ResNet-like Neural Network on FashionMNIST data (Xiao et al., 2017). Figure
1 and Figure 2a supports the directional bias results in Theorems 5 and 7, and Figure 2b validates
Theorem 11. For details of the experiments and more experiments, see Appendix H.
Remark 13. The purpose of experiment using a Neural Network (Figure 2) is as following: first, the
Neural Network results support our finding on kernel regression, since Neural Network is related to
kernel regression through NTK theory (Jacot et al., 2018); second, our experiment indicates that our
result may be empirically true for the more general deep learning framework (Belkin et al., 2018),
since this experiment uses a complicated network that may not be simply explained by the kernel
method.
5 Discussion and Further Work
Our work takes one more step towards understanding the directional bias of SGD in kernel learning.
Here we discuss some implications of our results to deep learning.
Implication for SGD scheme: Our result shows the directional bias applies to SGD with
annealing step size. Specifically, the first stage of SGD with moderate step size should run long
enough, then in the second stage by decreasing step size we have the directional bias towards the
largest eigenvalue of the Hessian, which helps in benign overfitting. This explains a technique for
tuning the learning rate that people use in practice: start with a large step size, run long enough until
the error plateaus, then decrease the step size (He et al., 2016). Although this technique is always
used to speed convergence, we show that it also helps in benign overfitting, which becomes even
better.
Implication for deep learning: Our assumption for analysis implies certain structures for deep
learning models. Per our examples in Appendix B and our discussion in Remark 3, our assumption
holds when the feature space is high dimensional and/or when features are possibly sparse. This
matches the deep learning scenario where we have a highly overparameterized model and when the
trained parameter estimator becomes sparse. Besides, considering that some deep learning tasks can
8
Under review as a conference paper at ICLR 2022
IUə-lono⅞ω>eQ=① >4(D0jQ:
(a) Relative Rayleigh Quotient.
(b) Test accuracy on FashionMNIST
Figure 2: The experiment of a small ResNet-like Neural Network on FashionMNIST. In (a), we
follow Wu et al. (2021) to use the Relative Rayleigh Quotient(RRQ) as the measurement of the
convergence direction, where higher RRQ means that the convergence direction is closer to the larger
eigenvalue direction of the Hessian. The SGD with moderate step size has higher RRQ than the GD
with either moderate step size or small step size, which supports the theory in Theorems 5 and 7. Itis
also interesting to observe SGD with a small step size also has a different directional bias compared
with SGD with a moderate step size. In (b), we plot the testing accuracy from 20 repetitions of
experiments, the test accuracy (inside bracket) of SGD with moderate step size is higher than the
other cases, and we have Wilcoxon signed-rank test to confirm that the difference is significant at
0.01 level. The test accuracy validates Theorem 11. For more details of the experiments, the rank
test, and more experiments, see Appendix H.2.
be approximated by kernel learning (Jacot et al., 2018), our results help in explaining why the SGD
estimator can benign overfitting in an overparameterized deep learning.
Just as stated in Belkin et al. (2018), to understand deep learning one needs to understand kernel
learning. This work takes a step in understanding kernel learning, and we expect more steps that
go beyond this work towards understanding deep learning, possibly for some complicated structure
that could not be approximated by kernel learning.
Reproducibility Statement: For all theoretical results presented in this paper, we carefully
state and justify the assumption, we also include the proof in Appendix. For all experiments, we
state the implementation details in Appendix, and we include the necessary code and data for
reproducing our result in the supplementary material.
References
Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In International Conference on Machine Learning, pp. 233-244. PMLR,
2020.
Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.
arXiv:2103.09177, 2021.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. In International Conference on Machine Learning, pp. 541-549. PMLR,
2018.
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep
neural networks driven by an ornstein-uhlenbeck like process. In Jacob Abernethy and Shiv-
ani Agarwal (eds.), Proceedings of Thirty Third Conference on Learning Theory, volume 125
of Proceedings of Machine Learning Research, pp. 483-513. PMLR, 09-12 Jul 2020. URL
http://proceedings.mlr.press/v125/blanc20a.html.
Emmanuel Candes and Terence Tao. The dantzig selector: Statistical estimation when p is much
larger than n. The annals of Statistics, 35(6):2313-2351, 2007.
9
Under review as a conference paper at ICLR 2022
Emmanuel J. Candes. The restricted isometry property and its implications for compressed
sensing. ComPtes Rendus Mathematique, 346(9):589-592, 2008. ISSN 1631-073X. doi:
https://doi.org/10.1016/j.crma.2008.03.014. URL https://www.sciencedirect.com/
science/article/pii/S1631073X08000964.
Shaobing Chen and David Donoho. Basis pursuit. In Proceedings of 1994 28th Asilomar Conference
on Signals, Systems and ComPuters, volume 1, pp. 41-44. IEEE, 1994.
Marco Cuturi. Fast global alignment kernels. In Proceedings of the 28th international conference
on machine learning (ICML-11), pp. 929-936, 2011.
Marco Cuturi, Jean-Philippe Vert, Oystein Birkenes, and Tomoko Matsui. A kernel for time series
based on global alignments. In 2007 IEEE International Conference on Acoustics, SPeech and
Signal Processing-ICASSP’07, volume 2, pp. II-413. IEEE, 2007.
Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expres-
sions for double descent and implicit regularization via surrogate random design. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 5152-5164. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf.
Jianqing Fan, Zhuoran Yang, and Mengxin Yu. Understanding implicit regularization in over-
parameterized nonlinear statistical model. arXiv:2007.08322, 2021.
Derek Greene and Padraig Cunningham. Practical solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the 23rd international conference on Machine
learning, pp. 377-384, 2006.
Hussein Hazimeh, Rahul Mazumder, and Peter Radchenko. Grouped variable selection with discrete
optimization: Computational and statistical perspectives. arXiv:2104.07084, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on ComPuter Vision and Pattern Recognition
(CVPR), June 2016.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.
Jaz Kandola, Thore Graepel, and John Shawe-Taylor. Reducing kernel matrix diagonal domi-
nance using semi-definite programming. In Learning Theory and Kernel Machines, pp. 288-302.
Springer, 2003.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “Ridgeless” regression can gener-
alize. The Annals of Statistics, 48(3):1329-1347, June 2020. doi: 10.1214/19-AOS1849.
Tianyi Liu, Zhehui Chen, Enlu Zhou, and Tuo Zhao. A diffusion approximation theory of momen-
tum sgd in nonconvex optimization. arXiv:1802.05155, 2018.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. arXiv:1712.06559, 2018.
Rahul Parhi and Robert D. Nowak. What kinds of functions do deep neural networks learn? insights
from variational spline theory. arXiv:2105.03361, 2021.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive
models over kernel classes via convex programming. Journal of Machine Learning Research, 13
(2), 2012.
10
Under review as a conference paper at ICLR 2022
Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda, and Tatsuya Akutsu. Protein homology detection
using string alignment kernels. Bioinformatics, 20(11):1682-1689, 2004.
Bernhard SchOlkopf, Jason Weston, Eleazar Eskin, Christina Leslie, and William Stafford Noble.
A kernel approach for learning from almost orthogonal patterns. In European Conference on
Machine Learning, pp. 511-528. Springer, 2002.
Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regular-
ization in stochastic gradient descent. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=rq_Qr0c1Hyo.
Hiroyuki Takeda, Sina Farsiu, and Peyman Milanfar. Kernel regression for image processing and
reconstruction. IEEE Transactions on image processing, 16(2):349-366, 2007.
Qingping Tao, Stephen Scott, NV Vinodchandran, Thomas Takeo Osugi, and Brandon Mueller. An
extended kernel for generalized multiple-instance learning. In 16th IEEE International Confer-
ence on Tools with Artificial Intelligence, pp. 272-277. IEEE, 2004.
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
5cf21ce30208cfffaa832c6e44bb567d- Paper.pdf.
Grace Wahba. Spline models for observational data. SIAM, 1990.
Jason Weston, Bernhard Scholkopf, Eleazar Eskin, Christina Leslie, and William Stafford Noble.
Dealing with large diagonals in kernel matrices. Annals of the institute of statistical mathematics,
55(2):391-408, 2003.
Jingfeng Wu, Difan Zou, Vladimir Braverman, and Quanquan Gu. Direction matters: On the im-
plicit bias of stochastic gradient descent with moderate learning rate. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
3X64RLgzY6O.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv:1708.07747, 2017.
Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning.
arXiv:2106.11342, 2021.
Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product over-
parametrization in high-dimensional linear regression. arXiv:1903.09367, 2019.
11
Under review as a conference paper at ICLR 2022
A	Background on RKHS
This section details background on RKHS in two subsections. The first subsection includes no-
tations, theorems, and an example of RKHS, the second section reduces the kernel regression in
RKHS from infinite dimension to finite dimension, which gives our objective function (2) .
A. 1 Nonparametric model in RKHS
In this subsection, we give the definition and notations for our model in RKHS, and its associated
norms, basis, etc. The definitions are similar to those in Raskutti et al. (2012).
Given n data pairs	{xi, yi}in=1,	where	xi	∈ X ⊂	Rp	and	yi	∈	R,	assume that	yis	are as-
Sociated with Xis through f (xi), where f (∙) is some unknown function in the reproducing kernel
Hilbert space (RKHS) of functions X →R, our goal is to estimate the function f (∙) from the data.
Denote the RKHS where f lives as H, with reproducing kernel K : X × X → R+ (which
is known to us). And We associate the functions in H with probability measure Q, assume w.l.o.g.
that JRp f (a)dQ(a) = 0. By Mercer,s theorem, K has eigen-expansion:
∞
K(a, b) =	γjφj (a)φj (b)
j=1
Where {φj∙}∞=1 are orthonormal basis in L2(Q), w.r.t. the usual inner product in L2(Q) as
hg(∙), h(∙)iiL2(Q)=I g(a)h(a)dQ(a)
X
Now for any f ∈ H, we can expand f (∙) = Pj=IcjΦj(∙), where Cj = hf (∙), φj(•))工2(Q). And for
f(∙) = P∞=1 Cjφj(∙), g(∙) = Pj=Icjφj(∙), by Parseval,s theorem
∞
hf (∙),g(∙)i IL2(Q) = Ecjcj
j=1
And we have another inner product that is defined for RKHS H as
∞0
hf(∙),g(∙yiH = X ɪj
j=1 γj
The reproducing property of RKHS says that ∀f ∈ H, we have
hf(∙),K(∙, x)iH = f(X)
Cubic Splines Formulate a RKHS. We go over an example of RKHS for better understanding.
Consider the cubic spline of one dimension, we can show that the space of cubic splines is a RKHS.
One can also find the cubic spline example in Hazimeh et al. (2021). For more details on the
relationship between polynomial smoothing splines and RKHS, one can check Section 1.2 of Wahba
(1990).
Assume w.l.o.g. that xi ∈ X = [0, 1] ⊂ R. The cubic spline f on X is continuous, has a contin-
uous first-order derivative and square integrable second order derivative. By Taylor’s theorem with
remainder, we have
f(t)=f(0)+tf0(0)+Z t(t-u)f00(u)du
0
= f(0) + tf0(0) +	(t - u)+f00(u)du
0
where (t - u)+ = max{0, t - u}. Let B be the set of cubic splines f on [0, 1] that satisfies the
boundary condition f(0) = f0(0) = 0, then for f ∈ B
f(t) = Z (t - u)+f00(u)du
0
12
Under review as a conference paper at ICLR 2022
Let G(t, u) = (t - u)+, then we claim that B is RKHS with reproducing kernel
K(s, t) =	G(s, u)G(t, u)du
0
and inner product
hf, giB = Z f 00(u)g00(u)du
0
as one can check the reproducing property
∣f(∖ K t∖∖ — f 1 ∂2K(u,t) f00(. ∖d
hf ( ), K (It))B = J	∂u2	f (U)du
= Z (t - u)+f00(u)du = f(t)
0
A.2 Optimization problem considered
This subsection gives problem formulation of kernel regression. Given data pairs {xi , yi }in=1 and
RKHS H, consider a loss function ` which is selected according to how y is connected with f (x),
we may estimate the model by
1n
miH — £'(yi,f (Xi))	⑹
f∈ n i=1
1n ∞
=min 一∑'(yi,∑cjφj(xi))
cj —
j i=1 j=1
Example for ` includes
•	Squared error loss '(y, f) = (y 一 f )2, which is usually used in regression;
•	0-1 loss '(y, f) = 1(y * f > 0), for binary classification;
•	Logistic loss '(y, f) = log(1 + exp(-y * f)), also a loss function for classification, can
be considered as a surrogate function of 0-1 loss, and is the same as negative log likelihood
function in logistic regression.
Let us come back to the nonparametric model part, to control the model smoothness, the usual
practice is to add a penalty to objective (6), result in
n
min X '(yi,f (Xi)) + λPen(f)
f∈H
i=1
A PoPular choice of Pen(f) is kfk2H, or any strictly increasing function of kfk2H. Such method
is exPlicitly controlling the model smoothness, and by RePresenter Theorem, it has solution of the
form
n
f (∙) = X αiK (∙, Xi)	(7)
i=1
Plug equation (7) into objective (6), we have the Problem becomes
min
αi0
1n	n
-f'(yi,fai K (Xi, Xi")
— i=1	i0 =1
Which gives the formulation (2) under loss function '(y, f (x)) = (y 一 f (x))2∕2.
13
Under review as a conference paper at ICLR 2022
B Diagonal Dominance of Some Popular Kernels
In this section, we justify Assumption 1 by figuring out a problem setting where some popular ker-
nels give a diagonal dominant gram matrix. For simplicity, we assume the following data distribution
throughout this section:
xi ∈ Rd, i = 1, . . . , n, are normalized such that kxik22 = 1;	(A.1)
The direction of xis are i.i.d. uniformly distributed on the unit sphere Sd-1 ;	(A.2)
d n (overparameterized setting).	(A.3)
Given assumption set (A), we can bound the inner product of data hxi, xji with high probability as
follows:
Lemma 14 (Lemma 1 in Wu et al. (2021)). Under assumption set (A), let d ≥ 4 log(2n2 /δ) for
some δ ∈ (0, 1). Then with probability at least 1 - δ, we have
lhxi,xji| < τ := O(√1d),∀i = j
Proof. See proof of Lemma 1 in WU et al. (2021).	□
The bound on the inner product hxi, xji induces bound on K(xi, xj) for some popular kernels. We
show the diagonal dominance for two groUps of kernels in the following propositions, and list some
examples for kernels in each groUp.
Proposition 15 (Inner prodUct kernel). The inner product kernel is defined as a smooth transforma-
tion of inner product. We can write it as:
K(xi, xj) = g(hxi, xji)
Assume assumptions (A) hold, and assume the function g : [-1, 1] → R satisfies:
g is convex;	(B.1)
g is L-smooth, that is, Vg is L-Lipschitz continuous;	(B.2)
|g(0) | ≤ cτ for a constant c, g0(0) ≥ 0.	(B.3)
we will have with probability 1 - δ
|Ki,j| ≤ (c + g0(0))T + 2T2 for i = j	(10)
where δ and τ the same as in Lemma 14.When g0(0)T + L T2《g(1) forasmall enough τ, the gram
matrix is diagonal dominant.
Proof. We have the following with probability at least 1 - δ by Lemma 14. For any off-diagonal
elements of K :
Ki,j = g(hxi, xji)
≥ g(0) + g0(0)hxi, xji
≥ -(g0(0) + c)T
and
Ki,j = g(hxi, xj i)
≤ g(0)+go(0)hχi, Xji + 2〈Xi, Xjy
≤ (c+g0 (0))T+2 τ2
Thus ∣Ki,j| ≤ (c + g0(0))T+ LT2.	□
Remark 16. We list some examples of inner product kernels that give diagonal dominant kernel
matrices:
14
Under review as a conference paper at ICLR 2022
•	Bilinear Kernel: K(x, x0) = hx, x0i, then
|K(Xi, Xj) | ≤ T《; K(xn, Xn) = 1.
•	Polynomial Kernel: K(x, x0) = (〈x, x0i + c)m for m ∈ N and c 〜O(T), then by
Proposition 15
∣K(Xi, Xj)1 ≤ (1 + m)τ+ m * exp(2m- 1)τ)T
when (1 + m)T + m:exp(2mT)T) T2《 (1 + c)m, we have diagonal dominant gram matrix.
•	Hyperbolic Tangent Kernel (Sigmoid Kernel): K(X, X0) = tanh(αhX, X0i + c), where
α > 0, c ≥ 0. Note that Hyperbolic Tangent Kernel does not satisfy all the assumptions in
Proposition 15, one can still calculate
|K(Xi, Xj )| ≤ tanh(αTT + c)
and
K(Xk, Xk) = tanh(α + c)
When tanh(αTT + c)	tanh(α + c) (which is the case when α is large, and c, TTare small
enough), we have |K(Xi, Xj)|	K(Xn, Xn) and the gram matrix is diagonal dominant.
Proposition 17 (Radial Basis Function (RBF) kernel). Radial Basis Function kernel depends on
two data points through their distance, which is of following form
K(Xi,Xj) = exp(-γkXi - Xjk22),γ > 0
Assume assumptions (A) hold, when γ = -c0 log(TT) for a constant c0, we have with probability
1-δ
∣Kij | ≤ T2c0(I-T) ≪ Kn,n = 1, for i= j
where δ and TTthe same as in Lemma 14. That is, the Gram matrix is diagonal dominant.
Proof. Bound off-diagonal terms of K by Lemma 14:
Ki,j = exp(-γkXi - Xjk22)
≤ exp(2γTT- 2γ )
=τ2co(1-T)
□
Remark 18. We note some popular kernels that are related to Radial Basis Function kernel, and
show that they lead to diagonal dominance:
•	Gaussian Kernel: K(x, x0) = exp(一位屋 k2). One can see that Gaussian Kernel is
reparameterizing RBF Kernel by Y = 1∕(2σ2). Thus Gaussian gram matrix is diagonal
dominant when σ2 〜O(-喋1(T)).
•	Laplace Kernel: K(x, x0) = exp(-kx—x k2) for σ > 0. The Laplace Kernel is very
similar to Gaussian Kernel, and one can check by similar steps that when σ 〜 O(-1⅞⅛),
Laplace gram matrix is diagonal dominant.
C Lemmas
This section includes two useful lemmas for characterizing the eigenvalues of a symmetric matrix.
Lemma 19 (Gershgorin circle theorem, restated for symmetric matrix). Let A ∈ Rn×n be a sym-
metric matrix. Let Aij be the entry in the i-th row and the j-th column. Let
Ri(A) :=	|Aij|, i = 1, . . . , n
j6=i
15
Under review as a conference paper at ICLR 2022
Consider n Gershgorin discs
Di (A)= {z ∈R, |z - Aii| ≤ Ri(A)}, i = 1,...,n
The eigenvalues of A are in the union of Gershgorin discs
G(A) := ∪in=1Di (A)
Furthermore, if the union of k of the n discs that comprise G(A) forms a set Gk(A) that is disjoint
from the remaining n - k discs, then Gk (A) contains exactly k eigenvalues ofA, counted according
to their algebraic multiplicities.
Proof. See Horn & Johnson (2012), ChaP 6.1, Theorem 6.1.1.	□
Lemma 20 (Cauchy interlacing theorem, restated for symmetric matrix). Let B ∈ Rm×m be a
By
symmetric matrix, let y ∈ Rn and a ∈ R, and let A = yT a . Then
λ1(A) ≥λ1(B) ≥ λ2(A) ≥ ...≥λm(A) ≥λm(B) ≥ λm+1(A).
Proof. See Horn & Johnson (2012), Chap 4.3, Theorem 4.3.17.	□
D Spectrum of gram matrix
This section analyzes the eigen structure of the gram matrix.
Lemma 21 (Characterizing K2). Under Assumption 1, we have
hKi,Kii ∈ [λi2,λi2+(n-1)τ2]	(11)
|hKi,Kji| ≤ [2λ1+(n-2)τ]τ,i 6=j	(12)
Proof. For hKi, Kii
hKi,Kii = Ki2,i + X Kl2,i
l6=i
∈ [λi2, λi2 + (n - 1)τ2]
And for hKi, Kji, i 6= j
n
|hKi,Kji|= |XKl,iKl,j|
l=1
= |Ki,iKi,j + Kj,iKj,j + X Kl,iKl,j |
l6=i,j
≤ |Ki,iKi,j | + |Kj,iKj,j | + X |Kl,iKl,j |
l6=i,j
≤ [λi + λj]τ + (n - 2)τ2
≤ [2λ1 + (n - 2)τ]τ
□
Lemma 22 (Eigenvalue of K). Under Assumption 1, we have
γ1 ≤ λ1 + nτ	(13)
γn ≥ λn - nτ	(14)
If we further assume λj + nτ < λj-1 - nτ, we will have
γj-1 ≥ λj-1 - nτ > λj + nτ ≥ γj .
16
Under review as a conference paper at ICLR 2022
Proof. Use Gershgorin circle theorem, calculate
Ri(K) = X |Ki,j| ≤ nτ
j6=i
then
Di (K) ⊂ [λi - nτ, λi + nτ].
By Gershgorin circle theorem, the lemma claim holds.	□
Lemma 23 (Characterize P1K and P-1K). Recall our definition: P-1 is the projection on column
space of K-1 = [K2, K3, . . . , Kn], and P1 = I - P-1. We claim the following hold
	P1K = [P1K1, 0, . . . , 0]	(15)
	P-1K = [P-1K1,K2,...,Kn]	(16)
Assume τ is small enough that nτ ≤ O(1), λn — nτ ≥ c1 > 0 and λ1 + nτ ≤		c2, let c3 := C2, then
we have the following:		
kP-1K1k2 ∈	[0, C3(2λ1 + (n — 2)τ)√nτ]	(17)
kP1K1k2 ∈	Jλ2 — c2[2λι + (n - 2)τ]2nτ2, λι + √nτ	.	(18)
Proof. For i 6= 1, we calculate
P-1Ki =Ki
P1Ki =Ki-P-1Ki =0
thus we have equations (15) and (16).
For kP-1K1k2:
kP-1K1k2
=kK-1(K-T1K-1)-1K-T1K1k2
≤kK-1(K-T1K-1)-1k2kK-T1K1k2
where
kK-T1K1k22
n
= X(KiTK1)2
i=2
(12)
≤ (n - 1)[2λ1 + (n - 2)τ]2τ2
≤[2λ1 + (n - 2)τ]2nτ2
and K-T1K-1 has all eigenvalues in [γn2, γ12] by Cauchy interlacing theorem (Lemma 20), that is, all
singular values of K-1 are in [c1, c2] by our assumption. Then
kK-ι(KT 1K-1)-1k2 ≤ C2 := c3
So we have
kP-1K1k2 ≤ kK-ι(KT 1K-1 )-1k2kKT 1K1k2
≤ C3[2λ1 + (n — 2)τ]√nτ.
For kP1K1k2:
kP1K1k2 ≤ kK1k2
≤(λ2 + (n — 1)τ 2).5
≤λι + √nτ
17
Under review as a conference paper at ICLR 2022
and
kP1 K1 k22
=kK1k22 - kP-1K1k22
≥λ21 - c32 [2λ1 + (n - 2)τ]2nτ2.
□
Lemma 24 (Spectrum of H-1 := P-1KKTP-1). Assume
c23 [2λ1 + (n - 2)τ]2nτ2 + 2[2λ1 + (n - 2)τ]nτ ≤ λ2n
We have the following:
•	0 is an eigenvalue of H-1, corresponding eigenspace is the column space of P1;
•	Restricted to the column space of P-1, the eigenvalues of H-1 are all in the interval:
(-n - [2λι + (n - 2)τ]nτ, λ2 + [2λι + (n - 1)τ]nτ).
Proof. The first claim is by construction of P1 and P-1.
For the second claim, note that H-1 has the same eigenvalues as
H-0 1 = (P-1K)TP-1K
Now the diagonal entries of H-0 1 are:
H0	P K 2	kP-1K1k22 ≤ c23[2λ1 + (n - 2)τ]2nτ2 ,i = 1
(H-1)ii = kP-1Kik2 = kKik22 ∈ [λi2,λi2+(n-1)τ2]	,i6=1
And the off-diagonal entries of H-0 1 are:
|(H-01)ij| = |hP-1Ki,P-1Kji|= |hKi,Kji| ≤ [2λ1 + (n - 2)τ]τ
To use Gershgorin circle theorem, calculate
Ri(H-0 1) = X |(H-0 1)ij| < [2λ1 + (n - 2)τ]nτ
j6=i
Thus the Gershgorin discs:
D1(H-0 1) ∈ (kP-1K1k22 - [2λ1+(n-2)τ]nτ,kP-1K1k22+ [2λ1 + (n - 2)τ]nτ)
Di(H-0 1) ∈ (kKi k22 - [2λ1 + (n - 2)τ]nτ, kKi k22 + [2λ1 + (n - 2)τ]nτ)
when c23[2λ1 + (n- 2)τ]2nτ2 + [2λ1 + (n- 2)τ]nτ ≤ λ2n - [2λ1 + (n- 2)τ]nτ, the first Gershgorin
discs does not intersect with the others, so we have n - 1 nonzero eigenvalues in
∪n=2Di (H — 1) ⊂ (λj — [2λι + (n - 2)τ]nτ, λ2 + [2λι + (n - 1)τ]nτ).
□
E	Directional bias of SGD with moderate step size
This section gives formal proof of Theorem 5 and specifies the constants. The proof is done in four
steps: Lemma 25 analyzes one update of SGD; Lemma 26 uses Lemma 25 to bound the first stage
updates of SGD with moderate step size; Lemma 27 again uses Lemma 25, and bounds the second
stage updates of SGD with small step size; finally, Theorem 28 combines Lemma 26 and Lemma 27
to formalize the directional bias of SGD, it is the same as Theorem 5, but restated using the constants
defined therein.
18
Under review as a conference paper at ICLR 2022
Lemma 25 (One step update of SGD). Under Assumption 1, denote At := E[∣∣P1btk2], Bt :
E[∣∣P-1 btk2] ,fix a Constant c4 ≥ (λι + √nτ )(2λι + (n - 2)τ )c3 ,then we have:
At+1 ≤ q1(η)At + ξ(η)Bt	(19)
At+1 ≥ q1(η)At - ξ(η)Bt	(20)
Bt+1 ≤ q-1(η)Bt + ξ(η)At	(21)
where
qι(η) = n-1 + 111 - ηkPιKιk2l
nn
q-1㈤={1+^^1：] [η2(λ2+ (n - I)T2)- 2n]
ξ(η) = c4ηn-1/2 τ.
Proof. One step of SGD update is:
bt+1 =bt - ηKiKiT bt = [I - ηKiKiT]bt
where i is uniformly random sample from [1, . . . , n].
For inequalities (19) and (20), check
P1bt+1 =P1[I-ηKiKiT]bt
=P1bt -ηP1KiKiT(P1+P-1)bt
= [I - ηP1KiKiT P1]P1bt - η[P1KiKiT P-1]P-1bt
where P1 Ki and P1 bt are in the same 1 dimensional linear space, thus
P1KiKiTP1P1bt
=kP1Kik2kP1btk2sign(hP1Ki,P1bti)P1Ki
=kP1Kik22P1bt
⇒	[I-ηP1KiKiTP1]P1bt = [1-ηkP1Kik22]P1bt
and
k[P1KiKiTP-1]P-1btk2
≤kP1Kik2kP-1Kik2kP-1btk2.
Then
E[kP1bt+1k2∣bt] ≤ E[|1 - ηkP1Kik2∣]kP1btk2 + ηE[kP1Kik2kP-1Kik2]kP-1btk2	(22)
E[kP1bt+1k2∣bt] ≥ E[|1 - ηkP1Kik2∣]kP1btk2 - ηE[kP1Kik2kP-1Kik2]kP-1btk2	(23)
where
E[|1 - ηkPιKik2∣]
1n
=-∑∣1 - ηkPιKik2l
n i=1
=n-1 + 1∣i-ηkPι Kιk2∣
nn
:=q1 (η)
and
ηE[kP1Kik2kP-1Kik2]
1n
=ηn EkPIKik2kP-1Kik2
n i=1
=η kP1K1k2kP-1K1k2
n
≤ η (λι + √nτ )c3(2λ1 + (n — 2)τ )√nτ
n
≤n。4√nτ := ξ(η)	(24)
n
19
Under review as a conference paper at ICLR 2022
where the first inequality by upper bounds (17) and (18), second inequality by nτ ≤ O(1). Plug
the term (24) into inequalities (22) and (23), take expectation on both sides, we get claims (19) and
(20).
For inequality (21), check
P-1bt+1 = P-1[I-ηKiKiT]bt
= [I - ηP-1KiKiT P-1]P-1bt - η[P-1KiKiT P1]P1bt
Then we have
E[kP-1bt+1k2|bt]
1n
≤ -Ek[I-ηP-ιKiKP-1]P-1btk2 + ηE[kP-1Kik2IlPIKiIl2]∣∣P1bt∣∣2
n
i=1
(24)	1 n
≤ — Ek [I — ηP-1KiKT P-1]P-1btk2 + ξ(η)kP1btk2
n i=1
where
1n
-Ek [I — ηP-1KiKiT P-1]P-1btk2
n i=1
n _______________________________
=-X √k[i — ηP-1KiKT P-1]P-1btk2
n i=1
n ___________________________________________________________________
=-X √kP-1btk2 + η2kP-1KiKTP-1P-1btk2 — 2ηhP-ιbt, P-iKiKTP-1 P-Ibti
n i=1
n ________________________________________________________
=-X √kP-ιbtk2 + η2(κTP-1P-1bt)2kP-1Kik2 - 2η(κTP-iP-也)2
n i=1
=-X SiJKTP-lP-y2 (n/'"2η) "k2
≤t1 + -X (KT"P-9" (n2k jK'k2 - 2n)kPTbtk2
where the last inequality from Jensen’s inequality. Now the term
-n
-X
n i=1
(KT P-IP-M2
kP-ibtk2
(n2kP-ιKik2 - 2n)
(11)- X (KTP-iP-ibt)
≤ n = 一Etk―
η2 (λ22 + (n — -)τ2 ) — 2η
kKP-ibtk2
nkP-ibtk2
η2 (λ22 + (n — -)τ2 ) — 2η
Let	____________________________________
q-1(n := {ι+nKP-ittk； [n2(λ2+(n - i)t 2)- 2n]
combine all three inequalities above, take the expectation w.r.t. bt, we have claim (21).
□
Lemma 26 (Long run behavior of SGD with moderate step size). Assume b° is awayfrom 0, Xn >
(2λι + (n - 2)τ)nτ + c4√nτ, λ2 + C6√nτ < λ2 一 c5√nτ where c5,c6 are constants SuCh that
C5 ≥ c2[2λι + (n — 2)τ]2√nτ + c4
20
Under review as a conference paper at ICLR 2022
√nτ - cln--^τ∕[λn - (2λι + (n - 2)τ)nτ] + 入储/区-[2λ1 + (n - 2)τ]nτ]
CA ≥ ----------------------------------------------------------
—	1 - C4√nτ∕[λn - (2λι + (n - 2)τ)nτ]
Consider first kι steps of SGD updates with step size η:
22
Tq----L < η < F-------L
ʌi - c5√nτ	λQ + c6√nτ
FiX a βo ≤ Ao, then for 0 < e < 1 and 0 < β < βo such that √nτ ≤ poly(eβ), there exists
kι = O (log W) satisfying:
•	Bkl ≤ eβ
•	AkI ≤ Ilbo∣∣2 * ρk1 + eβ∕2 for some ρι > 1
•	Ak > βo for k = 0,..., k1.
Proof. For this choice of η, denote qι = qι(η), q-ι = q-ι(η),ξ = ξ(η). By Lemma 21, we have
Ak ≥ qιAk-I - ξBk-I
Ak] ≤	∣^qι ξ ]	∣^Ak-I
Bk - ξ q-i Bk-i
Decompose the coefficient matrix as
qi ξ _ cos θ
ξ q-i = sin θ
-sin θ pi 0 cos θ sin θ
cos θ 0 ρ-ι — sin θ cos θ
Assume w.l.o.g. that sin θ ≥ 0 (since otherwise we can take θ → θ + π), then we have
k
Ak ≤ qi ξ Ao]
BkJ — [ξ q-i_|	|_Bo_|
cos θ — sin θ] Γpk
sin θ cos θ 0
0 cos θ sin θ	Ao
pk-i	- sin θ cos θ	Bo
Ao(pk cos2 θ + p-i sin2 θ) + Bo(pk cos θ sin θ — p- cos θ sin θ)
Bo(p-i cos2 θ + pk sin2 θ) + Ao(pk cos θ sin θ — p-i cos θ sin θ)
Aopk + (pk - pk-1) sin θ(Bo cos θ - Ao sin θ)
Bop-i + (pk — p-i) sin θ(Bo sin θ + Ao cos θ)
≤ ^AOPk + IPk-P-ilsinθ√bq + Ao-
一Bop-i + |pf - p-i∣ sinθ√B2 + Ao_
=	Aopk + ∣pk - p-i∣ sinθ∣∣bo∣∣2
一Bop-i + ∣pk - p-i∣ sinθ∣∣bo∣L
We claim the following holds:
0 < p-i < 1 < pi ≤ qi + ξ	(25a)
p-1i∣bo∣2 ≤ eβ∕2	(25b)
PkIkM2 sinθ ≤ eβ∕2	(25c)
(Bo + eβo∕2)ξ < (qi - 1)βo	(25d)
which we check later. Using inequalities (25), we can upper bound Bk1 as
BkI ≤ BOP-1i + (PkI- P-II) sin θ∣∣bo∣∣2
≤ kbok2p-1i + pki sinθ∣∣M2
(25b),(25c)
≤ eβ
21
Under review as a conference paper at ICLR 2022
In addition, for k = 0,..., kι
Bk ≤ B0p-1 + (Pk - P-I) Sin θ∣∣b0∣∣2
≤ B0 + ρ11sinθ∣∣b01∣2
(25c)
≤ B0 + eβ/2
We now lower bound Ak by mathematical induction. We have A0 ≥ β0, assume Ak-ι > β0, then
Ak ≥ 91 Ak-I - ξBk-1
≥ q1β0 - ξ(B0 + eβ∕2)
(25d)
> q1β0 - (qi - 1)β0 = β0
For upper bound Ak1, check
Ak1 ≤ A0ρk1 + (ρk1 - P-1Jsinθ∣∣b0∣∣2
≤∣∣M2 ρk1 + ρk1 sin θ∣∣b0∣∣2
(25c)	k
≤ I∣b0∣∣2ρ11 + eβ∕2
We have all lemma claims proved. Now it remains to check inequalities (25). First note that our
choice of the upper bound on η guarantees that q-ι < 1.
For inequality (25a): By Gershgorin circle theorem, it suffices to show q-ι + ξ < 1, qi - ξ > 1,
then we have pi ≥ qi - ξ > 1 and ρ-ι ≤ q-ι + ξ < 1. In addition, we need the matrix to be
positive definite so that p-ι > 0, we just need q1q-1 > ξ2 to make the matrix p.d..
q-i + ξ < 1
^⇒1 j1+ "Kpτ⅛2 [η2(λ2 + (n - I)T2)- 2η] < 1 - C4ηnτ∕2τ
V	nkP-1btk2
U "Kp-⅛ [η2(λ2 + (n - I)T2) - 2η] < C2η2n-1τ2 - 2c4ηn-"T
nkp-1bM2
Lemm= 242c4ηn-"T ≤ c2η2n-1τ2 + λn - [2λ1 + (n - 2)T]nT [2η - η2(λ2 + (n - 1)τ2)]
n
O λn -[2λ1 +(n - 2)t]nT 区 + (n - 1)t2] η - c2n-1τ2η
≤ 2- - [2λ1 + (n - 2)T]" - 2c4n-1∕2τ
n
η ≤ 2	1 - c4√nτ∕[λn- [2λ1+(n - 2)t 与7]
—λ2 + (n - 1)τ2 - c2τ2∕[λn - [2λ1 + (n - 2)τ]nτ]
2
"	⇒η ≤ λ 2 + (n-1)τ2 -c2τ2∕[λ]-[2λ1+(n-2)τ]nτ]+λ[ [c4√nτ∕[λ,-[2λ1+ (n-2)τ]nτ]]
2	1-c4 √nτ∕[λ⅛-[2λ1 + (n-2)τ ] nτ ]
which is true by our choice of η.
91 - ξ > 1
y⇒n1 + 111 - η∣∣P1K1∣∣2∣ - C4ηn-1∕2τ > 1
nn
Q⇒1 + c4η√nτ < ∣1 - η∣∣P1K1∣∣2∣
^= η∣∣P1K1∣∣2 - 1 > 1 + c4η√nτ
2
b η> ∣∣P1K1∣∣2-C4√nτ
,(18)	> __________________2__________________
η	λ1 — C3[2λ1 + (n — 2)τ ]2nτ2 — c4√nτ
22
Under review as a conference paper at ICLR 2022
which is true by our lower bound on η.
q1q-1 > ξ2
^=q-ι ≥ ξ
^⇒1 + kKP-bti，[η2(λ2 + (n - I)T2) - 2η] > C4ηn-1∕2τ
nkP-1btk22	2
-1 - 2ηλ2 + (2λ1 + (n - I)T)nτ > C4ηn-I∕2τ
n
n
< ⇒η < —-------:- ----:---：~~：-------
2λ2 + 2(2λι + (n — 1)τ )nτ + c4√nτ
which is true.
For inequality (25b): It suffices to take
ki = g ⅛)
For inequality (25c): We just need to show Sinθ <	(ρ-1∕ρ1 )k1.	Calculate that
ξ∕(qι — q-ι) = CoYθsinθ A, then
1	-1	cos2 θ-sin2 θ ,
sin θ < (ρ-1∕ρ1)k1
^=ξ∕m - q-1) < 0∙9(P-ι∕ρI)k1
Uξ< 0.9(qι — q-i)(q-1-ξ)k1
q1 + ξ
log(qι +ξ)
一ξ < 0.9(qι — q-i)(Eβ∕(2∣∣b0k2)) -Y)
=ξ < (qι — l)poly(eβ)
^= √nτ ≤ poly(eβ)
For inequality (25d): Suffice to show
ξ < (qi - I)e0
B0 + eβ0∕2
^=√nτ ≤。⑴
□
Lemma 27 (Long run behavior of SGD with small step size). Under the same notations and as-
sumptions as Lemma 26 unless otherwise specified. Consider another k2 — k1 steps of SGD update
with step size
η < λ2 + c7√nτ
where the constant c7 ≥ √nτ + c4. Then we havefor k > kι:
•	Bk ≤ eβ
qAk-1	, Ak-1 > β
•	Ak ≤ β	, Ak-1 < β
where q := q1(η0) + ξ(η0)e < 1.
Proof. Denote q1 = q1(η'),q-1 = q-1(η0),ξ0 = ξ(η0), then q = q1 + ξ0e, denote B = ∣∣b0∣∣2 *
ρ1k1 + eβ∕2. We have by proof of Lemma 26 that q-0 1 + ξ < 1. We claim the following holds:
q < 1	(26a)
ξ0B ≤ (1 — q10 )eβ	(26b)
23
Under review as a conference paper at ICLR 2022
Check inequality (26a):
q10 + ξ0 < 1
^=n■-1 + 111 - η0∣∣PιKιk2∣ + c4η0n-1∕2τ < 1
nn
^⇒∣1 -η0∣∣PιKι∣∣2∣ < 1 - c4η√nτ
^⇒c4η0√nτ < η[∣PιKιk2 < 2 — c4η0√nτ
c4√nτ < ∣∣PιKιk2
O < η0 < _______2______
I kp	∣∣P1K1 k2+c4√nτ
√ √nτ<。⑴
U= ∖ η <________2______
λ1+nτ 2+c4√nτ
which are true by assumption.
Check inequality (26b):
ξ0B ≤ (1 - q10 )β
U⇒c4η0n-1/2T(∣bo∣2 * Pk + eβ∕2) ≤ (1 - n-1 - 1(1 - η0∣PιKι∣∣2))eβ
nn
U⇒c4η0√nτ(∣bo∣2 * exp(kι)logρ1 + eβ∕2) ≤ η0∣PιKιk2eβ
√nτ ≤ poly(eβ).
With (26) we can prove the lemma by mathematical induction. Suppose Bk-1 ≤ eβ and Ak-1 ≤
Ak1 ≤ B, then check
Bk ≤ ξ0Ak-1 + q-0 1Bk-1
≤ ξ0B + q-0 1eβ
(26b)
≤ eβ
and
Ak ≤ q10 Ak-1 + ξ0Bk-1
≤ q10 Ak-1 + ξ0eβ
≤ (q10 + ξ0e) max{Ak-1, β}
qAk-1	, Ak-1 > β
qβ < β , Ak-1 < β
□
We recap Theorem 5 using our notations in previous lemmas as follows:
Theorem 28 (Directional bias of the two-stage SGD). Use the two stage SGD scheme as defined in
Lemma 26 and 27. Assume nτ < poly(e), then there exists kι = O (log ɪ) and k? such that
(1 - 2e)γ1 ≤
E[∣K bk2k2]
E[kbk2k2]
≤ γ1
where γ1 is the largest eigenvalue of K.
Proof. In Lemma 26 let β = βo, then for kι = O (log ɪ) We have B3 ≤ eβo. For the 2nd stage, by
Lemma 27 we can early stop at k2 such that Ak2 ≥ β0 and Ak2+1 < β0 . We then have
Bk2 ≤ eβ0 ≤ eAk2
24
Under review as a conference paper at ICLR 2022
Then we check
EkKbk⅛
Eg、
E VZkKP-1bk2 k2 + ∣∣KP1bk2 k2 + 2hK P-Ibk2, KTPbkJ
Ekbk2 k2
≥ E VZkKP1bk2 ∣∣2-2kP-ι K1k2∣∣P1K1k2kbk2∣∣2
—	E版 ∣∣2
≥EVZkKP1bk2112- E,2kP-ιKιk2kPιKιk2kbk2∣∣2
一	Ekbk21∣2
EkKT P1bk2∣∣2 二 √2kP-κ¾Pκ¾⅛
Ekbk2∣∣2
(17),(18) /---------------------------------
≥	λ21 - c32 [2λ1 + (n - 2)τ]2nτ2
EkPIbk2∣∣2
EIlPIbk2∣∣2 + E kP-1bk2 k2
-J2(λ1 + √n )(c3(2λ1 + (n — 2)τ ∖√n)
≥√λ2 - c2[2λ1 + (n - 2)τ ]2nτ 2 ^βoβ+-β0- √2(λ1+ √nτ )(c3(2λ1 + (n - 2)τ )√nτ)
(13)
≥ (γ1 - nτ
—c3(2λ1 + (n — 2)τ )√nτ )(1 — e) — J2(λ1 + √nτ )(c3(2λ1 + (n — 2)τ ∖√τ)
≥γ1(1 - ) - γ1	(By nτ < poly())
=γ1(1 — 2e)
And the upper bound in the theorem is by definition of γ1 .
□
F Directional bias of GD with moderate or small step size
This section includes the proof of Theorem 7. We first rewrite the GD updates as linear combination
of eigenvectors. Then the theorem is proved using the transformed variables and finally transformed
back to original parameters.
The directional bias of GD does not require diagonal dominant gram matrix.
Reloading notations Denote the eigen decomposition of K:
K=GΓGT,Γ=diag(γ1,...,γn),G= [g1,...,gn]
where the eigenvectors gi’s are orthogonal. The GD update as
η
at+ι = at — K (K at — y)
n
Denote wt := GT(at — a), We can rewrite GD update in wt：
wt+1=wt—n r2wt=U-In r2)wt
We recap Theorem 7 to make reading easier as follows：
Theorem 29 (Direction bias of GD). Assume a0 is away from 0, λn + 2nτ < λn-1, GD with step
size:
n
n < K-------湍
(λ1 + nτ)2
For a small e > 0, take k = O (log ɪ), we have
kK(αk — a)k2 V F〜
Yn ≤	kak - ak2	≤ Gτγn
25
Under review as a conference paper at ICLR 2022
Proof. For i = 1, . . . , n, We have
wk(i) = (1 - ηYi2 /n)k w0(i)
Denote q = 1 - ηγ2 /n, then 0 < qι ≤ ... ≤ qn < 1 since
n (13) n n
0 < η <	≤ F ≤ F
(λ1 + nτ)2	Y12	Yi2
Since λn + nτ < λn-1 - nτ, We have Yn < Yn-1 by lemma 20, it folloWs that qn > qn-1. Denote
q = qn-1 /qn < 1, then
Let /k ≤	Y e(wOn))2
q ≤ Y2Pn=ι1(w"
qnk (WOn))2
Pin=-11qn2k-1(w0(i))2
=q2k
log 2
γ12
qnk(w0n))2
Pin=-11(w0(i))2
γn2
(w0n))2
(w(n))2
Pn-ι1(w0i))2
log q
O (log ɪ), We have
^⇒ k ≥ 2
≤
V γ2e
≤ -2
Y12
Thus
kκ(αk- α)k2
kak- α 112
=kΓwkk2
Ilwkk2
=pn=ι(wki))2γ2
Pn=I(Wki))2
.	(Wkn))2γn + pn-ι1 (Wki))2γ2
Pn=I(Wki))2	Pn=I(Wki))2
≤ 寸 + Pn=II (Wki))/
≤Yn + Pn=I(Wki))2 γ1
≤ Yn + γ2 Y e = Yn (1 + e)
γ1
thus
kKfk-Mk2 ≤ PYnJ^
Ilak - α∣∣2
The loWer bound of the theorem holds by definition of Yn .
□
G Effect of directional bias
In this section, We provide the proof for theorems in Section 4.2. There are tWo theorems there,
so We split this section into tWo subsections. Subsection G.1 proves Theorem 9, for a general
problem setting of squared error minimization, it provides a straightforWard understanding for Why
directional bias toWards the largest eigenvalue of the Hessian is good for generalization. Section G.2
proves Theorem 11 by giving concrete generalization bounds of SGD and GD estimators in kernel
regression.
26
Under review as a conference paper at ICLR 2022
G.1 Proof of Theorem 9
Denote V = W - w*, rewrite the objective function as
min kvk22
v
s.t. kAvk22 = a
Denote the eigen decomposition of ATA = QΓQT where Q = [q1 , . . . , qn], QQT = QTQ = I
and Γ = diag([ρ1, . . . ,ρn]), ρ1 ≥ . . . ≥ ρn ≥ 0. Then
n
kAv k22 = X ρi (qiT v)2
i=1
So
n
kAvk22 ≤ ρ1[X(qiT v)2] = ρ1vTQQTv = ρ1kvk22
i=1
The equality is achieved when v is in the direction of q1 , and ρ1 = kAT Ak2. Take L(w) =
kAv k22 = a then the theorem holds.
G.2 Proof of Theorem 11
Calculate △1: Denote f=α^TK(∙, X) + f, then we have for a f ∈ Hs, f = ατK(∙, X), let
b = α — α, then
kf *-fkH
=kbT K(∙,X) + fkH
= kbτK(∙, X)kH + 吁kH + 2hbTK(∙, X), fiH
where we can check
hbTK(∙, X), fiH = X bihK(∙, Xi),f* - αTK(∙, X)iH
i=1
=X bi[hK(∙, xi),f*iH - hK(∙, xi), αTK(∙,x)iH]
i=1
= E bi[f *(xi) - αTK(xi, X)](By reproducing property)
i=1
=	bi [yi - yi] = 0
i=1
And we further calculate that
nn
kbTK(∙,x)kH = hXbiK(∙, xi), XbjK(∙, Xj)iH
i=1	j=1
n
=X bibjhK(∙, Xi),K(∙, Xj)i
i,j=1
n
= X bibjK(Xi, Xj) = bTKb
i,j=1
That is,
and
LD (f ) = bT K b + kfkH
inf LD(f) = kfkH
f∈Hs
27
Under review as a conference paper at ICLR 2022
It follows that
∆(f) = LD(f) - inf LD(f) = bTKb
f∈Hs
We claim that
δ =	1 minl 2	bT Kb = -1 kK bk2 = 2na∕γι
b: 2n kKbk2=a	γ1
where the equality is obtained when b is in the direction of the largest eigenvector of K. To see
this, we check kKbk22 ≤ γ1 bT Kb. Recall the eigendecomposition of K = GΓGT where G =
[g1, . . . , gn] has orthogonal columns and Γ = diag(γ1, . . . , γn). Then
n
kKbk22=Xγi2(giTb)2
i=1
and
n
bTKb = X γi(giTb)2
i=1
So we have
n
kKbk22 ≤ γ1[Xγi(giTb)2] = γ1bTKb
i=1
This finishes our claim on ∆1.
SGD output: By Theorem 5, the SGD output has
(1-2)γ1E[kbk2k2] ≤E[kKbk2k2]
Thus
un
EQl∕2(fSGD )] = E ʌ χ γi(gT bSGD )2
i=1
n
≤ √τ1E[(X(gTbSGD)2)1/2]
i=1
=√7TEkbsGD k2
≤ √YTE[kKbSGDk2]∕[(1 - 2e)γ1]
=p2na∕γ1∕(1 - 2e)
=——(∆* )1/2
1	- 2e ( a)
< (1+4e)(∆a)1∕2
last inequality by let e < 1∕4.
GD output: By Theorem 7, the GD output has
kK bGD k2
kbGD k2
≤ (1 +e0)γn2
28
Under review as a conference paper at ICLR 2022
Thus
n
∆(fGD) = Xγi(giTbGD)2
i=1
n
≥ γn X(giTbGD)2
i=1
= γnkbGDk22
≥ YnkKbGDk2∕[(1 + CY]
= 2na/[(1 + 0)γn]
=W 7
>γ (1-e0)∆a
γn
：=M ∆a
where M > 1 by taking 0 < 1 - γn ∕γ1 .
H	Experiments
We list the implementation details of the experiments at the end of Section 4 and include more
experiment results. For better presenting, we split into two subsections: Subsection H.1 includes the
details of simulation; Subsection H.2 is about the NN experiment on FashionMNIST, including the
data description, network structure, and algorithm details, also there are more experiment results in
Subsection H.2 that are not listed in Section 4 due to page limit.
H. 1 S imulation
This subsection is corresponding to Figure 1.
Data Generation. The training data is simulated as follows: Set n = 10,p = 100, simulate Xn×p
where elements of X are i.i.d. N(0, 1); denote ith row of X as xi, normalize xi such that it has
squared '2 norm in [.49,1]; set yi = Pjp==r Sin(Xij) + ei where e% i" N(0, .01). The testing data
is simulated in exactly the same way, except that we only simulate n = 5 testing data.
Kernel Function. We set the kernel function to be the polynomial kernel
K(x1,x2) = (hx1, x2i + .01)2
SGD and GD implementation. Both SGD and GD is run for small and moderate step sizes. The
moderate step size scheme for SGD is: η1 = .1 for the first 50 steps, and η2 = .01 for the next 1000
steps; for GD is: η1 = .5 for the first 50 steps, and η2 = .05 for the next 1000 steps. The small step
size scheme for SGD is η = 0.01 for 1050 steps; for GD is η = 0.05 for 1050 steps. Note that the
step size for SGD is a fraction of that for GD, this matches our Theorem 5 and 7 that the step size of
GD is of magnitude n∕2 times that of SGD.
H.2 Neural Network on FashionMNIST
This subsection is corresponding to Figure 2.
Dataset. The original FashionMNIST consist of 60, 000 training data and 10, 000 testing data. We
randomly sample 1, 500 data from original training data for training, and use all 10, 000 original
testing data for testing. All data entries are normalized to [0, 1].
Network structure. we use a 6-layer ResNet-like (He et al., 2016) Neural Network, and the struc-
ture is as follows
Input ⇒ 7 × 7 Conv ⇒ BatchNorm ⇒ ReLU ⇒ 3 × 3 MaxPool
⇒ ResBlock1 ⇒ ResBlock2 ⇒ Global AvePool ⇒ FC ⇒ output
29
Under review as a conference paper at ICLR 2022
The Residual Blocks are as Figure 7.6.3 in Zhang et al. (2021) (without 1 × 1 convolution). Note
that each residual block contains two 3 × 3 convolutional layers, thus total number of layers is as
stated.
Algorithm. We minimize the Cross Entropy Loss objective L(W) = n PZi Ii(W), where li(w) is
the loss function at ith sample. One step SGD is as follows:
Wt+1 = Wt - ηt ∣I∣ X Nli(Wt)
|I| i∈I
where Iis a randomly sampled subset of {1, . . . , n} (uniform random sample without replacement).
We choose the batch size ∣I∣ to be 25.
One step GD is as follows:
Wt+i = Wt - ηtNL(Wt)
Both SGD and GD are run using two settings of step sizes ηt . The moderate step size setting is as
follows:
ηt
And the small step size setting has ηt
0.2,	t=1,...,5000
0.02, t= 5001,...,20000
= 0.02, t = 1, . . . , 20000.
Comparison of convergence direction. Since the loss surface is nonconvex and the Hessian
varies, we follow Wu et al. (2021) to measure the convergence direction by Relative Rayleigh Quo-
tient(RRQ), which normalizes the Rayleigh Quotient by the maximum eigenvalue of the Hessian as
follows
▽L(W)>	n2 L (in)	yL(W)
kVL(w)k2 ∙ V L(W) ^ W(w)k2
RRQ(W) =-----------kvwɪ----------------
where L(w) is the loss function on the whole training set. A high RRQ indicates that the convergence
direction of w is close to a larger eigenvector of the Hessian.
Comparison of test accuracy. We set 20 different random seeds. For each random seed, we run:
SGD with moderate step size, GD with moderate step size, SGD with small step size, GD with small
step size. For each algorithm, we evaluate its test accuracy once every 500 steps, and use the average
of the last 5 values as its test accuracy. We list the test accuracy in Table 1.
Experiment	#1	#2	#3	#4	#5	#6	#7	#8	#9	#10
SGD + moderate LR	83.69	82.95	82.37	82.05	83.4	83.16	83.72	83.29	83.28	83.23
GD + moderate LR	80.93	80.79	80.79	81.80	81.68	81.12	82.43	81.63	80.94	81.54
-SGD + small LR-	82.00	81.72	81.34	81.92	82.63	82.67	82.99	82.22	80.78	82.10
GD + Small LR	78.88	78.71	78.49	79.3	80.45	79.78	80.15	79.66	79.54	79.68
Experiment	#11	#12	#13	#14	#15	#16	#17	#18	#19	#20
SGD + moderate LR	83.12	82.92	83.58	83.47	82.35	83.57	83.59	82.43	84.21	83.12
GD + moderate LR	82.41	81.56	81.42	80.86	81.23	81.25	81.82	80.42	81.80	82.12
SGD + Small LR	82.62	80.66	82.01	81.01	81.32	81.66	82.12	80.78	82.28	82.48
GD + small LR	80.08	78.29	79.93	79.36	78.9	79.69	80.2	79.62	79.98	79.69
Table 1: Test Accuracy
We also use one-side Wilcoxon signed-rank test to check if the test accuracy of different algorithm
are significantly different, the result is in Table 2. All the p-values are significant at 0.01 level, so
we reject the null hypothesis and conclude that the SGD with moderate step size has test accuracy
significantly higher than all other algorithms.
Null Hypothesis on Test Accuracy	p-value
SGD + moderate LR ≤ GD + moderate LR	9.54 X 101
SGD + moderate LR ≤ SGD + small LR	9.54 X 101
SGD + moderate LR ≤ GD + small LR	9.54 × 10-1
Table 2: Wilcoxon signed-rank test result
30
Under review as a conference paper at ICLR 2022
Augmuels 电
IOOOO
# iterations
-GD, step size 0.001 (77.13)
GD, step size 0.005 (79.91)
-GD, step size 0.01 (79.98)
-GD, step size 0.02 (79.98)
-GD, step size 0.1 (80.96)
-GD, step size 0.2 (81.45)
-GD, step size 0.5 (81.65)
-GD, step size 1.0 (81.2)
10000	15000	20000
# iterations
Figure 3: Use more step sizes in SGD/GD. The test accuracy is evaluated once every 500 iterations,
and inside the bracket is the average of the last 5 test accuracy values.
Additional experiments. We conduct more experiments using different step sizes. The initial step
size is taken in {1, 0.5, 0.2, 0.1, 0.02, 0.01, 0.005, 0.001}, and the step size is divided by a factor of
10 after 5000 steps. The test accuracy is in Figure 3, where we see that SGD with step size 0.2 has
the best test accuracy, and GD with step size 0.5 performs better than GD with any other step sizes,
but is still worse than the best SGD.
31