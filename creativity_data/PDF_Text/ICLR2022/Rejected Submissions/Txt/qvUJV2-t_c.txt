Under review as a conference paper at ICLR 2022
Using a one dimensional parab olic model
of the full-batch loss to estimate learning
RATES DURING TRAINING
Anonymous authors
Paper under double-blind review
Ab stract
A fundamental challenge in Deep Learning is to find optimal step sizes for
stochastic gradient descent automatically. In traditional optimization, line
searches are a commonly used method to determine step sizes. One problem in
Deep Learning is that finding appropriate step sizes on the full-batch loss is unfea-
sibly expensive. Therefore, classical line search approaches, designed for losses
without inherent noise, are usually not applicable. Recent empirical findings sug-
gest that the full-batch loss behaves locally parabolically in the direction of noisy
update step directions. Furthermore, the trend of the optimal update step size
changes slowly. By exploiting these findings, this work introduces a line-search
method that approximates the full-batch loss with a parabola estimated over sev-
eral mini-batches. Learning rates are derived from such parabolas during training.
In the experiments conducted, our approach mostly outperforms SGD tuned with
a piece-wise constant learning rate schedule and other line search approaches for
Deep Learning across models, datasets, and batch sizes on validation and test ac-
curacy.
1	Introduction
Automatic determination of an appropriate and loss function-dependent learning rate schedule to
train models with stochastic gradient descent (SGD) or similar optimizers is still not solved satisfac-
torily for Deep Learning tasks. The long-term goal is to design optimizers that work out-of-the-box
for a wide range of Deep Learning problems without requiring hyper-parameter tuning. Therefore,
although well-working hand-designed schedules such as piece-wise constant learning rate or cosine
decay exist (see (Loshchilov & Hutter, 2017; Smith, 2017)), it is desired to infer problem depended
and better learning rate schedules automatically.
This work builds on recent empirical findings; among those are that the full-batch loss tends to have
a simple parabolic shape in SGD update step direction (Mutschler & Zell, 2021; 2020) (see Figure
1) and that the trend of the optimal update step changes slowly (Mutschler & Zell, 2021) (see Figure
Figure 2). Exploiting these and more found observations, we introduce a line search approach,
approximating the full-batch loss along lines in SGD update step direction with parabolas. One
parabola is sampled over several batches to obtain a more exact approximation of the full-batch loss.
The learning rate is then derived from this parabola. As the trend of the optimal update step-size on
the full-batch loss changes slowly, the line search only needs to be performed occasionally; usually,
every 1000th step. This results in low computational overhead.
The major contribution of this work is the combination of recent empirical findings to derive a line
search method, which is built upon real-world observations and less on theoretical assumptions.
This method outperforms the most prominent line search approaches introduced for Deep Learning
(Vaswani et al. (2019); Mutschler & Zell (2020); Kafka & Wilke (2019); Mahsereci & Hennig
(2017)) across models, datasets usually considered in optimization for Deep Learning, in almost
all experiments. In addition, it almost always outperforms SGD tuned with a piece-wise constant
learning rate schedule on validation and test accuracy. The second important contribution is that
we are the first to analyze how the considered line searches perform under high gradient noise
that originates from small batch sizes. While all considered line searches perform poorly -mostly
because they rely on mini-batch losses-, our approach adapts well to increasing gradient noise by
approximating the full-batch loss. However, a significant performance gap still exists between our
1
Under review as a conference paper at ICLR 2022
Figure 1: Losses along the lines of the SGD training processes exhibit a parabolic shape. The
loss of the direction defining mini-batch (green) is excluded from the distribution of mini-batch
losses to show that it is significantly different. This makes line searches on it unfavorable. In
addition, the parabolic property articulates stronger for the full-batch loss (red); thus, this work
aims to approximate it efficiently with a parabola. The figure is created with code and data from
Mutschler & Zell (2021).
line search and an optimally tuned SGD optimizer using momentum, weight decay, and a problem-
specific hand-designed learning rate schedule.
The paper is organized as follows: Section 2 provides an overview of related work. Section 3
derives our line search approach and introduces its mathematical and empirical foundations in detail.
In Section 4 we analyze the performance of our approach across datasets, models, and gradient
noise levels. Also, a comprehensive hyper-parameter, runtime, and memory consumption analysis
is performed. Finally, we end with discussion including limitations in Sections 5 & 6.
2	Related Work
Deterministic line searches: According to (Jorge & Stephen, 2006, §3), line searches are consid-
ered a solved problem, in the noise-free case. However, such methods are not robust to gradient and
loss noise and often fail in this scenario since they shrink the search space inadequately or use too
inexact approximations of the loss. (Jorge & Stephen, 2006, §3.5) introduces a deterministic line
search using parabolic and cubic approximations of the loss, which motivated our approach.
Line searches on mini-batch and full-batch losses and why to favor the latter. The following
motivates the goal of our work to introduce a simple, reasonably fast, and well-performing line-
search approach that approximates full-batch loss.
Many exact and inexact line search approaches for Deep Learning are applied on mini-batch losses
(Mutschler & Zell, 2020; Berrada et al., 2020; Rolinek & Martius, 2018; Baydin et al., 2018;
Vaswani et al., 2019). (Mutschler & Zell, 2020) approximates an exact line search by estimating
the minimum of the mini-batch loss along lines with a one-dimensional parabolic approximation.
The other approaches perform inexact line searches by estimating positions of the mini-batch loss
along lines, which fulfill specific conditions. Such, inter alia, are the Goldberg, Armijo, and Wolfe
conditions (see Jorge & Stephen (2006)). For these, convergence on convex stochastic functions can
be assured under the interpolation condition (Vaswani et al., 2019), which holds if the gradient with
respect to each batch converges to zero at the optimum of the convex function. Under this condition,
the convergence rates match those of gradient descent on the full-batch loss for convex functions
(Vaswani et al., 2019). However, relying on those assumptions and on mini-batch losses only does
not lead to robust optimization, especially not if the gradient noise is high, as will be shown in Sec-
tion 4. (Mutschler & Zell, 2021; 2020) even showed that exact line searches on mini-batch losses
are not working at all. Line searches on the non-stochastic full-batch loss show linear convergence
on any deterministic function that is twice continuously differentiable, has a relative minimum, and
only positive eigenvalues of the Hessian at the minimum (see Luenberger et al. (1984)). In addition,
they are independent of gradient noise. Therefore, it is reasonable to consider line searches on the
full-batch loss. However, these are cost-intensive since a massive amount of mini-batch losses for
multiple positions along a line must be determined to measure the full-batch loss.
Probabilistic Line Search (PLS) (Mahsereci & Hennig, 2017) addresses this problem by performing
Gaussian Process Regressions, which result in multiple one-dimensional cubic splines. In addition,
a probabilistic belief over the first (aka Armijo condition) and second Wolfe condition is introduced
to find appropriate update positions. The major drawback of this conceptually appealing method is
2
Under review as a conference paper at ICLR 2022
update steps (smoothed)
ezis pets etadp
0	2,000	4,000	6,000	8,000
line number/update step
accumulated loss improvement
0	2,000	4,000	6,000	8,000
line number/update step
.darg.rid fo mron
5251505
.2 0. .1 0. .0 .0
0000
-
ratio of step to full-batch min. and norm of dir. grad.
0	2,000	4,000	6,000	8,000
line number/update step
Figure 2: Several metrics to compare update step strategies on the full-batch losses along 10,000
lines measured by Mutschler & Zell (2021): 1. update step sizes, 2. accumulated loss improvement
per step given as: l(0) - l(supd) where supd is the update step of a specific optimizer. This is the
locally optimal improvement to the minimum of the full-batch loss along a line. The right plot shows
almost proportional behavior between the optimal update step and the negative gradient norm of the
direction defining mini-batch loss. The LABPAL&SGD version of our approach performs almost
optimal on ground truth data. Results LABPAL&NSGD are almost identical and thus omitted.
Plotting code based on Mutschler & Zell (2021) with addition of our proposed approach.
its high complexity and slow training speed. A different approach working on the full-batch loss
is Gradient-only line search (GOLSI) (Kafka & Wilke, 2019). It approximates a line search on the
full-batch loss by considering consecutive noisy directional derivatives whose noise is considerably
smaller than the noise of the mini-batch losses. In practice, its performance is rather weak.
Empirical properties of the loss landscape: In Deep Learning, loss landscapes of the true loss
(over the whole distribution), the full-batch loss, and the mini-batch loss can, in general, be highly
non-convex. However, to efficiently perform a line search, some properties of these losses have to
be apparent. Little is known about such properties from a theoretical perspective; however, several
works suggest that loss landscapes tend to be simple and have some properties: Mutschler & Zell
(2021); Li et al. (2018); Xing et al. (2018); Mutschler & Zell (2020); Chae & Wilke (2019); Mah-
sereci & Hennig (2017); Goodfellow et al. (2015); Fort & Jastrzebski (2019); Draxler et al. (2018).
(Mahsereci & Hennig, 2017; Xing et al., 2018; Mutschler & Zell, 2021; 2020) suggest that the full-
batch loss L along lines in negative gradient directions tend to exhibit a simple shape for a set of
Deep Learning problems. This set includes at least classification tasks on CIFAR-10, CIFAR-100,
and ImageNet. (Mutschler & Zell, 2021) sampled the full-batch loss along the lines in SGD update
step directions. This was done for 10, 000 consecutive SGD update steps of a ResNet18’s training
process on a subset of CIFAR-10. Representative plots of their 10, 000 measured full-batch losses
along lines are presented in Figure 1. Relevant insights and found properties of these works will be
introduced and exploited to derive our algorithm in Section 3.
Using the batch size to tackle gradient noise: Besides decreasing the learning rate, increasing the
batch size remains an important choice to tackle gradient noise. McCandlish et al. exploits empirical
information to predict the largest piratical batch size over datasets and models. De et al. adaptively
increases the batch size over update steps to assure that the negative gradient is a descent direction.
Smith & Le introduces the noise scale, which controls the magnitude of the random fluctuations of
consecutive gradients interpreted as a differential equation. The latter leads to the observation that
increasing the batch size has a similar effect as decreasing the learning rate (Smith et al., 2018),
which is exploited by our algorithm.
3	Our approach: Large-Batch Parab olic approximation line
search (LabPal)
3.1	Mathematical Foundations
In this subsection, we introduce the mathematical background relevant for line searches and chal-
lenges that must be solved in order to perform line searches in Deep Learning.
We consider the problem of minimizing the full-batch loss L, which is the mean over a large amount
of sample losses L:
L : Rn → R, θ → |D| X Ld(θ),	(I)
d∈D
3
Under review as a conference paper at ICLR 2022
where D is a finite dataset and θ are n parameters to optimize. To increase training speed generally,
a mini-batch loss LB , which is a noisy estimate of L, is considered:
LB : Rn → R, θ → 而^ X Ld(θ),
|B| d∈B⊂D
(2)
With |B|《|D|. We define the mini-batch gradient at step t as gB,t ∈ Rn as ^θtLb(ΘJ
For our approach, we need the full-batch loss along the direction of the negative normalized gradient
of a specific mini-batch loss. At optimization step t With current parameters θt and a direction
defining batch Bt, LB along a line With origin θt in the negative direction of the normalized batch
gradient gB,t = gB,t∕∣∣gB,t∣∣ is given as：
lB,t : R → R, s → LB(θt + S ∙ -gBt,t),	⑶
Where s is the step size along the line. The corresponding full-batch loss along the same line is given
by：
lt : R → R, s → L(θt + S ∙ -gBt,t).	(4)
Let the step size to the first encountered minimum of lt be smin,t.
TWo major challenges have to be solved in order to perform line searches on L：
1.	To measure lt exactly it is required to determine every Ld(θt + S ∙ -gBt,t) for all d ∈ D
and for all step sizes S on a line.
2.	To assure convergence line searches have to be performed in a descent direction (De et al.,
2016). The simplest form is the direction of steepest descent (Luenberger et al., 1984).
Therefore, the full-batch gradient VθL : Rn → Rn, θ → 者 P VLd(θt) has to be
approximated.
To be efficient, lt has to be approximated sufficiently Well With as little data points d and steps S as
possible, and one has to use as little d as possible to approximate VθL approximated sufficiently
Well. Such approximations are highly dependent on properties ofL. Due to the complex structure of
Deep Neural NetWorks, little is knoWn about such properties from a theoretical perspective. Thus,
We fall back to empirical properties.
3.2	Deriving the algorithm
In the folloWing, We derive our line search approach on the full-batch loss by iteratively exploiting
empirically found observations of (Mutschler & Zell, 2021) and solving the challenges for a line
search on the full-batch loss (see Section 3.1). Given default values are inferred from a detailed
hyper parameter analysis (Section 4.4)
Observation 1: Minima of lB,t can be at significantly different points than minima of lt and can
even lead to update steps, which increase L (Figure 2 center, green and red curve).
Derivation Step 1: This consolidates that line searches on a too loW mini-batch loss are unpromis-
ing. Consequently, We concentrate on a better Way to approximate lt.
Observation 2: lt can be approximated with parabolas of positive curvature, whose fitting errors
are of less than 0.6 ∙ 10-2 mean absolute distance (exemplarily shown in Figure 1).
Derivation Step 2: We approximate lt With a parabola (l(S)t ≈ atS2 + btS + ct With at > 0). A
parabolic approximation needs three measurements of lt. However, already computing lt for one S
only is computationally unfeasible. Assuming i.i.d sample losses, the standard error of M,t(s), de-
creases with 1/√⅛ Thus, M,t -with a reasonable large batch size- is already a good estimator for the
full-batch loss parabola. Consequently, we approximate lt with lBa ,t by averaging over multiple lBi ,t
measured with multiple inferences. Thus, the approximation batch size Ba, is significantly larger as
the, by GPU memory limited, possible batch size Bi . In our experiments, Ba is usually chosen to
be 1280, which is 10 times larger as Bi. In detail, we measure lBa,t at the points S = 0, 0.0001 and
0.01, then we simply infer the parabola’s parameters and the update step to the minimum. These
values of S empirically lead to the best and numerically most stable approximations.
Observation 3: The trend of Smin,t of consecutive lt changes slowly and consecutive lt do not
change locally significantly. (Figure 2 left, red curve).
Observation 4: Smin,t and the direction defining batch’s ||gBt,t|| are almost proportional during
training. (Figure 2 right).
4
Under review as a conference paper at ICLR 2022
Derivation Step 3: Using measurements of lBa,t to approximate lt with a parabola is by far to slow
to compete against SGD if done for each weight update. By exploiting Observation 3 we can ap-
proximate lt after a constant amount of steps and reuse the measured learning rate λ or update step
size supd for subsequent steps. In this case, λ is a factor multiplied by gB,t, whereas supd is a factor
multiplied by gB,t. Observation 4 allows Us to reuse λ. In our experiments, it is sufficient to measure
a new λ or supd every 1000 steps only.
Derivation Step 4: So far, we can approximate lt efficiently and, thus, overcome the first challenge
(see Section 3.1). Now, we will overcome the second challenge; approximating the full batch loss
gradient for each weight update step:
For this, we revisit Smith et al. (2018) who approximates the magnitude of random gradient fluctu-
ations, that appear if training with a mini-batch gradient, by the noise scale ν ∈ R:
V ≈ (λlDl)∕∣B∣,	(5)
where λ is the learning rate, |D| the dataset size and |B| the batch size. If the random gradient fluc-
tuations are reduced, the approximation of the gradient gets better. Since we want to estimate the
learning rate automatically, the only tunable parameter to reduce the noise scale is the batch size.
Observation 5: The variance of consecutive smin,t is low, however, it increases continuously during
training (Figure 2 left, red curve).
Derivation Step 5: It stands to reason that the latter happens because the random gradient fluc-
tuations increase. Consequently, during training, we increase the batch size for weight updates by
iteratively sampling a larger batch with multiple inferences. This reduces the variance of consecutive
smin,t and lets us reuse estimated the λ or supd for more steps. After experiencing unusable results
with the approach of (De et al., 2016) to determine appropriate batch sizes, we stick to a simple
piece-wise constant batch size schedule doubling the batch size after two and after three-quarters of
the training.
Observation 6: On a global perspective a supd that overestimates smin,t optimizes and generalizes
better.
Derivation Step 6: Thus, after estimating λ (or supd) we multiply it with a factor α ∈]1, 2[:
λ = αsmin,t∕∣∣gB,t∣∣.	(6)
Note that under out parabolic property, the first wolfe condition w1, which is commonly used for
line searches, simply relates to α: wι = - α + 1 (See Appendix F).
Algorithm 1 LABPAL&SGD. Simplified conceptional pseudo-code of our proposed algorithm,
which estimates update steps on a parabolic approximation of the full-batch loss. See the published
source code for technical details. Default values are given in parenthesis. For LABPAL&NSGD
SGD is replaced with NSGD, and the update step is measured instead of the learning rate.
Input: Hyperparameters:	18: procedure PERFORM-LINE.SEARCH-STEP()	
-	initial parameters θ0 -	approximation batch size |Ba | (1280) -	inference batch size |Bi | (128) -	SGD steps nSGD (1000),	# or NSGD steps -	step size adaptation α > 1 (1.8) -	training steps tmax (150000) 11, if t ≤ btmax ∙	0.5c	19:	if SamPledBatchSize < |Ba | then 20:	update estimate L of L with over multiple inferences sampled LBt ,t with |Bt| = k(t) • |Bi|) 21:	increase SamPledBatchSize by |Bt | and t by k(t) 22:	else 23:	learningRate — perform parabolic approximation with
- batch size schedule k(t) =	2, elif t ≤ btmax	• 0.75C	3 values of L along the search direction and estimate the
[4, elif t > btmax	• 0.75C	learning rate.
1:	# Variables have global scope 2:	SamPledBatchSize — 0 3:	PerformedSGDStePS — 0 4:	learningRate — 0 5:	θ 一 θo 6:	state — ’line search’ 7:	direction — current batch gradient 8:	t — 0 9:	while t < tmax do 10:	if State is ’line search’ then 11:	PERFORM-LINE-SEARCH.STEP() 12:	end if 13:	if state	is ’SGDTraining’ then 14:	PERFROM_LARGE-BATCH-SGD-STEP() 15:	end if 16:	end while 17:	return θ		24:	learningRate — learningRate ∙α 25:	set SamPledBatchSize and PerformedSGDStePS to 0 26:	state — 'SGDTraining' 27:	end if 28:	end procedure 29: 30:	procedure PERFROM_LARGE-BATCH-SGD-STEP() 31:	if performedSGDsteps < nSGD then 32:	θ — perform SGD update with learningRate and over multiple inferences sampled LBt ,t with |Bt | = k(t) • |Bi |) 33:	increase t by k(t) 34:	increase PerformedSGDStePS by 1 35:	else 36:	direction — current batch gradient 37:	state — 'line search’ 38:	end if 39:	end procedure
5
Under review as a conference paper at ICLR 2022
Combining all derivations leads to our line search named large-batch parabolic approximation line
search (LABPAL), which is given in Algorithm 1. It samples the desired batch size over multiple
inferences to perform a close approximation of the full-batch loss and then reuses the estimated
learning rate to train with SGD (LABPAL&SGD), or it reuses the update step to train with SGD
with a normalized gradient (LABPAL&NSGD). While LABPAL&SGD elaborates Observation 4,
LABPAL&NSGD completely ignores information from ||g||.
4	Empirical Analysis
Our two approaches are compared against other line search methods across several datasets and
networks in the following. To reasonably compare different line search methods, we define a
step as the sampling of a new input batch. Consequently, the steps/batches that LABPAL takes to
estimate a new learning rate/step size are considered, and optimization processes are compared on
their data efficiency.
Since most line search approaches are introduced without a momentum term, no momentum terms
are used. Note that the base ideas of the introduced line search approaches can be applied upon any
direction giving technique such as Momentum, Adagrad (Duchi et al., 2011) or Adam (Kingma &
Ba, 2015). Results are averaged over 3 runs.
4.1	Performance analysis on ground truth full-batch loss and proof of
CONCEPT
To analyze how well our approach approximates the full-batch loss along lines, we extended the
experiments of Mutschler & Zell (2021) by LABPAL. Mutschler & Zell (2021) measured the full-
batch loss along lines in SGD update step directions of a training process; thus, this data provides
ground truth to test how well the approach approximates the full-batch loss. In this scenario, LAB-
PAL&SGD uses the full-batch size to estimate the learning rate and reuses it for 100 steps. No
update step adaptation is applied. Figure 2 shows that LABPAL&SGD fits the update step sizes to
the minimum of the full-batch loss and performs near-optimal local improvement. The same holds
for LABPAL&NSGD.
We now test how our approaches perform in a scenario for which we can assure that the used em-
pirical observations hold. Therefore, we consider the optimization problem of (Mutschler & Zell,
2021) from which all empirical observations were inferred, which is training a ResNet20 on 8% of
CIFAR10. Ba of 1280 is used for both approaches. Learning rates are reused for 100 steps, and
α = 1.8 is considered. The batch size is doubled after 5000 and 7500 steps. For SGD λ is halved af-
ter the same steps. A grid search for the best λ is performed. Figure 3 shows that LABPAL&NSGD
with update step adaptation outperforms SGD, even though 9% of the training steps are used to es-
timate new update step sizes. This shows that using the estimated learning rates and step sizes leads
to better performance than keeping them constant or decaying them with a piece-wise schedule. In-
ycarucca.la
0.75
0.7
0.65
0.6
0.55
05
0.50	1	2	3	4	5	6	7	8	9	10
training step ∙103
42
pets etadpu
6
0	1	2	3	4	5	6	7	8	9	10	0	1	2	3	4	5	6	7	8	9	10
training step ∙103	training step ∙103
Figure 3: Training process on the problem of which the empirical observations were inferred (
ResNet-20 trained on 8% of CIFAR-10 with SGD). LABPAL&NSGD and LABPAL&SGD outper-
form SGD. Interestingly LABPAL&NSGD estimated huge λs, whereas supds are decreasing
6
Under review as a conference paper at ICLR 2022
terestingly huge λs of up to 80, 000 are estimated, whereas supds are decreasing. LABPAL&SGD
shows similar performance as SGD; however, it seems beneficial to ignore gradient size information
as the better performance of LABPAL&NSGD shows.
4.2	Performance comparison to SGD and to other line search approaches
We compare the SGD and NSGD variants of our approach against PLS (Mahsereci & Hennig, 2017),
GOLSI (Kafka & Wilke, 2019), PAL (Mutschler & Zell, 2020), SLS (Vaswani et al., 2019) and SGD
(Robbins & Monro, 1951). The latter is a commonly used optimizer for Deep Learning problems
and can be reinterpreted as a parabolic approximation line search on mini-batch losses (Mutschler
& Zell, 2021). PLS is of interest since it approximates the full-batch loss to perform line searches.
PAL, GOLSI, SLS on the other hand are line searches optimizing on mini-batch losses directly. For
SGD, a piece-wise constant learning rate schedule divides the learning rate after two and again after
three-quarters of the training.
113
10 -0 -0
etar gninrae
10-5
ssol gniniart
learning rate CIFAR-100 ResNet-20
0	0.2 0.4 0.6 0.8	1	1.2 1.4
steps	∙io5
training loss CIFAR-100 ResNet-20
0	100	200	300	400
epochs
validation accuracy CIFAR-100 ResNet-20
learning rate CIFAR-100 MobileNet-V2
0	0.2 0.4 0.6 0.8	1	1.2 1.4
steps	∙io5
training loss CIFAR-100 MobileNet-V2
01
10 -0
ssol gniniart
0	100	200	300	400
epochs
validation accuracy CIFAR-100 MobileNet-V2
102
101
100
10-1
10-2
10-3
10-4
learning rate CIFAR-100 DenseNet-121
0	0.2	0.4	0.6	0.8	1
steps	∙io5
training loss CIFAR-100 DenseNet-121
100
10-1
10-2
0	50	100	150	200	250
epochs
validation accuracy CIFAR-100 DenseNet-121
test accuracy CIFAR-100 ResNet-20
ycarucca tset
一LABPAL-SGD: |Ba|:1280, s:1, α: 1.8, n: 1000
一LABPAL-NSGD: |Ba|:1280, s:1, α: 1.8, n: 1000
^PAL : a: 1.66, μ: 1.0
—GOLSI: c: 0.9, η 2.0, α: 0.0001, β: 0.0
-SLS : c: 0.1, β: 0.9, μ : 0.1
—SGD: λ: 0.1, β: 0.0
一PLS : α: 0.0001, cw: 0.2, c1: 0.005, β: 0.0
0.7
0.65
0.6
test accuracy CIFAR-100 MobileNet-V2
0.75
ycarucca tse
epoch with best val. acc.
test accuracy CIFAR-100 DenseNet-121
0.75
0.
ycarucca tset
epoch with best val. acc.
epoch with best val. acc.
Figure 4: Performance comparison on CIFAR-100 of our approach LABPAL in the SGD and NSGD
variants against several line searches and SGD. Optimal hyperparameters for CIFAR-10 found with
a detailed grid search are reused (Appendix G.1). Here, our approaches surpass the other approaches
on training loss, validation, and test accuracy. Columns indicate different models. Rows indicate
different metrics. Results for CIFAR-10, ImageNet and SVHN are given in appendix Figures 8, 9
and 10. The batch-size used is 128.

7
Under review as a conference paper at ICLR 2022
Comparison is done across several datasets and models. Specifically, we compare ResNet-20 (He
et al., 2016), DenseNet-40 (Huang et al., 2017), and MobileNetV2 (Sandler et al., 2018) trained on
CIFAR-10 (Krizhevsky & Hinton, 2009a), CIFAR-100 (Krizhevsky & Hinton, 2009b), and SVHN
(Netzer et al., 2011). In addition, we compare MobileNetV3 (Howard et al., 2019) trained on Im-
ageNet (Deng et al., 2009). We concentrate on classification problems since the empirical obser-
vations are inferred from a classification task and since those problems are usually considered to
benchmark new optimization approaches.
For each optimizer, we perform a comprehensive hyper-parameter grid search over all models trained
on CIFAR-10 (see Appendix G.1). The best performing hyper-parameters on the validation set are
then reused for all other experiments. The latter is done to check the robustness of the optimizer by
handling all other datasets as if they were unknown, as is usually the case in practice. Our aim here
is to show that satisfactory results can be achieved on new problems without any fine-tuning needed.
Further experimental details are found in Appendix G.
Figure 4 as well as Appendix Figures 8, 10, 9 show that both LABPAL approaches outperform PLS,
GOLSI and PAL considering training loss, validation accuracy, and test accuracy. LABPAL&NSGD
tends to perform more robust and better than LABPAL&SGD. LABPAL&NSGD outperforms SGD
tuned with a piece-wise constant schedule and challenges SLS on validation and test accuracy. On
CIFAR-100, our approaches even perform better than all others. The important result is that hyper-
parameter tuning for LABPAL is not needed to achieve good results across several models and
datasets. However, this also is true for pure SGD, which suggests that the simple rule of performing
a step size proportional to ||g|| is sufficient to implement a well-performing line search. This also
strengthens the observation of (Mutschler & Zell, 2021), which states that SGD, with the correct
learning rate, is already performing an almost optimal line search.
The derived learning rate schedules of the LABPAL approaches are significantly different from those
of the other line search approaches (Figure 4, 8, 10 first row). Interestingly they show a strong warm
up phase at the beginning of the training followed by a rather constant phase which can show minor
learning rate changes with an increasing trend. The warm up phase is often seen in sophisticated
learning rate schedules for SGD; however, usually combined with a cool down phase. The latter
is not apparent for LABPAL since we increase the batch size. LABPAL&NSGD indirectly uses
learning rates of up to 107 but still trains robustly.
A comparison of training speed and memory consumption is given in Appendix D. In short, LAB-
PAL has identical GPU memory consumption as SGD and is on average only 19.6% slower. How-
ever, for SGD usually a grid search is needed to find a good λ, which makes LABPAL considerably
cheap.
4.3	Adaptation to varying gradient noise
Recent literature, w.g. (Mutschler & Zell, 2020), (Vaswani et al., 2019), (Kafka & Wilke, 2019)
show that line searches work with a relatively large batch size of 128 and a training set size of
approximately 40000 on CIFAR-10. However, a major, yet not deeply considered problem is that
line searches operating on the mini-batch loss vary their behavior with another batch- and training
set sizes leading to varying gradient noise. E.g., Figure 5 shows that training with PAL and a batch
size of 10 on CIFAR-10 does not work at all. The reason is that the by mini-batches induced gradient
noise, and with it the difference between the full-batch loss and the mini-batch loss, increases.
However, we can adapt LABPAL to work in these scenarios by holding the noise scale it is exposed
to approximately constant. As the learning rate is inferred directly, the batch size has to be adapted.
Based on the linear approximation of the noise scale (see Equation 5), we directly estimate a noise
adaptation factor ∈ R to adapt LABPAL’s hyperparameter:
νnew
€ ：=------
νori
IBoriIIDnew | =	128 |Dnew|
∣Bnew I IDoriI — ∣Bnew ∣ 40, 000
(7)
The values the original batch size IBori I and the original dataset size IDoriI originate from our search
for best-performing hyperparameters on CIFAR-10 with a training set size of 40,000, a batch size of
128, and 150,000 training steps. We set the number of training steps to 150, 000€ and multiply the
batch sizes in the batch size schedule k by €. This rule makes the approach fully parameter less in
practice.
8
Under review as a conference paper at ICLR 2022
validation accuracy CIFAR-10 DenseNet-121
0.92
0.9
0.88
0.86
0.84
0.82
0.8
50
150
200
100
epochs
LABPAL-NSGD : t: 1.0
—Labpal-SGD ： s：i.o
PAL : α: 1.66, μ: 1.0
--- GOLSI: c: 0.9, η: 2.0, α: 0.0001, β: 0.0
---SGD: λ: 0.01, β: 0.0
-SLS:c: 0.1, β: 0.9, μnt: 0.1
---PLS: α: 0.0001, cw: 0.3, c1: 0.5, β: 0.0
0.92
0.9
g
0.88
0.86
0.84
>
0.82
0.8
0	50	100	150	200
validation accuracy CIFAR-10 DenseNet-121
*~.. f∙.	,∙ 1V 1. 1 ■ ∙ A * » ,
---LABPAL-NSGD
---LABPAL-SGD : t: 12.8
S— PAL: a: 1.0, μ: 1.0
---SGD: λ: 0.01, β: 0.0
—SLS:c: 0.1, β: 0.99, μ,n,y. 0.1
---PLS: α: 0.0001, cw: 0.3, c1: 0.5, β: 0.0
---GOLSI: c: 0.99, η 2.0, α: 0.0001, β: 0.0
epochs
0
Figure 5: Performance comparison at batch size 10 on CIFAR-10. Left: reusing the same hyper-
parameters as for batch size 128. Right: applying the directly estimable noise adaptation factor to
the LABPAL approaches and performing a grid search for optimal hyperparameters for the other
approaches. On validation and test accuracy the LABPAL approaches outperform SGD, whereas on
training loss they compete with it. (For more details see Appendix Figure 11 & 12). PLS curves are
incomplete since training failed.
Figure 5 shows that LABPAL approaches fail when the gradient noise factor is not applied, but they
improve their performance significantly if it is applied. Now, they even surpass SGD on validation
and test accuracy (Appendix Figure 11 & 12). For all other approaches, a comprehensive hyperpa-
rameter search is done (see Appendix G.1). Except for SGD, even the best-found hyperparameters
could not handle the higher noise.
4.4	Hyperparameter Sensitivity Analysis
We performed a detailed hyperparameter sensitivity analysis for LABPAL&SGD and LAB-
PAL&NSGD. To keep the calculation cost feasible, we investigated the influence of each hyperpa-
rameter, keeping all other hyperparameters fixed to the default values (see Algorithm 1). Appendix
Figure 6 and 7 show the following characteristics: Estimating new supd or λ with Ba smaller than
640 decreases the performance since lt is not fitted well enough (row 1). The performance also
decreases if reusing the λ (or supd) for more update steps (row 2), and if using a step size adaptation
α of less than 1.8 (row 3, except for ResNet). This shows that optimizing for the locally optimal
minimum in line direction is not beneficial. From a global perspective, a slight decrease of the loss
by performing steps to the other side of the parabola shows more promise. Interestingly, even using
α larger than two still leads to good results. (Mutschler & Zell, 2021) showed that the loss valley in
line direction becomes wider during training. This might be a reason why these update steps, which
should actually increase the loss, work. Using a maximal step size of less than 1.5 (row 7) and
increasing the noise adaptation factor (row 9) while keeping the batch size constant also decreases
the performance. The latter indicates that the inherent noise of SGD is essential for optimization. In
addition, we considered a momentum factor and conclude that a value between 0.4 and 0.6 increases
the performance for both LABPAL approaches (row 5).
5	Limitations
Our approach can only work if the empirically found properties we rely on are apparent or are still
a well enough approximation. In Section 4.2 We showed that this is valid for classification tasks. In
additional sample experiments, we observed that our approach also works on regression tasks using
the square loss. However, it tends to fail if different kinds of losses from significantly different heads
of a model are added, as it is often the case for object detection and object segmentation.
A theoretical analysis is lacking since the optimization field still does not know the reason for the
local parabolic behavior of lt is, and consequently, what an appropriate function space to consider
for convergence is.
6	Discussion & Outlook
This work introduces a robust line search approach for Deep Learning problems based upon em-
pirically found properties of the full-batch loss. Our approach estimates learning rates well across
models, datasets, and batch sizes. It mostly surpasses other line search approaches and challenges
SGD tuned with a piece-wise constant learning rate schedule. We are the first line search work that
analyses and adapts to varying gradient noise. In addition, we show that mini-batch gradient norm
information is not necessary for training. In future, we will analyze the causes for the local parabolic
behavior of the full-batch loss along lines, to get a better understanding of DNN loss landscapes and
especially of why and when specific optimization approaches work.
9
Under review as a conference paper at ICLR 2022
Reproducibility
Experimental details including all hyperparameters used for the experiments presented in Section 4
are found in App. G.1. The source code to reproduce our experiments including our implementations
of SLS, GOLSI, PLS, PAL and LABPAL is provided in the supplementary materials.
Ethics S tatement
Since we understand our work as basic research, it is extremely error-prone to estimate its spe-
cific ethical aspects and future positive or negative social consequences. As optimization research
influences the whole field of deep learning, we refer to the following works, which discuss the
ethical aspects and social consequences of AI and Deep Learning in a comprehensive and general
way:Yudkowsky et al. (2008); Muehlhauser & Helm (2012); Bostrom & Yudkowsky (2014).
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wat-
tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learn-
ing on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software
available from tensorflow.org.
Lukas Balles. Probabilistic line search tensorflow implementation, 2017. URL
https://github.com/ProbabilisticNumerics/probabilistic_line_
search/commit/a83dfb0.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood.
Online learning rate adaptation with hypergradient descent. ICLR, 2018.
Leonard Berrada, Andrew Zisserman, and M. Pawan Kumar. Training neural networks for and by
interpolation. ICML, 2020.
Nick Bostrom and Eliezer Yudkowsky. The ethics of artificial intelligence. The Cambridge hand-
book ofartificial intelligence, 1:316-334, 2014.
Younghwan Chae and Daniel N. Wilke. Empirical study towards understanding line search approx-
imations for training neural networks. arXiv, 2019.
Soham De, Abhay Kumar Yadav, David W. Jacobs, and Tom Goldstein. Big batch SGD: automated
inference using adaptive batch sizes. arXiv, 2016.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. CVPR, 2009.
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A. Hamprecht. Essentially no bar-
riers in neural network energy landscape. ICML, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. J. Mach. Learn. Res., 12:2121-2159, 2011.
Stanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural network loss landscapes.
NeurIPS, 2019.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network
optimization problems. ICLR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. CVPR, 2016.
10
Under review as a conference paper at ICLR 2022
Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun
Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Search-
ing for mobilenetv3. ICCV, 2019.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. CVPR, 2017.
Nocedal Jorge and Wright Stephen. Numerical Optimization. Springer series in operations research.
Springer, 2nd ed edition, 2006. ISBN 9780387303031,0387303030.
Dominic Kafka and Daniel Wilke. Gradient-only line searches: An alternative to probabilistic line
searches. arXiv, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, 2009a.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, 2009b.
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets.
NeurIPS, 2018.
Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. ICLR,
2017.
David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming, volume 2. Springer,
1984.
Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization. J.
Mach. Learn. Res.,18:119:1-119:59, 2017.
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of
large-batch training. arXiv, 2018.
Luke Muehlhauser and Louie Helm. The singularity and machine ethics. In Singularity Hypotheses,
pp. 101-126. Springer, 2012.
Maximus Mutschler and Andreas Zell. Parabolic approximation line search for dnns. NeurIPS,
2020.
Maximus Mutschler and Andreas Zell. Empirically explaining sgd from a line search perspective.
ICANN, 2021.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. NeurIPS Workshop, 2011.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. NeurIPS, 2019.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,
22:400-407, 1951.
Michal Rolinek and Georg Martius. L4: Practical loss-based stepsize adaptation for deep learning.
NeurIPS, 2018.
Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR, 2018.
Leslie N. Smith. Cyclical learning rates for training neural networks. WACV, 2017.
11
Under review as a conference paper at ICLR 2022
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. ICLR, 2018.
Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don’t decay the learning
rate, increase the batch size. ICLR, 2018.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. NeurIPS,
2019.
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv, 2018.
Eliezer Yudkowsky et al. Artificial intelligence as a positive and negative factor in global risk.
Global catastrophic risks, 1(303):184, 2008.
12
Under review as a conference paper at ICLR 2022
A Hyperparamter
ResNet-20
Sensitivity Analysis
MobileNet-V2
DenseNet-121
1.00-
0.90-
0.88-
1.00-
0.90-
• training acc.・ validation acc. ▼ test acc.
1.0	5.0	10.0	20.0	30.0	40.0	50.0
approximation batch size
0.88-
1.0	5.0	1 0.0	20.0	30.0	40.0	50.0
approximation batch size
0.90 -
0.88-
1.0	5.0	10.0	20.0	30.0	40.0	50.0
approximation batch size
SGD steps
SGD steps
1.00 -
0.98；
X 0.94 ∙
⅛ 0.92-
0.9)-
0.88-
0.8 1.0 1.2 1.4 1.6 1.8 1.9 2.0 2.1 2.2 3.0 4.0
step size adaptation
[M, 1) (75000.0, 2) “i25≈o, 3)]	[(oo, 1) (75≡0o, 2). “n、4)]	[(0.0, 1) (75≡0o, 3), “i25≈a 9)]
batch size schedule
batch size schedule
0.90 -
0.88-
[2、1) (75000∙0,2⅛ (112500∙⅛3)]	[0、1) (75000∙0,2⅛ “125=,4)]	[M, 1) (750^.0,3⅛ (1125≈O,9)]
batch size schedule
momentum
momentum
0.98；
0.94：
0.92-
0.90 ■
0.88-
0.0	0.2	0.4	0.6	0.8	0.9	0.99
momentum
max step size
0.98;
0.94：
0.92-
0.90 ■
0.88-
0.1	0.5	1.
0	1.5	2.0	2.5	3.0	4.0	5.0
max step size
noise factor
noise factor
1.00 ■
0.98⅛
0.94 -
g
0.92-
0.90 ■
0.88-
1.0	2.0
5.0	10.0	20,0	1 00.0
noise factor
Figure 6:	Sensitivity analysis of parameters of LABPAL&SGD. The
PAL&NSGD described in Figure 7 are also valid for LABPAL&NSGD.
observations of LAB-
13
Under review as a conference paper at ICLR 2022
ResNet-20
MobileNet-V2
DenseNet-121
approximation batch size
approximation batch size
approximation batch size
NSGD steps
step size adaptation
step size adaptation
0.88
0.88-
[2、1)	(75=0'	2>	(112500∙⅛3)]	[0、1)	(75"0'2'	"125=,4)]	[M,	1)	(750^.0,3⅛	(1125≈O,9)]
0.88
[M, 1) (75000.0, 2), “i25≈o,3]	[(0o, 1» ,5≡0o,2), “n、4)]	[(0.0,	1) (750^^3) “i2500a 9)]	[(oo, 1) ,5≡0o,2), "i25≈o,3]	[(0.0, 1)	(75000,¾2⅛ (195≈.0,4)]	[(0.0, 1ι, (750^^3) "i25≈o,9)]
batch size schedule	batch size schedule
batch size schedule
momentum
momentum

momentum
1.00-
noise factor
noise factor
noise factor
Figure 7:	Sensitivity analysis of parameters of LABPAL&NSGD. The default parameters are: ap-
proximation batch size Ba = 1280, SGD steps s = 1000, step size adaptation α = 1.8, batch size
schedule k = (0:1, 75000:2, 112500:4), momentum β = 0, maximal step size = 1.0, noise-factor
= 1. For Ba the factor 128 is multiplied with is given on the x axis.
14
Under review as a conference paper at ICLR 2022
B Further performance comparisons
learning rate CIFAR-10 ResNet-20
learning rate CIFAR-10 MobileNet-V2
0	0.2	0.4	0.6	0.8	1	1.2	1.4
steps	∙105
training loss CIFAR-10 ResNet-20
0	0.2	0.4	0.6	0.8	1	1.2	1.4
stePs	∙105
training loss CIFAR-10 MobileNet-V2
107
531
10 10 0
etar gninrae
10-1
10-3
learning rate CIFAR-10 DenseNet-121

0	0.2	0.4	0.6	0.8	1
stePs	∙105
training loss CIFAR-10 DenseNet-121
ssol gniniar
10-2
100
100
10-3
12
-10 -
ssol gniniart
100
10-3
10-1
10-2
0	100	200	300	400
epochs
0	100	200	300	400
epochs
0	50	100	150	200	250
epochs
9EJn3。E uo=s5>
ycarucca tse
0.92
0.9
0.88
0.86
0.84
0.82
0.8
0.91
0.9
0.89
0.88
validation accuracy CIFAR-10 ResNet-20
9EJn3。E uo=s5>
validation accuracy CIFAR-10 MobileNet-V2
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8 -
0	100	200	300	400
epochs
test accuracy CIFAR-10 MobileNet-V2
0.94
0.93
9EJn3。E Uo-ap5>
0.9
0.88
0.86
0.84
0.82
0.92
0.94
0.8
0	50	100	150	200	250
epochs
FLABPAL-NSGD: |Ba|: 1280, e. 1, a: 1.8, n: 1000
—LABPAL-SGD: |Ba|: 1280, s: 1, α: 1.8, n: 1000
^PAL: a: 1.66, μ: 1.0
-GOLSI: c: 0.9, η: 2.0, α: 0.0001, β: 0.0
-SLS : c: 0.1, β: 0.9, μ: 0.1
—SGD: λ: 0.1, β: 0.0
— PLS : α: 0.0001, cw: 0.2, c1: 0.005, β: 0.0
ycarucca tse
epoch with best val. acc.
0.89
0.92
0.91
0.9
epoch with best val. acc.
0.89
test accuracy CIFAR-10 DenseNet-121
0.93
0.9
0. 0.
ycarucca tse
epoch with best val. acc.
Figure 8:	Performance comparison on CIFAR-10 of our approach LABPAL in the SGD and NSGD
variants against several line searches and SGD. Optimal hyperparameters are found with an elaborate
grid search. Our approaches challenge and often outperform the other approaches on training loss,
validation, and test accuracy. Columns indicate different models. Rows indicate different metrics.
15
Under review as a conference paper at ICLR 2022
etar gninrael
101
learning rate ImageNet MobileNet-V3
100
10-1
10-2
10-3
0	0.2	0.4	0.6	0.8	1
steps	∙106
validation accuracy ImageNet MobileNet-V3
0.58
0.56
0.54
0.52
ssol gniniart
100.6
100.4
100.2
100.8
training loss ImageNet MobileNet-V3
0
50	100	150	200
epochs
test accuracy ImageNet MobileNet-V3
ycarucca noitadila
ycarucca tse
0.64
0.63
0.62
0.61
LABPAL-NSGD : |Ba|: 10, e: 7, α: 1.8,n: 1000
LABPAL-SGD : |Ba|: 10, e: 7, α: 1.8,n: 1000
SGD : λ: 0.1, β: 0.0
■ SLS : c: 0.1, β: 0.9, μ: 0.1
0.5^---------IMlIi■■ ■■■ L I----------1----------r-
0	50	100	150	200
epochs
0.6
-4	-2	0	2	4
epoch with best VaL acc.	∙10-2
Figure 9:	Performance comparison of top 1 error on IMAGENET of our approach LABPAL in
the SGD and NSGD variants against SLS and SGD. Optimal hyperparameters are found with an
elaborate grid search. Optimal hyperparameters found with a detailed grid search for CIFAR-10 are
reused. Our approaches challenge the other approaches on training loss and test accuracy. SLS fails
shortly after the beginning of the training due to too high estimated learning rates. For LABPAL the
adaptation factor introduced in Section 4.3 is applied.
16
Under review as a conference paper at ICLR 2022
9E.m∞E uo=β=E>
ycarucca tse
learning rate SVHN ResNet-20
97 53113 5
0 0 0 0 0- - -
1111100
etar gninrael
12
-10 -10
ssol gniniart
10-3
0	100	200	300	400
epochs
validation accuracy SVHN ResNet-20
0.96
0.94
0.92
0.9
0.88
0.86
0	100	200	300	400
epochs
test accuracy SVHN ResNet-20
0.980
0.975
0.970
0.965
0.960
0.955
epoch with best val. acc.
一LABPAL-NSGD: |Ba|:1280, c:1, α: 1.8, n: 1000
—LABPAL-SGD: |Ba|: 1280, s:1, α: 1.8, n: 1000
PAL : a: 1.66, μ: 1.0
GOLSI: c: 0.9, η: 2.0, α: 0.0001, β: 0.0
-SLS : c: 0.1, β: 0.9, μ: 0.1
—SGD: λ: 0.1, β: 0.0
— PLS : α: 0.0001, cw: 0.2, c1: 0.005, β: 0.0
ssol gniniart
ycarucca tse
learning rate SVHN MobileNet-V2
97 53113
0 0 0 0 0- -
1 1 1 1 1 10
etar gninrael
i.ι l..⅛Ul.
0	0.2	0.4	0.6	0.8	1	1.2	1.4
steps	∙105
102
0.96
0.9
0.985
0.980
0.975
0.970
0.965
0.960
0.955
0.950
training loss SVHN MobileNet-V2
0
100
400
200	300
epochs
validation accuracy SVHN MobileNet-V2
0.95
0.94
0.93
0.92
0.91
0
100
400
200	300
epochs
test accuracy SVHN MobileNet-V2
epoch with best val. acc.
learning rate SVHN DenseNet-121
864202
0 0 0 0 0-
111110
etar gninrael
0	0.2	0.4	0.6	0.8	1
steps	∙105
training loss SVHN DenseNet-121
102
02
10 -
ssol gniniart
0	50	100	150	200	250
epochs
validation accuracy SVHN DenseNet-121
0.96
0.95
0.94
0.93
0.92
0.91
0.9
0	50	100	150	200	250
epochs
0.980
0.972
test accuracy SVHN DenseNet-121
0.978
0.976
0.974
epoch with best val. acc.
9E.m∞E uo=β=E>
9E.m∞E uo=β5>
9EJn3。E,£
Figure 10: Performance comparison on SVHN of our approach LABPAL in the SGD and NSGD
variants against several line search and SGD. Optimal hyperparameters found with a detailed grid
search for CIFAR-10 are reused. Our approaches challenge and often surpass the other approaches
on training loss, validation, and test accuracy. Columns indicate different models. Rows indicate
different metrics.
17
Under review as a conference paper at ICLR 2022
C Further results for batch size 10
learning rate CIFAR-10 ResNet-20
64202468
10 10 10 10 -10 -10 -10 -10
etar gninrael
etar gninrael
0.2	0.4	0.6	0.8	1	1.2	1.4
stePs	∙106
training loss CIFAR-10 ResNet-20
63
10 10
etar gninrael
1
-10
ssol gniniart
0	50	100	150	200	250	300
epochs
0	50	100	150	200	250	300
epochs
ssol gniniart
ycarucca tse
9EJn3。E u。5P3>
test accuracy CIFAR-10 ResNet-20
9EJn3。E u。5P3>
9EJn3。E u。5P3>
0.900
0.890
0.880
0.870
0.860
一LABPAL-NSGD: s: 1.0
一LABPAL-SGD: s: 1.0
・ PAL : a: 1.66, μ: 1.0
一SGD: λ: 0.01, β: 0.0
—SLS : c: 0.1, β: 0.9, ,snt： 0.1
-GOLSI: c: 0.9, η: 2.0, α: 0.0001, β: 0.0
— PLS : α: 0.0001, cw: 0.3, c1: 0.5, β: 0.0
epoch with best val. acc.
ycarucca tse
ycarucca tse
Figure 11: Performance comparison of several models on CIFAR-10 with batch size 10. The same
hyperparameters are used as for batch size 128 (see Figure 8). PAL and PLS fail in this scenario.
The LABPAL approaches work well in the beginning but fail to estimate well learning rates later;
this is solved in Figure 12. Interestingly, they still achieve competitive validation and test accuracies.
18
Under review as a conference paper at ICLR 2022
etar gninrael
learning rate CIFAR-10 ResNet-20
24
-10 -10
6
-10
etar gninrael
0.2	0.4	0.6	0.8	1	1.2	1.4
StePS	∙106
training loss CIFAR-10 ResNet-20
learning rate CIFAR-10 MobileNet-V2
etar gninrael
learning rate CIFAR-10 DenSeNet-121
ycarucca tSe
1
-10
SSol gniniart
0	50	100	150	200	250	300
epochs
9E.m∞E uo=β=E>
test accuracy CIFAR-10 ResNet-20
0.900
0.890
0.880
0.870
-LABPAL-NSGD: t: 12.8
-LABPAL-SGD: t: 12.8
PAL: a: 1.0, 3:1.0
-SGD: λ: 0.01, β: 0.0
-SLS:c: 0.1, β: 0.99, μm: 0.1
GOLSI: c: 0.99, η: 2.0, α: 0.0001, β: 0.0
-PLS : α: 0.0001, cw: 0.3, c1: 0.5, β: 0.0
ycarucca tSe
epoch with best val. acc.
training loSS CIFAR-10 MobileNet-V2
0	50	100	150	200	250	300
epochs
validation accuracy CIFAR-10 MobileNet-V2
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8-
0	50	100	150	200	250	300
epochs
test accuracy CIFAR-10 MobileNet-V2
0.930
0.925
0.920
0.915
0.910
0.905
0.900
epoch with best val. acc.
training loSS CIFAR-10 DenSeNet-121
0	50	100	150	200
epochs
9E.m∞E uo=β=E>
test accuracy CIFAR-10 DenseNet-121
ycarucca tSe
9E.m∞E uo=β=E>
Figure 12: Performance comPariSon of Several modelS on CIFAR-10 with batch size 10. For the
LABPAL aPProacheS only the noiSe factor iS adaPted according to equation 7. For all other aP-
ProacheS, a grid Search iS Performed to find the beSt hyPerParameterS for thiS Scenario. (See AP-
Pendix G.1). In comPariSon to Figure 11, now, the LABPAL aPProacheS Perform comPetitive on
the training loSS and SurPaSS the other aPProacheS on validation and teSt accuracy. PLS PlotS are
incomPlete Since the training failed after Some StePS.
19
Under review as a conference paper at ICLR 2022
D Wall clock time and GPU memory comparison
Iilabpal-Sgdiilabpal-NSgd
口口	SGD	n	SLS
口口	PAL	H	GOLSI
口口 PLS
speed comparison
.q U--O'生邑。Uo əuɪ=∞u≡si3
BG ni 01-RAFIC no yromem UP
Figure 13: Left: Training time comparison on CIFAR-10. SGD, SLS, and PAL show similar
training times. GOLSI, and both variants of LABPAL are slightly slower (up to 19.6%). However, a
slightly longer training time is acceptable if less time has to be spent in hyper-parameter tuning. PLS
is significantly slower. Note that in comparison to SGD, the implementations of the other optimizers
are not optimized on CUDA level. Right: Maximum allocated memory comparison on CIFAR-10.
Except for PLS all approaches need approximately the same amount of memory.
E Theoretical considerations
As the field does not know what the reason for the local parabolic behavior of the full-batch loss
is and, thus, what an appropriate function space to consider for convergence is, we refer to the
theoretical analysis of (Mutschler & Zell, 2020). They show convergence on a quadratic loss. This
is also valid for LABPAL, with the addition that each mini batch-loss can be of any form as long as
the mean over these losses is a quadratic function.
F RELATION OF UPDATE STEP ADAPTATION α AND THE FIRST WOLFE
CONSTANT w1.
Let f : R → R be of form x 7→ ay2 + by+c. We start with the first Wolfe condition (a.k.a. Armijo
condition, sufficient decrease condition):
f(x0 + y)	≤	f(xo) - yVf (x0)w1	in our case x0 = 0, w1 wolfe constant	(8)
f(y)	≤	f(0) + ybw1		(9)
ay2 + by + c	≤	c + ybw1	use quadratic shape, Vf(x0) = b	(10)
ay2 + by - ybw1	!	0		(11)
ay2 + b		ay -ʒ- + 1= W1 b		(12)
by				
α - 2 + 1	=	w1	-b sety =α 2a, α ∈ [1,2)	(13)
-2w1 + 2	=	α		(14)
20
Under review as a conference paper at ICLR 2022
G	Further experimental details
Further experimental details for the optimizer comparison in Figure 8,4,10,5,11,12 of Sections 4.2
& 4.3.
PLS: We adapted the only available and empirically improved TensorFlow (Abadi et al., 2015)
implementation of PLS (Balles, 2017), which was transferred to PyTorch (Paszke et al., 2019) by
(Vaswani et al., 2019), to run on several state-of-the-art models and datasets.
The training steps for the experiments in section Section 4 were 100,000 for DenseNet and 150,000
steps for MobileNetv2 and ResNet-20. Note that we define one training step as processing one input
batch to keep line search approaches comparable.
The batch size was 128 for all experiments. The validation/train set splits were: 5,000/45,000 for
CIFAR-10 and CIFAR-100 20,000/45,000 for SVHN.
All images were normalized with a mean and standard deviation determined over the dataset. We
used random horizontal flips and random cropping of size 32. The padding of the random crop was
8 for CIFAR-100 and 4 for SVHN and CIFAR-10.
All trainings were performed on Nvidia Geforce 1080-TI GPUs.
Results were averaged over three runs initialized with three different seeds for each experiment.
For implementation details, refer to the source code provided in the supplementary materials .
G.1 Hyperparameter grid search on CIFAR- 1 0
For our evaluation, we used all combinations out of the following hyperparameters.
SGD:
hyperparameter symbol	values
learning rate	λ momentum	α	{0.1, 0.01, 0.001, 0.0001} {0.0}
PAL:
hyperparameter
measuring step size
direction adaptation factor
update step adaptation
maximum step size
symbol values
μ
β
α
smax
{0.1, 1}
{0.0}
{1, 1.66}
{3.16 (≈ 100.5)}
LABPAL (SGD and NSGD):
hyperparameter	symbol	values
step size adaptation	α	{1.0, 1.25, 1.5, 1.8, 1.9}
SGD steps	β	{1000} to keep speed comparable
initial measuring step size	γ	{0.01, 0.1}
parabolic approximation sample step size		{0.1, 0.01}
approximation batch size	b	{5, 10, 20}
batch size schedule	bs	{{0 : 1, 75000 : 2, 112500 : 3}, {0 : 1, 75000 : 2, 112500 : 4}}
GOLSI:
hyperparameter	symbol	values
initial step size	μ	{0.1, 1}
momentum	β	{0.0}
step size scaling parameter	η	{0.2, 2.0}
modified wolfe condition parameter	c2	{0.9, 0.99}
21
Under review as a conference paper at ICLR 2022
PLS:
hyperparameter	symbol	values
first Wolfe condition parameter	c1	{0.005, 0.05, 0.5}
acceptance threshold for the wolfe probability	cW	{0.2, 0.3, 0.4}
initial step size	α0	{10-4}
SLS:
hyperparameter	symbol	values
initial step size	μ	{0.1, 1}
step size decay	β	{0.9, 0.99}
step size reset	γ	{2.0}
Armijo constant	c	{0.1, 0.01}
maximum step size	μmax	{10.0}
22