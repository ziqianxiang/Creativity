Under review as a conference paper at ICLR 2021
Federated learning framework based on
TRIMMED MEAN AGGREGATION RULES
Anonymous authors
Paper under double-blind review
Ab stract
This paper studies the problem of information security in the distributed learn-
ing framework. In particular, we consider the clients will always be attacked
by Byzantine nodes and poisoning in the federated learning. Typically, aggrega-
tion rules are utilized to protect the model from the attacks in federated learning.
However, classical aggregation methods such as Krum(∙) and Mean(∙) are not
capable enough to deal with Byzantine attacks in which general deviations and
multiple clients are attacked at the same time. We propose new aggregation rules,
Tmean(∙), to the federated learning algorithm, and propose a federated learning
framework based on Byzantine resilient aggregation algorithm. Our Tmean(∙)
rules are derived from Mean(∙) by appropriately trimming some of the values
before averaging them. Theoretically, we provide theoretical analysis and under-
standing of Tmean(∙). Extensive experiments validate the effectiveness of our
approaches.
1 Introduction
As one special case of distributed machine learning, federated learning (FL) draws increasing re-
search attention recently. FL has become one promising approach to enable clients collaboratively
to learn a shared model with the decentralized and private data on each client node. Thus it is of
central importance to keep the node information secured in FL. Unfortunately, FL is very vulnera-
ble to software/hardware errors and adversarial attacks; especially Byzantine attack from distributed
systems has arose to be a key node attack sample for federated learning.
To ensure the FL model resistance to Byzantine attack, research focuses are made on how to intro-
duce aggregation rules into the gradient information iteration process, and how to ensure that this
aggregation rule can make the data robust. In particular, classical approaches to avoid the Byzan-
tine failures would employ the state machine replication strategy (Alistarh et al., 2018), which can
be roughly categorized into two ways in distributed machine learning: (1) the processes agree on a
sample of data based on which the clients update their local parameter vectors; (2) the clients agree
on how the parameter vector should be updated (Blanchard et al., 2017). The former ones demand
transmitting data samples to each individual node, resulting in high costs. The latter ways are not re-
liable neither, as we cannot detect whether clients are trustworthy or not, from the mixed Byzantine
vectors. The attacker can know any information about the process of FL, and can use any vectors to
initiate the attack during the node’s information transmission (Yin et al., 2018). More specifically,
the data between machines can be replaced by any value. This problem is not fully addressed in
previous works.
When facing the attacks from Byzantine nodes, the FL models have to rely on robust aggregation
rules to minimize the influence of the Byzantine attack on the data model (Bottou, 2010). For in-
stance, Krum(∙) is a strong aggregation rule designed to identify an honest node such that it can
effectively prevent Byzantine attacks in most cases (Blanchard et al., 2017). However, if the Byzan-
tine node attack is changed from the original single miner node attack to multiple server attacks,
Krum(∙) is not able to ensure the learning process robustness to noisy data. As another class of
aggregation rules, simple mean-based aggregation rules can maintain learning robustness ifit is not
attacked by Byzantine nodes. However, the Byzantine attack will make the update direction of node
gradient information largely deviate from the original function by using simple mean-based aggre-
gation rules. Some variants of mean-based aggregation rules, such as geometric median (Blanchard
1
Under review as a conference paper at ICLR 2021
(a) Federated learning gradient update iteration (b) SGD gradient update
Figure 1: (a) is the procession of FL gradient updating. It consists of four parts: master server, master
server distributed network, miner servers and miner network. (b) represents the schematic diagram
of the gradient update direction: blue dashed lines represent the estimated gradients of miner nodes;
red dashed lines represent the Byzantine update gradients under the attack of the Byzantine nodes;
black solid line represents the ideal gradient.
et al., 2017), marginal median (Alistarh et al., 2018), are the classical methods to solve the Byzantine
node attack problem. But they are not much more robust than simple mean-based aggregation rules
in the case of some large deviation Byzantine attacks. The main difficulty that prevents mean-based
aggregation rules from robust is the unstable distribution of data (Jin et al., 2020). We find that the
difficulty can be tackled, if the data is averaged by trimming a part of the data and then imported
into the aggregation rules. This motivates our work in this paper.
Most of FL approaches are built upon the Stochastic Gradient Descent (SGD) algorithm (Castro
et al., 1999) or its variants, and the statistical properties of SGD such as convergence are well
developed. Our approach also employs the SGD; and the typical iterative process of SGD algorithm
in distributed system is represented by Figure 1. First, a client, known as a miner node, estimates
the gradient of the node, makes an estimate of the deviation between the estimated information and
the ideal gradient information (El-Mhamdi et al., 2020), then passes this information to the server
node in the network, and finally update the gradient there. So the miner node network transmits the
information to the master server, and the master server then passes the gradient update information
through a series of aggregation operations. When the aggregation conditions are met, the server
transmits the information to the distribution network, and then transmitted to the miner nodes. This is
a cyclic process, such that the gradient information is continuously updated in the entire network (Li
et al., 2014).
In this paper, We mainly propose new aggregation rules, Tmean(∙), by trimming part of the data
before the average operation. We provide theoretical understandings of our aggregation rules, espe-
cially, why they are robust to the Byzantine attack. Through attack experiments and mathematical
proofs, we have concluded that appropriately trimming the original data and averaging can make the
model more robust from the decentralized data.
Specifically, in section 3 we introduce the federated learning Byzantine distributed system model,
briefly describes the working principle of SGD, and summarizes the update iteration rules based
on aggregation rules. Then, we present the concept of Byzantine resilience, and the conditions to
satisfy the aggregation rules of Byzantine resilience. We provide concept of trimmed mean and
the rigorous theoretical proof and understanding of Tmean(∙) based aggregation rules in section 4.
Then, we prove the convergence of the proposed federated learning aggregation rules in section 5.
In section 6, we conduct experiments by Gaussian attack, omniscient attack and multiple servers
attack. Under these attacks, Tmean(∙)-based FL aggregation rules can still maintain robustness.
These experiments thus validate the effectiveness of our approaches.
2
Under review as a conference paper at ICLR 2021
Contributions: (1) We present new aggregation rules, Tmean(∙), to the Byzantine resilient feder-
ated learning algorithm, and propose federated learning frameworks based on aggregation algorithm.
Our proposed approaches are shown to be robust to Byzantine attacks.
(2)	We provide rigorous theoretical proof and understanding of our approaches and aggregation
rules. To the best of our knowledge, these theorems and theoretical understandings are for the first
time contributed to the community. Critically, we make convergence certificates and prove that
Tmean(∙) can converge in the general convex optimization setting.
(3)	Empirically, we demonstrate that the effectiveness of our approaches can make the FL model
robust to Byzantine attack.
2	Related Work
Federated Learning. Federated learning has become a prominent distributed learning paradigm.
In (Jin et al., 2020), the authors proposed Stochastic-Sign SGD, a parameter estimation method
with convergence guarantees. It uses a gradient compressor based on random symbols to unify that
the gradient is updated in the framework. The FedAvg algorithm was first proposed by (Konecny
et al., 2016), and many scholars subsequently improved it based on this algorithm. (Karimireddy
et al., 2020) pointed out that FedAvg can be improved by adding one additional parameters control
variables to correct the client drift, and proved that FedAvg may be seriously affected by the gradient
difference of different clients, and may even be slower than SGD.
Byzantine attack in FL. To make the secure transmission of the master server node, the FL mod-
els have to deal with the Byzantine attack. (Yin et al., 2018) showed that certain aggregation rules
in the master server node can ensure the robustness of the data. (Blanchard et al., 2017) pointed out
that Krum(∙) can be used to make the robustness of data by computing the local sum of squared
Euclidean distance to the other candidates, and outputting the one with minimal sum. When the gra-
dient is updated to the saddle point, there is no guarantee that the SGD algorithm converges to the
global optimum. Some scholars have proposed ByzantinePGD (Yin et al., 2019), which can escape
saddle points and false local minimums, and can converge to an approximate true local minimum
with low iteration complexity. Later, in the strong anti-Byzantine model, some scholars carried out
poisoning attacks on Byzantine robust data, from the original data collection process to the subse-
quent information exchange process, and at the same time, these attacks were defended accordingly,
in (Zhao et al., 2021). The research on the Byzantine structure model of federated learning has been
expanded once again (Zhao et al., 2021).
Attack and Defence. In the framework of federated learning, miner nodes are usually attacked,
such as Byzantine node attacks, poisoning attacks (Zhao et al., 2021), gradient leakage (Wei et al.,
2020), etc. There exist various defense methods also, such as robust aggregation, secure aggregation,
encryption, etc. Our paper mainly studies Byzantine node attacks. Byzantine node attacks can
usually be summarized into two types: (1) To update gradient information of nodes, nodes are
replaced by Byzantine nodes, and normal worker nodes cannot make judgments so that the gradient
estimates deviate from the actual gradient update direction. (2) During the gradient process, the local
node suffering from the interference of the Byzantine node, cannot reach a consensus with the master
node, making the entire process unable to proceed normally (Lamport et al., 2019). Krum(∙) is a
popular aggregation rule to deal with these node attacks. We shall propose Tmean(∙) as alternative
aggregation rules.
3	Preliminary and Problem S etup
3.1	Federated learning setting
Federated learning was first proposed in (KOneCny et al., 2016), where the prevalent asynchronous
SGD is used to update a global model in a distributed fashion. A pioneering work in this field
proposed the currently most widely used algorithm, FedAvg (McMahan et al., 2017), which is also
the first synchronous algorithm dedicated to federated setting. Recent studies attempt to expand
federated learning with the aim of providing learning in more diverse and practical environments
3
Under review as a conference paper at ICLR 2021
Server:
Initialize x° J rand();
for t = 0, 1, . . . , T do
Broadcast x(t) to all the workers;
Wait until all the gradients v(t), v(t), ..., Vm) arrive;
Compute Gltt = Aggr(V(9,V?,..., Vmt));
Update the parameter x(t+1) J x(t) - γ(t) G(t) ;
end
Worker:
for t = 0, 1, . . . , T do
Receive x(t) from the server;
Compute and send the local randomized gradient v(t) = VF(χ(t),ξk) to the server;
end
Algorithm 1: Distributed synchronous SGD with robust aggregation server.
such as multi-task learning, generative models, continual learning, and data with noisy labels. How-
ever, these algorithms may obtain suboptimal performance when miners participating in FL have
non-independently and identical distributions (non-i.i.d.) (Zhao et al., 2018).
While the convergence of FedAvg on such settings was initially shown by experiments in (McMahan
et al., 2017), it does not guarantee performance as good as that in an i.i.d. setting. These algorithms
pointed out the issue have major limitations, such as privacy violation by partial global sharing of
local data (Zhao et al., 2018) or no indication of improvement over baseline algorithms such as
FedAvg (Hsieh et al., 2020). Our paper focuses on a general federated setting.
In order to better study the problem of aggregation in federated learning, we consider the following
optimization problem:
min F (x),	(1)
x∈Rd
where F(x) = Eξ〜D [f (x; ξ)] is a smooth convex function, ξ is sampled from some unknown distri-
bution D. Ideally, the problem (1) can be solved by the gradient descent method as
x(t+1) J x(t) - γ(t)VF(x(t)),	(2)
where γ(t) is the learning rate at t-th round. We assume that there exists at least one minimizer of
F(x), denoted by x*, that satisfies VF (x*) = 0.
The problem (1) is solved in a distributed manner with m miner nodes, and up to q of them may
be Byzantine nodes. The detailed algorithm of distributed synchronous SGD with aggregation rule
Aggr(∙) is shown in Algorithm 1. In each iteration, each miner samples n i.i.d. data points from the
distribution D, and computes the gradient of the local empirical loss. Using a certain aggregation
rule Aggr(∙), the server collects and aggregates the gradients sent by the miners, and estimate the
gradient through VF(x(t)) ≈ Aggr(V1, V2 , . . . , Vm). Without Byzantine failures, the k-th worker
calculates Vkt) 〜G(t), where G(t) = Vf (x(t),ξ). When there exist Byzantine faults, Vkt) can be
replaced by any arbitrary value V kt), and hence
VF(x(t)) ≈ Aggr(V1,V2,...,Vm).	(3)
3.2	Byzantine Resilience
In the following we introduce the concept of Byzantine resilience. Suppose that in a specific it-
eration, the correct vectors V1, V2 , . . . , Vm are i.i.d. samples drawn from the random variable
G = Vf (x; ξk), where E[G] = g is an unbiased estimator of the gradient based on the current
parameter x. Thus, E[Vk] = E[G] = g for k ∈ {1, 2, . . . , m}. We simplify the notations by omitting
the index of iteration t. The following Definition 1 defines the concept of ∆-Byzantine resilience.
For simplicity, we sometimes use Byzantine resilient in short when the concrete value of ∆ is either
clear from the context or not important.
4
Under review as a conference paper at ICLR 2021
Definition 1 (Byzantine Resilience). Suppose that 0 ≤ q ≤ m, and ∆ is a given positive number.
Let vι, v2, ..., Vm be i.i.d. random vectors in Rd, where Vk 〜 G with E[G] = g. Let Vι, V2,...,
Vm be attacked copies of Vk 's, such that at least m — q ofwhich are equal to the Corresponding v『s
while the rest are arbitrary vectors in Rd. An aggregation rule Aggr(∙) is said to be ∆-Byzantine
resilient if
E[∣∣Aggr(V1,V2, ... ,Vm) — gk2] ≤ ∆.
4	FL Framework with Robust Aggregation
Nowadays, the aggregation rules based on the FL framework are mainly based on Krum(∙) and
Mean(∙). They are usually used in the defense OfByzantine node attacks. However, the aggregation
rules based on Mean(∙) often cannot exhibit strong robustness, if it is affected by Byzantine nodes.
The attack makes the gradient estimation direction deviate too far from the actual gradient update
direction; and it cannot guarantee the robustness of the data. Thus we shall propose trimmed mean-
based aggregation rules to resolve this issue.
4.1	Krum
The Krum rule is a strong aggregation rule that attempts to identify an honest computing node, and
discards the data of other computing nodes. The data from the identified honest computing node is
used in the next round of algorithm. The selection strategy is to find one whose data is closest to that
on other computing nodes. In other words, it computes the local sum of squared Euclidean distances
to the other candidates, and outputs the one with minimal sum. A notable property of Krum(∙) is
shown in Lemma 1.
Lemma 1 ((Blanchard et al., 2017)). Let V1, . . . , Vm ∈ Rd be any i.i.d. random vectors such that
Vk 〜G with E[G] = g and E [∣∣G — g∣∣2] ≤ V. Let Vi, V2, ..., Vm be attacked copies of Vk 's, where
UP to q of them are Byzantine. If 2q + 2 < m, then Krum(∙) is ∆o -Byzantine resilient, where
4q(m—q—2)+4q2(m—q— 1)
∆o = 6m — 6q H-------------------------- V.
m — 2q — 2
4.2 Mean-based aggregation rules
An important class of aggregation rules is mean-based aggregation rules. Mathematically, an ag-
gregation rule Aggr(∙) is a mean-based one if Aggr(V1, V2,..., Vm) is guaranteed to be a convex
combination of V1, V2, . . ., Vm. Roughly speaking, a mean-based rule produces an averaged value
of the input data. For instance, the arithmetic mean Mean(V1, V2,..., Vm) = mm Pm=I Vk is the
simplest mean-based aggregation rule. However, simple mean-based aggregation rules are in gen-
eral not robust under Byzantine attacks, in the sense that they do not satisfy Byzantine resilience
conditions (Yin et al., 2018).
In the following, we show that by carefully handling the data, certain mean-based rules can satisfy
Byzantine resilience conditions. We first introduce the concept of b-trimmed means based on order
statistics as in Definition 2. This concept is closely related to the coordinate-wise trimmed mean
defined in (Yin et al., 2018).
Definition 2. Let u1, u2, . . . , um be scalars whose order statistics is
U(1) ≤ U(2) ≤ …≤ U(m).
For an integer b with 0 ≤ b < m/2, the b-trimmed mean of u1, u2, . . . , um is defined as
Tmeanb(u1, u2,
1 m-b
Um)= m-2b X "(kA
k=b+1
For d-vectors V1, V2, . . . , Vm ∈ Rd, the b-trimmed mean Tmeanb(V1, V2, . . . , Vm) ∈ Rd is defined
in the component-wise sense, i.e.,
d
Tmeanb(V1,V2,...,Vm) = Eei ∙ Tmeanb«1心，gG,…，®m, e"),
i=1
5
Under review as a conference paper at ICLR 2021
where ei is the i-th column of the d × d identity matrix.
Intuitively, aggregation rules using b-trimmed means have opportunities to discard abnormal values
from the input data. Lemma 2 shows that b-trimmed means are usually within a meaningful range
under mild assumptions.
Lemma 2. Let u1, u2, . . . , um be scalars whose order statistics is
u(1) ≤ u(2) ≤ …≤ u(m),
and Ui, U2, ..., Um be attacked copies of uι, u2, ..., Um with UP to q Byzantine elements. Ifthe
order statistics of Uks is
U(1) ≤ U(2) ≤ …≤ U(m),
then for q ≤ b < m/2, we have
U(k-b) ≤ U(k-q) ≤ U(k) ≤ U(k+q) ≤ U(k+b),	(b + 1 ≤ k ≤ m - b).
Proof. Let U®) be the Iargestnon-Byzantine element in {U(1),U(2)..., U(k)}. Since the number of
Byzantine elements is less than or equal to q, We have U(k) ≥ U(k0) ≥ U(k-q) ≥ U(k-b) if k ≥ b +1.
The proof of U(k) ≤ U(k+q) ≤ U(k+b) for k ≤ m 一 b is similar.	□
With the help of Lemma 2, we can prove that Tmeanb (∙) is Byzantine resilient when b is greater or
equal to the number of Byzantine nodes. The result is formulated as Theorem 1.
Theorem 1. Let vi, ..., Vm ∈ Rd be i.i.d. random vectors such that Vk 〜G with E[G] = g and
E[G 一 g]2 ≤ V. Let Vi, V2, ..., Vm be attacked copies of Vk 's, where up to q ofthem are Byzantine.
If q ≤ b < m/2, then Tmeanb(∙) is ∆ι-Byzantine resilient, where
mV
1	(m - b)2 .
Proof. See the Appendix.	□
The exact time complexity for evaluating Tmeanb(V1,V2,...,Vm,) has no closed-form expression in
terms of (m, b, d). However, by sorting in each dimension, we can evaluate Tmeanb(V1,V2,...,Vm)
using O(dm log m) operations. The cost is almost linear in practice, and is not much more expensive
than evaluating Mean(∙). Hence, Tmean(∙) improves the robustness with only negligible overhead
compared to Mean(∙).
5 Convergence Analysis
In this section, we conduct a convergence analysis for SGD using a Byzantine resilient aggregation
rule, such as Krum(∙) and Tmean(∙). Since a general convex optimization setting is used, we quote
some classic convex analysis theories (see, e.g., (Bubeck, 2014)), as shown in Lemma 3.
Lemma 3. Let F: Rd → R be a μ-strongly convex and L-smooth function. Thenfor X, y ∈ Rd and
α ∈ [0, 1], one has
(VF(x) - VF(y),x -y ≥ αμkx - y∣∣2 + 1yα∣∣VF(x) - VF(y)k2.
L
Proof. The μ-strong convexity and L-smoothness of F(x) implies
(VF(x) - VF(y),x - yi ≥ μ∣x -y∣2,
(VF(x) - VF(y),x - yi ≥ IkVF(x) - VF(y)∣2.
L
The conclusion is a simple convex combination of these inequalities.	□
Using Lemma 3 we establish the convergence of SGD as shown in Theorem 2. The expected error
bound consists of a linear convergence term similar to that for usual gradient descent methods, and a
term caused by randomness. The theorem also suggests theoretically the largest step size: γ = L-i.
6
Under review as a conference paper at ICLR 2021
Table 1: StructureofMLP.
Layer type	Flatten->fc1->relu1->fc2->relu2->fc3->softmax
Parameters	NUll->#oUtPUt128->null>#oUtPUt128->null->#oUtPUt10->null
Previous Layer	data->flatten->fc1->relu1->fc2->relu2->fc3
Table 2: Experiment Summary.
data set	#train	#test	#roUnds	Y	Batch size	EvalUation metric
MNIST	60,000	10,000	600	0.1	32	Top-1 accUracy
CIFAR10	50,000	10,000	3,000	5 × 10-4	128	Top-3 accUracy
Theorem 2. Suppose that the SGD method with (3) is adopted to solve problem (1), where
F: Rd → R is a μ-strongly convex and L-smooth function. Let v,), ..., Vm be local gradi-
ents at t-th iteration, and V(t, ..., Vm be the corresponding attacked copies. Ifthe aggregation
rule Aggr(∙) is ∆-Byzantine resilient, and the step size Y satisfies Y ≤ L-1 then
Elkx⑴-x*∣∣] ≤ ηt∣∣χ(O) - x*k + 1 + √1 - γμ√∆,
μ
where x* is the exact solution, and
η = 1- - γμ ≥
Proof. See the Appendix.
□
We remark that if we allow a general α ∈ [0, 1] in the proof of Theorem 2 and adjust the correspond-
ing step size as Y ≤ 2(1 - α)L-1, the decay ratio η is then bounded through
η = √1 - 2ɑγμ ≥
1— - 4α(1 — a) μ ≥
The optimal lower bound is achieved when α = 1/2 and Y = L-1
6	Experiments
In this section we verify the robustness and convergence of Tmean aggregation rules by experi-
ments. We use a multi-layer perceptron with two hidden layers to handwritten digits classification
on the MNIST data set, in m = 20 worker processes, we repeat each experiment for ten times and
take the average value. Table 1 shows the detailed network structures of the MLP used in our ex-
periments. Then, we conduct recognition on convolutional neural network in the Cifar10 dataset,
repeat each experiment for ten times and report the averaged value. In our experiments, we use
Mean(∙) without Byzantine attack as a reference. We use top-1 or top-3 accuracy on testing sets as
the evaluation metrics. Table 2 lists the details of the data set and the default hyper-parameters for
the corresponding model. Our experiments are implemented by Python 3.7.4 with packages Tensor-
Flow 2.4.1 and NumPy 1.19.2, on a physical node Intel i7-9700 CPU with 32 GB of memory and
two NVIDIA GeForce RTX 1080 Ti.
6.1	Experiment setting
In our experiment setting, 6 out of the 20 vectors are Byzantine vectors (i.e., m = 20, q = 6),
different values of b will affect different convergence performance. For the MNIST dataset, we take
b = 6 and b = 8 to train the experiment; and we take b = 8 on the Cifar10 dataset to conduct
recognition task.
6.2	Gaussian attack
In Gaussian attack experiment, We use the Gaussian random vector with zero mean and isotopic
covariance matrix with standard deviation 200 instead of some correct gradient vectors.
7
Under review as a conference paper at ICLR 2021
From Table 3 for the MNIST dataset, We find that compared with Mean(∙) without Byzantine,
Tmeanb(∙) works well, and is more robust when b = 8. However, without trimming the data,
Mean(∙) is not robust under Byzantine attack. The method of Krum(∙) is also robust, while it is
relatively weaker than Tmean(∙).
For the Cifar10 dataset, we set b = 8. From Table 4, the aggregation rule based on the Mean(∙) is
not as robust as other aggregation rules, but after trimming the Mean(∙), the output is more robust
than before.
Table 3: Accuracy of different aggregations under Gaussian attack in MLP.
Aggregation rule	100	Number of iterations									600
		150	200	250	300	350	400	450	500	550	
Mean without Byzantine	0.87	0.88	0.89	-0.89	0.90	0.90	0.91	0.91	0.92	0.92	0.93
Krum	0.71	0.82	0.85	0.86	0.87	0.87	0.88	0.89	0.88	0.89	0.89
Mean	0.63	0.62	0.62	0.63	0.64	0.64	0.64	0.65	0.65	0.65	0.66
Tmean(b = 6)	0.80	0.82	0.83	0.84	0.84	0.85	0.85	0.86	0.86	0.86	0.87
Tmean(b = 8)	0.82	0.85	0.88	0.89	0.89	0.91	0.89	0.90	0.91	0.91	0.92
Table 4: Accuracy of different aggregations under Gaussian attack in Cifar10.
Aggregation rule	500	Number of iterations				
		1000	1500	2000	2500	3000
Mean without Byzantine	0.63	0.71	0.73	0.75	0.77	0.79
Krum	0.62	0.70	0.72	0.75	0.76	0.78
Mean	0.51	0.55	0.56	0.57	0.57	0.58
Tmean(b = 8)	0.62	0.71	0.73	0.74	0.76	0.77
6.3	Omniscient attack
Assuming that the attackers (Byzantine nodes) know all the correct gradients information. For each
Byzantine gradient vector, all correct gradients of the gradient are replaced by their negative sum.
In other words, this attack attempts to make the parameter server go into the opposite direction.
For the MNIST dataset, we see from Table 5 that when the Byzantine node makes the gradient
update direction deviate from the maximum actual update direction, Mean(∙) cannot be robust. On
the contrary, Krum(∙) is more robust, while Tmean(∙) shows only weak robustness. Because trim
is a part of the data intercepted from the mean value for aggregation, it removes the head and tail
parts of the entire data set; such that it does not decrease much compared to the overall set mean
accuracy when it is attacked by Byzantine.
For the Cifar10 dataset, Table 6 shows that Krum(∙) is more robust than others. Mean-based ag-
gregation behaved badly in data robustness under omniscient attack. If we trim Mean(∙), it can be
clearly seen that, the output becomes more robust. However, under this attack, Tmean(∙) is not as
robust as Krum(∙).
Table 5: Accuracy of different aggregations under omniscient attack in MLP.
Aggregation rule	100	Number of iterations									600
		150	200	250	300	350	400	450	500	550	
Mean without Byzantine	0.82	0.83	0.84	^^084	0.85	0.85	0.86	0.86	0.87	0.87	0.88
Krum	0.79	0.81	0.82	0.83	0.83	0.84	0.85	0.85	0.86	0.87	0.87
Mean	0.26	0.26	0.27	0.27	0.28	0.29	0.29	0.30	0.31	0.31	0.32
Tmean(b = 6)	0.23	0.28	0.32	0.38	0.47	0.55	0.61	0.66	0.70	0.73	0.75
Tmean(b = 8)	0.50	0.52	0.57	0.61	0.64	0.67	0.70	0.72	0.74	0.77	0.79
Table 6: Accuracy of different aggregations under omniscient attack in Cifar10.
Aggregation rule	500	Number of iterations				
		1000	1500	2000	2500	3000
Mean without Byzantine	0.62	0.71	0.74	0.76	0.78	0.79
Krum	0.64	0.72	0.73	0.74	0.76	0.78
Mean	0.31	0.31	0.32	0.32	0.32	0.33
Tmean(b = 8)	0.42	0.43	0.45	0.45	0.46	0.48
8
Under review as a conference paper at ICLR 2021
6.4	General attack with multiple servers
We evaluate the robust aggregation rules under a more general and realistic type of attack. It is very
popular to partition the parameters into disjoint subsets, and use multiple server nodes to store and
aggregate them. We assume that the parameters are evenly partitioned and assigned to the server
nodes. The attacker picks one server, and manipulates any floating number by multiplying -1020
with probability of 0.05%. Because the attacker randomly manipulates the values, with the goal that
in some iterations the assumptions/prerequisites of the robust aggregation rules are broken, which
crashes the training. Such an attack requires less global information, and can be concentrated on
one single server, which makes it more realistic and easier to implement.
For the MNIST dataset, in Table 7 we evaluate the performance of all robust aggregation rules
under multiple serves attack. The number of servers is 20. For Krum(∙) and Mean(∙), We set the
estimated Byzantine number q = 6. We find that Krum(∙) and not passed. The intercepted Mean(∙)
aggregation rules cannot be robust. In this case, they cannot converge to the global optimum. On
the other hand, for Tmean(∙), different values of b will affect different convergence performance.
And if We take b = 6, it can also behave robustly.
For the Cifar10 dataset in Table 8, we can find the advantage of the Tmean(∙). Neither Krum(∙)
nor Mean(∙) is robust, while Tmean(∙) can make the learning robust.
Table 7: Accuracy of different aggregations under multiple server attack in MLP.
Aggregation rule	100	Number of iterations									600
		150	200	250	300	350	400	450	500	550	
Mean without Byzantine	0.83	0.83	0.84	-0.84	0.85	0.85	0.86	0.86	0.87	0.87	0.87
Krum	0.28	0.20	0.10	0.10	0.10	0.10	0.10	0.10	0.10	0.10	0.10
Mean	0.10	0.10	0.10	0.10	0.10	0.10	0.10	0.10	0.10	0.10	0.10
Tmean(b = 6)	0.80	0.81	0.81	0.82	0.82	0.83	0.83	0.84	0.84	0.84	0.85
Tmean(b = 8)	0.81	0.82	0.82	0.83	0.83	0.83	0.84	0.84	0.85	0.86	0.86
Table 8: Accuracy of different aggregations under multiple server attack in Cifar10.
Aggregation rule	500	Number of iterations				
		1000	1500	2000	2500	3000
Mean without Byzantine	0.65	0.70	0.73	0.75	0.77	0.78
Krum	0.30	0.30	0.30	0.30	0.30	0.30
Mean	0.30	0.30	0.30	0.30	0.30	0.30
Tmean(b = 8)	0.64	0.69	0.73	0.74	0.76	0.77
6.5	Experiment conclusion
From the above experiments, we find that it is difficult for Mean(∙) to ensure the robustness of the
data when it is attacked by any Byzantine node. When multiple server nodes are used at the same
time, Krum(∙) also becomes vulnerable. However, after the data is properly trimmed, Tmean(∙)
can maintain data robustness. Under omniscient attacks, Tmean(∙) is not as robust as Krum(∙).But
Tmean(∙) still improves a lot compared to Mean(∙) in this case. The data robustness of Tmean(∙)
is also related to the value of b. We find that when the value of b is closer to m/2, it can achieve
stronger robustness.
7	Conclusions
We analyzed the aggregation rules of Tmean(∙). We demonstrated that the effectiveness of our
approaches can make the FL model robust to Byzantine attacks. We used three different Byzan-
tine node attacks to show that the original data set can be more robustly handled after partial data
trimming and averaging operations.
This work focuses on Tmean(∙). In future work we plan to refine the trimming range of Tmean(∙)
and prove its convergence in a non-convex environment. At the same time, we will add momentum
on the basis of Tmean(∙), or add some constraints to strengthen its robustness.
9
Under review as a conference paper at ICLR 2021
References
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. arXiv preprint
arXiv:1803.08917, 2018.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Byzantine-tolerant
machine learning. In Conference and Workshop on Neural Information Processing Systems, pp.
118-128,2017.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT, pp. 177-186. Springer, 2010.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R in
Machine Learning, 2014.
Miguel Castro, Barbara Liskov, et al. Practical Byzantine fault tolerance. In OSDI, volume 99, pp.
173-186, 1999.
El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Le Nguyen Hoang, and SebaStien
Rouault. Genuinely distributed Byzantine machine learning. In Proceedings of the 39th Sym-
posium on Principles of Distributed Computing, pp. 355-364, 2020.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire of
decentralized machine learning. In International Conference on Machine Learning, pp. 4387-
4398. PMLR, 2020.
Richeng Jin, Yufan Huang, Xiaofan He, Tianfu Wu, and Huaiyu Dai. Stochastic-sign SGD for
federated learning with theoretical guarantees. arXiv preprint arXiv:2002.10940, 2020.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Jakub Konecny, H. Brendan McMahan, Felix X. Yu, Peter Richtarik, Ananda Theertha Suresh,
and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Leslie Lamport, Robert Shostak, and Marshall Pease. The Byzantine generals problem. In Concur-
rency: the Works of Leslie Lamport, pp. 203-226, 2019.
Mu Li, David G. Andersen, Alexander J. Smola, and Kai Yu. Communication efficient distributed
machine learning with the parameter server. Advances in Neural Information Processing Systems,
27:19-27, 2014.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PMLR, 2017.
Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Mehmet Emre Gursoy, Stacey Truex, and
Yanzhao Wu. A framework for evaluating gradient leakage attacks in federated learning. arXiv
preprint arXiv:2004.10397, 2020.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650-5659. PMLR, 2018.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Defending against saddle point
attack in Byzantine-robust distributed learning. In International Conference on Machine Learn-
ing, pp. 7074-7084. PMLR, 2019.
Bo Zhao, Peng Sun, Liming Fang, Tao Wang, and Keyu Jiang. FedCom: A Byzantine-robust local
model aggregation rule using data commitment for federated learning. USENIX Security Sympo-
sium, 2021.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
10
Under review as a conference paper at ICLR 2021
Appendix
ProofofTheorem 1. We first assume that vk's, Vk's, and g are all scalars (i.e., d = 1), with variance
V = σ2. Let v(i)≤ v(2)≤ …≤ v(m)and V(i)≤ V(2)
statistics of vι, v2,..., Vm and Vι, V2, ..., Vm. Notice that
≤ …≤ V(m), respectively, be order
E
E
m-2b
X (V(k) - g)2
k=1
E
(k) - g)2
----1----I
(m — 2b)2
2
mσ2
(m — 2b)2 .
m
X(Vi - g)2
k=1
E
Similarly, we have
E [(m⅛
m
V(k)
k=2b+1
-g
2
mσ2
(m - 2b)2 .
It follows from Lemma 2 that
1	m-2b
-----VC ∑2 V(k) — g ≤ Tmeanb(VI,V2,... ,Vm) — g
m — 2b
k=1
1
m — 2b
m
V(k) — g.
k=2b+1
Therefore
E [(Tmeanb(V1, V2,..., Vm) — g)2]
≤
≤
1 m-2b	2
max{e[(m-ib XV(k)- g)
,E
m — 2b
m
X	V(k) — g
k=2b+1
1
2
≤
2
mσ
(m — 2b)2 .
Now We consider the general case—Vk's, Vk's, and g are d-vectors, with variance V = Pid=ι σ2,
where
EhG — g, eii2] ≤ σi2.
Using the result for the scalar case, we have
d
E[hTmeanb(V1,V2,.. .,Vm) — g,eii2] ≤ X
i=1
2
mσi
(m — 2b)2
Thus we conclude that
d	mσ2
E[kτmeanb(V1,V2,...,Vm)—gk2] ≤ X (m-2 b)2
mV
(m — 2b)2
□
Proofof Theorem 2. Let g(t) = Aggr(V(t),..., Vm)). Then
kx(t+1) — x*k = Ilx⑶—Yg⑴—x*k ≤ Ilx⑴—x* — γVF(x(t))k + γkVF(x(t)) — g(t)||. (4)
It follows from Lemma 3 that
〈VF(x⑴),x⑴—x*)≥ μkx㈤—x*k2 + ɪ∣∣VF(x(t))k2.
2	2L
Then we obtain
Ix(t) — x* — γVF(x(t))I2
11
Under review as a conference paper at ICLR 2021
=∣∣x(t) - x*∣∣2 + γ2∣∣VF(x(t))k2 - 2γ(x(e) - x*, VF(X㈤))
≤ ||x(t) - x*∣2 + γ2∣VF(X㈤)∣2 - 2γ(α〃∣x⑴一x*∣2 + 1-α∣∣VF(X⑴)∣∣2)
(1 - 2αγμ)∣x⑴-x*∣2 + γ(γ -
2lf^ )∣VF (X⑴)12.
Taking α = 1/2 yields
||x(t) — x* — YVF(x(t))∣ ≤ pl — γμ ||x(t) — x* ∣ = η∣x(t') — x"∣.
The estimate (4) simplifies to
||x(t+1) - x*∣∣≤ η∣∣x1t) - x*∣∣ + YIlVF(X(t)) - g(t)||.
By taking the expectation on both sides, we obtain
E[∣∣xt+1 - x*∣∣] ≤ η E[∣x㈤-x*∣] + Y E[∣VF(X(t)) - g(t)∣]
≤ η E[∣x㈤-x*∣] + Y,E[∣VF(X㈤)-g(t)|2]
≤ η E [|x(t) — x*∣] + γ√∆ .
It can then be easily verified that
t-1
E[∣x㈤一x*∣] ≤ ηt∣x⑼-x*∣ + γ√∆ Xηi
i=0
∞
≤ ηt∣x(0) — x*∣∣ + γ√∆ X^ ηi
i=0
=ηt∣x(0)-x*∣ + 1 + √1-γ” √∆.
μ
Finally, the decay ratio η can be bounded through
η = √1 - γμ ≥
□
12