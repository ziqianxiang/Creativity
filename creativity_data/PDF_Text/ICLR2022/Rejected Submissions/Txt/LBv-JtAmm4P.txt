Under review as a conference paper at ICLR 2022
Is Heterophily A Real Nightmare For Graph
Neural Networks on Performing Node Classi-
fication?
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using
the graph structures based on the relational inductive bias (homophily assumption).
Though GNNs are believed to outperform NNs in real-world tasks, performance
advantages of GNNs over graph-agnostic NNs seem not generally satisfactory.
Heterophily has been considered as a main cause and numerous works have been
put forward to address it. In this paper, we first show that not all cases of het-
erophily are harmful1 for GNNs with aggregation operation. Then, we propose
new metrics based on a similarity matrix which considers the influence of both
graph structure and input features on GNNs. The metrics demonstrate advantages
over the commonly used homophily metrics by tests on synthetic graphs. From the
metrics and the observations, we find some cases of harmful heterophily can be
addressed by diversification operation. With this fact and knowledge of filterbanks,
we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit
aggregation, diversification and identity channels in each GNN layer to address
harmful heterophily. We validate the ACM-augmented baselines with 10 real-
world node classification tasks. They consistently achieve significant performance
gain and exceed the state-of-the-art GNNs on most of the tasks without incurring
significant computational burden.
1	Introduction
Deep Neural Networks (NNs) (LeCun et al., 2015) have revolutionized many machine learning
areas, including image recognition (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013)
and natural language processing (Bahdanau et al., 2014), etc.One major strength is their capacity
and effectiveness of learning latent representation from Euclidean data. Recently, the focus has
been put on its applications on non-Euclidean data (Bronstein et al., 2016), e.g., relational data or
graphs. Combining graph signal processing and convolutional neural networks (LeCun et al., 1998),
numerous Graph Neural Networks (GNNs) (Scarselli et al., 2008; Defferrard et al., 2016; Hamilton
et al., 2017; Velickovic et al., 2017; Kipf & Welling, 2016; Luan et al., 2019) have been proposed
which empirically outperform traditional neural networks on graph-based machine learning tasks,
e.g., node classification, graph classification, link prediction and graph generation, etc.GNNs are
built on the homophily assumption (McPherson et al., 2001), i.e., connected nodes tend to share
similar attributes with each other (Hamilton, 2020), which offers additional information besides node
features. Such relational inductive bias (Battaglia et al., 2018) is believed to be a key factor leading
to GNNs’ superior performance over NNs’ in many tasks.
Nevertheless, growing evidence shows that GNNs do not always gain advantages over traditional
NNs when dealing with relational data. In some cases, even simple Multi-Layer Perceptrons (MLPs)
can outperform GNNs by a large margin (Zhu et al., 2020b; Liu et al., 2020; Luan et al., 2020b; Chien
et al., 2021). An important reason for the performance degradation is believed to be the heterophily
problem, i.e., connected nodes tend to have different labels which makes the homophily assumption
fail. Heterophily challenge has received lots of attention recently and there are increasing number of
models being put forward to address this problem (Zhu et al., 2020b; Liu et al., 2020; Chien et al.,
2021; Zhu et al., 2020a; Yan et al., 2021).
1In general, harmful heterophily means the heterophilous structure that will make a graph-aware model
underperform its corresponding graph-agnostic model.
1
Under review as a conference paper at ICLR 2022
Contributions In this paper, we first demonstrate that not all heterophilous graphs are harmful for
aggregation-based GNNs and the existing metrics of homophily are insufficient to decide whether
the aggregation operation will make nodes less distinguishable or not. By constructing a similarity
matrix from backpropagation analysis, we derive new homophily metrics to depict how much GNNs
are influenced by the graph structure and node features. We show the advantage of our metrics
over the existing metrics by comparing the ability of characterizing the performance of two baseline
GNNs on synthetic graphs of different levels of homophily. According to the similarity matrix, we
observe that diversification operation is able to address some harmful heterophily cases, and based
on which we propose Adaptive Channel Mixing (ACM) GNN framework. The experiments on the
synthetic datasets, ablation studies and real-world datasets consistently show that the baseline GNNs
augmented by ACM framework are able to obtain significant performance boost on node classification
tasks on heterophilous graphs.
The rest of this paper is mainly organized as follows: In section 2, we introduce the notation and
the background knowledge. In section 3, we conduct node-wise analysis on heterophily, derive new
homophily metrics based on a similarity matrix and conduct experiments to show their advantages
over the existing homophily metrics. In section 4, we demonstrate the capability of diversification
operation on addressing some cases of harmful heterophily and propose the ACM-GNN framework
to adaptively utilize the information from different filterbank channels for each node to address
heterophily problem. In section 5, we discuss the related works and clarify the differences with our
method. In section 6, we provide empirical evaluations on ACM framework, including ablation study
and tests on real-world node classification tasks.
2	Preliminaries
We will introduce the related notation and background knowledge in this section. We use bold fonts
for vectors (e.g., v). Suppose we have an undirected connected graph G = (V, E, A), where V is the
node set with |V| = N; E is the edge set without self-loop; A ∈ RN×N is the symmetric adjacency
matrix with Ai,j = 1 iff eij ∈ E, otherwise Ai,j = 0. We use D to denote the diagonal degree
matrix of G, i.e., Di,i = di = Pj Ai,j and use Ni to denote the neighborhood set of node i, i.e.,
Ni = {j : eij ∈ E}. A graph signal is a vector x ∈ RN defined on V, where xi is defined on the
node i. We also have a feature matrix X ∈ RN×F, whose columns are graph signals and whose i-th
row Xi,: is a feature vector of node i. We use Z ∈ RN ×C to denote the label encoding matrix, whose
i-th row Zi,: is the one-hot encoding of the label of node i.
2.1	Graph Laplacian, Affinity Matrix and Their Variants
The (combinatorial) graph Laplacian is defined as L = D - A, which is Symmetric Positive Semi-
Definite (SPSD) (Chung & Graham, 1997). Its eigendecomposition gives L = UΛUT, where the
columns ui of U ∈ RN ×N are orthonormal eigenvectors, namely the graph Fourier basis, Λ =
diag(λι,..., λN) with λι ≤ ∙∙∙ ≤》n , and these eigenvalues are also called frequencies. The graph
Fourier transform of the graph signal x is defined as xF = U-1x = UTx = [u1T x, . . . , uTN x]T,
where uiTx is the component of x in the direction of ui.
In additional to L, some variants are also commonly used, e.g., the symmetric normalized Laplacian
Lsym = D-1/2LD-1/2 = I - D-1/2AD-1/2 and the random walk normalized Laplacian Lrw =
D-1L = I - D-1A. The graph Laplacian and its variants can be considered as high-pass filters.
The affinity (transition) matrices can be derived from the Laplacians, e.g., Arw = I - Lrw = D-1A,
Asym = I - Lsym = D-1/2AD-1/2 and are considered to be low-pass (LP) filters (Maehara, 2019).
Their eigenvalues satisfy λi(Arw) = λi(Asym) = 1 - λi(Lsym) = 1 - λi(Lrw) ∈ (-1, 1]. Applying
the renormalization trick (Kipf & Welling, 2016) to affinity and Laplacian matrices respectively
leads to ASym = D-1/2 AD-1/2 and LSym = I - Asym, where A ≡ A + I and D ≡ D + I. The
renormalized affinity matrix essentially adds a self-loop to each node in the graph, and is widely used
in Graph Convolutional Network (GCN) (Kipf & Welling, 2016) as follows,
Y = Softmax(ASym ReLU(AsymXWo) W1)	(1)
where W0 ∈ RF ×F1 and W1 ∈ RF1×O are learnable parameter matrices. GCN can be trained by
minimizing the following cross entropy loss
L = -trace(Z T logY)
(2)
2
Under review as a conference paper at ICLR 2022
where log(∙) is a component-wise logarithm operation. The random walk renormalized matrix
Arw = D-1A, which shares the same eigenvalues as Asym, can also be applied in GCN. The
corresponding Laplacian is defined as Lrw = I - Arw. The matrix Arw is essentially a random walk
matrix and behaves as a mean aggregator that is applied in spatial-based GNNs (Hamilton et al.,
2017; Hamilton, 2020). To bridge the spectral and spatial methods, we use Arw in the paper.
2.2	Metrics of Homophily
The metrics of homophily are defined by considering different relations between node labels and
graph structures defined by adjacency matrix. There are three commonly used homophily metrics:
edge homophily (Abu-El-Haija et al., 2019; Zhu et al., 2020b), node homophily (Pei et al., 2020),
and class homophily (Lim et al., 2021) 2 defined as follows:
Hedge (G )
{euv | euv ∈ E , Zu,: = Zv,: }
|E|
Hnode (G )
1 X ∣{u | U ∈ Nv, Zu,: = Zv,: } I
M v∈V	dv
4 O) 1	7 I{v | Zv,k = 1}∣	7 Pv∈V I{u | Zv,k = 1,u ∈ Nv, Zu,： = ZvJI
HCIass(G) = 7~~h > hk----------------芯-------- ,hk = ----------------------------------i--------------
C T k=1 L	N	」+	^v∈{v∣Zv,k = 1} dv
(3)
where [a]+ = max(a, 0); hk is the class-wise homophily metric (Lim et al., 2021). They are all in the
range of [0, 1] and a value close to 1 corresponds to strong homophily while a value close to 0 indicates
strong heterophily. Hedge(G) measures the proportion of edges that connect two nodes in the same
class; Hnode(G) evaluates the average proportion of edge-label consistency of all nodes; Hclass(G)
tries to avoid the sensitivity to imbalanced class, which can cause Hedge(G) misleadingly large. The
above definitions are all based on the graph-label consistency and imply that the inconsistency will
cause negative effect to the performance of GNNs. With this in mind, we will show a counter example
to illustrate the insufficiency of the above metrics and propose new metrics in the following section.
3 Analysis of Heterophily
3.1	Motivation and Aggregation Homophily
Heterophily is believed to be harmful for
message-passing based GNNs (Zhu et al.,
2020b; Pei et al., 2020; Chien et al., 2021) be-
cause intuitively features of nodes in different
classes will be falsely mixed and this will lead
nodes indistinguishable (Zhu et al., 2020b). Nev-
ertheless, it is not always the case, e.g., the bi-
partite graph shown in Figure 1 is highly het-
erophilous according to the existing homophily
metrics in equation 3, but after mean aggrega-
tion (operated by mean aggregator), the nodes
in classes 1 and 2 only exchange colors and are
still distinguishable. Authors in (Chien et al.,
■ Class 1
C) Class 2
Metrics:
Hedge ( G)=O
Hnode(G) = 0
HClaSS (G)=。
¾gg( G)= 1
¾g(G )=1
Figure 1: Example of harmless heterophily
2021) also point out the insufficiency of Hnode by examples to show that different graph typologies
with the same Hnode(G) can carry different label information.
To analyze to what extent the graph structure can affect the output of a GNN, we first simplify GCN
by removing its nonlinearity as (WU et al., 2019). Let A ∈ RN×N denote a general aggregation
operator. Then, equation 1 can be simplified as,
Y = SOftmax(AXW) = softmax(Y )
(4)
2The aUthors in (Lim et al., 2021) did not name this homophily metric. We name it class homophily based on
its definition.
3
Under review as a conference paper at ICLR 2022
After each gradient decent step ∆W = Y dW, where Y is the learning rate, the update of Y0 will be
(see Appendix C for derivation),
△Y0 = AXδw = YAX焉 X AX 称=AXXTAT(Z - Y) = S(A, X)(z - Y) (5)
where S(A, X) ≡ AX(AX)T is a post-aggregation node similarity matrix, Z - Y is the prediction
error matrix. The update direction of node i is essentially a weighted sum of the prediction error, i.e.,
△(Y0)i,: = Pj∈v [s(A,x)i，JZ - Y)j,:.
Instead of measuring the graph-label consistency, we study the effect of heterophily by considering
the post-aggregation node similarity. To this end, we first define the aggregation similarity score as
follows.
Definition 1. Aggregation similarity score
nv
Sagg (S(A,X))=
Meanu({S(A,X)v,u|Zu,： = Zv,：}) ≥ Meanu({S(A,X)v,u∣Zu,: = ZvJ)}∣
|V|
where Meanu ({∙}) takes the average over U ofa given multiset ofvalues or variables.
(6)
Sagg(S(A, X)) measures the proportion of nodes v ∈ V that will put relatively larger similar-
ity weights on nodes in the same class than in other classes after aggregation. It is easy to see
that Sagg(S(A, X)) ∈ [0, 1]. But in practice, we observe that in most datasets, we will have
Sagg(S(A, X)) ≥ 0.5. Based on this observation, we rescale equation 6 to the following modified
aggregation similarity for practical usage,
SMg 卜(A, X)) = [2Sagg 卜(A, X)) - ι[	⑺
In order to measure the consistency between labels and graph structures without considering node
features and make a fair comparison with the existing homophily metrics in equation 3, we define the
graph (G) aggregation (A) homophily and its modified version as
Hagg(G) = Sagg 卜(A,Z)) , HMg(G) = SMg 卜(A, Z))	(8)
In practice, we will only check Hagg(G) when HaMg(G) = 0. As Figure 1 shows, when A = A1∙w,
Hagg(G) = HaMgg(G) = 1. Thus, this new metric reflects the fact that nodes in classes 1 and 2 are still
highly distinguishable after aggregation, while other metrics mentioned before fail to capture such
information and misleadingly give value 0. This shows the advantage of Hagg(G) and HaMgg(G) by
additionally exploiting information from aggregation operator A and the similarity matrix.
To comprehensively compare HaMgg (G) with the metrics in equation 3 on how they can reveal the
influence of graph structure on GNN performance, we generate synthetic graphs with different
homophily levels and evaluate SGC (Wu et al., 2019) and GCN (Kipf & Welling, 2016) on them in
the next subsection.
3.2	Empirical Evaluation and Comparison on Synthetic Graphs
In this subsection, we conduct experiments on synthetic graphs with different HeMdge(G) to empirically
verify the effectiveness of HaMgg(G) and compare it with the existing metrics.
Data Generation & Experimental Setup We first generate 280 graphs in total with 28 edge
homophily levels varied from 0.005 to 0.95, each corresponding to 10 graphs. For every generated
graph, we have 5 classes with 400 nodes in each class. For nodes in each class, we randomly generate
800 intra-class edges and [ H：：(G) - 800] inter-class edges. The features of nodes in each class are
sampled from node features in the corresponding class of 6 base datasets (Cora, CiteSeer, PubMed,
Chameleon, Squirrel, Film). Nodes are randomly splitted into 60%/20%/20% for train/validation/test.
We train 1-hop SGC (sgc-1) (Wu et al., 2019) and GCN (Kipf & Welling, 2016) on the synthetic
graphs 3. For each value of Hedge(G), we take the average test accuracy and standard deviation of
3See Appendix A.1 for hyperparameter searching range and Appendix B for more detailed description of the
data generation process
4
Under review as a conference paper at ICLR 2022
runs over 10 generated graphs. For each generated graph, we also calculate its Hnode(G), Hclass(G)
and HaMgg(G). Model performance with respect to different homophily values are shown in Figure 2.
>1.0
I 0.8
y 0.6
告04
∣-0.2
0.0 0.2 0.4 0.6 0.8
Edge Homophily
sync-cora-gcn
-sync-cora-sgcl
sync-citeseer-gcn
-sync-citeseer-sgcl
SynC-PUbmeCLgCn
-SynC-PUbmeeLSgCl
Node Homophily Class Homophily Modified Agg Homophily
(a) Hedge(G)	(b) Hnode(G)	(c)Hclass(G)	(d)HaMgg(G)
Figure 2: Comparison of baseline performance under different homophily metrics.
sync-chameleon-gcn
sync-chameleon-sgcl
sync-squirrel-gcn
sy nc-squ irrel-sgc 1
sync-film-gcn
sync-film-sgcl
Comparison of Homophily Metrics The performance of SGC-1 and GCN are expected to be
monotonically increasing with a proper and informative homophily metric. However, Figure 2(a)(b)(c)
show that the performance curves under Hedge(G), Hnode(G) and Hclass (G) are U -shaped 4, while
Figure 2(d) reveals a nearly monotonic curve with a little perturbation around 1. This indicates
that HaMgg (G) can describe how the graph structure affects the performance of SGC-1 and GCN
more appropriately and adequately than the existing metrics. (See more discussion on aggregation
homophily and theoretical results for regular graphs in Appendix B.)
4 Adaptive Channel Mixing (ACM) Framework
High-frequency graph sig-
nal, which can be extracted
by high-pass filter, is em-
pirically shown to be useful
for addressing heterophily
problem (Luan et al., 2020b;
Chien et al., 2021; Bo et al.,
2021). In this section, based
on the similarity matrix pro-
posed in equation 5, we
will figure out how diversifi-
cation operation i.e., high-
pass (HP) filter, is poten-
tially capable to address
.Class 1 (ɔ Class 2
X = I
1： [1,0]
2： [1,0]
3： [1,0]
4: [0,1]
5： [0,1]
6： [0,1]
7: [0,1]
ʌ f ^
AXXτA
/50 .50 .50 .50 .50 .50 .50∖
.50 1.0 .33 .20 .25 .20 .25
,50 .33 .56 .60 .58 .60 .58
.50 .20 .60
.50 .25 .58 .65
50 .25 .60
.50 .25 .58 .65 .63 .65 .63
Large positive weighs in
inter-class block for node 1,3.
(J - A)XXτ(∖ - Λ)
/ 0 .0	-.20 -.25 -.20 -.25∖
,00 .00 .00 .00	.00 .00 .00 ∖
,67 .00 .89 -.27 -.33 -.27 -.33
-	.20 .00 -.27
-	.25 .00 -.33
-	.20 .00 -.27
V.25 .00 -.33 10 .13 .10 .13/
Positive weights in intra-CIaSS blocks,
Non-negative weights in cross-class
blocks.
Figure 3: Example of how diversification operation addresses harmful
heterophily
some cases of harmful heterophily. From the node-wise analysis, we argue that aggregation (LP
filter) and diversification (HP filter) should be combined together as a filterbank (Ekambaram, 2014)
for feature extraction and different nodes may have different needs for the information processed by
different filters. Based on the above argument, we generalize filterbank method and propose Adaptive
Channel Mixing framework in subsection 4.2 to address heterophily challenge.
4.1 How Diversification Operation Helps with Harmful Heterophily
We first consider the example shown in Figure 3. From S(A, X), we can see that nodes 1,3 assign
relatively large positive weights to nodes in class 2 after aggregation, which will make node 1,3
hard to be distinguished from nodes in class 2. Despite the fact, we can still distinguish nodes
1,3 and 4,5,6,7 by considering their neighborhood differences: nodes 1,3 are different from most
of their neighbors while nodes 4,5,6,7 are similar to most of their neighbors. This indicates, in
some cases, although some nodes become similar after aggregation, they are still distinguishable via
their surrounding dissimilarities. This leads Us to use diversification operation, i.e., HP filter I - A
(Ekambaram, 2014) to extract the information of neighborhood differences and address harmful
heterophily. As S(I - A, X) in Figure 3 shows, nodes 1,3 will assign negative weights to nodes
4A similar J-shaped curve for Hedge(G) is found in (Zhu et al., 2020b), though using different data generation
processes. The authors do not mention the insufficiency of edge homophily.
5
Under review as a conference paper at ICLR 2022
4,5,6,7 after diversification operation, i.e., nodes 1,3 treat nodes 4,5,6,7 as negative samples and will
move away from them during backpropagation. This example reveals that there exist some cases that
diversification operation is helpful while aggregation operation is not. Based on this observation, we
first define diversification distinguishability of a node and graph diversification distinguishability
value as follows to measure the proportion of nodes that diversification operation is potentially helpful
for.
Definition 2. Diversification Distinguishability (DD) based on S(I 一 A, X).
Given S(I 一 A, X), a node v is diversification distinguishable if the following two conditions are
satisfied at the same time,
1.	Meanu ({S(I - A,X )v,u∣u ∈V∧ Zu,: = Zv,}) ≥ 0；
2.	Meanu ({S(I - A, X)v,ulu ∈ V ∧ Zu,: = Zv,:}) ≤ 0
Then, graph diversification distinguishability value is defined as
DDAX(G)=木 ∣{v∣v ∈ V ∧ V is diversification distinguishable}
(9)
(10)
We can see that DDAX (G) ∈ [°，". Based on definition 2, the effectiveness of diversification
operation on addressing heterophily can be proved for binary classification problems under certain
conditions, leading us to:
Theorem 1. (See Appendix E for proof). For a binary classification problem, i.e., C = 2, sup-
pose X = Z, A = Arw. Then for any I 一 Arw, all nodes are diversification distinguishable and
DDa,z(G) = 1.
Theorem 1 theoretically demonstrates the importance of diversification operation on handling het-
erophily. Combined with aggregation operation, we can get a filterbank which uses both LP and HP
filters to distinctively extract the low- and high-frequency information from graph signals. We will
introduce filterbank in the next subsection.
4.2 Filterbank and Adaptive Channel Mixing (ACM) Framework
Filterbank For the graph signal x defined on G, a 2-channel linear (analysis) filterbank (Ekam-
baram, 2014) 5 includes a pair of low-pass (LP) and high-pass (HP) filters HLP , HHP , where HLP and
HHP retain the low-frequency and high-frequency content of x, respectively.
Most existing GNNs are under uni-channel filtering architecture (Kipf & Welling, 2016; Velickovic
et al., 2017; Hamilton et al., 2017) with either HLP or HHP channel that only partially preserves the
input information. Unlike the uni-channel architecture, filterbanks with HLP + HHP = I will not
lose any information of the input signal, i.e., perfect reconstruction property (Ekambaram, 2014).
Generally, the Laplacian matrices (Lsym, Lrw, Lsym, Lrw) can be regarded as HP filters (Ekambaram,
2014) and affinity matrices (Asym, Arw, Asym, Arw) can be treated as LP filters (Maehara, 2019;
Hamilton, 2020). Moreover, it can be interpreted that MLPs uses the identity filterbank with HLP = I
and HHP = 0 that satisfy HLP + HHP = I + 0 = I.
From Figure 3, we also observe that different nodes may have different needs for the information
from different channels, e.g., nodes 1,3 demand information from HP channel while node 2 only
needs information from LP channel. To adaptively leverage the LP, HP and identity channels in
GNNs, we propose the Adaptive Channel Mixing (ACM) architecture in the following part.
Adaptive Channel Mixing (ACM) Framework ACM framework can be applied to lots of baseline
GNNs and in this part, we use GCN as an example to introduce ACM framework in matrix form. We
use HLP and HHP to represent general LP and HP filters. The ACM framework includes 3 steps as
5In graph signal processing, an additional synthesis filter (Ekambaram, 2014) is required to form the 2-channel
filterbank. But synthesis filter is not needed in our framework, so we do not introduce it in our paper.
6
Under review as a conference paper at ICLR 2022
follows,
Step 1. Feature Extraction for Each Channel:
Option 1:	HL	=	ReLU (HLPHl-1 WLT),	HH =	ReLU (HHPHl-1Wj-1), H11	= ReLU (IHl-1Wl-1);
Option 2:	H1L	=	HLPReLU (Hl-1 WLT),	HH =	HHPReLU (Hl-1 Wj-1) ,Hl1	= I ReLU (Hl-1 W；-1);
H0 = X ∈ RN×F0, WLl-1, WHl-1, WIl-1 ∈ RFl-1×Fl;
Step 2. Row-wise Feature-based Weight Learning
OL = σ (hLWL), αlH = σ (hHWH) , αI = σ (hiWI) , WL-1, WH-1, WIT ∈ RFl×1
[αL, Oh, αI] = SoftmaX (([&L, Oh, OI] /T)WMix) ∈ RN×3, T ∈ R temperature, WMix ∈ R3×3;
Step 3. Node-wise Adaptive Channel Mixing:
Hl = diag(αlL)HLl + diag(αHl )HHl + diag(αlI)HIl
(11)
The framework with option 1 in step 1 is called ACM framework and with option 2 is named
ACMII framework. In step 1, ACM-GCN and ACMII-GCN implement distinct feature extraction
for 3 channels by a set of filterbank and 3 filtered components HLl , HHl , HIl are obtained. To
adaptively exploit information from each channel, ACM-GCN and ACMII-GCN first extract nonlinear
information from the filtered signals, then use WMl ix to learn which channel is important or not for
each node, leading to the row-wise weight vectors αlL, αlH, αlI ∈ RN×1 whose i-th elements are the
weights for the i-th node. These three vectors are then used as weights in defining the updated Hl in
step 3. See Appendix F for the performance comparison with basline models on synthetic datasets.
Complexity Number of learnable parameters in layer l of ACM-GCN and ACMII-GCN is
3Fl-1(Fl + 1) + 9, while it is Fl-1Fl in GCN. The computation of step 1-3 takes NFl(8 + 6Fl-1) +
2Fl(nnz(HLP) +nnz(HHP)) + 18N flops, while GCN layer takes 2NFl-1Fl + 2Fl(nnz(HLP)) flops,
where nnz(∙) is the number of non-zero elements. An ablation study and a detailed comparison on
running time is conducted in section 6.1.
Limitations of Diversification Operation Just like any other method, Diversification operation
may not work well in all harmful heterophily cases. For example, when we have more than 2 classes
and consider an imbalanced dataset where several small clusters with distinctive labels are densely
connected to a large cluster. In this case, the surrounding differences of nodes in small clusters are
similar, i.e., the neighborhood differences are mainly from their connection to the same large cluster,
and this possibly makes diversification operation fail to discriminate them. See a more detailed
demonstration and discussion in Appendix G.
5	Prior Work
We discuss relevant work of GNNs on addressing heterophily challenge in this part. Authors in
(Abu-El-Haija et al., 2019) acknowledge the difficulty of learning on graphs with weak homophily and
propose MixHop to extract features from multi-hop neighborhood to get more information. Authors
in (Hou et al., 2019) propose measurements based on feature smoothness and label smoothness
that are potentially helpful to guide GNNs on dealing with heterophilous graphs. Geom-GCN
(Pei et al., 2020) precomputes unsupervised node embeddings and uses graph structure defined by
geometric relationships in the embedding space to define the bi-level aggregation process to handle
heterophily. H2GCN (Zhu et al., 2020b) combines 3 key designs to address heterophily: (1) ego- and
neighbor-embedding separation; (2) higher-order neighborhoods; (3) combination of intermediate
representations. CPGNN (Zhu et al., 2020a) models label correlations by the compatibility matrix,
which is beneficial for heterophily settings, and propagates a prior belief estimation into GNNs by
the compatibility matrix. FBGNN (Luan et al., 2020b) first proposes to use filterbank to address
heterophily problem, but it does not fully explain the insights behind HP filters and does not contain
identity channel and node-wise channel mixing mechanism. FAGCN (Bo et al., 2021) learns edge-
level aggregation weights as GAT (Velickovic et al., 2017) but allows the weights to be negative which
enables the network to capture the high-frequency components in graph signals. GPRGNN (Chien
et al., 2021) uses learnable weights that can be both positive and negative for feature propagation,
it allows GPRGNN to adapt heterophily structure of graph and is able to handle both high- and
low-frequency parts of the graph signals. (See Appendix J for a more comprehensive comparison
between ACM-GNNs, ACMII-GNNs and FAGCN, GPRGNN.)
7
Under review as a conference paper at ICLR 2022
6	Experiments on Real-World Datasets
In this section, we evaluate ACM and ACMII framework on real-world datasets. We first conduct
ablation studies in subsection 6.1 to validate the effectiveness of different components. Then, we
compare with the state-of-the-arts (SOTA) models in subsection 6.2. The hyperparameter searching
range and computing resources for all experiments are attached in Appendix A.
6.1	Ablation Study & Efficiency
Ablation Study on Different Components in ACM-SGC and ACM-GCN (%)								
Baseline	I Model Components ∣	Cornell	Wisconsin	Texas	Film	Chameleon	Squirrel	Cora	CiteSeer	PubMed Rank
Models	ILP HP Identity Mixing ∣	Acc ± Std	Acc ± Std	Acc ± Std Acc ± Std Acc ± Std	Acc ± Std	Acc ± Std	Acc ± Std	ACC ± Std I
ACM-SGC-1 w/	X XX	X XXX XX	X XX	X	X	70.98 ± 8.39 70.38 ± 2.85 83.28 ± 5.81 91.88 ± 1.61 93.93 ± 3.6 95.25 ± 1.84 88.2 ± 4.39 93.5 ± 2.95 93.77 ± 1.91 93.25 ± 2.92	83.28 ±	5.43	25.26 ±	1.18	64.86	±	1.81	47.62	±	1.27 90.98 ±	2.46	36.76 ±	1.01	65.27	±	1.9	47.27	±	1.37 93.93 ±	2.54	38.38 ±	1.13	63.83	±	2.07	46.79	±	0.75 92.95 ±	2.94	37.19 ±	0.87	62.82	±	1.84	44.94	±	0.93 93.61 ±	1.55	39.33 ±	1.25	63.68	±	1.62	46.4	±	1.13	85.12 ± 1.64 79.66 ± 0.75 85.5 ± 0.76 86.8 ± 1.08 80.98 ± 1.68 87.21 ± 0.42 86.73 ± 1.28 80.57 ± 0.99 87.8 ± 0.58 85.22 ± 1.35 80.75 ± 1.68 88.11 ± 0.21 86.63 ± 1.13 80.96 ± 0.93 87.75 ± 0.88	12.89 10.44 9.44 11.00 10.00
ACM-GCN w/	X X X	X XXX X X	X XX	X	X	82.46 ± 3.11 75.5 ± 2.92 82.13 ± 2.59 86.62 ± 4.61 94.26 ± 2.23 96.13 ± 2.2 91.64 ± 2 95.37 ± 3.31 94.75 ± 2.62 96.75 ± 1.6	83.11 ± 3.2 35.51 ± 0.99	64.18	± 2.62	44.76	±	1.39 89.19 ± 3.04 38.06 ± 1.35	69.21	± 1.68	57.2 ±	1.01 94.1 ± 2.95 41.51 ± 0.99	67.44	± 2.14	53.97	±	1.39 95.25 ± 2.37 40.47 ± 1.49	68.93	± 2.04	54.78	±	1.27 95.08 ± 3.2 41.62 ± 1.15	69.04	± 1.74	58.02	±	1.86	87.78 ± 0.96 81.39 ± 1.23 88.9 ± 0.32 88.93 ± 1.55 81.96 ± 0.91 90.01 ± 0.8 88.95 ± 0.9 81.72 ± 1.22 90.88 ± 0.55 89.13 ± 1.77 81.96 ± 2.03 91.01 ± 0.7 88.95 ± 1.3 81.80 ± 1.26 90.69 ± 0.53	11.44 7.22 4.44 3.11 2.78
ACMII-GCN w/	X X	X XXX X X	X XX	X	X	82.46 ± 3.03 91.00 ± 1.75 94.26 ± 2.57 96.00 ± 2.15 91.48 ± 1.43 96.25 ± 2.09 95.9 ± 1.83 96.62 ± 2.44	90.33 ±	2.69	38.39 ±	0.75	67.59	±	2.14	53.67	± 1.71 94.26 ±	2.96	40.96 ±	1.2	66.35	±	1.76	50.78	± 2.07 93.77 ±	2.91	40.27 ±	1.07	66.52	±	2.65	52.9	± 1.64 95.25 ±	3.15	41.84 ±	1.15	68.38	±	1.36	54.53	± 2.09	89.13 ± 1.14 81.75 ± 0.85 89.87 ± 0.39 89.06 ± 1.07 81.86 ± 1.22 90.71 ± 0.67 88.83 ± 1.16 81.54 ± 0.95 90.6 ± 0.47 89.00 ± 0.72 81.79 ± 0.95 90.74 ± 0.5	7.44 4.67 6.67 2.78
Comparison of Average Running Time Per Epoch(ms)
	X				2.53	2.83	2.5	3.18	3.48	4.65	3.47	3.43	4.04
	X	X		X	4.01	4.57	4.24	4.55	4.76	5.09	5.39	4.69	4.75
ACM-SGC-1 w/	X		X	X	3.88	4.01	4.04	4.43	4.06	4.5	4.38	3.82	4.16
	X	X	X		3.31	3.49	3.18	3.7	3.53	4.83	3.92	3.87	4.24
	X	X	X	X	5.53	5.96	5.43	5.21	5.41	6.96	6	5.9	6.04
	X				3.67	3.74	3.59	4.86	4.96	6.41	4.24	4.18	5.08
	X	X		X	6.63	8.06	7.89	8.11	7.8	9.39	7.82	7.38	8.74
ACM-GCN w/	X		X	X	5.73	5.91	5.93	6.86	6.35	7.15	7.34	6.65	6.8
	X	X	X		5.16	5.25	5.2	5.93	5.64	8.02	5.73	5.65	6.16
	X	X	X	X	8.25	8.11	7.89	7.97	8.41	11.9	8.84	8.38	8.63
	X	X		X	6.62	7.35	7.39	7.62	7.33	9.69	7.49	7.58	7.97
	X		X	X	6.3	6.05	6.26	6.87	6.44	6.5	6.14	7.21	6.6
ACMII-GCN w/	X	X	X		5.24	5.27	5.46	5.72	5.65	7.87	5.48	5.65	6.33
	X	X	X	X	7.59	8.28	8.06	8.85	8	10	8.27	8.5	8.68
Table 1: Ablation study on 9 real-world datasets Pei et al. (2020). Cell with Xmeans the component
is applied to the baseline model. The best test results are highlighted.
We investigate the effectiveness and efficiency of adding HP, identity channels and the adaptive mixing
mechanism in the proposed framework by ablation study. Specifically, we apply the components
of ACM to SGC-1 (Wu et al., 2019) 6 and the components of ACM and ACMII to GCN (Kipf &
Welling, 2016) separately. We run 10 times on each dataset with the same 60%/20%/20% random
splits for train/validation/test used in (Chien et al., 2021) and report the average test accuracy as well
as the standard deviation. We also record the average running time per epoch (in milliseconds) to
compare the efficiency. We set the temperature T in equation 11 to be 3.
From the results we can see that on most datasets, the additional HP and identity channels are helpful,
even on strong homophily datasets, such as Cora, CiteSeer and PubMed. The adaptive mixing
mechanism also shows its advantage over the method that directly adds the three channels together.
This illustrates the necessity of learning to customize the channel usage adaptively for different nodes.
As for efficiency, we can see that the running time is approximately doubled under ACM and ACMII
framework than the original model.
6.2	Comparison with State-of-the-art Models
Datasets & Experimental Setup In this section, we implement SGC (Wu et al., 2019) with 1 hop
and 2 hops (SGC-1, SGC-2), GCNn(Chen et al., 2020), GCNII* (Chen et al., 2020), GCN (KiPf &
Welling, 2016) and snowball networks with 2 and 3 layers (snowball-2, snowball-3) and apply them
6We only test ACM-SGC-1 because SGC-1 does not contrain any non-linearity and ACM-SGC-1 and
ACMII-SGC-1 are the same.
8
Under review as a conference paper at ICLR 2022
Figure 4: Comparison of selected baseline GNNs (red), ACM-baseline (green), ACMII-baseline
(blue) with SOTA (magenta) models on 6 selected datasets. The black line and the error bar indicate
the standard deviation. The Symbol"f" means the amount of improvement of the best ACM-baseline
and ACM-baseline over the SOTA models. See Appendix H for a detailed discussion of the relation
between HaMgg and the performance of GNNs.
• A CTI ιr	A CTI ∕r-r-r Γ∙	1 7	Λ	τrcci,	1 ,1	1 ∙	ɪ ɪirʌ r∙ 1 , ∙ T	Λ
in ACM or ACMII framework7: we use Arw as LP filter and the corresponding HP filter is I - Arw
and they are deterministic. We compare them with several baseline and SOTA GNN models: MLP
with 2 layers (MLP-2), GAT (Velickovic et al., 2017), APPNP (Klicpera et al., 2018), GPRGNN
(Chien et al., 2021), H2GCN (Zhu et al., 2020b), MixHop (Abu-El-Haija et al., 2019), GCN+JK
(Kipf & Welling, 2016; Xu et al., 2018; Lim et al., 2021), GAT+JK (Velickovic et al., 2017; Xu
et al., 2018; Lim et al., 2021), FAGCN (Bo et al., 2021) GraphSAGE (Hamilton et al., 2017) and
Geom-GCN (Pei et al., 2020). Besides the 9 benchmark datasets Cornell, Wisconsin, Texas, Film,
Chameleon, Squirrel, Cora, Citeseer and Pubmed used in (Rozemberczki et al., 2021; Pei et al.,
2020), we further test the above models on a new benchmark dataset, Deezer-Europe, that is proposed
in (Rozemberczki & Sarkar, 2020). On each dataset used in (Rozemberczki et al., 2021; Pei et al.,
2020), we test the models 10 times following the same early stopping strategy, the same random data
splitting method 8 and Adam (Kingma & Ba, 2014) optimizer as used in GPRGNN (Chien et al.,
2021). For Deezer-Europe, we test the above models 5 times with the same early stopping strategy,
the same fixed splits and AdamW (Loshchilov & Hutter, 2017) used in (Lim et al., 2021).
To better visualize the performance boost and the comparison with SOTA models, in Figure 4, we
plot the bar charts of the test accuracy of SOTA models, 3 selected baselines (GCN, snowball-2,
snowball-3) and their ACM and ACMII augmented models on 6 most commonly used benchmark
heterophily datasets (See table 4 in Appendix A.3 for the full results and comparison). We can see
that after being applied in ACM or ACMII framework, the performance of the 3 baseline models are
significantly boosted on all tasks and can achieve SOTA performance. Especially on Cornell, Texas,
Film and Squirrel, the augmented models significantly outperform the current SOTA models. Overall,
It suggests that ACM or ACMII framework can help GNNs to generalize better on node classification
tasks on heterophilous graphs.
7GCNII and GCNII* are hard to be implemented under ACMn framework. See Appendix A.6 for explanation.
8See table 9 in Appendix I for the performance comparison with several SOTA models on the fixed
48%/32%/20% splits provided by (Pei et al., 2020).
9
Under review as a conference paper at ICLR 2022
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional
architectures via sparsified neighborhood mixing. In international conference on machine learning,
pp. 21-29. PMLR, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph
convolutional networks. arXiv preprint arXiv:2101.00797, 2021.
Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond euclidean data. arXiv, abs/1611.08097, 2016. URL http://
arxiv.org/abs/1611.08097.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In International Conference on Machine Learning, pp. 1725-1735. PMLR,
2020.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In International Conference on Learning Representations. https://openreview.
net/forum, 2021.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical
Soc., 1997.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst Convolutional neural networks
on graphs with fast localized spectral filtering. arXiv, abs/1606.09375, 2016. URL http:
//arxiv.org/abs/1606.09375.
Venkatesan Nallampatti Ekambaram. Graph structured data viewed through a fourier lens. University
of California, Berkeley, 2014.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv
preprint arXiv:1903.02428, 2019.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing,
pp. 6645-6649. Ieee, 2013.
William L Hamilton. Graph representation learning. Synthesis Lectures on Artifical Intelligence and
Machine Learning, 14(3):1-159, 2020.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
arXiv, abs/1706.02216, 2017. URL http://arxiv.org/abs/1706.02216.
Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming-Chang
Yang. Measuring and improving the use of graph information in graph neural networks. In
International Conference on Learning Representations, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph
neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
Honal neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. New benchmarks for learning on non-
homophilous graphs. arXiv preprint arXiv:2104.01404, 2021.
Vijay Lingam, Rahul Ragesh, Arun Iyer, and Sundararajan Sellamanickam. Simple truncated svd
based model for node classification on heterophilic graphs. arXiv preprint arXiv:2106.12807,
2021.
Meng Liu, Zhengyang Wang, and Shuiwang Ji. Non-local graph neural networks. arXiv preprint
arXiv:2005.14612, 2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multi-
scale deep graph convolutional networks. arXiv preprint arXiv:1906.02174, 2019.
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Training matters: Unlocking
potentials of deeper graph convolutional neural networks. arXiv preprint arXiv:2008.08838,
2020a.
Sitao Luan, Mingde Zhao, Chenqing Hua, Xiao-Wen Chang, and Doina Precup. Complete the missing
half: Augmenting aggregation filtering with diversification for graph convolutional networks. arXiv
preprint arXiv:2008.08844, 2020b.
Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters. arXiv preprint
arXiv:1905.09550, 2019.
Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social
networks. Annual review of sociology, 27(1):415-444, 2001.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.
Benedek Rozemberczki and Rik Sarkar. Characteristic Functions on Graphs: Birds of a Feather,
from Statistical Descriptors to Parametric Models. In Proceedings of the 29th ACM International
Conference on Information and Knowledge Management (CIKM ’20), pp. 1325-1334. ACM, 2020.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-Scale Attributed Node Embedding.
Journal of Complex Networks, 9(2), 2021.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE transactions on neural networks, 20(1):61-80, 2008.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv, abs/1710.10903, 2017.
Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q
Weinberger. Simplifying graph convolutional networks. arXiv preprint arXiv:1902.07153, 2019.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 5453-5462. PMLR, 10-15 Jul 2018.
URL http://proceedings.mlr.press/v80/xu18c.html.
11
Under review as a conference paper at ICLR 2022
Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two sides of the
same coin: Heterophily and oversmoothing in graph convolutional neural networks. arXiv preprint
arXiv:2102.06462, 2021.
Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra.
Graph neural networks with heterophily. arXiv preprint arXiv:2009.13566, 2020a.
Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. Advances in Neural
Information Processing Systems, 33, 2020b.
12
Under review as a conference paper at ICLR 2022
A Hyperparameters & Details of The Experiments
A. 1 Hyperparameter Searching Range for Synthetic Experiments
Hyperparameter Searching Range for Synthetic Experiments
Models'Hyperparameters ∣ lr		weight_decay	dropout	hidden
MLP-1	0.05	{5e-5, 1e-4, 5e-4, 1e-3, 5e-3 }	-	-
SGC-1	0.05	{5e-5, 1e-4, 5e-4, 1e-3, 5e-3}	-	-
ACM-SGC-1	0.05	{5e-5, 1e-4, 5e-4, 1e-3, 5e-3}	{ 0.1, 0.3, 0.5, 0.7, 0.9}	-
MLP-2	0.05	{5e-5, 1e-4, 5e-4, 1e-3, 5e-3}	{ 0.1, 0.3, 0.5, 0.7, 0.9}	64
GCN	0.05	{5e-5, 1e-4, 5e-4, 1e-3, 5e-3}	{ 0.1, 0.3, 0.5, 0.7, 0.9}	64
ACM-GCN	0.05	{5e-5, 1e-4, 5e-4, 1e-3, 5e-3}	{ 0.1, 0.3, 0.5, 0.7, 0.9}	64
Table 2: Hyperparameter Searching Range for Synthetic Experiments
A.2 Hyperparameter Searching Range for Ablation S tudy
Hyperparameter Searching Range for Ablation Study				
ModelsVHyperparameters ∣	lr	weight_decay	dropout	hidden
{0.01, 0.05, 0.1}
{0.01, 0.05, 0.1}
{0.01, 0.05, 0.1}
{0.01, 0.05, 0.1}
{0.01, 0.05, 0.1}
{0.01, 0.05, 0.1}
{0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
{0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
{0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
{0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
{0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
{0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}
{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}
{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}
{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}
{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}
SGC-LP+HP
SGC-LP+Identity
ACM-SGC-no adaptive mixing
GCN-LP+HP
GCN-LP+Identity
ACM-GCN-no adaptive mixing
64
64
64
Table 3: Hyperparameter Searching Range for Ablation Study
A.3 Full Results of The Comparison with SOTA Models
The main results of the full sets of experiments 9 with statistics of datasets are summarized in Table 4,
where we report the mean accuracy and standard deviation. We can see that after applied in ACM or
ACMII framework, the performance of baseline models are boosted on almost all tasks and achieve
SOTA performance on 8 out of 10 datasets. Especially, ACMII-GCN performs the best in terms of
average rank (3.40) across all datasets. Overall, It suggests that ACM or ACMII framework can
significantly increase the performance of GNNs on node classification tasks on heterophilous graphs.
A.4 Hyperparameter Searching Range for GNNs on Real-world Datasets
See table 5 for the hyperparameter seaching range of baseline GNNs, ACM-GNNs, ACMII-GNNs
and several SOTA models.
A.5 Optimal Hyperparameters for Baselines and ACM(II)-GNNs on Real-world
Tasks
See the reported optimal hyperparameters for baseline GNNs in table 6 and for ACM-GNNs and
ACMII-GNNs in table 7.
9The splits for all these experiments are random 60%/20%/20% splits for train/valid/test.
The open source code we use is from https://github.com/jianhao2016/GPRGNN/blob/
f4aaad6ca28c83d3121338a4c4fe5d162edfa9a2/src/utils.py#L16. See table 9 in Appendix
I for the performance comparison with several SOTA models on the fixed 48%/32%/20% splits provided by (Pei
et al., 2020).
13
Under review as a conference paper at ICLR 2022
	Cornell	Wisconsin	Texas	Film	Chameleon	Squirrel	Deezer-Europe	Cora	CiteSeer	PubMed ∣	
#nodes	183	251	183	7,600	2,277	5,201	28,281	2,708	3,327	19,717	
#edges	295	499	309	33,544	36,101	217,073	92,752	5,429	4,732	44,338	
#features	1,703	1,703	1,703	931	2,325	2,089	31,241	1,433	3,703	500	
#classes	5	5	5	5	5	5	2	7	6	3	
Hedge	0.5669	0.4480	0.4106	0.3750	0.2795	0.2416	0.5251	0.8100	0.7362	0.8024	
Hnode	0.3855	0.1498	0.0968	0.2210	0.2470	0.2156	0.5299	0.8252	0.7175	0.7924	
Hclass	0.0468	0.0941	0.0013	0.0110	0.0620	0.0254	0.0304	0.7657	0.6270	0.6641	
Data Splits	60%/20%/20% 60%∕20%∕2O% 60%∕20%∕2O% 60%∕20%∕2O% 60%∕20%∕2O% 60%∕20%∕2O% 50%/25%/25% 60%∕20%∕2O% 60%∕20%∕2O% 60%∕20%∕2O%										
HaMgg(G)	0.8032	0.7768	0.694	0.6822	0.61	0.3566	0.5790	0.9904	0.9826	0.9432	
											
					Test Accuracy (%) of State-of-the-art Models, Baseline GNN Models and ACM-GNN models							I Rank	
MLP-2*	91.30 ± 0.70	93.87 ± 3.33	92.26 ± 0.71	38.58 ± 0.25	46.72 ± 0.46	31.28 ± 0.27	66.55 ± 0.72	76.44 ± 0.30	76.25 ± 0.28	86.43 ± 0.13	18.60
GAT*	76.00 ± 1.01	71.01 ± 4.66	78.87 ± 0.86	35.98 ± 0.23	63.9 ± 0.46	42.72 ± 0.33	61.09 ± 0.77	76.70 ± 0.42	67.20 ± 0.46	83.28 ± 0.12	21.40
APPNP*	91.80 ± 0.63	92.00 ± 3.59	91.18 ± 0.70	38.86 ± 0.24	51.91 ± 0.56	34.77 ± 0.34	67.21 ± 0.56	79.41 ± 0.38	68.59 ± 0.30	85.02 ± 0.09	18.00
GPRGNN*	91.36 ± 0.70	93.75 ± 2.37	92.92 ± 0.61	39.30 ± 0.27	67.48 ± 0.40	49.93 ± 0.53	66.90 ± 0.50	79.51pm0.36	67.63 ± 0.38	85.07 ± 0.09	14.40
H2GCN	86.23 ± 4.71	87.5 ± 1.77	85.90 ± 3.53	38.85 ± 1.17	52.30 ± 0.48	30.39 ± 1.22	67.22 ± 0.90	87.52 ± 0.61	79.97 ± 0.69	87.78 ± 0.28	17.00
MixHop	60.33 ± 28.53	77.25 ± 7.80	76.39 ± 7.66	33.13 ± 2.40	36.28 ± 10.22	24.55 ± 2.60	66.80 ± 0.58	65.65 ± 11.31 49.52 ± 13.35		87.04 ± 4.10	23.50
GCN+JK	66.56 ± 13.82 62.50 ± 15.75		80.66 ± 1.91	32.72 ± 2.62	64.68 ± 2.85	53.40 ± 1.90	60.99 ± 0.14	86.90 ± 1.51	73.77 ± 1.85	90.09 ± 0.68	18.80
GAT+JK	74.43 ± 10.24	69.50 ± 3.12	75.41 ± 7.18	35.41 ± 0.97	68.14 ± 1.18	52.28 ± 3.61	59.66 ± 0.92	89.52 ± 0.43	74.49 ± 2.76	89.15 ± 0.87	16.70
FAGCN	88.03 ± 5.6	89.75 ± 6.37	88.85 ± 4.39	31.59 ± 1.37	49.47 ± 2.84	42.24 ± 1.2	66.86 p, 0.53	88.85 ± 1.36 82.37 ± 1.46 89.98 ± 0.54			14.10
GraphSAGE	71.41 ± 1.24	64.85 ± 5.14	79.03 ± 1.20	36.37 ± 0.21	62.15 ± 0.42	41.26 ± 0.26	OOM	86.58 ± 0.26	78.24 ± 0.30	86.85 ± 0.11	20.89
GeOm-GCNt	60.81	64.12	67.57	31.63	60.9	38.14	NA	85.27	77.99	90.05	22.67
SGC-1	70.98 ± 8.39	70.38 ± 2.85	83.28 ± 5.43	25.26 ± 1.18	64.86 ± 1.81	47.62 ± 1.27	59.73 ± 0.12	85.12 ± 1.64	79.66 ± 0.75	85.5 ± 0.76	20.10
SGC-2	72.62 ± 9.92	74.75 ± 2.89	81.31 ± 3.3	28.81 ± 1.11	62.67 ± 2.41	41.25 ± 1.4	61.56 ± 0.51	85.48 ± 1.48	80.75 ± 1.15	85.36 ± 0.52	20.70
GCNII	89.18 ± 3.96	83.25 ± 2.69	82.46 ± 4.58	40.82 ± 1.79	60.35 ± 2.7	38.81 ± 1.97	66.38 ± 0.45	88.98 ± 1.33	81.58 ± 1.3	89.8 ± 0.3	14.80
GCNII*	90.49 ± 4.45	89.12 ± 3.06	88.52 ± 3.02	41.54 ± 0.99	62.8 ± 2.87	38.31 ± 1.3	66.42 ± 0.56	88.93 ± 1.37	81.83 ± 1.78	89.98 ± 0.52	12.30
GCN	82.46 ± 3.11	75.5 ± 2.92	83.11 ± 3.2	35.51 ± 0.99	64.18 ± 2.62	44.76 ± 1.39	62.23 ± 0.53	87.78 ± 0.96	81.39 ± 1.23	88.9 ± 0.32	16.30
Snowball-2	82.62 ± 2.34	74.88 ± 3.42	83.11 ± 3.2	35.97 ± 0.66	64.99 ± 2.39	47.88 ± 1.23	OOM	88.64 ± 1.15	81.53 ± 1.71	89.04 ± 0.49	15.22
Snowball-3	82.95 ± 2.1	69.5 ± 5.01	83.11 ± 3.2	36.00 ± 1.36	65.49 ± 1.64	48.25 ± 0.94	OOM	89.33 ± 1.3	80.93 ± 1.32	88.8 ± 0.82	14.78
ACM-SGC-1	93.77 ± 1.91	93.25 ± 2.92	93.61 ± 1.55	39.33 ± 1.25	63.68 ± 1.62	46.4 ± 1.13	66.67 ± 0.56	86.63 ± 1.13	80.96 ± 0.93	87.75 ± 0.88	12.60
ACM-SGC-2	93.77 ± 2.17	94.00 ± 2.61	93.44 ± 2.54	40.13 ± 1.21	60.48 ± 1.55	40.91 ± 1.39	66.53 ± 0.57	87.64 ± 0.99	80.93 ± 1.16	88.79 ± 0.5	13.40
ACM-GCNII	92.62 ± 3.13	94.63 ± 2.96	92.46 ± 1.97	41.37 ± 1.37	58.73 ± 2.52	40.9 ± 1.58	66.39 ± 0.56	89.1 ± 1.61	82.28 ± 1.12	90.12 ± 0.4	10.40
ACM-GCNII*	93.44 ± 2.74	94.37 ± 2.81	93.28 ± 2.79	41.27 ± 1.24	61.66 ± 2.29	38.32 ± 1.5	66.6 ± 0.57	89.00 ± 1.35	81.69 ± 1.25	90.18 0.51	10.10
ACM-GCN	94.75 ± 3.8	95.75 ± 2.03	94.92 ± 2.88	41.62 ± 1.15	69.04 ± 1.74	58.02 ± 1.86	67.01 ± 0.38	88.62 ± 1.22	81.68 ± 0.97	90.66 ± 0.47	4.80
ACM-Snowball-2	95.08 ± 3.11	96.38 ± 2.59	95.74 ± 2.22	41.4 ± 1.23	68.51 ± 1.7	55.97 ± 2.03	OOM	88.83 ± 1.49	81.58 ± 1.23	90.81 ± 0.52	4.44
ACM-Snowball-3	94.26 ± 2.57	96.62 ± 1.86	94.75 ± 2.41	41.27 ± 0.8	68.4 ± 2.05	55.73 ± 2.39	OOM	89.59 ± 1.58	81.32 ± 0.97	91.44 ± 0.59	4.44
											
ACMII-GCN	95.9 ± 1.83	96.62 ± 2.44	95.08 ± 2.07 41.84 ± 1.15 68.38 ± 1.36			54.53 ± 2.09	67.15 ± 0.41	89.00 ± 0.72	81.79 ± 0.95	90.74 ± 0.5	3.40
ACMII-Snowball-2	95.25 ± 1.55	96.63 ± 2.24	95.25 ± 1.55	41.1 ± 0.75	67.83 ± 2.63	53.48 ± 0.6	OOM	88.95 ± 1.04	82.07 ± 1.04	90.56 ± 0.39	4.78
ACMII-Snowball-3	93.61 ± 2.79 97.00 ± 2.63 94.75 ± 3.09			40.31 ± 1.6	67.53 ± 2.83	52.31 ± 1.57	OOM	89.36 ± 1.26	81.56 ± 1.15	91.31 ± 0.6	5.89
											
Table 4: Experimental results: average test accuracy ± standard deviation on 10 real-world benchmark
datasets. The best results are highlighted. Results "*" are reported from Chien et al. (2021); Lim et al.
(2021) and results "^" are from Pei et al. (2020). NA means the reported results are not available and
OOM means out of memory.
14
Under review as a conference paper at ICLR 2022
Models\Hyperparameters	lr	weight_decay	dropout	hidden	lambda	alpha_l	head	layers	JK type
H2GCN	0.01	0.001	{0, 0.5}	{8, 16, 32, 64}	-	-	-	{1, 2}	-
MixHop	0.01	0.001	0.5	{8, 16, 32}	-	-	-	{2, 3}	-
GCN+JK	{0.1, 0.01,	0.001	0.5	{4, 8, 16, 32, 64}	-	-	-	2	{max, cat}
	0.001}
GAT+JK	{0.1, 0.01,	0.001	0.5	{4, 8, 12, 32}	-	-	{2,4,8}	2	{max, cat}
	0.001}
GCNII, GCNII*	0.01	{0, 5e-6, 1e-5, 5e-5, 1e-	0.5	64	{0.5, 1, 1.5}	{0.1,0.2,0.3,0,4,0-.5}	{4, 8, 16, 32}	-
	4, 5e-4, 1e-3} for Deezer-	for Deezer-
	Europe and {0, 5e-6, 1e-	Europe and
	5, 5e-5, 1e-4, 5e-4, 1e-3,	{4,	8,	16,
	5e-3, 1e-2} for others	32, 64} for
	others
Baselines: {SGC-1, SGC-	{0.002,	{0, 5e-6, 1e-5, 5e-5, 1e-	{0, 0.1,	64	-	-	-	-	-
2, GCN, Snowball-2,	0.01,	4, 5e-4, 1e-3} for Deezer- 0.2, 0.3,
Snowball-3,	FAGCN};	0.05} for Europe and {0, 5e-6, 1e-	0.4, 0.5,
ACM-{SGC-1,	SGC-	Deezer-	5, 5e-5, 1e-4, 5e-4, 1e-3,	0.6, 0.7,
2, GCN, Snowball-2,	Europe	5e-3, 1e-2} for others	0.8, 0.9}
Snowball-3};	ACMII-	and {0.01,
{SGC-1, SGC-2, GCN,	0.05, 0.1}
Snowball-2, Snowball-3}	for others
GraphSAGE	{0.01,0.05,	{0, 5e-6, 1e-5, 5e-5, 1e-	{ 0, 0.1,	8 for Deezer- -	-	-	-	-
	0.1}	4, 5e-4, 1e-3} for Deezer- 0.2, 0.3, Europe and 64
	Europe and {0, 5e-6, 1e-	0.4, 0.5, for others
	5, 5e-5, 1e-4, 5e-4, 1e-3,	0.6, 0.7,
	5e-3, 1e-2} for others	0.8, 0.9}
ACM-{GCNII, GCNII*}	0.01	{0, 5e-6, 1e-5, 5e-5, 1e-	{ 0, 0.1,	64	-	-	-	{1,2,3,4}	-
	4, 5e-4, 1e-3} for Deezer- 0.2, 0.3,
	Europe and {0, 5e-6, 1e-	0.4, 0.5,
	5, 5e-5, 1e-4, 5e-4, 1e-3,	0.6, 0.7,
	5e-3, 1e-2} for others	0.8, 0.9}
Table 5: Hyperparameter Searching Range for Real-world Datasets
A.6 Details of the Implementation of ACM and ACMII Frameworks
Unlike some baseline GNN models, in ACM(II) framework, we first use dropout operation over the
input data. The implementation of ACM(II)-GCN and ACM(II)-snowball is straightforward, but
SGC-1, SGC-2, GCNII and GCNII* are not able to be applied under ACM(II) framework and we
will make an explanation as follows.
•	SGC-1 and SGC-2: SGC does not contain nonlinearity, so the option 1 and option 2 in step
1 of equation 11 is the same for ACM-SGC and ACMII-SGC. Thus, we only implement
ACM-SGC.
•	GCNII and GCNII*:
GCNII: H('+1) = σ (((1 - ɑ`) AH⑶ + a`H(O)) ((1 - β') In + β'W⑶))
GCNII*: HS = σ ((1 - a`) AH⑶((1 - β') In + β'W(')) + +α'H⑼((1 - β') In + β'W2')))
From the above architecture of GCNII and GCNII* We cam see that, without major modifi-
cation, GCNII and GCNII* are hard to be put into ACMII framework. In ACMII framework,
before apply A, we first implement a nonlinear feature extractor σ(H'W(')). But in GCNII
and GCNii*, before multiplying W '(or W', W') to extract features, we need to add another
term including H(0), which are not filtered by A. This makes the order of aggregator A and
nonlinear extractor unexchangable and thus, incompatible with ACMII framework. So we
did not implement GCNII and GCNII* in ACMII framework.
The open source code is attached in the supplementary material.
A.7 Computing Resources
For all experiments on synthetic datasets and real-world datasets, we use NVidia V100 GPUs with
16/32GB GPU memory, 8-core CPU, 16G Memory. The software implementation is based on
PyTorch and PyTorch Geometric (Fey & Lenssen, 2019).
15
Under review as a conference paper at ICLR 2022
Hyperparameters for Baseline GNNs					
Datasets	I Models\Hyperparameters ∣	lr	weight_decay dropout hidden # layers Gat heads JK Type lambda alpha_l results	std	average epoch time/average total time
Cornell
SGC-1	0.05	1.00E-02	0	64	-	
SGC-2	0.05	1.00E-03	0	64	
GCN	0.1	5.00E-03	0.5	64	2
Snowball-2	0.01	5.00E-03	0.4	64	2
Snowball-3	0.01	5.00E-03	0.4	64	3
GCNII	0.01	1.00E-03	0.5	64	16
GCNII*	0.01	1.00E-03	0.5	64	8
FAGCN	0.01	1.00E-04	0.7	32	2
Mixhop	0.01	0.001	0.5	16	2
H2GCN	0.01	0.001	0.5	64	1
GCN+JK	0.1	0.001	0.5	64	2
GAT+JK	0.1	0.001	0.5	32	2
0.5
0.5
0.5
0.5
cat
max
70.98	8.39	2.53ms/0.51s
72.62	9.92	2.46ms/0.53s
82.46	3.11	3.67ms/0.74s
82.62	2.34	4.24ms/0.87s
82.95	2.1	6.66ms/1.36s
89.18	3.96	25.41ms/8.11s
90.49	4.45	15.35ms/4.05s
88.03	5.6	8.1ms/3.8858s
60.33	28.53	10.379ms/2.105s
86.23	4.71	4.381ms/1.123s
66.56	13.82	5.589ms/1.227s
74.43	10.24	10.725ms/2.478s
	SGC-1	0.05	5.00E-03
	SGC-2	0.1	1.00E-03
	GCN	0.1	1.00E-03
	Snowball-2	0.1	1.00E-03
	Snowball-3	0.05	5.00E-04
	GCNn	0.01	1.00E-03
	GCNII*	0.01	1.00E-03
Wisconsin	FAGCN	0.05	1.00E-04
	Mixhop	0.01	0.001
	H2GCN	0.01	0.001
	GCN+JK	0.1	0.001
	GAT+JK	0.1	0.001
	APPNP	0.05	0.001
	GPRGNN	0.05	0.001
	SGC-1	-0.o5	1.00E-03
	SGC-2	0.01	1.00E-03
	GCN	0.05	1.00E-02
	Snowball-2	0.05	1.00E-02
	Snowball-3	0.05	1.00E-02
	GCNn	0.01	1.00E-04
	GCNII*	0.01	1.00E-04
	FAGCN	0.01	5.00E-04
	Mixhop	0.01	0.001
	H2GCN	0.01	0.001
	GCN+JK	0.1	0.001
	GAT+JK	0.1	0.001
	SGC-1	0.01	5.00E-06
	SGC-2	0.01	5.00E-06
	GCN	0.1	5.00E-04
	Snowball-2	0.1	5.00E-04
	Snowball-3	0.1	5.00E-04
	GCNn	0.01	1.00E-04
	GCNII*	0.01	1.00E-06
	FAGCN	0.01	5.00E-05
	Mixhop	0.01	0.001
	H2GCN	0.01	0.001
	GCN+JK	0.1	0.001
	GAT+JK	0.001	0.001
	SGC-1	0.1	5.00E-06
	SGC-2	0.1	0.00E+00
	GCN	0.01	1.00E-05
	Snowball-2	1.00E-01	1.00E-05
	Snowball-3	0.1	5.00E-06
	GCNn	0.01	5.00E-06
Chameleon	GCNII*	0.01	5.00E-04
	FAGCN	0.002	1.00E-04
	Mixhop	0.01	0.001
	H2GCN	0.01	0.001
	GCN+JK	0.001	0.001
	GAT+JK	0.001	0.001
	SGC-1	-0.O5	0.00E+00
	SGC-2	0.1	0.00E+00
	GCN	0.01	5.00E-05
	Snowball-2	0.1	0.00E+00
	Snowball-3	0.1	0.00E+00
Squirrel	GCNII	0.01	1.00E-04
	GCNII*	0.01	5.00E-04
	FAGCN	0.05	1.00E-04
	Mixhop	0.01	0.001
	H2GCN	0.01	0.001
	GCN+JK	0.001	0.001
	GAT+JK	0.001	0.001
Cora
SGC-1
SGC-2
GCN
Snowball-2
Snowball-3
GCNII
GCNII*
FAGCN
Mixhop
H2GCN
GCN+JK
GAT+JK
0.1	5.00E-06
0.1	1.00E-05
0.1	5.00E-04
0.1	5.00E-04
0.05	1.00E-03
0.01	1.00E-04
0.01	5.00E-04
0.05	5.00E-04
0.01	0.001
0.01	0.001
0.001	0.001
0.001	0.001
	SGC-1	0.1	5.00E-04
	SGC-2	0.01	5.00E-04
	GCN	0.1	1.00E-03
	Snowball-2	0.1	1.00E-03
	Snowball-3	0.1	1.00E-03
	GCNn	0.01	1.00E-03
CiteSeer	GCNII*	0.01	1.00E-03
	FAGCN	0.05	5.00E-04
	Mixhop	0.01	0.001
	H2GCN	0.01	0.001
	GCN+JK	0.001	0.001
	GAT+JK	0.001	0.001
	SGC-1	0.05	5.00E-06
	SGC-2	0.05	5.00E-05
	GCN	0.1	5.00E-05
	Snowball-2	0.1	5.00E-04
	Snowball-3	0.1	5.00E-06
PubMed	GCNn	0.01	1.00E-06
	GCNII*	0.01	1.00E-06
	FAGCN	0.05	5.00E-04
	Mixhop	0.01	0.001
	H2GCN	0.01	0.001
	GCN+JK	0.01	0.001
	GAT+JK	0.1	0.001
0 0 0.7 0.5 0.8 0.5 0.5 0 0.5 0.5 0.5 0.5 0.5 0.5	64 64 64 64 64 64 64 32 16 32 32 4 64 256
0 0 0.9 0.9 0.9 0.5 0.5 0 0.5 0.5 0.5 0.5	64 64 64 64 64 64 64 32 32 64 32 8
0 0 0 0 0.2 0.5 0.5 0.6 0.5 0 0.5 0.5	64 64 64 64 64 64 64 32 8 64 64 32
0 0 0.9 0.9 0.9 0.5 0.5 0 0.5 0 0.5 0.5	64 64 64 64 64 64 64 32 16 32 32 4
0 0.9 0.7 0.9 0.8 0.5 0.5 0 0.5 0 0.5 0.5	64 64 64 64 64 64 64 32 32 16 32 8
0 0 0.2 0.1 0.6 0.5 0.5 0 0.5 0 0.5 0.5	64 64 64 64 64 64 64 32 16 32 64 32
0 0.9 0.9 0.8 0.9 0.5 0.5 0 0.5 0 0.5 0.5	64 64 64 64 64 64 64 32 16 8 32 8
0.3 0.1 0.6 0 0 0.5 0.5 0 0.5 0 0.5 0.5	64 64 64 64 64 64 64 32 16 64 32 8
2
2
3
8
4
2
2
1
2
2
2
2
0.5
1.5
Deezer-Europe
cat
max
1.5
0.5
cat
cat
70.38	2.85
74.75	2.89
75.5	2.92
74.88	3.42
69.5	5.01
0.5	83.25	2.69
0.3	89.12	3.06
89.75	6.37
77.25	7.80
87.5	1.77
62.5	15.75
69.5	3.12
92	3.59
93.75	2.37
83.28	5.43
81.31	3.3
83.11	3.2
83.11	3.2
83.11	3.2
0.5	82.46	4.58
0.5	88.52	3.02
-	88.85	4.39
-	76.39	7.66
-	85.90	3.53
-	80.66	1.91
-	75.41	7.18
2.83ms/0.57s
2.14ms/0.43s
3.74ms/0.76s
3.73ms/0.76s
5.46ms/1.12s
9.26ms/1.96s
12.9ms/4.6359s
10.281ms/2.095s
4.324ms/1.134s
5.117ms/1.049s
10.762ms/2.25s
10.303ms/2.104s
11.856ms/2.415s
2.55ms/0.54s
2.61ms/2.53s
3.59ms/0.73s
3.98ms/0.82s
5.56ms/1.12s
15.64ms/3.47s
8.8ms/6.5252s
11.099ms/2.329s
4.197ms/0.95s
5.28ms/1.085s
10.937ms/2.402s
	-	-	-	-	25.26	1.18	3.18ms/0.70s
	-	-	-	-	28.81	1.11	2.13ms/0.43s
2	-	-	-	-	35.51	0.99	4.86ms/0.99s
2	-	-	-	-	35.97	0.66	5.59ms/1.14s
3	-	-	-	-	36	1.36	7.89ms/1.60s
8	-	-	1.5	0.3	40.82	1.79	15.85ms/3.22s
4	-	-	1	0.1	41.54	0.99	
2	-	-	-	-	31.59	1.37	45.4ms/11.107s
3	8	max	-	-	33.13	2.40	17.651ms/3.566s
1	8	max	-	-	38.85	1.17	8.101ms/1.695s
2	8	cat	-	-	32.72	2.62	8.946ms/1.807s
2	4	cat	-	-	35.41	0.97	20.726ms/4.187s
	-	-	-	-	64.86 -	-	-	-	62.67	1.81 2.41	3.48ms/2.96s 4.43ms/1.12s
2	-	-	-	-	64.18	2.62	4.96ms/1.18s
2	-	-	-	-	64.99	2.39	4.96ms/1.00s
3	-	-	-	-	65.49	1.64	7.44ms/1.50s
4	-	-	0.5	0.1	60.35	2.7	9.76ms/2.26s
4	-	-	1.5	0.5	62.8	2.87	10.40ms/2.17s
2	-	-	-	-	49.47	2.84	8.4ms/13.8696s
2	8	max	-	-	36.28	10.2	11.372ms/2.297s
1	8	max	-	-	52.3	0.48	4.059ms/0.82s
2	8	cat	-	-	64.68	2.85	5.211ms/1.053s
2	8	max	-	-	68.14	1.18	13.772ms/2.788s
-- --	-	-	-	47.62 -	-	-	41.25	1.27 1.4	4.65ms/1.44s 35.06ms/7.81s
2-	-	-	-	44.76	1.39	8.41ms/2.50s
2-	-	-	-	47.88	1.23	8.96ms/1.92s
3-	-	-	-	48.25	0.94	14.00ms/2.90s
4-	-	1.5	0.2	38.81	1.97	13.35ms/2.70s
4-	-	1.5	0.3	38.31	1.3	13.81ms/2.78s
2-	-	-	-	42.24	1.2	16ms/6.7961s
2-	-	-	-	24.55	2.6	17.634ms/3.562s
1-	-	-	-	30.39	1.22	9.315ms/1.882s
2-	max	-	-	53.4	1.9	14.321ms/2.905s
24	max	-	-	52.28	3.61	29.097ms/5.878s
-- --	-	-	-	85.12 -	-	-	85.48	1.64 1.48	3.47ms/11.55s 2.91ms/6.85s
2-	-	-	-	87.78	0.96	4.24ms/0.86s
2-	-	-	-	88.64	1.15	4.65ms/0.94s
3-	-	-	-	89.33	1.3	6.41ms/1.32s
16	-	-	0.5	0.2	88.98	1.33	
4-	-	0.5	0.5	88.93	1.37	10.16ms/2.24s
2-	-	-	-	88.85	1.36	8.4ms/3.3183s
2-	-	-	-	65.65	11.31	11.177ms/2.278s
1-	-	-	-	87.52	0.61	4.335ms/1.209s
2-	cat	-	-	86.90	1.51	6.656ms/1.346s
22	cat	-	-	89.52	0.43	12.91ms/2.608s
	-	-	-	-	79.66 -	-	-	-	80.75	0.75 1.15	3.43ms/7.30s 5.33ms/4.40s
2	-	-	-	-	81.39	1.23	4.18ms/0.86s
2	-	-	-	-	81.53	1.71	5.19ms/1.11s
3	-	-	-	-	80.93	1.32	7.64ms/1.69s
16	-	-	0.5	0.2	81.58	1.3	
16	-	-	0.5	0.2	81.83	1.78	32.50ms/10.29s
2	-	-	-	-	82.37	1.46	9.4ms/4.7648s
2	-	-	-	-	49.52	13.35	13.793ms/2.786s
1	-	-	-	-	79.97	0.69	5.794ms/3.049s
2	-	max	-	-	73.77	1.85	5.264ms/1.063s
2	4	max	-	-	74.49	2.76	12.326ms/2.49s
-- --	-	-	-	87.75 -	-	-	88.79	0.88 0.5	6.04ms/2.61s 8.62ms/3.18s
2-	-	-	-	88.9	0.32	5.08ms/1.03s
2-	-	-	-	89.04	0.49	5.68ms/1.19s
3-	-	-	-	88.8	0.82	8.54ms/1.75s
4-	-	0.5	0.5	89.8	0.3	10.98ms/3.21s
4-	-	0.5	0.1	89.98	0.52	11.47ms/3.24s
2-	-	-	-	89.98	0.54	14.5ms/6.411s
2-	-	-	-	87.04	4.10	17.459ms/3.527s
1-	-	-	-	87.78	0.28	8.039ms/2.28s
2-	cat	-	-	90.09	0.68	12.001ms/2.424s
24	max	-	-	89.15	0.87	20.403ms/4.125s
FAGCN	0.01	0.0001	0	32	2	-	-	-	-	66.86			0.53	41.7ms/20.8362s
GCNII	0.01	5e-6,1e-5	0.5	64	32	-	-	0.5	0.5	66.38	0.45	126.58ms/63.16s
GCNII*	0.01	1e-4,1e-3	0.5	64	32	-	-	0.5	0.5	66.42	0.56	134.05ms/66.89s
Table 6: Optimal Hyperparameters for baseline models
16
Under review as a conference paper at ICLR 2022
HyPerParameters for ACM-GNNs and ACMn-GNNs						
Datasets	I Models'Hyperparameters ∣	lr	weight_decay	dropout hidden # layers Gat heads JK Type lambda alpha_l results	std	average epoch time/average total time
	ACM-SGC-1	0.01	5.00E-03	0.6	64				93.77	1.91	5.53ms/2.31s
	ACM-SGC-2	0.01	5.00E-03	0.6	64				93.77	2.17	4.73ms/1.87s
	ACM-GCN	0.05	1.00E-02	0.2	64	2-			94.75	3.8	8.25ms/1.69s
	ACMII-GCN	0.1	1.00E-02	0.5	64	2-			95.25	2.79	8.43ms/1.71s
Cornell	ACM-GCNII	0.01	1.00E-03	0.5	64	1-	--	0-.5	0.4	92.62	3.13	6.81ms/1.43s
	ACM-GCNII*	0.01	5.00E-04	0.5	64	1-	-	0.5	0.1	93.44	2.74	6.76ms/1.39s
	ACM-Snowball-2	0.05	1.00E-02	0.2	64	2-			95.08	3.11	9.15ms/1.86s
	ACM-Snowball-3	0.1	1.00E-02	0.4	64	3-			94.26	2.57	13.20ms/2.68s
	ACMn-Snowball-2	0.05	1.00E-02	0.6	64	2-			95.25	1.55	8.23ms/1.72s
	ACMn-Snowball-3	0.05	1.00E-02	0.7	64	3-			93.61	2.79	11.70ms/2.37s
	ACM-SGC-1	0.05	5.00E-03	0.7	64				93.25	2.92	5.96ms/1.34s
	ACM-SGC-2	0.1	5.00E-03	0.2	64				94	2.61	4.60ms/0.95s
	ACM-GCN	0.1	5.00E-03	0	64	2-			95.75	2.03	8.11ms/1.64s
	ACMII-GCN	0.1	1.00E-02	0.2	64	2-			96.62	2.44	8.28ms/1.68s
Wisconsin	ACM-GCNII	0.01	5.00E-03	0.5	64	1-	-1	0.1	94.63	2.96	9.31ms/2.19s
	ACM-GCNII*	0.01	1.00E-03	0.5	64	1-	-	1.5	0.4	94.37	2.81	7.11ms/1.45s
	ACM-Snowball-2	0.1	5.00E-03	0.1	64	2-			96.38	2.59	8.63ms/1.74s
	ACM-Snowball-3	0.05	1.00E-02	0.3	64	3-			96.62	1.86	12.79ms/2.58s
	ACMII-Snowball-2	0.1	1.00E-02	0.1	64	2-			96.63	2.24	8.11ms/1.65s
	ACMII-Snowball-3	0.1	5.00E-03	0.1	64	3-			97	2.63	12.38ms/2.51s
	ACM-SGC-1	0.01	5.00E-03	0.6	64				93.61	1.55	5.43ms/2.18s
	ACM-SGC-2	0.05	5.00E-03	0.4	64				93.44	2.54	4.59ms/1.01s
	ACM-GCN	0.05	1.00E-02	0.6	64	2-			94.92	2.88	8.33ms/1.70s
	ACMII-GCN	0.1	5.00E-03	0.4	64	2-			95.08	2.54	8.49ms/1.72s
Texas	ACM-GCNII	0.01	1.00E-03	0.5	64	1-	--	0-.5	0.4	92.46	1.97	6.47ms/1.36s
	ACM-GCNII*	0.01	1.00E-03	0.5	64	1-	-	0.5	0.4	93.28	2.79	7.03ms/1.45s
	ACM-Snowball-2	0.05	1.00E-02	0.1	64	2-			95.74	2.22	8.35ms/1.71s
	ACM-Snowball-3	0.01	5.00E-03	0.6	64	3-			94.75	2.41	12.56ms/2.63s
	ACMII-Snowball-2	0.1	1.00E-02	0.4	64	2-			95.25	1.55	9.74ms/1.97s
	ACMII-Snowball-3	0.05	1.00E-02	0.6	64	3-			94.75	3.09	11.91ms/2.42s
	ACM-SGC-1	0.05	5.00E-05	0.7	64				39.33	1.25	5.21ms/2.33s
	ACM-SGC-2	0.1	5.00E-05	0.7	64				40.13	1.21	12.41ms/4.87s
	ACM-GCN	0.1	5.00E-04	0.5	64	2-			41.62	1.15	10.72ms/2.66s
	ACMII-GCN	0.1	5.00E-04	0.5	64	2-			41.24	1.16	10.51ms/2.44s
Film	ACM-GCNII	0.01	0.00E+00	0.5	64	3-	--	1-.5	0.2	41.37	1.37	13.65ms/2.74s
	ACM-GCNII*	0.01	1.00E-05	0.5	64	3-	-	1.5	0.1	41.27	1.24	14.98ms/3.01s
	ACM-Snowball-2	0.1	5.00E-03	0	64	2-			41.4	1.23	10.30ms/2.08s
	ACM-Snowball-3	0.05	1.00E-02	0	64	3-			41.27	0.8	16.43ms/3.52s
	ACMII-Snowball-2	0.1	5.00E-03	0	64	2-			41.1	0.75	10.74ms/2.19s
	ACMII-Snowball-3	0.05	5.00E-03	0.2	64	3-			40.31	1.6	16.31ms/3.29s
	ACM-SGC-1	0.1	5.00E-06	0.9	64				63.68	1.62	5.41ms/1.21s
	ACM-SGC-2	0.1	5.00E-06	0.9	64				60.48	1.55	7.86ms/1.81s
	ACM-GCN	0.01	5.00E-05	0.8	64	2-			68.18	1.67	10.55ms/3.12s
	ACMII-GCN	0.05	5.00E-05	0.7	64	2-			68.38	1.36	10.90ms/2.39s
Chameleon	ACM-GCNII	0.01	5.00E-06	0.5	64	4-	--	0-.5	0.1	58.73	2.52	18.31ms/3.68s
	ACM-GCNII*	0.01	1.00E-03	0.5	64	1-	-1	0.1	61.66	2.29	6.68ms/1.40s
	ACM-Snowball-2	0.05	5.00E-05	0.7	64	2-			68.51	1.7	9.92ms/2.06s
	ACM-Snowball-3	0.01	1.00E-04	0.7	64	3-			68.4	2.05	14.49ms/3.15s
	ACMII-Snowball-2	0.1	5.00E-05	0.6	64	2-			67.83	2.63	9.99ms/2.10s
	ACMII-Snowball-3	0.05	1.00E-04	0.7	64	3-			67.53	2.83	15.03ms/3.29s
	ACM-SGC-1	0.05	0.00E+00	0.9	64				46.4	1.13	6.96ms/2.16s
	ACM-SGC-2	0.05	0.00E+00	0.9	64				40.91	1.39	35.20ms/10.66s
	ACM-GCN	0.05	5.00E-06	0.6	64	2-			58.02	1.86	14.35ms/2.98s
	ACMII-GCN	0.05	0.00E+00	0.7	64	2-			53.76	1.63	14.08ms/3.39s
Squirrel	ACM-GCNII	0.01	1.00E-05	0.5	64	4-	--	0-.5	0.1	40.9	1.58	20.72ms/4.17s
	ACM-GCNII*	0.01	1.00E-03	0.5	64	4-	-	0.5	0.3	38.32	1.5	21.78ms/4.38s
	ACM-Snowball-2	0.05	5.00E-06	0.6	64	2-			55.97	2.03	15.38ms/3.15s
	ACM-Snowball-3	0.01	1.00E-04	0.6	64	3-			55.73	2.39	26.15ms/5.94s
	ACMII-Snowball-2	0.1	5.00E-06	0.6	64	2-			53.48	0.6	15.54ms/3.19s
	ACMII-Snowball-3	0.05	5.00E-05	0.5	64	3-			52.31	1.57	26.24ms/5.30s
	ACM-SGC-1	0.01	5.00E-06	0.9	64				86.63	1.13	6.00ms/7.40s
	ACM-SGC-2	0.1	5.00E-05	0.6	64				87.64	0.99	4.85ms/1.17s
	ACM-GCN	0.1	5.00E-03	0.5	64	2-			88.62	1.22	8.84ms/1.81s
	ACMII-GCN	0.1	5.00E-03	0.4	64	2-			89	0.72	8.93ms/1.83s
Cora	ACM-GCNII	0.01	1.00E-03	0.5	64	3-	-1	0.2	89.1	1.61	14.07ms/3.04s
	ACM-GCNII*	0.01	1.00E-02	0.5	64	4-	-1	0.2	89	1.35	11.36ms/2.48s
	ACM-Snowball-2	0.05	1.00E-03	0.6	64	2-			88.83	1.49	9.34ms/1.92s
	ACM-Snowball-3	0.1	1.00E-02	0.3	64	3-			89.59	1.58	13.33ms/2.75s
	ACMII-Snowball-2	0.1	5.00E-03	0.5	64	2-			88.95	1.04	9.29ms/1.90s
	ACMII-Snowball-3	0.1	5.00E-03	0.5	64	3-			89.36	1.26	14.18ms/2.89s
	ACM-SGC-1	0.01	5.00E-04	0.9	64				80.96	0.93	5.90ms/4.31s
	ACM-SGC-2	0.05	5.00E-04	0.9	64				80.93	1.16	5.01ms/1.42s
	ACM-GCN	0.05	5.00E-03	0.7	64	2-			81.68	0.97	11.35ms/2.57s
	ACMII-GCN	0.05	5.00E-05	0.7	64	2-			81.58	1.77	9.55ms/1.94s
CiteSeer	ACM-GCNII	0.01	1.00E-02	0.5	64	3-	--	0-.5	0.3	82.28	1.12	15.61ms/3.56s
	ACM-GCNII*	0.01	1.00E-02	0.5	64	3-	-	0.5	0.5	81.69	1.25	15.56ms/3.61s
	ACM-Snowball-2	0.05	5.00E-03	0.7	64	2-			81.58	1.23	11.14ms/2.50s
	ACM-Snowball-3	0.01	5.00E-03	0.9	64	3-			81.32	0.97	15.91ms/5.36s
	ACMII-Snowball-2	0.05	5.00E-03	0.7	64	2-			82.07	1.04	10.97ms/2.55s
	ACMII-Snowball-3	0.05	1.00E-04	0.6	64	3-			81.56	1.15	14.95ms/3.03s
	ACM-SGC-1	0.05	5.00E-06	0.3	64				87.75	0.88	6.04ms/2.61s
	ACM-SGC-2	0.05	5.00E-05	0.1	64				88.79	0.5	8.62ms/3.18s
	ACM-GCN	0.1	5.00E-04	0.2	64	2-			90.54	0.63	10.20ms/2.08s
	ACMII-GCN	0.1	5.00E-04	0.2	64	2-			90.74	0.5	10.20ms/2.07s
PubMed	ACM-GCNII	0.01	1.00E-04	0.5	64	3-	--	1-.5	0-.5	90.12	0.4	15.07ms/3.35s
	ACM-GCNII*	0.01	1.00E-04	0.5	64	3-	-	1.5	0.5	90.18	0.51	16.62ms/3.72s
	ACM-Snowball-2	0.1	1.00E-04	0.3	64	2-			90.81	0.52	11.52ms/2.36s
	ACM-Snowball-3	0.05	1.00E-03	0.2	64	3-			91.44	0.59	18.06ms/3.69s
	ACMII-Snowball-2	0.1	1.00E-04	0.3	64	2-			90.56	0.39	11.74ms/2.39s
	ACMII-Snowball-3	0.1	5.00E-04	0.2	64	3-			91.31	0.6	18.61ms/3.88s
	ACM-SGC-1	0.05	0,5e-6,1e-5,5e-5	0.3	64				66.67	0.56	146.41ms/73.06s
	ACM-SGC-2	0.002	5e-5,1e-4	0.3	64				66.53	0.57	195.21ms/97.41s
Deezer-EuroPe	ACM-GCN	0.002	5.00E-04	0.5	64	2-			67.01	0.38	136.45ms/68.09s
	ACMII-GCN	0.01	5.00E-05	0.8	64	2-			67.15	0.41	135.24ms/67.48s
	ACM-GCNII	0.01	0,5e-6	0.5	64	1-	--	0-.5	0.4	66.39	0.56	80.82ms/40.33s
	ACM-GCNII*	0.01	0.0001,1e-3	0.5	64	1-	-	1.5	0.2	66.6	0.57	80.95ms/40.40s
Table 7: Optimal Hyperparameters for ACM-GNNs and ACMII-GNNs
17
Under review as a conference paper at ICLR 2022
B Experimental Setup and Further Discussion on Synthetic
Graphs
B.1	Detailed Description of Data Generation Process
•	For each node v, we first randomly generate its degree dv .
•	Given dv, we sample hdv intra-class edges and (1 - h)dv inter-class edges.
More specifically in our synthetic experiments, for a given h,
•	we generate node degree dv for nodes in each class from multinomial distribution with
numpy.random.multinomial(800/h, numpy.ones(400)/400, size=1)[0].
•	For a sampled dv , we generate intra-class edges from (does not include self-loop)
numpy.random.multinomial(hdv , numpy.ones(399)/399, size=1)[0]
and inter-class edges from numpy.random.multinomial((1-h) dv , numpy.ones(1600)/1600,
size=1)[0].
We will release the code and the generated data later.
B.2	Further Discussion of Aggregation Homophily on Regular Graphs
We notice that in Figure 2(a), the performance of SGC-1 and GCN both have a turning point, i.e.,
when Hedge(G) is smaller than a certain value, the performance will get better instead of getting worse.
With some extra restriction on node degree in data generation process, we find that this interesting
phenomenon can be theoretically explained by the following proposition 1 based on our proposed
similarity matrix which can verify the usefulness of HaMgg(G). We first generate regular graphs ,i.e.,
each node has the same degree, as follows,
Generate Synthetic Regular Graphs We first generate 180 graphs in total with 18 edge homophily
levels varied from 0.05 to 0.9, each corresponding to 10 graphs. For every generated graph, we have
5 classes with 400 nodes in each class. For each node, we randomly generate 10 intra-class edges
and [ H，0(g)- 10] inter-class edges. The features of nodes in each class are sampled from node
features in the corresponding class of the base dataset. Nodes are randomly split into 60%/20%/20%
for train/validation/test. We train 1-hop SGC (sgc-1) Wu et al. (2019) and GCN Kipf & Welling
(2016) on synthetic data (see Appendix A.1 for hyperparameter searching range). For each value of
Hedge(G), we take the average test accuracy and standard deviation of runs over 10 generated graphs.
We plot the performance curves in Figure 5.
pubmed gcn
pubmedsgc
dιameleongcn
dιameleon sgc
SqIJirTeLgCn
squirrel sgc
Figure 5: Synthetic experiments for edge homophily on regular graphs.
From Figure 5 we can see that the turning point is a bit less than 0.2. We derive the following
proposition for d-regular graph to explain and predict it.
Proposition 1. (See Appendix D for proof). Suppose there are C classes in the graph G and G is a
d-regular graph (each node has d neighbors). Given d, edges for each node are i.i.d.generated, such
18
Under review as a conference paper at ICLR 2022
that each edge of any node has probability h to connect with nodes in the same class and probability
1 - h to connect with nodes in different classes. Let the aggregation operator A = Arw. Then, for
nodes v, u1 and u2, where Zu1,: = Zv,: and Zu2,: 6= Zv,:, we have
g(h) ≡ E 卜(A,Z)v,uj - E 卜(A,Z)v,u2) = ((C - 1* +1) -(1-h)d)2	(12)
(C - 1)(d + 1)
and the minimum of g(h) is reached at
d + 1 — C
-Cd-
h
dintra /h+ 1 - C
-------：-------------⇒ h
C (dintra/h)
dintra
Cdintra + C - 1
where dintra = dh, which is the expected number of neighbors of a node that have the same label as
the node.
The value of g(h) in equation 12 is the expected differences of the similarity values between nodes
in the same class as v and nodes in other classes. g(h) is strongly related to the definition of
aggregation homophily and its minimum potentially implies the turning point of performance curves.
In the synthetic experiments, we have dintra = 10, C = 5 and the minimum of g(h) is reached at
h= 5/27 ≈ 0.1852, which corresponds to the lowest point in the performance curve in Figure 5. In
other words, the Hedge(G) where SGC-1 and GCN perform worst is where g(h) gets the smallest
value, instead of the point with the smallest edge homophily value Hedge(G) = 0. This reveals the
advantage of Hagg(G) over Hedge(G) by taking use of the similarity matrix.
C Details of Gradient Calculation in equation 5
C.1 Derivation in Matrix Form
This derivation is similar to (Luan et al., 2020a).
In output layer, we have
Y = Softmax(AXW) ≡ SoftmaX(Y0) = (exp(Y0)1。IT) 1 Θ exp(Y0) > 0
L = -trace(ZT log Y)
where 1。∈ RC×1, (∙)-1 is point-wise inverse function and each element of Y is positive. Then
dL = -trace (ZT((Y)-1 Θ dY)) = —trace (ZT ((Softmax(Y0))-1 Θ d softmax(Y0)))
Note that
d softmax(Y0) = 一(exp(Y0)1CIT) 2 Θ [(exp(Y0) Θ dY0)1CIT] Θ exp(Y0)
+ (exp(Y0)1C 1T)-1 Θ (exp(Y0) Θ dY0)
=—softmax(Y0) Θ (exp(Y0)1C 1C)-1 Θ [(exp(Y0) Θ dY0)1CIT]
+ softmax(Y0) Θ dY0
=softmax(Y0) Θ (— (exp(Y0)1C 1C) 1 Θ [(exp(Y0) Θ dY0)1CIT] + dY0)
19
Under review as a conference paper at ICLR 2022
Then,
dL = - trace (Zt 卜SOftmax(Y0))-1 Θ SOftmax(Y0) Θ ( - (exp(YDIC1TC) 1
Θ [(exp(Y0) Θ dY0)1c 1C] + dY0)))
=-trace (ZT (- (exp(Y0)1cIT) 1 Θ [(exp(Y0) Θ dY0)1cIT] + dY0))
=trace (((Z Θ (exp(Y0)1c 1T)-1) 1c 1T)T [exp(Y') Θ dY0] - ZTdY)
=trace ((exp(Y0) Θ ((Z Θ (exp(Y0)1c 1T)-1) 1c 1T))T dY' - ZTdY)
=trace ((exp(Y') Θ (exp(Y')1CIT) 1) dY' — ZTdY')
=trace ((SOftmax(Y') — Z)tdY')
where the 4-th equation holds due to (Z Θ (exp(Y')1c 1T) ɔ 1c 1T = (exp(Y')1c 1T) 1. Thus,
we have
dL	.
-==softmax(Y') - Z = Y - Z
For Y' and W, we have
dY'=AXdWanddL=trace(焉 dY0) =trace(H AXdW) =trace(焉 dW
To get dw we have,
篇=XT AT 芸=XT AT (Y - Z)
(13)
C.2 Component-wise Derivation
Denote X = XW. We rewrite L as follows:
L = -trace (Zt log ((exp(Y')1c 1T)-1 Θ exp(Y')))
-trace (ZT (- log(exp(Y')1c 1T) + Y'))
-trace (ZTY') + trace (ZT log (exp(Y')1cIT))
-trace (zTAXW) + trace (Zt log (exp(Y')1cIT))
-trace (zTAXW) + trace (1T log (exp(Y「)1C))
N	N	Cc
-X X AijZix + X log X exp( X AijXm)
i=1 j∈Ni	i=1	∖c=1	j∈Ni
-Xlog exp X X AijZi,cXj,c
i=1	∖	∖ c=1 j∈Ni
N
+ X log
i=1
C
X exp
c=1
E Aij Xjc
j∈Ni
N
- X log
i=1
C
exp I P P AijZi,cXj,c
∖c=1 j∈Ni
(P exp( P AijXm))
∖c=1	j∈Ni	)
20
Under review as a conference paper at ICLR 2022
C
Note that P Zj,c
c=1
1 for any j. Consider the derivation of L over Xj0,c0 :
dL
C
N	exp(	Ai,jXj,c)
^X	c=1	j∈Ni
i=1 eχp (PP AijZi,cXj,c
c=1 j∈Ni
×
CC
Ai,j0Zid exp ∑ ∑ Ai,jZi,cXj,c	∑ exp( ∑ AijXj,c)
V	/	∖c=1 j∈Ni	C V=1	j∈Ni
,	∖^2
C
P exp( P A^i,j^^j,c)
c=1	j∈Ni
∖
(Aij) exp (PI ,PiAijZi,cχj,c!(XPjPiAij χj,c0)!
—
P exp( P AijXj,c))
c=1	j∈Ni
/
N
-X
i=1
(Aij0 ZiQ) (ILeXP(jPi Aij Xj,J -(Aij)(xpjPi Aij XjJ
∖
/
C
P eχp( P Aii,j}Cj,c)
c=1 j∈Ni
/
N
-X
i=1
c=1,c6=c0
Ai,j0
C
P	(Zie)eχp( P AijXj,C)	+ (Zie-I) eχp( P AijXj,c0)
j∈Ni	)
(C	一 「〜、
P exP( P AijXj,C)
c=1	j∈Ni
j∈Ni
∖
/
N
-X Ai,j0 (ZieP(Yi = C) + (ZiC — 1)Ρ(Y = c0))
i=1
N
-X AijO (Zie- P(Yi = CO))
i=L
Writing the above in matrix form, we have
dL = A(Z - Y), dL = XTA^T(Z - Y), ∆Y0 H AXXTA^T(Z - Y)	(14)
dX	dW
D Proof of Proposition 1
Proof. According to the given assumptions, for node v, We have A^v,k = d++ι ,the expected number of
intra-Class edges is dh (here the self-loop edge introduced by A is not counted based on the definition
of edge homophily and data generation process) and inter-class edges is (1 - h)d. Suppose there are
C ≥ 2 classes. Consider matrix √1Z,
Then, We have E
tor function.
[(AZ)v,c] = E 区 Av,k"=eT}
k∈V
E1	T
L {Zk,：=eC 丹
^d+T
Where 1 is the indica-
21
Under review as a conference paper at ICLR 2022
Eh1
When v is in class c, we have
k∈V
introduced by A).
{Zk,：=eT}]	hd+1
d+1	d+1
(hd+ 1 = hd intra-class edges + 1 self-loop
When v is not in class c, we have
E1	T
P L {Zk,：=eC}」
乙	d+1
k∈V
(C-11ʒhdji) ((1 - h)d inter-class edges
uniformly distributed in the other C - 1 classes).
For nodes v, u, we have (AZ)v,:, (AZ)u,: ∈ RC and since elements in Av,k and Au,k0 are indepen-
dently generated for all k, k0 ∈ V, we have
E
E kAz)v,c(Az)u,J = E
,k1{Zk,:=ecT
}) E (£ Aiu,k01{Zk0=T})
Thus,
E 卜(A, Z)v,u] = E [< (Az)v,：, (Az)u,: >] = XE
c
J ( hd+1 J + ((1 —h)d)2
,k1{Zk,:=ecT}) E
(E Au,k01{Zk0,：=eT})
k0∈V
2(hd+1)(1-h)d l
(C-1)(d+1)2 +
(C-2)(1-h)2d2
(C-1)2 (d+1)2，
u, v are in the same class
u, v are in different classes
For nodes u1, u2, and v, where Zu1,: = Zv,: and Zu2,: 6= Zv,:,
g(h) ≡ E [s(A, Z)v,uιi - E [s(A, Z)v,u2i
(15)
(C- 1)2 (hd + 1)2 + (C - 1) [(1 - h)d]2 - (C - 1) (2(hd + 1)(1 - h)d) - (C - 2) [(1 - h)d]2
(C-1)2(d+1)2
_ ((C - i)(hd + 1) - (1 - h)dγ
∖	(C - 1)(d +1)	)
Setting g(h) = 0, we obtain the optimal h:
d+1-C
h =
(16)
For the data generation process in the synthetic experiments, we fix dintra, then d = dintra/h, which is
a function of h. We change d in equation 16 to dintra/h, leading to
dintra/h + 1 - C
h —___________________
Cdintra/h
(17)
It is easy to observe that h satisfying equation 17 still makes g(h) = 0, when d in g(h) is replaced by
dintra/h. From equation 17 we obtain the optimal h in terms of dintra:
dintra
C dintra + C - 1
D.1 An Extension of Proposition 1
Base on the definition of aggregation similarity, we have
/	.	、	InvlMeanu({S(A,Z)v,u∣Zu,: = Z。,：}) ≥ Mean“({S(A,Z)v,u|Zu,： = ZvJ)o∣
Sagg (S(A,Z)) = E------------------------------------------------------------------U
|V|
h
□
22
Under review as a conference paper at ICLR 2022
1
v∈V {Meanu ({S(A,Z)ν,u |Zu,： = Zv,：}) ≥Meanu ({S(A,Z)v,u |Zu,：=Zv,：}) }
Then,
V V V w	V 1nM InMeanu ({S(A,Z)v,u|Zu,： = Zv,：})ZMeanu({S(A,Z)v,u|Zu,：=Zv,：})O
E (Sagg (S(A, Z))) = E I v∈κj-------------------------------V----------------------------------
P P (Meanu({S(A,Z)v,u∣Zu,: = Zv,：}) ≥ Meanu({S(A,Z)v,u|Zu,： = Zv,：
v∈V
|V|
P (Meanu ({S(A, Z )v,u|Zu,: = ZvJ) - Meanu ({S(A, Z)v,u|Zu,: = ZvJ) ≥ 0
Consider the random variable
RV = Meanu ({S(A)Z)v,ulZu,: = ZvJ) - Meanu ({S(A)Z)v,ulZu,: = ZvJ)
Since RV is symmetrically distributed and under the conditions in proposition 1, its expectation is
E[RV] = g(h) as showed in equation 15. Since the minimum of g(h) is 0 and RV is symmetrically
distributed, we have P(RV ≥ 0) ≥ 0.5 and this can explain why Hagg(G) is always greater than 0.5
in many real-world tasks.
E Proof of Theorem 1
Proof. Define Wvc = (AZ)v,c. Then,
C
wv = X Av,ki{Zk,：=eT} ∈ [0,1], X wv = 1
k∈V	c=1
Note that
s(i - A,Z) = (I - A)ZZT(I - A)τ = ZZT + AZZTAT - AZZT - zzTAT (18)
For any node v, let the class v belongs to be denoted by cv. For two nodes v, u, if Zv,: 6= Zu,:, we
have
(ZZT)v,u = 0
C
(AZZ T AT )v,u = χ WvWU
c=1
(AZZ T )v,u = wcu
(zz T AT )v,u = (AZZ T )u,v = WU
Then, from equation 18 it follows that
C
(S(I - A, z))v,u = X w；wu - w;u - w=v
c=1
When C = 2,
S (I - A, Z )v,u = wv≈u (WUu - 1) + WUv (WvJv - 1) ≤ 0
If Zv,: = Zu,: , i.e., cv = cu, we have
(ZZT)v,u = 1
C
(AZZ T AT )v,u = X WcWc
c=1
23
Under review as a conference paper at ICLR 2022
(AZZ T )v,u = W；V
(ZZ T AT )v,u = (AZZ T )u,v = WUU = Wuv
Then, from equation 18 it follows that
C
S(I- A, Z)v,u = ι + X w^wu - wcv - wcv
c=1
C
= X	Wvc Wuc + 1 + Wvcv Wucv - Wvcv - Wucv
c=1,c6=cv
C
= X	WvcWuc+(1-Wvcv)(1-Wucv) ≥0
c=1,c6=cv
Thus, if C = 2, for any V ∈ V, if Zu,： = Zv,：, We have S(I - A, Z)v,u ≤ 0; if Zu,： = Zv,：, We
have S(I - A, Z)v,u ≥ 0. Apparently, the two conditions in equation 9 are satisfied. Thus V is
diversification distinguishable and DDAX (G) = 1. The theorem is proved.	口
F Model Comparison on Synthetic Graphs
0.0
0.0
Q∙8∙64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(a) syn-Cora
Q∙8∙64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(d) syn-Chameleon
0.0
0.0
O-8-64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(b) syn-CiteSeer
O-8-64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(e) syn-Squirrel
O-8-64 2
Ioooo
>U23U<
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(c) syn-PubMed
O-8-64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(f) syn-Film
Figure 6:	Comparison of test accuracy (mean ± std) of MLP-1, SGC-1 and ACM-SGC-1 on synthetic
datasets
24
Under review as a conference paper at ICLR 2022
0.0
0.0
Q∙8∙64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(a) syn-Cora
Q∙8∙64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(d) syn-Chameleon
0.0
0.0
O-8-64 2
Ioooo
>U238<
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(b) syn-CiteSeer
O-8-64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(e) syn-Squirrel
O-8-64 2
Ioooo
>U23U<
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(c) syn-PubMed
O-8-64 2
Ioooo
0.2	0.4	0.6	0.8	1.0
Modified Aggregation Homophily
(f) syn-Film
Figure 7:	Comparison of test accuracy (mean ± std) of MLP-2, GCN and ACM-GCN on synthetic
datasets
In order to separate the effects of nonlinearity and graph structure, we compare sgc with 1 hop
(sgc-1) with MLP-1 (linear model). For GCN which includes nonlinearity, we use MLP-2 as its
corresponding graph-agnostic baseline model. We train the above GNN models, graph-agnostic
baseline models and ACM-GNN models on all synthetic datasets and plot the mean test accuracy
with standard deviation on each dataset. From Figure 6 and Figure 7, we can see that on each HaMgg(G)
level, ACM-GNNs will not underperform baseline GNNs and the graph-agnostic models. But when
HaMgg(G) is small, baseline GNNs will be outperformed by graph-agnostic models by a large margin.
This demonstrate the advantage of the ACM framework. The curves will almost be the same for
ACMII framework.
G Discussion of the Limitations of Diversification Operation
X=Z
I： [1,0,0,0]
2: [1,0,0,0]
3: [1,0,0,0]
4： [0,1,0,0]
5： [0,1,0,0]
6: [0,0,1,0]
7： [0,0,1,0]
8： [0,0,0,1]
9： [0,0,0,1]
10: [0,0,0,1]
.50
.50
.25
-.50
-.25
.00
-.25
.50
.67
.50
-.50
-.17
-.17
-.50
0 - A)XXτI - Tτ
.25
.50
.50
-.25
.00
-.25
-.50
-.50
-.50
-.25
.50
.25
-.25 .00 -.25
-.17 -.17 -.50
.00 -.25 -.50
.25 1.00 .25
.50 .25 .00
.25 .13 .00
.17 .17 .11
.00 .13 .17
-.25 -.13 .00
-.50 -.38 -.17
.25 .17 .00
.13 .17 .13
.00 .11 .17
.00 .25 .50 .25 -.25 -.38 -.33
.25 ：00| .25 .50 .00 -.13 -.17
-.25 -.50 -.25 .00 .50 .38 .17
-.13 -.38 -.38 -.13
.00 -.17 -.33 -.17 .17 .25 .
Figure 8:	Example of the case (the area in black box) that HP filter does not work well for harmful
heterophily
From the black box area of S(I - A, X) in the example in Figure 8 we can see that nodes in class
1 and 4 assign non-negative weights to each other although there is no edge between them; nodes
in class 2 and 3 assign non-negative weights to each other as well. This is because the surrounding
differences of class 1 are similar as class 4, so are class 2 and 3. In real-world applications, when
25
Under review as a conference paper at ICLR 2022
nodes in several small clusters connect to a large cluster, the surrounding differences of the nodes in
the small clusters will become similar. In such case, HP filter are not able to distinguish the nodes
from different small clusters.
H The Similarity, HOMOPHILY and DDAX(G) METRICS and THEIR
Estimations	,
Additional Metrics There are three key factors that influence the performance of GNNs in real-
world tasks: labels, features and graph structure. The (modified) aggregation homophily tries to
investigate how the consistency of graph structure and labels will influence the performance of GNNs
with features being fixed. And their correlation is verified through the synthetic experiments.
Besides graph-label consistency, we need to consider feature-label consistency and aggregated-
feature-label consistency as well to fully investigate the performance of NNs and GNNs. With
aggregation similarity score of the features Sagg (S(I, X)) and aggregated features Sagg (S(A, X))
listed in table 8, our methods open up a new perspective on analyzing and comparing the performance
of graph-agnostic models and graph-aware models in real-world tasks. Here are 2 examples.
Example 1: People observe that GCN (graph-aware model) underperforms MLP-2 (graph-agnostic
model) on Cornell, Wisconsin, Texas, Film. Based on the aggregation homophily, the graph structure
is not the main cause of the performance degradation. And from Table 6 we can see that the
Sagg(S(A, X)) for the above 4 datasets are lower than their corresponding Sagg (S(I, X)), which
implies that it is the aggregated-feature-label inconsistency that causes the performance degradation.
Example 2: According to the widely used analysis based on node or edge homophily, the graph
structure of Chameleon, and Squirrel are heterophilous and bad for GNNs. But in practice, GCN
outperforms MLP-2 on those 2 datasets which means the additional graph information is helpful
for node classification instead of being harmful. Traditional homophily metrics fail to explain such
phenomenon but our method can give an explanation from different angles: For Chameleon, its
modified aggregation homophily is not low and its Sagg (S(A, X)) is higher than its Sagg (S(I, X))
which means its graph-label consistency and aggregated-feature-label consistency help the graph-
aware model obtain the performance gain; for Squirrel, its modified aggregation homophily is low but
its Sagg(S( A,X)) is higher than its Sagg (S(I,X)) which means although its graph-label consistency
is bad but the aggregated-feature-label consistency is the key factor to help the graph-aware model
perform better.
We also need to point out that (modified) aggregation similarity score, Sagg(S(A,X)) and
Sagg (S(I, X)) are not deciding or threshold values because they do not consider the nonlinear
structure in the features. In practice, a low score does not tell us the GNN models will definitely
perform bad.
	Cornell	Wisconsin	Texas	Film	Chameleon	Squirrel	Cora	CiteSeer	PubMed
	 Hagg(G) Sagg (S(A,X)) Sagg(S(I,X)) DDAX (G)	0.9016	0.8884	0.847	0.8411	0.805	0.6783	0.9952	0.9913	0.9716 0.8251	0.7769	0.6557	0.5118	0.8292	0.7216	0.9439	0.9393	0.8623 0.9672	0.8287	0.9672	0.5405	0.7931	0.701	0.9103	0.9315	0.8823 0.3497	0.6096	0.459	0.3279	0.3109	0.2711	0.2681	0.4124	0.1889
Hagg (G) Sagg (S(A,X)) Sagg (S(I,X)) ddA,x (G)	0.9046 ± 0.0282	0.9147	±	0.0260	0.8596	±	0.0299	0.8451 ± 0.0041	0.8041	±	0.0078	0.6788 ±	0.0077	0.9959	±	0.0011	0.9907	±	0.0015	0.9724 ± 0.0015 0.8266 ± 0.0526	0.8280	±	0.0351	0.6835	±	0.0498	0.5345 ± 0.0421	0.8433	±	0.0070	0.7352 ±	0.0132	0.9487	±	0.0023	0.9451	±	0.0038	0.8626 ± 0.0021 0.9752 ± 0.0174	0.8680	±	0.0270	0.9661	±	0.0336	0.5438 ± 0.0184	0.8257	±	0.0050	0.7472 ±	0.0089	0.9204	±	0.0044	0.9441	±	0.0036	0.8835 ± 0.0019 0.3936 ± 0.0663	0.6073	±	0.0436	0.4817	±	0.0762	0.3300 ± 0.0136	0.3329	±	0.0151	0.3021 ±	0.0101	0.3198	±	0.0225	0.4424	±	0.0136	0.1919 ± 0.0046
Table 8: Additional metrics and their estimations with only training labels (mean ± std)
Furthermore, in most real-world applications, not all labels are available to calculate the dataset
statistics. In this section, We randomly split the data into 60%/20%/20% for training/validation/test,
and only use the training labels for the estimation of the statistics. We repeat each estimation for 10
times and report the mean with standard deviation. The results are shown in table 8.
26
Under review as a conference paper at ICLR 2022
Analysis From the reported results we can see that the estimations are accurate and the errors are
within the acceptable range, which means the proposed metrics and similarity scores can be accurately
estimated with a subset of labels and this is important for real-world applications.
I Experiments on Fixed Splits Provided by (Pei et al., 2020)
See table 9 for the results and table 10 the optimal searched hyperparameters.
Cornell Wisconsin Texas
Film Chameleon Squirrel Cora CiteSeer PUbMed Rank
GPRGNN
H2GCN
FAGCN
Geom-GCN*
78.11 ± 6.55
82.70 ± 5.28
76.76 ± 5.87
60.54 ± 3.67
82.55 ± 6.23
87.65 ± 4.98
79.61 ± 1.58
64.51 ± 3.66
81.35 ± 5.32
84.86 ± 7.23
76.49 ± 2.87
66.76 ± 2.72
35.16 ± 0.9
35.70 ± 1.00
34.82 ± 1.35
31.59 ± 1.15
62.59 ± 2.04
60.11 ± 2.15
46.07 ± 2.11
60.00 ± 2.81
46.31 ± 2.46
36.48 ± 1.86
30.83 ± 0.69
38.15 ± 0.92
87.95 ± 1.18
87.87 ± 1.20
88.05 ± 1.57
85.35 ± 1.57
77.13 ± 1.67
77.11 ± 1.57
77.07 ± 2.05
78.02 ± 1.15
87.54 ± 0.38
89.49 ± 0.38
88.09 ± 1.38
89.95 ± 0.47
8.22
6.78
9.56
9.22
ACM-SGC-1 82.43 ± 5.44 86.47 ± 3.77 81.89 ± 4.53 35.49 ± 1.06 63.99 ± 1.66 45.00 ± 1.4 86.9 ± 1.38 76.73 ± 1.59 88.49 ± 0.51
ACM-SGC-2
ACM-GCN
ACM-Snowball-2
ACM-Snowball-3
ACMII-GCN
ACMII-Snowball-2
ACMII-Snowball-3
82.43 ± 5.44
85.14 ± 6.07
85.41 ± 5.43
83.24 ± 5.38
85.95 ± 5.64
85.68 ± 5.93
86.47 ± 3.77
88.43 ± 3.22
87.06 ± 2
86.67 ± 4.37
87.45 ± 3.74
87.45 ± 2.8
81.89 ± 4.53
87.84 ± 4.4
87.57 ± 4.86
87.84 ± 3.87
86.76 ± 4.75
86.76 ± 4.43
36.04 ± 0.83
36.28 ± 1.09
36.89 ± 1.18
36.82 ± 0.94
36.16 ± 1.11
36.55 ± 1.24
82.7 ± 4.86
59.21 ± 2.22
66.93 ± 1.85
67.08 ± 2.04
66.91 ± 1.73
66.91 ± 2.55
66.49 ± 1.75
40.02 ± 0.96
54.4 ± 1.88
52.5 ± 1.49
53.31 ± 1.88
51.85 ± 1.38
50.15 ± 1.4
87.69 ± 1.07
87.91 ± 0.95
87.42 ± 1.09
87.1 ± 0.93
88.01 ± 1.08
87.57 ± 0.86
85.29 ± 4.23 85.41 ± 6.42 36.49 ± 1.41 66.86 ± 1.74 48.87 ± 1.23 87.16 ± 1.01
76.59 ± 1.69
77.32 ± 1.7
76.41 ± 1.38
75.91 ± 1.57
77.15 ± 1.45
76.92 ± 1.45
76.18 ± 1.55
89.01 ± 0.6
90.00 ± 0.52
89.89 ± 0.57
89.81 ± 0.43
89.89 ± 0.43
89.84 ± 0.48
89.73 ± 0.52
8.44
8.22
2.33
4.11
5.22
3.22
4.67
7.00
Table 9: Experimental results on fixed splits provided by Pei et al. (2020): average test accuracy ±
standard deviation on 9 real-world benchmark datasets. The best results are highlighted. Results
of Geom-GCN, H2GCN and GPRGNN are from Pei et al. (2020); Zhu et al. (2020b); Lingam et al.
(2021); results on the rest models are run by ourselves and the hyperparameter searching range is the
same as table 5.
J A Detailed Explanation of the Differences Between ACM-GNNs,
ACM(II)-GNNS AND GPRGNN, FAGCN
Difference with GPRGNN (Chien et al., 2021): To explain how the channel mixing
mechanism makes ACM-GNNs and ACMII-GNNs different from the learning mecha-
KK
nism in GPRGNN, we first rewrite GPRGNN as Z = P γkH(k) = P γk I H(k) =
k=0	k=0
K
P diag(γk, γk, . . . , γk)H(k). The node-wise channel mixing mechanism in GPRGNN
k=0
K
form is Z = P diag(γk1 , γk2, . . . , γkN)H(k), where N is the number of nodes and
k=0
γki , i = 1, . . . , N are learnable parametric mixing weights for different channels. ACM and
ACMII allow GNNs to learn more diverse parameters in diagonal than GPRGNN and thus,
have stronger expressive power than GPRGNN.
• Difference with FAGCN (Bo et al., 2021): instead of using a fixed A, FAGCN learns a new
filter A0 based on A in a similar way as GAT (Velickovic et al., 2017). Some people may
take A0 as a mixing matrix and think it is similar to our channel mixing mechanism, but
A0 is essentially a learnable aggregator or LP filter for LP channel, which is far different
from channel mixing. And A0 can be decomposed into A0 = A01 - A02, where A01 and -A02
represents positive and negative edge (propagation) information respectively. In our paper,
we are not discussing the advantages of using the learned filter A0 over the fixed filter A,
we are comparing the models with and without channel mixing mechanism. We believe the
empirical results on real-world tasks in table 4 and table 9 is the best way to compare the
models with fixed filter and node-wise channel mixing and the models with learned filter but
without channel mixing
27
Under review as a conference paper at ICLR 2022
Datasets	ModelS∖Hyperparameters	lr	Weight_decay	dropout	hidden	reSultS	Std	average epoch time/average total time
	ACM-SGC-1	0.01	5.00E-06	0	64	82.43	5.44	5.37mS/23.05S
	ACM-SGC-2	0.01	5.00E-06	0	64	82.43	5.44	5.93mS/25.66S
	ACM-GCN	0.05	5.00E-04	0.5	64	85.14	6.07	8.04mS/1.67S
	ACMII-GCN	0.1	1.00E-04	0	64	85.95	5.64	7.83mS/2.66S
Cornell	FAGCN	0.01	1.00E-04	0.6	64	76.76	5.87	8.80mS/7.67S
	ACM-SnoWball-2	0.05	5.00E-03	0.3	64	85.41	5.43	11.50mS/2.35S
	ACM-SnoWball-3	0.05	5.00E-03	0.2	64	83.24	5.38	15.06mS/3.12S
	ACMn-SnoWball-2	0.1	5.00E-03	0.2	64	85.68	5.93	12.63mS/2.58S
	ACMn-SnoWball-3	0.05	5.00E-03	0.2	64	82.7	4.86	14.59mS/3.06S
	ACM-SGC-1	0.1	5.00E-06	0	64	86.47	3.77	5.07mS/14.07S
	ACM-SGC-2	0.1	5.00E-06	0	64	86.47	3.77	5.30mS/16.05S
	ACM-GCN	0.05	1.00E-03	0.4	64	88.43	3.22	8.04mS/1.66S
	ACMII-GCN	0.01	5.00E-05	0.1	64	87.45	3.74	8.40mS/2.19S
Wisconsin	FAGCN	0.01	5.00E-05	0.5	64	79.61	1.59	8.61mS/5.84S
	ACM-Snowball-2	0.01	1.00E-03	0.4	64	87.06	2	12.51mS/2.60S
	ACM-Snowball-3	0.01	1.00E-02	0.1	64	86.67	4.37	14.92mS/3.15S
	ACMn-SnoWball-2	0.01	5.00E-04	0.1	64	87.45	2.8	11.96mS/2.63S
	ACMn-SnoWball-3	0.01	5.00E-03	0.5	64	85.29	4.23	14.87mS/3.10S
	ACM-SGC-1	0.01	1.00E-05	0	64	81.89	4.53	5.34mS/19.00S
	ACM-SGC-2	0.05	1.00E-05	0	64	81.89	4.53	5.50mS/9.26S
	ACM-GCN	0.05	5.00E-04	0.5	64	87.84	4.4	9.62mS/1.99S
	ACMII-GCN	0.01	1.00E-03	0.1	64	86.76	4.75	9.98mS/2.22S
Texas	FAGCN	0.01	1.00E-05	0	64	76.49	2.87	10.45mS/5.70S
	ACM-Snowball-2	0.01	5.00E-03	0.2	64	87.57	4.86	11.56mS/2.45S
	ACM-Snowball-3	0.01	5.00E-03	0.2	64	87.84	3.87	15.17mS/3.15S
	ACMII-Snowball-2	0.01	1.00E-03	0.2	64	86.76	4.43	11.36mS/2.30
	ACMII-Snowball-3	0.01	5.00E-03	0.6	64	85.41	6.42	15.84mS/3.48S
	ACM-SGC-1	0.05	5.00E-04	0	64	35.49	1.06	5.39mS/1.17S
	ACM-SGC-2	0.05	5.00E-04	0.1	64	36.04	0.83	13.22mS/3.31S
	ACM-GCN	0.01	5.00E-03	0	64	36.28	1.09	8.96mS/1.82S
	ACMII-GCN	0.01	5.00E-03	0	64	36.16	1.11	9.06mS/1.83S
Film	FAGCN	0.01	5.00E-05	0.4	64	34.82	1.35	15.60mS/2.51S
	ACM-Snowball-2	0.01	1.00E-02	0	64	36.89	1.18	14.77mS/3.01S
	ACM-Snowball-3	0.01	1.00E-02	0.2	64	36.82	0.94	16.57mS/3.36S
	ACMII-Snowball-2	0.01	5.00E-03	0.1	64	36.55	1.24	12.76mS/2.57S
	ACMII-Snowball-3	0.05	5.00E-03	0.3	64	36.49	1.41	16.51mS/3.49S
	ACM-SGC-1	0.1	5.00E-06	0.9	64	63.99	1.66	5.92mS/1.74S
	ACM-SGC-2	0.1	0.00E+00	0.9	64	59.21	2.22	8.84mS/1.78S
	ACM-GCN	0.05	5.00E-05	0.7	64	66.93	1.85	8.40mS/1.71S
	ACMII-GCN	0.05	5.00E-06	0.8	64	66.91	2.55	8.90mS/2.10S
Chameleon	FAGCN	0.01	5.00E-05	0	64	46.07	2.11	16.90mS/7.94S
	ACM-Snowball-2	0.01	1.00E-04	0.7	64	67.08	2.04	12.50mS/2.69S
	ACM-Snowball-3	0.01	1.00E-05	0.8	64	66.91	1.73	16.12mS/4.91S
	ACMII-Snowball-2	0.01	5.00E-05	0.8	64	66.49	1.75	12.65mS/3.42S
	ACMII-Snowball-3	0.05	5.00E-05	0.7	64	66.86	1.74	17.60mS/4.06S
	ACM-SGC-1	0.05	5.00E-06	0.9	64	45	1.4	6.10mS/2.18S
	ACM-SGC-2	0.05	0.00E+00	0.9	64	40.02	0.96	35.75mS/9.62S
	ACM-GCN	0.05	5.00E-06	0.7	64	54.4	1.88	10.48mS/2.68S
	ACMII-GCN	0.05	5.00E-06	0.7	64	51.85	1.38	11.69mS/2.91S
Squirrel	FAGCN	0	5.00E-03	0	64	30.86	0.69	10.90mS/13.91S
	ACM-Snowball-2	0.01	1.00E-04	0.7	64	52.5	1.49	17.89mS/5.78S
	ACM-Snowball-3	0.01	5.00E-05	0.7	64	53.31	1.88	22.60mS/7.53S
	ACMII-Snowball-2	0.05	5.00E-05	0.6	64	50.15	1.4	16.95mS/3.45S
	ACMII-Snowball-3	0.01	5.00E-04	0.6	64	48.87	1.23	23.52mS/4.94S
	ACM-SGC-1	0.05	5.00E-05	0.7	64	86.9	1.38	4.99mS/2.40S
	ACM-SGC-2	0.1	0	0.8	64	87.69	1.07	5.16mS/1.16S
	ACM-GCN	0.01	5.00E-05	0.6	64	87.91	0.95	8.41mS/1.84S
	ACMII-GCN	0.01	1.00E-04	0.6	64	88.01	1.08	8.59mS/1.96S
Cora	FAGCN	0.02	1.00E-04	0.5	64	88.05	1.57	9.30mS/10.64S
	ACM-Snowball-2	0.01	1.00E-03	0.5	64	87.42	1.09	12.54mS/2.72S
	ACM-Snowball-3	0.01	5.00E-06	0.9	64	87.1	0.93	15.83mS/11.33S
	ACMII-Snowball-2	0.01	1.00E-03	0.6	64	87.57	0.86	12.06mS/2.64S
	ACMII-Snowball-3	0.01	5.00E-03	0.5	64	87.16	1.01	16.29mS/3.62S
	ACM-SGC-1	0.05	0.00E+00	0.7	64	76.73	1.59	5.24mS/1.14S
	ACM-SGC-2	0.1	0.00E+00	0.8	64	76.59	1.69	5.14mS/1.03S
	ACM-GCN	0.01	5.00E-06	0.3	64	77.32	1.7	8.89mS/1.79S
	ACMII-GCN	0.01	5.00E-05	0.5	64	77.15	1.45	8.95mS/1.80S
CiteSeer	FAGCN	0.02	5.00E-05	0.4	64	77.07	2.05	10.05mS/5.69S
	ACM-Snowball-2	0.01	5.00E-05	0	64	76.41	1.38	12.87mS/2.59S
	ACM-Snowball-3	0.01	5.00E-06	0.9	64	75.91	1.57	17.40mS/11.92S
	ACMII-Snowball-2	0.01	5.00E-03	0.5	64	76.92	1.45	13.10mS/2.94S
	ACMII-Snowball-3	0.1	5.00E-05	0.9	64	76.18	1.55	17.47mS/5.88S
	ACM-SGC-1	0.05	5.00E-06	0.4	64	88.49	0.51	5.77mS/3.65S
	ACM-SGC-2	0.05	5.00E-06	0.3	64	89.01	0.6	8.50mS/5.18S
	ACM-GCN	0.01	5.00E-05	0.4	64	90	0.52	8.99mS/2.51S
	ACMII-GCN	0.01	1.00E-04	0.3	64	89.89	0.43	9.70mS/2.57S
PubMed	FAGCN	0.01	1.00E-04	0	64	88.09	1.38	10.30mS/8.75S
	ACM-Snowball-2	0.01	1.00E-03	0.3	64	89.89	0.57	15.05mS/3.11S
	ACM-Snowball-3	0.01	5.00E-03	0.1	64	89.81	0.43	20.51mS/4.63S
	ACMII-Snowball-2	0.01	5.00E-04	0.4	64	89.84	0.48	15.10mS/3.2S
	ACMII-Snowball-3	0.01	1.00E-03	0.4	64	89.73	0.52	20.46mS/4.32S
Table 10: Optimal Hyperparameters for FAGCN and ACM-GNNs on fixed splits
28