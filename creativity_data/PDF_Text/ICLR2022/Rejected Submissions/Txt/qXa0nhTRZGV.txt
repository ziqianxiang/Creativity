Under review as a conference paper at ICLR 2022
Understanding Sharpness-Aware Minimization
Anonymous authors
Paper under double-blind review
Ab stract
Sharpness-Aware Minimization (SAM) is a recent training method that relies on
worst-case weight perturbations. SAM significantly improves generalization in
various settings, however, existing justifications for its success do not seem conclu-
sive. First, we analyze the implicit bias of SAM over diagonal linear networks, and
prove that it always chooses a solution that enjoys better generalisation properties
than standard gradient descent. We also provide a convergence proof of SAM for
non-convex objectives when used with stochastic gradients and empirically discuss
the convergence and generalization behavior of SAM for deep networks. Next, we
discuss why SAM can be helpful in the noisy label setting where we first show that
it can help to improve generalization even for linear classifiers. Then we discuss a
gradient reweighting interpretation of SAM and show a further beneficial effect of
combining SAM with a robust loss. Finally, we draw parallels between overfitting
observed in learning with noisy labels and in adversarial training where SAM also
improves generalization. This connection suggests that, more generally, techniques
from the noisy label literature can be useful to improve robust generalization.
1	Introduction
Understanding generalization of overparametrized deep neural networks is a central topic of the
current machine learning research. Their training objective has many global optima where the
training data are perfectly fitted (Zhang et al., 2016), but different global optima lead to dramatically
different generalization performance (Liu et al., 2019). However, it has been observed that stochastic
gradient descent (SGD) tends to converge to well-generalizing solutions, even without any explicit
regularization methods (Zhang et al., 2016). This suggests that the leading role is played by the
implicit bias of the optimization algorithms used (Neyshabur et al., 2014): when the training objective
is minimized using a particular algorithm and initialization method, it converges to a specific solution
with favorable generalization properties. However, even though SGD has a very beneficial implicit
bias, significant overfitting can still occur, particularly in the presence of label noise (Nakkiran et al.,
2019) and adversarial perturbations (Rice et al., 2020).
Recently it has been observed that the sharpness of the training loss, i.e., how quickly it changes
in some neighborhood around the parameters of the model, correlates well with the generalization
error (Keskar et al., 2016; Jiang et al., 2019), and generalization bounds related to the sharpness
have been derived (Dziugaite & Roy, 2018). The idea of minimizing the sharpness to improve
generalization has motivated recent works of Foret et al. (2021) and Wu et al. (2020) which propose
to use worst-case perturbations of the weights on every iteration of training in order to improve
generalization. We refer to this method as Sharpness-Aware Minimization (SAM) and focus mainly
on the version proposed in Foret et al. (2021) that performs only one step of gradient ascent to
approximately solve the weight perturbation problem before updating the weights.
Despite the fact that SAM significantly improves generalization in various settings, the existing
justifications based on the generalization bounds provided by Foret et al. (2021) and Wu et al. (2020)
do not seem conclusive. The main reason is that their generalization bounds do not distinguish the
worst-case robustness to worst-case weight perturbation from average-case robustness to Gaussian
noise. However the latter does not sufficiently improve generalization as both Foret et al. (2021) and
Wu et al. (2020) report. Moreover, their analysis does not distinguish whether the weight perturbation
is applied based on some or on all training examples which, as we will discuss, has an important
impact on generalization.
We aim at further investigating the reasons for SAM’s success and make the following contributions:
1
Under review as a conference paper at ICLR 2022
•	We first discuss that the SAM formulation can lead to two different objectives. We show that the
gradient flows on these objectives over diagonal linear networks are implicitly biased towards
solutions which enjoy better generalization properties compared to the standard gradient flow.
•	We provide convergence results for the SAM algorithm used with stochastic gradients for
non-convex objectives and discuss the convergence and generalization behavior of SAM for
deep networks.
•	We discuss why SAM can prevent overfitting in the noisy label setting by interpreting SAM as
a gradient reweighting scheme and showing the further benefits of combining it with a robust
loss.
•	Finally, we draw parallels between overfitting in learning with noisy labels and in adversarial
training where SAM also improves generalization. This connection suggests that techniques
from the noisy label literature can be more generally useful to improve robust generalization.
2	Related work
Here we discuss relevant works on robustness in the weight space and cover the main references
on overfitting in the noisy label setting and in adversarial training, two settings where weight-space
robustness also improves generalization.
Weight-space robustness. Works on weight-space robustness of neural networks date back at least
to 1990s (Murray & Edwards, 1993; Hochreiter & Schmidhuber, 1995). Random perturbations
of the weights are used extensively in deep learning (Jim et al., 1996; Graves et al., 2013), and
most prominently in approaches such as dropout (Srivastava et al., 2014). Many practitioners have
observed that using SGD with larger batches for training leads to worse generalization (LeCun
et al., 2012), and Keskar et al. (2016) have shown that this degradation of performance is correlated
with the sharpness of the found parameters. This observation has motivated many further works
which focus on closing the generalization gap between small-batch and large-batch SGD (Wen
et al., 2018; Haruki et al., 2019; Lin et al., 2020). More recently, Jiang et al. (2019) have shown
a strong correlation between the sharpness and the generalization error on a large set of models
under a variety of different settings hyperparameters, beyond the batch size. This has motivated
the idea of minimizing the sharpness during training to improve standard generalization, leading to
Sharpness-Aware Minimization (SAM) (Foret et al., 2021). SAM modifies SGD such that on every
iteration of training, the gradient is taken not at the current iterate but rather at a worst-case point
in its vicinity. Zheng et al. (2021) concurrently propose a very similar weight perturbation method
which also successfully improves standard generalization on multiple deep learning benchmarks. Wu
et al. (2020) have also proposed a similar algorithm with the same motivation, although Wu et al.
(2020) focuses on improving robust generalization of adversarial training.
Overfitting in learning with noisy labels. Arpit et al. (2017) have observed that deep networks
noticeably overfit in the presence of mislabeled samples but early stopping can mitigate the problem.
There are multiple other approaches that can mitigate this overfitting behavior: removing (Song et al.,
2020) or downweighting noisy points (Jiang et al., 2018; Huang et al., 2020), using robust losses
(Ghosh et al., 2017; Zhang & Sabuncu, 2018; Menon et al., 2019). Most of these approaches explicitly
or implicitly leverage the early-learning phenomenon (Liu et al., 2020) where the network tends to fit
first correctly labeled points and noisy points later in training. In the context of adversarial robustness,
Sanyal et al. (2020) have discussed that fitting label noise can aggravate adversarial vulnerability,
although removing label noise is not sufficient to achieve adversarial robustness.
Overfitting in adversarial training. Adversarial training in deep learning has been formulated
as a robust optimization problem by Madry et al. (2018) where the worst-case perturbations are
typically found via projected gradient descent. Rice et al. (2020) have described the robust overfitting
phenomenon in adversarial training suggesting that early stopping is highly beneficial and provides
a competitive baseline. However, they do not provide explanations about the reasons behind the
overfitting phenomenon. The state-of-the-art approaches propose different ways to improve robust
generalization and mitigate robust overfitting such as using additional training data (Carmon et al.,
2019), weight averaging (Gowal et al., 2020; Chen et al., 2021), or advanced data augmentations
(Rebuffi et al., 2021). Additionally, Wu et al. (2020) have shown that combining adversarial training
with a method similar to sharpness-aware minimization also improves robust generalization.
2
Under review as a conference paper at ICLR 2022
3	Implicit bias and convergence of SAM
In this section, we discuss different objectives related to Sharpness-Aware Minimization (SAM) and
their generalization properties, then derive convergence results for the stochastic SAM algorithm, and
further discuss the convergence and generalization of SAM for deep networks.
3.1	Implicit bias of SAM
SAM objectives. Let {xi, yi}in=1 be the training data points and `i (w) be the loss of a classifier
parametrized by weights w ∈ R|w| and evaluated at point (xi, yi). Foret et al. (2021) theoretically
base their approach on the following objective which we denote as MaxSum SAM:
1n
MaxSum SAM:	min max 一	'i(w + δ)
w∈Rlwl kδ∣∣2≤ρ n 2-*,
2 i=1
(1)
They justify this objective via a PAC-Bayesian generalization bound, although they show empir-
ically (see Fig. 3 therein) that the following objective, denoted as SumMax SAM, leads to better
generalization:
1n
SumMax SAM: min	max 'i(w + δ).
w∈Rlwl n 2-*, ∣∣δk2≤p
i=1	2
(2)
The update rule of SAM for these objectives amounts then to a variation of gradient descent
with step size γt where the gradients are taken at intermediate points Wti+1/2, i.e., Wt+1 =
Wt - n pn=ι V'i(wi+1∕2). The two objectives, however, differ in how the points wi+“ are
computed since they approximately maximize different losses with inner step sizes ρt :
n
i	ρt	i
MaxSum: wi+1/2 = Wt +-----V'j(Wt) vs. SumMax: wi+1/2 = Wt + PtV'i(wt).	(3)
n
j=1
We next show formally that SumMax SAM has a better implicit bias than ERM and MaxSum SAM.
Implicit bias of SumMax and MaxSum SAM. The implicit bias of gradient methods is well
understood for linear models where all gradient-based algorithms enjoy the same implicit bias. For
diagonal linear neural networks, first-order algorithms have a richer implicit bias. We consider here a
sparse regression problem and a predictor parametrized as fu,v = uu-vv. Before understanding
how SAM induces a preferable bias, we first recall the seminal result of Woodworth et al. (2020):
assuming global convergence, the solution selected by the gradient flow initialised at α ∈ Rd and
denoted β∞α solves the constrained optimisation problem:
β∞α = arg min	φα (β).
β∈Rd s.t. Xβ=y
(4)
The potential φα whose precise expression is given Eq. (12) in App. A interpolates nicely between the
`1 and the `2 norms according to the initialization scale α: Small initialisations lead to low `1 -type
solutions which are known to induce good generalisation properties. Large initialisations lead to low
`2 -type solutions. Our main result is that both MaxSum and SumMax dynamics bias the flow towards
solutions which still minimise the potential φα. However it does so with effective parameter αMaxSum
and αSumMax which are strictly smaller than α for suitable inner step size ρ. Thus the chosen solution
has better sparsity-inducing properties than the solution of the gradient flow.
Proposition 1 (Informal). Assuming global convergence, the solutions selected by the MaxSum and
the SumMax algorithms defined Eq. (3), taken in the infinitesimally-small-γt limit and initialised at α,
solve the optimisation problem (4) with effective parameters αSumMax and αSumMax which satisfy:
αSumMax = αe-ρ∆SumMax+O(ρ2) and αMaxSum = αe-ρ∆MaxSum+O(ρ2),
where ∆SumMax and ∆MaxSum are two entrywise positive vectors for which typically:
Il δSumMaxk 1 ≈ d [ L(W(S))ds and k δMaxSum k 1 ≈ — / L(W(S))ds.
0	n0
3
Under review as a conference paper at ICLR 2022
The results are formally stated in Proposition 4 and 5 in App. A. The SumMax implementations has
better bias properties since its effective scale of α is considerably smaller than the one of MaxSum. It
is worth noting that the vectors ∆SumMax and ∆MaxSum are linked with the integral of the loss function
along the flow. Thereby, the speed of convergence of the training loss impacts the magnitude of the
biasing effect: the slower the convergence, the better the bias, similarly to what observed for SGD
by Pesme et al. (2021).
Empirical evidence for the implicit bias. We compare the per-
formance of different methods (ERM, MaxSum and SumMax
SAM) on the training and test losses in Fig. 1. As predicted,
the methods enjoy various generalization abilities: ERM and
MaxSum SAM enjoy the same performance whereas SumMax
benefits from a better implicit bias. We also note that the training
loss of all the variants is converging to zero but as alluded before
the convergence of SumMax SAM is slower. We show a similar
experiments in App. A with stochastic variants of the algorithms.
As expected, their performances are better than their deterministic
counterparts (Keskar et al., 2016; Pesme et al., 2021).
The results on the implicit bias presented above require that the
algorithm converges to zero training error. Therefore we analyze
next the convergence of the SAM algorithm for general non-
convex functions in the practically relevant stochastic case. Note
that we cannot expect rates as fast as the one observed for the
previous simple model. They were due to its special structure for
which fast convergence rates have been proven (Yun et al., 2021).
Figure 1: Implicit bias of Sum-
Max SAM for a diagonal network
on a sparse regression problem.
3.2	Convergence of SAM algorithm
To make the SAM algorithm practical, Foret et al. (2021) propose
to combine SAM with stochastic gradients. By denoting the batch indices at time t by It, this leads
to the following update rule:
wt+1 = Wt - |I-|
i∈It
∑V'j (Wt)).
j∈It
(5)
Importantly, the same batch It is used for the inner and outer gradient steps which allows to interpolate
between the MaxSum (for |It| = n) and SumMax (for |It| = 1) update rules defined in Eq. (3).
However, using batch size |It | = 1 is inefficient in practice since it does not fully utilize the
computational accelerators such as GPUs so for most experiments, Foret et al. (2021) use a higher
|It | to balance the generalization improvement with the computational efficiency.
In the following, we analyze the convergence of the update rule from Eq. (5). We make the following
assumptions on the training loss L(W) = n PNi 'i(w):
(A1) (Bounded variance). It exists σ ≥ 0 s.t. E[kV'i(w) -VL(w)k2] ≤ σ2,i 〜U(J1,nK),w ∈ Rd.
(A2) (Individual β-smoothness). It exists β > 0 s.t. kV'i(w) 一 V'i(v)k ≤ β∣∣w 一 Vk for all
W, v ∈ Rd and i ∈ J1, nK.
(A3) (Polyak-Lojasiewicz). It exists μ > 0 s.t. 11 ∣∣VL(w)k2 ≥ μ(L(w) — L*) for all w,v ∈ Rd.
We have the following convergence result for the stochastic SAM algorithm (see proof in App B.2).
Proposition 2. Assume (A1-2) for the iterates (5). For any T ≥ 0 and for step-sizes Yt = √=∙^ and
Pt = τι∕4β, we have:
1	T-1	4	8σ2
T E [X kVL(Wt)k ] ≤ β√T(L(WO) - Lj + b√T,
In addition, under (A3), with step-sizes Yt = min{ 3,；+4产,1β } and Pt = γ∕∕t∕β:
E[L(wt)] — L* ≤ 3β2(L(WO)-L*)+ 22βσ2
[ (T)]	* ≤	μ2T2	+ μ2bT
4
Under review as a conference paper at ICLR 2022
We make several remarks regarding this theorem:
•	We recover the rates of SGD with the usual condition on the step size γt (Ghadimi & Lan,
2013; Karimi et al., 2016). The ascent step-size Pt has to be O(√Yt) to ensure convergence.
Therefore it tolerates a slower decrease than γt . This finding is aligned with the fact that the
ascent step-size of SAM should not be decreased as drastically as the descent step-size when
training neural networks.
•	The proof relies on the bound BL(Wt + ηVL(w.), VL(wt)i ≥ (1 - ηβ)kVL(wt)k2 which
shows that SAM-step is well aligned with the gradient step (seem Lemma 16 in App B.2).
•	We can obtain the same convergence result for the different variants of SAM described in the
previous section. We also provide in App B.1 tighter convergence rates for the full-batch case.
SAM with normalized gradient. When SAM algorithm was first introduced, the gradient was
normalized in the inner step, in order to have an ascent-step of size exactly P. This normalization is
not needed to get similar performance in applications and is not favorable for optimization purposes.
Indeed when used with constant step-size normalized SAM is oscillating, and the averaged iterate
converges to a different point which is not necessary flatter than the solution. When decreasing
step-sizes are used, then the behaviors are comparable to the ones in the unnormalized case. In
App. B.3, we show experiments that suggest that SAM with and without normalization steps achieves
similar improvements in terms of generalization. However, for all other experiments in the paper, we
rely on the normalized SAM as introduced in Foret et al. (2021) which is the main focus of our study.
3.3	Convergence and generalization of SAM for deep networks
Here we relate the theoretical results from the previous section with the behavior of SAM for deep
networks on a standard image classification dataset.
ERM and SAM both converge but generalize differently.
We compare the behavior of ERM and SAM by training a
ResNet-18 on CIFAR-10 for 1000 epochs using mini-batch
SGD with momentum and piece-wise constant learning rates
(see App. C for experimental details) and plot the results over
epochs in Fig. 2. We observe that not only the ERM model
but also the model trained with SAM fits all the training
points and converges to a nearly zero training loss: 0.0012
for ERM vs 0.0009 for SAM. However, the SAM model
has significantly better generalization performance: 3.76%
vs 5.03% test error. Moreover, we observe no noticeable
overfitting on standard CIFAR-10: the best and last model
differ at most by 0.1% test error for both methods.
When is implicit bias of SAM beneficial during train-
ing? To better understand the implicit bias of SAM over
training iterations, we perform the following experiment:
we train with ERM for 900 epochs and then use SAM for
the remaining 100 epochs (ERM → SAM), and vice versa
(SAM → ERM). We can see from Fig. 2 (right) that for
SAM → ERM once SAM converges to a well-generalizing
minimum thanks to its implicit bias, then it is not important
whether we continue optimization with SAM or with ERM.
0	200	400	6∞	8∞	1000
Epoch
Epoch
Figure 2: ERM vs SAM training over
epochs. Top: standard SAM training.
Bottom: SAM training enabled for the
first 900 epochs and last 100 epochs.
In particular, we do not observe overfitting when switching to ERM. We note that it is still important
here that SAM converges since SAM has to minimize the training loss below some threshold after
which ERM can be potentially applied.
At the same time, for ERM → SAM we observe a different behavior: the test error clearly improves
when switching from ERM to SAM. This suggests that SAM (using a higher P than the standard value,
see App. C) can help to escape a suboptimal minimum where ERM converges to. This phenomenon
is interesting since it suggests that such a fine-tuning scheme can save computations as we can
potentially start from any pre-trained model and improve its generalization on the same dataset which
is different from fine-tuning with SAM in the transfer learning setup covered in Foret et al. (2021).
5
Under review as a conference paper at ICLR 2022
EPoCh
Epoch
(a) Overfitting with noisy labels
(b) Alignment with ideal 7LstrUe(W)
Figure 3: Plots over training for a ResNet-18 trained on CIFAR-10 with 60% label noise. We can
observe that (a) test error increases when we fit the noisy samples, (b) using gradients at the SAM
point doesn’t change the alignment with the ideal direction, (c) using gradients at the SAM point
decreases the alignment with the noisy part.
∞S<Vno⅛y( w), Vβff(w)), ERM model
Cθ5(Vno∕fy( w), V⅛j(w + £sam)), ERM model
(C) Alignment with noisy ^Lsnoisy (W)
400	6∞ β∞ 1000
Epoch
4	Why can SAM help against overfitting of noisy labels?
In this section, we discuss another important aspect of SAM: its beneficial effect for the noisy
label setting. We first analyze the development of the gradient direction over training and then
interpret SAM as a gradient reweighting schemes where the gradients are reweighted according to
the sharpness of the loss.
Overfitting on noisy labels and the effect of SAM. By the noisy label setting we refer to a situation
where some fraction of the training labels is changed to uniformly random labels and kept fixed
throughout the training.1 This setting is challenging since standard training with ERM leads to
overfitting as illustrated in Fig. 3a. In particular, it is commonly observed (Liu et al., 2020) that at
the beginning of the training ERM fits the correctly labeled training points but, after some point,
starts to also fit the incorrectly labeled points, which coincides with overfitting in terms of the test
error. Thereby, the noisy label setting is different from the standard setting from Sec. 3.3: training
to zero loss is harmful and leads to overfitting, and thus early stopping is needed (Li et al., 2020b).
Fig. 3a illustrates that early stopping indeed leads to a significantly better test error for the ERM
model compared to the model taken at the last epoch. At the same time, SAM noticeably improves
generalization over ERM, although later in training SAM also starts to fit the noisy points and thus
also requires early stopping, either explicitly via a (potentially noisy) validation set or implicitly via
restricting the number of training epochs as in Foret et al. (2021).
Benefits of following the gradient direction given by SAM. Consider a batch of points Sall :=
{(x, y)}in=1 where some labels y are noisy. We define Sclean and Snoisy to be the subsets of Sall
with clean and noisy labels respectively. We also define Strue to be the same set of points as Sclean
but where all the labels are correct. We denote by LS(W) = ∣S∣ P(X y)∈s 'x,y (W) the loss on a set
of points S where w denotes the model parameters. The gradient update then can be decomposed as:
VLS “(w)= lSCleanlVLsi	(w)+ lSnoisy| VLS . (w).
all	clean	noisy
Here we want to analyze the contribution of each part of the gradient for the ERM and SAM weight
updates. For this, in Fig. 3 (b) and (c), we compare the following directions over training epochs
of an ERM model: VLSall (W) vs. VLStrue (W) and VLSall (W) vs. VLSnoisy (W) for the current
point W (i.e., the ERM update) and the point W + SAM (i.e., the SAM update). To be robust to the
label noise, we would like the update direction of an algorithm to be maximally aligned with the
ideal direction VLStrue (W) and minimally aligned with the noisy direction VLSnoisy (W). The key
observation of Fig. 3b and Fig. 3c is that SAM is changing the direction of the gradient by making it
less aligned with the noisy direction at the later stage of training when the model starts to fit the label
noise. Therefore, the SAM update is less dominated by the noisy labels and can resist overfitting
longer. To better understand this effect, we discuss next the behavior of SAM when training a linear
classifier in a binary classification setting.
SAM improves generalization even for linear models. Since using SumMax SAM (Eq. (2)) leads
to better generalization than MaxSum SAM, we consider this technique for an overparametrized
1Thus, it is different from the setting where new label noise is added on each iteration (HaoChen et al., 2020).
6
Under review as a conference paper at ICLR 2022
linear model fx(w) = hw, xi trained with the cross-entropy loss ` using gradient descent. The
training objective of SumMax SAM then becomes:
1n	1n
min — T max ' 3i hw + δ,xii) = min — '' (yi hw,xii - PIlxiI。).
w∈Rd n	kδk2≤ρ	w∈Rd n	2
(6)
We consider the following simple dataset: x 〜N(yμ, σ2Id),
where y ∈ {-1,1} is the label, and μ is some unit vector.
We use σ = 0.1, input dimension d = 100, n = 50 train-
ing samples, and we randomize 90% of the training labels.
We select the best step sizes for ERM and SAM via a grid
search. We can see from Fig. 4 that SAM with early stopping
consistently (over ten random seeds) improves the test error
over ERM. This illustrates that benefits of SAM extend be-
yond non-convex learning tasks with multiple minima and
suggests that the linear setting can be a good starting point
for understanding the benefit of SAM.
50%
40%
10%
0%
30%
g
由20%
Iteration
Figure 4: SAM for a linear model on
two Gaussians with 90% label noise.
SAM as a gradient reweighting scheme. For a general non-linear binary classifier fx(w), on every
iteration of gradient descent, the update direction is given by the gradient on all points:
1n
VLSall (W)= — E''(yi ∙ fXi (W))Nfxi (W).	⑺
n i=1S{z}S{z}
loss derivative direction
When we perturb the weights with SAM, we change in general both the loss derivative '(y ∙ fx(w))
and the direction given by the classifier’s gradient Vfx(W). However, for a linear classifier fx(W),
only ' (y ∙ fx(w)) is changed while the direction Vfx(w) = x is independent of the weights. The fact
that SAM helps even in this simple setting suggests that its crucial effect can lie only in modifying
the loss derivative '0(y ∙ fx(w)) which leads to beneficial reweighting of the per-example gradients of
noisy and clean inputs. We note that gradient reweighting is also the idea behind robust losses (Ghosh
et al., 2017; Zhang & Sabuncu, 2018) as they also affect only the loss derivative while keeping the
direction Vfxi (W) the same. This suggests the following interpretation of SumMax SAM:
Gradient reweighting interpretation of SAM: SumMax SAM reweights per-example gradi-
ents amplifying the derivative of clean points more than the derivative of noisy points so that the
overall gradient direction VLSall (w) is more aligned with VLSClean (w).
We observe from Eq. (6) that SAM offsets the margin yi hW, xii of each example by ρ IxiI2 More
generally, using SumMax SAM for non-linear models guarantees that the offset is non-negative for
each example as maximization of a monotonic loss ` implies minimization of the margin. Note that
this would not be the case for MaxSum SAM or average-case perturbations of the weights. We
discuss next why offsetting the loss can be valuable to reweight the gradients of different examples.
We first note that the early-learning phenomenon which is proven to hold under some assumptions in
Liu et al. (2020) implies that the correctly labeled points are fitted first during training so that at some
early stage they tend to have a higher margin than the noisy points. We illustrate this in Fig. 5 for a
linear model. We then consider the following two losses: cross-entropy loss (CE) and generalized
cross-entropy loss (GCE) (Zhang & Sabuncu, 2018) (we hide the dependency of f on W for brevity)
'ce(y ∙ fx) = log(i + eχp(-y ∙ fx)),	'gce(y ∙ fx) = 1/q(1 - (1 + eχp(-y ∙ fx))-q).
We plot these losses in Fig. 6a (using q = 0.7 for GCE following Zhang & Sabuncu (2018)) together
with the derivative change from adding an offset c in Fig. 6b and 6c. We observe based on Fig. 6b
that the offset affects more significantly the points with small positive margins (i.e., mostly correctly
labeled points due to the early-learning phenomenon) whose derivative is amplified more making it
closer to its minimal value of -1. Fig. 6c suggests that GCE has a better effect than CE since GCE
does not only decrease the derivative of points with small positive margin but it also increases the
derivative of the points with small negative margin (i.e., mostly noisy points) making it closer to zero.
This suggests that combining GCE with SAM can lead to further benefits which is indeed confirmed
experimentally in App. D.
7
Under review as a conference paper at ICLR 2022
30
25
&20
Φ
Γ
u-w
5
-OA -02	0.0
Margin value
Noisy labels
Correct labels
(a) Start of training
Ou20
ΛOU0n balu-
Noisy labels
Correct labels
25201510
ΛOU0n balu.
(b) Early in training
Figure 5: An illustration of the early-learning phenomenon (Liu et al., 2020) on the training set for a
linear model. Early in training the model achieves a good separation between noisy and correctly
labeled points which disappears towards the end of training.
(c) End of training
(a) CE and GCE losses
Figure 6: Introducing a loss offset c mainly affects points with small positive (for CE and GCE) and
small negative margin (for GCE) which mostly correspond to clean and noisy points, respectively,
due to the early-learning phenomenon.
(b) Derivative change for CE
-8-4	-2	0	2	4 β
Marginyflx)
(c) Derivative change for GCE
Table 1: An ablation study for different variations of SAM on a two-class CIFAR-10 dataset. The
error rates are averaged over three random seeds and reported with the standard deviation
NAME	GRADIENT FORMULA LSall (w)	ERROR RATE * 5
ERM	1	pn=1 '0(yi∙ fxi (w))Vfxi (w)	13.47% ± 0.80
SAM DIRECTION	1 pn=1 '0(yi∙ fxi (w))Vfχi (wsam)	11.30% ± 0.68
SAM DERIVATIVE	1 £乙 '(yi ∙ fχ<wsAM))Vfxi(W)	7.45% ± 0.28
SAM	1	pn=1 '0(yi∙ fxi(wsAM))Vfxi(WSAM)	9.02% ± 0.34
Does the loss derivative change explain the behavior of a non-linear model? To further support
our gradient reweighting interpretation for non-linear models, we show that changing only the loss
derivative '0(yi ∙ fxJwsAM)) (See Eq. (7)) and discarding the change of the direction leads to the
main benefit in SAM. For this, we train a ResNet-18 on a two-class CIFAR-10 under 80% label
noise with early stopping and study different ways to modify the gradient RLS (W) used on each
iteration of training. We report results in Table 1 where we observe that SAM direction leads
only to a small improvement compared to ERM (13.47% to 11.30%) while SAM DERIVATIVE is
helping most leading to 7.45% test error. This gives more evidence that the main improvement from
using SAM comes from changing the derivative of the loss '0(yi ∙ fxi (WSAM)) and not the direction
Rfxi (wSAM) which is coherent with our gradient reweighting interpretation of SAM. We further
report extra experimental details and the plots over epochs in App. D.
5 Why can SAM help against robust overfitting?
Here we draw parallels between the overfitting observed in the noisy label setting and in robust
training (Rice et al., 2020) where SAM also improves generalization (Wu et al., 2020).
The effect of SAM is similar for robust overfitting. In Fig. 7, we plot the error under adversarial
perturbations obtained via the PGD attack (We use 10 iterations, see details in App. D) for an '∞
adversarially trained model (Madry et al., 2018) with = 8/255 trained with ERM and SAM for 1000
8
Under review as a conference paper at ICLR 2022
epochs. For this experiment, we use the SAM algorithm as in Foret et al. (2021) which also leads to
similar improvements of robust error as the method introduced in Wu et al. (2020) which relies on a
different scaling of the layerwise perturbation bounds. We observe that, when trained for sufficiently
long (contrary to Wu et al. (2020) that only train for 200 epochs), SAM also leads to overfitting,
showing high similarity to the noisy label setting (Fig. 3a).
Implicit label noise in adversarial training. Related
to the point above, a recent work of Dong et al. (2021) ar-
gues about the presence of inputs which are “noisy” for
adversarial training, and whose removal mitigates robust
overfitting. They further discuss that such inputs become
ambiguous for a human observer after adding '∞ adver-
sarial perturbations. The problem stems from the for-
mulation of adversarial examples which is widely used:
maxkδkg≤e '(x + δ) instead of the label-preserving for-
mulation max∣∣δk ≤ (X)=(X+δ)'] '(x + δ) which is ob-
k k∞ ≤, y(x)=y(x+ )
viously impractical since we cannot query the true label
y(x + δ) at an arbitrary point during training. Thus, the
commonly used adversarial perturbations can change
the effective label y(x + δ) leading to implicit label
noise. This can be confirmed by implementing label-
preserving adversarial training for a linear model where
we know the ground truth model. We can use the same
two Gaussian example as in Fig. 4 but without explicit
label noise and perform adversarial training in the sub-
SPace orthogonal to the discriminative direction μ which
ensures that the label of x + δ is not affected. We show
exPerimentally in Fig. 8 that (1) robust overfitting can be
observed also in simPle linear models and (2) using the
label-Preserving formulation of adversarial training Pre-
vents it. This suggests that robust overfitting is strongly
connected to the labels of adversarial examPles.
Mitigating implicit label noise by SAM and other
0	200	400	600	800	1000
Epoch
Figure 7: The effect of SAM on robust
overfitting in adversarial training is similar
to its effect for noisy labels (cf. Fig. 3a).
0.40
J 0.35
R
α)
■g 0.30
? 0.25
E
E2
> 0.20
<
0.15
Figure 8: Robust overfitting for a linear
model is not observed for label-Preserving
adversarial training.
methods. Performing label-Preserving adversarial training requires the knowledge of the ground truth
labeling function which is imPractical. Thus, in Practice one has to rely on algorithms that Prevent
fitting the imPlicit label noise in other ways. We hyPothesize that an early-learning Phenomenon
similar to the one observed in training with noisy labels (Liu et al., 2020) should also hold for
adversarial training, and our gradient reweighting exPlanation of SAM from Sec. 4 should also extend
to this setting. We also note that other solutions against robust overfitting that have been recently
ProPosed in the literature—such as early stoPPing (Rice et al., 2020), weight averaging (Gowal et al.,
2020; Rebuffi et al., 2021), bootstraP (Chen et al., 2021), data removal (Dong et al., 2021)—also rely
on the early-learning Phenomenon and have been Previously exPlored in the noisy label literature. We
confirm this further on GCE (Zhang & Sabuncu, 2018) and semi-suPervised Pairing terms (Luo et al.,
2019) in APP. D. This further suggests that both settings are closely related, and imProvements in
one setting can also translate to the other one which can exPlain the imProvement from SAM against
robust overfitting.
6 Conclusions and outlook
We have discussed two different objectives motivated by SAM and analyzed their imPlicit bias for a
diagonal linear network. Then we have Provided a convergence Proof for the variant of SAM used in
Practice and confirmed emPirically that the best generalization Performance is achieved at a solution
with nearly zero training loss on a standard image classification tasks. At the same time, in the noisy
label setting, it is harmful to achieve low training loss even with SAM since this would involve fitting
the noisy labels. We have discussed why SAM can be helPful in this case: we observed that SAM
amPlifies the contribution of the correctly labeled Points comPared to the noisy ones in the gradient
uPdate. Finally, we argue that the overfitting observed in noisy label setting shares a lot of similarities
with overfitting observed in adversarial training which can exPlain why SAM is also helPful there.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement
Theoretical results. We provide proofs of our theoretical results in the Appendix in Sec. A for the
implicit bias and in Sec. B for the convergence of different variants of SAM. The assumptions A1-A3
for the convergence proofs are mentioned at the beginning of Sec. 3.2. An additional assumption A2’
is used only for the statements in the appendix and thus is introduced only there.
Empirical results. To facilitate reproducibility of our experiments, in Sec. C we specify all the
hyperparameters of our experiments on deep networks and linear models. Moreover, in Sec. D, we
provide more details about some of the experiments from the main part.
References
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at
memorization in deep networks. arXiv Preprint arXiv:1706.05394, 2017.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel.
Mixmatch: A holistic approach to semi-supervised learning. arXiv PrePrint arXiv:1905.02249,
2019.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras,
Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness.
arXiv PrePrint arXiv:1902.06705, 2019.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In NeUrIPS, 2019.
Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting
may be mitigated by properly learned smoothening. In InternatiOnal COnference on Learning
RePreSentations, volume 1, 2021.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. ICML, 2020.
Chengyu Dong, Liyuan Liu, and Jingbo Shang. Data profiling for adversarial training: On the ruin of
problematic data. arXiv PrePrint arXiv:2102.07437, 2021.
Gintare Karolina Dziugaite and Daniel Roy. Entropy-sgd optimizes the prior of a pac-bayes bound:
Generalization properties of entropy-sgd and data-dependent priors. In InternatiOnal COnferenCe
on MaChine Learning, pp. 1377-1386. PMLR, 2018.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. In International COnferenCe on Learning RePreSentations,
2021. URL https://openreview.net/forum?id=6Tm1mposlrM.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex
stochastic programming. SIAM JOUrnal on Optimization, 23(4):2341-2368, 2013.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural networks. AAAL 2017.
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering
the limits of adversarial training against norm-bounded adversarial examples. arXiv PrePrint
arXiv:2010.03593, 2020.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtarik. SGD: General analysis and improved rates. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), PrOCeedingS of the 36th International COnferenCe on MaChine Learning,
volume 97 of PrOCeedingS of MaChine Learning ReSearch, pp. 5200-5209. PMLR, 09-15 Jun
2019. URL http://proceedings.mlr.press/v97/qian19b.html.
10
Under review as a conference paper at ICLR 2022
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep re-
current neural networks. In 2013 IEEE international COnferenCe on acoustics, SpeeCh and signal
processing, pp. 6645-6649. Ieee, 2013.
Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit
bias of the noise covariance. arXiv PrePrint arXiv:2006.08680, 2020.
Kosuke Haruki, Taiji Suzuki, Yohei Hamakawa, Takeshi Toda, Ryuji Sakai, Masahiro Ozawa, and
Mitsuhiro Kimura. Gradient noise convolution (gnc): Smoothing loss function for distributed
large-batch sgd. arXiv PrePrint arXiv:1906.10822, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. ECCV, 2016.
Sepp Hochreiter and JUrgen Schmidhuber. Simplifying neural nets by discovering flat minima. In
AdvanCeS in neural information PrOCeSSing systems, pp. 529-536, 1995.
Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk
minimization. arXiv PrePrint arXiv:2002.10319, 2020.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Av-
eraging weights leads to wider optima and better generalization. arXiv PrePrint arXiv:1803.05407,
2018.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International COnferenCe
on MaChine Learning, pp. 2304-2313. PMLR, 2018.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. arXiv PrePrint arXiv:1912.02178, 2019.
Kam-Chuen Jim, C Lee Giles, and Bill G Horne. An analysis of noise in recurrent neural networks:
convergence and generalization. IEEE TranSaCtiOnS on neural networks, 7(6):1424-1438, 1996.
H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods
under the polyak-Lojasiewicz condition. In MaChine Learning and Knowledge DiSCOVery in
Databases, pp. 795-811, 2016.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
PrePrint arXiv:1609.04836, 2016.
GM Korpelevich. Extragradient method for finding saddle points and other problems. Matekon, 13
(4):3549,1977.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
TeChniCaI Report, 2009.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv PrePrint
arXiv:1610.02242, 2016.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
NeUraI networks: TriCkS of the trade, pp. 9T8. Springer, 2012.
Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-
supervised learning. arXiv PrePrint arXiv:2002.07394, 2020a.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International COnferenCe
on ArtifiCiaI Intelligence and StatiStics, pp. 4313Y324. PMLR, 2020b.
Tao Lin, Lingjing Kong, Sebastian Stich, and Martin Jaggi. Extrapolation for large-batch training in
deep learning. In International COnferenCe on MaChine Learning, pp. 6094-6104. PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. AdVances in NeUraI InfOrmatiOn Processing
Systems, 33, 2020.
Shengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas. Bad global minima exist and sgd
can reach them. arXiv PrePrint arXiv:1906.02613, 2019.
Yucen Luo, Jun Zhu, and Tomas Pfister. A simple yet effective baseline for robust deep learning with
noisy labels. arXiv PrePrint arXiv:1909.09338, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris TsiPras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ICLR, 2018.
Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient
clipping mitigate label noise? In InternationaI ConferenCe on Learning RePreSentations, 2019.
Alan F Murray and Peter J Edwards. SynaPtic weight noise during mlP learning enhances fault-
tolerance, generalization and learning trajectory. AdVanCeS in neural information ProCeSSing
systems, pp. 491-491, 1993.
Preetum Nakkiran, Gal KaPlun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. DeeP
double descent: Where bigger models and more data hurt. arXiv PrePrint arXiv:1912.02292, 2019.
Y. Nesterov. IntrodUCtory LeCtUreS on ConVex Optimization, volume 87 of APPIied Optimization.
Kluwer Academic, 2004. A basic course.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv PrePrint arXiv:1412.6614, 2014.
Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. ImPlicit bias of sgd for diagonal
linear networks: a provable benefit of Stochasticity. In AdVanCeS in NeUraI Information ProCeSSing
SyStems, 2021.
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann. Fixing data augmentation to improve adversarial robustness. arXiv PrePrint
arXiv:2103.01946, 2021.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv PrePrint
arXiv:1412.6596, 2014.
Leslie Rice, Eric Wong, and J Zico Kolter. Overfitting in adversarially robust deep learning. In ICML,
2020.
Amartya Sanyal, Puneet K Dokania, Varun Kanade, and Philip HS Torr. How benign is benign
overfitting? arXiv PrePrint arXiv:2007.04028, 2020.
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex
Kurakin, Han Zhang, and Colin Raffel. Fixmatch: SimPlifying semi-suPervised learning with
consistency and confidence. arXiv PrePrint arXiv:2001.07685, 2020.
Jiaming Song, Lunjia Hu, Yann DauPhin, Michael Auli, and Tengyu Ma. Robust and on-the-fly
dataset denoising for image classification. arXiv PrePrint arXiv:2003.10647, 2020.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
Iearning research, 15(1):1929-1958, 2014.
Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, GoPinath ChennuPati, and Jamal Mohd-
Yusof. Combating label noise in deep learning using abstention. arXiv PrePrint arXiv:1905.10964,
2019.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
entropy for robust learning with noisy labels. In ProCeedingS of the IEEE/CVF InternationaI
ConferenCe on ComPUter Vision, pp. 322-330, 2019.
12
Under review as a conference paper at ICLR 2022
Wei Wen, Yandan Wang, Feng Yan, Cong Xu, Chunpeng Wu, Yiran Chen, and Hai Li. Smoothout:
Smoothing out sharp minima to improve generalization in deep learning. arXiv PrePrint
arXiv:1805.07898, 2018.
Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
ProceedingS of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of
Machine Learning ReSearch, pp. 3635-3673. PMLR, 09-12 JUl 2020.
Dongxian Wu, Shu-tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. NeUrIPS, 2020.
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training
linear neural networks. In International COnference on Learning RePreSentations, 2021.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv PrePrint arXiv:1611.03530, 2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv PrePrint arXiv:1710.09412, 2017.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. AdVanceS in neural information PrOceSSing systems, 31:8778-8788, 2018.
Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial
model perturbation. CVPR, 2021.
13
Under review as a conference paper at ICLR 2022
Appendix
Organization of the appendix
The appendix contains proofs related to sharpness and convergence of different variants of SAM that
we omitted from the main part of the paper due to the space constraints. Moreover, the appendix
also contains additional implementation details and extended experimental results. The appendix is
organized as follows:
•	Sec. A: proofs related to the implicit bias of SumMax and MaxSum SAM.
•	Sec. B: proofs related to the convergence of different variants of SAM.
•	Sec. C: experimental details for the epxeriments with deep networks and linear models.
•	Sec. D: additional experiments about the importance of the change in the loss derivative
for deep networks, improvement of standard generalization under label noise by adversarial
training, and the experiment with the GCE loss and semi-supervised pairing term that mitigate
robust overfitting.
A	Theoretical analysis of the implicit bias
To understand why SAM is generalizing better than ERM we consider the simpler problem of
noiseless regression with 2-layer diagonal linear network for which we can precisely characterize the
implicit bias of the optimizations algorithms in hand.
Overparametrised noiseless regression. We consider linear regression with inputs, outputs
(x1, yi, . . . , xn, yn) ∈ (Rd+1)n. We consider the overparametrised case and assume that there
exists at least one interpolating parameter β* ∈ Rd which fits the training set: yi = hβ*,Xii for all
1 ≤ i ≤ n. We study the quadratic loss for which the training loss is written as:
nn
L(β) ：=4-X(hβ,χii-yi)2 = 4- Xhe -β*,χii2.
4n i=1	4n i=1
2-layer diagonal linear network. The simplest parametrisation of β is to consider β = w which
corresponds to the classical least-squares framework. In this case, first order methods (GD, with and
without momentum) converge towards the same solution: they have the same implicit bias. The same
holds for the variants of SAM we consider and the linear case is not helping us to distinguish them and
to show their primacy on ERM. To theoretically resolve this question, we study a simple non-linear
parametrisation: the quadratic parametrisation β = w+2 - w-2 with w = [w+, w-]> ∈ R2d. The
training loss in w is therefore written:
1n	1n
L(w) := — ɪ2(hw+ - w-,Xii- yi)2 "E(hβ,Xii- yi)2 with β = w+ - w-.	(8)
4- i=1	4- i=1
This parametrisation can be viewed as a simplified linear network of depth 2. Additionally to being a
toy neural model, this model has received recently a lot of attention since it is possible to precisely
characterize the implicit bias of descent algorithm on it.
Optimization algorithms. We consider minimizing the loss L(w) using three different algorithms:
•	Gradient descent with infinitesimally small stepsize (in the gradient flow limit):
Wt = -VL(wt).	(9)
•	The MaxSum SAM algorithm with infinitesimally small outer-stepsize and inner step size
ρ≥0:
W t = -VL(Wt + PVL(Wty).	(10)
•	The SumMax SAM with infinitesimally small outer-stepsize and inner step size ρ ≥ 0:
1n
Wt = ―― V' V'i(wt + ρV'i(wt))	(11)
-
i=1
14
Under review as a conference paper at ICLR 2022
Implicit bias of the Gradient flow. Let us define the function φα for α ∈ Rd as
d	ZZ	______
φα(β) =	α2q(βi∕a2) where q(z) = / arcsinh(u∕2)du = 2 — v 4 + z2 + Z arcsinh(z∕2).
i=1	0
(12)
Note that if we have α∈ R then we consider φα = φα1 . We will make great uses of this function to
precisely characterize the implicit bias of the different algorithms we consider. Indeed, following
Woodworth et al. (2020) we can show the following result for the gradient flow dynamics (9)
Proposition 3 (Theorem 1 of Woodworth et al. (2020)). For any 0 < α< ∞, if the solution β∞ of
the gradient flow (9) started from w0 = α1 for the squared parameter problem in Eq.(8) satisfies
Xβ∞ = y, then
β∞ = arg min φα(β) s.t. Xβ = y,	(13)
β∈Rd
where φα is defined in Eq.(12).
It worth noting that the implicit regularizer φα interpolates between the `1 and `2 norms (see
Woodworth et al., 2020, Theorem 2). Therefore an implicit bias depending on the scale of the
initialization is observed. The algorithm, started from a1, converges to the minimum '1 -norm
interpolator for a small and to the minimum '2 -norm interpolator for a large. The proof follows from
(a) the KKT condition for the optimization problem (13): Vφα(w) = X>ν for a Lagrange multiplier
ν and (b) the closed form solution obtained by integrating the gradient flow, w = b(X>ν) for some
function b and some vector ν. Identifying Vφα (w) = b-1(w) leads to the solution.
Considering the same proof technique, we now derive the implicit bias of both variants of SAM
algorithms.
Implicit bias of MaxSum SAM algorithm. We characterize the implicit bias of the MaxSum
dynamics (10) in the following proposition. Recall that the function φα is defined in Eq. (12).
Proposition 4. For any 0 < α < ∞, if the solution β∞ of the gradient flow (10) started from
w0 = α1 for the squared parameter problem in Eq.(8) satisfies Xβ∞ = y, then
β∞ = arg mβin φαMaxSum (β) s.t. Xβ = y,
where aMaxSum = ɑe-4n2 R0∞(x>rs)2ds+o(ρ2).
We note that for P small enough aMaxSum < a. The scale of the vector 焉 f∞ (X>rs)2ds (which
influences the bias effect) is related to the loss integral d f∞ L(W(Sy)ds since Ilrsk2 = nL(w(s))
(see intuition in Eq.(16)). Thereby the speed of convergence of the loss controls the magnitude of
the biasing effect. However in the case of MaxSum SAM, as explained below, this effect is typically
negligible because of the extra prefactor d and this implementation behaves similarly as ERM (as
shown in the experiments in Sec. 3.1).
Proof. We follow the proof technique of Woodworth et al. (2020). We first derive the equation
satisfies by the flow
1
w(t) = -5-x>r+(t) ◦ w+(t)
2n
=— ɪX>r+(t) ◦ (w(t) + AX>r(t) ◦ w(t)),
2n	2n
where w+(t) = w(t) + PVL(W(t) is the intermediate SAM step, r(t) = Xw(t)2 — y and r+(t)=
XW+(t)2 — y are the residual of W(t) and W+(t) and we denote by X = [X — X]. We can directly
integrate this ODE to obtain a closed form expression for W(t):
w(t) = w(0) ◦ exp(———JX> Z r+ (s)ds) ◦ exp(——Py / (X>r+(s)) ◦ (X>r(s))ds).
2n	0	4n2 0
15
Under review as a conference paper at ICLR 2022
Using that the flow is initialized at w(0) = α1 and the definition of β(t) yields to
β(t) = u(t)2 - v(t)2
=α2 exp(-工X> Z r+(s)ds) ◦ exp(——P Z (X>r+(s)) ◦ (X>r(s))ds)
n 0	2n2 0
—α2 exp( JX> / r+(s)ds) ◦ exp(——P / (X>r+(s)) ◦ (X>r(s))ds)
n 0	2n2 0
=2α2 exp(——P ( (X>r+(s)) ◦ (X>r(s))ds) sinh(-工X> / r+(s)ds).
2n2 0	n 0
Recall we are assuming that β∞ is global minimum of the loss, i.e., Xβ∞ = y. Thus
Xβ∞ = y
β∞ = bαMaxSum (X ν),
With and bα(z) = 2α2 ◦ Sinh(z), V = — 1 f∞ r+(s)ds and
αMaχSum = αexp(-金 / (X>r+(s)) ◦ (X>r(s))ds).
(14)
Denoting by
Vφα(β) = b-1 arcsinh(2αα2 ◦ β),
and integrating this equation, We obtain the expression of φa = Pcd=I α2q(βi∕a2) where q(z)=
J0Z arcsinh(u∕2)du = 2 — λ∕4 + z2 + Z arcsinh(z∕2).
Therefore β∞ satisfies the KKT conditions for the minimum norm interpolator problem
arg minβ φα(β) s.t. Xβ = y, i.e.
Xβ∞ = y
Vφα(β∞) = X>ν.
This proves the first part of the result. NoW using the definition of r+(s) We obtain
r+(t) = Xw+(t)2 - y
=X(w(t) + PX>r(t) ◦ w(t))2 — y
=r(t) + 2ρX(X>r(t)) ◦ w(t)2 + ρ2X(X>r(t))2 ◦ w(t)2
= r(t) + 2PX(X>r(t)) ◦ (u(t)2 + v(t)2) + P2X(X>r(t))2 ◦ β(t).
And
X>r+(t) = X>r(t) + O(ρ),
Which concludes the second part of the result.
□
Implicit bias of SumMax SAM algorithm We characterize similarly the implicit bias of the
SumMax dynamics (11) in the folloWing proposition. Recall that the function φα is defined in
Eq. (12).
Proposition 5. For any 0 < α < ∞, if the solution β∞ of the gradient flow (11) started from
w0 = α1 for the squared parameter problem in Eq.(8) satisfies Xβ∞ = y, then
β∞ = arg mβin φαSumMax (β) s.t. Xβ = y,
where αSumMaX = αe-卷 R∞ Pn=1 x2(X>β(S)-yi)2ds+O(P2).
In addition, if we assume that there exist R, B ≥ 0 such that the features are bounded kxk ≤ R
almost surely and that the trajectory oftheflow is bounded ∣∣β(t)k ≤ B for all t ≥ 0, we have for all
ρ ≤------/ 1	=:
R R R2√B(B+kβ*k)
αSumMax ≤ α
16
Under review as a conference paper at ICLR 2022
We note that for ρ smaller than an explicit constant depending on the problem quantities, we can assert
that aMaχSum < α. Furthermore the scale of the vector 1 f∞ pn=1 χi∣(x>β(s) - yi)2ds (which
influences the bias effect) is related to the loss integral d 0∞ L(w(s))ds, through krs k2 = nL(w(s))
(see intuition Eq. (17)). Thus the speed of convergence of the loss controls the magnitude of the
biasing effect. This effect is typically important in the case of SumMax SAM (see experiments
Sec.3.1).
Proof. The proof follows the same lines as the proof of Proposition 4 Let us denote by Xi = [χ^ - Xi].
We then have that the dynamics of the flow (11) satisfies
1n
w(t) = - 2n∑Sxiri,+w ◦ w+(t)
i=1
1n	ρ
=-2n Ex""+⑴(I + 2 χiri(t)) ◦W⑴，
n i=1
where w+ (t) is the intermediate SAM step, ri(t) = X>w(t)2 - yi and r3+(t) = XiW+ (t)2 - yi are
the residual of w(t) and w+(t) at the observation (Xi, yi). Integrating we obtain:
1	t	nt
w(t) = w(0) ◦ exp(-XT	r+(s)ds) ◦ exp(-X2	ri +(s)ri(s)ds).
n 0	2n i=1 i 0	,
The remainder of the proof is similar to the one of Proposition 4 and we directly obtain that
αSumMax
α exp(—-
4n
nt
Vx2 / ri,+(s)ri(s)ds).
i=1	0
(15)
Using the definition of ri,+ (t) we have
ri,+(t) = x>w+⑴2 — y
=x>w(t)2 ◦ (1 + ρri,txi)2 - y
=x>w(t)2 ◦ (1 + 2ρri,tΓxi + ρr2,t>x2) - y
= ri,t + 2ρri,t (u(t) + v(t) ) xi +ρri,tβ(t) xi .
And therefore
xi2ri,+ (t)ri(t) = xi2ri(t)2 [1 + 2ρ(u(t)2 + v(t)2)>xi2 +ρ2ri(t)β(t)>xi3])
So ifρis such that 1 + ρ2ri(t)β(t)>xi3 ≥ 0 then αSumMax < α. Using Cauchy-Schwarz in-
equality ∣ri(t)β(t)τx3∣ ≤ ∣∣xi∣∣(∣∣β∣∣ + ∣∣β*∣∣)∣∣β∣∣∣∣x3∣∣ and Holder inequality ∣∣x3∣∣ ≤ ∣∣xi∣∣3.
Thus ∣ri(t)β(t)τx3∣ ≤ ||xi∣∣4(∣∣β∣∣ + ∣∣β*∣∣)∣∣β||. And we have the result for P ≤ R-2(B(B +
kβ*k)L2.
Comparison between SumMax and MaxSum implicit bias and connection with the loss inte-
gral We wish to compare the two leading terms IMaxSUm(t) = n2(Xτr(t))2 = *(Pi=I xi.42
and ISUmMax(t) = 1 Pn=I x2ri(t)2 which comprise the first order in the expansion of the α and
relate them to the loss value at w(t).
We first note using Cauchy-Schwarz inequality that ISumMax(t) ≥ IMaxSum(t) but we aim
at obtaining a more quantitative result, even though the derivation will be informal. Com-
paring their '1-norms amounts to compare the empirical fourth moment IlIMaxSUm(t)kι =
(w - W*)τn Pn=IIlxiII2xixT(W - w*) and second moment squared IlISUmMax(t)kι = (w -
w*)T[ n Pn=I xix>]2(w - wJ.
We can compare the typical operator norm of these two random matrices. Following the Bai-Yin’s
law, the operator norm of a Wishart matrix is with high probability ∣∣ nn Pn=I xixτ k ≈ d and that
17
Under review as a conference paper at ICLR 2022
with high-probability, the squared norm of a Gaussian vector is kxi k ≈ d. Therefore we obtain that
1 n	1 n	d2
∣KΣkxik2χix>k ≈ di— Exix>k ≈ —
n	nn
i=1	i=1
1n	1n	d
k[ n X χix>]2k ≤ k[ n X χix>]k2 ≈ (n)
Therefore in the overparametrized regime (d >>> n) we typically have that ISUmMax(t) ≈ n and the
biasing effect of the SumMax implementation would tend to be O(n) time better than the biasing
effect of the MaxSum implementation.
However this first insight only enables to compare IMaxSum(t) and ISumMax(t). It is not informative on
the intrinsic biasing effect of MaxSum and SumMax SAM. With this aim, we would like to relate
the quantities IMaxSum(t) and ISumMax(t) to the loss function evaluated in w(t). With high probability
(still using the concentration of a Gaussian vector) we have that
1n
IIIMaxSUm(t)kl = (w(t) — W*)> — E Ilxik2xix> (w(t) - W*)
i=1
1n
≈ d(w(t) — w*)>	xix>(w(t) — w*)
n
i=1
= dL(w(t)).	(16)
And using the concentration of Wishart matrices, i.e., 1 [XX>] ≈ I for large dimension d, we also
have
IlISUmMax(t)k1 = F (W⑴-w*)>X >XX > X (W⑴-WJ
n2
—((w(t) — W*)>X> — [XX>]X(w(t) — W*)
n2	d
≈ — (w(t) — w*)>J [X >X ](w(t) — w*)
nn
=-L(w(t)).	(17)
n
These approximations provide some intUitions on why the biasing effect of both SAM implementation
can be related to the integral of the loss. We let a formal derivation of these resUlts as fUtUre work.
Additional experiments on implicit bias of stochastic methods. We provide an additional experi-
ment to investigate the performance of stochastic implementation of the ERM, MaxSUm and SUmMax
SAM. As explained by Pesme et al. (2021), we observe in Fig.9 that the stochastic implementations
enjoy a better implicit bias than their deterministic coUnterparts. We note that the fact that small
batch versions generalize better than fUll batch version is commonly observed in practice for deep
networks Keskar et al. (2016). We let the characterization of the implicit bias of these stochastic
implementations as fUtUre works.
B Convergence of SAM algorithm
In this section we prove the convergence of the SAM algorithm to a stationary point of the fUnction L
or to a minimUm when the fUnction is convex.
B.1	Convergence of deterministic SAM algorithm
Here we aim to Understand if the one-step approximation of SAM makes sense from an optimization
perspective, i.e., whether the algorithm proposed in (Foret et al., 2021) can sUccessfUlly minimize the
18
Under review as a conference paper at ICLR 2022
Figure 9: Implicit bias of SAM on a sparse regression problem with d = 30, n = 20, Xi 〜N(0, I),
K = kβ* ko = 3, yi = x>β* and 九〃(x) = x>(u Θ v). All methods are initialized at α = 0.01 and
used with step-size γ = 1/L and ρ = 1/L. SumMax SGD converges to a solution which generalizes
better (left plot) and enjoys a different implicit bias from the other methods. At the same time, all
algorithms converge to a global minimum of f at linear rate (right plot). The convergence rate is
inversely proportional to the biasing effect.
100	102	104	106
Number of iterations
training objective. This is an important aspect since if an optimization algorithm cannot sufficiently
minimize the training loss, the resulting classifier typically will not lead to good generalization
performance, even if it has some useful implicit bias (Neyshabur et al., 2014). We first consider the
following first-order optimization algorithm on the objective L parameterized by w:
wt+1 = Wt - INL(Wt + PNL(Wty),	(18)
where ρt and γt are the step sizes of the inner and outer steps, respectively. This update rule is
reminiscent of the extra-gradient algorithm (Korpelevich, 1977) but with an ascent in the inner
step, i.e., the gradient is evaluated not at the current iterate but at the point which maximises
locally the function. Moreover, this update rule can also be seen as a realization of the general
extrapolated gradient descent framework suggested in Lin et al. (2020). However, taking an ascent
step for extrapolation is not discussed there, and the convergence properties of the update rule from
equation 18, to the best of our knowledge, have not been proven.
Summary of the convergence results. Let us first recall the definition of smoothness.
(A2’)(β-smoothness). It exists β >0 such that ∣∣VL(w) 一 VL(v)k ≤ β∣∣w - Vk for all w,v ∈ Rd.
When the function L is β-smooth, convergence to stationary points can be obtained.
Proposition 6. Assume (A29). For any γ < 1∕β and ρ < 1∕β, the iterates equation 18 satisfyfor all
T ≥ 0:
1 T-1	2
T X ∣vL(wt)k2 ≤ Y(I-Pe)T (L(W0 )-LJ,
If, in addition, the function L satisfies (A3), then:
L(WT) - L* ≤ (1 - Y(I J")T(L(wo) - L*).
We can make the following remarks:
•	We recover the rates of gradient descent but with constants increasing with the ascent step-size
P.
•	The condition P < 1∕β is necessary since the point W + 1∕βVL(W) can be a local maximum
of L. Such W would be a fixed point of the algorithm without being a stationary point of L.
•	The proof crucially relies on the bound hVL(Wt+PVL(Wt)), VL(Wt)i ≥ (1-Pβ)kVL(Wt)k2
which shows that SAM-step is well aligned with the gradient step (see Lemma 7) and on a
descent inequality similar to the classical one for gradient descent (see Lemma 8).
19
Under review as a conference paper at ICLR 2022
•	For non-convex functions, full details are provided in Proposition 9. When the function satifies
an addition Polyak-Lojasiewicz inequality, details are in Proposition 10.
•	For convex functions, BL(Wt + NL(Wty), VL(Wt)) ≥ ∣∣VL(wt)k2 and convergence holds
for any step-size ρ given that γρis small enough. Details are provided in Proposition 11 .
Auxillary Lemmas. The following lemma shows that the SAM update is well correlated with the
gradient VL(W) and will be cornerstone to our proof.
Lemma 7. Let L be a differentiable function and W ∈ Rd. We have the following bound for any
ρ≥ 0:
-β
hVL(W + ρVL(W)), VL(W)i ≥ (1 + αρ)∣VL(W)∣2 where α =	0
[μ
if L is β-smooth,
if L is convex
if L is μ-strongly convex.
Proof. We simply add and subtract a term ∣VL(W)∣2 in order to make use of classical inequalities
bounding hVL(W1) - VL(W2), W1 - W2i by ∣W1 - W2 ∣2 for smooth or convex functions and
W1, W2 ∈ Rd.
hVL(W + ρVL(W)), VL(W)i = hVL(W + ρVL(W)) - VL(W), VL(W)i) + ∣VL(W)∣2
=1∕ρhVL(w + PVL(W)) — VL(w),ρVL(w)i + IlVL(W)∣∣2
≥ (1 + αρ)∣VL(W)∣2,
where the last inequality is using that
-β
hVL(W1) - VL(W2), W1 - W2i ≥ α∣W2 - W1 ∣2, where α =	0
[μ
if L is β-smooth,
if L is convex
if L is μ-strongly convex.
□
The next lemma shows that the decrease of function values of the SAM algorithm defined in Eq.(18)
can be controlled similarly as in the case of gradient descent Nesterov (2004).
Lemma 8. Assume (A2,). For any Y ≤ 1∕β ,the iterates (18) Satisfiesfor all t ≥ 0:
L(Wt+i) ≤ L(Wt)- γ(1 - Pe)(1 - Y2-(1 - Pβ))kVL(Wt)k2.
Ifin addition the function L satisfies (A3) with potentially μ = 0, then for all γ, ρ ≥ 0 such that
γβ(2 - Pβ) ≤ 2, we have
L(Wt+i) ≤ L(Wt) - γ(1 - γ2- + ρμ(1 - γβ - I。； )) IlVL(Wt)k2.
We note that the constraints on the step-size are different depending on the assumptions on the
function L. In the non-convex case, P has to be smaller than 1∕β, whereas in the convex case, it has
to be smaller than 2∕β.
Proof. Let us define by Wt+1/2 = Wt + PVL(Wt) the SAM ascent step. Using the smoothness of
the function L (Assumption (A2’)) (see, e.g., Nesterov, 2004), we obtain
γ2β
L(Wt+1) ≤ L(Wt)- YhVL(Wt+1/2), VL(Wt)i +------2- kVL(Wt+1/2)k .
The main trick is to use the binomial squares
IVL(Wt+1/2)I2 = -IVL(Wt)I2 + IVL(Wt+1/2) - VL(Wt)I2 + 2hVL(Wt+1/2), VL(Wt)i,
20
Under review as a conference paper at ICLR 2022
to bound
γ2 β
L(wt+ι) ≤ L(Wt)- NLwt+", NL(Wt)) + ɪ∣∣VL(wt+1∕2)k2
=L(Wt)- Y2gIlVL(Wt)∣2 + "2^IlVL(Wt+i/2) - VL(Wt)Il2 - γ(1 - γβ)hvL(wt+ι∕2), VL(Wt))
≤ L(Wt)- γ[1 - Pe - Y2-(1 - PeY]kVL(Wt)k2,
where we have used Lemma 7 and that IVL(Wt+1/2 ) - VL(Wt)I2 ≤ β2 IWt+1/2 - Wt I2 ≤
β2P2IVL(Wt)I2.
If in addition the function L is convex then we can use its co-coercivity (see, e.g., Nesterov, 2004) to
bound IlVL(Wt+1/2) -VL(Wt)∣2 ≤ βhVL(Wt+1∕2) - NL(Wt),Wt+1/2 - Wti and obtain a tighter
bound:
γ2 β
L(Wt+ι) ≤ L(Wt)- YhVL(Wt+1/2), VL(Wt)) + ɪIlVL(Wt+1∕2)∣∣2
=L(Wt)- -ɪ- IlVL(Wt) Il2 + ɔɪ- IlVL(Wt+1/2) - VL(Wt) Il2 - γ(1 - Ye)hVL(Wt+1/2), VL(Wt))
≤ L(Wt) - γ(1 - 了)11VL(Wt)Il2 - γ(1 - γβ - CP： )〈VL(Wt+1/2) - VL(Wt), VL(Wt))
≤ L(Wt) - γ(1 - γ2- + ρμ(1 - Yβ - y，))11VL(Wt)『,
where We have used Lemma 7.	□
Convergence proofs. Using the previous Lemma 8 recursively, we can bound the average gradient
value of the iterates (18) of SAM algorithm and ensure convergence to stationary points.
Proposition 9. Assume (A29). For any γ < 1∕β and P < 1∕β ,the iterates equation 18 Satisfiesfor
all T ≥ 0:
1T
T EkVL(Wt)I2 ≤
T t=0
L(W0) - L(WT)
Tγ(1- ρβ)[1- γβ(1- ρβ)]
Proof. Using the Lemma 8 we obtain
Y(I- Pe) (1 - 了(I- Pe)) IVL(Wt)『≤ L(Wt)- L(Wt+1).
And summing these inequalities for t = 0,…，T - 1 yields
T-1
ɪ X IIVL(Wt)I2 ≤ ——L(Wo)- L(WT)——.
T t=0	— Tγ(1-Pβ)[1- Y(1 - Pβ)]
□
When the function L additionally satisfies a Polyak-Lojasiewicz condition, i.e., the Assumption (A3),
linear convergence of the function value to the minimum function value can be obtained. This is the
object of the following proposition:
Proposition 10. Assume (A2’-3). For any Y < 1∕β and P < 1∕β, the iterates (18) satisfies for all
T ≥ 0:
L(Wt)- L* ≤ (1 - 2γμ(I- Pe) (1 - ^2^(1 - Pe)))(L(WO)- L J
Proof. Using the Lemma 8 and that the function L is μ Polyak-Lojasiewicz (Assumption (A3)) we
obtain
L(Wt+1)≤ L(Wt) - 2μγ(1 - pl) (1 - ɔ2-(1 - PL)) (L(Wt) - L*)∙
21
Under review as a conference paper at ICLR 2022
And subtracting the optimal value L* We get
L(Wt)- L ≤ (1- 2γμ(I- Pe) (1- -2-(1 - Pe)))(L(Wt-ι)- LJ
≤ (1 - 2γμ(I- Pe) (1 - ɪ(1 - Pe)))(L(WO)- Lj
□
When the function L is convex, convergence of the average of the iterates can be proved.
Proposition 11. Assume (A2’) and L convex. For any step-sizes γ and P such that γe(1 + Pe) < 2,
then the averaged WT = T PTOI Wt Ofthe iterates (18) SatiSfieSfOr all T ≥ 0:
L(WT)-L* ≤ γ(2-Yρβ"ρβ))T kW0 - W*k2，
If, in addition, thefunction L is μ-strongly convex, then:
IlWT - W* Il2 ≤ (1 - γμ(2 - γβ(1 + Pe)))T(2ρ + 1)∣∣W0 - W*『.
The proof is using a different astute Lyapunov function (Which Works for the non-strongly convex
case).
Proof. Let us define by Vt = [L(Wt) — L(w*)] + 2ρ IlWt — w*∣∣2 and by Wt+1/2 = Wt + NL(Wt)
the SAM ascent step.
2
Vt+1 -	Vt	≤--"L(Wt+1/2), Wt	-	W*i	- γ"L(Wt+1/2),	PL(Wt))	+ ʒ-(I +	Pe)Il▽£(Wt+1/2)112
P	2P
2
=-P "L(Wt+1/2), Wt + NL(Wt) - W*i + 2P(I + Pe)IPL(Wt+1/2)k2
2
=----〈▽L(Wt+1/2), Wt+1/2 - W*i + 5-(I + Pe)IlPL(Wt+1/2)Il 2
P	2P
≤ - - (1 - F(I + Pe))hPL(Wt+1/2), Wt+1/2 - W*i.
P2
If L is convex then L(Wt+1/2) - L(W*) ≤ hPL(Wt+1/2), Wt+1/2 - W*) and therefore We obtain
Y (1 - Yχ^ (I + Pe))L(Wt+1/2)- L(W*) ≤ Vt - Vt+1.
P2
Using the definition of Wt+1/2 We alWays have that L(Wt+1/2) ≥ L(Wt) + PIPL(Wt)I2 therefore
γ(1 - γe(1+ Pe))L(Wt) - L(w*) ≤ Vt - Vt+1.
P2
And taking the sum and using Jensen inequality We finally obtain:
T
L(1/T X Wt) - L(W*) ≤
t=0
% - VT +1
TP (1-等(1+Pe))
If L is μ-strongly convex, we use that hVL(wt+1/2), Wt+1/2 - w*) ≥ μ∣∣Wt+1/2 - w*『to obtain
IWt+1/2 - W*I2 = IWt + PPL(Wt) - W*I2 = IWt - W*I2 + 2PhPL(Wt), Wt - W*) + P2IPL(Wt)I2
≥ IWt - W*I2 + 2PhVL(Wt), Wt - W*)
≥ IWt - W* I2 + 2P[L(Wt) - L(W*)]
≥ 2PVt .
Therefore we have
Vt+1 ≤ (1 - γμ(2 - γe(1 +Pe))Vt ≤ (1 - γμ(2 - γe(1 + Pe))t+1V0.
□
22
Under review as a conference paper at ICLR 2022
B.2	Convergence of stochastic implementations of SAM algorithm
B.2.1	Convergence of MaxSum gradient algorithm
When the SAM algorithm is implemented with the MaxSum objective as optimization objective, two
different batches are using in the ascent and descent steps. We obtain the MaxSum algorithm defined
as
wt+1 = Wt- Yt X v`i (wt + P X v`i(wt)),	(19)
i∈It	i∈Jt
where It and Jt are two different mini-batches of data of size b. For this variant of the algorithm we
obtain a similar convergence result that the one obtain with the same batch for the two steps.
Proposition 12. Assume (A1), (A2’) for the iterates (19). For any T ≥ 0 and for step-sizes γt
and ρt = τi∕4β, we have：
1
VTe
1 T-1	4	8σ2
T E X kVL(Wt)k ] ≤ β√τ(L(WO) - LJ + b√τ,
In addition, under (A2), with step-sizes Yt = min{ 3京+：产,表} and Pt = γ∕∕t∕β：
3β2(L(wo)- L*)	22βσ2
E[L(WT)] -L ≤ -μτ— + W
We obtain the same convergence result that in Proposition 2, but under the relaxed smoothness
assumption (A2’).
As in the deterministic case, the proof relies on two lemmas which shows that the SAM update is
well correlated with the gradient and that the decrease of function values can be controlled.
Auxilliary Lemmas. The following lemma shows that the SAM update is well correlated
with the gradient VL(Wt). Let Us denote by VLt+ι(w) = b Pi∈几 V'i(w), VLt+1/2(W) =
b Pi∈jt V'i(W) and Wt+1/2 = Wt + pVLt+1∕2(Wt) the SAM ascent step.
Lemma 13. Assume (A1) and (A2). Then for all ρ ≥ 0, t ≥ 0 and W ∈ Rd,
EhVLt+1(W + PVLt+1/2(W)), VL(W)i ≥ (1/2 - βP)kVL(W)k2 -月一；—.
The proof is similar to the proof of Lemma 7. Only the stochasticity of the noisy gradients has to
be taken into account. In this aim, we consider instead the update which would have been obtained
without noise, and bound the remainder using the bounded variance assumption (A1).
Proof. Let US denote by W = W + PVL(W), the true gradient step. We first add and subtract
VLt+1/2 (W)
hVLt+ι(W + pVLt+1∕2(W)), VL(W)i = hVLt+1(W + pVLt+1∕2(W))- VLt+1(W), VL(W)i-hVLt+1(W), VL(w)〉.
We bound the two terms separately. We use the smoothness of L (Assumption (A2’)) to bound the
first term:
-EhVLt+1(w + PVLt+1/2 (W))- VLt+1(W), VL(W)i = - EhVL(W + PVLt+1/2 (W))- VL(W), VL(W)i
≤
≤
≤
≤
2 EIlVL(W + PVLt+1/2(W))- VL(W) k2 + 2 kVL(W) k2
β2	1
ɪ EkW + PVLt+1/2(W)- Wk + 2 kVL(W)Il
学 EkVLt+1/2(W)- VL(w)k2 + 1 kVL(wt)k2
β2Pbσ2 + 1 kVL(w)k2,
23
Under review as a conference paper at ICLR 2022
where We have used that the variance of a mini-batch of size b is bounded by σ2∕b. Note that this
term can be equivalently bounded by βρσ∕√6kVL(w)k if needed. For the second term, we directly
apply Lemma 7 to obtain
EhVLt+1(1^), VL(w)i = EhVL(W), VL(w)i
≥ (1 - βρ)kVL(w)k2.
□
The next lemma shows that the decrease of function values of SN-SAM algorithm can be controlled
similarly as in the case of stochastic gradient descent.
Lemma 14. Let Us assume (A1, A2') then for all Y ≤ 1∕(2β) and P ≤ 1∕(2β) ,the iterates equa-
tion 19 satisfies
EL(wt+ι) ≤ EL(Wt) — γ∕4EkVL(Wt)k2 + γβσ2(γ + ρ2beta).
This lemma is analogous to Lemma 8 in the stochastic case. The proof is very similar, with the slight
difference that Lemma 13 is used instead of Lemma 7.
Proof. Let us define by Wt+1/2 = Wt + PVLt+1/2(Wt). Using the smoothness of the function L
(A2), we obtain
γ2β
L(Wt+1)≤ L(Wt)- Y hVLt+1(Wt+1/2), VL(Wt)i +-2 IlVLt+1(Wt+1/2)k .
Taking the expectation and using that the variance is bounded (A1) yields to
Y2β
E L(Wt+1) ≤ E L(Wt)- Y EhVL(Wt+1/2), VL(Wt)i + 2 EllVLt+1 (Wt+1/2)k
≤ EL(Wt) — YEhVL(Wt+1/2), VL(Wt)i + γ2βEkVLt+1(Wt+1∕2) — VL(Wt+1∕2)k2 + γ2βEkVL(Wt+1∕2)k2
≤ E L(Wt) - Y EhVL(Wt+1/2), VL(Wt)i + Y2βσ2∕b + Y2βE kVL(Wt+1/2)k2.
The main trick is still to use the binomial squares
kVL(Wt+1/2)k2 = —kVL(Wt)k2 + kVL(Wt+1/2) — VL(Wt)k2 + 2hVL(Wt+1/2), VL(Wt)i
to bound
Y2β
E L(Wt+1)≤ E L(Wt)-Y EhVL(Wt+1/2), VL(Wt)i + 2 EkVL(Wt+1/2)k + Y σ e/b
= E L(Wt) — Y2L E kVL(Wt)k2 + Y2β E kVL(Wt+1/2) — VL(Wt)k2
-	Y(1 - 2Yβ) EhVL(Wt+1/2), VL(Wt)i + Yσ2β∕b
=EL(Wt) - Y2βE kVL(Wt)k2 + Y2L3 EkWt+1/2 - Wtk2
-	Y(1 - 2Yβ)(1∕2 + αρ) E kVL(Wt)k2 + Y(1 - 2YL)σ2ρ2β2∕2 + Y2σ2β∕b
=EL(Wt) - Y2βE kVL(Wt)k2 + Y2β3ρ2 EkVLt+1/2(Wt)k2
-	Y(1 - 2Yβ)(1∕2 + αP) E kVL(Wt)k2 + Y(1 - 2Yβ)σ2∕bP2β2∕2 + Y2σ2β∕b
=EL(Wt) - Y2β E kVL(Wt)k2 + 2Y2β3P2 E kVL(Wt)k2 + 2Y2β3P2σ2∕b
-	Y(1-2Yβ)(1∕2+αP)EkVL(Wt)k2+Y(1-2Yβ)σ2P2β2∕2+Y2σ2β∕b
≤ L(Wt) - 2[1 - 2ρβ(1 - 2Yβ(1 - ρβ))] E kVL(Wt)k2 + Yσ2β∕b[Y + ρ2L∕2(1 + 2[β)]
where we have used Lemma 13and that kVL(Wt+1/2) -VL(Wt)k2 ≤ β2||Wt+1/2 — Wtk2.	□
Using Lemma 14 we directly obtain the following convergence result.
24
Under review as a conference paper at ICLR 2022
Proposition 15. Assume (A1) and (A2’).For Y ≤ 1∕(2β) and P ≤ 1∕(2β) ,the iterates equation 5
satisfies:
1 T-1
T EEkVL(Wt)k2
T t=0
≤ 4Lw⅜ELwT2 +4Tσ2β(Y + P2β)∕b.
This proposition gives the first part of Proposition 12. The proof of the stronger result obtained when
the function is in addition PL (Assumption (A3)) is similar to the proof of Theorem 3.2 of Gower
et al. (2019), only the constants are changing.
B.2.2 Convergence of SumMax Gradient algorithm
When the SAM algorithm is implemented to minimize the SumMax objective, the same batch is used
in the ascent and descent steps. We obtain then iterates (5) for which we have stated the convergence
result in Proposition 2. The proof follows the same lines as before with the minor different that we
are assuming the individual gradient Vft are Lipschitz (Assumption (A2)) to control the alignment
of the expected SAM direction. Let Us denote by VLt(W) = b Pi∈jjt V'i (w).
Lemma 16. Assume (A1-2). Then we have for all W ∈ Rd, P ≥ 0 and t ≥ 0
EhVLt(w + ρVLt(w)), VL(w)i ≥ (1/2 - ρβ)∣∣VL(w)k2 - e2^.
The proof is very similar to the proof of Lemma 13. The only difference is that the AssUmption (A2)
is Used instead of (A2’).
Proof. Let us denote by W = W + ρVL(w), the true gradient step. We first add and subtract VLt(W)
hVLt(w + PVLt(W)), VL(W)i = hVLt(W + PVLt(W))- VLt(W), VL(W))-<VLt(W), VL(w)).
We bound the two terms separately. We use the smoothness of Lt to bound the first term (Assumption
(A2)):
-ELt(W + ρVLt(w)) - VLt(W), VL(W)i ≤ ∣∣∣VLt(W + PVLt(w)) - VLt(W)『十 ∣∣∣VL(w)∣∣2
β2	1
≤ β E kw + ρVLt(w) - Wk2 + 2 IlVL(W)k2
≤ WkVLt(W) - VL(w)k2 + 2kVL(w)k2.
And taking the expectation, we obtain:
β2 P2 σ2	1
-EhVLt(W + PVLt(W))-VLt(W), VL(w)i ≤ β^- + 2 EkVL(W)『.
For the second term, we apply directly Lemma 7
EhVLt(W), VL(wt)i = hVL(W), VL(w)i
≥ (1 - βP)kVL(W)k2.
Assembling the two inequalities yields to the result.	□
The next lemma shows that the decrease of function values of SAM algorithm can be controlled
similarly as in the case of gradient descent. It is analogous to Lemma 8 where different batches are
used in both the ascent and descent steps of SAM algorithm.
Lemma 17. Assume (A1-2). For all γ ≤ 1∕β and P ≤ 1∕(4β), the iterates (5) satisfies
EL(Wt+1) ≤ EL(Wt) - 3γ∕8 E kVL(Wt)k2 + γβσ2∕b(γ + 2P2β).
25
Under review as a conference paper at ICLR 2022
Proof. Let Us define by wt+1/2 = Wt + NLt+∖(wt). Using the smoothness of the function L which
is implied by (A2), we obtain
γ2 β
L(Wt+1) ≤ L(Wt)- Y (VLt+ι(wt+ι∕2 ), VL(Wt)i + ɪ- ∣∣VLt+ι(wt+ι∕2)k2.
We still use the binomial squares
IlVLt+1(Wt+1/2)k2 = -IlVL(Wt)∣2 + IlVLt+1(Wt+1/2)- VL(Wt)∣2 + 2hvLt+1(wt+1∕2), VL(Wt)i
and bound L(Wt+1) by
L(Wt+1) ≤ L(Wt)- Y2- IlVL(Wt) k2 + Y2- IlVLt+1 (Wt+1/2)- VL(Wt) k 2 - Y(I- Ye)(VLt+1 (Wt+1/2), VL(Wt)i
γ2β
≤ L(Wt) —W IlVL(Wt)Il + γ β∣l VLt+1(Wt+1/2)- VLt+1(Wt)Il + γ β∣l VLt+1(Wt)- VL(Wt)Il
—Y(I- Ye)(VLt+1(Wt+1/2), VL(Wt)i
Y2β
≤ L(Wt) —2— IlVL(Wt)Il + Y ββ IlWt+1/2 - Wtll + Y βIlVLt+1(Wt)-VL(Wt)Il
-Y(1 - Ye)hVLt+1(Wt+1/2), VL(Wt)i
=L(Wt)-?IVL(Wt)I2 + Y2e3p2||VLt+1(Wt)『+ Y2β∣∣VLt+1(Wt) - VL(Wt)I2
一 y(1 - Ye)(VLt+1(Wt+1/2), VL(Wt)i
=L(Wt)-号(1 - 4β2ρ2)IVL(Wt)I2 + y2β(1 + 2β2ρ2)∣∣VLt+1(Wt) - VL(Wt)I2
-Y(I- Ye)(VLt+1(Wt+1/2), VL(Wt)i
Taking the expectation and using Lemma 16 we obtain
EL(Wt+1) ≤ EL(Wt) - Y2e(1 - 4β2ρ2) E IVL(Wt)I2 + Y2β(1 + 2β2ρ2) E ∣∣VLt+1(Wt) - VL(Wt)I2
-	Y(1 - Ye) E(VLt+1(Wt+1/2), VL(Wt)i
≤ EL(Wt)- 22”(I - 4e2ρ2) EIVL(Wt)∣∣2 + Y2e(1 + 2β2p2)σ2∕b
2σ2e2
-	Y(1 - Ye)(1/2 - βρ) E IVL(Wt)I2 + y(1 - Ye)P^
≤ EL(Wt)- ɔɪ-(1 - 4e2ρ2) EllVL(Wt)∣∣2 + Y2e(1 + 2β2p2)σ2∕b
-	2(1 - 2βρ(1 - Y(β - 2ρβ2))) E IVL(Wt)I2 + Yσ2∕b[Yβ + 号(1 + 3Yβ)]∙
□
Using Lemma 17 we directly obtain the following convergence result.
Proposition 18. Assume (A1-2). For Y ≤ 1∕e and ρ ≤ 1∕4e, the iterates (5) satisfies:
T-1
T X E IVL(Wt)I2 ≤ 8/3L(W0) :TEL(WT)
T t=0	TY
+ 8Tσ2β(Y + ρ2β)∕(3b)∙
In addition, under (A3), with step-sizes Yt = min{ 3京+4产,1∕(2β)} and Pt = 'Yt∕β:
E[L(wt)] — L* ≤
3β2 (L(wo) - LG	22βσ2
μ2T2	+ μ2bT ∙
Proof. The first bound directly comes from Lemma 17. The second bound is similar to the proof of
Theorem 3.2 of Gower et al. (2019), only the constants are changing.	□
26
Under review as a conference paper at ICLR 2022
----Test error, normalized SAM, p= 0.1
---- Test error, unnormalized SAM, p = 0.3
Train error, normalized SAM, p = 0.1
Train error, uππormalizβd SAM, p = 0.3
20%
15%
g 10%
LU
5%
0%
Epoch
600
Epoch
(a) Without label noise
(b) With 60% label noise
Figure 10: Training and test error over training epochs for the normalized and unnormalized
formulations of SAM. We can see that both variants of SAM improve the test error to a similar level.
B.3 Gradient normalization in SAM
We consider here the implementation of SAM algorithm where the inner gradient step is normalized:
wt+1
▽ /√	工	NL(Wt) ∖
Wt-YVL(Wt + P kvL(WlJ,
(20)
We state here several claims showing that the normalized formulation in Eq.(20) can be less favorable
than the unnormalized one of Eq.(18).
Fact I: Normalized SAM does not converge with constant step-size. The last iterate of SAM
with constant step-size is never converging which can be easily seen on the function L(W) = 0.5W2
for W ∈ R. Indeed SAM iterates equation 20 become Wt+1 = (1 - γ)Wt - γρsign(Wt), and
convergence implies that the limit W∞ would satisfy the fixed point W∞ = -ρ sign(W∞) which
never holds for ρ > 0.
Fact II: Normalized SAM when averaged does not necessarily converge to a flatter minimum.
Let us consider the following simple function
1/2W2 ifW ≥ 0
L(W) = 1/6W6 ifW<0.
When applied to this function with γ = ρ = 1, starting from W0 = -1, then it is experimentally
observed that the averaged of the iterates (20) converge to a positive point W∞ = 0.0149 which is
sharper than the origin, i.e, SL1 (W∞) = 0.515 > SL1 (0) = 0.5.
Fact III: Normalized SAM can stay stuck at non-stationary points of the function it optimizes.
For any ρ, there exists a non-convex function LP and a point WP such that VLP(WP+ρ /7嗯°卜)=0
and VLρ(Wρ) 6= 0. To see that, just consider the function -0.5x2 and Wρ = ρ.
Performance of the unnormalized SAM for deep networks. Given the potential shortcomings
of the normalization step in SAM, it is interesting to compare the generalization properties of the
normalized and unnormalized versions. For this, we repeat the experiments in Sec. 3.3 and Sec. 4 but
for the unnormalized formulation in Fig. 10 with a grid search over P. We can observe that although
the optimal P is not the same for both formulations (P = 0.3 vs P = 0.1 without label noise and
P = 0.4 vs P = 0.2 with 60% label noise), the test error of the normalized vs unnormalized variants
is very similar: 3.69% vs 3.70% without label noise, and 10.03% vs 10.29% with 60% label noise.
Thus, given the same generalization performance, the unnormalized version may be preferred due to
the lack of the shortcomings described above.
27
Under review as a conference paper at ICLR 2022
C Experimental details
Experiments on deep networks. In all experiments, we train deep networks using SGD with step
size 0.1 and momentum 0.9, and '2-regularization parameter λ = 0.0005. We use three datasets:
CIFAR-10 Krizhevsky & Hinton (2009), CIFAR-100 Krizhevsky & Hinton (2009), and two-class
CIFAR-10 where we select two random classes (horse and car). We train models with basic data
augmentations: random image crops and mirroring. We use a pre-activation ResNet-18 (He et al.,
2016) with a width factor 64 and piece-wise constant learning rates (with a 10-times decay at 50%
and 75% epochs) for all experiments except Table 2 where we follow the setup of (Huang et al., 2020)
and use ResNet-34 with cosine learning rates with one cycle. We train all models for 1000 epochs
except the experiments in Tables 2, and Fig. 13 for which we use 200 epochs as they require more
expensive training (due to adversarial training) or an expensive grid search over each dataset and
label noise amount.
For all experiments involving SAM, we select the best ρ based on a grid search over ρ ∈
{0.025, 0.05, 0.1, 0.2, 0.3, 0.4}. In the fine-tuning ERM with SAM experiment, we used ρ = 0.2.
We do not use the m-sharpness (Foret et al., 2021) since we were not able to reproduce exactly the
results of the SAM paper with batch size of 256 and m = 32 (as suggested by Foret et al. (2021)) for
the noisy label experiments. Instead, we observed that the same performance can be achieved using a
smaller batch size 128 and m = 128, thus we opted for this setting which has an advantage of being
significantly faster (thus, we could perform a detailed grid search for each setting) if only one GPU is
available per run.
For model selection to determine the best epoch early stopping and the best ρ, we use 10% of
examples from the noisy training set as the validation set following (Zhang & Sabuncu, 2018). We
emphasize that we do not assume access to a clean validation set to perform early stopping.
For stochastic weight averaging (SWA), we use an exponential moving average with parameter
τ = 0.999, and we update the moving average every iteration. Moreover, we start SWA from the
very beginning of training (e.g., similarly to Rebuffi et al. (2021)) which is not the same setting as
proposed in Izmailov et al. (2018) who start SWA from a specific epoch, use a different moving
average coefficient, and a modified learning rate schedule. We deviate from the setting of Izmailov
et al. (2018) since it has much more hyperparameters than the single moving average parameter τ ,
and those hyperparameters have to be chosen differently for the noisy label setting compared to the
standard setting of Izmailov et al. (2018).
For experiments with the generalized cross-entropy loss (GCE) Zhang & Sabuncu (2018) we use
q = 0.7 as recommended in Zhang & Sabuncu (2018) unless mentioned otherwise.
For experiments with '∞ adversarial training, We use e = 8/255 as in Madry et al. (2018); Rice et al.
(2020), 5 iterations of Projected Gradient Descent (PGD) for training and 10 iterations for testing
(following Rice et al. (2020)), step size 2/nsteps for training (as in Madry et al. (2018)) and step
size /4 for evaluation (as in Croce & Hein (2020)). Note that we use PGD with 10 iterations to
get an understanding of the qualitative behavior over epochs without using expensive computations.
However, to strengthen the evaluation, we also use AutoAttack Croce & Hein (2020) to assess the
robustness of the best model over training. The model selection is performed via the validation robust
error obtained via 10-iteration PGD.
Experiments on a linear model. Here we provide details for the experiments shown in Fig. 4. We
train a logistic regression model without regularization using full-batch gradient descent. We use 50
training examples which are sampled from two Gaussians in a 100-dimensional space. Each Gaussian
has the standard deviation σ = 0.1, and the mean of each Gaussian μ is sampled randomly from the
unit sphere. For optimization, we initialize the weight vector randomly on the sphere of radius 1/3
and use 50 iterations of gradient descent with a step size 2.0. We use ρ = 5.0 for SAM.
Computing infrastructure. We perform all our experiments with deep networks on NVIDIA V100
GPUs with 32GB of memory.
28
Under review as a conference paper at ICLR 2022
Interpolation	Interpolation
Figure 11:	Loss interpolations of wSAM vs wERM and wSWA vs wERM. We see that the loss interpolation
for SAM is qualitatively different from SWA.
D	Additional experiments
Differences between SAM and Stochastic Weight Averaging. Stochastic Weight Averaging (SWA)
(Izmailov et al., 2018) is a weight averaging technique that improves generalization and, similarly
to SAM, also motivated from the perspective of sharpness of the loss, thus it is natural to ask
whether SAM shares some similarities with SWA. To study this, we plot the loss interpolations
between the weights of SAM, ERM, and SWA models (wSAM, wERM, wSAM) in Fig. 11 which
suggest that SAM and ERM converge to different minima. This is further supported by the fact that
kwSAM - wSWAk2 is large, i.e., ≈ 1/3 of the distance of wSAM to its initialization. Interestingly,
there is a peak between wSAM and wERM which is, however, not too high and the model at the peak
still has non-trivial test error (around 40%). This is in contrast to wSWA which is located in the
same valley as wERM with a very sharp increase close to werm (similarly to the interpolations from
Izmailov et al. (2018)). We also note another difference between SAM and SWA in the context of the
fine-tuning experiment from Fig. 2: fine-tuning an SWA model will lead to slight overfitting as it
would converge to a standard ERM model (Izmailov et al., 2018) contrary to SAM which improves
generalization.
Does the loss derivative change explain the behavior of a non-linear model? Here we provide
more details on the experiment from Sec. 4 about the importance of changing the loss derivative
' (yi ∙ fxi (w)) with SAm as opposed to the direction of the individual gradients Nfxi (W).
We train a pre-activation ResNet-18 (He et al., 2016) with a width factor 16 (instead of the standard
64) on a two-class CIFAR-10 dataset (horse vs car classes) using batch size 256 and mSAM = 64
which refers to the m-sharpness defined in Foret et al. (2021)). We note that this experiment would
be more difficult to perform for the multi-class setting due to the fact that '0(yi ∙ fχ(wsam)) will
not be a scalar. In order to select ρ for each of the SAM-based methods, we performed a grid search
over {0.001, 0.003, 0.01, 0.03, 0.1, 0.2}. We train each model three times and report the average test
error (obtained with early stopping on the validation set) with the standard deviation.
In Fig. 12, we plot the test error of all the four methods over epochs. We can see that although
the test error for SAM derivative is oscillating (unlike for other methods), the best test error over
epochs (7.45% ± 0.28) is lower than for other methods and close to that of SAM (9.02% ± 0.34).
We emphasize that all reported results are obtained via proper model selection over epochs using a
validation set. To conclude, we can see that by discarding the direction of SAM the test errors start to
oscillate but we still preserve the improved generalization if we use early stopping.
Beneficial effect of combining SAM with GCE. We check how predictive is our explanation about
the advantage of using SAM with a robust loss such as GCE (Zhang & Sabuncu, 2018) instead of
CE to better leverage the early-learning phenomenon. We validate this hypothesis experimentally
using a ResNet-34 network trained on CIFAR-10 and CIFAR-100 datasets and present results in
Table 2 for {40%, 60%} label noise (for the further details see App. C). We present results from
the literature (Huang et al., 2020) where all of them use ResNet-34 except MentorNet which uses
WRN-28-10. Early stopping based on a noisy validation set is done for each method except those
reported from the literature. We can see that SAM indeed performs better with GCE than with CE
which leads to competitive results. Moreover, the results can be further enhanced by using weight
averaging (Izmailov et al., 2018) where it outperforms many recently proposed approaches including
29
Under review as a conference paper at ICLR 2022
10%
0	20	40	60	80	1∞
Epoch
10%
O 20	40	60	80	100
Ep∞h
(b)	Random seed 2
10%
O 20	40	60	80	100
Epoch
(c)	Random seed 3
(a)	Random seed 1
Figure 12:	Test error over epochs for ERM, SAM direction, SAM derivative, and SAM for three
random seeds. Although the test error for SAM derivative is oscillating, the best test error over epochs
(dashed line) is lower than for other methods.
Table 2: Test error of CE + SAM and GCE + SAM under label noise compared to methods from the
literature. Using SAM with the GCE loss works better against label noise compared to using SAM
with the CE loss as predicted by our gradient reweighting interpretation of SAM.
% label noise	CIFAR-10		CIFAR-100	
	40%	60%	40%	60%
SCE(WANG ETAL., 2019)	13.26%	19.20%	33.59%	42.57%
MentorNet (Jiang et al., 2018)	11.00%	-	32.00%	-
DAC (Thulasidasan et al., 2019)	9.29%	13.70%	33.08%	42.83%
S elf-Adaptive Training (Huang et al., 2020)	7.36%	10.77%	28.62%	37.31%
CE	17.06%	23.06%	44.82%	58.30%
CE + SAM	7.29%	10.98%	36.79%	43.02%
GCE + SAM	7.18%	10.41%	28.82%	37.32%
GCE + SAM + weight averaging	7.62%	9.74 %	27.79%	35.91%
Self-Adaptive Training (Huang et al., 2020). We also note that it can be further combined with other
successful approaches against label noise such as MixUp (Zhang et al., 2017), bootstrapping (Reed
et al., 2014), and semi-supervised learning approaches (Li et al., 2020a).
Mitigating robust overfitting. To illustrate how techniques from label noise can be transferred to
adversarial training and improve upon robust overfitting, we benchmark here two approaches: (1)
generalized cross-entropy (GCE) loss and (2) semi-supervised pairing terms (Laine & Aila, 2016;
Luo et al., 2019). We train all methods for 200 epochs as in Rice et al. (2020). To the best of our
knowledge, these approaches have not been used to prevent robust overfitting.
In Fig. 13a, we show the performance of adversarial training with the GCE loss (we use q = 0.9)
instead of the standard cross-entropy loss. We observe that GCE is able to mitigate robust overfitting,
however it does not improve the performance of the best model over epochs compared to the cross-
entropy loss, so its usefulness is limited.
In Fig. 13b, we show the results of adversarial training with an additional semi-supervised pairing
term (Laine & Aila, 2016) that has been used to mitigate label noise in, e.g., Luo et al. (2019) (see
also Li et al. (2020a) for a more advanced approach). Specifically, let f be the logits of the network,
Xadv be the '∞ adversarial example generated for sample Xi using the PGD attack and Xi be a version
of xi with a different augmentation. Then the additional term that we use in the objective is:
1n	2
λ ∙而 EIIf(Xadv )-f(χi)∣∣2,	(21)
n i=1
which we use together with the cross-entropy on adversarial examples as in standard adversarial
training (Madry et al., 2018). We use λ = 1.0 and a “warmup” scheme for the regularizer which
consists in starting the regularizer after 50% of training epochs which is a standard practice (Laine &
Aila, 2016; Luo et al., 2019). Both λ and the starting epoch were determined via a grid search. We
show in Fig. 13b the robust error over epochs given by PGD with 10 iteration of adversarial training
with and without the pairing term. We can observe that adding the pairing term mitigates the robust
overfitting trend, and leads to improved robust error.
30
Under review as a conference paper at ICLR 2022
Epoch
Epoch
(a)	Adversarial training without/with GCE
(b)	Adversarial training without/with `2 pairing
Figure 13:	Robust error over epochs for standard adversarial training compared to adversarial training
with GCE and with the `2 pairing term.
To make sure that the improvement is not due to gradient masking (Carlini et al., 2019), we evaluate
the best model additionally with AutoAttack (Croce & Hein, 2020). The model trained with the
pairing term achieves 51.15% robust error while the standard adversarial training leads to 52.56%
robust error and adversarial training with SAM achieves 50.07% robust error. Thus, we conclude that
the techniques from the label noise literature (beyond simple early stopping (Rice et al., 2020)) can be
successfully transferred to improve upon robust overfitting. Moreover, we note that more advanced
pairing terms are possible, e.g. involving temperature scaling of the logits (Berthelot et al., 2019),
more advanced data augmentations (Sohn et al., 2020), and other improvements over the standard
pairing (Li et al., 2020a). We leave applications of more advanced pairing techniques to improve
upon robust overfitting as future work.
Finally, the improvement from the pairing term may also be connected to the fact that the TRADES
objective (Zhang et al., 2019) often works better than standard adversarial training (see Gowal
et al. (2020) for an exhaustive empirical study). The approach of TRADES also adds an additional
pairing term that does not rely on the ground truth labels to the standard cross-entropy loss (although
TRADES uses a different loss and does not use a different augmentation on x) This suggests that
even some of the existing successful techniques can be interpreted based on the connection between
label noise and robust overfitting.
31