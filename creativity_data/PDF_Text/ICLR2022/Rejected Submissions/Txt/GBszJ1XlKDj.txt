Under review as a conference paper at ICLR 2022
Quasi-Newton policy gradient algorithms
Anonymous authors
Paper under double-blind review
Ab stract
Policy gradient algorithms have been widely applied to reinforcement learning
(RL) problems in recent years. Regularization with various entropy functions is
often used to encourage exploration and improve stability. In this paper, we pro-
pose a quasi-Newton method for the policy gradient algorithm with entropy regu-
larization. In the case of Shannon entropy, the resulting algorithm reproduces the
natural policy gradient (NPG) algorithm. For other entropy functions, this method
results in brand new policy gradient algorithms. We provide a simple proof that
all these algorithms enjoy the Newton-type quadratic convergence near the opti-
mal policy. Using synthetic and industrial-scale examples, we demonstrate that
the proposed quasi-Newton method typically converges in single-digit iterations,
often orders of magnitude faster than other state-of-the-art algorithms.
1	Introduction
Consider an infinite-horizon MDP (Bellman, 1957; Sutton & Barto, 2018) M = (S, A, P, r, γ),
where S is a set of states of the system studied, A is a set of actions made by the agent, P is a
transition probability tensor with Psat being the probability of transitioning from state s to state t
when taking action a, r is a reward tensor with rsa being the reward obtained when taking action
a at state s, and 0 < γ < 1 is a discount factor. Throughout the paper, the state space S and the
action space A are assumed to be finite. A policy π is a randomized rule of action-selection where
πsa denotes the probability of choosing action a at state s. For a given policy π, the value function
vπ defined as
∞
(v∏)s= E X(YkTa | S0 = S)	(1)
k=0
satisfies the Bellman equation:
(I - γPπ)vπ = rπ,	(2)
where (Pπ)st = Pa πsaPsat, (rπ)s = Pa πsarsa, and I is the identity operator.
In order to promote exploration and enhance stability, one often regularizes the problem with a
function hπ such as the negative Shannon entropy (hπ)s = Pa πsa log πsa. With hπ, the original
reward r∏ is replaced with the regularized reward r∏ = r∏ - τh∏ where T is the regularization
coefficient and (2) becomes
(I - γP∏ )v∏ = r∏ = r∏ - τh∏,	(3)
where we overload the notation v for the regularized value function. Other continuously differen-
tiable entropy functions can be used as well, as we will show later. Since γ < 1 and Pπ is a transition
probability matrix, (I - γPπ ) is invertible, and
vπ = (I - γPπ)-1(rπ - τhπ).	(4)
In a policy optimization problem, we seek a policy π that maximizes e>vπ for some (in fact arbi-
trary) positive weight vector e ∈ R|S| . Using (4), the problem can be stated as
max e>(I - γPπ)-1(rπ - τhπ).	(5)
π
This problem can be solved by, for example, the policy gradient (PG) method. However, the vanilla
PG method converges quite slowly. In Agarwal et al. (2020), for example, the vanilla PG method
is shown to have a O(T -1) convergence rate, where T denotes the number of iterations. For the
1
Under review as a conference paper at ICLR 2022
PG method with entropy regularization and some of its variants, the convergence rate can be im-
proved to O(e-cT), i.e., linear convergence (Mei et al., 2020), which can still be slow since c is in
general close to 0. It is also demonstrated in numerical examples that these algorithms with linear
convergence rate can suffer from slow convergence. For example, in the example in Zhan et al.
(2021), thousands of iterations are needed for the algorithm to converge, even though the model is
relatively small and sparse. Therefore, there is a clear need for designing new methods with faster
convergence and one idea is to take the geometry of the problem into consideration. The Newton
method, for example, preconditions the gradient with the Hessian matrix and obtains second-order
local convergence. Since the exact Hessian matrix is usually too computationally expensive to ob-
tain, the quasi-Newton type methods, which use structurally simpler approximations of the Hessian
instead, are more widely used in generic optimization problems, and are known to enjoy superlinear
convergence (Rodomanov & Nesterov, 2021a;b).
1.1	Contributions
In this paper, we investigate the quasi-Newton approach for solving (5). The main contributions of
this paper are the following.
•	First, we present a unified quasi-Newton type method for the policy optimization prob-
lem. The main observation is to decompose the Hessian as a sum of a diagonal part and
a remainder, where at the optimal solution the remainder part vanishes so that the Hes-
sian becomes diagonal. This inspires us to use only the diagonal part in the quasi-Newton
method. As a result, the proposed method not only leverage the second order information
but also enjoys low computational cost due to the diagonal structure of the preconditioner
used. When the negative Shannon entropy is used, this method reproduces the natural pol-
icy gradient (NPG) algorithm. For other entropic regularizations, this method results in
brand new policy gradient algorithms.
•	Second, we analyze the convergence property of the proposed quasi-Newton algorithms and
demonstrate local quadratic convergence both theoretically and numerically. By leveraging
the quasi-Newton framework (Dennis & More, 1974), We provide a simple and Straight-
forward proof for quadratic convergence near the optimal policy. In the numerical tests,
we verify that the proposed method leads to fast quadratic convergence even under small
regularization and large discount rate (close to 1). Even for industrial-size problems with
hundreds of thousands states, the quasi-Newton method converges in single-digit iterations
and within a few minutes on a regular laptop.
1.2	Background and related work
A major workhorse behind the recent success in reinforcement learning (RL) is the large family
of policy gradient (PG) methods (Williams, 1992; Sutton et al., 1999), for example, the natural
policy gradient (NPG) method (Kakade, 2001), the actor-critic method (Konda & Tsitsiklis, 2000),
the asynchronous advantage actor-critic (A3C) method (Mnih et al., 2016), the deterministic policy
gradient (DPG) method (Silver et al., 2014), the trust region policy optimization (TRPO) (Schulman
et al., 2015a), the generalized advantage estimation (GAE) (Schulman et al., 2015b), and proximal
policy optimization (PPO) (Schulman et al., 2017), to mention but a few. The NPG method is
known to be drastically faster than the original PG method, intuitively because in the NPG method
the policy gradient is preconditioned by the Fisher information (an approximation of the Hessian of
the KL-divergence) matrix and fits the problem geometry better. This idea is extended in TRPO and
PPO where the problem geometry is taken into consideration via trust region constraints (in terms
of KL-divergence) and a clipping function of the relative ratio of policies in the objective function,
respectively. These implicit ways (in the sense that they do not adjust the gradient by an explicit
preconditioner) of adjusting the policy gradient is in essence similar to the mirror descent (MD)
method (Nemirovskij & Yudin, 1983) in generic optimization problems.
This similarity in addressing the inherent geometry of the problem is noticed by a line of recent work
including Neu et al. (2017); Geist et al. (2019); Shani et al. (2020); Tomar et al. (2020); Lan (2021),
and the analysis techniques inMD methods have been adapted to the PG setting. The connection was
first built explicitly in Neu et al. (2017). The authors consider a linear program formulation where
the objective function is the average reward and the domain is the set of stationary state-action
2
Under review as a conference paper at ICLR 2022
distributions, in which case the TRPO method can be viewed as an approximate mirror descent
method and the A3C method as an MD method for the dual-averaging (Nesterov, 2009) objective.
As a complement, Geist et al. (2019) considers an actor-critic type method where the policy is
updated via either a regularized greedy step or an MD step and the value function is updated by a
regularized Bellman operator, which also includes TRPO as a special case, and error propagation
analysis is provided. In Shani et al. (2020), an adaptive scaling that naturally arises in the policy
gradient is applied to the proximity term of the MD formulation, and sublinear convergence result is
proved with a properly decreasing learning rate. In Tomar et al. (2020), the application to the non-
tabular setting is enabled by parameterizing the policy and applying MD to the policy parameters,
and the corresponding sublinear convergence result is presented.
Regularization, a strategy that considers the modified objective function with an additional penalty
term on the policy, is another crucial component in the development of PG type methods. Intu-
itively, regularization is able to encourage exploration in the policy iteration process and thus avoid
local minima. It is also suggested (Ahmed et al., 2019) that regularization makes the optimization
landscape smoother and thus enables possibly faster convergence. Linear convergence result is then
established for regularized PG and NPG methods (Agarwal et al., 2020; Mei et al., 2020; Cen et al.,
2020). In these relatively earlier works (Agarwal et al., 2020; Mei et al., 2020; Cen et al., 2020),
the regularization usually takes the form of (negative) entropy or relative entropy. In the more re-
cent work Lan (2021) and Zhan et al. (2021) that follow the MD type methods, the regularization
is extended to general convex functions with the resulting Bregman divergences different from the
KL-divergence and linear convergence is guaranteed as well.
However, most of these algorithms are of either sublinear or linear convergence except the entropy
regularized NPG with full step length (which is a special case of the quasi-Newton method we
propose), and even the linear convergence rate O(e-cT ) can be slow since c can be close to zero.
This motivates us to invent the quasi-Newton policy gradient method to be introduced in Section 2.
2	Quasi-Newton method
This section derives the quasi-Newton method for the entropic-regularized policy optimization prob-
lems. In this paper, we use a more general definition of the term “quasi-Newton method”. Unlike
classical quasi-Newton methods where the approximate Hessian matrix is constructed using first-
order information, the proposed quasi-Newton method uses second-order information, but the ap-
proximate Hessian matrix remains simple and its inverse is easy to obtain. We start with the negative
Shannon entropy (hπ)s = Pa πsa log πsa.
In What follows, assume that ∏* is the optimizer of the problem stated in (5). By introducing Zn :=
I - γPπ, the objective function can be written as
E(π) = e>(I - γPπ)-1(rπ - τhπ) = e>Zπ-1(rπ - τhπ) = wπ>(rπ - τhπ),	(6)
where w∏ := Z->e. For any E with Pa Ea = 0, introduce re ∈ R1S1 and Ze ∈ RlSl×lSl such that
(re)s:=	Esarsa,	(Ze)st :=	Esa(δst-γPsat),
aa
where δst = 1 if s = t and δst = 0 otherwise.
Let us first outline the main idea of the quasi-Newton method. The gradient of E(π) in R|S||A| is
∂E
∂∏S
(rsa - τ (log πsa + 1) - [(I - γPa)vπ]s + cs)(wπ)s,
(7)
where cs is the Lagrange multiplier associated with the constraint	a πsa = 1. Our key observation
is to decompose the Hessian matrix D2E(∏) in RlSllAl×lSllAl into two parts
D2E(∏) = H(∏)+ Hr (∏),	(8)
where H is a diagonal matrix given by H(§0),(比)= -丁6{(§0),(助} (Wna)S and Hr is a remainder that
vanishes at π = π*, i.e., Hr = O(k∏ - π*k) (shown in Theorem 1). With this decomposition, we
3
Under review as a conference paper at ICLR 2022
♦	.	.1 T T ♦	.	♦	7~λ9 T~! /	∖ F τ~T	1 F ,	♦	,1 CIl ♦	-HT	C
can approximate the Hessian matrix D2E(π) by H and obtain the following quasi-Newton flow:
dπa	∂E
^d^ = TH- v∏E)Sa = -(H(sa),(sa)厂而
=πa(ra - T(log πa + I) - [(I-YPa)Vn]S + cs"τ,
By introducing the parameterization θsa = log πsa and discretizing in time with learning rate η, we
arrive at
θ; 一 η(r; - T - [(I - γPa)v∏]s + CsMT + (i-η)θS.
Writing this update back in terms of πsa leads to the following update rule
π Y (πa)1-η eχp(η(ra + (YPavn)s)/T).
This result is summarized in the following theorem with the proof given in Appendix A.
Theorem 1.	Let hn ∈ R|S| be the negative Shannon entropy (hn)s = Pa πsa logπsa. (a) For any
with Pa Ea = 0 and |琮 | < ∏S ,at π = π*
r - TDhn - ZZn-1(rn - Thn) = 0,	(9)
where Dhn ∈ RlSl×lSllAl is the gradient matrix of h∏ with respect to π. (b) There exists a diagonal
approximation H(π) ofthe HeSSian matrix D2E(π) such that
H(π) — D2E(π) = O(k∏ — π*k).
(c) The quasi-Newton flow from H(π) is
dπa
~π = ∏a(ra - T(log∏ + 1) - [(I - γPa)vn]s + Cs)IT
With learning rate η, the gradient update is
a ,	(πa)1-η exp(η(ra + (YPavn )s )∕τ)
π 《.
S	∑a(πa)1-η exP(η(ra + (YP avn )s)∕τ)'
(10)
(11)
Remark 1. The policy update scheme (11) is the same as the entropy regularized natural policy
gradient scheme in Cen et al. (2020).
The conclusion in Theorem 1 can be extended to general entropies of the form
(hn)s = X φ (μs) μa ,
where φ is convex on (0, +∞) and φ(1) = 0, and μ is a prior distribution over A. The term
(h∏)s is also called the “f-divergence” between ∏s and μ (Renyi, 1961; Ali & Silvey, 1966). In
what follows, we shall use the uniform prior unless otherwise specified. When φ(x) = xlogx,
(hn)s = Pa (μa log μa) μa = Pa ∏a log ∏a - log A and by omitting the constant log PAJ We
recover the regularization used in Theorem 1. When φ(x) = ɪ-θ2(1 一 X(I+α"2) (α < 1),
(h∏ )s = w⅛ X(I-(IAIna)(I+α)/2)
(12)
is the α-divergence. In particular, when α = 0 we obtain the Hellinger divergence (hn)s
篙 after omitting the constants and dividing by 2, and when α → -1 we obtain the
reverse-KL divergence (h∏)s = 备 Pa log ∏a after omitting constants.
In the following theorem, we extend the quasi-Newton method in Theorem 1 to the entropy functions
described above. The proof of this theorem can be found in Appendix B.
Theorem 2.	Assume that π* is the optimizer of (5) where h∏ denotes the α-divergence (12). Then
(a) the Hessian matrix D2E(π) can be approximated by a diagonal matrix H(π) near π* such that
4
Under review as a conference paper at ICLR 2022
H(π) — D2E(π) = O(k∏ — ∏*k) and (b) the quasi-Newton methodfrom H(π) (thePolicygradient
ascent preconditioned by —HH(π)) is
2
πa —	((I- ηXna)α-1	-	--γηηlAI1-αCra	-	[(I	- YPa)Vn]s"τ	+ CS)	,	(13)
where 0 < η ≤ 1 is the learning rate and with parameterization θsa = φ0(lAlπsa), the update scheme
above can be expressed as:
θa 一 η(r; — [(I — γPα)v∏]s + CSMT + (1 — η)θa.	(14)
where φ(x) = ɪ-oɪ(1 一 x(1+α)/2) (α < 1) and CS is the Lagrange multiplier introduced by the
constraint	a πSa = 1.
Remark 2. When α = 0 (the Hellinger divergence), the corresponding update scheme is ∏S J
(√-a — ηp∣A∣(ra — [(I — YPα)v∏]s)/t + CS) , which can be obtained by inserting a = 0
in (13). When α = —1 (the reverse-KL divergence), the corresponding update scheme is πSa J
(1-η — η∣A∣(rS — [(I — YPa)v∏]s)/t + CS) , which can be obtained by inserting α = -1.
Remark 3. When a different prior μ is used, the CorreSPonding algorithm can be obtained by
inserting θ= = φ0(∏α∕μα) in (14).
Remark 4. All the policy update schemes can be extended to the case where Y = 1 and the MDP
has a terminal state sT and the Markov chain is irreducible. In that case we can remove sT from S
and replace Y with 1 in the update schemes. For example, the update scheme (11) becomes
πSa J
,_. -l , _ , ~ _ ...
(∏a)1-η exp(η(ra + (Pαv∏)s))
,
∑α (∏a)1-η eχp(η(ra + (P ɑv∏ %))
where P is the SUbmatrix of P with the row and column COrreSPOnding to ST removed.
The remaining problem in the update schemes above is the determination of the multipliers CS, since
they cannot be solved explicitly as in the case of the negative Shannon entropy (Theorem 1). The
determination of CS can be done in a similar way as in Ying (2020) based on the following lemma.
The proof of this lemma can be found in Appendix C.
Lemma 3. Assume that σ < 0, then for any x1 , x2 , . . . , xk, there is a unique solution to the equa-
tion:
(x + χι)σ + •…+ (x + Xk )σ = 1,	(15)
such that x + xi ≥ 0, i = 1, 2, . . . , k. Moreover, the solution is on the interval
max < — min Xi, k-σ — max Xj > , k-1∕σ — min Xi .	(16)
1≤i≤k i	1≤j≤k j	1≤i≤k i
By Lemma 3 and the monotonicity of (x + xι)σ + •… + (x + Xk)σ — 1, many of the established
numerical methods (e.g. bisection) for nonlinear equations can be applied to determine the solution
for (15). This routine can be used to find the multipliers CS as stated in Proposition 4 whose proof is
given in Appendix D.
Proposition 4. The multipliers CS in the update scheme (13) can be determined uniquely such that
the updated policy π satisfies πSa ≥ 0for any (s, a), and	a πSa = 1for any s.
The algorithm proposed in this section is summarized in Algorithm 1 below.
3 Quadratic Convergence
In this section, we study the quadratic convergence of the quasi-Newton method at learning rate
η = 1. Our analysis is inspired by the results in Dennis & More (1974); Wang & Yan (2021).
The following theorem states the second-order convergence when η = 1, with the proof given in
Appendix E.
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Quasi-Newton method for the regularized MDP
Require: the MDP model M = (S, A, P, r, γ), initial policy πinit, convergence threshold tol, reg-
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
ularization coefficient τ , learning rate η, the regularization type (KL, reverse-KL, Hellinger or
α-divergence).
Initialize the policy π = πinit.
Set q = 1 + tol and k = |A|.
while q > tol do
Calculate the regularization term h∏ by (h∏)s = 备 Pa φ(∣A∣∏a).
Calculate Pπ and rπ by (Pπ )st = Pa πsaPsat, (rπ )s = Pa πsarsa.
Calculate vπ by (4), i.e., vπ = (I - γPπ)-1(rπ - τhπ).
if the KL divergence is used then
(∏new)a 一	P(πa )tX-exp(η(ra+” “蓝)s")、for a = 1, 2,..., |A|,	S = 1, 2,...,	|S |.
∖ new ) s	工 a(∏a )1 η exp(η(ra + (γPa Vn )s)∕τ)	, ,	,l l,	, ,	,	I I
end if
if the α-divergence is used then
for s = 1, 2, . . . , |S | do
Set σ = 2∕(α - 1).
CalculateXa = (1-η)(πs)α-1 - 1-αη∣A∣ 1-α(rs-[(I-γPa)v∏]s)∕τ, a = 1,...,∣A∣.
Solve for cs with the bisection method on the interval described in (16).
UPdate (πnew)s4-(CS + Xa)" for a = 1, 2, . . . , |A|.
end for
end if
q = kπnew - πkF∕kπkF.
π = πnew
end while
Theorem 5. Let η = 1, f (π)sa = -(rsa - ((I - γPa)vπ)s) and
φKL(π) = X πa log πa, φrKL(π) =击 X log ∏a,
sa	sa	s
ɪ__	/ Πa	4	W~,	/. , ∖，C
φH (∏) = -2 X J而，φα(∏) = |A|(1- a2) X(1 - (|Aka)(I+a)/2), (α < 1).
sa	sa
Denote the k-th policy obtained in Algorithm 1 by π(k), then for η = 1 the update scheme in
Algorithm 1 can be summarized as
VΦ(π(k+1)) - VΦ(π(k)) = - ff (π(k)) + VΦ(π(k)) - B>c(π(k))) ,	(17)
where we denote by B the |S|-by-(|S| × |A|) matrix such that Bij = 1 for |A|(i - 1) + 1 ≤ j ≤ |A|i
and Bij = 0 otherwise. Define (θ(k))a = φ0(∣A∣(π(k))a), where φ(x) is the Convexfunction used in
the regularization (h∏)s =5 Ea φ(∣A∣∏a). Assume that(1) Vf is Lipschitz on a closed subset of
{π : B>π = 1∣s∣, ∏a > 0} and (2) θ(k) → θ*. Then n(k) converges quadratically to π*, i.e.,
IIn(k+1) - π*∣∣ ≤ CIIn⑹-π*∣∣2,	(18)
for some constant C.
Connection with mirror descent The quasi-Newton algorithm (17) for η = 1 has a deeP con-
nection with mirror descent. The vanilla mirror descent of -E(n) with a learning rate β and the
Bregman divergence associated with Φ is given by
n(k+1) = argmin{-E(n(k)) - VE(n(k))(n - n(k)) + 1(Φ(n) - Φ(n(k)) - VΦ(n(k))(n - n(k)))}
πβ
=argmin{(diag(w∏(k))乳 I∣a∣)(f Wk)) + VΦ(n(k)))(n - n(k)) + 1(Φ(n) - VΦ(n(k))n)},
πβ
(19)
6
Under review as a conference paper at ICLR 2022
where diag(wπ(k) ) is the diagonal matrix with the diagonal equal to wπ(k) := (I - γPπ>(k) )-1e,
0 denotes the Kronecker product, and I∣a∣ denotes the identity matrix with size |A| by |A|. In the
last equality, the terms independent of ∏ are dropped and the multiplier term in VE is canceled out
using Bπ = Bπ(k) = 1|S|. The first order stationary condition of this minimization problem reads
VΦ(π(k+1)) - VΦ(π(k)) = -β(diag(wπ(k)) 0 I|A|)(f (π(k)) + VΦ(π(k)) - B>c(π(k))). (20)
This suggests that (17) can be reinterpreted as an accelerated mirror descent method with adaptive
learning rates βs ≡ 1∕(w∏(k) )s that depend on the state S and the current iterate ∏(k).
In Zhan et al. (2021), a variant of mirror descent is proposed based on an implicit update scheme
(VΦ(π(k+1)))sa - (VΦ(π(k)))sa = -β0 f(π(k))sa + VΦ(π(k+1))sa - (c(π(k)))s,	(21)
with a manually tuned learning rate β0 . In the next section, we will compare this variant with our
quasi-Newton method (17) in several examples and show that the quasi-Newton method converges
orders of magnitudes faster than the ones in Zhan et al. (2021).
4 Numerical experiments
4.1	Experiment I
We first test the quasi-Newton methods derived in Section 2 on the model in Zhan et al. (2021). For
the sake of completeness, we include the model description here. The MDP considered has a state
space S of size 200 and an action space A of size 50. For each state t and action a, a subset Sta of S
is uniformly randomly chosen such that |Sta| = 20, and Ptat0 = 1/20 for any t0 ∈ Sta. The reward is
given by rsa = UsaUs, where Usa and Us are independently uniformly chosen on [0, 1]. The discount
rate γ is set as 0.99 and the regularization coefficient τ = 0.001.
In the numerical experiment, we implement Algorithm 1 with the KL divergence, the reverse KL
divergence, the Hellinger divergence and the α-divergence with α = -3. We set the initial policy as
the uniform policy, the convergence threshold as = 10-12 and the learning rate η as 1. Figure 1(a)
demonstrates that, for these four tests, the quasi-Newton algorithm converges in 7, 7, 7, and 6
iterations, respectively. In comparison, we apply the policy mirror descent (PMD) and the general
policy mirror descent (GPMD) method in Zhan et al. (2021) to the same MDP with the same stopping
criterion. As also shown in Figure 1(a), many more iterations are needed for GPMD and PMD to
reach the same precision: GPMD converges in 14822 iterations, and PMD does not reach the desired
precision after 3 × 105 iterations.
In order to verify the quadratic convergence proved in Section 3, we draw the plots of
log |log ∣∣∏ 一 ∏*∣∣f | in Figures 1(b), 1(c), 1(d) and 1(e), where ∏* is the final policy and the norm
used is the Frobenius norm. A green reference line with slope log 2 through the origin is plotted for
comparison. If the error converges exactly at a quadratic rate, the plot of log |log ∣∣∏ 一 ∏*∣∣ shall
be parallel to the reference line. The convergence curves approach the reference lines in the end
(and are even steeper than the reference lines in the begining), demonstrating clearly a quadratic
convergence for all regularizations used here.
4.2	Experiment II
Next, we apply the quasi-Newton methods derived in Section 2 to an MDP model constructed from
the search logs of an online shopping store, with two different ranking strategies. Each issued query
is represented as a state in the MDP. In response to a query, the search can be done by choosing
one of the two ranking strategies (actions) to return a ranked list of products shown to the customer.
Based on the shown products, the customer can refine or update the query, thus entering a new state.
The reward at each state-action pair is a weighted sum of the clicks and purchases resulting from the
action. Based on the data collected from two separate 5-week periods for both ranking strategies, we
construct an MDP with 135k states, and a very sparse transition tensor P with only 0.01% nonzero
entries. The discount rate γ is set as 0.99 and the regularization coefficient is τ = 0.001.
When calculating vπ by vπ = (I 一 γPπ)-1(rπ 一 τhπ), we apply the iterative solver Bi-CGSTAB
(Van der Vorst, 1992), a widely used numerical method with high efficiency and robustness for
7
Under review as a conference paper at ICLR 2022
AɑodO① 6ueιp φ>leφ一
(a) Relative change of the policy
using Algorithm 1 and methods
from Zhan et al. (2021).
(b) The policy error in the process
of training using KL-divergence.
(c) The policy error in the pro-
cess of training using reverse KL-
divergence.
(d) The policy error in the process
of training using Hellinger diver-
gence.
(e) The policy error in the process
of training using α-divergence
with α = -3.
Figure 1:	Figures for the synthetic medium scale MDP. (a): Relative change of the policy kπnew -
π kF /kπ kF during training of Algorithm 1 compared with PMD and GPMD in Zhan et al. (2021),
with the logarithmic scale used for both axes. Notice that Algorithm 1 converges in 6-7 iterations
to 10-12 in all cases while PMD and GPMD take more than 104 iterations. (b) - (e): Blue: The
convergence of log |log ∣∣π - π* kF| in the training process with the KL divergence, the reverse KL
divergence, the Hellinger divergence and the α-divergence with α = -3, respectively. Green: A line
through the origin with slope log 2. Comparison of the convergence plots with the green reference
lines shows a clear quadratic convergence for Algorithm 1.
solving large sparse nonsymmetric systems of linear equations (Saad, 2003; de Pillis, 1998), in
order to leverage the sparsity of the transition tensor.
In the numerical experiment, we implement Algorithm 1 with the KL divergence, the reverse KL
divergence, the Hellinger divergence and the α-divergence with α = -3. We set the initial policy
as the uniform policy, the convergence threshold as = 10-12 and the learning rate η as 1. All the
tests end up with fast convergence as shown in Figure 2(a), where logarithmic scale is used for the
vertical axis. More specifically, the quasi-Newton algorithm using the KL divergence, the reverse
KL divergence, the Hellinger divergence and the α-divergence with α = -3 converge in 6, 6, 6, 5
iterations, respectively. It is worth noticing that even though the size of the state space S here is
some magnitudes larger than the examples in Section 4.1, the number of quasi-Newton iterations
used is about the same.
In Table 1, we report the number of BiCGSTAB steps used in the algorithm. In each quasi-Newton
iteration, less than 20 BiCGSTAB steps are used in order to find vπ . For all the 4 regularizers used
here, altogether only about 100 BiCGSTAB steps are needed in the whole training process, thanks
to the fast convergence of the quasi-Newton method.
Regularizer	KL	reverse-KL	Hellinger	α-divergence (α =	-3)
Quasi-Newton Iterations	6	6	=	6	5	
Total Bi-CGSTAB steps	110	109	110	83	
Average Bi-CGSTAB steps	18.3	18.2 —	18.3	16.6	
Table 1: Number of quasi-Newton iterations and BiCGSTAB steps used in the training process.
8
Under review as a conference paper at ICLR 2022
AɑodO① 6ueιp φ>leφ一
(a) Relative change of the policy
kπnew - π kF /kπkF in the training
process.
(b) The policy error in the process (c) The policy error in the pro-
of training using KL-divergence. cess of training using reverse KL-
divergence.
(d) The policy error in the process
of training using Hellinger diver-
gence.
(e) The policy error in the process
of training using α-divergence
with α = -3.
Figure 2:	Figures for the synthetic medium scale MDP. (a): Relative change of the policy
kπnew - π kF /kπ kF in the training process of Algorithm 1. Logarithmic scale is used for the vertical
axis. (b) - (e): Blue: The convergence of log |log ∣∣π - π* kF| in the training process with the KL
divergence, the reverse KL divergence, the Hellinger divergence and the α-divergence with α = -3,
respectively. Green: A line through the origin with slope log 2.
As in the previous numerical example, in Figure 2(b), 2(c), 2(d) and 2(e) we verify the quadratic
convergence by comparing the plot of log |log kπ - π * k| with a green reference line through the
origin with slope log 2. As the convergence curves are approximately parallel to the reference lines,
this verifies that the proposed algorithm converges quadratically with all the regularizations in this
example as well.
5 Discussion
In this paper, we present a fast quasi-Newton method for the policy gradient algorithm that is capable
of solving large MDP problems with large (≈ 1) discount rate and small (≈ 0) regularizations. The
proposed method includes the well-known natural policy gradient algorithm as a special case, and
naturally extends to other regularizers such as the reverse KL divergence, the Hellinger divergence
and the α-divergence.
Theoretically, we show local quadratic convergence of the proposed quasi-Newton method with a
relatively simple proof. This quadratic convergence is confirmed numerically on both medium and
large sparse models. In contrast with mirror descent type methods (e.g. Zhan et al. (2021)) that
take up to tens of thousands iterations even with manually tuned learning rate, the proposed quasi-
Newton algorithms typically converge in less than 10 iterations, despite the large discount rate (close
to 1) and small regularization coefficient (τ ≈ 0).
For future work, we plan to adapt the technique used here to other gradient based algorithms for
solving the MDP problems. Other types of f -divergence can also be included. Another direction
is to consider continuous MDP problems by leveraging function approximation, effective spatial
discretization, or model reduction.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in Markov decision processes. In Conference on Learning Theory,
pp. 64-66. PMLR, 2020.
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the
impact of entropy on policy optimization. In International Conference on Machine Learning, pp.
151-160. PMLR, 2019.
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distri-
bution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):
131-142, 1966.
Richard Bellman. A Markovian decision process. Journal of mathematics and mechanics, 6(5):
679-684, 1957.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558,
2020.
Lisette G de Pillis. A comparison of iterative methods for solving nonsymmetric linear systems.
Acta Applicandae Mathematica, 51(2):141-159, 1998.
John E Dennis and Jorge J More. A characterization of superlinear convergence and its application
to quasi-Newton methods. Mathematics of computation, 28(126):549-560, 1974.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision
processes. In International Conference on Machine Learning, pp. 2160-2169. PMLR, 2019.
Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,
14, 2001.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. arXiv preprint arXiv:2102.00135, 2021.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820-6829. PMLR, 2020.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.
Arkadij Semenovic Nemirovskij and David Borisovich Yudin. Problem Complexity and Method Ef-
ficiency in Optimization. A Wiley-Interscience publication. Wiley, 1983. ISBN 9780471103455.
Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming,
120(1):221-259, 2009.
Gergely Neu, Anders Jonsson, and ViCenC Gomez. A unified view of entropy-regularized Markov
decision processes. arXiv preprint arXiv:1705.07798, 2017.
Alfred Renyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics, pp. 547-561. University of California Press, 1961.
Anton Rodomanov and Yurii Nesterov. New results on superlinear convergence of classical quasi-
Newton methods. Journal of optimization theory and applications, 188(3):744-769, 2021a.
Anton Rodomanov and Yurii Nesterov. Rates of superlinear convergence for classical quasi-Newton
methods. Mathematical Programming, pp. 1-32, 2021b.
10
Under review as a conference paper at ICLR 2022
Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 5668-5675, 2020.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387-395. PMLR, 2014.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057-
1063, 1999.
Manan Tomar, Lior Shani, Yonathan Efroni, and Mohammad Ghavamzadeh. Mirror descent policy
optimization. arXiv preprint arXiv:2005.09814, 2020.
Henk A Van der Vorst. Bi-cgstab: A fast and smoothly converging variant of Bi-CG for the solution
of nonsymmetric linear systems. SIAM Journal on scientific and Statistical Computing, 13(2):
631-644, 1992.
Li Wang and Ming Yan. Hessian informed mirror descent. arXiv preprint arXiv:2106.13477, 2021.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229-256, 1992.
Lexing Ying. Mirror descent algorithms for minimizing interacting free energy. Journal of Scientific
Computing, 84(3):1-14, 2020.
Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason D Lee, and Yuejie Chi. Policy
mirror descent for regularized reinforcement learning: A generalized framework with linear con-
vergence. arXiv preprint arXiv:2105.11066, 2021.
11
Under review as a conference paper at ICLR 2022
A Proof of Theorem 1
Theorem 1. Let hπ ∈ R|S| be the negative Shannon entropy (hπ)s = Pa πsa logπsa. (a) For any
with Pa Ea = 0 and |琛 | < ∏S, at π = π*
r - τDhπ - ZZπ-1 (rπ - τhπ) = 0,	(9)
where Dhn ∈ RlSl×lSllAl is the gradient matrix of h∏ with respect to π. (b) There exists a diagonal
approximation H(π) of the Hessian matrix D2E(π) such that
HH(π) — D2E(π) = O(k∏ — π*k).
(c) The quasi-Newton flow from H (π) is
dπa
-ds- = ∏a(ra -T(log∏α + 1) - [(I - γPa)v∏]s + CS)IT
With learning rate η, the gradient update is
a ,	(πS)1-η exp(η(ra + (YPav∏)S)∕τ)
∏ 《-------------------------------------.
s	a(πsa)1-ηexp(η(rsa + (γPavπ)s)∕τ)
(10)
(11)
Proof. Since π is a policy,	a πsa = 1 for any s. Thus
(Zπ )st = δst - γXπsaPsat =Xπsa(δst -γPsat).	(22)
aa
Now consider a policy π + E close to π, i.e., Pa Esa = 0 and |Esa|	πsa, then by (22),
Zπ+ = Zπ + Z, rπ+ = rπ + r.	(23)
With (23), we have
E(π + E)
= e Zπ-+ (rπ+ - τ hπ+ )
= e>(Zπ + Z)-1(rπ + r - τhπ+)
=e>(Z-1 - Z-IZeZ-I + Z-IZeZ-IZeZ-1)(rπ + Ire- τhπ - TDhn E- g e>τD2hπE) + o(e2)
≈ e>Zπ-1(rπ - τhπ) + e> -Zπ-1ZZπ-1(rπ - τhπ) + Zπ-1(r - τ Dhπ E)
+ e> Z-1(-5E>τD2hnE) + (-Z-1ZeZ-1)(re - TDhnE) + (Z-IZeZ- 1 ZeZ- 1 )(rn - Thn) ,
(24)
where we omit the O(kEk3) term and keep the first two orders in the last step, and Dhn is a second-
order tensor that maps from S X A to S, and D2h∏ is a third-order tensor that maps from (S X A)02
to S. With this expansion, we can see that
∂E
研= (rα - τ(log ∏a + 1) - [(I -YPa)Vn]s + CS)(Wn)s,
∂ πs
where Wn = Z->e and CS(Wn)s is the Lagrange multiplier. Then at π = π*,
∂E
∂-a = (IrS — T(log πs +1) — [(I — YP )vn]s + CS)(Wn)s = 0∙
∂ πS
(25)
Since wn = (I - γPn>)-1e = e + Pi∞=1 γi(Pn>)ie and all elements of e are positive, we also know
that all elements of Wn are positive, thus at ∏ = ∏*,
rSa - T (log πSa + 1) - [(I - YPa)vn]S + CS = 0.
Multiplying the left hand side with ESa and taking the sum over a we obtain:
(re - TDhnE - Zevn )S + CS	ESa = 0,	∀s,	∀E,
a
12
Under review as a conference paper at ICLR 2022
and since Pa sa = 0 for any s and vπ = Zπ-1(rπ - τhπ), we have
r - τDhπ - ZZπ-1 (rπ - τhπ) = 0, ∀,
at ∏ = ∏*, which proves (9).
We can now simplify the second-order term in (24) by approximating r - τDhπ - ZZπ-1 (rπ -
τh∏) with 0 for π near π*,
e> Z-1(- 2 e>τD2hπ E) + (-Z-1 ZeZ-1)(re - TDhnE) + (Z-IZeZ-IZeZ-1)(rπ - τhπ )
=e> Z-I (-2 E> τD2 hπ E)- Z-IZeZ-1 (re - TDhn E- ZeZ-I (rπ - Thn ))
≈ e>Z∏ 1(- 2 e> τ D2 h∏E) = 2E>H(n)E.
(26)
By (9) and the twice continuous differentiability of h, it is clear that the approximate Hessian H
converge to the true Hessian as ∏ converges to ∏*, and H(∏) — D2E(∏) = O(IIn — ∏*k). The
second-order derivatives are approximately given by
d2E 〜Er	— 一 S	(Wn)s	，cr、
dπadπb 〜H(sa),(tb) = -τδ{(sa),(tb)}	，	(27)
from which we have shown that HE is diagonal.
Using this approximate second-order derivative as preconditioner, wn is canceled out in the policy
gradient algorithm, which becomes
~dtL = πa(ra - T(log πa + 1) - [(I - YPa)v∏]s + cs)∕τ∙
Adopting the parameterization πsa = exp(θsa ), we have
dθa
甫= (rα - T叱 + 1) - [(I - γPa)v∏]s + Cs)∕τ.	(28)
With a learning rate η , this becomes
θS 一 η(ra - T - [(I - γPa)v∏]s + Cs)∕T + (1 - η)θa,	(29)
which corresponds to
∏a — (∏a)1-η exp(η(ra - T - [(I - YPa)Vn]s + Cs)/T),	(30)
and cs is determined by the condition that Pa πsa = 1 for any s. Equivalently, we have
a ,	(πa)1-η exP(η(ra - T - [(I - Y严)vn]s"t)
∏ -
∑a(πα)1-η exP(η(r? - T - [(I - YPa)Vn]s)∕t)
(∏α)1-η exp(η(rα + (YPaVn)s)∕t)
Pa(πα)1-η exp(η(ra + (YPaVn)s)∕t)
where We cancel out the factors independent of a. This is exactly (11), which finishes the proof. □
B Proof of Theorem 2
Theorem 2. Assume that π* is the optimizer of (5) where hn denotes the α-divergence (12). Then
(a) the HeSSian matrix D2E(π) can be approximated by a diagonal matrix H(n) near π* such that
H(π) - D2E(π) = O(k∏ - π*k) and (b) the quasi-Newton methodfrom H(π) (thePolicygradient
ascent preconditioned by -HE (π)) is
2
πa —	((1	- η)(πa)α-1	-^-^ɑηlAI1-α(ra	-	[(I - YPa)Vn]s)∕t +	Cs)	,	(13)
where 0 < η ≤ 1 is the learning rate and with parameterization θS = φ0(∣A∣∏a), the update scheme
above can be expressed as:
θs 一 η(ra - [(I - YPa)Vn]s + cs)∕T + (1 - η)θs.	(14)
where φ(x) = ɪ-oɪ(1 - x(1+α)/2) (α < 1) and cs is the Lagrange multiplier introduced by the
constraint a πsa = 1.
13
Under review as a conference paper at ICLR 2022
Proof. Since the only difference between the functional E(π) defined here and the E(π) in Theo-
rem 1 lies in the regularizer h, we still have
w> (re — τDh∏E- ZeZ-1(r∏ - τh∏)) = 0, ∀e,
at ∏ = ∏*, where w∏ = (I - YPn)-1 e. Moreover, We also have
∂E
市= (ra - τφ0(∣A∣∏a) - [(I - γPa)v∏]s + cs)(w∏)s,
(31)
where cs(wπ )s is the Lagrange multiplier. Since wπ = e + Pi∞=1 γi (Pπ> )ie and all elements of e
are positive, all elements of W are positive as well. Thus at ∏ = ∏*,
(rS - τφ0(∣A∣∏a) - [(I - γPa)v∏]s + Cs) = 0,
which leads to
r - τDhπE - ZZπ-1(rπ - τhπ) = 0, ∀E,
and the approximation:
E(∏ + e) - E(∏) = w>(-2e>τD2h∏e) + O(ke∣∣3) = 1 e>H(π)e +。(|旧户).
Hence we have proved that D2E(∏) - H(∏)= O(Ilei∣). With this approximation, we have
∂ 2 E	00 a
QπaQπb ≈ H(SaMtb = -τδ{(sa),(tb)}(w∏)s|A|° (|A|ns),
Which shows that H is diagonal. Then the policy gradient scheme becomes
dπa
常=(∣A∣φ00(∣A∣∏α))T(rα - τφ0(∣A∣∏a) - [(I - γPa)v∏]s + cs)∕τ,
or equivalently,
d(φTIna)) = (rS - τφ0(∣A∣∏a) - [(I - γPa)v∏]s + Cs)∕τ.
Let θa = φ0(∣A∣∏a), then
dθa
甫= (ra - τθS - [(I - YPa)v∏]s + Cs)∕τ,
With a learning rate η , this becomes
ΘS 一 η(rS - [(I - γPa)v∏]s + Cs)∕τ + (1 - η)θS,
(32)
(33)
(34)
(35)
(36)
which is exactly (14). We derive the quasi-Newton schemes by direct calculations below.
1.
When (h∏ )s =高 Pa log π1a, we have θS = - p^ɪ ,then (14) corresponds to
∏a 一 (1∏-aτ - η∣A∣(ra - [(I - γPa)v∏]s + cs)∕τ) 1,
and Cs is determined by the condition that Pa πsa = 1 for any s.
2.
When (h∏)s = -2 Pa J高,we have θ(S
/ 1 , then (14) corresponds to
√ιA∣πa
—
a
nS J
-ηVZiAi(ra - [(I-YPa)v∏]s)∕τ + CS
and Cs is determined by the condition that	a πsa = 1 for any s.
3. When (h∏ )S = ∣A∣(ι-α2 ) Pa(I-(IAIna)(I+α"2),wehave 优=-含(IAIna)	, then
(14) corresponds to
2
na —	((1	- η)(na)α-1	-	^-2"a切川1-α(Tis	-	[(I	- YPa)Vn]s)∕τ +	CS)	,
and CS is determined by the condition that Pa nSa = 1 for any s.
□
14
Under review as a conference paper at ICLR 2022
C Proof of Lemma 3
Lemma 3. Assume that σ < 0, then for any x1, x2, . . . , xk, there is a unique solution to the equa-
tion:
(X + xι)σ + •…+ (x + Xk )σ = 1,	(15)
such that x + xi ≥ 0, i = 1, 2, . . . , k. Moreover, the solution is on the interval
max < 一 min Xi, k-σ — max Xj > , k-1/‘ 一 min Xi .	(16)
1≤i≤k i	1≤j≤k j	1≤i≤k i
Proof. Let
f (x) = (X + Xι)σ +-+ (x + Xk )σ,
then f(X) is positive and decreasing on (— min Xi, ∞) since each summand is positive and decreas-
1≤i≤k
ing. When X → — min Xi from the right, f(X) → +∞ since at least one of the summand goes to
1≤i≤k
+∞. If k-1/。一 max Xi ≥ — min Xi, then when X = k-1/。一 max Xi,
1≤i≤k	1≤i≤k	1≤i≤k
kk
f (X) = X(k-1/σ - 1maxkXj + Xi) σ ≥ X(k-1σ )σ = k × k = 1.
i=1	≤j≤	i=1	k
Moreover, when X = k-^σ — min Xi,
1≤i≤k
kk
f (X) = X(k-0 - 1minkXj + Xi)σ ≤ X(k-1σ )σ = k × k = 1.
i=1	≤j≤	i=1	k
By the continuity of f, there exists a solution X to (15) on
max{— min Xi,k-^σ — max Xj},k-^σ — min Xi ,
1≤i≤k i	1≤j≤k j	1≤i≤k i
and the solution is unique by the monotonicity of f on (— min Xi, ∞).	□
1≤i≤k
D Proof of Proposition 4
Proposition 4. The multipliers cs in the update scheme (13) can be determined uniquely such that
the updated policy π satisfies πsa ≥ 0 for any (s, a), and	a πsa = 1 for any s.
Proof. For all the three update schemes, we apply the routine stated in Lemma 3 with k = |A| for
|S| times in each iteration. For the reverse-KL divergence, cs is the solution to (15) for σ = —1 and
Xa = 1—aη ― n|A|(ra — [(I ― YPa)Vn ]s"τ, a = 1, 2, ... , |A|.
πsa	s
For the Hellinger divergence, cs is the solution to (15) for σ = —2 and
Xa = 1-η ― ηPjAi(ra — [(I ― YPa)Vn]s"τ, a = 1, 2, .. . , |A|.
πs
For the update scheme (13), CS is the solution to (15) for σ = 2∕(α — 1) and
Xa = (1 — η)(∏a)α-1 - 1-α η∣A∣1-α (ra — [(I -YPa)Vn ]s)∕τ,	a = 1, 2,..., |A|.
By Lemma 3, the solution to the multipliers such that the updated πSa ≥ 0 is unique, and the con-
straint Pa πSa = 1 is satisfied since cS is the solution to (15) (with the Xi-s defined above) and the
right hand side of the (15)is 1.	□
15
Under review as a conference paper at ICLR 2022
E Proof of Theorem 5
Theorem 5. Let η = 1, f (π)sa = -(rsa - ((I - γPa)vπ)s) and
φKL(π) = X πa log πa, φrKL(π) = |Ai X log ∏a,
sa	sa	s
W~/ πa	4	W~/. , 、，C
Φh(∏) = -2∑ √πs,	Φα(∏) =	-2Γ £(1 - (|AIna)(I+a)/2), (α < 1).
sa	|A|	|A|(1 - α2) sa	s
Denote the k-th policy obtained in Algorithm 1 by π(k), then for η = 1 the update scheme in
Algorithm 1 can be summarized as
VΦ(π(k+1)) - VΦ(π(k)) = - (f (n(k)) + VΦ(π(k)) - B>c(π(k))) ,	(17)
where we denote by B the |S|-by-(|S| × |A|) matrix such that Bij = 1 for |A|(i - 1) + 1 ≤ j ≤ |A|i
and Bij = 0 otherwise. Define (θ(k))a = φ0(∣A∣(π(k))a), where φ(x) is the Convexfunction used in
the regularization (h∏)s = JAj Ea φ(∣A∣π^). Assume that(1) Vf is Lipschitz on a closed subset of
{π : B>π = 1∣s∣,∏a > 0} and (2) θ(k) → θ*. Then n(k) converges quadratically to π*, i.e.,
k∏(k+1) - π*∣∣ ≤ Ck∏(k) - π*∣∣2,	(18)
for some constant C.
Proof. Since Bij = 1 for ∣A∣(i - 1) + 1 ≤ j ≤ ∣A∣i and Bij = 0 otherwise, the constraint on π can
be expressed by Bπ = 1∣S∣. The gradient flow (28) can thus be expressed by
d(vdt(π)) = -(f(n) + VΦ(π) - B>c(n)),	(37)
where c(π) is the Lagrange multiplier. The corresponding discrete scheme used is hence
VΦ(π(k+1)) - VΦ(π(k)) = - f (π(k)) + VΦ(π(k)) - B>c(π(k))) .	(38)
With this update scheme, we have
f (π(k+1)) - f (n(k)) - Vf (π*) (π(k+1) - n(k))
(39)
=f (n(k+1)) + VΦ(∏(k+1)) - B>c(π(k)) - Vfg (n(k+1) - n(k))
Since θ(k) = VΦ(n(k)) converges to θ* and the map from θ to n is continuous, We have n converges
to some π* as well. Since n(k) ∈ {π : B>π = 1∣s∣,∏a ≥ 0} and {π : B>π = 1∣s∣,∏a ≥ 0} is a
closed set, we also have n* ∈ {n : B>n = 1∣s∣, ∏S ≥ 0}. Moreover, since VΦ(n) goes to infinity
as any entry n； → 0, we have (n*)a > 0 for any S and a. Hence n(k) is contained in a closed set K
contained in {n : B>n = 1∣S∣ , nsa > 0} for large enough k. By the assumption that Vf is Lipschitz
continuous, we have
f (n(k+1))- f (n(k)) - Vf (n*) (n(k+1)- n(k))
k→∞	k∏(k+1) — n(k) k
(40)
Moreover, since f (n)sa = -(rsa - ((I - γPa)vπ)s), which has a similar form with E(n), we can
directly obtain Vf (n):
(Vf (n))sa,tb = λsa,t(n) (-f (n)tb + C(n)t - VΦ(n)tb)	(41)
16
Under review as a conference paper at ICLR 2022
where λsα,t(∏) = Z-TgSa and esα is the s-th row of I — YPa. Notice that at π* we have "乎"
0, so f (π*) + VΦ(π*) = BTC(π*), thus
(Vf (π*)(π(fc+1) — π(k)))sα
X λsa,t(π*)(-f (π*)tb + C(π*)t — VΦ(π*)tb)(忒+1) — π(?)
tb
X λsa,t(π*)(2(π*)t — c(π*)t)(忒+1)— n(k))
tb
(42)
X "sa,t(n*)(C(n*)t— c(π*)t)) (x(n(k+1)— πf?))]
0,
where the last equality results from the fact that Pb ∏(k+1) = Pb ∏(k = 1 for any t. With (40) and
(42), we arrive at
f(π(k+1)) + VΦ(π(k+1)) — BTC(π(k)) _
k→∞	k∏(k+1) — ∏(k)∣∣
n ʃ 1	1 J 丁	n(k + 2) —n(k+1)	J -♦
Multiply the fraction above by the unit vector 口*忆+2)/@+1)口, We obtain
(f (n(k+1)) + VΦ(π(k+1)) — Bτc(π(k)))T (π(k+2) — n(k+1))
k∏(k+1) — ∏(k) kk∏(k + 2) — n(k + 1)k
_ (f (n(k+1)) + VΦ(π(k+1)) — BTCwk+1)))T (n(k+2) — n(k+1))
k∏(k + 1) — ∏(k)kk∏(k + 2) — n(k+1)k
_ (VΦ(^k+2)) — VΦ(π^+1)))t (π(k+2) — n(k+1))
k∏(k+1) — ∏(k)kk∏(k+2) — n(k+1)k	，
(43)
(44)
where the second equality uses the constraint Bn(k+1) = Bπ(k+2) = 1∣s∣ . Then from (43) we have
lim
k→∞
(VΦ(π(k+2)) — VΦ(π(k+1)))> (π(k+2) — n(k+1))
k∏(k+1) — ∏(k)kk∏(k + 2) — n(k+1)k
0.
(45)
By direct calculation of V2 Φ, which is diagonal for all the functions Φ defined in this theorem, we
can see that V2Φ is lower bounded on K, so Φ is strongly convex. As a result, there is some constant
ρ > 0 such that
(VΦ(π) — VΦ(π))τ(π — π) ≥ ρ∣∣π — ∏∣∣2
for any π and π in K. Then
0 = lim
k→∞
k∏(k+2) — n(k+1) I2
k∏(k+1) — ∏(k)kk∏(k + 2) — ∏(k+1) Il
,∣∣π(k+2) — π(k+1)H
kl→∞ ∣∏(k+1) — ∏(k)∣ ,	(46)
from which we can conclude that n(k) converges to π* superlinearly, i.e., (50) holds. In fact, for any
E (assume e < 1/2 without loss of generality), there is some k(e) such that for any k > k(e),
|n(k + 2)— ∏(k+1)∣
∣∏(k + 1) — ∏(k) Il < €，
then for any k > k(e)
∞	∞
|n(k+1)—π*∣≤ X |n(n+1) — n(n)| ≤ X en-k|n(k+1) — n(k)∣
n=k⅛1	n=k⅛1	(47)
≤ ɪʌɪ∣∣π(k+1) — n(k)| ≤ 2e∣π(k+1) — n(k)|.
Then
∣∣π(k) — π*∣ ≥ ∣∣π(k+1) — n(k)| — |n(k+1) — π*∣
≥ (* — 1)|n(k+1)—∏*∣.	(48)
17
Under review as a conference paper at ICLR 2022
For any M > 0, take E = 1/(2M + 2), then for any k > f (e),
∣∣π(k) - π*∣∣ ≥ (ɪ - 1)kπ(k+1) - π*∣∣ = (M + 1)||n(k+1) - π*∣∣ > M∣∣π(fc+1) - π*∣∣,	(49)
一，，	J .一	kπ(k)-π*k	,	一，
which shows that lιm J%*i———= +∞ and thus
k→∞mk+1)-π*k
㈣ ∣⅛⅛=0∙
(50)
Now, from (39) and (42) we have
f (n(k+1)) + VΦ(π(k+1)) - B>c(π(W))
=f (π(k+1)) - f (π(k)) - Vf (π*')卜(k+1) - n(k))
=IL [Vf (π(k) + t(π(k+1) - π(k))) - Vf (π*)] dt)卜(k+1) - π
≤ C∣∣∏(k) - π*kk∏(k+1) - n(k)k,
(51)
〜
for some constant C, where we used (50) and the Lipschitz contiuity of Vf in the last equality.
M 31 1 ” r 1	∏(k + 2).∏(k+1)
Multiply both sides by k∏(%+2)-∏(k+i)k
and by (44) and the strong convexity of Φ, we have
ρ∣Wk+2)- π(k+1)∣2
≤ (VΦ(∏(k+2)) - VΦ(π(k+1)))> (n(k+2) - n(k+1))
=(f (n(k+1)) + VΦ(π(k+1)) - B>c(π(k)))> (n(k+2) - n(k+1))
≤ Ckn(Aɔ - π*∣∣π(fc+1) - n(k)kkn(k+2) - π(k+1)∣,
which implies that
∣∣π(fc+2) - n(k+1)K ≤ Ckn(A) - n*||n(k+1) - n(k)|.
From (50), we have
Iwk) - n(k+I)Il _	k∏(k+I) - n(k+2)K _
k→∞ |n(k) — π*∣	k→∞ |n(k+1) — π*∣
Combine this with (53) we get
∣∣π(k+1) - π*∣ ≤ C|n(k) - π*∣2,
for some constant C, which closes the proof.
(52)
(53)
(54)
(55)
□
18