Under review as a conference paper at ICLR 2022
Safe Opponent-Exploitation Subgame Refine-
MENT
Anonymous authors
Paper under double-blind review
Ab stract
Search algorithms have been playing a vital role in the success of superhuman AI
in both perfect information and imperfect information games. Specifically, search
algorithms can generate a refinement of Nash equilibrium (NE) approximation
in games such as Texas hold’em with theoretical guarantees. However, when
confronted with opponents of limited rationality, an NE strategy tends to be overly
conservative, because it prefers to achieve its low exploitability rather than actively
exploiting the weakness of opponents. In this paper, we investigate the dilemma of
safety and opponent exploitation. We present a new real-time search framework
that smoothly interpolates between the two extremes of strategy search, hence
unifying safe search and opponent exploitation. We provide our new strategy with
a theoretically upper-bounded exploitability and lower-bounded reward against
an opponent. Our algorithm enables computationally efficient online adaptations
to a possibly changing opponent model. Empirical results show that our method
significantly outperforms NE baselines when opponents play non-NE strategies
and keeps low exploitability at the same time. It is also much more efficient than
previous safe exploitation baselines.
1	Introduction
Behind the recent breakthroughs of superhuman AIs in Go (Silver et al., 2016; 2017; Schrittwieser
et al., 2020), heads-up no-limit Texas hold'em (HUNL)(BroWn et al., 2018; Moravdk et al., 2017;
Brown & Sandholm, 2019; Brown et al., 2020), and Hanabi (Lerer et al., 2020), search plays a vital
role. In perfect information games, Monte Carlo tree search (MCTS) is Widely applied to improve
policy’s strength. In zero-sum imperfect information games such as poker, search algorithms are
used to find a Nash equilibrium (NE) approximation in subgames encountered in real time (BroWn
& Sandholm, 2017; Burch et al., 2014a). They are both theoretically sounded and empirically
poWerful. In fully-cooperative imperfect information games, the search algorithm proposed in Tian
et al. (2020b) is proved to never be detrimental to the current policy. Lerer et al. (2020) also ensures
original performance asymptotically.
In zero-sum games, NE-based search algorithms (Burch et al., 2014a; Moravcik et al., 2016; BroWn
& Sandholm, 2017; BroWn et al., 2018) find safe strategies With loW exploitability and produce strong
baselines against all opponents (BroWn & Sandholm, 2019). HoWever, it may be overly conservative
confronted With opponents With limited rationality and fail to take advantage of their Weaknesses
to obtain higher reWards (McCracken & BoWling, 2004; Johanson et al., 2007; Li & Miikkulainen,
2018). From the other perspective, there have been extensive studies on opponent exploitation to
address the problem. Some typical Works (Carmel & Markovitch, 1996; Billings et al., 2003; Gilpin
& Sandholm, 2006; Li & Miikkulainen, 2018) model the opponent’s strategy based on previous
observations and then search for a neW strategy to exploit this model. HoWever, these methods often
neglect the significance of the strategy safety, thus being highly exploitable by the opponent. FeW
exceptions including Johanson et al. (2007) and Ganzfried & Sandholm (2015a) aimed to search for
safe and robust counter strategies, but they are computationally inefficient in an online setting Where
the opponent model is being updated continuously With streamed data.
In this paper, We study the dilemma of safety and opponent exploitation and present a neW scalable
real-time search frameWork Safe Exploitation Search (SES) that smoothly interpolates betWeen the
tWo extremes of strategy search, hence unifying safe search and opponent exploitation. It enables
1
Under review as a conference paper at ICLR 2022
computationally efficient online adaptations to a continuously changing opponent model, which is
hard to address by previous safe exploitation algorithms. The safety criterion requires the refined
strategy to stay close to NE, formally speaking, to expose limited exploitability against any opponents,
while the opponent exploitation criterion requires the strategy to adapt to its specific opponent and
to exploit its weaknesses. We propose a novel maximization objective which combines the safety
objective and exploitation, controlled by the exploitation level α. We construct a new gadget game to
optimize this objective, which enables our method’s scalability to large games such as Texas Hold’em.
Theoretically, we prove that SES is guaranteed to outperform NE at the cost of some constant
increase in its own exploitability confronted with non-NE opponents. Empirically, we evaluate the
effectiveness of our search algorithm in 1 didactic matrix game 2 poker games: Leduc Hold’em
(Southey et al., 2005) and Flop Hold’em Poker (FHP)(Brown et al., 2019). The experiment results
demonstrate that our algorithm significantly outperforms NE baselines against non-NE opponents and
keeps low exploitability at the same time. Additionally, SES is much more computationally efficient
than previous safe exploitation baselines.
2	Related work
This paper investigates the problem of safe opponent exploitation in two-player zero-sum imperfect
information games. We propose a novel search algorithm which balances between NE and exploiting
opponents. Two major relevant research areas are search algorithms in imperfect information games,
and opponent exploitation.
Search in imperfect information games. In recent literature, search techniques are witnessed to
be important in developing strong AI strategies in both perfect and imperfect information games
(Burch et al., 2014a; Moravcik et al., 2016; Brown & Sandholm, 2017). Texas hold ’em poker is
widely employed as a benchmark for imperfect information games. A primary part of the long-term
research on Texas hold’em poker is the evolution of subgame solving algorithms, which aim at
achieving a more accurate Nash equilibrium approximation in the subgame encountered given a
pre-computed strategy for the full game which we refer to as the blueprint strategy. Unsafe search
(Billings et al., 2003; Ganzfried & Sandholm, 2015b; Gilpin & Sandholm, 2006; 2007) estimates
the subgame reach probability assuming the opponent follows blueprint, and searches for a refined
subgame strategy. Subgame resolving (Burch et al., 2014a) and maxmargin search (Moravcik et al.,
2016) are theoretically sounded safe search algorithms which ensure that the subgame strategy
obtained is no worse than the blueprint. They search in a gadget game and achieve safety by providing
the opponent with the option not entering the current subgame. DeePStack (Moravcik et al., 2017)
and Libratus (Brown et al., 2018) build strong poker AIs with the aid of search. Beyond poker,
search algorithms for subgame refinement have also shown promise in improving joint strategies in
cooperative imperfect information games such as Hanabi (Lerer et al., 2020) and the bidding phase of
contract bridge (Tian et al., 2020a). The purpose of our search algorithm is different from previous
methods in poker literature. We seek to exploit opponents while keeping exploitability low, rather
than simply approximating NE.
Opponent exploitation. Most previous opponent exploitation researches (Carmel & Markovitch,
1996; Billings et al., 2003; Gilpin & Sandholm, 2006; Li & Miikkulainen, 2018) typically model the
opponent’s strategy based on previous observations and then search for a new strategy to exploit this
model, but put little emphasis on safety.
One similar work is Johanson et al. (2007) which proposes p-restricted Nash response (RNR) to find
a safe exploitation strategy to the estimated opponent’s strategy. It calculates a Nash equilibrium for
the whole game restricting that the opponent plays the estimated strategy σfix with probability p, and
any strategy with probability 1 - p. In that paper, Johanson et al. (2007) prove that a p-RNR to σfix
is Pareto optimal with respect to exploitation and safety. However, it does not provide an explicit
bound. Additionally, whenever the estimated opponent model changes or we want to use a different p
to balance between safety and exploitation, p-RNR has to recompute the strategy for the whole game.
It is computationally inefficient in an online setting, where the opponent model is updated after every
round with new game data. Our algorithm instead takes modelling error into account and provides
explicit bounds for both safety and exploitation. With the aid of real-time search, it only searches for
strategies in subgames encountered instead of the whole game. Our experiments show that it is more
efficient than Johanson et al. (2007).
2
Under review as a conference paper at ICLR 2022
Ganzfried & Sandholm (2015a) study safe exploitation strategies in repeated games, which is a
different setting from this paper. Intuitively, it achieves safety by risking in exploitability at most
what it has earned over NE in expectation in previous rounds. Therefore, its expected value in the
whole repeated game is never worse than the NE. In contrast, this paper focuses on the safety of stage
game strategies. Furthermore, our algorithm is complementary to Ganzfried & Sandholm (2015a).
Ganzfried & Sandholm (2015a) calculate an ε-safe best response for the whole game at each iteration
with LP. This procedure is one of the main limitations on the algorithm’s scalability. Our algorithm,
which only refines strategies in subgames in real-time, can be a possible substitute for LP.
To our knowledge, we are the first paper to investigate the safe opponent exploitation problem
in subgame resolving schemes. Subgame resolving enables online adaptations to a continuously
changing opponent model, eliminating the need to recompute a whole game strategy. It offers
computational benefits in practical opponent exploitation circumstances.
It is worthwhile mentioning that there also has been extensive research (Albrecht & Stone, 2018)
on agent modeling. However, this paper only focuses on the theoretical and empirical results of the
search algorithm, but not the agent modeling techniques. We can use off-the-shelf agent modeling
algorithms to estimate opponent’s strategies. Agent modeling provides the ability to reason about the
others, making predictions of their strategies, types, etc. He et al. (2016) embeds opponent model
learning into deep Q-learning, which stabilizes the training process faced with opponents whose
policy is changing over time. LOLA (Foerster et al., 2018) considers the evolution of other agents in
the training process of multi-agent reinforcement learning. Raileanu et al. (2018) proposes to model
others from one’s own policy. Rabinowitz et al. (2018) uses meta learning to predict strategies of
different kinds of agents.
3	Notations and background
An extensive-form imperfect information game G = (P, H, Z, A, χ, ρ,∙, σc, u, I) describes SeqUen-
tial interactions among agents, where agents have private information. A finite set P consists of n
players and a chance node c which represents the stochastic natUre of the environment. The set of
non-terminal decision nodes is denoted as H, and Z is a set of terminal nodes, or leaves. The set of
possible actions is A, and χ : H → 2|A| is a fUnction which assigns to each decision node h ∈ H a
set of legal actions. A player fUnction ρ : H → P assigns to each decision node a player p ∈ P who
acts at that node. If action a leads from h to h0, We write h ∙ a = h0. If there exists a sequence of
actions leading from h to h0, we write h v h0. At each node h ∈ H, the acting player p = ρ(h) picks
an action from legal actions a ∈ χ(h), and leads node h into its child h ∙ a. The chance node always
samples an action from its own distribution σc , which is common knowledge to all players. Utility
functions are u = (u1, u2, . . . , un), where ui : Z → R defines the utility of player i at terminal node
z ∈ Z. The nature of imperfect information is characterized by infosets I = (I1,I2, . . . , In), where
Ii = (Ii,1, . . . , Ii,ki) is a partition of H for player i. Two states in the same infoset must have the
same acting player and the same legal action sets. We use I(h) to denote the infoset that h belongs to.
A player p cannot distinguish between states h1 and h2 if I(h1) = I(h2), and thus should behave
identically on all states in the same infoset.
The strategy of a player p is σp : Ip × A → R, where σp(I, a) is a distribution over valid actions on
infoset I. For simplicity, we also use σp(h, a) to denote σp(I(h), a). We use πσ (h) to denote the prob-
ability of reaching state h from the root when agents choose a strategy profile σ = hσ1, σ2, . . . , σni.
FOrmally, π ㈤=Qh0∙aVh σρ(h0)(h0,a). Weuse π-p㈤=Qhθ∙aVh∧ρ(h0)=p σρ(h0)(h0,a) tode-
note the probability of reaching h when player p always chooses the action that leads to h whenever
possible. πσ (h, h0) is the reaching probability of h0 from h. πσ (h ∙ a, h0) is the the probability of
reaching h0 from h if action a is taken at h. These probabilities can be formally defined in a similar
manner.
The expected utility of player p given strategy profile σ is upσ = z∈Z πσ (z)up(z). The counter-
factual value vpσ(I, a) is the expected utility that player p will obtain after taking action a at infoset
I , given the joint policy profile is σ. Mathematically, it is the weighted sum of expected values at all
states h ∈ I.
vpσ (I, a) =
h∈I,z∈Z π-σp(h)πσ(h ∙ a, z)up(z)
Ph∈ι ∏-p(h)
(1)
3
Under review as a conference paper at ICLR 2022
In the rest of the paper, we focus on two-player zero-sum games with perfect recall. Zero-sum means
∀z ∈ Z, u1(z) + u2(z) = 0. Perfect recall means that no player will forget the information which
has been obtained previously in the game. This is a common assumption in related literature.
A best response strategy BRp(σ-p) = arg maxσp uhpσp,σ-pi for player p is the strategy that maximize
his own expected utility against fixed opponent strategy σ-p. The exploitability of strategy σp is
exp(σp) = up* - Upp,BR-p(σp)i where σ* is the optimal strategy, and is an NE in two-player
zero-sum games. It measures the performance of σp against its best response comparing with the
NE. A counterfactual best response CBRp (σ-p) is a strategy where σp(I, a) > 0 if and only if
vpσ(I, a) ≥ maxb vpσ(I, b). Counterfactual best response is a best response, but not vice versa. The
counterfactual best response value CBVpσ-p(I) = vphCBRp(σ-p),σ-pi is the expected utility of the
counterfactual best response policy. Since we focus on two-player zero-sum games, we will use
CBV σp(I) as a shorthand notation for CBV-σpp(I).
We follow the imperfect information subgame definition as in Burch et al. (2014b). An augmented
infoset contains states which cannot be distinguished by the remaining players.
Definition 1. An imperfect information subgame S is a forest of trees, closed under both the
descendant relation and membership within augmented infosets for any player. Let Stop be the set of
nodes which are roots of each tree in S.
4	Method
In this section, we introduce our novel search algorithm called safe exploitation search (SES), which
exploits the weaknesses of opponent while ensuring a bounded exploitability. Let σ be the pre-
computed blueprint strategy. Without loss of generality, assume we search for player 2’s refined
strategy σ2S by applying SES to all subgames S ∈ S. Finally, the refined strategy for P2 after search
is 必,which is the same as σ? in {I2i ∣∀S ∈ S, I2i / S} and is replaced with σs in S ∈ S.
4.1	Safe Exploitation Search
Our algorithm offers a unified approach to balance between these two demands with theoretical
guarantees. The objective of our search algorithm is to find a new subgame strategy σ2S for S ∈ S
which maximizes
SE(σS) = α E P(Ij)
I1j ∈Stop
v1σ(I1j) - CBV1σ2S(I1j) + (1 - α) min
I1j∈Stop
v1σ(I1j) -CBV1σ2S(I1j),
(2)
where α ∈ [0,1] is a hyper-parameter controlling the exploitation level, and P(Ii) is the estimated
probability of player 1 entering infoset I1j ∈ Stop. Given P2’s strategy (which is the blueprint σ2)
and P1’s actual strategy (which does not have to be the blueprint σ1), the real probability of player 1
entering infoset Ij / StoP (which we denote as P(II)) is determined. P(Ii) isjust an estimation of
P(I1j). For instance, in poker, it is the estimated distribution of private cards player 1 holds. Both
theoretically and empirically, such estimation does not have to be fully accurate. It can be done with
off-the-shelf opponent modeling techniques, which lies beyond the focus of this paper.
Though seemingly complicated, intuitively, the maximization objective achieves a balance between
opponent exploitation and safety, controlled by exploitation level α. The first part of the objective
j	σS j
IS maximized when EIj∈5t P(II )CBV12 (Ij) IS minimized. It aims at finding a strategy σf which
results in the lowest value for P1 under the assumption that the reach probabilities is P. It can be
interpreted as exploiting the estimated P1’s strategy. The second part of the objective demands the
resolved strategy to behave well against any reach probability distribution. We use the subgame
margin minIj ∈S v1σ (I1j) - CBV1σ2 (I1j) (Moravcik et al., 2016) which can be regarded as the
worst-case utility increase for P2.
By maximizing the objective 2, we provide sound theoretical results for both safety and opponent
exploitation. Additionally, we provide analyses of how (1) exploitation level α, (2) accuracy of
4
Under review as a conference paper at ICLR 2022
opponent modeling, and (3) strength of the blueprint strategy impact the theoretical bound. By
gradually increasing α from 0 to 1, our algorithm tends to exploit rather than keeping safety.
Theorem 1. (safety) Let S be a disjoint set of subgames S. Let σ* = hσ1,σ2i be the NEwhereP2's
*	.
strategy is constrained to be the same with σ2 outside S. Define ∆ = maxS∈S,I i ∈Stop |CBV1 2 (I1i) -
vσ (Ii)|. Let P(Ii) be the reach probability given by σj. Let P(Ii) be the estimation of reach
probability P(Ii) given by the real opponent strategy. Define T 二 maxs∈s∣i∈Stop | P(Ip-P(II) |.
Whenever 1 - (2τ + 1)α > 0, we have a bounded exploitability given by:
2
exp(σ2) ≤ exP(σ2) + 1 - (2τ + 1)ɑ△.	⑶
Recall that σ20 is the refined strategy after search. The proof is provided in Appendix A. This
theorem implies that the exploitability of the new strategy is smaller than that of strategy 琏 plus a
constant value, which is the closest strategy to NE if constrained to differ from σ2 only in S. The
corresponding theoretical result of maxmargin search (Moravcik et al., 2016), a safe search algorithm
with no opponent exploitation abilities, is exp(σ2) ≤ exp(σ^) + 2∆. Comparing these two results, we
can interpret the term 2/(1 - (2τ + 1)α) as the additional risk introduced by exploiting the opponent.
If exploitation level α = 0, then our bound is as tight as that of maxmargin search (Moravcik et al.,
2016). The bound also gets tighter if the T gets smaller, or the blueprint σ2 is closer to σg.
Theorem 2. (opponent exploitation) Let E = kp — pk 1 be the L1 distance ofthe distribution P(Ii) and
P(Ii). Let η 二 mins∈s maxIj∈st (CBVI(Ij,σS) — CBVI(Ij,σg)) ≥ 0. We use BRpS,σp](σ)
to denote the strategy for player P which maximizes its utility in subgame S ∈ S against σ-p under
the constraint that B R[pS,σp] (σ) and σp differs only inside S. By maximizing objective 2, for all S ∈ S,
the refined strategy σ20 satisfies
DBR[S,σ1] (σ0 ),σ0 E	DBR[S,σ1](σ*),σ*E	1 — α
u2	1	( 2), 2/(S) ≥ u2	1	2	2/(S) +-(η — 2∆)—卬
22	α
(4)
The proof is provided in Appendix A. Observe that the reach probability P is characterized by P1’s
strategy outside S and P is its estimation. Because the search algorithm always find a stronger
response strategy for P1 in S (which is exactly BR[1S,σ1] (σ20 )) as well, opponent exploitation refers
to adapting to P1’s strategy σ1 outside S. This theorem implies that the utility of the new strategy
必 is lower bounded by the utility of σg when both confronted with P1's unknown strategy outside
S. It provides theoretical guarantees for the opponent exploitation ability of our algorithm. E can
be interpreted as estimation error. The lower bound increases if the estimation error get smaller or
the blueprint σ2 is closer to σ2. We show empirically how exploitation level α and estimation error
impact both safety and exploitation abilities in section 5.
4.2	Gadget Game
In order to find σ2S which maximize objective 2, a straight-forward method is to reformulate the
maximization problem as a Linear Programming problem (Moravcik et al., 2016). However, LP
solvers (Koller et al., 1994) cannot handle large-scale problems. Alternatively, inspired by Moravcik
et al. (2016), we create a gadget game and then apply iteration-based NE algorithms such as CFR
(Zinkevich et al., 2007; Tammelin et al., 2015; Lanctot et al., 2009) in the gadget game. The gadget
game is carefully designed such that the NE solution found in it is exactly the solution to the original
optimization problem.
As shown in Figure 1, the original subgame is copied into two identical parts S1 , S2 in the gadget
game. Player 2’s infosets stretch over both branches, while player 1 can distinguish between the two
parts. The procedure of constructing such gadget game can be summarized into 4 steps: (i). Create a
chance node at the top of the gadget game. (ii) For the left part of the gadget game, we construct a
P1 node to let P1 choose an infoset I1i to enter. The following chance node samples a specific state
with probability proportional to π-σ 1(h) for all h ∈ I1. (iii) For the right part, create a chance node
sampling an infoset I1i . The following chance node again samples a specific state with probability
5
Under review as a conference paper at ICLR 2022
Figure 1: The gadget game of SES. The shadow and dashed line indicate that player 2 cannot
distinguish between the two branches. C represents chance node, P1 represents player 1’s action
node. S1 and S2 are two identical copies of the subgame S with utility shifted.
∖P1 P2	L	M	R
U		2	^4-
^o	-2	3	^99^
-D		2	^99^
F	-100	-100	~T0~
Table 1: The payoff matrix of the example zero-sum matrix game. The values are the payoffs of
player 2. We will resolve for player 2.
proportional to π-σ 1 (h) for all h ∈ I1. (iv). Shift the utility of the gadget game by v1σ (I1i). The details
are described below.
1.	The chance node at the top goes to the left part with probability 1 - α, and the right part with
probability α. The outcome is visible to P1 but not P2. Therefore, corresponding nodes in both
branches are in the same infosets for P2, and his strategy σ2S will be the same for both parts. Since
σ1S is the best-response to σ2S and the two parts only differ at how to go to each infoset of player 1,
player 1 will also keep his strategy the same in both parts.
2.	We subtract u1 (z) by v1σ (I1i) for all z v h, h ∈ I1i, and add u2 (z) by v1σ (I1i) in order to keep
the subgame zero-sum. By doing so, the objective of a Nash Equilibrium of p2 will change from
SS
maximizing -CBV1 2 (I1i) to maximizing v1σ (I1i) - CBV1 2 (I1i).
3.	As for the left part of the gadget game, the P1 node on the second level in Figure 1 enables P1
to enter an arbitrary infoset I1i . Since this is a zero-sum game, in an NE strategy, he will enter the
one with lowest v： (Ii) - CBVI(Ii 5) which is exactly the minimization in the second term of
SE(σ2S).
4.	The chance node on the second level of the right part will sample an infoset I1i according to reach
probability P(Ii). So that the NE objective of this part is exactly the summation in the first term of
SE(σ2S).
4.3 Matrix Game
In this part, we offer a matrix game as an example to show the necessity of considering safety and
expected payoff simultaneously, and to demonstrate the superiority of SES over a simple mixing
strategy, which follows a best response to the estimated opponent model with probability α and
follows the blueprint with probability 1 - α.
In the matrix game shown in Table 1, let’s consider two specific NEs. In both NEs, P1 will play L/M
with 0.5 probability each. P2 will play U/O with 0.5 probability in the first NE and O/D with 0.5
probability in the second NE. Suppose the blueprint strategy is the first NE. Consider the case when
P1 plays a rather weak strategy that he will only play action R. We apply SES to search for P2’s
refined strategy.
6
Under review as a conference paper at ICLR 2022
Figure 2: Left: Expected payoffs of SES and the mixing strategy in the proposed matrix game
example. Right: Exploitability of the two algorithms.
When the estimation of opponent strategy is accurate such that P = p, the best response of P2 is
always playing F, which is highly exploitable, while SES finds the second NE under proper α. To
give more details, the exploitability and expected payoff of the strategy found by SES and the mixing
strategy are shown in Figure 2. We can see that SES achieves both lower exploitability and better
performance than the mixing strategy at almost all α values. The reason behind this success is that P1
always playing R is a Gift Strategy (Ganzfried & Sandholm, 2015a) in the designed matrix game and
SES manages to utilize such gift strategy. Strategy σ-p is a Gift Strategy if it is not a best response to
a NE strategy σp. Therefore, We can switch to σp to get better performance against σ-p while also
keeping exploitability low. However, the simple mixing strategy cannot find such “good" NE strategy
so that it will perform much worse in both exploitability and expected payoff.
5	Experiment
Our experiment is done in Leduc Hold’em (Southey et al., 2005) and Flop Hold’em Poker (FHP)
(Brown et al., 2019). Leduc Hold’em is a smaller-scale poker games and FHP is a larger one. The rules
of these two pokers are provided in Appendix B. We demonstrate the exploitability and evaluation
performance of SES against opponents of various strengths. The exploitability measures a search
algorithm’s safety, while head-to-head evaluation measures the ability of opponent exploitation. We
also illustrate how estimation accuracy of opponent’s strategy and the exploitation level α impact the
results. Please refer to Appendix C for implementation details.
5.1	Opponents
In our experiments, we test the performance of our algorithm against opponents of various strengths.
For both Leduc Poker and FHP, we create 3 types of opponents with 3 random seeds each. The first
type of opponent is an approximation of NE in the full game, and is regarded as a strong opponent.
It is computed in the same way as the blueprint strategy with different seed. For the second and
third type of opponents, we enumerate every infoset in the blueprint strategy and shift the action
distribution randomly with probability Prshuffle = 0.3 or 0.7. We multiply the probability of each
action by a random variable from Uniform(0, 1), and then re-normalize the probability distribution.
The procedure is motivated by Brown et al. (2018), in which such method is applied to create a
number of diverse but reasonably strong agents. Even when Prshuffle = 0.7, the strategy keeps close
to NE with average L1 distance of each infoset 0.132 comparing to 1.036 of a random strategy to NE.
So they are regarded as opponents who are not fully rational but with competitive strength.
5.2	Safe Opponent Search
In Figure 3, we demonstrate the head-to-head evaluation performances and corresponding exploitabil-
ity of the refined strategies found by SES against opponents of various strengths, under different
exploitation level α and estimation errors of opponent’s strategy. Different lines in each plot refers
to corresponding estimation error e, which is the L1 distance of P and p. We evaluate our refined
7
Under review as a conference paper at ICLR 2022
Evaluat∣on( Prshuffle = 0.3)
Evaluation(Prs/)offle = 0.0)
---BliNpdnt
-50	n«oi«4 Stntegyte=0∙β
—Rewitnd Stntegylt=0∙3>
-60 — ReMiEStmtegyk=OQ
—RteeN«4 StnteeyU=OS	\
-TQ ----- KeMh«4 Smte⅞y⅛=12)
0.0	0.2	0.4	0.6	0.8	1.0
EXPIoitabi Iity(PrSftU 用e = 0.0)
350 ---- BIiNprInt
n«oi«4 StntegyU=0∙β
38 — ReMMSftate⅞*s=03
--------ReBeN«4 Stnte⅞y⅛=a0
2sβ — RteeMStntey(t=。3
--------R«oi«4 SWte9y(r=ι⅛
D
0.0	0.2	0.4	0.6	0.8	1.0
0.0	0.2
0.4	0.6	0.8	1.0
二二350300250315010050。10:Γ.
WqqlU WqqW
---Blueprint
Reso∣v*4 Stfatcgy(β=QΛ>)
—ReMXswategy(C=OJ)
---ReghMSwat.y(e = OQ
----Reso∣v*4 Stfatcgy(c=Q.9)
—Resolved su∙ateβy(β=12)
0.0	0.2	0.4	0.6	0.8
EXPloitabiIity(PrShUMe = 0.3)
—Blaqrlnt
Reso∣v*4 SWaMgy(C=QA)
EValUatiOn(PrSftU 用e = 0.3)
—Bluqrlnt
ResoM SwaMgy(C = 0Λ))
---ResoMSWyfc=(M)
---RegMSwaMgyfc = OQ
—ReMMSwategyte=OS)	^⅞isl,
—Resoiwd Stfateaytc=12}	_
0.0	0.2	0.4	0.6	0.8
Evaluat∣on( Prshuffle = 0.7)
360
350
340
---Bloepdnt
n«oi«4 Smwayte=oλ>)
—n«oi«4 Smwayte=oj)
---R«ol«4 Smteayte = 0Λ)
---R«ol«4 S⅛ate⅛y(r = 0.9)
---R«oi«4 s⅛ate⅛y(β=ι⅛
0.0	0.2	0.4	0.6	0.8	1.0
EXPIoitability(PrSftU 用e = 0.7)
---Bleepdnt
R«eoi«4 StnteeyU=0Λ>)
WqqE
18161412
WqqlU
0.0	0.2	0.4	0.6	0.8	1.0
Exploitabi I ity( Prsftuffle = 0.7)
Exp IO itab i I ity( Prshu ffle = 0.0)
Q 5 Q 5 Q 5 O
5 4 4 3 3 2 2
WqqUJ
0.0	0.2	0.4	0.6	0.8	1.0
EXPIoitability(PrShUfffe = 0.3)
504540353025
WqqE
Figure 3: Row 1&2: Experiment results on Leduc poker. Row 3&4: Experiment results on FHP.
From left to right, each column represents a type of opponent with Prshuffle = 0.0, 0.3, 0.7. The first
row of each game shows the head-to-head expected payoffs against corresponding opponents, while
the second row demonstrates the exploitability calculated for different refined strategies. The X-axis
is the parameter α which controls exploitation level.
strategy when = 0.0, 0.3, 0.6, 0.9, 1.2. The blue line is the result of blueprint strategy without
conducting any search.
Generally speaking, SES balances between safety and opponent exploitation. The increase of
exploitation level α helps win more chips from opponents, while resulting in the increase of the
strategy’s own exploitability. As can be seen in Figure 3, the exploitability increases when the
exploitation level α grows from 0 to 1, which is consistent with Theorem 1. One exception is in FHP
when is small: the exploitability surprisingly keeps decreasing even if SES puts more emphasis
on opponent exploitation. Similar situations have also occurred in previous literature (Brown &
Sandholm, 2017). The reason is that our opponent is quite close to NE outside the subgame which
will make P close to P when e is small, which means the τ in Theorem 1 is small. As a result, We will
have a low-exploitability resolved strategy when using unsafe search and the exploitability increases
as e increases.
When the estimation is completely correct (e = 0.0, the yellow line), the expected payoff in FHP
increases as the exploitation level α grows higher. In Leduc poker, since the game is very small,
the pre-computed blueprint is very close to NE. Therefore, when confronted with relatively strong
opponents (Prshuffle = 0.0, 0.3) which are also close to NE, actually few things can be done other
8
Under review as a conference paper at ICLR 2022
than sticking with the blueprint. So the improvement introduced by SES is small. When facing
relatively weak opponent (Prshuffle = 0.7), the improvement margin is slightly larger.
SES relies on an estimation of opponent’s strategy. In order to test the robustness of our algorithm
when the prediction of p(I1i) is not accurate, we evaluate the performance of our algorithm with
different values of estimation error . As illustrated in Figure.3, the exploitability increases and the
expected payoff drops when grows larger. The result is expected since an accurate estimation always
provides benefits. However, it also demonstrates that SES can still achieve a trade-off between safety
and opponent exploitation even when is considerably high. For instance, in FHP, is between 0 and
2, and = 1.2 means that the predicted distribution is almost random. When ≤ 0.6, the expected
payoff still keeps increasing with respect to α. In case of a bad estimation, we can always choose
smaller α to ensure safety.
5.3	Comparison with Restricted Nash Response
Eva I uatι on (PrSf,uffle = 0.0)
Evaluatιon( Prshuffle = 0.3)
KHR(nαnna∣>
——RHRftlS
——SES
KHK{nαnnal)
-RHR{k>lg)
—SES
WqqE
WqqE
RHR(nαnnal>
——RHIφlg)
——SES
0.2	0.4	0.6	0.8	1.0
P⅛)
Eva I u at I on( Prsftuffle = 0.7)
0.4	0.6	0.8	1.0
P(α)
Exp IO ιtab I hty{ Prshu me = 0.0)
Exploitabi hty(Prsħ0ffle = 0.3)
Exploitabi hty( Prsftuffle = 0.7)
⅛qqE
WqqE
⅛qqE
30
20
0.0	0.2	0.4	0.6	0.8	1.0
P(«)
RHMn。E».
RHIφlg)
SES
m∣R{nαvna∣>
——RNRftlg)
——SS
Figure 4: Comparison between SES and RNR. From left to right, each column represents a type of
opponent with Prshuffle = 0.0, 0.3, 0.7. The X-axis is the parameter α for SES and p for RNR.
We also compare SES with restricted Nash response (RNR) (Johanson et al., 2007) in FHP. In each
round, we limit the computation time of RNR(normal) to 10 CPU second1, which is the same for
SES. However, as stated in section 2, RNR needs to recompute a strategy for the whole game in each
round. It cannot converge in 10s. So we also compare with RNR(big), which has a budget of 10M
CFR iterations in each round (around 190 CPU second in time). In contrast, SES only uses 10M
CFR iterations to calculate its blueprint once. As is shown in Figure 4, SES significantly outperforms
RNR(normal) in both exploitability and evaluation. SES also achieves much lower exploitability than
RNR(big) and comparable evaluation results with much less computation time.
6 Conclusion
We propose a novel safe exploitation search (SES) algorithm which unifies both safe search and
opponent exploitation. With the aid of real-time search, SES can make online adaptations to a
changing opponent model. We also prove safety and opponent exploitation guarantees of SES in
Theorem 1 and Theorem 2. The experimental results in our designed matrix game confirm the
existence of the refined strategy which is both safe and actively exploiting the opponent. In games of
poker, our method outperforms NE baselines while keeping exploitability low. SES is also much more
efficient than previous safe exploitation algorithms without search. The exploitation level α is now
regarded as a hyperparameter in our algorithm. However, ideally, α should be learnt automatically
from opponents, and should be adaptive to opponent’s strategy change. We leave this for future work.
1We test it on Intel(R) Xeon(R) Platinum 8276L CPU @ 2.20GHz
9
Under review as a conference paper at ICLR 2022
References
Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive
survey and open problems. Artificial Intelligence, 258:66-95, 2018. ISSN 0004-3702. doi:
https://doi.org/10.1016/j.artint.2018.01.002. URL https://www.sciencedirect.com/
science/article/pii/S0004370218300249.
Darse Billings, Neil Burch, Aaron Davidson, Robert Holte, Jonathan Schaeffer, Terence Schauenberg,
and Duane Szafron. Approximating game-theoretic optimal strategies for full-scale poker. In
IJCAI, volume 3, pp. 661, 2003.
Noam Brown and Tuomas Sandholm. Safe and nested subgame solving for imperfect-information
games. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 689-699, 2017.
Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885-890, 2019.
Noam Brown, Tuomas Sandholm, and Brandon Amos. Depth-limited solving for
imperfect-information games. In Advances in Neural Information Processing Sys-
tems 31: Annual Conference on Neural Information Processing Systems, Mon-
treal, Canada, pp. 7674-7685, 2018. URL http://papers.nips.cc/paper/
7993-depth-limited-solving-for-imperfect-information-games.
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-
mization. In International conference on machine learning, pp. 793-802. PMLR, 2019.
Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement
learning and search for imperfect-information games. arXiv preprint arXiv:2007.13544, 2020.
Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using
decomposition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28,
2014a.
Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using
decomposition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28,
2014b.
David Carmel and Shaul Markovitch. Learning models of intelligent agents. In AAAI/IAAI, Vol. 1,
pp. 62-67, 1996.
Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor
Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, pp. 122-130, Richland,
SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems.
Sam Ganzfried and Tuomas Sandholm. Safe opponent exploitation. ACM Transactions on Economics
and Computation (TEAC), 3(2):1-28, 2015a.
Sam Ganzfried and Tuomas Sandholm. Endgame solving in large imperfect-information games. In
Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,
pp. 37-45. Citeseer, 2015b.
Andrew Gilpin and Tuomas Sandholm. A competitive texas hold’em poker player via automated
abstraction and real-time equilibrium computation. In AAAI, pp. 1007-1013, 2006.
Andrew Gilpin and Tuomas Sandholm. Better automated abstraction techniques for imperfect
information games, with application to texas hold’em poker. In Proceedings of the 6th international
joint conference on Autonomous agents and multiagent systems, pp. 1-8, 2007.
He He, Jordan Boyd-Graber, Kevin Kwok, and Hal DaUma III. Opponent modeling in deep
reinforcement learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 1804-1813, New York, New York, USA, 20-22 Jun 2016. PMLR. URL
http://proceedings.mlr.press/v48/he16.html.
10
Under review as a conference paper at ICLR 2022
Michael Johanson, Michael Bowling, and Martin Zinkevich. Computing robust counter-strategies.
2007.
Michael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state-space
abstractions in extensive-form games. In Proceedings of the 2013 international conference on
Autonomous agents and multi-agent Systems, pp. 271-278, 2013.
Daphne Koller, Nimrod Megiddo, and Bernhard Von Stengel. Fast algorithms for finding randomized
strategies in game trees. In Proceedings of the twenty-sixth annual ACM symposium on Theory of
computing, pp. 750-759, 1994.
Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H Bowling. Monte carlo sampling for
regret minimization in extensive games. In NIPS, pp. 1078-1086, 2009.
Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search
in cooperative partially observable games. Proceedings of the AAAI Conference on Artificial
Intelligence, 34(05):7187-7194, Apr. 2020. doi: 10.1609/aaai.v34i05.6208. URL https:
//ojs.aaai.org/index.php/AAAI/article/view/6208.
Xun Li and Risto Miikkulainen. Dynamic adaptation and opponent exploitation in computer poker.
In Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Peter McCracken and Michael Bowling. Safe strategies for agent modelling in games. In Artificial
Multiagent Learning, Papers from the 2004 AAAI Fall Symposium. Arlington, VA, USA, October
22-24, 2004, volume FS-04-02, pp. 103-110. AAAI Press, 2004. URL https://www.aaai.
org/Library/Symposia/Fall/2004/fs04-02-014.php.
Matej Moravcik, Martin Schmid, Karel Ha, Milan Hladik, and Stephen Gaukrodger. Refining
subgames in large imperfect information games. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 30, 2016.
Matej Moravcik, Martina Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor Davis,
Kevin Waugh, Michael Johanson, and Michael H. Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356:508-513, 2017.
Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S. M. Ali Eslami, and Matthew
Botvinick. Machine theory of mind. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4218-4227. PMLR, 10-15 Jul 2018. URL http://proceedings.
mlr.press/v80/rabinowitz18a.html.
Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in
multi-agent reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4257-4266. PMLR, 10-15 Jul 2018. URL http://proceedings.
mlr.press/v80/raileanu18a.html.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016. doi:
10.1038/nature16961. URL https://doi.org/10.1038/nature16961.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017.
11
Under review as a conference paper at ICLR 2022
Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes’ bluff: opponent modelling in poker. In Proceedings of the Twenty-First
Conference on Uncertainty in Artificial Intelligence, pp. 550-558, 2005.
Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit
texas hold’em. In Twenty-fourth international joint conference on artificial intelligence, 2015.
Yuandong Tian, Qucheng Gong, and Tina Jiang. Joint policy search for multi-agent collaboration
with imperfect information. arXiv preprint arXiv:2008.06495, 2020a.
Yuandong Tian, Qucheng Gong, and Yu Jiang. Joint policy search for multi-agent collaboration with
imperfect information. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 19931-19942. Curran Asso-
ciates, Inc., 2020b. URL https://proceedings.neurips.cc/paper/2020/file/
e64f346817ce0c93d7166546ac8ce683-Paper.pdf.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. Advances in neural information processing systems, 20:
1729-1736, 2007.
12
Under review as a conference paper at ICLR 2022
A	Proofs
A.1 Proof of Theorem 1
Theorem 1. (safety) Let S be a disjoint set of subgames S. Let σ* = hσj,σ2i be the NEwhereP2's
σ σ σ	1 ,
strategy is constrained to be the same with σ2 outside S. Define ∆ = maxS∈S,I i ∈Stop |CBV1 2 (I1i ) -
vf (Ii)|. Let P(Ii) be the reach probability given by σj. Let P(Ii) be the estimation of reach
probability P(Ii) given by the real opponent strategy. Define T 二 maxs∈s∣i∈Stop | P(Ip-P(II) |.
Whenever 1 - (2τ + 1)α > 0, we have a bounded exploitability given by:
2
exp(σ2) ≤ exP(σ2) + 1 - (2τ + 1)α△.	⑸
Proof:
*
For simplicity, we will omit the subscript of CBV1 2 by default. In order to prove Theorem 1, we
will use mathematical induction on the level of the infoset. The depth L has the same definition as in
Brown & Sandholm (2017), i.e.
•	For all the infosets which are direct parents of the subgames, we define L(I) = 0.
•	For the infosets that are not ancestors of the subgames, we define L(I) = 0.
•	For any infosets that are ancestors of the subgames, we define
L(I) = maxI0∈succ(I)L(I0) + 1. That is, it has a higher level than any of its successors.
Base Case of Induction
Firstly, we will prove that for any infoset with level 0, the inequality of theorem 1 holds. For
convenience, we consider that theorem 1 in a specific subgame S.
We will prove the infoset at the top of the subgame first. Since SE(σ2) ≥ (1 - α)(-∆) +
α PiP(Ii)(-△) = -△, we have
(1 - α)min 卜：(Ij) - CBV σ2 (Ij)) + α X P(Ii )(vf(Ii) - CBV σS (Ii))
I1j	i
=SE(σ2S)	(6)
≥SE ⑹)
≥ - △
since σs = argmax^ SE(σ2).
Furthermore, we have
X P(Ii )(vτ(Ii) - CBV σS (Ii))
i
=X P(Ii )(vf(Ii) - CBVσ* (Ii)) + X P(Ii )(CBV 咯(Ii) - CBV σS (Ii))
ii
≤∆ + XP(Ii)(CBVσ* (Ii) - CBVσS(Ii))	(7)
i
=△ + XP(Ii)(CBVσ* (Ii) - CBVσS(Ii))
i
+ X(P(Ii) - P(Ii ))(cbv σ* (Ii) - CBV σS (Ii))
i
where the second term PiP(Ii)(CBVσ*(Ii) - CBVσS(Ii)) is no larger than 0 because
*
Ei P(II)CBVσ2 (Ii) is exactly what σ胃 minimized. Otherwise, σ曰 can change the strategy in
the subgame so that he will get higher reward against σj which conflicts the definition of NE.
13
Under review as a conference paper at ICLR 2022
And we will further decompose I1i ∈ Stop into two parts, {I1i,-} and {I1i,+}. They have the property
that CBVσ2 (I：,-) - CBVσS (I；,-) ≤ 0 and CBVσ2 (I；,+ ) - CBVσS (I；,+) > 0. And since
PiP(Ii)(CBVσ2 (I；) - CBVσS (Ii)) ≤ 0 as discussed above, We have
XP(II)(CBV,“I；,-) - CBVσS (I；,-)) + XP(II)(CBVσ^ (I；,+) - CBVσS (I；,+))
Ii,-	Ii,+
=Xp(I1 )(CBVσ2(Il) - CBVσS (I；))	(8)
；
≤0
Which implies that
Xp(I；)(CBVσ2(Il,+ ) - CBVσS(I；,+ )) ≤ - Xp(I；)(CBVσ^ (I；,-) - CBVσS(I；,-)) (9)
I1i,+	I1i,-
Then We have
X(P(I；) -p(I1 ))(CBVσ2 (I；) - CBVσS(I；))
；
=X(P(I；,-) -P(I；,-))(CBVS(I；,-) - CBVσS(I；,-))
I1i,-
+ X(P(I；,+ ) - P(I；,+ ))(CBV々I；,+) - CBVσS(IIi,+ ))
I1i,+
≤τ( - XP(I；,-)(CBV球(I；,-) - CBVσS(IIiL))
I1i,-
(10)
+ XP(I；,+ )(CBVb“I；,+) - CBVσS(I：")))
I1i,+
≤ - 2τ Xp(I；,-)(CBV々I；,-) - CBVσS(IIiL))
I1i,-
≤ - 2τ min (CBVσ2 (Ij) - CBVσS (Ij))
The last inequation holds since minIj (CBVσ2 (Ij) - CBVσS(Ij)) ≤ 0 since σ^ is the strategy
With loWest exploitability by only changing strategy of σ2 in the subgames.
Back to Equation 7, We have
XP(I；)(v,(I；) - CBVσS(I；)) ≤ ∆ - 2τmjn(CBVσ2 (Ij)- CBVσS(Ij))	(u)
；	I1
And substitute it into Equation 6,
∆+ (1 - α - 2ατ) min(CBVσ (Ij) - CBV σS (Ij))
≥(1 - α) min	(v,(Ij)	- CBVσS (Ij))	+ α(∆ -	2τ) min(CBVG	(Ij)	- CBVσS (Ij))	(12)
≥-∆
so that
_s .人.	_* .	2
CBVσ2 (Ij) ≤ CBVσ2 (Ij) +	∆	(13)
1 - (2τ + 1)α
for all I1j in the subgame.
And for infoset I out of the subgame With level 0, since the refined strategy σ2S and blueprint strategy
σ2 are the same here, the CBV value is exactly the same and the inequality holds.
14
Under review as a conference paper at ICLR 2022
Inductive Step
The inductive step mostly follows that of Brown & Sandholm (2017).
S	-*,一
Since CBVσ2 (Ii) ≤ CBVσ2 (Ii) + 玳+Μ△ holds for every subgame S,必 Will also satisfy
this inequation since ∆ and τ are defined as maximum over all subgames.
Now, suppose CBVσ2 (Ii) ≤ CBVσ"lι) + -μ+必△ holds for any infoset with level lower or
equal to k, We Will prove that it also holds for infoset With level k + 1.
By definition of CBV(Ii),
CBVσ2(I1,a) = (X π-1(h)v9BR(σ"2 (h ∙ a))/ X π-21(h)
h∈I1	h∈I1
=(X∏-2i(h)	X	∏-i(h,h∙yJ,CBRg。巩h,/X∏-2i(h)	(14)
h∈I1	h0 ∈succ(h,a)	h∈I1
=(X X	π-σ2i(h0)vhCBR(σ2),σ2i(h0))/ X π-σ2i(h)
h∈I1 h0 ∈succ(h,a)	h∈I1
We can swap the two summations above since the game is perfect recall, then
CBVσ2(Ii,a) = ( X X π-σ2i(h0)vhCBR(σ2),σ2i(h0))/ X π-σ2i(h)	(15)
I10 ∈succ(I1 ,a) h0 ∈I10	h∈I1
By substituting the definition of CBV(Ii0) into the equation above,
CBVσ2(Iι,a)= (	X	CBVσ2(∣ι) X ∏-2i(h0))/ X ∏-2i(h)	(16)
I10 ∈succ(I1 ,a)	h0 ∈I10	h∈I1
And by the induction hypothesis,
CBVσ2(I" ≤ ( X	(CBV公(∣ι) +△ X ∏-2ι(h0))/ X ∏-2ι(h	⑺
I10 ∈succ(I1 ,a)	h0 ∈I10	h∈I1
Because Ii is out of the subgame and σg, σ2 is exactly the same outside the subgame, we will get
CBVσ2(Ii,a) ≤ ( X	(CBVσ2 (Ii) +三△ X 元^⑺)/X 元^㈤
I10 ∈succ(I1 ,a)	h0 ∈I10	h∈I1
=CBVσ"Ii,a) + 2-f △(	X X ∏-i(h0))/X ∏-i(h)(⑻
I10 ∈succ(I1 ,a) h0 ∈I10	h∈I1
*	2 — fα
=CBVσ2 (Ii,a) + -——△
1 — Q
Finally, by mathematical induction we get
-
exp(σ2) ≤ exp(σ2) + 1 —印 + 1)Q △	(19)
A.2 Proof of Theorem 2
Theorem 2. (opponent exploitation) Let E = kp — Pki be the L1 distance Ofthe distribution P(Ii) and
P(Ii). Let η = mins∈s max1j三5土 (CBVi(Ij,σS) — CBVI(Ij,σg)) ≥ 0. We use BRpS,σp](σ)
tO denOte the strategy fOr player P which maximizes its utility in subgame S ∈ S against σ-p under
the cOnstraint that B R[pS,σp] (σ) and σp differs Only inside S. By maximizing Objective 2, fOr all S ∈ S,
the refined strategy σ20 satisfies
DBR[S,σ1](σ2 ),σ2 E	D BR[S,σ1] (σ*)^σ*E	1 — Q
u2	1	( 2), 2(S) ≥u2	1	2(S) + ——(η - 2△)-Eη	(20)
Q
15
Under review as a conference paper at ICLR 2022
Proof: Still, we only consider a specific subgame S first.
σ2S is maximizing
(1 - α) min
I1j
v1σ(I1j)-CBVσ2S(I1j)
_________ - /
g(σ2S)
+α X P(Ii )(vσ (Ii) - CBV σS (Ii))
i
1-------------------{--------------------}
f(σ2S)
(21)
So, we have
(1 - α)g(σS) + αf (σS) ≥ (1 - α)g(σ2) + αf(σ2)
and
maxCBV(Ij,σS) - CBV(Ij,σ2) = η ≥ η
I1j
⇔g(σ2S) - ∆ ≤ -η
⇔g(σ2) - δ ≤ δ + g(σ2) - η	(g(σ2) ≥ -△)
Therefore,
αf (σS) ≥ αf (σ2) +(I- α)(η - 2δ)
which means
(22)
(23)
(24)
XP(Ii)(CBVσ2 (II)- CBVσS(Ii))
i
≥ 1-α(η - 2∆) + Xp(I1 )(CBVG (II)- CBVG (Ii))
α
i
⇔ - Xp(I1 )CBVσS (Ii) ≥ 丁 (η - 2∆) - Xp(I1 )CBVσ2 (Ii)
ii
⇔ - Xp(I1 )CBVσS (Ii) ≥ 1-α (η - 2∆) - XP(II)CBVσ2 (Ii)
ii
-X(p(Ii) -P^(IiX(CBVσS(Ii) - CBV咯(Ii))
i
⇒ - XP(Ii)CBVσS (Ii) ≥ j (η - 2∆) - Xp(Ii)CBVσ"Ii) - eη
α
ii
⇔ Xp(Ii)V2(Ii, BR(σS),σS) ≥ 1-α (η - 2∆) - eη + XP(Ii)V2 (Ii,BR(σ2),σ2)
α
ii
h BR[S,σ1] (rr[s~σ2] ) π∙[S-σ2]i	h B R[S,σ1 ] (rτ* ) rr*i	1 — Q
⇔u2BR1	(σ2	),σ2	i(S) ≥ u2BR1	(σ2 ),σ2i(S) +-(η - 2∆) - eη
2	2α
Since η is defined as minimum over all subgames, we have
DBR[S,σ1](σ20 ),σ20 E	DBR[S,σ1](σ2*),σ2*E	1 - α
U 1	( 2)	2 (S) ≥ U 1	2	2 (S) +----------(η - 2∆) - eη
α
(25)
(26)
B Poker Rules
Rules of Leduc Poker
Leduc Poker is a two players game. In Leduc Poker, there are 6 cards in total, three ranks({J, Q, K})
with two suits({a, b}) each. And at the beginning, every player should put 1 chip into the pot and
then will be dealt with one private card. Then, two players alternatively bet. They can call, raise and
fold. If any of them fold, the game ends and all chips in the pot belongs to the other player. And
16
Under review as a conference paper at ICLR 2022
when a player call, he has to put chips in the pot to ensure that he contributes equal chips as the other
player in the pot. When a player raise, he has to ensure that he contributes more chips than the other
player in the pot. A betting round ends when a player calls.
Leduc Poker is divided into two betting rounds. In the first round, a private card is dealt to each
player and then two player start to bet. After the first betting round ends and nobody folds, a public
card is dealt on board and the second betting round starts. When the second round ends, both of the
player show their private hands and the stronger hands win. If a player’s private card has the same
rank as the public card, then he wins. Otherwise, we have J < Q < K and the higher one wins.
And in each betting round, there will be at most two raises in our experiment and each raise should
contribute 1 more chip in the first round and 2 more chips in the second round.
Rules of Flop Hold’ em Poker
The rules of Flop Hold’em Poker is similar to that of Leduc Poker. In FHP, we use the standard
52-card deck. At the beginning, the first player will contribute 1 chip to the pot and the second player
will contribute 2 chips. And then they will be dealt with 2 private cards each and the first player start
to bet. There are still two betting rounds and the raise sizes are both 2 chips. At the end of the first
betting round, there will be 3 public cards dealt on board. And the players will show their private
card at the end of the second betting round and the larger one wins the game. In FHP, we have the
same rule of card order as a standard Texas hold ’em .
C Implementation Details
Leduc Poker. In Leduc Poker, we solve for a blueprint strategy using a variant of CFR algorithm
(Lanctot et al., 2009; Tammelin et al., 2015) with 1M iterations in the full game. Then we apply
search in subgames when the board card is dealt.
Flop Hold’em Poker (FHP). As for FHP, there are 1,286,792 different infosets for each betting
sequence. We cluster them into 200 infosets by an abstraction algorithm (Johanson et al., 2013) in
order to make equilibrium finding feasible. Then, we compute a blueprint strategy in this abstraction
with 10,000,000 iterations. We apply search immediately once the flop cards are dealt.
17