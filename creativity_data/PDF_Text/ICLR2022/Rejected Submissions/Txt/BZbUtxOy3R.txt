Under review as a conference paper at ICLR 2022
Character Generation through
Self-Supervised Vectorization
Anonymous authors
Paper under double-blind review
Ab stract
The prevalent approach in self-supervised image generation is to operate on pixel
level representations. While this approach can produce high quality images, it
cannot benefit from the simplicity and innate quality of vectorization. Here we
present a drawing agent that operates on stroke-level representation of images.
At each time step, the agent first assesses the current canvas and decides whether
to stop or keep drawing. When a ‘draw’ decision is made, the agent outputs a
program indicating the stroke to be drawn. As a result, it produces a final raster
image by drawing the strokes on a canvas, using a minimal number of strokes and
dynamically deciding when to stop. We train our agent through reinforcement
learning on MNIST and Omniglot datasets for unconditional generation and pars-
ing (reconstruction) tasks. We utilize our parsing agent for exemplar generation
and type conditioned concept generation in Omniglot challenge without any fur-
ther training. We present successful results on all three generation tasks and the
parsing task. Crucially, we do not need any stroke-level or vector supervision; we
only use raster images for training. Code will be made available upon acceptance.
1	Introduction
While, innately, humans sketch or write through strokes, this type of visual depiction is a more
difficult task for machines. Image generation problems are typically addressed by raster-based al-
gorithms. The introduction of generative adversarial networks (GAN) (Goodfellow et al., 2014),
variational autoencoders (VAE) (Kingma & Welling, 2013) and autoregressive models (Van Oord
et al., 2016) has led to a variety of applications. Style transfer (Gatys et al., 2015; Isola et al., 2017),
photo realistic image generation (Brock et al., 2018; Karras et al., 2019), and super resolution (Ledig
et al., 2017; Bin et al., 2017) are some of the significant instances of the advancing field. Addition-
ally, Hierarchical Bayesian models formulated by deep neural networks are able to use the same
generative model for multiple tasks such as classification, conditional and unconditional generation
(Hewitt et al., 2018; Edwards & Storkey, 2016). These raster-based algorithms can produce high
quality images, yet they cannot benefit from the leverage that higher level abstractions bring about.
Vector-level image representation intrinsically prevents models from generating blurry samples and
allows for compositional image generation which eventually may contribute to our understanding
of how humans create or replicate images (Lake et al., 2017). This idea, with the introduction of
sketch-based datasets such as Omniglot (Lake et al., 2012), Sketchy (Sangkloy et al., 2016), and
QuickDraw (Ha & Eck, 2017) has triggered a significant body of work in recent years. Stroke
based image generation and parsing has been addressed with both vector supervised models and
self-supervised generation. Of these, one prominent algorithm is Bayesian Program Learning (Lake
et al., 2015), where a single model can be utilized for 5 tasks in the Omniglot challenge: (i) parsing,
(ii) unconditional generation or, (iii) generating exemplars of a given concept, (iv) generating novel
concepts of a type, and (v) one-shot classification. This approach is also shown to be scalable
when supported by the representative capabilities of neural networks (Feinman & Lake, 2020b;a),
however, it requires stroke-level or vector supervision, which is costly to obtain or simply non-
existent. VAE/RNN (Ha & Eck, 2017; Cao et al., 2019; Chen et al., 2017; Aksan et al., 2020) and
Transformer based models (Ribeiro et al., 2020; Lin et al., 2020) are other common methods applied
to vector based image generation. Although impressive results have been presented, stroke-level
supervision is required to train these models.
1
Under review as a conference paper at ICLR 2022
Recently, self-supervised
(i.e. the absence of stroke-
level supervision) stroke-
based image generation
has been addressed with
Reinforcement Learning
(RL) (Ganin et al., 2018;
Mellor et al., 2019; Huang
et al., 2019; Schaldenbrand
& Oh, 2020). We call this
approach self-supervised
vectorization, since the
vectorization of images is
learned using only raster-
images as supervision.
，丫 mt0
K J巾T
6 Jztx O⅛
Figure 1: Our drawing agent can accomplish four different tasks.
From left to right: it can generate novel characters, parse a given char-
acter into its strokes, generate new exemplars for a given character,
and generate novel concepts (i.e. characters) given a type (i.e. al-
phabet). Ours is the first stroke-based method to tackle all of the
generation and parsing tasks in the Omniglot Challenge, without
requiring any stroke-level supervision.
These methods mostly focus on image reconstruction and their exploration in generation is limited.
For example, none of them address the conditional generation problem, or, they need the number of
actions (i.e. strokes) as input.
In this paper, we propose a self-supervised reinforcement learning approach where we train a draw-
ing agent for character generation and parsing. Our drawing agent operates on the stroke-level
(i.e. vector) representation of images. At each time step, our agent takes the current canvas as in-
put and dynamically decides whether to continue drawing or stop. When a ‘continue’ decision is
made, the agent outputs a program specifying the stroke to be drawn. A non-differentiable renderer
takes this program and draws it on the current canvas. Consequently, a raster image is produced
stroke-by-stroke. We first train this agent for two tasks by formulating appropriate loss functions:
(i) unconditional character generation and (ii) parsing.
Unconditional character generation is the task of generating a novel concept1 (i.e. character) given
a dataset of concepts. For this task, our loss function includes the following components: an adver-
sarial loss produced by a discriminator to make generated characters as “real” as possible, and two
data fidelity losses assessing the conformity of the current canvas with the statistical properties of
the overall dataset. We also use an additional entropy loss to prevent mode collapse.
In the parsing task, the goal for our agent is to reconstruct a given character (in raster-image) by
drawing it through strokes using as few of them as possible. We utilize the same action space and
environment as in the unconditional generation model, only difference being the input fed to the
policy is a complete canvas to be reconstructed. Our reward function in this task has two compo-
nents: a fidelity reward that indicates how much of a stroke is consistent with the target image and
a penalty that increases with every ‘continue’ action being taken. This model explicitly learns the
vectorization of the input raster-image in a self-supervised manner.
Next, we show that our parsing model can be exploited for exemplar generation (i.e. a novel drawing
of a given character) and novel concept generation from type (i.e. novel character generation given
an alphabet of 10 characters) without any further training. Given a character, the policy network of
our parsing model outputs a distribution over the action space where likelihood of actions at each
time step eventually allows us to generate variations of the input image. For novel concept generation
conditioned on a type (i.e. alphabet), we compose a stroke library by parsing the provided inputs.
As we sample strokes from this library, we observe novel samples forming, in coherence with the
overall structure of the alphabet. To the best of our knowledge, we are the first to tackle these tasks
with a self-supervised approach that operates on stroke space.
Through experiments we show that our agent can successfully generate novel characters in all three
ways (unconditionally, conditioned on a given alphabet, conditioned on a given character), and parse
and reconstruct input characters. For both exemplar generation and type conditioned novel concept
generation, we provide LPIPS (Zhang et al., 2018), L2 and SSIM measures between input samples
and generated images.
Our contributions in this paper are two-fold: (i) we present a drawing agent that can successfully
handle all of the generation and parsing tasks in the Omniglot challenge in a self-supervised, stroke-
1Omniglot challenge terminology.
2
Under review as a conference paper at ICLR 2022
based manner - such a model did not exist (ii) We provide for the first time perceptual similarity
based quantitative benchmarks for the ‘exemplar generation’ and ‘type conditioned novel concept
generation’ tasks.
2	Related Work
The main purpose of this Work is to present a self-supervised approach in order to solve the gener-
ation and parsing tasks in the Omniglot Challenge (Lake et al., 2015), by capturing the stroke-level
representation of images. Here We initially examine the supervised and self-supervised approaches
to Omniglot challenge. Then, We revieW the Work on image vectorization. And lastly, We touch
upon the research on program synthesis in the context of this study.
Omniglot Challenge Omniglot dataset of World alphabets Was released With a set of challenges:
parsing a given letter, one shot classification, generating a neW letter given an alphabet, generating
a novel sample of a character, and unconditional generation. Omniglot letters have samples that are
conditionally independent based on the alphabet-character hierarchy, hence, a distinctive approach
to achieve all these tasks is Hierarchical Bayesian modeling (Lake et al., 2015), (Lake et al., 2013).
As the Omniglot letters included human strokes as labels, the compositional and causal nature of
letters are leveraged to model the generation process. Later, neurosymbolic models are also shoWn
tobe successful for unconditional generation (Feinman & Lake, 2020a) and conceptual compression
for multiple tasks presented Within the Omniglot Challenge (Feinman & Lake, 2020b).
HoWever, Without the stroke set that generated a concept, these tasks become more difficult. The
idea of sequential image generation is examined by recurrent VAE models (Rezende et al., 2016),
(Gregor et al., 2015), (Gregor et al., 2016). DRAW (Gregor et al., 2015) and Convolutional DRAW
(Gregor et al., 2016) Were able to generate quality unconditional samples from MNIST and Om-
niglot datasets respectively. DRAW is proposed as an algorithm to generate images recurrently. The
netWork is able to iteratively generate a given image by attending to certain parts of the input at
each time step. Convolutional DRAW improved the idea With an RNN/VAE based algorithm that
can capture the global structure and loW-level details of an image separately in order to increase the
quality of generations. Later, it is shoWn that Hierarchical Bayesian Modeling can be improved by
the representational poWer of deep learning and attentional mechanisms in order to achieve three
of the five Omniglot challenges (Rezende et al., 2016). Another novel idea to leverage Bayesian
modeling to tackle Omniglot Challenge Was performing modifications on the VAE architecture to
represent hierarchical datasets (EdWards & Storkey, 2016) (HeWitt et al., 2018). The significance
of these studies is that they Were able obtain latent variables to describe class-level features effec-
tively. Despite the ability to utilize the same model for different problems (one-shot classification,
unconditional and conditional generation), raster-based one-step generative models have tWo disad-
vantages We Want to address. First, they cannot leverage the higher level abstraction and quality
comes With Working on a vector space. Secondly, one-step generation does not provide an inter-
pretable compositional and causal process describing hoW a character is generated. In this Work, We
combine the advantages of tWo groups of aforementioned models With an agent operating on stroke
representation of images that uses only raster images during training. Thus, We aim to solve all three
generative and the parsing (reconstruction) tasks of the Omniglot challenge. We shoW that the model
trained for reconstruction can also be adopted as a tool that captures the compositional structure of
a given character. Without any further training, our agent can solve exemplar generation and type
conditioned novel concept generation problems.
Image Generation by Vectorization — With Stroke Supervision Sketch-RNN (Ha & Eck,
2017) is the first LSTM/VAE based sketch generation algorithm. It is later improved to gener-
ate multiclass samples (Cao et al., 2019) and increase the quality of generations by representing
strokes as Bezier curves (Song, 2020). The idea of obtaining a generalizable latent space by image-
stroke mapping is studied by many (Aksan et al., 2020; Das et al., 2021; Bhunia et al., 2021; Wang
et al., 2020). In CoSE (Aksan et al., 2020), the problem is articulated as ‘completion of partially
draWn sketch’. They achieved state of the art reconstruction performance by utilizing variable-length
strokes and a novel relational model that is able to capture the global structure of the sketch. The
progress in stroke representation is continued With incorporation of variable-degree Bezier curves
(Das et al., 2021), and capturing Gestalt structure of partially occluded sketches (Lin et al., 2020).
3
Under review as a conference paper at ICLR 2022
Self Supervised Vectorization Self-supervised vector-based image generation problem has been
approached by RL based frameworks (Zhou et al., 2018), (Ganin et al., 2018), (Mellor et al., 2019),
(Huang et al., 2019), (Schaldenbrand & Oh, 2020), and (Zou et al., 2020). In SPIRAL (Ganin et al.,
2018), unconditional generation and reconstruction tasks are tackled with adversarially trained RL
agents. Succeeding research enhanced the reconstruction process by a differentiable renderer, mak-
ing it possible for agents to operate on a continuous space (Huang et al., 2019; Schaldenbrand &
Oh, 2020). In order to avert the computational expense of RL based algorithms, end-to-end differen-
tiable models are developed through altering the rendering process (Nakano, 2019) or formulating
the generation process as a parameter search (Zou et al., 2020). More recently, a differentiable ren-
derer and compositor is utilized for generating closed Bezier paths and the final image respectively
(Reddy et al., 2021). This method led to successful interpolation, reconstruction, and sampling pro-
cesses. Most related to our work is SPIRAL where both reconstruction and unconditional generation
is studied through self-supervised deep reinforcement learning. However, our approach has some
significant differences. First, in SPIRAL each stroke is also represented as a Bezier curve, yet, the
starting point of each curve is set as the final point of the previous curve. In our model, all control
points of the Bezier curve are predicted by the agent at each time step. Hence, the agent has to learn
the continuity and the compositionality of the given character in order to produce quality samples.
Secondly, SPIRAL provides a generative model that works through a graphics renderer without ad-
dressing the conditional generation problem. They show impressive results on both natural images
and handwritten characters. While we provide a solution for multiple generative tasks, we have
not explored our model in the context of natural images. Another approach that presents a similar
scheme to the reconstruction problem is “Learning to Paint” (Huang et al., 2019). In Learning to
Paint, the proposed model is utilized specifically for reconstruction. When reconstruction is consid-
ered, the main difference of our model is that since we try to model a human-like generation process,
our agent outputs a single stroke at each time step with the environment being altered throughout
this process while in Learning to Paint, 5 strokes are predicted by the agent at each time step. As a
major difference from previous studies, our agent decides whether to stop or keep drawing before
generating a stroke. This enables the agent to synthesize an image with as few actions as possible
when motivated with our reward formulations.
Self Supervised Program Synthesis Our method essentially outputs a visual program that de-
pends only on the rastered data. In that sense, studies on Constructive Solid Geometry (CSG)are
also related. Different RL frameworks for reconstruction of a given CSG image, that is essentially
a composition of geometric shapes, are proposed (Ellis et al., 2019; Zhou et al., 2020). The for-
mer considered parsing as a search problem that is solved by using a read-eval-print-loop within a
Markov Decision Process. The latter adopted a Tree-LSTM model to eliminate invalid programs and
the reward is considered to be the Chamfer distance between the target image and current canvas.
3	Method
Our model consists of a
policy network and a (non-
differentiable) renderer. At
time step t, the policy net-
work takes the current can-
vas, Ct -araster-image, as
input and outputs two dis-
tributions, πB and πS . The
first distribution, πB, is for
stroke (i.e. Bezier curve)-
parameters and the second
Figure 2: Generator model. At each time step, the policy network
receives a canvas and outputs two distributions for Bezier curve pa-
rameters and stop/continue decision. When the ‘continue’ decision is
sampled, the resulting stroke is rendered and added to the final output.
one, πS, is for the continue/stop decision. From the first distribution, we randomly sample a stroke
defined by its 7 parameters (x-y coordinates of start, end, control points of the quadratic Bezier
curve, and a brush-width). From the second distribution, we randomly sample a decision. If the
decision happens to be ‘continue’, we add the newly sampled stroke to the current canvas, Ct, in-
crement time (i.e. t J t + 1) and restart. If the decision was to 'stop'，then Ct is returned as the
final output. Our model is able to handle parsing and different generation tasks, and the processing
4
Under review as a conference paper at ICLR 2022
pipeline we just described is common in all these tasks. What changes among tasks is the reward
functions and/or training procedures, which we explain below.
Unconditional Generation The task of ‘generating new concepts’ as dubbed in Omniglot chal-
lenge, is essentially unconditional sampling from a distribution obtained from the whole Omniglot
training set. Here, the model is asked to generate completely novel samples (i.e. characters) without
any constraints. For this task, at each time step t, we calculate an instantaneous reward, rt , that has
three components:
rt = D(Ct) + λιalign(Ct,I) + λ2N(|Ct|; μ,σ).
(1)
The first term is a reward based on a discriminator to make generated characters as ‘real’ as possible.
D(∙) is a discriminator that outputs the “realness" score of its input canvas. We train it in an adver-
sarial manner by using the generated examples as negatives and the elements of the input dataset
as positives. The second term is a clustering-based data fidelity reward. The function align(Ct, I)
measures the alignment between the current canvas Ct and another canvas I, which is a randomly
selected cluster center at the beginning of each episode. The cluster centers are obtained by apply-
ing k-means on all characters in the input dataset. align basically counts the number of intersecting
on-pixels (between the two canvases) minus the number of non-intersecting on-pixels in Ct, and
divides this quantity by the number of on-pixels in I. The final term assesses the conformity of
the current canvas with the dataset in terms of the number of on-pixels. N(|Ct|; μ, σ) evaluates a
normal distribution with (μ, σ) at |Ct| which is the number of on-pixels in the current canvas. We
obtain (μ, σ) by fitting a normal distribution to the on-pixel counts of characters in the training set.
We observed that the second and third terms accelerate learning as they guide the exploration within
the vicinity of real characters. During training, instead of using the instantaneous reward, rt, we use
the difference of successive rewards, i.e. rt - rt-1.
In order to encourage exploration and avoid mode collapse, we use an entropy penalty term as
α max(0, KL([πB, πS], U) - τ).
(2)
Here, KL indicates KL-divergence and U is the uniform distribution. This term first measures the
divergence between the uniform distribution and πB , πS, the distributions output by the policy net-
work. Then, through the hinge function, if the divergence exceeds a threshold (τ ), this term activates
and increases the penalty. The policy network and the discriminator D are updated alternatingly af-
ter 256 images are generated at each iteration. We employ the REINFORCE algorithm (Williams,
1992) to update the weights of the policy network. Discriminator is trained using hinge loss. In
order to stabilize the discriminator and keep the Lipschitz constant for the whole network equal to
1, Spectral Normalization is applied at each layer (Miyato et al., 2018). Throughout the training, we
kept the balance ratio between generated and real samples at 3.
Image Reconstruction by Parsing In the “parsing” task, the goal is to reconstruct the given input
image by re-drawing it through strokes as accurately as possible. To this end, we formulate a new
reward function with two terms: a fidelity reward that indicates how much of a stroke is consistent
with the input image (using the “align” function introduced above) and a penalty that is added with
every time increment represented by t as ‘continue’ decisions being made:
rt = align(St, Ct) - λ1t,
(3)
where St is the newly sampled stroke and Ct is the current canvas (input). Second term simply acts
as a penalty for every ‘continue’ action. The first term ensures the sampled stroke to be well-aligned
with the input and the second term forces the model to use as few strokes as possible. There is no
need for a discriminator. This model explicitly learns the vectorization of the input raster-image in
a self-supervised manner.
Apart from the different reward function, another crucial difference between the training of the
unconditional generation model and the parsing model is how the input and output are handled.
In unconditional generation, the newly-sampled stroke is added to the current canvas, whereas in
5
Under review as a conference paper at ICLR 2022
parsing, we do the opposite: the sampled stroke is removed (masked out) from the current canvas,
and the returned final canvas is the combination of all sampled strokes until the ‘stop’ decision. λ, α
and τ in Equations 1, 2, and 3 are hyperparameters adjusted experimentally. (see ‘Training Details’
in Appendix B ).
Generating New Exemplars In this task, a model is required to generate a new exemplar (i.e.
a variation) of an unseen concept (i.e. character). To the best of our knowledge, we are the first
to tackle this task in a self-supervised stroke-based setting. Most importantly, we do not require
any training to achieve this task. We utilize our parsing network described in the previous section
to capture the overall structure of a given letter. In order to produce new exemplars, we randomly
sample different parsings (a set of strokes) from the distribution generated by the agent. In order to
eliminate ‘unlikely’ samples, we compute the likelihood of the parsing given the resulting policy,
and apply a threshold.
Generating Novel Concepts from Type In this task, the goal is to to generate a novel concept
(i.e. character) given a previously unseen type (i.e. alphabet) consisting of 10 concepts. The novel
concepts should conform to the overall structure, that is, the stroke formulation and composition of
the given type (alphabet). We, again, tackle this challenge using our parsing network without any
further training. To do so, we first parse all input images into its strokes. For each input image, we
sample five stroke sets from the stroke-parameters distribution output by the policy network. During
the sampling process, we again use the likelihood-based quality function described in the previous
section. We add all the strokes sampled during this process to form a stroke library. Here the strokes
are stored with the time steps they are generated. Noting that the number of strokes sampled for
a given character is not constant, we approximate a distribution for stopping actions. This process
provides a stroke set representing the structure of letters and the way they are composed, that is, we
can exploit the compositionality and causality of an alphabet. Throughout the character generation
process, a stroke is sampled at each time step belonging to that particular group of the library. The
sampled strokes are summed together to obtain the final canvas.
4	Experiments
Datasets and Implementation Details We report generation and reconstruction (parsing) results
on the Omniglot dataset (Lake et al., 2015), which includes 1623 characters from 50 different alpha-
bets, with 20 samples for each character. 30 alphabets are used for training and the remaining 20
are used for evaluation. For unconditional generation and reconstruction, we also report results on
the MNIST dataset (LeCun, 1998). For both datasets, we rescale input images to 32x32 in order for
them to conform with our model.
Our policy network is composed of a ResNet feature extraction backbone and three MLP branches
for computing the distributions over the action space. Architectural details can be found in Appendix
A. For the Omniglot dataset, we take brush width as a constant and omit the corresponding MLP
branch.
We tune the learning rate and weight decay of the
generator, λ hyperparameters in Equation 1 and
Equation 3, α and τ hyperparameters in Equa-
tion 2, using the Tree-structured Parzen Estimator
algorithm (Bergstra et al., 2011) in the RayTune
library (Liaw et al., 2018).
For unconditional generation, we use the dis-
criminator architecture proposed by Miyato et al.
(2018). In order to stabilize the discriminator and
keep the Lipschitz constant for the whole network
equal to 1, Spectral Normalization is applied at
each layer. Discriminator is trained using the
hinge loss. Throughout the training, we set the
	FID
LSGAN (Maoetal., 2017)	9.8 ± 0.9
NSGAN (Goodfellow etal., 2014)	6.8 ± 0.5
WGAN (Arjovsky et al., 2017)	6.7 ± 0.4
WGAN GP (Gulrajani et al., 2017)	20.3 ± 5.0
DRAGAN (Kodali et al., 2017)	7.6 ± 0.4
VAE		23.8 ± 0.6
Ours	17.3 ± 3.0
Table 1: Comparison of the FID scores for dif-
ferent models on the MNIST dataset. We report
the mean and the variance of FID scores from 5
simulations with different weight initializations.
balance ratio between fake and real samples as 3. We performed hard-negative mining to speed up
convergence during this process.
6
Under review as a conference paper at ICLR 2022
6 06 IJ 名 q√-/3
CK43I¾勺/17*l⅜z
4≠ λ%o¾
6 r⅛ Ga 19工七 L5
Ktt><3^λα6 2茗9 *
3 * 2 5 3 9w⅛IA¾
Gl>∖52A，2/、
夕ZqSI¢-i-。
<ψIaMΓ*jgf
^c*f^ltΓIICΓI6*IΓI
KJve*c∕0 A⅛8r⅛r
SrZ Sz 6 a—SAS
⅛ 4 5 F T σ P4?密
Vgr<lɛe KIrXtfg
IYIZ-6 今夕&S & 彳％
Z Γ4r4* 勺69W
26 9Q2.∙∕MG 始
⅛ΛUI2SO4fI户 乂歙号
壬月：QKA才卖二
l-z0l÷5 甲之 Ul⅛l>%
斗G济⅜>y L蛤6嫉《
M1ΓI乂寸夕 S W1fil⅜^⅞
l⅜l⅜*住 c⅛ly，V
SN舌羊∕⅜⅛^%l.⅛lR
Figure 3: Quality of generated MNIST characters as training progresses (i.e. policy is updated) from
left to right.
㈣画画应
远四囚叵
回皿回匝
/冏网F
0 0EEE
0] HlriIriTT
®回回国叵
团 EfflfflE
E ΞSΞE
ffi [SEfflE
& A	X	3
X /	<	X
0 O	D	O
B B	P	F
	τ	「
O ɑ	O	Q
Figure 4: Omniglot unconditional samples. For randomly sampled generations, four closest samples
(in terms of pixelwise L2 distance) from the training dataset are presented.
4.1 Unconditional Generation
We initially tested our approach on the MNIST dataset. Figure 3 presents the improvement in the
quality of samples generated throughout the policy network updates. At the beginning, generated
characters are mostly random scribbles. Towards the end, they start to look like real digits. Table
1 shows that our method achieves an acceptable FID score (Heusel et al., 2017) given the scores of
other prominent GAN and VAE methods. Presented FID values are taken from Lucic et al. (2017).
Figure 4 shows sample generations for the Omniglot
dataset. To demonstrate that our generations are not du-
plicates of the characters in the training set, we present the
four most similar characters from the training set to our
generations. Similarity is computed using pixelwise L2
distance. Finally, Figure 5 presents more generated char-
acters, which demonstrate the variability and the quality
of generated concepts. The agent was able to capture the
type of strokes, number of strokes a character has and let-
ter structures without any stroke supervision.
4.2 Image Reconstruction by Parsing
Figure 5: Randomly sampled uncon-
ditional generations for the Omniglot
dataset.
Figure 6 presents sample parsing and reconstruction re-
sults on MNIST. Our agent can reconstruct a character
from the test set in a minimal number of actions within
the abilities of quadratic Bezier curves. Selected brush
widths also conform with the stroke heterogeneity of the
dataset.
Then, we train our model with the characters in the Om-
niglot training set. For evaluation, we utilize the evaluation set with completely novel characters
from unseen alphabets. Thereby, we can see that our agent has learned how to parse a given char-
acter. Due to the penalty term that increases with the number of strokes, there is a tradeoff for the
agent to replicate a character exactly and replicate it in a small number of actions. This indirectly
demotivates the agent from retouching the image with small strokes to minimize the difference to the
7
Under review as a conference paper at ICLR 2022
夕。夕
夕夕夕
3 ?办
7 7/
7 7 7
/U 6 6
6 6
r, 5
IPr 5
/V
l√y “
ʒ 3 5
3 3 5
> Z Λ
2Z 2
Ood
。。〃
Figure 6: MNIST reconstructions. For each sample on the left hand side of the columns, parsing
processes are demonstrated. Colors represent the order of the strokes. (pink: first stroke, green:
second stroke, blue: third stroke)
Figure 7: Omniglot reconstruction. For each sample on the left hand side of the columns, resulting
reconstructions are demonstrated.
target. Results in Figure 7 show that overall structure of the target images are preserved, however,
small details are lacking in some of the examples. This reflects on the distance measures (Table 2).
4.3	Generating New Exemplars
For this task, we use the evaluation set of the Omniglot dataset. For each character in the test set,
we sample 500 different parses from the policy. In Figure 8, it can be observed that given an unseen
letter from a novel alphabet, our agent can sample from the resulting distribution, and output quality
variations. The major indications of variation are structures of the strokes, number of actions to
generate a sample and the fine details of certain characters. We compare each produced character
with its corresponding input image using LPIPS, SSIM and L2 distance values. The mean and
standard deviation of these values for the whole evaluation set are 0.078 ± 0.002, 0.616 ± 0.018 and
0.08 ± 0.016, respectively. Results per alphabet can be found in Appendix C.3.
4.4	Generating Novel Concepts from Type
In order to generate a concept that is likely to belong in a given alphabet, we again leverage our
reconstruction model. Given 10 different characters of an unseen alphabet, we are able to generate
novel images with similar structural features. Results presented in Figure 9 show that our algorithm
can model the compositional pattern of an alphabet in stroke space. In order to obtain quantitative
results, (e.g. LPIPS, L2 and SSIM), we produce 10000 images conditioned on each input set and
randomly sample characters by utilizing the discriminator trained for the unconditional generation
model, assuming it has learned what features of a given input imply a real character. We generate
Method	MNIST	Omniglot
ImageVAE	0.0033	N/A
Im2Vec (Reddy et al., 2021)	0.0036	N/A
Learning to Paint (Huang et al., 2019)	0.006	N/A
SPIRAL (Training distance) (Ganin et al., 2018)	0.01	0.02
StrokeNet (Training distance) (Zheng et al., 2018)	0.015	0.02
Ours	0.04	0.06
Table 2: Reconstruction quality. L2 distance between target and the reconstructed image. (Im-
ageVAE is taken from Reddy et al. (2021), it indicates a purely raster-based autoencoder. )
8
Under review as a conference paper at ICLR 2022
E	0	S	H	0	
1 1 7	Γ∏ E E	Ce G)	ð © 6	X h h	够由雷
X 1 1	Em m	r⅛∕⅛ r⅛	a ð >		屯E有
ILL	m m(r\	四金八	β ð ⅞	M> /中修	场倡（S
_^9 夕?
7 79
7/7
-T一7τ7φ
7r7
Xxx
S乂 Xy
Xxx
Ν以双
圆0 W双
回 0
fi	α	n	6 6 4
Ti	∏	α	Z)Qz)
n	n	λ	ZJQ 心
Figure 8:	New exemplar generation. Given an unseen character from a new alphabet (highlighted in
red boxes), the model generated 9 exemplars.
-QqUV SoU 〜、3h
d>,0αΓX∙J3 夕Λ
9 4 : ：W∙IrIh PHH
<dqoQv1q4s8"
口 6%丸9彳，4IZI口
Iff5 A 至 E5H巩口VL
C- IOG n∖φGFrrA
6 9ΓrJ*3Λr49
5 P A 5 3 ? W I ⅛ rt_
143*3y(φ.√py
0 5bqJ214v1
一尸IH>IVJ 夕；3eJl⅛ 4J _
YdJy∕τ.J才工 S
H1715 匕 4 Tl 7π^1
PnH ”力J〃甲力邦
9 5 Hv H74 彳 CJ 0 7
K WΛτm
Figure 9:	Novel sample generation conditioned on a type. Given 10 characters from an alphabet,
our model produced 20 new samples.
a sampling distribution according to the discriminator scores of generated samples and repeat the
sampling process multiple times for each input to obtain a set of outputs to be considered. For a
sample generated, we calculate performance metrics with respect to all characters in the input. In
order to report final metrics presented in supplemental figures 12b and 12a, we consider the most
similar input-output pairs. The mean and standard deviation of LPIPS, SSIM and L2 values for the
whole evaluation set are 0.0801 ± 0.003, 0.502 ± 0.068 and 0.1263 ± 0.00086 respectively.
5 Conclusion
We proposed a self-supervised reinforcement learning approach for stroke based image generation.
We trained our model for unconditional generation and parsing on handwritten character datasets by
defining a single action space and environment. Through experiments, we showed that, given the
whole training set, our agent is able to capture the overall distribution and generate quality novel
samples for the challenging Omniglot dataset. Then, we trained our agent for the parsing task; given
a raster image, the goal is to reconstruct it through as few strokes as possible. We demonstrated that
the parsing agent can be utilized for generating exemplars of a concept and creating novel samples
conditioned on a type, without any further training, only difference being how it is called among
tasks. To the best of our knowledge, we are the first to tackle these tasks with a self-supervised
approach that operates on a stroke level. In this work, we used quadratic Bezier curves as the
smallest unit of sketching. However, for human-level generations, the stroke representations should
be enhanced to capture more complex structures. We anticipate that this will improve the overall
performance.
9
Under review as a conference paper at ICLR 2022
References
Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, and Otmar Hilliges. Cose: Compositional
stroke embeddings. arXiv preprint arXiv:2006.09930, 2020.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyper-parameter
optimization. Advances in neural information processing systems, 24, 2011.
Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin Yang, Timothy M Hospedales, Tao Xiang,
and Yi-Zhe Song. Vectorization and rasterization: Self-supervised learning for sketch and hand-
writing. arXiv preprint arXiv:2103.13716, 2021.
Huang Bin, Chen Weihai, Wu Xingming, and Lin Chun-Liang. High-quality face image sr using
conditional generative adversarial networks. arXiv preprint arXiv:1707.00737, 2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Nan Cao, Xin Yan, Yang Shi, and Chaoran Chen. Ai-sketcher: A deep generative model for pro-
ducing high-quality sketches. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 2564-2571, 2019.
Yajing Chen, Shikui Tu, Yuqi Yi, and Lei Xu. Sketch-pix2seq: a model to generate sketches of
multiple categories. arXiv preprint arXiv:1709.04121, 2017.
Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, and Yi-Zhe Song. Cloud2curve: Gen-
eration and vectorization of parametric sketches. arXiv preprint arXiv:2103.15536, 2021.
Harrison Edwards and Amos Storkey. Towards a neural statistician. arXiv preprint
arXiv:1606.02185, 2016.
Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama.
Write, execute, assess: Program synthesis with a repl. arXiv preprint arXiv:1906.04604, 2019.
Reuben Feinman and Brenden M Lake. Generating new concepts with hybrid neuro-symbolic mod-
els. arXiv preprint arXiv:2003.08978, 2020a.
Reuben Feinman and Brenden M Lake. Learning task-general representations with generative neuro-
symbolic modeling. arXiv preprint arXiv:2006.14448, 2020b.
Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Ali Eslami, and Oriol Vinyals. Synthesiz-
ing programs for images using reinforced adversarial learning. In International Conference on
Machine Learning, pp. 1666-1675. PMLR, 2018.
Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv
preprint arXiv:1508.06576, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: A recurrent
neural network for image generation. In International Conference on Machine Learning, pp.
1462-1471. PMLR, 2015.
Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. arXiv preprint arXiv:1604.08772, 2016.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
10
Under review as a conference paper at ICLR 2022
David Ha and Douglas Eck. A neural representation of sketch drawings. arXiv preprint
arXiv:1704.03477, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017.
Luke B Hewitt, Maxwell I Nye, Andreea Gane, Tommi Jaakkola, and Joshua B Tenenbaum. The
variational homoencoder: Learning to learn high capacity generative models from few examples.
arXiv preprint arXiv:1807.08919, 2018.
Zhewei Huang, Wen Heng, and Shuchang Zhou. Learning to paint with model-based deep reinforce-
ment learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 8709-8718, 2019.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401-4410, 2019.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.
arXiv preprint arXiv:1705.07215, 2017.
Brenden Lake, Ruslan Salakhutdinov, and Joshua Tenenbaum. Concept learning as motor program
induction: A large-scale empirical study. In Proceedings of the Annual Meeting of the Cognitive
Science Society, volume 34, 2012.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. One-shot learning by inverting
a compositional causal process. 2013.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and brain sciences, 40, 2017.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 4681-4690, 2017.
Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Sto-
ica. Tune: A research platform for distributed model selection and training. arXiv preprint
arXiv:1807.05118, 2018.
Hangyu Lin, Yanwei Fu, Xiangyang Xue, and Yu-Gang Jiang. Sketch-bert: Learning sketch bidi-
rectional encoder representation from transformers by self-supervised learning of sketch gestalt.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
6758-6767, 2020.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans
created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.
11
Under review as a conference paper at ICLR 2022
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE international confer-
ence on computer vision,pp. 2794-2802, 2017.
John FJ Mellor, Eunbyung Park, Yaroslav Ganin, Igor Babuschkin, Tejas Kulkarni, Dan Rosenbaum,
Andy Ballard, Theophane Weber, Oriol Vinyals, and SM Eslami. Unsupervised doodling and
painting with improved spiral. arXiv preprint arXiv:1910.01007, 2019.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Reiichiro Nakano. Neural painters: A learned differentiable constraint for generating brushstroke
paintings. arXiv preprint arXiv:1904.08410, 2019.
Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy J Mitra. Im2vec: Synthesizing vector
graphics without vector supervision. arXiv preprint arXiv:2102.02798, 2021.
Danilo Rezende, Ivo Danihelka, Karol Gregor, Daan Wierstra, et al. One-shot generalization in deep
generative models. In International Conference on Machine Learning, pp. 1521-1529. PMLR,
2016.
Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and Moacir Ponti. Sketchformer:
Transformer-based representation for sketched structure. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 14153-14162, 2020.
Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. The sketchy database: learning to
retrieve badly drawn bunnies. ACM Transactions on Graphics (TOG), 35(4):1-12, 2016.
Peter Schaldenbrand and Jean Oh. Content masked loss: Human-like brush stroke planning in a
reinforcement learning painting agent. arXiv preprint arXiv:2012.10043, 2020.
Yi-Zhe Song. Beziersketch: A generative model for scalable vector sketches. Computer Vision-
ECCV 2020, 2020.
Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In
International Conference on Machine Learning, pp. 1747-1756. PMLR, 2016.
Alexander Wang, Mengye Ren, and Richard Zemel. Sketchembednet: Learning novel concepts by
imitating drawings. arXiv preprint arXiv:2009.04806, 2020.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229-256, 1992.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018.
Ningyuan Zheng, Yifan Jiang, and Dingjiang Huang. Strokenet: A neural painting environment. In
International Conference on Learning Representations, 2018.
Chenghui Zhou, Chun-Liang Li, and Barnabas Poczos. Unsupervised program synthesis for images
using tree-structured lstm. arXiv preprint arXiv:2001.10119, 2020.
Tao Zhou, Chen Fang, Zhaowen Wang, Jimei Yang, Byungmoon Kim, Zhili Chen, Jonathan Brandt,
and Demetri Terzopoulos. Learning to sketch with deep q networks and demonstrated strokes.
arXiv preprint arXiv:1810.05977, 2018.
Zhengxia Zou, Tianyang Shi, Shuang Qiu, Yi Yuan, and Zhenwei Shi. Stylized neural painting.
arXiv preprint arXiv:2011.08114, 2020.
12
Under review as a conference paper at ICLR 2022
A	Network Architecture
The backbone is a ResNet with 3 convolutional layers and 8 residual layers. The first convolutional
layer has 32 filters of size 5x5. The second and third convolutional layers have 32 filters of size
4x4 and stride of 2, resulting in a tensor with dimensions 8x8x32. Then, we use standard residual
layers described in (He et al., 2016). Each convolutional layer is followed by a Batch Normalization
process and ReLU activation. The output of the final residual layer is flattened to a 2048x1 vector to
be processed by the MLPs. The first MLP outputs a set of distributions for each control point of the
Bezier curve. It has 1 fully connected layer that outputs a 192x1 vector. This vector is reshaped to a
32x6 matrix where each 32x1 vector defines a distribution over the possible coordinates. The MLPs
used for selecting the brush width and sampling the stop/continue decision consist of 2 layers with
64 and 2 neurons.
B Training Details
The hyperparamaters used for unconditional generation and reconstruction are presented in Table 3
and 4, respectively.
λl	1.016
λ2	1
α	0.336
T	0.415
Policy network optimizer	AdamW
Policy network learning rate	3.096e - 05
Policy network weight decay	0.0064
Discriminator learning rate	0.0001
Batch size	256
Table 3: Hyperparameters for unconditional generation. λ1 and λ2 refer to the respective hyper-
parameters in Equation 1. α and tau refer to the respective hyperparameters of entropy penalty in
Equation 2.
λ1	0.089
α	0.59
τ	2.72
Policy network optimizer	AdamW
Policy network learning rate	1.5e - 4
Policy network weight decay	1.6e - 5
Batch size	256
Table 4: Hyperparameters for reconstruction (parsing). λ1 refers to the ‘number of action’ penalty
in Equation 3. α and tau refer to the respective hyperparameters of entropy penalty in Equation 2.
C	Experiments: Supplemental Figures
C.1 Unconditional Generation
In Figure 10, we present the FID values for the generated images along the training on Omniglot
dataset.
C.2 Parsing
In Table 5, we present the mean number of strokes our agent used to parse the characters for each
alphabet in the test set.
13
Under review as a conference paper at ICLR 2022
Figure 10: FID values for unconditional generations of Omniglot dataset throughout the training
process. The experiment is repeated over 3 seeds.
Alphabet	Number of Strokes		Sample Image
	Our Model	Human Labeled Data	
Angelic	3.935	4.49	gp口叫%!＞用 o T 1 IIlIllHIql]∣y w fir 1刚 回收⑹斌百IMI@ I邓b∣⅞"∣ IKAlvTl上已已NIz仄W lɪ正市丁弓R I 3|叫 出if%rf⅛T窗内用养F ∣⅞^∣⅜∣^⅞ 晒 W m 亮 金/黝 δi ⅛ ISM J3. jE=. TL aɪ .⅛ ⅜* JH u in 如 卫.3 皓3Ea∣HC Ga 曲 JJ- «T 3*"万下开 ɪj? H K Jir l√Ert 力不 ɪɪʌɪɪɪɪɪɪɪ !现0修网(3冏0恂恸网 阻μ虫可至 5只用瓦VI 9、j u 亚； 9^^3以0 3^口4 三三三豆亘至m三亘三 Q r UW 9 0 I》寸、、
Atemayar Qelisayer	1035	3.571	
Atlantean	6.209	2.078	
AUrek-Besh	7.6	2.565	
Avesta	95∏	T52	
GeZez	-10TTΓ2-	T984	
Glagolitic	524	2.88	
Gurmukhi	6.080-	3.09	
Kannada	-4217	2.33	
Keble	一	8573	4.140	
Malayalam	7.215	T453	
ManiPUri	-10.676-	2.82	
Mongolian	893	2.405	
Old Church Slavonic	5T7T	2.954	
Oriya	559	2.82	
Sylheti	TT38	2.84	
Syriac	6.35	2.206	
TengWar	8.088	2.492	
Tibetan	TT69	3.62	
ULOG	6.417 一	3.253 —	
Table 5: For each alphabet in the Omniglot evaluation set, we present the number of strokes our
agent used to reconstruct the given image vs. mean number of strokes obtained from human-labeled
data. The stroke count for human-labeled data is calculated using the labels within the Omniglot
dataset.
C.3 Exemplar Generation
In Figure 11a, we demonstrate LPIPS metrics calculated by using 3 different backbones (AlexNet,
VGG, and SqueezeNet). In Figure 11b, we present L2 and SSIM values. These metrics are calcu-
lated over all examples generated for the test set.
C.4 Generating Novel Concepts from Type
In Figure 12a, we demonstrate LPIPS metrics calculated by using 3 different backbones (AlexNet,
VGG, and SqueezeNet). In Figure 12b, we present L2 and SSIM values.
14
Under review as a conference paper at ICLR 2022
(a)	(b)
Figure 11:	LPIPS values for each alphabet in the test set calculated from sampled exemplars (a),
SSIM and L2 values for each alphabet in the test set calculated from sampled exemplars (b).
ULOG
Tibetan
Tengwar
Syriac
Sylheti
Oriya
Cyrillic
Mongolian
Manipuri
Malayalam
Keble
Kannada
Gurmukhi
Glagolitic
Ge_ez
Avesta
Aurek-Besh
Atlantean
Atemayar
Angelic
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
Mean LPIPS
(b)
(a)
Figure 12:	LPIPS values for each alphabet in the test set calculated from novel samples produced
(a), L2-SSIM values for each alphabet in the test set calculated from novel samples produced (b).
15