Under review as a conference paper at ICLR 2022
Improved Generalization Bounds for Deep
Neural Networks Using Geometric Func-
tional Analysis
Anonymous authors
Paper under double-blind review
Ab stract
Understanding how a neural network behaves in multiple domains is the key to
further its explainability, generalizability, and robustness. In this paper, we prove
a novel generalization bound using the fundamental concepts of geometric func-
tional analysis. Specifically, by leveraging the covering number of the training
dataset and applying certain geometric inequalities we show that a sharp bound
can be obtained. To the best of our knowledge this is the first approach which
utilizes covering numbers to estimate such generalization bounds.
1	Introduction
The problem of generalization of deep neural networks from the perspective of theoretical analysis
has recently received a considerable amount of interest (Neyshabur et al., 2015; Zhang et al., 2017;
Dziugaite & Roy, 2017; Neyshabur et al., 2018; Golowich et al., 2018; Nagarajan & Kolter, 2018;
Daniely & Granot, 2019). More specifically, most of the state-of-the-art bound are based on spectral
norm based generalization bounds and have shown to give tighter and sharper bounds compared to
conventional ones leveraging PAC Bayesian theory (Bartlett et al., 2017; Neyshabur et al., 2015;
2018). However, the bounds in such works are quite limited as they only apply in cases where the
parameters are drawn from a distribution or when they are represented by fewer bits than required
(Nagarajan & Kolter, 2018).
In this paper, we provide a novel framework based on the foundational mathematics of geometric
functional analysis to obtain a sharp bound. We show that this analysis has many advantages over
the conventional conceptions which solely rely on stochastic assumptions ignoring the geometric
structure of the neural networks and to the best of our knowledge this is the first work attempts to
foray in this direction. We compare our result with that of Neyshabur et al. to show that our bound
is better than the bounds derived using spectral norms.
Geometric functional analysis deals with infinite dimensional vector spaces from a geometric per-
spective (Holmes, 2012), of its many applications it is widely in the theoretical analysis of wavelet
theory (Young, 2012). In fact we believe this is the most suited framework for dealing with deep
neural networks, as their parameters are generally huge and any frame work that is finite will not
cover all possible models. Specifically, our approach involves using the covering number of a vector
space to derive a bound for the generalization error of a given neural network. However, comput-
ing the covering number of a high dimensional vector space becomes an intractable problem as the
size of the input space becomes sufficiently large. Thus, we make use of the Vitushkin’s inequality
(Friedland & Yomdin, 2014) to bound the covering numbers using the Lesbesgue measure, which is
a tractable quantity. This bound along with functional inequalities forms the groundwork by which
we extend it generalization problem of neural networks which also takes into account the geometry
of the data input. Another by product of the analysis is that the bound obtained is in terms of the
Frobenius norm of the weight matrices rather than its spectral norm, which has shown to scale more
rapidly with the size of the matrix (Vershynin, 2011).
We approach the neural network in a rather unorthodox manner by treating it as a recurrent poly-
nomial function which can be approximated to some arbitrary degree (Telgarsky, 2017), as it is
the most amicable framework for exploiting its geometric properties. We explain this in detail in
1
Under review as a conference paper at ICLR 2022
the following sections along with other mathematical preliminaries. This is followed by several
intermediate theorems, which are then used to derive our generalization bound.
2	Related Work
The concept of generalization bounds were introduced in (Bartlett, 1998) rather indirectly by ana-
lyzing the probability of misclassification. (Zhang et al., 2017) showed that uniform convergence
over data points is what is required to understand the generalization of networks. Dziugaite & Roy
(2017) obtained non vacuous bounds. (Neyshabur et al., 2017) compared the effect of norm based
control, sharpness and robustness on generalization. (Neyshabur et al., 2018) used spectral norm to
give a bound which incorporated the weights of the neural networks. (Arora et al., 2018) obtained
sharp generalization bounds in terms of sample complexity. (Bietti et al., 2019) used Reproducible
kernel Hilbert spaces for studying regularization of deep neural network from analytic viewpoint.
3	Mathematical Preliminaries
3.1	Polynomial Framework of Neural Networks
Consider the mathematical formulation of Neural Networks, a neural network N with l layers is a
function which takes an input vector x and returns a vector N(x) such that:
N(x) =fl(Wl...f2(W2f1(W1x))...)
(1)
where Wi is the weight matrix of network at layer ith layer, fi is the corresponding non-linear
activation function. If xl = [xl1 . . . xlnl] is the output of the lth layer with nl nodes, then:
xl = gl(Wlxl-1)	(2)
As mentioned earlier if the activation functions are approximated using polynomials then mathe-
matically there are only polynomials at each node at every layer in the network, so in our analysis
we consider the activation functions as polynomials with certain degrees. Let ai(jl-1,l) be the weight
of the neural network from the ith node in l - 1th layer to the jth node in the lth layer and Pl be
the Polynomial approximation of the activation function. Then based on the above mathematical
description of neural networks the output at 2nd layer with n2 nodes is
x2
P1
ai(11,2)xi1
, P1
(3)
Similarly for the 3rd layer, x3 can be obtained as
n2	n1	n2
X aj2，3)Pi (X ad，2*) I R (X aj∣,3)Pι
P (Xa(2⑶Pl XX a(1,2)χi!ʌ #
. . . , P2 (	ajn3 P1	ain2 x1 I
(4)
Clearly, by standard properties of polynomials the above tuple is also a polynomial. Now, if we look
into the pattern followed by each layer then the general form of the output at any arbitrary lth layer
can be obtained as
2
Under review as a conference paper at ICLR 2022
xl
nl-1
Pl-1	X
nl-1	nl-2
X aj(l1-1,l)Pl-2	X ai(1l-2,l-1)xli-2
j=1	i=1
Pl-1
j=1
ai(2l-2,l-1)xli-2
a(l-2,l-1)xi
inl-1	l-2
(5)
By the same logic applied hitherto the above tuple is also a polynomial. The reason for such an
unconventional formulation of a neural network as in the above framework is to analyze the degree
of the polynomials using some of its basic properties. Regarding degree of polynomials we have the
following standard results:
For any c1, c2, . . . ck ∈ R and polynomials P1 , P2, . . . Pk
deg(c1P1 + . . . + ckPk) ≤ sup	|deg(Pi)|	(6)
i={1,...,k}
where deg(.) is the degree of the polynomial and for any polynomial Pa and Pb
deg(PaPb) = deg(Pa)deg(Pb)	(7)
By applying equation (7) on the first element of the vector xl from equation (5) we get
nl-1
X a(jl1-1,l)Pl-2
j=1
(8)
Similarly, by applying equation (6) on equation (8) we obtain
nl-1	nl-2
X a(jl1-1,l)Pl-2	X ai(1l-2,l-1)xli-2
j=1	i=1
≤ deg(Pl-1)|deg(Pl-2)|
(9)
By iterating over all the layers in the network and applying equation (9) we get
deg(Pl) ≤ deg(Pl-1)deg(Pl-2) . . . deg(P1)	(10)
We make use of the above inequality in the next sections in obtaining the generalization bound.
3.2	Covering number
Covering number of a general topological space counts the number of spherical balls needed to
cover the entire space (Munkres, 2000). This is relevant to our present analysis as it gleans the
behaviour of a neural network on an unknown set by understanding its behaviour on a known one.
Formally, M(, V ) is the covering number of the space V , conceptually this gives a measure of
generalizability of the neural network on potentially infinite and unknown datapoints when trained
on a finite subset (i.e. the training dataset). Thus, any bound on the covering number would imply
that we can predict the lower limit of the number of datapoints essential for accurate prediction on
unknown sets.
3
Under review as a conference paper at ICLR 2022
3.3	Vitushkin’ s Inequality
The Vitushkin’s inequality (Friedland & Yomdin, 2014; Yomdin, 2011) has been widely used in ge-
ometric functional analysis for studying the behaviour of level sets of analytic functions (Kovalenko,
2017). Along with its many applications this inequality can be used to bound the covering number
of a metric space V using the lesbesgue measure (Burk, 2011; Bartle, 2014).
Formally, let P (x, n, d) = P (x1 , x2 , . . . , xn) be a polynomial of degree d in n variables, Bn be a
ball of unit radius in n-dimensions and Vρ (P ) be the set of all polynomials that is bounded by ρ,
i.e.:
Vρ(P) ={x∈Bn : |P (x, n, d)| ≤ρ}
Then according to Vitushkin’s inequality:
(11)
n-1
M(,V) ≤ XCi(n,d)
i=0
≤ Md(C) + Mn(V)
(12)
where μn(V) is the n-dimensional LesbesgUe measure (Bartle, 2014) of the set and Md(C) are
variables defined as:
Ci(n,d) ,2i ni (d-i)i
n-1
Md(C) , X Ci (n, d)
i=0
(13)
3.4	Metric (n,d)-span
Metric (n,d)-span measures the accuracy of approximation of the covering number with Lesbesgue
measure, that is to say it determines how well the covering number can be approximated by know-
ing the Lesbesgue measure of that set. This transforms the intractable problem of computing the
covering number of the set V into a tractable one, this is extremely important as this quantifies the
accuracy of computational experiments to that of theoretical expressions. More formally, for any
subset Z ⊂ Bn, we define a constant ωd(Z) which is the metric (n,d)-span of set Z denoted by:
ωd(Z) =supCn[M(C,Z) - Md(C)]	(14)
≥0
3.5	Differentiable Rigidity Constant
The differentiable rigidity constant (Yomdin, 2011) measures the minimum variation of a function
which is differentiable, i.e. it is a lower on the variation of a function. Formally, let f : Bn → R,
f ∈ Cd+1 where d ∈ N and Ql(f) = maxx∈Bn P |f (l) (x)|. Here Ck is the family of all k times
continuously differentiable functions and P |f l (x)| is the sum of absolute values of all the partial
derivatives of f of order l. Then the dth differentiable rigidity constant is given by
RCd = inf Qd+1(f)
f∈Ud
where Ud is the set of all Cd+1 smooth functions f(z) on Bn vanishing on Z with Q0(f) = 1.
(15)
3.6	Remez d-span
The Remez d-span (Brudnyi & Ganzburg, 1973; Yomdin, 2011) measures the variation of the func-
tion when extended from a smaller set to a bigger set, this instrument allows for an extensive analysis
4
Under review as a conference paper at ICLR 2022
of neural networks when it transitions from the training to the testing dataset. Formally, for any poly-
nomial P , set Z ⊂ Bn and d ∈ N the Remez d-span is the minimal number K that is the ratio of
supremum of polynomial on the whole set to that of supremum of polynomial on the set Z
Rd(Z) = min K :
SUpBn IP । ≤ K
SUpZ IP| 一
(16)
3.7	B rudnyi- Ganzburg’ s inequality
The Brudnyi-Ganzburg’s inequality (Brudnyi & Ganzburg, 1973; Yomdin, 2011) tells us the varia-
tion in performance from a continuous train dataset to a test dataset. However, it is not applicable
for discreet datapoints as in the case when deep neural networks and we have use of Rd(Z) and
RCd to derive the corresponding bound for the discreet dataset. Let B ⊂ Rn be a convex body and
let let Ω ⊂ B be a measurable set. Then for any real polynomial P(x) = P(xi,... Xn) of degree d
we have
SUp IP I ≤ Td ( 1 + (1 - λ) 1 ) sup |PI	(17)
B	∖1 — (1 — λ) n J Ω
Here λ = /喂),with μn being the Lebesgue measure on Rn and Td is the Chebychev polynomial
of the first kind (Rivlin, 1974).
4	Theorems
Theorem (1) gives a bound of on the differentiable rigidity constant in terms of the Chebychev
polynomial and since this has a closed form solution it is computationally efficient to obtain Rd(Z).
Theorem 1.	If ωd(Z) > 0 and μn(Vι(P)) ≥ ωd (Z) is finite and it satisfies the inequality
Rd(Z) ≤ Td
1 + (1 - ωd(z))ι
1 — (1 - ωd(Z)) 1
(18)
Proof. Suppose IPI ≤ 1 on Z. Clearly Z ⊂ V1(P) as P is bounded in absolute value by 1 on
V1(P). Then by applying Brudnyi-Ganzburg’s inequality (17) with B = Rn and ω = V1(P) we get
the above expression.	□
Theorem 2.	For any real number 0 ≤ ω ≤ 1 the following inequality holds
Td
1 + (1 — ω)1
1 — (1 — ω) n
(19)
Proof. We use the simplified expression of Td(x) when x ≥ 1, then by simple algebraic manipula-
tions we get
Td [≡] =2 (2≡Y
(20)
By substituting y = (1 — ω) 1 in equation (20) We obtain
T 1 + (1 - ω)1
d 1 — (1 — ω) 1
Now, as n < 1 and as ω ≤ 1, by applying Bernoulli,s inequality on y we obtain
/r	、1	, r ω
(1 — ω) n ≤ 1------
n
1 ∕21 + (1 — ω)1
2 ∖ 1 — (1 — ω)1
(21)
(22)
5
Under review as a conference paper at ICLR 2022
Applying equation (22) in (21) leads to the final result
21 + (1 - ω)1 ≤ (4n	-	2∖
1 - (1 - ω)1 — (ω	)
<21 + (1 — ω)1 V ≤	但 _2' d
∖1 - (1 - ω)1) —	(ω	2)
(23)
Td
1 + (1 - ω) 1
1 — (1 — ω) n
□
Theorem (3) gives a relation between the differentiable rigidity constant and the Remez d-span, this
enables us to relate our generalization bound with Remez d-span.
Theorem 3.	For any subset Z ⊂ Bn and d ∈ N
RCd(Z) ≥
1
2Rd(Z)
(24)
Proof. Let f : Bn → R, f ∈ Ck, P(n, d) is the set of all real polynomials of degree d inn variables
and for d ∈ N we define Ed(f) as
Ed(f)
min max
P ∈P (n,d) x∈Bn
|f(x)-P(x)|
(25)
Let Pd(x) be the polynomial of degree d for any fixed d such that the minimum is achieved. Thus,
equation (25) can we rewritten as
Ed(f) = maxn |f (x) - Pd(x)|
By applying the triangle inequality on equation (26) for the subset Z ∈ Bn we obtain
max |Pd(x)| ≤ max |f (x)| + Ed(f)
x∈Z	x∈Z
By the definition of Remez d-span in equation (16) we get
max|Pd(x)| ≤ Rd(Z)[L+Ed(f)]
(26)
(27)
(28)
where L = maxx∈Z |f (x)|. Again, by applying triangle inequality on maxx∈Bn |f (x)| we get
max |f (x)| ≤ max |f (x) - Pd(x)| + max |Pd (x)|	(29)
x∈Bn	x∈Bn	x∈Bn
By combining equation (28) and equation (29) we get
maBxn |f (x)| ≤ Ed(f) +Rd(Z)[L+ Ed(f)]	(30)
As the inequality in equation (30) holds for any value of d we therefore get
inf [Rd(Z) [L + Ed(f)]] ≤ inf	[Rd(Z)[L+Ed(f)]+Ed(f)]	(31)
d	d∈{0,...,k-1}
Let Ed(f) = maXχ∈Bn f ;)!1 and as Ed(f) ≤ Ed(f) equation (31) further reduces to
6
Under review as a conference paper at ICLR 2022
xm∈aBxn |f(x)| ≤ d∈{0,i.n..f,k-1}[Rd(Z)[L + Edt(f)] + Edt(f)]	(32)
For a finite set Z ∪ {x} where x ∈ Bn we define the function fZ,x as
if x ∈ Z
if x ∈ Bn - Z
(33)
Let fz,χ be the analytic extension of fz,χ to Bn. Then by applying equation (32) We obtain
Q0(fz,χ) ≤7 inf	Rd(Z)[L + Ed (f)]+ Ed (f)]	(34)
d∈{0,...,k-1}
If L = 0 and Qo(fz,x) ≥ 1 then
,_	____ 、-	, Y 、
1 ≤ min	(Rd(Z) + 1)Qd+ι(,fz,χ)	(35)
d∈{0,...,k-1}
Thus, Qd+1 ≥ Rd(Z)+i and by the definition of the rigidity constant in equation (15) we get our
desired result RCd(Z) ≥ 2r；(Z).
□
Let ρ be the minimum distance between any two points in a set Z with cardinality k:
ρ = min ||x1 - x2||	(36)
x1,x2∈Z
And for neural network N with n input nodes and dl be the degree of approximation the activation
function for the lth then the following theorem holds true
Theorem 4.	The product of k and ρ is bounded by the metric (n, d)-span of the set Z for any n and
dl
ωdl (Z) ≥ kρ	(37)
The proof of theorem (4) has been relegated to the appendix. Using all the above theorems we have
derived till now the derive the final theorem describing the generalization bound.
By leveraging the bounds introduced in theorems (1), (2), (3) and (4) we now derive our generaliza-
tion bound for deep neural networks.
Theorem 5.	The loss l(N) incurred by the network N using the loss function l on the super-set Bn
given the loss in the subset Z is bounded by
EBn [l(N)] ≤EZ[l(N)] +O
(38)
Proof. Let us assume that the input x to the neural network is randomly sampled according to a
continuous probability distribution (i.e) Bayesian assumption. We first obtain the bound for the
randomly generated input and thereby extend it to the deterministic case by using the strong law of
large numbers.
Vρ(P) = {x ∈ Bn : |P (x)| ≤ ρ} if Z ⊂ Vρ(P) then M(, Z) ≤ M(, Vρ(P)) and further using
Vitushkin’s inequality (12) we get
M(g Z) ≤ M(g Vρ(P) ≤ Md(E) + μn(V)
(39)
So [M(e, Z) - Md(e)]en ≤ μn(V) for all E and by taking supremum of E on both sides we get
μn(V) ≥ ωd(Z). On combining equation (18) from theorem (1) and equation (19) from theorem
(2) we get
7
Under review as a conference paper at ICLR 2022
Rd(Z) ≤ 2
4 4n
Ud(Z)
(40)
It is easy to see from equation (40) it follows that 1 (ωd4(Z) - 2) ≤ 1 (ωd4(Z)) and by applying
equation (37) from theorem (4) we can further simplify the equation (40) as
Rd(Z) ≤ 2
(41)
For any x ∈ Bn we define a special function which captures the effect of sample points on the
generalization behaviour, mathematically we define it as
hl(Xl)= Pl(Xl) - XPl(Zj)Ψ (Z(Xl-Zj)) Ilfi ◦ W1∣∣F
j=1	ρ
(42)
where ψ is a C∞ function on Bn such that ψ(0) = 1 and ψ vanishes on the boundary on Bn , Xl is
the output lth layer embedding for the input X and {Z0 , . . . Zk } ∈ Z.
As each function ψj- = ψ ɑ (xl - Zj)) is supported on a ball of radius P centered at Zj We get
Qd+ι(ψj) = (p)dQd+ι(Ψ). From equation (42) it can be seen that hl is a linear combination of ψj-
and hence by definition Ql (f) = maxx∈Bn P |f(l)(X)| and substituting C(n, d) = 2d+1Qd+1(ψ)
We have
Qdι+ι(hl) ≤ mjX(Pl(Zj))CPdnd-Ilfl ◦ Wlll2dl	(43)
Sup(Qdl+ι(hl)) ≤ max(Pl(Zj))C^Ilfl ◦ Wl产	(44)
j	ρdl +1
From equation (44) it is evident that
C(⑺ <C(n,d) ∣∣fl ◦ Wlll2dl	(45)
RCd(Z) ≤ TdF	Rdl (Z)	(45)
From equation (24) in theorem (3) We knoW that
C(n,d) llfl ◦ Wlll2dl . u 4"8n∖-dl	g
L-^Zr ≥ (dl+ 1)!	(46)
Simplifying equation (46) We bound Rdl (Z) as
Rdi(Z) ≤ 信)dl Allfl ◦ WlH2dl
≤ C(n,d) (8nllfl ◦ Wlll2 )dl
_ Pdl+1 V kρ )
(47)
Since llfl ◦Wl ll2 ≤ llfl ll2 llWl ll2 and as llfl ll depends on the activation function Which is a constant
ll力◦ Wlll2 〜O(∣∣Wi∣∣2). The unknown constant C(n,d) which arises in equation (47) has an
effect on the final generalization bound and to capture its effect in all the layers We introduce a single
unknown parameter γ where 0 < γ ≤ 1. By the mathematical description of neural networks in
equation (1), and using equation (10) which validates the multiplicative convergence (Kolmogorov;
Bartle, 2014)
8
Under review as a conference paper at ICLR 2022
Rd(Z )=Y
(∏ 信《2r
(48)
As we have assumed that the data to comes from a data distribution p with finite mean and variance
we can now apply the strong law of large to obtain the final expression
EBn [l(N)] ≤EZ[l(N)]+O
(49)
□
5 Comparison with other bounds
Some of the most promising norm based methods like that of Neyshabur et al. use spectral norm
of the matrix to give tight generalization bounds. However, such bounds have an issue of scaling
abnormally high with the size of weight matrices (Vershynin, 2011) which showed that for an m × n
random matrix A with i.i.d entries the spectral norm is of order √m + √n, i.e. ∣∣A∣∣2 〜O(√m +
√n) which scales with the number of nodes. Figure (1) demonstrates this on neural networks with
varying depths trained on the MNIST dataset (LeCun & Cortes, 2010), the details of which has been
relegated to the appendix.
Figure 1: The comparison of our bound with spectrally-normalized margin bound (Neyshabur et al.,
2018) with neural networks of increasing depths trained on the MNIST dataset.
6 Conclusion & Future work
In this paper we introduced a novel generalization bound using geometric functional analysis. We
have compared our approach to that of the previously existing spectral norm based generalization
bounds and have shown the advantage of our method. We hope that this paper attracts more attention
towards the use of geometric functional analysis in theoretical analysis of deep neural networks.
Future work includes extending this idea to specialized architectures such that various different
properties of polynomials can be effectively leveraged to obtain architectural specific generalization
bounds. Also finding an optimal value of the total number of training samples k which minimizes
the generalization error would be an viable direction to pursue.
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pp.
254-263. PMLR, 2018.
9
Under review as a conference paper at ICLR 2022
Robert G Bartle. The elements of integration and Lebesgue measure. John Wiley & Sons, 2014.
Peter Bartlett, Dylan Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. Advances in Neural Information Processing Systems, 30:6241-6250, 2017.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of
the weights is more important than the size of the network. IEEE transactions on Information
Theory, 44(2):525-536, 1998.
Alberto Bietti, Gregoire Mialon, Dexiong Chen, and Julien Mairal. A kernel perspective for regu-
larizing deep neural networks. In International Conference on Machine Learning, pp. 664-674.
PMLR, 2019.
Yu A Brudnyi and Michael Iosifovich Ganzburg. On an extremal problem for polynomials in n
variables. Mathematics of the USSR-Izvestiya, 7(2):345, 1973.
Frank Burk. Lebesgue measure and integration: an introduction, volume 32. John Wiley & Sons,
2011.
Amit Daniely and Elad Granot. Generalization bounds for neural networks via approximate descrip-
tion length. Advances in Neural Information Processing Systems, 32:13008-13016, 2019.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Omer Friedland and Yosef Yomdin. Vitushkin-type theorems. In Geometric aspects of functional
analysis, pp. 147-157. Springer, 2014.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
Richard B Holmes. Geometric functional analysis and its applications, volume 24. Springer Science
& Business Media, 2012.
Andrel Kolmogorov. Introductory real analysis.
Oleg V Kovalenko. Ostrowski type inequalities for sets and functions of bounded variation. Journal
of inequalities and applications, 2017(1):1-16, 2017.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
James R Munkres. Topology, 2000.
Vaishnavh Nagarajan and Zico Kolter. Deterministic pac-bayesian generalization bounds for deep
networks via generalizing noise-resilience. In International Conference on Learning Representa-
tions, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401. PMLR, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring general-
ization in deep learning. Advances in Neural Information Processing Systems, 30:5947-5956,
2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
TJ Rivlin. The chebyshev polynomials, pure and applied mathematics, 1974.
Matus Telgarsky. Neural networks and rational functions. In International Conference on Machine
Learning, pp. 3387-3393. PMLR, 2017.
10
Under review as a conference paper at ICLR 2022
Roman Vershynin. Spectral norm of products of random and deterministic matrices. Probability
theory and related fields, 150(3):471-509, 2011.
Yosef Yomdin. Remez-type inequality for discrete sets. Israel Journal of Mathematics, 186(1):
45-60, 2011.
Randy K Young. Wavelet theory and its applications, volume 189. Springer Science & Business
Media, 2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization (2016). arXiv preprint arXiv:1611.03530, 2017.
11