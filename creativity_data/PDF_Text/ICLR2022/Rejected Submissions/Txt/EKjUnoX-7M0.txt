Under review as a conference paper at ICLR 2022
A new look at fairnes s in stochastic multi-
ARMED BANDIT PROBLEMS
Anonymous authors
Paper under double-blind review
Ab stract
We study an important variant of the stochastic multi-armed bandit (MAB) prob-
lem, which takes fairness into consideration. Instead of directly maximizing cu-
mulative expected reward, we need to balance between the total reward and fair-
ness level. In this paper, we present a new insight in MAB with fairness and
formulate the problem in the penalization framework, where rigorous penalized
regret can be well defined and more sophisticated regret analysis is possible. Un-
der such a framework, we propose a hard-threshold UCB-like algorithm, which
enjoys many merits including asymptotic fairness, nearly optimal regret, better
tradeoff between reward and fairness. Both gap-dependent and gap-independent
upper bounds have been established. Lower bounds are also given to illustrate the
tightness of our theoretical analysis. Numerous experimental results corroborate
the theory and show the superiority of our method over other existing methods.
1	Introduction
The multi-armed bandit (MAB) problem is a classical framework for sequential decision-making in
uncertain environments. Starting with the seminal work of Robbins (1952), over the years, a signifi-
cant body of work has been developed to address both theoretical aspects and practical applications
of this problem. In a traditional stochastic multi-armed bandit (MAB) problem (Lai & Robbins,
1985; Auer et al., 2002; Vermorel & Mohri, 2005; Bubeck & Cesa-Bianchi, 2012), a learner has ac-
cess to K arms and pulling arm k generates a stochastic reward for the principal from an unknown
distribution Fk with an unknown expected reward μk. If the mean rewards were known as prior in-
formation, the learner CoUldjUst repeatedly pull the best arm given by k = arg maxk μk. However,
the learner has no such knowledge of the reward of each arm. Hence, one should use some learning
algorithm π which operates in rounds, pulls arm πt ∈ {1, . . . , K} in round t, observes the stochastic
reward generated from reward distribution Fπt, and uses that information to learn the best arm over
time. The performance of learning algorithm π is evaluated based on its cumulative regret over time
horizon T , defined as
T
RR∏ (T) = μk* T — fEμ∏t∙
(1)
t=1
To achieve the minimum regret, a good learner should make a balance between exploration (pulling
different arms to get more information of reward distribution of each arm) and exploitation (pulling
the arm currently believed to have the highest reward).
In addition to the above classical MAB problem, many variations of the MAB framework have been
extensively studied in the literature recently. Various papers study MAB problems with additional
constraints which include bandits with knapsack constraints (Badanidiyuru et al., 2013), bandits
with budget constraints (Xia et al., 2015), sleeping bandits (Kleinberg et al., 2010; Chatterjee et al.,
2017), etc. Except these, there is a huge research interest in fairness within machine learning field.
Fairness has been a hot topic of many recent application tasks, including classification (Zafar et al.,
2017a;b; Agarwal et al., 2018; Roh et al., 2021), regression (Berk et al., 2017; Rezaei et al., 2019),
recommendation (Celis et al., 2017; Singh & Joachims, 2018; Beutel et al., 2019; Wang et al., 2021),
resource allocation (Baruah et al., 1996; Talebi & Proutiere, 2018; Li et al., 2020), Markov decision
process (Khan & Goodridge, 2019), etc. There are two popular definitions of fairness in the MAB
literature. 1). The fairness is introduced into the bandit learning framework by saying that it is unfair
1
Under review as a conference paper at ICLR 2022
to preferentially choose one arm over another if the chosen arm has lower expected reward than the
unchosen arm (Joseph et al., 2016). In other words, the learning algorithm cannot favor low-reward
arms. 2). The fairness is introduced such that the algorithm needs to ensure that uniformly (i.e.,
at the end of every round) each arm is pulled at least a pre-specified fraction of times (Patil et al.,
2020). In other words, it imposes an additional constraint to prevent the algorithm from playing
low-reward arms too few times.
In this paper, we adopt a new perspective, e.g., in addition to maximizing the cumulative expected
reward, it also allows the user to specify how “hard” or how “soft” the fairness requirement on each
arm is. In this view, it is not always easy even to formulate the problem and to introduce an appro-
priate notion of regret. We thus propose a new formulation of fairness MAB by introducing penalty
term Ak max(τkT -Nk(T), 0), where Ak, τk are the penalty rate and fairness fraction for arm k and
Nk (T ) is the number of times pulling arm k. Hence it gives penalization when the algorithm fails
to meet the fairness constraint and penalty term is proportional to the gap between pulling number
and its required level. To solve this regularized MAB problem, we also propose a hard-threshold
upper confidence bound (UCB) algorithm. It is similar to the classical UCB algorithm but adds an
additional term to encourage the learner to favor those arms whose pulling numbers are below the
required level at each round. The advantage of our approach is that it allows the user to distinguish ,
if desired, between arms for which is more important to sample an arm with required frequency and
those arms for which it is less important to do so.
To the best of our knowledge, there is no work on mathematical framework of fairness MAB with
regularization term in the literature. In this paper, we provide a relatively complete theory for the
fairness MAB. We rigorously formalize the penalized regret which can be used for evaluating the
performance of learning algorithm under fairness constraints. On theoretical side, the hard-threshold
UCB algorithm is proved to achieve asymptotic fairness when a large penalty rate is chosen. The al-
gorithm is shown to obtain O(log T) regret when the sub-optimality gap is assumed to be fixed. Ad-
ditionally, the characterization of fluctuation of non-fairness level, max1≤t≤T max(τkt - Nk(t), 0)
is also given. Its magnitude is also shown to be O(log T). Moreover, we establish a sub-optimal
gap-free regret bound of proposed method and provide insights on how hard-threshold based UCB
index works. We also point out that the analysis of proposed hard-threshold UCB algorithm is much
harder than the classical UCB due to the existence of interventions between different sub-optimal
arms. On numerical side, the experimental results confirm our theory and show that the perfor-
mance of the proposed algorithm is better than other popular methods. Our method achieves a better
trade-off between reward and fairness.
Notations. For real number x, (x)+ stands for max{0, x}; bxc is the largest integer smaller or equal
to x. For integer n, We use [n] to represent the set {1,..., n}. We say a = O(b); a = Ω(b) if there
exists a constant C such that a ≤ Cb a ≥ b/C. The symbols E and P(∙) denote generic expectation
and probability under a probability measure that may be determined from the context. We let π be a
generic policy / learning algorithm.
2	Achieving Fairnes s via Penalization
Consider a stochastic multi-armed bandit problem With K arms and unknoWn expected reWards
μι,...,μκ associated with these arms. The notion of fairness we introduce consists of proportions
Tk ≥ 0, k = 1,...,K with τι +----+ τκ < 1. We use T ∈ {1,2,..., } to denote the time horizon
and Nk,π(t) to denote the number of times that arm k has been pulled by time t ∈ [T] using policy
π. For notational simplicity, we may write Nk,π (t) as Nk(t). It is desired to pull arm k at least at
the uniform rate of τk , k = 1, . . . , K . In other words, the learner should obey the constraint that
Nk(t) ≥ τkt for any t ∈ [T]. Thus a good policy aims to solve the following optimization problem,
arg max E fμkNk,∏ (T), subject to Nk,∏(t) ≥ Tk t for all k and t.	(2)
k
Instead of directly working with such a constrained bandit problem, we consider a penalization
problem. That is, one gets penalized if the arm is not pulled sufficiently often. To reflect this, we
introduce the following design problem. Let Sπ(T) be the sum of the rewards obtained by time
t under policy π, i.e., Sπ(T) = PtT=1 rπt where πt is the arm index chosen by policy π at time
2
Under review as a conference paper at ICLR 2022
t ∈ [T] and rπt is the corresponding reward. Then the penalized total reward is defined as
K
Spen,π(T) = Sn(T) — X Ak(TkT - Nk,∏(T)) + ,	⑶
k=1
where A1 , . . . , AK are known nonnegative penalty rates. Our goal is to design a learning algorithm
to make the expectation of Spen,π (T) as large as possible. By taking the expectation, we have
KK
E[Spen,n (T)] = X μk E[Nk,π ⑴]- X Ak E[(τkT - Nk,π (T ))+],	(4)
k=1	k=1
which is the penalized reward achieved by policy π and we would like to maximize it over π. Now
we are ready to introduce the penalized regret function, which is the core for the regret analysis.
To derive the new regret, we first note that maximizing E[Spen,π (T)] is the same as minimizing the
following loss function,
K
L(T) = μ*T — E[Spen,∏(T)] = X [∆kE[Nk (t)]+ AkE[(τkT - Nk(T))+]],	(5)
k=1
where we denote
μ* = max μk, ∆k = μ* — μk, k = 1,..., K.
k=1,...,K
In order to find the minimum possible value of L(T), let us understand what a prophet (who knows
the expected rewards μι,..., μκ) would do. Clearly, a prophet (who, in addition, is not constrained
by integer value) would solve the following optimization problem,
min
x1,...,xK
K
X [∆k Xk + Ak E(TkT — Xk)+]
k=1
K
subject to Xk = T, Xk ≥ 0, k = 1, . . . , K,
k=1
and pull arm k for Xk times (k = 1, . . . , K). By denoting yk = Xk/T, k ∈ [K], we transform this
problem into
min
y1,...,yK
K
X [∆k Iyk + Ak(Tk — yk)+]
k=1
K
subject to yk = 1,yk ≥ 0, k = 1, . . . ,K.
k=1
(6)
We will solve the problem (6) by finding y1, . . . ,yK that satisfy the constraints and that minimize
simultaneously each term in the objective function. It is not hard to observe the following facts.
1.	For A	≥ 0, function y 7→ A(T —y	)+	achieves its minimum value of 0 fory ≥ T.
2.	For A	≥ ∆ > 0, function y 7→ ∆y	+	A(T —y )+ achieves its minimum of ∆T aty	= T .
3.	For ∆	> A ≥ 0, function y 7→ ∆y	+	A(T —y )+ achieves its minimum of AT aty	= 0.
As a result, we introduce the following three sets
AoPt = {k ∈ [K]: μk = μ*}, Acr= {k ∈ [K] : Ak ≥ ∆k > 0}, AnQn-Cr= {k ∈ [K] : ∆k > Ak},
where Aopt consists of all optimal arms, Acr consists of sub-optimal arms with penalty rate larger
than (or equal to) the sub-optimal gap and Anon-cr includes sub-optimal arms with penalty rate
smaller than the sub-optimal gap. Therefore an optimal solution to the problem (6) can be con-
structed as follows. Let k be an arbitrary arm in AQpt, and choose
(1 — ∑j∈AcrU(Aopt∖{k*}) Tj, k = k*,
Tk,	k ∈Acr ∪(Aopt∖{k*}),	(7)
0,	k ∈ Anon-cr .
Therefore, a prophet would choose (modulo rounding) in (5)
J (J 一 Σj∈AcrU(Aopt∖{k*}) Tj) T, k =忆
Nk(T )=	J	Tk T,	k ∈	Acr ∪(Aopt∖{k*}),
I	0,	k ∈	Anon-cr,
(8)
3
Under review as a conference paper at ICLR 2022
X min(∆k , Ak)Tk T.	(9)
leading to the following optimal value of L(T ),
L*(T)= X ∆kTkT +	X	AkTkT
k∈Acr	k∈An
on-cr
Given an arbitrary algorithm π, we can therefore define the penalized regret as
Rn(T ) = Ln(T) - L*(T )= X AkE(Tk T - Nk,∏ (T))+	(10)
k∈Aopt
+ X [∆kE(Nk,n(T) - TkT) + AkE(TkT - Nk,n(T))+]
+ X	[∆kENk,n(T)+ Ak(E(Tk T - Nk,∏ (T))+ - Tk T)].
k∈Anon-cr
3 A Hard-Threshold UCB Algorithm
We now introduce a UCB-like algorithm which aims to achieve the minimum penalized regret de-
scribed in the previous section. We assume that all rewards take values in the interval [0, 1]. We
denote by Xn(k) the reward obtained after pulling arm k for the nth time, k ∈ [K], n = 1, 2, . Let
1	Nn(k)
m k(n)= N (k) X X(k), k =1,...,k, n = 1, 2,...	(II)
and introduce the following index: for k = 1, . . . , K, n = 1, 2, . . . set
ik(n)
mk (n - 1) + Ak 1 (Nk (n - 1) < Tkn) +
2 log n
VNk (n -1).
(12)
The algorithm proceeds as follows. It starts by pulling each arm once. Then at each subsequent step,
we pull an arm with the highest value of the index ik(n). In equation 12, there is an additional term
Ak1(Nk(n - 1) < Tkn) compared with classical UCB algorithm. It takes the hard threshold form.
Once the number of times that arm k has been pulled before time n is less than the fairness level
(Tkn) at round n, penalty rate Ak will be added to the UCB index. In other words, the proposed algo-
rithm favors those arms which does not meet the fairness requirement. The detailed implementation
is given in Algorithm 1.
4 Theoretical Analysis of the Hard-threshold UCB algorithm
In this section, we present theoretical results for the hard-threshold UCB algorithm introduced in
Section 3. Throughout this section, we need to introduce additional notation and concepts. We say
Tk = Ω(1) if it is a positive constant which is independent of T. We assume that there exists a
positive constant c0 such that Pk Tk ≤ 1 - c0 and T is much larger than K. The penalty rates Ak ’s
are assumed to be known fixed constants. The expected reward μk (k ∈ [K]) is assumed between 0
and 1. Hence sub-optimality gap ∆k is between 0 and 1 as well.
Asymptotic Fairness. Given the large penalty rates, the proposed algorithm can guarantee the
asymptotic fairness for any arm k ∈ [K]. In other words, the algorithm can guarantee that the
number of times that arm k has been pulled up to time T is at least bTk Tc with high probability.
Theorem 1 If Ak - ∆k ≥ min{ J32T;TT, 1} and Tk = Ω(1) for all k, we have Nk(T) ≥ [TkTC
for any k with probability going to 1 as T → ∞.
Theorem 1 tells us that the proposed algorithm treats every arm fairly when the penalty rate domi-
nates the sub-optimality gap.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Hard-Threshold UCB Algorithm.
1:	Input. Number of arms K, fairness proportions τks, penalty rates Ak 's, time horizon T.
2:	Output. Cumulative reward, the number of times that each arm is played (Nk (T), k ∈
{1,...,K}.)
3:	Initialization.
For each k ∈ {1, . . . , K}, we set initial count Nk = 0 and arm-specific cumulative reward
Rk = 0.
4:	while n ≤ T do
5:	If n ≤ K, we choose kn = n.
6:	If n > K, we choose kn = arg maxk ik (n).
7:	We observe reward rn. We update count Nkn = Nkn +1 and update reward Rkn = Rkn +rn.
8:
We update hard-threshold index for each arm k ∈ {1, . . . , K} by calculating
ik (n + 1) = Rk /Nk + Ak 1(Nk < τk (n + 1)) +
/ 2 log n
V Nk
9:	Increase time index n by 1.
10:	end while
11:	Return vector (Nk ).
4.1 Regret Analysis: Upper Bounds
In this section, we provide upper bounds on the penalized regret defined in equation 10 under two
scenarios. (1) We establish the gap-dependent bound when the sub-optimality ∆k 's are fixed con-
stants. (2) We prove the gap-independent bound when ∆k's vary within the interval [0,1].
Theorem 2 (Gap-dependent Upper bound.) Assume that Ak -
holds for any arm k ∈ Aopt ∪ Acr and ∆k - Ak
have the following results.
∆k ≥ ca (ca is a positive constant)
holds for any k ∈ Anon-cr. We then
> ∕8K log T
≥ V ^^C0^
For any k ∈ Aopt ∪ Acr, it holds
E[(τkT-Nk(T))+] = O(1).
For any k ∈ Acr, it holds
8 log T
E[Nk (T)] ≤ max{ -T^2- ,τk T} + O(I).
∆k
For any arm kj ∈ Anon-cr, it holds
E[Nk(T)] ≤ max{min{ (EogT ,τkT}, * 8⅛^} + O(1).
(∆k - Ak)2	∆k
Therefore, we have
Rn(T) ≤ X (8^gT - TkT)+ + X max{min{ ʌlogT , (∆k - Ak)TkT}, 8⅛T} + O(K). (13)
k∈Acr	∆k	k∈Anon-cr	∆k - Ak	∆k
Theorem 2 tells us that the number of times that each arm k in critical set Acr is played is at least
around fairness requirement τkT when the the penalty rate is larger than the sub-optimality gap by
some constant. On the other hand, for each arm k in non-critical set Anon-cr, it could be played less
than fairness requirement when sub-optimality gap substantially dominates the penalty rate. The
total penalized regret has order of log T and is hence nearly not improvable. In addition, when
Ak ≡ 0 and it degenerates to the classical settings, then all arms become non-critical arms and our
bound reduces to O(Pk 8∆gT) which matches the existing result (Auer et al., 2002).
Maximal Inequality. In Theorem 2 above we have shown that E[(τkT - Nk(T))+] = O(1) for any
k ∈ Aopt ∪ Acr under mild conditions on ∆k's. In the result below, we derive a maximal inequality
for the non-fairness level, (τkt - Nk (t))+, t ∈ [T].
5
Under review as a conference paper at ICLR 2022
Theorem 3 Order the K arms in such a way that
AkI + μkι ≥ …≥ Akj + μkj ≥ …≥ AkK + Mkκ
Then for any arm kj ∈ Aopt ∪ Acr, we have
E[1m≤ta≤xT (τkj t - Nkj (t))+] ≤ ajlogT + O(1),
where the coefficient aj is defined as
j
aj =8	(j-d+1)
1
(μkd + Akd - μkm )2
K
+X
m=d+1
1
(μkd + Akd - μkm - Akm )2
(14)
Theorem 3	nearly guarantees the ANY-ROUND fairness for all arms k ∈ [K] up to a O(log T)
difference.
Gap-independent Upper bound. We now switch to establishing a gap-independent upper bound. It
relies on the following observations. The key challenge is how to bound the term E[(τkT -Nk(T))+]
for k ∈ Aopt ∪ Acr.
(Observation	1)	If	Ak	—	∆k	≤	4 J2TogT, then	(Ak	—	∆k)E[(τkT —	Nk(T))+]	≤
4τkT 2/3 (2log T )1/2.
Lemma 1 (Observation 2) Ifarm k satisfies that Ak — ∆k ≥ 4y 2TogT and Tk = Ω⑴,then we
have E[(τ" — Nk(T))+]=O(TkKT2/3)∙
Based on above observations, we have the following regret bound.
Theorem 4	When Tk = Ω(1) ,it holds that
Rn (T) ≤ 8,T log T (X √Tk )+8P(1 — Tmin)KT log T + AmaxKT 2∕3(2log T )1/2,	(15)
k
where Tmin = mink Tk and Amax = maxk Ak.
The first term in (15) is for Ak(E(TkT—Nk(T))+) with k ∈ Anon-cr. The second term gives a bound
for ∆kE(Nk(T) — TkT) for k ∈ [K]. The third term in (15) is for bounding AkE(TkT — Nk (T))+
for k ∈ Aopt ∪ Acr.
4.2 Regret Analysis: Lower Bounds
Gap-dependent Lower Bound. In this part, we first show that the bound given in inequality (13)
is tight. To see this, the results are stated in the following theorems.
Theorem 5	There exists a bandit setting for which the regret of proposed algorithm has the follow-
ing lower bound,
Rπ(T) ≥
Σ
k∈Anon-cr ,τk >0
log T
∆k - Ak
(16)
Theorem 6	There exists a bandit setting for which the regret of proposed algorithm has the follow-
ing lower bound,
Rn(T) ≥ X ∆k(T — TkT).
(17)
Theorem 5 says that the term log T /(∆k — Ak) is nearly optimal up a multiplicative constant 8 for
any arm in the non-critical set. Similarly, Theorem 6 tells Us that (8∆gT 一 TkT)+ is also nearly
6
Under review as a conference paper at ICLR 2022
optimal for arms in the critical set. Therefore, Theorem 2 gives a relatively sharp gap-dependent
upper bound. It is almost impossible to improve the regret bound analysis for our proposed hard-
threshold UCB algorithm in the instance-dependent scenario.
Gap-independent Lower Bound. We also obtain a gap-independent lower bound as follows.
Theorem 7 Let K > 1 and T be a large integer. Penalty rates A1, A2, . . . , AK are fixed positive
constants. Assume that the fairness parameters τ1, . . . , τK ∈ [0, 1] with k τk < 1. Then, for any
policy π, there exists a mean vector μ = (μι,..., μκ) such that
Rn(T) ≥ C(1 — 2maxTk)p(K — 1)T,
πk
where C is a universal constant which does not depend on Ak, τk ’s.
By comparing Theorems 4 and 7, we can see that there is a substantial gap. This is because term
AkE[(τkT — Nk (T))+] is very hard to handle. This term can be trivially lower bounded below by
zero for any algorithm. However, this term is proved to be O(T2/3) (ignoring logT factor) by our
current techniques under the proposed algorithm. Whether we can improve the gap-independent
upper bound to be O(T 1/2) is an open question in the future work.
5	Comments on the Hard-Threshold UCB Algorithm
On hard threshold. In the proposed algorithm, we use a hard-threshold term Ak1(Nk (n — 1) <
τkn) in constructing a UCB-like index ik(n). A natural question is whether we can use a soft-
threshold index by defining
ik(n) = mk(n - 1)+ Ak max(Tkn - N(n - 1)，0)+ sɪɪlognɪ?
τkn	Nk(n — 1)
The answer is negative in the sense that ik (n) becomes a continuous function of Nk and does not
have a jump point at the critical value Tkn. Hence it does not give sufficient penalization to those
arms k which are below the fairness proportion Tk. Hence, a soft-threshold UCB-like index fails to
guarantee the asymptotic fairness and nearly-optimal penalized regret.
When Tk is not constant. In our theoretical analysis, We only consider the case that Tk = Ω(1) for
ease of presentation. The current results could also apply when threshold Tk is dependent on time
horizonTWithTk(T) = 1/Tb(0 < b < 1).
Technical Challenges. Since the index ik(n) is a discontinuous function of Nk, this brings addi-
tional difficulties in analyzing the regret bound. The most distinguished feature from the classical
regret analysis is that We cannot analyze term Nk(T) separately for each sub-optimal gap k. In fact,
the optimal arm (argmaxk μk) is fixed for all rounds in the classical setting. In contrast, the “opti-
mal arm” (argmaxk μk + Ak 1{N < Tkn}) varies as the algorithm progresses in our framework.
Due to such interventions among different arms, term (TkT — Nk (T))+ should be treated carefully.
Connections to LASSO problems. We would like to point out that our current framework shares
similarities with LASSO problem (Tibshirani, 1996; Zhao & Yu, 2006; Zou, 2006) in linear regres-
sion models. Both of them introduces the penalization terms to enforce the solution to obey fairness
constraints / sparsity to some degree. In our penalized MAB framework, whether an arm k is played
at least TkT times or not depends on the penalty rate Ak and the sub-optimality gap ∆k. Similarly,
in the LASSO framework, whether a coefficient is to be estimated as zero depends on the penalty
parameter and its true coefficient value.
Comparison with Baselines. We compare the proposed methods with related existing methods.
Learning with Fairness Guarantee (LFG, Li et al. (2019)). It is implemented via following steps.
•	For each round n, we compute the index for each arm, ik(n) = min{mk(n 一 1) +
N N2(n-ni), 1} and compute queue length for each arm, Qk(n) = max{Qk(n - 1) +
Tk — 1{arm k is pulled}, 0}.
7
Under review as a conference paper at ICLR 2022
•	The learner plays the arm which maximizes Qk (n) + η0wkik(n) and receive the corre-
sponding reward, where η0 is the tuning parameter and wk is the known weight. Without
loss of generality, we assume wk ≡ 1 by treating each arm equally when we have no
additional information.
Fair-Learn (Flearn, Patil et al. (2020)). Its main procedure is given as below.
•	For each round n, we compute set A(n), A(n) := {k : τk(n - 1) - Nk(n - 1) > α},
which contains those arms which are not fair at round n at level.
If A(n) = 0, We play arm which maximizes τk(n - 1) - Nk(n - 1). Otherwise, We play
arm which maximizes mk(n — 1) +

2 log n
Nk (n-I) .
Fair-learn method can enforce each arm k should be played at proportion level τk only when α = 0.
LFG method fails to guarantee the asymptotic fairness when η0 > 0. Neither of these methods can
well balance between total rewards and fairness constraint as our method does.
6	Experiment Results
A In this experimental setting, we examine the relationship between number of times that non-
critical arm k has been pulled at T (=20000) rounds and the inverse gap 1∕(∆k - Ak)2. In
particular, we construct the following three parameter settings (τk ≡ 1/20).
Case 1:	K = 9; μ = (0.9,0.8, 0.7,0.6, 0.6,0.4, 0.3,0.2, 0.1); Ak ≡ 0.45.
Case 2:	K = 9; μ =(0.95,0.8, 0.7,0.6, 0.6,0.4,0.3,0.2,0.1); Ak ≡ 0.41.
Case 3: K = 9; μ =(0.9,0.8, 0.7,0.6, 0.6,0.425,0.4, 0.375, 0.35); A ≡ 0.45.
B Similarly, we examine the relationship between number of times that critical arm k has been
pulled at T (=20000) rounds and the inverse gap 1∕∆2k. We set τk ≡ 1∕20, Ak ≡ 0.45.
Case 1: K = 9; μ = (0.9,0.86, 0.84, 0.82,0.6, 0.4,0.3, 0.2,0.1).
Case 2:	K = 9; μ =(0.95,0.85,0.84, 0.83, 0.82,0.4, 0.3,0.2, 0.1).
Case 3:	K = 9; μ =(0.9,0.8, 0.7,0.6, 0.5,0.4, 0.3,0.2, 0.1).
C We investigate the relationship between cumulative penalized regret and total time hori-
zon (T) under three algorithms (proposed method, LFG, and Flearn). The parameters
are constructed as follows. The number of arms (K) is set to be 5 or 20. The total
time horizon (T) varies from 500 to 16000. The fairness proportion τk of each arm is
set to be τk = τ∕K with τ ∈ {0.2, 0.4, 0.8}. The penalty rate Ak is constructed as
Ak ≡ (maxk μk - mink μk)/2. Each entry of the mean reward vector (μk) is randomly
generated between [0, 1]. The reward distribution of each arm is a Gaussian distribution,
e.g., N(μk, K). For FIearn algorithm, we take tuning parameter α = 0. For LFG algo-
rithm, we take no = √T. Each case is replicated for 50 times.
From Figure 1, we can see that the pulling number Nk(T) is proportional to 1∕(∆k - Ak)2 for
k ∈ Anon-cr when Nk(T) does not reach fairness level τkT. We also see that Nk(T) is proportional
to 1∕∆k for k ∈ Acr when the pulling number is larger than fairness level Tk T. These phenomena
match the results in Theorem 2. From Figure 2, we observe that the proposed method achieves
smaller penalized regret compared with LFG and Flearn. This confirms that our method is indeed a
good learning algorithm under penalization framework.
In the appendix, we also study the paths of unfairness level ((τkT - Nk(T))+) when tuning parame-
ter varies and investigate the relationship between total expected reward (PT=I μ∏) and unfairness
level (Pk∈[K ] (τkT - Nk(T))+) for three algorithms. From Figure 3 (See Appendix A), the paths
of unfairness level show different behaviors under three algorithms. For our method, with scale
parameter decreasing, each arm becomes unfair one by one. By contrast, all arms under both Flearn
and LFG methods suddenly become unfair once scale parameter decreases from 1. This suggests
that our method has sparsity feature as LASSO does, e.g., making arms with small sub-optimality
gap fair. From Figure 4 (See Appendix A), we can tell that the proposed method always achieves
the highest reward given the same unfairness level under different parameter settings. This gives
evidence that hard-threshold UCB algorithm makes better balance between total reward and fairness
constraints compared with other competing methods.
8
Under review as a conference paper at ICLR 2022
Figure 1: Upper row: Nk (T) Vs 1∕(∆k - Ak)2 for arm k ∈ Anon-cr. Bottom row： Nk(T) Vs ∖/∆
for arm k ∈ Acr. In all plots, the blue horizontal line stands for fairness level τkT.
LFG
Flearn
Proposed
LFG
Flearn
Proposed
20 arms, τ =
2□ arms, τ= 0.4
20 arms, τ — 0.8
4000 8000 12000 16000
T
4000 8000 12000 16000
T
4000 8000 12000 16000
T
Flearn
Proposed
Flearn
Proposed
Flearn
Proposed
Figure 2: Penalized Regret (Rπ (T)) Vs Different Time Horizon (T) under different settings.
7	Conclusion
In this paper, we proVide a new framework of fairness MAB problem by introducing regularization
terms. The adVantage of our new approach is that it allows the user to distinguish between arms for
which is more important to sample an arm with required frequency leVel and arms for which it is
less important to do so. A hard-threshold UCB algorithm is proposed and is shown to haVe good
performance under this framework. Unlike other existing algorithms, the proposed algorithm not
only achieVes the asymptotic fairness but also handles well in balance between reward and fairness
constraints. A relatiVely complete theory, including both gap-dependent / independent bounds, has
been established. The new theoretical results contribute to the fairness in machine learning field and
bring better insights in how to play smartly in the exploitation and exploration games.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A re-
ductions approach to fair classification. In International Conference on Machine Learning, pp.
60-69. PMLR, 2018.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2):235-256, 2002.
Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.
In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pp. 207-216. IEEE,
2013.
Sanjoy K Baruah, Neil K Cohen, C Greg Plaxton, and Donald A Varvel. Proportionate progress: A
notion of fairness in resource allocation. Algorithmica, 15(6):600-625, 1996.
Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgen-
stern, Seth Neel, and Aaron Roth. A convex framework for fair regression. arXiv preprint
arXiv:1706.02409, 2017.
Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan
Hong, Ed H Chi, et al. Fairness in recommendation ranking through pairwise comparisons. In
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 2212-2220, 2019.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.
L Elisa Celis, Damian Straszak, and Nisheeth K Vishnoi. Ranking with fairness constraints. arXiv
preprint arXiv:1704.06840, 2017.
Aritra Chatterjee, Ganesh Ghalme, Shweta Jain, Rohit Vaish, andY Narahari. Analysis of thompson
sampling for stochastic sleeping bandits. In UAI, 2017.
Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
Classic and contextual bandits. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.,
2016.
Koffka Khan and Wayne Goodridge. S-mdp: Streaming with markov decision processes. IEEE
Transactions on Multimedia, 21(8):2012-2025, 2019.
Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping
experts and bandits. Machine learning, 80(2):245-272, 2010.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances
in applied mathematics, 6(1):4-22, 1985.
Fengjiao Li, Jia Liu, and Bo Ji. Combinatorial sleeping bandits with fairness constraints. IEEE
Transactions on Network Science and Engineering, 7(3):1799-1813, 2019.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. In 8th International Conference on Learning Representations, 2020.
Vishakha Patil, Ganesh Ghalme, Vineet Nair, and Y Narahari. Achieving fairness in the stochastic
multi-armed bandit problem. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 5379-5386, 2020.
Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian D Ziebart. Fair logistic regression: An
adversarial perspective. 2019.
Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American
Mathematical Society, 58(5):527-535, 1952.
10
Under review as a conference paper at ICLR 2022
Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fairbatch: Batch selection for
model fairness. In 9th International Conference on Learning Representations, 2021.
Ashudeep Singh and Thorsten Joachims. Fairness of exposure in rankings. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
2219-2228, 2018.
Mohammad Sadegh Talebi and Alexandre Proutiere. Learning proportionally fair allocations with
low regret. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 2(2):
1-31, 2018.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267-288, 1996.
Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical evaluation. In
European conference on machine learning, pp. 437-448. Springer, 2005.
Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims. Fairness of exposure in stochastic
bandits. In Proceedings of the 38th International Conference on Machine Learning, volume 139,
pp. 10686-10696. PMLR, 2021.
Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Thompson sampling for budgeted
multi-armed bandits. In Twenty-Fourth International Joint Conference on Artificial Intelligence,
2015.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171-
1180, 2017a.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp.
962-970. PMLR, 2017b.
Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine Learning
Research, 7:2541-2563, 2006.
Hui Zou. The adaptive lasso and its oracle properties. Journal of the American statistical associa-
tion, 101(476):1418-1429, 2006.
11
Under review as a conference paper at ICLR 2022
Appendices
In this appendix, the first section is dedicated for experimental results of Experiments D and E. In
the rest, we collect all technical proofs. Specifically, the proofs of gap-dependent upper and lower
bounds are given in Section B and C. The proofs of gap independent upper and lower bounds are
given in Section E and F, respectively. The proof of E[max1≤t≤T (τkt-Nk(t))]+ is given in Section
D.
A	The Plots for Experiments D and E
D We investigate the path of unfairness level ((τkT - Nk(T))+) of each arm when the tuning
parameter varies. The parameters of two settings are constructed as follows.
Setting 1: K = 8,T = 10000; (μι,...,μ8) = (0.9, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1); τι =
...= τ8 = 2K. The reward distribution of each arm is a Gaussian distribution, e.g.,
N (μk , K12 ).
Setting 2: K = 8,T = 10000; (μι,...,μ8) = (0.95,0.7, 0.65, 0.6,0.2, 0.15,0.1, 0.05);
τι = ... = τ4 = 0.8Kk and τ5 = ... = 丁8 = 0.4τι. Again, the reward distribution of each
arm is a Gaussian distribution, e.g., N(μk, K2).
The penalty rates Ak ≡ η, where we call η is the scale parameter which takes value between
0 and 1. For Flearn algorithm, the tuning parameter α = (1 - η)τ1T with η varying from
0 to 1. For LFG algorithm, the tuning parameter η0 = (1 - η)T with η ∈ (0, 1].
When scale parameter η → 1, three algorithms will prefer to exploit the arm with highest
reward and pay less attention to the fairness. On the other hand, η → 0, three algorithms
tend to treat the fairness as the priority. (Due to space limit, the results are in Appendix A)
E We investigate the relationship between total expected reward (PT=I μ∏) and unfairness
level (Pk∈[K] (τkT- Nk(T))+) for three algorithms. The parameters are given as follows.
We set K ∈ {5,20} and Tk ≡ τ∕K with T ∈ {0, 20.5}. Each element in mean reward
vector (μk) is generated between 0 and 1. Moreover, we generate the reward from three
different distributions, (1) Gaussian N(μk, 6),(2) Beta Beta(μk, 1 - μk), (3) Bernoulli
Bern(1,μk).
Oooo
5 0 5
4 3 1
SS ① U.I-EJUn
-∙- arm O
-arm 1
—∙- arm O
→- arm 1
-→- arm 2
→- arm 3
—arm 4
—arm 5
→- arm 6
arm 7
0.0	0.2	0.4	0.6	0.8	1.0
Scale
→- arm 2
—∙- arm 3
-→- arm 4
*⅝ J -→- arm 5
'-→- arm 6
arm 7
0.0	0.2	0.4	0.6	0.8	1.0
Scale
0.0	0.2	0.4	0.6	0.8	1.0
Scale
Ooooo
0 0^0
8 6 4 2
ssφu 上4Bun
0.0	0.2	0.4	0.6	0.8	1.0
Scale
arm O
arm 1
arm 2
arm 3
arm 4
arm 5
arm 6
Figure 3: Unfairness path ((τkT - Nk (T))+, k ∈ [K]) for three algorithms under two settings
described in Experiment D. (Upper row is for Setting 1 and bottom row is for Setting 2.) For
sub-optimal arms, the proposed method can guarantee the fairness with a wider range of tuning
parameter. By contrast, Flearn and LFG can break the fairness easily.
12
Under review as a conference paper at ICLR 2022
p-JEΛΛφa
p.JrσM(υa
p-ipmoo:
8700
8100
5900
5700
6100
4500
4400
8500
7900
9300
8900
8400
7900
7400
9100
8900
8700
8500
7200
6950
6700
6450
6200
4300
4600
8300
8100
200	400	600	800
Unfairness
proposed
Flearn
LFG
Binary, 20 arms, τ = O.5jχri0∙*
proposed
Flearn
9750
9700
9650
9600
9550
0	150	300	450	600
Unfairness
8600
7900
7200
6500
0	800 1600 2400 3200	0	800 1600 2400 3200
Unfairness	Unfairness
9250
9050
8850
8650
9400
8900
8400
7900
7400
p」EM①α
①a
p-leMBa:
p」EM①a
proposed
Flearn
LFG
0
0	200	400	600
Unfairness
P-JPMIa
Figure 4: Total regret vs unfairness level for three algorithms under different settings described in
Experiment E. (The first row is for 5 arms with required fraction of times τ = 0.2; The second row
is for 5 arms with required fraction of times τ = 0.5; The third row is for 20 arms with required
fraction of times τ = 0.2; The fourth row is for 20 arms with required fraction of times τ = 0.5.
The first column is for Gaussian reward distribution; the second column is for Beta distribution; and
the third column is for Bernoulli distribution.) Given the fixed unfairness level, the proposed method
can have larger total reward than other two methods consistently over all experimental settings.
13
Under review as a conference paper at ICLR 2022
Highlight of the proof. The technical challenge lies in handling the term (τkT - Nk(T))+. (1)
Our main task in proving upper bound is to show that, for any k ∈ Acr, (τkT - Nk(T))+ is O(1)
for in gap-dependent setting and it is O(T2/3) for gap-independent setting. Unlike the classical
UCB algorithm analysis, we cannot bound Nk (T) separately for each arm k. Instead, we need to
study the relationship between any pair of critical arms and the relationship between critical arm
and non-critical arm. A key step is to find a stopping time n1 such that any arm k ∈ Acr satisfies
Nk(n1) ≥ τkn1. Therefore, between rounds n1 and T, the behavior of (τkT - Nk(T))+ can
be well controlled. (2) In proving maximal inequality, we need to order K arms according to the
values of μk + Ak. Then the bound of maxι≤t≤τ(Tkt - Nk(T))+ can be obtained by a recursive
formula (see equation 30) starting from k = kι to k = kκ, where kι := arg max{μk + Ak} and
kκ ：= argmin{μk + Ak}.
B Proof of Gap-dependent Upper Bounds
Proof of Theorem 1 Its proof is essentially same as the proof of first part in Theorem 2 by treating
the non-critical set Anon-cr empty.
Proof of Theorem 2We first prove the first part: E[(TkT -Nk(T))+] = O(1) for any k ∈ Aopt∪Acr.
Suppose at time n that a critical arm k is played less than Tkn. We can prove that the algorithm
pulls critical arm k0 at time n such that Nk0 (n) ≥ 8 log T/c2a and Nk0 (n) > Tkn with vanishing
probability. This is because
2 log n	2 log n
P(Ak+mk (n)+n N^) ≤ mk0 (n)+n N-(n)
≤
≤
P(Ak + μk ≤ +μk + 2
2log n)+ 2
Nko (n)) + n2
P(Ak - ∆k ≤ -∆k0 + 2
2log n)	2
Nko (n)) + n2
2/n2.
(18)
By the same reason, the algorithm pulls non-critical arm k00 at time n when Nk00 (n) ≥ 8 log T /c2a
with vanishing probability.
(Observation 3) In other words, it holds with high probability that once a critical arm k is played
with proportion less than required level Tk’s, it must be pulled in next round when all other arms is
played with proportion greater than level Tk’s and is played more than 8 log T /c2a times.
(Observation 4) It also holds with high probability that once a non-critical arm is played more than
8 log T/c2a, it can be only played when all critical arms are played with frequency more than the
required level Tk ’s.
Moreover, we can show that Nk0 (n) ≥ 8 log T/c2a at time n = c0T/2 for each critical arm k0. If
not, note that 8 log T /c2a ≤ Tk0c0T/4, then Nk0 (n) < Tk0n for any n ∈ {dc0T/4e, . . . , bc0 T /2c}.
Hence, for any critical arm k0 can be played at most max{Tk0 c0T /4, 8 log T/c2a} times between
rounds c0T/2 and c0T; every non-critical arm k00 can be played at most 8 log T /c2a times. Then, we
must have
c0T/2 - c0T/4 ≤	Tkc0T/4 +	8 log T/c2a.
k
k
However, the above inequality fails to hold when T/ log T ≥ 16c02K/c2a. This leads to the contra-
diction. Thus, we have Nk0 (n) ≥ 8 log T /c2a for any critical arm k0 at time n = c0T/2.
Actually, this further gives us that we must have Nk0 (n) ≥ bTk0 nc for all very critical arms at some
time n ∈ [coT/2, T]. To see this, We observe the fact that for any arm k, it will be played with prob-
ability less than 备 at time n once Nk (n) ≥ max{τkn, 8log T/ca } and one critical arm k0 is played
less than τk√n. (In other words, this tells us that once arm k has been played max{τkn, 8 log T/c1}
times, then it can only be played at time when all very critical arms k0s have been played for Tk0n
times or bτkknc jumps by one with probability greater than 1 - 2/T2.)
14
Under review as a conference paper at ICLR 2022
Let n1(≥ c0T/2) be the first ever time such that Nk0 (n1) ≥ bτk0n1c for all critical arms k0s. By
straightforward calculation, it gives that n1 must be bounded by
n1 ≤ c0T/2+	8logT/c2a+ (	τk)T
k:non-critical	k:critical
with probability greater than 1 - 2K/T. That is, n1 is well defined between c0T and T. At time n1,
we have all critical arms k0 such that Nk0 (n1) ≥ τk0n1.
Moreover, we consider the first time n2(> n1) such that every non-critical arm k00 has been played
for at least 8 log T /c2a times when ∆k00
≤ JCa log6CθTT2) (If Ca > 4, it automatically holds for
any ∆k00). We claim that n2 ≤ n1 + c0T/2. This is because, between rounds n1 and n2, the
algorithm will choose non-critical arm k00 when Nk0 (n) ≥ τk0 (n) for all critical arms k0s and
Nkoo (n) ≤ log(coT∕2)∕2∆k”. To see this, we know that
≤
≤
≤
P(mk0 (n)+/NH ≥ mk00⑺+jN°g⅞)
P(μk+2S∣II ≥ …S Ngn)
p(〃k+2 J ≥ …「)
2/C0T.
(19)
That is, index of arm k00 is larger than k0 with high probability.
In other words, for each round between n1 and n2, each critical arm k0 can be only
pulled at most τk0 (n2 - n1) before every non-critical arm k00 has been played for
min{8logT/Ca，log(coT∕2)∕2∆koo}. Additionally, each non-critical arm k00 can be only played
for at most 8 log T /(∆k - Ak)2 with high-probability. Therefore, it must hold that
n2 - nι ≤ (ETk)(n2 - nι) + ∑28logT∕(∆k - Ak)2.
k
k
However, the above inequality fails to hold when n2-n1 ≥ C0T/2 under assumption that ∆k -Ak ≥
q8KClog T-. This validates the claim n ≤ nι + c0T∕2.
Starting from time n2, by the observations 3 and 4, it can be seen that the maximum values of
(τk0 n - Nk0 (n))+ for any critical arm k0 is always bounded by 1 with probability 1 - 2K/T (n ∈
[n2, T]). This completes the proof of the first part.
For the second part, We need to prove E[Nk(T)] ≤ max{ 8∆gT ,TkT} + O(1) for k ∈ Acr.
When 8∆g T > TkT, we can calculate the probability
k
8	log T
P(arm k is pulled at round n +1 ∣Nk(n) ≥ ——2—)
∆k
≤ P(ik(n + 1) ≥ ik*(n + 1))
.IP)- / _Llj ∕2log(n + 1) r八(I n I ∕2log(n +1) ʌ
≤ P(mk(n +1) + y Nk(n)	≥ mIAn +1) + y Nk(n) )
≤ 1/n2 ≤ 1/(8 log T /∆k)2 ≤ 1/(TkT)2.
(20)
15
Under review as a conference paper at ICLR 2022
When 8∆g T ≤ TkT, We can similarly calculate the probability
P(arm k is pulled at round n + 1 |Nk(n) ≥ τkT)
≤ P(ik(n + 1) ≥ ik*(n + 1))
V PL /	, n ,	∕2log(n +1)	ʌ Z , n ,	∕2log(n +1)、
≤ P(mk(n +1) + y	Nk(n)	≥ mIAn +1) + y	Nk(n)	)
≤ 1/n2 ≤ 1/(TkT)2.	(21)
Hence we can easily obtain that E[N(T)] ≤ max{ 8∆gT, TkT} + O(1) by union bound.
For the third part that E[N (T)] ≤ min{(鼠—AT)2 ,TkT} + O(1) (kj ∈ AnOn-Cr),itfollows from the
fact that we can treat μk + Ak as new expected reward for arm k ∈ Anon-Cr. ThUs the corresponding
sub-optimality gap is ∆k - Ak. The result follow by using standard technique in the classical UCB
algorithm. Hence we omit the details here.
Finally, by combining three parts and straightforward calculation, we obtain the desired gap-
dependent upper bounds. This concludes the proof.
C Proof of Gap-dependent Lower Bounds
Proof of Theorem 5. We consider the following setting, where arm 1 is the optimal arm with a
deterministic reward ∆ and arms k, (k ≥ 2) are sub-optimal arms with reward zero. Let penalty
rate Ak = A for all k ∈ [K] with ∆ > A. Assuming that(：—% ≤ TkT/2, we construct a lower
bound as follows.
We claim that each arm k ≥ 2 will be played at least nι ：= (∆⅛ times. If there exists an arm k0
has not been played for n1 times, we then consider the time index na = T/2 + 1 +(K - 2) (∆-AT2 +
n1 . At this time, we have that arm 1 is the arm with largest index since that for each sub-optimal
arm k = ko, its index will never exceeds ∆ once it has been played(：—% times. According to
assumption that arm k0 has been played less than n1 times, thus arm 1 is the arm with largest index
at time na .
However, the index of arm 1 at time na is never larger than
J2TgT + ∆. The index of arm ko at
time na is always larger than A +
√2≡21 .Itgives
ii(na) ≤ s2HT + ∆ <A +∕≡≡P ≤ iko (na),	(22)
which leads to the contradiction of the mechanism of the proposed algorithm. Hence, we have that
each sub-optimal should have been played for at least (∆-gA)2 times.
Proof of Theorem 6. We consider the another setting, where where arm 1 is the optimal arm with
deterministic reward ∆1 + ∆2, arm k’s (k ∈ Acr) are sub-optimal arms with reward being ∆1 and
arm k’s (k ∈ Anon-cr) ar sub-optimal arms with reward being ∆2. Let penalty rate Ak = A2 for
all k ∈ Acr with ∆2 < A2 and penalty rate Ak = A1 for all k ∈ Anon-cr with ∆1 > A1. Assume
that Pk∈Anon-cr (78-^ + PkCAcr 8^ < T/2 and TkT ≤	f^ k ∈ AC「，We then have
the following lower bound.
We claim that for each arm k ∈ Acr will be played for at least n := loggτ times. If not, there will
be at least one arm k1 ∈ Acr has been played for less than n2 times. We consider the time stamp,
nb = T/2+1 + Pk∈Anon-cr (^^ + PkGAcr；k=k\ ^^ + ^^	^m^ We have that aΓm
1	is the arm with the largest index since that for each arm in Anon-cr, its index is always smaller
than ∆ι + ∆2 once it has been played for (∆J-AT)2 times. For each arm k ∈ Acr (k = kι), its index
is also smaller than ∆ι + ∆2 once it has been played for 8∆gT times. According to assumption that
arm k1 has been played less than n2 times, thus arm 1 is the arm with largest index at time nb .
16
Under review as a conference paper at ICLR 2022
However, on other hand, the index of arm 1 at time nb is never larger than
+∆1+∆2. The
index of arm k1 is not smaller than ∆1 +
产膏.Itleadsto
ii(nb) ≤ J2l0gT- +∆ι +∆2 ≤ ∆ι + J2…≤ i2(nb),
T/2	n2
this contradicts with arm 1 is arm with largest index at time nb . Hence, any arm in Acr should be
played at least 粤2T times.
D Proof of Maximal Inequality (Proof of Theorem 3)
We can order K arms according to the sums μk + Ak 's. Specifically, let the order k1,k2,...,kκ be
defined by
μkι + AkI > μk2 + Ak2 > …> μkκ + Akκ∙	(23)
For simplicity we assume no ties in equation 23. We also assume that Ak > ∆k for all k ∈
Aopt ∪ Acr.
We now aim to bound expectations of the E maxt∈[T] τkt - Nk(t) + for k ∈ Aopt ∪ Acr. We will
use the ordering of the arms k1, k2, . . . , kK defined in equation 23. Take any arbitrary t ∈ [T] and
let kj ∈ Aopt ∪ Acr,
m(j) = sup{n =1,...,t : Tkjn ≤ Nkj (n)}.	(24)
Suppose for a moment that m(tj) < t.
We have
(Tkjt- Nkj (t))+ ≤ Tkj	(25)
+τkj #n = mt(j) + 1, . . . ,t : τkdn > Nkd (n - 1) for some d = 1, . . . ,j - 1}
+Tkj #n = m(tj) + 1, . . . ,t : Tkdn ≤ Nkd (n - 1) for all d = 1, . . . ,j - 1,
arm kj not pulled at time n
-	(1 - Tkj)#n = mt(j) + 1, . . . ,t : Tkdn ≤ Nkd (n - 1) for all d = 1, . . . ,j - 1,
arm kj pulled at time n
=Tkj + Tkj #n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n - 1) for some d = 1, . . . ,j - 1}
-	(1 - Tkj)#n = m(tj) + 1, . . . ,t : Tkdn ≤ Nkd (n - 1) for all d = 1, . . . ,j - 1}
+#n = mt(j) + 1, . . . , t : Tkdn ≤ Nkd (n - 1) for all d = 1, . . . , j - 1,
arm kj not pulled at time n
=Tkj + #n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n - 1) for some d = 1, . . . ,j - 1}
+#n = mt(j) + 1, . . . ,t : Tkdn ≤ Nkd (n - 1) for all d = 1, . . . ,j - 1,
arm kj not pulled at time n
-	(1 - Tkj )(t - mt(j)).
The final bound is, clearly, also valid in the case m(tj) = t.
Next,
#	n = mt(j) + 1, . . . , t : Tkdn > Nkd (n - 1) for some d = 1, . . . , j - 1}	(26)
j-1
=	#n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n - 1), Tkmn ≤ Nkm (n - 1), m = 1, . . . ,d - 1}.
d=1
For d = 1, . . . , j - 1 denote
m(tj,d) = supn = mt(j), . . . , t: Tkdn > Nkd (n - 1)}.	(27)
17
Under review as a conference paper at ICLR 2022
Suppose, for a moment, that mt(j,d) > m(tj) . Then
0 ‹ Tkdmtj,d)-Nkd(m(j,d) - 1) = Tkdm(j) - Nkd (m(j) - 1)
+τkd (m(j,d) — m(j) — #{n = m(j) + 1,..., m(j,d) : arm k& pulled})
-	(1 — Tkd)#{n = m(j) + 1,..., mj,d) : arm kd pulled}
=	Tkdm(j) - Nkd (mtj) - 1) + τkd(mjdd - mtj))
—	#n = mt(j) + 1, . . . , mt(j,d) : arm kd pulled}.
We conclude that
#	n = mt(j) + 1, . . . , mt(j,d) : arm kd pulled}
≤ n=maχ t(τkdn - Nkd(n))+ + Tkd(m(j,d) - m(j)).
Therefore,
#n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n — 1), Tkmn ≤ Nkm (n — 1), m= 1, . . . , d — 1}
=#n = mt(j) + 1, . . . , mt(j,d) : Tkdn > Nkd (n — 1), Tkmn ≤ Nkm (n — 1), m= 1, . . . , d — 1}
S#{n = m(j) + 1,..., mj,d) : arm kd pulled}
+#n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n — 1), Tkmn ≤ Nkm (n — 1), m= 1, . . . , d — 1,
arm kd not pulled
≤ max ATkdn - Nkd(n))+ + Tkda - mtj))
n=1,...,t
+#n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n — 1), Tkmn ≤ Nkm (n — 1), m= 1, . . . , d — 1,
arm kd not pulled ,
(j,d)	(j )
and the final bound is clearly valid even if mT , = mT . Substituting this bound into equation 26
we obtain
#n = mt(j) + 1, . . . , t : Tkdn > Nkd (n — 1) for some d = 1, . . . , j — 1}
j-1	j-1
≤(t - m(j)) ETkd + £ “maχ ATkd t0 - Nkd (t0))+
t =1,...,t
d=1	d=1
j-1
+	#n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n — 1), Tkmn ≤ Nkm (n — 1), m= 1, . . . ,d — 1,
d=1
arm kd not pulled ,
Substituting this bound into equation 25 gives us
(Tkjt - Nkj(t))+ ≤ Tkj	(28)
+X/maχ ATkdt0 - Nkd(t0))+- (t - m(j)) (1- XTkd)
d=1 t =1,...,t	d=1
j
+	#n = mt(j) + 1, . . . ,t : Tkdn > Nkd (n - 1),
d=1
Tkmn ≤ Nkm (n - 1), m= 1, . . . , d - 1, arm kd not pulled
j-1
≤t% + £ 0max ,(Tkdt0 - Nkd (t0)) +
t =1,...,t
d=1
j
+	#n = 1, . . . ,t : Tkdn > Nkd (n - 1),
d=1
Tkmn ≤ Nkm (n - 1), m= 1, . . . , d - 1, arm kd not pulled}.
18
Under review as a conference paper at ICLR 2022
Taking the maximum over t on both sides of above inequality, we then have
j-1
t jm axT (Tkjt - Nkj ⑴)+ ≤ τkj+ X t jm axT (τkdt - Nkd ⑴)+
t=1,...,	t=1,...,
d=1
j
+X#n = 1, . . . ,T : τkdn > Nkd (n - 1),
d=1
Tkmn ≤ Nkm (n - 1), m =1,...,d - 1, arm kd not pulled).
Therefore, we arrive at
(29)
(30)
E	t=m1,a..x.,T Tkjt-Nkj(t) +
+
Tkmn ≤ Nkm (n - 1), m = 1, . . . , d - 1, arm kd not
We will prove that for kd ∈ Aopt ∪ Acr
E 1 Tkdn > Nkd (n - 1),
(31)
Tkmn ≤ Nkm (n - 1), m = 1, . . . , d - 1, arm kd not
≤bdlogT+O(1)
for bd > 0 that we will compute. It is elementary that kj ∈ Aopt ∪ Acr implies kd ∈ Aopt ∪ Acr
for d = 1, . . . , j - 1. Therefore, it will follow from equation 31, equation 30 and a simple inductive
argument that for any kj ∈ Aopt ∪ Acr,
E max Tkjt - Nkj (t) + ≤ aj logT + O(1)
(32)
with a1 = b1 and for j > 1,
j-1	j
aj =	ad +	bd,
which means that
j
aj =	(j - d+ 1)bd.
d=1
We now prove equation 31. We have
Eπ	1 Tkdn > Nkd (n - 1), Tkmn ≤ Nkm (n - 1), m = 1, . . . , d - 1, arm kd
d-1	T
=E E∏ ( EI(Tkdn > Nkd (n - 1),Tkmn ≤ Nkm (n - 1), arm km is pulled at time n)
m=1	n=1
+ E En I E l(τ^kdn > Nkd (n - 1), arm km, is pulled at time n) 1.
m=d+1
19
Under review as a conference paper at ICLR 2022
Observe that a “no-tie” assumption imposed at the beginning of the section implies that
μkd + Akd > μ* ≥ μkm .
Therefore, we can use once again the usual UCB-type argument to see that for any m = 1, . . . , d- 1,
for any B > 0,
En
1 Tkdn > Nkd (n - 1), Tkm n ≤ Nkm (n - 1), arm km is pulled at time n
T
≤B log T + X Pn (Nkm (n - 1)
n=1
>
BlogT,
τkdn
>
Nkd (n - 1), Tkmn ≤ Nkm (n - 1), arm km is pulled at time n
T
≤B log T + X Pn (Nkm (n - 1)
n=1
>
B log T,
τkdn
>
Nkd(n - 1), Tkm n ≤ Nkm(n - 1), ikm (n) ≥ ikd (n)
≤B log T + X Pn (Nkm (n - 1) > B log T, mkm (n - 1) + jNfe2l(0g- 1)
≥ mkd In - 1)+Akd+s Nkd (og-n 1)!.
By carefully choosing
8
we obtain the bound
(μkd + Akd - μkm )2，
En
1 Tkdn > Nkd (n - 1), Tkm n ≤ Nkm (n - 1), arm km is pulled at time n (34)
8
一(μkd + Akd — μkm )
2 logT+O(1),
m = 1, . . . , d - 1. The same argument shows that for every m = d + 1, . . . , K,
En ( E 1 (Tkdn > Nkd (n - 1), arm km is pulled at time n)
8
一(μkd + Akd - μkm
-Akm)2logT+O(1).
(35)
Now equation 34 and equation 35 imply equation 31 with
d-1
bd = X
m=1
(μkd + Akd - μkm )2
+ X _____________8__________
m=d+1 (μkd + Akd - μkm - Akm )2
(36)
8
Now it follows from equation 36 and equation 33 that for every j such that kj ∈ Aopt ∪ Acr ,
j
aj =8X(j-d+1)
(μkd + Akd - μkm )2
+ X ________________1___________1
m=d+1 (μkd + Akd - μkm - Akm )2 J
(37)
1
We conclude by equation 32 that every j such that kj ∈ Aopt ∪ Acr,
En(TkjT - Nkj (T))+ ≤ aj log T + O(I),
with aj given in equation 37.
(38)
Remark. In the proof, We assume that there is no tie, i.e., Akji + μj = Akj2 + μj for any
jι = j2 ∈ [K ]. This assumption is not restrictive since the probability that event “Aj + μj =
Akj2 + μkj2 for some jι = j2 ∈ [K].” is zero when we pick penalty rates Ak,s uniformly randomly.
20
Under review as a conference paper at ICLR 2022
E Proof of Gap-independent Upper B ounds
Proof of Lemma 1 We first prove that the algorithm pulls arm k0 with Ak，一 ∆ko ≤ 2 J2TgT at
time n when Nk0 (n) ≥ T2/3 and Nk(n) < τkn with vanishing probability. This is because
	2 log n	2 log n P(Ak + mk(n) + NNP ≤ Ak0 + mk0(n) + NNkTP)
≤	2 log n	2 P(Ak+μk ≤ Ak+μk + 2y Nk0(n))+ n
≤	P(Ak - 4k ≤ Ak -、k + 2∖j n g 1)+ 2 Nk， (n)	n2 2/n2.	(39)
Next, We say arm k is a very critical arm if arm k satisfies Ak - ∆k ≥ 2y 2τj0gT. Otherwise k
is a non-very critical arm. In other words, each non-very critical arm can be only played at most
O(T 2/3 ) times with high probability.
Furthermore, we can show that Nk0 (n) ≥ T2/3 at time n = c0T/2 for each very critical arm k0.
If not, note that T2/3 ≤ τk0 c20T /4, then Nk0 (n) < τk0n for any n ∈ {dc02 T /4e, . . . , bc0 T /2c}.
Hence, for any arm k00 can be played at most max{τk00 c0T /2, T2/3} times between rounds c02T/4
and c0T/2. Then, we must have
coT/2 - c0T∕4 ≤ ETkcoT/2 + £T2/3.
kk
However, the above inequality fails to hold when T ≥ (4K/c20)3 . This leads to the contradiction.
Thus, we have Nk0 (n) ≥ T2/3 for any very critical arm k0 at time n = c0T/2.
This further gives us that we must have Nk0 (n) ≥ bτk0 nc for all very critical arms at some time
n ∈ [coT, T]. To prove this, we observe the fact that for any arm k, it will be played with probability
less than 尧 at time n once Nk (n) ≥ max{τkn, T2/3} and one critical arm k0 is played less than
Tk，n. (In other words, this tells US that once arm k has been played max{τkn, T2/3} times, then it
can only be played at time when all very critical arms k0s have been played for τk0n times or bτkknc
jumps by one with probability greater than 1 - 2/n2 .)
Let n1(≥ c0T/2) be the first ever time such that Nk， (n1) ≥ bTk， n1c. By straightforward calcula-
tion, it gives that n1 must be bounded by
nι ≤ coT/2+	X	T2/3 + (XTk，)T ≤ (co + XTk，)T
k，， :non-very critical	k，	k，
with probability greater than 1 - 2K/T.
That is, n1 is well defined between coT/2 and T. At time n1, we have all very critical arms k0
such that Nk， (n1) ≥ Tk，n1. Therefore, starting from time n1, the maximum difference between any
non-fairness level (Tk， n - Nk， (n))+’s with k0 in the set of very-critical arms is always bounded by
1 with probability 1 - 2K/T for all n ∈ [n1, T].
Lastly, suppose n2 be the last time that arm k is above fairness level. We know at time n = n2, each
very critical arm k0 is played for at least Tk， n2 - 1. by previous argument. Then in the remaining
T - n2 rounds, we know that each very critical arm is played at most Tk，T - Tk， n2 + 1. Then we
must have
T-n2 ≤ (	Tk，)(T-n2)+K+	T2/3,
k，:very critical	k:non-very critical
which implies T - n2 ≤ (KT2/3 + K)/co. This finally implies that Nk (T) ≥ Nk (n2) ≥ TkT -
Tk (KT2/3 + K)/co - 1 with probability at least 1 - 2K/T. That is, E[(TkT - Nk(T))+] =
Tk(KT2/3+K)/co+1 =O(TkKT2/3).
21
Under review as a conference paper at ICLR 2022
We prove the gap-independent upper bound (Theorem 4) by considering the following situations.
Situation 1.a For arm k ∈ Anon-cr and ∆k ≤ 4 JloTT, the regret on arm k is upper bounded by
(∆k - Ak)(τk T - Nk (T))	(40)
if 0 ≤ Nk(T) ≤ τkT; or bounded by
∆k (Nk (T) - τkT) + (∆k - Ak)τkT	(41)
if Nk (T) ≥ τkT.
Situation 1.b For arm k ∈ Anon-Cr and ∆k > 4∖ ∕θ2T-,
•	if ∆k - Ak > 4vzlog T∣TkT the regret on arm k is upper bounded by
Ck-Ak)⅛8j-‰ + O(1)).
•	if ∆k - Ak ≤ 4，log TlTkT the regret on arm k is upper bounded by
8 log T
(∆k - Ak)TkT + ∆k [(-∆-----TkT)+ + O(1)]∙
In other words, for any arm k ∈ Anon-cr, its regret is always bounded by
4 V ~T~ TkT + 4Pτk T log T + 4,bTT(Nk (T) - Tk T)+.
(42)
(43)
(44)
Situation 2 We then split set Aopt ∪ Acr into two subsets, Acr, large and Acr,small, where
Acr, large
：={k : Ak - ∆k > 4 J2MT}
T2/3
and
Acr, small := {k : Ak - ∆k ≤ 4
2log T
T 2/3 }.
For arm k ∈ Acr,large, we have E[(TkT - Nk(T))+] = O(TkKT 2/3) by Lemma 1. The regret on
arm k is then bounded by
∆kE[Nk(T) -TkT] +O(AkTkKT2/3)
8 log T
≤ ∆k min{ -∆2------TkT, Nk (T) - TkT} + O(AkTkKT	)∙	(45)
For arm k ∈ Acr,small, the regret on arm k is then bounded by
(Ak - ∆k)(TkT - Nk(T)) ≤ 4TkT2/3(log T)1/2
if 0 ≤ Nk (T) ≤ TkT, or
8 log T
∆k min{ -T^2----TkT + O(1), Nk(T) - TkT}
∆k
if Nk(T) ≥ TkT.
In summary, for any arm k ∈ Aopt ∪ Acr,
8 log T
∆k min{-^g- - TkT,Nk(T) - TkT} + O(max{AkTkKT2/3,47kT2/3(logT)1/2}).
(46)
(47)
(48)
22
Under review as a conference paper at ICLR 2022
Combining above situations, the total regret is upper bounded by
^X max { 8 p τk t log t + 4γ —T— (Nk (T ) - τk T )+}
k∈An
on-cr
8 log T
+	∆k min{ -∆--------τkT, Nk (T) - TkT} + O(max{AkτkKT	, 4TkT	(log T)	})
k∈Aopt∪Acr	k
≤
8pTi0gT(	X	√Tk)	+ AmaxKT2/3(logT)1/2(	X	τk)	+ 4^0Tt	X	(Nk(T)	-	τkT) +
k∈Anon-cr	k∈Acr ∪Aopt	k∈Anon-cr
+ E	√8(Nk(T) - TkT)+ log T
k∈Aopt∪Acr
≤
8 PTlogT (^X √τk) + AmaxKT2∕3(log T )1/2 + 4J —T— (1 - Tmin)T + p8 log T p KT (1 - τmin)
k
(49)
where 49 uses the fact that	k∈Acr∪Aopt Tk ≤	k Tk ≤ 1;	k∈Anon-cr (Nk(T) - TkT)+ ≤ T(1 -
Tmin ) and
X	P(Nk(Ty-TkTK ≤ X P(Nk(TFTkTK ≤ Ik χ^(N^k (T) - TkT)+ ≤ PKT(1 - Tmin)
k∈Aopt∪Acr	k	k
by Jenson’s inequality.
F Proof of Gap-independent Lower Bounds
Consider a K-arm setting with μ2 = μ3 = ... = μκ = 0, μ1 = ∆ (0 < ∆ < 1/2),
A1, A2, . . . , AK > 0, ∆ < Ak for k ∈ [K], T1, T2, . . . , TK ∈ [0, 1].
Since PkT=2 Nk(T)	≤ T, then it holds Eπ[Nk1 (T)]	≤ T/(K - 1) with k1	=
argmink>1 En [Nk (T)] for any policy π. We then construct another K-arm setting with μ^ = 2∆
and all other parameters remain the same.
For policy π, the regret of the first setting is
R1,π(T) ≥ AE[(T1T - N1(T))+] + {∆E[Nk1 (T) -Tk1T]+AE(Tk1T-Nk1(T))+}
and the regret of the second setting is
R2,π(T) ≥AE[(Tk1T-Nk1(T))+]+{∆E[N1(T) -T1T]+AE(T1T-N1(T))+}
If N1(T) < (1 + T1 - Tkι)T∕2, then R1,∏(T) ≥ ∆ 1-τ12~τk1 T. While N1(T) > (1 + T1 - TkN2,
then R2,π (T) ≥ ∆ 1-τ12-τk1 T. In other words, for policy π,
worst regret ≥	2(R1,∏ (T) + R2,∏ (T))
≥ ∣(∆T 1 - T；- TkI P(N1(T) < 1 + TI- TkI) + ∆T 1 - T；- TkI P(N1(T) ≥ 1 + T；- TkI))
≥ (I- TI-TkI)4 eχp{-KL(P1∣∣P2)}	(50)
8
≥ (I- TI- TkI)K exp{-CT∆2∕(K - 1)},	(51)
8
where P1 and P2 are two probability distributions under two settings associated with policy
∏; 50 follows from the Bretagnolle-Huber inequality. Inequality 51 holds since KL-divergence
KL(P1kP2) ≤ CT∆2∕(K - 1) for many probability distributions. (E.g. C = 1∕2 if the reward of
each arm follows Gaussian distribution with variance 1.)
23
Under review as a conference paper at ICLR 2022
Taking ∆ = JKTI, We have
orst regret ≥
≥
(I-TLmaXk=ι Tk心T eχp{-cτMl(K - 1)}
8
(1 - 2maχk Tk)P(K - IrTIC
8e	,
(52)
Where e = eXp{1}. This completes the proof of Theorem 7.
24