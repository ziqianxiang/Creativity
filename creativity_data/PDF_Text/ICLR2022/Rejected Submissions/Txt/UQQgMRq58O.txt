Under review as a conference paper at ICLR 2022
Understanding Generalized Label Smoothing
when Learning with Noisy Labels
Anonymous authors
Paper under double-blind review
Ab stract
Label smoothing (LS) is an arising learning paradigm that uses the positively weighted
average of both the hard training labels and uniformly distributed soft labels. It was shown
that LS serves as a regularizer for training data with hard labels and therefore improves the
generalization of the model. Later it was reported LS even helps with improving robustness
when learning with noisy labels. However, we observe that the advantage of LS vanishes
when we operate in a high label noise regime. Puzzled by the observation, we proceeded to
discover that several proposed learning-with-noisy-labels solutions in the literature instead
relate more closely to negative label smoothing (NLS), which defines as using a negative
weight to combine the hard and soft labels! We show that NLS differs substantially from
LS in their achieved model confidence. To differentiate the two cases, we will call LS the
positive label smoothing (PLS), and this paper unifies PLS and NLS into generalized label
smoothing (GLS). We provide understandings for the properties of GLS when learning with
noisy labels. Among other established properties, we theoretically show NLS is considered
more beneficial when the label noise rates are high. We provide extensive experimental
results on multiple benchmarks to support our findings too.
1 Introduction
Label smoothing (LS) (Szegedy et al., 2016) is an arising learning paradigm that uses positively weighted
average of both the hard training labels and uniformly distributed soft label:
yLS,r = (1 - r) ∙ y + -r ∙ 1
K
(1)
where we denote the one-hot vector form of hard label and an all one vector as y, 1 respectively. K is the
number of label classes, and r is the smooth rate in the range of [0, 1]. It was shown that LS serves as a
regularizer for the hard training data and therefore improves generalization of the model. The regularizer role
of LS prevents the model from fitting overly on the target class. Empirical studies have demonstrated the
effectiveness of LS in improving the model performance across various benchmarks (Pereyra et al., 2017)
(such as image classification (Szegedy et al., 2016), machine translation (Vaswani et al., 2017), language
modelling (ChoroWski & Jaitly, 2017)) and model calibration (Muller et al., 2019).
Later it was reported LS even helps with improving robustness when learning with noisy labels (Lukasik et al.,
2020). HoWever, We observe that the advantage of LS vanishes When We operate in a high label noise regime.
In Figure 1, We present a set of experiments on UCI datasets (Dua & Graff, 2017). We highlight best tWo
smooth rates (possible to have tied smooth rates) under each label noise rate. Indeed, non-negative smooth
rates (circles colored in red) outperform negative ones When the label noise rates are loW. Nonetheless, With
the increasing of noise rates, negative smooth rates r < 0 (Eqn. (1), diamonds colored in green) appear to be
more competitive When learning With noisy labels. Puzzled by the observation, We proceeded to discover
1
Under review as a conference paper at ICLR 2022
that several proposed learning-with-noisy-labels solutions in the literature, including Loss Correction (Patrini
et al., 2017), NLNL (Kim et al., 2019) and Peer Loss (Liu & Guo, 2020), instead relate more closely to
negative label smoothing (NLS), which defines as using a negative weight to combine the hard and soft labels!
	UCl DataSet - TWonorm	n	UCl Dataset - Waveform	UCI Dataset - Splice
o.o	]：.,	1.0	 • ∙ ∙	0.0	1.
	.	0.5-	
-2.5	.	*	-2.5
		0.0	∙	∙	
	♦ ♦		♦ ♦ ♦
-5.0		♦ ♦	-5.0
		♦ ♦	
	• Optimal Smooth Rate (PLS)	-0.5	
-7.5	♦ Optimal Smooth Rate (NLS).	.	F] ,	♦	♦	♦
	0.0	0.2	0.4	~1.0 ι	ι	ι 0.0	0.2	0.4	0.0	0.2	0.4
	Noise Rate	Noise Rate	Noise Rate
Figure 1: Optimal smooth rates on UCI datasets with different label noise rates.
This paper unifies label smoothing with either positive or negative smooth rate into a generalized label
smoothing (GLS) framework. Our paper is motivated by the above inconsistent observations. And we aim to
provide a more thorough understanding of GLS under the setting of learning with noisy labels, rather than
proposing a new method to compete for state-of-the-art performances. We firstly show that negative label
smoothing differs substantially from positive label smoothing in their achieved model confidence. With the
presence of label noise, we then proceed to show that there exists a phase transition behavior when finding
the optimal label smoothing rate for GLS. Particularly, when label noise is low, positive label smoothing is
able to uncover the optimal model while negative label smoothing is considered more beneficial in a high
label noise regime.
We provide extensive experimental evidences to support our findings. For instance, on multiple benchmark
datasets, we present the clear transition of the optimal smoothing rate going from positive to negative when we
keep increasing noise rates. On CIFAR-10 test data, we show a negative smoothing rate elicits higher model
confidence on correct predictions and lower confidence on wrong predictions compared with the behavior of
a positive one.
Our contributions summarize as follows:
•	We provide understandings for a generalized notion of label smoothing (GLS) when learning with noisy
labels, where the label smooth rate can go negative. (An understanding paper rather than a method paper)
•	We show that several robust loss functions in the noise learning literature correspond to learning with GLS,
under certain noise rate models. (Section 3)
•	We show that negative label smoothing improves the confidence of model prediction. With the presence
of label noise, we demonstrate learning with a negative smooth rate can be more robust to label noise
compared with a positive rate when label noise rates are high. (Section 4, 5)
•	Empirical experiments on multiple datasets and methods demonstrate that with the presence of label noise,
NLS becomes competitively robust to label noise. We also empirically show how GLS results in trade-offs
in model confidence, bias and variance of the generalization error. (Section 6, Appendix D and E)
•	Extensive experiment results validate our main theoretical conclusions. Besides, we also discuss practical
considerations and multi-class extensions of GLS to mitigate the impact of label noise. (Appendix B, C)
We defer all proofs to Appendix F. Our work primarily contributes to the literature of learning with noisy
labels (Scott et al., 2013; Natarajan et al., 2013; Liu & Tao, 2015; Patrini et al., 2017; Liu & Guo, 2020).
Our core results are contingent on recent works of understanding the effect of label smoothing when training
deep neural network models, i.e., label smoothing improves model calibration (Muller et al., 2019), more
complexed forms of label smoothing (Li et al., 2020; Yuan et al., 2020), and in particular when label noise
2
Under review as a conference paper at ICLR 2022
presents (Lukasik et al., 2020; Liu, 2021). We generalize a concept called generalized label smoothing and
provide new understandings for when, instead of setting a positive smoothing rate as the literature would
normally do, a negative smoothing rate is considered a better option. Due to the space limit, we defer a more
detailed discussion of related works to Appendix A.
2 Preliminaries
2.1	Learning with smoothed labels
For a K-class classification task, we denote by X ∈ X a high-dimensional feature and Y ∈ Y := {1, 2, ..., K}
the corresponding label. Suppose (X, Y ) ∈ X × Y are drawn from a joint distribution D. Let yi be the
one-hot encoded vector form of yi which generates according to Y . The random variable of smoothed label
Y LS,r with smooth rate r ∈ [0, 1] generates yiLS,r as (Szegedy et al., 2016):
yLS,r = (1 - r) ∙ yi + : ∙ 1
K
For example, when r = 0.3, the smoothed label of yi = [0, 1, 0] becomes yiLS,r=0.3 = [0.1, 0.8, 0.1].
We consider a broader setting where the smoothed label might be negatively related to the corresponding
feature. As a supplementary to existed works on label smoothing, we explore the benefits of learning with
generalized label smoothing (GLS), i.e., r ∈ (-∞, 1] instead of a non-negative r.
yGLS,r :=(1 - r) ∙ yi + r_ ∙ 1	(2)
K
where yiGLS,r is given by the random variable of generalized smooth label Y GLS,r . We name the scenario
r < 0 as negative label smoothing (NLS). To clarify, we don’t assume a strict lower bound for r. If
r → -∞, normalizing yGLS,r by 1 - r returns yGLS,r = Ni - K. We will show when imposing a negative
smoothing parameter will be considered beneficial as compared to a positive one. In the main paper, we
focus on the binary classification task where yi ∈ {0, 1} and K = 2. And we defer multi-class extensions
to Appendix C. Denote f as a deep neural network, f(xi) is the model prediction of xi ∈ X with element
f (xi)y := P(Y = yi|X = xi, f). Given the sample x ∈ X and a hard label y ∈ Y, binary CE loss is defined
as 'CE(f (x), y) := - log(f (x)y). Throughout this paper, We shorthand ' as 'ce for a clean presentation.
2.2	Learning with noisy labels
The noisy label literature considers the setting where we only have access to samples with noisy labels
from (X, Y ). Suppose random variables (X, Y ) ∈ X × Y are drawn from a noisy joint distribution D.
Statistically, the random variable of noisy labels Y can be characterized by a noise transition matrix T , where
each element Ti,j represents the probability of flipping the clean label Y = i to the noisy label Y = j, i.e,
Tij = P(Y = j|Y = i). In this paper, we are interested in the widely studied class-dependent label noise.
We assume the label noise is conditionally independent of features, i.e.,
P(Ye =j|Y =i) = P(Ye =j|X,Y =i),∀i,j ∈ [K]
For the binary classification setting, define e0 := P(Ye = 1|Y = 0), e1 := P(Ye = 0|Y = 1). Without loss of
generality, we assume e1 - e0 = e∆ ≥ 0. We denote the binary noise transition matrix in the noisy label
setting as: T =	1 -e1e0 1 -e0e1	.
3
Under review as a conference paper at ICLR 2022
2.3	Model confidence
We define a key quantity, model confidence, that plays an important role in later sections.
Definition 1. Model confidence of model f for sample (x, y). Given a model f, a sample x with its target
label y ∈ {0, 1}, the model confidence of f w.r.t. sample x is defined as MC(f; x, y) = f (x)y - f (x)1-y.
MC(f; x, y) in definition 1 characterizes the difference of the predicted probability between target class and
the other class. MC(f; x, y) = 0 simply means f has no confident on its predictions since the model can not
identify the target class of x. MC(f ; x, y) returns a negative value when f gives a wrong prediction and is not
confident to predict the label of x as the target label y. To dig into how GLS influences the model confidence
on correct and wrong predictions in following sections, we separate the distribution D into
d+ := {(X,Y)〜D : MC(f; X, Y) > 0},	D- := {(X,Y)〜D : MC(f;X,Y) ≤ 0}
3 Connection to other robust methods
In this section, we aim to theoretically explore the connection between GLS and popular methods such as
backward/forward loss correction (Natarajan et al., 2013; Patrini et al., 2017), NLNL (Kim et al., 2019) and
peer loss (Liu & Guo, 2020). We defer the corresponding empirical validations to Appendix B.
For r ≤ 1, let y be the vector form of noisy label y obtained from Y, We define the r smoothed label of y
GLS	GLS	GLS
as yGLS,r, where yGLS,r := (1 — r) ∙ y + (r/K) ∙ 1 and is generated by the random variable YGLS,r. Risk
minimization of the Generalized Label Smoothing (GLS) W.r.t. noisy labels becomes:
Risk Minimization Using GLS: min E(X Y)〜力['(f (X),YGLS,r)]	(3)
The GLS framework covers three special methods: PLS (r ∈ (0, 1]), Vanilla (CE) Loss (r = 0) and NLS
(r < 0). Besides, we observe that NLS connects to a special case of label smoothing regularization. We
highlight this in Theorem 1.
Theorem 1.	∀r ∈ [0, 1], NLS with smooth rate —r is a special form of label smoothing regularization:
min E(χ,γ 卜 d h'(f (X),Y GLS ST)] = min E(χ,γ 八百12 ∙'(f (X),Y) — '(f (X),Y gls ,r)]
3.1	Loss correction
Loss correction (Patrini et al., 2017) studies two robust loss designs which are based on the knowledge of non-
•	1	∙	, ♦ m El 11 ι	J	λ√— /cm -Γr∖	♦ 1 , ,1 ι 八/c/m -Γr∖ t
singular noise transition matrix T. The backward correction '—(f (X), Y) re-weights the loss '(f (X), Y) by
1
T- e with Y being the model predicted label, while the proposed forward correction '→(f (X), Y) multiplies
Y,Y
the model predictions by T .
Proposition 1. For r LC := ^^e-ɪ < 0, λ LC := e∆ ∙ ι-2eθ, risk minimization ofboth backward and forward
correction (with the knowledge of noise rates) are equivalent to the combination of NLS and an extra bias
term Bias-LC
minE(χ,γ卜D ,一(f(X), Y)] = minE(χ,γ卜D ['→(f(X), Y)]
=minE(χ,γ2['(f(X), YGLS,rlc)] + λlc ∙ Eχ,γ =1 ['(f(X), 1) — '(f(X), 0)]
X------------------------------}
^™^{1^^^™
Bias-LC
4
Under review as a conference paper at ICLR 2022
The incurred Bias-LC controls the model confidence on (X, Y = 1)〜Df. Note that when the noise rate is
not substantially high, i.e, eo ∈ [0, 2), Xlc > 0. Then, compared with loss correction, NLS with smooth rate
rLC makes the model f to be less confident on (X, Y = 1)〜D； and more confident on (X, Y = 1)〜D-
(wrong predictions). However, the impact of term Bias-LC is diminishing when either e∆ → 0 (symmetric
noise rates) or e0 → 0 (low noise rates) as specified in Theorem 2.
Theorem 2.	Assume the noise transition matrix is symmetric, i.e, e∆ = 0, backward and forward loss
correction are a special form of NLS with smooth rate rLC.
3.2 Learning from complementary labels
Complementary label (Ishida et al., 2017) was firstly introduced to mitigate the cost of collecting data.
Rather than encouraging the model to fit directly on the target, learning from complementary labels trains
the model to not fit on the complementary label which differs from the target. Later, an indirect training
method “Negative Learnin” (NL) (Kim et al., 2019) was proposed to reduce the risk of providing incorrect
information with the presence of noisy labels and is robust to label noise in multi-class classification tasks.
A more generic unbiased risk estimator of learning with complementary labels was proposed (Ishida et al.,
2019) and is defined as: 'cL(f (X),Y):= '(f (X),Y) - '(f (X),1 - Y).
Theorem 3.	Learningfrom complementary labels with 'CL is equivalent to NLS with smooth rate rCL → 一∞:
minE(χ,γ卜De h'CL(f(X),Y)[ 0 min E(χ,γ)〜D['(f(X),YGGS,rcl--∞)]
3.3 Peer loss functions
Peer loss functions (Liu & Guo, 2020) propose a family of robust loss measures which do not require
the knowledge of noise rates. The mathematical representation of peer loss functions is d(f (X), Y):=
'(f (X), Y) 一 '(f (Xι),Y2), where (Xi,匕)〜 D. The second term of peer loss evaluates on randomly paired
data samples and labels to punish f from overly fitting on noisy labels.
Proposition 2. For r PL := 2 ∙ P(Y = 1), Xpl := 1 — r pl, risk minimization of peer loss is equivalent to
negative label smoothing regularization with an extra term Bias-PL, i.e.,
minE(χ,γ卜De h'PL(f(X),Y)[ = min E(χ,γ)〜D ,(f(X),Y) — '(f(X),YGGL,rpl)[
+Xpl ∙ Eχ,γ =ι ['(f(X), 1) — '(f(X), 0)[
'--------------V---------------}
Bias-PL
The incurred term Bias-PL controls the model confidence on (X, Y = 1)〜D and has a diminishing effect
as P(Ye = 1) → 1/2. Generally, peer loss relates to GLS as the negatively weighted GLS term appears to be
a regularizer. Note that we have access to the P(Y = 1), we can bridge the gap between GLS and peer loss
by adding an estimable term Bias-PL. With some derivations, we further show in Theorem 4, when noisy
priors are equal, peer loss has an exact GLS form.
Theorem 4.	When the noisy labels have equal prior, i.e, P(Y = 0) = P(Y = 1), peer loss is a special form
of NLS regularization with smooth rate rPL. Besides,
min E(χ,Y 卜De ['PL (f (X),Y)[ ^⇒ min E(χ,Y)〜D ['(f (X),Y GGL ,r--∞)[
4 GLS and model confidence
Now we show that NLS differs substantially from PLS in their achieved model confidence. This discussion
sets the foundation for our discussion when learning with noisy labels in next section.
5
Under review as a conference paper at ICLR 2022
4.1	GLS on clean data
We firstly show that NLS differs substantially from PLS in their achieved model confidence. This discussion
sets the foundation for our discussion when learning with noisy labels in proceeding subsections.
When the label is clean, i.e, e0 = e1 = 0, Eqn. (3) reduces to:
minE(χ,γ)~d ['(f(X),Y)] +； ∙ E(χ,γ)~d ['(f(X), 1 - Y)- '(f(X),Y)[	(4)
S------------{z------------}
TermMC' (f ;X,Y)
To clarify, we are not restricting D to have infinite samples, i.e., for the discrete distribution D = {xi, yi}iN=1,
Eqn. (4) becomes: min hNPi∈[N]'(f(Xi),yi)] + 2N hPi∈[N] ('(f(Xi), 1-yi)-'(f(Xi),yi川.NOte：
MC'(f; X,Y) = log (f(X)γ/(1 - f(X)Y)); MC(f; X,Y) = 2 ∙ f(X)Y - 1
Both log x/(1 - x) and 2x - 1 are monotonically increasing for x ∈ (0, 1), model f with a high
MC'(f; X, Y) has high MC(f; X, Y). The difference between PLS and NLS lie in the weight of Term
MC'(f; X, Y): NLS encourages high MC'(f; X, Y) and MC(f; X, Y) while PLS has an opposite effect.
4.2	Insights from an empirical observation
We adopt the generation of 2D (binary) synthetic dataset from (Amid et al., 2019) by randomly sampling two
circularly distributed classes. The inner annulus indicates one class (blue), while the outer annulus denotes
the other class (red). We hold 20% data samples for performance comparison.
Side-effects of over-confident In Fig-
ure 2, the colored bands depict the dif-
ferent levels of prediction probabilities:
light blue + orange bands indicate sam-
ples that satisfy MC < 0.4 (low model
confidence). When learning with clean
data, GLS with non-positive smooth rate
may yield over-confidence on the model
prediction and a relatively low test accu-
racy.
Label noise reduces model confidence
Recent works (Liu, 2021; Cheng et al.,
2020) have demonstrated that with the
presence of label noise, learning with
noisy labels directly will eventually result
in unconfident model predictions. Con-
tinuing the synthetic 2D dataset, we flip
the clean labels according to a symmetric
noise transition matrix with noise rate ei
for both classes. With the presence of
label noise in Figure 3, GLS generally
Figure 2: Model confidence visualization of GLS on synthetic
data (Type 1) with the clean data. r* ∈ [0,0.4]. (left: NLS;
middle: Vanilla Loss; right: PLS).
Figure 3: Model confidence visualization of GLS on synthetic
data (Type 1) with noise rate ei = 0.4. ropt = -0.4. (left: NLS;
middle: Vanilla Loss; right: PLS).
becomes less confident on its predictions. Besides, when the smooth rate increases from negative to positive,
more samples are of uncertain predictions. Thus, a smaller/negative smooth rate is beneficial when the noise
rate increases by encouraging more confident predictions.
6
Under review as a conference paper at ICLR 2022
5 GLS with label noise
In this section, we target at the optimal candidates of r in GLS when the label noise presents. In Section 4,
we have shown that NLS and PLS have the opposite functionality on the model confidence when training
on clean data. Given the unseen test data, learning with non-negative smooth rates may not always return
the best outcome (Figure 3). Based on this observation, we delve into details to show when NLS is more
favorable than PLS and Vanilla Loss. We start with stating Assumption 1:
Assumption 1. We assume learning with clean data distribution D with smooth rate r* ≤ 1 in GLS returns
the best performance on the unseen clean test data distribution Dtest .
Assumption 1 simply offers an “anchor” point to initiate our analysis for the noisy label setting. To clarify, we
don't rule out the possibility that other methods outperform GLS with optimal smooth rate r*. Later in Section
6.1 and Appendix D, we will empirically test what r* usually is on various benchmarks. We define optimal
classifier on Y * which follows r* smooth label distribution as: fD := arg minf E(χ,γ)〜D ['(f (X), Y *)].
With the introduction of r* and fD* , our goal is then to recover the classifier f using the noisy training labels.
To bridge learning with noisy labels and clean labels for GLS, we define λ1, λ2 and offer Theorem 5.
λι :=	[(eo -	r^)	+ (1 —	2eo)	∙	W],	λ2:=巳δ	∙ (1 - r)
Theorem 5.	The risk minimization of GLS (Eqn. (3)) in the noisy setting relates to the risk defined on the
clean data with two additional bias terms:
min
E(X,Y)〜D
['(f(X),Y*)] +λι ∙ e(X,Y )〜D
True Risk
['(f (X), 1 — Y) — '(f (X),Y)]
_	-	1
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
M-InCI
+λ2 ∙ Eχ,γ=1
['(f (X), 0)- '(f (X),1)]
_______ - /
{^^^^^^^^^^^^^^^^^^^^^^
M-Inc2
(5)
The True Risk is the risk minimization w.r.t. clean optimal label distribution Y*. Training GLS on noisy
labels results in two extra bias terms which affect the model confidence. We defer an empirical validation of
Theorem 5 to Appendix B. Now we proceed to answer “what parameters are preferred in the noisy setting”.
5.1	SYMMETRIC ERROR RATES WITH e∆ = 0
Symmetric error rates e := e0 = e1 indicates the probability of flipping to the other class is equal for both
classes. In this case, λ2 = 0 and Term M-Inc2 is cancelled and Eqn. (5) reduces to
min
E(X,Y)〜D['(f(X),Y*)] +λι ∙ e(X,Y )〜D
'----------7----------'、--------------
True Risk
'(f(X),1—Y)—'(f(X),Y)
_ - -
{z
M-Inc1
(6)
Noisy labels impairs model confidence on Vanilla Loss In the GLS frame-
work, define the optimal r that will cancel the impact of Term M-Inc1 as:
r* — 2e
when r0pt := ] - 2已, M-InCI = 0
The threshold ropt in Eqn. 7 implies:
Theorem 6.	With Assumption 1, GLS with smooth rate r = ropt yields fD* .
•	When error rate e < r*/2, r = ropt > 0 (PLS);
(7)
Figure 4: Decision between
NLS, PLS given e, r*.
7
Under review as a conference paper at ICLR 2022
•	When error rate e = r*/2, r = 0 (Vanilla Loss);
•	When error rate e>r*∕2, r = r opt < 0 (NLS).
In Theorem 6, adopting NLS When noise rate e < r*/2 induces λι < 0, Term M-IncI makes f overly-
confident on its predictions compared with Y*. In Figure 4, with the decreasing of r*, PLS is less tolerant of
labels with high noise. Similarly, if e ≥ 号,with the decreasing of r*, NLS is more robust in the high noise
regime while PLS makes the model f become less-confident on its predictions. Clearly, NLS outperforms
PLS especially when noise rates are large and r* is small.
5.2 ASYMMETRIC ERROR RATES WITH e∆ 6= 0
In this case, adopting r = r1--2e0 removes the Term M-Inc1. However, when r < 1, Term M-Inc2 is not
negligible due to assymetric noise transition matrix. As a result, Term M-Inc2 becomes:
e∆ ∙ 1 - 2e0 ∙ EX,Y=1 h'(f(X), 0) - '(f(X), 1)i, with e∆ ∙ 11-2r* ≥ 0
Term M-Inc2 in the minimization increases the model confidence on (X, Y = 0)〜D：. The model will then
become overly-confident with the class that has a low noise rate e0 . Meanwhile, Term M-Inc2 decreases the
model confidence on (X, Y = 1)〜D： (less-confident to the class with a high noise rate eι).
6 Experiment results
In this section, we present our empirical observations regarding the role of GLS under clean and noisy labels
by using UCI datasets, CIFAR-10 and CIFAR-100.
6.1 WHAT IS THE PRACTICAL DISTRIBUTION OF r* AND rOPT?
r* and ropt on UCI datasets (Dua & Graff, 2017) As for UCI datasets, we pick Twonorm and Splice for
illustration in the main paper. The noisy labels are generated by a symmetric noise transition matrix with
noise rate ei = [0.1, 0.2, 0.3, 0.4]. As highlighted in Table 1, ropt appears with positive values when the data
is clean (same as r*) or of a low noise rate. With the increasing of noise rates, the performance of PLS results
in a much larger degradation compared with NLS. We color-code different noise regimes where either PLS
(red-ish) or NLS (green-ish) outperforms the other. Clearly there is a separation of the favored smoothing rate
for different noise scenarios (upper left & low noise for PLS, bottom right & high noise for NLS).
Table 1: Test accuracies of GLS on clean and noisy UCI datasets with best two smooth rates (green: NLS;
red: PLS). Results on more benchmark datasets are deferred to Appendix D.
Smooth Rate	e, = 0	ei = 0.1	Twonorm ei = 0.2	ei = 0.3	e, =0.4	e, = 0	e, = 0.1	Splice e, = 0.2	e, = 0.3	e, = 0.4
r = 0.8	0.990	0.990	0.986	0.982	-0.968-	0.980	0.946	0.919	0.856	0.760
r = 0.6	0.990	-0.989-	0.987	0.981	-0972-	0.978	0.939	0.913	0.869	0.778
r = 0.4	-0.990-	-0.990-	0.987	0.983	-0.971-	-0.978-	0.948	0.922	0.885	0.797
r = 0.2	0.990	-0.989-	0.986	0.985	-0.969-	0.978	0.948	0.919	0.878	0.800
r = 0.0	-0.990-	0.989	0.987	-^0985	-0.973-	-0.976-	0.948	0.926	0.876	0.806
r = -0.4	-0986-	0.988	0.988	-^0986	-0.972-	-0.961-	0.956	0.928	0.880	0.817
r = -0.6	-0986-	0.988	0.987	-^0984	-0.974-	-0.961-	0.956	0.926	0.880	0.819
r=-1.0	-0986-	0.986	0.988	-^0985	-0.977-	-0.956-	0.954	0.932	0.889	-0.819-
r= -2.0	-0986-	0.986	0.986	-^0986	-0.978-	-0.952-	0.946	0935	-0.898-	-0.830-
r = -4.0	0.986-	0.986	-^0.986	0.986	0.983	0.946-	0.943	0.939	0.911	0.830
r = -8.0	-0986-	0.986	0.986	-^0985	-0.986-	-0.943-	0.946	0.939	-0.915-	-0.845-
ropt =	[0.0, 0.8]	[0.4, 0.8]	[-1.0,-0.4]-	卜4.0,-0.4]-	-8.0	[0.0, 0.8]	[-0.6,-0.4]-	卜8.0,-4.0]-	-8.0	-8.0
r* and ropt on CIFAR datasets (Krizhevsky et al., 2009) When learning with a larger scale and more
complex dataset, like CIFAR-10 and CIFAR-100, models are prone to converge on a local optimal solution
8
Under review as a conference paper at ICLR 2022
rather than the global optimum. This phenomenon occurs frequently in NLS which ends up with performance
degradation. Thus, in Table 2, when learning with noisy labels, we report the better performance of GLS
between direct training and loading the same warm-up model. And we observe that the performance of NLS
is more competitive than PLS when learning with clean data. Clearly, NLS outperforms PLS in CIFAR-10
and CIFAR-100 under various synthetic noise settings. The gap is larger when the noise rates are high.
Table 2: Test accuracies (meanstd) of GLS on synthetic noisy CIFAR datasets. Best two smooth rates for
each synthetic noise setting are highlighted for each (green: NLS; red: PLS).
Smooth Rate		ε = 0.0	CIFAR-10 Symmetric ε= 0.2	ε=0.4		ε = 0.6	CIFAR-10 Asymmetric ε = 0.2	ε = 0.3		CIFAR-100 Symmetric ε = 0.4	ε = 0.6	
r	= 0.8	92.91 ±0.06 二	88.88±1.61	81.48±2.91	73.16±0.16=	90.45±0.06	87.83±0.13	54.04±0.93	39.50±0.18
r	= 0.6	92.33 ±0.09	87.50±1.31	82.11±0.86	73.59±0.15	90.41±0.09	87.83±0.13	52.72±0.15	40.49±0.07
r	= 0.4	93.05 ±0.04	87.13±0.07	81.50±1.42	74.21±0.19	90.49±0.10	87.90±0.13	54.26±0.07	41.57±0.05
r	= 0.0	91.44±0.16	85.08±0.86	80.42±2.29	75.34±0.13	88.32±0.24	86.27±0.32	48.03±0.29	38.11±0.14
r=	-0.4	93.55±0.06	87.55±0.08	81.58±0.19	75.95±0.13	87.27±1.83	88.33±0.06	56.87±0.08	43.70±0.16
r=	-0.8	92.74±0.05	88.46±0.11	81.56±0.15	76.15±0.14	86.40±1.32	87.96±0.43	57.35±0.08	44.10±0.06
r=	-1.0	92.58±0.08	88.58±0.08	81.95±0.10	76.20±0.10	88.47±0.15	87.50±0.73	57.44±0.09	43.85±0.19
r=	-2.0	93.30±0.03	88.78±0.09	83.64±0.15	76.11 ±0.07	88.66±0.17	87.27±0.70	58.10±0.08	44.88 ±0.11
r=	-4.0	93.13±0.04	88.90±0.07	84.34±0.13	77.22±0.09	89.56±0.17	87.29±0.59	58.35±0.09	46.38±0.05
r=	-6.0	93.14±0.08一	88.94±0.11一	84.52±0.13一	77.42±0.16一	89.70±0.24一	87.57±0.42	57.73±0.10-	46.46 ±0.09一
6.2 Comparisons between GLS and more robust methods
In Table 3, we compare GLS with several robust methods in synthetic noisy CIFAR datasets. We emphasize
that we are not proposing a new method to compete with state-of-the-art methods. Instead, we hope to
help readers understand how the generalized label smoothing fare when label noise presents. Clearly, GLS
(especially NLS) can definitely be viewed as a competitive and efficient robust loss function which outperforms
Cross Entropy, Bootsrap (Reed et al., 2014), SCE (Wang et al., 2019), APL (Ma et al., 2020) and Forward
correction (Patrini et al., 2017) in most settings.
Table 3: Performance comparisons on synthetic noisy CIFAR datasets: we adopt the same model architecture
for all methods (ResNet 34 (He et al., 2016)), best achieved test accuracy is reported.
eto	CIFAR-10, Symmetric			CIFAR-10, Asymmetric		CIFAR-100, Symmetric	
	ε = 0.2	ε = 0.4	ε=0.6	ε = 0.2	ε=0.3	ε = 0.4	ε=0.6
Cross Entropy	86.45	82.72	74.04	88.59	86.14	48.20	38.27
Bootstrap (Reed et al., 2014)	86.06	81.65	75.26	87.69	85.51	47.28	35.81
Forward correction (Patrini et al., 2017)	84.85	84.98	73.97	89.42	88.25	53.04	41.59
SCE (Wang et al., 2019)	89.39	80.31	75.28	88.07	85.93	49.34	38.87
APL (Ma et al., 2020)	88.42	81.27	76.62	88.75	87.41	51.63	42.31
Peer Loss (Liu & Guo, 2020)	90.21	86.40	79.64	91.38	89.65	62.16	53.72
ELR (Liu et al., 2020)	92.57	91.32	88.86	93.48	92.21	68.03	60.49
AUM (Pleiss et al., 2020)	91.52	87.85	81.71	92.17	90.63	59.29	44.05
Positive Label Smoothing (PLS)	90.24	83.78	75.01	90.61	88.04	55.17	41.63
Negative Label Smoothing (NLS)	89.05	84.85	77.82	90.02	88.42	58.47	46.58
Additional results More extensive empirical results are included in Appendix: empirical validation of
main theorems (Appendix B); practical considerations of GLS (Appendix C); experiment results on additional
datasets (Appendix D); bias and variance of the generalization error on the clean data (Appendix E).
Conclusion In this paper, we provide understandings for a generalized notion of label smoothing where
the label smoothing rate can go negative. We show that learning with negatively smoothed labels explicitly
improves the confidence of model prediction. This key property acts as a significant role when the confidence
of model prediction drops. We make connections between negative label smoothing and existing learning
with noisy label solutions. In contrast to existing works that promote the use of positive label smoothing, we
show both theoretically and empirically the advantage of a negative smooth rate when the label noise rate
increases. Our observations provide new understanding for the effects of label smoothing, especially when
the training labels are imperfect.
9
Under review as a conference paper at ICLR 2022
References
Ehsan Amid, Manfred KK Warmuth, Rohan Anil, and Tomer Koren. Robust bi-tempered logistic loss based
on bregman divergences. In Advances in Neural Information Processing Systems, pp. 14987-14996, 2019.
Antonin Berthon, Bo Han, Gang Niu, Tongliang Liu, and Masashi Sugiyama. Confidence scores make
instance-dependent label-noise learning possible. In International Conference on Machine Learning, pp.
825-836. PMLR, 2021.
Blair Chen, Liu Ziyin, Zihao Wang, and Paul Pu Liang. An investigation of how label smoothing affects
generalization. arXiv preprint arXiv:2010.12648, 2020.
Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependent
label noise: A sample sieve approach. In International Conference on Learning Representations, 2020.
Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in sequence to
sequence models. Proc. Interspeech 2017, pp. 523-527, 2017.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.
uci.edu/ml.
Xin Geng. Label distribution learning. IEEE Transactions on Knowledge and Data Engineering, 28(7):
1734-1748, 2016.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama.
Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in neural
information processing systems, pp. 8527-8537, 2018.
Ramaswamy Harish, Clayton Scott, and Ambuj Tewari. Mixture proportion estimation via kernel embeddings
of distributions. In International conference on machine learning, pp. 2052-2060, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama. Learning from complementary labels.
In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
5644-5654, 2017.
Takashi Ishida, Gang Niu, Aditya Menon, and Masashi Sugiyama. Complementary-label learning for arbitrary
losses and models. In International Conference on Machine Learning, pp. 2971-2980. PMLR, 2019.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven
curriculum for very deep neural networks on corrupted labels. In International Conference on Machine
Learning, pp. 2304-2313. PMLR, 2018.
Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 101-110, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical
report, Citeseer, 2009.
10
Under review as a conference paper at ICLR 2022
Weizhi Li, Gautam Dasarathy, and Visar Berisha. Regularization via structural label smoothing. In Interna-
tional Conference OnArtificial Intelligence and Statistics, pp.1453-1463. PMLR, 2020.
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regular-
ization prevents memorization of noisy labels. Advances in Neural Information Processing Systems, 33,
2020.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE Transac-
tions on pattern analysis and machine intelligence, 38(3):447-461, 2015.
Yang Liu. The importance of understanding instance-level noisy labels. arXiv preprint arXiv:2102.05336,
2021.
Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In
International Conference on Machine Learning, pp. 6226-6236. PMLR, 2020.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and Sanjiv Kumar. Does label smoothing mitigate
label noise? In International Conference on Machine Learning, pp. 6448-6458. PMLR, 2020.
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized
loss functions for deep learning with noisy labels. In International Conference on Machine Learning, pp.
6543-6553. PMLR, 2020.
Aditya Menon, Brendan Van Rooyen, Cheng Soon Ong, and Bob Williamson. Learning from corrupted binary
labels via class-probability estimation. In International Conference on Machine Learning, pp. 125-134,
2015.
Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? In Proceedings of
the 33rd International Conference on Neural Information Processing Systems, pp. 4696-4705, 2019.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy
labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Curtis G Northcutt, Lu Jiang, and Isaac L Chuang. Confident learning: Estimating uncertainty in dataset
labels. Journal of Artificial Intelligence Research, 2021.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep
neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural
networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.
Geoff Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying mislabeled data using
the area under the margin ranking. arXiv preprint arXiv:2001.10528, 2020.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich.
Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
Clayton Scott, Gilles Blanchard, Gregory Handy, Sara Pozzi, and Marek Flaska. Classification with asymmet-
ric label noise: Consistency and maximal denoising. In COLT, pp. 489-511, 2013.
11
Under review as a conference paper at ICLR 2022
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2818-2826, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 6000-6010, 2017.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for
robust learning with noisy labels. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 322-330, 2019.
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint training
method with co-regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 13726-13735, 2020.
Jiaheng Wei and Yang Liu. When optimizing f -divergence is robust with label noise. In International
Conference on Learning Representations, 2020.
Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, and Rong Jin. Towards understanding label smoothing. arXiv preprint
arXiv:2006.11653, 2020.
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: An information-theoretic noise-robust loss
function. NeurIPS, arXiv:1909.03388, 2019.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-off for
generalization of neural networks. In International Conference on Machine Learning, pp. 10767-10777.
PMLR, 2020.
Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit memorization
effect in learning with noisy labels. In International Conference on Machine Learning, pp. 10789-10798.
PMLR, 2020a.
Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi Sugiyama. Dual t:
Reducing estimation error for transition matrix in label-noise learning. arXiv preprint arXiv:2006.07805,
2020b.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement
help generalization against label corruption? In International Conference on Machine Learning, pp.
7164-7173. PMLR, 2019.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label
smoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 3903-3911, 2020.
Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang. Re-
thinking soft labels for knowledge distillation: A bias-variance tradeoff perspective. arXiv preprint
arXiv:2102.00650, 2021.
Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when learning
with noisy labels. arXiv preprint arXiv:2102.05291, 2021.
12
Under review as a conference paper at ICLR 2022
Appendix
The Appendix is organized as follows.
•	Section A presents the full version of related works.
•	Section B includes empirical validations of theoretical conclusions in Section 3 and Theorem 5.
•	Section C discusses practical considerations and multi-class extensions of GLS.
•	Section D shows additional experiments on synthetic dataset and UCI datasets.
•	Section E illustrates the bias and variance trade-off when learning with GLS from clean data.
•	Section F includes omitted proofs for theoretical conclusions in the main paper.
A	Full version of related works
Our work supplements to two lines of related works.
Learning with noisy labels Annotated labels from human labelers usually consists of an non-negligible
amount of mis-labeled data samples. Making deep neural nets perform robust training on “noisily" labeled
datasets remains a challenge. Classical approaches of learning with noisy labels assume the noisy labels are
independent to features. They firstly estimate the noise transition matrix (Liu & Tao, 2015; Menon et al.,
2015; Harish et al., 2016; Patrini et al., 2017), then proceed with a loss correction (Natarajan et al., 2013;
Patrini et al., 2017; Liu & Tao, 2015) to mitigate label noise. Recent works propose robust loss functions
(Xu et al., 2019; Kim et al., 2019; Liu & Guo, 2020; Wei & Liu, 2020) to train deep neural nets directly
without the knowledge of noise rates, or design a pipeline which dynamically select and train on “clean"
samples with small loss (Jiang et al., 2018; Han et al., 2018; Yu et al., 2019; Yao et al., 2020a). More recently,
several approaches target at addressing more challenging noise settings, such as instance-dependent label
noise (Cheng et al., 2020; Berthon et al., 2021).
Understanding the effect of label smoothing Learning with one-hot labels is prone to over-fitting, soft
label learning then naturally draws attentions of machine learning researchers. Successful applications of
soft label learning include the label distribution learning (Geng, 2016) which provides an instance with
description degrees of all the labels. Label smoothing (LS) (Szegedy et al., 2016) is another arising learning
paradigm that uses positively weighted average of both the hard training labels and uniformly distributed soft
labels. Empirical studies have demonstrated the effectiveness of LS in improving the model performance
(Pereyra et al., 2017; Szegedy et al., 2016; Vaswani et al., 2017; Chorowski & Jaitly, 2017) and model
calibration (Muller et al., 2019). However, knowledge distilling a teacher network (trained on smoothed
labels) into a student network is much less effective (Muller et al., 2019). Later, generalization effects of
more advanced forms of label smoothing was studied, such as structural label smoothing (Li et al., 2020)
and non-uniform label smoothing (Chen et al., 2020). More recently, it was shown that an appropriate label
smoothing regularizer with reduced label variance boosts the convergence (Xu et al., 2020). When label noise
presents, (Liu, 2021) gives theoretical justifications for the memorizing effects of label smoothing. And the
effectiveness of label smoothing in mitigating label noise is investigated in (Lukasik et al., 2020).
B	Empirical validations of main theorems
In this section, we empirically validate our main theoretical conclusions, i.e, the connection between GLS
and popular methods, and the comparison between clean empirical risks and noisy ones.
13
Under review as a conference paper at ICLR 2022
B.1	Connection to other robust methods
We firstly compare GLS with backward correction (Natarajan et al., 2013), forward correction (Patrini
et al., 2017) and peer loss (Liu & Guo, 2020) on CIFAR-10 dataset. To approximate the performance of
backward/forward Loss Correction, We adopt GLS with smooth rate(^). As for the approximation of peer
loss, we choose `(f (X), Ye) - `(f (X), Ye GLS,r=0.5) which is equivalent to GLS when r → -∞. Experiment
results in Table 4 on CIFAR-10 under symmetric noise settings demonstrate that the equivalent forms of GLS
are robust to label noise.
Table 4: Comparison of test accuracies on CIFAR-10 under symmetric label noise.
Method	CIFAR-10, Symmetric		
	ε = 0.2	ε = 0.4	ε = 0.6
Backward T (Patrini etal.,2017)	84.79	83.40	71.52
Forward T (Patrini et al., 2017)	84.85	83.98	73.97
GLS form	87.33	81.73	75.80
-Peer Loss (LiU & Guo, 2020)-	90.21	86.40	79.64
	GLS form		88.98	85.05	76.66
Explanation of the performance gap In practice, we adopt the same hyper-parameter setting as used
for all other smooth rates for GLS form. Loss corrections will firstly warm-up with the cross-entropy
loss, estimate the noise transition matrix with this pre-trained model, and then proceed to train with the
backward/forward corrected loss. Peer loss functions adopt a dynamical adjustment for learning rate. The
warming up, estimation error of noise transition matrix as well as the special hyper-parameter settings explain
performance gaps.
B.2	Clean empirical risk v.s. noisy empirical risk
Now we empirically verify Theorem 8, which relates the risk of GLS in the noisy setting to the clean ones.
Assume the the noise label is generated through the symmetric noise transition matrix. We restate the
connection between two risks as below:
E(X,Y)~D ['(f(X),YGLS,r)]
'-------------------------}
E(X,Y)〜D h'(f(X),YF + λ1 ∙ E(X,Y)〜D['(f(X), 1 - Y)-'(f (X),Y)]
Noisy Risk
(Clean) True Risk
Gap of Risk
In practice, obtaining the exact value of r* is not trivial. Finding an accurate r for GLS to make λ1 be
exactly 0 is pretty hard. Note that when M-Inc1 is equipped with the Cross-entropy loss and NLS encourages
confident predictions, one mistake could make the value of M-Inc1 has a huge difference. To be more specific,
suppose the label of an image is a “cat”, if the NLS-trained model makes a mistake on this image, it is very
likely to predict this image “cat” with a probability that p → 0 since NLS may push the model become
overly-confident. Then log(p) can be extremely small. Thus, the empirical risk (evaluated on the clean data)
for NLS will change much more significantly than PLS ones.
We use a UCI dataset (Waveform) for illustration. The value of r* is approximately 0, and the estimated
(clean) true risk in above is 0.1798. When the noise rates are 0.1, 0.2, 0.3, 0.4, the optimal smooth rate should
be -0.25, -0.67, -1.5, -4 according to Eqn. (7). The estimated noisy risk of GLS on these noise settings
can be summarized in Table 1. Clearly, when e = 0.1, r = -0.25 is closest to the estimated clean true risk
(also returns the best test accuracy among these smooth rates). Similarly observations reach to all other e.
Although the risks evaluated on noisy settings with the corresponding ropt can not be exactly the same as the
true risk (evaluates on the clean data with r = 0.0), ropt already reaches the smallest one among these smooth
rates and the non-zero Gap of Risk is best explained by the precision of r* as well as the estimation error of
risks.
14
Under review as a conference paper at ICLR 2022
Table 5: The difference between the empirical true risk of Y * on the clean data and empirical risk GLS on
noisy labels (UCI-WavefOrm dataset): r*, empirical true risk, and empirical noisy risks under various noise
levels are highlighted in purple.
Smooth rate		Risk (clean)	Risk (e = 0.1)	Risk (e = 0.2)	Risk (e = 0.3)	Risk (e = 0.4)
r	=o	0.6773	0.6831	0.6873	0.6899	0.6923
r	=06-	0.6295	0.6521	0.6689	0.6833	0.6905
r	=04-	0.5437	0.5994	0.6408	0.6718	0.6873
r	=02	0.4134	0.5212	0.5956	0.6550	0.6828
r^	=0.0	0.1798	0.4057	0.5399	0.6314	0.6758
r=	-0.25	--36.8095-	0.1983	0.4381	0.5957	0.6685
r=	-0.67	-333.1283	-28.3508	0.2167	0.5132	0.6503
r=	-1.5	-97.4378	--61892.8047-	-94.9509	0.1911	0.6003
r=	-4.0	-270.6456	-179.0037	-93.1257	-156.4539	0.1165
C	Practical consideration of GLS
In the main paper, we theoretically show when we should adopt NLS and PLS. In this section, we discuss
more practical considerations of GLS, including the optimal smoothing parameter, how to reduce the impacts
of bias terms, and multi-class extensions.
C.1 The optimal smoothing parameter
In practice, we don’t have access to noise rates ei. Our work does not intend to particularly focus on the
noise rate estimation. For readers interested in the noise rate estimation, please refer to (Liu & Tao, 2015;
Menon et al., 2015; Harish et al., 2016; Patrini et al., 2017; Yao et al., 2020b; Zhu et al., 2021). To estimate
ropt = rι-2e, one can simply assume r* → 0. And the noise rate e is estimable by a large family of noise
estimation methods mentioned above. Our practical observations show that NLS with a CE warm-up is not
sensitive to the negative smooth rate, for example, on CIFAR-10 and CIFAR-100 synthetic noisy datasets,
r < -1.0 frequently achieves best results (see Table 2 in the main paper). Our current contribution focuses
on understanding the generalized label smoothing, and we prefer leaving the task of identifying the optimal
smooth rate to future works.
C.2 Making GLS more robust to label noise
There is a line of related works targeting at distinguishing clean labels from the noisy labels. Current literature
in selecting clean samples from noisily labeled dataset is based on the empirical evidence that samples with
noisy/wrong labels have a larger loss than clean ones. For interested readers, please refer to (Han et al., 2018;
Jiang et al., 2018; Yu et al., 2019; Yao et al., 2020a; Wei et al., 2020; Northcutt et al., 2021). Compared with
the risk minimization over the clean data distribution (X, Y) ^ D, learning directly with GLS on the noisy
，__ ~ .	O" . .	，	.	.	一	. . ,	__ .	. , _ __ . r
distribution (X, Y)〜D	will result	in an extra term (eι - e0)	∙ (1 - r)	∙ E(χ,γ =i)〜D ['(f (X), 0) - '(f (X),	1)]
compared to the clean scenario. Empirically, we can estimate the bias term, perform a bias correction by
subtracting the estimated bias term from the objective function in Eqn. (3).
Suppose we have access to a clean distribution Dclean which consists of selected clean samples. Denote the
estimated noise rates as ^i, when e∆ = 0, in order to make GLS be more robust to label noise and fit on the
optimal distribution Y*, we improve GLS by performing a model confidence correction on the dominating
class through:
15
Under review as a conference paper at ICLR 2022
GLS-C:
min E(X,Y)〜D	['(f(x), YGLS,r)]
-(^1 - eo) ∙ (1 - r) ∙ E(X,Y =1)〜DClean ['(f(X), 0) - '(f(X),1)]
'-------------------------------}
confidence correction
C.3 Multi-class extension
As an extension to the binary classification task, we next show how GLS extends to the multi-class setting
under two broad families of noise transition model. When learning with multi-class classification tasks, we
assume Assumption 1 holds in the multi-class setting. And for Y, Y ∈ [K], we extend the definition of model
confidence to multi-class classification tasks as:
Definition 2. Model confidence of sample x (K-class classification). Given a model f, a sample x with its
target label y ∈ [K], the model confidence score of f w.r.t. sample x is defined as
MC(f ； x, y) = f (X) y- ^1-j- X f (X)i
y K-1
i6=y
Sparse noise transition matrix Sparse noise model (Wei & Liu, 2020) assumes K is an even number.
For c ∈ [K/2], ic < jc, sparse noise model specifies K/2 disjoint pairs of classes (ic, jc) to simulate
the scenario where particular pairs of classes are ambiguity and misleading for human annotators. The
off-diagonal element of T reads Tic,jc = e0, Tjc,ic = e1. Suppose e0 + e1 < 1, the diagonal entries become
Tic,ic = 1 - e1, Tjc,jc = 1 - e0. Clearly, our conclusions in Section 5 extends directly to the sparse noise
transition matrix by simply splitting the K-class classification task into 与 disjoint binary ones.
Symmetric noise transition matrix Symmetric noise model (Kim et al., 2019) is a widely accepted
synthetic noise model in the literature of learning with noisy labels. The symmetric noise model generates
the noisy labels by randomly flipping the clean label to the other possible classes with probability . ∀i 6= j ,
Ti,j = /(K - 1), and the diagonal entry Ti,i = 1 - . Define the optimal r for GLS in the multi-class setting
as ropt := (KKPI)--K, Theorem 6 can be extended to the multi-class setting as:
Theorem 7. Under Assumption 1, suppose the symmetric noise rate is not too large, i.e, e < K-I, GLS with
smooth rate r = ropt yields fD.
•	When error rate e <	(K-1"	,	r =	r0pt	> 0	(PLS);
•	When error rate e =	(K-K>r	,	r =	0 (Vanilla Loss);
•	When error rate e >	(K-1"	,	r =	ropt	< 0	(NLS).
D Additional Experiment Results and Details
In this section, we include more experiment results, observations and details for learning with GLS.
D.1 Experiment details on CIFAR-10, CIFAR-100
We firstly introduce experiment details on CIFAR-10 dataset adopted in our experiment designs.
16
Under review as a conference paper at ICLR 2022
Training settings of clean CIFAR-10 dataset (Krizhevsky et al., 2009) We adopted ResNet34 (He et al.,
2016), trained for 200 epochs with batch-size 128, SGD (Robbins & Monro, 1951) optimizer with Nesterov
momentum of 0.9 and weight decay 1e-4. The learning rate of first 100 epochs is 0.1. Then it multiples with
0.1 for every 50 epochs.
Generating noise labels on CIFAR datasets We adopt symmetric noise model which generates noisy
labels by randomly flipping the clean label to the other possible classes with probability . And we set
= 0.2, 0.4, 0.6 for CIFAR-10, = 0.4, 0.6 for CIFAR-100. We also make use of asymmetric noise model.
The asymmetric noise is generated by flipping the true label to the next class with probability . We set
= 0.2, 0.3 for CIFAR-10.
Training settings of synthetic noisy CIFAR datasets The generation of symmetric noisy dataset is
adopted from (Cheng et al., 2020). The symmetric noise rates are [0.2, 0.4, 0.6]. We choose two meth-
ods to train GLS.
•	Direct training: this setting is the same as training on clean CIFAR-10 dataset.
•	Warm-up: in this case, we firstly train a ResNet34 model with Cross-Entropy loss for 120 epochs. For
this warm-up, the only difference in hyper-parameter setting is the learning rate, where the initial learning
rate is 0.1 and it multiplies 0.1 for every 40 epochs. After the warm-up, GLS loads the same pre-trained
model and trains for 100 epochs with learning rate 1e-6.
D.2 Why NLS is overlooked?
When learning from a relative large scale dataset, NLS tends to push the model become overly confident
early in the training. The poor performances of NLS (direct-train) in Table 6 explain why NLS is neglected.
When there is no warm-up, training NLS directly without warming up will reach a 88% - 92% test accuracy
on the clean data. The performance will degrade much more significantly than PLS when the noise level
is high or |r| is large. In Table 6, we provide the comparisons between direct-train and warm-up in several
settings. The improvement bring by a warm-up procedure becomes much more significantly in the high noise
regime. NLS makes the classifier be overly confident at the early training which results in converging to a
bad local optimum (without CE warm-up, NLS frequently results in a worse performance in CIFAR-10 and
CIFAR-100). Since the model will usually fit on the clean data first, then over-fits on the noisy ones (Liu
et al., 2020), a large number of approaches (such as Loss corrections (Patrini et al., 2017), Peer Loss (Liu &
Guo, 2020), etc) adopt a CE warm-up firstly. Note that there is no difference in the computing costs between
NLS (with CE warmup) and CE loss, proceeding with NLS to enhance the model confidence makes NLS
much more competitive in the high noise regime, also gives practical insights on how to make NLS work
better when learning with clean data.
Table 6: Test accuracies of GLS on assymetric noisy CIFAR-10 and symmetric CIFAR-100 (left/right denotes
direct train / warm-up).
Smooth Rate		Cifar_10 Asymmetric ε = 0.2	ε = 0.3		CIFAR-100 Symmetric ε = 0.4	ε = 0.6	
r	= 0.8	87.89/90.51	86.38/87.97	54.78/51.27	40.21 / 39.80
r	= 0.6	89.14/90.55	85.97/88.01	52.83/52.88	39.64 / 40.57
r	= 0.4	88.23/ 90.61	86.95 /88.04	51.40/54.36	38.29 / 41.63
r=	-0.4	19.71/89.60	21.86/ 88.42	40.30/56.97	31.35 / 43.91
r=	-0.8	--/ 89.02-	--/88.28-	22.63/57.45	26.75 / 44.19
r=	-1.0	--/ 88.68-	--/88.29-	--/57.53-	- / 44.59
r=	-2.0	--/88.86-	--/88.13-	--/58.21-	- / 45.47
r=	-4.0	--/ 89.80-	--/88.20-	--/ 58.47-	- / 46.86
r=	-6.0	-/90.02 —	-/88.18 —	-/57.87 —	- / 47.18
17
Under review as a conference paper at ICLR 2022
D.3 Experiment details on synthetic datasets and UCI
We introduce experiment details on synthetic datasets and UCI datasets adopted in our experiment designs.
Generation of synthetic dataset In the synthetic (Type 1) dataset, we generate 500 points for both classes.
Class +1 distributes inside the circle with radius 0.25. Class -1 generates by randomly sampling 500 data
points in the annulus with inner radius 0.28 and outer radius 0.45. As for synthetic (Type 2) dataset, we
uniformly assign labels for 50% samples in the annulus (with inner radius 0.22, outer radius 0.31) based on
Type 1 dataset.
Generating noise labels on synthetic datasets and UCI datasets Note that these datasets are all binary
classification datasets, each label in the training and validation set is flipped to the other class with probability
e, and we set e = 0.1, 0.4 for synthetic Type 1 dataset, e = 0.1, 0.3 for synthetic Type 2 dataset.
Training settings of synthetic datasets For both types of synthetic datasets, we adopted a three-layer
ReLU Multi-Layer Perceptron (MLP), trained for 200 epochs with batch-size 128 and Adam (Kingma & Ba,
2014) optimizer. The initial learning rate is 0.1, and it multiplies 0.1 for every 40 epochs.
Training settings of UCI datasets Dua & Graff (2017) We adopted (Liu & Guo, 2020) a two-layer ReLU
Multi-Layer Perceptron (MLP) for classification tasks on multiple UCI datasets, trained for 1000 episodes
with batch-size 64 and Adam (Kingma & Ba, 2014) optimizer. We report the best performance for each
smooth rate under a set of learning rate settings, [0.0007, 0.001, 0.005, 0.01, 0.05].
D.4 Additional experiment on r* and rɑpɪ
r* and ropt on synthetic dataset We generate 2D (binary) synthetic dataset by randomly sampling two
circularly distributed classes. The inner annulus indicates one class (blue), while the outer annulus denotes
the other class (red). Clearly, the generated synthetic dataset is well-separable (Type 1) and we hold 20%
data samples for performance comparison. The noise transition matrix takes a symmetric form with noise
rate ei for both classes. To simulate the scenario where the clean data may not be perfectly separated due
to a non-negligible amount of uncertainty samples clustering at the decision boundary, we flip the label
of 50% samples near the intersection of two annulus to the other class (Type 2). As specified in Table 7,
r* = [0.1,0.4] for TyPe 1 data and r* = [0.0,0.2] for TyPe 2 data. With the presence of label noise, the
distribution of ropt shifts from non-negative ones to negative values. Even though NLS fails to outperform
pLS on clean data, we observe that NLS is less sensitive to noisy labels. Data with high level noise rates
clearly favor NLS with a low smooth rate!
Table 7: Test accuracies of GLS on clean and noisy synthetic data. We report best test accuracy for each
method. ropt and the corresponding test accuracy are highlighted (green: NLS; red: pLS).
Method	Synthetic data (Type 1) ei = 0	ei = 0.2	ei = 0.4			Synthetic data (Type 2) ei = 0 ei = 0.2	ei = 0.4		
PLS 一	0.896	0.878	0786-	-0.894-	0.848	0.842
Vanilla Loss	-0.889^^	0.882	0.806-	-0.894-	~~0.875	0.868
NLS 一	0.893	0.885	0.825	.	0.883	0.884	0.875
ropt =	[0.1,0.4]	-0.2	-0.4	.	[0, 0.2]	-0.3	-0.5
r* and ropt on more UCI datasets We further test the performance of generalized label smoothing on 7
more UCi datasets (Heart, Breast 1, Breast 2, Diabetes, German, image and Waveform). our observation
remains unchanged: there exists a general trend that with the increasing of noise rates, NLS becomes much
more competitive than pLS. Here, we attach the results of 4 additional UCi datasets for illustration.
The noisy labels are generated by a symmetric noise transition matrix with noise rate ei = [0.1, 0.2, 0.3, 0.4].
As highlighted in Table 8, ropt appears with positive values when the data is clean (same as r*) or of a low
18
Under review as a conference paper at ICLR 2022
Table 8: Test accuracies of GLS on clean and noisy UCI datasets (Image, Waveform, Heart, Banana) with
best two smooth rates (green: NLS; red: PLS).
Smooth Rate	ei = 0	ei = 0.1	Image ei = 0.2	ei = 0.3	ei = 0.4	ei = 0	ei = 0.1	Waveform ei = 0.2	ei = 0.3	ei = 0.4
r = 0.8	0.993-	~~0.983	-~0.973-	-0.946-	-~0175	-0.939-	~~0.935~~	-~0.931	0.927	0.885
r = 0.6	0.993-	-0.987-	~~0.970	-0.939-	-~0169	-0.943-	-0.943-	-0.943	0.929	0.901
r = 0.4	0.997-	~~0.980	0.973	-^0.939	0165	-0.941-	~~0.937	-~0.943	0.931	0.905
r = 0.2	-0.993-	~~0.993	-~0.966~~	-^0.936	-~0175	-0.941-	0.935	-~0.933	-^0931	0.913
r = 0.0	-0.990-	~~0.976	0.963	0.929	-~0165	-0.945-	~~0.935	0.937 I	-0933-	0.911
r = -0.2	-0.912-	0.96	0.953	0.919	-~0172	-0.937-	-0.939-	~~0.939	-0933-	~~0.907
r = -0.4	-0.882-	0.923	0.953	0.936	-~0172	-0.925-	~~0.937~~	-~0.939	-^0933	~~0.917
r = -0.8	-0142-	0.882	0.926	0.933	-~0172	-0.921-	0.925	-~0.939	-^0931	~~0.923
r = -1.0	-0.832-	0.869	0.909	0.929	-~0182	-0.921-	0.923	0.933	0.929	~~0.907
r= -2.0	0818	0.815	0.889	0.909	0.906	0911	0.913	0.921	0.927	0.911
Smooth Rate	ei = 0	ei = 0.1	Heart ei = 0.2	ei = 0.3	ei = 0.4	ei = 0	ei = 0.1	Banana ei = 0.2	ei = 0.3	ei = 0.4
r = 0.8	-0.885-	0.853	0.836	0.820	-~0738	-0.896-	-0.893-	-0.876	0.847	0.790
r = 0.6	-0.902-	0.836	0.820	0.836	-~0738	-0.903-	~~0.881	0.876	-^0155	0.811
r = 0.4	-0.885-	0.853	0.836	0.820	0771	-0.900-	~~0.887	0.874	-^0159	0.807
r = 0.2	-0.902-	0.853	0.820	0.803	-~0754	-0.896-	~~0.894	-~0.876	-^0156	0.810
r = 0.0	-0.902-	0.853	0.820	-^0.820	-~0771	-0.897-	~~0.881	0.871	0.849	~~0.833
r = -0.4	-0.869-	0.836	0.803	-0.853-	-~0754	-0.847-	0.874	-~0.859	-^0153~~	-0.840-
r = -0.6	-0.869-	~~0.836	-~0.820	-0.853-	-~0721	-0.845-	0.864	-~0.861	I	-0159-	-0.837-
r = -1.0	-0.885-	~~0.869	-~0.803	0.853	0754	-0.796-	0.812	0.852	-^0154	0.811
r= -2.0	-0.885-	~~0.869	-~0.820	-^0.853	-~0787	-0.759-	0.764	0.819	0.852	0.819
r = -4.0	-0.885-	-0.869-	0.853	0.885	-0120-	-0.718-	0.723	0.738	0.787	0.813
r = -8.0	0.869	0.869	0.885	0.853	0.853	0.703	0.700	0.699	0.735	0.735
noise rate. With the increasing of noise rates, NLS becomes more competitive than PLS. We color-code
different noise regimes where either PLS (red-ish) or NLS (green-ish) outperforms the other. Clearly, there
is a separation of the favored smoothing rate for different noise scenarios (upper left & low noise for PLS,
bottom right & high noise for NLS).
D.5 Additional experiment results on model confidence
NLS improves model confidence on Synthetic Type 2 dataset In this case, the clean data that are close
to decision boundary distributes randomly. In Figure 5-6, the colored bands depict the different levels of
prediction probabilities. When the smooth rate increases from negative to positive, more samples fall in
the orange and light blue band which indicates uncertain predictions. When the smooth rate increases from
negative to positive, learning with GLS will result in more uncertain predictions. With the increasing of noise
rates (ei = 0 → 0.4), GLS with a fixed smooth rate becomes less confident on its predictions. Thus, a smaller
smooth rate is required when the noise rate increases.
Clean, r=-0.17 lest Acc=O.883	Clean, r=0.0f Test Acc=O.894	Clean, r=0.2, lest Acc=O.894
Figure 5: Model confidence visualization of GLS on synthetic data (Type 2) With the clean data. r* ∈ [0, 0.2].
(left: NLS; middle: Vanilla Loss; right: PLS).
Model confidence of GLS on CIFAR-10 test dataset When GLS trained on symmetric 0.2 noisy CIFAR-
10 training dataset (see Figure 7), With the decreasing of smooth rates (from right to left), the model
19
Under review as a conference paper at ICLR 2022
Figure 6: Model confidence visualization of GLS on synthetic data (Type 2) with noise rate ei = 0.3.
ropt = -0.5. (left: NLS; middle: Vanilla Loss; right: PLS).
confidence on correct predictions gradually approach to its maximum, while for wrong predictions, the model
confidence converges to its minimum value. NLS makes the model prediction become over-confident on
correct predictions and in-confident on wrong predictions.
Figure 7: Model confidence distribution of correct and wrong predictions on CIFAR-10 test data. (From left
to right: NLS (r = -0.8, -0.4), Vanilla Loss, PLS (r = 0.4), trained on symmetric 0.2 noisy CIFAR-10
dataset).
IUnoɔ
D.6 Effect of GLS on pre-logits
We visualise the pre-logits of a ReSNet-34 for three classes on CIFAR-10. We adopt the method from (Muller
et al., 2019) which illustrates how representations differ between penultimate layers of networks trained with
different smooth rates in GLS. In Figure 8, NLS makes the model f be confident on her predictions and the
distances between three clusters are clearly larger than those appeared in Vanilla Loss and PLS.
Figure 8: Effect of GLS on pre-logits (left: NLS; middle: Vanilla Loss; right: PLS; trained with symmetric
0.2 noisy CIFAR-10 training dataset).
20
Under review as a conference paper at ICLR 2022
E Bias and variance trade- off of GLS
Denote fH, fS as pre-trained models on the training dataset D w.r.t. hard labels and soft labels, respectively.
The vector form of the prediction w.r.t. sample X given by fH and fs are fH(x; D) and fs(x; D). For the ease
of presentation, we relate notations with subscript H/S to hard/soft labels without further explanation. Given
the sample x and the one-hot label y, we denote the averaged model prediction by:
fH(x； D) := ɪ exp [Ed log%(x; D))],	%(x； D) ：= J exp [Ed logfs(x; D))]
ZH	ZS
where ZH , ZS are normalization constants. The bias of model prediction is defined as the KL divergence DKL
between target distribution (one-hot encoded vector form) y and the averaged model prediction.
BiasH := Ex,y [ylog IHyDy],
BiasS := Eχ,y [y lθg fɪ]
While the variance of model prediction measures the expectation of KL divergence between the averaged
model prediction and model prediction over D:
r	大一 f W m、r
VarH := ED Eχ,y [珀也 D) log (fH( ;	))]
fH(x; D)
r	KAw m、r
VarS := ED Eχ∕fs(χ; D)log (fS( ;	))]
fS(x; D)
Empirical observation from (Zhou et al., 2021) shows that the variance brought by learning with positive soft
labels given by a teacher’s model (Hinton et al., 2015) is less than the direct training w.r.t hard labels. As an
extension, we are interested in how GLS interferes with the bias and variance of model prediction.
Bias and variance of GLS on clean dataset We introduce our empirical observation regarding the role of
GLS in bias and variance trade-off in Figure 9. We select nine smooth rates of GLS for illustration. Each
smooth rate setting of GLS trains on the CIFAR-10 dataset for 5 times with different data augmentations.
To estimate the variance and bias of pre-trained models, we adopt the implementation in (Yang et al., 2020).
Empirical results show that learning directly with a larger positive smooth rate typically results in lower
variance and higher bias. In Figure 9, we can observe almost constant bias values and very low variance for
NLS. This is best explained by the warm-up of pre-trained models and the fact that NLS pushes the classifier
to give confident predictions. As for PLS, with the increase of smooth rate, the overall bias has an increasing
tendency while the variance has the decreasing pattern. Especially when the smooth rate approaches to 1, i.e.,
r = 0.9, the variance is close to 0.
Figure 9: Bias and variance of pre-trained GLS models on clean CIFAR-10 test dataset.
21
Under review as a conference paper at ICLR 2022
F Omitted proofs
F.1 PROOF OF THEOREM 1
Before we prove Theorem 1, we first introduce Lemma 1.
Lemma 1. ∀(x, ygls,r), '(f(x), ygls,r) = (1 - 2) ∙ 0(f (x),y) + 2 ∙'(f(x), 1 - y).
Proof of Lemma 1
Proof. For CE loss, due to its linear property w.r.t. the label, we directly have:
'(f(χ), yGLS,r)= '(f(χ), y ∙ (1 - r) + 2) = (1 - 2) ∙ '(f(χ), y) + r ∙ '(f(χ),1 - y)
□
Proof of Theorem 1
Proof. Based on Lemma 1, with a bit of math, for NLS, we have:
minE(χ,γ)~d ['(f(X),YGLSL)]
= minE(χ,γ)〜D [(1 + W) ∙ '(f(X),Y) - W ∙ '(f(X), 1 - Y)]
= minE(χ,γ卜不[[(1 + 2) + (1 - 2)] ∙ '(f(χ),γ) - [(1 - 2) ∙ '(f(χ), Y) + 2 ∙ '(f(χ),1 - γ)]]
= minE(χ,γ相 [2 ∙ '(f(X),Y) - '(f(X),YGLS,r)]
□
22
Under review as a conference paper at ICLR 2022
F.2 Proof of Theorem 5
Proof.
Eqn.3 =min E(χ,Y)〜D [ (1 -
I---
{z^
： = C1
r.	~、 r
2 )∙,(f (x),Y) + 2
}l{z}
：=C2
∙'(f(X), I - Y)]
=minEχ,γ=0 [P(Y = 0|Y = 0) ∙ (ci ∙ '(f(X), 0) + c ∙ '(f(X), 1))
+ P(Y = 1|Y = 0) ∙ (ci ∙ '(f(X), 1) + c2 ∙ '(f(X),0))]
+ Ex,y=i [P(Y = 0|Y = 1) ∙ (ci ∙ '(f(X), 0) + c2 ∙ '(f(X), 1))
+ P(Y = 1|Y = 1) ∙ (ci ∙ '(f(X), 1) + C2 ∙ '(f(X),0))]
= minEχ,γ=o [[(1 - e°) ∙ ci + e0 ∙ c2] ∙ '(f(X), 0) + [(1 - e°) ∙ c + e° ∙ c/ ∙ '(f(X), 1)]
+ Eχ,Y=i [[(1 - ei) ∙ ci + ei ∙ c2] ∙ '(f(X), 1) + [(1 - ei) ∙ c2 + ei ∙ ci] ∙ '(f(X), 0)]
=minex,y=0 [[(I- eo) ∙ ci + e0 ∙ c2] ∙ '(f(X), 0) + [(1 - e0) ∙ c2 + e0 ∙ ci] ∙ '(f(X), 1)]
+ Eχ,Y =i [[(1 - e0) ∙ ci + e0 ∙ c2] ∙ '(f(X), 1) + [(1 - e0) ∙ c2 + e0 ∙ ci] ∙ '(f(X), 0)]
+ Eχ,γ=i [eʌ ∙ (c2 - ci) ∙ '(f(X), 1) - eʌ ∙ (c2 - ci) ∙ '(f(X), 0)]
= minE(χ,γM [[(1 - e°) ∙ ci + e0 ∙ c2] ∙ '(f(X),Y) + [(1 - e°) ∙ c + e0 ∙ ci] ∙ '(f(X), 1 - Y)]
-eʌ ∙ (ci - c2) ∙ Eχ,γ =i['(f(X), 1) -'(f(X), 0)]
= minE(χ,γ)〜D [(ci + c2) ∙'(f(X), Y)]
+ [(1-e0)∙ c2 + e0 ∙ ci] ∙ E(χ,Y)〜。['(f(X), 1 - Y) - '(f(X),Y)]
-eʌ ∙ (ci - c2) ∙ Eχ,γ =i['(f(X), 1) -'(f(X), 0)]
= minE(χ,γ)〜D [(ci + c2) ∙'(f(X), Y*)]
+ [ - r2- + (1 - e0)∙ c2 + e0 ∙ ci ] ∙ E(χ,γ)〜。['(f (X), 1 - Y) -'(f (X),Y)]
-eʌ ∙ (ci - c2) ∙ Eχ,γ =i['(f(X), 1) -'(f(X), 0)]
= minE(χ,γ)〜D ['(f(X),Y*)] +λi ∙
E(χ,γ)〜。 ['(f(X), 1 - Y) - '(f (X),Y)]
I
}1
True Risk
+ λ2 ∙ Eχ,γ =1h`(f(X), 0) -'(f(X),1)i
'------------------------------------'
,
^^{^^"
M-Inc1
^~{^^™
M-Inc2
□
23
Under review as a conference paper at ICLR 2022
F.3 Proof of Proposition 1
Proof. The risk minimization of backward correction is equivalent to:
E(X Y)〜D ['J (f(X), Ye)] =E(x,y)” ['(f(X), Y)] (By Theorem 1 in (Patrini et al., 2017))
The risk minimization of forward correction is equivalent to:
E(X Y)〜D ['→(f(X), Ye)] =E(x,y)-d ['(f(X),Y)] (By Theorem 2 in (Patrini etal., 2017))
Theorem 1 and 2 in (Patrini et al., 2017) demonstrate that forward and backward corrected losses equal the
original loss ' computed on the clean data in expectation. Thus, for rLC = 22e01 , by Theorem 5 (adopt
2e0 -1
r* = 0), We have:
minE(χ,γ)〜D	['(f(X),YeGLS,rLC)] + Xlc ∙ Eχ,γ=ι['(f(X), 1)-'(f(X),0)]
、--------------V--------------}
Bias-LC
= minE(x,y)〜D ['(f(X),Y)] + [eo + (1 - 2e°) •等] • e(X,Y )〜D h'(f(X), 1 - Y) - '(f(X),Y)]
+ e∆ ∙ (1 - rLc) ∙ Ex,y =ι['(f(X), 0) - '(f(X), 1)] + Xlc • Eχ,γ =1 ['(f(X), 1) - '(f(X), 0)]
= minE(x,y)〜D ['(f(X),Y)] + e∆ ∙ (τ-12^ - τ-12^) • Eχ,γ=ι ['(f(X), 0) - '(f (X), 1)]
= minE(x,y)〜D ['(f(X),Y)]
Thus,
minE(x,Y)〜D ['F(X), Ye)] = minE(x,Y)〜D ['→(f(X), Ye)]
= minE(χ,γ)〜D ['f (X), YGLS,rLC)] + Xlc ∙ Eχ,γ =ι['(f(X), 1) - '(f(X), 0)]
、--------------V--------------}
Bias-Lc
□
F.4 Proof of Theorem 2
Proof. Based on Proposition 1, When e∆ = 0, XLc = 0, We directly have:
minE(x,Y)〜D ,-f (X),Y)] = minE(x,Y)〜D ['→(f(X),Ye)]
= mine(x,Y)〜D ['(f(X),YGLS,rLC)]
□
F.5 Proof of Theorem 3
Proof. Note that
minE(x,Y)〜D ['cL(f(X),Y)] = minE(x,Y)〜D ['f (X),Y) -'(f(X), 1 - Y)]
24
Under review as a conference paper at ICLR 2022
We have:
min%B~fψ(f(X)fGLS,g)]
= IninE(χf)~25 [(1-*)∙ 4f(X),r) + 等∙4f(χ),ι- K)]
OminE(X田〜石 p(f(X), Y) +	∙4f(X),l- K)]
When γcl T -∞, we have /羡 → -1. Thus,
IninE(XB〜25,cL(f(X),声)]=min 0x,y)~W ['(f(X)，声。1$，-一0°)]
□
F.6 Proof of Proposition 2
Proof. Note that:
⅛,y)-p ['(f(X)，^)]-限(x,V)~l5 卜(f(X)，尸5LSb)]
=E(x,y)-p [i-(i-∣)∙，(f (X)，Y)-γ 以f (X), 1 —0]
=2 ° E(x,P)〜力p(f(x),y)-4f(x),ι-y)]
And we have:
E(X “玲〜2sK(f(Xι),H)]
=Ex [p(y = o) ∙4f(x),o) + (ι- p(y = o)) ∙，(f(x), ι)]
=Ex?=0[p(y = o) ∙4f(x), o) + (ι - p(y = o)) .4f(x),ι)j
+ Eχ,y=1 [p(K = O) ∙以f(X), O) + (1 - P①=0)) ∙<f(x),ι)]
=Eχ,y=o 归①=O). "f(X), O) + (1 - P①=0)) ∙<f(X),ι)]
+ Eχ,？=1[(i- P(K = 0)). 4f(X),O) + P(K = O) ∙4f(X),ι)]
+ (1 - 2 ∙ P(y = 0)) ∙ Ex ?=1 [4f(X), 1)-以f(X), 0)]
25
Under review as a conference paper at ICLR 2022
Thus,
叫nE(χ,γ)~d ,PL(f(X),Y)] =minEχγ)~D '(f(X),Y) - '(f(X1),K)
=mfinE(χ,γ)~d ['(f(X), Y)] - E(Xi…['(f(Xi), Y2)]
=mn EX,Υ=0['(f (X), 0)] + Ex,Υ =1 K(f (X), 1)]
-Eχ,γ=0 [P(Y = 0) ∙ '(f(X), 0) + (1 - P(Y = 0)) ∙ '(f(X), 1)]
-Eχ,γ=ι[(1 - P(Y = 0)) ∙ '(f (X), 0) + P(Y = 0) ∙ '(f(X), 1)]
-(1 - 2 ∙ P(Y = 0)) ∙ Eχ,γ =ι['(f(X), 1) - '(f(X), 0)]
=mnEχ,γ=0[(1 - P(Y = 0)) ∙ ['(f(X),0) - '(f(X), 1)]]
+ Eχ,γ=ι[(1 - P(Y = 0)) ∙ ['(f(X), 1) - '(f(X), 0)]]
-(1 - 2 ∙ P(Y = 0)) ∙ Eχ,γ =ι['(f(X), 1) - '(f(X), 0)]
=mnE(χ,γ)~d [(1 - P(Y = 0)) ∙ ['(f(X), Y) - '(f(X), 1 - Y)]]
-(1 - 2 ∙ P(Y = 0)) ∙ Eχ,γ =ι['(f(X), 1) - '(f(X), 0)]
Thus, for γpl = 2 ∙ P(Y = 1), Xpl = 1 - γpl, we have:
E(X,Υ)~D ['PL(f(X),Y)] - E(X,γ)~D p(f(X),Y)] - E(X,γ)~D ['(f(X),YGLS,rPL)]
=E(X,Υ)~D [(1 - P(Y = 0)) ∙ ['(f(X), Y) - '(f(X), 1 - Y)]]
-	(1 - 2 ∙ P(Y = 0)) ∙ Ex,υ =ι['(f(X), 1) - '(f(X), 0)]
-	r2L ∙ E(X,γ)~d ['(f(χ),Y) - '(f(χ),1 - Y)]
=E(X,γ)~d [(1 - P(Y = 0) - P(Y = 1)) ∙ [，(f(X), Y) - '(f(X), 1 - Y)]]
-	(2 ∙ P(Y =1) - 1) ∙ Ex,υ =ι['(f(X), 1) - '(f(X), 0)]
=λpL ∙Ex,υ =ι['(f(X), 1) -'(f(X),0)]
And we can conclude that:
minE(X,γ)* [`pL(f(X),Y)] =min E(X,γh石['(f(X),Y) -'(f(X),YGLSbPL)]
+Xpl ∙ Ex,υ =1 ['(f(X), 1) - '(f(X), 0)]
、--------------V-------------/
Bias-PL
□
26
Under review as a conference paper at ICLR 2022
F.7 Proof of Theorem 4
>Λ C 5T1 TΓΛ ∕^Γ5-	z-> ∖	TΓΛ	1 ∖	f . τ-⅜	∙ . ∙ r∖	1	、	Cl
Proof. When P(Y = 0) = P(Y = 1), according to Proposition 2, we have λPL = 0 and:
minE(χ,γ)~d [，PL(f(X),Y)] =minE(χ,γ)空['(f(X),Y) -'f (X),YGLS,rPL)]
= minE(χ,γ)~d [r2L ∙ '(f(X), Y) - r2L ∙ '(f(X), 1 - Y)]
0 minE(χ,Y)~D ['f (X), Y) —'f (X), 1 — Y)]
When rPL → -∞, we further have:
min e(x,Y)~D ['f (X),YGLS,rPL)] O min E(X,Y)~D ['(f (X),Y) + 2--CL^ ∙ '(f (X),1 — Y)]
0 min E(x,Y)~D ['f (X),Y) — 'f (X), 1 — Y)]
Thus, Theorem 4 is proved.	口
F.8 Proof of Theorem 6
Proof. Note that the optimal r that will cancel the impact of Term M-Inc1 is:
r* — 2e
ropt := I-Ie
• When e < r~, r°pt > 0. In this case, learning PLS with smooth rate r°pt results in:
min E(χ,γ)~d ['(f (X),YGLS,r=ropt)] = min E(χ,γ)“ ['f (X),Y *)]
which yields fD;
• When e = r~, r°pt = 0. Learning with the Vanilla Loss yields fD since:
minE(χ,γ)~d ['(f(X),Y)] = minE(x,y)~d ['f (X),Y*)]
• Similarly, when e > ?, learning NLS with r = r°pt < 0 yields fD.
□
27
Under review as a conference paper at ICLR 2022
F.9 Proof of Theorem 7
n	C ɪʌ	.	ITT./，广	■ \	.1	1	IFli ♦ ,	♦	~	πTι∕^Γ^^	∙∖	. I	I	IFli ♦ , ♦ F , ♦
Proof. Denote Pi = P(Y = i) as the clean label distribution, Pi = P(Y = i) as the clean label distribution.
Let ez = ~κ-ι, we have:
E(X,Y)〜D [(1 - r) ∙ '(f(X), Y)] + EX [ X K ∙ '(f(X), i)
i∈[K]
X E(X,Y)〜D,γ=i [(1 - r) ∙ '(f(X),γ)]
-i∈[K]
+ Ex [ X K ∙ '(f(X),i)]
i∈[K]
(1-r) ∙ X E(X,Y)〜Dy=」X Ti,j ∙ '(f(X),Y =j)
.	i∈[K]	j∈[K]
+ Eχ[ X S
i∈[K]
∙'(f (X),成
(1 - r) ∙ X Eχ,y=i [(1 - C ∙ '(f(X),i) + X K ∙ '(f(X), j)i
+ EX
i∈[K]
j∈[K]
X K ∙ '(f(χ),i)
-i∈[K]
(1-r) ∙ X Eχ,y=i[(1-e0) ∙'(f(X),i)]
.	i∈[K]
+ EX
[:+ 方]X 'f W)
j∈[K]
(1 - r) ∙ (1 - e) E(Xy)〜DK(f (X),Y)]
+ EX
I
^{^―
： = C3
,
[:+ V ]X '(f (X),j)
X-------------------' j∈ [K]
：=C4
I
1-r7 ∙ E(X,y )~d
*
p(f(x),y*) - K X `f(X),j)]
j∈[K]
+ C4 ∙ Eχ[ X '(f (X),j)]
-	j∈[K]
1 Jr* ∙ E(X,Y)〜D p(f (x),y *)] + (∙
^^^^{^^^^≡
True Risk
✓
I
*
C4- (ɪrv )∙ EX [X '(f (X),j)]
'	j	j∈[K]
~{^^^^^^^^^^^^^^^^^^^^^^^^—
M-Inc1
,
*	/
Adopting r°pt = H , with a bit of math, the weight of Term M-Inc1 becomes 0 and
E
心(X,Y)〜D
E
E(X,Y)〜D
p(f (X),YGLS2)]
h(1 - ropt) ∙ '(f(X), Y)] + EX [ X 智∙ '(f(X), i)]
i∈[K]
1 _3产∙ E(X,Y)〜DMf (X),Y *)]
0 E(X,Y)S p(f(X),Y*)]
□
28