Under review as a conference paper at ICLR 2022
Learning Stochastic Shortest Path with Lin-
ear Function Approximation
Anonymous authors
Paper under double-blind review
Abstract
We study the stochastic shortest path (SSP) problem in reinforcement learning with
linear function approximation, where the transition kernel is represented as a linear
mixture of unknown models. We call this class of SSP problems as linear mixture
SSR We propose a novel algorithm for learning the linear mixture SSH which can
attain a s y∕K∕cτn[n) regret. Here K is the number of episodes, d is the
dimension of the feature mapping in the mixture model, B* bounds the expected
cumulative cost of the optimal policy, and Cmin > 0 is the lower bound of the cost
function. Our algorithm also applies to the case when Cmin = 0, where a C7(K2/3)
regret is guaranteed. To the best of our knowledge, this is the first algorithm with a
sublinear regret guarantee for learning linear mixture SSP. In complement to the
regret upper bounds, we also prove a lower bound of OMBwhich nearly
matches our upper bound.
1	Introduction
The Stochastic Shortest Path (SSP) model refers to a type of reinforcement learning (RL) problems
where an agent repeatedly interacts with a stochastic environment and aims to reach some specific
goal state while minimizing the cumulative cost. Compared with other popular RL settings such
as episodic and infinite-horizon Markov Decision Processes (MDPs), the horizon length in SSP
is random, varies across different policies, and can potentially be infinite because the interaction
only stops when arriving at the goal state. Therefore, the SSP model includes both episodic and
infinite-horizon MDPs as special cases, and is comparably more general and of broader applicability.
In particular, many goal-oriented real-world problems fit better into the SSP model, such as navigation
and GO game (Andiychowicz et al., 2017; Nasiriany et al., 2019).
In recent years, there emerges a line of works on developing efficient algorithms and the corresponding
analyses for learning SSB Most of them consider the episodic setting, where the interaction between
the agent and the environment proceeds in K episodes (Cohen et al., 2020; Tarbouriech et al., 2020a).
For tabular SSP models where the sizes of the action and state space are finite, Cohen et al. (2021)
developed a finite-horizon reduction algorithm that achieves the minimax regret O{B^y∕SAK),
where Bir is the largest expected cost of the optimal policy starting from any state, S is the number of
states and A is the number of actions. In a similar setting, Tarbouriech et al. (2021b) proposed the first
algorithm that is minimax optimal, parameter-free and horizon-free at the same time. However, the
algorithms mentioned above only apply to tabular SSP problems where the state and action space are
small. In order to deal with SSP problems with large state and action spaces, function approximation
techniques (Yang & Wang, 2019; Jin et al., 2020; Jia et al., 2020; Zhou et al., 2021b; Wang et al.,
2020b;a) are needed.
Following the recent line of work on model-based reinforcement learning with linear function
approximation (Modi et al., 2020; Jia et al., 2020; Ayoub et al., 2020; Zhou et al., 2021b), we consider
a linear mixture SSP model, which extends the tabular SSP. More specifically, we assume that
the transition probability is parametrized by P(s,∣s, α) = (φ(s,∣s, a), θ*) for all triplet (s, a, s,) ∈
S × A × S, where S is the state space and A is the action space. Here we assume that φ ∈ Rd is a
known ternary feature mapping, and 0* ∈ Rd is an unknown model parameter vector that needs to be
learned. Such a setting has been previously studied for episodic MDPs (Modi et al.5 2020; Jia et al.,
2020; Ayoub et al., 2020; Cai et al., 2020) and infinite-horizon discounted MDPs (Zhou et al., 2021b).
Nevertheless, algorithms developed in these works do not apply to SSP since the horizon length is
random as mentioned above.
1
Under review as a conference paper at ICLR 2022
To tackle the challenge of varying horizon length, we propose a model-based optimistic algorithm
with linear function approximation, dubbed LEVI S, for learning the linear mixture SSP. At the core
of our algorithm are a confidence set of the model parameters and a specially designed Extended
Value Iteration (EVI) subroutine for computing the optimistic estimate of the value function, which
together guarantee that the algorithm will reach the goal state in every episode. Compared with the
EvI subroutine developed for infinite-horizon discounted MDPs (Zhou et al., 202lb), we introduce
a shrinking factor q ≈ 1/t in our EVI with t being the cumulative number of time steps, which
guarantees the convergence of EVI. To compensate for the bias introduced by this shrinking factor,
our algorithm performs lazy policy update, which is triggered by the doubling of the time interval
between two policy updates or the doubling of the determinant of the covariance matrix. With all
these algorithmic designs, our algorithm is guaranteed to achieve a O(dB1∙5 ,K∕Cmin) regret when
cmin > 0. To the best of our knowledge, this is the first algorithm that enjoys a sublinear regret for
linear mixture SSB
It is worth noting that a recent work by %al et al. (2021) studied a different linear SSP model
that is similar to linear MDPs (Yang & Wang, 2019; Jin et al., 2020), where both the underlying
transition probability and cost function are linear in a knoʌvn (/-dimensional feature mapping ≠ ∈ Rd,
i.e., P(√∣s,α) =	μ(s)) and c(s, a) — (-≠(sjα),θ), and μ(∙) and θ are unknown. They
proposed an algorithm with linear function approximation, which achieves O(y∕B^d^K∕cιnin) regret.
The linear SSP model is different from our model, and we refer the interested readers to Ayoub et al.
(2020); Zhou et al. (202Ib) for a detailed comparison between these two assumptions. Besides the
model difference, Vial et al. (2021) further assumed the feature mapping to be orthonormal in order
to obtain the O(y∕K) regret. We do not need such restrictive assumptions on the feature mapping,
thus our algorithm provably works for more general cases.
Our contributions are summarized as follows:
•	We propose to study a linear mixture SSP model, and devise a novel algorithm, dubbed Lower
confidence Extended Value Iteration for SSP (LEVIS), for learning SSP with linear function
approximation.
•	We prove that LEVIS achieves a regret of order O{B^dy∕K∕cm[n) when Cmin > 0 and the agent
has an order-accurate estimate B > 5*1. For the general case where Cmin = O5 our algorithm can
achieve O(∕f2/3) regret guarantee by using a cost perturbation trick (Tarbouriech et al., 2021b).
•	We prove that for linear mixture SSP, the regret of any learning algorithms is at least Ql(dBiry∕K)t
This suggests that when Cmin > 0, our algorithm is optimal with regard to the dimension of the
feature mapping d and number of episodes K.
Notation We use lower case letters to denote scalars, and use lower and upper case bold face letters
to denote vectors and matrices respectively. For any positive integer n, we denote by [n] the set
{1,...,n}. For a vector x ∈ Rd , we denote by ∣∣x∣∣ι the Manhattan norm and denote by ∣∣x∣∣2 the
Euclidean norm. For a vector x ∈ Rd and matrix Σ ∈ Wixd, we define ∣∣x∣∣∑ = √xτ∑x. For two
sequences {αn} and {bn}, we write an = O(bn) if there exists an absolute constant C such that
an ≤ Cbn. We use O(∙) to hide the logarithmic factors.
2	Related Work
Online learning in SSP SSP problems can be dated back to (Bertsekas & Tsitsiklis, 1991; Bertsekas
& Yu, 2013; Bertsekas, 2012), but it is until recently that the regret minimization in online learning
of SSP has been studied. In the tabular case, Tarbouriech et al. (2020a) proposed the first algorithm
achieving a O(D3/2Sy/AK∕cτnιn) regret where D is the diameter of SSP2. The regret was further
improved to O{B^Sy∕AK} by Rosenberg et al. (2020); Cohen et al. (2020), with an extra y/S
factor compared with the Q(B&SAK) lower bound (Rosenberg et al., 2020). More recently, the
G(B*y∕SAK) minimax optimal regret were obtained by Cohen et al. (2021) and Tarbouriech et al.
(2020b) independently using different approaches. Specifically, Cohen et al. (2021) reduced SSP
to a finite-horizon MDP with a large terminal cost assuming Bit. is known; while Tarbouriech et al.
(202lb) avoid such requirement by adaptively estimating Bir with a doubling trick, together with
1We say B is an order-accurate estimate of B*, if there exists some unknown constant κ ≥ 1 such that
Bit ≤ B ≤ κBiv.
2The diameter of an SSP is defined as the longest possible shortest path from any initial state to the goal state.
2
Under review as a conference paper at ICLR 2022
a value iteration sub-routine ensuring the optimistic estimate of the value function. Our proposed
method shares a similar spirit with the latter approach, but for learning SSP with linear function
approximation.
The above algorithms are all model-based. Veiy recently, Chen et al. (2021a) developed the first model-
free algorithm for SSP which achieves the minimax optimal regret when the minimum cost among
all state-action pairs Cmin ɪs strictly positive. Their method is motivated by the U C B-AD VANT AGE
algorithm (Zhang et al., 2020). For other settings of SSP9 (Rosenberg & Mansour, 2020; Chen &
Luo, 2021; Chen et al.5 202lb) studied the case of adversarial costs. Also, the pioneering work by
(Bertsekas & Tsitsiklis, 1991) studied the pure planning problem in SSP where the agent has full
knowledge of all the model parameters, and is followed by a series of works (Bonet, 2007; Kolobov
et al., 2011; Bertsekas & Yu, 2013; Guillot & Stauffer, 2020). On the other hand, Tarbouriech
et al. (202la) studied the sample complexity of SSP assuming the access to a generative model.
Jafarnia-Jahromi et al. (2021) proposed the first posterior sampling algorithm for SSP. Multi-goal
SSP have also been studied by Lim & Auer (2012); Tarbouriech et al. (2020b).
Linear function approximation Linear MDP is one of the most widely studied models for RL with
linear function approximation, which assumes both the transition probability and reward functions are
linear functions of a known feature mapping (Yang & Wang, 2019; Jin et al., 2020). Representative
work in this direction include Du et al. (2019); Zanette et al. (2020); Wang et al. (2020a); He et al.
(2021), to mention a few.
Another popular model for RL with linear function approximation is the so-called linear mixture
MDP/linear kernel MDP (Yang & Wang, 2020; Modi et al., 2020; Jia et al., 2020; Ayoub et al.s 2020;
Cai et al., 2020; Zhou et al., 2021b;a). For the finite-horizon setting, Jia et al. (2020) proposed a
UCRL-VTR algorithm that achieves a O{dy∕H3T) regret bound. Zhou et al. (202la) further improve
the result by proposing a UCRL-VTR+ algorithm that attains the nearly minimax optimal regret
O{dH∖∕T) based on a novel Bernstein-type concentration inequality. For the discounted infinite
horizon setting, Zhou et al. (202lb) proposed a UCLK algorithm with a O(dy∕T/(1 -7)2) regret, and
also give a O(dy∕T/(1 — 7)1,5) lower bound. The lower bound is later matched up to logarithmic
factors by the UCLK+ algorithm (Zhou et al., 2021a). The SSP model studied in this paper can be
seen as an extension of linear mixture MDPs.
3	Preliminaries
Stochastic Shortest Path An SSP instance is an MDP M := {S, X, P5 c, $血匕 g}, where S and A
are the finite state space and action space respectively. Here Sinit denotes the initial state and g ∈ S is
the goal state. We denote the cost function by c : 5 × √4 → [0,1], where c(s, Q) is the immediate cost
of taking action a at state s. The goal state g incurs zero cost, i.e., c(g, Q) = O for all α ∈ √4. For any
(s,, s, α) ∈ 5 × Λ × 5, P(s,∣s, α) is the probability to transition to sf given the current state s and
action a being taken. The goal state g is an absorbing state, i.e., P(g∖g, Q) = I for all action a E A.
Linear mixture SSP In this work, we assume the transition probability function P to be a linear
mixture of some basis kernels (Modi et al., 2020; Ayoub et al., 2020; Zhou et al., 2021a).
Assumption 3.1. Suppose the feature mapping φtS×A×S^^Rd is known and pregiven. There
exists an unknown vector θ* ∈ Rd with ∣∣0*∣∣2 ≤ Vd such that IP)(SiS,α) = (≠(s,∣s,α), θ*) for any
state-action-state triplet (s, α,s,) ∈ 5 × √4 × <S. Moreover, for any bounded function V : S → [0, B],
it holds that 11φy (s, α) 112 ≤ B∖∕d for all (s, α) ∈ 5 × Λ, where ≠v(s, α) ：= £$《5 OS® Q)V(S)
For simplicity, for any function V : 5 —> R5 we denote PV(s,α) = IP(SlS,α)P(s') for all
(sj α) ∈ <S × A Therefore, under Assumption 3.1, we have
PV(s,α) = £ P(s1s,a)V(s') = £ <0(s[s, a), 0*W(s")=〈加(s, a), C*).
s,es	s,es
Proper policies A stationary and deterministic policy is a mapping π : 5 → √4 such that the action
Tr(S) is taken given the current state s. We denote by T7r(s) the expected time that it takes by
following Tr to reach the goal state g starting from 5. We say a policy τr is proper if T7r(s) < ∞ for
any SGS (otherwise it is improper). We denote by IIPrOPer the set of all stationary, deterministic and
proper policies. We assume ±at IlPrOPer ɪs non-empty, which is the common assumption in previous
works on online learning of SSP (Rosenberg et al., 2020; Rosenberg & Mansour, 2020; Cohen et al.,
2021; Tarbouriech et al., 2021b; Jafamia-Jahromi et al., 2021; Chen et al.s 202la).
3
Under review as a conference paper at ICLR 2022
Assumption 3.2. The set of all stationary, deterministic and proper policies is non-empty, i.e.,
ɪɪproper * 0∙
Remark 3.3. The above assumption is weaker than Assumption 1 in MlaI et al. (2021) which requires
that all stationary policies are proper.
For any policy π, we define the cost-to-go function (a.k.a., value function) as
-τ	'
V7r(s) := Iim E ɪ2 c(stj7r(st)) Sl = S
Tτ+oo L=1	_
where st+ι 〜P( ■ ∣瓯,τr(瓯)).
V7r (s) can possibly be infinite if Tr is improper. The action-value function of policy π is defined as
-	τ	~
Q7r(s,a) := Iim E e(si,ɑɪ) + ɪ2c(s⅛,7r(st)) s1 = s, a1 = a ,
_	t=2	.
where s? 〜P(-∣sι,αι) and st+ι 〜P(∙∣si,π(s⅛)) for all t ≥ 2. Since c(∙, ∙) ∈ [0,1], for any proper
policy π ∈ ∏proper, V7r and QjV are both bounded functions.
Bellman optimality For any function V : 5 → R, we define the optimal Bellman operator C as
CV(S) := min{c(s7 a) + IpV(s, α)}.	(3.1)
α∈Λ	*
Intuitively speaking, we want to Ieam the optimal policy π* such that V*(∙) := V7τ (∙) is the unique
solution to the Bellman optimality equation V = CV and π* minimizes the value function V7r(s)
component-wise over all policies. It is known that, in order for such π* to exist, one sufficient
condition is Assumption 3.2 together with an extra condition that any improper policy Tr has at
least one infinite-value state, i.e.5 for any Tr 串 ∏pr0perj there exists some SGS s.t. V7r(s) = +∞
(Bertsekas & Tsitsiklis, 1991; Bertsekas & Yu5 2013; Tarbouriech et al., 2021b). Note that this
additional condition is satisfied in the case of strictly positive cost, where for any state s ≠ and
a ∈ √4, it holds that c(s, α) ≥ cmin. To deal with the case of general cost function, one can adopt the
cost perturbation trick (Tarbouriech et al., 2021b) and consider a modified problem with cost function
cp(s, a) := max{c(s, q), p} for some p> 0. This will introduce an additional cost of order O(PT)
to the regret of the original problem, where T is the total number of steps. Therefore, the second
condition can be avoided, and we can assume the existence of 7r*.
Throughout the paper, we denote by Bir the upper bound of the optimal value function V*, i.e.,
Bir ：— maXSWS V*(s). Also, we define T-k := maxs∈5 T7r (s)5 which is finite under Assumption 3.2.
Since the cost is bounded by 1, we have Bif < Tif < +∞. Without loss of generality, We assume that
Bif > 1. Furthermore, we denote the corresponding optimal action-value function by Q* := Q7r
which satisfies the following Bellman equation for (s, α) ∈ 5 × √4:
Q*(s,α) = c(s, d) + PV*(s,α), V*(s) = min Q*(s,α).	(3.2)
α∈Λ
Learning objective Under Assumption 3.1, we assume c to be known for the ease of presentation.
We study the episodic setting where each episode starts from a fixed initial state Sinit and ends only if
the agent reaches the goal state g. Given the total number of episodes, K, the objective of the agent
is to minimize the regret over K episodes defined as
K ιk
RK ：= ΣΣCfc,i - κ ∙ V*(Sinit),	(3.3)
k=l i=l
where Ik is the length of the ⅛-th episode and c⅛3 = C(S比心 α⅛,i) is the cost triggered at the z-th step
during the ∕c-th episode. Note that RK might be infinite if some episode never ends.
4 Algorithms
In this section, we propose a model-based algorithm named LEVIS, as displayed in Algorithm 1.
LEVIS is inspired by the UCLK-type of algorithms originally designed for discounted linear mixture
MDPs (Zhou et at, 2021a;b). Our algorithm takes a multi-epoch form, where each episode is divided
into epochs of different lengths (Jaksch et al., 2010; Lattimore & Hutter, 2012). Within each epoch,
the agent executes the greedy policy induced by some optimistic estimator of the optimal Q-function.
The switch between any two epochs is triggered by a doubling criterion, and then the estimated
Q-function is updated through an Extend Value Iteration (EVI) sub-routine (Algorithm 2). We now
give a detailed description of Algorithm 1.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 LEVIS
3 4 5 6
7 8 9 0 1 2 3 4 5678
11 1± lɪ 11 lɪ 11 lɪ 1± 11
Input: regularization parameter λ, confidence radius {βt}∙> cost perturbation p ∈ [0,1], an
estimate B > Bif
Initialize: set t 1, j 0, to = °, ∑o — ʌl, bo x— 0, Qo(s, ♦), ¾j(s) ― IVS ≠ g and 0
otherwise
for /c = I5... 5T< do
Set Sf — S⅛it
while st ≠ p do
Take action at = argminae>t Qj(s⅛, a), receive cost ct = c(si, ai) and next state St+ι ~
P(∙M3
Set ∑t ÷- ∑t-ι + φvj (s. at)φyj (SA a⅛)τ
Set bt ÷- bt_i + φvj (st, Ωt)½(si+ι)
if det(∑t) ≥ 2 det(∑⅛j.) or t ≥ 2tj then
Set，—+ 1	"
Set tj «— t, €j V— 2
Set confidence SetCj — {e : ∣∣ ∑Jʃ2(0 — θj)∣∣2 ≤ βtj ∣
Set Qj-(-, ■) J EVl(Cj∙,ej∙, ɪ,p)
Set Vj(∙) ∙ miHα∈∕Q∕(∙,0)
end if
Set t V— 1 + 1
end while
end for
Algonthm 2 EVI
1:	Input: confidence set C, error parameter e, transition bonus q, cost perturbation p ∈ [O51]
2:	Initialize: z 0, and ∙), Vr(O)(∙) = O, and Vr(-D(∙) =+00
3:	Set Q(∙,∙)-Q(。)(∙,∙)
4:	if C ∩ K / ° then
5:	wh∏e ||V« - V(ET) ∣∣θo > e do
6:
Q(2+1)— cp(∙, ∙) + (1 — q) ∙ min (0,≠y(i)(∙,∙))	(4.1)
σ∈C∩o
V(i+1) (.) . min Q('+1) (∙, a)	(4.2)
α∈-4
7:	Set£ + 1
8:	end while
9： Q(∙,∙)-Q('+1)(∙,∙)
10:	end if
11:	Output: QG ∙)
In Algorithm 1, we maintain two global indices. Index t represents the total number of steps, and
index j tracks the number of calls to the EVI sub-routine, where the output of EVI is an updated
Optnnistic estimator of the optimal action-value function. Each episode starts from a fixed initial state
Sinit (Line 4), ends when the goal state g is reached (Line 5) and is decomposed into epochs indexed
by the global index j. Within epoch j, the agent repeatedly executes the policy induced by the current
estimation Qj of the action-value function (Line 6) and updates ∑t and b± (Lines 7 and 8). The
current epoch ends when the either criterion in Line 9 is triggered, and the EVI subroutine performs
an optimistic planning to update the action-value function estimator (Lines 10 to 15).
Update criteria As mentioned before, Algorithm 1 runs in epochs indexed by j, and one epoch ends
when either of the two update criteria is triggered (Line 9). The first updating criterion is satisfied once
the determinant of ∑t is doubled compared to the determinant at the end of the previous epoch. This
5
Under review as a conference paper at ICLR 2022
is called lazy policy update that has been used in the linear bandits and RL literature (Abbasi-Yadkori
et al., 2011; Zhou et al., 2021b), which reflects the diminishing return of learning the underlying
transition. One intuition behind the determinant doubling criterion is that the determinant can be
viewed as a surrogate measure of the exploration in the feature space. Thus, one only updates the
policy when there is enough exploration being made since last update. Moreover, this update criterion
reduces the computational cost as the total number of epochs would be bounded by O (log T). Here
T denotes the total number of steps through all K episodes. The doubling visitation criterion used in
tabular SSP (Jafamia-Jahromi et al., 2021; Tarbouriech et al., 2021b) can be viewed as a special case
of this doubling determinant-based criterion.
However, the above criterion alone cannot guarantee finite length for each epoch as we do not
have that ∣∣φy(-, -)∣∣ is bounded from below, which holds for tabular SSP naturally since at most
∣<S∣∣fcA∣ maxs∈5jθ∈,4九⑶ ɑ) steps suffice to double n(s,a) for at least a pair of s,a by the pigeonhole
principle. To address this problem, we show that we only need to add an extra triggering criterion:
t > 2tj. It turns out that despite of being extremely simple this criterion endows the algorithm with
several nice properties. First, together with the EVI error parameter Ej = l∕tj, we can bound the
cumulative error from value iterations in epoch j by a constant, i.e., (2与—tj) ∙ 6j = 1. Second, it
will not increase the total number of epochs since the time step doubling can happen at most O(IOg T)
times, which is consistent with the first criterion. These two properties together allow us to bound the
total error from value iteration by O(IogT). Finally, this criterion is fairly easy to implement and has
negligible time and space complexity.
Optimistic planning The optimism of Algorithm 1 is realized by the construction of the confidence
set Cj (Line 11), which is fed into the EVI subroutine. We now describe the construction of the
Q-function estimator in the EVI sub-routine (Algorithm 2). EVI requires the access to a confidence
ellipsoid Cj that contains the true model parameter θ* with high probability (Line 13). Here we
construct the confidence set Cj centered at the minimizer of the ridge regression problem with a
confidence radius parameter βt (Line 13). Since not eveιy θ ECj defines a valid transition probability
function, we further take the intersection between Cj and a constraint set B defined as follows
B := {θ : V(s,α), ((∕>(∙∣s,α), Θ} is a probability distribution and {φ(s,∖g,a),θ) = l{s, = g}}.
Then Cj ∩ B is still a confidence set containing the true model parameter θ* with high probability as
Θ* ∈ B. Algorithm 2 requires two additional inputs: optimality gap βj∙ and discount factor q. The use
of Ej is standard, but this discount factor is the key to ensuring convergence of EVI.
Specifically, (4.1) in Algorithm 2 repeatedly conducts one-step value iteration by applying the best
possible Bellman operator to the set Cj ∩ B. This is motivated by the Bellman optimality equation in
(3.2), and uses miɪɪeeCnB(仇。吠⑸)as an optimistic estimate for PV*. However, using this estimate
alone cannot guarantee the convergence of EVI because (∙, φy(i)) is not a contractive map, which
holds for free in the discounted setting (Jaksch et al., 2010; Zhou et al., 2021b), but not in SSP.
More specifically, in the EVI algorithm for the discounted setting (e.g., Algorithm 2 in (Zhou et al.,
2021b)), there is an intrinsic discount factor O < 7 < 1, which ensures that the Bellman operator is a
contraction. As a result, the value iteration converges in a finite number of iterations. However, the
Bellman equation of SSP does not have a discount fector. To address this issue, in (4.1), we introduce
an extra 1 — q discount factor to ensure the contraction property. Although this causes an additional
bias to the estimated transition probability function, we can alleviate it by choosing q properly. In
particular, for each epoch j we set q = 1/tj (Line 14), and as will be shown, this bias will only
introduce an additive term of order C? (log T) in the final regret bound.
Besides the convergence guarantee, Ihel — q factor also brings an additional benefit that it biases
the estimated transition kernel towards the goal state g, further encouraging optimism. Similar
design can also be found in the VISGO value iteration algorithm used by Tarbouriech et al. (202lb).
The intuition behind such a design is to ensure the existence of proper policies under the estimated
transition probability function. As a result, the output of the value iteration, which solves V = CV
approximately for the Bellman operator £ induced by the estimated transition, can induce a greedy
policy that is proper under the estimated transition.
Regarding the implementation of LEVIS, note that the main computational overhead is from EVI,
where within each inner iteration we need to solve an optimization problem. Fortunately, the loss
function is strongly convex, thus it can be efficiently solved by many convex optimization algorithms.
6
Under review as a conference paper at ICLR 2022
5 Main Results
In this section, we present the main theoretical results for Algorithm 1. We provide regret upper
bounds for both positive cost functions and general cost functions, followed by a lower bound.
5.1	Upper Bounds： Positive Cost Functions
We first consider a special case where the cost is strictly positive (except for the goal state g).
Assumption 5.1. There exists an unknown constant Qmin ∈ (0,1) such that c(s, a) ≥ Cmhl for all
s ∈ S ∖ {g} and a E A .
Let T be the total number of steps in Algorithm 1, then the above assumption allows us to lower
bound the total cumulative cost after the K episodes by Cmin ∙ T. Note that this provides a relation
between the deterministic K and the random quantity T. To simplify the expression, we assume the
agent has access to B, an order-accurate estimate of 8* satisfying Bic < B < KIBif for some unknown
constant κ,>l. Similar assumptions have also been imposed in previous works (Tarbouriech et al.,
2021b; Vial et al., 2021).
Theorem 5.2. Under Assumptions 3.1, 3.2 and 5.1, for any J > 0, let p = 0 and βt —
By/diog (4(⅛2 + t3B2∕X)∕δ) ⅛ y∕λd for all t ≥ 1, where B > Bir and λ ≥ 1. Then with probability
at least 1 — 5, the regret of Algorithm 1 satisfies
Rκ = O	∙ log2 (1^∖ + — log2 (} .	(5.1)
∖	vɑminð / ɑmin vɑminð / /
If B — O(B★), Algorithm 1 attains an O(B^1'5dy∕K∕cmιn) regret. The dominating term in (5.1)
has an dependency on l∕cmi∏. For the tabular SSP, Cohen et al. (2021); Jafamia-Jahromi et al.
(2021); Tarbouriech et al. (202lb) avoid such dependency by using a Bemstein-type confidence
set. However, it remains an open question whether a similar result can be achieved under the linear
function approximation setting.
Remark 5.3. If we set the parameter δ in Theorem 5.2 as δ = 1 /K and define the high probability
event Ω as Theorem 5.2 holds. Then3 for the expected regret, We have
E[‰] ≤ E[‰∣Ω] Pr[Ω] + KPr[Ω]
喈(强)+丝喈(%))，
∖	∖ ɛmin / GIIin ∖ Qmin / /
which implies an O(Bi.15dy/K∕cmin) expected regret.
5.2	Upper Bound： General Cost Functions
When Assumption 5.1 does not hold, an O(K2/3)regret can be achieved by running Algorithm 1
WithP = KT/3.
Theorem 5.4. Under Assumptions 3.1 and 3.2, for any 6 > 0, let p = K~1^3 and βt —
By/d log (4(t2 + t3B2/A)∕5) + χ∕λd for al∏ ≥ 1, where B > Bir and λ ≥ 1. Then with probability
at least 1 — 5, the regret of Algorithm 1 satisfies
Rκ = O (β1∙5dK2/3 ∙ X + T*κ2∕3 + 52d2κ1∕3 . χ),
where B = B + T*∣K'∣3 a∏d χ = log2 ((B + Tir)Kd∕δ).
In Theorem 5.4, the regret depends on B instead of B. Note that B is approximately equal to 8*
when K = Ω(T^) and B = O(B*). Here T* is defined in Section 3 as the maximum expected time
it takes for the optimal policy to reach the goal state starting from any state.
The cost perturbation PiSa common trick to deal with the case of general cost functions in the
SSP literature (Tarbouriech et al., 2020a; Cohen et al., 2020; Tarbouriech et al” 202lb). Similar
to Tarbouriech et al. (2020a), the term c^Jn is multiplicative with K in our regret bound given by
Theorem 5.2. As a result, the perturbation can only give an 0(K2/3)「egret in the case of general
cost functions. Similarly, the regret bound of learning linear SSP (Vial et al., 2021) also has a
multiplicative crnJn. Some later work on tabular SSP (Cohen et al., 2020; Tarbouriech et al., 2021b)
has shown that it is possible to make the term c^^rι additive instead of multiplication, which improves
the regret to O(Jf1/2) for general cost functions. How to get an additive crn^n term in the linear
function approximation setting is an interesting future direction.
7
Under review as a conference paper at ICLR 2022
For the choice of the other parameters in Algorithm 1, by Theorems 5.2 and 5.4, we can set λ ɪ 1
in both the positive and general cost cases. For the upper bound B > Bic, note that assuming a
known B is common in existing SSP literature (Cohen et al., 2021; Sal et al., 2021). Although it is
possible to deal with an unknown B in the tabular SSP with a doubling trick (Rosenberg et al., 2020;
Tarbouriech et al., 202lb), it remains an open question for SSP with linear function approximation.
5.3	Lower Bound
We also provide a hardness result for learning linear mixture SSP by proving the lower bound for the
expected regret suffered by any deterministic learning algorithms.
Theorem 5.5. Under Assumption 3.1, suppose d ≥ 2, 8* ≥ 2 and K > (d— 1)2∕212. Then for any
possibly non-stationary history-dependent policy π, there exists a linear mixture SSP instance with
parameter θ* such that
E",e* [‰] ≥ $
(5.2)
Remark 5.6. The expectation in (5.2) is over the trajectories induced by executing the policy π in
the SSP environment parameterized by 6*. Note that here we allow the policy Tr to be non-stationaιy
and history-dependent. This is equivalent to assuming a deterministic learning algorithm, which is
sufficient for establishing a lower bound (Cohen et al., 2020).
Remark 5.7. Our instance for the lower bound can be also adapted to a linear SSP instance (Vial
et al., 2021), which yields a	lower bound. (See Remark E.l for a detailed discussion.)
6 Proof Sketch of the Main Results
In this section, we give a proof sketch of the main results in Section 5. Due to space limit, we defer
the proof of the lemmas to the appendix.
6.1	Proof of Theorem 5.2
In this subsection, we prove Theorem 5.2, which gives the regret upper bound of Algorithm 1 for
positive cost functions. The proof relies on the following intermediate result.
Theorem 6.1. Under Assumption 3.1 and 3.2, for any 5 > 0, let p = 0 and βt =
By∕d∖og (4(t2 + ⅛3B2∕λ)∕5) + >∕λd for some B > where λ ≥ 1 and p = 0. Then with
probability at least 1 — ʌ, the regret of Algorithm 1 satisfies
RK ≤ ^βτ∖ dT log I 1 +
+ 7dBir log T ÷
where T is the total number of steps.
Remark 6.2. Theorem 6.1 gives an O(T) regret upper bound with respect to the total number
of steps T. However, for SSP problems, the horizon of each episode is unknown and T can be far
greater than K. Thus, Theorem 6.1 is not satisfactory due to its dependence on T. To deal With this
problem, we further prove Theorem 5.2, which translates the dependence on T into the dependence
on K but has a worse dependence on the dimension d and other logarithmic factors.
Theorem 6.1 applies to the general cost function with p set to 0. Note that the regret upper bound
depends on the total number of time steps T, which is random. To replace the T-dependence by the
2f-dependence, it suffices to show that T = O(K). As mentioned in Section 5.1, this can be easily
derived under Assumption 5.1. We are now ready to prove Theorem 5.2.
Proof of Theorem 5,2. The total cost in K episodes is upper bound by RK + KB* and is lower
bounded by T ∙ Cmirι∙ TOgether With Theorem 6.1, With probability at least 1 — 5, we have
T ∙ Cmin ≤ 6Bτ{dTlog (1 H—ʌʌ
+ 7 (IB* log (T1 +

Solving the above inequality for the total number of steps T5 we obtain that
T = O
CnIin
KB* * B2d2
cmin
Plugging this into Theorem 6.1 yields the desired result.	□
Note that for the general cost functions, by simply picking p = K_1/3 the result immediately follows
from the case of positive costs, which is summarized in Theorem 5.4.
8
Under review as a conference paper at ICLR 2022
6.2	Proof Sketch of Theorem 6.1
The main steps in proving Theorem 6.1 include an analysis of EVI and a regret decomposition. The
complete proof can be found in Appendix D.
Analysis of EVI. By the algorithmic design we elaborated in Section 4, EVI guarantees optimism
and finite-time convergence, which is summarized in Lemma 6.3 below.
Lemma 6.3. Let P = O and Bt = By/dlog (4(t2 + i3B2∕λ)∕5) ÷ Vλd for all t ≥ 1, where
B > B*. Then with probability at least 1 —(5/2, for all j ≥ 1, EVI converges in finite time and the
following holds
O* ∈ G∙ G H , 0≤ Qj(∙,∙) ≤ Q*(∙√), and 0≤ %∙(∙) ≤ V*(-).
Note that in Lemma 6.3 the optimism only holds for the EVI output, i.e.5 Vj for any j > 1. The
initialization % in Line 2 of the main Algorithm 1 does not necessarily satisfy tħe optimism since it is
possible that ¼*(s) < 1 for some s. Still, such an initialization guarantees IMIIoO = 1 ≤ B*, which
is crucial for establishing the optimism for j ≥ 1. The proof of Lemma 6.3 is given in Appendix F.l.
Regret Decomposition. In our analysis, instead of dealing with (3.3) directly, we first implicitly
decompose the times steps into intervals, which are indexed by m = 1,... ,M in Lemma 6.4 below.
The basic idea here is to decompose all the time steps into disjoint intervals of which the end points
are either the end of an episode or the time steps when the EVI subroutine is triggered 3. The purpose
of such a regret decomposition is to guarantee that within each interval the optimistic action-value
function remains the same, so the induced policy. This is a necessary and common requirement
and can be found in the case of discounted infinite horizon MDPs (Zhou et al., 2021b). Similar
decomposition trick has also been used in existing works on SSP (Rosenberg et al., 2020; Rosenberg
& Mansour, 2020; Tarbouriech et al., 202lb).
Lemma 6.4. Assume the event in Lemma 6.3 holds, then we have the following upper bound for the
regret defined in (3.3) 4:
∙R(Λf) ≤ ∑m=l ^2h=l [cτn,h ÷ (sm3hi aπι1h) ~ (sτn,h)]
EI
+	月Mrl 必(m)(s-,∕ι+l)一叫九,，孙九)ɪ	伊」)
E2
+ 2dB^ log ɑ H + 2B* lθg(ɪ,) + 2.
Bounding E1 and E2 We bound the terms E∖ and E2 separately. Note that E2 is the sum
of a martingale difference sequence, and can be bounded by 0(，TlOg(T1∕B)) using standard
concentration. Bounding Ei is more technical and it requires almost dl the properties of our
algorithmic design. In detail, we need to show that every time when EVI is triggered, it can output an
optimistic action-value function estimator with high probability (by Lemma 6.3). Second, we need
to bound the total difference between the estimated functions and the optimal action-value function.
This follows from the elliptical potential lemma and the determinant-based doubling criterion. Third,
we need to bound the length of the epochs (i.e., the number of time steps between two EVIs), which
is achieved by the time-step doubling criterion as explained in Section 4.
7 Conclusions
In this paper, we proposed a novel algorithm for linear mixture SSP and proved its regret upper
and lower bounds. For future work, there are several important directions. First, there is a βθ∙5
gap between the current upper and lower bounds. We believe this gap can be closed by using a
Bernstein-type of confidence set (Zhou et al., 2021 a). Second, it remains open to prove a O(λ∕K)
regret bound for linear mixture SSPs for general cost functions when Cmin = O-
3The interval decomposition is indexed by m in Lemma 6.4. It is implicit and only for the purpose of
analysis. This is different from the epoch decomposition, which is explicit and indexed by j in Algorithm 1. The
difference is that an epoch ends when EVI is triggered, while an interval ends when either EVI is triggered or
the goal state g is reached (i.e., an episode ends).
^R(M) is the same as Rκ∙ We use a different notation to emphasize the interval decomposition.
9
Under review as a conference paper at ICLR 2022
Ethics statement
We donst see any potential ethical issues in our work.
References
Yasin Abbasi-Yadkori5 David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems, 24:2312-2320, 2011.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin5 Pieter Abbeel, and Wbjciech Zaremba. Hindsight experience replay. arXiv
preprint arXiv:1707.01495, 2017.
Alex j^oub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463T74. PMLR, 2020.
Dimitri Bertsekas. Dynamic programming and optimal control: Volume I, volume 1. Athena scientific,
2012.
Dimitri P Bertsekas and John N Tsitsiklis. An analysis of stochastic shortest path problems. Mathe-
matics of Operations Research, 16(3):580-595,1991.
Dimitri P Bertsekas and Huizhen Yu. Stochastic shortest path problems under weak conditions. Lab.
for Information and Decision Systems Report LIDS-P-2909t MIT, 2013.
Blai Bonet. On the speed of convergence of value iteration on stochastic shortest-path problems.
Mathematics OfOperations Research, 32(2):365-373, 2007.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimiza-
tion. In International Conference on Machine Learning, pp. 1283-1294. PMLR, 2020.
Liyu Chen and Haipeng Luo. Finding the stochastic shortest path with low regret: The adversarial
cost and unknown transition case. arXiv preprint arXiv:2102,05284, 2021.
Liyu Chen, Mehdi Jafamia-Jahromi, Rahul Jain, and Haipeng Luo. Implicit finite-horizon approxima-
tion and efficient optimal algorithms for stochastic shortest path. arXiv preprint arXiv:2106.08377,
2021a.
Liyu Chen, Haipeng Luo, and Chen-Yu Wei. Minimax regret for stochastic shortest path with
adversarial costs and known transition. In Conference on Learning Theory, pp. 1180-1215. PMLR,
2021b.
Alon Cohen, Haim Kaplan, Yishay Mansour, and Aviv Rosenberg. Near-optimal regret bounds for
stochastic shortest path. arXiv preprint arXiv:2002.09869, 2020.
Alon Cohen, Yonathan Efronis Yishay Mansour, and Aviv Rosenberg. Minimax regret for stochastic
shortest path. arXiv preprint arXiv:2103.13056, 2021.
Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F ¾ng. Is a good representation sufficient for
sample efficient reinforcement learning? In International Conference on Learning Representations,
2019.
Matthieu Guillot and Gautier Stauffer. The stochastic shortest path problem: a polyhedral combina-
torics perspective. European Journal of Operational Research, 285(1):148-158, 2020.
Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with
linear function approximation. In International Conference on Machine Learning, pp. 4171-4180.
PMLR, 2021.
Mehdi Jafamia-Jahromi3 Liyu Chen, Rahul Jain3 and Haipeng Luo. Online learning for stochastic
shortest path model via posterior sampling. arXiv preprint arXiv:2106.05335, 2021.
Thomas Jaksch5 Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal σf Machine Learning Research, 11(4), 2010.
10
Under review as a conference paper at ICLR 2022
Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang. Model-based reinforcement learning with
value-targeted regression. In Leamingfor Dynamics and Control, pp. 666-686. PMLR, 2020.
Chi Jin3 Zhuoran Yang3 Zhaoran Wang3 and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Andrey Kolobov, Mausam Mausam, Daniel S Weld, and Hector Geffner. Heuristic search for
generalized stochastic shortest path mdps. In Twenty-First International Conference on Automated
Planning and Scheduling, 2011.
Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In International Conference on
Algorithmic Learning Theory, pp. 320-334. Springer, 2012.
Shiau Hong Lim and Peter Auer. Autonomous exploration for navigating in mdps. In Conference on
Learning Theory, pp. 40-1. JMLR Workshop and Conference Proceedings, 2012.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics, pp. 2010-2020. PMLR, 2020.
Soroush Nasiriany, Vitchyr H Pong, Steven Lin, and Sergey Levine. Planning with goal-conditioned
policies. arXiv preprint arXiv:1911.08453, 2019.
Aviv Rosenberg and Yishay Mansour. Stochastic shortest path with adversarially changing costs.
arXiv preprint arXiv:2006.11561, 2020.
Aviv Rosenberg, Alon Cohen, Yishay Mansour, and Haim Kaplan. Near-optimal regret bounds for
stochastic shortest path. In International Conference on Machine Learning, pp. 8210-8219. PMLR,
2020.
Jean Tarbouriechs Evrard Garcelon, Michal λ^lko, Matteo Pirotta, and Alessandro Lazaric. No-regret
exploration in goal-oriented reinforcement learning. In International Conference on Machine
Learning, pp. 9428-9437. PMLR5 2020a.
Jean Tarbouriech5 Matteo Pirotta5 Michal Wlko, and Alessandro Lazaric. Improved sample complexity
for incremental autonomous exploration in mdps. In NeurIPS, 2020b.
Jean Tarbouriech, Matteo Pirotta, Michal Valko, and Alessandro Lazaric. Sample complexity bounds
for stochastic shortest path with a generative model. In Algorithmic Learning Theory, pp. 1157-
1178. PMLR, 2021a.
Jean Tarbouriech, Runlong Zhou, Simon S Du, Matteo Pirotta, Michal Valko, and Alessandro Lazaric.
Stochastic shortest path: Minimax, parameter-free and towards horizon-free regret. arXiv preprint
arXiv:2104,11186, 2021b.
Daniel Vial, Advait Parulekar, Sanjay Shakkottai, and R Srikant. Regret bounds for stochastic shortest
path problems with linear function approximation. arXiv preprint arXiv:2105. Ol593, 2021.
Ruosong Wangj Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value
function approximation: Provably efficient approach via bounded eluder dimension. Advances in
Neural Information Processing Systems, 33, 2020a.
Yining Wang, Ruosong Wang, Simon Shaolei Du5 and Akshay Krishnamurthy. Optimism in rein-
forcement learning with generalized linear function approximation. In International Conference
on Learning Representations, 2020b.
Lin Yang aɪɪd Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995—7004. PMLR, 2019.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746-10756. PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric.
Frequentist regret bounds for randomized least-squares value iteration. In International Conference
on Artificial Intelligence and Statistics, pp. 1954—1964. PMLR, 2020.
Zihan Zhang, Yuan Zhou, and Xiangyang JL Almost optimal model-free reinforcement Ieamingvia
reference-advantage decomposition. Advances in Neural Information Processing Systems, 33,
2020.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning
for linear mixture markov decision processes. In Conference on Learning Theory. PMLR, 202la.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted
mdps with feature mapping. In International Cortference on Machine Learning, pp. 12793-12802.
PMLR, 2021b.
12