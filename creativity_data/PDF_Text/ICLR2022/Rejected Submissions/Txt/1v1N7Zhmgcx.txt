Under review as a conference paper at ICLR 2022
Maximum Likelihood Training of Parametrized
Diffusion Model
Anonymous authors
Paper under double-blind review
Ab stract
Whereas diverse variations of diffusion models exist in image synthesis, the previ-
ous variations have not innovated on a topic of their diffusing mechanism that was
always kept linear. Meanwhile, it is intuitive that there would be more promising
diffusion patterns to result in a distribution closer to the approximating data dis-
tribution. This paper introduces such adaptive and nonlinear diffusion method for
score-based diffusion models. Unlike static and linear Variance Exploring (VE) or
Variance Preserving (VP) Stochastic Differential Equations (SDE) of the previous
diffusion models, our Parametrized Diffusion Model (PDM) learns the optimal
diffusion process by combining a normalizing flow and a diffusion process. Specif-
ically, PDM utilizes the flow to nonlinearly transform a data variable into a latent
variable, and PDM applies the diffusion process to the transformed latent variable
with the linear diffusing mechanism. Subsequently, PDM enjoys a nonlinear and
learned diffusion from the perspective of the data variable. We train PDM with a
variational proxy of the log-likelihood, and we prove that variational gap between
the variational bound and the log-likelihood becomes tight when the normalizing
flow is optimal.
1	Introduction
Diffusion models have recently achieved successes on a task of sample generations, and some
researches claim the state-of-the-art performance over Generative Adversarial Networks (GAN)
(Karras et al., 2019). This success is highlighted particularly in the community of likelihood-based
models, including normalizing flows (Grcic et al., 2021), autoregressive models (Parmar et al., 2018),
and variational autoencoders (VAE) (Vahdat and Kautz, 2020). Moreover, this success is noteworthy
because it is achieved merely using linear diffusing mechanisms, such as VESDEs (Song and Ermon,
2020) and VPSDEs (Ho et al., 2020).
This paper expands these linear diffusing mechanisms of VE/VPSDE to a data-adaptive trainable
nonlinear diffusion. To motivate the expansion, though there are structural similarities between
diffusion models and VAEs, the forward diffusion in a diffusion model has not been trained in existing
literature while its counterpart, which is the encoder of VAE, is trainable. Rather, the current diffusion
models assume the linear diffusing mechanism to be fixed throughout training procedure. Because
of this static nature of the diffusing mechanism, variational gap (Cremer et al., 2018) between the
log-likelihood and the Evidence Lower BOund (ELBO) remains to be strictly positive unless the
score perfectly estimates the data score. This variational gap prevents the score training from being
Maximum Likelihood Estimation (MLE). This gap is a fundamental motivation to develop a trainable
encoder of the diffusion model, so that the gap can be tight.
As we tighten variational gap by training a nonlinear forward diffusion, the biggest challenge comes
from an intractable optimization loss. The denoising diffusion loss requires a transition probability to
be closed-form in order to achieve fast optimization, but a nonlinear diffusing mechanism, in general,
has no closed-form perturbation probability. Hence, our innovation becomes designing a diffusion
model with a tractable loss while the diffusion is learnable and flexible, which we achieve by merging
previous diffusion models and the tractable variable transformation.
Our innovation concentrates on the theoretic and the practical aspects of a new diffusion, which is
tractable, learnable, and flexible. Theoretically, we observe that a nonlinear diffusing mechanism can
be transformed to a linear diffusion under an invertible transformation, and vice versa. In practice,
1
Under review as a conference paper at ICLR 2022
we implement PDM by merging a normalizing flow model and a diffusion model, which constructs a
tractable, learnable, and flexible nonlinear diffusing mechanism. From this construction, we prove
that variational gap can be further tightened by training the forward diffusion, nonlinearly. Also, we
demonstrate the state-of-the-art performance in CelebA on Frechet Inception Distance (FID) (Heusel
et al., 2017). We summarize our key contributions, as below.
•	PDM expands the scope of a forward diffusion process from linear and fixed dynamics to
nonlinear and trainable dynamics.
•	PDM minimizes variational gap between the log-likelihood and ELBO, lower than the gap
of linear diffusion models because PDM learns a data-adaptive nonlinear diffusion.
•	PDM explicitly proves in Theorem 2 that the optimal generative distribution equals to the
real distribution, which was not the case for a linear diffusion model.
2	Preliminary
A diffusion model is constructed with bidirectional stochastic processes. The forward direction
diffuses an input variable to a noise variable, and the reverse direction denoises the random noise to
construct a realistic input data instance. The diffusion model learns this reverse direction (generative
process) by estimating data score. We provide a brief summary of diffusion models, as below.
Forward Diffusion At the beginning of model build-up, a diffusion process requires a diffusing
mechanism on an input variable. This paper assumes that the diffusion process is governed by a SDE
of dxt = f (xt,t) dt + G(xt,t) dωt with x° 〜Pr, where Pr is the distribution from a real-world
dataset; and xt is the solution of the SDE.
Reverse Diffusion The theory of stochastic calculus guarantees that we could create an identical
diffusion process, {xt }tT=0, by solving an associated reverse SDE backwards in time (Anderson,
1982). The associated reverse SDE is
dxt = [f(xt,t) - G(xt,t)GT(Xt,t)Vχt logPt(Xt)] dt + G(xt,t)dωt, XT 〜PT,	(1)
where ωt and Bt are standard Wiener processes with time flows forward and backward, respectively;
and Pt is a probability law of Xt .
Generative Diffusion A diffusion model approximates the above SDE 1 to eventually yield an
estimation on the data distribution ofPr. In SDE 1, previous literature setup that drift and diffusion
terms, f and G, are determined a-priori in the forward diffusion. However, data score, Vxt logPt(Xt),
is intractable to compute, so we estimate this data score with a score network of sθ(Xt, t) in order to
mimic the reverse diffusion with our generative process. This score network approximates the reverse
diffusion by plugging the estimated score in place of data score with the below generative process:
dxθ = [f (xθ ,t)-G(xθ ,t)GT (xθ ,t)sθ (xθ ,t)]dt + G(xθ ,t)dω t, XT 〜π. (2)
The generative process starts from a prior distribution (π), and it constructs time-continuous random
variables xtθ by solving a SDE 2 backwards in time. The generated stochastic process is denoted by
{xtθ}tT=0, and we omit θ in the superscript if no confusion arises. With this generative process, we
define a generative distribution, xg 〜pθ, as the probability density of the generated random variable
at time t = 0.
Score Estimation We train the score network by a variational bound of the log-likelihood, given by
Eχo [ — logPθ(xo)] ≤L({xt}T=o,λ = g2; θ) - Eχ∕logπ(xτ)]
Z g2⑴Lt({xt}T=o;θ)
0
dt - ExT log π(xT) ,
(3)
where Lt {xt}tT=0; θ = Ex0,xt ksθ(xt, t) - Vxt logP0t(xt|x0)k22 up to a constant, where
P0t(xt|x0) is a transition probability from x0 to xt. Here, λ is a weighting function that deter-
mines the level of contribution for each diffusion time on the overall diffusion loss, L({xt}tT=0, λ; θ)
(Song et al., 2020). Variational bound holds when the weighting function is the likelihood weighting
(g) (Song et al., 2021), where G is a scalar-valued g function.
2
Under review as a conference paper at ICLR 2022
3	Motivation of Nonlinear Diffusing Mechanism
Though it has long been theoretically and empirically grounded to train the encoder part in VAE, such
solid ground is not accomplished in diffusion models. This section analyzes structural similarities
between a VAE model and a diffusion model, which brings the foundation of a data-adaptive nonlinear
diffusing mechanism.
3.1	Variational Gap of VAE
Given Negative ELBO (NELBO) of Negative Log-Likelihood (NLL) in VAE as
-logPθ(x) ≤Eqφ(z∣x)[- logPθ(x|z)] + Dκz(qφ(z∣x)kp(z))	(4)
=-logPθ(x) + DκL(qφ(z∖x)kpθ(z∣x)),	(5)
we have a pair of interpretations on this NELBO. First, when we focus on Eq. 4, NELBO 1) aids
data reconstruction by optimizing Eqφ(z∣χ) [ — logpθ(x∖z)]; and NELBO 2) regularizes the inference
distribution of the encoder to a prior distribution by DKL qφ(z∖x)kp(z) . On the other hand, if we
concentrate on Eq. 5, NELBO bounds NLL by approximating an intractable decoder posterior of
pθ(z∖x) with a tractable encoder posterior of qφ(z∖x). A vanilla VAE (Kingma and Welling, 2013)
assumes this approximate posterior to be a Gaussian distribution with mean and diagonal covariance
estimated by amortized inference: qφ(z∖x) = N(z; μφ(x), σφ(X)I). By expanding the flexibility of
this approximate posterior into a variational family of general distributions, for instance, Rezende and
Mohamed (2015) resulted in tighter NELBO, which leads the optimization of VAE closer to MLE,
and their choice of a generalizable model was normalizing flow.
To connect this NELBO to a diffusion loss, we restate NELBO in the language of a stochastic
process. Having that VAE attains a stochastic process of bivariate random variables, {x, z}, NELBO
is reformulated to a KL divergence between two joint distributions modeled in bidirectional ways:
DKL(prkpθ) ≤DKL(prkpθ) + Epr(x) DKL(qφ(z∖x)kpθ(z∖x))
=Epr(X [log pθ(⅛+Eqφ(ZIx) hlog qφ⅛R i
=Epr (x)qφ (z|x)
Pr(X)qφ(ZIX)-
gPθ (x)Pθ (z∖x).
(6)
=DKL(qφ(x, z)kpθ(x, z)),
The inequality 6, i.e., DKL(prkpθ) ≤ DKL(qφ(x, z)kpθ(x, z)), is well-known by itself in the field
of information theory (Duchi, 2016) by the name of data processing inequality. This restated bound
interprets that VAE is essentially a bimodeling approach of a joint distribution on the bivariate
stochastic process, {x, z}. On the forward direction (x → z), a latent variable is conditioned on
a data variable, and the joint distribution is modeled by qφ(x, z) = pr(x)qφ(z∖x). On the reverse
direction (z → x), a generative data variable is conditioned on the latent variable, and the joint
distribution is modeled bypθ(x, z) = p(z)pθ(x∖z). Under this equivalent framework, we present an
analytic tool to measure the closeness of NELBO and NLL (see Table 5) by
Gap(qφ,pθ) =NELBO -NLL
=DKL(qφ(x,z)kpθ(x,z)) - DKL(prkpθ)
=Epr(x) DKL(qφ(z∖x)kpθ(z∖x)),
and we denote this quantity as variational gap (Cremer et al., 2018). Though it is one of central
tasks in VAE to minimize this variational gap (Kingma et al., 2016), only limited works (Burda et al.,
2015; Neal, 2001) have estimated variational gap due to its computational burden. We introduce the
tractable computation of variational gap in the next section on diffusion models.
3.2	Variational Gap of Diffusion Models
This section constructs an analogy of a diffusion model to VAE. To begin with, we observe that
the classical data processing inequality in VAE holds on a stochastic process with bivariate random
variables on a finite-dimensional Euclidean space. On the other hand, the time horizon of a diffusion
3
Under review as a conference paper at ICLR 2022
Figure 1: Schematic view of PDM. Normalizing flow transforms a data variable to a latent variable,
and a linear diffusion (VE/VPSDE) diffuses the latent variable to a prior distribution. Data generation
starts from the prior distribution, and the generation denoises randomly generated noise to a latent
variable, and the generation eventually transforms the latent variable into the data space.
model is continuous, so it contains an infinite number of random variables in its stochastic process
as {xt}tT=0. Fortunately, the finite-dimensional classical data processing inequality on the bivariate
random variables is directly extendable to a stochastic process with a continuous index. Formally,
an infinite-variate joint distribution is called a path measure (Oksendal, 2013), which is defined as a
probability law on a space of continuous functions induced by {xt}tT=0. Therefore, the path measure
is an alternative concept of a joint distribution for a stochastic process with a continuous index. The
path measure for the forward diffusion in a diffusion model corresponds to a joint distribution for the
encoder part in VAE, and similarly, the path measure for a generative diffusion corresponds to the
decoder part in VAE. If μ is the forward (inference) path measure, and if vθ is the generative path
measure; then the data processing inequality becomes
DKL(Pr kpθ) ≤ DκL(μ∣∣Vθ).
The real distribution of Pr is a marginal distribution of μ at t = 0, and the generative distribution of
pθ becomes a marginal distribution of νθ at t = 0. Under this concept, the data processing inequality
means that a distributional divergence of DKL (μ∣∣Vθ) between two path measures lower bounded by
a divergence of DKL (Pr kPθ) between two sliced path measures at t = 0.
Previous works (Song et al., 2021; Huang et al., 2021) regard this variational bound being equivalent
to a diffusion loss,
DκL(μkνθ) = L({χt}τ=0,g2; θ) + DKL(PT IIn),
where G(xt, t) = g(t) is a scaler function on t, and λ(t) is a square of the diffusion term g2 (t).
Since the previous diffusion models optimize the diffusion loss, the KL divergence of DKL (μ∣∣"θ)
is an equivalent target of optimization on the diffusion loss. Subsequently, we can credit variational
gap as a training performance to tighten NELBO to NLL by
Gap(μ, vθ) = Dkl(μ∣Vθ) — DKL(Pr∣Pθ)∙
This variational gap satisfies (see Appendix B.2)
Gap(μ, νθ) = Epr (χo ) [DKL(μ({xt}|xo)kνθ({xt}|xo))],
so variational gap is minimized if an inference posterior, "({xt}∣x0), estimates the intractable
generative posterior, vθ({xt}∣xo). However, unlike VAE, the inference posterior is not trainable,
which restricts variational gap to be strictly positive in previous diffusion models. We note that a
diffusion model provides tractable NLL using the Instantaneous Change of Variable (Song et al.,
2020), and this tractable variational gap becomes useful in analyzing a diffusion model quality.
4	Parametrized Diffusion Model
4.1	No Closed-form Transition Probability on Nonlinear Diffusion
As far as we know, all previously proposed SDEs, including VESDE and VPSDE, are categorized as
linear SDEs because they have linear drift term of f (xt , t). This drift term determines the dynamics
of the diffusion, so a data-adaptive diffusion could be attained simply by parametrizing drift and
diffusion terms. However, a transition probability with nonlinear coefficients for a SDE, P0t(xt|x0),
is intractable in general, and a diffusion loss becomes intractable to optimize. In fact, it is worth
noting that a closed-form of the transition probability exists only on a certain type of SDEs, like
VESDE and VPSDE.
4
Under review as a conference paper at ICLR 2022
dχf = [f0(χf,t)- GGTSe(xf,i)] dt + Gφ(x≠ i) dωt
INonlinear GeneratiVe Path (noise → data)]
[Nonlinear Inference Path (data → noise)]
dxf = %(xf, t) dt + G0(xf") dωt
Invertible Path
Zo = h°(x0)
dzt = -∣^(t)ztdi + p(i) dωt
Linear Inference Path (latent → noise))
Invertible Path
xο,θ = hψ1(zο)
dz? =[- ɪ/^(i)zf -52(i)sθ(zf,i)] dt + g(t) dωt
Linear Generative Path (noise → latent)]

Figure 2: PDM attains a ladder structure between a data space and a latent space by the invertibility of
normalizing flow. A score network estimates data score following blue arrows, and a model generates
images following red arrows.
4.2	Parametrized Forward Diffusion with Normalizing Flow
To construct a tractable transition probability while keeping nonlinearity, we observe that a nonlinear
diffusion can be decomposed into a linear diffusion and a nonlinear invertible transformation, where
the transformation provides the flexible nonlinearity on a diffusion process. Figure 1 and 2 provide a
schematic view of our model. Normalizing flow transforms a data input to a latent variable, and a
diffusion model linearly permeates the latent variable to a noise distribution.
Concretely, let us define z0 to be a transformed latent variable, z0 = hφ (x0), where φ is flow model
parameters. Then, a canonical diffusion with linear dynamics of
dzt = - 1 β(t)zt dt + g(t) dωt,	z0 = hφ(x0) with x0 〜Pr,
2
describes a diffusing mechanism of a latent variable. Since the transformation is invertible, if we
define an inverse random variable by xtφ = hφ-1(zt), the dynamics of a stochastic process of {xtφ}tT=0
is governed by a SDE of
dxφ = fφ (xφ ,t) dt + Gφ (xφ, t) dωt,	xφ 〜Pr,
where drift and diffusion terms are derived by
(fφ (Xt ,t)	= -1 β [Vhφ]-1hφ + 1 g2tr(V2h-1)
I Gφ(Xt,t) = g [Vhφ]T,
from the Ito’s lemma (Oksendal, 2013). See full derivation and explanation in Appendix B.1. This
diffusion process is induced diffusion on the data space that is defined indirectly by transforming
back from the latent space to the data space. The nonlinearity of this induced diffusion fully depends
on the invertible transformation as illustrated in Figure 2.
It is not straightforward to directly construct a generative reverse process on the data space, Xtφ,θ .
Figures 1 and 2 build the generative process by red arrows using the transformation. Starting from a
prior distribution of π, we denoise a random variable from the prior to z0θ according to the generative
process of the linear diffusion on the latent space, and we transform the fully denoised latent of z0θ
back to the data space X0φ,θ = hφ-1(z0θ).
4.3	Tractable Variational Bound of Parametrized Diffusion Model
The joint modeling of normalizing flow and diffusion enables a tractable training of nonlinear
dynamics. The log-likelihood on a data space reduces to the log-likelihood on a latent space by
adding the log determinant of Jacobian to the latent log-likelihood. This calculation of log-likelihoods
provides Theorem 1 that bounds the log-likelihood with a tractable ELBO.
Theorem 1. Suppose thatPφ,θ(X0) is the log-likelihood of a generative random variable X0φ,θ. Then,
the log-likelihood is bounded by
Epr(x0) [-logPφ,θ(X0力 ≤ L({xt}T=0,g2； {φ, θ}),
5
Under review as a conference paper at ICLR 2022
where
L({xt}τ=0,g2; {φ, θ}) = - EPr(X0)h log I det (d∂χφ0 )|i + L({zt}τ=0,g2; θ) - EZT [ log π(ZT 儿
0	(7)
with L({zt}T=ο,g2; θ) = R0tg2(t)Ezο,zt [ksθ(zt,t) - VztlogPot(zt∣zο)k2] dt, up to a constant.
Here, p0t(zt|z0) is a transition probability of the linear diffusion process in the latent space.
PDM ELBO in Eq. 7 is different from the vanilla diffusion ELBO in Eq. 3 from three perspectives.
First, the normalizing flow loss, i.e., the log determinant of Jacobian, is added to the loss of PDM
ELBO. This normalizing flow loss represents a transformation of a data space into a latent space.
Second, the remaining terms, related to the diffusion model, is a linear diffusion on the latent space
with linear dynamics, which provides tractable optimization, rather than an intractable data diffusion
of nonlinear dynamics. Third, the optimization loss in Eq. 7 estimates the score on the latent space
by a score network of sθ(zt, t), while the diffusion loss in Eq. 3 estimates the score on the data space
by a score network of sθ (xt, t).
4.4	Optimization Losses
Previous works (Ho et al., 2020; Song et al., 2021; Vahdat et al., 2021) have empirically demonstrated
that training a diffusion model with a likelihood weighting, λ(t) = g2 (t), could bring worse
performances than applying other weighting functions. For instance, Ho et al. (2020); Song and
Ermon (2020) introduce a variance weighting function, λ(t) = σ2(t), and Ho et al. (2020) show that
this weighting outperforms a likelihood weighting in terms of the sample generation.
Within our framework, we also observe that a variance weighting function is advantageous over a
likelihood weighting. Henceforth, we optimize a flow model and a diffusion model with different
losses as follows:
Flow: min L({xt}tT=0, g2; {φ, θ})
φ
Diffusion: min L({zt}tT=0, {g2 or σ2}; θ),
θ
Note that the first term of the log Jacobian and the third term of the cross entropy in Eq. 7 are
independent to score parameters, θ. Therefore, diffusion parameters are not influenced by these
terms, in which only the second term remains to update the diffusion parameters.
4.5	Tight Variational Gap of Parametrized Forward Diffusion
This section theoretically analyzes variational gap of PDM. To construct variational gap of PDM, we
enumerate few nomenclatures for path measures (see Table 6). On a data space, μφ is a path measure
for an inference process; and νφd,θ is a path measure for a generative process. On a latent space,
μlφ is a path measure for an inference process; and νθ is a path measure for a generative process.
Here, νθl does not depend on φ because the generative diffusion on the latent space starts from the
φ-independent noise distribution (π); and because the diffusion proceeds a denoising process by
following a score estimation of sθ(zt, t) that depends only on θ.
Variational gap on the data space becomes
Gapd(μφ, νφ,θ) =NELBO - NLL	(8)
=DKL3ΦkνΦ,θ ) - DKL(Pr kPφ,θ ),	⑼
where the subscript in Gapd represents the data space. Recall that training the encoder in VAE
guarantees tighter variational gap because the encoder distribution of qφ (x, z) approaches toward the
decoder distribution ofpθ(x, z) by minimizing NELBO of DKL qφ (x, z)kpθ (x, z) . However, this
analogy is not valid in variational gap in Eq. 8 because both inference and generative path measures
depend on φ. This means that the generative path measure could arbitrarily deviate from the inference
path measure when we optimize φ.
6
Under review as a conference paper at ICLR 2022
Using the change of variable, however, variational gap is equivalent to
GaPd(μφ, νφ,θ ) =DKL(μφkνφ,θ ) - DKL (Pr kPφ,θ )
= DKL(μφkνθ) - DKL(pφkpθ)
=GaPl (μφ, νθ).
With this derivation, optimizing an invertible transformation guarantees tighter variational gap because
the generative measure of νθl is now fixed on φ. Also, we note that this variational gaP reduces to a
KL divergence between the inference Posterior and the generative Posterior by
GaPd(μφ, νφ,θ) = GaPl(μφ, νθ) = Ezo [DKL (μφ({zt}lzo)kνθ HZtHZo))],	5 * * * * (IO)
which Provides another analogy to VAE. We enumerate variational gaPs of the exPlained models in
Table 5. In fact, we show in Theorem 2 that the oPtimum of PDM guarantees that the generative
distribution Pφ*,e equals to the data distribution Pr, and variational gap reduces to zero in the
oPtimum. See APPendix A for Proofs.
Theorem 2. Suppose a flow network is flexible enough to transform Pr to arbitrary continuous
distribution. Forany θ, if V∙Sθ(∙,t) is symmetricfor any t ∈ [0,T] and ▽. log π(∙) ≡ sθ(∙, T), then
there exists φ* such that Gapd(μφ*, νφ* θ) = 0. Furthermore, we have
φ* = arg min GaPd (μφ νd,θ) = arg min DKL("φ∣∣νφ,e) = arg min DKL(PrkPφ,θ).
φφφ
To the best of our knowledge, Theorem 2 is the first analysis in the community of diffusion models
that the theoretic oPtimum of a model comPletely recovers the data distribution. To Proceed this
argument, let us consider a linear diffusion model in a data sPace with no flow model. Then, a
marginal distribution of the forward Process is PT at t = T, which differs from a Prior distribution
of π. In addition, suPPose that the diffusion model reaches to the oPtimum Point, i.e., its score
network estimates the exact data score by sθ* (xt, t) = Vxt logPt(xt). Then, a generative transition
Probability equals to the groundtruth (reverse) transition Probability of PT0(x0|xT). Therefore,
the generative distribution, pe* (xo) = ∏∏(xτ)pτ0(x0∣xτ) dxτ, naturally diverges from the real
distribution, Pr(x0) = PT(xT)PT0(x0|xT) dxT, because of the mismatch of PT and π. This
imPlies that the original linear diffusion model cannot estimate the real distribution, Pr (x0). In
contrast to the original diffusion model, aPPlying normalizing flow to a diffusion model enables PDM
to recover the real distribution by Theorem 2. See Figures 8 and 9 for further illustration.
5 Related Work
Recently, a diffusion model has been jointly modeled in conjunction
with other deeP generative models. There are two major Previous
works on this line of research. First, Vahdat et al. (2021) (LSGM) in Figure 3: LSGM.
Figure 3 aPPlies a diffusion model to the latent sPace of VAE. Since VAE does not attain an invertible
structure, LSGM is not suggesting the exPansion of diffusing mechanisms. In other words, LSGM
cannot Provide diffused data learning at the middle of the Process because LSGM does not induce the
diffusion Process on the data sPace due to the lack of VAE’s invertibility. Consequently, the analyzing
tool for LSGM should be variational gaP of VAE, rather than that of the diffusion model.
%。
Inference Path
I (data→ noise)
Flow
I y∣l = xll u. uz p； UIxiJ
UiiPftr Diffnsivin
、i Forward)
,0 ；-------------------------«
Generative Path
(noise → data)
Θl.ir∣∩r∣.∣, I.>iITiIsion
(Crncrativc)
Figure 4: ScoreFlow.
(D
Second, Song et al. (2021) (ScoreFlow) in Figure 4 simPly con-
catenate normalizing flow on toP of a diffusion model to Provide a
dequantized data distribution only at the initial diffusion time. De-
sPite of the structural similarity of ScoreFlow and PDM, a major
difference of ScoreFlow and PDM lies on their density model con-
structions: Pθ in Scoreflow and Pφ,θ in PDM. ScoreFlow constructs
Pθ only with a diffusion model, so the flow and the diffusion networks are not jointly combined in
the loss function of RPr (x) /0》qφ(u∣x) log M(x+u) dudx, where U is a dequantization variable
that lies in a unit cube, [0, 1)d. In other words, ScoreFlow adoPts the flow outside of a model density,
so the flow is indePendent to the model density. In fact, ScoreFlow can be best understood as a
dequantization technique (Ho et al., 2019), whose goal is estimating the oPtimal dequantizing region
7
Under review as a conference paper at ICLR 2022
Table 1: Performance comparison on CIFAR-10.
SDE	Name	Model	NLL	NELBO	Gap	FID
V 口	NCSN++	Diff Baseline	3.45	4.43	0.98	2.20
VE	PDM (FID)	Diff & Flow	3.04	3.09	0.05	3.36
	DDPM++ (FID)	Diff Baseline	3.13	3.29	0.16	3.16
	PDM (FID)	Diff & Flow	3.04	3.09	0.05	6.79
	PDM (FID, dec)	Diff & Flow-v2	3.11	3.17	0.06	4.43
	DDPM++ (NLL)	DiffBaseline	3.05	3.22	0.17	9.68
	PDM (NLL)	Diff & Flow	2.98	2.99	0.01	9.29
	PDM (NLL, dec)	Diff & Flow-v2	2.96	2.99	0.03	7.23
VP	DDPM++ (NLL, IS)	Diff Baseline	2.91	3.10	0.19	5.70
	PDM (NLL, IS)	Diff & Flow	2.99	3.01	0.02	8.94
	PDM (NLL, dec, IS)	Diff & Flow-v2	2.94	2.95	0.01	6.84
	LSGM-109M (balanced)	Diff & VAE	-	2.96	-	4.60
	LSGM-480M (balanced)	Diff & VAE	-	2.95	-	2.17
	LSGM-480M (NLL)	Diff & VAE	-	2.87	-	6.89
	LSGM-480M (FID)	Diff & VAE	-	3.43	-	2.10
Table 2: Performance comparison on CelebA.
Model	NLL	NELBO	Gap	FID
NCSN++ (VE)	2.39	3.96	1.57	3.95
PDM (VE, FID)	2.00	2.09	0.09	2.50
DDPM++ (VP)	1.79	2.19	0.40	3.03
PDM (VP, FID)	2.04	2.10	0.06	2.23
UDM	1.93	-	-	2.78
DDGM	-	-	-	2.92
CR-NVAE	-	1.86	-	-
NCP-VAE	-	-	-	5.25
DenseFlow-74-10	1.99	-	-	-
Styleformer	-	-	-	3.66
on [0, 1)d to minimize dequantization gap (Hoogeboom et al., 2020). Therefore, variational gap of
ScoreFlow remains to be strictly positive because training the flow does not optimize the forward
diffusing mechanism. On the other hand, PDM constructs flow-embedded model density of pφ,θ, and
PDM optimizes the loss of pr(x) log [0,1)d pφ,θ(x + u) dudx that puts normalizing flow inside
of the model density. Although we train PDM with a uniform dequantization (Theis et al., 2015) by
default, we could apply the variational dequantization to PDM as well.
6	Experiments
This section quantitatively and qualitatively analyzes suggested PDM on CIFAR-10 (Krizhevsky
et al., 2009) and CelebA (Liu et al., 2015) 64 × 64. We compute NLL for density estimations in
bits per dimension scale by solving probability flow (Song et al., 2020) backwards in time, and we
compare baseline performances of Song et al. (2020; 2021) computed on a uniform dequantization.
For sample generations, we use the Predictor-Corrector algorithm (Song et al., 2020) for PDM on
VESDE, and we use the numerical ODE sampler of the probability flow (Song et al., 2020) with the
RK45 method on VPSDE. Throughout experimental results, we use NCSN++ (VE) and DDPM++
(VP) (Song et al., 2020) as backbones of diffusion models, and a pair of ResNet-based flow models
(Chen et al., 2019; Ma et al., 2020) as backbones of flow models. We give a full description on
models for our experiments in Appendix C. We name PDM (FID) and PDM (NLL) for experiments
that use weighting functions as σ2 and g2, respectively.
6.1	Quantitative Results
We compare PDM to baselines in Table 1 on VESDE and VPSDE as linear diffusing mechanisms.
Although minor performance degradation on the sample performance, Table 1 empirically demon-
strates reduced variational gap. In all experimental settings, variational gaps significantly drop less
than 0.1. On the other hand, variational gaps of the baseline diffusions on VPSDE for the FID model
and the NLL model are 0.16 and 0.19, respectively. We conclude that PDM effectively decreases
variational gap by training a parametrized inference path measure.
Table 2 compares experiments on CelebA to the current state-of-the-art baseline models in various
disciplines: the diffusion model of UDM (Kim et al., 2021) and DDGM (Nachmani et al., 2021), the
VAE model of CR-NVAE (Sinha and Dieng, 2021), the flow model OfDenseFlow (Grcic et al., 2021),
and the GAN model of Styleformer (Park and Kim, 2021). We observe a pair of implications. First,
training a diffusing mechanism with PDM largely reduces variational gap. In particular, PDM reduces
this inference gap from 1.57 to 0.09 on the NCSN++ backbone with VESDE and from 0.40 to 0.05 on
the DDPM++ backbone with VPSDE. Also, we emphasize that NELBO of PDM largely outperforms
NELBO of the baseline models. Second, our PDM reports the state-of-the-art performance with 2.23
FID score in the CelebA sample generation. This opposite trend on FID (compared to the results in
CIFAR-10) indicates that a nonlinear diffusion is more effective on datasets of higher dimensions.
In fact, this is a quite intuitive result because as the data dimension increases, the family of feasible
diffusion expands exponentially, and the data-optimal diffusing mechanism is more likely to deviate
from linear diffusions.
8
Under review as a conference paper at ICLR 2022
(a) Test NELBO
Figure 5: Comparison of the linear and nonlinear diffusing mecha-
nisms on a shallow network.
(b) Score scale
Figure 6: Comparison of the
linear diffusion model and
PDM on a deep network.
Figure 7: Non cherry-picked samples from latent/data.
Table 4: Ablation study on flow layers.
	L (# params)	NLL	NELBO	Gap
PDM	4 (1.2M)	3.19	3.30	0.11
ResFlow		5.72	-	-
PDM	8 (2.3M)	3.19	3.24	0.05
ResFlow		4.48	-	-
PDM	32 (10.6M)	3.04	3.09	0.05
ResFlow		3.72	-	-
6.2	Qualitative Results
Test NELBO To figure out an effect of a nonlinear diffusing mechanism on the training procedure,
Figure 5 (a) shows test NELBO by training iterations on a diffusion model with 35M parameters.
Without any modification on hyperparameters, we train PDM with a shallow ResFlow (Chen et al.,
2019) of 3M parameters, and Figure 5 (a) shows that the training on PDM surprisingly stabilizes test
NELBO. Figure 5 (b) illustrates that such training stability can be attributed to nearly identical score
scales. As described in Figure 5 (b), the score scale on a data space is unbounded as the diffusion time
goes to zero, and this extreme scale variation eventually leads to training instability on the baseline
diffusion models. On the other hand, adopting flow transformation on a data space mitigates such
extreme scale variations that destabilize training.
Figure 6 shows test NELBO on a diffusion model with 107.6M number of parameters. To stabilize
the diffusion model, we apply the AdamW optimizer (Loshchilov and Hutter, 2017) with 0.01 weight
decay, and Exponential Moving Average (Song et al., 2020) of 0.9999. Using the ResFlow model
with 10.6M parameters, Figure 6 indicates that PDM consistently outperforms the baseline diffusion
in terms of NELBO with smaller variations.
Flow Depth The flow model architecture is a building block that fundamentally determines the
family of inference path measures. Table 4 empirically demonstrates that an inflexible variational
family results in large variational gap on CIFAR-10. Additionally, the absolute magnitude of NLL
performance keeps decreasing as the number of layers (L) increases.
Sample Generation Figure 7 presents samples from (a) a latent space and (b) a data space. Since a
sample generation on the latent space is unnormalized, we normalize the sampled latent variables to
be bounded on the range of [0, 255], so we can visualize the latent information in image. Figure 7
indicates that a diffusion model and a flow model have distinctive and complementary roles in sample
generation: 1) the diffusion model constructs the global context and 2) the flow model enriches this
global context by colorizing it into a realistic image.
7	Conclusion
This paper expands the linear diffusing mechanism to be nonlinear by combining an invertible
transformation and a diffusion model. This nonlinear diffusing mechanism learns the forward
diffusion process out of variational family of inference path measures, and such optimization provides
tighter variational gap compared to a baseline model with linear diffusion.
9
Under review as a conference paper at ICLR 2022
8	Ethics Statement
As our work improves sample quality, our methodology could be a source of fake datasets, i.e. fake
images. This is a common threat from the line of deep generative modeling.
9	Reproducibility Statement
Every proofs and derivations are explained in details on Appendix A and B. Also, for reproducibility,
we would release our code as soon as the discussion period opens through an anonymous repository
to reviewers. We note that most of our code is built based on the released code of Song et al. (2020)
in order to compare with baselines in a fair setting introduced in Song et al. (2020) and Song et al.
(2021). Additionally, we provide the details on training/evaluation/neural architecture in Appendix C.
References
Anderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their
Applications ,12(3):313-326.
Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519.
Chen, R. T., Behrmann, J., Duvenaud, D., and Jacobsen, J.-H. (2019). Residual flows for invertible
generative modeling. arXiv preprint arXiv:1906.02735.
Cremer, C., Li, X., and Duvenaud, D. (2018). Inference suboptimality in variational autoencoders. In
International Conference on Machine Learning, pages 1078-1086. PMLR.
Do Carmo, M. P. (2012). Differential forms and applications. Springer Science & Business Media.
Duchi, J. (2016). Lecture notes for statistics 311/electrical engineering 377. URL: https://stanford.
edu/class/stats311/Lectures/full_notes. pdf. Last visited on, 2:23.
Grcic, M., Grubisic, I., and Segvic, S. (2021). Densely connected normalizing flows. arXivpreprint
arXiv:2106.04627.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). Gans trained by a
two time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30.
Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. (2019). Flow++: Improving flow-based
generative models with variational dequantization and architecture design. In International
Conference on Machine Learning, pages 2722-2730. PMLR.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. arXiv preprint
arXiv:2006.11239.
Hoogeboom, E., Cohen, T. S., and Tomczak, J. M. (2020). Learning discrete distributions by
dequantization. arXiv preprint arXiv:2001.11235.
Huang, C.-W., Lim, J. H., and Courville, A. (2021). A variational perspective on diffusion-based
generative models and score matching. arXiv preprint arXiv:2106.02808.
Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pages 448-456.
PMLR.
Karras, T., Laine, S., and Aila, T. (2019). A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 4401-4410.
Kim, D., Shin, S., Song, K., Kang, W., and Moon, I.-C. (2021). Score matching model for unbounded
data score. arXiv preprint arXiv:2106.05527.
10
Under review as a conference paper at ICLR 2022
Kingma, D. P. and Dhariwal, P. (2018). Glow: Generative flow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039.
Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016).
Improved variational inference with inverse autoregressive flow. Advances in neural information
processing systems, 29:4743-4751.
Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114.
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.
Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. (2019). Improved precision and
recall metric for assessing generative models. arXiv preprint arXiv:1904.06991.
Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep learning face attributes in the wild. In
Proceedings of the IEEE international conference on computer vision, pages 3730-3738.
Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101.
Ma, X., Kong, X., Zhang, S., and Hovy, E. (2020). Decoupling global and local representations via
invertible generative flows. arXiv preprint arXiv:2004.11820.
Nachmani, E., Roman, R. S., and Wolf, L. (2021). Non gaussian denoising diffusion models. arXiv
preprint arXiv:2106.07582.
Neal, R. M. (2001). Annealed importance sampling. Statistics and computing, 11(2):125-139.
Nichol, A. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. arXiv
preprint arXiv:2102.09672.
Oksendal, B. (2013). Stochastic differential equations: an introduction with applications. Springer
Science & Business Media.
Park, J. and Kim, Y. (2021). Styleformer: Transformer based generative adversarial networks with
style vector. arXiv preprint arXiv:2106.07023.
Parmar, G., Zhang, R., and Zhu, J.-Y. (2021). On buggy resizing libraries and surprising subtleties in
fid calculation. arXiv preprint arXiv:2104.11222.
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and Tran, D. (2018). Image
transformer. In International Conference on Machine Learning, pages 4055-4064. PMLR.
Rezende, D. and Mohamed, S. (2015). Variational inference with normalizing flows. In International
conference on machine learning, pages 1530-1538. PMLR.
Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pages 234-241. Springer.
Rudin, W. et al. (1964). Principles of mathematical analysis, volume 3. McGraw-hill New York.
Sinha, S. and Dieng, A. B. (2021). Consistency regularization for variational auto-encoders. arXiv
preprint arXiv:2105.14859.
Song, Y., Durkan, C., Murray, I., and Ermon, S. (2021). Maximum likelihood training of score-based
diffusion models. arXiv e-prints, pages arXiv-2101.
Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution.
arXiv preprint arXiv:1907.05600.
Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models.
arXiv preprint arXiv:2006.09011.
11
Under review as a conference paper at ICLR 2022
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2020). Score-based
generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.
Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ra-
mamoorthi, R., Barron, J. T., and Ng, R. (2020). Fourier features let networks learn high frequency
functions in low dimensional domains. arXiv preprint arXiv:2006.10739.
Theis, L., Oord, A. v. d., and Bethge, M. (2015). A note on the evaluation of generative models.
arXiv preprint arXiv:1511.01844.
Vahdat, A. and Kautz, J. (2020). Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898.
Vahdat, A., Kreis, K., and Kautz, J. (2021). Score-based generative modeling in latent space. arXiv
preprint arXiv:2106.05931.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,七.，and
Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing
systems, pages 5998-6008.
12
Under review as a conference paper at ICLR 2022
A Proofs of Theorem 1 and Theorem 2
Theorem 1. Suppose that pφ,θ (x0) is the log-likelihood of the generative random variable x0φ,θ.
Then, the log-likelihood is bounded by
EPr(X0)[ - log Pφ,θ (XO)] ≤ Lφ,θ ({xt}T=0; g2),
where
LΦ,θ ({xt}t=0; g ) = - Epr(X0)h log Idet( dhxφ)n+Lθ ({zt}T=0; g2) - EzT log π(zT )
+2Z t e(t)- 舞;
2 0 σ (t)
with σ2 (t) = 1 - e- R0t β(s) ds + R0t [g2 (s) - β(s)] ds.
Proof of Theorem 1. From the change of variable, the transformation of z0 = hφ(x0) induces
pr(x0)
Po(zo)
det
and thus the entropy of the data distribution becomes
H(Pr) = - Pr(xO) logPr(xO) dxO
=- /Pθ(zο)log -__PO(ZO)
I det
-	pr(x0) log III det
-Epr (XO) h logl det ( ∂χφ
p0(z0) logp0(z0) dz0
-	p0(z0) logp0(z0) dz0
0
-Epr(x0) log II det
+ H(p0).
From Theorem 4 of Song et al. (2021), the entropy at t = 0 equals to
1T
H(PO) = H(PT) - 2 / Ept(Zt) RvZt ∙ f (Zt,t) + g2(t)kvzt logPt(Zt) k2] dt,
where f(zt, t) is a drift term of the diffusion for zt and pt is the probability distribution of zt.
Therefore, the negative log-likelihood becomes
-Epr(x0) [ log Pφ,θ (xO) = DKL(PrIPφ,θ) + H(Pr)
≤ DκL(”φkνφ,θ) + H(Pr)
+ H(p0)
DKL(μφkνφ,θ) - Epr(xο) h log I det
DKL(μφkνθ) - Epr(xο) h log l det (∂χφ) J]
+ H(pT )
1T
-2 J Ezφ [ - dβ(t) + g2(t)kVzt logpt(zt)∣∣2] dt.
Now, from Theorem 1 of Song et al. (2021), the KL-divergence between the path measures becomes
1T
DκL(μφkνθ) = dkl(ptIIn) + 2 / g2⑴Ept(Zt/II%(zt,t) -Vzt logPt(Zt)k2]dt,
13
Under review as a conference paper at ICLR 2022
so if we plug in this into the negative log-likelihood, we yield the following:
—Epr(x0) [ logPφ,θ(x0)]
=-Epr (xo)[ log I det (∂Xφ∙ )|i +2 / g2 (t)Ezt [ksθ (Zt,t) — Vzt log Pt(Zt)k2] dt
1T
+Dkl(ptk∏) + H(PT) — 2 J Ezt [ — dβ(t) + g2(t)kVzt logPt(Zt)k2]dt
=—Epr(xo) h log I det (∂Xφ∙)∣] — Ezτ [log ∏(zt)] + d J： β(t) dt
+ 2 Z0 g2(t)Ezt [ksθ(Zt,t) — Vzt logPt(zt)k2 — kVzt logPt(zt)k2] dt
Also, we have
Ezt [sθ(zt,t) ∙ Vzt logPt(Zt)] = /Pt(Zt)Sθ(zt,t) ∙ Vzt logPt(Zt) dzt
=/ Sθ(Zt,t) ∙ VztPt(Zt) dZt
= / Sθ(Zt,t) ∙ /P0(Z0)Vztpot(Zt∣Z0)dZ0 dZt
=Ezo Ezt ∣zο [sθ (Zt,t) ∙ VZtlog P0t(Zt∣Z0)]
Therefore,
1	Z0 g2⑴Ezt[ksθ(Zt,t) - VZtlogPt(Zt)k2 - kVzt logPt(Zt)k2]
=Z0 g2(t)Ezt h1 ksθ(Zt,t)k2 — Sθ(Zt,t) ∙ Vzt logPt(Zt)]
=Z0 g2(t)EzοEzt|zo h1 ksθ(Zt,t)k2 — Sθ(Zt,t) ∙ Vzt logP0t(Zt∣Zο)]
=1 Z0 g2(t)EzοEzt∣zο [ksθ(Zt,t) — Vzt logP0t(Zt∣Zο)k2 — kVzt logP0t(Zt∣Zο)k2].
Now, since P0t(Zt∣Z0) = N(Zt； μ(t)Zt,σ2(t)I) for μ(t) and σ2(t) determined by β(t) and g(t), We
have
Ezt∣zo [kVzt logp0t(ZtIZO)] = Ezt∣zο [||
and we have the desired result.
Zt - μ(t)z0∣∣2]	_ „	「kzk2 1	_ d
σ2(t)	H2J	= EN(z;0，I)	L2(t)J	= σ2(t),
□
Theorem 2. Suppose the flow network is flexible enough to transform Pr to arbitrary continuous
distribution. Forany θ, if V.s® (∙,t) is symmetricfor any t ∈ [0,T] and V. log π(∙) ≡ sθ(∙, T), then
there exists φ* such that Gapd(μφ*, νφ* 8) = 0. Furthermore, we have
φ*
arg min GaPd (“φ, νd,θ) = arg min DκL(μφkνφ,θ) = arg min DκL(pr kpφ,θ).
φφφ
ProofofTheorem 2. The symmetry of NSe(∙,t) indicates that sθ(∙,t) is a closed 1-form. Therefore,
from the Poincare,s lemma (Do Carmo, 2012), se(∙, t) is exact, and there exists a scalar field fθ
that satisfies se(∙,t) = V∙fθ(∙) for any t. If we define by qθ(Zt) = ReXpfθ(z))dz, then we have
sθ(Zt, t) = Vzt log qtθ(Zt, t) for any t. With this construction, the reverse SDE on the transformed
latent space becomes
dzθ = h - 1 β(t)zθ - g2(t)Vzθ log qθ (zθ)] dt + g(t) dωt, ZT 〜π = qT.
14
Under review as a conference paper at ICLR 2022
Probability in
Data Space
(Forward) Inference Path Measure
Optimal Generative Path Measure
Desired Generative Path Measure
t = 0	t = T
(a) Optimal Generative Distribution Not Equal to Data
Distribution
Hypothesis Space of
Generative Path Measures
{vθ: vθ∖τ =兀}
I------1------1	9" ≠ 〃
I 1----------------------------
Update θ
Optimal
Generative Path
Measure %*
Forward Path
Measure μ
(b) Forward Path Measure Not Included in Hypothesis
Class of Generative Process
一 (Forward) Inference Path Measure
— (ReVerSe) Generative Path Measure
(a) Nonlinear Diffusion in Data and Latent Spaces
Figure 9: (a) While PT and π is different in PDM as well, the marginal distribution of μψ at t = 0
depends on the transformation, hφ, modeled with normalizing flows. This ^-dependent initial
random variable at z0 influences its fully diffused variable of ZT, and optimizing φ minimizes the
discrepancy between PT and π, where ZT 〜Pt. Theorem 2 guarantees that this discrepancy is
completely eradicated with the optimum point of φ*. (b) Theorem 2 guarantees that for any θ, there
exists φ* such that DKL(PrkPφ*,e) = 0, which implies thatPr(x0) = Pφ*,e(x0).
Figure 8: (a) For the vanilla linear diffusion model in the data space, PT and π naturally diverges. If
the score network estimates the exact data score (i.e., Se(xt, t) = Vχt logPt(Xt)), then the (optimal)
generative distribution becomes Pe (x0) = / PT (XT )pt0(x0∣ XT) dxτ.On the other hand, the real
distribution is Pr (x0) = J π(xτ)pτ0(x0∣xτ) dxτ, with identical (reverse) transition probability,
Pt0(x0 ∣xτ) to the generative process. Mismatch of PT and π yields that the optimality of the
diffusion model does not guarantee to estimate the real distribution: Pe (x0) = π(x0). (b) Since PT
and π are distinctive distribution, the forward path measure μ is not included in the hypothesis class
of generative path measures, {νe : the marginal distribution of Ve at t = T is π}. In addition, μ is
not the element of the closure of the hypothesis class, and we conclude that DKL (PrllPe) > 0.
(b) Illustration of Theorem 2: Existence of φ* for any
θ such that DKL(Pr∣∣Pφ*,θ) = 0
If pe is the path measure of the solution of the above SDE, then the marginal distribution of Pe
at t would be qe. Therefore, if h-1 maps Pr to qe, then the forward process on the latent space
{zφ }T=0 becomes the generative process on the latent space {ze}T=0. Thus, we have μφ* = Ve in
distribution, which leads Dkl (μφ* ∣∣vΘ) = 0. We get the desired result by the change of variable
formula: DκL(μφ* ∣νφ* e) = Dkl(μφ* ∣ν1θ) = 0.	□
15
Under review as a conference paper at ICLR 2022
B Derivations
B.1 Derivation of Nonlinear Diffusion Terms
Throughout this section, we omit φ for the notational simplicity. Beginning with the linear diffusion
on the latent space given by
dzt = -1 β(t)zt dt + g(t)dωt, zo = h(xo) With x0 〜Pr,	(11)
we have defined xt = h-1 (zt) as the induced random variables. Then by the multivariate Ito’s
Lemma (Oksendal, 2013), the k-th component of the induced variable satisfies
∂h-1	T 1
dxt,k = ~∂k- dt + [Vzthk1 (Zt)] dzt + 2tr NzthJ (Zt)dzt dzT)	(12)
Plugging the linear diffusion SDE 11 in the SDE of Eq. 12, Eq. 12 is derived by
dχt,k = [Vzt h-1(zt)]T --1 β⑴Zt dt + g⑴dωt} + 1 g2(t)tr (v2th-1(Zt)) dt
={-2β⑴[vzth-1(Zt)]Tzt + 2g2⑴tr (vZth-1(Zt)) } dt + g⑴[vzth-1(Zt)]T dωt,
(13)
∂h-1
because -t^-
0. Then, Eq. 13 in vector form becomes
dxt = f(xt,t)dt+ G(xt, t) dωt,
Where vector-valued drift and diffusion terms are given by
[f(xt,t) = -1 β(t)Vzth-1 (zt)zt + 1 g2(t)tr (Vzth-1(zt))
G(xt, t) = g(t)vzt h-1(Zt),
(14)
Where vz2 h-1(zt) is a 3-dimensional tensor With (i, j, k)-th element to be v2z hk-1(zt) i,j, and the
trace operator applied on this tensor results in the vector of
[tr(vzth-1(Zt)),…,tr(vzth-1(zt))].
From the inverse function theorem (Rudin et al., 1964), the Jacobian of the inverse function
vzth-1(zt) equals to the inverse Jacobian vxth(xt) -1. Therefore, Eq. 14 is transformed to
(f(xt,t) = -2β(t)[vχth(xt)]-1h(xt) + 2g2(t)tr(vzth-1(zt))
G(xt, t) = g(t)[vxt h(xt)]-1.
(15)
∂h-1
Now, we derive the second term of f in terms of Xt as follows: observe that Ek dziɪ ∂∂Xhk" = δi,j,
Where δi,j = 1 if i = j and 0 otherWise. Differentiating both sides With respect to χt,l, We have
∂hk + ∂h-1
∂xt,j	∂zt,k
{∂⅛ (∂⅛) }=0
where the first term is
X ∂Xml { £ ( £ )}工=X (Vxt h(Xt))Tm(Vzt h-1(Zt))m,k (Vxth(Xt))k,j,
k,m t,l t,m	t,k	t,j k,m
using the chain rule, and the second term becomes
X 务{∂⅛ (W)} = X (Vzt h-1(zt))i,k (Vxt hk (Xt))ι,j.
From above, we derive the trace term of f in Eq. 15 as
tr (vzth-1(Zt)) = -tr ([vxth(χt)]-T ([vxth(χt)]Τ *v£h(χt)) [vxth(Xt)]-1),
16
Under review as a conference paper at ICLR 2022
Table 5: Comparison of variational gap between the VAE, the diffusion model, and PDM.
Models	Diffusion	Stochastic Process	Variational Gap (=NLL-NELBO)
VAE	No	{x, z}	Ex [Dkl(qφ(z∣x)∣∣Pθ(z|x))]
Diffusion Models	Data space	{xt}tT=0	Exo [DKL(μ({χt}lχ0)kνθHxtHxO))]
PDM	Latent space	{xt}T=o∕{zt}T=0	Ezo [DKL(μφHztHzO) kνθHztHzO))]
where VXth(xt) is a 3-dimensional tensor with (i,j, k)-th element to be (VXthk(Xt)), j. Also, We
define * operation as the element-wise matrix multiplication given by
([vxth(Xt)]T * vXth(Xt)I . := vXt [h(Xt)]-1(vXth(Xt) j-
Combining all together, thus, we derive nonlinear drift term in Eq. 15 as a function of Xt given by
f M,t) = - 2 β(t)[VXth(Xt)]-1h(Xt)
-1 g2(t)tr ([VXth(Xt)]-T ([VXth(Xt)]-1 * VXth(X» [VXth(Xt)L).
B.2 Derivation of Variational Gap
First, the KL divergence between two path measures satisfies
DKL(μkνθ )=/MXtDlog VIxIdX0:T
pPr(XO)μ({xt}IXO)Iog Pr(X0}({Xt]|XO) dX0:T
J	Pθ (Xθ)Vθ ({Xt}∣Xo)
ZPr(Xo)μ({Xt}∣Xo) [log Pr(X0) + log "({Xt}IXO)) dXo:T
Pθ(XO)	νθ({Xt}IXO)
ZPr(XO) log * dX0+ZPr(XO)〃({Xt}|Xo) log VXtMOO))
dXO:T
=DKL(PrkPθ) + Epr(X0)DKKL (μ({Xt}∣Xo)∣∣Vθ ({Xt}∣Xo))],
where XO:T = {Xt}tT=O. Therefore, variational gap is derived
by
Gap(μ, Vθ) =KκL(μkνθ) — DKL (PrkPθ)
=Epr(X0)|DKL(〃({Xt}|Xo)kve({Xt}|Xo))].
The gap Gapd(μφ, νφθ) in Eq. 10 is derived to be
Ezo [KKL(μφ({zt}∣zo)kνθ({zt}∣zo))], analogously.
Table 6: List of path measures.
Space (Diffusion)	Measure	Type
Data	“φ	Inference
(Nonlinear)	νΦ,θ	Generative
Latent	μΦ	Inference
(Linear)	νlθ	Generative
C Implementation Details
C.1 Model Architecture
Diffusion Model We implement two diffusion models as
backbone: NCSN++ (VE) (Song et al., 2020) and DDPM++ (VP) (Song et al., 2020), where two
backbones are one of the best performers in CIFAR-10 dataset. In our setting, NCSN++ assumes
the score network with parametrization of sθ(zt,logσ2(t)), where σ2(t) = σ^^^(^max)2t is the
variance of the transition probability Pot (zt Izo) with VESDE. As introduced in Song et al. (2020), we
use the Gaussian Fourier embeddings (Tancik et al., 2020) to model the high frequency details across
the temporal embedding. DDPM++ models the score network with parametrization ofθ(zt, t), which
17
Under review as a conference paper at ICLR 2022
targets to estimate -σ(t)Vzt logPt(Zt). We use the Transformer sinusoidal temporal embedding
(Vaswani et al., 2017).
We use the U-Net (Ronneberger et al., 2015) architecture for the score networks on both NCSN++
and DDPM++ based on (Ho et al., 2020). We stack U-Net resblocks of up-and-down convolutions
with skip connections that connect the identical-dimensional layers. Also, we follow Ho et al. (2020)
by applying the global attention at the resolution of 16 × 16. We use four U-Net resblocks with four
feature map resolutions (32 × 32 to 4 × 4). There are eight and four resblocks at each resolution on
CIFAR-10 and CelebA, respectively.
Flow Model We implement two normalizing flow models as backbone: the ResFlow (Chen et al.,
2019) and the decoupled ResFlow (Ma et al., 2020). Although Ma et al. (2020) makes use of the
Glow (Kingma and Dhariwal, 2018) for their flow model, we replace this glow architecture with the
ResFlow, mainly because the ResFlow transforms the variables from the identity function. We name
the experiments on this decoupled ResFlow as PDM (FID, dec) and PDM (NLL, dec). Otherwise, we
train the model with ResFlow as default. For the ResFlow, we drop three components from the original
paper: 1) the activation normalization Kingma and Dhariwal (2018), 2) the batch normalization Ioffe
and Szegedy (2015), and 3) the fully connected layers.
On the deep ResFlow, we use resolutions of 32 × 32 and 16 × 16 on both CIFAR-10 and CelebA.
Otherwise, we use a single resolution of 32 × 32 on other experiments whose models have networks
with layers less than 32. We apply the LipSwish activation (Chen et al., 2019) that provides the
Lipschitz coefficients less than a unity. For the multi-GPU training, we use the Neumann log-
determinant gradient estimator, instead of the memory efficient estimator (Chen et al., 2019).
C.2 Experimental details
Training All the models are trained with batch size of 128 and EMA rate of 0.999. We train each
model for around 10 days on four V-100 GPUs with 128Gb GPU memory for all experiments. At the
initial training phase, we train the diffusion model parameters with fixed flow parameters initialized
by the identity until the sample generation performance saturates. After this pretraining phase, we
start updating both diffusion and flow models. In this learning phase, we apply the learning rate
scheduling after the sample performance saturation. For the diffusion model, we drop the learning
rate from 2 × 10-4 to 10-5. For the flow model, we drop the learning rate from 10-3 to 10-5
for VPSDE and 5 × 10-5 to 10-5 for VESDE. We emphasize this learning rate scheduling takes
additional performance boost.
VESDE and VPSDE have different training details. VESDE assumes σmin = 10-2 on all exper-
iments. VESDE has σmax = 50 on CIFAR-10 and σmax = 90 on CelebA. On the other hand,
VPSDE assumes β(t) =βmin+ (βmax - βmin)t with βmin = 0.1 and βmax = 20. Both VESDE
and VPSDE truncate the diffusion time on [, T] in order to stabilize the diffusion model, where
= 10-5 and T = 1. On sample generation, however, DDPM++/PDM with VPSDE draws samples
by proceeding the generative diffusion process up to = 10-3, which corresponds to the diffusion
variance to be σmin = 10-2. On the sample generation, we apply the Predictor-Corrector (PC)
sampler Song et al. (2020) for NCSN++/PDM with VESDE and the Probability Flow (PF) ODE
sampler Song et al. (2020) for DDPM++/PDM. Throughout the experiments, we provide the identical
diffusion model structures to compare the baseline model and the PDM model in a fair setting.
Evaluation While we use the EMA rates of 0.999 for both diffusion and flow models for training, we
use this EMA only for the diffusion model on the evaluation mode because the flow model without
the EMA on the evaluation gives better samples. One peculiarity occurs in the experiment on the
decoupled ResFlow. We empirically find that turning off the EMA for both diffusion model and flow
model on the evaluation mode provides better performances of 3.11 in test NLL and 3.17 in test
NELBO as reported in Table 1 on PDM (FID, dec). With the EMA applied to the diffusion model,
we have 3.15 for test NLL and 3.22 for test NELBO, which shows slight degradation on the density
estimation performance.
We compute the FID score based on Song et al. (2020) for CIFAR-10 with the modified Inception V1
network1 in order to compare PDM to the baselines (Song et al., 2020; 2021) in a fair setting. On the
1https://tfhub.dev/tensorflow/tfgan/eval/inception/1
18
Under review as a conference paper at ICLR 2022
other hand, for the CelebA dataset, we compute the clean-FID (Parmar et al., 2021) that provides
consistently antialiased results.
C.3 Variance Reduction
Flow Training When we train the flow network with Lφ,θ {xt}tT=0; g2), this NELBO contains
the integration of Lθ {zt}tT=0 ; g2 . However, previous works on the diffusion models (Nichol
and Dhariwal, 2021) show that the estimation variance is largely reduced with the importance
sampling, which leads the performance variations (Song et al., 2021). Concretely, the importance
sampling chooses an importance weight that is proportional to ^ɪ(t), and estimates the integration
by Lθ HZt}T=0； g2) = R0T g2 ⑴LtHZt}T=0； θ) dt ≈ PN=1 σ2(tn)Ltn HZt}T=0； θ), Where tn is
sampled from the importance distribution.
For VESDE, it satisfies β(t) = 0 and g(t) = σmin(σmax)t J2log(σmax). Throughout the ex-
periments, we select σmin = 0.01, and, σmax = 50, 90 for CIFAR-10 and CelebA, respectively.
The transition probability becomes p0t(Zt|Z0) = N(Zt; Z0, σ2(t)), where σ2 (t) = R0t g2(s) ds =
σmin [(σσmax)2t - 1]. Song et al. (2020) approximates this variance to be σ2pp(t) = σ^b^(^max)2t
so that the variance is proportional to g2(t). Therefore, the importance weight follows the uniform
distribution, and the importance sampling is equivalent with choosing the uniform t.
On the other hand, VPSDE satisfies β(t) = βmin + (βmaχ - βmin)t with g(t) = ,β(t), Then, the
transition probability becomes pot(zt∣zo) = N(zt； μ(t)zt, σ2(t)I), where μ(t) = e-2Λtβ(S) ds and
σ2(t) = 1 - e- Ro β(S) ds. Thus, VPSDE has the importance weight of σ2(t) = ɪ~~-β(t)(s)ds，
The Monte-Carlo sample from this importance weight is the solution of the inverse Cumulative
Distribution Function (CDF) of the importance distribution as
t = F-1(u)	(16)
^⇒ U = F(t) = Z Z gTs)ds =春(F⑴一F(E)),	(17)
Z	σ2 (s)	Z
where u is a uniform sample from [0, 1], F(t) is the antiderivative of the importance weight given
by F(t) = log (1 - e-0.5t2(βmax-βmin)-tβmin) + 0.5t2(βmax - βmin) + tβmin, and Z is the
normalizing constant given by
Z = ZT 啜 dt
σ2 (t)
=h log (1 - e-0.5t2(βmax-βmin)-tβmin ) + 0.5t2 (βmax
T
- βmin ) + tβmin

= log (1 - e-0.5T (βmax -βmin )-T βmin ) - log (1 - e-0.5	(βmax -βmin )-βmin )
+ 0.5(T 2 - E2 )(βmax - β
min ) + (T - E)βmin
=23.86
for T = 1 and E = 10-5. The solution for the inverse CDF in Eq. 16 becomes
< ⇒ J β(S) ds = 2 (βmax - βmin)t + βmint = log (1 + exp (Zu + F(E)))
u⇒ t
-βmin +	βm2 in + 2(β
max
βmin) log 1 + exp (Zu + F (E))
βmax - βmin
—
The variation of the Monte-Carlo diffusion time depends on the uniform sample ofu.
Diffusion Training Now, on the side of training the diffusion network, as described in the main
paper, selecting the weighting function heavily influences the model performances. For instance,
19
Under review as a conference paper at ICLR 2022
Table 7: Additional experimental results on the pretrained DDPM++ with VPSDE. The reported
DDPM++ result (Song et al., 2021) is not fully reproducible in density estimation performances.
Model	tolerance	NLL	NELBO	Gap	FID
DDPM++ (NLL, IS, reported)	10-5	2.91	3.10	0.19	5.70
DDPM++ (NLL, IS, ours)	10-5 10-1	3.03 3.00	3.20	0.17 0.20	5.67 37.28
PDM (NLL, dec, IS)	10-5	2.94	2.95	0.01	6.84
	10-1	2.86		0.10	8.14
Table 8: Additional experimental results for precision and recall.
SDE	Model	NLL	NELBO	Precision	Recall	FID
VE	NCSN++	3.45	4.43	0.67	0.61	2.20
	PDM (FID)	3.04	3.09	0.67	0.60	3.36
	DDPM++ (FID)	3.13	3.29	0.68	0.60	3.16
VP	PDM (FID, dec)	3.11	3.17	0.65	0.61	4.43
	DDPM++ (NLL, IS, ours)	3.03	3.20	0.59	0.62	5.67
	PDM (NLL, dec, IS)	2.94	2.95	0.59	0.61	6.84
Song et al. (2021) compare the weighting functions ofg2 and σ2, and Song et al. (2021) conclude that
the weighting function of λ(t) = σ2(t) works the best for the sample generation, and λ(t) = g2 (t)
yields the best NLL performance. We train the diffusion model with σ2 (t) for PDM (VE/VP, FID),
and with g2 (t) for PDM (VP, NLL) models.
D Additional Experimental Results
D. 1 Detailed Comparison and Ablation on Tolerance
Table 7 has two implications. First, we find that the re-
ported NLL and NELBO performances are slightly better
than what we actually obtain in our pretraining step, de-
noted by DDPM++ (NLL, IS, ours). Comparing with our
results, optimizing PDM is indeed beneficial on density es-
timation, and variational gap is significantly reduced from
0.17 to 0.01. Second, we find the tolerance of 10-1 for the
numerical ODE solver on computing NLL is the sweet spot
on obtaining best NLL of 2.86, which beats LSGM-480M
(NLL) on density estimation. This is noteworthy because
our model uses 120M number of parameters, a quarter
of parameters in LSGM. On the other hand, NELBO is
independent of the tolerance because it does not use the
Tolerance of RK45 ODE Solver
Figure 10: Ablation study on tolerance
level of the RK45 ODE solver.
numerical ODE solver, and Table 7 shows that variational gap is minimized when the tolerance is
small enough.
Figure 10 illustrates the ablation study on the tolerance for the ODE solver. Surprisingly, PDM is
robust on the tolerance level up to 10-1, in contrast to LSGM with poor sample quality at 10-1
(Vahdat et al., 2021). From this robustness on the tolerance, we could reduce the sampling wall clock
time from 360s (tolerance 10-5) to 72s (tolerance 10-1) for 1,024 samples, which is 5x faster, with a
minor sacrifice in FID score from 6.79 to 8.56 in PDM (FID) with VPSDE.
D.2 Precision and Recall
While FID measures the overall sample quality that counts both fidelity and diversity at once, we
provide precision and recall (Kynkaanniemi et al., 2019) based on the modified Inception V1 network
20
Under review as a conference paper at ICLR 2022
Table 10: FID-1k of PDM with VPSDE on CIFAR-
10.
Table 9: FID-5k of PDM (VE, FID) on CIFAR-10.
	SNR				Temperature	FID-1k	
	0.14	0.145	0.15	0.155			
						PDM (FID)	PDM (FID, dec)
0.5	16.59	15.99	15.95	15.7	0.5	40.09	36.84
0.6	12.61	12.13	12.17	11.99	0.6	38.28	35.31
0.7	10.41	9.95	10.08	9.94	0.7	37.10	34.36
0.8	9.2	8.75	8.94	8.84	0.8	36.28	33.77
0.9	8.58	8.13	8.35	8.28	0.9	35.87	33.44
Temperature 1.0	8.31	7.85	8.07	8.06	1.0	35.57	33.24
1.1	8.27	7.84	8.01	8.07	1.1	35.40	33.21
1.2	8.49	8.11	8.21	8.31	1.2	35.28	33.24
1.3	8.97	8.61	8.67	8.84	1.3	35.28	33.24
1.4	9.76	9.44	9.45	9.73	1.4	35.38	33.42
1.5	10.93	10.68	10.63	11.01	1.5	35.68	33.65
Table 11: FID-1k of PDM (VE, FID) on CelebA.
Table 12: FID-1k of PDM
(VP, FID) on CelebA.
Temperature	FID-1k
0.8	15.60
0.9	15.27
1.0	15.04
1.05	14.95
1.1	14.90
1.15	14.94
1.2	15.05
1.3	15.44
SNR
	I 0.16		0.17	0.18
	0.8	17.37	16.88	17.8
	0.9	15.37	15.04	16.02
	1.0	14.72	14.56	15.37
Temperature	1.05	14.72	14.53	15.24
	1.1	15.00	14.83	15.47
	1.15	15.52	15.36	15.94
	1.2	16.3	16.14	16.71
	1.3	19.09	18.81	19.40
2 with 50k samples in order to separate the sample fidelity and diversity. Precision is a metric that
measures the sample fidelity, and the recall is for the sample diversity. Table 8 indicates that both
precision and recall remains at the identical scale, except for precision at PDM (FID, dec).
D.3 Effect of Signal-to-Noise and Temperature
As the flow colorizes the latent sample, we experiment the effect of
temperature (Kingma and Dhariwal, 2018) on this colorization in
Figure 11. With temperature τ , the normalizing flow puts its latent
input scaled by τ to the flow network. In our PDM, the image color
with a higher temperature tends to be brighter, and we find that the
optimal temperature depends on the experimental settings.
On sample generation, we draw latent samples from either the PC
sampling or the PF ODE sampling. PC sampling assumes the Signal-
to-Noise Ratio (SNR) that modifies the Corrector algorithm (Song
et al., 2020) of the annealed Langevin dynamics (Song and Ermon,
0.6
0.7
0.8
0.9
1.0
1.1
1.2
∏.3
Figure 11: Ablation study for
the flow temperature.
2019). From the sampled latent variable, we scale by multiplying the temperature τ (Kingma and
Dhariwal, 2018) to the latent variable. Since the diffusion model learns the global image structure,
the normalizing flow only paints the color of the image, and the temperature controls the brightness of
the generated image. This is a contrastive result to the previous flow models (Kingma and Dhariwal,
2018; Chen et al., 2019) that the temperature in the previous models controls both global context (such
as the smoothness) and color. Therefore, we can create images with different colors by manipulating
this temperature.
2Since we already used this inception network in calculating the FID score, we compute precision and recall
based on this inception network. Note that we used this version of inception network to comply the baseline
(Song et al., 2020).
21
Under review as a conference paper at ICLR 2022
Figure 12: Non cherry-picked random samples from CIFAR-10 trained on PDM (VE, FID).
CIFAR-10 With these hyperparameters of the SNR and the temperatures, we find optimal values in
the below Tables. Table 9 computes the FID-5k score of all combinations for (SNR, τ) in PDM (VE,
FID) on CIFAR-10. Unlike the SNR of 0.16 that is previously suggested in (Song et al., 2020) for the
vanilla diffusion model, we find that SNR of 0.145 consistently outperforming other values for SNR
in terms of FID. Similarly, Table 9 presents that the optimal temperature is 1.1.
For VPSDE, Table 10 shows that the temperature of 1.3 performs the best for sample generation in
PDM (VP, FID). For PDM (VP, FID, dec), we find that the temperature of 1.1 performs the best, and
for PDM (VP, NLL, dec), the vanilla temperature of 1.0 generates the best samples.
CelebA Table 11 shows the FID-1k performance of PDM (VE, FID) on CelebA. It shows that the
SNR of 0.17 proposed in (Song et al., 2020) performs the best also in our PDM. In addition, it shows
that the optimal temperature is either 1.0 or 1.05. Table 12 shows the FID-1k performance of PDM
(VP) on CelebA. It is clear in this table that the sample quality is the best on the temperature range of
1.05 to 1.15.
D.4 Random samples
Figure 12 and 13 show the non cherry-picked random samples from PDM (VE, FID) on CIFAR-10
and PDM (VP, FID) on CelebA, respectively.
22
Under review as a conference paper at ICLR 2022
Figure 13: Non cherry-picked random samples from CelebA trained on PDM (VP, FID).
23