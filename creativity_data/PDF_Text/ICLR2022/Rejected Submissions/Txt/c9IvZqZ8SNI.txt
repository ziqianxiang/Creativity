Under review as a conference paper at ICLR 2022
Learning Structure from the Ground up—
Hierarchical Representation Learning by
Chunking
Anonymous authors
Paper under double-blind review
Ab stract
From learning to play the piano to speaking a new language, reusing and recom-
bining previously acquired representations enables us to master complex skills and
easily adapt to new environments. Inspired by the Gestalt principle of grouping by
proximity and theories of chunking in cognitive science, we propose a hierarchi-
cal chunking model (HCM). HCM learns representations from non-i.i.d sequential
data from the ground up by first discovering the minimal atomic sequential units
as chunks. As learning progresses, a hierarchy of chunk representation is acquired
by chunking previously learned representations into more complex representations
guided by sequential dependence. We provide learning guarantees on an idealized
version of HCM, and demonstrate that HCM learns meaningful and interpretable
representations in visual, temporal, visual-temporal domains and language data.
Furthermore, the interpretability of the learned chunks enables flexible transfer be-
tween environments that share partial representational structure. Taken together,
our results show how cognitive science in general and theories of chunking in
particular could inform novel and more interpretable approaches to representation
learning.
1	Introduction
The last decade has witnessed a meteoric rise in the abilities of artificial systems, in particular deep
learning models (LeCun et al., 2015). From beating the world champion of Go (Silver et al., 2016)
to predicting the structure of the human proteome (Jumper et al., 2021), deep learning models have
accomplished impressive end-to-end learning achievements. Yet, researchers were quick to point out
shortcomings (Marcus, 2018; Lake et al., 2017). Two such shortcomings concern the hierarchical
structure and interpretability of the representations learned by neural network architectures. Specif-
ically, deep learning models suffer from a lack of interpretability since ANNs are sub-symbolic,
nested, non-linear structures. This means they the way predictions are generated can be difficult to
understand (Samek et al., 2017; Ribeiro et al., 2016; Doshi-Velez & Kim, 2017). Furthermore, deep
learning models can also struggle to learn hierarchical representations altogether (Lake & Baroni,
2018; Fodor & Pylyshyn, 1988). To address these shortcomings, it has been suggested that machine
learning researchers could seek inspiration from cognitive science and construct models that resem-
ble the hierarchical and interpretable representations observed in human learners (Chollet, 2017;
Lake et al., 2015).
We take inspiration from the cognitive phenomenon of chunking and the Gestalt principle of group-
ing by proximity. To get an intuition for chunking, try to read through the following sequence of
letters: “DFJKJKJKDFDFJKJKDFDF”. Upon reaching the end, if you were tasked to repeat the
letters from memory, you might recall fragments of the sequence such as “DF” or “JK”. By parsing
the sequence of letters only once, one detects frequently occurring patterns within and memorize of
them together as units, i.e. chunks. Chunking has been observed in a range of sensory modalities.
We perceive units and structures when learning a language (Perruchet et al., 2014; McCauley &
Christiansen, 2017), in action sequences (Penhune & Steele, 2012; Rosenbaum et al., 1983), and in
visual structures Hinton (1979); Brady et al. (2009); Egan & Schwartz (1979). The extracted chunks
have been argued to facilitate memory compression (Gobet et al., 2001; Miller, 1956), compositional
1
Under review as a conference paper at ICLR 2022
Training Sequence：
b) Chunking Decision
Marginal Transition
∣□ Nonindependent
Transition Entries
d) Gradual Representation Build-up
e) Final Chunk Hierarchy
Figure 1: Schematic of the Hierarchical Chunking Model. a) Example of a hierarchical model
generating training sequences. b) Intermediate representation of learned marginal and transition
matrices. The most frequent transition that violates the independence testing criterion is marked in
red and can be turned into a new chunk. c) HCM combines the two chunks cL and cR to form a new
chunk. d) As HCM observes longer sequences, it gradually learns a hierarchical representation of
chunks. e) HCM arrives at the finally chunk hierarchy isomorphic to the generative hierarchy.
generalization (Schulz et al., 2017), predictive processing (Koch & Hoffmann, 2000; Mussgens &
Ullen, 2015), and communication (Schulz et al., 2020).
In the current work, we propose a hierarchical chunking model (HCM) that learns chunks from
non-i.i.d sequential data with a hierarchical structure. HCM starts out learning an atomic set of
chunks to explain the sequence and gradually combines them into increasingly larger and more
complex chunks, thereby learning interpretable hierarchical structures. The output of the model
is a dynamical graph that is a trace of the evolving representation. The resulting representations
are therefore easy to interpret, and flexibly reusable (e.g. we can choose to re-use specific parts).
We derive learning guarantees on an idealized generative model and demonstrate convergence on
sequential data coming from this generative model. Furthermore, we show that our model can benefit
from having learned previous structures with shared components, leading to flexible transfer and
generalization. Finally, we demonstrate that HCM can return interpretable representation in discrete
sequential, visual, and language domains, in several small-scale experiments. Taken together, our
results show how cognitive science in general and theories of chunking and grouping by proximity
in particular can inform novel and more interpretable approaches to representation learning.
2	Hierarchical Chunking Model
When talking about chunks in a sequence, a chunk is defined as a unit created by concatenating sev-
eral atomic sequential units together. Take the training sequence shown in Figure 1a as an example.
The sequence consists of discrete, size-one atomic units from an atomic alphabet A0 : in this case
A0 = {0, 1, 2}. A chunk c is made up of a combination of one or more atomic units in A0\{0}. 0
denotes an empty observation in the sequence.
Intuitively, if the training sequence contains inherent hierarchical structure, then there are patterns
which span several sequential units sharing these internal structures, like repeated melodies and sub-
melodies in a piece of music. If the pattern occurs in the sequence, observations between sequential
units within the pattern will be correlated. In this case, chunking patterns within a sequence as
units simplifies perceptual processing in the sense that the sequence can be perceived one chunk at
2
Under review as a conference paper at ICLR 2022
a time, instead of one sequential unit at a time. Furthermore, the acquired “primary” chunks serve
as building blocks to discover larger chunks that are embedded within the intricate hierarchy of the
sequential structure.
More formally, HCM acquires a belief set B of chunks, and uses chunks from the belief set
to parse the sequence one chunk at a time. HCM assumes that a sequence is generated from
samples of independently occurring chunks with probability of PB(c) evaluated on the belief set
B. The probability of observing a sequence of parsed chunks c1, c2, ..., cN can be denoted as
P(c1, c2, ..., cN) = Qc ∈B PB(ci). Chunks as perceiving units serve as independent factors that
disentangle observations in the sequence.
From the beginning of the sequence, HCM ‘perceives’ sequential units one chunk at a time, in other
words, the training sequence is parsed by HCM into chunks. At every parsing step, the longest chunk
in the belief set that is consistent with the upcoming sequence is chosen to explain the up-coming
sequential observations. The end of the previous chunk parse initiates the next parse.
Observing a hierarchically structured sequence as illustrated in Figure 1a, HCM gradually builds up
a hierarchy of chunks starting from an empty belief set. It first identifies a set of atomic chunks to
construct its initial belief set B. Initially, these will be chunks of length one, yielding one-by-one
processing of the primitive elements.
For one belief set B, HCM keeps track of the marginal parsing frequency M (ci) for each chunk ci
in B, a vector with size |B| and the transition frequency T between chunk ci followed by chunk cj ,
as illustrated in Figure 1b. Entries in M and T are used to test the hypothesis that consecutive chunk
parses have a correlated consecutive occurrence within the sequence. If two chunks cL and cR have
a significant adjacency dependence based on their entries in M and T, they are chunked together to
become CL ㊉ cr, which augments the belief set B by one. One example of chunk merging is shown
in Figure 1c.
There are two different versions of HCM. The Rational Chunk Learning HCM produces chunks
in an idealized way which we use to study learning guarantees. The online version of HCM is an
approximation to the rational HCM that can be adapted to different environments and processes
sequences online. Pseudo-code for both algorithms can be found in Algorithm 1 and 2 in the SI.
Rational Chunk Learning: HCM as an Ideal Observer The model is initiated with an empty
belief set, and it first finds a minimally complete belief set after the first sequence parse. In each
iteration, the entire sequence is parsed to evaluate M and T , which are used to find consecutive
chunk parses in the existing belief set fulfilling the dependence testing criterion. From these depen-
dent chunk pairs, the pair with the largest estimated joint probability is combined into a new chunk.
The new chunk enlarges the belief set by one. The chunks in the new belief set are used to parse the
sequence in the next iteration. This process repeats until all of the chunks in the belief set pass an
independence testing criterion.
Online Chunk Learning The online chunk learning HCM approximates the ideal observer HCM
by learning new chunks when the training sequence is processed online. To have a feature that
encourages adaptation to new environmental statistics, entries in M and T can be subject to memory
decay. We use the ideal observer model to demonstrate learning guarantees, but use the online model
to learn representations in realistic and more complex set-ups.
2.1	HCM Builds Representations from the Ground Up
As HCM learns from a sequence, it starts with no representation and gradually builds up the rep-
resentations which can be readily interpreted at any learning stage. The representation can be de-
scribed by a chunk hierarchy graph G with the vertex set being the chunks and edges pointing from
chunk constituents to the constructed chunks. Shown in Figure 1d is the gradual build-up of one
such chunk hierarchy graph as the model learns from a training sequence coming from the genera-
tive hierarchy displayed in Figure 1a. At t = 10, HCM learns only the atomic chunks, at t = 60,
HCM has already constructed two additional chunks; when t = 100, two more additional chunks
are constructed based on the previously acquired chunks, and at t = 150, HCM arrives at the final
chunk hierarchy.
3
Under review as a conference paper at ICLR 2022
Figure 2: a) Example graph generated from the hierarchical generative model with a depth of d = 3.
b) Learning performance of HCM and RNN with increasing training length and for different depths.
Performance was averaged over 30 randomly-generated graphs.
3	Hierarchical Generative Model
To study learning guarantee of the HCM, a generative model with a random hierarchy of chunks is
constructed. The relation between chunks and their constructive components in the generative model
is described by a chunk hierarchy graph Gd with vertex set VAd and edge set EAd . One example is
illustrated in Figure 1a. Ad is the set of chunks used to construct the sequence. The depth d specifies
the number of chunks created in the generative process.
Starting with an initial set of atomic chunks A0, at the i-th iteration, two chunks cL, cR are randomly
chosen from the current set of chunks Ai and are concatenated into a new chunk CL ㊉ CR, augmenting
Ai by one to Ai+1. Meanwhile, an independent occurrence probability is assigned to each chunk
under the condition that the probability of occurrence for every new chunk Ci in the construction
process evaluated on the support set Ai carries the largest probability mass.
To generate a sequence from a constructed hierarchical graph, chunks are sampled independently
from the set of all chunks and appended after each other, under the constraint that no two chunks
with a child chunk should be sampled consecutively.
3.1	Learning Guarantee
Theorem: As the length of the sequence approaches infinity, HCM learns a hierarchical chunking
graph G isomorphic to the generative hierarchical graph G .
Proof Sketch: We approach this proof by induction. Further details can be found in SI. Base step: The
first step of the rational chunking algorithm is to find the minimally complete atomic set of chunks
to form its initial belief set. This procedure guarantees that G0 = G0. Additionally, the probability
mass of the learning model at step 0 and the generative model at step 0 is asymptotically the same
as the sequence length approaches infinity. Induction hypothesis: Assume that the learned belief
set Bi at step i contains the same chunks as the alphabet set Ai in the generative model, the chunk
combination pair with the biggest evaluated joint occurrence probability violating the independence
test is picked to be concatenated into a chunk to extend the belief set: this chunk is the same chunk
node created by the hierarchical generative model. End step: The chunk learning process stops once
the independence criterion is passed. This is the case once the chunk learning algorithm has learned
a belief set Bd = Ad .
3.2	Learning Convergence
HCM’s learning performance with increasing sequence length is evaluated and shown in Figure 2.
For this, HCM was trained on random graph hierarchies constructed from the hierarchical generative
model over 3000 trials while also varying the depth d. Figure 2a displays an example of a random
generative hierarchy with a depth of d = 3. We used the Kullback-Leibler divergence to evaluate
the deviation of HCM’s learned representations from the generative hierarchical model. This was
done by using HCM’s representation to produce “imagined” sequences, which were then compared
to the ground truth probability for each chunk in the used generative model. Figure 2b shows the
KL-divergence between the learned and ground-truth distribution for different depths d of the gener-
ative graphs. For each depth, the KL-divergence is evaluated on 30 random generative models with
sequence length increasing from 50 to 3000 in each model. Overall, the KL-divergence decreased as
the length of training sequence increased and converged with larger training sequences. This shows
empirically that HCM learns representations bearing closer resemblance to the generative model.
4
Under review as a conference paper at ICLR 2022
Figure 3: a) Example of a representation learned by an HCM. b) An environment facilitative to the
learned representation. Gray shadows mark the chunks that can be directly transferred. c) Average
performance over the first 500 trials after environment change in the facilitative environment. d) In-
terfering environment. Gray shadows marks chunks that the learned representation needs to acquire
from scratch. e) Average performance over the first 500 trials after environment changes into an the
interfering environment.
We used the same sequences to train a 3-layer Recurrent Neural Network (RNN) with 40 hidden
units and the same method to measure the KL-divergence by using “imagined” sequences produced
by the RNN evaluated on the support set of the generative hierarchy. As the length of the train-
ing sequence increased, the KL-divergence also converged but at a much slower rate. Note that
HCM’s competitive advantage increased further as the depth of the generative hierarchy increased.
Thus, HCM learns quickly about the hierarchical generative model and does so faster than a neural
network.
3.3	HCM Permits Transfer Between Environments with Overlapping
Structure
After training on a sequence, HCM acquires an interpretable representation. Knowing what the
model has learned enables us to directly know what type of hierarchical environment would facilitate
or interfere with the learned representations. This is impossible to do using standard neural networks
because we generally would not know what representation they have learned, and therefore do not
have fine-grained control over retaining or removing parts of it.
More formally, two HCM models might have acquired different hierarchical chunking graphs Gi and
Gj from their past experience. These might lie on the graph construction path (G0, G1 , G2, ..., Gd).
The HCM with a chunk hierarchy graph ‘closer’ to the ground truth chunk Gd on the path, takes
fewer iteration to arrive at Gd . This also applies when the chunk hierarchies starting out are not
along the graph construction path but only showing partial overlap.
Similarly, the chunk hierarchy Gi learned by an HCM might facilitate its performance in a new
environment where Gi lies along the graph construction path to the true Gd, i.e. there is partial
overlap between the chunk hierarchies. We demonstrate this in Figure 3. Here, HCM was trained
on a random hierarchical generative model and acquired the representation shown in Figure 3a.
Knowing this representation, we also know that an environment with a hierarchical generative graph
as shown in Figure 3b allows for positive transfer, i.e. the HCM that already contains the chunk
hierarchy Figure 3a would learn faster than a naive one by transferring its previously learned chunks.
As shown in Figure 3c, the trained HCM learns faster than a naive HCM which had to learn about
the structure from scratch.
Vice versa, we might have a situation in which transfer is detrimental. For example, there is no
overlap in the chunk hierarchy learned by the HCM in Figure 3a with the graph shown in Figure 3d.
The shaded chunks need to be learned anew by the HCM trained on Figure 3a, yet the previously
acquired chunks might mislead HCM causing it to adapt to the new environment more slowly. As a
result, the performance of the pre-trained HCM suffers more from an interfering environment than a
naive HCM (Figure 3e). This is similar to catastrophic interferences in neural networks (Sharkey &
Sharkey, 1995), where performance on one task interferes with others. The advantage of the HCM
is that with visibility into the chunks learned, we have a better a priori sense of whether transfer
will be facilitative or interfering. This means that we can examine whether to re-use the previously
learned chunks, or to start from scratch with a naive model.
5
Under review as a conference paper at ICLR 2022
b) Simple Intermediate
■・..「」・L
."H □ ≡- H
■ ■ ' -	* ɪ '=
,,,・ r I J ,
■ , , ,	F-T =
Complex
j-" n-
Sequence: j ∣ L，+* *~ …
Figure 4: a) A designed visual hierarchical model where elementary components compose more
complex images. b) Initial, intermediate, and complex chunks learned by HCM trained on sequences
of images generated by the visual hierarchical model.
4	Generalizing to Visual Temporal Chunks via the Principle of
Proximal Grouping
Humans are not only able to identify sequential chunks but also to find structure in visual-temporal
stimuli and group visual points as a whole. For example, despite the fact that our retina receives
pixel-wise visual inputs that vary across temporal slices, we are able to perceive complex moving
objects. The Gestalt principle of grouping by proximity states that objects that are close to one an-
other appear to form groups (Wagemans et al., 2012). This principle has been argued to play a key
role in human perceptual grouping (Compton & Logan, 1993) and benefit working memory (Peter-
son & Berryhill, 2013) and the reduction of visual complexity (Donderi, 2006). Indeed, in humans
and other animals, learning of adjacent relationships prevails over non-adjacent ones (Malassis et al.,
2018). Therefore, the adjacent dependency structure can be seen as the primary driver of chunking
in visual temporal domains (Froyen et al., 2015). To emulate this ability of chunking via proximal
grouping in visual temporal perception, we extend HCM to also learn visual temporal chunks.
Visual temporal chunks not only subsume temporal length but also varying visual slices in each tem-
poral slice. One can imagine visual temporal chunks as having a 3D shape — the first two height
and width dimensions are the visual part of the chunk, the length of the object is the temporal part,
made of stacked visual-temporal pixels. Within each temporal slice are the visual features identified
by the chunk. Since nearby points are more likely to be in the same chunk, this assumption is an
implementation of the principle of grouping by proximity in the visual-temporal domain. As the
model iterates through data across its temporal slice, the chunk that attains the biggest visual tempo-
ral volume is chosen to explain parts of the visual-temporal observations. Multiple visual temporal
chunks can be identified to occur simultaneously. Starting at the visual temporal time point marked
by the previous chunk, chunks are identified and stored in M . The transition matrix T is modified
to account for the temporal lag-difference between adjacent chunk pairs and records the frequency
that one chunk transitions into another one with a given time-lag. A hypothesis test is conducted
every time when a pair of adjacent chunks are identified, chunks that violate the hypothesis test are
grouped together to augment the belief set which is then used to parse future sequences.
4.1	Learning Part-Whole Relationship Between Visual Components
We let HCM learn chunks in a visual domain by learning from a sequence of images. To this end, a
hierarchical generative model in the pixel-wise image domain shown in Figure 4a was constructed
to test HCM’s visual chunking ability. Specifically, a set of elementary visual units in the lowest
level of the hierarchy are combined to construct intermediate and more complex visual units higher
up in the hierarchy. An empty image is included to denote pauses. All of the constructed elements
in the hierarchy occurred independently according to a draw from a Dirichlet flat distribution. To
generate the sequence which was used to train HCM, images in the hierarchy were sampled from
the generative distribution and appended to the end of the sequence. In this way, despite the visual
correlations in each image described by the hierarchy, each image slice was temporally independent
from other slices.
6
Under review as a conference paper at ICLR 2022
Figure 5: a) A GIF of a moving squid used as a sequence to train HCM. b) Examples of temporal
chunks learned by HCM. c) Examples of visual chunks learned by HCM.
HCM learns a hierarchy of visual chunks from a sequence of visually correlated but temporally
independent images sampled from the visual hierarchical model. Shown in Figure 4b are the chunk
representations learned by HCM at different stages. Having no knowledge about the image parts
before starting to learn, HCM acquires the individual pixels as chunks to explain the observations.
As HCM proceeds with learning, visual correlations among the pixels are discovered and larger
chunks are formed. As the number of observations increases, the model learns more sophisticated
yet still interpretable chunks.
4.2	Learning Visual-Temporal Movement Hierarchies
Instead of seeing one image after another sampled from an independent, identically distributed dis-
tribution, real world experiences contain correlations in both the visual and temporal dimension.
From observing object movements across space and time, the visual system learns structures from
correlated visual and temporal observations, decomposes motion structure and groups moving ob-
jects together as a whole (Bill et al., 2020). To emulate this type of environment as a learning task, an
animated GIF of a squid swimming in the sea as shown in Figure 5a was used as a visual-temporal
sequence to train HCM. As learning advances, HCM learns chunks spanning both the visual and
temporal domain. Examples of such visual-temporal chunks are shown in Figure 5b and c. There
are visual-temporal chunks that mark movements of a tentacle and the rising-up motion of a bubble.
Additionally, there are visual chunks that resemble a part of the squid’s eye and face.
5	Learning Chunks from Realistic Language Data
We so far have only shown demonstrations of HCM learning chunks on simple sequences. Thus,
one step further is to run HCM on real world datasets that contain complex hierarchical structures.
One immediate testbed containing such structures is natural language. To this end, we trained HCM
to learn chunks on the first book of The Hunger Games.
HCM starts with learning individual English letters and punctuation. After having seen 10,000
characters of the book, HCM acquires frequently used chunks that resemble common English pre-
and suffixes such as “ity”, “ing” , “re”, and “ith”; definite and indefinite articles such as “a”, “an”,
“the”; conjunctions such as “and”, “but”, “that”, and “as”; prepositions such as “of, “to”, “in”, “at”,
variants of “is”, “are”, “was”; and pronouns such as “he”, “my”, “me”, “we”, “she”.
After having seen 100,000 characters of the book, HCM learns intermediate chunks that include
various commonly used verbs such as “come”, “sing”, “leave”, “try”; nouns such as “bed”, “day”,
“mother”; common word combinations such as “in the”, “lose to”, “the wood”. Additionally, HCM
has acquired words specific to The Hunger Games such as “prim”, “death”, “hunger”, and “district
12”.
After having seen 300,000 characters of the book, HCM learns more complex phrases from the
previously learned words, such as the commonly used phrases “it is not just”, “in the school”, “our
district”, and “cause of the”.
7
Under review as a conference paper at ICLR 2022
Table	1: Example Chunks Learned from The Hunger Games
Simple Chunks	‘an‘，’in ‘，’be‘，’at‘，’me‘，’le‘，’a ’，‘ar‘，’re'，‘and ‘，’ve'，‘ing ‘，‘on’， ’st’, ’se’, ’to ’, ’of ’, ’he’, ’my ’, ’te’, ’pe’, ’ou’, ’we’, ’ad’, ’de’, ’li’, ‘the‘， ‘ce‘， ‘is‘， ‘as‘， ‘il‘， ‘ch‘， ‘al‘， ‘no‘， ‘she‘， ‘ing‘， ‘am‘， ‘ack‘， ‘we‘， ‘raw‘， ‘on the‘， ‘day‘， ‘ear‘， ‘oug‘， ‘bea‘， ‘tree‘， ‘sin‘， ‘that‘， ‘log‘， ‘ters‘， ‘wood’, ‘now’, ‘was’, ‘even’, ‘leven', ‘ater’, ‘ever’, ‘but’, ‘ith’, ‘ity’
Intermediate Chunks	‘this’, ‘pas’, ‘eak’, ‘if’, ‘sing’, ‘bed’, ‘men’, ‘raw’, ‘day’, ‘in the’, ‘link‘， ‘for‘， ‘one‘， ‘the wood‘， ‘bell‘， ‘other‘， ‘...‘， ‘lose to‘， ‘hunger‘， ‘mother‘, ‘death‘, ‘would‘, ‘district 12‘, ‘try‘, ‘under‘, ‘prim‘, ‘beg‘, ‘then‘, ‘into‘, ‘gale‘, ‘read‘, ‘come‘, ‘he want‘, ‘leave‘, ‘where‘, ‘older‘, ‘says’, ‘might’, ‘dont’, ‘add’, ‘know’, ‘man who’, ‘of the’
Complex Chunks	‘out of’, ‘itout’, ‘our district’, ‘capitol’, ‘reaping’, ‘fair’, ‘berries’, ‘the last‘, ‘fish‘, ‘again‘, ‘as well‘, ‘the square‘, ‘scomers‘, ‘fully‘, ‘, but the ‘, ‘in the school‘, ‘at the‘, ‘you can‘, ‘tribute‘, ‘to remember‘, ‘it is not just‘, ‘I can‘, ‘peace‘, ‘feel‘, ‘you have to‘, ‘I know‘, ‘bother‘, ‘in our‘, ‘kill’, ’cause of the’, ‘the pig’, ‘to the baker’, ‘I have’, ‘what was’
6	Related Work
Our model is successor to decades of work on different approaches to chunk learning. In cognitive
science, researchers have put forward models that produce qualitatively similar chunks as humans
learning a natural or artificial languages (Servan-Schreiber & Anderson, 1990; Perruchet & Vinter,
1998), as well as models that can extract chunks from visual inputs (Mareschal & French, 2016).
HCM can be viewed as a principled version of these earlier cognitive models because it uses hy-
pothesis testing in its decision to chunk elements together instead of using chunking heuristics.
Therefore, HCM comes with learning guarantees for a fairly general class of hierarchically struc-
tured environments. Additionally, HCM extends past cognitive models to the higher dimensional
domain of visual-temporal chunking.
The primary approach to chunk learning in the language domain were n-gram models, dating all the
way back to Shannon (1948). An n-gram model learns the marginal probabilities of all chunks of
size n. A major limitation of this approach is that the number of chunks grows exponentially as a
function of chunk length. For instance, with an alphabet of26 letters, the number of possible 5-letter
words already goes beyond 10 million. Due to the large vocabulary size, building word-level n-gram
models is virtually unfeasible. To this end, a Bayesian non-parametric extension of n-gram models
has been put forward (Teh, 2006). A Bayesian non-parametric n-gram model builds up structure as
evidence is accumulated. That is, it flexibly reduces to a 1-gram model if chunks are not present or
‘opens up’ higher and higher n-gram levels depending on the size of chunks. Teh (2006)’s model is
different from HCM in several regards. Instead of using the chunks to ‘look forward’ and parse the
sequence in large steps, it ‘looks back’ and predicts only one element conditioning on the context
of the previous elements. Then, instead of storing an explicit bag of chunks, it represents a chunk
distribution weighted by evidence. For prediction, it smooths over the evidence of all chunks that
are consistent with the current context. A shortcoming of Teh (2006)’s model and n-gram models
in general is that they do not leverage the concatenation process observed in humans but the chunks
are built up primitive element-wise.
Hierarchical hidden Markov models (Fine et al., 1998) were developed in a similar vein, extend-
ing hidden Markov models to be able to capture multi-scale sequential structure. Although these
models are able to capture larger patterns than non-hierarchical versions while maintaining the com-
putational tractability of simple Markov processes, they still lack the adaptive recombination and
reuse of pre-existing components. Fragment grammars (O’Donnell et al., 2009) address this by bal-
ancing the creation and re-use of chunks by Bayesian principles, while also preserving the symbolic
interpretability. However, fragment grammars are intractable and their inference is exceptionally
costly.
In the era of deep learning, both natural language processing and image processing became in-
creasingly dominated by neural networks, with one of their primary tasks being the extraction and
prediction of chunks (Zhai et al., 2017; Si et al., 2020; Ortmann, 2021). Commonly, these so-called
8
Under review as a conference paper at ICLR 2022
sequence chunks are used as units of segmentation for other downstream tasks such as text trans-
lation. However, in these approaches, the architecture needs to be pre-specified before training as
compared to HCM, where no such specification is required. HCM can therefore be seen as an
interpretable and transparent alternative to neural networks for some of these tasks.
A final related line of research attempts to learn explicit representations by inducing programs (Lake
et al., 2015). Fore example, in a recent approach to this challenge, the interpretable structures re-
turned by program induction algorithms was combined with a deep neural network to learn mean-
ingful representations from data (Ellis et al., 2021). Yet in these approaches the retrieved represen-
tations are highly dependent on the initial set of building blocks which are usually specified by the
experimenter. HCM, in comparsion, is task-agnostic and requires no primitive program description.
7	Discussion
We have proposed the Hierarchical Chunking Model (HCM) as a cognitively-inspired method for
learning representations from the ground up. HCM starts out by learning atomic units from se-
quential data which it then chunks together, gradually building up a hierarchical representation that
can be expressed as a dynamical and intepretable graph. We have provided learning guarantees for
an idealized version of HCM, shown how HCM’s interpretability can facilitate generalization, and
demonstrated that HCM learns meaningful representations from visual, temporal, visual-temporal,
and language data.
Although we have showcased HCM’s abilities across a set of diverse experiments, some challenges
remain. First of all, HCM is currently not computationally efficient, such that we needed to run
the online version for most of the presented tasks. There are several directions that could enhance
HCM’s computational efficiency. One method to speed up learning could be to stitch multiple
chunks together in one decision. Another direction could be to have the chunk decision process
between all of the acquired chunks separated from the process of parsing observations, which could
be used for parallelized implementations. We believe that scaling HCM up will be beneficial to learn
in increasingly more complex data sets than the ones we have applied here.
Secondly, all patterns of chunks in the current project came from adjacent events in the sequences.
This feature was inspired by the Gestalt principle of grouping by proximity. Yet many patterns ob-
served in natural data sets might exhibit non-adjacent dependencies in space or time. The adjacency
assumption therefore limits the model from detecting such patterns. How to relax the adjacency
assumption as a grouping criterion to allow for non-adjacent relationships to be chunked together
remains an open challenge. Furthermore, HCM currently assumes that there is a hierarchy of chunks
which occur independently in the observational sequence. This set-up was intended to be a simpli-
fying assumption for a first approach toward building a cognitively plausible hierarchical chunking
model. Nonetheless, our approach can and should be extended to more sophisticated assumptions
such as higher order Markovian dependencies between chunks. Finally, HCM passes visual data
“as is” and does not take into consideration any additional assumptions about visual inputs such as
translation or rotational symmetries, which humans can detect when perceiving visual structures.
There are also several avenues for future investigations. One immediate step is to run HCM on other,
more complex data sets such as musical scores, neural data, and large natural language corpora, to
name but a few. Furthermore, we intend to not only run HCM on raw visual inputs directly but also
to employ neural networks to compress inputs into a latent space and then train HCM on these latent
dimensions (Franklin et al., 2020). Additionally, one could also use deep learning models to label
parts of objects and then train HCM on the movements of these labelled parts (Insafutdinov et al.,
2016). Finally, we are currently only testing for independence when deciding on whether or not to
chunk, although other statistical tests are conceivable. One general class of tests could be to assess
whether or not chunks are causally related with each other, in an attempt to find the best causal
structure to explain the sequential, observational data Heinze-Deml et al. (2018). This would make
HCM a useful model of causal representation learning (Scholkopf et al., 2021).
9
Under review as a conference paper at ICLR 2022
8	Reproducibility Statement
Detailed information about the HCM algorithm, proof, generative model, independence test and
experimental details and results can be found in the supplementary information section. The code
used for the algorithm and experiments will be available as a comment to the reviewers and area
chairs as a link to an anonymous repository as soon as the discussion forum for all submitted papers
is open.
References
Johannes Bill, Hrag Pailian, Samuel J. Gershman, and Jan Drugowitsch. Hierarchical structure is
employed by humans during visual motion perception. Proceedings of the National Academy of
Sciences, 117(39):24581-24589,2020. ISSN0027-8424. doi: 10.1073∕pnas.2008961117. URL
https://www.pnas.org/content/117/39/24581.
Timothy F. Brady, Talia Konkle, and George A. Alvarez. Compression in Visual Working Mem-
ory: Using Statistical Regularities to Form More Efficient Memory Representations. Journal of
Experimental Psychology: General, 138(4), 2009. ISSN 00963445. doi: 10.1037/a0016797.
Francois Chollet. Deep Learning with Python. Manning Publications Co., USA, 1st edition, 2017.
ISBN 1617294438.
Brian J Compton and Gordon D Logan. Evaluating a computational model of perceptual grouping
by proximity. Perception & Psychophysics, 53(4):403-421, 1993.
Don C Donderi. Visual complexity: a review. Psychological bulletin, 132(1):73, 2006.
Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608, 2017.
Dennis E. Egan and Barry J. Schwartz. Chunking in recall of symbolic drawings. Memory &
Cognition, 7(2), 1979. ISSN 0090502X. doi: 10.3758/BF03197595.
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias SablC-Meyer, Lucas Morales, LUke Hewitt,
Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Bootstrapping in-
ductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd ACM SIG-
PLAN International Conference on Programming Language Design and Implementation, PLDI
2021, pp. 835-850, New York, NY, USA, 2021. Association for Computing Machinery. ISBN
9781450383912. doi: 10.1145/3453483.3454080. URL https://doi.org/10.1145/
3453483.3454080.
Shai Fine, Yoram Singer, and Naftali Tishby. The hierarchical hidden markov model: Analysis and
applications. Machine learning, 32(1):41-62, 1998.
Jerry A. Fodor and Zenon W. Pylyshyn. Connectionism and cognitive architecture: A critical anal-
ysis. Cognition, 28(1-2):3-71, 1988. doi: 10.1016/0010-0277(88)90031-5.
Nicholas T Franklin, Kenneth A Norman, Charan Ranganath, Jeffrey M Zacks, and Samuel J Ger-
shman. Structured event memory: A neuro-symbolic model of event cognition. Psychological
Review, 127(3):327, 2020.
Vicky Froyen, Jacob Feldman, and Manish Singh. Bayesian hierarchical grouping: Perceptual
grouping as mixture estimation. Psychological Review, 122(4):575, 2015.
Fernand Gobet, Peter C.R. Lane, Steve Croker, Peter C.H. Cheng, Gary Jones, Iain Oliver, and
Julian M. Pine. Chunking mechanisms in human learning. Trends in Cognitive Sciences, 5(6),
2001. ISSN 13646613. doi: 10.1016/S1364-6613(00)01662-4.
Christina Heinze-Deml, Marloes H. Maathuis, and Nicolai Meinshausen. Causal Structure Learn-
ing. Annual Review of Statistics and Its Application, 2018. ISSN 2326-8298. doi: 10.1146/
annurev-statistics-031017-100630.
10
Under review as a conference paper at ICLR 2022
Geoffrey Hinton. Some demonstrations of the effects of structural descriptions in men-
tal imagery*. Cognitive Science, 3(3):231-250, 1979. doi: https://doi.org/10.1207/
s15516709cog0303\_3. URL https://onlinelibrary.wiley.com/doi/abs/10.
1207/s15516709cog0303_3.
Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele.
Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European
Conference on Computer Vision, pp. 34-50. Springer, 2016.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, RUss Bates, AUgUstin %idek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
Iring Koch and Joachim Hoffmann. Patterns, chunks, and hierarchies in serial reaction-time tasks.
Psychological Research, 63(1), 2000. ISSN 14302772. doi: 10.1007/PL00008165.
B. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of
sequence-to-sequence recurrent networks. In ICML, 2018.
B. Lake, R. Salakhutdinov, and J. Tenenbaum. Human-level concept learning through probabilistic
program induction. Science, 350:1332 - 1338, 2015.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. doi:
10.1017/S0140525X16001837.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Raphaelle Malassis, Arnaud Rey, and Joel Fagot. Non-adjacent dependencies processing in human
and non-human primates. Cognitive Science, 42(5):1677-1699, 2018.
G. Marcus. Deep learning: A critical appraisal. ArXiv, abs/1801.00631, 2018.
Denis Mareschal and Robert French. Tracx2: a connectionist autoencoder using graded chunks
to model infant visual statistical learning. Philosophical Transactions of the Royal Society B:
Biological Sciences, 372:20160057, 11 2016. doi: 10.1098/rstb.2016.0057.
Stewart M McCauley and Morten H Christiansen. Computational investigations of multiword
chunks in language learning. Topics in Cognitive Science, 9(3):637-652, 2017.
George A. Miller. The magical number seven, plus or minus two: some limits on our capacity for
processing information. Psychological Review, 1956. ISSN 0033295X. doi: 10.1037/h0043158.
Diana M Mussgens and Fredrik Ull6n. Transfer in Motor Sequence Learning: Effects of Practice
Schedule and Sequence Context. 9(November), 2015. doi: 10.3389/fnhum.2015.00642.
Timothy J O’Donnell, Joshua B Tenenbaum, and Noah D Goodman. Fragment grammars: Exploring
computation and reuse in language. 2009.
Katrin Ortmann. Chunking historical german. In NODALIDA, 2021.
Virginia B. Penhune and Christopher J. Steele. Parallel contributions of cerebellar, striatal and M1
mechanisms to motor sequence learning, 2012. ISSN 01664328.
Pierre Perruchet and Annie Vinter. Parser: A model for word segmentation. Journal of Mem-
ory and Language, 39(2):246 - 263, 1998. ISSN 0749-596X. doi: https://doi.org/10.1006/
jmla.1998.2576. URL http://www.sciencedirect.com/science/article/pii/
S0749596X98925761.
Pierre Perruchet, BenediCte Poulin-Charronnat, Barbara Tillmann, and Ronald Peereman. New
evidence for chunk-based models in word segmentation. Acta psychologica, 149:1-8, 2014.
Dwight J Peterson and Marian E Berryhill. The gestalt principle of similarity benefits visual working
memory. Psychonomic bulletin & review, 20(6):1282-1289, 2013.
11
Under review as a conference paper at ICLR 2022
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should i trust you?": Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ,16, pp. 1135-1144, New York, NY, USA,
2016. Association for Computing Machinery. ISBN 9781450342322. doi: 10.1145/2939672.
2939778. URL https://doi.org/10.1145/2939672.2939778.
David A. Rosenbaum, Sandra B. Kenny, and Marcia A. Derr. Hierarchical control of rapid movement
sequences. Journal of Experimental Psychology: Human Perception and Performance, 1983.
ISSN 00961523. doi: 10.1037/0096-1523.9.1.86.
Wojciech Samek, Thomas Wiegand, and KlaUs-Robert Muller. Explainable artificial intelligence:
Understanding, visualizing and interpreting deep learning models. ITU Journal: ICT Discoveries
- Special Issue 1 - The Impact of Artificial Intelligence (AI) on Communication Networks and
Services, 1:1-10, 10 2017.
Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
AnirUdh Goyal, and YoshUa Bengio. Toward caUsal representation learning. Proceedings of
the IEEE, 109(5):612-634, 2021.
Eric Schulz, Joshua B. Tenenbaum, David Duvenaud, Maarten Speekenbrink, and Samuel J. Ger-
shman. Compositional inductive biases in function learning. Cognitive Psychology, 2017. ISSN
00100285. doi: 10.1016/j.cogpsych.2017.11.002.
Eric Schulz, Francisco Quiroga, and Samuel J Gershman. Communicating compositional patterns.
Open Mind, 4:25-39, 2020.
Emile Servan-Schreiber and John Anderson. Learning artificial grammars with competitive chunk-
ing. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16:592-608, 07
1990. doi: 10.1037/0278-7393.16.4.592.
Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical
journal, 27(3):379-423, 1948.
Noel E Sharkey and Amanda JC Sharkey. An analysis of catastrophic interference. Connection
Science, 1995.
Sun Si, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu, and Jie Bao. Joint keyphrase chunking and
salience ranking with bert, 04 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Yee Whye Teh. A hierarchical bayesian language model based on pitman-yor processes. In Proceed-
ings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics, pp. 985-992, 2006.
Johan Wagemans, Jacob Feldman, Sergei Gepshtein, Ruth Kimchi, James R Pomerantz, Peter A
Van der Helm, and Cees Van Leeuwen. A century of gestalt psychology in visual perception: Ii.
conceptual and theoretical foundations. Psychological bulletin, 138(6):1218, 2012.
Feifei Zhai, Saloni Potdar, Bing Xiang, and Bowen Zhou. Neural models for sequence chunking. In
AAAI, 2017.
A	Independence Test
We use hypothesis testing on the assumption of independence to decide whether there exists an
association between consecutive parses of any two chunks cL and cR in the current belief set. If the
independence test is violated, then the two chunks are combined together. Another independence
test is used to evaluate if there are still statistical associations between chunk observations for each
possible chunks in the current belief set; this is used as a criterion to continue or halt the chunking
process.
12
Under review as a conference paper at ICLR 2022
We use a χ2-test of independence to assess if the consecutive parses of cL and cR observed in T
violate the independence criterion. Observations of cL and cR in parses are categorical variables
and can be represented as rows and columns of a contingency table. The number of observations
that cL = 1 or any other observations (cL = 0) consists of the row entries, indicating observations
of cL, while the number of observations cR = 1 and cR = 0 make up the column entries. The table,
therefore, consists of two rows and two columns.
The null hypothesis is that the occurrence of the consecutive observations is statistically indepen-
dent. Given the hypothesis of independence, the theoretical frequency for observing cL followed
by cR is E[cL = 1, cR = 1] = Np(cL = 1)p(cR = 1), with N being the total number of parses.
P(CL = 1) = N (CL ) = N (CL→cR) + N (CL→-cR) p(cR = 1) = N (CR) = N (CL →cR) + N (-cL→cR )
Σ Σ
l={0,1} r={0,1}
χ
2
N(CL = l,CR = r) - E[cl = l,CR = r]
E[cl = l,CR = r]
Nl=X1} r=Xi}P(CL= I)P(CR= " : K O )
(1)
The degree of freedom for this test is 1. A χ2-probability of less than or equal to 0.05 is used as a
criterion for rejecting the hypothesis of independence.
The independence test is also employed to evaluate whether there are still statistical associations
between chunk observations for each possible chunk in the current belief set, which we use as a
criterion to continue or to halt the chunking process. For this test, the contingency table contains
rows and columns corresponding to all possible chunks in the current belief set, and the χ2-statistic
is calculated as:
Σ Σ
CL ∈sB CR =sB
N(cl, CR) - E[cl, cr]
E[cl,cr]
N	P(CL)P(CR)(
CL ∈sB CR =sB
NcNR - P(CL)P(CR))
P(CL)P(CR^)
(2)
The degrees of freedom are (|sB| - 1) * (|sB| - 1), and aP-ValUe of P ≤ 0.05 is again used as a
criterion to reject the null hypothesis and therefore as evidence to continue the chunking process.
χ
2
B Rational Chunking Algorithm
Algorithm 1: Rational Chunking Algorithm
input : Seq, maxIter
output： Bd, Gd, Td, Md
d — 0, iter — 0;
Bd, Md, Td = getSingleElementSets(Seq);	/* minimally complete atomic set
*/
while !IndependenceTest(Md, Td) or iter ≤ maxiter do
Md, Td = Parse(Seq, Bd);
cl, cr — None; MaxChunk, MaxChunkP — None; PreCk = {};
for(Ci,Cj) ∈ Bd\{0} × Bd\{0} do
Pd(Ci ㊉ Cj) = CalculateJoint(Md, Td,Ci,Cj);
Pd+1(c ㊉ Cj) = 1 PpCiCXjj);
if Pd+ι(ci ㊉ Cj) ≥ MaxChunkP and Ci ㊉ cj ∈ PreCk and !IndependenceTest(Ci, Cj)
then
cL — ci, cR — cj;
MaxChUnkP - Pd+ι(ci ㊉ Cj), MaxChunk — Ci ㊉ Cj
end
end
C — CL ㊉ Cr; Bd+1 J Bd ∪ c; Gd+1 -AugmentGraph(Gd, (cl, c), (cr, c));
P reC k.add(C);
end
13
Under review as a conference paper at ICLR 2022
C Online HCM with Generalization to Visual-Temporal
Sequences
HCM learns a chunk hierarchy graph G by training on a visual-temporal sequence. The visual-
temporal chunks can retain various continuous shapes and may not fill the entire space, which means
that there are no jumps from any visual temporal pixel within a chunk to another. Within a chunk,
there is always a path that connects any two visual temporal pixels.
The input chunk hierarchy graph G could be an empty graph which corresponds to the case that the
HCM model has not been trained yet, or it can be a chunk hierarchy graph that HCM has acquired
from training. M contains the frequency count of each chunk in the belief set B. T stores pairs of
parsed adjacent chunks and their difference in temporal lag. The temporal lag is the time difference
between the end of the previous chunk and start of the following chunk. The generative model of
the world assumed by the chunk learning model is such that each visual temporal chunk occurs
independently for each parse. It then keeps checking whether adjacent chunks in the visual and
temporal domain violate the independence testing criterion. If so, then the visual/temporal adjacent
chunks are grouped together. The constituent parts of a chunk remains in the belief set.
Algorithm 2: Visual-Temporal Chunking
input : Seq, G, θ, DT
output: G
_ _ ʌ _ _ ʌ —
M, T V- G.M, G.T;
PrevioUsChUnkBoUndaryRecord - [];	/* Record Chunk Endings */
ChunkTerminationTime.setall(-1);
while Sequence not over do
CUrrentChUnks, ChUnkTerminationTime =
IdentifyTheLatestChUnks(ChUnkTerminationTime);
ObservationToExplain - refactor(Seq, ChUnkTerminationTime);
for Chunk in CurrentChunks do
for CandidateAdjacentChunk in PreviousChunkBoundaryRecord do
if CheckAdjacency(Chunk, CandidateAdjacentChunk) then
n k E F工	τ	八，	一	…	，八	…	…	八，	，
M , T , B, G - LearnChUnking(ChUnk, CandidateAdjacentChUnk.
_ _ _ ʌ
M, T, B, G);
end
end
ChUnkTerminationTime.Update(CUrrentChUnks)
end
PrevioUsChUnkBoUndaryRecord.add(CUrrentChUnks);
Forgettmg(M, T, B, G, θ, DT, PreviousChunkBoundaryRecord);
end
The pseudocode for the Visual-Temporal HCM is shown in Algorithm 2. The input can be a visual-
temporal sequence, an i.i.d visual sequence, or a temporal sequence.
To process and update chunks online, HCM iterates through the visual temporal sequence, identifies
chunks, and marks the termination time corresponding to each visual dimension stored in Chunk-
TerminationTime. ChunkTerminationTime is a matrix with its size being the same as the visual
dimension that stores the end point in each visual dimension that the previous chunk finished. Be-
cause chunks could finish at different time points in each visual temporal dimension, the algorithm
explain chunks starting at the end point in each visual temporal dimension when the last identified
chunk is finished up until the current time point to identify current, on-going chunks. The chunks
with the biggest visual temporal volume are prioritized to explain the relevant observations up to the
current time point. As multiple visual temporal chunks can be identified to occur simoutaneously,
CurrentChunks is a set that stores the identified chunk that has not reached its end point.
Once one or several chunks are identified to be ending at a time point, they are stored inside the Pre-
viousChunkBoundaryRecord and their finishing time is updated for each visual pixel in ChunkTer-
minationTime. Corresponding entries in M are updated upon once a chunk has ended. The chunks
14
Under review as a conference paper at ICLR 2022
that finishes after the start of the current chunk is checked with each current chunk on whether there
is a visual temporal adjacency. Additionally, the independence test between adjacent chunk pairs
are carried out for the decision to possibly merge chunks.
Once a pair of adjacent chunks cL cR violates the independence testing criterion, they are combined
into one chunk CL ㊉ cr. A new entry is created in M with the estimatedjoint occurrence frequency
for CL ㊉ Cr, which is subtracted from the marginal record of CL and cr. The transition entries
associated with CL ㊉ CR are initialized to be 1. Other chunks inherit the transition entries from CR
to other chunks.
At each time step the visual temporal chunking algorithm does the following things:
1.	Identifies the chunks biggest in volume that explain observation in each dimension from
the time point when the last chunk ended to the current time point, mark their time point,
and store them in the set of current chunks.
2.	Identifies adjacent chunks in previous chunks with each of the currently ending chunk and
updates their marginal and transition counts.
3.	Modifies the set of chunks used to parse the sequence based on their adjacencies.
Entries in M and T are subject to memory decay at the rate of θ . If any entry goes below the
deletion threshold DT, their corresponding entries in M, T, B and G are deleted.
D Details to the Proof of Recoverability
D.1 Definitions
An observational sequence is made up of discrete, integer valued, size-one elementary observational
unit coming from an atomic alphabet set A0, where 0 represents the empty observation set.
One example of such an observational sequence S is:
010021002112000...
The elementary observation units such as ‘0’, ‘1’, and ‘2’ come from the atomic alphabet set A0 =
{0, 1, 2}.
Definition 1 (Chunk)
A chunk is composed of several non-empty, concatenated elementary observations, embedded in the
sequence. That is, a chunk can be made up from any combination of elements in A0 \ {0}
Examples of chunks from the observational sequence can be ‘1’, ‘21’, ‘211’, ‘12’, ‘2112’, ... etc. 0
represents an empty observation in the sequence.
Definition 2 (BeliefSet)
A belief set is the set of chunks that the HCM uses to parse training sequences. The belief set is
denoted as B.
An example of a belief set that that the model has learned from S could be B =
{0, 1, 21, 211, 12, 2112}.
Definition 3 (Parsing)
The parsing of chunks initiates from the beginning of the sequence. At every parsing step, the biggest
chunk in the belief set that matches the upcoming sequence is chosen to explain the observation. The
end of the previous chunk parse initiates the next parse.
The sequence S parsed by the model that uses the belief set {0, 1, 21, 211, 12, 2112} results in the
following partition. 0 1002100 2112 000.
We say that a belief set is complete ifat any point when the model parses the sequence, the upcoming
observations can be explained at least by one chunk within the belief set.
15
Under review as a conference paper at ICLR 2022
Figure 6: Illustration of Visual Temporal Chunks
In this work, we only refer to complete belief sets.
Definition 4 (Parsing Length NB )
A parsing length NB of a sequence parsed by a belief set B is the length of the parsing result after
the sequence has been parsed by chunks within B.
Definition 5 (Count Function NB (c))
NB(c) denotes the count of how many times the chunk c in the belief set B appears in the parsed
sequence.
The parsing length and the count function for all of the chunks c on a belief set B satisfy the following
relation:
NB =	NB (c)	(3)
c∈B
Definition 6 (NB (X → y))
The number of times chunk x is being parsed following the parse of chunk y . x and y are both
chunks in the belief set B.
For any chunk x within any belief set B, one can relate NB(x) with NB(x → y) by:
NB(x) = XNB(x → y)	(4)
y∈B
When the length of the sequence becomes infinite, it is easier to work with probabilities instead of
the count function. One can define a probability space over the belief set:
Definition 7 (Probability space of a belief set)
With a belief set B, one can define a associated probability space (SB, FB, PB). SB is the sample
space representing all of the possible outcomes of a chunk parse.
An event space F is the space for all possible sets of events. F contains all the subsets of SB .
Additionally, the probability function PAB : FB → R is defined on the event space SB . The proba-
bility function PAB satisfies the basic axioms of probability:
•	PAB(E) ≥ 0 ∀E ∈ F. For any subset in the event space, the probability of an observation
being in the subset is positive.
•	M,N ∈ F, andM∩N = E, thenP(M∪N) = P(M)+P(N). For two non-intersecting
subsets in the event space, the probability of observing any element that falls within the
union of the two subsets is the sum of the probability of observing any event within one
subset and the probability of observing any event from the other subset.
•	P(S) = L The Probability of observing any event that belongs to the SamPle SPaCe is one.
16
Under review as a conference paper at ICLR 2022
The probability of a chunk c on a support set of chunks B, is the limiting case of this ratio when NB
goes to infinity.
PB(c)
lim nb(c)
NB →∞ NB
(5)
A learning model keeps track of the occurrence probability associated with each chunk in the be-
lief set. For a current belief set, the model assumes that the chunks within the belief set occurs
independently.
The occurrence probability of chunk ci with the belief set Bd is PBd (ci), which refers to the nor-
malized frequency of observing chunk ci when the number of parsing the sequence using chunks
from the belief set goes to infinity. In this way, the probability of observing a sequence of chunks
c1,c2,  cN can be denoted as P(c1,c2,  cN). The joint probability of observing any chunk in the
generative process is:
P (c1 , c2 ,  cN ) =	PBd (ci)	(6)
ci∈Bd
In this formulation, chunks as observation units serve as independent factors that disentangle obser-
vations in the sequence.
Definition 8 (Marginal Parsing Frequency Md)
A vector that stores the number of parses for each chunk c in the belief set Bd.
Md contains size |Bd |. Additionally the model keeps track of the transition probability from one
chunk to another, as they are used to test whether two chunks have immediate temporal adjacency
association.
Definition 9 (Td)
The set of transition frequency from any chunk ci ∈ Bd to cj ∈ Bd
Definition 10 (Chunk Hierarchy Graph Gd)
The relation between chunks and their constructive components in the generative model is described
by a chunk hierarchy graph Gd with vertex set VAd and edge set EAd . One example of a chunk
hierarchy graph is illustrated in Figure 1 a). In this hierarchical generative model, d is the depth of
the graph and Ad is the set of chunks used as atomic units to construct the sequence. Each vertex in
VAd is a chunk, and edges connect the parent chunk vertices to their child chunk vertices.
As the belief set B keeps changing when one modifies the chunks in a sequence, so does the parsing
length NB and the probability associated with the belief set PAB . Based on the definition of N
on how chunks are parsed when the support set changes, this change of N changes a set of
constraints on the probabilities defined on the new, augmented support set. To do this, we
need to:
•	Formulate the definition of probability based on N.
•	Identify all relevant changes of N before and after the chunk update.
•	Translate this change of N to the constraints on probability updates.
We derive the relation between the probabilities when two chunks cL and cR ∈ Ad are concatenated
together to form a new chunk CL ㊉ CR and update the alphabet to Ad+ι.
D.2 Identify N Updates
This is how the count function changes before and after the update. We start with the update of the
summary N when the alphabet goes from Ad to Ad+1 , and proceed to the update for the marginal N,
and then the transitional N.
17
Under review as a conference paper at ICLR 2022
D.2.1 Summary N
When going from Ad to Ad+1, cL and cR are both chunks in Ad and merged together as anew chunk
to augment Ad .
The number of parses changes only in places where the cL and cR associated with the new chunk
occurs. The chunks in Ad can be divided into three groups, cL, cR, and Ad \ {cL , cR}. The count
function associated with chunks from the set Ad \ {cL , cR} does not change when Ad updates to
Ad+1.
Since with the chunk update Ad+1, cL and cR are chunked together, they are recognized together
as a whole, so every time when they are recognized together as a whole, the count reduces twofold.
Nd+1(cL) and Nd+1(cR) are the number of counts for cL and cR when they do not occur together.
This count is different from Nd (cL) and Nd(cR) because the occasions when they occur one after
another is taken into the count by Nd+ι(u ㊉ cr). The relation between Nd and Nd+ι is:
Nd+1 =	E	Nd(C) + Nd+ι(cL + Nd+ι(cR) + Nd+ι("㊉ CR)	⑺
c∈Ad-cL -cR
Nd+1(CL) could occur in the case where CR does not follow CL.
Nd+1(CL ㊉ CR) = Nd(CL → CR)	(8)
Nd+1 (CL) = Nd(CL) - Nd(CL → CR)	(9)
Nd+1 (CR) = Nd(CR) - Nd(CL → CR)	(10)
Nd+ι(CL ㊉ cr) * 2 + Nd+ι(α) + Nd+ι(CR) = Nd(CL) + Nd(CR)	(11)
Chunking reduces the number of times sub-chunks are being parsed when sub-chunks occur right
after each other by twofold.
Nd=	Nd(C) =	Nd(C) +Nd(CL)+Nd(CR)	(12)
c∈Ad	c∈Ad-cL-cR
Comparing the above two equations we arrive at
Nd 一 Nd(CL)-	Nd(CR)	= Nd+ι —	Nd+ι(CL)	-	Nd+ι(CR)	- Nd+、® ㊉	CR)	(13)
Since:
Nd(CL) + Nd(CR) = Nd+ι(α) + Nd+ι(CR) + 2Nd+ι(CL ㊉ cr)	(14)
This is related to:
Nd(CL → CR) = Nd+ι(cL ㊉ CR)	(15)
Because both are counting the number of times they occur consecutively.
Nd+1 (CL) + Nd+1(CR) = Nd(CL) + Nd(CR) - 2Nd(CL → CR)	(16)
One can define Nd+1 in terms of counts in Nd as:
Nd+1 =	X	Nd(C) + Nd(CL) + Nd(CR) - 2Nd(CL → CR) + Nd(CL → CR)
c∈Ad-cL-cR
Nd+1 =	X	Nd(C) + Nd(CL) + Nd(CR) - Nd(CL → CR)
c∈Ad-cL-cR
(17)
18
Under review as a conference paper at ICLR 2022
Generally, the result is also the case with the total count Nd and Nd+1 when one switches from the
alphabet set Ad to Ad+1 by chunking cL and cR in Ad together.
Nd+1 =	Nd(c) + Nd(cL) + Nd(cR) - Nd(cL → cR)
c∈Ad -cL -cR
Nd+1 = Nd - Nd(cL → cR)
(18)
(19)
D.2.2 Marginal N
To proceed into formulating the joint and conditional probability given a particular belief space, we
need to formulate how the count of N(c) for a chunk changes when the belief space when switching
from Ad to Ad+1, with the same division as before.
Of course, the count function should be fixed. However, the probability function associated with the
chunks will change based on the update of the belief set. We use the update of the count function to
find the relation between the probability updates.
For all X in Ad - {cl, cr, CL ㊉ CR}：
Nd+1 (x) = Nd(x)	(20)
Nd+1(CR) = Nd(CR) - Nd(CL → CR)	(21)
Nd+1 (CL) = Nd(CL) - Nd(CL → CR)	(22)
Nd+1(CL ㊉ CR) = Nd(CL → CR)	(23)
D.2.3 Transitional N
The following relationship needs to hold：
for all x, y in Ad \ {cl, cr, CL ㊉ cr}:
Nd+1(x → y) = Nd(x → y)	(24)
Nd+1 (X → CL) = Nd(X → CL) - Nd+1 (X → CL ㊉ CR)	(25)
Nd+1(x → CR) = Nd(x → CR)	(26)
Nd+1(x → cL	㊉ CR)	=	Nd+1(X)	-	Nd+1(x →	cR)	-	Nd+1(x →	CL)	- Nd+1(x	→	y)	(27)
Nd+ι(x → CL ㊉ cr) ≤ Nd(CL → cr)	(28)
From CL we have this relation.
Nd+1(CL → X) = Nd(CL → X)	(29)
Nd+1(CL → CL) = Nd(CL → CL) - Nd+1(CL → CL ㊉ Cr)	(30)
Nd+1 (CL → CR) = 0	(31)
Nd+1(CL → cL ㊉ CR) = Nd+1(CL) - Nd+1(CL → CR) - Nd+1(CL → cL) - Nd+1(CL → X) (32)
Nd+1(CL → CL	㊉ CR)	≤	Nd(CL	→ Cr)	(33)
Nd+1(CL → CL	㊉ CR)	≤	Nd(CL	→ Cr)	(34)
From CR we have the following relation:
Nd+1(CR → y) = Nd(CR → y) - Nd+1(CL ㊉ CR → y)	(35)
Nd+1 (CR → CL) = Nd(cR → CL) - Nd+1(CL ㊉ CR → CL)	(36)
Nd+1(CR → Cr) = Nd(cR → Cr) - Nd+1® ㊉ CR → Cr )	(37)
Nd+1(CR → cL ㊉ CR) = Nd+1(CR ) - Nd+1(CR → CR) - Nd+1(CR → CL) - Nd+1(CR → y) (38)
Nd+1(CR → CL ㊉ Cr) ≤ Nd(CL ㊉ Cr)	(39)
19
Under review as a conference paper at ICLR 2022
From the chunked unit CL ㊉ CR We have the following relation:
Nd+1® ㊉ CR → x) ≤ min(Nd(CL ㊉ CR),Nd+ι(cR ㊉ x))	(40)
Nd+1® ㊉ cr → CL) ≤ min(Nd(CL ㊉ CR),Nm(cr ㊉ CL))	(41)
Nd+1(CL ㊉ CR → CR) ≤ min(Nd+ι(CL ㊉ CR), Nd+i(CR ㊉ CR))	(42)
Nd+1 (CL ㊉ Cr → CL ㊉ CR) ≤ min(Nd+ι(CL ㊉ CR),Nm(cr ㊉ CL))	(43)
Finally, we need to satisfy the marginal constraint, that is:
Nd+l(cL㊉CR) = Nd+l(cL㊉CR → Cl) + Nm+i(cl㊉CR → x) + Nd+l(cL㊉CR → Cr)+Nd+l(α㊉CR → CL㊉CR)
(44)
D.3 PROBABILITY DENSITY SWITCH WHEN Ad EXPANDS TO Ad+1
The constraint is: the number of counts N for all chunks defined for the support set Ad must remain
the same for the support set sAd+1 , so that the definition of PAd for all relevant chunks within Ad
remains the same when Ad expands to Ad+1.
Relating the number of counts to probability: The probability of a chunk occurring in the alphabet
set Ad is defined as:
PAd(C)
lim
Nd→∞
Nd(C)
~nΓ
(45)
Because Nd and Nd+1 are only a constant away, both go to infinity if one of them does, so there is
a relation between the definition of probability PAd (C) and PAd+1 (C). For any chunk x in Ad that is
not CL and CR, Nd+1(x) = Nd(x):
PAd+1 (x)
lim
Nd+1 →∞
Nd+ι(x)
Nd+ι
Nd(x)
lim	/	、
Nd→∞ Nd - Nd(CL → CR)
That is, the probability of a chunk of this category at d and d+1 satisfies this relationship:
Nd+li1m→∞PAd+1(x)Nd+1=Nldi→m∞PAd(x)Nd
PAd+i (X) = PAd (x)∏⅛⅛
PAd+1
For CL and CR in Ad+1:
(x) = PAd (x)
________limNd→∞ Nd________
limNd+1
→∞ Nd - Nd(CL → CR)
PAd+1 (CL) =	lim Nd+1→∞	Nd+1(CL) Nd+ι
PAd (CL) =	lim	Nd(CL)
	Nd+1→∞	Nd
PAd 1 (CR) =	lim	Nd+ι(cr)
d+1	Nd+1→∞	Nd+ι
PAd (CR)	= lim Nd→∞	Nd(CR)
		Nd
Nd+1(CL) = Nd(CL)- Nd(CL ㊉ cR)
Because:
(46)
(47)
(48)
(49)
(50)
(51)
(52)
(53)
(54)
PAd+1 (CL)
lim	Nd(CL)-Nd(CL ㊉ CR)
Nd+1 →∞	Nd+1
(55)
Nd(CL) - Nd(CL → CR)
Ad+1 (CL) = Ndi→∞	Nd - Nd(CL → CR)
(56)
20
Under review as a conference paper at ICLR 2022
Finally:
PAd+1 (cR)
lim	Nd(CR) - Nd(CL → CR)
Nd→∞	Nd — Nd(CL → cr)
PAd+ι (CL ㊉ CR)
lim
Nd+1→∞
Nd+l(CL ㊉ CR)
Nd+
PAd (CL ㊉ CR)
lim
Nd→∞
Nd(CL → CR)
Nd
since Nd(CL → CR) = Nd+ι(s ㊉ cr), We have
PAd+1 (CL ㊉ CR)
lim	PAd (CL㊉ CR)Nd
Nd→∞	Nd+1
D.3.1 Summary Probabilities
Nd+1 = Nd — Nd(CL ㊉ Cr) = Nd - NdPd(CL ㊉ CR)
ʌ?+1 = 1 — Pd(CL ㊉ CR)
Nd
(57)
(58)
(59)
(60)
(61)
(62)
D.3.2 Marginal Probabilities
The constraints on marginal probabilities When the support set changes from Ad to Ad+1, derived
from the constraints on the marginal counts, are the folloWing:
Pd+1(x)
Pd(X)
1 — Pd(CL ㊉ CR)
(63)
Pd+1 (CR)
Pd(CR) - Pd(CL ㊉ CR)
1 一 Pd(CL ㊉ Cr)
Pd+1(CL)
Pd(CL) 一 Pd(CL ㊉ CR)
1 一 Pd(CL ㊉ Cr)
Pd+l(cL ㊉ CR)
Pd(CL ㊉ CR )
1 一 Pd(CL ㊉ CR)
(64)
(65)
(66)
With this set of formulations, We have defined the next level marginal probability measures as a
function of the previous level observations and their implied probability measures.
D.3.3 Transitional Probabilities
for all x, y in Ad — {cl,cr,cl ㊉ cr}, from X we have this relation:
0 ≤ Pd+ι(∙∣x) ≤ 1	(67)
Pd+1(y|x) = Pd(y|x)	(68)
Pd+1(CR|x) = Pd(CR|x)	(69)
Pd+l(cL∣x) + Pd+1(CL ㊉ CR∣x) = Pd(cL |x)	(70)
This equation is sampled so that both Pd+ι(α |x) and Pd+ι(cL ㊉ cr|x) satisfy the following Con-
straints:	Pd+1(CL |x) ≤ Pd+1(cL)	(71) Pd+1(x) D P P I、/ Pd+1(CL ㊉ CR) Pd+1(CL ㊉ Cr |x) ≤ 			L		(72) Pd+1(x)
21
Under review as a conference paper at ICLR 2022
MarginalS
Transitions
Basically, Pd+1(cL|x) is sampled from the range
[0, min{1, P+1I(Cx)) ,Pd(CLIx)”.
Additionally,	Pd+ι (CL ㊉ cr Ix) is constrained to be within this range:
[θ, min{1, PdPd*CR) ,Pd(cL∣x)}].
From CL we have this relation:
Pd+1(xICL)
Pd(x∣CL)
1 - pd(cRlcL)
(73)
Pd+1(CRICL) = 0
Pd+1(CLICL)
Pd(CLICL) - (1 - Pd(CRICL))Pd+l(cL ㊉ CR |。L)
1 - Pd(CRICL)
Put into simplified terms:
Pd+1(CLICL)+Pd+1(CL ㊉CRICL) = 1 P(PcLCRLCL)
(74)
(75)
(76)
P(+i(clIcl) ≤ Pd+1 (CL) = 1	(77)
Pd+1(CL)
Pd+1(cL ㊉ CR)
Pd+1(CL ㊉ CRICL) ≤ --5--7~~∖-	(78)
Pd+1(CL)
Additionally, P√+i(clIcl) is constrained to fall within this range: [0, min{1,「；(；%%)”.
Pd+ι(cL ㊉ CRICL) within the range10, min{1, Pd+1(CL㊉CR), Pd(CLICL) }]
d+1 L R L	,	,	Pd+1 (cL ) , 1-Pd(cR|cL )
From CR we have the following relation:
0 ≤ P(+i(∙Icr) ≤ 1	(79)
Pd+1(yICR)
Pd(yIcR)Pd(cr) - Pd+l(yIcL ㊉ CR)Pd(CL ㊉ CR)
Pd(CR) - Pd(CL ㊉ cR)
(80)
22
Under review as a conference paper at ICLR 2022
Pd+1(cLIcR)	_ Pd(cLIcR)Pd(cR) - Pd+1 (cLIcL ㊉ cR)Pd(cL ㊉ cR)	⑻) Pd(cR) - Pd(cL ㊉ cr)
Pd+1(cRIcR) =	_ Pd(cR IcR)Pd(cR ) - Pd+l(cRIcL ㊉ cR)Pd(cL ㊉ Cr)	他) Pd(cR) - Pd(cL ㊉ cR) Pd+l(∙IcR) ≥ Pd+ι(∙IcL ㊉ cr)	(83)
Additionally, Pd+ι(cL ㊉ cr∣cr) and Pd+ι(cL ㊉ cr∣cl ㊉ CR) needs to satisfy:
Pd+1(cL ㊉ CRIcR) = I- Pd+1(yIcR)- Pd+1(CRIcR)- Pd+1(cL↑cR)	(84)
Pd+1(CL ㊉ CRIcL ㊉ Cr) = 1 - Pd+ι(y∣cL ㊉ Cr) - Pd+ι(cR∣cL ㊉ Cr) - Pd+ι(cL∣cL ㊉ Cr) (85)
For the generative model, when the alphabet set goes from Ad to Ad + 1 by chunking cL and cR
together, the above constraints associated with chunks in Ad and Ad+1 need to be satisfied.
D.4 Hierarchical Generative Model
At the beginning of the generative process, the atomic alphabet set A0 is specified. Another param-
eter, d, specifies the number of additional chunks that are created in the process of generating the
hierarchical chunks. Starting from the alphabet A0 with initialized elementary chunks ci from the
alphabet, the probability associated with each chunk ci in A0 needs to satisfy the following criterion:
X PA0(ci) = 1	(86)
ci∈A0
Meanwhile, P(ci ) ≥ 0, ∀ci ∈ A0 .
We assume that at each step the marginal and transitional probability of the previous steps are known.
The next chunk is chosen as the combined chunks with the biggest probability. The order of con-
struction in the generative model follows the rule that the combined chunk with the biggest proba-
bility on the support set of pre-existing chunk sets is chosen to be added to the set of chunks.
CL ㊉ CR = arg max PAd (cL ㊉ cr)	(87)
cL ,cR ∈Ad \{0}
Under the constraint that:
PAd(cL)PAd (cr) ≤ PAd(cL ㊉ cr) ≤ min{PAd (cl), PAd (cr)}	(88)
This can be calculated from the transitional and marginal probability of the previous step.
cL ㊉ cR = arg max PAd (cL ㊉ cR) = arg max PAd (cL)PAd (cr∣cl)	(89)
CL,CR∈Ad∖{0}	CL,CR∈Ad∖{0}
Once the chunk of the next step: cL ㊉ cr is chosen, the support set changes from Ad to Ad+ι, which
is one size bigger. On the new support set, the marginal probability is specified by the marginal
update. That is, the marginal and transitional probability defined on the support set Ad fully spec-
ifies the marginal probability of the support set Ad. Going from generating a new chunk from the
support set Ad to the set Ad+1, the transitional probabilities between chunks in Ad need to satisfy
the following constraints:
•	PAd+1 (cL∣x) + PAd+1 (cL ㊉ cR∣x) = PAd(cL∣x) forall X in Ad
•	PAd+1 (CLIcL )(I- PAd (CRIcL)) + PAd+1 (CL ㊉ cR|cL)(I- PAd (CRIcL))= PAd (CLIcL)
•	PAd+13匕R)(PAd (CR)	-	PAd (cL	㊉	CR))	+	PAd+ι ^cL	㊉	CR)PAd (CL	㊉	cR)	=
PAd (yIcR)PAd (cR)
•	PAd+ι	(CLIcR)(PAd (CR) -	PAd (cL ㊉	CR))	+	PAd+ι (CLIcL	㊉ CR)PAd (CL	㊉	cR)	=
PAd (cLIcR)PAd (cR)
•	PAd+ι	(CRIcR)(PAd (CR) -	PAd (cL	㊉	CR))	+	PAd+ι (CRIcL	㊉ cR IPAd (CL	㊉	CR)=
PAd(cRIcR)PAd(cR)
23
Under review as a conference paper at ICLR 2022
•	Pd+l(cL ㊉ CR∣Cr) = 1 - Pd+l(y∖cR) - Pd+1(CR\CR)- Pd+l(cL∣CR)
•	Pd+1(CL ㊉ CR∣CL ㊉ CR) = 1 - Pd+l(y∖cL ㊉ CR ) - Pd+l(cR∣CL ㊉ CR) — Pd+l(cL∣CL ㊉ CR)
In total, there are ∖x∖ + ∖y∖ + 3 degrees of freedom. Since ∖x∖ = ∖y ∖ = ∖Ad ∖ - 2, at each step, there
are 2∖Ad ∖ - 1 number of values to be specified.
In practice, after the chunks are specified in Ad, the probability value associated with chunks in A0
are sampled from a flat Dirichlet distribution, which is then sorted so that the smaller sized chunks
contain more of the probability mass and the null-chunk carries the biggest probability mass. Then,
the above constraint is checked for the assigned probability on each of the newly generated chunk
with their associated alphabet set Ai . This process repeats until the probability drawn satisfies the
condition for every newly created chunk.
Theorem 1 (Marginal Probability Space Conservation). After the addition of Cd,i ㊉ Cd,j and the
change of probability, PAd is still a valid probability distribution.
Proof:
PAd (Cd,k) =	PAd-1 (Cd-1,k)+
cd,k ∈Ad	cd,k ∈Ad-1 -cd-1,i -cd-1,j
+ PAd-1 (Cd-1,i) - PAd-1 (Cd-1,j ∖Cd-1,i)PAd-1 (Cd-1,i)	(90)
+ PAd-1 (Cd-1,j) + PAd-1(Cd-1,j∖Cd-1,i)PAd-1(Cd-1,i)
=1
Theorem 2 (Conditional Probability Space Preservation). The conditional probability Pd+1(z∖C)
for any C ∈ Ad+1 after sampling is still a valid distribution.
Proof: Show by manipulating the equations, that the sum of the conditions on c sums to 1, and
each of them is bigger than 0.
Theorem 3 (Measure Space Preservation). Given that at the end of the generative process with
depth d one ends up having an alphabet set Ad, the probability space defined on Ai, which includes
the marginal and joint probability of any chunk and combinations of chunks in Ai, i = 0, 1, 2, . . . d,
which are predecessor alphabet sets of Ad, all values in the set Md and Td remain the same no
matter how the future support set changes according to the generative model.
Proof: By induction.
•	Base case: starting from the initialized alphabet set A0, the probability of PA0 (C), C ∈ A0,
and the probability of PA1 (xy), x, y ∈ A0, for all valid c, x, y, when the alphabet is A1.
Going from A0 to A1, N0(C), N0 and N0(x → y) does not change, therefore PA0 (C) and
PA0 (x → y) at the alphabet A1 is the same as that when the alphabet is A0.
•	Induction Step: starting from the initialized alphabet set Ad, the probability of PAd (C), C ∈
Ad, and the probability of PAd (xy), x, y ∈ Ad, for all valid c, x, y, when the alphabet is
Ad+1. Going from Ad to Ad+1, Nd(C), Nd and Nd(x → y) does not change, therefore
PAd (C) and PAd (x → y) at the alphabet Ad+1 is the same as that when the alphabet is Ad.
Theorem 4. The order of PAi (xy), x, y ∈ Ai for any i = 0, 1, 2, . . . d at any previous belief space
is preserved throughout the update.
Proof: At the end of the generative process with depth d, one ends up having such an alphabet set:
Ad. The probability space defined on Ai , which includes the marginal and joint probability of any
chunk and combinations of chunks in Ai, i = 0, 1, 2, . . . d is preserved, hence the order is preserved.
The generative process can be described by a graph update path. The specification of the initial set
of atomic chunks A0 corresponds to an initial graph G0 with the atomic chunks as its vertices. At
the i-th iteration, as the generative graph goes from GAi to graph GAi+1 , two none zero chunks CL,
cr chosen from the pre-existent set of chunks Ai and are concatenated into a new chunk CL ㊉ CR,
24
Under review as a conference paper at ICLR 2022
augmenting Ai by one to Ai+ι. The vertex set also increments from VAi to ‰+ι = VAi ∪ CL ㊉ CR.
Moreover, two directed edges connecting the parental chunks to the newly-created chunk are added
to the set of edges: EAi to EAi = EAi ∪ (cl,cl ㊉ CR) ∪ (cr,cl ㊉ cr). The series of graphs created
during the chunk construction process going from GA0 to the final graph GAd with d constructed
chunks can be denoted as a graph generating path P(GA0, GA0) = (GA0, GA1, GA2, ..., GAd).
D.5 Learning the Hierarchy
The rational chunking model is initialized with one minimally complete belief set, the learning
algorithm ranks the joint probability of every possible new chunk concatenated by its pre-existing
belief set, and picks the one with the maximal occurrence joint probability on the basis of the current
set of chunks as the next new chunk to enlarge the belief set. With the one-step agglomerated belief
set, the learning model parses the sequence again. This process repeats until the chunks in the belief
set pass the independence testing criterion.
Theorem 5 (Learning Guarantees on the Hierarchical Generative Model). As N → ∞, the chunk
construction graph learned by the model G is the same as the chunk construction graph of the
generative model: G = G, which entails that they have the same vertex set: V = VG and the same
edge set: E = EG. Additionally, the belief set learned by the chunk learning model Bd = Ad, and
the marginal probability evaluated on the learned belief set MBd associated with each chunk is the
same as the marginal probability imposed by the generative model on the generative belief set MAd.
Proof: Given that all of the empirical estimates are the same as the true probabilities defined by
the generative model, we prove that starting with B0, the learning algorithm will learn BD = AD .
AD is the belief set imposed by the generative model. We approach this proof by induction.
Base Step: As the chunk learner acquires a minimal set of atomic chunks that can be used to explain
the sequence at first, the set of elementary atomic chunks learned by the model is the same as the
elementary alphabet imposed by the generative model, i.e. B0 = A0. Hence, the root of the graph,
which contains the nodes without their parents, is the same, G = G; put differently, V0 = V0
Additionally, the learning model approximates the probability of a specific atomic chunk as 九0 3).
As n → ∞, for all chunks C in the set of atomic elementary chunks in B0 , the empirical probability
evaluated on the support set is the same as the true probability assigned in the generative model with
the alphabet set A0 :
PBo (C)= lim	(O)= PAo(C)
n→∞	N0
(91)
Induction hypothesis: Assume that the learned belief set Bd at step d contains the same chunks as
the alphabet set Ad in the generative model.
The HCM, by keeping track of the transition probability between any pairs of chunks, calculates
PBd (Ci∣Cj) for all Ci, Cj in Bd. Afterwards, it finds the pair of chunks Ci, Cj, such that the chunk
created by combining Ci and Cj together contains the maximum joint probability violating the inde-
pendence test as candidate chunks to be combined together.
A / 小、	A /	∖ A / i ∖
PBd (Ci ㊉ Cj ) = SUp PBd (Ci)PBd (CjIci)
(92)
ci,cj ∈Bd
We know that in the generative step the supremum of the joint probability with the support set Ad is
being picked to form the next chunk in the representation graph, so each step of the process at step
d satisfies the condition that:
PAd (Ci ㊉ Cj) =	SUp	PAd (Ci)PAd (Cj ICi)
ci ,cj ∈Ad
(93)
n ∙	7-»	/ I ∖	∙A∕l∖τ-⅝∕∖	τ∖ /	∖	.1	1	1	1	1	F,Il	♦
Since PAd(CjICi) = PBd(CjICi), PAd(Ci) = PBd(Ci), the chunks Ci and Cj chosen by the learning
model will be the same ones as those created in the generative model.
End step: The chunk learning process stops once an independence test has been passed, which
means that the sequence is better explained by the current set of chunks than any of the other possible
next-step chunk combinations. This is the case once the chunk learning algorithm has learned a
belief set Bd that is the same as the generative alphabet set Ad . At this point G = G
25
Under review as a conference paper at ICLR 2022
E Experiment Detail: Chunk Recovery and Convergence
To test the model’s learning behavior on this type of sequential data, random graphs of chunk hi-
erarchies with an associated occurrence probability for each chunk are specified by the hierarchical
generative process. To do so, an initial set of specified atomic chunks A0 and a pre-specified level
of depth (new chunks) d is used to initiate the generation of a random hierarchical generative graph
G. In the end, the generative model generates a set of chunks A. In total, there are |A0 | + d number
of chunks in the generative alphabet A, with chunk c from the alphabet set having an occurrence
probability of PA(c) on the sample space A.
Once the hierarchical generative model is specified, it is then used to produce training sequences
with varying length N to test the chunk recovery.
The rational chunk model is trained on the the sequence S with increasing sizes (from 100 to 3000
with steps of 100) generated by the hierarchical generative graph, and it learns a hierarchical chunk-
ing graph G . To test how good the representation learned by the chunking graph is compared to
the ground truth generative model G, a discrete version of KUllback-Leibler divergence is used to
compare the ground truth probability PA(c) of every chunk on the sample space of the ground truth
alphabet, to the learned probability Q(c) of the chunking model.
KL(P IIQ) = X PA(C)i0g2( PAM)
c∈A	2 QA(c)
(94)
While PA is clearly defined by the generative model, QA(c) needs to be calculated from what the
model has learned. Note that the set of chunks B learned by HCM may or may not be the same as
A, as B also augments or shrinks in different learning stages.
To evaluate QA(c), the hierarchical chunk graph with their occurrence probability associated with
each chunk is used to produce “imagined” sequences of length 1000. Imagined sequences are the
sequences that HCM produces based on its learned representations. After that, the occurrence prob-
ability of each chunk c in A(c) is used to evaluate Q, comparing the HCM’s learned representation
with the ground truth.
KL divergence is used to evaluate the deviation of learned representations in the hierarchical chunk-
ing model from the original representations on the corresponding support set of the original repre-
sentations.
For the comparsion, we used the same sequence used for training HCM to train a 3 layer recurrent
neural network (one embedding layer with 40 hidden units, one LSTM with drop-out rate = 0.2, and
one fully connected layer with batch size = 5, sequence length = 3, epoch = 1, so that the data used
for training is the same as N) of the training sequence, and used it to generate imagined sequences
which are then used to calculate the KL divergence as before.
To generate the KL divergence plot for Figure 2, sequences with varying size N were used to train
HCM. HCM was then used to generate imaginative sequence so that KL measurements can be taken.
F	Experiment Detail: Visual Hierarchical Chunks
The visual hierarchical chunks are crafted by hand as binary arrays. The dark pixels correspond to
having a array value of 1 and background a value of 0. Each image in the generative hierarchy is
25 dimensional (5 x 5) in the visual domain and size 1 in the temporal domain. An empty array is
included to denote no observation. The alphabet A of the generative model include all 14 the images
in the generative hierarchy.
The probability of occurrance for each generative visual chunk is drawn from a flat dirichlet distri-
bution with the empty observation retaining the highest mass, this is to emulate the process that real
world observatipons are mostly sparse in the environment.
26
Under review as a conference paper at ICLR 2022
The parameters (α1, .., αK) with K = |A| are all set to one, and PA(c), c1, ..., cK ∈ A are sampled
from the probability density function
1K
f (x1, …,xk; αι,..∙, aK) =	, ʌ TT Xai I)	(95)
B(a) i=1
Where the beta function when expressed using gamma function is: B(a) = QiPjra，, and a =
(α1, .., αK).
To generate the sequence, images in the hierarchy are sampled from the occurrence distribution and
appended to the end of the sequence. As a result, there are visual correlations in the sequence defined
by the hierarchy, but temporally, each image slice is i.i.d. from the dirichlet distribution.
Examples of chunk representations learned by the hierarchical chunking model at different stages is
shown are collected at the learning stages of t = 10, t = 100, t = 1000 respectively.
G	Experiment Detail: Gif Movement
The gif file is converted into an [T x H x W] sized tensor. With T being the temporal dimension of
the spatial vision sequence. The entire moment is 10 frames of 25 x 25 images. Each color in the
gif file is mapped to a unique integer, with the background having a value of 0. TO construct the
training sequence for spatial temporal chunks. In this way, the gif file is converted into a tensor with
size 10 x 25 x 25. The entire movement is repeated 100 times and trained on HCM.
H	Experiment Detail: The Hunger Games
The first book of The Hunger Games is stored as ’test_data.txt’ with the code in the supple-
mentary material. The text file contains approximately 520,000 characters in total.
To convert the book into a temporal sequences, a unique mapping between each character and an
integer is created. This sequence of integer is used to train the online HCM, with a forgetting rate
of 0.99 and a chunk deletion threshold of 1e-5. Empty spaces are also mapped onto a nonzero
integer to enable the model to learn chunks with the inclusion of empty spaces. The HCM trains
on sequences of 1,000 characters in length at each step. More examples of learned chunks taken
from M at different stages of learning are displayed in Table 2. Simple representation examples are
taken from chunks learned after 10,000 characters in the book. Intermediate examples are taken after
100,000 characters have been parsed, and Complex chunks are taken when HCM reaches 300,000
characters of the book.
27
Under review as a conference paper at ICLR 2022
Table 2: Representation Learned in Hunger Games
Simple Chunks	‘an‘, ‘in ‘，’be‘，’at‘，’me‘，’le‘，’a’，‘ar‘，’re'，‘and ‘，’ve'，‘ing‘，‘on’， ’st’, ’se’, ’to ’, ’i ’, ’n ’, ’of ’, ’he’, ’my ’, ’te’, ’pe’, ’ou’,’we’, ’ad’, ’de’, ‘li‘, ‘oo‘, ‘bu‘, ‘fo‘, ‘ave‘, ‘the‘, ‘ce‘, ‘is‘, ‘as‘,‘il‘, ‘ch‘, ‘al‘, ‘no‘, ‘she‘, ‘ing‘, ‘am‘, ‘ack‘, ‘we‘, ‘raw‘, ‘on the‘,‘day‘, ‘ear‘, ‘oug‘, ‘bea‘, ‘tree‘, ‘sin‘, ‘that‘, ‘log‘, ‘ters‘, ‘wood‘,‘now‘, ‘was‘, ‘even‘, ‘leven‘, ‘ater‘, ‘ever',‘but',‘ith',‘ity’,‘if',‘the wood',‘bell',‘other’	
Intermediate Chunks	'old’, ‘gather’, ‘as’, ‘under', ‘way.’, ‘day’, ‘hunger', ‘very’, ‘death’, ‘ping‘, ‘the seam‘, ‘add‘, ‘ally‘, ‘king‘, ‘lose‘, ‘sing‘, ‘loser‘, ‘money‘, ‘man who‘, ‘in the‘, ‘says‘, ‘tome‘, ‘might‘, ‘rave‘, ‘even‘, ‘ick‘, ‘wood‘, ‘he want‘, ‘for‘, ‘into‘, ‘leave‘, ‘reg‘, ‘lose to‘, ‘lock‘, ‘where‘, ‘up‘, ‘gale‘, ‘older‘, ‘ask‘, ‘come‘, ‘raw‘, ‘real‘, ‘bed‘, ‘ing for‘, ‘from‘, ‘link‘, ‘few‘, ‘close‘, ‘arrow‘, ‘ull‘, ‘cater‘, ‘this‘, ‘one‘, ‘almo‘, ‘lack‘, ‘shop‘, ‘year‘, ‘ring‘, ‘cause‘, ‘is the‘, ‘ugh‘, ‘eve‘, ‘are‘, ‘the leap‘, ‘lly‘, ‘still‘, ‘heal‘, ‘tow‘, ‘never‘, ‘try‘, ‘prim‘, ‘iting‘, ‘bread‘, ‘ould‘, ‘, but‘, ‘Now‘, ‘beg‘, ‘liday‘, ‘arm‘, ‘quick‘, ‘hot‘, ‘men‘, ‘know‘, ‘then‘, ‘bell‘, ‘pan‘, ‘mother‘, ‘only‘, ‘war‘, ‘eak‘, ‘high‘, ‘read‘, ‘district 12‘, ‘can’, ‘would’, ‘pas’,
Complex Chunks	‘arent’, ‘he may’, ‘IWant’, ‘gather’, ‘CaPitol’, ‘been’, ‘trip’, ‘a baker’, ‘, but‘, ‘madge‘, ‘heir‘, ‘mouth‘, ‘you can‘, ‘a few‘, ‘berries‘, ‘fully‘, ‘tribute‘, ‘I can‘, ‘cause of the‘, ‘feel‘, ‘to the baker‘, ‘its not just‘, ‘he want‘, ‘slim‘, ‘hand‘, ‘the ball‘, ‘quick‘, ‘green‘, ‘the last‘, ‘peace‘, ‘off‘, ‘you have to‘, ‘kill‘, ‘of the‘, ‘the back‘, ‘they‘, ‘scomers‘, ‘have been‘, ‘reaping‘, ‘as well‘, ‘, but the ‘, ‘cent‘, ‘thing‘, ‘I remem- ber‘, ‘ally‘, ‘though‘, ‘again‘, ‘dont‘, ‘need‘, ‘in the school‘, ‘the pig‘, ‘in our‘, ‘them‘, ‘to remember‘, ‘fair‘, ‘bother‘, ‘at the‘, ‘older‘, ‘the square‘, ‘I know‘, ‘house‘, ‘its not‘, ‘once‘, ‘what was‘, ‘out of‘, ‘it is not just’, ‘our district’, ‘too’, ‘I have’, ‘it out’
28