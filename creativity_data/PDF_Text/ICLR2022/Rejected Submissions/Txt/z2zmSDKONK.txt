Under review as a conference paper at ICLR 2022
Exploring the Robustness of Distributional
Reinforcement Learning against Noisy State
Observations
Anonymous authors
Paper under double-blind review
Ab stract
In real scenarios, state observations that an agent observes may contain measure-
ment errors or adversarial noises, misleading the agent to take suboptimal actions
or even collapse while training. In this paper, we study the training robustness
of distributional Reinforcement Learning (RL), a class of state-of-the-art meth-
ods that estimate the whole distribution, as opposed to only the expectation, of
the total return. Firstly, we propose State-Noisy Markov Decision Process (SN-
MDP) in the tabular case to incorporate both random and adversarial state ob-
servation noises, in which the contraction of both expectation-based and distribu-
tional Bellman operators is derived. Beyond SN-MDP with the function approx-
imation, we firstly analyze the vulnerability of least squared loss in expectation-
based RL, and by contrast theoretically characterize the bounded gradient norm
of histogram-based distributional loss, accounting for the better training robust-
ness of distribution RL. Finally, extensive experiments on the suite of games show
that in SN-MDP-like setting both expectation-based and distributional RL can
converge albeit corresponding to different levels under various state observation
noises. However, distributional RL can enjoys better training robustness in the
more complicated noisy state observation settings compared with its expectation-
based counterpart.
1	Introduction
Learning robust and high-performance policies for continuous state-action reinforcement learn-
ing (RL) domains is crucial to enable the successful adoption of deep RL in robotics, autonomy,
and control problems. However, recent works have demonstrated that deep RL algorithms are vul-
nerable either to model uncertainties or external disturbances (Huang et al., 2017; Pattanaik et al.,
2017; Ilahi et al., 2020; Chen et al., 2019; Zhang et al., 2020; Shen et al., 2020; Singh et al., 2020;
Guan et al., 2020). Particularly, model uncertainties normally occur in a noisy reinforcement learn-
ing environment where the agent often encounters systematic or stochastic measurement errors on
state observations, such as the inexact locations and velocity obtained from the equipped sensors ofa
robot. On the other hand, external disturbances are normally adversarial in nature. For instance, the
adversary can construct adversarial perturbations on state observations to degrade the performance
of deep RL algorithms. These two factors lead to noisy state observations that influence the perfor-
mance of algorithms, precluding the success of reinforcement learning in real-world applications.
Existing works mainly focus on improving the robustness of algorithms in the test environment with
noisy state observations. Smooth Regularized Reinforcement Learning (Shen et al., 2020) intro-
duced a regularization to enforce smoothness in the learned policy, and thus improved its robustness
against measurement errors in the test environment. Similarly, the State-Adversarial Markov de-
cision process (SA-MDP) (Zhang et al., 2020) was proposed and the resulting principled policy
regularization enhances the adversarial robustness of various kinds of RL algorithms against adver-
sarial noisy state observations. However, both of these works assumed that the agent can access
clean state observations during training, which is normally not feasible when the environment is in-
herently noisy, such as unavoidable measurement errors. Thus, the maintenance and formal analysis
of policies robust to noisy state observations during training is a worthwhile area of research.
1
Under review as a conference paper at ICLR 2022
On the other hand, recent distributional reinforcement learning algorithms, including C51 (Belle-
mare et al., 2017), Quantile-Regression DQN (QRDQN) (Dabney et al., 2018b), Implicit Quantile
Networks (Dabney et al., 2018a) and Moment-Matching DQN (MMD) (Nguyen et al., 2020), con-
stantly set new records in Atari games, gaining huge attention in the research community. However,
existing literature mainly focuses on the performance of algorithms, other benefits, including the
robustness in the noisy environment, of distributional RL algorithms are less studied. As distribu-
tional RL can leverage additional information about distribution that captures the uncertainty of the
environment more accurately, it is natural to expect that distributional RL with this better represen-
tation capability can be less vulnerable to the noisy environment while training, which motivates our
research.
In this paper, we investigate the robustness of distributional RL against various kinds of state ob-
servation noises encountered during training. Firstly, we propose a general State-Noisy MDP in the
tabular setting, in which we prove the convergence of distributional Bellman operator. We further
extend SN-MDP to the function approximation case by considering more complex noisy state ob-
servations. Notably, we analyze the vulnerability of classical RL and in contrast characterize the
Lipschitz continuity blessing resulting from the Histogram distributional loss in distributional RL,
which leads to a bounded gradient norm. This better behaved gradient mitigates the impact of noisy
states on the objective function, accounting for the less vulnerability of distributional RL while
training. Finally, extensive experiments demonstrate that both expectation-based and distributional
RL algorithms can converge in SN-MDP-like settings. More importantly, distributional RL algo-
rithms tend to achieve better robust performance in the presence of more complex state observation
noises compared with its expectation-based counterpart that may even diverge in some cases. These
empirical results in Section 5 echo our previous theoretical results in both Section 3 and 4. Over-
all, the training robustness advantage of distributional RL algorithms we revealed facilitates their
deployment especially in the noisy environment.
2	Background
2.1	Distributional Reinforcement Learning
In the tabular setting without noisy states, the agent’s interaction with its environment can be nat-
urally modeled as a standard Markov Decision Process (MDP), a 5-tuple (S , A, R, P, γ). S and A
are the state and action spaces, P : S × A × S → [0, 1] is the environment transition dynamics,
R : S × A × S → R is the reward function and γ ∈ (0, 1) is the discount factor.
Value Function vs Value Distribution. Firstly, we denote the return where st = s as Zπ(s) =
Pk∞=0 γkrt+k+1, representing the cumulative rewards following a policy π, and rt+k+1 is reward
scalar obtained in the step t + k + 1. In the algorithm design, traditional expectation-based RL
normally focuses on value function V π(s), the expectation of the random variable Zπ(s):
∞
Vπ(s):= E[Zπ (s)]= E X Ykrt+k+1 | St = S .	⑴
k=0
In contrast, in the distributional RL setting, we focus on the value distribution, the full distribution of
Zπ(S), and the state-action value distribution Zπ(S, a) in the control problem where St = S, at = a.
Both of these distributions can better capture the uncertainty of returns in the MDP beyond just its
expectation (Dabney et al., 2018a; Mavrin et al., 2019).
Distributional Bellman Operator. In expectation-based RL, we update the value function via
the Bellman operator Tπ , while in distributional RL, the updating is on the value distribution via
the distributional Bellman operator Tπ . To derive Tπ , we firstly define the transition operator
Pπ : Z → Z:
PπZ(s,a) := Z (S0,A0) ,S0 〜P(∙∣s,a),A0 〜π (∙∣S0),	⑵
where we use capital letters S0 and A0 to emphasize the random nature of both, and :=D indicates
convergence in distribution. For simplicity, we denote Zπ(S, a) by Z(S, a). Thus, the distributional
Bellman operator Tπ is defined as:
TπZ(S,a) :== R(S, a, S0) +γPπZ(S,a).	(3)
2
Under review as a conference paper at ICLR 2022
More importantly, Tπ is still a contraction for policy evaluation under the maximal form of the
Wasserstein metric dp over the true and parametric value distributions (Bellemare et al., 2017; Dab-
ney et al., 2018b), where the p-Wasserstein metric dp is defined as
dp = (I。1∣F-1(ω) — Fzθ1(ω)∣p dω)”,	⑷
which minimizes the distance between the true value distribution Z* and the parametric distribution
Zθ. F-1 is the inverse cumulative distribution function of a random variable with the cumulative
distribution function as F . In the control setting, the distributional analogue of the Bellman opti-
mality operator converges to the set of optimal value distributions, although it is in a weak sense and
requires more involved arguments (Dabney et al., 2018b).
2.2 Two Kinds of Noisy State Observations
We investigate both random and adversarial training robustness, i.e., the performance of RL algo-
rithms under these two types of noisy state observations, between the expectation-based and distri-
butional RL algorithms. We consider continuous state observations with continuous noises. In the
random noisy state case, we apply Gaussian noises with mean 0 and different standard deviations to
state features to simulate the measurement error stemming from various sources.
In the adversarial state perturbation setting, we construct white-box adversarial perturbations on state
observations for the current policy during training, following the strategy proposed in (Huang et al.,
2017; Pattanaik et al., 2017) that leveraged the gradient information of an engineered loss function.
In particular, we denote atw as the “worst” action, with the lowest probability from the current policy
∏t(a∣s) in the training step t. Thus, the optimal adversarial perturbation ηt, constrained in an e-ball,
can be derived by minimizing the objective function J :
n
min J(St + η,∏t) = - Y'Pt log∏t(a∕st + η), s.t∙kηk ≤ e,	(5)
η
i=1
where pit = 1 if i corresponds to the index of the least-chosen action, i.e. the w-th index in the
vector a, otherwise pit = 0. In other words, we construct a target one-hot action pt with 1 assigned
to the index of the least-chosen action. Through this minimization in the form of the cross entropy
loss, we can construct the state perturbations ηt that can force the policy to choose the least-chosen
action atw in each t step.
3 Tabular Case: State-Noisy Markov Decision Process
In this section, we extend State-Adversarial MDP (Zhang et al., 2020) to a more general State-
Noisy Markov Decision Process (SN-MDP), and particularly provide a proof of the convergence
and contraction of distributional Bellman operator in this setting.
3.1	Definitions
As shown in Figure 1, SN-MDP is a 6-tuple (S, A, R, P, γ, N), where the noise generating mecha-
nism N(∙∣s) maps the state from S to V(S) using either random or adversarial noise with the Marko-
4F(・ I	t)>
Figure 1: State-Noisy Markov Decision Process where V(St) is perturbed by the noise mechanism
N.
3
Under review as a conference paper at ICLR 2022
vian and stationary probability N (v(s)|s). It is worthwhile to note that the explicit definition of
the noise mechanism N here is based on discrete state transitions, but the analysis can be naturally
extended to the continuous case if we let the state space go to infinity. Moreover, let B(s) be the set
that contains the allowed noise space for the noise generating mechanism N, i.e., v(s) ∈ B(s).
Following the setting in (Zhang et al., 2020), we only manipulate state observations but do not
change the underlying environment transition dynamics based on s or the agent’s actions directly. As
such, our SN-MDP is more suitable to model the random measurement error, e.g., sensor errors and
equipment inaccuracies, and adversarial state observation perturbations in safety-critical scenarios.
3.2	Analysis of SN-MDP for Expectation-based RL
-»-v T 1 r∙	.1	1	1'	. ∙	^T'7^	♦	♦ 1-1TL T -» « l ʌ l ʌ EI 1 ʌ	1 -	♦	F
We define the value function VnoN given ∏ m SN-MDp The Bellman Equations are given by:
VnoN (S)= XX
N(v(s)∣s)∏(a∣v(s)) Xp(s0∣s,a) ∙ [R(s,a,s0) + YVn°n(s0)].	⑹
a v(s)	s0
The random noise transits s into v(s) with a certain probability and the adversarial noise is the
special case of N (v(s)∣s) where N (v*(s)∣s) = 1 if v*(s) is the optimal adversarial noisy state given
s, and N (v (s)|s) = 0 otherwise. We denote Bellman operators under random noise mechanism
Nr(∙∣s) and adversarial noise mechanism N*(∙∣s) as Tn and Ta, respectively. This implies that
Tn V∏oN = V∏oNr and T∏V∏oN = V∏0n*. We extend Theorem 1 in (Zhang et al., 2020) to both
random and adversarial noise scenario, and immediately obtain that both Trn and Tan are contraction
operators in SN-MDP. We explain this in Theorem 3 of Appendix A.
The pivotal conclusion from Theorem 3 is Tan VnoN = minN VnoN . This implies that the adversary
attempts to minimize the value function, forcing the agent to select the worse-case action among the
allowed transition probability space N(∙∣s) for each state s. The main proof idea is that Bellman
updates in SN-MDP result in the convergence to the value function for another “merged” policy π0
where π0(a∣s) = Pv(S) N (v(s)∣s)π(a∣v(s)). The value function for the merged policy might ba far
away from that for the original policy π, which tends to worsen the performance of RL algorithms.
3.3	Analysis of SN-MDP in distributional RL
In the SN-MDP setting for distributional RL, the new distributional Bellman equations use new
transition operators in place of Pn in Eq. 2. The new transition operators Prn and Pan , for the
random and adversarial settings, are defined as:
PnZN(s,a) := ZNr(S0,A0),A0 〜∏(∙∣V(S0)), and PnZN(s,a) := ZN* (S0,A0),A0 〜π(∙∣V*(S0)),
(7)
where V(S0)〜 Nr(∙∣S0) is the state random variable after the transition, and V*(S0) is attained
from N*(∙∣S0) under the optimal adversary. Besides, S0 〜P(∙∣s, a). Thus, the corresponding new
distributional Bellman operators Trn and Tan are:
Trn ZN (s, a) :=D R(s, a, S0) + γPrnZN(s, a), and TanZN(s, a) :=D R(s, a, S0) + γPanZN(s, a).
(8)
In this sense, four sources of randomness define the new compound distribution in the SN-MDP: (1)
randomness of reward, (2) randomness in the new environment transition dynamics Prn or Pan that
additionally includes (3) the stochasticity of the noisy transition N, and (4) the random next-state
value distribution Z(S0, A0). Besides, the premise of the robustness of distributional RL against
noisy state observations lies in the convergence of the new derived distribution Bellman Operators
in SN-MDP setting. We proved this convergence and contraction for policy evaluation in Theorem 1.
Theorem 1. (Convergence and Contraction of Distributional Bellman operators in the SN-MDP)
Given a policy π, we define the distributional Bellman operators Trn and Tan in Eq. 8, and consider
the Wasserstein metric dp, the following results hold.
(1)	Trn is a contraction under the maximal form of dp.
(2)	Tan is also a contraction under the maximal form of dp, following the greedy adversarial rule,
i.e., N*(∙∣s0) = argminN(,恰/)E [Z(s0, a0)] where a0 〜π(∙∣V(s0)) and V(s0)〜N(∙∣s0).
4
Under review as a conference paper at ICLR 2022
We provide the proof in Appendix B. The convergence of distributional Bellman operators in the SN-
MDP is one of our main contributions. This result allows us to deploy distributional reinforcement
learning algorithms comfortably even in settings with noisy state observations.
4	Function Approximation Case: Noisy Settings Beyond SN-MDP
In real scenarios, especially safety-critical cases, perturbations on state observations can be more
complicated. For instance, the adversary might perform attacks at certain intervals, yielding un-
balanced state observation pairs with a perturbed current state and a benign next state and vice
versa. This type of unbalanced perturbations is outside the scope of State-Noisy MDP we ana-
lyzed in the last section and can have different impacts on the convergence of expectation-based
and distributional RL algorithms. In this section, we firstly point out the vulnerability nature of
expectation-based RL although we consider the bounded rewards in Section 4.1. Next, we char-
acterize the robustness blessing of distributional RL based on Histogram distributional loss (Imani
& White, 2018) with derived bounded gradient norms unlike the unrestricted one in expectation-
based RL. Finally, as an extension we analyze the impact of more complex state observations on TD
convergence and further conduct a sensitivity analysis by the influence function.
Notation. We derive theoretical results with the linear function approximator. For the expectation-
based RL, the value estimate V : SX Rd → R is formed simply as the inner product between state
features X(S) and weights W ∈ Rd, given by V(s, W) = w>x(s). At each step, the state feature
can be rewritten as xt d=ef x (St ) ∈ Rd . The distributional RL setting is given in Section 4.2.
4.1	Vulnerability for expectation-based RL
In the classical RL with function approximation, a natural objective function is Mean Squared Value
Error (Sutton & Barto, 2018) denoted VE, which is weighted by the state distribution μ:
VE(w) = £〃(s) [v∏(s) — V(s, w)]2 ,
s∈S
(9)
where μ(s) is the fraction of time spent in s. To solve this kind of weighted least squared minimiza-
tion, we leverage Stochastic Gradient Descent (SGD) and the weight updating rule can be formulated
as
Wt+1 = Wt + α [v∏ (st) — V (st, Wt)] Vv (st, Wt)
=. wt + α Ut - wt>Xt Xt
(10)
where α is the step size and the target Ut can be either an unbiased estimate via Monte Carlo method
with Ut = Pk∞=0 γkrt+k+1, or a biased estimate via TD learning with Ut = rt+1 + γWt>Xt+1.
Let gvE be the empirical version of VE(w). To investigate the vulnerability of expectation-based
RL over state features, we focus on the gradient norm of gVE regarding x(s) (or xt). For a fair
comparison with distributional RL in Section 4.2, we additionally bound the norm ofW, i.e., kWk ≤
l, which can also be easily satisfied by imposing `1 or `2 regularization. Therefore, the upper bound
of gradient norm of gvE is
∂
Il @ X(S) gVEk = IUt — Wt xt |kWtk ≤ IUt - wt xt|l.
(11)
However, this upper bound IUt — Wt>XtIl can be arbitrary large as there is no restriction on IUt —
Wt>XtI in either Monte Carlo or TD bootstrap estimate of Ut, which we elaborate as follows.
Case 1:	Estimate Ut via Monte Carlo. Ifwe consider the bounded rewards, i.e., r ∈ [Rmin, Rmax],
we can bound Ut as Ut = P∞=o γkrt+k+ι ∈ [R-n, Rmx]. However, the bounded rewards can not
guarantee the bounded gradient norm of gv^ as w> Xt can be sufficiently large.
Case 2:	Estimate Ut via bootstrap estimate, e.g., TD (0). Consider TD (0) where Ut = rt+1 +
γWt>Xt+1.	However, we can find that	IUt	—	Wt>XtI	=	Irt+1	+ Wt>(γXt+1	—	Xt)I	can still be
sufficiently large due to the unrestricted term Wt> (γXt+1 — Xt).
Therefore, we conclude that the unbounded gradient norm of objective function regarding state
features for the expectation-based RL results in its vulnerability against state perturbations.
5
Under review as a conference paper at ICLR 2022
4.2	Robustness Blessing for distributional RL
We show that in the function approximation setting, the distributional loss in distributional RL can
additionally yield Lipschitz continuity regarding state features, thus leading to more stable gradients
relatively to expectation-based RL.
In particular, in distributional RL our goal is to minimize L (Zθ , TZθ), where T is the distribu-
tional Bellman operator. Here we leverage histogram to parameterize the distribution Zθ based on
KL divergence as L, yielding the histogram distributional loss (Imani & White, 2018). Specifi-
cally, we uniformly partition the support of x(s) into k bins, and let function f : X → [0, 1]k
provide k-dimensional vector f (x(s)) of the coefficients indicating the probability the target is in
>
that bin given x(s). Next, we use softmax based on the linear approximation x(s) θi to express f ,
i.e., fi(x(s)) = exp (x(s)>θj / P：=i exp (x(s)>θj). Therefore, the histogram distributional loss
L(Zθ, TZθ) between Zθ and TZθ can be derived as
k
Lθ = - Xpi log fiθ(x(s)),	(12)
i=1
where θ = {θ1, ..., θk} and the target probability pi is the cumulative probability increment of target
distribution TZθ within the i-th bin. Detailed derivation of the histogram distributional loss is given
in Appendix C.
Based on this histogram distributional loss in distribution RL, we obtain Theorem 2 (proof in Ap-
pendix C), revealing that the distribution loss can result in additional Lipschitz continuity property
that bounds the norm of gradient over state features x(s):
Theorem 2. (Lipschitz Continuity of distributional RL) Consider the histogram distributional loss
Lθ = - Pik=1 pi log fiθ(x(s)), where fi(x(s)) = exp (x(s)>θ, / pk=ι exp (x(s)>θj) Parame-
terized by θ = {θ1, ..., θk}. Assume kθik ≤ l for ∀i = 1, .., k, then Lθ is kl-Lipschitz continuous
w.r.t. x(s), yielding a bounded norm of its gradient, i.e.,
∂
∂xs Lθ ≤kl
(13)
In conclusion, Theorem 2 shows that distributional loss in distributional RL can additionally en-
joy kl-Lipschitz continuity compared with the expectation-based RL where the gradient norm of
objective function is unbounded. Thus, the bounded gradient norm regarding state features of distri-
butional RL mitigates the impact of state noises on the objective function while training, therefore
yielding better training robustness.
Extension of TD Convergence and Sensitivity Analysis. Beyond the aforementioned robustness
analysis on both expectation-based and distributional RL, we also conduct the analysis on the per-
turbation impacts of different state observations in the TD learning based on expectation-based RL,
including the current, next and both state observations. In particular, conditions for TD convergence
in these three different noisy state observation settings are derived in Theorem 4 of Appendix D,
which are strongly related to the positive definiteness property of relevant matrixs. More impor-
tantly, the convergence conditions for current and next noisy state observations are harder than the
condition in the noise-free setting and are normally divergent. Nevertheless, which situation is
milder heavily depends on the task. Interested readers can refer to Appendix D for more details. In
addition, we also conduct a sensitivity analysis by the influence function to characterize the effects
that the state noise in particular observation has on an estimator in Theorem 5 of Appendix E. The
conclusions we made are similar to the TD convergence analysis part. Note that although the exten-
sion analysis is based on the expection-based RL with linear function approximation, our empirical
observations demonstrate our analysis results across both expectation-based and distributional RL.
We defer detailed empirical verification in the following experimental section.
Remark. In this section, we firstly present the vulnerability of expectation-based RL against noisy
state observations in Section 4.1, while we further derived the Lipschitz continuity blessing of
distributional RL in Section 4.2, leading to more robustness against noisy state observations than
expectation-based RL. In Section 5, we show that our empirical observations coincide with our
theoretical results to further demonstrate the robustness advantage of distributional RL.
6
Under review as a conference paper at ICLR 2022
5	Experiments
We make a comparison between expectation-based and distributional RL algorithms against various
noisy state observations. We select DQN (Mnih et al., 2015) as the baseline, and QR-DQN (Dabney
et al., 2018b) as its distributional counterpart. The previous analysis is either for policy evaluation
or linear function approximation, but there are natural—though in some cases heuristic-extensions
to the control setting and to non-linear function approximation.
Experimental Setup. We perform our algorithms on Cart Pole, Mountain Car, Breakout and Qbert
games. We followed the procedure in (Ghiassian et al., 2020; Zhang & Yao, 2019). All the experi-
mental settings, including parameters, are identical to the distributional RL baselines implemented
by (Zhang, 2018; Dabney et al., 2018b). We perform 200 runs on both Cart Pole and Mountain
Car and 3 runs on Breakout and Qbert. Reported Results are averaged with shading indicating the
standard error. The learning curve is smoothed over a window of size 10 before averaging across
runs. Please refer to Appendix H for more details.
Noisy State Observations. For the random noise, we use Gaussian noise with different standard
deviations. In particular, for a better presentation to compare the difference, we select the standard
deviations as 0.05, 0.1 in Cart Pole, 0.01, 0.0125 in Mountain Car, 0.01, 0.05 in Breakout and 0.05
in Qbert. For the adversarial noise, we followed (Zhang et al., 2020), where the set of noises B(S) is
defined as an '∞ norm ball around S with a radius e, given by '∞B(s) := {S: IIS - S∣∣∞ ≤ e}. We
apply Projected Gradient Descent (PGD) version in (Pattanaik et al., 2017), with 3 fixed iterations
while adjusting e to control the perturbation strength. Concretely, for better presentation we select
the perturbation size e as 0.05, 0.1 in Cart Pole, 0.01, 0.1 in Mountain Car, 0.005, 0.01 in Breakout,
and 0.005 in Qbert.
Experiments Structure. In Section 5.1, we consider the function approximation setting with TD
learning where both current and next state observations are perturbed, leading to a SN-MDP-like
setting as the vanilla SN-MDP is a tabular case. In this case, we found both DQN and QRDQN con-
verge to different convergence points determined by the noise strength. Next, in Section 5.2 and 5.3,
we investigate the robustness advantage of distributional RL, i.e., QRDQN, relative to expectation-
based RL, i.e., DQN, under various random and adversarial state observations, respectively.
5.1	SN-MDP-Like Setting
In the classical SN-MDP tabular case, each state will be perturbed via the noise mechanism N. We
keep this noise structure and transform it into the function approximation setting with TD learning,
Adversarial
UJnIə"uwluno 乏
Adversanal
UJnIəɑ:Ino*eφ,lm
0.0	2,0	4,0	6.0	8.0	10,0	12,0
Steratx lθ∙)
Figure 2: Average returns ofDQN and QRDQN against adversarial state observation noises across
four games. advX in the legend indicates adversarial state observations with the perturbation size
e as X. Both QRDQN (solid lines) and DQN (dashed lines) converge and the convergence level is
determined by the perturbation strength e.
EnIəɑ:七 φqc
o.o o,ι
S⅛a(XW4)
Adversarial
0.0	2,0	4.0	6.0	8.0	10,Q
Stef9(×10*)
7
Under review as a conference paper at ICLR 2022
and thus both current and next state observations would be noisy. We further investigate the training
robustness of both DQN and QRDQN against either random or adversarial state perturbations across
these four games.
We focus on the adversarial setting as suggested in Figure 2. It turns out that all DQN and QRDQN
algorithms under various strength of adversarial state noises converge, although they eventually
obtained different average returns. This empirical observation is consistent with our theoretical
analysis in Section 3 where both classical and distributional Bellman operators are contractive and
thus convergent. In addition, Figure 5 also manifests that the final performance that algorithms
attained has a decreasing tendency as the perturbation strength, i.e., standard deviation, increases
especially for DQN. Remarkably, the final performance of QRDQN is more robust than DQN against
different perturbation strength, especially in Breakout and Qbert games, although both DQN and
QRDQN are convergent in this SN-MDP-like setting. For the random state observation setting, we
provide the similar results in Figure 5 with detailed explanation in Appendix I.
5.2	Setting Beyond SN-MDP with Random State Noises
Beyond SN-MDP, we further investigate the training robustness of DQN and QRDQN in the more
complicated unbalanced state observation setting as mentioned in Section 4, where the agent ob-
serve perturbations on current state observations. Similar results in the setting with the perturbed
next state observations are given in Appendix J. In this subsection, we firstly focus on the function
approximation case with random noises on current state observations, and the learning curves when
the agent encounters different strength of random state noises are provided in Figure 3 across four
games.
0.0
g1501251007s502500
Enlətt:一(υod trao
Random
0.1	0.1
Steps (× 104)
Random
DQN-rendOX)1
DQN-rendOX)125
——QRDQN
—QRDQN_randO.O1
QRDQNrandO.OI 25
0.0 o.o o.ι
Steps (×104)
....□QN-rwτdO.□S
DQN.rwxiO.1
—QRDQN
QRDQN-rendOX)5
QRDQN_rwK<M
0.0	2.0	4.0	6.0	8.0	10.0	12.0
Steps (× IOG)
Figure 3: Average returns of DQN and QRDQN against random state observation noises across
four games. randX in the legend indicates random state observations with the standard deviation X.
QRDQN (solid lines) almost consistently outperform DQN (dashed lines) given the same perturba-
tion strength, yielding more training robustness against noisy state observations.
From Figure 3, it illustrates that the final performance of QRDQN (solid lines) is superior to
DQN (DQN) in the same color given the same random state perturbations (the same standard de-
viation) when both DQN and QRDQN achieved similar average returns eventually (red dashed and
solid lines). This implies that QRDQN enjoys better training robustness than DQN under the ran-
dom noisy state observation setting. It is worthy to note that in Mountain Car game although DQN
outperforms QRDQN almost across all training process, QRDQN under random state noises (blue
and green lines) can surpass DQN with the same noise setting, yielding more training robustness.
A more significant result is in Breakout where the training of QRDQN is less vulnerable to vari-
ous random state perturbations, while the performance of DQN degrades and even diverges as the
perturbation strength increases.
8
Under review as a conference paper at ICLR 2022
5.3	Setting Beyond SN-MDP WITH Adversarial State Noises
Next, we probe the training robust of DQN and QRDQN in the setting where the agent encounters
the adversarial state observations in the current state in the function approximation case. Figure 4
presents the learning curves of algorithms over four games against noisy states under different ad-
versarial perturbation sizes e.
200175150125100755025
EnIətt:ωod trao
0.0	0.0	0.1	0.1	0.1	0.1	0.1	0.2	0.2
Steps (× 104)
7006005004003002∞100
EnIətt:AnOxe
0.0	2.0	4.0	6.0	8.0	10.0	12.0
Steps (× 106)
UJnlətt:Ue。UC61U now UJmEtaqσ
Figure 4: Average returns ofDQN and QRDQN against adversarial state observation noises across
four games. advX in the legend indicates random state observations with the perturbation size e X.
QRDQN (solid lines) almost consistently outperform DQN (dashed lines) given the same perturba-
tion strength, yielding more training robustness against noisy state observations.
The results under the adversarial state observations are similar to those in the random noises case.
Specifically, both DQN and QRDQN algorithms tend to degrade when getting exposed to adversarial
state observations, and even are more likely to diverge. For example, in Cart Pole both DQN (blue
and green dotted lines) and QRDQN (blue and green solid lines) degraded to similar poor results
when exposing to strong perturbation with e = 0.05 and 0.1. However, a key observation is that
QRDQN is capable of obtaining desirable performance while DQN even diverges. For instance, in
both Breakout and Qbert game, DQN (dotted blue lines) under the adversarial perturbation with e =
0.005 led to divergence, QRDQN (solid blue lines) can still attain relatively satisfactory performance
eventually on the condition that the performance of both two algorithms are on part with each other
without any noisy state observations (red lines).
Sensitivity Analysis of Different Perturbed States. To demonstrate the theoretical results (Theo-
rem 4 and 5) in the Appendix based on TD convergence condition and influence function we ever
mentioned in Section 4.2, we provide the consistent results in Appendix D and E. Interested readers
can refer to Appendix D and E for a more detailed discussion.
6	Discussion and Conclusion
The Lipschitz continuity blessing is based on the histogram distributional loss, but it is more ex-
pected that similar conclusions can be made under Wasserstein or Crammer distance as these dis-
tances are more approachable in real distributional RL algorithms. We leave it as future works.
in this paper, we explored the training robustness of distributional RL against noisy state observa-
tions. After the convergence analysis of distributional RL in the SN-MDP, we proved the Lipschitz
continuity property of distributional RL, accounting for its less vulnerability. We also provided the
TD convergence conditions and a sensitivity analysis on more complex noisy settings. Experimental
observations coincides with our theoretical results.
9
Under review as a conference paper at ICLR 2022
Ethics Statement. Our works reveals that distributional RL can enjoy the training robustness
against noisy state observations. The advantage is useful to defend against the poisoning attacks,
thus contributing to the privacy of algorithms. Based on our experience, there is no other ethic
concerns of our work.
Reproducibility Statement. For the theoretical part, we clearly state the related assumption and
detailed proof process in the appendix. In terms of the algorithm, our implementation is directly
adapted from the public RL algorithms, including DQN and QR-DQN.
References
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. International Conference on Machine Learning (ICML), 2017.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.
Tong Chen, Jiqiang Liu, Yingxiao Xiang, Wenjia Niu, Endong Tong, and Zhen Han. Adversarial
attack and defense in reinforcement learning-from ai security view. Cybersecurity, 2(1):11, 2019.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. International Conference on Machine Learning (ICML),
2018a.
Will Dabney, Mark Rowland, Marc G Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. Association for the Advancement of Artificial Intelligence
(AAAI), 2018b.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Learning for Dynamics and Control, pp. 486-489. PMLR, 2020.
Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and Martha White.
Gradient temporal-difference learning with regularized corrections. In International Conference
on Machine Learning, pp. 3524-3534. PMLR, 2020.
Ziwei Guan, Kaiyi Ji, Donald J Bucci Jr, Timothy Y Hu, Joseph Palombo, Michael Liston, and
Yingbin Liang. Robust stochastic bandit algorithms under probabilistic unbounded adversarial
attack. In AAAI, pp. 4036-4043, 2020.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. Advances in Neural Information Processing Systems, 2017.
Peter J Huber. Robust Statistics, volume 523. John Wiley & Sons, 2004.
Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala Al-Fuqaha, Dinh Thai
Hoang, and Dusit Niyato. Challenges and countermeasures for adversarial attacks on deep rein-
forcement learning. arXiv preprint arXiv:2001.09684, 2020.
Ehsan Imani and Martha White. Improving regression performance with distributional losses. In
International Conference on Machine Learning, pp. 2157-2166. PMLR, 2018.
Borislav Mavrin, Shangtong Zhang, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang
Yu. Distributional reinforcement learning for efficient exploration. International Conference on
Machine Learning (ICML), 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Thanh Tang Nguyen, Sunil Gupta, and Svetha Venkatesh. Distributional reinforcement learning with
maximum mean discrepancy. Association for the Advancement of Artificial Intelligence (AAAI),
2020.
10
Under review as a conference paper at ICLR 2022
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. Advances in Neural Information Processing
Systems, 2017.
Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao. Deep reinforcement learning
with robust and smooth policy. In International Conference on Machine Learning, pp. 8707-
8718. PMLR, 2020.
Rahul Singh, Qinsheng Zhang, and Yongxin Chen. Improving robustness via risk averse distribu-
tional reinforcement learning. arXiv preprint arXiv:2005.00585, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An Introduction. MIT press, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Robust deep
reinforcement learning against adversarial perturbations on observations. Advances in Neural
Information Processing Systems, 2020.
Shangtong Zhang. Modularized implementation of deep rl algorithms in pytorch. https://
github.com/ShangtongZhang/DeepRL, 2018.
Shangtong Zhang and Hengshuai Yao. Quota: The quantile option architecture for reinforcement
learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 5797-
5804, 2019.
11
Under review as a conference paper at ICLR 2022
A	Theorem 3 with proof
Theorem 3. (Convergence and Contraction of Bellman operators in the SN-MDP) Given a policy π, define the
Bellman operator T : R|S| → R|S| under random and adversarial states noises by Trπ and Taπ, respectively.
Denote a “merged” Policy π0 where π0(a∣s) = Ev(S) N (v(s)∣s)π(a∣v(s)) and S(π) is a policy set giv^n π.
Then we have:
ππ
(1)	Tr	Isa contraction operator and can converge to	V∏o,	i.e.,	Tr	VroN	=	VroN =	V∏o,	where multiple
policies ∏r ∈ S(∏) might exist with Ev(S) N (v(s)∣s)∏r (a∣v(s)) = π0(a∣s).
(2)	Ta is a contraction with the convergence satisfying TarVrON * = minN V∏oN = V∏oN *, where N * is the
optimal adversarial noise strategy. Ifthe optimal policy πα, exists, it satisfies πα(a∣v*(s)) = π(a∣s) for each S
and a, where v* (S) is the adversarial noisy state manipulated by N * (∙∣s).
Proof. Our proof is partly based on Theorem 1 and 2 in (Zhang et al., 2020), but adds more analysis on the
converged policy especially under the random noisy states setting. The most important insight in the following
proof is that the noise transition can be merged into the agent’s policy, resulting in a new “merged” policy π0 .
Proof of (1) Firstly, as the Bellman Equation under the random noisy states is right the general form in Eq. 6,
r
it automatically satisfies that Trr VroN = VroN when it converges. As for the proof of contraction, based on
our insight about the new “merged” policy π0 where π0(a∣s) = Pv(S) N(v(s)∣s)π(a∣v(s)), We can rewrite
our Bellman Operator as:
一 ~ ,.
Tπ VroN (S)
a
S0
. τV
+ γ VroN
(14)
R(S)+ Y £P；,s，VVroN (S0)
S0
where R(S) = Pa π0(a∣s) £§, p(s0∣s,a)R(s,a,s0), and P；§o = Pa π0(a∣s)p(s0∣s, a) determined by the
“merged” policy π0. Then for two different value function VION and V2oN we have:
=max ∣Y £P；,s0 VON (s0)-Y Y^PSE VON (s0)∣
S	S0	S0
≤ Y max X P0,s0∣VoN (S)- VoN (s0)I
S0
≤ Y max	PS,s0 max IVoN (S)- VoN (s0)I
S	S0
S0
=Y max T P；ekVoN - VoN k∞
S0
= YkVroN - VroN k∞
(15)
Then according to the Banach fixed-point theorem, since Y ∈ (0,1), VToN converges to a unique fixed-point
吟o. However, even though the obtained policy π0 satisfies that π0(a∣S) = Pv(S) N(v(s)∣s)π(o∣v(s)) for
each S, a, these equations can not necessarily guarantee a unique π especially when these equations behind
this condition are underdetermined. In such scenario, multiple policies πr will exist as long as they satisfy the
equations above.
Proof of (2) Firstly, based on Theorem 1 (Zhang et al., 2020) that shows an optimal policy does not always
exist, we assume that an optimal policy exists in the adversarial noisy state setting for the convenience of
following analysis. Based on this assumption, we need to derive the explicit value function under the adversary.
Inspired by (Zhang et al., 2020), the proof insight is that the behavior of optimal adversary can be also viewed as
finding another optimal policy, yielding a zero-sum two player game. Specifically, in the SN-MDP setting, the
adversary selects an action a ∈ S satisfying a = v(s), attempting to maximize its state-action value function
12
Under review as a conference paper at ICLR 2022
χ-∖	/	^ ∖ rnl	,1	1	，	1	f	,∙	T^Λ /	∖	1 ʃ'	1,1
Qna (s, a). Then the adversary's value function Vna (S) can be formulated as:
V∏a (s) = max Q∏a(s, a)
^
=max Xp(s0∣s,^)(R(s,a, s0) + YVk (S))
s0
= maxΣΣπ(a∣v(s))p(s0∣s, a)(-R(s, a, s')
v(s)	0
s0 a
+ YV∏a (SO))
(16)
where p(s0∣s, a) is the transition dynamics of the adversary, satisfying p(s0∣s, a) = Pa π(a∣v(s))p(s0∣s, a)
from the perspective of the agent. R(s, a, s0) is the adversary,s reward function while taking action a, which is
the opposite number of R(S, a, S0) given the action a. In addition, since both the adversary and agent can serve
as a zero-sum two-player game, it indicates that Vna (S) = -Vna (S) for the agent’s value function Vna in the
adversary setting. Then we rearrange the equation above as follows:
~ .. ^ ,.
Vna(S) = -Vk(S)
=-Nmin) E E n0(a|S)P(SOB a)(-R(S, a, SO)
s0 a
-YKa(S0))
= min	π0(a∣S)p(S0∣S, a)(R(S, a, so)
v(s) s0 a
+ YV∏a(SO))
min1∑∑∏ (a| s)p(s0 |s, a)(rt+ι + Y min E∏°n
NQ ⑸ 丁 Y	N
∞
rt+k+2 |St+1 = SO )
k=0
♦ τV / ∖
min V∏°N (s)
Note that We optimize over N, which means We consider N(∙∣s) for each state s.
contraction of the Bellman operator Tan . We rewrite our Bellman Operator Tan as:
n
Ta VnoN (s) = min VnoN (s)
= mNin R(S) +YXPsO,s0VnoN (S0)
We firstly assume Tar VIa(S) ≥ Tar V2a (s),then We have:
Tn VoN (S)-Ta VoN (S)
≤ max {γ X PsW VnON(s0) - γ X PS,s0 V2oN(S0)}
s0	s0
≤ Y max X P；,so|V1oN (s0) - V2oN (S0)I
N (∙ls) s0
≤ Y max X PsO,s0 max|VION(SO)- VnON(SO)I
N(∙ls) L	S
=Y max y2PlekVoN - V2oNk∞
N (∙ls) S0
≤ YkVION - VnON k∞
(17)
Further, we derive the
(18)
(19)
where the first inequality holds as minx1 f(x1) - minx2 g(x2) ≤ maxx (f (x) - g(x)) and we extends this
inequality into the Wasserstein distance in the proof of convergence of distributional RL setting in Appendix B.
The last inequality holds since only Pss,s∕ depends on N(∙∣s) while the infinity norm is a constant, which is
independent with the current N(∙∣s). Similarly, the other scenario can be still proved. Thus, we have:
kτnVoN -Tn V2oN k∞ ≤ YkVON-VnON k∞	(2。)
n
Thus, we proved that Tar is still a contraction and converge to mιnN V∏on. We denote it as 匕0n* In addition,
based on the insight of the “merged” policy ∏, we have ∏ = Pv(S) N*(v(s)∣s)π(a∣v(s)) = π(a∣v*(s))
where the deterministic state v* (s) is the adversarial noisy state from the state s.
□
13
Under review as a conference paper at ICLR 2022
B	Proof of Theorem 1
Proof. Firstly, we will provide the properties of Wassertein distance dp in Lemma 1 that we leverage in our
following convergence proof.
Lemma 1. (Properties of Wasserstein Metric) We consider the distribution distance between the random vari-
able U and V . Denote dp as the Wasserstein distance between two distribution defined in Eq. 4. For any scalar
a and random variable A independent of U and V , the following relationships hold:
dp(aU,aV) ≤ |a|dp(U,V)
dp(A+U,A+V) ≤ dp(U,V)	(21)
dp(AU,AV) ≤ kAkpdp(U,V)
Further, let A1 , A2, ... be a set of random variables describing the a partition of ω, when the partition lemma
holds:
dp(U,V) ≤	dp(AiU,AiV).
i
(22)
Then, the following contraction proof is in the maximal form of dp and We denote it as dp.
Proof of (1) This contraction proof is similar to the original one (Bellemare et al., 2017) in the distributional
RL without state observation noises. The only difference lies in the new transition operator Prπ, but it dose not
change the main proof process. For two different random variables ZN1 and ZN2 about returns, we have:
dp(τ∏ ZN ,τ∏ ZN)
= sup dp (Trπ ZN1 (s, a), Trπ ZN2 (s, a))
s,a
= sup dp (R(s, a, S0) + γ Prπ ZN1 (s, a), R(s, a, S0) + γ Prπ ZN2 (s, a))
s,a
≤ γ sup dp (Prπ ZN1 (s, a), Prπ ZN2 (s, a))
s,a
≤ γ sup sup dp (ZN1 (s0 , a0), ZN2 (s0 , a0))
s,a s0,a0
= γ sup dp(Z1 (s0, a0), Z2(s0, a0))
s0,a0
= γsupdp(ZN1 (s, a), ZN2 (s, a))
s,a
=Ydp(ZN ,zN ).
Thus, we conclude that Tn : Z → Z is a Y-contraction in dp.
Proof of (2) Firstly, we define the distributional Bellman optimality operator T in MDP as
TZ(s, a) := R (s, a, S0) + γZ(S0,∏z(s0))
(23)
(24)
where S0 〜P(∙∣s,α) and ∏z(S0) = argmax., E [Z(S0,a0)]. By contrast, in SN-MDROurgreedy adversar-
ial rule N * (∙∣s0) is based on the greedy policy rule in distributional Bellman optimality operator, which attempts
to find adversarial N*(∙∣s0) in order to minimize E [Zn(s0,a0)], where a0 〜 π(∙∣V(s0)) and V(s0)〜 N(∙∣s0).
We assume N * (∙∣s0) yields a deterministic state s*, and thus the agent always takes action based on s*, which
we denote as A* 〜π(∙∣s*). Therefore, we can obtain the state-action function QN* (s, a) under the adversary
as
QπN* (s, a) = minE [ZNπ (s, a)]
E hZπ* (s, a)i
(25)
where π*(∙∣s) = π(∙∣s*) for ∀s that follows the adversarial policy A*.
14
Under review as a conference paper at ICLR 2022
Next, to derive the contractive property of Taπ, we denote two state-action valued distributions as ZN1 (s, a) and
ZN2 (s, a). Then we have:
dp(τ∏ ZN ,T∏ ZN)
= sup dp (TaπZN1 (s, a), TaπZN2 (s, a))
s,a
= sup dp (R(s, a, S0) + γPaπZN1 (s, a), R(s, a, S0) + γPaπZN2 (s, a))
s,a
≤ YSuPX P(s0∣s,a)dp(ZN(s0,A*),ZN(s0,A*))
s,a s0
=γX P(s0∣s, a)dp(ZN(s',A*),zN(s0, A*))
s0
≤ γsuPdp(ZN1 (s0,A*),ZN2 (s0,A*))
s0
= γsuPdp(X π(a0* |s*)ZN1 (s0, a0*), X π(a0* |s*)ZN2 (s0, a0*))
s0	a0	a0
*	*
≤ γ suPX π(a0* |s*)dp(ZN1 (s0, a0*), ZN2 (s0, a0*))
s	a0*
*
≤ γ suP dp(ZN1 (s0, a0*), ZN2 (s0, a0*))
s0,a0*
= γ suP dp (ZN1 (s, a), ZN2 (s, a))
s,a
=dp(zN ,ZN)
(26)
Thus, We conclude that Tn is still a Y-ContraCtion in dp.
□
C Proof of Theorem 2
Proof. Firstly, We shoW the derivation details of the Histogram distribution loss starting from KL divergence
betWeen p and qθ . pi is the cumulative probability increment of target distribution TZθ Within the i-th bin, and
f θ (x(s))
qθ corresponds to a (normalized) histogram, and has density values i per bin. Thus, We have:
w
L(Zθ, TZθ) = - Z p(y) log qθ (y)dy
a
= -Xk Z li+wip(y)log
i=1 li
f≡! dy
wi
=— X log ff(X(S)) (Ftzθ (li + Wi) - Ftzθ (li))
i=ι	wi '------------------{z------------}
pi
k
=. - X pi log fiθ (X(s))
i=1
(27)
Where the last equality holds because the Width parameter wi can be ignored for this minimization problem.
15
Under review as a conference paper at ICLR 2022
Next, we compute the gradient of the Histogram distributional loss.
∂
∂x(s)
k
Xpj log fjθ(x(s))
j=1
k1
X Pj E fθ(X(S))
j=1
X Pj F⅛π f (X(S))X PkeXP(X(S)>θi> θ、⑹-θi)
j=1 fj (x(s))	i=1	pk=1 exp(X(S)>θp)
kk
XPjXfiθ(X(S))(θj-θi)
j=1	i=1
kk
XPjθj -Xfiθ(X(S))θi
j=1	i=1
k
X(Pi - fiθ(X(S)))θi
i=1
Then, as we have kθi k ≤ l for ∀i, we bound the norm of its gradient
k
k ∂X(S) X PjlOg fθ(X(S)) k
k
≤ Xk(Pi-fiθ(X(S)))θik
i=1
k
=X |Pi - fiθ(X(S))lkθik
i=1
≤ kl
(28)
(29)
The last equality satisfies because |Pi - fiθ (X(S))| is less than 1 and even smaller. In summary, compared
with the least squared loss in expectation-based RL, the histogram distributional loss in distributional RL can
additionally enjoy kl-Lipschitz continuity with bounded gradient norm regarding the state features X(S). This
upper bound of gradient norm can mitigate the impact of the noises on state observations on the loss function,
therefore yielding training robustness for distributional RL.
□
D TD Convergence Under Noisy State Observations
Let μ(S) be the stationary distribution under the policy π and p(s0∣s) be the transition probability from S to
S0 satisfying p(s0∣s) = Pa π(a∣S)p(S0∣S, a). We analyze conditions of TD convergence when exposing state
observation noises. Firstly, we recall the classical TD update at step t:
wt+ι — Wt + at(Rt+ι + γw>Xt+ι - w>Xt)Xt	(30)
where αt is the step size at time t. Once the system has reached the steady state for any wt, then the expected
next weight vector can be written as E[Wt+1 |Wt] = Wt + αt(b - AWt), where b = E(Rt+1Xt) ∈ Rd
and A =. E Xt dt> ∈ Rd×d . The TD fixed point WTD to the system satisfies AWTD = b. From (Sutton &
Barto, 2018), we know that the matrix A determines the convergence in the linear TD setting. In particular, Wt
converges with probability one to the TD fixed point if A is positive definite. However, if we add state noises η
on either Xt or Xt+1 in Eq. 30, the convergence condition will be different. Theorem 4 (proof in Appendix F)
provides conditions for TD convergence in three different noisy state observation settings.
Theorem 4. (Conditions for TD Convergence under Noisy State Observations) Define P as the |S| × |S|
matrix forming from p(s0∣s) , D as the |S| X |S| diagonal matrix with μ(S) on its diagonal, and X as the
|S| × d matrix with X(S) as its rows, and E is the |S| × d perturbation matrix with each perturbation vector
e(S) as its rows. The stepsizes αt ∈ (0, 1] satisfy Pt∞=0 αt = ∞ and Pt∞=0 αt2 = 0. For noisy states, we
consider the following three cases: (i) e(S) on current state features, i.e., Xt — Xt + et, (ii) e(S0) on next state
features, i.e., Xt+ι — Xt+ι + et+ι, (iii) the same e on both state features. We can attain that Wt converges to
TD fixed point if the following conditions are satisfied, respectively.
16
Under review as a conference paper at ICLR 2022
Case (i): both A and (X + E)>DPE are positive definite. Case (ii): both A and -X>DPE are positive
definite. Case (iii): A is positive definite.
From the convergence conditions for the three cases in Theorem 4, it is clear that (iii) is the mildest. This is
the same condition as that in the normal TD learning without noisy state observations. Note that the case (iii)
can be viewed as the SN-MDP setting, whose convergence has been already rigorously analyzed in Section 3.
In Section 5, our experiments demonstrate that both expectation-based and distribution RL are more likely to
converge in case (iii) compared with case (i) and (ii).
In cases (i) and (ii), the positive definiteness of X>DPE + E>DPE and -X>DPE is crucial. We partition
(X + E)>DPE into X>DPE + E>DPE, where the first term has the opposite positive definiteness to
-X>DPE, and the second term is positive definite (Sutton & Barto, 2018). Based on these observations, we
discuss the subtle convergence relationship in cases (i) and (ii):
(1)	If -X>DPE is positive definite, which indicates that TD is convergent in case (ii), TD can still converge
in case (i) unless the positive definiteness of E>DPE dominates in X>DPE + E>DPE.
(2)	If -X>DPE is negative definite, TD is likely to diverge in case (ii). By contrast, TD will converge in case
(i).
In summary, there exists a subtle trade-off ofTD convergence in case (i) and (ii) ifwe approximately ignore the
term E>DPE in case (i). The key of it lies in the positive definiteness of the matrix X>DPE, which heavily
depends on the task. In Section 5, we empirically verify that the convergence situations for current and next
state observations are normally different. Which situation is superior is heavily dependent on the task.
E Sensitivity Analysis by Influence Function
Next, we conduct an outlier analysis by the influence function, a key facet in the robust statistics (Huber, 2004).
The influence function characterizes the effect that the noise in particular observation has on an estimator, and
can be utilized to investigate the impact of one particular state observation noise on the training of reinforcement
learning algorithms. Specifically, suppose that F is the contaminated distribution function that combines the
clear data distribution F and an outlier x. The distribution F can be defined as
F = (1 - )F + δx ,	(31)
where δx is a probability measure assigning probability 1 to x. Let θ be a regression estimator. The influence
function of θ at F, ψ : X → Γ is defined as
Λ ,	... Λ ,	.
ψθ,F(x) = lim θ ㈤(X)) -IF).	(32)
Mathematically, the influence function is the Gateaux derivative of θ at F in the direction δx . Owing to the
fact that traditional value-based RL algorithms, e.g., DQN (Mnih et al., 2015), can be viewed as a regression
problem (Fan et al., 2020), the linear TD approximator also has a strong connection with regression problems.
Based on this correlation, in the following Theorem 5, we quantitatively evaluate the influence function of TD
learning in the case of linear function approximation.
Theorem 5. (Influence Function Analysis in TD Learning with linear function approximation) Denote dt =
xt -γxt+1 ∈ Rd, and A =. E xtdt> ∈ Rd×d. Let Fπ be the data distribution generated from the environment
dynamics given a policy π. Consider an outlier pair (xt, xt+1) with the reward Rt+1, the influence function ψ
of this pair on the estimator w is derived as
ψw,Fπ (xt, xt+1) = E(A>A)-1dtxt>xt(Rt+1 - dt>w).	(33)
Please refer to Appendix G for the proof. Theorem 5 shows the quantitative impact of an outlier pair (xt, xt+1)
on the learned parameter w. Moreover, a corollary can be immediately obtained to make a precise comparison
of the impacts of perturbations on current and next state features.
Corollary 1. Given the same perturbation η on either current or next state features, i.e., xt, and xt+1, at the
step t, if we approximate ηη > xt and ηη> w as 0 as η is small enough, the following relationship between the
resulting variations of influence function, ∆xt ψ and ∆xt+1 ψ, holds:
γ∆xt ψ + ∆xt+1 ψ = 2γdtηxt>(Rt+1 - dt>w).	(34)
We provide the proof of Corollary 1 in Appendix G. Under this equation, the sensitivity of noises on xt and
xt+1 , measured by ∆xt ψ and ∆xt+1 ψ, present a trade-off relationship as their weighted sum is definite.
However, there is not an ordered relationship between ∆xt ψ and ∆xt+1 ψ. In summary, we conclude that
the sensitivity of current and next state features against perturbations is normally divergent, and the degree
of sensitivity is heavily determined by the task. These conclusions are similar to those we derived in the TD
convergence part.
17
Under review as a conference paper at ICLR 2022
F Proof of Theorem 4
Proof. To prove the convergence of TD under the noisy states, we use the results from (Borkar & Meyn, 2000)
that require the condition about stepsizes αt holds: Pt∞=0 αt = ∞ and Pt∞=0 αt2 = 0. Based on (Sutton
& Barto, 2018), the positive definiteness of A will determine the TD convergence. For linear TD(0), in the
continuing case with γ < 1, A can be re-written as:
A = E "(S) E π⑷S) £p(r, s0|s, a)xt (xt - γxt+1)>
s	a	r,s0
=fμ ⑹1 P(SIS)Xt(Xt- Y xt+ι)>
=£4(S)Xt(Xt- YEp(SIS)χt+1)>
s	s0
=X>DX-X>DYPX
= X>D(I - YP)X
(35)
Then we use At to present the convergence matrix in the case (i) where the perturbation vector et is added onto
the current state features, i.e., Xt — Xt + et, while We use At+ι and At,t+ιto present the counterparts in the
case (ii) and (iii), respectively. Based on Eq. 35, in the case (iii), we have:
At,t+1 =(X+E)>D(X+E)- (X+E)>DYP(X+E)
= (X+E)>D(I-YP)(X+E)
(36)
From (Sutton & Barto, 2018), we know that the inner matrix D(I - YP) is the key to determine the positive
definiteness of A. If we assume that A is positive definite, which also indicates that D(I - YP) is positive
definite equivalently. As such, At,t+1 is positive definite automatically, and thus the liner TD would converge
to the TD fixed point. Next, in the case (i) we have:
At = (X+E)>D(X+E) - (X+E)>DYPX
= A+X>DE+E>DX+E>DE-E>DYPX
= (X+E)>D(I-YP)(X+E)+ (X+E)>DYPE
= At,t+1 + Y(X + E)>DPE
= At,t+1 + Y(X>DYPE + E>DYPE)
Similarly, in the case (ii), we can also attain:
At+1 = X>DX - X>DYP(X + E)
=A-YX>DPE
We know that the positive definiteness of A and At,t+1 is only determined by the positive definiteness of the
inner matrix D(I - YP). If we assume the positive definiteness of A, i.e., the positive definiteness of At,t+1
and D(I- YP), as Y > 0, what we only need to focus on are the positive definiteness ofX>DPE+E>DPE
and -X>DPE. If they are positive definite, TD learning will converge under their cases, respectively. □
(37)
(38)
G Proof of Theorem 5 and Corollary 1
Proof. We combine the proof of Theorem 5 and Corollary 1 together. The TD fixed point wTD to the system
satisfies AwTD = b. Thus, the TD convergence point, i.e., TD fixed point, can be attained by solving the
following regression problem:
min kb - Awk2	(39)
w
To derive the influence function, consider the contaminated distribution which puts a little more weight on the
outlier pair (Xt , Xt+1):
w^ = argmin(1 — e)E[(b — Aw)>(b — Aw)] +
w	(40)
(yb - xA>w)> (yb - xA>w),
where yb = Rt+1Xt and xb = dtXt>. We take the first condition:
(1 - )E(2A>Aw - 2A>b) - 2xA(yb - xA>w) = 0.	(41)
18
Under review as a conference paper at ICLR 2022
Then we arrange this equality and obtain:
(1 - )E(A>A + xAxA>)w = (1 - )E(A>b) + xAyb.
Then we take the gradient on and let = 0, then we have:
(-E(A>A) + xAxA>)w + E(A>A)ψw,Fπ = -E(A>b)
+xAyb.
(42)
(43)
We know that under the least square estimation, the closed-form solution of w is E(A> A)-1 E(A> b). Thus,
after the simplicity, we finally attain:
ψw,Fπ (xt, xt+1) = E(A>A)-1xA(yb - xA>w)
= E(A>A)-1dtxt>xt(Rt+1 - dt>w).
(44)
Next, we prove the Corollary. We only need to focus on the item dtxt>xt (Rt+1 - dt>w), which we denote
as ψ0. Then we use ∆xtψ and ∆xt+1 ψ to represent the change of ψ after adding perturbations η on xt and
xt+1, respectively. In particular, since we approximate ηη>xt and ηη>w as 0, then we have that the change
of influence function for the perturbation η on the current state feature xt :
∆xtψ ≈ (dt + η)(xt>xt + 2η>xt)(Rt+1 - dt>w - η>w) - ψ0
≈ -dtx>xtη>W + 2dtη>xt(Rt+ι - d>w) + η ∙ x>xt(Rt+ι - d>w)
=2dtη>xt(Rt+1 - d>w) - 1(γdtx>xtη>w - YnXxt(Rt+ι - d>w)).
γ
Then the influence function for the perturbation η on the next state feature xt+1 is:
∆xt+1 ψ = (dt - γη)xt>xt (Rt+1 - dt>W + γη>W) - ψ0
≈ γdtxt>xtη>W - γηxt>xt (Rt+1 - dt>W).
Finally, it is easy to observe that the following relationship holds:
γ∆xt ψ = 2γdtηxt>(Rt+1 - dt>W) - ∆xt+1 ψ.
(45)
(46)
(47)
□
H Experimental Setup
After a linear search, in the QR-DQN, We set κ = 1 for the Huber quantile loss across all tasks due to its
smoothness.
Cart Pole After a linear search, in the QR-DQN, we set the number of quantiles N to be 20, and evaluate
both DQN and QR-DQN on 200,000 training iterations.
Mountain Car After a linear search, in the QR-DQN, we set the number of quantiles N to be 2, and
evaluate both DQN and QR-DQN on 100,000 training iterations.
Breakout and Qbert After a linear search, in the QR-DQN, we set the number of quantiles N to be 200,
and evaluate both DQN and QR-DQN on 12,000,000 training iterations.
I SN-MDP setting with Random Noises
From Figure 5, we can easily observe that all DQN and QRDQN algorithms under various strength of random
state noises converge, although they eventually obtained different average returns. This empirical observation is
consistent with our theoretical analysis in Section 3 where both classical and distributional Bellman operators
are contractive and thus convergent. In addition, Figure 5 also manifests that the eventual performance that
algorithms attained has a decreasing tendency as the perturbation strength, i.e., standard deviation, increases
especially for DQN. Remarkably, the final performance of QRDQN is more robust than DQN against different
perturbation strength, especially in Mount Car and Breakout games, although both DQN and QRDQN are
convergent in this SN-MDP-like setting.
19
Under review as a conference paper at ICLR 2022
2001751501251007550%
EnIətt:ωod trao
DQNjBndO.我
DQN_rand0.1
——QRDQN
—QRDQN_iWIdO.05
QRDQfLgdo.1
Random
77.7
64.2
0.0	0.0	0.1	0.1	0.1	0.1	0.1	0.2
Steps (× 104)
EnIətt:Urao ucolunow
……DQN
...DQN-rend0X)1
-DQN-rwκBX)125
——QRDQN
——QRDQfLmndO.5
QRDQN-EM.012$
Random
0.0 o.ι
Steps (×104)
Random
7006005004003002∞100
EnIətt:LnOXEg
2.0	4.0	6.0	8.0	10.0
Steps (× IoS)
Figure 5: Average returns ofDQN and QRDQN against random state observation noises across four
games. randX in the legend indicates random state observations with the standard deviation X. Both
QRDQN (solid lines) and DQN (dashed lines) converge and the convergence level is determined by
the perturbation strength.
Random
Random / Current
Random / Next
Random / Both
Oooooo
Oooooo
6 5 4 3 2 1
UJrU8a AnOXEO.
300
D.0
200
100
0	’……
0.0 2.0 4.0 6.0 8.0 10.0 12.0
Steps (× 106)
DQN
DQN 0.01
DQN 0.05
QRDQN
700
...DQN
...DQN 0-01
DQN 0-05
——QRDQN
——QRDQN 0-01
QRDQN 0-05
600
500
300
200
100
J- l--⅛,g
8.0 10.0 12.0
Stq>s(× 106)
Steps (× 106)
f "-VQ

S
Figure 6:	Average returns ofDQN and QRDQN against random state observation noises on Breakout
environment over 200 runs.
20
Under review as a conference paper at ICLR 2022
J Extension Analysis of Perturbations on Current, NEXT and
Both State Observation
We provide the results of robust performance with different perturbation states in Breakout under random and
adversarial noisy state observations in Figure 6 and 7, respectively.
in the random state observation setting as shown in Figure 6, it turns out that perturbations on current state
observations is more robust than next state by comparing the first and second subfigures given the same pertur-
bation strength. This empirical finding demonstrates the conclusion in Appendix D and E that the sensitivity
of current and next states are normally divergent. The result where noises are added on both states seems to be
most robust, consistent with the mildest TD convergence condition as shown in Theorem 4.

700
Adversarial / Current
600
500
……DQN
..DQN 0005
DQN 0-01
——QRDQN
——QRDQN 0-005
QRDQN 0-01
700
600
500
Adversarial / Next
400
300
200
100
100
2.2
0.0 2.0 4.0 6.0 8.0 10.0 12.0
Steps (× 106)
的蝴O
!316.0
300
238.0
200
……DQN
...DQN 0-005
DQN 0-01
——QRDQN
——QRDQN 0.005
QRDQN0-01
0.0 2.0 4.0 6.0 8.0 10.0 12.0
Steps (× 106)
700
600
500
Adversarial / Both
……DQN
..DQN 0005
DQN 0-01
——QRDQN
——QRDQN 0-005
QRDQN 0-01
300
200
100
产∙n
64.0
0
0.0 2.0 4.0 6.0 8.0 10.0 12.0
StqB(X 106)


o
o

Figure 7:	Average returns of DQN and QRDQN against adversarial state observation noises on
Breakout environment over 200 runs.
in the adversarial state observation setting as shown in Figure 7, it also turns out that perturbations on cur-
rent state observations is more robust than next state by comparing the first and second subfigures given the
same perturbation strength, and algorithms converge easiest. These empirical results also demonstrates the
conclusion we made in Appendix D and E.
21