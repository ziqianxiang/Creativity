Under review as a conference paper at ICLR 2022
GCF: Generalized Causal Forest for Hetero-
geneous Treatment Effect Estimation using
Nonparametric Methods
Anonymous authors
Paper under double-blind review
Ab stract
Heterogeneous treatment effect (HTE) estimation with continuous treatment is es-
sential in multiple disciplines, such as the online marketplace and pharmaceutical
industry. The existing machine learning (ML) methods, like forest-based model-
ing, either work only for discrete treatments or make partially linear or paramet-
ric assumptions that may suffer from model misspecification. To alleviate these
problems, we extend causal forest (CF) with non-parametric dose-response func-
tions (DRFs) that can be estimated locally using kernel-based Double/Debiased
ML estimators. Moreover, we propose a distance-based splitting criterion in the
functional space of Partial DRFs to capture the heterogeneity for continuous treat-
ments. We call the proposed algorithm generalized causal forest (GCF) as it gen-
eralizes the use case of CF to a much broader setup. We show the effectiveness of
GCF compared to SOTA on synthetic data and proprietary real-world data sets.
1	Introduction
Heterogeneous treatment effect (HTE) estimation has been of growing interest for decision-makers
in a wide spectrum of contexts. It uncovers the effect of interventions at sub-group levels, thereby
providing highly tailored suggestions rather than a one-size-fits-all policy. When it comes to pre-
cision medicine, HTE provides the leveraged information that physicians need to precisely treat
different patients with proper dosages of drugs depending on their genes, living habits, and EHR
history. Recently, with the emerging big data that exhibits astronomical complexities, HTE has been
essential for online marketplaces (Syrgkanis et al., 2021; Du et al., 2019; Ye et al., 2018). For large-
scale data that presents a huge challenge, algorithms that adapt the booming machine learning (ML)
techniques to HTE estimation are proposed (Kunzel et al., 2019; Zhao et al., 2017; Chernozhukov
et al., 2018; Hill, 2011). However, these methods largely focus on binary or discrete treatments and
thereby being not widely applicable, given the fact that continuous treatments are prevalent in prac-
tice. Examples include targeting customers with continuous incentives, customizing the duration
of ads to improve user engagement, and design optimal interest rates to maximize revenue while
control for default.
For HTE estimation with continuous treatments, it could be feasible to discretize the treatments and
utilize the above approaches, but the limitations are two-phase and bring non-trivial challenges to
researchers. First, ordinal treatments like different levels of education cannot be treated as cate-
gorical otherwise the order information across treatments will be lost, and secondly, discretizing
or bucketing has the limitations of model inaccuracy and cannot recognize the right patterns of the
outcome across treatments. To elucidate the complex relationship between treatment and outcome,
researchers put forward ideas of dose-response function (DRF).
The estimation of DRF in the presence of high dimensional features can be viewed as a natural
extension of regression problems with additional complexities. The success of random forest (RF)
for regression problems motivates the development of causal forest (CF) (Athey et al., 2019) and
orthogonal random forest (Oprescu et al., 2019) and the modification of Bayesian Additive Random
Forest (Woody et al., 2020; Hahn et al., 2020). These methods partition the feature space with
a splitting criterion fitted on treatments that can provide accurate estimations for HTE. But often
1
Under review as a conference paper at ICLR 2022
Figure 1: ADRF estimates of random forest and causal forest in both linear and nonlinear DRF
specifications. Blue line denotes model average estimations. Confidence bounds of model estimates
generated by multiple simulation run are marked in light-blue shade. Ground truth of testing data
set is represented in red dash-line.
they posit linear or partially linear models of DRF which are vulnerable to model misspecifications.
Figure 1 is an illustration of model misspecification of random forest and CF.
Doubly Robust estimators (DREs) allows for misspecification of either the treatment or outcome
regression. Recent work (Kennedy et al., 2017) combines the estimation of generalized propensity
score with regression models of the outcome that can further reduce the bias and use kernel regres-
sion for non-parametric estimation of DRF. However, it may suffer from the curse of dimensionality
when the feature space is high-dimensional given large-scale instances. To advance, (Colangelo &
Lee, 2020) provide kernel-based DML estimators that can easily deal with high-dimensional data.
Nevertheless, the localization and weighting mechanism of CF is not taken into account, which is
essential for capturing heterogeneity. It remains unexplored that how to effectively combine ML
methods with localized estimations under no model assumptions.
In this paper, we propose generalized causal forest (GCF) by extending CF to a generalized one with
a brand new splitting criterion, and herein the generalization is two-fold. First, we generalize the
local linear model of CF to a non-parametric one with DRF. Meanwhile, we employ the distance
metrics in the functional space of Partial DRF as the splitting criterion and use kernel-based DML
estimators as the DRF approximation.
When it comes to implementations, many open source packages integrate recent research with
the trending ML methods that facilitate HTE estimation, such as EconML (Oprescu et al.),
CausalML (Chen et al., 2020) and GRF. EconML, CausalML offer a suite of high-performance ML
algorithms like DML, while GRF specially supports CF. However, they are only compatible with
single-machine systems and may not be efficient when it comes to large-scale instances. Apache
Spark (Meng et al., 2016) is integrated for handling large-scale data with distributed and in-memory
computing, while inclusively provides APIs of trending ML algorithms with the built-in MLlib.
Therefore, as one of our contributions, we build our algorithm on Spark that enables large-scale data
processing and easy use of any ML techniques. To summarize, the main contributions are:
•	We are the first to generalize CF’s partially linear model to a non-parametric one with
DRF and apply kernel-based DML estimators. The proposed algorithm pictures the non-
parametric behaviors of how outcome varies with treatments.
•	We propose a distance-based splitting criterion with various distance metrics in the func-
tional space of Partial DRF that generalizes the differences of CAPE in CF.
•	Our proposed GCF has been tested on both synthetic and large-scale real-world datasets. It
improves on SOTA in terms of multiple evaluation metrics. We implement GCF on Apache
Spark MLlib and achieves higher computatinal efficiency by distributed computing.
The rest of the paper is organized as follows. In Section 2, we present related works in the literature.
In Section 3, we introduce some notations and backgrounds. In Section 4, we introduce the Gener-
alized Causal Forest. In Section 5, we examine the empirical performance of GCF by applying it to
both synthetic and real-world data sets. We conclude the paper with some discussions in Section 6.
2
Under review as a conference paper at ICLR 2022
2	Related works
A growing amount of literature has been devoted to address the problem of HTE estimation with
continuous treatments. The algorithms in the context of DRF include methods targeting at con-
founding bias, kernel-based or ML-based methods for regression bias reduction, and techniques of
DRE or DML that balance the trade-off between the two biases. DRF combined with IPW (Zhu
et al., 2015; Graham et al., 2015) can achieve consistent estimators by weighting the estimation with
probability density of treatment. For the regression bias, (Galagate & Schafer, 2015) employ para-
metric estimations and (Flores et al., 2007) model DRF as non-parametric functions using kernel
regression. To advance, (Kennedy et al., 2017) propose a doubly robust estimators for DRF by
combine the estimation of generalized propensity score with the estimation of outcome using kernel
regression. Thus far, these approaches only provide global estimations and cannot handle massive
amounts of high-dimensional data.
Recently, the great efficiency of ML methods motivates their generalization to the problem of HTE
estimation. Towards that end, estimating DRF with ML-based algorithms is developed and among
which tree-based models are a great candidate since it partitions the feature space for dimension-
ality reduction and maximizes the heterogeneity as well. Causal Forest (CF) proposed in (Athey
et al., 2019) utilizes a subset of training samples for growing trees by recursively partitioning via a
splitting criterion. Then HTE estimation is given by weighted average over the outcomes of the re-
maining training samples, known as honesty principle. The final estimator obtained from CF further
exhibits a lower degree of bias by tree ensembles. However, their splitting criterion relies on a linear
model assumption and is formulated as the difference between the slopes of linear models. The
same limitation applies to Orthogonal Random Forest (Oprescu et al., 2019) and Bayesian Additive
Regression Tress (Hill, 2011; Woody et al., 2020; Hahn et al., 2020). Nevertheless, the complexity
of HTE estimation may not be fully captured by linear or general parametric models, which brings
the necessity of non-parametric ML models.
Kernel-based DML (Colangelo & Lee, 2020) estimates the nuisance functions with cross-fitting,
constructs a non-parametric DML estimator by Gaeutax Derivative. This motivates the utilization
of DML and non-parametric estimation in our work, though it only provide global estimators with
the limited capacity of localization. A fully non-parametric DML with locally weighted estimations
for continuous treatments has not been considered yet.
Building on prior art, we point out that our integration of DRF estimation into CF overcome the
challenges aforementioned by using local non-parametric DRF to constructing splitting criterion.
We introduce the conceptual partial DRF as a component of the splitting criterion and employ the
distance in the functional space of Partial DRF as a proxy for heterogeneity instead of the difference
of slopes. Moreover, the partial DRF can be estimated precisely by the robust kernel-based DML
estimators in our splitting criterion.
3	Prerequisites
3.1	Notations and Assumptions
We formally introduce the notations for HTE estimation with continuous treatments. Following the
potential outcome framework in (Neyman, 1923; Rubin, 1974), weletT be the continuous treatment,
X = (Xj)jp=X1 bethepX -dim confounding variables, U be the outcome-specific covariates, Z be
the treatment-specific covariates that are independent of U, and Y be the outcome of interests. The
population Ω = (X, U, Z, Y,T) ∈ RpX +pU +pZ+1+1 satisfies
Y = g(T, X, U) + e；T = f (X, Z)+ V
where , ν are noises of zero mean and g : Rp × R → R and f : Rp → R.
{(Xi, Ui, Zi, Yi, Ti), i = 1,..., n} are i.i.d. samples drawn from the population Ω. In practice, the
decomposition ofX, U, Z from observed covariates is difficult. Therefore, without additional spec-
ification, we use X to represent observed covariates throughout the paper to simply notations. The
potential outcomes under treatment t is Y(t). Recall that Propensity Score (PS) (Rubin, 1974) for a
discrete treatment is P(T = t|X), the probability for a unit receiving treatment t given the covari-
ates X. For continuous treatments, (Rubin, 1974; Hirano & Imbens, 2004) introduce Generalized
Propensity Score (GPS) which is the probability density function π(T = t|X).
3
Under review as a conference paper at ICLR 2022
The estimand of interests, CATE θ(t, X), is formally defined as
θ(t, X) = E[Y(t)|X] - E[Y(0)|X]
To identify θ(t, X) , common assumptions as in (Holland, 1986; Kennedy et al., 2017) are made
throughout the paper.
Assumption 1. Consistency: E[Y |T = t] = E[Y(t) |T = t], i.e. the outcome of any sample solely
depends on its treatment;
Assumption 2. Ignorability: The potential outcomes Y(T) is independent of treatment T given
covariates X.
Assumption 3. Positivity: The GPS π(T = t|X) > pmin > 0, ∀t, X, i.e. the density for any
sample receiving any treatment is bounded away from 0.
Under the above assumptions, we have
θ(t, X) =E[Y(t)|X]-E[Y(0)|X] =E[Y|T =t,X]-E[Y|T =0,X]
=E[g(t,X)|T=t,X]-E[g(t,X)|T=0,X]
where the first equality holds by Assumption 1 and the second equality holds by Assumption 2.
Positivity is indispensable for the conditional expectation in the last line to be well-defined whereas
being often too strong. In practice, it can be reduced to the following.
Assumption 4. Weak Positivity: The variance of GPS σ(π) = Jt t2 ∙ π(T = t∣X)dt 一
(Rt t ∙ π(T = t|X)dt)2 >σmin > 0, ∀t, X.
3.2	Dose-Response Function
For continuous treatments, the treatment effect can be characterized by DRF, formally defined as
μ(t) := E[Y|T = t]
(1)
Generally speaking, DRF can be either parametric or non-parametric, with parametric ones being
applied to CATE estimation as in (Robins, 2000; Van der Laan et al., 2003). Non-parametric ones
can better explore the functional space of ADRF out of flexibility and hence being the tool of our
proposed algorithm.
The target estimand of DRF heavily depends on the contexts (Galagate, 2016), such as treatment ef-
(∂log(E%)])∖
k ∂log(t))
fect (E[Y(t)] 一 E[Y(o)]),partial effect ( ©"2"——""1" ) and the elasticity
t2 一 t1
Specifically, conditional DRF (CDRF) μ(t, X) = E[Y|T = t, X] characterizes the treatment ef-
fect at individual levels and thereby leading to a CATE estimator as θ(t, X) = μ(t, X) 一 μ(0, X),
which is the target estimand hereafter.
3.3	Kernel Regression and Double/Debiased Machine Learning
For non-parametric modeling, kernel regression (Fan & Gijbels, 2018) works with solid theoretical
foundations. Specifically, given a non-parametric model y = g(x) + with data (Xi, Yi), kernel
regression with linear kernel smoothers gives the estimator g(x)
En=I Kh(x, Xi) ∙ y
Pn=I Kh(X, Xi)
where
Kh (x, Xi) is a scaled kernel density with h being the bandwidth for smoothness. Typical choices
of kernel include Uniform, Epanechnikov, Biweight, Triweight and Gaussian.
The kernel-based DML estimator proposed in (Colangelo & Lee, 2020) performs a 2-stage DRF
estimation maintaining asymptotically normal. The DML estimator for the DRF μ(t) is proposed as
follows. First, We estimate the CDRF μ(t, X) with μ(t, X) and the GPS π(T = t|X) with π(t∣X).
Then, with the nuisance functions plugged in, kernel-based double/debiased estimator gives
1(.\ _ 1 X 八 X X、, Kh(Ti 一 t)
μ(t) = n ∙ ⅛μ(t, Xi)+	∏(t∣Xi)
.(匕 一 μ(t, Xi))
4
Under review as a conference paper at ICLR 2022
4 Generalized Causal Forest
In this section, we formally present the details of our algorithm, GCF. Building on CF, GCF relaxes
partially linear assumption by adapting non-parametric DRF to the splitting criterion that can be
estimated using kernel-based DML estimator described in the previous section. In what follows, we
give it to a workflow of GCF at both the training stage and prediction stage, followed by the elabo-
rations on the details of splitting criterion, estimations, and practical tweaks for implementation.
Algorithm 1: Generalized Causal Forest
Input Data set O = (Xi, Ti, Yi), i = 1, . . . , n; number of trees B; number of features sampled
for growing a tree mtry; the minimum sample size on each leaf node min.node.size; honesty
fraction α; tolerance τ ; positivity threshold ζ ;
Trainings begin
Data O1 and data O2 by a (α,1 - α) split rule applied to O for honesty;
Pre-train outcome regression model μ and treatment density estimation ∏ on sample Ωι;
b4—1;
while b ≤ B do
Randomly sample a feature set of size mtry X S from X ;
for The stopping rule is not satisfied do
Identify each parent node P ;
Compute the splitting criterion ∆(∙) (3) over the samples in Oi with XS;
Grow the tree Tb by splitting at the parent node P according to the △(•);
end
Assign samples in O2 to leaf nodes based on Tb and let b4—b + 1;
end
end
Output Causal forest with B trees by recursive partitioning on X; node assignments for
samples in O2 ;
Predictions begin
CDRF estimation μb(t, x) by local weighted average over the outcomes of samples in Ω2
that falls into Lb(X) as Pn=i lχi∈Lb(X) ITi=t Yi; CATE estimation
|Lb(x)|
1
θb(t, x) = μb(t, x) - μb(0, x) and θ(t, x) = — ∑b=ι θb(t x);
B
end
We introduce the partial derivative of DRF, PDRF at treatment t with a slightly abuse use of notations
… ∂μ(T)
φ(t) := ~TΓ
…/ 丁、 ∂μ(T, X)
and φ(t, X ):二 ∖τ
T =t	∂T
T =t
Intuitively, it tells about how μ(∙) varies with treatments and characterizes the ”slope” along t.
Different leaf nodes on a tree shall have different slopes as the heterogeneity of covariates X .
Let the number of trees be B and then GCF grows trees Tb, b = 1, . . . , B, by repeating tree-growing
process B times with bootstrapping. At the training stage, with PDRF we construct the tree Tb by
recursive partitioning based on a splitting criterion △(c1, c2). It is proportional to the discrepancy
between the PDRF in the left and right child nodes, to maximize the heterogeneity. We elaborate on
this splitting criterion in the next part.
Our algorithm is implemented on Apache Spark for large-scale data processing and the mechanism
of the tree-growing process is different from that in CF. To elucidate on that, the data is stored at the
master machine and trees are copied to each branch machine. Then data is randomly distributed to
branch machines for the computation and collected back to the master machine for the final criterion
that is used for updating a tree. The updated tree will be copied to each branch machine again and we
repeat the process until the tree stops splitting. This distributed computation and clustered storage
leverage the computational power of multiple machines for speeding up the training process. We
refer readers to the appendix for more information on GCF’s Spark implementation.
5
Under review as a conference paper at ICLR 2022
The splits or the tree-growing process terminate when various pre-specified stopping criteria are
met, for example, the sample size of child nodes is smaller than the parameter min.node.size or the
distance D between PDRF is smaller than min.inf o.g ain τ.
For prediction, we also follow the honesty principle proposed in (Athey et al., 2019) by partition-
ing the training samples into two parts where one is for constructing the tree and the other is for
estimating CATE on the leaf node. That is to say that each training data can either be utilized to es-
timate CATE or contribute to growing a tree. At the prediction stage, the leaf node of given samples
X on each tree Tb is denoted by Lb(x). The estimation of CDRF μb(t, x) on Lb(X) takes a local
weighted average on training samples in Lb(x), which is given by Pin=1
1Xi∈Lb(x) ∙ 1Ti = t ∙ Yi
|Lb(x)|
and
%/,	∖	ʌ / >	∖	ʌ ∕r∖	∖ EI C 1 z-x * rɪ-n- . ∙	. ∙	/` PT- ∙	.∖	CC /	∖	I >
θb(t, x) = μb(t, x) - μb(0, x). The final CATE estimation for X is the average of Lb(X) over B
trees, as θ(t, x) = — PB=I θb(t, x).
B
4.1	Splitting Criterion
Given a parent node P and training samples with covariates Xω, our splitting criterion ∆(c1, c2) is
proposed as
∆(C1,C2) = nnnc2D(Φci (t, Xω), ΦC2 (t, Xω)),	(2)
where Φci , Φc2 are the PDRF estimators, respectively. The sample sizes of parent node, left
nn
child node and right child node are np, n^ and ∏c2, respectively. The ratio -12-2 in ∆(c1,c2)
nP
is to balance the sample sizes between the two child nodes. Distance metric D measures the
distance between Φc1 (t, Xω ) and Φc2 (t, Xω ) in the functional space of PDRF, which is to be
maximized in the splitting so as to optimize the heterogeneity. Some commonly used metrics
are Li,	L2	and	L∞	as in (Dette et al., 2018)	Di	=	Jt	∣Φcι(t, Xω)	- Φc2(t,	Xω)|dt, D2	=
ft ∣φci (t, Xω) - φC2 (t, Xω)∣2dt,D∞ = maxt∣Φci (t, Xω) - Φc2(t, Xω)|. WenextShow how
to estimate PDRF Φc and Φc2 using kernel-based DML estimator (Colangelo & Lee, 2020).
4.2	PDRF Estimation
The estimation for DRF is a precondition for the estimation of PDRF. In this paper, we point out
kernel-based DML estimator in (Colangelo & Lee, 2020) can be adapted to the DRF estimation in
our GCF and guides the tree splitting.
Recall that, the DML estimation for DRF is given by
μ(t) = X Xi) + Kh(T-)t) ∙(Yi - μ(t, Xi)))
where μ(t, Xi) is the pretrained estimator for μ(t, X) at Xi, π(t∣Xi) is the pretrained estimator for
GPS at Xi and C denotes the child node.
When adapted to our framework, we additionally derive the following estimator for CDRF by inno-
vatively combine kernel regression with the DML estimator.
CDRF estimator The estimator is given by
μ(t, Xi) = μ(t, Xi) + KhTX-： ∙ (Yi - μ(t, Xi))
PDRF estimator The PDRF estimator is given by Φ(t) = d∂tt) and Φ(t, x)=。噜X).
In estimating PDRF, the smoothness condition of DRF plays an important role. For smooth DRF,
we can explicitly derive the closed-form PDRF. For smooth Gaussian kernels, Φ(t) is
φ	∂μ(t)	_	Pn=I	Kh(Ti- t)匕 X K	(t_ t)γ] Pi=i	Kh(Ti- t)
()=∂t = Pn=IKh(Ti-1) 心 h( i 阖Pn=IKh(Ti-1)]2
6
Under review as a conference paper at ICLR 2022
However, for general DRF, the PDRF may be discontinuous or even ill-defined. For example, non-
differentiable kernel functions like uniform, epanechnikov, biweight and triweight, Φ can only be
estimated via numerical approximations of Φ(t)
E[μ(t + δ) - μ(t)]
δ
4.3	Practical Considerations
In the work of (Kennedy et al., 2017), they claim that the choice of bandwidth h for kernels weighs
more than the choice of density functions, since h deals with the trade off between bias and variance.
Usually, a small h avoids a large variance while a large h reduces the bias. The empirical ways of
choosing an optimal h are Cross Validation and Rule-of-thumb (Dehnad, 1987)
When utilizing kernel densities for weighting, we often suffer from the boundary bias if without
normalization. To this end, we normalize estimators with the cumulative density function of range
[Tmin, Tmax] of treatments. More specifically, the estimators are divided by RTTmax Kh(Ti - t)dt.
Regarding the positivity assumption in practice, we use a hyperparameter ζ to regularize for the
boundary of σ(π) that is the variance of GPS. This is also a guarantee on the weak assumption as
stated in Assumption 4. Formally, our proposed splitting criterion will be a regularized one as
∆(C1,C2) = nc12nc2D(Φcι (t, Xω ), ΦC2 (t, Xω )) + Z ∙ σ(∏),	(3)
n2P	1	2
where ∏ is the estimated GPS. With this additional term, the splitting will tend to maximize the
variance of GPS and hence taking the variety of treatments across samples into account.
4.4	Asymptotic Property
Let Assumptions 1-3 in (Colangelo & Lee, 2020) hold and thusfar the convergence of kernel-based
DML estimator is naturally guaranteed at a rate of √n as long as the outcome is unbiased or the
generalized propensity score is unbiased. That is to say that We can utilize a subsample S of order
√n to achieve a converged kernel DML estimator. Then S satisfies that s/n → 0 and S → ∞
which is stated as Specification 1 in (Athey et al., 2019). Under the above assumptions and suppose
one additional condition be that the score function is lipschitz with a bounded support of treatment,
then Assumption 1-6 in (Athey et al., 2019) automatically hold for the score function. Throughout
our algorithm, we also follow the honesty principle. Then by Theorem 3 (Athey et al., 2019),
our final estimator is asymptotically unbiased. This shows the doubly robustness of our estimator by
combining kernel DML and Causal Forest under slightly stronger assumptions on the score function.
5	Experiment
In this section, we validate GCF by showing that 1) GCF can provide CATE estimations in a variety
of contexts such as simulated data with true labels and real-world data without the ground truth 2)
GCF is flexible yet effective compared to SOTA which provides enough evidence for its practical
use. More specifically, we present the detailed experimental results of our proposed algorithm and
demonstrate its effectiveness compared to the prior art on synthetic data and real-world data sets.
5.1	Evaluation
In what follows we introduce the commonly used metrics for model evaluation on both synthetic
data and real-world data sets, named as model errors and Qini score and Qini curve, respectively.
For real-world data sets, the model errors are not available since the counterfactual outcomes in the
potential outcomes cannot be observed.
Synthetic Data We adopt PEHE and RMSE as the measure for model precision. Formally, let
n be the number of treatment groups, θtt and θtt be the predicted and true treatment effect of ith
sample at treatment t, θt = Pt=ι yt - y0, y0 be the reference outcome that is pre-determined.
PEHE,RMSE,and ADRF are
pn=ι(∣θt-θt∣)) jpn=ι(θt-θt)
2
7c ʌ /ʃ ∖ 7
-,y(t) = Jx μ(t,x)dx.
n
n
7
Under review as a conference paper at ICLR 2022
Real-world data sets We refer readers to (Gutierrez & Gerardy, 2017) for the formal definition
of Qini Score and Qini Curve. In practice, a larger Qini Score indicates that model has better
performance in identifying HTE.
5.2	Model Setups
The baselines for GCF include RF, CF, and Kennedy (Kennedy et al., 2017). Both RF and CF use
num.trees equals to 500, min.node.size to 50. For Kennedy (Kennedy et al., 2017), we use the
code of function cts.ef f from R package npcausal available in (ehkennedy, 2017). SuperLearner
library is set to SL.ranger and SL.glm. We made small tweaks to SL.ranger to make share same
hyperparaters with RF and CF for comparability.
GCF We implement the proposed GCF on Apache Spark MLlib (Meng et al., 2016) with kernel-
based DML estimator, denoted by GCF. For the implementation of GCF, we fix the distance metric
D to be d2 out of simplicity. The number of trees, the minimum node size and other hyperparameters
are set to match those in RF and CF.
RF is a forest-ensemble method without controlling confoundedness. CF makes the partially lin-
ear assumption on DRF. Kennedy is a non-parametric doubly robust algorithm but uses a normal
splitting criterion. The comparisons inclusively validate our algorithm for both non-parametric DRF
estimation and specially designed splitting criterion.
5.3	Simulation
Here we show a comprehensive simulation study on synthetic data. First, we elaborate on the data
generating process. It is followed by the details of model implementations. Finally we illustrate the
numerical results that consistently show the superior performance of our GCF.
5.3.1	Data Generating Process
Let n denote the number of samples and p = pX + pU + pZ be the dimension of covariates. The
covariate matrix is (Xij)ij==11,,.,,npX = (X1 , . . . , Xn) ∈ Rn×pX . Data generating process (DGP) is
Y = μ(T) + 0∙2(X1 ∙ X1 + X4)T + X ∙ βχ + U ∙ βu + C
T = 20 ∙ Ψ(φ(X ∙ βχ + Z ∙ βz)) + V
where φ is the sigmoid function and Ψ is the Pdf of Beta distribution. Here We set DRF μ to be
polynomial(Poly), exponential(Exp), and sinusoidal functions(Sinus).
The above DGP gives us a total of 6 datasets by taking the possible combinations of 3 DRFs μ(t) and
2 setuPs of covariate Σ. Noises C, ν follow Unif (-1, 1). Covariates X and coefficients β follow
N(0, IpX). We also allow sparsity in covariates by randomly setting some of the coefficients to 0.
Testing data sets are generated with randomly assigned treatments to allow unbiased evaluation.
Figure 2: Comparison of ADRF estimates for RF, CF, Kennedy and GCF. DRF follows sinusoidal
pattern. Blue line denotes model average estimations. Confidence bounds of model estimates gen-
erated by multiple simulation run are marked in light-blue shade. Ground truth of testing data set is
represented in red dash-line.
8
Under review as a conference paper at ICLR 2022
5.3.2	S imulation Results
Since some models output the final prediction of Y (t) and some output the treatment effect θ(t), we
1 Λ∙1	.1	.1	1	.	，广7，\	，广 /，、	，广 ∕C∖	JlFl ♦ ,1	CT
deliberately compute the pseudo outcome Y (t) = Y (t) - Y (0) as the label in the purpose of the
comparability across baselines. The simulation results pf 3 datasets are summarized in Table 1 and
those of the other 3 datasets are in Appendix. Here PEHE and RMSE are averaged and standard
deviations of the metrics across simulation runs are also reported. Overall, GCF significantly out-
performs baseline methods, which again validates the superiority of the newly proposed algorithm.
More specifically, GCF exhibits the smallest biases across multiple DGP setups which we defer
details to the supplementary.
Additionally, we compare the ADRF curve given by different models to the ground truth. An exam-
ple of ADRF under a DGP specification is shown in Fig. 2.
Table 1: Simulation results on datasets with different DRFs where number of samples n =
1000, pX = 50, pY = 5,pZ = 5. Standard deviations are in parenthesis over 100 simulations.
Methods	Polynomial		Sinusoidal		Exponential	
	PEHE	RMSE	PEHE	RMSE	PEHE	RMSE
RF	5.63(0.4)	4.61(0.3)	4.27(0.4)	3.21(0.2)	3.57(0.4)	2.62(0.2)
CF	14.09(0.4)	12.58(0.4)	5.15(0.4)	3.96(0.2)	4.34(0.3)	3.37(0.2)
Kennedy	4.37(0.5)	3.36(0.5)	4.14(0.5)	2.78(0.3)	3.86(0.4)	2.54(0.2)
GCF	4.14(0.3)	2.88(0.2)	4.05(0.4)	2.7(0.3)	3.85(0.4)	2.48(0.2)
5.4	Real-world data sets
Our method is also implemented on real-world data sets with size of 10,698,884 which sourced
from a leading ride-hailing company. A discount that randomly sampled (one of 6 different values
{d0, d1, d2, d3, d4, d5 : d0 < d1 < . . . < d5}) is assigned to all trips in a given origin-destination-
time tuple. The effect of discount on the trip complete rate is of primary interests. We apply GCF to
this data set and evaluate the performance of models by calculating Qini Scores.
Table 2: Qini scores of models under different treatments
Methods	d5	d4	d3	d2	d1
Xgboost	0.253	0.171	0.177	0.206	0.177
CF	0.253	0.194	0.202	0.272	0.300
GCF	0.309	0.248	0.305	0.444	0.780
The Qini scores of different models are summarized in Table 2. The performance of GCF is superior
than the company’s current models as GCF has the highest Qini scores across all levels of discounts.
We further create an incentive policy based on GCF output and deploy it for online purpose. Online
A/B testing shows a significant lift in KPI for GCF-driven policy.
6	Conclusion
In this paper, we propose a novel forest-based non-parametric algorithm to address the problem of
HTE estimation with continuous treatments. Under the situation that a fully non-parametric and
localized ML-based algorithm has not been proposed yet, we extend Causal Forest with a DRF-
based splitting criterion computed by the distance in the functional space of PDRF. To estimate this
DRF, We use the kernel-based DML estimator to guarantee double robustness that mitigates model
misspecifications. We implement GCF on Spark to leverage the computational efficiency and test it
on both synthetic data and real-world data sets and compare it with the prior art approaches. The
numerical results demonstrate that our method significantly outperform competing methods.
9
Under review as a conference paper at ICLR 2022
References
Susan Athey, Julie Tibshirani, Stefan Wager, et al. Generalized random forests. The Annals of
Statistics, 47(2):1148-1178, 2019.
Huigang Chen, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. Causalml: Python
package for causal machine learning. arXiv preprint arXiv:2002.11631, 2020.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney
Newey, and James Robins. Double/debiased machine learning for treatment and structural pa-
rameters, 2018.
Kyle Colangelo and Ying-Ying Lee. Double debiased machine learning nonparametric inference
with continuous treatments. arXiv preprint arXiv:2004.03036, 2020.
Khosrow Dehnad. Density estimation for statistics and data analysis, 1987.
Holger Dette, Kathrin Mollenhoff, Stanislav Volgushev, and Frank Bretz. Equivalence of regression
curves. Journal of the American Statistical Association, 113(522):711-729, 2018.
Shuyang Du, James Lee, and Farzin Ghaffarizadeh. Improve user retention with causal learning.
In Proceedings of Machine Learning Research, volume 104 of Proceedings of Machine Learning
Research, pp. 34-49. PMLR, 05 Aug 2019. URL https://proceedings.mlr.press/
v104/du19a.html.
ehkennedy. npcausal. https://github.com/ehkennedy/npcausal, 2017.
Jianqing Fan and Irene Gijbels. Local polynomial modelling and its applications: monographs on
statistics and applied probability 66. Routledge, 2018.
Carlos A Flores et al. Estimation of dose-response functions and optimal doses with a continuous
treatment. University of Miami, Department of Economics, November, 2007.
Douglas Galagate. Causal Inference With a Continuous Treatment And Outcome: Alternative Esti-
mators For Parametric Dose-Response Functions With Applications. PhD thesis, 2016.
Douglas Galagate and Joseph L Schafer. Estimating average dose response functions using the r
package causaldrf, 2015.
Daniel J Graham, Emma J McCoy, and David A Stephens. Doubly robust dose-response estimation
for continuous treatments via generalized propensity score augmented outcome regression. arXiv
preprint arXiv:1506.04991, 2015.
Pierre Gutierrez and Jean-Yves Gerardy. Causal inference and uplift modelling: A review of the
literature. In International Conference on Predictive Applications and APIs, pp. 1-13, 2017.
P Richard Hahn, Jared S Murray, Carlos M Carvalho, et al. Bayesian regression tree models
for causal inference: Regularization, confounding, and heterogeneous effects (with discussion).
Bayesian Analysis, 15(3):965-1056, 2020.
Jennifer L. Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1):217-240, March 2011. ISSN 1061-8600. doi: 10.1198/jcgs.
2010.08162. Funding Information: The author gratefully acknowledges financial support for
this project from NSF grant 0532400. I thank Andrew Gelman, James Robins, Kosuke Imai, and
anonymous referees for helpful comments on the article and the methods.
Keisuke Hirano and Guido W Imbens. The propensity score with continuous treatments. Applied
Bayesian modeling and causal inference from incomplete-data perspectives, 226164:73-84, 2004.
Paul W Holland. Statistics and causal inference. Journal of the American statistical Association, 81
(396):945-960, 1986.
Edward H Kennedy, Zongming Ma, Matthew D McHugh, and Dylan S Small. Nonparametric meth-
ods for doubly robust estimation of continuous treatment effects. Journal of the Royal Statistical
Society. Series B, Statistical Methodology, 79(4):1229, 2017.
10
Under review as a conference paper at ICLR 2022
Soren R KunzeLJasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heteroge-
neous treatment effects using machine learning. Proceedings of the national academy of sciences,
116(10):4156—4165, 2019.
Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu,
Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al. Mllib: Machine learning in apache
spark. The Journal of Machine Learning Research, 17(1):1235-1241, 2016.
Jersey Neyman. Sur les applications de la the´orie des probabilite´s aux experiences agricoles: Essai
des principes. Roczniki Nauk Rolniczych, 10:1-51, 1923.
Miruna Oprescu, Vasilis Syrgkanis, Keith Battocchi, Maggie Hei, and Greg Lewis. Econml: A
machine learning library for estimating heterogeneous treatment effects.
Miruna Oprescu, Vasilis Syrgkanis, and Zhiwei Steven Wu. Orthogonal random forest for causal
inference. In International Conference on Machine Learning, pp. 4932-4941. PMLR, 2019.
James M Robins. Marginal structural models versus structural nested models as tools for causal
inference. In Statistical models in epidemiology, the environment, and clinical trials, pp. 95-133.
Springer, 2000.
Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies.
Journal of educational Psychology, 66(5):688, 1974.
Vasilis Syrgkanis, Greg Lewis, Miruna Oprescu, Maggie Hei, Keith Battocchi, Eleanor Dillon, Jing
Pan, Yifeng Wu, Paul Lo, Huigang Chen, et al. Causal inference and machine learning in practice
with econml and causalml: Industrial use cases at microsoft, tripadvisor, uber. In Proceedings
of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 4072-4073,
2021.
Mark J Van der Laan, MJ Laan, and James M Robins. Unified methods for censored longitudinal
data and causality. Springer Science & Business Media, 2003.
Spencer Woody, Carlos M. Carvalho, P. Richard Hahn, and Jared Murray. Estimating heterogeneous
effects of continuous exposures using bayesian tree ensembles: revisiting the impact of abortion
rates on crime. arXiv: Applications, 2020.
Peng Ye, Julian Qian, Jieying Chen, Chen-hung Wu, Yitong Zhou, Spencer De Mars, Frank Yang,
and Li Zhang. Customized regression model for airbnb dynamic pricing. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
932-940, 2018.
Yan Zhao, Xiao Fang, and David Simchi-Levi. Uplift modeling with multiple treatments and general
response types. In Proceedings of the 2017 SIAM International Conference on Data Mining, pp.
588-596. SIAM, 2017.
Yeying Zhu, Donna L Coffman, and Debashis Ghosh. A boosting algorithm for estimating gen-
eralized propensity scores with continuous treatments. Journal of causal inference, 3(1):25-40,
2015.
11
Under review as a conference paper at ICLR 2022
7	Appendix
The appendix runs as follows. First, we list all the notations used in the appendix. Then we dive into
the details of the data generating process for the simulation study and present additional simulation
results which further show the effectiveness of our proposed method. Subsequently, we briefly
outline GCF’s Spark framework, which enables large-scale instances processing and faster model
training. Finally, we demonstrate how to apply our method to an online schema and achieve superior
performance as its another validation.
7.1	Notations
We restate the notations that are consistent with the main paper.
Following the potential outcome framework in (Neyman, 1923; Rubin, 1974), we let T be the con-
tinuous treatments, X = (X j)jp=1 be the pX -dim confounder, U be the pU -dim outcome-specific
adjustment variable, Z be the pZ -dim treatment-specific adjustment variable, and Y be the ob-
served outcome.The potential outcomes under treatment t is Y(t). The population Ω = (Σ =
(X, U, Z), Y, T) ∈ RpX +pU +pZ +1 satisfies
Y =g(T,X,U)+
T=f(X,Z)+ν
where , ν are noises of zero mean,
andg : RpX+pU × R → Randf : RpX+pZ → R.
{(∑i,匕,Ti), i = 1,... ,n} are i.i.d. samples drawn from the population Ω. Then the covariate
matrix Σ = (Σij)11≤≤ij≤≤np = (Σ1,...,Σn) ∈ Rn×p where p = pX + pU + pZ . The Generalized
Propensity Score is π(T = t|X), which is the probability density for a unit receiving treatment t
given the covariate X .
7.2	Data Generating Process
Recall that the covariate matrix Σ = (Xij, Uij, Zij) = (X1, . . . , Xn, U1, . . . , Un, Z1, . . . , Zn) ∈
Rn×(pX +pU +pZ) and treatment T ∈ R where n denotes the number of observations and p is the
dimension of covariates. Data generating process (DGP) works as
Y = μ(T) + 0.2(X2 + X4)T + X ∙ βχ + U ∙ βu + e
T = 20 ∙ Ψ(φ(X ∙ βχ + Z ∙ βz)) + V
where φ is the sigmoid function and Ψ is the pdf of Beta distribution with shape parameters set to 2
and 3. Here We set DRF μ as polinomial(Poly), exponential(Exp), and sinusoidal functions(Sinus).
μ(t) = 0.2 * (t - 5)2 — t 一 5, Polynomial
μ(t) = log(1 + exp(t)∕(t + 0.1)) — log(11), Exponential
μ(t) = 5 * Sin(X) + x, Sinusoidal
The above DGP gives us a variety of data sets by taking the combination ofDRF μ(t) and covariate
Σ that are specified by n,p and DRF. Parameter , ν are noises following N(0, 1). Covariates
X, U, Z follows N(0, I) and coefficient vector β follow [U nif (-1, 1)]. Following this rule, we
generate 100 rounds. Meanwhile, in test data, we randomly assigned the treatments to make sure
unbiased evaluations.
7.3	Simulation results
In addition to the results provided in the main paper, here we provide the inclusive results of our
simulation study, which covers all 6 data sets from the Data Generating process.
The results are summarized in Table 3. Compared to CF (Athey et al., 2019) that is the benchmark of
our generalization, our GCF outperforms it with significantly smaller errors. With the baseine fixed
to be Kennedy (Kennedy et al., 2017), our algorithm almost enjoys smaller errors except one dataset
12
Under review as a conference paper at ICLR 2022
Table 3: Simulation Results on Different data sets
Setup: 1k,100,100,10,10					Exponential	
Methods	Polynomial		Sinusoidal			
	PEHE	RMSE	PEHE	RMSE	PEHE	RMSE
RF	8.08(0.4)	7(0.4)	4.89(0.4)	3.74(0.2)	4.28(0.4)	3.3(0.2)
CF	13.58(0.4)	12.04(0.4)	5.13(0.3)	3.96(0.2)	4.27(0.3)	3.25(0.2)
Kennedy	4.36(0.4)	3.27(0.3)	4.16(0.4)	2.75(0.2)	3.9(0.4)	2.56(0.2)
GCF	4.3(0.3)	3.01(0.2)	4.19(0.4)	2.86(0.2)	3.91(0.4)	2.53(0.2)
Setup: 1k,100,50,5,5						
	Polynomial		Sinusoidal		Exponential	
Methods	PEHE	RMSE	PEHE	RMSE	PEHE	RMSE
RF	5.63(0.4)	4.61(0.3)	4.27(0.4)	3.21(0.2)	3.57(0.4)	2.62(0.2)
CF	14.09(0.4)	12.58(0.4)	5.15(0.4)	3.96(0.2)	4.34(0.3)	3.37(0.2)
Kennedy	4.37(0.5)	3.36(0.5)	4.14(0.5)	2.78(0.3)	3.86(0.4)	2.54(0.2)
GCF	4.14(0.3)	2.88(0.2)	4.05(0.4)	2.7(0.3)	3.85(0.4)	2.48(0.2)
where DRF is sin. Overall, these experimental evidence demonstrates that our GCF is superior than
the algorithms for continuous treatments.
Meanwhile, ADRF curves are also inclusively presented here as validations of our proposed GCF.
Figure 3-8 are ADRF curves of baseline methods and GCF on different synthetic data sets. Across
all the data sets, our proposed GCF performs the best by being the closest to the ground truth.
7.4	Spark Implementation
Apache Spark (Meng et al., 2016) has the power of large-scale data processing and provides APIs
of any machine learning algorithms. Consequently, we build our proposed algorithm on Spark to do
parallel computing and distributed model training that leverage the resources of multiple machines
simultaneously.
The workflow of Spark with GCF is shown in Figure 9, which include integrated data and distributed
computation. More specifically, with the parallel structure of spark, the tree-growing process runs
and the process of GCF is depicted in Figure 10, which is significantly different from that in CF
(Athey et al., 2019) by distributing tasks for computations
13
Under review as a conference paper at ICLR 2022
Figure 3: ADRF estimates . Blue line denotes model average estimations. Red dashline represents
Figure 4: ADRF estimates . Blue line denotes model average estimations. Red dashline represents
Figure 5: ADRF estimates . Blue line denotes model average estimations. Red dashline represents
Figure 6: ADRF estimates . Blue line denotes model average estimations. Red dashline represents
14
Under review as a conference paper at ICLR 2022
Figure 7: ADRF estimates . Blue line denotes model average estimations. Red dashline represents
Figure 8: ADRF estimates . Blue line denotes model average estimations. Red dashline represents
Figure 9: Parallel architecture of GCF Spark Implementation. Data are partitioned at the nodes level
and distributed across executors to achieve high performance
15
Under review as a conference paper at ICLR 2022
If the stack is empy
If Splitting
Figure 10: Diagram of GCF algorithm workflow
Compute the gain at
each feature and
splitting candidate
Compute the gain at
each feature and
SDIittinQ CanClidate
Search for nodes for
splitting
Samples
Honesty Tree Splitting
Statistics aggregator
for each node
Repeat for all the
samples and update
features and StatiStiCS
Stacks for spitting
candidates
For all nodes do
feature splitting and
broadccstirIq
Create queries for the
search
Initialize the stack by
the root nodes
Left and right nodes
into the stack
Generate a leaf node
Repeat for all the
samples and update
features and StatiStiCS
For all nodes do
feature splitting and
broadcasting
Statistics aggregator
for each node
Create queries for the
search
Search for nodes for
splitting
16
Under review as a conference paper at ICLR 2022
7.5	Online Deployment and Experiments
The deployment of models in the online marketplace is twofold as illustrated in Figure 11. On the
one hand, the offline model that is trained on historical data provides HTE estimation that decision-
makers need to customize incentives at the origin-destination-time level. On the other hand, the
outcome in response to the customized incentives contributes to the data prepared for the subsequent
model training. The following is an example of an online experiment with the GCF-driven policy.
We conduct online A/B testings with the model on 2 equal-size non-overlapping groups, i.e. the
cohort. The evaluation metric of the online experiment is the finish order (FO) increment. The
numerical results of GCF and CF are shown in Figure 12. GCF improves on CF by 15.1%, ben-
efiting from the relaxation of the assumptions on the model specification. It shows that the more
complicated or more general the treatment is, the more efficient the model is, which again shows the
superiority of our novel combination of DRF and CF for continuous treatment.
Figure 11: Online Experimental Design
Figure 12: GCF vs CF on online experiment
17