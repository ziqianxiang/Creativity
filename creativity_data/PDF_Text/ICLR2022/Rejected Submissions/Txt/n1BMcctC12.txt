Under review as a conference paper at ICLR 2022
Randomized Primal-Dual Coordinate Method
for Large-scale Linearly Constrained Nons-
mooth Nonconvex Optimization
Anonymous authors
Paper under double-blind review
Ab stract
The large-scale linearly constrained nonsmooth nonconvex optimization finds
wide applications in machine learning, including non-PSD Kernel SVM, linearly
constrained Lasso with nonsmooth nonconvex penalty, etc. To tackle this class
of optimization problems, we propose an efficient algorithm called Nonconvex
Randomized Primal-Dual Coordinate (N-RPDC) method. At each iteration, this
method only randomly selects a block of primal variables to update rather than
updating all the variables, which is suitable for large-scale problems. We provide
two types of convergence results for N-RPDC. We first show that any cluster point
of the sequence of iterates generated by N-RPDC is almost surely (i.e., with proba-
bility 1) a stationary point. In addition, We also provide an almost sure asymptotic
convergence rate of O(1∕√k). Next, We establish the expected O(ε-2) iteration
complexity of N-RPDC in order to drive a natural stationarity measure beloW ε
in expectation. The fundamental aspect to establishing the aforementioned con-
vergence results is a surrogate stationarity measure We discovered for analyzing
N-RPDC. Finally, We conduct a set of experiments to shoW the efficacy of N-
RPDC.
1	Introduction
Many large scale problems arising in machine learning amounts to solving the folloWing linearly
constrained nonsmooth nonconvex optimization problem:
min F(x) = f(x) + g(x)
s.t	Ax - b = 0,
(P)
Where A ∈ Rn×d and b ∈ Rn . Throughout this paper, We impose the folloWing assumptions on
problem (P): the function f : Rd → R is possibly nonconvex and continuously differentiable With
its gradient Vf being Lf-LiPSChitz continuous, X is a convex and compact set of Rd, i.e., there
exits a positive number M such that M = maxx,x0∈X kx - x0k, and g : Rd → R is nonsmooth and
nonconvex. More precisely, g is assumed to be loWer semicontinuous (l.s.c) ρg-Weakly convex and
has bounded subgradients over X, i.e., there exists Lg > 0 such that ksk ≤ Lg, ∀s ∈ ∂g(x), x ∈ X,
where ∂g is the subdifferential of g (see (4) for definition). Let F * be the optimal value of (P).
Recall that a function g is said to be Pg-weakly convex if g(∙) + Pg ∣∣ ∙ k2 is convex for some constant
ρg ≥ 0 (Vial, 1983). It is Worth mentioning that a Wide class of nonsmooth nonconvex functions
belong to the weakly convex class; see, e.g., (Vial, 1983; Davis & Drusvyatskiy, 2019) for more
discussions on weak convexity.
We further assume g(x) = PiN=1 gi(xi) is block separable with respect to the space decomposition
N
X = Xi X X2	× …×	XN,	Xi	∈	Xi	⊂ Rdi,	and X	di	= d.
i=1
(1)
Let A = (A1, A2, . . . , AN) ∈ Rn×d be the corresponding partition ofA, where Ai ∈ Rn×di. When
N = d, each Xi ⊂ R corresponds to a box constraint on the coordinate xi for i = 1, . . . , d.
1
Under review as a conference paper at ICLR 2022
Motivations. Our motivation for studying large scale problem (P) stems from the fact that this
problem class finds wide applications in machine learning. To be more specific, we will present
several practical examples below that give rise to (P).
Application 1: Non-PSD kernel support vector machine. Support vector machine (SVM) is a widely
utilized technique for supervised learning (Boser et al., 1992; Cortes & Vapnik, 1995). In order
to improve the interpretability and enhance the robustness, some non-PSD kernels are used in the
kernalized SVM method, such as the sigmoid kernel (Lin & Lin, 2003), the jittering kernel (DeCoste
& SchOlkopf, 2002), the tangent distance kernel (Haasdonk & Keysers, 2002, August), to name a
few. Formally, the kernalized SVM problem can be written as
min	1 x> Qx — 1>x
x∈[0,c]d 2	d	(2)
s.t. y>x = 0,
where Q is a d × d non-PSD matrix, c ∈ R is the upper bound of all variables, y ∈ {—1, 1}d is the
vector of labels, and 1d is an d-dimensional vector of all 1s.
Application 2: Linearly constrained Lasso with nonsmooth nonconvex penalty. The Lasso is one of the
most popular methods for variable selection. The standard LaSSo uses 'ι-norm to select important
variables. Some works propose to utilize nonsmooth nonconvex regularizers to further improve the
performance of the Lasso; see, e.g., (Breheny & Huang, 2011; 2015; Rakotomamonjy et al., 2019).
If additional prior information is available, the works (Deng et al., 2020, May; Gaines et al., 2018;
James et al., 2013; Won et al., 2019) propose the constrained Lasso model. The linearly constrained
LASSO with nonsmooth nonconvex penalty can be formulated as follows:
d
min 2kA，-bk2+ip1φ(Xi)
s.t. Bx — c = 0
(3)
where x = (x1, ..., xd)>, X ⊂ Rd, A ∈ Rn×d is the design matrix, b ∈ Rn is the response vector,
B ∈ Rm×d and c ∈ Rm are given constraints, and φ is a nonsmooth nonconvex regularizer such as
MCP (Zhang et al., 2010) or SCAD (Fan & Li, 2001) penalty.
There are plenty of other applications that give rise to (P), e.g., the robust M-estimators, distributed
learning, etc. Due to the limitation of space, we will not expand them in details here.
Related works. Linearly constrained nonconvex optimization. Perhaps the most widely utilized class
of algorithms for solving constrained optimization problems are the Primal-dual methods. Previous
works on single-loop primal-dual methods mainly consider linearly contained convex optimization
problems. The work(Bot & Nguyen, 2020) considers a nonconvex instance with a special linear
constraint Ax - z = 0, where both x and z are decision variables and the matrix A is assumed to be
surjective. Under these assumptions, convergence results were established for a proximal alternat-
ing direction method of multipliers (ADMM). The recent works (Zhang & Luo, 2020a;b) provide
convergence results of ADMM using gradient-based updates for linearly constrained smooth non-
convex optimization problems with a general linear constraint Ax = b, i.e., problem (P) with g ≡ 0.
The main insights of their results are a construction of an proximally regularized auxiliary problem
that dates back to (Bertsekas, 1979) and a properly constructed Lyapunov function. Then, they es-
tablished descent property on the Lyapunov function based on the dual error bound condition (see
also (Hong & Luo, 2017)), which leads to iteration complexity results. The method introduced in
(Zhu et al., 2020) can be used to solve problem (P) with specific linear constraints. This method
is based on the auxiliary problem principle of augmented Lagrangian (APP-AL) method (Cohen &
Zhu, 1984; Zhao & Zhu, 2019; 2021), which can be seen as a forward-backward splitting method
and applies a Jacobian updating strategy.1 However, both ADMM-type and APP-AL-type methods
compute the full gradient of primal variables and update all primal variables at each iteration. There-
fore, the computation complexity of one iteration of these two types of methods are expensive for
large-scale problems (See Remark 2.1).
Randomized coordinate methods for unconstrained optimization. In the past decade, big data applica-
tions are ubiquitous in machine learning. The formulated optimization problems often involve very
large datasets, and hence computing the function value or the gradient can be very expensive. These
1ADMM applies a Gauss-Seidel like minimization strategy, which is related to the Douglas-Rachford split-
ting method.
2
Under review as a conference paper at ICLR 2022
observations motivate the study of the randomized coordinate methods. The researches on this type
of methods can be traced back to (Nesterov, 2012; 2014). In (Nesterov, 2012), Nesterov studied
the iteration complexity of the randomized coordinate gradient (RCG) descent method for smooth
convex optimization. Later, in (Nesterov, 2014), the same author analyzed the randomized coordi-
nate subgradient method for a set of piece-wise linear nonsmooth convex optimization problems.
The works (Richtarik & Takac, 2014) and (LU & Xiao, 2015) extends Nesterov's results to convex
additive composite optimization. Iteration complexity and linear convergence of RCG were studied
in (Patrascu & Necoara, 2015) and (Karimi et al., 2016) for nonsmooth nonconvex optimization
and smooth nonconvex optimization, respectively. Moreover, by employing an asynchronous point
to evaluate the gradient in each iteration, The works (Liu & Wright, 2015) and (Liu et al., 2014)
establish the iteration complexity results of asynchronous RCG for convex smooth optimization and
additive composite optimization, respectively.
A Few results on randomized coordinate methods for linearly constrained optimization. Though the
randomized coordinate-type methods for unconstrained optimization are extensively studied, there
are only a few results for this type of methods concerning coupled linearly constrained optimization
problems. The works (Gao et al., 2019; Wang et al., 2014) study the randomized primal-dual co-
ordinate (RPDC) method and provides iteration complexity results for linearly constrained convex
optimization. Better complexity results of RPDC were obtained for linearly constrained strongly
convex optimization (Xu & Zhang, 2018). The recent works (Zhu & Zhao, 2020; Latafat et al.,
2019; Fercoq & Bianchi, 2019; Alacaoglu et al., 2020) provide almost sure (i.e., with probability
1) asymptotic convergence results of RPDC-type methods for linearly constrained convex optimiza-
tion. Itis worth emphasizing that the works (Latafat et al., 2019; Fercoq & Bianchi, 2019; Alacaoglu
et al., 2020) establish sequential convergence results (i.e., the convergence of the whole sequence of
iterates).
However, the above mentioned works only considers either ADMM-type and APP-AL-type meth-
ods for linearly constrained smooth nonconvex optimization problem or the randomized coordinate
methods for linearly constrained convex optimization. To the best of our knowledge, no previous re-
sult concerns convergence and iterate complexity of the randomized coordinate methods for linearly
constrained nonsmooth nonconvex optimization, which will be the main focus of this paper.
Main contributions. In this paper, we aim to solve the large-scale linearly constrained nonsmooth
nonconvex optimization problem (P). To tackle it, we propose the Nonconvex Randomized Primal-
Dual Coordinate (N-RPDC) method based on an auxiliary problem (see Section 2). Relying on the
mild local uniform metric subregularity property (see Section 3.3), we provide two types of conver-
gence results for N-RPDC (see Section 4). We first show that any cluster point of the sequence of
iterates generated by N-RPDC is almost surely (i.e., with probability 1) a stationary point of (P). In
addition, We also provide an almost sure asymptotic convergence rate of O(1∕√k), where k repre-
sents iteration number. Next, we establish the expected iteration complexity of N-RPDC. Namely,
N-RPDC needs at most O(ε-2) number of iterations in order to drive a surrogate stationarity mea-
sure below ε in expectation.
The fundamental aspect to our convergence analysis is a surrogate stationarity measure we discov-
ered for analyzing N-RPDC (see Section 3.1), which is one of the main contributions of this work.
The standard stationarity measure (i.e., checking KKT conditions) is not directly applicable here
due to the stochastic nature of N-RPDC. Instead, we define the notion of reference point and use its
distance to the iterate generated by N-RPDC as a surrogate stationarity measure. Then, such a surro-
gate stationarity measure is clarified by showing that it is an upper bound of the standard stationarity
measure at the reference point up to a numerical constant (see Proposition 3.1 and Remark 3.1).
There are also some other interesting techniques that we utilized for establishing the above conver-
gence results. For example, the utilization of the local uniform metric subregularity is crucial in
order to deal with the nonsmooth nonconvex term g in problem (P), as it is far from obvious how to
prove the previously used error bound condition if g is not null (see Section 3.3).
Notations. We useh∙,)and k ∙ k to denote the Euclidean inner product and the Euclidean norm,
respectively. For a matrix A, its minimum eigenvalue is denoted by λmin(A). The spectral norm of
a matrix A is denoted by kAk. Let S be a subset of Rd. We use projS(z) to denote the orthogonal
projector onto S and dist(z, S) := infx∈S kx - zk to denote the distance between x and S. When
3
Under review as a conference paper at ICLR 2022
S = 0, we set dist(z, S) = +∞. ZS(Z) = < .0, Z ∈ S represents the indicator function of the
+∞, z ∈/ S
set S. IfS is a convex set, then the limiting normal cone to S is defined as NS(Z) = {ξ : hξ, ζ - Zi ≤
0, ∀ζ ∈ S}.
2	Auxiliary Problem and Proposed Algorithm
In this section, we will define an auxiliary problem of (P). An important feature of the auxiliary
problem is that it has exactly the same set of KKT points to the original problem (P). Furthermore,
the auxiliary problem has a better geometric structure, which makes it easier to solve. Finally, we
will propose a primal-dual coordinate method based on the auxiliary problem.
Let us first present some preliminaries on the subdifferential and KKT conditions. Due to the fact
that problem (P) is highly nonsmooth and nonconvex, our general purpose is to design an efficient
algorithm for finding a stationary point rather than a globally optimal solution. Hence, we will define
certain suitable stationarity measures in this subsection.
Recall that the function g in problem (P) is τ -weakly convex. By (Vial, 1983, Proposition 4.6), we
have
∂g(x) = ∂h(x) - τx,	(4)
where h is the associated convex function such that g(x) = h(x) 一 2∣∣x∣∣2 (see (Vial, 1983, ProPo-
sition 4.3) for the guarantee of the existence of such a function h) and ∂h(x) is the usual convex
subdifferential. Thus, the subdifferential of a weakly convex function is always well defined.
The Lagrangian of problem (P) can be written as
L(x, p) = f(x) + g(x) + hp, Ax 一 bi,
where p is the dual variable. Then, we have the following KKT conditions for problem (P):
Vf (x) + ∂g(x) + NX (x) + A>p
Ax 一 b
A feasible point (x, p) ∈ X × Rn is called a stationary point of (P) if it satisfies the above KKT
conditions, i.e., it satisfies dist (0, ∂L(x, p)) = 0.
2.1	The Auxiliary Problem
Thanks to the construction used in (Bertsekas, 1979), we introduce an auxiliary problem to (P) in
the following:
min	F +(x,z) = f (x) + g(x) + 2 kx — zk2
x∈X,z∈Rd	2	(P+)
s.t Ax 一 b = 0,
where σ > Lf + ρg is a regularization parameter.
We now characterize the set of stationary points of (P+). Towards that end, let w = (x, Z, p) ∈
X × Rd × Rn. The Lagrangian of (P+) is given by
L+(w) = L+(x, z,p) = f (x) + g(x) + 2∣∣x — z∣2 + hp,Ax — bi.
Thus, we have the following KKT conditions for problem (P+):
(Vf (x) + ∂g(x) + σ(x — z) + NX(x) + A>p ʌ
0 ∈ ∂L+ (w) = ∂L+ (x, Z, p) :=	σ(Z — x)	.	(6)
Ax — b
A point w = (x, Z, p) ∈ X × Rd × Rn satisfying the above KKT conditions is called a stationary
point of (P+). One crucial feature of the auxiliary problem is that it has exactly the same set of
stationary points as that of the original problem (P), which is presented in the following lemma.
Lemma 2.1 (relation between stationary points of (P) and (P+)). The following two statements are
equivalent: (a) (x*,p*) is a stationary point of problem (P); and (b) (x*,x*,p*) is a stationary
point of problem (P+).
0 ∈ ∂ L(x, p) :
4
Under review as a conference paper at ICLR 2022
Algorithm 1 N-RPDC: Nonconvex Randomized Primal-Dual Coordinate Method
Initialization: set x0 ∈ X, z0 ∈ Rd, and p0 ∈ Null(A>); Step sizes η, αx, and αz.
1:	for k = 0, 1, . . . do
2:	Update p as pk+1 = pk + η(Axk - b).
3:	Choose i(k) from {1, ..., N} uniformly at random;
4:	Update x and z through coordinate steps:
xk+1 = arg min <S(k)f (Xk) + σ(xk -Zk)i(k),x“k)〉+ gi(k) (xi(k))
+ (pk+1+Y(Axk - b), Ai(k)xi(k)) +—^ao—',
zk+1 = arg min (σ(zk - xk)i(k), Zi(k))+ 1— ∣∣z - zk∣∣* 1 2 3.
z∈Rd	2αz
5:	end for
Proof. We first prove that part (a) implies part (b). Since (X,p*) is a stationary point of (P), We
have 0 ∈ ∂L(x*,p*). Upon plugging Z = x* in the L+(x*,z,p*) shows that 0 ∈ ∂L+(x*,x*,p*).
The inverse direction is proved by a similar argument.	□
The auxiliary problem can be viewed as a quadratically regularized version of the original problem
(P) with an additional variable Z. Recall that f has Lf -Lipschitz gradient andg is ρg-weakly convex.
We have the following benign properties of the auxiliary problem (P+), whose derivations can be
found in Appendix A.2:
(1) f (x) + 2 ∣∣x 一 z∣2 is continuously differentiable in (x, Z) with Lipschitz continuous gradient;
(2) F +(x,z) = f (x) + g(x) + 2 ∣∣x 一 z∣2 is weakly convex in (x,z) with parameter Lf + Pg;
(3) F+ (x, Z) is bi-strongly convex, i.e., F+ (x, Z) is strongly convex in x for every fixed Z and
F+ (x, Z) is strongly convex in Z for every fixed x.
2.2 Nonconvex Randomized Primal-Dual Coordinate Method
As we established in the last subsection, problem (P+) has the same set of stationary points as that
of the original problem (P) and the former has a series of benign properties. These observations
motivate us to design algorithm for solving the original problem by targeting on (P+). Similar
to standard primal-dual methods, our algorithm for solving problem (P+) builds on the following
augmented Lagrangian of (P+):
L+(x,z,p) = f(x) + g(x) + 2Ilx - Z∣2 + hp,Ax - bi + 2∣∣Ax - b∣2.	(7)
With the augmented Lagrangian, our algorithm is designed by performing an APP-AL update over
(x, Z, p) at each iteration. Furthermore, the updates for variables x and Z are achieved through
randomized coordinate steps. We depict our algorithmic procedures—i.e., the N-RPDC method—
in Algorithm 1.
The proximal term D(x, xk) used in N-RPDC is the so-called Bregman distance function. Let βK
be the strong convexity parameter of the Bregman distance function D(x, xk). It is safe for the
readers to think D(x, xk) = 2 ∣∣x - xk ∣∣2 for now. We put the more general choices of D(x, xk) in
Appendix A.3.
It is worth mentioning that the subproblem for updating xk+1 in N-RPDC has a closed form solution
once the Bregman distance function D(x, xk) = 2 ∣∣x - xk∣2 and g%(xi) is SCAD, MCP, quadratic,
or 'ν-norms with V ∈ {1, 2, ∞}.
Remark 2.1 (computational complexity). It is claimed in (Nesterov, 2012; Wright, 2015) that a co-
ordinate method can have a much lower computational complexity than its full counterpart. Let us
take the second motivating application (i.e., nonsmooth nonconvex constrained Lasso, problem (3)
in Application 2) in Section 1 as an illustrative example to show that the computational complexity
of one iteration of N-RPDC with N = d can be much cheaper than the full primal-dual counterpart
(ADMM-type or APP-type method). Note that in this case, each Xi is a box constraint on the coordi-
5
Under review as a conference paper at ICLR 2022
nate x》Suppose We set D(χ, Xk) = 2||x - Xk ∣∣2. Then, the update of wk+1 = (χk+1,zk+1,pk+1)
COnsists of: 1) pk+1 = Pk + ηsk . 2) Xk(+1 = PrOXax,iχi(k) +φ (Xk(k) - αx((Ai(k))> rk + σ(Xk -
Zk)i(k) + (Bi(k))> (pk+1 + Ysk))) and Xk+i1(k) = Xk=i(k), where Iχi(k) is the indicator function
of Xi(k). 3) zik(+k)1 = zik(k) - αzσ(zk - Xk)i(k) With zjk6=+i1(k) = zjk6=i(k). We also update the residual
vectors as rk+1 = rk +Ai(k)(Xik(+k)1 -Xik(k)) and sk+1 = sk +Bi(k)(Xik(+k)1 -Xik(k)). If we precompute
r0 = AX0 - b and s0 = BX0 - c, and the proximal mapping proXαx,IX +φ admits a closed form
solution (which is the case when φ is the SCAD or MCP penalty), then the above updates have a
total complexity O(maX{m, n}). Therefore, the complexity of N-RPDC is d× cheaper than its full
counterpart (which has a computational complexity of O(d maX{m, n}) in one iteration).
3 S tationarity Measure, Lyapunov Function, and Uniform Metric
Subregularity
In this section, we will establish a series of important results, which serve as the foundations for our
later convergence analysis.
3.1	The S tationarity Measure
Consider the current iterate wk. If wk+1 is updated by the full primal-dual counterpart to N-RPDC
(i.e., N = 1 in Algorithm 1), then the step length |wk - wk+1 | is a natural stationarity measure
due to the facts that dist(0, ∂L+(wk+1)) = O(|wk - wk+1 |) with some step sizes hidden in
the big-O and limk→+∞ |wk - wk+1 | = 0 can be established by an easy argument based on an
appropriate Lyapunov function. However, if wk+1 is updated by our N-RPDC with N 6= 1, which
is a stochastic algorithm that only updates part of wk to wk+1, it is no longer obvious to show
limk→+∞ |wk - wk+1 | = 0. Therefore, due to the stochastic nature of N-RPDC, the step length
|wk - wk+1 | cannot play the role of stationarity measure. This observation motivates us to work
with a surrogate stationarity measure. Towards that end, for a given point w = (X, z, p), we define
the reference point of w as T (w) := (Tx (w), Tz(w), Tp(w)) and
'Tp(W) = p + η(AX — b);
Tx(W) = arg min hVf (x) + σ(X - z),yi + g(y) + hTp(w) + Y(AX - b), Ayi + D(OyXX;
Tz(W) = arg min hσ(z - X),yi + ɪ∣∣y - z∣2.
y∈Rd	2αz
(8)
One possible way to understanding the reference point is to let Wk = (Xk, zk,pk), then T(Wk) is
nothing else other than the next iterate Wk+1 if the full primal-dual method is utilized, i.e., N =1 in
N-RPDC. However, it is worthing mentioning in the case where N >1, we do not need to compute
the reference point T(Wk) explicitly. Instead, N-RPDC only use a block coordinate for updating.
The next proposition provides us a valid stationary measure, which is crucial for our later conver-
gence analysis.
Proposition 3.1 (surrogate stationarity measure). For all W = (X, z,p) ∈ X × Rd × Rn, we have
dist(0,∂L+(T(W))) ≤ CkW - T(W)Il =: Φ(w).
Here, T (W) is the reference point of W defined in (8) and C := √3max {LK + Lf + 2σ + ∣∣A∣, O- +
2σ, YkAk+ 1} is a positive constant.
Remark 3.1 (interpretation of the surrogate stationarity measure). The mapping Φ(W) will serve
as the surrogate stationarity measure for analyzing the convergence of N-RPDC. Particularly, it is
clear that dist 0, ∂Lγ+ (T (W)) = 0 when Φ(W) = 0. In order to characterize the iteration com-
plexity of N-RPDC, we focusing on achieving Φ(W) ≤ ε since it implies that W is ε-close to the
reference point T(W) which is an ε-stationary point (here, T(W) is defined to be ε-stationary since
6
Under review as a conference paper at ICLR 2022
dist (0, ∂L+ (T (w))) ≤ ε). These interpretations clarify our surrogate stationarity measure. Note
that similar notion of surrogate stationarity measure dates back to Ekeland’s variational principle
(Ekeland, 1974) and also appeared in recent advances of the analysis of the subgradient-type meth-
ods; see, e.g., (Davis & Drusvyatskiy, 2019; Li et al., 2021). It is interesting to note that the main
difference between our approach and the existing ones lies in the choice of the reference point. Let
us take the approach used in (Davis & Drusvyatskiy, 2019) for analyzing subgradient-type method
as an example. The authors define the proximal mapping of the objective function as the reference
point, while we utilize the update of the full primal-dual method (i.e., N = 1 in Algorithm 1) as
the reference point. Such a main difference comes from different sources of difficulties. The main
difficulty for the analysis of subgradient-type method in (Davis & Drusvyatskiy, 2019) comes from
the fact that this algorithm is intrinsically not a descent method on the objective function, while our
main difficulty is due to the stochastic nature of N-RPDC.
3.2	The Lyapunov Function and One-Step Analysis
Recall that the function F+ defined in (P+) is strongly convex in x for every fixed z (see our analysis
in Section 2.1). Let us define
ν(z) =	min	F+(x, z) and x(z) = arg min	F+(x, z).	(9)
x∈X, Ax-b=0	x∈X, Ax-b=0
In addition, we also define
ψγ(z,p) = min Lγ+ (x, z,p) and x(z, p) = arg min Lγ+(x, z,p),	(10)
x∈X	γ	x∈X	γ
where Lγ+ is defined in (7).
Next, mimicking the construction of the Lyapunov function used in (Zhang & Luo, 2020b), we
define
Λ(w) = Lγ+(w) + 2 (ν(z) - ψγ (z, p))	(11)
as the Lyapunov function for our convergence analysis. The first part of Λ(∙) is augmented La-
grangian Lγ+ (w) of (P+). The second part is dual gap of (P+) with fixed z. Additionally, by
the definition of ψγ (z, p), we have that Lγ+ (w) ≥ ψγ (z, p). By the strong duality, we have
V(Z) = max ψγ (z,p). Therefore, we have the fact that Λ(w) ≥ V(Z) ≥ F *.
p
Standard analysis for primal-dual methods develops certain descent property on the Lyapunov func-
tion. Before that, we first present a preliminary one-step analysis for N-RPDC. Note that indices
i(k), k = 0, 1, 2, . . . in N-RPDC are random variables. Thus, it generates a random output. We use
Fk := {i(0), i(1),..., i(k)}
to denote the filtration generated by the random variables i(0), i(1), . . . , i(k). Clearly, we have
Fk ⊂ Fk+1. We use Ei(k) and EFk to denote the expectations taken over the random variable i(k)
and the filtration Fk, respectively. The following lemma provides estimations for V(Z) - ψγ(Z,p)
and Lγ+(w) after one-step execution of N-RPDC.
Lemma 3.1 (one-step analysis of N-RPDC). Suppose 0 < αx ≤ βK/ Lf + 2σ + γkAk2 + 5 and
0 < η < 1/ 2N∣∣Ak2 ( Lf+σ-LkA-P + αx + l)	. For all k ≥ 0, we have
Λ(wk) - Ei(k)Λ(wk+1)
≥ ɪkxk	- Tx(Wk)k2	+ ɪ	[ɪ	- 2σ —	σ--------3σ2- 1	∣zk - Tz(Wk)k2	(12)
N	N	αz	λ	σ - Lf - ρg
+ η∣Ax(Zk, Tp(Wk)) -b∣2 - σλ∣x(Zk, Tp(Wk)) -x(Zk)∣2.
where σ is defined in (P+) and λ is any positive constant.
Toward establishing certain decent property on the Lyapunov function in terms of the stationar-
ity measure Φ(Wk), we have to connect all the terms on the right-hand side of the inequality of
Lemma 3.1 to Φ2(Wk) = ∣Wk-T (Wk)∣2. Thus, it remains to upper bound ∣x(Zk, Tp(Wk))-x(Zk)∣
by ∣Ax(Zk, Tp(Wk)) - b∣, which needs the so-called uniform metric subregularity property.
7
Under review as a conference paper at ICLR 2022
∂Lzγ(x, p)
(13)
0
Ax(z, p) -
3.3 Uniform Metric Subregularity and its Consequence
Let us define the parameter set-valued mapping
Vf (x) + ∂g(x) + σ(x - Z) + NX(x) + A>(p + Y(Ax - b))
Ax - b
as the subdifferential of Lγ+ with respect to (x, p) with fixed z. Clearly, we have ∂Lzγ(x(z,p),p) =
b due to the optimality of x(z,p) in (10). To proceed, we introduce the notion of
local uniform metric subregularity property for the parameter set-valued mapping.
Definition 3.1 (local uniform metric subregularity, cf. (Kruger & Duy Cuong, 2021)). Let Hz (u) :
U ⇒ V be a set-valued mapping. We call that Hz (u) satisfies the locally metric subregularity
property with parameter (δ, κ) uniformly over all z ∈ Z if
dist(u, H-1(0)) ≤ K ∙dist(0, Hz(U)),	∀z ∈ Z,	(14)
for any u ∈ {u0 ∈ U : dist(0, Hz(u0)) ≤ δ}, where δ, κ > 0 are uniform constants.
This local error bound condition gives the following result.
Lemma 3.2. Suppose the set-valued mapping ∂Lzγ(x,p) defined in (13) satisfies the local uniform
metric subregularity property, then there is κ > 0 (independent of z) such that
kx(z, p) - x(z)k ≤ κkAx(z,p) - bk, whenever kAx(z,p) - bk ≤ δ.
Equipped with this lemma, we are able to upper bound kx(zk, Tp(wk)) - x(zk)k by
kAx(zk, Tp(wk)) - bk locally in order to establish the descent property on the Lyapunov func-
tion. We will discuss this descent property in the next section. Now, let us provide the conditions on
problem (P) in order to ensure the local uniform metric subregularity assumed in Lemma 3.2.
Lemma 3.3 (sufficient condition for local uniform metric subregularity). Consider problem (P).
Suppose that Vf (x) is piecewise affine and ∂g(x) is polyhedral multifunction andX is a polyhedral
set, then ∂Lzγ (x, p) defined in (13) satisfies the local uniform metric subregularity property.
Remark 3.2 (generality of local uniform metric subregularity). This sufficient condition ensures
that ∂Lzγ (x, p) must satisfy the local uniform metric subregularity property whenever f is quadratic
(can be nonconvex, i.e., its Hessian is not required to be positive semidefinite), g can be written as
k ∙ kι 一 Pgk ∙ k2, '2-norm, '∞-norm structure, and X is a polyhedron, which covers all the two
motivating applications listed in Section 1.
Remark 3.3 (comparison to the error bound condition utilized in (Zhang & Luo, 2020b)). Recall
that the work (Zhang & Luo, 2020b) considers problem (P) with g ≡ 0. They use the so-called dual
error bound condition (see (Hong & Luo, 2017)) instead of the local uniform metric subregularity
property. However, we argue that it is far form obvious how to extend their technique to cover
our nonsmooth nonconvex case, as it is not clear whether one can establish the dual error bound
condition if g is not null.
Remark 3.4 (further comments on the surrogate stationarity measure Φ(w) defined in Proposi-
tion 3.1). It is interesting that the sufficient condition in Lemma 3.3 also ensures that ∂L+ (w)
satisfies the local metric subregularity when Φ(w) ≤ ε with sufficient small ε (Robinson, 1981),
i.e., dist(w, W) ≤ κ0 dist(0, ∂L+(w)) for some κ0 > 0, where W is the set of stationary points of
(P+). Then, We have dist(w, W) ≤ (1/c + κ0)ε which follows from dist(T(w), W) ≤ κ0ε (due to
Φ(w) ≤ ε) and the triangle inequality.
4 Convergence and Iteration Complexity of N-RPDC
Equipped with all the machineries developed in Section 3, we are now ready to establish the conver-
gence properties of N-RPDC.
4.1	Descent property on the Lyapnov Function
Upon invoking Lemma 3.2 in Lemma 3.1, we can derive the descent property in expectation on the
Lyapnov Function.
8
Under review as a conference paper at ICLR 2022
Lemma 4.1 (expected sufficient decrease for Λk). Suppose that Assumptions of Lemma 3.1 hold.
If the set-valued mapping ∂Lzγ (x, p) defined in (13) satisfies the local uniform metric subregularity
property (see Definition 3.1). Let λ = min{ 2σMM⅞+1, σκη+Γ }，the parameter az satisfy 0 < αz <
1/ [2σ + λ + σ-Lf-ρg + 1] ,and cl =min{1, 2η(σκN+2) + 1 ). Then，we have
Λ(wk) - Ei(k)Λ(wk+1) ≥ Nkwk - T(wk)k2.
4.2	Almost Sure Convergence Results and Expected Iteration Complexity
In this subsection, we establish the almost sure (i.e., with probability 1) convergence results for
N-RPDC.
Theorem 4.1 (almost sure convergence result). Under the setting of Lemma 4.1, then
(a)	limk→+∞ Φ(wk) = 0 almost surely.
(b)	The sequence {wk} is almost surely bounded.
(c)	Any cluster point of the sequence {wk} is almost surely a stationary point of (P).
In addition, we can also provide the almost surely asymptotic rate of convergence in the sense of
limit inferior for our N-RPDC method.
Theorem 4.2 (almost sure O(1∕√k) convergence rate). Under the setting of Lemma 4.1, we have
liminf √k +1 ∙ Φ(wk) = 0 almost surely
k→+∞
i.e., the asymptotic O(1∕√k) convergence rate holds almost surely.
The almost sure convergence rate established above is of asymptotic nature, which concerns the
behavior of N-RPDC when k → ∞. Next, we also provide the iteration complexity results of
N-RPDC in expectation.
Theorem 4.3 (expected iteration complexity). Under the setting of Lemma 4.1. The sequence {wk}
is generated by N-RPDC. Then
0m≤kin≤tEFtΦ(wk)≤
Consequently, to achieve EFt Φ(w^) ≤ ε for ,
Nc2(Λ(w0) - F*)
Nc2(Λ(w0)-F *)
cιε2
number of iterations.
c1 (t + 1)
some 0 ≤ t ≤ t, N-RPDC needs at most t
Remark 4.1. Note that the closely related work (Zhang & Luo, 2020b) provides a similar itera-
tion complexity result (i.e., O(ε2 )) if g ≡ 0. By contrast, our results can deal with the additional
nonsmooth nonconvex term g. In addition, we also provide the almost sure convergence results,
complementing the results in (Zhang & Luo, 2020b). Let us also emphasize that the main moti-
vation of our algorithm lies in that N-RPDC is a randomized coordinate method, which is more
suitable for modern large-scale problems.
Remark 4.2. The recent work (Zhang & Luo, 2020a) establishes a global dual error bound condi-
tion, which allows them to avoid the compactness assumption on X. Due to the existence of g, we
utilize the local uniform metric subregularity rather than this dual error bound condition. To further
relax the compactness assumption on X in problem (P), one possible direction is to establish a global
version of the utilized local uniform metric subregularity. We leave this direction as a future work.
5 Numerical Experiments
Because of the limitation of space, we put the numerical experiments in Appendix A.1. We can
observe that N-RPDC with larger N slightly outperforms that with smaller N in terms of conver-
gence speed. We also compared our N-RPDC to the algorithm proposed in (Zhang & Luo, 2020b);
see Figure 1. We can observe that N-RPDC slightly outperforms their algorithm. Note that the
main motivation of N-RPDC is that it is more suitable for modern large-scale problems due to its
reasonably fast convergence speed and low computational complexity at each iteration.
9
Under review as a conference paper at ICLR 2022
References
Ahmet Alacaoglu, Olivier Fercoq, and Volkan Cevher. Random extrapolation for primal-dual coor-
dinate descent. In International conference on machine learning, pp. 191-201. PMLR, 2020.
D.P. Bertsekas. Convexification procedures and decomposition methods for nonconvex optimization
problems. Journal of Optimization Theory and Applications, 29(2):169-197, 1979.
Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal
margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory,
pp. 144-152. ACM, 1992.
Radu Ioan Bot and Dang-Khoa Nguyen. The proximal alternating direction method of multipliers
in the nonconvex setting: convergence analysis and rates. Mathematics of Operations Research,
45(2):682-712, 2020.
P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with
applications to biological feature selection. The annals of applied statistics, 5(1):232, 2011.
P. Breheny and J. Huang. Group descent algorithms for nonconvex penalized linear and logistic
regression models with grouped predictors. Statistics and computing, 25(2):173-187, 2015.
Pierre Carpentier, Guy Cohen, Jean-Philippe Chancelier, and Michel De Lara. Stochastic multi-
stage optimization. At the Crossroads between Discrete Time Stochastic Control and Stochastic
Programming Springer-Verlag, Berlin, 2015.
Guy Cohen and Daoli Zhu. Decomposition and coordination methods in large scale optimization
problems: The nondifferentiable case and the use of augmented lagrangians. Adv. in Large Scale
Systems, 1:203-266, 1984.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273-297,
1995.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1):207-239, 2019.
D. DeCoste and B. Scholkopf. Training invariant support vector machines. Machine learning, 46
(1):161-190, 2002.
Z. Deng, M. C. Yue, and A. M. C. So. An efficient augmented lagrangian-based method for linear
equality-constrained lasso. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 5760-5764. IEEE, 2020, May.
Marie Duflo. Random iterative models, volume 34. Springer Science & Business Media, 2013.
I. Ekeland. On the variational principle. Journal of Mathematical Analysis and Applications, 47(2):
324-353, 1974.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American statistical Association, 96(456):1348-1360, 2001.
Olivier Fercoq and Pascal Bianchi. A coordinate-descent primal-dual algorithm with large step size
and possibly nonseparable functions. SIAM Journal on Optimization, 29(1):100-134, 2019.
B. R. Gaines, J. Kim, and H. Zhou. Algorithms for fitting the constrained lasso. Journal of Compu-
tational and Graphical Statistics, 27(4):861-871, 2018.
Xiang Gao, Yang-Yang Xu, and Shu-Zhong Zhang. Randomized primal-dual proximal block coor-
dinate updates. Journal of the Operations Research Society of China, 7(2):205-250, 2019.
B. Haasdonk and D. Keysers. Tangent distance kernels for support vector machines. In Object
recognition supported by user interaction for service robots, volume 2, pp. 864-868. IEEE, 2002,
August.
10
Under review as a conference paper at ICLR 2022
M. Hong and Z. Q. Luo. On the linear convergence of the alternating direction method of multipliers.
Mathematical Programming, 162(1-2):165-199, 2017.
G.	M. James, C. Paulson, and P. Rusmevichientong. Penalized and constrained regression. Unpub-
lished manuscript, 2013.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
A. Y. Kruger and N. Duy Cuong. Uniform regularity of set-valued maPPings and stability of imPlicit
multifunctions. Journal of Nonsmooth Analysis and Optimization, 2, 2021.
Puya Latafat, Nikolaos M Freris, and Panagiotis Patrinos. A neW randomized block-coordinate
Primal-dual Proximal algorithm for distributed oPtimization. IEEE Transactions on Automatic
Control, 64(10):4050-4065, 2019.
Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, and Anthony Man-Cho So. Weakly
convex oPtimization over stiefel manifold using riemannian subgradient-tyPe methods. SIAM
Journal on Optimization, 31(3):1605-1634, 2021.
H.	T. Lin and C. J. Lin. A study on sigmoid kernels for svm and the training of non-Psd kernels by
smo-tyPe methods. Unpublished manuscript, 2003.
Ji Liu and StePhen J Wright. Asynchronous stochastic coordinate descent: Parallelism and conver-
gence ProPerties. SIAM Journal on Optimization, 25(1):351-376, 2015.
Ji Liu, Steve Wright, Christopher Re, Victor Bittorf, and Srikrishna Sridhar. An asynchronous par-
allel stochastic coordinate descent algorithm. In International Conference on Machine Learning,
pp. 469-477, 2014.
Zhaosong Lu and Lin Xiao. On the complexity analysis of randomized block-coordinate descent
methods. Mathematical Programming, 152(1-2):615-642, 2015.
Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341-362, 2012.
Yu Nesterov. Subgradient methods for huge-scale optimization problems. Mathematical Program-
ming, 146(1-2):275-297, 2014.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Andrei Patrascu and Ion Necoara. Efficient random coordinate descent algorithms for large-scale
structured nonconvex optimization. Journal of Global Optimization, 61(1):19-46, 2015.
A. Rakotomamonjy, G. Gasso, and J. Salmon. Screening rules for lasso With non-convex sparse
regularizers. In International Conference on Machine Learning, pp. 5341-5350. PMLR, 2019.
Peter Richtarik and Martin Takac. Iteration complexity of randomized block-coordinate descent
methods for minimizing a composite function. Mathematical Programming, 144(1-2):1-38, 2014.
Herbert Robbins and David Siegmund. A convergence theorem for non negative almost super-
martingales and some applications. In Optimizing methods in statistics, pp. 233-257. Elsevier,
1971.
S. M. Robinson. Some continuity properties of polyhedral multifunctions. Mathematical Program-
ming at Oberwolfach, pp. 206-214, 1981.
Jean-Philippe Vial. Strong and Weak convexity of sets and functions. Mathematics of Operations
Research, 8(2):231-259, 1983.
H. Wang, A. Banerjee, and Z. Q. Luo. Parallel direction method of multipliers. arXiv preprint
arXiv:1406.4064., 2014.
11
Under review as a conference paper at ICLR 2022
J. H. Won, J. Xu, and K. Lange. Projection onto minkowski sums with application to constrained
learning. In International Conference on Machine Learning, pp. 3642-3651. PMLR, 2019.
StephenJWright. Coordinate descent algorithms. Mathematical Programming, 151(1):3-34, 2015.
Yangyang Xu and Shuzhong Zhang. Accelerated primal-dual proximal block coordinate updating
methods for constrained convex optimization. Computational Optimization and Applications, 70
(1):91-128, 2018.
Cun-Hui Zhang et al. Nearly unbiased variable selection under minimax concave penalty. The
Annals of statistics, 38(2):894-942, 2010.
J. Zhang and Z. Luo. A global dual error bound and its application to the analysis of linearly
constrained nonconvex optimization. arXiv preprint arXiv:2006.16440, 2020a.
J. Zhang and Z. Q. Luo. A proximal alternating direction method of multiplier for linearly con-
strained nonconvex minimization. SIAM journal on Optimization, 30(3):2272-2302, 2020b.
L. Zhao and D. Zhu. First-order primal-dual method for nonlinear convex cone programs. arXiv
preprint arXiv:1801.00261v5, 2019.
L. Zhao and D. Zhu. On iteration complexity of a first-order primal-dual method for nonlinear
convex cone programming. Journal ofthe Operations Research Society of China, pp. 1-35, 2021.
D. Zhu, L. Zhao, and S. Zhang. A first-order primal-dual method for nonconvex constrained opti-
mization based on the augmented lagrangian. arXiv preprint arXiv:2007.12219, 2020.
Daoli Zhu and Lei Zhao. Linear convergence of randomized primal-dual coordinate method for
large-scale linear constrained convex programming. In International Conference on Machine
Learning, pp. 11619-11628. PMLR, 2020.
A Appendix
The Appendix is organized as follows. Subsection A.1 discusses implementation details for N-
RPDC and presents numerical experiments on non-PSD kernel SVM and sum to zero constrained
LASSO problem with SCAD penalty. Some properties of the auxiliary problem (P+) are given in
subsection A.2. Some examples of Bregman distance function are given in A.3. Subsection A.4
shows the proof of Proposition 3.1. Subsection A.5 provides the proof of Lemma 3.1. Proof of
Lemma 3.2 is given in subsection A.6. Subsection A.7 provides the proof of Lemma 3.3. Sub-
section A.8 shows the proof of Lemma 4.1. Subsection A.9 provides the proof of Theorem 4.1.
Subsection A.10 gives the proof of Theorem 4.2. The proof of Theorem 4.3 is given in subsec-
tion A.11.
A.1 Numerical Examples
This subsection discusses experiments conducted using MATLAB R2020a on a personal computer
with Intel Core i5-6200U CPU (2.40GHz) and 8.00 GB RAM.
A.1.1 Non-PSD kernel support vector machine problem
Consider the non-PSD kernel support vector machine problem,
min	1 x> Qx — 1>x
x∈[0,c]d 2	d ,
s.t.	y>x = 0
where x ∈ Rd are the decision variables, and Q ∈ Rd×d is a matrix, possibly non-PSD. Let Q =
(Q>,Q>, ∙∙∙ , QN)> ∈ Rd×d be an appropriate partition of matrix Q and Qi be an d% X d matrix.
Lγ+ of Non-PSD kernel SVM can be written as
L+(w) = L+(χ,z,p) = 1 χ>Qχ — ι>χ + 2 l∣χ — zk2 + hp,yτχi + γ l∣yτχk2.
12
Under review as a conference paper at ICLR 2022
heart scale
epoch
Figure 1: Evolution of min0≤k≤t Φ(wk) versus epoch counts for several different choices of N (i.e.,
the number of blocks in N-RPDC), where nADMM refers to the algorithm proposed in (Zhang &
Luo, 2020b). Left: dataset heart_scale. Right: dataset ionosphere. The results are obtained
by averaging 50 independent trials.
epoch
Thanks of Proposition 3.3, for problem Non-PSD kernel SVM, ∂Lzγ is locally metric subregular in
(x,z,p) uniformly in Z at point 0. N-RPDC with D(x,xk) = 2∣∣x 一 xk∣∣2 for non-PSD kernel
support vector machine problem is
pk+1 — Pk + ηy>xk;
Choose i(k) from {1, 2, . . . , N} with equal probability
xk+1 - min AQi(k)Xk + σ(xk 一 zk)i(k) - 14“%)+ y“k)(pk+1 + YyTxk), xi(k)i
d
+2⅛kx -xk k2 ；
zk+1 一 Z∈Rdhσ(Zk-Xk )i(k),zi(k)i + 20zkz -zkk2.
Thus, the primal subproblem of N-RPDC has the closed form
xk+1 = Π d
xi(k) = Π[0,c]di(k)
k+1	k
xj6=i(k) = xj6=i(k);
k
xik(k) 一 αx
xk + σ(xk 一 Zk)i(k) 一 1ni(k) + (pk + γy>xk)yi(k)
and
( Zik(+k)1 = Zik(k) 一 αzσ(Zk 一 xk)i(k),
Zj6=+i(k) = Zjk6=i(k) .
We used LIBSVM dataset heart_scale and ionosphere in the experiment. Q was generated
using the sigmoid kernel (Lin & Lin, 2003), and we selected c = 1. We compared two algorithms:
nonconvex ADMM in (Zhang & Luo, 2020b) (nADMM), and N-RPDC from this paper on these
two datasets.
For N-RPDC algorithm, We partitioned the variables N = 10,90 blocks for heart_scale and
N = 10, 70 blocks for ionosphere.
In order to verify the theoretical results of this paper, we compute the sequence of reference point
{T (wk)} along with the sequence {wk} generated by N-RPDC. In the real world applications of
N-RPDC, the reference point sequence is not necessary to compute. In Figure 1, the left graph
show the number of blocks and min°≤k≤t Φ(wk) with respect to epoch count for heart_scale;
the right graph show the number of blocks and min0≤k≤t Φ(wk) with respect to epoch count for
ionosphere. For both datasets, min0≤k≤t Φ(wk) of each epoch of N-RPDC is the average value
of 50 times result. Moreover, we can draw the conclusion that N-RPDC is slightly outperforms
nADMM and larger N is slightly outperforms small N.
13
Under review as a conference paper at ICLR 2022
A.1.2 Sum to zero constrained LASSO problem with SCAD penalty
Consider the sum to zero constrained LASSO problem with SCAD penalty:
(CLASSO-SCAD) min
x∈[-1,1]d
s.t.
n
2 kAx - bk2 + P φ(Xi)
i=1
1n> x = 0;
where x ∈ Rd is the decision variables and A ∈ Rn×d is the regressors matrix. Let A =
(Aι,A2, ∙ ∙ ∙ ,an ) ∈ Rn×d be an appropriate partition of matrix A and Ai be an n X d% matrix.
b ∈ Rn is the response vector. 1d is an d-dimensional vector of 1s. φ(∙) is the SCAD penalty:
(|t|
Φ(∣t∣) = ∖ -¾2-1t)τ
[1+θ
Lγ+ of (CLASSO-SCAD) can be written as
|t| ≤ 1
1 < |t| ≤ θ , t ∈ R
|t| > θ.
1	nσ	γ
L+(w) = LY(x,z,p) = 2 IIAx - bk2 + Eφ(Xi) + 2 kχ - zk2 + hp, 1>xi + 2 l∣1>xk2.
i=1
Thanks of Proposition 3.3, for problem (CLASSO-SCAD), ∂Lzγ is locally metric subregular in
(x, z,p) uniformly in Z at point 0. We take D(x, Xk) = 2 ∣∣x 一 xk∣2 and construct N-RPDC al-
gorithm for (CLASSO-SCAD) as following:
pk+1 - Pk + η(i>χk),
Choose i(k) from {1, 2, . . . , N} with equal probability
xk + 1 — eminι ]dhA>k)(Axk - b)+ σ(Xk -Zk )i(k) + 1di (qk ), xi(k)i
Pij(=k1) dj	1
+ X	φ(Xi) + 2— kx - xkk2;
2α
i=1+Pij(=k1)-1 dj
zk+1 — minhσ(Zk - xk)i(k), zi(k)i + π IlZ - zkk2;
z∈Rd	2αz
where qk = pk+1 + γ(1d>xk). Thus, the primal subproblem of RPDC has the closed form
xk+1
xi(k)
k+1
xj 6=i(k) = x
Π
Π
Π
k
[-1,1]
[-1,1]
di(k){sign(Zk) Θ max(0, |Zk| - 〃，*%))} |Zk| ≤ 1 + μ
di(k) { θ-l-μ
[-1,1]di(k) {ζk}
j6=i(k);
[(θ - 1)Zk - μθsign(Zk)]}	1+ μ< |Zk | ≤ θ
IZk| >θ
with Zk = xk(k) - αχ [A>k)(Axk - b) + σ(xk - Zk)i(k) + qk 1^^)] and μ = αχ.
( Zik(+k)1 = Zik(k) - αzσ(Zk - xk)i(k);
Zj6=+i1(k) = Zjk6=i(k) .
In this experiment, the elements of A ∈ Rn×d are selected i.i.d. from a Gaussian N (0, 1) distribu-
tion. To construct a sparse true solution x* ∈ Rd, given the dimension d and sparsity s, We select
S entries of x* at random to be nonzero and N(0,1) normally distributed, and set the rest to zero.
The measurement vector b ∈ Rn is obtained by b = Ax* + δ with the elements of the noise vector
δ ∈ Rn are i.i.d. N(0, 0.001). We choose θ = 2.3 with n = 360, d = 1280, s = 8 for the first
dataset and n = 720, d = 2560, s = 16 for the second dataset.
We partitioned the variables N = 10, 40, 80 blocks. Thus di = 128, 32, 16 for the first dataset and
di = 256, 64, 32 for the second dataset.
In order to verify the theoretical results of this paper, we compute the sequence of reference point
{T (wk)} along with the sequence {wk} generated by N-RPDC. In the real world applications of
N-RPDC, the reference point sequence is not necessary to compute. In Figure 2, the left graph
14
Under review as a conference paper at ICLR 2022
epoch
Figure 2: Evolution of min0≤k≤t Φ(wk) versus epoch counts for several different choices of N (i.e.,
the number of blocks in N-RPDC) on synthetic dataset. Left: n = 360, d = 1280, s = 8. Right:
n = 720, d = 2560, s = 16. The results are obtained by averaging 50 independent trials.
epoch
show the number of blocks and min Φ(wk) with respect to epoch count for the first dataset; the
right graph show the number of blocks and min Φ(wk) with respect to epoch count for the second
0≤k≤t
dataset. For both datasets, min Φ(wk) for each epoch of N-RPDC is the average value of 50 times
result. Additionally, we can draw the conclusion that larger N is slightly outperforms small N .
A.2 Some properties of the auxiliary problem P+
The auxiliary problem can be viewed as a quadratically regularized version of the original problem
(P) with an additional variable z. Recall that f has Lf -Lipschitz gradient and g is ρg-weakly convex.
We have the following benign properties of the auxiliary problem (P+):
(1)	f (x) + 2 ∣∣x - zk2 is continuously differentiable in (x, Z) with LiPschitz continuous gradient;
(2)	F +(x,z) = f (x) + g(x) + 2 ∣∣x - z∣2 is weakly convex in (x,z) with parameter Lf + Pg;
(3)	F+ (x, z) is bi-strongly convex, i.e., F+ (x, z) is strongly convex in x for every fixed z and
F+ (x, z) is strongly convex in z for every fixed x.
The first property is trivially true. To see the second property, we note that the Lf -Lipschitz gradient
property of f implies that f(x) ≥ f (y) + (s,x - y)- L2f ∣∣χ 一 y∣2,∀x,y ∈ Rd (Nesterov, 2003,
Lemma 1.2.3). This, together with (Vial, 1983, Proposition 4.8), establishes that f is Lf -weakly
convex2. By (Vial, 1983, Proposition 4.1), we have that f + g is weakly convex with parameter
Lf + Pg. We have shown the second property y recognizing that (x, Z) → 2||x - z∣2 is a convex
mapping. The third property is a direct consequence of the definition of weak convexity, which
yields that f (x) + g(x) + 2Ilx - z∣2 is σ - (Lf + Pg) strongly convex in x.
A.3 Some examples of Bregman distance function
The proximal term D(x, xk) used in N-RPDC is the so-called Bregman distance function, which is
defined as
D(x,xk) := K(x) - K(Xk) - NK(xk),x - xk).
Here, K : Rd → R is the core function, which is βK -strongly convex and has LK -Lipschitz
continuous gradient on Rd. One popular choice of the core function is K (x) = ∖ ∣∣x∣2, which yields
D(χ, xk) = 2∣∣x - xkk2. However, more generally, the core function can also be selected as the
Q-quadratic proximal regularization term K(x) = 1 ∣∣x∣Q, where Q is a positive definite matrix.
This choice covers some second-order methods as we can set Q tobe the regularized Hessian matrix.
2Actually, this derivation verifies the fact that any continuously differentiable function with L-Lipschitz
continuous gradient is L-weakly convex.
15
Under review as a conference paper at ICLR 2022
A.4 Proof of Proposition 3.1
Proof. The optimal condition of
Tx(Wk) = argminhVf (xk) + σ(xk — Zk), Xi + g(x) + hTp(wk) + Y(Axk — b), Axi + D(x,x )
x∈X	p	αx
and Tz(Wk) = arg min <σ(zk — xk), Z + 2ɪkz — Zk k2 yields that
z∈Rd	2αz
0 ∈ Vf(xk) + ∂g(Tx(Wk)) + σ(xk — Zk) + NX(Tx(Wk)) + A>[Tp(Wk) + γ(Axk —b)]
+ ~∖VK (Tx(wk ))-VK(xk)];
αx
0 = σ(zk — xk) + L(Tz(Wk) — Zk).
αz
Let
ξx = Vf(Tx(Wk)) —Vf(xk)+ σ(Tx(wk) — Tz (wk)) — σ(xk — ζk) — γA>(Axk — b)
--1[VK (Tx(Wk))—VK(xk)]
αx
= Vf (Tx(Wk)) — Vf (xk) + σ(Tx(Wk) — Tz(Wk)) — σ(xk — zk) — γA>(TD(Wk)- Pk)
η
--1[VK (Tx(Wk)) — VK(xk)];	(15)
αx
ξz = σ(Tz(Wk) — Tx(Wk)) — σ(zk — xk) — L(Tz(Wk)- zk);	(16)
αz
and
ξp = 1(TB(Wk) — Pk)+ A(Tx(Wk) — xk).	(17)
η
Then we have that
ξx ∈ Vf (Tx(Wk)) + ∂g(Tx(Wk)) + σ(Tx(Wk) — Tz(Wk)) + NX(Tx(Wk)) + A>Tp(Wk);
ξz = σ(Tz(Wk) — Tx(Wk));
ξp = ATx(Wk) — b;
ξx
By the expression (15-17) of ξ =	ξz	, αx , αz and η are given posi-
ξp
tive numbers and Assumptions of problem (P), there exists positive number c ≥
√3max n LaK + Lf +2σ + ∣∣Ak, O- +2σ, YkAk+1 0 and the following inequality holds
kξk≤ CkWk - T(Wk)∣.
Combining ξ ∈ ∂L+ (T(Wk)) and it follow the desired statement.	□
A.5 Proof of Lemma 3.1
In order to provide the Lemma 3.1, we need the following two propositions as preparation.
Proposition A.1 (Lipschitz properties of x(Z) and x(Z, P)). Suppose σ > Lf + ρg. Then
⑴ kx(Z) - x(z0)k ≤ σ-(Lσ +ρg ) kz - z0k ；
Qi) kx(z,P) - x(z0,P)k ≤ σ-(Lf +ρg) kz - z0k∙
16
Under review as a conference paper at ICLR 2022
Proof.⑴ Let φ(x, Z) = f(x) + g(x) + 2∣∣x 一 z∣∣2 + IX(x) With X = {x ∈ X|Ax 一 b = 0}. Since
σ > Lf + ρg, we have φ(x, z) is σ - (Lf + ρg)-strongly convex in x. Therefore
φ (x(z), z0) 一 φ (x(z0), z0)
= [φ (x(z), z0) 一 φ (x(z), z)] + [φ (x(z), z) 一 φ (x(z0), z)] + [φ (x(z0), z) 一 φ (x(z0), z0)]
=2[kx(Z)-z0k2 - kx(Z)-Zk2]+[φ (X(Z),z) -。(X(Zo),z)]
+2[kχ(z0)-z∣2 - kx(ZO)-Z0k2]
= [φ (X(Z), Z) - φ (X(Z0), Z)] + σhZ0 - Z, X(Z0) - X(Z)i
≤ -σ(Lf + P) ∣∣x(z) - x(Z0)k2 + σ(Z0 - z,x(z0) - x(Zyi
Again using the strongly convex of φ(x, Z) in x, We have that
φ(x(z),z0) 一 φ(x(Z0),Z0) ≥ σ(Lf + Pg) kx(Z)— x(Z0)k2.
Therefore
kx(Z) - x(Z0)k ≤ -----7Tσ--VkZ - Z0k.
σ - (Lf +ρg)
(ii) Let 夕(x, Z,p) = L+ (x, Z,p) + TX(x). Since σ > Lf + Pg, function 夕(x, Z,p) also is σ - (Lf +
ρg )-strongly convex in x. Therefore
Ψ (X(Z,p), Z,p) - Ψ (x(z/,P),Z',p)
= [φ(x(Z,p),Z',p) - ψ(x(Z,p),Z,p)] + [ψ(X(Z,p),Z,p) - ψ(x(Z',p),Z,p)]
+[? (x(z',p),Z,p) - Ψ (x(Z',P),Z',p)]
= 2[kx(Z,p) - Z'k2 - kx(Z,p) - Zk2] + [φ(X(Z,p),z,p) - ψ(X(Z',p)Z,p)]
+ 2[kx(Z',p) - Zk2 -kx(Z',p) - Z'k2]
= k (x(Z,p), Z,p) - φ (x(z',p), Z,p)] + σ(Z' - z, x(z',p) - x(Z,p))
≤	-σ~(Lf + Pg) kx(z,p) - x(z',p)k2 + σ(z' - z,x(z',p) - x(z,p)i
Again using the strongly convex of 夕(x, z, P) in x, we have that
Ψ (x(z,p),z',p) - φ (x(z',p),z',p) ≥ σ_(Lf + Pg) kx(z,p) - x(z',p)k2.
Therefore, kx(z,p) - x(z',p)k ≤ σ-(Lσf + ρg) kz - z'k.	□
Proposition A.2. Suppose σ > Lf + Pg. Let {wk } be generated by N-RPDC. For given wk, the
primal output (xk+1, Zk+1) are random variables, and the following assertion hold:
kxk - x(zk,τp(wk))k ≤ (Lf + σ + YkAk2 + Lx +1)kxk - Tx(Wk)k.	(18)
σ -Lf -Pg
Proof. Since x(Zk, Tp(wk)) = arg min Lγ+(x, Zk, Tp(wk)) and Lγ+(x, Z,p) is σ - Lf - Pg -strongly
convex in x, then ∀ζ ∈ ∂g(Tx (wk)) + NX(Tx(wk)) we have that
(σ - Lf - Pg)kTx(wk) - x(Zk, Tp(wk))k2
≤ hVf (Tx(wk)) + σ(Tx(wk) - Zk) + ζ, Tx(Wk) - x(zk,Tp(wk)))
+hA>[Tp(wk) + γ(ATx(wk) - b)], Tx(wk) - x(Zk, Tp(wk))i.
It follows that
(σ-Lf - Pg)kTx(Wk) - x(Zk, Tp(Wk))k
≤	kVf (Tx(Wk)) + σ(Tx(Wk) - Zk) + ζ + A>[Tp(Wk) + γ(ATx(Wk) - b)]k.
Again using the optimal condition of
17
Under review as a conference paper at ICLR 2022
Tx(Wk) = arg minhVf (xk) + σ(xk — Zk), Xi + g(x) + h^Tp(wk) + Y(Axk — b), Axi +
x∈X	p
D(x,xk)
αx
we have that
0 ∈ Vf(xk) + ∂g(Tx(wk)) + σ(xk — zk) + NX(Tx(wk)) + A>[Tp(wk) + γ(Axk —b)]
+ -[VK (Tx(wk)) — VK (xk)].
αx
Therefore
ξ ∈ Vf (Tx(wk)) + σ(Tx(wk) — zk) + ∂g(Tx(wk)) + NX(Tx(wk))
+A>[Tp(wk) + γ(ATx(wk) — b)],
with
ξ = Vf (Tx(Wk)) — Vf(xk) + σ(Tx(wk) — xk) + γA>A(Tx(wk) — xk)—古[VK(Tx(wk))—
VK(xk)].
By Assumptions on (P), we obtain that
kξk≤ (Lf + σ + γIAI2 + LK)kxk — Tx(Wk)k,
αx
and
kTx(wk) — x(zk,TP(wk))k ≤ Lf + + + YkAk2 + ax kxk — Tx(Wk)∣.
σ — Lf — Pg
Then by the triangle inequality, above inequality yields the desired result.	□
With these two propositions in hands, we can provide the technical proof of Lemma 3.1.
Proof of Lemma 3.1:
Step 1: Estimate the term Lγ+(Wk) — Ei(k)Lγ+(Wk+1 ). By the dual update of N-RPDC, we have
that
LY(xk ,zk ,pk) — L+(xk,zk,pk+1) = hpk — pk+1,Axk — bi = —1 ∣pk — pk+1k2.	(19)
γγ	η
Since xk+1 is the solution of subproblem of x in N-RPDC scheme, we have that
hVi(k) f (x ) + σ(x — z )i(k) , (x — x	)i(k) i + gi(k) (xi(k) ) — gi(k) (xi(k) )
+ hpk+1 + Y(Axk- b),Ai(k)(x — xk+1)i(k)i + ɪD(x,xk) — 1-D(xk+1,xk) ≥ 0	(20)
αx	αx
Take x = xk in (20), by the fact that gi(k) (xik(k)) — gi(k)(xik(+k)1) = g(xk) — g(xk+1)
and
hVi(k)f(xk) + σ(xk —zk)i(k)+(Ai(k))>[pk+1+Y(Axk — b)], (xk — xk+1)i(k)i
= hVf(xk) + σ(xk —zk) + A>[pk+1 + Y(Axk — b)], xk — xk+1i,
we have that
hVf(xk) + σ(xk — zk),xk — xk+1i +g(xk) — g(xk+1) + hpk+1 +Y(Axk — b), A(xk — xk+1)i
--1 D(xk+1,xk) ≥ 0
αx
It yields that
≥
≥
F+(xk, zk) — F +(xk+1, zk) + hpk+1 +Y(Axk — b), A(xk — xk+1)i
OxD(Xk+1,xk) - [fOf(Xk) - hVf(Xk),xk+1 - xki] - 2kxk -xk+1 k2
βK - αx (Lf + Q)
2αx
kxk
— xk+1k2
18
Under review as a conference paper at ICLR 2022
Since {γ(Axk — b), A(xk — xk+1)) = γ (∣∣Axk - b∣∣2 -Mxk+1 — b∣∣2 + M(Xk — xk+1)∣∣2),
therefore
L+(xk,zk,pk+1) - L+(xk+1,zk,pk+1) ≥ Bk - °x(Lf + ° + YkAk2) ∣∣χk - χk+1∣∣2.	(21)
Y	Y	2ax
Additionally, since zk+1 is the solution of subproblem of Z in N-RPDC, we have that
(σ(zk - xk)i(k), (z - zk+1)通))+ ɪhzk+1 - zk,z - zk+1)≥ 0.	(22)
Take Z = zk in (22), by the factthat {σ(zk — xk)认2,(Zk — zk+1)i(k)) = {σ(zk — xk), zk — zk+1),
we have that
{σ(zk — xk),zk — zk+1) ≥ ɪ∣∣zk - zk+1∣2.
αz
Since
(σ(zk - xk),zk - zk+1) = (σ(zk - xk+1),zk - zk+1) + (σ(xk+1 - xk),zk - zk+1)
=σ (∣xk+1 - zkk2 - ∣xk+1 - zk+1∣2 + ∣∣zk - zk+1∣2)
+ (σ(xk+1 - xk),zk - zk+1)
≤ σ (∣∣xk+1 - zkk2 - kxk+1 - zk+1k2 + ∣zk - zk+1k2)
+2 (∣xk-xk+1k2 + kzk -zk+1k2)
=σ (∣xk+1 - zkk2 - ∣xk+1 - zk+1k2) + σ∣zk - zk+1k2
+2 ∣xk-xk+1k2,
above inequality yields that
L+(xk+1,zk,pk+1) -L+(xk+1,zk+1,pk+1) = £ (∣∣xk+1 -zk∣2 - ∣xk+1 - zk+112)
≥ (ɪ -σ)∣zk - zk+1∣2 - Ikxk -xk+1k2.
αz	2
(23)
By the combination of (19), (21) and (23), we obtain that
”(xk,zk,pk) -L+ (xk+1,zk+1,pk+1)
≥ 强-(Lf + % + YkAk2)∣xk - xk+1∣2 + (ɪ - σ)∣zk - zk+1∣2 - Ikpk- /HE
2	αz	η
Take expectation with respect to i(k) on both side of above inequality, we have that
L+ (xk,zk,pk) - Ei(k)L+(xk+1, zk+1,pk+1)
≥ 管-(Lf+2σ+YkAk2)
2
-1 kpk -pk+1k2.
η
Ei(k)kxk - xk+1k2 + (ɪ - σ)Ei(k)kzk - zk+1 k2
' '	QZ	' '
By the fact that E*k)kxk-xk+1k2 = N ∣∣xk - Tx(Wk )k2, Ei(k)kzk - zk+1k2 = N ∣∣zk - TZ(Wk)k2
andpk+1 = Tp(wk), above inequality follows that
L+(xk,zk,pk) - Ei(k)L+(xk+1, zk+1,pk+1)
管-(Lf + 2σ + γkAk2) kxk
≥
-Tx(Wk )k2+N Qz- σ)kzk - TZ(Wk)∣2
-1 kpk - TP(Wk)k2
19
Under review as a conference paper at ICLR 2022
Step 2: Estimate the term V(Zk) - V(zk+1). By the Danskin's theorem, We have that Vν(Z)
σ (z - x(z)). By statement (i) of Proposition A.1, we have that
kvν (Zk )-Vv (zk+1)k ≤ (σfg+σ)kzk-zk+1k.
The gradient Lipschitz property of V folloWs that
V (Zk S ≥ σhzk-x(zk),Zk-Zk+1i- (2(σ⅛y+2 >k - zk+1k2
(24)
Step 3: Estimate the term ψγ (Zk+1, pk+1) - ψγ(Zk, pk). Since x(Zk,pk)
arg min L+ (x, Zk, pk), We have
x∈X γ
ψγ(Zk+1,pk+1) -ψγ(Zk,pk+1)
= Lγ+(x(Zk+1, pk+1), Zk+1, pk+1) - Lγ+(x(Zk, pk+1), Zk, pk+1)
≥ Lγ+(x(Zk+1, pk+1), Zk+1, pk+1) - Lγ+(x(Zk+1, pk+1), Zk, pk+1)
=2 (kx(Zk+1,pk+1) - Zk+1k2 -kx(Zk+1 ,pk+1) - Zk k2)
≥ σhZk - x(Zk+1, pk+1), Zk+1 -Zki.	(25)
By the concavity of ψγ(z, ∙) and Vψγ(z,p) = Ax(z,p) - b, we have
ψγ(Zk, pk+1) - ψγ(Zk, pk) ≥ hpk+1 - pk, Ax(Zk, pk+1) -bi
=2ηkpk -pk+1k2 + 2kAx(Zk,pk+1)-bk2
-ɪkpk+1 - Pk - η[Ax(Zk,pk+1) - b]k2.
2η
By the dual update of N-RPDC, the above inequality yields that
ψγ(Zk,pk+1) -ψγ(Zk,pk)
≥ ɪkPk -Pk+1k2 + *kAx(Zk,Pk+1) - bk2 - ɪkηA[xk -X(Zk,Pk+1)]k2
2η	2	2η
≥ ɪkPk - Pk+1k2 + 2 kAx(Zk,Pk+1) - bk2 - 2 kAk2 ∙kxk - x(Zk,Pk+1)k2. (26)
By Proposition A.2, (26) guarantees that
ψγ(Zk,pk+1) -ψγ(Zk,pk)
≥ 2η kPk - Pk+1k2 + 2 kAx(Zk ,Pk+1)-bk2
-n kAk2 ( L- 0 + ””2 + Lx + J kxk - Tx(Wk )k2.
2	σ - Lf - ρg
(27)
By the combination of (25) and (27), we obtain that
ψγ(Zk+1, pk+1) - ψγ(Zk, pk)
≥ σhZk - x(Zk+1,pk+1),Zk+1 - Zki + 2nkpk - pk+1k2
+ nkAx(Zk,Pk+1) - bk2 - nkAk2 (Lf + σ + YkAk2 + Lx +l) kxk - Tx(Wk)k2
2	2	σ - Lf - ρg
(28)
20
Under review as a conference paper at ICLR 2022
Step 4: Estimate the term [ν(zk) - ψγ(zk,pk)] - Ei(k) [ν(zk+1) - ψγ(zk+1, pk+1)]. Summing
statement (24) and (28), we obtain the variation of dual gap:
[ν(zk) - ψγ(zk, pk)] - [ν(zk+1) -ψγ(zk
≥ σ hx(zk+1, pk+1) - x(zk), zk - zk+1i -
+ɪ kpk - pk+1k2+η kAx(zk ,Pk+1)-ι
2η	2
+1, pk+1)]
σ2
-2 kAk2
Lf+σ+γkAk2+
σ - Lf - ρg
LK
4+1
2(σ - Lf - ρg)
bk2
2
+ 2) kzk -zk+1k2
kxk -Tx(wk)k2.
(29)
By Proposition A.1, we have that
σhx(zk+1, pk+1) - x(zk), zk - zk+1i
= σ[hx(zk, pk+1) - x(zk), zk - zk+1i + hx(zk+1, pk+1) - x(zk, pk+1), zk - zk+1i]
σ
≥	- 2λ kz
-σkzk
k -zk+1k2- σ2λkx(zk,pk+1)-x(zk)k2
-zk+1k ∙ kx(zk,pk+1) - x(zk+1
,p
≥ -(⅛ + —:— )kzk -zk+1k2- σλ
2λ σ - Lf - ρg	2
k+1)k
kx(zk,pk+1) - x(zk)k2,
(30)
with λ > 0 is any positive number. By the combination of (29) and (30), we have that
[ν(zk) - ψγ(zk,pk)] - [ν(zk+1) - ψγ(zk+1,pk+1)]
3σ2
一	12λ + 2(σ - Lf-Pg)
+ 2)kzk -zk+1k2
+2η kpk- pk+1k2+2 kAx(Zk ,pk+1)-
-2 kAk2
Lf+σ+γkAk2+
σ- Lf -ρg
LK
工 + 1
bk2
2
kxk
- Tx(wk)k2.
(31)
σ
Take expectation with respect to i(k) on both side of above inequality, we have that
[ν(zk) - ψγ(zk,pk)] - Ei(k)[ν(zk+1) - ψγ(zk+1, pk+1)]
3σ2
一 12λ + 2(σ - Lf-Pg)
+ 2) Ei(k)kzk - zk+1k2 - σ2λkx(zk,pk+1)-x(zk)k2
+ 2η kpk - pk+1 k2 + 2 kAx(zk,Pk+1)-
-2 kAk2
Lf+σ+γkAk2+
σ - Lf - ρg
LK
工 + 1
bk2
2
kxk - Tx(Wk)k2.
σ
Bythefactthat Ei(k)∣∣zk - zk+1k2 = N∣∣zk - Tz(Wk)k2 andpk+1 = Tp(Wk), above inequality
follows that
[ν(zk) - ψγ(zk,pk)] - Ei(k)[ν(zk+1) - ψγ(zk+1,pk+1)]
1σ
3σ2
≥	-N (2λ + 2(σ - Lf - Pg)
+ 2η kpk - Tp(Wk )k2 + 2 kAx(zk Z(Wk))- bk2
-2 kAk2
Lf+σ+γkAk2+
σ - Lf - ρg
直 + l! kxk - Tx(Wk)k2.
(32)
21
Under review as a conference paper at ICLR 2022
Step 5: Estimate the term Λ(wk) - Ei(k)Λ(wk+1). By the combination of Step 1 and Step 4, we
obtain that
Λ(wk) - Ei(k)Λ(wk+1)
爱-(Lf +2σ + YIAk2)
2N
- ηkAk2
Lf+σ+γkAk2+
σ - Lf - ρg
LK ʌ 2
-α^ + 1
kxk - Tx(wk)k2
0z- 2σ - λ -
3σ2
Q - Lf - Pg一
+ηkAx(zk, Tp(wk)) - bk2 - σλkx(zk, Tp(wk)) - x(zk)k2
(33)
Since 0 < αx ≤ βκ/[Lf + 2σ + Y∣∣A∣2 + 5], then OK — (Lf + 2σ + YkAk2) ≥ 5. Moreover,
0 < η ≤ 1/ 2N IAI2
+12
implies
Λ(wk) - Ei(k)Λ(wk+1)
21
≥ Nkχk - Tx(Wk)k2 + N
----2σ - ʌ -
3σ2
Q - Lf - Pg一
+ηkAx(zk, Tp(wk)) - bk2 - σλkx(zk, Tp(wk)) - x(zk)k2
(34)
A.6 Proof of Lemma 3.2
Proof. The result is directly by the fact that dist 0, ∂Lzγ (x(z, p), p) = kAx(z, p) - bk.
A.7 Proof of Lemma 3.3
Proof. The claim is provided by the results of Proposition 1 and Corollary of (Robinson, 1981).
A.8 Proof of Lemma 4.1
Proof. Taking λ = min{2σM⅞+1, σκη+ι} and 0 < αz < 1/ ∣2σ + M +
3σ2
+1
in
Lemma 3.1, we have that
Λ(Wk) - Ei(k)Λ(Wk+1)
21
≥ N kxk - Tx(wk )k2 + N kzk - Tz (wk )k2 + ηkAx(zk ,Tp(wk))- bk2
-σ min{
δη
2σM2 + 1 , σκ2 + 1
}kx(zk, Tp(wk)) - x(zk)k2.
(35)
≥
1
+N
1
σ
1
η
σ
□
□
For the last two terms of above inequality, we have two cases to discuss.
• Case 1: kAx(zk,Tp(wk)) - bk ≤ 2σ2ηM2 ≤ δ.
Since ∂ Lzγ (x, p) is locally metric subregular in (x, p) uniformly in z over Rd at point 0
with parameters κ, δ > 0, (35) yields that
21
Λ(wk) - Ei(k)Λ(wk+1) ≥ NIlxk - Tx(Wk)k2 + Nkzk
—
+σκ⅛ kAx(zk，Tp(Wk))- bk2.
(36)
• Case2: ∣∣Ax(zk,Tp(wk)) - bk > 2σ2ηM2
22
Under review as a conference paper at ICLR 2022
Since M = max kx - x0 k, we have that
x,x0∈X
ηkAx(zk,Tp(wk)) - bk2 — σλkx(zk,Tp(wk)) -x*(Zk)k2
≥ 2kAx(zk,Tp(wk))- bk2 + 2 ∙ 2σλM2 -σλM2
2	2η
=2kAx(zk,Tp(wk))- bk2.	(37)
For both case, (35) yields that
Λ(wk) - Ei(k)Λ(wk+1)
21	η
≥ 和 kxk — Tx(Wk )k2 + 为 kzk — Tz(Wk )k2 + —2⅛ kAx(zk ,Tp(wk)) — bk2. (38)
N	N	σκ + 2
By Proposition A.2, above inequality yields that
Λ(Wk) — Ei(k)Λ(Wk+1)
≥
ɪ kxk — Tx(Wk )k2 + ------------J——f kxk — x(zk,Tp(wk )k2
N	N L Lf +σ+YkAk2 + LK	+ 1∖
∖l σ-Lf-Pg	)
≥
≥
+ NN kzk - Tz(Wk)k2 + σKη+2 kAx(zk ,Tp(wk)) — bk2
ɪ kxk - Tx(wk )k2 +--7---1——Z——不 kA(xk -X(Zk ,Tp(wk ))k2
N	N kAk2( Lf +σ-LTg+ +α +1)
+ N kzk - Tz(Wk)k2 + σκη+2 kAx(zk ,Tp(wk)) — bk2
Nkxk - Tx(Wk)k2 + N kzk - Tz(Wk) k2
≥
≥
+ “公屋 > (2∣M(xk - x(zk,Tp(Wk)W + 2kAx(zk,Tp(Wk)) — bk2)
2(σκ + 2)
N kxk - Tx(Wk )k2 + NN kzk — Tz(Wk)k2 + 2(J2 +2) kAxk — bk2
N kxk-Tx(Wk )k2+N kzk—Tz(Wk)k2+2η(σκ+2) kpk-Tp(Wk )k2
cl kWk - T (Wk)k2,	(39)
with cl = min { 1, 2η(σκ%) + 1 ).	□
A.9 Proof of Theorem 4.1
In order to provide the Theorem 4.1, we need the famous Robbins-Siegmund theorem.
Theorem A.1. (Robbins-Siegmund Theorem, Robbins & Siegmund (1971), Theorem 1.3.12
in Duflo (2013), Theorem 2.27 in Carpentier et al. (2015)) Let {Λk}k∈N, {μk}k∈N, {νk}k∈N
and {ηk }k∈N be four positive sequences of real-valued random variables adapted to the filtration
{ξk}k∈N. Assume that
EξkΛk+1 ≤ (1+ μk )Λk + Vk — ηk, ∀k ∈ N,
and that
ɪ2 μk < +∞ and	νk < +∞, a.s..
k∈N	k∈N
23
Under review as a conference paper at ICLR 2022
Then, the sequence {Λk}k∈N almost surely converges to a finite3 random variable Λ∞, and	ηk <
k∈N
+∞, a.s..
Then we can derive the technical proof of this theorem.
Proof of Theorem 4.1:
(a)	By Lemma 4.1 and the fact that Λ(w) ≥ F *, we have that
Ei(k)[Λ(wk+1) - F*] ≤ [Λ(wk) - F*] - Nkwk - T(Wk)∣∣2.
By the Robbins-Siegmund Theorem (Theorem A.1), we obtain that, lim Λ(wk) is almost
k→+∞
+∞
surely exists and P kwk - T(wk)k2 < +∞ a.s.. Therefore, we have limk→+∞ Φ(wk) = 0;
k=0
(b)	By the compact ofX, we have the sequences {xk} and {x(zk)} are bounded.
By statement (a), we have lim Λ(wk) is almost surely exists. It follows that the sequence
k→+∞
{Λ(wk)} is almost surely bounded. By the definition of Λ(wk), we also have that Λ(wk) ≥
ν(zk) ≥ F(x(zk)) ≥ F*. Therefore the sequence {ν(zk)} is almost surely bounded.
Now we show the almost surely boundness of the sequence {zk} by contra-
diction. Suppose P kzk k → +∞	> 0.	Since the sequence {x(zk)} is
bounded, there exists a subsequence {zk0} such that x(zk0) → x. Therefore
P {ν(zk0) = F(x(zk0)) + 2k(x(zk0)) - zk0k2 → 十∞}
> 0, which follows a contradiction
with that {ν(zk0)} is almost surely bounded. Therefore, we obtain the almost surely boundness
of the sequence {zk}.
Next we propose the boundness of {pk}. By the optimal condition of problem of Tx (wk) =
argminhVf (xk) + σ(xk — Zk), Xi + g(x) + CTp(wk) + Y(Axk — b), Axi + D(x,x ), We have
x∈X	αx
0∈ Vf(xk)+σ(xk - zk) + ∂g(Tx(wk)) + NX(Tx(wk)) + A>[Tp(wk) + γ(Axk -b)]
+ -[VK (Tx(wk ))-VK(xk)].
αx
Since Tx(wk) = N Ei(k)xk+1 - (N - 1)xk and the boundness of the sequence {xk}, we
have the sequence {Tx(wk)} is bounded. By bounded of subgradient of g, the boundness of
{xk}, {Tx(wk)} and the almost surely boundness of {zk}, it follows that kA>Tp(wk)k < +∞,
∀k > 0, a.s.. By the fact that pk+1 = Tp(wk), we have that
kA>pk+1 k < +∞, ∀k > 0, a.s.
By update of p in N-RPDC, we have that
kk
pk+1 = p0 + pk+1 when pk+1 = η^X(Axj — b) = nA^^(xj — x*) ∈ Im(A),
j=1	j=1
with x* be the optimal solution of (P). Since p0 ∈ Null(A>), we have A>p0 = 0 and
kA>pk+1k < +∞, a.s.. Then We have two cases to discuss.
• Case 1: rank(A) = n. Obviously, we have that
kpk+1k≤
kAvpk+1k
Pλm1(AA>)
< +∞,
with λmin(AA>) be the smallest eigenvalue of matrix AA>.
• Case 2: rank(A) = r < n. Without loss of generality, assume the first r rows of A
(denoted by Ar ∈ Rr×d) are linearly independent, we have
Ar
BAr
Ar,
3A random variable X is finite if P ({ω ∈ Ω∣X (ω) = +∞}) = 0.
24
Under review as a conference paper at ICLR 2022
where Ir×r ∈ Rr×r is the indentity matrix and B ∈ R(n-r)×r. Let Q := Ir×r + B>B.
It is easy to show that Q is symmetric and positive definite. By the fact that Pk+1 =
nA Pj=1(χj - x*), we have that
2
(Ar)> (Ir×r, B>) n ( IBr ) Ar X(xj - x*)
j =1
2
k
(Ar)>QηAr X(xj -x*)
j=1
≥
2
k
λmin (Ar (Ar )>) QnAr X(xj - x*)
≥
k
λmin (Ar (Ar )>) λmin(Q>Q)knAr £3 - X*)∣∣2,
j=1
where λmin Ar(Ar)> is the smallest eigenvalue of positive semidefinite matrix Ar(Ar)>
and λmin(Q>Q) is the smallest eigenvalue of positive semidefinite matrix Q>Q. Using the
fact that
k
kPk+1k2	= knAX(χj - χ*)k2
j =1
= kn	IrB×r	Ar Xk (xj - x*)k2
j=1
k
≤ λmax (Q) knAr X(xj -x*)k2,
j=1
where λmax(Q) is the largest eigenvalue of matrix Q. Therefore, we have that
kpk + 1k2 ≤ λmin (Ar (Am)>)QL(Q>Q) MIK
Therefore, for both cases, there exists d > 0 such that
kPk+1k ≤ dkA>pk+1k < +∞,a.s.
which yields the almost surely boundness of {pk} by the triangle inequality. Therefore, the
sequence {wk} is almost surely bounded.
(c)	By lim kwk - T (wk)k = 0 a.s. in statment (a), Lemma 2.1, Proposition 3.1, the almost
k→+∞
surely boundness of {wk}, and the closedness of ∂L+ (∙), We obtain the desired result.
A.10 Proof of Theorem 4.2
Proof. Suppose this statement does not hold, that is,
P < liminf √k + 1Φ(wk) ≥ δ > > 0,
k→+∞
for some d > 0. Then for k0 large enough, for all k ≥ k0 we have
Therefore
P 卜(Wk) ≥√⅛} >0.
(+∞	+∞	+∞	2
X (Φ(wk))2 ≥ X (Φ(wk))2 ≥ X k⅛1 > 0.
k=k0	k=k0	k=k0
25
Under review as a conference paper at ICLR 2022
+∞	2
Since the fact that E kd+1 = +∞, We have that above inequality is contradicted with the fact that
k=k0
+∞
P (Φ(wk))2 < +∞ a.s. in the proofofTheorem4.1. Therefore, we obtain the desired result. 口
k=0
A.11 Proof of Theorem 4.3
Proof. Recalling the inequality of Lemma 4.1 and the fact that Λ(wk) ≥ V(Zk) ≥ F*, we have
Ei(k)[Λ(wk+1) - F*] ≤ [Λ(wk) - F*] - CcN(Φ(wk))2.
Taking expectation with respect to Ft, t > k for above inequality, we obtain that
EFt [Λ(wk+1) - F*] ≤ EFt [Λ(wk) — F*]-果EFt(Φ(wk))2.
t	t	c2 N	t
By the fact Λ(wk) ≥ V (zk) ≥ F *, it follows
t
ccN XEFt(Φ(wk))2 ≤ Λ(w0)- F*.
c k=0
By the Jensen’s inequality and the convexity of function h(x) = x2, x ∈ R, it follows that
t
ccN X(EFtφ"))2 ≤ A(w0)- F*.
k=0
By the fact that EFt Φ(wk) ≥ 0 and h(x) = x2, x ∈ R is monotonic increasing in x on [0, +∞), we
have that
昌(t + 1)(叫EFtφ(Wk))2 ≤ A(W0)- F*.
Here comes that
,…/ k、/ CcCN(Λ(w0) - F*)
min EFΦ(wk) ≤ 1 -1----------------.
0≤k≤t Ft	t + 1
□
26