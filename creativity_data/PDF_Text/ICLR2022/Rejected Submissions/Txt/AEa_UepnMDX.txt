Under review as a conference paper at ICLR 2022
Resolving label uncertainty
WITH IMPLICIT GENERATIVE MODELS
Anonymous authors
Paper under double-blind review
Ab stract
In prediction problems, coarse and imprecise sources of input can provide rich
information about labels, but are not readily used by discriminative learners. In
this work, we propose a method for jointly inferring labels across a collection
of data samples, where each sample consists of an observation and a prior be-
lief about the label. By implicitly assuming the existence of a generative model
for which a differentiable predictor is the posterior, we derive a training objective
that allows learning under weak beliefs. This formulation unifies various machine
learning settings: the weak beliefs can come in the form of noisy or incomplete
labels, likelihoods given by a different prediction mechanism on auxiliary input,
or common-sense priors reflecting knowledge about the structure of the problem
at hand. We demonstrate the proposed algorithms on diverse problems: classifi-
cation with negative training examples, learning from rankings, weakly and self-
supervised aerial imagery segmentation, co-segmentation of video frames, and
coarsely supervised text classification.
1	Introduction
We consider the problem of joint inference of latent label variables li in a collection of data samples
indexed by i consisting of observations (features) Xi and corresponding prior beliefs about their
latent label variables Pi(I).
Two illustrative examples are shown in Fig. 1. In the first example, the Xi are 784-dimensional
vectors representing 28×28 MNIST digits. We aim to infer the digit classes I ∈ {0,1,…，9} for
all images in the given collection based on data in which we are given just one negative label per
sample, i.e., the prior beliefs Pi (I) (top row) are uniform over all classes except for one incorrect
class. The procedure described in this paper produces inferred distributions over labels (bottom row)
that are usually peaky and place the maximum at the correct digit 97% of the time (Fig. 3).
In the second example, the observations Xi are image patches centered around each pixel coordinate i
in a Surrealist painting, with patch size (11×11) equal to the receptive field of a 5-layer convolutional
neural network used in our inference procedure. The prior beliefs Pi (I) are distributions over 3
classes (sky, boat, water) depending on the coordinate i. The joint inference of all labels in this
image yields a feasible segmentation despite the high similarity in colors and textures (see §E.4).
In both examples, the inference technique needs to estimate statistical links between observations
Xi and corresponding latents Iithat would both be highly confident (i.e., lead to low entropy in the
posterior distributions) and explain the varying prior beliefs, which typically have low confidence
(high entropy in the prior distributions). This problem of training on weak beliefs, in some form, is
often encountered in machine learning, e.g, weak supervision and semi-supervised learning, domain
transfer, and integration of modalities - settings where coarse, partial, or inexact sources of data can
provide rich information about the state of a prediction instance, though not always a “ground truth”
label for each instance.
In many such settings, fusing the weak input data into a probability distribution over classes is a
more natural alternative to transforming the weak input into hard labels [26]. However, because
supervised models target the distribution over labels, training machine learning models with super-
vision from probabilistic prior beliefs results in uncertain predictions. Most approaches to resolve
these uncertainties involve iterative generation of hard pseudo labels [56], or loss functions promot-
1
Under review as a conference paper at ICLR 2022
(a) Le SeduCteur, Rene Magritte
(b) Boat Prior, anonymous artist
(c) Inferred segmentation
Figure 1: Above: Inference of latent MNIST digit classes with negative label supervision using a
small CNN trained on the RQ criterion (§2.1). Below: Joint inference of latent pixel classes in an
image (a). The prior beliefs Pi (I) over three classes - Sky (red), boat (green), water (blue) - are
manually set (b). A small CNN using no data except (a,b) infers the posterior classes (c).
ing low entropy of predictions [35; 55; 59; 54]. Typically, these approaches are application-specific
[10; 57; 1; 22]. Further discussion of related work is provided in §B and §C.
Our key modeling insight is to associate the output distribution of a discriminative model, a feed-
forward neural network q, with an implicit generative model (§2.1) of features and to consider the
given prior belief as part of that model. We show that without estimating the full generative model,
it is possible to learn the network that performs inference in it and reap the benefits of generative
modeling, including high certainty in the posterior under soft priors and rich opportunities to model
structure in the prior beliefs. We validate the effectiveness of our approach with experiments (§4,
§E) that highlight: prior beliefs as a natural way to fuse weak inputs, graceful degradation of per-
formance with increasingly noisy or incomplete inputs, and comparison of our implicit generative
model with explicitly generative modeling approaches.
2	Background and approach
Supervised learning on prior beliefs. Supervised learning models, including many neural nets,
are typically trained to minimize the cross-entropy - Ei E1Pld (I) log qi (I) between the data distri-
bution over labels P: (I) and the distribution qI (I) = q (11xl; θ) output by a predictor q using data
features Xl. This is equivalent to minimizing the KL divergence ^l KL(P: kqI) which is not well-
suited to training with uncertain labels defined by prior beliefs. If we were to set P: (I) to equal a
much softer prior over latent labels, PI (I), the minimum would be attained when the two distribu-
tions PI (I) and qI (I) are equal: when the prior belief is soft, the trained model q will also be highly
uncertain. Turning soft labels into hard training targets, (P:(I) = l[l = arg maxl PI(1)], or by
sampling), introduces the opposite bias. Now, the cost would indeed be minimized if the predictions
had zero entropy, but learning such a prediction function faces difficulty with overconfident labels
which are often wrong, as well as the possibility that certain labels often receive substantial weight
in the prior, but never the maximum. These issues are illustrated in Figure D.3.
Generative modeling resolves the prior’s uncertainty. The approach to classification problems
through generative modeling, instead of targeting the conditional probability of latents given the
data features, assumes that there is a forward (generative) distribution P (xl 11) and optimizes the
log-likelihood of the observed features, ^l log(xl) = ^l log 2l P(xl 11)PI (I), with respect to the
parameters of the forward distribution. The posterior under the model P (11xl) æ P (x111)PI (I) is
then used to infer latent labels for individual data points [44]. As a result, the generative modeling
approach does not suffer from uncertainty in the posterior distribution over latents given the input
2
Under review as a conference paper at ICLR 2022
features, even when the priors Pt (I) are soft.1 Further discussion relating our method with existing
generative modeling approaches is given in §C.
However, expressive generative models are typically harder and more expensive to train compared to
supervised training of neural networks, as they often require sampling (e.g., sampling of the posterior
in VAEs [21] and sampling of the generative model in GANs [8]). Furthermore, the modeling often
requires doubling of parameters to express both the forward (generative) model and the reverse
(posterior) model. And, in case of GANs, the learning algorithms may not even cover all modes in
the data, which would prevent joint inference for all data points.
2.1	Optimization under implicit generative models
Suppose that there is a generative model P(x|l) of observed features conditioned on latent labels.
Optimization of the log-likelihood of observed features, 2t log(xt) = 2t log Ei P(Xt |l)Pt (I), can
be achieved by introducing a variational distribution qt (I) over the latent variable for each instance
xt, then minimizing the variational free energy, which we review next.
Recall that the free energy, also known as the negative evidence lower bound (ELBO), is defined as
F :=? qt(ι) log PXqI^=Z(KL© 旧) - log P(X 力,	⑴
where G (I) = S(⅞图l()i) is the posterior distribution under the generative model. As the KL
distance is always positive, the free energy is minimized when qt =片,in which case the free energy
equals the negative log-likelihood of the data.
Free energy minimization is used in various approaches to latent variable modeling. The
expectation-maximization (EM) [6] algorithm minimizes (1) by iteratively setting qt to equal the
posteriors G and updating the parameters of the forward distributions P (xt |l) while keeping qt fixed,
until guaranteed convergence to a local minimum. In applications of EM predating deep generative
models, the qt distributions are not given by a predictor taking Xt as input; they are are auxiliary
distributions used only as part of the learning procedure and replaced at each EM iteration. Their
dependence on Xt is implicit in energy minimization.
In variational auto-encoders (VAEs) [21], both the generative model P(x|h) and the posterior
q(川x) are represented as neural networks. Here P is a deep nonlinear generative model with hidden
latents h (not necessearily corresponding to labels), and evaluation of the full posterior is intractable.
Instead, q is learned as a neural network q(h|xt, θ) jointly with p. VAE training involves sampling
from qt (h) = q(x|xt, θ) in learning to improve the agreement between the forward (generative)
model P (and its true posterior 片(h)) and the reverse (posterior) neural network model q.
Implicit generative models. To derive our training objectives, we will assume the existence of the
generative model P(x∣l) without ever parametrizing it or sampling from it. Instead, we parametrize
only the reverse (variational posterior) model qt (I) = q (l|Xt ； θ) in the form of a neural net taking Xt
as input. The trained model q is the direct output of our algorithms.
Given the variational distributions qt, we can minimize (1) with respect to the forward probabilities
P (Xt 11) for all Xt and I such that 21P (Xt 11) = 1 for all I. The optimum is achieved by:
P(xt ∣l)=
qt (I)
2j qJ(I)
(2)
We refer to this as the implicit generative model associated to q. Much like the posterior is implicit
in the free energy minimization in the EM algorithm, here instead the forward model is just a matrix
of numbers pt,i, implicit to free energy minimization - the opposite, in some sense, to what is done
in EM optimization. The link X — I is left entirely to the neural network q to capture explicitly. The
probability mass of the implicit generative model is not uniformly distributed, rather, the data points
for which the variational posterior qt (I) is more certain are considered more likely under that latent
I, unless it corresponds to a popular class I to which many other data points are also assigned.
1For intuition, consider a model where I is the mixture index in a mixture of high-dimensional Gaussians.
If the observations Xt naturally cluster into several well-defined clusters, then the prior Pt (I) may be flat, but
the posterior distributions under the model assign data points to clusters with high certainty.
3
Under review as a conference paper at ICLR 2022
The posterior under the implied generative
model is
P i (l) G (l)
2 j∙qj (l)
(3)
Note that We can compute ri (l) by multiplying
the re-normalized model outputs with the prior
at each instance, so that for each instance i We
have two outputs: qi and ri.
We propose tWo methods of optimizing the free
energy with respect to the parameters θ by gra-
dient steps. Each method iterates the folloWing:
(1)	Calculate the posterior distributions ri in
terms of qi as in (3)
(2)	Update the parameters of q with a gradient
step:
•	Option QR: θ — θ - ηV9 ^i KL(qi∣∣ri).
•	Option RQ: θ — θ - ηVθ ^i KL(ri∣∣qi).
# IOg_q : ( batch_size, n_classes ) log-likelihoods from model
# prior : ( batch_size, n_classes ) prior likelihoods
def ce_loss(log_q, prior):
return -(IOg_q * prior).sum(l)
def qr_loss(log_q, prior):
log_r = (log_q.Iog_softmax(0) + prior.IogO).log_softmax(l)
return (log_q * log_q.exp()).sum(l) - (log_r * log_q.exp()).sum(l)
def rq_loss(log_q, prior):
log_r = (log_q.Iog_softmax(0) + prior.IogO).log_softmax(l)
return (log_r * log_r.exp()).sum(l) - (log_q * log_r.exp()).sum(l)
Figure 2: Cross-entropy and implicit QR / RQ
losses in PyTorch. Note that normalization in (2)
is done within a batch, rather than across the en-
tire dataset. In practice, this may be sufficient if
batches are large and representative of the diver-
sity in the data. Otherwise, the denominator in (2)
may need to be updated in an online fashion.
厂 i (I) 区
Gradients of the model parameters are propagated to the expression of ri through qi (see Fig. 2).
Both losses have a stable point when qi = ri. A discussion of the relative benefits and limitations of
the QR and RQ loss formulations is given in §A, along with practical considerations for implement-
ing these methods.
Option QR uses the KL distance in the direction it appears in (1) and thus guarantees continual
improvements in free energy and convergence to a local minimum (with the exception for the effects
of stochasticity in minibatch sampling). Substituting ri from (3), the free energy (1) becomes:
F = 2 qi(l) log I^ qj(I))- 2 qi(l) log (Pi(l))	3 (4)
i,l	∖ j	) i,l
This criterion does not encourage entropy of individual qi distributions, but of their average. The
second term alone would be minimized if the q network could put all the mass on arg maxi Pi (l)
for each data point, but the first term promotes diversity in assignment of latents (labels) l across
the entire dataset. Thus a network q can optimize (4) if it makes different confident predictions for
different data points. To illustrate this, consider the case when Pi(l) = P(l), i.e. all data points have
the same prior. Then (4) is minimized when 七 ^i qi (h) = P(l), and this can be achieved when
the network learns a constant distribution q (11xi; θ) = P (l). But the free energy is also minimized
if the network predicts only a single label for each data point with high certainty, but it varies in
predictions so that the counts of label predictions match the prior.
As demonstrated in Fig. 1 and in our experiments, avoiding degenerate solutions is not hard. We
attribute this to two factors. First, the situations of interest typically involve uncertain, but varying
distributions Pi(l) which break symmetries that could lead to ignoring the data features xi. Second,
the neural networks used as posterior models q come with their own constraints in parametrization
(e.g. a multi-layer convolutional net with certain number of filters in each layer), and, the inductive
biases as a consequence of gradient descent learning in nonlinear, multilayer networks. In fact, as
discussed in §3 and §E.1, even unsupervised clustering is possible with suitably chosen priors that
break symmetry, allowing this approach to be used for self-supervised training. See also §C for
more on relationships with other approaches.
3 Sources of label priors
In this section, we describe a range of machine learning settings where priors Pi (l) emerge. These
settings are illustrated in experiments in §4, with additional experiments in §E.
Negative or partial labels (§4.1). When we are given a set of equally possible labels Li for each
point data point i, instead of a single label li, then we set the prior P i (l)= 由 1[l ∈ Li ]. An
extreme example is when one negative label is given, as shown in Fig. 1.
4
Under review as a conference paper at ICLR 2022
Joint labels and learning from rankings (§4.2). Priors may also come in the form of joint dis-
tributions over labels of multiple instances. For example, ranking supervision - the knowledge of
which example in a pair is greater with respect to an ordering of the labels - gives prior beliefs about
pairs of labels. Suppose our data is organized into pairs of images of digits Tj■ = {xj,ι,xj,?}, and
for each pair we are told which image represents the digit (0-9) which is greater. This sets a prior
P (lι, I2) over pairs oflabels in each pair, represented by either an upper or a lower triangular matrix,
depending on which digit in the pair is known to be greater, with all nonzero entries equal to 1/55.
We assume the implicit generative model has the form P(x1,x2∣I1,I2) = P(ɪi |li)P(X2∣I2). Weaim
to fit a posterior model q(I|x; θ). For each pair T`, We have two outputs of the predictor network,
q(lι ∣xj,ι) and q(I2|x/,2), for the two images in the pair. The joint posterior under the implicit
generative model is
rj (I1,I2) 8 P(I1,I2)P(Xj,ι |li)P(Xj,2∣I2) 8
P(I1,I2)q(lι |xj,ι)q(I2|x/,2)
2 j q (lι Ix/,1) 2 j q(I |x/,2)
(5)
and we can now use QR or RQ loss to fit q(lι |x7∙,ι) to the marginal r7∙ (lι) and q(I2|x/,2) to r7∙ (I2).
Coarse data in weakly supervised segmentation (§4.3, §E.2, §E.4). We often have side infor-
mation Z associated to each instance i that allows setting the priors Pi(I) = P(I|号)for each point
directly by hand. These include situations when we have beliefs about labels for different points, as
in the Seducer example (Fig. 1), but also more interesting weak supervision settings, such as ones
that sometimes arise in remote sensing (§4.3) and medical pathology (§E.2) applications. For ex-
ample, in a task of segmenting aerial imagery into land cover classes, we often have coarse labels c
associated to large blocks of pixels, but not the target labels I for individual pixels. If the conditional
P (I| c) is known, it sets a belief about the high-resolution labels I for pixels in a block of class c.
Fusing models and data sources (§4.4, S4.5). Auxiliary information Z may not always come
with a known correspondence P(I|z). In the land cover mapping problem, auxiliary information
may include different modalities and resolutions (road maps, sparse point labels, etc.). While these
sources can be fused into a prior by hand-coded rules, the prior may be more accurately set as the
output of a model P(l|Zi) trained on a separate dataset of points (I,Zi). This is especially useful
when the data Xi (e.g. satellite imagery) is more informative about the latents I, but is prone to
domain shift problems, while the auxiliary data Zi does not suffer from domain shift issues but is not
sufficient on its own to predict the labels. In a text classification problem, Zi might be the encoding
of text Xi by a pretrained language model, and P (l|Zi) a noisy distribution over labels given by their
likelihoods under the language model as continuations of a prompt.
Priors for self-supervision (§E.1). In §2.1 we discussed the pitfalls of using a constant prior
Pi (I) = P (I) for all data points in training models under the QR loss as a potential method for un-
supervised clustering. However, in §E.1 we give an example of joint learning of the posterior model
q and an energy model (Markov random field) on the latent labels I that expresses local structure of
labels in an image. This results in unsupervised clusterings that are useful in downstream segmen-
tation tasks. Such an approach is an example ofa benefit of generative modeling - the possibility of
learning of a parametrized distribution over latents - being inherited by implicit generative models.
Priors with latent structure (§E.3). Implicit generative modeling allows building hierarchical
latent structure into the prior (another benefit of classical generative models), as we demonstrate
in §E.3 on a video segmentation task. The prior is an admixture of possible segmentations with a
structure similar to [17], but using a set of mask proposals P (I |m) from a Mask R-CNN model [11],
indexed by a latent m. The prior is Pi (I) = ^m P(I |m)P(m), where P (m), a probabilistic selection
of the masks for the admixture in the given frame, is estimated by minimizing the free energy.
4 Experiments
4.1	PARTIAL LABELS IN MNIST AND CIFAR- 1 0
In this illustrative experiment, we compare algorithms for learning with partial labels on two 10-way
image classification datasets, MNIST and CIFAR-10. To each training example Xi, we randomly
assign a set Ni of k negative labels, chosen from the 9 labels distinct from the ground truth. The
prior Pi (I) is set to be uniform over I ∉ Ni and 0for I ∈ Ni. We vary k from 1 (one negative label per
example) to 9 (one-hot prior, full supervision). The data of k negative labels carries - log2(1 -k/10)
bits of label information - if k = 1, 22× less label information than in the fully supervised setting.
5
Under review as a conference paper at ICLR 2022
step 10
step 20 step 40 step 80 step 160 step 320 step 640 step 1280
Figure 3: Above: Accuracies of MNIST and CIFAR-10 classifiers trained with varying numbers of
negative labels per example; the lighter variant of each color and marker shows the peak accuracy
over 300 training epochs. (Average of 10 runs with standard error region.) Below: Confusion ma-
trices of MNIST classifiers in the course of training on batches of 128 pairs of digits. The trajectory
of convergence to the diagonal shows that uncertainty is first resolved for the digits 0 and 9, then 1
and 8, etc.
For both datasets, the base model q is taken to be a small convolutional network, With four layers
of ReLU-activated 3 × 3 convolutions with stride 2 and a linear map to the 10 output logits (〜33k
learnable parameters for MNIST,〜34k for CIFAR-10). We experiment with four training losses:
• CE: cross-entropy between predictions q(11/；θ) and the prior Pi (I).
•	NLL (union): negative logarithm of the sum of likelihoods assigned by q to labels in I ∉ Ni, or,
equivalently, log 2l pi (I)q (11Xi; θ), as done, e.g., by [15; 19].
•	The QR and RQ losses.
The CE, NLL (union), and RQ losses are equivalent when k = 9, and the RQ and NLL (union)
losses are equivalent when 2i qi (I) is uniform over I, which approximately holds late in training.
All models are trained for 300 epochs on batches of 256 images with the Adam optimizer [20] and
a learning rate of 10-4 . After each epoch, we compute the accuracy of the predictor q on the ground
truth labels in the train and test sets. Fig. 3 shows the final accuracies, as well as the maximum
accuracies over epochs, averaged over 10 choices of partial label sets and random initializations.
Models trained on RQ loss perform best, with the greatest benefit over CE seen for very few negative
labels. We hypothesize that the small advantage of RQ over NLL (union) loss can be attributed to
regularization in early training. Meanwhile, QR performs as well as CE for very uncertain priors at
the peak epoch, but its predictions degenerate with longer training.
4.2	Multiple-instance supervision: Learning from ranks
We train a CNN of the same architecture as in §4.1 on MNIST, but with the only supervision coming
in the form of pairs of images in which it is known which image represents the greater digit. The
training set of 60k images is divided into pairs that are fixed throughout the training procedure; each
digit appears in exactly one pair. We optimize to match the predictor q with the implicit generative
model’s posterior (5) using the RQ loss. Fig. 3 shows the confusion matrices at initial iterations of
training. The learned classifier has 97% accuracy on both training and testing sets, which means that
from pairwise comparisons alone, we can group the digit images and place them in order.
4.3	Label super-resolution: Chesapeake Bay land cover mapping
We benchmark our method’s performance on the Chesapeake Land Cover dataset2, a large
1m-resolution land cover dataset used previously for label super-resolution [27; 42]. It con-
2https://lila.science/datasets/chesapeakelandcover
6
Under review as a conference paper at ICLR 2022
Figure 4: Predictions of models trained with QR loss on the NLCD-only prior in the Chesapeake
region, shown on regions of 1000×1000 pixels in Pennsylvania and 500×500 pixels in New York.
sists of several aligned data layers, including: NAIP (4-channel high-resolution aerial im-
agery at about 1m/px), NLCD (16-class, 30m-resolution coarse land cover labels), and
high-resolution land cover labels (LC) in four classes. The task is to train high-resolution segmenta-
tion models, in the four target classes, using only NLCD labels as supervision. Although the NLCD
layer is at 30× lower resolution than the imagery and target labels and follows a different class
scheme, the cooccurrence statistics of NLCD classes c and LC labels I are assumed to be known.
To form a prior over land cover classes I at each pixel position, We map the NLCD classes to proba-
bilities over the target LC classes using these known cooccurrence counts and apply a spatial blur to
reduce low-resolution block artifacts (Fig. 4, “Prior”). We then train small convolutional networks
(receptive field 11 × 11) to predict high-resolution land cover from input imagery. We evaluate both
the QR and RQ variants of our approach on the two states that comprise the “Chesapeake North”
test set: Pennsylvania (PA) and New York (NY), and the two states combined, after picking hy-
perparameters based on an independent validation set in Delaware (details in Appendix D.1.3). A
depiction of the data and prediction results is given in Figure 4.
Table 1 compares our performance against the algorithmic technique with the best published per-
formance on the Chesapake dataset, self-epitomic LSR [28] and the hard naive baseline from [27].
Self-epitomic LSR, a generative modeling approach that explicitly produces likelihoods P(x|l), an-
alyzes small patches of data by making a large number of comparisons between sampled 7 × 7 image
patches and all other image patches. It does not produce a trained feedforward inference model, and
the inference procedure is at least an order of magnitude slower than evaluation of our convolutional
model. The hard naive baseline maps the NLCD classes to LC classes based on a given concurrence
matrix, then trains a standard semantic segmentation model on these pseudo-labels.
Not only does training on the QR loss achieve comparable performance with self-epitomic LSR
(Table 1), but the implicit generative model for P(x|c) from (2) is largely consistent with the epito-
mic generative model (Fig. D.4). Moreover, our method handles batched input, where self-epitomic
LSR trains on one tile at a time, and similar approaches have been shown to degrade in performance
and exhaust computation capacity when training on multiple tiles [28]). Thus, optimization under
an implied generative model has the computational advantage of scaling naturally to large training
data while maintaining the benefits of leading generative modeling approaches. (See also §E.2.)
4.4	Data fusion and learned priors: EnviroAtlas
In this set of experiments, we augment NLCD with information about the presence of buildings, road
networks, water bodies, and waterways from public sources (see Fig. 5 and §D.1.1). To evaluate the
ability of models to generalize to new geographic regions, we use 1m 5-class land cover labels from
the geographically diverse EnviroAtlas dataset [39] in four cities in the US: Pittsburgh, PA, Durham,
NC, Austin, TX, and Phoenix, AZ. The NLCD-based prior model from §4.3 is augmented with
the auxiliary information to obtain a hand-coded prior for each image (see §D.1.2.) These types of
priors can be made everywhere in the United States, while hard labels are rarely available.
The standard alternative to performing local inference under such priors is to simply apply super-
vised models trained on hard labels elsewhere, hoping that the domain shift is tolerable. Table 2
7
Under review as a conference paper at ICLR 2022
Table 1: Pixel accuracy and class mean intersection over union on the Chesapeake Land Cover
dataset. All models use only coarse NLCD labels as supervision. For our proposed methods, we
evaluate both the trained predictor (qi) and the posterior under the implicit generative model (小).
Model	PA		NY		Chesapeake	
	acc %	IoU %	acc %	IoU %	acc %	IoU %
Self-epitomic LSR [28]	86.2	67.6	86.4	70.5	86.3	69.7
Hard naive [27]	85.3	63.0	83.6	59.8	83.6	59.7
QR (q)	84.2	66.6	86.2	71.0	84.6	67.7
QR &)	85.0	68.1	86.9	72.4	85.4	69.4
RQ (q)	82.2	63.4	79.0	61.4	78.7	59.4
RQ &)	82.3	63.5	79.0	61.6	78.8	59.7
NLCD R°ads, buildings, nlcd only prior Hand-coded prior Learned prior
<dx=pnqswd ONUeΨ∏α Xpu一 sn< z<xueoqd
sir
Labels
EnviroAtIas classes
Water
Impervious Surface
Soil and Barren
Trees and Forest
■ Grass and Herbaceous
Shrubs (not predicted)
NLCD classes
Open Water
∑∏1 Developed Open Space
Developed Low Intensity
Developed Medium Intensity
Developed High Intensity
Deciduous Forest
Evergreen Forest
Mixed Forest
■I Shrub/Scrub
ΣΣ! GrassIandZHerbaceous
■ Pastu re/H ay
■ Woody Wetlands
Emergent Herbaceous Wetlands
Figure 5: Prior generation process for land cover mapping: “NLCD only prior” as used in §4.3 and
“hand-coded prior” and ‘learned prior” as used in §4.4.
compares the performance of a model (of the same architecture as in §4.3) trained on Pittsburgh
high-resolution data (HR) in each of the three other cities with that of models tuned on the hand-
coded prior in each other city. The QR method trained on the local handmade prior outperforms
the HR model in each evaluation city. This may be attributed to the extra data in each city given to
our method in the form of prior beliefs. To isolate this effect, we also compare to a high-resolution
model that consumes the prior belief to input data, concatenated with the NAIP imagery (HR + aux).
While the HR + aux model does increase performance substantially from the HR model with NAIP
imagery alone as input, the QR model remains the highest-fidelity approach. These results illustrate
that information that generalizes across domains may find its best USe within a separate model - to
build a prior in our setting - and then used to supervise local inference.
A prior belief coUld be crafted by a domain expert to reflect the UniqUities in geographic and strUc-
tural features for each city. We emulate incorporating such context-specific knowledge by training
(on a disjoint set of instances) a neural network that consumes the inputs to the handmade prior
function (NLCD and auxiliary map data), and predicts high-resolution labels (Figure 5, “Learned
prior”). Alongside structural interactions between the inputs that generalize across cities (e.g. tree
canopy supersedes rivers, road supersede water), the learned prior captures region-specific knowl-
edge (e.g., buildings in Durham, tend to be have grass surrounding them and trees farther out, while
in Austin, this is reversed, and in Phoenix, riverbeds surrounded by barren land are likely to be dry).
As shown in Table 2, these tailored prior beliefs tend to increase scores.
The final row in Table 2 benchmarks the performance of a high-resolution land cover model trained
on imagery and labels over the entire contiguous US [42]. This large model takes NAIP, Landsat 8
satellite imagery, and building footprints as inputs. Small, local models with priors created from only
weak supervision outperform the US-wide model in all cities. (See §D.1.4 for evaluation details.)
8
Under review as a conference paper at ICLR 2022
Table 2: Land cover classification experiments for generalizing across cities. In each column, the
score of the best model not depending on auxiliary data as input is italicized. The score of the best
overall model is bolded. (A larger set of experimental results is given in Table D.1.)
Durham, NC Austin, TX Phoenix, AZ
Train region	Model	acc %	IoU %	acc %	IoU %	acc %	IoU %
Pittsburgh	HR	72.9	34.8	71.9	36.8	6.7	7.5
(supervised)	HR + aux	78.9	47.7	77.2	50.6	66.0	27.9
Local	QR (q)	78.9	46.6	79.3	51.7	73.2	39.1
(hand-coded prior)	QR （厂）	79.1	48.8	79.6	53.0	73.4	40.6
Local	QR (q)	78.7	47.4^^	79.8	51.1	74.6	39.6
(learned prior)	QR （厂）	78.9	49.6	80.3	52.6	75.3	42.2
Full US [42]	U-Net Large	77.4	48.8	76.4	51.4	24.5	23.2
Table 3: F1-scores of various models on the coarsely supervised text classification task. The first
five columns are taken from [32]. The last two columns use the GPT-2 prior defined in §4.5 as weak
supervision with cross-entropy and RQ loss, respectively (mean of 10 random initializations).
pseudolabeling	GPT-2 prior, trigram features
WeSTClass [33] COnWea [31] LOTClass [34] X-Class [51]	C2F[32] prior argmax CE RQ
Micro-F1 %	76.23	73.96	15.00	91.16	92.62	86.33	87.18	93.18
Macro-F1 %	69.82	65.03	20.21	81.09	87.01	77.61	77.90	84.26
4.5 Uncertain teachers: Text classification
This experiment follows the very recent work of [32] and illustrates the effectiveness of learning
on prior beliefs beyond computer vision. We work with a dataset of 〜12k New York Times news
articles. Each article belongs to one of20 fine categories (e.g., ‘energy companies’, ‘tennis’,‘golf’),
which are grouped into 5 coarse categories (e.g., ‘business’, ‘sports’). The goal is to train text
classifiers that predict fine labels, but only the coarse label for each article is available in training.
Some external knowledge about the fine categories is necessary to resolve the coarse labels into fine
labels. Past work on this problem [33; 31; 34; 51] has trained supervised models on pseudolabels
created by mechanisms such as propagation of seed words and querying large pretrained language
models. On the other hand, [32] create training data by sampling additional features (articles) from a
finetuned version of the large generative language model GPT-2 [41] conditioned on fine categories,
then tune a classifier based on the almost equally large model BERT [7] in a supervised manner.
We obtain comparable results with an elementary predictor, far less computation, and no finetuning
of massive language models. We form a prior Pi (I) on the fine class I of each article Xi by querying
GPT-2 for the likelihood of each fine category name I compatible with the known coarse label
following the prompt “[article text] Topic: " and normalizing over I. We then divide Pi (I) by
the mean likelihood of I over all articles Xi and renormalize. We represent each article as a vector
of alphabetic trigram counts (263 features, of which only 8k are ever nonzero) and train a logistic
regression with the RQ objective against the 'GPT-2 prior,. After ten epochs of training (〜10s on
our hardware), the trained classifier nears or exceeds the performance of models requiring at least
100× longer to train, not to mention to generate pseudo-training data for (Table 3).
5 Conclusions
We found that the generative distribution in a free energy criterion can be left implicit to the mini-
mization process in posterior (discriminative) model training. This allowed us to unite the training
of neural networks q (I ∣Xi; θ) for prediction of labels I from features X and the modeling of the prior
Pi (I), possibly with its own latent structure. Implicit modeling of the conditional generative dis-
tributions removes the burden of training accurate (and therefore large or deep) generative models,
but allows natural generative approaches to modeling priors. We expect interesting new modeling
paradigms that integrate different modalities and models to arise from this formulation.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement
This submission comes with two code directories that illustrate our algorithms for partial-label learn-
ing and weakly supervised segmentation. The provided code is sufficient to reproduce results re-
sembling those in Fig. 1. Upon publication, we intend to release code for all land cover mapping
experiments that is compatible with the TorchGeo library.
Ethics statement
The authors anticipate that the immediate consequences arising from this work will have nonnegative
social impacts. In general, learning from weak, coarse, or imprecise data may be susceptible to data
biases in differential quality or precision in data sources across sub-populations or geographies. We
encourage further work to examine these issues, especially in cases where uncertain labels are used
in machine learning systems that impact people or decision-making systems. Overall, we hope that
the ability to learn from a variety of weak data sources can aid applicability of machine learning
systems in diverse settings.
References
[1]	Qianyue Bao, Yang Liu, Zixiao Zhang, Dafan Chen, Yuting Yang, Licheng Jiao, and Fang
Liu. Mrta: Multi-resolution training algorithm for multitemporal semantic change detection.
International Geoscience and Remote Sensing Symposium (IGARSS), 2021.
[2]	Geoff Boeing. Osmnx: New methods for acquiring, constructing, analyzing, and visualizing
complex street networks. Computers, Environment and Urban Systems, 65:126-139, 2017.
[3]	S. Caelles, K.K. Maninis, J. Pont-Tuset, L. Leal-Taixe, D. Cremers, and L. Van GooL One-shot
video object segmentation. Computer Vision and Pattern Recognition (CVPR), 2017.
[4]	Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethinking space-time networks with
improved memory coverage for efficient video object segmentation. Neural Information Pro-
cessing Systems (NeurIPS), 2021.
[5]	J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang. Fast and accurate online video
object segmentation via tracking parts. Computer Vision and Pattern Recognition (CVPR),
2018.
[6]	A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society B, 39(1):1-38, 1977.
[7]	Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. North American Chapter of the
Association for Computational Linguistics (NAACL), 2019.
[8]	Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Neural Information
Processing Systems (NeurIPS), 2014.
[9]	Mordechai Haklay and Patrick Weber. OpenStreetMap: User-generated street maps. IEEE
Pervasive Computing, 7(4):12-18, 2008.
[10]	Junwei Han, Dingwen Zhang, Gong Cheng, Lei Guo, and Jinchang Ren. Object detection
in optical remote sensing images based on weakly supervised learning and high-level feature
learning. IEEE Transactions on Geoscience and Remote Sensing, 53(6):3325-3337, 2014.
[11]	Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. International
Conference on Computer Vision (ICCV), 2017.
[12]	Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The ”wake-sleep” algorithm
for unsupervised neural networks. Science, 268 5214:1158-61, 1995.
10
Under review as a conference paper at ICLR 2022
[13]	Le Hou, Vu Nguyen, Ariel B Kanevsky, Dimitris Samaras, Tahsin M Kurc, Tianhao Zhao, Ra-
jarsi R Gupta, Yi Gao, Wenjin Chen, David Foran, et al. Sparse autoencoder for unsupervised
nucleus detection and representation in histopathology images. Pattern Recognition, 2019.
[14]	Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon.
Tile2vec: Unsupervised representation learning for spatially distributed data. Association for
the Advancement of Artificial Intelligence (AAAI), 2019.
[15]	Rong Jin and Zoubin Ghahramani. Learning with multiple labels. Neural Information Pro-
cessing Systems (NeurIPS), 2002.
[16]	Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan, and Michael Fels-
berg. A generative appearance model for end-to-end video object segmentation. Computer
Vision and Pattern Recognition (CVPR), 2019.
[17]	Nebojsa Jojic, Alessandro Perina, Marco Cristani, Vittorio Murino, and Brendan Frey. Stel
component analysis: Modeling spatial correlations in image class structure. In 2009 IEEE
conference on computer vision and pattern recognition, pp. 2044-2051. IEEE, 2009.
[18]	A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele. Lucid data dreaming for object
tracking. The 2017 DAVIS Challenge on Video Object Segmentation - CVPR Workshops, 2017.
[19]	Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. NLNL: Negative learning for
noisy labels. International Conference on Computer Vision (ICCV), 2019.
[20]	Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
[21]	Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. International Con-
ference on Learning Representations (ICLR), 2014.
[22]	Zhuohong Li, Fangxiao Lu, Hongyan Zhang, Guangyi Yang, and Liangpei Zhang. Change
cross-detection based on label improvements and multi-model fusion for multi-temporal re-
mote sensing images. International Geoscience and Remote Sensing Symposium (IGARSS),
2021.
[23]	Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,
Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. Euro-
pean Conference on Computer Vision (ECCV), 2014.
[24]	Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
[25]	Jonathon Luiten, Paul Voigtlaender, and Bastian Leibe. Premvos: Proposal-generation, re-
finement and merging for video object segmentation. Asian Conference on Computer Vision
(ACCV), 2018.
[26]	Oisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-only geographical priors for fine-
grained image classification. International Conference on Computer Vision (ICCV), 2019.
[27]	Nikolay Malkin, Caleb Robinson, Le Hou, Rachel Soobitsky, Jacob Czawlytko, Dimitris
Samaras, Joel Saltz, Lucas Joppa, and Nebojsa Jojic. Label super-resolution networks. In-
ternational Conference on Learning Representations (ICLR), 2019.
[28]	Nikolay Malkin, Anthony Ortiz, and Nebojsa Jojic. Mining self-similarity: Label super-
resolution with epitomic representations. European Conference on Computer Vision (ECCV),
2020.
[29]	Kevis-Kokitsi Maninis, Sergi Caelles, YUhUa Chen, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel
Cremers, and Luc Van Gool. Video object segmentation without temporal information. IEEE
Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018.
[30]	Tim Meinhardt and Laura Leal-Taixe. Make one-shot video object segmentation efficient
again. Neural Information Processing Systems (NeurIPS), 2020.
11
Under review as a conference paper at ICLR 2022
[31]	Dheeraj Mekala and Jingbo Shang. Contextualized weak supervision for text classification.
Association for Computational Linguistics (ACL), 2020.
[32]	Dheeraj Mekala, Varun Gangal, and Jingbo Shang. Coarse2Fine: Fine-grained text classifica-
tion on coarsely-grained annotated data. Empirical Methods in Natural Language Processing
(EMNLP), 2021.
[33]	Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. Weakly-supervised neural text classi-
fication. International Conference on Information and Knowledge Management, 2018.
[34]	Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei
Han. Text classification using label names only: A language model self-training approach.
Empirical Methods in Natural Language Processing (EMNLP), 2020.
[35]	Nam Nguyen and Rich Caruana. Classification with partial labels. Knowledge Discovery and
Data Mining (KDD), 2008.
[36]	Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation
using space-time memory networks. International Conference on Computer Vision (ICCV),
2019.
[37]	F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung.
A benchmark dataset and evaluation methodology for video object segmentation. Computer
Vision and Pattern Recognition (CVPR), 2016.
[38]	Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and Alexander Sorkine-
Hornung. Learning video object segmentation from static images. Computer Vision and Pat-
tern Recognition (CVPR), 2017.
[39]	Brian R Pickard, Jessica Daniel, Megan Mehaffey, Laura E Jackson, and Anne Neale. Envi-
roAtlas: A new geospatial tool to foster ecosystem services science and resource management.
EcoSyStem Services,14:45-55, 2015.
[40]	Andrew Pilant, Keith Endres, Daniel Rosenbaum, and Gillian Gundersen. US EPA EnviroAtlas
meter-scale urban land cover (MULC): 1-m pixel land cover class definitions and guidance.
Remote SenSing, 12(12), 2020.
[41]	Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. 2019.
[42]	Caleb Robinson, Le Hou, Nikolay Malkin, Rachel Soobitsky, Jacob Czawlytko, Bistra Dilkina,
and Nebojsa Jojic. Large scale high-resolution land cover mapping with multi-resolution data.
Computer ViSion and Pattern Recognition (CVPR), 2019.
[43]	Caleb Robinson, Anthony Ortiz, Nikolay Malkin, Blake Elias, Andi Peng, Dan Morris, Bis-
tra Dilkina, and Nebojsa Jojic. Human-machine collaboration for fast land cover mapping.
ASSociation for the Advancement of Artificial Intelligence (AAAI), 2020.
[44]	Matthias Seeger. Learning with Labeled and Unlabeled Data. 2002. URL https:
//infoscience.epfl.ch/record/161327/files/review.pdf.
[45]	Balaji Lakshminarayanan Shakir Mohamed. Learning in implicit generative models. Interna-
tional Conference on Learning RepreSentationS (ICLR), 2017.
[46]	Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via
information. arXiv preprint arXiv:1703.00810, 2017.
[47]	Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let
networks learn high frequency functions in low dimensional domains. Neural Information
ProceSSing SyStemS (NeurIPS), 2020.
[48]	TorchGeo contributors. TorchGeo. https://github.com/microsoft/torchgeo,
2021.
12
Under review as a conference paper at ICLR 2022
[49]	Paul Voigtlaender and Bastian Leibe. Online adaptation of convolutional neural networks for
video object segmentation. British Machine Vision Conference (BVMC), 2017.
[50]	Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-
Chieh Chen. FEELVOS: Fast end-to-end embedding learning for video object segmentation.
Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
[51]	Zihan Wang, Dheeraj Mekala, and Jingbo Shang. X-Class: Text classification with extremely
weak supervision. North American Chapter of the Association for Computational Linguistics
(NAACL), 2021.
[52]	Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos K. Katsaggelos. Effi-
cient video object segmentation via network modulation. Computer Vision and Pattern Recog-
nition (CVPR), 2018.
[53]	Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by
foreground-background integration. European Conference on Computer Vision (ECCV), 2020.
[54]	Yao Yao, Jiehui Deng, Xiuhua Chen, Chen Gong, Jianxin Wu, and Jian Yang. Deep discrimina-
tive CNN with temporal ensembling for ambiguously-labeled image classification. Association
for the Advancement of Artificial Intelligence (AAAI), 2020.
[55]	Fei Yu and Min-Ling Zhang. Maximum margin partial label learning. In Asian conference on
machine learning, pp. 96-111. PMLR, 2016.
[56]	Xiao Zhang, Yixiao Ge, Yu Qiao, and Hongsheng Li. Refining pseudo labels with clustering
consensus over generations for unsupervised object re-identification. Computer Vision and
Pattern Recognition (CVPR), 2021.
[57]	Zhuo Zheng, Yinhe Liu, Shiqi Tian, Junjue Wang, Ailong Ma, and Yanfei Zhong. Weakly su-
pervised semantic change detection via label refinement framework. International Geoscience
and Remote Sensing Symposium (IGARSS), 2021.
[58]	Naiyun Zhou, Xiaxia Yu, Tianhao Zhao, Si Wen, Fusheng Wang, Wei Zhu, Tahsin Kurc, Allen
Tannenbaum, Joel Saltz, and Yi Gao. Evaluation of nucleus segmentation in digital pathology
images through large scale image synthesis. In Medical Imaging 2017: Digital Pathology,
volume 10140. International Society for Optics and Photonics, 2017.
[59]	Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian, Jia-Bin Huang, and
Tomas Pfister. Pseudoseg: Designing pseudo labels for semantic segmentation. arXiv preprint
arXiv:2010.09713, 2020.
13
Under review as a conference paper at ICLR 2022
A	Practical Considerations
Mini-batches: Figure 2 shows a PyTorch implementation of the QR and RQ loss functions, where
loss is computed over batches of training data. Our experiments validate that so long as these batches
are large enough to include enough diversity of (xi, Pt (/)) pairs, our method works when eq. (2) and
eq. (3) are applied directly to batches. As discussed in §4.4, handling batched input is important
for leveraging the scale of large training datasets. As discussed in the caption of Figure 2, should
mini-batch training become an issue in future implementations, it may be beneficial to estimate the
denominator of eq. (2) across multiple batches.
Relative benefits/limitations of the QR and RQ loss formulations: The algorithm presented in
§2.1 details two loss options: a QR option and an RQ option, both with unique strengths. The QR
algorithm is guaranteed to converge as each step reduces loss (except for randomness in the learning
algorithm). The RQ algorithm, on the other hand, has the appealing property that it reduces to
standard minimization of cross entropy loss in the case of hard labels. In §C, we discuss connections
between QR option and variational auto-encoders (VAEs), and between the RQ option and the wake-
sleep algorithm. Ultimately, though, we find that which option works better may depend on the
application, with RQ working across all applications we tried but sometimes being slightly beaten
by QR.
Simple ways to avoid degenerate solutions As discussed in §2.1, minimizing eq. (1) can lead
to degenerate solutions. However, avoiding these solutions can be quite simple, and in most of our
experiments we did not make any interventions to explicitly avoid such local minima. In a targeted
experiment in table D.1 we show that pre-training on hard labels (even out-of-domain) or using
sharper learned priors can help break symmetries during early training phases. When hard labels
are not available, one could similarly start the training process with a cross-entropy loss on the prior
belief, and then switch toRQ or QR loss. The intuition is that first training to minimize cross-entropy
breaks the symmetry at the start, while implicit generative modeling sharpens the predictions in later
iterations.
B Additional Related Work
There are several approaches to learning with uncertain, weak, or coarse labels under different as-
sumptions and settings. Work on partial-label learning often employs loss functions that aim to
decrease prediction entropy [35; 54; 55]; In contrast to our work, these approaches do not use a
generative formulation in these loss functions, makes them ill-suited to problems with more var-
ied forms of uncertainty encoded in priors. Another line of work studies the generation and use of
pseudo labels in learning settings. Specifically, [59] relies on a domain-specific augmentation pro-
cedure for semantic segmentation with image-level labels, and, [56] studies unsupervised clustering
applied to object re-identification. Application-specific solutions also include object detection in
remote sensing images [10] and change detection with temporal satellite imagery [57; 1; 22].
In our experimental setups, we chose a mix of baselines to compare algorithm design and benchmark
performance on certain tasks.
To compare our approach on an algorithmic basis, we compare to the negative logarithm of the sum
of likelihoods (NLL), which is used in prior works to handle multiple ambiguous labels [15] and
negative labels [19]. As We discuss in §4.1, NLL and RQ are equivalent when Ei qi(I) is uniform
over I, evidenced by the comparable performance between the two in Figure 3. We compare to self-
epitomic LSR [28] as an algorithmic comparison by which to contrast our method with an “explicit”
generative modeling approach. Our similar performance to self-epitomic LSR in regimes where
self-epitomic LSR has been shown to perform well (super-resolution in land cover mapping (§4.3)
and the tumor-infiltrating lymphocytes task (§E.2)) is an important validation of our motivation in
§2.
To benchmark performance of our approach across tasks, we compare to state-of-the-art pseudo-
labeling methods in supervised text classification (see §4.5), an established 1m resolution map
of land cover predictions across the United States [42] and best-performing published results for
the land cover mapping tasks we study [28; 43], the best known published results for the tumor-
14
Under review as a conference paper at ICLR 2022
infiltrating lymphocyte segmentation task ([27; 28]), and a host of comparisons for the video instance
segmentation task (see Table E.3 for a full list).
Lastly, it is worth noting that the term “implicit generative model” has also been used in prior
literature to refer to amortized sampling procedures for nonparametric (or not specified) energy
functions, such as generative adversarial models (e.g., [45]). Although we do not make an explicit
connection with such models, our formulation also does not assume a parametrization of the data
distribution. However, we assume tractability of sampling from a posterior over certain distinguished
latents (classes) conditioned on observed data (features, e.g., images), rather than directly sampling
latents.
C Relationships with EM, VAEs, and wake-sleep algorithm
As discussed in §2.1, the QR loss guarantees continual improvements in the free energy (1). On the
other hand, option RQ is equivalent to performing a gradient step on the cross-entropy of qi and ri
and a gradient step on the negative entropy of r. In the case that the priors Pi (I) are hard (supported
only on one ground truth label), the same is true of ri, and the RQ loss is equivalent to cross-entropy.
This option reverses the KL distance in a manner reminiscent of the training procedure in the wake-
sleep algorithm [12], where parameter updates for the forward and reverse models are iterated, but
the KL distance optimized always places the probabilities under the model being optimized in the
second position in the KL distance (inside the logarithm), so that the generative and the inference
models each optimize log-likelihoods of their predictions. The wake-sleep algorithm, however, also
trains a generative model rather than treating it as an auxiliary distribution like we do, and that
requires sampling. As opposed to VAEs, the wake-sleep algorithm samples the generative model,
not the posterior.
It is interesting to contrast our approach to the EM formulation. In standard EM, the q distributions
are considered auxiliary, rather than parametrized as direct functions of the inputs ɪ. The qi(I) is
simply a matrix of numbers normalized across I. Its dependence on the data X is only implicit in the
iterative re-estimation of the minimum of the free energy. The link X -1 is modeled explicitly in the
parametrized forward distribution P(x|l). We instead treat forward probabilities P(Xi |l) as auxiliary
parameters, a matrix of numbers pi,ι normalized across i that We fit to minimize the free energy at
each data point, and optimize only the parameters of the q model which explicitly models the link
x-I. This then allows us to capture nonlinear (and 'deep')structure and benefit from inductive biases
inherent to training deep models with SGD, but without the cost of training an actual parametrized
generative model and other problems associated with deep generative model fitting. The resulting
q network approximates the posterior in a generative model - which (locally) maximizes the log
likelihood of the data - and it is usually highly confident (as seen in Fig. 1).
The implicit modeling of the posterior in EM does not lead to over-fitting of the generative model.
But, given that degenerate solutions to optimization with implicit generative models are possible
when the prior is constant across all data points (§2.1), we can imagine that our approach of im-
plicit generative modeling might lead to degenerate solutions. As demonstrated in Fig. 1 and in our
experiments, avoiding degenerate solutions is not too hard. We attribute this to two factors. First,
the situations of interest typically involve uncertain, but varying distributions Pi (I) which break
symmetries that could lead to ignoring the data features Xi as in the example above of a constant
prior. Second, the neural networks to be used as posterior models q come with their own constraints
in parametrization, and, equally importantly, the inductive biases that come as a consequence of
gradient descent learning in nonlinear, multilayer networks.
D Experiment details
D.1 Land cover mapping
D.1.1 Datasets
Imagery Data Our land cover mapping experiments use imagery from the National Agriculture
Imagery Program (NAIP), which is 4-channel aerial imagery at a ≤ 1m/px resolution taken in the
United States (US).
15
Under review as a conference paper at ICLR 2022
Chesapeake Conservancy land cover dataset The Chesapeake Conservancy land cover dataset
consists of several raster layers of both imagery and labels covering parts of6 states in the Northeast-
ern United States: Maryland, Delaware, Virginia, West Virginia, Pennsylvania, and New York [42]3.
The raster layers include: high resolution (1m/px) NAIP imagery, high resolution (1m/px) land
cover labels created semi-autonomously by the Chesapeake Conservancy, low resolution (30m/px)
Landsat-8 mosaics imagery, low resolution (30m/px) land cover labels from the National Land Cover
Database (NLCD), and building footprint masks from the Microsoft Building Footprint dataset. The
dataset is partitioned into train, validation, and test splits per-state, where each split is a set of
≈ 7km × 6km tiles containing the aligned raster layers.
EPA EnviroAtlas data The EnviroAtlas land cover data consists of high resolution (1m/px) land
cover maps over 30 cities in the US, and is collected and hosted by the US Environmental Protection
Agency (EPA) [39]. A detailed description of the dataset and its land cover definitions is provided
by [40]. As with most high-resolution land cover datasets (including the Chesapeake Conservancy
land cover labels), the EnviroAtlas land cover labels are themselves derived by remote sensing and
learning procedures, and thus are not themselves a perfect “ground truth” representation of land
cover. For example, the estimated accuracy of the provided labels is 86.5% in Pittsburgh, PA, 83.0%
in Durham, NC, 86.5% in Austin, TX, and 69.2% in Phoenix, AZ [40].
The high-resolution label files were aligned to match the extent of the NAIP tiles from the clos-
est available years to the years that the EnviroAtlas labels were collected: for Pittsburgh, PA and
Phoenix, AZ, we used data from 2010 and for Durham, NC and Austin, TX, we used data from
2012. We chose these four cities to get a wide coverage across the United States (US), and due to a
mostly consistent set of classes being used between the four cities.
National Land Cover Database (NLCD) The National Land Cover Database is produced by the
United States Geological Survey (USGS) and uses 16 land cover classes. Maps are generated every
2-3 years, with spatial resolution of 30m/px. Data and more information can be found at: https:
//www.usgs.gov/centers/eros/science/national-land-cover-database.
Microsoft Building Footprint dataset The Microsoft Building Footprint dataset consists of pre-
dicted building polygons over the continental US from Bing Maps imagery. As of the time of
writing, the most updated Microsoft Building Footprints dataset in the US can be accessed at:
https://github.com/Microsoft/USBuildingFootprints.
Open Street Map (OSM) data Open Street Map (https://www.openstreetmap.org/)
is an ongoing effort to make publicly available and editable map of the world, generated largely
from volunteer efforts. The data is available under the Open Database License. From the many
different sources of information provided by OSM [9], we download raster data for road networks,
waterways, and water bodies, using the OSMnx python package [2].
Data splits and data processing For experiments using the Chesapeake Conservancy dataset (Ta-
ble 1), we used established train, test, and validation splits. In particular, we used the 20 test tiles in
New York (NY) and the 20 test tiles in Pennsylvania on which to conduct our experiments. Here a
tile matches the extent of a NAIP tile, roughly 7km × 6km. To facilitate comparison of our results
with previous published results on this dataset, we condensed the labels into four classes: (1) water,
(2) impervious surfaces (roads, buildings, barren land), (3) grass/field, and (4) tree canopy.
For experiments with the EnviroAtlas dataset (Table 2), we aligned the high resolution land cover
data, NLCD, OSM, and Microsoft Building Footprints data with NAIP imagery tiles, matching years
as closely as possible to the EnviroAtlas data collection year for NLCD and NAIP. We instantiated a
split of 10 train, 8 validation, and 10 test tiles in Pittsburgh, and 10 test tiles in Durham, NC, Austin,
TX, and Phoenix, AZ. For Pittsburgh we assigned tiles to splits randomly from the set of 28 tiles that
had no missing labels. There were not enough such tiles in Durham to follow the same procedure,
so we chose the ten evaluation tiles at random from a set with no number of missing labels per
tile. For Austin and Phoenix, we chose the 10 evaluation tiles at random from the tiles in each city
that had no agriculture class (as it is not present in Pittsburgh or Durham) and no missing labels.
3Dataset	can be downloaded from:	https://lila.science/datasets/
chesapeakelandcover.
16
We Sef aside 5 SeParafe files in each Cify for USe in -earning fhe PriO 飞(in PmSbUrghfheSe 5≡es
are a SUbSef Offhe 8 VaHdafiOnfiles) ∙>'above。each≡e COrreSPOndSOOne NAlP≡e∙ The files
in these COnSfrUefed SefS for PiffSbUrgk Durham。and AUSfin COmain Hve UniqUe IabelS: (I) WafeL
(2) impervious SUrfaCeS (road∞buildings)" (2) barren Iana (4) grassmeH and (5) frees∙ PhOeniX
additionally has CeShrUbs class; When forming fhe PriOrWe merge fhis ClaSS Wifh frees。and We ignore
fhe ShrUbelaSS When evaluating in PhOenix∙ We CrOPPed all dafa files fo ensure no SPafiaI OVerlaP in
any files between Or Wifhin fhe frain/valAeSf SPHfS ∙
D∙l∙2 FORMlNGTHEPRIORS
TO form fhe PriorS for fhe Iand COVer ClaSSifiCafiOn fask∞we Hrsf Spafially SmoOfh fhe NLCD IabeIS
by applying a 2D GaUSSianBfer (Wifh Sfandard deviation 31 PiXelS) across every ChanneIin a one—
hof represenfafion Offhe NLCD ClaSSeS ∙ The main reason for applying fhis SmOofhing isoreduce
arfifacfs dueofhe 30m2 boundaries Offhe NLCD dafa 二O UndO fhe blocking PrOCedUre induced by
fhe aggregationO30m × 30m exfenFoincorporafe fhe SPafiaI COrrelafiOnS befween nearby NLCD
blocky andOremove erroneous SharP differentials befween inpufs fhaf Can CaUSe arfifacfs during
Iaferfraining SfageS ∙
We fhen remap fhe blurred NLCD IayerSofhe CIaSSeS OfinfereSf by mulfipIying by a matrix Of co—
OCCUrrenCe COUmS befween fhe (UnbIUrred) NLCD dafa and fhe high resolution IabeiSin each region∙
FOrfhe CheSaPeake regio尸 We USe fhe train files ProVided Wifhfhe CheSaPeake COnSerVanCy Iand
COVer dafase 二 O define CO—occurrence matrices in NY and PA∙ FOr EnVirOAfla∞We COmPUfe co—
OCCUrrenCeS USing fhe enfire Cify (eXClUding files Wifh agriculfure in PhOeniX Az。and AUSfm TX)∙
The CO—occurrence matrices for each region We SfUdy are ShOWn in FigUre D∙L
The PriorS forfhe CheSaPeake COnSerVanCy dafasef are fhen generafed by normaHZing fhe blurred
and remapped NLCD dafa SOfhaf SUmming OVer all Hve ClaSSeS gives Probab≡fy 1 for each PiXeL
17
UllderreVieW as a CoIlfereIlCe PaPer af ICLR 2022
Label class
NLCD c_ass
CheSaPeake Consewancy dasrsa
Co-Oi- Scae
En≤foa-as dasrsef
FigUre D∙h Cooccurrence matrices befween NLCD ClaSSeS and high resolufion land COVerlabelSfOr
each region We SfUdy・
Under review as a conference paper at ICLR 2022
For the EnviroAtlas data, we augment this prior with publicly available data on buildings, road
networks, water bodies, and waterways. We obtain building maps from the Microsoft Buildings
Footprint database and road, water bodies, and waterways data from Open Street Map, using the
OSMnx tool [2] to download the data (see Appendix D.1.1). We apply a small spatial blur to each
of these input sources to account for (a) vector representation of roads and waterways being unre-
alistically thin, and (b) possible data-image misalignment on the order of pixels. Where this results
in probability mass on impervious surfaces or water, we add these probability masses to the blurred
NLCD prior, and then renormalize to obtain a valid set of probabilities for each pixel.
In §4.4, we describe a method for “learning the prior,” which uses a more sophisticated process
to aggregate the individually weak and coarse inputs that we use in the handmade prior. In this
method, we train a neural net to take as input the blurred, remapped NLCD representation (5 classes)
concatenated with the 4 classes of additional data: buildings, roads, waterways, water bodies, and
to predict high-resolution labels in each city. We train these networks using 5 tiles of imagery and
high-resolution labels from the EnviroAtlas Dataset in each city which are distinct from the 10 test
tiles in each city. The training procedure for these prior generation networks is described in in
§D.1.3. To create the priors that we then train our method on (‘learned prior’ rows in Table 2) we
ran these learned models forward on (blurred and remapped NLCD, buildings, roads, waterways,
and waterbodies) input for each of the 10 evaluation tiles in each city.
D.1.3 Experimental procedure
We use priors generated as described in Appendix D.1.2, with Gaussian spatial smoothing standard
deviation of 31 pixels, and co-occurrence matrix determined via the training splits in each city/state.
We apply an additive smoothing constant of 1e-4 to that is applied pixel-wise the the probability
vectors output by the neural network as well as to the prior probability vectors used as the model su-
pervision data. This additive smoothing constant ensures that there are no extremely low probability
classes in either the prior or the predicted outputs during training.
Experiments summarized in Table 1 and Table 2 use a 5-layer fully connected network with kernel
sizes of3 at each layer, 128 filters per layer, and leaky ReLUs between layers. Note that the receptive
field of this model is only 11 × 11 pixels. We use batch sizes of 128 instances during training,
where each image instance is a cropped 128 × 128 pixels from a larger tile. Training is and model
evaluation is done within the torchgeo framework for geo-spatial machine learning [48]. All models
use the AdamW optimizer [24] during training and torchgeo defaults unless otherwise noted.
Comparison to previous label super-resolution for LC mapping To obtain the parameter set-
ting used for the runs in New York (NY) and Pennsylvania (PA) in Table 1, we first perform a
hyperparameter search with the 20 tiles test set in Delaware (DE) from the same overall dataset. We
use a learning rate schedule that decreases learning rate when the validation loss plateaus, as well as
early stopping to prevent over training of models. Of the grid of learning rates in [1e-3, 1e-4,1e-5],
we describe below, we pick learning rate as 1e-4 for both QR and RQ variants of our method, as
this is the setting that minimizes the IoU of the q output on the 20 DE tiles for both variants.
When training on NY and PA jointly (“Chesapeake” in Table 1), we use the per-state co-occurrence
matrices. This ensure that the co-occurrence matrices used are consistent between our method and
the self-epitomic LSR benchmark across all columns in Table 1.
Generalization across cities. For the high-resolution model with NAIP imagery from Pittsburgh
as input, we consider learning rates in {10-2, 10-3, 10-4, 10-5} and pick based on the best validation
performance on the validation set in Pittsburgh. The chosen learning rate is 1e-3. We search over the
same set of learning rates for the model with NAIP imagery and the prior concatenated as input; the
chosen learning rate is also 1e-3. For this model with concatenated image and prior as input, only
the number of input channels changes in the fully connected network model architecture. When
training on the high-resolution land cover labels, we use a very small additive constant 1e-8 for the
last layer of the model.
When training our method, we initialize model weights using the best NAIP image input model from
the Pittsburgh validation set runs, and then train using the priors and the training procedure described
in the main text. We pick the learning rate for this training step using again the validation set in
Pittsburgh; we search learning rates in {10-3, 10-4, 10-5}, and pick 1e-5 as the learning rate, since
18
Under review as a conference paper at ICLR 2022
resulted in the best performance for both QR and RQ in the Pittsburgh validation set. We discuss
the results of a similar procedure using randomly initialized model weights in Appendix D.1.4.
For the learned prior, we use a 3 layer fully connected network is kernel sizes of 11,7, 5 respectively,
128 filters per layer and leaky ReLUs between layers. For each city, we train this model on the
prior inputs (blurred and remapped NLCD, roads, buildings, waterways, and water bodies) using
a validation set of 5 tiles separate from from the 10 evaluation tiles in each city. We considered
learning rates in {10-3, 10-4, 10-5} for learning the prior in each city, and chose 1e-4 as it gave most
often resulted in the highest accuracies of each validation set. For learning on this learned prior, we
searched in a condensed space of learning rate in{10-4, 10-5}, and chose 1e-5 for evaluation runs in
each city based on validation set performance in Pittsburgh, PA.
Table D.1: Supplementary results to accompany Table 2.
Train region	Model	Pittsburgh, PA		Durham, NC		Austin, TX		Phoenix, AZ	
		acc %	IoU%	acc %	IoU%	acc %	IoU%	acc %	IoU%
Pittsburgh	HR	89.55	70.03	72.87	34.83	71.86	36.81	6.69	7.49
(supervised)	HR + aux	89.48	70.75	78.86	47.67	77.20	50.62	66.04	27.92
Same as test	QR ⑷^^	80.46	56.81	77.77	43.92	79.59	51.93	72.20	21.86
(random	QR （厂）	80.62	57.44	77.91	44.86	79.59	52.30	72.39	22.32
initialization)	RQ⑷	51.74	10.35	70.63	26.83	74.32	45.99	66.96	19.16
	RQ （厂）	2.95	0.59	70.63	26.83	74.33	46.00	66.96	19.16
Same as test	QR ⑷^^	85.41	64.71	78.90	46.56	79.28	51.67	73.21	39.09
(pretrained	QR （厂）	85.38	65.74	79.09	48.80	79.63	53.01	73.44	40.62
in Pittsburgh)	RQ⑷	87.35	67.88	78.80	44.43	78.65	51.90	70.59	34.94
	RQ （厂）	87.45	68.59	78.83	44.61	78.79	52.47	70.70	36.10
Same as test	QR ⑷^^	87.21	68.54	78.74	47.35	79.83	51.10	74.57	39.59
(learned prior )	QR （厂）	86.97	69.27	78.93	49.57	80.27	52.60	75.31	42.22
Full US* [42]	U-Net Lrg.	79.51	61.62	77.35	48.78	76.40	51.35	24.49	23.21
D.1.4 Additional Results
Table D.2: Comparison of the Full US* U-Net Large [42] map predictions when evaluated on the
full 5 classes considered in Table 2 (water, grass/field, trees/shrub, impervious surfaces, and barren
land) and evaluated on the four prediction classes predicted by the model (where barren land and
impervious surfaces are merged as a single class), and when barren is post-facto assigned whenever
the predicted class is “impervious surfaces” and the label class is “barren land”.
Pittsburgh, PA Durham, NC Austin, TX Phoenix, AZ
Classication Scheme	acc %	IoU %	acc %	IoU %	acc %	IoU %	acc %	IoU %
5 Classes	79.29	55.01	76.86	42.62	76.05	48.80	18.06	18.49
4 Classes	79.29	68.76	77.35	53.13	76.40	59.84	24.49	16.33
Barren reassigned	79.51	61.62	77.35	48.78	76.40	51.35	24.49	23.21
Extended results for generalizing across EnviroAtlas cities. The extended results for generaliz-
ing across cities with the EnviroAtlas datasets in Table D.1 contain the results of the RQ runs trained
on the handmade prior in each city, and additionally shows the evaluation results in Pittsburgh, PA,
to give further context full comparison of generalization across cities by each method.
Table D.1 also details the result of initializing the model weights randomly for the QR method.
For this experiment, we again search over learning rate in [1e-3,1e-4,1e-5] in Pittsburgh, this time
resulting in a choice of 1e-4 for the QR evaluation runs and 1e-3 for the RQ evaluation runs. This
seems to be a suboptimal choice of learning rate for the RQ method in other cities (see Table 2), so
the most fruitfull comparison between initialization schemes is between the QR variant. Table D.1
shows that the choice of model initialization can be important for our method - this is most apparent
19
Under review as a conference paper at ICLR 2022
Figure D.2: Example predictions on the hand-coded and learned prior in each EnviroAtlas city we
study.
in Pittsburgh, PA (unsurprisingly since the high-resolution model was trained in Pittsburgh) and
Phoenix, AZ. In Phoenix, much of the handmade prior is consistent across geographies and the
randomly initialized model has trouble distinguishing between infrequent classes that most often
occur together in the handmade prior. The results in Table D.1 suggest that using pre-trained models
as a starting point for our method can help to break some of these symmetry issues in resolving the
information in the prior. Results in Table 2 suggest that using a more detailed prior map may help
with this as well.
Evaluating the Full US map from [42]. Recall that the row for the full US Map [42] in Table 2
reflects the performance of the model evaluated on all 5 classes we consider in our experiments,
where we give the map predictions the “benefit of the doubt” in that any prediction of “impervious
surfaces” where the true label is “barren land” gets assigned a correct classification of “barren land.”
The results reported in Table 2 are thus a sort of upper bound on the predictive performance of the
method that generated the predictive maps. It was important for us to keep the barren class while
evaluating across cities, as it is the dominant class in Phoenix, AZ. In the remaining three cities, the
barren class is challenging to predict as it is infrequent. In Table D.2, we compare this classification
scheme with two alternatives: a 5 class scheme that will penalizes the map predictions for never
predicts the barren class, and a 4 class scheme that merges the barren land and impervious surfaces
classes in evaluation. Table D.2 shows that while the choice of evaluation scheme does not greatly
effect accuracy (outside of Phoenix, AZ, where the accuracy of the Full US Map is low for both
classification schemes), the average IoU drops significantly for all cities apart from Phoenix.
Comparing loss functions: qualitative results with land cover mapping. Figure D.3 compares
predictions under different loss functions with an illustrative example. Here the prior is similar to the
“hand-coded” prior described in Appendix D.1.2, but with the prior defined over all NLCD classes.
We train each model (a slight variant on the network used in experimental results) on the single NAIP
tile region encompassing the zoom-in in the figure for 2000 iterations with the Adam algorithm [20],
a batch size of 64, and a learning rate fixed at 1e-4 during training. Qualitative comparisons show
that predictions made by the QR and RQ loss functions are more certain (sharper colors in plots)
than training with cross entropy or squared-error loss on the soft priors, and, in in most places, arrive
at better solutions than training with a standard cross entropy loss on the argmax of the prior.
E	Additional experiments
E.1	Self-supervision for unsupervised image clustering
Neural networks are usually trained on large amounts of hard-labeled data {xi, li}, yet, due to the
biases induced by the typical architectures and learning algorithms, a lot of the modeling power of
20
Under review as a conference paper at ICLR 2022
naip image	preds using cross-entropy loss on argmaxed prior	argmaxed prior
PredS using cross-entropy loss on soft prior	soft prior
NLCD Legend
Open Water
Developed Open Space
Developed Low Intensity
Developed Med. Intensity
Developed High Intensity
Barren Land
Deciduous Forest
Evergreen Forest
Mixed Forest
Shrub/Scrub
Grassland/Herbaceous
Pasture/Hay
Cultivated Crops
Woody Wetlands
Emergent Herbaceous
Wetlands
========
preds using squared loss on soft prior	soft prior
r preds using QR loss
q preds using QR loss	soft prior
r = preds using RQ loss
q preds using RQ loss	soft prior
Figure D.3: Comparison of different loss functions on hard and soft prior.
21
Under review as a conference paper at ICLR 2022
Imagery, X
P(XW = Water) P(X\£ = Trees)	P(Xl2 = Low Veg) P(XW = Impervious)
Implied p
Epitome model
Figure D.	4: Comparison of forward model likelihoods under the implicit generative model trained
with QR loss (above) and the likelihood under an epitome model [28] for part of a test tile from
§4.3.
these networks seem to focus on correlations in the input space [46]. This means that a network
trained for one application, i.e., for one label space I ∈ Li, can be adopted to another application,
i.e., a different labels space I ∈ L2, as long as the input features are in a similar domain. The
canonical example of this is the use of lower levels of the networks pre-trained on ImageNet as
part of the networks solving a completely different set of image classification problems. Pretrained
networks require smaller training sets in fine tuning, as long as they have learned to represent the
variation in the input space well. Self-supervised models attempt to go a step further and learn these
representations without any labels. In our framework, self-supervision can simply be seen as the
appropriate choice of subset priors P (CT) over appropriately chosen tuples of labels.
To discuss the pitfalls and opportunities, consider the QR loss (4)
F = - 2 qi (I) log Pi (I)-+ 2 qi (I) log ?幻(I) ∙	(6)
i,l	i,l	J
If We were to simply set Pi(I) to a constant (e.g., uniform) distribution P (I) for all data points i,
then the optimal solution would be any function qi(I) = q(l由)such that = k q(l由)=P(I).
Thus simply using the uniform prior may not lead to appropriate unsupervised clustering (or self-
supervised learning of the network q). The inductive biases in the network architecture and training
may not help, because one solution is q (l∣x) = P (I), which can be achieved by zeroing out all
weights except for biases in the last softmax layer that outputs probabilities for labels l. As the
softmax bias vector is the closest to the top in gradient descent, it will quickly be learned to match
log P(l) and this will not only slow down the propagation of gradients into the network, but can
eventually stop it completely, as this solution is a global optimum. Another optimal solution would
be a function satisfying 春》q(l|xi) = P (l), but where individual entropies for each data point are
small: - 2ι q (l|xj log q (l |xi) < e, which motivates an alternative cost criterion
F = - 2 qi (l) log qi (l) + 2 qi (l) log(2 q7(l))∙	⑺
where the first term promotes certainty in predictions q(l|xi) for each point i and the second is pro-
moting the diversity of the predictions across the different inputs, i.e., a high entropy of the average
= 2用 qi(九).This prevents learning a network with a constant output q(九)=P(九)and forces the
model to find some statistics in the input data that break it into clusters indexed by labels l. The
result will be highly dependent on the inductive biases associated with the network architecture and
SGD method used, as we can imagine degenerate solutions here as well. For example, we can ig-
nore completely some subset of features and still train a network that is certain in its modeling of
the remaining ones, and achieves a high diversity of predicted classes across the dataset. Obviously,
this may be dangerous if the features omitted end up being the most important ones for the down-
stream task. However, due to the stochastic gradient descent training as well as their architecture, it
has been difficult to prevent neural networks from learning statistics involving all the input features.
For example, training a neural network using a weak generative model as a teacher corresponds to
22
Under review as a conference paper at ICLR 2022
using a simpler mixture model, whose posterior is used as a target Pi (I) and then learning a neural
network that can approximate it. The inductive bias then leads to networks that do not match Pi (I)
exactly but learn more complex statistics instead.
Equation (7) can be seen as a degenerate example of using a tuple prior where the tuple has the
same data point repeated and the prior simply expects the two predictions to be the same. In many
applications, there are natural constraints involving multiple data points that are easily modeled with
priors over tuples or over the entire collection of labels. Consider unsupervised image segmentation,
for an example. It is usually expected that nearby pixels should belong to the same class (or a small
subset of classes), and that faraway pixels are more likely to belong to a different subset of classes.
This belief is typically modeled in terms of Markov random field models of joint probabilities of
labels in the image,
P({li})e exp2 Φ(li, {lj}j∈N1).	⑻
i
We experimented with potentials of the form
φ(Ii = l, {lj}j∈nP = yι + ^iτyτ 2 ɪ[1 = lj]+ βι~∖T~∖ Σ ɪ[1 = lj],
»1/ ∈*	1 i 1 j ∈Li
(9)
where for pixel i, Si is a small (5 × 5) neighborhood around it and Li is a larger (50 × 50) neighbor-
hood. If we set aι = 1, βι = -1 for all l, then We consider this a contrastive prior, as it favors labels
li to match the labels found more concentrated in its immediate neighborhood than in the larger
scope. On the other hand aι, and βι can be estimated based on the current statistics in the label dis-
tribution using logistic regression, and We refer to this as a self-similarity prior P({li};aι,βι,yι)
with parameters which are periodically fit to the current statistics in the predictions 2j∈s. q(l∣x)),
and £ j∈l. q(l|x/) to promote similar label patterns across the image. The criterion (7) can also be
seen as a degenerate version of this setting with S being 1 × 1 and L being infinite (or the whole
image).
The contrastive version of this prior relies on the insight previously pursued in image self-
supervision, e.g., [14]. However, contrasting is accomplished without sampling triplets, but con-
sidering all the data jointly, simply by expressing the goal of contrasting with far away regions
within the prior in our framework.
As an example of self-supervised pretraining in our framework, in Fig. E.1 we show an example
of clustering of a large tile of aerial imagery into 12 classes using 5 layer FCN as network q of the
architecture used in §4.4. The clustering is achieved by updating the prior every 50 steps of gradient
descent on batches of 200 256 × 256 patches. The prior is initialized to a contrasting prior, and then
updated through gradient descent. After 7 iterations, the result is sharpened by continuing training
using (7).
Figure E.	1: Unsupervised clustering using implicit QR loss (middle) of a NAIP tile (left). On the
right, we show the assignment of the 12 clusters to 4 land cover labels: water (blue), tall vegetation
(darker green), low vegetation (lighter green) and impervious/barren (gray).
This tile was recently used in testing the fine-tuning of a pretrained model with minimal amount of
new labels in a new region [43]. Both the pre-training region, the state of Maryland, and the testing
region, the tiles in New York State, come from the 4-class Chesapeake Land Cover dataset (§4.3).
23
Under review as a conference paper at ICLR 2022
Yet, the slight shift in geography results in reduction of accuracy from around 90% in Maryland
down to around 72.5% in New York. In [43], various techniques for quick model adaptation are
studied, on labels acquirable in up to 15 minutes of human labeling effort per tile. In Table E.1 we
compare the tunability of our self-supervised models on the four 85km2 regions tested in [43] with
active learning approaches to tuning a pre-trained Maryland model with 400 labeled points. We
show in the table the accuracy and mean intersection over union from [43] for tuning the pretrained
model’s last 64 × 4 layer with different active learning strategies for selecting points to be labeled.
A random selection of 400 points for which the labels are provided yields an average accuracy
improvement from 72.5% to 80.6%.
On the other hand, recall that we have created an unsupervised segmentation into 12 clusters, with
posteriors over the clusters qi(I). To investigate how well these clusters align with ground truth land
cover labels, we compute a simple assignment of clusters to land cover labels. Given a set of labeled
points {(i, ci)}• ∈/, we infer a mapping from clusters to four target labels,
P(cII) 8	2 qt(I).
i ∈I :Ci =C
The label of any point j can now be inferred as Ij = arg max。2ι qt(I)P(c|l). This procedure,
using 400 randomly selected labeled points, yields an average accuracy of 81.1% (averaged over
50 random collections of labeled points), which is above the performance of the pretrained model
tuned on as many randomly selected points, and on par with the more sophisticated methods for point
selection and the use of the pretrained model. (Note that the large model pretrained was trained on
a large similar dataset in a nearby state). (Table E.1).
Table E.1: Finetuning a pre-trained model by gradient descent [43] versus implicit QR clustering +
label assignment in low-label regimes.
Query method	pretrained model in [43]				Implicit QR
	No tuning	Random	Entropy Min-margin		Random
Tuned parameters	0	64×4	64×4	64×4	12×4
Accuracy %	72.5	80.6	73.6	81.1	81.1
IoU %	51.0	60.8	50.1	60.8	59.8
E.2 Tumor-infiltrating lymphocyte segmentation
The setup of this experiments mimics that of the land cover label super-resolution experiment in
§4.3. The training data consists of 50k 240 × 240 crops of H&E-stained histological imagery at
0.5〃m/px resolution, paired with coarse estimates of the density of tumor-infiltrating lymphocytes
(TILs) created by a simple classifier, at the resolution of 100 × 100 blocks. The goal is to produce
models for high-resolution TIL segmentation. Models are evaluated on a held-out set of 1786 images
with high-resolution point labels for the center pixel.
The coarse density estimates c belong to one of 11 classes, from 0 (no TILs) to 9 (highest estimated
TIL density). We use an estimated conditional likelihood P(I∣c) of the likelihood of the positive
TIL label at pixels with each low-resolution class C to construct a prior Pi(I) over the TIL label
probability. Notice that this prior is the same for all pixels in any given low-resolution, coarsely
labeled block.4
We train a small CNN with receptive field 11 × 11 (five ReLU-activated convolutional layers with
64 filters) under the RQ loss against this prior for 200 epochs with learning rate 10-5, then evaluate
on the held-out testing set. Inspired by [28], we apply a spatial blur of 11 pixels to the predicted
log-likelihoods (again correcting for the model’s small receptive field and the dataset bias).
4We experimented with setting pt(l∣c) to conditional likelihoods estimated from a held-out set and with
simply setting Pi(I = 1∣c = 0) = 0.05, Pi(I = 1∣c = 1) = 0.15, ..., Pi(I = 1∣c = 9) = 0.95. The latter gave
better results, perhaps due to the bias of the evaluation set, in which every image is known to be centered on a
cell of some kind.
24
Under review as a conference paper at ICLR 2022
Table E.2: Area under ROC curve for various predictors on the TIL segmentation task.
Model	fully supervised		weakly supervised	
	SVM [58; 13]	CNN [13] CSP-CNN [13]	U-Net [27] Epitome [28]	RQ
AUC	0.713	0.494	0.786	0.783	0.801	0.802
The AUC scores of this model and of the baselines are shown in Table E.2. Interestingly, the best-
performing models - RQ and epitomic super-resolution (a generative model) 一 both have receptive
fields of 11 × 11, much smaller than those of the U-Net and fully supervised CNNs. This means that
prediction of TIL likelihood is possible using only local image data, but the challenge is learning
to resolve highly uncertain label information. Unlike U-Nets and deep CNN autoencoders, small
models are not able to learn and overfit to distant spurious clues to the classes of nearby pixels.
E.3 Video segmentation with a structured prior
To demonstrate the use of priors with latent structure, we set up the problem of video segmentation
as follows. Given a frame t, We tune networks qt (li,t ∣xi,t) predicting one of L pixel classes for
a pixel at coordinate i in frame t. The prior in each frame comes f⅛om a Mask R-CNN model
[11] pre-trained on still images in the COCO dataset [23]. The Mask R-CNN model finds several
possible instances of objects of different categories and outputs the soft object masks in form of
confidence scores for each pixel. We convert this into a probability distribution over the index f
(foreground/background) of the form P(力,tMt), where 叫 are different detected instances by the
model, and the distributions P (fi,t∖mt) are the soft masks for these instances converted to probability
distributions, i.e. value of the probability of foreground differs for each pixel and each instance based
on the Mask R-CNN confidence scores. Although the COCO dataset may not have had instances
of object of interest in our frame Xt, we assume that some admixture of detected instances (likely
involving unrelated types of objects) does model reasonably well the foreground segmentation in the
frame. Mathematically, P(力,t) = Emt P(力,t∖mt)P(mt), where P(mt) expresses the probabilistic
selection of the foreground masks for different instances from which the foreground is constructed.
(One can think of instances mt as topics in topic models, which are also admixture models). To
complete the prior, we fix the distribution P(I∖ f) as fixed binary L × 2 matrix assigning a subset of
L pixel classes to foreground and the rest to the background. (For example, we assign first 3 classes
to foreground and the remaining 5 to the background for a total of L=8 pixel classes). Therefore,
P (li,t = l) = 2 P (l∖ f)P 2 P (fi,t = f ∖mt)P (mt)	(10)
We can now select the instances mt in each frame by optimizing the free energy with this prior
over P(mt). The procedure involves standard variational inference of the posterior distribution over
possible instances mt for each pixel i in frame t which involves the posterior qt (li,t ∖xi,t). In practice
we found that it is enough to do this inference once, using the network qt-1 estimated in the previous
frame.
This requires the inference of mt for each pixel i:
Si (mt) 8 exp(22 P (l∖于)qt (li,t = lM,t) log P (力,t =于∖mt)P(mt),	(11)
i ι,f
and then optimizing Pmt as the count of times each instance is used,
P (mt ) 8	Si (mt )	(12)
i
Selection of instances mt in frame t therefore involves comparing the predictions from the network
qt (li,t = l∖Xi,t) grouped into foreground/background segmentation with the foreground/background
segmentation for different instances from Mask R-CNN, and making a selection ofa subset (proba-
bilistically in P(mt)) based on which instances most overlap with the predictions from network qt .
While the above two equations should in principle be iterated, and iterated with updates to network
qt (li,t = l∖Xi,t), we found that in practice it is sufficient to just select the instances mt based on their
25
Under review as a conference paper at ICLR 2022
Figure E.2: Example of inferring the foreground mask on a video frame.
intersection with the network predictions once, at the very beginning, to make a soft fixed prior,
and leave it to optimizing the prediction network with the RQ loss to find confident segmentation
(Fig. E.2).
We tested the approach on the DAVIS 2016 dataset [37]. The dataset is comprised of 50 unique
scenes, accompanied by per-pixel foreground/background segmentation masks. The objective is to
produce foreground segmentation masks for all frames in a scene, given only the ground truth an-
notations of the first frame (Semi-Supervised). We evaluated our method on the 20-scene validation
set at 480p resolution.
The network q used in this experiment combines both the pixel intensities and spatial position in-
formation for its predictions. At each pixel location i, j, We augment the intensity information with
learned Fourier features [sin (W [i,J ]τ), cos (W [i,J ]τ )]r [47]. The image and spatial position are
first processed separately; A 4-layer, 64-channel, fully-convolutional network with 3 × 3 kernels,
ReLU activations and Batch Normalization produces the image features. A 3-layer, 16-channel,
pixel-wise MLP with ReLU activations and Batch Normalization processes the learned Fourier fea-
tures. These two are concatenated and passed through a single 3 × 3 convolution-ReLU-Batch
Normalization layer before being mapped to output predictions. We also experimented with adding
optical flow as another auxiliary input to the network.
For each scene, the network q0 is trained on the first frame, using the given ground truth annota-
tions split uniformly between 3 foreground and 5 background classes as prior, for 300 iterations.
This network is then used to predict the foreground pixels in the next frame and after computing
the intersection over union between the predicted foreground pixels and the Mask R-CNN output
masks, we select masks that overlap more than a pre-specified threshold. The chosen masks are then
summated, weighted by their Mask R-CNN confidence scores (0-1), to form the prior for the next
frame. The process of selecting masks from the Mask-RCNN predictions and forming the prior for
a frame is showcased in Figure E.3. The network q0 is then fine-tuned for 10 iterations to obtain
q1 and this process repeats for all subsequent frames. We used the Adam optimizer, with a starting
learning rate of 10-3 for the first frame, reduced to 10-5 for fine-tuning, and trained with batches of
128 64×64 patches.
To infer the foreground pixels we first need a pre-trained the Mask R-CNN on the COCO dataset.
Then, for each scene We only require 〜1m of training time on the ground truth-annotated first frame
and 〜3s per every following frame for the entire process of forming the prior and inferring the
foreground pixels. We do not train on any video data, in contrast to most video object segmentation
methodologies that rely on both a pre-trained network on static image datasets (such as COCO) and
additionally on offline training on video sequences. In Table E.3 we compare our results on the
DAVIS 2016 validation set to other video object segmentation algorithms from 2017 - present.
26
Under review as a conference paper at ICLR 2022
Figure E.3: Video segmentation procedure. Starting with a network qt-1 trained on frame t - 1, we
apply qt-1 on frame t to get a rough foreground estimation (top). By running the pre-trained Mask
R-CNN model on frame t and selecting only the masks that overlap with the qt-1 prediction we get
the candidate object masks (middle). The prior is constructed as the sum of the candidate masks,
weighted by their corresponding Mask R-CNN scores, and qt-1 is finetuned on frame t with this
prior to get the detailed predictions (bottom).
27
Under review as a conference paper at ICLR 2022
Table E.3: Jaccard and F1 measures for various algorithms on the video instance segmentation task.
Model	J&F ↑	J			F			Year
		Mean ↑	Recall ↑	Decay J	Mean ↑	Recall ↑	Decay J	
OSVOS [3]	80.2	79.8	93.6	14.9^^	80.6	92.6	15	2017
MSK [38]	77.55	79.7	93.1	8.9	75.4	87.1	9	2017
OnAVOS [49]	85.5	86.1	96.1	5.2	84.9	89.7	5.8	2017
Lucid [18]	82.95	83.9	95	9.1	82	88.1	9.7	2017
OSVOS-S [29]	86.55	85.6	96.8	5.5	87.5	95.9	8.2	2018
FAVOS [5]	80.95	82.4	96.5	4.5	79.5	89.4	5.5	2018
PReMVOS [25]	86.75	84.9	96.1	8.8	88.6	94.7	9.8	2018
OSMN [52]	73.45	74	87.6	9	72.9	84	10.6	2018
AGAME [16]	81.85	81.5	93.6	9.4	82.2	90.3	9.8	2019
STM [36]	89.4	88.7	97.4	5	90.1	95.2	4.2	2019
FEELVOS [50]	81.65	81.1	90.5	13.7	82.2	86.6	14.1	2019
CFBI [53]	89.4	88.3	-	-	90.5	-	-	2020
e-OSVOS [30]	86.8	86.6	-	-	87	-	-	2020
STCN [4]	91.7	90.4	98.1	4.1	93	97.1	4.3	2021
Ours	83.8	84	96.2	84^^	83.6	94.2	10.2	
Ours (+flow)	83.9	83.2	95.5	9.5	84.6	93.3	9.1	
E.4 Seducers seducing Seducers
One of the conclusions from our experiments on EnviroAtlas (§4.4) is that training a network with
the goal of generalizing to new input data is often inferior to simply performing in-collection in-
ference. In other words, given the collection of pairs xi,pi(I), learning the posterior q under the
implicit generative model is optimized for resolving ambiguities in that collection, and possibly that
collection alone. As pointed out in [28], which performs collection inference using large generative
models to mine self-similarity among the examples in the collection, this is appropriate when we
can expect our data xi to always come paired with prior beliefs P(li). It is interesting to reconsider
the Seducer example from Fig. 1. The artist created several versions of that painting, and as shown
in Fig. E.4 collection inference applied separately to each of these painting works equally well.
However, using a learned q network from one image onto others yields inferior segmentations (Fig.
E.5), as the learned network specialized for inference in the data it saw. (A fully generative model
would similarly overtrain on the input data features /,as would a supervised neural network trained
on hard-labeled pairs (Xi ,1) due to the domain shift.) Yet, if we know we will always be given
collections with beliefs in the form of priors Pi (I), local (collection) inference is all we need.
28
Under review as a conference paper at ICLR 2022
Figure E.4: Two additional versions of Le SeduCteur (left), hand-made priors (middle) and inferred
segmentations (right).
(a)
(b)
(c)
Figure E.5: Result of applying a network q trained to infer (b), on all three Le SeduCteur versions.
29