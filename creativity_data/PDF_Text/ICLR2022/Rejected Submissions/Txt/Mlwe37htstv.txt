Under review as a conference paper at ICLR 2022
Efficient Wasserstein and Sinkhorn Policy Op-
TIMIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Trust-region methods based on Kullback-Leibler divergence are pervasively used to
stabilize policy optimization in reinforcement learning. In this paper, we examine
two natural extensions of policy optimization with Wasserstein and Sinkhorn
trust regions, namely Wasserstein policy optimization (WPO) and Sinkhorn policy
optimization (SPO). Instead of restricting the policy to a parametric distribution
class, we directly optimize the policy distribution and derive their close-form
policy updates based on the Lagrangian duality. Theoretically, we show that WPO
guarantees a monotonic performance improvement, and SPO provably converges to
WPO as the entropic regularizer diminishes. Experiments across tabular domains
and robotic locomotion tasks further demonstrate the performance improvement
of both approaches, more robustness of WPO to sample insufficiency, and faster
convergence of SPO, over state-of-art policy gradient methods.
1	Introduction
Policy-based reinforcement learning (RL) approaches have received remarkable success in many
domains, including video games (Mnih et al., 2013; Mnih et al., 2015), board games (Silver et al.,
2016; Heinrich & Silver, 2016), robotics (Grudic et al., 2003; Gu et al., 2017), and continuous control
tasks (Duan et al., 2016; Schulman et al., 2016). One prominent example is policy gradient method
(Grudic et al., 2003; Peters & Schaal, 2006; Lillicrap et al., 2016; Sutton et al., 1999; Williams, 1992;
Mnih et al., 2016; Silver et al., 2014). The core idea is to represent the policy with a probability
distribution ∏θ(a|s) = P[a|s; θ], such that the action a in state S is chosen stochastically following
the policy πθ controlled by parameter θ. Determining the right step size to update the policy is crucial
for maintaining the stability of policy gradient methods: too conservative choice of stepsizes result in
slow convergence, while too large stepsizes may lead to catastrophically bad updates.
To control the size of policy updates, Kullback-Leibler (KL) divergence is commonly adopted to
measure the difference between two policies. For example, the seminal work on trust region policy
optimization (TRPO) by Schulman et al. (2015) introduced KL divergence based constraints (trust
region constraints) to restrict the size of the policy update; see also Peng et al. (2019); Abdolmaleki
et al. (2018). Kakade (2001) and Schulman et al. (2017) introduced a KL-based penalty term to the
objective to prevent excessive policy shift.
Though KL-based policy optimization has achieved promising results, it remains interesting whether
using other metrics to gauge the similarity between policies could bring additional benefits. Recently,
few work (Richemond & Maginnis, 2017; Zhang et al., 2018; Moskovitz et al., 2020; Pacchiano
et al., 2020) has explored the Wasserstein metric to restrict the deviation between consecutive policies.
Compared with KL divergence, the Wasserstein metric has several desirable properties. Firstly, it is a
true symmetric distance measure. Secondly, it allows flexible user-defined costs between actions and
is less sensitive to ill-posed likelihood ratios. Thirdly but most importantly, the Wasserstein metric
has a weaker topology (Arjovsky et al., 2017), thus possibly leading to a more robust policy and more
stable performance.
However, the challenge of applying the Wasserstein metric for policy optimization is also evident:
evaluating the Wasserstein distance requires solving an optimal transport problem, which could
be computationally expensive. To avoid this computation hurdle, existing work resorts to differ-
ent techniques to approximate the policy update under Wasserstein regularization. For example,
Richemond & Maginnis (2017) solved the resulting RL problem using Fokker-Planck equations;
1
Under review as a conference paper at ICLR 2022
Zhang et al. (2018) introduced particle approximation method to estimate the Wasserstein gradient
flow. Recently, Moskovitz et al. (2020) instead considered the second-order Taylor expansion of
Wasserstein distance based on Wasserstein information matrix to characterize the local behavioral
structure of policies. Pacchiano et al. (2020) tackled behavior-guided policy optimization with smooth
Wasserstein regularization by solving an approximate dual reformulation defined on reproducing
kernel Hilbert spaces. Aside from such approximation, some of these work also limits the policy
representation to a particular parametric distribution class, As indicated in Tessler et al. (2019), since
parametric distributions are not convex in the distribution space, optimizing over such distributions
results in local movements in the action space and thus leads to convergence to a sub-optimal solution.
Henceforth, the theoretical performance of policy optimization under the Wasserstein metric remains
elusive in light of these approximation errors.
In this paper, we study policy optimization with trust regions based on Wasserstein distance and
Sinkhorn divergence. The latter is a smooth variant of Waserstein distance by imposing an entropic
regularization to the optimal transport problem (Cuturi, 2013). We call them, Wasserstein Policy
Optimization (WPO) and Sinkhorn Policy Optimization (SPO), respectively. Instead of confining the
distribution of policy to a particular distribution class, we work on the space of policy distribution
directly, and consider all admissible policies that are within the trust regions with the goal of avoiding
approximation errors. Unlike existing work, we focus on exact characterization of the policy updates.
We highlight our contributions as follows:
1.	Algorithms: We develop close-form expressions of the policy updates for both WPO
and SPO based on the corresponding optimal Lagrangian multipliers of the trust region
constraints. In particular, the optimal Lagrangian multiplier of SPO admits a simple form
and can be computed efficiently. A practical on-policy actor-critic algorithm is proposed
based on the derived expressions of policy updates and advantage value function estimation.
2.	Theory: We theoretically show that WPO guarantees a monotonic performance improvement
through the iterations, even with non-optimal Lagrangian multipliers, yielding better and
more robust guarantee compared to that using KL divergence. Moreover, we prove that SPO
converges to WPO as the entropic regularizer diminishes.
3.	Experiments: A comprehensive evaluation with several types of testing environments
including tabular domains and robotic locomotion tasks demonstrates the efficiency and
effectiveness of WPO and SPO. Compared to state-of-art policy gradients approaches using
KL divergence such as TRPO and PPO, our methods achieve better sample efficiency,
faster convergence, and improved final performance. Our numerical study indicates that by
properly choosing the weight of the entropic regularizer, SPO achieves a better trade-off
between convergence and final performance than WPO.
Related work: Wasserstiein-like metrics have been explored in a number of works in the context
reinforcement learning. Ferns et al. (2004) first introduced bisimulation metrics based on Wasserstein
distance to quantify behavioral similarity between states for the purpose of state aggregation. Such
bisimulation metrics were recently utilized for representation learning of RL; see e.g., Castro (2020);
Agarwal et al. (2020). The most related work to ours are Richemond & Maginnis (2017); Zhang et al.
(2018); Moskovitz et al. (2020); Pacchiano et al. (2020). These work directly use Wasserstein-like
distance to measure proximity of policies instead of states. Unlike ours, these work apply Wasserstein
distance as an explicit penalty function instead of trust-region constraints. Moreover, they use
different strategies to approximate the Wasserstein distance. The only work that exploited Sinkhorn
divergence in RL, to our best knowledge, is Pacchiano et al. (2020). In addition, few recent work has
also exploited Wasserstein distance for imitation learning; see e.g., Xiao et al. (2019); Dadashi et al.
(2021).
Wasserstsein-like metrics are also pervasively studied in distributionally robust optimization (DRO);
see e.g., the survey by Kuhn et al. (2019) and references therein. Despite the similarity shared in
the duality formulations, the DRO problems are fundamentally different from constrained policy
optimization. We also point out that a recent concurrent work by Wang et al. (2021a) studied DRO
using the Sinkhorn distance.
2
Under review as a conference paper at ICLR 2022
2	Background and Notations
Markov Decision Process (MDP): We consider an infinite-horizon discounted MDP, defined by the
tuple (S, A, P, r, ρ0, γ), where S is the state space, A is the action space, P : S × A × S -→ R is the
transition probability, r : S × A -→ R is the reward function, ρ0 : S -→ R is the distribution of the
initial state s0, and γ is the discount factor. We define the return of timestep t as the accumulated
discounted reward from t, Rt = Pk∞=0 γkr(st+k, at+k), and the performance of a stochastic policy
∏ as J(∏) = Es0,a0,s1 …[P∞=0 Ytr(St,at)] where at 〜∏(at∣st), st+1 〜P(st+ι∣st, at). As shown
in Kakade & Langford (2002), the expected return of a new policy π0 can be expressed in terms
of the advantage over the old policy π: J(π0) = J(π) + Es〜ρ∏o 0〜∏o[Aπ (s,a)], where An (s, a)=
E[Rt|st = s, at = a; π] - E[Rt|st = s; π] represents the advantage function and ρυπ represents
the unnormalized discounted visitation frequencies with initial state distribution υ, i.e., ρυπ (s) =
Es0〜U [P∞=0 YtP(st = s|so)].
Trust Region Policy Optimization (TRPO): In TRPO (Schulman et al., 2015), the policy π is
parameterized as πθ with parameter vector θ. For notation brevity, we use θ to represent the policy
πθ . Then, the new policy θ0 is found in each iteration to maximize the expected improvement
J(π0) - J(π), or equivalently, the expected value of the advantage function:
mθax Es〜ρU,a〜θo [Aθ (s,a)]
s.t. Es〜ρυ [dκL(θ0,θ)] ≤ δ,
(1)
where dKL represents the KL divergence and δ is the threshold of the distance between the new
and the old policies. Note that here the expected value of the advantage function is an estimation
as the visitation frequency ρθυ is used rather than ρθυ0, which means the changes in state visitation
frequencies caused by the changes in policy are ignored.
Wasserstein Distance: Given two probability distributions of policies π and π0 on the discrete
action space A = {a1, a2, . . . , aN}, the Wasserstein distance between the policies is defined as:
dW(π0,π)=	inf	hQ,Mi,	(2)
Q∈Π(π0,π)
where〈•，•〉denotes the Frobenius inner product. The infimum is taken over all joint distributions Q
with marginals π0 and π, and M is the cost matrix with elements Mij = d(ai, aj ), where d(ai, aj ) is
defined as the distance between actions ai and aj .
Sinkhorn Divergence: Sinkhorn divergence (Cuturi, 2013) provides a smooth approximation of
the Wasserstein distance by adding an entropic regularizer. The Sinkhorn divergence is defined as
follows:
ds(∏0,∏∣λ)=	inf JhQ,Mi - 1 h(Q)},	⑶
Q∈Π(π0,π)	λ
where h(Q) = - PiN=1 PjN=1 Qij log Qij represents the entropy term, and λ > 0 is a regularization
parameter. Similarly, we use Qs to denote thejoint distribution of ∏(∙∣s) and∏0(∙∣s) with PN=I QSj =
π(aj |s) and PjN=1 Qisj = π0(ai |s). The intuition of adding the entropic regularization is: since most
elements of the optimal joint distribution Q will be 0 with a high probability, by trading the sparsity
with entropy, a smoother and denser coupling between distributions can be achieved (Courty et al.,
2014; 2016). Therefore, when the weight of the entropic regularization decreases (i.e., λ increases),
the sparsity of the divergence increases, and the Sinkhorn divergence converges to the Wasserstein
metric, i.e., limλ→∞ ds(∏0,π∣λ) = dw(π0,π). More critically, Sinkhorn divergence is useful
to mitigate the computational burden of computing Wasserstein distance. In fact, the efficiency
improvement that Sinkhorn divergence and the related algorithms brought paves the way to utilize
Wassersterin-like metrics in many machine learning domains, including online learning (Cesa-Bianchi
& Lugosi, 2006), model selection (Juditsky et al., 2008; Rigollet & Tsybakov, 2011), generative
modeling (Genevay et al., 2018; Petzka et al., 2017; Patrini et al., 2019), dimensionality reduction
(Huang et al., 2021; Lin et al., 2020; Wang et al., 2021b).
3
Under review as a conference paper at ICLR 2022
3 Wasserstein Policy Optimization
Motivated by TRPO, here we consider a trust region based on the Wasserstein metric. Moreover, we
lift the restrictive assumption that a policy has to follow a parametric distribution class by allowing
all admissible policies. Then, the new policy π0 is found in each iteration to maximize the estimated
expected value of the advantage function. Therefore, the Wasserstein Policy Optimization (WPO)
framework is shown as follows:
max	Es 〜ρ∏ ,a 〜π0(∙∣s)[A (S, a)]
where D = {π0∣Es〜。方[dw(π0(∙∣s),π(∙∣s))] ≤ δ},
(4)
where the Wasserstein distance dw(∙, ∙) is defined in (2).
In most practical cases, the reward r is bounded and correspondingly, the accumulated discounted
reward Rt is bounded. So without loss of generality, we can make the following assumption:
Assumption 1. Assume Aπ(s, a) is bounded, i.e., suPa∈A,s∈s ∣Aπ (s, a)| ≤ A max forsome A max > 0.
With Wasserstein metric based trust region constraint, we are able to derive the closed-form of the
policy update shown in Theorem 1. The main idea is to form the Lagrangian of the constrained
optimization problem presented above, and the detailed proof can be found in Appendix A.
Theorem 1.	(Closed-form policy update) Let ksπ (β, j) = argmaxk=1...N {Aπ (s, ak) - βMkj },
where M denotes the cost matrix. If Assumption 1 holds, then an optimal solution to the WPO
problem in (4) is given by:
N
π*(ai |s) = X π(aj I (ii)s)f；(i,j),	⑸
j=1
where fS(i,j) = 1 if i = k∏(β*,j) and f*(i,j) = 0 otherwise, and β* is an optimal Lagrangian
multipler corresponds to the following dual formulation:
N
βδ + Es〜Pπ y^π(aj Is) [Aπ(S, akπ(β,j)) - βMkπ(β,j)j]
j=1
Moreover, we have β* ≤ β= maxs∈s,k,j=ι...N,k=j (Mkj)-1(Aπ(s,ak) - An(s,aj)).
The exact policy update for WPO in (5) requires computing the optimal Lagrangian multiplier β* by
solving the one-dimensional subproblem (6). A closed form of β* is not easy to obtain in general,
except for special cases of the distance d(x, y) or cost matrix M. In Appendix G, we provide the
closed form of β* for the case when d(χ, y) = 0 if X = y and 1 otherwise.
min F(β) = min
WPO Policy Update: Based on Theorem 1, we introduce the following WPO policy updating rule:
N
∏t+ι(ai∣s) = FWPO(∏t) := X∏t(aj∣s)ft(i,j),	(WPO)
j=1
where we choose an arbitrary ksπt (βt, j) ∈ argmaxk=1,...,N {Aπt (s, ak) - βtMkj} and set
fst(ksπt (βt,j),j) = 1 and other entries to be 0.
Note that different from (5), we allow βt to be chosen arbitrarily and time dependently. We show that
such policy update always leads to a monotonic improvement of the performance even when βt is not
the optimal Lagrangian multiplier. In particular, we propose two efficient strategies to update the
multiplier βt :
(i) Time-dependent βt : To improve the computational efficiency, we can simply treat βt as a
time-dependent control parameter, e.g., we can choose βt to be a diminishing sequence.
(ii) Approximation of optimal βt : To improve the convergence, we can approximately solve the
optimal Lagrangian multiplier based on Sinkhorn divergence. We will discuss this in more
detail in Section 4.
4
Under review as a conference paper at ICLR 2022
Next, we provide theoretical justification that WPO policy update is always guaranteed to improve the
true performance J monotonically if we have access to the true advantage function. If the advantage
function can only be evaluated inexactly with limited samples, then an extra estimation error will
incur. The detailed proof can be found in Appendix B.
Theorem 2.	(Performance improvement) For any initial state distribution μ and any βt ≥ 0, if
∣∣Aπ — Aπ∣∣∞ ≤ E for some e > 0, the WPO policy update with the inaccurate advantage function
An, guarantees the following performance improvement bound,
N	2E
J(πt+1) ≥ J(πt) + βtEs〜Pπt+1EInt (aj ls)M^∏t (βt ,j)j - 1一^.	⑺
μ	j=1	- - Y
Note that when the estimation error E = 0, we have a monotonic performance improvement
J(πt+1) ≥ J(πt) for any βt ≥ 0. If the second term in (7) is non-zero, then we have a strict
monotonic improvement. Compared to the performance bound when using KL-based trust region
J(∏t+ι) ≥ J(∏t) - ι⅛γ (see, e.g., Schulman et al. (2015); Cen et al. (2020)), using the Wasserstein
metric yields a tighter performance improvement bound and is more robust to the choice of βt .
4 Sinkhorn Policy Optimization
In this section, we introduce Sinkhorn policy optimization (SPO) by constructing trust region with
Sinkhorn divergence. In the following theorem, we derive the optimal policy update in each step
when using Sinkhorn divergence based trust region. The proof follows a similar procedure as the
Wasserstein policy optimization framework by forming the Lagrangian of the constrained optimization
problem. Details are provided in Appendix C.
Theorem 3.	If Assumption 1 holds, then the optimal solution to the trust region constrained problem
(4) with Sinkhorn divergence is:
^Ny	exp(会Aπ (s, αi) - λMij )
nt (ai|s) = V F——J--------------------∏(aj∣s),
j = 1 Pk=1 exp (氏 An (S, ak ) - λMkj )
(8)
where M denotes the cost matrix and βλt is an optimal solution to the following dual formulation:
minβ≥o Fλ(β) = minβ≥o {βδ - Es〜ρ∏ PN=I n(aj|s)(λ + λ ln(n(aj|s)) -
β I T^^N	/ λ ATT /	∖ ∖ λ∕f~ Mλ I N	Xɔ N V-^N β exp (β A (s,ai)-λMij >n(aj Is) ] ∕c∖
λ lnE i=1 eXp( β A Gai) - λMij )〕)+ Es~Pπ Ei=I Σj=1 λ pN=ι eχp( λ A∏ (s,afc )-λMkj)卜(9)
2Amax
Moreover, we have βλ ≤ 2A-∙
In contrast to the Wasserstein dual formulation (6), the objective in the Sinkhorn dual formulation (9)
is differentiable in β and admits closed-form gradients (shown in Appendix E). With this gradient
information, we can use gradient-based global optimization algorithms (Wales & Doye, 1998; Zhan
et al., 2006; Leary, 2000) to find a global optimal solution βλt to (9).
Next, we show that if the entropic regularization parameter λ is large enough, then the optimal
solution βλt is a close approximation to the optimal solution β t to the Wasserstein dual formulation.
The detailed proof is provided in Appendix F.
Theorem 4.	Define βUB = max{ 2Ama', β}. The following holds:
1.	Fλ (β) converges to F(β) uniformly on [0, βUB],
2. lim arg min Fλ (β ) ⊆ arg min F (β ).
Although it is difficult to obtain the exact value of the optimal solution βt to the Wasserstein dual
formulation (6), the above theorem suggests that we can approximate βt via βλt by setting up a
relative large λ. In practice, we can also adopt a smooth homotopy approach by setting an increasing
sequence λt for each iteration and letting λt → ∞.
5
Under review as a conference paper at ICLR 2022
SPO Policy Update: Based on Theorem 3, we introduce the following SPO policy updating rule:
∏t+ι(ai∣s) = FSPO (∏t):
ʌ exp ( λ Ant (S,ai) - λtMij )
〉 ——十-----------；--------------------∏t(aj | s),
j = 1 Pk=1 exp ( βtAnt (S, ak ) - λtMkj )
(SPO)
Here λt ≥ 0 and βt ≥ 0 are some control parameters. The parameter βt can be either computed via
solving the one-dimensional subproblem (9) or simply set as a diminishing sequence.
5 A Practical Algorithm
In practice, the advantage value functions are often estimated from sampled trajectories. In this
section, we provide a practical on-policy actor-critic algorithm, described in Algorithm 1, that
combines WPO/SPO with advantage function estimation.
In each iteration of Algorithm 1, the first step
is to collect trajectories, which can be either
complete or partial. The difference is whether
the return is considered thoroughly to the end
of a planning horizon. If the trajectory is com-
plete, the total return can be directly expressed
as the accumulated discounted rewards Rt =
PkT=-0t-1 γkrt+k. If the trajectory is partial,
it can be estimated by applying the multi-
step temporal difference (TD) methods (De
Asis et al., 2017): Rt：t+n = Pn-(I YkTt+k +
γnV (St+n). Then for the advantage estima-
tion, we can use Monte Carlo advantage esti-
mation, i.e., Ank = Rt - Vψk (St) or General-
ized Advantage Estimation (GAE) (Schulman
et al., 2016), which provides a more explicit
control over the bias-variance trade-off. In the
value update step, we use a neural net to rep-
Algorithm 1: On-policy WPO/SPO algorithm
Input: number of iterations K , learning rate α
Initialize policy π( and value network Vψ0 with
random parameter ψ(
for k = 0, 1, 2 . . . K do
Collect a set of trajectories Dk on policy πk
For each timestep t in each trajectory,
compute total returns Gt and estimate
advantages Ank
Update value:
ψk + 1 J ψk - aVψk	(Gt - Vψk (St))2
Update policy:
∏k+ι J F(∏k) via WPO or SPO with Ank
end
resent the value function, where ψ is the parameter that specifies the value net S → V (S). Then, we
can update ψ by using gradient descent, which significantly reduces the computational burden of
computing advantage directly.
6	Experiments
In this section, we evaluate the proposed WPO and SPO approaches on tabular domains and robotic
locomotion tasks as presented in Algorithm 1. We compare the performance of our proposed methods
with several benchmarks, including TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017), and
A2C (Mnih et al., 2016)1. We compare with A2C because it is similar to our framework in the sense
that both of them are simple on-policy actor-critic methods that utilize the advantage information to
perform policy updates. For environments with a discrete state space (e.g., tabular domains), policy
updates are performed for all states at each iteration. For environments with a continuous state space,
a random subset of states is sampled at each iteration to perform policy updates.
6.1	Ablation Study
In this experiment, we first examine the sensitivity of WPO in terms of different strategies of βt . We
test four settings of β value for WPO policy update: (1) Setting 1: computing optimal β value for all
policy update; (2) Setting 2: computing optimal β value for first 20% of policy updates and decaying
β for the remaining; (3) Setting 3: computing optimal β value for first 20% of policy updates and fix
β as its last updated value for the remaining; (4) Setting 4: decaying β for all policy updates (e.g.,
βt = t2). In particular, Setting 3 is rooted in the observation that β* does not change significantly
1We use the implementations of TRPO, PPO and A2C from OpenAI Baselines (Dhariwal et al., 2017) for
MuJuCo tasks and Stable Baselines (Hill et al., 2018) for other tasks.
6
Under review as a conference paper at ICLR 2022
throughout all the policy updates, especially in the later stage in the experiments carried out in the
paper. Small perturbations are added to the approximate values to avoid any stagnation in updating.
Taxi task (Dietterich, 1998) from tabular domain is selected for this experiment.
The performance comparisons and average run times are shown
in Figure 1 and Table 1 respectively. Figure 1 and Table 1 clearly
indicate a tradeoff between computation efficiency and accuracy in
terms of different choices of β value. Setting 2 is the most effective
way to balance the tradeoff between performance and run time. For
the rest of experiments, we adopt this setting for both WPO and
SPO. We also compare WPO with SPO under different constant
and time varying λ values on the Taxi task. As shown in Figure 1,
SPO converges faster than WPO. With more weight on the entropic
regularization of Sinkhorn divergence (i.e., smaller λ), SPO can
Table 1: Run time comparison
for different β settings
Runtime	Taxi (s)	CartPole (s)
Setting 1	1224~	130
Setting 2	648	63
Setting 3	630	67
Setting 4	522	44
speed up its convergence more; while as λ increases, the convergence becomes slower but the final
performance of SPO improves and becomes closer to the final performance of WPO, which verifies
the convergence property of Sinkhorn to Wasserstein distance shown in Theorem 4. Therefore, the
choice of λ can effectively adjust the trade-off between convergence and final performance. With
a proper λ choice, SPO is able to attain a faster convergence speed with an optimum that is only
slightly lower than WPO.
More experiments for ablation study is conducted on the Chain (Dearden et al., 1998) and CartPole
(Barto et al., 1983) tasks. Results are provided in Appendix H.
U-Iməɑ 8 be-iφλv
Ooooo
Ooooo
1 2 3 4 5
- - - - -
Taxi
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Timesteps ×105
WPO: Optimaljg
WPO: Optimal + decay β
WPO: Optimal + ∞nstant β
WPO: Decay β
EnφB 36R」① ʌv
Enω比 36b」① Ay
Taxi
—SPO(Λ~t2)
0.5 F.0	1.5	2.0
Timesteps ×105
SPO(A-Iogt)
SPO M-t)
Figure 1:	Episode rewards during the training process for the Taxi task with different β and λ settings,
averaged across 3 runs with a random initialization. The shaded area depicts the mean ± the standard
deviation.
6.2 Tabular Domains
We evaluate WPO and SPO on a set of tasks including Taxi, Chain
(Dearden et al., 1998), and Cliff Walking (Sutton & Barto, 2018),
which are intentionally designed to test the exploration ability of the
algorithms. The tabular domain has a special environment structure
with a discrete state space and a discrete action space. Thus, we use
an array of size |S| X |A| to represent the policy π(a∣s). For the
value function, we use a neural net to smoothly update the values.
The performance of WPO and SPO are compared to the performance
of TRPO, PPO and A2C under the same neural net structure. Each
algorithm is evaluated 5 times with a random initialization. Results
are reported in Table 2 and Figure 2. The setting of hyperparamaters
Table 2: Trained agents perfor-
mance on Taxi (averaged over
1000 episodes)
	WPO	TRPO
Success (+20)	0.753-	0
Fail (-10)	0.232-	0
Timesteps (-1)	70.891	200
Avg Return	-58.151	-200
and network sizes of our algorithms and additional results are provided in Appendix H.
As shown in Figure 2, the performances of WPO, SPO and TRPO are manifestly better than A2C
and PPO. Between the trust region based methods, WPO and SPO outperform TRPO in most tasks,
except in Chain, where the performances of these three methods are not significantly different. In
Taxi and Cliff Walking, SPO converges to the optimum the fastest, while in Taxi, WPO converges to
the best optimum, among all methods. We further analyze the performance of the trained agent for
each algorithm on the Taxi environment. As shown in Table 2, WPO has a higher successful drop-off
7
Under review as a conference paper at ICLR 2022
rate and a lower task completion time while the original TRPO reaches the time limit with a drop-off
rate 0. Therefore, the results in Taxi show that WPO finds a better policy than the original TRPO.
Timesteps
u-lnφα ΦCT2Φ><
U」nl8S ωCT2φ><
Figure 2:	Episode rewards during the training process for the tabular domain tasks, averaged across 5
runs with a random initialization. The shaded area depicts the mean ± the standard deviation.
We also show that compared with the KL divergence, which is used in traditional TRPO and PPO
approaches, the utilization of Wasserstein metric can cope with the inaccurate advantage function
estimations caused by the lack of samples. We compare WPO with KL (Algorithm 1 framework with
KL based policy update derived in Peng et al. (2019)) on the Chain task. We evaluate the performance
of these two algorithms under different NA , which denotes the number of samples used to estimate
the advantage function at each iteration. As shown in Figure 3, when NA is 1000, KL performs
slightly better than WPO. However, when NA decreases to 100 or 250, WPO outperforms KL. These
results indicate that WPO is more robust than KL under inaccurate advantage values.
u-lnφB Φ6e-lφ><
u-lnφB ΦCTSΦ><
u-lnφB ΦCTSΦ><
(a) NA = 100	(b) NA = 250	(c) NA = 1000
Figure 3:	Episode rewards during the training process for the Chain task, averaged across 3 runs with
a random initialization. The shaded area depicts the mean ± the standard deviation.
6.3 Robotic Locomotion Tasks
We then integrate deep neural network architecture into MPO and SPO and evaluate their performance
on several discrete locomotion tasks, including CartPole (Barto et al., 1983) and Acrobot (Geramifard
et al., 2015). We use two separate neural nets to represent the policy and the value. The policy neural
net receives state S as an input and outputs the categorical distribution of ∏(a∣s). The performance of
WPO and SPO are compared to that of TRPO, PPO and A2C under the same neural net structure. We
run each algorithm 5 times with a random initialization.
Final Performance: Figure 4 shows the episode rewards during training process for WPO, SPO
and baseline algorithms. As seen in Figure 4, WPO and SPO outperform TRPO, PPO and A2C in
most tasks in terms of final performance, except in Acrobot where PPO performs the best. In most
cases, SPO converges faster but WPO has a better final performance.
Training Time: To train 105 timesteps in the discrete locomotion tasks, the training wall-clock time
is around 63s for WPO, 65s for SPO, 59s for TRPO and 70s for PPO. Therefore, WPO has a similar
computational efficiency as TRPO and PPO.
The performances of WPO and KL are also compared for the discrete locomotion tasks under
different NA . As shown in Figure 5, when NA is 500, KL performs better than WPO. However, when
NA decreases to 100, WPO significantly outperforms KL. These results indicate that for discrete
locomotion tasks, WPO is more robust than KL when advantage values are inaccurate.
8
Under review as a conference paper at ICLR 2022
Figure 4:	Episode rewards during the training process for the locomotion tasks, averaged across 5
runs with a random initialization. The shaded area depicts the mean ± the standard deviation.
u-lnφQ≤ωCT2φ><
(a) NA = 100
(b) NA = 500
Ooooooo
0 5 0 5 0 5 0
TT-2-2,TT-4
u-lməɑ ΦCTra⅛><
O 25 50 75 100 125 150 175 200
Episode
(c) NA = 100
(d) NA = 500
Figure 5:	Episode rewards during the training process for the locomotion tasks, averaged across 3
runs with a random initialization. The shaded area depicts the mean ± the standard deviation.
6.4 Continuous Action Space:
We further extend the evaluation of WPO and SPO to environments with a continuous action space
by discretizing the action space following Tang & Agrawal (2020). For comparison, we additionally
consider Behavior Guided Policy Gradient (BGPG) algorithm from Pacchiano et al. (2020). Similar
results are observed in Figure 6 as the discrete action tasks: WPO and SPO outperform the benchmark
algorithms in terms of final performance.
Emeu ΦCTSΦ><
U-Inφg ΦCTra⅛><
Figure 6:	Episode rewards during the training process for continuous action space tasks, averaged
across 3 runs with a random initialization. The shaded area depicts the mean ± the standard deviation.
7 Conclusion
In this paper, we present two policy optimization frameworks, WPO and SPO, which can exactly
characterize the policy updates instead of confining their distributions to particular distribution
class or requiring any approximation. Our methods outperform TRPO and PPO with better sample
efficiency, faster convergence, and improved final performance. Our numerical results show that the
Wasserstein metric is more robust to the ambiguity of advantage functions, compared with the KL
divergence. Our strategy for adjusting β value for WPO can reduce the computational time and boost
the convergence without noticeable performance degradation. SPO improves the convergence speed
of WPO by properly choosing the weight of the entropic regularizer. For future work, it remains
interesting to extend the idea to PPO and natural policy gradients, which penalize the policy update
instead of imposing trust region constraint, and extend it to off-policy frameworks.
9
Under review as a conference paper at ICLR 2022
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin
Riedmiller. Maximum a posteriori policy optimisation. ArXiv Preprint, pp. arXiv:1806.06920,
2018.
Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive
behavioral similarity embeddings for generalization in reinforcement learning. In Proceedings of
the 8th International Conference on Learning Representations, 2020.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. ArXiv Preprint, pp.
arXiv:1701.07875, 2017.
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics,
SMC-13(5):834-846,1983.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic Markov
decision processes. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence,
volume 34, pp. 10069-10076, 2020.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods with entropy regularization. ArXiv Preprint, pp. arXiv:2007.06558,
2020.
Nicolo Cesa-Bianchi and Ggbor Lugosi. Prediction, learning, and games. Cambridge University
Press, 2006.
Nicolas Courty, Remi Flamary, and Devis Tuia. Domain adaptation with regularized optimal transport.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
274-289. Springer, 2014.
Nicolas Courty, Remi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain
adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9):1853-1865,
2016.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
Neural Information Processing Systems, volume 26, pp. 2292-2300, 2013.
Robert Dadashi, Leonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal Wasserstein
imitation learning. In Proceedings of the 9th International Conference on Learning Representations,
2021.
Kristopher De Asis, J. Fernando Hernandez-Garcia, G. Zacharias Holland, and Richard S. Sutton.
Multi-step reinforcement learning: A unifying algorithm. ArXiv Preprint, pp. arXiv:1703.01327,
2017.
Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian Q-learning. In Proceedings of the
Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of
Artificial Intelligence Conference, pp. 761-768, 1998.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI baselines. https:
//github.com/openai/baselines, 2017.
Thomas G. Dietterich. The MAXQ method for hierarchical reinforcement learning. In Proceedings
of the 15th International Conference on Machine Learning, pp. 118-126, 1998.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In Proceedings of the 33rd International Conference
on Machine Learning, pp. 1329-1338, 2016.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite Markov decision processes.
In Uncertainty in Artificial Intelligence, volume 4, pp. 162-169, 2004.
10
Under review as a conference paper at ICLR 2022
AUde Genevay, Gabriel Peyra and Marco CUtUrL Learning generative models with Sinkhorn
divergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608-1617,
2018.
Alborz Geramifard, Christoph Dann, Robert H. Klein, William Dabney, and Jonathan P. How. RLPy:
A valUe-fUnction-based reinforcement learning framework for edUcation and research. Journal of
Machine Learning Research, 16(46):1573-1578, 2015.
Gregory Z. GrUdic, Vijay KUmar, and Lyle H. Ungar. Using policy gradient reinforcement learning
on aUtonomoUs robot controllers. In Proceedings of the 2003 IEEE/RSJ International Conference
on Intelligent Robots and Systems, pp. 406-411, 2003.
Shixiang GU, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipUlation with asynchronoUs off-policy Updates. In Proceedings of the 2017 IEEE
International Conference on Robotics and Automation, pp. 3389-3396, 2017.
Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-
information games. ArXiv Preprint, pp. arXiv:1603.01121, 2016.
Ashley Hill, Antonin Raffin, Maximilian ErnestUs, Adam Gleave, Anssi Kanervisto, Rene Traore,
PrafUlla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John SchUlman, Szymon Sidor, and YUhUai WU. Stable baselines. https://github.com/
hill-a/stable-baselines, 2018.
MinhUi HUang, Shiqian Ma, and Lifeng Lai. A Riemannian block coordinate descent method for
compUting the projection robUst Wasserstein distance. In Proceedings of the 38th International
Conference on Machine Learning, pp. 4446-4455, 2021.
Anatoli JUditsky, Philippe Rigollet, and Alexandre B Tsybakov. Learning by mirror averaging. The
Annals of Statistics, 36(5):2183-2206, 2008.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
Proceedings of the 19th International Conference on Machine Learning, pp. 267-274, 2002.
Sham M Kakade. A natUral policy gradient. In Advances in Neural Information Processing Systems,
volUme 14, 2001.
Daniel KUhn, Peyman Mohajerin Esfahani, Viet Anh NgUyen, and Soroosh Shafieezadeh-Abadeh.
Wasserstein distribUtionally robUst optimization: Theory and applications in machine learning. In
Operations Research & Management Science in the Age of Analytics, pp. 130-166. INFORMS,
2019.
Robert Leary. Global optimization on fUnneling landscapes. Journal of Global Optimization, 18:
367-383, 2000.
Timothy P. Lillicrap, Jonathan J. HUnt, Alexander Pritzel, Nicolas Heess, Tom Erez, YUval Tassa,
David Silver, and Daan Wierstra. ContinUoUs control with deep reinforcement learning. In
Proceedings of the 4th International Conference on Learning Representations, 2016.
Tianyi Lin, ChenyoU Fan, Nhat Ho, Marco CUtUri, and Michael I Jordan. Projection robUst Wasserstein
distance and Riemannian optimization. ArXiv Preprint, pp. arXiv:2006.07458, 2020.
Volodymyr Mnih, Koray KavUkcUoglU, David Silver, Alex Graves, Ioannis AntonogloU, Daan
Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. ArXiv Preprint,
pp. arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray KavUkcUoglU, David Silver, Andrei A. RUsU, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis AntonogloU, Helen King, Dharshan KUmaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. HUman-level control throUgh deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
11
Under review as a conference paper at ICLR 2022
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings ofthe 33rd International Conference on Machine Learning, pp. 1928-
1937, 2016.
Ted Moskovitz, Michael Arbel, Ferenc Huszar, and Arthur Gretton. Efficient Wasserstein natural
gradients for reinforcement learning. ArXiv Preprint, pp. arXiv:2010.05380, 2020.
Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Krzysztof Choromanski, Anna Choromanska,
and Michael Jordan. Learning to score behaviors for guided policy optimization. In Proceedings
of the 37th International Conference on Machine Learning, pp. 7445-7454, 2020.
Giorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max
Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Uncertainty in Artificial
Intelligence, pp. 733-743, 2019.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. ArXiv Preprint, pp. arXiv:1910.00177,
2019.
Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Proceedings of the 2006
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2219-2225, 2006.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs.
ArXiv Preprint, pp. arXiv:1709.08894, 2017.
Pierre H. Richemond and Brendan Maginnis. On Wasserstein reinforcement learning and the Fokker-
Planck equation. ArXiv Preprint, pp. arXiv:1712.07185, 2017.
Philippe Rigollet and Alexandre Tsybakov. Exponential screening and optimal rates of sparse
estimation. The Annals of Statistics, 39(2):731-771, 2011.
R. Tyrrell Rockafellar and Roger J. B. Wets. Variational Analysis. Springer, 1998.
Johannes O. Royset. Approximations and solution estimates in optimization. Mathematical Program-
ming, 170:479-506, 2018.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning,
pp. 1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In Proceedings of the 4th
International Conference on Learning Representations, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. ArXiv Preprint, pp. arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on
Machine Learning, pp. 387-395, 2014.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of Go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics, 8(1):171-176, 1958.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2018.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems, pp. 1057-1063, 1999.
12
Under review as a conference paper at ICLR 2022
Yunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization.
In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, volume 34. AAAI
Press, 2020.
Chen Tessler, Guy Tennenholtz, and Shie Mannor. Distributional policy optimization: An alternative
approach for continuous control. In Advances in Neural Information Processing Systems, pp.
1350-1360, 2019.
David Wales and Jonathan Doye. Global optimization by basin-hopping and the lowest energy
structures of Lennard-Jones clusters containing up to 110 atoms. The Journal of Physical Chemistry
A, 101(28):5111-5116, 1998.
Jie Wang, Rui Gao, and Yao Xie. Sinkhorn distributionally robust optimization. ArXiv Preprint, pp.
arXiv:2109.11926, 2021a.
Jie Wang, Rui Gao, and Yao Xie. Two-sample test using projected Wasserstein distance. In
Proceedings of IEEE International Symposium on Information Theory, volume 21, 2021b.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256, 1992.
Huang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, and Thai Hong Linh.
Wasserstein adversarial imitation learning. ArXiv Preprint, pp. arXiv:1906.08113, 2019.
Lixin Zhan, Jeff Chen, and Wing-Ki Liu. Monte Carlo basin paving: An improved global optimization
method. Physical Review E, 73:015701, 2006.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as Wasserstein
gradient flows. In Proceedings of the 35th International Conference on Machine Learning, pp.
5737-5746, 2018.
13
Under review as a conference paper at ICLR 2022
A Proof of Theorem 1
Theorem 1. (Closed-form policy update) Let ksπ (β, j) = argmaxk=1...N {Aπ (s, ak) - βMkj },
where M denotes the cost matrix. If Assumption 1 holds, then an optimal solution to the WPO
problem in (4) is given by:
N
π*(ai |s) = X π(aj Is)f；(i,j),	⑸
j=1
where fS(i,j) = 1 if i = k∏(β*,j) and fS(i,j) = 0 otherwise, and β* is an optimal Lagrangian
multipler corresponds to the following dual formulation:
N
βδ + Es 〜Pπ En(Oj Is)[Aπ(S, akg(e,j)) - BMkS(β,j)j∖
j=1
Moreover, we have β* ≤ /3 := maXs∈s,k,j=ι...N,k=j (Mkj)-1(Aπ(s,ak) - Aπ(s,aj)).
min F (β) = min
ProofofTheorem 1. First, We denote QS as the joint distribution of π(∙∣s) and π0(∙∣s) with
PiN=1 Qisj = π(ajIs) and PjN=1 Qisj = π0(aiIs). Also, let fs(i, j) represent the conditional
distribution of	π0(aiIs)	under π(ajIs).	Then	Qisj	=	π(aj Is)fs(i, j),	π0(aiIs) = PjN=1	Qisj	=
PjN=1 π(ajIs)fs(i, j). In addition:
NN	NN
dw(∏0(∙∣s),∏(∙∣s))	=mSnXXMijQsj= min XXMij∏(ajIs)fs(i,j), and
Qij i=1 j=1	fs (i,j) i=1 j=1
N	NN
Ea 〜∏0(∙∣s)[Aπ(s,a)]	= X An (s,ai)∏0(ai∣s)= XX
Aπ(s, ai)π(ajIs)fs(i,j).
Thus, the WPO problem in (4) can be reformulated as:
max
fs (i,j)≥0
s.t.
NN
ES 〜PU XX
Aπ(s, ai)π(ajIs)fS(i,j)	(10a)
i=1 j=1
NN
ES 〜PU XX
Mijπ(ajIs)fS(i,j) ≤ δ,	(10b)
i=1 j=1
N
XfS(i,j)= 1,	∀s ∈S,j= 1...N.	(10c)
i=1
Note here that (10b) is equivalent to ES〜Pn minfs(i,j) PlN=I PN=I Mijπ(aj ∖s)fs(i,j)	≤
δ because if we have a feasible fS(i,j) to make (10b) hold, we must have
ES 〜Pn minfs(i,j) Pi=I PN=1 Mij n(aj ∣s)fs(i,j) ≤ δ.
Since both the objective function and the constraint are linear in fS(i,j), (10) is a convex optimization
problem. Also, Slater’s condition holds for (10) as the feasible region has an interior point, which is
fS(i, i) = 1 ∀i, and fS(i,j) = 0 ∀i 6= j. Meanwhile, since Aπ(s, a) is bounded based on Assumption
1, the objective is bounded above. Therefore, strong duality holds for (10). At this point we can
derive the dual problem of (10) as its equivalent reformulation:
min
β≥0,ζjs
βδ +
s.t.
Aπ (s, ai)π(aj Is) - βMijπ(aj Is) -
-ɪ ≤ 0
PU(s)一
(11)
∀s ∈ S, i, j = 1 . . . N.
We observe that with a fixed β , the optimal ζjS will be achieved at:
ZΓ(β) = max PU(s)∏(aj∣s)(Aπ(s,ai) - βMij).	(12)
i=1...N
14
Under review as a conference paper at ICLR 2022
Denote β* as an optimal solution to (11) and fS(i,j) as an optimal solution to (10). Due to the
complimentary slackness, the following equations hold:
，亿j)
(An(s,ai)π(aj |s) - β*Mjπ(aj∣s)-
0,
∀s, i, j.
In this case, fS(i,j) can have non-zero values only when An(s,ai)π(aj|s) - β*Mj∏(aj∣s)-
Zj*(β*)
PU (S)
0, which means ZJ*(β*) = PJ(s)π(aj∣s)(Aπ(s, ai) - β*Mj). Given the expression
of the optimal Zj^ in (12), fS(i,j) can have non-zero values only when i ∈ Kn(β*,j), where
K∏(β,j) = argmaXk=ι...N Aπ(s,ak) - βMkj. Since PN=I fj(ij = 1 as indicated in (10c), we
can choose an arbitrary optimizer k∏(β*,j) ∈ Kn(β*,j) to derive an optimal solution fj(i,j):
fS(i,j) = ∣0
if i = k∏ (β *,j)
otherwise,
And then the corresponding optimal solution is, π* (ai | S)=PIN=I π(aj ls)fs(i,j).
Last, by substituting Zj*⑼ =Pn(s)π(aj ∣s)(An(s,ak∏⑸刃)-βMk∏(β,j)j) into the dual problem
(11), we can reformulate (11) into:
NN
m≥n {βδ+L X 写⑺ dds=m≥n {βδ+Es~ρπX π(ajls)[An (S,akπ ⑸…M j"
(13)
The optimal β can then be obtained by solving (13).
We will further show that β* ≤ β := maXs∈s,k,j=ι...N,k=j (Mkj)-1(An(s, a®) - An(s,aj)).
In the general case, i.e., β ≥ 0, (10a) is non-negative because:
NN
Es~0π XXAn (s,ai)π(aj IS)K (i,j)	(14a)
i=1 j=1
NN
=Es~ρ∏ X π(aj|s)XAn(S,ai)fj*(i,j)	(14b)
j=1	i=1
N
=Es~Pπ En(aj IS)An(S,akπ (β*,j) )	(14C)
j=1
N
≥ Es~ρπ X n(aj|S){An (S, aj ) + β* Mks (β*,j)j }	(14d)
j=1
N
=Es~Pπ X πaj |S)e*Mkn(β* ,j)j	(14e)
j=1
≥ 0,	(14f)
where (14d) holds since An(S, akπ(β*,j)) - β*Mkπ(β*,j)j ≥ An(S, aj) - β*Mjj = An(S, aj).
When β* > maxs∈s,kj=ι…N,k=j{ A"Mj^a)}, We have that for all s ∈ S, kn(β*,j) = j.
Thus, fj*(i, i) = 1, ∀i and fj*(i,j) = 0, ∀i 6= j. The objective value (10a) will be 0 because
Es~PU Pi=ι PN=1 An (S,ai)∏(aj ∣S)f*(i,j)= Es~ρ∏ PN= An (S,ai)∏(ai∣S)=0. The lefthand
side of (10b) equals to Es^ρπ PN=I PN=I Mij∏(aj∣S)f*(i,j) = Es~ρπ PN=I Miin3∣s) = 0.
Thus, for any δ > 0, (10b) is always satisfied.
Since the objective of the primal Wasserstein trust-region constrained problem in (6) constantly
evaluates to 0 when β* > maxs∈s,kj=ι…N,k=j{ A (s,akM-A (s,aj)}, and is non-negative when β* ≤
15
Under review as a conference paper at ICLR 2022
Aπ(s,ak)-Aπ(s,aj)	Aπ (s,ak)-Aπ (s,aj)
maxs∈s,k,j=ι…N,k=j{ -I , Mkj < , " },we can use maxs∈s,k,j=ι…N,k=j{ -< , Mkj < , j，} as
an upper bound for the optimal dual variable β*.	□
B Proof of Theorem 2
Theorem 2.	(Performance improvement) For any initial state distribution μ and any βt ≥ 0, if
∣∣Aπ — Aπ∣∣∞ ≤ E for some e > 0, the WPO policy update with the inaccurate advantage function
An, guarantees the following performance improvement bound,
N	2E
J (πt+1) ≥ J (πt)+ βtEs 〜ρμt+1 Ej=Int (aj|S)M审⑸,j)j-1-.	⑺
Proof of Theorem 2.
J(∏t+ι) - J(∏t) = Es〜ρμt+ι Ea〜∏t+ι[Aπt (s, a)]	(15a)
N
=Es〜Pnt+1 X nt+1(ai|S)Ant (s,ai)	(15b)
i=1
NN
=Es 〜P；t+1 XX ∏t(aj ∣s)fs(i,j )Aπt (s,ai)	(15c)
NN
=Es"t+ι X ∏t(aj |s) X fs(i,j)Aπt (s, ai)	(15d)
N
=Es"t+1 X πt(ajIS)Ant (S, akst (βt,j))	(15e)
j=1
N
≥ Es 〜ρμt+1 X πt(ajIS)[Ant(S,a) + BtMkst (βt,jj - 2e]	(15f)
j=1
N	2E
=βtEs 〜ρμt+1 I>t(aj|S)Mnt (βt,j)j- L，	(15g)
-γ
where (15a) holds due to the performance difference lemma in Kakade & Langford (2002);
(15f) follows from the definition of k∏t (βt,j) and the fact that ∣∣Ant - Ant ∣∣∞ ≤ e, therefore
[A t(s,a^∏:(βt,j)) + e] - βtM^πt(βt,j)j ≥ A t(s,a^πt(βt,j)) - βtM^πt(βt,j)j ≥ A t(S,aj) -
BtMjj = Ant (s, aj) ≥ Ant(s, aj) - e; (15g) holds since Ea〜∏ [An(s, a)] = 0.	□
C Proof of Theorem 3
Theorem 3.	If Assumption 1 holds, then the optimal solution to the trust region constrained problem
(4) with Sinkhorn divergence is:
πλ (ai|S)
^N^	exp (*An (s, ai) - λMij )
N —N------J----------------∏(aj IS),
j = 1 Pk=1 exp (氏 An (S, ak ) - λMkj )
(8)
where M denotes the cost matrix and βλ is an optimal solution to thefollowing dual formulation:
minβ≥0 Fλ (B)
minβ≥0 {βδ - Es〜Pπ PN=I π(aj |S)(λ + λ ln(π(aj IS))-
β i ∣^∖^^N	( λ 人n (	∖	∖ ι∖∕Γ Mλ I N	n^N n^,N	β exp (βA (s,ai)-λMij )∙n(aj|s) 1
λ lnE i=1 eXp( β A (S,ai) - λMij )]) + Es~Pπ Ei=I Σj=1 λ PN=I exp(专 A∏(s,ak )-λMkj∙) j. (9)
max
Moreover, we have βλ ≤ 2A-
16
Under review as a conference paper at ICLR 2022
Proof of Theorem 3. Invoking the definition of Sinkhorn divergence in (3), the trust region con-
strained problem with Sinkhorn divergence can be reformulated as:
NN
max	Es~ρ∏[〉,广ιA (S, Oi)T --ιQij]
Q	i=1	j=1
St Es〜Pπ XNI XNIMjQ j + 1 Qjlog Q j] ≤ δ
i=1 j=1	λ
N
i=1Qisj = π(aj |S),	∀j =1, . . . ,N, S ∈ S.
(16a)
(16b)
(16c)
Let β and ω represent the dual variables of constraints (16b) and (16c) respectively, then the La-
grangian duality of (16) can be derived as:
N
N
max βmnωL(Q,β,ω)=max β≥0nωEs 〜pπ [Σ An (S,ai) EQj]
,	,	i=1	j=1
N N	NN
+	Xωjs(XQisj	-	π(aj IS))dS +	β(δ	-Es 〜ρ∏	[XX MjQj	+ λQjlog QjD
s∈S j=1	i=1	i=1 j=1
(17a)
N	N	N N ωs
max βm≥0nω Es 〜ρ∏ X An (s，ai) X Qj]+/ es XX 拓js) Qj ρυ (s)ds
N	NN	1
-	X ωjπ(aj IS)ds + βδ - βEs〜Pπ [XX MijQsj + λQjlog QjD
s∈S j=1	i=1 j=1	λ
N
max min βδ -	ωjs π(aj IS)dS
Q β≥0,ω	s∈S j=1
N N	ωs	β
+ Es 〜ρ∏ [E E(An (s, ai) - βMj +	j ) )Qj - λQjlog Qj]
i=1 j=1	ρυ (S)	λ
N
min max βδ -	ωjs π(aj IS)dS
β≥0,ω Q	s∈S j=1 j j
N N	ωs	β
+ Es 〜Pπ [XX(An (s, ai) - βMij + ∏j ) )Qsj - λQjlog Qj],
i=1 j=1	ρυ (S)	λ
(17b)
(17c)
(17d)
where (17d) holds since the Lagrangian function L(Q, β, ω) is concave in Q and linear in β and ω,
and we can exchange the max and the min following the Minimax theorem (Sion, 1958).
Note that the inner max problem of (17d) is an unconstrained concave problem, and we can obtain
the optimal Q by taking the derivatives and setting them to 0. That is,
∂L	ωs	β
∂Qs	= A	(S,ai)	- βMij	+	ρ∏ (S)	- ʌ (log Qij	+ I)= 0,	∀i,j = 1,…，N,S ∈	S.	(18)
Therefore, We have the optimal Qij as:
λ	λωs
Qs =exP(万 An (S,αQ- λMj )exp( ɪj - 1), ∀i,j = 1,…，N,s ∈S∙	(19)
ij	β	βρυπ (S)
In addition, since PiN=1 Qisjj = π(aj IS), We have the folloWing hold:
λωjs
exp (-J - 1)
,βρ∏ (s)'
π(aj IS)
Pi=I exp(λAn(s,ai)- λMj)
(20)
17
Under review as a conference paper at ICLR 2022
By substituting the left hand side of (20) into (19), We can further reformulate the optimal Qsj as:
Qsj =	:xP( β A(S,ai) - λMij)_ ∏(aj∣s), ∀i,j = 1,…，N,s ∈S.	(21)
i	PN= exp( λ Aπ (s,ak) — λMkj)
To obtain the optimal dual variables, based on (20), we have the optimal ωj as:
ωsj = ρU (s){T + V ln(π(aj IS))- V ln[X exp (万An (S, ai) — λMij)]}, ∀j = 1, ∙∙∙ ,N,s ∈ S
λλ	λ	β
i=1
(22)
By substituting (21) and (22) into (17d), we can obtain the optimal βj via:
N	ββ	β N λ
m≥in βδ - Esy EnS IS){ λ + λ ln(n(aj|S))- λ ln[ΣexP( βA (s,ai) - λMij )]}
j=1	i=1
B	XX XX β exP( λAπ (s,ai) — λMij ) ∙ π(aj IS)
+ F i=1 j=1 λ Pk= exp(λAn(s,ak) - λMk) .
The proof for the upper bound of sinkhorn optimal β can be found in Appendix D.
□
D Upper b ound of Sinkhorn Optimal Beta
In this section, we will derive the upper bound of Sinkhorn optimal β. First, for a given β, the optimal
Qisjj (β) to the Lagrangian dual L(Q, β, ω) can be expressed in (21). With this, we will present the
following two lemmas:
Lemma 1. The objective function (16a) with respect to Qisjj (β) decreases as the dual variable β
increases.
Lemma 2. If Assumption 1 holds, then for every δ > 0，Qsj (2Amax) is feasible to (16b)for any λ.
We provide proofs for Lemma 1 and Lemma 2 in Appendix D.1 and Appendix D.2 respectively.
Given the above two lemmas, we are able to prove the following proposition on the upper bound of
Sinkhorn optimal β:
Proposition 1. If βλj is the optimal dual solution to the Sinkhorn dual formulation (9)， then βλj ≤
2Amax
言一for any λ.
Proofof Proposition 1. We will prove it by contradiction. According to Lemma 2, Qsj (2Amax) is
feasible to (16b). Since βj is the optimal dual solution, Qsj(βj) is optimal to (16). If βj > 2Amax,
according to Lemma 1, the objective value in (16a) with respect to 2A- is smaller than the objective
value in (16a) with respect to βλj , which contradicts the fact that Qisjj (βλj ) is the optimal solution to
(16).	□
D.1 Proof of Lemma 1
Lemma 1. The objective function (16a) with respect to Qisjj (β) decreases as the dual variable β
increases.
Proof of Lemma 1. Let Gλ(β) represent the objective function (16a). By substituting the optimal
Qisjj in (21) into (16a), we have:
Gλ(β)
ES〜P∏ [∑ An(S,ai)∑
i=1	j=1
NN
exp (λAn(S,ai)- λMij)
Pk=1 exp(λ An (S,ak) — λMkj)"⑸⑹
F [£ π(aj IS) EAn (S,ai)
j=1	i=1
exp (λAn(S,ai) — λMij)
Pk=I exp ( λ An (s, ak ) — λMkj )
(23a)
(23b)
N
N
18
Under review as a conference paper at ICLR 2022
For any β2 > β1 > 0, we have:
Gλ(β1) -Gλ(β2)
NN
X π(aj Is) X An (s,ai){
j=1	i=1
exp ( βAn(S, ai) - λMij )
PN=I exp ( β An (S, ak) - λMkj )
exp ( βλ2An(S, ai) - λMij )
PN=I exp ( βAn (s, ak) - λMkj )
(24a)
NN
En(Oj |s) EAn (s,a[i]){
j=1	i=1
exp ( β Aπ (s, a[i]) - λM[i]j )
PN=I exp (βAn(s, a[k]) - λM[k]j)
exp ( βAn(S, a[i] ) - λM[i]j )	}
PN=I exp ( βAn (s, a[k]) - λM[k]j )
(24b)
where [i] denotes sorted indices that satisfy Aπ(s,a[i]) ≥ Aπ(s,a[2]) ≥ … ≥ Aπ(s,a[N]).Let
f (.)=	exp (βAn(S, a[i]) - λM[i]j)	exp (βAn(S, a[i]) - λM[i]j)
PN=I exp ( β An (S, a[k]) - λM[k]j )	PN=I exp ( βAn (S, a[k]) - λM[k]j )
(25a)
=	exp ((β - β)An(S, a[i]))exp (βAn(S, a[i]) - λM[i]j)
PN=I exp (( β - β )Aπ (S, a[k])) exp ( βAn (S, a[k]) - λM[k]j )
exp( βAn(S,a[i] ) - λM[i]j )	C0
-----N--------7--------------------.	(25b)
Pk = 1 exp ( βAn (S, a[k]) - λM[k]j )
For notation brevity, we let m§(i)	= exp ((金一含)Aπ(s, a[i]))	>	0, w§(i)	=
eχp( 含An(S, a[i] ) - λM[i]j ) > 0 and qs(i) = PN=I m1(k)ws(k) - PN=I mS(i)ws(k) .Then we have
(25b) =	ms(i)ws⑴Ws⑴
PN=I ms(k)ws(k)	PN=I ws(k)
ms("ws("(PN=I ms(k)ws(k)	PN=I ms(i)ws(k)
= ms(.)ws(.)qs(.).
(26a)
(26b)
(26c)
Since 金一金 > 0, ms(i) decreases as i increases. Thus, qs(i) decreases as i increases. Since
ms(1) ≥ ms (k) and ms(N) ≤ ms (k) for all k = 1, . . . , N, we have qs(1)
________1_______ ≥ __________1___________________1_______
Pk=I ms (1)ws(k) — Pk=I ms(k)ws(k)	Pk=I ms (k)ws(k)
2	1________ ≤	2	1______________“	1_______
PN=1 ms (N )ws(k) _ PN=1 ms(k)ws(k)	PN=I ms(k)ws(k)
1_______
PN=1 ms(k)ws (k)
0, and qs(N) = PN=I m1(k)ws(k) -
0. Since qs(1) ≥ 0, qs(N) ≤ 0 and
qs(.) decreases as . increases, there exists an index 1 ≤ ks ≤ N such that qs(.) ≥ 0 for . ≤ ks
and qs(.) < 0 for . > ks. Since ms(.), ws(.) > 0, we have fs(.) ≥ 0 for . ≤ ks and fs(.) < 0
for . > ks. In addition, we have PiN=1 fs(.) = 0 directly follows from the definition. Thus,
PiN=1 fs(.) = Pik=s 1 |fs(.)| - PiN=ks+1 |fs(.)| = 0. Therefore,
—
NN
Gλ(β1) - Gλ(β2) = Es〜ρ∏ X∏(aj∣s) X An(s,a[i])fs(i)
j=1	i=1
(27a)
N	ks	N
Es〜p∏ X∏(aj∣s){X Aπ(s,aw)∣fs(i)∣- X An(s,aw)∣fs(i)∣}
j=1	i=1	i=ks+1
(27b)
N	ks	N
≥ Esy X∏(aj∣s){X Aπ(s,a[ks])∣fs(i)∣-	X An(s,a[ks + 1])∣fs(i)∣}
j=1	i=1	i=ks+1
(27c)
19
Under review as a conference paper at ICLR 2022
N	ks	N
=Es~ρ∏ X∏(aj∣s){Aπ(s,a[ks]) X ∣fs(i)∣-An3。及+i]) X 1fs(i)∣}
j=1	i=1	i=ks+1
(27d)
N	ks	ks
=Es~P∏ X∏S∣s){Aπ(s, a[ks]) X Ifs⑶一 An(s,。及+1]) X lfs(i)∣}
j=1	i=1	i=1
(27e)
N	ks
=Es~pπ X∏(aj∣s)(Aπ(s,a[ks]) - An区。也 + 1])) X Ifs(i)I	QIf
j=1	i=1
≥ 0.	(27g)
where (27c) and (27g) hold since Aπ(s, a[i] ) is non-increasing as i increases. Furthermore, at least
one inequality of (27c) and (27g) will not hold at equality since PN=I π(a∕s)Aπ(s, a# = 0, ∀s ∈ S,
and for non-trivial cases, P r{Aπ (s, a) = 0, ∀s ∈ S, ∀a ∈ A} < 1, which means Pr{∃s1, s2 ∈
S, a1,a2 ∈ A, s.t. Aπ(s1,a1) = Aπ(s2,a2)} > 0. Therefore, we have Gλ(βι) — Gλ(β2) > 0. 口
D.2 Proof of Lemma 2
Lemma 2. If Assumption 1 holds, then for every δ > 0, QSj( 2Amax) is feasible to (16b)for any λ.
ProofofLemma 2. By substituting the optimal Qsj in (21) into (16b), we can reformulate the left
hand side of (16b) as follows:
NN	1
Es~Pπ [X X Mij Qsj + λ Qsj log Qs ]	(28a)
i=1 j=1
NN	1 λ
Es~ρ∏ {£ EMij Qij + λQij[ βA (s,ai) -
i=1 j=1
λMij + log
_________Maj Is)_________]}
PN=1 exp(λAnGak) - λMkj)
(28b)
NN1	1
Es~ρ∏ {£ £ βQsj An (S, ai) + λQsj log
i=1 j=1
___________π(aj Is)________}
Pk=ι exp(ʌAπ(s,ak) - λMkj) ʃ
Now We prove that when β = 2Amax,旧§~。2{PN=ι PN=I βQsj(β)Aπ(s,ai)} ≤
Es-pπ { 1 Qsne) log PN= exp(∏Aπls,ak)-λMkj ) } ≤ δ hold. For the first part, We have:
NN
Es~P∏{XX βQijAπ(s,ai)}
i=1 j=1
NN
=β Es~pu {X[X QsjAn (s,ai)}
β	i=1 j=1
1N
=β Es~p∏ {∑ ∏0(ai∣s)Aπ (s,ai)}
β	i=1
1N
≤ βEs~°π{J2n(ai|S)IA (s,ai)|}
Amax	δ
≤ β = 2.
For the second part, the followings hold:
NN
Es~ρ∏ {X X 1 Qsj log F---ʌ-------------
i=1 j⅛ λ i Pk=I exp( λ An (s,ak)-λMkj)
(28c)
δ and
(29a)
(29b)
(29c)
(29d)
(29e)
(30a)
π(ajIs)
}
20
Under review as a conference paper at ICLR 2022
F {X λ (X Qsj )* log Pk= exp( λ An；S\)-λMkj) }
1 j {X π(aj ls)log Pk= exp( /?；：)-λMkj) ?
≤
1 Es〜Pπ {X π(aj Is) log -Tπλ⅛)~~T7}
λ	j=1	eχp(λ An (s,aj))
≤
≤
≤
1k	1
λ Jρ∏<gπ(ajls)logeχp(λA∏(s,aj)) }
1k	λ
λEs〜pπ{£π(ajIs)(-βA (s,aj))}
1k
βEs〜pπ{£π(aj|s)|A (s,aj)|)
β	j	=1
Amax	δ
—=一
β	2
(30b)
(30c)
(30d)
(30e)
(30f)
(30g)
(30h)
Therefore, QSj(2Amax) is feasible to (16b) for any λ.
□
E Gradient of the Objective in the S inkhorn Dual Formulation
The closed-form gradient of the objective in the Sinkhorn dual formulation (9) is as follows:
δ — Es 〜0π
N	11
XWajIs)+ 鼠In(WajIs))-
λλ
j=1
1N	λ
λ ln[工 exp(βA (s,ai) - λMj)]
β	1	Nλ
-λ ∙ PN----(入 d7√---ʌ^^ΓΓTT × X[exp (WA (s,ai) - λMij) ×一λA (s,ai)β ]}
λ ∑i=ι exp(λAπ(s,ai) - χMij)	M β	J
E X X n ∏(aj'Is)	exp(λAπ(s,ai) - λMij)
+ s~Pu = j=1	λ	PN=I exP(λAπ(s,ak) - λMkj)
β∏(aj Is) exp ( βλ Aπ(s, ai) - λMij ) × -λAπ(s, ai)β 2 × PN=I exp ( λ Aπ(s, ak) - λMkj )
(PN=I exp( λ Aπ(s,ak)-λMkj ))2
β∏(aj∣s) exp ( λ Aπ(s, ai) - λMij ) × PN=ι[exp ( λ Aπ(s, ak) - λMkj ) × -λAπ(s, ak)β 2]
(PN=1 exp(λAn(s,ak)- λMkj))2
F Proof of Theorem 4
Given the upper bound of Wassertein optimal β in Theorem 1 and the upper bound of Sinkhorn
optimal β in Proposition 1, we are able to derive the following theorem:
Theorem 4. Define βUB = max{ 2A竺，β}. The following holds:
1. Fλ(β) converges to F(β) uniformly on [0, βUB],
2. lim arg min Fλ(β) ⊆ arg min F (β).
Proof of Theorem 4. To show that Fλ(β) converges to F(β) uniformly on [0, βUB], it is equivalent
to show that limλ-→∞ sup0≤β≤βUB Fλ(β) - F (β) = 0. Let sπ(β, i, j) = Aπ (s, aksπ (β,j) ) -
21
Under review as a conference paper at ICLR 2022
βMk∏(β,j)j - [Aπ(s,ai) - βMij] where k∏(β,j) ∈ K∏(β,j) = argmaXk=L..nAπ(s,ak) - βMkj
is an arbitrary optimizer, and eɪ (β, i,j) ≥ 0. First, we have
Fλ(β)- F (β)∣
I	旦..一8 β ............ β	ʌ λ _	一
=∣ βδ - Es~ρj; y^π(αj∣s){^	+ λ	ln(π(ajIS))	- λ ln[y^exp	(βA (s,	a，)-	λMij)]}
N N
∖^∖^β exp(》Aπ(s, a，)- λMij) ∙ π(aj∣s)
+ SPP i=ι j=ι λ PN=I exP(λAn(S,ak)-λMkj)
-βδ
N
-ES〜Pn En(aj Is)[An (S) akp(β,j)) - βMkn (β,j)j]
j=1
(31a)
β	N	β	N
≤ I ʌ ES 〜Pn X n(a |s)|+1 λ ES 〜Pn X n(a|s)in(WajIS)) I
j=i	j=i
i e X X β eXP(λ An(S,ai) - λMij) ∙ n(aj IS) i
+ l "*p 2 j=1 λ PN=I exp(λ An(S,ak) - λMkj) 1
N	β N ʌ
+ R 〜Pn X π(aj |s) ʌ ln[Xexp (βAn (S,电)-λMij)]
j=1	i=1	P
N
-ES〜Pn X π(aj |s)[An (S, akp(β,j)) - βMkn (β,j)j] l
j=i
β	N	β	N
≤ 2 l λ ES 〜Pn X Waj |s)| + I λ ES 〜Pn X Waj IS)In(Waj IS)) I
(31b)
j=1
j = 1
i _ A , . . β, C ,λ 一 .— —
+ R〜Pn Xπ(aj|s) ʌ ln[Xexp (βA (S,电)一λMij)]
j=1	i=1	P
N
-ES〜Pn £n(aj |s)[An (S, akJ (β,j)) - βMkn (β,j)j]
j=i
(31c)
In addition,
N	β N ʌ
R 〜Pn X π(aj |s) λ ln[X exp ( βAn (S, ai) - λMij )]
j=1	i=1	P
N
-ES〜Pn X π(aj |s)[An (s, akJ (β,j)) - βMkn (β,j)j] l
j=i
i	、b∕λ 4πz	λ n»
= IES 〜Pn X π(aj |s) ʌ ln[exp ( βA (S) akn(β,j)) - λMkn (β,j)j ) X exp (-βeS (β, i, j))]
(32a)
N
-ES〜Pn X π(aj |s)[An (S, akJ (β,j)) - βMkn (β,j)j] l
j=i
i 口 £	/ i ∖β「一 zλ W	、	、»
=I ES 〜Pn ΣWajIS)λ(ln[exp(βAn(s,akj(β,j)) - λMkn(β,j)j
j=i	β
N
-ES〜Pn X π(aj |s)[An (s, akJ (β,j)) - βMkn (β,j)j] l
j=i
(32b)
Nλ
)]+ lnZ exp(- β^π (β, i, j))]}
i=1	β
(32c)
22
Under review as a conference paper at ICLR 2022
N	R N	λ
Esy X π(aj∣s)λ lnX exp(-Ren(R,i, j))]∣∙
j=1	i=ι	P
(32d)
Therefore,
ʌlim	sup J Fχ(β') — F (R) ∣	(33a)
―	2Rm∣ 一 Λ . . .∣ 一	Rm∣ 一 ，一......................∣
≤ λ-m∞ -ɪ ∣ Es~PU X Waj ls)| + λ-m∞ — ∣ Es~Pπ X Waj ∣s)ln(π(aj∣s)) ∣
j=ι
j=ι
+ lim sup
λ→∞ 0≤β≤βUB
=lim sup
λ→∞ 0≤β≤βUB
β	N	N	入
λ ∣ Esy X π(a IS)in[X eχp(-Ren(R, i, j))] ∣
j=1	i=1	R
β	N	N	ʌ
λ IES 〜°π X WajIS)ln[X eχp(- Re (R,i,j ))] ∣ ∙
j=1	i=1	R
(33b)
(33c)
In addition, ∀R ∈ [0, Rub] and ∀λ, e∏ (R, i, j) is bounded since
∣ eZ (R, i, j)| = ∣ Aπ (S,akKβ,j力一RMks (β,j)j - [An (s, ai) - BMij] ∣	(34)
=(An(S)aks (β,j)) - Aπ (S,ai X-(RMks (β,j)j - RMij ) ∣
≤ An (s, aks(β,j)) - An (s, ai) j + 卜 Mkn (β,j)j - R Mij ∣
≤ 2 max An (s, a) + RUB max Mij
s,a	i,j
≤ 2Amax + Rub max Mij < ∞.	(35)
一	i,j
Then,	∣ ES 〜Pn Pj=I ∏(aj ∣S)ln[P3 exp(- λ en(R,i,j))] ∣	is	bounded.
Therefore in (33c), the optimal R can be achieved. Let Rλ =
argmaχ0≤β≤βuB λ ∣ ES 〜Pn PN=I π(aj |S)ln[p3 eχp(-λ en(R,i,j))] ∣ , and then we have:
R	N	N	ʌ
.lim	SUP 二 ∣ ES〜PnXMaj|S)ln[Xeχp(-万理(R,i,j))] ∣
λ→∞0≤β≤βuBλl	j=1	M	R	1
R λ	N	N	λ
=λ→→o ɪ IES 〜Pn X n(aIS)ln[X eχp(-于琮(Rλ,i, j))] ∣ .
∞	j=1	i=1	P
(36a)
(36b)
Define a,”) = min°≤β≤βuB mi%=ι…N,i∈κ∏(β,j) e:(R,i,j). Then since e?(R,i, j) > 0 for i ∈
Kn (R, j) based on its definition, we have a$(j) > 0. On one hand, we have
.lim ln[Xeχp(-点C(Q,i,j))]
λ—→∞ z—*	R 人
i=ι	X
(37a)
N λ N λ
.lim ln[	E	eχp(-不en(Rλ,i,j))+	E	eχp(-而en(R"i,j))] (37b)
入 →C∞	R	R
i=1∣i∈K∏ (βλ,j)	X	i=1∣i∈K∏ (βλ,j)	X
N	ʌ	N
≤ λl→∞ln[	X	eχp(-R—aS(j))+	X	eχp (0)]
∞ i=1∣i∈K∏ (βλ,j)	IOUB	i=1∣i∈K∏ (βλ,j)
N λ
.lim ln[ E eχp(-丁aS(j)) + IKn(Ri,j)|]
λ→∞ i=ι∣i∈Kπ (βλ,j)	RUB
λl→∞ ln[∣κn (Rλ,j)∣].
(37c)
(37d)
(37e)
23
Under review as a conference paper at ICLR 2022
On the other hand, we have
，lim ln[Xexp(-点eS(βλ,i,j))]
λ-→∞	β
i=1
(38a)
= lim λ-→∞	N ln[	X i=1∣i∈K∏ (βλ,j)	eχp (—不 eS (β λ, i, j)) + β		N X i=1∣i∈K∏ (βλ,j)
≥ lim λ-→∞	N ln[	X i=1∣i∈K∏ (βλ,j)	exp(-β	esS(βλ,i,j))]	
= lim λ-→∞	N ln[	X i=1∣i∈K∏ (βλ,j)	exp (0)]		
=λ→∞ ln[∣K∏(βλ,j)∣].				
exp(-βeπMij))] (38b)
(38c)
(38d)
(38e)
Therefore, limλ→∞ ∣ ln[pN=1 exp(-含e∏(βλ,i,j))]∣ = limλ→∞ ln[∣K∏(βλ, j)∣]. Based on that,
we have
βλ	N	N	λ
，lim k∣Es~PπXn(aj|S)InXeχp(-eS(βx,i,j
λ-→∞ λ υ	β
j=1	i=1
≤ λlim βτ∖Xln[Xeχp(-λλeeS(βλ,i,jM
λ-→∞ λ	β
j=1 i=1
≤ λl→∞ β X∖ln[X exp(- βeS (βλ,i,j 训
j=1	i=1
βλ N
=λl→o 彳 X ln[∣KS (βλ,j)∣]
∞ λ j=1
≤ lim βUBN ln N = 0,
λ-→∞ λ
(39a)
(39b)
(39c)
(39d)
(39e)
which means limλ-→∞ sup0≤β≤βUB ∖∖Fλ(β ) - F(β )∖∖	≤ 0. Furthermore, since
limλ→∞ supo≤β≤βUB ∣Fλ(β) - F(β)∣ ≥ 0 holds naturally, We have limλ→∞ sup°≤β≤βUB ∣Fλ(β)-
F(β )| = 0. Therefore, Fλ(β ) converges to F(β ) uniformly on [0, βUB], which also indicates
Fλ(β ) epi-converges to F(β ) on [0, βUB] (Royset, 2018; Rockafellar & Wets, 1998). By properties
of epi-convergence, We have that λl-→im∞ 0a≤rgβ≤mβin Fλ(β ) ⊆ 0a≤rgβ≤mβin F(β ) (Rockafellar & Wets,
1998).	— — UB	— — UB	口
G	Optimal Beta for a Special Distance
Proposition 2. (1). If the initial point β0 is in [maxs,j {AS (S, aks) - AS (S, aj)}, +∞), the local
optimal β solution is maxs,j {AS (S, aks ) - AS (S, aj)}.
(2)	. If the initial point β0 is in [0, mins,j 6=ks {AS (S, aks) - AS (S, aj)}]: if δ - s∈S ρS (S)(1 -
π(aks |S))dS < 0, the local optimal β is mins,j6=ks {AS (S, aks) - AS (S, aj)}; otherwise, the local
optimal β solution is 0.
(3)	. If the initial point β0 is in (mins,j6=ks {AS (S, aks ) - AS (S, aj)}, maxs,j {AS (S, aks) -
AS (S, aj)}), we construct sets Is1 and Is2 as:
for S ∈ S, j ∈ {1, 2 . . . N} : if β0 ≥ AS(S, aks) - AS(S, aj) then Add j to Is1 else Add j to Is2.
Then, if δ — Es~ρ∏ Ej∈∕ 2 ∏(aj |s) < 0, the local optimal β is mins∈s ,j∈12 {Aπ (s,aks) — AS (s,aj)};
otherwise, the local optimal β is maxs∈S,j ∈I 1 {AS (S, aks ) - AS (S, aj)}.
24
Under review as a conference paper at ICLR 2022
Proof of Proposition 2. (1). When β ∈ [maxs,j {Aπ (s, aks) - Aπ (s, aj)}, +∞), we have
Aπ(s,aj) ≥ Aπ(s,aks) - βforalls ∈ S,j = 1 . . .N. Since Aπ (s, aks) - β ≥ Aπ(s,ak) - βfor
all k = 1 . . . N, we have Aπ (s, aj) ≥ Aπ (s, ak) - β for all s ∈ S, j = 1 . . . N, k = 1 . . . N. Thus,
j ∈ K∏(β*,j), for all S ∈ S, j = 1 ...N. Therefore, (6) can be reformulated as:
N
m≥in{βδ + Es” En(OjIs)An(S,aj)}.
β≥	j=1
Since δ ≥ 0, we have the local optimal β = maxs,j{Aπ(s, aks) - Aπ(s, aj)}.
(2). When β ∈ [0, mins,j6=ks {Aπ (s, aks) - Aπ(s, aj)}], we have Aπ(s, aj) ≤ Aπ(s, aks) - β for
all s ∈ S, j = 1 ...N, j = ks. Thus k§ ∈ K∏(β*,j) for all s ∈ S, j = 1 ...N. The inner part of
(6) then is:
N
βδ + Es〜ρπ{ X n(aj∣s)(Aπ(s,aks) - β) + n(aks ∣s)Aπ(s,aks)}
j=1,j 6=ks
NN
β(δ- Es〜ρ∏	E n(ajIs)) + Es〜ρ∏ ]Tn(aj∣s)Aπ(s,。®,)
j=1,j 6=ks	j=1
β(δ -	ρυπ(s)(1
s∈S
N
-π(aks ∣s))ds) + Es〜ρπ Ε∏(aj∣s)Aπ(s, ak,).
j=1
If δ - s∈S ρυπ(s)(1 - π(ak, Is))ds < 0, we have the local optimal β = mins,j6=k, {Aπ(s, ak, ) -
Aπ(s, aj)}. If δ - s∈S ρυπ(s)(1 - π(ak, Is))ds ≥ 0, we have the local optimal β = 0.
(3). For an initial point β0 in (mins,j6=k, {Aπ (s, ak,)-Aπ (s, aj)}, maxs,j {Aπ (s, ak,)-Aπ (s, aj)}),
we construct partitions Is1 and Is2 of the set {1, 2 . . . N} in the way described in Proposition 2 for all
s ∈ S. Consider β in the neightborhood of β0, i.e., β ≥ Aπ(s, ak, ) - Aπ(s, aj) for s ∈ S,j ∈ Is1
and β ≤ Aπ(s, ak, ) - Aπ(s, aj) for s ∈ S,j ∈ Is2. Then the inner part of (6) can be reformulated
as:
βδ + Es〜ρπ{X n(aj∣s)Aπ(s,aj) + X π(aj∣s)(An(s,。k,) - β)}
j∈I,1	j∈I,2
=β(δ - Es 〜Pn E π(aj IS)) + Es 〜Pn {£ π(aj IS)An (s,aj ) + E π(aj IS)An (s,aks )}.
j∈I,2	j∈I,1	j∈I,2
If δ - Es〜Pn Pj∈ι2 ∏(ajIs) < 0, We have the local optimal β = mins∈sj∈ι2{An(s, aks)-
An(s,aj)}. If δ - Es〜Pn Pj∈12 n(ajIs) ≥ 0, we have the local optimal β =
maxs∈s,j∈11 {An(s,aks) - An(s,aj)}.	□
H Implementation Details
Visitation Frequencies Estimation: The unnormalized discounted visitation frequencies are needed
to compute the global optimal β*. At the k-th iteration, the visitation frequencies Pn are estimated
using samples of the trajectory set Dk. Specifically, we first initialize ρkn(s) = 0, ∀s ∈ S. Then for
each timestep t in each trajectory from Dk, we update Pn as Pn(St) J Pn(St) + Yt/IDk I.
Policy Representation: The general approach depicted in Algorithm 1 allows various policy
representations including arrays and neural networks. Let Sk ⊆ S represent a subset of states to
perform the policy update at the k-th iteration. When an array is used, the policy update step is simply
∏k+ι (∙I s) = F (∏k)(∙I s), ∀s ∈ Sk. When a neural network is employed, the policy update step can
be achieved by obtaining the gradient descent, i.e.,
V E(F(∏k)(∙Is) - ∏k(∙Is))2
s∈Sk
Policy Updating Strategy:
•	State space: For environments with a discrete state space (e.g., tabular domains), the
WPO/SPO policy update is performed for all states at each iteration. For the environments
25
Under review as a conference paper at ICLR 2022
with a continuous state space, a random subset of states is sampled at each iteration to
perform the policy update.
•	Action space: For environments with a continuous action space, we first discretize the action
space following Tang & Agrawal (2020) and then WPO/SPO policy update is performed on
the discretized action space.
Hyperparamaters and Additional Results:
Our main experimental results are reported in Section 6. In addition, we provide the setting of
hyperparamaters and network sizes of our WPO/SPO algorithms in Table 3. And we present the
numerical results of the final performance comparison among our algorithms and the baseline methods
(i.e., TRPO, PPO, A2C) in Table 4.
Table 3: Hyperparamaters and network sizes
	Taxi-V3, NChain-V0 CliffWalking-VO	CartPole-V1	Acrobot-V1
γ	^09	0.95	0.95
lrπ		10-2	5 × 10-3
lrvalue	-10-2	10-2	5 × 10-3
|Dk|	60 (Taxi); 1 (Chain); 3 (CliffWalking)	2	3
π size	2D array	[64, 64]	[64, 64]
Q/v size	[10,7,5]	一	[64, 64]	[64, 64]
λ	5, 50,10	10	10
Table 4: Averaged rewards over last 10% episodes during the training process
EnVironment	WPO	SPO	TRPO	PPO	A2C
Taxi-V3	-45 ± 27	-87±11	-202 ± 3	-381 ± 34	-338 ± 30
NChain-V0	3549 ± 197	3432 ± 131	3522 ± 258	3506 ± 237	1606 ± 10
CliffWalking-V0	-35 ± 15	-25±1	-159 ± 94	-3290 ± 2106	-5587 ± 1942
CartPole-V1	388 ± 54	370 ± 30	297 ± 65	193 ±45	267 ± 61
Acrobot-V1	-162±8	-185 ± 15	-248 ± 33	-103 ± 5	-379 ± 39
Additional Experiments on Ablation Study:
Episode
①α ΦCT2Φ><
Figure 7: Episode rewards during the training process for different β and λ settings, averaged across
3 runs with a random initialization. The shaded area depicts the mean ± the standard deviation.
26