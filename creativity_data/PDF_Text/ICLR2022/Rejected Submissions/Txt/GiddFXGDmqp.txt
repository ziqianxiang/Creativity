Under review as a conference paper at ICLR 2022
Spatially Invariant Unsupervised 3D Object-
Centric Learning and Scene Decomposition
Anonymous authors
Paper under double-blind review
Ab stract
We tackle the problem of deep object-centric learning from a point cloud which is
crucial for high-level relational reasoning and scalable machine intelligence. In
particular, we introduce a framework, SPAIR3D, to factorize a 3D point cloud
into a spatial mixture model where each component corresponds to one object. To
model the spatial mixture model on point clouds, we derive the Chamfer Mixture
Loss, which fits naturally into our variational training pipeline. Moreover, we adopt
an object-specification scheme that describes each object’s location relative to its
local voxel grid cell. Such a scheme allows SPAIR3D to model scenes with an
arbitrary number of objects. We evaluate our method on the task of unsupervised
scene decomposition. Experimental results demonstrate that SPAIR3D has strong
scalability and is capable of detecting and segmenting an unknown number of
objects from a point cloud in an unsupervised manner.
1	Introduction
Motivated in part by cognitive psychology studies (Kahneman et al., 1992) that suggest human brains
organize observations at an object level, recent advances in physical prediction (Chang et al., 2017)
and reinforcement learning (Diuk et al., 2008; Kansky et al., 2017) have demonstrated superior
robustness with environments that are modeled in an object-oriented manner. To exploit these
techniques for 3D scenes, which can exhibit complex and combinatorially large observation spaces
even when there are only a few basic elements, robust algorithms are needed to identity objects from
3D observations. We tackle in this paper the problem of deep object-centric learning from a point
cloud, which is crucial for high-level relational reasoning and scalable machine intelligence.
There is a good body of existing literature on unsupervised object-centric generative models for images
and videos. In particular, generative models based on Variational Autoencoders (VAE) (Kingma &
Welling, 2014) have been employed to model the pixel intensities of an image with spatial Gaussian
mixture models (Greff et al., 2019; Burgess et al., 2019; Lin et al., 2020; Crawford & Pineau, 2019;
Eslami et al., 2016). SPAIR (Crawford & Pineau, 2019) makes use of an object-specification scheme
which allows the method to scale well to scenes with a large number of objects.
The aforementioned works define objectness as a region with strong appearance correlations, and all
of them employ VAE (Kingma & Welling, 2014; Higgins et al., 2017) in their structure. To be more
precise, the encoder-decoder structure of VAE effectively creates an information bottleneck (Tishby
et al., 1999; Burgess et al., 2018) limiting the amount of information passing through. To reconstruct
the observation under limited information budget, highly correlated information must be exploited.
Thus, VAE plays a critical role in exploiting such correlations. The above mentioned works mainly
exploit appearance correlations on objects colored uniformly. In this work, we aim to prove that this
paradigm is also applicable to structural correlations conveyed by point clouds without appearance
information.
The irregularity of point cloud (detailed in section 3) renders a direct transfer of the above method
infeasible. Inspired by SPAIR and the fact that 3D point cloud object generation requires centred-
points (Chang et al., 2015), we propose in this paper a VAE-based model called Spatially Invariant
Attend, Infer, Repeat in 3D (SPAIR3D), a model that generates spatial mixture distributions on point
clouds to discover 3D objects in static scenes. Here we summarize the key contributions of this paper:
1
Under review as a conference paper at ICLR 2022
•	We propose, to the best of our knowledge, the first unsupervised point cloud object-centric
learning pipeline, named SPAIR3D.
•	We also propose a new Chamfer Mixture Loss function tailored for learning mixture models
over point cloud data with a novel graph neural network that can be used to model and
generate a variable number of 3D points.
•	We provide qualitative and quantitative results to show that SPAIR3D learns meaningful
object-centric representation and decompose point clouds scene with an arbitrary number of
objects in an object-oriented manner.
2	Related Work
Unsupervised Object-centric Learning. Unsupervised object-centric learning has attracted increas-
ing attention recently. A major focus of these methods is on joint object representation-learning
and scene decomposition from images or videos via generative models (Burgess et al., 2019; Greff
et al., 2019; Engelcke et al., 2020; Lin et al., 2020; Crawford & Pineau, 2019; Li et al., 2020; Chen
et al., 2021). In particular, spatial Gaussian mixture models are commonly adopted to model pixel
colors. The object-centric representation learning problem is then framed as a generative latent-
variable modelling task. For example, IODINE (Greff et al., 2019) employs amortized inference that
iteratively updates the latent representation of each object. GENESIS (Engelcke et al., 2020) and
MONET (Burgess et al., 2019) sequentially decode each object. Slot attention (Locatello et al., 2020)
and Neural Expectation Maximization (NEM) (Greff et al., 2017) also adopt the same formulation.
Instead of treating each component of the mixture model as a full-scale observation, Attend, Infer,
Repeat (AIR) (Eslami et al., 2016) confines the extent of each object to a local region. To improve
the scalability of AIR, SPAIR (Crawford & Pineau, 2019) employs a grid spatial attention mechanism
to propose objects locally, which has proven effective in object-tracking tasks (Crawford & Pineau,
2020). To achieve a complete scene segmentation, SPACE (Lin et al., 2020) includes MONET in its
framework for background modeling. Spatial attention models are also employed to reconstruct 3D
scenes in the form of meshes or voxel in an object-centric fashion from a sequence of RGB frames
(Henderson & Lampert, 2020). These methods similarly rely heavily on appearance and motion cues.
Graph Neural Network for Point Cloud Generation. Generative models such as VAEs (Gadelha
et al., 2018) and generative adversarial networks (Achlioptas et al., 2018) have been successfully
used for point-cloud generation but with a pre-defined number of points per object. Luo & Hu (2021)
models the point cloud generation process as a latent variable conditioned Markov chain. Yang
et al. (2019) proposed a VAE and normalizing flow-based approach that models object shapes as
continuous distributions. While the proposed approach allows the generation of a variable number of
points, it could not be naturally integrated into our framework because of the need for an ODE solver.
3	SPAIR3D
While the literature on deep unsupervised object-centric learning is rich, none of the methods listed
above can be applied directly to 3D point cloud data. The reconstructions of 2D images, 3D voxel
space, and neural radiance field in these methods are all coordinate-dependent (Watters et al., 2019;
Stelzner et al., 2021). To be more precise, given a coordinate, an occupancy value (mask or density)
and a feature vector (RGB color) are generated for each mixture component for that coordinate to form
a Gaussian mixture model. For image data, the coordinate dependency can be implicitly embedded in
the network structure since the input and output are of fixed sizes (Islam* et al., 2020). The coordinate
thus provides the correspondence between input and reconstruction, inducing a likelihood function.
However, point cloud data are irregular and take the form of an unordered set. Each point cloud may
have a varying number of points. Most importantly, the point coordinates carry all the structural
information and form the reconstruction target, making coordinate-dependent reconstruction not an
option. Due to the irregularity of point-cloud data, there is also usually no natural correspondence
between the input and the reconstruction.While Chamfer Distance commonly serves as a loss function
for point cloud reconstruction, it does not support mixture model formulation directly. Such data
irregularity makes defining a mixture model over point cloud a non-trivial task. We, therefore,
introduce SPAIR3D, a VAE-based generative model, to achieve 3D object-centric learning as well as
2
Under review as a conference paper at ICLR 2022
3D scene decomposition via object-centric point-cloud generation. It takes a point cloud as input and
generates a structured latent representation for foreground objects and scene layout.
In the following, we first describe latent representation learning. We then leverage variational
inference to jointly learn the generative model (§3.2) and inference model (§3.4). We further discuss
the particular challenges arising for generative models in handling a varying number of points with a
novel Chamfer Mixture Loss (§3.3) and Point Graph Decoder (§3.4).
Reconstruction
Voxel Grid Encoder Glimpse Proposal Glimpse Encoder Glimpse Decoder
Glimpse Encoder
(a) Structure of SPAIR3D	(b) Structure of Glimpse VAE
Figure 1: (a) Structure of SPAIR3D. For better illustration, we adopt 2D abstraction and use colors
to highlight important correspondence. (b) Structure of Glimpse VAE. Glimpse encoder encodes
foreground glimpses and produce ziwhat , zimask and zipres for each glimpse. Point Graph Decoder
takes ziwhat and reconstructs input points (left branch). Mask Decoder takes zimask and generates
masks for each point (middle branch). The dashed line represents the dependency on the coordinates
of the intermediate points in the hierarchy and Gi . Multi-layer PointGNN networks enable message
passing between (ci , fi) and produces zipres (right branch).
3.1	Loacl Object Proposal
As shown in Fig. 1a, SPAIR3D first divides a 3D scene into a spatial attention voxel grid. There can
be empty voxel cells covering no points. We discard empty cells and associate a bounding box with
each non-empty voxel cell. The set of input points captured by a bounding box is termed an object
glimpse. Besides object glimpses, SPAIR3D also defines a scene glimpse covering all points in an
input scene. Later, we show that we encode and decode (reconstruct) points in each glimpse and
generate a mixing weight on each point to form a probability mixture model.
3.2	Generative Model
Similar to SPAIR, each grid cell generates posterior distributions over a set of latent variables defined
as zicell = {ziwhere , ziapothem }, where ziwhere ∈ R3 encodes the relative position of the center of
the ith bounding box to the center of the ith cell, ziapothem ∈ R3 encodes the apothem of the
bounding box. Thus, each zicell induces one object glimpse associated with the ith cell. Each object
glimpse is then associated with posterior distributions over latent variables specified as ziobject =
{ziwhat, zimask, zipres}, where ziwhat ∈ RA encodes the structure information of the corresponding
object glimpse, zimask ∈ RB encodes the mask for each point in the glimpse, zipres ∈ {0, 1} is a
binary variable indicating whether the proposed object should exist (zipres = 1) or not (zipres = 0).
The scene glimpse is associated with only one latent variable zscene = {z0what }. We assume zipres
follows a Bernoulli distribution. The posteriors and priors of other latent variables are all set to
isotropic Gaussian distributions (see the appendix Sec. A for details).
Given latent representations of objects and the scene, the complete likelihood for a point cloud
X is formulated as p(X) = Rz p(z)p(X |z)dz, where z = (Si zicell) ∪ (Si ziobject) ∪ zscene. As
maximising the objective p(X) is intractable, we resort to variational inference method to maximize
its evidence lower bound (ELBO).
3.3	Chamfer Mixture Loss
Unlike generative model-based unsupervised 2D segmentation methods that reconstruct the pixel-wise
appearance conditioning on its spatial coordinate, the reconstruction of a point cloud lost its point-wise
3
Under review as a conference paper at ICLR 2022
correspondence to the original point cloud. Chamfer distance is commonly adopted to measure the
discrepancy between the generated point cloud (X) and the input point cloud (X). Formally, Chamfer
distance is defined by d°D(X, X) = Pχ∈χ mi%∈χ ∣∣x 一 X∣∣2 + Pχ∈χ minχ∈χ ∣∣x 一 X∣∣2∙ We
refer to the first and the second term on the r.h.s as the forward loss and the backward loss, respectively.
Unfortunately, the Chamfer distance does not fit into the variational-inference framework. To get
around that, we propose a Chamfer Mixture Loss (CML) tailored for training probability mixture
models defined on point clouds. The Chamfer Mixture Loss is composed of a forward likelihood and
a backward regularization corresponding to the forward and backward loss, respectively.
Denote the ith glimpse as Gi, i ∈ {0,..., n} and its reconstruction as Gi, i ∈ {0,..., n}. Specifically,
we treat the scene glimpse as the 0th glimpse that contains all input points, that is, G0 = X . Note
that one input point can be a member of multiple glimpses. Below We use N (μ, σ)(x) to denote the
probability density value of point X evaluated at a Gaussian distribution of mean μ and variance σ.
For each input point x in the ith glimpse, the glimpse-wise forward likelihood of that point is defined
as LiF (x) =UI maXχ∈G. N(X,σJ(x), where Ui = JX∈χ maxχ∈g. N(X,σc)(x)dx is the normalizer
and σc is a hyperparameter. For each glimpse Gi, i ∈ {0, . . . , n}, αix ∈ [0, 1] defines a mixing weight
for point x in the glimpse and Pin=0 αix = 1. In particular, αix, i ∈ {1, . . . , n}, is determined by
pres x
Of = Pni Zpres∏χ Zpres∏χ, where ∏χ is the predicted mask value and ∏χ = 0 if x ∈ Gi. The mixing
weight for the scene layout points completes the distribution through α0x = 1 一 Pin=1 αix for x ∈ G0 .
Thus, the final mixture model for an input point x is LF (x) = Pin=0 αixLiF (x). The total forward
likelihood of X is then defined as LF (X) = Qx∈X LF (x).
The forward likelihood alone leads to a trivial sub-optimal solution with X distributed densely
and uniformly in the space. To enforce a high-quality reconstruction, we define a backward reg-
ularization term. For each predicted point X, the point-wise backward regularization is defined
as LB (X) = maXχ∈Ge(^) N(x,σc)(X), where i(X) returns the glimpse index of X. We denote
x(X) = arg maXχ∈Ge(χ) N(x,σJ(X) and X = Un=0 Gi. The backward regularization is then defined
as LB(X) = Qn=o Qχ∈Ga LB (X)aX(x). The exponential weighting, i.e. αf(x) ∈ [0,1], is crucial. As
each predicted point X ∈ X belongs to one and only one glimpse, it is difficult to impose a mixture
model interpretation on the backward regularization. The exponential weighting encourages the
generated points in object glimpse to be close to input points with high probability belonging to
Gi . Combining the forward likelihood and the backward regularization together, we define Chamfer
Mixture Loss as LCD(X, X) = LF(X) ∙ LB (X). During inference, the segmentation label for each
point X is naturally obtained by arg maxi αix .
The overall loss function is L = 一 log LCD(X, X) + LKL(Zcell, zobject, zscene), where LKLiS the
KL divergence between the prior and posterior of the latent variables (appendix Sec. A for details).
In general, the normalizers in Chamfer Mixture Loss don’t have closed-form solutions. However, the
experiments below show that we can safely ignore the normalization constants during optimization.
3.4	Model Structure
We next introduce the encoder and decoder network structure for SPAIR3D. The building blocks are
based on graph neural networks and point convolution operations (See appendix Sec. C for details).
Encoder network. We design an encoder network qφ(z|x) to obtain the latent representations
{zicell}in=1 and {ziobject}in=1from a point cloud. To achieve the spatially invariant property, we group
one PointConv (Wu et al., 2019) layer and one PointGNN (Shi & Rajkumar, 2020) layer into pairs
for message passing and information aggregation among points and between cells.
(a)	Voxel Grid Encoding. The voxel-grid encoder takes a point cloud as input and generates for
each spatial attention voxel cell Ci two latent variables ziwhere ∈ R3 and ziapothem ∈ R3 to propose a
glimpse Gi potentially occupied by an object.
To better capture the point cloud information in Ci , we build a voxel pyramid within each cell Ci
with the bottom level corresponding to the finest voxel grid. We aggregate information hierarchically
using PointConv-PointGNN pairs from bottom to top through each level of the pyramid. For each
layer of the pyramid, we aggregate the features of all points and assign it to the point spawned at the
4
Under review as a conference paper at ICLR 2022
center of mass of the voxel cell. Then PointGNN is employed to perform message passing on the
radius graph built on all spawned points. The output of the final aggregation block produces ziwhere
and ziapothem via the re-parametrization trick (Kingma & Welling, 2014).
We obtain the offset distance of a glimpse center from its corresponding grid cell center using
∆gi = tanh(Zwhere) ∙ L, where L is the maximum offset distance. The apothems of the glimpse
in the x, y, Z direction is given by ∆gapo = T(ZaPathem)"max 一 rmin) + rmin, Where T(∙) is the
sigmoid function and [rmin, rmax] defines the range of apothem.
(b)	Glimpse Encoding. Given the predicted glimpse center offset and the apothems, we can associate
one glimpse with each spatial attention voxel cell. We adopt the same encoder structure to encode
each glimpse Gi into one point ai = (ci, fi), where ci is the glimpse center coordinate and fi is the
glimpse feature vector. We then generate Ziwhat and Zimask from ai via the re-parameterization trick.
The generation of zipres determines the glimpse rejection process and is crucial to the final decom-
position quality. Unlike previous work (Crawford & Pineau, 2019; Lin et al., 2020), SPAIR3D
generates z pres from glimpse features instead of cell features based on our observation that message
passing across glimpses provides more benefits in the glimpse-rejection process. To this end, a radius
graph is first built on the point set {(ci, fi)}in=1 to connect nearby glimpse centers, which is followed
by multiple PointGNN layers with decreasing output channels to perform local message passing.
The zipres of each glimpse is then obtained via the re-parameterization trick. Information exchange
between nearby glimpses can help avoid over-segmentation that would otherwise occur because of
the high dimensionality of point cloud data.
(c)	Global Encoding. The global encoding module adopts the same encoder as the object glimpse
encoder to encode scene glimpse G0. The learned latent representation is Z0what with z0pres = 1.
Decoder network. We now introduce the decoders used for point-cloud and mask generation.
(a)	Point Graph Decoder (PGD). Given the Ziobject of each glimpse, the decoder is used for point-
cloud reconstruction as well as segmentation-mask generation. In reconstruction, the number of
generated points has a direct effect on the magnitudes of the forward and backward terms in the
Chamfer Mixture Loss. An unbalanced number of reconstruction points can lead to under- or over-
segmentation. To balance the forward likelihood and the backward regularization, the number of
predictions for each glimpse should be approximately the same as the number of input points. We
propose a graph network based point decoder to allow setting the size of X in run time.
Similar to (Luo & Hu, 2021), we treat the point cloud reconstruction as a point diffusion process.
The input to the PGD is a set of 3D points with coordinates sampled from a zero-centered Gaussian
distribution, with the population determined by the number of points in the current glimpse. Features
of the input points are set uniformly to the latent variable Ziwhat . PGD is composed of several
PointGNN layers, each of which is preceded by a radius graph operation. The output of each
PointGNN layer is of dimension f + 3, with the first f dimensions interpreted as the updated features
and the last 3 dimensions interpreted as the updated 3D coordinates for estimated points. Since we
only focus on point coordinates prediction, we set f = 0 for the last PointGNN layer.
(b)	Mask Decoder. The Mask Decoder decodes (ci, Zimask) to the mask value, πix ∈ [0, 1], of each
point within a glimpse Gi . The decoding process follows the exact inverse pyramid structure of the
Glimpse Encoder. To be more precise, the mask decoder can access the spatial coordinates of the
intermediate aggregation points of the Glimpse Encoder as well as the point coordinates of Gi . During
decoding, PointConv is used as deconvolution operation.
Glimpse VAE and Global VAE. The complete Glimpse VAE structure is presented in Fig. 1b.
The Glimpse VAE is composed of a Glimpse Encoder, Point Graph Decoder, Mask Decoder and a
multi-layer PointGNN network. The Glimpse Encoder takes all glimpses as input and encodes each
glimpse Gi individually and in parallel into feature points (ci, fi). Via the re-parameterization trick,
Ziwhat and Zimask are then obtained from fi . From there, we use the Point Graph Decoder to decode
Ziwhat to reconstruct the input points, and we use the Mask Decoder to decode Zimask to assign a
mask value for each input point within Gi . Finally, Zipres is generated using message passing among
neighbour glimpses. The processing of all glimpses happens in parallel. The Global VAE consisting
of the Global Encoder and a PGD outputs the reconstructed scene layout.
5
Under review as a conference paper at ICLR 2022
3.5	Soft B oundary
The prior of zapothem is set to encourage apothem to shrink so that the size of the glimpses will not
be overly large. However, if points are excluded from one glimpse, the gradient from the likelihood
of the excluded points will not influence the size and location of the glimpse anymore, and this can
lead to over-segmentation. To solve this problem, we introduce a soft boundary weight bix ∈ [0, 1]
which decreases when a point x ∈ Gi moves away from the bounding box of Gi . Taking bix into
the computation of a, We obtain an updated mixing weight αχ	Pn W_ZPresnx bx zipresπixbix . By
j=1 zj πj bj
employing such a boundary loss, the gradual exclusion of points from glimpses will be reflected in
gradients to counter over-segmentation. Details can be found in appendix Sec. B.
4	Experiments
4.1	Datasets
While many benchmark datasets are established (Johnson et al., 2017; Kabra et al., 2019) for
unsupervised object-centric learning, they don’t provide camera poses and depth images, which are
necessary for point cloud generation. Thus, we introduce two new point-cloud datasets Unity Object
Room and Unity Object Table built on the Unity platform (Juliani et al., 2020). The Unity Object
Room (UOR) dataset is built to approximate Object Room (Kabra et al., 2019) dataset but with
increased scope and complexity. In each scene, objects sampled from a list of 8 regular geometries
are randomly placed on a square floor. The Unity Object Table (UOT) dataset approximates Robotic
Object Grasping scenario where multiple objects are placed on a round table. Instead of using objects
of simple geometries, we create each scene of the UOT dataset with more challenging objects selected
from a pool of 9 irregular objects. For both datasets, the number of objects placed in each scene
varies from 2 to 5 with equal probabilities. During the scene generation, the size and orientation of
the objects are varied randomly within a pre-defined range.
We capture the depth, RGB, normal frames, and pixel-wise semantics as well as instance labels for
each scene from 10 different viewpoints. This setup aims to approximate the scenario where a robot
equipped with depth and RGB sensors navigates around target objects and captures data. The point
cloud data for each scene is then constructed by merging these 10 depth maps. For each dataset,
we collect 50K training scenes, 10K validation scenes and 5K testing scenes. In-depth dataset
specification and analysis can be found in supplementary D.
4.2	Unsupervised S egmentation
Baseline. Due to the sparse unsupervised 3D point cloud object-centric learning literature, we could
not find a generative baseline to compare with. Thus, we compare SPAIR3D with PointGroup (PG)
(Jiang et al., 2020), a recent supervised 3D point cloud segmentation model. PointGroup performs
semantic prediction and instance predictions from a point cloud and RGB data using a model trained
with ground-truth semantic labels and instance labels. To ensure a fair comparison, we assign each
point the same color (white) so appearance information doesn’t play a role. The PointGroup network
is fine-tuned on the validation set to achieve the best performance.
Performance Metric. We use the Adjust Rand Index (ARI) (Hubert & Arabie, 1985) to measure
the segmentation performance against the ground truth instance labels. We also employ foreground
Segmentation Covering (SC) and foreground unweighted mean Segmentation Covering (mSC)
(Engelcke et al., 2020) for performance measurements as ARI does not penalize models for object
over-segmentation (Engelcke et al., 2020).
Evaluation. Table 1 shows that SPAIR3D achieves comparable performance to the supervised
baseline on both UOT and UOR datasets. As demonstrated in Fig. 2, each foreground object is
proposed by one and only one glimpse. The scene layout is separated from objects and accurately
modelled by the global VAE. It is worth noting that the segmentation errors mainly happen at the
bottom of objects. Without appearance information, points at the bottom of objects are also correlated
to the ground. In Fig. 4, we sort the test data based on their performance in an ascending order and
plot the performance distributions. As expected, the supervised baseline (Orange) performs better
but SPAIR3D manages to achieve high-quality segmentation (SC score > 0.8) on around 80% of
the scenes without supervision. See appendix Sec. E for more segmentation results including failure
cases.
6
Under review as a conference paper at ICLR 2022
(a) UOR input
(d) our reconstruction
(e) UOR instance label
(b) our reconstruction
(c) UOT input
(h) our segmentation
(f) our segmentation
(i) UOR close-up glimpses visualization, fore-
ground alpha and scene layout reconstruction.
(g) UOT instance label
(j) UOT close-up glimpses visualization, fore-
ground alpha and scene layout reconstruction.
Figure 2: Visualization of segmentation results on UOR and UOT dataset.
Table 1: 3D point cloud segmentation results on UOR dataset (blue) and UOT dataset (red).
UOR	ARI↑	SC↑	mSC↑
UOT			
PG	0.976	0W7	0≡Q
	0.923	0.917	0.907
Ours	0.915 ± 0.03	0.832 ± 0.04	0.836 ± 0.04
	0.901 ± 0.02	0.835 ± 0.03	0.831 ± 0.03
voxel size 0.75l	0.932	0^53	0^50
voxel size 1.25l	0.922	0.857	0.861
6 - 12 objects	0.912 0.892	0≡6 0.843	0^42 0.834
object matrix	0.872 0.879	0W 0.877	0W 0.886
It is worth noting that UOR and UOT datasets include around 25% objects (for scenes of 2 to 5
objects) and 60% objects (for scenes of 6 to 12 objects) that are close to its neighbors with touching
or almost touching surfaces. The reported quantitative (Table 1) and qualitative results (Fig. 5(a)-(d))
show that our method achieves stable performance for those challenging scenes.
• pot
• box
car
• cup
• kettle
• knight
• teddy
tissue
• toaster
Figure 3: t-SNE visualization of zwhat on UOT
7
Under review as a conference paper at ICLR 2022
Figure 4: Test set performance distributions on UOR (first row) and UOT (second row).
(a) UOR instance label
(b) Our segmentation
(c) UOT instance label
(d) Our segmentation
(e) UOR instance label
(f) Our segmentation
(g) UOT instance label
(h) Our segmentation
Figure 5: Segmentation on scenes with 6 to 12 objects (a - d) and on object matrix (e-h)
4.3	Object Centric Representation
To show that our model learns meaningful representations, for each object type in UOT dataset
we collect the zwhat of 200 instances and visualize them with t-SNE algorithm (van der Maaten &
Hinton, 2008). Fig 3 clearly shows the zwhat of different object types cluster at different regions. Not
surprisingly, the embeddings of pot and box instances occupy the same area since they have almost
identical spatial structure. See appendix Sec. D for object specification.
4.4	Voxel Size Robustness and Scalability
In the literature (Lin et al., 2020; Crawford & Pineau, 2019), the cell voxel size, an important
hyperparameter, is chosen to match the object size in the scene. To evaluate the robustness of our
method w.r.t voxel size, we train our model on the UOR dataset with voxel size set to 0.75l and 1.25l
with l being the average size of the objects. Results in Table 1 show that our method achieves the
stable performance w.r.t the voxel size.
To demonstrate scalability, we evaluate our pre-trained model on 1000 scenes containing 6 - 12
randomly selected objects and report performance in Table 1. Due to the spatial invariance property,
SPAIR3D suffers no performance drop on 6 - 12 object scenes that were never used for training.
We also evaluated our approach on scenes termed as Object Matrix, which consists of 16 objects
placed in a matrix form. We fixed the position of all 16 objects but set their size and rotation randomly.
For each dataset, SPAIR3D is evaluated on 100 Object Matrix scenes. The results are reported in
Table 1. Note that our model is trained on scenes with 2 to 5 objects, which is less than one-third of
the number of objects in Object Matrix scenes. Fig 5(e)-(h) is illustrative of the results.
8
Under review as a conference paper at ICLR 2022
(a) With PointGNNs (b) w/o PointGNNs
Figure 6: The comparison between models with (left) and without (right) multi-layer PointGNNs. It
shows that objects are over-segmented severely without multi-layer PointGNNs.
4.5	Ablation Study of Multi-layer PointGNN
To evaluate the importance of multi-layer PointGNN in zipres generation (right branch in Fig. 1b),
we remove the multi-layer PointGNN and generate zipres directly from fi . The ablated model on
the UOR dataset achieves ARI:0.841, SC:0.610, and mSC:0.627, which is significantly worse
than the full SPAIR3D model. The performance distribution of ablated SPAIR3D (Fig 4, first row)
indicates that removing the multi-layer PointGNN has a negative influence on the entire dataset.
Fig. 6 shows that the multi-layer PointGNN is crucial to preventing over-segmentation.
4.6	Empirical Evaluation of PGD
3D objects of the same category can be modeled by a varying number of points. The generation
quality of the point cloud largely depends on the robustness of our model against the number of
points representing each object. To demonstrate that PGD can reconstruct each object with a dynamic
number of points, we train the global VAE on the ShapeNet dataset (Chang et al., 2015), where each
object is composed of roughly 2000 points, and reconstruct the object with a varying number of
points. For reference input point clouds of size N , we force PGD to reconstruct a point cloud of
size 1.5N, 1.25N, N, 0.75N, and 0.5N, respectively. As shown in Fig. 7, while with less details
compared to the input, the reconstructions capture the overall object structure in all 5 settings.
(a)	(b)	(c)	(d)	(e)
(f)
Figure 7: PGD trained on ShapeNet. (a) Input point cloud with N points. Reconstruction with (b)
1.5N, (c) 1.25N, (d) N, (e) 0.75N , and (f) 0.5N points.
5	Limitations
SPAIR3D extends the SPAIR framework to point cloud data and inherits its strengths and limitations.
Similar to SPAIR, the key to achieving high scalability in SPAIR3D is the independence assumption
among objects, which is reflected in the local attention and reconstruction mechanisms. The price we
pay for that scalability is that the local object proposal mechanism can only reliably detect objects
whose size fall within a bounded range. By design, each voxel cell can only propose one object. Thus,
it is difficult to detect multiple objects that exist in the same voxel cell. If one object is much larger
than the size of the voxel cells, no voxel cells can accurately infer complete object information from
its local perceptive field. Given these limitations are inherent to the SPAIR framework, they can really
only be resolved at a meta level. In our intended use case, an intelligent agent that employs SPAIR3D
as a subroutine can use voxel cells of vastly different sizes to segment out objects of different sizes
and use a spatial knowledge representation and reasoning formalism [Cohn & Renz (2008)] to resolve
inconsistencies and get to a full scene representation.
6	Conclusion and Future Work
We propose SPAIR3D, to the best of our knowledge, the first generative unsupervised object-
centric learning model on point cloud with applications to 3D object segmentation task. Based on
experimental results on UOR and UOT datasets, we demonstrate that SPAIR3D can generalise well
to previously unseen scenes with a large number of objects without performance degeneration. The
spatial mixture interpretation of SPAIR3D allows the integration of memory mechanism (Bornschein
et al., 2017) or iterative refinement (Greff et al., 2019). Our model may tend to over-segment objects
of a significant different scale from those in the training dataset, which is left as our future work.
9
Under review as a conference paper at ICLR 2022
7	Ethical S tatements
Similar to most data-driven approaches, the proposed method also potentially brings the risk of
learning biases. While our object-centric representation learning has shown good generalisation
ability to new scenes, the application of our method should be done with careful consideration of the
consequences from any potential underlying biases in the data-collection process.
8	Reproducibility Statement
We described in details our model structure, the choice of hyperparameter in appendix Sec. A, B, and
C. Dataset construction details are presented in appendix Sec D. We believe our framework can be
reproduced based on these information.
References
Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations
and generative models for 3d point clouds. In ICML,pp. 40-49. PMLR, 2018.
J. Bornschein, A. Mnih, Daniel Zoran, and Danilo Jimenez Rezende. Variational memory addressing
in generative models. In NIPS, 2017.
Christopher Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick,
and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. ArXiv,
abs/1901.11390, 01 2019. URL https://arxiv.org/abs/1901.11390.
Christopher P. Burgess, Irina Higgins, Arka Pal, Lolc Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in beta-vae. ArXiv, abs/1804.03599, 2018.
Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,
Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu.
ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012
[cs.GR], Stanford University — Princeton University — Toyota Technological Institute at Chicago,
2015.
B. Michael Chang, Tomer Ullman, Antonio Torralba, and B. Joshua Tenenbaum. A compositional
object-based approach to learning physical dynamics. ICLR, 2017.
Chang Chen, Fei Deng, and Sungjin Ahn. Roots: Object-centric representation and rendering of 3d
scenes, 2021.
Anthony G. Cohn and Jochen Renz. Qualitative spatial representation and reasoning. In Frank van
Harmelen, Vladimir Lifschitz, and Bruce Porter (eds.), Handbook of Knowledge Representation,
volume 3 of Foundations OfArtificial Intelligence, pp. 551-596. Elsevier, 2008.
Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional
neural networks. AAAI, 33:3412-3420, 07 2019. doi: 10.1609/aaai.v33i01.33013412.
Eric Crawford and Joelle Pineau. Exploiting spatial invariance for scalable unsupervised object
tracking. AAAI, 34:3684-3692, 04 2020. doi: 10.1609/aaai.v34i04.5777.
Carlos Diuk, Andre Cohen, and Michael L. Littman. An object-oriented representation for efficient
reinforcement learning. In ICML, ICML ’08, pp. 240-247, New York, NY, USA, 2008. Association
for Computing Machinery. ISBN 9781605582054. doi: 10.1145/1390156.1390187. URL
https://doi.org/10.1145/1390156.1390187.
Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative
scene inference and sampling with object-centric latent representations. In ICLR, 2020. URL
https://openreview.net/forum?id=BkxfaTVFwH.
10
Under review as a conference paper at ICLR 2022
S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray
Kavukcuoglu, and Geoffrey E. Hinton. Attend, infer, repeat: Fast scene understanding with
generative models. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, NIPS'16,pp. 3233-3241, Red Hook, NY, USA, 2016. Curran Associates Inc.
ISBN 9781510838819.
S. M. Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta
Garnelo, Avraham Ruderman, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P. Reichert,
Lars Buesing, Theophane Weber, Oriol Vinyals, Dan Rosenbaum, Neil Rabinowitz, Helen King,
Chloe Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu, and Demis Hassabis. Neural
scene representation and rendering. Science, 360(6394):1204-1210, 2018. ISSN 0036-8075. doi:
10.1126/science.aar6170. URL https://science.sciencemag.org/content/360/
6394/1204.
Matheus Gadelha, Rui Wang, and Subhransu Maji. Multiresolution tree networks for 3d point cloud
processing. In ECCV, pp. 103-118, 2018.
Klaus Greff, Sjoerd van Steenkiste, and Jurgen Schmidhuber. Neural expectation maximization. In
NeurIPS, NIPS’17, pp. 6694-6704, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN
9781510860964.
Klaus Greff, Raphael Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel
Zoran, LoIC Matthey, Matthew M Botvinick, and Alexander Lerchner. Multi-object representation
learning with iterative variational inference. In ICML, 2019.
Paul Henderson and Christoph H. Lampert. Unsupervised object-centric video generation and
decomposition in 3D. In NeurIPS, 2020.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew M Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In ICLR, 2017.
L. Hubert and P. Arabie. Comparing partitions. Journal of Classification, 2:193-218, 1985.
Md Amirul Islam*, Sen Jia*, and Neil D. B. Bruce. How much position information do convolutional
neural networks encode? In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=rJeB36NKvB.
Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup:
Dual-set point grouping for 3d instance segmentation. CVPR, 2020.
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.
pp. 1988-1997, 07 2017. doi: 10.1109/CVPR.2017.215.
A. Juliani, V. Berges, E. Teng, A. Cohen, J. Harper, C. Elion, C. Goy, Y. Gao, H. Henry, M. Mattar,
and D. Lange. Unity: A general platform for intelligent agents. ArXiv, abs/1809.02627, 2020.
Rishabh Kabra, Chris Burgess, Loic Matthey, Raphael Lopez Kaufman, Klaus Greff, Malcolm
Reynolds, and Alexander Lerchner. Multi-object datasets. https://github.com/deepmind/multi-
object-datasets/, 2019.
Daniel Kahneman, Anne Treisman, and Brian J Gibbs. The reviewing of object files: Object-specific
integration of information. Cognitive Psychology, 24(2):175 - 219, 1992. ISSN 0010-0285. doi:
https://doi.org/10.1016/0010-0285(92)90007-O. URL http://www.sciencedirect.com/
science/article/pii/001002859290007O.
Ken Kansky, Tom Silver, David A. Mely, Mohamed Eldawy, Miguel Lazaro-Gredilla, Xinghua
Lou, Nimrod Dorfman, Szymon Sidor, D. Scott Phoenix, and Dileep George. Schema networks:
Zero-shot transfer with a generative causal model of intuitive physics. ArXiv, abs/1706.04317,
2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
11
Under review as a conference paper at ICLR 2022
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel
Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment
for Visual AI. arXiv, 2017.
Nanbo Li, Cian Eastwood, and Robert Fisher. Learning object-centric representations of multi-
object scenes from multiple views. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5656-
5666. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/
2020/file/3d9dabe52805a1ea21864b09f3397593-Paper.pdf.
Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong
Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial
attention and decomposition. In ICLR, 2020. URL https://openreview.net/forum?
id=rkl03ySYDH.
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention.
In NeurIPS, 2020.
Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.
W. Shi and R. Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud.
In CVPR, pp. 1708-1716, 2020.
Karl Stelzner, Kristian Kersting, and Adam R. Kosiorek. Decomposing 3d scenes into objects via
unsupervised volume segmentation, 2021.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp.
368-377, 1999. URL https://arxiv.org/abs/physics/0004057.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Ma-
chine Learning Research, 9:2579-2605, 2008. URL http://www.jmlr.org/papers/v9/
vandermaaten08a.html.
Nicholas Watters, Loic Matthey, Christopher Burgess, and Alexander Lerchner. Spatial broad-
cast decoder: A simple architecture for learning disentangled representations in vaes. ArXiv,
abs/1901.07017, 08 2019. URL https://arxiv.org/abs/1901.07017.
Wenxuan Wu, Zhongang Qi, and Fuxin Li. Pointconv: Deep convolutional networks on 3d point
clouds. In CVPR, pp. 9613-9622, 06 2019. doi: 10.1109/CVPR.2019.00985.
Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan.
Pointflow: 3d point cloud generation with continuous normalizing flows. pp. 4540-4549, 10 2019.
doi: 10.1109/ICCV.2019.00464.
12
Under review as a conference paper at ICLR 2022
A Hyperparameters and Prior Distributions
In this section, we present hyperparameters we used in our model as well as definitions of the
prior distributions of latent variables. We use tuples in the form of (n, m, p, q) to denote annealing
of hyperparameter values from n to m, starting from iteration p to iteration q. Glimpse related
hyperparameters are shown in Table 2. Priors are specified in Table 3. Other hyperparameters are
specified in Table 4.
Table 2: Glimpse related hyperparameters.
Term	Value
spatial attention grid cell size	1
glimpse max apothem	1
glimpse min apothem	0.25
glimpse max center offset	0.75
Table 3: Prior distributions.
Term	Value
apothem μ	(2,-1,10000, 20000)
βpres	(0.01, 0.0001, 0, 15000)
zwhere prior	N (0, 0.5)
z apothem prior	N(μapothem, 0.5)
zpres prior	Bernoulli(βpres)
zwhat prior	N(0,1)
zmask prior	N(0,1)
Table 4: Other hyperparameters.
Term	Value
CML σc	(0.1, 0.05,10000,15000)
CML dist.	N (0, σc)
relaxed Bernoulli temp	(2.5, 0.5, 0, 10000)
PGD initial distribution	N (0, 0.3)
Mixing Weight. Recall that the mixing weight defined in Eq. (6) in our main paper for the forward
Chamfer Likelihood implies the segmentation of the point cloud. To encourage the mixing weight
to approach 0 or 1, we include a temperature factor 10 into the computation of αix , and the mixing
weight is implemented as
(zpresπx)10
X —	Izi	πi )___ Pres x
ai = Pn—(zpres∏x)10 zi	πi .
(1)
Weight for Regularisation with Priors. Here we introduce the detailed formulation of LKL, the
second term of the evidence lower bound, in the following Eq. 2.
LKL(zcell, zobject, zscene) = wDKL(p(zpres)||q(zpres|x)) + zpres[
DKL(p(zwhat)||q(zwhat|x))+
DKL(p(zmask)||q(zmask|x))+	(2)
DKL(p(zwhere)||q(zwhere|x))+
DKL(p(zapothem)||q(zapothem|x))].
Note that the weight w for KL Divergence of zpres is to encourage glimpse rejection. More specifi-
cally, w is set to 10 for UOR dataset and is annealed from 10 to 20 in the first 15000 steps for UOT
dataset in our experiments. Following Crawford & Pineau (2019), we also set the weight for KL
divergence of the rest latent variables with zpres so that the rejected glimpses won’t produce penalties
encouraging glimpse rejection.
13
Under review as a conference paper at ICLR 2022
Training. We use Adam (Kingma & Ba, 2014) optimizer with learning rate set to 0.0001 during our
training process. Training takes roughly 4 days on one RTX 3090 for both datasets.
B	Soft B oundary
(a) Boundary weight function.
(b) Gradient.
(c) Illustration of glimpse
boundary structure.
Figure 8: Visualization of glimpse boundary structure, glimpse boundary weights and the correspond-
ing gradients. Fig. 8c illustrates the glimpse structure where c is the glimpse center and x is an point
living in the glimpse boundary. The linear decay function (orange) and parabola decay function (blue)
are plotted in Fig. 8a with the corresponding gradients shown in Fig. 8b.
bix =
As shown in Fig 8c, we divide a glimpse into glimpse interior of apothem r and glimpse boundary of
width w. For an input point x in glimpse Gi with center location ci, its boundary weight bix ∈ (0, 1]
is defined as
1, x ∈ glimpse interior;
f (d, w), x ∈ glimpse boundary,
where f is a continuous function and d = kx - ci - rkinf .
One example of f is a linear decay function f (d,w) = 1 - Wd. Another example is a parabola decay
function f (d, w) = (Wd)2 — 2( Wd) + 1, which generates a larger gradient compared with the linear one
when d ≤ W, and a smaller gradient, otherwise. For the case where the distance between two objects
is smaller than W but larger than W, parabola decay function is preferred to preventing overly large
glimpse. In this work, we use parabola decay function and set the width of the glimpse boundary as
w = 0.75r.
From Fig. 8a and Fig. 8b, we observe that both types of boundary decay function f provide a negative
gradient when points are being excluded from a glimpse which has increasing Wd value.
C Model Structure
In this section, we introduce the detailed model structure. We first introduce the notation that we used
in our network structure. One PointConv layer is specified with (cmid, cout) (Wu et al., 2019) (See
Fig. 9 for structure illustration). To achieve spatially invariant operation in PointConv, we divide
the feature value of each point by the total number of points involved for each point convolution
operation instead of the pre-computed point density as in (Wu et al., 2019).
One PointGNN layer contains three two-layer MLPs which are MLPh, MLPf and MLPg, respec-
tively (Shi & Rajkumar, 2020) and the structure can be summarized by
siout =g(jm∈aNxi{f(xj - xi + h(siin), siin)}, siin),
(3)
where siin and siout are the features of the point i before and after PointGNN layer, Ni defines the
points connected to i, xi indicates the 3D coordinate of the point i. The max operation is performed
over all points j that are connected to the point i. For all PointGNN layers, we consistently set
hhidden = 32 and hout = 3. Thus, we define the structure of a PointGNN layer with parameters
as (fhidden, fout, ghidden, gout). We also list the parameters for other operations such as the radius
14
Under review as a conference paper at ICLR 2022
for radius graph operation, the voxel cell size for voxel pooling operation, and the output size for
linear layers in following tables. Table 5 shows the structure of voxel grid encoder. The structure of
glimpse VAE including the glimpse encoder, the mask decoder, the glimpse Point Graph Flow, and
the Multi-layer PointGNN is presented in Table 6, Table 9, Table 8 and Table 11. The structure of
global VAE including the Global encoder and the Global Point Graph Flow is detailed in Table 10
and Table 7. Input point coordinates are reduced by a factor of 16.
Table 5: Voxel grid encoder.	Table 6: Glimpse encoder.
Layer/OPeration	Parameter	Layer/OPeration	Parameter
Radius GraPh	0.0625	Radius GraPh	0.25
PointConv	(8, 8)	PointGNN	(8,8,8,8)
Celu		LayerNorm	
PointConv	(16, 16)	Voxel Pool	0.25
Celu		PointConv	(16, 32)
PointConv	(32, 32)	Celu	
Celu		Radius GraPh	0.5
Voxel Pool	0.03125	PointGNN	(32, 32, 32, 32)
PointConv	(32, 64)	LayerNorm	
Celu		Voxel Pool	0.25
Radius GraPh	0.03125	PointConv	(64, 128)
PointGNN	(64, 64, 64, 64)	Celu	
LayerNorm		Radius GraPh	1.0
Voxel Pool	0.0625	PointGNN	(128, 128, 128, 128)
PointConv	(64, 128)	LayerNorm	
Celu		PointConv	(128, 256)
Radius GraPh	0.125	Celu	
PointGNN	(128, 128, 128, 128)	Linear	256
LayerNorm Voxel Pool	0.125		
PointConv Celu	(128, 256)		
Radius GraPh	0.25		
PointGNN PointGNN	(256, 256, 256, 256) (256, 256, 256, 256)	Table 7: Global Point GraPh Flow.	
PointGNN LayerNorm PointConv	(256, 256, 256, 256)	Layer/OPeration	Parameter
	(256, 256) 12	Random SamPling Radius GraPh	0.2
Linear			
		PointGNN	(128, 128, 128, 64 + 3)
		Radius GraPh	0.1
		PointGNN	(64, 64, 64, 32 + 3)
		Radius GraPh	0.05
		PointGNN	(16, 16, 16, 3)
Table 8: Glimpse Point Graph Flow.
Layer/OPeration	Parameter
Random Sampling
Radius GraPh
PointGNN
Radius GraPh
PointGNN
Radius GraPh
PointGNN
0.2
(128, 128, 128, 64 + 3)
0.1
(64, 64, 64, 32 + 3)
0.05
(16, 16, 16, 3)
15
Under review as a conference paper at ICLR 2022
Table 9: Mask decoder.
Layer/Operation Parameter
PointConv	(64, 32)
Celu PointConv	(16, 16)
Celu PointConv	(8, 8)
Celu Linear	1
Figure 9: Structure of PointConv.
Table 10: Global encoder.
Layer/OPeration Parameter
Radius GraPh	^^0.25
PointGNN	(8,8,8,8)
LayerNorm Voxel Pool	0.25
PointConv	(16, 32)
Celu Radius GraPh	0.5
PointGNN	(32, 32, 32, 32)
LayerNorm Voxel Pool	0.25
PointConv	(64, 128)
Celu Radius GraPh	1.0
PointGNN	(128, 128, 128, 128)
LayerNorm PointConv	(128, 256)
Celu PointConv	(256, 512)
Table 11:	Multi-layer PointGNN.
Layer/OPeration	Parameter
Random Sampling
Radius GraPh	1.0
PointGNN	(128, 64, 64,	64)
PointGNN	(32, 32, 32, 32)
PointGNN	(16, 16, 16, 8)
Linear	1
16
Under review as a conference paper at ICLR 2022
D	Dataset Spec
We provide more details about the UOR and UOT dataset in this section. For both datasets, in each
scene, 2-5 objects are uniformly randomly selected (with replacement) from the candidate set and
placed at random locations in the scene with the constraint that they cannot largely overlap with each
other (slight overlapping and touching are permitted). All objects are randomly rotated along y-axis.
For the point cloud construction, we convert the 10 depth frames for each scene into 10 partial point
clouds. To down sample the point clouds, we apply voxel grid pooling on the 10 partial point clouds.
More specifically, a voxel grid with cell size 0.15 is placed across the space where each partial point
cloud exists. For all points in one cell, the coordinates are aggregated by average pooling operation.
After we merge the 10 partial point clouds into one, we perform voxel grid pooling again to get the
final input point cloud. We divide the coordinates of all points by 8 to scale the complete point clouds.
The intrinsic parameters of cameras are shown in Table 12, which is crucial to accurate point cloud
construction. The object pool for UOR and UOT are presented in Table 13 and Table 14, respectively,
where we also specify the detailed dimension range of each object.
Table 12:	Camera intrinsic parameters
Term	Value
focal length	10	mm
sensor size x	16	mm
sensor size y	16	mm
clipping plane	20	m
Figure 10: Data captured by each camera in UOR dataset (top) and UOT dataset (bottom). From left
to right are RGB, depth, normal, instance label, semantic label and constructed point cloud. Point
clouds are obtained by merging multi-view depth images. Instance labels and semantic labels are
used to train PointGroup (Jiang et al., 2020) baseline. RGB images and normal maps are not used in
this work.
To show that our dataset is indeed challenging for models capturing spatial structure correlations, we
plot the empirical closest neighbor distance distribution of our data generation process. To obtain
the distribution, for each object in each scene, we note down the distance between the center of this
object and the center of its closest neighbor (the surface to surface distance is hard to compute). Thus,
distance zero means a complete overlapping between two objects, which is not physically plausible.
In our UOR and UOT dataset the minimum distance is set to one (not applicable to Object Matrix
layout). With object dimensions specified in Table 13 and Table 14, we consider distance below 2
to be extremely close. Thus, reading from Fig.11, for scenes containing 2-5 objects, 25 percent of
objects are spawned close to at least one other object. For scenes containing 6-12 objects, the number
goes up to more than 60 percent.
17
Under review as a conference paper at ICLR 2022

1.0-
0.8-
0.6-
0.4-
0.2-
0.0	I	I
2	4	6	8
Distance
10

Figure 11: Closest neighbor distance distribution of our data generation process for 2-5 objects
(above), and for 6-12 objects (below)
18
Under review as a conference paper at ICLR 2022
Table 13: UOR object pool.
geometry	figure			min length/max length	min width/max width	min height/max height
Capsule (standing)		I		0.75/1.25	0.75/1.25	1.5/2.5
						
Capsule (flat)		ɪ		1.5/2.5	1.5/2.5	0.75/1.25
Cube				0.75/1.25	0.75/1.25	0.75/1.25
Cylinder (standing)		I		0.75/1.25	0.75/1.25	1.5/2.5
						
Cylinder (flat)		I		1.5/2.5	1.5/2.5	0.75/1.25
Hexagonal Prism		■		1/1.33	1/1.33	0.5/0.83
Sphere		.		0.75/1.25	0.75/1.25	0.75/1.25
Rhombicosidodecahedron				0.75/1.25	0.75/1.25	0.75/1.25
Square Antiprism				1/1.5	1/1.5	0.67/1
Triangular Prism (standing)				0.75/1.5	0.75/1.5	0.75/1.5
Triangular Prism (flat)				0.75/1.5	0.75/1.5	0.75/1.5
19
Under review as a conference paper at ICLR 2022
Table 14: UOT Object pool. Some object meshes are obtained from ai2thor (Kolve et al., 2017)
environment.
object	figure	min length/max length	min width/max width	min height/max height
Chess piece		0.69/0.86	0.69/0.86	1.43/1.80
Bear	■	1.1/1.46	0.71/0.95	1.06/1.41
Box	□	0.98/1.23	0.98/1.23	0.67/0.84
Car		1.17/1.96	0.74/1.24	0.70/0.78
Cup	α	0.87/1.10	0.70/0.87	0.92/1.16
Kettle		0.85/1.13	0.69/0.91	1.01/1.35
Pot	H	1.06/1.42	0.88/1.17	0.80/1.07
Tissue		1.05/1.31	0.95/1.18	1.16/1.44
Toaster		0.58/0.78	1.01/1.36	0.61/0.81
20
Under review as a conference paper at ICLR 2022
E More results
In this section, we show more segmentation results of our model. To demonstrate that our model can
handle non-trivial scene layout, we apply SPAIR3D on classic Object Room dataset (Eslami et al.,
2018) and show quantitative results in Fig. 12.
(a)
(b)
(c)
(d)
Figure 12: Scene layout in Object Room dataset has four walls. SPAIR3D groups four walls together
with the floor in each scene as the scene layout component. Column (a) and column (c) are instance
labels. Column (b) and column (d) are the corresponding SPAIR3D segmentation.
We show more test cases where SPAIR3D achieves above 0.8 SC scores on UOR in Fig. 13 and UOT
in Fig. 14. To provide a more comprehensive inspection, we further display segmentation results
on scenes where SPAIR3D achieves below 0.7 SC scores on UOR in Fig. 15, and UOT in Fig. 16,
respectively. Segmentation results demonstrated below, when examined together with performance
distribution shown in Fig. 4, confirm that our model achieves high quality segmentation on majority
scenes.
More segmentation results on scenes with 6 - 12 objects are shown in Fig. 17 and Fig. 18. In Fig. 19,
we show more examples on Object Matrix scenes to further demonstrate the scalability of our model.
21
Under review as a conference paper at ICLR 2022
(b)
(d)
(a)
(c)
Figure 13: Typical UOR test cases that achieves above 0.8 SC. Column (a) and column (c) are
instance labels. Column (b) and column (d) are the corresponding SPAIR3D segmentation.
22
Under review as a conference paper at ICLR 2022
(b)
(d)
(a)
(c)
Figure 14: Typical UOT test cases that achieve above 0.8 SC. Column (a) and column (c) are instance
labels. Column (b) and column (d) are the corresponding SPAIR3D segmentation results.
23
Under review as a conference paper at ICLR 2022
(b)
(d)
(a)
(c)
Figure 15: UOR test cases that achieves below 0.7 SC. Column (a) and column (c) are instance labels.
Column (b) and column (d) are the corresponding SPAIR3D segmentation. Failed cases reflect that
(1) objects clustered together are more vulnerable to mis-segmentation due to complicated local
spatial structure layout (2) and objects with extreme dimensions, e.g. cylinder, are vulnerable to
over-segmentations. Note that currently all derivatives of SPAIR (Crawford & Pineau, 2019) are
sensitive to object size.
(a)	(b)
(c)	(d)
Figure 16: UOT test cases that achieve below 0.7 SC. Column (a) and column (c) are instance labels.
Column (b) and column (d) are the corresponding SPAIR3D segmentation results.
24
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)
(d)
Figure 17: More results on UOR scenes with 6-12 objects. Column (a) and column (c) are instance
labels. Column (b) and column (d) are the corresponding SPAIR3D segmentation.
(a)
(b)
(c)
(d)
Figure 18: More results on UOT scenes with 6-12 objects. Column (a) and column (c) are instance
labels. Column (b) and column (d) are the corresponding SPAIR3D segmentation.
25
Under review as a conference paper at ICLR 2022
(a)
(b)
(d)
Figure 19: More results on UOR and UOT Object Matrix scenes. Column (a) and column (c) are
instance labels. Column (b) and column (d) are the corresponding SPAIR3D segmentation results.
26