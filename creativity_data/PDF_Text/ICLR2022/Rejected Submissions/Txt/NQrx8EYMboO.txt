Under review as a conference paper at ICLR 2022
Task-Agnostic Graph Explanations
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks (GNNs) have emerged as powerful tools to encode graph
structured data. Due to their broad applications, there is an increasing need to
develop tools to explain how GNNs make decisions given graph structured data.
Existing learning-based GNN explanation approaches are task-specific in training
and hence suffer from crucial drawbacks. Specifically, they are incapable of pro-
ducing explanations for a multitask prediction model with a single explainer. They
are also unable to provide explanations in cases where the GNN is trained in a
self-supervised manner, and the resulting representations are used in future down-
stream tasks. To address these limitations, We propose a Task-Agnostic GNN
Explainer (Tage) trained under self-supervision with no knowledge of down-
stream tasks. Tage enables the explanation of GNN embedding models With-
out downstream tasks and allows efficient explanation of multitask models. Our
extensive experiments show that Tage can significantly speed up the explanation
efficiency by using the same model to explain predictions for multiple downstream
tasks while achieving explanation quality as good as or even better than current
state-of-the-art GNN explanation approaches.
1 Introduction
Graph neural networks (GNNs) (KiPf & Welling, 2017; Velickovic et al., 2018; XUet al., 2019) have
achieved remarkable success in learning from real-world graph-structured data due to their unique
ability to capture both feature-wise and topological information. Extending their success, GNNs
are widely applied in various research fields and industrial applications including quantum chem-
istry (Gilmer et al., 2017), drug discovery (Wu et al., 2018; Wang et al., 2020), social networks (Fan
et al., 2019), and recommender systems (Ying et al., 2018). While multiple approaches have been
proposed and studied to improve GNN performance, GNN explainability is an emerging area and
has a smaller body of research behind it. Recently, explainability has gained more attention due to
an increasing desire for GNNs with more security, fairness, and reliability. Being able to provide a
good explanation to a GNN prediction increases model reliability and reduces the risk of incorrect
predictions, which is crucial in fields such as molecular biology, chemistry, fraud detection, etc.
Existing methods adapting the explanation methods for convolutional neural networks (CNNs) or
specifically designed for GNNs have shown promising explanations on multiple types of graph data.
A recent survey (Yuan et al., 2020) categorizes existing explanation methods into gradient-based,
perturbation, decomposition, and surrogate methods. In particular, perturbation methods involve
learning or optimization (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Lin et al., 2021) and,
while bearing higher computational costs, generally achieve state-of-the-art performance in terms
of explanation quality. These methods train post-hoc explanation models on top of the prediction
model to be explained. Earlier approaches like GNNExplainer (Ying et al., 2019) require training or
optimizing an individual explainer for each data instance, i.e., a graph or a node to be explained. In
contrast, PGExplainer (Luo et al., 2020) performs inductive learning, i.e., it only requires a one-time
training, and the explainer can be generalized to explain all data instances without individual op-
timization. Compared to other optimization-based explanation methods, PGExplainer significantly
improves the efficiency in terms of time cost without performance loss by learning.
However, even state-of-the-art explanation methods like PGExplainer are still task-specific at train-
ing and hence suffer from two crucial drawbacks. First, current methods are inefficient in explaining
multitask prediction for graph-structured data. For example, one may need to predict multiple chem-
ical properties in drug discovery for a molecular graph. In particular, ToxCast from MoleculeNet has
1
Under review as a conference paper at ICLR 2022
End-to-end task-specific GNN explainers
Need train different explainers to explain a multitask
prediction model. Unable to train without downstream.
Two-stage task-agnostic GNN explanations
Figure 1: A comparison between typical end-to-end task-specific GNN explainers and the proposed
task-agnostic explanation pipeline. To explain a multitask model, typical explanation pipelines need
to optimize multiple explainers, whereas the two-stage explanation pipeline only learns one embed-
ding explainer that can cooperate with multiple lightweight downstream explainers.
167 prediction tasks. In these cases, it is common to apply a single GNN model with multiple output
dimensions to make predictions for all tasks. However, one is unable to employ a single explainer to
explain the above model, since current explainers are trained specifically to explain one prediction
task. As a result, in the case of ToxCast, one must train 167 explainers to explain the GNN model.
Second, in industry settings, it is common to train GNN models in a two-stage fashion due to scal-
ing, latency, and label sparsity issues. The first stage trains a GNN-based embedding model with
a massive amount of unlabeled data in an unsupervised manner to learn embeddings for nodes or
graphs. The second stage trains lightweight models such as multilayer perceptrons (MLPs) using the
frozen embeddings as input to predict the downstream tasks. In the first stage, the downstream tasks
are usually unknown or undefined, and existing task-specific explainers cannot be applied. Also,
there can be tens to hundreds of downstream tasks trained on these GNN embeddings, and training
a separate explainer for each task is undesirable and downright impossible.
To address the above limitations, we present a new task-agnostic explanation pipeline, shown in
Figure 1, where we decompose a prediction model into a GNN embedding model and a downstream
model, designing separate explainers for each component. We design the downstream explainers to
cooperate with the embedding explainer. The embedding explainer is trained using a self-supervised
training framework, which we dub Task-Agnostic GNN Explainer (Tage), with no knowledge of
downstream tasks, models, or labels. In contrast to existing explainers, the learning objective for
Tage is computed at the graph or node embeddings without involving task-related predictions. In
addition to eliminating the need for downstream tasks in Tage, we argue that the self-supervision
performed on the embeddings can bring additional performance boost in terms of the explanation
quality compared to existing task-specific baselines such as GNNExplainer and PGExplainer.
We summarize our contributions as follows: 1) We introduce the task-agnostic explanation problem
and propose a two-stage explanation pipeline involving an embedding explainer and a downstream
explainer. This enables the explanation of multiple downstream tasks with a single embedding ex-
plainer. 2) We propose a self-supervised training framework Tage, which is based on conditioned
contrastive learning to train the embedding explainer. Tage requires no knowledge of downstream
tasks. 3) We perform experiments on real-world datasets and observe that Tage outperforms ex-
isting learning-based explanation baselines in terms of explanation quality, universal explanation
ability, and the time required for training and inference.
2	Task-Agnostic Explanations
2.1	Notations and Learning-based GNN explanation
Our study considers the attributed graph G with node set V and edge set E. We formulate the
attributed graph as a tuple of matrices (A, X), where A ∈ {0,1}|Vl×lV| denotes the adjacency
matrix and X ∈ R1Vl×df denotes the feature matrix with feature dimension of df. We assume
that the prediction model F that is to be explained operates on graph-structured data through two
components: a GNN-based embedding model and lighter downstream models. Denoting the input
space by G, a node-level embedding model En : G → RlVl×d takes a graph as input and computes
2
Under review as a conference paper at ICLR 2022
Table 1: Comparisons on properties of common GNN explainers. Inductivity and task-agnosticism
are inapplicable for gradient/rule-based methods as they do not require learning. In the last column,
We show the number of required explainers for a dataset with N samples and M tasks.
Learning		Inductive	Task-agnostic	# explainers required
Gradient- & Rule-based	No	-	-	1
GNNExplainer (Ying et al., 2019)	Yes	No	No	M * N
SubgraphX (Yuan et al., 2021)	Yes	No	No	M * N
PGExplainer (Luo et al., 2020)	Yes	Yes	No	M
Task-agnostic explainers	Yes	Yes	Yes	1
embeddings of dimension d for all nodes in the graph, whereas a graph-level embedding model
Eg : G → R1×d computes an embedding for the input graph. Subsequently, the downstream model
D : Rd → R computes predictions for the downstream task based on the embeddings.
Typical GNN explainers consider a task-specific GNN-based model as a complete unit, i.e., F :=
D ◦ E. Given a graph G and the GNN-based model F to be explained , our goal is to identify
the subgraph Gsub that contributes the most to the final prediction made by F . In other words,
we claim that a given prediction is made because F captures crucial information provided by some
subgraph Gsub. The learning-based (or optimization-based) GNN explanation employs a parametric
explainer Tθ associated with the GNN model F to compute the subgraph Gsub of the given graph
data. Concretely, the explainer Tθ computes the importance score for each node or edge, denoted as
wi or wij, or masks for node attributes denoted as m. It then selects the subgraph Gsub induced by
important nodes and edges, i.e., whose scores exceed a threshold t, and by masking the unimportant
attributes. In our study, we follow Luo et al. (2020), focusing on the importance of edges to provide
explanations to GNNs. Formally, we have Gsub := (V, Esub) = Tθ(G), where Esub = {(vi, vj) :
(vi , vj ) ∈ E, wij ≥ t}.
2.2	Task-Agnostic Explanations
As introduced in Section 1, all existing learning-based or optimization-based explanation approaches
are task-specific and hence suffer from infeasiblity or inefficiency in many real-application scenar-
ios. In particular, they are of limited use when downstream tasks are unknown or undefined and fail
to employ a single explainer to explain a multitask prediction model.
To enable the explanation of GNNs in two-stage training and multitask scenarios, we introduce a new
explanation paradigm called the task-agnostic explanation. The task-agnostic explanation considers
a whole prediction model as an embedding model followed by any number of downstream models. It
focuses on explaining the embedding model regardless of the number or the existence of downstream
models. In particular, the task-agnostic explanation trains only one explainer Tθ(tag) to explain the
embedding model E, which should satisfy the following features. First, given an input graph G, the
explainer Tθ(tag) should be able to provide different explanations according to specific downstream
tasks being studied. Table 1 compares the properties of common GNN explanation methods and the
desired task-agnostic explainers in multitask scenarios. Second, the explainer Tθ(tag) can be trained
when only the embedding model is available, e.g., at the first stage ofa two-stage training paradigm,
regardless of the presence of downstream tasks. When downstream tasks and models are unknown,
Tθ(tag) can still identify which components of the input graph are important for certain embedding
dimensions of interest.
3	The Tage Framework
Our explanation framework Tage follows the typical scheme of GNN explanation introduced in
the previous section. It provides explanations by identifying important edges in a given graph and
removing the edges that lead to significant changes in the final prediction. Specifically, the goal of
the Tage is to predict the importance score for each edge in a given graph. Different from existing
methods, the proposed Tage breaks down typical end-to-end GNN explainers into two components.
We now provide general descriptions and detailed formulations to the proposed framework.
3
Under review as a conference paper at ICLR 2022
3.1	Task-Agnostic Explanation Pipeline
Following the principle of the desired task-agnostic explanations, we introduce the task-agnostic
explanation pipeline, where a typical explanation procedure is performed in two steps. In particular,
we decompose the typical end-to-end learning-based GNN explainer into two parts: the embedding
explainer TE and the downstream explainer Tdown, corresponding to the two components in the
two-stage training and prediction procedure. We compare the typical explanation pipeline and the
two-stage explanation pipeline in Figure 1. The embedding explainer and downstream explainers
can be trained or constructed independently from each other. In addition, the embedding explainer
can cooperate with any downstream explainers to perform end-to-end explanations on input graphs.
The downstream explainer aims to explain task-specific downstream models. As downstream mod-
els are usually lightweight MLPs, we simply adopt gradient-based explainers for downstream ex-
plainers without training. The downstream explainer takes a downstream model and the graph or
node embedding vector as inputs and computes the importance score of each dimension on the em-
bedding vector. The importance scores then serve as a condition vector input to the embedding
explainer. Given the condition vector, the embedding explainer explains the GNN-based embed-
ding model by identifying an important subgraph from the input graph data. In other words, given
different condition vectors associated with different downstream tasks or models, the embedding
explainer can provide corresponding explanations for the same embedding model. Formally, we
denote the downstream explainer for models from D by Tdown : D × Rd → Rd, which maps in-
put models and embeddings into importance scores m for all embedding dimensions. We denote
the embedding explainer associated with the embedding model E by TE : Rd × G → G, which
maps a given graph into a subgraph of higher importance, conditioned on the embedding dimension
importance m ∈ Rd.
The training procedures of the embedding explainer are independent of downstream tasks or down-
stream explainers. In particular, the downstream explainer is obtained from the downstream model
only, and the training of the embedding explainer only requires the embedding model and the in-
put graphs. As downstream models are usually constructed as stacked fully connected (FC) layers
and the explanation of FC layers has been well studied, our study mainly focuses on the non-trivial
training procedure and design of the embedding explainer.
3.2	Training embedding explainer under Self-supervision
A straightforward idea of explaining an embedding model with no knowledge of downstream tasks
is to employ existing explainers and perform explanation on the pretext task, such as graph recon-
struction (Kipf & Welling, 2016) or context prediction (Hu et al., 2020), used during the pre-training
of GNNs. However, such explanations cannot generalize to future downstream tasks as there are lim-
ited dependencies between the pretext task and downstream tasks. Therefore, training an embedding
explainer without downstream models or labels is challenging, and it is desirable to develop a gen-
eralizable training approach for the embedding explainer. To this end, we propose a self-supervised
learning framework for the embedding explainer.
The learning objective of the proposed framework seeks to maximize the mutual information (MI)
between two embeddings with certain dimensions masked, i.e., one of the given graphs and one of
the corresponding subgraph of high importance induced by the explainer. We introduce a masking
vector p ∈ Rd to indicate specific dimensions of embeddings on which to maximize MI. During
explanation, we obtain the masking vector from the importance vector computed by any downstream
explainer Tdown . As no downstream importance vector is available at training, we sample the mask-
ing vector p from a multivariate Laplace distribution due to the sparse gradient assumption, i.e., only
a few dimensions are of high importance. Formally, the MI-based learning objective is
maxEpMI(P X E(G),P X E(Tθ(p, G)))],	(1)
θ
where MI(∙, ∙) computes the mutual information between two random vectors, P denotes the random
masking vector sampled from a certain distribution, Tθ(P, G) computes the subgraph of high impor-
tance, and X denotes the element-wise multiplication, which applies masking to the embeddings
E(∙). Figure 2 outlines the training framework and objective. Intuitively, given an input graph and
the desired embedding dimensions to be explained, the explainer Tθ predicts the subgraph whose
4
Under review as a conference paper at ICLR 2022
Figure 2: Overviews of the self-supervised training framework for the embedding model (right) and
the architecture of the parametric explainers (left). During training, we generate random condition
vectors p as an input to the embedding explainer and mask the embeddings. The learning objective
seeks to maximize the mutual information between two embeddings on certain dimensions.
embedding shares the maximum mutual information with the original embedding on the desired
dimensions.
Practically, the mutual information is intractable and is hence hard to directly compute. A common
approach to achieve efficient computation and optimization is to adopt the upper bound estimations
of mutual information, namely, the Jenson-Shannon Estimator (JSE) (Nowozin et al., 2016) and
the InfoNCE (Gutmann & Hyvarinen, 2010). These upper bound estimations are also referred to as
contrastive loss and are widely applied in self-supervised representation learning (Hjelm et al., 2019;
SUn et al., 2019; VelickoVic et al., 2019) for both images and graphs. Adopting these estimators, the
objectives are efficiently computed as
1N	T	1	T
min N ElOg	[σ((P 乳	zi)	(P 乳	zi,θ))] + N2	- N ElOg	[1 - σ	((P	乳	Zi)T(P ③	zj,θ))],
i=1	i6=j
(2)
min -ɪ XX [log LeXM(PrV Zi)T(P J zi,θ)'
θ N i=1 [	∑j=i exP{(P v Zi)T(P v zj,θ)}
for JSE and InfoNCE, respectively, where N denotes the number of samples in a mini-batch, σ
denotes Sigmoid function, Zi and Zi,θ are embeddings of the original graph Gi and its subgraph
Tθ (Gi), or target nodes of the two graphs. Our objective involves condition vectors as masks on
the embeddings, which differs from typical contrastive loss used in self-supervised representation
learning. We hence call the proposed objective the conditioned contrastive loss.
To restrict the size of subgraphs given by the explainer, we additionally add a size regularization
term R, computed as the averaged importance score, to the above objectives. In the case where edge
importance scores wij ∈ [0, 1] are computed, the regularization term is computed as
R(G) =	E	λs∣Wij | - λe [wijlog Wij-(I - Wij )log(1 - Wij)] ,	(4)
(vi,vj)∈E
where λs and λe are hyper-parameters controlling the size and the entropy of edge importance
scores, respectively.
3.3	Explainer Architectures
Embedding explainers. Inspired by explainer architectures used by PGExplainer, we adopt the
multilayer perceptron (MLP) to predict the importance score Wij for each edge (ui, uj ) ∈ E, on
top of learned embeddings Zi and Zj of the two nodes connected by the edge. Edges with scores
higher than a threshold are considered as important edges that remain in the selected subgraph. In
order for the embedding explainer to cooperate with different downstream explainers and provide
diverse explanations for different tasks, it additionally requires a condition vector as input indicating
5
Under review as a conference paper at ICLR 2022
the specific downstream task to be explained. We handle the condition vector in a similar manner to
Conditional GAN (Mirza & Osindero, 2014). Formally, the graph-level embedding explainer takes
the embeddings, zi and zj , and the condition vector p as inputs and computes the importance score
by
Wij= MLPg ([Zi； Zj]乳 σ(fg(p))),	(5)
where [∙; ∙] denotes the concatenation along the feature dimension, 0 denotes the element-wise mul-
tiplication, σ denotes the activation function, and fg : Rd → R2d is a linear projection. The
node-level embedding explainer takes an additional node embedding as its input, as the explainers
are expected to predict different scores for the same edge when explaining different target nodes.
The formulation of computing the importance score is as follows,
Wij = MLPn (IZi； Zj ； ztarget] 0 σ(fn (P))),
(6)
where fg : Rd → R3d is a linear projection, and ztarget denotes the embedding of the target node
whose prediction is to be explained.
Downstream explainers. We adopt the gradient-based explainer to explain the downstream models.
Formally, given an input embedding Z and its prediction probabilities D(Z) ∈ [0, 1]C among all C
classes, we compute the gradient of the maximal probability w.r.t. the input embedding:
g
∂ maxc≤C D(Z)[c]
∂Z
∈ R1×d,
where D(Z)[c] denotes the probability for class c. To convert the gradient into the condition vector,
we further perform normalization and only take positive values reflecting only positive influence to
the predicted class probability, i.e., p = ReLU(norm(gT )).
4	Experimental Studies
We conduct two groups of quantitative studies evaluating the explanation quality and the universal
explanation ability, i.e., training a single explainer to explain all downstream tasks, of TAGE. We
then compare the efficiency of multiple learning-based GNN explainers in terms of training and
explanation time cost. We further provide visualizations to demonstrate the explanation quality as
well as the ability to explain GNN models without downstream tasks.
4.1	Datasets
To demonstrate the effectiveness of the proposed Tage on both node-level and graph-level tasks,
we evaluate Tage on three groups of real-world datasets that contain potentially multiple tasks. The
datasets are described as follows and their statistics are summarized in Appendix A.
MoleculeNet. The MoleculeNet (Wu et al., 2018) library provides a collection of molecular graph
datasets for the prediction of different molecule properties. In a molecular graph, each atom in
the molecule is considered as a node, and each bond is considered as an edge. The prediction of
molecule properties is a graph-level task. We include three graph classification tasks from Molecu-
leNet to evaluate the explanation of graph-level tasks: HIV, SIDER, and BACE.
Protein-Protein Interaction. The Protein-Protein Interaction (PPI) (Zitnik & Leskovec, 2017)
dataset documents the physical interactions between proteins in 24 different human tissues. In PPI
graphs, each protein is considered as a node with its motif and immunological features, and there
is an edge between two proteins if they interact with each other. Each node in the graphs has 121
binary labels associated with different protein functions. As different protein functions are not ex-
clusive to each other, the prediction of each protein function is considered an individual task. We
utilize the first five out of 121 tasks to evaluate the explanation of node-level tasks.
E-commerce Product Network. The E-commerce Product Network (EPN)1 is constructed with
subsampled, anonymized logs from an e-commerce store, where entities including buyers, products,
merchants, and reviews are considered as nodes, and interactions between entities are considered as
edges. We subsample the data for the sake of experimental evaluations and the dataset characteristics
1Proprietary dataset
6
Under review as a conference paper at ICLR 2022
TAGExpIainer
—PGExpIainer
—GNNExpIainer
→- DeepLIFT
GradCam
—TAGExpIainer
T- PGExpIainer
—GNNExpIainer
+ DeepLIFT
GradCAM
0.95	0.85	0.75	0.65
Sparsity
0.9	0.8	0.7	0.6	0.95	0.85	0.75	0.65
Sparsity	Sparsity
Figure 3: Quantitative performance comparisons with baseline methods on six tasks from Molecu-
leNet (top row) and PPI (bottom row). The curves are obtained by varying the threshold for selecting
important edges.
do not mirror actual production traffic. We study the explanation of the classification of fraudulent
entities (nodes), where the prediction for different types of entities are considered as individual tasks.
We evaluate our framework specifically on classifications of the buyer, merchant, and review nodes.
4.2	Experiment Settings and Evaluation Metrics
For each real-world dataset, we evaluate explainers on multiple downstream tasks that share a single
embedding model. For consistency with industrial use cases, we perform the two-stage training
paradigm to obtain GNN models to be explained. In particular, we first use unlabeled graphs to train
the GNN-based embedding model in an unsupervised fashion. We then freeze the embedding model
and use the learned embeddings to train individual downstream models structured as 2-layer MLPs.
Specifically, for graph-level classification tasks in MoleculeNet, we employ the GNN pretraining
strategy context prediction (Hu et al., 2020) to train a 5-layer GIN (Xu et al., 2019) as the embedding
model on ZINC-2M (Sterling & Irwin, 2015) containing 2 million unlabeled molecules. For the
node-level classification on PPI, we employ the self-supervised training method GRACE (Zhu et al.,
2020) to train a 2-layer GCN (Kipf & Welling, 2017) on all 21 graphs from PPI without using labels.
For the larger-scale node-level classification on EPN, we use graph autoencoder (GAE) (Kipf &
Welling, 2016) to train the embedding model on sampled subgraphs of EPN. More implementation
details are provided in Appendix B.
As the involved real-world datasets do not have ground truth for explanations, we follow previous
studies (Pope et al., 2019; Yuan et al., 2020; 2021) to adopt a fidelity score and a sparsity score to
quantitatively evaluate the explanations. Intuitively, the fidelity score measures the level of change in
the probability of the predicted class when removing important nodes or edges, whereas the sparsity
score measures the relative amount of important nodes or nodes associated with important edges. A
formulation of the scores are provided in Appendix B. Note that compared to explanation evaluation
with ground truths, the fidelity score is considered more faithful to the model, especially when the
model makes incorrect predictions, in which case the explanation ground truths become inconsistent
with evidence to making the wrong predictions. In practice, one needs to trade off between the
fidelity score and the sparsity score by selecting the proper threshold for the importance.
4.3	Quantitative Studies
We conduct two groups of quantitatively experimental comparisons. We first demonstrate the ex-
planation quality of individual tasks in terms of the fidelity score and the sparsity score. We do this
by comparing Tage with multiple baseline methods including non-learning-based methods Grad-
CAM (Pope et al., 2019) and DeepLIFT (Shrikumar et al., 2017), as well as learning-based methods
GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020). We do not include other
optimization or search-based methods such as Monte-Carlo tree search (Jin et al., 2020) due to the
7
Under review as a conference paper at ICLR 2022
Table 2: Fidelity scores with controlled sparsity on the node-level classification dataset PPI. Each
column corresponds to an explainer model trained on (or without) a specific downstream task. Un-
derlines highlight the best explanation quality in terms of fidelity, on the same level of sparsity.
Eval on	Task 0	PGExplainer (trained on)			Task 4	TAGE w/o downstream
		Task 1	Task2	Task 3		
Task0	0.184 ±0.3443	-0.005 ±0.268	0.033 ±0.335	0.034 ±0.310	0.018 ±0.194	0.271 ±0∙385~~
Task1	0.046 ±0.447	0.197 ±0.380	0.043 ±0.314	0.008 ±0.297	0.021 ±0.183	0.300 ±0.415
Task2	0.028 ±0.434	0.001 ±0.283	0.345 ±0.458	0.024 ±0.320	0.097 ±0.320	0.499 ±0.480
Task3	0.075 ±0.364	-0.015 ±0.219	0.036 ±0.317	0.262 ±0.418	0.040 ±0.221	0.289 ±0.427
Task4	0.035 ±0.413	-0.021 ±0.238	0.223 ±0.438	0.075 ±0.374	0.242 ±0.373	0.330 ±0.442
significant time cost on real-world datasets. Note that to show the effectiveness of universal expla-
nations over different downstream tasks, we only train one embedding explainer for all tasks in a
dataset, on top of which a gradient-based downstream explainer is applied to explain multiple down-
stream tasks. In contrast, for existing learning-based methods, we need to train multiple explainers
to explain downstream tasks individually. For all methods, we vary the threshold for selecting im-
portant nodes or edges and compare how fidelity scores change over sparsity scores on each task and
dataset. The results are shown in Figure 4. In particular, Tage outperforms other learning-based
explainers on BACE, SIDER, and PPI (tasks 0 and 1). For HIV and PPI (task 2), TAGE is more
effective at higher sparsity levels, i.e., when fewer nodes are considered important and masked.
To justify the necessity of task-agnostic explanation and demonstrate the universal explanation abil-
ity of Tage, we include PGExplainer as our baseline and compare the explanation quality when
adopting a single explainer to explain multiple downstream tasks. For PGExplainer, we train mul-
tiple explainers on different downstream tasks and evaluate each explainer on different downstream
tasks. For Tage, we train one explainer without downstream tasks and evaluate iton different down-
stream tasks. Results shown in Table 2 (PPI) and Appendix D (MoleculeNet and EPN) indicate that
task-specific explainers fail to generalize to different downstream tasks and hence are unable to pro-
vide universal explanations. On the other hand, the task-agnostic explainer, although trained without
downstream tasks, can provide explanations with even higher quality for any downstream tasks.
GNNExplainer and PGExplainer should generally outperform task-agnostic explainers, as they are
specific to data examples or tasks. This should especially be true when Tage and PGExplainer have
the same level of parameters. However, we surprisingly find that Tage outperforms the learning-
based baselines. One possible reason can be the non-injective characteristic of the downstream
MLPs on top of GNNs, where different embeddings can produce similar downstream prediction
results. Due to this characteristic, the learning objective of Tage computed between embeddings
brings stronger supervision than the objective computed between final predictions, as the latter ob-
jective does not guarantee consistency between embeddings or between input graphs and subgraphs.
4.4	Multitask Explanation Efficiency
A major advantage of the task-agnostic explanation is that it removes the need for training individual
explainers, which consumes the majority of the total time cost to explain a model on a dataset. We
hence evaluate the efficiency of Tage in terms of time cost for explanation and compare it to the
two learning-based explainer baselines. We record the time cost for the training and inference of
different explanation methods on the same dataset and device, shown in Table 3. All results are
obtained from running the explanation on the PPI dataset with 121 node classification tasks with a
single Nvidia Tesla V100 GPU. Although the inference time cost of Tage is slightly higher than
that of PGExplainer, the results show Tage costs significantly less time than GNNExplainer and
PGExplainer, especially in the multitask cases (T > 1). TAGE allows the explanation of many
downstream tasks within a reasonable time duration.
4.5	Visualizations and Explanation to Embedding Dimensions
We visualize the explanations of the three learning-based explanation methods on the BACE task,
which aims to predict the inhibitor effect of molecules to human β-secretase 1 (BACE-1) (Wu et al.,
2018). Additional visualizations on HIV and SIDER are also provided in Appendix E. The visualiza-
tion results are shown in Figure 4. Each molecule visualization shows the top 10% important edges
8
Under review as a conference paper at ICLR 2022
Table 3: Comparison of computational time cost among three learning-based GNN explainers on
the PPI dataset. The left two columns record time cost breakdown for T downstream tasks. The
fourth column estimates the total time cost for explaining all 121 tasks of PPI. The last row shows
the speedup times compared to GNNExplainer and PGExplainer, respectively.
Time cost	Training (s)	Inference (S)	Total time (T=1) (s)	Est. total for 121 tasks
GNNExplainer	20040.1*T	—	20040.1	28 d
PGExplainer	7117.0*T	427.2*T	7604.2	10.7 d
Tage	1405.3	582.7*T	1988.0	0.83 d
Speedup	14.3*T × / 5.1*T ×	-^-/0.73 x	10.1× / 3.8×	33.7× / 12.9×
(With downstream task)
(No downstream task)
(0.8153)
H2N
H2N
0.6183
(0.0193)
(0.3705)
0.5447
0.4659
F
Figure 4: Visualizations on explanations to the GNN model for the BACE task. Top 10% impor-
tant edges are highlighted with red shadow. The numbers below molecules are fidelity scores when
masking-out the top 10% important edges. Right two columns are explanations to two certain em-
bedding dimensions without downstream tasks. Fidelity scores in the right two columns explaining
two embedding dimensions are still computed for the BACE task but are just for reference.
0.9964
-0.0145
0.5022
(0.0494)
(bonds) predicted by an explainer with red shadow, together with the fidelity score on the molecule.
The left three columns are explanation results with the BACE downstream task. The right columns
are explanations by Tage to two specific graph embedding dimensions, without downstream mod-
els. Embedding dimensions with greater values among all are selected in the visualizations. To
obtain explanations to certain embedding dimensions, we input the one-hot vectors to the embed-
ding explainer as condition vectors. The visualization results indicate that while baseline methods
select scattered edges as important, Tage tends to select edges that form a connected substructure,
which is more reasonable when explaining molecule property predictions where a certain functional
group is important for the property. In addition, the right three columns indicate that dimensions in
the embedding correspond to different substructures and Tage is able to provide explanations to the
dimensions without downstream tasks.
5	Conclusions
Existing task-specific learning-based explainers become inapplicable under real scenarios when
downstream tasks or models are unavailable and suffer from inefficiency when explaining real-world
graph datasets with multiple downstream tasks. We introduced Tage, including the task-agnostic
GNN explanation pipeline and the self-supervised training framework to train the embedding ex-
plainer without knowing downstream tasks or models. Our experiments demonstrate that the Tage
generally achieves higher explanation quality in terms of fidelity and sparsity with the significantly
reduced explanation time cost. We discuss potential limitations and their solutions in Appendix G.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
The experiment setting and evaluation protocol are described in Section 4. In particular, we de-
scribe how the models to be explained are trained. To further ensure reproducibility, we provide
implementation details including explainer configurations, training settings, and how evaluation are
performed in Appendix B. The detailed computation of the sparsity and fidelity scores are provided
in Appendix C. The code to fully reproduce the results will be released upon acceptance.
References
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pp. 417-426, 2019.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning, pp.
1263-1272. PMLR, 2017.
Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference
on Artificial Intelligence and Statistics, pp. 297-304, 2010.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2019.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. In International Conference on
Learning Representations, 2020.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using
interpretable substructures. In International Conference on Machine Learning, pp. 4849-4859.
PMLR, 2020.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308, 2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations, 2017.
Wanyu Lin, Hao Lan, and Baochun Li. Generative Causal Explanations for Graph Neural Networks.
In International Conference on Machine Learning, 2021.
Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu,
Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and
Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. Journal of
Machine Learning Research, 22(240):1-9, 2021. URL http://jmlr.org/papers/v22/
21-0343.html.
Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang
Zhang. Parameterized explainer for graph neural network. In Advances in neural information
processing systems, 2020.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
10
Under review as a conference paper at ICLR 2022
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32, pp. 8θ24-8035.2019.
Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Ex-
plainability methods for graph convolutional neural networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 10772-10781, 2019.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International Conference on Machine Learning, pp. 3145-
3153, 2017.
Teague Sterling and John J Irwin. Zinc 15-ligand discovery for everyone. Journal of Chemical
Information and Modeling, 55(11):2324-2337, 2015.
Fan-Yun Sun, Jordan Hoffman, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-
supervised graph-level representation learning via mutual information maximization. In Interna-
tional Conference on Learning Representations, 2019.
Petar VeliCkovic, William Fedus, William L. Hamilton, Pietro Lio, YoshUa Bengio, and Devon
Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019.
Petar VeliCkoviC, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, YaoChen Xie, Limei Wang, Lei Cai, Qi Qi,
Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. AdvanCed graph and sequenCe neural networks
for moleCular property prediCtion and drug disCovery. arXiv preprint arXiv:2012.01981, 2020.
Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S
Pappu, Karl Leswing, and Vijay Pande. MoleCulenet: a benChmark for moleCular maChine learn-
ing. Chemical Science, 9(2):513-530, 2018.
Keyulu Xu, Weihua Hu, Jure LeskoveC, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019.
Rex Ying, Ruining He, Kaifeng Chen, Pong EksombatChai, William L Hamilton, and Jure LeskoveC.
Graph Convolutional neural networks for web-sCale reCommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974-
983, 2018.
Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure LeskoveC. Gnnexplainer:
Generating explanations for graph neural networks. In Advances in neural information processing
systems, pp. 9244-9255, 2019.
Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A
taxonomiC survey. arXiv preprint arXiv:2012.15445, 2020.
Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural
networks via subgraph explorations. arXiv preprint arXiv:2102.05152, 2021.
Yanqiao Zhu, YiChen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph Contrastive
representation learning. In ICML Workshop on Graph Representation Learning and Beyond,
2020.
Marinka Zitnik and Jure LeskoveC. PrediCting multiCellular funCtion through multi-layer tissue
networks. Bioinformatics, 33(14):i190-i198, 2017.
11
Under review as a conference paper at ICLR 2022
A Dataset Statistics
The statistics of datasets used for evaluating Tage are shown in Table 4.
Table 4: Statistics of multitask datasets used for explanation quality evaluation. The column “Total
under MolecUleNet indicates total number of commonly studied tasks from MolecUleNet.
	MoleculeNet					PPI	EPN
	HIV	BBBP	BACE	Sider	Total		
# of Graphs	41127	2039	1513^^	1427	—	-24	1
Avg. # of Nodes	25.53	24.05	34.12	33.64	—	56,944	5.86 mn.
Avg. # of Edges	27.48	25.94	36.89	35.36	—	818,716	63.07 mn.
# of Tasks	1	1	1	27	227	121	3
B Implementation Details
Structure of explainer. Our implementation is based on Pytorch (Paszke et al., 2019), Pytorch Ge-
ometric (Fey & Lenssen, 2019), and Dive-into-graphs (Liu et al., 2021). We implement the explainer
with a linear projection fp that maps the condition vector p to the same dimension as concatenated
embeddings, and a 2-layer MLP with ReLU activation that maps concatenated embeddings with
mask to the important score.
Implementation of training objectives. We adopt the Jason-Shannon Estimator as the lower bound
for mutual information maximization for the two public datasets. For graph-level tasks, given a mini-
batch of N samples, we consider the embeddings of a graph G and its subgraph Gs as a positive pair
(with N positive pairs in total), and the embeddings ofa graph Gi and the subgraph Gj,s of another
sample as a negative pair (with N2 - N pairs in total). For node-level tasks, we still randomly
sample N nodes from the entire graph at each iteration and compute the contrastive losses on original
embeddings of the N nodes, and embedding of the N nodes when the important subgraph is selected,
respectively for each node. Similar to graph-level tasks, we consider embeddings of the same node
(in original graph or in subgraph) as a positive pair, and a embeddings of node i in full graph and
node j in selected subgraph as a negative pair (N2 - N in total).
Training configurations. We set the hyperparameters in the size regularization term to λs = 0.05
and λe = 0.002, respectively. For graph-level explanation on MoleculeNet, we train the embed-
ding explainer on ZINC-2M with learning rate 1e - 4 and mini-batch size 256 for 1 epoch. The
random condition vectors are generated from laplace distribution Laplace(0, 0.2). For node-level
explanation on PPI, we train the embedding explainer on PPI without labels with learning rate 5e-6
and mini-batch size 4 for 1 epoch. The random condition vectors are generated from laplace dis-
tribution Laplace(0, 0.1). For the EPN dataset, we train the embedding explainer with InfoNCE
loss, learning rate 1e - 4 and mini-batch size 16. The random condition vectors are generated from
laplace distribution Laplace(0, 0.25). The hyperparameters in the size regularization term are set to
λs = 0.5 and λe = 0 for the stable training with InfoNCE.
Evaluation. In molecule and protein property prediction, we are usually interested in the positive
samples, i.e., the existence of what substructure lead to a certain property. For learning-based base-
line methods, we find it common that only one class of the two have good explanation, and the class
with higher explanation quality is not necessarily the positive class. For example, PGExplainer has
a near-to-zero fidelity score for the positive class of SIDER. We hence compare only the higher
fidelity score among two classes for all explanation methods and datasets.
12
Under review as a conference paper at ICLR 2022
C Fidelity and Sparsity
Given a set of graphs {Gi} and node masks m predicted by the explainer, the fidelity score and the
sparsity score are computed as follows.
1N
Fidelity Prob= NN Ei=I [f (Gi)ci - f (Gl-mi)d],	⑺
SparSity = NN Xi=ImiMVi1,	⑻
where N denotes the number of graphs or nodes to be explained, f denotes the GNN model as-
sociated with a specific downstream task, ci denotes the class of interest, which can be either the
labeled class or the original predicted class, Gi and Gi1-mi denote the original graph and graph with
important nodes removed, respectively. Explanations with both scores higher are better.
D Additional Results for Universal Explanation Ability
Universal explanation performance on MoleculeNet and EPN. Evaluation results for the uni-
versal explanation ability on the MoleculeNet and EPN datasets are shown in Table 5 and Table 6,
respectively.
Table 5: Fidelity scores with controlled sparsity on graph-level molecule property prediction tasks.
Each column corresponds to an explainer model trained on (or without) a specific downstream task.
Underlines highlight the best explanation quality in terms of fidelity, on the same level of sparsity.
Eval on	BACE	PGExplainer (trained on)		SIDER	TAGE w/o downstream
		HIV	BBBP		
BACE	0.252 ±0.340	0.007 ±0.251	0.026 ±0.022	-0.151 ±0.330	0.378 ±0.293-
HIV	-0.001 ±0.197	0.473 ±0.404	0.013 ±0.029	-0.060 ±0.356	0.595 ±0.321
BBBP	0.001 ±0.237	-0.056 ±0.226	0.182 ±0.169	-0.252 ±0.440	0.193 ±0.161
SIDER	0.012±0.219	-0.009 ±0.212	0.003 ±0.029	0.444 ±0.391	0.521 ±0.278
Table 6: Fidelity scores with controlled sparsity on the E-commerce product dataset. Each column
corresponds to one explainer model trained on different tasks or without downstream task. Under-
lines highlight the best explanation quality in terms of fidelity, on the same level of sparsity.
Eval on	PGExplainer (trained on)			TAGE w/o downstream
	Buyers	Sellers	Reviews	
Buyers	0.2009 ±0.2233	0.1731±0.3774	0.1740 ±0.4463	0.2713 ±0.1834~
Sellers	0.5465 ±0.4773	0.3246 ±0.4026	0.1128 ±0.3019	0.6515 ±0.3426
Reviews	0.4178 ±0.3683	0.1258 ±0.3492	0.2310 ±0.4178	0.5692 ±0.4214
Comparison of explanation performance when trained on different datasets. Specifically for
the MoleculeNet dataset, as there is a larger unlabeled dataset, ZINC, available for the first stage
training of encoder, the training of our exlainer is also performed on the ZINC dataset. For a more
strict comparison with the baseline explainer who is trained on individual MoleculeNet dataset, we
additional evaluate the explanation quality when the same individual MoleculeNet dataset is used
to train TAGE. The results are shown in Table 7. When trained on the same datasets individually,
TAGE still performs better than the baseline explainer in terms of fidelity scores. In the individual
dataset case, we need to train different explainers, similarly to the training of PGExplainer, as the
datasets for the four tasks are different.
Evaluation on the synthetic dataset BA-Shapes. On the BACE dataset and task, we additionally
compare TAGE with another recent SOTA learning-based method GEM (Lin et al., 2021) whose
explainer is trained based on the Granger causality in Table 8. Note that GEM is not originally
proposed under our setting. It assumes that there is a fixed number of important nodes when per-
forming explanation and hence the final explanation is a boolean selection of nodes. We adapt GEM
to compute fidelity scores under different sparsity scores by varying the threshold when generating
explanation groundtruth with Granger causality.
13
Under review as a conference paper at ICLR 2022
Table 7: An ablation on training TAGE on different datasets (ZINC v.s. individual MoleculeNet
datasets)._____________________________________________________________________
Method	BACE	HIV	BBBP	SIDER
PGExplainer	0.252 ±0.340	0.473 ±0.404	0.182 ±0.169	0.444 ±0.391
TAGE (individual)	0.402 ±0.281	0.541 ±0.330	0.202 ±0.157	0.516 ±0.292
TAGE (ZINC)	0.378 ±0.293	0.595 ±0.321	0.193 ±0.161	0.521 ±0.278
Table 8: A comparison between TAGE, GEM, and PGExplainer on BACE in terms of fidelity scores
when fixing the sparsity scores. For GEM, we vary the threshold when generating explanation
groundtruth with Granger causality to obtain explanations with different sparsity scores.
Sparsity	0.90	0.85	0.80	0.75
TAGE	0.3349	0.4992	0.5383	0.5309
GEM (Lin et al., 2021)	0.2829	0.3607	0.4260	0.4035
PGExplainer	0.2521	0.3207	0.4605	0.5161
E Visualizations on HIV and SIDER
Visualizations on HIV and SIDER are shown in Figure 5 and Figure 6, respectively.
(With downstream task)
(No downstream task)
Cl
Cl
ClOH
0.9810
0.7741
0.9820
OH
O
0.4294
0.5763
(0.4154)
(0.4357)
0.2156
Figure 5: Visualizations on explanations to the GNN model for the HIV task. Top 10% impor-
tant edges are highlighted with red shadow. The numbers below molecules are fidelity scores when
masking-out the top 10% important edges. Right two columns are explanations to two certain em-
bedding dimensions without downstream tasks.
(0.9894)
14
Under review as a conference paper at ICLR 2022
(With downstream task)
(No downstream task)
GNNEXpIainer
PGEXpIainer
TAGE
TAGE-Single Dimension TAGE-Single Dimension
Cl
Cl
-0.0528
0.3105
0.8071
(0.3551)
(0.0286)
Cl
Cl
Cl
Cl
Cl
0.4669
(0.2652)
(0.1826)
-0.1046
-0.2515
Figure 6: Visualizations on explanations to the GNN model for the SIDER task. Top 10% im-
portant edges are highlighted with red shadow. The numbers below molecules are fidelity scores
when masking-out the top 10% important edges. Right two columns are explanations to two certain
embedding dimensions without downstream tasks.
F	Experimental studies on the synthetic datasets BA-Shapes
We perform an additional evaluation on the BA-Shapes synthetic datasets used in GNNEx-
plainer Ying et al. (2019) and provided by Pytorch-Geometric (Fey & Lenssen, 2019). The syn-
thetic dataset is less complicated compared to the real-world datasets. We train a 3-layer GCN for
the node-classification with a training accuracy of 0.95. The AUC score (for importance edges) of
TAGE is 0.999 compared to 0.963 and 0.925 of PGExplainer and GNNExplainer, respectively. Note
that the baseline scores are from the PGExplainer paper and some re-implementations23 of PGEx-
plainer can also achieve an AUC score of 0.999. Our purpose to show our score on BA-Shapes
is to demonstrate that TAGE is on par with its baselines even when considering the typical single-
task setting. Figure 7 visualizes 20 examples of explanations. TAGE is able to provide accurate
explanations for all the 20 examples.
G Discussion of limitations and potential solutions
Inductive learning of explanations. Our study focus on the setting of inductive learning of the
explanation, i.e., to train the explainer on a given dataset and perform inference on new coming
data. There are many work conducted under the inductive setting, such as PGExplainer. All methods
under this setting may have a potential limitation that the explainer may suffer from some dataset
bias when training data and the data to be explained are inconsistent. This is an interesting problem
that requires further investigation. However, we believe that this is a separate problem and applies
2https://github.com/LarsHoldijk/RE-ParameterizedExplainerForGraphNeuralNetworks
3https://openreview.net/forum?id=tt04glo-VrT
15
Under review as a conference paper at ICLR 2022
Figure 7: Visualizations on explanations to the synthetic dataset BA-Shapes.
16
Under review as a conference paper at ICLR 2022
to all inductive learning methods. In addition, the size of graph could be inconsistent for training
and inference. To tackle this issue, we obtain the substructure by selecting top k percentage of edges
according to their important scores.
Black-box explanations. Similarly to our baseline method PGExplainer, our explainer relies on
node embeddings as inputs to the explainer. In particular, the node embeddings serve as represen-
tations to allow explainers identify each node. It is required by any (inductively) learning based
explanations to tell neural network-based explainers which edge they are looking at. A limitation of
the inductive methods is that when the node embeddings may become unavailable when explaining
a black-box model. The study of explaining black-box models (where only output is available) is a
different direction of study in scenarios like attacking. Many current SOTA explanation approaches,
such as Grad-Cam, GNN-LRP, and PGExplainer, fail under the black-box setting. However, if one
would like to adapt our approach to the black-box setting, it is still feasible by adopting a surro-
gate model for the black-box model and perform explanation on the surrogate model. In addition,
as mentioned above, the node embeddings are mainly used to identify which node the explainer
is looking at, we does not necessarily require the original embedding. When node embedding are
unavailable, we can still use any representation of nodes as long as it can identify the node based on
its feature and topology.
17