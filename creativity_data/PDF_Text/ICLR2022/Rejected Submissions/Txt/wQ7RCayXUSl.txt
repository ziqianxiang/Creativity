Under review as a conference paper at ICLR 2022
Why so pessimistic ? estimating uncertainties for
offline rl through ensembles, and why their indepen-
DENCE MATTERS
Anonymous authors
Paper under double-blind review
Ab stract
In offline/batch reinforcement learning (RL), the predominant class of approaches
with most success have been “support constraint” methods, where trained poli-
cies are encouraged to remain within the support of the provided offline dataset.
However, support constraints correspond to an overly pessimistic assumption that
actions outside the provided data may lead to worst-case outcomes. In this work,
we aim to relax this assumption by obtaining uncertainty estimates for predicted
action values, and acting conservatively with respect to a lower-confidence bound
(LCB) on these estimates. Motivated by the success of ensembles for uncertainty
estimation in supervised learning, we propose MSG, an offline RL method that
employs an ensemble of independently updated Q-functions. First, theoretically,
by referring to the literature on infinite-width neural networks, we demonstrate
the crucial dependence of the quality of derived uncertainties on the manner in
which ensembling is performed, a phenomenon that arises due to the dynamic
programming nature of RL and overlooked by existing offline RL methods. Our
theoretical predictions are corroborated by pedagogical examples on toy MDPs, as
well as empirical comparisons in benchmark continuous control domains. In the
significantly more challenging antmaze domains of the D4RL benchmark, MSG
with deep ensembles by a wide margin surpasses highly well-tuned state-of-the-
art methods. Consequently, we investigate whether efficient approximations can
be similarly effective. We demonstrate that while some very efficient variants
also outperform current state-of-the-art, they do not match the performance and
robustness of MSG with deep ensembles. We hope that the significant impact of
our less pessimistic approach engenders increased focus into uncertainty estima-
tion techniques directed at RL, and engenders new efforts from the community
of deep network uncertainty estimation researchers whom thus far have not em-
ployed offline reinforcement learning domains as a testbed for validating modern
uncertainty estimation techniques.
1	Introduction
Offline reinforcement learning (RL), also referred to as batch RL, is the setting where we are pro-
vided with a dataset of interactions with a Markov Decision Process (MDP), and the goal is to learn
an effective policy without further interactions with the MDP. Offline RL holds the promise of data-
efficiency through data reuse and improved safety due to minimizing the need for policy rollouts.
As a result, offline RL has been a subject of significant renewed interest in the machine learning
literature (Levine et al., 2020).
One common approach to offline RL, known as model-free, uses value estimation through approxi-
mate dynamic programming (ADP). The predominant algorithmic philosophy with most success in
ADP-based offline RL is to limit obtained policies to the support set of the available offline data,
with the intuition being that such constraints would reduce inaccurate value estimates since the ac-
tions chosen by the policy are close to the observed data. A large variety of methods have been
developed for enforcing such constraints, examples of which include regularizing policies with be-
havior cloning objectives (Kumar et al., 2019; Fujimoto & Gu, 2021), only performing updates on
actions observed in (Peng et al., 2019; Nair et al., 2020; Wang et al., 2020; Ghasemipour et al., 2021)
1
Under review as a conference paper at ICLR 2022
or close to (Fujimoto et al., 2019) the offline dataset, and regularizing to lower the estimated value
of actions not seen in the dataset (Wu et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021).
Support constraint forms of regularization correspond to an overly pessimistic assumption that any
action outside the provided data leads to highly negative outcomes (Buckman et al., 2020). Instead,
it would be preferable if we could place more trust into the predictions of value networks beyond
the training dataset. Indeed in adjacent fields of A.I., for different tasks of interest, researchers
have spent significant effort developing architectures with built-in inductive biases to help neural
networks not only obtain strong accuracy on the training set, but perform well beyond the data they
were trained on: convolutional and residual networks for computer vision (He et al., 2016), recurrent
networks and transformers for NLP (Vaswani et al., 2017), graph neural networks for graph-based
data (Li et al., 2015), and Nerf and Siren for effective signal modelling (Sitzmann et al., 2020;
Mildenhall et al., 2020).
However, trusting the predictions of neural networks beyond the training data is a challenging ordeal.
Many works have demonstrated that neural networks are prone to making high-confidence incorrect
predictions outside the training set, even on i.i.d. test data (Guo et al., 2017). This is a particularly
major issue for value estimation, since errors rapidly accumulate through dynamic programming and
lead to catastrophic results. Thus, in order to trust our models beyond the training set, a potentially
promising approach would be to obtain high quality uncertainty estimates on predicted action values.
In current supervised learning literature, “Deep Ensembles” and their more efficient variants have
been shown to be the most effective method for uncertainty estimation (Ovadia et al., 2019). In this
work, we aim to transfer the success of ensembles for uncertainty estimation to the setting of offline
RL. We begin by presenting MSG, an actor-critic offline RL algorithm leveraging an ensemble
of Q-value functions. MSG trains N Q-value functions completely independent of one-another,
and updates an actor network with respect to a lower confidence bound (LCB) on action values
obtained from the ensemble. By referring to the literature on infinite-width neural networks, we
theoretically demonstrate the critical importance of independence in Q-functions, a deviation from
standard practice in RL which more often uses the full ensemble to compute identical target values
for training each ensemble member. Our theoretical predictions are corroborated by experiments in
a toy MDP, and their relevance to practical settings is also verified through benchmark experiments.
In established benchmarks for offline RL, we demonstrate that MSG matches, and in the more
challenging domains, significantly exceeds the prior state-of-the-art. Inspired by this success, we
investigate whether the performance of MSG can be recovered through modern efficient approaches
to ensembling. While we demonstrate that efficient ensembles continue to outperform current state-
of-the-art batch RL algorithms, they cannot recover the performance of MSG using full deep ensem-
bles.
We hope that our work highlights some of the unique challenges of uncertainty estimation
in reinforcement learning, and the significant impact it can have on the performance of algo-
rithms. We also hope that our work encourages stronger engagement from researchers specializing
in uncertainty estimation techniques - Who typically use computer vision and NLP tasks as bench-
marks -to use offline RLasan additional testbed. Our results reveal that offline RL presents unique
challenges not seen in standard supervised learning, due to the accumulation of errors over multiple
back ups, and can be a valuable domain for testing novel uncertainty estimation techniques.
2	Related Work
Uncertainty estimation is a core component of RL, since an agent only has a limited vieW into
the mechanics of the environment through its available experience data. Traditionally, uncertainty
estimation has been key to developing proper exploration strategies such as upper confidence bound
(UCB) and Thompson sampling (Lattimore & Szepesvari, 2020), in which an agent is encouraged
to seek out paths Where its uncertainty is high. Offline RL presents an alternative paradigm, Where
the agent must act conservatively and is thus encouraged to seek out paths where its uncertainty
is low (Buckman et al., 2020). In either case, proper and accurate estimation of uncertainties is
paramount. To this end, much research has been produced towards the end of devising provably
correct uncertainty estimates (Thomas et al., 2015; Feng et al., 2020; Dai et al., 2020), or, at least,
bounds on uncertainty that are good enough for acting either exploratory (Strehl et al., 2009) or
2
Under review as a conference paper at ICLR 2022
conservatively (Kuzborskij et al., 2021). However, these approaches require exceedingly simple
environment structure, typically either a finite discrete state and action space or linear spaces with
linear dynamics and rewards.
While theoretical guarantees for uncertainty estimation are more limited in practical situations with
deep neural network function approximators, a number of works have been able to achieve practical
success, for example using deep network analogues for count-based uncertainty (Ostrovski et al.,
2017), Bayesian uncertainty (Ghavamzadeh et al., 2016; Yang et al., 2020), and bootstrapping (Os-
band et al., 2019; Kostrikov & Nachum, 2020). Many of these methods employ ensembles. In fact,
in continuous control RL, it is common to use an ensemble of two value functions and use their
minimum for computing a target value during Bellman error minimization (Fujimoto et al., 2018).
A number of works in offline RL have extended this to propose backing up minimums or lower
confidence bound estimates over larger ensembles (Kumar et al., 2019; Wu et al., 2019; Agarwal
et al., 2020). In our work, we continue to find that ensembles are extremely useful for acting con-
servatively, but the manner in which ensembles are used is critical. Specifically our proposed MSG
algorithm advocates for using independently learned ensembles, without sharing of target values,
and this import design decision is supported by empirical evidence.
The widespread success of ensembles for uncertainty estimation in RL echoes similar findings in
supervised deep learning. While there exist proposals for more technical approaches to uncertainty
estimation (Li et al., 2007; Neal, 2012; Pawlowski et al., 2017), ensembles have repeatedly been
found to perform best empirically (Lee et al., 2015; Lakshminarayanan et al., 2016). Much of the
active literature on ensembles in supervised learning is concerned with computational efficiency,
with various proposals for reducing the compute or memory footprint of training and inference on
large ensembles (Wen et al., 2020; Zhu et al., 2019; Havasi et al., 2020). While these approaches
have been able to achieve impressive results in supervised learning, our empirical results suggest
that their performance suffers significantly in an offline RL setting compared to deep ensembles,
and even naive Multi-Head ensembles (Lee et al., 2015; Osband et al., 2016; Tran et al., 2020)
which are not considered to be as effective in the supervised learning setting(Havasi et al., 2020).
3	Methodology
Notation Throughout this work, we represent Markov Decision Process (MDP) as M =
hS, A, r, P, γi, with state space S, action space A, reward function r : S × A → R, transition
dynamics P, and discount γ. In offline RL, we assume access to a dataset of interactions with the
MDP, which we will represent as collection of tuples D = {(s, a, s0, r, term)}N, where t is an
indicator variable that is set to True when s0 is a terminal state.
Two neural networks with an identical architecture, trained in an identical manner, will make differ-
ent predictions outside the training set when their weights are initialized with different random draws
from the initial weight distribution. So a natural question is, “Which network’s predictions can we
trust?” An answer to this question can be motivated by referring to the literature on infinite-width
networks. When performing mean-squared error regression in the infinite-width regime, informally
speaking, an intriguing property is that the distribution of predictions on unseen data is given by a
Gaussian Process whose kernel function is solely defined by, 1) the architecture, and 2) the choice
of initial weight distribution (Lee et al., 2019). Hence, to leverage the built-in architectural in-
ductive biases of value networks for offline RL, we can train policies to act with respect to the
lower-confidence bound (LCB) of the derived Gaussian Process.
In what follows, we present our proposed algorithm, MSG, which leverages ensembles to approx-
imate the LCB. Additionally, using the literature on infinite-width networks, we demonstrate the
theoretical advantage of forming ensembles in the manner we propose, which deviates from the
current use of ensembles in the reinforcement learning literature.
3.1	Model Standard-deviation Gradients (MSG)
MSG follows an actor critic setup where in each iteration, we first estimate the Q-values of the
current policy, and subsequently optimize the policy through gradient ascent on the lower confidence
bound of action value estimates.
3
Under review as a conference paper at ICLR 2022
Policy Evaluation At the beginning of training, we create an ensemble ofN Q-functions by taking
N samples from the initial weight distribution. Throughout training, the loss for the Q-functions is
the standard least-square Q-evaluation loss,
L(θi ) = E(s,a,r,s0,t)~D[ (Qθi Ga)- VI(T, 4 s0, term, n) ) 2 ]
yi ：= r + (1 — term) ∙ Y ∙ Ea0~∏3) Q>g((s0, a')]
(1)
(2)
where θi, θi denote the parameters and target network parameters for the ith Q-function, and term =
1[s0 is terminal]. In practice, the expectation in equation 1 is estimated by a minibatch, and the
expectation in equation 2 is estimated with a single action sample from the policy. After every
update to the Q-function parameters, their corresponding target parameters are updated to be an
exponential moving average of the parameters in the standard fashion. A key factor to note is
that, in contrast to the typical usage of ensembles in actor critic algorithms, each ensemble
member’s update (and yi) is completely independent from other ensemble members. As we
will present below, this an important algorithmic choice, both theoretically (Theorem 4.1) and
empirically (section 5.3.2).
Policy Optimization As described above, the choice of architecture and weight initialization in-
duces a distribution on predictions. Having approximated this distribution using an ensemble of
value networks, we can optimize the policy with respect to a lower confidence bound on action
values. Specifically, our proposed policy optimization objective in MSG is,
max Es~D,a~∏(s) [μ(s,a) + β ∙ σ(s,a)] where μ(s,a) = mean [Q&i (s,a)], σ(s,a)=std [Q&i (s,a)]
where β ∈ R is a hyperparameter that trades off conservatism and optimism. As our problem setting
is that of offline RL requiring conservatism, we use β ≤ 0.
3.2	The trade-off between trust and pessimism
While our hope is to leverage the distributions induced by the architecture choice, it is not always
feasible to do so; designed architectures can still be fundamentally biased in some manner, or we
can simply be in a setting with insufficient data coverage. Thus we need to trade-off trusting the gen-
eralization ability of our models with the pessimistic approach of support constraints. In this work,
inspired by CQL (Kumar et al., 2020), we add the following regularizer to our policy evaluation
objective (equation 1),
R(θi) = Es~D [Ea~π(a∣s)[Qθi(s, a)] — Q9, (s,。0)]
(3)
where aD denotes the action taken in state s in the dataset. We control the contribution of R(θi) by
weighting this term with weight parameter α. Practically, we estimate the outer expectation using
the states in the mini-batch, and we approximate the inner expectation using a single sample from
the policy. An example scenario where we have observed such a regularizer is helpful is when the
offline dataset only contains a narrow (e.g., expert) data distribution. We believe this is because
the power of ensembles comes from predicting a value distribution for unseen (s, a) based on the
available training data. Thus, if no data for sub-optimal actions is present, ensembles cannot make
accurate predictions and increased pessimism for unseen actions becomes necessary.
4 Independence in ensembles matters
4.1 The structure of uncertainties depends on how ensembling is done
In the reinforcement learning literature, ensembles are widely used for combatting over-estimation
bias (Haarnoja et al., 2018; Fujimoto et al., 2018; 2019; Kumar et al., 2020; Agarwal et al., 2020;
Ghasemipour et al., 2021). However, the typical usage of ensembles is to compute a target value y
using an ensemble of target networks, and subsequently update all the Q-functions with respect to
the same target, typically the minimum over targets. Hence, the objectives for all the Q-functions
share the same target value. In this section, drawing upon the literature of infinitely wide networks,
We demonstrate that having the updates for ensemble members be independent of one another - as
4
Under review as a conference paper at ICLR 2022
done in MSG - results in uncertainty estimates that align more closely with intuitive expectations,
compared to when their targets are shared.
We study this difference in the setting of policy evaluation using an infinite ensemble of infinite-
width Q-function networks (i.e. an ensemble of infinitely many Q-functions, each of which is an
infinite-width neural network, with the only difference being that their weights are initialized through
independent random draws from the initial weight distribution). For sake of simplicity of derivations
we assume that the policy we are trying to evaluate is deterministic and that we do not have terminal
states in the MDP. The policy evaluation routine we consider is Fitted Q-Evaluation (Fonteneau et al.,
2013), which can be described as repeatedly performing the following steps,
•	Compute TD Targets For each (s, a, r, s0) ∈ D compute the TD targets yi (r, s0, π)
•	Fit the Q-functions For each ensemble member, optimize the following objective until conver-
gence using full batch gradient descent.
∣DD∣	X	[(Qθi Ga)- yi (r, s0,π))2 i.
(s,a,r,s0)∈D
(4)
First we establish some notation. Let X denote a matrix where the rows are the state-action pairs
(s, a) in the offline dataset. Let R be the |D| × 1 matrix containing the rewards observed after each
(s, a) in X. Let X0 denote a matrix where the rows are the next state and policy action (s0, π(s0))
Additionally let,
Θo(A,B) := VθQθ(A) ∙VθQθ(B)T∣t=o	Θ-1 := Θo(X, X)-1	C := Θo(X0, X) ∙ Θ-1.
Θ0 (A, B) above is referred to as the tangent kernel (Jacot et al., 2018), which is defined as the
outerproduct of gradients of the Q-function, at initialization (iteration t = 0). The definition of Θ0
does not contain the variable i indexing the ensemble members because, at infinite width, Θ0 (A, B)
converges to a deterministic kernel and hence is the same for all ensemble members.
Intuitively, C is a ∣D∣ × ∣D ∣ matrix where cp,q (the element at row p, column q) captures a notion
of similarity between (s0, π(s0)) in the pth row of X0, and (s, a) in the qth row of X. Let Yti (with
shape ∣D∣ × 1) denote the targets used for fitting Qθi at iteration t.
Here, we will study the difference between the following two methods for computing targets: one
where each ensemble member uses its own predictions to compute the TD targets (analogous to
MSG), and another where all ensemble members share same target (analogous to typical use of
ensembles in offline RL):
• Method 1 (MSG, Independent Targets): yi(s, a) = r + Y ∙ Qθi(s0, π(s))
• Method 2 (Shared LCB): ∀i, yi (s, a) = r + γ
E
ensemble
Qθi (s0, π(s))
Std
ensemble
Qθi (s0, π(s))
—
As can be seen in Appendix F, for the two methods considered above, under the described policy
evaluation procedure, the values Qθi (s, a)∣t (at iteration t) can be computed in closed form for all
(s, a) ∈ S × A. This enables us to compare the final distribution of Q(s, a) after policy evaluation
under two methods.
Theorem 4.1. After t iterations, for both Method 1 and Method 2 we have,
Independent: LCB(Qt+i(.X,)) ≈ (1 + ... + YtC t)CR - JEh((1 + …+ YtCt)(QO(X 0) - CQo(X )))j
SharedLCB: LCB(Qt+1(X')) ≈ (1 + ... + YtC t)CR — (1 + ... + Yt Ct) JEh(QO(X 0) - CQo(X ))j
where LCB refers to mean - std, and the square and square-root operations are applied element-wise.
Proof. Please refer to Appendix F.
□
5
Under review as a conference paper at ICLR 2022
As can be seen, the equations for the lower-confidence bound (LCB) in both settings are very similar,
with the main difference being in the second terms which correspond to the “pessimism” terms. In
ensembles, the only source of randomness is in the initialization of the networks. In the infinite-
width setting this presents itself in the two equations above, where the random variables Q0(X0) -
CQ0 (X) produce the uncertainty in the ensemble of networks: For both Independent and Shared-
LCB, at iteration t + 1 we have,
Qt+1(X0) = Q0(X0) + C(Yt - Q0(X))	(5)
= CYt + (Q0 (X0) - CQ0 (X))	(6)
Thus, Q0(X0) -CQ0(X) represents the random value accumulated in each iteration. The accumula-
tion of the uncertainty is captured by the geometric term (1 + . . . + γtCt). Here is where we observe
the key difference between Independent and Shared-LCB: whether the term (1 + . . . + γtCt) is
applied inside or outside the expectation. In Independent ensembles, the randomness/uncertainty is
first backed-up by the geometric term and afterward the standard-deviation is computed. In Shared-
LCB however, first the standard-deviation of the randomness/uncertainty is computed, and after-
wards this value is backed up. Not only do we believe that the former (Independent) makes more
sense intuitively, but in the case of Shared-LCB, the pessimism term may contain negative values
which would actually result in an optimism bonus! As will be discussed below, we also empirically
investigate this question in section 5.3.2 (results in Figure 2) and find that while Shared-LCB can
perform decently in D4RL gym locmotion (Fu et al., 2020), Shared-LCB (and Shared-Min) com-
pletely fail on the more challenging domains. This is in line with the observation that no prior offline
RL methods rely solely on Shared-LCB or Shared-Min as the source of pessimism/conservatism
(Fujimoto et al., 2019; Kumar et al., 2019; Ghasemipour et al., 2021; An et al., 2021).
4.2 Validating theoretical predictions
In this section we aim to evaluate the validity of our theoretical discussion above in a simple toy
MDP that allows us to follow the idealized setting of the presented theorems more closely, and
allows for visualization of uncertainties obtained through different ensembling approaches.
Continuous Chain MDP The MDP we consider has state-space S = [-1, 1], action space A ∈
R, deterministic transition dynamics s0 = s + a clipped to remain inside S, and reward function
r(s, a) = 1[s0 ∈ [0.75, 1]].
Data Collection & Evaluation Policy The offline dataset we generate consists of 40 episodes,
each of length 30. At the beginning of each episode we initialize at a random state s ∈ S. In each
step take a random action sampled from Unif(-0.3, 0.3), and record all transitions (s, a, r, s0). For
evaluating the uncertainties obtained from different approaches, we create regions of missing data
by removing all transitions such that s or s0 are in the range [-0.33, 0.33]. The policy we choose to
evaluate with the different approaches is ∀s, π(s) = 0.1.
Optimal Desired Form of Uncertainty Note that the evaluation policy π(s) = 0.1 is always
moving towards the positive direction, and there is lack of data for states in the interval [-0.33, 0.33].
Hence, what we would expect is that in the region [0.33, 1] there should not be a significant amount
of uncertainty, while in the region [-1, -0.33] there should be significantly more uncertainty about
the Q-values of π because the policy will be passing through [-0.33, 0.33] where there is no data.
Results We visualize and compare the uncertainties obtained when the targets in the policy evalu-
ation procedure are computed as:
•	Independent (MSG): yi = r + Y ∙ Qgi(s0, π(s0))
•	Independent Double-Q: yi
r + Y ∙ min∣Qθi(s',∏(s0)), Q2i(s0,∏(s'))]
• Shared Mean: y = r + Y
mean
Qθi (s0, π(s0))
• Shared LCB: y = r + Y ∙
mean
Qθi(s0,π(s0))
-2 ∙ Std [Qθi(s0,∏(s0))]
•	Shared Min: y = r + Y ∙ min Qθθ( (s0, ∏(s0))]
6
Under review as a conference paper at ICLR 2022
Figure 1: Verifying theoretical predictions on the toy Continuous Chain MDP. The marked interval
[-0.33, 0.33] denotes the region of state-space with no data. As anticipated by Theorem 4.1, when the value
functions are trained independently, the derived uncertainties capture the interaction between available data,
the structure of the MDP, and the policy being evaluated. When the targets are shared, the networks behave
similarly to performing regression for oracle-given target values, i.e. there is randomness amongst ensemble
members only between [-0.33, 0.33] because there is no data in that region.
We include Independent Double-Q, as using Q-functions of the form Q(s, a) =
min Q1 (s, a), Q2(s, a) has become common practice in recent deep RL literature (Fujimoto et al.,
2018; Haarnoja et al., 2018)1.
In Figure 1 we plot the mean and two standard deviations of the predicted values for the policy we
evaluated, π(s) = 0.1 (additional experimental details presented in Appendix G.1). The first thing
to note is that, “Independent” ensembles effectively match our desired form of uncertainty: states
that under the evaluation policy lead to regions with little data have wider uncertainties than states
that do not. A second observation is that Shared LCB and Shared Min provide a seemingly
good approximation to the lower-bound of Independent predictions. Nonetheless, our theoretical
considerations suggest that these lower bounds may have important failure cases. Furthermore,
empirically - as we discuss below (Figure 2) - we were unable to train effective policies on offline
RL benchmarks using Shared LCB and Shared Min, despite the implementation differing in
only 2 lines of code.
Appendix G.2 presents additional very interesting empirical observations in this toy setting
which due space limitations we were unable to include in the main manuscript. We highly
encourage readers interested in the intersection of infinite-width networks and RL to take a look at
our observations as they may be hinting at intriguing avenues for future theoretical and practically
important work.
5 Experiments
In this section we seek to empirically answer the following questions: 1) How well does MSG
perform compared to current state-of-the-art in offline RL? 2) Can we match the performance of
MSG through efficient ensemble approaches popular in supervised learning literature? 3) Are the
theoretical differences in ensembling approaches elaborated on in the previous section practically
relevant?
5.1	D4RL Benchmark
We begin by evaluating MSG on the Gym (Brockman et al., 2016) subset of the D4RL offline RL
benchmark (Fu et al., 2020). Amongst the different data settings we focus our experiments on
the medium and medium-replay (sometimes referred to as mixed) settings as the other data
setting could not adequately differentiate between competitive methods. Experimental details such
as hyperparameter search procedure are described in Appendix C.
In addition to validating MSG, a secondary objective of ours is to gain a sense for the upper bound of
performance for various algorithms on d4rl Gym. For this reason - with an equal hyperparameter
tuning budget - we tune the main hyperparameter for each algorithm. As can be seen in Table 3
(Appendix A), across the board in D4RL Gym, MSG is competitive with very well-tuned current
state-of-the-art algorithms, CQL (Kumar et al., 2020) and F-BRC (Kostrikov et al., 2021). We
1 Note that Independent Double-Q is still an independent ensemble, where each ensemble member
has an architecture containing a min-pooling on top of two subnetworks.
7
Under review as a conference paper at ICLR 2022
I Domain	ICQL(RePorted)IMSG(N = 64) β α ∣
maze2d-umaze	5.7	100.2 ± 33.0 -8	0
maze2d-medium	5.0	87.4 ± 10.6	0	0
maze2d-large	12.5	147.8 ± 55.8 -4 0.1
antmaze-umaze	74.0	96.8 ± 2.0	-8 0.1
antmaze-umaze-diverse	84.0	60.2 ± 7.1	-8 0.5
antmaze-medium-play	61.2	80.0 ± 9.4	-4 0.1
antmaze-medium-diverse	53.7	78.8 ± 5.5	-4 0.1
antmaze-large-play	15.8	64.8 ± 10.7 -8 0
antmaze-large-diverse	14.9	68.8 ± 11.7 -8 0
Table 2: D4RL antmaze tasks. Figure
Table 1: Result on D4RL maze2d and antmaze domains. As taken from Fu et al. (2020).
we were unable to reProduce CQL antmaze results, we Present the
numbers rePorted by the original PaPer which uses the same net-
work architectures.
also note that our results for baseline algorithms exceed Prior rePorted results (often significantly),
Providing us confidence in their imPlementation.
Thus far we established the validity of MSG as an offline RL algorithm. However, on the gym
domains considered above, MSG’s Performance is on Par with CQL and F-BRC, and does not mo-
tivate the use of ensembles in lieu of suPPort constraints. To enable a better comParison amongst
comPeting algorithms, we exPeriment with the significantly more challenging antmaze tasks.
The antmaze tasks in D4RL, and in Particular the two antmaze-large settings, are considered to
be extremely challenging. The data for antmaze tasks consists of many ePisodes of an Ant agent
(Brockman et al., 2016) running along arbitrary Paths in a maze. The data from these trajectories is
relabeled with a reward of 1 when near a Particular location in the maze (at which Point the ePisode
is terminated), and 0 otherwise. The undirected, extremely sParse reward nature of antmaze tasks
make them very challenging, esPecially for the large maze sizes.
To the best of our knowledge, the antmaze-large domains are considered unsolved, unless specialized
technique such as hierarchical policies - which significantly simplify the problem - are used (e.g.
Ajay et al. (2020)). The current state-of-the-art is CQL (Kumar et al., 2019), and is the method we
comPare to. Table 1 Presents our emPirical results. As we were unable to reProduce the rePorted
results for CQL, for fairness we include the numbers rePorted by the original work which uses the
same network architectures. As can be seen, MSG achieves unPrecedented results on the antmaze
domains, clearly demonstrating the significant advantage of emPloying ensembles for batch RL.
5.2	RL Unplugged
Figure 4 Presents additional results using the RL UnPlugged benchmark (Gulcehre et al., 2020). We
comPare to results for Behavioral Cloning (BC) and two state-of-the-art methods, Critic-Regularized
Regression (CRR) (Wang et al., 2020) and MuZero UnPlugged (Schrittwieser et al., 2021). DesPite
the relatively very small architectures We used (焉 number of parameters), We observe that MSG
is on Par with the current state-of-the-art these tasks with the excePtion of humanoid.run which
appears to require the larger architectures used by (Gulcehre et al., 2020). Additional experimental
details as Well as numerical presentation of results can be found in appendix E.
5.3	Efficient Ensembles & Ensemble Ablations
In this section We dissect What aspects of MSG contribute to its superior results, and investigate
Whether the advantages of MSG can be realized through efficient ensemble approximations popular
in supervised learning.
5.3.1	Efficient Ensmebles
Thus far We have demonstrated the significant performance gains attainable through MSG. An im-
portant concern hoWever, is that of parameter and computational efficiency: “Deep Ensembles” re-
sult in an N -fold increase in memory and compute usage for the Q-netWorks. While this might not
be a significant problem for D4RL benchmark domains due to small model footprints , it becomes
a major bottleneck With larger architectures such as those used in language and vision domains. To
8
Under review as a conference paper at ICLR 2022
Figure 2: Results for efficient ensembles and ensemble ablations. Numerical values can be found in Table 4.
this end, we evaluate whether recent advances in efficient ensemble approaches also transfer well to
the problem of batch RL. Specifically, the efficient ensemble approaches we consider are:
Multi-Head (Lee et al., 2015; Osband et al., 2016; Tran et al., 2020) Multi-Head refers to en-
sembles that share a “trunk” network and have separate “head” networks for each ensemble member.
In this work, we modify the last layer of a value network to output N predictions instead of a single
Q-value, making the computational cost of this ensemble on par with a single network.
Multi-Input Multi-Output (MIMO) (Havasi et al., 2020) MIMO is an ensembling approach that
approximately has the same parameter and computational footprint as a single network. The MIMO
approach only modifies the input and output layers of a given network. In MIMO, to compute
predictions for data-points x1, ..., xN under an ensemble of size N, the data-points are concatenated
and passes to the network. The output of the network is then split into N predictions y1, ..., yN. For
added clarification, we include Figure 3a depicting how a MIMO ensemble network functions.
Batch Ensembles (Wen et al., 2020) Batch Ensembles incorporate rank-1 modulations to the
weights of fully-connected layers. More specifically, let W be the weight matrix of a given fully-
connected layer and let x be the input to the layer. The output of the layer for ensemble member i
is computed as σ(((WT (x ◦ ri)) ◦ si) + bi), where ◦ is the element-wise product, parameters with
superscript i are separate for each ensemble member, and σ is the activation function. While Batch
Ensemble is efficient in terms of number of parameters, in our actor-critic setup its computational
cost is on the same order as deep ensembles since for policy updates we need to evaluate each
ensemble member separately.
Results As can be seen in Table 4, the performance gains of MSG can to some extent also be
realized by efficient ensemble variants. Interestingly, in our experiments the most effective efficient
ensemble approach is Multi-Head ensembles, which is not considered to be the most competitive
uncertainty estimation technique in the supervised learning literature (Havasi et al., 2020). Com-
pared to CQL, Multi-Head ensembles continue to be competitive on D4RL gym, and noticeably
outperform CQL on antmaze-large tasks. Additionally, training Multi-Head ensembles is the most
efficient method, on par with training without ensembling (N = 1).
Nonetheless, compared to MSG using deep ensembles, there is a significant performance gap. We
believe this observation very clearly motivates future work in developing efficient uncertainty esti-
mation approaches that are better suited to the domain of reinforcement learning. To facilitate this
direction of research, in our codebase which will be open-sourced, we also include a complete boil-
erplate example amenable to drop-in implementation of novel uncertainty-estimation techniques.
5.3.2 Ensemble Ablations
Finally, through the ablations in Table 4 we seek to create a better sense for the various components
of MSG.
•	Comparing MSG with deep ensembles (N = 64), and Multi-Head ensembles (N = 64), to no
ensembling (N = 1) we see very clearly the massive advantage of ensembling.
•	Comparing MSG, which uses Independent targets, to Shared LCB and Shared Min we
see that the latter non-independent approaches significantly underperform.
•	Comparing CQL to N = 1 (no ensembling, only CQL-inspired regularizer), we observe that the
CQL regularizer tends to be better on Gym domains, and our regularizer may be better on antmaze
domains but our results are inconclusive. The advantage of the regularizer we used is significant
computationl efficieny due to not using importance sampling.
9
Under review as a conference paper at ICLR 2022
6 Discussion
Our work has highlighted the significant power of ensembling as a mechanism for uncertainty es-
timation for offline RL. Theoretically, and practically through benchmark experiments, we have
studied the critical importance of the manner in which ensembling is done. An important out-
standing direction is how can we design improved efficient ensemble approximations, as we have
demonstrated that current approaches used in supervised learning - some of which do lead to state-
of-the-art results offline RL results - are not nearly as effective as deep ensembles. We hope that this
work engenders new efforts from the community of deep network uncertainty estimation researchers
whom thus far have not employed offline reinforcement learning domains as a testbed for validating
modern uncertainty estimation techniques.
10
Under review as a conference paper at ICLR 2022
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, pp. 104-114. PMLR,
2020.
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline prim-
itive discovery for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611,
2020.
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline re-
inforcement learning with diversified q-ensemble. Advances in Neural Information Processing
Systems, 34, 2021.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020.
Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. Coindice:
Off-policy confidence interval estimation. arXiv preprint arXiv:2010.11652, 2020.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel bellman statistics. In International Conference on Machine Learning, pp. 3102-3111.
PMLR, 2020.
Raphael Fonteneau, Susan A Murphy, Louis Wehenkel, and Damien Ernst. Batch mode reinforce-
ment learning based on the synthesis of artificial trajectories. Annals of operations research, 208
(1):383-416, 2013.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Seyed Kamyar Seyed Ghasemipour, Shixiang Gu, and Richard Zemel. Smile: Scalable meta inverse
reinforcement learning through context-conditional policies. 2019.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective offline and online rl. In International Conference
on Machine Learning, pp. 3682-3691. PMLR, 2021.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. arXiv preprint arXiv:1609.04436, 2016.
Caglar Gulcehre, ZiyU Wang, Alexander Novikov, Thomas Paine, Sergio Gomez, Konrad Zolna,
Rishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl unplugged:
A collection of benchmarks for offline reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018.
11
Under review as a conference paper at ICLR 2022
Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Laksh-
minarayanan, Andrew M Dai, and Dustin Tran. Training independent subnetworks for robust
prediction. arXiv preprint arXiv:2010.06610, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Ilya Kostrikov and Ofir Nachum. Statistical bootstrapping for uncertainty estimation in off-policy
evaluation. arXiv preprint arXiv:2007.13609, 2020.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774-5783. PMLR, 2021.
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesvari. Confident off-policy eval-
uation and selection through self-normalized importance weighting. In International Conference
on Artificial Intelligence and Statistics, pp. 640-648. PMLR, 2021.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572-8583, 2019.
Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why
m heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint
arXiv:1511.06314, 2015.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Ping Li, Jinde Cao, and Zidong Wang. Robust impulsive synchronization of coupled delayed neural
networks with uncertainties. Physica A: Statistical Mechanics and its Applications, 373:261-272,
2007.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018.
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European
conference on computer vision, pp. 405-421. Springer, 2020.
12
Under review as a conference paper at ICLR 2022
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Awac: Accelerating online rein-
forcement learning with offline datasets. 2020.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-Dickstein,
and Samuel S Schoenholz. Neural tangents: Fast and easy infinite neural networks in python.
arXiv preprint arXiv:1912.02803, 2019.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing systems, 29:4026-4034, 2016.
Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via randomized
value functions. J. Mach. Learn. Res., 20(124):1-62, 2019.
Georg Ostrovski, Marc G Bellemare, Aaron Oord, and Remi Munos. Count-based exploration with
neural density models. In International conference on machine learning, pp. 2721-2730. PMLR,
2017.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.
Nick Pawlowski, Andrew Brock, Matthew CH Lee, Martin Rajchl, and Ben Glocker. Implicit weight
uncertainty in neural networks. arXiv preprint arXiv:1711.01297, 2017.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis
Antonoglou, and David Silver. Online and offline reinforcement learning by planning with a
learned model. arXiv preprint arXiv:2104.06294, 2021.
Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020.
Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finite mdps: Pac
analysis. Journal of Machine Learning Research, 10(11), 2009.
P. Thomas, G. Theocharous, and M. Ghavamzadeh. High confidence off-policy evaluation. In
Proceedings of the 29th Conference on Artificial Intelligence, 2015.
Linh Tran, Bastiaan S Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V Dillon, Jasper Snoek,
Stephan Mandt, Tim Salimans, Sebastian Nowozin, and Rodolphe Jenatton. Hydra: Preserving
ensemble diversity for model distillation. arXiv preprint arXiv:2001.04694, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020.
Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient
ensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020.
13
Under review as a conference paper at ICLR 2022
Mengjiao Yang, Bo Dai, Ofir Nachum, George Tucker, and Dale Schuurmans. Offline policy selec-
tion under uncertainty. arXiv preprint arXiv:2012.06919, 2020.
Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse reinforcement learning with
probabilistic context variables. arXiv preprint arXiv:1909.09314, 2019.
Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or
more networks per bit? In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4923-4932, 2019.
14
Under review as a conference paper at ICLR 2022
A D4RL Gym Locomotion B enchmark Table
I Domain	IBCl CQL α ∣ F-BRC (no bonus) a IlMSG(N = 64) β α IlDataToP 25 ∣
halfcheetah-medium	36.1	50.1 ± 1.2	0	45.7 ± 0.5	0.01	50.6 ± 1.5	0	0	37.0 ± 2.0
hopper-medium	29.0	90.2 ± 10.5	2.58	99.0 ± 4.1	0.012	88.2 ± 11.6	-1	0.5	101.1 ± 1.1
WaIker2d-medium	6.6	82.8 ± 0.6	3.38	81.6 ± 0.2	0.016	83.6 ± 2.0	-1	0.1	77.6 ± 1.4
halfcheetah-mixed	^38Γ	47.6 ± 2.5	0.67	45.3 ± 0.1 ^^	^^OF	52.1 ± 1.7	0	ɪ	38.1 ± 4.1
hopper-mixed	11.8	61.1 ± 36.8	0.67	29.3 ± 0.8	0.019	82.1 ± 24.2	-2	0	101.8 ± 1.5
WaIker2d-mixed	11.3	20.5 ± 3.8	3.38	28.4 ± 5.7	0.038	24.0 ± 9.4	-2	0.1	50.6 ± 3.9
Table 3: Results on Gym subset of the D4RL benchmark. For MSG we use an ensemble size of N = 64. For
fairness of comParison, F-BRC is ran without adding a survival reward bonus. For each method we also rePort
the hyPerParameter value used. Full exPerimental details are Presented in APPendix C.
B Efficient Ensembles & Ensemble Ablations Table
Domain	I CQL Il Deep Ens.		MIMO-0.5	MIMO-1	Multi-Head	Batch Ens	N =1	Shared LCB	Shared Min
halfcheetah-medium	50.1	50.6	40.0	44.4	49.8	40.2	48.6	47.9	36.0
hopper-medium	90.2	88.2	60.2	65.1	94.8	41.6	50.2	30.0	43.6
WaIker2d-medium	82.8	83.6	80.7	83.6	84.5	13.2	84.9	85.5	0.0
halfcheetah-mixed	47.6	52.1	0.0	6.7	49.4	43.0	49.6	51.0	2.3
hopper-mixed	61.1	82.1	22.7	32.5	45.5	35.0	30.5	32.5	6.7
walker2d-mixed	20.5	24.0	14.5	16.8	15.9	12.0	11.5	26.3	0.0
antmaze-large-diverse	15.8	64.8	11.0	26.0	33.5	0.0	noisy 30.5	0.0	0.0
antmaze-large-play	14.9	68.8	7.5	20.5	27.5	0.0	noisy 11.0	0.0	0.0
Table 4: Results for efficient ensembles and ensemble ablations.
C D4RL Gym Details
All Policies and Q-functions are a 3 layer neural network with relu activations and hidden layer size
256. The Policy outPut is a normal distribution that is squashed to [-1, 1] using the tanh function.
All methods were trained for 1M stePs. CQL and MSG are trained with behavioral cloning (BC) for
the first 50K stePs. F-BRC Pretrains with 1M stePs of BC.
CQL, F-BRC, and MSG are tuned with an equal hyPerParameter search budget. Running the best
found hyPerParameter using 5 new random seeds. Each run is evaluated for 10 ePisodes, and the
mean and standard deviation across the 5 runs is rePorted in the table. For fairness of comParison,
F-BRC is ran without adding a survival reward bonus. MSG and CQL are imPlemented in our code,
and for F-BRC we use the oPensourced codebase.
C.1 Hyperparameter Search
For all methods we Performed hyPerParameter search using 2 seeds. Based on the results of the 2
seeds, we chose the hyPerParameter to use, and using this choice of hyPerParameter we ran exPeri-
ments with 5 new random seed.
MSG β ∈ {0., -1., -2., -4. -8.},α ∈ {0., 0.1, 0.5, 1., 2.}
CQL α ∈ {0., 0.1} + np.exp(np.linspace(np.log(0.1), np.log(10.), steps = 23))
F-BRC α ∈ {0.} + np.exp(np.linspace(np.log(0.01), np.log(10.0), steps = 24))
15
Under review as a conference paper at ICLR 2022
750
500
250
0
(a) Visual depiction of MIMO Ensemble
yι
(b) D4RL antmaze tasks. Figure taken from
Fu et al. (2020).
ILJILJIJlLlILJLJLJILIJI
CartPole.swingup
finger.turn_hard
fish.swim
manipulator.insert_ball manipulator.insert_peg
walker.stand
walker.walk
cheetah.run
humanoid.run
mean
■ BC BMSG (N = 64) ■ CRR ■ MuZero Unplugged
Figure 4: Results for DM Control Suite subset of the RL Unplugged benchmark (Gulcehre et al., 2020). We
note that: 1) the architecture we used are smaller by a factor of approximately 60x, 2) CRR results are reported
by their best checkpoint throughout training which differs from ours, BC, and MuZero Unplugged which report
performance at the end of training. Baseline results taken from Schrittwieser et al. (2021).
D Antmaze Details
We use the same hyperparameter search procedure as Gym results, with same architectures. The
only difference is that models are trained for 2M steps and at evaluation time they are rolled out for
100 episodes instead of 10.
In prior work, the rewards in the offline dataset are converted using the formula 4(r - 0.5). We also
use the same reward transformation.
E RL Unplugged
E.1 DM Control Suite Tasks
The networks used in Gulcehre et al. (2020) for DM Control Suite Tasks are very large relative to
the networks we used in the D4RL benchmark; roughly the networks contain 60x more parame-
ters. Using a large ensemble size with such architectures requires training using a large number of
devices. Furthermore, since in our experiments with efficient ensemble approximations we did not
find a suitable alternative to deep ensembles (section 5.3.1), we decided to use the same network
architectures and N = 64 as in the D4RL setting (enabling single-GPU training as before).
Our hyperparameter search procedure was similar to before, where we first performed a coarse
search using 2 random seeds and hyperparameters β ∈ {-1., -2., -4. - 8.}, α ∈ {0., 0.5}, and for
the best found hyperparameter, ran final experiments with 5 new random seeds.
F	Theory
For our notation to match Lee et al. (2019), throughout this section we use the f to denote the
network Q, and we use x instead of (s, a).
In Lee et al. (2019) (section 2.4) it is shown that when training an infinitely wide neural network
to perform regression using mean squared error, subject to technical conditions on the learning rate
used, the predictions of the trained network are equivalent to if we had linearized (Taylor expanded)
16
Under review as a conference paper at ICLR 2022
the network at its initialization, and trained the linearized network instead. This means that after t
iterations of our policy evaluation procedure, ∀x, t, ftlin(x) = ft(x), where
ftin(x) := fo(x) + Vθfo(x)∣θ=θo ωt
ωt := θt - θ0
Hence we only need to study the evolution of the linearized network ftlin across iterations. The
theorems in the main manuscript are direct corollaries of the following two derivations.
Below, we will overload some notation. When flin is applied to a matrix we mean that we apply flin
to each row of the matrix and stack the results into a vector. By Θ0(x, X) we mean to treat x as a
row matrix.
F.1 Closed form when ensemble members use their own targets
For a single - infinitely wide - ensemble member, using the equations in Lee et al. (2019) (Sec-
tion 2.2, equations 9-10-11) we can write the following recursive updates for our policy evaluation
procedure
Θ-1=Θ0(X, X)-1	⑺
C = Θ 0 (X0, X )Θ-1	(8)
Yt = R+γftlin(X0)	(9)
ftli+n1(X) =Yt	(10)
∀x,ft+ ι(x) = fo(x) + Θo(x, X)Θ-I(Yt- fo(X))	(11)
ftii+ι(X0) = fo(X0) + Θo(X0, X)Θ-I(Yt- fo(X))	(12)
=fo(X0) + Θo(X0, X)Θ-1(R + Yftin(X0) - fo(X))	(13)
=f0(X0)+CR+γCftlin(X0)-Cf0(X)	(14)
= f0(X0) + CR - Cf0(X) + γCftlin(X0)	(15)
= . . .	(16)
= (1+...+γtCt)f0(X0)+CR-Cf0(X) +(γC)t+1f0(X0)	(17)
E[ftli+n1(X0)] = (1+...+γtCt)CR	(18)
Var[ftli+n1(X0)] =Eh(1+...+γtCt)(f0(X0) - Cf0(X)) + (γC)t+1f0(X0)2i (19)
≈Eh(1+...+γtCt)(f0(X0) -Cf0(X))2i ast→∞	(20)
F.2 Closed form when ensemble members use shared mean targets
Let us consider the setting where all ensemble memebers use their mean as the target.
17
Under review as a conference paper at ICLR 2022
Θ-1	ʌ	1 ：=Θo(X, X)-1	(21)
C	=Θ o(X 0, X )Θ-1	(22)
Yt	= R + γftlin(X0)	(23)
ftli+n1(X)	= Yt	(24)
∀x, ftli+n1(x)	=fo(x) + Θo(x, X)Θ-I(Yt- fo(X))	(25)
Yt	= R + γ E	[ftlin(X0)] ensemble	(26)
	=R+γ E	[f0(X0)]+γCYt-1 -γC E	[f0(X)] ensemble	ensemble	(27)
	= R + γCYt	(28)
	=...	(29)
	= (1 + . . . + γtCt)R	(30)
F.3 Closed form when ensemble members use shared LCB targets
Let us consider the setting where all ensemble memebers use shared LCB as the target.
Θ-1	ʌ	1 :=Θo(X, X)-1	(31)
C	:=Θ o(X 0, X )Θ-1	(32)
Yt	= LCBR+γftlin(X0)	(33)
	= R + γLCBftlin(X0)	(34)
ftli+n1(X)	= Yt	(35)
∀x, ftli+n1(x)	=fo(x) + Θo(x, X)Θ-I(Yt- fo(X))	(36)
E[ft+1(X0)]	=E[fo(X0) + C∙(Yt- fo(X))]	(37)
	=C ∙Yt	(38)
Var[ft+1(X0)]	=E[(fo(X0) — C ∙ fo(X))2] = constant	(39)
A	:=√ constant	(40)
LCBftli+n1(X0)	=C ∙Yt - A	(41)
Yt	= R + γLCBftlin(X0))	(42)
	= R + γCYt - γA	(43)
	= (1+...+γtCt)R- (1+...+γt-1Ct-1)γA	+ γt+1CtLCBf0(X0))
		(44)
	≈ (1 + ... + γtCt)R - (1 + ... + γt-1Ct-1)γA	as t → ∞	(45)
LCBftli+n1(X0)	≈ (1+...+γtCt)CR- (1+...+γtCt)A	(46)
F.4 Why is Independent preferable to Shared-LCB
An important question to consider is why Independent ensembles should be preferred over Shared-
LCB ensembles? Here we present our reasoning for why we Independent ensembles would be
preferable to Shared-LCB ensembles.
With the derivations in the above sections, we can compare the difference amongst uncertainty
estimation techniques. The key comparison needed is to understand the difference between
18
Under review as a conference paper at ICLR 2022
LCB ftli+n1(X0) under Independent vs. Shared-LCB settings. As a reminder, X0 is a matrix where
each row contains (s, π(s)), and ft+1 = ftli+n 1 under the infinite-width regime. From the above
equations we have:
Independent: LCB ftli+n1 (X0)
Shared-LCB: LCB ftli+n1 (X0)
≈ (1 + ... + YtC t)CR - JEh((1 + …+ YtCt)(fo(X 0) - Cfo(X )))2i
≈ (1 + ... + YtC t)CR -(1 + ... + Y tCt) ^h(fo(X 0) - Cfo(X ))2i
where the square and square-root operations are applied element-wise to the vector values.
As can be seen, the equations for the lower-confidence bound (LCB) in both settings are very similar,
with the main difference being in the second terms which correspond to the “pessimism” terms. In
the infinite-width setting, the only source of randomness is in the initialization of the networks. This
fact presents itself in the two equations above, where the random variables f0(X 0)-Cf0(X) produce
the uncertainty in the ensemble of networks; regardless of using Independent or Shared-LCB, after
any iteration t we have,
Yt = R+Yft(X0)	(47)
ft+1(X0) =	f0(X0)	+ C(Yt	-	f0(X))	(48)
=	CYt +	(f0(X0)	-	Cf0 (X))	(49)
E[ft+1 (X0)] = CE[Yt]	(50)
Thus, f0(X0) - Cf0 (X) represents the random value accumulated in each iteration, and they are
accumulated through backups by the geometric term (1 + . . . + YtCt).
Here is where we observe the key difference between Independent and Shared-LCB: whether the
term (1+. . .+YtCt) is applied inside or outside the expectation. In Independent ensembles, the ran-
domness/uncertainties is first backed-up by the geometric term and afterward the standard-deviation
is computed. In Shared-LCB however, first the standard-deviation of the randomness/uncertainties
is computed, and afterwards this value is backed up. Not only do we believe that the former (Inde-
pendent) makes more sense intuitively, but in the case of Shared-LCB, the second term may contain
negative values which would actually result in an optimism bonus!
F.5 Comparing the structure of uncertainties under Independent and
Shared-Mean
The above results enable us to compute in closed form the predictions of the infinite-width networks
under different training regimes ∀x.
The equations above present the closed form expressions for the predictions of each ensemble mem-
ber after t + 1 iterations of the policy evaluation procedure. Since the ensemble members only differ
in their weight initialization (random draws from the initial weight distribution), the random vari-
ables are f0(x), f0(X), f0(X0). As mentioned in the main text, the neural tangent Θ0 is identical
across ensemble members due to being in the infinite-width regime (Jacot et al., 2018).
Since ∀x,	E	[f0(x)] = 0 (Lee et al., 2019; 2017; Matthews et al., 2018), the expected values
ensemble
of ft+1(x) is identical in for both methods of computing TD targets,
E	[Yt+1] = (1+...+YtCt)R
ensemble
∀x,	E	[ft+ι(x)] = Θ o(x, X )Θ-1(1 + ... + γ tC t)R
ensemble
(51)
(52)
However, the expression for variances is very different. When the targets used are independent
we have,
∀x, Var [ft+1(x)]
ensemble
= E	[(fo (x)+Θo(x, X )Θ-1((1 + ... + YtC t)(γfo(X 0) - fo(X )))21	(53)
ensemble
19
Under review as a conference paper at ICLR 2022
In contrast, when the targets are shared mean of targets, we have,
∀x, Var [ft+1 (x)]=	E	ffo(x) + Θo(x, X)Θ-1( - fo(X)))2	(54)
ensemble	ensemble
The matrix C captures a notion of similarity between the (s, a) in X, and the (s0, π(s0)) in X0. Thus,
the term Ct has the interpretation of where the policy π(s) would find itself t steps into the future,
and (1 + . . . + γtCt) can be interpreted as the policy’s discounted state-action visitation, but in the
feature-space given by the neural network architecture. Since in ensembles the standard deviation of
predictions quantifies the amount of uncertainty, the expression in equation 53 tells us that when
the targets are independent, the ensemble of Q-functions “backs up the uncertainties through
dynamic programming with respect to the policy being evaluated”.
In contrast, when the targets are shared, the closed form expression for ft+1(x) is equivalent
to an oracle presenting us with targets (1 + . . . + γtCt)R for training examples X, and
training the ensemble members using mean squared error regression to regress these values.
G Additional Toy Experiments & Details
G.1 Additional Implementation Details for Figure 1
To evaluate the quality of uncertainties obtained from different Q-function ensembling approaches,
we create N = 64 Q-function networks, each being a one hidden layer neural network with hidden
dimension 512 and tanh activation. The initial weight distribution is a fan-in truncated normal dis-
tribution with scale 10.0, and the initial bias distribution is fan-in truncated normal distribution with
scale 0.05. We did not find results with other activation functions and choices of initial weight and
bias distribution to be qualitatively different. We use discount γ = 0.99 and the networks are opti-
mized using the Adam (Kingma & Ba, 2014) optimizer with learning rate 1e-4. In each iteration,
we first compute the TD targets using the desired approach (e.g. independent vs. shared targets)
and then fit the Q-functions to their respective targets with 2000 steps of full batch gradient descent.
We train the networks for 1000 such iterations (for a total of 2000 × 1000 gradient steps). Note that
we do not use target networks. Given the small size of networks and data, these experiments can be
done within a few minutes using a single GPU in Google Colaboratory which we will also
opensource.
G.2 Additional Toy Experiments
The toy experiment presented in section 4.2 uses a single-hidden layer finite-width neural network
architecture with tanh activations, uses the “standard weight parameterization” (i.e. the weight
parameterization used in practice) as opposed to the NTK parameterization (Novak et al., 2019), and
optimizes the networks using the Adam optimizer (Kingma & Ba, 2014). While this setup is close
to the practical setting and demonstrates the relevance of our proposal for independent ensembles
for the practical setting, an important question posed by our reviewers is how close these results are
too the theoretical predictions present in 4.1. To answer this question, we present the following set
of results.
Using the identical MDP and offline data as before, we implement 1 hidden layer neural networks
with erf non-lineartiy. The networks are implemented using the Neural Tangents library (Novak
et al., 2019), and use the NTK parameterization. The networks in the ensemble are optimized using
full-batch gradient descent with learning rate 1 for 500 steps of FQE Fonteneau et al. (2013), where
each in each step the networks are updated for 1000 gradient steps. We vary the width of the
networks from 32 to 32768 in increments of a factor of 4, plotting the mean and standard deviation
of the network predictions. The ensemble size is set to N = 16, except for width 32768 where
N = 4.
We compare the results from finite-width networks to computing the mean and standard deviation in
closed for using 4.1. Using the Neural Tangents library (Novak et al., 2019) we obtained the NTK
for the architecture described in the previous paragraph (1 hidden layer with erf non-linearity).
We found that the matrix inversion required in our equations results in numerical errors. Hence, we
make the modification Θ(X, X) J Θ(X, X) + 1e-3 ∙ I.
20
Under review as a conference paper at ICLR 2022
Figure 5: Comparing results of finite-width networks to closed form equations derived in Theorem
4.1. In the NTK parameterization, as width → ∞, the structure of the variances collapse and resem-
ble the infinite-width closed-form results. We believe this is due to infinite-width networks under
the NTK regime not being able to learn features (Yang & Hu, 2020). Supporting this hypothesis,
we observe that networks parameterized by the Maximal Parameterization of Yang & Hu (2020)
maintain the desired uncertainty structure as the width of the networks grows larger.
Figure 5 presents our results. As the width of the networks grow larger, the shape of the uncertainties
becomes more similar to our closed-form equations (i.e. the variances become very small). While
we do not have a rigorous explanation for why finite-width networks exhibit intuitively more de-
sirable behaviors, we present below a strong hypothesis backed by empirical evidence. We believe
rigorously answering this question is an incredibly interesting avenue for future work.
Hypothesis: Infinite-width networks in the NTK parameterization/regime do not learn data-
dependent features (Yang & Hu, 2020). Furthermore, as can be seen in the equations of Theorem
4.1, the variances depend the function values and features (kernel, C matrix, etc.) at initialization.
Yang & Hu (2020) present a different approach for parameterizing infinite-width networks called
the “Maximal Parameterization”, which enables inifinite-width networks to learn data-dependent
features. We perform the same experiment as above, by replacing the NTK-parameterized networks
with Maximal Parameterizations. Figure 5 presents our empirical results for network widths from
32 to 32768. Excitingly, we observe that with Maximial Parameterization, even our widest
networks recover the intuitively desired form of uncertainty described in section 4.2! The so-
lutions of these networks also appear much more accurate, particularly on the right hand side of the
plot where with the stepped structure; each step appears to be approximately 0.1 in width, which is
the action of the policy being evaluated.
21
Under review as a conference paper at ICLR 2022
(a) Q-function generative process: the graphical model
above represents the induced distribution over Q-
functions when conditioning on a particular policy
evaluation algorithm, policy, offline dataset, and Q-
function network architecture.
(b) An example of an interesting extension
H Statistical Model
An interesting question posed by reviewers of our work was “[W]hatever formal reasoning system
we’d like to use, what is the ideal answer, given access to arbitrary computational resources, so that
approximations are unnecessary? I.e., how do we quantify our uncertainty about the MDP and value
function before seeing data, and how do we quantify it after seeing data?”
It is important to begin by clarifying what is the mathematical object we are trying to obtain un-
certainties over. In this work, we do not quantify uncertainties about any aspects of the MDP itself
(although this is an interesting question which comes up in model-based methods as well as other
settings such as Meta-Learning (Yu et al., 2019; Ghasemipour et al., 2019)). Our goal in this work
is to directly estimate Qn (s, a) = r(s, a) + Y ∙ Es，〜MDp,a，〜∏ [Q(s0, a0)], for a 〜π(s), and obtain
uncertainties about Qπ(s, a).
Let Q(s, a) be a predictor S×A→ R that needs to be evaluated on - and hopefully generalize well
to - (s, a) / D (D being the offline dataset). When We choose to represent Q(s, a) using neural
networks, Gaussian Processes, or K-nearest-neighbours, we are not just making approximations for
computational reasons, but are actually choosing a function class which we believe will generalize
well to unseen (s, a).
One practical example of learning Q-functions is to use Fitted Q iteration (Fonteneau et al., 2013)
on the provided data using gradient descent with a particular neural network architecutre. Due to
the random weight initialization, this procedure induces a distribution on the Q-functions which is
captured by the probabilistic graphical model (PGM) in Figure 6a. In other words, by conditionining
on the policy, data, architecture, and policy evaluation algorithm, we are imposing a belief over Q-
functions. Note that this is essentially the same justification as using ensembles in supervised deep
learning, where ensembles are state-of-the-art for accuracy and calibration (Ovadia et al., 2019). For
the sake of theoretical analysis (Section 4), we studied this belief distribution under the infinite-width
NTK network setting, in which case the distribution over Q-functions is a Gaussian Process.
The focus of this work is to ask the question: “Under this imposed belief, what should the policy
update be?”. Our proposed answer is to optimize the policy with respect to the lower-confidence
bound of our beliefs: In an actor-critic setup, the policy optimization objective takes a form
like maxπ Ed(s) [Q(s, π(s))], where d(s) is some distribution over states (e.g. initial state dis-
tribution, or the states in the offline dataset D, etc.). Thus, our proposed policy objective takes
the form maxπ LCB Ed(s) [Q(s, π(s))] , and for practical reasons, in MSG we convert this to
maxπ Ed(s) LCB Q(s, π(s))	(which is a lower-bound of the first).
The graphical model in Figure 6a also highlights an example of interesting future directions: Con-
sider an offline RL setup where we keep track of the various policies generating the data, and their
Q-functions. Then, by imposing a prior on the architecture we can first infer a posterior distribu-
tion over architectures, then learn the Q-function of a new policy under the posterior architecture
distribution.
22
Under review as a conference paper at ICLR 2022
task_name=antmaze-large-play-v0,algorithm=msg,ensemble_size=64,critic_network_hidden_sizes=[25
6, 256, 256l,num_q_repr_pretrain_iters=200000,behavior_reqularization_tvpe=v1,use_double_q=False
task_name=antmaze-medium-diverse-vO,algorithm=msg,ensemble_size=64,critic_network_hidden_sizes
=f256, 256. 256l,num_q_repr_pretrain_iters=200000,behavior_regularization_tvpe=v1,use_double_q=Fal
task_name=antmaze-medium-play-v0,algorithm=msg,ensemble_size=64,critic_network_hidden_sizes=
1256, 256, 256l,num_q_repr_pretrain_iters=200000.behavior_regularization_tvpe=v1,use_double_q=Fals
Figure 7: Results on the six antmaze domains. Each color represents the mean and standard devia-
tion of results across 5 seeds for a particular hyperparameter setting (β ∈ -4, -8 and α ∈ 0, 0.1).
As can be seen, our results are quite robust across a wide range of hyperparameter values.
I Practical Hyperparameter Tuning Advice
In reporting our results, we preferred to also report the hyperparameter values that we used, which
may given an impression of significant hyperparameter tuning. We emphasize that our results are
generally robust across a range of hyperparameter values. As an example, in Figure 7 we present
results on the six antmaze domains. Each color represents the mean and standard deviation of results
across 5 seeds for a particular hyperparameter setting (β ∈ -4, -8 and α ∈ 0, 0.1). As can be seen,
our results are quite robust across a wide range of hyperparameter values.
Here, we include practical advice on hyperparameter tuning when faced with a new domain. For a
given new domain, we would first use low β values: β ∈ {-4, -8}. If β = -4 is clearly better
than β = -8, then we would guess that in this new domain high pessimism may not be necessary
and explore the use of β ∈ {0, -1, -2}. For the α hyperparameter, we found that {0, 0.1, 0.5, 1.0}
is a wide enough range to explore. If α = 1.0 was clearly better than the other values, then this
would indicate to us that maybe the offline dataset is narrow (lacks diversity, e.g. imitation learning
datasets) and then we could increase the value of α. Generally, we would prefer to lower β before
attempting to increase α, but we never tried β < -8.
23