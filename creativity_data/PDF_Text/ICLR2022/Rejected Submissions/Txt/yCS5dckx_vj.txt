Under review as a conference paper at ICLR 2022
Towards Demystifying Representation Learn-
ing with Non-contrastive Self-supervision
Anonymous authors
Paper under double-blind review
Ab stract
Non-contrastive methods of self-supervised learning (such as BYOL and Sim-
Siam) learn representations by minimizing the distance between two views of the
same image. These approaches have achieved remarkable performance in prac-
tice, but it is not well understood how the representation is learned based on the
augmentation process. Tian et al. (2021) explained why the representation does
not collapse to zero and proposed DirectPred that sets the predictor directly. In
our work, we analyze a generalized version of DirectPred, called DirectSet(α).
We show that in a simple linear network, DirectSet(α) provably learns a desirable
projection matrix and also reduces the sample complexity on downstream tasks.
Our analysis suggests that weight decay acts as an implicit threshold that discard
the features with high variance under augmentation, and keep the features with
low variance. Inspired by our theory, we simplify DirectPred by removing the ex-
pensive eigen-decomposition step. On CIFAR-10, CIFAR-100, STL-10 and Im-
ageNet, DirectCopy, our simpler and more computationally efficient algorithm,
rivals or even outperforms DirectPred.
1	Introduction
Self-supervised learning recently emerges as a promising direction to learn representations without
manual labels. While contrastive learning (Oord et al., 2018; Tian et al., 2019; Bachman et al.,
2019; He et al., 2020; Chen et al., 2020a) minimizes the distance of representation between pos-
itive pairs, and maximizes such distances between negative pairs, recently, non-contrastive self-
supervised learning (abbreviated as nc-SSL) is able to learn nontrivial representation with only
positive pairs, using an extra predictor and a stop-gradient operation. Furthermore, the learned
representation shows comparable (or even better) performance for downstream tasks (e.g., image
classification) (Grill et al., 2020; Chen & He, 2020). This brings about two fundamental ques-
tions: (1) why the learned representation does not collapse to trivial (i.e., constant) solutions, and
(2) without negative pairs, what representation nc-SSL learns from the training and how the learned
representation reduces the sample complexity in downstream tasks.
While many theoretical results on contrastive SSL (Arora et al., 2019; Lee et al., 2020; Tosh et al.,
2020; Wen & Li, 2021) do exist, similar study on nc-SSL has been very rare. As one of the first
work towards this direction, Tian et al. (2021) show that while the global optimum of the non-
contrastive loss is indeed a trivial one, following gradient direction in nc-SSL, one can find a local
optimum that admits a nontrivial representation. Based on their theoretical findings on gradient-
based methods, they proposed a new approach, DirectPred, that directly sets the predictor using the
eigen-decomposition of the correlation matrix of input before the predictor, rather than updating it
with gradient methods. As a method for nc-SSL, DirectPred shows comparable or better perfor-
mance in multiple datasets, including CIFAR-10 (Krizhevsky et al., 2009), STL-10 (Coates et al.,
2011) and ImageNet (Deng et al., 2009), compared to BYOL (Grill et al., 2020) and SimSiam (Chen
& He, 2020) that optimize the predictor using gradient descent.
While Tian et al. (2021) address the first question, i.e., why the learned representation does not
collapse, they do not address the second question, i.e., what representation is learned in nc-SSL and
how the learned representation is related to the data distribution and augmentation process and in
turn whether it reduces the sample complexity in downstream tasks.
1
Under review as a conference paper at ICLR 2022
Main Contributions. In this paper, we make a first attempt towards the second question, by study-
ing a family of algorithms named DirectSet(α), in which the DirectPred algorithm proposed by Tian
et al. (2021) is a special case with α = 1/2. Our contribution is two-folds:
First, we perform a theoretical analysis on DirectSet(α) with linear networks. Our analysis shows
that there exists an implicit threshold, determined by weight decay parameter η, that governs which
features are learned and which are discarded. More specifically, the threshold is applied to the vari-
ance of the feature across different data augmentations (or “views”) of the same instance: nuisance
features (features with high variances under augmentation) are discarded, while invariant features
(i.e., with low variances) are kept. We further make a formal statement on the sample complexity
of the learning process and performance guarantees of the downstream tasks, in the linear setting
similar to Tian et al. (2021). To our knowledge, this is the first sample complexity result in nc-SSL.
Second, we show that DirectCopy, a special case of DirectSet(α) when α = 1, performs comparably
with (or even outperforms) DirectPred in downstream tasks in CIFAR-10, CIFAR-100, STL-10
and ImageNet. In DirectCopy, the predictor can be set without the expensive eigen-decomposition
operation, which makes DirectCopy much simpler and more efficient than DirectPred.
Related works. In nc-SSL, different techniques are proposed to avoid collapsing. BYOL and Sim-
Siam use an extra predictor and stop gradient operation. Beyond these, BatchNorm (including its
variants (Richemond et al., 2020)), de-correlation (Zbontar et al., 2021; Bardes et al., 2021; Hua
et al., 2021), whitening (Ermolov et al., 2021), centering (Caron et al., 2021), and online clus-
tering (Caron et al., 2020) are all effective ways to enforce implicit contrastive constraints among
samples for collapsing prevention. We study BYOL and SimSiam as representative nc-SSL methods.
Organization. The paper is organized as follows. Section 2-3 introduce DirectSet(α), prove it
learns a projection matrix onto the invariant features, and the learned representation reduces sample
complexity in downstream tasks. Section 4 demonstrates that DirectCopy achieves comparable or
even better performance than the original DirectPred algorithm in various datasets, and Section 5
shows ablation experiments. Finally, limitation and future works are discussed in Section 6-7.
2	Preliminaries
2.1	Notations
We use Id to denote the d× d identity matrix and simply write I when the dimension is clear. For any
linear subspace S in Rd, we use PS ∈ Rd×d to denote the projection matrix on S. More precisely,
the projection matrix PS equals UU>, where the columns of U constitute a set of orthonormal bases
for subspace S. We use N(μ, Σ) to denote the Gaussian distribution with mean μ and covariance Σ.
We use ∣∣∙k to denote spectral norm for a matrix, or '2 norm for a vector and use |卜|恨 to denote
Frobenius norm for a matrix. For a real symmetric matrix A ∈ Rd×d whose eigen-decomposition is
Pd=ι λi%u>, We use |A| to denote Pid=1 ∣λi∣ %u>. If A is also positive semi-definite, We use Aa
to denote Pid=1 λiαuiui> for any positive α ∈ R.
2.2	DIRECTSET(α) AND DIRECTCOPY
In nc-SSL, recent methods as BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020) employ a
dual pair of Siamese netWorks (Bromley et al., 1994): one side is a composition ofan online netWork
(including a projector) and a predictor netWork, the other side is a target netWork (see Figure 1 for
a simple example). The target netWork has the same architecture as the online netWork, but has
potentially different Weights. Given an input x, tWo augmented vieWs x1, x2 are generated, and the
netWork is trained to match the representation of x1 (through the online netWork and the predictor
netWork) and the representation of x2 (through the target netWork). More precisely, suppose the
online netWork and the target netWork are tWo mappings fθ , fθa : Rd 7→ Rh and the predictor
netWork is a mapping gθp : Rh 7→ Rh, the netWork is trained to minimize the folloWing loss:
L(θ,θp,θa):= 2Ex1,x2L∣gθpfθ(X1))∣∣-StOPGrad(∣fθa⅛ι).
In BYOL and SimSiam, the online netWork and the target netWork are trained by running gradient
methods on L. The target netWork is not trained by gradient methods; instead, it is directly set With
2
Under review as a conference paper at ICLR 2022
Gradient
methods
BYOL:
Gradient
methods
DirectSet(α):
Fa
可=E +",
with F = EXl fθ Ql) fθ QI)T
T I2 loss
Sθp(fθ QI))
fθ(^l)	fθa(x2)
Figure 1: Problem Setup. Comparison between BYOL and DireCtSet(α) on a linear network.
the weights in the online network (Chen & He, 2020) or an exponential moving average (EMA) of
the online network (Grill et al., 2020; He et al., 2020; Chen et 1 2020b).
The DireCtSet(α) algorithm, as shown in Figure 1, direCtly sets the prediCtor based on the Correlation
matrix F of the prediCtor inputs:
Fα
Wp = kFαk + eI,
where F = Ex1fθ(x1)fθ(x1)>. In praCtiCe, F is estimated by a moving average over batChes. That
is, F = μF + (1 - μ)Em [fθ(xι)fθ(xι) > ], where EB is the expectation over one batch.
In the original DirectPred proposed by Tian et al. (2021), a is fixed at 1/2. To compute F 1/2, one
needs to first compute the eigen-decomposition of F, and then taking the root of each eigenvalue.
This step of eigen-decomposition can be expensive especially when the representation dimension
h is high. To avoid the eigen-decomposition step, we propose DirectCopy (α = 1), in which the
predictor Wp is a direct copy of the F (with normalization and regularization)* 1. As we shall see,
DirectCopy enjoys both theoretical guarantees and strong empirical performance.
3	THEORETICAL ANALYSIS OF DIRECTSET(α)
Deep linear networks have been widely used as a tractable theoretical model for studying nonconvex
loss landscapes (Kawaguchi, 2016; Du & Hu, 2019; Laurent & Brecht, 2018) and nonlinear learning
dynamics (Saxe et al., 2013; 2019; Lampinen & Ganguli, 2018; Arora et al., 2018). However, most
of them are for supervised learning setting. Tian et al. (2021) analyzed nc-SSL on a linear network,
but did not analyze their proposed approach DirectPred. Here, we analyze the representation learn-
ing process of DirectSet(α) on a minimal setting where the online network fθ is a single linear layer.
We also verify DirectSet(α) works for practical nonlinear deep models and realistic datasets.
3.1	Setup
In this subsection, we define the network model, data distribution and simplify DirectSet(α) algo-
rithm for our theoretical analysis. We consider the following network model (see Figure 1),
Assumption 1 (Linear network model). The online, predictor and target network are all single-layer
linear network without bias, with weight matrices denoted as W, Wp , Wa ∈ Rd×d respectively.
For the data distribution, we assume the input space is a direct sum of a invariant feature subspace
and a nuisance feature subspace. Specifically, we assume
Assumption 2 (Data distribution). The input x is sampled from N(0, Id), and its augmented view
x1, x2 are independently sampled from N(x, σ2PB), where B is a (d - r)-dimensional subspace.
We denote S as the orthogonal subspace of B in Rd.
1
1Computing the spectral norm of F is much faster than computing the eigen-decomposition of F, because
the former only needs the top eigen-vector of F. Table 4 shows that the spectral norm can also be replaced by
Frobenius norm or no normalization, and similar performance can be achieved.
3
Under review as a conference paper at ICLR 2022
In this simple data distribution, subspace S corresponds to the features that are invariant to augmen-
tations and its orthogonal subspace B is the nuisance subspace which the augmentation changes. We
will prove that DirectSet(α) can learn the projection matrix onto S subspace. Note in the previous
work (Tian et al., 2021), they assumed the covariance of the augmentation distribution to be σ2I and
did not study what representation is learned.
For the convenience of analysis, we consider a simplified version of DirectSet(α). We compute the
loss function without normalizing the two representations, so the population loss is
L(W,Wa,Wp) := 2 Eχ1,χ2 kWpWxι - StoPGrad (W0x2)『,	⑴
and the empirical loss is
1 n	2
L(W,Wp,Wa) := 2n XIWpWxIi)- StoPGrad(Wax2i)) J],	⑵
i=1
where x(i)’s are indePendently samPled from N(0, I), and augmented views x(1i) and x(2i) are inde-
Pendently samPled from N (x(i), σ2PB). To train our model, first, we initialize W as δI with δ a
Positive real number. We run gradient flow or gradient descent on online network W with weight
decay η, and set the the target network Wa = W. For clarity of Presentation, when training on the
PoPulation loss, we set Wp as (WExxx>W>)α = (WW>)α instead of (WEx1x1x1>W>)α as in
practice; when training on the empirical loss, We set Wp as (Wn Pη=i χ(i)[x(i)]>W>)α. Here, We
set the Predictor regularization = 0 and its influence will be studied in Section 5.
In the folloWing, DirectSet(α) is shoWn to recover the projection matrix PS With polynomial number
of samples. Furthermore, given that the learned matrix is close to PS , the sample complexity on
doWnstream tasks is reduced.
3.2	Gradient Flow on Population Loss
In this section, We shoW that DirectSet(α) running on the population loss With infinitesimal learning
rate and η Weight decay can learn the projection matrix onto the invariant feature subspace S.
Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-
tion 2, respectively. Suppose we initialize online network W as δI, and run DirectSet(α) on popu-
lation loss (see Eqn. 1) with infinitesimal step size and η weight decay. If we set the weight decay
coefficient η ∈ Qq+:2), 4)and initialization scale δ >
1-√τ-⅞)1/Qa)
, then W converges to
ι+√-⅞)1/Qa)
PS when time goes to infinity.
Theorem 1 shoWs that When the Weight decay is in certain range, and When the initialization is
large enough, the online netWork can converge to the desired projection matrix PS 2. In sequel,
We explain hoW the dynamics of W leads to a projection matrix and hoW the Weight decay and
initialization scale come into play. We leave the full proof in Appendix B.1. We also consider the
setting When Wp is set as (WEx1x1x1>W>)a in Appendix B.4 and extend the result to deep linear
netWorks in Appendix C.
Due to the identity initialization, We can ensure that W is alWays a real symmetric matrix and is
simultaneously diagonalizable With PB. We can then analyze the evolution of each eigenvalue in W
separately. Under our assumptions, it turns out that all the eigenvalues Whose eigenvectors lie in the
B subspace share the same value λB, and all the eigenvalues in the S subspace share the value λS
as shoWn in the folloWing time dynamics:
λB =	Ab	h—(1 +	σ2) ∣Λb∣4a	+	|Ab ∣2a	- “，λS	= λs	[-∣λs∣4a	+	∣λs∣2a - η]	.	(3)
Next, We shoW λB converges to zero and λS converges to a positive number, Which immediately
implies that W converges to some scaling of PS .
2Note that Theorem 1 also holds with negative initialization δ < 一 (1-√1-4η ) K ，, in which case W
converges to - (1+√1-4η ) " ' Ps . Our other results can be extended to negative δ in a similar way.
4
Under review as a conference paper at ICLR 2022
Figure 2: Left: With appropriate weight decay, λp always converge to zero; λs converges to zero when it's
initialized in the bad basin and converges to positive λ+ when it's initialized in the good basin. Middle: The
evolvement of the eigenvalues of F when it's trained by DirectCopy with e = 0.2 on STL-10. With weight
decay η = 0.0004 (bottom), the eigen-spectrum at epoch 95 has sharp drop; while the drop is much milder
when η = 0 (top). Right: Similar phenomenon on CIFAR-10 with = 0.3.
Similar as the analysis in Tian et al. (2021), when η >4(J .2), We know λ B < 0 for any Xb > 0
and λB = 0 is a stable stationary point, as illustrated in Figure 2 (top, left). Therefore, as long as
η >《(J 仃2), XB must converge to zero. When 0 < η < 4, there are three non-negative solutions
to Xs = 0, which are 0, X- = (1-√1-4η) ")and X； = (1+√4-4η) " ，. As illustrated in
Figure 2 (bottom, left), if initialization δ > XS- (good basin), XS converges to a positive value XS+ ;
if 0 < δ < XS- (bad basin), XS converges to zero.
Thresholding role of weight decay in feature learning: While Tian et al. (2021) shows why nc-
SSL does not collapse, one key question is how nc-SSL learns useful features and how the method
determines which feature is learned. Now it is clear: the weight decay factor η makes a call on what
features should be learned. Nuisance features subject to significant change under data augmentation
has larger variance σ2 and 4(4；.2)< η, the eigenspace corresponds to this feature goes to zero;
on the other hand, invariant features that are robust to data augmentation has much smaller σ2 and
4(i+-2)> η and these features are kept. In our above analysis, B subspace corresponds to the
nuisance features and collapses to zero; S subspace corresponds to the invariant features (whose
variance was assumed as zero for simplicity) and is kept after training.
Figure 2 (middle and right) shows the spectrum ofF (which is the correlation matrix of the predictor
inputs) when the network is trained by DirectSet(1) with and without weight decay η on STL10
and CIFAR10: when η = 0, the eigen-spectrum of F in later epochs does not have a sharp drop
compared with the case of η = 0.0004. This means that the nuisance features are not significantly
suppressed when η = 0.
Therefore, it is crucially important to choose weight decay appropriately: a too small η may not be
sufficient to suppress the nuisance features; a too large η can also collapse the invariant features. As
shown in Section 5, both cases lead to worse downstream performance.
3.3	Gradient Descent on Empirical Loss
In this section, we then proceed to prove that DirectCopy (one special case of DirectSet(α) with
α = 1) successfully learns the projection matrix given polynomial number of samples.
Theorem 2.	Suppose network architecture and data distribution are as defined in Assumption 1
and Assumption 2, respectively. Suppose we initialize online network as δI , and run DirectCopy
on empirical loss (see Eqn. 2) with γ step size and η weight decay. Suppose the noise scale σ2
is a positive constant, the weight decay coefficient η ∈ f 41+4+-/4), 1+4+σ/4) and the initialization
5
Under review as a conference paper at ICLR 2022
scale δ is a constant at least 1/√2. Choose the step size Y as a small enough constant. For any
accuracy ^ > 0, given n ≥ poly (d, 1∕^) number of samples, with probability at least 0.99 there
exists t = O(log(1∕e)) such that (here Wt is the online network weights at the t-th step):
ft- 1+√ √1-4ηPS	≤
The proof proceeds by first proving that gradient descent on the population loss converges in linear
rate and then couples the gradient descent dynamics on empirical loss and that on population loss.
See the detailed proof in Appendix B.2.
3.4 Sample Complexity on Downstream Tasks
In this section, we show that the learned representations can indeed reduce the sample complexity
on the downstream tasks. We consider the following data distribution for the down-stream task:
Assumption 3 (Downstream data distribution). Each input x(i) is sampled from N (0, Id) and its
label y(i) = (x(i), w*〉+ ξ(i), where w* is the ground truth vector with unit '2 norm and ξ(i) is
independently sampled from N(0,β2). We assume the ground truth w* lies on an r-dimensional
subspace S and we denote the projection matrix on subspace S simply as P.
In practice, usually the semantically relevant features (S subspace here) are invariant to augmenta-
tions and the nuisance features (orthogonal subspace of S) have high variance under augmentations.
Therefore, by previous analysis, we expect DirectSet(α) to learn the projection matrix P.
Suppose {(x(i), y(i))}n=i are n training samples. Each input x(i) is transformed by a matrix P^ ∈
Rd×d (for example the learned online network W) to get its representation Px⑺.The regularized
loss is then defined as L(W) = ɪ Pn=IKPx(i)，w - y(i) ∣∣ + P I∣wk2. In the below theorem,
We show that when ∣∣P - P∣∣ is small, the above ridge regression can recover the ground truth w*
given only O(r) number of samples.
Theorem 3.	Suppose the downstream data distribution is as defined in Assumption 3. Suppose
11P — P∣∣ ≤ ^ with ^ < 1. Choose the regularizer coefficient P = ^1∕3. For any Z < 1/2, given
n ≥ O(r + log(1∕Z)) number ofsamples, with probability at least 1 — Z, the training loss minimizer
W satisfies
∣∣Pw - w* ∣∣ ≤ o (1/3 + β√r + Pog(1/Z)! .
In the above theorem, when n is at least O (β (r+lθg(1/Z))) , we have ∣∣Pw - w*∣∣ ≤ O(^1/3).
Note that if we directly estimate W without transforming the inputs by P, we need Ω(d) number
of samples to ensure that kw - w*∣ ≤ o⑴(Wainwright, 2019). The proof of Theorem 3 follows
from bounding the difference between PW and w* by matrix concentration inequalities and matrix
perturbation bounds. The full proof is in Appendix B.3.
4	Empirical Performance of DirectCopy
In the previous analysis, we show DirectSet(α), and in particular DirectCopy (DirectSet(α) with
α = 1), could recover the input feature structure with polynomial samples and make the down-
stream task more sample efficient in a simple linear setting. Compared with the original DirectPred
(DirectSet(α) with α = 1/2), DirectCopy is a simpler and computationally more efficient algorithm
since it directly set the predictor as the correlation matrix F , without the eigen-decomposition step.
By our analysis in Theorem 1, DirectCopy also learns the projection matrix PS with larger scale 3
3Recall that in Theorem 1 under DireCtSet(α), online matrix W converges to (1+√1-4η ) K ‘ Ps . So
with a larger α, the scalar in front ofPS becomes larger.
6
Under review as a conference paper at ICLR 2022
compared with DirectPred, which suggests that the invariant features learned by DirectCopy are
stronger and more distinguishable. Next, we show that DirectCopy is on par with (or even outper-
forms) the original DirectPred in various datasets, when coupling with deep nonlinear models on
real datasets.
4.1 RESULTS ON STL-10, CIFAR- 1 0 AND CIFAR- 1 00
We use ResNet-18 (He et al., 2016) as the backbone network, a two-layer nonlinear MLP as the
projector, and a linear predictor. Unless specified otherwise, SGD is used as the optimizer with
weight decay η = 0.0004. To evaluate the quality of the pre-trained representations, we follow
the linear evaluation protocol. Each setting is repeated 5 times to compute the mean and standard
deviation. The accuracy is reported as “mean±std”. Unless explicitly specified, we use learning rate
γ = 0.01, regularization = 0.2 on STL-10; γ = 0.02, = 0.3 on CIFAR-10 and γ = 0.03, = 0.3
on CIFAR-100. See more detailed experiment settings in Appendix A.
Num of epochs		
100	300	500
STL-10
DirectCopy	77.83±0.56	82.01±0.28	82.95±0.29
DirectPred	77.86±0.16	78.77±0.97	78.86±1.15
DirectPred (freq=5)	77.54±0.11	79.90±0.66	80.28±0.62
SGD baseline	75.06±0.52	75.25±0.74	75.25±0.74
CIFAR-10
DirectCopy	84.02±0.37	89.17±0.12	89.62±0.10
DirectPred	85.21±0.23	88.88±0.15	89.52±0.04
DirectPred (freq=5)	84.93±0.29	88.83±0.10	89.56±0.13
SGD baseline	84.49±0.20	88.57±O15	89.33±O27
CIFAR-100
		I epochs I 100
ImageNet	
DirectCopy	68.8
DirectPred	68.5
SGD Baseline	68.6
Table 2: ImageNet Top-1 accu-
racy of DirectCopy, DirectPred
and BYOL baseline.
DirectCopy	55.40±0.19	61.06±0.14	62.23±0.06
DirectPred	56.60±0.27	61.65±0.18	62.68±0.35
DirectPred (freq=5)	56.43±0.21	62.01±0.22	63.15±0.27
SGD baseline	54.94±0.50	60.88±0.59	61.42±0.89
Table 1: STL-10/CIFAR-10/CIFAR-100 Top-1 accuracy of DirectCopy.
The numbers for DirectPred, DirectPred (freq=5) and SGD baseline on
STL-10/CIFAR-10 are obtained from Tian et al. (2021).
STL-10: We evaluate the quality of the learned representation after each epoch, and report the
best accuracy in the first 100/300/500 epochs in Table 1. DirectCopy achieves substantially better
performance than the original DirectPred and SGD baseline, especially when trained with longer
epochs. DirectPred (freq=5) means the predictor is set by DirectPred every 5 batchs, and is trained
with gradient updates in other batchs, which outperforms DirectPred in later epochs, but is still much
worse than DirectCopy. The SGD baseline is obtained by training the linear predictor using SGD.
CIFAR-10/100: For CIFAR-10, DirectCopy is slighly worse than DirectPred at epoch 100, but
catches up and gets even better performance in epoch 300 and 500 (Table 1). For CIFAR-100, at
earlier epochs, the performance of DirectCopy is not as good as DirectPred, but the gap gradually
diminishes in later epochs. Both DirectCopy and DirectPred outperfoms the SGD baseline. Direct-
Pred (freq=5) achieves even better performance, but at the cost of a more complicated algorithm.
4.2 Results on ImageNet
Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-layer MLP as
the projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs. See
more detailed experiment settings in Appendix A.
For fairness, we compare DirectCopy to the gradient-based baseline which uses the same-sized linear
predictor as ours. As shown in Table 2, at 100-epoch, this baseline achieves 68.6 top-1 accuracy,
which is already significantly higher than BYOL with two-layer predictors reported in the literature
(e.g., Chen & He (2020) reports 66.5 top-1 under 100-epoch training). DirectCopy using normalized
F accumulated with EMA μ = 0.99 on the correlation matrix, regularization parameter e = 0.01
achieves 68.8 under the same setting, better than this strong baseline. In contrast, DirectPred (Tian
et al., 2021) achieves 68.5, slightly lower than the linear baseline.
7
Under review as a conference paper at ICLR 2022
Figure 3:	Left: Change of λS when predictor regularization e increases. Right: Eigenvalues of F when
trained by DireCtCoPy under different e on CIFAR-10 for 100 epochs.
5 ABLATION Study
In this section, We study the influence of predictor regularization e, normalization method, weight
decay and degree α on the performance of DirectCopy.
Predictor regularization: Table 3 shows that when the predictor regularization e increases, the
performance of DirectCopy on STL-10 and CIFAR-10 improves at first and then deteriorates. On
STL-10, DirectCopy with e = 1 completely fails. On CIfAr-10, although DirectCopy with e = 1
achieved reasonable performance at epoch 300, it’s still much worse than e = 0.3.
To better understand the role of e, we analyze the simple linear setting as in Section 3.1 while
setting Wp = WW> + eI. Recall that λB is the eigenvalue of W in B subspace and λS is that in S
subspace. When the weight decay is appropriate, λB still converges to zero. On the other hand, the
dynamics for λS is as follows:
λ S = -λs (λS + e - 1 -√1-4η) (λS + e - 1±√≡).
Increasing e shifts the two positive stationary points λS- , λS+ towards zero. As illustrated in Figure 3
(left), as e increases, when λS+ is still positive, the good attraction basin expands, which means λS
can converge to a positive value from a smaller initialization; when λS+ shifts to zero, λS converges
to zero regardless the initialization size. See the full analysis in Appendix D.
Intuitively, a reasonable e can alleviate representation collapse, but a too large e also encourages
representation collapse. As shown in Figure 3 (right), when e increases from zero, more eigenvalues
of F becomes large; but when e exceeds 0.3, eigenvalues of F begin to collapse.
Normalization on F : In our experiments, we have been normalizing F by its spectral norm
before adding the regularization: Wp = F/ kF k + eI . It turns out that we can also normalize
F by its Frobenius norm or simply skip the normalization step. In Table 4, we see comparable
performance from DirectCopy with Frobenius normalization or no normalization, especially when
trained longer.
Number of epochs	
100	I	I	300
STL-10
	=0	76.57±0.66	81.19±0.39
=	0.1	78.05±0.14	81.60±0.15
=	0.2	77.83±0.56	82.01±0.28
	=1	31.10±0.80	31.10±0.80
CIFAR-10
	=0	80.53±1.14	86.07±0.71
=	0.1	83.97±0.25	88.58±0.11
=	0.3	84.02±0.37	89.17±0.12
	=1	57.38±11.62	83.15±4.24
Table 3: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying regularization .
	I	Number of epochs I	100	I	300	
	STL-10			
Spectral Frobenius None	77.83±0.56 77.71±0.18 77.81±0.20	82.01±0.28 82.06±0.28 82.00±1.24
	CIFAR-10			
Spectral Frobenius None	84.02±0.37 84.33±0.25 81.76±0.34	89.17±0.12 89.62±0.14 89.21±0.17
Table 4: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with F matrix normalized by spectral
norm/Frobenius norm or no normalization.
Weight decay: Table 5 shows that when weight decay η increases, the performance of DirectCopy
improves at first and then deteriorates. This fits our analysis on simple linear networks. Basically,
when the weight decay η increases, it can suppress the nuisance features more effectively, but a too
large weight decay also collapses the useful features.
8
Under review as a conference paper at ICLR 2022
Number of epochs
100 I 300
η=0
η = 0.0004
η= 0.001
η=0.01
STL-10
71.94±0.93
77.83±0.56
77.65±0.16
58.12±0.94
CIFAR-10
η=0
η = 0.0004
η= 0.001
η = 0.01
79.15±0.08
84.02±0.37
83.91±0.33
65.31±1.19
78.53±0.40
82.01±0.28
80.28±0.16
58.53±0.76
85.35±0.31
89.17±0.12
87.75±0.16
65.63±1.30
		I	Number of epochs	
		I 100	300
STL-10			
α	=2	76.80±0.22	80.90±0.18
α	=1	77.83±0.56	82.01±0.28
α=	1/2	77.82±0.37	77.83±0.37
α=	1/4	76.82±0.36	76.82±0.36
	CIFAR-10				
α	=2	82.96±0.56	88.60±0.11
α	=1	84.02±0.37	89.17±0.12
α=	1/2	84.88±0.21	88.32±0.57
α=	1/4	84.78±0.21	87.82±0.32
Table 6: STL-10/CIFAR-10 Top-1 accuracy of
DirectSet(α) with varying degree α.
Table 5: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying weight decay.
Predictor degree: We compare DirectCopy against DirectSet(α) with α = 2, 1/2, 1/4. Table 6
shows that DirectCopy outperforms other algorithms on STL-10. On CIFAR-10, DirectCopy is
slightly worse at epoch 100, but catches up in later epochs.
6 Beyond Linear Models: Limitations and Discussion
Evolverrient of 日genvalues of F (DIrectCopy)	Evolverrient of 曰genvalues of F (BYOL, linear predictor) Evoivement of 曰genvalues of F (BYOL, nonlinear predictor)
XIJ5E°s8n">UB6a⅛ 6oη
xμπeE JO SQn∙>UQ6it6oη
O 20	40	60	80 IOO 120	O 20	40	60	80 IOO 120	O 20	40	60	80 IOO 120
Sorted eigenvalue Index	Sorted eigenvalue Index	Sorted eigenvalue Index
Figure 4:	Eigenvalues of F when trained by DirectCopy, BYOL With linear predictor and BYOL With two-
layer nonlinear predictor on CIFAR-10 for different epochs. Top-1 accuracy at 500 epoch is 89.62 for Direct-
Copy, 88.83 for BYOL with linear predictor and 90.25 for BYOL with two-layer nonlinear predictor.
As a linear model used to study the behavior of nc-SSL, our model does not capture all of its
intriguing empirical phenomena. For example, we observed that the discarded nuisance features
gradually come back after training over longer epochs. Moreover, whether it comes back or not is
related to the downstream task performance. In Figure 4 on CIFAR-10 dataset, both DirectCopy
and BYOL with two-layer nonlinear predictor show this resurgence of nuisance features, as well as
strong performance, while BYOL with linear predictor does not seem to learn new features even
when trained longer, which might explain its worse performance.
One conjecture is that at the beginning of training, weight decay prioritize the invariant features (i.e.,
low variance under augmentation) over nuisance ones. The invariant features then grow, building
their own supporting low-level features. After that, the nuisance feature, which is also useful, are
gradually picked up in later stage. Since the low-level features are already trained through previous
steps of back-propagation, the nuisance features are encouraged to use them as the supporting fea-
tures, rather than creating their own. In contrast, if we train both the invariant and nuisance features
simultaneously, they will compete over the limited pool of low-level supporting features defined by
the capacity of the network, leading to worse learned representations. We believe understanding
these phenomena require analysis on the non-linear networks, and we leave it as future work.
7 Conclusion
In this paper, we have proved DirectSet(α) can learn the desirable projection matrix in a linear
network setting and reduce the sample complexity on down-stream tasks. Our analysis sheds light on
the crucial role of weight decay in nc-SSL, which discards the features that have high variance under
augmentations and keep the invariant features. Inspired by the analysis, we also designed a simpler
and more efficient algorithm DirectCopy, which achieves comparable or even better performance
than the original DirectPred (Tian et al., 2021) on various datasets.
We view our paper as an initial step towards demystifying the representation learning in nc-SSL.
Many mysteries lie beyond the explanation of the current theory and we leave them for future work.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In ICML. PMLR, 2018.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In ICLR, 2019.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019.
Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization
for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.
Jane Bromley,Isabelle Guyon, Yann LeCun, Eduard Sackinger, and RooPak Shah. Signature Verifi-
cation using a“ siamese” time delay neural network. NeurIPS, 1994.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
UnsuPervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020.
Mathilde Caron, HUgo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging ProPerties in self-suPervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simPle framework for
contrastive learning of visual rePresentations. arXiv preprint arXiv:2002.05709, 2020a.
Xinlei Chen and Kaiming He. ExPloring simPle siamese rePresentation learning. arXiv preprint
arXiv:2011.10566, 2020.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. ImProved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuPervised
feature learning. In International conference on artificial intelligence and statistics, 2011.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009.
Simon Du and Wei Hu. Width Provably matters in oPtimization for deeP linear neural networks. In
ICML, 2019.
Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-
supervised representation learning. In International Conference on Machine Learning, pp. 3015—
3024. PMLR, 2021.
Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimen-
sions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp.
761-770. ACM, 2015.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: Anew approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020.
Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature
decorrelation in self-supervised learning. ICCV, 2021.
Kenji Kawaguchi. Deep learning without poor local minima. NeurIPS, 2016.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In ICLR, 2018.
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In ICML,pp. 2902-2907. PMLR, 2018.
Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Pierre H. Richemond, Jean-Bastien Grill, Florent Altche, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. Byol works
even without batch statistics. arXiv, 2020.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., 2019.
Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM review, 19(4):634-662, 1977.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019.
Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. arXiv preprint arXiv:2008.10150, 2020.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021.
Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
arXiv:1708.03888, 2017.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and StePhane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. ICML, 2021.
11
Under review as a conference paper at ICLR 2022
A Detailed Experiment Setting
STL-10, CIFAR-10, CIFAR-100 : We use ResNet-18 (He et al., 2016) as the backbone network,
a two-layer nonlinear MLP (with batch normalization, ReLU activation, hidden layer width 512,
output width 128) as the projector, and a linear predictor. Unless specified otherwise, SGD is used
as the optimizer with momentum 0.9, weight decay η = 0.0004 and batch size 128. The EMA
parameter for the target network is set as 0.996 and the EMA parameter μ of the correlation matrix 户
is set as 0.5. Our code is adapted from Tian et al. (2021) 4, and we follow the same data augmentation
process.
To evaluate the quality of the pre-trained representations, we follow the linear evaluation protocol.
Each setting is repeated 5 times to compute the mean and standard deviation. The accuracy is
reported as “mean±std”. Unless explicitly specified, we use learning rate γ = 0.01, regularization
= 0.2 on STL-10; γ = 0.02, = 0.3 on CIFAR-10 and γ = 0.03, = 0.3 on CIFAR-100.
ImageNet : Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-
layer MLP (with batch normalization, ReLU, hidden layer width 4096, output width 256) as the
projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs, with a
batch size 4096. The learning rate is 7.2, which is linearly scaled from the base learning rate 0.45
at batch size 256. Other setups such as weight decay (η = 1e-6), target EMA (scheduled from 0.99
to 1), augmentation recipe (color jitters, blur, etc.), and linear evaluation protocol are the same as
BYOL.
B Proofs of S ingle-layer Linear Networks
B.1 Gradient Flow on Population Loss
In this section, we give the proof of Theorem 1, which shows that DirectSet(α) running on the
population loss with infinitesimal learning rate and η weight decay can learn the projection matrix
onto subspace S.
Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-
tion 2, respectively. Suppose we initialize online network W as δI, and run DirectSet(α) on popu-
lation loss (see Eqn. 1) with infinitesimal step size and η weight decay. If we set the weight decay
coefficient η ∈ Q(J=2), 1) and initialization scale δ > (1-√1-4η)	, then W converges to
ι+√-η)1/Qa)
PS when time goes to infinity.
As we already mentioned in the main text, Theorem 1 is proved by analyzing each eigenvalue of W
separately. We show that the eigenvalues in the B subspace converge to zero, and the eigenvalues in
the S subspace converge to the same positive number, which immediately implies that W converges
to a scaling of the projection matrix PS.
Proof of Theorem 1. We can compute the gradient in terms of W as follows,
▽L(W) =Eχ1,χ2 Wp> (WpWxI- WaX2) x>
= Wp> (WpWEχιx1x> — WaEχι,X2x2x>) ∙
Note that the two augmented views x1 , x2 are sampled by first sampling input x from N (0, Id), and
then independently sampling x1, x2 from N (x, σ2PB). Therefore, we know Ex1x1x1> = I + σ2PB
and Ex1,x2x2x1> = I. Recall that we run gradient flow on W with weight decay η, so the dynamics
on W is as follows:
W =Wp> (-WpW (I + σ2PB) + Wa) — ηW,
where the first term comes from the gradient and the second term is due to weight decay.
Since W is initialized as δI, and Wa = W, Wp = (WW>)α, so we know initially W, Wp, Wa, Iand
PB are all simultaneously diagonalizable, which then implies W is simultaneously diagonalizable
4Their open source code is at https://github.com/facebookresearch/luckmatters/tree/main/ssl
12
Under review as a conference paper at ICLR 2022
with W . This argument can continue to show that at any time point, W, Wp, Wa, I and PB are
all simultaneously diagonalizable. Since W is always a real symmetric matrix, we have Wp =
(WW>)α = |W∣2α . The dynamics on W can then be written as
W = |W ∣2α (- |W ∣2α W (I + σ2PB) + W) - ηW
=W (-(I + σ2PB) |W∣4α + |W∣2α - η).
Let the eigenvalue decomposition of W be Pd=1 λiuiu>, with span({ud-r+1,…，ud}) equals to
subspace B. We can separately analyze the dynamics of each λi. Furthermore, We know λι,…，λr
have the same value λs and λd-r+ι, ∙∙∙ ,λd have the same value Xb . Next, we separately show that
λB converge to zero and λS converges to a positive value.
Dynamics for λB : We can write down the dynamics for λB as follows:
λB = Xb h-(1 + σ2) |Xb ∣4α + |Xb ∣2α - η]
Similar as the analysis in Tian et al. (2021), when η >4^+仃2), we know λB < 0 for any Xb > 0
and Xb = 0 is a critical point. This means, as long as η >4.)σ2), Xb must converge to zero.
Dynamics for XS: We can write down the dynamics for XS as follows:
λs = λs h- |Xs|4a + |Xs|2a - η].
When 0 < η < ɪ, we know λS > 0 for λSα ∈ (1一√1-4n, 1+√1-4n) and λS < 0 for λ2sα ∈
(1+√1-4η, ∞) . Furthermore, we know λS = 0 when λ2Sα = 1+√1-4η. Therefore, as long as
0 < η < 4 and initialization δ2ɑ > 1-√1-4η, we know λSα converges to 1+√1-4η.
11	√141/(2α)
Overall, we know when4^+仃2)< η < 4 and δ > ( 一ʌ^- η)	, we have λB converge to
zero and λs converge to (1+√1-4η ) "，. That is, matrix W converges to (1+√1-4η ) "，PS.
B.2 Gradient Descent on Empirical Loss
In this section, we prove that DirectCopy successfully learns the projection matrix given polynomial
number of samples.
Theorem 2. Suppose network architecture and data distribution are as defined in Assumption 1
and Assumption 2, respectively. Suppose we initialize online network as δI, and run DirectCopy
on empirical loss (see Eqn. 2) with γ step size and η weight decay. Suppose the noise scale σ2
is a positive constant, the weight decay coefficient η ∈ (；++息,1++σ/4) and the initialization
scale δ is a constant at least 1/√2. Choose the step size Y as a small enough constant. For any
accuracy ^ > 0, given n ≥ poly (d, 1∕^) number of samples, with probability at least 0.99 there
exists t = O(log(1∕e)) such that (here Wt is the online network weights at the t-th step):
ft-J1+√2-4ηPS ≤
When running gradient descent on the empirical loss, the eigenspace of Wft can shift and become no
longer simultaneously diagonalizable with PB. Sowe cannot independently analyze each eigenvalue
of Wt as before, which brings significant challenge into the analysis. Instead of directly analyzing
the dynamics of Wft , we first show that the gradient descent iterates Wt on the population loss
converges to PS in linear rate, and then show that Wt stays close to Wt within certain iterations.
13
Under review as a conference paper at ICLR 2022
Lemma 1. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population
loss L. Given any accuracy ^ > 0, for any t ≥ Clog(1∕^), we have
W r1 + √1 - 4η D / ʌ
Wt -V ——^2---------PS ≤ ^,
where C is a positive constant.
The proof of Lemma 1 is similar as the gradient flow analysis in Section 3.2. Next, we show that the
gradient descent trajectory on the empirical loss stays close to the gradient descent trajectory on the
population loss within O(log(1∕^)) iterations.
Lemma 2. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population
loss and let Wt be the gradient descent iterations on the empirical loss. For any accuracy ^ > 0,
given n ≥ poly (d, 1∕^) number of samples, with probability at least 0.99 ,for any t ≤ C log(1∕^),
we have
ft- Wt Ii ≤ ^,
where the constant C comes from Lemma 1.
Then the proof of Theorem 2 directly follows from Lemma 1 and Lemma 2.
Proof of Theorem 2. According to Lemma 1, We know given any accuracy ^0, for t = C log(1∕^),
We have	____________
Wt- r 1+，； - 4ηPS ≤ ^0,
where C is a positive constant.
According to Lemma 2, we know given n ≥ poly(d, 1∕^0) number of samples, with probability at
least 0.99,
Ift- Wt∣∣ ≤ ^0.
Therefore, we have
Wt- J1 + √;	-	4ηPS	≤	Wt	-	J1 +	√;	- 4ηPS	+ Ift- Wt II	≤ 2^0.
Replacing ^0 by ^∕2 finishes the proof.	□
In section B.2.1, we give the proof of Lemma 1 and Lemma 2. Proofs of some technical lemmas are
left in Appendix B.5.
B.2.1	Proofs for Lemma 1 and Lemma 2
Proof of Lemma 1. Similar as in Theorem 1, we can show that at any step t, Wt is simultaneously
diagonalizable with Wa,t, Wp,t , I and PB . The update on Wt is as follows,
Wt+1 = Wt + γWt (-(I + σ2PB)W4 + Wt2 - η).
Let the eigenvalue decomposition of Wt be Pid=1 λ%,tuiu>, with span({ud-r+ι,…,ud}) equals
to subspace B. We can separately analyze the dynamics of each λi,t . Furthermore, we know
λι,t, ∙∙∙ , λr,t have the same value λstj and λd-r+ι,t, •…,λd,t have the same value λB,t. Next,
we separately show that λB,t converge to zero and λS,t converges to a positive value in linear rate.
Dynamics of λB,t : We show that
0 ≤ λB,t ≤ (1 -γC1)tδ
for any step size γ ≤ C2, where C1, C2 are two positive constants.
14
Under review as a conference paper at ICLR 2022
According to the gradient update, we have
λB,t+1 = λB,t + γλB,t -(1 + σ2)λ4B,t + λ2B,t - η .
We only need to prove that for any λB,t ∈ [0, δ], we have
-(1 + σ2)λ4B,t + λ2B,t - η = -Θ(1).
This is true since η ∈ (g+^, 1++σ/4) and σ2, δ are two positive constants.
Dynamics of λS : We show that
0≤ λ2S,t
1 + √Γ-⅞
≤ (1 - γC3)t δ2 -
1 + √Γ-⅞
—
2
2
for any step size Y ≤ C4 , where C3, C4 are two positive constants.
There are two cases to consider: when the initialization scale δ2 ∈ [1/2, 1+以-4n],we prove
0 ≤ 1 + √T-⅞
— λ2B,t ≤ (1 — γC3)t
1 + √Γ-⅞
- δ2 ;
2
2
when the initialization scale δ2 > 1+'1-4n,we prove
0 ≤ λ2B,t —
1 + √T-⅞
≤ (1-γC3)t δ2 -
1 + √T-⅞
2
2
We focus on the second case; the proof for the first case is similar.
According to the gradient update, we have
λS,t+1 =λS,t + γλS,t [ —λS,t + λS,t — η]
=λS,t — γλS,t
—
—
1 — ʌ/i — 4η
1 + √Γ-⅞
—
We only need to show that λS,t λ2S,t
ι-√r-⅞
=Θ(1) for any λS,t ∈ [ 1+√2-4η, δ]. This is
2
2
true because η ∈ (立+高,I+%/；) and σ2, δ are two positive constants.
Overall, we know that there exists constant step size such that after t = O (log (1 /^)) steps, we have
0 ≤ λB,t ≤ ^ and λs,t — J
1 + √Γ-⅞
J ʌ
≤ ^.
2
This then implies,
Wt —
1 + √Γ-⅞
Proof of Lemma 2. We know the update on Wft is
Wt+1 - ft =Yft ( -fp,tft (1 X Xf) [x1i)]>) + fa,t (n X XIi) [x2i)]>
i=1
i=1
— γηWft ,
2
PS ≤ ^

and the update on Wt is
Wt+1 — Wt = YWpt (—Wp,tWt (I + O2Pb) + W0,t) — γηWt.
Next, we bound Wft+1 — Wft — (Wt+1 — Wt) . According to Lemma 3, we know with probability
at least 1 — O(d2) exp (—Ω(^02n∕d2)),
n
-X Xf) [xιi)]> — I — O1Pb ,
n i=1
n
n x XIi) [χii)]> -1,
i=1
1n
1 X χ(i)[χ(i)]> — I ≤ ^0.
n i=1
15
Under review as a conference paper at ICLR 2022
Recall that we set Wfa,t	=	Wft	and set	Wa,t	as	Wt ,	so we have	Wfa,t	-	Wa,t	=	Wft	- Wt	.
Also since We set Wpt = Wt n PZi x(i)[x(i)]>) W> and set Wp,t = WtW>, We have
∣∣Wp,t - Wp,t∣∣ = O (IWt- Wt∣ + ^0) since ∣∣Wt∣∣ = O(1).
Combing the above bounds and recall γ is a constant, We have
∣∣fft+ι — Wt-(Wt+1 - Wt)∣∣ = O (∣∣Wt - Wt∣∣ + ^0).
Therefore,
∣∣ Wt- Wt∣∣ ≤ Ct^0,
where Ci is a constant larger than 1. So for any t ≤ C log(1∕^), we have
∣∣Wt- Wt ∣∣ ≤ CC log(1∕*⅜ ≤ (i∕^)c2^o,
for some positive constant C?. Choosing ^0 = ^c2+1, we know as long as n ≥ poly(d, 1∕^), with
probability at least 0.99, for any t ≤ Clog(1∕^), we have
∣∣Wt- Wt∣∣ ≤ ^.
B.3 Sample Complexity on Down-stream Tasks
In this section, we give a proof for Theorem 3, which shows that the learned representations can
indeed reduce sample complexity in downstream tasks.
Theorem 3. Suppose the downstream data distribution is as defined in Assumption 3. Suppose
∣∣pP 一 P∣∣ ≤ ^ with ^ < 1. Choose the regularizer coefficient P = ^1/3. For any Z < 1/2, given
n ≥ O(r + log(1∕ζ)) number of samples, with probability at least 1 - ζ, the training loss minimizer
W satisfies
∣∣Pw - w* ∣∣ ≤ o (i/3 + β√r + √g(1∕C! .
Suppose {(x(i), y(i))}in=1are n training samples in the downstream task, let X ∈ Rn×d be the data
matrix with its i-th row equal to x(i). Denote y ∈ Rn as the label vector with its i-th entry as y(i).
Each input x(i) is transformed by a matrix P ∈ Rd×d to get its representation Pxw. The regularized
loss can be written as
L(w) := 2n ∣∣χppw - y∣∣ +ρ Iiwk2.
This is the ridge regression problem on inputs {(Px(i), y(i))}n=ι, and the unique global minimizer
W has the following close form:
W= (- P >X > XP + PI)	- P >X >y	(4)
nn
With the above closed form of W, the proof of Theorem 3 follows by bounding the difference be-
tween PW and w* by matrix concentration inequalities and matrix perturbation bounds. Some proofs
of technical lemmas are left in Appendix B.5.
Proof of Theorem 3. Denoting P as P + ∆, we know ∣∣Δ∣∣f ≤ ^ by assumption. We can also write
y as XW* + ξ where ξ ∈ Rn is the noise vector with its i-th entry equal to ξ(i). Then, we can divide
W into two terms,
W= ( - P>X >XP + PI)	- P>X >y
nn
=(-P>X>XP + PI)	-P>X> (Xw* + ξ) + (-P>X>xP + PI)	-∆>X> (Xw* + ξ)
nn	nn
Let’s first give an upper bound for the second term that comes from the error term ∆> .
16
Under review as a conference paper at ICLR 2022
Upper bounding 11( 1 P>X>XP + PI) -1 ɪ∆>X> (Xw* + ξ)
We first bound the norm of
n∆>X>Xw*. According to Lemma 5, We know with probability at least 1 - exp(-Ω(n)),
Il√n∆>X>∣∣ ≤ O(^). Since Xw* is a standard Gaussian vector with dimension n, according
to Lemma 8, with probability at least 1 - exp(-Ω(n)), ∣∣ √^ Xw*∣∣ ≤ O(1). Therefore, we have
Il 1 ∆>x>Xw*∣∣ ≤ O(^).	n
Then we bound the norm of § ∆>X> ξ.
with probability at least 1 - exp(-Ω(n)), ∣∣ 十ξ∣∣
know with probability at least 1 - Z/3, ∣∣∆>X>g∣∣
II n δ>x >ξ∣ ≤ O
β^√iog(i∕Z)
√
According to Lemma 8, we know
≤ O(β). According to Lemma 6, we
≤ O (^plog(1∕Z)) . Therefore, we have
Since λmin (1 P>X>XP + PI) ≥ ρ, we have || (1 P>X>XP + PI)	≤ P. Combining with
above bound on ∣∣ 1 ∆>X> (Xw* + ξ) ∣,we know with probability at least 1 - exp(-Ω(n)) - Z/3,
||( 1P>X>XP + PI) 11∆>X> (Xw* + ξ) ≤ O (I + β'p√1∕Q!.
Analyzing (§P>X>XP + PI) 11P>X> (Xw* + ξ) We can write §P>X>XP as
n1 P>X>XP + E, where
E = 1∆>X >XP + - P >X >X ∆ + 1∆>X >X ∆.
nnn
Let’s first bound the spectral norm of XP. Since P is a projection matrix on an r-dimensional
subspace S, we can write P as UU>, where U ∈ Rd×r has columns as an orthonormal basis of
subspace S. According to Lemma 4, we know with probability at least 1 - exp(-Ω(n)),
ω(1) ≤ σmin (√nXU) ≤ σmax ^√nXU) ≤ O(1)∙
SinCe kU k ≤ 1, WehaVe Il √ XPII = l∣ √ XUU >∣l ≤ O(1).
According to Lemma 5, we know with probability at least 1 - exp(-Ω(n)),
X∆
≤ O(^).
F
So overall, we know ∣∣E∣∣ ≤ ∣∣E∣∣f ≤ O(^).
Then, we can write
+ F.
1P >x >xp +
n
1 P >X >XP + PI)
n
According to the perturbation bound for matrix inverse (Lemma 11), we have ∣∣Fk ≤ O(P). Then,
we have
1P>X>XP, + PI)	1P>X> (Xw* + ξ)=
nn
1P > X >XP + PI∖	1P >X >Xw*
nn
+ F1P >X >Xw*
n
+ ( (1P >X >XP + PI)	+ F) 1P >X >ξ
17
Under review as a conference paper at ICLR 2022
We first show that the first term is close to w*. Let the eigenvalue decomposition of 1P>X>XP
be V ΣV >, where V ’s columns are an orthonormal basis for subspace S. Here Σ ∈ Rr×r is the
diagonal matrix that contains all the eigenvalues of 1P>X>XP. According to Lemma 4, We
know that with probability at least 1 - exp(-Ω(n)), all the non-zero eigenvalues of 1P>X>XP
are Θ(1).
Then, it’s not hard to show that
1	-11
-P>X>XP + PI	-P>X>XP - P
nn
This immediately implies that
1	-1 1
-P>X>XP + PI	-P>X>Xw* - w*
nn
≤ O(ρ).
≤ O(P)
Next, we bound the norm of the second term F1P>X>Xw*. Similar as before, we know
with probability at least 1 - exp(-Ω(n)), ∣∣ 表 Xw* ∣∣ ≤ O ⑴ and ∣∣ √ P > X >∣∣ ≤ O⑴.There-
fore, we have
F - P >X >Xw*
n
≤ kFk
上P>X>∣∣ ∣∣ɪXw*
≤ OG
Finally, let,s bound the third term ((1 P>X>XP + PI)-1 + F) 1P>X>ξ. We first bound the
norm of 1P>X>ξ. with probability at least 1 - exp(-Ω(n)), we know ∣∣ξk ≤ 2β√n. Therefore,
we know ∣∣ 1P>X>ξ∣∣ ≤ O(β∕√n) ∣∣P>X>ξ∣∣, where g = ξ/ ∣∣ξ∣ . According to Lemma 7,
with probability at least 1 - Z/3, we have ∣∣P>X>g∣∣ ≤ √r + O(Plog(1/Z)). Overall,
with probability at least 1 - exp(-Ω(n)) - Z/3,
1P>X>ξ ≤ O (√rβ + √⅞7Z)β
n	∖	√n
It’s not hard to verify that for any vector v ∈	Rd in the subspace S, we have
∣∣(( 1PTXTXP+ρI )-1+F)v∣∣≤
O(∣v∣). Since 1P>X>ξ lies on subspace S, we have
nP >x >xp+PI )-1+F) nP >x >ξ
≤O
√rβ + Sog(I/ζ)β
√n
Combining the above analysis and taking a union bound over all the events, we know
with probability at least 1 - exp(-Ω(n)) - 2Z/3,
kw - w*k
c( q/q ^ q β^pi⅞w q √rβ + pι0g(i7θ β
o1p + P+ P2 +	ρ√n	+	√n
Suppose n ≥ O(log(1/Z)) and setting P = ^1/3, we further have with probability at least 1 - Z,
kw - w*∣ =o j/3+铲/3ρo≡)+√+ρo≡)β!
nn
≤o (1/3 + β √r + Pog(1/z)! ,
where the last inequality assumes ^ < 1.
18
Under review as a conference paper at ICLR 2022
We can also bound UPW — w*U as follows,
—w* U = ∣∣pp W — Pw + Pw — Pw*
≤
≤
P w — PwU + kPw — Pw*k
P - PU kwk + kPkkw - w*k
≤^O(1+*/3+β √r +*gα∕Z) u O (,1/3+β √r +*gα∕Q
nn
≤O (^1∕3 + β √r +*g(1/Z)
∖	√n

B.4 ANALYSIS WITH Wp := (WEx1x1x1>W>)α
In this section, we prove that DirectSet(α) can also learn the projection matrix when we set Wp :=
(WEx1x1x1>W>)α. For the network architecture and data distribution, we follow exactly the same
setting as in Section 3.2. Therefore, we know Wp := (WEx1x1x1>W>)α = (W(I + σ2PB)W >)α.
Theorem 4. Suppose network architecture and data distribution are as defined in Assumption 1 and
Assumption 2, respectively. Suppose we initialize online network W as δI, and run DirectPred(α)
on population loss (see Eqn. 1) with infinitesimal step size andη weight decay. Suppose we set Wa =
W and Wp = (WEχιxιx>W>)α. Assuming the weight decay coefficient η ∈ Q(i+σ2)i+2α, 1)
and initialization scale δ > (1-√1-4η) " ，, we know W converges to (1+√1-4η) " ，PS
when time goes to infinity.
The only difference from Theorem 4 is that now the initialization δ is only required to be larger than
4q+σ2)i+2α. The proof is almost the same as in Theorem 1.
Proof of Theorem 4. Similar as in the proof of Theorem 1, we can write the dynamics on W is as
follows:
W=W> (—Wp W (I + σ2PB) + Wa)- nW
=∣W2(I + ©2Pb)∣α (—∣W2(I + σ2PB)∣α W(I + σ2PB) + W) — nW
=W (—(I + σ2PB)1+2α |W∣4α + |W∣2α - n).
Dynamics for λB : We can write down the dynamics for λB as follows:
λB = Ab h-(1 + σ2)1+2α ∣Λb ∣4α + ∣Λb ∣2α — n]
When n > 4q+σ2)i+2α, We know AB < 0 for any Ab > 0 and Ab = 0 is a critical point. This
means, as long as n > 4q+σ2)i+2α, AB must converge to zero.
Dynamics for AS: The dynamics is same as when setting Wp = (WW> )α,
As = AS h- |As∣4α + |As ∣2α — n].
so when 0 < n < 11 and initialization δ2ɑ > 1-√1-4η, we know ASa converges to 1+√1-4η.
Overall, we know when 1q+σ2)i+2ɑ < n < 1 and δ > (1-√1-4η) " ), we have AB converge to
zero and AS converge to (1+√1-4η ) 葭).That is, matrix W converges to (1+√1-4η ) 葭)PS.

19
Under review as a conference paper at ICLR 2022
B.5 Technical Lemmas
Lemma 3. Suppose {x(i) , x(1i) , x(2i) }in=1 are sampled as decribed in Section 3. Suppose n ≥
O(d∕^2), with probability at least 1 一 O(d2)exp (—Ω(^2n∕d2)) , we have
n	nn
1 Xχ1i)[χ1i)]> — I-σ2PB , 1 Xχ1i)[χ2i)]> — I , 1 Xx(i)[x(i)]> — I ≤ ^
Proof of Lemma 3. For each xf), We can write it as x(i) + z(i) where x(i)〜 N (0, I) and z(i)〜
N(0, σ2PB). So we have
nn
一 ^X x1i) [x1i)]> = 一 ^X (χ(i) [x(i)]> + z(i) [z(i)]> + x(i) [z(i)]> + z(i) [x(i)]>).
n i=1	n i=1
According to Lemma 9, we know as long as n ≥ O(d∕^2), with probability at least 1 —
exp(一Ω(^2n)),
1 n
1 X χ(i)[χ(i)]> — I ≤ ^
n i=1
Similarly, with probability at least 1 — exp(—Ω(^2n)),
n
1 X z(i)[z(i)]> — σ2PB ≤ ^.
n i=1
Next we bound ∣∣ n1 Pn=1 x(i)[z(i)]>|| . We know each entry in matrix 1 pn=1 x(i)[z(i)]> is the
average of n zero-mean O(1)-subexponential independent random variables. Therefore, according
to the Bernstein,s inequality, for any fixed entry (k, l), with probability at least 1 — exp (—^2n∕d2),
1 XX⑴[z(i)]>	≤ ^∕d.
n i=1	k,l
Taking a union bound over all the entries, we know with probability at least 1 — d2 exp (—^2n∕d2),
n
1 X Xc⅜(i)]>
i=1
n
n X χ%i)]>
i=1
J ʌ
≤ €.
F
≤
The same analysis also applies to 11 Pn=1 z(i) [χ(i)]> ∣∣ . Combing all the bounds, we know with
probability at least 1 — O(d2)exp —Ω(e2n∕d2)),
n
-X XIi) [x1i)]> — I — σ2PB
n i=1
≤ 4^.
Similarly, we can prove that with probability at least 1 — O(d2)exp (—Ω(e2n∕d2)),
n
1 X XIi) [χ2i)]> -1
n i=1
≤ 4^.
Changing ^ to ^0/4 finishes the proof.	□
Lemma 4. Let X ∈ Rn×d be a standard Gaussian matrix, and let U ∈ Rd×r be a matrix with
orthonormal columns. Suppose n ≥ 2r, with probability at least 1 — exp(—Ω(n)), we know
Ω(1) ≤ λmin (1 U>X>XU) ≤ λmaχ (JU>X>Xu) ≤ O(1).
20
Under review as a conference paper at ICLR 2022
Proof of Lemma 4. Since U has orthonormal columns, we know X U is a n × r matrix with each
entry independently sampled from N (0, 1). According to Lemma 9, we know when n ≥ 2r, with
probability at least 1 - exp(-Ω(n)),
C(1) ≤ σmin (√nXU) ≤ σmax ^√nXU) ≤ O(I) ∙
This immediately implies that
Ω(1) ≤ λmin (JU>X>XU) ≤ λmaχ (^U>X>Xu) ≤ O(1).
Lemma 5. Let ∆ be a d X d matrix with Frobenius norm ^, and let X be a n X d standard Gaussian
matrix. We know with probability at least 1 — exp(-Ω(n)),
X∆
≤ O(^).
F
Proof of Lemma 5. Let the singular value decomposition of ∆ be UΣV > , where U, V have or-
thonormal columns and Σ is a diagonal matrix with diagonals equal to singular values σi's. Since
〔1△kF =" WeknoW Pd=I σ2 =容.
Since U is an orthonormal matrix, we know X := XU is still an n X d standard Gaussian matrix.
Next, we bound the Frobenius norm of X := XΣ. It’s not hard to verify that all the entries in X are
independent Gaussian variables and Xij 〜N(0, σj'). According to the Bernstein S inequality for
sum of independent and sub-exponential random variables, we have for every t > 0,
Pr £	Xj- n^2 ≥ t
i∈[n],j∈[d]
≤ 2 exp
"i" , maxj\ )
Since Pj=I σj = k∆kF = ^2, we know maxj∈[d] σj ≤ ^2. We also have Pj∈[d] σj- ≤
(Pj∈[d] σj) = ^4. Therefore, we have
Pr
E	Xj-n^2≥ t
≤ 2 exp
i∈[n],j∈[d]
-c min
Replacing t by n^2, we concluded that with probability at least 1 -
Furthermore, since V > = 1, we have
exp(-Ω(n)),∣∣X∣∣^
≤ 2n^2.
X △
1
十X V >
n
≤
F
F
1
KeFkk OQ
Lemma 6. Let △> bea d×d matrix with Frebenius norm ^ and let X > be a d×n Standard Gaussian
matrix. Let ξξ be a unit vector with dimension n. We know with probability at least 1 — Z/3,
∣∣∆>X>f∣∣ ≤ oG^pi0g(17Z)).
Proof of Lemma 6. Let the sigular value decomposition of △> be UΣV>. We know X>g is a d-
dimensional standard Gaussian vector. Further, we know V>X>g is also a d-dimensional standard
Gaussian vector. So ΣV> X>ξ has independent Gaussian entries with its i-th entry distributed
as N(0, σi2 ). According to the Bernstein’s inequality for sum of independent and sub-exponential
random variables, we have for every t > 0,
Pr h∣∣∣∑V>X>ξ∣∣2 - ^2∣ ≥ t] ≤ 2exp
t2 t
-C min (^4 ,^2
21
Under review as a conference paper at ICLR 2022
Choosing t as O(^2 log(1∕Z)), We know with probability at least 1 - Z/3, We have
∣∣∑v>x>f∣∣2 ≤ o (^2iog(i∕Z)).
Since kUk = 1, we further have
∣∣δ>x>划=∣∣uςv>x>f∣∣ ≤ kU∣ ∣∣∑v>x>f∣∣ ≤ o ^Piog(i∕ζ))
Lemma 7. Let P ∈ Rd×d be a projection matrix on a r-dimensional subspace, and let ξ be a unit
vector in Rd . Let X > be a d × n standard Gaussian matrix that is independent with P and ξ. With
probability at least 1 - ζ∕3, we have
∣∣p>x>f∣∣ ≤√r + o(Piog(i∕ζ)).
Proof of Lemma 7. Since P is a projection matrix on an r-dimensional subspace, we can write P
as UU>, where U ∈ Rd×r has orthonormal columns. We know U>X> is still a standard Gaussian
matrix with dimension r X n. Furthermore, U>X>g is an r-dimensional standard Gaussian vector.
According to Lemma 8, with probability at least 1 - ζ∕3, we have
∣∣u>x>划 ≤√r + o(Piog(i∕Z)).
Since kUk = 1, we further have
∣∣p>x>f∣∣ = ∣∣uu>x>f∣∣ ≤ kUI ∣∣u>x>可 ≤ √r + o(Piog(i∕Z)).
C Analysis of Deep Linear Networks
In this section, we extend the analysis in Section 3.2 to deep linear networks. We consider the same
data distribution as defined in Assumption 2. We consider the following network,
Assumption 4 (Deep linear network). The online network is an l-layer linear networks
WlWl-I •…Wi with each Wi ∈ Rd×d. The target network has the same architecture with weight
matrices Wa,lWa,l-ι •…Wa,i. For convenience, we denote W as WlWl-I •…Wi and denote Wa
as Wa,lWa,l-1 …Wa,1.
Training procedure: At the initialization, we initialize each Wi as δ1"∕d. Through the training,
we fix Wp as WW> α and fix each Wa,i as Wi . We run gradient flow on every Wi with weight
decay η. The population loss is
L({Wi},Wp, {Wa,i}) ：= 2 Eχ1,x2 k WpWlWl-I ∙∙∙ Wi xi - StOPGrad(W0,l Wa,l-ι …Wa,1x2)k2.
Theorem 5. Suppose the data distribution and network architecture satisfies Assumption 2 and As-
sumption 4, respectively. Suppose we train the network as described above. Assuming the weight
decay coefficient
η∈
2al(2αl+2l-2)1 + 1 -金
2al(2αl+2l-2)1+ 1 -吉
(4αl+2l-2)2+ i - 01l (i+σ2)1+ 1 - ⅛l ,	(4αl+2l-2)2+ 1 -吉
, and initialization scale δ ≥
(4al+2l-2)2α , we know W ConvergeS to CPS as time goes to infinity, where C is a positive number
within ((4αl+2l-2 )20 ,1) ∙
Similar as in the setting of single-layer linear networks, we prove Theorem 5 by analyzing the
dynamics of the eigenvalues of W. Note that with constant α, the upper/lower bounds for η and
scalar C in the Theorem are always constants no matter how large l is.
Proof of Theorem 5. For j ≥ i, we use Wj”] to denote WjWj-I •…Wi and for j < i have
W[j:i] = I. We use similar notations for Wa,[j:i] . For each Wi, we can compute its dynamics as
follows:
Wi = -(WpW[l：i+i])> (WpW (I + σ2PB )) (W[iTi])> + (WpWa,[l：i+i])> Wa (Wa,[iTi])> - η Wi .
22
Under review as a conference paper at ICLR 2022
It’s clear that through the training all Wi ’s remains the same and they are simultaneously diagonal-
izable with Wp,I and PB. We also have Wa = W and Wp = |W∣2α . Since We will ensure that W
is always positive semi-definite so Wp = |W∣2α = W2α = W2αl. So the dynamics for each Wi
can be simplified as follows:
Wi = -Wi4αl+2l-1(I + σ2Pβ) + Wi2αl+2l-1 - ηWi.
Let the eigenvalue decomposition of Wi be Pd=ι Viuiu>, with span({u√-r+ι, ∙∙∙ , Ud}) equals to
subspace B. We can separately analyze the dynamics of each %. Furthermore, we know νι,…,Vr
have the same value VS and νd-r+ι, ∙∙∙ ,νd have the same value VB. We can write down the dy-
namics for VS and VB as follows,
4αl+2l-1	2αl+2l-1
VS = - US	+ US	- ηνs,
4αl+2l-1	2	2αl+2l-1
VB = - Vb +	(1 + σ ) + Vb +	- ηVB.
Let λS be the eigenvalue of W corresponding to eigen-directions uι,…,ur, and let Xb be the
eigenvalue of W corresponding to eigen-directions ud-r+ι, ∙∙∙ ,ud. We know Xs = VS and Xb =
VBl . So we can write down the dynamics for λB as follows,
λB = IvB-1Vb =-短。1+31-2(1 + σ2) + «+”-科
=-lλ-α+3-2∕l(1 + σ2) + lλ-α+3-2∕l - lηλ-,
and similarly for XS we have
λS = -iλSα+3-2∕l + iλSα+3-2∕l - iηλs.
Dynamics for X- : We can write the dynamics on X- as follows,
X- = lX-g(X-),
where g(X-) := -X4-α+2-2∕l(1 + σ2) + X2-α+2-2∕l - η. We show that when η is large enough,
g(X-) is negative for any positive X- . We compute the maximum value of g(X-) for X- > 0. We
first compute the derivative of g as follows:
g'(λ-) = - (4α + 2 - 2/1)(1 + σ2)λ-α+1-2∕l + (2α + 2 - 2∕l)λ-α+1-2/1
=λ-α+1-2∕l (-Ra + 2 - 2/1)(1 + σ2)λ-α + (2α + 2 - 2/1)).
It’s clear that g0(XB ) > 0 for X2Bα ∈ (0,
2αl + 2l-2
2αl + 2l-2
(4αl+2l-2)(1+σ2)
) and g0(XB ) < 0 for X2Bα ∈
(4αl+2l-2)(1+σ2)
’	2al+2l-2
(4αl+2l-2)(1+σ2)
,+∞). Therefore, the maximum value of g(λ-) for positive λ- takes at λ-
J 2α and
g(λ-)
2a1 + 21 - 2 Y
(4a1 + 21 - 2)(1+ σ2))
2a1(2a1 + 21 - 2)1+1 -
2+ α-01l	2}	(	2a1 + 21 - 2
(1+ σ )+ V(4a1 + 21 - 2)(1+ σ2)
1
al
1+ α - Oi
-η
(4a1 + 21 - 2)2+1 -O (1 + σ2)1+ 1 -Oi	"
As long as η > -------2αl(20l+2l二2) + α ：； ι——, we know g(λ-) < 0 for any λ- > 0, which
(4αl+2l-2)2+ a-O (l+σ2)1+ O-Ol
further implies that X- < 0 for any X- > 0. So X- converges to zero.
(
—
Dynamics for XS : We can write down the dynamics on XS as follows,
λ s = 1λs h(λs),
where h(XS) = -X4Sα+2-2∕l + X2Sα+2-2∕l - η. We compute the derivative of h as follows:
h(λs) = λSα+1-2∕l (-(4a + 2 - 2∕1)λSα + (2a + 2 - 2/1)).
23
Under review as a conference paper at ICLR 2022
1	1
So h(λs) is increasing in (0, (4αl[2l-2) 2α) and is decreasing in ((Io⅛⅛⅛) 2° , ∞)∙ The maxi-
mum value of h for positive λs takes at λS = (∣θl+2l-2) 2a and We have
7	2αl(2αl + 2l - 2)1+1 -0ι
h(λS) =	W ”1I ι——η
(4αl + 2l - 2)2+a-a
As long as η < 2αl(2αl+2l-2[ + a「, we have h(λS) > 0. Furthermore, since h is increasing in
(4αl+2l-2)2+a - a
(0,λS) and is decreasing in (λS, ∞) and h(0),h(∞) < 0, we know there exists λ- ∈ (0,λS),λ+ ∈
(λS, ∞) such that h(λs) < 0	in	(0, λ-),	h(λs) > 0 in (λ-, λ+) and h(λs) < 0	in	(λ1, ∞).
Therefore, as long as δ ≥ λS	>	λ-, we	have λs converges to λ1. Since h(1) <	0,	we know
λ+ ∈ ((40+≡ ]2α, i).
Overall as long as η ∈
2al(2αl+2l-2)1+1 - al	2αl(2αl+2l-2)1+1 - al
(4αl+2l-2)2+1 -a1l (1+σ2)1+1 -⅛l ,	(4αl+2l-2)2+1 -0l
converges to CPS, where C is a positive number within ((2θl+2l-2) 2a , 1).
we know W
D Analysis of Predictor Regularization.
In this section, we study the influence of predictor regularization in a simple linear setting. In
particular, we consider the same setting as in Section 3.2 except that we set Wp := (WW>)α + I.
Theorem 6. In the setting of Theorem 1 except that we set Wp = (WW> )α + I. We have
•	when E ∈ [0, 1+√1-4η), as long as δ > (max (1-√1-4η — e, 0)) 2a , we have W Con-
1
verges to (1+√1-4η — E) 2a PS;
•	when E ≥ 1+vz1-4η, W always converges to zero.
Proof of Theorem 6. We can write the dynamics of W as follows,
W =Wp> (-WpW (I + σ2PB) + Wa) — ηW
=W (-(I + σ2PB) (|W∣2α + EI)2 + (|W∣2α + EI) - η).
Let the eigenvalue decomposition of W be Pd=ι λiuiu>, with span({ud-r+ι, ∙∙∙ , ud}) equals to
subspace B. We can separately analyze the dynamics of each λi. Furthermore, we know λι,…，λr
have the same value λs and λd-r+ι, ∙∙∙ ,λd have the same value Xb .
Dynamics for λB : We can write down the dynamics for λB as follows:
λB = Xb -(1 + σ2) (|Xb∣2α + E) + (|Xb ∣2α + E) - η
When η > ∣q+仃2), we still know λB < 0 for any Xb > 0 and Xb = 0 is a critical point. So Xb
converges to zero.
Dynamics for XS: We can write down the dynamics for XS as follows:
λS =λs - (∣λs∣2α + e) + (∣λs∣2α + e) - η
=-λs (∣λs ∣2α + E - 1 -√1-4η) (∣λs ∣2α + E - 1±√≡),
where the second inequality assumes 0 < η < ∣. We have
24
Under review as a conference paper at ICLR 2022
____ _______________________________ ɪ
•	when E ∈ [0, 1+√1-4η), as long as δ > (max (1-√1-4η - e, 0))2。, We have λs con-
verges tθ ( 1 + √1-4η - E) 2a > 0；
•	when E ≥ 1+^1-4η, λs always converges to zero.

E	Technical Tools
E.1 Norm of Random Vectors
The following lemma shows that a standard Gaussian vector with dimension n has '2 norm concen-
trated at √n.
Lemma 8 (Theorem 3.1.1 in Vershynin (2018)). Let X = (X1, X2, ∙∙∙ , Xn) ∈ Rn be a random
vector with each entry independently sampled from N (0, 1). Then
Pr[∣kxk - √n∣ ≥ t] ≤ 2exp(-t2∕C2),
where C is an absolute constant.
E.2 Singular Values of Gaussian Matrices
The following lemma shows a tall random Gaussian matrix is well-conditioned with high probability.
Lemma 9 (Corollary 5.35 in Vershynin (2010)). Let A be an N × n matrix whose entries are
independent standard normal random variables. Then for every t ≥ 0 with probability at least
1 - 2 exp(-t2∕2) one has
√N - √n - t ≤ Smin(A) ≤ SmaX(A) ≤ √N + √n + t
E.3 Perturbation B ound for Matrix Pseudo-inverse
With a lowerbound on σmin(A), we can get bounds for the perturbation of pseudo-inverse.
Lemma 10 (Theorem 3.4 in Stewart (1977)). Consider the perturbation of a matrix A ∈ Rm×n :
B = A + E. Assume that rank (A) = rank(B) = n, then
IIB t- Atll ≤√2∣W∣∣W∣∣kEk.
The following corollary is particularly useful for us.
Lemma 11 (Lemma G.8 in Ge et al. (2015)). Consider the perturbation of a matrix A ∈ Rm×n :
B = A + E where kEk ≤ σmin(A)∕2. Assume that rank(A) = rank(B) = n, then
IlBt - AtII ≤ 2√2 kEk ∕σmin(A)2.
25