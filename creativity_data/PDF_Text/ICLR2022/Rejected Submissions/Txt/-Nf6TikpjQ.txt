Under review as a conference paper at ICLR 2022
Multi-agent Performative Prediction: From
Global Stability and Optimality to Chaos
Anonymous authors
Paper under double-blind review
Ab stract
The recent framework of performative prediction (Perdomo et al., 2020) is aimed
at capturing settings where predictions influence the target/outcome they want to
predict. In this paper, we introduce a natural multi-agent version of this frame-
work, where multiple decision makers try to predict the same outcome. We show-
case that such competition can result in interesting phenomena by proving the
possibility of phase transitions from stability to instability and eventually chaos.
Specifically, we present settings of multi-agent performative prediction where un-
der sufficient conditions their dynamics lead to global stability and optimality. In
the opposite direction, when the agents are not sufficiently cautious in their learn-
ing/updates rates, we show that instability and in fact formal chaos is possible. We
complement our theoretical predictions with simulations showcasing the predic-
tive power of our results.
1	Introduction
Performative prediction (Perdomo et al., 2020) is a recently introduced framework that focuses on a
natural but largely unexplored element of supervised learning. In many practical cases the predictive
model can affect the very outcome that it is trying to predict. For example, predictions about which
part of a road network will have high congestion trigger responses from the drivers which affect the
resulting traffic realization leading to a shift of the target distribution. In such settings, (Perdomo
et al., 2020) explored conditions for the existence and approximate optimality of stable equilibria of
such processes.
One possible way to interpret the performative prediction setting is a single agent “game”, where the
predictive agent is playing a game against himself. An agent chooses the parameters of his model
as his actions but the predictive accuracy/cost of the model depends on his own past actions. Fixed
points of this process do not allow for profitable deviations. Once cast in this light, it becomes
self-evident that the restriction to a single predictive agent/model is arguably only the first step in
capturing more general phenomena where there is a closed loop between predictive models and their
environment. This motivates our central question: What is the interplay between stability, optimality
in cases where multiple predictive models operate in parallel to each other? Can the competition be-
tween multiple models lead to novel phenomena such as phase transitions from stability to instability
and chaos?
A natural setting to consider for example is market competition, where multiple hedge funds are
trying to simultaneously predict future prices, volatility of financial instruments. Of course as they
act upon their predictions they also move the prices of these commodities in a highly correlated
way. As more agents enter the market and the competition becomes increasingly fierce, is it possible
that at some point the market flips from quickly discovering accurate stable predictions reflecting
the underlying market fundamentals to self-induced unpredictability and chaos? When it comes to
performative prediction can too many cooks spoil the soup?
Our model Standard supervised learning consists of three components: a set of predictive mod-
els, a loss function, and a data distribution. The learner (agent) observes samples of the distribution,
and then decides a predictive model. When predictions are performative, the agent’s decision of a
predictive model influences the data distribution. Thus, instead of a fixed data distribution, a pre-
dictive prediction also has a distribution map: a mapping from the agent’s predictive models to
1
Under review as a conference paper at ICLR 2022
data distributions. We further propose multi-agent performative prediction which model the influ-
ence of multiple agents’ decisions on the data distribution, and ask whether these influence leads to
convergent or chaotic systems.
To understand the framework of multi-agent performative prediction, we study a natural regression
problem with multi-agent location-scale distribution maps (definition 2.2) where the agents’ influ-
ence is linear in the agents’ models. Specifically, the data consist of features and outcomes. For
each agent i ∈ [n], his influence on the distribution of outcome is his model weighted by a scalar
λi > 0. When n = 1, our multi-agent location-scale distribution map is a special case of location-
scale family in Miller et al. (2021). In the market example, each hedge fund company tries to predict
the price (outcome) based on macroeconomic data or other information (features). These influence
parameters λ1, . . . , λn can be seen as each hedge fund’s capital that can influence the distribution of
the future price.
Finally, we consider the agents do not know the dependency between their model and the data
distribution, and they can only improve their predictive models myopically and iteratively. In this
paper, the agents myopically use a reinforcement learning algorithm, exponentiated gradient for
linear regression (Kivinen & Warmuth, 1997), to improve their predictive models in rounds. We
study the long-term behavior, and ask if the system converge to the performative stable and optimal
point, or behave chaotically.
Our results We first study basic properties of our multi-agent preformative prediction with multi-
agent location-scale distribution map. We show 1) the existence of performative stable point in our
setting (proposition 3.1) and 2) the performative stability and performative optimality are equivalent
(proposition 3.2). This equivalence allows us to focus on the dynamical behavior of the system.
In section 4, we introduce learning dynamics to the multi-agent performative prediction, and study
their long-term behavior. We provide a threshold result which depends on the learning rates of
exponentiated gradient descent and the collective influence. Theorem 4.1 shows the dynamics of
exponential gradient descent converge to the performative stable and optimal point when the learn-
ing rate is small enough. Our convergence result in theorem 4.1 holds when the feature space is
multi-dimensional, and every learning agent can use different learning rates starting at arbitrarily
interior states. Our exact convergence result also works in the single-agent performative predic-
tion setting. Contrarily, previous convergent results in Perdomo et al. (2020); Miller et al. (2021);
Mendler-Dunner et al. (2020) only show that their dynamics converge to a small neighborhood of
the performative stable point.
On the other hand, section 4.2 shows the dynamics can have chaotic behavior if the collective in-
fluence Ln is large enough. Specifically, theorem 4.6 shows that even when the feature space is
in R2 these systems provably exhibit Li-Yorke chaos, when the collective influence Pi λi is large
enough. This implies that there exists an uncountable “scrambled” set so that given any two initial
conditions in the set, the liminf of the distance between these two dynamics is zero, but the lim-
sup of their distance is positive. (definition 2.4) The chaotic result in theorem 4.6 also holds for
the original single-agent performative prediction so long as the agent’s influence is large enough,
and, thus, complements previous performative prediction works on convergence behavior, which
primarily consider that the agent’s influence on the data distribution is sufficiently small. Moreover,
no matter how small the agents’ learning rates, theorem 4.6 shows that chaos is inevitable in some
performative predictions settings when the number of agents exceeds a carrying capacity. After that
the system becomes totally unpredictable with small perturbations exploding exponentially fast. 1
Finally, section 5 provides numerical examples of our dynamics and show convergent and chaotic
behavior. Additionally, through simulation, we demonstrate that our convergent and chaotic results
also hold when the agents can only access noisy estimation of the gradient and conduct stochastic
exponentiated gradient descent.
Related work Data distribution shift is not anew topic in ML, but earlier works focused primarily
on exogenous changes to the data generating distribution. Performativity is machine learning was
introduced by Perdomo et al. (2020). The original work and several follow-ups study the discrep-
ancy between performative stability and performative optimality and prove approximated conver-
1We highlight our revision in blue color.
2
Under review as a conference paper at ICLR 2022
gence of learning dynamics, e.g., stochastic gradient descent, or iterative empirical risk minimiza-
tion. (Mendler-Dunner et al., 2020; DrUsVyatskiy & Xiao, 2020; Izzo et al., 2021) Performativity of
prediction is also related to several applications: strategic classification (Hardt et al., 2016), retrain-
ing Bartlett (1992); Kuh et al. (1990).
Inspired by the instability of training algorithms in ML applications such as Generative Adversarial
Networks (GANs), there has been a lot of recent interest in understanding conditions (particularly
in multi-agent systems) where learning behavior may be non-equilibrating/unstable (Cheung & Tao,
2020; Balduzzi et al., 2020; Flokas et al., 2020; Andrade et al., 2021; Letcher, 2021; Giannou et al.,
2021). The (in)stability and performance of exponentiated gradient descent in particular (also re-
ferred to as Multiplicative Weights Updates) and other closely related dynamics has attracted a lot
of attention (Cohen et al., 2017; Bailey & Piliouras, 2018; Cheung, 2018; Panageas et al., 2019;
Cheung & Piliouras, 2020; Vadori et al., 2021). The technique of Li-Yorke chaos has recently found
applications across several different domains such as routing games, Cournot games and blockchain
protocols (Palaiopanos et al., 2017; Chotibut et al., 2020; Bielawski et al., 2021; Cheung et al., 2021;
Leonardos et al., 2021). To our knowledge, this is the first time where such formal chaotic results
are established in settings related to performative prediction and supervised learning more generally.
Another line of related works is learning in games. Specifically, our dynamics can be seen as special
cases multiplicative weight update (Hedge algorithms) on congestion games. Previous works on
Hedge algorithms only show exact convergence when the learning rate is decreasing Kleinberg et al.
(2009); Cohen et al. (2017), and, to our best knowledge, our results are the first that shows exact
convergence of Hedge algorithms with small constant learning rates. Our results also complement
the exact convergence result of the linear variant of multiplicative weight update by Palaiopanos
et al. (2017).
2	Preliminary
2.1	Multi-agent Performative Prediction
A multi-agent performative prediction comprises n agents deploying their predictive models
fθ1 , . . . , fθn with parameters θ1 , θ2, . . . , θn ∈ Θ that collectively influence the future data dis-
tribution. We formalize such dependency via a distribution map D(∙) which outputs a distribution
1
on the data, D(θ), given a models profile θ := (θ1, . . . , θn) ∈ Θn. A loss function `(z, θ0) mea-
sures the loss of a model fθ0 on a data point z ∈ Z, and the expected loss on a distribution D is
EZ〜D ['(z, θ0)]. For performative prediction, We further define the decoupled performative loss on a
distribution mapping D as
'(~, θ0) ：= EZ〜D(~)'(Z, θ0)
where θ0 ∈ Θ denotes a predictive model, while θ~ ∈ Θn denotes a deployed model profile.
Given the set of model Θ, the loss function `, and the distribution mapping D, each agent in a
multi-agent performative prediction (Θ, `, D) pursues minimal loss on the distribution that they
collectively induce. We consider two solution concepts performative optimality and performative
stability which generalizes the original ones in Perdomo et al. (2020).
Definition 2.1. Given (Θ,', D), a models profile ~*∈ Θn is Performatively optimal if the total loss
is minimized,
铲 ∈ arg min ^X '(~, θi).
θ~∈Θn	i
Another desirable property of a model profile is that, given all agents deploy their models, their
models are also simultaneously optimal for distribution that their model induces. Formally, ~* is
performatively stable if for all i ∈ [n]
....∙ .—
(θ*)i ∈ argmin'(θ*, θi).
θi∈Θ
The performative optimality does not implies the performative stable point. For performative opti-
mal point, the variable for minimization θ affects both the first and the second argument, but only
affect the second one for performative stable point. Now we introduce our model in this paper.
3
Under review as a conference paper at ICLR 2022
Location-scale distribution map In this paper, we study the family of multi-agent location-scale
map for regression problem where a data point consists of d-dimensional feature and scalar outcome,
z = (x, y). These are natural classes of distribution maps in which performative effects enter
through an additive or multiplicative factor that is linear in θ.
Definition 2.2. Given d ∈ N and d ≥ 2, and Θn ⊆ Rd, a distribution map D : Θn → Rd × R
is a multi-agent location-scale distribution map on n parties if there exists a static distribution DX
on Rd+1, θ0 ∈ Rd, and n linear functions Λ1, . . . , Λn from Rd to Rd so that the distribution of
(x, y)〜D(θ) has the following form: The feature is X ∈ Rd and noise xo ∈ R is jointly sampled
from DX . Given feature x ∈ Rd and noise x0 ∈ R, and the the outcome is
y
θ0 - Xn Λi(θi),x
+ x0.
In this paper, we consider the scaling maps Λi(θ) = λiθ for all θ ∈ Θ with scalar λi > 0 for
all i ∈ [d]. We call λ := (λ1, . . . , λn) the influence parameters, and Ln := Pin=1 λi collective
influence. Furthermore, we let A := E xx> ∈ Rd×d be the covariance matrix of the feature, and
b := Aθ0 + E[x0x] ∈ Rd. We will specify the multi agent location-scale distribution map with
parameters n, d, λ, A, and b.
When n = 1, our multi-agent location-scale distribution map is a special case of location-scale
family in Miller et al. (2021) where the model θ may both the outcome y as well as the feature x.
Predictive Models and Loss Function We consider linear predictive model with constraint where
fθ0 (x) = hθ0, xi, and the collection of parameter is the d-simplex, Θ = {θ : Pdk=1 θk = 1, θk ≥
0}. We use mean squared error to measure a predictive model θ0 ∈ Θ on a distribution map with a
deployedmodelProfile~ ∈ θn, '(θ, θO) = E(x,y)〜D(~)[(y-fe0(X))2] = E(x,y)〜D(~)[(y—θ0∙X)2].
Given a deployed model profile θ and a predictive model θ0, the gradient of the decoupled loss is
z →	一*、	_ _ z →	_ , κ	_	. .	_	.	.	.	一、、	一	r . z _ ,
g(θ, θ0) := Vθo'(θ, θ0), and if D is a location-scale distribution map, g(θ, θ0) = Er S D,~ [2(θ0 ∙
(X,y)~D (θ) L
X - y)X]. Furthermore, with A and b defined in definition 2.2, the gradient can be written as
g(θ, θ0) = V'(θ, θ0) = 2A 卜 + X λiθi) - 2b.	(1)
Additionally, given a deployed model profile θ~ and a predictive model profile θ~0, we define the
gradient of agent i’s decoupled loss as gi(θθ, θθ0) := g(θθ, (θ0)i), and gi(θθ) := gi(θθ, θθ) when the
deployed model profile is identical to the predictive model profile. We denote the gradient of agent
i,s average loss as gi(~) := Pl θ↑g[((~') for all θ ∈ Θn. Finally, We define ~(θ) = (ξ1,..., ξn)
with ξi ∈ Rd so that for all i ∈ [n] and k ∈ [d] ξki (θθ) := θki (P θligli - gki ). For brevity, we omit θθ
and define θg := θg(θ) and ξ := ξ (θ) when there is no ambiguity.
2.2	Learning dynamics
We consider the agents myopically use a reinforcement learning algorithm exponentiated gradient
for linear regression to improve their predictive models in rounds. We first define the original single
agent’s exponentiated gradient for linear regression on a sequence of data points here and will state
our dynamics on multi agent performative prediction in section 4.
Definition 2.3 (Kivinen & Warmuth (1997)). Given a learning rate η > 0, an initial parameter
θ0 ∈ Θ and a sequential of data points (xt, yt)t≥1, the exponentiated gradient descent for linear
regression iteratively updates the model as follows: At round t + 1, with previous parameter θt ∈ Θ
the exponentiated gradient algorithm updates it to θt+1 so that
θ	θt,k exp(-2η(θt ∙ Xt - yt)xt,k)
θt+1,k = Pl%, exp(-2η(θt∙xt-yt )xt,l) for all k ∈ ⑷
Note that each exponent 2(θt ∙ Xt - yt)xt,k is the k-th coordinate of the gradient of squared error
'((xt,yt), θt) at θt which yields the name of exponentiated gradient descent.
4
Under review as a conference paper at ICLR 2022
2.3	Dynamical system
Definition 2.4. Let (X, f) be a dynamical system where X is a metric space and f is a mapping on
X. We say (x, x0) ∈ X × X is a Li-Yorke pair if
lim inf dist(f t (x), ft (x0)) = 0 and lim sup dist(f t (x), ft(x0) > 0.
tt
(X, f) is Li-Yorke chaotic if there is an uncountable set S ⊂ X (scrambled set) such that any pair
of points x 6= x0 ∈ S is Li-Yorke pair.
3	Multi-agent Performative Learning: S tab ility and Optimality
Having introduced multi-agent performative prediction, we show some basic property of our
location-scale distribution map with mean squared error as a warm-up. Using the first order condi-
tion and a potential function argument, we show 1) the existence of performative stable point in our
model (proposition 3.1) and 2) the performative stability and performative optimality are equivalent
(proposition 3.2). The proofs of both propositions are in appendix A.
We first show the existence and uniqueness of performative stable point through a potential function
argument. The first part is due to the first order condition of convex optimization. The second part
also use the first order condition, and ∂LΦ(~) = λigk (θ) for all i ∈ [n] and k ∈ [d].
k
Proposition 3.1 (existence). Given the family of linear predictive models with constrains Θ, mean
squared error ` and multi-agent location-scale distribution map with parameters n, d, λ, A, b in
definition 2.2, ~* on (Θ,', D) is performative stable if and only if the gradient (defined in eq. (1))
satisfies gk (~*) ≤ g； (~*) for all i ∈ [n] and k ∈ [d] with θk > 0.
Furthermore, if A is positive definite, there exists a unique performative Stable point ~*, and ~* is
also a global minimizer of the following convex function for all θ ∈ Θn
Φ(θ~) := (X λiθi)>A(X λiθi) + X λi(θi)>Aθi - 2b> X λiθi.	(2)
While model profile is performative stable does not necessary mean the loss is minimized, the fol-
lowing proposition shows that optimality and stability are equivalent in our setting.
Proposition 3.2. Given (Θ,', D) defined in proposition 3.1, ~* is performative stable ifand only if
θ* is performatively optimal defined in definition 2.1.
4 Multi-agent Performative Learning: Convergence and Chaos
In section 3, we study the stability and optimality of location-scale distribution map with mean
squared error. Now we ask when each agent myopically improves his predictive model through
reinforcement learning but their predictions are performative, what is the long term behavior of the
system? Can the system converges to performative stable and optimal point, or behave chaotically?
We provide a threshold result depending on the learning rate and the collective influence Ln . Sec-
tion 4.1 shows the dynamics converge to the performative stable and optimal point when when the
learning rate is small enough. On the other hand, section 4.2 shows the dynamics can have chaotic
behavior if the collective influence Ln is large enough.
Now, we define our dynamics (θt)t≥0 formally. At each round, each agent accesses the data dis-
tribution which influenced by their previous model, and updates his model through exponentiated
gradient descent. Specifically, given an initial model profile θ~0 ∈ Θn and a learning rate profile
η := (η1, . . . , ηn), each agent i applies the exponentiated gradient descent with initial parameter θ0i
and learning rate ηi > 0: At round t + 1 > 1, each agent i use D(θ~t) and estimates the gradient of
the (expected) loss, gi(θ~t) defined in eq. (1), and updates his model according to definition 2.3,
θti+1,k
θi,k exP(-n；gk (仇))
Pd=I θi,ι eχp(一ηigχθt))
for all t ≥ 0, i ∈ [n], and k ∈ [d].
(3)
5
Under review as a conference paper at ICLR 2022
We will use superscript to denote agent, i ∈ [n], and subscript for time, t ≥ 0, and index of feature,
k, l ∈ [d]. Recall that the gradient of agent i,s average loss is gi(~) := Pi θfgi(~), and eq. (3)
can be rewritten as
θk exp(-ηigk(θt)
Pi θi exp(-ηigKθt))
we define the support of ~* as Si ⊆ [d]
- . √. —÷.	. —÷...
θk exP(ni(g 2t)-gKθt)))
Pi θi exP(ni(gi(St)-gi(St))) .
Finally, given ~* and i ∈ [d],
equilibrium (or fixed point) of eq. (3) if
{k : (θ*)k > 0}, and Si := [d] \ Si. Then ~* is an
gk(~*)= g'(θ*), for all i ∈ Hk ∈ Si.
(4)
We say a fixed point of eq. (3) is isolated if there exists an open set ofit so that no other fixed point
is in the set. Additionally, the performative stable condition in proposition 3.1 is equivalent to
.∙ . . .∙ . . .∙ . -. .∙.—. - — -
gk(θ*) = gi(θ*) and gi(~*) ≥ gi(θ*), for all i ∈ Hk ∈ Si,l ∈ Si.	(5)
Therefore, the set of fixed points of eq. (3) contains the performative stable point.
4.1 Converging with Small Learning Rate
In this section, we show when the learning rate of each agent is small enough the the dynamics in
eq. (3) converge to performative stable point. Specifically, if the parameter of multi-agent perfor-
mative learning (n, d, A, λ) is fixed, the dynamics in eq. (3) converge to performative stable point
when η is “small” enough.
By eq. (5), θ~* is a performative stable if the gradient in the support is no less than the average
gradient. We call a performative stable point θ* proper if the gradient of non-support coordinate
is greater than the average gradient: for all i ∈ [n] l ∈ Si, gi(~*) › gi(~*). The below theorem
shows if the performative stable point is proper and equilibria satisfying eq. (4) are isolated, eq. (3)
converges when maxi ηi is small enough and the ratio of ηi /ηj is bounded for all i and j .
Theorem 4.1 (Convergence). Consider a constant Rη > 0, anda multi-agent performative learning
setting with parameter n, d, A, b, λ such that A is positive definite, the performative stable point θ~*
is proper and its equilibria (defined in eq. (4)) are isolated. There exists η* > 0 so that dynamic in
eq. (3) with learning rate profile η converges to the performative stable point, limt→∞ θ~t = θ~*, if
the initial state is an interior point and η satisfies maxi ηi ≤ η* and maxi ηi / mini ηi ≤ Rη.
Informally, when the learning rate η are small, θ~t in eq. (3) can be approximated by a solution ofan
ordinary differential equation, E(t∣∣ηkι), where the initial condition is £(0) = θo and
J"k (t)=儡吸(t)(gi(~(t)) - gk (~ (t)))	(6)
dt k kηk1 k	k
for all t ≥ 0, i ∈ [n], and k ∈ [d]. We formalize this intuition in the proof of lemma 4.4. Note that
the set of fixed points of eq. (6) is identical to eq. (3) and satisfies eq. (4).
The proof of theorem 4.1 has two parts. First we show the continuous approximation eq. (6) con-
verges to the performative stable point. Then we study the relationship between eq. (3) and eq. (6),
and prove the eq. (3) also converges to a performative stable point.
From Potential Function to Convergence of Equation (6) In this section, theorem 4.2 shows
the dynamics of eq. (6) converge to performative stable point which will be useful to show the
convergence of eq. (3).
Theorem 4.2. If all points satisfying eq. (4) are isolated and A is positive definite, and £~ (0) is in
the interior of Θn, the limit limt→∞ £~ (t) = θ~* is the performative stable point.
Theorem 4.2 can be seen as the continuous version of theorem 4.1. Compared to theorem 4.1, theo-
rem 4.2 does not require that the performative stable point is proper, but theorem 4.2 also implicitly
requires the ratio of learning rate between any two agents is bounded.
To prove theorem 4.2. we prove lemma 4.3 which shows that Φ in eq. (2) is a potential function
for eq. (6) so that time derivative of Φ is decreasing for all non-fixed points. Then, in the proof of
theorem 4.2, we further prove that the dynamics in eq. (6) converge to performative stable points
when the initial condition £(0) is an interior point. The proof is similar to a proof in Kleinberg et al.
(2009), and is presented in appendix B.
6
Under review as a conference paper at ICLR 2022
Lemma 4.3. Given a solution of eq. (6), the time derivative of Φ in eq. (2) is 0 at fixed points of
eq. (6), and negative at all other points. Furthermore,
-dφM))≤,p -P X	(X3吸(X切gi-成)\\ ≤?pmin?iηi k~(~)k2.
dt	2	i ηi i λiηi	ik k l l l k	2	i ηi i λiηi	1
i,k	l
From Approximation to Convergence of Equation (3) Given the convergence of eq. (6), we
show the dynamics of eq. (3) also converges to a performative stable point. The argument has two
stages. We first show given any neighborhood of performative stable points D, eq. (3) will hits D
and stay in D in finite amount of steps in lemma 4.4. Then lemma 4.5 shows that eq. (3) converges
to a performative stable point which completes the proof of theorem 4.1.
Lemma 4.4. Given any open set D ∈ Θn that contains the performative stable point, there exists
η small enough so thatfor all θo ∈ Θn ,there exists T so that θt ∈ D for all t ≥ τ.
The proof is based on standard approximation result of numerical solution to ordinary differential
equations, and is presented in appendix C.
Lemma 4.5. Let θ* is the performative stable point and D is a small enough neighborhood of θ*.
Besides the condition in theorem 4.1, if the initial condition of eq. (3), θ~0, is in D and θ~t ∈ D for
all t ≥ 0, the limit ofeq. (3) is the performative stable point limt→∞ 瓦 =θ*.
The proof uses the fact that the right-hand side of eq. (3) decreases as the dynamic converges to the
fixed point. Therefore, even though the learning rate profile is fixed, the error between eq. (3) and
eq. (6) vanishes as they converge to the fixed point. We present the proof in appendix D.
4.2 Chaos with Constant Learning Rate
Now we want to ask the converse question in section 4.1. Do the dynamics in eq. (3) still converge, if
the learning rate is fixed, as the number of agent increases? Alternatively, do the dynamics converge
when the agents have overwhelming influence on the data distribution?
Theorem 4.6 shows that the dynamics is Li-York chaotic when Ln = Pi λi is large even with d= 2.
Note that large Ln may be due to a fixed number of agent which have overwhelming influence, or
the number of agents is large. The later case implies for any small η, no matter how cautious the
agents are, as number of agent increases the chaos inevitably occurs.
Theorem 4.6. Given a multi-party induced location scale family distribution with A, b, d = 2,
and a common learning rate η > 0, if all n agents use the exponentiated gradient with a common
learning rate η in eq. (3) and A is diagonally dominant, there exists a carrying capacity L such
that if Ln = pn=ι λi ≥ L the dynamics (θt)t∈N is Li-Yorke chaotic and thus has periodic orbits
of all possible periods.
To prove theorem 4.6, we show there exists an uncountable scrambled set defined in definition 2.4.
First we consider all agents start from the same initial model, and show the dynamics eq. (3) can be
captured by the following function,
x
fu,v(X)= X +(1 - x)exp(u(x - V)) ∀x ∈ [0,1]
(7)
with proper choice of u ∈ R and v ∈ R. Note that v is an equilibrium, and u controls steepness.
First, when all agents start from the same initial model, θ0i = θ0 for all i ∈ [n], and use the same
learning rate η, their models are identical for all t ≥ 0. For all t there exists θt so that θti = θt for
all i. Additionally, since d= 2, we can use a single parameter to encode a linear predictive model
with constraints. Given θ ∈ Θ = ∆2, we define p(θ) = θ1 ∈ [0, 1] and omit the input θ when it
is clear. Thus, we can rewrite the dynamics as pt = p(θt) which are one-dimensional dynamics on
[0，1]，andPt+1 = pt + (i-pt)eηpti(~t)-g2(~t)). We Set
α(L) =2η (1 + L) (A1,1 - A1,2 - A2,1 + A2,2) and β(L)
(I + L)(A2,2 - A2,I) + (b1 - b2)
(1 + L)(A1,1 — A1,2 — A2,1 + A2,2)
7
Under review as a conference paper at ICLR 2022
and write αn = α(Ln) and βn = β(Ln). By direct computation the dynamic ofpt is
pt+1 = fαn,βn (pt)
where αn encodes the update pace and βn is an equilibrium of the dynamic. We further set β∞
(8)
Al LA：；-：：： +A 2 and δ(L) ：= β(L)-β∞ which converges to zero as L → ∞. If A is diagonally
dominant α(L) is positive and increasing for all L ≥ 0. Additionally, because A1,1 - A1,2 and
A2,2 - A2,1 are positive, we can permute the coordinate so that β∞ ∈ (0, 1/2]. With Li et al.
(1982), the lemma below implies the existence of period three points and the proof is in appendix E.
Lemma 4.7. If β∞ ∈ (0,1/2), there exists a constant L* > 0 so that if L ≥ L*, there exists a
trajectory x0, x1, x2, and x3 with fα(L),β(L) (xi) = xi+1 such that x3 < x0 < x1.
Proof of theorem 4.6. First by lemma 4.7, for all L ≥ L*, there exists a trajectory x0, x1, x2, x3 so
that x3 < x0 < x1. Additionally, existence of such trajectory implies the exists of period three
points by Li et al. (1982). Finally, by the seminal work ofLi & Yorke (1975), it implies that the map
is Li-York chaotic.	□
5 Simulation
(d) (14, 0.001) with noise
(e) (1.4, 0.05) with noise
(f) (14, 0.05) with noise
Figure 1: Here we plot the temporal behavior ofpt starting atp0 = 0.2 for 100 rounds under various
L and η, and noisy estimation of gradient of mean squared error with m samples. The top row
(figs. 1a to 1c) present different combination of (L, η), and bottom row (figs. 1d to 1f) consider the
gradient is estimated from m = 10 and m = 100 samples.
Now we simulate the one-dimensional dynamics in eq. (8) of one agent with different learning rate
η and collective influence L.
We first define a location-scale distribution map. Let the feature x1, x2 and noise x0 are mutually
independent Gaussian distribution with zero mean, and the variance are E[x21] = 3, E[x22] = 7, and
E[x20] = 1. Finally, θ0 = 0. Therefore,
A = E[xx>] = 30 70 , and b = Aθ0 + E[x0x] = 00 .
8
Under review as a conference paper at ICLR 2022
Given learning rate η > 0, and collective influence L, we have α(L) = 20η(1 + L), and the
performative stable point β(L) = 0.7.
The top row of fig. 1 shows the temporal behavior of pt under different (L, η). The bottom row in
fig. 1 demonstrates our (convergent and chaotic) results are robust even when the value of gradient is
noisy. Specifically, we consider at each round t, instead of g1i (θ~t) -g2i (θ~t) = 2(1 + L)(1, -1)Aθt -
2(1, -1)b, the agent replace A and b with empirical moments on m samples. This process can
be seen as a stochastic exponentiated gradient descent which uses a stochastic estimation of the
gradient.
We can see chaotic behavior happens in fig. 1c with L = 14 and η = 0.05, and such behavior
persists in fig. 1f when the evaluations of gradient are noisy. On the other hand, when the learning
rate is small enough (figs. 1a and 1b) the dynamics converge to the performative stable point 0.7.
Furthermore, in figs. 1d and 1e the dynamics converge even with noisy estimation of gradient.
Finally, fig. 2 shows the temporal behavior of the loss. Under the same setting as fig. 1c, fig. 2b
shows not only the temporal behavior pt is chaotic, but also the mean squared loss. On the other
hand, under the same setting as fig. 1a, fig. 2a shows convergent behavior of loss.
(a) (14, 0.001) without noise (b) (14, 0.05) without noise
Figure 2: The temporal behavior of the total cost for 100 rounds under various η.
6 Conclusion
We introduce a framework of multi-agent performative prediction and investigate whether classical
reinforcement learning algorithms can converge or behave chaotically depending on the collective
influence of the agents model and learning rate. However, we view our example as only scratching
the surface of the work of multi-agent performative predictions.
Our framework leads to several new theoretical problems. In particular, it would be interesting
to understand whether this threshold is generic and if our results still hold on other reinforcement
learning algorithms or other general multi-agent distribution maps. Our framework also provides a
new viewpoint to several applications. One natural application is strategic classification. Hardt et al.
(2016) In this context, our framework can be seen as strategic learners in strategic classification
problem where both data and classifiers are strategic. However, the features also respond to the
deployed models in conventional strategic classification, which is not captured in our multi-agent
location-scale distribution map. It would be interesting to investigate our dynamics in the context of
strategic prediction. Another application is pricing strategy/prediction, where multiple companies
predict the demand function and set their prices.
Both our digital and real-world environments are increasingly under the influence of or ever more
powerful and numerous ML systems. As these algorithms trigger actions that change the state of
the system that produces their joint input data (e.g., AI-triggered ads shown to users affecting their
behavior, or automated trading systems affecting stock prices, etc), they are effectively forming
closed loop systems and can no longer be understood fully in isolation. As we show in this paper,
even fairly simple and innocuous such settings can exhibit phase transitions going from optimal
behavior to chaos. We believe such phenomena are worthy of a careful investigation and we hope
that this paper sets out some useful building blocks by bringing together optimization/performance
analysis with Lyapunov theory and chaos theory.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Our work showcases the possibility of competing prediction algorithms leading to instability and
chaos. Extra steps should be taken, whenever possible, so that related systems operate within the
range of parameters leading to stability. We hope that our work helps both by identifying a potential
threat for ML systems as well as providing some guidance on how to counter or minimize it.
Reproducibility
All of our theoretical results are stated fully, including previous definitions and results on which they
are based. They are accompanied by proofs, either in the main paper or in the appendix. The setting
of our simulation results are stated fully in the main paper.
References
Gabriel P Andrade, Rafael Frongillo, and Georgios Piliouras. Learning in matrix games can be
arbitrarily complex. In Conference on Learning Theory (COLT), 2021.
James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In ACM
Confernce on Economics and Computation, pp. 321-338, 2018.
David Balduzzi, Wojciech M Czarnecki, Thomas W Anthony, Ian M Gemp, Edward Hughes, Joel Z
Leibo, Georgios Piliouras, and Thore Graepel. Smooth markets: A basic mechanism for organiz-
ing gradient-based learners. 2020.
Peter L Bartlett. Learning with a slowly changing distribution. In Proceedings of the fifth annual
workshop on Computational learning theory, pp. 243-252, 1992.
Jakub Bielawski, Thiparat Chotibut, Fryderyk Falniowski, Grzegorz Kosiorowski, MichaI Misi-
urewicz, and Georgios Piliouras. Follow-the-regularized-leader routes to chaos in routing games.
ICML, 2021.
Yun Kuen Cheung. Multiplicative weights updates with constant step-size in graphical constant-sum
games. In NeurIPS 2018, pp. 3532-3542, 2018.
Yun Kuen Cheung and Georgios Piliouras. Chaos, extremism and optimism: Volume analysis of
learning in games. In NeurIPS 2020, 2020.
Yun Kuen Cheung and Yixin Tao. Chaos of learning beyond zero-sum and coordination via game
decompositions. In International Conference on Learning Representations, 2020.
Yun Kuen Cheung, Stefanos Leonardos, and Georgios Piliouras. Learning in markets: Greed leads
to chaos but following the price is right. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth
International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal,
Canada, 19-27 August 2021, pp. 111-117. ijcai.org, 2021. doi: 10.24963/ijcai.2021/16.
Thiparat Chotibut, Fryderyk Falniowski, MichaI Misiurewicz, and Georgios Piliouras. The route to
chaos in routing games: When is price of anarchy too optimistic? NeurIPS, 2020.
Johanne Cohen, Amelie Heliou, and Panayotis Mertikopoulos. Learning with bandit feedback in
potential games. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 6372-6381, 2017.
Dmitriy Drusvyatskiy and Lin Xiao. Stochastic optimization with decision-dependent distributions,
2020.
Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Thanasis Lianeas, Panayotis Mer-
tikopoulos, and Georgios Piliouras. No-regret learning and mixed nash equilibria: They do not
mix. In Conference on Neural Information Processing Systems (NeurIPS), 2020.
10
Under review as a conference paper at ICLR 2022
Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos.
Survival of the strictest: Stable and unstable equilibria under regularized learning with partial
information. COLT, 2021.
Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classifica-
tion. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science,
pp.111-122, 2016.
Zachary Izzo, Lexing Ying, and James Zou. How to learn when data reacts to your model: perfor-
mative gradient descent. arXiv preprint arXiv:2102.07698, 2021.
Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear
predictors. information and computation, 132(1):1-63, 1997.
Robert Kleinberg, Georgios Piliouras, and Eva Tardos. Multiplicative updates outperform generic
no-regret learning in congestion games. In Proceedings of the forty-first annual ACM symposium
on Theory of computing, pp. 533-542, 2009.
Anthony Kuh, Thomas Petsche, and Ronald L Rivest. Learning time-varying concepts. In NIPS, pp.
183-189, 1990.
Stefanos Leonardos, Barnabe Monnot, Daniel Reijsbergen, Stratis Skoulakis, and Georgios Pil-
iouras. Dynamical analysis of the eip-1559 ethereum fee market, 2021.
Alistair Letcher. On the impossibility of global convergence in multi-loss optimization. In Interna-
tional Conference on Learning Representations, 2021.
Tien-Yien Li and James A. Yorke. Period three implies chaos. The American Mathematical
Monthly, 82(10):985-992, 1975. ISSN 00029890, 19300972. URL http://www.jstor.
org/stable/2318254.
Tien-Yien Li, MichaI Misiurewicz, Giulio Pianigiani, and James A. Yorke. Odd chaos.
Physics Letters A, 87(6):271-273, 1982. ISSN 0375-9601. doi: https://doi.org/10.
1016/0375-9601(82)90692-2. URL https://www.sciencedirect.com/science/
article/pii/0375960182906922.
Celestine Mendler-Dunner, Juan C Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic optimiza-
tion for performative prediction. arXiv preprint arXiv:2006.06887, 2020.
John Miller, Juan C Perdomo, and Tijana Zrnic. Outside the echo chamber: Optimizing the perfor-
mative risk. arXiv preprint arXiv:2102.08570, 2021.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights up-
date with constant step-size in congestion games: Convergence, limit cycles and chaos. CoRR,
abs/1703.01138, 2017. URL http://arxiv.org/abs/1703.01138.
Ioannis Panageas, Georgios Piliouras, and Xiao Wang. Multiplicative weights updates as a dis-
tributed constrained optimization algorithm: Convergence to second-order stationary points al-
most always. In International Conference on Machine Learning, pp. 4961-4969. PMLR, 2019.
Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dunner, and Moritz Hardt. Performative prediction.
In International Conference on Machine Learning, pp. 7599-7609. PMLR, 2020.
Nelson Vadori, Rahul Savani, Thomas Spooner, and Sumitra Ganesh. Consensus multiplica-
tive weights update: Learning to learn using projector-based game signatures. arXiv preprint
arXiv:2106.02615, 2021.
11
Under review as a conference paper at ICLR 2022
A Proofs and Details in section 3
Proof of proposition 3.1. First under the mean squared loss function, each agent i’s decoupled per-
formative loss function '(~0, θi) is convex in θi. Thus, for linear predictive model, We can apply the
KKT conditions on definition 2.1 so that a collection of predictive models θ ∈ Θn is performatively
stable if and only if for all i ∈ [n], k ∈ [d] with θ* > 0, ∂θτ'(~0, θi) ≤ 磊'(~0, θi) for all l ∈ [d]
kl
where θ = θ. Therefore, with eq. (1), θ* is performative stable if
.∙ . . .∙.—.
gk(θ*) ≤ gi(θ*)	(9)
holds for all i ∈ [n] and k ∈ [d] with θki > 0.
With eq. (9), we now show that there exists a unique performative stable point ~* by proving that 1)
Φ in eq. (2) is strictly convex, and 2) θ* is a minimizer of Φ. First, to show Φ is strictly convex, it is
sufficient to show the Hessian of Φ positive definite. Because Φ is a quadratic function on Θn, the
Hessian ofΦ is a constant matrix in Rnd×nd By the partial derivative of eq. (2), for all i, j ∈ [n] and
k,l ∈ [d], we have (V2Φ)ik,ji = /:叫 Φ = 2入内Ak,ι if i = j and (V2Φ)ik,n = 2λi(λi + 1)Ak,ι.
Let L ∈ Rn×n with Lij = 2λiλj if i 6= j and 2λi(1 + λi) which is positive definite because λi > 0
for all i ∈ [n]. Then, the Hessian of Φ is the Kronecker product of L and A,
∕2λι(1 + λι)A	...	2λιλnA ∖
V2Φ =	.	...	. I	= L 0 A.
2λnλ1A	...	2λn(1+λn)A
Because L and A are both positive definite, the Hessian is also positive definite. Therefore, Φ is
strictly convex, and there exist a unique minimum of Φ in the compact set Θn. Now we show θ~ is
performative stable if and only if θ is a minimizer of Φ. By the first order condition and the partial
derivative of eq. (2), the minimum of eq. (2) at θ if and only if gk (~*) ≤ g*θ*) for all i ∈ [n]
k, l ∈ [d] with (θ" > 0. Therefore, θ is the minimum of Φ if and only if ~* is a performative
stable point.	口
Proof of proposition 3.2. Given a profile of models θ, the total cost is
Σ'(θ, (θ)i)=Σ E(x,y)~D(~)[(y- θi ∙ V
ii
E
i
(*x，θ0 - X λjθj - θi+ + xoj
1 ∙ 1 ∙	r .∙	K 一…… τ,∙	11
which is a convex function on θ ∈ Θn . Additionally,
∂d- X '(~, θ1) = (1 + λi)gk (~, ~)+ X λigk (θ, ~) = (1 + nλi)gik(θ, θ)
k i	ι∈[n]%=i
which is the gradient of decoupled performative loss scaled by (1 + nλk). Thus, we can apply the
KKT conditions and the minimum happens if and only if eq. (9) holds.	口
B Proof and Details for theorem 4.2
Proof of lemma 4.3. By chain rule,
dtφ(M))=	χ	∂L(~⑻) ∙ ddt~⑴
k∈[n],k∈[d]	k
(10)
so the time derivative of the potential function is zero if the dynamics is at a fixed point of eq. (6).
12
Under review as a conference paper at ICLR 2022
Now we compute a closed form of the time derivative and show the derivative is zero only if the
dynamics is at a fixed point of eq. (6). Here we omit the input t to simplify the notation.
dΦ(~⑴)=X 看(~⑴)•氤噩⑴(X牖⑴gi(~⑻-gk(~⑴))(by eqs. (6) and (10))
1
llηkι
∑λiηi £(gk) •吸 Ew gk
ik
而1犷 Xλη X 吸叼(2gkgi- 2(gk)2)
η 1 i	k,l∈[d]
2-k- X λη X 吸切(gk -gi)2
η 1 i	k,l∈[d]
»
(by the partial derivative of eq. (2))
(because Pl %=1)
Now we bound the time derivative.
-2kηk1 (X %η) dΦ(~⑴)
E	λiη明@I
i∈[n],k,l∈[d]
E	%η 成牖(gk- gi)2
i∈[n],k,l∈[d]
(Pk 吸=Pl 伙=1)
≥ I E %ηMW∣gk-gi∣
i∈[n],k,l∈[d]
(by Cauchy inequality)
E	λi用 比£ 龙(gk-gi)
(by triangle inequality)
i∈[n],k∈[d]
l∈[d]
≥
l
2
2
Therefore, dtφ(~(t)) = 0 Only if吸(gk - Pl∈[d]
fixed point of eq. (6).
0 for all i ∈ [n] and k ∈ [d] which is a
We Can further simplify the bound by ξ.	Because Pik X浦/然(Pl Higi 一 gk)1	≥
(mini 片η)P* ∣ξk (~ )| = (mini %小)k~(~ )∣ι, d Φ(~ ⑴)≤ 2 PmnPinini k~(~ )k2.
□
Proof of theorem 4.2. When the fixed points are isolated satisfying eq. (4), by lemma 4.3, the dy-
namics in eq. (6) converges to a fixed point of eq. (6), ~* ∈ Θn. Because ~* is a fixed point, for all
i ∈ [n], k ∈ [d],
(θ*)k (gi(~*) - gk (~*)) = (θ*)k E(θ*)igi(θ*) - gk(θ*) = ο
l
By proposition 3.1, the fixed point condition implies each coordinate of the gradient of loss in the
support are identical, gk (~*) = Ui(~*) for all i and k ∈ Si. Thus, if the fixed point θ* is not
performative stable, there exists ι and K with (θ*)K = 0 so that gK(~*) < 矿(~*). Furthermore,
exp(-ηιgK(~*)) > El(θ*)l exp(-η⅛g∣(θ*)). We can pick a small enough E > 0 and define
Ue= {~: exp(一¾gK) > Xθ1exp(-%g1) + e}	(11)
l
which contains θ* and is an open set because g and the exponential function are continuous. Since
E(t) converges to θ* as t → ∞, there exists a time te so that for all t ≥ te, E(t) ∈ Ue. However,
if ~(t) ∈ Ue and ~(t) is an interior point, We get d"(t) > 0 by eqs. (6) and (11). Therefore,
Hk(t) is positive and increasing for t ≥ te. We reached a contradiction because Hk(t) → (θ*)k = 0.
Therefore, ~* is a performative stable point.	□
13
Under review as a conference paper at ICLR 2022
C Proof ans Details for lemma 4.4
To prove lemma 4.4, show the dynamic eq. (3) can be approximated by eq. (6) and the error vanishes
as η decreases. Thus, We can show the dynamic can hit an arbitrary neighborhood of the Performa-
tive stable point. We further use Φ to show the dynamic will stay in the neighborhood. Below we
state two ancillary lemmas to control the error of our aPProximation.
We define an error vector ~e(θ~) ∈ Rn×d between eqs. (3) and (6) so that
宗√ θ∖ - θk expg(9i - gik))	θi	Fi
ηi ek(θ) := Pl θi eχpg(g∙ - gi)) -θk-ηiξk
We omit the inPut θ~ of ~e and ~et := ~e(θ~t) when there is no ambiguity. We show if the the error
vector is small, Φ is decreasing in dynamics eq. (3).
Claim C.1 (APProximated Potential). There exist C4, so that for all t and θ~0 ∈ Θn, the difference
of eq. (2) on eq. (3) satisfies
Φ(~t+1) — Φ(~t) ≤ -7iPni λ2η2 kξtk2 + max%褚 maχ 尿(θ)HI~tkι + d2C4 ∣∣~m - ~tf .
2 iλiηi	1 i i i,k,θ~	1
Claim C.2. If maxi ηi is small enough and satisfies eq. (12), there exists a constant C so that
∣eik (~)| ≤ C for all i ∈ [n], k ∈ [d], and ~ ∈ Θn.
The Proofs of these two claims are based on first order aPProximation.
2
Proofofclaim C.1. By the partial derivative of eq. (2), we have 端 j Φ(θ) = 2λiλj Al,k if i = j
∂θk ∂θl	,
and ∂θd∂θi Φ(~) = 2λi(1 + λi)Al,k. Thus, the second order partial derivatives of Φ can bounded by
the twice of C4 := max? λι(λι + 1) maxk,l Ak,l. By Taylor,s expansion, we have
Φ(θ~t+1) -Φ(θ~t)
≤vΦ(θt) ∙(瓦+1 - ~)+d22c4 ∣∣~(t+1)—~(t)∣∣2
≤ X	λigki (θ~t)(ηiξki (θ~t) + ηi2eik(θ~t)) + d2C4 θ~(t + 1) - θ~(t)2
i∈[n],k∈[d]
(by the partial derivative of eq. (2))
= X	λiηigti,kξti,k+λiηi2gti,keit,k+d2C4∣∣∣θ~t+1-θ~t∣∣∣2
i∈[n],k∈[d]
2
+ X	λiηi2gti,keti,k +d2C4 ∣∣∣θ~t+1 -θ~t∣∣∣2 (by lemma 4.3)
i∈[n],k∈[d]
≤2∑-⅛ (Xλiηilξi,k|
Therefore, we have
φ(~t+1)—φ(~t) ≤ 罢箸 k~tk1+max λiη2 mkaxlgk (~)1 ∙k~tk1+d2c4 ∣∣~t+ι -~t∣∣2
which completes the proof.
□
Proofofclaim C.2. By if η is small enough and satisfies eq. (12), we have 力(矿 一 gk) ≤ 1, and
θk+ ηiθik(gi - gk) ≤ θkeηi(gi-gk) ≤ θk+ ηiθk(gi - gk) + 2η2θk(gi-gk)2 foralli,kand~∈ Θn.
Additionally, with ξi = θii(gi — gii), we can rewrite it as
θk ≤ θk eηi(gi-gk) — ηiξk ≤ θk + Wneii (gi — gk )2.
14
Under review as a conference paper at ICLR 2022
Therefore, the error can be upper bounded as
ηi2eik ≤
θk + mξk + 2η2θk Ogk )2
Pl θi + η ξi
-(θk + ηξ) = eη2θk(gi - gk)2,
because Pl θli = 1 and Pl ξli = 0. For lower bound, we have,
η2ek ≥ Pl θi + ηiξk+2⅛(gj)2 - (θk + 泡) ≥ -(θk + 小 ξk) (苧 X θi(gi "
If We Set C := emax~ ∣Pl θ∖(gi - g↑')2∖, because 匠§力| ≤ ηi2maxk 尿| ≤ 1 by eq. (12) and
θk ≤ 1, we have 卜k(~)∖ ≤ C.	□
Proofoflemma 4.4. Because D is open, we can set D0 so that ~* ∈ D0, D0 ⊂ D, and
sup~∈D0 Φ(θ) < 1 inf~∈d Φ(~). We will pick η* small enough so that ~t+ι ∈ D for all
θ~t ∈ D0 ⊂ D. The proof has two parts: we first show the dynamics eq. (3) hits the set D. Then we
prove eq. (3) stays in D afterward.
By theorem 4.2, there exists T so that ~(τ) ∈ D0. Now by Gronwall,s inequality we can set η* small
enough so that θτ∕knkι ∈ D. Formally, given the dynamics in eq. (3), we define right-continuous
step functions θ(t) = θbtc and ~e(t) := e~(θbtc ) for all t ≥ 0. Then the dynamics in eq. (3) can be
written as
t-1	t-1	t
θki(t)-θki(0)=	(θsi+1,k-θsi,k)=	ηiξki(θ~s)+ηi2eik(θ~s) =ηi	ξki(θ~(s))+ηieik(s)ds
s=0	s=0	0
On the other hand, the solution of eq. (6) can be written as
C —t—	c +
「	、	」、	ηi∙	八ηkι 」τ, 、、	Γ 」T.........
Gk(ηit) - %(0) = kn∣^ J0	ξk("(S)) ds = ηi J0 ξk("(kηkιS)) ds
Because ξki are continuous and Θn is compact, there exists L > 0 so that ξki is L-Lipschitz with
respect to one norm for all i and k . Thus, the difference between above equations is
∖θk(t) -吸(kηkιt)∖ ≤ηjt'ξklk(~(s))-ξk(~(kηkιs))∖ ds + η2]：层(s)|ds
≤ηL
-~(ηis)∣∣ ds + η2Ct	(L-Lipschitz and claim C.2)
Therefore, by Gronwall,s inequality and ηi ≤ η*, we have
θ(t) - ~(knkιt)∣∣ι ≤ 小ndL J ∣∣~(s) - ~(kηkιs)( ds + (η*)2ndCt ≤ (η*)2ndCteη*ndLt
Because
_ ， →
n /	\	_	1~\ /	♦	1	11	1	.1	Λ*/ /H Il ∖	Λ*	_	τ~∖	1
"(τ) ∈ D0, We can PiCk η* small enough so that θ(τ∕∣η∣ι) = θ∖τ∕knkιC ∈ D and
Φ(θbτ∕kηk1c
) < infθ~∈∕D Φ(θ~).
Now we show the second part that θ~t ∈ D for allt ≥ bτ /kηk1c. First, with claim C.1, we will prove
that the potential function is decreasing for all θt ∈ D0 when η* small enough. We estimate three
terms inclaιmC.1 separately. First, becauseθ* ∈ D andΘn∖D0 is compact, mιn~∕D0 kξ(θ)∣k > 0
exists. Then mPi；% ∣∣ξtk2 = Ω(maxi ηi) By claim C.2, maxi λiη2 maxi,fc,~ 尿(~)HI~tkι =
O(maxi η2). Finally, d2C4 ∣∣θt+ι - ~t∣∣ = O(maxi η2). Therefore, there exists η* small enough
so that the potential function Φ(θt+1) - Φ(θt) < 0 for all θt ∈/ D0 and η. Therefore Φ(θt) <
mιn~∈D Φ(θ) for all t ≥ [τ∕kη∣ J which completes the proof.	□
15
Under review as a conference paper at ICLR 2022
D Proofs and Details for lemma 4.5
Lemma 4.3, shows the value of Φ is decreasing in eq. (6), and the decrease rate is lower bounded by
the one norm of ξ. Thus, if we can show the error between eq. (3) and eq. (6) is bounded by kξk1,
We have the value of Φ is also decreasing on eq.(3). The main challenge is that because ξ(θ*) = 0,
we need to control the error as ξ converges to zero but η is fixed.
We first shoW three ancillary claims, claim D.1 to D.3. Claim D.1 shoW the vanishing components
of θt decrease rapidly once the dynamic is in D. Claim D.2 and D.3 show the supporting component
also decrease once the vanishing components are small enough.
Given (n, d, A, λ), we define the following constants C1
4 maxi,l,θ∈Θn |gi|, C2 := mg；0 )k ,
C3 := ed max(C1, C2),and C4 := max? λι(λι +1) maxk,ι Ak,ι. We require the maximum learning
rate is bounded by η which satisfies the following conditions.
η* max |gk ⑥1 ≤ 1
i,k,θ~∈Θn	2
η*ndC3 max k~(~)kι ≤ 1
θ~
(12)
(13)
Additionally, given the bound of learning rate ratio, m；Xi ；i ≤ R, we requires
R2 η3 < mini λ2 min ʃ_____________4___________
ηn*	16 Pi λi	C C3 maxQi)max~ k~kι
Note that m：X；i is less then the right hand side of eq. (14). On the other hand, by lemma 4.4, we
can pick D small enough so that the following conditions holds.
1
x . min q (θ )k ≤ min 一 θk	(15)
2 i∈[n],k∈Si	i∈[n],k∈Si,θ~∈D
We first show for all i and k ∈ S%, θ∖ k is decreasing and converges to zero exponentially fast
as t increases. Because ~* is a proper performative stable, PWyieT瓜或⑹)=e-ηi3i(~*) >
e-ηigk(θ*) for all i and k ∈ Si. We can take n*, s, and D small enough so that for all ~ ∈ D and
all learning rate profile η with max η ≤ %,
e-ηigk < (1 - eι) Xθie-ηigi,i ∈ [n],k ∈ Si	(16)
l
i . C - ♦一	，	~	Y 7-Λ .	/…∙cX _ C 八	TT ,	、 C 八	TT
Claim D.1 (Vanishing components). Given %, ^, and D in eq. (16), If θt ∈ D for all t ≥ 0, for all
i ∈ [n] and k ∈ Si, θki (t) is decreasing in t and for all t ≥ 0
0 ≤ θti,k ≤ θ0i,ke-1t.
Proof of claim D.1. By eq. (16), for all t ≥ 0, 4十^ = θ^ P Npexp-泉、)≤ (1 -⑴纥®.
Therefore, θit 卜 is decreasing, and θ∖ 卜 ≤ θ0 卜e-e1t.	□
Cl-.-- CG El	∙ .	. ,c	ι .ι .	II λ* — ʌʌʃn ∙,r C-C .ι . 11 力 K∖ 11	-
Claim D.2. There exists a constant C3 such that for all θ ∈ Θn with δ1 > 0 so that kξ(θ)k1 ≥
2√δ1, and ∣ξii (θ) | ≤ δι for all i ∈ [n] and k ∈ Si, then
|ek(~)∣≤ C3k~(~)k2,	(17)
for all i ∈ [n] and k ∈ [d].
KeCl	.…	d	-.	..	∙ . ∙	.	..
Claim D.2 shows if the one norm ofξ is much bigger than the vanishing components, the error term
e~ can be bounded by the kξk12. Moreover, claim D.1 ensures that the vanishing components decrease
rapidly, so the condition of claim D.2 readily holds.
16
Under review as a conference paper at ICLR 2022
Proof of claim D.2. Given θ and δ1 > 0 that satisfy the condition, we first show two inequalities to
bound θk(gi - gk)2 for supporting and vanishing component respectively. For a vanishing compo-
nent k ∈ Si,
θk (gi-gk )2 ≤ .maχ ∣gi∣∙∣θk (gi- gk )l ≤ 4Cι∙ δι ≤ Cιk~k2.	(18)
i,l,θ∈Θn
Then for a supporting component k ∈ Si, with eq. (15) we have
12
θk(gi - gk)2 ≤ ———θ(θk(gi - gk))2 ≤ min (θ*V ∣ξk 12 ≤ C2kξk2.	(19)
mini,l,~∈D θl	mini,k(θ )k
Now we use above two inequalities to approximate eq. (3). For nominator, because 1 + x ≤
exp(x) ≤ 1 + X + 2x2 for all X ≤ 1, θk exp(η(gi - gk)) ≥ θk+ 小θi (gi - gk) = θk+ ηiξk.Onthe
other hand, because ηi(gi-gk) ≤ 1 by eq. (12), θ]i exp(ηi(gi - gik)) ≤ θk + ηiξk + f θk W- gk )2.
By eqs. (18) and (19), we have
0 ≤ θkeηi(gi-gk) - θk - ηiξk ≤ $ max(Cι, C2)η2∣∣~∣2	(20)
For denominator, We sum over eq. (20). Because Pl θ∖ = 1 and Pl θf (gi - g∣') = 0, we have
0 ≤ X θieηi(Jgi)- 1 ≤ ed max(Cι, C2)η2 k~k2
l
(21)
Given i ∈ [n] and k ∈ [d], we apply the above equation to eq. (3). For upper bounds, we have
For lower bounds,
ηi2eik(θ~t)
nepi(~ )= θk exp(ηi(gi - gk)) -θi _ rιξi
ηi k(t)=Plθiexp(ηi(gi-gii))	θt,k	ηiξt,k
≤θti,ke-ηigki(θ~t) - θti,k -ηiξti,k
≤e max(C1,C2) η2k~∣2
θk exp(ηi(gi - gk))
Pl θi exp(ηi(gi - gi))
- θti,k - ηiξti,k
≥	θi,k + η ξi,k
≥ 1+ ed max2c1,c2)-η2k~∣1
- θti,k - ηiξti,k
≥-(θi,k + ηiξi,k) (edmax2C1 ,C2)η2k~l2
(by eq. (21))
(by eq. (20))
(by eqs. (20) and (21))
(1/(1 + X) ≥ 1 - X)
≥ - edmax(C1,C2)ηi2 ∣ξ~∣12	(θti,k ≤ 1 andηiξti,k ≤ 1 by eq. (12))
Therefore, with C3 := edmax(C1, C2) we finish the proof.
□
Claim D.3. There exists e2 > 0 so that for all 瓦 ∈ Θn and δι > 0 so that ∣∣ξ(~t)kι ≥ 2√δ1, and
∣ξk(~t)l ≤ δι forall i ∈ [n] and k ∈ Si, then Φ(~t+ι) - Φ(~t) ≤ e2k~(~t)k2∙
Proof for claim D.3. By claim C.1,
φ(~t+ι) - φ(~t) ≤	9P iʌ i^i llξtkι + maxλiη2 max |gk(θ)1 ∙ k~tkι + d2C4 I∣θt+1 -矶.
2 i λiηi	1 i i i,k,θ~	1
-I-V T	F	1 .1 1 .	.	F	1 ∙ iʌ -» I	.1	1 .	Il → Il	TC 11 ʃ* 11 9 I - . 1
We can bound the later two terms by claim D.2. For the second term, ∣e~t ∣1 ≤ ndC3 ∣ξt ∣12. For the
third term,
∣∣~t+ι-~t∣∣1 ≤ X(ηi∣ξi,kl + C3η2k~kl)
i,k
≤maxηi ∣ξ~∣1 + ndC3 max ηi2 ∣ξ~∣12.
ii
≤2 max ηi ∣ξ~∣1	(byeq. (13))
i
17
Under review as a conference paper at ICLR 2022
With above inequalities, by eq. (14), We have ：P ；) > maxi λiη22 max,卜 ~ |g*(~)∣ndC3 and
4ΓP λi U > 4d2C4 max, η2. Therefore there exists a constant
€2 ：= mni Bi ηi——max λiη2 max |gk (~)∣ndC3 — 4d2C4 max ni > 0
2	i λiηi	i	i,k,θ~	i
sothatΦ(θt+ι) — Φ(θt) < —E2∣∣ξtk2∙	口
Proofoflemma 4.5. To prove limt→∞ ~t = ~*, there are two equivalent ways to measure the
progress, besides ∣∣θt — ~* k 1. First because 铲 is an isolated fixed point of eq. (6), ~t converges to
θ* if and only if limt→∞ ξ(θt) = 0.On the other hand, because Φ is strictly convex and and θ* is
the minimum point, θt converges to θ* if and only if limt→∞ Φ(θt) = mιn~ Φ(θ). With these two
equivalent conditions, given any € > 0, there exists δ > 0 so that ∣~ — ~*∣ι < € when
θ~ ∈ Vδ :=	θ~ : Φ(θ~) ≤ max Φ(θ~0) .
1	~0"∣~(~0)kι≤δ	J
Therefore, it is sufficient for us to show for all δ > 0, there is tδ so that θ~t ∈ Vδ for all t ≥ tδ . With
technical claim D.1 to D.2, our proof has two parts. First we show that the dynamic hits Vδ . Then,
the dynamic stays in Vδ .
For the first part, given δ > 0 and 0 < C < 1, by claim D.1 there exists T1 such that each vanishing
component ∣ξi 卜| ≤ C2δ2∕4 for all t ≥ Ti. Then by claim D.3, there exists T2 ≥ Ti such that
∣ξT2 ∣i ≤ Cδ. Otherwise, the value of Φ decreases by a nonzero constant €2 ∣ξt ∣2i ≥ C€2 δ2 > 0 at
each round which contradicts that the minimum of Φ bounded.
For the second part, if ∣ξt ∣i ≤ Cδ ≤ δ for all t ≥ T2, then we finish the proof. Other-
wise, there exists τ ≥ T2 so that ∣ξτ ∣i ≤ Cδ < ∣ξτ+i ∣i. Now we prove that Φ(θτ+i) ≤
max~0∙k~(~0)k]≤δ Φ(θ0). Because the difference between θτ +1 and θτ is ∣∣θτ+1 — θτ∣∣ι ≤
max, ηikξT ||i + max, η211 ~T ||i ≤ 2 max, η, ∣ ~r ||i when η* is small enough, and ~ is a Lξ -Lipschitz
θ for some constant Lξ in one norm, we have
_ . . .. _>	- .. -*	—* — .
Cδ < ∣ξτ+1∣1 ≤ kξτkι + kξτ+1 — ξτki ≤ kξτkι + 2Lξmaxη,∣ξτki ≤ δ, (22)
,
when C is small enough so that C(1 + 2Lξ max, η,) ≤ 1. Therefore, ∣ξT+1k1 ≤ δ and Φ(θT+1) ≤
max~0.k~(~0)k ≤δ Φ(~0). Finally, because ∣ξ,卜| ≤ C2δ2∕4 for all t ≥ T + 1, by claim D.3, the
potential function is decreasing for all t ≥ T + 1, unless ∣∣ξ∣∣t ≤ Cδ. Both make the potential
function less than max~0:口~(~0)∣∣ɪ <§ Φ(~0) which completes the proof.	口
E Proof and Detail for theorem 4.6
Proofoflemma 4.7. Define PL(X) := a(L)(x—β(L)) which is increasing for all L because α(L) >
0. With PL, fα(L),β(L)(X) = χ+(1-χ)eXp(PL(χ)) . Take XI(L) = x1 := 1 - 1∕α(L), x2(L) = x2 : =
fα(L),β(L)(x1(L)), and x3(L) = x3 := fα(L),β(L)(x2(L)). We will define x0(L) later. We will
omit L and use X1 , X2, and X3 .
To define X0(L), we set y(L) := β(L)∕2, and want to show
fα(L),β(L)(y) > X1,	(23)
which is equivalent to (α(L) — 1)(1 — y(L)) exp(PL(y(L))) < y(L). When L is large enough, we
have β(L) ∈ (2∞, 4β3∞). Additionally because α(L) > 0, we have PL(y(L)) = — ɪα(L)β(L) <
—1 α(L)β∞. Therefore, we have (α(L) — 1)(1 — y(L))epL(y(L)) ≤ α(L)e-α(L)β∞ < β∞ <
y(L) when L is large enough, and prove eq. (23). On the other hand, because β(L) < 3β∞ and
18
Under review as a conference paper at ICLR 2022
PL(β(L)) = 0, Wehavefa(L),β(L)(β(L)) = β(L) < 2(β∞ + 1). Moreover,1 (β∞ + 1) < 1-忐,
holds when L is large enough. These two imply
fα(L),β(L)(β(L)) = β(L) < x1(L).	(24)
Combining eqs. (23) and (24), by intermediate value theorem, there exists x0 such that
fα(L),β(L) (x0) = x1 With y < x0 < β(L) < x1.	(25)
Now we show x3 < x°. With 1-xι(L) = 1∕α(L) and 0 < xι ‹ 1, we have x2 = fa(L),β(L) (xι)=
xι+α(L)-ιxexp(PL(xι)) ≤ expOSL)) ∙ Because β∞ < 1/2, When L is large enough, XI(L)=
1 - aιL) > 1 (2β(L) + 3) and, thus, Pl(xi) > α(L) ɑ - β2L)). Therefore, we have
χ2 ≤ α(L)eχp (-α(L) (4 -尸2)))	(26)
Finally, because PL(x2) ≥ PL(0) = -β(L), we get
x3 =fa(L),β(L)(X2)≤ χ2 + (1- χ2)exp(-α(L)β(L))
x2 exp(α(L)β (L))
1 + x2(exp(α(L)β(L)) - 1)
≤X2 exp(α(L)β(L))	(since exp(α(L)β(L)) > 1 when L is large)
≤α(L)eχp (α(L) (β(L)- (4 -队2))))	(eq.(26))
_小	(3α(L)乙小 nʌ
=α(L) eχp ( —2 — Ie(L) - 2 11
Finally, because β∞ < 1/2, X3 converges to 0 when L is large enough. Therefore,
X3 < y < X0.	(27)
By eqs. (25) and (27), we have χ3 < xo < xi which completes the proof.	口
19