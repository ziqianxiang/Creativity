Under review as a conference paper at ICLR 2022

GENERALIZABLE LEARNING TO OPTIMIZE INTO WIDE

VALLEYS

Anonymous authors

Paper under double-blind review

ABSTRACT

Learning to optimize (L2O) has gained increasing popularity in various optimiza-
tion tasks, since classical optimizers usually require laborious, problem-specific
design  and  hyperparameter  tuning.   However,  current  L2O  approaches  are  de-
signed  for  fast  minimization  of  the  objective  function  value  (i.e.,  training  er-
ror),  hence  often  suffering  from  poor  generalization  ability  such  as  in  training
deep neural networks (DNNs), including (i) disappointing performance across un-
seen optimizees (optimizer generalization); (ii) unsatisfactory test-set accuracy of
trained DNNs (optmizee generalization).  To overcome the limitations, this paper
introduces flatness-aware regularizers into L2O for shaping the local geometry
of  optimizee’s  loss  landscape.   Specifically,  it  guides  optimizee  to  locate  well-
generalizable minimas in large flat regions of loss surface, while tending to avoid
sharp valleys.  Such optimizee generalization abilities of flatness-aware regular-
izers have been proved theoretically.  Extensive experiments consistently validate
the effectiveness of our proposals with substantially improved generalization on
multiple sophisticated L2O models and diverse optimizees.  Our theoretical and
empirical results solidify the foundation for L2O’s practically usage.  All codes
and pre-trained models will be shared upon acceptance.


1    INTRODUCTION

One  cornerstone  of  deep  learning’s  success  is  ar-
guably  the  stochastic  gradient-based  optimization
methods,  such  as  SGD  (Robbins  &  Monro,  1951),
Adam (Kingma & Ba, 2014), AdaGrad (Duchi et al.,
2011), RProp (Riedmiller & Braun, 1993), and RM-
SProp (Tieleman & Hinton, 2012).  The performance

Learnable
Optimizer

Flatness-aware

Regularizer


of deep neural networks (DNNs) hinges on the choice
of  optimization  methods  and  the  corresponding  pa-
rameter settings. Thus, intensive human labor is often
required  to  empirically  select  the  best  optimization

Update
Rule

Training
Dynamics


method and its parameters for each specific problem.

A seemingly promising data-driven approach, learn-
ing to optimize (L2O), arose from the meta learning
community to alleviate this issue.  It aims to replace
traditional optimizers tuned by human hand with neu-

Optimizee
NNs

Figure 1:  The pipeline illustration of our pro-
posed Flatness-aware regularizer in L2O. Loss
plots are credits to Li et al. (2018).

ral network based optimizers that can learn update rules from data.  Existing works have demon-
strated that a learned optimizer is not only able to decrease the objective function faster, but 
also
able to tremendously reduce the required human labor.  Andrychowicz et al. (2016a) first proposed
to parameterize the update rules using a long short-term memory (LSTM) network.   The LSTM
optimizer tries to simulate the behavior of iterative methods by unrolling.  By aggregating the loss
of    the function to be optimized (optimizee) at each time step, it aims to minimize the overall 
loss
along the optimization path.  Wichrowska et al. (2017) enlarged the optimizer neural network to a
hierarchical recurrent neural network (RNN) architecture, which improves its capability on larger
or  unseen optimization problems. Li & Malik (2016) also proposed a reinforcement learning based
approach for this sub-field.

1


Under review as a conference paper at ICLR 2022

Although L2O methods are able to achieve better performance than analytical optimizers in many
traditional optimization tasks, they are yet mature to serve as practical optimizers for deep neural
networks.  The main reason is that all existing L2O methods are designed for solving traditional
optimization problems where the generalization ability is not a concern. Generalization ability, one
of the core problems in machine learning, is neither guaranteed for deep neural networks optimized
by L2O, nor studied by literature in the context of L2O before.  It has been shown that for today’s
heavily overparameterized networks, it is easy to memorize the entire training data which lead to
zero  training  loss  (Zhang  et  al.,  2021).   Therefore,  for  a  L2O  method,  fast  
minimization  of  the
training error cannot really lead to generalization ability of the optimizee.

The generalization ability of machine learning methods has been extensively studied both theoret-
ically and empirically in the past decades (Keskar et al., 2017; Hochreiter & Schmidhuber, 1997;
1994; Jiang et al., 2020; Chaudhari et al., 2017; Damian et al., 2021). Among them, a prevailing 
the-
ory that links the generalization ability of models to the geometry of the loss landscape has 
recently
shown its empirical effectiveness.  Most methods taking this idea measure the geometry of the loss
landscape using Hessian spectrum, while some others use the local entropy (Damian et al., 2021;
Chaudhari et al., 2017). Further, Foret et al. (2021) proposes the SAM method, which minimizes the
loss value and the loss sharpness simultaneously.  Specifically, an optimization method that prefers
to converge to wide valleys (i.e., flat basin) in loss landscape shows better generalization 
ability.

Inspired by this, we propose flatness-aware regularizers i.e., Hessian regularizer and Entropy reg-
ularizer for learning to optimize.  The Hessian regularization is the pseudonorm of the optimizee’s
Hessian matrix and the Entropy regularization measures the local Entropy of the optimizee. Both of
them characterize the geometry of the loss landscape which intends to teach optimizer favor update
rules that lead to wide valleys in loss landscape.  To summarize, the contributions of this paper 
can
be outlined below:

•  We propose to use the flatness-aware regularizers in the training of L2O optimizers.  The
update rules learned with such regularizers would favor converging to wide valleys in loss
landscape when minimizing loss functions.  Thus, L2O optimizers trained with flatness-
aware regularizers can be deemed as a plug-and-play optimizer that favors generalization
and requires no more time calculating Hessian or Entropy information while in use.

•  We demonstrate theoretically that adopting flatness-aware regularizers in L2O can enhance
the generalization ability of optimizees trained by regularized optimizers. Note that the the-
oretical result of Entropy regularizer implies Entropy-SGD favors wide valleys in original
loss landscape rather than only in Entropy energy landscape.

•  Comprehensive experiments over various tasks demonstrate the effectiveness of our meth-
ods. Empirical results show that our methods significantly improve the generalization abil-
ity of existing L2O methods, enabling them to outperform current state-of-the-art by a large
margin.

2    RELATED  WORK

Learning to Optimize (L2O)    As a special case of learning to learn, L2O has been widely inves-
tigated in various machine learning problems, including black-box optimization (Chen et al., 2017),
Bayesian swarm optimization (Cao et al., 2019), minmax optimization (Shen et al., 2021), domain
adaptation (Li et al., 2020; Chen et al., 2020b), adversarial training (Jiang et al., 2018; Xiong &
Hsieh, 2020), graph learning (You et al., 2020), and noisy label training (Chen et al., 2020c).  The
first L2O framework dates back to Andrychowicz et al. (2016b), in which the gradients and update
rules of optimizee are formulated as the input features and outputs for a RNN optimizer, 
respectively.
It proposes a coordinate-wise design which enables trained L2O to be applicable for different neural
networks with diverse amount of parameters.  Li & Malik (2016) proposes an alternative reinforce-
ment learning frameworks for L2O, leveraging gradient history and objective values as observations
and step vectors as actions.   Later on,  more advanced variants arise to power up the generaliza-
tion ability of L2O. For example, (i) regularizers like random scaling, objective convexifying (Lv
et al., 2017), and Jacobian regularization (Li et al., 2020), (ii) enhanced L2O model like 
hierarchical
RNN architecture (Wichrowska et al., 2017), and (iii) improved training techniques like curriculum
learning and imitation learning (Chen et al., 2020a). A more comprehensive literature is referred to
a recent survey paper of learning to optimize.

2


Under review as a conference paper at ICLR 2022

Flatness on Generalization of Neural Network    Generalization analysis of neural networks has
been widely studied by various methods, including VC-dimension (Bartlett et al., 2019), covering
number (Bartlett et al., 2017), stability (Hardt et al., 2016; Zhou et al., 2018a), Rademacher com-
plexity (Golowich et al., 2018; Ji & Liang, 2018; Ji et al., 2021; Arora et al., 2018; 2019), etc.  
In
particular, the landscape flatness has been known to be associated with better generalization.  On
the empirical side, Keskar et al. (2017) and He et al. (2019) showed that minima in wide valleys
often generalize better than those in sharp basins. Further, Wilson et al. (2017) and Keskar & 
Socher
(2017) showed empirically that SGD favors better generalization solutions than Adam. On the the-
ory side, Zhou et al. (2020) showed that SGD is more unstable at sharp minima than Adam and
explained why SGD generalize better than Adam theoretically and Zou et al. (2021) explained that
the  inferior  generalization  performance  of  Adam  is  connected  to  nonconvex  loss  
landscape.   To
improve the generalization performance, Entropy-SGD was introduced in Chaudhari et al. (2017)
which was shown to outperform SGD in terms of the generalization error and the training time.
Meanwhile,  the  spectral  norm  regularization  has  been  proposed  in  Yoshida  &  Miyato  
(2017)  to
improve the generalization ability of neural networks empirically.

3    METHODOLOGY

In this section, we provide basic notations and the detailed formulations about our flatness-aware

regularizers, i.e., Hessian and Entropy regularizers.

3.1    PRELIMINARY

We define F ⁽¹⁾(θ; ξ) and F ⁽²⁾(θ; ζ) respectively as the non-negative meta-traning and meta-testing
functions, where θ      Rp  is the optimizee parameter, and ξ and ζ respectively denote training and
testing data samples.  Suppose there are N  training data samples ξ        ξi, i = (1, . . . , N )  
 and M
testing data samples ζ        ζj, j  =  (1, . . . , M )  .  Then we define the empirical 
meta-training and
meta-testing functions and their corresponding population risk functions as follows:

N           N                      i                                 ξ
i=1

M

(Meta-Testing) L⁽²⁾(θ) =   1  Σ F ⁽²⁾(θ; ζ  ),     L⁽²⁾(θ) = E  F ⁽²⁾(θ; ζ).          (2)

An L2O algorithm aims to learn an update rule for optimizee θ based on the meta-training function.
An update rule can be expressed as θ⁽¹⁾ (φ)  =  θ⁽¹⁾(φ) + m(z⁽¹⁾; φ),  where t  =  0, 1, . . . , T  
−

1  denotes  the  iteration  index  over  one  epoch,  the  variable  z  captures  the  information  
(e.g.,  loss

values, gradients) that we collect on the optimization path, and the optimizer function m(z; φ) is
parameterized by φ and captures how the update of the optimizee parameter θ depends on the loss
landscape information included in z. In order to find a desirable optimizer parameter φ, L2O solves
the following meta-training problem:

min{L⁽¹⁾(θ⁽¹⁾(φ))}      where     θ⁽¹⁾ (φ) = θ⁽¹⁾(φ) + m(z⁽¹⁾; φ).                      (3)

A popular L2O meta-training algorithm applies the gradient descent method, which updates φ based
on the gradient of the objective function L⁽¹⁾(θ⁽¹⁾(φ)) with respect to φ.  As suggested by eq. 
(3),

N      T

each update of φ requires T  iterations of the optimizee parameter θ⁽¹⁾(φ) to obtain θ⁽¹⁾(φ).

0                                       T

In meta-testing, we apply the output φ of meta-training and its corresponding optimizer to update
the optimizee as θ⁽²⁾ (φ) = θ⁽²⁾(φ) + m(z⁽²⁾; φ)(t = 0, 1, . . . , T  − 1).  Note that we 
differentiate

the optimizee updates in training and testing by superscripts (1) and (2), respectively.

3.2    HESSIAN REGULARIZER

Motivated by the idea that the optimizee in a flat area of the training objective has superior 
general-
ization capability, we propose to incorporate flatness-aware regularizers into L2O meta-training, in
order to learn optimizers that favors to land the optimizee into a flat region. We introduce two 
such
regularizers in this and next subsection.

3


Under review as a conference paper at ICLR 2022

The first regularizer we introduce is based on the spectral norm of the Hessian, smaller values of
which corresponds to a flatter landscape. Thus, the new L2O meta-training objective is given by:

min{L⁽¹⁾(θ⁽¹⁾(φ)) + λǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ}      where     θ⁽¹⁾ (φ) = θ⁽¹⁾(φ) + m(z⁽¹⁾; φ),     (4)

where λ is the regularizer hyperparameter. Note that the Hessian regularizer is adopted for 
training

the optimizer parameter φ, and its impact on the update rule m(z⁽¹⁾; φ) is only through φ, i.e., the
information in zt does not include such regularization. We let φ∗ be the optimal optimizer 
parameter,
which can be written as

φ∗ = arg min{L⁽¹⁾(θ⁽¹⁾(φ)) + λǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ}.                               (5)

N      T                          θ    N      T

φ

Due to the computational intractability of directly penalizing ∇2L⁽¹⁾(θ⁽¹⁾(φ)), we investigate 
three

approximation variants in the implementation.  © Hessian EV: the eigenvalue of largest module of
Hessian matrix, computed by power iteration (Yao et al., 2020); ? Hessian Trace: the trace of Hes-
sian matrix, calculated via Hutchinson method (Yao et al., 2020); © Jacobian Trace:  the trace of
Hessian’s Jacobian approximation ∇θL⁽¹⁾(θ⁽¹⁾(φ))T∇θL⁽¹⁾(θ⁽¹⁾(φ)).  Note that such Hessian ap-

proximation methods do not involve computing Hessian explicitly which helps to reduce the memory

and computational cost. In our case, we perform 10 iterations for Hessian norms’ approximation.

3.3    ENTROPY REGULARIZER

The second flatness-aware regularizer we incorporate to L2O is based on the local entropy function

G⁽¹⁾(θ; γ) = log ∫   exp .−L⁽¹⁾(θ′) −  γ ǁθ − θ′ǁ2Σ dθ′ proposed in Chaudhari et al. (2017).  Due

to the exponential decay with respect to ǁθ − θ′ǁ2, the integral mainly captures the value of the 
loss
function L⁽¹⁾(θ′) over the neighborhood of θ. Thus, the value of G⁽¹⁾(θ; γ) measures the flatness 
of

N                                                                                                   
    N

the local area around θ. Thus, the L2O meta training objective with Entropy regularizer is given 
by:

min{L⁽¹⁾(θ⁽¹⁾(φ)) − λG⁽¹⁾(θ⁽¹⁾(φ); γ)}      where     θ⁽¹⁾ (φ) = θ⁽¹⁾(φ) + m(z⁽¹⁾; φ).     (6)

We let φ∗ be the optimal optimizer parameter, which can be written as

φ∗ = arg min L⁽¹⁾(θ⁽¹⁾(φ)) − λG⁽¹⁾(θ⁽¹⁾(φ); γ).                                   (7)

N      T                        N      T

φ

In   order   to   implement   the   gradient   descent   algorithm   for   meta-training,    the   
gradient

(1)                                                                                                 
                                       (1)

φGN (θT   (φ); γ) can be calculated by the entropy gradient        θGN  (θ; γ) and the chain rule.

In particular, as given in Chaudhari et al. (2017), the entropy gradient takes the following form

−∇θG⁽¹⁾(θ; γ) = γ(θ − E[θ′; ξ]),                                               (8)
where  ξ   ∈   {ξi, iΣ =   (1, . . . , N )}  are  traiΣning  samples  and  the  distribution  of  
θ′  is  given  by

	

4    THEORETICAL  ANALYSIS

In this section, we first introduce several assumptions, and then present the generalization 
analysis
of L2O with Hessian and Entropy regularizers.

4.1    ASSUMPTIONS

We first define the local basin of θ with the radius d as Dᵈ(θ) =   θ′ :   θ    θ′  ₂      d  . As 
have been
observed widely in training a variety of machine learning objectives, the convergent point enters 
into
a  local neighborhood where the strong convexity (or similar properties such as gradient dominance

condition, reguarity condition, etc) holds (Du et al., 2019; Li & Yuan, 2017; Zhou et al., 2018b;
Safran & Shamir, 2016; Milne, 2019). We thus make the following assumption on the geometry of
the meta-training function.

4


Under review as a conference paper at ICLR 2022

Assumption  1.  We  assume  that  there  exist  a  a  local  basin  Dᵈ(θ⁽¹⁾(φ))(d  >  0)  of  the  
conver-
gence point θ⁽¹⁾(φ) that in such local basin, L⁽¹⁾(θ) and L⁽¹⁾(θ) are µ-strongly convex w.r.t.  θ.

T                                                                                        N

Futhermore, there exist a unique optimal point θ∗⁽¹⁾ of function L⁽¹⁾(θ) and a optimal point θ∗⁽¹⁾

of function L⁽¹⁾(θ) in local basin Dᵈ(θ⁽¹⁾(φ∗)).

N                                               T

Assumption 2.  We assume that L⁽¹⁾(θ) function is M-Lipschitz;  ∇θL⁽¹⁾(θ) and ∇θL⁽²⁾(θ) are

L-Lipschitz; ∇2L⁽¹⁾(θ), ∇2L⁽²⁾(θ), ∇2L⁽¹⁾(θ) and ∇2L⁽²⁾(θ) are ρ-Lipschitz.

We further adopt the following assumptions introduced in Mei et al. (2018), in order to guarantee
the similarity between the landscape of the empirical and population objective functions.

Assumption 3.  The gradient of the training loss    F ⁽¹⁾(θ; ξ) is τ ²-sub-Gaussian and the Hessian
of the loss function is τ ²-sub-exponential. See Appendix B.1 for more details.

Assumption 4.  There exist a basin Dʳ(0) that the meta-training functions L⁽¹⁾(θ) is (ϵ, η)-strongly
Morse in Dʳ(0) and the local basins Dᵈ(θ⁽ⁱ⁾(φ∗)) for i = 1, 2 of convergence points θ⁽ⁱ⁾(φ∗)(i =

T                                                                                   T

1, 2) are in Dʳ(0)

4.2    GENERALIZATION ANALYSIS OF HESSIAN-REGULARIZED L2O

In this section, we adopt the optimizer learned by regularized L2O to train a new optimizee, and
analyze the advantages of the Hessian regularizer on the optimizee generalization ability.

We first note that by optimization theory, the regularized optimization problem eq. (5) is 
equivalent
to the following constrained optimization


min{L⁽¹⁾(θ⁽¹⁾(φ))}      where     θ⁽¹⁾

= θ⁽¹⁾ + m(z⁽¹⁾; φ)

subject to ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ ≤ BHₑssiₐn(λ),                                           (9)

where  BHₑssiₐn(λ)  is  the  constraint  bound  on  the  Hessian  determined  by  λ.    Thus,  the  
opti-
mizer  parameter  φ∗  learned  by  the  Hessian-regularized  L2O  meta-training  in  eq.  (5)  is  
also  a
solution  to  eq.  (9),  i.e.,  its  Hessian  satisfy  the  constraint.    Then  let  θ⁽²⁾(φ∗)  
denote  the  opti-
mizee  parameters  trained  by  optimizer  φ∗  in  meta-testing,  and  θ∗⁽²⁾  denote  the  optimal  
point
of the population meta-testing function L⁽²⁾(·).   We then characterize the generalization error as

L⁽²⁾(θ⁽²⁾(φ∗))     L⁽²⁾(θ∗⁽²⁾), which capture how well the optimizer φ∗ performs on a testing task
with respect to the best possible testing loss value.

The following theorem characterizes the generalization performance of the optimizee trained with
Hessian regularized optimizer as defined above.

Theorem 1 (Generalization Error of Hessian-Regularized L2O).  Suppose Assumptions 1, 2, 3, 4

hold.   We  let  N   ≥   max{4Cp log N/η², Cp log p}  where  C   =  C₀ max{ch, 1, log( ʳτ )},  η²   
=

		

η       η    

min{ s2       ,        }  and  C₀  is  an  universal  constant.   Then,  with  probability  at  
least  1 − 2δ  we


have

L⁽²⁾(θ⁽²⁾(φ∗)) − L⁽²⁾(θ∗⁽²⁾) ≤  1 A²A  ,                                        (10)

where A₁  = BHₑssiₐn(λ) + ρ∆∗ + ρ∆∗ + ∆∗  + O(wT −T ' ) + O(. C log N ), A₂  = ∆∗ + ∆∗ +


O(wT −T ' ) + O(. C log N ),  with w  =   L−µ ,  ∆∗

=  ǁ∇2L(2)(θ∗(2)) − ∇2L(1)(θ∗(1))ǁ,  ∆∗  =


ǁθ∗(1)  − θ∗(2)ǁ, ∆∗

= ǁθ⁽²⁾(φ∗) − θ⁽¹⁾(φ∗)ǁ, and T ′ is the minimum gradient descent iterations

for θ⁽¹⁾(GD) to enter into the local basin of θ⁽¹⁾(φ∗).

T '                                                                                         T

In Theorem 1 the generalization error is bounded by the terms A₁ and A₂, where the Hessian regular-
izer affects the generalization error through the constraint BHₑssiₐn(λ) in A₁. Clearly, by 
choosing the
regularization hyperparameter λ, we control the value of BHₑssiₐn(λ), which then makes an impact
on         the generalization.  Specifically, larger λ corresponds to smaller BHₑssiₐn(λ) and hence 
smaller
generalization error.  This also explains that flatter landscape (i.e., smaller BHₑssiₐn(λ) on 
Hessian)
yields  better generalization performance (i.e., smaller generalization error).

5


Under review as a conference paper at ICLR 2022

The generalization error in Theorem 1 also contains other terms which we explain as follows:  (a)
ρ∆∗T  + ρ∆∗θ + ∆∗H  capture the similarity between the training and testing tasks; more similar 
tasks
yields better generalization; (b) O(wT −T ' ) captures the exponential decay.rate of the 
optimizee’s

	

the difference between the empirical and population loss functions, and vanishes as the sample size

N gets large.

4.3    GENERALIZATION ANALYSIS OF ENTROPY-REGULARIZED L2O

In  this  section,  we  analyze  the  generalization  error  of  the  Entropy  regularizer  on  the 
 optimizee
generalization ability.

Similarly to the Hessian regularizer, the regularized optimization problem eq. (6) is equivalent to 
the
following constrained optimization:

minL⁽¹⁾(θ⁽¹⁾(φ))     where     θ⁽¹⁾  = θ⁽¹⁾ + m(z⁽¹⁾; φ)


φ      N      T

t+1           t                  t

subject to  − G⁽¹⁾(θ⁽¹⁾(φ); γ) ≤ BEntrₒpy(λ),                                       (11)

where BEntrₒpy(λ) is the constraint bound on the Entropy determined by λ.  Thus, the optimizer φ∗
learned by the Entropy-regularized L2O meta-training in eq. (6) is also a solution to eq. (11), 
i.e.,
the local entropy satisfies the constraint.

In the following, we first establish an important connection between the Entropy constraint bound

BEntrₒpy(λ) and the Hessian ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ of the optimizee.

Theorem 2 (Connection between Hessian and Entropy Regularizer).  Suppose Assumptions 1 and 2


hold.  We define C(γ, p, m)  =  log

θ ∈ Rp. Then, we have

θ':ǁθ'−θǁ>m

exp(− γ ǁθ − θ′ǁ2)dθ′ where m is a constant and

ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ ≤ D−¹(BEntrₒpy(λ))

where D(x) = L⁽¹⁾(θ⁽¹⁾(φ)) + (p − 1) log(γ + µ) − mM −  p log(2π) −  1 ρm³ − C(γ, p, m) +

log(x + γ) and D−¹(x) denotes the inverse function of D(x).

It can be observed that the function D(x) in monotonically increasing w.r.t. x and so is its inverse
D−¹(x), as determined by the only x-dependent term log(x + γ). Hence, Theorem 2 shows that the
entropy bound constraint BEntrₒpy(λ) implies a corresponding Hessian constraint D−¹(BEntrₒpy(λ)).
Thus, Theorem 1 can be applied to provide a bound on the generalization error here.

Corollary 1 (Generalization Error of Entropy-Regularized L2O).  Suppose the same conditions of
Theorem 1 hold.  Then the generalization error of L2O with Entropy regularizer takes the bound in
eq. (10) with BHₑssiₐn(λ) in A₁ being replaced by D−¹(BEntrₒpy(λ)).

Theorem 2 and Corollary 1 establish that the bound D−¹(BEntrₒpy(λ)) serves the same role as the
Hessian bound in the generalization performance.   Thus,  by controlling the hyperparameter λ to
be large enough in the L2O training, BEntrₒpy(λ) as well as D−¹(BEntrₒpy(λ)) and Hessian can be
controlled to be sufficiently small. In this way, the optimizee will be landed into a flat basin 
(due to
small Hessian) to enjoy better generalization.

To  compare  with  the  result  in  Chaudhari  et  al.  (2017),  we  note  that    Chaudhari  et  
al.  (2017)
proposed  the  Entropy-SGD  method  and  showed  that  Entropy-SGD  favors  better  generalization
solutions  in  terms  of  the  Entropy  energy  landscape.   As  a  comparison,  Theorem  2  and  
Corol-
lary     1  establish  the  generalization  error  for  Entropy-SGD  in  terms  of  the  
un-regularized  loss
L⁽²⁾(θ⁽²⁾(φ∗))      L⁽²⁾(θ∗⁽²⁾) in original loss landscape, which is the ultimate goal of 
generaliza-

tion.  Such a favorable result is established by exploiting the equivalence between the regularized

optimization and the un-regularized constrained optimization problems.

We further note that Dinh et al. (2017) theoretically shows that sharp minimas can also generalize
well for deep neural networks.  However, such a result does not contradict the fact that flat minima
generalizes well, which has strong evidence (He et al., 2019; Keskar et al., 2017), and is the 
property
that     we exploit in the paper.

6


Under review as a conference paper at ICLR 2022


3.0

2.5

2.0

Conv-MNIST

SGD
ADAM

L2O-Scale

L2O-Scale + Hessian Trace

3.0

2.5

2.0

Conv-CIFAR

3.0

2.5

2.0

MLP-ReLU

3.0

2.5

2.0

Conv-Large-MNIST


1.5

1.5

1.5

1.5


1.0

1.0

1.0

1.0


0.5

0.5

0.5

0.5


0.0

0          2000       4000       6000       8000      10000

Training Iterations

0.0

0          2000       4000       6000       8000      10000

Training Iterations

0.0

0          2000       4000       6000       8000      10000

Training Iterations

0.0

0          2000       4000       6000       8000      10000

Training Iterations


1.0

0.6

1.0

1.0


0.8

0.5

0.8

0.8


0.6

0.4

0.6

0.6


0.4

0.2

SGD
ADAM

L2O-Scale

L2O-Scale + Hessian Trace

0.3

0.2

0.1

0.4

0.2

0.4

0.2


0          2000       4000       6000       8000      10000

Training Iterations

0          2000       4000       6000       8000      10000

Training Iterations

0          2000       4000       6000       8000      10000

Training Iterations

0          2000       4000       6000       8000      10000

Training Iterations

Figure 2:  Comparison of the training loss/testing accuracy of optimizees trained using analytical 
optimizers
and L2O-Scale (Wichrowska et al., 2017) with/without the proposed Hessian regularization.


3.0

2.5

2.0

Conv-MNIST

SGD
ADAM

L2O-DM-CL

L2O-DM-CL + Hessian Trace
L2O-DM-CL + Entropy

3.0

2.5

2.0

Conv-CIFAR

3.0

2.5

2.0

MLP-ReLU

3.0

2.5

2.0

Conv-Large-MNIST


1.5

1.5

1.5

1.5


1.0

1.0

1.0

1.0


0.5

0.5

0.5

0.5


0.0

0          2000       4000       6000       8000      10000

Training Iterations

0.0

0          2000       4000       6000       8000      10000

Training Iterations

0.0

0          2000       4000       6000       8000      10000

Training Iterations

0.0

0          2000       4000       6000       8000      10000

Training Iterations

1.0


0.8

0.6

0.5

0.4

0.8

0.6

1.0

0.8

0.6


0.4

0.2

SGD
ADAM

L2O-DM-CL

L2O-DM-CL + Hessian Trace
L2O-DM-CL + Entropy

0.3

0.2

0.1

0.4

0.2

0.4

0.2

0.0


0          2000       4000       6000       8000      10000

Training Iterations

0          2000       4000       6000       8000      10000

Training Iterations

0          2000       4000       6000       8000      10000

Training Iterations

0          2000       4000       6000       8000      10000

Training Iterations

Figure 3:  Comparison of the training loss/testing accuracy of optimizees trained using analytical 
optimizers
and L2O-DM-CL (Chen et al., 2020a) with/without the proposed Hessian regularization.

5    EXPERIMENT

In our experiments,  we consider two advanced L2O algorithms,  i.e.,  L2O-DM-CL¹  (Chen et al.,
2020a) and L2O-Scale (Wichrowska et al., 2017) on four diverse meta testing optimizees.

Meta Training Optimizees.    For training L2O-Scale, we use a three-layer convolutional neural
network (CNN) which has one fully-connected layer, and two convolutional layers with eight 3     3
and 5     5 kernels respectively.  For training L2O-DM, we adopt the same meta training optimizee
from Andrychowicz et al. (2016b), which is a simple Multi-Layer Perceptron (MLP) with one hidden
layer of 20 dimensions and the sigmoid activation function. MNIST (LeCun et al., 1998) dataset is
used for all the meta-training.

Meta Testing Optimizees.    We select four distinct and representative meta testing optimizees from
Andrychowicz et al. (2016b) and Chen et al. (2020a) to evaluate the generalization ability of the

¹It is an enhanced version of the earliest L2O-DM introduced by DeepMind Andrychowicz et al. 
(2016b).
We  choose  it  as  a  much  stronger  baseline  instead  of  the  vanilla  L2O-DM  with  
“poor-generalization  base-
line” (Wichrowska et al., 2017; Lv et al., 2017).

7


Under review as a conference paper at ICLR 2022

learned optimizer.  Specifically, © MLP-ReLU: a single layer MLP with 20 neurons and the ReLU
activation function on MNIST. ? Conv-MNIST: a CNN has one fully-connected layer, and two
convolutional layers with 16 3      3 and 32 5      5 kernels on MNIST. © Conv-Large-MNIST:
a large CNN has one fully-connected layer, and four convolutional layers with two 32 3      3 and
two 32 5     5 kernels on MNIST. № Conv-CIFAR: a CNN has one fully-connected layer, and two
convolutional layers with 16 3    3 and 32 5    5 kernels on CIFAR-10 (Krizhevsky & Hinton, 2009).
Optimizees ©, ?, and © are for evaluating the generalization of L2O across network architectures.
Then, № evaluates the generalization of L2O across both network architectures and datasets.

Training and Evaluation details.    During the meta training stage of L2O, L2O-Scale is trained
with 5 epochs, where the number of each epoch’s iteration is drawing from a heavy tailed distri-
bution (Wichrowska et al., 2017).  L2O-DM-CL is trained with a curriculum schedule of training
epochs and iterations, following the default setup in Chen et al. (2020a).  RMSprop with the learn-
ing rate 1 × 10−⁶  is used to update L2Os.  For the {Hessian, Entropy} regularization coefficients

{λHₑssiₐn, λEntrₒpy, γ}, we perform a grid search and choose {5 × 10−⁵,-,-}/{1 × 10−⁸,1 × 10−⁶,1 ×

10−⁴} for L2O-Scale/L2O-DM-CL.

In the meta testing stage of L2O, we compare our methods with classical optimizers like SGD and
Adam, and state-of-the-art (SOTA) L2Os such as L2O-Scale and L2O-DM-CL. Hyperparameters
of both classical optimizers and L2O baselines are carefully tuned through the grid search and all
other irrelevant variables are strictly controlled for a fair comparison. We run 10, 000 iterations 
for
the meta testing,  and the corresponding training loss and test accuracy on all unseen optimizees
are  collected  to  evaluate  the  optimizer  and  optimizee  generalization.   Note  that  the  
training  loss

corresponds to meta testing L⁽²⁾ and test accuracy corresponds to L⁽²⁾ in Section 3.1.  We conduct
ten independent replicates with different random seeds and report the average performance.  All of
our experiments are conducted on a computing facility of NVIDIA GeForce GTX 1080Ti GPUs.

5.1    IMPROVE GENERALIZATION WITH HESSIAN REGULARIZATION

In this section, we conduct extensive evaluations of our proposed Hessian regularization on previous
state-of-the-art L2O methods, i.e., L2O-Scale (Wichrowska et al., 2017) and L2O-DM-CL (Chen
et           al., 2020a).  Achieved training loss and testing accuracy are collected in Figure 2 
and 3 which
also  include  comparisons  with  representative  analytical  optimizers  like  SGD  (Ruder,  2016) 
 and
Adam (Kingma & Ba, 2014). Several consistent observations can be drawn from our results: ❶ Hes-
sian Trace regularizer consistently enhances the generalization abilities of learned L2Os and 
trained
optimizees.  Specifically, L2Os with Hessian Trace enable fast training loss decay and much lower
final   loss on all four unseen meta-testing optimizees, demonstrating the improved optimizer gen-
eralization ability.  Furthermore, all unseen optimizees trained by Hessian regularized L2Os enjoy
substantial testing accuracy which boosts up to 31%, showing the enhanced optimizee generalization
ability. Such impressive performance gains effective evidence of our proposal, which again suggests
that Hessian regularization potentially leads to well-generalizable minimas in wide valleys in loss
landscape.  ➎ Adopting vanilla L2O-DM-CL to train meta-testing optimizees (e.g., Conv-MNIST
and Conv-CIFAR) suffers from instability as shown in Figure 3, and it can be significantly mit-
igated by introducing our flatness-aware regularization.   Conv-Large-MNIST is an exception,
where the L2O-DM-CL fails to train this optimizee and ends up with random guessed accuracy, i.e.,
10%.  Although plugging Hessian Trace into L2O-DM-CL greatly improves its test accuracy from
10% to 95%+, it still undergoes a unsatisfactory training loss. Potential reasons may lie in the 
rough
model architecture and limited input features of L2O-DM-CL, coincided with the findings in Chen
et al. (2020a). We will further investigate this interesting phenomenon in the future. ⮊ For 
advanced
L2O-Scale, Hessian Trace regularization facilitates it to converge a significantly lower minima and
obtain considerable accuracy improvements.  It enlarges the advantages of L2O methods compared
to analytical optimizers, SGD and Adam, unleashing the power of parameterized optimizers.

5.2    IMPROVE GENERALIZATION WITH ENTROPY REGULARIZATION

We investigate the generalization ability improvements from the Entropy regularization. Generally,
it boosts both optimizee and optimizer generalizations of L2O in most cases, as shown in Figure 3.
Hessian v.s.  Entropy Regularization.    We compare our two kinds of flatness-aware regularizers

from both computational cost and performance benefits perspectives.  ❶ In order to calculate the

local entropy’s gradient in eq. (8), it involves gradients from multiple unroll steps for the 
estima-
8


Under review as a conference paper at ICLR 2022


1.1

1.0

Conv-MNIST

0.45

0.40

Conv-CIFAR

0.94

MLP-ReLU

1.0

Conv-Large-MNIST


0.9

0.8

0.7

0.6

0.5

0.4

0.3

1.1

1.0

0.9

0.8

L2O-DM-CL

L2O-DM-CL + Hessian EV
L2O-DM-CL + Jacobian Trace
L2O-DM-CL + Hessian Trace

0          2000       4000       6000       8000      10000

Training Iterations

0.35

0.30

0.25

0.20

0.15

0.10

0.7

0.6

0.5

0          2000       4000       6000       8000      10000

Training Iterations

0.93

0.92

0.91

1.0

0.9

0.8

0.7

0          2000       4000       6000       8000      10000

Training Iterations

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0          2000       4000       6000       8000      10000

Training Iterations


0.7

0.6

0.5

0.4

0.3

L2O-Scale

L2O-Scale + Hessian EV

           L2O-Scale + Jacobian Trace
L2O-Scale + Hessian Trace

0          2000       4000       6000       8000      10000

Training Iterations

0.4

0.3

0.2

0          2000       4000       6000       8000      10000

Training Iterations

0.6

0.5

0.4

0.3

0          2000       4000       6000       8000      10000

Training Iterations

0.6

0.4

0.2

0          2000       4000       6000       8000      10000

Training Iterations

Figure 4:  Comparison of the testing accuracy of optimizees trained using analytical optimizers and 
SOTA
L2O with/without different Hessian regularization, Hessian EV, Hessian Trace, and Jacobian Trace.

tion (Chaudhari et al., 2017), leading to extra memory and computing outlays. Compared to Hessian
augmented L2O, it costs      2.6x memory and      3x running time for L2O-DM-CL experiments².

➎ As for generalization gains, Entropy regularizer performs slightly better on Conv-MNIST and

Conv-CIFAR, while behaves marginally worse on MLP-ReLU and Conv-Large-MNIST com-
pared to Hessian regularizer. We would like to draw reader’s attention to Conv-Large-MNIST, in
which Entropy regularized L2O-DM-CL is capable of decaying the training loss and finding a much
lower minima than Adam.  Note that on this optimizee, both L2O-DM-CL and its Hessian variant
can not decrease the training loss.  The possible reason is that multi-layer convolutional neural 
net-
work without BN cannot be stably trained on MNIST. However, our L2O-DM-CL+Entropy is more
stable in training and improves testing accuracy compared with L2O-DM-CL. This indicates that
L2O-DM-CL + Entropy may also produce a more trainable loss surface for optimizees.

Based on the above experiments as well as the experiment on ResNet20 in Appendix A.1, we ob-
serve  that  L2O+Entropy  is  preferred  when  we  adopt  L2O  to  train  large  neural  networks,  
where
L2O+Entropy yields better optimizers and optimizee generalization abilities.  On the other hand,
L2O+Hessain optimizer requires less time per iteration to train and achieves lower training loss as
well          as higher test accuracy than L2O+Entropy in small networks, e.g. MLP. The possible 
reason is
that Entropy takes account of the landscape over a large range of loss to measure the flatness, and 
can
hence capture complex landscape information in large neural networks. On the other hand, Hessian
regularizer captures the flatness information only for the individual point, but in a more accurate
manner, and thus is more suitable to smaller neural networks with a relative simple landscape.

5.3    ABLATION AND VISUALIZATION

In this section, we carefully examine the effect of Hessian regularization’s different approximation
variants, including Hessian EV, Hessian Trace, and Jacobian Trace.  Results are presented in Fig-
ure   4.  We find that Hessian Trace regularizer achieves the most stable and substantial 
performance
boosts across all optimizees. Jacobian Trace performs the worst which is within expectation since it
provides the roughest estimation of Hessian.

6    CONCLUSION

In  this  paper,  we  propose  several  flatness-aware  regularizers  to  improve  both  optimizer  
and  op-
timizee generalization abilities of current state-of-the-art L2O approaches.   Such regularizers are
capable of shaping the local geometry of optimizee’s loss surface, and leading to well-generalizable
minimas in wide valleys which have been proved theoretically. Our empirical results validate the ef-
fectiveness of our proposal, taking a further step for L2O’s practically usage in real-world 
scenarios.

²We conduct entropy-related experiments on light-weight L2O-DM-CL rather heavy L2O-Scale models,
since RTX TITAN with 24G memory is the largest GPU we can access and afford.

9


Under review as a conference paper at ICLR 2022

7    REPRODUCIBILITY  CHECKLIST

To ensure reproducibility,  we use the Machine Learning Reproducibility Checklist v2.0,  Apr.   7
2020  (Pineau  et  al.,  2021).    An  earlier  version  of  this  checklist  (v1.2)  was  used  
for  NeurIPS
2019 (Pineau et al., 2021).

•  For all models and algorithms presented,

–  A clear description of the mathematical settings, algorithm, and/or model.  We
clearly describe all of the settings, formulations, and algorithms in Section 3.

–  A clear explanation of any assumptions.  All assumptions are stated in Section 4.1
and details are clearly explained in Appendix B.1.

–  An analysis of the complexity (time, space, sample size) of any algorithm. We do
not make the analysis.

•  For any theoretical claim,

–  A clear statement of the claim.  A clear statement of theoretical claims are made in
Section 4.2 and Section 4.3.

–  A complete proof of the claim.  The complete proofs of all claims are available in
Appendix B and Appendix C.

•  For all datasets used, check if you include:

–  The  relevant  statistics,  such  as  number  of  examples.    We  use  widely  adopted
datasets  MNIST  and  CIFAR-10  in  Section  5.      The  related  statistics  can  be
seen  at http://yann.lecun.com/exdb/mnist/ and https://www.cs.
toronto.edu/˜kriz/cifar.html.

–  The details of train/validation/test splits. We give this information in our repository

in the supplementary material.

–  An explanation of any data that were excluded, and all pre-processing step.  We
did not exclude any data or perform any pre-processing.

–  A link to a downloadable version of the dataset or simulation environment.  Our
repository contains all instructions to download and run experiments on the datasets.

–  For new data collected,a complete description of the data collection process, such
as instructions to annotators and methods for quality control.  We do not collect
or release new datasets.

•  For all shared code related to this work, check if you include:

–  Specification of dependencies. We give installation instructions in the README of
our repository.

–  Training code. The training code is available in our repository.

–  Evaluation code. The evaluation code is available in our repository.

–  (Pre-)trained model(s). We do not release any pre-trained models.

–  README file includes table of results accompanied by precise command to run
to produce those results. We include a README with detailed instructions to repro-
duce all of our experimental results.

•  For all reported experimental results, check if you include:

–  The  range  of  hyper-parameters  considered,  method  to  select  the  best  hyper-
parameter configuration, and specification of all hyper-parameters used to gen-
erate results. We provide all details of the hyper-parameter tuning in Section 5.

–  The exact number of training and evaluation runs.  The details about training and
evaluation can be seen in Section 5.

–  A clear definition of the specific measure or statistics used to report results.  We
use the classification accuracy on test-set and the loss on the train-set.

–  A description of results with central tendency (e.g. mean) & variation (e.g. error
bars). We do not report the mean and standard deviation for experiments.

–  The average runtime for each result, or estimated energy cost.  We do not report
the running time or energy cost.

–  A description of the computing infrastructure used.  All detailed descriptions are
presented in Section 5.

10


Under review as a conference paper at ICLR 2022

REFERENCES

Marcin Andrychowicz, Misha Denil, Sergio Go´mez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando de Freitas.  Learning to learn by gradient descent by gradient
descent.   In D. Lee,  M. Sugiyama,  U. Luxburg,  I. Guyon,  and R. Garnett (eds.),  Advances in
Neural Information Processing Systems (NeurIPS), volume 29. Curran Associates, Inc., 2016a.

Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas.  Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems (NeurIPS), 2016b.

Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang.  Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning (ICML),
pp. 254–263, 2018.

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.  Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks.  In International
Conference on Machine Learning (ICML), pp. 322–332, 2019.

Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky.  Spectrally-normalized margin bounds for
neural networks.  In Advances in Neural Information Processing Systems (NeurIPS), volume 30,
pp. 6240–6249, 2017.

Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension
and  pseudodimension  bounds  for  piecewise  linear  neural  networks.   The  Journal  of  Machine
Learning Research (JMLR), 20(1):2285–2301, 2019.

Yue Cao, Tianlong Chen, Zhangyang Wang, and Yang Shen.  Learning to optimize in swarms.  In

Advances in Neural Information Processing Systems (NeurIPS), pp. 15018–15028, 2019.

Pratik  Chaudhari,  Anna  Choromanska,  Stefano  Soatto,  Yann  LeCun,  Carlo  Baldassi,  Christian
Borgs,  Jennifer Chayes,  Levent Sagun,  and Riccardo Zecchina.   Entropy-SGD: Biasing gradi-
ent descent into wide valleys.  In International Conference on Learning Representations (ICLR),
2017.

Tianlong Chen, Weiyi Zhang, Jingyang Zhou, Shiyu Chang, Sijia Liu, Lisa Amini, and Zhangyang
Wang.  Training stronger baselines for learning to optimize.  arXiv preprint arXiv:2010.09089,
2020a.

Wuyang Chen, Zhiding Yu, Zhangyang Wang, and Animashree Anandkumar. Automated synthetic-
to-real generalization. In International Conference on Machine Learning (ICML), pp. 1746–1756,
2020b.

Xuxi Chen,  Wuyang Chen,  Tianlong Chen,  Ye Yuan,  Chen Gong,  Kewei Chen,  and Zhangyang
Wang.  Self-pu: Self boosted and calibrated positive-unlabeled training.  In International Confer-
ence on Machine Learning (ICML), pp. 1510–1519, 2020c.

Yutian Chen, Matthew W Hoffman, Sergio Go´mez Colmenarejo, Misha Denil, Timothy P Lillicrap,
Matt Botvinick, and Nando De Freitas.  Learning to learn without gradient descent by gradient
descent. In International Conference on Machine Learning (ICML), pp. 748–756, 2017.

Alex Damian, Tengyu Ma, and Jason D. Lee.  Label noise SGD provably prefers flat global mini-
mizers. CoRR, abs/2106.06530, 2021.

Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.  Sharp minima can generalize
for deep nets. In International Conference on Machine Learning (ICML), pp. 1019–1028, 2017.

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.  Gradient descent provably optimizes
over-parameterized neural networks.  In International Conference on Learning Representations
(ICLR), 2019.

John  Duchi,  Elad  Hazan,  and  Yoram  Singer.   Adaptive  subgradient  methods  for  online  
learning
and stochastic optimization. Journal of Machine Learning Research (JMLR), 12(61):2121–2159,
2011.

11


Under review as a conference paper at ICLR 2022

Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.  Sharpness-aware minimiza-
tion for efficiently improving generalization. In International Conference on Learning Represen-
tations (ICLR), 2021.

Noah Golowich,  Alexander Rakhlin,  and Ohad Shamir.   Size-independent sample complexity of
neural networks. In Conference On Learning Theory (COLT), pp. 297–299, 2018.

Moritz Hardt, Ben Recht, and Yoram Singer.  Train faster, generalize better:  Stability of 
stochastic
gradient descent.   In International Conference on Machine Learning (ICML), pp. 1225–1234,
2016.

Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local minima.
In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pp. 2553–2564,
2019.

Sepp Hochreiter and Ju¨rgen Schmidhuber.  Simplifying neural nets by discovering flat minima.  In
Advances in neural information processing systems (NeurIPS), pp. 529–536,  Cambridge, MA,
USA, 1994. MIT Press.

Sepp Hochreiter and Ju¨rgen Schmidhuber.  Flat Minima.  Neural Computation, 9(1):1–42, 01 1997.
ISSN 0899-7667. doi: 10.1162/neco.1997.9.1.1.

Kaiyi Ji and Yingbin Liang.   Minimax estimation of neural net distance.   In Advances in Neural
Information Processing Systems (NeurIPS), pp. 3849–3858, 2018.

Kaiyi Ji, Yi Zhou, and Yingbin Liang.  Understanding estimation and generalization error of gener-
ative adversarial networks. IEEE Transactions on Information Theory, 67(5):3114–3129, 2021.

Haoming Jiang, Zhehui Chen, Yuyang Shi, Bo Dai, and Tuo Zhao. Learning to defense by learning
to attack. arXiv preprint arXiv:1811.01213, 2018.

Yiding Jiang,  Behnam Neyshabur,  Hossein Mobahi,  Dilip Krishnan,  and Samy Bengio.   Fantas-
tic generalization measures and where to find them.   In International Conference on Learning
Representations (ICLR), 2020.

Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
Adam to SGD. arXiv preprint arXiv:1712.07628, 2017.

Nitish  Shirish  Keskar,  Jorge  Nocedal,  Ping  Tak  Peter  Tang,  Dheevatsa  Mudigere,  and  
Mikhail
Smelyanskiy.  On large-batch training for deep learning:  Generalization gap and sharp minima.
In International Conference on Learning Representations (ICLR), 2017.

Diederik P Kingma and Jimmy Ba.  Adam:  A method for stochastic optimization.  arXiv preprint
arXiv:1412.6980, 2014.

A. Krizhevsky and G. Hinton.   Learning multiple layers of features from tiny images.   Master’s
thesis, Department of Computer Science, University of Toronto, 2009.

Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Chaojian Li, Tianlong Chen, Haoran You, Zhangyang Wang, and Yingyan Lin.  Halo:  Hardware-
aware learning to optimize.  In European Conference on Computer Vision (ECCV), pp. 500–518.
Springer, 2020.

Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.  Visualizing the loss land-
scape of neural nets. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.

Yuanzhi Li and Yang Yuan.  Convergence analysis of two-layer neural networks with ReLU activa-
tion. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.

Kaifeng Lv, Shunhua Jiang, and Jian Li. Learning gradient descent: Better generalization and longer
horizons. In International Conference on Machine Learning (ICML), pp. 2247–2255, 2017.

12


Under review as a conference paper at ICLR 2022

Song Mei, Yu Bai, and Andrea Montanari.  The landscape of empirical risk for nonconvex losses.

The Annals of Statistics, 46(6A):2747–2774, 2018.

Tristan Milne.  Piecewise strong convexity of neural networks.  In Advances in Neural Information
Processing Systems (NeurIPS), volume 32, pp. 12973–12983, 2019.

Joelle  Pineau,  Philippe  Vincent-Lamarre,  Koustuv  Sinha,  Vincent  Lariviere,  Alina  
Beygelzimer,
Florence d’Alche´ Buc, Emily Fox, and Hugo Larochelle.  Improving reproducibility in machine
learning research. Journal of Machine Learning Research, 22:1–20, 2021.

M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: the rprop
algorithm. In IEEE International Conference on Neural Networks, pp. 586–591 vol.1, 1993. doi:
10.1109/ICNN.1993.298623.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400–407, 1951. ISSN 00034851.

Sebastian  Ruder.     An  overview  of  gradient  descent  optimization  algorithms.     arXiv  
preprint
arXiv:1609.04747, 2016.

Itay Safran and Ohad Shamir.  On the quality of the initial basin in overspecified neural networks.
In International Conference on Machine Learning (ICML), pp. 774–782, 2016.

Jiayi Shen, Xiaohan Chen, Howard Heaton, Tianlong Chen, Jialin Liu, Wotao Yin, and Zhangyang
Wang.  Learning a minimax optimizer:  A pilot study.  In International Conference on Learning
Representations (ICLR), 2021.

T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.

Olga  Wichrowska,  Niru  Maheswaranathan,  Matthew  W  Hoffman,  Sergio  Gomez  Colmenarejo,
Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein.   Learned optimizers that scale and
generalize. In International Conference on Machine Learning (ICML), 2017.

Ashia  C  Wilson,  Rebecca  Roelofs,  Mitchell  Stern,  Nathan  Srebro,  and  Benjamin  Recht.    
The
marginal value of adaptive gradient methods in machine learning.   In Advances in Neural In-
formation Processing Systems (NeurIPS), 2017.

Yuanhao Xiong and Cho-Jui Hsieh. Improved adversarial training via learned optimizer, 2020.

Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney.  Pyhessian: Neural networks
through the lens of the Hessian. In 2020 IEEE International Conference on Big Data (Big Data),
pp. 581–590. IEEE, 2020.

Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017.

Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen.  L2-gcn:  Layer-wise and learned
efficient training of graph convolutional networks.  In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 2127–2135, 2020.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.  Understanding
deep learning (still) requires rethinking generalization. Commun. ACM, 64(3):107–115, February
2021. ISSN 0001-0782. doi: 10.1145/3446776.

Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards theoretically
understanding why SGD generalizes better than Adam in deep learning.  In Advances in Neural
Information Processing Systems (NeurIPS), volume 33, 2020.

Yi Zhou, Yingbin Liang, and Huishuai Zhang.  Generalization error bounds with probabilistic guar-
antee for SGD in nonconvex optimization. arXiv preprint arXiv:1802.06903, 2018a.

Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global
minimum in deep learning via star-convex path. In International Conference on Learning Repre-
sentations (ICLR), 2018b.

13


Under review as a conference paper at ICLR 2022

Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of Adam in
learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371, 2021.

14


Under review as a conference paper at ICLR 2022

Supplementary Materials

A    ADDITIONAL  EXPERIMENTAL  RESULTS

A.1    RESNET20 EXPERIMENTS

In this section,  we evaluate the performance of our trained optimizers on larger neural networks
ResNet-20 on CIFAR-10 dataset.  The training loss and testing accuracy are plotted in Figure 5.
We    can see that the Entropy regularizer is able to outperform other methods in both training loss
and testing accuracy, demonstrating its generalization ability on large unseen models.  Further note
that although the Hessian regularizer may not be preferred in large neural networks, it does perform
better than the Entropy regularizer in small networks as we have shown in Figure 3.


3.0

2.5

ResNet-20-CIFAR

L2O-DM-CL

L2O-DM-CL + Hessian Trace
L2O-DM-CL + Entropy

0.8

0.7

2.0                                                                                      0.6


1.5

0.5


1.0

0.4


0.5

0.0

0         2000      4000      6000      8000     10000

Training Iterations

0.3

0.2

L2O-DM-CL

L2O-DM-CL + Hessian Trace
L2O-DM-CL + Entropy

0         2000      4000      6000      8000     10000

Training Iterations

Figure 5:  Comparison of the training loss/testing accuracy of ResNet-20 trained using L2O-DM-CL 
(Chen
et al., 2020a) with/without the proposed Hessian/Entropy regularization.

A.2    WALL CLOCK COMPARISON BETWEEN DIFFERENT ALGORITHMS

We  further  conduct  an  optimizee  training  time  comparison  between  our  methods  and  
analytical
optimizers, L2O-DM-CL, and Entropy-SGD (Chaudhari et al., 2017) in Table 1.  Note that L2O-
DM-CL+Hessian and L2O-DM-CL+Entropy share the same time to train optimizee as L2O-DM-
CL.          From Table 1, we can see that trained L2O-DM-CL requires only      1.5x time than 
analytical
optimizers in terms of inference time, which is thus time efficient for practical usage.  However,
Entropy-SGD  requires       21x  time  than  analytical  optimizers  to  train  optimizees.   Such  
cost  is
because Entropy-SGD requires multiple Langevin dynamic steps per iteration to estimate the local
entropy.

Table 1: Empirical Time Cost Comparison per Iteration

Methods           SGD      ADAM      L2O(L2O+Hessian, L2O+Entropy)      Entropy-SGD
Time (secs)      0.045       0.045                               0.067                              
      0.958

A.3    ACCURACY COMPARISON BETWEEN DIFFERENT ALGORITHMS

We also compare the testing accuracy (%) of our proposed methods with Entropy-SGD (Chaudhari
et al., 2017) and SGD with Hessian regularization.  The Conv-MNIST results shown in Table 2 are
evaluated on L2O-DM-CL and the Conv-CIFAR results shown in Table 3 are evaluated on L2O-
Scale.  We adopt the same experimental setting as in Section 5 for training except that the running

15


Under review as a conference paper at ICLR 2022

epochs are limited to 100 to investigate whether the performance of trained optimizers would persist
in long term.

Table 2: Additional Testing Accuracy Comparison on Conv-MNIST

Methods                  L2O      L2O+Hessian     L2O+Entropy     SGD     Entropy-SGD     
SGD+Hessian
Testing Accuracy     92.74          97.34                97.87           80.73          97.54       
         95.37

Table 3: Additional Testing Accuracy Comparison on Conv-CIFAR
Methods                  L2O+Hessian     Entropy-SGD      SGD      SGD+Hessian

Testing Accuracy           59.57                 57.73           54.69           51.41

From these comparisons, we can see that our proposed optimizers (L2O+Hessian, L2O+Entropy)
achieve  the  best  performance  compared  with  regularized  analytical  optimizers.   
Specifically,  in
Conv-CIFAR setting as shown in Table 3, our algorithm L2O+Hessian outperforms SGD+Hessian
and Entropy-SGD. In Conv-MNIST setting as shown in Table 2, the performances of top three al-
gorithms, i.e. L2O+Entropy, Entropy-SGD and L2O+Hessian, are similar and much better than the
performances of L2O and SGD+Hessian.  Among the top three algorithms,  the iteration running
time      for Entropy-SGD is 0.958 secs while L2O+Hessian and L2O+Entropy only take 0.067 secs
as shown in Table 1.  Such wall clock comparison shows that L2O+Hessian and L2O+Entropy are
more time efficient than Entropy-SGD while achieving the high accuracy, which are preferred for
practical usage.

B    PROOF  OF  THEOREM  1

B.1    RESTATEMENT OF ASSUMPTIONS

Assumption  5  (Restatement  of  Assumption  2).   Lipschitz  properties  are  assumed  on  
functions

L⁽¹⁾(θ) and L⁽²⁾(θ).

a)  L⁽¹⁾(θ) function is M-Lipschitz, i.e., for any θ₁ and θ₂, ǁL⁽¹⁾(θ₁) − L⁽¹⁾(θ₂)ǁ ≤ M ǁθ₁ − θ₂ǁ.

b)  ∇θL⁽¹⁾(θ)   and   ∇θL⁽²⁾(θ)   are   L-Lipschitz,   i.e.,   for   any   θ₁   and   θ₂,   
ǁ∇θL⁽ⁱ⁾(θ₁)  −

∇θL⁽ⁱ⁾(θ₂)ǁ ≤ Lǁθ₁ − θ₂ǁ(i = 1, 2).

c)  ∇2L⁽¹⁾(θ)   and   ∇2L⁽²⁾(θ)   are   ρ-Lipschitz,   i.e.,   for   any   θ₁   and   θ₂,   
ǁ∇2L⁽ⁱ⁾(θ₁)  −

∇2L⁽ⁱ⁾(θ₂)ǁ  ≤  ρǁθ₁ − θ₂ǁ(i  =  1, 2).   This assumption also holds for stochastic ∇2L⁽¹⁾(θ)

θ                                                                                                   
                                                              θ    N

and ∇2L⁽²⁾(θ).

Assumption 6 (Restatement of Assumption 3).  Similarly as in Mei et al. (2018), we assume the
loss gradient ∇F ⁽¹⁾(θ; ξ) is τ ²-sub-Gaussian, i.e., for any Q ∈ Rp, and θ ∈ Dʳ(0) where Dʳ(0) ≡

{θ ∈ Rp, ǁθǁ₂ ≤ r},


E{exp(⟨Q, ∇F

(1)

(θ; ξ) −

E[∇F

(1)

(θ; ξ)]⟩)} ≤ exp

. τ ²ǁQǁ2 Σ

Meanwhile, we assume the loss Hessian is τ ²-sub-exponential, i.e., for any Q      D¹(0), and θ
Dʳ(0),


ξq,θ

≡ ⟨Q, ∇2F ⁽¹⁾(θ; ξ)Q⟩,     E, exp .  1  |ξ

q,θ

− Eξ

q,θ

|Σ , ≤ 2,

and there exists a constant ch such that L ≤ τ ²pᶜh , ρ ≤ τ ³pᶜh .

Assumption  7  (Restatement  of  Assumption  4).  We  assume  functions  L⁽¹⁾(θ)  is  (ϵ, 
η)-strongly
Morse in Dʳ(0),  i.e.,  if       L⁽¹⁾(θ)  ₂  >  ϵ for   θ  ₂  =  r  and,  for any θ       Rp,    θ  
₂  <  r,  the
following holds:


2

2                                   i

i∈[p]

16

(1)

(θ))| ≥ η,


Under review as a conference paper at ICLR 2022

where λi(∇2L⁽¹⁾(θ)) denotes the i-th eigenvalue of ∇2L⁽¹⁾(θ).  We further make the assumption
that the local basins Dᵈ(θ⁽ⁱ⁾(φ∗))(i = 1, 2) of convergence points θ⁽ⁱ⁾(φ∗)(i = 1, 2) are in Dʳ(0).

T                                                                             T

B.2    PROOF OF SUPPORTING LEMMAS

Lemma 1 (Restatement of Theorem 1(b) in Mei et al. (2018)).  We assume θ∗ corresponding to

θN∗   in local basin.  Based on Assumptions 2 and 3, there exists a universal constant C₀, and we 
let

C = C₀ max{ch, log(rτ/δ), 1}. If N  ≥ Cp log p, then we have                  


sup

θ∈Dp(r)

with probability at least 1 − δ.

ǁ∇  LN

(θ) − ∇2L(θ)ǁ ≤ τ ². Cp log N ,

Lemma 2 (Restatement of Theorem 2 in Mei et al. (2018)).  Based on Assumptions 2, 3 and 4, we

set C  as in Lemma 1, assume that θ∗ is corresponding to θ∗ , and let N  ≥  4Cp log N/η²  where

N                                                       ∗

η² = min{(ϵ²/τ ²), (η²/τ ⁴), (η⁴/(L²τ ²))}                                                         
θ∗  and θ∗, we have


ǁθ∗

− θ∗ǁ

≤  2τ . Cp log N ,

N         ²       η            N

with probability at least 1 − δ.

Lemma 3.  Suppose Assumptions 1 and 2 hold. Then, we have


(1)

∗(1)

. L . L − µ ΣT −T '

	

(1)

∗(1)

where T ′ is the minimum that after T ′ gradient descent updates, the updated optimizee parameter

θ⁽¹⁾(GD) locates into the local basin of θ⁽¹⁾(φ∗).

T '                                                                                        T

Proof.  Since the local basin is µ-strongly convex and θ∗⁽¹⁾ is the optimal point of smooth 
function

L⁽¹⁾(θ) in local basin. Then, we have


L(1)(θ(1)(φ∗)) − L(1)(θ∗(1))

θ(1)(φ∗) − θ∗(1)ǁ2.

Furthermore, we rearrange the terms and obtain

ǁθ⁽¹⁾(φ∗) − θ∗⁽¹⁾ǁ ≤. 2 (L⁽¹⁾(θ⁽¹⁾(φ∗)) − L⁽¹⁾(θ∗⁽¹⁾))


T                    N                 µ

(i). 2

N

(1)

T

(1)

N

(1)

N

∗(1)


(ii). 2 L

(1)

∗(1)   2

≤. L ǁθ⁽¹⁾(GD) − θ∗⁽¹⁾ǁ


(iii). L . L − µ ΣT −T '

	

(1)

∗(1)

where (i) follows because φ∗ = arg min   L⁽¹⁾(θ⁽¹⁾(φ)) and θ⁽¹⁾(GD) locates in the local basin of

φ    N      T                       T

θ∗⁽¹⁾, (ii) follows from Assumptioin 2 and the fact that θ∗⁽¹⁾ = arg min   L⁽¹⁾(θ), and (iii) 
follows


N

if we set step size of GD as    ²    .

N                          θ    N

Lemma 4.  Based on Assumptions 1, 2, 3 and 4, we let N  ≥ max{Cp log p, 4Cp log N/η²} where


rτ          2                        s²    ²             ⁴

C  =  C  max{c  , 1, log(    )}, η    =  min{     , η  ,   η

∗

}, C

probability at least 1 − 2δ we have

17


Under review as a conference paper at ICLR 2022


ǁ∇2L⁽²⁾(θ∗⁽²⁾)ǁ ≤ρ

. 2τ . Cp log N

	

. L . L − µ ΣT −T '

	

ǁθ⁽¹⁾(GD) − θ

∗(1)ǁΣ


+ ∆∗H

+ τ ²     Cp log N  + BHₑssiₐn(λ),

N

where ∆∗H  = ǁ∇2L⁽²⁾(θ∗⁽²⁾) − ∇2L⁽¹⁾(θ∗⁽¹⁾)ǁ and T ′ is defined in Lemma 3.

Proof.  Firstly, we bound ǁ∇2L⁽²⁾(θ∗⁽²⁾)ǁ as following:

ǁ∇2L(2)(θ∗(2))ǁ ≤ǁ∇2L(2)(θ∗(2)) − ∇2L(1)(θ∗(1))ǁ + ǁ∇2L(1)(θ∗(1)) − ∇2L(1)(θ∗(1))ǁ

+ ǁ∇2L(1)(θ∗(1)) − ∇2L(1)(θ∗(1))ǁ + ǁ∇2L(1)(θ∗(1)) − ∇2L(1)(θ(1)(φ∗))ǁ

+ ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ∗))ǁ,

where θ∗⁽¹⁾ is corresponding to θ∗⁽¹⁾ in the same local basin of θ⁽¹⁾(φ∗).

N                                                         T

Based on the constrained problem formulation in eq. (9),  the optimal optimizer parameter φ∗ is
equivalent to the following:

φ∗ = arg min L⁽¹⁾(θ⁽¹⁾(φ)) subject to ∇2L⁽¹⁾(θ⁽¹⁾(φ)) ≤ BHₑssiₐn(λ).

N      T                                    θ    N      T

φ

Thus, we obtain ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ∗))ǁ  ≤  BHₑssiₐn(λ).  Furthermore, if we let N  ≥  Cp log p where


C  =  C

max{

θ    N rτ T

and C

is an universal constant, based on Lemmas 1, 2 and 3, and


Assumptions 2, we have

ǁ∇2L⁽²⁾(θ∗⁽²⁾)ǁ ≤ρ

.ǁθ∗(1)  − θ

∗(1)ǁ +

. L . L − µ ΣT −T '

	

ǁθ⁽¹⁾(GD) − θ

∗(1)ǁΣ


+ ∆∗H

+ τ ²     Cp log N  + BHₑssiₐn(λ),

N

with probability at least 1 − δ.

Furthermore, if we assume N  ≥  max{4Cp log N/η², Cp log p} where η²  =  min{ s2 , η2 ,   η4  },


based on Lemma 2, we have

∗                                    ∗                  τ 2

τ 4   ρ2 τ 2


ǁ∇2L⁽²⁾(θ∗⁽²⁾)ǁ ≤ρ

. 2τ . Cp log N

	

. L . L − µ ΣT −T '

	

ǁθ⁽¹⁾(GD) − θ

∗(1)ǁΣ


+ BHessian(λ) + ∆∗H

+ τ 2       Cp log N ,

N

with probability at least 1 − 2δ.

Lemma 5.  Based on Assumptions 1, 2, 3, and 4, we let N  ≥  4Cp log N/η² where C  and η² are

defined in Lemma 2. Then, with probability at least 1 − δ, we have                ∗                 
            ∗


(2)

(φ∗) − θ∗⁽²⁾ǁ ≤ ∆∗T  +

. L . L − µ ΣT −T '

	

ǁθ⁽¹⁾(GD) − θ

∗(1)ǁ +

2τ . Cp log N

	

+ ∆∗θ ,

where ∆∗ = ǁθ∗⁽¹⁾ − θ∗⁽²⁾ǁ, ∆∗  = ǁθ⁽²⁾(φ∗) − θ⁽¹⁾(φ∗)ǁ and T ′ is defined in Lemma 3.

Proof.  Based on triangle inequality, we obtain

ǁθ(2)(φ∗) − θ∗(2)ǁ

18


Under review as a conference paper at ICLR 2022

≤ ǁθ⁽²⁾(φ∗) − θ⁽¹⁾(φ∗)ǁ + ǁθ⁽¹⁾(φ∗) − θ∗⁽¹⁾ǁ + ǁθ∗⁽¹⁾ − θ∗⁽¹⁾ǁ + ǁθ∗⁽¹⁾ − θ∗⁽²⁾ǁ


T

(i)

∗

T

(1)      ∗

∗(1)

T

∗(1)

N

∗(1)

N

∗(1)

∗(2)


≤  ∆T  + ǁθT   (φ  ) − θN    ǁ + ǁθN     − θ

ǁ + ǁθ

− θ      ǁ


(ii)

≤  ∆∗T  +

. L . L − µ ΣT −T '

	

ǁθ⁽¹⁾(GD) − θ

∗(1)

ǁ + ǁθ

∗(1)

− θ∗(1)ǁ + ∆∗θ ,

where (i) follows from definition of ∆∗T , (ii) follows from Lemma 3 and definition of ∆∗θ .  Based
on Lemma 2, if we let N  ≥ 4Cp log N/η². Then, with probability at least 1 − δ, we have


(2)

(φ∗) − θ∗⁽²⁾ǁ ≤∆∗T  +

. L . L − µ ΣT −T '

	

ǁθ⁽¹⁾(GD) − θ

∗(1)ǁ +

2τ . Cp log N

	

+ ∆∗θ

=∆∗ + ∆∗ + O(wT −T ' ) + O(. C log N ),

T           θ                                                  N

where w =  L−µ .

B.3    PROOF OF THEOREM 1

Generalization loss is defined as below:

L(2)(θ(2)(φ∗)) − L(2)(θ∗(2))


(i)

(2)      ∗

∗(2)   T

(2)

∗(2)

1    (2)      ∗

∗(2)   T

2    (2)      ′

(2)      ∗

∗(2)


= (θT   (φ  ) − θ

)   ∇θL    (θ

) + 2 (θT   (φ  ) − θ

)   ∇θL

(θ )(θT   (φ  ) − θ      )


(ii)  1

(2)      ∗

∗(2)   T

2    (2)      ′

(2)      ∗

∗(2)


=  2 (θT   (φ  ) − θ

)   ∇θL

(θ )(θT   (φ  ) − θ      )

=  1 (θ(2)(φ∗) − θ∗(2))T (∇2L(2)(θ′) − ∇2L(2)(θ∗(2)) + ∇2L(2)(θ∗(2)))(θ(2)(φ∗) − θ∗(2))


2    T

1     (2)      ∗

θ

∗(2)   2          2    (2)      ′

θ

2    (2)

∗(2)

θ

2    (2)

T

∗(2)


≤  2 ǁθT   (φ  ) − θ

ǁ  (ǁ∇θL

(θ ) − ∇θL    (θ

)ǁ + ǁ∇θL    (θ

)ǁ)


(iii)  1

(2)      ∗

∗(2)   3        1

2    (2)

∗(2)

(2)      ∗

∗(2)   2


≤   2 ρǁθT   (φ  ) − θ

ǁ   + 2 ǁ∇θL    (θ

)ǁǁθT   (φ  ) − θ      ǁ


≤  1 ǁθ⁽²⁾(φ∗) − θ

∗(2)ǁ2(ρǁθ⁽²⁾(φ∗) − θ

∗(2)

ǁ + ǁ∇θL    (θ

∗(2)

)ǁ),

where (i) follows from Taylor expansion and θ′ follows from the conditions that ǁθ′ − θ⁽²⁾(φ∗)ǁ ≤
ǁθ∗⁽²⁾ − θ⁽²⁾(φ∗)ǁ and ǁθ′ − θ∗⁽²⁾ǁ ≤ ǁθ∗⁽²⁾ − θ⁽²⁾(φ∗)ǁ, (ii) follows because ∇θL⁽²⁾(θ∗⁽²⁾) = 0,

and (iii) follows from Assumption 2 and the fact that ǁθ′ − θ∗⁽²⁾ǁ ≤ ǁθ∗⁽²⁾ − θ⁽²⁾(φ∗)ǁ.

Based  on  Lemmas  4  and  5,  if  we  let  N  ≥  max{4Cp log N/η², Cp log p}  where  C  and  η²  
are

defined in Lemma 4. Then, with probability at least 1 − 2δ, we ha∗ve                                
         ∗

ρǁθ⁽²⁾(φ∗) − θ∗⁽²⁾ǁ + ǁ∇2L⁽²⁾(θ∗⁽²⁾)ǁ                                                               
  


.         . L . L − µ ΣT −T '

	

(1)

∗(1)

2τ . Cp log N           Σ


+ τ ²

. Cp log N

+ ρ . 2τ . Cp log N

. L . L − µ ΣT −T '

ǁθ⁽¹⁾(GD) − θ

∗(1)ǁΣ


N

+ ∆∗H  + B(λ)

η            N                µ

L + µ                T                N


T −T '

=ρ∆∗T  + 2ρ

ǁθ⁽¹⁾(GD) − θ

∗(1)ǁ +

. 4ρτ

+ τ ²

Σ . Cp log N


+ ρ∆∗θ + ∆∗H  + BHessian(λ)

=BHₑssiₐn(λ) + ρ∆∗ + ρ∆∗ + ∆∗

+ O(wT −T ' ) + O(. C log N ),

19


Under review as a conference paper at ICLR 2022


where  w  =   L−µ ,  ∆∗

=  ǁ∇2L(2)(θ∗(2)) −  ∇2L(1)(θ∗(1))ǁ,  ∆∗

=  ǁθ∗(1)  −  θ∗(2)ǁ,  ∆∗    =


ǁθ⁽²⁾(φ∗) − θ⁽¹⁾(φ∗)ǁ.

Furthermore, We let A₁  = BHₑssiₐn(λ) + ρ∆∗ + ρ∆∗ + ∆∗

+ O(wT −T ' ) + O(. C log N ), A₂  =

∆∗ + ∆∗ + O(wT −T ' ) + O(. C log N ). Then, we have

L(2)(θ(2)(φ∗)) − L(2)(θ∗(2))           θ(2)(φ∗) − θ∗(2)ǁ2(ρǁθ(2)(φ∗) − θ∗(2)ǁ + ǁ∇2L(2)(θ∗(2))ǁ)

1    2

≤  2 A2A₁,

with probability at least 1 − 2δ. Then, the proof is complete.

C    PROOF  OF  THEOREM  2

C.1    PROOF OF SUPPORTING LEMMA

Lemma 6.  Based on Assumptions 1 and 2, in terms of entropy regularizer constraint BEntrₒpy(λ),
we have

BEntrₒpy(λ) + mM + p log(2π) + 1 ρm³ + C(γ, p, m) ≥ log(det(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI))

2                    2                                                           N      T

+ L⁽¹⁾(θ⁽¹⁾(φ)),

N      T


where m is a constant, C(γ, p, m) = log ∫

θ':ǁθ'−θǁ>m

exp .− γ ǁθ − θ′ǁ2Σ dθ′ and θ ∈ Rp.

Proof.  We firstly split the integral area θ′  ∈  Rp  into two parts:  {θ′  :  ǁθ′ − θǁ  ≤  m}  and 
{θ′  :

ǁθ′ − θǁ > m}. Based on the definition of G⁽¹⁾(θ; γ), we have


G⁽¹⁾(θ; γ)

= log ∫

exp .−L⁽¹⁾(θ′)

γ   θ − θ′ǁ2Σ dθ′


= log ∫

θ':ǁθ'−θǁ≤m

exp .−L⁽¹⁾(θ′)

γ

−  2 ǁ

θ − θ′ǁ2Σ dθ′


+ log ∫

θ':ǁθ'−θǁ>m

exp .−L⁽¹⁾(θ′) −  γ ǁ

2

θ − θ′ǁ2Σ dθ′


(i)          ∫

.     (1)      ′        γ

′  2Σ    ′


+ log ∫

θ':ǁθ'−θǁ>m

exp .− γ ǁθ − θ′ǁ2Σ dθ′

2


(ii)          ∫

.     (1)      ′        γ

′  2Σ    ′


(iii)          ∫

.      (1)

′          T        (1)

1    ′          T

2    (1)      ′′      ′

γ            ′  2Σ   ′

where (i) follows from the fact that L⁽¹⁾(θ′) is non-negative, (ii) follows from the fact that θ  ∈


Rp  and  the  definiton  of  C(γ, p, m),

N

(iii)

follows  from  Taylor  expansion.   Note  that  θ′′ satisfies

ǁθ′′ − θǁ ≤ ǁθ − θ′ǁ and ǁθ′′ − θ′ǁ ≤ ǁθ − θ′ǁ.

Based on Assumption 2, −(θ′ − θ)T ∇L⁽¹⁾(θ) ≤ mǁ∇L⁽¹⁾(θ)ǁ ≤ mM . Then, we obtain

N                            N

G⁽¹⁾(θ; γ)

20


Under review as a conference paper at ICLR 2022

≤ − L⁽¹⁾(θ) + mM + log ∫                exp . −  1 (θ′ − θ)T ∇2L⁽¹⁾(θ′′)(θ′ − θ)


N

γ            ′  2Σ   ′

θ':ǁθ'−θǁ≤m                       2

= − L⁽¹⁾(θ) + mM + log ∫                exp . −  1 (θ′ − θ)T (∇2L⁽¹⁾(θ′′) + γI)(θ′ − θ)Σdθ′


N

+ C(γ, p, m)

θ':ǁθ'−θǁ≤m                       2

= − L⁽¹⁾(θ) + mM + C(γ, p, m)

+ log ∫                exp . −  1 (θ′ − θ)T .∇2L⁽¹⁾(θ′′) − ∇2L⁽¹⁾(θ) + ∇2L⁽¹⁾(θ)

	


(i)

N                       N

θ':ǁθ'−θǁ≤m

+ γIΣ(θ′ − θ)   dθ′

(1)                                                                    1      3

≤ − LN  (θ) + mM + C(γ, p, m) + 2 ρm

+ log ∫                exp . −  1 (θ′ − θ)T .∇2L⁽¹⁾(θ) + γIΣ (θ′ − θ)Σdθ′


(ii)

(1)

θ':ǁθ'−θǁ≤m                       2

1      3                ∫

N

.    1    ′

T  .   2

(1)

Σ   ′          Σ   ′


≤  − LN  (θ) + mM + 2 ρm

+ C(γ, p, m),

+ log

exp

θ'

−  2 (θ

− θ)

∇  LN  (θ) + γI

(θ  − θ)   dθ

where (i) follows from Assumption 2 and the fact that ǁθ′′ − θǁ ≤ ǁθ − θ′ǁ and (ii) follows because

exp(− 1 (θ′ − θ)T (∇2L⁽¹⁾(θ) + γI)(θ′ − θ)) ≥ 0.

Based on Assumption 1, we have the fact that (∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI) is a symmetric and positive-

N      T

definite matrix. Hence, we obtain

G⁽¹⁾(θ⁽¹⁾(φ); γ) ≤ −L⁽¹⁾(θ⁽¹⁾(φ)) − log(det(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI)) + C(γ, p, m)


N      T

+ mM +

N

1 ρm³ + p

2             2

T                                           N      T

log(2π).

We rearrange the terms and get


(1)

(1)

(1)

(1)

2    (1)

(1)

−GN  (θT   (φ); γ) ≥ LN  (θT   (φ)) + log(det(∇  LN  (θT   (φ)) + γI)) − C(γ, p, m)

p                   1      3

− mM −  2 log(2π) −  2 ρm  .

Since −G⁽¹⁾(θ⁽¹⁾(φ); γ) ≤ BEntrₒpy(λ). Then, we obtain

BEntrₒpy(λ) + mM + p log(2π) + 1 ρm³ + C(γ, p, m) ≥ log(det(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI))

2                    2                                                           N      T

+ L⁽¹⁾(θ⁽¹⁾(φ)).

N      T

C.2    PROOF OF THEOREM 2
Based on Lemma 6, we have

BEntrₒpy(λ) + mM + p log(2π) + 1 ρm³ + C(γ, p, m) ≥ log(det(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI))

2                    2                                                           N      T

+ L⁽¹⁾(θ⁽¹⁾(φ)).

N      T

Since  ∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI  is  positive  definite  and  λi(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI)   ≥   γ  + µ  
for

any  i  =  1, . . . , p.   Then,  based  on  the  definition  of  Matrix  norm  ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ)) + 
γIǁ   =

21


Under review as a conference paper at ICLR 2022

λmₐₓ(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI), we have

ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γIǁp  ≥ det(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI)) ≥ (γ + µ)ᵖ−¹ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γIǁ.

Note that we use λi(H) to denote the i-th eigenvalue of matrix H. Then,

log(det(∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γI))) ≥(p − 1) log(γ + µ) + log ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ)) + γIǁ

=(p − 1) log(γ + µ) + log(ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ + γ).

N      T

Then, we can obtain

BEntrₒpy(λ)+mM + p log(2π) + 1 ρm³ + C(γ, p, m) ≥ L⁽¹⁾(θ⁽¹⁾(φ)) + (p − 1) log(γ + µ)

+ log(ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ + γ).

Hence, we can get a new function D(x) that

ǁ∇2L⁽¹⁾(θ⁽¹⁾(φ))ǁ ≤ D−¹(BEntrₒpy(λ)),

where D(x) = L⁽¹⁾(θ⁽¹⁾(φ)) + (p − 1) log(γ + µ) − mM −  p log(2π) −  1 ρm³ − C(γ, p, m) +

log(x + γ). Then, the proof is complete.

22

