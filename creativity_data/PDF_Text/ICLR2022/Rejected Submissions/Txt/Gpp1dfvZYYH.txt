Under review as a conference paper at ICLR 2022
ProgFed: Effective, Communication, and
Computation Efficient Federated Learning by
Progressive Training
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning is a powerful distributed learning scheme that allows numerous
edge devices to collaboratively train a model without sharing their data. However,
training is resource-intensive for edge devices, and limited network bandwidth is
often the main bottleneck. Prior work often overcomes the constraints by condens-
ing the models or messages into compact formats, e.g., by gradient compression or
distillation. In contrast, we propose ProgFed, the first progressive training frame-
work for efficient and effective federated learning. It inherently reduces computa-
tion and two-way communication costs while maintaining the strong performance
of the final models. We theoretically prove that ProgFed converges at the same
asymptotic rate as standard training on full models. Extensive results on a broad
range of architectures, including CNNs (VGG, ResNet, ConvNets) and U-nets,
and diverse tasks from simple classification to medical image segmentation show
that our highly effective training approach saves up to 20% computation and up to
63% communication costs for converged models. As our approach is also compli-
mentary to prior work on compression, we can achieve a wide range of trade-offs,
showing reduced communication of up to 50× at only 0.1% loss in utility.
1	Introduction
Federated Learning (FL) (McMahan et al., 2017) has led to remarkable advances in the development
of extremely large machine learning systems. Federated training methods allow multiple clients
(edge devices) to jointly train a global model without sharing their private data with others. Despite
the progress, training methods in FL still suffer from high communication and computational costs,
as edge devices are often equipped with limited hardware resources and limited network bandwidth.
Prior literature has studied various compression techniques to address the computation and com-
munication bottlenecks. We can divide these methods into three main categories (see also Table 1):
(i) Compression techniques that represent gradients (or parameters) with fewer bits to reduce com-
munication costs. Prominent examples are quantization (Alistarh et al., 2017; Lin et al., 2018; Fu
et al., 2020) or SParsification (Stich et al., 2018; Konecny et al., 2016). (ii) Model pruning techniques
that identify (much smaller) sub-networks within the original models to reduce computational cost at
inference (Li & Wang, 2019; Lin et al., 2020). And (iii) knowledge distillation (Hinton et al., 2015)
techniques, that allow the server to distill the knowledge from the clients with hold-out datasets (Li
& Wang, 2019; Lin et al., 2020). However, these methods can sometimes degrade performance.
Progressive learning (Karras et al., 2018) is a well-known technique that has been used in image
generation to stabilize the training. The primary idea is to first train the shallower layers on simpler
tasks (e.g., images with lower resolution) and gradually grow the network to tackle more complicated
tasks (e.g., images with higher resolution). By applying progressive training in a federated learning
setting, we can benefit from highly reduced resource demands (the computation and communication
load is drastically reduced when training shallow models). By growing the models to their original
size, we can recover (and often surpass) the accuracies reached by standard training. Moreover, we
observe that progressive training facilitates the learning process and stabilizes training. Despite the
appealing features, no previous study has systematically investigated exploiting progressive learning
to reduce the costs in federated learning.
1
Under review as a conference paper at ICLR 2022
Table 1: Comparison of ProgFed to other compression schemes.
Technique	Computation Reduction	Communication Reduction	Dataset Efficiency
Message Compression		X	X
Model Pruning	X(only for inference)		X
Model Distillation	X	X	
ProgFed (Ours)	X	X	X
In this work, we propose ProgFed, the first progressive learning framework that reduces both com-
munication and computation costs in FL. In our approach, we divide the model into several over-
lapping partitions and introduce lightweight local monitoring heads to guide the training of the
submodels. During training, the model capacity is gradually increased, until it reaches the full
model of interest. This, of course, reduces the computation and communication costs, since the
shallow sub models have much fewer parameters than the full model. We show theoretically that
ProgFed training converges at the same asymptotic rate as standard training on full models. Due
to the nature of progressive learning, our method can reduce computational overheads and provides
two-way communication savings (both from server-to-client and client-to-server directions). We
demonstrate that our method can reach impressive performance (matching the baselines) using much
fewer communication. ProgFed is compatible with classical compression—including quantization
and sparsification—which can be applied on top of progressive training. We experimentally show
that combing these features enables a higher compression ratio and may motivate more advanced
compression schemes based on progressive learning.
We summarize our main contributions as follows.
•	We propose ProgFed, the first progressive learning framework to reduce the training resource
demands (computation and two-way communication). We theoretically prove that ProgFed con-
verges at the same asymptotic rate as standard training the full model.
•	We conduct extensive experiments on various datasets (CIFAR-10/100, EMNIST and BraTS)
and architectures (VGG-16/19, ResNet-18/152, ConvNets, 3D-Unet) to show that with the same
number of epochs, our method can save around 25% computation cost, up to 32% two-way
communication costs in federated classification, and 63% in federated segmentation without sac-
rificing performance.
•	In addition, our method helps the models save around 2× fewer communication costs in classifi-
cation and 6.5× fewer costs in U-net segmentation to achieve practicable performance (≥ 98% of
the best baseline). This is beneficial for combating limited training budgets in federated learning.
•	Lastly, we show that our method complements classical compression and appears robust against
compression errors. It permits a higher compression ratio and may motivate more advanced
compression schemes based on progressive learning. With these combined techniques, we are
able to reduce communication of up to 50× at only 0.1% loss in utility.
2	Related Work
In this section, we review work on cost reduction and leave progressive learning in Section D.
Message compression. Fruitful literature has studied message compression (e.g. on gradients
or model weights) to reduce the communication costs in distributed learning. The first category
focuses solely on client-to-server compression (Alistarh et al., 2017; Wen et al., 2017; Lin et al.,
2018; Bemstein et al., 2018; Stich et al., 2018; Konecny et al., 2016; Karimireddy et al., 2019; FU
et al., 2020; Stich, 2020). To name a few, Konecny et al. (2016) reduce the costs by sending sparsified
gradients and compressing them with probabilistic qUantization. Alistarh et al. (2017); Wen et al.
(2017) prove the convergence of probabilistic quantized SGD. SignSGD (Bernstein et al., 2018)
significantly compresses the gradients with only one bit, while Karimireddy et al. (2019) fix the
biased nature of SignSGD with an error-feedback mechanism and generalize to other compression
schemes. Compression on server-to-client communication has been shown non-trivial and attracted
many recent focuses (Yu et al., 2019; Tang et al., 2019; Liu et al., 2020; Philippenko & Dieuleveut,
2020). Instead of dedicated designs, our method inherently reduces two-way communication costs
and complements existing methods as we will show in Section 4.
2
Under review as a conference paper at ICLR 2022
(a) feed-forward networks
(b) U-nets (Symmetric).
Figure 1: An overview of ProgFed on (a) feed-forward networks and (b) U-nets (Symmetric). We progressively
train a deep neural network from the shallower sub-models, e.g. M1 consisting of main block E1 and head G1
(Eq. 2), gradually expanding to the full model MS = M (Eq. 1). Note that the local heads Gi in feed-forward
networks are only used for training sub-models and discarded when progressing to the next stage.
Model pruning. Model pruning removes redundant weights to address the resource con-
straints (Mozer & Smolensky, 1989; LeCun et al., 1990; Frankle & Carbin, 2019; Lin et al., 2019).
There are two categories: unstructured and structured pruning. Unstructured methods prune indi-
vidual model weights according to certain criteria such as Hessian of the loss function (LeCun et al.,
1990; Hassibi & Stork, 1993) and small magnitudes (Han et al., 2015). However, these methods
cannot fully accelerate without dedicated hardware since they often result in sparse weights. In
contrast, structured pruning methods prune channels or even layers to alleviate the issue. They of-
ten learn importance weights for different components and only keep relatively important ones (Liu
et al., 2017; Yu et al., 2018; Mohtashami et al., 2021; Li et al., 2021). Despite the efficiency, model
pruning usually happens at inference and does not reduce training costs while our method achieves
computation- and communication-efficiency even during training.
Model distillation. Another line of research for model compression is model distillation (BUCilUa
et al., 2006; Hinton et al., 2015), which requires a student model to mimic the behavior of a teacher
model (Polino et al., 2018; SUn et al., 2019). Recent work has investigated transmitting logits rather
than gradients (Li & Wang, 2019; Lin et al., 2020; He & Annavaram, 2020; ChoqUette-Choo et al.,
2020), which significantly redUces commUnication costs. However, these methods either reqUire
additional qUery datasets (Li & Wang, 2019; Lin et al., 2020) or cannot enjoy the merit of datasets
from different soUrces (ChoqUette-Choo et al., 2020). In contrast, oUr work redUces the costs while
retaining the dataset efficiency.
3	ProgFed
Federated learning is a distribUted learning framework in which a hUge amoUnt of clients train local
models independently, and a central server aggregates the client Updates to learn a global model.
Motivated by progressive learning, we propose ProgFed that progressively expands the network
from a shallower one to the complete model. OUr method is effective and efficient for mUlti-client
training while we consider only one client in theoretical analysis for conciseness. The proposed
model splitting and progressive growing are illUstrated in FigUre 1, and the federated optimization
scheme is sUmmarized in Algorithm 1. We now present the proposed method in detail below.
3.1	Proposed Method
Model Structure. We now describe the proposed training method. For a given a machine learning
model M, i.e. a function M(∙, x): Rn → Rk that maps n-dimenSional input to k logits for Pa-
rameters (weights) x ∈ Rd , we assUme that the network can be written as a composition of blocks
(feature extractors) Ei along with a task head GS, namely,
S
M := GS ◦ O Ei = GS ◦ ES ◦…O E2 ◦ E1.	(1)
i=1
Note that the Ei’s could denote e.g., a stack of residual blocks or simply a single layer. The learning
task is solved by minimizing a loss function of interest L : Rk → R (e.g., cross-entropy) that maps
the predicted logits of M to a real value, i.e. minimization of f (x) := L ◦ M(x).
3
Under review as a conference paper at ICLR 2022
Progressive Model Splitting. To achieve progressive learning, we first divide the network M into
S stages, denoted by Ms, for s ∈ {1, . . . , S} associated with the split indices. We additionally
introduce local supervision heads for providing supervision signals. Formally, we define
s
Ms ：= Gs ◦O Ei,	(2)
where Gs is a newly introduced head. Each head Gs , for s < S consists of only a pooling layer
and a fully-connected layer in our experiments for feed-forward networks. The motivation is that
simpler heads may encourage the feature extractors Ei to learn more meaningful representations.
Note that the sub-model Ms : Rn → Rk produces the same output size as the desired model M;
therefore, its corresponding loss fs(xs) := L ◦ Ms(xs) can be trained with the same loss criterion
L as the full model.
Training of Progressive Models. We propose to train each sub-model Ms for Ts iterations (a cer-
tain fraction of the total training budget) and gradually grow the network from M1 to MS = M.
When growing the network from stage s to s + 1, we pass the corresponding parameters of the
pretrained blocks Ei , i ≤ s, to the next model Ms+1 and initialize its blocks Es+1 and Gs+1 with
random weights. Once the progressive training phase is completed, we continue training the full
model M in an end-to-end manner for the remaining iterations. The length Ts of each progressive
training phase is a parameter that could be fine-tuned individually for each stage (depending on the
application) for best performance. However, as a practical guideline that we adopted for all exper-
iments in this paper, we found that denoting roughly half of the total number of training iterations
T to progressive training, and setting Ts = 2T for s < S, TS = T(S+1), SUch that T = PS=ιTs,
works well across all considered training tasks.
Extension to U-nets. In addition to feed-forward networks (FigUre 1(a)), we show that oUr method
can generalize to U-net (FigUre 1(b)). U-net typically consists of an encoder and a decoder. Unlike
feed-forward networks, the encoder sends both the final and intermediate featUres to the decoder.
Therefore, we propose to grow the network from oUter to inner layers as shown in FigUre 1(b) and
retain the original oUtpUt heads as Gi. We refer to the strategy as the Symmetric strategy. In contrast,
we propose another baseline, the Asymmetric strategy, which adopts the fUll encoder at the beginning
and gradUally grows Until it reaches the fUll decoder. For this strategy, we also adopt several temporal
heads for earlier training stages. As we will show in Section 4.3, the Symmetric strategy significantly
oUtperforms the Asymmetric strategy, which sUpports the notion of progressive learning.
Practical considerations. We empirically observe that learning rate restart (Loshchilov & HUtter,
2017) clearly facilitates training in the centralized setting. This is becaUse sUb-models may overfit
the local sUpervision while learning rate restart helps the sUb-models with the newly added layers
escape from the local minima and qUickly converge to a better minima. On the other hand, warm-
Up (Goyal et al., 2017) for the new layers plays an important role in federated learning. Model
weights often take a longer time to converge in federated learning, which makes the newly added
layers introdUce more noise to the previoUs layers. With warm-Up, the new layers recover the per-
formance withoUt affecting previoUs layers. For instance, the existence of warm-Up leads to aroUnd
2% difference (53.23 vs. 51.09) on CIFAR-100 with ResNet-18.
3.2	Theoretical Analysis
In this section We prove that progressing training converges to a stationary point at the rate O G),
i.e. with the same asymptotic rate as SGD training on the fUll network. For this, we extend the
analysis from (Mohtashami et al., 2021) that analyzed the training of partial sUbnetWorks. HoWever,
in oUr case the netWorks are not sUbnetWorks, Ms 6⊂ Ms+1 (as the head is not shared), and We need
to extend their analysis to progressive training With different heads.
Notation. We denote by xs the parameters of fs, s ∈ {1, . . . , S} and abbreviate xS = x ∈ Rd
for convenience. For S ≤ i ≤ S let χi∣Es and Vf i(xi)∣Es denote the projection of the parameter
Xi and gradient Vfi(Xi) to the dimensions corresponding to the parameters of Ei to Es (without
parameter of Es+1 to Ei and WithoUt head Gi). In iteration t, the progressive training procedUre
updates the parameters of model fs, S = min(S, dT]). For convenience, we do not explicitly write
the dependency of s on t below, and use the shorthand xts to denote the corresponding model at
timestep t. We further define Xt such that Xt|Es = xts|E and Xt|E{ = X0|E { on the complement.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 ProgFed—Progressive training in a Federated Learing setting
1: 2: 3: 4: 5: 6: 7: 8: 9:	Input: initialization xj, model M(∙,xo), iteration budgets T, Ts, number of stages S, S = 1, desired number of local updates J ≥ 1, learning rate η Output: parameters XT and trained model M (∙, XT) for t = 1, . . . , T do if min(S, dT]) > S then	. switch from Ms to Ms+1 after Ts iterations initialize parameter Xts+1 randomly	. initialize new block Es+1 and new head Gs+1 X；+1 J Xs∣E	. copy parameters of shared blocks Ei,... ,Es. s — s + 1	(the old head Gs is discarded) end if Sample a subset C of clients	O standard Federated Learning on active model Ms
10:	for each active client c ∈ C do
11: 12:	initialize Xcs,1 J Xts	. send Xts to active clients if warm-up is needed after growing then
13:	freeze Xcs,1∣E	and warm-up Xs {	. warm up the newly added layers
14:	end if
15:	for j = 1, . . . , J do	O Local SGD updates
16:	Xcs,j+1 = Xsc,j - η gcs (Xcs,j)	. compute (mini-batch) gradient gcs on client c’s data
17:	end for
18:	∆c = Xc,J - Xc,1
19:	end for
20:	Xs+i = Xs + 也 PC=I ∆c	. aggregate updates from the clients
21:	end for
Assumption 1 (L-smoothness). The function f : Rd → R is differentiable and there exists a con-
stant L > 0 such that
kvf(X)-Vf (y)k ≤ L kx - yk.	⑶
We assume that for every input Xs, we can query an unbiased stochastic gradient gs (Xs) with
E[gs(Xs)] = vfs(Xs). We assume that the stochastic noise is bounded.
Assumption 2 (bounded noise). There exist a parameter σ2 ≥ 0 such that for any s ∈ {1, . . . , S}:
E kgs(Xs) -vfs(Xs)k2 ≤σ2,	∀Xs.
(4)
The progressive training updates Xts+1 = Xts - γtgs (Xts) with a SGD update on the model Xts. With
the two assumptions above, which are standard in the literature, we prove the convergence of sub-
models Ms as well as the model of interest M.
Theorem 1. Let Assumptions 1 and 2 hold, and let the stepsize in iteration t be γt = αtγ with
γ = min n L1,( σFτ) 10, αt =min {1,〈▽""fE(XSf ]；2)| Esi 0. Then it holds for any e > 0,
• T P=1 α2 IlvfS(Xs)|Es l∣2 < 3 after at most the following number of iterations T:
O (⅞+3) ∙ LF0.
(5)
• Let q := maxt∈[T] qt
following iterations T :
ɑtkVfS(xs)IEj ), then T PT-OIkVf(Xt) k2 < 3 after at most the
(6)
where F0 := f(X0) - (minx f (X)).
Theorem 1 shows the convergence of the full model M. The convergence is controlled by two fac-
tors, the alignment factor αt and the norm discrepancy qt . The former term measures the similarity
between the corresponding parts of the gradients computed from the sub-models and the full model
(note that αt ≡ 1 in the last phase of training on fS = f). The latter term q measures the magnitude
discrepancy (in Figure 7 we display the evolution of qt during training for one example task, note
5
Under review as a conference paper at ICLR 2022
Figure 2: Accuracy (%) vs. GFLOPs on CIFAR-100 in the centralized setting.
that qt = 1 in the last phase of training). We would like to highlight that the convergence crite-
rion in the first statement is lower bounded by the average gradient in the last phase of the training,
2 ∙ 2T PT-T/2 l∣Vf (xt)k2 ≤ 1 PT-Ia2∣∣VfS(Xs)|Es k2 (this is due to our choice of the length
of the phases, with TS ≥ T /2). This means, that progressive training will provably require at most
twice as many iterations but can reach the performance of SGD training on the full model with much
cheaper per-iteration costs.
4 Experiments
4.1	Setup
We now describe the main implementation details and provide supplementary details in Section B.
Datasets, tasks, and models. We consider four datasets: CIFAR-10 (Krizhevsky et al., 2009),
CIFAR-100 (Krizhevsky et al., 2009), EMNIST (Cohen et al., 2017), and BraTS (Menze et al.,
2014; Bakas et al., 2017; 2018). The former three are for image classification, while the last one is
a medical image dataset for tumor segmentation. We conduct centralized experiments for analyzing
the basic properties of our method while considering practical applications in federated settings. For
the centralized settings, we train VGG-16, VGG-19 (Simonyan & Zisserman, 2014), ResNet-18,
and ResNet-152 (He et al., 2016) on CIFAR-100 (100 clients, IID). For the federated settings, we
train ConvNets on CIFAR-10 and EMNIST (3400 clients, non-IID), ResNet-18 on CIFAR-100 (500
clients, non-IID), and 3D-Unet (Sheller et al., 2020) on the BraTS dataset (10 clients, IID). Note that
we follow Hsieh et al. (2020) to replace batch normalization in ResNet-18 with group normalization.
Implementation. We implement all settings with Pytorch (Paszke et al., 2019). For the experiments
in the centralized settings, we implement models based on DeVries & Taylor (2017), where we run
all experiments for 200 epochs and decay the learning rates in {60, 120, 160} epochs by a factor
of 0.1. We additionally adopt warm-up (Goyal et al., 2017) and learning rate restart (Loshchilov
& Hutter, 2017) in our method to better fit in progressive learning. For federated classification, we
follow federated learning benchmarks in (McMahan et al., 2017; Reddi et al., 2021) to implement
CIFAR-10, CIFAR-100, and EMNIST, respectively. For federated tumor segmentation, we follow
Sheller et al. (2020) for the settings and data splits. We set S = 3 for EMNIST and S = 4 for
all the other datasets and Ts as the practical guideline described in Section 3. We adopt 5 and 25
warm-up epochs for federated EMNIST and federated CIFAR-100, respectively. We summarize the
parameters (and number of clients) in Table 6.
4.2	Computation Efficiency
We first analyze the computation efficiency of our method in the centralized setting (where all
data is available on a single device) to study the effect of the progressive training in isolation,
before moving to the federated use cases. We average the outcomes over three random seeds
and we consider four architectures on CIFAR-100, including VGG-16, VGG-19, ResNet-18, and
ResNet-152. As shown in Table 2, our method performs comparably to the baselines (that train
on the full model) after 200 epochs while consuming fewer floating-point operations per second
(FLOPs) and training wall-clock time.
6
Under review as a conference paper at ICLR 2022
Figure 3: Computation cost reduction at {98%, 99%,
99.95%, best} compared to the baseline (training full
models) performance in the centralized setting.
Figure 4: Communication cost reduction at {98%,
99%, 99.95%, best} compared to the baseline perfor-
mance in the federated setting.
Figure 5: Communication cost vs. Accuracy (%) in federated settings on EMNIST (3400 clients, non-IID),
CIFAR-10 (100 clients, IID), CIFAR-100 (500 clients, non-IID), and BraTS (10 clients, IID).
To analyze the efficiency, we report the performance when consuming different levels of costs.
Figure 2 shows that our method (orange lines) consistently lies above end-to-end training on the full
model (blue lines), meaning that our method consumes fewer computation resources to improve the
models. Moreover, we visualize {98%, 99%, 99.95%, best} of the performance of the converged
baseline (analysis with a larger range is presented in Figure 12). Figure 3 indicates that our method
improves computation efficiency across architectures. In the best case, our method can accelerate
training up to 7× faster when considering limited computation budgets. We also observe that VGG
models improve more than ResNets. A possible reason might be that due to local supervision,
sub-models enjoy larger gradients compared to end-to-end training, while it rarely benefits ResNets
since skip-connections could partially avoid the problem.
4.3	Communication Efficiency
Reduction
Accuracy
Table 2: Results on CIFAR-100 in the centralized setting.
	End-to-end	Ours	Walltime	FLOPs
ResNet-18	76.08 ± 0.12	75.84 ± 0.28	-24.75%	-14.60%
ResNet-152	77.77 ± 0.38	78.57 ± 0.33	-22.75%	-19.68%
VGG16 (bn)	71.79 ± 0.15	71.54 ± 0.45	-14.57%	-13.02%
VGG19 (bn)	70.81 ± 1.18	70.90 ± 0.43	-22.10%	-14.43%
We experiment in the federated
setting to verify the communi-
cation efficiency of our method.
In particular, we consider
classification tasks on three
datasets, EMNIST, CIFAR-10,
and CIFAR-100, and tumor seg-
mentation tasks on the BraTS
dataset. We follow the standard
protocol as described in Sec-
tion 4.1 to train the models and
average the results over three random seeds. Results in Table 3 indicate that our method achieves
comparable results on EMNIST and outperforms the baselines on all the other datasets. In addition,
our method saves 20% to 30% two-way communication costs in classification and up to 63% costs
in segmentation. The result simultaneously confirms the effectiveness and efficiency of our method.
We compare the performance at different communication costs in Figure 5. We observe that our
method is communication-efficient over every cost budget, especially when the model parameters
are not evenly distributed across sub-models. For instance, 3D-Unet has most of its parameters in
the middle part of the model, making our Symmetric update strategy extremely efficient. On the
other hand, the Asymmetric strategy shows marginal improvement since it starts from the heaviest
portion of the model. The finding aligns with the motivation of progressive learning: learning from
simpler models might facilitate training. More analysis on segmentation is in Section C.4.
7
Under review as a conference paper at ICLR 2022
Table 4: Federated ResNet-18 on CIFAR-100 with compression. LQ-X denotes linear quantization followed by
used bits representing gradients, and SP-X denotes sparsification followed by the percentage of kept gradients
(See Table 10 for standard deviations).
	Float	LQ-8	LQ-4	LQ-2	SP-25	SP-10	LQ-8 +SP-25	LQ-8 +SP-10
				Accuracy				
Baseline	52.54	49.40	49.55	47.26	51.23	51.79	49.67	50.25
Ours	53.23	53.07	52.32	52.87	52.00	51.86	52.19	52.24
				Compression Ratio (%)				
Baseline	100	25.00	12.50	6.25	25.00	10.00	6.25	2.50
Ours	77.10	19.28	9.64	4.82	19.28	7.71	4.82	1.93
→-	Ours
→-	Ours-LQ8
—⅛-	Ours-LQ4
-⅛-	Ours-SP25
-∙-	Baseline-LQ8
-∙-	Baseline-LQ4
-∙-	Baseline-SP25
50	100	150	200
Cost Reduction Factor
(b) Intensive compression
Figure 6: Relative performance vs. communication cost reduction with federated ResNet-18 on CIFAR-100
with (a) modest compression and (b) intensive compression.
-⅛-	Ours-LQ2
-⅛-	Ours-SPlO
→-	Ours-LQ8+SP25
-⅛-	Ours-LQ8+SP10
-∙-	Baseline-LQ2
-∙-	BaseIine-SPlO
-∙-	Baseline-LQ8+SP25
l -∙-	Baseline-LQ8+SP10
10	20	30	40
Cost Reduction Factor
(a) Moderate compression
Table 3: Results in federated settings. We report accuracy
(%) for classification and Dice scores (%) for segmentation,
followed by cost reduction (CR) as compared to the base-
lines.
	Baseline	Ours	CR
EMNIST	85.75 ± 0.11	85.67 ± 0.06	-29.49%
CIFAR-10	84.67 ± 0.14	84.85 ± 0.30	-29.70%
CIFAR-100	52.54 ± 0.44	53.23 ± 0.09	-22.90%
BraTS (Aym.)	86.77 ± 0.45	87.66 ± 0.49	-5.02%
BraTS (Sym.)	86.77 ± 0.45	87.96 ± 0.03	-63.60 %
Lastly, we analyze the cost reduction when
achieving {98%, 99%, 99.5%, best} of the
performance of the converged baseline.
This experiment studies the behavior of
our method when only granted limited
budgets. Figure 4 (and Figure 13 in
the appendix that displays a larger range)
presents that except for the Asymmetric
strategy, our method improves communi-
cation efficiency across all datasets. In
particular, it achieves practicable perfor-
mance with 2x fewer costs in classifica-
tion and up to 6.5x fewer costs in tumor
segmentation. We also observe that the communication efficiency improves more when consid-
ering lower budgets. This property benefits when the time and communication budgets are lim-
ited (McMahan et al., 2017).
4.4	Compatibility
We show that our method complements classical compression techniques including quantization and
sparsification. We train several ResNet-18 on CIFAR-100 in the federated setting and apply linear
quantization and SParsifiCation following Konecny et al. (2016). Specifically, We consider 8 bits, 4
bits, and 2 bits for quantization (denoted by LQ-X), 25% and 10% for sparsification (denoted by
SP-X), and their combinations. Table 4 demonstrates the results of our method and the baseline
equipped with various compression techniques. Our method clearly outperforms the baselines in all
settings. It indicates that our method is more robust against compression errors, compatible with
classical techniques, and thus permits a higher compression ratio.
In addition, we visualize {50%, 80%, 90%, 99%, 99.5%, best} of relative performance against com-
munication cost reduction in Figure 6. We observe that our method is more efficient across all
percentages in every pair (Ours vs. Baseline, plotted in the same color). Besides, the baseline fails
to achieve comparable performance in many settings, e.g., the ones with quantization, while our
8
Under review as a conference paper at ICLR 2022
Table 5: Comparison between update strategies on CIFAR-100 with ResNet-18.
	Baseline	Ours	Layerwise	Partial	Mixed	Random
Acc (%)	76.08±0.12	75.84±0.28	72.40±0.16	74.70±0.04	75.04±1.26	74.38±0.97
Ratio	1	0.86	1	1	≈1	0.88
method retains comparable performance even with high compression ratios. Interestingly, even with
additional compression, our method still facilitates learning at earlier stages. For example, Ours-
LQ8+SP25 achieves comparable performance around 50x faster than the baseline, 60x faster to
achieve 80%, and more than 200x faster to achieve 50% of performance. Overall, these properties
grant our method to adequately approach limited network bandwidth and open up the possibility of
more advanced compression schemes.
4.5	Analysis of ProgFed
Effect of norm discrepancy. As discussed in Section 3.2, the convergence rate of the full model
is controlled by norm discrepancy, namely q. As qt approaches 1, the convergence rate will be
closer to the convergence speed of the sub-models. We empirically evaluate the norm discrepancy
on CIFAR-100 with ResNet-18 in the centralized setting. Figure 7 shows that the norm discrepancy
decreases as the sub-models gradually recover the full model. It suggests that spending too much
time on earlier stages may hurt the convergence speed while it offers a higher compression ratio.
This outlines the trade-off between communication and training efficiency.
Figure 7: Norm discrepancy.
Comparison between update strategies. As described
in Section 3, ProgFed progressively trains the network
from the shallowest sub-model M1 to the full model M.
We verify our update strategy by comparing it with var-
ious baselines in the centralized setting. Baseline:
end-to-end training; Layerwise only updates the latest
layer Ei while still passing the input through the whole
model M; Partial is similar to our method but ac-
quires supervision signals from the last head GS; Mixed
combines Partial and Ours, trained on supervision
from both Gi and GS ; Random randomly chooses a
sub-model Ms to update, rather than follows progressive
learning.
Table 5 presents the performance and the computation
cost ratio. We make several crucial observations. (1) Ours and Random do not pass the input
through the whole network, making them consume fewer computation costs. (2) Layerwise
greedily trains the network but achieves the worst performance, which highlights the importance
of end-to-end fine-tuning. (3) Ours outperforms Random in both costs and accuracy, verifying the
necessity of progressive learning. We also note that our method does not require additional memory
space (compared to Random) and is easy to implement.
5	Conclusion
Beyond prior work on expressing models in compact formats, we show a novel approach to modify-
ing the training pipeline to reduce the training costs. We propose ProgFed and show that progressive
learning can be seamlessly applied to federated learning for communication and computation cost
reduction. Extensive results on different architectures from small CNNs to U-Net and different tasks
from simple classification to medical image segmentation show that our method is communication-
and computation-efficient, especially when the training budgets are limited. Interestingly, we found
that a progressive learning scheme has even led to improved performance in vanilla learning and
more robust results when learning is perturbed e.g. in the case of gradient compression, which high-
lights progressive learning as a promising technique in itself with future application domains such
as privacy-preserving learning and advanced compression schemes.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
We have provided all assumptions in the main paper and attached the proof of Theorem 1 in the
appendix. All implementation details and datasets have been discussed in the main paper and the
appendix. We attached the source code as a supplementary material and will publicly release the
source code upon acceptance.
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient
sgd via gradient quantization and encoding. Advances in Neural Information Processing Systems (NeurIPS),
30:1709-1720, 2017.
Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B
Freymann, Keyvan Farahani, and Christos Davatzikos. Advancing the cancer genome atlas glioma mri
collections with expert segmentation labels and radiomic features. Scientific data, 4(1):1-13, 2017.
Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi, Rus-
sell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, et al. Identifying the best machine
learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in
the brats challenge. arXiv preprint arXiv:1811.02629, 2018.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Decoupled greedy learning of cnns. In Pro-
ceedings of the International Conference on Machine Learning (ICML), pp. 736-745. PMLR, 2020.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Com-
pressed optimisation for non-convex problems. In Proceedings of the International Conference on Machine
Learning (ICML), pp. 560-569. PMLR, 2018.
Cristian Bucilua, RiCh Caruana, and Alexandru NiCulesCu-MiziL Model compression. In Proceedings of the
12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535-541, 2006.
Christopher A Choquette-Choo, Natalie Dullerud, Adam DziedziC, Yunxiang Zhang, Somesh Jha, NiColas
Papernot, and Xiao Wang. CapC learning: Confidential and private Collaborative learning. In Proceedings of
the International Conference on Learning Representations (ICLR), 2020.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van SChaik. Emnist: Extending mnist to hand-
written letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921-2926. IEEE,
2017.
TerranCe DeVries and Graham W Taylor. Improved regularization of Convolutional neural networks with Cutout.
arXiv preprint arXiv:1708.04552, 2017.
Jonathan Frankle and MiChael Carbin. The lottery tiCket hypothesis: Finding sparse, trainable neural networks.
In Proceedings of the International Conference on Learning Representations (ICLR), 2019. URL https:
//openreview.net/forum?id=rJl-b3RcF7.
FangCheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Don’t waste
your bits! squeeze aCtivations and gradients for deep neural networks via tinysCript. In Proceedings of the
International Conference on Machine Learning (ICML), pp. 3304-3314. PMLR, 2020.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, AaPo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He. ACCurate, large minibatCh sgd: Training imagenet in 1 hour. arXiv preprint
arXiv:1706.02677, 2017.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural
network. Advances in Neural Information Processing Systems (NeurIPS), 28, 2015.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon.
Morgan Kaufmann, 1993.
Chaoyang He and Murali Annavaram. Group knowledge transfer: Federated learning of large cnns at the edge.
Advances in Neural Information Processing Systems (NeurIPS), (33), 2020.
Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr. Pipetransformer: Automated elastic
pipelining for distributed training of transformers. arXiv preprint arXiv:2102.03161, 2021.
10
Under review as a conference paper at ICLR 2022
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778,
2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire of decentralized
machine learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 4387-
4398. PMLR, 2020.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes signsgd
and other gradient compression schemes. In Proceedings of the International Conference on Machine Learn-
ing (ICML), pp. 3252-3261. PMLR, 2019.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality,
stability, and variation. In Proceedings of the International Conference on Learning Representations (ICLR),
2018.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and Dave Bacon.
Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492,
2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 598-605, 1990.
Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li, and Xiaojun Chang. Dynamic slimmable
network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
8607-8617, 2021.
Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv preprint
arXiv:1910.03581, 2019.
Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, and Linwei Wang. Progressive learning and
disentanglement of hierarchical representations. In Proceedings of the International Conference on Learning
Representations (ICLR), 2019.
Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with
feedback. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in
federated learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing the com-
munication bandwidth for distributed training. In Proceedings of the International Conference on Learning
Representations (ICLR), 2018.
Xiaorui Liu, Yao Li, Jiliang Tang, and Ming Yan. A double residual compression algorithm for efficient
distributed learning. In Proceedings of the International Conference on Artificial Intelligence and Statistics
(AISTATS), pp. 133-143. PMLR, 2020.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient
convolutional networks through network slimming. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 2736-2744, 2017.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In Proceedings of the
International Conference on Learning Representations (ICLR), 2017.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Proceedings of the Interna-
tional Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1273-1282. PMLR, 2017.
Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby,
Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image
segmentation benchmark (brats). IEEE transactions on medical imaging, 34(10):1993-2024, 2014.
11
Under review as a conference paper at ICLR 2022
Amirkeivan Mohtashami, Martin Jaggi, and Sebastian U Stich. Simultaneous training of partially masked
neural networks. arXiv preprint arXiv:2106.08895, 2021.
Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network
via relevance assessment. In Advances in Neural Information Processing Systems (NeurIPS), pp. 107-115,
1989.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances
in Neural Information Processing Systems (NeurIPS), volume 32, pp. 8026-8037. Curran Associates, Inc.,
2019.
Constantin Philippenko and Aymeric Dieuleveut. Bidirectional compression in heterogeneous settings for
distributed or federated learning with partial participation: tight convergence guarantees. arXiv preprint
arXiv:2006.14591, 2020.
Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. In
Proceedings of the International Conference on Learning Representations (ICLR), 2018.
SaShank Reddi, ZaChary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar,
and H Brendan McMahan. Adaptive federated optimization. In Proceedings of the International Conference
on Learning Representations (ICLR), 2021.
Micah J Sheller, Brandon Edwards, G Anthony Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrotsou,
Mikhail Milchenko, Weilin Xu, Daniel Marcus, Rivka R Colen, et al. Federated learning in medicine:
facilitating multi-institutional collaborations without sharing patient data. Scientific reports, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
Sebastian U. Stich. On communication compression for distributed optimization on heterogeneous data. arXiv
preprint arXiv:2009.02388, 2020.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. Advances in
Neural Information Processing Systems (NeurIPS), pp. 4452-4463, 2018.
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4323-4332, 2019.
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. Doublesqueeze: Parallel stochastic gradient
descent with double-pass error-compensated compression. In Proceedings of the International Conference
on Machine Learning (ICML), 2019.
Yifan Wang, Federico Perazzi, Brian McWilliams, Alexander Sorkine-Hornung, Olga Sorkine-Hornung, and
Christopher Schroers. A fully progressive approach to single-image super-resolution. In Proceedings of the
IEEE conference on computer vision and pattern recognition workshops, pp. 864-873, 2018.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: ternary gra-
dients to reduce communication in distributed deep learning. In Advances in Neural Information Processing
Systems (NeurIPS), 2017.
Rongliang Wu, Gongjie Zhang, Shijian Lu, and Tao Chen. Cascade ef-gan: Progressive facial expression edit-
ing with local focuses. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 5021-5030, 2020.
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. In Proceed-
ings of the International Conference on Learning Representations (ICLR), 2018.
Yue Yu, Jiaxiang Wu, and Longbo Huang. Double quantization for communication-efficient distributed opti-
mization. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proof of Theorem 1
In this section we prove Theorem 1. The proof builds on (Mohtashami et al., 2021) that considered
training of subnetworks, but not the progressive learning case.
Lemma 2. Let xt denote the weights of the full model, and xts the weights of the model that is
active in iteration t. Note that it holds xt|Es = xts|E as per the definition in the main text. It holds,
Ef(Xt+1) ≤ f (Xt)- 2 α2 ∣Wfs (Xs)IEs『+ Y2- σ2	⑺
Proof. Let’s abbreviate gts = gs(xts). By the update equation xts+1 = xts - γtgts it holds xt+1|Es =
Xt I Es - γtgs∣ E . With the L-Smoothness assumption and the definition of αt,
Ef(Xt+1) ≤ f(xt) -Yt Nf(Xt)IEs , E[gs]∣Es i + Y2-EksIEjl
=f (Xt) -Yt "f (Xt)IEs , E[gs]∣Es i + Y2-E(IgsIEs- EgsIEJl + IEgsIEJl)
≤ f (Xt) - Yt"f (Xt)IEs, NfS(XS)IEsi + y2lE(kgs - Egsk2 + ll VfS(XS)IEs ll2)
≤ f (Xt) - Yt(Vf(Xt)IEs, Vfs(Xs)IEsi + γ2L ∣∣Vfs(Xs)IEs ll2 + γ2Lσ2
≤ f (Xt) - Yt αt(1 - 20^ L) ll Vfs(Xs)IEs『+竽 σ2
≤ f (Xt) - Ytαt llVfs(Xs)IEs『+ Yt2-σ2
≤ f (Xt)- 2 α2l Vfs(Xs)IEs『+Y2- σ2
Where in the last equation We used the facts that at ≤ 1 and Yt = αtY.	□
We now prove Theorem 1.
Proof. We first define Ft := Ef(Xt) - (minx f (X)). By rearranging Lemma 2, We have
1	Eα2 llVfs(Xs)IEs ll2 ≤ Ft -Ft+1 + 呼。2.	(8)
2	Y2
Next, With telescoping summation, We have
T-1
T X Eα2 llVfs(Xs)IEs ll2 ≤ ( 0 T TT) + YLσ2 ≤ 2F + [Lσ2	(9)
T	TY	TY
We now can prove the first of part Theorem 1 by setting the step size Y to be O(min{ L, (σF20T) 2 } as
in (Mohtashami et al., 2021).
To prove the convergence of the model of interest (the second part),
1 T-1
T EkVf(Xt)II2
T t=0
1 T-1
T X
t=0
kVf(Xt)『
α2llVfs (Xs)IEsll2
αt2 llVfs(Xts)IEs ll2
T-1	T-1
T X q2α2 llVfs(Xs)IEs ll2 ≤ q2T X α2 llVfs(Xs)IEs ll2
(10)
t=0
t=0
where
kVf(Xt)k
ɑt llVfs(Xs)IEsl
and	q = max qt .
t∈[T]
(11)
13
Under review as a conference paper at ICLR 2022
Table 6: Parameters for federated experiments
Dataset	#clients	#ClientS_per_epoch	batch_size	#epochs
EMNIST	3400	68	20	1500
CIFAR-10	100	10	50	2000
CIFAR-100	500	40	20	3000
BraTS	10	10	3	100
	#epoch_per_client	#StageS (S)	Ts	#epochsfor_warmup
EMNIST	1	3	250	5
CIFAR-10	5	4	250	0
CIFAR-100	1	4	375	25
BraTS	3	4	25	0
By definition q ≥ qt for all t ∈ [T], we reach the last inequality and combine it with the first part of
the theorem.
T-1
T X 02 kVfs(Xt)k2 ≤ /
t=0	q
(12)
□
Using q^2 as the new threshold, We immediately prove the second part.
B Implementation details
We describe details of the datasets used in Section 4 and present the hyper-parameters in Table 6.
CIFAR-10. We conduct experiments on CIFAR-10 datasets for federated learning, following the
setup of previous work (McMahan et al., 2017). The dataset is divided into 100 clients randomly,
namely iid distributions for every client. We adopt the same CNN architecture with 122,570 param-
eters.
CIFAR-100. We follow the federated learning benchmark of CIFAR-100 proposed in (Reddi
et al., 2021) to conduct the experiments on CIFAR-100. We use ResNet-18/-152 (batch norm are
replaced with group norm (Hsieh et al., 2020)) and VGG-16/-19 in the centralized setting, while only
considering ResNet-18 in the federated experiments. This setup allows us to evaluate the federated
learning systems on non-IID distributions, where we use the splits as suggested in (Reddi et al.,
2021).
EMNIST. We follow the benchmark setting in (Reddi et al., 2021) to experiment. There are 3,400
clients and 671,585 training examples distributed in a non-iid fashion. The models are eventually
evaluated on 77,483 examples, resulting in a challenging task.
BraTS. In addition to image classification, we conduct experiments on brain tumor segmentation
based on (Sheller et al., 2020). We train a 3D-Unet on the BraTS2018 dataset, which includes
285 MRI scans annotated by five classes of labels. The network has 9,451,567 parameters. The
training set is randomly partitioned into ten clients. All clients participate in every training round
and locally train their models for three local epochs. This setting matches the practical medical
applications. Institutions often own relatively stable network conditions, and the data are rare and
of high resolution.
Architectures. ConvNets for EMNIST and MNIST consist of two convolution layers, termed
Conv1 and Conv2, followed by two fully connected layers, termed FC1 and FC2. To apply pro-
gressive learning with S = 3, we set Conv1, Conv2, FC1 to be the three stages, namely Ei , and
FC2 to the final head, namely GS. As for VGGs, we divide the whole networks into five compo-
nents according to the max-pooling layers. We combine the first two to be E1 and set the others to
be the remaining Ei under the setting S = 4. To apply ProgFed to ResNets He et al. (2016), we
first replace the batch normalization layers with group normalization. By convention, ResNets have
14
Under review as a conference paper at ICLR 2022
6040
20
(％) Ausnuuv
Figure 8: Performance vs. computation costs and Performance vs. epochs when comparing our method to
different updating strategies.
Figure 9: Accuracy (%) vs. Epochs on CIFAR-100 in the centralized setting.
five convolution components, i.e. Conv1, Conv2_x, Conv3_x, Conv4_x, and Conv5_x. We combine
ConvI and Conv2,x to be E1 and all the other components to be the remaining Ei. It thus matches
S = 4 in our setting.
C	More Results
C.1 Comparison between update strategies
As described in Section 4.5, we compare ProgFed to other baselines. We additionally report the
performance vs. computation costs and performance vs. epochs in Figure 8, where Ours reaches
comparable performance while consuming the least cost.
C.2 Computation Efficiency
We present more experiments in the centralized setting to prove the computation efficiency of our
method. Figure 9 presents accuracy vs. epochs with four architectures on CIFAR-100. The result
Figure 10: DICE (%) vs. computation costs on BraTS.
15
Under review as a conference paper at ICLR 2022
Figure 11: Computation acceleration at different percentage of performance. The orange bar indicates the best
performance of our method.
5	10	15	20
Cost Reduction Factor
Ooooo
9 8 7 6 5
(％)① DUeE.IOtBdω>4e-①QC
Figure 12: Computation cost reduction at {50%, Figure 13: Communication cost reduction at {50%,
60%, 70%, 80%, 90% 98%, 99%, 99.95%, best} of 60%, 70%, 80%, 90% 98%, 99%, 99.95%, best} of
the baseline performance in the centralized setting. the baseline performance in the federated setting.
indicates that our method converges comparably faster to end-to-end training in practice. Figure 11
presents Figure 3 in bar charts. Similar to Figure 3, our method improves across architectures
while VGGs benefit even more from our method. Figure 10 presents the computation costs of
3D-Unets on the BraTS dataset. We make the first observation that tumor segmentation requires
heavy computation. Interestingly, even though the earlier stages of Symmetric consume much fewer
communication costs (Figure 5), they require more computation costs than Asymmetric. It might
root from the higher resolution of feature maps that Symmetric keeps and thus lead to a trade-off
between communication and computation costs.
Figure 11 extend Figure 3 to a larger range {50%, 60%, 70%, 80%, 90% 98%, 99%, 99.95%, best}.
The result shows that our method benefits across models and is especially efficient when training
budgets are limited.
C.3 Communication Efficiency
We present more experiments in the federated setting to prove the communication efficiency of our
method. To complement Figure 5, we additionally visualize performance vs. communication costs
and performance vs. epochs in Figure 14. Although our method causes performance fluctuation in
some datasets, the performance recovers very quickly. Figure 15 presents Figure 4 in bar charts.
The results show that our method saves considerable costs in almost all settings except for EMNIST
at 95%. It is because both baseline and our method improve fast at the beginning while our method
stands out in the latter phase of training (e.g. after 98%). The result also supports that our method
improves across datasets.
We present Figure 4 in a region that models are applicable. Here, we plot the figure with a larger
range {50%, 60%, 70%, 80%, 90% 98%, 99%, 99.95%, best} in Figure 13. The result is consistent,
showing that our method benefits across datasets, and is efficient when granted limited training
budgets.
16
Under review as a conference paper at ICLR 2022
(c) ResNet-18 on CIFAR-100
(d) 3D-Unet on BraTS
Figure 14: Accuracy vs. computation costs and accuracy vs. epochs in the federated setting. (a)(b)(c) shows the
result for three classification tasks; (d) shows the result for the segmentation task, where two update strategies
Symmetric and Asymmetric are adopted for 3D-Unet.
Figure 15: Communication cost reduction at different percentage of performance. The orange bar indicates the
best performance of our method.
C.4 Visualization of Federated Segmentation
We visualize the outputs of 3D-Unet (see Section 4.3) in Figure 16. The result matches the number
reported in Table 3 that the models perform similarly after they have converged. However, the com-
munication cost consumption is disparate. Symmetric only consumes 36.40% while the others either
do not save any cost or marginally improve it. We visualize the results in Figure 17 when granted
limited communication budgets. Note that even with the same cost, Symmetric and Asymmetric may
stay in different stages because Symmetric starts from the outer part, consisting of significantly fewer
parameters. We find that Baseline and Asymmetric fail to compress the models with 0.18% since
their models are of size 34 MB (i.e. 0.908%), while only Symmetric achieve it. Interestingly, Sym-
metric has produced promising results at 0.18% of costs. It suggests that our method significantly
facilitates learning even given limited communication budgets. Meanwhile, we hope these findings
could inspire more further work on medical learning problems.
Table 7: Raw results on CIFAR-100 with four architectures in the centralized setting (to complement Table 2).
End-to-End	ProgFed (Ours)
	Seed1	Seed2	Seed3	Mean	Std	Seed1	Seed2	Seed3	Mean	Std
ResNet-18	75.95	76.12	76.17	76.08	0.12	75.57	76.13	75.83	75.84	0.28
ResNet-152	77.69	77.44	78.19	77.77	0.38	78.95	78.46	78.31	78.57	0.33
VGG16 (bn)	71.94	71.77	71.65	71.79	0.15	71.36	72.05	71.21	71.54	0.45
VGG19 (bn)	69.47	71.25	71.70	70.81	1.18	71.30	70.45	70.95	70.90	0.43
17
Under review as a conference paper at ICLR 2022
Table 8: Raw results in federated settings on EMNIST, CIFAR-10, and CIFAR-100 (to complement Table 3).
EMNIST
	Seed1	Seed2	Seed3	Mean	Std
Baseline	85.63	85.77	85.85	85.75	0.11
Ours	85.65	85.62	85.73	85.67	0.06
		CIFAR-10			
	Seed1	Seed2	Seed3	Mean	Std
Baseline	84.62	84.82	84.56	84.67	0.14
Ours	84.64	84.73	85.19	84.85	0.30
		CIFAR-100			
	Seed1	Seed2	Seed3	Mean	Std
Baseline	51.79	51.86	52.58	52.08	0.44
Ours	53.33	53.16	53.21	53.23	0.09
Table 9: Raw results on BraTS in the federated setting (to complement Table 3).
	Seed 1	Seed2	Seed 3	Mean	Std
Baseline	87.29	86.51	86.51	86.77	0.45
Idea1 (Ours)	88.19	87.22	87.57	87.66	0.49
Idea2 (Ours)	87.92	87.98	87.98	87.96	0.03
Table 10: Federated ResNet-18 on CIFAR-100 with compression. LQ-X denotes linear quantization followed
by used bits representing gradients, and SP-X denotes sparsification followed by the percentage of kept gradi-
ents. (to complement Table 4).
	Float	LQ-8	LQ-4	LQ-2
		Accuracy (%)		
Baseline	52.54 ± 0.44	49.40 ± 0.75	49.55 ± 0.59	47.26 ± 0.29
Ours	53.23 ± 0.09	53.07 ± 1.00	52.32 ± 0.15	52.87 ± 0.54
		Compression ratio (%)		
Baseline	100	25.00	12.50	6.25
Ours	77.10	19.28	9.64	4.82
	SP-25	SP-10	LQ-8 +SP-25	LQ-8 +SP-10
		Accuracy (%)		
Baseline	51.23 ± 0.56	51.79 ± 0.10	49.67 ± 1.58	50.25 ± 1.03
Ours	52.00 ± 0.19	51.86 ± 0.23	52.19 ± 0.03	52.24 ± 0.12
		Compression ratio (%)		
Baseline	25.00	10.00	6.25	2.50
Ours	19.28	7.71	4.82	1.93
18
Under review as a conference paper at ICLR 2022
Table 11: Results with FedAvg and FedProx on EMNIST and CIFAR-100.
	EMNIST	CIFAR-100
End-to-end+FedAvg	85.75	52.54
ProgFed+FedAvg	85.67	53.36
End-to-end+FedProx	86.36	53.25
ProgFed+FedProx	86.03	52.30
C.5 Raw numbers and more statistics
Table 7 8, and 9 present the raw numbers of the experiments in Table 2 and 3 over three random
seeds. Table 10 presents the standard deviations of Table 4 over three random seeds.
C.6 Generalizability of ProgFed beyond FedAvg
We show that ProgFed can generalize to other federated optimizations beyond FedAvg. In this
section we show that ProgFed can generalize to advanced optimizations. We combine our method
with FedProx and conduct experiments on EMNIST and CIFAR-100 with ConvNets and ResNet-18,
respectively. Table 11 shows that both end-to-end training and ProgFed improves over FedAvg on
EMNIST when applying FedProx. On the other, our method with FedProx shows little improvement
on CIFAR-100 while end-to-end training benefits from FedProx (52.54 vs. 53.25). However, despite
the improvement from FedProx, it remains comparable to ProgFed with FedAvg.
D More Related work
We discuss more related work in this section.
Progressive Learning. The core idea of progressive learning (PL) is to train the model from easier
tasks (e.g., low-resolution outputs or shallower models) to difficult but desired tasks (e.g., high-
resolution outputs or deeper models). It was originally proposed to stabilize the training process
and has been widely considered in vision tasks such as image synthesis (Karras et al., 2018), image
super-resolution (Wang et al., 2018), facial attribute editing (Wu et al., 2020) and representation
learning (Li et al., 2019). One of the most representative methods is PGGAN (Karras et al., 2018),
which trains GANs first for generating 4x4 images with a shallow network and progressively extends
the network for generating images of size 1024x1024. In addition to stabilizing the training process,
PL naturally reduces the computation demands since it typically employs smaller models before it
reaches the full model. This even benefits federated learning since the message size also decreases.
Despite the benefits, little work has investigated applying PL to federated learning. We note that
although some works (He et al., 2021; Belilovsky et al., 2020) also consider partially training, they
often seek solutions to fixing layers rather than considering training difficulty, and the application in
general federated tasks remain unexplored.
19
Under review as a conference paper at ICLR 2022
Input
Ground Truth
Baseline
(100%)
Asymmetric	Symmetric
(94.98%)	(36.40%)
Figure 16: Visualization of federated segmentation. From left to right: Input, Ground Truth, Baseline, Asym-
metric, and Symmetric updating strategies. Despite the comparable performance, Symmetric consumes signifi-
cantly fewer communication costs.
(a) Input
(b) ≈ 0.18%	(c) ≈ 9.40%	(d) ≈ 36.40%	(e) Ground Truth
Figure 17: Segmentation results under {≈ 0.18%, ≈ 9.40% , ≈ 36.40%} of communication costs of the
converged baseline. From top to bottom: baseline, Asymmetric (Ours), and Symmetric (Ours). Only Symmetric
can achieve 0.18% (6.7536 MB) compression ratio, since the size of the other models is already around 34 MB
(i.e. 0.908%).
20