Under review as a conference paper at ICLR 2022
Two Sides of the Same Coin:
Heterophily and Oversmoothing in Graph Con-
volutional Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
In node classification tasks, heterophily and oversmoothing are two problems that
can hurt the performance of graph convolutional neural networks (GCNs). The
heterophily problem refers to the model’s inability to handle heterophilous graphs
where neighboring nodes belong to different classes; the oversmoothing problem
refers to the model’s degenerated performance with increasing number of layers.
These two seemingly unrelated problems have been studied mostly independently,
but there is recent empirical evidence that solving one problem may benefit the
other. In this work, beyond empirical observations, we aim to: (1) analyze the
heterophily and oversmoothing problems from a unified theoretical perspective,
(2) identify the common causes of the two problems based on our theories, and
(3) propose simple yet effective strategies to address the common causes.
In our theoretical analysis, we show that the common causes of the heterophily and
oversmoothing problems—namely, the relative degree of a node (compared to its
neighbors) and its heterophily level—trigger the node representations in consec-
utive layers to "move" closer to the original decision boundary, which increases
the misclassification rate of node labels under certain constraints. We theoretically
show that: (1) Nodes with high heterophily have a higher misclassification rate.
(2) Even with low heterophily, degree disparity in a node’s neighborhood can influ-
ence the movements of node representations and result in a "pseudo-heterophily"
situation, which helps to explain oversmoothing. (3) Allowing not only positive, but
also negative messages during message passing can help counteract the common
causes of the two problems. Based on our theoretical insights, we propose simple
modifications to the GCN architecture (i.e., learned degree corrections and signed
messages), and we show that they alleviate the heteorophily and oversmoothing
problems with extensive experiments on nine real networks. Compared to other
approaches, which tend to work well in either heterophily or oversmoothing, our
modified GCN model performs well in both problems.
1 Introduction
In recent years, GCNs (Defferrard et al., 2016; KiPf & Welling, 2016; VeliCkovic et al., 2017)
have been widely used in applications ranging from social science (Li & Goldwasser, 2019) and
biology (Yan et al., 2019) to Program understanding (Allamanis et al., 2018; Shi et al., 2019). A
tyPical GCN architecture (Gilmer et al., 2017) for the node classification task can be decomPosed
into two main comPonents: ProPagation/aggregation, and combination. Messages are first exchanged
between neighboring nodes, then aggregated. Afterwards, the messages are combined with the
self-rePresentations (a.k.a., the current node rePresentations) to uPdate the node rePresentations.
Though GCNs are effective, they have some key limitations.
The first limitation is the “oversmoothing" Problem (Li et al., 2018): the Performance of GCNs
degrade when stacking many layers. Recent work has found that oversmoothing could be caused by
GCNs exPonentially losing exPressive Power in the node classification task (Oono & Suzuki, 2019)
and that the node rePresentations converge to a stationary state which is decided by the degree of the
nodes and the inPut features (Chen et al., 2020; Wang et al., 2019; Rong et al., 2019; Rossi et al.,
2020). These works focus on the analysis of the steady state in the limit of infinite layers, but they do
not exPlore the dynamics on how oversmoothing is triggered, or which nodes tend to cause it. Other
works like (Xu et al., 2018; Wang et al., 2019; Chen et al., 2020) focus on heuristic-based model
designs to alleviate the oversmoothing Problem. To fill this gaP, we seek to theoretically analyze the
dynamics around oversmoothing. In our emPirical analysis, by dividing the nodes in real graPhs into
1
Under review as a conference paper at ICLR 2022
several degree groups and tracking their average accuracy while increasing the layers of GCN (Kipf
& Welling, 2016), we observe two phases (Fig. 1): the initial stage when higher-degree nodes have
higher accuracy and the developing stage when the accuracy of higher-degree nodes decreases more
sharply (cf. § 5.4 for details). As we will show theoretically and empirically, the low-degree nodes
are the ones that trigger oversmoothing. Though Chen et al. (2020) provide some explanations for the
developing stage by analyzing the convergence rate, their theory fails to characterize the initial stage.
In fact, we find that the analysis of initial stage provides valuable insights about oversmoothing.
The second limitation of GCNs is their poor per-
formance on heterophilous graphs (Pei et al.,
2019; Lim et al., 2021), which—unlike ho-
mophilous graphs—have many neighboring
nodes that belong to different classes (Newman,
2002). For instance, in protein networks, amino
acids of different types tend to form links (Zhu
et al., 2020), and in transaction networks, fraud-
sters are more likely to connect to accomplices
than to other fraudsters (Pandit et al., 2007). Most
GCNs (KiPf & Welling, 2016; VelickoVic et al.,
2017) fail to effectively capture heterophily, so
emerging works haVe ProPosed some effectiVe
designs (Pei et al., 2019; Zhu et al., 2020), in-
cluding signed messages (PositiVe and negatiVe
interactions between nodes) ProPosed by concur-
Figure 1: Accuracy of nodes grouped by degree di
on Citeseer. Initial stage: when mean effectiVe ho-
mophily bhli (ratio of a node’s neighbors in the same
class—§ 3.3.2) is high, the accuracy increases as the
degree increases. DeVeloping stage: when bhli is low,
the accuracy of high-degree nodes drops more sharply.
rent works (Chien et al., 2021; Bo et al., 2021). Though these works justify signed messages from a
spectral perspectiVe, they focus on asymptotic results, requiring an infinite number of filters. Our
work does not make this assumption, and our theory works in more practical scenarios (§ 3.4).
These two limitations haVe mostly been studied independently, but recent work on oVersmooth-
ing (Chen et al., 2020) was shown empirically to address heterophily, and Vice Versa (Chien et al.,
2021). MotiVated by this empirical obserVation, we set out to understand the theoretical connection
between the oVersmoothing and heterophily problems by studying the change in node representations
during message passing. Specifically, we make the following contributions:
•	Theory: We introduce the first theory that explains the connections between heterophily and
oversmoothing which opens up the possibility of jointly studying the two problems.
•	Insights: We find that under conditions, the relative degree of a node (compared to its neighbors)
and its heterophily level affect the misclassification rate of node labels in the oversmoothing and
heterophily problems. We further prove that signed messages can mitigate their negative impact.
•	Improved model & empirical analysis: Based on these insights, we design a generalized GCN
model, GGCN, that allows negative interactions between nodes and compensates for the effect of
low-degree nodes through a learned rescaling scheme. Our empirical results show that our model
is robust to oversmoothing, achieves state-of-the-art performance on datasets with high levels of
heterophily, and achieves competitive performance on homophilous datasets.
2 Preliminaries
We first provide the notations & definitions that we use in the paper, and a brief background on GCNs.
Notation. We denote an unweighted and self-loop-free graph as G (V , E) and its adjacency matrix as
A. We represent the degree of node vi ∈ V by di , and the degree matrix—which is a diagonal matrix
whose elements are node degrees—by D. Let Ni be the set of nodes directly connected to vi, i.e., its
neighbors. I is the identity matrix. We denote the node representations at l-th layer as F(l) , and the
i-th row of F is fi(l) , which is the representation of node vi. The input node features are given by
F(0). The weight matrix and bias vector at the l-th layer are denoted as W(l) and b(l), respectively.
Homophily and Heterophily. Given a set of node labels/classes, homophily captures the tendency
of a node to have the same class as its neighbors. Specifically, the homophily of node vi is defined
as hi ≡ E (|NNqI), where Ni is the set of neighboring nodes with the same label as Vi, | ∙ | is the
cardinality operator, and the expectation is taken over the randomness of the node labels. High
homophily corresponds to low heterophily, and vice versa, so we use these terms interchangeably.
2
Under review as a conference paper at ICLR 2022
Supervised Node Classification Task. We focus on node classification: given a subset of labeled
nodes (from a label set L), the goal is to learn a mapping F : fi(0) 7→ yi between each node vi
and its ground truth label yi ∈ L. We use y^i to represent the predicted label of Vi and define the
misclassification rate of a fixed learnt model over a node set V as: E (l{vilyi=V∣,vi∈v"). The
expectation is taken over the randomness of graph structures and input node features.
Graph Convolutional Neural Networks. In node classification tasks, an L-layer GCN con-
tains two components (Gilmer et al., 2017): (1) neighborhood propagation and aggrega-
c(l)	(l)	(l+1)	c(l)
tion: fi(l) = AGGREGATE(fj(l), vj ∈ Ni), and (2) combination: fi(l+1) = COMBINE(fi(l),
fi(l)), where AGGREGATE and COMBINE are learnable functions.	The loss is given by
LCE=CrossEntropy(Softmax(fi(L)W(L) + b(L)), yi). The vanilla GCN suggests a renor-
malization trick on the adjacency A to prevent gradient explosion (Kipf & Welling, 2016). The
(l + 1)-th output is given by: F(l+1) = σ(AF(I)W(I)), where A = DT/2(I + A)DT/2, D is the
degree matrix of I + A, and σ is ReLU. When the non-linearities in the vanilla GCN are removed,
it reduces to a linear model called SGC (Wu et al., 2019), which has competitive performance and
is widely used in theoretical analyses (Oono & Suzuki, 2019; Chen et al., 2020). For SGC, the l-th
layer representations are given by: F(I) = AIF(O) and the last layer is a logistic-regression layer:
y^i = Softmax(F(L)W(L) + b(L)). We note that only one weight matrix W(L) is learned; it is
equivalent to the products of all weight matrices in a linear GCN. Find more related works in § 6.
3 Theoretical Analysis
In this section, we formalize the connection between the oversmoothing and heterophily problems in
two steps: (1) We show how the movements of node representations relate to the misclassification
rate of node labels; and (2) we identify the factors that affect the movements of representations. We
use the SGC model to analyze binary node classification with nodes in class 1 denoted as set V1 and
nodes in class 2 denoted as set V2. We show empirically (§ 5) that the insights obtained in this section
are effective for other non-linear models in multi-class classification.
3.1	Assumptions
Next, we use “i.d.” to represent random variables/vectors that follow the same marginal distribution
and their joint probability density function (PDF) is a permutation-invariant functionp(x1, . . . , xn) =
p(P(xι,..., Xn)), where P(∙) means permutation.
⑴ Random graph: Node degrees {di} are i.d. random variables. {(∙)i} represents a set with
i = 1, . . . , |V|. (2) Inputs: (2.1) Node labels {yi} are i.d. Bernoulli random variables given by the
ratio ρ: P ≡ P(yi=2), ∀i. (2.2) Initial input node features {fi(0)} are i.d. random vectors given by (PDF)
f(x) WhiChi	d	(X) — ʃf1 (X), when yi = 1. E(f(0) l, ) _ ʃμ,
f(x),whichisexpressedas:f(x)= f2(x), whenyi=2. E(fi |yi) = -ρ
so E(fi(0)) = 0. (3) Independence: {di} are independent of {yi} and {fi(0)}.
when yi = 1.
when yi = 2.
3.2	Misclassification Rate & Movements of Node Representations
The following lemmas illustrate why the movements of node representations are a good indicator
of SGC’s performance. We view the changes of node representations across the layers as their
movements; i.e., fi(l+1) is moved from fi(l), ∀i. The misclassification rate of an L-layer SGC is
determined by {fi(L)}, which is the input to the last regression layer, and can be studied through the
decision boundary of fi(L).
Lemma 1 In SGC, the decision boundaries w.r.t. {fi(L)} in multi-class classification are linear
hyperplanes. In particular, the decision boundary is a single hyperplane for binary classification.
Proof. We provide the proof in App. A.1.	□
The decision boundary of an (L + 1)-layer SGC can be viewed as a perturbation to the decision
boundary of an L-layer SGC due to the movements from {fi(L)} to {fi(L+1)}. Due to Lemma 1,
we can use the hyperplane xw + b = 0 to represent the decision boundary of {fi(L)}, where w
is a unit vector, b is a scalar, and the {fi(L)} are classified to class 1 if fi(L)w + b > 0. Suppose
when W = W and b = b*, we find an optimal hyperplane xw* + b = 0 that achieves the lowest
3
Under review as a conference paper at ICLR 2022
Figure 2: Node representation dynamics during neighborhood aggregation (in 1D for illustration purposes; ‘MR’:
misclassification rate). The expectation of node representations from class 1 & 2 are denoted by μ and -μ,
respectively. The bars show the expected node representations of node vi before and after the aggregation.
total misclassification rate (§ 2) for {fi(L)}. Due to the movements of node representations, the new
optimal hyperplane is decided by: xw*0 + b*0 = 0. In Lemma 2, We show that, under constraints,
moving towards the original decision boundary with a non-zero step results in a non-decreasing total
misclassification rate under the new decision boundary.
Lemma 2 Moving representations {f(L)} from class 1 by adding -tw*T, t > 0, s.t. w*Tw*0 > 0,
the new total misclassification rate is no less than the misclassification rate before the movements.
Proof. We provide the proof in App. A.2.	□
Note that the special case where the representations of the two classes swap positions (e.g, bipartite
graphs) violates the condition w*Tw*0 > 0 and leads to different conclusions. We refer to the
condition w*Tw*0 > 0 as "non-swapping condition" and throughout the paper, we analyze SGC
under that. Lemma 2 shows that, Under the "non-swapping COndition", if {fi(I)} move towards the
original decision boundary (or the other class), SGC tends to perform worse.
3.3	Movements of Node Representations in Shallow and Deeper Layers
In the following theorems, we will show how relative degree and homophily level will affect the
movement of node representations in both shallow and deeper layers. We show results given the
degree, di, of node vi and assume that vi is in class 1. The results in class 2 can be derived similarly.
3.3.1	Initial Stage: Shallow Layers
Theorem 1. [Initial Stage] Under the "non-swapping condition", during message passing in shallow
layers, the nodes with low homophily hi ≤ ι+ρρ and the nodes with high homophily hi > 1Pρ but
small relative degree are prone to be misclassified. Defining rij∙ ≡ Jdi"； and the relative degree
r ≡ EAIdi ,vi ∈V1 (J- Pj ∈Ni rij), the conditional expectation of representation f(1) is given by:
EA,{yiW”H-∈Vι(fi(I)∣vi ∈ Vι,di)= (((I + PPhdi + PT方 +1 ) E(fi(0)) ≡ Y1 Ed)),⑴
where the multiplicative factor γi1 is:
(-∞, 2], if hi ≤ 1+ρ
Yi ∈	(°, 1],	if hi > 七 & ri ≤ (i+ρyhi-ρ	(2)
、(1, ∞),	otherwise.
Proof. We provide the proof in App. A.3.	□
From Thm. 1, we identify three types of movements of node representations, which are characterized
by relative degree ri and homophily level hi. For illustration purposes, in Fig. 2, we illustrate the
three cases when we apply our theorem to 1D node representations. The bars reflect the value change
of vi’s node representation. Intuitively, under heterophily (case 1) or under high homophily but
low degrees (case 2), the expected representations will move heavily towards the opposing class.
Otherwise, they will move away from the opposing class (case 3). Among the three cases, only nodes
with high homophily and high relative degree benefit from propagation and aggregation.
Explanation of the heterophily and oversmoothing problems:
•	In heterophilous graphs, node representations move heavily towards the opposite class. According
to Lemma 2, under the "non-swapping" condition, it indicates higher misclassification rate (i.e.,
poorer performance) for SGC.
•	In homophilous graphs, high-degree nodes may initially benefit (case 3), leading to performance
gain in shallow layers. However, the representations of low-degree nodes tend to move towards the
opposing class (case 2) and eventually flip the labels. The misclassification of low-degree nodes
causes a pseudo-heterophily situation (case 1) later.
4
Under review as a conference paper at ICLR 2022
3.3.2 Developing Stage: Deeper Layers
The following theorem analyzes the pseudo-heterophily scenario in deeper layers. Based on Thm. 1,
message passing will result in scaling the node representations (γi1) and some nodes may cross
the origin. To account for those, let ξil be an accumulated discount factor at the l-th layer and
EA {y } {f(0)}∣d (df+11"i) = ξilμ. Conditioned on A and ξl, the nodes contributing positively
are defined as: NS(A,ξi) ≡ {Vj |Vj ∈ Ni and (ξiμ) ∙ E{yi},{f.(0)}lξι,A (f(')Tlξi, A) > 0}. We
denote： %3},{咪}匹段吗∈Ni 1(%1di, ξi,vj ∈ N)= {-。，4〃	∈ NS(a, ξi). ξi and
ρli characterize the neighborhood property of vi . We further define the effective homophily of node i
as h = P(Vj ∈ NS |vj ∈ Ni,di,ξi). Intiuitively, even if the original graph is not heterophilous, from
the perspective of message passing under this pseudo-heterophily scenario, the deeper layers will act
as if the graph is more heterophilous than it truly is, which may increase the misclassification rate.
Theorem 2.	[Developing Stage] Under the “non-swapping condition”, during message passing in
deeper layers, when the effective homophily hi is sufficiently low (hi ≤ -ρiτ), the nodes with higher
1+ρi
effective relative degree are prone to misclassification. Specifically, by defining the effective relative
degree as ri ≡ ξi-, the conditional expectation of fi(l+1) is:
ea,3},{吸.,S)f(I+1)ldi,ξi)! = "hi(1+曲；L+1 ξiμ ≡ γ(l+1)Ef%. (3)
Proof. We provide the proof in App. A.4. Other nodes can be derived similarly.	□
We note that Eq. (3) holds for layers l ≥ 2 in general. We regard layers for which hi > 1+^ holds
for most nodes i as part of the initial stage, as they exhibit three cases similar to Eq. (2).
3.4 Signed Messages & Movements of Representations
In this section, we provide theory to show that signed messages can help enhance the performance in
heterophilous graphs and also alleviate oversmoothing. Due to limited space, we only show the effect
of signed messages in the initial stage; similar results can be derived in the developing stage.
Setup. Signed messages consist of the negated messages sent by neighbors of the opposing class (i.e.,
after multiplying initial messages by -1), and the unchanged messages ("positive messages") sent by
neighbors of the same class. In reality, ground-truth labels are not accessible for every node, so we
use approximations, which introduces errors. For node vi , we define mli as the ratio of neighbors
that send incorrect messages at the l-th layer (i.e., different-class neighbors that send non-negated
messages and same-class neighbors that send negated messages). We define the l-th layer error rate
as eli = E(mli), where the expectation is over the randomness of the neighbors that send incorrect
messages. We make the following assumption: mli is independent of {di}, {yi} and {fi0}.
Theorem 3.	[Signed Messages] With the independence assumptions and under the “non-swapping
condition”, by allowing the messages to be optionally multiplied by a negative sign, in the initial
stage, the movements of representations will be less affected by the initial homophily level hi, and
will be dependent on the error rate eli. Specifically, the multiplicative factor γi1 at the first layer is:
EA,{yi},{f(0)}∣di,Vi∈Vι (f(1) ldi, vi ∈ VI)
(I - 2e0)(p + (I — p)hi)diri + 1
di + 1
((-∞, 1 ],
where γi1 ∈	(0, 1],
[(1, ∞),
Proof. The proof is provided in App. A.5.
μ ≡ γlE(fi%,
(4)
(5)
□
if e0 ≥ 0.5
if e0 < 05& ri ≤ (i-2e0)(ρ+(i-ρ)hi)
otherwise.
From Eq. 5, we see that when using signed messages, to benefit from case 3 (γi1 > 1), the minimum
relative degree satisfies: ri >(1-2：0)(.;(1—)%.). Given hi ≤ 1, if the error rate is low (e0《0.5),
we get: (1-2e0)(ρ+(1-ρ)hi) ≤ (1+ρ1hi-ρ , and (1+ρ1hi-ρ is the minimum relative degree required
when not using signed messages. This implies that more nodes can benefit from using signed messages.
We note that if low error rate cannot be guaranteed, signed messages may hurt the performance.
5
Under review as a conference paper at ICLR 2022
4	Model Design
Based on our theoretical analysis, we propose two new, simple mechanisms to address both the
heterophily and oversmoothing problems: signed messages and degree corrections. We integrate
these mechanisms, along with a decaying combination of the current and previous node represen-
tations (Chen et al., 2020), into a generalized GCN model, GGCN. In § 5, we empirically show its
effectiveness in addressing the two closely-related problems.
4.1	Signed Messages
Thm 3 points out the importance of signed messages in tackling the heterophily and oversmoothing
problems. Our first proposed mechanism uses cosine similarity to send signed messages.
For expressiveness, as in GCN (Kipf & Welling, 2016), we first perform a learnable linear transfor-
(l)	(l)	(l)
mation of each node’s representation at the l-th layer: F(l) = F(l) W(l) + b(l) Then, we define a sign
function to be multiplied with the messages exchanged between neighbors. To allow for backpropaga-
tion of the gradient information, we approximate the sign function with cosine similarity. Denote Sl
as the matrix which stores the sign information about the edges, defined as: S(l) [i, j] =Cosine(fi(l),
fj(l)) if (i 6= j) & (vj ∈ Ni); 0 otherwise.
In order to separate the contribution of similar neighbors (likely in the same class) from that of
dissimilar neighbors (unlikely to be in the same class), we split S(l) into a positive matrix S(plo)s and
a negative matrix S(nle)g. Thus, our proposed GGCN model learns a weighted combination of the
self-representations, the positive messages, and the negative messages:
F(l+1) = σ

............ αi(β0 d +β1(spos © A)d +β2(sneg © A)d)),	(6)
where β0l , β1l and β2l are scalars obtained by applying softmax to the learned scalars β0l , β1l and
l	ll
β2l ; the non-negative scaling factor αl = softplus(αl) is derived from the learned scalar αl ; is
element-wise multiplication; and σ is the nonlinear function Elu. We note that we learn different
α and β parameters Per layer for flexibility. We also require the combined weights, αlβ1, to be
non-negative so that they do not negate the intended effect of the signed information.
4.2	Degree Corrections
Our analysis in § 3.3 and § 3.4 highlights that, when the error rate and homophily level are high,
oversmoothing is initially triggered by low-degree nodes. Thus, our second proposed mechanism
aims to compensate for low degrees via degree corrections.
Based on Eq. (5), and given that most well-trained graph models have a relatively low error rate
at the shallow layers (i.e, ei《0.5 for small l), We require that the node degrees satisfy r >
(i-2e0)(∏+(i-∏)h∙) to prevent oversmoothing. Since the node degrees cannot be modified, our
strategy is to rescale them. Specifically, we correct the degrees by multiplying with scalars τilj and
change the original formulation as follows:
(Ad)[i, ：] = F^
di + 1
+ X	F叫j,:]
vj∑Ni √di + 1 pdj + 1
Σ
vj ∈Ni
τl,j F(I) [j,:]
√di + 1 Pd + 1
(7)
This multiplication is equivalent to changing the ratio rj in Thm. 1 to JTij d,+d1+1). That is, a larger
Tij increases the effective r at layer l. Training independent τ-,j is not practical because it would
require O(|V |2) additional parameters per layer, which can lead to overfitting. Moreover, low-rank
parameterizations suffer from unstable training dynamics. Intuitively, when rij is small, we would
like to compensate for it via a larger τil,j. Thus, we set τil,j to be a function of rij as follows:
Tilj = SoftPIUS (λ0 (rij - 1) + λl) ,	(8)
where λl0 and λl1 are learnable parameters. We subtract 1 so that when rij = 1 (i.e., di = dj), then
τilj = softplus(λl1) is a constant bias.
In our proposed GGCN model, we combine the signed messages in Eq. (6) and our degree correction
mechanism to obtain the representations at layer l + 1:
F(l+1)= σ(αl(β0 d + β1 ( Spos © A ©T(l) )Fd + β2 ( Sneg © A ©T (I))d)}	(9)
where T(l) is a matrix with elements τj.
6
Under review as a conference paper at ICLR 2022
4.3	Decaying Aggregation
In addition to our two proposed mechanisms that are theoretically grounded in our analysis (§ 3), we
also incorporate into GGCN an existing design—decaying aggregation of messages—that empirically
increases performance. However, we note that, even without this design, our GCN architecture still
performs well under heterophily and is robust to oversmoothing (App. §B.1).
Decaying aggregation was introduced in (Chen et al., 2020) as a way to slow down the convergence
rate of node representations. Inspired by this work, We modify the decaying function, η^, and integrate
it to our GGCN model:
F(l+1)
F(I) + η
(10)
In practice, we found that the following decaying function works well: η ≡ ln(* +1), iff l ≥ lo； η
1, otherwise. The hyperparameters k, l0, η are tuned on the validation set.
5	Experiments
Due to the space limit, in the main paper, we focus on the following three questions: (Q1) Compared
to the baselines, how does GGCN perform on homophilous and heterophilous graphs? (Q2) How
robust is it against oversmoothing under homophily and heterophily? (Q3) How can we verify the
correctness of our theorems on real datasets? We provide the ablation study of signed messages and
degree correction in App. § B.1 and the study of different normalization strategies in App. § B.2.
5.1	Experimental Setup
Datasets. We evaluate the performance of our GGCN model and existing GNNs in node classification
on various real-world datasets (Tang et al., 2009; Rozemberczki et al., 2019; Sen et al., 2008; Namata
et al., 2012; Bojchevski & Gunnemann, 2018; Shchur et al., 2018). We provide their summary
statistics in Table 1, where we compute the homophily level h of a graph as the average of hi of all
nodes vi ∈ V . For all benchmarks, we use the feature vectors, class labels, and 10 random splits
(48%/32%/20% of nodes per class for train/validation/test1) from (Pei et al., 2019).
Baselines. For baselines we use (1) classic GNN models for node classification: vanilla GCN (Kipf
& Welling, 2016), GAT (Velickovic et al., 2017) and GraphSage (Hamilton et al., 2017); (2) re-
cent models tackling heterophily: Geom-GCN (Pei et al., 2019), H2GCN (Zhu et al., 2020) and
GPRGNN (Chien et al., 2021); (3) models tackling oversmoothing: PairNorm (Zhao & Akoglu,
2019) and GCNII (Chen et al., 2020) (state-of-the-art); and (4) 2-layer MLP (with dropout and Elu
non-linearity). For GCN, PairNorm, Geom-GCN, GCNII, H2GCN and GPRGNN, we use the original
codes provided by the authors. For GAT, we use the code from a well-accepted Github repository2.
For GraphSage, we report the results from (Zhu et al., 2020), which uses the same data and splits. For
the baselines that have multiple variants (Geom-GCN, GCNII, H2GCN), we choose the best variant
for each dataset and denote them as [model]*. For each dataset and baseline, if applicable, we use the
best hyperparameters provided by the authors. Otherwise, we perform parameter search to set the
best hyperparameters for each baseline. The setting and analysis of hyperparameters are in App. § C.
Machine. We ran our experiments on Nvidia V100 GPU.		
Table 1: Real data: mean accuracy ± stdev over different data splits. Per GNN model, we report the best		
performance across different layers. Best model per benchmark highlighted in gray. The "'"results (GraphSAGE)		
are obtained from (Zhu et al., 2020).		
Texas Wisconsin Actor	Squirrel Chameleon Cornell Citeseer Pubmed	Cora		
Hom. level h	0.11	0.21	0.22	0.22	0.23	0.3	0.74	0.8	0.81	
#Nodes	183	251	7,600	5,201	2,277	183	3,327	19,717	2,708	
#Edges	295	466	26,752	198,493	31,421	280	4,676	44,327	5,278	
#Classes	5	5	5	5	5	5	7	36	
GGCN (ours) 84.86±4.55 86.86±3.29 37.54±1.56 5 5.17±1.58 71.14±1.84 85.68±6.63 77.14±1	.45 89.15±0.37 87.95±1	05	1.78
GPRGNN	78.38±4.36 82.94±4.21 34.63±1.22 31.61±1.24 46.58±1.71 80.27±8.ιι 77.13±1	.67 87.54±0.38 87.95±1	18	5.56
H2GCN*	84.86± 7.23 87.65 ±4.98 35.70±1.00 3 6.48±1.86 60.11±2.15 82.70±5.28 77.11±1	57 89.49±0.38 87.87±1	20	3.89
GCNII*	77.57±3.83 80.39±3.4 37.44±1.30 38.47±1.58 63.86±3.04 77.86 ±3.79 77.33±1	48 90.15±0.43 88.37±1	25	3.56
Geom-GCN* 66.76±2.72 64.51±3.66 31.59±1.15 38.15±0.92 60.00±2.81 60.54±3.67 78.02±1	15 89.95±0.47 85.35±1	57	6.11
PairNorm	60.27±4.34 48.43 ±6.14 27.40±1.24 50.44±2.04 62.74±2.82 58.92±3.15 73.59±1	47 87.53±0.44 85.79±1	.01	7.78
GraPhSAGEt 82.43±6.14 81.18±5.56 34.23±0.99 41.61±0.74 58.73±1.68 75.95±5.01 76.04±1	.30 88.45±0.50 86.90±1	.04	5.78
GCN	55.14±5.16 51.76±3.06 27.32±1.10 53.43±2.01 64.82±2.24 60.54±5.3 76.50±1	.36 88.42±0.5 86.98±1	.27	6.56
GAT	52.16±6.63 49.41±4.09 27.44±0.89 40.72±1.55	60.26±2.5 61.89±5.05 76.55±1	.23 86.33±0.48 87.30±1	.10	7.22
MLP	80.81±4.75 85.29±3.31 36.53±0.70 28.77±1.56 46.21±2.99 81.89±6.40 74.02±1	.90 87.16±0.37 75.69±2	.00	6.78
1(Pei et al., 2019) claims that the ratios are 60%/20%/20%, which is different from the actual data splits
shared on GitHub.
2https://github.com/Diego999/pyGAT
7
Under review as a conference paper at ICLR 2022
Table 2: Model performance for different layers: mean accuracy ± stdev over different data splits. Per dataset
and GNN model, we also report the layer at which the best performance (given in Table 1) is achieved. ‘OOM’:
out of memory; ‘INS’: numerical instability. For larger font, refer to Table D.1 in the Appendix.
Layers	2	4	8	16	32	64	Best	2	4	8	16	32	64	Best
			Cora (h=0.81)							Citeseer (h=0.74)				
GGCN (ours)	87.00±1.15	87.48±1.32	87.63±1.33	87.51±ι.i9	87.95±1.05	87.28±1.41	32	76.83±1.82	76.77±1.48	76.91±1.56	76.88±i.56	76.97±1.52	76.65±1.38	10
GPRGNN	87.93±1.11	87.95±1.18	87.87±1.41	87.26±1.51	87.18±1.29	87.32±1.21	4	77.13±1.67	77.05±1.43	77.09±1.62	76.00±1.64	74.97±1.47	74.41±1.65	2
H2GCN*	87.87±1.20	86.10±1.51	86.18±2.10	OOM	OOM	OOM	2	76.90±1.80	76.09±1.54	74.10±1.83	OOM	OOM	OOM	1
GCNII*	85.35±1.56	85.35±1.48	86.38±0.98	87.12±1.11	87.95±1.23	88.37±1.25	64	75.42±1.78	75.29±1.90	76.00±1.66	76.96±1.38	77.33±1.48	77.18±1.47	32
PairNorm	85.79±1.01	85.07±0.91	84.65±1.09	82.21±2.84	60.32±8.28	44.39±5.60	2	73.59±1.47	72.62±1.97	72.32±1.58	59.71±15.97	27.21±10.95	23.82±6.64	2
Geom-GCN*	85.35±1.57	21.01±2.61	13.98±1.48	13.98±1.48	13.98±1.48	13.98±1.48	2	78.02±1.15	23.01±1.95	7.23±0.87	7.23±0.87	7.23±0.87	7.23±0.87	2
GCN	86.98±1.27	83.24±1.56	31.03±3.08	31.05±2.36	30.76±3.43	31.89±2.08	2	76.50±1.36	64.33±8.27	24.18±1.71	23.07±2.95	25.3±1.77	24.73±1.66	2
GAT	87.30±1.10	86.50±1.20	84.97±1.24	INS	INS	INS	2	76.55±1.23	75.33±1.39	66.57±5.08	INS	INS	INS	2
			Cornell (h=0.3)							Chameleon (h=0.23)				
GGCN (ours)	83.78±6.73	83.78±6.16	84.86±5.69	83.78±6.73	83.78±6.51	84.32±5.90	6	70.77±1.42	69.58±2.68	70.33±1.70	70.44±i.82	70.29±1.62	70.20±1.95	5
GPRGNN	76.76±8.22	77.57±7.46	80.27±8.11	78.38±6.04	74.59±7.66	70.00±5.73	8	46.58±1.771	45.72±3.45	41.16±5.79	39.58±7.85	35.42±8.52	36.38±2.40	2
H2GCN*	81.89±5.98	82.70±6.27	80.27±6.63	OOM	OOM	OOM	1	59.06±1.85	60.11±2.15	OOM	OOM	OOM	OOM	4
GCNII*	67.57±11.34	64.59±9.63	73.24±5.91	77.84±3.97	75.41±5.47	73.78±4.37	16	61.07±4.10	63.86±3.04	62.89±1.18	60.20±2.10	56.97±1.81	55.99±2.27	4
PairNorm	50.27±7.17	53.51±8.00	58.38±5.01	58.38±3.01	58.92±3.15	58.92±3.15	32	62.74±2.82	59.01±2.80	54.12±2.24	46.38±2.23	46.78±2.26	46.27±3.24	2
Geom-GCN*	60.54±3.67	23.78±11.64	12.97±2.91	12.97±2.91	12.97±2.91	12.97±2.91	2	60.00±2.81	19.17±1.66	19.58±1.73	19.58±1.73	19.58±1.73	19.58±1.73	2
GCN	60.54±5.30	59.19±3.30	58.92±3.15	58.92±3.15	58.92±3.15	58.92±3.15	2	64.82±2.24	53.11±4.44	35.15±3.14	35.39±3.23	35.20±3.25	35.50±3.08	2
GAT	61.89±5.05	58.38±4.05	58.38±3.86	INS	INS	INS	2	60.26±2.50	48.71±2.96	35.09±3.55	INS	INS	INS	2
5.2	Q1. Performance Under Homophily & Heterophily														
Table 1 provides the test accuracy of different GNNs on the supervised node classification task over
datasets with varying homophily levels (arranged from low homophily to high homophily). A graph’s
homophily level is the average of nodes’ homophily levels. We report the best performance of each
model across different layers. In App. § B.4, we provide the analysis based on the nodes’ homophily
levels (§ 3), which is a better metric to predict GCN’s performance and is aligned with our theory.
GGCN performs the best in terms of average rank (1.78) across all datasets, which suggests its strong
adaptability to graphs of various homophily levels. In particular, GGCN achieves the highest accuracy
in 5 out of 6 heterophilous graphs (h is low). For datasets like Chameleon and Cornell, GGCN
enhances accuracy by around 6% and 3% compared to the second-best model. On homophily datasets
(Citeseer, Pubmed, Cora), the accuracy of GGCN is within a 1% difference of the best model.
Our experiments highlight that MLP is a good baseline in heterophilous datasets. In heterophilous
graphs, the models that are not specifically designed for heterophily usually perform worse than an
MLP. Though H2GCN* is the second best model in heterophilous datasets, we can still see that in the
Actor dataset, MLP performs better. GPRGNN and Geom-GCN*, which are specifically designed for
heterophily, achieve better performance than classic GNNs (GCN and GAT) in heterophilous datasets,
but do not show clear advantage over MLP. Our GGCN model is the only model that performs better
than MLP across all the datasets.
In general, GNN models perform well in homophilous datasets. GCNII* performs the best, and
GGCN, H2GCN*, GPRGNN and Geom-GCN* also achieve high performance.
5.3	Q2. Oversmoothing
We also test how robust the models are to oversmoothing. To this end, we measure the supervised
node classification accuracy for 2 to 64 layers. Table 2 presents the results for two homophilous
datasets (top) and two heterophilous datasets (bottom). Per model, we also report the layer at which
the best performance is achieved (column ‘Best’). We provides Table 2 in larger font in App. § D.
According to Table 2, GGCN and GCNII* achieve increase in accuracy when stacking more layers in
four datasets, while GPRGNN and PairNorm exhibit robustness against oversmoothing. Models that
are not designed for oversmoothing have various issues. The performance of GCN and Geom-GCN*
drops rapidly as the number of layers grows; H2GCN* requires concatenating all the intermediate
outputs and quickly reaches memory capacity; GAT’s attention mechanism also has high memory
requirements. We also find that GAT needs careful initialization when stacking many layers as it may
suffer from numerical instability in sparse tensor operations.
In general, models like GGCN, GCNII*, and GPRGNN that perform well under heterophily usually
exhibit higher resilience against oversmoothing. One exception is Geom-GCN*, which suffers more
than GCN. This model incorporates structurally similar nodes into each node’s neighborhood; this
design may benefit Geom-GCN* in the shallow layers as the node degrees increase. However, as
we point out in Thm. 2, when the effective homophily is low, higher degrees are harmful. If the
structurally similar nodes introduce lower homophily levels, their performance will rapidly degrade
once the effective homoPhily is lower than ι^. On the other hand, GGCN virtually changes the
degrees thanks to the degree correction mechanism (§ 4.2), and, in practice, this design has positive
imPact on its robustness to oversmoothing.
8
Under review as a conference paper at ICLR 2022
5.4	Q3. Empirical Verification of the Initial & Developing Stages
Using the vanilla GCN model (Kipf & Welling, 2016), we validate our theorems by measuring the test
accuracy and effective homophily for different node degrees (binned logarithmically) on real datasets.
We estimate the effective homophily as the portion of the same-class neighbors that are correctly
l
classified before the last propagation. Figure 1 shows the results for Citeseer. In the initial stage (hli
is high), the accuracy increases with the degree, but in the developing stage, the trend changes, with
high-degree nodes being impacted the most, as predicted by our theorems. In App. § B.3, we provide
more details for this analysis and we further verify our conclusion on Cora.
6	Related Work
Graph Convolutional Neural Networks. Early on, (Defferrard et al., 2016) proposed a GCN model
that combines spectral filtering of graph signals and non-linearity for supervised node classification.
The scalability and numerical stability of GCNs was later improved with a localized first-order
approximation of spectral graph convolutions proposed in (KiPf & Welling, 2016). (VeliCkoviC et al.,
2017) proposes the first graph attention network to improve neighborhood aggregation. Many more
GCN variants have been proposed for different applications such as: computer vision (Satorras &
Estrach, 2018), social science (Li & Goldwasser, 2019), biology (Yan et al., 2019), algorithmic
tasks (VeliCkoviC et al., 2020; Yan et al., 2020), and inductive classification (Hamilton et al., 2017).
Concurrent work (Baranwal et al., 2021) provides theoretic analysis on the linear separability of
graph convolution but does not provide effective strategies to increase the separability.
Oversmoothing. The oversmoothing problem was first discussed in (Li et al., 2018), which proved
that by repeatedly applying Laplacian smoothing, the representations of nodes within each connected
component of the graph converge to the same value. Since then, various empirical solutions have been
proposed: residual connections and dilated convolutions (Li et al., 2019); skip links (Xu et al., 2018);
new normalization strategies (Zhao & Akoglu, 2019); edge dropout (Rong et al., 2019); and a new
model that even increases performance as more layers are stacked (Chen et al., 2020). Some recent
works provide theoretical analyses: (Oono & Suzuki, 2019) showed that a k-layer renormalized graph
convolution with a residual link simulates a lazy random walk and (Chen et al., 2020) proved that the
convergence rate is related to the spectral gap of the graph.
Heterophily & GCNs. Heterophily has recently been recognized as an important issue for GCNs.
It is first outlined in the context of GCNs in (Pei et al., 2019). (Zhu et al., 2020) identified a set of
effective designs that allow GCNs to generalize to challenging heterophilous settings, and (Zhu et al.,
2021) introduced a new GCN model that leverages ideas from belief propagation (Gatterbauer et al.,
2015). Though recent work (Chen et al., 2020) focused on solving the oversmoothing problem, it
also empirically showed improvement on heterophilous datasets; these empirical observations formed
the basis of our work. Finally, (Chien et al., 2021) recently proposed a PageRank-based model that
performs well under heterophily and alleviates the oversmoothing problem. However, they view the
two problems independently and analyze their model via an asymptotic spectral perspective. Our
work studies the representation dynamics and unveils the connections between the oversmoothing
and heterophily problems theoretically and empirically. As we demonstrate with GGCN, addressing
both issues in a principled manner provides superior performance across a variety of datasets.
7	Conclusion
Our work provides the first theoretical and empirical analysis that unveils the connections between
the oversmoothing and heterophily problems. By analyzing the statistical change of the node
representations after the graph convolution, we identified two causes, i.e., the relative degree of a
node compared to its neighbors and the level of heterophily in its neighborhood, which influence the
movements of node representations and lead to a higher misclassification rate. Based on our new,
unified theoretical perspective, we obtained three important insights: (1) Nodes with high heterophily
tend to be misclassified after graph convolution; (2) Even with low heterophily, low-degree nodes can
trigger a pseudo-heterophily situation that explains oversmoothing. (3) Allowing signed messages
(instead of only positive messages) helps alleviate the heterophily and oversmoothing problems.
Based on these insights, we designed a generalized model, GGCN, that addresses the identified
causes using signed messages and degree corrections. Though other designs may also address these
two problems, our work points out two effective directions that are theoretically grounded (§ 3). In
summary, our research suggests it is beneficial to study the oversmoothing and heterophily problems
jointly; this leads to architectural insights that can improve the learned representations of graph neural
network models across a variety of domains.
9
Under review as a conference paper at ICLR 2022
8	Reproducibility Statement
To reproduce our experimental results, we provide the references to the datasets and baselines in § 5.
For the baselines, we use code provided by the authors and set the hyperparameters as the authors
suggest. In App. § C, we provide detailed hyperparameter setting for GGCN. We also provide our
code in the supplementary material and we will share the link to our code upon acceptance.
References
Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs,
2018.
Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Graph convolution for semi-supervised classifi-
cation: Improved linear separability and out-of-distribution generalization. arXiv preprint arXiv:2102.06966,
2021.
Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolutional
networks. arXiv preprint arXiv:2101.00797, 2021.
Aleksandar Bojchevski and StePhan Gunnemann. Deep gaussian embedding of graphs: UnsUPervised inductive
learning via ranking. International Conference on Learning Representations (ICLR), 2018.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional
networks. In International Conference on Machine Learning, pp. 1725-1735. PMLR, 2020.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural
network, 2021.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with
fast localized spectral filtering. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, pp. 3844-3852, 2016.
Wolfgang Gatterbauer, Stephan Gunnemann, Danai Koutra, and Christos Faloutsos. Linearized and single-pass
belief propagation. Proc. VLDB Endow., 8(5):581-592, January 2015.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing
for quantum chemistry. In International conference on machine learning, pp. 1263-1272. PMLR, 2017.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025-1035,
2017.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv
preprint arXiv:1609.02907, 2016.
Chang Li and Dan Goldwasser. Encoding social information with graph convolutional networks forpolitical
perspective detection in news media. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pp. 2594-2604, 2019.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9267-9276, 2019.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-
supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim.
Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in
Neural Information Processing Systems, 34, 2021.
Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. Query-driven active surveying for
collective classification. In 10th International Workshop on Mining and Learning with Graphs, volume 8,
2012.
Mark EJ Newman. Assortative mixing in networks. Physical review letters, 89(20):208701, 2002.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification.
International Conference on Learning Representations, 2019.
10
Under review as a conference paper at ICLR 2022
Shashank Pandit, Duen Horng Chau, Samuel Wang, and Christos Faloutsos. Netprobe: a fast and scalable system
for fraud detection in online auction networks. In Proceedings of the 16th international conference on World
Wide Web,pp. 201-210, 2007.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph
convolutional networks. International Conference on Learning Representations, 2019.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional
networks on node classification. International Conference on Learning Representations, 2019.
Ryan A. Rossi, Di Jin, Sungchul Kim, Nesreen K. Ahmed, Danai Koutra, and John Boaz Lee. On proximity and
structural role-based embeddings in networks: Misconceptions, techniques, and applications. ACM Trans.
Knowl. Discov. Data, 14(5), August 2020.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. arXiv preprint
arXiv:1909.13021, 2019.
Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. International
Conference on Learning Representations, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective
classification in network data. AI magazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls of graph
neural network evaluation. Relational Representation Learning Workshop, NeurIPS 2018, 2018.
Zhan Shi, Kevin Swersky, Daniel Tarlow, Parthasarathy Ranganathan, and Milad Hashemi. Learning execution
through neural code fusion. International Conference on Learning Representations, 2019.
Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks. In Proceedings
of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 807-816,
2009.
Petar VelickoviC, Guillem Cucurull, Arantxa Casanova, Adriana Romero, PietrO Lio, and Yoshua Bengio. Graph
attention networks. arXiv preprint arXiv:1710.10903, 2017.
Petar Velickovic, Lars Buesing, Matthew Overlan, Razvan Pascanu, Oriol Vinyals, and Charles Blundell. Pointer
graph networks. Advances in Neural Information Processing Systems, 33, 2020.
Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Improving graph attention networks with large
margin-based constraints. arXiv preprint arXiv:1910.11945, 2019.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph
convolutional networks. In International conference on machine learning, pp. 6861-6871. PMLR, 2019.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka.
Representation learning on graphs with jumping knowledge networks. In International Conference on
Machine Learning, pp. 5453-5462. PMLR, 2018.
Yujun Yan, Jiong Zhu, Marlena Duda, Eric Solarz, Chandra Sripada, and Danai Koutra. Groupinn: Grouping-
based interpretable neural network for classification of limited, noisy brain data. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 772-782, 2019.
Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. Neural execution
engines: Learning to execute subroutines. Advances in Neural Information Processing Systems, 33, 2020.
Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. International Conference on
Learning Representations, 2019.
Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in
graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing
Systems, 33, 2020.
Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra. Graph
neural networks with heterophily. In AAAI Conference on Artificial Intelligence, 2021.
11
Under review as a conference paper at ICLR 2022
A Detailed Proofs of Theorems in § 3
A.1 Proof of Lemma 1
Proof. The loss function for SGC is: CrossEntropy(Softmax(F(L)W(L) + b(L)), yi). Rewrite
W(L) into an array of column vectors: W(L) = [w0(L), w(1L), . . . w|(LL|)], b(L) into an array of scalars:
b(L) = [b0L),b1L),... b(L)], and F(L) into an array of row vectors: F(L) = [f0L); f(L);... f(L∣)] ([∙; ∙]
fi(L)w(cL) +b(cL)	1, iff yi = c
means StaCking VertiCally). Let g,c ≡ Pi f(L)W(L)+b(L) and let yi,c ≡ ∣0	OtherWise then we
rewrite the loss function as:
n |L|
-1Σ Σ>,c ∙log(Pi,c),	(11)
i=1 c=1
n is the number of nodes used for training. To prediCt that the node vi belongs to Class c, we require:
∀c0 6= c, pi,c > pi,c0. Thus, we have:
ef(L)W(L)+bCL)	、ef(L)WcL )+bCL)	C
L	f(L)W(L)+ b(L) > L	f(L)W(L)+b(L) .	(12)
jei j j	jei j j
It is equivalent to:
fi(L)w(cL) + b(cL) > fi(L)wc(L0 ) + b(cL0 ).	(13)
It shows that the deCision region of Class c is deCided by a set of hyper-planes:
x(wc(L) - wc(L0 )) + (b(cL) - b(cL0 )) > 0.	(14)
□
x is the veCtor variable that desCribes the plane. Any veCtors that satisfy the equation are on this
plane. We note that for binary ClassifiCation, there is one hyper-plane.
A.2 Proof of Lemma 2
Before giving the proof, we first explain the meaning of the Lemma. As we point out in § 3, the
misClassifiCation rate of an L-layer SGC Can be studied through its last logistiC-regression layer, the
input to whiCh is {fi(L)}. To study the misClassifiCation rate of an (L + 1)-layer SGC, we Can view
the input to the last layer {fi(L+1)} as being moved from {fi(L)}. The misClassifiCation rate is Closely
related to the deCision boundary and will be our tool for studying the Change of the misClassifiCation
rate. This Lemma studies the movements that bring the node representations Closer to the original
deCision boundary; we will prove that moving towards the original deCision boundary by a non-zero
step is not benefiCial (i.e., harmful) to the SGC’s performanCe.
Proof. We prove it by ContradiCtion and suppose that after the movements, the total misClassifiCation
rate is lowered. We denote the Conditional distribution of {fi(L)} as f1L(x) Conditioned on that they
are from Class 1 and f2L(x) Conditioned on that they are from Class 1 or Class 2, respeCtively. To note
that, the distributions of different layers are different.
After the movements, the representations from class 1 become: {f(L) - tw*T},t > 0. tw*T
represents moving towards the original deCision boundary along the norm direCtion by a non-zero
step. This causes a corresponding change in the conditional PDF given that they are from class 1:
fL(x + tw*T).
Recall that in § 3, the parameters for the original optimal hyperplane W and b*, and the parameters
for the later optimal hyperplane w*0 and b*0 are normalized such that when xw* + b > 0 and
xw*0 + b*0 > 0, we predict class 1.
The new misclassification rate conditioned on class 1 is:
M0 = [	fL(x + tw*T )dx = [	fL(x)dx.
Jxw*0 + b*∕<0	(x-tw*T )w*0 + b*∕<0
(15)
12
Under review as a conference paper at ICLR 2022
The new misclassification rate conditioned on class 2 is:
Mr0 =	f2L (X)dX.
xw*0+b*0>0
The new total misclassification rate is:
M0 = Mt0P(vi ∈ V1) + Mr0 P(vi ∈ V2)
=M0 P(Vi ∈ V1)	+ M0 P(Vi ∈ V2)
=t P(Vi ∈V1)+ P(Vi ∈V2)+ r P(Vi ∈V1) + P(Vi ∈V2)
(16)
(17)
=-P-M0 + ^^ MO.
P+1 t P+1 r
Next, we will prove that if the total misclassification is lowered, Xw* + b* = 0 is not the optimal
hyper-plane before the movements which should achieve the lowest total misclassification rate.
Consider a hyper-plane xw*0 + b*0 - tw*Tw*0 = 0. Given that w*Tw*0 > 0 and t > 0, for ∀x, s.t.
xw*0 + b*0 - tw*2w*0 < 0, we have: (X -tw*T)w*0 + b*0 = xw*0 + b*0 - tw*Tw*0
It means:
tw*T w*0
< 0.
{x∣(x 一 tw*T)w*0 + b*0 < 0} ⊇ {x∣xw*0 + b*0 一 力W :— < 0}.
(18)
Because f1L(x) is a PDF which is a nonnegative function,
(x-tw* T)w* 0+b* 0<0	xw* 0+b* 0 -
Similarly, we can obtain
tw* Tw*0
2
f1L (x)dx.
<0
(19)
Thus,
{x∣xw*0 + b*0 > 0} ⊇ {x∣xw*0 + b*0 —
tw*T W*0	、
-2> 0}.
(20)
Lw*0+b*0>0，(	)	_ Lw*0+b*0-tw*Tw*0
f2L (x)dx.
>0
(21)
—
2
Define
Mf
二P
—P +1
/
xw* 0 +b* 0

tw* T w*0
2
fL(x)dx +---1 j
<0	P + 1 √xw*0 + b*0-tw*T w*0
f2L(x)dx.
>0
(22)
The quantity Mf represents the total misclassification rate before the movements if the decision
*T *0
boundary is xw*0 + b* 一 tw 2W
0. Given Eq. 17, 19 and 21, we have:
M0 ≥ Mf .
(23)
Let M denotes the total classification rate before the movements. Based on the assumption that the
total misclassification rate is lowered after the movements (M > M0), we have:
M > Mf.
(24)
*T *0
Eq. 24 indicates that we find a hyper-plane xw* + b* — tw ? W = 0, which yields smaller total
misclassification rate than xw* + b* = 0 before the movements. This contradicts to the fact that
xw* + b = 0 is the optimal hyper-plane before the movements.
□
A.3 Proof of Theorem 1
Proof. The first layer node representations are given by:
f(1) = -L + X
i	di+ 1	j∈Ni
√di + 1 ∙ ʌ/dj + 1
(25)
13
Under review as a conference paper at ICLR 2022
Without loss of generality, we assume node vi is in the first class V1. Then, we can express the
conditional expectation of fi(1) as:
EA,{yi},{f(0)}∣di,Vi∈Vι (fi(1)|vi ∈ V1,di) = EAIdi,vi∈Vι (E{yi},{fi(0)}|A,VieVi(fi(1)|vi ∈ V1, A))
(26)
Recall that A is the graph adjacency matrix; the first expectation is taken over the random-
ness of ground truth labels and initial input features, and the second expectation is taken over
the randomness of graph structure (A) given the degree of node vi and its label. Denote
ki = E{yi},{f(0)}∣A,Vi∈Vι(KI) and hi = EA,{yi},{f(0)}∣Vi∈Vι,di(N).
E{yi},{f(0)}∣A,Vi∈Vι(fi(i)|vi ∈ V1,A)
=%i},{fi(0)}∣A,Vi∈Vι (f+1 |vi ∈ V1, A) + jXj{yi}W)}∣A,Vi∈Vι (√di∙+lj pj+1 |vi ∈ V1, A,
=dμi + X (%i},{f(0)}∣A,Vi,Vj∈vι (√d-+τfj∙ pd-+1 |A,vi,Vj ∈ V1) ∙P(Vj ∈ VIAvi ∈ V1)
+%i},{f(0)}∣A,Vi∈Vι,Vj∈V2 (√d-+1 j∙ pd-+1 lA,vi ∈ V1,vj ∈ V)，P(Vj ∈ V2|A,vi ∈ V1))
(27)
Let 1{∙} represent the indicator function: it equals 1 if and only if the event in the bracket holds.
E{yi},{f(0)}∣A,vi∈V1 (E 1{vj	∈	VIIA,vi	∈	V1}) = E	P(Vj	∈	VIIA,vi	∈ V1)	=	diki.	(28)
j∈Ni	j∈Ni
Due to our assumptions in § 3, though the labels of nodes are dependent, they share the same
marginal distribution and their joint distribution is a permutation-invariant function. Thus, given
vi ∈ V1(yi = 1), the conditional joint distribution of {ytIt 6= i} is also permutation invariant—
i.e., ∀j, j0 ∈ Ni,P(vj ∈ V1Ivi ∈ V1) = P(vj0 ∈ V1Ivi ∈ V1). Then for ∀j ∈ Ni, we have:
diP(vj ∈ V1 Ivi ∈ V1) = diki and P(vj ∈ V1Ivi ∈ V1) = ki. Due to the independence of A and
{yi},P(vj ∈V1IA,vi ∈V1) =ki.
Efyi},出町lvi∈Vι,A(fi(1)|vi ∈ V1, A) = Ji + ,N (√d^+1kipd-+1μ - √d⅛p⅜+1ρμ
μ + ((I + ρ)ki - ρ)μ X 1
di+ 1	√di+ 1	j⅛ pdj+1
1
di + 1
j∈N√+ ((1+ρ)ki-ρ))μ
(
1
di + 1
∖
(ɪ+
∖di + 1
P	√'i + 1
j∈Ni √dj+1	I
di + 1	((1 + ρ)ki-叫 μ
d di ji rij ((1 + ρ)ki-ρ)) μ.
di + 1	di
(29)
Combining Equation (26) and Equation (29) and given the independence of rij and ki , we have:
EA,{yi},{f(0)}∣di,Vi∈Vι (fi(1)lvi ∈ V1,di)	=	di	+ 1 +	di	+ 1 EAIdi,vi∈Vι	(-j∈d^	)	EAIdi,vi∈Vι ((1 + ρ)ki	- P))	μ
=(1 + ((1+ ρ)hi- PPdiir ) μ ≡ γi1μ.
i	(30)
14
Under review as a conference paper at ICLR 2022
Define e ≡ (1 + ρ)h - ρ, and We consider three cases: (1) hi ≤ ι++ρ,(2) hi > ι++ρ & ri ≤ ɪ, and
⑶ hi > ι+ρ & ri > ɪ.
• CASE 1: hi ≤ ɪ
•	Upper Bound
Wehave:	Yl ≤d⅛ ≤ 2.	(31)
•	Lower Bound
To see if there exists a lower bound, We first show that when hi ≤ τρ~, 1+((1+P)hi-P)diri is a
1+ρ	di +1
decreasing function of di given that di ≥ 1.
When hi ≤ ^++^, we have:
1.	((1 + ρ)hi - ρ) ≤ 0
2.	d⅛ is an increasing non-negative function of d
di+1
3.	ri is an increasing non-negative function of d
4.	τ⅛r is a decreasing function of di
di+1
Thus 1+9+ρ)hijρ)diri is a decreasing function of d.
di+1
When hi =击，1+((I+P*P)际=3⅛ and 0 < 3⅛ ≤ 1.
When hi < l+ρ,
1	+ ((1 + ρ)hi - ρ)diT ≤ ((1 + ρ)hi - ρ)diri + 1
di +1	— di + 1	2
≤ ((I + P)hi - P)ri + 1
-	2	+ 2.
And we know that when hi < ι+ρ:
Jim ((I+ ρ)hi- Pm + 1 = -∞.
ri→∞	2	2
Thus,
1.1	+ ((1 + ρ)hi - ρ)dr
Jim 	--------------= -∞.
ri→∞	di + 1
(32)
(33)
(34)
• CASE 2: hi > 1+ρ & r ≤ 1
If hi > 1+ρ, 0 <e ≤ 1;if r ≤ !, 0 <er- ≤ 1. Given
E(fi1)
1 + ediri、
^Tr
(35)
wehave 0 < di+1 <Yi1 ≤ 1.
• CASE 3: hi > ɪ & 不 > ɪ
In this case, Equation (35) still holds, since hi > 1+ρ.
• Lower Bound
If r > 1, then eri > 1 and therefore Y- > 1.
15
Under review as a conference paper at ICLR 2022
• Upper Bound
When > 0,
1 + edir
di + 1
edir
di + 1
eri
.
2
(36)
Because
eri
JIm --- = ∞,
ri→∞ 2
(37)
we have:
1 + edir
JIm  ------- = ∞.
ri→∞ di + 1
(38)
≥
To sum it up,
γi1 ∈	(0, 1],
I (1, ∞),
(-∞, 2], if hi ≤ l+ρ
if hi > l+p & ri ≤ (1+P)hi-ρ
otherwise.
(39)
Remarks: Special cases
For different types of graphs and different nodes in the graph, di ri might be different. For a d-regular
graph whose nodes have constant degree or a graph whose adjacency matrix is row-normalized, we
will have:
γi1
1 + ((1 + ρ)hi - ρ)dir
di + 1
1	di
≤ (- ----- +———------)=1
一(di + I)	(di + I)
(40)
Equality is achieved if and only if hi = 1. To note that, hi = 1 is not achievable for every node as
long as there are more than one class. Boundary nodes will suffer most.
A.4 Proof of Theorem 2
Proof. After propagating over multiple layers, we use ξil to account for the accumulated scaling
effects (Yi). That is, EA {夕} {乱⑼}”. (⅛11°) = ξμ. Furthermore, conditioned on A and ξil, the
set of nodes contributing positively to node vi is defined as:
NS(A, ξi) ≡ {vj |vj ∈ Ni and (ξi μ) ∙ E{yi},{f.(0) }lξι ,A
EA,{yi},{fi(0)}∣di,ξl,Vj∈Ni (fj")rijldi,ξi,vj ∈Ni) = [
fj(l)T
ξi0μ
∣ξi, A > 0}.
Vj ∈Ns(A,ξi)
-ρiξi0μ	”j ∈NilA,ξ)
□
In the following proof, we use Nis to refer to Nis(A, ξil) for conciseness.
The representations at (l + 1)-th layer:
f (i+i) = fi(l + X
i = di+ 1 j∈Ni
(l)
fi
√di + 1 ∙ ʌ/dj + 1
(41)
16
Under review as a conference paper at ICLR 2022
f(l)	f(l)
EA,{yi},{C (d⅛1 ldi,ξζ) + EA,{yi},{咪}∣di,ξl IXN √di∏' pd"+rldi,ξ43)
dξiμi + d⅛Γ683},{咪Md* (j∑ ^jpd=⅛Tldi,ξi)	(44)
In § 3, we assume that {dj } follow the same distribution and their joint distribution function is
permutation-invariant, and {yj } and {fjl } also have this property. Thus, conditioned on di, ξil and Ni,
the distribution of
j √di+
dj+1
is the same for ∀j ∈ Ni (neighbors are indistinguishable) and we obtain:
f L fjr)√d~+1	,	\
EA,{yi},{fi(0)}∣di,ξi,Ni ( .Σ	pdj+1 Mi(45)
∕f(l)√dn	I	\
diEA,{yi },{fi(0)}∣di Wi ,Ni,Vj∈Ni ( pdj+~1 ldi,ξi, JNiNj ∈Ni}	(46)
of {yj } and
for any Ni ,
Given that the joint distribution of {dj }, the joint distribution
the joint distribution of {fjl } are permutation-invariant functions,
f	f(l)√di+ι ι	八
EA,{yi},{fi(0)}∣di,ξi (X pdj+1 ldi,ξi)	(47)
f L f(l)√d7+T	,	1
EA,{yi},{C,Ni ( X Pdj+J ldi,ξi, N)	(48)
f f(l)√d^+ι	,	\
diEA,{yi},{fi(0)}∣di,ξl,Vj∈Ni ( pd-ψ1 ldi,ξi ,vj ∈Ni	(49)
We note that vj in Equation 49 can be any node except vi due to the equivalence of those nodes.
17
Under review as a conference paper at ICLR 2022
Combining Equation 44 and Equation 49, we can obtain:
ξiμ
di + 1
+
i E
di + 1 A,{yi},{f(0)}ldi,ξi ,vj ∈Ni
f(l)√d~+i
Pdj +1
∣di,ξi,Vj ∈Ni
(51)
dξiμi + di~+i (EA,{yi},{f(0)}∣di,ξi,vj ∈Nis,Vj ∈Ni ( jpd-⅛ι |di, ξi, Vj ∈ NNv ∈ Ni
(52)
f("√di + 1 l ,
P ( vj	∈ Ni	lvj	∈ Ni,	di,	ξi∖ + EA,{yi},{fi(0)}∣di,ξl,Vj∈Nis,Vi∈Ni	[	pdj	+ 1	ldi,	ξi,vj ∈ N ,vj	∈ N
P Vj∈Ns,∣di,ξi,V- ∈Ni
))
ξiμ	l	d diP(Vj	eNs∣di,ξi,Vj ∈Ni)	“0..	di(1	- P(Vj	∈Ns∣di,ξi ,Vj	∈Ni))	IH 0
L + l--------------di+l-----------ξiμ----------------di+l-------------ρiξiμ
ξiμ + di(P(Vj ∈Nsldi,ξi,vj ∈ NO(I + Pi) - Pi)ξi0”
di + 1	di + 1
(54)
(55)
(56)
Define ril ≡ 言.If we know the degree di, ξi, and that a neighbor Vj is in the group Ns, ril
ξi
actually represents the ratio of expected fjlri,j to expected fil. We regard it as the effective related
degree at l-th layer. For the initial layer, if we know the degree di , yi , and that a neighbor Vj is in
the group Ns, the ratio of fjri,j to fi0 is rij. Recall that the related degree ri at the initial layer
is the expected average of rij in the neighborhood. Thus ril is an extension of ri. Moreover, let
hi = P(Vj ∈ Ns |v- ∈ Ni, di, ξi) represent the probability of a neighbor whose representation has
a positive contribution in expectation. This naturally extends the meaning of homophily in deeper
layers. Thus, we regard it as the effective homophily of node Vi at l-th layer. Given the degree di, ξil,
Pli represents the ratio of the probability that a neighbor will have a positive rather than a negative
contribution to Vi . This naturally extends the meaning of P.
We further write Equation 56 as:
l

d + J [1+( (hi(1 + PIi) - PIi)dirl) J μ
and it will have three cases similar to Thm. 1.
□
18
Under review as a conference paper at ICLR 2022
A.5 Proof of Theorem 3
Proof. Similar to Equation 26, we can express the conditional expectation of fi(1) as:
EA,{yi},{fi(0)}∣di,Vi∈V1 f(1)|vi ∈ V1,di)
=Em0lvi∈Vι,di (EA,{yi},{fi(0)}∣di,Vi∈Vι ,m0 f(1)|vi ∈ V1,di,m0))
=Em0lvi∈Vι,di I EA,{yi},{f(0)}∣Vi∈Vι,di,m0
|vi ∈ V1 , di , mi0
+EAKyaM^WiCVi^mO IlN √d=+1 ∙ pd-+1 |Vi ∈ V1,di, m0
μ
di + 1
+Em0lvi∈vι,di I EAIdi,vi∈Vι,m0 ( E I E{yi},{fi(0)}∣A,Vi,Vj∈Vι,m0
j∈Ni
・P(Vj ∈Vι∣m0,Vi ∈Vι, A)
√di + 1 ∙ ʌ/dj + 1
|A,mi0, vj ∈ V1,vi ∈ V1
+E{yi},{fi(0)}|A,vi ∈V1,vj ∈V2,mi0
√ d + 1 ∙ ʌ/dj + 1
|A, m0,vi ∈ Vι,vj ∈ V2) ∙ P(Vj ∈ V2∣m0))))∙
(57)
Next, we will show how to compute the conditional expectation and conditional probability in the
summand.
E{yi},{fi(0)}∣A,Vi,Vj∈Vι,m0 (√dTΓj pd^+Γm0,vi,vj ∈ V1, AJ
=E{yi},{fi(0)}∣A,Vi∈Vι,m0,Vj (√^+fpj+Γ |A,m0,Vi ∈ V1,vj ∈ V1,vj WrOnglysendinfOrmatiOn
• P(Vjwrongly send information|A, m0, Vi ∈ Vι,Vj ∈ Vι)
+E{yi},{fi(0)}|A,viEVi,m0,vj
|A, mi0, vi ∈ V1, vj ∈ V1, vj cOrrectly send infOrmatiOn
• P(Vjcorrectly send information|A, mi0, Vi ∈ V1, Vj ∈ V1).
(58)
Combined with the independence assumption, we have:
P(Vjwrongly send information|A, mi0, Vi ∈ V1, Vj ∈ V1) = mi0.	(59)
Similarly, we obtain:
P(Vj correctly send information|A, mi0, Vi ∈ V1, Vj ∈ V1) = 1 - mi0.	(60)
19
Under review as a conference paper at ICLR 2022
Then Equation (58) can be rewritten as:
E{yi},{fi(0)}A,Vi,Vj∈V1 ,m0 I√di+1j pdj+1|m0, vi, Vv
f(0)
=%"}，{吸}区，“,冲0吗(Tdi+lipdj^|A, mi,vi
fj(0)
+E{yi},{fi(0)}A,Vi∈Vι,m0,Vj (√di+1ipdjz+1|A, mi,vi
=________m0μ	+ (I - mO)”
√di + 1 ∙ ʌ/dv + 1	√di + 1 ∙ ʌ/dv + 1
∈ V1,A
∈ Vι, Vv ∈ Vι, Vvwrongly send information) ∙ m0
∈ Vι, vv ∈ Vι, Vvcorrectly send information ∙ (1 - m0)
(I - 2mO)μ
√di + 1 ∙ Pdv + 1
(61)
Similarly, we have:
E{yi},{fi(0)}|A,vi ∈V1,vj ∈V2,mi0
(62)
Consider the independence between miO and node class & degrees, and insert Equation (61) and
Equation (62) into Equation (57), we will have:
√di+fvpd+ lA,m0,vi ∈ v1,vv ∈ V2) = √d+2mpρ+
EA,{yi},{f(0)}∣di,Vi∈Vι (fF)|vi ∈ V1,di)
μ + E	E	X ( (1 - 2m0)kiμ + (1 - 2m0)(1	- ki)ρμ
di + 1 + Em0lvi∈v1,di	EAIdi,vi∈v1,m0	IlNJ√di+1 ∙ pd+ + √di+1	∙	pd~+1
μ + E	E	X (I - 2mO)(P + (I - p)ki)μ ∖
di+1 + Em01VilV1,di	EAIdi,vi∈v1,m0	[v∈N -√d"+1 ∙ pd~+1-))
μ q E	(1 - 2m0)(ρ +(1 - ρ)hi)di ri
di+1+ Em0lvi∈v1,di ( 1	d-+1
1 + (1 - 2e0)(ρ + (1 - P)hi)diri
di + 1
μ ≡ Yi1E(fi(0))∙
(63)
When hi ≤ 1, P+ (1 - P)hi = (1 - hi)P+ hi > 0∙ Define 0 ≡ (1 - 2eiO)(P+ (1 - P)hi). To obtain
the ranges of γi1, when eiO ≥ 0∙5, 0 ≤ 0, it resembles the derivations of CASE 1 in Proof A.3; when
eO < 0.5, 0 < eri ≤ 1, it resembles the derivations of CASE 2; when eO < 0.5, eri > 1, it resembles
the derivations of CASE 3.	□
B	Additional Experiments
B.1	Ablation study
Table B.1: Ablation study: degree correction has consistent benefits (robust to oversmoothing & ability to handle
heterophily) in different datasets while signed information has more benefits in heterophilous datasets. Best
performance of each model is highlighted in gray.
Layers	2	4	8	16	32	64	2	4	8	16	32	64
			Cora (h=0.81)						Citeseer (h=0.74)			
Base	86.56±ι.2i	86.04±0.72	85.51±1.51	85.33±o.72	85.37±1.58	72.17±8.89	76.51±i.63	75.03±1.67	73.96±1.52	73.59±ι.5i	71.91±1.94	32.08±15.74
+deg	86.72±i.29	86.02±o.97	85.49±i.32	85.27±1.59	85.27±1.51	84.21±1.22	76.63±i.38	74.64±1.97	74.15±1.61	73.73±1.31	73.61±1.84	70.56±2.27
+sign	84.81±i.63	86.06±i.7	85.67±i.26	85.39±0.97	84.85±0.98	78.57±6.73	77.13±i.69	74.56±2.02	73.64±1.65	72.31±2.32	71.98±3.44	68.68±6.72
+deg,sign	86.96±i.38	86.20±0.89	85.63±o.78	85.47±1.18	85.55±1.66	77.81±7.95	76.81±i.71	74.68±1.97	74.69±2.35	73.28±1.45	71.81±2.28	69.91±3.97
			Cornell (h=0.3)						Chameleon (h=0.23)			
Base	61.89±3.72	60.00±5.24	58.92±5.24	56.49±5.73	58.92±3.15	49.19±16.70	64.98±i.84	62.65±3.09	62.43±3.28	54.69±2.58	47.68±2.63	29.74±5.21
+deg	63.78±5.57	62.70±5.90	59.46±4.52	56.49±5.73	57.57±4.20	58.92±3.15	66.54±2.i9	68.31±2.70	68.99±2.38	67.68±3.70	56.86±8.80	41.95±9.56
+sign	85.41±7.27	76.76±7.07	70.00±5.19	67.57±9.44	63.24±6.07	63.24±6.53	65.31±3.20	53.55±6.35	53.05±2.28	51.93±4.0o	57.17±3.39	51.93±8.95
+deg,sign	84.32±6.37	78.92±8.09	73.51±5.90	70.81±5.64	68.11±5.14	62.43±6.67	65.75±ι.8i	61.49±7.38	53.73±7.79	52.43±5.37	55.92±5.14	56.95±3.93
20
Under review as a conference paper at ICLR 2022
We now study the impact of two proposed mechanisms (signed messages and degree correction, § 4).
To better show their effects, we add each design choice to a base model and track the changes in
the node classification performance. As the base model, we choose a GCN variant that uses the
message passing mechanism (Kipf & Welling, 2016) with weight bias, a residual connection (but
not decaying aggregation) for robustness to oversmoothing, and Elu non-linearity σ. We denote the
model that replaces the message passing with our signed messages mechanism as +sign, and the
model that incorporates the degree correction as +deg. The model that uses both designs is denoted
as +sign,deg. Table B.1 gives the accuracy of the models in the supervised node classification
task for different layers.
We observe that both mechanisms alleviate the oversmoothing problem. Specifically, the base model
has a sharp performance decrease after 32 layers, while the other models have significantly higher
performance. In general, the +deg model is better than +sign in alleviating oversmoothing, and has
a consistent performance gain across different data. For Chameleon, we observe increase in accuracy
as we stack more layers; the large performance gain of GGCN results from the degree correction. In
Cora and Cornell, +sign,deg consistently performs better across different layers than the model
with only one design, demonstrating the constructive effect from our two proposed mechanisms.
The signed message design has an advantage in heterophilous datasets. In the Cornell dataset, using
signed information rather than plain message passing results in over 20% gain, which explains the
strong performance of GGCN. However, in homophilous datasets, the benefit from signed messages
is limited, as these datasets have few different-class neighbors. This is because when the effective
homoPhily lhi is high and error rate e0 is low, lim和→ι e0→l3 丁 2(0+；%-：)b∙) = 1, So using signed
messages is less beneficial in homophilous datasets.
B.2	Batch norm & Layer norm
In § 4, we use decaying aggregation instead of other normalizing mechanisms. Other mechanisms,
such as batch or layer norm, may be seen as solutions to the heteroPhily and oversmoothing Problems.
However, batch norm cannot comPensate for the disPersion of the mean vectors (§ 3) due to different
degrees and homoPhily levels of the nodes. Although, to some extent, it reduces the sPeed by
which the rePresentations of the suscePtible nodes (case 1 & 2) move towards the other class (good
for oversmoothing), it also Prevents the rePresentations of the nodes that could benefit from the
ProPagation (case 3) from increasing the distances (droP in accuracy). Layer norm is better at
overcoming the disPersion effect but may lead to a significant accuracy droP in some datsets when a
subset of features are more imPortant than the others. Thus, we do not use any of these normalizations.
Next, we Provide exPeriments to show the effects of batch norm and layer norm. We use the following
base model (the same model used in §B.1): a GCN KiPf & Welling (2016) with weight bias, Elu
non-linearity and residual connection. We do not include any of our designs so as to exclude any
other factors that can affect the Performance. The models we comPare against are +BN and +LN,
which rePresent the models that add batch norm and layer norm right before the non-linear activation,
resPectively.
Table B.2: Effects of using batch norm & layer norm: decrease in accuracy but imProvement in oversmoothing.
Best Performance of each model across different layers is highlighted in gray.
Layers	2	4	8	16	32	64	2	4	8	16	32	64
			Cora (h=0.81)					Citeseer (h=0.74)		
Base	86.56±ι.21	86.04±o.72	85.51±ι.5i 85.33±o.72	85.37±1.58	72.17±8.89	76.51±i.63	75.03±i.6,	73.96±i.5 2 73.59±ι.5i	71.91±i.94	32.08±15.74
+BN	84.73±ι.ιo	83.76±ι.6i	83.94±ι.5i 84.57±i.22	84.63±1.58	85.17±ι.i8	71.62±i.48	71.58±ι.00	72.18±1.39 72.45±1.42	72.76±ι.3i	72.61±1.41
+LN	84.73±1.63	86.60±1.01	86.72±ι.36 8 6.08±ι.16	85.67±1.23	85.13±i.20	76.11±i.80	74.02±2.77	75.00±1.95 74.50±0.96	74.49±2.io	73.94±2.03
			Cornen(h=0.3)					Chameleon (h=0.23)		
Base	61.89±3.72	60.0O±5.24	58.92±5.24 56.49±5.73	58.92±3.15	49.19±16.70	64.98±i.84	62.65±3.09	62.43±3.28 54.69±2.58	47.68±2.63	29.74±5.21
+BN	58.38±6.42	59.19±4.59	55.41±6.65 5 7.30±3.15	57.57±6.29	57.02±6.19	60.88±2.24	61.38±2.17	61.84±4.08 J 61.97±3.oi	59.04±3.79	57.84±3.67
+LN	58.11±6.19	55.68±6.19	58.92±7.63 ∣59.19±3.07	58.92±3.15	58.00±3.03	61.86±1.73	62.17±2.48	62.41±2.99 60.37±2.36	58.25±3.03	58.92±3.15
Table B.2 shows that both batch norm and layer norm can helP with oversmoothing. Moreover,
adding layer norm is in general better than adding batch norm. This is exPected because the scaling
effect caused by the ProPagation can be alleviated by normalizing across the node rePresentations.
Thus, the disPersion of the exPected rePresentations can be mitigated. On the other hand, batch norm
normalizes across all the nodes, so it requires sacrificing the nodes that benefit (case 3) to comPensate
21
Under review as a conference paper at ICLR 2022
for the nodes that are prone to moving towards the other classes (case 1 & case 2). As a result, batch
norm is less effective in mitigating oversmoothing and leads to a bigger decrease in accuracy.
Another finding is that both layer norm and batch norm lead to a significant accuracy decrease
(2%-3%) in the heterophilous datasets. +BN has a clear accuracy drop even in the homophilous
datasets. As Thm. 1 points out: higher heterophily level may result in sign flip. If the representations
flip the sign, using batch norm or layer norm will not revert the sign, but they may instead encourage
the representations to move towards the other class more.
Given the limitations shown above, we do not use either batch norm or layer norm in our proposed
model, GGCN.
B.3	More on the Initial & Developing Stages
Section 5.4 shows how the node classification accuracy changes for nodes of different degrees with
the number of layers on Citeseer. Here, we provide more details of this experiment and give the
results on another dataset, Cora.
Datasets. According to Thm. 1 and 2, in order to see both the initial and the developing stage, we
need to use homophilous datasets. In heterophilous datasets, most nodes satisfy case 1, so the initial
stage does not exist.
Measurement of effective homophily bhli. To estimate the effective homophily bhli , we measure
the portion of neighbors that have the same ground truth label as vi and are correctly classified.
Following our theory, we estimate bhli before the last propagation takes place and analyze its impact
on the accuracy of the final layer. In more detail, we obtain the node representations before the
last propagation from a trained vanilla GCN Kipf & Welling (2016) and then perform a linear
transformation using the weight matrix and bias vector from the last layer. Then, we use the
transformed representations to classify the neighbors of node vi and compute bhli . We note that we
leverage the intermediate representations only for the estimation of bhli ; the accuracy of the final layer
is still measured using the outputs from the final layer.
Degree intervals. To investigate the change in GCN accuracy with different layers for nodes with different degrees, we categorize the nodes in n degree	Table B.3: Citeseer: Accuracy (Acc) and average effective ho- mophily (bhli) for nodes with different degrees across various layers. Last layer of initial stage marked in gray.						
intervals. For the degree intervals, we use logarithmic binning (base 2). In detail, we denote the highest and lowest degree by dmax and dmin , respectively, and let Ω ≡ log2 dmax-log2 dmin. Then, n,					Degrees		
	Layers		[1, 2]	Wr	[7, 15]	[16, 39]	[40, 99]
	2	Acc bhli	74.44 0.65	78.51 0.67	84.04 0.72	96.00 0.83	100.00 0.91
we divide the nodes into n intervals, where the j-th interval is defined as: [dmin∙ 2j-1.dmin∙ 2j。). Dataset: Citeseer. Figure 1 and Ta- ble B.3 show how the accuracy changes with the number of layers for differ- ent node degree intervals. We observe that in the initial stage, the accuracy in- creases as the degree and bhli increase. However, in the developing stage, the	3	Acc bhli	70.92 0.63	77.59 0.66	83.03 0.71	92.75 0.84	100.00 0.92
	4	Acc	60.54	68.56	77.63	94.00	100.00
		bhli	0.54	0.58	0.66	0.79	0.84
	5	Acc bhli	41.66 0.36	48.70 0.38	56.97 0.45	61.33 0.48	11.11 0.11
	6	Acc bhli	23.61 0.18	28.87 0.19	41.17 0.27	31.83 0.22	0.00 0.00
accuracy of high-degree nodes drops
more sharply than that of low-degree
nodes.
Dataset: Cora. The results for Cora are shown in Figure B.1 and Table B.4. In the initial stage, the
nodes with lower
22
Under review as a conference paper at ICLR 2022
Figure B.1: Cora: Accuracy per (logarithmic) degree bin.
Table B.4: Cora: Accuracy and average effective
一
l
homophily (hli) for nodes with different degrees
across different layers. Last layer of initial stage
marked in gray.
Layers		Degrees				
		[1, 2]	∏3πT	18；W	[22, 60]	[61, 168]
2	Acc 一	85.14	88.52	83.78	90.00	100.00
	bhSli	0.77	0.77	0.72	0.63	0.81
3	Acc	82.28	87.26	84.42	85.00	100.00
	bhSli	0.77	0.78	0.73	0.64	0.84
4	Acc	79.57	85.52	83.26	85.00	100.00
	bhSli	0.75	0.76	0.71	0.63	0.80
5	Acc 一	77.38	83.75	82.83	85.00	96.19
	bhSli	0.72	0.73	0.68	0.56	0.83
6	Acc	63.35	68.20	66.01	70.00	78.57
	bhSli	0.57	0.60	0.54	0.52	0.67
7	Acc	30.91	30.62	29.12	17.50	21.43
	bhSli	0.25	0.26	0.21	0.26	0.23
8	Acc 一	31.48	31.02	28.44	32.50	25.24
	bhSli	0.24	0.26	0.20	0.28	0.24
degrees usually have lower accuracy. One exception is the nodes with degrees in the range [3, 7].
S
These nodes have higher accuracy because the average effective homophily bhli of that degree group is
the second highest. In the developing stage, the accuracy of the high-degree nodes drops more than
the accuracy of the remaining node groups.
GCN’s behavior on both Citesser and Cora datasets verifies our conjecture based on our theorems in
§ 3.3.
B.4 Homophily Metric
In Table 1, we provide the graph homophily level h, which is defined as the average of the nodes’
homophily levels. In our theoretical analysis in § 3, we use a node’s homophily level and relative
degree to characterize three cases (Fig. 2 and Thm. 1). To better align our empirical results with our
theoretical analysis, we provide below the proportion of nodes in each case.
Table B.5: The percentage (%) of nodes in each case (Fig. 2 and Thm. 1): case 1: hi ≤ ι+ρρ, case 2:
hi > P- & r ≤ L I 1.--------and case 3: otherwise. The dominant case for each dataset is colored in grey.
i 1+ρ i (1+ρ)hi -ρ
	Texas	Wisconsin	Actor	Squirrel	Chameleon	Cornell	Citeseer	Pubmed	Cora
Hom. level h	0.11	0.21	0.22	0.22	0.23	0.3	0.74	0.8	0.81
#Nodes	183	251	7,600	5,201	2,277	183	3,327	19,717	2,708
#Edges	295	466	26,752	198,493	31,421	280	4,676	44,327	5,278
#Classes	5	5	5	5	5	5	7	3	6
Case 1	87.43	78.49	63.49	47.74	48.48	79.23	18.33	14.80	6.50
Case 2	8.74	15.93	29.53	50.14	48.84	6.01	25.58	33.75	32.98
Case 3	3.83	5.58	6.99	2.11	2.68	14.75	56.09	51.45	60.52
We observe that a low graph homophily level indicates that case 1 is the dominant case, while a high
graph homophily level indicates that case 3 is the dominant case. As we stated in Thm 1, under
"non-swapping" condition, case 3 is the only case to have a benefit. Thus, in the commonly-used
homophilous datasets (Citeseer, Pubmed and Cora), case 3 dominates and enjoys the benefits from
graph convolution. In the Texas, Wisconsin and Actor datasets, only case 1 dominates and case 1
hurts the performance most, this explains why GCN performs worse than most methods in Table 1. In
the Squirrel and Chameleon datasets, both case 1 and case 2 dominate, GCN yields acceptable results.
Using portions of the three cases to analyze the datasets is a better metric, because it aligns more
with the GCN’s performance. As shown by Table B.5, Wisconsin, Actor, Squirrel and Chameleon
have very close graph level homophily, but GCN’s performance are quite different as shown in Table
1. Our way to define homophily level does predict that GCN should perform decently in Squirrel and
Chameleon datasets.
23
Under review as a conference paper at ICLR 2022
B.5 Complexity analysis
We first analyze the time complexity of the forward path GGCN. For degree correction, we need to
compute the rij . However, we do not need to compute all of them, and we only need to compute
rij for node pairs who are linked. The time complexity is O(|E |), but it is a one-time computation,
the results of which can be saved. We compute τilj based on the learned weights λl0, λl1. The time
complexity is O(∣E∣∙ L), where L is the number of layers. The time complexity to compute the signed
function is O(∣E∣∙ H ∙ L), where H is the hidden dimension of representations, and We only compute
the cosine similarity between the nodes that are linked. We also need to compute the multiplication
of the propagation matrix and representation matrices. The time complexity is O(∣V∣2 ∙ H ∙ L),.
Similarly, the time complexity of the multiplication of representation matrices and the weight matrices
is O(∣V∣∙ H2 ∙ L). The total time complexity would be O(max(∣V∣2, |E|) ∙ H ∙ L). The complexity of
GGCN resembles the complexity of attention-based model. Thus we compare with GAT (Velickovic
et al., 2017) the total time to train and test on 4 larger homophilous and heterophilous datasets. We
run the training and testing for 10 times.
Table B.6: Training and test time (m:minutes, s:seconds) for 10 runs. Shorter time is colored in grey.
	Actor	Chameleon	Citeseer	Cora
#Nodes	7,600	2,277	3,327	2,708
#Edges	26,752	31,421	4,676	5,278
GGCN	7m24s	2m29s	2m9s	5m47s
GAT	8m14s	7m34s	9m39s	4m11s
In general, GGCN runs faster than GAT because degree correction and signed messages in GGCN
learn fewer parameters.
C Hyperparameters and Parameters
C.1 Hyperparameter settings
Experiments for Table 1 & Table 2
For the baselines, we set the same hyperparameters that are provided by the original papers or the
authors’ github repositories, and we match the results they reported in their respective papers. In our
experiments, we find that the original hyperparameters set by the authors are already well-tuned.
All the models use Adam as the optimizer. GAT sets the initial learning rate as 0.005 and Geom-GCN
uses a custom learning scheduler. All the other models (include GGCN) use the initial learning rate
0.01.
For GGCN, we use the following hyperparameters:
•	k in the decaying aggregation: 3
We tune the parameters in the following ranges:
•	Dropout rate: [0.0, 0.7]
•	Weight decay: [1e-7, 1e-2]
•	Hidden units: {8, 16, 32, 64, 80}
•	Decay rate η: [0.0, 1.5]
Experiments for Table B.1 and Table B.2
The hyperparameters that are used in all the models (Base, +deg, +sign, +deg,sign, +BN, +LN)
are set to be the same and they are tuned for every dataset. Those common hyperparameters are:
•	Dropout rate: [0.0, 0.7]
•	Weight decay: [1e-7, 1e-2]
•	Hidden units: {8, 16, 32, 64, 80}
24
Under review as a conference paper at ICLR 2022
C.2 Initialization of parameters
Initialization
For GGCN, we adopt the following parameter initialization in the experiments for Table 1 & Table 2
•	Initialization of λ* l0 and λl1: 0.5 and 0, respectively
•	Initialization of αl, β0l , β1l and β2l : 2, 0, 0, 0, respectively.
We initialize β{0,1,2} = 0 in Eq. (6) because, after applying Softmax, β{0,1,2} = 1/3; this ensures equal
contributions from positive and negative neighbors and themselves (and Sum=1). We initialize λ1 = 0
in Eq. (9) following the common practice for initializing the bias. We set αl = 2 and λl0 = 0.5 in
Eq. (6) and Eq. (9), because when rij → +∞, the degree correction (including global scaling) is:
αlTij = softplus(2) ∙ softplus(0.5 ∙ (0 — 1) + 0) ≈ 1. AS We mention in Thm. 1, when the homoPhily
level is high, nodes with large ri may benefit (case 3), thus We do not want to compensate for these
nodes and Would like to keep αl Tij CloSe to 1.
C.3 Parameters after training
Figure C.1 shows the original and corrected rij
l
(avg rij). Corrected rij are given by αlTilj rij ,
which are a combination of global scaling and
local degree correction. Because β{l 0,1,2} sum to
1 and do not change global scaling, they are not
considered in the corrected rij. As can be seen
in Fig. C.1, after the training, GGCN learns to
increase rj, which satisfies our theorems.
Figure C.1: Original rij (avg. rij) and corrected (pre-
and post-training).
D Table 2 with larger fonts
25
Table D.l: Model performance for different layers: mean accuracy ± stdev over different data splits. Per dataset and GNN model, we also report the layer at which the best performance
(given in Table 1) is achieved. ςOOM,: out of memory; ςINS,: numerical instability.
	Layers	2	4	8	16	32	64	Best	2	4	8	16	32	64	Best
				Cora S=O.81)							Citeseer (h=0.74)				
	GGCN (ours)	87.00±ι.i5	87.48±i.32	87.63±i.33	87.51±ι.i9	87.95±i.05	87.28±ι.4i		76.83±i.82	76.77±i.48	76.91±i.56	76.88±i.56	76.97±i.52	76.65±i.38	^W
	GPRGNN	87.93±ι.ιι	87.95±i.18	87.87±ι.4i	87.26±ι.5i	87.18±i.29	87.32±ι.2i	4	77.13±i.67	77.05±i.43	77.09±i.62	76.00±i.64	74.97±i.47	74.41±i.65	2
	H2GCN*	87.87±i.2o	86.10±ι.5i	86.18±2.io	OOM	OOM	OOM	2	76.90±i.8o	76.09±i.54	74.10±i.83	OOM	OOM	OOM	1
	GCNII*	85.35±i.56	85.35±i.48	86.38±o.98	87.12±ι.n	87.95±i.23	88.37±i.25	64	75.42±i.78	75.29±i.9o	76.00±i.66	76.96±i.38	77.33±i.48	77.18±i.47	32
	PairNorm	85.79±ι.oι	85.07±o.9i	84.65±i.09	82.21±2.84	60.32±8.28	44.39±5.6o	2	73.59±i.47	72.62±i.97	72.32±i.58	59.71±15.97	27.21±io.95	23.82±6.64	2
	Geom-GCN*	85.35±i.57	21.01±2.61	13.98±i.48	13.98±i.48	13.98±i.48	13.98±i.48	2	78.02±i.15	23.01±i.95	7.23±o.87	7.23±o.87	7.23±o.87	7.23±o.87	2
	GCN	86.98±i.27	83.24±i.56	31.03±3.o8	31.05±2.36	30.76±3.43	31.89±2.o8	2	76.50±i.36	64.33±8.27	24.18±i.ti	23.07±2.95	25.3±i.77	24.73±i.66	2
	GAT	87.30±ι.ιo	86.50±i.2o	84.97±i.24	INS	INS	INS	2	76.55±i.23	75.33±i.39	66.57±5.o8	INS	INS	INS	2
UnderreVieW as a ConferenCe PaPersICLR 2022
Cornell (h=0.3)	Chameleon (h=0.23)
GGCN (ours)	83.78±6.73	83.78±6.i6	84.86±5.69	83.78±6.73	83.78±6.5i	84.32±5.9o	6	70.77±i.42	69.58±2.68	70.33±i.to	70.44±i.82	70.29±i.62	70.20±i.95	5
GPRGNN	76.76±8.22	77.57±7.46	80.27±8.ii	78.38±6.04	74.59±7.66	70.00±5.73	8	46.58±ι.77i	45.72±3.45	41.16±5.79	39.58±t.85	35.42±8.52	36.38±2.4o	2
H2GCN*	81.89±5.98	82.70±6.27	80.27±6.63	OOM	OOM	OOM	1	59.06±i.85	60.11±2.15	OOM	OOM	OOM	OOM	4
GCNII*	67.57±ii.34	64.59±9.63	73.24±5.9i	77.84±3.97	75.41±5.47	73.78±4.37	16	61.07±4.io	63.86±3.o4	62.89±ι.iδ	60.20±2.io	56.97±ι.8i	55.99±2.27	4
PairNorm	50.27±7.it	53.51±8.oo	58.38±5.oi	58.38±3.oi	58.92±3.i5	58.92±3.i5	32	62.74±2.82	59.01±2.8o	54.12±2.24	46.38±2.23	46.78±2.26	46.27±3.24	2
Geom-GCN*	60.54±3.67	23.78±ii.64	12.97±2.9i	12.97±2.9i	12.97±2.9i	12.97±2.9i	2	60.00±2.81	19.17±i.66	19.58±i.73	19.58±i.73	19.58±i.73	19.58±i.73	2
GCN	60.54±5.3o	59.19±3.3o	58.92±3.i5	58.92±3.i5	58.92±3.i5	58.92±3.i5	2	64.82±2.24	53.11±4.44	35.15±3.i4	35.39±3.23	35.20±3.25	35.50±3.o8	2
GAT	61.89+5.05	58.38+4.05	58.38+3.86	INS	INS	INS	2	60.2642.50	48.71 4 2.96	35.09 + 3.55	INS	INS	INS	2