Under review as a conference paper at ICLR 2022
Non-Parametric Neuro-Adaptive Control
Anonymous authors
Paper under double-blind review
Ab stract
We develop a learning-based algorithm for the control of autonomous systems
governed by unknown, nonlinear dynamics to satisfy user-specified tasks expressed
via time-varying reference trajectories. Most existing algorithms either assume
certain parametric forms for the unknown dynamic terms or resort to unnecessarily
large control inputs in order to provide theoretical guarantees. The proposed algo-
rithm addresses these drawbacks by integrating neural-network-based learning with
adaptive control. More specifically, the algorithm learns a controller, represented as
a neural network, using training data that correspond to a collection of system pa-
rameters and tasks. These parameters and tasks are derived by varying the nominal
parameters and the reference trajectories, respectively. It then incorporates this neu-
ral network into an online closed-form adaptive control policy in such a way that the
resulting behavior satisfies the user-defined task. The proposed algorithm does not
use any a priori information on the unknown dynamic terms or any approximation
schemes. We provide formal theoretical guarantees on the satisfaction of the task.
Numerical experiments on a robotic manipulator and a unicycle robot demonstrate
that the proposed algorithm guarantees the satisfaction of 50 user-defined tasks,
and outperforms control policies that do not employ online adaptation or the neural-
network controller. Finally, we show that the proposed algorithm achieves greater
performance than standard reinforcement-learning algorithms in the pendulum
benchmarking environment.
1	Introduction
Learning and control of autonomous systems with uncertain dynamics is a critical and challenging
topic that has been widely studied during the last decades. One can identify plenty of motivating
reasons, ranging from uncertain geometrical or dynamical parameters and unknown exogenous
disturbances to abrupt faults that significantly modify the dynamics. There has been, therefore, an
increasing need for developing control algorithms that do not rely on the underlying system dynamics.
At the same time, such algorithms can be easily implemented on different, heterogeneous systems,
since one does not need to be occupied with the tedious computation of the dynamic terms.
A promising step towards the control of systems with uncertain dynamics is the use of data obtained
a priori from system runs. However, engineering systems often undergo purposeful modifications
(e.g., substitution of a motor or link in a robotic arm or exposure to new working environments) or
suffer gradual faults (e.g., mechanical degradation), which might change the systems’ dynamics or
operating conditions. Therefore, one cannot rely on the aforementioned data to provably guarantee the
successful control of the system. On the other hand, the exact incorporation of these changes in the
dynamic model, and consequently, the design of new model-based algorithms, can be a challenging
and often impossible procedure. Hence, the goal in such cases is to exploit the data obtained a
priori and construct intelligent online policies that achieve a user-defined task while adapting to the
aforementioned changes.
There has been a large variety of works that tackle the problem of control of autonomous systems
with uncertain dynamics, exhibiting, however, certain limitations. The existing algorithms are based
on adaptive and learning-based approaches or the so-called funnel control (Krstic et al. (1995);
Kanellakopoulos et al. (1991); Xu & Ioannou (2003); Ge & Wang (2004); Wen et al. (2011); Chen
et al. (2011); Slotine & Li (1987); Slotine & Coetsee (1986); Chen et al. (2008); Xian et al. (2004);
Vamvoudakis & Lewis (2010); Berger et al. (2018); Bechlioulis & Rovithakis (2014); Joshi et al.
(2020); Capotondi et al. (2020); Bertsekas & Tsitsiklis (1996); Sutton & Barto (2018)). Nevertheless,
1
Under review as a conference paper at ICLR 2022
adaptive control methodologies are restricted to system dynamics that can be linearly parameterized
with respect to certain unknown parameters (e.g., masses, moments of inertia), assuming the system
structure perfectly known; funnel controllers employ reciprocal terms that drive the control input to
infinity when the tracking error approaches a pre-specified funnel function, creating thus unnecessarily
large control inputs that might damage the system actuators. Data-based learning approaches either
consider some system characteristic known (e.g., a nominal model, Lipschitz constants, or global
bounds), or use neural networks to learn a tracking controller or the system dynamics; the correctness
of such methods, however, relies on strong assumptions on the parametric approximation by the neural
network and knowledge of the underlying radial basis functions. Finally, standard reinforcement-
learning techniques (Bertsekas & Tsitsiklis (1996); Sutton & Barto (2018)) usually assume certain
state and/or time discretizations of the system and rely on exhaustive search of the state space, which
might lead to undesirable transient properties (e.g., collision with obstacles while learning).
1.1	Contributions and Significance
This paper addresses the control of systems with continuous, unknown nonlinear dynamics subject to
tasks expressed via time-varying reference trajectories. Our main contribution lies in the development
of a learning-based control algorithm that guarantees the accomplishment of a given task using only
mild assumptions on the system dynamics. The algorithm draws a novel connection between adaptive
control and learning with neural network representations, and consists of the following steps. Firstly,
it trains a neural network that aims to learn a controller that accomplishes a given task from data
obtained off-line. Secondly, we develop an online adaptive feedback control policy that uses the
trained network to guarantee convergence to the given reference trajectory and hence satisfaction of
the task. Essentially, our approach builds on a combination of off-line trained controllers and on-line
adaptations, which was recently shown to significantly enhance performance with respect to single
use of the off-line part (Bertsekas (2021)).
The major significance of our contribution is twofold. Firstly, we guarantee the theoretical correctness
of the proposed algorithm by considering only mild conditions on the neural network, removing the
long-standing assumptions on parametric approximations and boundedness of the estimation error.
Secondly, we demonstrate via the experimental results the generality of the algorithm with respect
to different tasks and system parameters. That is, the training data that we generate for the training
of the neural network in the first step correspond to tasks that are different from the given one to be
executed1. Additionally, we employ systems with different dynamic parameters to generate these data.
We evaluate the proposed algorithm in numerous scenarios comprising different variations of the
given task and different system dynamic parameters, which do not necessarily match the training data.
We show that the algorithm, owing to its adaptation properties, is able to guarantee the satisfaction of
the respective tasks in all the aforementioned scenarios by using the same neural network.
1.2	Related work
A large variety of previous works considers neuro-adaptive control with stability guarantees, focusing
on the optimal control problem (Vamvoudakis & Lewis (2010); Yang et al. (2020); Cheng et al.
(2007); Fan et al. (2018); Kiumarsi et al. (2017); Zhao & Gan (2020); Vrabie & Lewis (2009); Sun
& Vamvoudakis (2020); Kamalapurkar et al. (2015); Huang et al. (2018b); Mo et al. (2019); Joshi
et al. (2020)). Nevertheless, the related works draw motivation from the neural network density
property (see, e.g., (Cybenko (1989)))2 and assume sufficiently small approximation errors and linear
parameterizations of the unknown terms of the form W (x)θ (dynamics, optimal controllers, or value
functions), with W and θ known and unknown, respectively. Similarly, more traditional adaptive
control methodologies that handle uncertain nonlinear systems assume either linear parameterizations
of the unknown dynamic terms (Krstic et al. (1995); Hong et al. (2009); Chen (2019); Huang et al.
(2018a); Kanellakopoulos et al. (1991); Xu & Ioannou (2003); Ge & Wang (2004); Wen et al. (2011);
Chen et al. (2011); Slotine & Li (1987); Slotine & Coetsee (1986); Chen et al. (2008)), use known
upper-bound functions (Slotine & Coetsee (1986)), or provide local stability results dictated by the
dynamic bounds (Xian et al. (2004); Chen et al. (2008)). This paper relaxes the aforementioned
assumptions and proposes a non-parametric neuro-adaptive controller, whose stability guarantees
1The task difference is illustrated in Section 4.
2A sufficiently large neural network can approximate a continuous function arbitrarily well in a compact set.
2
Under review as a conference paper at ICLR 2022
rely on a mild boundedness condition of the closed-loop system state that is driven by the learned
controller. The proposed approach exhibits similarities with (Liu et al. (1994)), which employs
off-line-trained neural networks with online feedback control, but fails to provide convergence
guarantees. Similarly, Zeng et al. (2020) develops an algorithm that combines a learning module
and a physics-based controller for robot throwing of arbitrary objects, assuming however access to a
physics simulator and not providing theoretical guarantees.
Other learning-based related works include modeling with Gaussian processes (Capotondi et al.
(2020); Leahy et al. (2019); Jain et al. (2018); Berkenkamp & Schoellig (2015)), or use neural
networks (Ma et al. (2020); Shah et al. (2018); Yan & Julius (2021); Liu et al. (2021); Hahn et al.
(2020); Cai et al. (2021); Wang et al. (2020); Camacho & McIlraith (2019); Hahn et al. (2020);
Riegel et al. (2020); Hu et al. (2020); Ivanov et al. (2019)) to accomplish reachability, verification or
temporal logic specifications. Nevertheless, the aforementioned works either use partial information
on the underlying system dynamics, or do not consider them at all. In addition, works based on
Gaussian processes usually propagate the dynamic uncertainties, possibly leading to conservative
results. Similarly, data-driven model-predictive control techniques (Nubert et al. (2020); Maddalena
et al. (2020)) use data to over-approximate additive disturbances or are restricted to linear systems.
Control of unknown nonlinear continuous-time systems has been also tackled in the literature by
using funnel control, without necessarily using off-line data or dynamic approximations (Berger et al.
(2018); Bechlioulis & Rovithakis (2014); Lindemann et al. (2017); Verginis & Dimarogonas (2018);
Verginis et al. (2021)). Nevertheless, funnel controllers usually depend on reciprocal time-varying
barrier functions that drive the control input to infinity when the error approaches a pre-specified
funnel, creating thus unnecessarily large control inputs that might damage the system actuators.
2	Problem Formulation
Consider a continuous-time dynamical system governed by the 2nd-order continuous-time dynamics
X = f (x,t)+ g(x, t)u(x, t),	(1)
where x := [χ>,X>]> ∈ R2n, n ∈ N, is the system state, assumed available for measurement,and
U : R2n X [0, ∞) → Rn is the time-varying feedback-control input. The terms f (∙) and g(∙)
are nonlinear vector fields that are locally Lipschitz in X over R2n for each fixed t ≥ 0, and
uniformly bounded in t over [0, ∞) for each fixed X ∈ R2n. The dynamics (1) comprise a large
class of nonlinear dynamical systems (Zhong & Leonard (2020); Yu et al. (1996); Doya (1997)) that
capture contemporary engineering problems in mechanical, electromechanical and power electronics
applications, such as rigid/flexible robots, induction motors and DC-to-DC converters, to name a few.
The continuity in time and state provides a direct link to the actual underlying system, and we further
do not require any time or state discretizations.
We consider that f (∙) and g(∙) are completely unknown; We do not assume any knowledge of the
structure, Lipschitz constants, or bounds, and we do not use any scheme to approximate them. Note
also that we do not assume global Lipschitz continuity or global boundedness of f (∙, t) and g(∙,t) or
the solution x(t) of (1). Nevertheless, we do assume that g(x, t) is positive definite:
Assumption 1. The matrix g(x, t) is positive definite, for all (X, t) ∈ R2n × [0, ∞).
Such assumption is a sufficiently controllability condition for (1); intuitively, it states that the
multiplier of u (the input matrix) does not change the direction imposed to the system by the
underlying control algorithm. Systems not covered by (1) or Assumption 1 consist of underactuated
or non-holonomic systems, such as unicycle robots or underactuated aerial vehicles. Nevertheless, we
provide an extension of our results for a non-holonomic unicycle vehicle in Appendix B. Moreover,
the 2nd-order model (1) can be easily extended to account for higher-order integrator systems (Slotine
et al. (1991)).
Consider now a time-constrained task expressed as a time-varying reference trajectory pd : R≥0 →
Rn. The objective of this paper is to construct a time-varying feedback-control algorithm u(x, t) such
that the state of the closed-loop system (1) asymptotically tracks pd, i.e., limt→∞(x(t) - pd(t)) = 0.
3
Under review as a conference paper at ICLR 2022
3 Main Results
This section describes the proposed algorithm, which consists of two steps. Firstly, it learns a
controller, represented as a neural network, using training data that correspond to a collection of
different tasks and system parameters. Secondly, we design an adaptive, time-varying feedback
controller that uses the neural-network approximation and guarantees tracking of the reference
trajectory, consequently achieving satisfaction of the task.
3.1	Neural-network learning
As discussed in Section 1, we are inspired by cases where systems undergo changes that modify their
dynamics and hence the underlying controllers no longer guarantee the satisfaction of a specific task.
In such cases, instead of carrying out the challenging and tedious procedure of identification of the
new dynamic models and design of new model-based controllers, we aim to exploit data from off-line
system trajectories and develop an online policy that is able to adapt to the aforementioned changes
and achieve the task expressed via pd. Consequently, we assume the existence of offline data from a
finite set of T system trajectories that satisfy a collection of tasks, corresponding to bounded reference
trajectories, including pd, and possibly produced by systems with different dynamic parameters. The
data from each trajectory i ∈ {1,...,T} comprise a finite set of triplets {Xs(t),t, us(t)}t∈τi, where
Ti is a finite set of time instants, Xs (t) ∈ R2n are system states, and Us (t) ∈ Rn are the respective
control inputs, compliant with the dynamics (1). We use the data to train a neural network in order
to approximate the respective controller u(x, t). More specifically, We use the pairs (Xs(t), t)t∈τi
as input to a neural network, and us (t)t∈Ti as the respective output targets, for all trajectories
i ∈ {l,...,T}. For given X ∈ R2n,t ∈ R≥o, we denote by Unn(x,t) the output of the neural
network. Note that the controller u(x,t), which the neural network aims to approximate, is not
associated to the specific task expressed via pd and mentioned in Section 2, but a collection of several
tasks. Therefore, we do not expect the neural network to learn how to track pd, but rather to be
able to adapt to the entire collection of tasks. This is an important attribute of the proposed scheme,
since it can generalize over tasks. The motivation for training the neural network with different tasks
and dynamic parameters is the following. Since the tasks correspond to bounded trajectories, the
respective stabilizing controllers compensate successfully the dynamics in (1). Therefore, as will
be clarified in the next section, the neural network aims to approximate an “average” controller the
retains this property, i.e., the boundedness of the dynamics of (1). By using such approximation, the
online feedback-control policy - illustrated in the next section - is able to guarantee tracking of pd
without using any explicit information on the dynamics.
3.2	Feedback control design
As mentioned in Section 3.1, we do not expect the neural-network controller to accomplish tracking of
pd, since the system (1) is trained on potentially different tasks and different system parameters, and
(2) the neural network provides only an approximation of a stabilizing controller; potential deviations
in certain regions of the state space might lead to instability. Moreover, the neural-network controller
has no error feedback with respect to the open-loop trajectory pd; such feedback is substantial in
the stability of control systems with dynamic uncertainties. Therefore, this section is devoted to the
design of a feedback-control policy to track the trajectory pd(t) by using the output of the trained
neural network (see Fig. 1). The goal is to drive the error e := X - pd to zero. As mentioned in
Section 3.1, the motivation for training the neural network with several different tasks and dynamic
parameters is the learning of a controller that is able to retain the property of the training controllers to
compensate the system dynamics (1). This is officially stated in the following assumption regarding
the closed-loop system trajectory that is driven by the neural network’s output.
Assumption 2. The output u∏∏(x, t) Ofthe trained neural network satisfies
Ilf(X,t) + g(x,t)unn(x,t)k ≤ d∣∣X∣∣ + B	(2)
for positive constants d,, B, for all X ∈ R2n, t ≥ 0.
Intuitively, Assumption 2 states that the neural-network controller u∏∏(x, t) is able to maintain the
boundedness of the system state by the constants d, B , which are considered to be unknown. The
assumption is motivated by the property of neural networks to approximate a continuous function
4
Under review as a conference paper at ICLR 2022
Figure 1: Block diagram of the proposed algorithm.
arbitrarily well in a compact domain for a large enough number of neurons and layers (Cybenko
(1989))3. As mentioned before, since the collection of tasks, which the neural network is trained
with, correspond to bounded trajectories, the system states are expected to remain bounded. Since
f (x, t), and g(x, t) are continuous in X and bounded in t, they are also expected to be bounded as per
(2). Contrary to the related works (e.g., (Vamvoudakis & Lewis (2010); Yang et al. (2020); Cheng
et al. (2007); Fan et al. (2018))), however, we do not adopt approximation schemes for the system
dynamics and we do not impose restrictions on the size of d, B . Moreover, Assumption 2 does not
imply that the neural network controller u∩∩(x, t) guarantees tracking of the open-loop trajectory
pd. Instead, it is merely a growth condition. Additionally, note that inequality 2 does not depend
specifically on any of the tasks that the neural network is trained with. We exploit this property in
the control design and achieve task generalization; that is, the open-loop trajectory pd to be tracked
(corresponding to the task 夕)can be any of the tasks that the neural network is trained with.
We now define the feedback-control policy. Consider the adaptation variables 'ι, 6, corresponding
to upper bounds of d, B in (2), with '1 (0) > 0, [2 (0) > 0. We design first a reference signal for X as
Vd := Pd - kιe,	(3)
that would stabilize the subsystem kek2, where k1 is a positive control gain constant. Following the
back-stepping methodology (Krstic et al. (1995)), we define next the respective error e9 := X - Vd
and design the neural-network-based adaptive control law as
O	O
o 0
u(x,'l,[2,t) = U∏∏(x,t) - k2ev - 'ιev -0。
(4a)


[ι = k`i ∣∣ev∣∣2, & = k'2 IlevIl	(4b)
where k2,跖,k`2 are positive constants, and ev = ^^ if ev = 0, and ^v = 0 if ev = 0. The control
design is inspired by adaptive control methodologies (Krstic et al. (1995)), where the time-varying
gains 'ι(t), 22(t), adapt to the unknown dynamics and counteract the effect of d and B in (2) in
order to ensure closed-loop stability. Note that the policy (3), (4) does not use any information on the
system dynamics f (∙), g(∙) or the constants B, d. The tracking of pd is guaranteed by the following
theorem.
Theorem 1. Let a system evolve according to (1) and let an open-loop trajectory pd : R≥0 →
R6 encoding a user-defined task. Under Assumption 2, the control algorithm (4) guarantees
limt→∞(e(t), ev (t)) = 0, as well as the boundedness of all closed-loop signals.
We provide a sketch of the proof here and give the details in Appendix A.
Proof (Sketch). In view of (2), one can find positive constants d1, d2, D such that
1Ige + f (x,t) + g(x,t)unn(x,t) - Vd∣∣ ≤ di ∣e∣ + d2∣ev ∣∣ + D,
g —
3For simplicity, we consider that (2) holds globally, but it can be extended to hold in a compact set.
5
Under review as a conference paper at ICLR 2022
for all X ∈ R2n, t ≥ 0, where g is the minimum eigenvalue of g(x,t), which is positive owing
to the positive definitiveness of g(∙). The constants di, d2, and D define upper bounds '1 and '2,
which we aim to approximate by the adaptation variables '1 and '2 , respectively. We next define the
continuously differentiable function
V(e)	= 1	kek2 + ɪkevk2+ X	ɪ('i	- 'i)2
2	2g	2—2	2k'.
-	i∈{1,2}	'i
which represents an energy-like function of the system, and which we wish to drive to zero. One
can prove, by analyzing the derivative V and using the control policy (4), that V is non-decreasing
along the solutions of the closed-loop system and that limt→∞ V(t) = 0, which implies that
limt→∞ e(t) = limt→∞ e (t) = 0.	□
Note that, contrary to works in the related literature (e.g., (Verginis & Dimarogonas (2020); Bech-
lioulis & Rovithakis (2014))), we do not impose reciprocal terms in the control input that grow
unbounded in order to guarantee closed-loop stability. The resulting controller is essentially a simple
linear feedback on (e(t), ev (t)) with time-varying adaptive control gains, accompanied by the neural
network output that ensures the growth condition (2). The positive gains k1, k2, k`1, k`2 do not affect
the stability results of Theorem 1, but might affect the evolution of the closed-loop system; e.g., larger
gains lead to faster convergence but possibly larger control inputs.
The proposed control algorithm does not require any of the long-standing assumptions on the system
dynamics (1), such as linear parameterization, growth conditions, or boundedness by known functions
(e.g., Krstic et al. (1995); Hong et al. (2009); Chen (2019); Huang et al. (2018a); Kanellakopoulos
et al. (1991); Xu & Ioannou (2003); Ge & Wang (2004); Wen et al. (2011); Chen et al. (2011); Slotine
& Li (1987); Slotine & Coetsee (1986); Chen et al. (2008); Slotine & Coetsee (1986); Chen et al.
(2008); Xian et al. (2004)). Additionally, we do not assume the boundedness of the solution of (1)
or of the dynamic terms f (∙,t), g(∙, t); instead, the control algorithm guarantees via Theorem 1 the
boundedness of the system state as well as the asymptotic tracking of pd(t). The only boundedness
condition that we require is (2) in Assumption 2, which can be accomplished by neural-network
component in view of the universal approximation property Cybenko (1989).
4 Numerical Experiments
This section is devoted to a series of numerical experiments. More details can be found in Appendix
B. We first test the proposed algorithm on a 6-dof UR5 robotic manipulator with dynamics
x = B(X)-I (U — C(X)X — g(x) + d(x, t))
(5)
where x,X ∈ R6 are the vectors of robot joint angles and angular velocities, respectively; B(X) ∈
R6×6 is the positive definite inertia matrix, C(X) ∈ R6×6 is the Coriolis matrix, g(x) ∈ R6 is the
gravity vector, and d(x, t) ∈ R6 is a vector of friction terms and exogenous time-varying disturbances.
The workspace consists of four points of interest T1, T2,
T3 , T4 (end-effector position and Euler-angle orientation),
as depicted in Fig. 2, which correspond to the joint-angle
vectors c1, c2 , c3, c4. More information is provided in
Appendix A. We consider a nominal task expressed via the
spatio-temporal constraint Vi∈{1,...,4} G[0,∞)FIi(kX1 -
ci k ≤ 0.1), where G and F are the always and eventually
operators respectively. The task consists of visits of X1 to
ci ∈ R6 (within the radius 0.1) infinitely often within the
time intervals dictated by Ii, for i ∈ {1, . . . , 4}.
We create 150 problem instances by varying the positions
of ci, the time intervals Ii , the dynamic parameters of the
robot (masses and moments of inertia of the robot’s links
and actuators), the friction and disturbance term d(∙), the
initial position and velocity of the robot, and the sequence
of visits to the points ci, as dictated by φ, i.e., one instance
Figure 2: A UR5 robot in a workspace
with four points of interest Ti , i ∈
{1,...,4}.
6
Under review as a conference paper at ICLR 2022
( = (⅛= ±(⅞=)ue ①日
3
2
1
0
0	10
Figure 3: Mean (top) and standard deviation (bottom) of ∣∣e(t)k + ∣∣e(t)k for the proposed (left),
non-adaptive (center), and no-neural-network (right) control policies.
might correspond to the visit sequence ((x(0), 0) → (c1, t11) → (c2, t12) → (c3, t13) → (c4, t14),
and another to ((x(0), 0) → (c3, t13) → (c1, t11 ) → (c4, t14) → (c2, t12). We separate the
aforementioned 150 problem instances into 100 training instances and 50 test instances. We generate
trajectories using the 100 training instances from system runs that satisfy different variations of
one cycle of the task (i.e., one visit to each point). Each trajectory consists of 500 points and is
generated using a nominal model-based controller. We use these trajectories to train a neural network
and we test the control policy (4) in the 50 test instances. We also compare our algorithm with the
non-adaptive controller uc(x, t) = u∩∩(x, t) 一 kιe 一 k?e, as well as with a modified version ud(x, t)
of (4) that does not employ the neural network (i.e., the term u∩∩(x, t)). The comparison results are
depicted in Fig. 3, which depicts the mean and standard deviation of the signal ∣e(t)∣ + ∣∣<e(t)∣ for
the 50 instances and 20 seconds. It is clear from the figure that the proposed algorithm performs
better than the non-adaptive and no-neural-network policies both in terms of convergence speed
and steady-state error. It is worth noting that the non-adaptive policy results on average in unstable
closed-loop system.
We next test the proposed algorithm, following a similar procedure, on a unicycle robot (the respective
dynamics and control algorithm can be found in Appendix B). Fig. 5 depicts the mean and standard
deviation of the errors ed(t), eβ(t) for 50 test instances. We note that the performance of the no-
neural-network control policy is much more similar to the proposed one than in the UR5 case. This
can be attributed to (1) the lack of gravitational terms in the unicycle dynamics, which often lead to
instability, and (2) the chosen control gains of the no-neural-network policy, which are sufficiently
large to counteract the effect of the dynamic uncertainties.
7
Under review as a conference paper at ICLR 2022
Finally, we compare the performance of the proposed control policy with the re-
ported data of (Wang et al. (2019)) on the benchmarking enivronment of the pen-
dulum, where a single-link mechanical structure aims to reach the upright position.
Following (Wang et al. (2019)), the reward and costs are rpend(t) = - cos q(t) -
0.1 Sin q(t) — 0.iq(t) — 0.001u(t)2 and
Similar to the previous cases and in contrast to
Wang et al. (2019), We generate 150 instances
by varying the system parameters (pendulum
length and mass), the external disturbances, and
the initial conditions. We generate 100 trajec-
tories, consisting of 500 points each, for the
100 training instances, by employing a nomi-
nal controller, and we use them train a neural
network. Moreover, in order to guarantee the
feasibility of the proposed algorithm (4) we con-
sider larger control-action bounds than in (Wang
et al. (2019)). The original bounds render these
systems under-actuated, which is not included
in the considered class of systems (1) and con-
sist part of our future work. It should be noted
that such larger bounds affect negatively the ac-
quired rewards. We test the control policy (4) on
the 50 test instances, for 5000 steps each, corre-
sponding to 10 seconds. We set H = 200 and
γ = 1 (Wang et al. (2019)). Fig. 4 depicts the
Jpend	=	PtH=1 γtrpend(t),	respectively.
Figure 4: Mean and standard deviation of the re-
ward and cost for the pendulum environment.
mean and standard deviation of the time-varying reward and cost functions, illustrating successful
regulation to the upright position. After 5000 steps, we obtain a mean reward and cost of 0.99 and
200, respectively, showing better performance than the reported cost of 180 in (Wang et al. (2019)).
Moreover, the proposed control algorithm achieves this performance without resorting to exhaustive
exploration of the state-space, which is the case in the reinforcement-learning algorithms used in
(Wang et al. (2019)). This is a very important property in practical engineering systems, where safety
is of paramount significance and certain areas of the state space must be avoided.
5	Discussion and Limitations
As shown in the experimental results, the control algorithm is able to asymptotically track reference
trajectories which were not considered when generating the training data. Similarly, the trajectories
used in the training data were generated using systems with different dynamical parameters, and not
specifically the ones used in the tests. The aforementioned attributes signify the ability of the proposed
algorithm to generalize to different tasks and systems with different parameters. Nevertheless, the
proposed control policy is currently limited by systems satisfying Assumption 1 and is not able
to take into account under-actuated systems (e.g., the acrobot or cart-pole system); such systems
consist part of our future work. Moreover, the proposed algorithm is dependent on the performance
of the learning component to satisfy Assumption 2. Lack of satisfactory training might cause the
neural-network output to add extra terms in the dynamics that render the closed-loop system unstable.
A less conservative and more robust approach consists of using the off-line data to train multiple
neural networks, each for a certain region of the state space; such an approach constitutes part of
our future work. Finally, the discontinuities of (4), (12) might be problematic and create chattering
when implemented in real actuators. A continuous approximation that has shown to yield satisfying
performance is the boundary-layer technique (Slotine et al. (1991)).
6	Conclusion and Future Work
We develop a novel control algorithm for the control of robotic systems with unknown nonlinear
dynamics subject to tasks expressed as reference trajectories. The algorithm integrates neural network-
based learning and adaptive control. We provide formal guarantees and perform extensive numerical
8
Under review as a conference paper at ICLR 2022
4
FDj (G)suEgUI
1.5
3 2 1
(ɪ(£ Pa) up ①日
---Proposed
---Non-adaptive
No-nn
1
0.5
0-------------1-----------1------------------------
0	5	10	15	20
t [sec∖
0.5
0.4
0.3
---Proposed
---Non-adaptive
—No-nn
0----------ι--------1--------1——
0	5	10	15	20
0.2
0.1
0
0	5	10	15	20
t [sec∖
——Proposed
---Non-adaptive
No-nn
Figure 5: Mean (left) and standard deviation (right) of ed(t), β(t) for the proposed, non-adaptive,
and no-neural-network control policies.
experiments. Future directions will focus on relaxing the considered assumptions and extending the
proposed methodology to underactuated systems.
7	Reproducibility statement
We provide the proof of Theorem 1 in Appendix A. Additionally, we elaborate on the required
assumptions (Assumptions 1, 2, 3) throughout the text - see pages 3, 5, and 7. Finally, we provide
implementation details, instructions, and the respective code, required to reproduce the results, in
Appendix C and the supplementary material.
References
Charalampos P Bechlioulis and George A Rovithakis. A low-complexity global approximation-free
control scheme with prescribed performance for unknown pure feedback systems. Automatica, 50
(4):1217-1226, 2014.
Thomas Berger, HUy Hoang Le, and Timo Reis. Funnel control for nonlinear systems with known
strict relative degree. Automatica, 87:345-357, 2018.
Felix Berkenkamp and Angela P Schoellig. Safe and robust learning control with gaussian processes.
European Control Conference (ECC), pp. 2496-2501, 2015.
Dimitri Bertsekas. Lessons from alphazero for optimal, model predictive, and adaptive control. arXiv
preprint arXiv:2108.10315, 2021.
Dimitri Bertsekas and John Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, MIT, 1996.
Mingyu Cai, Mohammadhosein Hasanbeig, Shaoping Xiao, Alessandro Abate, and Zhen Kan.
Modular deep reinforcement learning for continuous motion planning with temporal logic. arXiv
preprint arXiv:2102.12855, 2021.
Alberto Camacho and Sheila A McIlraith. Towards neural-guided program synthesis for linear
temporal logic specifications. arXiv preprint arXiv:1912.13430, 2019.
9
Under review as a conference paper at ICLR 2022
M Capotondi, G Turrisi, C Gaz, Valerio Modugno, Giuseppe Oriolo, and A De Luca. An online
learning procedure for feedback linearization control without torque measurements. Conference
on Robot Learning ,pp.1359-1368, 2020.
Jian Chen, Aman Behal, and Darren M Dawson. Robust feedback control for a class of uncertain
mimo nonlinear systems. IEEE Transactions on Automatic Control, 53(2):591-596, 2008.
Mou Chen, Shuzhi Sam Ge, and Beibei Ren. Adaptive tracking control of uncertain mimo nonlinear
systems with input constraints. Automatica, 47(3):452-465, 2011.
Zhiyong Chen. Nussbaum functions in adaptive control with time-varying unknown control coeffi-
cients. Automatica, 102:72-79, 2019.
Tao Cheng, Frank L Lewis, and Murad Abu-Khalaf. A neural network solution for fixed-final time
optimal control of nonlinear systems. Automatica, 43(3):482-490, 2007.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Kenji Doya. Efficient nonlinear control with actor-tutor architecture mozer, jordan, petsche (eds.)
advances in neural information processing systems 9. 1997.
Bo Fan, Qinmin Yang, Xiaoyu Tang, and Youxian Sun. Robust adp design for continuous-time
nonlinear systems with output constraints. IEEE transactions on neural networks and learning
systems, 29(6):2127-2138, 2018.
Nicholas Fischer, Rushikesh Kamalapurkar, and Warren E Dixon. Lasalle-yoshizawa corollaries for
nonsmooth systems. IEEE Transactions on Automatic Control, 58(9):2333-2338, 2013.
Shuzhi Sam Ge and Cong Wang. Adaptive neural control of uncertain mimo nonlinear systems. IEEE
Transactions on Neural Networks, 15(3):674-692, 2004.
Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus N Rabe, and Bernd Finkbeiner. Teaching
temporal logics to neural networks. arXiv preprint arXiv:2003.04218, 2020.
F Hong, SS Ge, B Ren, and TH Lee. Robust adaptive control for a class of uncertain strict-feedback
nonlinear systems. International Journal of Robust and Nonlinear Control: IFAC-Affiliated Journal,
19(7):746-767, 2009.
Haimin Hu, Mahyar Fazlyab, Manfred Morari, and George J Pappas. Reach-sdp: Reachability
analysis of closed-loop systems with neural network controllers via semidefinite programming.
IEEE Conference on Decision and Control (CDC), pp. 5929-5934, 2020.
Jiangshuai Huang, Wei Wang, Changyun Wen, and Jing Zhou. Adaptive control of a class of strict-
feedback time-varying nonlinear systems with unknown control coefficients. Automatica, 93:
98-105, 2018a.
Xiucai Huang, Yongduan Song, and Junfeng Lai. Neuro-adaptive control with given performance
specifications for strict feedback systems under full-state constraints. IEEE transactions on neural
networks and learning systems, 30(1):25-34, 2018b.
Edouard Ivanjko, Toni Petrinic, and Ivan Petrovic. Modelling of mobile robot dynamics. 7th
EUROSIM Congress on Modelling and Simulation, 2, 2010.
Radoslav Ivanov, James Weimer, Rajeev Alur, George J Pappas, and Insup Lee. Verisig: verifying
safety properties of hybrid systems with neural network controllers. Proceedings of the 22nd ACM
International Conference on Hybrid Systems: Computation and Control, pp. 169-178, 2019.
Achin Jain, Truong Nghiem, Manfred Morari, and Rahul Mangharam. Learning and control using
gaussian processes. ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS), pp.
140-149, 2018.
Girish Joshi, Jasvir Virdi, and Girish Chowdhary. Asynchronous deep model reference adaptive
control. Conference on Robot Learning, 2020.
10
Under review as a conference paper at ICLR 2022
Rushikesh Kamalapurkar, Huyen Dinh, Shubhendu Bhasin, and Warren E Dixon. Approximate
optimal trajectory tracking for continuous-time nonlinear systems. Automatica, 51:40—48, 2015.
Ioannis Kanellakopoulos, Petar V Kokotovic, and A Stephen Morse. Systematic design of adaptive
controllers for feedback linearizable systems. American control conference, pp. 649-654, l991.
Bahare Kiumarsi, Kyriakos G Vamvoudakis, Hamidreza Modares, and Frank L Lewis. Optimal
and autonomous control using reinforcement learning: A survey. IEEE transactions on neural
networks and learning systems, 29(6):2042-2062, 2017.
M. Krstic, I. Kanellakopoulos, and P. Kokotovic. Nonlinear and Adaptive Control Design. Publisher:
Wiley New York, 1995.
Kevin Leahy, Eric Cristofalo, Cristian-Ioan Vasile, Austin Jones, Eduardo Montijano, Mac Schwager,
and Calin Belta. Control in belief space with temporal logic specifications using vision-based
localization. The International Journal of Robotics Research, 38(6):702-722, 2019.
Lars Lindemann, Christos K Verginis, and Dimos V Dimarogonas. Prescribed performance control
for signal temporal logic specifications. IEEE Conference on Decision and Control (CDC), pp.
2997-3002, 2017.
Ke Liu, Robert Tokar, and Brain McVey. An integrated architecture of adaptive neural network
control for dynamic systems. Advances in neural information processing systems, 7:1031-1038,
1994.
Wenliang Liu, Noushin Mehdipour, and Calin Belta. Recurrent neural network controllers for signal
temporal logic specifications subject to safety constraints. IEEE Control Systems Letters, 2021.
Meiyi Ma, Ji Gao, Lu Feng, and John Stankovic. Stlnet: Signal temporal logic enforced multivariate
recurrent neural networks. Advances in Neural Information Processing Systems, 33, 2020.
Emilio Tanowe Maddalena, CG da S Moraes, Gierri Waltrich, and Colin N Jones. A neural network
architecture to learn explicit mpc controllers from data. IFAC-PapersOnLine, 53(2):11362-11367,
2020.
Lipo Mo, Xiaolin Yuan, and Yongguang Yu. Neuro-adaptive leaderless consensus of fractional-order
multi-agent systems. Neurocomputing, 339:17-25, 2019.
Julian Nubert, Johannes Kohler, Vincent Berenz, Frank Allgower, and Sebastian Trimpe. Safe and
fast tracking on a robot manipulator: Robust mpc and neural network control. IEEE Robotics and
Automation Letters, 5(2):3050-3057, 2020.
Brad Paden and Shankar Sastry. A calculus for computing filippov’s differential inclusion with
application to the variable structure control of robot manipulators. IEEE transactions on circuits
and systems, 34(1):73-82, 1987.
Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus
Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, et al. Logical neural
networks. arXiv preprint arXiv:2006.13155, 2020.
Ankit Jayesh Shah, Pritish Kamath, Shen Li, and Julie A Shah. Bayesian inference of temporal task
specifications from demonstrations. 2018.
Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, and Giuseppe Oriolo. Modelling, planning and
control. Advanced Textbooks in Control and Signal Processing. Springer,, 2009.
J-JE Slotine and JA Coetsee. Adaptive sliding controller synthesis for non-linear systems. Interna-
tional Journal of Control, 43(6):1631-1651, 1986.
Jean-Jacques E Slotine and Weiping Li. On the adaptive control of robot manipulators. The
international journal of robotics research, 6(3):49-59, 1987.
Jean-Jacques E Slotine, Weiping Li, et al. Applied nonlinear control, volume 199. Prentice hall
Englewood Cliffs, NJ, 1991.
11
Under review as a conference paper at ICLR 2022
Chuangchuang Sun and Kyriakos G Vamvoudakis. Continuous-time safe learning with temporal logic
constraints in adversarial environments. American Control Conference (ACC), pp. 4786-4791,
2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Kyriakos G Vamvoudakis and Frank L Lewis. Online actor-critic algorithm to solve the continuous-
time infinite horizon optimal control problem. Automatica, 46(5):878-888, 2010.
Christos Verginis and Dimos V Dimarogonas. Asymptotic tracking of second-order nonsmooth
feedback stabilizable unknown systems with prescribed transient response. IEEE Transactions on
Automatic Control, 2020.
Christos K Verginis and Dimos V Dimarogonas. Timed abstractions for distributed cooperative
manipulation. Autonomous Robots, 42(4):781-799, 2018.
Christos K. Verginis, Dimos V. Dimarogonas, and Lydia E. Kavraki. Kdf: Kinodynamic motion
planning via geometric sampling-based algorithms and funnel control. 2021.
Draguna Vrabie and Frank Lewis. Neural network approach to continuous-time direct adaptive
optimal control for partially unknown nonlinear systems. Neural Networks, 22(3):237-246, 2009.
Chuanzheng Wang, Yinan Li, Stephen L Smith, and Jun Liu. Continuous motion planning with
temporal logic specifications using deep neural networks. arXiv preprint arXiv:2004.02610, 2020.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement
learning. arXiv preprint arXiv:1907.02057, 2019.
Changyun Wen, Jing Zhou, Zhitao Liu, and Hongye Su. Robust adaptive control of uncertain
nonlinear systems in the presence of input saturation and external disturbance. IEEE Transactions
on Automatic Control, 56(7):1672-1678, 2011.
Bin Xian, Darren M Dawson, Marcio S de Queiroz, and Jian Chen. A continuous asymptotic tracking
control strategy for uncertain nonlinear systems. IEEE Transactions on Automatic Control, 49(7):
1206-1211, 2004.
Haojian Xu and Petros A Ioannou. Robust adaptive control for a class of mimo nonlinear systems
with guaranteed error bounds. IEEE Transactions on Automatic Control, 48(5):728-742, 2003.
Ruixuan Yan and Agung Julius. Neural network for weighted signal temporal logic. arXiv preprint
arXiv:2104.05435, 2021.
Yongliang Yang, Kyriakos G Vamvoudakis, and Hamidreza Modares. Safe reinforcement learning
for dynamical games. International Journal of Robust and Nonlinear Control, 30(9):3706-3726,
2020.
Ssu-Hsin Yu, Anuradha M Annaswamy, DS Touretzky, MC Mozer, and ME Hasselmo. Neural
control for nonlinear dynamic systems. Advances in Neural Information Processing Systems, pp.
1010-1016, 1996.
Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Tossingbot:
Learning to throw arbitrary objects with residual physics. IEEE Transactions on Robotics, 36(4):
1307-1319, 2020.
Jingang Zhao and Minggang Gan. Finite-horizon optimal control for continuous-time uncertain
nonlinear systems using reinforcement learning. International Journal of Systems Science, 51(13):
2429-2440, 2020.
Yaofeng Desmond Zhong and Naomi Leonard. Unsupervised learning of lagrangian dynamics from
images for prediction and control. Advances in Neural Information Processing Systems, 33, 2020.
12
Under review as a conference paper at ICLR 2022
A Appendix
We provide here the proof of Theorem 1. We first give some preliminary notation and background on
systems with discontinuous dynamics.
Notation
Given a function f : Rn → Rk, its Filippov regularization is defined as (Paden & Sastry (1987))
K[f](x):= \	\ co(f (B(x, δ)∖NN),t),	(6)
δ>0 μ(N)=0
where T*(n)=0 is the intersection over all sets N of Lebesgue measure zero, Co(E) is the closure of
the convex hull co(E) of the set E, and B(x, δ) is the ball with radius δ centered at x.
Nonsmooth Analysis
Consider the following differential equation with a discontinuous right-hand side:
X = f (χ,t),	⑺
where f : D × [t0 , ∞) → Rn, D ⊂ Rn, is Lebesgue measurable and locally essentially bounded.
Definition 1 (Def. 1 of (Fischer et al. (2013))). A function x : [t0, t1) → Rn, with t1 > t0, is called
a Filippov solution of (7) on [t0, t1) if x(t) is absolutely continuous and if, for almost all t ∈ [t0, t1),
it satisfies X ∈ K[f](x,t), where K[f](x,t) is the Filippov regularization of f (x,t).
Lemma 1 (Lemma 1 of (Fischer et al. (2013))). Let x(t) be a Filippov solution of (7) and V :
D × [t0, t1) → R be a locally Lipschitz, regular function4. Then V (X(t), t) is absolutely continuous,
∂
V(x(t),t) = ∂∂tV(x(t),t) exists almost everywhere (α.e.), i.e., for almost all t ∈ [t0,t1), and
a.e
V(x(t),t) ∈ V(x(t),t), where
V :=	\ ξ> K[f](χ,t) ,
ξ∈∂V (x,t)
and ∂V(X, t) is Clarke’s generalized gradient at (X, t) (Fischer et al. (2013)).
Corollary 1 (Corollary 2 of (Fischer et al. (2013))). For the system given in (7), let D ⊂ Rn be
an open and connected set containing X = 0 and suppose that f is Lebesgue measurable and
X 7→ f(X, t) is essentially locally bounded, uniformly in t. Let V : D × [t0, t1) → R be locally
Lipschitz and regular such that W1 (X) ≤ V(X, t) ≤ W2(X), ∀t ∈ [t0, t1), X ∈ D, and
•
z ≤ -W (X(t)), ∀z ∈ V (X(t), t), t ∈ [t0, t1), X ∈ D,
where W1 and W2 are continuous positive definite functions and W is a continuous positive semi-
definite on D. Choose r > 0 and c > 0 such that B(0, r) ⊂ D and C < min∣∣χk=r Wι(x). Thenfor
all Filippov solutions X : [t0,t1) → Rn of (7), with x(to) ∈ D := {x ∈ B(0,r) : W2(x) ≤ c}, it
holds that t1 = ∞, X(t) ∈ D, ∀t ∈ [t0, ∞), and limt→∞ W (X(t)) = 0.
Proof of Theorem 1. We re-write first the condition of Assumption 2. Note that pd(t) and its deriva-
tives are bounded functions of time. Moreover, since e = X - pd, e = X - pd, it holds that
IlXIl ≤ IleIl + IleIl + IlpdIl + IIpdIl and Vd = Pd - kιe, as well as ev = X - Vd = e + kιe implying
e = ev - k∖e. Therefore, in view of (2), one can find positive constants di, d?, D such that
1Ige + f (X,t) + g(X,t)unn(X,t) - Vdll ≤ di∣∣e∣∣ + d2∣∣ev∣∣ + D,	(8)
g 一
for all X ∈ R2n, t ≥ 0, where g is the minimum eigenvalue of g(X, t), which is positive owing to the
positive definitiveness of g(∙). Inequality (8) will be used later in the proof.
4See (Fischer et al. (2013)) for a definition of regular functions.
13
Under review as a conference paper at ICLR 2022
Let now a constant α such that d2α < kι. As will be clarified later, the adaptation variables A,
^2 aim to approximate the constants 会 + d2 and D, respectively. Therefore, let '1 := d1 + d?,
'2 ：= D, and the respective error terms '1 := & - 'ι, '2 ：= '2 - '2, as well as the overall state
>	>	>	2 +2
xe := [e>, ev>, '1, '2]> ∈ R2n+2. Since the control policy is discontinuous, we use the notion of
Filippov solutions. The Filippov regularization of u is K[u] = unn(x, t) - k2ev - '1ev - '2Ev, where
Ev ：= kevk if ev = 0 and Ev ∈ (-1,1) otherwise. Note that, In any case, It holds that eV Ev = ∣∣ev∣∣.
Let now the continuously differentiable function
V(e)：=2 kek2 + 2g B"2 + i∈X2} 21i ei
which satisfies Wι(e) ≤ V(X) ≤ W2(x) for Wι(x) := min{2, 2g, ɪ, ɪ}ke∣2, W2(e) :=
a.e.
max{2, 2g, 2⅛, 2⅛}"xk2. According to Lemma 1, V(X) ∈ V(x), with V := Tξ∈dVGe)K[⅛].
Since V(X) is continuously differentiable, its generalized gradient reduces to the standard gradient
and thus it holds that V(x) = W>K[X], where W = [e>,1 e>, 4'1, 4'2]>. By differentiating
g	k'ι	k'2
V and using (3), one obtains
V ⊂fs := e>(χ - Pd) + ge>(f(x,t) + g(χ,t)u - Vd) + k-'i%1 + k-X2'
and by using X = ev + Vd and substituting the control policy (3), (4), and inequality (8),
f s := - k1ke∣∣2 +---e> (ge + f (x, t) + g(x, t)unn(x, t) - Vd)----e>g(x, t) (k2ev + 'lev + &Ev)
g
g
+ 'X1 ∣ev ∣2 + 'X2 ∣ev ∣
≤-k1∣e∣2+d1∣ev∣∣e∣ + d2∣ev∣2 + D∣ev∣ -
-ev g(x,t)(k2ev + 'lev + '2 Ev )
+ 'X1 ∣ev ∣2 + 'X2 ∣ev ∣
Note from (4) and the fact that 'ι(0) > 0, '2(0) > 0 that 'ι(t) > 0 and '2(t)) > 0 for all t ≥ 0.
Moreover, recall that g(X, t) is positive definite for all X ∈ R2n, t ≥ 0, with g being its minimum
(and positive) eigenvalue. Therefore, we conclude that Wfs becomes
-----	..C	.... ..C	....	,	ʌ . ..	..C	ʌ ..	~ ..	..C	~ ..
fs ≤ — kι ∣∣e∣∣ + di ∣∣ev Ilkek + d2 ∣∣ev k + Dkev Il- (k2 + '1) ∣∣ev k — '2 ∣∣ev k + '1 ∣∣ev k + '2 ∣∣ev k
By incorporating the term α in the term d1 ∣ev ∣∣e∣, we obtain d1 ∣evlike" = dιαkevkIlek andby
using the property ab ≤ 11 a2 + 2 b2 for any constants a,b, we obtain dιαkθd kek ≤ d2ɑ kek2 +
d
dakevk2. Therefore, Ws becomes
fs ≤-卜1 — d2α) kek2 + dα+ + d2) kev k2 + Dkev k - 阳 + 'ι)kev k2 - '2 kev k + '1∣ev k2 + e2kev k
=— (kI — d2α) kek2 + 'ikevk2 + '2kevk- (k2 + %ι)kevk2 - '2kevk + eikevk2 + e2kevk
ki - d2α) kek2 — k2kevk2 =： —Q(e)
Therefore, it holds that ζ ≤ -Q(xX), for all ζ ∈ V, with Q being a continuous and positive
semi-definite function in R2n+2, since ki - d2α > 0. Choose now any finite r > 0 and let
c < minkxek=r W1(xX). Hence, all the conditions of Corollary 1 are satisfied and hence, all Filippov
solutions starting from X(0) ∈ Ωf := {X ∈ B(0, r) : W2(X) ≤ c} are bounded and remain in Ωf,
satisfying limt→∞ Q(xX(t)) = 0. Note that r, and hence c, can be arbitrarily large allowing and finite
initial condition x(0). Moreover, the boundedness of X implies the boundedness of e, e, ev, '1, and
TT 1 1	，幺 /，、	•1△/,、「 ∙M , 一 E τ .	…、	C ∙M	1	1	.1	1	1	1
'2, and hence of'1(t) and '2(t), for all t ∈ R≥0. In view of (5), we finally conclude the boundendess
• ∙
of u(∙), '1, and '2, for all t ∈ R≥o, leading to the conclusion of the proof.
□
14
Under review as a conference paper at ICLR 2022
Figure 6: A unicycle vehicle.
B Appendix
Extension to non-holonomic unicycle dynamics
As mentioned in Section 1, the dynamics 1 do not represent all kinds of systems, with one particular
example being when non-holonomic constraints are present. In such cases, the control law design (4)
and Theorem 1 no longer hold. In this section, we extend the control policy to account for unicycle
vehicles subject to first-order non-holonomic constraints. More specifically, we consider the dynamics
pi = V cos φ, p2 = V sin φ, φ = ω
Mθ = U + fθ (x, t)
(9a)
(9b)
where x = [p1,p2, φ]> ∈ R3 are the unicycle’s position and orientation, (V, ω) are its linear
and angular velocity (see Fig. 6), θ := [θR, θL]> ∈ R2 are its wheel’s angular positions, and
U = [UR, UL]> ∈ R2 are the wheel’s torques, representing the control input. The unicycle vehicle
is subject to the non-holonomic constraint pi sin φ — p⅛ cos φ = 0, which implies that the vehicle
cannot move laterally. Additionally, M ∈ R2×2 is the vehicle’s inertia matrix, which is symmetric
and positive definite, and fθ(∙) is a function representing friction and external disturbances. The
rr
velocities satisfy the relations V = r(Θr + Θl), ω = 2rR(Θr — Θl)), where r and R are the
wheels, radius and axle length, respectively. The terms r, R, M, and fθ(∙) are considered to be
completely unknown. As before, the goal is for the vehicle’s position p := [pi,p2]> to track a desired
trajectory pd = [pd,i,pd,2]> ∈ R2. Towards that end, we define the error variables ei := pi — pd,i,
e2 := p2 — pd,2, ed := kp — pdk, as well as the angle β measured from the the longitudinal axis of
the vehicle, i.e., the unicycle’s direction vector [cos φ, sin φ], to the error vector —[ei, e2] (see Fig.
6). The angle β can be derived by using the cross product between the aforementioned vectors, i.e.,
ed sin(β) = [cos φ, sin φ] × [—ei, —e2] = ei sin φ — e2 cos φ. The purpose of the control design,
illustrated next, is to drive ed and β to zero. By differentiating the latter and using (9) as well as the
relations ei = —ed cos(φ + β), e2 = —ed sin(φ + β) (see Fig. 6), we derive
<≡d = —V cos β + pd,i cos(φ + β) + pd,2 sin(φ + β)
β = —ω + sinβv — ppd1 sin(φ + β) + pd2 cos(φ + β)
ed ed	ed
In view of (10), we set reference signals for the vehicle’s velocity as
1
cos(β )
(pd,i cos(β + Φ) + pd,2 sin(β + Φ) + kdɛd)
sin(φ)pd,i
cos(β)ed
+ COS(O)Pd,2
cos(β)ed
+ kd tan β + kββ
(10a)
(10b)
(11a)
(11b)
where kd , kβ are positive gains, aiming to create exponentially stable subsystems via the terms kded
and kββ. We define next the respective velocity errors ev := V — Vd, eω := ω — ωd and design the
15
Under review as a conference paper at ICLR 2022
adaptive and neural-network-based control input as u(x, d, t) := [US+uD, US-UD ]> + u∩∩(x, t), with
sin β
US := 'vVd — (kv + 'ι)e° — '26v + ed CoS β — β------	(12a)
ed
UD := 'ωωd - (kω + 'I)eω - '2eω + β	(12b)
• ∙
'v := -kv ev vd,	'ω := -kω eω 力d	(12C)
'1 := kι(eV + eω)	'2 := k2(∣ev | + ∣eω |)	(12d)
where 'v, 'ω, 'i are adaptation variables (similar to (4)), with 'v(0) > 0, 'ω(0) > 0 and kv, kω, ki,
are positive gains, i ∈ {l, 2}; e@, with a ∈ {v, ω}, is defined as ^a = ∣ea∣ if e° = 0 and e& = 0
otherwise. We now re-state Assumption 2 to apply for the unicycle analysis as follows.
Assumption 3. The output u∏∏(x,t) ofthe trained neural network satisfies ∣∣u∏∏(x,t) + fθ(x,t)k ≤
d∣∣xk + B ,for positive, unknown constants d, B.
Similar to assumption 2, assumption 3 is merely a growth-boundedness condition by the unknown
constants d and B . The stability of the proposed scheme is provided in the next corollary.
Corollary 2. Let the unicycle system (9) and let an open-loop trajectory pd : R≥0 → R2 *.
Assume that β(t) ∈ (—β,β), ∣p⅛,ι Sin Φ — pd,2 CoS Φ∣ < edɑι, Sinβ < edα? for positive con-
stants β < 2, αι, α2 and all t ≥ 0. Under Assumption 3, the control policy (12) guarantees
limt→∞(ed(t), β(t), ev (t), eω (t)) = 0, and the boundedness of all closed-loop signals.
Proof of Corollary 2. The derivatives of ed, β in (10) can be written as
ed
β
v
ω
G
Ppd,1 cos(φ + β) + Pd,2 sin(φ + β)
+ [pdd2 cos(φ + β) — Pdf sin(φ + β)
where
— CoS β
G := Sin β
e ed
0
—1
It can be verified that the reference velocity signals, designed in (11), can be written as
Zd] _ G_1「一pd,ι cos(φ + β) — Pd,2 sin(φ + β) — kded
ωd] =	[一Pf cos(φ + β) + Pef sin(φ + β) — kββ_
and therefore, by using the relations v = ev + Vd and ω = eω + ωd, ed and β can be written as
ed
β
—kd ed	ev
—kβ β + G eω
—kded
—kβ 8
—ev CoS β
sin β
ev -e----eω
ed
(13)
The control design follows the back-stepping methodology (Krstic et al. (1995)). Let, therefore, the
function V1 := 2(ed + β2), which, after time differentiation and use of (13), yields
匕
2	2	Sin β
—kd，d — kββ — CdCv CoS β + βev-------βeω
ed
(14)
We will cancel the two last terms of V1 using the control input (12).
First, we note that the inertia matrix of the system M, appearing in the unicycle dynamics (9),
has the form (Ivanjko et al. (2010)) M = MI M2 with Mi := mr2- + (IC +Rd )r + Io,
M2 M1	4	4R
M2 := mr2 — (IC +md )r , and where IC is the moment of inertia of the vehicle with respect to point
C (see Fig. 6), I0 is the moment of inertia of the the wheels, and d is the distance of the between
16
Under review as a conference paper at ICLR 2022
point C and the vehicle’s center of mass (p1,p2). Therefore, the second part of the unicycle dynamics
(9) can be written as
MiΘr + M20L = UR + fθ,R (x,t)
M2θR + MM = UL + fθ,L(X,t)
with fθ,R and fθ,L denoting the elements of fθ . By summing and subtracting the aforementioned
equations, we obtain
(MI + M2)(θR	+	θL)= UR + UL +	fθ,R(x, t)	+ fθ,L(x, t)	(15a)
(MI- M2)(θR	-	θL)	= UR - UL +	fθ,R(χ, t)	- fθ,L(χ, t)	(15b)
From the definition of ev and eω, it holds that e?v = V — v⅛, e^ω = ω - ωd, which, by using the
1	ʃr / A . A ∖	r / A	A ∖	∙>∕<∕~∖F
relations V = 2(Θr + Θl), ω = 2R(Θr — Θl) and (15), becomes
ev = 2(MIr+ M) (UR + UL + fθ,R(x,t) + fθ,L(x,t)) - Vd
r
eω = 2R(Mι - M2)
UR — UL + fθ,R(X,t) — fθ,L(X,t)	— Q d
(16a)
(16b)
Define now 'v := 2 M1+M2, 'ω := 2R MI-M2. The adaptation terms Mv, and %ω, used in the control
mechanism (12), aim to approximate 'v and 'ω, respectively. Note that 'v and 'ω are positive, and
we define the errors 'v := 'v - 'v, 'ω := 'ω - 'ω .
We re-write next the condition of Assumption 3. Itholds that kpk = pjι+ + p2 ≤ 2ed + ∣pd,ι∣ + ∣pd,21.
Moreover, in view of (11) as well as the fact that β ∈ (-β,β) ⊂ (-∏, ∏), where β is the bound of
β, stated in Corollary 2, it holds that |v| ≤ 除| + |vd| ≤ 除| + ^^^(∣Pd,ι∣ + ∣Pd,2∣ + kded), ∣ω∣ ≤
∣eω | + ∣Qd∣ ≤ 归ω | + Cosjf (αι + kd) + kβ ∣β∣, where we also use the fact that lpd,1 Sm φ-pd,2 cosφl ≤ αι,
By also recalling that φ moves on the unit circle, we conclude that Assumption 3 becomes
kfθ(x,t) + Unn(x,t)k ≤ dded + dβ∣β∣ + d∣ev∣ + d∣eω| + D,	(17)
with dd := d(2 + ~dββ ), dβ := dkβ, and D := d(2pd + 2π + CosJ(α1 + 2vd + 1)), where Pd and vd
are the upper bounds ofpd,i, Vd,i, respectively, i ∈ {1, 2}.
Let now positive constants γd and γβ such that kd > 2ddγd and kβ > 2dβγβ, and define
'1 := dd + dβ +4d
γd	γβ
(18)
which is the constant to be approximated by '1; its derivation will be clarified later. Let also '2 := 2D,
which we aim to approximate with the adaptation variable '2 . We define hence the respective errors
7Γ 介〃 1 7Γ 2〃 IlC ,1	H	~ r 0	ST 7Γ 7Γ 7Γ ι -Γ _ τ∏>⅛⅞
'1 := '1 - '1 and '2 := '2 -'2 and define the overall vector xe := [ed, β, ev, eω, 'v, 'ω, '1, '2]> ∈ R8.
Since the control mechanism is discontinuous, we use again the notion of Filippov solutions. The
Filippov regularization K[u] of U differs from U only in the terms ev and eω, which are replaced by
EV, defined as EV := ∣∣v∣ if ev = 0, and EV ∈ (-1,1) otherwise, and Eω, defined as Eω :=言 if
eω = 0, and J ∈ (-1,1), respectively.
Consider now the continuously differentiable and positive definite function
V (x) := V1 + X ev +
Qe2 +ɪe2 + ɪe2 + χ ɪe2
2 ω + 2kv v + 2kω ωi + i餐} 2ki i
which satisfies WI(X) ≤ V(X) ≤ W2(X) for WI(X) := min{2, 'v, '2ω, 2kv, 2kω, 211, 2k2}kek2,
a.e.
W2 (x)	:= max{ 2 ,	2v^, ^2ω,	2kV,	2kω,	2k1,	2k2 }kxk	. ACCOrding to	Lemma 1,	V(X)	∈	V (X),
17
Under review as a conference paper at ICLR 2022
with V := ∩ξ∈∂v(e) K[e]. Since V(e) is continuously differentiable, its generalized gradi-
ent reduces to the standard gradient and thus it holds that V(X) = VVτK[e], where VV =
[ed, β, 'vev,'ωeω,十Ie,七乙,卷eι, *e2]τ. Therefore, by differentiating V and employing (14)
and (15), we obtain
V U W := — kded — kββ2 — ed©v cos β + βev —~~— — βeω + ev (UR + UL + fθ,R + fθ,L — 'vVd)
ed
+ eω (UR	-	uL	+	fθ,R — fθ,L —	'ω力d) + 厂ev'v	+	^]-'ω^ω	+ V2	丁。工
kv	kω	ki
V	i∈{1,2} i
C	......	.....	-	“、
By substituting the control and adaptation policy (12), W becomes
W = — kded — kββ2 — kv ev — kβeω + evVd'v + eωωd'ω — '1(/ + 竟)—%2 (IeVl + ∣eω I)
+ ev (Unn,R + unn,L + fθ,R + fθ,L) + eω (unn,R — unn,L + fθ,R — fθ,L)
一'v ev Vd — 'ω eω 力 d + '1(^ + 亮)+ '2(∣ev | + ∣eω |)
where ur,∩∩ and ul,∏∏ are the elements of u∩∩, i.e., Unn = [ur,∩∩, ul,∏∏]t. It is easy to conclude that
∣u∏∏,R + u∏∏,l + fθ,R + fθ,L∣ ≤ ∣u∏∏,r + fθ,R| + ∣u∏∏,l + fθ,L∣ ≤ 2∣∣u∏∏ + fθ∣∣ and ∣u∏∏,r — U∏∏,l +
fθ,R — fθ,L∣ ≤ ∣u∏∏,r + fθ,R∣ + ∣u∏∏,l + fθ,L∣ ≤ 2∣u∏∏ + fθIl and hence, in view of (17),
∣U∏∏,r + Un1,L + fθ,R + ?L∣) ≤ D := 2dded + 2dβ∣β∣ + 2d∣ev∣ + 2必∣ + 2D
∣u∏∏,R 一 u∏∏,L + fθ,R 一 Jθ,L∣)	"
Therefore, W becomes
W = — kded — kββ2 — kv e2 — kβeω — %i(e2 + eω ) — %2(∣ev ∣ + ∣eω ∣) + D(IeV ∣ + ∣eω ∣)
+ e1(ev + eω )+ '2(∣ev ∣ + ∣eω ∣)
The term D(∣ev∣ + 归ω ∣) yields
D(IeV ∣ + ∣eω ∣) =2dd ed∣ev ∣ + 2dβ ∣β∣∣ev ∣ + 2de2 + 4d∣ev∣∣eω ∣ + 2DIeV ∣ + 2dded∣eω ∣
+ 2dβ |β||eω i + 2deω + 2D ]eω i
By setting 2dded|ev l = 2ddYded ɪ-dɪ, 2ddedleω1 = 2ddYded l^^, and 2dβ lβHev l = 2dβ Yβ[βl l∣^,
2dβ∣β∣∣eω I = 2dβγβ∣β∣e and completing the squares, one obtains
D(IeV ∣ + ∣eω ∣) ≤2ddYded + 2dβ Yββ 2 + (工 +--β + 4d) (e2 + eω ) + 2D(IeV I + ∣eω ∣)
∖Yd	Yβ	)
=2ddYded + 2dβYββ2 + 21(eV + eω ) + '2 (∣ev ∣ + ∣eω ∣)
Hence, W becomes
W ≤ — (kd — 2ddYd)ed — (kβ — 2dβYβ)β2 — kve2 — kβeω — %i(e2 + eω) — %2(|ev | + ∣eω |)
+ 21(eV + eω )+ `2 (∣ev | + ∣eω |)+ `1(e2 + eω )+ `2 (∣ev | + ∣eω |)
=—(kd — 2ddγd)ed — (kβ — 2dβγβ)β2 — kveV — kβeω =： Q(X)
Therefore, it holds that Z ≤ -Q(X), for all Z ∈ V, with Q being a continuous and positive semi-
definite function in R8, since kd — 2ddγd > 0, kβ — 2dβγβ > 0. Choose now any finite r > 0 and let
C < min|@k =『Wι(e). Hence, all the conditions of Corollary 1 are satisfied and hence, all Filippov
solutions starting from x(0) ∈ Ωf := {x ∈ B(0, r) : W2(x) ≤ c} are bounded and remain in Ωf,
satisfying limt→∞ Q(e(t)) = 0.
Note that r, and hence c, can be arbitrarily large allowing and finite initial condition e(0). Moreover,
the boundedness of x implies the boundedness of e, e, ev, '1, and '2, and hence of 'ι(t) and '2(t),
for all t ∈ R>q. Finally, in view of the conditions 1 Sm φpd,1-cos φpd,2∣ < αι and Snβ < α2, one
concludes the boundedness of all closed-loop signals, for all t ≥ 0
□
18
Under review as a conference paper at ICLR 2022
The assumptions ∣p⅛,ι Sin φ - p⅛,2 Cos φ∣ < edαι, Sin β < eda2 are imposed to avoid the singularity
of ed = 0; note that β and ωd are not defined in that case. Intuitively, they imply that ed will not
be driven to zero faster than β or p)d,i sin φ - p⅛,2 Cos φ; the latter becomes zero when the vehicle's
velocity vector V aligns with the desired one p⅛. In the experiments, we tune the control gains
according to k§ 〜10kd in order to satisfy these assumptions.
C Appendix
We provide here more information on the experimental results of Section 4. All the results were
obtained on MATLAB environment using the ODE simulator. We performed the training of the
neural networks in with the pytorch library in Python environment. The neural networks consist of 4
fully connected layers of 512 neurons; each layer is followed by a batch normalization module and a
ReLU activation function. For the training we use the adam optimizer and the mean-square-error loss
function. In all cases we choose a batch size of 256, and we train until a desirable average (per batch)
loss of the order of 10-4 is achieved. All the values of the data used for the training were normalized
in [0, 1].
Regarding the robotic-arm	task, the points of interest are set as	T1	=
[-0.15,-0.475,0.675, 2,0,0]>,	T2	=	[-0.6, 0, 2.5,0, -π, -2]>,	T3	=
[-0.025,0.595,0.6, -2, 0,π]>,	and	T4	=	[-0.525, -0.55,0.28,π, 0, -2]>,	and	the
corresponding joint-angle vectors as c1	= [-0.07, -1.05, 0.45, 2.3, 1.37, -1.33]>,
c2 = [1.28, 0.35, 1.75, 0.03, 0.1, -1.22]>, c3 = [-0.08, 0.85, -0.23, 2.58, 2.09, -2.36]>,
c4 = [-0.7, -0.76, -1.05, -0.05, -3.08, 2.37]> (radians).
We set a nominal value for the time intervals as Ii = [0, 20] (seconds), and we create 150 problem
instances by varying the following attributes: firstly, we add uniformly random offsets in [-0.3, 0.3]
(radians) to the elements of all ci, and in [-2, 2] (seconds) to the right end-points of the intervals
Ii ; secondly, we add random offsets to the dynamic parameters of the robot (masses and moments
of inertia of the robot’s links and actuators) and we set a different friction and disturbance term
d(∙), leading to a different dynamic model in (5); thirdly, we set different sequences of visits to the
points ci, i ∈ {1, . . . , 4}, as dictated by φ, i.e., one trajectory might correspond to the visit sequence
((x(0), 0) → (c1 , t11 ) → (c2, t12 ) → (c3, t13 ) → (c4, t14 ), and another to ((x(0), 0) → (c3, t13 ) →
(c1, t11 ) → (c4, t14) → (c2, t12). Finally, we add uniformly random offsets in [-0.5, 0.5] to the
initial position of the robot (from the first point of the sequence), and we set its initial velocity
randomly in the interval [0, 1]6.
Regarding the dynamics (5), we use the methodology described in (Siciliano et al. (2009)) to
derive the B, C, and g terms. We set nominal link masses and moments of inertia as m =
[1, 2.5, 5.7, 3.9, 2.5, 2.5, 0.7] (kg) and I = [0.02, 0.04, 0.06, 0.05, 0.04, 0.04, 0.01] (kgm2), respec-
tively, and we add random offsets in (-mm, mm), (-2, 2) in the created instances. Regarding the
function d() used in (5); we set d(x, t) = dt(t) + df (x), where
dt = At
sin(ηιt + 2 1)
.
.
.
sin(η6t + 2 6)
df = Btx 0 X
At = diag{ Ati}i∈{1,...,6} ∈ R6×6, Ati is a random term in (0, 2mi), ηi is a random term in
(0,1), ψi is a random term in (0,2), Bt ∈ R6×36 is a random matrix taking values in (0,2m∕,
and 0 denotes the Kronecker product. We chose the control gains of the control policy (4) as
k1 = 1, k2 = 10, and k`1 = k`2 = 10.
Further experimental results are depicted in Figs. 7-11; Fig. 7a depicts the mean and standard
deviation of kev(t)k of the proposed control policy for the 50 test instances, whereas Figs. 10a depicts
the control input that results from the control policy (4) as well as the the neural-network output.
Note that the control input converges to the neural-network output, i.e., limt→∞(u(t) - unn(t)) = 0,
which can be also verified by (4) and the fact that limt→∞ ev(t) = 0. Fig. 11a depicts the mean
and standard deviation of the adaptation signals 'ι(t), '2 (t) for the 50 test instances. Finally, Fig. 9
shows timestamps of one of the test trajectories, illustrating the visit of the robot end-effector to the
points of interest at the pre-specified time stamps.
19
Under review as a conference paper at ICLR 2022
1.5
(=sq2 = )ue。日
-5
O
5	10	15	20
t [sec∖
0.7
0.6
0.2
0.1
0.5
S 0.4
Q?
0.3
P
⅛
5	10	15	20
t [sec∖
0
0
0
0
Figure 7: Mean (left) and standard deviation (right) of ev(t) for the proposed control policy.
( s/ɪ(G)^)ueφUI
—Proposed
—Non-adaptive
No-rm
252 5 1o∙5o
一s/pg (£ sueBUl
15
20
15
20
15
——Proposed
—Non-adaptive
No-un
10	15	20
t [sec]
Figure 8: Mean (top) and standard deviation (bottom) of e。(t) and eω (t) for the proposed, non-
adaptive, and no-neural-network control policies.
20
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)
(d)
Figure 9: Illustration of the execution of one of the test trajectories of the UR5 robotic arm, visiting
the points of interest at the pre-specified time stamps.
Regarding the unicycle experiments, the dynamic terms in (9) have the form
M = M1 M2
M = M2 M1
fθ (x,t) = d(x,t)
With Mi := m4r2 + (IC +?Rd ) + Io, M2 := m4r2 — (IC +?Rd ) , and where IC is the moment of
inertia of the vehicle with respect to point C (see Fig. 6), I0 is the moment of inertia of the the
wheels, and d is the distance of the between point C and the vehicle’s center of mass (p1, p2). The
term d(x, t) is chosen as in the robotic-manipulator case. The points to visit are chosen here as
c1 = [0, 0]>, c2 = [0, 2]>, c3 = [2, 0]>, c4 = [2, 2]> and Ii is chosen as Ii = [0, 20], i ∈ {1, . . . , 4}.
We derive 150 problem instances by varying the following attributes: we vary ci with random offsets
in [—0.3, 0.3] and the right end-points of Ii with random offsets in [—2, 2]; we add random offsets
to the dynamic parameters (elaborated subsequently) and the function d(x,t), and we set different
sequences of visits to the points ci ; we further vary the unicycle’s initial position from the first ci
with random offsets in [—0.3, 0.3], and its initial orientation with random offsets in [—0.25, 0.25]
(rad) from θ(0) = arctan(e2(0)/e1(0)); we further set random values in [—0.25, 0.25] (rad/s) to the
initial wheel velocities.
We set nominal values for the dynamic and geometric parameters, appearing in the inertia matrix,
as m = 28 (kg), IC = 0.1 (kgm2), I0 = 0.01 (kgm2), r = 0.01 (m), d = 0.01 (m), and we added
random offsets in (—登,22), (— IA, IA), (— I0, I0), (— 2r, 2), (— 2, 2), respectively, for the problem
instances. We chose the control gains of (12) as kd = 0.25, kβ = 1, kv = kω = k`1 = k`2 = 10,
kv = kω = 1. The non-adaptive control policy we compared our algorithm with was selected as
sin β
US = JMs Vd — kv e° + ed CoS β 一 β-------
ed
UD = MD ω d — kω eω + β
where Vd, ωd are chosen as (11) and MS, MD are static estimates of 2 M1+M2,2R MI-M2, respec-
tively. More specifically, we set a deviation of 25% in the parameters appearing in these terms to form
MS and MD . Further experimental results are depicted in Figs. 7-11; Fig. 7b depicts the mean and
standard deviation of the velocity errors ev (t), eβ (t) for the 50 test instances, showing convergence to
21
Under review as a conference paper at ICLR 2022
-UI 忌(sTl)TrB e Ul
----«1------1⅛
----1⅛ — Ui
----«6 ----⅜⅛
G100
50
∞	0
0
2_.200
息150
10	20
------Ul --------U2
------1⅛ --------«4
------1⅛ --------Ufi
60
40
60
50
40
S 20
于0
I-软R----肛I
5	10	15	20
一飞6q 一 ((4) Wjl)UB①日
t [sec]
I--l⅛n,R-%ιn,El
:EEΞΞ
0	5	10	15	20
5	10	15	20
t [sec∖
(b)
(a)
Figure 10: (a): Mean (left) and standard deviation (right) of u(t) (top) and unn(t) (bottom) for the
proposed control policy and the robotic-manipulator environment. (b): Mean and standard deviation
(right) of u(t) (top) and unn(t) (bottom) for the proposed control policy and the unicycle environment.
0	5	10	15	20
t [sec]
(a)
7
6
5
4
3
2
1
0
0	2	4	6	8	10	12	14	16	18	20
(b)
Figure 11: (a): Mean (top) and standard deviation (bottom) of 'ι(t), '2 (t) for the proposed control
policy and the robotic-manipulator environment. (b): Mean (top) and standard deviation (bottom) of
'v(t), 'ω(t), 'ι(t), '2(t) for the proposed control policy and the unicycle environment.
zero, whereas Figs. 10b depicts the control input that results from the control policy (4) as well as the
the neural-network output. Similarly to the robotic-manipulator case, the control input converges to
the neural-network output. Finally, Fig. 11b depicts the mean and standard deviation of the adaptation
signals 'v(t), 'β(t), 'ι(t), '2(t) for the 50 test instances.
Finally, regarding the pendulum environment, we consider the dynamics
q = g Sin q + U + d(t)
L
where L is the pendulum’s link length, g is the gravitational constant, and d(t) is a term of exogenous
disturbances. We created 150 instances by varying the mass and link length of the pendulum, the
external disturbances, and setting its initial position and velocity randomly in [-1, 1] (rad) and [-1, 1]
(rad/s), respectively. We selected unitary value for its nominal link length, and we added random
offsets in (-0.5, 0.5) in each instance. We set the external disturbances as d = A sin(ηt + φ), with
A, η, and φ taking random values in [0, 0.2], [0, 1], [0, 2], respectively. In contrast to the robotic
manipulator case, We assume feedback of sin q, cos q, q, and set the error e in (3) as 1 - cos(q - ∏).
Finally, we chose the control gains as k1 = k2 = 1, and k`1 = k`2 = 10.
22