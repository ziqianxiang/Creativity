Under review as a conference paper at ICLR 2022
Successive POI Recommendation via Brain-
inspired Spatiotemporal Aware Representa-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Existing approaches usually perform spatiotemporal representation in the spatial
and temporal dimensions, respectively, which isolates the spatial and temporal na-
tures of the target and leads to sub-optimal embeddings. Neuroscience research
has shown that the mammalian brain entorhinal-hippocampal system provides ef-
ficient graph representations for general knowledge. Moreover, entorhinal grid
cells present concise spatial representations, while hippocampal place cells rep-
resent perception conjunctions effectively. Thus, the entorhinal-hippocampal sys-
tem provides a novel angle for spatiotemporal representation, which inspires us to
propose the SpatioTemporal aware Embedding framework (STE) and apply it to
POIs (STEP). STEP considers two types of POI-specific representations: sequen-
tial representation and spatiotemporal conjunctive representation, learned using
sparse unlabeled data based on the proposed graph-building policies. Notably,
STEP jointly represents the spatiotemporal natures of POIs using both observa-
tions and contextual information from integrated spatiotemporal dimensions by
constructing a spatiotemporal context graph. Furthermore, we introduce a user pri-
vacy secure successive POI recommendation method using STEP, and it achieves
the state-of-the-art performance on two benchmarks. In addition, we demonstrate
the excellent performance of the STE representation approach in other spatiotem-
poral representation-centered tasks through a case study of traffic flow prediction
problem. Therefore, this work provides a novel solution to spatiotemporal aware
representation and paves a new way for spatiotemporal modeling-related tasks.
1 Introduction
With the rapid growth of location-based web services like Instagram and Yelp, there has been a
seismic shift in how people interact with locations around them. Through exploitation of Points-of-
Interest (POIs) and their contexts, successive POI recommendation can benefit users and businesses
greatly. As a core of POI information utilization, encoding POIs into vector representation space
is of great significance for advanced POI analysis and downstream applications. Existing studies
attempt to represent POI from different perspectives and collaborate with user preference modeling
to achieve recommendation. Since consecutive check-ins are usually highly correlated, naturally,
sequence modeling approaches like the Markov chain model were used to capture the check-in se-
quential characteristics of POIs (Ye et al., 2011; Liu et al., 2013; Zhang, 2014; Feng et al., 2015).
Employing tensor factorization technique, the works (Yang et al., 2017; Wang et al., 2018) modeled
target users and POIs separately by interacted features for POI recommendation. More recently,
enlightened by neural networks’ success, recurrent neural nets were remolded to represent POIs
and user preferences implicitly (Liu et al., 2016a; Zhao et al., 2019; Zhu et al., 2017). Considering
the geographical attributes of POIs, researchers have used power-law distribution, Gaussian distri-
bution, or hierarchical tiling methods to depict the geographical influence over POI distributional
features (Ye et al., 2011; Lian et al., 2014; Feng et al., 2017; Chang & Kim, 2020; Luo et al., 2020).
However, geographical modeling methods above only provide single-scale or coarse-grained man-
ually designed representations of POI geographical influences, which is deficient in capturing the
POI-specific spatial features. Also, the arbitrary modeling might even lead to over-parameterization.
While temporal dimension offers indeterminate auxiliary information for POI modeling, to utilize
1
Under review as a conference paper at ICLR 2022
the POI temporal information within the check-ins, some works using time interval, time state vari-
ables or temporal transition vectors to promote the POI representing (Zhao et al., 2019; 2016; 2017;
Li et al., 2018; Manotumruksa et al., 2018; Zhao et al., 2020). However, these methods focused
on utilizing general temporal patterns among all POIs and failed to exploit the POI-specific visiting
time patterns sufficiently. Still, the POI-specific spatiotemporal characteristics were not adequately
mined and utilized.
Inspirations. The entorhinal-hippocampal system plays a central role in the mammal cognition
architecture. The Nobel Prize-winning neuroscience research (O’keefe & Nadel, 1978) demon-
strated that entorhinal grid cells provide an effective multi-scale periodic spatial representation (Yuan
et al., 2015; Banino et al., 2018; Mai et al., 2020; Dang et al., 2021). Moreover, the entorhinal-
hippocampal system is also critical for the non-spatial inference that relies on understanding the
associations between perceptions from various perspectives (Whittington et al., 2018; Stachenfeld
et al., 2018; Whittington et al., 2020). Some promising researches cast spatial and non-spatial prob-
lems as connected graphs and point out the cells inside entorhinal-hippocampal structure provide
efficient conjunctive representation for those graphs (Stachenfeld et al., 2018; Gustafson & Daw,
2011). As the representation mechanism in the entorhinal-hippocampal system was extensively
studied, it is widely accepted that conjunctions of representations from different aspects form the
hippocampal representation for relational memory (Whittington et al., 2018; 2020; Eichenbaum,
2017; MacDonald et al., 2011; Sargolini et al., 2006). For the general spatiotemporal aware em-
bedding, various contexts can be constructed into affinity graphs for latent representation learning.
Furthermore, strategies like conjunctive-representing in entorhinal-hippocampal structure can be
translated to improve the quality of the representations (see Fig.1 left part).
In this paper, borrowing inspirations from the entorhinal-hippocampal system, we propose the Spa-
tioTemporal aware Embedding framework, namely STE and apply to POIs (STEP) for successive
POI recommendation, the model architectures are shown in Figure 1. Firstly, we build context graphs
to enable unsupervised embedding learning on sparse check-ins. Secondly, we employ a sequential
model to represent POIs from the check-in sequence perspective. Most importantly, we elaborate a
spatiotemporal model consists of a grid-cell spatial encoder and a visiting time encoder to capture
the POI-specific spatiotemporal characteristics. The spatiotemporal model learns to get the POI spa-
tiotemporal latent representations using the spatiotemporal context graph. Finally, we implement
successive POI recommendation systems based on the STEP and achieve high-performance using
simple recurrent neural networks as recommenders.
This work’s main contributions are summarized as follows:
(1)	Motivated by the graph-representing strategy of structural knowledge in the entorhinal-
hippocampal system, we solve the spatiotemporal aware embedding learning in a graph-based unsu-
pervised learning manner through specific context graph building policies, especially the spatiotem-
poral context graph, to fully exploit rich unlabeled data.
(2)	Inspired by the conjunctive representation mechanism in the entorhinal-hippocampal complex,
we present a spatiotemporal model with a grid-cell spatial encoder and a time pattern encoder to
utilize the spatiotemporal information. The conjunctive representing approach based on a unique
spatiotemporal context graph addresses the problem of previous spatiotemporal modeling methods
in which spatial and temporal information are isolated and represented separately.
(3)	We introduce a successive POI recommendation system by incorporating STEP and simple se-
quence predictors to show the feasibility of implementing specific applications based on the pro-
posed STE framework. We perform experiments on large real-world datasets to demonstrate the
effectiveness of STEP, and our method outperforms baselines according to experimental results.
(4)	Moreover, compared with classical recommendation systems, our POI-centered solution can
avoid the ethical risks of artificial intelligence, like personal data leakage, as it does not need ac-
cess to private information such as user preferences. Furthermore, our framework can be applied
to more valuable applications like wildlife preservation and urban traffic scheduling as a general
spatiotemporal aware modeling method.
2 Preliminaries
2
Under review as a conference paper at ICLR 2022
E-H system:
①	GraPh-representing
Grid cells within E-H:
② Self-position representing
Place cells within E-H:
③ COnjUnctive-representing
Observations & Contexts
Spatial & Temporal
Entorhinal-Hippocampal
Cognitive System
SpatioTemporal aware Embedding framework

Figure 1: Representing mechanisms in entorhinal-hippocampal system (E-H system for short) and
the framework of spatiotemporal aware embedding model. The proposed STE framework consists of
three essential parts: the context graph building strategies to construct simplified affinity graphs, the
spatiotemporal model to extract rich item-specific spatiotemporal features, and the sequential model
to extract sequential feature embeddings. The uniqueness about our spatiotemporal information
usage is that we represent item from spatiotemporal perspective (not isolated) using observations
and contexts conjunctively.
Given a set of POIs with corresponding coor-
dinates P = {pi}, pi = (xi, yi), a check-
in sequence is one set of continuous check-
ins of one user in one day, denoted as Sj =
{(p1, t11), ..., (pn, tnm)}. Unlike previous works,
we do not regard all check-in records of a user
as one sequence since check-ins with relatively
long intervals are not very relevant. Although
we assign notation to users for generality, the
user information is not used in the training
phase except to split sequences.
Table 1: Notations in this paper.
Notation	Definition
tij	j-th timestamp of pi
ti	Visiting time pattern matrix of pi
ei spa	Spatial vector representation of pi
ei eseq	Sequential vector representation of pi
ei est	Spatiotemporal conjunctive represen- tation of pi
ei step	STEP vector representation of pi
㊉	Tensor concatenation operation
We define context graphs as graphs that encode context information as affinity among POIs. Various
contexts in the check-in records can be easily built into graphs Gp = {Vp , Ep}, where Vp is the
set of POIs and Ep is the set of edges between adjacent POIs. The edges in context graphs repre-
sent the correlation between neighboring POIs defined by geographical distance, relative position
in check-in sequences, or spatiotemporal adjacent criterions. We summarize notations in this paper
using Table 1.
Data description. The Instagram Check-in dataset (Chang et al., 2018) was collected from Insta-
gram in New York and the data was preprocessed in the same manner as previous works (Zhao et al.,
2016; 2017). The Instagram Check-in dataset has been pre-processed when it is made public, it in-
cludes 2,216,631 check-in records at 13,187 POIs of 78,233 users. Check-in sequences are sorted by
timestamps, the first 70% are used as a training set and the remaining 30% for validation and testing.
The Gowalla dataset is a globally-collected large-scale social media dataset (Cho et al., 2011). We
eliminate users with fewer than ten check-ins and POIs accessed by fewer than ten users. Then the
check-in records are sorted according to timestamps and first 70% check-ins are used for training
and the remaining latest records for testing. We perform vivid data analyses in the Appendix due to
the space constraints.
3 SpatioTemporal aware Embedding Model of POIs
We illustrate components of STEP: the sequential model in Sec. 3.1, the spatiotemporal model
in Sec. 3.2, and state the STEP-based successive POI recommendation method in Sec. 3.3. We
adopt a simple-minded (no-parameters) edge weighting policy for constructing all context graphs.
Weight Ai,j = 1 if vertices i and j is connected, this simplification avoids the necessity of choosing
edge-weighting parameters.
3
Under review as a conference paper at ICLR 2022
3.1	Sequential Model
The sequential sub-model represents POIs using context graph Gseq . Given one POI and its context
in the check-in sequence, entry Ai,j in adjacency matrix of Gseq is 1 if pi, pj are within the same
context window. This is a common way to mine the sequential correlations of tokens like words
(Mikolov et al., 2013) or POIs (Lim et al., 2020). Our sequential model aims to predict true con-
textual POIs, i.e., connected vertices in Gseq. Intuitively, minimizing the objective function over
all target-neighbor pairs guarantees that POIs sharing similar sequential context will have shorter
distances in embedding space (Hadsell et al., 2006). To avoid the intractable summation over the
whole context space, We follow the noise contrastive sampling approach (Gutmann & Hyvarinen,
2012; Mikolov et al., 2013) to get an approximated surrogate loss
Lseq (θseq) = - X [I(Y =I)IOg σ(eSeq ∙ ejeq) + I(Y = T)IOg σ(-eSeq ∙ ejeq)],	⑴
pi ,pj ∈P
where Y = 1 if (pi, pj) is a sequential neighboring pair and Y = -1 if not, indicator I outputs 1
when the argument condition is true and otherwise 0. This unsupervised loss can also be seen as
taking expectation with respect to the distribution P(pi,pj, Y) over P, which is conditioned on the
POI sequential context graph Gseq .
3.2	Spatiotemporal Model
In this section, we illustrate the POI spatiotemporal conjunctive embedding model in detail. The
proposed spatiotemporal model is composed of two key components: a POI spatial model and a POI
visiting time encoder, an intuitive illustration can be found in Fig. 1.
3.2.1	S patial Model
The spatial sub-model takes location observations (xi, yi) and spatial context graph Gspa to produce
spatial representations.
Grid-cell encoding. Inspired by the multi-scale periodic representation of grid cells in mammals,
we formulate our POI spatial contextual encoder to use sinusoidal and cosinusoidal functions of
different scales to encode the raw locations of POIs in geographical space following previous works
Gao et al. (2019); Mai et al. (2020). Given a POI pi = (xi, yi) ∈ R2, the grid-cell model based
encoder encodes the coordinates in 2-D Euclidean space into spatial latent representations in Rdspa.
We denote the grid cell encoder based spatial embedding of POI pi as
espa = φ(ψ (xi, yi); θspa),	(2)
where
ψ(χi,yi) = ψ1(χi,yi)…㊉ ψs(χi,yi)…㊉ ΨS(χi,yi)	(3)
is concatenated multi-scale representations of 6S dimensions, S denotes the number of grid scales
and φ represents fully connected non-linear layers. Considering three unit vectors a1=[1, 0]T,
a2=[-1∕2, √3∕2]t, a3=[-l∕2, -√3∕2]t ∈ R2, at each frequency, position codes
ψs(χi,yi) = ψ1 ㊉ ψ2 ㊉ ψ3	(4)
are computed via
Ψk(Xi,yi)=kos([xi,yi] ∙ a ), sin([xi,yi]∙ a )1 ,k ∈{1, 2, 3}	(5)
ρλmin	ρλmin
and P = (λmaχ∕λmin)s"ST). λmin and λmaχ are the minimum and maximum scale values, here
we use S = 64 following the previous work (Mai et al., 2020) and set λmax = 1km, λmin = 0.1km.
Spatial-neighboring definition. We project the coordinates in the geographical coordinate system
WGS84 to the projection coordinate system NAD27 to get locations of POIs in R2 . For each en-
try Ai,j in adjacency matrix of spatial context graph Gspa, we assign Ai,j using the geographical
distances. Specifically, we computed the geographical distances between POIs and construct an
undirected spatial context graph Gspa with uniform edges among the top-ten closest POIs (nearest
neighbors policy). As the grid cell encoder can handle geographical distributions at different scales
4
Under review as a conference paper at ICLR 2022
(Mai et al., 2020), we do not use a specific radius (-neighborhoods policy) to filter the neighboring
POIs to fully exploit the multi-scale representation capability. The spatial graph construction pro-
cess is related to Lim et al. (2020), in which the edges are weighted according to average distances
to enable graph attention computations.
Given a target POI pi , neighboring contextual POI set Ps+pa and negative set Ps-pa sampled from
Gspa , the unsupervised embedding learning can simply be maximizing the log-likelihood of ob-
serving the true context POIs. We can formulate this target with negative sampling via a general
objective function:
O(Ctx) = - x x	logσ(eCtx ∙ eCtx)+ Kk X logσ(-eCtx ∙ &),	(6)
pi∈Ppj∈Pc+tx	pk∈Pc-tx
where ctx indicates the context graph type and ctx ∈ {seq, spa, st} in this work, σ is the sigmoid
function and K denotes the number of samples in negative sample set Pc-tx . Following Eq.6, the
loss function for the spatial context embedding model is:
Lspa (θspa) = O(spa).	(7)
3.2.2 Spatiotemporal Context Graph Construction
For constructing spatiotemporal context graph Gst , we want to
mine the item-specific spatiotemporal conjunctions, so for each
entry Ai,j of the adjacency matrix of Gst, we assign Ai,j = 1
following the hierarchy of neighboring timestamps → temporal
neighboring → spatiotemporal neghboring:
1.	Neighboring timestamps. Given an arbitrary timestamp pair
(t1, t2), time interval ∆t , |t1 - t2| and ∆wkd , |wkd (t1) -
wkd(t2)| where wkd (t) = 1 ift is weekend else 0. For one time t,
its temporal-neighboring timestamps are those within the neigh-
borhood window and satisfy ∆wkd = 0, as shown in Fig. 2. h is
a hyper-parameter indicates temporal context window width and
h ∈ (0, 24) hours.
2.	POI temporal neighboring. POI pi and pj with corre-
sponding visiting timestamp sets Ti = {ti1, ti2, . . . } and Tj =
{tj1 , tj2 , . . . }. The number of neighboring timestamp pairs (ti, tj )
> m, where ti ∈ Ti , tj ∈ Tj , m is a threshold.
3.	POI spatiotemporal neighboring. If pi , pj are spatial and
temporal neighboring, they are spatiotemporal neighboring.
Uoeuəe5abp—b--
00:00
02:00
04:00
06:00
08:00
10:00
12:00
14:00
16:00
18:00
20:00
22:00
t2-24-h
tι-24 J
t2-24+h
t2-24-h t2-h t2+24-h
t2-24 It2	t2+24
t2-24+h t2+h t2+24+h
Inter-day Dimension
Figure 2: Temporal neighbor-
ing examples with h = 2.
For t1 , some times are ex-
cluded from the temporal neigh-
borhood window as ∆wkd 6= 0.
。廿A专
It is redundant for individual POI-specific temporal modeling since solely relying on time informa-
tion, we are not able to recommend reasonable candidate POIs (visits may take place contemporarily
all over the world). Thus, time is regarded as a supplementary dimension of basic spatial infor-
mation, our spatiotemporal context graph provides an effective way to combine the POI-specific
temporal and geographical information.
Timestamps
3.2.3 Visiting Time Pattern Encoding
We develop a visiting time pattern encoding method to tensorise the tem-
poral observations (visiting records in timestamps t) to provide observa-
tion inputs for spatiotemporal model. Unlike previous works (Liu et al.,
2016a; Zhao et al., 2019; Zhu et al., 2017; Zhao et al., 2016), we fo-
cus on the POI-specific temporal patterns rather than general temporal
characteristics among all timestamps. Compared with previously used
time interval-based or hard-coded methods, our encoding scheme can
tensorise the item-specific temporal information more precisely and able
to provide a reliable decision basis for spatiotemporal modeling.
Visiting time encoding. Since the visiting time patterns are relatively
High
Low
Accumulation
04-01-19-04
06-14-17-38
11-20-02-21
09-22-12-45
t0
Normalization Augmentation
[0,1T …7]
[0, 0.93, 0.13,…, 0.05]	W
t
AOUenbe.IL ωωφοο<
Figure 3: Schematic of
the visiting time encod-
*
ing process.
5
Under review as a conference paper at ICLR 2022
stable on the scale of year but fickle on the hour and the date scale, in our encoding scheme, visits
are counted by the check-in timestamps to raw matrices t0 ∈ R24×366. Then, the raw matrices t0 are
normalized and applied Gaussian kernel for smoothing, this process also reasonably augment the
POI accessing time data. The visiting time encoding process is shown schematically in Fig. 3. The
final matrix representation t = smooth(norm(t0)) retains the fine-grained check-in time patterns as
well as rough item-specific visiting time features.
3.2.4 S patiotemporal Conjunctive Embedding Learning
We formulate the spatiotemporal conjunctive representation as:
eSt = φθst (φθtime(ti), eSpa)㊉ eSpa,	(8)
where φ indicates fully-connected layers. Follow the formulation in Eq.6, we implement the spa-
tiotemporal conjunctive representation learning by minimizing:
L(θtime, θSt) = O(st).	(9)
We sample PS+t and PS-t from GSt, where PS+t is the set of spatiotemporal-neighboring POIs whereas
PS-t is the set of negative POIs.
During the optimization procedure, the spatial model is jointly optimized as a sub-model of spa-
tiotemporal model, the full objective of the spatiotemporal model is
LSt
L(θtime, θSt) + λSpa LSpa (θSpa ),
(10)
λSpa is a weighting factor for preserving the spatial context information during the spatiotemporal
modeling. We first sample a batch of spatial context GSpa to optimize the spatial context loss LSpa
to preserve geographical context. Next we sample a batch of spatiotemporal context GSt to optimize
the spatiotemporal loss LSt to preserve the spatiotemporal context. We repeat above procedures for
Io and Ii = Io∕λspa iterations respectively to approximate the balancing factor λspa. We update
all parameters {θSpa , θtime, θSt } of spatiotemporal model in iterations until the overall loss LSt
converges.
3.3 Successive POI Recommendation with STEP
We present the STEP (STE of POIs)-based recommendation method in detail in this section. Taking
the spatiotemporal data as input, we construct context graphs G and feed the observations (loca-
tions and time patterns) into the STEP model to perform embedding learning. The embeddings
are smoothed according to corresponding context graphs to preserve contextual information. Then,
POI vector representations (embeddings) eseq , est are merged as spatiotemporal aware embedding
estep and fed into the recommender to generate an estimated embedding e. Specifically, We use
concatenation
eStep = eSeq ㊉ eSt	(II)
to merge the tWo sequential and spatiotemporal embeddings. This merging policy can preserve
information from different spaces Without extra-parameters and not requires the embeddings to be
in the same dimension (e.g. dSt = dSeq ) thus provides more flexibility. We adopt tWo-layer recurrent
netWorks as the recommender model.
Embedding model optimizing. Parameters in the STEP embedding model are optimized according
to corresponding loss function L(θ*) = L* + α∣∣θ*∣∣2, where L* ∈ {L§eq, L§t}, θ* ∈ {0Seq, θSt},
ΘSt = {θSpa, θtime, θSt}. α is the Weighting factor of the 2-norm regularizer.
Recommender model optimizing. The predictor is then optimized with the pre-trained STEP em-
bedding model for the next POI recommendation task. During the training phase, given a n-length
check-in sequence Sj, we can get corresponding STEP embedding series of POIs {e(S1te)p, . . . , e(Sgtet)p},
the last POI is regarded as the recommendation target. The target of the recommender is to predict
latent representation e§tep similar to the embedding of true successive POI eSgtP, formally described
as:
arg max
θpred
sim
Sj∈S
e
(gt)
Step
(12)
6
Under review as a conference paper at ICLR 2022
Table 2: Comparisons with baselines on two datasets, we mark best values with bold fonts and
underline the SUboPtimal ones. CAPE-based methods are not applicable on Gowalla (no tweets were
provided) and We do not report results of some methods on Instagram Check-in as they cannot be
reproduced faithfully. ↑: with user preference consideration, ¢: using semantic content information.
Dataset	Instagram Check-in	Gowalla
Method\METRIC	HIT@1	HIT@5	HIT@10	MRR	HIT@1	HIT@5	HIT@10	MRR
Random+GRU	0.1197	0.2207	0.2726	0.1792	0.0715	0.0725	0.0732	0.0727
Random+LSTM	0.1207	0.2225	0.2751	0.1805	0.0722	0.0736	0.0749	0.0737
Skip-Gram+GRU	0.1356	0.2419	0.3040	0.1919	0.1090	0.2111	0.2617	0.1612
Skip-Gram+LSTM	0.1318	0.2344	0.2984	0.1875	0.1085	0.2101	0.2585	0.1594
CAPE+GRU*	0.1390	0.2433	0.3079	0.1953	N/A	N/A	N/A	N/A
CAPE+LSTM*	0.1381	0.2412	0.3054	0.1939	N/A	N/A	N/A	N/A
Geo+GRU	0.1619	0.2616	0.3248	0.2093	0.1267	0.2309	0.2834	0.1684
Geo+LSTM	0.1622	0.2594	0.3128	0.1875	0.1233	0.2296	0.2811	0.1701
ST-RNNt	0.1054	0.2019	0.2426	0.1681	0.0519	0.0953	0.1304	0.2187
STGNt									0.0256	0.0784	0.1144	0.0590
STGCNt									0.0424	0.1134	0.1625	0.0842
LSTPMt	0.1261	0.2134	0.3121	0.1957	0.1468	0.2506	0.2983	0.1998
STP-DGArt									0.1344	0.2414	0.2653	0.1856
STP-UDGATt									0.1475	0.2911	0.3285	0.2130
STEP+RNN	0.2458	0.3170	0.3502	0.2822	0.1495	0.2878	0.3634	0.2222
STEP+GRU	02467	0.3057	0.3336	0.2781	0.1490	0.2912	0.3636	0.2233
STEP+LSTM	0.2454	0.3204	0.3556	0.2835	0.1539	0.2968	0.3728	0.2282
The objective function of recommender is:
Lpred(θpred) = - X log σ0 (Sim&*,Wstep)) - log( X σ0(sim(eStep, estep))) , (13)
Sj ∈S	pi ∈P
where σ0 = exp(LeakyReLU(∙)) and sim(∙, ∙) = |斯词|. During the testing phase, We compute
the cosine similarity scores to rank the candidate POIs to generate recommendation lists.
4	Experiments	Table 3: Context graphs statistics
on two POI datasets.
We perform the successive POI recommendation task on the AVERAGE VALUE	Ins.	Gow.
Instagram Check-in dataset, Gowalla dataset, and the traffic Records per POI	168.1	34.2
flow forecasting task on TaxiBJ15 and TaxiBJ.	Eseq per POI	66.2	35.5
Espa per POI	10.0	10.0
4.1 Successive POI Recommendation Task	Est Per POI	0.997	0.596
Metrics. During the system inferencing phase, the recommendation system recommends a POI list
according to the estimated scores of candidate POIs for every trial sequence. We apply widely-used
metrics HIT@K (if the ground truth is within the top-k of the list, a score of 1 is awarded, else
0), k = 1, 5, 10 and MRR (Mean Reciprocal Rank) for evaluation. These metrics reflect different
aspects of the recommendation lists, HIT@K measures the rate of valid recommendation among all
trials, whereas MRR scores the quality of the entire recommendation list.
Hyper-parameter settings. We set the hyper-parameters of our proposed method to the following
default values. We set context window size in the POI sequential model to 2 and adopt h = 2,
m = 11 for building Gst. We utilize Adam optimizer with batch size 512, β1 = 0.9, β2 = 0.999 and
set the initial learning rate to 0.001 followed by a reduce-on-plateau decay policy, the decay factor
is 0.1 during the training. Weighting factors α, λspa are set to 1 × 10-4, 0.2 and the embedding
dimensions {dseq , dspa , dst } are set to {32, 64, 96}.
We compare the STEP-based successive POI recommendation method with representative methods:
Embedding-recommender methods. We choose six methods consisting of three embedding mod-
els and two recommenders. For the embedding model, we use the following embedding models.
(1) Random, (2) Skip-Gram (Liu et al., 2016b), (3) CAPE (Chang et al., 2018) and (4) Geo (Mai
7
Under review as a conference paper at ICLR 2022
Instagram	Gowalla
)%( .snI no1@TIH
.snI no RRM
Figure 4: Effect of hyper-parameters h, m, λspa on Instagram Check-in and Gowalla datasets. The
means and standard deviations are computed over five runs using different random seeds.
.woG no RRM
)%( .woG no 1@TI
woG no RR
,∙iuuclcl≡
)%( .woG no1@TI
woG no RR
8) 3∣J-IJo-atH
8) 3∣J-IJo-atH
SU-IJo EE≡

et al., 2020). For the recommender model, two-layer networks based on (1) GRU unit (Merri &
Fellow, 2014) and (2) LSTM unit (Hochreiter & Schmidhuber, 1997) are used. One-stage meth-
ods. We choose representative one-stage methods as baselines. (1) ST-RNN (Liu et al., 2016a). (2)
STGN (Zhao et al., 2019), a LSTM variant which models visit preferences with time and distance
considerations, and the improved variant STGCN. (3) LSTPM (Sun et al., 2020) is a LSTM-based
method. (4) STP-DGAT and STP-UDGAT (Lim et al., 2020) are spatial-temporal-preference user
dimensional graph attention networks.
Comparison results. According to results in Table 5, our method outperforms the baselines by
significant margins on both datasets, and the gains in recommendation accuracy are especially sub-
stantial on the Instagram dataset with rich temporal information (according to Table 3, POIs in
Gowalla have less visiting timestamps). The one-stage recurrent network-based methods, such as
ST-RNN, LSTPM surpassing basic embedding-based methods by more sufficient exploitation of user-
preference spatiotemporal properties. However, these methods remain noncompetitive with STEP-
based ones, although the STEP stands without user preference consideration. The advantageous per-
formance of our method over the competitors can be attributed to its efficient use of the item-specific
spatiotemporal nature. We observe significant performance improvements of the STEP-based meth-
ods in terms of MRR, indicating the STEP-based methods provide better candidates lists on both
datasets, benefit from the efficiency of the proposed spatiotemporal aware embedding model. The
usage of LSTM units in recommender slightly improves the performance compared with basic RNN
cells because of their advantages in gate functions of recurrent connections. Moreover, the basic
RNN recurrent net equipped with STEP embedding can also outperform one-stage SOTAs. This
also proves the effectiveness of our brain-inspired spatiotemporal aware embedding model.
Effect of hyper-parameters. We study the effect of newly-introduced hyper-parameters h, m, λspa ,
and report the results using HIT@1 and MRR in Fig.4. h andm control the sparsity of the spatiotem-
poral context graph and λspa regulates the importance of the spatial context smoothness term in the
spatiotemporal model objective function. We alter h to build spatiotemporal context graphs with de-
creasing sparsity as larger h corresponds to coarse temporal-neighboring condition. The increasing
h within a certain range results in performance improvements but appears detrimental to the precise
top-1 recommendation. The best performance (determined by MRR) is obtained when h = 2. The
choice of h is task-related, according to our results on two dataset, h = 2 can be a good initial value
for POI recommendation according to our results. This value can be further adjusted for different
application scenarios or datasets to build spatiotemporal context graphs with desired sparsity. We set
m from 3 to 15, larger m leads to a sparser spatiotemporal context graph. We observe the performance
slightly changes after increasing the threshold m, when m = 11, the best performance is obtained.
We also investigate the effect of the balancing factor λspa for spatiotemporal model training, the
recommendation system achieves the best performance when λspa = 0.2 and further increases only
bring minor improvements, thus we select 0.2 as a default value in this work, this also helps reduce
unnecessary iterations during the model training.
Ablation study. We study the effectiveness of STEP modules by performing successive POI rec-
ommendation task with LSTM recommender, the method using standard STEP embedding model is
referred as Full.
After the removal of (A) POI-specific time information processing module, the spatiotemporal
model degenerates to a spatial model. According to Table 4, the method performs worse with-
out (A) on both datasets. According to Table 3, POIs in Gowalla have sparser specific obser-
vation ts and contextual information from Gst . This results in more significant performance
8
Under review as a conference paper at ICLR 2022
Query
Gr Grand Central Oyster Bar
midtown, famous seafood spot
Chrysler Building
midtown, landmarks &
historical buildings
A Metropolitan Oval
bronx, park
φ Grand Hyatt New York
。 midtown, hotels, venues &
event spaces
1* Luke's Lobster
midtown, seafood
“Grand Central Terminal
midtown, landmarks & historical
buildings, train stations
“Bea
midtown, cocktail bars
A La Biblioteca
midtown, night club, lounges
AHUntington Harbor
huntington, harbor
Top 5 retrieved POIs
τ*^ Grand Hyatt New York
midtown, hotels, venues &
event spaces
A Ann Loftus Playground
upper manhattan, park
A The Campbell
midtown, cocktail bars,
lounges
S Sushi Yasuda
midtown, japanese food
M MTA 42nd St. Grand Central 1* Num Pang Kitchen
midtown, public transportation midtown, sandwiches,
cambodian
R Roosevelt Island Sports Park 1* Unix Gallery
roosevelt island, park	upper east side, art gallery
“Tudor City Greens
midtown, parks
“ InterCOntinental N.Y. Barclay
midtown, hotels
1* Momosan Ramen&Sake
midtown, ramen, tapas, small
plates
G Grand Central Oyster Bar
midtown, famous seafood spot
Figure 5: Retrieval results on Instagram dataset Top 5 candidates are placed from left to right.
Green and red flag marks represent positive and negative POIs determined by both opening hours
and geographical position. Under names of those POIs, we list their region and representative tags
collected from websites with bold fonts.
improvements on the Instagram set than on
Gowalla. We replace POI visiting time pattern ma-
trices t (applying Sec.3.2.3) with random-initialized
matrices in R24×366 . We note that the use of (B)
POI visiting time encoder improves the recommen-
dation performance on both datasets, and the im-
provement are positively correlated with the tempo-
ral information abundance of the dataset. We use a
one-layer neural network location encoder ψ0(x, y)
to replace (C) grid-cell encoder in the STEP to
demonstrate its effectiveness. Results in Table 4
demonstrate that the grid-cell encoder improves the
quality of STEP representation and leads to better
successive POI recommendation performances on
both datasets. We observe noticeable performance
drops after the removal of (D) spatial embedding
preserving in Table 4 since the spatiotemporal con-
junctive representation integrate spatial and tempo-
ral attributes at the cost of spatial information loss
(due to the dimensional reduction). The use of (D)
Table 4: Effectiveness of using (A) tempo-
ral information, (B) visiting time encoder,
(C) grid-cell encoder, (D) spatial embedding
preserving and (E) the spatiotemporal model.
We mark the best ones with bold fonts and
underline the SUboPtimal ones.
IHrT@1HrT@5HrT@10 MRR
Full
w/o (A)
w/o (B)
w/o (C)
w/o (D)
only (E)
Full
w/o (A)
w/o (B)
w/o (C)
w/o (D)
only (E)
0.2454
0.2433
0.2452
0.2399
0.2298
0.2466
0.3204
0.2544
03151
0.3037
0.2531
0.2840
0.3556 0.2835
0.2607 0.2504
0.3442 0.2798
0.3305 0.2727
0.2674 0.2451
0.3021 0.2664
0.1539 0.2968
0.1461 0.2825
0.1509 0.2921
0.1006 0.2169
0.0973 0.2117
0.1252 0.2142
0.3728 0.2282
0.3540 0.2174
0.3664 0.2248
0.2922 0.1657
0.2866 0.1610
0.2519 0.1683
sPatial embedding information Preserving alleviate this Problem to a certain extent.
A simPle LSTM recommender can achieve comPetitive recommendation Performance without
check-in sequential information consideration (Table 4 only(E)), demonstrating the effectiveness
of the sPatiotemPoral model in STEP. As visiting sequential information Provides relatively coarse-
grained POr dePictions, the removal of the sequential model even leads to HrT@1 imProvement on
the rnstagram dataset. Also, the metric fallen on Gowalla (-18.6%, -27.8%, -32.4%, -26.2%) is more
significant than those on rnstagram set (+3.1%, -11.4%, -15.0%, -6.0%), exactly oPPosed to their
temPoral information abundance, quantitatively evaluated by average timestamPs Per POr.
4.2 Performing Traffic Flow Forecasting task with STE
To demonstrate the generalizability of STE, we Perform traffic flow forecasting task with the
ProPosed sPatiotemPoral embedding methods. The relevant content is Presented in detail in AP-
Pendix.A.
5 Conclusion and Discussion
rn this PaPer, we ProPose the sPatiotemPoral aware embedding framework STE and aPPly it to POrs
(STEP). To the best of our knowledge, this is the first work that translating entorhinal-hiPPocamPal
rePresenting mechanisms to the sPatiotemPoral embedding. rnsPired by the graPh-rePresenting Pol-
icy in brain entorhinal-hiPPocamPal system, STEP caPtures sequential and sPatiotemPoral rePre-
sentation from unlabeled sParse data through context graPh building and graPh-based embedding
learning. Moreover, STEP Provides a highly efficient sPatiotemPoral model motivated by grid cells’
multi-scale sPatial rePresentation and Place cells’ conjunctive rePresentation, which overcomes the
Problem caused by frequently used seParate-rePresenting. STEP-based successive POr recommen-
dation method outPerforms baselines and SOTAs on two real datasets without user Preference in-
vasion. Furthermore, this work Presents a Practical framework for effective sPatiotemPoral aware
modeling of general items, enabling more valuable sPatiotemPoral-related tasks such as wildlife
Preservation and urban traffic management.
9
Under review as a conference paper at ICLR 2022
References
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski,
Alexander Pritzel, Martin J. Chadwick, Thomas Degris, Joseph Modayil, Greg Wayne, Hubert
Soyer, Fabio Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie
Beattie, Stig Petersen, Amir Sadik, Stephen Gaffney, Helen King, Koray Kavukcuoglu, Demis
Hassabis, Raia Hadsell, and Dharshan Kumaran. Vector-based navigation using grid-like repre-
Sentations in artificial agents. Nature, 557(7705):429-433, 2018.
Buru Chang and Seoyoon Kim. Learning Graph-Based Geographical Latent Representation for
Point-of-Interest Recommendation. In Proceedings of the ACM International Conference on In-
formation & Knowledge Management, pp. 135-144, 2020.
Buru Chang, Yonggyu Park, Donghyeon Park, Seongsoon Kim, and Jaewoo Kang. Content-aware
hierarchical point-of-interest embedding model for successive POI recommendation. In Proceed-
ings of the International Joint Conference on Artificial Intelligence, pp. 3301-3307, 2018.
Eunjoon Cho, Seth A Myers, and Jure Leskovec. Friendship and mobility: user movement in
location-based social networks. In Proceedings of the ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 1082-1090, 2011.
Suogui Dang, Yining Wu, Rui Yan, and Huajin Tang. Why grid cells function as a metric for space.
Neural Networks, 142:128-137, 2021.
Howard Eichenbaum. On the integration of space, time, and memory. Neuron, 95(5):1007-1018,
2017.
Shanshan Feng, Xutao Li, Yifeng Zeng, Gao Cong, Yeow Meng, and Chee Quan. Personalized
Ranking Metric Embedding for Next New POI Recommendation. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence, 2015.
Shanshan Feng, Gao Cong, Bo An, and Yeow Meng Chee. POI2Vec : Geographical Latent Rep-
resentation for Predicting Future Visitors. In Proceedings of the AAAI Conference on Artificial
Intelligence, pp. 102-108, 2017.
Ruiqi Gao, Jianwen Xie, and Ying, Nian Zhu, Songchunand Wu. Learning Grid Cells as Vector Rep-
resentation of Self-Position Coupled with Matrix Representation of Self-Motion. In Proceedings
of the International Conference on Learning Representations, 2019.
Nicholas J Gustafson and Nathaniel D Daw. Grid Cells , Place Cells , and Geodesic Generalization
for Spatial Reinforcement Learning. PLoS Computational Biology, 7(10):1-14, 2011.
Michael U Gutmann and AaPo Hyvarinen. Noise-contrastive estimation of unnormalized statistical
models, with applications to natural image statistics. Journal of Machine Learning Research, 13
(2), 2012.
Raia Hadsell, Sumit ChoPra, and Yann LeCun. Dimensionality reduction by learning an invariant
maPPing. In Proceedings of the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR), volume 2, PP. 1735-1742, 2006.
Minh X Hoang, Yu Zheng, and Ambuj K Singh. Fccf: forecasting citywide crowd flows based on
big data. In ACM SIGSPATIAL, 2016.
Sepp Hochreiter and Jurgen SChmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Ranzhen Li, Yanyan Shen, and Yanmin Zhu. Next Point-of-Interest Recommendation with Temporal
and Multi-level Context Attention. Proceedings of the IEEE International Conference on Data
Mining, pp. 1110-1115, 2018.
Defu Lian, Cong Zhao, Xing Xie, Guangzhong Sun, Enhong Chen, and Yong Rui. GeoMF : Joint
Geographical Modeling and Matrix Factorization for Point-of-Interest Recommendation. In Pro-
ceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Min-
ing, pp. 831-840, 2014.
10
Under review as a conference paper at ICLR 2022
Nicholas Lim, Bryan Hooi, and Xueou Wang. STP-UDGAT : Spatial-Temporal-Preference User
Dimensional Graph Attention Network for Next POI Recommendation. In Proceedings of the
ACM International Conference on Information & Knowledge Management, pp. 845-854, 2020.
Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Predicting the next location: A recurrent model
with spatial and temporal contexts. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, pp. 194-200, 2016a.
Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. Personalized Point-of-Interest Recommenda-
tion by Mining Users ’ Preference Transition. In Proceedings of the ACM International Confer-
ence on Information & Knowledge Management, pp. 733-738, 2013.
Xin Liu, Yong Liu, and Xiaoli Li. Exploring the Context of Locations for Personalized Location
Recommendations. In Proceedings of the International Joint Conference on Artificial Intelli-
gence, pp. 1188-1194, 2016b.
Hui Luo, Jingbo Zhou, Zhifeng Bao, and Shuangli Li. Spatial Object Recommendation with Hints
: When Spatial Granularity Matters. In Proceedings of the International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 781-790, 2020.
Christopher J MacDonald, Kyle Q Lepage, Uri T Eden, and Howard Eichenbaum. Hippocampal
?time cells? bridge the gap in memory for discontiguous events. Neuron, 71(4):737-749, 2011.
Gengchen Mai, Janowicz Krzysztof, Yan Bo, Zhu Rui, Cai Ling, and Ni Lao. Multi-Scale Rep-
resentation Learning for Spatial Feature Distributions using Grid Cells. In Proceedings of the
International Conference on Learning Representations, 2020.
Jarana Manotumruksa, Craig Macdonald, and Iadh Ounis. A Contextual Attention Recurrent Ar-
chitecture for Context-Aware Venue Recommendation. In Proceedings of the International ACM
SIGIR Conference on Research and Development in Information Retrieval, pp. 555-564, 2018.
Bart Van Merri and Cifar Senior Fellow. Learning Phrase Representations using RNN Encoder
- Decoder for Statistical Machine Translation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pp. 1724-1734, 2014.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words
and Phrases and their Compositionality. In Proceedings of the International Conference on Neural
Information Processing Systems, 2013.
John O’keefe and Lynn Nadel. The hippocampus as a cognitive map. Oxford: Clarendon Press,
1978.
Francesca Sargolini, Marianne Fyhn, Torkel Hafting, Bruce L McNaughton, Menno P Witter, May-
Britt Moser, and Edvard I Moser. Conjunctive representation of position, direction, and velocity
in entorhinal cortex. Science, 312(5774):758-762, 2006.
Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hippocampus as a
predictive map. Nature Neuroscience, 20(11):1643-1653, 2018.
Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, and Hongzhi Yin. Where
to Go Next: Modeling Long- and Short-Term User Preferences for Point-of-Interest Recommen-
dation. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 214-221, 2020.
Yongxin Tong, Yuqiang Chen, Zimu Zhou, Lei Chen, Jie Wang, Qiang Yang, Jieping Ye, and
Weifeng Lv. The simpler the better: a unified approach to predicting original taxi demands based
on large-scale online platforms. In Proceedings of the 23rd ACM SIGKDD international confer-
ence on knowledge discovery and data mining, pp. 1653-1662, 2017.
Hao Wang, Huawei Shen, Wentao Ouyang, and Xueqi Cheng. Exploiting POI-specific geograph-
ical influence for point-of-interest recommendation. In Proceedings of the International Joint
Conference on Artificial Intelligence, pp. 3877-3883, 2018.
11
Under review as a conference paper at ICLR 2022
James C.R. Whittington, Timothy H. Muller, Caswell Barry, Shirley Mark, and Timothy E.J.
Behrens. Generalisation of structural knowledge in the hippocampal-entorhinal system. In Pro-
ceedings of the International Conference on Neural Information Processing Systems, 2018.
James C.R. Whittington, Timothy H. Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil
Burgess, and Timothy E.J. Behrens. The Tolman-Eichenbaum Machine: Unifying Space and
Relational Memory through Generalization in the Hippocampal Formation. Cell, 183(5):1249-
1263.e23, 2020.
SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Ad-
vances in neural information processing systems, pp. 802-810, 2015.
Carl Yang, Lanxiao Bai, Chao Zhang, Quan Yuan, and Jiawei Han. Bridging collaborative filtering
and semi-supervised learning: A neural approach for POI recommendation. In Proceedings of the
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1245-
1254, 2017.
Mao Ye, Peifeng Yin, Wang-chien Lee, and Dik-lun Lee. Exploiting Geographical Influence for
Collaborative Point-of-Interest Recommendation. In Proceedings of the International ACM SIGIR
Conference on Research and Development in Information Retrieval, pp. 325-334, 2011.
Miaolong Yuan, Bo Tian, Vui Ann Shim, Huajin Tang, and Haizhou Li. An entorhinal-hippocampal
model for simultaneous cognitive map building. In Proceedings of the AAAI Conference on Arti-
ficial Intelligence, volume 29, pp. 582-596, 2015.
Jia-dong Zhang. LORE : Exploiting Sequential Influence for Location Recommendations. In Pro-
ceedings of the ACM SIGSPATIAL International Conference on Advances in Geographic Infor-
mation Systems, pp. 103-112, 2014.
Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, and Xiuwen Yi. Dnn-based prediction model for
spatio-temporal data. In ACM SIGSPATIAL, 2016.
Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide
crowd flows prediction. In AAAI, 2017.
Pengpeng Zhao, Haifeng Zhu, Yanchi Liu, Jiajie Xu, Zhixu Li, Fuzhen Zhuang, Victor S Sheng, and
Xiaofang Zhou. Where to Go Next : A Spatio-Temporal Gated Network for Next POI Recom-
mendation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.
Pengpeng Zhao, Haifeng Zhu, Yanchi Liu, Zhixu Li, Jiajie Xu, and Victor S. Sheng. Where to Go
Next: A Spatio-Temporal Gated Network for Next POI Recommendation. IEEE Transactions on
Knowledge and Data Engineering, 4347(c):1-13, 2020.
Shenglin Zhao, Tong Zhao, Haiqin Yang, Michael R Lyu, and Irwin King. STELLAR : Spatial-
Temporal Latent Ranking for Successive Point-of-Interest Recommendation. In Proceedings of
the AAAI Conference on Artificial Intelligence, pp. 315-321, 2016.
Shenglin Zhao, Tong Zhao, Irwin King, and Michael R. Lyu. Geo-Temporal Sequential Embedding
Rank for Point-of-interest Recommendation. In Proceedings of the International World Wide Web
Conference, pp. 153-162, 2017.
Yu Zhu, Hao Li, Yikang Liao, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai. What to do
next: Modeling user behaviors by time-lstm. In Proceedings of the International Joint Conference
on Artificial Intelligence, pp. 3602-3608, 2017.
12
Under review as a conference paper at ICLR 2022
A Case Study: Traffic Flow Forecasting with STE
As an essential basis of traffic scheduling, accurate traffic flow forecasting is of great significance.
In this part, we perform traffic flow forecasting with the proposed STE, which is abbreviated as
STE-TG (STE of Traffic Grid).
A.1 Problem Formulation
Figure 6: An illustration of (a) grid map partition and (b) flow of a grid.
(b)
Following previous works (Hoang et al., 2016; Zhang et al., 2016), we partition a city into an M × N
traffic grid map based on the longitude and latitude where a traffic grid represents a geographical
region. For a grid (m, n) that lies at the mth row and the nth column, two types of traffic flows at
kth timestamp are considered, namely inflow and outflow, defined as
xikn,m,n = X |{i > 1 | gi-1 ∈/ (m, n) ∧gi ∈ (m, n)}|	(14)
Trk∈P
xokut,m,n = X |{i ≥ 1 | gi ∈ (m, n) ∧ gi+1 ∈/ (m, n)}|	(15)
Trk∈P
where T rk means the trajectory at the kth timestamps, gi indicates the geographical coordinate,
gi ∈ (m, n) means point gi lies within grid (m, n) and | ∙ | means the cardinality of a set. At
timestamp k, the traffic flow of grid (m, n) is represented as Xkm,n = (xikn,m,n , xokut,m,n). The
goad of traffic flow forecasting is: given Xk, k ∈ {0, 1, ..., t - 1}, predict Xt.
A.2 Methodology
As illustrated in Fig.1 in the main text, the input of STE-TG is composed of observations (coordi-
nates and visiting time patterns) and contexts (context graphs). Unlike STEP, as there is no sequential
traffic information, the sequential model is not applicable.
STE usage Following Sec.3.2.1 in the main body of the article, we use spatial observation and
build a spatial context graph in the same way. Specifically, same hyper-parameter settings of
S, λmin∕max is adopted in grid-cell encoder, and top-10 policy is used for building GSpa. Also,
the embedding objective of the spatial sub-model is still the term in Eq.7. Different from POIs, the
timestamps of the traffic grid flow data are collected using a standard time interval. Thus both the
encoding of visiting time patterns and the construction of spatiotemporal context graph could be
simplified. Notably, we generate the visiting time pattern matrix as the temporal observation input
for the spatiotemporal model in the same way we did for POIs, and since the timestamp is standard,
we skipped the accumulation step in Fig.3. For constructing GSt, we define the grid temporal ad-
jacency using a specified distance function d(∙, ∙) which is calculated by the normalized traffic flow
difference value of two traffic grids.
1t
d((m,n), (m0,n0)) 4 IE [abs (Xkn，m，n - xRm'n') + abs (Xkut，m，n - XS)] . (16)
t k=1
13
Under review as a conference paper at ICLR 2022
Figure 7: Visualization of predicted results and ground truth traffic flow on TaxiBJ dataset. Figures
in the top row are ground truth flows and the bottom ones are the predicted values. STE-TG can
achieve accurate traffic flow prediction only using the traffic information of the previous time step
and the spatiotemporal representations estetg of the center grid and the adjacent grids.
We use the top-10 strategy to get the temporal neighboring relations of grids with low distances. To-
gether with the spatial context information Gspa , Gst is built to help the embedding model preserve
the spatiotemporal conjunctions between the traffic grids. The objective function used to optimize
the spatiotemporal model remains the same as in the main text.
Predictor Design The flow of one grid is significantly related to the nature of the grids around it
because an outflow from one region can lead to an inflow to neighboring regions. In this work, the
spatiotemporal natures of grids and correlations between grids are encoded in the embedding vector
estetg . Thus, Given a query Xtm-,1n , spatiotemporal embeddings of four geographical closest grids
and grid (m, n) itself will also be fed into predictor fθ to produce the estimation
Xm,n =(xin，m，n,xout，m，n) = fθ(Xm-1n, em,n)	(17)
where em,n = (esmte,ntg, esnteeitggh) and esnteeitggh indicates embeddings of four spatial neighbors of grid
(m, n). We use a simple three-layer MLP fθ with ReLU activations for implementation, the dimen-
Sion of hidden state is 256. The loss function used is mean squared error ||X - X ||2.
A.3 Experiments
Following the same pipeline we used in POI recommendation task, we first train the spatiotemporal
embedding model STE-TG and then optimize the recommender model on the training set. All
models are optimized using Adam optimizer with default βs, the learning rate is set to 1 × 10-3 .
The embedding dimensions {dspa, dst} are set to {64, 96}.
14
Under review as a conference paper at ICLR 2022
Table 5: Comparisons with baselines on TaxiBJ15 dataset.	
Method	RMSE
ARIMA	25.58
SARIMA	29.11
VAR	25.59
CNN	26.08
DeepST-CPTM	22.59
STE-TG (Ours)	21.92
Table 6: Comparisons with baselines on TaxiBJ dataset.	
Method	RMSE
HA	57.69
ARIMA	22.78
SARIMA	26.88
LinUOTD	21.23
ConvLSTM	19.54
DeepST-CPTM	18.18
STE-TG (Ours)	18.03
Metrics We measure the traffic flow forecasting performance by Root Mean Square Error (RMSE)
as
RMSE 4	X(Xi- Xi)2
(18)
where X and X are the predicted value and ground truth, respectively and t is the number of all
queries.
Datasets We perform experiments on two datasets:
•	TaxiBJ15 (Zhang et al., 2016) consists of calculated inflow/outflow based on taxicab GPS
data collected in Beijing city in 2015.
•	TaxiBJ (Zhang et al., 2017) is an extended version including data from 2013 to 2016.
For TaxiBJ15, data from the last week is used for testing. For TaxiBJ, we choose the last four weeks
as the testing data.
Baselines We choose representative baselines for comparison:
•	HA: a predict method by the average value of historical values.
•	ARIMA (Auto-Regressive Integrated Moving Average)
•	SARIMA: seasonal ARIMA
•	VAR (Vector Auto-Regressive)
•	ST-ANN: It first extracts spatial (nearby 8 regions’ values) and temporal (8 previous time
intervals) features, then fed into an ANN.
•	DeepST (Zhang et al., 2016): a deep neural network (DNN)-based prediction model for
spatiotemporal data, which shows state-of-the-art results on crowd flows prediction, we
only consider the most potent variant, i.e. DeepST-CPTM.
•	ST-ResNet (Zhang et al., 2017) ST-ResNet models citywide traffic flow at different times
into 2D images to perform prediction.
•	LinUOTD (Tong et al., 2017): a linear regression method with a spatio-temporal regular-
ization.
•	ConvLSTM (Xingjian et al., 2015): a LSTM variant with convolution modules.
15
Under review as a conference paper at ICLR 2022
Results We listed the comparison results in Table.5 and Table.6. In STE-TG, only the grid map
and historical traffic flow are used to perform flow prediction, while existing works usually take
meta-data like weather into account. However, we observe that the hastily constructed traffic flow
prediction methods STE-TG achieve competitive performance, which also demonstrates the effec-
tiveness of the proposed STE framework.
16