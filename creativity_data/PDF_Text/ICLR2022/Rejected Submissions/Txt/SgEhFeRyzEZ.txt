Under review as a conference paper at ICLR 2022
Convergence Analysis and Implicit
Regularization of Feedback Alignment
for Deep Linear Networks
Anonymous authors
Paper under double-blind review
Ab stract
We theoretically analyze the Feedback Alignment (FA) algorithm, an efficient al-
ternative to backpropagation for training neural networks. We provide convergence
guarantees with rates for deep linear networks for both continuous and discrete
dynamics. Additionally, we study incremental learning phenomena for shallow
linear networks. Interestingly, certain specific initializations imply that negligible
components are learned before the principal ones, thus potentially negatively af-
fecting the effectiveness of such a learning algorithm; a phenomenon we classify
as implicit anti-regularization. We also provide initialization schemes where the
components of the problem are approximately learned by decreasing order of
importance, thus providing a form of implicit regularization.
1 Introduction
In order to train a Machine Learning (ML) architecture in the context of supervised learning, it
is a consolidated practice to resort to the Gradient Descent (GD) algorithm and its variants (e.g.
stochastic GD) and to calculate the gradient of the loss function via backpropagation (Rumelhart
et al., 1986). However, one of its limitations is the so-called weight transport problem (Grossberg,
1987): the update of each neuron during the learning phase relies on the downstream weights (which
are therefore "transported" along the backward pass). This implies that 1) the synaptic weights are
the same, up to transposition, for both the forward (inference) and backward (learning) pass and 2)
each neuron has a precise knowledge of all of its forward synapses. It is known that such a model
does not accurately reflect the behaviour of a human brain, thus it is biologically implausible (Crick,
1989). Feedback Alignment (FA), originally proposed by Lillicrap (Lillicrap et al., 2016), has been
an attempt to mitigate this discrepancy between the neuroscience side and the engineering side of
AI and to tackle the weight transport problem in a systematic way. The simple, yet powerful idea
is to replace the transposed weight matrices in the backpropagation algorithm with arbitrary, fixed
matrices: via this strategy, each neuron during training receives a random projection of the error
vector.
In order to illustrate the algorithm and for the purpose of our study, let us consider a fully-connected
L-layer neural network (NN): given the input, X = ho ∈ Rd, the predicted label y ∈ Ro is computed
as
a` = w`h`-i,	h` = σ(a'), ' ∈ {1,...,L} := [L],
(1)
σ being a (potentially) nonlinear function applied entry-wise to the vector a`, and finally y = f (。工)
for some output nonlinear function f . We consider a regression task with loss function L being the
standard Mean Square Error (MSE). The backpropagation strategy for GD prescribes the following
updates for each layer in the network: given a step size η > 0 and the error vector e = δaL = ∂y^ =
y - y where y is the true label, we have
δWβ = -ηeh>-ι, δW' = -ηδa'h>-ι and δag = (We+ιδa'+ι)Θσ0(a`), ' ∈ [L-1] (GD)
where denotes the Hadamard product. The FA recipe prescribes the following:
Feedback Alignment:	in (GD) use	δag = (M'δa'+ι) Θ σ0(ag),	' ∈ [L - 1].	(2)
where {M'} are a collection of arbitrarily chosen matrices, which do not evolve during training.
1
Under review as a conference paper at ICLR 2022
Related and previous work. Inspired by Lillicrap et al. (2016) and shortly after its publication,
N0kland (2016) proposed two new algorithms called Direct and Inverse FA. In particular, Direct
FA reduces to the FA algorithm in the case of a 2-layer NN or multi-layer linear NN; the difference
between these two algorithms in the general multi-layer nonlinear NN resides in injecting the
error vector directly into the update of each hidden layer of the network: δa? = (M'e) Θ σ0(ag),
` ∈ [1, L - 1]. Such a modification of the FA algorithm can be interpreted as a noisy version of
layer-wise training (Gilmer et al., 2017).
Since its first formulation, there has been an extensive numerical analysis on testing whether FA
and Direct FA algorithms can be successfully applied to ML problems, in particular in the presence
of complex datasets and deep architectures (Bartunov et al., 2018; Moskovitz et al., 2018; Launay
et al., 2019). We mention the recent survey paper Launay et al. (2020) where the focus is on Direct
FA and it is shown that the algorithm performs well in modern deep architectures (Transformers
and Graph NN, among others) applied to problems like neural view synthesis or natural language
processing. Furthermore, more on the applied (biologically-inspired) side, there have been recent
pushes in furthering the idea of FA: spike-train level direct feedback alignment (Lee et al., 2020),
memory-efficient direct feedback alignment (Chu et al., 2020) and direct random target projection
(Frenkel et al., 2021) to mention a few examples.
On the other hand, rigorous theoretical results are scarce. Preliminary asymptotic results can be
found in the original papers by Lillicrap et al. (2016) and N0kland (2016). More recently, Refinetti
et al. (2020) presented the derivation of a system of differential equations that governs the Direct FA
dynamics for a 2-layer non-linear NN in the regime where the input dimension goes to infinity. There
is no explicit formula for the solution of the system, but via a careful local analysis it is possible to
identify two separate phases of learning (alignment and memorization) and a "degeneracy breaking"
effect: at the end of the FA training, the selected solution maximizes the overlap between the second
weight matrix W2 and the FA matrix M. Such an "alignment" dynamics is also evident in our
analysis (Section 2). Convergence guarantees for deep networks are still missing from the literature
and one of the goals of this paper is indeed to provide rigorous proofs of convergence, together with
convergence rates.
Our analysis is in a similar vein as some recent works on implicit regularization in linear neural
networks Saxe et al. (2018); Gidel et al. (2019); Arora et al. (2018). Even though our setting is
very similar, our analysis is different: because of the feedback alignment matrices, the autonomous
differential equation obtained significantly differ and lead to different dynamics. Moreover our focus
is slightly different from Saxe et al. (2018); Gidel et al. (2019); Arora et al. (2018) which are fully
dedicated to implicit regularization of GD while we also aim at proving the convergence of FA.
Our contributions. We aim to provide a theoretical understanding of how and when FA works. We
focus on two aspects: 1) proving convergence of the algorithm and 2) analyzing implicit regularization
phenomena. We will extensively study the case of deep linear NNs. Despite being sometimes regarded
as too simplistic, the study of linear networks is a crucial starting point for a systematic analysis of
the FA algorithm: it will allow us to rigorously analyze a tractable model and it will give us insights
and intuition on the general nonlinear setting (Section 7).
Our main results are the following.
•	We prove convergence of the FA continuous dynamics, with rates, for L-layer NNs with
L ≥ 2 (Sections 2 and 3). Such results hold for any input dimension and number of neurons:
the network is not necessarily overparametrized.
•	We prove that, for certain initialization schemes, the continuous FA dynamics sequen-
tially learn the solutions of a reduced-rank regression problem, but in a backward fashion:
smaller components first, dominant components later (Section 4). Additionally, we provide
guidelines for an initialization scheme that avoids such phenomena.
•	Finally (Section 5), we analyze the corresponding discrete FA dynamics and we prove
convergence to the true labels with linear convergence rates.
We assumed some conditions on the data and on the model in order to render the exposition and the
reading more clear. However, some of the "strong" assumptions that will appear in the paper can be
easily relaxed (see Section 2).
2
Under review as a conference paper at ICLR 2022
2	Warm-up: Shallow networks
In this section, We consider a 2-layer linear NN with vector output y^ = W2 Wιx ∈ Ro and input
vector X ∈ Rd such that (x, y)〜 D for some distribution D. Define Σχχ := E[xx>] ∈ Rd×d, that
we assume to be positive definite, and Σxy := E[yx>] ∈ Ro×d, then the following result holds:
Proposition 1. For any distribution D, the FA dynamics that dictates the learning process is
WI = M (Σχy - W2W1∑XX) ,	W2 = (Σχy - W2WlΣχχ) W>,	(3)
where the dot refers to the time derivative.
Note that this result particularly holds for empirical distributions corresponding to finite datasets.
In this work, we will assume that the two matrices Σxx, Σxy can be simultaneously decomposed
as Σxx = V ΛxxV > and Σxy = UΛxyV >, where the latter corresponds to the singular value
decomposition (SVD) of Σxy . This assumption holds in many situations, as discussed in details
by Gidel et al. (2019). Moreover, a perturbation analysis similar as the one done in Gidel et al.
(2019, Theorem 1) could be performed in the general case in order to handle the non-commutative
case. Next, we introduce the change of variables Wi = RW1V> and W2 = UW2R>, for some
arbitrary left-orthogonal matrix R, and we choose the FA matrix M to be decomposable in a similar
fashion: M = RDU >, for some matrix D with positive diagonal entries Dii > 0 and zero entries
otherwise. In particular, this implies that M is full rank, therefore guaranteeing convergence to a
global minimum of the dynamics. The equations for the FA flow will then become
W1 = D (Λχy - W2WιΛχχ)	W2 = (Λχy - W2WιΛχχ) W>.	(4)
Moreover, by the change of variable W0 := WIΛ-X72, W0 := A-J2W2, D0 := DA-J2 where
λXX2 is the coordinate-wise square root of Axx,1 we get after some simple calculations that
W0 = DNXAxyΛ-* - W2W0),	W2 = (Λx∕2ΛxyΛ-* - W2W0)Wj	(5)
Therefore, by considering Axy := ΛxX2ΛxyA-1/2, we may equivalently assume Σxx = Id
(isotropic features), without any loss of generality.
It is now crucial to observe that if W10 ∈ Rh×d , W20 ∈ Ro×h are diagonal matrices, then the dynamics
decouples into k independent equations with k = min{d, h, o}. In particular, let θ1i = (W10)ii and
θ2i = (W20)ii and assume that at time t = 0 the matrices W10(0) and W20(0) are diagonal. Then, for
each i = 1, . . . , k we obtain the following scalar system:
θ1 = di (λi - θ2 θ1)	θ2 = (λi - θ2 θ1) θ1,	(6)
where λi = (Axy)ii and di = Dii. Note that for i > k one could define diagonal coefficients for the
matrices W10 or W20, but their derivative will be 0, thus non-trivial dynamics only occur for i ∈ [k].
2.1	The scalar dynamics
We now turn our attention to analyzing a nonlinear system of the form (6) for the scalar functions
θ1, θ2 ∈ C1(R) (we suppressed the indices).
Theorem 2. For any FA constant d∈ R>0, and ∀ θ0 ∈ R initial value with initialization scheme
θi (0) = θo	and	θ2(O)=算,	(7)
∃ C > 0 such that the product θ2 (t)θ1 (t) of the solution of the system (6) converges exponentially
to the signal λ in a monotoniCfashion: ∖θ2(t)θ1(t) - λ∣ < Ce-2(dλ)31 as t → +∞. Moreover the
first layer gets "aligned" with the feedback matrix such that θ1(t) → 个 2rd.
1Note that we used a slight abuse of notation: depending on the matrix it is multiplied to Λ-1/2 corresponds
to the diagonal matrix either of size d X d or o X o with the diagonal coefficients [Λ-1∕2]ii, 1 ≤ i ≤ rank(Λχχ).
3
Under review as a conference paper at ICLR 2022
(a) FA trajectories in the phase space
(c) FA dynamics with i.i.d ini-
(b) FA (continuous lines) and GD
(dotted lines), with initial conditions
(7); θo 〜U([-5, 5]) and d = 2.
(θ1, θ2) for d = .2. The dotted hy-
perbola correspond to the solution
set θ1 θ2 = λ.
tial conditions (θι(0),θ2(0))〜
U([-5,5])×U([-5,5])andd= 2.
Figure 1: Numerical simulations of the solution θ2(t)θ1(t) of the FA dynamics with true signal λ = 3 (left and
middle) and λ = 10 (right), with different initializations.
The proof can be found in Appendix A.2: along with the proof, it appears that the FA weights drive
the learning of the forward weights for the first layers to the point that their limit is "aligned" to the
value of the FA constants (in a nonlinear fashion). Similarly, this holds for the deep network case
(Section 3 and Appendix B.2). One can also observe that components with a large FA constant d are
learned faster (see (35)-(36) in Appendix A.2), boosting the alignment.
From Theorem 2 it is clear that the choice of the FA constant will force the optimization to select one
particular pair of solutions (θ1(t), θ2(t)) among the infinitely many possible solutions (the equation
θ1 θ2 = λ is a 1-dimensional manifold on the R2-plane). However, all of these solutions represent
global minima (those are the only stationary points of FA), and therefore via the FA dynamics, we
can find the global minima of the loss function.
We can easily notice that zero-initialization does guarantee convergence for FA, unlike in the case
of GD dynamics (Figure 1b). Furthermore, we want to highlight the ideological novelty of the
proposed initialization scheme (7): unlike considering all weight matrices as independent players at
the beginning of the optimization process and initializing them at random, we are imposing that once
the weight matrix W1 of the first layers is initialized, the initial value of the weight matrix W2 of the
second layer is deterministic, and it depends directly from the first layer.
More generally, it is easy (though lengthy) to prove that the product of the solutions (θ1 (t), θ2 (t)) of
the system (6) always converges to the signal λ, regardless of the initialization scheme considered.
Some intuitions can be built from the phase diagram in Fig 1a where one can see that the trajectories
of the dynamics lie on parabolas with equation 2dθ2 = θ12 + 2dK while the solution set is represented
by the hyperbola θ1 θ2 = λ. The difficulties in the analysis arise when the initialization correspond
to a parabola that intersect the solution set more than once. Moreover, the non-mononicity of the
quantity ∣θ1 (t)θ2(t) - λ∣ can be seen on this diagram as the result of some trajectories getting close
to the bottom left part of the hyperbola but eventually converging to the top right one.
Theorem 3. For any FA constant d ∈ R>0, and for any initial values θ1(0) and θ2(0), the system
(6) admits a unique solution (θ1 (t), θ2(t)) such that θ1 (t), θ2(t) are bounded for all times t ≥ 0
and θ2(t)θ1(t) → λ as t → +∞. The convergence rate is exponential for any pair of initial values
(θ1(0), θ2(0)) except for a set of (Lebesgue) measure zero in R2.
For the proof, see Appendix A.3. The main consequence of Theorem 3 is that even by initializing
all layers randomly and independently (as it is common practice), the FA dynamics still converges
to the true signal. However, we want to stress that the initialization scheme (7) is more suitable for
ML applications: for arbitrary initial conditions, the product of the solutions θ2(t)θ1(t) may not be
monotonic (compare with Theorem 2). Thus it may trigger inefficient early stopping practices (see
Figure 1c). Furthermore, for a randomly and independently sampled pair of initial conditions, we
may observe a highly undesirable "reversed" incremental learning phenomenon (Section 4), which is
avoided if (7) is put into place.
Relaxing some assumptions. Considering the the assumption that the weight matrices W1, W2
share the left/right singular vectors with Σxy is not strictly needed: with more work, the more general
case can be analyzed via a perturbation analysis, along the same lines as in Gidel et al. (2019)
(Assumption 1). Furthermore, even though the initialization of the matrices W1, W2 are diagonal
4
Under review as a conference paper at ICLR 2022
Saxe et al. (2018) argues that the diagonal solutions provide good approximations to the full solutions
when W1(0), W2(0) are full matrices, initialized with small random weights.
3	Deep Networks
In this section we extend the results of Section 2 to deep linear NNs: given a distribution of
input-output pairs (x, y)〜D, the predicted output vector is now y = WL ... W2W1x with
weight matrices w` ∈ Rh'×h'-1 (h0 = d, hL = o) and We note Σχχ = E[xx>] ∈ Rd×d and
Σxy = E[yx>] ∈ Ro×d.
Proposition 4. For any distribution D, the FA equations of motion are
W£ = M'(£Xy - w[LL]ςXx)W[L-1]	' ∈ [L],	⑻
for some FA weight matrices {M'}'∈[l], with ML = I, and with W[i：'] = W' ... Wi.
Notice that the legitimate FA update would be W⅞ = M ['2-i](Σχy-W[LL]Σχχ)W>.'-i],however
since we are requiring the FA matrices to be full rank, we can redefine the FA matrices as Me =
M['.l-1]. We now perform a similar change of variables as in Section 2: W' = R' W'R>-ι with
Ro = V, RL = U and {R'}'=ι,…,l-i arbitrary left-orthogonal matrices. We additionally assume
that the FA matrices M' have the following prescribed form M' = R'D' U> (with D' a matrix
with zero entries except for entries on the diagonal, which are positive). Then, the system becomes
•
We = D'(Axy - W；1:L]AxX)W[L'-1]	' ∈ [L].	⑼
Considering the change of variable W0 := WιΛxxx1/, WL := A-V2WL, D' := D'Λ-x1/2,
Axy := Axx2Axy Axx1/ and assuming as in §2 that W0(0) are diagonal, the system decouples:
θ' = d'(λi- θL...θ∖ )θ'-ι...θ1,	' ∈ [L],	(10)
and for each entry i = 1, . . . , k, with k = min{d, h1, . . . , hL-1, o}, the analysis reduces to studying
a system of scalar functions {θ1i , . . . , θLi }.
Theorem 5. For any set of FA constants {d'}'∈[L-1], d' ∈ R>0 and for any θ0 ∈ R, there exists an
initialization scheme of the form
θι(0) = θo,	θ`(θ) = C'θ/	,	' = 2,...,L,	(11)
where C' ∈ R>0 depends on {d1, . . . , d'}, such that the system of differential equations (10) has a
unique solution (θ1(t), . . . , θL(t)), which converges exponentially to the signal λ as t → +∞ in a
monotone fashion.
As in the 2-layer case, the proposed initialization scheme is characterized by the fact that only the
first layer is initialized at random, while all the other layers depend deterministically on it. Note that,
along the same lines as Theorem 3, it is also possible to prove convergence to the true signal for any
initialization set {θ'(0)}'∈[L]. However, some desirable properties of the dynamics (monotonicity,
exponential convergence) may be lost. For the proofs and the explicit construction of the initialization
scheme (11) see Appendix B.2.
4 Implicit regularization phenomena
In this section, we analyze the phenomenon of hierarchical learning (or implicit regularization)
for FA, as described in Gidel et al. (2019) and Gissin et al. (2019) in the case of GD: along the
optimization path, features of the model are learned in a sequential way, so that one feature is fully
learned before the dynamics starts to learn the next feature. For FA, we may observe a similar
behavior as GD, however, in certain settings the dynamics seems to favor the learning of secondary
component prior than the learning of principal components, i.e. signals λi,s with small magnitude
are learned earlier than those of big magnitude. This phenomenon is potentially highly problematic
since smaller singular values λi,s are normally tied to noisy measurements and do not represent nor
5
Under review as a conference paper at ICLR 2022
give meaningful information on the structure of the data. Privileging negligible features goes against
the standard principles of reasoned data analysis (Principal Component Analysis, for example). We
will see that the occurrence of this behavior depends on particular initializations of the algorithm and
it can be therefore successfully avoided.
Theorem 6 (Informal). Let λ1 > . . . > λk be the principal components of Λxy. There exists
initialization schemes such that the component λi is incrementally learned at time Ti. Moreover,
there exists two different initialization schemes such that
Anti-Regularization(§4.1): T1 > . . . > Tk and Regularization(§4.2): T1 < . . . < Tk (12)
To illustrate such a phenomenon and rigorously describe it, consider the 2-layer case in the SVD-
setting as in Section 2: ∀ i = 1, . . . , k, the system (6) can be equivalently rewritten as
θ1 = -1 ((θ1 )3 + 2dKiθ1 - 2dλi) ,	θ1(0) = θ0,	(13)
and θ2(t)=击θ1 (t)2 + Ki, with Ki = θ2(0)—赤(θ1(0))2 ∈ R and d,λ > 0 (for simplicity We
assumed di = d, ∀ i). Depending on the value of Ki (i.e. the initial condition of the i-th system), the
cubic polynomial on the right-hand-side of (13) may have between 1 and 3 distinct real roots. The
value of the discriminant ∆ = -4d2 8d(K i)3 + 27(λi)2 will dictate which regime we fall into.
We will be considering the case where ∆ > 0 and the case Ki = 0 (i.e. the initialization scheme
(7)), which is a special instance of the case ∆ < 0. The case ∆ = 0 can be equally analyzed, but it
will not be considered in this paper, as it has zero probability of occurring.2 The sequential learning
phenomenon becomes evident when we perform a double scaling limit of the time variable t 7→ δt
and the initial condition θ0i 7→ f(δ, θ0i ), δ → +∞, as it will be clear below. Note that the rescaling
is uniform across the components i. Proof of Theorem 7 and lengthier discussion can be found in
Appendix C.
4.1 CASE ∆ > 0: BACKWARD LEARNING
If Ki < — 3 (λi)3/2 < 0, the cubic polynomial in (13) has 3 real, distinct root 片 < r2 < r3. The
intuition is that by initializing θ0i at a value that is very close to r2i and applying the time scaling
t 7→ δt, δ 1 (i.e. we are "fast-forwarding" the solution θ1(δt)), we can clearly identify the time
threshold Ti after which the signal is fully learned. More formally,
Theorem 7. ∀ i = 1, . . . , k, if θ1i (0) = r2i + e-δ (δ > 0), then the solution θ1i (δt) converges to a
step function:
θ1 (δt) → r2I{t<Ti} + α I{t=T i} + r3I{t>Ti}	as δ → +∞,	(14)
where I{t∈A} is the indicator function on the set A,
2
Ti
(r3- r2 )(r2 -r1),
αi ∈ (r2i, r3i).
(15)
Furthermore, Ti is an increasing function of λi: for λ1 > λ2 > . . . > λk and fixed FA constant d,
we have T1 > T2 > . . . > Tk.
Similar conclusions hold when θ1i (0) = r2i - e-δ. Therefore, as the value of λi increases, the value
of Ti increases as well: negligible features of the data are learned sooner than the dominant ones.
On the other hand, Theorem 7 shows that such a backward incremental learning does not happen with
high probability: indeed, we are requiring all the parameters Ki (which depends on the initialization)
to be sufficiently negative and the probability of sampling initial conditions that will yield anti-
regularization depends on the value of the signals λi (i.e., it is data-dependent): the bigger the
value of λi , the smaller the probability. Nevertheless, we will see in the next subsection how such a
phenomenon can be safely bypassed and we discuss some properties that suggest that the initialization
scheme (7) can guarantee the correct sequential learning of features as GD exhibits.
2The claim is correct if we sample the initial conditions θ1 (0), θ2(0) independently and generically: for
example, from a uniform distribution U ([-R, R]) × U ([-R, R]), for some R ∈ R>0.
6
Under review as a conference paper at ICLR 2022
4.2 CASE K = 0: EVIDENCE OF INCREMENTAL LEARNING
Imposing Ki = 0 for all i’s means to recur to the initialization scheme (7): θ1i (0) = θ0i , θ2i (0) =
21d(θ0)2. In this case, Theorem 2 already provides exponential convergence rates that depend on the
2
magnitude of the signal (∣Θ2 (t)θɪ (t) - λi∣ < Ce-2 (dλ)31 as t → +∞), suggesting that the bigger
the magnitude of the signal λi , the faster the convergence.
If θ0i < 0, there exists a unique time T0i (vanishing time) such that θ1i(T0i)θ2i(T0i) = 0 and
θ1 (t)θ2(t). 0 for t . T0 respectively, where
T0i
π	1	(	(ri - θ0)2	ʌ 2	(2θ0 + ri
(ri)23√3 + 3W	(θ0)2 + riθ0 + (ri)2 - (rψ√3 arctan< ri√3
(16)
We can interpret such a vanishing time as the first milestone of the learning process. In the regime as
θ0i → -∞ ∀ i = 1, . . . , k, we have
T0i
3(ri)2√3+ O (百
(17)
where ri = 飞2dλi, therefore for bigger λi,s (i.e. dominant singular values) the vanishing time
happens sooner: for λ1 > λ2 > . . . > λk and fixed FA constant d > 0, T01 < T02 < . . . < T0k .
5	Discrete dynamics
5.1 Forward Euler discretization
In this section, we are interested in analyzing the discrete FA dynamics and its convergence properties.
For the discrete time setting, we focus again on a 2-layer linear NN. The standard forward Euler
method yields the following discretized system:
W1(t+1) = W1(t) +ηM Σxy - W2(t)W1(t)	(18)
W2(t+1) = W2(t+1) +η Σxy - W2(t)W1(t)	W1(t)>	(19)
and performing again the SVD change of variables, the system becomes
WW(t+1) = WW(t) + ηD (Axy - WW2((t)WW(t))	(20)
W(t+1) = W(t) + η (Axy - W(t)W(t)) (Wf))>	(21)
If we set the initial conditions W1(0), W2(0) as diagonal, the matrices remain diagonal for all times
and the dynamic decouples. For each i = 1, . . . , k, we have
(θ1(t+1)i = (θ1(t)i+ηdi (λi- (θ2(t)i (θ1(t)i	(22)
(θ2(t+1)i =	(θ2(t)i + η (λi-	(θ2(t)i	(θ1(t)i	(θ1(t)i	(23)
The following result holds (the index i is suppressed for simplicity).
Theorem 8. Consider the system (22)-(23). With zero initialization θ(0) = θ(0) = 0, ∀ λ,d > 0 and
with constant step size η small enough, the product θ2(t)θ1(t) converges to the true signal λ with linear
rates: θ2(t)θ1(t) - λ ≤ Cqt, for some C > 0.
For the sake of clarity, we deferred the precise prescription for the magnitude of η and the linear rate
q to Appendix D, where the full proof of the Theorem can be found.
7
Under review as a conference paper at ICLR 2022
5.2 Modified forward dis cretization
In addition to the standard discretization strategy, we are proposing here anew discretization technique
that is particularly instrumental for the FA setting, as it will allow a thorough inspection of its
convergence guarantees and convergence rates for NNs of any depth. Inspired by the fact that the
behaviour of the second layer strictly depend on the choices that we impose on the first layer in
the continuous dynamics (Theorem 2 and initialization (7)), we slightly modify the update rule for
second weight matrix W2 in the following way:
W1(t+1) = W1(t) +ηM Σxy - W2(t)W1(t)	(24)
w2(t+1) = w2(t+1) + η (∑χy - Wy)Wy)) (w1(t+2]	(25)
with W(t+ 2) = 2 (w(t+1) + Wf)). The above discretization technique Can be considered as a
hybrid between the Euler forward method and the trapezoidal rule (Iserles, 1996), although we are
still evaluating the error Σxy - W2(t)W1(t) at the present time t. The underlying idea is that we want
to already use the information encoded in the new (t + 1)-iterate of W1 to update the second layer
by averaging W1(t) and W1(t+1).
Performing again the SVD change of variables, and setting W1(0), W2(0) as diagonal, we obtain a
scalar system of the form (∀ i = 1, . . . , k)
(Θ1 )(t+1) = (θ1 严 + ηdi (λi - (θ2严(θ1 严)	(26)
(θ2)(t+1) = (θ2)(t) + η (λi -您)(t) (θi)(t)) (θi)(t+2).	(27)
A convergence result follows (suppressing the i-dependence).
Theorem 9. Consider the System (26)-(27). With zero initialization θ(0) = θg" = 0, ∀ λ,d > 0
and with constant step size η < 人/,the product θg"θ(" linearly converges to the true Signal λ:
∣θ2t)θ(t) - λ∣≤ 3λ(1 - 3η(2dλ)3 )t.
The discretization scheme easily generalizes to the case of L layers for L ≥ 3 (we again suppress the
i-index for readability):
θ't+1) = θ't) + ηd'θ(1+2)] (λ -θLt ... θ(t)) ,	' = 1,...,L,	(28)
with dL = 1. Also in this case, the dynamics closely mimic the continuous counterpart, reducing
the system to a single recurrence equation for {θ1(t)}. By selecting a step size η small enough, the
sequence of products {θL(t) . . . θ2(t)θ1(t)} will converge to the signal λ with exponential rate.
Theorem 10. Consider the system (28) with λ, d` > 0 and zero initialization θ'0) = 0, ∀ ' =
1,...,L. Ifthe constant step size satisfies η < (dιγ(KXγ~1)1/1 )~1, where Y = 2l 一 1 and K is a
Suitable positive constant depending on the FA constants dι,...,dL-ι, then the product QL=I θ't)
linearly converges to the true signal λ: ∣θ(t)... θ(t) — λ∣ ≤ Cλ(1 — ηdιγ (Kλγ-1)Y )t, for some
constant Cλ ∈ R>0 only dependent on λ.
The proofs of Theorems 9 and 10 can be found in Appendix D.
6	Experiments
In this last section, we discuss one ML application of our theoretical results. The set-up is borrowed
from Gidel et al. (2019) (Section 4.2), where a linear autoencoder is considered.
For an autoencoder, the (true) output is equal to its input y = x ∈ Rd and we want to compare the
reconstruction properties ofW2(t)W1(t) (2 layer NN) and W3(t)W2(t)W1(t) (3 layer NN) computed via
8
Under review as a conference paper at ICLR 2022
5 4 3 2 1 0
E」ON ①ue.U-
——FA (31)
FA (21)
-GD (31)
——GD (21)
IO1 IO3 IO5
Number of iterations
IO1 IO3 IO5
Number of iterations
Figure 2: Comparison of trace norms and reconstruction errors for linear autoencoders. For the FA
experiments, we are plotting the average (with error bars) of 15 experiments.
FA versus GD. In this experiment, we have d = o = 20 and k = 5. We generate n = 1000 synthetic
data {xi } in the following way: each data point is given as xi = Azi + i, where A ∈ Rd×k
is a fixed matrix with entries sampled as Akl 〜 U([0,1]), Zi 〜N(0, A := diag{4, 2,1, 1, 4))
and the noise Wi 〜 10-3N(0, Id). We initialize the weight matrices to a CloSe-to-zero initial-
ization W(0), W(0), W(0) 〜10-5U([0,1]): this will guarantees the FA dynamics to avoid the
anti-regularization pattern and to converge to the signal (Theorems 8, 9 and 10).
We generate the FA matrices M (2-layers) and M1, M2 (3-layers) with entries Mij 〜U([0,1]).
Keeping the same initial conditions W(0) and same fixed matrix A, We repeat the FA training 15
times, sampling a different set of FA matrices each time: in Figure 2, we report the average (with error
bars) of the trace norm ∣∣Θ(t)k^ with Θ ⑶:=w2(e)W(e) or Θ ⑴:=W(e)w2(e) Wf) respectively,
as well as thee average of the reconstruction errors Θ(t) - AAA> 2, as a function of t the number
of iterations.
We can see that the FA learning dynamics is distinctly faster than GD, which is learning at a much
slower pace, as we initialized the dynamics close to one of GD’s local maxima. In particular, the
3-layer NN trained with GD hasn’t yet started to learn at the end of the selected training period of
the experiment. Notably, the FA training for the 3-layer NN hints at the presence of an implicit
regularization behavior, similar to what has been observed and proved in Gidel et al. (2019) for a
2-layer NN. Additional technical details can be found in Appendix E.
7	Final discussions and insights
In the present paper we rigorously analyze a biologically plausible optimization algorithm, we
describe its potential as well as its limitations in possible ML applications. We provide convergence
guarantees in both the continuous and discrete-time settings; additionally, we thoroughly discuss
implicit regularization phenomena that may arise with certain initialization schemes and that may
deeply affect the optimization effectiveness. The occurrence of drastically opposite regularization by
only modifying an algorithm’s initialization showcases the critical importance of the latter for ML
applications.
Although the analysis is conducted only for linear models, the results give already numerous insights
on the behaviour of nonlinear architectures. The analysis of Theorem 5 shows that, in some regimes,
the deeper layers behave as "a power" of the first layer: this result suggests that each layer is
converging at a different speed. It thus defines a hierarchy (in terms of speed of convergence) between
the layers of the network. Furthermore, one can notice that in the linear case FA "removes" some
stationary points: unlike GD that potentially has many stationary points that are saddle points, all the
stationary points of FA are global minima. Therefore, we believe that FA will also "remove" some
undesirable stationary points in the non-linear case. Several challenges remain, but the above pieces
of intuition will be critical for the analysis in the nonlinear setting.
9
Under review as a conference paper at ICLR 2022
Beyond this, it is enticing to conduct a comparison between FA and GD and illustrate the advantages
of one algorithm over the other (and in which setting). All of these directions will be the central
objects of study in future works.
References
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In International Conference on Learning Representations,
2018.
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey E. Hinton, and Timo-
thy Lillicrap. Assessing the scalability of biologically-motivated deep learning algorithms and
architectures. In Advances in Neural Information Processing Systems, volume 31, 2018.
Tien Chu, Kamil Mykitiuk, Miron Szewczyk, Adam Wiktor, and Zbigniew Wojna. Training DNNs in
O(1) memory with MEM-DFA using random matrices. arXiv:2012.11745, 2020.
Francis Crick. The recent excitement about neural networks. Nature, 337, 1989.
Charlotte Frenkel, Martin Lefebvre, and David Bol. Learning without feedback: Fixed random
learning signals allow for feedforward training of deep neural networks. Frontiers in Neuroscience,
15, 2021.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. arXiv:1904.13262, 2019.
J Gilmer, C. Raffel, S. S. Schoenholz, M. Raghu, and J. Sohl-Dickstein. Explaining the learning
dynamics of direct feedback alignment. ICLR, 2017.
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental
learning drives generalization. arXiv:1909.12051, 2019.
Stephen Grossberg. Competitive learning: from interactive activation to adaptive resonance. Cogn.
Sci., 11, 1987.
Arieh Iserles. A First Course in the Numerical Analysis of Differential Equations. Cambridge
University Press, 1996.
Julien Launay, Iacopo Poli, and Florent Krzakala. Principled training of neural networks with direct
feedback alignment. arXiv:1906.04554, 2019.
Julien Launay, IacoPo Poli, Frangois Boniface, and Florent Krzakala. Direct feedback alignment
scales to modern deep learning tasks and architectures. In Advances in Neural Information
Processing Systems, volume 33, 2020.
J. Lee, R. Zhang, W. Zhang, Y. Liu, and P. Li. SPike-Train level Direct Feedback Alignment:
SidestePPing backProPagation for on-chiP training of sPiking neural nets. Front Neurosci., 2020.
Timothy P. LillicraP, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random synaPtic
feedback weights suPPort error backProPagation for deeP learning. Nature Communications, 7,
2016.
Theodore H. Moskovitz, Ashok Litwin-Kumar, and L.F. Abbott. Feedback alignment in deeP
convolutional networks. arXiv:1812.06488, 2018.
A. N0kland. Direct feedback alignment provides learning in deep neural networks. arXiv:1609.01596,
2016.
Maria Refinetti, StePhane d,Ascoli, Ruben Ohana, and Sebastian Goldt. The dynamics of learning
with feedback alignment. arXiv:2011.12428, 2020.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature, 323, 1986.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. arXiv:1810.10531, 2018.
10
Under review as a conference paper at ICLR 2022
A CONVERGENCE OF FA FOR 2- LAYER NNS
A.1 Proof of Proposition 1
Proof. We have that L := 1 E[ky - y∣∣2] and y = W2W1x, thus
VwιL = E[ 1 Vwi ky - y『]	(29)
= E[W2> (y - W2W1x)x>]	(30)
=W2>(Σxy-W2W1Σxx),	(31)
where the last line comes from the linearity of the expectation. Similarly we have,
Vw2L = E[ 1 Vw2 ky - yk2]	(32)
= E[(y - W2W1x)x>W1>]	(33)
= (Σxy -W2W1Σxx)W1>.	(34)
Now we conclude by noticing that by definition of the FA dynamics (2), the matrix W2> is replaced
by a random matrix M.	□
A.2 Proof of Theorem 2
Consider the following system of ODEs for the functions θ1, θ2 ∈ C1(R):
-♦ _ ,. ..
θι = d (λ - θ2θ1)	(35)
-♦ ，- ,.
θ2 = (λ - θ2θ1) θι	(36)
)-
for some d,λ ∈ R>o. From equation (35), We substitute θ1 = (λ - θ2θ1) in the second equation
(36) to obtain
θ2 = 1dθθl	i.e. θ2(t) = ^dθ2(t) + K,	(37)
2d	2d
for some integration constant K ∈ R. We then substitute such expression for θ2 (t) back into the
,♦ /C L、 ♦	1	.	1 .	∙	1	FC	∙ f λ
equation (35) in order to obtain a closed form expression for θ1:
Θ1
d (λ - 2dθ3 - Kθι
We noW choose the initial conditions for θ1(t) and θ2(t) in the folloWing Way:
θ1(0) =θ0
and
θ2(0)
θ02
2d
so that the equation for θ1 becomes
θ'ι = 2 (2dλ - θ3)
(38)
(39)
(40)
We are noW ready to prove Theorem 2, Which We recall here for convenience:
Theorem 11. For any FA constant d ∈ R>0, and∀ θ0 ∈ R initial value with initialization scheme
(39), ∃ C > 0 such that the product θ2(t)θ1(t) of the solution of the System (35)-(36) converges
exponentially to the signal λ and in a monotonic fashion:
∣θ2(t)θ1(t) - λ∣ < Ce-3(dλ)31	as t → 十∞.	(41)
Proof. By Picard-Lindelof Theorem, equation (40) admits a unique local solution θι ∈ C 1([0, T])
(T > 0), for any initial condition θ0 ∈ R. The local solution can be then easily extended to a global
solution on [0, +∞).
The cubic polynomial on the right hand side of (40) has a unique real root r := √2dλ: 2dλ - θ3 =
(r - θ1 -) θ12 + θ1r + r2 Where the quadratic θ12 + θ1r + r2 is irreducible (indeed, its discriminant
is ∆ = -3r2 < 0).
11
Under review as a conference paper at ICLR 2022
If θ0 = r, the solution of the ODE (40) is the constant function θ1(t) = r, ∀ t ∈ R≥0. In this case, it
2
is trivial to see that θ2(t)=务 and θ2(t)θ1(t) = λ, ∀ t ∈ R≥o.
If θ0 6= r, we simply need to analyze a separable ODE whose solution reads
-3r2 ln 仇⑴-r| + 3r2	ln	(θ2⑴ +	rθι⑴ + r2)	+	r2√3	arctan (-1r√3	) +	CO	= t
(42)
with constant of integration
C0
21
3r2ln lθ0- r|- 3r2ln lθo + rθ0+r I-
2
----arctan
r2√3
2θo + r
∖ r√3
(43)
Equation (42) is transcendental and it cannot be solved for θ1 explicitly. However, we can still analyze
the behaviour of the solution.
Lemma 12. θι(t) is monotonic: in particular, θι < 0, if θ0 > r, and θι > 0, if θ0 < r.
The above claims easily follows by inspecting the sign of the right hand side of (40). In particular,
Lemma 12 implies that θ1 (t) is always bounded, for any initial condition θ0 ∈ R.
As t → +∞, the left hand side of (42) may only diverge in the first logarithmic term (θ1(t) → r),
since the term involving the arctangent is always bounded and the other logarithmic term has argument
that is always bounded (Proposition 12) and positive (the quadratic is irreducible).
Therefore, as t → +∞
θι(t) → r and	θ2(t)θ1(t) → ]r3 = λ	(44)
for any initial condition θ0 ∈ R. See Figure 3.
Furthermore, we can derive from (42) the rate of convergence:
仇⑴一r|
exp < -32r-t + 3r2C0 + 1ln ∣θι(t)2 + rθι(t) + r2∣ + √3sign(r) arctan
I	X------------------------------------------------
^^{^^™
=:g(t)
2θι(t)+ r A
∣r∣√3 )
(45)
where g(t) = 3r2C0 + 1 ln(3r2) + √3 + o(1), as t → ∞; therefore,
∣θι - r| = e-3r2t+O⑴.	(46)
Finally, from θ2 (t)= 击θι(t)2, we get the desired result:
1	3r2
θ2(t)θ1(t) - 2^r3 = ∣Θ2(t)θι(t) - λ∣≤ Ce-ɪt	(47)
for some positive constant C.
Remark 13. All the above calculations are valid also in the general case r, λ ∈ R \ {0}. In the case
where r < 0 (i.e. sign(d) 6= sign(λ)), the solution θ1(t) satisfies the implicit equation
-3 ln lθι(t) - r| 十 白 ln∣θ2(t) + rθι(t) + r2l + 2sign(r) arctan (2θl(t)z+ r) + CO = t
3r2	3r2	r2 3	|r| 3
(48)
and
co =总ln lθo - r| -白 lnlθO + rθo + r2l - 2sign^) arctan (2θO +r) .	(49)
3r2	3r2	r2 3	|r| 3
12
Under review as a conference paper at ICLR 2022
Figure 3: The plots of θι(t), θ2(t) and the product θ2(t)θ1 (t) With initial condition θ0 〜U(-5, 5),
λ = 3 and FA constant d = 2 (10 simulations).
To conclude the proof, We need to shoW that the product θ2(t)θ1 (t) is monotonic increasing (if
θ0 < r) or decreasing (if θ0 > r) to the limit value λ. See Figure 3 for a comparative plot of the
solutions θ1(t), θ2(t) and their product.
The monotonicity properties of θ1 are described in Lemma 12). We recall here the first derivative of
the functions θ1 and θ2 = 2d θ1(t)2
θι = 1 (r3 —。3) ,	θ2 =；。血,	(50)
and its the second derivative
θ1 = 4θ2 (θ3 - r3) ,	θ2 = d (MI)2 + θ1θl)	(51)
If θ0 > r, then θ1 (t) is strictly convex and strictly decreasing; by construction, θ2 (t) is also
strictly convex and decreasing. Therefore, the product θ2(t)θ1 (t) Will be strictly convex and strictly
decreasing ∀ t ∈ R≥0.
If θo < r, then θι(t) is strictly increasing and by construction the product θ2(t)θι(t) = 2dθ3 is
increasing ∀ t ∈ R≥0. The description of convexity/concavity in this case is more involved and We
do not pursue this direction, as it is not necessary for our results.	□
(52)
(53)
Case λ = 0. If We assume the signal λ = 0, We can folloW the same argument as above and perform
similar calculations.
Equation (40) has a simple explicit solution (assuming θ0 6= 0)
θ1(t) = ±St+θ-2,	θ2(t)=2d(t + θ-2),
Where ± depends on the sign of θ0 . It is easy to see that
θ2 (t)θ1 (t) → 0 = λ as t → +∞,
3
however the convergence rate is of the order of O(t-2), therefore not exponential.
A.3 Proof of Theorem 3
In the setting of Theorem 3, we are considering the initial conditions of θ1 and θ2 as independent,
therefore the integration constant
K = θ2(0) - 2dθ1 (0)2 ∈ R
13
Under review as a conference paper at ICLR 2022
may assume any value. The cubic polynomial in the ODE for θ1 has the general form
θ1 = -1 (θ3 + 2dKθ1 - 2dλ)	(54)
and it could have 1 real root (and 2 complex conjugate roots), 2 real roots (a simple and a double
root) or 3 real distinct roots. To discern the cases, we need to evaluate the discriminant:
∆ = -4d2 (8dK3 + 27λ2) .	(55)
Regardless of the sign of the discriminant, there exists a unique global solution θ1 ∈ C1([0, +∞))
for any initial condition θι (0) ∈ R and any constant K ∈ R (Picard-LindelOf Theorem).
Case ∆ > 0. The polynomial has 3 real, distinct root r1 < r2 < r3 , that may be written explicitly
with the help of trigonometric functions, if needed. Note also that ∆ > 0 implies K < 0. There are
three constant solutions of the equation (54): θ1(t) = rj, ∀ t ≥ 0 (provided that θ0 = rj for some
j = 1, 2, 3). Otherwise (θ0 6= rj ∀ j = 1, 2, 3), the implicit solution of (54) is the following:
ln ∣θι(t) -r3|ln 仇⑴一切 + ln 仇⑴一勺|	+ 加=-1_
(r3 -	r2)(r3 -	ri)	(r3 -	r2)(r2	-	ri)	(r3	- r1)(r2 -	ri)	2
with constant of integration
ln ∣θ0 - r3∣	ln ∣θ0 - r2∣	ln ∣θ0 - ri∣
++	—
(r3 - r2)(r3	- ri)	(r3	-	r2)(r2	- ri)	(r3	- ri)(r2	-	ri)
(56)
(57)
Lemma 14. θi(t) is monotonic:
: . 、 ....
θi < 0	if	θo	∈ (r1,r2) ∪	(r3,+∞),	(58)
： . 、 ， 、 ....
θi > 0	if	θo	∈ (-∞,rι)	∪ (r2,r3).	(59)
In particular, from Lemma 14 we can conclude that the constant solutions θi(t) = ri and θi(t) = r3
are attractive and θi(t) = r2 is repulsive; the general solution (regardless of the initial value θ0 ∈ R)
is bounded for all times.
Following similar arguments as in Section A.2, we can conclude that the convergence is exponentially
fast:
∣θi (t) - r3 ∣
(r3-r22(r3-r1) t+O(1)
as t → +∞,
(60)
e
for θ0 ∈ (r2, r3) ∪ (r3, +∞). A similar asymptotic expansion is valid for the quantity ∣θi(t) - ri∣
when θ0 ∈ (-∞, ri) ∪ (ri, r2).
Using the relation θ2(t) = "θi(t)2 + K, We have
θ2 ⑴ θ1 ⑴-2dr3 - Kr3 ≤ Ce-
(r3-r2)(r3-rI) ,
2	t
(61)
for some C > 0. Notice that, being r3 a root of the cubic polynomial x3 + 2dx - 2dλ, it folloWs that
2dr3 - Kr3 = λ. Similar arguments hold for ri.
Case ∆ < 0. The polynomial has 1 real root r ∈ R and tWo complex conjugate roots. Let us
assume λ > 0 (therefore, r 6= 0); the case λ= 0 is analyzed shortly beloW.
Using Cardano’s formula, the single root has the explicit expression
3	ZZ^^8d3K3	3	/2^^8d3K3
r = y dλ + Vd λ + 27 + y dλ - Vd λ + 27
(62)
The setting is the similar to the one analyzed Section A.2. From a monotonicity argument, it folloWs
that the constant solution θi(t) = r (With θ0 = r) is attractive, therefore for any given initial condition
θ0, θi(t) is bounded for all times t and it converges to r as t → +∞.
14
Under review as a conference paper at ICLR 2022
The implicit solution reads
3( 3 I rlY∖ ln 仇 (t) - r∣ -	3 I j∖∖ ln fθ1(t)2 + rθ1(t) +	)
2 (r3 + dλ)	4 (r3 + dλ)	r
3r3	r (2θ1 (t) + r)	t
- 8(r3 + dλ)(dλ -r3) arctan 1 4(dλ f ++ C0 = - 2	(63)
with C0 the constant of integration.
From θ2(t) = 2dθ1(t)2 + K, convergence follows: θ2(t)θ1(t) → 4r3 + Kr = λ as t → +∞.
Furthermore, convergence is exponentially fast :
∣θ2(t)θ1(t) - λ∣ ≤ Ce-r3+dλt	as t → +∞,	(64)
for some C > 0.
Case ∆ = 0. The vanishing of the discriminant implies that the constant of integration K has a
specific value
κ = - 31落.
2 V d
Remark 15. Notice that the set of initial conditions (θ1(0), θ2 (0)) such that ∆ = 0 is a set of
Lebesgue measure zero in R2.
In this case, the polynomial has 1 simple root rs and one double root rd :
rs = -3λ = 2 √dλ,	rd = 3λ- = -√3dλ
K	2K
The solution of the differential equation (54) (for θ0 6= rs , rd) reads
ln ∣θι(t) — rs∣- ln ∣θι(t)—川	1	C _ t
(rs - rd)2	+(rs-rd)(θι(t)-rd) + 0 = -2
with C0 the constant of integration.
Lemma 16. The solution θ1(t) is monotonic: if rs > rd,
θι < 0	if θo ∈ (rs,+∞)
θι > 0	if θo ∈ (-∞,rd) ∪ (rd,rs)
and similarly ifrs < rd,
θι < 0	if θo ∈ (rs, rd) ∪ (rd, +∞)
θι > 0	if θo ∈ (-∞,rs).
(65)
(66)
(67)
(68)
(69)
(70)
Therefore, the solution θ1(t) = rs is always attractive, while the solution θ1(t) = rd is "one-sided"
attractive. The solution θ1(t) is bounded for all times and the convergence result follows:
θ2(t)θ1(t) → 2d/3 + Krs = λ as t → ∞;	(71)
similarly, for rd. Exponential convergence holds when θ1(t) → rs, while when θ1(t) → rd the rate
is sub-exponential.
In the special case when ∆ = 0 and λ = 0, the polynomial has a triple root r = 0. The solution is
the same as the one derived in Section A.2 in the case of λ = 0 and the convergence to the signal
λ = 0 is polynomial:
∣θ2(t)θ1(t)∣ ≤ Ct-2	as t → +∞,	(72)
for some C > 0.
15
Under review as a conference paper at ICLR 2022
Figure 4: The plot of the product θ3(t)θ2(t)θ1(t) with initial conditions θ0 〜U([-1, 2]), FA
constants d1 = 2 and d2 = 2.5, λ = 1 (10 simulations).
B	CONVERGENCE OF FA FOR L-LAYER NNS
B.1	Proof of Proposition 4
We have that L := 2E[∣∣y - y∣∣2] and y = WL …W2W1 x, thus noting W[a:b]:=
Wb Wb-1 …Wa we have,
Vw'L = E[2Vw' ky - yk2]	(73)
=E[W>+1：L] (y - WL …Wix)x>W[>：i-1]]	(74)
=W>+i：L] (∑χy - WL …Wι∑χχ) W[L-i] ,	(75)
where the last line comes from the linearity of the expectation. Now we conclude by noticing that by
definition of the FA dynamics (2), the matrices on the left hand-side of (y - W2W1x) are replaced
by random matrices Ml . Thus we get
w` = M'(∑xy - W[1:L]∑xx)W[>：'-1]	' ∈ [L],	(76)
where we reparametrized m` = M['+1：L] (see discussion in the main text).
B.2	Proof of Theorem 5
We will first state Theorem 5 in a more extensive form. Given the system
r	,	.	一 、一	.
θL = (λ - θL …Θ1)ΘL-1...Θ1	(77)
• _ ,一 一 一 、一 一
ΘL-1 = dL-i(λ - BL ... θi)θL-2 ...θi	(78)
.
..	(79)
• , . 一 、 一
θ2 = d2(λ - θL ∙∙∙θ1)θ1	(80)
r	,	一、
θi = di(λ - θL ∙∙∙θi)	(81)
where we suppressed the index i for simplicity, the following result holds.
Theorem 17. For any set of FA constants {d'}'=1,...,L-1, d` ∈ R>0, there exists an initialization
scheme such that the SyStem of differential equations (77)一(81) can be reduced into an equation for
θ1 of the form
θi = di (λ - Kθγ)	(82)
for some constant K ∈ R>0 that depends on {d'}, with Y = 2L — 1. The other functions depend
directly on θi in a power-like fashion:
θ' ⑴=D°-1 θi ⑴2	,	' = 2,...,L,	(83)
2' 1
16
Under review as a conference paper at ICLR 2022
for some C' ∈ R>o. Furthermore, thefollowing convergence result holds:
θL(t)θL-1(t) . . . θ1 (t) → λ as t → +∞.	(84)
See Figure 4 for a numerical simulation of a 3-layer scalar network.
Proof. The proof of reducing the system of differential equations into a single equation for θ1 will be
constructive, following a simple iterative procedure. We start by considering the last equation (81)
5
and by substituting (λ - Θl ...θι) = d1 in the second to last equation
θ2 = 2θθιθι =粤(θ2) , implying θ2(t)=鲁θι⑴2 + K2,	(85)
d1	2d1	2d1
for some constant of integration K ∈ R, which can be set to zero if We impose θ? (0) = d2θι (0)2.
The same substitution strategy can be consecutively applied to the third to last equation, fourth to
last equation and so on. In general, it is easy to prove by induction that ∀ ` = 2, . . . , L we have
2' 1	1
θ` = e`θ`	θι, for some C' = C' (di, ...,d`) ∈ R>o, which implies
θ'(t) = ɔ`-i θi(t)2	+ K'	(86)
2'-1
where as before we can set the constant of integration K' = 0 by imposing θ'(0) =2C-'1 θi(O)2	.
Once we have derived the relations between the functions {θ'(t)}'=2,...,L and θ1 (t), we can plug all
of them into (81) to obtain a closed-form differential equation for θ1 :
θi = di (λ - Κθγ),	(87)
where K = QL=I 2C⅛ ∈ R>o and Y = PL=O 2' = 2l - 1. This is a separable equation which can
be solved (with tears) by using partial fraction decomposition.
The proof of convergence of the product Q'L=i θ'(t) to the true signal λ follows a similar argument
as in Appendix A.2. Let r := γJ~K be the unique real root of the polynomial λ - Kθγ, then the
solution θi(t) = r is a constant solution, provided that θi(0) = r. By construction, it follows that
{θ'(t)}'=2,...,L are also constant functions and their product is such that
θL⑴...θi ⑴=Y 2'=ι rγ '=i	=λ	∀t ≥ 0.	(88)
Lemma 18. The general solution θi(t) is monotonic:			
θi > 0	if	θi (0) < r	(89)
θi < 0	if	θi (0) > r	(90)
Therefore, θi (t) is bounded for all times and θi (t) → r	as t → +∞. By construction, it follows that		
θL(t ... θi(t) → Y 2⅛ rY =	λ,	as t → +∞.	(91)
Exponential convergence follows from a careful analysis of the implicit solution of the separable
equation (87), in a similar way as it has been done in Appendix A.2.	口
The proof of convergence of the FA algorithm for a generic initialization (θi(0), . . . , θL(0)), where
each layer is initialized independently, follows the same guidelines as the 2-layer case (Appendix
A.3).
Sketch of the proof. In the same setting as the proof above, by keeping all the constants of integration
K' 6= 0, we reduce the system of ODEs (81) to a single ODE for θi of the form
θi = PY(θi; dι,...,dL,λ)	(92)
where P is a polynomial of degree γ := 2L - 1 in the variable θi with coefficients depending on
the FA constants di , . . . , dL and the signal λ. It is still a separable equation that can be studied via a
careful analysis of its discriminant.	口
17
Under review as a conference paper at ICLR 2022
C Implicit Regularization phenomena
C.1 PROOF OF THEOREM 7 (CASE ∆ > 0)
For readability, we are suppressing the index i from all the formulæ below. Recall the Initial Value
Problem for θ1 (and θ2):
θ1 = -1 (θ3 + 2dKθ1 - 2dλ)
θ1(0) = θ0
(93)
(94)
and θ2(t) = 2dθ2(t) + K, where K = θ2(0)-白θ2. By assuming that the discriminant is positive
∆ = -4d2 (8dK3 + 27λ2) > 0, the cubic polynomial on the right hand side of (93) has 3 real
distinct root r1 < r2 < r3 and the solution of the ODE is (see also Section A.3)
ln ∣θι(t)- r3∣	ln ∣θι(t) - r2∣ l Γ	ln lθ1(t) - r1 | 	+ Co =	t 二—	(95)
r32r31	r32r21	r31r21	2	
C	ln ∣θo - r3∣ l ln ∣θo - r2∣ Co =1	ln ∣θo - rι∣ 		 ,		(96)
r32r31	r32r21	r31r21		
where we set rij := ri - rj.			
If we set as initial value θo = r2 + e-δ (δ > 0), then			
Co = - ɪ - ln(r32 -L)- S + L)；	(97)
r32r21	r32r31	r31r21
we additionally rescale the time variable as t 7→ δt. Rewriting equation (95), we have
r3 - θ1(δt)	r32r31 二 F—-「δ	2 t	 r32r21	+ ln (r32 - e-δ	)+型 r21	ln r21 + e	-δ)	
	+ r31 ln(θι(δt) - r2) r21		—r32 ln (θι(δt) r21	- r1)	,		(98)
θ1(δt) - r2	r32r21 = exp{ --- δ	2 	1 r32r21	+ r21 ln (上 r31	r32	θι(δt)' -e-δ /	)+ 型 ln r31	(θι(δt) - ri 、r2i + e-δ	). (99)
From (98)-(99), We have the following asymptotic expansions: as δ → +∞
for t > T
θ1(δt) → r3
θι(δT) → α := r3 - exp [(1 + r31) lnr32 + r32 ln (r21 )1
r21	r21	r31
θ1(δt) → r2
for t < T
with T
2
r32r21
In the case θo = r2 - e-δ, we can follow a similar argument: as δ → +∞,
θ1(δt) → r1
θ1(δT) → α:= ri +exp [(1 + r31) ln r21 + r21 ln (r32))
r32	r32	r31
θ1(δt) → r2
for t > T
for t < T.
(100)
(101)
(102)
(103)
(104)
(105)
The last bit of result we need is to analyze if the value of the critical value
T
2
r32r21
(106)
is monotonic as λ increases. Recall the cubic polynomial on the right hand side of equation (93)
(without loss of generality, we are setting 2d = 1): P(x) = x3 +Kx - λ = (x -r1)(x -r2)(x -r3),
where λ > 0 and K < - 2 λ2/3 < 0 depends on the initial conditions of the FA system.
18
Under review as a conference paper at ICLR 2022
(a) Numerical simulation of θ1 (t) with r1 = -2, r2 = (b) Plot of θ1 (δt) (∆ > 0), with initial condition
1, r3 = 2 and initial conditions θ0 = r2 + e-δ with θ0 = r2 + e-δ, δ = 20), FA constant d = 0.5, initial
δ∈ {10, 20, 30}.
condition K = -4.
Figure 5: (Anti)-implicit regularization.
The quantity r32r21 can be expressed as r32r21 = -P0(r2) = -3r22 - K, where r2 < 0 for λ > 0.
2
It is easy to see that if λ < λ, then |r2| < |re2| and consequently r32r21 = -r22 - K > -re2 - K =
rf32rf21, i.e.
22
Tλ =------- <	= Tλ	(107)
r32r21	rf32rf21	λ
Therefore the threshold time T is an increasing function of λ. See Figure 5b: in the δ-scaling limit
we see indeed that for smaller values of λ, the step appears sooner.
C.2 EVIDENCE OF INCREMENTAL LEARNING IN THE CASE K = 0
Consider again the system (93) in the case K = 0:
θι = --(θ3 - 2dλ)	(108)
2
θ1(0) = θ0	(109)
and θ2(t) = 2d θ2(t). In this case, the solution reads (See also Section A.2)
-3r2	ln	ιθι ⑴ - r| +	3r2	ln	(θ2 ⑴ + rθι ⑴+r2)+	r2 √3	arctan (-1T√3^	+ +	Co	= t
(110)
C0 = 322 ln 仇-r| - 3r2 ln (θ2 + rθ0 + r2) - r2√3 arctan (2^r√3r)	(III)
With r := √2dλ.
As the limit value of the function θ1 (t) is the positive root r, θ1 (T0) = 0 (for some T0 > 0) if and
only if θ0 < 0. The value of T0 can be easily derived from the implicit expression (110):
∏ l 1	(r - θo)2	ʌ 2	2θo 十 八
rw3 + 3r2ln (俏+rθo+r I- r2√3arctan (-√3r)
(112)
and in the regime as θ0 → -∞
T0 = 3r2√3 - ⅛+O G),	θo →-∞.	(113)
It is now clear that for bigger λ,s (i.e. dominant singular values) the crossing of the x-axis happens
sooner: for λ > λ and fixed FA constant d > 0,
T0,λ < T0,eλ .
(114)
19
Under review as a conference paper at ICLR 2022
D Convergence of the discrete algorithms
We will report here all the proofs of the convergence theorems for the discrete FA dynamics. We will
first discuss convergence guarantees and convergence rates for the modified (mid-point) FA dynamics
for both 2- and L-layer NNs (L ≥ 3), as it is more straightforward. The convergence proof for the
forward Euler discretization method can be found in Section D.2.
D.1 Convergence of FA under the "midpoint" discretization scheme
We recall the 2-layer system that we want to analyze. For simplicity we are suppressing all the i
indices and we are denoting (xt , yt) the unknown functions in the system:
xt+1 = xt + ηd (λ - xtyt)
yt+1 = yt + 2 (λ - Xtyt)(Xt+1 + Xt)
(115)
(116)
We begin with proving a series of easy lemmas that will be instrumental for the proof of the theorem.
Lemma 19. With initial condition (X0, y0) = (0, 0), the second component of the solution of the
system (115)-(116) reads
(117)
2
yt = — V t ≥ 0.
yt	2d	≥
Proof. From equation (115), xt+1-xt = λ 一 Xtyt； substituting in equation (116), We obtain the
result:	(Xt+1 + Xt )(Xt+1 一 Xt )	Xt2+1 一 Xt2
yt+1	=yt +	2d	=% + 二"。+XX x⅛2 = yo - 2d + 号	(118) j=0	'{z~*} =。
via a telescoping sum.	□
Substituting the expression of yt back in equation (115), we have a recurrence relation for Xt of the
form:
Xt+1 = Xt + ηdλ(1 一 Xt) =: f(xt).	(119)
where r :=弋2dλ.
Lemma 20. With parameter
2 _	2
η < 3r2 = 3(2dλ)2 ,
(120)
the function f(X) = X + ηdλ
maps the interval [0, r] into itself: ∀ X ∈ [0, r], f(X) ∈ [0, r].
Proof. By inspecting the function f and its derivative we can conclude that f (x) ≥ 0, V X ∈ [0, r]
and f is increasing on the interval [0, ʌ/ɪ]. Notice also that f (r) = r. Imposing (120) guarantees
that r <、13η, therefore
ηdλ = f(0) ≤ f(X) ≤ f (r),	VX ∈ [0, r].
□
Lemma 21. In the same hypotheses as Lemmas 19 and 20, the sequence {Xt}t≥0 is bounded, positive
and increasing:
0 ≤ Xt ≤ r and Xt ≤ Xt+1,	V t ≥ 0.	(121)
20
Under review as a conference paper at ICLR 2022
Proof. For t = 0, trivially 0 ≤ xo = 0 ≤ r. By induction, assume that 0 ≤ Xt ≤ r, then by Lemma
20
0 ≤ Xt+1 = Xt + ηd 卜-Xd) = f (xt) ≤ f (r) = r.	(122)
Furthermore,
Xt+1 = Xt + 2 (2dλ — x3) ≥ Xt + 2 0dλ — (√2dλ)3) = Xt.	(123)
□
We can now prove Theorem 9 that we rewrite here for convenience:
Theorem 22. With initial conditions (xo, yo) = (0,0), ∀ λ,d > 0 and with step size η <
thefollowing convergence result hold:
2
2
3(2dλ)3
IXtyt- ʌl ≤ 3λ (1 - 2(2dλ)3) .	(124)
Remark 23. Note that if η = -(which is outside the range prescribed in Theorem 9) and
(2dλ)3
xo = 0, we achieve convergence in one step: Xt =g2dλ ∀ t > 0.
Proof. By Lemma 21, ∃ ' ∈ R≥o such that Xt → ' as t → +∞. Furthermore, by solving
' =' + ηdλ (1 - 5)	(125)
it follows that' = r =弋2dλ. Finally, we have
|xt - r| = ∣xt-i - r + 2 (r3 - x3-i)∣ = ∣Xt-i - r| ∣ 1 - 2 (x2-i + rxt-i + r2
≤ ∣Xt-i - r∣ (1 - 3ηr2
≤r
(126)
Note that 0 < 1 -萼 r2 < 1, since η < 条.In conclusion, thanks to the arguments above and
Lemma 19, we have: as t → ∞
Xt3	1	2	2
∣Xtyt - λ∣ = 刀-λ =力 ∣X2 + rxt + r2 ∣ ∣Xt - r∣
2d	2d
3r2	3r3	3η 2∖t
≤ 万 ∣xt - r∣≤ 于(1-工r)
= 3λ(1 - ?r2)t.	(127)
□
For the general L-layer case, we first recall Theorem 10.
Theorem 24. Consider thefollowing system:
碎+i)=噌+ ηθ!-i2)... θf+2)(λ -噌...埠))	(128)
碎+?=喂 i + ηdL-i1?)... ∕+2)(λ -碎)...At))	(129)
.
..	(130)
碎+1) = θ(t) + ηd2 θf+2)(λ -碎)...θ (t))	(131)
θ(t+i)= θ，)+ ηd I (λ - θ(t) ... θ(t))	(132)
21
Under review as a conference paper at ICLR 2022
with zero initialization θ'0) = 0, ∀ ' = 1,...,L. Assume that λ, d` > 0, ∀ ' = 1,...,L 一 1, and the
constant step size satisfies
1
η <----------------T
dιγ (KλY-1)Y
where γ = 2L 一 1 andK is a suitable positive constant depending on the FA constants d1, . . . , dL-1.
Then, the product QL=I θ't) linearly converges to the true signal λ:
W) ...θ(t) - λ∣ ≤ Cλ(1 - ηdιγ (Kλγ-1)1)t	(133)
for some constant Cλ ∈ R>0 only dependent on λ.
Proof. We consider the last equation (132) and substitute
θ(t+1)	θ(t)
λ - θLt... θ(t) = -1_--_(134)
in the second to last one to obtain
-2t+1)= -2t) + a](-尸)2 - ")j ⇒	θ(t) =a ”)2	(135)
by telescoping sum. The same substitution strategy can be consecutively applied to the third to
last equation, fourth to last equation and so on. In general, it is easy to prove by induction that
∀ ` = 2, . . . , L we have
`-1	`-1	`-1
θ't+1) = θ't)	+	C'	(θ(t+1))	-	(θ(t))	⇒	-P=	C'	")	(136)
for some C' = C'(d1, . . . , d') ∈ R>0.
Once derived the relations between the sequences {θ'(t)}'=2,...,L and {θ1(t)}, we can go back to the
recurrence equation (132) for θ1(t) to obtain
θ1(t+1) = θ1(t) +ηd1 hλ - K θ1(t))γ i	(137)
where K = Q'L=1 C' ∈ R>0 and γ = P'L=-01 2' = 2L - 1.
Lemma 25. With initial condition θ1(t) = 0 and for η <
-----1-----1 ,theSeqUenCe {θ(t)} is positive,
dιγ(KλY-1)Y
increasing and bounded by r := γ K.
Proof. The proof follows the same argument as in the 2-layer case. The odd polynomial f(x) = x +
ηd1 (λ - Kxγ) has at least one fixed points at x = r and f(0) = ηd1λ > 0. Furthermore, f ([0, r]) ⊆
1
[0, r]. Indeed, the function is increasing on the interval [0, XmaX] with XmaX = ( d, ɪ K ) Y . If
ηd1γK
η< ——-then r < Xmax, implying that
dιγ(Kλγ-1) Y
0 < ηd1λ = f(0) ≤ f(X) ≤ f(r) = r, ∀X ∈ [0, r].
It follows (by induction) that 0 ≤ θ(t) ≤ r ∀ t ≥ 0 and θ(t) ≤ θ(t+1).	□
From Lemma 25 we can conclude that θ1(t) → r and
θ1(t) - r∣∣∣ = ∣∣∣θ1(t-1) - r - ηd1K	θ1(t-1)
≤ r(1 - ηdιγ (Kλγ-1) Y )
≤ ∣∣∣θ1(t-1) - r∣∣∣ ∣∣1 - ηd1Kγrγ-1
22
Under review as a conference paper at ICLR 2022
Finally, we have the following linear rate of convergence
∣θ(t) ... θ(t) - λ∣ = ∣K (θ(t))γ - λ∣ ≤ Cλ∣θ(t) - r∣ ≤ Cλ(1 - ηdιγ (Kλγ-1)1 )t.	(139)
□
D.2 Discretization under the Euler forward method
By discretizing the FA dynamics via the standard Euler forward method, we obtain the following
recurrence relation for the iterates:
xt+1 = xt + ηd(λ - xtyt)
yt+1 = yt + ηxt (λ - xtyt)
(140)
(141)
with initial condition x0 = y0 = 0. If we use the same substitution strategy as before (λ - xtyt
xt+η-xt), We get
xt+1 - xt	xt
yt+ι = yt + nχt ——而——=Iyt + ɪ(χt+ι - Xt)
=yt+ 方[(Xt+ι + Xt) - (Xx+ι - Xt)] (Xt+ι - Xt) = yt + 2d (χ2+ι - χ2) - 2d (Xt+ι - Xt)2
x2	1	1 t
y0 - 2d +2dXt+i - 2d X(Xj - XjT),
'{z~*}	j=1
=0
(142)
i.e.
11
yt = 2dX2 - 2dSt	Vt ≥0,	(143)
Where St := Pit=1 (Xi - Xi-1)2 and S0 = 0. Notice that 0 ≤ St ≤ St+1 ∀ t ≥ 0, by definition.
Plugging (143) back into equation (140), We get a highly nonlinear recurrence relation Where all the
past terms of the sequence (up to time t) are involved in determining the subsequent term Xt+1:
Xt+1 = Xt + 2 22λλ — X3 + Xt
(144)
We Will noW provide a proof of Theorem 8. The proof is quite long and it is divided into four parts
for readability. We Will first reWrite the statement in the folloWing Way:
Theorem 26. Consider the sequence {(Xt, yt)} defined recursively in (143)-(144) with zero initial-
ization X0 = y0 = 0. For any λ, d > 0, set the constant step size as
2	2
3(S* + 1)2 , max(x,s)∈R P(x, S)
where P(x, S) = 2dλ — x3 + xS, S* is the unique positive solution of P(x, x) = 0, and R is the
following compact, convex set
R := {(x, S) ∈ R≥o | S ≤ x, P(x, S) ≥ 0} .	(146)
Then, the product Xtyt linearly converges to the true signal λ:
∣χtyt - λ∣ ≤ Cqt.	(147)
for some C > 0 and 0 < q < 1.
(145)
η < min
Proof of convergence - part 1. In order to prove convergence of the sequence {Xt}, We Will first
consider a more general sequence defined by the folloWing recurrence relation
Xt+1 = Xt + 2 0dλ — x3 + Xtat)	(148)
23
Under review as a conference paper at ICLR 2022
where {αt } is an auxiliary sequence that we assume to be increasing, positive and bounded (therefore
convergent: αt → α∞ for some α∞ ∈ R>0).
Consider the following fixed-point problem:
X = f (x; α)	with f (x; α) = X + η 0dλ - x3 + ax
(149)
with x ≥ 0 and α ∈ [0, α∞]. Notice that ∀ α ≥ 0, we have that f(0; α) = ηdλ > 0 and
f (χ; α) → -∞ as χ → +∞. Therefore, ∀ α ≥ 0, there exists a unique fixed point 'a > 0 for the
function f (uniqueness follows by directly inspecting the cubic polynomial f (∙; α) with α fixed).
From f = 2x ≥ 0 for X ≥ 0, it follows that f (x; α) ≤ f (x; α0) ∀ α ≤ α0. Furthermore, 'a ≤ 'ao:
indeed, ∀ a ≥ 0, 'a is the unique zero of the cubic polynomial Pa(X) = —x3 + ɑx + 2dλ on the
positive real line (Pα (0) = 2dλ > 0, Pα0 (0) = α > 0 and Pα (x) → -∞ as x → -∞); its partial
derivative ∂P0α = X ≥ 0, for X ≥ 0, implies that as α increases the zero 'a shifts rightwards on R.
On the other hand, from
dfX=1+2α - 2χ2 ≥0
(150)
We obtain that the function f is increasing on the interval [0,	+ 3]. In order to ensure mono-
tonicity on the interval [0,'0], we tune the parameter η such that
Z,	∕^2	α
'α ≤ √3η+3.
(151)
By taking the supremum over α ∈ [0, α∞] on the left hand side and the infimum on the right hand
side, we have
'ɑ∞ ≤ v3η;
such a bound is satisfied if
2
η ≤ 3'2Γ.
α∞
(152)
(153)
As it will be clear in the next parts of the proof (see Remark 29), the value of 'a∞ is smaller than S*,
therefore (145) guarantees that the above bound (153) is satisfied. This implies that
1.	f (x; α) ≤ f (x0; α) ∀ x,x0 ∈ [0, 'a∞], X ≤ x0
2.	f (x; α) maps [0,'s∞ ] into itself ∀ α ∈ [0, a∞].
In conclusion, we have
f(x; α) ≤ f(x0; α0)	∀ x ≤ x0,∀ α ≤ α0.	(154)
We are now ready to prove the following lemma.
Lemma 27. For η ≤ 然—,the Sequence (148) (with zero initial condition) is positive, increasing
α∞
and bounded: ∀ t ≥ 0
0 ≤ xt ≤ xt+1 ≤ 'a∞	(155)
Proof. The proof is by induction: for t = 0 We have 0 = xo ≤ xι = ηdλ ≤ 'α∞ and assuming that
xt-1 ≤ xt, it follows that
0 ≤ Xt = f (xt-1; αt-i) ≤ f (xt; αt) = Xt+1 ≤ f ('a∞; α∞) = 'a∞	(156)
□
24
Under review as a conference paper at ICLR 2022
Therefore the sequence (148) converges: xt → ω for some ω ∈ R>0. By construction, it is clear that
ω = 'α∞. Indeed, given
xt+ι = Xt + 2 (2dλ - χ3 + Xtat)	(157)
and taking the limit as t → ∞ on both sides of the equation, we obtain
ω = ω + (2dλ — ω* + ωa∞),	(158)
i.e. ω is a (positive) solution of the fixed point problem X = f (x; a∞) whose unique solution is 'a∞.
A major problem arises when we consider the case of αt = St = Pit=1(Xi -Xi-1)2, i.e. the auxiliary
sequence {αt} depends directly on the primary sequence {Xt}. Clearly, 0 ≤ αt ≤ αt+1 for all times
t ≥ 0, but the boundedness property is not straightforward.
Boundedness of the sequence of partial sums {St}. Since St+1 = St + (Xt+1 - Xxt)2, consider
the recurrence relations:
χt+ι = χt + 2 (2dλ - χ3 + XtSt)
2
St+1 = St + ɪ (2dλ - x3 + XtSt)
Lemma 28. Given the convex, compact set
R := {(x, S) ∈ R≥o | S ≤ X, P(x, S) ≥ 0},
with P(X, S) = 2dλ - X3 + XS, and
2
2
η< mint 3(S* + 1)2 , maX(x,s)∈R P(x,S)「
(159)
(160)
(161)
(162)
where S * is the unique positive solution to P (x,x) = 0, and with initial values xo = So = 0, the
sequence {(Xt, St)} defined recursively in (159)-(160) is bounded and lies in the set R.
See Figure 6 for a sketch of the region R.
Proof. The set of fixed points of the system is the set of zeros of the function P(X, S),
X = X + ηP(x, S)	/	、	3
S = S + 2η2 P(XS)2	⇔	P(X,s) = 2dλ - X3 + XS = 0,	(163)
which is an algebraic curve in R2, with solutions in (X, S) ∈ R2≥o
S=X2
2dλ
(164)
—
X
;
furthermore, there exists a unique solution (S*, S*) ∈ R2>o such that P(S*, S*) = 0: indeed, S* is
the unique solution to the equation X3 - X2 + 2dλ = 0 and it’s explicit expression can be recovered
from Cardano’s formula (if needed).
We will prove the theorem by induction. For t = 0, trivially 0 = So = Xo and P(Xo, So) = 2dλ > 0.
For t = 1,
Si = η2d2λ2 ≤ Xi = ηdλ if η ≤ ɪ	(165)
dλ
P(Xi,Sι) = 2dλ — (ηdλ)3 + ηdλ ∙ (ηdλ2) = 2dλ > 0.	(166)
It is easy to notice that if (162) is satisfied, then η ≤ 六 as required in (165).
Since R is compact, the value of the polynomial P(X, S) is bounded and positive for all (X, S) ∈ R:
0 < 2dλ = P(Xo, So) ≤ maX P(X, S) < +∞.
(x,s)∈R
25
Under review as a conference paper at ICLR 2022
Figure 6: The compact, convex set R ⊂ R2≥0
Assume that ∀ s = 1, . . . , t we have
Ss ≤ xs,	P(xs, Ss) = 2dλ - xs3 + xsSs ≥ 0	(167)
then,
St+1
2
St + WP(Xt, St)2 ≤ Xt + IP(Xt, St) ∙ 2P(Xt, St) ≤ Xt + PP(Xt, St)
l≤z}	4	2	1--------∙	2
'------
≤1
Xt+1 ,
(168)
thanks to the induction step and (162). Additionally,
P(Xt+1, St+1) = P(Xt, St) + VP(X, S) ∙ η2P(Xt，St)2
(169)
for some (X, S) belonging to the line segment connecting the point (Xt, St) to the point (Xt+ι, St+ι),
by the Mean Value Theorem;
2
P(Xt+1, St+1) = P(Xt, St) + 2P(Xt, St) (-3x + S) + 4-XP(Xt, St)2
2
=P(Xt, St) 1 + 2(-3x2 + S) + WXP(Xt, St)
≥ P(xt, St) 1 2x2 .	(170)
Notice now that 0 ≤ X = Xt + δ(xt+ι 一 Xt) = Xt + δ2P(xt, St), for some δ ∈ [0,1]; since
(Xt , St ) ∈ R by induction,
X ≤ S* + 1 ∙ η max冗 P(x,S) ≤ S* + 1.	(171)
'---，---{--------}
≤1
Therefore,
P(Xt+1,St+l) ≥ P(Xt,St)	1---2χ2	≥ P(Xt,St)	1---2 (S* +	1)2	≥	0,	(172)
by (162). The result follows.
□
Proof of convergence - part 2. Theorem 28 above implies that the sequence of partial sums
{St = Pit=1(Xi - Xi-1)2} converges: St → S∞ := Pi∞=0(Xi - Xi-1)2 < +∞ and we can resort
to the arguments in the first part of the proof to conclude that Xt → 's∞.
26
Under review as a conference paper at ICLR 2022
Remark 29. Note that 's∞ is the unique positive solution to the equation P(x, S∞) = 0 and, since
the sequence {(xt, St)} ⊂ R, 's∞ lies on the boundary of R. This implies that 's∞ ≤ S*.
Finally, the product Xtyt → λ as t → +∞. Indeed, since Xt → 's∞ =: L, then, by construction
yt will also converge to some value L (because of (143)). Moreover, L is the fixed point of the
recurrence equation (144):
L = L + 2 (2dλ-L + LS∞),	(173)
which can be alternatively written as
L = L + nd (λ - lL) ;	(174)
L is fixed point if and only if LL = λ, implying Xtyt → λ.
Linear rates. The last piece of results that needs to be proven is the linear rate of convergence.
Consider the difference '$ - xt, where '$ is the unique positive solution to P(x, St) = 0 and by
construction Xt ≤ St :
—
(175)
(176)
(177)
We will bound each term B1 and B2 separately. The first term can be easily estimated:
Bi ≤ 1 - 2 ((S*)2 + S*'s∞ + 2dλ) =1 - nM
where we defined
M := (S*)2 + S*'s∞ +(2dλ)2
(recall that 's0 = √ 2dλ).
In order to estimate B2, we will first prove a series of useful lemmas that illustrate some properties of
the sequence {'sj.
Lemma 30. ∀ t ≥ 0,
2dλ	2	2dλ
0 <七 ≤ 'St- St ≤ 总.
(178)
Proof. By definition of '&,
P('t, St) = 2dλ - 'St + 'StSt = 0	(179)
i.e.
'St - St = -I)- > 0.	(180)
t	'St
Using the fact that {'St } is increasing and bounded ('S0 ≤ 'St ≤ 'S∞, ∀ t ≥ 0), the result
follows.	□
Lemma 31. ∀ t ≥ 0,
Xt+1 - Xt ≤ ηM ('St - Xt)
(181)
with M defined in (177).
27
Under review as a conference paper at ICLR 2022
Proof. The proof easily follows from the recurrence relation of the (increasing) sequence {χt}:
xt+1 — xt = 2 (2dλ — x3 + XtSt)
≤ ('St — Xt) ∙ 2 ((S*)2 + S*'s∞ + (2dλ)3)	(182)
、------------V------------}
=ηM
□
Lemma 32. ∀ t ≥ 0,
'St+1 - 'St ≤ C∞ (Xt+1 — Xt)2	(183)
for some COnStant C∞ ∈ R>0.
Proof. Using again the definition of 'st, We have
'St+1 — 'St = 'St+1 st+1 — 'st St
=('St+1 — 'St) st+1 + 'St (St+1 — St)
=('St+1 — 'St) St+1 + 'St (Xt+1 — Xt)2 ；	(184)
on the other hand,
'St+1 — 'St = ('St+1 — 'St) ('St+1 + 'St+1 'St + 'St) ；	(185)
combining (184) and (185) and using Lemma 30, we obtain
'St+1
'St
—
_______'St (Xt+1 一 Xt)2_______
'St+1 + 'St+1 'St + 'St - St+1
≤
's∞ (Xt+1 - Xt)2
2'So + 爰
's∞
2(2dλ)3 + 冷
S∞
S------V------
=Q∞
(Xt+1 - Xt 产.
(186)
□
Collecting the results from Lemmas 31 and 32, we have
B2 ≤ Cg(Xt+1 — Xt)I2 ≤ η2Mc∞ ('St - Xt)I2	(187)
In conclusion, for η < M We have
'St+1 — Xt+1 ≤ ('St — Xt)(I — ηM) + η2M2c∞ ('St — Xt)2
=('St — Xt) (1 — ηM + η2M)
≤ Λ√2dλ (1 — ηM + η2M)t	(188)
1	Tl~4^	rʌ Λ Tl /Γr) C	zʌ Z	1 ,1 Γ∙ ,,1	. C fl 1	1 C 1	1	1	1∖	,	,1
where M := 2's∞M2C∞ > 0 (we used the fact that {'sJ and {Xt} are bounded); we stress that
the quantity 1 — ηM — η2M is smaller than 1 (thus being indeed a convergence rate) for η small
enough.
28
Under review as a conference paper at ICLR 2022
E Experiments
E.1	Solution of the FA ODE system
Figures 1, 3, 4, 5, plotting solutions (and products of solutions) of the FA system
: ,一 - -.. .
Θl = (λ - Θl …Θi)Θl-i ...θι	(189)
Θl-i = dL-ι(λ — Θl ... Θi)Θl-2 ...θι	(190)
.
..	(191)
θ2 = d，2 (λ — Θl ...θ1)θ1	(192)
: _ - -.
θι = dι(λ — Θl ...θι)	(193)
in the continuous setting (t ∈ R≥0) are obtained using the standard ODE solver ode45 on Matlab
(version R2020b).
E.2 Linear autoencoders
We set up two linear autoencoders
y^ = W2W1x	(2-layer NN)	(194)
y^ = W3W2 Wix	(3-layerNN	(195)
where w` ∈ Rd×d, d = 20 and the data are synthetically generated as
Xi = Azi + €i,	Zi 〜N(0, Ih), €i 〜10-3N(0, Id)	(196)
h = 5 and A ∈ Rd×h is a fixed matrix that We sampled with entries Aij 〜 U ([0, 1]).
For the experiments showed in Figure 2, we set the step size η = 0.01 and we initialized the weight
matrices as W1(0), W2(0), W3(0) SUCh that W(O)〜10-5U([0,1]).
The FA matrices m` are generated as Me；j 〜U([0,1]). We repeat the FA training for 15 times,
sampling a different set of FA matrices each time (but same initial conditions W(0) and same matrix
A): in FigUre 2 we then report the average of the trace norm and the reconstrUction error with error
bars calculated as mean ± 2・standard_deviation (the factor 2 is to make the error bars more
visible in the final plot). We complement these experiments with the corresponding GD training.
29