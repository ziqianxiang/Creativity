Under review as a conference paper at ICLR 2022
Denoising Diffusion Gamma Models
Anonymous authors
Paper under double-blind review
Ab stract
Generative diffusion processes are an emerging and effective tool for image and
speech generation. In the existing methods, the underlying noise distribution of
the diffusion process is Gaussian noise. However, fitting distributions with more
degrees of freedom could improve the performance of such generative models. In
this work, we investigate other types of noise distribution for the diffusion process.
Specifically, we introduce the Denoising Diffusion Gamma Model (DDGM) and
show that noise from Gamma distribution provides improved results for image and
speech generation. Our approach preserves the ability to efficiently sample state in
the training diffusion process while using Gamma noise.
1	Introduction
Deep generative neural networks have shown significant progress over the last years. The main
architectures for generation are: (i) VAE (Kingma & Welling, 2013) based, for example, NVAE
(Vahdat & Kautz, 2020) and VQ-VAE (Razavi et al., 2019), (ii) GAN (Goodfellow et al., 2014) based,
for example, StyleGAN (Karras et al., 2020) for vision application and WaveGAN (Donahue et al.,
2018) for speech (iii) Flow-based, for example Glow (Kingma & Dhariwal, 2018) (iv) Autoregessive,
for example, Wavenet for speech (Oord et al., 2016) and (v) Diffusion Probabilistic Models (Sohl-
Dickstein et al., 2015), for example, Denoising Diffusion Probabilistic Models (DDPM) (Ho et al.,
2020) and its implicit version DDIM (Song et al., 2020a).
Models from this last family have shown significant progress in generation capabilities in the last years,
e.g., (Chen et al., 2020; Kong et al., 2020b), and have achieved results comparable to state-of-the-art
generation architecture for both images and speech.
A DDPM is a Markov chain of latent variables. Two processes are modeled: (i) a diffusion process
and (ii) a denoising process. During training, the diffusion process learns to transform data samples
into Gaussian noise. Denoising is the reverse process and it is used during inference for generating
data samples, starting from Gaussian noise. The second process can be conditioned on attributes to
control the generation sample. To obtain high-quality synthesis, a large number of denoising steps is
used (i.e. 1000 steps). A notable property of the diffusion process is a closed-form formulation of
the noise that arises from accumulating diffusion stems. This allows sampling arbitrary states in the
Markov chain of the diffusion process without calculating the previous steps.
In the Gaussian case, this property stems from the fact that adding Gaussian random variables leads
to another Gaussian random variable. Other distributions have similar properties. For example, for
the Gamma distribution, the sum of two random variables that share the scale parameter is a Gamma
random variable of the same scale. The Poisson distribution has a similar property. However, its
discrete nature makes it less suitable for DDPM.
In DDPM, the mean of the Gaussian random variables is set at zero. The Gamma random variable,
with its two parameters (shape and scale), is better suited to fit the data than a Gaussian random
variable with one degree of freedom (scale). Furthermore, the Gamma random variable generalizes
other distributions, and many other distributions can be derived from it (Leemis & McQueston, 2008).
The added modeling capacity of the Gamma random variable can help speed up the convergence
of the DDPM model. Consider, for example, a conventional DDPM model that was trained with
Gaussian noise on the CelebA dataset (Liu et al., 2015).
The noise distribution throughout the diffusion process can be visualized by computing the histogram
of the estimated residual noise in the generation process. The estimated residual noise ^ is given by
1
Under review as a conference paper at ICLR 2022
(a)	(b)
Figure 1: Fitting a distribution to the histogram of the generation error, which given by the scaled
difference between x0 and the image Xt after t DDPM steps ^ = ，^x0-xt. The model is a pretrained
DDPM (Gaussian) celebA (64x64) model. (a) The fitting of a Gaussian to the histogram of a
typical image after t - 50 steps. (b) Fitting a Gamma distribution. (c) The fitting error to Gaussian
and Gamma distribution, measured as the MSE between the histogram and the fitted probability
distribution function. Each point is the average value for the generation of 100 images. The vertical
error bars denote the standard deviation.
(c)
^ = ∖√tx0-χt, where αt is the noise schedule, x0 is the data point and Xt is the estimate state at
timestep t, as can be derived from Eq.4 from (Song et al., 2020a). Both a Gaussian distribution and
Gamma distribution can then be fitted to this histogram, as shown in Fig. 1(a,b). As can be seen,
the Gamma distribution provides a better fit to the estimated residual noise ^. Moreover, Fig. 1(c)
presents the mean fitting error between the histogram and the fitted probability distribution function.
Evidently, the Gamma distribution is a better fit than the Gaussian distribution. While the model was
trained to estimate Gaussian noise, at inference time it has to address a different distribution.
In this paper, we investigate the non-Gaussian Gamma noise distribution. As noted, this distribution
seems to fit the histogram of the generation error better than the Gaussian distribution, and it also
has favorable properties such as its behavior under addition and scalar multiplication. The proposed
models maintain the property of the diffusion process of sampling arbitrary states without calculating
the previous steps. Our results are demonstrated in two major domains: vision and audio. In the first
domain, the proposed method is shown to provide a better FID score for generated images. For speech
data, we show that the proposed method improves various measures, such as Perceptual Evaluation
of Speech Quality (PESQ) and short-time objective intelligibility (STOI).
2	Related Work
In their seminal work, Sohl-Dickstein et al. (2015) introduce the Diffusion Probabilistic Model. This
model is applied to various domains, such as time series and images. The main drawback in the
proposed model is that it needs up to thousands of iterative steps to generate a valid data sample. Song
& Ermon (2019) proposed a diffusion generative model based on Langevin dynamics and the score
matching method (Hyvarinen & Dayan, 2005). The model estimates the Stein score function (LiU
et al., 2016) which is the gradient of the logarithm of data density. Given the Stein score function, the
model can generate data points.
Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020) combine generative models
based on score matching and neural Diffusion Probabilistic Models into a single model. Similarly, in
Chen et al. (2020); Kong et al. (2020a) a generative neural diffusion process based on score matching
was applied to speech generation. These models achieve state-of-the-art results for speech generation,
and show superior results over well-established methods, such as Wavernn (Kalchbrenner et al.,
2018), Wavenet (Oord et al., 2016), and GAN-TTS (BinkOWSki et al., 2019).
Diffusion Implicit Models (DDIM) offer a way to accelerate the denoising process (Song et al.,
2020a). The model employs a non-Markovian diffusion process to generate a higher quality sample.
The model helps reduce the number of diffusion steps, e.g., from a thousand steps to a few hundred.
2
Under review as a conference paper at ICLR 2022
Algorithm 1 DDPM training procedure.		Algorithm 2 DDPM sampling algorithm	
1	Input: dataset d, diffusion process length T,	1	XT 〜N(0,I)
	noise schedule β1 , ..., βT	2	for t= T, ..., 1 do
2	: repeat	3	Z 〜N(0,I)
3	xo 〜d(xo)	4	ε = εθ(Xt, t)
4	t 〜U ({1,…,T})		1-αt Xt	/1	- ε / I 	 f∖/ J
5	ε 〜N(0,I)			5	XtT=	⅛T
6	Xt = √αt xo + √1 — &t ε	6	if t 6= 1 then
7	: Take gradient descent step on:	7	Xt-1 = Xt-1 + σtz
	kε - εθ(xt,t)k1	8	end if
8	: until converged	9	end for
		10	return X0
Dhariwal & Nichol (2021) find a better diffusion architecture through a series of exploratory experi-
ments, leading to the Ablated Diffusion Model (ADM). This model outperforms the state-of-the-art
in image synthesis, which was previously provided by GAN based-models, such as BigGAN-deep
(Brock et al., 2018) and StyleGAN2 (Karras et al., 2020). ADM is further improved using a novel
Cascaded Diffusion Model (CDM). Our contribution is fundamental and can be incorporated into the
proposed ADM and CDM architectures.
Watson et al. (2021) proposed an efficient method for sampling from diffusion probabilistic models by
a dynamic programming algorithm that finds the optimal discrete time schedules. Choi et al. (2021)
introduces the Iterative Latent Variable Refinement (ILVR) method for guiding the generative process
in DDPM. Moreover, Kong & Ping (2021) systematically investigates fast sampling methods for
diffusion denoising models. Lam et al. (2021) propose bilateral denoising diffusion models (BDDM),
which take significantly fewer steps to generate high-quality samples.
Huang et al. (2021) derive a variational framework for likelihood estimating of the marginal likelihood
of continuous-time diffusion models. Moreover, Kingma et al. (2021) shows equivalence between
various diffusion processes by using a simplification of the variational lower bound (VLB).
Song et al. (2020b) show that score-based generative models can be considered a solution to a
stochastic differential equation. Gao et al. (2020) provide an alternative approach for training an
energy-based generative model using a diffusion process.
Another line of work in audio is that of neural vocoders based on a denoising diffusion process.
WaveGrad (Chen et al., 2020) and DiffWave (Kong et al., 2020a) are conditioned on the mel-
spectrogram and produce high-fidelity audio samples, using as few as six steps of the diffusion
process. These models outperform adversarial non-autoregressive baselines. Popov et al. (2021)
propose a text-to-speech diffusion base model, which allows generating speech with the flexibility of
controlling the trade-off between sound quality and inference speed.
Diffusion models were also applied to natural language processing tasks. Hoogeboom et al. (2021)
proposed a multinomial diffusion process for categorical data and applied it to language modeling.
Austin et al. (2021) generalize the multinomial diffusion process with Discrete Denoising Diffusion
Probabilistic Models (D3PMs) and improve the generated results for the text8 and One Billion Word
(LM1B) datasets.
3	Diffusion models for Gamma Distribution
We start by recapitulating the Gaussian case, after which we derive diffusion models for the Gamma
distribution.
3.1	Background - Gaussian DDPM
Diffusion networks learn the gradients of the data log density:
s(y) = Vy log p(y)
(1)
3
Under review as a conference paper at ICLR 2022
By using Langevin Dynamics and the gradients of the data log density Vy logp(y), a sample
procedure from the probability can be done by:
yi+ι=yi+2 s(yi)+√ηzi	⑵
where Zi 〜N(0, I) and η > 0 is the step size.
The diffusion process in DDPM (Ho et al., 2020) is defined by a Markov chain that gradually adds
Gaussian noise to the data according to a noise schedule. The diffusion process is defined by:
T
q(x1:T |x0) =	q(xt|xt-1) ,	(3)
t=1
where T is the length of the diffusion process, and xT, ..., xt, xt-1, ..., x0 is a sequence of latent
variables with the same size as the clean sample x0 . The Diffusion process is parameterized with a
set of parameters called noise schedule (β1, . . . βT), which defines the variance of the noise added at
each step:
q(xt∣Xt-i) := N(Xt, √1 — βtXt-i,βtI) ,	(4)
Since we are using a Gaussian noise random variable at each step, the diffusion process can be
simulated for any number of steps with the closed formula:
Xt = √0txo + λ∕1 — &t ε ,	(5)
where a = 1 一 βi, & =Qit=1 αiandε = N(0, I).
Diffusion models are a class of generative neural network of the form pθ(X0) = pθ (X0:T)dX0:T
that learn to reverse the diffusion process. One can write that:
T
pθ (X0:T) = p(XT)	pθ(Xt-1 |Xt)	(6)
t=1
As described in (Ho et al., 2020), one can learn to predict the noise present in the data with a network
θ and sample from pθ (Xt-1 |Xt) using the following formula:
Xt-1
χt 一 √-ααtεθ(χt,t)
+ σtε ,
(7)
√0t
where ε is white noise and σt is the standard deviation of added noise. (Song et al., 2020a) use
σt2 = βt.
The training procedure ofεθ is defined in Alg.1. Given the input dataset d, the algorithm samples ,
X0 and t. The noisy latent state Xt is calculated and fed to the DDPM neural network εθ . A gradient
descent step is taken in order to estimate theε noise with the DDPM network εθ . The objective for
the diffusion model is a variational bound on the model data log likelihood.
The complete inference algorithm present at Alg. 2. Starting from Gaussian noise and then reversing
the diffusion process step-by-step, by iteratively employing the update rule of Eq. 7. To perform
generation with few denoising iterations one can use the update equation introduced in Song et al.
(2020a). This work greatly improves the results of diffusion networks when performing sampling
with few generative steps.
Xn-1 = √¾-iX0,n + √1 一 an-1 一 6^ɛθ(Xn,即)+ 39ε ,	(8)
Intuitively this equation changes the added noise for the generative steps. It uses a blend
of the noise from the previous state (ɛθ(xn,&n) and random noise (ε). One can write σ =
η,βn(1 - an-ι)(1 - an) this allows to have a simple parameter η to choose the ratio of the
blend. On top of improving the results in short samplings regimes, this allows to generate in a
deterministic way when using η = 0.
4
Under review as a conference paper at ICLR 2022
3.2	Denoising Diffusion Gamma Models (DDGM)
We expand the framework of diffusion generative processes by incorporating a new noise distribution,
namely the Gamma Distribution. We call this new type of models Denoising Diffusion Gamma
Models. First, we define the Gamma diffusion process, then we present a way to sample from this
process, and finally we show how to train those models by computing the variational lower bound
and deriving a novel loss function from it.
3.2.1	The Gamma Model
In the Gaussian case the diffusion equation (Eq. 4) can be written as:
Xt = √1 - βtXt-1 + VZetet	(9)
where t is the Gaussian noise of step t. One can denote Γ(k, θ) as the Gamma distribution, where
k and θ are the shape and the scale respectively. We modify Eq. 9 by adding, during the diffusion
process, noise that follows a Gamma distribution:
Xt = p1 - βxt-1 + (gt - E(Ot))	(10)
βt	［八…	F
where gt 〜Γ(kt, θt), θt = √αtθo and k =----2.Note that θ° and βt are hyperparameters (and
αtθ0
the noise term has zero mean.)
Since the sum of Gamma random variables (with the same scale parameter) is distributed as Gamma
distribution, one can derive a closed form for Xt, i.e. an equation to calculate Xt from X0:
χt = √αtχo + (gt - ktθt)	(ii)
where gt 〜Γ(gt, Ot) and Ig = Pt=I ki.
βt
Lemma 1. Let θo ∈ R, Assuming Vt ∈ {1, ...,T}, kt = --2, θt = √αtθo, and gt 〜Γ(kt,θt).
αtθ02
Then ∀t ∈ {1, ..., T } the following hold:
E(gt - E(gt)) = 0, V(gt - E(gt)) = βt	(12)
Xt = √⅞X0 + (gt - E(gt))
where gt 〜Γ(gt, θt) and gt = £；=1 ki
(13)
The complete proof for Lemma 1 is given in Appendix A.1.
Similarly to Eq.7, the inference is given by:
Xt-1
xt - √1-ααtεθ(χt,t)
+ σt gt -Egt)
pv(g)
(14)
In Algorithm 3 we describe the training procedure. As input we have the: (i) initial scale θ0 , (ii) the
dataset d, (iii) the maximum number of steps in the diffusion process T and (iv) the noise schedule
β1 , ..., βT . The training algorithm sample: (i) an example X0, (ii) number of step t and (iii) noise
ε. Then it calculates Xt from X0 by using Eq.11. The neural network εθ has an input Xt and is
conditional on the time step t. Next, it takes a gradient descent step to approximate the normalized
noise gt-ktθt with the neural network εj. The main changes between Algorithm 3 and the single
V L
Gaussian case (i.e. Alg. 1) are the following: (i) calculating the Gamma parameters, (ii) Xt update
equation and (iii) the gradient update equation.
The inference procedure is given in Algorithm 4. It starts from a zero mean noise XT sampled from
Γ(θT, kgT). Next, for T steps the algorithm estimates Xt-1 from Xt by using Eq.14. Note that as
in (Song et al., 2020a) σt = βt . Algorithm 4 replaces the Gaussian version (i.e. Alg. 2) with the
following: (i) the starting sampling point XT, (ii) the sampling noise z and (iii) the Xt update equation.
5
Under review as a conference paper at ICLR 2022
Algorithm 3 Gamma Training Algorithm				Algorithm 4 Gamma Inference Algorithm	
1	Input: initial scale θo, dataset d, diffusion			1	γ 〜 Γ(θT , kT )
	process length T, noise schedule β1, ..., βT			2	XT = Y — θτ * kτ
2	: repeat			3	for t = T, ..., 1 do
3		X0 〜 d(X0)			Xt——1-αt e(xt,t)
4	t 〜U({1,...,T})			4	ι>-,——	V1 αt	
					XtT =	√ɑ
5	gt 〜Γ(kt,θt)			5	if t > 1 then
6		Xt = √αt Xo + (kt 一	kktθt)	6	z 〜 Γ(θt-1, kkt-1)
7	: Take a gradient descent step on:			7	= = Z-θt-i 鼠-1
8	: un	√⅛ 一εθ (Xt，t) til converged		8 9 10	=√(l-at) Xt-1 = Xt-1 + σtz end if end for
3.2.2 The Reverse Process for DDGM
The reverse process q(xt-1 |x0, xt) defines the underlying generation process. Therefore, in this
section, we will obtain the reverse process for the Gamma denoising diffusion model. Furthermore, we
will use the reverse process q(xt-1 |x0, xt) to obtain the variational lower bound and the appropriate
loss function for the Gamma distribution denoising diffusion model.
Lemma 2. Denote q(xt-1 |x0, xt) as the reverse process of the proposed Gamma diffusion model.
Then, the reverse process is proportional to:

q(xt-i∣xo,xt) H
χ kt-1e-Xt∕θt X kt-1-1e-Xt-ι∕θt-ι
XktTe-Xt/θt
(15)
where
1.	Xt = Xt — λ∕1 — βtxt-1 + kt θt
2.	Xt = Xt 一 √αtX0 + ktθt
3.	Xt-I = xt-1 一 √αt-1χ0 + kt-1θt-1
The complete proof for Lemma 2 is given in Appendix A.1. It states that the reverse process is
proportional to three Gamma random variables Xt, Xkt-1, and Xkt. This observation allows us to
develop the associated variational lower bound.
3.2.3 Variational Lower Bound for DDGM
Denoising diffusion models (Ho et al., 2020) trained by optimizing the usual variational bound on
negative log likelihood:
E [-log(pθ(X0)] ≤ Eq
一 log p(XT ) 一	log
t≥1
Pθ (xt-1∣Xt)
q(χt∣χt-ι).
LVLB
(16)
To get the variational lower bound for the proposed Gamma denoising diffusion model, one can use
Eq.5 from Ho et al. (2020):
LV LB = Eq
LT +	Lt-1 + L0
t>1
(17)
where LT , Lt-1 and L0 define by:
1.	LT = DKL(q(XT |X0)||q(XT))
2.	Lt-1 = DKL(q(xt-1∖xo,Xt)∖∖Pθ(xt-1∣X0,Xt))
3.	Lo = ―log(pθ(X0∣χι))
6
Under review as a conference paper at ICLR 2022
It should be noted that in the Gaussian case the KL terms have a closed form. LT is constant and
ignored during training since it doesn’t have learnable parameters. Moreover, in (Ho et al., 2020) L0
modeled with discrete decoder, however, in our proposed model we empirically found that the impact
L0 is negligible and can be removed.
Therefore, to calculate the variatonal lower bound one needs to obtain:
Lt-1 = DκL(q(xt-1∣X0,Xt)∣∣Pθ (xt-l∣X0,Xt))	(18)
where:
Xo ,	_ χt - √∕1 - αtεθ (xt,t)	门°、 =	7≡Fz	(19) V at
Lemma 3. The Lt-1 for the proposed Gamma diffusion model is upper bounded by the following L1
norm:
Lt-I ≤ CC1 + C2 + -∑- + _ 4 ) ∣x0 - x0∣	(20)
∖	gt	gt-1J
whereC1,C2,C3 and C4 are constant terms.
The complete proof for Lemma 3 is given in Appendix A.1.
As can be seen, the variational lower bound is bounded by some constant forms multiplied by the L1
norm between the data point xo and its estimation X°. The constant terms Ci, C2, C3 and C4 as well
as gt and g— are known values during the training.
3.2.4 Loss Function for DDGM
Denoising diffusion probabilistic models use the variational lower bound to minimize the negative
log likelihood. As described in Sec.3.2.1, one can minimize the variational lower bound by Lt for
t ≥ 1. To do so, one can minimize the L1 norm from Eq.34. Our model optimizes the L1 norm
between the sampled noise θ and the estimated noise εθ . This is verified in the following lemmas.
Lemma 4. Minimizing the variational lower bound for DDGM (i.e. Lt for t ≥ 1) is equivalent to
minimizing the L1 norm between the sampled noise and the estimated noise:
L = g√ [ &t°t - εθ(χt,t)	QI)
√1 - at
The complete proof for Lemma 4 is given in Sec.A.4 at the appendix. Thus, the loss that is used in
the Alg.3 is given by L = gt	"t - ɛθ(xt, t).
√1 - at
4	Experiments
4.1	Speech Generation
For our speech experiments we used a version of Wavegrad (Chen et al., 2020) based on this
implementation Vovk (2020) (under BSD-3-Clause License). We evaluate our model with high-level
perceptual quality of speech measurements, PESQ (Rix et al., 2001) and STOI (Taal et al., 2011). We
used the standard Wavegrad method with the Gaussian diffusion process as a baseline. We use two
Nvidia Volta V100 GPUs to train our models.
For all the experiments, the inference noise schedules (β0, .., βT) were defined as described in the
Wavegrad paper (Chen et al., 2020). For 1000 and 100 iterations the noise schedule is linear, for 25
iterations it comes from the Fibonacci and for 6 iterations we performed a model-dependent grid
search to find the best noise schedule parameters. For other hyper-parameters (e.g. learning rate,
batch size, etc) we use the same as in Wavegrad (Chen et al., 2020). Training was performed using
βt	1	1	,	,
the following form of Eq. 10, e.g. θt = √atθo and kt =-——2. Our best results were obtained using
αtθo
θ0 = 0.001.
7
Under review as a conference paper at ICLR 2022
Table 1: PESQ and STOI metrics for the LJ dataset for various Wavegrad-like models.
Model \ Iteration	6	PESQ ⑴		STOI (↑)			
		25	100	1000	6	25	100	1000
WaveGrad (Chen et al., 2020) 2.78		3.194 3.211	3.290	0.924	0.957	0.958	0.959
DDGM (ours)	3.07 3.208 3.214 3.308			0.948	0.972	0.969 0.969	
Table 2: FID (1) score comparison for CelebA(64x64) dataset. Lower is better.
Model \ Iteration	10	20	50	100	1000
DDPM (Ho et al., 2020)	299.71	183.83	71.71	45.2	3.26
DDGM - Gamma Distribution DDPM (ours)	35.59	28.24	20.24	14.22	4.09
DDIM (Song et al., 2020a)	17.33	13.73	9.17	6.53	3.51
DDGM - Gamma Distribution DDIM (ours)	11.64	6.83	4.28	3.17	2.92
Table 3: FID (1) score comparison for LSUN Church (256x256) dataset. Lower is better. The results
of DDIM for T = 1000 are not reported by Song et al. (2020a).
Model \ Iteration	10	20	50	100	1000
DDPM (Ho et al., 2020)	51.56	23.37	11.16	8.27	7.89
DDGM - Gamma Distribution DDPM (ours)	28.56	19.68	10.53	7.87	6.91
DDIM (Song et al., 2020a)	19.45	12.47	10.84	10.58	NA
DDGM - Gamma Distribution DDIM (ours)	18.11	11.32	10.31	8.75	7.34
Table 4: FID (1) score comparison for ImageNet (64x64) dataset. Lower is better.
Model \ Iteration	10	20	50	100	1000
DDIM (Song et al., 2020a)	42.88	35.40	31.98	30.74	28.81
DDGM - Gamma Distribution DDIM (ours)	42.17	31.84	28.75	27.02	24.22
Table 5: FID Q) score comparison during the inference process for CelebA (64x64) dataset for the
full inference procedure and for an earlier stage in the process.
Iteration	T=10	T=20	T=100
Model	(first 5 steps/10 steps) (first 10 steps/20 steps) (first 50 steps/100 steps)		
DDIM (Song et al., 2020a)	54.32/17.33	43.35/13.73	36.68/6.53
DDGM (ours)	55.40/11.64	42.43/6.83	35.20/3.17
Results Tab. 1 presents the PESQ and STOI measurement for the LJ dataset (Ito & Johnson, 2017).
As can be seen, for the proposed Gamma denoising diffusion model our results are better than the
Wavegrad baseline for all number of iterations in both PESQ and STOI.
4.2	Image Generation
Our model is based on the DDIM implementation available in (Jiaming Song & Ermon, 2020) (under
the MIT license). We trained our model on three image datasets (i) CelebA 64x64 (Liu et al., 2015),
(ii) LSUN Church 256x256 (Yu et al., 2015) and (iii) ImageNet 64x64 (Deng et al., 2009). The
FreChet Inception Distance (FID) (Heusel et al., 2017) is used as the benchmark metric. For all
experiments, similarly to previous work (Song et al., 2020a), we compute the FID score with 50, 000
generated images, using the torch-fidelity implementation (Obukhov et al., 2020). Similar to (Song
et al., 2020a), the training noise schedule β1, ..., βT is linear with values raging from 0.0001 to 0.02.
For other hyperparameters (e.g. learning rate, batch size etc) we use the same parameters that appear
in DDPM (Ho et al., 2020). We use eight Nvidia Volta V100 GPUs to train our models. The θ0
parameter for Gamma distribution set to 0.001.
8
Under review as a conference paper at ICLR 2022
Figure 2: Typical examples of images generated with 100 iterations and η = 0. For models trained
with different noise distributions - (i) First row - Gaussian noise and (ii) Second row - Gamma noise.
All models start from the same noise instance.
Results We test our models with the inference procedure from DDPM (Ho et al., 2020) and DDIM
(Song et al., 2020a). In Tab. 2 we provide the FID score for CelebA (64x64) dataset (Liu et al., 2015)
(under non-commercial research purposes license). As can be seen for DDPM inference procedure
for 10, 20, 50, 100 steps, the best results were obtained from the Gamma model, which improves
results by a gap of 264 FID scores for ten iterations. For 100 iterations, the Gamma model improves
results by 31 FID scores. For 1000 iterations, the best results were obtained from the DDPM model.
Nevertheless, our Gamma model obtains results that are closer to the DDPM by a gap of 0.83. For the
DDIM procedure, the best results were obtained with the Gamma model for all number of iterations.
Fig. 2 presents samples generated by the three models. Our models provide better quality images
when compared to DDPM and DDIM methods.
In Tab. 3 we provide the FID score for the LSUN church dataset (Yu et al., 2015). As can be seen, the
Gamma model improves results over the baseline for 10, 20, 50, 100, 1000 iterations.
Tab. 4 lists the FID score for the ImageNet 64x64 dataset. The Gamma model obtains better results
than the DDIM baseline method for all iteration counts. Figure 3 compares between our proposed
DDGM and the baseline DDIM method. The random samples are generated with 1000 iterations
with the DDIM generation algorithm and η = 0. In each of the nine instances, the same random noise
is used as x1000 for both models.
In Tab. 5, we present results for training the network on a Markov chain of size T and perform all the
processes until step T/2, from which we infer x0 in one inference step. As can be seen, after the T /2
first iterations, there is no significant difference between Gaussian and Gamma networks. However,
after the remaining T/2 iterations, the Gamma based model obtains a performance GAP over the
Gaussian model. This suggests that our method is improving the results mostly during the final stage
of the inference process.
5	Conclusions
We present a novel Gamma diffusion model. The model employs a Gamma noise distribution. A key
enabler for using these distributions is a closed-form formulation (Eq. 11) of the multi-step noising
process, which allows for efficient training. We also present the reverse process and the variational
lower bound for the Gamma diffusion model. The proposed model improves the quality of generated
image and audio, as well as the speed of generation in comparison to conventional, Gaussian-based
diffusion processes. Our DDGM methods shows that diffusion model can benefit from non-Gaussian
noise distributions. This comes at a cost of adding one new hyperparameter to tune (θ0). Working
with other probability distributions, such as mixture models, may improve results even further.
Reproducibility S tatement
We provide in the supplementary file the complete code that was used to perform all of our experi-
ments. This archive includes audio samples and the code for both image and speech experiments.
Hyperparameters choices are clearly stated in Sec. 4 and the values are obtained from publicly
available implementation of previous work. The proof of all the theoretical results are available in the
appendix or are derived in the paper.
9
Under review as a conference paper at ICLR 2022
Figure 3: Comparison of generated samples with the (left) Gaussian noise (DDIM) and (right) Gamma
noise (DDGM). In each of the nine cases, the two models start from the same noise instance, which
leads to similar output images between the models.
References
Jacob Austin, Daniel Johnson, Jonathan Ho, Danny Tarlow, and Rianne van den Berg. Structured
denoising diffusion models in discrete state-spaces. arXiv preprint arXiv:2107.03006, 2021.
Mikotaj Binkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande,
Luis C Cobo, and Karen Simonyan. High fidelity speech synthesis with adversarial networks.
arXiv preprint arXiv:1909.11646, 2019.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad:
Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.
Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Condi-
tioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938,
2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint
arXiv:2105.05233, 2021.
Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. arXiv preprint
arXiv:1802.04208, 2018.
Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. Learning energy-based
models by diffusion recovery likelihood. arXiv preprint arXiv:2012.08125, 2020.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint
arXiv:1706.08500, 2017.
10
Under review as a conference paper at ICLR 2022
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint
arXiv:2006.11239, 2020.
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forra and Max Welling. Argmax
flows and multinomial diffusion: Towards non-autoregressive language models. arXiv preprint
arXiv:2102.05379, 2021.
Chin-Wei Huang, Jae Hyun Lim, and Aaron Courville. A variational perspective on diffusion-based
generative models and score matching. arXiv preprint arXiv:2106.02808, 2021.
Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.
Journal of Machine Learning Research, 6(4), 2005.
Keith Ito and Linda Johnson. The lj speech dataset. https://keithito.com/
LJ-Speech-Dataset/, 2017.
Chenlin Meng Jiaming Song and Stefano Ermon. Denoising diffusion implicit models. https:
//github.com/ermongroup/ddim, 2020.
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart,
Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio
synthesis. In International Conference on Machine Learning, pp. 2410-2419. PMLR, 2018.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8110-8119, 2020.
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. arXiv
preprint arXiv:2107.00630, 2021.
Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint
arXiv:2106.00132, 2021.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020a.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020b.
Max WY Lam, Jun Wang, Rongjie Huang, Dan Su, and Dong Yu. Bilateral denoising diffusion
models. arXiv preprint arXiv:2108.11514, 2021.
Lawrence M Leemis and Jacquelyn T McQueston. Univariate distribution relationships. The American
Statistician, 62(1):45-53, 2008.
Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests.
In International conference on machine learning, pp. 276-284. PMLR, 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing
Lin. High-fidelity performance metrics for generative models in pytorch, 2020. URL https://
github.com/toshas/torch-fidelity. Version: 0.2.0, DOI: 10.5281/zenodo.3786540.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499, 2016.
11
Under review as a conference paper at ICLR 2022
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A
diffusion probabilistic model for text-to-speech. arXiv preprint arXiv:2105.06337, 2021.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. arXiv preprint arXiv:1906.00446, 2019.
A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra. Perceptual evaluation of speech quality
(pesq)-a new method for speech quality assessment of telephone networks and codecs. In 2001
IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat.
No.01CH37221), volume 2,pp. 749-752 vol.2, 2001. doi:10.1109/ICASSP.2001.941023.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning,
pp. 2256-2265. PMLR, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020a.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
arXiv preprint arXiv:1907.05600, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456, 2020b.
C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility prediction
of time-frequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language
Processing, 19(7):2125-2136, 2011. doi: 10.1109/TASL.2011.2114881.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020.
Ivan Vovk. Wavegrad. https://github.com/ivanvovk/WaveGrad, 2020.
Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sample
from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-
scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,
2015.
A Proofs
A.1 Proof of lemma 1
<	八	K	ʊ	r	f ,	βt 八	一	1	八、
Lemma 1. Let	θ°	∈	R,	Assuming ∀t	∈ {1,…，T},	kt	= -2,	θt =	√α^θo,	and gt	〜Γ(kt,θt).
αtθ0
Then ∀t ∈ {1, ..., T } the following hold:
E(gt - E(gt)) = 0, V(gt - E(gt)) = βt	(12)
Xt = √OtX0 + (gt - E(gt))	(13)
where gt 〜Γ(gt, θt) and gt = P= ki
Proof. The first part of Eq. 12 is immediate. The variance part is also straightforward:
V (gt - E(gt)) = ktθt2 = βt
12
Under review as a conference paper at ICLR 2022
Eq. 13 is proved by induction on t ∈ {1,...T}. For t = 1:
X1 = vz1 - βιX0 + gι - E(gι)
since kι = kι, gι = gι. We also have that √1 - βι = ʌ/θɪ. Thus we have:
χι = √O1χ0 + (gι - E(gι))
Assume Eq. 13 holds for some t ∈ {1,…T}. The next iteration is obtained as
χt+ι = "∖ - βt+ιXt + gt+ι - E(gt+ι)	(22)
=√1 - βt+ι(√0tx0 + (gt - E(gt))) + gt+1 - E(gt+1)	(23)
=√αt+ιX0 + √1 - βt+ιgt + gt+1 - (p1 - βt+ιE(gt) + E(gt+1))	(24)
It remains to be proven that ⑴ √1 - βt+1g + gt+1 = gt+1 and (ii) √1 - βt+1 E(g∕ + E(gt+1)=
E(gt+1). Since gt 〜Γ(kt,θt) hold, then:
P1 - βt+1gt 〜r(gt, p1 - βt+1 θt) = r(gt,θt+1)
Therefore, we prove (i):
ʌ/1 - βt+1 gt + gt+1 〜r(gt + kt+1, θt+1) = r(gt+1,θt+1)
which implies that Vz1 - βt+1 gt + gt+1 and gt+1 have the same probability distribution.
Furthermore, by the linearity of the expectation, one can obtain (ii):
√1 - βt+1 E(gt) + E(gt+1) = E(√1 - βt+1 gt + gt+1)
= E(ggt+1)
Thus, we have:
xt+1 = √αt+1x0 + (gt+1 - E(gt+1))
which ends the proof by induction.	□
A.2 PROOF OF LEMMA 2
Lemma 2. Denote q(xt—1 ∣x0,xt) as the reverse process ofthe proposed Gamma diffusion model.
Then, the reverse process is proportional to:

q(xt-1∣X0,Xt) b
X kt-1e-Xt∕θt X kt-】 Te-XtT∕θt-ι
(15)
XktTef∕%
where
1.	Xt = Xt- √1 - βtxt-1 + ktθt
2.	Xt = Xt — √αtχ0 + gt θt
3.	Xt-1 = xt-1 - V<gt-1x0 + kt-1θt-1
Proof. The reverse process is given by:
q(xt-1∣x0)
q(xt∣x0)
q(xt-1∣x0,xt) = q(xt∣xt-1,x0)
(25)
13
Under review as a conference paper at ICLR 2022
Next, one can calculate each one of the three main components of the reverse process, i.e.
(i) q(xt∣x-1,x0), (ii) q(xt-ι∖xo) and (iii) q(xt∖xo). Since q is memoryless, q(xt∖xt-1,x0) =
q(xt ∖xt-ι). Therefore, the first component (i) of Eq. 25 is the forward process. The forward process
is given by:
q(xt∖xt-i) = p(gt = Xt- √1 - βtxt-1 + ktθt)	(26)
_ (xt - √1 - βtxt-ι + ktθt)kt-1e-(xt-√1-βtxt-* 1 2 3+ktθt)/θt
=	「“wk
The second component of Eq.25 is given by:
(xt-1 - √0-x0 + Lιθt-1 声TTe-(Xt-√atzrxo+廉-1θt-1"θt-1
q(xt-i∖x0) =------------------------Z----k-----------------------
Γ(⅛t-1 )θkt-1
Similarly, the third component of Eq.25 is given by:
q(xt∖x0) = P(Ut = Xt - √0tx0 + ⅛tθt)
(xt - FtX0 + ktθt/t-1e-(Xt-√ktxo+ktθt"θt
Γ(kt)θkt
Overall, the reverse process q(xt-ι ∖x0, Xt) is given by:
((Xt - √1 - βtxt-ι + ktθt)kt-1e-(χt-√τ-βtxt-1+ktθt"θt)
q(xt-1∖x0，xt) =------------------------Et----------------------------
((Xt-I - √0-1X0 + kt-1θt-1)ktτ-1e-(χt-√≡zrxo+ktτθtτ"θt-1)
Γ(kt-1)θ”1
___________________Γ(kt)θkt__________________
((Xt - √0tx0 + ktθt)kt-1e-(χt-√αtχo + ktθt)∕θt)
One can denote:
(29)
(30)
1. Xt = Xt — √1 — βtXt-1 + kt θt
2. Xt = xt - √ɑktx0 + ktθt
3. Xt-1 = xt-1 - V<kt-1x0 + kt-1θt-1
Thus, the reverse process q(xt-1∖x0, Xt) is proportional to:
q(xt-1∖x0,xt) X
-Ie-Xt∕θt X kt-ι-1e-Xt-ι∕θt-ι
X kt-1e-Xt∕θt
(31)
□
A.3 Proof of lemma 3
Lemma 3. The Lt-1 for the proposed Gamma diffusion model is upper bounded by the following L1
norm:
Lt-1 ≤
+ C2 + C + 2) ∖X0 - X0∖
gkt	gkt-1
(20)
where C1,C2, C3 and C4 are constant terms.
14
Under review as a conference paper at ICLR 2022
Proof. We can calculate the L— with the exact form:
Lt-I
DκL(q(Xt-1∖x0,Xt)∖∖pθ (χt-1∣x0,xt)) = Eq(Xt-1∣x0 ,xt) lθg
f q(xt-1∣X0,Xt)
∖PΘ (xt-1∣Xθ,Xt)
(32)
Using Eq.15 the RHS ofEq.32 become:
q(Xt-1|X0,Xt)	Xt-1、	Xt-I — Xt-I	Xt、Xt — Xt
log (pθ (Xt-1∣X0 ,Xt) )= Ikj-1) log( XZ7)— —θ—— — (kt —1) log(元)+
(33)
One can show that the four terms present in the previous equation can be upper bounded with the L1
distance between the predicted X0 and the ground truth x°:
Xt-1 -Xt-I
θ-
∣ = |(x0 — x0) 广-11 ≤ CI ∣x0 — x01
t-1
• ∣ Xt-Xt ∣ = ∣(x0 — x0) √^αα~^ ∣ ≤ c2∣x0 — x0∣
•	(kt	-
log 1 +
1)log( Xt)
√⅞7(X0 — X。)
Xt — √0tX0 + ktθt
≤∣
(kkt
—
1) log
Xt — √07X0 + ktθt
√αt-1(x0 - X0)
Xt-1 — √kt-1X0 + kt-1θt-1
=gτ4r ∣x0 - x0∣
Xt 一 √Ox0 + ktθt
Xt — √0tX0 + ktθt
∣ = C ∣x0— x0∣
•	(kt-ι — i)log( X- ) = log(ι +
≤ ∣	√αt-1(x0 - X0)	∣
―Xt-1 — √kt-1X0 + kt-1θt-1
The complete form of the Lt-1 upper bound can be expressed as follows:
LtT ≤ Eq- (CI+C2 + K + gt⅛) ∣x0 - x°∣
工厂工C3qC4 H ʌ,
+ C2 + 二—+ 二—I ∣x0 — x0∣
kt	kt-1√
(34)
□
A.4 PROOF OF LEMMA 4
Lemma 4. Minimizing the variational lower boundfor DDGM (i.e. Lt for t ≥ 1 )is equivalent to
minimizing the L1 norm between the sampled noise and the estimated noise:
kt - ktθt	L / 八	C]'
L —— —∕1	_ 7 ——εθ(xt,t)	(21)
√1 — Qt
Proof. From Eq.34, the variational lower bound of DDGM is given by Lt-1 ≤
(C1 + C2 + C + gC4^) ∣x0 — X01. Substitute Eq.19 and Eq.11 to the variational lower bound
15
Under review as a conference paper at ICLR 2022
we have:
Lt-1 ≤
「 C3 C4
+ C2 + — + -l
gt	gt-ι
∣χ0 - X0∣
C3 . C . C3	C4 ʌ I	Xt - √1 - atεθ (xt,t)
C1 + C2 +	+	∣X0
∖	gt	gt-ι) I	√αt
-Xt+ √1- αtεθ(χt,t)I
(35)
(36)
(37)
C	C3	C4 ∖	1 ∣h	= 一 丁八、 y----------------h /	、|
+ C2 + -----+ T--- ) ~√α l√αtx0 一 ʌ/θtʃθ — (Ut — ktθt) + √1 — αɛθ(xt,t)1
gt	gt-ι) √Qt
(38)
+ C2 + ~~ + _ 4 ) L 1 (gt - utθt) - √1 - αtɛθ(Xt,tV
gt	gt-ι) √Qt
l ry	1	C3	1 C4 ∖	√1 - J ∖ gt - ktθt / 八
+ C2	+	—	+ 二一	一7^— ∖ 八 _	- ɛθ(xt, t)
gt	Qt-1)	√Qt ∖ √1 - Qt
(39)
(40)
Since We are minimizing the variational lower bound, one can drop the constant term
(CI + c2 + C3 + g
minimizing the term
v√-at. Therefore, minimizing the variational lower bound is equal to
gt-kt θt
√1-αt
-ɛθ(xt, t) ∖ .
□
16