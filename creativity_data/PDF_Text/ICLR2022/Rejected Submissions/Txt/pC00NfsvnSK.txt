Under review as a conference paper at ICLR 2022
Improving Zero-shot Generalization in Of-
fline Reinforcement Learning using General-
ized Similarity Functions
Anonymous authors
Paper under double-blind review
Abstract
Reinforcement learning (RL) agents are widely used for solving complex sequen-
tial decision making tasks, but still exhibit difficulty in generalizing to scenarios
not seen during training. While prior online approaches demonstrated that using
additional signals beyond the reward function can lead to better generalization ca-
pabilities in RL agents, i.e. using self-supervised learning (SSL), they struggle in
the offline RL setting, i.e. learning from a static dataset. We show that perfor-
mance of online algorithms for generalization in RL can be hindered in the offline
setting due to poor estimation of similarity between observations. We propose a
new theoretically-motivated framework called Generalized Similarity Functions
(GSF), which uses contrastive learning to train an offline RL agent to aggregate
observations based on the similarity of their expected future behavior, where we
quantify this similarity using generalized value functions. We show that GSF is
general enough to recover existing SSL objectives while also improving zero-shot
generalization performance on a complex offline RL benchmark, offline Procgen.
1	Introduction
Reinforcement learning (RL) is a powerful framework for solving complex tasks that require a se-
quence of decisions. The RL paradigm has allowed for major breakthroughs in various fields, e.g.
outperforming humans on video games (Mnih et al., 2015; Schwarzer et al., 2020), controlling strato-
spheric balloons (Bellemare et al., 2020) and learning reward functions from robot manipulation
videos (Chen et al., 2021). More recently, RL agents have been tested in a generalization setting,
i.e. in which training involves a finite number of related tasks sampled from some distribution,
with a potentially distinct sampling distribution during test time (Cobbe et al., 2019; Song et al.,
2019). The main issue for designing generalizable agents is the lack of on-policy data from tasks
not seen during training: it is impossible to enumerate all variations of a real-world environment
during training and hence the agent must extrapolate from a (limited) training task collection onto
a broader set of problems. Since the learning agent is given no training data from test-time tasks,
this problem is referred to as zero-shot generalization. In our work, we are interested in the problem
of zero-shot generalization where the difference between tasks is predominantly due to perceptually
distinct observations. An example of this setting is any environment with distractor features (Cobbe
et al., 2020; Stone et al., 2021), i.e. features with no dependence on the reward signal nor the agent’s
decisions. This generalization setting has recently received much attention (Liu et al., 2020a; Agar-
wal et al., 2021; Mazoure et al., 2021), due to its particular relevance to real-world scenarios, for
example deploying a single autonomous driving agent at day or at night.
Generalization capabilities ofan agent can be analyzed through the prism of representation learning,
under which the agent’s current belief about a rich and high-dimensional environment are summa-
rized in a low-dimensional entity, called a representation. Recent work in online RL has shown that
learning state representations with specific properties such as disentanglement (Higgins et al., 2017)
or linear separability (Lin et al., 2020) can improve zero-shot generalization performance. Achiev-
ing this with limited data (i.e. offline RL) is challenging, since the representation will have a large
estimation error over regions of low data coverage. A common solution to mitigate this task-specific
overfitting and extracting the most information out of the data consists in introducing auxiliary learn-
ing signals other than instantaneous reward (Raileanu and Fergus, 2021). As we show later in the
1
Under review as a conference paper at ICLR 2022
paper, many such signals already contained in the dataset can be used to further improve generaliza-
tion performance. For instance, the generalization performance of PPO on Procgen remains limited
even when training on 200M frames, while generalization-oriented agents (Raileanu and Fergus,
2021; Mazoure et al., 2021) can outperform it by leveraging additional auxiliary signals. However,
a major issue with the aforementioned methods is their exorbitant reliance on online access to the
environment, an impractical restriction for real-world scenarios.
In contrast, in many real-world scenarios access to the environment is restricted to an offline, fixed
dataset of experience (Ernst et al., 2005; Lange et al., 2012). A natural limitation for generalization
from offline data is that policy improvement is dependent on dataset quality. Specifically, high-
dimensional problems such as control from pixels require large amounts of training experience: a
standard training of PPO (Schulman et al., 2017) for 25 million frames on Procgen (Cobbe et al.,
2020) generates more than 300 Gb of data, an impractical amount of data to share for offline RL re-
search. Improving zero-shot generalization performance from an offline dataset of high-dimensional
observations is therefore a hard problem due to limitations on dataset size and quality.
In this work, we are interested in improving zero-shot generalization across a family of Partially-
Observable Markov decision processes (MDPs, Puterman, 1990) in an offline RL setting, i.e. by
training agents on a fixed dataset. We hypothesize that in order for an RL agent to be able to gen-
eralize across perceptually different POMDPs without adaptation, observations with similar future
behavior should be assigned to close representations. We use the generalized value function (GVF)
framework (Sutton et al., 2011) to capture future behavior with respect to an arbitrary instantaneous
signal (called cumulant) for a given state. The choice of cumulant then determines the nature of the
behavioral similarity that is encouraged for generalization. For example, using reward as the signal
gives rise to of reward-aware behavioral similarity such as bisimulation (Ferns et al., 2004; Li et al.,
2006; Castro, 2020; Zhang et al., 2020); using future state-action counts encourages reward-free
behavioral similarity (Misra et al., 2020; Liu et al., 2020a; Agarwal et al., 2021; Mazoure et al.,
2021).
Our main contributions are as follows:
1.	We propose Generalized Similarity Functions (GSF), a novel self-supervised learning al-
gorithm for reinforcement learning, that aggregates latent representations by the future be-
havior (or generalized value function) under their respective observations.
2.	We devise a new benchmark constructed to test zero-shot generalization of offline RL algo-
rithms: offline Procgen. It consists of 5M transitions from 200 related levels of 16 distinct
games.
3.	We evaluate performance of GSF and other baseline methods on offline Procgen, and show
that GSF outperforms both previous state-of-the-art offline RL and representation learning
baselines on the entire distribution of levels.
4.	We analyze the theoretical properties of GSF and describe the impact of hyperparameters
and cumulant functions on empirical behavior.
2	Related Works
Generalization in reinforcement learning Generalizing a model’s predictions across a variety of
unseen, high-dimensional inputs has been extensively studied in the static supervised learning set-
ting (Bartlett,1998; Triantafillou et al., 2019; Valle-Perez and Louis, 2020; LiU et al., 2020b). Gener-
alization in RL has received a lot of attention: extrapolation to unseen rewards (Barreto et al., 2016;
Misra et al., 2020), observations (Zhang et al., 2020; Raileanu and Fergus, 2021; Liu et al., 2020a;
Agarwal et al., 2021; Mazoure et al., 2021) and transition dynamics (Ball et al., 2021). Each gen-
eralization scenario is best solved by their respective set of methods: sufficient exploration (Misra
et al., 2020; Agarwal et al., 2020), auxiliary learning signals (Srinivas et al., 2020; Mazoure et al.,
2020; Stooke et al., 2021) or data augmentation (Ball et al., 2021; Sinha and Garg, 2021). Data aug-
mentation is a promising technique, but typically relies on handcrafted domain information, which
might not be available a priori. In fact, we will show in our experiments that generalization in the
offline RL setting is poor even when using such handcrafted data augmentations, without additional
representation learning mechanisms. In this work, we posit that representation learning should use
instantaneous auxiliary signals in order to prevent overfitting onto a unique signal (e.g. reward
2
Under review as a conference paper at ICLR 2022
across tasks) and improve generalization performance. Theoretical generalization guarantees have
only been provided so far for limited scenarios, mostly for bandits (Swaminathan and Joachims,
2015), linear MDPs (Boyan and Moore, 1995; Wang et al., 2021b; Nachum and Yang, 2021) and
across reward functions (Castro and Precup, 2010; Barreto et al., 2016; Wang et al., 2021a; Touati
and Ollivier, 2021).
Representation learning For simple POMDPs, near-optimal policies can be found by optimiz-
ing for the reward alone. However, more complex settings may require additional auxiliary signals
in order to find state abstractions better suited for control. The problem of learning meaningful
state representations (or abstractions) for planning and control has been extensively studied previ-
ously (Jong and Stone, 2005; Li et al., 2006), but saw real breakthroughs only recently, in particular
due to advances in self-supervised learning (SSL). Outside of RL, SSL has achieved spectacular
results by closing the gap between unsupervised and supervised learning on certain datasets (Hjelm
et al., 2018; Oord et al., 2018; Caron et al., 2020; Grill et al., 2020). Representation learning,
and specifically self-supervised learning, has also been used to achieve state-of-the-art generaliza-
tion and sample efficiency results in RL on challenging control problems such as data efficient
Atari (Schwarzer et al., 2020; 2021), DeepMind Control (Agarwal et al., 2021) and Procgen (Ma-
zoure et al., 2020; Stooke et al., 2021; Raileanu and Fergus, 2021; Mazoure et al., 2021).Noteworthy
instances of theoretically-motivated representation learning methods for RL include heuristic-guided
learning (Sun et al., 2018; Cheng et al., 2021), and random Fourier features (Nachum and Yang,
2021).
Offline reinforcement learning When learning from a static dataset, agents should balance inter-
polation and extrapolation errors, while ensuring proper diversity of actions (i.e. prevent collapse to
most frequent action in the data). Popular offline RL algorithms such as BCQ (Fujimoto et al., 2019),
MBS (Liu et al., 2020c), and CQL (Kumar et al., 2020) rely on a behavior regularization loss (Wu
et al., 2019) as a tool to control the extrapolation error. Some methods, such as F-BRC (Kostrikov
et al., 2021) are defined only for continuous action spaces while others, such as MOReL (Kidambi
et al., 2020) estimate a pessimistic transition model. The major issue with current offline RL al-
gorithms such as CQL is that they are perhaps overly pessimistic for generalization purposes, i.e.
CQL and MBS ensure that the policy improvement is well-supported by the batch of data. As we
will show in our empirical comparisons, using overly conservative policy updates can prevent the
representation from fully leveraging the information of the training dataset.
3	Problem setting
3.1	Partially-observable Markov decision processes
A (infinite-horizon) partially-observable Markov decision process (POMDP, Murphy, 2000) M is
defined by the tuple M = hS,p0, A,pS, O,pO,r,γi, where S is a state space, p0 = P[s0] is the
starting state distribution, A is an action space, PS = PHst,at] : S X A → ∆(S) is a transition
function, O is an observation space, PO=PHst] : S → ∆(O)1 is an observation function, r :
S×A→[rmin,rmax] is a reward function and γ ∈ [0, 1) is a discount factor. The system starts in
one of the initial states so 〜po with observation oo 〜PO(∙∣so). At every timestep t = 1,2,3,..,the
agent, parameterized by a policy ∏ : O → ∆(A), samples an action at 〜∏(∙∣ot). The environment
transitions into a next state st+ι 〜PS(∙∣st, at) and emits a reward r = r(st, a. along with a next
observation ot+ι 〜PO(∙∣st+ι).
The goal of an RL agent is to maximize the cumulative rewards P∞=0 γtrt obtained over the entire
episode. Value-based off-policy RL algorithms achieve this by estimating the state-action value
function under a target policy π:
Qπ(st,at)=EPtπ [	γkr(st+k, at+k)|st, at], ∀st ∈S,at ∈A	(1)
k=1
where Pπ denotes the joint distribution of {st+k , at+k }∞=1 obtained by executing π in the environ-
ment.
1∆(X) denotes the entire set of distributions over the space X.
3
Under review as a conference paper at ICLR 2022
An important distinction from online RL is that, instead of sampling access to the environment, we
assume access to a historical dataset Dμ collected by logging experience of the policy, μ, in the
form {oi,t ,ai,t,ri,t}i==1N,t==1T where, for practical purposes, the episode is truncated at T timesteps.
Furthermore, we assume that the agent can only be trained on a limited collection of POMDPs
Mtrain = {Mi}m=1, and its performance is evaluated on the set of test POMDPs Mtest. We assume
that both Mtrain and Mtest were sampled from a common task distribution and that every POMDP
Mi ∈M = Mtrain ∪ Mtest shares the same transition dynamics and reward function with M but
has a different observation function pi,O . Importantly, since we perform control from pixels, we
are in the POMDP setting (see Yarats et al., 2019) and therefore emphasize the difference between
observations ot and corresponding states st throughout the paper.
3.2	Representation learning
Previous works in the RL literature have studied the use of auxiliary signals to improve generaliza-
tion performance. Among others, Liu et al. (2020a); Agarwal et al. (2021) define the similarity of
two observations to depend on the distance between action sequences rolled out from that observa-
tion under their respective optimal policies. They achieve this by finding a latent space Z⊆Sin
which the distance dZ (z, z0) for all z, z0 ∈Zis equivalent to distance between true latent states
dS (s, s0) for all s, s0 ∈S; the aforementionned works learn Z by optimizing action-based similari-
ties between observations.
In practice, latent space z is decoded from observation o using a latent state decoder f : O→Zfrom
observation ot . Through the paper, we assume that all value functions have a linear form in the latent
decoded state, i.e. Qθ (o, a)=θa>fψ(o)=θa>zψ, which agrees with our practical implementation of
all algorithms. Within this model family, the ability of an RL agent to correctly decode latent states
from unseen observations directly affects its policy, and therefore, its generalization capabilities.
In the next section, we discuss why representation learning is important for offline RL, and how
existing action-based similarity metrics fail to recover the true latent states for important families of
POMDPs.
4	Motivating example
Figure 1: Two levels of the Climber game from the Procgen benchmark (Cobbe et al., 2020) with
near-identical true latent states and near-identical value functions but drastically different action
sequences.
Multiple recently proposed self-supervised objectives (Liu et al., 2020a; Agarwal et al., 2021) con-
jecture that observations o1 ∈ M1,o2 ∈ M2 that emit similar future action sequences under optimal
policies nɪ, πζ should be decoded into nearby latent states z1, z2. While this heuristic can correctly
group observations with respect to their true latent state in simple action spaces, it fails to identify
similar pairs of trajectories in POMDPs with multiple optimal policies. For instance, two trajectories
might visit an identical set of latent states, but have drastically different actions.
Fig. 1 shows one such example: two levels of the Climber game have a near-identical true latent
state (see Appendix) and value function (average normalized mean-squared error of 0.0398 across
episode), while having very different action sequences from a same PPO policy (average total vari-
ation distance of 0.4423 across episode). The problem is especially acute in Procgen, since the PPO
policy is high-entropy for some environments (see Fig. 4), i.e. various levels can have multiple dras-
tically different near-optimal policies, and hence fail to properly capture observation similarities.
In this scenario, assigning observations to a similar latent state by value function similarity would
yield a better state representation than reasoning about action similarities. In a POMDP with a
4
Under review as a conference paper at ICLR 2022
different structure, grouping representations by action sequences can be optimal. So how do we
unify these similarity metrics under a single framework?
In the next section, we use this insight to design a general way of improve representation learning
through self-supervised learning of discounted future behavior.
5	Method
We propose to measure a generalized notion of future behavior similarity using generalized value
functions, as defined by the corresponding cumulant function. The choice of cumulant determines
which components of the future trajectory are most relevant for generalization.
5.1	Quantifying future behavior with GVFs
An RL agent’s future discounted behavior can be quantified not only by its the value function, but
other auxiliary signals, for example, by its observation occupancy measure, known as successor fea-
tures (Dayan, 1993; Barreto et al., 2016). The choice of the examined signal quantifies the properties
the agent will exhibit in the future, such as accumulated returns, or observation visitation density.
See Thm. 2 in the Appendix for the connection between successor features and interpolation error
in our method.
Following the work of Sutton et al. (2011), we can broaden the class of value functions to any kind
of cumulative discounted signal, as defined by a bounded cumulant function c : O × A → Rd,
s.t. |c(o, a)| ≤ cmax for cmax = supo,a∈O×A c(o, a). While typically cumulants are scalar-valued
functions (e.g. reward), we also make use of the vector-valued case for learning the successor
features (Barreto et al., 2016), in which case the norm of c(o, a) is bounded.
Definition 1 (Generalized value function) Let c be any bounded function over Rd, let γ ∈ [0, 1]
and μ any policy. The generaIized value function is defined as
∞
G”(Ot)= EPμ [X Y k c(ot+k ,at+k )|ot]	⑵
for any timestep t ≥ 1 and ot ∈ O.
Since, in our case, We can learn Gμ for each distinct POMDP Mi for the dataset Dμ, We index
the GVF using the POMDP index, i.e. G? = LearnGVF(c, Dμ,i) (in practice, the learning is
parallelized).
Algorithm 1: LearnGVF(c, Dμ, i, θ(0), J, α, γ): Offline estimation of GVF Gμ
Input : Cumulant function c, dataset Dμ, POMDP label i, initial parameters θ(0), target
parameters θ, latent state decoder f, iterations J, learning rate α, discount γ
1	for j =1,.., Jdo
2	o,a,o0 〜 D[i]; // Sample transition from POMDP i
3	c J c(o, a);
4	o J random crop(o);
5	z, z0 J f(o),f(o0);
6	θ(j) J θ(j-1) - αJ(j-ι) (Gθ(j-1) (Z)- C - lGθ(j-∖) (ZO))2 ;
7	Update target parameters θ With β of online parameters θ;
5	.2 Measuring distances between GVFs of different POMDPs
Examining the difference betWeen future behaviors of tWo observations quantifies the exact amount
of expected behavior change betWeen these tWo observations. Using the GVF frameWork, We could
compute the distance betWeen o1 ∈ M1 and o2 ∈ M2 by first estimating the latent state With
Z = f(o) using a (learned) latent state decoder f, and then evaluating the distance
dμ(oιi, O2j) = ∣Gμ(f (oι)) - Gμ(f (02))∣ i,j = 1, 2,..,	(3)
5
Under review as a conference paper at ICLR 2022
a measure of dissimilarity that can then be used in a contrastive loss.
However, the distance between GVFs from two different POMDPs can have drastically different
μ	μ	Cμ	+Cμ
scales: ∣Gμ(oι) - Gμ(02)∣ ≤ 1,max-ɔ2 , making point-WisecomPansonmeamngless. The issue
is less acute for cumulants which induce a unnormalized density estimate (e.g. indicator functions
for successor representation), and more problematic When the cumulant incorporates the extrinsic
reWard function. To avoid this problem, We suggest performing a comparison based on order statis-
tics.
A robust distance estimate betWeen GVF signals across POMDPs can be obtained by looking at
the cumulative distribution function of Gi denoted Fi(g) = P[Gi (ot) ≤ g] for all ot ∈O. Gi
is a deterministic GVF With the set of discontinuity points of measure 0, and as such Fi can be
understood through the induced state distribution Pμ (using continuous mapping theorem from Mann
and Wald (1943)). It can be estimated from n independent and identically distributed samples of Dμ
as
1n
Fi(g) = _X 1Gi<g,Gi = LearnGVF(C) Dμ, i), g ∈ - 丁一, 丁一	(4)
n =1 i	1 - γ 1 - γ
and its inverse, the empirical quantile function (van der Vaart, 1998)
„ ,	Γ	cμ	cμ	1
FiT(P)=inf{g ∈	-产,产 :P ≤ Fi(g)},p ∈ [0,1]	(5)
1-γ 1-γ
We use the empirical quantile function to partition the range of all GVFs into K quantile bins, i.e.
disjoint sets With identical size Where the set corresponding to quantile k is defined as Ii(k) = {o ∈
Mi : F-1(令)≤ Gɪ(o) ≤ F-1(k+1)} and it,s aggregated version as I (k) = ∪m=J(k).
Importantly, we augment the dataset Dμ with observation-specific labels, which correspond to the
index of the quantile bin into Which the GVF G of an observation o ∈ Mi falls into:
li(o) = max lο∈ii(fc)
(6)
These self-supervised labels are then used in a multiclass InfoNCE loss (Oord et al., 2018), which
is a variation of metric learning with respect to the quantile distance defined above (Khosla et al.,
2020; Song and Ermon, 2020).
5	.3 Self-supervised learning of GSFs
After augmenting the offline dataset with observation labels, we use a simple self-supervised learn-
ing procedure to minimize distance in the latent representation space between observations with
identical labels.
First, the observation o is encoded using a non-linear encoder fψ : O→Zwith parameters ψ
into a latent state representation z = fψ (o)2. The representation z is then passed into two separate
trunks: 1) a linear matrix θa which recovers the state-action value function Qθ(o, a) = θa>z, and 2)
a non-linear projection network hθ : Z→Zwith parameters θh to obtain a new embedding, used
for contrastive learning.
The projection h(z) is then used used in a multiclass InfoNCE loss (Oord et al., 2018; Song and
Ermon, 2020) where a linear classifier W ∈ R|Z| ×K aims to correctly predict the observation labels
(i.e. quantile bins k =1, 2,.., K) from h(z):
'NCE(θh, ψ, W) = -Eo〜d“|£ li(ο)=fcLogSoftmax[W>h(fψ(ο))∕τ]k ,	⑺
k=1
where τ>0 is a temperature parameter.
Our empirical findings suggest that this version of the loss is more stable than other multi-class
contrastive losses (see Appendix 7.3).
2This encoder is different from the one used to evaluate the GVFs.
6
Under review as a conference paper at ICLR 2022
Figure 2: Schematic VieW of GSF : the offline dataset Dμ is used to estimate POMDP-SPeCifiC
GVFs wrt some cumulant function c, whose quantiles are then used to label each observation in the
dataset. These labels are then used in a multi-class contrastiVe learning Procedure along With offline
RL learning.
1
2
3
4
5
6
7
8
9
10
11
5.	4 Algorithm
Our method relies on the aPProximation oracle LearnGVF, to Produce GVF estimates, later used
in the contrastiVe learning Phase.
Since We are concerned With the offline RL setting, We add our auxiliary loss on toP of ConserVatiVe
Q-learning (CQL, Kumar et al., 2020), a strong baseline. CQL is trained using a linear combination
of Q-learning (Watkins and Dayan, 1992; Ernst et al., 2005) and behaVior-regularization:
'cql(θ) = Eo,a,r,o0〜Dμ [(r+γ maxQθ(o', a)-Qθ(o, a))2]+λEs〜dh [LSE(Qθ(o, a))-Ea〜μ[Qθ(o, a)]],
a0∈A
(8)
for λ ≥ 0, θ target network parameters3 4 and LSE being the log-sum-exp operator 4.
Algorithm 2: GSF : Offline RL With future behaVior obserVation matching
Input : Dataset D 〜μ, initialized Q-function Qθ with encoder fθf and action weights θa,
per-POMDP set of GVFs G = {Gμ }i∈, state projection network hψ, epoch number J,
number of POMDPs m, number of quantiles K, temperature parameter τ , exponential
moVing aVerage parameter β
for epoch j =1, 2,..,Jdo
for minibatch B 〜D do
/* Data augmentation on observation	*/
o — random crop(o) for all o ∈ B;
Z — fψ (o) for all o ∈ B ;
/* Update CQL agent	*/
Update θa, ψ using Vθa ,ψ'cql(θ);
/* Compute G quantiles	*/
for POMDP Mi =1,2,..,mdo
Estimate FT of Gμ from B ;
for observation o ∈ B∩Mi do
I l(o) J k if F-1 (KK) ≤ Gμ(o) ≤ RT(kK+1);
/* Update encoder and projection network	*/
Update θh, ψ, W using Vθh,ψW'NCE(θh, ψ, W) computed with z, l(o) and τ ;
Update CQL agent’s target network with β of online parameters ψ, θ;
Alg. 2 summarizes the learning procedure for GSF as implemented on top of a CQL agent for a
discrete action space. In our experiments, all baselines use random crops as data augmentation.
3A copy of θ updated solely using an exponential moVing aVerage (see Appendix).
4https://en.wikipedia.org/wiki/LogSumExp
7
Under review as a conference paper at ICLR 2022
Connection to existing methods Our framework is able to recover objectives similar to those of
prior works by carefully designing the cumulant function.
•	Cross-State Self-Constraint (CSSC, Liu et al., 2020a): In CSSC, observations o1,o2
are considered similar if they have identical future action sequences of length K under
some fixed policy; a total of |A|K distinct classes are possible. This approach can be
approximated in our framework by picking c(ot, at) = ILat (a), Na ∈ A. The Problem
reduces to a |A|T-t-way classification problem for observations of timestep t, which GSF
approximates using K quantiles.
•	Policy similarity embedding (PSE, Agarwal et al., 2021): PSEs balance the distance
between local optimal behaviors and long-term dependencies in the transitions, notably
using dTV. If we consider the space of Boltzmann policies πBoltzmann with respect to an
POMDP-specific value function Q, then choosing c(ot,at)=r(st,at) in GSF will effec-
tively compute the distance between unnormalized policies.
5.5 Choice of number of quantiles K
How should the number of quantiles K be set, and what is the effect of smaller/ larger values of K
on the observation distance? Thm. 1 highlights a trade-off when choosing the number of quantiles
bins empirically.
Theorem 1 Let G1 , G2 be generalized value functions with cumulants c1 ,c2 from respective
POMDPs M1,M2, K be the number of quantile bins, n1,n2 the number of sample transitions from
each POMDP. Suppose that P[supt=ι 2 .. ∣cι(oι,t,μ(oι,t)) — c2(o2,t,μ(02,t))∣ > (I-Y)ε∕γ] ≤ δ.
Then, for any k =1, 2,..,Kand ε>0 the following holds without loss of generality:
where
P sup	∣G1(01) — G2(02) | > 3ε ≤ 2e-2n1ε /4 + p(nι, K, ε) + δ
o1,o2 ∈I(k)
p(n, K, ε) = P sup	IF-I (k+1∕κ) — F-1 (k∕K)I > ε
k=1,2,..,K
(9)
(10)
The proof can be found in the Appendix Sec. 7.2. For POMDP M1, the error decreases monoton-
ically with increasing bin number K (second term) but the variance of bin labels depends on the
number of sample transitions n1 (first term). The inter-POMDP error (third term) does not affect
the bin assignment. Hence, choosing a large K will amount to pairing states by rankings, but results
in high variance, as orderings are estimated from data and each bin will have n =1. Setting K too
small will group together unrelated observations, inducing high bias.
6	Experiments
Unlike for single task offline RL (Fu et al., 2020), most works on zero-shot generalization from of-
fline data either come up with an ad hoc solution suiting their needs, e.g. (Ball et al., 2021), or assess
performance on benchmarks that do not evalute generalization across observation functions (e.g., Yu
et al., 2020). To accelerate progress in this field, we devised the offline Procgen benchmark, an of-
fline RL dataset to directly test for generalization of offline RL agents across observation functions5.
Offline Procgen benchmark We evaluate the proposed approach on an offline version of the
Procgen benchmark (Cobbe et al., 2020), which is widely used to evaluate zero-shot generalization
across complex visual perturbations. Given a random seed, Procgen allows to sample procedurally
generated level configurations for 16 games under various complexity modes: “easy”, “hard” and
“exploration”. The dataset is obtained as follows: we first pre-train a PPO (Schulman et al., 2017)
agent for 25M timesteps on 200 levels of “easy” distribution for each environment6 (“easy” mode is
widely used to test generalization capabilities (Cobbe et al., 2020; Raileanu and Fergus, 2021; Ma-
zoure et al., 2021)). All agents use the IMPALA encoder architecture (Espeholt et al., 2018), which
has enough parameters to allow better generalization performance, compared to other models (e.g.,
Mnih et al., 2015).
5The benchmark will be open-sourced.
6We use the TFAgents’ implementation (Guadarrama et al., 2018)
8
Under review as a conference paper at ICLR 2022
Results We compare the zero-shot performance on the entire distribution of ”easy” POMDPs for
GSF against that of strong RL and representation learning baselines: behavioral cloning (BC) - to
assess the quality of the PPO policy, CQL (Kumar et al., 2020) - the current state-of-the-art on
multiple offline benchmarks which balances RL and BC objectives, CURL (Srinivas et al., 2020),
CTRL (Mazoure et al., 2021), DeepMDP (Gelada et al., 2019) - which learns a metric closely re-
lated to bisimulation across the MDP, Value Prediction Network (VPN, Oh et al., 2017) - which
combines model-free and model-based learning of values, observations, next observations, rewards
and discounts, Cross-State Self-Constraint (CSSC, Liu et al., 2020a) - which boosts similarity of ob-
servations with identical action sequences, as well as Policy Similarity Embeddings (Agarwal et al.,
2020), which groups observation representations based on distance in optimal policy space.
Figure 3: Returns on the offline Procgen benchmark (Cobbe et al., 2020) after 1M training steps.
Boxplots are constructed over 5 random seeds and all 16 games; each method is normalized by the
per-game median CQL performance. White dots represent average of distribution.
Fig. 3 shows the performance of all methods over 5 random seeds and all 16 games on the offline
Procgen benchmark after 1 million training steps. Per-game average scores for all methods can be
found in Tab. 2 (Appendix). The scores are standardized per-game using the downstream task’s
(offline RL) performance, in this case implemented by CQL. It can be seen that GSF performs better
than other offline RL and representation learning baselines.
Using different cumulants functions can lead to different label assignments and hence different sim-
ilarity groups. Fig. 3 examines the performance of GSF with respect to 3 cumulants: 1) r(st ,at),
rewards s.t. GSF learns the policy's Qμ-value, 2) Uot (o), the successor representation7 (Dayan,
1993; Barreto et al., 2016) s.t. GSF learns induced distribution over Dμ (Machado et al., 2020) and
3) lat (a), action counts, s.t. GSF learns discounted policy. While rewards and successor feature
cumulant choices leads to similar performance, using action-based distance leads to larger variance.
7	Discussion
In this work we proposed GSF, a novel algorithm which combines reinforcement learning with rep-
resentation learning to improve zero-shot generalization performance on challenging, pixel-based
control tasks. GSF relies on computing the similarity between observation pairs with respect to any
instantaneous accumulated signal, which leads to improved empirical performance on the newly in-
troduced offline Procgen benchmark. Theoretical results suggest that GSF ’s hyperparameter choice
depends on a trade-off between finite sample approximation and extrapolation error.
While our work answered some questions regarding zero-shot generalization in offline RL, some
questions persist: can GVF-based distances be included in a contrastive objective without the need
for quantile discretization (perhaps through re-scaling or order statistics)? Can the cumulant function
be chosen a priori for a specific task structure other than Procgen, and shown to lead to optimal
representations?
7In the continuous observation space, we learn a d-dimensional successor feature vector zψ via TD and
computing the quantiles over ||zψ||i.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
To ensure reproducibility of our work, we attach the source code to the submission, provide all
proofs of both theorems in the appendix (with assumptions) and will open-source the offline Procgen
benchmark.
References
A. Agarwal, M. Henaff, S. Kakade, and W. Sun. Pc-pg: Policy cover directed exploration for
provable policy gradient learning. Neural Information Processing Systems, 2020.
R. Agarwal, M. C. Machado, P. S. Castro, and M. G. Bellemare. Contrastive behavioral similarity
embeddings for generalization in reinforcement learning. arXiv preprint arXiv:2101.05265, 2021.
P. J. Ball, C. Lu, J. Parker-Holder, and S. Roberts. Augmented world models facilitate zero-shot
dynamics generalization from a single offline environment. International Conference on Machine
Learning, 2021.
A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. Van Hasselt, and D. Silver. Successor
features for transfer in reinforcement learning. arXiv preprint arXiv:1606.05312, 2016.
P. L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE transactions on Information Theory,
44(2):525-536,1998.
M.	G. Bellemare, S. Candido, P. S. Castro, J. Gong, M. C. Machado, S. Moitra, S. S. Ponda, and
Z. Wang. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature,
588(7836):77-82, 2020.
J.	Boyan and A. W. Moore. Generalization in reinforcement learning: Safely approximating the
value function. Advances in neural information processing systems, pages 369-376, 1995.
M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of
visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.
P. S. Castro. Scalable methods for computing state similarity in deterministic markov decision
processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
10069-10076, 2020.
P. S. Castro and D. Precup. Using bisimulation for policy transfer in mdps. In Twenty-Fourth AAAI
Conference on Artificial Intelligence, 2010.
A. S. Chen, S. Nair, and C. Finn. Learning generalizable robotic reward functions from” in-the-wild”
human videos. arXiv preprint arXiv:2103.16817, 2021.
C.-A. Cheng, A. Kolobov, and A. Swaminathan. Heuristic-guided reinforcement learning. arXiv
preprint arXiv:2106.02757, 2021.
K.	Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforce-
ment learning. In International Conference on Machine Learning, pages 1282-1289. PMLR,
2019.
K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark
reinforcement learning. In International conference on machine learning, pages 2048-2056.
PMLR, 2020.
P. Dayan. Improving generalization for temporal difference learning: The successor representation.
Neural Computation, 5(4):613-624, 1993.
A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution
function and of the classical multinomial estimator. The Annals of Mathematical Statistics, pages
642-669, 1956.
10
Under review as a conference paper at ICLR 2022
D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of
Machine Learning Research, 6:503-556, 2005.
L.	Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,
I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner
architectures. In International Conference on Machine Learning, pages 1407-1416. PMLR, 2018.
N.	Ferns, P. Panangaden, and D. Precup. Metrics for finite markov decision processes. In UAI,
volume 4, pages 162-169, 2004.
J.	Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven rein-
forcement learning, 2020.
S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without exploration.
In International Conference on Machine Learning, pages 2052-2062. PMLR, 2019.
C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning con-
tinuous latent space models for representation learning. In International Conference on Machine
Learning, pages 2170-2179. PMLR, 2019.
J.-B. Grill, F. Strub, F. Altche, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires,
Z. D. Guo, M. G. Azar, et al. Bootstrap your own latent: A new approach to self-supervised
learning. arXiv preprint arXiv:2006.07733, 2020.
S. Guadarrama, A. Korattikara, O. Ramirez, P. Castro, E. Holly, S. Fishman, K. Wang, E. Go-
nina, N. Wu, E. Kokiopoulou, L. Sbaiz, J. Smith, G. Bartok, J. Berent, C. Harris, V. Van-
houcke, and E. Brevdo. TF-Agents: A library for reinforcement learning in tensorflow.
https://github.com/tensorflow/agents, 2018. URL https://github.com/
tensorflow/agents. [Online; accessed 4-October-2021].
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual repre-
sentation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 9729-9738, 2020.
M.	Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt, and H. van Hasselt. Multi-task deep
reinforcement learning with popart. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 33, pages 3796-3803, 2019.
I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick, C. Blundell, and
A. Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. In International
Conference on Machine Learning, pages 1480-1490. PMLR, 2017.
R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, and Y. Ben-
gio. Learning deep representations by mutual information estimation and maximization. arXiv
preprint arXiv:1808.06670, 2018.
N.	K. Jong and P. Stone. State abstraction discovery from irrelevant state variables. In IJCAI,
volume 8, pages 752-757. Citeseer, 2005.
P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan.
Supervised contrastive learning. Neural Information Processing Systems, 2020.
R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims. Morel: Model-based offline reinforce-
ment learning. arXiv preprint arXiv:2005.05951, 2020.
I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.
I. Kostrikov, J. Tompson, R. Fergus, and O. Nachum. Offline reinforcement learning with fisher
divergence critic regularization. arXiv preprint arXiv:2103.08050, 2021.
A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offline reinforcement
learning. arXiv preprint arXiv:2006.04779, 2020.
11
Under review as a conference paper at ICLR 2022
S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In Reinforcement learning,
pages 45-73. Springer, 2012.
L. Li, T. J. Walsh, and M. L. Littman. Towards a unified theory of state abstraction for mdps. ISAIM,
4:5, 2006.
Z. Lin, D. Yang, L. Zhao, T. Qin, G. Yang, and T.-Y. Liu. Rd ^2: Reward decomposition with
representation decomposition. Advances in Neural Information Processing Systems, 33, 2020.
G. T. Liu, P.-J. Cheng, and G. Lin. Cross-state self-constraint for feature generalization in deep
reinforcement learning. 2020a.
L.	Liu, W. Hamilton, G. Long, J. Jiang, and H. Larochelle. A universal representation transformer
layer for few-shot image classification. arXiv preprint arXiv:2006.11702, 2020b.
Y. Liu, A. Swaminathan, A. Agarwal, and E. Brunskill. Provably good batch reinforcement learning
without great exploration. NeurIPS, 2020c.
M.	C. Machado, M. G. Bellemare, and M. Bowling. Count-based exploration with the successor
representation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pages 5125-5133, 2020.
H. B. Mann and A. Wald. On stochastic limit and order relationships. The Annals of Mathematical
Statistics, 14(3):217-226, 1943.
B. Mazoure, R. T. d. Combes, T. Doan, P. Bachman, and R. D. Hjelm. Deep reinforcement and
infomax learning. Neural Information Processing Systems, 2020.
B. Mazoure, A. M. Ahmed, P. MacAlpine, R. D. Hjelm, and A. Kolobov. Cross-trajectory represen-
tation learning for zero-shot generalization in rl. arXiv preprint arXiv:2106.02193, 2021.
D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and prov-
ably efficient rich-observation reinforcement learning. In International conference on machine
learning, pages 6961-6971. PMLR, 2020.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. nature, 518(7540):529-533, 2015.
K. P. Murphy. A survey of pomdp solution techniques. environment, 2:X3, 2000.
O. Nachum and M. Yang. Provable representation learning for imitation with contrastive fourier
features. Neural Information Processing Systems, 2021.
J. Oh, S. Singh, and H. Lee. Value prediction network. arXiv preprint arXiv:1707.03497, 2017.
A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748, 2018.
M. L. Puterman. Markov decision processes. Handbooks in operations research and management
science, 2:331-434, 1990.
R.	Raileanu and R. Fergus. Decoupling value and policy for generalization in reinforcement learn-
ing. International Conference on Machine Learning, 2021.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. Courville, and P. Bachman. Data-efficient re-
inforcement learning with self-predictive representations. International Conference on Learning
Representations, 2020.
M. Schwarzer, N. Rajkumar, M. Noukhovitch, A. Anand, L. Charlin, D. Hjelm, P. Bachman, and
A. Courville. Pretraining representations for data-efficient reinforcement learning. arXiv preprint
arXiv:2106.04799, 2021.
12
Under review as a conference paper at ICLR 2022
S.	Sinha and A. Garg. S4rl: Surprisingly simple self-supervision for offline reinforcement learning.
arXiv preprint arXiv:2103.06326, 2021.
J. Song and S. Ermon. Multi-label contrastive predictive coding. Neural Information Processing
Systems, 2020.
X. Song, Y. Jiang, S. Tu, Y. Du, and B. Neyshabur. Observational overfitting in reinforcement
learning. International Conference on Learning Representations, 2019.
A. Srinivas, M. Laskin, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforce-
ment learning. International Conference on Machine Learning, 2020.
A. Stone, O. Ramirez, K. Konolige, and R. Jonschkowski. The distracting control SUite-a Chaneng-
ing benchmark for reinforcement learning from pixels. arXiv preprint arXiv:2101.02722, 2021.
A. Stooke, K. Lee, P. Abbeel, and M. Laskin. DecoUpling representation learning from reinforce-
ment learning. In International Conference on Machine Learning, pages 9870-9879. PMLR,
2021.
W. SUn, J. A. Bagnell, and B. Boots. TrUncated horizon policy search: Combining reinforcement
learning & imitation learning. arXiv preprint arXiv:1805.11240, 2018.
R. S. SUtton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. PrecUp. Horde: A scal-
able real-time architectUre for learning knowledge from UnsUpervised sensorimotor interaction.
In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2,
pages 761-768, 2011.
A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback throUgh coUnterfac-
tUal risk minimization. The Journal of Machine Learning Research, 16(1):1731-1755, 2015.
A. ToUati and Y. Ollivier. Learning one representation to optimize all rewards. arXiv preprint
arXiv:2103.07945, 2021.
E. TriantafilloU, T. ZhU, V. DUmoUlin, P. Lamblin, U. Evci, K. XU, R. Goroshin, C. Gelada, K. Swer-
sky, P.-A. Manzagol, et al. Meta-dataset: A dataset of datasets for learning to learn from few
examples. International Conference on Learning Representations, 2019.
G. Valle-Perez and A. A. Louis. Generalization bounds for deep learning. arXiv preprint
arXiv:2012.04115, 2020.
A. W. van der Vaart. Asymptotic statistics. cambridge series in statistical and probabilistic mathe-
matics, 1998.
R. Wang, Y. Wu, R. Salakhutdinov, and S. M. Kakade. Instabilities of offline rl with pre-trained
neural representation. arXiv preprint arXiv:2103.04947, 2021a.
Y. Wang, R. Wang, and S. M. Kakade. An exponential lower bound for linearly-realizable mdps
with constant suboptimality gap. arXiv preprint arXiv:2103.12690, 2021b.
C.	J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Y. Wu, G. Tucker, and O. Nachum. Behavior regularized offline reinforcement learning. arXiv
preprint arXiv:1911.11361, 2019.
D.	Yarats, A. Zhang, I. Kostrikov, B. Amos, J. Pineau, and R. Fergus. Improving sample efficiency
in model-free reinforcement learning from images. arXiv preprint arXiv:1910.01741, 2019.
T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark
and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning,
pages 1094-1100. PMLR, 2020.
A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine. Learning invariant representations for
reinforcement learning without reconstruction. International Conference on Learning Represen-
tations, 2020.
13