Under review as a conference paper at ICLR 2022
Neural networks with
TRAINABLE MATRIX ACTIVATION FUNCTIONS
Anonymous authors
Paper under double-blind review
Ab stract
The training process of neural networks usually optimizes weights and bias pa-
rameters of linear transformations, while nonlinear activation functions are pre-
specified and fixed. This work develops a systematic approach to constructing
matrix activation functions whose entries are generalized from ReLU. The acti-
vation is based on matrix-vector multiplications using only scalar multiplications
and comparisons. The proposed activation functions depend on parameters that
are trained along with the weights and bias vectors. Neural networks based on
this approach are simple and efficient and are shown to be robust in numerical
experiments.
1	Introduction
In recent decades, deep neural networks (DNNs) have achieved significant successes in many fields
such as computer vision and natural language processing (Voulodimos et al., 2018), (Otter et al.,
2018). The DNN surrogate model is constructed using recursive composition of linear transfor-
mations and nonlinear activation functions. To achieve good performance, it is essential to choose
activation functions suitable for specific applications. In practice, Rectified Linear Unit (ReLU) is
one of the most popular activation functions for its simplicity and efficiency. A drawback of ReLU
is the presence of vanishing gradient in the training process, known as dying ReLU problem (Lu
et al., 2019). Several relatively new activation approaches are proposed to overcome this problem,
e.g., the simple Leaky ReLU, and Piecewise Linear Unit (PLU) (Nicolae, 2018), Softplus (Glo-
rot et al., 2011), Exponential Linear Unit (ELU) (Clevert et al., 2016), Scaled Exponential Linear
Unit (SELU) (Klambauer et al., 2017), Gaussian Error Linear Unit (GELU) (Hendrycks & Gimpel,
2016).
Although the aforementioned activation functions are shown to be competitive in benchmark tests,
they are still fixed nonlinear functions. In a DNN structure, it is often hard to determine a priori the
optimal activation function for a specific application. In this paper, we shall generalize ReLU and
introduce arbitrary trainable matrix activation functions. The effectiveness of the proposed approach
is validated using function approximation examples and well-known benchmark datasets such as
CIFAR-10 and CIFAR-100. There are a few classical works on adaptively tuning of parameters
in the training process, e.g., the parametric ReLU (He et al., 2015b). However, our adaptive matrix
activation functions are shown to be competitive and more robust in those experiments.
1.1	Preliminaries
We consider the general learning process based on a given training set {(xn, fn)}nN=1, where the
inputs {xn}nN=1 ⊂ Rd and outputs {fn}nN=1 ⊂ RJ are implicitly related via an unknown target
function f : Rd → RJ with fn = f (xn). The ReLU activation function is a piecewise linear
function given by
σ(t) = max {t, 0} ,	for t ∈ R.	(1)
In the literature σ is acting component-wise on an input vector. In a DNN, let L be the number
of layers and n` denote the number of neurons at the `-th layer for 0 ≤ ` ≤ L with n0 = d and
nL = J. Let W = (W1,W2,..., WL) ∈ QL=I Rn'×n'-1 denote the tuple of admissible weight
matrices and B = (bi, b2,..., bL ∈ QL= 1 Rn' the tuple of admissible bias vectors. The ReLU
1
Under review as a conference paper at ICLR 2022
DNN approximation to f at the L-th layer is recursively defined as
ηL = ηL,W,B := hwL,bL ◦ σ ◦•••◦ hw2,b2 ◦ σ ◦ hwι,bι,	(2)
where hw`,b`(x) = W'x + b` is an affine linear transformation at the '-th layer, and ◦ denotes
the composition of functions. The traditional training process for such a DNN is to find optimal
W* ∈ QL=ι Rn'×n'-1, B* ∈ QL=ι Rn', (and thus optimal ηL,w*,B*)suchthat
1N
(W*, BJ = argmin N E fn - ηL,W,B (Xn)『.	⑶
W,
n=1
In other words, ηL,W*,B* best fits the data with respect to the `2 norm within the function class
{ηL,W,B}. In practice, the sum of squares norm used in minimization could be replaced with other
convenient norms.
2	Trainable matrix activation function
Having a closer look at ReLU σ, We observe that the activation σ ◦ η'(x) = σ(a(x)) could be
realized as a matrix-vector multiplication σ ◦ η'(x) = D'(η'(x))η'(x) or equivalently σ ◦ n` =
(d` ◦ n`)n`, where n` is column vector-valued in Rn' and d` is a diagonal matrix-valued function
mapping from Rn' to Rn'×n' with entries from the discrete set {0,1}. This is a simple but quite
useful observation. There is no reason to restrict on {0, 1} and we thus look for a larger set of values
over which the diagonal entries of d` are running or sampled. With slight abuse of notation, our
new DNN approximation to f is calculated using the following recurrence relation
ηι = hwι,bι,	η'+ι = hw'+1,b'+1 ◦ [(d`◦ n`)n`],	' =i,...,l- ι.	(4)
Here each d` is diagonal and is of the form
D'(y) = diag(α'(yι), α'(y2),..., a`(jn`)),	y ∈ Rn',	(5)
where a` is a nonlinear function to be determined. Since piecewise constant functions can approx-
imate a continuous function within arbitrarily high accuracy, we choose ɑ` to be the following step
function
't',0,	S ∈ (-∞, s1],
t',1,	S ∈ (s1, s2],
a`(s) = <	.	(6)
.
t',m-1, S ∈ (sm-1, sm],
、t',m,	S ∈ (Sm, ∞),
where m is a positive integer and {t',j}j=0 and {Sj}m=ι are constants. For the time being, let
D'(η'(x))η'(x) = σg(η'(x)) With σ' being a scalar-valued function applied to η'(χ) component-
wise. In the following, we list several o`s corresponding to special a` and visualize them in Figure
1.
m = 1,Sι = 0, t',0 = 0, t`,i = 1 =⇒ σ` is ReLU,
m =1,si =0, t',0 > 0, t`,i = 1 0 σ' is Leaky ReLU,
m = 2,si = 0,S2 = 1, t',0 = t',2 = 0,t',ι = 1 =⇒ σ' is discontinuous.
In our case, we will choose the grid points {Sj}m=ι a priori and train the step values ∪L=1{t',j}jm=0
of α' . In such a way, the resulting DNN may use different activation functions for different layers,
and these activation functions are naturally adapted to the target function f and the target dataset.
Since ReLU and Leaky ReLU are included by our DNN as special cases, the proposed DNN is
clearly not worse than the traditional ones in practice. In the following, we call the neural network
in equation 4, with the activation approach given in equation 5 and equation 6, a DNN based on the
“Trainable Matrix Activation Function (TMAF)”.
2
Under review as a conference paper at ICLR 2022
REMARK
The activation functions used in TMAF neural network are not piecewise constants. Instead, TMAF
activation is realized using matrix-vector multiplication, where entries of those matrices are train-
able piecewise constants. There exist several trainable activation functions in the literature, e.g.,
Parametric ReLU (He et al., 2015b), Pade Activation Unit (PAU)(Molina et al., 2020), Piecewise
Linear Unit (PWLU) (Zhou et al., 2021), Swish (Ramachandran et al., 2017), adaptive rational
functions (Boulle et al., 2020), adaptive splines (Bohra etal., 2020), adaptive activation functions
for physics informed neural networks (ada, 2020). We point out that TMAF is different from the
aforementioned activation approaches. One might think TMAF is similar to PWLU because they
are both defined piecewise. However, by construction TMAF includes discontinuous functions as
special cases (see Figure 1) while PWLU always results in a continuous piecewise linear function
with trainable slopes.
Starting from the diagonal activation d`, We can go one step further to construct more general
activation matrices. First we note that d` could be viewed as a nonlinear operator t` : [C(Rd)]n' →
[C(Rd)]n', where
[TXg)](x) = D'(g(x))g(x),	g = (gι,...,gn )> ∈ [C(Rd)]n', X ∈ Rd.
There seems to be no special reason, aside from keeping the computational cost as low as possible, to
consider only diagonal operators D . A more ambitious approach is to consider a trainable nonlinear
activation operator determined by more general matrices, for example, by a tri-diagonal operator
∕α'(gι(x))	β'(g2(x))	0	…	0	∖
Y'(gι(x))	α'(g2(x))	β'(g3(x))	…	0
PXg)](X)=	.	...	...	...	.	g(X),⑺
0	0	…	α'(gn'-i(x))	e`(gn`(X))
0	0	0	…	Y'(gn'-l(x)) ɑ`(gn` (x))/
for x ∈ Rd. The diagonal entry ɑ` is given in equation 6 while the off-diagonal entries β', γ'
are piecewise constant functions defined in a fashion similar to a`. Theoretically speaking, even
trainable full matrix activation is possible despite of potentially huge training cost. In summary, a
DNN based on trainable nonlinear activation operators {T'}L=1 reads
ηι = hw1,b1,	η'+ι = hW'+1,b'+1 ◦ TXng),	' = 1,...,L- 1.	(8)
As we indicated above, TMAF can be used with any matrix and this will lead to a great flexibility
in training and approximation. In fact, if a sparse matrix D is used, so that the matrix-vector mul-
tiplication is proportional to the number of rows in the matrix, then the resulting algorithm will be
computationally efficient. We can choose p-diagonal matrices, as well as matrices with prescribed
sparsity patterns. Clearly, based on the simple examples we have considered here, one may conclude
that such possibilities, when investigated in depth, can increase the robustness and the accuracy of
the TMAF-NN.
3
Under review as a conference paper at ICLR 2022
Remark
Our observation also applies to an activation function σ other than ReLU. For example, we may
rescale σ(x) to obtain σ(ω3'x) using a set of constants {ωi,'}ι≤i≤n',ι≤'≤L varying layer by layer
and neuron by neuron. Then σ(ω3'x) are used to form a matrix activation function and a TMAF
DNN, where {ω*'} are trained according to given data and are adapted to the target function.
3 Numerical results
In this section, we demonstrate the feasibility and efficiency of TMAF by comparing it with tradi-
tional ReLU-type activation functions. Recall that neurons in the `-th layer will be activated by the
matrix d`. In principle, all parameters in equation 6 are allowed to be trained. To ensure practical
efficiency, We shall fix interval grid points {sj}m=ι Used in ɑ` and only let function values {t',j} in
equation 6 be trained in the following. In each experiment, we use the same learning rates, stochastic
optimization methods, and number NE of epochs (optimization iterations). In particular, the learn-
ing rate is 1e-4 from epoch 1 to n2e and 1e-5 is used from epoch NE + 1 to NE. The optimization
method is ADAM (Kingma & Ba (2015)) based on mini-batches of size 500. Numerical experiments
are performed in PyTorch (Paszke et al. (2019)). We provide two sets of numerical examples:
•	Function approximations by TMAF networks and ReLU-type networks;
•	Classification problems for MNIST and CIFAR set solved by TMAF and ReLU networks.
For the first class of examples we use the '2-loss function as defined in equation 3. For the classifi-
cation problems we consider the cross-entropy that is widely used as a loss function in classification
models. The cross entropy is defined using a training set having p images, each with N pixels. Thus,
the training dataset is equivalent to the vector set {zj}jp=1 ⊂ RN with each zj being an image. The
j-th image belongs to a class cj ∈ {1, . . . , M}. The neural network maps zj to xj ∈ RM,
xj := ηL,W,B(zj) ∈ RM,	zj ∈ RN, j = 1, . . . ,p.
The cross entropy loss function of ηL,W,B then is defined by
E(W, B)
-1XX log (	eχp(Xck，k)
P 白	∖pj=ι exp(Xj,k)
3.1	Approximation of a smooth function
As our first example, we use neural networks to approximate
f(X1,…，Xn) = sin(∏xι +-----+ ∏Xn), Xk ∈ [-2, 2], k = 1,...,n.
The training datasets consist of 20000 input-output data pairs where the input data are randomly
sampled from the hypercube [-2, 2]n based on uniform distribution. The neural networks have sin-
gle or double hidden layers. Each layer (except input and output layers) has 20 neurons. For TMAF
d` in equation 5, the function ɑ` uses intervals (-∞,-1.4), (-1.4, -0.92], (-0.92, -0.56],
(-0.56, -0.26], (-0.26, 0], (0, 0.26], (0.26, 0.56], (0.56, 0.92], (0.92, 1.4], (1.4, ∞) such that prob-
ability over each of the ten intervals is 0.1 with respect to Gaussian distribution. Moreover, we apply
BatchNorm1d in PyTorch to the linear output of neural networks in each hidden layer. The approx-
imation results are shown in Table 1 and Figures 2-3, where Para-ReLU stands for the parametric
ReLU neural network. It is observed that TMAF is the most accurate activation approach in these
examples.
3.2	Approximation of an oscillatory function
The next example is on approximating the following function having high, medium and low fre-
quency components
f(X) = sin(100πX) + cos(50πX) + sin(πX),	(9)
see Figure 4a for an illustration. In fact, the function in equation 9 is rather difficult to capture by
traditional approximation methods as it is highly oscillatory. We consider ReLU, parametric ReLU,
4
Under review as a conference paper at ICLR 2022
	Approximation error			
	Single hiden layer		Two hiden layers	
n	1	2	5	6
ReLU	-0:09-	^034	~144~	^048
Para ReLU	-0.0^	ɪn	-0:09-	-0.47
TMAF	~0.0T	0.05	~0.0F	0.13
Table 1: Approximation errors for sin(πxι +-+ ∏Xn) by neural networks
(a) n = 1
Figure 2: Training errors for sin(πxι +----+ ∏Xn), single hidden layer
and diagonal TMAF neural networks with 3 hidden layers and 400 neurons per layer (except the
first and last layers). We also consider the tri-diagonal TMAF (denoted by Tri-diag TMAF, see
equation 7) with 3 hidden layers and 300 neurons per layer. The training datasets are 20000 input-
output data pairs where the input data are randomly sampled from the interval [-1, 1] based on
uniform distribution.
The diagonal TMAF uses α = a` in equation 6 with intervals (-∞, -1], (-1 + kh, -1 + (k +1)h],
(1, ∞) for h = 0.02, 0 ≤ k ≤ 99. The tri-diagonal TMAF is given in equation 7, where ɑ` is the
same as the diagonal TMAF, while β' is piecewise constant with respect to intervals (-∞, -2.01 +
h], {(-2.01 + kh, -2.01 + (k + 1)h]}k=0, (-0.01, ∞), and γ' is a piecewise constant based on
(-∞, 0.01], {(0.01 + kh, 0.01 + (k + 1)h]}k=0, (2.01, ∞), respectively. Numerical results could
be found in Figures 4b, 5, 6 and Table 2.
Figure 3: Training errors for sin(πxι T---+ ∏Xn), two hidden layers.
5
Under review as a conference paper at ICLR 2022
	final loss
ReLU	T.00
Para ReLU	T.00
Diag TMAF	6.45e-2
Tri-diag TMAF~	5.81e-2
Table 2: Error comparison for sin(100πx) + cos(50πx) + sin(πx)
For this challenging problem, we note that the diagonal TMAF and tri-diagonal TMAF produce
high-quality approximations (see Figures 5 and 4b) while ReLU and parametric ReLU are not able
to approximate the highly oscillating function within reasonable accuracy, see Figure 4b and Table
2. Moreover, it is observed from Figure 6 that ReLU and parametric ReLU actually approximate the
low frequency part of the target function. To capture the high frequency, ReLU-type neural networks
are clearly required to use much more neurons, introducing significantly amount of weight and bias
parameters. On the other hand, increasing the number of intervals in TMAF only lead to a few more
training parameters.
(a) Exact oscillating function
(b) Training loss comparison
Figure 4: Plot of sin(100πx) + cos(50πx) + sin(πx) and training loss comparison
(a) Diag TMAF approximation
(b) Tri-diag TMAF approximation
Figure 5: Approximations to sin(100πx) + cos(50πx) + sin(πx), TMAF-type
3.3	Classification of MNIST and CIFAR datasets
We now test TMAF by classifying images in the MNIST, CIFAR-10 and CIFAR-100 dataset.
For the MNIST set, we implement double layer fully connected networks defined as in equation 2
and equation 4 with 10 neurons per layer (except at the first layer n0 = 764), and we use ReLU or
6
Under review as a conference paper at ICLR 2022
Figure 6: Approximations to sin(100πx) + cos(50πx) + sin(πx), ReLU-type
diagonal TMAF as described in Subsection 3.1. Numerical results are shown in Figures 7a, 7b and
Table 3. We observe that performance of the TMAF and the ReLU networks are similar. Such a
behavior clearly should be expected for a simple dataset such as MNIST.
For the CIFAR-10 dataset, we use the ResNet18 network structure provided by (He et al., 2015a).
The activation functions are still ReLU and the diagonal TMAF used in Subsection 3.1. Numerical
results are presented in Figures 8a, 8b and Table 3. Those parameters given in (Paszke et al., 2019)
are already tuned well with respect to ReLU. Nevertheless, TMAF still produces smaller errors in
the training process and returns better classification results in the evaluation stage, see Table 3.
For the CIFAR-100 dataset, we use the ResNet34 network structure provided by (He et al.,
2015a) with the ReLU and TMAF activation functions in Subsection 3.1. Numerical results are
presented in Figures 9a and 9b. In the training process, TMAF again outperforms the classical
ReLU network.
It is possible to improve the performance of TMAF applied to those benchmark datasets. The key
point is to select suitable intervals in ɑ` to optimize the performance. A simple strategy is to let those
intervals in equation 6 be varying and adjusted in the training (or the evaluation) process, which will
be investigated in our future research.
Figure 7: MNIST: Two hidden layers
References
Adaptive activation functions accelerate convergence in deep and physics-informed neural net-
works. Journal of Computational Physics, 404:109136, 2020. ISSN 0021-9991. doi: https://doi.
7
Under review as a conference paper at ICLR 2022
Dataset	Evaluation Accuracy		
	ReLU	TMAF	Para-ReLU
MNIST (2 hidden IayerS)	91.8%	92.2%	91.5%
CIFAR-10 (ReSnet18)-	77.5%	80.2%	78.1%
Table 3: Evaluation accuracy for CIFAR and MNIST
(SSO-)601
(a) Training loss
(b) Classification accuracy
-sso-)6q
(a) Training loss
Figure 9: Comparison among ReLU, Para-ReLU and TMAF for CIFAR-100
Figure 8: Comparison among ReLU, Para-ReLU and TMAF for CIFAR-10
(b) Classification accuracy
org/10.1016/j.jcp.2019.109136. URL https://www.sciencedirect.com/science/
article/pii/S0021999119308411.
Pakshal Bohra, Joaquim Campos, Harshit Gupta, Shayan Aziznejad, and Michael Unser. Learning
activation functions in deep (spline) neural networks. IEEE Open Journal of Signal Processing,
1:295-309,2020. doi:10.1109/OJSP.2020.3039379.
Nicolas Boulie, Yuji Nakatsukasa, and Alex Townsend. Rational neural networks. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
a3f390d88e4c41f2747bfa2f1b5f87db- Abstract.html.
Djork-Ame CleVerL Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In Yoshua Bengio and Yann LeCun (eds.), 4th Inter-
8
Under review as a conference paper at ICLR 2022
national Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4,
2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.07289.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Geoffrey J. Gordon, David B. Dunson, and Miroslav DUdIk (eds.), Proceedings ofthe Fourteenth
International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale,
USA, April 11-13, 2011, volume 15 of JMLR Proceedings,pp. 315-323. JMLR.org, 2011. URL
http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. arXiv preprint, arXiv: 1512.03385, 2015a. URL http://arxiv.org/arXiv:
1512.03385.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. arXiv preprint, arXiv: 1502.01852, 2015b.
URL http://arxiv.org/abs/1502.01852.
Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian
error linear units. arXiv preprint, arXiv: 1606.08415, 2016. URL http://arxiv.org/abs/
1606.08415.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
GUnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-
normalizing neural networks. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
971-980, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
5d44ee6f2c3f71b73125876103c8f6c4- Abstract.html.
Lu Lu, Yeonjong Shin, Yanhui Su, and George E. Karniadakis. Dying relu and initialization: Theory
and numerical examples. arXiv preprint, arXiv: 1903.06733, 2019. URL http://arxiv.
org/abs/1903.06733.
Alejandro Molina, Patrick Schramowski, and Kristian Kersting. Pade activation units: End-to-end
learning of flexible activation functions in deep networks. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=BJlBSkHtDS.
Andrei Nicolae. PLU: the piecewise linear unit activation function. arXiv preprint, arXiv:
1809.09534, 2018. URL http://arxiv.org/abs/1809.09534.
Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita. A survey of the usages of deep learning in
natural language processing. arXiv preprint, arXiv: 1807.10854, 2018. URL http://arxiv.
org/abs/1807.10854.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. CoRR,
abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941.
9
Under review as a conference paper at ICLR 2022
Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis.
Deep learning for computer vision: A brief review. Computational Intelligence and Neuro-
science, 2018:7068349, February 2018. ISSN 1687-5265. URL https://doi.org/10.
1155/2018/7068349.
Yucong Zhou, Zezhou Zhu, and Zhao Zhong. Learning specialized activation functions with the
piecewise linear unit. CoRR, abs/2104.03693, 2021. URL https://arxiv.org/abs/
2104.03693.
10