Under review as a conference paper at ICLR 2022
Reward Learning as Doubly Nonparametric
Bandits: Optimal Design and Scaling Laws
Anonymous authors
Paper under double-blind review
Ab stract
Specifying reward functions for complex tasks like object manipulation or driving
is challenging to do by hand. Reward learning seeks to address this by learning
a reward model using human feedback on selected query policies. This shifts the
burden of reward specification to the optimal design of the queries. We propose
a theoretical framework for studying reward learning and the associated optimal
experiment design problem. Our framework models rewards and policies as non-
parametric functions belonging to subsets of Reproducing Kernel Hilbert Spaces
(RKHSs). The learner receives (noisy) oracle access to a true reward and must
output a policy that performs well under the true reward. For this setting, we first
derive non-asymptotic excess risk bounds for a simple plug-in estimator based on
ridge regression. We then solve the query design problem by optimizing these risk
bounds with respect to the choice of query set and obtain a finite sample statistical
rate, which depends primarily on the eigenvalue spectrum of a certain linear oper-
ator on the RKHSs. Despite the generality of these results, our bounds are stronger
than previous bounds developed for more specialized problems. We specifically
show that the well-studied problem of Gaussian process (GP) bandit optimization
is a special case of our framework, and that our bounds either improve or are
competitive with known regret guarantees for the Matern kernel.
1	Introduction
Specifying the reward function accurately fora desired objective, or reward engineering, is challeng-
ing to perform by hand, as the consequences of even small errors can be drastic (Hadfield-Menell
et al., 2017). To address this, reward learning seeks to learn a predictive model of the reward func-
tion from data, which is obtained from carefully selected queries to human annotators. The learned
reward model is then used as the optimization objective for policy learning. Reward learning has
achieved significant empirical success in domains such as text summarization (Stiennon et al., 2020;
Bohm et al., 2019), robot locomotion (Daniel et al., 2014), predicting driving styles (Kuderer et al.,
2015), and Atari game playing (Christiano et al., 2017).
Despite their success, reward learning methods still lack theoretical grounding. Moreover, their
behavior can be brittle even on simple tasks, due to the difficulty of choosing appropriate queries
and due to feedback loops from adaptive querying (Freire et al., 2020). Indeed, an ablation study
in Christiano et al. (2017) suggests that random queries can outperform orbe competitive with adap-
tive query procedures. To address these issues, we provide a theoretical framework for analyzing
reward learning, framing it as a doubly nonparametric experimental design problem. This frame-
work helps elucidate the role of query selection (Chaloner & Verdinelli, 1995) and also enables us to
derive scaling laws—how the sizes of the policy and reward models affect the query complexity—for
reward learning (Kaplan et al., 2020).
Proposed framework. In our framework, we suppose we are given a reward class Cr and policy
class Cn. Our goal is to find a policy ∏ ∈ Cn that performs well according to an unknown true
reward r* ∈ Cr. To do this, We query policies π ∈ Cn, observing noisy estimates of their true
reward, and use this information to choose the eventual policy ∏.
To be compatible with modern nonparametric learning methods (i.e. neural nets), we view Cr and
Cn as subsets of Reproducing Kernel Hilbert Spaces (RKHS). The learner therefore optimizes a non-
parametric reward function over a nonparametric space of policies, making the task “doubly” non-
1
Under review as a conference paper at ICLR 2022
parametric. In contrast, previous work considers a nonparametric function class or reward class, but
typically not both. For instance, nonparametric zeroth order or bandit optimization (Srinivas et al.,
2010; Mockus, 2012; Wang et al., 2018) considers a nonparametric function on a finite-dimensional
input space. Conversely, nonparametric supervised learning (Wahba, 1990; Hofmann et al., 2008)
minimizes a known loss function over a nonparametric input space.
The doubly nonparametric nature of our task poses new challenges. The (possibly) infinite-
dimensional RKHS requires the learner to select which subspace to explore given a finite number
of queries. Furthermore, the unknown reward function makes it challenging for the learner to rea-
son about the information gained from the selected query policies. We address these challenges by
deriving a risk upper bound for a family of plug-in estimators based on ridge regression, and then
optimizing this bound to solve the optimal design task.
In addition to the optimal design problem, our framework allows us to study scaling laws with
respect to the reward (or policy) class by varying the rate of decay of their corresponding eigen-
spectrum. This decay rate determines the effective dimensionality of a RKHS (Zhang, 2002), and
provides a natural proxy for varying the the size of the reward or policy class. Qualitatively, our
main results show that the excess risk asymptotically vanishes as long as the policy class grows at a
slower rate relative to the reward class.
Sharpness of analysis. Our risk bounds apply to reward and policy classes of arbitrary or even infi-
nite dimensionality. Despite this generality, we show they provide stronger guarantees than previous
known bounds for the specialized settings of compact policy sets and kernel multi-armed bandits.
In Section 4.3, we look at a special case of our problem when the policy set Cπ is a compact
subspace and thus has finite rank. For these instances, we show that our learning algorithm obtains
β	β-1
a better excess risk O(n-β+2) versus a rate of O(n 2(β+1)) obtained by the adaptive GP-UCB
algorithm (Srinivas et al., 2010), where β > 0 is a power law decay rate.
In Section 5, we specialize our general results to the well-studied problem of Gaussian process bandit
optimization (Williams & Rasmussen, 2006), also known as kernel multi-armed bandit (MAB).
Specifically, for the class of Matern kernels with parameter V in d dimensions, We show that our
〜	4ν + d(4d + 6)
algorithm achieves a regret bound of O(T 6ν+d(4d+7)) which is strictly better than those achieved by
the GP-UCB and GP-Thompson Sampling (GP-TS) (Chowdhury & Gopalan, 2017) algorithms and
comparable with π-GP UCB (Janz et al., 2020) and supKernelUCB (Valko et al., 2013; Vakili et al.,
2021); see Table 1 for details. GP-UCB and GP-TS are only yield sub-linear regret bounds when
the smoothness of the kernel ν > d2—thus in high dimensions, these bounds essentially become
vacuous. The π-GP UCB algorithm was designed specifically to overcome this issue. Our proposed
algorithm achieves sublinear regret for all ν > 3/2.
Our Contributions. We propose doubly-nonparametric bandits as a framework for theoretically
studying the reward learning problem. Within this framework, we obtain finite sample risk bounds
for a ridge regression based plug-in estimator and derive scaling laws for reward learning. From a
technical standpoint, we study the optimal design problem for our estimator to select informative
query points by showing that the excess risk depends only on the spectral properties of a certain
operator of the two RKHSs and the empirical covariance matrix. As a corollary of our risk bounds,
we provide sharper regret bounds for a class of kernel MAB problems compared to several exist-
ing algorithms, showing that the doubly-nonparametric lens of reward learning is fruitful even for
“singly-nonparametric” tasks. To obtain these bounds, our reduction carefully constructs two differ-
ent RKHSs to embed the input space and reward function into a policy and reward class.
2	Framework: Doubly nonparametric Bandits
Our framework considers non-parametric policy learning with non-parametric reward models. We
let π ∈ Hπ denote an arbitrary policy and r ∈ Hr denote an arbitrary reward function, where Hπ
and Hr are Reproducing Kernel Hilbert Spaces. For technical reasons, we assume the corresponding
kernel functions Kπ and Kr both satisfy the Hilbert-Schmidt condition (see Appendix A for details).
We let F (π, r) ∈ R denote the reward obtained by selecting policy π under reward function r
and consider the case where the evaluation functional F is linear in both π and r. In other words,
F(π, r) = hr, MπiHr where M : Hπ 7→ Hr is a known linear mapping from the policy space to the
2
Under review as a conference paper at ICLR 2022
Algorithm	Regret RT	Non-vacuous regime
GP-UCB (Srinivas et al., 2010)	〜	2ν + d(3d+3) O(T 4v + d(2d+2))	ν> d2+d
GP-TS (Chowdhury & Gopalan, 2017)	—二	2ν + d(3d+3) O(T 4v + d(2d+2))	ν> d⅛d —
Our work	-Z	4ν + d(4d+6) O(T 6v + d(4d+7))	ν> 2
π-GP-UCB (Janz et al., 2020)	-Z	2ν + d(2d+3) O(T 4v + d(2d+4))	ν > 1
SupKernelUCB (Vakili et al., 2021)	ν+d O(T 2ν+d)	ν > 1
Table 1. Our algorithm specializes to the case of kernel multi-armed bandits and yields strong bounds
(See eq. (9) for precise definition of regret). For a d-dimensional Matern kernel With smoothness ν,
we outperform both GP-UCB and GP-TS unless ν & d2 . The only works to achieve better bounds
for small V are π-GP UCB, which was designed specifically for the Matern kernel and a very recent
analysis of the SupKernelUCB Which achieves near minimax rates.
reward space. Since Hn and Hr may be infinite-dimensional, linearity is only a weak restriction-
e.g. the map f 7→ f(x) is linear in f for any RKHS.
To incorporate problem structure, we let r* denote the true reward function and assume that r* ∈ Cr
for some known set Cr ⊆ Hr such that ∣∣r* ∣∣Hr = 1. We further assume that policies ∏ are restricted
to lie in some Cπ which is a subset of the unit ball in Hπ (for instance, Cπ might incorporate
physical constraints on implementable policies). Thus, given the true reward r*, the optimal policy
(for a compact Cn) is π* ∈ argmax∏∈cπ F(π,r*). This proposed framework, which allows for
infinite-dimensional policy as well as reward classes, allows us to study how both the policy and
reward space affect the difficulty of learning.
Query access to reward r*. The true reward function r* is unknown to the learner but is acces-
sible via queries to an oracle (e.g. a human expert), which provide noisy zeroth-order (or bandit)
evaluations of the reward r* . When queried with a policy π ∈ Cn , the oracle provides a response
Oracle O『* : π → F(π, r*)+ E where E 〜N(0, τ2) ,	(1)
with τ2 denoting the variance of the response. There are two possible query models: passive
queries (Atkinson, 1996; Sebastiani & Wynn, 2000), where the learner selects all queries at the same
time, and active queries (Bubeck et al., 2011; Lattimore & Szepesvari, 2020), where the learner is
allowed to select queries sequentially. Our focus in this work will be on the passive query model,
but in many cases we will outperform existing active query algorithms.
Problem statement. Given passive access to the oracle Or*, the objective of the learner is to output
a policy ∏ ∈ Cn that has small excess risk ∆, defined as
∆(∏;r*) := F(π*,r*) - F(∏,r*) .	(2)
We think of queries to the oracle as expensive, and are interested in achieving low excess risk with
as few queries as possible. This notion of excess risk is also studied by the term simple regret in
pure exploration bandit problems (Lattimore & Szepesvari, 2020).
Representations in '2(N). By Mercer,s theorem, we can represent any RKHS as a subset of '2(N).
Formally, the policy and the reward spaces are isomorphic to the ellipsoids
Hπ
:={X κ∏,jφ∏,j I (Knj )∞=ι ∈ `2 (N) with X μπ,j < ∞}
and
(X∞∞2
Kr,j φr,j I(Kr,j )j∞=1 ∈ ' (N) With X : -2^- < ∞
for appropriately chosen eigenfunctions φ∏,j and φr,j, and corresponding eigenvalues μ∏,j and
μr,j (Wainwright, 2019). These are defined with respect to a base measure P over the input domain;
see Appendix A for details. With a slight abuse of notation, going forward, we will use π and r
to denote the corresponding coefficients (κn,j) and (κr,j) in the expansion above. 1 With this, the
1While the eigenfunctions φπ and φr can be different, this representation can still be used by modifying the
map M appropriately. This is detailed in Appendix A.
3
Under review as a conference paper at ICLR 2022
Algorithm 1: Policy Learning via Reward Learning
Input: Number of queries n, policy set Cn, oracle Or*
Select n policies Q = {∏ι,..., πn} and receive noisy reward evaluations yi = Or* (∏i).
Estimate r using observed responses {(∏1,y1),..., (∏n, y~n)} using ridge regression (4).
Obtain plug-in policy ∏piug ∈ argmax∏∈cπ F(π, r).
Output: Policy ∏piug
inner products associated with Hπ and Hr simplify
∞∞
h∏ι,∏2>H∏ ：= X a/7r2,j	and hr1,r2iHr := X r';", .	(3)
Also let Sr : = diag(μ-i) and Sn : = diag(μ-j) be diagonal matrices comprising the inverse of
the eigenvalues ofHr and Hπ. With this notation, ifwe view the map M as a (infinite-dimensional)
matrix, its Hermitian adjoint2 is equal to M* = S-1M> Sr.
In order for the evaluation functional F(∏,r*) to be finite for all ∏ ∈ Hn, the operator norm
1	-1
∣∣Sr2 MSn 2 IloP must be bounded (see Appendix A). We will see later that the decay of this op-
erator’s singular values is closely related to the difficulty of learning in our setting.
3	Algorithm: Policy Learning via Reward Learning
Given the setup above, we now describe a meta-algorithm, policy learning via reward learning (Al-
gorithm 1), for the non-parametric policy learning problem. The algorithm is a three-stage proce-
dure: it (i) selects a subset of policies Q to query for reward feedback, (ii) uses the responses to
learn a reward estimate r, and (iii) optimizes this learnt estimate to output the policy ∏piug, that is,
∏piug ∈ argmin∏∈cπ h^ M∏iHr. Such general plug-in procedure have been studied in the statis-
tics (Van der Vaart, 2000) and the machine learning (Devroye et al., 2013) literature. We analyze the
excess risk of this estimator for our doubly-nonparametric setup and use this risk bound to select our
query set Q. We now discuss the two key design choices in our algorithm: the choice of the reward
estimation procedure as well as the choice of query set Q.
ReWard learning via ridge regression. We estimate the reward r via ridge regression in the
RKHS Hr (Friedman et al., 2001; Shawe-Taylor et al., 2004). Suppose that in the first step of
the algorithm, we have already queried the oracle on n policies and let {(πi, yi)}in=1 represent the
query-response pairs. For a regularization parameter λreg > 0, the ridge regression estimate of the
reward function is
1n
r ∈ argmin 一	(yi - hr, MπiiHr)2 + λreg∣r∣2Hr .	(4)
r∈Hr n i=1
The parameter λreg, which is usually set as a function of n, controls the bias-variance trade-off in
estimating r* —smaller values of λreg reduce bias while larger values help reduce variance.
Excess risk bound for fixed query set. Observe that the plug-in estimator ∏piug(Q) is implicitly
a function of the query set Q. Ideally, we want to choose the set Q which minimizes the expected
risk of the plugin estimator. This requires us to solve the optimization problem
Q = argminE[∆(∏piug(S)；r*)] .	(5)
S:|S|<n
However, solving the above precisely requires knowledge about the underlying reward function r*,
and the combinatorial nature of the optimization problem makes it hard to find an exact solution. To
address this, We first upper bound the excess risk of the plug-in policy ∏piug in terms of the query set
Q = {π1, . . . , πn}. The following theorem3 bounds the excess risk in terms of the spectrum of the
spaces Hr and Hn, as well as the covariance matrix of the queried policies Σq : = ɪ P∏∈q ∏∏>.
2Recall the Hermitian adjoint of M satisfies(r, MniHr =(M*r, ∏)hπ
3Throughout the paper, for clarity purposes, we denote by c a universal constant whose value changes across
lines. All our proofs in the appendices explicitly track this constant.
4
Under review as a conference paper at ICLR 2022
Theorem 1 (Excess risk of plug-in). For any query set Q consisting of n policies and regularization
parameter 入胃 > 0, the excess risk ofthe plug-in estimator ∏piug is upper bounded as
E[∆(∏piug; r*)] ≤ 2E[∣∣M*(r* -⑶底].	(6)
In addition, letting A = MΣQM>Sr + λregI, the expected squared distance is equal to
E[kM*(r* - ^)kH∏] = λ2eg ∙∣∣M*AT叫Hn + 三∙ tr [s∏(M*A-1M)Σq(M*A-1M)>].	⑺
The proof follows a standard analysis of ridge regression and is deferred to Appendix B. Observe
that in the above theorem, the query set π ∈ Q participates in the excess risk only via the covariance
ΣQ. The risk bound is the sum of two term: the first corresponding to the bias and the second
corresponding to the variance. In both these terms, ΣQ appears as part of A-1—thus query sets Q
which induce a larger correlation with the map M will generally have lower excess risk. Choices
of queries which are orthogonal to the right singular vectors of M will have a constant excess risk,
since for those directions the matrix A ≈ λreg I .
As shown later in the appendix, in the special case when the policy set consists of the entire unit
ball Cπ = {π ∈ Hπ | kπkHπ ≤ 1}, the excess risk bound can be improved by a quadratic factor:
E[∆(∏piug;r*)] ≤ O4|M*(r* — r)kH). SUch a phenomenon was first observed in the finite-
dimensional setup by Rusmevichientong & Tsitsiklis (2010).
4 Query selection and statistical guarantees
We now show how to select the query set Q effectively and study the excess risk of the corresponding
plug-in estimator ∏piug. We will start with the special case where the policy set Cn is the unit ball in
Hn and the map M is diagonal, and then generalize to arbitrary policy sets. In both cases, low excess
risk can be achieved by repeatedly querying (approximations of) the projections of top eigenvectors
of M*M onto the Hn space. For the special case when the map M is diagonal, this is reduces to
querying the top eigenvectors of Hn .
The excess risk will ultimately depend on the the eigensPectrUm of the operator S- 2 M> Sr MSn 2,
which is similar to the operator M*M. Additionally, to interpret our results, we instantiate them
1
1
for a power law spectrum with exponent β > 0, that is, σj(Sn 2 M>SrMSn 2) N j-β. where σj
corresponds to the j th singUlar valUe of the corresponding operator. SUch power law spectra have
been observed in a variety of practical settings, for instance, in the Hessian of trained deep neural
networks (Ghorbani et al., 2019).
4.1 WARM-UP: Cn = UNIT BALL, M = DIAGONAL
In order to get some intuition, we study the special case where the policy set Cn consists of the entire
unit ball in the space Hn and the map M is diagonal with M = diag(νj). Further, let us denote the
operator M = SJ/MS-1/.
For this special case, our sampling algorithm (Algorithm 2) simply selects the top J eigenvectors
of the space Hn to query, for some value J which depends on the decay exponent β. To see why,
observe that for a diagonal map M, the right singular vectors of the operator M are the same as the
eigenvectors of the policy space Hn . Therefore, the choice of policy πj in our algorithm is simply
the scaled eigenfunction √μ∏~j ∙ φ∏,j. Having selected these J queries, the algorithm queries each
one of the J times and uses this as query set Q.
The intuition for this choice of query set Q is that since we are in the passive setup with no knowl-
edge of r * , any policy π ∈ Cn can be an optimal policy. By querying the top J ones out of these,
we can obtain a good enough approximation to the performance of any policy in the unit ball. The
particular choice of the parameter J depends on the number of queries n available. Since the or-
acle responses are noisy, to reduce variance in the responses along those directions, our algorithm
performs multiple queries along the same direction.
If we further consider the special case when the policies and rewards correspond to the unit balls in
the finite dimensional spaces Rdπ and Rdr respectively, our choice of query set queries the directions
5
Under review as a conference paper at ICLR 2022
{ei}i=∏ι, each for J = + number of times. Intuitively, this strategy works well because without any
prior over the unknown reward function, the optimal strategy in the passive setup is to explore all
directions equally and this is precisely our set of chosen queries. This simple query strategy enjoys
the following excess risk bound.
Proposition 1 (Risk bound for Cπ = unit ball.). For any J ≤ n and regularization parameter
λreg > 0, consider the plug-in estimator obtained via the passive sampling algorithm which explores
the first J eigenfunctions of Hπ. The excess risk satisfies
E[∆(∏piug; r*)] ≤ C ∙(1 + nλ^)∙ max{sγp ZZJ? , Sup Zj} ,	(8)
.	,.	...A	ν2μ∏,j	7-c ∙	T	,
where the quantity Zj = j and c> 0 Is some universal constant.
We defer the proof of the above proposition to Appendix B. The choice of the exploration parameter
J allows us to trade-off between the two terms inside the maximum. Typically, the second term will
be maximized at j = J + 1. For the first term, the supremum depends on the choice of λreg — for
small values of λreg , the sup is achieved at j = 1 while for larger values, it is achieved at j = J. In
order to gain more intuition about this bound, we instantiate this for the power law decay.
Corollary 1 (Risk bound for power-law decay). Suppose that eigenvalues of the police space Hπ
decay asj-βπ, reward space Hr as j-βr and the singular values of map M as j-βM. This satisfies
the power law assumption with exponent β = βπ + βM - βr. The plug-in estimator with exploration
1	β + 1	β
parameter J = nβ+2 and regularization λy = n-β+2 satisfies E[∆(∏piug; r*)] ≤ Cn-β+2.
The above bound shows that our algorithm can learn in the framework as long as β > 0 or equiva-
lently βπ + βM > βr, with better rates for larger values of β. Thus, for a fixed size of reward class
βr, the learning rate improves as the policy class grows smaller (β∏ increases) 一 this is intuitive since
we are required to search over a smaller policy space. On the other hand, for a fixed policy class βπ ,
our excess risk rate gets better as the reward class grows in size (βr increases) 一 this is because a
larger set of reward functions have similar optimal policies and hence learning gets easier.
4.2 General policy sets
We now describe our choice of query sets Q for general policy sets Cπ. Our strategy, described in
Algorithm 2, differs from the above special case in that we need to take into account the interaction
of the policy space Hπ with the map M. Specifically, we show in Appendix B that the upper bound
in Theorem 1 can be diagonalized for this general case via a transformation.
Let us denote the operator MM = SyMSn'1l Our transformation reveals that the relevant directions
to query for this general case corresponds to the columns of Φ∏ S-1/2 Φ> VM where , then VM are
the eigenvectors of the self-adjoint operator M 1 M - and it is precisely a subset of these directions
that our algorithm queries.
In order to be able to query these policies, we require the set Cπ to contain some policies which
align well with them. We formally state this regularity assumption below.
Assumption 1 (Regularity assumption on Cn). For any eigenfunction φM j ofthe operator M 1 M,
consider the policy πj = Φ∏ S-1∕2Φ>φM j∙. There exists a policy ∏j in policy set Cn such that for
some constant Cn > 0, we have ∏j∏> 占 Cnπjπ>.
The above assumption requires that for every choice of the policy πj in Algorithm 2, the set Cn
has the another policy ∏j which is collinear with it. This assumption can be relaxed in various
ways (for instance via convexification) but we omit this as it is not needed for our results. Given
this assumption, the following theorem, a generalization of Proposition 1, provides a bound on the
excess risk for the plug-in estimate for general policy sets Cn.
Theorem 2 (Risk bound for general policy sets Cn .). For any J ≤ n, regularization parameter
λreg > 0 and set Cn satisfying Assumption 1, let ∏piug be the estimator output by Algorithm 1. The
squared excess risk satisfies
(EHnPlUg；r*用2 ≤C∙ (1 + nλ2eg) ∙maχ{supζ2λ+JJ,Supg},
6
Under review as a conference paper at ICLR 2022
Algorithm 2: Passive querying strategy
Input: Number of queries n, map M, policy set Cn, exploration parameter J
-n~^>	-
Construct linear map M = Sr MSn 2 and compute eigenvectors {Φm j}j of operator M 1 M
Set policy ∏j- = Φn S- - Φ>ΦMj for all j ≤ J
Obtain policy ∏j ∈ Cn such that ∏j∏> 占 Cn ∏j ∏>
Form query set Q = {n(n/J),..., ∏(n/')} where a(b) = {a,..., a}} repeated b times
Output: Query set Q
ι	■**	1	1
where the values Zj correspond to the jtth eigen values of the operator M *M with M = Sr MSn.
We defer the proof of this theorem to Appendix B. The proof of this theorem goes via a transfor-
mation which diagonalizes the excess risk bound and reduces the problem to a similar setup as that
of Proposition 1. Additionally, Assumption 1 allows us to generalize the results to arbitrary policy
sets Cπ. Note that the above upper bounds the square of the excess risk. As discussed in Section 3,
one can obtain a quadratic improvement in this rate if the set Cπ is the entire unit ball in Hπ. We
specialize the above bound for the power law decay assumption in the following corollary.
Corollary 2 (Risk bound for power-law decay). Suppose that eigenspectrum of the opera-
—1 丁	— 1
tor Sn 2 M>Sr MSn 2
σj (S- 2 M>Sr MSn2)
satisfy the power law assumption with exponent β > 0, that is,
X j-β. The plug-in estimator ∏piug with parameter J = nβ+2 and reg-
ularization λreg
_ β + 1
n-β+2 satisfies E[∆(∏piug; r*)] ≤ Cn
β
2(β + 2)
1
1

The above bound indicates that for the general case, learning is possible if the spectrum decay has
parameter β > 0. To get such a spectrum decay with the operator defined in the above corollary,
one sufficient condition is that the map M does not flip the larger eigenvectors of Hπ towards
the smaller eigenvectors of Hr , that is, the map M preserves the ordering of the eigenvectors of
Hπ when transformed to the space Hr . Such a misaligned scenario would require learning a very
accurate representation of the reward to learn a good policy and will make learning harder.
4.3 Comparison with UCB-style adaptive algorithms
We next turn to evaluating the sharpness of Theorem 2. Existing frameworks for studying “singly”-
nonparametric setups require the input domain to be compact. In our doubly-nonparametric setup,
the input space is the policy set Cn which is often non-compact (i.e. the unit ball is not compact
in infinite dimensions). We address this for singly-nonparametrics algorithm by taking a finite-
dimensional approximation.
Even though our proposed method is passive, it achieves better rates than well-known adaptive
sampling algorithms. Specifically, in the power law setting of Section 4.1, the analysis of GP-
- β-1
UCB algorithm (Srinivas et al., 2010) provides a rate of O(n 2(β+1)), which is strictly worse than
the O(n-β+1) obtained by our analysis in Corollary 1. We refer the reader to Proposition 2 in
Appendix D for an exact statement. The proof adapts the analysis from Srinivas et al. (2010),
which hinges on a quantity called the information gain, which we bound for our setup. While we
are comparing upper bounds for the two algorithms, we believe that our improved bound is due
to a better algorithm and not an analysis gap. While we expect adaptive algorithms to perform
better than passive ones in general (Lattimore & Hao, 2021), UCB style algorithms require the
construction of confidence intervals around input points, which crucially dictate the regret bounds of
such algorithms. In the frequentist setup, the best known such bounds (Vakili et al., 2021) are known
to yield suboptimal regret rates and it is an open question as to whether these can be improved.
5	New bounds for kernel multi-armed bandits
In the previous subsection, we saw that our passive sampling algorithm actually outperforms exist-
ing adaptive sampling algorithms for the reward learning task we care about. Here we take this a step
7
Under review as a conference paper at ICLR 2022
further—we specialize our algorithm to the case of kernel MABs, and show that it outperforms stan-
dard algorithms for that setting and is competitive with a specialized algorithm for Matern kernels.
We consider the task of maximizing an unknown function f * : X → R over its domain X ⊂ Rd.
In the kernel multi-armed bandit (MAB) setup, this unknown function f belongs to an RKHS H,
equipped with a positive-definite kernel4 K, such that kf* kH = 1. Let us further restrict our attention
to the space of input points X = {x ∈ Rd | kxk2 ≤ 1}. The learner is allowed to access this function
via a noisy zeroth-order oracle Of* : X → f *(χ) + η where η 〜N (0, T 2). Going forward We will
assume that T = 1. The above oracle is similar to the reward oracle Or*, except that the query points
x belong to a finite dimensional space and f * is a non-linear function of the query point x. The goal
in MAB is to minimize the T -step regret
T
Rt : = maxf*(x) - Xf*(xt) ,	(9)
x∈X
t=1
where xt is the datapoint queried in the tth round. There have been several algorithms proposed
to solve this problem including general purpose UCB algorithms (Srinivas et al., 2010; Chowdhury
& Gopalan, 2017), Thompson sampling approaches (Chowdhury & Gopalan, 2017), and special-
purpose algorithms for specific kernels (Janz et al., 2020).
We next show that kernel MAB can be cast as a special case of our non-parametric policy learning
framework. The resulting regret bounds, derived from an application of Theorem 3, are better than
several general purpose algorithms (GP-UCB, IGP-UCB, GP-TS) and comparable to those special-
ized for the Matern kernel (π-GP-UCB).
In order to reduce kernel MAB to our framework, we need to introduce three elements - the policy
space Hπ, the reward space Hr and the map M. We would like spaces Hr and Hπ such that (1)
the resulting objective F (r, π) is linear in this space, (2) the resulting rewards and policies have
unit norm in their respective space, and (3) we have a good understanding of the eigenvalues of the
resulting operator. This last point ensures that we can employ our upper bounds from Section 4.
Before we define these, we let C denote an -net of the input space X under the `2 norm and denote
its size by Ncov(). We define the kernel matrix K ∈ RNcov×Ncov on points selected in the cover as
K(i, j) = K(xi, xj) for all (xi, xj) ∈ C × C.
Reward space Hr . Given the RKHS H as well as the elements of the cover C, we view the reward
function as a map from C to R, or equivalently as a vector in RNcov() . More precisely, letting
f = [f(x1), . . . , f (xNcov)] denote the vector of evaluations ofa function f, we define
Hr := span{f∣f ∈ H} with (九/2)叫：=f>K-1f2, .	(10)
With this notation, we define the true reward r* : = f* = [f *(χι),..., f *(xncov )].
Policy Space Hπ. Similarly to rewards, we will embed policies in RNcov . For any point x ∈ C, let
kx = [K(x, x1), . . . , K(x, xNcov)] denote the corresponding vector in RNcov obtained by evaluating
the kernel K over the cover. Then, the space
Hπ : = span{kx | x∈C} with hk1,k2iHπ := hk1,K-2k2i .	(11)
The choice of the above norm ensures that hki, kj iHπ = hK-1ki, K-1kji = δi,j for all (xi, xj) ∈
C × C . Thus in particular, Hπ contains an orthonormal embedding of the set of vectors {kx}x∈C .
Map M. Both the reward space Hr and policy space Hπ can be associated with RNcov . Under this
transformation, the evaluation f* (x) for any x ∈ C corresponds to the standard inner product with
F(r*,∏χ) = f *(x) = (f*)τKTkx = hr*,kχ>Hr. This indicates that we should take the map M
to be the identity. Furthermore, as a simple application of Mercer’s theorem it follows that this map
M is a bounded linear operator.
We make an additional assumption on the kernel function K, requiring it to be Lipschitz in its input
arguments. This assumption is often satisfied, in particular for the Matern kernel when ν > 3/2.
Assumption 2 (Lipschitz Kernel K). The Kernel K associated with the Hilbert space H is LK -
Lipschitz with respect to the '2-norm for some LK > 0: ∣K(x,y) — K(x,x)∣ ≤ LKkx 一
yk2 for all x ∈ X, y ∈ X. Furthermore, the kernel satisfies K(x, x) = 1for all points x ∈ X.
4We require that the kernel K be a Mercer’s kernel satisfying K(x, x) = c for all x ∈ X .
8
Under review as a conference paper at ICLR 2022
(a)
Figure 1. (a) Corroborating upper bound from Corollary 1. Our theoretical bounds predict a rate
of n-0.27 and the experiment shows an almost matching rate of n-0.28. (b) As the dimension d is
increased, the excess risk curves asymptote at different levels for different n. This shows that our
algorithm achieves non-vacuous error for the doubly-nonparametric set in the regime d → ∞.
Dimension
(b)
Applying Theorem 2 under the above assumption, we obtain the following excess risk bound for the
plug-in estimator evaluated on the unknown function f *.
Theorem 3 (Excess risk for Kernel MAB). Suppose that the eigenvalues of a LK-Lipschitz kernel
K satisfy the power-law decay μj N j-β. Let XPlUg be the output of Algorithm 1 using n queries to
the oracle Of*. Then, for any value of β > 1 + d + log( 1) and e ∈ (0,1), the excess risk satisfies
1—	β	1 -β
max f * (X)- f (XPlUg) . NCovYe) ∙ n 2(β+2) + NCov (e) + LLKe ,
χ“∣χ∣∣2≤i
with probability at least 1 - δ.
For Matern kernels, it is known that the eigenvalues decay with parameter β = 1 + 2ν (Janz et al.,
2020). Using this, we can obtain the following corollary.
Corollary 3 (Regret bound for Matern Kernel). Consider the family OfMatern kernels with pa-
rameter ν > 2 defined with the Euclidean norm over Rd. The T-step regret of our algorithm is
4ν + d(6 + 4d)、
Rmat,T = O ( T 6ν + d(7+4d))
The above bound is for regret, which is an online notion, while our previous results are offline
notions. We get from one to the other using a standard batch-to-online conversion bound based on
an explore-then-commit strategy. Table 1 compares the above bound to the existing bounds in the
literature. While the bounds for GP-UCB and GP-TS become vacuous for ν . d2 , our bound from
Corollary 3 is always sublinear in T.
6 Experimental evaluation
We experimentally evaluate our algorithm via a simulation study. We use these experiments to
establish the dimension free nature of our results as well as to conjecture optimality of our bounds.
Setup. In the simulation study, we work with d dimensional RKHSs Hr andHπ. In order to simulate
the nonparmeteric regime, we typically use value of n which are less or at most a constant times the
dimension d. We set the matrices Sπ = diag(j-1.75), Sr = diag(j-1) and the map M = I. With
this, the effective decay parameter β = βπ - βr = 0.75. We further sampled the oracle noise
e 〜N(0,0.01). All plots were averaged over 10 runs.
Observations. Figure 1(a) shows the variation of excess risk as the number of queries n are varied
from 256 to 4096 on a log-log plot. Our bounds in Corollary 1 for this setup predict that the excess
risk should decay at a rate O(n-0.27). By fitting a linear line through the plot, we found that observed
risk to vary as O(n-0.28). This plot is suggestive of the fact that our theoretical upper bounds might
be tight in a minimax way over choices of decay parameter β . In Figure 1(b), we plot the excess
risk as we vary the dimension d from 32 to 8192 for four different choices of sample size, again,
on a log-log scale. Increasing the number of queries decreases the excess risk for all dimensions
consistently. The risk curves tend to asymptote at different error levels for different values ofn. This
corroborates our theoretical findings that our proposed algorithm provides non-vacuous bounds for
the doubly-nonparametric setup in the regime d → ∞.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Our main contribution is a theoretical framework to study reward learning and the associated optimal
design problem. Since our contributions are mostly theoretical in nature, we do not anticipate any
ethical issues arising in the near future.
Reproducibility S tatement
On the theoretical side, we provide detailed proofs for all our results in the appendix and appro-
priately reference the intermediate results we might have used in the proofs. For the experimental
aspect, we have attached our matlab code as a supplementary file and have provided all necessary
hyper parameters details required to reproduce the experiments.
10
Under review as a conference paper at ICLR 2022
References
Anthony C Atkinson. The usefulness of optimum experimental designs. Journal of the Royal
Statistical Society: Series B (Methodological), 58, 1996.
Florian Bohm, Yang Gao, Christian M Meyer, Ori Shapira, Ido Dagan, and Iryna GUrevych. Bet-
ter rewards yield better summaries: Learning to summarise without references. arXiv preprint
arXiv:1909.01214, 2019.
Sebastien Bubeck, Remi Munos, Gilles Stoltz, and Csaba Szepesvari. X-armed bandits. Journal of
Machine Learning Research, 12(5), 2011.
Xu Cai and Jonathan Scarlett. On lower bounds for standard and robust gaussian process bandit
optimization. In International Conference on Machine Learning. PMLR, 2021.
Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical
Science,pp. 273-304,1995.
Xiaohui Chen and Yun Yang. HanSon-wright inequality in hilbert spaces with application to k-
means clustering for non-euclidean data. Bernoulli, 27(1):586-614, 2021.
Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning, 2017.
Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. arXiv preprint arXiv:1706.03741, 2017.
Christian Daniel, Malte Viering, Jan Metz, Oliver Kroemer, and Jan Peters. Active reward learning.
In Robotics: Science and systems, volume 98, 2014.
LUC Devroye, Laszlo Gyorfi, and Gabor Lugosi. A probabilistic theory of pattern recognition, vol-
ume 31. Springer Science & Business Media, 2013.
Pedro Freire, Adam Gleave, Sam Toyer, and Stuart Russell. In Proceedings of the Workshop on
Deep Reinforcement Learning at NeurIPS, 2020.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,
volume 1. Springer series in statistics New York, 2001.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. In International Conference on Machine Learning, pp. 2232-
2241. PMLR, 2019.
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca Dragan. Inverse
reward design. arXiv preprint arXiv:1711.02827, 2017.
Thomas Hofmann, Bernhard Scholkopf, and Alexander J Smola. Kernel methods in machine learn-
ing. The annals of statistics, 36(3):1171-1220, 2008.
David Janz, David Burt, and Javier Gonzalez. Bandit optimisation of functions in the matern kernel
rkhs. In International Conference on Artificial Intelligence and Statistics, 2020.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.
Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sam-
ple covariance operators. Bernoulli, 23, 2017.
Erwin Kreyszig. Introductory functional analysis with applications, volume 1. wiley New York,
1978.
Markus Kuderer, Shilpa Gulati, and Wolfram Burgard. Learning driving styles for autonomous ve-
hicles from demonstration. In 2015 IEEE International Conference on Robotics and Automation
(ICRA), pp. 2641-2646. IEEE, 2015.
11
Under review as a conference paper at ICLR 2022
Tor Lattimore and Botao Hao. Bandit phase retrieval. arXiv preprint arXiv:2106.01660, 2021.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
J. Mercer. Functions of positive and negative type, and their connection with the theory of integral
equations. Philosophical Transactions of the Royal Society of London. Series A, Containing
Papers of a Mathematical or Physical Character, 209, 1909.
Jonas Mockus. Bayesian approach to global optimization: theory and applications, volume 37.
Springer Science & Business Media, 2012.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35, 2010.
Paola Sebastiani and Henry P Wynn. Maximum entropy sampling and optimal bayesian experi-
mental design. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 62,
2000.
John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis. Cambridge uni-
versity press, 2004.
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias W Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. In International Conference
on Machine Learning, 2010.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. arXiv preprint
arXiv:2009.01325, 2020.
Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in gaussian
process bandits. In International Conference on Artificial Intelligence and Statistics, pp. 82-90.
PMLR, 2021.
Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time
analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
Grace Wahba. Spline models for observational data. SIAM, 1990.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order opti-
mization in high dimensions. In International Conference on Artificial Intelligence and Statistics,
pp. 1356-1365. PMLR, 2018.
Christopher K Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
volume 2. MIT press Cambridge, MA, 2006.
Tong Zhang. Effective dimension and generalization of kernel learning. In NIPs, volume 4, pp.
454-461. Citeseer, 2002.
12
Under review as a conference paper at ICLR 2022
A Technical details for proposed framework
A.1 RKHS assumption
The Hilbert spaces Hπ and Hr are Reproducing Kernel Hilbert Spaces defined by kernel functions
Kπ, Kr : X × X 7→ [0, 1] respectively defined over a compact instance space X. Further, the kernels
Kπ and Kr satisfy the Hilbert-Schmidt condition
Ki(x, z)2dP(x)dP(z) ≤ ∞ for i = {π, r} ,
(12)
for some distribution P over space X. Mercer’s theorem (Mercer, 1909) implies that such kernel
functions have an associated set of eigenfunctions (with corresponding eigenvalues) that form an
orthonormal basis for L2(X, P). We restate a version of this theorem below (Wainwright, 2019).
Theorem 4 (Mercer’s theorem). Suppose that the space X is compact and the positive semi-definite
kernel K satisfies the Hilbert-Schmidt condition (12). Then there exists a sequence of eigenfunctions
(φj )∞=ι that form an orthonormal basis of L2 (X, P) and non-negative eigenvalues (μj )∞=ι such
that
X
X
K(x,z)φj (z)dP(z) = μj φj (x)
for all j = 1, 2, .
Furthermore, the kernel function has the expansion
∞
K(X,Z)= fμj φj(X)φj(Z),
j=1
where the convergence of the sequence holds absolutely and uniformly.
(13)
(14)
A.2 Conditions for reward boundedness
For learning to be feasible in the proposed framework, we would require that the evaluation func-
tional F(π, r*) is bounded for any policy π ∈ Hn. Using the fact that ∣∣r* ∣∣Hr ≤ 1 and kπkHπ ≤ 1,
we have
T	1	—1
F(∏,r*) = hr*,MniHr = (r*)>SrMn ≤ kSr2MS-2kop .	(15)
Thus one sufficient condition for the reward functional to be bounded is to ensure that the operator
1	- 1
norm ∣∣Sr2 MSn 2 ∣∣op is finite. In the special case when the map is diagonal with M = diag(ν7-), the
above condition simplifies to
F(∏,r*) ≤ sup	V：nj	.	(16)
j≥1 [ μr"
A.3 REGULARITY ASSUMPTIONS ON MAP M
We assume that the map M is a compact bounded operator from the policy space Hn to the reward
space Hr . By Schauder’s theorem, the adjoint M* is also a compact operator. Thus, the map
M*M : Hn → Hn is a compact self-adjoint operator. This allows us to use the spectral theorem
for compact self-adjoint operators which guarantees the existence of eignevalues and eignefunctions
for the operator M*M and a corresponding singular value decomposition for the map M (Kreyszig,
1978).
A.4 Non-aligned RKHSs
As mentioned in the Section 2, if the eigenvectors of the spaces Hr and Hn are not aligned, one
can consider the following simple transformation which resolves this. Let Φn and Φr represent the
eigenvectors.
r = Φrr, Π = Φ>n, and M = Φ>MΦ∏ .	(17)
The above transformation implies that ∣r∣Hr ≤ 1 and IInkHn ≤ 1.
13
Under review as a conference paper at ICLR 2022
B Proof of main results
In this section we provide the proofs for the main results of this work. Appendix D to follow contains
the proofs for the other results.
B.1 Proof of Theorem 1
We begin by proving the result for the special case when the policy set Cπ consists of the entire unit
ball and then generalize the analysis to arbitrary policy sets.
Case 1:	Cn is unit ball in Hn. For this special case, observe that the the optimal policy ∏ and
the plug-in policy ∏piug for any reward estimate r can be written as
a
π
M *r*	…二
Il “* *∣∣一	and πplug
IIM r IIh∏
M *r
W7r‰
(18)
where the operator M* is the adjoint of of the map M. To prove a bound on the excess risk using
the plug-in estimate, we use the following lemma which bounds this error in terms a deviation of the
estimated and true rewards.
Lemma 1. Consider any vectors X and y with finite non-zero norm under some inner product(•, •).
Then, we have
X__y_\ < ∣χ — yk2
画-画i- FF
(19)
The proof of the above lemma is presented in Section B.1.1. Taking the above as given, we can
upper bound the excess risk
M *r*	M * r
/r*) = hM *r*, Et -E:加
≤ ∣M*(r*- r)监
-2∣M *刃血	.
(20)
Case 2:	Arbitrary set Cn. For this case, consider the excess risk of plug-in estimator ∏piug ob-
tained by maximizing reward estimate r
、g r*) = hM*r*,π* - πplugiH∏
=hM*(r* — r), ∏*>H. + hM*r, ∏* -分械加 + (M*(r - r*),郎观加
(i)
≤ 2∣M*(r* — r)∣H∏,	(21)
where the final inequality follows from the fact that ∏piug maximizes F(∏; r) over the set C∏.
Thus, we see that for both the cases above, we can upper bound the excess risk of the plug-in
estimator in terms of the norm ∣∣M*(r* 一 r)∣Hπ. Next, we evaluate this for the ridge regression
based reward estimator for any set of n queries Q = {π1, . . . , πn} with covariance matrix Σ =
§ Pi ∏i∏>. For any regularization parameter λreg > 0, we have,
1n
r = arg min — y',-h — hr, MniiHr )2 + λregkr∣Hr
r∈Hr n	r	r
i=1
(i)	1 n
=(M∑M>Sr + λregI)-1 ∙ - X yiM∏i
n i=1
Mn
=r* — λreg(M ΣM >Sr + λregI )-1r* + (M ΣM >Sr + λregI )-1 -- E Gni ,	(22)
14
Under review as a conference paper at ICLR 2022
where and equality (i) follows by substituting the value of y% = F(∏i, r*) + /. Let Us denote by
matrix A = MΣM>Sr + λregI. Therefore, the error in reward estimation
r - r* = λregA-1r* + A-1 yn X Gni)
~ N λλregA-1r*, τ2ATMΣM>A->
n
(23)
where the final distribution follows from our assumption on the noise variables Ei 〜N(0,τ2).
Using this above distributional form, we have
τ2
E[kM*(r* - r)kHj = λ2eg ∙ hM*A-1r*, M*A-1r*>Hπ + 一 ∙ tr [SπM*A-1MΣnM>A->(M*)>]
=λ2eg ∙ tr [(r*)>A->(M*)>SπM*A-1r*] + τ2 ∙ tr [SπM*A-1MΣM>A->(M*)>].
n	(24)
The final bound for the general policy set Cπ follows from using the above bound with a an ap-
plication of Jensen’s inequality. In order to convert the above bound to a high probability bound,
we require an infinite dimensional analog of the Hanson-Wright concentration inequality. Using
Theorem 2.6 from Chen & Yang (2021) along with equation (23), we obtain
Pr(∆(π;r*) ≥ E[∆(π;r*)] + t) ≤ 2exp
where the covariance matrix Γ = SnM*A-1MΣM>A->(M*)>S∏2.	口
B.1.1 Proof of Lemma 1
Let the vector y = x + δx for some difference vector δx . Using this, we have
X y ∖ _ ! X X + δχ)
画一庙= hx,画-FWk
=kχl÷δχk (kx + δχk-kxk-
≤ kχ∣	(kxk + 3 + m
≤ kx + δχk C k+ kxk +2kxk
=	δ2
2∣∣x + δχ∣ ,
(25)
where (i) follows from using the inequality √α2 + z ≤ a + 2a. This establishes the result. 口
B.2 Proof of Proposition 1
Let us denote the the map M = diag(νj) and the covariance matrix Σ = diag(σj). From the upper
bound obtained in Theorem 1, we have,
E[kM*(r* - r)kH∏] = λ2eg ∙kM*A-1 门IHn
+ n ∙ X kM*A-1M∏ikH
n	i=1
(i)	1	_1	T 2
≤ Reg ∙ ∣∣S∏fM*A-1S-2 kθp + — ∙ tr [SπM*A-1MΣM>A->(M*)>]
(ii)
≤ λ2eg ∙ SUp
j≥1
2
Vj μr,jμπ,j
ν4σ2 + λ2eg 〃r,j
+ — ∙ sup
n j ≥1
ν4μ∏,j
ν4σ2 + λ2eg 〃r,j
(26)
where inequality (i) follows from using the fact that lr* lHr ≤ 1 and inequality (ii) uses the diagonal
structure of the map M as well as the fact that each policy πi ∈ Q has unit Hπ -norm.
15
Under review as a conference paper at ICLR 2022
Recall that the choice of querying strategy queries each scaled eigenfunction √μ∏j φ∏j of the policy
space n1-α times. Therefore the jth entry of the covariance matrix Σ is given by
σ = ∫μπɑj	for j ≤ nɑ
j 0 otherwise
(27)
Plugging the above value of σj into equation (26), we obtain
2	λr2eg n2α ζj
E[∣∣M *(r* - r)kHπ] ≤ max [≤P ζ2+λ2egn2α, Za q
+
τ2
——∙ max
n
suP
j≤nα
(28)
This concludes the proof of the proposition.
□
B.3 Proof of Corollary 1
We now derive explicit finial sample rates for the case when the spectrum of the map M>SrMSπ-1
satisfies a power law decay for some parameter β > 0. In the notation used in Proposition 1, we
have the quantity
ζj	j -β .	(29)
Our proof strategy will be to instantiate the bias and variance terms for this setting of ζj and finally
select a setting for the exploration parameter α and regularization parameter λreg.
Bounding Bias. The bias term in the proposition is a max over two terms
2	λr2egn2αζj
Bias = max SUP 2 λ2 rι2α , SUP Z
j≤nα ζj +λregn	j>nα
We consider the two terms in the analysis here separately. For the first term,
λr2eg n2α ζj	2	1	α
SUP ζ2 + λ2 n2α = λreg SUP j-β	∖2^^β ≤ λregn ,
j≤na Zj + λregn	j≤na na+ + λ2egj β _
where the final inequality follows from using a2 +b2 ≥ 2ab. For the second term, we have
SUP ζj = SUP j -β = n-αβ.
j≥nα	j≥nα
Bounding Variance. Recall that the variance term (assuming τ = 1) is given by
Variance
1
—∙ max
n
SUP
j≤nα
We again consider both terms of the maximum separately. For the first term,
1
—∙ SUP
n j ≤nα
≤ n2α-1
where the inequality follows from ignoring the term λr2egn2α
variance term,
in the denominator. For the second
ζj	n-2ɑβ
j>nα nλ2eg	λ2egn
(30)
(31)
(32)
(33)
(34)
(35)
Setting regularization parameter. By setting λreg > n-αβ-α, we can have that the bias term is
dominated by λregnα . Similarly, the above setting also implies that the variance term is dominated
by n2α-1. Combing these observations, we have that the expected error is upper bounded by
∆(∏piug; r*) ≤ λregnα + n2α-1 where λreg > n-αβ-α.	(36)
Setting λreg = n-ɑ(β+1) and then α = e+^, We get that
β
∆(∏plug；r*) ≤ n-β+2.	(37)
This completes the proof of the corollary.	口
16
Under review as a conference paper at ICLR 2022
B.4 Proof of Theorem 2
In order to prove the general theorem, we exhibit a transformation which allows us to reduce the
problem to that with the diagonal structure described in Proposition 1.
We will consider orthogonally diagonalizable matrices Sr and Sπ which represent the eigenvectors
and eigenvalues of the Hilbert spaces Hr and Hπ . Consider the following set of transformations for
any reward r ∈ Cr and policy π ∈ Cπ .
1	1	~	1	_1
r = Sr r,	∏ = Sn π,	M = Sr2 MSn2.	(38)
With this transformation, we can rewrite the objective function above
. .
max hr, M ∏ s.t. hπ, ∏ = 1 and hr, r)= 1 ,
π
where the inner product(•，•) denotes the standard '2 inner product. Observe that We have overloaded
notation to denote by r* = r. Further, using these above transformations, we can rewrite the adjoint
operator
∙1 T	_1	1	_1 T 1	_1	~ T 1
M* = S-1M>Sr = S-2 (SrMS-2)>Sr2 = S-2M>Sr2 .	(39)
Recall from Theorem 1, the matrix
A = MΣM>Sr + λregI = S-2 [MΣM> + λregIi Sr2 ,	(40)
where the covariance matrix Σ = * Pi ∏∏>. We have used the fact here that the matrices S∏ and
Sr are orthogonally diagonalizable and hence symmetric. Finally, we will denote the singular value
decomposition of the compact map M in the matrix form as
M = UM AM VM .
The existence of such a decomposition is guaranteed by the regularity assumptions we consider on
the map M in Appendix A. We will now analyze the bias and the variance terms from the upper
bound on E[∣∣M*(r* 一 丹扁」from Theorem 1.
Bound on bias. The squared bias term is given by
λ-g∙ Bias2 = r>A->(M*)>S∏M*A-1r
11	1	1	1	1
=r>Sr2 S-2 ∙ Sr (MΣM> + λregI)-1S-2 ∙ Sr MS-2 ∙ Sπ ∙ M*A-1r
1 1	1	1	1	1	1 1
=r>(MΣM> + λregI)-1M ∙ Sn Sπ 2 M>Sr2 ∙ Sr 2 (MΣM> + λregI)-1 Sr r
=r>(MΣM> + λregI)-1M ∙ M>(MΣM> + λregI)-1r
=r>UM(AMvMςVM1AM + λregI)-1AM(AMVMςVM1AM + 21)-1UMr , (41)
where we have used the SVD decomposition for the matrix M in the last step.
Bound on variance. The variance term is given by
Var = τ2 ∙ tr [SnM*A-1 MΣnM>A->(M*)>]
τ2
=—∙ tr [m>(MΣM> + λregI)-1MΣM>(MΣM> + 入0I)-1Mf]
=τn2 ∙ tr hAM(AMVMςvMAM + λregI)-1AMVMςVMAM(AMVMςVMAM + MI)-1 AM]
(42)
Finally, by making a substitution for reward r = UM r and policy π = V> π in equations (41)
and (42), we recover back the bias variance expressions used in the analysis for Proposition 1. What
17
Under review as a conference paper at ICLR 2022
remains to be shown is that our particular choice of query policies correspond to basis vectors in this
transformed space. For this, observe that the sampling policies
∞
Πj = E√μ∏i ∙ hΦM,j,φπ,iiφπ,i for j ≤ nα ,
i=1
is such that the transformed policies
∏j = VM> S∏1 ∏j = Vm> S工 S- 2 Vmej = ej ,	(43)
indeed correspond to the basis vector. This finishes the proof of the desired claim.	□
B.5 Proof of Corollary 2
The proof of this corollary follows similar to that of Corollary 1 in terms of bounding the bias and
the variance. The final rate follows by an application of Jensen’s inequality to conclude
E[kM*(r* — 打血.]≤ (E[kM*(r* — r)kH∏])2 ∙	(44)
The final rate that we get in this case is thus upper bounded by the square root of the rate observed
in Corollary 1. This concludes the proof.	□
C Gaus sian process bandit optimization
In this section, we discuss in detail the application of our framework to the problem of frequen-
tist Gaussian process bandit optimization, also known as Kernelized multi-armed bandits (MAB)
problem. Recall the reduction of the Kernel MAB problem to our setup required us to define three
elements.
Reward space Hr. Given the RKHS H as well as the elements of the cover C , we view the
reward function as a map from C to R, or equivalently as a vector in RNcov() . More precisely,
letting f = [f (x1), . . . , f (xNcov)] denote the vector of evaluations ofa function f, we define
__ , ~ . . _________
Hr : = span{f | f ∈ H}
,~	~ .	~~Γ	-I ~
with hfι, f2)Hr : = f>K-1f2
(45)
where〈•，•〉represents the standard '2 inner product. With this notation, let US define the true reward
r* := f* = [f*(XI),…，f*(XNcov)].
Policy Space Hπ. For the policy space Hπ in our setup, we let
Hπ : = span{kx = [K(X,X1),...,K(X,XNcov)] ∈ RNcov | X∈C}
with hk1,k2iHπ := hk1,K-2k2i .
(46)
The choice of the above norm ensures that
hki,kjiHπ = hK-1ki,K-1kji = hei,eji = δi,j for all	(Xi,Xj) ∈C ×C .
For the policy space Hπ, we have created an orthonormal embedding of the set of vectors {kx}x∈C .
Observe that this policy set that we construct satisfies the regularity Assumption 1 because each
vector k is an eigenvector of the space Hπ .
Map M. By our assumption that the kernel K is a Mercer’s kernel, we have that Hπ ⊆ Hr , that
is, for all X ∈ C, the vector kx ∈ Hr. Furthermore, both Hr and Hπ are sub-spaces of RNcov and we
can take the map M = INcov .
With these definitions, we now explicitly establish a correspondence between our doubly nonparam-
eteric bandit problem and the Kernel MAB problem.
18
Under review as a conference paper at ICLR 2022
C.1 Connecting the problems
Given an RKHS H with an associated Mercer’s kernel K, the objective of the zeroth-order bandit
optimization problem is
max f (x) such that ∣∣f*血 ≤ 1 ,	(P1)
x∈X
with access to oracle
Of * : x - f *(x) + η where η 〜N (0, τ2).
Equivalently, the objective in our reward learning framework is
max hr*,∏i≡r such that ||门|咻 ≤ 1 and IInllHn ≤ 1 ,	(P2)
π∈Hπ	r	r	π
with the corresponding spaces and inner products are defined in the previous section. The oracle
required in our setup responds with
Or* : π → (r*, n〉Hr + η where η 〜N(0, τ2),
for any policy π ∈ Hπ such that∣π∣Hπ ≤ 1. Our first lemma below states that obtaining such a n
oracle is indeed feasible if we are able to restrict our queries π to include only points kx for which
the vector kx ∈ C .
Lemma 2. Given access to oracle Of* for a function f *, the corresponding oracle Or* can be
implemented when the query set consists of{kx}x∈C.
Proof. For any query point k, the oracle Or* needs to compute the value hr*, kiHr = f* (x). Thus,
these two oracles on the provided query set are exactly identical.	□
Lemma 3. For any f* ∈ H satisfying ∣f* ∣H ≤ 1, we have that ∣r* ∣Hr ≤ 1.
Proof. Observe that an alternate way to define the RKHS norm is given by
∣f∣H : = sup	f|SKS-1f|S .
S⊆X 1S∣≤∞
The fact that ∣r* ∣Hr is computed on Ce ⊂ X establishes the desired claim.	□
Finally, we turn to establishing a relation between the solutions obtained from solving the relaxed
problem (P2) as compared to solving the original problem (P1). We denote the corresponding max-
imizers for both problems
x* ∈ arg max f* (x) and x*π ∈ arg maxhr*, kxiHr ,	(47)
x∈X	x∈C
The following lemma now relates both these maximizers together.
Lemma 4. For an RKHS H with kernel K satisfying Assumption 2 with constant LK > 0 and any
function f * ∈ H, let x* ∈ X and x*π ∈ Ce be the maximizers as defined in equation (47), we have
f *(x∏) ≥ f *(x*) - ,2CLKe .	(48)
Proof. Denote by ΠC (x*) : = arg minx∈C ∣x* - x∣2 the projection of the point x* onto the set
Ce . Then, we have
f*(χ*) - f*(x∏ )= f *(x*) - f*(Πq(x*)) + f*(∏Ce(x*)) - f*(x∏ )
≤ a∕2cLk e .
This completes the proof of the lemma.	□
The above lemma shows that solving Problem P2 is equivalent to solving Problem P1 up to an
additive factor of √2cLκe when we are working with an e-cover over the domain space.
19
Under review as a conference paper at ICLR 2022
C.2 Analysis for bandit optimization
Recall from the previous section that the quantity which determines the rate of decay is the ratio of
eigenvalues
2
广—μ∏,j — μr,j — A
Zj = K =诟="r,j,
where 4“• is the jth eigenvalue of the kernel matrix K. Let Us denote by P denote the uniform
distribution over the input space X and let us suppose that the cover Ncov is formed using random
samples from this distribution. Let Us denote by {μj } the eigenvalues and by φj the corresponding
eigen vectors of the Mercer kernel K. For every point x ∈ X, let us denote by
φ(X) := (√μjφj(X))∞=1 ,
the corresponding featurization of the point x. Then, for S : = Ex〜p[Φ(x)Φ(x)>], We have
[S]j,k = [Ex~P[φ(X)φ(X) ]]j,k = Ex 〜P [√ μj √ μk φj (x)φk (X)] = μj δj,k .	(49)
Observe that the kernel matrix K and the (scaled) sample covariance matrix Ncov ∙ S =
x∈C Φ(X)Φ(X)> are similar matrices and thus have the same eigenvalues. The following lemma,
adapted from Koltchinskii & Lounici (2017, Theorem 9) relates the eigenvalues of the sample co-
variance matrix S to those of the underlying kernel K.
Lemma 5. For any λS > 0 andsizeofthe Coversatisfying Ncov(E) > c∙ HS(S[jSI))+* log (1),
we have,
μj ≤ (1 + ES)μj + λsES forall j ,	(50)
with probability at least 1 - δ.
The following corollary of Lemma 5 provides us with a way to control the deviation of the eigen-
values μj from the corresponding μj in a multiplicative manner.
Corollary 4. For any value of decay parameter β > 1 andγ < β, we have, for all j, the eigenvalues
3	N-γ
μj ≤ 2 〃j + N2~,	(51)
with high probability.
Proof. Let us understand the condition Ncov(E)》"(,(,,S I) and See what restrictions it puts
on the value of the covering number. Lets suppose that the true eigen values μj N j-β and we set
the value of λS Nc-ovγ . Therefore, the sum
X 厂.Ncβv + Nb EjT
j>N烹
Y
.Ncβv+/
Thus, if we set ES = 2, then for any β > 1 and γ < β, the above condition on the covering
number will be satisfied and we get desired bound on the deviation of the empirical eigenvalues
from population eigenvalues.	□
The above corollary is essential to our argument because often times we have a good understanding
of the decay of the eigenvalues of the kernel K associated with the RKHS and this allows us to relate
the set of empirical eigenvalues to these.
We now present a proof of Theorem 3, restated below, which upper bounds the excess risk for this
setup. We will then use a batch to online conversion bound to convert this to a regret bound and
specialize to the Matern kernel later.
20
Under review as a conference paper at ICLR 2022
Theorem 5 (Restated Theorem 3). Suppose that the eigenvalues of a LK -Lipschitz kernel K with
respect to a distribution P over X satisfy the power-law decay μj N j-β. Let XPlUg be the output of
Algorithm 1 using n queries to the oracle Of*. Then, for any value of Y ∈ (1 + d ]ogg(1/?2),β) and
∈ (0, 1), the excess risk
1—β	1-Y
max f *(X)- f * (Xplug) . NcO+2 ⑹∙ n2(β+2) + N∞T ㈤ + VLK ,
x
with high probability.
Proof. Our strategy, as before, will be to explore nα directions and assume τ2 = 1. Recall, that for
symmetric matrices, Theorem 2, the excess error of the plug-in estimator can be upper bounded as
1	1	ν4μ2 -
EA(nplug； r*)]2 ≤ λ2eg sup	-----λ27Γ~ + - sup 4 2 j Πj 2	∙
g j≥1 V σ + λ速Nr,2	n j≥l ν4σ2 + λ2egμr,j
μ∏,j μr,j	μ∏,j Vj
Bounding Bias. We will split the analysis into two cases.
Case 1:	j > ηα. For this case, We have that σ7- = 0 and therefore
λ2eg sup	J"?	= sup	NcovRj	. SuP	Ncov(〃j	+	N-Y)	≤	Ncovn-0β +	NcO-Y	,	(52)
j>nα λregμr,j	j>nα	j >nα
With the above holding With high probability from an application of Corollary 4 for any 1 < γ < β .
Case 2:	j ≤ nɑ. For this case, we have σ? = μπj. The bias can then be upper bounded as
sup
j ≤nα
ν2μ∏,j
n2α μr,j
1
+ λ2egμr,j
μ∏,jνj
≤ λregnα
(53)
where the final inequality follows from using a2 + b2 ≥ 2ab.
Bounding variance. As we did in the section above, let us split the analysis into two cases.
Case 1:	j > nα. For this case, the variance term simplifies to
1	μnj
-sup 、2	2
n j>nα XegMjj
1	N2
T2  sup [Ncovμj] ≤ ^2  sup
λregn j>nα	λregn j>nα
Ncov n-2αβ + N2Vl-γ)
λ2egn	∙
(54)
Case 2:	j ≤ nɑ. For the second case, we can upper bound the variance term
1
—sup
n j ≤nα
Vj μ∏j	≤ n2
¾j + λ2egμj - n
(55)
where the last inequality follows from ignoring the second term in the denominator.
Setting regularization parameter. From the analysis in the above paragraphs, we have
Bias2 ≤ max{Ncovn-αβ + Nc1o-v Y, λregnα } ≤ max{Ncovn-αβ , λregnα} + Nc1o-vY ,
Variance
≤ max{
Nov n-2αβ + No(T-Y
λ2egn
,n2α }
n
Nc2ovn-2αβ n2α
≤ max{	，—} +
NcovL-Y)
λ2egn
(56)
(57)
For regularization parameter λreg > NcOVn-αβ-α and Y › logαNoιV, we have
Bias2 ≤ λregnα + NcLo-vY ,
Variance ≤ n—.
n
21
Under review as a conference paper at ICLR 2022
Excess risk bound.
To obtain the final excess risk bound, we set α
1 + lθgn NcOv
β+2
n2α
EA(nplug; r*)]2 ≤ λregnα + ~n~ + NcovY
≤ NcOvn-αβ + n2α-1 + Nc1O-v γ
⑴ -ɪ -β
.NcO+2 n + + Nco-γ ,
(58)
where inequality (i) follows from our particular choice of α. Combining the above bound with
Lemma 4 completes the proof.	□
The following corollary instantiates the above theorem for the case when the input space is the unit
ball, that is, X = Bd(1).
Corollary 5. Let the input space X = Bd(1) and the kernel K satisfy Assumption 2. Then, for any
β > 1 + d, we have
__d 	-β
mχax f *(x) — Ex〜∏piugf *(x) . LK+2+2 n2(β+2+2d) .	(59)
Proof. From the bound in Theorem 3, we have,
—1—	—β	1-γ	,---
max f *(X)- Ex〜而Ugf *(x) . NcO+2 (e) ∙ n2β+) + Ncor (e) + √LK^
⑴	/	e2	—β	1 —γ f2
≤ Ncov2 (γ-) ∙ n 2(β+2) + Nco2 (y-) + f
LK	LK
2d
1、β+2	—β
—	∙ n2(B+2) +
LK
d(1-γ)
2
+
(iii) d
≤	LK+ •
2d
1 ∖ β+2
-B
.n2w2y + 2e ,
(60)
Where inequality (i) folloWs from substituting →	2/LK, (ii) folloWs
NCov(O X (I)d, and (iii) follows from using the assumption that β > γ > 1 +
from the fact that
2 log(1∕e)
d log(Lκ∕e2).
Finally, setting f X LK+2+2d
n 2(β+2+2d), We get
mxax f *(X)- Ex〜∏piugf *(x) . LK++2
This establishes the desired claim.
—β
n 2(β + 2 + 2d).
C.3 Regret bound for MATERN Kernel
In this section, we specialize the bound from Theorem 3 for the special class of Matern kernels.
Recall that the Matern kernel is a distanced based kernel with K(X, y) = f(kX - yk). Denote by
r = kX - yk, the exact form for the kernel is given by
KMatern,ν (r)
(61)

d
d
□
with parameters ν and l and where Kν is the modified Bessel function of the second kind. Going
forward, lets fix the scale parameter l = 1 without loss of generality.
The following lemma then bounds the Lipschitz constant for this class of kernels when the distance
function is the `2 norm.
Lemma 6 (Lipschitz Matem Kernel). Consider the Matern kernel with parameter ν > 3. The
Lipschitz constant of this kernel is bounded by
LK ≤
sup [
r∈(0,1)
e22-ν νKν-i(1)
Γ(V)
• re-√2νr ]
(62)
22
Under review as a conference paper at ICLR 2022
Proof. The approach will be to show that the kernel function KMatern,ν is a Lipschitz function of the
distance r and then cover the `2 ball in the d dimensional space appropriately. We now look at the
derivative of the function KMatern,ν (r) with respect to r.
∂KMatern,ν (r)
21-ν (√2V )ν
^^(V)
VrV-IKV (√2νr∂r + rν ∂Kν (√2νr
(i) 21-ν(√2V)ν
= ^(V)
VrV-IKV(√2νr) — rν (√2VKν-ι(√Vr) + ^y√^KV(√2Vr))) ∂r
—21-;y2ν)” (rV√2VKV-i(√2Vr)) ∂r,
Γ(v)
(63)
where (i) follows from the identity ∂K(Z) = (-Kv-i(Z) — VKV(z))∂z.
For any ν > ɪ, we have the inequality
KV (x) < expy-x (y)	for 0 < x <y.	(64)
KV (y)	x
Instantiating the above with y = 1 and ν > ∣, we have
∣∂KMatern,V 6l≤ 2^ 卜 后 ∙ ∖ ^ ,^ ⑴
≤ e2"VKV-I ⑴	√2Vr
≤-Γ(V)	re .
(65)
The LiPschitz constant for this case can now be obtained by taking a SUP over r ∈ (0,1). 口
While our upper bound was in terms of sample complexity, in order to compete with the cumulative
regret formulation, we adaPt an exPlore-then-commit strategy. The following lemma relates the
samPle comPlexity bound to a cumulative regret bound.
Lemma 7 (Batch to online conversion). Suppose an algorithm has sample complexity O(n-α)) in
the passive learning setup, the explore then commit strategy based on this learning algorithm would
have regret O(T 1+α).
Proof. For some Parameter γ > 0, let the exPlore then commit algorithm exPlore for Tγ stePs and
the commit to the strategy obtained Post this exPloration for the remaining T — Tγ time stePs. The
cumulative regret for such an algorithm is
RT = TY + T-αγ(T — TY) ≤ TY + T1-αγ
(66)
Setting Y = j+1α finishes the proof.
□
We now Proceed to Prove Corollary 3 which instantiates the bound in Theorem 3 for the class of
Matern kernels.
Corollary 6 (Restated Corollary 3). Consider the family of Matern kernels with parameter ν > ∣
defined with the euclidean norm over Rd. The T -step regret of the explore-then-commit algorithm is
d2
RMat,T . O LKV+d(3+2d)
4ν + d(6+4d)
• T 6ν + d(7+4d)
with high probability.
Proof. First, observe that excess risk bound in Corollary 5 can be converted to a corresPonding
T -steP regret bound by an aPPlication of Lemma 7 such that
d
Rt . O LK++d
2β + 4+4d
• T 3β + 4+4d
(67)
23
Under review as a conference paper at ICLR 2022
For the class of Matern kernels, the decay parameter β = 1 + 当(Janz et al., 2020, Theorem 9).
Using this wit the above regret bound, we get,
d C),意4小	4ν+d(6+4d) ʌ
RMat,T . O LKV+d(3+2d) ∙ T 6ν+d(7+4d).
This completes the proof.
□
D Adaptive sampling via GP-UCB
In this section, we prove an upper bound on the expected risk of the Gaussian process upper con-
fidence bound algorithm (GP-UCB) algorithm of Srinivas et al. (2010). In order to adapt their
algorithm for our setup, consider the function
fr(x) : = hr, M xiHr such that D = {x | kxkHπ ≤ 1}.	(68)
We have used x to denote policies in this setup to be consistent with the notation in Srinivas et al.
(2010). Observe that the domain defined above is not compact - a necessary condition for the algo-
rithm to work. One work around this is to truncate the unit ball after a finite number of dimensions
and bound this truncation error. The excess risk incurred by this truncation can be made arbitrary
small. Going forward, We ignore this truncation. The regret for the UCB algorithm is shown to be
upper bounded by O(YT √T) where YT is the information gain with
YT :=	max Ilogdet(I + [K(xi,xj)]Tj=ι) ,	(69)
x1,...,xT ∈D 2	,
where we have assumed without loss of generality that the noise variance τ = 1. For our setup,
the kernel function K(xi, xj ) = hMxi, Mxj iHr . We additionally require that the reward function
I-T	1
r belongs to the RKHS spanned by the set {Mx | X ∈ D}. Denote by S = Sn M>S-1S∏ and
suppose that its eigenvalues satisfy a power law decay with σj (S) = ζj = j-β. The following
lemma upper bounds the information gain for this setup in terms of the power law parameter β > 0.
Lemma 8 (Information Gain.). The information gain YT for the above setup is bounded as
YT = O(log(T) ∙ Tβ+1).
(70)
Proof. The quantity of interest here is the information gain
Yt : = max Ulogdet(I + XSX >) such that ∀j ∣∣Xj∣∣2 ≤ 1 ,	(71)
x1,...,xT 2
where the matrix X = [x1> ; . . . ; xT] and we have assumed that the noise variance is 1. From the
setup described above, we have that the eigen values of S decay as λj j-β . It is easy to see that
Fig({xt}) ：= 2logdet(I + XSX>)	(72)
is a monotonic sub-modular function. Thus, the value of YT can be upper bounded by (1 - 1/e)-1
times the value of the greedy maximization algorithm. The greedy maximization algorithm is equiv-
alent to picking
xt = arg max Fig(Xt-1 ∪ {x}) .
It is easy to see that at each time t, the unit vector xt will be an eigen vector of the matrix S. Given
this observation, we can finally upper bound the value of the info gain
T
Yt ≤ C ∙ max log(1 + mj λj) such that mj ≥ 0 and	mj = T.
m1,...,mT
j=1	j
Solving the above optimization problem, the optimal choice of the variables
mj = max { — — ɪ, 0} and ^X mj = T .
(73)
24
Under review as a conference paper at ICLR 2022
β	1
Setting λ = T β+1 ensures that there are T β+1 active directions. Substituting the above values of
mj in the expression for γT, we get
∞λ
YT ≤ C ∙ X log(1 +max(-j - 1, 0))
λ
j=1
λ∞
≤ C ∙log (jr) ∙ Σ I[λj > λ]
j=1
=O(log(T) ∙ T券),
β
where (i) follows from setting λ = T-β+1. This establishes the required claim.	口
We are now ready to state this our sample complexity bound for GP-UCB for this subclass of prob-
lems.
Proposition 2 (Sample complexity for GP-UCB). Suppose that the police space Hπ, reward space
Hr and the map M satisfy the power law decay assumption with exponent β > 0. The estimator
∏ucb output by the GP-UCB algorithm satisfies
β-1
E[∆(∏ucb; r*)] ≤ O(n- 2(β+1)) .	(74)
The proof of the sample complexity bound in Proposition 2 now follows the regret bound of
O(YT √T) along with using the upper bound on the information gain from Lemma 8.
1	1	β-1
E[∆(∏piug； r*)] = O(nβ+1 -2) = O(n-2≡∏) .	(75)
More recently, Cai & Scarlett (2021) extended the analysis of Valko et al. (2013) to show that the
SupKernelUCB algorithm achieves a regret bound O(√γτT). Using this modified bound, one can
improve the above analysis to obtain excess risk
11	β
E[∆(∏piug; r*)] = O(n2(β+τ) 2) = O(n 2(β+τ)) ,	(76)
which is still worse than those obtained by the bounds by our proposed ridge regression estimator.
E Further details on experimental evaluation
In the simulation study, we work with d dimensional RKHSs Hr and Hπ . In order to simulate the
nonparmeteric regime, we typically use value of n which are less or at most a constant times the
dimension d. We set the matrices Sπ = diag(j-1.75), Sr = diag(j-1) and the map M = I. This
is allowed since the policy space is smaller than the reward space. With this, the effective decay
parameter β = βπ - βr = 0.75. We sampled the true reward r* uniformly at random from the unit
ball in Hr. We further sampled the oracle noise E 〜N(0,0.01). All plots were averaged over 10
runs.
25