Under review as a conference paper at ICLR 2022
Greedy-based Value Representation for Effi-
cient Coordination in Multi-agent Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Due to the representation limitation of the joint Q value function, multi-agent reinforcement
learning (MARL) methods with linear or monotonic value decomposition suffer from the
relative overgeneralization. As a result, they can not ensure the optimal coordination,
leading to instability and poor performance. Existing methods address the relative overgen-
eralization by achieving complete expressiveness or learning a bias, which are insufficient
to solve the problem. In this paper, we propose the optimal consistency, a criterion to
evaluate the optimality of coordination. To achieve the optimal consistency, we introduce
the True-Global-Max (TGM) condition for linear and monotonic value decomposition,
where the TGM condition can be ensured when the optimal stable point is the unique stable
point. Therefore, we propose the greedy-based value representation (GVR) to ensure the
optimal stable point via inferior target shaping and eliminate the non-optimal stable points
via superior experience replay. We conduct experiments on various benchmarks, where
GVR significantly outperforms state-of-the-art baselines. Experiment results demonstrate
that our method can ensure the optimal consistency under sufficient exploration.
1	Introduction
By taking advantage of the deep learning technique, cooperative multi-agent reinforcement learning (MARL)
shows great scalability and excellent performance on challenging tasks (Vorotnikov et al., 2018; Wu et al.,
2020) such as StarCraft unit micromanagement (Foerster et al., 2018). As an efficient paradigm of cooperative
MARL, centralized training with decentralized execution (CTDE) (Oliehoek et al., 2008; Foerster et al., 2016;
Lowe et al., 2017) gains growing attention. A simple and effective approach to adopt CTDE in value-based
cooperative MARL is linear value decomposition (LVD) or monotonic value decomposition (MVD). However,
both LVD and MVD suffer from relative overgeneralization (Panait et al., 2006; Wei et al., 2018) due to
the representation limitation of the joint Q value function. As a result, they can not guarantee optimal
coordination.
Recent works address the problem from two different perspectives. The first kind of method aims to solve
the representation limitation directly through value functions with complete expressiveness capacity (e.g.,
QTRAN (Son et al., 2019) and QPLEX (Wang et al., 2020)). However, learning the complete expressiveness
is impractical in complicated MARL tasks because the joint action space increases exponentially with the
number of agents. The other kind of method tries to overcome relative overgeneralization by learning a
bias (e.g., WQMIX (Rashid et al., 2020) and MAVEN (Mahajan et al., 2019)), which lacks theoretical and
quantitative analysis of the problem and is only applicable in specific tasks. As a result, these methods
are insufficient to guarantee optimal coordination. More discussions about related works are provided in
Appendix A.
1
Under review as a conference paper at ICLR 2022
Value decomposition is a popular approach to assign credit for individual agents in fully cooperative MARL
tasks, where the main concern is the optimality of coordination. In a successful case of credit assignment
via value decomposition, individual agents act according to their local policies and achieve the best team’s
performance. To evaluate the value decomposition, we propose the optimal consistency, a criterion con-
cerning the optimality of coordination. The optimal consistency can be decomposed into two conditions:
Individual-Global-Max (IGM) and True-Global-Max (TGM).
In this paper, to achieve the optimal consistency efficiently, we investigate the requirements of the TGM
condition and go deep into the mechanism of the value representation for LVD and MVD, where the IGM
condition always holds. We first derive the expression of the joint Q value function of LVD and MVD, by
which we draw some interesting conclusions. Firstly, LVD and MVD share the same expression of the
joint Q value function. Secondly, the correspondence between the joint greedy action and the maximal Q
true value (i.e., the team’s best performance) depends heavily on the task-specific reward function for LVD
and MVD. Thirdly, there may be multiple stable points of the joint greedy action. As a result, the joint
policy may converge to different results. More importantly, in some of the stable points, the true Q value
of the joint greedy action is not maximal, which is the root cause of non-optimal coordination and relative
overgeneralization. To ensure the TGM condition for LVD and MVD, the stable point satisfying the TGM
condition (we call it the optimal stable point) is required to be the unique stable point, which is the target
problem to be solved in this paper.
To solve the target problem, we propose the greedy-based value representation (GVR). According to previous
conclusions, the stable points are task-specific due to their dependency on the reward function, for which we
propose the inferior target shaping (ITS). ITS dynamically modifies the true Q value of inferior samples (i.e.,
the samples worse than the current greedy) according to current greedy Q value, which is theoretically proved
to stabilize the optimal point under any reward function. Besides, under ITS, the stability of a non-optimal
point depends only on the probability ratio of superior samples (i.e., the samples better than the current
greedy) to the non-optimal sample, where the non-optimal stable points can be eliminated under a large
enough ratio (Eq.7). We prove two simple ways applied by previous works (i.e. applying weight on the
superior samples (Rashid et al., 2020) and improving exploration (Mahajan et al., 2019)) are both inapplicable
to raise the ratio because the probability of superior samples decreases exponentially with the number of
agents. Therefore, we further propose the superior experience replay (SER), which achieves almost constant
probability of superior samples by saving them in a superior buffer. SER is theoretically proved to eliminate
the non-optimal stable points under ITS.
We have three contributions in this work. (1) This is the first work to derive the exact expression of the joint
Q value function for LVD and MVD. (2) We point out the root cause of non-optimal coordination and further
propose the target problem to be solved for LVD and MVD. (3) We propose the GVR method, which is proved
theoretically to ensure the optimal consistency under sufficient exploration, and our method outperforms
state-of-the-art baselines in various benchmarks.
2	Preliminaries
2.1	Dec-POMDP
We model a fully cooperative multi-agent task as a decentralized partially observable Markov decision process
(Dec-POMDP) described by a tuple G =< S, U, P, r, Z, O, n, γ > (Guestrin et al., 2001; Oliehoek & Amato,
2016). S ∈ S denotes the true state of the environment. At each time step, each agent a ∈ A ≡ {1, 2, ∙∙∙ ,n}
receives a local observation za ∈ Z produced by the observation function O : S × A → Z, and then
chooses an individual action Ua ∈ U according to a local policy ∏a(ua∣τa) : T X U → [0,1], where
Ta ∈ T ≡ (Z × U)* denotes the local action-observation history. Thejoint action of n agents U results in a
shared reward r(s, U) and a transition to the next state s0 〜P(∙∣s, u). Y ∈ [0,1) is a discount factor.
2
Under review as a conference paper at ICLR 2022
We denote the joint variable of group agents with bold symbols, e.g., the joint action u ∈ U ≡ Un, the
joint action-observation history τ ∈ T ≡ Tn, and the joint interactive policy (i.e., the policy interacts with
environment to generate trajectories) π(u∣τ). The true Q value of π(ut∣τt) is denoted by Qn(st, Ut)=
Est+L∞,ut+L∞ [Rt∣st, Ut], where Rt = P∞=0 γirt+ι is the discounted return. The action-state value function
of agent a and the group of agents are defined as utility function Ua (ua, τa) and joint Q value function
Q(U, τ) respectively. The true Q value is the target of the joint Q value in training, serving as the unique
external criterion of the team,s performance. The greedy action u* := argmaXuQ(u, T) is defined as the
joint action with the maximal joint Q value . The optimal action Uopt := argmaxuQ(s, U) is defined as
the joint action with the best team’s performance. For brevity, we sometimes omit the prefix ”joint” for the
joint variables.
2.2	Optimal Consistency and TGM Condition
In CTDE paradigm, agents are expected to act individually according to their local policies (i.e., the individual
greedy actions) while achieve the optimal coordination (i.e., the maximal true Q value). Here we define
the correspondence between the individual greedy actions and the maximal true Q value as the optimal
consistency.
Definition 1 (Optimal consistency). Given a set of utility functions {U 1(u1, T 1)), •…,Un(un, Tn)}, and
the true Q value Q(s, U), if the following holds
{argmax U 1(u1,τ1), .…,argmax U n(un ,τn)} = argmax Q(s, U)	(1)
u1	un	u
then We say the set of utility functions {U 1(u1,τ 1)),…,Un(un, Tn)} satisfies the optimal consistency. For
simplicity, we ignore situations with non-unique optimal actions.
The optimal consistency can be decomposed into two conditions: Individual-Global-Max (IGM) and
True-Global-Max (TGM). The IGM condition proposed by QTRAN (Son et al., 2019) is defined
on the correspondence between individual greedy actions and the joint greedy actions (formally,
{argmaXui U1 (u1,τ1),…，argmaxun Un(on,τn)} = argmaxuQ(u, T)). To achieve the optimal con-
sistency, the correspondence between the joint greedy action and the maximal true Q value is required, for
which we define the TGM condition:
Definition 2 (TGM). Given a joint value function Q(U, T), and the true Q value Q(s, U), if the following
holds
argmax Q(U, T) = argmax Q(s, U)	(2)
uu
then we say the joint value function Q(U, T) satisfies the TGM condition. For simplicity, we ignore situations
with non-unique optimal actions.
3	INVESTIGATION OF THE TGM CONDITION FOR LVD & MVD
Linear value decomposition (LVD) and monotonic value decomposition (MVD) are simple and naturally meet
the IGM condition. To achieve the optimal consistency, we investigate the requirements of the TGM condition
for LVD and MVD. According to Def.2, the TGM condition is related to the joint Q value function Q(U, T).
In this section, we first derive the expression of Q(U, T) for LVD and MVD under -greedy visitation. The
expression indicates there may be non-optimal stable points that violate the TGM condition, which is the root
cause of non-optimal coordination.
3
Under review as a conference paper at ICLR 2022
3.1	EXPRESSION OF THE JOINT Q VALUE FUNCTION FOR LVD & MVD
Firstly, take two-agent linear value decomposition as an example, where the joint Q value function
Q(ui1, uj2, τ) is linearly factorized into two utility functions Q(ui1, uj2, τ) = U1 (ui1, τ1) + U2(uj2, τ2).
u1,uj ∈ {uι,…，Um} denote the individual actions of agent 1,2 respectively, where {uι,…，um} is
the discrete individual action space. Specially, we denote the individual greedy action of agent 1,2 with
〃1*, Uj* respectively. For brevity, Q(uJ,uj, T) and U a (Ua, τ a) are represented by Qij and Uf (a ∈ {1, 2})
respectively. Through the derivation provided in Appendix B.1, Qij can be represented by the true Q values
as
m	2mm
Qij = m X(Qik + Qkj ) + (I -E)(Qi*j + Qij* ) - m χχ Qij
k=1	i=1 j=1
—
E(1 - E) m
-------〉：(Qi* k + Qkj* ) — (1 — e) Qi*j*
m	k=1
(3)
Verification of the expression is provided in Appendix B.2. For monotonic value decomposition, the expression
is identical to Eq.3 (the proof is provided in Appendix C), which indicates the coefficients and bias on utility
functions do not affect the joint Q value function. For situations with more than two agents, by referring to
the derivation in Appendix B.1 and C, the expression of joint Q values can also be obtained.
3.2	REQUIREMENTS TO ENSURE THE TGM CONDITION FOR LVD & MVD
According to Eq.3, the joint Q values change with the greedy action {Ui1* ， Uj2* }. An example of the joint
Q values under 3 different {Ui1* ， Uj2* } is shown in Tab.1(b,c,d), where e = 0.2.
8(oPtimal)	-12	-12
-Γ2	0	0
-12	0	6
(a) The given Qij .
7.40	-8.33	-7.93
--833-	--24.06-	--23.66-
-7.93	-23.66	-23.26
(b) Qij under {Ui1*， Uj2*} = {0， 0}.
-19.90	-10.04	-9.64
--Γ≡4-	-0.17	023
-9.64	0.23	0.63
(c) Qij under {Ui1*， Uj2*} = {1， 1}.
Table 1: (a) The given true Q values Qij(Ui1， Uj2 ∈ {0， 1， 2}). (b,c,d) Calculation results of the joint Q values
Qij with LVD or MVD under different greedy actions {Ui1* ， Uj2* }, where {Ui1* ， Uj2* } is marked with a pink
background. We denote the maximal joint Q values with numbers in bold. Due to the representation limitation,
Qij 6= Qij. Instead, Qij overgeneralizes to Qij.
-24.38	-14.52	-9.32
--14.52-	--4.65-	055
-9.32	0.5	5.75
(d) Qij under {Ui1*， Uj2*} = {2， 2}.
Notice that the example in Tab.1(c) is an unstable point, because {Ui1* ， Uj2* } = {1， 1} 6= argmax Qij =
{2， 2}. To explain this, assume {Ui1* ， Uj2* } and Qij of iteration t are {Ui1* ， Uj2* }t and Qij,t respectively,
where {Ui1* ， Uj2* }t ({Ui1* ， Uj2* } in Tab.1(c)) = argmax Qij,t = {1， 1}. After training, {Ui1* ， Uj2* }t+1 =
argmax Qij,t+1 (Qij in Tab.1(c)) = {2， 2}, which means {Ui1* ， Uj2* }t changes with iteration. Since
Qij,t+1 is related to {Ui1* ， Uj2* }t, both {Ui1* ， Uj2* }t and Qij,t+1 are unstable. As a result, Tab.1(c) finally
converges to the stable point in Tab.1(d). Here we define the stable point of LVD and MVD.
Definition 3 (Stable point of LVD and MVD). Given the joint Q value function Q(u， τ) which has con-
verged under LVD (or MVD) with the target Q(s， u), where Q(s， u) is the true Q value, and given an
4
Under review as a conference paper at ICLR 2022
interactive policy π(u, T) with the greedy action u* (u* = argmax π(s, u)), if the following holds
argmax Q(u, T) = u*	(4)
u
then we say Q(u, T) and u* are stable, and this is a stable point of LVD (or MVD).
According to Def.3, the examples in Tab.1(b) (argmax Qij = {u1*,uj*} = {0,0}) and Tab.1(d)
(argmax Qij = {u1* ,uj*} = {2, 2}) are both stable points (i.e., the joint Q values and the greedy ac-
tion may converge to Tab.1(b) or Tab.1(d)). However, the stable points in Tab.1(d) violates the TGM
condition (i.e., argmax Qij 6= argmax Qij), which is the root cause of non-optimal coordination. Tab.1(b)
and Tab.1(d) are defined as non-optimal stable point and optimal stable point respectively. To ensure the
TGM condition, the optimal stable point is required to be the unique stable point. More discussion about the
stable points is provided in appendix H.
4	Method
In this section, we introduce the greedy-based value representation (GVR). We first propose inferior target
shaping (ITS) to ensure the optimal stable point under any reward function. Besides, under ITS, the stability of
a non-optimal point depends only on the probability ratio of superior samples to the non-optimal sample. To
eliminate the non-optimal stable points, we then investigate two simple methods (i.e., improving exploration
and applying a weight on the superior samples) which raise the ratio. However, both methods are proved
inapplicable due to the extremely small probability of superior samples under large joint action space. Finally,
we describe superior experience replay, which achieves an almost constant proportion of superior samples
through a superior buffer.
4.1	Inferior Target Shaping
According to Eq.3, the joint Q value of any action is related to the true Q values of the whole joint action
space for LVD and MVD, which indicates the stable points depend heavily on the reward function (proofs are
provided in Appendix H). Firstly, we consider to remove the dependency. Because the exact true Q values of
non-optimal samples are uninformative, we only represent the ”non-optimality” of these samples. Here we
propose the ITS target Qits (s, u)
C 晨万一ʃ Q(u*,τ) - α∣Q(u*, τ )|	Q(s, u) < Q(u*, τ) and u = u*
Qits(s,u) =	Q(s, u)	others	(5)
where u* is the joint greedy action and α ∈ (0, 1]. A large enough α prevents the confusion between greedy
and inferior samples. The samples in the first case are called inferior samples. Besides, the action samples u
which satisfy Q(s, u) > Q(s, u*) are called superior samples. ITS target only remains the information of
non-optimality for inferior samples, which eases representation. Under ITS, given the greedy action u* and
any action us(us 6= u*), assuming Q(s, u*) > 0, we have
∆Q(us,T) = Q(us, T) - Q(u*, T) = n(η1 -η2) [Q(s,u*) - (1 - α)Q(u*, T)] +nη1eQQ(s,u*) (6)
where eQ = Q3UQs)—Q*()s,u ) ,ηι =(亲)n-1 and η = (1 - E + m)n-1. We provide two different versions
of proof for Eq.6 in Appendix D, and the calculation result is verified in the experimental part (Fig.1). It is
proved in Appendix D.1 that there is always an optimal stable point under ITS.
When ∆Q(us, T) > 0, argmaxu Q(u, T) 6= u*, which suggests current greedy action is unstable. If u* is a
non-optimal action, to destabilize it, let ∆Q(us, T) > 0 and assume Q(u*, T) ≈ Q(s, u*) (this assumption
is quite accurate, as verified in Appendix E.2), we have
α
η ›
η2
α + eQ
(7)
5
Under review as a conference paper at ICLR 2022
which indicates the non-optimal stable points can be eliminated by raising n1 under ITS. A simple way to
raise n1 is improving exploration. Substituting the expression of ηι and η into Eq.7, we have
m
€ > -----1------------
(eQ)n-1 +1 + m — 1
(8)
When us = argmaxu Q(s, u), we obtain the lower bound of (denoted by €0) from the right side of Eq.8.
However, as the number of agents n and the size of individual action spaces m increases, €0 grows close to 1
(as verified in Fig.1(b)), which is inapplicable in tasks with long episodes.
Another way to raise n1 is increasing the relative weights of the superior samples u§. It is proved in
Appendix E that the non-optimal stable points can be eliminated by applying a weight w(w > α⅛F)) to
the superior samples under ITS. When us = argmaxuQ(s, u), we obtain the lower bound ofw (denoted
by w0). However, the w0 grows exponentially as the number of agents n increases (as verified in Appendix
E.2, w0 = 659.50 when n = 4), which introduces instability in training.
Since n1 decreases rapidly as the joint action space grows, the essence to raise n1 is increasing the proportion
of superior samples and freeing the raised proportion from the dependency on the joint action space
size, for which we introduce the superior experience replay.
4.2 Superior Experience Replay
To distinguish superior samples, we first introduce a critic V (s), which approximates the true Q value of the
joint greedy action. The target of the critic is defined as
V (「ʃ Q(S, U) V(S) < Q(S, U) or U = u*	(9)
Vgvr(s) = V(s) others	(9)
Inspired by prioritized experience replay (Schaul et al., 2015; Zhang & Sutton, 2017), we introduce a
superior buffer where the episodic trajectories are stored. The number of superior samples which satisfy
Q(s, U) > V(s) and U = u* within a trajectory is defined as the priority of the trajectory. In the superior
buffer, trajectories are ranked according to their priorities. The training batch consists of two parts: trajectories
randomly sampled from the replay buffer and the top k trajectories from the superior buffer. At the end of
each episode, the trajectories sampled from the superior buffer will be put back after the update of their
priorities. The working principle of GVR and the algorithm is given in Appendix G.
With SER, the superior samples are consist of two part: sampled from the superior buffer with a small
probability and sampled from the superior buffer with a constant probability. As a result, the proportion of
superior sample is mainly determined by the relative weight on samples from superior buffer, which is a
constant and irrelevant to the joint action space size. It is proved in Appendix F that SER can eliminate the
non-optimal stable points under ITS.
5	Experiments
In this section, firstly we verify the attributes of stable points for linear value decomposition (LVD) in matrix
games, where we also evaluate the improvements of GVR. Secondly, to evaluate the stability and scalability
of our method, we test the performance of GVR in predator-prey tasks with extreme reward shaping. Thirdly,
we conduct experiments on challenging tasks of StarCraft multi-agent challenge (SMAC) (Samvelyan et al.,
2019). Finally, we design ablation studies to investigate the improvement of GVR. Our method is compared
with state-of-the-art baselines including QMIX (Rashid et al., 2018), QPLEX (Wang et al., 2020), and
WQMIX (Rashid et al., 2020). All results are evaluated over 5 seeds. Additional experiments are provided in
Appendix I.
6
Under review as a conference paper at ICLR 2022
5.1	One-step Matrix Game
Matrix game is a simple fully cooperative multi-agent task, where the shared reward is defined by a payoff
matrix. In one-step matrix games, the true Q values are directly accessible from the payoff matrix, which is
convenient for the verification of the optimal consistency.
The verification of stable points for LVD. We conduct experiments on two-agent one-step matrix game to
verify the expression of joint Q values (i.e., Eq.3). We also verify the effect of reward function and exploration
rate on the stable points. The experimental results and conclusions are provided in Appendix G.
Evaluation of GVR. Since the stability under ITS is determined by Eq.6, we first verify the expression under
a random inferior true Q values (Q). Two payoff matrices of size 34 (i.e., m=3 and n=4) are generated for
VDN and ITS respectively:
6(1 + eQ)
Q(vdn) =	6
6(1 - α)
u={0,0,0,0}
u={2,2,2,2}	Q(its)
others
6(1 + eQ)
6
random(-20, 6)
u={0,0,0,0}
u={2,2,2,2}	(10)
others
where eQ = 0.3, α = 0.1. We measure ∆Qopt = Q(0, 0, 0, 0) - Q(2, 2, 2, 2) for VDN and ITS trained
with corresponding matrices, where the greedy action is fixed to u* = {2, 2, 2, 2}. AS shown in Fig.1(a),
the tested ∆Q(uopt, τ) consists with our calculation result (∆Q(uopt, τ) = -1.951) from Eq.6. Besides,
∆Q(uopt, τ) of ITS with random inferior Q equals to that of VDN with fixed inferior Q, which suggests that
under ITS target, the stable points are irrelevant to the return of inferior samples. As a result, ITS largely
frees the stable points from the interference of the reward function.
VDN
----ITS
∣7.85.4 ∙∙∙ 5.411	7.8 rand ∙ ∙ ∙ randl
5.4 ..：	rand ・.:
：,5.4	：・ rand
15.4 …5.4 6 I I	rand ∙ ∙ ∙ rand ð I
payoff matirx payoff matirx
for VDN for ITS
0	2	4	6	8	10
steps(k)
su∙2+->0 V APəəjoOIqEJS IPUIPdO
・ JO.2急X JSəj Uwpon ∙
Figure 1:	Evaluation of GVR in 4-agent matrix games. (a) Comparison of ∆Qopt between ITS with random
reward shaping and VDN with fixed reward shaping. The red dash line (∆Q(uopt, τ) = -1.951) denotes the
calculation result (from Eq.6) under ITS target. (b) Median test ratio of the optimal stable points as grows
(i.e., test probability of the optimal convergence), where y = 1 indicates the optimal stable point is the unique
stable point and y = 0 indicates the optimal point is unstable. The red dash line (0 = 0.837) denotes the
calculated lower bound of (from Eq.8) when ITS eliminates non-optimal stable points.
We also evaluate the ratio of optimal stable points for VDN, ITS and GVR as grows, where the payoff
matrices are generated according to Q(its) (Eq.10) over 5 seeds. At each value of , 100 times of independent
training and test are executed. According to Fig.1(b), for VDN, the optimal point become unstable (i.e.,
y = 0) when > 0.8. For ITS, the optimal points is always stable (i.e., y > 0 (∀ ∈ (0, 1])). Besides, ITS
eliminates most of the non-optimal stable points under large exploration ( > 0.837), which consists with
our calculation result (the red dash line) from Eq.8. For GVR, the optimal stable point is the unique stable
point (i.e., the optimal consistency holds) under a wild range of . However, GVR is unable to ensure the
7
Under review as a conference paper at ICLR 2022
QMIX
matrix size 124
matrix size 124
1412108 6 4
JOXlωUOHPjUəsaɪdəX QJBnbS
upM JSəj UPIPOn
O
^o
Ize
S
X
atrl
ma
8
6
4
2
10
412108 6 4
JOXlωUoHpjUOSaIdOX əjdnbs
ISəj ufpəw
2	4	6

steps(k)
Figure 2:	GVR vs methods with complete expressiveness capacity in matrix games.
optimal consistency under a small , where the optimal sample is not explored in given training steps (e.g., the
probability of the optimal sample UOpt = {0, 0, 0,0} is 1.98e - 5 under the greedy action u* = {2, 2, 2, 2}).
GVR vs methods with complete expressiveness capacity. To compare the efficiency between GVR and
methods with complete expressiveness capacity, we evaluate GVR, QPLEX and CWQMIX in matrix games
with different scales of action space. Similar to Q(its) in Eq.10, we generate random matrices of size 32,
63, and 124 over 5 seeds, where the first and the last element are set to be 8 and 6 respectively. From Fig.2,
in the last two tasks (Un = 63 and Un = 124), the representation errors of CWQMIX and QPLEX do not
decrease during training, which suggests that they are unable to learn the complete expressiveness within
given steps. We do not measure the representation error of GVR because the representation target is modified
by ITS. GVR is the only method ensuring the optimal coordination (i.e. median test return = 8).
—GVR —
punishment = -2
OWQMIX
----CWQMIX
punishment = -5
steps(mil)
(a)
Figure 3: Experiment results on predator-prey. (a) Comparison between GVR and baselines. (b) Performance
of GVR with different punishments.
QMIX
UlmOx JSOI u,spəpv
8
Under review as a conference paper at ICLR 2022
5.2	Predator-Prey
In this subsection, We compare GVR with state-of-the-art baselines in the predator-prey tasks (Bohmer et al.,
2020). In our experiments, 8 predators are trained to capture 8 preys, where the preys are controlled by
random policies. The reward is shared by all predators, and a punishment is applied to the reward when only
a single agent capture a prey. Our experiments are carried out under 3 punishment values.
From Fig.3, VDN and QMIX fail in all experiments, where all agents tend to avoid the preys. QPLEX also
fails in spite of its complete expressiveness capacity. WQMIX can only solve the task with a small punishment
of -2, while GVR is able to solve the tasks under all punishments.
5.3	StarCraft Multi-agent Challenge
StarCraft multi-agent challenge (SMAC) is a popular benchmark for the evaluation of MARL algorithms. We
compare GVR with baselines in various difficult tasks of SMAC. The game version is 69232. Each algorithm
is trained for 2e6 steps in MMM2, 2c_vs_64_zg and 6h_vs_8z, with e damping from 1 to 0.05 during the first
5e4 steps. Especially, in 6h_vs_8z, each algorithm is trained for 5e6 steps, with e damping from 1 to 0.05
during the first 1e6 steps.
---GVR ------ OWQMIX
5m vs 6m	MMM2
QMIX
6h_vs_8z
0 8 6 4 2 0
Lo.o.SSS
QlEX Uw 密Hufpəw
8 6 4 2 0
O.0.0.0.S
QlEX 与。I UfPQn
8 6 4 2 0
0.s0.0.0.
QlEX 与 əhu,spəw
---CWQMIX ------ QPLEX
steps(mil)
Figure 4: Median test win rate vs training steps.
From Fig.4, GVR shows the best performance. Different from predator-prey with abnormal punishments,
the reward function in SMAC is more reasonable, where the linear and monotonic value decomposition can
meet the TGM condition approximately. As a result, the algorithms with full representation expressiveness
capacity (QPLEX, OWQMIX, CWQMIX) do not perform better than QMIX due to the difficulty of complete
expressiveness.
6 Conclusion and Future Work
This paper discusses the optimal coordination in fully cooperative MARL tasks and proposes a new criterion
(i.e., optimal consistency) to evaluate the optimality of coordination in value decomposition. To achieve the
optimal consistency, we introduce the TGM condition for linear and monotonic value decomposition, where
it is proved the TGM can be ensured if the optimal stable point is the unique stable point. Therefore, we
propose the GVR algorithm. Which ensures the optimal stable points via ITS and eliminates the non-optimal
stable points via SER. In experiments on matrix games, we verify our calculation results, which directly
demonstrates the effect of GVR. Besides, in experiments on predator-prey and SMAC, GVR outperforms
state-of-the-art baselines. However, GVR is unable to ensure the TGM condition in hard exploration tasks,
where the superior samples are difficult to obtain. We are interested in the combination of GVR with efficient
exploration approaches in future work.
9
Under review as a conference paper at ICLR 2022
References
Wendelin Bohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International
Conference on Machine Learning, pp. 980-991. PMLR, 2020.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counter-
factual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.
Jakob N Foerster, Yannis M Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with
deep multi-agent reinforcement learning. arXiv preprint arXiv:1605.06676, 2016.
Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. In NIPS, volume 1,
pp. 1523-1530, 2001.
Tarun Gupta, Anuj Mahajan, Bei Peng, Wendelin Bohmer, and Shimon Whiteson. Uneven: Universal value
exploration for multi-agent reinforcement learning. In International Conference on Machine Learning, pp.
3930-3941. PMLR, 2021a.
Tarun Gupta, Anuj Mahajan, Bei Peng, Wendelin Bohmer, and Shimon Whiteson. Uneven: Universal value
exploration for multi-agent reinforcement learning. In International Conference on Machine Learning, pp.
3930-3941. PMLR, 2021b.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational
exploration. arXiv preprint arXiv:1910.07483, 2019.
Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions for
decentralized pomdps. Journal of Artificial Intelligence Research, 32:289-353, 2008.
Liviu Panait, Sean Luke, and R Paul Wiegand. Biasing coevolutionary search for optimal multiagent behaviors.
IEEE Transactions on Evolutionary Computation, 10(6):629-645, 2006.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In
International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic
value function factorisation for deep multi-agent reinforcement learning. arXiv preprint arXiv:2006.10800,
2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ
Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. arXiv preprint arXiv:1902.04043, 2019.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887-5896. PMLR, 2019.
10
Under review as a conference paper at ICLR 2022
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg,
Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for
cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
Sergey Vorotnikov, Konstantin Ermishin, Anaid Nazarova, and Arkady Yuschenko. Multi-agent robotic
systems in collaborative robotics. In International Conference on Interactive Collaborative Robotics, pp.
270-279. Springer, 2018.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent
q-learning. arXiv preprint arXiv:2008.01062, 2020.
Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Influence-based multi-agent exploration. arXiv
preprint arXiv:1910.05512, 2019.
Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. In 2018 AAAI Spring
Symposium Series, 2018.
Chao Wen, Xinghu Yao, Yuhui Wang, and Xiaoyang Tan. Smix (λ): Enhancing centralized value functions
for cooperative multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 7301-7308, 2020.
Tong Wu, Pan Zhou, Kai Liu, Yali Yuan, Xiumin Wang, Huawei Huang, and Dapeng Oliver Wu. Multi-agent
deep reinforcement learning for urban traffic light control in vehicular networks. IEEE Transactions on
Vehicular Technology, 69(8):8243-8256, 2020.
Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu, Changjie Fan, and
Zhongyu Wei. Q-value path decomposition for deep multiagent reinforcement learning. In International
Conference on Machine Learning, pp. 10706-10715. PMLR, 2020a.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten:
A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939,
2020b.
Shangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint arXiv:1712.01275,
2017.
11