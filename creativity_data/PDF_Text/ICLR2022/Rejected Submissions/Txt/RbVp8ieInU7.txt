Under review as a conference paper at ICLR 2022
Low-rank Matrix
Correspondence
Recovery with Unknown
Anonymous authors
Paper under double-blind review
Ab stract
We study a matrix recovery problem with unknown correspondence: given the
∙-v
∙-v
observation matrix Mo = [A, PB], where P is an unknown permutation matrix,
we aim to recover the underlying matrix M = [A, B]. Such problem commonly
arises in many applications where heterogeneous data are utilized and the corre-
spondence among them are unknown, e.g., due to privacy concerns. We show that
it is possible to recover M via solving a nuclear norm minimization problem under
a proper low-rank condition on M, with provable non-asymptotic error bound for
the recovery of M . We propose an algorithm, M3 O (Matrix recovery via Min-Max
Optimization) which recasts this combinatorial problem as a continuous minimax
optimization problem and solves it by proximal gradient with a Max-Oracle. M3O
can also be applied to a more general scenario where we have missing entries in
Mo and multiple groups of data with distinct unknown correspondence. Experi-
ments on simulated data, the MovieLens 100K dataset and Yale B database show
that M3O achieves state-of-the-art performance over several baselines and can re-
cover the ground-truth correspondence with high accuracy.
1 Introduction
In the era of big data, one usually needs to utilize data gathered from multiple disparate platforms
when accomplishing a specific task. However, the correspondence among the data samples from
these different sources are often unknown due to either missing identity information or privacy
reasons (Unnikrishnan et al., 2018; Gruteser et al., 2003; Das & Lee, 2018). Examples include the
multi-image matching problem studied in (Ji et al., 2014; Zeng et al., 2012; Zhou et al., 2015), the
record linkage problem (Chan & Loh, 2001) and the federated recommender system (Yang et al.,
2020).
In the simplest scenario, we have two data matrices A = [a1, ..., an]>, B = [b1, ..., bn]> with
ai ∈ RmA and bi ∈ RmB , which are from two different platforms (data sources). As discussed
above, the correspondence (ai, bi) may not be available, and thereby the goal is to recover the
underlying correspondence between a 1,…,an and b∏(i),…,b∏(n), where π( ∙) denotes an unknown
permutation. We can translate such problem described above as a matrix recovery problem, i.e., to
∙-v
∙-v
recover the matrix M = [A, B] from the permuted observation Mo = [A, PB], where P ∈ Pn is
an unknown permutation matrix and Pn denotes the set of all n × n permutation matrices. We term
this problem as Matrix Recovery with Unknown Correspondence (MRUC).
Inspired by the classical low-rank model for matrix recovery (Wright & Ma, 2021; Mazumder et al.,
2010; Hastie et al., 2015), we especially focus on the scenario where the matrix M features a certain
low-rank structure. Such low-rank model has achieved great success in many applications like the
recommender system (Schafer et al., 2007; Mazumder et al., 2010) and the image recovery and
∙-v
alignment problem (Zeng et al., 2012; Zhou et al., 2015). By denoting Bo = PB, we want to solve
the following rank minimization problem for MRUC,
min rank([A, PBo]).
P∈Pn
(1)
Practical applications. It is known that the recommender system often suffers from data sparsity
(Zhang et al., 2012) because users typically only provide ratings for very few items. To enlarge the
set of observable ratings for each user, we may harness extra data from multiple platforms (Netflix,
Amazon, Youtube, etc.). One classical work on this problem is the multi-domain recommender
system considered in (Zhang et al., 2012). Unfortunately, their work neglects a crucial issue that
1
Under review as a conference paper at ICLR 2022
data from these diverse platforms (or domains) are not always well aligned for two primary reasons.
The first is that the same user may use different identities, or even leave nothing about their identities,
on these platforms. Another reason is that, those platforms are not allowed to share with each other
the identity information about their users for preserving privacy. Another application is the visual
permutation learning problem (Santa Cruz et al., 2017), where one needs to recover the original
image from the shuffled pixels. Both of the two applications give rise to a challenging extension of
the MRUC problem, where we not only need to recover multiple correspondence across different
data sources, but also face the difficulty of dealing with the missing values in data matrix .
Relationship to the multivariate unlabeled sensing problem. Problem (1) is closely related to
the Multivariate Unlabeled Sensing (MUS) problem, which has been studied in (Pananjady et al.,
2017; Zhang et al., 2019a;b; Zhang & Li, 2020; Slawski et al., 2020b;a). Specifically, the MUS is
the multivariate linear regression problem with unknown correspondence, i.e., it solves
min kY - PXWk2F,	(2)
P ∈Pn ,W ∈Rm2 ×m1
where W ∈ Rm2×m1 is the regression coefficient matrix, Y ∈ Rn×m1 and X ∈ Rn×m2 denotes
the output and the permuted input respectively, and ∣∣∙^f is the matrix FrobeniUs norm. In fact, a
concurrent work (Yao et al., 2021) studies the same rank minimization problem as (1), but their
approach is to solve it using the algorithm developed for MUS problem. Despite of the similarity
to the MUS problem, we remark that MRUC problem has it own distinct features and, as shown in
Section 4, the algorithm for the MUS algorithm can not be directly and effectively applied, especially
when there are multiple unknown correspondence and missing entries to be considered.
Related works. To the best of our knowledge, the concurrent and independent (Yao et al., 2021) is
the only work that also considers the MRUC problem. Theoretically, (Yao et al., 2021) showed that
there exists an non-empty open subset U ⊆ Rn×(m1+m2), such that ∀M ∈ U, solving (1) is bound
to recover the original correspondence. However, such results only prove its existence for the subset
U and do not provide a concrete characterization. Regarding the algorithm design, (Yao et al., 2021)
follows the idea of (Slawski et al., 2020b;a) and treats problem (1) heuristically as a MUS problem.
However, there are two main drawbacks in their algorithm that largely limit its practical value. First,
in the dense permutation scenario, it ignores the interaction among the shuffled columns and hence
can not utilize the prior knowledge on the unknown permutation to have an improved performance;
Second, their method can not deal with data with missing values.
Contributions of this work. Our contributions in this work lie in both theoretical and practical
aspects. Theoretically, we are the first to rigorously study how the rank of the data matrix is per-
turbed by the permutation, and show that problem (1) can be used to recover a generic low-rank
random matrix almost surely. Besides, we also propose a nuclear norm minimization problem as
a surrogate for problem (1). The most important theoretical result in this work is that we provide
a non-asymptotic analysis to bound the error of the nuclear norm minimization problem under a
mild assumption. Practically, we propose an efficient algorithm M3O that solves the nuclear norm
minimization problem, which overcomes the aforementioned two shortcomings in (Yao et al., 2021).
Notably, M3O works very well even for an extremely difficult task, where we need to recover mul-
tiple unknown correspondence from the data that are densely permuted and contain missing values.
We remark that this is so far a challenging problem unexplored in the existing literature.
Outline. For conciseness, we will first study the MRUC problem with single unknown correspon-
dence, and then show that the theoretical results and the algorithm can be readily extended to the
more complicated scenarios. We start with building the theoretical results for (1) and its convex
relaxation in Section 2. Then, the algorithm is developed in Section 3. The simulation results are
presented in Section 4 and the conclusions are drawn in Section 5.
Notations. Given two matrices X, Y ∈ Rn×m, we denote hX, Y i = Pin=1 Pjm=1 Xij Yij as the
matrix inner product. We denote X(i) as the ith row of the matrix X and X(i, j) as the element
at the ith row and the jth column. We denote 1m ∈ Rm and 1n×m ∈ Rn×m as the all-one vector
and matrix, respectively, and In be the n × n identity matrix. For α ∈ Rm, β ∈ Rn, we define the
operator ㊉ as ɑ ㊉ β = ɑ 1 > + 1 mβ> ∈ Rm×n. We denote ∣∙∣∣* as the nuclear norm for matrices.
For vectors, We denote ∣∣ ∙ ∣∣0, ∣∣∙∣∣ 1 as the zero norm and 1-norm respectively.
2 Matrix Recovery via a Low-rank Model
How is the matrix rank perturbed by the row permutation? To ansWer this fundamental question,
We first introduce the cycle decomposition of a permutation.
2
Under review as a conference paper at ICLR 2022
Definition 1 (Cycle decomposition of a permutation (Dummit & Foote, 1991)). Let S be a finite
set and π(∙) be a permutation on S. A cycle (a 1,..., an) is a permutation sending a§ to a§ +1 for
1 ≤ j ≤ n — 1 and an to a 1. Then a cycle decomposition of π (∙) is an expression of π (∙) as a union
of several disjoint cycles1 .
It can be verified that any permutation on a finite set has a unique cycle decomposition (Dummit &
Foote, 1991). Therefore, We can define the CyCle number of a permutation ∏(∙) as the number of
disjoint cycles with length greater than 1, which is denoted as C(π). We also define the non-sparsity
of a permutation as the Hamming distance betWeen it and the original sequence, i.e., H(π) =
Ps∈S I[π(s) 6= s]. It is obvious that H(π) > C(π) ifπ is not an identity permutation. As a simple
example, we consider the permutation ∏(∙) that maps the sequence (1,2,3,4,5,6) to (3,1,2,5,4,6).
Now the cycle decomposition for it is π(∙) = (132)(45)(6), and C(π) = 2, H(π) = 5.
In all the following theoretical results, we denote the original matrix as M = [A, B] ∈ Rn×m with
A ∈ Rn×mA , B ∈ Rn×mB , and rank(M) = r, rank(A) = rA, rank(B) = rB . We denote the
corresponding permutation as ∏p (∙) for any permutation matrix P ∈ Pn.. The following proposition
says that the perturbation effect of a permutation π on the rank of M becomes stronger, ifπ permutes
more rows and contains less cycles.
Proposition 1. ∀P ∈ Pn, we have
rank([A, PB]) ≤ min{n, m, rA + rB, r + H(πP) — C (πP)}.	(3)
We have similar result for the case with multiple permutations, which is summarized in Corollary 1
in Appendix A.1. It turns out that, without any assumption on M, (3) is the tightest upper bound for
the rank of a perturbed matrix. Notably, the following proposition says that the upper bound in (3)
is attained with probability 1 for a generic low-rank random matrix.
Definition 2. A probability distribution on R is called a proper distribution if its density function
P(∙) is absolutely continuous with respect the Lebesgue measure on R.
Proposition 2. If the original matrix M is a random matrix with M = RE where R ∈ Rn×r and
E ∈ R r×m are two random matrices whose entries are i.i.d and follow a proper distribution on R,
and r ≤ min{yfn, mA, mB }, then ∀P ∈ Pr,, the equality
rank([A, PB]) = min{2r, r + H(πP) — C(πP)}	(4)
holds with probability 1.
Convex relaxation for the rank function. Despite the previous theoretical justification for problem
(1), it is non-convex and non-smooth. Another crucial issue is that we often have a noisy observation
matrix and it is well known that the rank function is extremely sensitive to the additive noise. In this
paper, we assume that the observation matrix is corrupted by i.i.d Gaussian additive noise, i.e.,
Mo = [Ao, Bo] = [A,PB] + W, where 卬(i,j)〜N(0,σ2),
where σ2 reflects the strength of the noise. We first denote the singular values of a matrix X ∈ Rn×m
as σX1 , ..., σXk where k = min{n, m}. Since rank(X) = k[σX1 , ..., σXk ]k0, from Proposition 2 we
can view the perturbation effect of a permutation to a low-rank matrix as breaking the sparsity of its
singular values. This view leads naturally to the well-known 1-norm minimization problem which
has been proven robust to additive noise and can yield a sparse solution (Wright & Ma, 2021), i.e.,
min Il[Ao, PBoW* = Il[σMo,--,σMo]bι.	(5)
P∈Pn
Since for an arbitrary matrix, the 1-norm of its singular values is equivalent to its nuclear norm, we
refer problem (5) as the nuclear norm minimization problem.
Theoretical justification for the nuclear norm. Nuclear norm has a long history used as a convex
surrogate for the rank, and it has been theoretically justified for applications like low-rank matrix
completion (Candes & Tao, 2010; Wright & Ma, 2021). It is also important to see whether the
nuclear norm is still a good surrogate for the rank minimization problem (1). In this work, we
establish a sufficient condition on A and B under which problem (5) is provably justified for corre-
spondence recovery. We denote A = Pir=A1 σAi uiAvAi>, B = Pir=B1 σBi uiB vBi> as the singular values
decomposition of A and B, where the σAi and σBi are the non-zero singular values.
Firstly, from the definition of nuclear norm, it can be simply verified for any P ∈ Pn that
-Z/N ≤ (I[A,PB]∣* - IlMll*)∕∣M∣* ≤ Z/N,	(6)
where we denote N = max{IAI*, IBI*} andZ = min{IAI*, IBI*}. The inequality (6) indicates
that A and B should have comparable magnitude, i.e., IAI* ≈ IBI*, otherwise the influence of the
permutation will be less significant. Therefore, we are interested in the scenario where the singular
values of A and B are comparable, which is described as the following Assumption 1.
1Two cycles are disjoint if they do not have common elements
3
Under review as a conference paper at ICLR 2022
Assumption 1. There exists a constant 1 ≥ 0 such that
lσA - σB 1 ≤ C1, ∀i = 1, ..,r,
(7)
where we denote that σAi = 0 if i > rA, and σBi = 0 if i > rB.
Similar to the matrix rank, we also need a proper low-rank assumption on the matrix M for the
nuclear norm. In this work, we particularly study the scenario that the left singular vectors of A and
B are similar, which we formally describe as Assumption 2. We refer Assumption 2 as a proper
low-rank assumption, because it indicates that the column space of M can be approximated by the
column space of one of its submatrices.
Assumption 2. There exists a constant C2 ≥ 0 such that
kuiA - uiBk ≤ C2,∀i = 1, ..., T,
(8)
where we denote T = min{rA, rB }.
Furthermore, we also need that all the column singular vectors u1A, ..., uTA, u1B, ..., uTB are variant
under any P ∈ Pn with P 6= In : we define a vector u ∈ Rn to be variant under a P ∈ Pn if
P u 6= u. One simple and weak condition for a vector u to satisfy such property is that u dose not
contains duplicated elements, which leads to the following Assumption 3.
Assumption 3. There exists a constant C3 ≥ 0 such that
min min |u(i) - u(j)| ≥ C3 > 0,
u∈U i6=j
(9)
where U = {u1A, ..., uTA, u1B, ..., uTB }.
In summary, the assumptions mentioned above feature a typical low-rank structure in M, and implies
that the nuclear norm of M is sensitive to permutation. With the three assumptions, we have the
following important theorem, which provides high probability bound for the approximation error of
(5).
We denote the solution to (5) as P *, and let ∏ and ∏ be the corresponding permutation to the per-
mutation matrices P*τ and P, respectively. We define the difference between the two permutations
∏* and ∏ as the Hamming distance
n
dH ( π*,π) t=, X I( π* ( i ) = π( i )) .
i=1
Theorem 1. Under Assumptions 1, 2 and 3, if additionally c ι ≤ 4M, c2 ≤ min{ 2√τ, A2M }，and
σ ≤ 16M2, then thefollowing bound for the Hamming distance
2
2D D
dH(π*, π) ≤ -2 I 2 -
C3
3
D + (√2 + 2) c 1 r + √2 c 2 N + 2√2 DLσ
-TCΓe2	(10)
holds with probability at least 1 — 2exp { — 8Lσ }, where L = max {私 m}, D = ∣Ak* + ∣∣B∣∣*.
The proof to all the aforementioned theoretical results are provided in Appendix A.1.
Remark 1. From Theorem 1 we can see that when C3 > 0, and C1 → 0, C2 → 0, σ → 0, the
error d，H(∏*,Π) will converge to zero with probability 1. Furthermore, We can also discover that the
correspondence can be difficult to recover when:
•	The rank of original matrix M is high, which can be seen from (10).
•	The magnitude of A and B w.r.t rank or nuclear norm are not comparable, which can be seen
from (6) and (7).
•	The strength of noise is high, which can be seen from (10) and the probability in Theorem 1.
Notably, the numerical experiments in Section 4.1 corroborate our claims as well.
4
Under review as a conference paper at ICLR 2022
Remark 2. Additionally, from the proof of Theorem 1 we find
that the fundamental reason for the success of (5) is that if M
satisfies the previous assumptions, we have
∣∣[A, PB] k*∕∣M∣* ≈ O ((1 — H(∏P)/2n)-1) .	(11)
In many applications, we can only observe part of the full data.
Therefore, it is also worthwhile to investigate whether (11) still
holds when we can only access a small subset of the entries in Mo .
Notably, Figure 1 gives the positive answer and shows that the
relationship (11) is gracefully degraded when the percentage of
observable entries is decreasing. This phenomenon is remarkable
since it indicates the original correspondence can be recovered
1.10
* 1.08
= 1.06
京'1.04-
≥1.02
1.00
O 20	40	60	80 IOO
H(πp)
Figure 1: The relationship (11) un-
der different percentages of observ-
able entries.
from only part of the full data. The matrices used to generate Figure 1 are the same as those in
Section 4.1, and the nuclear norm is computed approximately by first filling the missing entries
using Soft-Impute algorithm (Mazumder et al., 2010).
3 Algorithm
In this section, We consider the scenario with missing values, i.e., our observed data is Pω (Mo)=
Pω([Ao, Bo]), where Pω is an operator that selects entries that are in the set of observable indices
Ω. In this scenario, problem (5) can not be directly used since the evaluation of the nuclear norm
and optimization of the permutation are coupled together. Inspired by the matrix completion method
(Hastie et al., 2015; Mazumder et al., 2010), we propose to solve an alternative form of (5) as follows,
_min	min IlPω([Ao,PBo]) - Pω(c)「+ λ Ilcl ,	(12)
M ∈ R n×m P ∈Pn	F	*
where λ > 0 is the penalty coefficient. We denote that Mc = [McA , McB] and McA , McB are the two
submatrices with the same dimension as Ao and Bo respectively. We can write (12) equivalently as
_min	min IlPω(Ao)-Pω(MA)「+〈C(MB),Pi + λllcll ,	(13)
Mc∈Rn×m P∈Pn	F	*
where C(MB) ∈ Rn×n is the pairing cost matrix with	2
C(McB)(i,j) =	X	McB(i,j00)-Bo(j,j00)	, ∀i,j= 1,..., n.
(j,j00) ∈Ω '	'
Baseline algorithm. A conventional strategy to handle an optimization problem like (13) is the
alternating minimization or the block coordinate descent algorithm (Abid et al., 2017). Specifically,
it executes the following two updates iteratively until it converges.
cnew J arg min l PΩ ([ Ao, Pold Bo ])-PΩ (c)「+ λ|∣ cl ,	(14)
Mc∈Rn×m	F	*
Pbnew J arg min hC(McBnew),Pi.	(15)
P∈Pn
The first update step (14) is a convex optimization problem and can be solved by the proximal
gradient algorithm (Mazumder et al., 2010). The second update step (15) is actually a discrete
optimal transport problem which can be solved by the classical Hungarian algorithm with time
complexity O(n3) (Jonker & Volgenant, 1986). However, as we will see in the Section 4, this
algorithm performs poorly, and it is likely to fall into an undesirable local solution quickly in practice.
Specifically, the main reason is that the solution of (15) is often not unique and a small change in
MB would lead to large change of P. To address this issue, we propose a novel and efficient
algorithm M3O algorithm based on the entropic optimal transport (Peyre et al., 2019) and min-max
optimization (Jin et al., 2020).
Smoothing the permutation with entropy regularization. For any a ∈ Rn , b ∈ Rm , we define
Π(a, b) ={S∈Rn×m :S1m =a,S>1n=b,S(i,j) ≥0, ∀i,j},
which is also known as the Birkhoff polytope. The famous Birkhoff-von Neumann theorem
(Birkhoff, 1946) states that the set of extremal points of Π(1n, 1n) is equal to Pn. Inspired by
(Xie et al., 2021) and the interior point method for linear programming (Bertsekas, 1997), in order
to smooth the optimization process of the baseline algorithm, we relax P from being an exact per-
mutation matrix, i.e., to keep P staying inside the Birkhoff polytope Π(1n, 1n). That is, we propose
to replace the combinatorial problem (15) with the following continuous optimization problem
min	hC(McB),Pi +H(P),	(16)
P∈Π(1n,1n)
5
Under review as a conference paper at ICLR 2022
where H(P) d=ef. Pi,j P (i, j)(log(P (i, j)) - 1) is the matrix negative entropy and > 0 is the
regularization coefficient. Notably, (16) is also known as the Entropic Optimal Transport (EOT)
problem. According to (Peyre et al., 2019), (16) is a strongly convex optimization problem with
respect to 1-norm and can be solved roughly in the O(n2) complexity per iteration by the Sinkhorn
algorithm. Specifically, the Sinkhorn algorithm solves the dual problem of (16),
—∙^~∙
max W (MB , α, β)
α,β∈Rn
d=f√ 1 n,α + h 1 n龄 Y 1 n×n, exp[ ㊉	MB )
(17)

which reduces the variables dimension from n2 to 2n and is thus greatly favorable in the high
dimension scenario. By substituting the inner minimization problem of (13) with (16), we end up
with solving the following unconstrained min-max optimization problem
min max ∣∣ A — TcA∣∣ + We(Mb, α, β) + λ ∣∣Tc∣∣ .	(18)
Follows the idea of (Jin et al., 2020), we consider to adopt a proximal gradient algorithm with a
Max-Oracle for (18). Specifically, we employ the Skinhorn algorithm (Peyre et al., 2019) as the Max-
Oracle to retrieve an ε-good solution of the inner max problem (17). We summarize our proposed
algorithm M3O (Matrix recovery via Min-Max Optimization) in Algorithm 1, where prox入卜口 (∙)
is the proximal operator of nuclear norm and ρk is the gradient stepsize. The convergence property
of M3O can be obtained by following (Jin et al., 2020), which shows that, with a decaying stepsize,
M3O is bound to converge to an ε-good Nash equilibrium within O(ε-2) iterations.
Algorithm 1: M3O
1	while not converged do
2	For the tolerance ε, run the Sinkhorn algorithm to find α*, β* such that
We(MB,α^,β^) > max We(MB,α,β) — ε;
α,β
3	PerformMk+1 . prox工卜口 (Mk — PkPMFe(Mk,α^,β^)), where
Fe(Mc,α,β) d=ef. ∣A — McA∣2F +We(McB,α,β);
4	end
Remark 3. A recent work (Xie et al., 2020) proposes a decaying strategy for the entropy regular-
ization coefficient in (16) so that the optimal solutions of (15) and (16) do not deviate too much.
Inspired by it, in our practice, we take large in the beginning and gradually shrink it by half until
the objective function stops improving for K steps.
Remark 4. A useful trick is that we should not take large stepsize ρk in the early iterations because
the permutation matrix could still be far away from the optimal one. However, a small stepsize would
lead to slow convergence. Heuristically, we propose an adaptive stepsize strategy that performs well
in practice. For the solution of (16) Pk at the kth iteration, we compute the two statistics
δk = ∣∣Pk-1 — Pk ∣ F /2n and Ck = ∣∣maxjPk (∙, j) — 1 n 11 /n∙
Here δk represents how fast the permutation matrix Pbk changes over the iterations, while ck
measures how far the current Pbk is close to an exact permutation matrix. Both δk and ck re-
flect the confidence on the current found correspondence. Based on them, we set the stepsize as
ρk+1 = (1 — δk)(1 — ck)ω, where ω > 0 is a tunable parameter which is often set to a value between
0.5 to 3. ω actually trades off the convergence speed and final performance. The smaller the ω, the
faster the convergence. Therefore, a practical way is to start with a small ω, and gradually increase
it until the final performance stops improving.
Remark 5. As discussed in Section 1, in many cases we have to deal with the problem that in-
volves multiple correspondence, i.e., we need to recover the matrix M = [A, B1, ..., Bd] from the
observation data Pω(Mo), where
〜	Mo = [Ao, Bo,…,Bo ] = [A, P B ι,…,PdBd ]+ W,
where Pl ∈ Pn and W is a noise matrix. We refer such problem as the d-correspondence problem.
An important observation is that, although the number of possible correspondence increase expo-
nentially as d grows, the complexity of M3O per iteration only linearly increases with d and can be
implemented in a fully parallel fashion. Specifically, in this scenario, we solve the problem
6
Under review as a conference paper at ICLR 2022
men P1⅛JPΩ (Ao ) -PΩ ( MA / + XX "(MBl) ,Pl+ 田(Pl)
l=1
*
(19)
s.t. Pl∈ Π(1n,1n), l= 1,..., d,
where we
___— ____— ___— _______— __________—
denote M = [MA, MB1 , ..., MBd]. Here MA
_—
and MBl have the same dimension with Ao
and Bol, respectively. One can find that the inner problems for solving Pl are actually decoupled for
each l , which guarantees an efficient parallel implementation.
Remark 6. Since x problem (12) has a similar form to that considered in (Mazumder et al., 2010).
We adopt the same tuning strategy of λ as in (Mazumder et al., 2010), which suggests that we should
start with large λ and gradually decrease it.
We relegate more details about M3O to Appendix A.6.
4	Experiments
In this section, we evaluate our proposed
M3O on both synthetic and real-world
datasets, including the MovieLens 100K
and the Extended Yale B dataset. We
also provide an ablation study for the
decaying entropy regularization strategy
and the adaptive stepsize strategy pro-
posed in Remarks 3 and 4. In all the ex-
periments, we employ the Soft-Impute
algorithm (Mazumder et al., 2010) as a standard algorithm for matrix completion. Extra experiment
details and auxiliary results can be found in Appendix A.9.
Algorithms. We denote the following algorithms for comparison in all the experiments:
Figure 2: Performance of various algorithms on a simulated 1-
correspondence problem.
1.	Oracle: Running the Soft-Impute algorithm with ground-truth correspondence.
2.	Baseline: The Baseline algorithm in (14) and (15).
3.	MUS: Since there is currently no existing algorithm directly applicable to the scenario consid-
ered by (19), inspired by (Yao et al., 2021), we modify and extend the algorithm in (Zhang &
Li, 2020), which is originally proposed for the MUS problem, to deal with the MRUC problem.
The details of the adapted algorithm are provided in Appendix A.8.
4.1	Synthetic data
We first investigate the property of our proposed M3O algorithm on the synthetic data.
Data generation. We generate the original data matrix in this form M = RE + ηW, where
R ∈ Rn×r, E ∈ Rr×m, W ∈ Rn×m and η > 0 indicates the strength of the additive noise.
The entries of R, E, W are all i.i.d sampled from the N(0, 1). Then we split the data matrix
Mby M = [A, B1,..., Bd] where we denote A ∈ Rn×mA, B1 ∈ Rn×m1, ..., Bd ∈ Rn×md
to represent data from d + 1 data sources. The permuted observation matrix Mo is obtained by
first generating d permutation matrices P1, ..., Pd randomly and independently, and then computing
Mo = [A, P1B1,…，PdBd]. Finally, We remove (1 一 ∣Ω| . 100%/(n ∙ m)) percent of the entries of
Mo randomly and uniformly, where ∣Ω ∣ indicating the number of observable entries.
Ablation study. We denote the folloWing variants of M3O for the ablation study.
1.	M3 O-AS-DE: M3O with both Adpative Stepsize and Decaying Entropy regularization.
2.	M3 O-DE: M3O with Decaying Entropy regularization only. M3O-DE-1 and M3O-DE-2 adopt
constant stepsize ρk = 0.5 and ρk = 0.01, respectively.
3.	M3 O-AS: M3O with Adpative Stepsize only. The entropy coefficient is fixed to 0.0005.
In the following results, we denote πlas the corresponding permutation to Pl. We initialize Mc from
Gaussian distribution for the M3O algorithm and its variants. We choose initial as 0.1 and K = 100
as the default for the decaying entropy regularization, and set ω = 3 as the default for the adaptive
stepsize. We also report the achieved objective values of (19) for the tested algorithms, except for
the MUS algorithm since it has a different objective. We denote π as the recovered permutation.
Results. Figure 2 displays the result under the setting η = 0.1, ∣Ω ∣ ∙ 100%/(n ∙ m) = 80%,
n = m = 100, r = 5, d = 1, mA = 60 and m1 = 40. The algorithm M3O-AS-DE achieves the
best result, and can recover the ground-truth correspondence. M3O-AS behaves similarly to Baseline
7
Under review as a conference paper at ICLR 2022
and MUS. They all converge to a poor local solution quickly. M3O-DE-1 converges quickly and also
falls into a poor local solution due to large stepsize, while M3O-DE-2 adopts a small stepsize and
hence suffers from slow convergence. Due to the superiority of M3O-AS-DE over the other variants,
in the following results, we refer M3O as M3O-AS-DE for short.
Figure 3 examine M3O on a I-CorreSPondenCe problem under different regimes w.r.t ∣ Ω ∣, η, r and
mA/n. Here we use mA /n to control the difference of the magnitude of the submatrices. As we
Can see, the results are well aligned with our prediCtion in Remarks 1 and 2.
Finally, we examine M3O on a few d-CorrespondenCe problems. See Table 1 for various results,
where we set r = 5 and ε = 0.1. NotiCe that for the 4-CorrespondenCe problem in the table, there
are (100!)4 possible CorrespondenCe. Even for suCh a diffiCult problem, M3O is able to reCover
61.5% of the ground-truth CorrespondenCe with a good initialization.
(a) dH v.s. IΩI
(b) dH v.s. η
(C) dH v.s. r
0.1	0.3	0.5	0.7	0.9
mA/n
(d) dH v.s. mA /n
Figure 3: Performance of M3O on a I-CorreSPondenCe problem under different levels of ∣Ω∣, η, r and mA/n.
The default setting is ∣Ω∣ ∙ 100%/(n ∙ m) = 80%, η = 0.1, n = m = 100, r = 5, mA = 60, and m 1 = 40.
The mean with minimum and maximum are CalCulated from 10 different random initializations.
Table 1: PerformanCe of M3O for various d-CorrespondenCe problems. The normalized permutation error
Pd=1 dH (πι, ∏ι)/d is reported as mean±std (min) over 10 different random initializations.
(n,mA,m1, ...,md)	d	IΩI ∙ 100%/ (n ∙ m)	P Pd=1 dH (∏ι ,∏ι)/d
(100,40,30,30)	2	40%	33.35 ± 32.85 (0.00)
(100,20,40,40)	2	40%	58.90 ± 27.21 (2.00)
(100,45,25,25,25)	3	50%	61.97 ± 15.41 (37.33)
(100,40,25,25,25,25)	4	60%	59.90 ± 13.64 (38.50)
4.2	Multi-domain recommender system without correspondence
In this seCtion, we study the performanCe of M3O on a real world dataset MovieLens 100K2, whiCh
is a widely used movie reCommendation dataset (Harper & Konstan, 2015). In this appliCation, we
mainly foCus on the metriC Root Mean Squared Error (RMSE), i.e.,
RMSE 驾｛N X(Mij- Mij).
Data. MovieLens 100K Contains 100,000 ratings within the sCale 1-5. The ratings are given by
943 users on 1,682 movies. Genre information about movies is also provided. We adopt a similar
setting with (Zhang et al., 2012). We extraCt five most popular genres, whiCh are Comedy, RomanCe,
Drama, ACtion, Thriller respeCtively, to define the data from 5 different domains (or platforms).
In addition to (Zhang et al., 2012), we randomly permute the indexes of the users from these five
domains respeCtively, so that the CorrespondenCe among these data beCome unknown. In this way,
the problem belongs to the 4-CorrespondenCe problem as disCussed before. The ratings are split
randomly, with 80% of them as the training data and the other 20% of them as the test data.
Algorithms. We Consider the following additional algorithms for Comparison.
1.	SIC: Running the Soft-Impute algorithm independently for the 5 different platforms.
2.	SIR: Running the Soft-Impute algorithm with Randomly generated CorrespondenCe.
Results. As disCussed in experiments on the simulated data, the exaCt reCovery of CorrespondenCe
beComes impossible due to the small amount of observable entries. Therefore, in the following
experiment, sinCe exaCt CorrespondenCe is not needed, we fix = 0.05 for M3O. Table 2 shows the
results by averaging the RMSE on the test data over 10 different random seeds.
We Can first see that the matrix Completion with a wrong CorrespondenCe, i.e., SIR, Can be harm-
ful to the overall performanCe sinCe it is even worse than the results of SIC. Notably, although the
ground-truth CorrespondenCe Can not be reCovered, eaCh platform Can still benefit from M3O sinCe it
2https://grouplens.org/datasets/movielens/100k/
8
Under review as a conference paper at ICLR 2022
improves the performance over SIC. This is mainly because M3O is still able to correspond similar
users for inferring missing ratings. On the contrary, since both Baseline and MUS can only establish
an exact one-to-one correspondence for each user, they fail to improve SIC significantly. Remark-
ably, M3O is only inferior to the Oracle method a little, and even achieves lower test RMSE than the
Oracle method on the Comedy genre.
Table 2: Test RMSE of various algorithms on MovieLens 100K
Method	Comedy	Romance	Drama	Action	Thriller	Total
SIR	1.0202	1.0158	0.9808	0.9803	0.9811	0.9944
SIC	0.9694	0.9695	0.9317	0.9175	0.9253	0.9418
MUS	0.9659	0.9842	0.9423	0.9305	0.9306	0.9485
Baseline	0.9728	0.9562	0.9379	0.9105	0.9145	0.9395
M3O	0.9389	0.8787	0.9139	0.8556	0.8567	0.8948
Oracle	0.9444	0.7825	0.9058	0.8176	0.8098	0.8667
4.3	Visual permutation recovery
We show that M3 O is flexible and can also be used to recover
matrix that is not in the form [A, PB]. We can see this from
the problem formulation in (13), where the cost matrix C(∙)
can be constructed in other ways as long as itis a function ofa
permutation. Typically, M3O can be used to solve a challeng-
ing face image recovery problem. The original face image
with size 180 × 180 in Figure 4(a) comes from the Extend Yale
B database (Georghiades et al., 2001). The corrupted image
is visualized in Figure 4(b), where the pixel blocks with size
30 × 30 in the upper left are shuffled randomly, and 30% of the
total pixels are removed. This kind of problem is recently con-
sidered in (Santa Cruz et al., 2017), which proposes to recover
the corrupted image in a data-driven way using convolutional
neural networks. However, we show that it is possible to re-
cover the image without additional data by merely exploiting
the underlying low-rank structure of the image itself.
(a) Original
(b) Corrupted
(c) Baseline	(d) M3O
Figure 4: Performance of M3O on a
face recovery problem.
This experiment setting is similar to that in (Yao et al., 2021) but the algorithm in (Yao et al., 2021)
can not be applied since it can not work with the missing values. The MUS algorithm is also not
applicable since this problem can not be written in the form of linear regression problem. From
Figure 4(c) and 4(d) we can find that M3O performs better than the Baseline, and can even recover
the original orders of pixel blocks. More results similar to the Figure 4 and experiment details are
provided in Appendix A.9.
5 Conclusion
In this paper, we have studied the important MRUC problem where part of the observed submatrix
is shuffled. This problem has not been well explored in the existing literature. Theoretically, we
are the first to rigorously analyze the role of low-rank model in the MRUC problem, and is also the
first to show that minimizing nuclear norm is provably efficient for recovering a typical low-rank
matrix. For practical implementations, we propose a highly efficient algorithm, the M3O algorithm,
which is shown to consistently achieve the best performance over several baselines in all the tested
scenarios.
It is worthwhile to point out that apart from the two applications we have studied in this paper, this
problem could arise in more scenarios like the gnome assembly problem (Huang & Madan, 1999),
the video pose tracking problem (Ganapathi et al., 2012) and the privacy-aware sensor networks
(Gruteser et al., 2003), etc. We believes that our work provides a general framework to deal with
unknown correspondence issue in these scenarios.
As we have shown in Figure 3, one major limit of our algorithm is the sensitivity to the initialization.
The phenomenon is exacerbated when the additive noise is high or the numbers of observable entries
are small. We suggest to try with a few different initialization strategy when applying M3 O to a
specific task. Finding stable initialization strategy is also an important task for our future works.
9
Under review as a conference paper at ICLR 2022
References
Abubakar Abid, Ada Poon, and James Zou. Linear regression with shuffled labels. arXiv preprint
arXiv:1705.01342, 2017.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):
334-334,1997.
Garrett Birkhoff. Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser A, 5:147-151,
1946.
Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix com-
pletion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.
Hock-Peng Chan and Wei-Liem Loh. A file linkage problem of degroot and goel revisited. Statistica
Sinica, pp. 1031-1045, 2001.
Debasmit Das and C. S. George Lee. Sample-to-Sample Correspondence for Unsupervised Domain
Adaptation. Engineering Applications of Artificial Intelligence, 73:80-91, August 2018. ISSN
09521976. doi: 10.1016/j.engappai.2018.05.001. URL http://arxiv.org/abs/1805.
00355. arXiv: 1805.00355.
Herbert A David and Haikady N Nagaraja. Order statistics. John Wiley & Sons, 2004.
David S Dummit and Richard M Foote. Abstract algebra, volume 1999. Prentice Hall Englewood
Cliffs, NJ, 1991.
Varun Ganapathi, Christian Plagemann, Daphne Koller, and Sebastian Thrun. Real-time human pose
tracking from range data. In European conference on computer vision, pp. 738-751. Springer,
2012.
A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman. From few to many: Illumination cone
models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach.
Intelligence, 23(6):643-660, 2001.
Marco Gruteser, Graham Schelle, Ashish Jain, Richard Han, and Dirk Grunwald. Privacy-aware
location sensor networks. In HotOS, volume 3, pp. 163-168, 2003.
Paul R Halmos. Measure theory, volume 18. Springer, 2013.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis), 5(4):1-19, 2015.
Trevor Hastie, Rahul Mazumder, Jason D. Lee, and Reza Zadeh. Matrix completion and low-rank
SVD via fast alternating least squares. The Journal of Machine Learning Research, 16(1):3367-
3402, 2015. Publisher: JMLR. org.
Xiaoqiu Huang and Anup Madan. Cap3: A dna sequence assembly program. Genome research, 9
(9):868-877, 1999.
Pan Ji, Hongdong Li, Mathieu Salzmann, and Yuchao Dai. Robust motion segmentation with un-
known correspondence. In European conference on computer vision, pp. 204-219. Springer,
2014.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In International Conference on Machine Learning, pp. 4880-4889.
PMLR, 2020.
Roy Jonker and Ton Volgenant. Improving the hungarian assignment algorithm. Operations Re-
search Letters, 5(4):171-175, 1986.
Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learn-
ing large incomplete matrices. The Journal of Machine Learning Research, 11:2287-2322, 2010.
Publisher: JMLR. org.
10
Under review as a conference paper at ICLR 2022
Ashwin Pananjady, Martin J Wainwright, and Thomas A Courtade. Denoising linear models with
permuted data. In 2017 IEEE International Symposium on Information Theory (ISIT), pp. 446-
450. IEEE, 2017.
Gabriel Peyr6, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and TrendsR in Machine Learning, 11(5-6):355-607, 2019.
Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual
permutation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3949-3957, 2017.
J Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. Collaborative filtering recommender
systems. In The adaptive web, pp. 291-324. Springer, 2007.
Martin Slawski, Emanuel Ben-David, and Ping Li. Two-stage approach to multivariate linear regres-
sion with sparsely mismatched data. J. Mach. Learn. Res., 21(204):1-42, 2020a.
Martin Slawski, Mostafa Rahmani, and Ping Li. A sparse representation-based approach to linear re-
gression with partially shuffled labels. In Uncertainty in Artificial Intelligence, pp. 38-48. PMLR,
2020b.
Jayakrishnan Unnikrishnan, Saeid Haghighatshoar, and Martin Vetterli. Unlabeled sensing with
random linear measurements. IEEE Transactions on Information Theory, 64(5):3237-3253, 2018.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
John Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models: Princi-
ples, Computation, and Applications. Cambridge University Press, 2021.
Yujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan Zha. A fast proximal point method
for computing exact wasserstein distance. In Ryan P. Adams and Vibhav Gogate (eds.), Pro-
ceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115 of Proceed-
ings of Machine Learning Research, pp. 433-453. PMLR, 22-25 Jul 2020. URL https:
//proceedings.mlr.press/v115/xie20b.html.
Yujia Xie, Yixiu Mao, Simiao Zuo, Hongteng Xu, Xiaojing Ye, Tuo Zhao, and Hongyuan Zha. A
hypergradient approach to robust regression without correspondence. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
l35SB-_raSQ.
Liu Yang, Ben Tan, Vincent W Zheng, Kai Chen, and Qiang Yang. Federated recommendation
systems. In Federated Learning, pp. 225-239. Springer, 2020.
Yunzhen Yao, Liangzu Peng, and Manolis C Tsakiris. Unlabeled principal component analysis.
arXiv preprint arXiv:2101.09446, 2021.
Zinan Zeng, Tsung-Han Chan, Kui Jia, and Dong Xu. Finding correspondence from multiple images
via sparse and low-rank decomposition. In European Conference on Computer Vision, pp. 325-
339. Springer, 2012.
Hang Zhang and Ping Li. Optimal estimator for unlabeled linear regression. In International Con-
ference on Machine Learning, pp. 11153-11162. PMLR, 2020.
Hang Zhang, Martin Slawski, and Ping Li. The benefits of diversity: Permutation recovery in
unlabeled sensing from multiple measurement vectors. arXiv preprint arXiv:1909.02496, 2019a.
Hang Zhang, Martin Slawski, and Ping Li. Permutation recovery from multiple measurement vectors
in unlabeled sensing. In 2019 IEEE International Symposium on Information Theory (ISIT), pp.
1857-1861. IEEE, 2019b.
Yu Zhang, Bin Cao, and Dit-Yan Yeung. Multi-domain collaborative filtering. arXiv preprint
arXiv:1203.3535, 2012.
Xiaowei Zhou, Menglong Zhu, and Kostas Daniilidis. Multi-image matching via fast alternating
minimization. In Proceedings of the IEEE International Conference on Computer Vision, pp.
4032-4040, 2015.
11