Under review as a conference paper at ICLR 2022
Adversarial Distributions Against
Out-of-Distribution Detectors
Anonymous authors
Paper under double-blind review
Ab stract
Out-of-distribution (OOD) detection is the task of determining whether an input
lies outside the training data distribution. As an outlier may deviate from the
training distribution in unexpected ways, an ideal OOD detector should be able to
detect all types of outliers. However, current evaluation protocols test a detector
over OOD datasets that cover only a small fraction of all possible outliers, leading
to overly optimistic views of OOD detector performance. In this paper, we pro-
pose a novel evaluation framework for OOD detection that tests a detector over
a larger, unexplored space of outliers. In our framework, a detector is evaluated
with samples from its adversarial distribution, which generates diverse outlier
samples that are likely to be misclassified as in-distribution by the detector. Using
adversarial distributions, we investigate OOD detectors with reported near-perfect
performance on standard benchmarks like CIFAR-10 vs SVHN. Our methods dis-
cover a wide range of samples that are obviously outlier but recognized as in-
distribution by the detectors, indicating that current state-of-the-art detectors are
not as perfect as they seem on existing benchmarks.
1 Introduction
Identifying whether an input datum lies outside the training data distribution is one of the canoni-
cal problems in machine learning. Over its long history, the problem has been called by multiple
names, including novelty detection (Markou & Singh, 2003), outlier detection (Hawkins, 1980), one-
class classification (Japkowicz et al., 1995), and more recently, out-of-distribution (OOD) detection
(Hendrycks & Gimpel, 2016). Investigation of the problem has resulted in a number of real-world
applications, for example, medical diagnosis (Li et al., 2019) and inspection of defective parts and
products (Bergmann et al., 2019). The relevance of OOD detection is growing beyond these appli-
cations, as an OOD detector is considered an essential component of a trustworthy machine learning
system. For example, without a reliable OOD detector, an image classifier trained to classify cats
and dogs may incorrectly classify a human as belonging to one of these two classes (Hendrycks
et al., 2019b). In order to advance the development of reliable OOD detection algorithms, a more
comprehensive evaluation protocol is needed.
Current evaluation protocols adopted by the community provide a distorted view of a detector’s
performance for two reasons. First, OOD detectors are tested over a small fraction of possible
outliers. Detectors are typically evaluated using test OOD datasets chosen by a researcher. Since
the test OOD datasets do not cover the entire space of outliers, there may exist untested outliers
that the detector fails to classify correctly, even though the detector perfectly detects the chosen test
OOD points as shown in Figure 1. To assess a detector’s performance in a more comprehensive and
systematic way, a method is needed to test a detector over a larger, unexplored space beyond what
is covered by the test OOD datasets.
Second, the current evaluation protocol neglects the worst-case behavior of an OOD detector. The
average performance metric is often not sufficient to build trust on a detector, because in safety-
critical applications even a single mistake can result in fatal consequences. A detector should be
tested adversarially, through an active search for its worst-case failure mode, i.e., an outlier that is
classified with maximal confidence as an inlier. Such failure cases may reveal weaknesses of the
tested detector, and provide informative clues for building a better detection algorithm.
1
Under review as a conference paper at ICLR 2022
In this paper, we propose a novel evaluation protocol of
OOD detectors that addresses the above-mentioned limi-
tations of current evaluation methods. We first formulate
the notion of adversarial search against an OOD detec-
tor, a search problem of finding an outlier that a detector
classifies as an inlier with the greatest confidence. A de-
tector may have more than one significant failure mode,
and it is more informative to find a set of diverse failure
cases instead of the most critical one. To that end, we pro-
pose the adversarial distribution against an OOD detec-
tor, which generates outlier samples that are likely to be
misclassified by the given detector. Measuring OOD de-
tection performance against samples from a detector’s ad-
versarial distribution gives an accurate and finer-grained
assessment on its performance.
To ensure that samples from an adversarial distribution
are indeed outliers, the distribution needs to be supported
on a zero-inlier space, a set without any overlap to the in-
lier distribution. Meanwhile, the zero-inlier space should
In-Dist
i
*
Figure 1: An illustration of OOD detec-
tion. The test data (red stars) are per-
fectly predicted as OOD (red hatching),
but other untested outliers (gray stars)
are misclassified as in-distribution. The
blue shade indicates the support of the
training data distribution, the ideal deci-
sion boundary for OOD detection.
be large so that we may observe meaningful failure mode of OOD detectors within the space. How-
ever, finding such a space is generally challenging, as the true boundary between inliers and outliers
is unknown. We circumvent this challenge by building a generative model over known outliers. Our
construction of zero-inlier spaces contain diverse samples.
We implement 11 previously proposed OOD detectors and investigate their behavior using their ad-
versarial distributions. Among the tested detectors, 8 detectors report near-perfect OOD detection
performance on a popular benchmark, CIFAR-10 (in) vs SVHN (out), effectively being indistin-
guishable with respect to their performance. Our investigation reveals that the 8 detectors in fact
have diverging degrees of detection quality outside the SVHN test set. Our methods also lead to
several interesting insights that suggest techniques for improving OOD detection.
Our main contributions can be summarized as follows:
•	We propose the adversarial search and adversarial distributions, which can be used to evaluate
OOD detection algorithms beyond a pre-defined test OOD dataset;
•	We provide practical techniques to define the space of outliers that contains samples outside the
test OOD dataset;
•	By examining the state-of-the-art OOD detectors, we show that OOD detectors with seemingly
equivalent performance differ significantly in their failure modes;
Related Work Testing the worst-case performance of an algorithm is highly related to evaluating
adversarial robustness of the detector. Detection of adversarially perturbed outliers is investigated
in previous literature where perturbation is assumed to be restricted in a small norm-ball Hein et al.
(2019); Meinke & Hein (2020); Bitterwolf et al. (2020). The idea of using an autoencoder during an
adversarial attack is investigated in Tu et al. (2019). The idea of generating outliers are investigated
in the context of improving OOD detection Chen et al. (2020) or adversarial attack Song et al.
(2018).
In Section 2, we provide essential preliminaries. Section 3 formulates adversarial search and ad-
versarial distributions, and Section 4 introduces techniques to construct a zero-inlier space. Our
main experimental results are presented in Section 5, with deeper discussions provided in Section 6.
Section 7 concludes the paper.
2	Background: Out-of-Distribution Detection
2.1	Definition
We consider a probability distribution of interest Pin , samples from which we consider as inliers.
Each sample is represented as a D-dimensional real-valued vector. The probability density function
of Pin is denoted as pin(x). We write the support of Pin as Sin = {x|pin(x) > 0} ⊂ X ⊂ RD,
2
Under review as a conference paper at ICLR 2022
where X is the data space, the set of all possible values for data, which we assume to be compact.
Then, we define OOD-ness as follows:
Definition 1. A vector x is out-of-distribution (OOD), or an outlier, with respect to Pin if x does
not belong to the support of Pin, i.e., x 6∈ Sin . Conversely, x is in-distribution when x ∈ Sin. A
distribution Q having the support SQ is OOD to Pin, when SQ ∩ Sin = 0.
Another popular definition of OOD-ness characterizes a vector x as OOD when the vector belongs to
a density sub-level set, x ∈ {x|pin(x) ≤ η} (Steinwart et al., 2005). However, the density sub-level
set does not provide a consistent characterization of the OOD-ness. A vector classified as OOD in
one coordinate may not be classified as OOD in a different coordinate, because a probability density
function can be arbitrarily distorted via an invertible coordinate transform as pointed out in (Lan &
Dinh, 2020). On the contrary, our definition of OOD using the density support provides an invariant
characterization of outliers with respect to such transforms.
An OOD detector f (x) : RD → R is a function which outputs a larger value for an input more
likely to be an outlier. A test vector x* is classified as OOD f (x*) > 为 for the threshold 为.
A detector score refers to the function value of f (x). We shall assume f(x) is bounded. In this
paper, we consider a black-box setting, where any information other than the function value off(x),
such as its gradient, is not accessible. An OOD detector is normally trained using an in-distribution
dataset Din, a set of iid samples from Pin. However, some OOD detectors utilize additional datasets
other than Din .
2.2	Evaluation of OOD Detectors
The currently accepted evaluation protocol for OOD detectors relies on one or multiple test OOD
datasets Dout, which contain a finite number of samples considered as OOD with respect to Pin by
human prior knowledge. When Pin is a distribution of images, test OOD sets are often chosen from
separately published image datasets of which contents are different from that of Pin . For example,
when an image dataset of animals is selected as in-distribution, a set of digit images can be used as
a test OOD dataset.
Given a test OOD dataset, an OOD detector f(x) performs the binary classification against in-
distribution dataset, and the quality of the classification is considered as an indicator for how good
f(x) is as an OOD detector. The classification result is summarized using metrics such as the area
under the receiver operating characteristic curve (AUROC or AUC). AUC is a preferred metric in a
number of literature, as it does not require the specification of the threshold ηf. AUC score of 1.0
indicates the perfect classification, and AUC of 0.5 implies the random guess.
The research community has been focusing on a few representative in-distribution and OOD dataset
pairs, such as CIFAR-10 (in) vs SVHN (out) and Fashion-MNIST (in) vs MNIST (out). These
dataset pairs become popular after the reports showing that OOD detectors built upon deep genera-
tive models, such as PixelCNN++ (Salimans et al., 2017) or Glow (Kingma & Dhariwal, 2018), fail
to detect outliers (Hendrycks et al., 2019a; Nalisnick et al., 2019). In fact, the generative-model-
based detectors score AUC lower than 0.5, meaning that SVHN images are more strongly perceived
as CIFAR-10 than the actual CIFAR-10 images by the detectors. This observation spurred intense
research efforts, and now there are multiple OOD detectors achieving AUC scores higher than 0.9
or even higher than 0.99 on CIFAR-10 vs SVHN as listed in Section 5. Given the near-perfect de-
tection score, we question whether the detectors are indeed good OOD detectors beyond the tested
examples.
3	Adversarial Distributions
Our goal is to test how robust the state-of-the-art OOD detectors are. The robustness of an OOD
detector can be investigated through a search process to find its failure modes. We are particularly
interested in failure cases where outlier examples misclassified as in-distribution by the detector.
Ideally, we would search for failure modes over all possible outliers, but the true set of outliers
Sout ≡ X - Sin is unknown. Instead, we specify a search space T which is a subset of Sout
and only contains outliers. We call T a zero-inlier space. In this section, we formulate two search
problems on T. The first formulation, Adversarial Search, is based on optimization, and the second
3
Under review as a conference paper at ICLR 2022
formulation, Adversarial Distribution, is based on sampling. Details on selecting meaningful T
will be discussed in Section 4.
3.1	Adversarial Search
Finding failure modes of an OOD detector can be formulated as an optimization problem. For a
detector f (x) and a search space T, an adversarial search problem is finding an element x* in T
which has the lowest detector score.
x* = arg min f(x) such that x ∈ T ⊂ Sout,	(1)
x∈T
where x* is the worst-case outlier we search for. If the detector score for the worst-case outlier
f(x*) is greater than the score of any samples in in-distribution set, then the detector f(x) can be
confirmed as robust in the search space T. Otherwise, we conclude that the tested detector has at
least one failure mode and deviates from the ideal detector. The degree of deviation may be used to
quantify the robustness of a detector.
Example: Adversarial Attack An example of the adversarial search is applying an adversarial
perturbation on outliers so that they have small f(x) (Bitterwolf et al., 2020). In this case, T is a
ball centered at an outlier from a test OOD dataset T = {x| ∣∣x - X∣∣ ≤ r, X ∈ Dout}. A point With
the smallest detector score is found via a gradient descent method. The AUC score for classifying
the perturbed samples from inliers is called Adversarial AUC (BitterWolf et al., 2020).
3.2	Adversarial Distributions
When the search space T is larger than a ball centered at an outlier, f(x) may have multiple local
minima, and each minimum may reveal different failure modes of a detector. To explore multiple
failure modes in f(x), We propose a novel approach. We deliberately design a probability distri-
bution pf (x) so that samples from pf (x) cover multiple minima of f(x) in T. We call pf (x) an
Adversarial Distribution of a detector f (x).
An adversarial distribution is a Gibbs distribution, also knoWn as an energy-based model. Its energy
function is the detector score f(x), i.e., pf (x) is proportional to exp(-f (x)/T). The support of
pf (x) is the search space T.
Pf(X) = -1exp(-f (x)/T) if X ∈T⊂ Sout and Pf(X) = 0 otherwise,	(2)
Z
where Z = T exp(-f (X)/T)dX is the normalization constant, and T > 0 is called temperature.
Z < ∞, as we have assumed the bounded f(X) and the compact data space in Section 2.
An adversarial distribution Pf (X) has three important properties by construction. First, samples from
Pf (X) are guaranteed to be OOD, as the support is T. Second, an adversarial distribution assigns
a high probability density on samples likely to be classified as in-distribution by the detector, i.e.,
Pf (X) is high when f(X) is small. Third, for the limit T → 0, sampling from an adversarial
distribution becomes equivalent to the adversarial search, as the probability mass is concentrated on
X* in Eq. (1). Hence, an adversarial distribution can be viewed as a relaxation of the adversarial
search, and T governs the degree of relaxation.
Samples from Pf (X) can be drawn using Markov Chain Monte Carlo (MCMC). MCMC is guaran-
teed to visit all modes of a probability distribution at least in theory, and therefore every failure mode
of a detector can be generated. Measuring how well f(X) classifies inliers from Pf (X)’s samples
using metrics such as AUC gives a comprehensive measure on the performance of the detector.
4	Zero-inlier Space
The support T of an adversarial distribution defines the search space where the robustness of an
OOD detector is evaluated. While there is some degree of freedom in the choice ofT, one condition
should be strictly satisfied: T should not contain inlier data points, i.e., should be a zero-inlier space.
However, as the true decision boundary between outliers and inliers is unknown, it is challenging to
ensure T only contains outliers.
4
Under review as a conference paper at ICLR 2022
Figure 2: The illustration of Tg and Th .
In the example of the adversarial attack provided in Section 3.1, T is set as a ball around a known
outlier. T is ensured to be a zero-inlier space as the radius of the ball is very small. However, such
a small ball only contains examples visually indistinguishable from its center, lacking diversity. In
what follows, we provide an example construction of T which contains a more diverse set of outliers.
Here, we describe a procedure for constructing a zero-inlier search space T given a set of in-
distribution dataset Din and OOD dataset Dout . Note that T is constructed during the evaluation
stage of OOD detectors and does not affect an OOD detector being tested. Also, the construction
of T requires no additional data compared to a typical evaluation protocol which assumes a human-
curated OOD dataset.
Instead of a ball around each outlier, we consider a generator function g(z) spanning a set of outlier
samples Dout . Such a generator is able to produce an unseen sample which resembles the known
outliers. As the generator is trained only using Dout, it is unlikely for an inlier to be generated from
g(z). Nevertheless, to avoid a fortuitous generation of an inlier, we introduce a binary classifier h(x)
which is supervised to discriminate Din and Dout .
Let Tg be a set of samples generated by g(z), and Th be a set of samples classified as they are from
Dout . Our search space T is given as an intersection of Tg and Th :
T = Tg ∩ Th .	(3)
4.1	Outlier-spanning Generator
A generator g(z) : Z → X is a map from a lower-dimensional space Z ⊂ RDZ to the data space
X, where DZ smaller than the dimensionality of X. We denote Tg as a set of samples that can be
produced by g(z):
Tg = {x = g(z)|z ∈ Z}.	(4)
In our implementation of adversarial distributions, we train an autoencoder to reconstruct Dout and
use its decoder as g(z). As the autoencoder can reconstruct samples in Dout (with some error),
the generator (approximately) spans the space where outliers reside. Similarly, a generator of a
generative adversarial network can also serve as g(z).
It is reported that an autoencoder may reconstruct an input vector that is significantly different from
its training data even though the autoencoder is not the identity mapping (Tong et al., 2019; Gong
et al., 2019; Yoon et al., 2021). This phenomenon implies that g(z) may be capable of generat-
ing samples that is significantly different from Dout. The ability of g(z) to reach outside of Dout
may positively affect the diversity of Tg , but increases the chance of generating x that is close to
inliers. To ensure that Tg only contains outliers, we introduce an additional component can filter out
accidentally generated inliers.
4.2	B inary classifier separating inliers and outliers
Here, we develop an additional mechanism for ensuring that no inlier is present in our search space
by exploiting the fact that there is no overlap of the supports between an inlier distribution, and a
distribution generating an OOD dataset. When two distributions do not overlap, there exists a binary
classifier that perfectly classifies the two distributions.
Proposition 1 (Binary classification of disjoint distributions). Suppose two probability distributions
P and Q with their supports SP and SQ , respectively. P and Q are OOD to each other, if and only
5
Under review as a conference paper at ICLR 2022
if there exists a binary classifier h*(x) : X → R that perfectly discriminates P and Q, i.e., there
exists a threshold ηh* ∈ R such that the classification boundary h (x) = ηh* separates SP and Sq.
More formal arguments will be provided in Appendix A. Assume an oracle binary classifier h (x)
which perfectly discriminating Din and Dout. From Proposition 1, the oracle classifier partitions the
whole space into two, where one partition contains only Din and the other contains only Dout . The
partition containing Dout can be chosen as a zero-inlier space.
Proposition 1 holds approximately for an empirically obtained classifier h(x). A binary classifier
often achieves AUC score higher than 0.9999 when evaluated on the test splits of Din and Dout .
Hence, we define an outlier-side of h(x)’s decision boundary as Th. Assuming that h(x) assigns
lower value for outliers,
Th = {x|h(x) < ηh, x ∈ X},	(5)
for a threshold ηh. However, a real-world classifier is not an oracle h(x) = h*(x) and can never
be perfect. The classifier may misclassify or may be vulnerable to adversarial perturbations. To
minimize the effect, we apply a few techniques. First, we set the threshold ηh lower than a zero-
inlier threshold, which is the smallest h(x) value among all training, validation, and testing inlier
examples. Second, test-time augmentation is applied on h(x) to make the prediction more robust.
We use horizontal flip and rotations of 90, 180, 270 degrees. Total 5 predictions are averaged.
Detailed discussion on the robustness of h(x) is given in Section E.1.
5	Experiments
In our experiments, we aim to reveal previously unknown failure modes of the state-of-the-art OOD
detectors using adversarial distributions. We first show that adversarial distributions can find the
known weaknesses of baseline detectors. Then, we apply our methods on the state-of-the-art OOD
detectors which show near-perfect performance on a popular benchmark. Finally, we show that
the failure modes of the detectors are not readily transferable. However, a simple ensemble does
not improve the robustness of OOD detection significantly. Table 1 summarizes our experimental
results.
5.1	Experimental Settings
Datasets We use CIFAR-10 as in-distribution data Din . SVHN and CelebA are used as test OOD
datasets Dout . All datasets are splitted into training, validation, and testing sets. All data are 32×32
RGB images. Details on datasets can be found in Appendix B.
OOD Detectors We implement 11 previously proposed outlier detectors. The detectors are
grouped into two, the weak and the strong, based on their performance on CIFAR-10 vs SVHN.
The weak detectors are outlier detectors that fail dramatically on the benchmark by predicting im-
ages in SVHN being more likely to be in-distribution than images in CIFAR-10.
The following is the list of the weak detectors used in our experiment.
•	Autoencoder (AE) (Rumelhart et al., 1986): A neural network trained to reconstruct its input.
The detector score f(x) is the reconstruction error ofx (Japkowicz et al., 1995).
•	PixelCNN++ (PXCNN) (Salimans et al., 2017): An autoregressive generative model for images.
•	Glow (Kingma & Dhariwal, 2018): A generative model based on a normalizing flow.
For PXCNN and Glow, the negative log-likelihood is used as the detector score.
We select strong detectors satisfying three criteria: 1) AUC score on CIFAR-10 vs SVHN is higher
than 0.9. 2) The code is publicly available and written in PyTorch. The language constraint is
introduced for the unified experiment. 3) The performance claimed in the original work should be
reproducible.
• Normalized AE (NAE) (Yoon et al., 2021): AE with an additional mechanism for suppressing
the reconstruction of outliers. As in AE, f (x) is the reconstruction error. Only trained with
in-distribution data.
6
Under review as a conference paper at ICLR 2022
Table 1: Evaluation of OOD detectors using the adversarial distributions. Test Set indicates the test
split of a test OOD dataset. Dz denotes the dimension of the latent space of the generator. AUC
scores are evaluated using 1,000 samples from adversarial distributions.
OOD Dataset	SVHN			CelebA				
OOD Samples	Test Set	AdvDist		Test Set		AdvDist		
Dz	-	16	32	64	-	16	32	64
Weak Detectors								
Glow	.069	.018	.038	.104	.542	.007	.007	.087
PXCNN	.076	.135	.051	.119	.639	.023	.088	.043
AE	.080	.016	.055	.092	.533	.050	.136	.449
Strong Detectors								
NAE	.935	.227	.153	.126	.874	.654	.537	.470
GOOD	.943	.399	.386	.372	.939	.376	.343	.300
ACET	.966	.370	.398	.369	.986	.382	.342	.303
CEDA	.978	.623	.628	.626	.981	.618	.617	.612
SSD	.989	.567	.464	.453	.780	.379	.410	.392
MD	.993	.636	.619	.620	.796	.417	.346	.419
OE	.997	.700	.696	.678	.992	.692	.697	.700
CSI	.998	.866	.894	.986	.890	.588	.582	.561
Ensemble								
GOOD+ACET+CEDA	.983	.322	.331	.316	.991	.389	.397	.465
MD+NAE+OE	.996	.723	.525	.747	.978	.732	.684	.664
•	Outlier Exposure (OE) (Hendrycks et al., 2019a): The predictive confidence of a classifier on an
external dataset is minimized.
•	Confidence Enhancing Data Augmentation (CEDA) (Hein et al., 2019): Similarly to OE, the
predictive confidence of a classifier on an external dataset is minimized.
•	Adversarial Confidence Enhancing Training (ACET) (Hein et al., 2019): The worst-case con-
fidence of a classifier on an external dataset is minimized.
•	Guaranteed OOD Detector (GOOD) (Bitterwolf et al., 2020): IBP (Gowal et al., 2018) is used
to obtain the confidence interval in the neighborhood of OOD data points, and the interval is
minimized during training.
•	Mahalnobis Distance (MD) (Lee et al., 2018): Negative minimum Mahalanobis distance to in-
distribution data in the hidden representation spaces is used as f(x). Mahalanobis distances are
separately defined for each class and the weight is tuned with a few out-of-distribution data.
•	Self-Supervised Detector (SSD) (Sehwag et al., 2021): f(x) is computed based on Mahalanobis
distance on the representation space which is learned via self-supervised learning.
•	CSI (Tack et al., 2020): Cosine similarity to the nearest training data in the representation space
learned via self-supervised learning is used as f (x).
For OE, CEDA, ACET, and GOOD, the negative maximum softmax probability of a classifier is
used as f (x). We utilize the publicly available pretrained models except for MD and SSD where we
use the publicly available training scripts. There are a number of other competitive OOD detectors
we considered but not included in the experiment of this paper. See Appendix D.2 for discussion
on such detectors. We will continuously update our leaderboard afterward to include other OOD
detection methods.
5.2	Implementation of Adversarial Distributions
Generator g(x) We use the decoder of an convolutional AE with the latent space Z = [-1, 1]DZ
where DZ = {16, 32, 64}. The autoencoder is trained to reconstruct samples in Dout.
Binary Classifier h(x) Our binary classifier h(x) is based on ResNet-50 pre-trained on ImageNet.
The last layer is replaced with a newly initialized binary classification layer, and then the whole
7
Under review as a conference paper at ICLR 2022
samples are provided in Appendix F.
ht∆F	GCCK
SSD
b F
“EEC
® ⅛ *夕
Figure 3: Random samples from the adversarial distribution of the weak detectors (DZ = 32). More
<
φ
φ
I
ω
φ

Figure 4: Random samples from adversarial distributions of the strong detectors. More samples can
be found in Appendix F.
network is fine-tuned to classify Din from Dout. We use the logit value as h(x). As a zero-inlier
threshold, we set ηh = -5 for SVHN and ηh = -1 for CelebA.
Adversarial Distribution pf (x) We apply standardization on f (x). The mean and the standard
deviation of detector score is computed on the validation split of in-distribution data. For all experi-
ments, the temperature is set T = 0.1.
Gibbs sampling is performed on Z to generate samples from pf (x). At each step, a dimension of z
is randomly selected, and a proposal is generated by a unit Gaussian distribution. A proposal outside
the domain is projected back. A proposal is accepted based on Metropolis-Hastings criterion. We
run 1,000 independent Markov chains with each chain runs for 10,000 steps. The final states of
Markov chains are accepted as samples. Samples that do not belong to Th are rejected. More
implementation details can be found in Appendix C.
5.3	Adversarial Distributions Against Detectors with Known Weaknesses
We confirm the effectiveness of adversarial distributions by applying them on the weak detectors,
Glow, PXCNN, and AE, where the weaknesses have already been analyzed. The likelihoods of Glow
and PXCNN become spuriously high for images with low complexity (Serra et al., 2020). Similarly,
AE produces also low reconstruction error simple images (Yoon et al., 2021). From Figure 3, the
samples from adversarial distributions pf (x) are consistent with the previously known tendency.
The samples are very blurry and a large portion of pixels are monotone. Simplicity of the samples
are more clear when they are compared to the samples from a strong detector’s pf (x) in Figure 4.
5.4	Evaluating State-of-the-Art Detectors
The proposed methods reveals that 8 strong detectors, with almost equivalent and near-perfect per-
formance on CIFAR-10 vs SVHN, are actually very vulnerable outside of the test points. All the
strong detectors show significantly decreased AUC against samples from adversarial distributions.
Among the tested detectors, CSI is the most robust when SVHN is Dout. However, CSI does not per-
form well when outliers and adversarial distributions are based on CelebA. All other detectors show
a similar degree of performance drop against samples from their adversarial distributions pf (x) vi-
sualized in Figure 4. Note that most of samples from pf (x) retain the visual features of the OOD
dataset on which pf (x) based. Also, the samples are visually dissimilar to CIFAR-10.
In the range from 16 to 64, the choice of latent space dimensionality DZ does not significantly affect
the efficacy of adversarial distributions in terms of AUC scores. Using smaller DZ makes the search
space narrower, but MCMC can be performed more effectively. Meanwhile, larger DZ increases the
diversity of samples, but MCMC becomes challenging.
8
Under review as a conference paper at ICLR 2022
Transferability and Detector Ensemble We question whether an adversarial distribution is trans-
ferable, i.e., given two detectors f1(x) and f2(x), are samples from pf1 (x) able to deceive another
detector f2(x)? We collect samples from pf1 (x) then check if f2(x) can detect them as outliers. The
pairwise results are displayed in Figure 5, showing that there are varying degree of transferability.
Adversarial distributions are largely transferable between GOOD, CEDA, and ACET, possibly be-
cause their constructions are significantly similar (Hein et al., 2019). Except for the three, the other
detectors are generally capable of detecting another detector’s adversarial distribution samples. It
is interesting to see that NAE also performs well on detecting adversarial distribution samples from
other detectors, while NAE is not the best performing detector. We suspect the limited transfer-
ability is originated from the distinct inductive biases of detectors. For example, NAE is the only
autoencoder-based algorithm, and SSD and CSI are trained via different self-supervised learning
processes.
When detectors do not share failure modes, a more robust detector may be formed through averaging
their (normalized) detection scores. We construct two ensembles of three OOD detectors, one with
highly correlated failure modes (GOOD+ACET+CEDA) and the other with less correlated failure
modes (MD+NAE+OE), and measure their robustness. From the last two rows in Table 1, both
ensembles barely improve the robustness of OOD detector compared to the individual detectors.
6	Discussion
Extensions The outlier space utilized in the analysis can be expanded by augmenting the test OOD
dataset with an additional OOD dataset. Moreover, there is significant flexibility in the design of
g(x). In a specific application, domain knowledge can be embedded in g(x) to provide a more
meaningful outlier space.
Limitations Our methods still rely on test OOD datasets to obtain g(x) and h(x), and the choice of
the datasets need human design. Furthermore, operating in the reduced outlier space, our methods
can not confirm that a detector is optimal. However, these limitations are also present in the current
evaluation protocol as well.
Future Work To facilitate the development of OOD detection methods, we intend to publish an
online software suite for evaluating OOD detectors based on adversarial distributions. We also plan
to publish an online leaderboard displaying the results from an adversarial distribution analysis, and
to update the leaderboard with the latest OOD detection methods.
7	Conclusion
In this paper, we have addressed the limitations of the current evaluation protocol for OOD detection
and proposed a novel framework, adversarial distributions, that can be used to investigate failure
modes of OOD detectors. The proposed framework posing new challenges in OOD detection by
discovering unexplored weaknesses in the existing OOD detectors, We expect our framework to
stimulate the advance of trustworthy machine learning, where a model needs to be evaluated beyond
the fixed test dataset.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Our main ethical concern is that a subset of OOD detectors used in our experiment, OE, CEDA,
ACET and GOOD, are trained using 80 Million Tiny Images dataset (Torralba et al., 2008), which is
retracted by authors over ethical concerns. While we were aware of the issue of the dataset, the use
of models trained on the dataset was inevitable because of the reproducibility. We have tried to train
OOD detectors using alternative datasets but failed to reach the performance of detectors originally
trained on 80 Million Tiny Images. To minimize the effect of the retracted dataset, the dataset was
never used directly. We only used the publicly available model checkpoints, and did not download
or access to a copy of dataset.
Reproducibility Statement
As mentioned in intro, we will provide the software suite to evaluate the weaknesses of the OOD
detector. Since our algorithm consists of widely used techniques such as autoencoders, image clas-
sifiers, and MCMC sampling, there will be no problem with reproduction.
References
Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad-a ComPrehen-
sive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, PP. 9592-9600, 2019.
Julian Bitterwolf, Alexander Meinke, and Matthias Hein. Certifiably adversarially robust detection
of out-of-distribution data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, PP. 16085-16095. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/b90c46963248e6d7aab1e0f429743ca0-Paper.pdf.
Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Informative outlier matters:
Robustifying out-of-distribution detection using outlier mining. arXiv preprint arXiv:2006.15207,
2020.
Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, and Bernhard ScholkoPf. From
variational to deterministic autoencoders. In International Conference on Learning Representa-
tions, 2020. URL https://openreview.net/forum?id=S1g7tpEYDS.
Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh,
and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deeP
autoencoder for unsuPervised anomaly detection. In IEEE International Conference on Computer
Vision (ICCV), 2019.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound ProPagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Douglas M Hawkins. Identification of outliers, volume 11. SPringer, 1980.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence Predictions far away from the training data and how to mitigate the Problem. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Dan Hendrycks and Kevin GimPel. A baseline for detecting misclassified and out-of-distribution
examPles in neural networks. arXiv preprint arXiv:1610.02136, 2016.
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. DeeP anomaly detection with outlier
exPosure. In International Conference on Learning Representations, 2019a. URL https://
openreview.net/forum?id=HyxCxhRcY7.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examPles. arXiv preprint arXiv:1907.07174, 2019b.
10
Under review as a conference paper at ICLR 2022
Nathalie Japkowicz, Catherine Myers, Mark Gluck, et al. A novelty detection approach to classifi-
cation. In Proceedings of the International Joint Conference on Artificial Intelligence, volume 1,
pp.518-523,1995.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
A Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of
Tront, 2009.
Charline Le Lan and Laurent Dinh. Perfect density models cannot guarantee anomaly detection.
arXiv preprint arXiv:2012.03808, 2020.
KIMIN Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In 32nd Conference on Neural Information
Processing Systems (NIPS). Neural Information Processing Systems Foundation, 2018.
Xingyu Li, Marko Radulovic, Ksenija Kanjer, and Konstantinos N Plataniotis. Discriminative pat-
tern mining for breast cancer histopathology image classification via fully convolutional autoen-
coder. IEEE Access, 7:36433-36445, 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Markos Markou and Sameer Singh. Novelty detection: a review—part 1: statistical approaches.
Signal processing, 83(12):2481-2497, 2003.
Alexander Meinke and Matthias Hein. Towards neural networks that provably know when they
don’t know. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ByxGkySKwH.
Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward
Teller. Equation of state calculations by fast computing machines. The journal of chemical
physics, 21(6):1087-1092, 1953.
Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do
deep generative models know what they don’t know? In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=H1xwNhCcYm.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua
Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1e79596878b2320cac26dd792a6c51c9- Paper.pdf.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning Internal Representations by Error
Propagation, pp. 318-362. MIT Press, Cambridge, MA, USA, 1986. ISBN 026268053X.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint
arXiv:1701.05517, 2017.
Vikash Sehwag, Mung Chiang, and Prateek Mittal. {SSD}: A unified framework for self-supervised
outlier detection. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=v5gjXpmR8J.
Joan Serra, David Alvarez, Vicenc Gomez, Olga Slizovskaia, Jose F. Nufiez, and Jordi Luque. Input
complexity and out-of-distribution detection with likelihood-based generative models. In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=SyxIWpVYvr.
11
Under review as a conference paper at ICLR 2022
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. Advances in Neural Information Processing Systems, 31:8312-
8323, 2018.
Ingo Steinwart, Don Hush, and Clint Scovel. A classification framework for anomaly detection.
Journal of Machine Learning Research, 6(Feb):211-232, 2005.
Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive
learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176, 2020.
LUcas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.
Alexander Tong, Roozbah Yousefzadeh, Guy Wolf, and Smita Krishnaswamy. Fixing bias
in reconstruction-based anomaly detection with lipschitz discriminators. arXiv preprint
arXiv:1905.10710, 2019.
Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 30(11):1958-1970, 2008.
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and
Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attack-
ing black-box neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 742-749, 2019.
Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690-9700. PMLR, 2020.
Zhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection score
for variational auto-encoder. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20685-20696. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf.
Sangwoong Yoon, Yung-Kyun Noh, and Frank Park. Autoencoding under normalization constraints.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12087-12097.
PMLR, 18-24 Jul 2021.
12
Under review as a conference paper at ICLR 2022
Appendix
Appendix is organized as follows:
•	Section A: Proof of Proposition 1
•	Section B: Datasets
•	Section C: Implementation of Adversarial Distributions
•	Section D: Implementation of OOD Detectors
•	Section E: Extended Experimental Results
•	Section F: Additional Samples from Adversarial Distributions
A	Proof of Proposition 1
Proposition 1 (Restated). Suppose two probability distributions P and Q with their supports SP
and SQ , respectively. P and Q are OOD to each other, if and only if there exists a binary classifier
h(x) : X → R that perfectly discriminates P and Q, i.e., there exists a threshold ηh ∈ R such that
the classification boundary h(x) = ηh separates SP and SQ .
Proof. If P and Q are OOD to each other, then SP ∩Sq = 0. Then, We can construct a classifier
h(x) such that h(x) > ηh for x ∈ SP and h(x) ≤ ηh for x ∈ SQ for any constant ηh ∈ R.
Therefore, h(x) = ηh separates SP and SQ .
Given a classifier h(x) such that h(x) > ηh for x ∈ SP, suppose that P and Q are not OOD to each
other. Then, there exists x* such that x* ∈ SP and x* ∈ SQ at the same time. However, h(x*)
can not be h(x) > ηh and h(x) ≤ ηh at the same time. Due to the contradiction, given a perfectly
classifier, P and Q should be OOD to each other.	□
B Datasets
In our experiments, we use CIFAR-10, SVHN, and CelebA to demonstrate the proposed methods
and to evaluate OOD detection algorithms, and their details are described in Table 2. In all cases,
we use the official test split as our test datasets. For CIFAR-10 and SVHN, the official train split is
randomly splitted into train (90%) and valid split (10%). For CelebA, we use the official train-valid
split as itis given. Each CelebA image is center-cropped into a 140×140 image and scaled to 32×32
using the bilinear interpolation.
Table 2:	Statistics for datasets.
Dataset	Train	Valid	Test
CIFAR-10(KrizhevSky,2009)	45,000	5,000	10,000
SVHN (Netzer et al., 2011)	65,930	7,327	26,032
CelebA (Liu et al., 2015)	162,770	19,867	19,962
No augmentation is applied to test data. However, random horizontal flip with probability 50%
and uniform dequantization (Theis et al., 2015) are applied to training images in the training of the
binary classifiers and the autoencoders in the adversarial search and adversarial distributions.
C Implementation of Adversarial Distributions
C.1 Implementation of B inary Classifiers and Autoencoders
Binary Classifiers We train binary classifiers h(x) that classify Din from Dout. Two binary clas-
sifier are trained with different Dout’s. For one classifier, Dout is SVHN, and for the other, Dout is
CelebA. Din is CIFAR-10 for both classifiers. Both classifiers have an identical architecture. We
replace the last layer of ResNet-50 pre-trained on ImageNet with a newly initialized binary classifi-
cation layer and fine-tune the whole network using binary cross entropy loss. The pre-trained model
is provided by TorchVision. Training is performed for 100 epochs using Adam with the learning rate
13
Under review as a conference paper at ICLR 2022
of 1 × 10-5, where an epoch is defined with respect to the smaller of Din or Dout. Each mini-batch
consists of 128 samples from each of Din and Dout . During training, validation loss is recorded and
model weights with the best validation loss are used in our experiments.
Autoencoders The decoder of a convolutional autoencoder is used as the generator g(z). The
architecture of the autoencoder is similar to what is used in (Ghosh et al., 2020) and is described in
Table 3. The latent space of this autoencoder is given as Z = [-1, 1]Dz where Dz = {16, 32, 64}.
Table 3:	Autoencoder architecture. ConvN (M) indicates a 2D convolution operation with a N × N
kernel and M output channels. BN denotes batch normalization, and ReLU means the rectified
linear unit activation.
Encoder
Decoder
Conv4(128)-BN-ReLU-
Conv4(256)-BN-ReLU-
Conv4(512)-BN-ReLU-
Conv4(1024)-BN-ReLU-
FC(1024)-BN-ReLU-
FC(32)-Tanh
ConvT8(1024)-ReLU-
ConvT4(512)-ReLU-
ConvT4(256)-ReLU-
ConvT1 (3)-Sigmoid
We train the autoencoder with samples in the union of Din ∪ Dout , where Dout is SVHN or CelebA.
Autoencoders are trained for 500 epochs using Adam with the learning rate of 1 × 10-4 .
First, instead of the original adversarial distribution in Eq. (2), we run MCMC for
distribution p0f (z; g, h), where two barrier functions, H1(z) and H2(z), are added
function.
Pf(Z； g,h) = ,“J Q exP(-E(Z)/T),	Z ∈ RDZ,
f	Z0(f, g, h)
E(Z) = f(g(Z)) +H1(Z) +H2(Z),
H1 (Z) = max(0, h(f (Z)) - ηh),
DZ
H2(Z) = 10 max(0, |Z(d)|-1)2,
d=1
C.2 Implementation of Adversarial Distributions
Here, we describe the method for sampling from the practical version of adversarial distribution
(Eq. (2)).
Enforcing Support Constraint In Eq. (2), the constraint z ∈ Th needs to be enforced, while most
MCMC algorithms for real-valued states assume an unbounded space. We enforce the constraint by
two-staged approach.
an alternative
to the energy
(6)
(7)
(8)
(9)
where z(d) is d-th element of a vector z, and Z0(f, g, h) = exp(-E (z)/T)dz. H1 (z) drives
a Markov chain into {z|f (z) ≤ ηh} by assigning higher energy on z that violates the constraint.
Similarly, H2(z) assigns higher energy for z outside [-1, 1]DZ. Due to H1(z) and H2(z), samples
that comply the constraint z ∈ Th appear more frequently in MCMC. Note that H1 (z) and H2(z)
have non-zero only when the constraint is violated, and the E(z) = f(g(z)) for z ∈ Th.
Second, after running MCMC, we reject any sample that does not obey the constraint z ∈ Th . This
rejection process ensures that the generated samples are from the original adversarial distribution
Eq. (2).
Metropolis-Hasting Acceptance To generate samples from an adversarial distribution (Eq. (2)),
we apply Metropolis-Hastings algorithm (Metropolis et al., 1953) in Z. An initial state z0 is drawn
from an uniform distribution. Given a state Zt at time t, a candidate for the next state Zt+ι is
generated by the proposal distribution. We use a Gaussian distribution which is centered at zt and
14
Under review as a conference paper at ICLR 2022
has the fixed standard deviation of 0.1 as our proposal distribution. A proposed sample is accepted
with the probability of min{1, exp((E(Zt+ι) - E(zt))∕T)}, yielding zt+ι = Zt+ι. Otherwise,
zt+1 = zt . We take the final state of a Markov chain as a generated sample. As stated in the
previous paragraph, the final state is rejected if z 6∈ Th . An accepted sample is mapped to X by
x = f(z).
C.3 Ensemble
An ensemble OOD detector is built by combining predictions from multiple OOD detectors. First,
each OOD detector’s detector score is standardized separately. For standardization, the mean and the
standard deviation of each detector score is computed on the validation in-distribution dataset. The
standardization process is required to account the scale difference between OOD detectors. Then,
the standardized detector score is averaged to yield the detector score for the ensemble detector.
D Implementation of OOD Detectors
D. 1 Implementation of Weak and Strong Detectors
In this section, we describe how OOD detectors are implemented in our experiments.
AE The overall architecture ofAE is identical to what is described in Section C but with a different
dimensionality of the latent space DZ = 128. AE is trained for 500 epochs on in-distribution
training set using Adam optimizer with the learning rate 1 × 10-4. A mini-batch contains 128
samples. A model with the smallest reconstruction error on validation split is selected.
PXCNN PXCNN is implemented based on an open-sourced code repository1 and re-trained
on CIFAR-10. All the model parameters are set to the default values, i.e., nr_resnet=5,
nr_filters=8 0, nr_logistic_mix=10, resnet_nonlinearity='concat_elu'.
An input image is linearly scaled into [-1, 1], and horizontal flipped with the probability of 50%.
The model is trained for 200 epochs where a mini-batch contains 64 samples. We use Adam
optimizer with the learning rate 1 × 10-4 which is decayed by a multiplicative factor of 0.999995
every iteration.
Glow Glow is implemented based on two repositories2 3 and traianed on CIFAR-10. Our Glow
model utilizes a multi-scale architecture with three levels of the latent representations are present,
i.e., L=3. Each level contains 32 flow steps. We use 1×1 invertible convolution and ActNorm with
the scale of 1.0. The model is trained for 500 epochs with the batch size of 64. Adam optimizer with
the learning rate 1×10-4 is utilized. The gradient is clipped so that its norm is no larger than 0.1.
NAE The pre-trained model is downloaded from the official repository4. We use Conv32Big ver-
sion, which showed better performance on CIFAR-10 vs SVHN.
OE Among the available pre-trained models in the official repository5, we select the best per-
forming version, i.e., cifar10_allconv_oe_scratch_epoch_99.pt. We normalize an
input image with means=(0.4914, 0.4822, 0.4465) and std=(0.2471, 0.2435,
0.2615), where each component corresponds to RGB, respectively.
CEDA, ACET, GOOD The pre-trained models are provided by the official repository of GOOD6.
Among multiple versions of GOOD, we use GOOD80, as recommended in the original paper.
1https://github.com/pclucas14/pixel-cnn-pp
2https://github.com/chaiyujin/glow-pytorch
3https://github.com/chrischute/glow
4https://github.com/swyoon/normalized-autoencoders
5https://github.com/hendrycks/outlier-exposure
6https://gitlab.com/Bitterwolf/GOOD
15
Under review as a conference paper at ICLR 2022
MD We implement MD following the official repository7. Based on the pre-trained ResNet-32
provided by the official repository, we train the weights of each layer where Mahalanobis distance
is computed. 1,000 SVHN images are used to determine such weights. Note that using test OOD
dataset during the training is usually not acceptable in a typical OOD detection setting.
SSD We use the official training script to train SSD 8.
CSI We download the unlabelled multi-class CIFAR-10 model from the official repository9 .
D.2 Detectors Not Included in Experiments
We have considered other OOD detectors such as Likelihood Ratio (Ren et al., 2019), Input Com-
Plexity (Serra et al., 2020), Deterministic Uncertainty Quantification (DUQ) (Van Amersfoort et al.,
2020), and Likelihood Regret (Xiao et al., 2020). However, we could not include them in our exper-
iments for the following reasons.
Likelihood Ratio While there is the official Public rePository written in TensorFlow10, we have
failed to reProduce the result in PyTorch.
Input Complexity We confirm that the log-density estimates from generative models are corre-
lated to the bits after comPression, but fail to achieve AUC score higher than 0.9 in CIFAR-10 vs
SVHN setting. For generative models, PXCNN and Glow are tested, and for image comPression al-
gorithms, PNG, JPEG2000, and FLIF are tried. Our generative models are imPlemented as Section
C. For PNG and JPEG2000, we use OPenCV11, and for FLIF, we use imageio-flif12. No combination
of a generative model and a comPression algorithm results in AUC higher than 0.9.
DUQ Investigating the official rePository 13, we find that different normalization transforms
are aPPlied to in-distribution (CIFAR-10) and test OOD dataset (SVHN). CIFAR-10 is normal-
ized with Parameters mean=(0.4914, 0.4822, 0.4465) and std=(0.2023, 0.1994,
0.2010), whereas SVHN is normalized using mean=(0.5, 0.5, 0.5) and std=(0.5,
0.5, 0.5). APPlying different transformations makes classification between CIFAR-10 and
SVHN easier. When we modify the code so that an identical transformation is aPPlied to both
datasets, AUC score droPs below 0.9. Therefore, we do not include DUQ in our exPeriments.
Likelihood Regret The method is very slow, Particularly because it does not suPPort batch Pro-
cessing, i.e., only one samPle can be Processed at a time. We decide that the method is too slow to
aPPly the adversarial distribution aPProach.
7https://github.com/pokaxpoka/deep_Mahalanobis_detector
8 https://github.com/inspire- group/SSD
9https://github.com/alinlab/CSI
10https://github.com/google-research/google-research/tree/master/
genomics_ood/images_ood
11https://opencv.org/
12https://codeberg.org/monilophyta/imageio-flif
13 https://github.com/y0ast/deterministic- uncertainty- quantification
16
Under review as a conference paper at ICLR 2022
E Extended Experimental Results
E.1 B inary Classification
Here, we show the effect of test-time augmentation and report the zero-inlier threshold. Test-time
augmentation dramatically increases the zero-inlier threshold, meaning that the classifiers become
more robust.
Binary classifier logit values (No Aug)
0.2
0.0
0
logit
20
Binary classifier logit values (TestTimeAug)
Figure 6: Distribution of the logit values in binary classification. The zero-inlier threshold, the
smallest logit of all inliers are denoted as dotted lines.
E.2 Runtime and Computing Environment
Table 4 shows time required to perform MCMC
sampling and for each OOD detectors. For
MCMC, time required for running 10,000 steps
of MH update is shown. The main factor that
determines the runtime is the inference time of
an OOD detector. Inference of all OOD detec-
tor is performed on a single Tesla V100 GPU.
Table 4: Time required for performing a single run
ofMCMC _______________________
Detector MCMC
Glow	3hr 15min
PXCNN	10min
AE	1hr 10min
NAE	50min
GOOD	22min
ACET	22min
CEDA	22min
SSD	37min
MD	1hr 30min
OE	22min
CSI	20hr
17
Under review as a conference paper at ICLR 2022
F Additional Samples from Adversarial Distributions
From Figure 7 to Figure 12 present samples from adversarial distributions of each OOD detectors.
PXCNN D乙=64
ʃ WBlPVl
WHTc 电 J*⅛⅛e
Figure 7: Samples from adversarial distributions.(1/6)
18
Under review as a conference paper at ICLR 2022
AE Dz= 64
"EEKB 睇 T
〉•好A®#L
NAE Dz= 16
3：9皿［油小啊耐吧耕&
*⅛KB03≡*0W国♦
NAE Dz =32
IM耀限酬ftl刎®胆睡小HE
务的9屈，篇&可出 a
Figure 8: Samples from adversarial distributions.(2/6)
19
Under review as a conference paper at ICLR 2022
GOOD Dz= 16
Figure 9: Samples from adversarial distributions.(3/6)
20
Under review as a conference paper at ICLR 2022
Figure 10: Samples from adversarial distributions.(4/6)
21
Under review as a conference paper at ICLR 2022
MD Dz= 16


OE DN = 32
'∣ t
Vt

OE Dz = 64
Figure 11: Samples from adversarial distributions.(5/6)
22
Under review as a conference paper at ICLR 2022
Figure 12: Samples from adversarial distributions.(6/6)
23