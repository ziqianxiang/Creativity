SoftHebb: Bayesian inference in unsupervised
Hebbian soft winner-take-all networks
Anonymous authors
Paper under double-blind review
Ab stract
State-of-the-art artificial neural networks (ANNs) require labelled data or feedback
between layers, are often biologically implausible, and are vulnerable to adversarial
attacks that humans are not susceptible to. On the other hand, Hebbian learning in
winner-take-all (WTA) networks is unsupervised, feed-forward, and biologically
plausible. However, a modern objective optimization theory for WTA networks has
been missing, except under very limiting assumptions. Here we derive formally
such a theory, based on biologically plausible but generic ANN elements. Through
Hebbian learning, network parameters maintain a Bayesian generative model of
the data. There is no supervisory loss function, but the network does minimize
cross-entropy between its activations and the input distribution. The key is a
"soft" WTA where there is no absolute "hard" winner neuron, and a specific
type of Hebbian-like plasticity of weights and biases. We confirm our theory in
practice, where, in handwritten digit (MNIST) recognition, our Hebbian algorithm,
SoftHebb, minimizes cross-entropy without having access to it, and outperforms
the more frequently used, hard-WTA-based method. Strikingly, it even outperforms
supervised end-to-end backpropagation, under certain conditions. Specifically, in a
two-layered network, SoftHebb outperforms backpropagation when the training
dataset is only presented once, when the testing data is noisy, and under gradient-
based adversarial attacks. Notably, adversarial attacks that maximally confuse
SoftHebb are also confusing to the human eye. Finally, the model can generate
interpolations of objects from its input distribution. All in all, SoftHebb extends
Hebbian WTA theory with modern machine learning tools, thus making these
networks relevant to pertinent issues in deep learning.
1	Introduction
State-of-the-art (SOTA) artificial neural networks (ANNs) achieve impressive results in a variety of
machine intelligence tasks (Sejnowski, 2020). However, they largely rely on mechanisms that diverge
from the original inspiration from biological neural networks (Bengio et al., 2015; Illing et al., 2019).
As a result, only a small part of this prolific field also contributes to computational neuroscience.
In fact, this biological implausibility is also an important issue for machine intelligence. For their
impressive performance, ANNs trade off other desired properties, which are present in biological
systems. For example, ANN training often demands very large and labelled datasets. When labels
are unavailable, self-supervised learning schemes exist, where supervisory error signals generated by
the network itself are exploited and backpropagated from the output towards the input to update the
network’s parameters (Goodfellow et al., 2014; Devlin et al., 2018; Chen et al., 2020). However, this
global propagation of signals in deep networks introduces another limitation. Namely, it prevents
the implementation of efficient distributed computing hardware that would be based on only local
signals from neighbouring physical nodes in the network, and is in contrast to local synaptic plasticity
rules that partly govern biological learning. Several pieces of work have been addressing parts of
the biological implausibility and hardware-inefficiency of backpropagation in ANNs (Bengio et al.,
2015; Lillicrap et al., 2016; Guerguiev et al., 2017; Pfeiffer & Pfeil, 2018; Illing et al., 2019; Pogodin
& Latham, 2020; Millidge et al., 2020; Pogodin et al., 2021). such as the need for exactly symmetric
forward and backward weights or the waiting time caused by the network’s forward-backward pass
between two training updates in a layer (weight transport and update-locking problems). Recently,
an approximation to backpropagation that is mostly Hebbian, i.e. relies on mostly pre- and post-
1
synaptic activity of each synapse, has been achieved by reducing the global error requirements to
1-bit information (Pogodin & Latham, 2020). Two schemes that further localize the signal that is
required for a weight update are Equilibrium Propagation (Scellier & Bengio, 2017) and Predictive
Coding (Millidge et al., 2020). Both methods approximate backpropagation through Hebbian-like
learning, by delegating the global aspect of the computation, from a global error signal, to a global
convergence of the network state to an equilibrium. This equilibrium is reached through several
iterative steps of feed-forward and feed-back communication throughout the network, before the
ultimate weight update by one training example. The biological plausibility and hardware-efficiency
of this added iterative process of signal propagation are open questions that begin to be addressed
(Ernoult et al., 2020).
Moreover, learning through backpropagation, and presumably also its approximations, has another
indication of biological implausibility, which also significantly limits ANN applicability. Namely,
it produces networks that are confused by small adversarial perturbations of the input, which are
imperceptible by humans. It has recently been proposed that a defence strategy of "deflection" of
adversarial attacks may be the ultimate solution to that problem (Qin et al., 2020). Through this
strategy, to cause confusion in the network’s inferred class, the adversary is forced to generate such
a changed input that really belongs to the distribution of a different input class. Intuitively, but
also strictly by definition, this deflection is achieved if a human assigns to the perturbed input the
same label that the network does. Deflection of adversarial attacks in ANNs has been demonstrated
by an elaborate scheme that is based on detecting the attacks (Qin et al., 2020). However, the
human ability to deflect adversarial perturbations likely does not rely on detecting them, but rather on
effectively ignoring them, making the deflecting type of robustness an emergent property of biological
computation rather than a defence mechanism. The biological principles that underlie this property
of robustness are unclear, but it might emerge from the distinct algorithms that govern learning in the
brain.
Therefore, what is missing is a biologically plausible model that can learn from fewer data-points,
without labels, through local plasticity, and without feedback from distant layers. This model could
then be tested for emergent adversarial robustness. A good candidate category of biological networks
and learning algorithms is that of competitive learning. Neurons that compete for their activation
through lateral inhibition are a common connectivity pattern in the superficial layers of the cerebral
cortex (Douglas & Martin, 2004; Binzegger et al., 2004). This pattern is described as winner-take-all
(WTA), because competition suppresses activity of weakly activated neurons, and emphasizes strong
ones. Combined with Hebbian-like plasticity rules, WTA connectivity gives rise to competitive-
learning algorithms. These networks and learning schemes have been long studied (Von der Malsburg,
1973) and a large literature based on simulations and analyses describes their functional properties. A
WTA neuronal layer, depending on its specifics, can restore missing input signals (Rutishauser et al.,
2011; Diehl & Cook, 2016), perform decision making i.e. winner selection (Hahnloser et al., 1999;
Maass, 2000; Rutishauser et al., 2011), and generate oscillations such as those that underlie brain
rhythms (Cannon et al., 2014). Perhaps more importantly, its neurons can learn to become selective
to different input patterns, such as orientation of visual bars in models of the primary visual cortex
(Von der Malsburg, 1973), MNIST handwritten digits (Nessler et al., 2013; Diehl & Cook, 2015;
Krotov & Hopfield, 2019), CIFAR10 objects (Krotov & Hopfield, 2019), spatiotemporal spiking
patterns (Nessler et al., 2013), and can adapt dynamically to model changing objects (Moraitis et al.,
2020). The WTA model is indeed biologically plausible, Hebbian plasticity is local, and learning
is input-driven, relying on only feed-forward communication of neurons - properties that Seem to
address several of the limitations of ANNs. However, the model’s applicability is limited to simple
tasks. That is partly because the related theoretical literature remains surprisingly unsettled, despite
its long history, and the strong and productive community interest (Sanger, 1989; Foldidk & Fdilr,
1989; Foldiak, 1990; Linsker, 1992; Olshausen & Field, 1996; Bell & Sejnowski, 1995; Olshausen
& Field, 1997; Lee et al., 1999; Nessler et al., 2013; Pehlevan & Chklovskii, 2014; Hu et al., 2014;
Pehlevan & Chklovskii, 2015; Pehlevan et al., 2017; Isomura & Toyoizumi, 2018).
Nessler et al. (2009; 2013) described a very related theory but for a model that is largely incompatible
with ANNs and thus less practical. It uses spiking and stochastic neurons, input has to be discretized,
and each input feature must be encoded through multiple binary neurons. Moreover, it was only
proven for neurons with an exponential activation function. It remains therefore unclear which
specific plasticity rule and structure could optimize an ANN WTA for Bayesian inference. It is also
unclear how to minimize a common loss function such as cross-entropy despite unsupervised learning,
2
and how a WTA could represent varying families of probability distributions. In summary, on the
theoretical side, an algorithm that is simultaneously normative, based on WTA networks and Hebbian
unsupervised plasticity, performs Bayesian inference, and, importantly, is composed of conventional,
i.e. non-spiking, ANN elements and is rigorously linked to modern ANN tools such as cross-entropy
loss, would be an important advance but has been missing. On the practical side, evidence that
Hebbian WTA networks could be useful for presently pertinent issues of modern ANNs such as
adversarial robustness, generation of synthetic images, or faster learning, has remained limited. Here
we aim to fill these gaps. Recently, when WTA networks were studied in a theoretical framework
compatible with conventional machine learning (ML), but in the context of short-term as opposed to
long-term Hebbian plasticity, it resulted in surprising practical advantages over supervised ANNs
(Moraitis et al., 2020). A similar theoretical approach could also reveal unknown advantages of
long-term Hebbian plasticity in WTA networks. In addition, it could provide insights into how a WTA
microcircuit could participate in larger-scale computation by deeper cortical or artificial networks.
Here we construct "SoftHebb", a biologically plausible WTA model that is based on standard rate-
based neurons as in ANNs, can accommodate various activation functions, and learns without labels,
using local plasticity and only feed-forward communication, i.e. the properties we seek in an ANN.
Importantly, it is equipped with a simple normalization of the layer’s activations, and an optional
temperature-scaling mechanism (Hinton et al., 2015), producing a soft WTA instead of selecting a
single "hard" winner neuron. This allows us to prove formally that a SoftHebb layer is a generative
mixture model that objectively minimizes its Kullback-Leibler (KL) divergence from the input
distribution through Bayesian inference, thus providing a new formal ML-theoretic perspective of
these networks. We complement our main results, which are theoretical, with experiments that are
small-scale but produce intriguing results. As a generative model, SoftHebb has a broader scope than
classification, but we test it on image classification tasks. Surprisingly, in addition to overcoming
several inefficiencies of backpropagation, the unsupervised WTA model also outperforms a supervised
two-layer perceptron in several aspects: learning speed and accuracy in the first presentation of the
training dataset, robustness to noisy data and to one of the strongest white-box adversarial attacks, i.e.
projected gradient descent (PGD) (Madry et al., 2017), and without any explicit defence. Interestingly,
the SoftHebb model also exhibits inherent properties of deflection (Qin et al., 2020) of the adversarial
attacks, and generates object interpolations.
2	Theory
A supporting diagram summarising the theoretical and neural model, and a succinct description of
the learning algorithm are provided in the beginning of Appendix A.
Definition 2.1 (The input assumptions). Each observation j x ∈ Rn is generated by a hidden
"cause" jC from a finite set of K possible such causes: j C ∈ {Ck , ∀k ≤ K ∈ N}. Therefore, the
data is generated by a mixture of the probability distributions attributed to each of the K classes Ck:
K
p(x) = Xp(x|Ck)P(Ck).	(1)
k=1
In addition, the dimensions of x, xi are conditionally independent from each other, i.e.
p(x) = in=1 p(xi). The number K of the true causes or classes of the data is assumed to be known.
The term "cause" is used here in the sense of causal inference. It is important to emphasize that
the true cause of each input is hidden, i.e. not known. In the case of a labelled dataset, labels may
correspond to causes, and the labels are deleted before presenting the training data to the model. We
choose a mixture model that corresponds to the data assumptions but is also interpretable in neural
terms (Paragraph 2.4):
Definition 2.2 (The generative probabilistic mixture model). We consider a mixture model distribu-
tion q: q(x) = PkK=1 q (x|Ck) Q(Ck), approximating the data distribution p. We choose specifically
a mixture of exponentials and we parametrize Q(Ck ; w0k) also as an exponential, specifically:
xi
q(xi∣Ck； Wik) = ewik 时,∀k	(2)
Q(Ck;w0k) =ew0k, ∀k.	(3)
In addition, the parameter vectors are subject to the normalization constraints: ||wk || = 1, ∀k, and
PkK=1 ew0k = 1.
3
The model we have chosen is a reasonable choice because it factorizes similarly to the data of
Definition 2.1:
n
qk := q(x∣Ck； Wk) = Y q(xi∖Ck； Wik) = ePi=1 wiknx1| = euk,	(4)
i=1
where Uk = ∣∣wwk∙xχ∣∣, i.e. the cosine similarity of the two vectors. A similar probabilistic model
was used in related previous theoretical work Nessler et al. (2009; 2013); Moraitis et al. (2020), but
for different data assumptions, and with certain further constraints to the model. Namely, (Nessler
et al., 2009; 2013) considered data that was binary, and created by a population code, while the
model was stochastic. These works provide the foundation of our derivation, but here we consider the
more generic scenario where data are continuous-valued and input directly into the model, which is
deterministic and, as we will show, more compatible with standard ANNs. In Moraitis et al. (2020),
data had particular short-term temporal dependencies, whereas here we consider the distinct case
of independent and identically distributed (i.i.d.) input samples. The Bayes-optimal parameters of
a model mixture of exponentials can be found analytically as functions of the input distribution’s
parameters, and the model is equivalent to a soft winner-take-all neural network (Moraitis et al.,
2020). After describing this, we will prove here that Hebbian plasticity of synapses combined with
local plasticity of the neuronal biases sets the parameters to their optimal values.
Theorem 2.3 (The optimal parameters of the model). The parameters that minimize the KL
divergence of such a mixture model from the data are, for every k,
optw0k = ln P (Ck)
and optwk
optwk	=	μpk (X)
∖∖ optwk∖∖	∖∖μpk(X) ∖∖
(5)
(6)
where optwk = C ∙ μpk (x), C ∈ R+, μpk (x) is the mean ofthe distributionPk, andPk := p(x∖Ck).
In other words, the optimal parameter vector of each component k in this mixture is proportional to the
mean of the corresponding component of the input distribution, i.e. it is a centroid of the component.
In addition, the optimal parameter of the model’s prior Q(Ck) is the logarithm of the corresponding
component’s prior probability. This Theorem’s proof was provided in the supplementary material
of Moraitis et al. (2020), but for completeness we also provide it in our Appendix. These centroids
and priors of the input’s component distributions, as well as the method of their estimation, however,
are different for different input assumptions, and we will derive a learning rule that provably sets
the parameters to their Maximum Likelihood Estimate for the inputs addressed here. The learning
rule is a Hebbian type of synaptic plasticity combined with a plasticity for neuronal biases. Before
providing the rule and the related proof, we will describe how our mixture model is equivalent to a
WTA neural network.
2.4	Equivalence of the probabilistic model to a WTA neural network
The cosine similarity between the input vector and each centroid’s parameters underpins the model
(Eq. 4). This similarity is precisely computed by a linear neuron that receives normalized inputs
x* := ∣pX^, and that normalizes its vector of synaptic weights: wk := 1^. Specifically, the neuron's
summed weighted input Uk = w* ∙ Xk then determines the cosine similarity of an input sample to the
weight vector, thus computing the likelihood function of each component of the input mixture (Eq. 2).
It should be noted that even though Uk depends on the weights of all input synapses, the weight values
of other synapses do not need to be known to each updated synapse. Therefore, in the SoftHebb
plasticity rule that we will present (Eq. 8), the term Uk is a local, postsynaptic variable that does not
undermine the locality of the plasticity. The bias term of each neuron can store the parameter w0k of
the prior Q(Ck; w0k). Based on these, it can also be shown that a set of K such neurons can actually
compute the Bayesian posterior, if the neurons are connected in a configuration that implements
softmax. Softmax has a biologically-plausible implementation through lateral inhibition (divisive
normalization) between neurons (Nessler et al., 2009; 2013; Moraitis et al., 2020). Specifically, based
on the model of Definition 2.2, the posterior probability is
euk +w0k
Q(Ck∖χ) = B—.	⑺
lK=1 eul+w0l
But in the neural description, Uk + w0k is the activation of the k-th linear neuron. That is, Eq. 7
shows that the result of Bayesian inference of the hidden cause from the input Q(Ck∖X) is found
4
by a softmax operation on the linear neural activations. In this equivalence, we will be using
yk := Q(Ck|x; w) to symbolize the softmax output of the k-th neuron, i.e. the output after the WTA
operation, interchangeably with Q(Ck|x). It can be seen in Eq. 7 that the probabilistic model has
one more, alternative, but equivalent neural interpretation. Specifically, Q(Ck|x) can be described as
the output of a neuron with exponential activation function (numerator in Eq. 7) that is normalized
by its layer’s total output (denominator). This is equally accurate, and more directly analogous to
the biological description (Nessler et al., 2009; 2013; Moraitis et al., 2020). This shows that the
exponential activation of each individual neuron k directly equals the k-th exponential component
distribution of the generative mixture model (Eq. 4). Therefore, the softmax-configured linear
neurons, or equivalently, the normalized exponential neurons, fully implement the generative model
of Definition 2.2, and also infer the Bayesian posterior probability given an input and the model
parameters. However, the problem of calculating the model’s parameters from data samples is a
difficult one, if the input distribution’s parameters are unknown. In the next sections we will show that
this neural network can find these optimal parameters through Bayesian inference, in an unsupervised
and on-line manner, based on only local Hebbian plasticity.
2.5	A Hebbian rule that optimizes the weights
Several Hebbian-like rules exist and have been combined with WTA networks. For example, in
the case of stochastic binary neurons and binary population-coded inputs, it has been shown that
weight updates with an exponential weight-dependence find the optimal weights (Nessler et al.,
2009; 2013). Oja’s rule is another candidate (Oja, 1982). An individual linear neuron equipped
with this learning rule finds the first principal component of the input data (Oja, 1982). A variation
of Oja’s rule combined with hard-WTA networks and additional mechanisms has achieved good
experimental results performance on classification tasks (Krotov & Hopfield, 2019), but lacks the
theoretical underpinning that we aim for. Here we propose a Hebbian-like rule for which we will
show it optimizes the soft WTA’s generative model. The rule is similar to Oja’s rule, but considers,
for each neuron k, both its linear weighted summation of the inputs uk, and its nonlinear output of
the WTA yk:
∆w((SoftHebbb ：= η ∙ yk . (χi — Uk Wik) ,	(8)
where wik is the synaptic weight from the i-th input to the k-th neuron, and η is the learning rate
hyperparameter. As can be seen, all involved variables are local to the synapse, i.e. only indices i and
k are relevant. No signals from distant layers, from non-perisynaptic neurons, or from other synapses
are involved. By solving the equation E[∆wik] = 0 where E[∙] is the expected value over the input
distribution, we can show that, with this rule, there exists a stable equilibrium value of the weights,
and this equilibrium value is an optimal value according to Theorem 2.3:
Theorem 2.5. The equilibrium weights of the SoftHebb synaptic plasticity rule are
SoftHebb
wik
C ∙ μpk (Xi)
optwik, where c
1
llμpk (X) ||
(9)
The proof is provided in the Appendix. Therefore, our update rule (Eq. 8) optimizes the weights of
the neural network.
2.6	Local learning of neuronal biases as Bayesian priors
For the complete optimization of the model, the neuronal biases w0k must also be optimized to satisfy
Eq. 5, i.e. to optimize the Bayesian prior belief for the probability distribution over the K input
causes. We define the following rate-based rule inspired from the spike-based bias rule of (Nessler
et al., 2013):
∆w0SkoftHebb = ηe-w0k (yk -ew0k) .	(10)
With the same technique we used for Theorem 2.5, we also provide proof in the Appendix that the
equilibrium of the bias with this rule matches the optimal value optw0k = ln P(Ck) of Theorem 2.3:
Theorem 2.6. The equilibrium biases of the SoftHebb bias learning rule are
w0SkoftHebb = ln P(Ck) = optw0k.	(11)
2.7	Alternate activation functions and relation to Hard WTA
The model of Definition 2.2 uses for each component p(x|Ck) an exponential probability distribution
with a base of Euler’s e, equivalent to a model using similarly exponential neurons (Subsection 2.4).
5
A	100 epochs	B	IePoCh
(％) AUe.JrDOV
8
8
,
9
1 Layer2 Layers	ILayer2 Layers 2 Layers SlmUltgreedy SlmUItgreedy slmult pretr.L2 pretr.L1
Hard WTA SoftHebb Backprop Hard WTA SoftHebb Backprop
SoftHebb
一 Hand WTA
Backprop
C
Running loss
(moving average)
0	30000	60000
Raining examples
D Rl features
0	30000	60000
TTaInIng examples
Figure 1: Performance of SoftHebb on MNIST compared to hard-WTA and backpropagation.
Depending on the task, different probability distribution shapes, i.e. different neuronal activation
functions, may be better models. This is compatible with our theory (see Appendix B). Firstly,
the base of the exponential activation function can be chosen differently, resulting in a softmax
function with a different base, such that Eq. 7 becomes more generally Q(Ck |x)
buk+w0k
PK=1 bul+w0l *
This is equivalent to Temperature Scaling (Hinton et al., 2015), a mechanism that also maintains
the probabilistic interpretation of the output. It can also be implemented by a normalized layer of
exponential neurons, and are compatible with our theoretical derivations and the optimization by the
plasticity rule of Eq. 8. This allows us to integrate the hard WTA into the SoftHebb framework, as a
special case with an infinite base. Therefore, the hard WlA, if used with the plasticity rule that we
derived, is expected to have some similar properties to a soft WTA implementation. Moreover, we
show in the Appendix that soft WTA models can be constructed by rectified linear units (ReLU) or in
general by neurons with any non-negative monotonically increasing activation function, and their
weights are also optimized by the same plasticity rule.
2.8 POST-HOC CROSS-ENTROPY
It is not obvious how to measure the loss of the WTA network during the learning process, since the
ground truth for causes C may not match the labels, or even their number. For example, in reality, the
cause C of each MNIST example in the sense implied by causal inference is not the digit cause itself,
but a combination of a single digit cause D, which is the MNIST label, with one of many handwriting
styles S. In the Appendix C, we derive a method for measuring the loss minimization during training
in spite of this mismatch. The method,s steps can be summarised as (a) unsupervised training of
SoftHebb, then (b) training a supervised classifier on top, and finally (c1) repeating the training of
SoftHebb with the same initial weights and ordering of the training inputs, while (c2) measuring the
trained classifier,s loss. In this way, we can observe the cross-entropy loss H labels of SoftHebb while
it is being minimized, and infer that HCauses is also minimized (Eq. 72). We call this the post-hoc
cross-entropy method, and we have used it in our experiments (Section 3.2 and Fig. 1 C and D) to
evaluate the learning process in a theoretically sound manner.
3	Experiments
3.1	MNIST TOP-ACCURACY BENCHMARKS
We implemented the theoretical SoftHebb model in simulations and tested it in the task of learning
to classify MNIST handwritten digits. The network received the MNIST frames normalized by
their Euclidean norm, while the plasticity rule that we derived updated its weights and biases in an
unsupervised manner. We used K = 2000 neurons. First we trained the network for 100 epochs, i.e.
randomly ordered presentations of the 60000 training digits. Each training experiment was repeated
five times with varying random initializations and input order. We will report the mean and standard
deviation of accuracies. Inference of the input labels by the WTA network of 2000 neurons was
performed in two different ways. The first approach is single-layer, where, after training the network,
we assigned a label to each of the 2000 neurons, in a standard approach that is used in unsupervised
clustering. Namely, for each neuron, we found the label of the training set that makes it win the WTA
6
competition most often. In this single-layer approach, this is the only time when labels were used,
and at no point were weights updated using labels. The second approach was two-layer and based on
supervised training of a perceptron classifier on top of the WTA layer. The classifier layer was trained
with the Adam optimizer and cross-entropy loss for 100 epochs, while the previously-trained WTA
parameters were frozen. SoftHebb achieved an accuracy of (96.31 ± 0.06)% and (97.80 ± 0.02)%
in its 1- and 2-layer form respectively. To test the strengths of the soft-WTA approach combined with
training the priors through biases, which makes the network Bayesian, we also trained the weights of
a hard-WTA network. The SoftHebb model slightly outperformed the hard WTA (Fig. 1A), especially
in the 1-layer case where the supervised 2nd layer cannot compensate the drop. However, SoftHebb’s
accuracy is significantly lower than a multi-layer perceptron (MLP) with one hidden layer of also
2000 neurons that is trained exhaustively ((98.65 ± 0.06)%). This is not surprising, due to end-to-end
training, supervision, and the MLP being a discriminative model as opposed to a generative model
merely applied to a classification task, as SoftHebb is. If the Bayesian and generative aspects that
follow from our theory were not required, several mechanisms exist to enhance the discriminative
power of WTA networks (Krotov & Hopfield, 2019), and even a random projection layer instead
of a trained WTA performs well (Illing et al., 2019). The generative approach however has its own
advantages even for a discriminative task, and we will show some of these here.
3.2	Cross-entropy minimization. Speed advantage of SoftHebb.
Next, we tested SoftHebb’s speed and efficiency by comparing it to other models during the first
training epoch. In the common, "greedy" training of such networks, layer L+1 is trained only after
layer L is trained on the full dataset and its weights frozen. We trained in this manner a second
layer as a supervised classifier for 100 epochs. SoftHebb again slightly outperformed the Hard WTA
showing that it extracts superior features. However, it was outperformed by 1-epoch backpropagation
pretraining that was followed by 100 epochs of training the 2nd layer only (Fig. 1B, light-coloured
bars). Then, we demonstrated that in a truly single-epoch scenario, without longer training for the
second layer, SoftHebb further outperforms HardWTA, and, strikingly, it even outperforms end-to-end
(e2e) backpropagation in accuracy (Fig. 1B, bars label "simult."). In fact, it even outperforms a
backprop-trained network that is pretrained for 100 epochs, before its 1st layer is reset and re-trained
end-to-end for 1 epoch (Fig. 1B, "pretr. L2"). In this experiment, the Hebbian networks had an
additional important advantage. By using the delta rule for the 2nd layer, each individual training
example updated both layers. In contrast to backpropagation, this simultaneous method does not
suffer from the update-locking problem, i.e. the first layer can learn from the next example before the
current input is even processed by the higher layer, let alone backpropagated. Moreover, we measured
the post-hoc loss. Consistently with the first-epoch accuracy results, SoftHebb is faster than hard
WTA and both are faster than backpropagation, which is remarkable considering the absence of input
labels. Also, it validates our theory that SoftHebb minimizes cross-entropy. As a further insight
into the differences between SoftHebb and hard-WTA learning, we measured throughout learning
the number of learned features that lie on a hypersphere with a radius of 1 ± 0.01 (R1 features),
according to the Euclidean norm of the weight vectors. The SoftHebb learning algorithm converges
to such a normalization in theory (end of Appendix A, Theorem A.2) , and Fig. 1D validates that it
does, but also that it does so faster than the hard WTA implementation, demonstrating SoftHebb’s
superiority in unsupervised representation learning, irrespective of discriminative ability. Hebbian
learning compared to backpropagation has not generally been considered superior for its accuracy,
but for other potential benefits. Here we show evidence that, for small problems demanding fast
learning, SoftHebb may be superior to backpropagation even in terms of accuracy, in addition to its
biological plausibility and efficiency.
3.3	Robustness to noise and adversarial attacks - Generative adv. properties
Based on the Bayesian, generative, and purely input-driven learning nature of the algorithm, we
hypothesized that SoftHebb may be more robust to input perturbations. Indeed, we tested the trained
SoftHebb and MLP models for robustness, and found that SoftHebb is significantly more robust
than the backprop-trained MLP, both to added Gaussian noise and to PGD adversarial attacks (see
Fig. 2). PGD (Madry et al., 2017) produces perturbations in a direction that maximizes the loss of
each targeted network, and in size controlled by a parameter . We also attacked a model where
the Hebbian layer was trained as hard WTA but in forward propagation to the supervised classifier
head and for inference, softmax as in SoftHebb was used. Hard- and soft-trained WTA are almost
identically robust to these perturbations. This may be an expression of the fact that the hard-WTA
model is essentially a special case of the SoftHebb framework (Section 2.7). Strikingly, the Hebbian
7
A
Gaussian noise
Standard deviation
ε( Ub norm)
Figure 2: Noise and adversarial attack robustness of SoftHebb and of backpropagation-trained MLP
on MNIST and Fashion-MNIST. The insets show one example from the testing set and its perturbed
versions, for increasing perturbations. (A) SoftHebb is highly robust to noise. (B) MLP’s MNIST
accuracy drops to ~60% by hardly perceptible perturbations ( = 16/255), while SoftHebb requires
visually noticeable perturbations ( = 64/255) for similar drop in performance. At that degree of
perturbation, the MLP has already dropped to zero. SoftHebb deflects the attack: it forces the attacker
to produce examples of truly different classes - the original digit "4" is perturbed to look like a "0"
(see also Fig. 3). The hard-WTA curves (purple) are almost identical to SoftHebb’s.
WTA model has a visible tendency to deflect the attacks, i.e. its most confusing examples actually
belong to a perceptually different class (Fig. 2B and 3). This effectively nullifies the attack and was
previously shown in elaborate SOTA adversarial-defence models (Qin et al., 2020). The attack’s
parameters were tuned systematically (Appendix E). The pair of the adversarial attacker with the
generative SoftHebb model essentially composes a generative adversarial network (GAN), even
though the term is usually reserved for pairs trained in tandem (Goodfellow et al., 2014; Creswell
et al., 2018). As a result, the model could inherit certain properties of GANs. It can be seen that it
is able to generate interpolations between input classes (Fig. 3). The parameter of the adversarial
attack can control the balance between the interpolated objects. Similar functionality has existed in
the realm of GANs (Radford et al., 2015), autoencoders (Berthelot et al., 2018), and other deep neural
networks (Bojanowski et al., 2017), but was not known for simple biologically-plausible models.
3.4	Extensibility of SoftHebb: F-MNIST, CIFAR- 1 0, conv-SoftHebb
Finally, we performed preliminary tests on two more difficult datasets, namely Fashion-MNIST (Xiao
et al., 2017), which contains grey-scale images of clothing products, and CIFAR-10 (Krizhevsky et al.,
2009), which contains RGB images of animals and vehicles. We did not tune the Hebbian networks’
hyper-parameters extensively, so accuracies on these tasks are not definitive but do give a good indi-
cation. On F-MNIST, the SoftHebb model achieved a top accuracy of 87.46% whereas a hard WTA
reached a similar accuracy of 87.49%. A supervised MLP of the same size achieved a test accuracy
of 90.55%. SoftHebb’s generative interpolations (Fig. 3B) are reconfirmed on the F-MNIST dataset,
as is its robustness to attacks, whereas, with very small adversarial perturbations, the MLP drops to an
accuracy lower than the SoftHebb model (dashed lines in Fig. 2). On CIFAR-10’s preliminary results,
the hard WTA and SoftHebb achieved an accuracy of 49.78% and 50.27% respectively. In every
tested dataset, it became clear that SoftHebb learns faster than either backpropagation or a hard WTA,
by observing the loss and the learned features as in Fig. 1 C & D. The fully-connected SoftHebb
layer is applicable on MNIST because the data classes are well-clustered directly in the feature-space
of pixels. That is, SoftHebb’s probabilistic model’s assumptions (Def. 2.1) are quite valid for
this feature space, and increasing the number of neurons for discovering more refined sub-clusters
does help. However, for more complex datasets, this approach alone has diminishing returns and
multilayer networks will be needed. Towards this, we implemented a first version of a convolutional
SoftHebb, with an added supervised classifier. In these early results, conv-SoftHebb achieved 98.63%
on MNIST, and 60.30% on CIFAR-10. This confirms that conv-SoftHebb is functional, but we did
not systematically optimize its accuracy or explore its other strengths. This feasibility is evidence
that SoftHebb’s strengths may soon also apply to larger networks and datasets.
8
B
NΛ∖>*?Oo3367s
v<⅛^w 犷-5mws7夕
rΛ^^κ^3%^s
⅛∙『夕◎«5■居ɪ¥ f
/19，/茅骂Ec*3
/，夕。/IWXOVS
∕i∕∕Λ^Z5¾x7^/
∕√∙√∕λ^z50ox7^/
/∕√∕Λ^Z¾ooσoo∕^z
//夕，/53803
ε (L∞ norm)
ε (L∞ norm)
Figure 3:	Examples generated by the adversarial pair PGD attacker/SoftHebb model. SoftHebb’s
inherent tendency to deflect the attack towards truly different classes is visible. This tendency can be
repurposed to generate interpolations between different classes of the data distribution, a generative
property previously unknown for such simple networks.
4 Discussion
We have described SoftHebb, a highly biologically plausible neural algorithm that is founded
on a Bayesian ML-theoretic framework. The model consists of elements fully compatible with
conventional ANNs. It was previously not known which plasticity rule should be used to learn a
Bayesian generative model of the input distribution in ANN WTA networks. Moreover, we showed
that Hard WTA networks and neurons with other activation functions can be described within the
same framework as variations of the probabilistic model. This theory could provide a new foundation
for normative Hebbian ANN designs with practical significance. For example, SoftHebb’s properties
are sought-after by efficient neuromorphic learning chips. It is unsupervised, local, and requires
no error or other feedback currents from upper layers, thus solving hardware-inefficiencies and
bio-implausibilities of backpropagation such as weight-transport and update-locking. Surprisingly,
it surpasses backpropagation even in accuracy, when training time and network size are limited.
In a demonstration that goes beyond the common greedy-training approach to such networks, we
achieved update-unlocked operation in practice, by updating the first layer before the input’s full
processing by the next layer. It is intriguing that, through its biological plausibility, emerge properties
commonly associated with biological intelligence, such as speed of learning, and robustness to noise
and adversarial attacks. Significant robustness emerges without specialized defences. Furthermore,
SoftHebb tends to not merely be robust to attacks, but actually deflect them as specialized SOTA
defences aim to do.
Here, we explored SoftHebb’s applicability on several datasets. We measured its accuracy on MNIST,
Fashion-MNIST, and CIFAR-10 in preliminary results, and we reported a functional convolutional
SoftHebb network that improves accuracy on the significantly harder dataset of CIFAR-10. The con-
volutional implementation could become the foundation for deeper networks and complex problems.
Ultimately, this could provide insights into the role of WTA microcircuits in larger networks in cortex
with localized receptive fields (Pogodin et al., 2021), similar to area V1 of cortex (Hubel & Wiesel,
1962). All in all, the algorithm has several properties that are individually interesting and novel,
and worth future extension. Combined, however, SoftHebb’s properties shown in this work may
already enable certain small-scale but previously-impossible applications. For example, fast, on-line,
unsupervised learning of simple tasks by edge sensing devices, operating in noisy conditions, with a
small battery and only local processing, requires those algorithmic properties that we demonstrated
here.
9
Reproducibility S tatement
Some of our main contributions are theoretical and their proof is fully reproducible by following the
rigorous derivations in the main text and in the appendices. Our experimental results are based on
standard machine-learning techniques, while we also describe the hyperparameters and experimental
protocols. Moreover, we provide Python code with specific instructions to reproduce the main
experiments.
References
Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separation
and blind deconvolution. Neural computation, 7(6):1129-1159, 1995.
Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, and Zhouhan Lin. Towards
biologically plausible deep learning. arXiv preprint arXiv:1502.04156, 2015.
David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Understanding and improving
interpolation in autoencoders via an adversarial regularizer. arXiv preprint arXiv:1807.07543,
2018.
Tom Binzegger, Rodney J Douglas, and Kevan AC Martin. A quantitative map of the circuit of cat
primary visual cortex. Journal of Neuroscience, 24(39):8441-8453, 2004.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space
of generative networks. arXiv preprint arXiv:1707.05776, 2017.
Jonathan Cannon, Michelle M McCarthy, Shane Lee, JUng Lee, ChristOPh Borgers, Miles A Whit-
tington, and Nancy Kopell. Neurosystems: brain rhythms and cognitive processing. European
Journal of Neuroscience, 39(5):705-719, 2014.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simPle framework for
contrastive learning of visual rePresentations. In International conference on machine learning, PP.
1597-1607. PMLR, 2020.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa SenguPta, and Anil A
Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):
53-65, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deeP
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Peter U Diehl and Matthew Cook. UnsuPervised learning of digit recognition using sPike-timing-
dePendent Plasticity. Frontiers in computational neuroscience, 9:99, 2015.
Peter U Diehl and Matthew Cook. Learning and inferring relations in cortical networks. arXiv
preprint arXiv:1608.08267, 2016.
Rodney J Douglas and Kevan AC Martin. Neuronal circuits of the neocortex. Annu. Rev. Neurosci.,
27:419-451, 2004.
Maxence Ernoult, Julie Grollier, Damien Querlioz, Yoshua Bengio, and Benjamin Scellier. Equilib-
rium ProPagation with continual weight uPdates. arXiv preprint arXiv:2005.04168, 2020.
Peter Foldiak. Forming sParse rePresentations by local anti-hebbian learning. Biological cybernetics,
64(2):165-170, 1990.
Peter FOldidk and Peter Fdilr. Adaptive network for optimal linear feature extraction. 1989.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
10
Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segregated
dendrites. ELife, 6:e22901, 2017.
Richard Hahnloser, Rodney J Douglas, Misha Mahowald, and Klaus Hepp. Feedback interactions
between neuronal pointers and maps for attentional processing. nature neuroscience, 2(8):746-752,
1999.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Tao Hu, Cengiz Pehlevan, and Dmitri B Chklovskii. A hebbian/anti-hebbian network for online
sparse dictionary learning derived from symmetric matrix factorization. In 2014 48th Asilomar
Conference on Signals, Systems and Computers, pp. 613-619. IEEE, 2014.
David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architec-
ture in the cat’s visual cortex. The Journal of physiology, 160(1):106-154, 1962.
Bernd Illing, Wulfram Gerstner, and Johanni Brea. Biologically plausible deep learning—but how
far can we go with shallow networks? Neural Networks, 118:90-101, 2019.
Takuya Isomura and Taro Toyoizumi. Error-gated hebbian rule: A local learning rule for principal
and independent component analysis. Scientific reports, 8(1):1-11, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Dmitry Krotov and John J Hopfield. Unsupervised learning by competing hidden units. Proceedings
of the National Academy of Sciences, 116(16):7723-7731, 2019.
Te-Won Lee, Mark Girolami, and Terrence J Sejnowski. Independent component analysis using an
extended infomax algorithm for mixed subgaussian and supergaussian sources. Neural computation,
11(2):417-441, 1999.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic
feedback weights support error backpropagation for deep learning. Nature communications, 7(1):
1-10, 2016.
Ralph Linsker. Local synaptic learning rules suffice to maximize mutual information in a linear
network. Neural Computation, 4(5):691-702, 1992.
Wolfgang Maass. On the computational power of winner-take-all. Neural computation, 12(11):
2519-2535, 2000.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Predictive coding approximates
backprop along arbitrary computation graphs. arXiv preprint arXiv:2006.04182, 2020.
Timoleon Moraitis, Abu Sebastian, and Evangelos Eleftheriou. Short-term synaptic plasticity opti-
mally models continuous environments, 2020.
Bernhard Nessler, Michael Pfeiffer, and Wolfgang Maass. Stdp enables spiking neurons to detect
hidden causes of their inputs. Advances in neural information processing systems, 22:1357-1365,
2009.
Bernhard Nessler, Michael Pfeiffer, Lars Buesing, and Wolfgang Maass. Bayesian computation
emerges in generic cortical microcircuits through spike-timing-dependent plasticity. PLoS compu-
tational biology, 9(4):e1003037, 2013.
Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical
biology, 15(3):267-273, 1982.
Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning
a sparse code for natural images. Nature, 381(6583):607-609, 1996.
11
Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy
employed byv1? Vision research, 37(23):3311-3325,1997.
Cengiz Pehlevan and Dmitri Chklovskii. A normative theory of adaptive dimensionality reduc-
tion in neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
861dc9bd7f4e7dd3cccd534d0ae2a2e9- Paper.pdf.
Cengiz Pehlevan and Dmitri B Chklovskii. A hebbian/anti-hebbian network derived from online
non-negative matrix factorization can cluster and discover sparse features. In 2014 48th Asilomar
Conference on Signals, Systems and Computers, pp. 769-775. IEEE, 2014.
Cengiz Pehlevan, Alexander Genkin, and Dmitri B Chklovskii. A clustering neural network model
of insect olfaction. In 2017 51st Asilomar Conference on Signals, Systems, and Computers, pp.
593-600. IEEE, 2017.
Michael Pfeiffer and Thomas Pfeil. Deep learning with spiking neurons: opportunities and challenges.
Frontiers in neuroscience, 12:774, 2018.
Roman Pogodin and Peter E Latham. Kernelized information bottleneck leads to biologically
plausible 3-factor hebbian learning in deep networks. arXiv preprint arXiv:2006.07123, 2020.
Roman Pogodin, Yash Mehta, Timothy P Lillicrap, and Peter E Latham. Towards biologically
plausible convolutional networks. arXiv preprint arXiv:2106.13031, 2021.
Yao Qin, Nicholas Frosst, Colin Raffel, Garrison Cottrell, and Geoffrey Hinton. Deflecting adversarial
attacks. arXiv preprint arXiv:2002.07405, 2020.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Ueli Rutishauser, Rodney J Douglas, and Jean-Jacques Slotine. Collective stability of networks of
winner-take-all circuits. Neural computation, 23(3):735-773, 2011.
Terence D Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network.
Neural networks, 2(6):459-473, 1989.
Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-
based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.
Terrence J Sejnowski. The unreasonable effectiveness of deep learning in artificial intelligence.
Proceedings of the National Academy of Sciences, 117(48):30033-30038, 2020.
Chr Von der Malsburg. Self-organization of orientation sensitive cells in the striate cortex. Kybernetik,
14(2):85-100, 1973.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
12
Appendix A S upplementary information to the Theory section
Probabilistic
interpretation
Mathematical
description
NeUronaI
interpretation
mixture model ?(x) = 9(x∣Cfc) - Q(Cfc)
likelihood function
?/fc(®;wofc) ：= Q(Cfc|®;wOfc)=
posterior probability
e∙wfcβ>+∞0fe
一 ∑U=191 _ ∑^1 ewlas+v3Ql
Qfc ：= Qfc(®；wfc) ∙ Q(Cfc；wOfc)=
=eWfc≡+Wo⅛
modeled prior probability
Q(C,fc5W0fc) = ew°k
parameter of modeled
prior probability
modeled ⅛-th component of
mixture likelihood function
WQk
cosine similarity
qk(x∖Wk) := q(x∖Ck;wk)=
=e«fc(e；wfc) _ ewfca
Uk(x]Wk) = WkX
normalized parameter vectors
of modeled likelihood function
sample of normalized data
a≈ = τ⅛



Figure 4:	The soft WTA model (Moraitis et al., 2020) used in SoftHebb. The network graph is shown
on the right. The input to the layer is shown at the bottom and the output is at the top. Each depicted
computational element in the diagram is in a white or grey row that also includes the element’s
description on the left.
Algorithm 1 SoftHebb learning
1:	for all neurons k ∈ {1, 2, ..., K} in the layer, do
2:	initialize random weights and biases
3:	for all training examples x do
4:	for all neurons k do
5:	Calculate preactivation uk = wkx
6:	for all neurons k do
7:	Optional: calculate activation qk0 = h(uk + w0k)	. e.g. h(x) = exp(x)
8:	Calculate posterior (i.e. normalized activation) yk	. e.g. Softmax
9:	for all neurons k do
10:	for all synapses i do
11:	calculate weight change ∆w(SOftHebb) ：= η ∙ yk ∙ (χi - Ukwik)
12:	update weight wik — wik + ∆w(SOftHebb)
13:	calculate bias change ∆w0SkoftHebb = ηe-w0k (yk - ew0k )
14:	update bias wok J wok + ∆wSkftHebb
Proof of Theorem 2.3. The parameters of model q are optimal w = Optw if they minimize the
model’s Kullback-Leibler divergence with the data distribution p. DKL(p(x)||q(x; w)). Because
pk := p(x|Ck) is independent from pl, and qk := q(x|Ck; wk) is independent from wl for every
l 6= k, we can find the set of parameters that minimize the KL divergence of the mixtures, by
minimizing the KL divergence of each component k: minDKL(pk||qk), ∀k, and simultaneously
setting
P(Ck) =Q(Ck;wok), ∀k.	(12)
From Eq. 3 and this last condition, Eq. 5 of the Theorem is proven:
Optwok = ln P (Ck ).
13
Further,
optwk := argminDKL(pk||qk)
wk
= arg min	Pk ln pk dx
arg min	pk lnpk - pk ln qkdx
= arg min	-pk ln qk dx
= arg max	pk ln qkdx
wk x
= arg max	pk ln euk dx
wk	x
= arg max	pk uk dx
wk x
=arg max μpk (Uk)
wk
=arg max μpk (Cos (Wk, x)).
wk
(13)
(14)
(15)
where we used for Eq. 13 the fact that xpk lnpkdx is a constant because it is determined by the
environment’s data and not by the model’s parametrization on w. Eq. 14 follows from the definition
of qk. The result in Eq. 15 is the mean value of the cosine similarity uk.
Due to the symmetry of the cosine similarity, it follows that
optWk = arg max μpk (Cos (wk, x)) = arg maxcos (wk ,μp% (x))
wk	wk
=C ∙ μpk (X) , c ∈ R.
Enforcement of the requirement for normalization of the vector leads to the unique solution
optwk
μpk(X)
ll*Pk (X)|| .
(16)
(17)
□
Proof of Theorem 2.5. We will find the equilibrium point of the SoftHebb plasticity rule, i.e. the
weight wik that implies E [∆wi(kSoftHebb)] = 0.
We will expand this expected value based on the plasticity rule itself, and on the probability distribution
of the input x.
E[∆w(softHebb)] = η / yk (x) ∙ (Xi - Uk(X)Wik)p(x)dx
X
K
Xl=1ZX
= η Z yk(X)(xi - wkXwik) X pl (X)P (Cl ) dX
K	/	一
xiyk (X)pl (X)P (Cl)dX -	wkwik	Xyk(X)pl (X)P (Cl)dX .
l=1	X
(18)
(19)
(20)
Based on this, we will now show that
wk
_,,*
optwk
μpk(X)
Mk (x)||
and w0k = optw0k , ∀k
=⇒ E[∆wi(kSoftHebb)] = 0 ∀i, k.
(21)
(22)
η
Using the premise 21, we can take the following steps, where steps 3, 4, and 6 are the main ones, and
steps 1, 2, and 5, as well as Theorem A.1 support those.
14
1.
2.
The cosine similarity function uk (x) = xwk, as determined by the total weighted input
to the neuron k, and appropriately normalized, defines a probability distribution centred
symmetrically around the vector wk,i.e. μ" (x) = Wk and Uk (μ稣 — X)= Uk(μ" + x).
Wk, as premised, is equal to the normalized μpk (x), i.e. the mean of the distribution
Pk(x) = p(x∣Ck), therefore: μ稣 (x) = μpk (x).
The soft-WTA version of the neuronal transformation, i.e. the softmax version of the model’s
inference is
yk(x)
exp (uk(x)) exp(w0k)
PlK=1 exp (ul (x)) exp(w0l)
(23)
But because of the premise 21 that the parameters of the model uk are set to their optimal
value, it follows that exp(uk (x)) = pk (x) and exp(w0k) = P (Ck), ∀k (see also Theorem
2.6), therefore
pk(x)P(Ck)
yk(x)
PK=ι Pi(χ)P (Cl)
(24)
3.	Eq. 20 involves twice the function yk(x)p(x) = PlK=1 yk (x)pl (x)P (Cl). Using the above
two points, we will now show that this is approximately equal to yk(x)pk(x)P(Ck).
(a)	We assume that the "support" Opk of the componentpk(x), i.e. the region where pk is
not negligible, is not fully overlapping with that of other components pl .
In addition, Opk is narrow relative to the input space ∀k, because, first, the cosine
similarity Uk(x) diminishes fast from its maximum at arg max Uk = Wk in case
of non-trivial dimensionality of the input space, and, second, pk(x) = exp (Uk(x))
applies a further exponential decrease.
Therefore, the overlap Okp ∩ Olp is small, or none, ∀l 6= k .
If Oky is the "support" of yk, then this is even narrower than Okp, due to the softmax.
As a result, the overlap Oyk ∩ Olp of yk and pl is even smaller than the overlap Opk ∩ Olp
ofpk and pl, ∀l 6= k.
(b)	Because of the numerator in Eq. 24, the overlap Oyk ∩ Opk of yk and pk is large.
Based on these two points, the overlaps Oyk ∩ Olp can be neglected for l 6= k, and it
follows that
K
X yk(x)pl(x)P(Cl) ≈ yk (x)pk (x)P (Ck).	(25)
l=1
Therefore, we can write Eq. 20 as
E[∆wi(kSoftHebb)]
≈η
xi yk (x)pk (x)P (Ck)dx — Wkwik
xyk (x)pk (x)P (Ck)dx
(26)
x
x
Next, we aim to show that the integrals x xiyk (x)pk (x)dx and x xyk(x)pk(x)dx in-
volved in that Equation equal the mean value of xi and x respectively according to the
distribution pk.
To show this, we observe that the integrals are indeed mean values of xi and x according to
a probability distribution, and specifically the distribution ykpk. We will first show that the
distributionPk is symmetric around its mean μpk (x). Then We will show that yk(x) is also
symmetric around the same mean. Then we will use the fact that the product of two such
symmetric distributions with common mean, such as ykPk, is a distribution with the same
mean, a fact that we will prove in Theorem A.1.
4.	Because the cosine similarity function Uk (x) is symmetric around the mean value μuk (x)=
Wk Uk(μuk — X)= Uk(μuk + x), it follows that Pk(μ稣 — X)= exp(uk(μ以 — x))=
exp(Uk (μUk + X)) = pk (μUk + x).
15
Therefore, pk(x) = exp (uk(x)) does have the sought property of symmetry, around
μPk (X).
In point 1 of this list We have also shown that μuk (x) = μpk (x), thus Pk is symmetric
around its own mean μpk (x).
5.	The reason why the softmax output yk is symmetric around the same mean as pk consists in
the following arguments:
(a)	The numerator Pk of the yk softmax in Eq. 24 is symmetric around μpk (x), as was
shown in the preceding point.
(b)	The denominator of Eq. 24, i.e. PlK=1 exp (Pl(x)) P(Cl) is also symmetric around
μpk (x) in Op, where Op is the "support" of the Pk distribution, i.e. the region where
the numerator Pk is not negligible.
This is because:
i.	We assume that the data is distributed on the unit hypersphere of the input space
according to P(x) without a bias. Therefore the total contribution of components
Pl6=kPl(x)P(Cl) to P(x) in that neighbourhood Okp is approximately symmetric
around μpk (x).
ii.	Pl6=k Pl (x)P (Cl) is not only approximately symmetric, but also its remaining
asymmetry has a negligible contribution to P(x) in Okp, because P(x) in Opk is
mostly determined by Pk(x).
This is true, because as we showed in point 3a, Opk ∩ Olp is a narrow overlap ∀l 6= k.
iii.	Pk is also symmetric, therefore, the total sum PlK=1 exp (Pl (x)) P (Cl) is symmet-
ric around μpk (x).
(c)	The inverse fraction 1 of a symmetric function f is also symmetric around the same
mean, therefore the inverse of the denominator -ʌ = :FrK------J ,CS、is also
,	p(x)	lK=1 exp(pl (x))P(Cl)
symmetric around the mean value μpk (x).
(d)	The normalized product of two distributions that are symmetric around the same mean
is a probability distribution with the same mean. We prove this formally in Theorem
A.1 and in its Proof at the end of the present Appendix A.
Therefore yk = Pk舟 is indeed symmetric around μyk (x) = μ次 (x) = Wk =
μpk(X).
In summary,
μpk (X) = μyk (X),	(27)
and, due to the symmetry,
pk (μpk + X) = pk (μpk - X),	(28)
yk (μpk + X)= yk (μpk - x) .	(29)
6.	Because the means of yk(x) and of Pk (x) are both equal to μpk (x), and because both distri-
butions are symmetric around that mean, the probability distribution yk(X)Pk(X)P (Ck)/Ik,
where Ik is the normalization factor, also has a mean 2耳小(x) equal to μpk (x):
I Xyk(X)Pk(X)P(Ck)/Ik dX = μpk (x).
x
(30)
We prove this formally in Theorem A.1 and in its Proof at the end of the present Appendix
A.
Therefore, the first component of the sum in Eq. 26 is
x Xiyk (X)Pk (X)P(Ck )dX = Ik ∙ μpk (Xi)
x
and, similarly, the second component is
-WkWik / Xyk(X)Pk(x)P(Ck)dX = -IkWkWik ∙ μpk (x).
x
(31)
(32)
16
From the above conclusions about the two components of the sum in Eq. 26, it follows that
E[∆w(koftHebb')] = Ik ∙ μpk (Xi) - IkWkwik ∙ μpk (x)	(33)
=Ik ∙ (μpk (Xi) - wkμpk (X) ∙ wik)	(34)
=Ik ∙ (μPk (Xi)- MμPk (X)||，wik )	(35)
=Ik ∙ "k(Xi) - ||〃pk(x)|| ∣[pkR)	(36)
∖	llμPk (X川，
= 0.	(37)
Therefore, it is indeed true that [wk = 0ptW1 = ∣∣μpk(X)∣∣ ∀k] =⇒ E[∆w(softHebb')] = 0∀i, k.
Thus, the optimal weights of the model °ptW1 = ∣∣μpk(X)∣∣ ∀k are equilibrium weights of the
SoftHebb plasticity rule and network.
However, it is not yet clear that the weights that are normalized to a unit vector are those that the rule
converges to, and that other norms of the vector are unstable. We will now give an intuition, and then
prove that this is the case.
The multiplicative factor uk is common between our rule and Oja’s rule (Oja, 1982). The effect of
this factor is known to normalize the weight vector of each neuron to a length of one (Oja, 1982),
as also shown in similar rules with this multiplicative factor (Krotov & Hopfield, 2019). We prove
that this is the effect of the factor also in the SoftHebb rule, separately in Theorem A.2 and its Proof,
provided at the end of the present Appendix A.
This proves Theorem 2.5, and satisfies the optimality condition derived in Theorem 2.3.	□
Proof of Theorem 2.6. Similarly to the Proof of Theorem 2.5, we find the equilibrium parameter
w0k of the SoftHebb plasticity rule.
E[∆w0k]
η
yk
e-w0k
η
e-w0k	p(yk)dyk - 1
yk
=η (e-w0kE[yk] - 1)
(38)
(39)
(40)
η [e-w0kμpk (Q(Ck∣x))- 1]	(41)
η [e-w0kμpk (P(Ck∣χ))- 1]	(42)
η e-w0kP(Ck) - 1	(43)
Therefore,
E[∆w0k] = 0 =⇒
w0SkoftHebb = lnP(Ck),
(44)
which proves Theorem 2.6 and shows the SoftHebb plasticity rule of the neuronal bias finds the
optimal parameter of the Bayesian generative model as defined by Eq. 5 of Theorem 2.3.	□
Theorem A.1. Given two probability density functions (PDF) y(X) and p(X) that are both centred
symmetrically around the same mean value μ, their product y(x)p(x), normalized appropriately, is a
PDF with the same mean, i.e.
Jx xy(x)dx = μ
JX xp(x)dx = μ
p(μ + X)= p(μ — x)
y(μ + x) = y(μ - x)
Xy (X)p(X)dX
μ.
(45)
17
Proof of Theorem A.1.
I
-∞
xy(x)p(x)dx
Zxy(x)p(x)dx +	xy(x)p(x)dx
-∞	J μ
(46)
,.r ，、一
I1 :=	xy(x)p(x)dx
-∞
(	(μ — u)y(μ — u)p(μ — u)d(μ — U)
+∞
=—	(μ — u)y(μ — u)p(μ — u)du
+∞
—Z0
+∞
0
μy(μ — u)p(μ — u)du + / uy(μ — u)p(μ — u)du
— / μy(μ + u)p(μ + u)d(—u) — /	uy(μ — u)p(μ — u)du
-∞	0
Z0	+∞
μy(μ + u)p(μ + u)du — /	uy(μ + u)p(μ + u)du
∞0
μ /	y(x)p(x)dx — /	uy(μ + u)p(μ + u)du.
-∞	0
(47)
(48)
(49)
(50)
(51)
(52)
(53)
Therefore,
+∞
I2 := xy(x)p(x)dx
J μ
(	(μ + u)y(μ + u)p(μ + u)d(μ + U)
0
+∞	+∞
I	μy(μ + u)p(μ + u)du + / uy(μ + u)p(μ + u)du
00
y(x)p(x)dx + /	uy(μ + u)p(μ + u)du.
0
Zxy(x)p(x)dx = I1 + I2
∞
+μ
μ J
-∞
y(x)p(x)dx — /	uy(μ + u)p(μ + u)du
0
Z +∞ y(x)p(x)dx + Z +∞
J μ	0 0
uy(μ + u)p(μ + u)du = μ ∙ I,
where I = R-+∞∞ y(x)p(x)dx.
(54)
(55)
(56)
(57)
(58)
(59)
(60)
□
〃/
J μ
Theorem A.2. The equilibrium weights of the SoftHebb synaptic plasticity rule of Eq. 8 are implicitly
normalized by the rule to a vector of length 1.
Proof of Theorem A.2. Using a technique similar to (Krotov & Hopfield, 2019), we write the Soft-
Hebb plasticity rule as a differential equation
τ
dw
(S of tH ebb)
ik
dt
T∆w(SoftHebb) = τη ∙ yk ∙ (xi — UkWik).
(61)
18
The derivative of the norm of the weight vector is
d∣Wk|| = d(wkWk) = 2w dw
dt	dt Wk dt .
Replacing dwtk in this equation with the SoftHebb rule of Eq. 61, it is
(62)
SoftHebb)
一Wk N+--------= 2ηWk ∙ yk ∙ (x - UkWk)
dt	τ
η
2—Wk ∙ yk ∙ (x - Wkxwk)
τ
=2~ukyk ∙ (1 - ||wk II) .	(63)
τ
This differential equation shows that the derivative of the norm of the weight vector increases if
IIWkII < 1 and decreases if IIWkII > 1, such that the weight vector tends a sphere of radius 1, which
proves the Theorem.	□
19
APPENDIX B DETAILS TO Alternate activation functions (SECTION 2.7)
Theorem 2.3, which concerns the synaptic plasticity rule in Eq. 8, was proven for the model of
Definition 2.2, which uses a mixture of natural exponential component distributions, i.e. with base e
(Eq. 4):
qk := q(x|Ck; wk) = euk.	(64)
This implied an equivalence to a WTA neural network with natural exponential activation functions
(Section 2.4). However, it is simple to show that these results can be extended to other model
probability distributions, and thus other neuronal activations.
Firstly, in the simplest of the alternatives, the base of the exponential function can be chosen differently.
In that case, the posterior probabilities that are produced by the model’s Bayesian inference, i.e. the
network outputs, Q(Ck|x; w) = yk(x; w) are given by a softmax with a different base. If the base
of the exponential is b, then
Q(Ck |x; w) = yk
buk+w0k
P = 1 buι+woι
(65)
It is obvious in the Proof of Theorem 2.3 in Appendix A that the same proof also applies to the
changed base, if we use the appropriate logarithm for describing KL divergence. Therefore, the
optimal parameter vector does not change, and the SoftHebb plasticity rule also applies to the
SoftHebb model with a different exponential base. This change of the base in the softmax bears
similarities to the change of its exponent, in a technique that is called Temperature Scaling and has
been proven useful in classification (Hinton et al., 2015).
Secondly, the more conventional type of Temperature Scaling, i.e. that which scales exponent, is also
possible in our model, while maintaining a Bayesian probabilistic interpretation of the outputs, a
neural interpretation of the model, and the optimality of the plasticity rule. In this case, the model
becomes
Q(Ck |x; W)= yk = P3 e(u…IT .
(66)
The Proof of Theorem 2.3 in Appendix A also applies in this case, with a change in Eq. 14, but
resulting in the same solution. Therefore, the SoftHebb synaptic plasticity rule is applicable in this
case too. The solution for the neuronal biases, i.e. the parameters of the prior in the Theorem (Eq. 5),
also remains the same, but with a factor of T: optw0k = T lnP(Ck).
Finally, and most generally, the model can be generalized to use any non-negative and monotonically
increasing function h(x) for the component distributions, i.e. for the activation function of the
neurons, assuming h(x) is appropriately normalized to be interpretable as a probability density
function. In this case the model becomes
Q(Ck|x;w) = yk
h(uk) ∙ W0k
PK=I h(uι) ∙ woι
(67)
Note that there is a change in the parametrization of the priors into a multiplicative bias w0, compared
to the additive bias in the previous versions above. This change is necessary in this general case,
because not all functions have the property ea+b = ea ∙ eb that We used in the exponential case.
We can show that the optimal weight parameters remain the same as in the previous case of an
exponential activation function, also for this more general case of activation h. It can be seen in the
Proof of Theorem 2.3, that for a more general function h(x) than the exponential, Eq. 14 Would
instead become:
optwk = argminDKL(pk||qk) = arg max	pk ln h(uk)dx
wk	wk x
= arg max pk ln h(cos (wk, x))dx
wk x
=arg max μpk (ln h(cos (Wk, x)))
wk
=arg max μpk (g(cos(wk, x))),
wk
(68)
20
where g(x) = ln h(x). We have assumed that h is an increasing function, therefore g is also increasing.
The cosine similarity is symmetrically decreasing as a function of x around wk . Therefore, the
function g0(x) = g(cos(wk, x)) also decreases symmetrically around wk. Thus, the mean of that
function g0 under the probability distribution Pk is maximum when μpk = Wk. As a result, Eq. 68
implies that in this more general model too, the optimal weight vector is optfWk = C ∙ μpk (x), c ∈ R,
and, consequently, it is also optimized by the same SoftHebb plasticity rule.
The implication of this is that the SoftHebb WTA neural network can use activation functions such
as rectified linear units (ReLU), or other non-negative and increasing activations, such as rectified
polynomials (Krotov & Hopfield, 2019) etc., and maintain its generative properties, its Bayesian
computation, and the theoretical optimality of the plasticity rule. A more complex derivation of the
optimal weight vector for alternative activation functions, which was specific to ReLU only, and did
not also derive the associated long-term plasticity rule for our problem category (Definition 2.1), was
provided by Moraitis et al. (2020).
Appendix C	Cros s -entropy and true causes, as opposed to labels
It is important to note that, in labelled datasets, the labels that have been assigned by a human
supervisor may not correspond exactly to the true causes that generate the data, which SoftHebb
infers. For example, consider MNIST. The 10 labels indicating the 10 decimal digits do not correspond
exactly to the true cause of each example image. In reality, the cause C of each MNIST example in
the sense implied by causal inference is not the digit cause itself, but a combination of a single digit
cause D, which is the MNIST label, with one of many handwriting styles S. That is, the probabilistic
model is such that in the Eq. P(x) = Pk P(x|Ck)P(Ck) of Definition 2.1, the cause C of each
sample is dual, i.e. there exists a digit Dd (d ∈ [0, 9]) and a style Ss such that
P (Ck) := P (C = Ck) = P (Dd)P (Ss) 6= P (Dd).	(69)
andP(Dd) = XP(Ck)P(Dd|Ck).	(70)
k
This is important for our unsupervised model. To illustrate this point, a network with K competing
neurons trained on MNIST may learn not to specialize to K digits D, but rather to K handwriting
styles S of one digit D&, or in general K combinations of digits with styles - combinations, which
are the true causes C that generate the data. This leads in this case to a mismatch between the labels
D, and the true causes C of the data. Therefore, given the labels and not the causes, it is not obvious
which number K should be chosen for the number of neurons. Practically speaking, K can be chosen
using common heuristics from cluster analysis. It is also not obvious how to measure the loss of
the WTA network during the learning process, since the ground truth for causes C is missing. We
will now provide the theoretical tools for achieving this loss-evaluation based on the labels. Even
though SoftHebb is a generative model, it can be used for discrimination of the input classes Ck,
using Bayes’ theorem. More formally, the proof of Theorem 2.3 involved showing that SoftHebb
minimizes the KL divergence of the model q(x) from the data p(x). Based on this it can be shown
that the algorithm also minimizes its cross-entropy HQcauses := H(P(C), Q(C|x)) of the causes
Q(Ck|x) that it infers, from the true causes of the data P(Ck): wSoftHebb = arg minw HQcauses.
An additional consequence is that by minimizing H causes , SoftHebb also minimizes its label-based
cross-entropy HQlabels := H(P (Dd), Q(Dd)) between the true labels P (Dd) and the implicitly
inferred labels Q(Dd):
Q(Dd) := XQ(Ck)P(Dd|Ck)	(71)
k
wSoftHebb = arg min HQcauses = arg min HQlabels.	(72)
This is because, in Eqs. 70 and 71, the dependence of the labels on the true causes P(Dd|Ck) is
fixed by the data generation process. To obtain Q(Dd|x) and measure the cross-entropy, the causal
structure P(Dd|C) is missing, but it can be represented by a supervised classifier Q2(Dd|Q(C|x))
of SoftHebb’s outputs, trained using the labels Dd. Therefore, by (a) unsupervised training of
SoftHebb, then (b) training a supervised classifier on top, and finally (c1) repeating the training of
SoftHebb with the same initial weights and ordering of the training inputs, while (c2) measuring the
trained classifier’s loss, we can observe the cross-entropy loss H labels of SoftHebb while it is being
21
minimized, and infer that Hcauses is also minimized (Eq. 72). We call this the post-hoc cross-entropy
method, and we have used it in our experiments (Section 3.2 and Fig. 1 C and D) to evaluate the
learning process in a theoretically sound manner.
Appendix D	Details on Hebbian experiments
In our validation experiments we found that softmax with a base of 1000 (see Section 2.7) performed
best. The learning rate η of Eq. 8 decreased linearly from 0.03 to 0 throughout training.
We found that an initial learning rate of 0.05 was best for the hard-WTA network. There are
certain tunable options and hyperparameters for the training of SoftHebb as well as for its forward
propagation to the next layer. Here we provide some guidelines for choosing the best options, and we
report those that we used. We searched over the initial learning rate η of Eq. 8 in the range of 0.1 and
0.001. The activation function (forward pass) was searched among RePU (rectified polynomial unit -
a generalization of ReLU) ??, softmax, sigmoid, tanh. For the 100 epochs experiment, we found that
SoftHebb and its limit case of hard WTA are both stable with a learning rate that decays linearly to
zero and starts between 0.08 and 0.02 while being optimum at an initial value of 0.045. As for the
activation, we found that there are 3 accurate configurations; RePU with a power of 4.5, softmax with
base of 200 and softmax with a smaller base of 1 if batch-normalization is added. For the 1 epoch
run, the learning rate is best decaying exponentially, starting at 0.55 and ending at 0.0055. In that fast
learning mode, only softmax activation with a high base of 200 gives a good accuracy.
For Fashion-MNIST the same options are best, except that an initial learning rate value of 0.065
performs better.
Supervised training of 100 epochs was done using Adam optimizer with mini-batch of 64. The
learning-rate schedule starts at 0.001 and every 15 epochs is divided by 2.
In the single-epoch case, learning is online, i.e. mini-batch size is 1, for both the Hebbian and the
supervised layer.
D.1 Convolutional SoftHebb
We implemented a convolutional version of SoftHebb. In this case, the WTA soft competition is
between convolutional kernels. This WTA computation and the corresponding SoftHebb update is
iterated over each patch of the layer’s input features, and this repeated operation over the patches
is parallelized on GPUs. Effectively, this is similar to a fully connected SoftHebb that is trained on
smaller patches of the original images.
Appendix E	Details on adversarial attacks
We used "Foolbox", a Python library for adversarial attacks.
E.1 Tuning PGD’s parameters
Figure 5: Noise and adversarial attack robustness of SoftHebb and of other unsupervised algorithms.
PGD has a few parameters that influence the effectiveness of the attack. Namely, which is a
parameter determining the size of the perturbation, the number of iterations for the attack’s gradient
ascent, the step size per iteration, and a number of possible random restarts per attacked sample. Here
we chose 5 random restarts. Then we found that 200 iterations are sufficient for both MNIST and
22
F-MNIST (Fig. 5 A and B). Then, using 200 iterations, and different values, we searched for a
sufficiently good step size. We found that relative to e a step size value of 0.01 e ≈ 0.33e (which is
also the default value of the toolbox that we used) is a good value. An example. curve for = 32/255
is shown in Fig. 5 C for MNIST and F-MNIST.
E.2 Adversarial robustness of k-means and PCA
B
A
GaUSSian noise
“	PGD adversarial attack
ιoo.oo-------------------------------::--------------------------------
——Hard WlA
--- 一_	___ ——SoftHebb
`	~~∙  	___ KmeanS
ʌs.	X∙∖	——PCA
75.00
Figure 6: Noise and adversarial attack robustness of SoftHebb and of other unsupervised algorithms.
It is possible that the observed robustness of SoftHebb can be reproduced by other unsupervised
learning rules. To test this possibility, we compared SoftHebb with PCA and k-means. We used 100
neurons, principal components, or centroids respectively. In PCA and k-means we then treated the
learned coefficients as weight vectors of neurons and applied an activation function to then train a
supervised classifier on top. First we attempted softmax as in SoftHebb. However the unperturbed
test accuracy achieved at convergence was much lower. For example, on MNIST, k-means only
reached an accuracy of 53.64% and PCA 28.55%, whereas SoftHebb reached 91.06. Therefore, we
performed the experiment again, but with ReLU activation for k-means and PCA, reaching 90.61%
and 82.74% respectively. Then we tested for robustness, revealing that SoftHebb’s learned features
are in fact more robust than those of other unsupervised algorithms (Fig. 6). For completeness, we
also include the hard WTA network, which is essentially a special case of SoftHebb with very high
base in the softmax (Section 2.7), and is therefore learning equally robust features.
E.3 Effect of activation function on adversarial robustness
A
GaUSSian noise
B
Standard deviation
PGD adversarial attack
4/255	8/255	16/255	32/255	64/255	128/255	255/255
c(L"noπn)
Figure 7: Noise and adversarial attack robustness of SoftHebb and of backpropagation-trained
softmax-MLP on MNIST and Fashion-MNIST. Both SoftHebb and the MLP use a softmax activation
at the hidden layer.
It is possible that the observed robustness of SoftHebb is due to the use of softmax as an activation
function. To test this, we compared the SoftHebb network from Fig. 2 with a same-size backprop-
trained 2-layer network, but here where the hidden layer’s summed weighted input was passed
through a softmax instead of ReLU before forwarding to the 2nd layer. First, we observed that at
convergence, the backprop-trained network did not achieve SoftHebb’s accuracy on either MNIST
23
(94.38%) or Fashion-MNIST (75.90%). Increasing the training time to 300 epochs did not help.
Second, as can be seen in 7, backpropagation remains significantly less robust than SoftHebb to the
input perturbations. This, together with the previous control experiments, suggests that, rather than
its activation function, it is SoftHebb’s learned representations that are responsible for the network’s
robustness.
24