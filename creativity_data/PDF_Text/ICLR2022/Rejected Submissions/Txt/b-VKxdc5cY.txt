Under review as a conference paper at ICLR 2022
Forward Operator Estimation in Generative
Models with Kernel Transfer Operators
Anonymous authors
Paper under double-blind review
Abstract
Generative models which use explicit density modeling (e.g., variational
autoencoders, flow-based generative models) involve finding a mapping
from a known distribution, e.g. Gaussian, to the unknown input distribu-
tion. This often requires searching over a class of non-linear functions (e.g.,
representable by a deep neural network). While effective in practice, the
associated runtime/memory costs can increase rapidly, usually as a function
of the performance desired in an application. We propose a much cheaper
(and simpler) strategy to estimate this mapping based on adapting known
results in kernel transfer operators. We show that our formulation enables
highly efficient distribution approximation and sampling, and offers surpris-
ingly good empirical performance that compares favorably with powerful
baselines, but with significant runtime savings. We show that the algorithm
also performs well in small sample size settings (in brain imaging).
1 Introduction
Generative modeling, in its unconditional form, refers to the problem of estimating the
data generating distribution: given i.i.d. samples X with an unknown distribution PX ,
a generative model seeks to find a parametric distribution that closely resembles PX . In
modern deep generative models, We often approach this problem via a latent variable - i.e.,
we assume that there is some variable Z ∈ Z associated with the observed data X ∈ X that
folloWs a known distribution PZ (also referred to as the prior in generative models). Thus,
We can learn a mapping f : Z → X such that the distribution after transformation, denoted
by Pf(Z) , aligns Well With the data generating distribution PX . Therefore, sampling from
PX becomes convenient since PZ can be efficiently sampled. Frequently, f is parameterized
by deep neural netWorks and optimized With stochastic gradient descent (SGD).
Existing generative modeling methods variously optimize the transformation f , most com-
monly modeling it as a Maximum Likelihood Estimation (MLE) or distribution matching
problem. For instance, given data X = {x1, . . . , xn}, a variational autoencoder (VAE)
(Kingma & Welling, 2013) first constructs Z through the approximate posterior qZ |X and
maximizes a loWer bound of likelihood pf(Z) (X). Generative adversarial netWorks (GANs)
(GoodfelloW et al., 2014) relies on a simultaneously learned discriminator such that samples of
Pf(Z) are indistinguishable from X. Results in (Arjovsky et al., 2017; Li et al., 2017) suggest
that GANs minimize the distributional discrepancies betWeen Pf(Z) and PX . FloW-based
generative models optimize pf(Z) (X) explicitly through the change of variable rule and
efficiently calculating the Jacobian determinant of the inverse mapping f-1.
In all examples above, the architecture or objective notWithstanding, the common goal is to
find a suitable function f that reduces the difference betWeen Pf(Z) and PX . Thus, a key
component in many deep generative models is to learn a forward operator as defined beloW.
Definition 1.1 (ForWard operator). A forward operator f? ∈ C : Z → X is defined to be a
mapping associated with some latent variable Z 〜PZ such that f ? = arg minf ∈c d(Pf(z), PX)
for some function class C and a distance measure d(∙, ∙).
Motivation: The specifics of the forWard operator may differ from case to case. But its
properties and hoW it is estimated numerically greatly influences the empirical performance
1
Under review as a conference paper at ICLR 2022
of the model. For instance, mode collapse issues in GANs are well known and solutions
continue to emerge (Srivastava et al., 2017). To learn the forward operator, VAEs use an
approximate posterior qZ|X that may sometimes fail to align with the prior (Kingma et al.,
2016; Dai & Wipf, 2019). Flow-based generative models enable direct access to the posterior
likelihood, yet in order to tractably evaluate the Jacobian of the transformation during
training, one must either restrict the expressiveness at each layer (Dinh et al., 2017; Kingma
& Dhariwal, 2018) or use more involved solutions (Chen et al., 2018). Of course, solutions to
mitigate these weaknesses (Ho et al., 2019) remains an active area of research.
The starting point of our work is to evaluate the extent to which we can radically simplify
the forward operator in deep generative models. Consider some desirable properties of a
hypothetical forward operator (in Def. (1.1)): (a) Upon convergence, the learned operator
f? minimizes the distance between PX and Pf(Z) over all possible operators of a certain
class. (b) The training directly learns the mapping from the prior distribution PZ , rather
than a variational approximation. (c) The forward operator f? can be efficiently learned and
sample generation is also efficient. It would appear that these criteria violate the “no free
lunch rule”, and some compromise must be involved. Our goal is to investigate this trade-off:
which design choices can make this approach work? Specifically, a well studied construct in
dynamical systems, namely the Perron-Frobenius operator (Lemmens & Nussbaum, 2012),
suggests an alternative linear route to model the forward operator. Here, we show that
if We are willing to give up on a few features in existing models - this may be acceptable
depending on the downstream use case - then, the forward operator in generative models
can be efficiently approximated as the estimation of a closed-form linear operator in the
reproducing kernel Hilbert space (RKHS). With simple adjustments of existing results, we
identify a novel way to replace the expensive training for generative tasks with a simple
principled kernel approach.
Contributions. Our results are largely based on results in kernel methods and dynamical
systems, but we demonstrate their relevance in generative modeling and complement recent
ideas that emphasize links between deep generative models and dynamical systems. Our
contributions are (a) We propose a non-parametric method for transferring a known prior
density linearly in RKHS to an unknown data density - equivalent to learning a nonlinear
forward operator in the input space. When compared to its functionally-analogous module
used in other deep generative methods, our method avoids multiple expensive training steps
yielding significant efficiency gains; (b) We evaluate this idea in multiple scenarios and show
competitive generation performance and efficiency benefits with pre-trained autoencoders on
popular image datasets including MNIST, CIFAR-10, CelebA and FFHQ; (c) As a special
use case, we demonstrate the advantages over other methods in limited data settings.
2 Preliminaries
We briefly introduce reproducing kernel Hilbert
space (RKHS) and kernel embedding of probabil-
ity distributions, concepts we will use frequently.
Definition 2.1 (RKHS (Aronszajn, 1950)). For
a set X , let H be a set of functions g : X → R.
Then, H is a reproducing kernel Hilbert space
(RKHS) with a product〈•，∙)h if there exists a
function k : X × X → R (called a reproducing
kernel) such that (i) ∀x ∈ X , g ∈ H, g(x) =
hg, k(x, ∙))h; (ii) H = Cl(Span({k(x, ∙), X ∈ X})),
where cl(∙) is the set closure.
Notations Meaning
Z	X	Random variable
Z	X	Data samples
Z	X	Domain
PZ	PX	Distribution
pZ	pX	Density function
k	l	Kernel function
H	G	RKHS
Φ(∙)	ψ(∙)	Feature mapping
Ek	El	Mean embedding operator
μz	μx	Kernel mean embedding
Table 1: Notations used in this paper.
The function φ(x) = k(x, ∙) : X → H is referred to as the feature mapping of the induced
RKHS H. A useful identity derived from feature mappings is the kernel mean embedding: it
defines a mapping from a probablity measure in X to an element in the RKHS.
Definition 2.2 (Kernel Mean Embedding (Smola et al., 2007)). Given a probability mea-
sure p on X with an associated RKHS H equipped with a reproducing kernel k such that
supx∈X k(x,x) < ∞, the kernel mean embedding of P in RKHS H, denoted by μp ∈ H, is
2
Under review as a conference paper at ICLR 2022
defined as μp = Ep[φ(x)] = J k(x, ∙)p(x)dx, and the mean embedding operator E : L1 (X) → H
is defined as μp = Ep.
Remark 1. For characteristic kernels, the operator E is injective. Thus, two distri-
butions (p, q) in X are identical iff EP = Eq.
This property allows using of Maximum Mean Discrepancy (MMD) for distribution match-
ing (Gretton et al., 2012; Li et al., 2017) and is common, see (Muandet et al., 2017;
Zhou et al., 2018). For a finite number of samples {xi}in=1 drawn from the probabil-
ity measure p, an unbiased empirical estimate of μH is μ^H =，Pn=I k(xi, ∙) such that
limn→∞ n Pi=I k(xi, ∙) = μH∙
Next, we review the covariance/cross covariance operators, two widely-used identities in
kernel methods (Fukumizu et al., 2013; Song et al., 2013) and building blocks of our approach.
Definition 2.3 (Covariance/Cross-covariance Operator). Let X, Z be random variables
defined on X × Z with joint distribution PX,Z and marginal distributions PX , PZ . Let
(l, φ, H) and (k, ψ, G) be two sets of (a) bounded kernel, (b) their corresponding feature map,
and (c) their induced RKHS, respectively. The (uncentered) covariance operator CZZ : H → H
and cross-covariance operator CXZ : H → G are defined as
CZZ , Ez〜PZ [。(Z)③ φ(Z)]	CXZ , E(x,z)〜Pχ,z M(X)③ φ(Z)]	⑴
where 0 is the outer product operator.
3 Simplifying the estimation of the forward operator
Forward operator as a dynamical system: The dynamical system view of generative
models has been described by others (Chen et al., 2018; Grathwohl et al., 2019; Behrmann
et al., 2019). These strategies model the evolution of latent variables in a residual neural
network in terms of its dynamics over continuous or discrete time t, and consider the
output function f as the evaluation function at a predetermined boundary condition t = t1 .
Specifically, given an input (i.e., initial condition) Z(t0), f is defined as
f(Z(t0))
Z(t0) +
Zt1
t0
∆t(Z(t))dt
(2)
where ∆t is a time-dependent neural network function and Z(t) is the intermediate solution
at t. This view of generative models is not limited to specific methods or model archetypes,
but generally useful, for example, by viewing the outputs of each hidden layer as evaluations
in discrete-time dynamics. After applying f on a random variable Z ∈ Z , the marginal
density of the output over any subspace Λ ⊆ X can be expressed as
pf(Z) (x)dx
Λ
pZ (Z)dZ
z∈f-1(Λ)
(3)
If there exists some neural network instance ∆t? such that the corresponding output func-
tion f? satisfies PX = Pf?(Z) , by Def. 1.1, f? is a forward operator. Let X be a set of
i.i.d. samples drawn from PX . In typical generative learning, either maximizing the likeli-
hood 击 ∑2χ∈χPf(Z)(X) or minimizing the distributional divergence d(Pf(z),Pχ) requires
evaluating and differentiating through f or f-1 many times.
Towards a one-step estimation of forward operator: Since f and f-1 in (3) will
be highly nonlinear in practice, evaluating and computing the gradients can be expensive.
Nevertheless, the dynamical systems literature suggests a linear extension of f *, namely the
Perron-Frobenius operator or transfer operator, that conveniently transfers pZ to pX .
Definition 3.1 (Perron-Frobenius operator (Mayer, 1980)). Given a dynamical system
f : X → X, the Perron-Frobenius (PF) operator P : L1 (X) → L1 (X) is an infinite-
dimensional linear operator defined as Λ (P pZ)(X)dX = z∈f-1(Λ) pZ (Z)dZ for all Λ ⊆ X.
3
Under review as a conference paper at ICLR 2022
Figure 1: Summary of our framework. The proposed operator is estimated to transfer RKHS
embedded densities from one to anothter (blue arrows). Generating new samples (red arrows)
ʌ
involves embedding the prior samples z* 〜PZ, applying the operator Ψ* = PEφ(z*), and
finding preimages ψi(Ψ*). The pre-trained autoencoder projects the data onto a smooth
latent space and is only required when generating high-dimensional data such as images.
Although in Def. 3.1, the PF operator P is defined for self-maps, it is trivial to extend P to
mappings f : Z → X by restricting the RHS integral z∈f-1 (Λ) pZ (z)dz to Z.
It can be seen that, for the forward operator f*, the corresponding PF operator P satisfies
pX = PpZ .	(4)
If P can be efficiently computed, transferring the tractable density pZ to the target density
pX can be accomplished simply by applying P . However, since P is an infinite-dimensional
operator on L1 (X ), it is impractical to instantiate it explicitly and exactly. Nonetheless,
there exist several methods for estimating the Perron-Frobenius operator, including Ulam’s
method (Ulam, 1960) and the Extended Dynamical Mode Decomposition (EDMD) (Williams
et al., 2015a). Both strategies pro ject P onto a finite number of hand-crafted basis functions
一this may suffice in many settings but may fall short in modeling highly complex dynamics.
Kernel-embedded form of PF operator: A natural extension of PF operator is to rep-
resent P by an infinite set of functions (Klus et al., 2020), e.g., projecting it onto the bases of
an RKHS via the kernel trick. There, for a characteristic kernel l, the kernel mean embedding
uniquely identifies an element μχ = ElpX ∈ G for any PX ∈ L1 (X). Thus, to approximate P,
we may alternatively solve for the dynamics from pZ to pX in their embedded form. Using
Tab. 1 notations, we have the following linear operator that defines the dynamics between
two embedded densities.
Definition 3.2 (Kernel-embedded Perron-Frobenius operator (Klus et al., 2020)). Given
pZ ∈ L1 (X) and pX ∈ L1 (X). Denote k as the input kernel and l as the output kernel.
Let μχ = Elpχ and μZ = Ekpz be their corresponding mean kernel embeddings. The
kernel-embedded Perron-Frobenius (kPF) operator, denoted by PE : H → G, is defined as
PE = CX Z CZ-Z1	(5)
Proposition 3.1 (Song et al. (2013)). With the above definition, PE satisfies
μX = PEμZ	(6)
under the conditions: (i) CZZ is injective (ii) μt ∈ range(Czz) (iii) E[g(X)|Z = ∙] ∈ H for
any g ∈ G.
The last two assumptions can sometimes be difficult to satisfy for certain RKHS (see Theorem
2 of Fukumizu et al. (2013)). In such cases, a relaxed solution can be constructed by replacing
C-Z by a regularized inverse (CZZ + λI)-1 or a Moore-Penrose pseudoinverse CZZ∙
The following proposition shows commutativity between the (kernel-embedded) PF operator
and the mean embedding operator, showing its equivalence to P when l is characteristic.
Proposition 3.2 ((Klus et al., 2020)). With the above notations, El ◦ P = PE ◦ Ek .
4
Under review as a conference paper at ICLR 2022
Transferring embedded densities with the kPF operator: The kPF operator is a
powerful tool that allows transferring embedded densities in RKHS. The main steps are:
(1)	Use mean embedding operator El on PZ. Let Us denote it by μz.
(2)	Transfer μz using kPF operator PE to get the mean embedded pχ, given by μχ.
Of course, in practice with finite data, {xi}i∈[n]〜Pχ and {zi}i∈[n]〜Pχ, PE must be
estimated empirically (see Klus et al. (2020) for an error analysis).
PE = CXZ(CZZ)-1 ≈ Ψ(ΦtΦ + λnI)-1Φt ≈ Ψ(ΦtΦ)tΦT
where Φ = [k(z1, ∙),… ,k(zn, •)], Ψ = [l(x1, ∙),…，l(xn, •)] are simply the corresponding
feature matrices for samples of Pχ and PZ , and λ is a small penalty term.
Learning kPF for unconditional generative modeling: Some generative modeling
methods such as VAEs and flow-based formulations explicitly model the latent variable Z
as conditionally dependent on the data variable X . This allows deriving/optimizing the
likelihood pf(Z) (X). This is desirable but may not be essential in all applications. To learn a
kPF, however, X and Z can be independent RVs. While it may not be immediately obvious
why we could assume this independence, we can observe the following property for the
empirical kPF operator, assuming that the empirical covariance operator CZZ is non-singular:
-Pe μZ = CXZ CZZ ^Z = Ψ{φ>}(ΦΦ>})-1Φin = Ψ(ΦτΦ)-1ΦτΦin = Ψin = μχ	⑺
日XZ CZZ
Suppose that {xi}i∈[n] and {zj }i∈[n] are independently sampled from the marginals Pχ
and PZ. It is easy to verify that (7) holds for any pairing {(xi, zj)}(i,j)∈[n]×[n] . However,
instantiating the RVs in this way rules out the use of kPF for certain downstream tasks such
as controlled generation or mode detection, since Z does not contain information regarding
X . Nevertheless, if sampling is our only goal, then this instantiation of kPF will suffice.
Mapping Z to G: Now, since PE is a deterministic linear operator, we can easily set up a
scheme to map samples of Z to elements of G where the expectation of the mapped samples
equals μχ
Define φ(z) = k(z, ∙) and ψ(x) = l(x, ∙) as feature maps of kernels k and l. We can rewrite
μχ as
μχ = PEEkpZ = PEEz[φ(z)] = Ez[Pe(Φ(Z))] = Ez[ψ (Ψ-1 (PEφ (Z)))]	⑻
Here ψ-1 is the inverse or the preimage map of ψ. Such an inverse, in general, may not
exist (Kwok & Tsang, 2004; Honeine & Richard, 2011). We will discuss a procedure to
approximate ψ-1 in §4.1. In what follows, we will temporarily assume that an exact preimage
map exists and is tractable to compute.
ʌ
Define Ψ* = PEφ(Z) as the transferred sample in G using the empirical embedded PF operator
PE. Then the next result shows that asymptotically the transferred samples converge in
distribution to the target distribution.
Proposition 3.3. As n → ∞, ψ-1 (Ψ*) → Pχ. That is, the preimage of the transferred
sample approximately conforms to Pχ under previous assumptions when n is large.
Proof. Since PE a4 s→mp' P, the proof immediately follows from (8).
□
4 Sample generation using the Kernel transfer operator
At this point, the transferred sample Ψ*, obtained by the kPF operator, remains an element
of RKHS G. To translate the samples back to the input space, we must find the preimage x*
such that ψ(x*) = Ψ*.
5
Under review as a conference paper at ICLR 2022
4.1	Solving for an approximate preimage
Solving the preimage in kernel-based methods is known to be ill-posed (Mika et al.,
1999) because the mapping ψ(∙) is not necessarily surjective, i.e., a unique preimage
x* = ψ-1(Ψ*), Ψ* ∈ H may not exist. Often, an approximate preimage ψ*(X, Ψ*) ≈ ψ-1(Ψ*)
is constructed instead based on relational properties among the training data in the RKHS.
We consider two options in our framework (1) MDS-based method (Kwok & Tsang, 2004;
Honeine & Richard, 2011),
ψMds(X,Ψ*) = 1 (X0X0>)-1X0(diag(X0>X0) - d>), where ∀i ∈ [γ],di = ∣∣l(xi, ∙) - Ψ*∣∣g
(9)
which optimally preserves the distances in RKHS to the preimages in the input space, and
(2) weighted FreChet mean (Friedman et al., 2001), which in Euclidean space takes the form
ψWFM(X, ψ*) = ψWFM(X0; 1 2 3 4 S) 6 7 8 9 10 11 12 = XOS/ksk1, where ∀i ∈ [Y], si = hl(xi, ∙),ψ*) (IO)
where X0 a neighborhood of γ training sam-
ples based on pairwise distance or similarity
in RKHS, following (Kwok & Tsang, 2004).
The weighted FreChet mean preimage uses
the inner product weights hΨ*, ψ(xi)i as
measures of similarities to interpolate train-
ing samples. On the toy data (as in Fig. 2),
weighted FreChet mean produces fewer sam-
ples that deviate from the true distribution
and is easier to compute. Based on this ob-
servation, we use the weighted FreChet mean
as the preimage module for all experiments
that requires samples, while acknowledging
that other preimage methods can also be
substituted in.
With all the ingredients in hand, we now
present an algorithm for sample generation
using the kPF operator in Alg. 1. The idea is simple yet powerful: at training time, we
construct the empirical kPF operator PE using the training data {xi}i∈[s] and samples of
the known prior {zi}ι∈[n]. At test time, we will transfer new points sampled from PZ to
feature maps in H, and construct their preimages as the generated output samples.
Algorithm 1 Sample Generation from kPF
1: Input: Training data X = {x1 , . . . , xn}, Op-
tional autoencoder (E, D), input/output ker-
nels (k, l), neighborhood size γ
2: Training
3:	X = (E (x1) , . . . , E (xn)) if E is provided
4:	Sample {zi}i∈[n]〜PZ independently
5:	Construct L, K ∈ Rn×n s.t.
Lij = l(xi, xj), Kij = k (zi,zj)
6:	Kinv = (K + λnI )-1 or K t
7: Inference
8:	Generate new prior sample Z 〜PZ
9:	S = L ∙ Kinv[k(zι, z*)... k(zn, z*)]>
10:	ind = argsort(s)[-γ :]
11:	x* = ΨwFM (X[ind]; s[ind]).
12: Output D(x*) if D is provided else x*
4.2	Image generation
Image generation is a common application for generative models (Goodfellow et al., 2014;
Dinh et al., 2017). While our proposal is not image specific, constructing sample preimages
in a high dimensional space with limited training samples can be challenging, since the space
of images is usually not dense in a reasonably sized neighborhood. However, empirically
images often lie near a low dimensional manifold in the ambient space (Seung & Lee, 2000),
and one may utilize an autoencoder (AE) (E, D) to embed the images onto a latent space
that represents coordinates on a learned manifold. If the learned manifold lies close to the
true manifold, we can learn densities on the manifold directly (Dai & Wipf, 2019).
Therefore, for image generation tasks, the training data is first projected onto the latent
space of a pretrained AE. Then, the operator will be constructed using the projected latent
representations, and samples will be mapped back to image space with the decoder of AE.
Our setup can be viewed analogously to other generative methods based on so called “ex-post”
density estimation of latent variables (Ghosh et al., 2020). We also restrict the AE latent
space to a hypersphere Sn-I to ensure that (a) k(∙, ∙) and l(∙, ∙) are bounded and (b) the
space is geodesically convex and complete, which is required by the preimage computation.
To compute the weighted FreChet mean on a hypersphere, we adopt the recursive algorithm
in Chakraborty & Vemuri (2015) (see appendix D for details).
6
Under review as a conference paper at ICLR 2022
Figure 2: Left figure: Density estimation on 2D toy data. Top to bottom: (1) Training
data samples, and learned densities of (2) GMM (3) Glow (4) Proposed kPF operator.
More details in appendix. Right figure: Sample generation results. Top: Data samples
Middle: MDS-based preimage samples Bottom: Weighted Frechet mean samples.
5	Experimental Results
Goals. In our experiments, we seek to answer three questions: (a) With sufficient data,
can our method generate new data with comparable performance with other state-of-the-art
generative models? (b) If only limited data samples were given, can our method still estimate
the density with reasonable accuracy? (c) What are the runtime benefits, if any?
Datasets/setup. To answer the first question, we evaluate our method on standard vision
datasets, including MNIST, CIFAR10, and CelebA, where the number of data samples is
much larger than the latent dimension. We compare our results with other VAE variants
(Two-stage VAE Dai & Wipf (2019), WAE Arjovsky et al. (2017), CV-VAE Ghosh et al.
(2020)) and flow-based generative models (Glow Kingma & Dhariwal (2018), CAGlow Liu
et al. (2019)) The second question is due to the broad use of kernel methods in small sample
size settings. For this more challenging case, we randomly choose 100 training samples (< 1%
of the full dataset) from CelebA and evaluate the quality of generation compared to other
density approximation schemes. We also use a dataset of T1 Magnetic Resonance (MR)
images from the Alzheimer’s Disease Neuromaging Initiative (ADNI) study.
Distribution transfer with many data samples. We evaluate the quality by calculating
the Frechet Inception Distance (FID) (HeuSel et al., 2017) with 10K generated images from
each model. Here, we use a pretained regularized autoencoder (Ghosh et al., 2020) with
a latent space restricted to the hypersphere (denoted by SRAE) to obtain smooth latent
representations. We compare our kPF to competitive end-to-end deep generative baselines
(i.e. flow and VAE variants) as well as other density estimation models over the same
SRAE latent space. For the latent space models, we experimented with Glow (Kingma &
Figure 3: Comparison of different sampling techniques using AE trained on CelebA 64x64.
Left to right: samples of (1) Two-Stage VAE (Dai & Wipf, 2019) (2) SRAEGlow (Kingma &
Dhariwal, 2018) (3) SRAEGMM (4) SRAE NTK-kPF using 10k latent points.
7
Under review as a conference paper at ICLR 2022
Figure 4: Representative samples from learned kPF on pre-trained NVAE latent space
Dhariwal, 2018), VAE, Gaussian mixture model (GMM), and two proposed kPF operators
with Gaussian kernel (RBF-kPF) and NTK (NTK-kPF) as the input kernel. The use of NTK
is motivated by promising results at the interface of kernel methods and neural networks
(Jacot et al., 2018; Arora et al., 2020). Implementation details are included in the appendix.
Comparative results are shown in Table 2. We see that for images with structured feature
spaces, e.g., MNIST and CelebA, our method matches other non-adversarial generative
models, which provides evidence in support of the premise that the forward operator can
be simplified. Further, we present qualitative results on all datasets (in Fig. 3), where we
compare our kPF operator based model with other density estimation techniques on the
latent space. Observe that our model generates comparable visual results as SRAEGlow .
Since kPF learns the distribution on a pre-trained AE latent space for image generation,
using a more powerful AE can offer improvements in generation quality. In Fig. 4, we
present representative images by learning our kPF on NVAE Vahdat & Kautz (2020) latent
space, pre-trained on the FFHQ dataset. NVAE builds a hierarchical prior and achieves
state-of-the-art generation quality among VAEs. We see that kPF can indeed generate
high-quality and diverse samples with the help of NVAE encoder/decoder. In fact, any
AE/VAE may be substituted in, assuming that the latent space is smooth.
Summary: When a sufficient number of samples are available, our algorithm performs as
well as the alternatives, which is attractive given the efficient training. In Fig. 5, we present
comparative result of FIDs with respect to the training time. Since kPF can be computed in
closed-form, it achieves significant training efficiency gain compared to other deep generative
methods while delivering competitive generative quality.
Distribution transfer with limited data samples. Next, we present our evaluations
when only a limited number of samples are available. Here, each of the density estimators was
trained on latent representations of the same set of 100 randomly sampled CelebA images,
and 10K images were generated to evaluate FID (see Table 3). Our method outperforms
Glow and VAE, while offering competitive performance with GMM. Surprisingly, GMM
remains a strong baseline for both tasks, which agrees with results in Ghosh et al. (2020).
However, note that GMM is restricted by its parametric form and is less flexible than our
method (as shown in Fig 2).
	MNIST	CIFAR	CelebA
Glow"	258	-	~103.7-
CAGlowt	26.3	-	104.9
Vanilla VAE	36.5	111.0	52.1
CV-VAEt	33.8	94.8	48.9
WAEt	20.4	117.4	53.7
Two-stage VAE	16.5	110.3	44.7
SRAEGlow	-^15.5^^	85.9	35.0
SRAEVAE	17.2	198.0	48.9
SRAEGMM	16.7	79.2	42.0
SRAERBF-kPF(ours)	19.7	77.9	41.9
SRAENTK-kPF(ours)	19.5	77.5	41.0
Table 2: Comparative FID values. SRAE indicates
an autoencoder with hyperspherical latent space
and spectral regularization following Ghosh et al.
(2020). Results reported from ∣: Liu et al. (2019).
f: Ghosh et al. (2020).
wall clock time (sec)
Figure 5: FID versus training time
for latent space models. All models
are learned on the latent representa-
tions encoded by the same pre-trained
SRAE.
8
Under review as a conference paper at ICLR 2022
VAE	Glow GMM RBF-kPF NTK-kPF
59.3	770	396	406	40b9
Table 3: FID values for few samples setting
density approximation on CelebA.
Learning from few samples is common in
biomedical applications where acquisition
is costly. Motivated by interest in mak-
ing synthetic but statistic preserving data
(rather than the real patient records) pub-
licly available to researchers (see NIH N3C Data Overview), we present results on gen-
erating high-resolution (160 × 196 × 160) brain images: 183 samples from group AD (di-
agnosed as Alzheimer’s disease) and 291 samples from group CN (control normals). For
n = 474 d = 5017600, using our kernel operator, we can generate high-quality samples
that are in-distribution. We present comparative results with VAEs. The generated samples
in Fig. 6 clearly show that our method generates sharper images. To check if the results
are also scientifically meaningful, we test consistency between group difference testing (i.e.,
cases versus controls differential tests on each voxel) on the real images (groups were AD
and CN) and the same test was performed on the generated samples (AD and CN groups),
using a FWER corrected two-sample t-test Ashburner & Friston (2000). The results (see
Fig 6) show that while there is a deterioration in regions identified to be affected by disease
(different across groups), many statistically-significant regions from tests on the real images
are preserved in voxel-wise tests on the generated images.
Summary: We achieve improvements in the small sample size setting compared to other
generative methods. This is useful in many data-poor settings. For larger datasets, our
method still compares competitively with alternatives, but with a smaller resource footprint.
6	Limitations
Our proposed simplifications can be variously useful, but deriving the density of the posterior
given a mean embedding or providing an exact preimage for the generated sample in RKHS
is unresolved at this time. While density estimation from kPF has been partially addressed
in SChuSter et al. (2020b), finding the pre-image is often ill-posed. The weighted FreChet
mean preimage only provides an approximate solution and evaluated empirically, and due to
the interpolation-based sampling strategy, samples Cannot be obtained beyond the Convex
hull of training examples. Making Z and X independent RVs also limits its use for Certain
downstream task suCh as representation learning or semantiC Clustering. Finally, like other
kernel-based methods, the sup-quadratiC memory/Compute Cost Can be a bottleneCk on large
datasets and kernel approximation (e.g. (Rahimi & ReCht, 2009)) may have to be applied;
we disCuss this in appendix F.
7	Conclusions
We show that using reCent developments in regularized autoenCoders, a linear kernel transfer
operator Can potentially be an effiCient substitute for the forward operator in some generative
models, if some Compromise in Capabilities/performanCe is aCCeptable. Our proposal, despite
its simpliCity, shows Comparable empiriCal results to other generative models, while offering
effiCienCy benefits. Results on brain images also show promise for appliCations to high-
resolution 3D imaging data generation, whiCh is being pursued aCtively in the Community.
Figure 6: Left. Top: data, generated samples of Middle: VAE , Bottom: kPF with SRAE. Right.
Statistically significant regions Top: voxel-wise tests on real data, Bottom: voxel-wise tests on
generated samples are shown in negative log p-value thresholded at α = 0.01.
9
Under review as a conference paper at ICLR 2022
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathe-
matical society, 68(3):337—404, 1950.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 32, pp. 8141-8150. Curran Associates, Inc., 2019.
Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli
Yu. Harnessing the power of infinitely wide deep nets on small-data tasks. In International
Conference on Learning Representations, 2020.
John Ashburner and Karl J Friston. Voxel-based morphometry—the methods. Neuroimage,
11(6):805-821, 2000.
Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jorn-Henrik
Jacobsen. Invertible residual networks. In International Conference on Machine Learning,
pp. 573-582. PMLR, 2019.
Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations
from data by sparse identification of nonlinear dynamical systems. Proceedings of the
National Academy of Sciences, 113(15):3932-3937, 2016. ISSN 0027-8424. doi: 10.1073/
pnas.1517384113.
Rudrasis Chakraborty and Baba C Vemuri. Recursive frechet mean computation on the
grassmannian and its applications to computer vision. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 4229-4237, 2015.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural
ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 6571-6583. Curran Associates, Inc., 2018.
Bin Dai and David Wipf. Diagnosing and enhancing VAE models. In International Conference
on Learning Representations, 2019.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp.
In International Conference on Learning Representations, 2017.
Paul F. Evangelista, Mark J. Embrechts, and Boleslaw K. Szymanski. Taming the curse of
dimensionality in kernels and novelty detection. In Ajith Abraham, Bernard de Baets,
Mario Koppen, and Bertram Nickolay (eds.), Applied Soft Computing Technologies: The
Challenge of Complexity, pp. 425-438, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.
ISBN 978-3-540-31662-6.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,
volume 1. Springer series in statistics New York, 2001.
Kenji Fukumizu, Le Song, and Arthur Gretton. Kernel bayes’ rule: Bayesian inference with
positive definite kernels. The Journal of Machine Learning Research, 14(1):3753-3783,
2013.
Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf.
From variational to deterministic autoencoders. In International Conference on Learning
Representations, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural
Information Processing Systems, volume 27, pp. 2672-2680. Curran Associates, Inc., 2014.
10
Under review as a conference paper at ICLR 2022
Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable re-
versible generative models with free-form continuous dynamics. In International Conference
on Learning Representations, 2019.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander
Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):
723-773, 2012.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.
Springer Series in Statistics. Springer New York Inc., New York, NY, USA, 2001.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778,
2016. doi: 10.1109/CVPR.2016.90.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochre-
iter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6626-6637.
Curran Associates, Inc., 2017.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving
flow-based generative models with variational dequantization and architecture design. In
International Conference on Machine Learning, pp. 2722-2730. PMLR, 2019.
Paul Honeine and Cedric Richard. Preimage problem in kernel-based machine learning.
IEEE Signal Processing Magazine, 28(2):77-88, 2011.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 8571-8580. Curran Associates, Inc., 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 con-
volutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 10215-10224.
Curran Associates, Inc., 2018.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 29, pp. 4743-4751. Curran Associates, Inc., 2016.
Stefan Klus, Ingmar Schuster, and Krikamol Muandet. Eigendecompositions of transfer
operators in reproducing kernel hilbert spaces. Journal of Nonlinear Science, 30(1):283-315,
2020.
JT-Y Kwok and IW-H Tsang. The pre-image problem in kernel methods. IEEE transactions
on neural networks, 15(6):1517-1525, 2004.
Bas Lemmens and Roger Nussbaum. Nonlinear Perron-Frobenius Theory, volume 189.
Cambridge University Press, 2012.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd
gan: Towards deeper understanding of moment matching network. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 30, pp. 2203-2213. Curran
Associates, Inc., 2017.
11
Under review as a conference paper at ICLR 2022
Rui Liu, Yu Liu, Xinyu Gong, Xiaogang Wang, and Hongsheng Li. Conditional adversar-
ial generative flow for controllable image synthesis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Dieter H Mayer. The ruelle-araki transfer operator in classical statistical mechanics. 1980.
Sebastian Mika, Bernhard Scholkopf, Alex Smola, Klaus-Robert Muller, Matthias Scholz,
and Gunnar Ratsch. Kernel pca and de-noising in feature spaces. In M. Kearns, S. Solla,
and D. Cohn (eds.), Advances in Neural Information Processing Systems, volume 11, pp.
536-542. MIT Press, 1999.
K. Muandet, K. Fukumizu, B. Sriperumbudur, and B. Scholkopf. 2017. doi: 10.1561/
2200000060.
Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing
minimization with randomization in learning. In D. Koller, D. Schuurmans, Y. Bengio,
and L. Bottou (eds.), Advances in Neural Information Processing Systems, volume 21.
Curran Associates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2008/
file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf.
M. Kafaei Razavi, A. Kerayechian, M. Gachpazan, and S. Shateyi. A new iterative method
for finding approximate inverses of complex matrices. Abstract and Applied Analysis, 2014:
563787, Sep 2014. ISSN 1085-3375. doi: 10.1155/2014/563787. URL https://doi.org/
10.1155/2014/563787.
Mehdi S. M. Sa jjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.
Assessing generative models via precision and recall. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.
neurips.cc/paper/2018/file/f7696a9b362ac5a51c3dc8f098b73923-Paper.pdf.
Ingmar Schuster, Mattes Mollenhauer, Stefan Klus, and Krikamol Muandet. Kernel condi-
tional density operators. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of
the Twenty Third International Conference on Artificial Intelligence and Statistics, volume
108 of Proceedings of Machine Learning Research, pp. 993-1004. PMLR, 26-28 Aug 2020a.
Ingmar Schuster, Mattes Mollenhauer, Stefan Klus, and Krikamol Muandet. Kernel condi-
tional density operators. volume 108 of Proceedings of Machine Learning Research, pp.
993-1004, Online, 26-28 Aug 2020b. PMLR.
H. Sebastian Seung and Daniel D. Lee. The manifold ways of perception. Science, 290(5500):
2268-2269, 2000. ISSN 0036-8075. doi: 10.1126/science.290.5500.2268.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A hilbert space embedding for
distributions. In Marcus Hutter, Rocco A. Servedio, and Eiji Takimoto (eds.), Algorithmic
Learning Theory, pp. 13-31, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg. ISBN
978-3-540-75225-7.
Le Song, Kenji Fukumizu, and Arthur Gretton. Kernel embeddings of conditional distribu-
tions: A unified kernel framework for nonparametric inference in graphical models. IEEE
Signal Processing Magazine, 30(4):98-111, 2013.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles Sutton.
Veegan: Reducing mode collapse in gans using implicit variational learning. arXiv preprint
arXiv:1705.07761, 2017.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein
auto-encoders. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=HkL7n1- 0b.
Stanislaw M Ulam. A collection of mathematical problems. New York, 29, 1960.
Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Neural
Information Processing Systems (NeurIPS), 2020.
12
Under review as a conference paper at ICLR 2022
Christopher Williams and Matthias Seeger. Using the nystrom method to speed up kernel
machines. In T. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information
Processing Systems, volume 13. MIT Press, 2001. URL https://proceedings.neurips.
cc/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf.
Matthew O Williams, Ioannis G Kevrekidis, and Clarence W Rowley. A data-driven
approximation of the koopman operator: Extending dynamic mode decomposition. Journal
of Nonlinear Science, 25(6):1307-1346, 2015a.
Matthew O. Williams, Ioannis G. Kevrekidis, and Clarence W. Rowley. A data-driven
approximation of the koopman operator: Extending dynamic mode decomposition. Journal
of Nonlinear Science, 25(6), 2015b. doi: 10.1007/s00332-015-9258-5.
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin
Li, and Vikas Singh. Nystromformer: A nystrom-based algorithm for approximating
self-attention. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,
Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021,
The Eleventh Symposium on Educational Advances in Artificial Intel ligence, EAAI 2021,
Virtual Event, February 2-9, 2021, pp. 14138-14148. AAAI Press, 2021. URL https:
//ojs.aaai.org/index.php/AAAI/article/view/17664.
Hao Henry Zhou, Vikas Singh, Sterling C. Johnson, and Grace Wahba. Statistical tests
and identifiability conditions for pooling and analyzing multisite datasets. Proceedings
of the National Academy of Sciences, 115(7):1481-1486, 2018. ISSN 0027-8424. doi:
10.1073/pnas.1719747115.
13
Under review as a conference paper at ICLR 2022
A Choice of Kernel is relevant yet flexible
In some cases, one would focus on identifying (finite) eigenfuntions and modes of the
underlying operator (Williams et al., 2015b; Brunton et al., 2016). But rather than finding
certain modes that best characterize the dynamics, we care most about minimizing the error
of the transferred density and therefore whether the span of functions is rich/expressive
enough. In particular, condition (iii) in Proposition 3.2 requires an input RKHS spanned
by sufficiently rich bases (Fukumizu et al., 2013). For this reason, the choice of kernel here
cannot be completely ignored since it determines the family of functions contained in the
induced RKHS.
To explore the appropriate kernel setup for our application, we empirically evaluate the
effect of using several different kernels via a simple experiment on MNIST. We first train
an autoencoder to embed MNIST digits on to a hypersphere S2 , then generate samples
from kPF by the procedure described by Alg. 1 using the respective kernel function as the
input kernel k. Subplot (b) and (c) in Fig. 7 show the generated samples using Radial Basis
Function (RBF) kernel and arc-cosine kernel, respectively. Observe that the choice of kernel
has an influence on the sample population, and a kernel function with superior empirical
behavior is desirable.
Motivated by this observation, we evaluated the Neural Tangent Kernel (NTK) (Jacot et al.,
2018), a well-studied neural kernel in recent works. We use it for a few reasons, (a) NTK, in
theory, corresponds to a trained infinitely-wide neural network, which spans a rich set of
functions that satisfies the assumption. (b) For well-conditioned inputs (i.e., no duplicates)
on hypersphere, the positive-definiteness of NTK is proved in (Jacot et al., 2018). Therefore,
invertibility of the Gram matrix KZZ = ΦT Φ is almost guaranteed if the prior distribution
pZ is restricted on a hypersphere (c) NTK can be non-asymptotically approximated (Arora
et al., 2019). (d) Unlike other parametric kernels such as RBF kernels, NTK is less sensitive
to hyperparameters, as long as the number of units used is large enough (Arora et al., 2019).
Subplot (d) of Fig. 7 shows that kPF learned with NTK as input kernel is able to generate
samples that are more consistent with the data distribution. However, we should note that
NTK is merely a convenient choice of kernel that requires less tuning, and is not otherwise
central to our work. In fact, as shown in our experiment in Tab. 2, a well-tuned RBF kernel
may also achieve a similar performance. Indeed, in practice, any suitable choice of kernel
may be conveniently adopted into the proposed framework without major modifications.
Figure 7: 10k samples from MNIST dataset (left to right) (a) projected on S2 shown in (θ, φ) using
auto-encoder, and 10K generated samples from kPF with input kernel of type (b) RBF (c) arccos
(d) NTK. Color of sampled points represents the class of their nearest training set neighbor in the
output RKHS.
14
Under review as a conference paper at ICLR 2022
B Density Estimation with kPF Operator
The displayed transferred density with the kPF operator on toy data in Fig. 1 is approximated
using the empirical kernel conditional density operator (CDO) (Schuster et al., 2020a), since
there is currently no known methods that can exactly reconstruct density from the transferred
mean embeddings. The marginalized transferred density pPE has the following form
PPE = C- PE μz = C- CXZ C-Z μz,	(11)
where Cρ = Ey〜ρ[l(y, ∙) 0 l(y, ∙)] is the covariance operator of a independent reference density
ρ in G . The above density function is also an element of RKHS G , and therefore we can
evaluate the density at x* by using the reproducing property PPE (x*) = {ppE, l(x*, •)》. The
results in Schuster et al. (2020a) show that the empirical estimate of pPE may be constructed
from m samples of the reference density {yi }i∈[m] and n training samples {xi}i∈[n] and
{zi }i∈[n] as
m
PPE = (CP + & I )-1 CXZ (CZZ + αI )-1 μz = X 即(仇,∙)	(12)
i=1
where
β = m-2 (LY + & I )-2 LYX (KZ + αI )-1 Φ> Rz	(13)
and
LY = [l(yi,yj)]ij ∈Rm×m,LYX = [l(yi,xj)]ij ∈Rm×n,KZ = [k(zi,zj)]ij ∈ Rn×n	(14)
In Fig. 8, we use a uniform density in the square (±4, ±4) as the reference density ρ and
constructed PPE using m = 10000 samples {yi}i∈[m] from ρ. Due to the form of the empirical
kernel CDO, where the estimated density function PPE is a linear combination of {l(yi, ∙)}i∈[m]
(as in Eq. 12), the approximation can be inaccurate if reference samples are relatively sparse
and the densities are ‘sharp’. In those cases, to obtain a better density estimate, we may
either increase the number of reference samples used to construct the empirical CDO (which
can be computationally difficult due to the need to compute (LY + &0I)-2), or, with some
prior knowledge to the true distribution, choose a reference density which is localized around
the ground truth density.
Figure 8: Inaccurate density estimation result from kernel CDO using 10k samples from
uniform reference density ρ
Therefore, to show a more faithful density estimate of the transferred distribution for
visualization purpose, we use a composite of the uniform density and the true density with
weight 4 : 1 as the reference density ρ in Fig. 2. In this case, approximately 20% of the
reference samples concentrates around the high-density areas of the true density, which helps
to form a better basis for PPE. Note that the choice of P does not affect the transferred
density embedding PEμz since it is independent of PX and PZ. After this modification,
the reconstructed density more accurately reflects the true density compared to GMM and
GLOW, indicating the transferred distribution by kPF in RKHS matches better to the
ground truth distribution. This is also reflected in the generated samples in Fig. 9, where
samples generated by the kPF operator are clearly more aligned with the ground truth
distribution.
15
Under review as a conference paper at ICLR 2022
GT
GMM
Glow
NTK-
kPF
Figure 9: Sample comparisons of the distribution matching methods
16
Under review as a conference paper at ICLR 2022
C Effect of γ on Sample Quality
In the sampling stage, our proposed method finds the approximate preimage of the transferred
kernel embeddings by taking the weighted Frechet mean of the top Y neighbors among the
training samples. The choice of γ therefore influences the quality of generation.
From Figure 10, we can observe that, in general, FID worsens as γ increases. This observation
aligns with our intuition of preserving only the local similarities represented by the kernel,
and similar ideas have been previously used in the literature (Hastie et al., 2001; Kwok &
Tsang, 2004). However, significantly decreasing γ leads to the undesirable result where the
generator merely generates the training samples (in the extreme case where γ = 1, generated
samples will just be reconstructions of training samples). Therefore, in our experiments, we
choose γ = 5 to achieve a balance between generation quality and the distance to training
samples.
5	7 W 15	50	1∞
Size of neighborhood 7
Figure 10: FID versus γ on a few computer vision datasets.
17
Under review as a conference paper at ICLR 2022
D Weighted FrWchet MEAN on THE HypERSpHERE
While the weighted Frechet Mean in Euclidean space can be computed in closed-form as a
weighted arithmetic mean (as in Eq. 10), on the hypersphere there is no known closed-form
solution. Thus, we adopt the iterative algorithm in (Chakraborty & Vemuri, 2015) for an
approximate solution given data points X = {x1 . . . xγ} and weight vector s:
M1 = x1
v
Mi+1 := cos(ksi+ιv∣∣)Mi + sιn(ksi+ιv∣∣)询
where, V = $瑞6)(Xi+ι — Mi cos(θ)), θ = arccos(MfXi+ι). This algorithm iterates through
the data points once, yielding a complexity of only O(γd), where d is the dimension of X .
Under the prescribed iteration, Mn converges asymptotically to the true Frechet mean for
finite data points. We refer the readers to (Chakraborty & Vemuri, 2015) for further details.
18
Under review as a conference paper at ICLR 2022
E Fast approximation of Moore-Penrose inverse
When computing the inverted kernel matrix Kinv in Algo. 1, conventional approaches
typically performs SVD or Cholesky decomposition. Both procedures are hard to parallelize,
and therefore, can be slow when K is large. Alternatively, we can utilize an iterative
procedure proposed in Razavi et al. (2014) to approximate the Moore-Penrose inverse.
Zi = K∕(kKkιkK k∞)	(15)
Zi+1 :=Zi(13I-KZi(15I-KZi(7I-KZi)))	(16)
where
nn
kKk1 = maxXKij, kKk∞ = max X Kij	(17)
ji
i=0	j=0
Since this iterative procedure mostly involves matrix multiplications, it can be efficiently
parallelized and implemented on GPU. The same procedure has also seen success in approx-
imating large self-attention matrices in language modeling (Xiong et al., 2021). For the
NVAE experiment, we run this iteration for 10 steps and use Kinv = Z10 .
19
Under review as a conference paper at ICLR 2022
F Nystrom Approximation of kPF
Due to the need to store and compute a kernel matrix inverse (K + λnI)-1 or K1, the
memory and computational complexity of kPF is at least O(n2) and O(n3), respectively.
The sup-quadratic complexity hinders the use of kPF on extremely large datasets. In our
experiments, we already adopted a simple subsampling strategy which randomly select 10k
training samples from each dataset (≤ 50k samples) to fit our hardware configuration which
works well. But for larger datasets with potentially more modes, a larger set of subsamples
must be considered, and in those cases kPF may not be suitable for commodity/affordable
hardware. In order to overcome this problem, we can combine kPF with conventional kernel
approximation methods such as the Nystrom method (Williams & Seeger, 2001).
Let (X?, Z?) be a size v subset of the training set (which we refer to as the landmark points)
and (Ψ?, Φ?) be their corresponding kernel feature maps. The weighting coefficients s for
each prior sample z* 〜Z derived in Alg. 1 can be approximated by
S = L(K + λnI户Φ>k(z?, ∙)
≈ Lψ? Wψ?L>? (Kφ? WΦ?K>? + λnI)tΦ>k(z?, ∙)
=Lψ?Wψ?L>?(λn)-1(I - K>?(λnWφ? + Kφ?K>?)tKφ*)Φ>k(z*,∙)	(18)
where Lψ? = Ψ>Ψ? ∈ Rn×v, Wψ? = Ψ>Ψ? ∈ Rv×v, Kφ? = Φ>Φ? ∈ Rn×v, Wφ? = Φ>Φ? ∈
Rv×v, and the last identity is due to applying the Woodbury formula on (KΦ? WΦt KΦ> +λnI)t.
Assuming v n, the memory complexity is reduced to O(nv) and the computation complexity
to O(nv2).
We empirically evaluated the Nystrom-approximated kPF on the CelebA experiment and
present the result in Tab. 4. It can be observed that when v is sufficiently large, the
performance of NyStrOm approximated kPFs is as good as the ones using the full kernel
matrices.
''n^W^∣ 100	500	1000	w/o Approximation
10,000 Il 45.9	41.6	42.3	41.8
30,000 Il 46.3	40.5	42.4		-	
50,000 Il 45.2	IZT	42.0		-	
Table 4: FIDs of samples generated by Nystrom-approximated NTK-kPF on CelebA. n
denotes the size of the training data subset we consider in computing kPF, while v denotes
the size of selected landmark points for NyStrOm approximation. Without approximation,
we cannot fit the kernel matrices onto a GPU with 11GB RAM when n > 10, 000. It is
worth noting that the approximated kPFs can perform similarly to the full kPF even with
V < 0.05n, which indicates that Nystrom approximation does not sacrifice much in terms of
performance while delivering significant efficiency gain.
20
Under review as a conference paper at ICLR 2022
G Does kPF Memorize Training Data?
Since in kPF, samples are generated by linearly interpolating between training samples, it
is natural to wonder whether it ‘fools’ the metrics by simply replicating training samples.
For comparison, we consider an alternative scheme that generates data through direct
manipulation of the training data, namely Kernel Density Estimation (KDE).
We fit KDEs by varying noise levels σ and compare their FIDs and nearest samples in the
latent space to kPF in Fig. 11. We observe that, although KDE can reach very low FIDs
when σ is small, almost all new samples closely resemble some instance in the training set,
which is a clear indication of memorization. In contrast, kPF can generate diverse samples
that do not simply replicate the observed data.
(a) KDE, σ = 0.005, FID: 30.9
(b) KDE, σ = 0.01, FID: 36.3
(c) KDE, σ = 0.02, FID: 48.0
Figure 11: Comparing KDE to kPF. Generated samples are presented in , followed by
their 5 nearest neighbors in the latent space (ordered from closest to furthest)
(d) kPF, FID: 41.0
21
Under review as a conference paper at ICLR 2022
H Assessing Sample Diversity
Although FID is one of the most frequently used measures for assessing sample quality
of generative models, certain diversity considerations, such as mode collapse, may not be
conveniently deduced from it (Sa jjadi et al., 2018). To enable explicit examination of
generative models with respect to both accuracy (i.e., generating samples within the support
of the data distribution) and diversity (i.e., covering the support of the data distribution
as much as possible), Sa jjadi et al. (2018) proposed an approach to evaluate generative
models with generalized definitions of precision and recall between distributions. Quality of
generation can then be assessed by evaluating the PRD curve, which depicts the trade-offs
between accuracy (precision) and diversity (recall). We present the PRD curves in Fig. 12.
The observations align with our results in Tab. 2 and kPF performs competitively in both
accuracy and sample diversity.
MNIST
6 4
■ ■
O O
UOLS3JJ
0.0
0.0	0.2	0.4	0.6
Recall
UO"'33Jd
CIFAR-10
0.0
0.0	0.2	0.4	0.6
Recall
CelebA
1.0
UO"'33Jd
0.0
0.0	0.2	0.4	0.6	0.8	1.0
Recall
Figure 12: PRD curves on all datasets. kPF is competitive to the other methods in terms of
Area Under Curve (AUC)
22
Under review as a conference paper at ICLR 2022
I Exploring Kernel Configurations
To investigate the implication of kernel choices on generation quality, we tested 25 different
kernel configurations for CelebA generation (results are presented in Tab. 5). For RBF kernels
used in the CelebA experiments of the main text, We use a bandwidth of σin = ，2∣Z∣∕8 ≈ 0.71
when used as input kernel and σout ≈ 0.34, and we adopt the same notation here.
^^^^ Output Input^∖liernel kernel	^^^^	RBF (σ = σout∕4)	RBF (σ = σout /2)	RBF (σ = σout )	RBF (σ = 2σout )	RBF (σ = 4σout )
RBF (σ = σin /2)	41.50	41.20	41.21	50.92	66.06
RBF (σ = σin)	41.90	42.11	41.91	45.83	50.70
RBF (σ = 2σin)	42.20	42.82	42.69	65.76	70.19
NTK (L = 8, w = 10, 000)	41.90	41.56	41.73	37.86	37.89
Arccos (L = 1, deg= 1)	41.71	42.03	42.22	52.83	63.13
Table 5: FID table for different kernel configurations.
It can be seen that kernel configurations and parameters indeed has a non-trivial impact on
the generation quality, with NTK-kPF being the most robust to the choice of parameters.
This aligns with our previous observations and offers some support for using NTK as an
input kernel despite the additional compute cost.
23
Under review as a conference paper at ICLR 2022
J Experimental Details
In this section, we provide the detailed specifications for all of our experiments. We have
also provided our code in the supplemental material.
J.1 Density Estimation on Toy Densities
We generated 10000 samples from each of the toy densities to learn the kPF operator. The
input kernel k is a ReLU-activated NTK corresponding to a fully-connected network with
depth L = 4 and width w = 10000 at each layer, and the output kernel l is a Gaussian kernel.
Unless specified otherwise, we always uses a Gaussian kernel as the output kernel for the
remainder of this appendix. The bandwidth of the output kernel was adjusted separately for
density estimation and sampling for the purpose of demonstration. For comparison, we also
fit/estimate a 10-component GMM and a Glow model with 50 coupling layers, where each of
them were trained until convergence.
J.2 Image Generation with Computer Vision Datasets
To generate results in Tab. 2 and Tab. 3, we first trained an autoencoder for each dataset
following the model setup in (Ghosh et al., 2020), which uses a modified Wasserstein
autoencoder (Tolstikhin et al., 2018) architecture with batch normalization. Additionally,
we applied spectral normalization on both the encoder and the decoder, following (Ghosh
et al., 2020), to obtain a regularized autoencoder. The latent representations were pro jected
onto a hypersphere before decoding to image space. We trained the models on two NVIDIA
GTX 1080TI GPUs. A detailed model specification is provided below in Table 6.
We used an NTK with L = 8 and w = 10000 as the input kernel k (i.e. the embedding
kernel of PZ) for NTK-kPF, and a Gaussian kernel with bandwidth σin = <2∣Z∣∕8 for
RBF-kPF. The bandwidth for the output Gaussian kernel is selected by grid search over
{2-i * σdata∣i ∈ [8]}, where σdata is the empirical data standard deviation, based on Cross-
validation of a degree 3 polynomial kernel MMD between the sampled and the ground-truth
latent points. Further, to mitigate the deterioration of performance of kernel methods in
a high-dimensional setting due to the curse of dimensionality (Evangelista et al., 2006), in
practice, we model Z as a space with fewer dimensions than the input space X . As a rule of
the thumb, we choose Z such that |Z| = |X |/4.
To generate images from kPF learned on NVAE latent space, we used the pre-trained
checkpoints provided in (Vahdat & Kautz, 2020) to obtain the latent embeddings for 2000
FFHQ images. We then construct the kPF from the concatenated latent space of the lowest
resolution (8 × 8). During sampling, prior samples at those resolutions are replace by the
kPF samples, while for other resolutions samples remain generated from inferred Gaussian
distributions. The batchnorm statistics were readjusted for 500 iterations following (Vahdat
& Kautz, 2020). We use rbf kernels as input and output kernels, with bandwidths σk , σl
chosen by the median heuristic (〜100 for input and 〜70 for output in our experiments).
J.3 Image Generation for Brain Images
For the high-resolution brain imaging dataset, we used a custom version of ResNet (He et al.,
2016) with 3D convolutions. The detailed architecture is shown in Fig. 7. Due to the large
size of the data, we trained the model on 4 NVIDIA Tesla V100 GPUs.
Mandatory ADNI statement regarding data use. Data used in preparation of this
article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database
(adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design
and implementation of ADNI and/or provided data but did not participate in analysis
or writing of this report. A complete listing of ADNI investigators can be found in the
ADNI Acknowledgement List.
The T1 MR brain dataset we utilize consists of images from 184 subjects diagnosed with
Alzheimers’s disease and 292 healthy controls/ normal subjects. Images were first coregistered
24
Under review as a conference paper at ICLR 2022
	MNIST	CIFAR-10	CelebA	_		5x5 conv, stride 4
					ResBlock64 × 2 ResBlock64 × 2 ResBlock128 × 2 ResBlock256 × 2
Encoder	Conv1×84 Conv2×64 Conv5×24 Conv4×4	Conv1×84 Conv425×64 Conv451×24 Conv4×244	Conv1×85 Conv2×65 Conv5×25 Conv1×24	Encoder	
					ResBlock256 × 2 ResBlock128 × 2 ResBlock64 × 2 ResBlock64 × 2
Decoder	COnVT4 ×24 ConvT4×64 ConvT4×4	COnVT5×24 ConvT425×64 ConvT34×4	ConvT5×25 ConvT2×65 ConvTl×85 ConvT5×5	Decoder	
					5x5 conv, stride 4
Table 6: Model architecture for computer vision
experiments. Subscript denotes the number of out-
put channels and superscript denotes the window
size of the convolution kernel. Batch normaliza-
tion and activation is applied between each pair of
convolution layers
Table 7: Model architecture for exper-
iments for image generation on brain
imaging dataset. Subscript denotes
the number of output channels. Up-
sampling and downsampling are per-
formed using strided convolutions.
to a MNI template and segmented to preserve only the white matter and grey matter. Then,
all images were resliced and resized to 160 × 196 × 160 and rescaled to the range of [-1, 1].
Voxel-based morphometry (VBM) was used to obtain the p-value map of data and generated
images.
25
Under review as a conference paper at ICLR 2022
K More Samples
In this section we present additional uncurated set of samples on MNIST, CIFAR-10, CelebA
based on pre-trained SRAE and FFHQ based on NVAE. From the figures, it can be seen that
kPF produces consistent and diverse samples, often better in quality than the alternatives.
O/
3
r
O 2
7Z
2 /
Z 3 4>√
q 0。27夕
3 Z √ 7s S
3g∕ 2 9。
3 S 06 4 O
nf√e N ” q /
2S∕SJ 77J-
6 ttn∕70lr
7z y∕57g g
∖∕22O q>2
33ΠJ5o∕f 夕
KU2√6 0「夕
□3 //，78 夕
3,MZz。。T
33 — 2丁^/乙
51vg7∕GJ
ΓΛ 0177//
/ —07 03 S C
ɑε>y∕6∕%o
0 4 50 5 9^4
7 904 6 3/33
4,I /9。6 O
73β0 of Λ 8/
3 J 7067/3
Figure 13: MNIST results from SRAEGMM (top left), SRAEGlow (top right) and our
SRAENTK-kPF.
26
Under review as a conference paper at ICLR 2022
Figure 14: CIFAR results from SRAEGMM (top left), SRAEGlow (top right) and our
SRAENTK-kPF.
27
Under review as a conference paper at ICLR 2022
Figure 15: CelebA results from SRAEGMM (top left), SRAEGlow (top right) and our
SRAENTK-kPF (bottom).
28
Under review as a conference paper at ICLR 2022
Figure 16: Additional samples from kPF+NVAE pre-trained on FFHQ
29