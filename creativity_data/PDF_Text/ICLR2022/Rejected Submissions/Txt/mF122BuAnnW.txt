Under review as a conference paper at ICLR 2022
Localized Randomized Smoothing
for Collective Robustness Certification
Anonymous authors
Paper under double-blind review
Ab stract
Models for image segmentation, node classification and many other tasks map a
single input to multiple labels. By perturbing this single shared input (e.g. the
image) an adversary can manipulate several predictions (e.g. misclassify several
pixels). A recent collective robustness certificate provides strong guarantees on
the number of predictions that are simultaneously robust. This method is however
limited to strictly local models, where each prediction is associated with a small
receptive field. We propose a more general collective certificate for the larger class
of softly local models, where each output is dependent on the entire input but as-
signs different levels of importance to different input regions (e.g. based on their
proximity in the image). The certificate is based on our novel localized random-
ized smoothing approach, where the random perturbation strength for different
input regions is proportional to their importance for the outputs. The resulting lo-
cally smoothed model yields strong collective guarantees while maintaining high
prediction quality on both image segmentation and node classification tasks.
1	Introduction
There is a wide range of tasks that require models making multiple predictions based on a single in-
put. For example, semantic segmentation requires assigning a label to each pixel in an image. When
deploying such multi-output classifiers in practice, their robustness should be a key concern. After
all - just like simple classifiers (Szegedy et al., 2014) - they can fall victim to adversarial attacks
(Xie et al., 2017; Zugner & Gunnemann, 2019; Belinkov & Bisk, 2018). Even without an adversary,
random noise or measuring errors could cause one or multiple predictions to unexpectedly change.
In the following, we derive a method that provides provable guarantees on how many predictions
can be changed by an adversary. Since all outputs operate on the same input, they also have to be
attacked simultaneously by choosing a single perturbed input. While attacks on a single prediction
may be easy, attacks on different predictions may be mutually exclusive. We have to explicitly
account for this fact to obtain a proper collective robustness certificate that provides tight bounds.
There already exists a dedicated collective robustness certificate for multi-output classifiers
(Schuchardt et al., 2021), but it is only benefical for models we call strictly local, where each output
depends only on a small, well-defined subset of the input. One example are graph neural networks
that classify each node in a graph based only on its neighborhood. Multi-output classifiers used in
practice, however, are often only softly local. While - unlike strictly local models - all of their pre-
dictions are in principle dependent on the entire input, each output may assign different importance
to different components. For example, deep convolutional networks used for image segmentation
can have very small effective receptive fields (Luo et al., 2016; Liu et al., 2018b), i.e. primarily
use a small region of the input in labeling each pixel. Many models used in node classification are
based on the homophily assumption that connected nodes are mostly of the same class. Thus, they
primarily use features from neighboring nodes to classify each node. Even if an architecture is not
inherently softly local, a model may learn a softly local mapping through training. For example, a
transformer (Vaswani et al., 2017) can in principle attend to any part ofan input sequence. However,
in practice the learned attention maps may be ”sparse”, with the prediction for each token being
determined primarily by a few (not necessarily nearby) tokens (Shi et al., 2021).
While an adversarial attack on a single prediction ofa softly local model is conceptually no different
from that on a single-output classifier, attacking multiple predictions simultaneously can be much
1
Under review as a conference paper at ICLR 2022
Figure 1: Localized randomized smoothing applied to semantic segmentation. We assume that the
most relevant information for labeling a pixel is contained in other nearby pixels. We partition the
input image into multiple grid cells. For each grid cell, we sample noisy images from a localized
smoothing distribution that applies more noise to far-away, less relevant grid cells. Segmenting all
noisy images using base model g, cropping the result and computing the majority vote (i.e. the most
common label for each pixel) yields a local segmentation mask. These per-cell segmentation masks
can then be combined into a complete segmentation mask.
more challenging. By definition, adversarial attacks have to be unnoticeable, meaning the adversary
only has a limited budget for perturbing the input. When each output is focused on a different part of
the input, the adversary has to decide on where to allocate their adversarial budget and may be unable
to attack all outputs at once. Our collective robustness certificate explicitly accounts for this budget
allocation problem faced by the adversary and can thus provide stronger robustness guarantees.
Our certificate is based on randomized smoothing (LiU et al., 2018a; Lecuyer et al., 2019; Cohen
et al., 2019). Randomized smoothing is a versatile black-box certification method that has originally
been proposed for single-output classifiers. Instead of directly analysing a model, it constructs a
smoothed classifier that returns the most likely prediction of the model under random perturbations
of its input. One can then use statistical methods to certify the robustness of this smoothed classifier.
We discuss more details in Section 2. Randomized smoothing is typically used with i.i.d. noise:
Each part of the input (e.g. each pixel) independently undergoes random perturbations sampled
from the same noise distribution. One can however also use non-i.i.d. noise (Eiras et al., 2021). This
results in a smoothed classifier that is certifiably more robust to parts of the input that are smoothed
with higher noise levels (e.g. larger standard deviation).
We apply randomized smoothing to softly-local multi-output classifiers in a scheme we call local-
ized randomized smoothing: Instead of using the same smoothing distribution for all outputs, we
randomly smooth each output (or set of outputs) using a different non-i.i.d. distribution that matches
its inherent soft locality. Using a low noise level for the most relevant parts of the input allows us
to retain a high prediction quality (e.g. accuracy). Less relevant parts of the input can be smoothed
with a higher noise level. The resulting certificates (one per output) explicitly quantify how robust
each prediction is to perturbations of which section of the input - they are certificates of soft locality.
After certifying each prediction independently using localized randomized smoothing, we construct
a (mixed-integer) linear program that combines these per-prediction base certificates into a collective
certificate that provably bounds the number of simultaneously attackable predictions. This linear
program explicitly accounts for soft locality and the budget allocation problem it causes for the
adversary. This allows us to prove much stronger guarantees of collective robustness than simply
certifying each prediction independently. Our core contributions are:
•	Localized randomized smoothing, a novel smoothing scheme for multi-output classifiers.
•	A variance smoothing method for efficiently certifying smoothed models on discrete data.
•	A collective certificate that leverages our identified common interface for base certificates.
2	Background and related work
Randomized smoothing. Randomized smoothing is a flexible certification technique that can
be used for various data types, perturbation models and tasks. For simplicity, we focus on a
2
Under review as a conference paper at ICLR 2022
classification certificate for l2 perturbations (Cohen et al., 2019). Assume we have a continu-
ous D-dimensional input space RD, a label set Y and a classifier g : RD → Y. We can use
isotropic Gaussian noise with standard deviation σ ∈ R+ to construct the smoothed classifier
f = argmaxy∈γ Prz〜N(χ,σ) [g(z) = y] that returns the most likely prediction of base classifier
g under the input distribution 1 . Given an input x ∈ RD and the smoothed prediction y = f (x), we
want to determine whether the prediction is robust to all l2 perturbations of magnitude , i.e. whether
∀x0 : ||x0 - x∣∣2 ≤ e : f (x0) = y. Let q = Prz 〜N (χ,σ) [g(x) = y] be the probability of g predicting
label y. The prediction of our smoothed classifier is robust if e < σΦ-1(q) (Cohen et al., 2019).
This result showcases a trade-off we alluded to in the previous section: The certificate can become
stronger if the noise-level (here σ) is increased. But doing so could also lower the accuracy of the
smoothed classifier or reduce q and thus weaken the certificate.
White-box certificates for multi-output classifiers. There are multiple recent methods for certify-
ing the robustness of specific multi-output models (see, for example, (Tran et al., 2021; Zugner &
Gunnemann, 2019; Bojchevski & Gunnemann, 2019; Zugner & Gunnemann, 2020; Ko et al., 2019;
Ryou et al., 2021; Shi et al., 2020; Bonaert et al., 2021)) by analyzing their specific architecture and
weights. They are however not designed to certify collective robustness. They can only determine
independently for each prediction whether or not it can be adversarially attacked.
Collective robustness certificates. Most directly related to our work is the certificate of Schuchardt
et al. (2021). Like ours, it combines many per-prediction certificates into a collective certificate.
But, unlike our novel localized smoothing approach, their certification procedure is only beneficial
for strictly local models, i.e. models whose outputs operate on small subsets of the input. Further-
more, their certificate assumes binary data, while our certificate defines a common interface for
various data types and perturbation models. A more detailed comparison can be found in Section D.
Recently, Fischer et al. (2021) proposed a certificate for semantic segmentation. They consider a dif-
ferent notion of collective robustness: They are interested in determining whether all predictions are
robust. In Section C.4 we discuss their method in detail and show that, when used for certifying our
notion of collective robustness (i.e. the number of robust predictions), their method is no better than
certifying each output independently using the certificate of Cohen et al. (2019). Furthermore, our
certificate can be used to provide equally strong guarantees for their notion of collective robustness
by checking whether the number of certified predictions equals the overall number of predictions.
Another method that can be used for certifying collective robustness is center smoothing (Kumar &
Goldstein, 2021). Center smoothing bounds how much a vector-valued prediction changes w.r.t to a
distance function under adversarial perturbations. With the l0 pseudo-norm as the distance function,
center smoothing bounds how many predictions of a classifier can be simultaneously changed.
Randomized smoothing with non-i.i.d. noise. While not designed for certifying collective ro-
bustness, two recent certificates for non-i.i.d. Gaussian (Fischer et al., 2020) and uniform smooth-
ing (Eiras et al., 2021) can be used as a component of our collective certification approach: They
can serve as per-prediction base certificates, which can then be combined into our stronger collec-
tive certificate (more details in Section 4) . Note that we do not use the procedure for optimizing the
smoothing distribution proposed by Eiras et al. (2021), as this would enable adversarial attacks on
the smoothing distribution itself and invalidate the certificate (see discussion by Wang et al. (2021)).
3	Collective threat model
Before certifying robustness, we have to define a threat model, which specifies the type of model
that is attacked, the objective of the adversary and which perturbations they are allowed to use. We
assume that we have a multi-output classifier f : XDin → YDout, that maps from a Din-dimensional
vector space to Dout labels from label set Y. We further assume that this classifier f is the result of
randomly smoothing a base classifier g, as discussed in Section 2. To simplify our notation, we write
fn to refer to the function x 7→ f (x)n that outputs the n-th label. Given this multi-output classifier
f, an input x ∈ XDin and the resulting vector of predictions y = f (x), the objective of the adversary
is to cause as many predictions from a set of targeted indices T ⊆ {1, . . . , Dout} to change. That
is, their objective is minx0∈Bx Pn∈T I [fn(x0) = yn], where Bx ⊆ XDin is the perturbation model.
Importantly, note that the minimization operator is outside the sum, meaning the predictions have to
1 In practice, all probabilities have to be estimated using Monte Carlo sampling (see discussion in Section C).
3
Under review as a conference paper at ICLR 2022
be attacked using a single input. As is common in robustness certification, we assume a norm-bound
perturbation model. That is, given an input x ∈ XDin, the adversary is only allowed to use perturbed
inputs from the set Bx = {x0 ∈ XDin | ||x0 - x||p ≤ e} withp, e ≥ 0.
4	A recipe for collective certificates
Before discussing technical details, we provide a high-level overview of our method. In localized
randomized smoothing, we assign each output gn of a base classifier g its own smoothing distribution
Ψ(n) that matches our assumptions or knowledge about the base classifier’s soft locality, i.e. for each
n ∈ {1, . . . , Dout} choose a Ψ(n) that induces more noise in input components that are less relevant
for gn. For example, in Fig. 1, we assume that far-away regions of the image are less relevant and
thus perturb pixels in the bottom left with more noise when classifying pixels in the top-right corner.
The chosen smoothing distributions can then be used to construct the smoothed classifier f.
Given an input x ∈ XDin and the corresponding smoothed prediction y = f (x), randomized
smoothing makes it possible to compute per-prediction base certificates. That is, for each yn, one
can compute a set H(n) ⊆ XDin of perturbed inputs that the prediction is robust to, i.e. ∀x0 ∈ Hn :
fn (x0) = yn. Our motivation for using non-i.i.d. distributions is that the H(n) will guarantee more
robustness for input dimensions smoothed with more noise, i.e. quantify model locality.
The objective of our adversary is minx0∈Bx n∈T I [fn(x0) = yn] with collective perturbation
model Bx ⊆ XDin . That is, they want to change as many predictions from the targeted set T as
possible. A trivial lower bound can be obtained by counting how many predictions are - according
to the base certificates - provably robust to the collective threat model. This can be expressed as
Pn∈τ minxo∈Bx I [x0 ∈ H(n)]. In the following, we refer to this as the naive Collective certificate.
Thanks to our proposed localized smoothing scheme, we can use the following, tighter bound:
xm0∈iBnx X I [fn(x0) = yn] ≥ xm0∈iBnx X I x0 ∈ H(n) ,
x n∈T	x n∈T
(1)
which preserves the fact that the adversary has to choose a single perturbed input. Because we use
different non-i.i.d. smoothing distributions for different outputs, we provably know that each fn has
varying levels of robustness for different parts of the input and that these robustness levels differ
among outputs. Thus, in the r.h.s. problem the adversary has to allocate their limited budget across
various input dimensions and may be unable to attack all predictions at once, just like when attacking
the classifier in the l.h.s. objective (recall Section 1). This makes our collective certificate stronger
than the naive collective certificate, which allows each prediction to be attacked independently.
As stated in Section 1, the idea of combining base certificates into stronger collective certificates
has already been explored by Schuchardt et al. (2021). But instead of using localized smoothing
to capture the (soft) locality of a model, their approach leverages the fact that perturbations outside
an output’s receptive field can be ignored. For softly local models, which have receptive fields
covering the entire input, their certificate is no better than the naive certificate. Another novel
insight underlying our approach is that various non-i.i.d. randomized smoothing certificates share
a common interface, which makes our method applicable to diverse data types and perturbation
models. In the next section, we formalize this common interface. We then discuss how it allows us
to compute the collective certificate from Eq. 1 using (mixed-integer) linear programming.
5	Common interface for base certificates
A base certificate for a prediction yn = fn (x) is a set Hn ⊆ XDin of perturbed inputs that yn is
provably robust to, i.e ∀x0 ∈ Hn : fn (x0) = yn . Note that base certificates do not have to be exact,
but have to be sound, i.e. they do not have to specify all inputs to which the fn are robust but they
must not contain any adversarial examples. As a common interface for base certificates, we propose
that the sets Hn are parameterized by a weight vector w (n) ∈ RDin and a scalar η(n) that define a
linear constraint on the element-wise distance between perturbed inputs and the clean input:
H(n) =	x0 ∈ XDin
Din
XWdn) ∙ |xd- xd|K <η(n)	.
d=1
(2)
4
Under review as a conference paper at ICLR 2022
The weight vector encodes how robust yn is to perturbations of different components of the input.
The scalar κ is important for collective robustness certification, because it encodes which collective
perturbation model the base certificate is compatible with. For example, κ = 2 means that the base
certificate can be used for certifying collective robustness to l2 perturbations.
In the following, we present two base certificates implementing our interface: One for l2 perturba-
tions of continuous data and one for perturbations of binary data. In Section B, we further present a
certificate for binary data that can distinguish between adding and deleting bits and a certificate for
l1 perturbations of continuous data. All base certificates guarantee more robustness for parts of the
input smoothed with a higher noise level. The certificates for continuous data are based on known
results (Fischer et al., 2020; Eiras et al., 2021) and merely reformulated to match our proposed in-
terface, so that they can be used as part of our collective certification procedure. The certificates for
discrete data however are original and based on the novel concept of variance smoothing.
Gaussian smoothing for l2 perturbations of continuous data The first base certificate is a gen-
eralization of Gaussian smoothing to anisotropic noise, a corollary of Theorem A.1 from (Fischer
et al., 2020). In the following, diag(z) refers to a diagonal matrix with diagonal entries z and
Φ-1 : [0, 1] → R refers to the the standard normal inverse cumulative distribution function.
Proposition 1. Given an output gn : RDin → Y, let fn(x) = argmaxy∈γ Prz~n(χ,∑) [gn(z) = y]
be the corresponding smoothed output with Σ = diag (σ)2 andσ ∈ R+Din. Given an input x ∈ RDin
and smoothed prediction yn = fn(x), let q = Piz~N(χ,∑) [gn(z) = yn]∙ Then, ∀x0 ∈ H(n):
fn(x0) = yn with H(n) defined as in Eq. 2, Wd =白,η = (Φ(-1) (q))2 and K = 2.
Bernoulli variance smoothing for perturbations of binary data For binary data, we use a smooth-
ing distribution F(x, θ) with θ ∈ [0, 1]Din that independently flips the d’th bit with probability θd,
i.e. for x, Z ∈ {0,1}Din and Z ~ F(x, θ) We have Pr[zd = xd] = θd. A corresponding certificate
could be derived by generalizing (Lee et al., 2019), which considers a single shared θ ∈ [0, 1] with
∀d : θd = θ. HoWever, the cost for computing this certificate Would be exponential in the number
of unique values in θ . We therefore propose a more efficient alternative. Instead of constructing
a smoothed classifier that returns the most likely labels of the base classifier (as discussed in Sec-
tion 2), We construct a smoothed classifier that returns the labels With the highest expected softmax
scores (similar to CDF-smoothing (Kumar et al., 2020)). For this smoothed model, We can com-
pute a robustness certificate in constant time. The certificate requires determining both the expected
value and variance of softmax scores. We therefore call this method variance smoothing. While We
use it for binary data, it is a general-purpose technique that can be applied to arbitrary domains and
smoothing distributions (see discussion in Section B.2). In the folloWing, We assume the label set Y
to consist of numerical labels {1, . . . , |Y|}, Which simplifies our notation.
Theorem 1. Given an output gnlι : {0,1}Din → ∆∣γ∣ mapping to scores from the |Y|-dimensional
probability simplex, let fn(x) = argmaXy∈γEz~F(x,e)[gn, (z)y] be the corresponding smoothed
classifier with θ ∈ [0, 1]Din. Given an input x ∈ {0, 1}Din and smoothed prediction yn = fn(x),
let μ = Ez~F(x,θ) [gn(z)y] and σ2 = Varz~f(⑦⑼[gn(z)y]. Then, ∀x0 ∈ H(n) : fn(x0) = yn
with H(n) defined as in Eq. 2, Wd = ln ((I-：d) + ɪθdθɪ), η = ln(1 + σ⅛ (μ 一 1 )2) and K = 0.
6	Computing the collective robustness certificate
With our common interface for base certificates in place, We can discuss hoW to compute the collec-
tive robustness certificate minx0∈Bx Pn∈T I x0 ∈ H(n) from Eq. 1. The result bounds the number
of predictions yn With n ∈ {1, . . . , Dout} that can be simultaneously attacked by the adversary. In
the folloWing, We assume that the base certificates Were obtained by using a smoothing distribution
that is compatible With our lp collective perturbation model (i.e. K = p), for example by using Gaus-
sian noise for p = 2 or Bernoulli noise for p = 0. Inserting the definition of our base certificate
interface from Eq. 2 and reWriting our perturbation model Bx = x0 ∈ XDin | ||x0 一 x||p ≤ as
x0 ∈ XDin | PdD=in1 |x0d 一 xd|p ≤ p , our objective from Eq. 1 can be expressed as
min
x0∈XDin
Din
EWdn) ∙ |xd - xd|p <η(n)
d=1
Din
s.t.	|x0d 一 xd|p ≤ p.
d=1
(3)
I
5
Under review as a conference paper at ICLR 2022
We can see that the perturbed input x0 only affects the element-wise distances |x0d-xd|p. Rather than
optimizing x0, we can instead directly optimize these distances, i.e. determine how much adversarial
budget is allocated to each input dimension. For this, we define a vector of variables b ∈ R+Din (or
b ∈ {0, 1}Din for binary data). Replacing sums with inner products, we can restate Eq. 3 as
min X I hbT w(n) < η(n)i	s.t. sum{b} ≤ p.	(4)
b∈R+Din n∈T
In a final step, we replace the indicator functions in Eq. 4 with a vector of boolean variables t ∈
{0,1}Dout. Define the constants η(n) = ep ∙ min(0, mind Wdn)). Then,
min	^X tn	s.t.	∀n	:	bτw(n)	≥ tn,η(n) + (1 —	tn)η(n),	sum{b}	≤	ep.	(5)
b∈RDin ,t∈{0,1}Dout n∈τ	一
is equivalent to Eq. 4. The first constraint guarantees that tn can only be set to 0 if the l.h.s. is
greater or equal η(n), i.e. only when the base certificate can no longer guarantee robustness. The
term involving η(n) ensures that for tn = 1 the problem is always feasible2. Eq. 5 can be solved
using any mixed-integer linear programming solver.
While the resulting MILP bears some semblance to that of Schuchardt et al. (2021), itis conceptually
different. When evaluating their base certificates, they mask out parts of the budget vector b based
on a model’s strict locality, while we weigh the budget vector based on the soft locality guaranteed
by the base certificates. In addition, thanks to the interface specified in Section 5, our problem
only involves a single linear constraint per prediction, making it much smaller and more efficient
to solve. Interestingly, when using randomized smoothing base certificates for binary data, our
certificate subsumes theirs, i.e. can provide the same robustness guarantees (see Section D.2).
Improving efficiency. Still, the efficiency of our certificate in Eq. 5. certificate can be further
improved. In Section A, we show that partitioning the outputs into Nout subsets sharing the same
smoothing distribution and the the inputs into Nin subsets sharing the same noise level (for example
like in Fig. 1), as well as quantizing the base certificate parameters η(n) into Nbin bins reduces
the number of variables and constraints from Din + Dout and Dout + 1 to Nin + Nout ∙ Nbins
and Nout ∙ Nbins + 1, respectively.We can thus control the problem size independent of the data's
dimensionality. We further derive a linear relaxation of the mixed-integer problem, which can be
more efficiently solved while preserving the soundness of the certificate.
7	Limitations
The main limitation of our approach is that it assumes softly local models. While it can be applied to
arbitrary multi-output classifiers, it may not necessarily result in better certificates than randomized
smoothing with i.i.d. distributions. Furthermore, choosing the smoothing distributions requires some
a-priori knowledge or assumptions about which parts of the input are how relevant to making a
prediction. Our experiments show that natural assumptions like homophily can be sufficient for
choosing effective smoothing distributions. But doing so in other tasks may be more challenging.
A limitation of (most) randomized smoothing certificates is that they use sampling to approximate
the smoothed classifier. Because we use different smoothing distributions for different outputs, we
can only use a fraction of the samples for each output. As discussed in Section A.1, we can alleviate
this problem by sharing smoothing distributions among multiple outputs. Our experiments show that
despite this issue, our method outperforms certificates that use a single smoothing distribution. Still,
future work should try to improve the sample efficiency of randomized smoothing (for example by
developing more methods for de-randomized smoothing (Levine & Feizi, 2020)).Any such advance
could then be incorporated into our localized smoothing framework.
8	Experimental evaluation
Our experimental evaluation has three objectives 1.) Verifying our main claim that localized random-
ized smoothing offers a better trade-off between accuracy and certifiable robustness than smoothing
2Because η(n) is the smallest value bT w(n) can take on, i.e. min	Din bT wd(n) s.t. sum{b} ≤ p.
b ∈R.
6
Under review as a conference paper at ICLR 2022
with i.i.d. distributions. 2.) Determining to what extend the linear program underlying the proposed
collective certificate strengthens our robustness guarantees. 3.) Assessing the efficacy of our novel
variance smoothing certificate for binary data. Any of the used datasets and classifiers only serve
as a means of comparing certificates. We thus use well-known and well-established architectures
instead of overly focusing on maximizing prediction accuracy by using the latest SOTA models.
We use two metrics to quantify certificate strength: Certified accuracy (i.e. the percentage of correct
and certifiably robust predictions) and certified ratio (i.e. the percentage of certifiably robust pre-
dictions, regardless of correctness)3. As single-number metrics, we report the AUC of the certified
accuracy/ratio functions w.r.t. adversarial budget (not to be confused with certifying some AUC
metric). For localized smoothing, We evaluate both the naive collective certificate, i.e. certifying
predictions independently (see Section 4), and the proposed LP-based certificate (using the linearly
relaxed version from Appendix A.4). We compare our method to tWo baselines using i.i.d. random-
ized smoothing: The naive collective certificate and center smoothing (Kumar & Goldstein, 2021).
For softly local models, the certificate of Schuchardt et al. (2021) is equivalent to the naive baseline.
When used to certify the number of robust predictions, the segmentation certificate of Fischer et al.
(2021) is at most as strong as the naive baseline (see Section C.4). Thus, our method is compared
to all existing collective certificates listed in Section 2. In all experiments, We use Monte Carlo
randomized smoothing. More details on the experimental setup can be found in Section E.
8.1	Semantic segmentation
Dataset and model. We evaluate our certifi-
cate for continuous data and l2 perturbations
on the Pascal-VOC 2012 segmentation valida-
tion set. Training is performed on 10582 pairs
of training samples extracted from SBD4 (Har-
iharan et al., 2011), To increase batch sizes
and thus alloW a more thorough investigation
of different smoothing parameters, all images
are doWnscaled to 50% of their original size.
Our base model is a U-Net segmentation model
With a ResNet-18 backbone. To obtain accu-
rate and robust smoothed classifiers, base mod-
els should be trained on the smoothing distribu-
tion. We thus train 51 different instances of our
base model, augmenting the training data With
a different σtrain ∈ {0, 0.01, . . . , 0.5}. At test
time, When evaluating a baseline i.i.d. certificate
With smoothing distribution N (0, σ), We load
the model trained With σtrain = σ. To perform
localized randomized smoothing, We choose pa-
rameters σmin , σmax ∈ R+ and partition all im-
ages into regular grids of size 4 × 6 (similar to
1.0
0 0.8
8 0.6
W 0.4
原0.2
0.0
0.0	0.5	1.0	1.5	2.0
Adversarial budget e
Figure 2: Certified ratios of U-Net models un-
der varying e. We compare the naive i.i.d. base-
line (σ = 0.4) to localized smoothing (σmin =
0.25, σmax = 1.5). Combining the base certifi-
cates via linear programming (solid orange line)
instead of evaluating them independently (dotted
orange line) outperforms the baseline.
example Fig. 1). To classify pixels in grid cell (i, j), We sample noise for grid cell (k, l) using
N(0, σ0), With σ0 ∈ [σmin, σmax] chosen proportional to the distance of (i, j) and (k, l) (more de-
tails in Section E.2.1). As the base model, We load the one trained With σtrain = σmin. Using the
same distribution at train and test time for the i.i.d. baselines but not for localized smoothing is meant
to skeW the results in the baseline’s favor. But, in Section E.2.3, We also repeat our experiments using
the same base model for i.i.d. and localized smoothing.
Evaluation. The main goal of our experiments on segmentation is to verify that localized smoothing
can offer a better trade-off betWeen accuracy and certifiable robustness. That is, for all or most
σ, there are σmin , σmax such that the locally smoothed model has higher accuracy and certifiable
collective robustness than i.i.d. smoothing baselines using N(0, σ). Because σ, σmin, σmax ∈ R+,
We can not evaluate all possible combinations. We therefore use the folloWing scheme: We focus
on the case σ ∈ [0, 0.5], Which covers all distributions used in (Kumar & Goldstein, 2021) and
3In the case of image segmentation, We compute these metrics per image and then average over the dataset.
4Also knoWn as ”Pascal trainaug”
7
Under review as a conference paper at ICLR 2022
(Fischer et al., 2021). First, we evaluate our two baselines for five σ ∈ {0.1, 0.2, 0.3, 0.4, 0.5}.
This results in baseline models with diverse levels of accuracy and robustness (e.g. the accuracy
of the naive baseline shrinks from 87.7% to 64.9% and the AUC of its certified accuracy grows
from 0.17 to 0.644). We then test whether, for each of the σ, we can find σmin, σmax such that the
locally smoothed models attains higher accuracy and is certifiably more robust. Finally, to verify
that {0.1, 0.2, 0.3, 0.4, 0.5} were not just a particularly poor choice of baseline parameters, we fix
the chosen σmin, σmax. We then perform a fine-grained search over σ ∈ [0, 0.5] with resolution 0.01
to find a baseline model that has at least the same accuracy and certifiable robustness (as measured
by certificate AUC) as any of the fixed locally smoothed models. If this is not possible, this provides
strong evidence that the proposed smoothing scheme and certificate indeed offer a better trade-off.
Fig. 2 shows one example. For σ = 0.4, the naive i.i.d. baseline has an accuracy of 72.5%. With
σmin = 0.25, σmax = 1.5, the proposed localized smoothing certificate yields both a higher accu-
racy of 76.4% and a higher certified accuracy for all . It can certify robustness for up to 1.825,
compared to 1.45 of the baseline and the AUC of its certified accuracy curve is 43.1% larger. Fig. 2
also highlights the usefulness of the linear program we derived in Section 5: Evaluating the local-
ized smoothing base certificates independently, i.e. computing the naive collective certificate (dotted
orange line), is not sufficient for outperforming the baseline. But combining them via the proposed
linear program drastically increases the certified accuracy
The results for all other combinations of smoothing distribution parameters, both baselines and both
metrics of certificate strength can be found in Section E.2.3. Tables 1 and 2 summarize the first part
of our evaluation procedure, in which we optimize the localized smoothing parameters. Safe for one
exception (with σ = 0.2, center smoothing has a lower accuracy, but slightly larger certified ratio),
the locally smoothed models have the same or higher accuracy, but provide stronger robustness
guarantees. The difference is particularly large for σ ∈ {0.3, 0.4, 0.5}, where the accuracy of models
smoothed with i.i.d. noise drops off, while our localized smoothing distribution preserves the most
relevant parts of the image to allow for high accuracy. Table 5 summarizes the second part of our
evaluation scheme, in which we perform a fine-grained search over [0, 0.5]. We find that there is no
σ such that either of the i.i.d. baselines can outperform any of the chosen locally smoothed models
w.r.t. AUC of their certified accuracy or certified ratio curves. This is ample evidence for our claim
that localized smoothing offers a better trade-off than i.i.d. smoothing. Also, the collective LPs
caused little computational overhead (avg. 0.68 s per LP, more details in Section E.2.3).
8.2	Node classification
Dataset and model. We evaluate our certificate for binary data on the Cora-ML node classification
dataset. We use two different base-models: Approximate Personalized Propagation of Neural Pre-
dictions (APPNP) (Klicpera et al., 2019) and a 6-layer Graph Convolutional network (GCN) (Kipf
& Welling, 2017). Both models have a receptive field that covers most or all of the graph, meaning
they are softly local. For details on model and training parameters, see Section E.3.1.
As center smoothing has only been derived for Gaussian smoothing, We only compare to the naive
baseline. For both, the baseline and our localized smoothing certificate, we use sparsity-aware ran-
domized smoothing (Bojchevski et al., 2020) , i.e. flip 1-bits and 0-bits with different probabilities
(θ- and θ+, respectively), which allows us to certify different levels of robustness to deletions and
additions of bits. With localized randomized smoothing, we use the variance smoothing base certifi-
cate derived in Section B.2.2. We choose the distribution parameters for localized smoothing based
on an assumption of homophily, i.e. nearby nodes are most relevant for classifying a node. We par-
tition the graph into 5 clusters and define parameters θm±in and θm±ax . When classifying a node in
cluster i, we randomly smooth attributes in cluster j with θi+j , θi-j that are based on linearly interpo-
lating in [θm-in, θm-ax] and [θm-in, θm-ax] based on the affinity of the clusters (details in Section E.3.1).
Evaluation. We first evaluate the new variance-based certificate and compare it to the certificate
derived by Bojchevski et al. (2020). For this, we use only one cluster, meaning we use the same
smoothing distribution for both. Fig. 11 in Section E.3 shows that the variance certificate is weaker
than the baseline for additions, but better for deletions. It appears sufficiently effective to be used as
a base certificate and integrated into a stronger, collective certificate.
The parameter space of our smoothing distributions is large. For the localized approach we have
four continuous parameters, as we have to specify both the minimal and maximal noise values.
8
Under review as a conference paper at ICLR 2022
O
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
10
20
30
40
50
O
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
10
20
30
40
50
Number of adversarial additions
Number of adversarial deletions
Figure 3: Certified accuracy for an APPNP model under varying number of attribute additions (left)
and deletions (right). We compare localized smoothing (θm+in = 0.05, θm-in = 0.65, θm+ax = 0.08,
θr-ax = 0.95) to two separately optimized naive i.i.d. smoothing baselines: addition (θ+ = 0.04,
θ - = 0.61) and deletion (θ+ = 0.04, θ- = 0.68). Combining the localized smoothing base
certificates via linear programming (solid orange line) instead of evaluating them independently
(dotted line) allows us to outperform the baselines for most adversarial budgets.
Therefore, it is difficult to show that our approach achieves a better accuracy-robustness trade-off
over the whole noise space. However, we can investigate the accuracy-robustness trade-off within
some areas of this space. For the localized approach we choose a few fixed combinations of the noise
parameters θ±min and θm±ax . To show our claim, we then optimise the baselines with parameters
in an interval around our θm+in and θm-in . This is a smaller space, as the baselines only have two
parameters. We select the baseline whose certified accuracy curve has the largest AUC. We perform
the search for the best baseline for the addition and deletion scenario independently, i.e., the best
baseline model for addition and deletion does not have to be the same.
In Fig. 3, we see the certified accuracy of an APPNP model for a varying number of attribute ad-
ditions and deletions (left and right respectively). To find the best distribution parameters for the
baselines, we evaluated combinations of θ+ ∈ {0.04, 0.055, 0.07} and θ- ∈ [0.1, . . . , 0.827], using
11 equally spaced values for the interval. For adversarial additions, the best baseline yields a certi-
fied accuracy curve with an AUC of 4.51 compared to our 5.65. The best baseline for deletions has
an AUC of 7.76 compared to our 16.26. Our method outperforms these optimized baselines for most
adversarial budgets, while maintaining the same clean accuracy (i.e. certified accuracy at = 0).
Experiments with different noise parameters and classifiers can be found in Section E.3. In general,
we find that we significantly outperform the baseline when certifying robustness to deletions, but
often have weaker certificates for additions (which may be inherent to the variance smoothing base
certificates). Due to the large continuous parameter space, we cannot claim that localized smoothing
outperforms the naive baseline everywhere. However, our results show that, for the tested parameter
regions, localized smoothing can provide a significantly better accuracy-robustness trade-off.
We found that using the collective LP instead of naively combining the base certificates can result in
much stronger certificates: The AUC of the certified accuracy curve (averaged over all experiments)
increased by 38.8% and 33.6% for addition and deletion, respectively. The collective LPs caused
little computational overhead (avg. 10.9 s per LP, more details in Section E.3.3).
9	Conclusion
In this work, we have proposed the first collective robustness certificate for softly local multi-output
classifiers. It is based on localized randomized smoothing, i.e. randomly smoothing different
outputs using different non-i.i.d. smoothing distributions matching the model’s locality. We have
shown how per-output certificates based on localized smoothing can be computed and that they
share a common interface. This interface allows them to be combined into a strong collective ro-
bustness certificate. Experiments on image segmentation and node classification tasks demonstrate
that localized smoothing can offer a better robustness-accuracy trade-off than existing randomized
smoothing techniques. Our results show that locality is linked to robustness, which suggests the
research direction of building more effective local models to robustly solve multi-output tasks.
9
Under review as a conference paper at ICLR 2022
10	Reproducibility s tatement
We prove all theoretic results that were not already derived in the main text in Appendices A to C. To
ensure reproducibility of the experimental results we provide detailed descriptions of the evaluation
process with the respective parameters in Section E.2 and Section E.3. Code will be made available
to reviewers via an anonymous link posted on OpenReview, as suggested by the guidelines.
11	Ethics s tatement
In this paper, we propose a method to increase the robustness of machine learning models against
adversarial perturbations and to certify their robustness. We see this as an important step towards
general usage of models in practice, as many existing methods are brittle to crafted attacks. Through
the proposed method, we hope to contribute to the safe usage of machine learning. However, robust
models also have to be seen with caution. As they are harder to fool, harmful purposes like mass
surveillance are harder to avoid. We believe that it is still necessary to further research robustness
of machine learning models as the positive effects can outweigh the negatives, but it is necessary to
discuss the ethical implications of the usage in any specific application area.
References
T.W. Anderson. Confidence limits for the value of an arbitrary bounded random variable with a con-
tinuous distribution function. In Bulletin of The International and Statistical Institute, volume 43,
pp. 249-251,1969.
Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine
translation. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=BJ8vJebC-.
Aleksandar Bojchevski and Stephan Gunnemann. Certifiable robustness to graph perturbations. In
Advances in Neural Information Processing Systems, volume 32, pp. 8319-8330, 2019.
Aleksandar Bojchevski, Johannes Klicpera, and Stephan Gunnemann. Efficient robustness certifi-
cates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In
International Conference on Machine Learning, pp. 1003-1013. PMLR, 2020.
Gregory Bonaert, Dimitar I. Dimitrov, Maximilian Baader, and Martin Vechev. Fast and precise
certification of transformers. In Proceedings of the 42nd ACM SIGPLAN International Confer-
ence on Programming Language Design and Implementation, PLDI 2021, pp. 466-481, New
York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383912. doi:
10.1145/3453483.3454056. URL https://doi.org/10.1145/3453483.3454056.
Carlo Bonferroni. Teoria statistica delle classi e calcolo delle probabilita. Pubblicazioni del R
Istituto Superiore di Scienze Economiche e Commericiali di Firenze, 8:3-62, 1936.
G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.
Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin,
and Alexandr A. Kalinin. Albumentations: Fast and flexible image augmentations. Information,
11(2), 2020. ISSN 2078-2489. doi: 10.3390/info11020125. URL https://www.mdpi.com/
2078-2489/11/2/125.
C. J. Clopper and E. S. Pearson. The use of confidence or fiducial limits illustrated in the case of the
binomial. Biometrika, 26(4):404-413, 1934. ISSN 00063444. URL http://www.jstor.
org/stable/2331986.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 1310-1320. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.
press/v97/cohen19c.html.
10
Under review as a conference paper at ICLR 2022
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.
A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic Minimax Character of the Sample Distri-
bution Function and of the Classical Multinomial Estimator. The Annals of Mathematical Statis-
tics, 27(3):642 - 669, 1956. doi: 10.1214/aoms/1177728174. URL https://doi.org/10.
1214/aoms/1177728174.
Francisco Eiras, Motasem Alfarra, M. Pawan Kumar, Philip H. S. Torr, Puneet K. Dokania, Bernard
Ghanem, and Adel Bibi. Ancer: Anisotropic certification via sample-wise volume maximization.
2021.
Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transformations
via randomized smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8404-8417. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/5fb37d5bbdbbae16dea2f3104d7f9439-Paper.pdf.
Marc Fischer, Maximilian Baader, and Martin Vechev. Scalable certified segmentation via ran-
domized smoothing. In International Conference on Machine Learning, pp. 3340-3351. PMLR,
2021.
Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Se-
mantic contours from inverse detectors. In 2011 International Conference on Computer Vision,
pp. 991-998, 2011. doi: 10.1109/ICCV.2011.6126343.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016. doi: 10.1109/CVPR.2016.90.
Sture Holm. A simple sequentially rejective multiple test procedure. Scandinavian Journal of
Statistics, 6(2):65-70, 1979. ISSN 03036898, 14679469. URL http://www.jstor.org/
stable/4615733.
George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning ir-
regular graphs. SIAM Journal on Scientific Computing, 20(1):359-392, 1998. doi: 10.1137/
S1064827595287997. URL https://doi.org/10.1137/S1064827595287997.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:
//openreview.net/forum?id=SJU4ayYgl.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank, 2019.
Ching-Yun Ko, Zhaoyang Lyu, Lily Weng, Luca Daniel, Ngai Wong, and Dahua Lin. POPQORN:
Quantifying robustness of recurrent neural networks. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 3468-3477. PMLR, 09-15 Jun
2019. URL https://proceedings.mlr.press/v97/ko19a.html.
Aounon Kumar and Tom Goldstein. Center smoothing: Provable robustness for functions with
metric-space outputs. 2021.
Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying confidence via ran-
domized smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 5165-5177. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
37aa5dfc44dddd0d19d4311e2c7a0240- Paper.pdf.
11
Under review as a conference paper at ICLR 2022
Mathias LecUyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pp. 656-672. IEEE, 2019. doi:
10.1109/SP.2019.00044. URL https://doi.org/10.1109/SP.2019.00044.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S. Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. 2019.
Alexander Levine and Soheil Feizi. (de)randomized smoothing for certifiable defense against
patch attacks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 6465-6475. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
47ce0875420b2dbacfc5535f94e68433- Paper.pdf.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Computer Vision - ECCV 2018, pp. 381-397. Springer International
Publishing, 2018a. doi: 10.1007/978-3-030-01234-2.23.
Yongge Liu, Jianzhuang Yu, and Yahong Han. Understanding the effective receptive field in seman-
tic image segmentation. Multimedia Tools and Applications, 77(17):22159-22171, jan 2018b.
doi: 10.1007/s11042-018-5704-3.
Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive
field in deep convolutional neural networks. In Proceedings of the 30th International Conference
on Neural Information Processing Systems, NIPS’16, pp. 4905-4913, Red Hook, NY, USA, 2016.
Curran Associates Inc. ISBN 9781510838819.
MOSEK ApS. MOSEK Optimizer API for Python 9.2.46, 2019. URL https://docs.mosek.
com/9.2/pythonapi/index.html.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-
ical image segmentation. In Medical Image Computing and Computer-Assisted Intervention -
MICCAI 2015, pp. 234-241. Springer International Publishing, 2015. ISBN 978-3-319-24574-4.
Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Dan, and Martin Vechev.
Scalable polyhedral verification of recurrent neural networks. In Alexandra Silva and K. Rus-
tan M. Leino (eds.), Computer Aided Verification, pp. 225-248, Cham, 2021. Springer Interna-
tional Publishing. ISBN 978-3-030-81685-8.
Jan Schuchardt, Aleksandar Bojchevski, Johannes Klicpera, and Stephan Gunnemann. Collective
robustness certificates: Exploiting interdependence in graph neural networks. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=ULQdiUTHe3y.
Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James Tin-Yau
Kwok. Sparsebert: Rethinking the importance analysis in self-attention. In Marina Meila and
Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learn-
ing Research, pp. 9547-9557. PMLR, 2021. URL http://proceedings.mlr.press/
v139/shi21a.html.
Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh. Robustness veri-
fication for transformers. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=BJxwPJHFwS.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Hoang-Dung Tran, Neelanjana Pal, Patrick Musau, Diego Manzanas Lopez, Nathaniel Hamilton,
Xiaodong Yang, Stanley Bak, and Taylor T. Johnson. Robustness verification of semantic segmen-
tation neural networks using relaxed reachability. In Computer Aided Verification, pp. 263-286,
Cham, 2021. ISBN 978-3-030-81685-8.
12
Under review as a conference paper at ICLR 2022
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st Inter-
national Conference on Neural Information Processing Systems, NIPS’17, pp. 6000-6010, Red
Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
Lei Wang, Runtian Zhai, Di He, Liwei Wang, and Li Jian. Pretrain-to-finetune adversarial training
via sample-wise randomized smoothing, 2021. URL https://openreview.net/forum?
id=Te1aZ2myPIu.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversar-
ial examples for semantic segmentation and object detection. In International Conference on
Computer Vision. IEEE, 2017.
Pavel Yakubovskiy. Segmentation models pytorch. https://github.com/qubvel/
segmentation_models.pytorch, 2020.
Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Black-box cer-
tification with randomized smoothing: A functional optimization based framework. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 2316-2326. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1896a3bf730516dd643ba67b4c447d36-Paper.pdf.
Daniel ZUgner and Stephan Gunnemann. Adversarial attacks on graph neural networks via meta
learning. In International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=Bylnx209YX.
Daniel Zugner and Stephan Gunnemann. Certifiable robustness and robust training for graph con-
volutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, 2019.
Daniel Zugner and Stephan Gunnemann. Certifiable robustness of graph convolutional networks
under structure perturbations. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1656-1665, 2020.
A Improving efficiency
In this section, we discuss different modifications to our collective certificate that improve its sam-
ple efficiency and allow us fine-grained control over the size of the collective linear program. We
further discuss a linear relaxation of our collective linear program. All of the modifications preserve
the soundness of our collective certificate, i.e. we still obtain a provable bound on the number of
predictions that can be simultaneously attacked by an adversary. To avoid constant case distinc-
tions, we first present all results for real-valued data, i.e. X = R, before mentioning any additional
precautions that may be needed when working with binary data.
A.1 Sharing smoothing distributions among outputs
In principle, our proposed certificate allows a different smoothing distribution Ψ(n) to be used per
output gn of our base model. In practice, where we have to estimate properties of the smoothed
classifier using Monte Carlo methods, this is problematic: Samples cannot be re-used, each of the
many outputs requires its own round of sampling. We can increase the efficiency of our localized
smoothing approach by partitioning our Dout outputs into Nout subsets that share the same smooth-
ing distribution. When making smoothed predictions or computing base certificates, we can then
reuse the same samples for all outputs within each subsets.
More formally, we partition our Dout output dimensions into sets K(1) , . . . , K(Nout) with
I ' lNout	,.,
K(i) = {1,...,Dout}.
i=1
(6)
13
Under review as a conference paper at ICLR 2022
We then associate each set K(i) with a smoothing distribution Ψ(i). For each base model output gn
with n ∈ K(i), we then use smoothing distribution Ψ(i) to construct the smoothed output fn, e.g.
fn(x) = argmaxy∈γ Piz~ψ(i) [f (X + Z)= y] (note that We use a different smoothing paradigm
for binary data, see Section 5).
A.2 Quantizing certificate parameters
Recall that our base certificates from Section 5 are defined by a linear inequality: A prediction
yn = fn(x) is robust to a perturbed input x0 ∈ XDinif PD=I Wdn) ∙ |xd - x√∣p < η(n), for
some p ≥ 0. The Weight vectors w(n) ∈ RDin only depend on the smoothing distributions. A
side of effect of sharing the same smoothing Ψ(i) among all outputs from a set K(i), as discussed
in the previous section, is that the outputs also share the same Weight vector w(i) ∈ RDin With
∀n	∈	K(i)	:	w(i)	=	w(n).	Thus, for all smoothed outputs	fn	With n ∈	K(i),	the smoothed
prediction yn is robust if PD=I Wdi) ∙ |xd - xrf∣p < η(n).
Evidently, the base certificates for outputs from a set K(i) only differ in their parameter η(n). Recall
that in our collective linear program We use a vector of variables t ∈ {0, 1}Dout to indicate Which
predictions are robust according to their base certificates (see Section 6). If there are tWo outputs fn
and fm With η(n) = η(m) , then fn and fm have the same base certificate and their robustness can
be modelled by the same indicator variable. Conversely, for each set of outputs K(i), We only need
one indicator variable per unique η(n) . By quantizing the η(n) Within each subset K(i) (for example
by defining equally sized bins betWeen minn∈K(i) η(n) and maxn∈K(i) η(n) ), We can ensure that
there is alWays a fixed number Nbins of indicator variables per subset. This Way, We can reduce the
number of indicator variables from Dout to NoUt ∙ Nbins.
To implement this idea, We define matrix of thresholds E ∈ RNout ×Nbins With ∀i : min {Ei,:} ≤
mi□n∈κ(i) ({η(n) | n ∈ K⑶}). We then define a function ξ : {1,..., Nout} X R → R with
ξ(i, η) = max ({Ei,j | j ∈ {1, . . . , Nbins ∧ Ei,j < η})	(7)
that quantizes base certificate parameter η from output subset K(i) by mapping it to the next
smallest threshold in Ei,:. For feasibility, like in Section 6 we need to compute the constant
η(i) = mi□b∈RDin bτWdi) s.t. sum{b} ≤ Ep to ensure feasibility of the problem. Note that, be-
cause all outputs from a subset K(i) share the same weight vector w(i), we only have to compute
this constant once per subset. We can bound the collective robustness of the targeted dimensions T
of our vector of predictions y = f(x) as follows:
min X	X	Ti,j nn∈T∩K(i) ξi,η(n) = Ei,j o	(8)
i∈{1,...,Nout} j ∈{1,...,Nbins}
s.t. ∀i,j: bTw(i) ≥ Tij心 + (1 — Ti,j)Ei,j,	sum{b}≤ Ep	(9)
b ∈ R+Din, T ∈ {0, 1}Nout×Nbins.	(10)
Constraint Eq. 9 ensures that Ti,j is only set to 0 if bT w(i) ≥ Ei,j , i.e. all predictions from subset
K(i) whose base certificate parameter η(n) is quantized to Ei,j are no longer robust. When this is
the case, the objective function decreases by the number of these predictions. For Nout = Dout ,
Nbins = 1 and En,1 = η(n), we recover our general certificate from Section 6. Note that, if the
quantization maps any parameter η(n) to a smaller number, the set H(n) becomes more restrictive,
i.e. yn is considered robust to a smaller set of perturbed inputs. Thus, Eq. 8 is a lower bound on our
general certificate from Section 6.
A.3 Sharing noise levels among inputs
Similar to how partitioning the output dimensions allows us to control the number of output variables
t, partitioning the input dimensions and using the same noise level within each partition allows us
to control the number of variables b that model the allocation of adversarial budget.
Assume that we have partitioned our output dimensions into Nout subsets K(1), . . . , K(Nout, with
outputs in each subset sharing the same smoothing distribution Ψ(i), as explained in Section A.1.
14
Under review as a conference paper at ICLR 2022
Let us now define Nin input subsets J(1), . . . , J(Nin) with
I ' INout ,.,
J(i) = {1,...,Dout}.
i=1
(11)
Recall that a prediction yn = fn (x) with n ∈ K(i) is robust to a perturbed input x0 ∈ XDin
if PD=ι Wd ∙ |xd- xd|p < η(n) and that the weight vectors w(i) only depend on the smooth-
ing distributions. Assume that we choose each smoothing distribution Ψ(i) such that ∀l ∈
{1, . . . , Nin}, ∀d, d0 ∈ J(l) : wd(i) = wd(i0), i.e. all input dimensions within each set J(l) have the
same weight. This can be achieved by choosing Ψ(i) so that all dimensions in each input subset Jl
are smoothed with the noise level (note that we can still use different Ψ(i), i.e. different noise levels
for smoothing different sets of outputs). For example, one could use a Gaussian distribution with
covariance matrix Σ = diag (σ)2 with ∀l ∈ {1, . . . , Nin}, ∀d, d0 ∈ J(l) : σd = σd0.
In this case, the evaluation of our base certificates can be simplified. Prediction yn = fn(x) is robust
to a perturbed input x0 ∈ XDin if
D
X Wdi) ∙ |xd- xd|p < η(n)	(12)
d=1
=X (U⑴∙ X |xd - Xdlpl <η(n),	(13)
l=1	d∈J(l)
with u ∈ RNin and ∀i ∈ {1, . . . , Nout}, ∀l ∈ {1, . . . , Nin}, ∀d ∈ J : uli = Wdi . That is, we
can replace each weight vector w(i) that has one weight Wd(i) per input dimension d with a smaller
weight vector u(i) with one weight ul(i) per input subset J(l).
For our linear program, this means that we no longer need a budget vector b ∈ R+Din to model the
element-wise distance |x0d - xd |p in each dimension d. Instead, we can use a smaller budget vector
b ∈ RN+in to model the overall distance within each input subset J(l), i.e. Pd∈J(l) |x0d - xd|p.
Combined with the quantization of certificate parameters from the previous section, our optimization
problem becomes
min X	X	Ti,j nn ∈ T ∩ K(i) ξ i, η(n) = Ei,j o	(14)
i∈{1,...,Nout} j ∈{1,...,Nbins}
s.t. ∀i, j ： bTu(i) ≥ TijN + (1 - Tij)Ei,j， sum{b} ≤ 巴	(15)
b ∈ R+Nin, T ∈ {0, 1}Nout×Nbins.	(16)
with u ∈ RNin and ∀i ∈ {1, . . . , Nout}, ∀l ∈ {1, . . . , Nin}, ∀d ∈ J ： ωli = Wdi . For Nout = Dout,
Nin = Din, Nbins = 1 and En,1 = η(n), we recover our general certificate from Section 6.
When certifying robustness for binary data, we impose different constraints on b. To model that the
adversary can not flip more bits than are present within each subset, we use a budget vector b ∈ N0Nin
with ∀l ∈ {1, . . . , Nin} ： bl ≤ J(l) , instead of a continuous budget vector b ∈ R+Nin.
A.4 Linear relaxation
Combining the previous steps allows us to reduce the number of problem variables and linear con-
Straints from Din + Dout and Dout + 1 to Nin + Nout ∙ Nbins and Nout ∙ Nbins + 1, respectively.
Still, finding an optimal solution to the mixed-integer linear program may be too expensive. One
can obtain a lower bound on the optimal value and thus a valid, albeit more pessimistic, robustness
certificate by relaxing all to be continuous.
When using the general certificate from Section 6, the binary vector t ∈ {0, 1}Dout can be re-
laxed to t ∈ [0, 1]Dout . When using the certificate with quantized base certificate parameters
from Section A.2 or Section A.3, the binary matrix T ∈ [0, 1]Nout ×Nbins can be relaxed to T ∈
[0, 1]Nout ×Nbins. Conceptually, this means that predictions can be partially certified, i.e. tn ∈ (0, 1)
15
Under review as a conference paper at ICLR 2022
or Ti,j ∈ (0, 1). In particular, a prediction can be partially certified even if we know that is impos-
sible to attack under the collective perturbation model Bx = x0 ∈ XDin | ||x0 - x||p ≤ . Just
like Schuchardt et al. (2021), who encountered the same problem with their collective certificate,
we circumvent this issue by first computing a set L ⊆ T of all targeted predictions in T that are
guaranteed to always be robust:
L = ∣n ∈ TKm∈Bx X Wdn) ∙ |xd - xd|p) < η(n)}
{n ∈ T J max (max {w(n)} ∙巴 0)< η(n) }.
(17)
(18)
The equality follows from the fact that the most effective way of attacking a prediction is to allocate
all adversarial budget to the least robust dimension, i.e. the dimension with the largest weight -
unless all weights are negative. Because we know that all predictions with indices in L are robust,
we do not have to include them in the collective optimization problem and can instead compute
|L| + min X I x0 ∈ H(n) .
Xn∈T∖L
(19)
The r.h.s. optimization can be solved using the general collective certificate from Section 6 or any
of the more efficient, modified certificates from previous sections.
When using the general collective certificate from Section 6 with binary data, the budget variables
b ∈ {0, 1}Din can be relaxed to b ∈ [0, 1]Din. When using the modified collective certificate
from Section A.3, the budget variables with b ∈ N0Nin can be relaxed to b ∈ R+Nin . The additional
constraint ∀l ∈ {1, . . . , Nin} : bl ≤ JJ(l) J can be kept in order to model that the adversary cannot
flip (or partially flip) more bits than are present within each input subset J(l).
B Base certificates
In the following, we show why the base certificates presented in Section 5 hold and present alterna-
tives for other collective perturbation models.
B.1	GAUSSIAN SMOOTHING FOR l2 PERTURBATIONS OF CONTINUOUS DATA
Proposition 1. Given an output gn : RDin → Y, let fn(x) = argmaxy∈γ Prz〜N(χ,∑) [gn(z) = y]
be the corresponding smoothed output with Σ = diag (σ)2 andσ ∈ R+Din. Given an input x ∈ RDin
and Smoothed prediction y∏	=	fn(x),	let q = Prz〜N(x,∑)	[gn(z)	= yn]∙	Then,	∀x0	∈	H(n):
fn(x0) = yn with H(n) defined as in Eq. 2, Wd =”,η = (Φ(T) (q))2 and K = 2.
Proof. Based on the definition of the base certificate interface, we need to show that, ∀x0 ∈ H :
fn (x0) = yn with
H
x0 ∈ RDin
Din 1
X ^^2 ∙ |xd- xd|2 < (φ Ig))(.
d=1 σd
(20)
Eiras et al. (2021) have shown that under the same conditions as above, but with a general covariance
matrix Σ ∈ R+Din ×Din , a prediction yn is certifiably robust to a perturbed input x0 if
√(x - x0)Σ-1(x - x0) < 1 (Φ-1(q) - Φ-1(q0)) ,	(21)
where q0 = maxy^=yn Prz〜N(χ,∑) [gn(z) = y" is the probability of the second most likely pre-
diction under the smoothing distribution. Because the probabilities of all possible predictions have
to sum up to 1, we have q0 ≤ 1 - q. Since Φ-1 is monotonically increasing, we can obtain a
lower bound on the r.h.s. of Eq. 21 and thus a more pessimistic certificate by substituting 1 - q
16
Under review as a conference paper at ICLR 2022
for q0 (deriving such a ”binary certificate” from a ”multiclass certificate” is common in randomized
smoothing and was already discussed in (Cohen et al., 2019)):
√(x - x0)Σ-1(x - x0) < 1 (Φ-1 (q) - Φ-1(1 - q)) ,	(22)
In our case, Σ is a diagonal matrix diag (σ)2 with σ ∈ R+Din. Thus Eq. 22 is equivalent to
t X(Xd - Xd) σ (Xd- Xd) < 2 (φ-1(q) - φ-1(1 - q)) .	(23)
Finally, using the fact that Φ-1(q) - Φ-1 (1 - q) = 2Φ-1(q) and eliminating the square root shows
that we are certifiably robust if
Din 1
X F ∙ |xd - xd|2 < (φTg)) .	(24)
d=1 σd
□
B.1.1	UNIFORM SMOOTHING FOR l1 PERTURBATIONS OF CONTINUOUS DATA
An alternative base certificate for l1 perturbations is again due to Eiras et al. (2021). Using uniform
instead of Gaussian noise later allows us to collective certify robustness to l1-norm-bound perturba-
tions. In the following U(x, λ) with x ∈ RD, λ ∈ R+D refers to a vector-valued random distribution
in which the d-th element is uniformly distributed in [Xd - λd, Xd + λd].
Proposition 2. Given an output gn : RDin → Y, let f (x) = argmaxy∈γ Piz~u⑸入)[g(z) = y]
be the corresponding smoothed classifier with λ ∈ R+Din. Given an input x ∈ RDin and smoothed
prediction y = f (x), letP = Piz~u(χ,λ) [g(z) = y]. Then, ∀x0 ∈ H(n) : fn(x0) = yu with H(n)
defined as in Eq. 2, Wd = 1∕λd, η = Φ-1(q) and K = 1.
Proof. Based on the definition of H(n), we need to prove that ∀x0 ∈ H : fn(x0) = yn with
H
卜0 ∈ RDinI X λ- ∙∣χd - χd∣ < φ-1(q)},
(25)
Eiras et al. (2021) have shown that under the same conditions as above, a prediction yn is certifiably
robust to a perturbed input x0 if
Din 1
£| λ- ∙ (xd - xd) | <
d=1 d
1 (Φ-1(q) - Φ-1(1-q)),
(26)
where q0 = maxyn=yn Piz~u(χ,λ) [gn(z) = y" is the probability of the second most likely Predic-
tion under the smoothing distribution. As in our previous proof for Gaussian smoothing, we can ob-
tain a more pessimistic certificate by substituting 1-q for q0. Since Φ-1(q) -Φ-1(1-q) = 2Φ-1(q)
and all λd are non-negative, we know that our prediction is certifiably robust if
Din 1
Xγ~ ∙ |xd - xd| < φ-1(p).
d=1 λd
(27)
□
B.2	Variance smoothing
We propose variance smoothing as a base certificate for binary data. Variance smoothing certifies
predictions based on the mean and variance of the softmax score associated with a predicted label.
It is in principle applicable to arbitrary data types. We focus on discrete data, but all results can
17
Under review as a conference paper at ICLR 2022
be generalized from discrete to continuous data by replacing any sum over probability mass func-
tions with integrals over probability density functions. We first derive a general form of variance
smoothing before discussing our certificates for binary data in Section B.2.1 and Section B.2.2.
Variance smoothing assumes that we make predictions by randomly smoothing a base model’s soft-
max scores. That is, given base model g : X → ∆∣γ∣ mapping from an arbitrary discrete input space
X to scores from the |Y|-dimensional probability simplex ∆∣γ∣, we define the smoothed classifier
f (x) = argmaXy∈γEz〜ψ(χ) [g(z)y]. Here, Ψ(x) is an arbitrary distribution over X parameterized
by x, e.g a Normal distribution with mean x. The smoothed classifier does not return the most likely
prediction, but the prediction associated with the highest expected softmax score.
Given an input x ∈ X, smoothed prediction y = f(x) and a perturbed input x0 ∈ X, we want to
determine whether f(x0) = y. By definition of our smoothed classifier, we know that f(x0) = y if
y is the label with the highest expected softmax score. In particular, we know that f(x0) = y if y’s
softmax score is larger than all other softmax scores combined, i.e.
Ez〜Ψ(x0) [g(Z)y] > 0.5 =⇒ f (XO)= y.	(28)
Computing Ez〜33)[g(z)y] exactly is usually not tractable - especially if We later want to evaluate
robustness to many x0 from a whole perturbation model B ⊆ X. Therefore, we compute a lower
bound on Ez〜ψq,) [g(z)y]. If even this lower bound is larger than 0.5, we know that prediction y
is certainly robust. For this, we define a set of functions H with gy ∈ H and compute the minimum
softmax score across all functions from H:
min Ez〜Ψ(χ0) [h(z)] > 0.5 =⇒ f (x0) = y.
h∈H
(29)
For our variance smoothing approach, we define H to be the set of all functions that have a larger
or equal expected value and a smaller or equal variance, compared to our base model g applied to
unperturbed input x. Let μ = Ez〜ψ(χ) [g(z)y] be the expected softmax score of our base model
g for label y. Let σ2
Ez〜Ψ(x) [(g(Z)y - V) ]
be the expected squared distance of the softmax
score from a scalar V ∈ R. (Choosing V = μ yields the variance of the softmax score. An arbitrary
ν is only needed for technical reasons related to Monte Carlo estimation Section C.2). Then, we
define
H = {h : X → R J Ez〜Ψ(x) [h(z)] ≥ μ ∧ Ez〜Ψ(x) [(h(z) - ν)[ ≤ σ2 }	(30)
Clearly, by the definition of μ and σ2, we have gy ∈ H. Note that we do not restrict functions from
H to the domain [0, 1], but allow arbitrary real-valued outputs.
By evaluating Eq. 28 with H defined as in Eq. 29, we can determine if our prediciton is robust. To
compute the optimal value , we need the following two Lemmata:
Lemma 1. Given a discrete set X and the set Π of all probability mass functions over X, any two
probability mass functions π1 , π2 ∈ Π fulfill
π2 (z)
Zg 而 π2(Z) ≥
(31)
Proof. For a fixed probability mass function π1, Eq. 31 is lower-bounded by the minimal expected
likelihood ratio that can be achieved by another π(z) ∈ Π:
X π27zτ∏(z) ≥ min X WZ)π(z).	(32)
Z∈X πι(z)	π∈π Z∈X πι(z)
The r.h.s. term can be expressed as the constrained optimization problem
mn X ∏(z)π(z) s∙t∙	X π(z) =1
π z∈X π1 z	z ∈X
with the corresponding dual problem
max min ,X 示 V、∏(z) + λ ∣ —1 + ,X π(z)
λ∈R ∏ j∏ι(z)	∖ 乙
z∈X 1	z∈X
(33)
(34)
18
Under review as a conference paper at ICLR 2022
The inner problem is convex in each ∏(z). Taking the gradient w.r.t. to ∏(z) for all Z ∈ X shows
that it has its minimum at ∀z ∈ X : ∏(z) = - λπ2(Z). Substituting into Eq. 34 results in
max
λ∈R
λ2∏1(z)2
4∏ι(z)
+λ
max
λ∈R
-λ2X πι(zl
4
z∈X
-λ
^∩ax------- λ
λ∈R	4
1.
(35)
(36)
(37)
(38)
Σ
Eq. 37 follows from the fact that π1(z) is a valid probability mass function. Due to duality, the
optimal dual value 1 is a lower bound on the optimal value of our primal problem Eq. 31.	□
Lemma 2. Given a probability distribution D over a R and a scalar V ∈ R ,let μ = Ez 〜D and
ξ = Ez〜D h(z - v)2i. Then ξ ≥ (μ - V)2
Proof. Using the definitions of μ and ξ, as well as some simple algebra, we can show:
	ξ	≥ (μ	- V)2	(39)
	^⇒ Ez〜D [(z — V)2i	≥μ2	—2μV + V2	(40)
^⇒	Ez〜D [ζ2 - 2zv + V2]	≥μ2	—2μV + V2	(41)
^⇒	Ez〜D [z2 - 2zv + V2]	≥μ2	—2μV + V2	(42)
^⇒	Ez〜D [z2] - 2μV + V2	≥μ2	—2μV + V2	(43)
	^⇒ Ez〜D [z2]	≥μ2		(44)
It is well known for the variance that Ez〜D [(z - μ)2]		=Ez〜D [z2] - μ2.		Because the variance is
always non-negative, the above inequality holds.				□
Using the previously described approach and lemmata, we can show the soundness of the following
robustness certificate:
Theorem 3. Given a model g : X → ∆∣γ∣ mapping from discrete set X to Scores from
the |Y|-dimensional probability simplex, let f (x) = argmaXy∈γEz〜田⑺[g(z)y] be the corre-
sponding smoothed classifier with smoothing distribution Ψ(x) and probability mass function
∏χ(z) = Prz〜Ψ(x) [z = z]. Given an input X ∈ X and smoothed prediction y = f (x), let
μ = Ez〜ψ(χ) [g(z)y] and σ2 = Ez〜田⑺[(g(z)y — ν)2] with V ∈ R. If V ≤ μ, we know that
f(x0) = y if
∏χ0 (z)2
∏x(z)
<1+
σ2 - (μ - v)2 ("	2)
(45)
Σ
Proof. Following our discussion above, we know that f (x0) = y if Ez〜33)[g(z)y] > 0.5 with H
defined as in Section 5. We can compute a (tight) lower bound on minh∈H Ez〜田的,)by following
the functional optimization approach for randomized smoothing proposed by Zhang et al. (2020).
That is, we solve a dual problem in which we optimize the value h(z) for each z ∈ X. By the
definition of the set H, our optimization problem is
min Ez〜w(x，) [h(Z)]
h：X—→R
s.t. Ez〜ψ(χ) [h(z)] ≥ μ,	Ez〜ψ(χ) [(h(z) - V)〔 ≤ σ2.
19
Under review as a conference paper at ICLR 2022
The corresponding dual problem with dual variables α, β ≥ 0 is
max min
α,β≥0 h:X—R
+α (μ - Ez〜ψ(χ) [h(z)]) + β (Ez 〜Ψ(x)
Ez〜Ψ(χ0) [h(z)]
(46)
We first move move all terms that don’t involve h out of the inner optimization problem:
max αμ — βσ2 + min
α,β≥0	h:X—R
Ez〜Ψ(x0) [h(Z)] — αEz〜Ψ(x) [h(Z)]+ βEz〜Ψ(x)
(47)
Writing out the expectation terms and combining them into one sum (or - in the case of continuous
X - one integral), our dual problem becomes
=max αμ — βσ2 + min	h(z)∏χθ(z) — αh(z)∏χ(z) + β (h(z) — V)2 ∏χ(z)	(48)
α,β≥0	h:X—R
z∈X
(recall that πx0 and πx0 refer to the probability mass functions of the smoothing distributions). The
inner optimization problem can be solved by finding the optimal h(z) in each point z:
max ɑμ — βσ2 + min h(z)∏χθ(z) — αh(z)∏χ(z) + β (h(z) — V) ∏χ(z)	(49)
α,β≥0	h(z)∈R
z∈X
Because β ≥ 0, each inner optimization problem is convex in h(z). We can thus find the optimal
h (z) by setting the derivative to zero:
d2 dh(z) h(z)∏χθ(z) — αh(z)∏χ(z)+ β (h(z) — V) ∏χ	(z)	! !0	(50)
^⇒ ∏χ0(z) — α∏χ(z) + 2β (h(z) — v) ∏χ(z)二	=! 0		(51)
=⇒ h*(z) = — πx0(ZL + — + V. L	2β∏χ(z)	2β			(52)
Substituting into Eq. 48 and simplifying leaves us with the dual problem
max ɑμ — βσ2 — α~ + α~ — αν + V — 1- X 不建人「、	(53)
α,β≥0	4β 2β	4β πx(z)
In the following, let US use P = Pz∈χ πx0(Z) as a shorthand for the expected likelihood ratio. The
problem is concave in a. We can thus find the optimum a* by setting the derivative to zero, which
gives us α* = 2β(μ — V) + 1. Because β ≥ 0 and ou theorem assumes that V ≤ μ, α* is a feasible
solution to the dual problem. Substituting into Eq. 53 and simplifying results in
max α*μ — βσ2 —
β≥0
α*2 α*
4β + 2β
—α*V + V —
1
4βρ
maxβ ((M — V)2 — σ2) + μ + 4β (1 — P).
(54)
(55)
Lemma 1 shows that the expected likelihood ratio P is always greater than or equal to 1. Lemma 2
shows that (μ — v)2 — σ2 ≤ 0. Therefore Eq. 55 is concave in β. The optimal value of β can again
be found by setting the derivative to zero:
β* = j4(3—V)P -^.	(56)
Recall that our theorem assumes σ2 ≥ (μ — ν)2 and thus β* is real valued. Substituting into Eq. 56
shows that the maximum of our dual problem is
μ + √(1 — p) ((μ — ν )2 — σ2).
(57)
20
Under review as a conference paper at ICLR 2022
By duality, this is a lower bound on our primal problem minh∈H Ez〜ψ(χo)[h(z)]. We know that our
prediction is Certifiably robust, i.e. f (x) = y, if minh∈H Ez〜33)[h(z)] > 0.5. So, in particular,
our prediction is robust if
^⇒
μ + Vz(1 - ρ)((μ - ν)2 - σ2) > 0.5
0 ρ< 1+ σ2 - (I - ν)2 (〃 - 1)
X ∏χ0(z)2	< 1 ,	1	( _	1 )2
Z∈X ∏x(z)	< +σ2-(μ	- V)2 μ- -	2J
(58)
(59)
(60)
The last equivalence is the result of inserting the definition of the expected likelihood ratio ρ.	□
With Theorem 3 in place, we can certify robustness for arbitrary smoothing distributions, assuming
we can compute the expected likelihood ratio. When we are working with discrete data and the
smoothing distributions factorize (but are not necessarily i.i.d.), this can be done efficiently, as the
two following base certificates for binary data demonstrate.
B.2. 1 Bernoulli Variance smoothing for perturbations of binary data
We begin by proving the base certificate presented in Section 5. Recall that we we use a smoothing
distribution F(x, θ) with θ ∈ [0, 1]Din that independently flips the d’th bit with probability θd,
i.e. for x, Z ∈ {0,1}Din and Z 〜F(x, θ) we have Pr[zd = xd] = θd.
Theorem 1. Given an output gn : {0,1}Din → ∆∣γ∣ mapping to scores from the |Y|-dimensional
probability simplex, let fn(x) = argmaXy∈γEz〜F(比⑶[gn(z)y] be the corresponding smoothed
classifier with θ ∈ [0, 1]Din. Given an input x ∈ {0, 1}Din and smoothed prediction yn = fn(x),
let μ = Ez〜F(χ,θ) [gn(z)y] and σ2 = Varz〜f(x,θ) [gn(z)y]∙ Then, ∀x0 ∈ H(n) : fn(x0) = yn,
with H(n) defined as in Eq. 2, Wd = ln ((1—：d)+ ɪθdθɪ), η = ln(1 + 表(μ — 2)2) and K = 0.
Proof. Based on our definition of the base certificate interface from Section 5, we must show that
∀x0 ∈ H : fn (x0) = yn with
H=卜 0∈{0,ι}Din	X ιn((ι	θdθd)	+iθ⅛) ∙iχd-χdi0 < ln。+σ2	(μ	-	2))}
=	(61)
Because all bits are flipped independently, our probability mass function ∏χ(z) = Prz〜田⑺[Z = z]
factorizes:
Din
πx(Z) =	πxd(zd)	(62)
d=1
with
πxd(zd) = θ1d- θd
Thus, our expected likelihood ratio can be written as
if zd 6= xd
else
(63)
Σ
z∈{0,1}Din
∏χ0 (z)2
∏x(z)
Din
XY
z∈{0,1}Din d=1
KXd (zd)2
πXd(Zd)
Din
YX
d=1 zd∈{0,1}
KXd (Zd)2
πXd(Zd)
(64)
For each dimension d, we can distinguish two cases: If both the perturbed and unperturbed input are
πx0 (z)
the same in dimension d, i.e. x0 = xd, then —= 1 and thus
d	πxd (z)
X ∏∏xd ((Zd)) = X πxd (Zd)= θd + (1 - θd) = 1∙	(65)
zd∈{0,1}	Xd d zd∈{0,1}
21
Under review as a conference paper at ICLR 2022
If the perturbed and unperturbed input differ in dimension d, then
Txd (Zd) = (1 - θd)2 + (θd)2
πxd (Zd)	θd	1 - θd
(66)
Therefore, the expected likelihood ratio is
Din
YX
d=1 zd∈{0,1}
KXd(Zd)2
πXd(Zd)
Din
Y
d=1
((1-θd)
(θd
(67)
Due to Theorem 3 (and using V = μ when computing the variance), We know that our prediction is
robust, i.e. fn (x0) = yn, if
^⇒
Σ
z∈{0,1}Din
∏χ0 (z)2
∏x(z)
1
<1 +苒
Din
Y
d=1
((1-θd)
(θd
|x0d - xd | < ln
Din
X ln
d=1
((1-θd)
(θd
< 1+⅛ (μ - 2)
(68)
(69)
(70)
^⇒
Because xd and x0d are binary, the last inequality is equivalent to
Din
X ln
d=1
(71)
□
B.2.2 Sparsity-aware Variance smoothing for perturbations of binary data
Sparsity-aware randomized smoothing (Bojchevski et al., 2020) is an alternative smoothing ap-
proach for binary data. It uses different probabilities for randomly deleting (1 → 0) and adding
(0 → 1) bits to preserve data sparsity. For a random variable z distributed according to the
sparsity-aware distribution S(x, θ+, θ-) with x ∈ {0, 1}Din and addition and deletion probabil-
ities θ+, θ- ∈ [0, 1]Din, we have:
Pr[zd = 0] = (l-θ+) 1-xd∙ (θ-)xd ,
Pr[zd = 1] = (θ+)1-xd ∙ (l-θ-)xd .
The Bernoulli smoothing distribution we discussed in the previous section is a special case of
sparsity-aware smoothing with θ+ = θ-. The runtime of the robustness certificate derived by
Bojchevski et al. (2020) increases exponentially with the number of unique values in θ+ and θ-,
which makes it unsuitable for localized smoothing. Variance smoothing, on the other hand, allows
us to efficiently compute a certificate in closed form.
Theorem 2. Given an output gn : RDin → ∆∣γ∣ mapping to scores from the |Y|-dimensional prob-
ability simplex, let fn(x) = argmaXy∈γEz~s(χ,θ+,θ-) [gn(z)y] be the CorresPonding smoothed
classifier with θ+, θ- ∈ [0, 1]Din. Given an input x ∈ {0, 1}Din and smoothed prediction
yn = fn(X), let μ = Ez~S(x,θ+,θ-) [gn(Z) y] and σ? = Varz~S(x,θ+,θ-) [gn(z)y] ∙ Then,
∀x0 ∈ H : fn (x0) = yfor
H = {x' ∈ {0,1}Din | X γ+ ∙ I [xd = 0 = xd] + Y- ∙ I [xd = 1 = xd] < η},	(72)
where γ+, γ- ∈ RDin, γ+ = ln (编 + (I-
ln (1 + σ12 (μ - 2)”
, γd-
ln((1-θ+) + 1⅜) and η
22
Under review as a conference paper at ICLR 2022
Proof. Just like with the Bernoulli distribution we discussed in the previous section, all bits are
flipped independently, meaning our probability mass function ∏χ(z) = Prz〜ψ(χ) [z = z] factor-
izes:
Din
πx(z) =	πxd(zd)	(73)
d=1
with
πxd(zd) =	θ1d- θd
if zd 6= xd
else
(74)
As before, our expected likelihood ratio can be written as
Σ
z∈{0,1}Din
∏χ0 (z)2
∏x(z)
Din
XY
z∈{0,1}Din d=1
KXd(Zd)2
πXd(Zd)
Din
YX
d=1 zd∈{0,1}
KXd (Zd)2
πXd(Zd)
(75)
We can now distinguish three cases. If both the perturbed and unperturbed input are the same in
πx0 (z)
dimension d, i.e. xd = xd, then —ɪʒ = 1 and thus
d	πxd (z)
Σ
zd ∈{0,1}
KXd(Zd)2
πXd(Zd)
KX0d (Zd) = 1.
zd ∈{0,1}
(76)
If x0d = 1 and xd = 0, i.e. a bit was added, then
πχd(z)2 = X	∏ι(Zd)2 = π√0f + π√1f = (θ-)2 + (1 - θ-)2
πxd(Z)	Zd ∈⅛}π0(Zd)	π0 (0) π0 ⑴	1-θ+	θ+
If x0d = 0 and xd = 1, i.e. a bit was deleted
不Xd(Z)2 =	n0(Zd)2
∏x	∏X , (z)	∏1(	∏ι(Zd)
zd∈{0,1}	Xd	zd∈{0,1} 1 d
Therefore, the expected likelihood ratio is
Din
Y
d=1 z
then
.∏0(0)2	K0(1)2_ (1 - θ+)2	(θ+)2
------;~~十------;~———------------十---------
∏ι(0)	∏ι(1)	Θ-	1 - Θ-
X^ πx0d (Zd)2
z∈{0,1} πxd (Zd)
Din
Y
d=1
(⅛ 十一!
Din
=YeXp (γ+ y
(77)
(78)
(79)
(80)
(81)
d=1
In the last equation, we have simple used the shorthands γd+ and γd-, as defined in Theorem 2 Due
to Theorem 3 (and using V = μ when computing the variance), we know that our prediction is
robust, i.e. fn (x0) = yn, if
< 1+σ2(〃 - 2)
Σ
z∈{0,1}Din
∏χ0 (z)2
∏x(z)
(82)
^⇒
Din
O Y exp (γ+)I[xd=0=xdl] ∙ exp (Yd)l[xd=1=Xdl]
d=1
Din
X Y+ T[χd = 0 = xd|] ∙ YdT [xd = 1 = xd|] < ln
d=1
(83)
(84)
□
23
Under review as a conference paper at ICLR 2022
It should be noted that this certificate does not comply with our interface for base certificates Sec-
tion 5, meaning we can not directly use itto certify robustness to norm-bound perturbations using our
collective linear program from Section 6. We can however use it to certify collective robustness to the
more refined threat model used in (Schuchardt et al., 2021): Let the set of admissible perturbed in-
puts be Bx = nx0 ∈ {0, 1}Din | PdD=in1 [xd = 0 6= x0d|] ≤ + ∧ PdD=in1 [xd = 1 6= x0d|] ≤ -o with
+ , y ∈ N0 specifying the number of bits the adversary is allowed to add or delete. We can now
follow the procedure outlined in Section 6 to combine the per-prediction base certificates into a
collective certificate for our new collective perturbation model. As discussed in, we can bound the
number of predictions that are robust to simultaneous attacks by minimizing the number of predic-
tions that are certifiably robust according to their base certificates:
min
x0∈Bx
I [fn(x0) = yn] ≥
n∈T
min X I x0 ∈ H(n) .
x0∈Bx
x n∈T
(85)
Inserting the linear inequalities characterizing our perturbation model and base certificates results
in:
Din
X γ+ ∙ I [xd = 0 = xd] + γ- ∙ I [xd = 1 = xd] < η(n)	(86)
d=1
Din	Din
s.t.	X[xd =0 6= x0d|] ≤ +, X [xd = 1 6= x0d|] ≤ -.	(87)
d=1	d=1
min I
x0∈{0,1}Din n∈T
Instead of optimizing over the perturbed input x0, we can define two vectors b+, b- ∈ {0, 1}Din
that indicate in which dimension bits were added or deleted. Using these new variables, Eq. 86 can
be rewritten as
b+ b-m‰DinX I k+)Tb++(γ-)Tb- < η(n)i	(88)
n∈T
s.t. sum{b+} ≤ + , sum{b- } ≤ - ,	(89)
X bd+ = 0, X bd- = 0.	(90)
d|xd=1	d|xd=0
The last two constraints ensure that bits can only be deleted where xd = 1 and bits can only be
added where xd = 0. Finally, we can use the procedure for replacing the indicator functions with
indicator variables that We discussed in Section 6). Let η(n) be the minimum values the weighted
sums in the objective function can take on:	一
η(n) =	min	(Y+ )T b+ + (Y-)T b-	(91)
-	b+,b-∈{0,1}Din '	)	'	' '
s.t. sum{b+} ≤ + , sum{b- } ≤ - ,	(92)
bd+ = 0,
d|xd=1
bd- = 0.
d|xd=0
(93)
The above problem can be solved by finding all negative entries ofY+ and Y- and then summing up
the min (Sum {1 - x} , b；) (or min (Sum {x} , b-), respectively) smallest ones. Using these η(n)
and binary indicator variables t ∈ {0, 1}Dout, we can restate the above problem as the mixed-integer
problem
s.t.
min
b+,b-∈{0,1}Din,t∈{0,1}Dout
tn
(γ +)T b+ + (γ-)T b- ≥ tnŋ(n) + (1 - tn)η(n),
Sum{b+} ≤ +, Sum{b-} ≤ -,
bd+ = 0,
d|xd=1
bd- = 0.
d|xd=0
(94)
(95)
(96)
(97)
The first constraint ensures that tn can only be set to 0 if the l.h.s. is greater or equal ηn, i.e. only
when the base certificate can no longer guarantee robustness. The efficiency of the certificate can be
improved by applying any of the techniques discussed in Section A.
24
Under review as a conference paper at ICLR 2022
C Monte Carlo randomized smoothing
To make predictions and certify robustness, randomized smoothing requires computing certain prop-
erties of the distribution of a base model’s output, given an input smoothing distribution. For
example, the certificate of Cohen et al. (2019) assumes that the smoothed model f predicts the
most likely label output by base model g, given a smoothing distribution N(0,σ ∙ 1): f (x) =
argmaXy∈Y Prz〜N(o,σ∙i) [g(x + Z)= y]. To certify the robustness of a smoothed prediction y =
f (x) for a specific input x, We have to compute the probability q = Prz〜N(o,σ∙i) [g(x + Z)= y]
to then calculate the maximum certifiable radius σΦ-1 (q) with standard-normal inverse CDF Φ-1.
For complicated models like deep neural netWorks, computing such properties in closed form is
usually not tractable. Instead, they have to be estimated using Monte Carlo sampling. The result are
predictions and certificates that only hold With a certain probability.
Randomized smoothing With Monte Carlo sampling usually consists of three distinct steps. First,
a small number of samples N1 from the smoothing distribution are used to generate a candidate
prediction y, e.g. the most frequently predicted class. Then, a second round of N samples is taken
and a statistical test is used to determine Whether the candidate prediction is likely to be the actual
prediction of smoothed classifier f, i.e. whether y = f (x) with a certain probability (1 - αι).
If this is not the case, one has to abstain from making a prediction (or generate a neW candidate
prediction). To certify the robustness of prediction y, a final round of N3 samples is taken to estimate
all quantities needed for the certificate. In the case of (Cohen et al., 2019), we need to estimate the
probability q = Prz〜N(o,σ∙i) [g(x + Z)= y] to compute the certificate σΦ-1(q), whose strength
is monotonically increasing in q. To ensure that the certificate holds with high probability (1 - α2),
we have to compute a probabilistic lower bound q ≤ q. If one wants to certify robustness for all
predictions, as we do in our experiments, one can also use the same samples for the abstention test
and the certificates. One particularly simple abstention mechanism is to just compute the Monte
Carlo randomized smoothing certificate to determine whether ∀x0 ∈ {x} : f (x0) = y with high
probability, i.e. whether the prediction is robust to input x0 that is the result of ”perturbing” clean
input x with zero adversarial budget.
In the following, we discuss how we perform Monte Carlo randomized smoothing for our base
certificates, as well as the baselines we use for our experimental evaluation. In Section C.4, we
discuss how we account for the multiple comparisons problem, i.e. the fact that we are not just
trying to probabilistically certify a single prediction, but multiple predictions at once.
C.1 Monte Carlo base certificates for continuous data
For our base certificates for continuous data, we follow the approach we already discussed in the
previous paragraphs (recall that the certificate of Cohen et al. (2019) is a special case of our cer-
tificate with Gaussian noise for l2 perturbations). We are given an input space XDin, label space
Y, base model (or - in the case of multi-output classifiers - base model output) g : XDin → Y
and smoothing distribution Ψ(x) (either multivariate Gaussian or multivariate uniform). To gen-
erate a candidate prediction, we apply the base classifier to N1 samples from the smoothing
distribution in order to obtain predictions y(1), . . . , y(N1) and compute the majority prediction
y = argmaxy∈γ {n | y(n) = y}. Recall that for Gaussian and uniform noise, our certificate guar-
antees ∀x0 ∈ H : f (x) = y with
x0 ∈ XDin
H
Din
Ewd ∙ ∣χ0d - χd∣p < η
d=1
}
with η = (Φ-1(q))2 or η = Φ-1(q) (depending on the distribution), q =
Prz〜N(o,σ∙i) [g(x + Z)= y] and standard-normal inverse CDF Φ-1. To obtain a probabilistic cer-
tificate that holds with high probability 1 - α, we need a probabilistic lower bound on η. Both η are
monotonically increasing in q, i.e. we can bound them by finding a lower bound q on q. For this, we
take N more samples from the smoothing distribution and compute a Clopper-Pearson lower con-
fidence bound (Clopper & Pearson, 1934) on q. For abstentions, we use the aforementioned simple
25
Under review as a conference paper at ICLR 2022
mechanism: We test whether x ∈ H. Given the definition of H, this is equivalent to testing whether
0 < Φ-1(q)
^⇒ Φ(0) < q
^⇒ 0.5 < q.
If q ≤ 0.5, we abstain.
C.2 Monte Carlo variance smoothing
In variance smoothing, we smooth a model’s softmax scores. That is, we are given an input space
XDin, label space Y, base model (or - in the case of multi-output classifiers - base model output)
g : XDin → ∆∣γ∣ with ∣Y∣-dimensional probability simplex ∆∣γ∣ and smoothing distribution Ψ(x)
(Bernoullli or sparsity-aware noise, in the case of binary data). To generate a candidate prediction,
we apply the base classifier to N1 samples from the smoothing distribution in order to obtain vectors
(S(I),..., S(NI)) with S ∈ ∆∣y∣, compute the average softmax scores s = = PnN=I S and select
the label with the highest score y = arg maxy Sy.
Recall that our certificate guarantees robustness if the optimal value of the following optimization
problem is greater than 0.5:
min Ez〜w(x，) [h(z)]
h：X—→R
s.t.	Ez〜Ψ(χ) [h(z)] ≥ μ,	Ez〜ψ(χ) [(h(z) — V)〔 ≤ σ2,
(98)
(99)
with μ = Ez〜ψ(χ) [g(z)y], σ2 = Ez〜田⑺[(g(z)y — ν)2] and a fixed scalar V ∈ R. To obtain a
probabilistic certificate, we have to compute a probabilistic lower bound on the optimal value of the
optimization problem. Because it is a minimization problem, this can be achieved by loosening its
constraints, i.e. computing a probabilistic lower bound μ on μ and a probabilistic upper bound σ2
on σ2.
Like in CDF-smoothing (Kumar et al., 2020), we bound the parameters using CDF-based nonpara-
metric confidence intervals. Let F(s) = Prz〜ψ(χ) [g(z)y ≤ s] be the CDF of the distribution of gy
under the smoothing distribution Ψ(x). Define M thresholds ≤ 0T1 ≤ T2 . . . , TM-1 ≤ TM ≤ 1
with ∀m : Tm ∈ [0, 1]. We then take N2 samples x(1), . . . , x(N2) from the smoothing distribution
to compute the empirical CDF F(S) = PnN= 11 [g(z(n))y ≤ s]. We can then use the Dvoretzky-
Keifer-Wolfowitz inequality (Dvoretzky et al., 1956) to compute an upper bound F and a lower
bound F on the CDF of gy:
F(S) = max (F(S) — υ, 0)≤ F(S) ≤ min (F(S) + υ, 1)= F(s),
(100)
with υ
, which holds with high probability (1 - α). Using these bounds on the CDF, we
can bound μ = Ez〜田⑻ [g(z)y] as follows (Anderson, 1969):
M-1
μ ≥ TM — Tl F (τι) + E (Tm+1 — Tm) F (丁团).
m=1
(101)
The parameter σ2 = Ez〜田⑺ [(g(z)y — ν)2] can be bounded in a similar fashion. Define
ξ0 , . . . , ξM ∈ R+ with:
ξ0
ξM
max (κ — ν)2
κ∈[0,τ1]
max, 1 ((K — V )2)
κ∈[τM,1]
max ((K — V)2)	∀m ∈ {1,...,M — 1},
κ∈[τm ,τm +1]
(102)
26
Under review as a conference paper at ICLR 2022
i.e. compute the maximum squared distance to ν within each bin [τm, τm+1]. Then:
M-1
σ2 ≤ ξ0F(τ1) + ξM (1 - F(τM)) + X ξm (F (τm+1 - F(τm))	(103)
m=1
M-1
= ξM + X (ξm-1 - ξm) F(τm)	(104)
m=1
M-1
≤ ξM + X Sgn (ξm-1 - ξm) F(Tm) + (1 -Sgn (ξm-1 — ξm)) F(Tm)	(105)
m=1
with probability (1 - α). In the first inequality, we bound the expected squared distance from ν by
assuming that the probability mass in each bin [Tm, Tm+1] is concentrated at the farthest point from
ν. The equality is a result of reordering the telescope sum. In the second inequality, we upper-bound
the CDF where it is multiplied with a non-negative value and lower-bound it where it is multiplied
with a negative value.
With the probabilistic bounds μ and σ2 We can now - in principle - evaluate our robustness certifi-
cate, i.e. check whether
ɪ (μ- 1)
Xπx0 (z)
-ɪrɪ < 1 + =
∏x(z)	n-2
z∈X x	σ
-(μ - V)
(106)
1
where the π are the probability mass functions of smoothing distributions Ψ(x) and Ψ(x0). But one
crucial detail of Theorem 3 underlying the certificate was that it only holds for V ≤ μ, i.e. only when
this condition is fulfilled can we compute the certificate in closed form by solving the corresponding
dual problem. To use the method with Monte Carlo sampling, one has to ensure that V ≤ μ by first
computing μ and then choosing some smaller V.
In our experiments, we use an alternative method that allows us to use arbitrary V: From our proof
of Theorem 3we know that the dual problem of Eq. 98 is
max
α,β≥0
2
α2 α
4β + 2β
- αV + V -
1	πx0 (Z)2
4β Z∈X πχ(Z),
(107)
Instead of trying to find an optimal a (which causes problems in subsequent derivations if V 幺 μ),
we can simply choose α = 1. By duality, the result is still a lower bound on the primal problem,
i.e. the certificate remains valid. The dual problem becomes
max μ - βσ2 + ɪ - ɪ X "x' (Z) .
β≥0-	4β	4βJ∏χ(z)
(108)
The problem is concave in β (because the expected likelihood ratio is ≥ 1). Finding the optimal β,
comparing the result to 0.5 and solving for the expected likelihood ratio, shows that a prediction is
robust if
X ∏χ0 (z)2	1 (	1 )2
Z∈X B <1 + 本(μ - 2)
(109)
For our abstention mechanism, like in the previous section, we compute the certificate H and then
test whether x ∈ H. In the case of Bernoulli smoothing and sparsity-aware smoothing), this corre-
sponds to testing whether
1 < ln (1+⅛ Q- 2
1
O μ> 2.
(110)
(111)
C.3 Monte Carlo center smoothing
While we can not use center smoothing as a base certificate, we benchmark our method against it
during our experimental evaluation. The generation of candidate predictions, the abstention mech-
anism and the certificate are explained in (Kumar & Goldstein, 2021). The authors allow multiple
27
Under review as a conference paper at ICLR 2022
options for generating candidate predictions. We use the ''β minimum enclosing ball” with β = 2
that is based on pair-wise distance calculations.
C.4 Multiple comparisons problem
The first step of our collective certificate is to compute one base certificate for each of the Dout
predictions of the multi-output classifier. With Monte Carlo randomized smoothing, we want all of
these probabilistic certificates to simultaneously hold with a high probability (1 - α). But as the
number of certificates increases, so does the probability of at least one of them being invalid. To
account for this multiple comparisons problem, we use Bonferroni (Bonferroni, 1936) correction,
i.e. compute each Monte Carlo certificate such that it holds with probability (1 - n).
For base certificates that only depend on qn = Prz〜田⑴)[gn(z) = yn], i.e. the probability of the
base classifier predicting a particular label r^n under the smoothing distribution, one can also use the
strictly better Holm correction (Holm, 1979). This includes our Gaussian and uniform smoothing
certificates for continuous data. Holm correction is a procedure than can be used to correct for
the multiple comparisons problem when performing multiple arbitrary hypothesis tests. Given N
hypotheses, their p-values are ordered in ascending order p1 , . . . , pN. Starting at i = 1, the i’th
hypothesis is rejected if Pi < N工-分,until one reaches an i such that Pi ≥ N工-分.
Fischer et al. (2021) proposed to use Holm correction as part of their procedure for certifying that all
(non-abstaining) predictions ofan image segmentation model are robust to adversarial perturbations.
In the following, we first summarize their approach and then discuss how Holm correction can be
used for certifying our notion of collective robustness, i.e. certifying the number of robust predic-
tions. As in Section C.1, the goal is to obtain a lower bound q。on qn = Prz〜ψ(n) [gn(z) = yn] for
each of the Dout classifier outputs. Assume we take N2 samples z(1), . . . , z(N2) from the smoothing
distribution. Let Vn = PN=I I [gn(ζ(i)) = yn and let ∏ : {1,..., Dout} → {1,∙∙∙, Dout} be a
bijection that orders the Vn in descending order, i.e. ν∏(1) ≥ %⑵∙∙∙ ≥ Vn(Dou力.Instead of using
Clopper-Pearson confidence intervals to obtain tight lower bounds on the qn, Fischer et al. (2021)
define a threshold τ ∈ [0.5, 1) and use Binomial tests to determine for which n the bound τ ≤ qn
holds with high-probability. Let BinP (Vn, N2, ≤, τ) be the p-value of the one-sided binomial test,
which is monotonically decreasing in Vn . Following the Holm correction scheme, the authors test
Whether	BinP。(吐至，≤,τ) < ^^	(112)
Dout + 1 - k
for k = 1,..., Dout until reaching a k for which the null-hypothesis can no longer be rejected,
i.e. the p-value is g.e.q. D-⅛πξtt . They then know that with probability 1 - α, the bound T ≤ qn
Dout +1-k
holds for all n ∈ {∏(k) | k ∈ {1 ,...,k*}. For these outputs, they use the lower bound T to compute
robustness certificates. They abstain with all other outputs.
This approach is sensible when one is concerned with the least robust prediction from a set of
predictions. But our collective certificate benefits from having tight robustness guarantees for each
of the individual predictions. Holm correction can be used with arbitrary hypothesis tests. For
instance, we can use a different threshold Tn per output gn , i.e. test whether
α
BinP (ν∏(k),N2, ≤,Tpi(k)) < D { + 1 - k	(113)
for k = 1, . . . , Dout . In particular, we can use
α
Tn =SuP s.t. BmP(Vn,N2, ≤,t) < ŋ----------;------1^ ,	(114)
t	Dout + 1 - π-1 (n)
i.e. choose the largest threshold such that the null hypothesis can still be rejected. Eq. 114 is the
lower Clopper-Pearson confidence bound with significance D t+ι-∏-i(n) ∙ ThiS means that, instead
of performing hypothesis tests, we can obtain probabilistic lower bounds q。≤ qn by computing
Clopper-Pearson confidence bounds with significance parameters To- ,∙∙∙, α. The q can then be
ɪ^out	1	∙ln
used to compute the base certificates. Due to the definition of the Tn , all of the null hypotheses are
rejected, i.e. we obtain valid probabilistic lower bounds on all qn . We can thus use the abstention
mechanism from Section C.1, i.e. only abstain if q。≤ 0.5.
A direct consequence of the results above is that using Clopper-Pearson confidence intervals and
Holm correction will yield stronger per-prediction robustness guarantees and lower abstention rates
28
Under review as a conference paper at ICLR 2022
than the method of Fischer et al. (2021). The Clopper-Pearson-based method only abstains if one
cannot guarantee that qn > 0.5 with high probability, while their method abstains if one cannot
guarantee that qn ≥ τ with τ ≥ 0.5 (or specific other predictions abstain). For all non-abstaining
predictions, the Clopper-Pearson-based certificate will be at least as strong as that obtained using
a single threshold τ , as it computes the tightest bound for which the null hypothesis can still be
rejected (see Eq. 114). Consequently, a naive collective robustness certificate (i.e. counting the
number of predictions whose robustness are guaranteed by the base certificates) based on Clopper-
Pearson bounds will also be stronger. It should however be noted that this is only relevant for our
notion of collective robustness, not for the one considered by Fischer et al. (2021). It should also
be noted that their method could potentially be used with other methods of family-wise error rate
correction, although they state that “these methods do not scale to realistic segmentation problems”
and do not discuss any further details.
Our above discussion shows that, for our notion of collective robustness, the naive collective cer-
tificate using Clopper-Pearson confidence bounds and Holm correction is at least as strong as that
of Fischer et al. (2021) (or alternatively: the certificate of Fischer et al. (2021) is a naive collective
certificate with Holm correction, but with weaker per-prediction certificates). Conversely, the certifi-
cate based on Clopper-Pearson confidence bounds is at least as strong as that of Fischer et al. (2021)
when certifying their notion of collective robustness, i.e. determining whether all non-abstaining
predictions are robust, given adversarial budget . To certify this notion of robustness, they simply
iterate over all predictions and determine whether all non-abstaining predictions are certifiably ro-
bust, given . Naturally, as the Clopper-Pearson-based certificates are stronger, any prediction that
is robust according to (Fischer et al., 2021) is also robust acccording to the Clopper-Pearson-based
certificates. The only difference is that, for τ > 0.5, their method will have more abstaining predic-
tions. But, due to the direct correspondence of Clopper-Pearson confidence bounds and Binomial
tests, we can modify our abstention mechanism to obtain exactly the same set of abstaining pre-
dictions: We simply have to use qn ≤ τ instead of qn ≤ 0.5 as our abstention criterion. Finally, it
should be noted that the proposed collective certificate is at least as strong as the naive collective cer-
tificate (see Section 4). Thus, letting the set of targeted predictions T be the set of all non-abstaining
predictions and checking whether the collective certificate guarantees robustness for all of T will
also result in a certificate that is at least as strong as that of Fischer et al. (2021) in their setting.
In our experiments (see Section 8), We use Holm correction for our naive collective certificate and
the weaker Bonferroni correction for our base certificates. This is only meant to slightly skew the
results in favor of our baselines. Holm correction can in principle also be used when computing the
base certificates, in order to improve our proposed collective cert.
29
Under review as a conference paper at ICLR 2022
D Comparison to the collective certificate of Schuchardt et al.
In the following, we first present the collective certificate for binary graph-structured data proposed
by Schuchardt et al. (2021) (see Section D.1. We then show that, when using sparsity-aware smooth-
ing distributions (BojcheVski et al., 2020) - the family of smoothing distributions used both in our
work and that of Schuchardt et al. (2021) - our certificate subsumes their certificate. That is, our
collectiVe robustness certificate based on localized randomized smoothing can proVide the same
robustness guarantees (see Section D.2).
D. 1 The collective certificate
Their certificate assumes the input space to be G = {0, 1}N×D × {0, 1}N ×N - the set of undirected
attributed graphs with N nodes and D attributes per node. The model is assumed to be a multi-
output classifier f : G → YN that assigns a label from label set Y to each of the nodes. GiVen an
input graph G = (X, A) and a corresponding prediction y = f (G), they want to certify collectiVe
robustness to a set of perturbed graphs B ⊆ G. The perturbation model B is characterized by four
scalar parameters rX+ , rX- , rA+, rA+ ∈ N0, specifying the number of bits the adVersary is allowed to
add (0 → 1) and delete (1 → 0) in the attribute and adjacency matrix, respectiVely. It can also
be extended to feature additional constraints (e.g. per-node budgets). We discuss how these can be
integrated after showing our main result. A formal definition of the perturbation model can be found
in Section B of (Schuchardt et al., 2021).
The goal of their work is to certify collectiVe robustness for a set of targeted nodes T ⊆ {1, . . . , N},
i.e. compute a lower bound on
m0inXI[fn(G0)=yn].	(115)
G0∈B
n∈T
Their approach to obtaining this lower-bound shares the same high-leVel idea as ours: Combining
per-prediction base certificates and leVeraging some notion of locality. But while our method uses
localized randomized smoothing, i.e. smoothing different outputs with different non-i.i.d. smoothing
distributions, to obtain base certificates that encode locality, their method uses a-priori knowledge
about the strict locality of the classifier f . A model is strictly local if each of its outputs fn only
operates on a well-defined subset of the input data. To encode this strict locality, Schuchardt et al.
(2021) associate each output fn with an indicator Vector ψ (n) and an indicator matrix Ψ(n) that
fulfill
ND	NN
X X ψm(n)I Xm,d 6=Xi0,j + X X Ψ(mn)I Am,d 6=A0i,j =0
m=1 d=1	i=1 j=1
(116)
=⇒ fn(X,A) =fn(X0,A0).
for any perturbed graph G0 = (X0, A0). Eq. 116 expresses that the prediction of output fn remains
unchanged if all inputs in its receptiVe field remain unchanged. ConVersely, it expresses that per-
turbations outside the receptiVe field can be ignored. Unlike in our work, Schuchardt et al. (2021)
describe their base certificates as sets in adVersarial budget space. That is, some certification proce-
dure is applied to each output fn to obtain a set
K(n)	⊆	[rX+]	×	[rX-]	×	[rA+]	×	[rX-]	(117)
with [k] = {0, . . . , k}. If c+X c-X c+A c-AT ∈ K(n), then prediction yn is robust to any
perturbed input with exactly c+X attribute additions, c-X attribute deletions, c+A edge additions and
c-A edge deletions. A more detailed explanation can be found in Section 3 of (Schuchardt et al.,
2021). Note that the base certificates only depend on the number of perturbations, not their loca-
tion in the input. Only combining them using the receptiVe field indicators from Eq. 116 makes it
possible to obtain a collective certificate that is better than a naive combination of the base certifi-
cates (i.e. counting how many predictions are certifiably robust to the collectiVe threat model). The
30
Under review as a conference paper at ICLR 2022
resulting collective certificate is
b+ b+mB+	B-	XI ]h(ψ(n))τbx	(ψ(n))Tbχ	Pi,j	ψ(n)B+j	Pi,j 叱峭口、K(n)
, ,	,	n∈T
(118)
N	N	NN	NN
s.t. Xb+m	≤rX+,	Xb-m ≤rX-,	X X Bi+,j	≤rA+,	X X Bi-,j	≤ rA-,	(119)
m=1	m=1	i=1 j=1	i=1 j=1
b+, b- ∈ N0N B+, B-∈ N0N×N.	(120)
The variables defined in Eq. 120 model how the adversary allocates their adversarial budget, i.e. how
many attributes are perturbed per node and which edges are modified. Eq. 119 ensures that this
allocation in compliant with the collective threat model. Finally, in Eq. 118 the indicator vector and
matrix ψ(n) and Ψ(n) are used to mask out any allocated perturbation budget that falls outside the
receptive field of fn before evaluating its base certificate.
To solve the optimization problem, Schuchardt et al. (2021) replace each of the indicator functions
with binary variables and include additional constraints to ensure that they have value 1 i.f.f. the
indicator function would have value 1. To do so, they define one linear constraint per point separating
the set of certifiable budgets K(n) from its complement K(n) in adversarial budget space (the ”pareto
front” discussed in Section 3 of (Schuchardt et al., 2021)).
From the above explanation, the main drawbacks of this collective certificate compared to our lo-
calized randomized smoothing approach and corresponding collective certificate should be clear.
Firstly, if the classifier f is not strictly local, i.e. the receptive field indicators ψ and Ψ only have
non-zero entries, then all base certificates are evaluated using the entire collective adversarial bud-
get. It thus degenerates to the naive collective certificate. Secondly, even if the model is strictly
local, each of the output may assign varying levels of importance to different parts of its receptive
field. Their method is incapable of capturing this additional soft locality. Finally, their means of
evaluating the base certificates may involve evaluating a large number of linear constraints. Our
method, on the other hand, only requires a single constraint per prediction. Our collective certificate
can thus be more efficiently computed.
D.2 Proof of subsumption
In the following, we show that any robustness certificate obtained by using the collective certificate
of Schuchardt et al. (2021) with sparsity-aware randomized smoothing base certificates can also be
obtained by using our proposed collective certificate with an appropriately parameterized localized
smoothing distribution. The fundamental idea is that, for randomly smoothed models, completely
randomizing all input dimensions outside a receptive field is equivalent to masking out any pertur-
bations outside the receptive field.
First, we derive the certificate of Schuchardt et al. (2021) for predictions obtained via sparsity-
aware smoothing. Schuchardt et al. (2021) require base certificates that guarantee robustness when
c+X c-X c+A c-AT ∈ K(n), where the c indicate the number of added and deleted attribute and
adjacency bits. That is, the certificates must only depend on the number of perturbations, not on
their location. To achieve this, all entries of the attribute matrix and all entries of the adjacency
matrix, respectively, must share the same distribution. For the attribute matrix, they define scalar
distribution parameters p+X, p-A ∈ [0, 1]. Given attribute matrix X ∈ {0, 1}N×D, they then sample
random attribute matrices ZX that are distributed according to sparsity-aware smoothing distribu-
tion S (X, 1 ∙ PX, 1 ∙ PX) (see Section B.2.2), i.e.
Pr[(Zχ)m,d = 0] = (1 -PX)I-Xm,d ∙ (PX)Xm,d ,
Pr[(Zχ)m,d = 1] = (PX)1-Xm,d ∙ (1 - PX)Xm,d .
Given input adjacency matrix A, random adjacency matrices ZA are sampled from the distribution
S (A, 1 ∙ pA, 1 ∙ PA). Applying Theorem 2 (to the flattened and concatenated attribute and adja-
cency matrices) shows that smoothed prediction yn = fn (X,A) is robust to the perturbed graph
31
Under review as a conference paper at ICLR 2022
(X0, A0) if
ND
SX SX γX ∙ I [Xm,d = 0 = Xm,d] + IX ∙ 1 [Xm,d = 1 = Xm,d]
m=1 d=1
NN
+ XX yA ∙ 1 [Ai,j = 0 = Ai,j] + YAT [Ai,j = 1 = Ai,j]
< η(n)
(121)
with
ln
γX+
W +
inG⅛+
(1-Pa)2
PA
(1-Px )2
,γA- = ln
PX J
(1-pA)2
p-A
γX-
+ ≡
ln(	+
-
pX
.)and ηS) = In (1 + σ⅛2 (μ(n) - 1 )21
≡),	γ+
where μ(n) is the mean and σ(n) is the variance of the base classifier's output distribution, given the
input smoothing distribution. Since the indicator functions for each perturbation type in Eq. 121
share the same weights, Eq. 121 can be rewritten as
X+ c+X +γX- c-X +γA+ c+A +γA-c-A ≤ η(n),	(122)
where c+X , c-X , c+A , c-A are the overall number of added and deleted attribute and adjacency bits,
respectively. Eq. 122 matches the notion of base certificates defined by Schuchardt et al. (2021),
i.e. it corresponds to a set K(n) in adversarial budget space for which we provably know that pre-
diction yn is certifiably robust if [c+X c-X c+A c-A]T ∈ K(n). When this base certificate is used,
i.e. we insert the base certificate Eq. 122 into objective function Eq. 118, the collective certificate of
Schuchardt et al. (2021) becomes equivalent to
b+b+mBin+B-XIγX+	ψ(n)T	b+X	+ γX-	ψ(n)T	b-X
,	,	,	n∈T
+γ+ X 鸣助+XCYA叱)% ≤ n(n)#	(123)
i,j	i,j
N	N	NN	NN
s.t. Xb+m	≤rX+ ,	Xb-m	≤rX-,	XXB+i,j ≤rA+,	X X B-i,j	≤rA-,	(124)
m=1	m=1	i=1 j=1	i=1 j=1
b+,b- ∈ N0N B+, B- ∈ N0N×N.	(125)
Next, we show that obtaining base certificates through localized randomized smoothing with ap-
propriately chosen parameters and using these base certificates within our proposed collective cer-
tificate (see Section 6) will result in the same optimization problem. Instead of using the same
smoothing distribution for all outputs, we use different distribution parameters for each one. For the
n’th output, we sample random attributes matrices from distribution S X, Θ+X(n), Θ-X (n) with
Θ+X (n), Θ-X (n) ∈ [0, 1]N×D. Note that, in order to avoid having to index flattened vectors, we over-
load the definition of sparsity-aware smoothing to allow for matrix-valued parameters. For example,
the value Θ+X (nn,d) indicates the probability of flipping the value of input attribute Xn,d from 0 to 1
(n)
and the value Θ-X n,d indicates the probability of flipping the value of input attribute Xn,d from 1 to
0. We choose the following values for these parameters:
Θ+X(mn,)d=ψm(n)	∙p+X+	1 - ψm(n)	∙ 0.5,	(126)
Θ-X(mn,)d=ψm(n)	∙p-X+	1 - ψm(n)	∙ 0.5,	(127)
where ψ(n) is the receptive field indicator vector defined in Eq. 116 and p+X, ∙p-X ∈ [0, 1] are
the same flip probabilities we used for the certificate of Schuchardt et al. (2021). Due to this
parameterization, attribute bits inside the receptive field are randomized using the same distribu-
tion as in the certificate of Schuchardt et al. (2021), while attribute bits outside are set to either
32
Under review as a conference paper at ICLR 2022
0 or 1 with equal probability. Similarly, we sample random adjacency matrices from distribution
S A,Θ+A(n),Θ-A(n) withΘ+A(n),Θ-A(n) ∈ [0, 1]N×D and
θA(nj)=Ψ(j) ∙ pA+(1- Ψ(j))∙ 0.5,	(128)
θAUnj=ψ(j) ∙ PA + (1 - ψ(j ∙ 0.5,	(129)
where Ψ(n) is the receptive field indicator matrix defined in Eq. 116. Note that, since we only alter
the distribution of bits outside the receptive field, the smoothed prediction yn = fn (X, A) will
be the same as the one obtained via the smoothing distribution used by Schuchardt et al. (2021).
Applying Theorem 2 (to the flattened and concatenated attribute and adjacency matrices) shows that
smoothed prediction yn = fn (X, A) is robust to the perturbed graph (X0, A0) if
ND
X X τXm,d ∙ I [Xm,d =0 = Xm,d] + TXm,d ∙ 1 [Xm,d = 1 = Xm,d]
m=1 d=1
N N	(130)
+X X TAijT [A，，=0 = Ai,j]+ τ-i,j ∙ I [Ai,j =1 = Ai,j]
i=1 j=1
< η(n).
Because we only changed the distribution outside the receptive field, the scalar η(n), which depends
on the output distribution,s mean and variance μ and σ will be the same as the one obtained via the
smoothing scheme used by Schuchardt et al. (2021) et al. Due to Theorem 2 and the definition of
our smoothing distribution parameters in Eqs. (126) to (129) , the scalars TX+m,d, TX-m,d, TA+i,j, TA-i,j
have the following values:
T+ X m,d	=ψmn ∙ YX +(1-ψmmn) ∙ 2 ∙ln	(1	-0.5)2 -05-	+	0.52 ʌ 1 - 0.5	(131)
TX m,d	=Ψmn ∙ Yx +(1-Ψm) 2」n	(1	-0.5)2 -05	+	0.52! 1 - 0.5	(132)
TA i,j	=叱) ∙ Y- + (1 - 叱))∙ 2 ∙ ln	(1	-0.5)2 -05	+	0.52! 1 - 0.5	(133)
TA-i,j =	ψ(n? ∙ Y-+(1 -田黑)2 ∙ ln (	(1	-0.5)2 05	+1	0.52! 一一0.5 ,	(134)
where the γ are the same weights as those of the base certificate Eq. 121 of Schuchardt et al.
(2021). Inserting the above values of T into the base certificate Eq. 130 and using the fact that
ln	+
0.52
1-0.5
= ln(1) = 0 results in
ND
XX ψmn ∙ YX ∙ I [Xm,d = 0 = Xm ,d] + ψmn ∙ YX ∙ I [Xm,d = 1 = Xm ,d]
m=1 d=1
NN
+ XX ψ(j) ∙ Ya ∙ I [Ai,j =0 = Ai,j] + ψ(j) ∙ YrI [Ai,j = 1 = Ai,j]
< η(n).
(135)
While our collective certificate derived in Section 6 only considers one perturbation type, we have
already discussed how to certify robustness to perturbation models where there are multiple pertur-
bation types in Section B.2.2: We use a different budget variable per input dimension and perturba-
tion type. Furthermore, the attribute bits of each node share the same noise level. Therefore, we can
use the method discussed in Section A.3, i.e. use a single budget variable per node instead of using
33
Under review as a conference paper at ICLR 2022
one per node and attribute. Modelling our collective problem in this way, using Eq. 135 as our base
certificates and rewriting the first two sums using inner products results in the optimization problem
b+ b+mBin+ B-	X I	γX+	ψ(n)T	b+X	+ γX-	ψ(n)T	b-X
, ,	,	n∈T
+γ+ X ψinj)B+j + X YA嚅B-j ≤ n(n)#	(136)
i,j	i,j
N	N	NN	NN
s.t. Xb+m	≤rX+,	Xb-m	≤rX-,	XXB+i,j	≤rA+,	XXB-i,j	≤rA-,	(137)
m=1	m=1	i=1 j=1	i=1 j=1
b+, b- ∈ N0N B+, B- ∈ N0N×N.	(138)
This optimization problem is identical to that of Schuchardt et al. (2021) from Eqs. (123) to (125).
The only difference is in how these problems would be mapped to a mixed-integer linear program.
We would directly model the indicator functions in the objective using a single linear constraint.
Schuchardt et al. (2021) would use multiple linear constraints, each corresponding to one point in
the adversarial budget space.
To summarize: For randomly smoothed models, masking out perturbations using a-priori knowl-
edge about a model’s strict locality is equivalent to completely randomizing (here: flipping bits with
probability 50%) parts of the input. While Schuchardt et al. (2021) only derived their certificate for
binary data, it is conceivable that their approach could be applied to strictly local models for con-
tinuous data. Considering our certificates for Gaussian (Proposition 1) and (Proposition 2) uniform
smoothing, where the base certificate weights are = and λ ,respectively, it should again be possible
to perform the same masking operation as Schuchardt et al. (2021) by using σ → ∞ and λ → ∞.
Finally, it should be noted that the certificate by Schuchardt et al. (2021) allows for additional con-
straints, e.g. on the adversarial budget per node or the number of nodes controlled by the adversary.
As all of them can be modelled using linear constraints on the budget variables (see Section C of their
paper), they can be just as easily integrated into our mixed-integer linear programming certificate.
E Experimental setup and additional experiments
In the following, we first explain the metrics we use for measuring the strength of certificates, and
how they can be applied to the different types of randomized smoothing certificates used in our
experiments. We then discuss the experimental setup, as well hyperparameters and additional ex-
perimental results for our semantic segmentation Section E.2 and node classification Section 8.2
experiments.
E.1 Metrics
We use two metrics for measuring certificate strength: Certified accuracy (i.e. the percentage of
correct and certifiably robust predictions) and certified ratio (i.e. the percentage of certifiably robust
predictions, regardless of correctness). We use Monte Carlo randomized smoothing (see Section C.
Therefore, we may have to abstain from making predictions. Abstentions are counted as non-robust
and incorrect. In the case of center smoothing, either all or none predictions abstain (this is inherent
to the method. In our experiments, center smoothing never abstained). In the following, let Z =
{d ∈ {1,..., Dout} | yn = yn} be the indices of correct predictions, given an input x.
Naive collective certificate. The naive collective certificate certifies each prediction independently.
Like with our base certificates, let H(n) be the set of perturbed inputs yn is robust to. Let Bx be
the collective perturbation model. Then L = d ∈ {1, . . . , Dout } | Bx ⊆ H(n) is the set of all
certifiably robust predictions. The certified ratio can be computed as
DLY and the certified accuracy
as 1H.
Center smoothing Center smoothing does not determine which predictions are robust, but only how
many are. Let k be the number of certifiably robust predictions. The certified ratio can be easily
34
Under review as a conference paper at ICLR 2022
computed as Dk-. For the certified accuracy, We have to make the worst-case assumption that the
Dout
non-robust predictions are correct predictions, i.e. compute
max(0,k-(Dout-∣Z∣))
Dout
Collective certificate. Let l(T) be the optimal value of our collective certificate and set of targeted
nodes T. Then both the certified ratio and certified accuracy can be computed via ∙l(T) with T =
Dout
{1, . . . , Dout} and T = Z, respectively.
E.2 Semantic segmentation
We first list all parameters and details of our training and certification procedures, before discussing
additional experimental results in Section E.2.3.
E.2. 1 Experimental setup and hyperparameters
Model. As the base model for the semantic segmentation task, we use a U-Net model (Ronneberger
et al., 2015) with a ResNet-18 (He et al., 2016) backbone, as implemented by the Pytorch Segmen-
tation Models library (version 0.13) (Yakubovskiy, 2020). We use the library’s default parameters.
In particular, the inputs to the U-Net segmentation head are the features of the ResNet model after
the first convolutional layer and after each ResNet block (i.e. after every fourth of the subsequent
layers). The U-Net segmentation head uses (starting with the original resolution) 16, 32, 64, 128
and 256 convolutional filters for processing the features at the different scales. To avoid dimension
mismatches in the segmentation head, all input images are zero-padded to a height and width that is
the next multiple of 32.
Data and preprocessing. We evaluate our certificates on the Pascal-VOC 2012 segmentation vali-
dation set. We do not use the test set, because evaluating metrics like the certified accuracy requires
access to the ground-truth labels. For training, we use the 10582 Pascal segmentation masks ex-
tracted from the SBD dataset (Hariharan et al., 2011) (referred to as ”Pascal trainaug” or ”Pascal
augmented training set” in other papers). SBD uses a different data split than the official Pascal-
VOC 2012 segmentation dataset. We avoid data leakage by removing all training images that appear
in the validation set. We downscale both the training and the validation images and ground-truth
masks to 50% of their original height and width, so that we can use larger batch sizes and thus
use our compute time to more thoroughly evaluate a larger range of different smoothing distribu-
tions. The segmentation masks are downscaled using nearest-neighbor interpolation, the images are
downscaled using the INTER_AREA operation implemented in OPenCV (Bradski, 2000).
Training and data augmentation. We initialize our model weights using the weights provided by
the Pytorch Segmentation Models library, which were obtained by pre-training on ImageNet. We
train our models for 256 epochs, using Dice loss with batch size 64 and Adam(lr = 0.001, β1 =
0.9, β2 = 0.999, t = 10-8, Weight_decay = 0. Every 8 epochs, we compute the mean IOU on
the validation set. After training, we use the model that achieved the highest validation mean IOU.
We apply the following train-time augmentations: Each image is randomly shifted by up to 10%
of its height and width, scaled by a factor from [0.5, 2.0] and rotated between -10 and 10 degrees
using the ShiftScaleRotate augmentation implemented by the Albumentations library (version 0.5.2)
(Buslaev et al., 2020). The images are than cropped to a fixed size of 128×160. Where necessary, the
images are padded with zeros. Padded parts of the segmentation mask are ignored during training.
After these operations, each input is randomly perturbed using a Gaussian distribution with a fixed
standard deviation σ ∈ {0, 0.01, . . . , 0.5}. All samples are clipped to [0, 1] to retain valid RGB-
images.
Certification. We evaluate all certificates on the first 50 images from the validation set that - after
downscaling - have a resolution of 166 X 250. For our experiments, we use Monte Carlo randomized
smoothing (see discussion in Section C). We use 12288 samples for making smoothed predictions
and 153600 samples for certification. We use the significance parameter α to 0.01, i.e. all certifi-
cates hold with probability 0.99. For the center smoothing baseline, we use the default parameters
suggested by the authors (△ = 0.05, β = 2, αι = ɑ2). For the naive collective certificate baseline,
we use Holm correction to account for the multiple comparisons problem, which yields strictly bet-
ter results than Bonferroni correction. For our localized smoothing certificates, we use Bonferroni
correction. For our localized smoothing distribution, we partition the input image into a regular grid
of size 4 × 6 and define minimum standard deviation σmin and maximum standard deviation σmax .
35
Under review as a conference paper at ICLR 2022
Let J(k,l) be the set of all pixel coordinates in grid cell (k, l). To smooth outputs in grid cell (i, j),
we use a smoothing distribution N(0, diag(σ) with, ∀k ∈ {1, . . . , 4}, l ∈ {1, . . . , 6}, d ∈ J(k,l),
max (|i - k|, |l -j|)
σd = σmin + (σmax - σmin) • -------7---------
6
(139)
i.e. we linearly interpolate between σmin and σmax based on the l∞ distance of grid cells (i, j)
and (k, l). All results are reported for the relaxed linear programming formulation of our collective
11
certificate (See section a.4). For each grid ceil, We use 24 — i4^^6 of the Sa^mPles, WhiCh corresponds
to 512 samples for prediction and 6400 samples for certification. The collective linear program is
solved using MOSEK (version 9.2.46) (MOSEK ApS, 2019) through the CVXPY interface (version
1.1.13) (Diamond & Boyd, 2016).
E.2.2 Hardware
All experiments on image segmentation Were performed using a Xeon E5-2630 v4 CPU @ 2.20GHz,
an NVIDA GTX 1080TI GPU and 128 GB of RAM.
E.2.3 Complete experimental evaluation
As explained in Section 8.1, the objective of our experiments on semantic segmentation is to verify
our claim that localized smoothing alloWs for a better accuracy-robustness trade-off than random-
ized smoothing With i.i.d. distributions, i.e. it alloWs us to retain a higher prediction quality and
simultaneously provide stronger collective robustness guarantees.
The first step of our evaluation procedure is to evaluate the naive and the center smoothing baselines
for Gaussian standard deviations σ ∈ {0.1,0.2,0.3,0.4,0.5} and - for each σ 一 find a combination
of localized smoothing parameters σmin , σout that result in equal or higher accuracy and stronger
certificates, as measured by certified accuracy and certified ratio. Figs. 5 to 8 shoW the certified
accuracy and certified ratio curve for all 10 pairs of baseline and locally smoothed models (5 per
baseline). The AUC of the certified ratio and certified ratio curves for all smoothed models are
summarized in Tables 1 and 2. Safe for center smoothing With σ = 0.2, Which has a slightly higher
certified ratio but loWer accuracy, We can outperform all baselines.
The second step is to fix the found localized smoothing parameters and perform a fine-grained search
over σ ∈ [0, 1]. This is to ensure that our previous results Were not caused by a particularly poor
choice of baseline parameters. Table 5 summarizes the results. It shoWs that there is no σ in our
search space for Which the baselines outperform the locally smoothed model W.r.t. our metrics of
certificate strength.
Figure Fig. 4 demonstrates Why there is no σ that outperforms our baselines, using the certified ratio
of the naive collective certificate baseline as an example. The certificate strength, as measured by
the certified ratio’s AUC, increases With σ. But the model’s accuracy decreases With σ. Therefore,
strengthening the certificate requires sacrificing some accuracy. But the locally smoothed models
Were already as accurate or more accurate than the baselines, Which makes it impossible for the base-
lines to simultaneously attain a higher accuracy and stronger certificates. To summarize: Localized
smoothing appears to indeed offer a better trade-off betWeen certified accuracy and robustness.
In Section 8.1, We mentioned that We compare base classifiers With different trained Weights. For
i.i.d. smoothing With σ, We loaded models trained With σtrain = σ. For localized smoothing, We
loaded models trained With σtrain = σmin . To shoW that this is not the reason Why We outperform
the baselines, We compare the same i.i.d. and localized smoothing distributions as before, but use
σtrain = σmin for all models. The results are summarized in Appendix E.2.3 and Table 4. Overall,
there is only a single instance in Which the baselines are better than the localized smoothing cer-
tificate in any of our metrics: For σ — 0.3, the naive baseline attains an accuracy of 79.5% that is
0.4 p.p. higher than that of the locally smoothed model. The AUC of its certified accuracy and cer-
tified ratio curves - 0.492 and 0.5615, respectively - are however much smaller than the 0.5869 and
0.6579 guaranteed by localized smoothing. So, even With this different choice of Weights, localized
smoothing appears to guarantee a better trade-off betWeen accuracy and provable robustness.
While it resulted in significantly stronger certificates, the proposed collective certificate added very
little additional computational cost, compared to the naive collective certificate. The naive collec-
tive certificate requires taking Monte Carlo samples and then performing a small number of vector
36
Under review as a conference paper at ICLR 2022
operations to compute the per-prediction Gaussian smoothing certificates. The proposed collective
certificates requires taking Monte Carlo samples and computing the same type of per-prediction
certificate, but then solves a linear program on top. The sampling dominated the runtime, with an
average 460 s per image (both for the baseline and the proposed certificate, as we used the same num-
ber of samples for both). Averaged over all experiments and images, computing the per-prediction
certificates took 15.85 s for the baseline and 0.15 s for the proposed certificate (the increased cost
for the baseline is due to the more complicated Holm correction - with Bonferroni correction both
are equally fast). Averaged over all images and tested adversarial budgets, solving each collective
linear program only took 0.68 s, i.e. much less than the Monte Carlo sampling that is necessary for
both the baseline and our method.
o.0
0.0
6 4 2 _
♦ ♦ ♦
Ooo-
o∩vJ p0
0.1	0.2	0.3	0.4	0.5
σ
AoEjnSv
1.0
0.8
0.6
0.4
0.2
0.0
0.0	0.1	0.2	0.3	0.4	0.5
σ
(a) Certificate strength
(b) Accuracy
Figure 4: Trade-Off between accuracy and certifiably robustness at the example of the naive collec-
tive certificate and certified ratio AUC. Increasing the standard deviation σ strengthens the certifi-
cate, but decreases the accuracy.
37
Under review as a conference paper at ICLR 2022
Naive collective certificate				Localized smoothing				
σ	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio	σmin	σmax	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio
=OT	87.7%	0.1555	0.1702 二	0.055	0.28	87.7%	0.1807	0.193 二
0.2	83.8%	0.3127	0.3515	0.16	0.35	83.9%	0.3627	0.3925
0.3	79.1%	0.4899	0.548	0.25	0.7	79.1%	0.5869	0.6579
0.4	72.5%	0.5426	0.6067	0.25	1.5	76.4%	0.7764	0.8999
0.5	64.9%	0.5684	0.6444	0.3	1.5	74.8%	0.7143	0.8091
Table 1: Comparison of the naive collective certificate baseline and localized smoothing for σ ∈
{0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. In all cases, the localized smoothing certificate
retains an equal or higher accuracy and simultaneously provides stronger robustness guarantees.
Center smoothing				Localized smoothing				
σ	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio	σmin	σmax	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio
=OT	88.3%	0.1673	0.194	0.08	0.15	88.4%	0.1889	0.2004 二
0.2	84.7%	0.281	0.3464	0.12	0.4	84.8%	0.3151	0.3427
0.3	80.3%	0.3192	0.4339	0.2	0.6	80.5%	0.4935	0.5533
0.4	75%	0.2316	0.3720	0.25	1.5	76.4%	0.7764	0.8999
0.5	68.3%	0.1498	0.2914	0.25	1.5	76.4%	0.7764	0.8999
Table 2: Comparison of the center smoothing baseline and localized smoothing for σ ∈
{0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. Safe for σ = 0.2, where the baseline has a
slightly higher certified ratio AUC but a lower accuracy, localized smoothing retains an equal or
higher accuracy and simultaneously provides stronger robustness guarantees.
Naive collective certificate				Localized smoothing				
σ	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio	σmin	σmax	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio
=OT	86.9%	0.1519	0.1686 二	0.055	0.28	87.7%	0.1807	0.193 二
0.2	83.8%	0.3184	0.357	0.16	0.35	83.9%	0.3627	0.3925
0.3	79.5%	0.492	0.5615	0.25	0.7	79.1%	0.5869	0.6579
0.4	73.5%	0.6094	0.711	0.25	1.5	76.4%	0.7764	0.8999
0.5	72.5%	0.7001	0.7981	0.3	1.5	74.8%	0.7143	0.8091
Table 3: Comparison of the naive collective certificate baseline and localized smoothing for
σ ∈ {0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. Unlike in Table 1, we also load the
model trained with σmin for the baselines. Safe for σ = 0.3, where the baseline has a higher accu-
racy but much weaker certificates, localized smoothing again offers a higher accuracy and stronger
certificates.
Center smoothing				Localized smoothing				
σ	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio	σmin	σmax	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio
=OT	88.3%	0.1624	0.19	0.08	0.15	88.4%	0.1889	0.2004 二
0.2	83.9%	0.268	0.3392	0.12	0.4	84.8%	0.3151	0.3427
0.3	80%	0.3721	0.5009	0.2	0.6	80.5%	0.4935	0.5533
0.4	74.9%	0.3494	0.5397	0.25	1.5	76.4%	0.7764	0.8999
0.5	62.9%	0.1656	0.4212	0.25	1.5	76.4%	0.7764	0.8999
Table 4: Comparison of the center smoothing baseline and localized smoothing for σ ∈
{0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. Unlike in Table 2, we also load the model
trained with σmin for the baselines. In all cases, the locally smoothed model has a higher accuracy
and stronger certificates.
38
Under review as a conference paper at ICLR 2022
Localized smoothing					Baselines		
σmin	σmax	Acc.	AUC of Cert. Acc.	AUC of Cert. Ratio	Baseline	Best AUC of Cert. Acc	Best AUC of Cert. Ratio
0.055	0.28	87.7%	0.1807	0.193 二		0.1555	0.1702 二
0.16	0.35	83.9%	0.3627	0.3925	Naive	0.296	0.3293
0.25	0.7	79.1%	0.5869	0.6579	Collective	0.4899	0.548
0.25	1.5	76.4%	0.7764	0.8999	Certificate	0.5753	0.6465
0.3	1.5	74.8%	0.7143	0.8091		0.5753	0.6465
0.08	0.15	88.4%	0.1889	0.2004 二		0.1488	0.1704 二
0.12	0.4	84.8%	0.3151	0.3427	Center	0.2715	0.3286
0.2	0.6	80.5%	0.4935	0.5533	Smoothing	0.3306	0.4255
0.25	1.5	76.4%	0.7764	0.8999		0.3538	0.5029
Table 5: Optimizing baseline standard deviation σ. For the same (σmin , σmax) as in Tables 1 and 2,
we perform a grid search for σ ∈ [0, 0.5] that yield certified accuracy or certified ratio curves with
higher AUC, and an accuracy that at least equals the locally smoothed models. Such a σ could not
be found. Localized smoothing appears to offer a better accuracy-robustness trade-off.
39
Under review as a conference paper at ICLR 2022
1.0
8 6 4 2
♦ ♦ ♦ ♦
Oooo
1ej pəpjəo
1.0
0.0
0.0	0.5	1.0	1.5	2.0
Adversarial budget e
8 6 4 2
♦ ♦ ♦ ♦
Oooo
∙1ej pəp≡əo
0.0
0.0	0.5	1.0	1.5	2.0
Adversarial budget e
(a) σ = 0.1,σmin = 0.055,σmaχ = 0.28
1.0
8 6 4 2
♦ ♦ ♦ ♦
Oooo
1ej pəujəo
0.0
0.0	0.5	1.0	1.5
Adversarial budget e
(b) σ = 0.2, σmin = 0.16, σmaχ = 0.35
1.0
8 6 4 2
♦ ♦ ♦ ♦
Oooo
∙1ej pəujəo
2.0
0.0
0.0	0.5	1.0	1.5	2.0
Adversarial budget e
(c) σ = 0.3, σmin = 0.25, σmax =0.7
(d)σ= 0.4, σmin = 0.25, σmax = 1.5
0 8 6 4 2
-----
Ioooo
J pəhijəo
0.0
0.0	0.5	1.0
1.5	2.0
Adversarial budget e
(e) σ = 0.5, σmin = 0.3, σmax = 1.5
Figure 5:	Certified ratios of U-Net models under varying adversarial budgets e. We compare the
naive i.i.d. smoothing baseline (with standard deviation σ) to localized smoothing (with param-
eters σmin , σmax such that the locally smoothed model has a higher or equal accuracy). Com-
bining the localized smoothing base certificates using the proposed linear program (solid orange
line) instead of evaluating them independently (dotted orange line) yields stronger guarantees. For
σ ∈ {0.3, 0.4, 0.5}, the localized smoothing certificate outperforms the baseline for all e.
40
Under review as a conference paper at ICLR 2022
AoEjn8E P°≡1J°□
O
.
O
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnOOE p-jəo
O
.
O
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnOOE p-jəo
O
.
2
Adversarial budget e
Adversarial budget e
O
.
O
6 4 2 0
♦ ♦ ♦ ♦
Oooo
(a) σ = 0.1, σmin = 0.055, σmax = 0.28
O 8
♦ ♦
1 O
O
.
O
8 6 4 2 0
-----
Ooooo
(b) σ = 0.2, σmin = 0.16, σmax = 0.35
Adversarial budget e
(c) σ = 0.3, σmin = 0.25, σmax =0.7
Adversarial budget e
(d)σ= 0.4, σmin = 0.25, σmax = 1.5
O
.
O
2 O
♦ ♦
O O
0 8 6 4
♦ ♦ ♦ ♦
Iooo
5
.
O
5
.
O
O
.
2
5
.
O
O
.
2
AOEJnSE p0se°□
5
.
O

5
.
O
O
.
2
O
.
2
Adversarial budget e
(e) σ = 0.5, σmin = 0.3, σmax = 1.5
Figure 6:	Certified accuracies of U-Net models under varying adversarial budgets . We compare
the naive collective certificate baseline (with standard deviation σ) to localized smoothing (with
parameters σmin , σmax such that the locally smoothed model has a higher or equal accuracy). Com-
bining the localized smoothing base certificates using the proposed linear program (solid orange
line) instead of evaluating them independently (dotted orange line) yields stronger guarantees. For
σ ∈ {0.3, 0.4, 0.5}, the localized smoothing certificate outperforms the baseline for all .
41
Under review as a conference paper at ICLR 2022
1.0
1.0
8 6 4 2
♦ ♦ ♦ ♦
Oooo
1ej pəujəo
Adversarial budget e
0.0
0.0	0.5
1.0	1.5	2.0
(a) σ = 0.1, σmin = 0.08, σmax = 0.15
8 6 4 2
♦ ♦ ♦ ♦
Oooo
lEJ pəhijəo
Adversarial budget e
0.0
0.0	0.5	1.0
1.5	2.0
(c)σ = 0.3, σmin = 0.2, σmax =0.6
1.0
1.0
8 6 4 2
♦ ♦ ♦ ♦
Oooo
∙1ej pəujəo
Adversarial budget e
0.0
0.0	0.5	1.0
1.5	2.0
0.0
0.0
(b)σ= 0.2, σmin = 0.12, σmax =0.4
8 6 4 2
♦ ♦ ♦ ♦
Oooo
∙lEJ pəp"əo
Adversarial budget e
0.5	1.0	1.5	2.0
(d)σ= 0.4, σmin = 0.25, σmax = 1.5
0 8 6 4 2
-----
Ioooo
∙J pəhijəo
0.0
0.0	0.5	1.0
1.5	2.0
Adversarial budget e
(e) σ = 0.5, σmin = 0.25, σmax = 1.5
Figure 7:	Certified ratios of U-Net models under varying adversarial budgets . We compare the
center smoothing baseline (with standard deviation σ) to localized smoothing (with parameters
σmin , σmax such that the locally smoothed model has a higher or equal accuracy). For = 0,
center smoothing has higher certified ratios, i.e. it abstains at a lower rate. For σ = 0.2, the center
smoothing certified accuracy curve has a higher AUC than both the naive combination of localized
smoothing certificates (dotted line) and the proposed collective certificate (solid line). But for other
σ localized smoothing outperforms center smoothing. The gap widens with increasing σ.
42
Under review as a conference paper at ICLR 2022
AoEjn8E P°≡1J°□
O
.
O
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnOOE p-jəo
O
.
O
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnOOE p-jəo
O
.
2
Adversarial budget e
Adversarial budget e
O
.
O
2 O
♦ ♦
O O
(a) σ = 0.1, σmin = 0.08, σmax = 0.15
0 8 6 4
♦ ♦ ♦ ♦
Iooo
AOEJnSE p-jəo
O
.
O
8 6 4 2 0
-----
Ooooo
(b)σ= 0.2, σmin = 0.12, σmax =0.4
Adversarial budget e
Adversarial budget e
(c)σ = 0.3, σmin = 0.2, σmax =0.6
(d)σ= 0.4, σmin = 0.25, σmax = 1.5
O
.
O
8 6 4 2 0
-----
Ooooo
5
.
O
5
.
O
O
.
2
5
.
O
O
.
2
5
.
O
O
.
2
AOEJnSE p0se°□
O
♦
1
5
.
O
O
.
2
Adversarial budget e
(e) σ = 0.5, σmin = 0.25, σmax = 1.5
Figure 8:	Certified accuracies of U-Net models under varying adversarial budgets . We compare
the center smoothing baseline (with standard deviation σ) to localized smoothing (with parameters
σmin , σmax such that the locally smoothed model has a higher or equal accuracy). Combining the
localized smoothing base certificates using the proposed linear program (solid orange line) instead
of evaluating them independently (dotted orange line) yields stronger guarantees. The gap between
center smoothing and localized smoothing widens with increasing σ.
43
Under review as a conference paper at ICLR 2022
E.3 Node classification
We first discuss the experimental setup and the necessary parameters for our training and certification
procedure before we show additional experimental results in Section E.3.3.
E.3.1 Experimental Setup and Hyperparameters
Metric In the node classification setting we are generally only concerned with certified accuracy. To
calculate this accuracy, we only consider nodes that are correctly classified, where the method does
not abstain, and is certifiably robust for the given perturbation.
Model We test two different models: 2-layer APPNP (Klicpera et al., 2019) and 6-layer GCN (Kipf
& Welling, 2017). For both models we use a hidden size of 64 and dropout with a probability of 0.5.
Furthermore, for the propagation step of APPNP we use 10 for the number of interactions and 0.15
as value for α.
Data and preprocessing. We evaluate our approach on the Cora-ML node classification dataset. We
perform standard preprocessing, i.e., remove self-loops, make the graph undirected and select the
largest connected component. For the localized smoothing approach we perform Metis clustering
(Karypis & Kumar, 1998) to partition the graph into 5 clusters. We create an affinity ranking by
counting the number of edges which are connecting cluster i and j . This ranking is used to select
the noise parameter for smoothing the attributes of cluster j while classifying a node of cluster i.
Training and data augmentation All models are trained with a learning rate of 0.001 and weight
decay of 0.001. The models we use for sparse smoothing are trained with the noise distribution that
is also reported for certification. The localized smoothing models are trained on the their minimal
noise level, i.e., not with localized noise but with only θm+in and θm-in.
Certification In the node classification setting the noise parameter space is large. Instead of only
one noise parameter as used in the image segmentation scenario, we can vary flip probabilities for
addition and deletion. Apart from that we need lower and upper values for each of those. Therefore,
we just selected the minimum parameters and focused on optimising the baseline around these noise
levels as described in Section 8.2. The specific noise parameters and baseline search space regions
can be seen in the captions of the respective figures. The collective linear program is solved us-
ing MOSEK (version 9.2.46) (MOSEK ApS, 2019) through the CVXPY interface (version 1.1.13)
(Diamond & Boyd, 2016).
E.3.2 Hardware
All experiments on image segmentation were performed using an AMD EPYC 7543 CPU @
2.80GHz, an NVIDA A100 GPU and 32 GB of RAM.
E.3.3 Additional Experimental Evaluation
In Fig. 9, we can see a comparison of our localized smoothing approach to sparse smoothing for a
GCN model with 6-layers. Here, the smoothing distribution is the same as the one used in Fig. 3
of Section 8.2. In Fig. 10, we can see a comparison for an APPNP model but with smaller minimal
noise levels.
We can see that in all experiments for deletion, the localized smoothing approach outperforms the
baselines. In the deletion setting we increase the certified accuracy curve’s AUC from an average
12.28 to 18.93 (i.e. by 54%). In the addition scenario the AUC, averaged over all experiments,
decreases from 6.85 to 6.08 (i.e. by 11.25%). This is mainly due to the results of the experiment
that can be seen in Fig. 10. In the baseline evaluation process both noise parameters for sparse
smoothing are significantly larger than our min-values. This shows that our values may be too small
in this scenario.
Fig. 11 compares the variance certificate to the sparse smoothing certificate of Bojchevski et al.
(2020). That is, we use only one cluster and the same noise levels and models for our approach and
the baseline, i.e., θ+ = 0.01 and θ- = 0.6. We observe that for both models the variance certificate
yields better results for deletion. However, in the addition setting, it is outperformed by the baseline.
If we look at Theorem 2, we can see that we multiply the adversarial budget for addition with γ+
44
Under review as a conference paper at ICLR 2022
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
4()
3()
20
10
50
50
4()
3()
20
10
O
Number of adversarial additions	Number of adversarial deletions
Figure 9: Certified accuracy for varying number of attribute additions (left) and deletions (right)
for the GCN model. We compare localized smoothing (θm+in = 0.075, θm-in = 0.6, θm+ax = 0.15,
θm-ax = 0.95) with sparse smoothing with (θ+ = 0.085, θ- = 0.609) for addition and deletion.
This configuration yields the largest certified accuracy curve AUC of 10.41 for addition and 14.78
for deletion compared to all combinations θ+ ∈ {0.007, 0.0085, 0.01} and θ- ∈ [0.1, . . . , 0.827].
In the deletion scenario we outperform the baseline with an AUC of 21.76 (15.76 non-collective).
For addition the sparse smoothing performs slightly better than our certificate with AUC of 9.19
(6.39 non-collective). However, the our approach performs significantly better in small perturbation
regions and also in clean accuracy (0 perturbation).
and the one for deletion with γ-. As γ+ has θ+ in the denominator the small noise level for addition
results in larger values for γ+. For these parameters we have γ+ = 2.795 and γ- = 0.491 which
shows why the certificate is less robust to adversarial additions. This is consistent with the other
experiments where our results are on par or worse than the baseline for additions but we outperform
it for deletions.
The Monte Carlo sampling dominated the runtime. Averaged over all experiments, it took 1034 s
(both for the baseline and the proposed certificate, as we used the same number of samples for
both). Averaged over all experiments, the per-prediction certificates took 0.11 s for the baseline
and 0.66 s for the proposed certificate (note that the baseline uses the sparsity-aware smoothing cer-
tificate of Bojchevski et al. (2020), while the proposed collective certificate uses our novel variance
smoothing certificate (see Theorem 2), which requires estimating both the mean and variance of soft-
max scores). Averaged over all tested adversarial budgets, solving each collective linear program
only took 10.9 s, i.e. less than the Monte Carlo sampling that is necessary for both the baseline and
our method. The reason that the linear programs for graphs required more time than those for image,
even though they involve fewer constraints and variables, is that a different, not as well-vectorized
implementation was used.
45
Under review as a conference paper at ICLR 2022
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
20
10
4()
3()
50
O
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
50
40
3()
∙2()
10
Number of adversarial additions
Number of adversarial deletions
Figure 10: Certified accuracy for varying number of attribute additions (left) and deletions (right) for
the APPNP model. We compare localized smoothing (θm+in = 0.0075, θm-in = 0.65, θm+ax = 0.08,
θm-ax = 0.95) with sparse smoothing with (θ+ = 0.0085, θ- = 0.827) for addition and (θ+ =
0.007, θ- = 0.755) for deletion. These configurations yield the largest certified accuracy curve AUC
of 5.62 for addition and 14.29 for deletion compared to all combinations θ+ ∈ {0.007, 0.0085, 0.01}
and θ- ∈ [0.1, . . . , 0.827]. In the deletion scenario we outperform the baseline with an AUC of
18.76 (14.86 non-collective). However, the sparse smoothing performs better for addition than our
certificate which only yields a AUC of 3.39 (2.54 non-collective). We observe that the optimal
parameters for addition are both significantly larger than our minimal ones.
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
Var. cert., del.
Sparse, del.
Var. cert., add.
Sparse, add.
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
AOEJnSE p-jəo
-2()
10
40
10
40
3()
Number of adversarial perturbations
Number of adversarial perturbations
Figure 11: Comparison of the variance certificate with sparse smoothing. In this approach we only
used one cluster, i.e., use the same noise levels. Left we can see the results for an APPNP model and
on the right for a 6-layer GCN. The models are trained with the same noise parameters θ+ = 0.01
and θ- = 0.6. We observe that in for both models the variance certificate yields better results for
deletion. However, in the addition setting, it is outperformed by the baseline.
46