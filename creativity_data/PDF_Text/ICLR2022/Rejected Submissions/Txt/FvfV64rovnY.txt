Under review as a conference paper at ICLR 2022
Explaining Scaling Laws of
Neural Network Generalization
Anonymous authors
Paper under double-blind review
Ab stract
The test loss of well-trained neural networks often follows precise power-law
scaling relations with either the size of the training dataset or the number of
parameters in the network. We propose a theory that explains and connects these
scaling laws. We identify variance-limited and resolution-limited scaling behavior
for both dataset and model size, for a total of four scaling regimes. The variance-
limited scaling follows simply from the existence of a well-behaved infinite data
or infinite width limit, while the resolution-limited regime can be explained by
positing that models are effectively resolving a smooth data manifold. In the
large width limit, this can be equivalently obtained from the spectrum of certain
kernels, and we present evidence that large width and large dataset resolution-
limited scaling exponents are related by a duality. We exhibit all four scaling
regimes in the controlled setting of large random feature and pretrained models and
test the predictions empirically on a range of standard architectures and datasets.
We also observe several empirical relationships between datasets and scaling
exponents: super-classing image tasks does not change exponents, while changing
input distribution (via changing datasets or adding noise) has a strong effect. We
further explore the effect of architecture aspect ratio on scaling exponents.
1	S caling Laws for Neural Networks
For a large variety of models and datasets, neural network performance has been empirically observed
to scale as a power-law with model size and dataset size (Hestness et al., 2017; Kaplan et al., 2020;
Rosenfeld et al., 2020b; Henighan et al., 2020). These exponents determine how quickly performance
improves with more data and larger models. We would like to understand why these power-laws
emerge, and what features of the data and models determine the values of the power law exponents.
In this work, we present a theoretical framework for understanding scaling laws in trained neural
networks. We identify four related scaling regimes with respect to the number of model parameters
P and the dataset size D. With respect to each of D, P , there is both a variance-limited regime and a
resolution-limited regime.
Variance-Limited Regime In the limit of infinite data or an arbitrarily wide model, some aspects
of neural network training simplify. Specifically, if we fix one of D, P and study scaling with respect
to the other parameter as it becomes arbitrarily large, then the difference between the finite test loss
and its limiting value scales as 1/x, i.e. as a power-law with exponent 1, with X = D or √P H width
in deep networks and x = D orP in linear models.
Resolution-Limited Regime In this regime, one of D or P is effectively infinite, and we study
scaling as the other parameter increases. In this case, a variety of works have empirically observed
power-law scalings 1∕xα, typically with 0 < α < 1 for both X = P or D. We derive exponents in
this regime precisely in the setting of random feature models (c.f. next section). Empirically, we find
that our theoretical predictions for exponents hold in pretrained, fine-tuned models even though these
lie outside our theoretical setting.
1
Under review as a conference paper at ICLR 2022
Variance-limited : Theory ao = 1
0 2 4 6
O - - -
Iooo
111
so-sso
αo: 1.01
aD： 0.98 (CNN)
ao: 1.10
aD: 1.01
ao: 1.11 (SGD)
10°
lθ-ɪ
Resolution-limited
ιb3
0.0	0.2	0.4	0.6	0.8	1.0
interpolating Between Training Points in ^dimensions
Dataset size (D)
Resolution-limited
IO2
Width
Dataset size (D)
Variance-limited : Theory aw = 1
(8-So-' ss。一
aw: 1.02 (ERF)
aw-1-03 (ERF)
Width
2 4 6 8 10 12 14 16 18 20 22 24 26
Dimension
(b)
(a)
• Teacher-Student ♦ CiFAR-IO ・ CIFAR-100 ∙ SVHN ・ FashlonMNlST ∙ MNlST
Figure 1: (a) Four scaling regimes Here we exhibit the four regimes we focus on in this work. (top-left,
bottom-right) Variance-limited scaling of under-parameterized models with dataset size and over-parameterized
models with number of parameters (width) exhibit universal scaling (αD = αW = 1) independent of the
architecture or underlying dataset. (top-right, bottom-left) Resolution-limited over-parameterized models with
dataset or under-parameterized models with model size exhibit scaling with exponents that depend on the details
of the data distribution. These four regimes are also found in random feature (Figure 2a) and pretrained models
(see supplement). (b) Resolution-limited models interpolate the data manifold Linear interpolation between
two training points in a four-dimensional input space (top). We show a teacher model and four student models,
each trained on different sized datasets. In all cases teacher and student approximately agree on the training
endpoints, but as the training set size increases they increasingly match everywhere. (bottom) We show 4∕α0
versus the data manifold dimension (input dimension for teacher-student models, intrinsic dimension for standard
datasets). We find that the teacher-student models follow the 4∕α0 (dark dashed line), while the relationship for
a four layer CNN (solid) and WRN (hollow) on standard datasets is less clear.
For more general nonlinear models, we propose a refinement of naive bounds into estimates via
expansions that hold asymptotically. These rely on the idea that additional data (in the infinite
model-size limit) or added model parameters (in the infinite data limit) are used by the model to
carve up the data manifold into smaller components. For smooth manifolds, loss, and network, the
test loss will depend on the linear size of a sub-region, while it is the d-dimensional sub-region
volume that scales inversely with P or D, giving rise to a H 1/d.1 To test this empirically, we make
measurements of the resolution-limited exponents in neural networks and intrinsic dimension of the
data manifold, shown in Figure 1b.
Explicit Derivation We derive the scaling laws for these four regimes explicitly in the setting of
random feature teacher-student models, which also applies to neural networks in the large width limit.
This setting allows us to solve for the test error directly in terms of the feature covariance (kernel).
The scaling of the test loss then follows from the asymptotic decay of the spectrum of the covariance
matrix. For generic continuous kernels on a d-dimensional manifold, we can further relate this to the
dimension of the data manifold.
Summary of Contributions:
1.	We propose four scaling regimes for neural networks. The variance-limited and resolution-
limited regimes originate from different mechanisms, which we identify. To our knowledge,
1A visualization of this successively better approximation with dataset size is shown in Figure 1b for models
trained to predict data generated by a random fully-connected network.
2
Under review as a conference paper at ICLR 2022
this categorization has not been previously exhibited. We provide empirical support for all four
regimes in deep networks on standard datasets.
2.	We derive the variance-limited regime under simple yet general assumptions (Theorem 1).
3.	We present a hypothesis for resolution-limited scaling through refinement of naive bounds (Theo-
rems 2, 3), for general nonlinear models. We empirically test the dependence of the estimates on
intrinsic dimension of the data manifold for deep networks on standard datasets (Figure 1b).
4.	In the setting of random feature teacher-student networks, we derive both variance-limited and
resolution-limited scaling exponents exactly. In the latter case, we relate this to the spectral decay
of kernels. We identify a novel duality that exists between model and dataset size scaling.
5.	We empirically investigate predictions from the random features setting in pretrained, fine-tuned
models on standard datasets and find they give excellent agreement.
6.	We study the dependence of the scaling exponent on changes in architecture and data, finding that
(i) changing the input distribution via switching datasets and (ii) the addition of noise have strong
effects on the exponent, while (iii) changing the target task via superclassing does not.
Related Works: There have been a number of recent works demonstrating empirical scaling laws
(Hestness et al., 2017; Kaplan et al., 2020; Rosenfeld et al., 2020b; Henighan et al., 2020; Rosenfeld
et al., 2020a) in deep neural networks, including scaling laws with model size, dataset size, compute,
and other observables such as mutual information and pruning. Some precursors (Ahmad & Tesauro,
1989; Cohn & Tesauro, 1991) can be found in earlier literature. Recently, scaling laws have also
played a significant role in motivating work on the largest models that have yet been developed
(Brown et al., 2020; Fedus et al., 2021).
There has been comparatively little work on theoretical ideas (Sharma & Kaplan, 2020; Bisla et al.,
2021) that match and explain empirical findings in generic deep neural networks. In the particular
case of large width, deep neural networks behave as random feature models (Neal, 1994; Lee et al.,
2018; Matthews et al., 2018; Jacot et al., 2018; Lee et al., 2019; Dyer & Gur-Ari, 2020), and known
results on the loss scaling of kernel methods can be applied (Spigler et al., 2020; Bordelon et al.,
2020). Though not in the original, Bordelon et al. (2020) analyze resolution-limited dataset size
scaling for power-law spectra in later versions.
During the completion of this work, Hutter (2021) presented a specific solvable model of learning
exhibiting non-trivial power-law scaling for power-law (Zipf) distributed features. This does not
directly relate to the setups studied in this work, or present bounds that supersede our results.
Concurrent to our work, Bisla et al. (2021) presented a derivation of the resolution-limited scaling
with dataset size, also stemming from nearest neighbor distance scaling on data manifolds. However,
they do not discuss requirements on model versus dataset size or how this scaling behavior fits into
other asymptotic scaling regimes.
In the variance-limited regime, scaling laws in the context of random feature models (Rahimi &
Recht, 2008; Hastie et al., 2019; d’Ascoli et al., 2020), deep linear models (Advani & Saxe, 2017;
Advani et al., 2020), one-hidden-layer networks (Mei & Montanari, 2019; Adlam & Pennington,
2020a;b), and wide neural networks treated as Gaussian processes or trained in the NTK regime
(Lee et al., 2019; Dyer & Gur-Ari, 2020; Andreassen & Dyer, 2020; Geiger et al., 2020) have been
studied. In particular, this behavior was used in (Kaplan et al., 2020) to motivate a particular ansatz
for simultaneous scaling with data and model size. The resolution-limited analysis can perhaps be
viewed as an attempt to quantify the ideal-world generalization error of Nakkiran et al. (2021).
This work makes use of classic results connecting the spectrum of a smooth kernel to the geometry
it is defined over (Weyl,1912; Reade,1983; Kuhn, 1987; Ferreira & Menegatto, 2009) and on the
scaling of iteratively refined approximations to smooth manifolds (Stein, 1999; Bickel et al., 2007;
de Laat, 2011).
3
Under review as a conference paper at ICLR 2022
2	Four Scaling Regimes
Throughout this work we will be interested in how the average test loss L(D, P) depends on the
dataset size D and the number of model parameters P . Unless otherwise noted, L denotes the test
loss averaged over initialization of the parameters and draws of a size D training Set Some of our
results only pertain directly to the scaling with width W H √P, but we expect many of the intuitions
apply more generally. We use the notation αD, αP , and αW to indicate scaling exponents with
respect to dataset size, parameter count, and width. All proofs appear in the supplement.
2.1	Variance-Limited Exponents
In the limit of large D the outputs of an appropriately trained network approach a limiting form with
corrections which scale as D-1. Similarly, recent work shows that wide networks have a smooth
large P limit (Jacot et al., 2018), where fluctuations scale as 1∕√P. If the loss is sufficiently smooth
then its value will approach the asymptotic loss with corrections proportional to the variance, (1/D or
1 /√P). In Theorem 1 We present sufficient conditions on the loss to ensure this variance dominated
scaling. We note, these conditions are satisfied by mean squared error and cross entropy loss, though
we conjecture the result holds even more generally.
Theorem 1. Let `(f) be the test loss as a function of network output, (L = E [`(f)]), and let
fT be the network output after T training steps, thought of as a random variable over weight
initialization, draws of the training dataset, and optimization seed. Further let fT be concentrating
with E[(fT - E[fT])k] = O () ∀k ≥ 2. If ` is a finite degree polynomial, or has bounded second
derivative, or is 2-Holder, then E ['(fτ)] 一 ' (E [fτ])=O(E).
Dataset scaling Consider a neural network, and its associated training loss Ltrain(θ). For every
value of the weights, the training loss, thought of as a random variable over draws of a training set
of size D, concentrates around the population loss, with a variance which scales as O D-1 . If
the optimization procedure is sufficiently smooth, the trained weights, network output, and higher
moments, will approach their infinite D values,
ED (fT 一 ED [fT])k
O D-1
Here, the
subscript D on the expectation indicates an average over draws of the training set. This scaling
together with Theorem 1 gives the variance limited scaling of loss with dataset size.
This concentration result with respect to dataset size has appeared for linear models in Rahimi
& Recht (2008) and for single hidden layer networks with high-dimensional input data in Mei &
Montanari (2019); Adlam & Pennington (2020a;b). In the supplement we prove this for GD and SGD
with polynomial loss as well as present informal arguments more generally. Additionally, we present
examples violating the smoothness assumption and exhibiting different scaling.
Large Width Scaling We can make a very similar argument in the w → ∞ limit. It has been
shown that the predictions from an infinitely wide network, either under Bayesian inference (Neal,
1994; Lee et al., 2018), or when trained via gradient descent (Jacot et al., 2018; Lee et al., 2019)
approach a limiting distribution at large width equivalent to a linear model. Furthermore, corrections
to the infinite width behavior are controlled by the variance of the full model around the linear model
predictions. This variance (and higher moments) have been shown to scale as 1/w (Dyer & Gur-Ari,
2020; Yaida, 2020; Andreassen & Dyer, 2020), Ew [(fr - Ew [fτ])k] = O (w-1). Theorem 1 then
implies the loss will differ from its w = ∞ limit by a term proportional to 1/w.
We note that there has also been work studying the combined large depth and large width limit, where
Hanin & Nica (2020) found a well-defined infinite size limit with controlled fluctuations. In any such
context where the model predictions concentrate, we expect the loss to scale with the variance of
the model output. In the case of linear models, studied below, the variance is O(PT) rather than
O(√P), and we see the associated variance scaling in this case.
4
Under review as a conference paper at ICLR 2022
2.2	Resolution-Limited Exponents
In this section we consider training and test data drawn uniformly from a compact d-dimensional
manifold, x ∈ Md, and targets given by some smooth function y = F(x) on this manifold.
Over-Parameterized Dataset Scaling
Consider the double limit of an over-parameterized model with large training set size, P D 1.
We further consider well-trained models, i.e. models that interpolate all training data. The goal is to
understand L(D). If we assume that the learned model f is sufficiently smooth, then the dependence
of the loss on D can be bounded in terms of the dimension of the data manifold Md .
Informally, if our train and test data are drawn i.i.d. from the same manifold, then the distance from a
test point to the closest training data point decreases as we add more and more training data points. In
particular, this distance scales as O(D-1/d) (Levina & Bickel, 2005). Furthermore, if f, F are both
sufficiently smooth, they cannot differ too much over this distance. If in addition the loss function, L,
is a smooth function vanishing when f = F, we have L = O(D-1/d). This is summarized in the
following theorem.
Theorem 2.	Let L(f), f and F be Lipschitz with constants KL, Kf, and KF. Further let D
be a training dataset of size D sampled i.i.d from Md and let f (x) = F (x), ∀x ∈ D then
L(D) = O (KLmax(Kf,KF`)DT∕d∖.
Under-Parameterized Parameter Scaling
We will again assume that F varies smoothly on an underlying compact d-dimensional manifold
Md. We can obtain a bound on L(P) if we imagine that f approximates F as a piecewise function
with roughly P regions (see Sharma & Kaplan (2020)). Here, we instead make use of the argument
from the over-parameterized, resolution-limited regime above. If we construct a sufficiently smooth
estimator for F by interpolating among P randomly chosen points from the (arbitrarily large) training
set, then by the argument above the loss will be bounded by O(P -1/d).
Theorem 3.	Let L(f), f and F be Lipschitz with constants KL, Kf, and KF. Further let f(x) =
F(x) for P points sampled i.i.d from Md then L(P) = O KLmax(Kf, KF)P -1/d .
From Bounds to Estimates
Theorems 2 and 3 are phrased as bounds, but we expect the stronger statement that these bounds also
generically serve as estimates, so that eg L(D) = Ω(D-c") for C ≥ 2, and similarly for parameter
scaling. If we assume that F and f are analytic functions on Md and that the loss function L(f, F) is
analytic in f - F and minimized at f = F, then the loss at a given test input, xtest, can be expanded
around the nearest training point, Xtrain, L(XteSt) = P∞=n≥2 am(Xtrain)(xtest — Xtrain)m,2 where the
first term is of finite order n ≥ 2 because the loss vanishes at the training point. As the typical
distance between nearest neighbor points scales as D-1/d on a d-dimensional manifold, the loss
will be dominated by the leading term, L a D-n/d, at large D. Note that if the model provides an
accurate piecewise linear approximation, we will generically find n ≥ 4.
2.3 Explicit realization in Linear models
In the proceeding sections we have conjectured typical case scaling relations for a model’s test loss.
We have further given intuitive arguments for this behavior which relied on smoothness assumptions
on the loss and training procedure. In this section, we provide a concrete realization of all four scaling
regimes within the context of linear models. Of particular interest is the resolution-limited regime,
where the scaling of the loss is a consequence of the linear model kernel spectrum - the scaling of
over-parameterized models with dataset size and under-parameterized models with parameters is a
consequence of a classic result, originally due to Weyl (1912), bounding the spectrum of sufficiently
smooth kernel functions by the dimension of the manifold they act on.
2For simplicity we have used a very compressed notation for multi-tensor contractions in higher order terms.
5
Under review as a conference paper at ICLR 2022
Variance-limited
0 12 3
O - - -
Iooo
111
(8)660-1 '耨0~∣
Dataset size (D)
Resolution-limited
ιo-1
io-2
Io-3
io-4
ιo-5
ιo-6
IO-7
Dataset size (D)
Fit exponents
IO1 L
IO0 k
10-1
招 IO-2
Q r
-j 10^3
10-4
IO-5
10-6
Resolution-limited
ap： 0.34
ap: 0.44
aP: 0.52
apt 0.63
ap： 0.70
ap： 0.79	∙
ap: 0.92
ffp： 1.31
Parameter count (P)
IO-2
10-3
⑻
Variance-limited
lol
O O -
ɪ ɪ
)SSO1 I
io3 ............IO4
Parameter count (P)
ttp: 1.14
αp: 1.14
ap： 1.15
ap∙. 1.14
即：1.15
ap∙ 1.15
ap∙ 1.14
ap: 1.15
0.4	0.6	0.8	1.0	1.2
ɑo
(b)

Figure 2: (a) Random feature models exhibit all four scaling regimes Here we consider linear teacher-
student models with random features trained with MSE loss to convergence. We see both variance-limited
scaling (top-left, bottom-right) and resolution-limited scaling (top-right, bottom-left). Data is varied by
downsampling MNIST by the specified pool size. (b) Duality and spectra in random feature models Here
we show the relation between the decay of the kernel spectra, αK, and the scaling of the loss with number of
data points, αD, and with number of parameters, αP (top) The spectra of random FC kernels on pooled MNIST
(bottom) appear well described by a power law decay. The theoretical relation αD = αP = αK is given by the
black dashed line.
Linear predictors serve as a model system for learning. Such models are used frequently in practice
when more expressive models are unnecessary or infeasible (McCullagh & Nelder, 1989; Rifkin &
Lippert, 2007; Hastie et al., 2009) and also serve as an instructive test bed to study training dynam-
ics (Advani et al., 2020; Goh, 2017; Hastie et al., 2019; Nakkiran, 2019; Grosse, 2021). Furthermore,
in the large width limit, randomly initialized neural networks become Gaussian Processes (Neal,
1994; Lee et al., 2018; Matthews et al., 2018; Novak et al., 2019; Garriga-Alonso et al., 2019; Yang,
2019), and in the low-learning rate regime (Lee et al., 2019; Lewkowycz et al., 2020; Huang et al.,
2020) neural networks train as linear models at infinite width (Jacot et al., 2018; Lee et al., 2019;
Chizat et al., 2019).
Here we discuss linear models in general terms, though the results immediately hold for the special
cases of wide neural networks. In this section we focus on teacher-student models with weights
initialized to zero and trained with mean squared error (MSE) loss to their global optimum.
We consider a linear teacher, F, and student f, F (x) = PM=1 ωM FM (x), f (x) = Pj=1 θμ fμ (x).
Here {FM } are a (potentially infinite) pool of features and the teacher weights, ωM are taken to be
normal distributed, ω 〜N(0,1/S). The student model is built out of a subset of the teacher features.
To vary the number of parameters in this simple model, We construct P features, fμ=1,…,p, by
introducing a projector P onto a P-dimensional subspace of the teacher features, fμ = EM PμMFM.
We train by sampling a training set of size D and minimizing the MSE loss, Ltrain =
2D PD=I (f (Xa) - F(Xa))2. We are interested in the test loss averaged over draws of our teacher
and training dataset. The infinite data test loss, L(P) := limD→∞ L(D, P), takes the form.
L(P) = ɪTr hc-CPT(PCPT)-1 PC] .	(1)
2S
Here we have introduced the feature-feature second moment-matrix, C = Ex F(X)FT(X) .
If the teacher and student features had the same span, this would vanish, but due to the mismatch
the loss is non-zero. On the other hand, if we keep a finite number of training points, but allow the
6
Under review as a conference paper at ICLR 2022
student to use all of the teacher features, the test loss, L(D) := limP →S L(D, P), takes the form,
L(D) = IExhK(X,x) -K(X)KTK(x)i .	(2)
Here, K(x, x0) is the data-data second moment matrix, K~ indicates restricting one argument to the D
training points, while K indicates restricting both. This test loss vanishes as the number of training
points becomes infinite but is non-zero for finite training size.
We present a full derivation of these expressions in the supplement. In the remainder of this section,
we explore the scaling of the test loss with dataset and model size.
2.3.1	Variance-Limited exponents
To derive the limiting expressions (1) and (2) for the loss one makes use of the fact that the sample
expectation of the second moment matrix over the finite dataset, and finite feature set is close to the
full covariance, D PaD=I F(Xa)FT(Xa) = C + δC, P f T(χ)f (x0), = K + δK, with the fluctuations
satisfying ED δC2 = O(D-1) and EP δK2 = O(P -1), where expectations are taken over
draws of a dataset of size D and over feature sets. Using these expansions yields the variance-limited
scaling, L(D, P) - L(P) = O(D-1), L(D, P) - L(D) = O(P -1) in the under-parameterized and
over-parameterized settings respectively.
In Figure 2a we see evidence of these scaling relations for features built from randomly initialized
ReLU networks on pooled MNIST independent of the pool size. In the supplement we provide an
in-depth derivation of this behavior and expressions for the leading contributions to L(D, P) - L(P )
and L(D, P) - L(D).
2.3.2	Resolution-Limited exponents
We now would like to analyze the scaling behavior of our linear model in the resolution-limited
regimes, that is the scaling with P when 1 P D and the scaling with D when 1 D P .
In these cases, the scaling is controlled by the shared spectrum of C or K. This spectrum is often
well described by a power-law, where eigenvalues λi satisfy λi = 尹匕.See Figure 2b for example
spectra on pooled MNIST. In this case, we will argue that the losses also obey a power law scaling,
with the exponents controlled by the spectral decay factor, 1 + αK .
L(D) H D-αK , L(P) H P-aK .	(3)
In other words, in this setting, αP = αD = αK . This is supported empirically in Figure 2b. We
then argue that when the kernel function, K is sufficiently smooth on a manifold of dimension d,
αK H d-1, thus realizing the more general resolution-limited picture described above.
From spectra to scaling laws for the loss To be concrete let us focus on the over-parameterized
loss. If we introduce the notation ei for the eigenvectors of C and & for the eignvectors of
-D PaD=I F(Xa)FT(xa), the loss becomes,
SD
L(D) = G X "(I— χ(k ej )2).	(4)
Before discussing the general asymptotic behavior of (4), we can gain some intuition by considering
the case of large αK. In this case, eej ≈ ej (see e.g. Loukas (2017)), we can simplify (4) to,
∞
L(D) H X iι+αK = ακ DfK + θ(D-αK-1).	⑸
D+1
More generally in the supplement, following Bordelon et al. (2020); Canatar et al. (2021), we use
replica theory methods to derive, L(D) H D-αK and L(P) H P-αK, without requiring the large
αK limit.
7
Under review as a conference paper at ICLR 2022
50
20
10
5
NdaSS
π100
3
Figure 3: Effect of data distribution on scaling exponents For CIFAR-100 superclassed to N classes (left),
we find that the number of target classes does not have a visible effect on the scaling exponent. (right) For
CIFAR-10 with the addition of Gaussian noise to inputs, we find the strength of the noise has a strong effect on
performance scaling with dataset size. All models are WRN-28-10.
Data Manifolds and Kernels In Section 2.2, we discussed a simple argument that resolution-
limited exponents ɑ a 1/d, where d is the dimension of the data manifold. Our goal now is to
explain how this connects with the linearized models and kernels discussed above: how does the
spectrum of eigenvalues of a kernel relate to the dimension of the data manifold?
The key point is that sufficiently smooth kernels must have an eigenvalue spectrum with a bounded
tail. Specifically, a Ct kernel on a d-dimensional space must have eigenvalues λn . 二工加(Kuhn,
1987). In the generic case where the covariance matrices we have discussed can be interpreted as
kernels on a manifold, and they have spectra saturating the bound, linearized models will inherit
scaling exponents given by the dimension of the manifold.
As a simple example, consider a d-torus. In this case we can study the Fourier series de-
composition, and examine the case of a kernel K(x - y). This must take the form K =
PnI [anι sin(nɪ ∙ (x — y)) + b= cos(n∕ ∙ (x - y))], where nɪ = (nι,…，n) are integer indices,
and anI , bnI are the overall Fourier coefficients. To guarantee that K is a Ct function, we must have
anι, bnɪ . nd+t where nd = N indexes the number of an in decreasing order. But this means that
in this simple case, the tail eigenvalues of the kernel must be bounded by N，、加 as N → ∞.
2.4 Duality
We argued above that for kernels with pure power law spectra, the asymptotic scaling of the under-
parameterized loss with respect to model size and the over-parameterized loss with respect to
dataset size share a common exponent. In the linear setup at hand, the relation between the under-
parameterized parameter dependence and over-parameterized dataset dependence is even stronger.
The under-parameterized and over-parameterized losses are directly related by exchanging the
projection onto random features with the projection onto random training points. Note, sample-wise
double descent observed in Nakkiran (2019) is a concrete realization of this duality for a simple data
distribution. In the supplement, we present examples exhibiting the duality of the loss dependence on
model and dataset size outside of the asymptotic regime.
3	Experiments
3.1	Deep teacher-student models
Our theory can be tested very directly in the teacher-student framework, in which a teacher deep
neural network generates synthetic data used to train a student network. Here, it is possible to generate
unlimited training samples and, crucially, controllably tune the dimension of the data manifold. We
8
Under review as a conference paper at ICLR 2022
accomplish the latter by scanning over the dimension of the inputs to the teacher. We have found that
when scanning over both model size and dataset size, the interpolation exponents closely match the
prediction of 4/d. The dataset size scaling is shown in Figure 1, while model size scaling experiments
appear in the supplement and have previously been observed in Sharma & Kaplan (2020).
3.2	Variance-limited scaling in the wild
Variance-limited scaling, (Section 2.1), can be universally observed in real datasets. Figure 1a
(top-left, bottom-right) measures the variance-limited dataset scaling exponent αD and width scaling
exponent αW . In both cases, we find striking agreement with the theoretically predicted values
αD, αW = 1 across a variety of dataset, network architecture, stochastic batch size and loss type.
Our testbed includes deep fully-connected and convolutional networks with Relu or Erf nonlinearities
and MSE or softmax-cross-entropy losses. The supplement contains experimental details.
3.3	Resolution-limited scaling in the wild
In addition to teacher-student models, we explored resolution-limited scaling behavior in the context
of standard classification datasets. Wide ResNet (WRN) models (Zagoruyko & Komodakis, 2016)
were trained for a fixed number of steps with cosine decay. In Figure 1b we also include data from
a four hidden layer CNN detailed in the supplement. As detailed above, we find dataset dependent
scaling behavior in this context.
We further investigated the effect of the data distribution on the resolution-limited exponent, αD , by
tuning the number of target classes and input noise (Figure 3). To probe the effect of the number of
classes, we constructed tasks derived from CIFAR-100 by grouping classes into broader semantic
categories. We found that performance depends on the number of categories, but αD is insensitive
to this number. In contrast, the addition of Gaussian noise had a more pronounced effect on αD .
This suggest a picture in which the network learns to model the input data manifold, independent of
the classification task, consistent with observations in Nakkiran & Bansal (2020); Grathwohl et al.
(2020).
We also explored the effect of aspect ratio on dataset scaling, finding that the exponent magnitude
increases with width up to a critical width, while the dependence on depth is milder (see supplement).
4	Discussion
We have presented a framework for categorizing neural scaling laws, along with derivations that
help to explain their very general origins. Crucially, our predictions agree with empirical findings in
settings which have often proven challenging for theory - deep neural networks on real datasets. The
variance-scaling regime yields, for smooth test losses, a universal prediction of αD = 1 (for D P)
and αW = 1 (for w D). The resolution-limited regime yields exponents whose numerical value is
variable and data and model dependent.
There are many intriguing directions for future work; amongst these, we highlight one in particular.
The invariance of the dataset scaling exponent to superclassing (Figure 3) suggests that deep networks
may be largely learning properties of the input data manifold - akin to unsupervised learning -
rather than significant task-specific structure, which may shed light on the versatility of learned deep
network representations for different downstream tasks. This begs to be explored further.
Limitations One limitation is that our theoretical results are asymptotic, while experiments are
performed with finite models and datasets. This is apparent in the resolution-limited regime which
requires a hierarchy (D	P or P	D). In Figures 1a and 2a top-right (bottom-left), we see
a breakdown of the predicted scaling behavior as D (P) become large and the hierarchy is lost.
Furthermore in the resolution-limited regime for deep networks, our theoretical tools rely on positing
the existence of a data manifold. A precise definition of the data manifold, however, is lacking forcing
us to use imperfect proxies, such as nearest neighbor distances of final embedding layers.
9
Under review as a conference paper at ICLR 2022
Ethics Statement Work on scaling laws provides an opportunity for discussion on how to define and
measure progress in machine learning. The values of exponents allow us to estimate expected gains
that come from increases in scale of dataset, model, and compute. Applying similar considerations
to other metrics (i.e. transfer, bias, robustness) in principle allows one to quantify whether and how
models are improving or degrading with scale and at what environmental or computational cost. On
the other hand, one may require that truly non-trivial progress in machine learning be progress that
occurs modulo scale: namely, improvements in performance across different tasks that are not simple
extrapolations of existing behavior. And perhaps the right combinations of algorithmic, model, and
dataset improvements can lead to emergent behavior at new scales. Large language models such as
GPT-3 (Fig. 1.2 in Brown et al. (2020)) have exhibited this in the context of few-shot learning. We
hope our work spurs further research in understanding and controlling neural scaling laws.
References
Ben Adlam and Jeffrey Pennington. The Neural Tangent Kernel in high dimensions: Triple descent
and a multi-scale theory of generalization. In International Conference on Machine Learning, pp.
74-84. PMLR, 2020a.
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-
variance decomposition. Advances in Neural Information Processing Systems, 33, 2020b.
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of general-
ization error in neural networks. Neural Networks, 132:428-446, 2020.
Subutai Ahmad and Gerald Tesauro. Scaling and generalization in neural networks: a case study. In
Advances in neural information processing systems, pp. 160-168, 1989.
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1370-1378, 2019.
Anders Andreassen and Ethan Dyer. Asymptotics of wide convolutional neural networks. arxiv
preprint arXiv:2008.08675, 2020.
Peter J Bickel, Bo Li, et al. Local polynomial regression on unknown manifolds. In Complex datasets
and inverse problems, pp. 177-186. Institute of Mathematical Statistics, 2007.
Devansh Bisla, Apoorva Nandini Saridena, and Anna Choromanska. A theoretical-empirical approach
to estimating sample complexity of dnns. arXiv preprint arXiv:2105.01867, 2021.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024-1034. PMLR, 2020.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and infinitely wide neural networks. Nature
communications, 12(1):1-12, 2021.
10
Under review as a conference paper at ICLR 2022
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.
Omry Cohen, Or Malka, and Zohar Ringel. Learning curves for deep neural networks: a gaussian
field theory perspective. arXiv preprint arXiv:1906.05301, 2019.
David Cohn and Gerald Tesauro. Can neural networks do better than the vapnik-chervonenkis
bounds? In Advances in Neural Information Processing Systems, pp. 911-917, 1991.
David de Laat. Approximating manifolds by meshes: asymptotic bounds in higher codimension.
Master’s Thesis, University of Groningen, Groningen, 2011.
Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=S1gFvANKDS.
StePhane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double
descent: Bias and variance (s) in the lazy regime. In International Conference on Machine
Learning, pp. 2280-2290. PMLR, 2020.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity, 2021.
JC Ferreira and VA Menegatto. Eigenvalues of integral operators defined by smooth positive definite
kernels. Integral Equations and Operator Theory, 64(1):61-81, 2009.
Adria Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional
networks as shallow gaussian processes. In International Conference on Learning Representations,
2019.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, StePhane d'Ascoli,
Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. Journal of Statistical Mechanics: Theory and Experiment,
2020(2):023401, 2020.
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka ZdebOrova. General-
isation error in learning with random features and the hidden manifold model. In International
Conference on Machine Learning, pp. 3452-3462. PMLR, 2020.
Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
http://distill.pub/2017/momentum.
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat
it like one. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=Hkxzx0NtDB.
Roger Grosse. University of Toronto CSC2541 winter 2021 neural net training dynamics, lecture
notes, 2021. URL https://www.cs.toronto.edu/-rgrosse/courses/csc2 541_
2021.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=SJgndT4KwB.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
11
Under review as a conference paper at ICLR 2022
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
http://github.com/google/flax.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,
Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam Mc-
Candlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
2020.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017.
Wei Huang, Weitao Du, Richard Yi Da Xu, and Chunrui Liu. Implicit bias of deep linear networks in
the large learning rate phase. arXiv preprint arXiv:2011.12547, 2020.
Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems, 2018.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
arXiv:1912.11370, 6(2):8, 2019.
Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2661-2671,2019.
Thomas Kuhn. Eigenvalues of integral operators with smooth positive definite kernels. Archiv der
Mathematik, 49(6):525-534, 1987.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha
Sohl-dickstein. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019.
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and
Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. Advances in
Neural Information Processing Systems, 33, 2020.
Elizaveta Levina and Peter J Bickel. Maximum likelihood estimation of intrinsic dimension. In
Advances in neural information processing systems, pp. 777-784, 2005.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
12
Under review as a conference paper at ICLR 2022
Andreas Loukas. How close are the eigenvectors of the sample and actual covariance matrices? In
International Conference on Machine Learning,pp. 2228-2237. PMLR, 2017.
Dorthe Malzahn and Manfred Opper. Learning curves for gaussian processes regression: A framework
for good approximations. Advances in neural information processing systems, pp. 273-279, 2001.
Dorthe Malzahn and Manfred Opper. A variational approach to learning curves. In T. Dietterich,
S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing Systems, vol-
ume 14, pp. 463-469. MIT Press, 2002. URL https://proceedings.neurips.cc/
paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf.
Dorthe Malzahn and Manfred Opper. Learning curves and bootstrap estimates for inference with
gaussian processes: A statistical mechanics study. Complexity, 8(4):57-63, 2003.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In International Conference on Learning
Representations, 2018.
P McCullagh and John A Nelder. Generalized Linear Models, volume 37. CRC Press, 1989.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
preprint arXiv:1912.07242, 2019.
Preetum Nakkiran and Yamini Bansal. Distributional generalization: A new kind of generalization.
arXiv preprint arXiv:2009.08092, 2020.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good offline generalizers. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=guetrIHLFGI.
Radford M. Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, Dept.
of Computer Science, 1994.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations, 2019.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural Tangents: Fast and easy infinite neural networks in python. In
International Conference on Learning Representations, 2020. URL https://github.com/
google/neural-tangents.
Giorgio Parisi. A sequence of approximated solutions to the SK model for spin glasses. Journal of
Physics A: Mathematical and General, 13(4):L115, 1980.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems, 32:
8026-8037, 2019.
Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: replacing minimization
with randomization in learning. In Nips, pp. 1313-1320. Citeseer, 2008.
JB Reade. Eigenvalues of positive definite kernels. SIAM Journal on Mathematical Analysis, 14(1):
152-157, 1983.
Ryan M Rifkin and Ross A Lippert. Notes on regularized least squares, 2007.
13
Under review as a conference paper at ICLR 2022
Sam Ritchie, Ambrose Slone, and Vinay Ramasesh. Caliban: Docker-based job manager for
reproducible workflows. Journal of Open Source Software, 5(53):2403, 2020. doi: 10.21105/joss.
02403. URL https://doi.org/10.21105/joss.02403.
Jonathan S. Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of
pruning across scales. arXiv preprint arXiv:2006.10621, 2020a.
Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. In International Conference on Learning Representations,
2020b.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017.
Vaishaal Shankar, Alex Chengyu Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt,
Jonathan Ragan-Kelley, and Benjamin Recht. Neural kernels without tangents. In International
Conference on Machine Learning, 2020.
Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
arXiv preprint arXiv:2004.10802, 2020.
Peter Sollich. Learning curves for gaussian processes. In Proceedings of the 11th International
Conference on Neural Information Processing Systems, pp. 344-350,1998.
Peter Sollich and Anason Halees. Learning curves for gaussian process regression: Approximations
and bounds. Neural computation, 14(6):1393-1428, 2002.
Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher-student paradigm. Journal of Statistical Mechanics: Theory and
Experiment, 2020(12):124001, 2020.
Michael L Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer Science &
Business Media, 1999.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.
In International Conference on Machine Learning, pp. 6105-6114. PMLR, 2019.
Matthew J Urry and Peter Sollich. Replica theory for learning curves for gaussian processes on
random graphs. Journal of Physics A: Mathematical and Theoretical, 45(42):425005, 2012.
Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgle-
ichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen,
71(4):441-479, 1912.
Christopher KI Williams and Francesco Vivarelli. Upper and lower bounds on the learning curve for
gaussian processes. Machine Learning, 40(1):77-102, 2000.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla
convolutional neural networks. In International Conference on Machine Learning, 2018.
Sho Yaida. Non-Gaussian processes and neural networks at finite widths. In Mathematical and
Scientific Machine Learning Conference, 2020.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference, 2016.
14
Under review as a conference paper at ICLR 2022
A Experimental setup
Figure 1 (top-left)
Experiments utilize relatively small models, with the number of trainable Parameteters P 〜O(1000),
trained with full-batch gradient descent (GD) and small learning rate on datasets of size D P. Each
data point in the figure represents an average over subsets of size D sampled from the full dataset.
Experiments are done using Neural Tangents (Novak et al., 2020) based on JAX (Bradbury et al.,
2018). All experiment except denoted as (CNN), use 3-layer, width-8 fully-connected networks. CNN
architecture used is Myrtle-5 network (Shankar et al., 2020) with 8 channels. Relu activation function
with critical initialization (Schoenholz et al., 2017; Lee et al., 2018; Xiao et al., 2018) was used.
Unless specified softmax-cross-entropy loss was used. We performed full-batch gradient descent
update for all dataset sizes without L2 regularization. 20 different training data sampling seeds were
averaged for each point. For fully-connected networks, input pooling of size 4 was performed for
CIFAR-10/100 dataset and pooling of size 2 was performed for MNIST and Fashion-MNIST dataset.
This was to reduce number of parameters in the input layer (# of pixels × width) which can be quite
large even for small width networks.
Figure 1 (top-right) All experiments were performed using a Flax (Heek et al., 2020) implementation
of Wide ResNet 28-10 (Zagoruyko & Komodakis, 2016), and performed using the Caliban experiment
manager (Ritchie et al., 2020). Models were trained for 78125 total steps with a cosine learning rate
decay (Loshchilov & Hutter, 2016) and an augmentation policy consisting of random flips and crops.
We report final loss, though we found no qualitative difference between using final loss, best loss,
final accuracy or best accuracy (see Figure S1).
Figure 1 (bottom-left) The setup was identical to Figure 1 (top-right) except that the model consid-
ered was a depth 10 residual network with varying width.
Figure 1 (bottom-right)
Experiments are done using Neural Tangents. All experiments use 100 training samples and two-
hidden layer fully-connected networks of varying width (ranging from w = 64 to W = 11, 585) with
Relu nonlinearities unless specified as Erf. Full-batch gradient descent and cross-entropy loss were
used unless specified as MSE, and the figure shows curves from a random assortment of training
times ranging from 100 to 500 steps (equivalently, epochs). Training was done with learning rates
small enough so as to avoid catapult dynamics (Lewkowycz et al., 2020) and no L2 regularization; in
such a setting, the infinite-width learning dynamics is known to be equivalent to that of linearized
models (Lee et al., 2019). Consequently, for each random initialization of the parameters, the test
loss of the finite-width linearized model was additionally computed in the identical training setting.
This value approximates the limiting behavior L(∞) known theoretically and is subtracted off from
the final test loss of the (nonlinear) neural network before averaging over 50 random initializations to
yield each of the individual data points in the figure.
A.1 Deep teacher-student models
The teacher-student scaling with dataset size (figure S2) was performed with fully-connected teacher
and student networks with two hidden layers and widths 96 and 192, respectively, using PyTorch
(Paszke et al., 2019). The inputs were random vectors sampled uniformly from a hypercube of
dimension d = 2, 3,…，9. To mitigate noise, We ran the experiment on eight different random seeds,
fixing the random seed for the teacher and student as we scanned over dataset sizes. We also used a
fixed test dataset, and a fixed training set, which was sub-sampled for the experiments with smaller D.
The student networks were trained using MSE loss and Adam optimizer with a maximum learning
rate of 3 × 10-3, a cosine learning rate decay, and a batch size of 64, and 40, 000 steps of training.
The test losses were measured with early stopping. We combine test losses from different random
seeds by averaging the logarithm of the loss from each seed.
1
Under review as a conference paper at ICLR 2022
CIFAR-IO	CIFAR-100
ft & «	f I •	final loss A	best loss [	♦ final error	
* ∙ *	8 8 	4	□ best error ■■ ■,・
		∖
* * .	1	ti2	• final loss △ best loss ? final error □ best error
*	∙	∙	1	t « *		1 ** F
IO2	IO3	IO4	IO2	IO3	IO4
SVHN	FashionMNIST
	8 ft		•	final	loss △	best	loss ♦	final	error □	best	error .
	≡ * *	* ∙ ∙	・ . * ・0
I 含	:・ △ ∙ ʌ *∖ • .		I ∙ final loss △ best loss ♦ final error best error :∙∙ δ δ	∙∙ ^ʌʌʌ
	*1	
IO3	IO4	IO5	IO3	IO4
Figure S1: Alternate metrics and stopping conditions We find similar scaling behavior for both the loss and
error, and for final and best (early stopped) metrics.
In our experiments, we always use inputs that are uniformly sampled from a d-dimensional hypercube,
following the setup of Sharma & Kaplan (2020). They also utilized several intrisic dimension (ID)
estimation methods and found the estimates were close to the input dimension, so we simply use the
latter for comparisons. For the dataset size scans we used randomly initialized teachers with width
96, and students with width 192. We found similar results with other network sizes.
The final scaling exponents and input dimensions are show in the bottom of Figure 1b. We used the
same experiments for the top of that figure, interpolating the behavior of both teacher and a set of
students between two fixed training points. The students only differed by the size of their training
sets, but had the same random seeds and were trained in the same way. In that figure the input space
dimension was four.
Finally, we also used a similar setup to study variance-limited exponents and scaling. In that case we
used much smaller models, with 16-dimensional hidden layers, and a correspondingly larger learning
rate. We then studied scaling with D again, with results pictured in Figure 1a.
A.2 CNN architecture for resolution-limited scaling
Figure 1b includes data from CNN architectures trained on image datasets. The architectures are
summarized in Table 1. We used Adam optimizer for training, with cross-entropy loss. Each network
was trained for long enough to achieve either a clear minimum or a plateau in test loss. Specifically,
CIFAR10, MNIST and Fashion MNIST were trained for 50 epochs, CIFAR100 was trained for 100
epochs and SVHN was trained for 10 epochs. The default Keras training parameters were used. In
case of SVHN we included the additional images as training data. We averaged (in log space) over
2
Under review as a conference paper at ICLR 2022
CO-MCaE-D ⅞αc-
Figure S2: This figure shows scaling trends of MSE loss with dataset size for teacher/student models. The
exponents extracted from these fits and their associated input-space dimensionalities are shown in Figure 1.
Layer	Width		Layer	Width
CNN WindOW (3, 3)	50		CNN window (3, 3)	50
2D Max Pooling (2, 2)			2D Max Pooling (2, 2)	
CNN window (3, 3)	100		CNN window (3,3)	100
2D Max Pooling (2, 2)			2D Max Pooling (2, 2)	
CNN WindoW (3, 3)	100		CNN WindoW (3, 3)	200
Dense	-64-		Dense	256
Dense	10		Dense	100
Layer	Width
CNN window (3, 3)	64
2D Max Pooling (2, 2)	
CNN WindoW (3, 3)	64
2D MaX Pooling (2, 2)	
Dense	-128
Dense	10
Table 1: CNN architectures for CIFAR10, MNIST, Fashion MNIST (left), CIFAR100 (center) and SVHN
(right)
20 runs for CIFAR100 and CIFAR10, 16 runs for MNIST, 12 runs for Fashion MNIST, and 5 runs
for SVHN. The results of these experiments are shown in Figure S3.
The measurement of input-space dimensionality for these experiments was done using the nearest-
neighbour algorithm, described in detail in Appendix B and C in Sharma & Kaplan (2020). We used
2, 3 and 4 nearest neighbors and averaged over the three.
A.3 Teacher-student experiment for scaling of loss with model size
We replicated the teacher-student setup in Sharma & Kaplan (2020) to demonstrate the scaling of
loss with model size. The resulting variation of -4∕αp with input-space dimensionality is shown in
figure S4. In our implementation we averaged (in log space) over 15 iterations, with a fixed, randomly
generated teacher.
B Effect of aspect ratio on scaling exponents
We trained Wide ResNet architectures of various widths and depths on CIFAR-10 accross dataset
sizes. We found that the effect of depth on dataset scaling was mild for the range studied, while the
effect of width impacted the scaling behavior up until a saturating width, after which the scaling
behavior fixed. See Figure S5.
3
Under review as a conference paper at ICLR 2022
Ssoη-S¾L
C∣FAR10: Loss scaling with Dataset Size
O O O
O O %>
1 1 1
× ×
3 2
ssbj.
IO4
Dataset Size
4 3
Ssoη-S¾L
FashIonMNIST: Loss scaling with Dataset Size
ss3bbj.
IO3	IO4
Dataset Size
CIFAR1OO: Loss scaling with Dataset Size
5 ×100
IO4
Dataset Size
SVHN: Loss scaling with Dataset Size
0°
ssoηsaj.
103
ιo4	ιos
Dataset Size
Figure S3:	This figure shows scaling trends of CE loss with dataset size for various image datasets. The
exponents extracted from these fits and their associated input-space dimensionalities are shown in Figure 1.
TeacherZStudent Model Size Exponents
Dimension
Figure S4:	This figure shows the variation of αP with the input-space dimension. The exponent αP is the
scaling exponent of loss with model size for teacher-student setup.
4
Under review as a conference paper at ICLR 2022
Figure S5:	Effect of aspect ratio on dataset scaling We find that for WRN-d-k trained on CIFAR-10, varying
depth from 10 to 40 has a relatively mild effect on scaling behavior, while varying the width multiplier, k, from 1
to 12 has a more noticeable effect, up until a saturating width.
C	Proof of Theorem 1
We now prove Theorem 1 repeated below for convenience.
Theorem 1. Let `(f) be the test loss as a function of network output, (L = E [`(f)]), and let
fT be the network output after T training steps, thought of as a random variable over weight
initialization, draws of the training dataset, and optimization seed. Further let fT be concentrating
with E[(fT - E[fT])k] = O () ∀k ≥ 2. If ` is a finite degree polynomial, or has bounded second
derivative, or is 2-Holder, then E ['(fτ)] 一 ' (E [fτ]) = O(e).
Proof. Case 1 -finite degree polynomial: In this case, We can write,
'(fτ)-'(E[fτ]) = XX 处) (EJfTD (fτ 一 E [fτ])k ,	(S1)
k=1
where K is the polynomial degree and `(k) is the k-th derivative of `. Taking the expectation of (S1)
and using the moment scaling proves the result.
Case 2 - bounded second derivative: The quadratic mean value theorem states that for any fT , there
exists a c such that,
1
'(fτ) 一 '(E [fτ]) = (fτ 一 E [fτ]) '0(E [fτ]) + 2'00(c)(fτ - E [fτ])2 .	(S2)
Taking the expectation of (S2) and using the fact that f00 (c) is bounded yields the desired result.
Case 3 - 2-Holder: Lastly, the loss being 2-H0lder means We may write,
'(fτ) 一 '(E [fτ]) ≤ I'(fτ) - '(E [fτ])| ≤ K' (fτ 一 E [fτ])2 .	(S3)
Again, taking the expectation of this inequality completes the proof.	□
A note on loss variance Theorem 1 concerns the mean loss, however we would also like to
understand if this scaling holds for typical instances. This can be understood by examining how the
variance of the loss or altetnatively how E [∣' (fτ) 一 ' (E [fτ ])|] scales.
For Case 3 - 2-Holder loss, we can rerun the argument of Theorem 1, using (S3) to yield
E [∣' (fτ)- ' (E [fτ])|]= O (e).
For Cases 1 and 2, we can attempt to apply the same argument as in the proof. This almost works. In
particular, using Holder,s inequality, E[(fτ 一 E[fτ])k] = O (e) ∀k ≥ 2 implies E[∣fτ - E[fτ] |k]=
5
Under review as a conference paper at ICLR 2022
O () ∀k ≥ 2. Taking the absolute value and expectation of (S1) or (S2) then gives
E [∣' (fτ) — ' (E[fτ ])|] ≤ l'0 (E[fτ ])| E[∣fτ — E [fτ ]|] + O(E) ∙	(S4)
In general, the above assumptions on ' and fτ imply only that E [∣fτ 一 E [fτ]|] = O (√e) and thus
typical instances of the loss will exhibit a less dramatic scaling with E than the mean. If we further
assume, however, that fT on average has been trained such as to be sufficiently close to a local
minimum of the loss, such that ∣'0 (E [fτ ])| = O (√e), then typical instances will also obey the O (E)
scaling.
D Variance-limited dataset scaling
In this section, we expand on our discussion of the variance-limited dataset scaling, L(D) —
limD→∞ L(D) = O D-1 . We first explain some intuition for why this behavior might be
expected for sufficiently smooth loss. We then derive it explicitly for losses that are polynomial in the
weights. Finally, we present non-smooth examples where the scaling can be violated either by having
unbounded loss, or first derivative.
D.1 Intuition
At a high level, the intuition is as follows. For any fixed value of weights, θ, the training loss with
D training points (thought of as a random variable over draws of the dataset), Ltrain [θ] concentrates
around the population loss Lpop [θ], with variance that scales as O D-1 .
Our optimization procedure can be thought of as a map from initial weights and training loss to
final weights Op : (θo, Ltrain[θ]) → θτ. If this map is sufficiently smooth - for instance satisfying
the assumptions of Theorem 1 or well approximated by a Taylor series about all ED [Ltrain [θt]]
一 then the output, θτ, will also concentrate around its infinite D limit with variance scaling as
O D-1 . Finally, if the population loss is also sufficiently smooth, the test loss for a model
trained on D data points averaged over draws of the dataset, L(D) = ED [Lpop [θT]], satisfies
L(D) — limD→∞ L(D) = O D-1 . We now walk through this in a little more detail.
Early time We can follow this intuition a bit more explicitly for the first few steps of gradient
descent. As the training loss at initialization, Ltrain [θ0], is a sample average over D i.i.d draws, it
concentrates around the population loss Lpop [θ0] with variance O D-1 . As a result, the initial
gradient, go = d∂θrain will also concentrate with O (D-I) variance and so will the weights at time 1,
θ1 = θ0 — ηg0. The training loss at time step 1, is then given by
Ltrain [θ1 ] = Ltrain [θ0 — g0] .	(S5)
If Ltrain is sufficiently smooth around θ0 — ED [g0], then we get that Ltrain[θ1] concentrates around
Ltrain[θ1] with O D-1 variance. We can keep bouncing back and forth between gradient (or
equivalently weights) and training loss for any number of steps T which does not scale with D.
Plugging this final θT into the population loss and taking the expectation over draws of the training
set, L(D) = ED [Lpop [θT]]. If Lpop is also sufficiently smooth, this yields L(D) — limD→∞ L(D) =
O (DT).
Here we have used the term sufficiently smooth. A sufficient set of criteria are given in Theorem 1;
however this is likely too restrictive. Indeed, any set of train and population loss for which a Taylor
series (or asymptomatic series with optimal truncation) give an O D-1 error around the training
points ED [θt=0...T] will have this behavior.
Local minimum The above intuition relied on training for a number of steps that was fixed as D is
taken large. Here we present some alternative intuition for the variance-limited scaling at late times,
as training approaches a local minimum in the loss. For simplicity we discuss a one-dimensional loss.
Consider a local minimum, θ*, of the population loss. As D is taken large, with high probability, the
training loss will have a local minimum,夕，such that ∣θ* 一 /c | = O (DT). One way to see this,
6
Under review as a conference paper at ICLR 2022
is to note that for a generic local minimum the first derivative changes sign, i.e. we can find θ1 , θ2
such that θι < θ* < θ2 and either Lp°p[θι] < 0, LP0p[θ2] > 0 or Lpop [θ2] < 0, Lp°p[θι] > 0. To be
concrete let’s focus on the first case (the argument will be identical in either case). As D becomes
large, the probability that the training loss at θ1 and θ2 differs significantly from the population loss
approaches zero. This can be seen from Markov’s inequality, where, P Lt0rain [θ] - L0pop [θ] > a ≤
VrD (LlLn[θ])
a2
, or more dramatically from Hoeffding’s inequality (assuming bounded Ltrain -
lying in an interval of size I )
Lpop
P (∣L0rain[θ] - Lpop[θ]I > a) ≤ 2e-2D2a2 ∙	(S6)
Here to have non-vanishing probability as we take D large, Lt0rain[θ1] and Lt0rain[θ2] must be closer
than O D-1 . If θ1 and θ2 are taken to be O D0 , then Lt0rain must change sign, indicating an
extremum of Ltrain； however We can do even better. If We assume Ltrain is Lipshitz about θ* then We
can still ensure a sign change even if |θι - θ*∣, ∣Θ2 - θ*∣ = O (D-I). Using concentration of L00ain[θ]
ensures the extremum is a local minimum. For non-generic minimum (i.e. vanishing first derivatives)
We can apply the same arguments to higher order derivatives (assuming they exist) of Lpop . Thus for
a local minimum of Lpop, With high probability Ltrain Will have a corresponding minimum Within a
distance O (DT)
IfWe noW consider an initialization procedure, θ0, and training procedure such that training converges
to the local minimum of the training loss,夕，and that the population loss is sufficiently smooth about
θ* (e.g. Lipshitz),then ED [Ltrain 口]—Lpop [θ*]] = ED [Ltrain [θ^ ] - Lpop [θ^]] +ED [Lpop 口]-Lpop [θ*]]∙
The first term vanishes, While the second is O(D-1). If We further assume that this happens on
average over choices of θ0 then We expect L(D) - limD→∞ L(D) = O D-1 .
SGD At first blush it may be surprising that the variance-limited scaling holds even for mini-batch
training. Indeed in this case, there is batch noise that comes in at a much higher scale than any
variance due to the finite training set size. Indeed, the effect of mini-batching changes the final test
loss, hoWever if We fix the SGD procedure or average over SGD seeds, as We take D large, We can
still ask hoW the training loss for a model trained under SGD on a training set of size D differs from
that for a model trained under SGD on an infinite training set.
To see this, We fist consider averaging over minibatches of size B, but Where points are draWn i.i.d.
With replacement. If We denote the batch at step t by Bt and the average over independent draWs of
this batch by EB [•], then note We can translate moments With respect to batch draWs With empirical
averages over the entire training set. Explicitly, consider ca and da potentially correlated, but each
draWn i.i.d. Within a batch. We have that,
EB	I X
a∈Bt
a0=1
a=1
(S7)
This procedure means, after taking an average over draWs of SGD batch, rather than thinking about
a function of mini-batch averages, We can equivalently consider a modified function, With explicit
dependence on the batch size, but that is only a function of empirical means over the training set. We
can thus recycle the above intuition for the scaling of smooth functions of empirical means.
The above relied on independently draWing every sample from every batch. At the other extreme, We
can consider draWing batches Without shuffling and increasing training set size by B datapoints at
a time, so as to keep the initial set of batches in an epoch fixed. In this case, the first deviation in
training betWeen a dataset of size D and one of size D + B happens at the last batch in the first epoch
after processing D datapoints.
7
Under review as a conference paper at ICLR 2022
As an extreme example, consider the case where D > BT . In this case, as we only take T
steps, the loss is constant for all D > BT and so limD→∞ L(D; T ; B) = L(BT ; T ; B) and thus
L(D > BT) - lim0→∞ L(D) = 0 (and in particular is trivially O (DT)).
D.2 Polynomial loss
Before discussing neural network training we review the concentration behavior of polynomials of
sample means.
Lemma 1. Let c(i) = D P(D=I Cai) for i = 0 ...J be empirical means, over D i.i.d. draws of
Cai) and let c(i) denote the distributional mean. Further, let X = (c(0))k0 优⑴)k1 …(C(J))kJ be a
monomial in the sample means. Then X concentrates with moments O D-1 ,
ED [(X -(C(O))k0(C⑴卢…(C(J))kJ)ni =O(DT) .	(S8)
Here, ED [•] denotes the average over independent draws of D samples.
Proof. To establish this we can proceed by direct computation.
n	np
X(-i)n-p( n )ed [x p ] ((C(O) )k0 (C ⑴卢…(C(J ))kJ yP
p=0	p
(S9)
Each term in the sum can be computed using
ED [Xp] = ED [(C(O))Pk0 (C(I))PkI …(C(J))pkJ]
=丁，―- X ED "0) ∙∙∙ C(OO) ) (C((I)
D(P Pi = O ki) J)	|_V a1	apk0 / ∖ a1
(1)
…CaPk)i
(J)	(J)
Ca(J) •一 Ca(J)
aa
1	pkJ
1
D(P PJ=O ki)
ED
{a(αi) 6=a(βj)}
(O)
∙∙ ⅛,
• ∙ C
(1)
(1)
pk1
C(J)
Ca(1J)
C(J)
a(pJk)J
a
(C(J ))PkJ+O(D-I)
+ O(DT)
D(D - 1)…(D -(P PJ=O ki - 1)) ( (O)Yk0 (⑴ Yk1
D(P PJ=O ki)	IC )	(C )
Plugging this into (S9) establishes the lemma.
□
In the above, we use the multi-index notation {a(αi)} for the collection of indices on the Ci and the
notation {a(αi) 6= a(βj)} for the subset of terms in the sum where all indices take different values.
Lemma 1 immediately implies that the mean of polynomials of CC(i) concentrate around their infinite
data limit.
ED h(g(c⑼,C(I),...,C(K))-g(C⑼,C(I),...,C(K)))ni =O(DT) ,	(S10)
forg ∈ PK CC(O), CC(1), . . . ,CC(K).
With this out of the way, we can proceed to analysing the scaling of trained neural networks. Here
we consider the simplified setting where the network map, f , and loss ` evaluated on each training
example, Xa = (x(, y(), are polynomial of degree J and K in the weights, θμ,
JK
f(x)= X b%“2...“i (x)θμi θμ2 …^i 'g) = X CM2° i口必、θμ2 …θμ-	(S")
i=1	i=1
8
Under review as a conference paper at ICLR 2022
The training loss can then be written as,
KD
Ltram= X B) “2...“i θμι θ”2 …θ μ ,即=万 X C⑺(Xa) .	(S12)
i=1	a=1
Here We have used the convention that the repeated weight indices μj are summed over.
Gradient Descent As a result of the gradient descent weight update, θt+ι = θt - η d∂rain, the
weights at time T are a polynomial of degree (K - I)T in the c(i).
θτ ∈ P(K-I)The⑼,e⑴，…，e(K)i .	(S13)
The coefficients of this polynomial depend on the initial weights, θ0 . Plugging these weights back
into the network output, we have that the network function at time T is again a polynomial in ce(i),
now with degree J (K - 1)T.
fT (x) ∈ PJ(K-1)T hce(0),ce(1),...,ce(K)i .	(S14)
Thus, again using Lemma 1, fT concentrates with variance O(D-1).
ED [(fτ - ED [fτ])2i = O (DT) .	(S15)
and by Theorem 1 the loss will obey they variance-limited scaling.
Stochastic Gradient Descent We now consider the same setup of polynomial loss, but now trained
via stochastic gradient descent (SGD). We consider SGD batches drawn i.i.d. with replacement and
are interested in the test loss averaged over SGD draws, with fixed batch size, B.
We proceed by proving the following lemma, which allows us to reuse a similar argument to the GD
case.
Lemma 2. Let C(M垃 = 击 P°∈b古 Cai) for i = 0 ...J be mini-batch averages, over B i.i.d. draws of
Cai). Further, let X = (C(0;t0))k0 (C(1;tl))k1 …(C(J;tJ))kJ be a monomial in the mini-batch means.
Then EB [X] ∈ PPiJ=0 ki
de(0), de(1), . . . , de(QiJ=0(ki+1)-1) , where de(i) are empirical means over the
full training set of i.i.d. random variables as in Lemma 1 and EB [•] denotes the expectation over
draws of SGD batches of size B.
Proof. Expectations over draws of batches at different time steps are independent. Thus, WLOG, we
can consider t := to = tι = •… =tj. We can again proceed by direct computation, expanding the
mini-batch sums.
EB X] = JEB
Σ
{a(αi) }∈Bt
• ∙ C
(1)
a(1)
(j)	(j)
Ca(1J) • • • Ca(kJJ)
(S16)
(i)
To proceed, we must keep track of terms in the sum where the aα take the same or different values.
(i)
If all aα are different, the expectation over batch draws fully factorizes. More generally (S16) can
be decomposed as a sum over products.
One way of keeping track of the index combinatorics is to introduce a set of graphs, Γ, where each
graph γ ∈ Γ has k0 vertices of type 0, k1 vertices of type 1, . . . , and kj vertices of type J (one vertex
(i)
for each aα index). Any pair of vertices may have zero or one edge between them. For any set of
three vertices, v1, v2, and v3 with edges (v1, v2) and (v2, v3) there must also be an edge (v1, v3). The
set Γ consists of all possible ways of connecting these vertices consistent with these rules.
9
Under review as a conference paper at ICLR 2022
For each graph, γ, we denote connected components by σ and denote the number of vertices of type i
within the connected component σ by m(σi). With this we can write the sum, (S16) as
EB X] = X SY(B) Y Eb "B X 卜&mσ0) S)mσ1) …(caJ>
γ∈Γ	σ∈γ	a∈Bt
=X SY (B) Y D X (ca0))mσ0) (caι))mσ1) ∙∙∙ (CaJ))mσJ)	(S17)
γ∈Γ	σ∈γ	a=1
=X SY (B)Y dHmσ0),mσι),…,mJ)}).
Y∈Γ	σ∈Y
Here SY(B) is a combinatoric factor associated to each graph, not relevant for the argument. The
m(σi) take on values 0 to ki, so the multi-index, can take on QiJ=1(ki + 1) different values, which we
re-index to d⑼，d⑴，...，d(QJ=Kki+I)T). Meanwhile, the degree of(S17) in d⑴ is bounded by the
number of total vertices in each graph, i.e. PJ=° ki. This establishes the lemma.	□
For a polynomial loss of degree K , the mini-batch training loss at each time step takes the form
K
Ll = X ≡μilt)2...μi θμi θ"2 …θ"i ,	^ = B X Cei)(Xa) .
i=1	a=∈Bt
(S18)
∂L(t+1)
The update rule, θt+ι = θt - η —gθn- ensures that θτ is a polynomial of degree (K - 1)t in the
C(i;O),C(i;I),…，c(i;T)
θτ ∈ P(K-I)T hc(0；0),C(0；1),…，C(0；T),C(1；0),C(1；1),...,C(1；T),...,C(K；0),C(K⑴,…，C(KT)i ,
(S19)
and consequently, denoting the test loss evaluated at θT by L[θT],
L[θτ ] ∈ PK(K-I)T [c(0Q,C(0;1),..., C(0；T ),C(1；0),C(1；1),..., C(1；T),..., C(K；0),C(K；1),…,C(KT)].
(S20)
Using Lemma 2, the expectation of L[θT] over draws of SGD batches is given by
EB [L[Θt]] ∈ PK(K-I)Thd(O),…，d(KK(KT)TK)i .
Finally, denoting ED [EB [L[θT]]] by L(D; B) and applying Lemma 1 gives
L(D; B) - lim L(D; B) = O (D-I).
D→∞
(S21)
(S22)
D.3 Non-smooth examples
Here we present two worked examples where non-bounded or non-smooth loss leads to violations of
the variance dominated scaling. In example one, the system obeys the variance dominated scaling
at early times, but exhibits different behavior for times larger than the dataset size. In the second
example, the system violates the variance dominated scaling even for two gradient descent steps, as a
result of an unbounded derivative in the loss.3
Example 1 - unbounded loss at late times Consider a dataset with two varieties of data points,
drawn with probabilities α and 1 - α, and one-dimensional quadratic losses, `1 (concave up) and `2
(concave down), on these two varieties.
'ι(θ) = 2θ2, '2(θ) = -1 θ2.	(S23)
3We thank Anonymous for suggesting these two types of examples.
10
Under review as a conference paper at ICLR 2022
If, in a slight abuse of notation, we further denote the training loss on a sample with n1 points of type
1 and D - n1 points of type two by `n1 and the population loss at a given value of the weight by Lpop,
we have
`ni = (D1 - 2) θ2,	LpOp = (° - 1) θ2.
(S24)
FOr this example we take α > 1/2. In this case, the minimum Of the pOpulatiOn lOss is at zerO, while
the minimum Of the training lOss can be at zerO, Or at ±∞ depending On whether the training sample
has n1 greater than Or less than D/2. We can thus create a situatiOn where at late training times, θT
dOes nOt cOncentrate arOund the minimum Of the pOpulatiOn lOss.
As we wOrk thrOugh this example explicitly, we will see the fOllOwing. (i) A mismatch larger than
O D-1 between the pOpulatiOn minimum and the minimum fOund by training On a sample set Of
size D requires times T larger than a cOnstant multiple OfD. (ii) The quantity we study thrOughOut
this wOrk is the difference between the infinite data limit Of the test lOss, and the finite data value,
L(D) - limD→∞ L(D). The minimum Of the infinite data limit Of the test lOss is nOt the same as
the minimum Of the pOpulatiOn lOss, min limD→∞ L(D) 6= minθ LpOp. In this example One diverges,
while the Other is finite. In particular this example evades the scaling result by L(D ) fOr times larger
than D having a diverging limit.
Explicitly, we study the evOlutiOn Of the mOdel under gradient flOw.
θ = -2 (n - 2) θ, θτ = e-2(nD-1 )Tθo .
The test lOss averaged Over draws Of the dataset is given by
L(D; T) =	EnI	(α - 2)	θT	=	e2T	(α - 2)	(1	- α	(1	-	e-万))θ0
If we cOnsider this lOss at large D and fixed T we get
L(D; T) = e-4(α-2)T (α - 2) θ0 (1 + 8T- °) + O(D-2)),
and thus L(D; T) 一 lim0→∞ L(D; T) = O (D-I) as expected.
If On the Other hand we cOnsider taking T D we have
L(D; T》D) = e2T (α - 2)(1- α)D θ0 ,
(S25)
(S26)
(S27)
(S28)
the limit limD,T →∞ L(D; T	D) diverges.
Lastly, we nOte that if we take T = βD with β < | log(1 - °)|/2 we can apprOach the large D limit
with nOn-generic, tuneable expOnential cOnvergence.
Example 2 - unbounded derivative Again, consider a two variety setup, this case with equal
prObabilities and per sample lOsses,
'ι(θ) = 2θ2 + 2ɑ∣θ∣α, '2(θ) = 1 θ2 - 2°∣θ∣α.	(S29)
We will consider different values of ° > 0. The train loss and population loss are then,
'nι = 2θ2 + ~ (D1 - 2) |年，Lpop = 2θ2 .	(S3O)
2 °D 2	2
We consider a model initialized to θ0 = 1 and trained for two steps of gradient descent with learning
rate 1.
gt	= θt +	(D■	- 2)	θ/θ/a-2 ,	θt+1	=	θt	- gt .	(S31)
11
Under review as a conference paper at ICLR 2022
Two update steps gives
θ2
nι
D
(S32)
The test loss is given by the population loss evaluated at θ2 averaged over test set draws.
L(D)
(S33)
1
2
α
Here we have approximated the binomial distribution at large D with a normal distribution using
Stirling’s approximation.
Note that if α ≥ 1 then L(D) - limD→∞ L(D) = O D-1 i.e. the finite sample loss approaches
the infinite data loss with the predicted variance-limited scaling. For 0 < α < 1, we get a different
scaling controlled by α. Note that the gradient, expression (S31) is singular at the origin for α
precisely in this range.
In summary, this example achieves a different scaling exponent through a diverging gradient.
E Proof of Theorems 2 and 3
In this section we detail the proof of Theorems 2 and 3. The key observation is to make use of the
fact that nearest neighbor distances for D points sampled i.i.d. from a d-dimensional manifold have
mean Ed,x [|x - x|] = O (D-I/d), where X is the nearest neighbor of X and the expectation is the
mean over data-points and draws of the dataset see e.g. (Levina & Bickel, 2005).
The theorem statements are copied for convenience. In the main, in an abuse of notation, we used
L(f) to indicate the value of the test loss as a function of the network f, and L(D) to indicate the
test loss averaged over the population, draws of the dataset, model initializations and training. To be
more explicit below, we will use the notation `(f (X)) to indicate the test loss for a single network
evaluated at single test point.
Theorem 2.	Let '(f), f and F be Lipschitz with constants KL, Kf, and KF and '(F) = 0. Further
let D be a training dataset of size D sampled i.i.d from Md and let f(X) = F (X), ∀X ∈ D then
L(D) = O (KLmax(Kf ,Kf)D-1/d).
Proof. Consider a network trained on a particular draw of the training data. For each training
point, x, let X denote the neighboring training data point. Then by the above LiPschitz assump-
tions and the vanishing of the loss on the true target, we have `(f (X)) ≤ KL |f (X) - F (X)| ≤
KL (Kf + KF) |x - X|. With this, the average test loss is bounded as
L(D) ≤ KL (Kf + Kf) Ed,x [|x - X|] = O (KLmaX(Kf ,Kf)D-1*) .	(S34)
In the last equality, We used the above mentioned scaling of nearest neighbor distances.	□
Theorem 3.	Let `(f), f and F be Lipschitz with constants KL, Kf, and KF. Further let f(X) =
F(X) for P points sampled i.i.d from Md then L(P) = O KLmax(Kf, KF)P -1/d .
Proof. Denote by P the P points, z, for which f (Z) = F(z). For each test point X let X denote the
closest point in P, X = argminp (|x - z|). Adopting this notation, the result follows by the same
argument as Theorem 2.	□
12
Under review as a conference paper at ICLR 2022
F Random feature models
Here we present random feature models in more detail. We begin by reviewing exact expressions
for the loss. We then go onto derive its asymptotic properties. We again consider training a
model f (x) = Pp=ι θμfμ(x), where fμ are drawn from some larger pool of features, {Fm},
fμ(x) = PM=1 PμM Fm (x).
Note, if {FM (x)} form a complete set of functions over the data distribution, than any target function,
y(x), can be expressed as y = PM=1 ωM FM (x). The extra constraint in a teacher-student model is
specifying the distribution of the ωM. The variance-limited scaling goes through with or without the
teacher-student assumption, however it is crucial for analysing the variance-limited behavior.
As in Section 2.3 we consider models with weights initialized to zero and trained to convergence with
mean squared error loss.
1D
Ltrain = 2^	(f (Xa) - Iya )	.
a=1
(S35)
The data and feature second moments play a central role in our analysis. We introduce the notation,
1D
C = Ex [F (X)FT (x)] , C= D EF (Xa)F T (Xa), C = PCPT , C = PCPT .
a=1
K(χ,χ0) = FFt(X)F(XO), K = Kl , K(x,xO) = ɪft(X)f(XO), K = Kl
S	Dtrain	P	Dtrain
(S36)
Here the script notation indicates the full feature space while the block letters are restricted to the
student features. The bar represents restriction to the training dataset. We will also indicate kernels
with one index in the training set as K~ (X) := K(X, Xa=1...D) and K~ (X) := K(X, Xa=1...D). After
this notation spree, the test loss can be written for under-parameterized models, P ≤ D as
L(D, P) = 1-ED [Tr (C + CPTCTCCTPC - 2CPTCTPC)] .	(S37)
2S
and for over-parameterized models (at the unique minimum found by GD, SGD, or projected Newton’s
method),
L(D, P) = 1 Ex,D ∣Κ(x, x) + K(x)tKTKKTK(x) - 2K(x)tKTK(x)] .	(S38)
Here the expectation ED [•] is an expectation with respect to iid draws of a dataset of size D from the
input distribution, while Ex [•] is an ordinary expectation over the input distribution. Note, expression
(S37) is also valid for over-parameterized models and (S38) is valid for under-parameterized models
if the inverses are replaces with the Moore-Penrose pseudo-inverse. Also note, the two expressions
can be related by echanging the projections onto finite features with the projection onto the training
dataset and the sums of teacher features with the expectation over the data manifold. This realizes the
duality between dataset and features discussed above.
F.1 Asymptotic expressions
We are interested in (S37) and (S38) in the limits of large P and D.
Variance-limited scaling We begin with the under-parameterized case. In the limit of lots of data
the sample estimate of the feature feature second moment matrix, C approaches the true second
moment matrix, C. Explicitly, if we define the difference, δC by C = C + δC. We have
ED [δC] = 0
ED [δCM1N1 δCM2N2 ] = D (Ex [Fmi (X)FNI (X)FM2 (X)FN2 (x)] - CmiNi C M2 N2 )	(S39)
ED [δCM1 Ni …δCMnNn] = O (D-2) ∀n> 2 .
13
Under review as a conference paper at ICLR 2022
The key takeaway from (S39) is that the dependence on D is manifest.
Using these expressions in (S37) yields.
L(D, P) = ⅛Tr (C- CPTCTPC)
2S
1P
+ 2DS	X	TMINIM2N2 [δM1M2 (PTCTP)N1N2 + (CTPC2PTC-DMiM2CN1n
M1,2N1,2=1
-2 (CPTCTP)miM2 (PTCTP)N1N2i + O (D-2).
(S40)
Here we have introduced the notation, TM1N1M2N2 = Ex [FM1 (x)FN1 (x)FM2 (x)FN2 (x)].
As above, defining
L(P):= lim L(D, P) = ɪ Tr (C- CPTCTPC) .	(S41)
D→∞	2S
we see that though L(D, P) - L(P) is a somewhat cumbersome quantity to compute, involving the
average of a quartic tensor over the data distribution, its dependence on D is simple.
For the over-parameterized case, we can similarly expand (S38) using K = K+δK. With fluctuations
satisfying,
EP [δK] = 0
EP [δKa1b1 δKa2b2 ] = P (EP [fμ(Xaι )fμ(xbι )fμ(xa2 )fμ(Xb2 )] — 左“也 K”? b? )	(S42)
EP [δKa1a1 …δKanan] = O (P-2) ∀n > 2 .
This gives the expansion
L(D, P) = 1 Ex,d [K(χ, X)- K(X)TKTK(x)i + O(PT),	(S43)
and
L(D) = 1 Eχ,D ∣K(x,x) - K(X)TKTK(x)i .	(S44)
Resolution-limited scaling We now move onto studying the parameter scaling of L(P) and dataset
scaling of L(D). We explicitly analyse the dataset scaling of L(D), with the parameter scaling
following via the dataset parameter duality.
Much work has been devoted to evaluating the expression, (S44) (Williams & Vivarelli, 2000;
Malzahn & Opper, 2002; Sollich & Halees, 2002). One approach is to use the replica trick - a tool
originating in the study of disordered systems which computes the expectation of a logarithm of a
random variable via simpler moment contributions and analyticity assumption (Parisi, 1980). The
replica trick has a long history as a technique to study the generalization properties of kernel methods
(Sollich, 1998; Malzahn & Opper, 2001; 2003; Urry & Sollich, 2012; Cohen et al., 2019; Gerace
et al., 2020; Bordelon et al., 2020). We will most closely follow the work of Canatar et al. (2021)
who use the replica method to derive an expression for the test loss of linear feature models in terms
of the eigenvalues of the kernel C and ω, the coefficient vector of the target labels in terms of the
model features.
L； X (K≡⅛，
Xκλi
κ+Dλi, γ
i
Dλi2
(S45)
V
i (κ + Dλi)
2.
This is the ridge-less, noise-free limit of equation (4) of Canatar et al. (2021). Here we analyze the
asymptotic behavior of these expressions for eigenvalues satisfying a power-law decay, λi = i-(1+αK)
and for targets coming from a teacher-student setup, W 〜 N(0,1/S).
14
Under review as a conference paper at ICLR 2022
Fixed Low Regularization
10^1
6 × IO-2
4 × ICT2
3 × 10~2
2 × ICT2
IO1	IO2	IO3
D
：103
IO2
≡3⅞⅞*9⅞⅝%¾⅞⅞ ⅛,
IO1
Feature size (P, Solid), Dataset size (D, Dashed) Feature size (P, Solid), Dataset size (D, Dashed)

Figure S6:	Duality between dataset size vs feature number in pretrained features Using pre-
trained embedding features of EfficientNet-B5 (Tan & Le, 2019) for different levels of regularization,
we see that loss as function of dataset size or loss as a function of the feature dimension track each
other both for small regularization (left) and for tuned regularization (right). Note that regularization
strength with trained-feature kernels can be mapped to inverse training time (Ali et al., 2019; Lee
et al., 2020). Thus (left) corresponds to long training time and exhibits double descent behavior,
while (right) corresponds to optimal early stopping.
To begin, we note that for teacher-student models in the limit of many features, the overlap coefficients
ω are equal to the teacher weights, UP to a rotation ωi =OiM WM. As We are choosing an isotropic
Gaussian initialization, we are insensitive to this rotation and, in particular, Ew [ω2] = 1/S. See
Figure S8 for empirical support of the average constancy of eɔi for the teacher-student setting and
contrast with realistic labels.
With this simplification, we now compute the asymptotic scaling of (S45) by approximating the sums
with integrals and expanding the resulting expressions in large D. We use the identities:
∕∞	x-n(1+α)	-m γ n- - 1+α)	口 1	1 α a -D∖
dx、\m = K	2V2F1 m,n - ,n +	,
Ji (K + Dx-(I+叫	(1 + α)Γ (n + 1+α)	V 1 + α 1 + α κ J
2Fi (a, b, c, -y) Y y-a + Byi + ...,
(S46)
Here 2 Fi is the hypergeometric function and the second line gives its asymptotic form at large y. B is
a constant which does not effect the asymptotic scaling.
Using these relations yields
K Y D-αK, γ Y D0, and L(D) Y D-αK ,	(S47)
as promised. Here we have dropped sub-leading terms at large D. Scaling behavior for parameter
scaling L(P) follow via the dataset parameter duality.
F.2 Duality beyond asymptotics
Expressions (S37) and (S38) are related by changing projections onto finite feature set, and finite
dataset even without taking any asymptotic limits. We thus expect the dependence of test loss on
parameter count and dataset size to be related quite generally in linear feature models. See Section G
for further details.
G	Learned Features
In this section, we consider linear models with features coming from pretrained neural networks.
Such features are useful for transfer learning applications (e.g. Kornblith et al. (2019); Kolesnikov
et al. (2019)). In Figures S6 and S7, we take pretrained embedding features from an EfficientNet-B5
15
Under review as a conference paper at ICLR 2022
Variance-limited： Theory αo = l	Resolution-limited： Theory ao = G
ReSolUtIon-Ilmlted: TheOry t⅛=a⅛
l,-10,31.13
P3 %-1.25
l,-3S. a»-1.28
( P-1156, OD-0.23
• P-1587, Od-0.25
P-20+β, αn-0.2β
Λr 1 2 3 4 5 a
off----- O
Iiooooo 1
Illll
-B- WHT- - WHT- SSOn
IO1	ιoj	mp
Feature size (P)
ιo1	ιoj	io3
Feature size (P)
ιtr'
LoT
标士%
lθ-3i
□ataset size (0)
□ataset size (0)
Variance-limited： Theory ⅜ = 1
• D-1156, »-0.25
• D-1587, «>-0.27
0 -204β, <⅛-0.28
-8- WHT- ' S8-I
Datasetsize(D)
Resolution-limited： Theory 0>=a⅛
Resolution-limited： Theory ao =<⅛
P-1156,吁 0.29
P-1587,吁 0.30
D-10, α*-l∙27
D3 <r*-l∙35
D-35,31.41
Featuresize(P)
Itrz：	P-204β, ¾-0.30
~'	Datsisetsize (D)
Figure S7:	Four scaling regimes exhibited by pretrained embedding features Using pretrained
embedding features of EfficientNet-B5 (Tan & Le, 2019) for fixed low regularization (left) and tuned
regularization (right), we can identify four regimes of scaling using real CIFAR-10 labels.
model (Tan & Le, 2019) using TF hub4. The EfficientNet model is pretrained using the ImageNet
dataset with input image size of (456, 456). To extract features for the (32, 32) CIFAR-10 images,
we use bilinear resizing. We then train a linear classifier on top of the penultimate pretrained features.
To explore the effect feature size, P, and dataset size D, we randomly subset the feature dimension
and training dataset size and average over 5 random seeds. Prediction on test points are obtained as a
kernel ridge regression problem with linear kernel. We note that the regularization ridge parameter
can be mapped to an inverse early-stopping time (Ali et al., 2019; Lee et al., 2020) of a corresponding
ridgeless model trained via gradient descent. Inference with low regularization parameter denotes
training for long time while tuned regularization parameter is equivalent to optimal early stopping.
In Figure S7 we see evidence of all four scaling regimes for low regularization (left four) and optimal
regularization (right four). We speculate that the deviation from the predicted variance-limited
exponent αP = αD = 1 for the case of fixed low regularization (late time) is possibly due to the
double descent resonance at D = P which interferes with the power law fit.
In Figure S6, We observe the duality between dataset size D (solid) and feature size P (dashed) - the
loss as a function of the number of features is identical to the loss as function of dataset size for both
the optimal loss (tuned regularization) or late time loss (low regularization).
In Figure S8, we also compare properties of random features (using the infinite-width limit) and
learned features from trained WRN 28-10 models. We note that teacher-student models, where the
feature class matches the target function and ordinary, fully trained models on real data (Figure 1a),
have significantly larger exponents than models with fixed features and realistic targets.
The measured ω - the coefficient of the task labels under the i-th feature (S45) are approximately
constant as function of index i for all teacher-student settings. However for real targets, ω are only
constant for the well-performing Myrtle-10 and WRN trained features (last two columns).
4https://www.tensorflow.org/hub
16
Under review as a conference paper at ICLR 2022
IO-I
Myrtle-IO
IO-2
101 IO2 IO3 104
an"> -SN--AEkOZ
IO3
lθ-ɪ
IO-4
10-7
IO1
IO-2
10-5
IO2
Figure S8: Loss on the teacher targets scale better than real targets for both untrained and
trained features The first three columns are infinite width kernels while the last column is a kernel
built out of features from the penultimate layer of pretrained WRN 28-10 models on CIFAR-10. The
first row is the loss as a function of dataset size D for teacher-student targets vs real targets. The
observed dataset scaling exponent is denoted in the legend. The second row is the normalized partial
sum of kernel eigenvalues. The partial sum’s scaling exponent is measured to capture the effect of the
finite dataset size When empirical ακ is close to zero. The third row shows ω% for teacher-student
and real target compared against the kernel eigenvalue decay. We see the teacher-student 6⅛ are
approximately constant.
17