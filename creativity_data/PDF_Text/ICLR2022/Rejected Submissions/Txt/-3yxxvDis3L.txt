Under review as a conference paper at ICLR 2022
How to Improve Sample Complexity of SGD
over Highly Dependent Data?
Anonymous authors
Paper under double-blind review
Ab stract
Conventional machine learning applications typically assume that data samples
are independently and identically distributed (i.i.d.). However, practical scenar-
ios often involve a data-generating process that produces highly dependent data
samples, which are known to heavily bias the stochastic optimization process and
slow down the convergence of learning. In this paper, we conduct a fundamental
study on how different structures of stochastic update schemes affect the sam-
ple complexity of stochastic gradient descent (SGD) over highly dependent data.
Specifically, with a φ-mixing model of data dependence, we show that SGD with
proper periodic data-subsampling achieves an improved sample complexity over
the standard SGD in the full spectrum of the data dependence level. Interest-
ingly, even subsampling a subset of data samples can accelerate the convergence
of SGD over highly dependent data. Moreover, we show that mini-batch SGD
can further substantially improve the sample complexity over SGD with periodic
data-subsampling over highly dependent data. We also conduct some numerical
experiments to validate our theoretical results.
1	Introduction
Stochastic optimization algorithms have attracted great attention in the past decade due to its suc-
cessful applications to a broad research areas, including deep learning (Goodfellow et al., 2016), re-
inforcement learning (Sutton & Barto, 2018), online learning (Bottou, 2010; Hazan, 2017), control
(Marti, 2017), etc. In the conventional analysis of stochastic optimization algorithms, it is usually
assumed that all data samples are independently and identically distributed (i.i.d.) and queried. For
example, data samples in the traditional empirical risk minimization framework are assumed to be
queried independently from the underlying data distribution, while data samples in reinforcement
learning are assumed to be queried from the stationary distribution of the underlying Markov chain.
Although the i.i.d. data assumption leads to a comprehensive understanding of the statistical limit
and computation complexity of SGD, it violates the nature of many practical data-generating
stochastic processes, which generate highly correlated samples that depend on the history. In fact,
dependent data can be found almost everywhere, e.g., daily stock price, weather/climate data, state
transitions in Markov chains, etc. To understand the impact of data dependence on the convergence
and complexity of stochastic algorithms, there is a growing number of recent works that introduce
various definitions to quantify data dependence. Specifically, to analyze the finite-time convergence
of various stochastic reinforcement learning algorithms, recent studies assume that the dependent
samples queried from the Markov decision process satisfy a geometric mixing property (Dalal et al.,
2018; Zou et al., 2019; Xu & Gu, 2020; Qu & Wierman, 2020), which requires the underlying
Markov chain to be uniformly ergodic or has a finite mixing time (Even-Dar et al., 2003). On the
other hand, to analyze the convergence of stochastic optimization algorithms over dependent data,
Karimi et al. (2019) assumed the existence of a solution to the Poisson equation associated with the
underlying Markov chain, which is a weaker condition than the uniform ergodic condition (Glynn
& Meyn, 1996). Moreover, Agarwal & Duchi (2012) introduced a φ-mixing property of the data-
generating process that quantifies how fast the distribution of future data samples (conditioned on a
fixed filtration) converges to the underlying stationary data distribution. In particular, the φ-mixing
property is more general than the previous two notions of data dependence (Douc et al., 2018).
1
Under review as a conference paper at ICLR 2022
While the aforementioned works leveraged the above notions of data dependence to characterize the
sample complexity of various standard stochastic algorithms over dependent data, there still lacks
theoretical understanding of how algorithm structure affects the sample complexity of stochastic
algorithms under different levels of data dependence. In particular, a key algorithm structure is the
stochastic update scheme, which critically affects the bias and variance of the stochastic optimiza-
tion process. In fact, under i.i.d. data and convex geometry, it is well known that SGD achieves
the sample complexity lower bound under various stochastic update schemes (Bottou, 2010), e.g.,
single-sample update and mini-batch update. However, these stochastic update schemes may lead
to substantially different convergence behaviors over highly dependent data, as they are no longer
unbiased. Therefore, it is of vital importance to understand the interplay among data dependence,
structure of stochastic update and convergence rate of stochastic algorithms, and we want to ask the
following fundamental question.
• Q: HoW does the StrUctUre of Stochastic Updates affect the Convergence rate and Sample
Complexity of Stochastic algorithms over dependent data?
In this paper, we provide comprehensive answers to the above fundamental question. Specifically,
We condUct a comprehenSive StUdy of the convergence rate and Sample complexity of the SGD
algorithm over a Wide SpectrUm of data dependence levelS Under varioUS typeS of StochaStic UpdateS,
inclUding periodic SUbSampling and mini-batch Sampling. OUr reSUltS ShoW that SGD With both
StochaStic UpdateS achieveS a SUbStantially improved Sample complexity over the Standard SGD
Under highly dependent data. We SUmmarize oUr contribUtionS aS folloWS.
1.1	Our Contributions
We conSider the folloWing Standard StochaStic optimization problem.
min f (w) := Eξ〜μ [F(w; ξ)],	(P)
w∈W
Where the objective fUnction f iS convex and LipSchitz continUoUS, and the expectation iS taken over
the stationary distribution μ of the underlying data-generating process P. To perform stochastic
optimization, We qUery a Stream of dependent data SampleS from the Underlying data-generating
process. Specifically, We adopt the φ-mixing model to qUantify the data dependence via a decaying
mixing coefficient fUnction φξ(k) (see Definition 2.2) (AgarWal & DUchi, 2012). We stUdy the con-
vergence of the stochastic gradient descent (SGD) algorithm over φ-mixing dependent data samples
Under varioUs stochastic Update schemes, inclUding data sUbsampling and mini-batch sampling.
We first stUdy the convergence of SGD over φ-mixing dependent data samples Under the data sUb-
sampling Update scheme. In particUlar, the data sUbsampling Update scheme Utilizes only one data
sample per r consecUtive data samples by periodically skipping r - 1 samples. With this data sUb-
sampling scheme, the sUbsampled data samples are less dependent for a larger sUbsampling period
r. ConseqUently, We shoW that SGD With a proper data sUbsampling period achieves an improved
sample complexity over that of the standard SGD in the fUll spectrUm of the convergence rate of the
φ-mixing coefficient. In particUlar, the improvement is sUbstantial When the data is highly dependent
With an algebraic decaying φ-mixing coefficient.
Moreover, We stUdy the sample complexity of SGD over φ-mixing dependent data samples Under
the mini-batch Update scheme. Compare to the data sUbsampling Update, mini-batch Update can
sUbstantially redUce the mini-batch data dependence WithoUt skipping data samples. ConseqUently,
mini-batch Update leverages the sample average over a mini batch of data samples to redUce both the
bias (caUsed by the data dependence) and the optimization variance. Specifically, We shoW that SGD
With mini-batch Update achieves an orderWise loWer sample complexity than both the standard SGD
and the SGD With data sUbsampling in the fUll spectrUm of the convergence rate of the φ-mixing
coefficient. We sUmmarize and compare the sample complexities of these stochastic algorithms
Under different φ-mixing data dependence models in Table 1.
1.2	Related Work
Stochastic Algorithms over Dependent Data SteinWart & Christmann (2009) and Modha &
Masry (1996) established the convergence analysis of online stochastic algorithms for streaming
2
Under review as a conference paper at ICLR 2022
Table 1: Comparison of sample complexities of SGD, SGD with data subsampling and Mini-batch
SGD under different levels of data dependence for achieving f (W) - f (w*) ≤ e. Note that θ is a
parameter of the convergence rate of the φ-mixing coefficient.
Data dependence level	φξ(k)	SGD	SGD w/ subsampling	Mini-batch SGD
Geometric φ-mixing (Weakly dependent)	exp(-kθ), θ>0	O(c-2(iog L)2)	O<-2(log 6-1)θ )	O(-2)
Fast algebraic φ-mixing	k-θ,	O(e-2-2 )	O(c-2-θ)	Oe(-2)
(Medium dependent)	θ≥1			
Slow algebraic φ-mixing	k-θ,	O(e-2-2 )	O(c-2-θ)	O(CT-1)
(Highly dependent)	0<θ<1			
data with geometric ergodicity. Duchi et al. (2011) proved that the stochastic subgradient method
has strong convergence guarantee if the mixing time is uniformly bounded. Agarwal & Duchi (2012)
studied the convex/strongly convex stochastic optimization problem and proved high-probability
convergence bounds for general stochastic algorithms under general stationary mixing processes.
Godichon-Baggioni et al. (2021) provided the non-asymptotic analysis of stochastic algorithms with
strongly convex objective function over streaming mini-batch data. In a more general setting, the
stochastic approximation (SA) problem was studied in (Karimi et al., 2019) by assuming the exis-
tence of solution to a Poisson equation. Recently, Debavelaere et al. (2021) developed the asymptotic
convergence analysis of SA problem for sub-geometric Markov dynamic noises.
Finite-time convergence of reinforcement learning Recently, a series of work studied the finite-
time convergence of many stochastic reinforcement learning algorithms over Markovian dependent
samples, including TD learning (Dalal et al., 2018; Xu et al., 2019; Kaledin et al., 2020), Q-learning
(Qu & Wierman, 2020; Li et al., 2021; Melo et al., 2008; Chen et al., 2019; Xu & Gu, 2020), fitted Q-
iteration (Mnih et al., 2013; 2015; Agarwal et al., 2021), actor-critic algorithms (Wang et al., 2019;
Yang et al., 2019; Kumar et al., 2019; Qiu et al., 2019; Wu et al., 2020; Xu et al., 2020), etc. In these
studies, the dependent Markovian samples are assumed to satisfy the geometric φ-mixing property,
which is satisfied when the underlying Markov chain is uniformly ergodic or time-homogeneous
with finite-states.
Regret of Stochastic Convex Optimization There have been many known regret bounds for on-
line convex optimization problem. Hazan (2017) has built the standard O(√T) regret bound for on-
line SGD algorithm with assuming the bounded gradient. Xiao (2009) introduces the regret bound
of online dual averaging method. To our best knowledge, there is no high-probability guaranteed
regret bound for mini-batch SGD algorithm with considering the data dependence.
2 Problem Formulation and Assumptions
In this section, we introduce the problem formulation and some basic assumptions. Consider a
model with parameters w. For any data sample ξ, denote F(w; ξ) ∈ R as the sample loss of this data
sample under the model w. In this paper, we consider the following standard stochastic optimization
problem that has broad applications in machine learning.
min f (w) := Eξ〜μ [F(w; ξ)].	(P)
w∈W
Here, the expectation is taken over the randomness of the data sample ξ, which is drawn from an
underlying distribution μ. In particular, We make the following standard assumptions regarding the
problem (P) (Agarwal & Duchi, 2012).
Assumption 2.1. The stochastic optimization problem (P) satisfies
1.	For ^very ξ, function F(∙; ξ) is G-Lipschitz continuous over W, i.e.,for all w, v ∈ W,
|F(w； ξ) — F(v； ξ)l ≤ Gkw - vk.
2.	Function f (∙) is convex and bounded below, i.e., f (w*) := infw∈w f (W) > -∞.
3
Under review as a conference paper at ICLR 2022
3.	W is a convex and compact set with bounded diameter R, i.e., supw,v∈W kw - vk ≤ R.
To solve this stochastic optimization problem, one often needs to query a set of data samples from
the distribution μ to perform optimization. Unlike traditional stochastic optimization that usually
assumes that the data samples are i.i.d. we consider a more general and practical dependent data-
generating process as we elaborate below.
Dependent data-generating process: We consider a stochastic process P that generates a stream
of data samples {ξ1, ξ2, ..., }, which are not necessarily independent. In particular, the stochastic
process P has an underlying stationary distribution μ. To quantify the dependence of the data
generation process, we introduce the following standard φ-mixing model (Agarwal & Duchi, 2012),
where we denote {Ft }t as the canonical filtration generated by {ξt }t .
Definition 2.2 (φ-mixing process). Consider a stochastic process {ξt}t with a stationary distribution
μ. Let P(ξt+k ∈ ∙∣Ft) be the distribution of the (t + k)-th sample conditioned on Ft, and denote drv
as the total variation distance. Then, the process {ξt}t is called φ-mixing if the following mixing
coefficient φξ(∙) converges to 0 as k tends to infinity.
φξ(k)：=	sup 2dTV(P(ξt+k ∈∙∣A),μ)∙
t∈N,A∈Ft
Intuitively, the φ-mixing coefficient describes how fast the distribution of sample ξt+k converges to
the stationary distribution μ when conditioned on the filtration Ft, as the time gap k → ∞. The φ-
mixing process can be found in many applications, which involve mixing coefficients that converge
to zero at different convergence rates. Below we mention some popular examples.
•	Geometric φ-mixing process. Such a type of process has a geometrically diminishing
mixing coefficient, i.e., φξ(k) ≤ φ0 exp(-ckθ) for some φ0, c, θ > 0. Examples include
finite-state ergodic Markov chains and some aperiodic Harris-recurrent Markov processes
(Modha & Masry, 1996; Agarwal & Duchi, 2012; Meyn & Tweedie, 2012);
•	Algebraic φ-mixing process. Such a type of process has a polynomially diminishing mix-
ing coefficient, i.e., φξ(k) ≤ φ0k-θ for some φ0, θ > 0. Examples include a large class of
Metropolis-Hastings samplers (Jarner & Roberts, 2002) and some queuing systems (Agar-
wal & Duchi, 2012).
3 Convergence of SGD with Subsampling over Dependent Data
In this section, we study the convergence rate and sample complexity of SGD with data subsampling
update over φ-mixing dependent data. In Section 3.1, we recap the convergence results of the stan-
dard SGD over dependent data established in (Agarwal & Duchi, 2012). In Section 3.2, we establish
convergence results of SGD with the data subsampling update.
Throughout, we define the sample complexity as the total number of samples required for the algo-
rithm to output a model W that achieves an e convergence error, i.e., f (W) - f (w*) ≤ e. Also, the
standard regret of a stochastic algorithm is defined as
n
(Regret): Rn := X F(w(t); ξt) - F(w*; ξt),
t=1
where the models {W1, W2, ..., Wn} are generated using the data samples {ξ1, ξ2, ..., ξn}, respec-
tively, and w* is the minimizer of f (w). For this sequence of models {wι, w2,…,Wn}, We make the
following mild assumption, which is satisfied by many SGD-type algorithms.
Assumption 3.1. There is a non-increasing sequence {κ(t)}t such that kW(t + 1) - W(t)k ≤ κ(t).
3.1	Stochastic gradient descent
Stochastic gradient descent (SGD) is a popular and classical algorithm for stochastic optimization.
In every iteration t, SGD queries a sample ξt from the data-generating process and performs the
following update.
(SGD): w(t +1) = w(t) - ηtVF(w(t); ξt),	(1)
4
Under review as a conference paper at ICLR 2022
where ηt is the learning rate. In Theorem 2 of (Agarwal & Duchi, 2012), the authors established a
high probability convergence error bound for a generic class of stochastic algorithms. Specifically,
under the Assumptions 2.1 and 3.1, they showed that for any τ ∈ N with probability at least 1 - δ,
the averaged predictor Wn := 1 Pn=I w(t) satisfies
f(Wn)- f (w*) ≤ Rn + JIG XXκ(t) + 工IGR + 2G”2+ φξ(T)GR.⑵
n n	n	nδ
t=1
Here, Rn is the regret of the algorithm of interest, and τ ∈ N is an auxiliary parameter that is in-
troduced to decouple the dependence of the data samples. From the above bound, one can see that
the optimal choice of T depends on the convergence rate of the mixing coefficient φξ (τ). Specifi-
cally, consider the SGD algorithm in (1). It can be shown that it achieves the regret Rn = O(√n)
and satisfies κ(t) = O(1∕√t) with a proper diminishing learning rate. Consequently, the above
high-probability convergence bound for SGD reduces to
f (Wn) - f (w*)≤ O (√n+TnN n τ√-n1 + rT log δ+φξ(τ)}).	⑶
Such a bound further implies the following sample complexity results of SGD under different con-
vergence rates of the mixing coefficient φξ .
Corollary 3.2. The sample complexity of SGD in (1) for achieving an convergence error over
φ-mixing dependent data is given as follows.
•	Ifthe data is geometric φ-mixing with parameter θ > 0, then we choose T = O ((log ɪ) θ).
The resulting sample complexity is in the order of n = O(e-2(log ɪ) θ).
•	Ifthe data is algebraic φ-mixing with parameter θ > 0, then we choose T = O(e- 1). The
resulting sample complexity is in the order of n = O(e-2-2).
It can be seen that if the data-generating process has a fast geometrically diminishing mixing co-
efficient, i.e., the data samples are close to being independent from each other, then the resulting
sample complexity is almost the same as that of SGD with i.i.d. samples. On the other hand, if the
data-generating process mixes slowly with an algebraically diminishing mixing coefficient, i.e., the
data samples are highly dependent, then the data dependence increases the sample complexity by
a non-negligible factor of e-2. In particular, such a factor is substantially large if the mixing rate
parameter θ is close to zero.
3.2 SGD with subsampling
When apply SGD to solve stochastic optimization problems over dependent data, the key chal-
lenge is that the data dependence introduces non-negligible bias that slows down the convergence
of the algorithm. Hence, a straightforward solution is to reduce data dependence before performing
stochastic optimization. In the existing literature, a simple and useful approach is data subsam-
pling (Nagaraj et al., 2020; Kotsalis et al., 2020). Next, we show that such an approach leads to an
improved convergence bound and sample complexity of SGD over highly dependent data.
Specifically, consider a stream of φ-mixing data samples {ξ1, ξ2 , ξ3, . . . }. Instead of utilizing the
entire stream of data, we subsample a subset of this data stream with period r ∈ N and obtain the
following subsampled data stream
{ξ1, ξr+1, ξ2r+1, . . . }.
In particular, let {Ft }t be the canonical filtration generated by {ξtr+1}t . Since the consecutive
subsampled samples are r time steps away from each other, it is easy to verify that the subsampled
data stream {ξtr+1}t is also a φ-mixing process with mixing coefficient given by φrξ(t) = φξ(rt),
where φξr denotes the mixing coefficient of the subsampled data stream {ξtr+1}t . Therefore, by
periodically subsampling the data stream, the resulting subsampled process has a faster-converging
mixing coefficient. Then, we apply SGD over such subsampled data, i.e.,
(SGD with subsampling): w(t +1) = w(t) - nN F (w(t); ξtr+ι).	(4)
5
Under review as a conference paper at ICLR 2022
In particular, the convergence error bound in eq. (2) still holds by replacing φξ(τ) with φξ (rτ), and
we obtain the following bound for SGD with subsampling.
f (Wn) - f (W*) ≤O( √1n+TnNn ɪ + riɪogl+φξ (rτ)o).	⑸
Such a bound further implies the following sample complexity results of SGD with subsampling
under different convergence rates of the mixing coefficient φξ .
Corollary 3.3. The sample complexity of SGD with subsampling in (4) for achieving an conver-
gence error over φ-mixing dependent data is given as follows.
•	Ifthe data is geometric φ-mixing with parameter θ > 0, then we choose r = O ((log ɪ) 1)
and T = O(1). The resulting SamPk complexity is in the order of rn = O(e-2(log ɪ) θ).
•	Ifthe data is algebraic φ-mixing with parameter θ > 0, then we choose r = O(e-θ) and
τ = O(1). The resulting SamPIe COmPIexity is in the order of rn = O(e-2-θ).
Compare the above sample complexity results with those of the standard SGD in Corollary 3.2,
We conclude that data-subsampling can improve the sample complexity by a factor of (log ɪ) 1 and
e- 1 for geometric φ-mixing and algebraic φ-mixing data, respectively. Intuitively, this is because
With data subsampling, We can choose a sufficiently large subsampling period r to decouple the data
dependence in the term φξ (rτ), as opposed to choosing a large τ in Corollary 3.2. In this Way, the
order of the dominant term ' n log T is reduced. Therefore, when the data is highly dependent, it is
beneficial to subsample the dependent data before performing SGD. We also note another advantage
of using data-subsampling, i.e., it only requires computing the stochastic gradients of the subsampled
data, and therefore can substantially reduce the computation load.
4	Convergence of Mini-Batch SGD over Dependent Data
Although the data-subsampling update scheme studied in the previous section helps improve the
sample complexity of SGD, it does not leverage the full information of all the queried data. In
particular, when the data is highly dependent, we need to choose a large period r to reduce data
dependence, and this will throw away a huge amount of valuable samples. In this section, we study
SGD with another popular update scheme that leverages the full information of all the sampled
data, i.e., the mini-batch update scheme. We show that this simple and popular scheme can effec-
tively reduce data dependence without skipping data samples, and can achieve an improved sample
complexity over SGD with subsampling.
Specifically, we consider a data stream {ξt }t with φ-mixing dependent samples. We rearrange the
data samples into a stream of mini-batches {xt}t, where each mini-batch xt contains B samples,
i.e., xt = {ξ(t-1)B+1, ξ(t-1)B+2, . . . , ξtB}. Then, we perform mini-batch SGD update as follows.
(Mini-batch SGD): w(t + 1) = w(t) 一 ηt ^X VF(w(t); ξ).	(6)
Performing SGD updates with mini-batch data has several advantages. First, it substantially reduce
the optimization variance and allows to use a large learning rate to facilitate the convergence of the
algorithm. As a comparison, SGD with subsampling still suffers from a large optimization variance.
Second, unlike SGD with subsampling, mini-batch SGD utilizes the information of all the data
samples to improve the performance of the model. Moreover, as we show in the following lemma,
mini-batch update substantially reduces the stochastic bias caused by the data dependence. In the
sequel, we denote F(w; x) := B Pξ∈χ F(w; ξ) as the average loss on a mini-batch of samples.
With a bit abuse of notation, we also define {Ft}t as the canonical filtration generated by the mini-
batch samples {xt}t.
Lemma 4.1. Let Assumption 2.1 hold and consider the mini-batch data stream {xt}t. Then, for any
W, v ∈ W measureable with regard to Ft and any τ ∈ N, it holds that
E [F(w； xt+τ ) 一 F(v； xt+T )∣Ft] 一 (f (W)- f (V)) ≤ GBR X φξ (TB + i)∙	⑺
6
Under review as a conference paper at ICLR 2022
With dependent data, the above lemma shows that we can approximate the population risk f (w)
by the conditional expectation E[F(w; xt+丁)|Ft], which involves the sample xt+丁 that is T steps
ahead of the filtration Ft . Intuitively, by the φ-mixing principle, as τ gets larger, the distribution of
χt+τ conditional on Ft gets closer to the stationary distribution μ. In general, the estimation bias
GBR PB=I φξ(TB + i) depends on both the batch size and the accumulated mixing coefficient over
the corresponding batch of samples. To provide a concrete understanding, below we calculate the
estimation bias for several different mixing models.
•	Geometric φ-mixing: In this case, PiB=1 φξ(TB + i) ≤ Pi∞=1 φξ(i) = O(1). Hence, the
estimation bias is in the order of O(GBR).
•	Fast algebraic φ-mixing (θ ≥ 1): In thiscase,PiB=1φξ(TB+i) ≤ Pi∞=1 φξ(i) = Oe(1).
Hence, the estimation bias is in the order of O(GBR), where O hides all logarithm factors.
•	Slow algebraic φ-mixing (0 < θ < 1): In this case, PiB=1 φξ(TB + i) ≤ O((T B)1-θ).
Hence, the estimation bias is in the order of O(GRTθ~^)∙
It can be seen that if the mixing coefficient converges fast, i.e., either geometrically or fast alge-
braically, then the data dependence has a negligible impact on the estimation error. Consequently,
choosing a large batch size can substantially reduce the estimation bias. On the other hand, when
the mixing coefficient converges slow algebraically, it substantially increases the estimation bias,
but it is still beneficial to use a large batch size. This result shows that mini-batch update can effec-
tively reduce the statistical bias of stochastic approximation for a wide spectrum of dependent data
generating processes.
We obtain prove the following convergence error bound for mini-batch SGD over dependent data.
Theorem 4.2. Let Assumption 2.1 and 3.1 hold. Apply mini-batch SGD to solve the stochastic
optimization problem (P) over φ-mixing dependent data and assume that it achieves regret Rn.
Then, for any T ∈ N and any minimizer w* with probability at least 1 一 δ, the averaged predictor
Wn := 1 En=I w(t) satisfies
f(wn)-f(w*) ≤ -n+
G(T - I)
n
n-τ+1
X κ(t) +
t=1
GR(τ 一 1)
n
+ OfFXφ(τB + i) + r/ʒlogjlog了(B 4
nB	nB δ δ
i=1	i=1
1	∖
. (8)
To further understand the order of the above bound, a standard regret analysis shows that mini-batch
SGD achieves a regret in the order of Rnn = O( JPjnBφj)) and κ(t) ≡ 0( ^Pn) (see Theorem C.3
for the proof). Consequently, the above convergence error bound reduces to the following bound,
where we hide all logarithm factors for simplicity of presentation.
f(wbn) 一 f(w*)
(9)
(10)
Such a bound further implies the following sample complexity results of mini-batch SGD under
different convergence rates of the mixing coefficient φξ .
Corollary 4.3. The sample complexity of mini-batch SGD in (6) for achieving an convergence
error over φ-mixing dependent data is given as follows.
• If the data is geometric φ-mixing with parameter θ > 0, then we choose T = 1, B
O(-1), n= O(-1). The overall sample complexity is nB = O(-2).
• If the data is fast algebraic φ-mixing with parameter θ ≥ 1, then we choose T = 1, B
O(-1), n= O(-1). The overall sample complexity is nB = O(-2).
7
Under review as a conference paper at ICLR 2022
•	If the data is slow algebraic φ-mixing with parameter 0 < θ < 1, then we choose τ =
1, B = O(e-1 ),n = O(e-1). The overall Sample complexity is nB = O(e-1-1).
It can be seen that mini-batch SGD can achieve an order-wise lower sample complexity than the SGD
with subsampling in the full spectrum of φ-mixing convergence rate. Specifically, mini-batch SGD
improves the sample complexity over that of SGD with subsampling by a factor of O ((log ɪ) θ),
O(e- 1) and O(e-1) for geometric φ-mixing, fast algebraic φ-mixing and slow algebraic φ-mixing
data samples, respectively. This shows that mini-batch update can effectively reduce the bias caused
by data dependence and leverage the full information of all the data samples to improve the learning
performance.
To intuitively explain, this is because with mini-batch updates, we can choose a sufficiently large
batch size B to reduce the bias caused by the data dependence and choose a small auxiliary parame-
ter τ = 1. As a comparison, to control the bias caused by data dependence, the standard SGD needs
to choose a very large τ and the SGD with subsampling needs to choose a large subsampling period
r that skips a huge amount of valuable data samples, especially when the mixing coefficient con-
verges slowly. Therefore, our result proves that it is beneficial to use mini-batch stochastic gradient
updates when the data samples are highly dependent.
We note that our proof of the tight high-probability bound in Theorem 4.2 for mini-batch SGD
involves substantial new developments compared with the proof of (Agarwal & Duchi, 2012). Next,
we elaborate on our technical novelty.
•	In (Agarwal & Duchi, 2012), they defined the following random variable
Xi := f(w((t-1)τ + i)) - f (w*) + F(w((t - 1)τ + i); ξt+τ-i) - F(w*; ξt+τ-i)∙
As this random variable involves only one sample ξt+τ-1, they bound the bias term
Xti - E[Xti|Fti-1] as a universal constant. As a comparison, the random variable Xti would
involve a mini-batch of samples xt+τ -1 in our analysis. With the mini-batch structure, the
bias Xti -E[Xti|Fti-1] can be written as an average ofB zero-mean dependent random vari-
ables, which is close to zero with high probability due to the concentration phenomenon.
Consequently, we are able to apply a Bernstein-type inequality developed in (Delyon et al.,
2009) for dependent stochastic process to obtain an improved bias bound from O(1) to
O (1 / √B). This is critical for obtaining the improved bound.
•	Second, with the improved high-probability bias bound mentioned above, the remaining
proof of (Agarwal & Duchi, 2012) no longer holds. Specifically, we can no longer apply
the Azuma’s inequality to bound the accumulated bias Pt(Xti -E[Xti|Fti-1]), as each bias
term is no longer bounded with probability one. To address this issue, we developed a gen-
eralized Azuma’s inequality for martingale differences in Lemma B.3 based on Proposition
34 of (Tao et al., 2015) for independent zero-mean random variables.
•	Third, we develop a high-probability regret bound for mini-batch SGD over dependent data
so that it can be integrated with the high-probability convergence bound in Theorem 4.2.
To our best knowledge, the regret of SGD over dependent data has not been studied before.
5	Numerical Example
We examine our theory via a basic convex quadratic optimization problem, which is written as
min f (w) := Eξ〜μ [(w — ξ)>A(w — ξ)],
w∈Rd
where A 占 0 is a fixed positive semi-definite matrix and μ is the uniform distribution on [0,1]d.
Then, following the construction in (Jarner & Roberts, 2002), we generate an algebraic φ-mixing
Markov chain that has the stationary distribution μ. In particular, its mixing coefficient φξ (k) con-
verges at a sublinear convergence rate k- 1, where r > 0 is a parameter that controls the speed of
convergence. Please refer to Appendix D for more details of the experiment setup.
We first estimate the following stochastic bias at the fixed origin point w = 0d .
(Bias): ∣E[F(w;xτ)∣xo = 0d] 一 f(w)∣,
8
Under review as a conference paper at ICLR 2022
where the expectation is taken over the randomness of the mini-batch of samples queried at time
τ ∈ N. Such a bias is affected by several factors, including the time gap τ, the batch size B and the
convergence rate parameter r of the mixing coefficient.
In Figure 1, we investigate the impact of these factors on the stochastic bias, and we use 10k Monte
Carlo samples to estimate the stochastic bias. The left two figures fix the batch size, and it can be
seen that the bias decreases as τ increases, which matches the definition of the φ-mixing property.
Also, a faster-mixing Markov chain (i.e., smaller r) leads to a smaller bias. In particular, with batch
size B = 1 and a slow-mixing chain r = 2, it takes an unacceptably large τ to achieve a relatively
small bias. This provides an empirical justification to Corollary 3.2 and explains why the standard
SGD suffers from a high sample complexity over highly dependent data. Moreover, as the batch
size gets larger, one can achieve a numerically smaller bias, which matches our Lemma 4.1. The
right two figures fix the convergence rate parameter of the mixing coefficient, and it can be seen
that increasing the batch size significantly reduces the bias. Consequently, instead of choosing a
large τ to reduce the bias, one can simply choose a large batch size B = 100 and set τ = 1. This
observation matches and justifies our theoretical results in Corollary 4.3.
Figure 1: Impact of τ , batch size B and convergence rate of mixing coefficient on the bias.
			
,			B = I 	B=IO 	B=IOO	
			
			
We further compare the convergence of SGD, SGD with sub-
sampling and mini-batch SGD. Here, we set r = 2 to generate
highly dependent data samples. We set learning rate η = 0.01
for both SGD and SGD with subsampling, and set learning rate
η = 0.01 X J PB=Bφξ()
= 0.01 × 1001/4 for mini-batch SGD with
batch size B = 100, as suggested by Theorem C.3 in the appendix.
The results are plotted in Figure 2, where each curve corresponds
to the mean of 100 independent trails. It can be seen that SGD with
subsampling achieves a lower loss than the standard SGD asymptot-
ically, due to the use of less dependent data. Moreover, mini-batch
SGD achieves the smallest asymptotic loss. All these observations
Figure 2: Comparison of sam-
ple complexity of different
SGD algorithms.
are consistent with our theoretical results.
6	Conclusion
In this study, we investigate the convergence property of SGD under various popular stochastic
update schemes over highly dependent data. Unlike the conventional i.i.d. data setting in which the
stochastic update schemes do not affect the sample complexity of SGD, the convergence of SGD
in the data-dependent setting critically depends on the structure of the stochastic update scheme. In
particular, we show that both data subsampling and mini-batch sampling can substantially improve
the sample complexity of SGD over highly dependent data. Our study takes one step forward toward
understanding the theoretical limits of stochastic optimization over dependent data, and it opens
many directions for future study. For example, it is interesting to further explore the impact of
algorithm structure on the sample complexity of stochastic reinforcement learning algorithms. Also,
it is important to develop advanced algorithm update schemes that can facilitate the convergence of
learning over highly dependent data.
References
Alekh Agarwal and John C Duchi. The generalization ability of online algorithms for dependent
data. IEEE TranSaCtionS on Information Theory, 59(1):573-587, 2012.
Alekh Agarwal, Nan Jiang, Kakade Sham M, and Wen Sun. Reinforcement learning: Theory and
algorithms. https:〃rltheorybook.github.io/, 2021.
9
Under review as a conference paper at ICLR 2022
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Yves Lechevallier
and Gilbert Saporta (eds.), Proc. COMPSTAT, pp.177-186, 2010.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, John-Paul Clarke, and Siva Theja Maguluri. Finite-
sample analysis of nonlinear stochastic approximation with applications in reinforcement learn-
ing. arXiv:1905.11425, 2019.
Gal Dalal, BaIazS Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with
function approximation. In Proc. AAAI conference on artificial intelligence, 2018.
Vianney Debavelaere, Stanley Durrleman, and Stephanie Allassonniere. On the convergence of
stochastic approximations under a subgeometric ergodic Markov dynamic. EIeCtroniC JournaI of
StatiStics, 15(1):1583 - 1609, 2021.
Bernard Delyon et al. Exponential inequalities for sums of weakly dependent variables. EIeCtroniC
JoUrnaI ofProbability, 14:752-779, 2009.
Randal Douc, Eric Moulines, Pierre Priouret, and Philippe Soulier. MarkoV chains. Springer, 2018.
John Duchi, Alekh Agarwal, Mikael Johansson, and Michael Jordan. Ergodic subgradient descent.
In Allerton Conference, 2011.
Eyal Even-Dar, Yishay Mansour, and Peter Bartlett. Learning rates for q-learning. JoUrnaI of
machine Iearning ReSearch, 5(1), 2003.
Peter W. Glynn and Sean P. Meyn. A Lyapunov bound for solutions of the Poisson equation. The
AnnalS ofProbability, 24(2):916 - 931, 1996.	—
Antoine Godichon-Baggioni, Nicklas Werge, and Olivier Wintenberger. Non-asymptotic analysis of
stochastic approximation algorithms for streaming data. arXiv:2109.07117, 2021.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. DeeP Learning. MIT Press, 2016.
Elad Hazan. IntrodUCtion to OnIine ConVeX Optimization. Elad Hazan, Erscheinungsort nicht er-
mittelbar, 2017. ISBN 1521133301.
S0ren F Jarner and Gareth O Roberts. Polynomial convergence rates of markov chains. The AnnalS
ofApplied Probability, 12(1):224-247, 2002.
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time
analysis of linear two-timescale stochastic approximation with markovian noise. In ConferenCe
on Learning Theory, pp. 2144-2203. PMLR, 2020.
Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of
biased stochastic approximation scheme. In Proc. ConferenCe on Learning Theory, pp. 1944-
1974, 2019.
Georgios Kotsalis, Guanghui Lan, and Tianjiao Li. Simple and optimal methods for stochastic
variational inequalities, ii: Markovian noise and policy evaluation in reinforcement learning.
arXiv:2011.02987, 11 2020.
Harshat Kumar, Alec Koppel, and Alejandro Ribeiro. On the sample complexity of actor-critic
method for reinforcement learning with function approximation. ArXiV:1910.08412, 2019.
Gen Li, Changxiao Cai, Yuxin Chen, Yuantao Gu, Yuting Wei, and Yuejie Chi. Tightening the depen-
dence on horizon in the sample complexity of q-learning. In ICML, volume 139 of Proceedings
OfMaChine Learning ReSearch, pp. 6296-6306. PMLR, 2021.
Kurt Marti. Stochastic optimization of regulators. ComPUterS and Structures, 180:40-51, February
2017.
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with
function approximation. In Proc. International ConferenCe on MaChine Learning, pp. 664-671,
2008.
10
Under review as a conference paper at ICLR 2022
Sean P Meyn and Richard L Tweedie. Markov Chains and Stochastic stability. Springer Science &
Business Media, 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv:1312.5602,
2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Dharmendra S Modha and Elias Masry. Minimum complexity regression estimation with weakly
dependent observations. IEEE TranSaCtionS on Information Theory, 42(6):2133-2145, 1996.
Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares re-
gression with markovian data: Fundamental limits and algorithms. In Proc. AdvanCeS in Neural
Information ProCeSSing Systems, volume 33, 2020.
Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On the finite-time convergence of actor-
critic algorithm. In NeUrIPS OPtimization FoUndationS for ReinforCement Learning WorkShop,
2019.
Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation
and q-learning. In Proc. ConferenCe on Learning Theory, pp. 3185-3205, 2020.
Ingo Steinwart and Andreas Christmann. Fast learning from non-iid observations. AdvanceS in
neural information ProCeSSing systems, 22:1768-1776, 2009.
Richard S. Sutton and Andrew G. Barto. ReinforCement Learning: An Introduction. Bradford, 2018.
Terence Tao, Van Vu, et al. Random matrices: universality of local spectral statistics of non-
hermitian matrices. AnnaIS of probability, 43(2):782-874, 2015.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. In International ConferenCe on Learning RePreSentations,
2019.
Yue Frank Wu, Weitong ZHANG, Pan Xu, and Quanquan Gu. A finite-time analysis of two
time-scale actor-critic methods. In Proc. AdvanCeS in Neural Information ProceSSing SyStemS
(NeurIPS), volume 33, pp. 17617-17628, 2020.
Lin Xiao. Dual averaging method for regularized stochastic learning and online optimization. Proc.
AdvanCeS in NeuraI Information ProCeSSing Systems, 22:2116-2124, 2009.
Pan Xu and Quanquan Gu. A finite-time analysis of q-learning with neural network function ap-
proximation. In Proc. International Conference on MaChine Learning, pp. 10555-10565, 2020.
Tengyu Xu, Shaofeng Zou, and Yingbin Liang. Two time-scale off-policy TD learning: Non-
asymptotic analysis over Markovian samples. In Proc. AdvanCeS in NeuraI Information
ProCeSSing SyStemS (NeUrIPS), 2019.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for (natural)
actor-critic algorithms. In Proc. AdvanceS in NeUraI Information ProceSSing SyStemS (NeUrIPS),
volume 33, 2020.
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. Provably global convergence of
actor-critic: A case for linear quadratic regulator with ergodic cost. In Proc. AdvanceS in NeUraI
Information ProCeSSing SyStemS (NeUrIPS), 2019.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with linear
function approximation. In Proc. AdvanceS in NeUraI Information ProCeSSing Systems, pp. 8665-
8675, 2019.
11
Under review as a conference paper at ICLR 2022
Appendix
Table	of Contents
A	Proof of Corollary 3.3	12
B	Proof of Theorem 4.2	13
B.1	Key Lemmas ................................................... 13
B.2	Proof of the Main Result ..................................... 17
C	Regret Analysis of Mini-Batch SGD	20
D	Experiment Setup	24
Notation: To simplify the notation, throughout the appendix, we denote ξt(i) := ξ(t-1)B+i, which
corresponds to the i-th data sample of the t-th mini-batch data xt . With this notation, we have
xt = {ξt(1) , ξt(2) , ..., ξt(B)}.
A Proof of Corollary 3.3
In this section, we analyze the convergence error bound of the SGD with data-subsampling in (4).
Given a φξ-mixing data stream {ξ1, ξ2, ξ3, . . . }, we consider the following subsampled data stream
{ξ1, ξr+1, ξ2r+1, . . . }.
Let F be the canonical filtration generated by {xt}. Then the subsampled data stream {ξtr+1}t is
φξr -mixing with the mixing coefficient given by
φrξ (t) = φξ(rt).
With this mixing coefficient, we can apply Theorem 2 of (Agarwal & Duchi, 2012) and obtain the
following convergence error bound for any τ ∈ N.
f(Wn)- f(w*) ≤ O(学 + (τ n 1) X K⑴+ T + r T log δ + φξ (rτ)).
t=1
Consider the standard SGD with a diminishing learning rate, We have κ(t) = O(√^) and Rn =
O(√n). Then, the convergence error bound becomes
f (Wn) - f(w*) ≤ O( √1n+TinN n (τ-r + JT Iog δ+φξ (rT)}).
The above result further implies the following sample complexity results for different convergence
rates of the mixing coefficient.
• Geometric φ-mixing: In this case, φξ(k) ≤ O(exp(-kθ)) for some θ > 0. Set the last
term φξ (rτ) = O(e). We obtain that rτ = O ((log ɪ) 1). Further set the second term
τ√1 = O(e). We obtain that nτ-2 = O(e-2). By choosing T = O(1), the sample
complexity is in the order of
e-complexity = r ∙ n
O
1
• Algebraic φ-mixing: In this case, φξ(k) ≤ O(k-θ) for some θ > 0. Set the last term
φξ(rτ) = O(e). We obtain that Tr = O(e- 1). Set the second term τ√n1 = O(e). We
obtain that nT-2 = O(e-2). By setting T = O(1), the sample complexity is in the order of
e-complexity = r ∙ n = O(e- 1 T2e_2) = O(e-2-θ).
12
Under review as a conference paper at ICLR 2022
B Proof of Theorem 4.2
B.1 Key Lemmas
In this subsection, we present several useful preliminary results for proving Theorem 4.2. Define
N := {1, 2, 3, . . . }. Throughout this subsection, we assume that Assumption 2.1 holds. The follow-
ing lemma is a generalization of the Lemma 1 in (Agarwal & Duchi, 2012).
Lemma B.1. Let w, v be measurable with respect to Ft. Then for any τ ∈ N,
E[F(w; xt+τ)- F(v; xt+τ)|Ft] ≤ GBR X φξ(TB + i) + f (W)- f (V).
i=1
Proof. For any τ ∈ N, we consider the following decomposition.
E[F(w; xt+τ) - F(v; xt+τ)F
=E[F(w; xt+τ) - f (w) + f (v) - F(v; Xt+τ)|Ft] + f (w) - f(v)
|
B X Z F(w; ξ(+τ)dP(ξ(+τ ∈ ∙∣Ft) - Z F(w; ξ)dμ] - hB X
i=1	i=1
(i)
t+τ
∈∙∣Ft) - / F(v； x)dμ]
^{z
(A)
+ f(w) - f(v).
We can further bound the term (A) using the mixing property of the dependent data stream.
(A) =hB X Z F(w； ξ(+τ)dP(ξ(+τ ∈ ∙∣Ft) - Z F(w； ξ)dμ] - hB X
i=1	i=1
(+)τ ∈ ∙lFt) - / F(v； x)dμ]
1B
=B E	(F(w； ξ) - F(v； ξ))d(P(ξ(+τ ∈ dξ∣Ft) - μ(dξ))
i=1
≤B X Z GRd∣P(ξ(+τ ∈ dξ∣Ft) - μ(dξ)∣
B i=1
≤GBR X Φξ(τB + i),
i=1
where in the first inequality We use the facts that F(∙； ξ) is G-LiPschitz and the domain is bounded
by R, and the second inequality is implied by the φ-mixing property. Substituting the above upper
bound of (A) into the Previous equation yields that
E[F(w； xt+τ)- F(v； xt+τ)lFt] ≤ GBR X φξ(TB + i) + f (W)- f (V).
i=1
This comPletes the Proof.
□
Proposition B.2. Let {w(t)}t∈N be the model parameter sequence generated by (6). Also suppose
that Assumption 3.1 holds. Then for any T ∈ N, we have
n
X[f(w(t))-f(w*)]
t=1
n	n-τ+1
≤ ^X[f (w(t)) -	F(w(t)； xt+τ-l) +	F(w*；	xt+τ-l) - f (w*)] +	Rn +	G(T	- 1)	^X	κ(t)	+ GR(T	- 1).
t=1	t=1
13
Under review as a conference paper at ICLR 2022
Proof. For any T ∈ N, we consider the following decomposition,
n
X[f(w(t))-f(w*)]
t=1
n
=Xf (w(t)) - F(w(t); xt+τ-1) + F(w*; xt+τ-1) - f (w*) + F(w(t); xt+τ-ι) - F(w*; xt+τ-ι)]
t=1
n
=X[f (w(t)) - F(w(t); xt+τ-1) + F(w*; xt+τ-1) - f (w*)]	(II)
t=1
n
+ X [F(w(t); xt+τ-1) - F(w*; xt+τ-1)].
t=1
X-----------------V------------------}
(B)
We will keep the first term and bound the term (B).
n
(B) = X F(w(t); xt+τ-1) - F(w*; xt+τ-1)
t=1
n	n-τ+1
X[F(w(t); Xt)- F(w*; xt)] + X [F(w(t); xt+τ-1)- F(w(t + T - 1); xt+τ-1)]
t=1	t=1
'-----------------V--------------'	'--------------------------------
(B1)	(B2)
n	T-1	T-1	n+τ-1
+XF(w(t); xt+τ-1)	-	X	F(w(t); xt)	+	X F(w*;	xt)	-	X F(w*;	Xt).
t=n-τ+2	t=1	t=1	t=n+1
'--------------------------V----------------------------'
(B3)
Recall that the term (B1) is the regret Rn. We can bound the term (B2) by noting that
F(w(t); xt+τ-1) - F(w(t + τ - 1); xt+τ-1) ≤ G∣∣w(t + T - 1) - w(t)∣∣
T-2
≤ G)： ∣∣w(t + i + 1) — w(t + i)k
i=0
T-2
≤ G X κ(t + i)
i=0
≤ G(τ — 1)κ(t).
For the term (B3), we can bound it using the G-Lipschitzness of F(∙; ξ) and the R-bounded domain.
n	T-1	T-1	n+τ-1
X	F(w(t); xt+τ-1) - X F(w(t); xt) + X F(w*; xt) - X F(w*; xt+τ-1)
t=n-τ +2	t=1	t=1	t=n+1
n	n+τ-1	τ-1	T -1
=XF(w(t); xt+τ-1)- X F (w*; xt+τ-1)] - [XF(w(t); Xt)- X F(w*; x/]
t=n-τ +2	t=n+1	t=1	t=1
n	τ 一 1
≤ g[ X	kw(t)-w*∣ + g[ X ∣w(t)-w*∣
t=n-τ +2	t=1
≤ GR(τ - 1).
14
Under review as a conference paper at ICLR 2022
Combining the above bounds of (B1), (B2), and (B3), we obtain the upper bound of (B) as follows.
n
X F(w(t); Xt+τ-ι) - F(w*; Xt+τ-ι)
t=1
n
n-τ +1
£[F(w(t); Xt)- F(w*; Xt)]+ E [F(w(t); xt+τ-ι) - F(w(t + T - 1); xt+τ-ι)]
t=1
、	{
(B1)
n
+ X	F(w(t)；
t=n-τ +2
t=1
}、
τ-1
{^^^^^^^^^^^^^^^≡
(B2)
τ	n+τ -1
xt+τ-i) - EF(w(t);Xt) + EF(w*；Xt) - E F(w*；Xt).
t=1
t=1
^{^^^^^^^^^^^™
(B3)
t=n+1
}
}
n-τ+1
≤ Rn+G(τ - 1) X κ(t) + GR(τ - 1).
t=1
Then the proof is completed by substituting the upper bound of (B) into (11).
□
The following generalized Azuma’s inequality generalizes the Proposition 34 of (Tao et al., 2015).
The inequality can be used to bound sum of martingale difference random variables.
Lemma B.3 (Generalized Azuma’s Inequality). Let {Xt} be a martingale difference sequence with
respect to its canonical filtration F. Define Y = PiT=1 Xi and assume E|Y | < ∞. Then for any
{αt}t > 0,
P (Y - EYI ≥ λuXα2j ≤ 2exp (-λ2) + XP(∣Xt∣ ≥ ɑt).
Proof. Let T := min{t : |Xt| > αt}. Then the sets Bt := {ω : T (ω) = t} are disjoint. Construct
γ0(ω) := (Y(S)	ifω ∈(Ut=i Bt),
.[e[Y |Bt] if ω ∈ Btforall t ∈ {1, 2,...,T}.
By the above construction, the associated Doob martingale of Y0 with respect to F is {Zt :=
Pit∧=T1 Xi}. It satisfies the conditions of Azuma’s inequality, i.e.,
•	{Zt } forms a martingale with respect to F (because the stopped martingale is still a mar-
tingale).
•	|Zt - Zt-1 | ≤ αt.
Then we can apply Azuma’s inequality to Y0 .
P
|Y0 -EY0| ≥ λ
≤ 2 exp
15
Under review as a conference paper at ICLR 2022
Now We can bound P (∣Y - EY∣ ≥ λʌ/PTToF) as follows.
p (∣γ - EY ι≥ λJX a
T
T
=P |Y -EY| ≥λ	αt2, Y =Y0	+P |Y -EY| ≥λ	αt2, Y 6=Y0
t=1
t=1
≤P (∣Y0 - EY0| ≥ xJX a2)+ P (Y = Y0)
λ2	T
≤2exp (-—J + 工P(IXt| ≥ αt).
Then the proof is completed. Here We notice the fact that EY0 = EY by our construction.	□
The following lemma is taken from (22), Theorem 4 of (Delyon et al., 2009).
Lemma B.4 (Bernstein’s Inequality for Dependent Process). Let {Zt} be a centered adaptive pro-
cess with respect to F. Define the following quantities.
n k-1
q=Xi * * * * * * * * XX kZik∞ ∙kE[Zk∣Fi]k∞,
k=1 i=1
v=X	kE[Zk2|Zk-1,...,Z1]k∞,
k
m = sup kZi k∞ .
1≤i≤n
Then, it holds that
n	t2
P(X Zi ≥ t) ≤ exp (-2(v + 2q) + 2tm∕3) .
Application of Lemma B.4 to our proof. Here we make some comments about how to apply this
inequality in our main proof. We define the following random variable in our proof. Throughout, we
use the batch-level filtration F and the intra-batch level filtration Fb. The formal definition is given
in Section B.2.
Xi = f (w((t - 1)τ + 1)) - f (w*) + F(w*; xtτ+i-1)- F(w((t - 1)τ + 1); xtτ+i-i)∙
We also define the filtration Fti := Ftτ+i-1 for simplicity. Then, we have
E[Xi∣Fi-ι] = f (w((t - 1)τ + 1)) - f (w*) + E [F (w*; xtτ+i-ι) - F(w((t - 1)τ + 1); xtτ+i-ι) E-J
Then, the bias can be rewritten as
Xti -E[Xti|Fti-1]
=F(w*; xtτ+i-ι) - F(w((t - 1)τ + 1); xtτ+i-ι) - E[F(w*; xtτ+i-ι) - F(w((t - 1)τ + 1); xtτ+i-ι)IF-ι]
1
=B ∑	Yi(ξ),
ξ∈xtτ+i-1
where Yti is defined as
Yi(ξ) = F(w*; ξ) - F(w((t - 1)τ + 1); ξ) - E[F(w*; ξ) - F(w((t - 1)τ + 1); ξ) IFi-J
16
Under review as a conference paper at ICLR 2022
More specifically, we have
1	1B
Xi- EXiFi-1] = B E	Yi(ξ) = B EYi(ξj+i-ι).
ξ∈xtτ+i-1	j=1
Recall that Fb is the canonical filtration generated from the data stream (12). Moreover,
{Yti(ξt(τj)+i-1)}j=1,2,...,B is centered and adaptive with respect to this filtration. Then we can evalu-
ate the quantities q, v, and m in Lemma B.4 as follows.
•	Bounding m is simple. By Assumption 2.1 we have kYti(ξt(τj)+i-1)k ≤ 2GR.
•	The above bound of m leads to a simple bound for v, i.e., v ≤ 2nG2R2.
•	The quantity q can be bounded as follows.
n k-1
q := XX kYi(ξjH-I)k∞kE[Yi(ξ(T+i-ι)IFtj+i-ι]k∞
k=1 j=1
n k-1
≤ 2GRXX kEYi(ξ(T+i-ι)Ft(T+i-ι]k∞
k=1j=1
n k-1
=2GRXX kE[Yti(ξ(T+i-ι)Ftj+i-ι] - Eξ~μYi(ξ(k+i-ι)k
k=1j=1
n k-1
≤4G2R2XXφξ(k-i)
k=1 i=1
n
≤ 4G2R2n X φξ(i).
i=1
Then, by applying Lemma B.4, we obtain the following high-probability bound.
≤ 2 exp
= 2 exp
B2t2
P (Xi- E[Xi∣Fi-ι]∣ ≥ t) ≤ 2exp -	以
-	2(v + 2q) + 2Btm/3
_________________B2t2___________________
2(2G2R2B + 8G2R2B P= φξ(i)) + 4GRBt∕3
Bt2	!
2(2G2R2 + 8G2R2 PB=I φξ(i))+4GRt∕3	'
Simplifying yields that
Bt2
P (Xi - E[Xi∣Fi_1]∣ ≥ t) ≤ 2exp--------------------------后------
' Ltlt-I	c + 4GRt +16G2R2 P= φξ⑺
where C := 4G2R2 .
B.2 Proof of the Main Result
Recall that we are considering a data stream divided into small mini-batches. For convenience, we
re-label the data stream {ξ1, ξ2, ξ4, . . . } as follows to explicitly indicate its mini-batch index.
{ξ(1) ξ(2) ξ(B) ξ(1) ξ(2) ξ(B)	}	(12)
{ξ1 , ξ1 , . . . , ξ1 , ξ2 , ξ2 , . . . , ξ2 , . . . }.	(12)
The canonical filtration generated by the re-labeled data
stream is denoted by Fb. Also, when the
batch size is clear in the context, we denote the data in the specified mini-batch as x. For example,
17
Under review as a conference paper at ICLR 2022
we use xt to represent the t-th mini-batch {ξt(1) , ξt(2) , . . . , ξt(B) }. Then we can re-writhe the above
data stream as
{x1,x2,x3... }.
We denote the canonical filtration generated by the above sequence as F. Note that we have the
following relation:
Ft = Fbt(B) .
In summary, when we analyze the mini-batch SGD dynamics, we use the filtration F, and when we
need to consider intra-batch samples, we use the filtration Fb.
Theorem B.5. Let {w(t)}t∈N be the model parameter sequence generated by (6). Suppose Assump-
tions 2.1 and 3.1 hold. Then, for any τ ∈ N, with probability at least 1 - δ, we have
n
X[f(w(t))-f(w*)]
t=1
B
≤GRB X φξ(TB +i)
B i=1
uu 2τn	2 GR	4n	uu 4 G2R2	4n	B	4n	4τ	4n
+ t -B ∙ (3-Blog至 + t 9-B-(log丁)2 + (4G2R2 + 16G2R2 £奴(i))log丁) ∙ log至log~y
n-τ +1
+ Rn + G(τ - 1) X κ(t) + GR(τ - 1).
t=1
In particular, if τ = 1, then
n
X[f(w(t))-f(w*)]
t=1
B
≤Rn + GRB ^X φξ(B + i)
B i=1
uu 2n 2 GR	4n	uu 4 G2R2	4n	B	4n	4τ	4n
+ t B ∙ (3 -B log 于 + t 9 -B-(log 丁)2 + (4G2R2 + 16G2R2 £公(i)) log 丁) ∙ log 丁 log 了.
Proof. From Proposition B.2, we obtain the following bound.
n
X[f(w(t))-f(w*)]
t=1
n	n-τ +1
≤ X[f (w(t)) - F(w(t); xt+τ-1) + F(w*; xt+τ-1) - f (w*)] + Rn + G(τ - 1) X κ(t) + GR(T - 1).
t=1	t=1
To complete the proof, it suffices to bound the first term; we define this term as
n
Zn := X[f (w(t)) - F(w(t); Xt+τ-l) + F(w*; Xt+τ-l) - f (w*)].
t=1
We apply the same decomposition as the (13) of (Agarwal & Duchi, 2012). Define the index set
I(i) as {1,..., [TC + 1} for i ≤ n - T[TC and {1,..., [nC} otherwise. Then We have
ττ
Zn=X X [Xti - E[Xti|Fti-1]] + X X E[Xti|Fti-1],
i=1 t∈I(i)	i=1 t∈I(i)
18
Under review as a conference paper at ICLR 2022
where
Xt = f (w((t - I)T + I)) ― f (w*) + F(w*; xtτ+i-1)- F(w((t ― I)T + I); xtτ+i-l).
Note that by Lemma 4.1,we have that E[Xi∣Fi-J ≤ GR PB=I Φξ (TB + i). Then, wehave
B
τ
nGR
P (Zn > F
X φξ(TB +	i)	+	Y)	≤	P(X	X	Xi	- EXiFi-1]] > Y)
i=1
i1 t∈I(i)
τ
≤ p([{ X [Xi -EXiFi-1]]> γ]
τ
≤ Xp( X [Xi - E[XiFi-ι]]>T)∙
i1	t∈I(i)
Define Y := Pt∈i(i)[Xi 一 E[Xii∣Fi-ι]] and α := √=. Notice that Xi — E[Xi∣Fi-J is a centered
random variable, that is, E[Xti - E[Xti|Fti-1]] = 0. Then by the generalized Azuma’s inequality
(Lemma B.3), we conclude that
n
P(Y ≥ Y) ≤ 2exp (-2t⅛2) + XP(IXi- EXiFi-1]∣ ≥ α).
∖	T) t=1
The second term can be bounded by using the generalized Bernstein’s inequality. The detailed
calculation can be found in the discussion after Lemma B.4. We obtain that
P (Xi- E[Xi∣Fi-i]∣ ≥ α) ≤ 2exp
λ2
C + 3GR√B + 16G2R2 PB==1 φξ(i),
—
where C = 4G2R2. In summary, the concentration bound for Zn is
P(Zn >GRB X φξ (TB + i)+ Y)
i
n
≤ 2t exp (-2tJ ) + t X P(Xi - E[Xi∣FLι ]| ≥ α)
∖	T OL) t=1
≤ 2T exp -
+ 2n exp -
λ2
C + 4GR√λB + 16G2R2 PB=I φξ⑴
Then, let 2 = 2n exp ( 一
C+ 3GR√B + 16G2R2 PB= φξ(i)
, and we obtain that
λ2 = (C + 3gr√= + 16G2R2 X φξ (i)) ∙ log 了.
i=1
It is a quadratic function of λ. Solving it yields that
2 GR	4n u
3 ɪlog E + t
4G2R2
9 B
B	4n
+ 16G2R2 X φξ(i)) logɪ.	(13)
i=1
Also, let 2
2t exp (----J■ I, we have that
2τn ɪ )
Y2 = 2τnλ- ∙ log4τ.
Bδ
Substituting (13) into the above equation, we obtain that
Y=t
2Tn 2 GR 4n u
ɪ ∙(3 万log 于 + t
9 GBR- (log4n) + (C + 16G2R2 X φξ (i)) log 4n) ∙ log4T log4n.
i=1
λ
19
Under review as a conference paper at ICLR 2022
Then, we conclude that with probability at least 1 - δ,
n
X[f(w(t))-f(w*)]
t=1
B
≤GRB X φξ(TB +i)
i=1
+t 2Bn ∙ (IGR log4n+t I GBR2 (log 4n )2+(4G2R2+16G2R2 X φξ(iDlog 4n) ∙ log 4τ log 4n
i=1
n-τ +1
+ Rn + G(τ - 1) X κ(t) + GR(τ - 1).	(14)
t=1
The desired result follows by noting that Pn=I f (w(t)) ≥ nf (Wn).	□
C Regret Analysis of Mini-Batch SGD
In this section, we derive the regret bound of mini-batch SGD algorithm. Throughout, for each sam-
Ple loss F(w; ξ), recall that its gradient kVF(w; ξ)k is uniformly bounded by G (see Assumption
2.1). In particular, We assume the k-th coordinate of VF(w; ξ) is uniformly bounded by Gk, and
We have G = PPk Gk.
1.	Gradient Variance Bound under Dependent Data
In the i.i.d. setting, the variance of stochastic gradient decreases as the batch size increases. Specifi-
cally, We have
1	B	1 B	2G2
Ek B Σ VF(w; ξi) — Vf (w)k2 = B EEkVF(w; ξi) - Vf (w)k2 ≤ -B-.
i=1	i=1
Therefore, Ek B P3 VF(w;ξi) - Vf(w)k2 = O(B). However, this bound no longer holds if
the data samples are dependent. In the folloWing lemma, We develop a similar result When the data
is collected from a dependent stochastic process. Recall that VF (w(t); Xt) denotes the averaged
gradient over the mini-batch Xt, i.e.,
1B
VF(w(t); Xt) = B EF(w(t); ξ(i)).
i=1
Lemma C.1. Let {w(t)}t∈N be the model parameter sequence generated by the mini-batch SGD in
(6). Let Assumptions 2.1 and 3.1 hold. Then, with probability at least 1 - δ,
kVF(w(t); Xt)-Vf (w(t))k2 ≤ h268 G2 + 256G2 X φξ(j)] ∙ lθB2d +2G2
j=1
2
i)
Proof. Let Xt = {ξt(i)}iB=1 be the t-th mini-batch samples. We consider the filtration within Xt and
denote it as {Fbt(i)}. Then, by the definition of canonical filtration,
Xi := VF(w(t); ξ(i))
is measurable with respect to Fbt(i). Define
Yi,k := (Xi -E[Xi|Ft-1])k
where (∙)k denotes the k-th entry of the specified vector. And it is easy to see that {Yi,fc}i is a
centered process for any k ∈ {1, 2, . . . , d}. With these construction, we start from the following
20
Under review as a conference paper at ICLR 2022
decomposition.
kVF (w(t); Xt)-Vf (w(t))k2
= ∣∣VF(w(t); Xt)- E[VF(w(t); xt)∣Ft-ι]+ E[VF(w(t); Xt)∣Ft-ι] -Vf (w(t))k2
≤2 kVF (w(t); xt) - E[VF (w(t); xt)|Ft-1]k2 +2 kE[VF (w(t); xt)|Ft-1] - Vf (w(t))k2.
×-------------------V--------------------} X-------------------V------------------}
(A)	(B)
Then we will bound the term (A) and (B), respectively.
• Bounding (A): Note that
kVF (w(t); Xt) -
2
Then, we show that the process {Yi,k}i satisfies the conditions of Lemma B.4.
◦
◦
◦
Since E[Yi,k|Ft-1] = 0, we conclude that {Yi,k}i is a centered process.
Denote the k-th entry of Xi as Xi,k. We know that |Xi,k| ≤ Gk. Hence, we conclude
that 0 ≤ |Yi,k| ≤ 2Gk. Then, we can set bi = 2Gk for all i.
Lastly, we can bound the quantity q defined in Lemma B.4 as follows.
B j-1	4
q ≤ 2Gk∑EkE[Yj,k∣F(i)]k + -GkB
j=1 i=1
Bj -1	-
≤ -Gk	ΣSφξj - i) + 3 Gk B
B-
≤ -GkB X φξ j) + 3GkB.
j =1	3
Now, we can apply Lemma B.4 and obtain that
P (X Yi,k > j ≤ exp 1 134GkB +128GkB P= φξ(j)).
With a union bound, we obtain that
P (IX Yi,ki>λ! ≤ 2exp (-否b+而!BP=^!.
Further applying the union bound over k = 1, 2, . . . , d, we obtain that
P ([ι niX Yi,k|2 >λ2}) ≤ 2 X exp (- 134 Gk B + 128Gk B P= φξ j
Let d = 2 exp (- 134 GkB + 128GkB PB=ι φξ(j) ) , We obtain that
λk = h-ɪGkB + l28GkBXφξj)i ∙ log ɪ.
j=1
21
Under review as a conference paper at ICLR 2022
Then we conclude that,
P f \ n∣ X Yi,k∣2 * * * * * ≤ h ^3- Gk B + 128GkB X φξ Cj)i ∙ log ɪ }] ≥ 1 - δ∙
k=1 i	j=1
It implies that with the probability at least 1 - δ,
X∣XYi,k∣2 ≤ h^3^B(XGk) + 128B(Xφξ(j))(XGk)i ∙logɪ.
k i	k	j=1	k
By definition, G =，Pk Gk. Finally, We have the following bound for term (A): with
probability at least 1 - δ,
C r 134 C	C^B l log 2d
∣∣VF(w(t); Xt)- E[VF(w(t); xt)∣Ft-1]k2 ≤ [ɪG2 + 128G2 £ φξ(j)] ∙ -ɪɪ.
j=1
• Bounding (B): Note that
kE[VF(w(t); ξ(i))∣Ft-ι] -Vf (w(t))k = (j Z VF(w(t); ξ(i))dP(ξ(i) ∈ ∙∣Ft-ι) - Z VF(w(t); ξ)dμ(ξ)∣∣
≤ / kVF(w(t); ξ(i))k∣dP(ξ(i) ∈∙∣Ft-ι) - dμ∣
≤ G ∙ Φξ(i).
Then we bound the norm by triangle inequality,
1
kE[VF (w(t); xt)∣Ft-ι] -Vf (w(t))k ≤ B EkE[VF (w(t); ξ(i))∣Ft-ι] -Vf (w(t))k
B i=1
≤ GX φξ (i).
i=1
Finally, we obtain the bound for the term (B) as
kE[VF (w(t); xt)|Ft-1] - Vf(w(t))k2 ≤G2
Combing the bounds of (A) and (B) yields that with probability at least 1 - δ,
C r268 C	c <B	i log 2d	C
kVF(w(t);Xt)-Vf(w(t))k2 ≤ [-3-G2 + 256G2Eφξ(j)] ∙ -ɪɪ +2G2
j=1
□
2. High-Probability Regret Bound
To derive the regret bound for the mini-batch SGD algorithm, we make the following additional
mild assumption.
Assumption C.2. The stochastic optimization problem (P) satisfies
1. Each sample loss F(∙; ξ) : W → R is convex.
2. The objective function f : W → R is L-smooth.
22
Under review as a conference paper at ICLR 2022
Theorem C.3 (High-probability regret bound). Let {w(t)}t∈N be the model parameter sequence
generated by the mini-batch SGD in (6). Suppose Assumptions C.2, 3.1 and 2.1 hold. Then, with
probability at least 1 - δ,
RT ≤ kw「2 + ηT h(238G + 256G2 X φξ (j)) " + 2G2 (j^) 2i
η	j=1
T
+ 2ηL X(f(w(t))- f(w*)).
t=1
Moreover, let η = O( ,TρsB~φj), the optimized upper bound is in the order of
R
T EB φξO)) + 2ηLXX f(w(t)) - f(w*)).
t=1
Proof. For convenience, We define gt = B PB=I VF(w(t); ξ(i)). By the algorithm update (6), We
obtain that
2hgt,w(t)-w*i Jw(t)-w*k2-kw(t + 1)-w*k2 + ηkgtk2
η
≤ kw(t)-w*k2-kw(t + 1)-w*『+ 2η∣gt - Vf(w(t))k2 + 2ηkVf(w(t))∣∣2.
η
Summing the above inequality over t yields that
T
2 Xhgt,w(t)- w*i
t=1
≤kw ⑴一w*∣∣2-∣∣w(T + 1)-w*∣∣2 +2η X kgt - Vf (w(t))k2 + 4ηL XX (f (w(t)) - f (w*)).
η	t=1	t=1
By convexity of the function, We further obtain that
工	IlW(I)—w*H2 ɪ	C ɪ
2 X(F(w(t); Xt)- F(w*; Xt)) ≤ k-(-)——L +2ηX ∣gt -Vf (w(t))∣2 + 4ηL£(f (w(t)) - f(w*)).
t=1	η	t=1	t=1
Then, We apply Lemma C.1 to bound the second term PtT=1 kgt - Vf (w(t))k2 and then apply a
union bound on over t. We conclude that, With probability at least - - δ,
T
X(F(w(t); Xt)- F(w*; Xt))
t=1
∣∣w(1) - w*Il2	268 9	2 昌 z ʌ log 2dT	JPBl φξ(i)∖2]
≤k-(j2—-L + ηT ∙ [ (-3-G2 + 256G2 A φξ(j))	+ 2G2 (UBφξ( ) J
T
+ 2ηLX(f (w(t)) - f (w*)).
t=1
The proof is completed. Lastly, We set the learning rate η. To minimize the obtained upper bound, it
suffices to minimize the first tWo terms, as the last term can be combined With the left hand side of
(14) When We apply this regret bound. The optimized learning rate is achieved When
= ηT ∙ [(2∣8G2 + 256G2 x φξ(j))吟 + 2G2 (P⅛½) 2 i.
23
Under review as a conference paper at ICLR 2022
Then, η is chosen as
________________kw⑴-w*k2∕2
t T ∙ [(268 G2 + 256G2 PB=i φξ(j)) logB2dτ +2G2 (P=BMi)2 ]
s
O
T ∙
B
PB=I Φξ j)
□
D Experiment Setup
Recall that we consider the following convex quadratic optimization problem:
min Eξ〜μ(w - ξ)TA(W - ξ),
w∈Rd
where A is a fixed positive semi-definite matrix and μ is the uniform distribution on [0,1]d. The data
stream admitting such a stationary distribution μ can be generated by a certain MetroPoiiS-HaStingS
sampler provided in (Jarner & Roberts, 2002). Specifically, it is described as follows.
Step 1: Let the “proposal” distribution q(x) have the density of Beta(r + 1, 1); that is,
(r + 1)xr x ∈ [0, 1]
q(x) = 0	x∈∕ [0, 1] .
Define the acceptance probability α(x, y) = min{q(x), 1}.
Step 2: If the current state is ξt, then We sample Z 〜q. Define the next state ξt+ι:
ξ = ξt w.p. 1 - α(ξt,ζ),
t+1	ζ w.p. α(ξt, ζ).
Step 3: Go back to Step 2 to generate the next state.
We repeatedly generate d independent sequences starting from the same initial state s0 = 0 to obtain
a d-dimension Markov chain. It has been shown that the above generated Markov chain converges to
μ in distribution with an algebraic convergence rate φξ(k) ≤ O(k-1/r) in Proposition 5.2, (Jarner
& Roberts, 2002).
We consider the following bias term at the fixed point w = 0d .
(Bias):	∣E[F(w;xτ)∣xo = 0d] - f(w)∣.
It can be used to approximate the left-hand side of Lemma 4.1. Since E [F(w; xT)∣so = 0d] cannot
be explicitly obtained, we use Monte Carlo method to estimate this conditional expectation. That
is, we generate n = 10, 000 independent trajectories starting from x0 = 0d. At the step τ, we
estimate the expected value as n PZi F(w； x(i)), where XT with the superscript (i) indicates that
it is sampled from the i-th trajectory. Then we investigate the relation between the step τ and the
mixing parameter r and the relation between the step τ and the batch size B . All the results are
presented in Section 5.
24