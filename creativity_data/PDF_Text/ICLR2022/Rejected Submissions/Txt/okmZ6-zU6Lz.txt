Under review as a conference paper at ICLR 2022
Quantifying the controllability of coarsely
CHARACTERIZED NETWORKED DYNAMICAL SYSTEMS
Anonymous authors
Paper under double-blind review
Ab stract
We study the controllability of large-scale networked dynamical systems when
complete knowledge of network structure is unavailable. In particular, we establish
the power of learning community-based representations to understand the ability
of a group of control nodes to steer the network to a target state. We are moti-
vated by abundant real-world examples, ranging from power and water systems to
brain networks, in which practitioners do not have access to fine-scale knowledge
of the network. Rather, knowledge is limited to coarse summaries of network
structure. Existing work on "model order reduction" starts with full knowledge
of fine-scale structure and derives a coarse-scale (lower-dimensional) model that
well-approximates the fine-scale system. In contrast, in this paper the controlla-
bility aspects of the coarse system are derived from coarse summaries without
knowledge of the fine-scale structure. We study under what conditions measures of
controllability for the (unobserved) fine-scale system can be well approximated by
measures of controllability derived from the (observed) coarse-scale system. To
accomplish this, we require knowledge of some inherent parametric structure of the
fine-scale system that makes this type of inverse problem feasible. To this end, we
assume that the underlying fine-scale network is generated by the stochastic block
model (SBM) often studied in community detection. We quantify controllability
using the “average controllability” metric and bound the difference between the
controllability of the fine-scale system and that of the coarse-scale system. Our
analysis indicates the necessity of underlying structure to make possible the learn-
ing of community-based representations, and to be able to quantify accurately the
controllability of coarsely characterized networked dynamical systems.
1	Introduction
In this paper we study controllability for networked dynamical systems when our knowledge of
system structure is limited to coarse summaries. We are motivated by myriad real-world settings
where system identification must be performed based upon measurements taken by low-resolution
instruments unable to probe fine-scale structure. Our motivating example is the human brain. While
efforts are under way to produce a canonical human brain map, our knowledge of the brain as an
interconnected, network system is not yet to the level of the whole-brain individual neuron (Betzel
and Bassett, 2017). And yet, motivated by emerging medical technologies, there are important control
tasks we would like to tackle. For example, novel brain implants designed for epilepsy patients aim
to “steer” the brain away from states that correspond to seizures (Heck et al., 2014; Muldoon et al.,
2016). Our goal is to quantify the controllability of a fine-scale networked dynamical system given
access only to coarse knowledge of network structure. Generally, without parametric structure, this
is impossible. But real networks do have structure and so in our model we assume the fine-scale
network has a connectivity induced by an underlying stochastic block model (SBM).
Approximation of high-dimensional (fine-scale) dynamical systems by lower-dimensional (coarse-
scale) ones is known as “model order reduction” (MOR) in the controls literature. There is a key
difference in assumptions that differentiate our setting from that literature. In MOR the starting point
is a complete description of the high-dimensional system. The task is to formulate a lower-dimension
system, the dynamics of which well-approximate those of the full system. In contrast, we start from
coarse summaries of the fine-scale system. We do not have access to the fine-scale dynamics and
must exploit parametric knowledge (via the assumption of a generative SBM). One might think of the
1
Under review as a conference paper at ICLR 2022
distinction as akin to “active” versus “passive” MOR. Traditional MOR is active in that it actively
decides how to coarsen the system to yield the best reduction. But for us, our knowledge is limited by
the precision of our instrumental observations, so passively collected data is our starting point.
Controllability is a function both of system dynamics and how we actuate the system (Pasqualetti
et al., 2014; Yuan et al., 2013). Herein we assume that we both measure and actuate a system only
coarsely. A control question we study is which coarse-level actuations are most “influential” in
controlling the underlying fine-scale system. Such knowledge can assist with actuation selection;
e.g., in our motivating epilepsy application, where best to position devices to be able to collapse the
unstable brain-state oscillations that lead to seizures. (O’Leary et al., 2018; Pazhouhandeh et al.,
2019; Kassiri et al., 2017; Shulyzki et al., 2015) To accomplish our goal we characterize the average
controllability of a vector of systems, each corresponding to a different coarse-scale actuation input.
By comparing these vectors, and because these vectors well approximate the corresponding vectors
for the fine-scale system, we aim (in the long term) to produce clinically-usable information for the
neurologist.
Contribution: Our work is the first of its kind that proposes a learning-based framework for inferring
the controllability of fine networks from coarse measurements, and characterizes the mismatch
between the controllability of the coarse and fine-scale networks. We study two approaches.
1.	In Section 5, we build from MOR. We define an auxiliary, fictitious, reduced-order system
based on the coarse data, and use the average controllability vector of this system to approx-
imate that of the fine-scale system. We derive a tight upper bound on the “approximation-
error” which is the sum of two terms. One term goes to zero as the coarse network size
increases and the network becomes dense. The second term is a function of the synchro-
nization between the coarse summary data and the underlying community structure. If
synchronization is not sufficiently high, this term may not approach zero even as the network
size increases.
2.	In Section 6 we learn the fine-scale system’s average controllability vector directly from the
coarse data. This learning-based algorithm builds on the mixed-membership algorithm of
Mao et al. (2017) for unsupervised learning of the parameters and the community structure of
a SBM. We derive a tight upper bound on estimation error and characterize its convergence.
Although the error bound implicitly depends on synchronization, unlike in the MOR-based
approach, the error of this approach converges to zero as the coarse network size and its
density increases.
2	Background / Related work
Coarsened SBM as a generative process: The study of extracting community structure from coarse
summaries is recent. The authors in (Ghoroghchian et al., 2021) used the stochastic block model
(SBM), developed in the community detection literature (Abbe, 2017), to lay out a framework for a
coarsened and weighted variant of the SBM. We build off those results in this paper. The structure of
many real-world networks, including brain networks is, at least empirically, known to have community
structure across various spatial scales (SPorns and Betzel, 2016; Pavlovic et al., 2020). The SBM and
its variants provide a powerful modeling framework to facilitate fundamental understanding of graph
community organization and have found applications in many domains, including social and power
networks. (Dulac et al., 2020; Funke and Becker, 2019; Abbe, 2017).
Complex networks controllability: The development of control methods for complex networks is a
major effort in network science (Scheid et al., 2020). Coupling traditional notions of controllability
with graph theory reveals several insights into the role of network structure (e.g., presence of
communities, diameter, and sparsity), size, and edge weight strength in controlling large-scale
networks (Wu-Yan et al., 2018; Kim et al., 2018; Constantino et al., 2019; Sun, 2015). Further one
may want to understand which group of nodes, when actuated as inputs, can be used to steer the
network to an arbitrary target state, and at what cost (Cortesi et al., 2014; Gu et al., 2015). Recent
works in network neuroscience Gu et al. (2015) have popularized the notion of average controllability.
This scalar metric associate a measure the relative control influence of a group of nodes. In this paper
we consider a vector of such scalar measures to study the comparative influence of different sets of
nodes. To the best of our knowledge ours is the first work that characterizes this type of error bounds
for the controllability of coarse graphs.
2
Under review as a conference paper at ICLR 2022
3	Preliminary Notions
Notation: We denote vectors and matrices using bold faced small and upper case letters. The n
dimensional all-ones and -zero vectors are denoted by 1n and 0n. For M = [Muv] ∈ Rn×m, define
∣∣Mk∞ = maxι≤u≤n Pm=I |Muv|; IlMkmax = maxu,v |Muv|； and ∣∣M∣∣2 = Pxmx(MTMy Let
m = n, then define the spectral radius by ρ(M) = maxi{∣λ∕}; diag(M) = [M11,..., Mnn]t ∈
Rn; and Diag(M) sets the off-diagonal entries of M to zero. For matrices M0is with arbitrary
dimensions, BlkDiag(M1, . . . , Md) denotes the block diagonal matrix. The inequality M1 ≤ M2
implies element wise inequality. We write f(n) = O(h(n)) iff there exist positive reals c0 and n0
such that |f (n)| ≤ c0h(n) for all n ≥ n0. The support of a vector, supp(m), is the set of indices i
such that mi 6= 0. The cardinality of a set V is denoted by |V |. For a positive integer m, we denote
[m] , {1, . . . , m}. 1(m) returns a vector of same size with non-zero replaced by 1.
Networks: A network is defined by an un-directed graph G , (V, E), where the node set V ,
{1, . . . , n} and edge set E ⊆ V × V. For an edge (u, v) ∈ E, assign the weight Auv = Avu ∈ R,
and define the weighted symmetric adjacency matrix of G as A , [Auv], where Auv = Auv = 0
whenever Auv ∈/ E . A random network is an un-directed graph with a random adjacency matrix.
3.1	Linear dynamical system on random network
For a network G with n nodes and the symmetric adjacency matrix A, associate a state xi [k] ∈ R to
the i-th node, and let the nodes evolve with the linear and time-invariant (LTI) dynamics 1:
x[t +1]=——t^ʌa Ax[t] + Bu[t],	∀ t = 0,1,....	(1)
The state x[t] = [x1 [t], . . . , xn[t]]T is steered to an arbitrary value by an input u[t] ∈ Rn. Here, the
input matrix B = Diag(b) ∈ Rn×n, where b ∈ {0, 1}n determines which components of u[t] enters
the network2. For e.g., for B = Diag(1n1 , 0n-n1 ), the input enters the network through control
nodes set K = {1,..., nJ. The normalization C ∙ tr(A) factor, with appropriately chosen constant
c > 0, ensures that system in Eq. 1 is asymptotically stable. Finally, we define Anom，。心)A for
the normalized matrix, and use this convention throughout the paper.
For fixed system matrix Anom, a necessary and sufficient condition for the asymptotic stability3 of
Eq. 1 is that ρ(Anom) ≤ 1. For random Anom, we consider the probabilistic stability: P[ρ(Anom) ≤
1]—the greater the value, the greater the chance that Anom is stable. For SBM generated random
symmetric matrices, we provide sharp non-asymptotic lower bounds on P[ρ(Anom) ≤ 1].
The networked LTI system in Eq. 1 is T-step controllable if x[0] = 0 can be steered to any target
state x ∈ Rn for some inputs: u[0], . . . , u[T - 1]. The T -step controllability Gramian of Eq. 1 given
below, among other things, allows us to study if Eq. 1 is controllable or not.
CT (Anom, B) =PtT=-01(Anom)tBBT(Anom)t.	(2)
By definition CT (Anom, B) 0, and it is well known that G with n nodes is T -step controllable if
n-step controllable; or equivalently, CT (Anom, B) 0. For other interesting properties of Eq. 2 we
refer to (Chen, 1999). For the simplicity of exposition, we let T → ∞ and consider the infinite time
horizon Gramian: C(Anom, B) = limT →∞ CT (Anom, B), which exists with 1 - P[ρ(Anom) ≥ 1];
see also Pasqualetti et al. (2014). We drop the notation (Anom, B) in C when the context is clear.
Average energy: A widely used metric to measure how hard or easy it is to control the network is
average energy: 兀口2=1 XTCtX dx/ Jkxk2=1 dx, which evaluates to nTtr(Ct) (Cortesi et al.,2014).
Here, where Ct is the pseudo inverse, and XTCtX is the minimum control energy needed to steer
X[0] = 0 to an arbitrary target state X ∈ Rn . Thus, average energy measures the minimum control
energy required to steer X[0] = 0 to an arbitrary state uniformly distributed over the unit sphere.
1 One may think our LTI model as the linearized system of an underlying non-linear system. Controllability
of non-linear systems require a case by case analysis and we leave this topic for future research.
2Alternatively, Bu[t] = BKuK [t], where BK is the sub-matrix of B whose columns are indexed by K ⊂ [n].
However, we stick with notation in Eq. 1 to make our analysis less cumbersome.
3The LTI system Eq. 1 is asymptotically stable if kx[t]k2 → 0 as t → ∞, for u[t] = 0 and x[0] 6= 0.
3
Under review as a conference paper at ICLR 2022
Average controllability: Numerical computation of Ct for large-scale networks is demanding. Owing
to the fact that tr(Ct) ≥ 1∕tr(C), one uses tr(C)——called the average controllability——as a proxy for
average energy Gu et al. (2015). The higher the average controllability is for a given set of control
nodes defined by B, the smaller their average energy, thus higher their influence on the network.
3.2	Stochastic block models
Stochastic block models (SBMs) are probabilistic models that produce random graphs with planted
communities. Formally, let Gfine , (V, E) be the un-directed graph (also referred as fine graph) with
n nodes and random edge weights generated according to the general SBM(n, Q, p):
Definition 1. (General SBM) In the general SBM(n, Q, p), the graph Gfine is partitioned to K
disjoint sub-graphs (or communities) of relative sizes p = [p1, . . . ,pK] such that V = ∪kK=1Vk. Two
nodes u ∈ Vk and v ∈ Vk0 are joined by an edge with the weight Auv ∈ {0, 1}, which is drawn with
probability Qkko independently from other edges, for all k, k0 ∈ [K].	□
In General SBM, the probability distribution of weights Auv is common for all u ∈ Vk and u ∈ Vk0 .
The general SBM(n, Q, p) thus generates a weighted symmetric graph with K communities with
non-identical in- and cross-edge connection probabilities given by Q ∈ [0, 1]K×K. Alternatively,
Auv 〜BemOUlli(Qk,ko)	if k, k0 ∈ [K] : Pku > 0, Pk，V > 0,	(3)
where the community membership matrix P = [Pkv] ∈ RK×n is given by
PPT=Diag(|V1|,...,|VK|)withPkv=	10 oitfhevrw∈isVek.,	(4)
We define D，1PPT which is a diagonal matrix of relative community sizes.
Definition 2. (Coarse SBM Ghoroghchian et al. (2021)) Define a coarse-scale summary to A as
Ae , WAWT ∈ Rm×m ,	(5)
where the coarsening matrix W ∈ Rm,n is (a) r-homogeneous, for all i ∈ [m]; that is, each i-th row of
W (say Wi) has r non-zero terms and all rows have constant row sum and (b) (WWT = 1 Im). □
Here, A can be interpreted as the symmetric adjacency matrix of an un-directed graph Gcoarse with
m nodes—— referred to as coarse graph. This interpretation is helpful when we discuss LTI system
4
associated with A in Section 5. We refer the nodes in Gcoarse to as c-nodes4 as opposed to the fine
nodes in Gfine. Note that r ≤ m, and r《n indicating that c-nodes can cover the fine graph only
sparsely. In other words, there may exist (several) fine nodes that do not contribute to A (see Fig. 1).
The main goal of our paper is to quantify controllability of Gfine, with community structure, using the
coarsely inferred network Gcoarse . Importantly, we do not have access to the way the coarse graph is
acquired at the time of decision making though the results depend on them.
From Eq. 8 and Eq. 3, the expected quantities of A，E[A] and A，E[A] can be computed as
A = PTQP and A= (WPT) Q(WPT)T,	(6)
'{z~*}
Φ
where Φ ∈ Rm×K is the coarse community membership matrix, and Φik captures the extent to which
the i-th c-node overlaps with the k-th community. Let us also define the resolution parameter.
ν , min	Φik.	(7)
i∈[m],k∈[K] = Φik>0
By definition 1/r ≤ ν ≤ 1. In what follows, we assume that a c-node has a constant minimum
overlap (i.e. ν) with each community that independent of other system parameters.
The example below will highlight the structural differences among matrices P, Φ, and W.
Example 1. For fine network shown in Fig. 1, the following hold
4“c-” stands for compound or coarse.
4
Under review as a conference paper at ICLR 2022
1.	P = BlkDiag(11T2, 11T8, 16T). Here, 1d is the d-dimensional all-ones column vector.
2.	φ = [ΦT, ΦT,…,ΦT]t, where Φ1 = [1,0,0]; Φ2 = [ j, 1,0]; Φ3 = Φ4 = [0,1,0];
Φ5 = [0, 3, j];and Φ6 = [0,0,1].
3.	Finally, each row of the coarsening matrix W has three non-zero entries, all equal to 1/3.
Each c-node in {1,3,4,6} covers one community; each c-nodes in {2,5} overlap with two. □
Assumption 1. (SBM scaling Abbe (2017)) FOraU k ∈ [K], we have 0 < Cmin ≤ VI ≤ Cmax < 1,
where cmin, cmax are constants. There exists a ρn ∈ (0, 1) and a non-negative matrix Q(c), such that
Q = ρnQ(c),	(8)
Assumption 2. (Fully-Synchronized c-node): The coarse graph has at least one pure node per
community k ∈ [K]. Formally, for all k ∈ [K], there exist a coarse node i ∈ [m] such that Φik = 1.
Assumption 3. (Uniform Coarsening): There exist Cmin and Cmax independent of m such that
Cmin1K ≤ 1m φ/m ≤ Cmax 1K.
We also assume that communities have self-connections, that is, tr(Q(c)) > 0, and Qc ≤ 1K×K.
Assumption 1 helps us uniformly control the sparsity of connection in and cross communities. For
several real-world networks, ρn typically decreases with n Abbe (2017). Assumption 2 states that
for each community there exist at least one c-node that is fully inside one community. We call such
c-node fully synchronized (see Remark 1). Finally, Assumption 3 ensures that the relative coverage
of each community measured by coarse nodes scales linearly with respect to the graph size.
UoRUnPSa
MUeas,leoɔ
Algorithm
(1)
Algorithm
(2)
Figure 1: Schematic of MOR and learning-based approaches for estimating θgroup,A. (a) For Gfine consisting
of n = 36 nodes, we have K = 3 communities (V1, V2, V3) with m = 6 coarse (c)-nodes (K1,. . . ,K6), each
having a coverage size r = 3. The control node set is K3. (b) In MOR approach, we infer θgroup,A via reduced
order dynamical system Scoarse . (c) Learning based approach capitalizes on mixed membership (MM) Algorithm
2 to estimate θgroup,A directly from A, thereby avoiding the need to do consider Scoarse.
Remark 1. (Synchronization of community and coarsening): The coarsening operation is oblivi-
ous to the community structure in Gfine. Thus, the i-th c-node contains information about multiple
communities if SuPP(Wi,：) ∩ Supp(Pi,：) = 0 (the subscript denotes the i-th row), for i = j. Perfect
synchronization: A special case where the intersection is non-empty only when i = j . This happens
when all the communities have the same size and each c-node covers only one whole community.
4	Problem Statement
Consider the LTI system on the network Gfine, defined by the symmetric adjacency matrixin A Eq. 3:
Sfine : x[t + 1] = Anomx[t] + BKu[t],	(9)
where Anom，CIr(A) A, A ∈ Rn×n, and BK ∈ Rn×n selects a set of control nodes K ⊂ Gfine.
Depending on the controllability properties of Gfine (see, Section 3.1), the inputs at these |K| control
nodes may or may not effect all the states in x[k]. Let Ki = Supp(wi), be the set (hereafter, group)
of r nodes coarsened by wi . The following assumption states that Gfine is controllable from all
Ki ⊂ Gfine ; see Remark 2.
5
Under review as a conference paper at ICLR 2022
Assumption 4. Let BKi = Diag(wiT). For any i ∈ [m], the Gramian C(Anom, BKi) is full rank.
As ρ(Anom) < 1 holds with high probability (see Lemma 1), it follows that C(Anom, BKi) exits, and
hence, the infinite time Gramian C(Anom, BKi) exists. Assumption 4 ensures that average energy
(see Section 3.1) is finite; however, average controllability need not be finite.
For Ki-th control node set with input matrix BKi = Diag(wiT), associate the average controllabity
measure: θgRup A，tr[C(Anom, BKi)],for all i ∈ [m]. Since C(∙) is a p.s.d matrix, it follows that
θg(ir)oup,A ≥ 0. Accordingly, define the group average controllability vector for Sfine:
θgroup,A , θgroup,A, . . . , θgmroup,A ∈ R
(10)
which summarizes the average controllability measure (see Sec 3.1) for all control nodes sets
{K1, . . . , Km}. Thus, θgroup,A helps infer (i) Kis that drive the network to the desired target state
with least control effort, and (ii) if such control node sets should have a community structure.
In this paper, using only the knowledge of A in Eq. 5, we want to estimate the random vector
θgroup,A in Eq. 10. We consider two contrasting approaches: (i) the traditional model order reduction
(MOR), where we rely upon the reduced order auxiliary system Scoarse (see Eq. 11) governed by
A to infer θgroup,A; and (ii) the learning based approach, where we directly estimate θgroup,A
using a clustering based mixed membership community learning algorithm; see Section 6. Fig. 1
provides a nice graphical illustration of our approaches. Broadly, our analysis highlights the role of
community structure, coarsening process, and graph sparsity conditions on the performance of these
both approaches. Our numerical simulations show that both approaches can outperform each other;
however, learning based approach outperforms its counterpart in several parametric regions Finally,
our main results in Sections 5 and 6 are probabilistic in nature because A is a random matrix.
Remark 2. (Group nodes controllability) Assumption 4 demands that a group of nodes should be
able to control Gfine, which holds true for brain networks (Pasqualetti et al., 2019). This assumption is
a very weaker condition than asking Gfine to be controllable from every single node. Moreover, if Gfine
is not controllable from the control nodes, we can decompose the state-space of Gfine into controllable
and uncontrollable sub-spaces (Chen, 1999), and adapt our analysis to the controllable sub-space.
5	MOR approach for group average controllability
We provide a tight upper bound on the element wise error between θgroup,A in Eq. 10 and θcoarse,Ae
in Eq. 13. The latter quantity is the group average controllability vector of the reduced order system:
Scoarse : xe[t + 1] = An
omxe[t] + Bu[t] ,
(11)
1	T Δ 1 T' T" ɪɪ T A ɪɪ τT ∙	♦ F 1 - L 1 .1	t ∙ . ∙	∕'	~,∕T∖*
where Anom，τ-1eyA , A = WAWT is given by Eq. 5 and the normalization factor C ∙ tr(A) is
used for stability purposes, where C > 0. Importantly, the state x[t] ∈ Rm is not a compression
of the true state x[t] in Sfine and m n (hence the name MOR). Rather, xe[t] is a fictitious state
that describes the dynamics of the (scaled) matrix A. This fictitious state is controlled by the input
Be u[t] ∈ Rm .
The following lemma states that Sfine and Scoarse are asymptotically stable with very high probability.
Thus, θgroup,A in 10 and θcc,arse A in 13 are well defined. Let c,C,β ≥ 0, and define the stability
..	.	_	.	：	.	.	.	.	一 C.	一	.	.	.~.	..	~........... 一	C.
indices: Knom := (Ce tr(A)-∣∣ A∣∣∞)∕(n(1-cβ)2) and Knom := (Ce tr(A) — || A∣∣∞)∕(m(1-rcβ)2),
—

where A ∈ Rn×n and A ∈ Rm×m are given in Eq. 6 and r is the coverage size.
Lemma 1. (Probabilistic stability of Sfine and Scoarse): Suppose that the stability indices Knom and
eKnom are strictly positive for some constants C, CC > 0, and 0 ≤ e < 1. Then,
P [ρ(Anom ) ≥ e] ≤ n exp (—2Knom)	and	P[ρ(Anom ) ≥ e] ≤ m exp (—2eKnom ) .	(12)
For m = n, we have r = 1, and hence, the probabilistic inequalities in Eq. 12 coincide with each
other. Further, the higher Knom and eKnom , the higher the chance that Sfine and Scoarse are stable.
Interestingly, Knom and Kenom do not explicitly scale with n and m. In fact, Knom ≥ Knom, lb, where
6
Under review as a conference paper at ICLR 2022
κnom,ib := (Cecmin tr(Q) - CmaX ∣∣Q∣∣∞)∕(1 - cβ)2 is independent of n. Thus, stability is not
guaranteed for larger networks modeled using Sfine With c ∙ tr A normalization.
Instead, κnom, lb → ∞ as cβ → 1, thereby P [ρ(Anom) ≥ β] ≤ n exp(-2κnom, lb) → 0. So, under
what conditions κnom > 0 for cβ = 1? One such condition is cβ ≥ cmaX kQk∞∕(cmin tr(Q)).
Let cmaX = cmin . For diagonally dominant probability matrix Q (i.e., the community structure is
assortative—more in-community edges than across-community edgess), tr(Q) ≥ kQk∞, and hence,
we can choose c and β such that cβ = 1. However, non diagonally dominant matrices can satisfy
tr(Q) ≥ kQk∞. For example, a symmetric matrix Q ∈ R3×3, with Qii = 0.2, Q12 = 0.25, and
Q13 = 0.01 is not diagonally dominant because Qii ≤ Pj6=i Qij, for all i, but tr(Q) ≥ kQk∞.
We now bound the difference between θgroup,A in 10 and。^a^ A in 13. Let Anom，A∕(c ∙ tr(A))
and Bei = rWdiag(wiT)—the coarsened input matrix. Let the average controllability for the i-th
c-node be θ(i) e ,tr[C(Aenom, Bei)] ≥ 0. Define the average controllability vector for Scoarse:
θ
coarse,A
θ(1) e
coarse,A
∈ Rm .
(13)
In what follows, when the synchronization holds, we show that θcoarse,Ae associated with Scoarse can
well approximate θgroup,A associated with Sfine. Define the error metric:
∆i(A,Ae)
rθ(i)	1
r group,A -
--------T-、-----------------
Pi=jrθgrOup,A-1]	Pm=M
θ(i)	e - 1
coarse,A
M)
coarse,Ae
- 1]
for all i ∈ [m].
(14)
The proposed error metric ∆i (A, A) allows us to do a fair shifting- and scaling-free comparison
between the vectors θgroup,A and θcoarse,Ae. The shift factor "-1" accounts for the inherent "+1" shift
in the average controllability definition and the scale factor r discounts the 1∕r factor in θgroup,A.
Akin to ∆i(A, A) in Eq. 14, define ∆i(A, A) associated with the expected quantities A and A.
_.	-—
mi	Y / n-ɪ	j ∙	1	ι λ	λ	∖ τ , ʌ / *	* ∖ ι ∖	/ T T ∖ » ι ι- ι
Theorem 1. (Element wise bound on θgroup,A-08&田 A)： Let ∆i(A, A) and ∆i (A, A) be defined
as above. Then, under the assumptions in Lemma 1, ∆i (A, Ae) ≤ A(A，A) + O (ρn⅛ + √⅛)
with probability at least 1 - 6 exp(-2κ(Q(c), D, ν, m, n, ρn)), where, for constants 0 < δ, ζ < 1,
κ(Q(c), D, ν, m,n, ρn) = min nnρ2n(tr(DQ(c))ζ)2, m(ρnν2tr(Q(c))δ)2
pnmn(cmin cmin ||Q(C) || 1,1Z)2，pEm2 伍Kin || Q(C) || 1,1δ尸｝,
with ν, D, and Q(C) given by Eq. 7, Eq. 4, and Eq. 8, and emin is defined in Assumption 3.
Theorem 1 says that ∆i(A, A) ≈ O (P^m + √r2ρ3 ) provided ∆i (A, A) is small. Thus, for fixed n,
MOR based estimate θcoarse,Ae approximates θgroup,A if the graph density ρn or number of c-nodes
is large, which we validate using numerical simulations as well. It can be shown that the bias term
△i (A, A) is exactly zero if the communities are synchronized with the coarsening process (see
Remark 1) and that Qii = p > 0 and Qij = q > 0, for i 6= j.
6	Learning Approach for Group Average Controllability
We present our learning based approach to estimate θgroup,A in Eq. 10. Unlike the MOR based
approach that relies on Sfine, we directly estimate elements in θgroup,A based on the popular mixed
membership (MM) community learning algorithms (Mao et al., 2020; Huang et al., 2019; Mao et al.,
2018; 2017; Aicher et al., 2015). Specifically, we work with the MM algorithm by (Mao et al., 2020)
which is not only numerically efficient but also has strong theoretical guarantees.
Lemma 2. Let θgroup A be obtained by replacing A with the expected matrix A in θgroup,A, given
by Eq. 10. Let P and Φ be as in Eq. 4 and Eq. 6. Suppose that Assumption 1 hold. Then,
θgroup,A = (1m + dΦdiag(Υ)) ∕r,	(15)
7
Under review as a conference paper at ICLR 2022
where d = 1/(ntr[DQ(c)]), D = (1/n)PPT, and Q(c) is given by Eq. 8, and
Υ , (nd)Q(c)DQ(c)[I- ((nd)DQ(c))2]-1.
(16)
Lemma 2 gives us a formula to compute the group average controllability vector associated with the
expected matrix A (See Supplemental material for complementary explanation and interpretation).
Theorem 3 (see Appendix) shows that ∆i(A, Ae) ≤ , for arbitrary > 0, hold with high probability.
In view of this fact, we propose our candidate estimator as
θgroup , 1m + Φdiag(Υ),	(17)
一 ʌ Z ^^^ ，一 ^	∙^^	、c、	-1_ _ 一	一一 ʌ -A	一一__ .一 一 -U
where Y，t(DQ(I - (trD¾)2)-1. The hatted quantities Φ and Q are obtained from Algorithm5
2, which takes as input A and number of communities K . Instead, we obtain D from Algorithm 1.
Importantly, θgroup in Eq. 17 is obtained from coarsened matrix A but not the fine scale matrix A.
m«	A/八	j ∙	T	IT	Λ	1 Λ	∖ C	.1 ..1	■
Theorem 2. (Component wise error bound between θgroup and θgroup,A): Suppose that there exist
constants Cmin and Cmax that satisfy Assumption 3. Then, under the hypotheses stated in Lemma 1,
|
rθ(i)	1
r group,A -
方(i)	_ 1
group -
----------TTT-------- - ---------7Γ7------
Pi=ι(rθgrΟup,A - 1)	Pm=ι(θgrθup - 1)
=O ( — [-----+ ||EΦ || max + 11EQ || max + ||ED || max ]
m ρn
}
^^^{^^^≡
,∆bi(Ae)
holds with probability at least 1—3exp(-2^(Q(c), D,m,n, ρn)), where Eφ，Φ—Φ, EQ，Q-Q,
and ED , D - D are the error matrices. Further, for a constant 0 < ζ < 1, the exponent
^(Q(c), D,m,n,pn) = min ^ιρ2nτ (tr(DQ(c))Z )2, mnρ2l(C min C mn∣∣Q(c)∣∣ι,ιZ)2}.
Theorem 2 suggests that for sufficiently large m, the estimate θgroup approximates θgroup,A to an
arbitrary precision if: (a) the graph is dense enough (larger ρn), (b) the coarse community membership
matrix is well estimated (smaller6 ∣∣Eφ∣∣max), (c) the cross-community probability estimation do
not suffer from high error (smaller ||EQ||max), and the relative community sizes estimated from
the coarse graph are close the ones in the fine graph (smaller ||ED ||). The result of Theorem 2 is
important because one can directly infer the most influential control node set Ki (one with high
average controllability) via the most influential i-the c-nodes, and vice versa.
Algorithm 1: Direct Inference of the Group Average Controllability
Require: estimates Φ and Q from Algorithm 2, and the number of communities K
1:	compute D = diag(
ΦT1m
1K ΦTIm
2:	return θgroup
1m + Φdiag (tQDQ)(I-(trD⅛)2)-1))
)
Algorithm 2: Mixed Membership Community Estimation Algorithm (Mao et al., 2020)
Require: Coarse adjacency matrix A, number of communities K
1:	compute the highest K eigen-decomposition of A as V ΛV T and set Spruned = Prune(V )
2:	set X = V([m]\Sp.runed, ：) and compute SPure = Successive Projection Algorithm(Xt)
3:	set Xpure = X(Spure, ：) and compute un-normalized Φun-nom = VXpUIe
4:	Φun-nom _ 0 if Φun-nom < e-12, ∀i ∈ [m],k ∈ [K]
ʌ	-1	, ʌ	. ʌ	ʌ	Λ -I-
5:	return Φ = DiagT(Φun-nom1κ)Φun-nom and Q = XPUreAXTIre
5This MM algorithm adapted from Mao et al. (2020) is a type of spectral clustering method that first performs
eigen decomposition of A to find the overlapping membership (Φik) of the fine nodes. A pruning step is also
included (see steps 4 and 5 in Algorithm 2) to speed up the algorithm performance.
6 (Mao et al., 2020) showed that ∣∣Eφ∣∣max and ||Eq∣∣max stated in Theorem 2 approach zero under some
conditions, as m → ∞. But these conditions might not be applicable to our setup because of our coarsening
operation. However, our simulations show that ∆i(A) decreases with m. The behavior of ∣∣Eφ∣∣max and
||EQ||max with respect to graph scaling is left for future work.
8
Under review as a conference paper at ICLR 2022
7 Simulations
We validate our theoretical results by plotting7 the errors ∖ Pm=I ∆i(A, A) and ∖ Pm=I △ i(A) and
show that these errors are comparable to the bounds we obtained in Theorems 1 and 2. We generate
A 〜SBM(n, Q, P) and then determine A = WAWT (see Supplemental material for generating
W). We set number of fine nodes n = 5000, the overlap parameter η = 0.1, and the number of
communities K = 5. Finally, for Q ∈ RK×K, we set Qkk = p = 0.05 and Qkk0 = q = 0.01 (for
k 6= k0). If not specified, the number of c-nodes m = 100 and coverage size per c-node r = 4.
Fig. 2(a)-2(d) illustrate the qualitative behavior of the errors with respect to changes in m, ρn , and
the degree of (non-)synchronization in coarse nodes (i.e., η), and r. To fairly compare these errors,
rθ(i)	-1
We also consider a base line error: Pm=II Pmμi 1-1)----------group),A---|/m, where μ ∈ [1, 2]m
一乙 i=1 (μi—)	Σm=i (rθgroup,A-I)
contains i.i.d. uniform random variable drawn independently of A.
(a) w.r.t. no. coarse nodes m.
(b) w.r.t. Pn for P = 5q = 0.005.
(c) w.r.t. overlap extent η.
(d) w.r.t. coverage size r.
m
Figure 2: Estimation error based on MOR and Learning approaches. MOR Error= im=1 ∆i (A, A)/m and
m
Learning ErrOr=Ei=1 ∆i(Α)∕m. The shaded region in the figures represent one standard deviation computed
for 20 independent realizations. We make the following observations. First, both the learning and MOR based
errors are consistently better than the random baseline. Second, the learning based approach has consistently
smaller error than that of the MOR approach for large parametric regimes. Third, (a) shows that all errors
monotonically decrease as m increases. This is consistent with our bounds in Theorems 1 and 2. Fourth, (b)
shows that errors decrease as ρn increases. This is expected because larger values of ρn result in more distant
in- and cross-community edge densities. This makes community representation extraction and controllability
estimation easier. Fifth, (c) demonstrates the higher tolerance of the Learning approach to situations whenin
coarse measurements are less synchronized, in comparison to the MOR method. Finally, (d) shows that error
decreases with r . This should be the case as larger r means more fine nodes are sampled during coarsening.
8	Conclusion and Future work
We introduced a learning-based framework that exploits the power of community-based representation
learning to infer average controllability of fine graphs from coarse summary data. We compared the
performance of this approach with that of MOR approach. For both these methods, we derived high
probability error bounds on the deviation between the error estimate and ground truth, and validated
the theory with numerical simulations. Our results highlight the role of fine- and coarse-network
sizes, graph density, and community synchronization bias (see Remark 1 in modulating the estimation
errors. Interestingly, for the latter approach, we show that the estimation error decreases with network
size albeit the synchronization bias, which is not the case with the MOR-based approach. For
future, we plan to implement our theory to study the role of coarsening, community structures , and
synchronization aspects on the controllability of brain networks.
7The Python code to reproduce the results is attached to the submitted file.
9
Under review as a conference paper at ICLR 2022
9	Ethics Statement
Although our work mainly takes a theoretical perspective to the controllability of coarse graphs
motivated by therapeutic neuroscience applications, our results can potentially involve negative
impacts if employed in other applications. For instance, identifying the most influential groups of
nodes (or equivalently individuals) in a social network as a result of estimating the network group
average controllability, may motivate manipulative actions; i.e. the most influential node group
may be selected for control, in order to steer the whole network towards an unethical goal (like
manipulating individuals in a social network to vote in favour of a particular election candidate). The
negative impact may also result in bias against less-influential nodes, as they will be ignored when it
comes to the selection of node groups for control actuation.
10	Reproducibility S tatement
All the results presented in this paper are reproducible. The theoretical findings are annotated and
step-by-step elaborated in the appendix. The data generation process and the parameter values used
for numerical simulations are fully explained. In addition, the Python code from which the simulation
figures are generated is attached to the submission files. The code is also on Github and the repository
will go public upon submission acceptance.
10
Under review as a conference paper at ICLR 2022
References
Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of
Machine Learning Research, 18(1):6446-6531, 2017.
Christopher Aicher, Abigail Z Jacobs, and Aaron Clauset. Learning latent block structure in weighted networks.
Journal of Complex Networks, 3(2):221-248, 2015.
Richard F Betzel and Danielle S Bassett. Multi-scale brain networks. Neuroimage, 160:73-83, 2017.
Chi-Tsong Chen. Linear system theory and design. 1999.
Pedro H Constantino, Wentao Tang, and Prodromos Daoutidis. Topology effects on sparse control of complex
networks with laplacian dynamics. Scientific reports, 9(1):1-9, 2019.
Fabrizio L. Cortesi, Tyler H. Summers, and John Lygeros. Submodularity of energy related controllability
metrics. In 53rd IEEE Conference on Decision and Control, pages 2883-2888, 2014. doi: 10.1109/CDC.
2014.7039832.
Adrien Dulac, Eric Gaussier, and Christine Largeron. Mixed-membership stochastic block models for weighted
networks. In Conference on Uncertainty in Artificial Intelligence, pages 679-688. PMLR, 2020.
Thorben Funke and Till Becker. Stochastic block models: A comparison of variants and inference methods.
PloS one, 14(4):e0215296, 2019.
Nafiseh Ghoroghchian, Gautam Dasarathy, and Stark Draper. Graph community detection from coarse measure-
ments: Recovery conditions for the coarsened weighted stochastic block model. In International Conference
on Artificial Intelligence and Statistics, pages 3619-3627. PMLR, 2021.
Shi Gu, Fabio Pasqualetti, Matthew Cieslak, Qawi K Telesford, B Yu Alfred, Ari E Kahn, John D Medaglia,
Jean M Vettel, Michael B Miller, Scott T Grafton, et al. Controllability of structural brain networks. Nature
communications, 6(1):1-10, 2015.
Christianne N Heck, David King-Stephens, Andrew D Massey, Dileep R Nair, Barbara C Jobst, Gregory L
Barkley, Vicenta Salanova, Andrew J Cole, Michael C Smith, Ryder P Gwinn, et al. Two-year seizure
reduction in adults with medically intractable partial onset epilepsy treated with responsive neurostimulation:
final results of the rns system pivotal trial. Epilepsia, 55(3):432-441, 2014.
Ling Huang, Chang-Dong Wang, and Hongyang Chao. ocomm: Overlapping community detection in multi-view
brain network. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2019.
Hossein Kassiri, Sana Tonekaboni, M Tariqus Salam, Nima Soltani, Karim Abdelhalim, Jose Luis Perez
Velazquez, and Roman Genov. Closed-loop neurostimulators: A survey and a seizure-predicting design
example for intractable epilepsy treatment. IEEE trans. on biomedical circuits and systems, 11(5):1026-1040,
2017.
Fritz Keinert. Course notes: Applied linear algebra. https://orion.math.iastate.edu/keinert/
math507/notes/chapter5.pdf.
Jason Z Kim, Jonathan M Soffer, Ari E Kahn, Jean M Vettel, Fabio Pasqualetti, and Danielle S Bassett. Role of
graph architecture in controlling dynamical networks with applications to neural systems. Nature physics, 14
(1):91-98, 2018.
Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. On mixed memberships and symmetric nonnegative
matrix factorizations. In International Conference on Machine Learning, pages 2324-2333. PMLR, 2017.
Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. Overlapping clustering models, and one (class) svm
to bind them all. In Advances in Neural Information Processing Systems, pages 2126-2136, 2018.
Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. Estimating mixed memberships with sharp
eigenvector deviations. Journal of the American Statistical Association, (just-accepted):1-24, 2020.
Sarah Feldt Muldoon, Fabio Pasqualetti, Shi Gu, Matthew Cieslak, Scott T Grafton, Jean M Vettel, and Danielle S
Bassett. Stimulation-based control of dynamic brain networks. PLoS computational biology, 12(9):e1005076,
2016.
Gerard O’Leary, David M Groppe, Taufik A Valiante, Naveen Verma, and Roman Genov. NURIP: Neural
interface processor for brain-state classification and programmable-waveform neurostimulation. IEEE Journal
of Solid-State Circuits, 53(11):3150-3162, 2018.
11
Under review as a conference paper at ICLR 2022
Fabio Pasqualetti, Sandro Zampieri, and Francesco Bullo. Controllability metrics, limitations and algorithms for
complex networks. IEEE Transactions on Control ofNetwork Systems,1(1):40-52, 2014.
Fabio Pasqualetti, Shi Gu, and Danielle S Bassett. Re: Warnings and caveats in brain controllability. NeuroImage,
197:586-588, 2019.
Dragana M PavloviC, Bryan RL Guillaume, Emma K Towlson, Nicole MY Kuek, Soroosh Afyouni, Petra E
V6rtes, BT Thomas Yeo, Edward T Bullmore, and Thomas E Nichols. Multi-subject stochastic blockmodels
for adaptive analysis of individual differences in human brain network cluster structure. NeuroImage, 220:
116611, 2020.
M Reza Pazhouhandeh, Gerard O’Leary, Iliya Weisspapir, David Groppe, Xuan-Thuan Nguyen, Karim Ab-
delhalim, Hamed Mazhab Jafari, Taufik A Valiante, Peter Carlen, Naveen Verma, et al. 22.8 adaptively
clock-boosted auto-ranging responsive neurostimulator for emerging neuromodulation applications. In 2019
IEEE International Solid-State Circuits Conference-(ISSCC), pages 374-376. IEEE, 2019.
Brittany H Scheid, Arian Ashourvan, Jennifer Stiso, Kathryn A Davis, Fadi Mikhail, Fabio Pasqualetti, Brian
Litt, and Danielle S Bassett. Time-evolving controllability of effective connectivity networks during seizure
progression. arXiv preprint arXiv:2004.03059, 2020.
Ruslana Shulyzki, Karim Abdelhalim, Arezu Bagheri, M Tariqus Salam, Carlos M Florez, Jose Luis Perez
Velazquez, Peter L Carlen, and Roman Genov. 320-channel active probe for high-resolution neuromonitoring
and responsive neurostimulation. IEEE trans. on biomedical circuits and systems, 9(1):34-49, 2015.
Olaf Sporns and Richard F Betzel. Modular brain networks. Annual review of psychology, 67:613-640, 2016.
Peng Gang Sun. Controllability and modularity of complex networks. Information Sciences, 325:20-32, 2015.
Elena Wu-Yan, Richard F Betzel, Evelyn Tang, Shi Gu, Fabio Pasqualetti, and Danielle S Bassett. Benchmarking
measures of network controllability on canonical graph models. Journal of Nonlinear Science, pages 1-39,
2018.
Zhengzhong Yuan, Chen Zhao, Zengru Di, Wen-Xu Wang, and Ying-Cheng Lai. Exact controllability of complex
networks. Nature communications, 4(1):1-9, 2013.
12
Under review as a conference paper at ICLR 2022
A	Appendix
In this appendix, we first provide further detailed explanation for the numerical simulations that was
missing due to space constraints. Next we have a remark that adds complementary interpretation of
the group controllability formula estimated from the learning approach. Finally, we provide proofs
for all results stated in our paper.
A.1 MORE ON NUMERICAL SIMULATIONS: HOW TO GENERATE W
For a realization of Ae, we obtain the support of each row of W independently by first generating a
random vector π ∈ {η}K using a Dirichlet distribution; larger positive η result in greater overlap and
more communities. For each community k where ∏k > 0, [r ∙ ∏kC fine nodes are randomly chosen
from Vknon-sel (i.e., non-selected fine indices in community k) and set as the support of wi. The
chosen indices are removed from Vknon-sel.This process continues until the support of all wis are
selected.
A.2 More on the Estimated Group Controllability using the Learning Approach
Remark 3. (Synchronization versus controllability) Recall that Φi,k = |{v : v ∈ Vk ∩
supp(wi)}|/r (for all k ∈ [K], i ∈ [m]) is the fraction of c-node i’s overlap with community
k. Thus, from Eq. 15, it follows that θg(ir)o Ip A H Pk∈[κ] φi,kγkk. In view of this observation and
group,A	∈[ ]
Lemma 2, we observe that c-nodes that have the largest overlap with communities of strongest Υkk
are the most controllable.
Additional notation: For n X m dimensional real matrix M, denote IlMkF = /Pu Pv Muv =
vztr(MTM). For a symmetric matrix M, denote λmax(M) to be the maximum eigenvalue.
diag(M) = [M11, . . . , Mnn]T ∈ Rn, and Diag(M) sets the off-diagonal entries of M to zero.
Useful matrix norm bounds:
1.	Let Z = XY. Then, kZkF ≤ kXk2 kYkF ≤ kXkFkYkF.
2.	Cauchy-Schwartz inequality: tr(XTY) ≤ kXkFkYkF.
3.	For any norm: If ||M|| < 1 ⇒ ||(I 一 M)-1∣∣ < 1—居口 (Keinert).
4.	∣∣M∣∣∞ ≤√n∣∣M∣∣2
5.	∣∣m∣∣i ≤√n∣M∣∣2
6.	For any m × n matrix M, We have ∣∣M∣∣2 ≤ √m∣∣M∣∣∞.
7.	kMXYk
max ≤ kMk∞ kXkmax kYk1
8.	||M2 一 M02|| ≤ ||M 一 M0||(||M|| + ||M0||)
Proof. IIM2-Mo2II = ∣∣M2-MM0+MM0-M02|| = ∣∣M(M-M0)+(M-M0)M0∣∣ ≤
||M - M0∣∣(∣∣M∣∣ + ∣∣M0∣∣)	□
9.	M-1 -M0-1 = M-1(M0 - M)M0-1
Lemma 3. (Lower-Bound Probability of Joint Events:) For the intersection of two events f1 and
f2 we have: P{f1 ∩f2} ≥ P{f1}+P{f2} - 1.
A.3 Proofs for Section 5: MOR Approach for Group Average Controllability
A.3.1 Proof of Lemma 1
Proof. We prove only the left inequality in Eq. 12. The right inequality in Eq. 12 can be proved using
similar steps, and the details are omitted. Because Anom = A/(c ∙ tr(A)) is symmetric, it follows
that ρ(Anom) = kAnomk2. From this observation and the fact that kAnomk2 ≤ kAnomk∞, we have
P[ρ(Anom) ≥ β] = P [kAnomk2 ≥β] ≤ P [kAnomk∞ ≥β]
= P [kAk∞ - cβtr(A) ≥0].	(18)
13
Under review as a conference paper at ICLR 2022
Let F = ∣∣A∣∣∞ - Cetr(A) and note that E[F] = EkAk∞ - Cetr(A) ≥ ∣∣A∣∣∞ - Cetr(A).
The inequality follows because kAk∞ = maxi∈[n] Pjn=1 |aij | and that E[max{X1, . . . , Xt}] ≥
max{E[X1], . . . , E[Xt]}. From these observations, inequality in Eq. 18 can be further bounded as
P[F ≥ 0] =P[F-E[F] ≥ -E[F]]
≤ P [F - E[F] ≥ Cetr(A)-kAk∞] .	(19)
We show that F is a sub-Gaussian random variable and then bound the right term in Eq. 19 using the
well-known concentration inequality results. Rewrite F as follows
(n	n	n	)
|a1j|,	|a2j|,...,	|anj|	- Cetr(A)
j=1	j=1	j=1	
(n	n	n	)
|a1j | - Cetr(A),	|a2j | - Ce tr(A), . . . ,	|anj | - Cetr(A)
(n	n	n	)
(a1j - Ceajj ),	(a2j - Ceajj ), . . . ,	(anj - Ce ajj )	.	(20)
In the last equality, we drop the absolute values because aij ∈ [0, 1]. From the latter fact, we
also note that (akl - akk) ∈ [-Ce, 1]. Thus, for all k 6= l, (akl - akk) is bounded, and hence,
Sub-Gaussian with parameter at most，1 + (CeyI2 Instead, for k = l, ®i - akk) is Sub-Gaussian
with parameter at most |1 - Ce|/2. Finally, from Definition 1, notice that each term in the summation
Pjn=1(akj - Ceajj) is independent. From these facts and the linearity of sub-Gaussians, we note
that, for all k ∈ [n], summand Pjn=1(akj - Ceajj) is sub-Gaussian with parameter at most:
σ = 1 P(n - 1)(1 + (Ce)2) + (1-Ce)2.
Putting all these pieces together in conjunction with the facts that maxima of sub-Gaussians concen-
trates near its expectation and Cetr(A) -∣∣ A∣∣∞ ≥ 0 (by assumption), from Eq. 19, we have
P [F - E[F] ≥ Cetr(A) - kAk∞] ≤ nexp (-(Cetr(A) - ∣Ak∞)2∕2σ2)
≤ nexp (-2(Cetr(A) - ∣Ak∞)2∕(n(1 - Ce)2)) .	(21)
The last inequality follows because 2σ2 ≥ n∕2(1 - Ce)2. The left inequality in Eq. 12 follows by
combining inequalities in Eq. 18 and Eq. 21. The proof is now complete.	□
A.3.2 Lemma 4: Upper and Lower bounds of the trace of the fine- and
coarse-scale matrices
Lemma 4. (Upper and Lower bounds of the trace of the fine- and coarse-scale matrices). For
constants 0 < δ, ζ < 1:
P [nρnZ(i) ≤ tr(A) ≤ nρnZ(u)] ≥ 1 - 2exp (-2nρn(tr(DQ(C))Z)2),
P hmρnν2δ(l) ≤ tr(Ae) ≤ mρnδ(u)i ≥ 1 - 2exp (-2m(ρnν2tr(Q(c))δ)2),
P [1TWAWTI ≥ Cnin Pnm2IIQ(C)IlI,i(δ + 1)] ≥ 1 - eχp (-2夕匕Μ2 亿京加 ||Q(C) || 1,1δ)2 ),
P [imWA1n ≥ Cmin Cmin Pnmn|| Q(C) ∣∣ 1,1 (Z +1)] ≥ 1 一 exp (-2Pnmn 伍min Cmin ∣∣ Q(C) ∣∣ 1, lZ)2)
(22)
where ν is the coarsening resolution parameter (c.f. Eq. 7), and
ζ(l) ,tr(DQ(c))(1-ζ),ζ(u) , tr(DQ(c))(1 + ζ)
δ(l) , tr(Q(c))(1 - δ), δ(u) , tr(Q(c))(1 + δ)
(23)
14
Under review as a conference paper at ICLR 2022
A.3.3 Proof of Lemma 4
Proof. From definition, E[tr(A)] = tr(PTQP) = nρntr(DQ(c)) and E[tr(Ae)] = ρntr(ΦQ(c)ΦT).
Using the Hoeffding’s inequality for tail bounding independent random variables, for a constant
0 < ζ < 1 we have:
P Ijtr(A) - nρntr(DQ(C))| ≥ nρntr(DQ(C))Z] ≤ 2exp(-2(nPntr(IDQ(C))Z)2) = 2exp
( ∖
-2nρn(tr(DQ(C))Z)2
'-------------{z----}
∖	= ζ(u)-ζ(l)	J
(24)	2
Hence
nρntr(DQ(C))(1-Z) ≤ tr(A) ≤nρntr(DQ(C))(1+Z)
、------V-------}	、------V------}	(25)
ζ(l)	ζ(u)
We follow similar steps for A. For all i ∈ [m]:
X ΦikQ(kCk)0Φik0	≥ X	[Φi2kQ(kCk)+2Φik XQ(kCk)0Φik0]
k,k0∈[K]	k∈[K] ; Φik>0	k0>k
≥ V2tr(Q(C))+ 2ν2||Q(C)Ilmin
≥ ν 2 tr(Q(C))
(26)
based on which the following upper and lower bounds on tr( A) can be found.
m
mρnV2tr(Q(C)) ≤ tr(A) = Pntr(ΦQ(°)φT) = PnXX
ΦikQ(kCk)0 Φik0 ≤ mPntr(Q(C))
i=1 k,k0∈[K]
(27)
We can now use Hoeffding’s inequality to obtain the tail bound for a constant 0 < δ < 1:
P h∣tr(A) - ρntr(ΦQ(C)ΦT)∣ ≥ ρntr(ΦQ(C)ΦT)δ]	≤ 2exp (-2(Pntr(甯⑹委T))2)
≤ 2 exp (-2m(Pn ν 2 tr(Q(C))δ)2).
(28)
The event in Eq. 28 leads to the following upper and lower bound on tr(A):
mPnν2 tr(Q(C))(1 - δ) ≤ Pntr(ΦQ(C)ΦT)(1 -δ) ≤ tr(Ae) ≤ Pntr(ΦQ(C)ΦT)(1 + δ) ≤ mPn tr(Q(C))(1 + δ).
X-----------{------------}
δ(l)
V----------{----------}
δ(u)
(29)
We then replacing δ(l), δ(u) defined in Eq. 29 into Eq. 28.
Similarly, event
1tWAWt1 ≥	1TΦ› QΦt1(1 + δ) ≥ 1⅛inPnm2∣∣Q(C)∣∣ι,ι(1 + δ)	(30)
≥ Cminm 1k
happens with probability at least
1 - P [1tWAWt1 - 1tΦQΦt1 < ⅛inρnm2∣∣Q(C)I∣1,1δ]	≥ 1 - exp( -2(CminPnmm2Q(c)lll,lδ)2)
≥ 1 -exP '<-2pnm2(⅛inl1Q(C)“I,*)2)
(31)
using Assumption 3 and following using one-sided Hoeffding’s concentration inequality.
Finally, similar arguments can be applied to the following event
1*WA1n ≥ 1mφ} Q	P1n}	≥ ⅛inCminPnmn∣∣Q(C)∣∣1,l(1+ Z)	(32)
≥CCminm1K	=ndiag(D)(1+ζ)≥nCmin1K
15
Under review as a conference paper at ICLR 2022
that occurs with probability at least
1 - P [imWA1n - 1mΦQP1n < ≡minCminPnmn∣∣ Q(c) || 1,1Z]	≥ 1 - exp ( - 2 SmincminPnmmnQ(C) 1 | 1,1ζ)2 )
≥ 1 — exp (-2ρnmn(cminCmi∏∣∣Q(c) ∣∣1,lZ)2)
(33)
using one-sided Hoeffding’s inequality. This concludes the proof.
□
A.3.4 Lemma: Error between the Gramians of random and expected LTI systems
τ ,K	1 F	1	.	.1	.	1	1	∙	CTETL Cl	A	IA	1	1	∙,1	.1
Let S fine and S coarse denote the expected dynamics of LTI Eq. 9 When A and A are replaced With the
expected quantities A and A. The following result Provides an error bound between the difference of
Gramians of Sfine and S fine and that of Scoarse and S coarse.
Lemma 5. (Error between the Gramians of random and expected LTI systems): Under
the assumptions stated in Lemma 1, the following holds with probability at least 1 -
2exp (—2npn(tr(DQ(c))Z)2):
αn
kC (Anom , In×n ) - C (Anom , In×n )kmax
Γ1 _|_ kQ(C) kmax 1 Γ 1	∣ Pn kQ(C)kmax 1
[1+ tr (DQ(c))][ Z(V) + tr (DQ(c)) ]	1
c2(1-β2)Z(i)[i- FEi 而
(34)
and with probability at least 1 - 2 exp (-2m(ρn ν 2 tr(Q(c))δ)2):
~
αn
, kC(Aenom,
Ce
Im×m ) - C (Anom , Im×m )kmax
Γl I k Q(C) kmax 1 Γ 1	∣ Pn k Q(C) kmax 1
V ν2 tr (Q(c)) -I δ δ(i) tr(Q(C)) J	1
N(1 -β2)δ(i)(1-(cν⅛π)2) mρnν4
=O (m⅛4),
(35)
≤
where r = |supp(wi)| is the homogeneous coarsening parameter.
Note that C(D, I) = (I — D2)-1, where D can take Anom, Anom, Anom, or Anom. Thus, Theorem 5
is effectively bounding the difference of resolvents (zI - D2)-1 evaluated at z = 1. For n = m, we
have r = 1,ν = 1 and both an anden COincide. Lemma 5 is basically a concentration result for the
Gramians of Sfine (or Scoarse) and S fine (or S coarse); however the rate at which the difference goes to
zero is different for fine- and coarse systems. Further, smaller the stability margin 1 — β, looser are
the bounds in Eq. 34 and Eq. 35.
A.3.5 Proof of Lemma 5
We begin by proving the inequality in Eq. 34. From Eq. 2 and Lemma 1, the limit below exists with
the probability stated in the statement of lemma.
C(An
om, In×n)	, lim CT (An
om, In×n) = limT →∞ Pt=0 (Anom)t (AnTom)t
T→∞ T-1	2t	2	-1	(36)
= lim	t=0 (Anom) = (I — Anom)	.
T→∞
The last but one equality follows because A is a symmetric matrix, k Anomk2 = k OtlAA) ∣∣2 ≤ β < 1
(follows from the lemma’s hypothesis), the last one from the Neumann series formula. Also, we have
kA nomk2 = k CTfAAy k2
PTQ(C)P k ≤ √kPτQ(C)Pk∞kPτQ(C)PkI
c∙tr(PTQ(C)P)k2 —	c∙ntr(DQ(C))
≤ ----L__ < 1
—c∙tr(DQ(C)) < 1,
(37)
since [PTQ(C)P]`v — 1 for all', V ∈ [n]. Similarly
C(A nom,
In×n)
,lim CT(Anom,In×n) = (I — ^A2om)-1.
T→∞
16
Under review as a conference paper at ICLR 2022
From these observations, we establish the following identity (explanations for each step succeeds the
equations):
αn
= kC (Anom, In×n)-C(A
nom, In×n )kmax
= II(I-(Anom )2)T-(I-(A nom)2)Tllmax
=k(I - Anom)T(A2om - A濡)(1 - A2om)-1kmax
(b)
≤k(I - A2om)T∣∣∞∣∣Anom - (A2omkmaxk(I - A三加尸仙
(C)	_	_
≤ √nk (I- Anom)T k2 kAnom - Anomkmax√nk (I - AnOm)-I k2
(d)
≤ n ITAomkz kAnom - Anomkmax 1-kA 2cJ∣2
(e)
≤ ni-β2 k Anom - AnomkmaX '.F"Q(C))
(f )	1	—	—
≤ n 1-β2 k Anom - Anomkmax[k Anomkmax + ∣ |Anomk max]1
C
≤ n -1- k -A___________A_ k [________1____+ CPnkQ(C) kmax ]__
—'1 —β2 " c∙tr(A)	c∙tr(A) "max C cnpnZ(i)十 npntr(DQ(c)) 1 1-
1
1
:2心2(DQ(C))
1
1
(38)
≤ n 1—β2 C⅛) k A - Al∣max[1 + nkrAAmax] cnpn [ζ⅛) +
c2∙tr2(DQ(c))
PnkQ(C) kmax
tr(DQ(c)) ] 1-
1
1
2∙tr2(DQ(C))
nρn k Q3 kmax ]	1	[ 1
Pnntr(DQ(C))」CnPn ∣ Z(i)
≤
≤ n 1 1ο 2---ʒ— [1 +
1-β2 CnPn ζ(l)
PnkQ(C) kmax
tr(DQ(C))
c2(I - β2)ζ(l) 1 - c2∙tr2(DQ(C))
1
Pnn
PnkQ(C) kmax 1________1
tr(DQ(C)) ] 1— ɔ ɪ ≡≡
'	C	C2∙tr2(DQ(C))
O(1)
where (a)-(d),(f) follow the inequalities itemized at the beginning of the appendix; (e) is because of
the assumption in Lemma 1, Eq. 37, and that:
k(I-An2om)-1k2 ≤ 1/(1 - kAn2omk2) ≤ 1/(1 - kAnomk22) ≤ 1/(1 - β2);	(39)
and the rest of inequalities follow from definitions of Anom, Anom in the paragraphs processing Eq. 1
and Eq. 11.
The proof for the inequality in Eq. 35 follows similar lines as above, and hence, we provide a sketch,
but not the full details. From Eq. 2, note that the following
C (Anom, Im×m) , lim CT (Anom, Im×m) = limT →∞ Pt=0 Atnom Anom
= lim PtT=-01 Ae n2otm = (I- Aen2om)-1,
T→∞
e
where ∣∣Anomk2 = k WtAe) k2 ≤ β < 1 (from the lemma s hypothesis). Moreover,
2
C(Aenom,Im×m) = (I - Aenom)-1, where
∣∣e k _ k A k _ k √kΦQ(C)ΦTk∞kΦQ(C)ΦTkι n X	√mm	X 1
k Anomk2 = k Ctr(AA) k2 = k	δtr(ΦQ(C)φT)	k2 ≤ δmν2tr(Q(C)) ≤ δν2tr(Q(C))
(40)
we have
(41)
following the fact that [ΦQ(C)ΦT]ij ≤ 1.
17
Under review as a conference paper at ICLR 2022
Using these observations, we obtain the following inequality by taking similar steps as those for αn
in Eq. 38 (inner steps are removed due to redundancy) :

ee
i^n	, kC(AnOm,Im×n)	C(AnOm, Im×m) kmax
—
m ι-k(A 2omk2 ctr⅛) kA - A kmax[1 + mtrAr
m」__________1	[1 + mPnkQ(C) kmax 1 Γ___1
11 —β2 mpnV2δ(i) t m mpnV2tr(Q(C))"(≡mpnV2δ∣
][kAnom km
ax + kAnom kmax]
max
(l)
+ PnkQ(C) kmx
CmPnV'∙
，21T(Q(C))
m 1 1θ2 -----12X— [1 +
1-β2 mPnν2δ(l)
[1 + kQ(C)kmax ][ɪ +
[1 + ν2tr(Q(C))][ δ(i) +
k Q(C) kmax 1 Γ__1_____ _i_	Pn k Q(C) kmax 1___
ν2tr(Q(C))」[(≡mPnV2δ(i) ' CmPnV2tr(Q(C))」1 —(
PnkQ(C) kmx
1-k(袅))2k2
1_______
-1 , 、 )2
cν2tr(Q(C))
tr(Q(c))
N(1 - β2)δ(i)(1 - (cν2⅛π)2)
'---------------------------------}
{z^^
O(1)
1
mpnν4
≤
≤
≤
≤
1
]

]
1
1
O (mPnν4).
(42)
The proof is now complete.
A.3.6 Proof of Theorem 1
The proof of the theorem makes use of Lemma 5. Let i ∈ [m], and recall that Bi = diag(wiT) and
Bei = rWdiag(wiT). From Eq. 10 and Eq. 13, consider the following bound
∆i(A, A) , | m r%,A-1—
X(rθg(ir)Oup,A - 1)
i=1
—
rθ(i)	1
≤ |	tzgroup,A	j~
—| m
X(rθg(ir)oup,A - 1)
i=1
—
z
θ(i)	e—1
coarse, A	I
m	|
X(θ(i) e -1)
CoarSe,A
i=1
rθ⑴ ʌ - 1
group,A____।
m	|
X(rθgrθup,A- 1)
i=1
}

θ(i)
coarse,
+ | m
=∆i(A,A)
Ae - 1
—
X(θ(i)	e
coarse,A
i=1
θ(i)	- 1
COarSe,A	∣
m	|
) e -1)
COarSe,A
i=1
- 1)	X(θ(i)
coa
(43)
^^^~{^^^^―
-"W
.—
=∆i(A ,A)
}
+ I	r⅞Lp,A - 1
+ | m
—
X(rθgirOup,A - 1)	X©
i=1
θ ⑴=-1
CoarSe,A
m	||
(i)	= - 1)
CoarSe,A
i=1
z
}
bias=∆i(A),A)
_	-—
.,.	;、	. ，∙^r :、 一 .
≤ ∆i(A, A) + ∆i(A, A) + bias.
The complete proof of the bound on ∆i(A, A) will be elaborated in Thm. 3 and it follows:
∆i(A, A)
≤---------------(u)-----------
一 CminCmin||Q(C)∣∣1,l(l + Z)
[ι + kQ(C)kmx ][ ɪ + PnkQ(C) kmax ]
[ + tr(DQ(C))][ Z(i) + tr(DQ(C))]
(1 - β2)ζ(l) 1 -
1+
K||Q(c)DQ(c)||max
c2∙tr2(DQ(C))
||Q(c)DQ(c)||min(1- (
IlDQ(C)IIF
tr(DQ(C))
)2)
1
Pnm
1
|
✓
O( Pnm)
{z^^~
O(1)
(44)
18
Under review as a conference paper at ICLR 2022
_ , ---
We now derive an upper bound on ∆i(A, A).
〜 O
△i(A, A)
|m
θ(i)	-1
coarse, A
θ(i)
coarse
-1
，,A
X(θ(i)	e - 1)
CoarSe,A
i=1
X(θ(i)
CoarSe,
- 1)
i=1
m
X(θ(i)	e - 1)
CoarSe,A
i=1
m
X(θ(i)	e - 1)
CoarSe,A
i=1
m
X(θ(i)	e - 1)
CoarSe,A
i=1
coarse,
- θ(i)
A	CoarSe,
+ i=1
A +
m
X(θ(i)	e
CoarSe,A
i=1
(θ
(i)
llθcoarse,A - θcoarse,A ||max
αn
1 I	mllθcoarse,A - 111 max
1 + m
X(θ(i)
CoarSe,
- 1)
i=1
- θ(i)
CoarSe,
CoarSe,A
1+| m
i=1
(θ
(i)
- 1)
coarse,
[θ(i)	e
CoarSe,A
[θ(i)
1)
CoarSe,A
(45)
1]|
- 1]|
≤
≤
≤
1
1
1
—
m
|
m
~
m
—
~
From the equality after Eq. 40, we have
θ(i)
CoarSe,
diagi ((I- A2om)-1)
(46)
Furthermore, since ΦQ(c)ΦT ≤ 11T then (ΦQ(c)ΦT)2 ≤ 11T11T
m11T

CoarSe,A
- 1||max
k(I - Anom )	Anom kmax
tr2(ΦQ(C)ΦT)
tr2(ΦQ(C)ΦT)
k (I- Anom)Tm11Tkmax
k(I- Anom)-1k∞
tr2(ΦQ(C)ΦT)
k(I- Anom)Tk2
(47)
(mρnν2IT(Q(C))’
√mpn ν4tr2(Q(C))(1-(
3 k(I- A 2om)-1k2
1
CV2tr(Q(C))
)2).

≤
≤
≤
≤
≤
1
m
1
Similarly
m
X(θ(i)
CoarSe,
- 1)
2
tr[(I - Anom)-1 - I]
i=1
.rT r T 、—1ι
= tr[Anom (I - Anom) ]
,「：2
≥ tr[Anom]
tr[(φQ(C)ΦT)2]
(48)
tr2[ΦQ(C)ΦT]
m2ν3Wmintr[(Q(C))2]
(Wmtr[Q(c)])2
ν3Wmintr[(Q(C))2]
-W2tr2[Q(c)]-
where all inequalities follow well-known matrix norm axiom. The last line in Eq. 48 is based on
Assumption 2, and
tr[(ΦQ(C)Φt)2]	≥ ν≡minmtr[Φ(Q(c))2Φτ]
≥ VCminmmν2tr[(Q(C))2]
=m2 ν3 Cmintr[(Q(C))2]
(49)
≥
≥
2
2
19
Under review as a conference paper at ICLR 2022
similar to the derivation of Eq. 27 since for all k ∈ [K]:
[ΦTΦ]kk = £ Φ2k ≥ VE Φik ≥ νCminm
i∈[m]	i∈[m]
(50)
We use the definition of θcoarse,Ae in Eq. 13:
mm
। X(θ3e,A -	= X	((I- A- I)
i=1	i=1
= tr (I - Ae n2om)-1 - I
≥ tr(Aen2om)
ʌ- C
tr(A 2)
tr2(A)
m2Pn∣IQ(c) I∣1,1 瑞n(1+δ)
r2	(mPnδ(u))2
1	⅞in(1 + δ)IIQ(c)l∣1.1
Pnr2	δ(u))2	,
(51)
≥
(a)
≥
where the inequality (a) in Eq. 51 comes from
tr(Ae2)
=r4	E	Av' +	E	Av'Avθ'0
v∈ ∪iKi, ` ∈ ∪jKj	v6=v0∈ ∪iKi or ` 6= `0 ∈ ∪jKj
≥ r⅛ X WTAWj
i,j
=r2 ITWAWTI
≥ r12	1TΦ	Q φTι(1 + δ)
≥ Cminml K	≥Cmin m1^
≥病而吁,11,1湍n(1 + δ),
(52)
where the event defined in Eq. 30 is used whose corresponding probability is Eq. 31.
Replacing Eq. 47, Eq. 48, Eq. 51, and Eq. 42 into Eq. 45 yields:
〜 O
△i(A, A)
≤ r r2r---------δ(U)) , 、---α
≤ Pnr ≡min(i+δ)∣∣Q(c)ι∣ι,ιαn
m √mρnν4tr2(Q(C))(1-( cν2tr(Q(c))丙
ν3商mintr[(Q(C) )2]
C2tr2 [Q(C)]
(53)
The statement of the theorem follows by invoking Eq. 44 and Eq. 53 into Eq. 43:
△i( tr(A) A tr(A)A) = bias+ O (ρ1m + √⅛).
with a joint probability of at least
1 — 2exp (-2nρn(tr(DQ(C))Z)2) - exp (-2。n馆八(益口5也||Q(C)∣∣ι,ιZ)2)
— 2exp(-2m(ρnV2tr(Q(C))δ)2) — exp (-2。nm2(瑞/|Q(C)∣∣ιjδ)2)
(54)
(55)
per Lemma 3. We then get the minimum of the four exponents in Eq. 70. The proof is now complete.
20
Under review as a conference paper at ICLR 2022
A.4 Proofs for Section 6: Learning Approach for Group Average
Controllability
A.4. 1 Proof of Lemma 2
We start by defining θfine,M similar to Eq. 10 and Eq. 13 (M will be later substituted with A and A),
θfiTne,M , [tr[C(Mnom,e1)] ... tr[C(Mnom, en)]] ∈ Rn×1.
Using the Gramian definition in Eq. 2, we have:
θ(i)
θfine,M
tr [C(Mnom, ei)]
∞
tr X MnτomeieiTMnτom
τ=0
∞
tr(MnτomeieiTMnτom)
τ=0
∞
tr(eiTMn2oτmei)
τ=0
∞
(56)
(57)
diagi (Mn2oτm)
τ=0
diagi X Mn2oτm
diagi ((I - MnOm)T).
We first simplify the term θfi^,A in Eq. 60, by substituting A with PTQP introduced prior to Eq. 6:
θfine, A
tr[C(Anom, ei)]"∣	∞ ∞	∖
=	...	=diag EA 2Tm
tr[C(Anom, en)]J	V=0	)
∞∞
=diag(X(tr(PTQP)PTQP)2τ) = diag(X(ntr(DQ(C))PTQ(C)P 产)
=diag(I + (n⅛(C)) )2PTQ(C) PPTQ(C)P +(nr(⅛) )4PTQ(C)PPTQ(C) P + …)
=diag(I + 1 (tr(DQ(C)) )2PTQ(C) 1PPT Q(C)P
、{z}
,D
+1 (tr(DQQ(C)) )4PTQ(C) n PPT Q(C) 1PPT Q(C) 1PPT Q(C) P + …)
l^v^~}	|^^^—}	|^^^—}
-1 + —1— diaσ(PTQ(C) DQ(C) P + ɪ PTq(C) DQ(C)__DQ(C__DQ(C) P +，..)
=1n + ntr(DQ(c))diag(P Q tr(DQ(c))P + tr(A) P Q tr(DQ(c)) tr(DQ(c)) tr(DQ(c))P +	)
Q(C)DQ(C)	DQ(C)
=1n + nr(DQ(C)) diag(P	tr(DQ(% [I + (tr(DQ(C))) + …] P)
、------------{z-----------}
,Υ(Q(c),D)
=1n + 画D河diag(PTY(Q(C), D)P)
=1n + nr(D⅛)) (P ◦ P)Tdiag(Y(Q(C), D))
=1n + ntr⅛)) PTdiag(Y),
(58)
where (a) is due to the special structure of P Eq. 4 since for an arbitrary matrix M of appropriate
size:
diagi(PTMP) = X PkiMk,k0Pk0i = X PkiMk,k0Pk0i
k,k0∈[K]	k,k0 ∈[K]
=X PkiMk,kPki =X P2kiMk,k	(59)
k∈[K]	k∈[K]
= (Pi ◦ Pi)diag(M),
21
Under review as a conference paper at ICLR 2022
and (b) is true because P is binary. Replacing Eq. 58 into Eq. 60 yields:
θgroup,A = rW1n+
ntr(DQ(C))PTdiag(Y))) = 1(1m + ntr(D1Q(c)) Φdiag(Υ)))	(60)
The proof is now complete.
A.4.2 Theorem 3 and proof
Define the error metric similar to Eq. 14:
∆i(A, A)，
rθ(i)	1
rθgroup,A	1
rθ(i)	- 1
group,A
---------Γ^7--------- - ----------Γ^7--------
Pi=ι[rθgrOup,A - 1]	Pi=ι[r⅛Lp,A- 11
for all i ∈ [m].
(61)
Theorem 3. (Component wise error bound between θgroup,A and θgroup,A)： Let ∆i(A, A) be
defined as above and ν be the resolution parameter given by Eq. 7. Under the assumptions stated in
Section 3 and Lemma 1, the following holds:
∆i(A, A)
尸 2	[1 + kQ(C) k max ][ ɪ + PnkQ(C)kmax
Z(U)	[ + tr (DQ(c)) ][ Z(l) + tr (DQ(c) )
2minCmin||Q(C)ll1,1(I + Z)	(1 - β2)ζ(l) hɪ - c2”(D Q(C)) i
'-------------------------------------------------------V-------
O(1)
1+	K||Q(C) DQ(C)|| S____________
||Q(C)DQ(C)||mn(I - (ltD¾F )2) J Pnm
(62)
≤
with probability at least 1 一 3exp (—2κ(Q(c), D, m, n, Pn)), where Further, for a constant 0 <
ζ < 1, the exponent is K(Q(C), D,m,n,ρn) = min {nρn(tr(DQ(C))Z)2, mnpn(CminCmin||Q(C)∣∣1,1Z)2}.
A.4.3 Proposition 4 and Proof
Proposition 4. (Group Average Controllability for A) The group average controllability vectorfor
A is
θgr0up,M = J wiθfine ,M.
(63)
Proof. We begin by simplifying the group average controllability using its definition in Eq. 10 and
the definition of Gramian in Eq. 2, for a general matrix notation M which can be replaced by either
A or A:
θ(i)
group,M
tr C(Mnom, diag(wiT))
∞
tr X Mnτomdiag(wiT)diag(wiT)TMnτom
τ=0
∞
tr X Mnτomdiag((wi ◦ wi)T)Mnτom
τ=0
(a) 1
=r2
1
r2
∞
tr X MTom(	X	ev eT)MTom
τ=0	v∈supp(wi )
∞
X tr XMnτomevevTMnτom
v∈supp(wi )	τ=0
∞
1
r2
tr
v∈supp(wi ) |
MnτomevevTMnτom ,
τ=0
________ - /
θ(v)
fine,M
(64)
where (a) is due to the assumption of r-homogeneous W, ◦ denotes the Hadamard product, and we
have already defined θfine,M in Eq. 56. Putting Eq. 64 in vector form concludes the proof. □
22
Under review as a conference paper at ICLR 2022
A.4.4 Proof of Theorem 3
Proof. We start by substituting M into Eq. 63, from Proposition 4, with A and A, yields:
∆i(A, A) = ∣ m rθgr0up,AT
X(r4?oup,A ∙
i=1
—
rθ ⑶	AT
group, A
^m
-1)	Xs⅛Lp,A
∣
-1)
≤
≤
≤
≤
≤
≤
I______Wiθfine,A — 1
∣ m
i=1
wiθfine,A — 1
-------------------
m
E(Wiθfine,A - I)	E(Wiθfine,A
i=1
i=1
1
m
E(Wi θfine,A - 1)
i=1
∣wiθfine,A - 1 -
1
m
E(Wi θfine,A - 1)
i=1
1
m
E(Wi θfine,A - 1)
i=1
1
m
E(Wi θfine,A - 1)
i=1
1
m
E(Wi θfine,A - 1)
i=1
1
m
E(Wi θfine,A - 1)
i=1
1
m
E(Wi θfine,A - 1)
i=1
1
m
E(Wi θfine,A - 1)
i=1
∣
- 1)
m
E(Wi θfine,A - 1)
i=1
m
E(Wi θfine,A - I)
i=1
∣wi(θfine,A - θfine,A)-
[wiθfine,A - 1]∣
m
E(Wi θfine,A - 1)
i=1
m
E(Wiθfine,A - I)
i=1
m
-1
[wiθfine,A - 1]∣
EWi Wfine,A - θfine,A )
∣wi(θfine,A - θfine,A)∣ + ∣ i=1 m-------[wiθfine,A - 1]∣
E(Wiθfine,A -I)
i=1
m
EWi (efine,A - θfine,A)
∣wi(θfine,A - θfine,A )∣ + ∣ ^=-1-----------*(□£))小⑴①能(Y)I
X —Φ(i)diag(Y)
g ntr(DQ(C))
m
EWi (θfine,A - θfine,A)
∣wi(θfine,A - θfine,A)∣ + ∣ j="m--------Si)diag(Y)I
X Φ(i)diag(Y)
∣∣θfine,A - θfine,A∣∣max +
i=1
m∣∣θfine,A-θfine,A l∣max
m∣∣ Y Il min
∣∣Y∣∣max]
∣∣θfine,A - θfine,A ∣∣max [1 + ∣∣∣∣Υ∣∣∣∣Imax ]
α「1+ unmx]
αn [1 + ∣∣Υ∣∣minJ ,
(65)
where (a) is due to Cauchy-Schwartz inequality, an is defined in Eq. 34, θfine,A is substituted from
Eq. 58, and (b) is the result of the properties of the coarsening matrix in Definition 2. We use the
23
Under review as a conference paper at ICLR 2022
definition of θfine,A in Eq. 57:
mm
X(Wiθfine,A - I) = X Widiag((I- Anom)T- D
i=1	i=1
=ImWdiag ((I- A2om)-1 - I)
= 1mWdiag An2om
= 1mWdiag An2om
=C2⅛) ImWdiag(A2)
=c2∙tr2(A) ImWAIn
(a) EminCminPnmnuQ(C) | ∣1,1 (1 + Z)
—	@ (Z(U)Pnn)
=WminCmin|| Q(c)|| 1,1 (1 + Z) m
—	c2ζ2u)	Pnn.
(66)
inequality (a) uses the event defined in Eq. 32 is used whose corresponding probability is Eq. 33.
Using the definition of Υ in Eq. 16
||Y||max
≤ll QDQQ) (I-( trDQ⅛ )2)-1∣∣max
≤	K||Q(C)DQ(C)||max
一tr(DQ(C))(1-( 1DQQ⅛f)2)
= O(1)
(67)
||Y||min
≥ || Q(C)DQ(C) (I - ( DQ? )2)-1∣∣ ,
一|| tr(DQ(C)) (I	(tr(DQ(C)) ) )	||min
≥ ||Q(C)DQ(C)||min
一	tr(DQ(C))
=ω⑴，
(68)
where Ω(.) is the opposite scaling of O(.); We write f (m) = Ω(h(m)) iff there exist positive reals
c0 and m0 such that |f (n)| 一 c0h(m) for all m 一 m0.
We substitute Eq. 66 and Eq. 34 (with probability Eq. 24) into Eq. 65
1 +________K||Q(C)DQ(C) ||max
11Q(C)DQ(C) IUI(1-( IDiQC()Cl)lF )2)
(69)
∆i(A, A)
Z2u)
≤ EminCminI IQ(C) I ∣1,l(1 + Z)
[1+k⅜*][京+
(1-β2)Z(i) [i-
Pn k Q(C) IImaX ]
tr(DQ(C)) J
1	]
C2∙tr2(DQ(C))
1
Pnm
with a joint probability of at least
1 - 2exp (-2nρn(tr(DQ(C))Z)2) - exp (―2。不m八(益口5仙|Q(C)∣∣1,1Z)2)	(70)
per Lemma 3. We then get the minimum of the two exponents in Eq. 70 which concludes the proof.
□
A.4.5 Proof of Theorem 2
We start by using the triangle inequality for absolute values:
θ⑺	-1
group
rθgr0up,A-1
mm
X(θgrθup - 1)	X(rθgroup,A - 1)
i=1	i=1
<|	rθgrθup,A- 1
—| m
rθ(i)	1
rθgroup,A	1
rθ(i)	A - 1
group,A
m
i=1
m
)oup,AA - 1)	X(rθg(ir)oup,A - 1)
i=1
V(θ(i)	-1)
group
i=1
i=1
) A- 1)
oup,A
∆i (A,AA)
{z
,∆bi(AE ,AA)
—
—
|
|+|	θgrθup -1
|十| m
—
|
}
}
(71)
24
Under review as a conference paper at ICLR 2022
The first term on the RHS of Eq. 71 has already been bounded in Thm.3. Next, We bound the second
term in Eq. 71 and combine the two bounds at the end.
We substitute θgrθup from the output of Algorithm 1, and θgrθup A from Eq. 60, for Y defined in
Eq. 16. For notation simplicity we write Y(Q(C), D) as Y and Y(Q(C), D) as Y yields
^ , ~ —.
∆i(A, A)
*p-ι
m
X(θ(i)
/ ,Lgroup
i=1
rθ ⑶	AT
group, A
^m
-1)	X(r⅛∙0up,A
i=1
I
-1)
Φ ⑶ diag(Y)
m
X Φ⑴diag(Y)
m
X Φ⑴diag( Y)
i=1
Φ⑴diag(Y)-
i=1
m
X Φ(i)diag(Y)
i=1
m
X Φ⑴diag(Y)
i=1
Φ ⑸ diag(Y)
m
X φ(i)diag(Y)
i=1
Φ⑴ diag(Y)
I
—
1
—
(72)
We set Φ⑴=小⑶ + Eφ, Q(C) = Q(C) + Eq, D = D + Ed, and Y = Y + E where Eφ, EQ and
ED are error matrices of appropriate sizes. Substitution of theses error matrices into Eq. 72, as well
as multiple applications of the triangle inequality, gives:
^ , ` —.
∆i(A, A)
1
m
X Φ(i)diag(T)
i=1
1
m
X Φ(i)diag(T)
i=1
1
m
X Φ(i)diag(Y)
i=1
1
m
X Φ(i)diag(Y)
i=1
m
X Φ(i)diag(T)
i=1
m
X(φ(i) + Eφ)diag(T + E)
(φ(i) + Eφ)diag(T + 的-M~m----------------------------Φ(i)diag(T)
X Φ(i)diag(T)
i=1
m
X®(i) + Eφ)[diag(T) + diag(E)]
(Φ⑶ + Eφ)[diag(Υ) +diag(E)] - M------------m-----------------------Φ(i)diag(T)
X φ(i)diag(T)
i=1
m
X^ Φti'diag(E) + mEφ[diag(T) + diag(E)]
φ(i)diag(E) + Eφ[diag(Υ) +diag(E)] - W---------------m--------------------------Φ(i)diag(T)
X Φ(i)diag(T)
i=1
m
^X φ(i'diag(E) + mEφ[diag(Υ) + diag(E)]
[φ(i) + Eφ]diag(E) + [Eφ -0---------------m--------------------------Φ(i)]diag(Υ)
X Φ(i)diag(Υ)
i=1
m
^X φ(i'diag(E) + mEφ[diag(Υ) + diag(E)]
(1 + ∣∣Eφ∣∣1)∣∣E∣∣max + [∣∣Eφ∣∣1 + ^1--------m---------------------------]||丫扃.
X Φ(i)diag(Υ)
i=1
(73)
To further simplify Eq. 73, we find upper bounds on the terms inside. The two terms IIYIImax and
∣∣Y∣∣min have already been bounded in Eq. 67 and Eq. 68 and we have:
	1	 ≤_U_
口 ʌ ,.,____________________________八 m m||diag(Y)Umin
VΦ (i)diag(Y)
i=1	1	(74)
≤	,11,,
m∣∣ Y Il min
=O( m)
The following inequality holds from the definition of norms:
I∣Eφ∣∣1 ≤ K∣∣Eφ∣∣max	(75)
25
Under review as a conference paper at ICLR 2022
Using Eq. 74 and Eq. 75, the inner term in the last line of Eq. 73 is simplified as:
m	m
X Φ⑴diag(E) + mEφ [diag(Υ) + diag(E)]	X Φ⑴diag(E)
i=1_______________________________________________ = i=1___________________, m Eφ[diag(Υ) + diag(E)]
m	m	+ m m
X φ(i)diag(Υ)	X φ(i)diag(Υ)	X Φ(i)diag(Υ)
i=1	i=1	i=1
≤ m||diag(E)||mx ,	K ∣∣Eφ ∣∣max[∣∣diag(Υ)∣∣max + ∣∣diag(E)∣∣mx]
一m∣ diag(γ)∣∣min 十	_ m∣∣diag(Υ)∣∣mi∏
V ∣∣ E ∣ max I TA∣∣Eφ∣∣mx[∣∣Υ∣∣mx+∣∣E∣∣
max]
≤ ∣∣Υ ∣min + K	_	∣∣Υ∣∣min
=O( ∣Eφ∣∣max + IlEllmaX)
(76)
Replacing Eq. 67, Eq. 68, Eq. 74, Eq. 75, and Eq. 76 into Eq. 73 simplifies it as:
,	θ ⑸-1
_____group____
I m
X(θ(i) - 1)
/ ,「group	/
i=1
rθ ⑸ ʌ-1
group,A
m
X(rθgrθup,A-1)
i=1
I = O( mm [llEΦ Ilmax + ∣∣E∣∣max])∙
(77)
—
We now simplify the term ∣∣E∣∣max in Eq. 79:
llE Umax
∣∣Υ - Υ∣∣max
Il
tr(D Q (C))
(I-( D^(C) )2)-1
((tr(DQ(C))) )
ʌ ʌ
Q(C)DQ(C)
tr(D Q (C))
∣∣Q(C)DQ(C)(I -(
D Q(C)
tr(DQ(C))
tr(DQ(C))
)2)-1
(I - (tr(E)Q(c)) )2)-1Hmax
⅛Q⅛ Q(C)DQ(C)(I-(
DQ(c)
tr(DQ(C))
)2 )-1∣∣max
(78)
Q (C) D Q (C)
—
1
—
To continue the simplification of ∣∣E∣∣max, We bring the two error matrices EQ and ED introduced in
the statement of the theorem into play:
(Q(C) + EQ) (D + ED)(Q(C) + EQ) = (Q(C) + EQ)(DQ(C) + DEQ + EDQ(C) + EDEQ)
=Q(C)DQ(C) + Γ,
(79)
(80)
(81)
where we define
Γ , Q(c)DEq + Q(C)EDQ(C) + Q(C)EDEQ + EQDQ(C) + EQDEQ + EQEDQ(C) + EQEDEQ
and
(D + ED) (Q(C) + Eq) = DQ(C) + DEq + EDQ(C) + EdEq .
X--------------------------------------------V----------}
E
Replacing Eq. 79 and Eq. 81 into Eq. 78 yields:
llEIImax
1
tr(D Q (C))
1
tr(D Q (C))
∣∣(Q(C)DQ(C) + Γ)(I - (SDQCyy)2)-1 - tr(DQQ)CtE)Q(C)DQ(C)(I -(
IlQ(C)DQ(C)[(I - (SlDg)J)2)T-(I - (SDQ⅛)2)-1]
+γ(i - (⅛Q⅛)2)-1 - tr⅛⅛)Q(C)DQ(C)(I - (tr(D⅛⅛)2)-1II
DQ(c)
tr(DQ(C))
max
)2)-1
∣∣max
1
tr(D Q (C))
∞
[∣∣Q(C)DQ(C)[X(
2=1
D Q(C)
-ʌ ~ʌ - ^^Γ-
tr(D Q(C))
)2'
DQ(C)
-(tr(DQ(C))
Y
■\z
,eι
+γ(i - (⅛Q⅛)2)-1 - tr⅛⅛))Q(C)DQ(C)(I - (tr⅞⅛⅛)2)-1I∣max]
≤ tr(⅛) [∣lQ(C)DQ(C)IImaxIHIImax + ∣∣Γ∣∣max∣∣(I - (tr(DQ^)2)-1∣∣max
+ ⅛DQ⅛ IlQ(C)DQ(C)∣∣max∣∣(I-( ⅛Q‰ )2)-1∣∣max]
=0(ll∈1llmax + HrHmax + tr(E))
=O(IIE1 Umax + HrHmax + H EIImax)
(82)
26
Under review as a conference paper at ICLR 2022
We define another error term:
22
(DQ(C) + E)22 =(DQ(C)产 + X ∙∙∙
ν=1
l^{Z^}
,R=O(E)
(83)
We can then simplify e1 for ' ≥ 1 as:
∞	1 ∣∣eι∣∣max	= ∣∣ =(tr(D Q (C))产	(DQ(C))22 -(	tr(DQ(C)) )22(DQ(C))2； tr(DQ(C)))(DQ )		| max		
∞	1 =∣∣ X ——,、 2=1 (tr(D Q (C))产	(DQ(C))22 + R-(1+ tr(DQt)))依DQ(C))22				| max	
∞	1 =∣∣ 2=1 (tr(D Q (C)))22	RT消品产(DQ(C)产		| max			
∞ =∣∣∣(1 - (tr(D Q (C)))2)-1 - 1]R - X 2=1 =∣∣[(1 - (tr⅛) )2)T- 1]r-[(i- ≤ [(1 - (tr⅛) )2)T- 1]∣∣R∣∣max + =O(∣∣R∣∣max ) =O(IIE∣∣max)		(——二(R)…，、)22(DQ(C))22 'tr(DQ(C))tr(D Q (c)) (S*(R)DQ(C)) DQ(C))2)-1- I]∣ ∣[(1 - (CRDQ(C)) DQ(C))2)-1				| max max -I] ∣∣max (84)
Similarly, we can rewrite an upper bound on Ilrkmax as:
IIrlImaX	= K 2(∣∣Q(C)DIImaxlIEQIlmax + IIQ(C)IImaxkEDIImaxllQIImax + IIQ(C)IImaxkEDIlmax IIEQIImax
+ kEQIImaxllDQ(C)IImax + IIEQkmaXkDkmaxkEQkmax
+ ∣∣ EQkmaxkEDkmaxkQ(C) I max + IlEQkmaxkEDkmaxkEQkmax)
=K 2(IIEQkmax + IIEDkmaX + IIEDkmaxkEQkmax + IIEQkmax + IIEQkmax
+ kEQkmaxkEDkmax + ||EQkmaxkED ∣∣max)
=O(∣∣EQ∣∣max + ∣∣ED∣∣max),
(85)
and
IlEkmax = kDEQ + EDQ(C) + EDEQkmax
=K (IDkmaxkEQkmax + |旧口 kmaxk Q(C)IImax + k Ed kmax k Eq kmax)	(86)
= K (kEQ kmax + kED kmax + kED kmaxkEQkmax)
=O(∣∣EQ∣∣max + ∣∣ED∣∣max)∙
By substituting Eq. 84, Eq. 86, and Eq. 85 into Eq. 82, we get:
∣∣E∣∣max = O(∣∣EQIImaX + ∣∣ED∣∣maX).
Replacing Eq. 87 into the original error in Eq. 79 yields:
θ(i) —1	rθ(ii	— -1	八
∣ m grθup---------F-aA--------------∣ = O ( ml [∣∣Eφ∣∣max + ∣∣Eq ∣∣max + ∣∣Ed ∣∣max])
X(眼UP- 1)	X(r⅛lp,A - 1)
i=1	i=1
(87)
(88)
which happens with the same lower bound probability as in Eq. 70. The proof is now complete.
27