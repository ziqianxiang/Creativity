Under review as a conference paper at ICLR 2022
Sharper Utility Bounds for Differentially
Private Models
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, by introducing Generalized Bernstein condition, We propose the first
O (np) high probability excess population risk bound for differentially private al-
gorithms under the assumptions G-Lipschitz, L-smooth, and Polyak-Eojasiewicz
condition, based on gradient perturbation method. If We replace the properties
G-Lipschitz and L-smooth by α-Holder smoothness (which can be used in non-
smooth setting), the high probability bound comes to O (n- 1+αα) w.r.t n, which
cannot achieve O (1/n) when α ∈ (0, 1]. To solve this problem, we propose a
variant of gradient perturbation method, max{1, g}-Normalized Gradient Per-
turbation (m-NGP). We further show that by normalization, the high probability
excess population risk bound under assumptions α-Holder smooth and Polyak-
Eojasiewicz condition can achieve O(W), which is the first O (1/n) high proba-
bility utility bound w.r.t n for differentially private algorithms under non-smooth
conditions. Moreover, we evaluate the performance of the new proposed algorithm
m-NGP, the experimental results show that m-NGP improves the performance
(measured by accuracy) of the DP model over real datasets. It demonstrates that
m-NGP improves the excess population risk bound and the accuracy of the DP
model on real datasets simultaneously.
1 Introduction
Machine learning has been widely used and found effective in many fields in recent years (Singha
et al., 2021; Swapna & Soman, 2021; Ponnusamy et al., 2021). When training machine learning
models, tremendous data was collected, and the data often contains sensitive information of individ-
uals, which may leakage personal privacy (Shokri et al., 2017; Carlini et al., 2019).
Differential Privacy (DP) (Dwork et al., 2006; Dwork & Lei, 2009; Dwork et al., 2014) is a theo-
retically rigorous tool to prevent sensitive information. It introduces random noise to the machine
learning model and blocks adversaries from inferring any single individual included in the dataset by
observing the model. The mathematical definition of DP is well accepted and relative technologies
are performed by Google (Erlingsson et al., 2014), Apple (McMillan, 2016) and Microsoft (Ding
et al., 2017). As such, DP has attracted attention from the researchers and has been applied to nu-
merous machine learning problems (Ullman & Sealfon, 2019; Xu et al., 2019; Bernstein & Sheldon,
2019; Wang & Xu, 2019; Heikkila et al., 2019; Kulkarni et al., 2021; Bun et al., 2021; Nguyen &
Vullikanti, 2021).
There are mainly three approaches to guarantee differential privacy: output perturbation (Chaud-
huri et al., 2011), objective perturbation (Chaudhuri et al., 2011), and gradient perturbation (Song
et al., 2013). Considering that gradient descent is a widely used optimization method, the gradient
perturbation method can be used for a wide range of applications, and adding random noise to the
gradient allows the model to escape local minima (Raginsky et al., 2017), we focus on the gradient
perturbation method to guarantee DP in this paper.
In this paper, we aim to minimize the population risk, and measure the utility of the DP model by
the excess population risk. To get the excess population risk, an important step is to analyze the gen-
eralization error (the reason is demonstrated in Section 3). Complexity theory (Bartlett et al., 2002)
and algorithm stability theory (Bousquet & Elisseeff, 2002) are popular tools to analyze the gener-
alization error. On one hand, Chaudhuri et al. (2011) applied the complexity theory and achieved
1
Under review as a conference paper at ICLR 2022
an O( max{√1n, 2/3^}) high probability excess population risk bound under the assumption of
strongly convex; Kifer et al. (2012) achieved O( *) expected excess population risk bound via
complexity theory. On the other hand, the sharpest known high probability generalization bounds
for DP algorithms analyzed via stability theory under different assumptions (WU et al., 2017; Bass-
ily et al., 2019; Feldman et al., 2020; Bassily et al., 2020; Wang et al., 2021) are O(√p + √n) or
O(√2 * 4np), containing an inevitable O(√1n) term, which is a bottleneck on the utility analysis. Thus,
we are focusing on the following question, which is still an open problem:
Can we achieve the high probability excess risk bounds with rate O(3)for differentially private
models via uniform stability?
This paper answers the question positively under more (or different) assumptions and provides the
first high probability bound allowing an O(n)rate of convergence in the setting of DP. By in-
troducing Generalized Bernstein condition (Koltchinskii, 2006), We remove the O (√n) term in
the generalization error and furthermore improve the high probability excess population risk bound.
Comparing with previous high probability bounds, the improvement is approximately up to O (√n).
Contributions
We first prove that by introducing Generalized Bernstein condition (Koltchinskii, 2006), under the
assumptions G-Lipschitz, L-smooth, and Polyak-Ecjasiewicz (PL) condition, the high probability
excess population risk bound can be improved to O(Wp). To the best of our knowledge, this is the
first O(噜)high probability excess population risk bound in the field ofDP.
Then, we relax the assumptions G-Lipschitz and L-smooth, by introducing α-Holder smooth. Un-
der these assumptions, we prove that the high probability excess population risk bound comes to
O( W n ι+α ).Considering that α ∈ (0,1], the result cannot achieve O(Jnp).
To overcome the bottleneck, we design a variant of gradient perturbation method, called max {1, g}-
Normalized Gradient Perturbation (m-NGP) algorithm. Via this new proposed algorithm, we
prove that under the assumptions α-Holder smooth, PL condition, and generalized Bernstein condi-
tion, the high probability excess population risk bound can be improved to O(np). To the best of
our knowledge, this is the first O (np) high probability excess population risk bound for non-smooth
loss in the field of DP.
Moreover, to evaluate the performance of our proposed max {1, g}-Normalized Gradient Perturba-
tion algorithm, we perform experiments on real datasets, the experimental results show that m-NGP
method also improves the accuracy of the DP model on real datasets.
The rest of the paper is organized as follows. We discuss some related work in Section 2. Some pre-
liminaries are formally introduced in Section 3. In Section 4, we propose sharper utility bounds
under different assumptions and design a variant of gradient perturbation method, max {1, g}-
Normalized Gradient Perturbation. The experimental results are shown in Section 5. Finally,
we conclude the paper in Section 6.
2 Related Work
Dwork et al. (2006) proposed the mathematical definition of DP for the first time. Then, it was
developed to protect the privacy in the field of machine learning (e.g. Empirical Risk Minimization
(ERM)) via output perturbation, objective perturbation, and gradient perturbation methods. For
DP-ERM formulations, Chaudhuri et al. (2011) first proposed output perturbation and objective
perturbation methods, and Song et al. (2013) first proposed the gradient perturbation method. Based
on these works, Kifer et al. (2012); Bassily et al. (2014); Abadi et al. (2016); Wang et al. (2017);
Zhang et al. (2017); Wu et al. (2017); Bassily et al. (2019); Feldman et al. (2020); Bassily et al.
(2020) further improved the results under different assumptions.
2
Under review as a conference paper at ICLR 2022
Table 1: Previous excess population risk bounds and ours under different assumptions
	Assumptions	Method	Utility Bound
Bassily et al. (2019)	Lipschitz, smooth, convex	Gradient	O (√+√n)
Feldman et al. (2020)	Lipschitz, convex	Gradient	O (√+√n)
Bassily et al. (2020)	Lipschitz, convex	Gradient	O (√+√n)
Wang et al. (2021)	α-Holder smooth, convex	Gradient	O (√+√n)
Wang et al. (2021)	α-Holder smooth, convex	Output	O (篇
Ours	Lipschitz, smooth, PL condition	Gradient	o (√)
Ours	α-Holder smooth, PL condition	Gradient	O (-√α^) ∖nl+αe )
Ours (m-NGP)	α-Holder smooth, PL condition	Gradient	o (√)
1 In Table 1, n is the size of the dataset, is the privacy budget, and p is the dimension of the data.
Among the works mentioned above, some of them only analyzed the privacy guarantees (Song et al.,
2013; Abadi et al., 2016), some of them only discussed the excess empirical risk bound (Wang et al.,
2017; Zhang et al., 2017; Wu et al., 2017). Some works discussed the excess population risk un-
der expectation, from different points of view, such as complexity theory, optimization theory, and
stability theory: Kifer et al. (2012) achieved an O( Jnp) expected excess population risk bound via
complexity theory; Bassily et al. (2014) achieved similar expected bound under convexity assump-
tion, via optimization theory; and Wang et al. (2019) proposed an O (ɪogppɔ expected excess pop-
ulation risk bound under non-convex condition, via Langevin Dynamics (Gelfand & Mitter, 1991)
and the stability of Gibbs algorithm.
Considering that the high probability bound is more concerned by researchers, we focus on the
high probability utility bound. Meanwhile, we concentrate on the stability theory in this paper.
Among many notions of stability, uniform stability is arguably the most popular one, which yields
exponential generalization bounds. Via uniform stability, the high probability excess population risk
bounds under different assumptions given by previous works all contain an O (√n) term, details can
be found in Table 1. The reason is that when analyzing the generalization error, the technical routes
followed works Bousquet & Elisseeff (2002); Hardt et al. (2016).
In this paper, by introducing Generalized Bernstein condition (Koltchinskii, 2006), we remove the
O (√n) term from the generalization error, and further improve the excess population risk bound of
DP models. The improved convergence rate is up to O(np), which positively answers the question:
Can the high probability excess population risk bound achieve O (1/n) w.r.t n. The improvements
are shown in Table 1.
Table 1 first shows that by adding more assumptions (we assume the loss function to be Lipschitz,
smooth, and satisfy Polyak-Eojasiewicz (PL) condition, while previous results require α-Holder
smoothness and convexity), we achieve a better high probability excess population risk bound,
O(np), which is state-of-the-art to the best of our knowledge. Then, we replace the Lipschitz
and smooth property by α-Holder smoothness and achieve O ( 2p ) high probability excess pop-
n 1+2a J
ulation risk bound, when α ∈ [ɪ, 1], our result is better than previous ones, but it cannot achieve
3
Under review as a conference paper at ICLR 2022
the same bound (O (1/n) w.r.t n) under the condition that the loss function is Lipschitz, smooth,
and satisfies PL condition. To overcome it, We propose an algorithm called m-NGP, and achieve the
O(^np) result under the same assumptions: α-Holder smooth and PL condition.
Moreover, although it is hard to directly compare PL condition With convexity, PL condition can be
applied to many non-convex conditions (more information can be found in Section 4.2). So, in this
paper, We analyze the utility bound of DP algorithm under cases different from previous scenarios.
3 Preliminaries
In this paper, we assume that there are n data instances in dataset D, i.e. D = {zι,…，zn} where
z = (x, y) With input x ∈ X and label y ∈ Y, and Z = X × Y. The data space is denoted by D and
the parameter space is denoted by C, the loss function ' is defined as '(∙, ∙) : D×C → R. Databases
D, D0 ∈ Dn differing by one data instance are denoted as D 〜 Dl called adjacent databases. For
a given vector X = [xι,…,xd]τ, its '2-norm is ∣∣xk2 = (£. ɪ ∣Xi∣2)1. And A . B represents
that there exists c > 0, A ≤ cB .
Definition 1 (Differential Privacy (Dwork et al., 2006)). A randomized algorithm: A : Dn → Rp is
(e, δ)-differentialprivacy (DP) iffor all D 〜D0 and events S ∈ range(A):
P[A(D) ∈ S] ≤ eP [A(D0) ∈ S] +δ.
Definition 1 implies that the adversaries cannot infer whether an individual participates when train-
ing the machine learning model, because essentially the same distributions will be drawn over any
adjacent datasets. Some kind of attacks, such as membership inference attack, attribute inference
attack, and memorization attack, can be thwarted by DP (Backes et al., 2016; Jayaraman & Evans,
2019; Carlini et al., 2019).
Throughout this paper, we focus on gradient perturbation method to guarantee (, δ)-DP, the
paradigm is based on gradient descent: at iteration t,
θt - θt-1 - ηt (▽&Rn(θt-1) + b) ,	(I)
where ηt is the learning rate, b is the random noise injected into the gradient, θ is corresponding
model with privacy, and Rn(θ) is the empirical risk, defined as Rn(θ) := n Pn=ι '(zi, θ).
In this paper, we focus on minimizing the population risk: R(θ) = Ez〜D ['(z, θ)]. In the setting of
DP, the excess population risk is defined by R(θ) - minθ∈C R(θ), which can be decomposed into:
,ʌ . . . , ʌ . , ʌ . , ʌ . .. .. ..
R(θn) — R(θ*) = R(θn) — Rn(θn) + Rn(θn) — Rn(θ*) + Rn(θ*) - R(θ*)
≤ R(θn) — Rn(θn) + Rn(θn) — Rn(θ<) +Rn (θ*) — R(θ*),	⑵
X------{z------} X--------V-------}
GE	OE
where θ* = argmi□θ∈c R(θ), θnrb = arg minθ∈c Rn(θ), and the last inequality is becasuse of the
definition of θnn. In (2), Ge,OE mean the generalization error and the optimization error (also called
the excess empirical risk), respectively. Inequality (2) answers the question mentioned in Section 1:
Why generalization error is an important step towards excess population risk.
To get the generalization error, algorithm stability theory is a popular tool, in which uniform stability
yields exponential generalization bounds and is commonly used.
Definition 2 (Uniform Stability (Bousquet & Elisseeff, 2002)). An algorithm θn is γ-uniformly
stable iffor any z, zι, ∙∙∙ ,zi,…，Zn,z[ ∈ Z and i = 1, ∙∙∙ , n ,it holds that
∣'(z, θn(zi,…，Zn)) — '(z, θn(zi,…，Zi-1, Z；, Zi+1,…，Zn)) | ≤ Y
In this paper, we use notation θn for both algorithm and model parameter. By Definition 2, it is easy
to follow that the uniform stability measures the upper bound of the difference (on the loss function)
between the models derived from adjacent datasets.
Assumption 1 (G-Lipschitz). The loss function ` : D × C → R is G-Lipschitz over θ if for any
z ∈ D and θι, θ? ∈ C, we have: |'(z, θι) 一 '(z, θ2)∣ ≤ G∣θι 一 θ2∣∣2.
4
Under review as a conference paper at ICLR 2022
Assumption 2 (L-smooth). The loss function ` : D × C → R is L-smooth over θ if for any z ∈ D
and θ1,θ2 ∈ C, wehave: kVθ'(z, θι) - Vθ'(z,θ2)∣∣2 ≤ L∣∣θι - θ2∣∣2.
If 'is differentiable, smoothness yields: '(z, θι) -'(z,θ2) ≤(V&'(z,θ2),θ1 - θ2i + LL ∣∣θι - θ2∣∣2.
Assumptions G-Lipschitz and L-smooth are commonly used in the utility analysis of DP machine
learning (Chaudhuri et al., 2011; Kifer et al., 2012; Abadi et al., 2016; Bassily et al., 2019; Feldman
et al., 2020; Bassily et al., 2020). To relax the Lipschitz and smoothness assumptions, we introduce
the α-H0lder smoothness of the loss function:
Assumption 3 (α-Holder smooth). Let α ∈ (0,1]. The loss function ' : D × C → R
is α-HoIder Smooth over θ with parameter H if for any Z ∈ D and θ1,θ2 ∈ C, we have:
∣Vθ'(z,θι) - Vθ'(z, θ2)∣∣2 ≤ H∣∣θι - θ2kα.
Lemma 1. Ifthe lossfunction '(∙, ∙) isdiferentiable,then AssumPtion 3 yields '(z, θι) — '(z, θ2) ≤
hVθ '(z,θ2),θ1 - θ2i + H ∣θι- θ2kα+1.
By the definition, it is easy to follow that if α = 1, it is equivalent to H-smooth; and if α → 0,
it satisfies the Lipschitz property given in Assumption 1. Besides, with bounded parameter space,
i.e. ∣C∣2 ≤ MC, ɑ-Holder smoothness immediately implies max{2HMc, H}-Lipschitz. More-
over, Assumption 3 instantiates many non-smooth loss functions. For example, the q-norm hinge
loss `(z, θ) = (max (0, 1 - yhθ, zi))q for classification and the q-th power absolute distance loss
'(z,θ) = |y - hθ, z)|q for regression (Lei & Ying, 2020a), whose ' are (q - 1)-Holder smooth if
q ∈ (1, 2] (Li & Liu, 2021). Lemma 1 shows that Holder smoothness shares similar property with
smoothness defined in Assumption 2, details of the proof can be found in Appendix A.1.
4 Sharper utility bounds for differentially private models
4.1 Privacy Guarantees
Before analyzing the excess population risk bound, we first discuss the privacy guarantees in this
section. Abadi et al. (2016) proposed the moments accountant method to measure the privacy costs
of DP model training by stochastic gradient descent (SGD), Wang et al. (2017) further analyzed it
under the setting of gradient descent (GD). In this paper, we focus more on the utility analysis, to
improve the excess population risk, so we directly apply it to the gradient perturbation method.
Lemma 2 (Wang et al. (2017)). In gradient perturbation method in (1), for , δ > 0, it is (, δ)-DP
ifthe random noise b is zero mean Gaussian noise, i.e. b 〜N (0, σ2Ip), and for some ConStant c, *
σ = cG⅛≡
n22
(3)
Remark 1. (3) assumes the loss function to be G-Lipschitz. Ifwe only assume that '(∙, ∙) is α -HOlder
smooth with parameter H, then G can be replaced by max{2H MC, H} as discussed above.
4.2 Analysis of the excess population risk
To remove the O(1/√n) term in previous results, We further need the Generalized Bernstein condi-
tion when analyzing the excess population risk.
Assumption 4 (Generalized Bernstein condition (Koltchinskii, 2006)). We say the loss function `
satisfies the generalized Bernstein condition iffor some B > 0 for any θ ∈ C, we have:
E h('(z,θ) - '(z, θ*))2] ≤ B (R(θ) - R(θ*)).
Assumption 4 is a general condition, if the loss function '(∙, ∙) is G-Lipschitz and bounded by
m`, then many loss functions satisfy the generalized Bernstein condition, such as exponential loss
function, logistic loss function, quadratic loss function, truncated quadratic loss, and hinge loss
(Bartlett et al., 2006; Steinwart & Christmann, 2008).
Most of the previous works assumed that the loss function is convex (or strongly convex) when
analyzing the optimization error (the excess empirical risk) Rn(θn) - Rn(θ^). In this paper, We use
the Polyak-Ecjasiewicz (PL) condition to replace the convexity assumption.
5
Under review as a conference paper at ICLR 2022
Assumption 5 (Polyak-Eojasiewicz condition). The empirical risk Rn(θ) satisfies the Polyak-
Lojasiewicz (PL) condition ifthere exists μ > 0 andfor ^very θ,
kVθRn(θ)k2 ≥ 2μ (Rn(θ) - Rn(θn)).
The Polyak-Eojasiewicz condition is one of the weakest curvature conditions, so all the results given
in this paper can be expanded to strongly convex conditions. (Karimi et al., 2016; Li & Liu, 2021),
weaker than ‘one-point convexity’ (Kleinberg et al., 2018), ‘star convexity’ (Zhou et al., 2019), and
‘quasar convexity’ (Hinder et al., 2020). It is widely used in the analysis of non-convex learning
(Wang et al., 2017; Charles & Papailiopoulos, 2018; Lei & Ying, 2020b; Lei & Tang, 2021) and
many popular non-convex objective functions satisfy the PL condition, such as: matrix factorization
(Liu et al., 2016), robust regression (Liu et al., 2016), neural networks with one hidden layer (Li &
Yuan, 2017), mixture of two Gaussians (Balakrishnan et al., 2017), ResNets with linear activations
(Hardt & Ma, 2017), linear dynamical systems (Hardt et al., 2018), phase retrieval (Sun et al., 2018),
and blind deconvolution (Li et al., 2019).
Remark 2. With G-LiPSchitz and λ-strongly convex, we have E[('(z, θ) 一 '(z, θ*))2] ≤ G2∣∣θ —
θ*k2 ,and R(θ) — R(θ*) ≥ 2 ∣∣θ — θ*∣∣2, which implies E[('(z,θ) — '(z,θ*))2] ≤ (2G2∕λ)(R(θ)—
R(θ*)), Assumption 4 is naturally satisfied. And PL condition can be directly derivedfrom strongly
convex (Karimi et al., 2016), so all strongly convex loss functions satisfy Assumptions 4 and 5 si-
multaneously and all the results given in this paper can be directly extended to strongly convex
condition. Expect for strongly convex functions, several interesting machine learning setups also
satisfy Assumptions 4 and 5. (1) 1-layer neural networks with a squared error loss and leaky ReLU
activations. Charles & Papailiopoulos (2018) shows that 1-layer neural networks with a squared
error loss and leaky ReLU activations satisfy Assumption 5, and Bartlett et al. (2006) shows that
quadratic functions satisfy Assumption 4, so (1) holds. (2) Loss functions of least squares minimiza-
tions. Charles & Papailiopoulos (2018) shows that least squares minimization satisfy Assumption
5 and Bartlett et al. (2006) shows that the quadratic functions satisfy Assumption 4, so (2) holds.
(3) Squared piecewise-linear functions with regularized term. Bartlett et al. (2006) shows that the
composition of strongly convex functions with piecewise-linear functions satisfy Assumption 5, and
Bartlett et al. (2006) shows that squared piecewise-linear functions satisfy Assumption 4. We prove
that if a function satisfies Assumption 4, then with regularized term λ∣θ∣22, it also satisties Assump-
tion 4 (details can be found in Appendix A.5). Thus, (3) holds.
Theorem 1. If Assumptions 1, 2, 4 and 5 hold, the loss function is bounded, i.e. 0 ≤ '(∙, ∙) ≤ m` ,
taking σ given by Lemma 2, T = O (log(n)), ηι =…=ητ = L, if Z ∈ (exp(—p∕8), 1), then with
probability at least 1 — ζ:
R(θn) - R(吟 ≤ ci G2P 勺2og(10(1 + (1/C 1)2
+ c2 ( G2 log2(n) + B + M' )
2n	n
+ QG2log2.5(n)PpIOg(I/6 Λ +(8iog(i/z))1/4!
3 n	∖	∖ P J 厂
for some constants c1 , c2 , c3 > 0.
Detailed proof can be found in Appendix A.2, we give a proof sketch here. First, we discuss the
stability of the gradient perturbation based DP algorithm and show that it is O (Tη∕n) uniformly
stable w.r.t n with high probability. Then, we analyze the generalization error via stability theory.
Meanwhile, via Assumption 4 and its moments bound, we couple term R(θn) — Rn(θn) (the gener-
alization error of θ) and term Rn(θ*) — R(θ*) in (2) together, to remove the O (1/√n) term in the
generalization error. In this way, a better excess population risk bound is achieved by combining the
optimization error together.
The proof is motivated by Klochkov & Zhivotovskiy (2021) in the non-private case. The key
challenges include that in the setting of DP, the random noise is injected into the algorithm.
In Klochkov & Zhivotovskiy (2021), a key step to analyze the generalization error is summing
6
Under review as a conference paper at ICLR 2022
Xi = E0 ['(zi,。；) - '(zi, θ*)] for i = 1,…,n, where θ'n is derived from an independent copy of
the original dataset and E0 means the expectation taken over the independent copy. When summing,
Xi is required to be zero mean. However, in the cases of DP, ifwe replace θn0 by θn0 , then Xi are not
zero mean. Besides, for output perturbation, a common way to decompose the excess population
∏skis R(θn) - R(θ ) ≤ R(On)- R(θn) + R(On)- Rn(θn ) + Rn(θn ) - Rn(θn ) + Rn(θ ) - R(θ ),
which naturally solves the problem mentioned above (because the generalization error is discussed
over the non-private model). However, when it comes to the gradient perturbation method, we can-
not solve the problem easily in this way, because the random noise is coupled with the gradient. So,
we decouple the noise terms and overcome the challenge by the moment Bernstein inequality.
By Theorem 1, it is easy to follow that with high probability, R(θn - R(θ*) = O(√np), which is
the first O (1/n) high probability excess population risk bound over DP algorithm w.r.t n, to the best
of our knowledge.
Theorem 2. If Assumptions 3, 4, 5 hold, the loss function and the parameter space are bounded,
i.e. 0 ≤ '(∙, ∙) ≤ m`, ∣∣C∣∣2 ≤ Me. Taking σ given by Lemma 2, T = O (n 1+2α), and η =旧；嗡,
where K ≥ 吗,。,if Z ∈ (eχp(-p/8), 1), then Withprobability at least 1 — Z:
R(θn) - R(θ*) ≤ C1 G02p≡≡ (+ + (8⅛≡)”!
n 1+2α E	∖	∖ P J J
+ W ( G02 log2(n) + B + M' ]
2n	n
+ CF0 log2 (n) Pp log(1∕δ) Λ + (8log(1∕Z) y∕4!
3	n ι⅛ E	y k P ).,
for some constants c1, c2, c3 > 0, where G0 = max{2H MC, H}.
Detailed proof can be found in Appendix A.3. The proof is similar to Theorem 1, the challenge
is that the properties G-LiPSchitz and L-smooth are replaced by the assumption α-Holder smooth
when analyzing the optimization error (the excess empirical risk). To overcome the challenge, we
use Lemma 1 to bound the optimization error and Young’s inequality is used to normalize the expo-
nential rate, details are shown in the Appendix.
By Theorem 2, it is easy to follow that with high probability,
,ʌ . ..
R(θn) - R(θ*) = O
By the definition of ɑ-Holder smooth, a ∈ (0,1],soif α ∈ [2, 1],
R(θn) - R(θ*) = O (nι⅛) ≤ O (n-2)
w.r.t n, which implies that our result is better than previous results when α ∈ [2-, 1].
Via the discussion mentioned above, we observe that under the assumption α-Holder smooth, our
result is better than O(1∕√n) w.r.t n only in the case that α ∈ [2, 1]. Besides, the best result is
O (n-2∕3), which comes when α = 1. And it cannot achieve the convergence rate O(np). The
reason is that when applying Young’s inequality in the optimization error analysis, an additional
term Hη2(α+1-α) appears, leading a loose excess population risk bound.
Motivated by this, we design a variant of gradient perturbation method given in (1), called
max{1, g}-Normalized Gradient Perturbation DP algorithm, to overcome the loose excess pop-
ulation risk bound. Details are shown in Algorithm 1.
Remark 3. The difference between Algorithm 1 and (1) is that in lines 4 and 5, we normalize the `2-
norm of the gradient to 1 ifit is less than 1. In this way, we can ‘bypass’ the Young’s inequality when
scaling ∣∣θt — θnbk2+α (derivedfrom Lemma 1), further remove term Hηt(α+1-α) in the theoretical
analysis. Details can be found in Appendix A.4.
7
Under review as a conference paper at ICLR 2022
Algorithm 1 max{1, g}-Normalized Gradient Perturbation
Require: dataset D, learning rate at iteration t: ηt, the variance of the Gaussian noise injected to
the gradient: σ.
1:	function M-NGP(D, ηt , σ)
2:	Initialize θ0 .
3:	for t = 0 to T - 1 do
4：	if MRn(θt) ∣∣2 < 1 then
5:	Vθ Rnet) - Vθ Rn(θt)/ ∣∣ Vθ Rnet)I1.
6:	endif
7:	θt+1 J θt - ηt (vθRn(θt) + bJ , where b ~ N (0, σ Ip).
8:	endfor
9:	return θn = θT .
10:	end function
Then, via Algorithm 1, we can improve the excess population risk bound as shown below.
Theorem 3. If Assumptions 3, 4, 5 hold, the loss function and the parameter space are bounded,
i.e. 0 ≤ '(∙, ∙) ≤ m`, ∣∣C∣∣2 ≤ Me. Taking σ given by Lemma 2, T = O (log(n)), and ηι =…=
ητ = η, where (鲁 一μ⅛0-⅛∕ɑ)	< η < (H)1/a, if Z ∈ (exp(-p/8), 1), then with probability
at least 1 - ζ,
,ʌ .
R(θn)
D3∖ /	G0 VZp * 3 * 5 * * * * log(n) IOg(I∕δ)
一 R(θ ) ≤ c1--------------------
n
1+ ( 8log(T∕Z) )1/4
+ c2 ( G0 log2(n) + B + m` λ
2n	n
+『log2.5(n)√Plog(1∕δ) / + (8log(1∕Z)) 1/4
3	n	∖	∖ P )
for some constants c1 , c2 , c3 > 0, where G0 = max{2H MC , H}.
Detailed proof can be found in Appendix A.4. The proof is similar to Theorems 1 and 2, the key
difference is that by gradient normalization in Algorithm 1, Young’s inequality is abandoned in the
theoretical analysis (as discussed in Remark 3), which implies a better excess population risk bound.
By Theorem 3, it is easy to follow that with high probability, R(θn ― R(θ* ) = O (√np). The bound
is of the same order as the result given in Theorem 1.This is also the first O (1∕n) high probability
excess population risk bound over DP algorithm w.r.t n without smoothness assumption.
5 Experiments
In this section, we perform experiments on real datasets to evaluate the difference between our
proposed m-NGP algorithm and the traditional gradient perturbation (TGP), like (1).
The experiments are performed on classification task over datasets Iris (Dua & Graff, 2017), Breast
Cancer (Mangasarian & Wolberg, 1990), Credit Card Fraud (Bontempi & Worldline, 2018), Bank
(Moro et al., 2014), and Adult (Dua & Graff, 2017), the number of total data instances are 150, 699,
984, 41188, and 45222, respectively. We split the training and testing sets randomly and evaluate
the accuracy on the testing set and the convengence rate on the training set. In all the experiments,
the privacy budget δ is set n and We choose E = 0.1 to 1.0.
We apply the regularized logistic regression method to the classification task, the loss function sat-
isfies the assumptions mentioned before, and the experimental results are shown in Figure 1. We
show the experimental results over datasets Iris and Adult in this section and experiments on other
datasets are shown in Appendices B.1 and B.2. For convergence rate, the shadow area represents the
8
Under review as a conference paper at ICLR 2022
(b) Adult
(a) Iris
(d) Adult
Figure 1: Comparisons between Traditional Gradient Perturbation (TGP) method and max{1, g}-
Normalized Gradient Perturbation (m-NGP) method.
maximum and minimum loss over mutiple experiments, reflecting the variance. The shadow area in
part (d) of Figure 1 is not obvious, the reason is that the variances are small. Over most datasets, the
accuracy and the convergence rate of max{1, g}-Normalized Gradient Perturbation method is better
than traditional gradient perturbation method. Besides, the accuracy of the DP model increases with
the increasing of the privacy budget , which is in line with the theoretical analysis.
6 Conclusions
In this paper, We first propose a state-of-the-art O (n)high probability excess population risk bound
for gradient perturbation based DP algorithms, under the assumptions of G-Lipschitz, L-smooth,
Polyak-匕OjaSieWiCz condition, and generalized Bernstein condition. The result positively answers
the open problem: Can we achieve high probability excess risk bound with rate O(1/n) w.r.t n for
DP models via uniform stability? Then, We extend the result to a more general case, requiring α-
Holder smoothness, Polyak-Lojasiewicz condition, and generalized Bernstein condition. However,
-2 α
the result is not as satisfactory as before, we achieve an O (n 1+2α) high probability utility bound,
which is better than previous results when α ∈ [ 1, 1] and cannot achieve an O (1/n) bound. To get a
better result, we further propose a new algorithm: max{1, g}-Normalized Gradient Perturbation (m-
NGP). Detailed theoretical analysis shows that m-NGP can achieve O (吁)high probability excess
population risk bound, under the assumptions of α-Holder smoothness, Polyak-Lojasiewicz condi-
tion, and generalized Bernstein condition, which is the first O (1/n) high probability bound w.r.t
n under non-smoothness cases. Experimental results show that the accuracy of m-NGP algorithm
is better than traditional gradient perturbation method. Thus, our proposed max{1, g}-Normalized
Gradient Perturbation method improves the excess population risk bound and the accuracy of the
DP model over real datasets, simultaneously.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016.
Michael Backes, Pascal Berrang, Mathias Humbert, and Praveen Manoharan. Membership privacy
in microrna-based studies. In Proceedings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security, pp. 319-330, 2016.
Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the em algo-
rithm: From population to sample-based analysis. The Annals of Statistics, pp. 77 - 120, 2017.
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized rademacher complexities. In
Computational Learning Theory, 15th Annual Conference on Computational Learning Theory,
COLT 2002, pp. 44-58, 2002.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, pp. 138-156, 2006.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464-473, 2014.
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates. In Advances in Neural Information Processing Systems,
pp. 11279-11288, 2019.
RaefBassily, Vitaly Feldman, Cristobal Guzman, and KUnal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems, pp.
4381-4391, 2020.
Garrett Bernstein and Daniel R Sheldon. Differentially private bayesian linear regression. In Ad-
vances in Neural Information Processing Systems, pp. 523-533, 2019.
Gianluca Bontempi and Worldline. ULB the machine learning group, 2018.
Olivier BouSquet and Andre Elisseeff. Stability and generalization. The Journal OfMachine Learn-
ing Research, pp. 499-526, 2002.
Mark Bun, Marek Elias, and Janardhan Kulkarni. Differentially private correlation clustering. In
Proceedings of the 38th International Conference on Machine Learning, pp. 1136-1146, 2021.
Nicholas Carlini, Chang Liu, UJlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:
Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security
Symposium (USENIX Security 19), pp. 267-284, 2019.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In Proceedings of the 35th International Conference on Machine
Learning, pp. 745-754, 2018.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal of Machine Learning Research, pp. 1069-1109, 2011.
Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In
Advances in Neural Information Processing Systems, pp. 3571-3580, 2017.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the 41st
Annual ACM Symposium on Theory of Computing, 2009, pp. 371-380, 2009.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265-284, 2006.
10
Under review as a conference paper at ICLR 2022
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends® in Theoretical Computer Science, pp. 211-407, 2014.
UJlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable
privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC conference on
computer and communications security, pp. 1054-1067, 2014.
Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: Optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing, pp. 439-449, 2020.
Saul B Gelfand and Sanjoy K Mitter. Recursive stochastic algorithms for global optimization in r^d.
SIAM Journal on Control and Optimization, pp. 999-1018, 1991.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In 5th International Conference on
Learning Representations, 2017, 2017.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In Proceedings of The 33rd International Conference on Machine Learning, pp.
1225-1234, 2016.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.
Journal of Machine Learning Research, pp. 29:1-29:44, 2018.
Mikko Heikkila, Joonas Jalko, Onur Dikmen, and Antti Honkela. Differentially private markov
chain monte carlo. In Advances in Neural Information Processing Systems 32, pp. 4115-4125.
2019.
Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for minimizing star-convex
functions and beyond. In Proceedings of Thirty Third Conference on Learning Theory, pp. 1894-
1938, 2020.
Bargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice.
In Proceedings of the 28th USENIX Conference on Security Symposium, pp. 1895-1912, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-lojasiewicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811, 2016.
Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization
and high-dimensional regression. In Conference on Learning Theory, pp. 25-1, 2012.
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local
minima? In Proceedings of the 35th International Conference on Machine Learning, pp. 2698-
2707, 2018.
Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with conver-
gence rate o(1/n). arXiv preprint arXiv:2103.12024, 2021.
Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization.
The Annals of Statistics, pp. 2593-2656, 2006.
Tejas Kulkarni, Joonas Jalko, Antti Koskela, Samuel Kaski, and Antti Honkela. Differentially pri-
vate bayesian inference for generalized linear models. In Proceedings of the 38th International
Conference on Machine Learning, pp. 5838-5849, 2021.
Yunwen Lei and Ke Tang. Learning rates for stochastic gradient descent with nonconvex objectives.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In Proceedings of the 37th International Conference on Machine Learning, pp.
5809-5819, 2020a.
11
Under review as a conference paper at ICLR 2022
Yunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated
objective functions. In International Conference on Learning Representations, 2020b.
Shaojie Li and Yong Liu. Improved learning rates for stochastic optimization: Two theoretical
viewpoints, 2021.
Xiaodong Li, Shuyang Ling, Thomas Strohmer, and Ke Wei. Rapid, robust, and reliable blind
deconvolution via nonconvex optimization. Applied and computational harmonic analysis, pp.
893-934, 2019.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems 30, 2017, pp. 597-607, 2017.
Huikang Liu, Weijie Wu, and Anthony Man-Cho So. Quadratic optimization with orthogonality
constraints: Explicit lojasiewicz exponent and linear convergence of line-search methods. In
Proceedings of the 33nd International Conference on Machine Learning, 2016, pp. 1158-1167,
2016.
Olvi L Mangasarian and William H Wolberg. Cancer diagnosis via linear programming. Technical
report, University of Wisconsin-Madison Department of Computer Sciences, 1990.
Robert McMillan. Apple tries to peek at user habits without violating privacy. The Wall Street
Journal, 2016.
Sergio Moro, PaUlo Cortez, and PaUlo Rita. A data-driven approach to predict the success of bank
telemarketing. Decision Support Systems, pp. 22-31, 2014.
DUng NgUyen and Anil VUllikanti. Differentially private densest sUbgraph detection. In Proceedings
of the 38th International Conference on Machine Learning, pp. 8140-8151, 2021.
VijayakUmar PonnUsamy, J. Christopher Clement, K. C. Sriharipriya, and Sowmya Natarajan. Smart
Healthcare Technologies for Massive Internet of Medical Things, pp. 71-101. Springer Interna-
tional PUblishing, 2021.
Maxim Raginsky, Alexander Rakhlin, and MatUs Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. In Proceedings of the 2017 Conference
on Learning Theory, pp. 1674-1703, 2017.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy, 2017,
pp. 3-18, 2017.
Monoj KUmar Singha, Priyanka Dwivedi, GaUrav Sankhe, Aniket Patra, and Vineet Rojwal. Role
of Sensors, Devices and Technology for Detection of COVID-19 Virus, pp. 293-312. Springer
International PUblishing, 2021.
ShUang Song, Kamalika ChaUdhUri, and Anand D Sarwate. Stochastic gradient descent with differ-
entially private Updates. In 2013 IEEE Global Conference on Signal and Information Processing,
pp. 245-248, 2013.
Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & BUsiness
Media, 2008.
JU SUn, Qing QU, and John Wright. A geometric analysis of phase retrieval. Foundations of Com-
putational Mathematics, pp. 1131-1198, 2018.
G. Swapna and K. P. Soman. Diabetes Detection and Sensor-Based Continuous Glucose Monitoring
-ADeep Learning Approach, pp. 245-268. Springer International Publishing, 2021.
Jonathan Ullman and Adam Sealfon. Efficiently estimating erdos-renyi graphs with node differential
privacy. In Advances in Neural Information Processing Systems, pp. 3765-3775, 2019.
Di Wang and Jinhui Xu. Principal component analysis in the local differential privacy model. The-
oretical Computer Science, 2019.
12
Under review as a conference paper at ICLR 2022
Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited:
Faster and more general. In Advances in Neural Information Processing Systems, pp. 2722-2731,
2017.
Di Wang, Changyou Chen, and Jinhui Xu. Differentially private empirical risk minimization with
non-convex loss functions. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, pp. 6526-6535, 2019.
Puyu Wang, Yunwen Lei, Yiming Ying, and Hai Zhang. Differentially private sgd with non-smooth
loss. arXiv preprint arXiv:2101.08925, 2021.
Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on
differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the
2017 ACM International Conference on Management of Data, pp. 1307-1322, 2017.
Chugui Xu, Ju Ren, Deyu Zhang, Yaoxue Zhang, Zhan Qin, and Kui Ren. Ganobfuscator: Miti-
gating information leakage under gan via differential privacy. IEEE Transactions on Information
Forensics and Security, pp. 2358-2371, 2019.
Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efficient private erm for smooth objec-
tives. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,
IJCAI-17, pp. 3922-3928, 2017.
Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global
minimum in deep learning via star-convex path. In 7th International Conference on Learning
Representations, 2019, 2019.
13