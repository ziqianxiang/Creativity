Under review as a conference paper at ICLR 2022
DP-REC: Private & Communication-Efficient
Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Privacy and communication efficiency are important challenges in federated train-
ing of neural networks, and combining them is still an open problem. In this
work, we develop a method that unifies highly compressed communication and
differential privacy (DP). We introduce a compression technique based on Relative
Entropy Coding (REC) to the federated setting. With a minor modification to
REC, we obtain a provably differentially private learning algorithm, DP-REC,
and show how to compute its privacy guarantees. Our experiments demonstrate
that DP-REC drastically reduces communication costs while providing privacy
guarantees comparable to the state-of-the-art.
1	Introduction
The performance of modern neural-network-based machine learning models scales exceptionally well
with the amount of data that they are trained on (Kaplan et al., 2020; Henighan et al., 2020). At the
same time, industry (Xiao & Karlin), legislators (Dwork, 2019; Voigt & Von dem Bussche, 2017) and
consumers (Laziuk, 2021) have become more conscious about the need to protect the privacy of the
data that might be used in training such models. Federated learning (FL) describes a machine learning
principle that enables learning on decentralized data by computing updates on-device. Instead of
sending its data to a central location, a “client” in a federation of devices sends model updates
computed on its data to the central server. Such an approach to learning from decentralized data
promises to unlock the computing capabilities of billions of edge devices, enable personalized models
and new applications in e.g. healthcare due to the inherently more private nature of the approach.
On the other hand, the federated paradigm brings challenges along many dimensions such as learning
from non-i.i.d. data, resource-constrained devices, heterogeneous compute and communication
capabilities, questions of fairness and representation, as well as the focus of our paper: communication
overhead and characterization of privacy. Neural network training requires many passes over the data,
resulting in repeated transfer of the model and updates between the server and the clients, potentially
making communication a primary bottleneck (Kairouz et al., 2019; Wang et al., 2021). Compressing
updates is an active area of research in FL and an essential step in “untethering” edge devices from
WiFi. Moreover, while FL is intuitively more private through keeping data on-device, client updates
have been shown to reveal sensitive information, even allowing to reconstruct a client’s training
data (Geiping et al., 2020). To truly protect client privacy, a more rigorous mathematical notion of
Differential Privacy (DP) is widely adopted as the de-facto standard in FL. More specifically, DP for
FL is usually defined at a client-level, which provides plausible deniability of an individual client’s
contribution to the federated model. Both aspects of FL, communication efficiency and differential
privacy, have been extensively studied in separation. Effective combination of the two, however, is an
open problem and an active area of research (Kairouz et al., 2021; Chen et al., 2020; Girgis et al.,
2020). Some methods offer very limited compression capabilities for comparable utility (Kairouz
et al., 2021), whereas others (Girgis et al., 2020) offer significant compression but require additional
compromises, due to a looser privacy composition.
In this work, we present Differentially Private Relative Entropy Coding (DP-REC), a unified approach
to jointly tackle privacy and communication efficiency. DP-REC makes use of the information-
limiting constraints of DP to encode the client updates in FL with extremely short messages. First, we
build our method based on compression without quantization; namely, the lossy variant of Relative
Entropy Coding (REC), recently proposed by Flamich et al. (2020). Second, we show that it can be
1
Under review as a conference paper at ICLR 2022
modified to satisfy DP, and we provide a proof of its privacy guarantees along with the appropriate
accounting technique. Third, we run extensive evaluation on 4 datasets and 3 types of models to
demonstrate that our algorithm achieves extreme compression of client-to-server updates (down to 7
bits per tensor) at privacy levels ε < 1 with a small impact on utility (< 6.5% accuracy reduction
on FEMNIST compared to DP-FedAvg). Additionally, we show how to reduce server-to-client
communication by sending a history of updates accumulated since the client’s last participation.
2	DP-REC for private and efficient communication in FL
Federated learning has been described by McMahan et al. (2016) in the form of the FedAvg
algorithm. At each communication round t, the server sends the current model parameters w(t) to a
subset S(t) of all clients S. Each chosen client s updates the server-provided model w(t), e.g., via
stochastic gradient descent, to better fit its local dataset Ds = {dsi }iN=s1 with a given loss function
1 Ns
Ls(Ds, w) := N^L(dsi, w).	(1)
Ns
i=1
After E epochs of optimization on the local dataset, the client-side optimization procedure results in
an updated model ws(t), based on which the client computes its update to the global model
∆(st) = ws(t) - w(t) ;	(2)
and sends it to the server. The server then aggregates client-specific updates to get the new global
model w(t+1) = w(t) + ∣s1t)∣ Ps ∆St) = 01t^ Ps w(t). Outside the context of differential privacy,
client updates are usually weighted by the size Ns of the local dataset. A generalization of this server-
side scheme (Reddi et al., 2020) interprets	Ps∈s(t) ∆st) as a “gradient" for the server-side
model and introduces more advanced updating schemes, such as Adam (Kingma & Ba, 2014).
Federated training involves repeated communication of model updates from clients to the server and
vice versa. These updates can reveal sensitive information about the client data, so there is a need for
formal privacy guarantees. The total communication cost can be significant, thus constraining FL to
the use of unmetered channels, such as WiFi. Hence, compressing the update messages in a privacy-
preserving way plays an important role in moving FL to a truly mobile use-case. In the following
sections, we first describe the lossy version of Relative Entropy Coding (REC) (Flamich et al., 2020),
then show how to extend it to the FL scenario before discussing it in the context of differential privacy.
Finally, we show how DP-REC can be used to additionally compress server-to-client messages.
2.1	REC for efficient communication
Lossy REC, and its predecessor minimal random code learning (MIRACLE) (Havasi et al., 2018),
have been originally proposed as a way to compress a random sample w from a distribution qφ(w)
parameterized with φ, i.e., W 〜qφ(w), by using information that is “shared" between the sender
and the receiver. This information is given in terms of a shared prior distribution pθ (w) with
parameters θ along with a shared random seed R. The sender proceeds by generating K independent
random samples, w1, . . . , wK, from the prior distribution pθ(w) according to the random seed R.
Subsequently, it forms a categorical distribution q∏(wi：K) over the K samples with the probability
of each sample being proportional to the likelihood ratio ∏k α qφ(w = Wk)/pθ(w = Wk). Finally,
it draws a random sample wk* from q∏(wi：K), corresponding to the k*,th sample drawn from the
shared prior. The sender can then communicate to the receiver the index k with log2 K bits. On
the receiver side, wk* can be reconstructed by initializing the random number generator with R and
sampling the first k samples frompθ(w). These procedures are described in Algorithms 1 and 2.
Havasi et al. (2018) set K to the exponential of the Kullback-Leibler (KL) divergence of qφ(w) to
the prior pθ(w) with an extra constant c, i.e., K = exp(KL(q@(w)||pe(w)) + c). In this case, the
message length is at least O(KL(qφ(w) ∣∣pθ(w))). This can be theoretically motivated based on the
work by Harsha et al. (2007); when the sender and the receiver share a source of randomness, under
some assumptions, this KL divergence is a lower bound on the expected message length (Flamich
et al., 2020). This brings forth an intuitive notion of compression that connects the compression rate
2
Under review as a conference paper at ICLR 2022
Algorithm 1 The sender-side algorithm for lossy
REC with the shared random seed R, the shared
prior pθ(w) and the number of prior samples K.
Set state of pseudo-RNG to R
Draw wι,..., WK i'd pθ (W)
αk - qφ(w = Wk)∕Pθ (w = Wk)
∏k — αk∕ Ej aj
wk* 〜q∏(WLK)
return k encoded with log2 K bits
Algorithm 2 The receiver-side algorithm for
lossy REC.
Receive k from the sender
Set state of pseudo-RNG to R
DraWwι,...,Wk* 力.pθ(w)
Use wk* for downstream task
with the amount of additional information encoded in qφ(w) relative to the information in pθ(w);
the smaller the amount of extra information the shorter the message length will be and, in the extreme
case where qφ(w) = pθ(w), the message length will be O(1). Of course, achieving this efficiency
is meaningless if the bias of this procedure is high; fortunately, Havasi et al. (2018) show that for
appropriate values of C and under mild assumptions, the bias, namely ∖Eqπ(w〉k)[f] - Eqφ(w) [f] | for
arbitrary functions f, can be sufficiently small. In all of our subsequent discussions, we parametrize
K as a function of a binary bit-width b, i.e., K = 2b, and treat b as the hyperparameter.
2.2	REC for efficient communication in FL
We can adapt this procedure to FL by appropriately choosing distributions over client-to-server
messages (model updates), qφ(t)(∆(st)) , along with the prior distribution p(θt) (∆(st)) on each round t:
pθt)(∆St)) := N(Ast)∣0,σ2I),	qφ)(∆St)) := N(∆St)∣w(t) - W⑴,σ2I),	(3)
i.e., for the prior we use a Gaussian distribution centered at zero with appropriately chosen σ and for
the message distribution we opt for a Gaussian with the same standard deviation centered at the model
update. The form of q is chosen to provide a plug-in solution to potentially resource constrained
devices, as well as to readily satisfy the differential privacy constrains discussed in Section 2.3. Note
that, as opposed to the FedAvg client update definition in (2), here we consider ∆(st) to be a random
variable and the difference ws(t) - w(t) to be the mean of the client-update distribution q over ∆(st).
Let us now see how communication efficiency is realized in the FL pipeline. The length of the message
will be a function of how much “extra” information about the local dataset Ds is encoded in ws(t),
measured by KL divergence. As we show later, this has a nice interplay with DP: the DP constraints
bound the amount of information encoded in each update, resulting in highly compressible messages.
It is also worth noting that this procedure can be done parameter-wise (i.e., communicate log2 K
bits per parameter), layer-wise (log2 K bits for each layer in the network) or even network-wise
(log2 K bits in total). Any arbitrary intermediate vector size is also possible. This is done by splitting
∆(st) into M independent groups (which is possible due to our assumption of factorial distributions
over the dimensions of the vector) and applying compression independently to each group (see also
Theorem 3). If we have many groups (e.g., when we perform per-parameter compression), we can
boost the compression even further by entropy coding the indices with their empirical distribution.
2.3	Private & efficient communication for FL with DP-REC
In order to make the compression procedure described in Section 2.2 differentially private, we
need to bound the sensitivity of the mechanism and quantify the noise inherent to it. Similarly to
DP-FedAvg, bounding the sensitivity consists of clipping the norm of client updates ws(t) - w(t). In
the context of REC, this means that the client message distribution qφ(t) cannot be too different from
the server prior p(θt) in any given round t. After this step, instead of explicitly injecting additional
noise to the updates, we make use of the fact that the procedure in itself is stochastic. Two sources
of randomness play a role in each round t: (1) drawing K samples from the prior p(θt) ; (2) drawing
an update from the importance sampling distribution q∏. We coin the name Differentially-Private
Relative Entropy Coding (DP-REC) for the resulting mechanism, outlined in Algorithms 3 and 4.
3
Under review as a conference paper at ICLR 2022
Algorithm 3 The client-side algorithm for DP-REC at a given round t. R(st) is the client-chosen shared random seed, w(t) is the server model, ws(t) is the fine-tuned model of client s, K is the number of prior samples, C is the clipping threshold and σ is the prior standard deviation. Rst) J choose and set client-specific seed for round t φSt) J Wst) — w(t) ∆∖,...,∆κ i%N(∆st)∣0,σ2I) φst) = φst) min(1, C∕kφst)k2) αk JN(∆st) = ∆k∣φst),σ2I)∕N(∆st) = ∆k∣0,σ2I) πk J αk∕ Pj αj △k* 〜媒)。1:K) return k* encoded with log2 K bits, Rst)	Algorithm 4 The server side algo- rithmfor DP-REC.	 Receive k*, R(st) from client s Set state of pseudo-RNG to R(st) ∆ι,...,∆k* i%N(∆st)∣0,σ2I) Use ∆k* for downstream task
Finally, to prove that DP-REC is differentially private and calculate the corresponding ε, δ, we build
upon prior work in privacy accounting for ML and FL; particularly, on Renyi differential privacy
(RDP) (Mironov, 2017) and the moments accountant (Abadi et al., 2016). Our analysis in the next
section follows the established approach based on tail bounds of the privacy loss random variable,
addressing such important differences as two sources of randomness and non-Gaussianity of q∏t).
Additional discussion on other aspects of our method can be found in Appendices B.1 and D.2.
2.4 Privacy Analysis of DP-REC
A possible avenue to analyze privacy for DP-REC is to derive DP bounds relying on Theorem 3.2 of
Havasi et al. (2018). However, obtaining reasonable guarantees can be challenging, as the probability
bound in (Havasi et al., 2018, Theorem 3.2) has to be incorporated in δ and it is at least e-0.125 ln K.
As such, a fairly common value δ = 10-5 would require -e92 samples from the prior. To overcome
this issue, we consider an importance sampling bound by Agapiou et al. (2017, Section 2.2). It scales
(inversely) linearly with the number of samples, helping for smaller sample sizes.
Let us restate the bound for completeness. For some test function ζ , one can characterize the bound
between the true expectation and the importance sampling estimate in the following way:
12
sup ∣E [μN(Z)- μ(Z)] ∣ ≤ 万eD2-^lp),	(4)
IZI<ι	K
where K is the number of samples, μN(∙) = Egn [∙], μ(∙) = Eqψ [∙] and D2(∙∣∣∙) denotes the Renyi
divergence of order 2 between the target qφ and the proposal pθ. Our choice of ζ (cf. Appendix A.1)
guarantees |Z| < 1 for any input and enables the use of this bound.
Theorem 1 summarizes our main theoretical result. It enables DP-REC to combine the server-side
privacy accounting in the central model of DP with extreme model update compression, while
allowing clients to privatize their updates locally and protect against an honest-but-curious server.
Theorem 1. After T rounds, with the client-to-server bitrate b, DP-REC is differentially private with
TT
δ ≤ min e-λε Y ed-1)。"。* + 12 X *”,
λ	t=1	t=1
where the constant Cf) ≥ Dχ (q(t)∣∣pθt)) is the upper bound on the Renyi divergence of order λ
between the client model update distribution qφ(t) and the server prior p(θt) in any given round t.
Proof. See Appendix A.1.
□
An attentive reader may notice a strong resemblance between Theorem 1 and the moments accoun-
tant (Abadi et al., 2016) and Renyi DP (Mironov, 2017) results. Indeed, it turns out that the DP-REC
4
Under review as a conference paper at ICLR 2022
compression procedure yields the privacy loss that can be bound by almost the double of the normal
continuous Gaussian mechanism loss, plus a small overhead, and composed in a very similar manner.
Shared random seed Prior work relying on shared random seeds has been shown to be vulnerable
to attacks due to the possibility of “inverting” the noise (Kairouz et al., 2021). DP-REC is not
susceptible to this for two reasons. First, the randomness of the method comes from two sources
instead of one, and knowing one seed does not permit to fully reconstruct the un-noised sample.
In our guarantee, δ corresponds to the probability of the mechanism failure w.r.t. both sources of
randomness, making it as secure as the standard Gaussian mechanism even when one of the seeds
is shared. However, this alone is not sufficient because the shared seed R could be manipulated to
generate a set of samples on which to encode the update such that the guarantee would not hold (e.g.,
in the tails of the privacy loss distribution). Hence, we add the second line of defense as we shift the
task of picking R to a client, who then transmits the seed along with the index k for decoding. This
way, neither the server nor any other entity can manipulate the seed to break the privacy guarantee.
Privacy amplification Like many recent approaches, including (Kairouz et al., 2021), DP-REC can
be seen as a hybrid method, privatizing updates locally and providing an amplified central guarantee.
A key difference, however, is that our method does not calibrate noise to the aggregate. Namely, the
scale of noise and the guarantee do not depend on whether a thousand, a hundred, or just one client is
sampled in a round. This is due to non-additive nature of our mechanism, which does not let us easily
benefit from the variance reduction as the Gaussian mechanism does. As a result, we always calibrate
to an “aggregate” of one client. While this property increases the necessary noise, it has an upside of
allowing stronger privacy amplification. Substituting the typical sampling without replacement by
sampling with replacement, we can use the amplification factor of 1/|S| while accounting for T|S0|
rounds, instead of |S0 |/|S | while accounting for T rounds, and obtain a tighter overall guarantee.
Secure aggregation One of the considerations in FL is that the server might be honest-but-curious
rather than fully trusted. In DP-FedAvg (McMahan et al., 2017), the noise addition is delegated to
the server, and thus, secure aggregation (Bonawitz et al., 2017) is necessary to prevent the server from
receiving client updates in the clear. DDGauss (Kairouz et al., 2021) and analogous methods mitigate
this problem by shifting the noise addition to the client side, but still use secure aggregation for the
reasons stated in the previous paragraph: noise is calibrated to the aggregate and may be insufficient
to protect some clients. In DP-REC, because of the noise calibration to individual updates, we believe
that the provided local guarantee (which can be computed by removing subsampling amplification)
is sufficient against non-malicious servers. To further boost privacy protection, an important future
work direction is integrating the DP-REC compression scheme with secure aggregation.
Interplay of privacy and compression Theorem 1 relates privacy to our compression scheme in
an intuitive manner. In order to get meaningful privacy guarantees, we have to bound the Renyi
divergence in any given round t for any λ ≥ 1, which limits the amount of information encoded
in qφ(t) relative to p(θt). As a result, enforcing DP guarantees also implies that our scheme will have
highly compressible messages. We formalize this interplay in the following lemma.
Lemma 2. Consider compressing the expected value of ζ under the qφs (∆s) at (3), and let ξ be a
desired average compression bias of REC for ζ. To achieve this target, a sufficient bitrate b is at most
b ≤ log2 12 + Γ~πD2 (q6s Upθ) - log2 标，
log 2	G
where |Z| ≤ G and D2 (qφs ∣∣pθ), and thus the maximum bitrate b, is controlled by the chosen (ε, δ).
Proof. The proof follows from (4), by setting K = 2b, rearranging the terms and adjusting for G. □
We also verify the claim empirically (Figure 2), fixing the bound on the Renyi divergence and showing
that the performance with extreme compression (i.e., 7 bits/tensor) and no compression is similar.
Finally, to confirm that various granularity of compression (per-parameter, per-layer, per-network,
etc.) does not interfere with our privacy analysis, we introduce and prove the following theorem.
Theorem 3 (Compression-by-parts). Expectation of an arbitrary function ζ(∆[1], . . . , ∆[M]) over
the importance sampling distributions qπ[1], . . . , qπ[M], built for M non-intersecting parameter groups
5
Under review as a conference paper at ICLR 2022
independently, is equivalent to the expectation over the joint distribution qπ built on 2PiM=1 bi samples:
E∆[1]〜q∏1] [... E∆[M]〜q∏M] hZ("1]，...，"M])i …]=E∆[LM]〜q∏ [Z("1:M)i .
Proof. See Appendix A.2.	□
2.5 Compressing server-to-client messages
The compression procedure described in Section 2.2 is a specific example of (stochastic) vector
quantization where the shared codebook is determined by a shared random seed. Here we show
how the principle of communicating indices into such a shared codebook additionally allows for the
compression of the server-to-client communication. Instead of sending the full server-side model
to a specific client, the server can choose to collect all updates to the global model in-between two
subsequent rounds that the client participates in. Based on this history of codebook indices, the
client can deterministically reconstruct the current state of the server model before beginning local
optimization. Clearly, the expected length of the history is proportional to the total number of clients
and the amount of client subsampling we perform during training. At the beginning of a round, the
server can therefore compare the bit-size of the history and choose to send the full-precision model
w(t) instead. Taking a model with 1k parameters as an example, a single uncompressed model update
is approximately equal to 4k communicated indices when using 8-bit codebook compression of the
whole model. Crucially, compressing server-to-client messages this way has no influence on the DP
nature of DP-REC since the local model ofDP ensures privacy of client information (and the stronger
central guarantee applies as long as the secrecy of the sample is preserved within the updates history).
For clients participating in their first round of training, the first seed without accompanying indices
can be understood as seeding the random initialization of the server-side model. Algorithms 5 and 6
(in Appendix B.2) describe this scheme. It is important to note that the client-side update rule must
be equal to the server-side update rule, i.e. in generalized FedAvg (Reddi et al., 2020) it might be
necessary to additionally send the optimizer state when sending the current global model w(t).
3	Related work
The privacy promise of federated learning relies heavily on the use of additional techniques, such as
differential privacy and secure multi-party computation, to provide rigorous theoretical guarantees.
Without these techniques, FL has been shown to be vulnerable to attacks (Geiping et al., 2020) and
unintended leakage of sensitive information (Thakkar et al., 2020). One of the main open challenges
is to reduce the communication cost while preserving DP guarantees (Kairouz et al., 2019).
McMahan et al. (2017) outlined DP-FedAvg, which has since been the staple of DP federated
learning. In this default scheme, clients clip norms of their updates before submitting them to the
server (preferably, using a secure aggregation protocol (Bonawitz et al., 2017)). The server then
completes the DP mechanism by adding Gaussian noise to the aggregate of multiple clients. Without
secure aggregation, this method allows clients to compress their updates and reduce communication,
but becomes vulnerable to a malicious or honest-but-curious server. One could use the local model of
DP (Dwork et al., 2014) instead of the central model to address this issue: clients would add noise
locally before sending the updates. But this leads to a pronounced drop in accuracy due to the larger
scale of noise necessary in the local model (Kairouz et al., 2019). The few existing examples that use
the local model operate on very large numbers of clients and updates (Pihur et al., 2018).
To overcome this problem of excessive noise, researchers are increasingly looking in the direction
of hybrid solutions (Shi et al., 2011; Rastogi & Nath, 2010; Agarwal et al., 2018; Truex et al.,
2019). The key idea of these techniques is to reduce the variance of the locally added noise by
taking into account the larger global number of clients and accounting DP in the central model.
Since the smaller noise variance is insufficient to protect individual updates, secure aggregation is
necessary for these methods. In this context, the local noise distributions have to be discrete, such
that their sum after secure aggregation is also discrete and is of known shape, allowing the server to
compute guarantees centrally. As a result, until recently, most of these methods relied on the binomial
distribution. Compared to the traditional Gaussian mechanisms, it is at a disadvantage because it does
not yield Renyi or concentrated DP, and thus cannot benefit from tighter adaptive composition and
6
Under review as a conference paper at ICLR 2022
amplification through sampling, which is particularly important in ML applications (Kairouz et al.,
2021). Kairouz et al. (2021) presented a novel analysis of the discrete Gaussian mechanism (Canonne
et al., 2020) in terms of concentrated DP. Their mechanism, named Discrete Distributed Gaussian
(DDGauss), is adapted to the context of FL, can provide accuracy comparable to the centralized
Gaussian mechanism, and enables parameter-wise quantization. However, at higher compression
rates (e.g., 12 bits per parameter), DDGauss fails to train a model for reasonable ε values.
Finally, there is recent work featuring extreme compression rates for DP algorithms, bearing a
resemblance to our solution. Girgis et al. (2020) proposed to use a locally private mechanism along
with a secure shuffler to communicate models one (privatized) parameter at a time. It compresses
client messages to log d bits for a model of d parameters. This approach, however, seems to require a
large number of clients to participate in a single round (10k for their MNIST experiment), which is
impractical in realistic scenarios and detrimental to the total communication cost (upload + download).
Chen et al. (2020) achieve similarly high compression rates as (Girgis et al., 2020); however, they do
not consider the context of federated learning but rather focus on frequency and mean estimation. As
we did not have an FL baseline for (Chen et al., 2020) and could not reliably reproduce the results of
Girgis et al. (2020), we focused our comparison with these methods on mean estimation. Details can
be found in Appendix D.2. In short, we find that the method of Chen et al. (2020) has an edge over
DP-REC if a good prior is lacking, making it preferable for low-information, “one-shot” scenarios,
such as mean estimation. On the other hand, with a good prior (or one that is learned during training),
DP-REC performs better and thus is more appropriate in federated settings.
4	Experiments
We evaluate DP-REC on several non-i.i.d. FL tasks that provide client-level privacy guarantees:
image classification on a non-i.i.d. split of MNIST (LeCun et al., 1998) into 100 clients and FEM-
NIST (Caldas et al., 2018) into 3.5k clients, along with next character prediction on the Shakespeare
dataset (Caldas et al., 2018) with LSTMs (Hochreiter & Schmidhuber, 1997) and 660 clients as well
as tag prediction on the StackOverflow (SO-LR) dataset (TFF Authors, 2019) with a logistic regres-
sion model (Kairouz et al., 2021) and 342477 clients. For baselines, we consider DP-FedAvg as the
gold standard without compression and DDGauss as a baseline that involves parameter quantization.
Both of these methods target central DP guarantees and employ secure aggregation. All exeriments
were implemented in PyTorch (Paszke et al., 2019). We provide experimental details in Appendix C.
4.1	Results and discussion
We plot the global model accuracy for different privacy budgets ε as a function of the total communi-
cation cost in Figure 1; this highlights how efficiently each method spends bits of communication
in order to reach a target accuracy. Due to the substantial compression by DP-REC relative to
the baselines, we use log-scale for the x-axis of communication costs. Additionally, Figure 3 in
Appendix D shows the accuracy achieved as a function of the privacy budget ε; this highlights how
efficiently each method spends its privacy budget in order to reach a specific target accuracy. Note
that in certain cases the training is stopped before convergence, due to exhausted privacy budget.
In Table 1, we report the final model performance and total communication costs for different privacy
budgets. It should be mentioned that the evaluation loop for StackOverflow is prohibitively expensive
to be done in each round due to having -1.6M datapoints. We thus pick 10k random datapoints
for evaluation during training to plot the learning curves (not necessarily the same for each run),
following (Reddi et al., 2020). The numbers in Table 1 refer to the model performance at the end
of training, where we evaluate on the entire test set for DP-FedAvg and DP-REC. For DDGauss,
since Kairouz et al. (2021) do not report the model performance on the full test set, we use the
numbers shown at the end of their plot. Finally, results for some settings are omitted because of
either (i) convergence issues for stricter privacy budgets (a typical phenomenon for ε < 1 on smaller
federations); (ii) not being reported by the related work we compare with; or (iii) not being necessary
in light of sufficient performance under better privacy guarantees (e.g., on StackOverflow).
Observing the experimental results, we clearly see the trade-offs. DP-REC can drastically reduce the
total communication costs (download and upload) of federated training depending on the subsampling
rate and amount of clients in a federation. For certain cases, we can get extreme reduction, such
7
Under review as a conference paper at ICLR 2022
Table 1: Performance (± standard error obtained via multiple runs) and total communication (in
GB), achieved for different privacy guarantees ε. Results for DDGauss (marked DDGx for x bits)
are taken from (Kairouz et al., 2021), which uses a slightly smaller version of the FEMNIST dataset
(3.4k clients instead of 3.5k).
MNIST	DP-FedAvg	DP-REC	FEMNIST	DP-FedAvg	DDG16	DDG12	DP-REC
Acc. (ε = 3)	82.1 ± 0.8	66.5 ± 1.5	Acc. (ε= 1)	65.7 ± 0.3	—	—	59.3 ± 0.1
Acc. (ε = 6)	90.0 ± 0.4	79.0 ± 1.9	Acc. (ε=3)	74.2 ± 0.1	〜72	〜25	67.0 ± 0.1
Comm.	43	0.01	Acc. (ε = 6)	75.5 ± 0.2	〜77	〜71	69.1 ± 0.1
			Comm.	259	194	178	14.2
Shakespeare DP-FedAvg		DP-REC	SO LR	DP-FedAvg	DDG16	DDG12	DP-REC
Acc. (ε=3)	39.0 ± 0.1	29.0 ± 0.1	R@5 (ε= 1)	19.3 ± 0.0	〜14	〜10	18.4 ± 0.7
Comm.	81	0.1	Comm.	3356	2517	2307	32
(b) FEMNIST
(c) Shakespeare
(d) StackOverflow
(a) MNIST
DP-FedAvg, ε < 1
DP-REC, ε< 1
Figure 1: Test accuracy (%) with ±1 standard error as a function of communication (in GB, in
log-scale). For StackOverflow, we report compression at the end of training, as the learning curves
are based on different subsets of the validation set.
as MNIST with 4300x compression. But even for other cases, the reduction is still several orders
of magnitude (105x for SO-LR and 18x on FEMNIST). Nevertheless, DP-FedAvg and DDGauss
(depending on the task) can reach better model performance for a given privacy budget. This is
primarily due to two reasons. Firstly, both DP-FedAvg and DDGauss add noise calibrated to the
number of clients in a given round to get a central DP guarantee on the aggregate; in contrast, DP-REC
calibrates noise to the contribution of individual updates (see Section 2.4). The signal-to-noise-ratio
for the model updates is thus worse for DP-REC. Secondly, in DP-REC we have to clip client updates
more aggressively to account for the privacy loss overhead incurred from the REC compression
(roughly double for each iteration compared to Gaussian mechanism). This can be mitigated in larger
federations, since the clipping can be reduced based on stronger privacy amplification for the central
model of DP. For example, observe the StackOverflow experiment, where the accuracy delta between
DP-REC and DP-FedAvg is smaller. Furthermore, it is precisely in those cases that reducing the
communication cost is important from a practical perspective. When we target a specific accuracy
8
Under review as a conference paper at ICLR 2022
within the reach of both DP-REC and DP-FedAvg, we compress between 8.3x (FEMNIST) to
1533x (MNIST), at the additional cost of 1.9 to 2.2 in ε, relative to DP-FedAvg.
Comparison with local DP guarantees The nature of the DP-REC privacy mechanism requires
calibrating noise to individual client updates rather than their contribution to the aggregate. This
property warrants a different kind of comparison: equalizing local guarantees for each individual
update (i.e., as seen when the secrecy of the sample in client sampling is not preserved). We use
our non-i.i.d. FEMNIST setting with 1.5k rounds and tune the noise of DP-FedAvg such that
a single client update, without any privacy amplification, has the same εlocal guarantee. When
training DP-FedAvg with a local-DP guarantee that DP-REC obtains when targeting central-DP
with ε = 3, we see that DP-FedAvg obtains 68.9% accuracy, whereas DP-REC gets 63.4%. On
a setting where we target the local-DP guarantee that DP-REC gets when aiming for central-DP
with ε = 1, DP-FedAvg achieves 60.1% accuracy compared to DP-REC’s 57.4%. In both cases,
DP-REC achieves an overall compression rate of 72.5x with 7-bits per tensor, while losing 2.7% to
5.5% in accuracy compared to DP-FedAvg (depending on the privacy target) due to more aggressive
clipping. It is also worth noting that the performance delta relative to the results shown in Table 1 is
smaller, highlighting the negative effects of calibrating the noise to an “aggregate” of a single client.
Compressibility of differentially private updates As we
noted in Section 2.4, differential privacy has a positive inter-
play with the REC compression scheme. DP upper-bounds
the Renyi divergence for any order λ between the proposal
distribution p(θt) and the model update distribution qφ(t) and,
according to lemma 2, this limits the maximum bitrate b of in-
formation to be communicated. In order to empirically verify
this claim, we consider the non-i.i.d. FEMNIST classification
with DP-REC, and hyperparameters that target a model with
ε = 3 after 1.5k rounds. We then maintain the same bound on
Renyi divergence and vary the number of bits per tensor from
4 to 7. We also include a variant without compression, which
directly communicates a random sample from the clipped up-
date distribution qφ(t). The results appear to verify our claim
and can be seen in Figure 2. After 4 bits, we see very small
improvements with adding additional bits and, with 7 bits, the
performance is very similar to the uncompressed baseline.
Figure 2: Effect of the bit-width on
model performance with DP-REC
for clipping targeting ε = 3 after
1.5k rounds. The “no-compression”
variant communicates a random sam-
ple directly from qφ(t) instead of com-
pressing it via DP-REC.
5	Conclusion
With DP-REC, we formalized the intuitive notion that ε, δ differentially private messages necessarily
contain a small amount of information and can therefore be compressed significantly. A bound on the
Renyi divergence between the server-side prior p and the locally optimized q implies a small message
size for REC, elegantly tying together DP and communication efficiency for federated learning. The
nature of our bound in Theorem 1 reveals the flipside of DP-REC. As opposed to the standard
Gaussian mechanism in central DP, it requires stronger clipping, effectively reducing the utility of
models trained with DP-REC for a given privacy budget. Our experiments with the StackOverflow
dataset show that these limitations can be mitigated by the stronger privacy amplification in situations
with large numbers of clients. Contrary to intuition, spending more bits to communicate updates from
clients to the server cannot recover this utility as the necessary information is lost in clipping, i.e.,
before forming the importance sampling distribution q.
There are several important directions left for future work. The first would be to investigate the
convergence of DP-REC. Intuitively, we can expect the algorithm to be convergent if DP-FedAvg
is convergent. The main difference of DP-REC is that the gradient is sampled using importance
weights rather than the true Gaussian probabilities. Asymptotically, these two distributions should be
close, and the bias of the compressed gradients would be bounded by a small value (as discussed in
Section 2.4). Empirically, we observe no issues with convergence. The second would be to investigate
secure shuffling, similarly to Girgis et al. (2020) as a way to amplify our privacy further; this can lead
to less aggressive clipping and thus improved performance for a given privacy budget.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
This paper addresses a significant, present-day problem and provides a clear, detailed explanation
of our solution to facilitate reproducibility and promote research integrity. We expect our work
to have an overall beneficial societal impact through its main contributions of communication
reduction and differential privacy. One potential concern worth mentioning is the interplay between
differential privacy and fairness in FL, an actively researched open problem. Bagdasaryan et al.
(2019) demonstrate that DP training can have a disproportionate effect on underrepresented and more
complex sub-populations, resulting in a disparate accuracy reduction. At the same time, Hooker
et al. (2020) showed that compression of models can have negative impacts by amplifying biases. In
the context of FL, compression is performed on model updates instead of a final model, therefore
requiring further research on its influence on bias amplification.
Reproducibility S tatement
For the theoretical results, we clearly state our assumptions and present complete proofs in the
appendix. For experimental evaluation, although we cannot provide the source code at the time of the
submission because it is proprietary, we include extensive information on algorithms and settings in
the appendix, and repeat our experiments with multiple random seeds to aid reproducibility.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016.
Sergios Agapiou, Omiros Papaspiliopoulos, Daniel Sanz-Alonso, and AM Stuart. Importance
sampling: Intrinsic dimension and computational cost. Statistical Science, pp. 405-431, 2017.
Naman Agarwal, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, and H Brendan Mcma-
han. cpsgd: Communication-efficient and differentially-private distributed sgd. arXiv preprint
arXiv:1805.10559, 2018.
Galen Andrew, Om Thakkar, H Brendan McMahan, and Swaroop Ramaswamy. Differentially private
learning with adaptive clipping. arXiv preprint arXiv:1905.03871, 2019.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate
impact on model accuracy. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
fc0de4e0396fff257ea362983c2dda5a- Paper.pdf.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175-1191, 2017.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H Brendan MCMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv
preprint arXiv:1812.01097, 2018.
Clement Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. arXiv preprint arXiv:2004.00010, 2020.
Wei-Ning Chen, Peter Kairouz, and Ayfer Ozgur. Breaking the communication-privacy-accuracy
trilemma. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 3312-3324. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
222afbe0d68c61de60374b96f1d86715- Paper.pdf.
10
Under review as a conference paper at ICLR 2022
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
Cynthia Dwork. Differential privacy and the us census. In Proceedings of the 38th ACM SIGMOD-
SIGACT-SIGAI Symposium on Principles of Database Systems, pp. 1-1, 2019.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Gergely Flamich, Marton Havasi, and Jose MigUel Herndndez-Lobato. Compressing images by
encoding their latent representations with relative entropy coding. Advances in Neural Information
Processing Systems, 33, 2020.
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients -
how easy is it to break privacy in federated learning? In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 16937-16947. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf.
Antonious M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh.
Shuffled model of federated learning: Privacy, communication and accuracy trade-offs. arXiv
preprint arXiv:2008.07180, 2020.
Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communica-
tion complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational
Complexity (CCC’07), pp. 10-23. IEEE, 2007.
Marton Havasi, Robert Peharz, and Jose Miguel Herndndez-Lobato. Minimal random code learning:
Getting bits back from compressed model parameters. arXiv preprint arXiv:1810.00440, 2018.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Rad-
ford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam
McCandlish. Scaling laws for autoregressive generative modeling, 2020.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. Characterising
bias in compressed models. arXiv preprint arXiv:2010.03058, 2020.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Peter Kairouz, Ziyu Liu, and Thomas Steinke. The distributed discrete gaussian mechanism for
federated learning with secure aggregation. arXiv preprint arXiv:2102.06387, 2021.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Estelle Laziuk. Daily ios 14.5 opt-in rate, 2021. URL https://www.flurry.com/blog/ios-
14- 5- opt- in- rate- att- restricted- app- tracking- transparency-
worldwide-us-daily-latest-update/.
11
Under review as a conference paper at ICLR 2022
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. arXiv preprint arXiv:1710.06963, 2017.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263-275. IEEE, 2017.
Ilya Mironov, Kunal Talwar, and Li Zhang. Renyi differential privacy of the sampled gaussian
mechanism. arXiv preprint arXiv:1908.10530, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran
Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-
an-imperative-style-high-performance-deep-learning-library.pdf.
Vasyl Pihur, Aleksandra Korolova, Frederick Liu, Subhash Sankuratripati, Moti Yung, Dachuan
Huang, and Ruogu Zeng. Differentially-private" draw and discard" machine learning. arXiv
preprint arXiv:1807.04369, 2018.
Vibhor Rastogi and Suman Nath. Differentially private aggregation of distributed time-series with
transformation and encryption. In Proceedings of the 2010 ACM SIGMOD International Confer-
ence on Management of data, pp. 735-746, 2010.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Elaine Shi, TH Hubert Chan, Eleanor Rieffel, Richard Chow, and Dawn Song. Privacy-preserving
aggregation of time-series data. In Proc. NDSS, volume 2, pp. 1-17. Citeseer, 2011.
TFF Authors. Tensorflow federated stack overflow dataset. Online: https:
//www.tensorflow.org/federated/api_docs/python/tff/simulation/
datasets/stackoverflow, 2019.
Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and FrangoiSe Beaufays. Understanding
unintended memorization in federated learning. arXiv preprint arXiv:2006.07490, 2020.
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th
ACM Workshop on Artificial Intelligence and Security, pp. 1-11, 2019.
Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical
Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021.
Yao Xiao and Josh Karlin. Federated learning of cohorts. URL https://wicg.github.io/
floc/.
12
Under review as a conference paper at ICLR 2022
Appendix
A Proofs
A.1 Proof of Theorem 1
Theorem 1. After T rounds, with the client-to-server bitrate b, DP-REC is differentially private with
TT
δ ≤ min e-λε Y e(I-I)cλt)+λcλt+ι + 12 X *”,
λ	t=1	t=1
where the constant Cf) ≥ D∖ (q(t)∣∣pθt)) is the upper bound on the Renyi divergence of order λ
between the client model update distribution qφ(t) and the server prior p(θt) in any given round t.
Proof. Consider a privacy mechanism A : D → Rd, mapping a dataset D ∈ D to a d-dimensional
model update ∆ ∈ Rd . Recall that our mechanism features two sources of randomness: drawing
from distributions pe and then q∏ (which is based on qφ and pθ).
Similarly to the derivations for the moments accountant (Abadi et al., 2016), we can write
Pr[A(D) ∈ Ω] = Pr[A(D) ∈ Ω ∩S] + Pr[A(D) ∈ Ω ∩S]	(5)
≤ eε Pr[A(D0) ∈ Ω] + Pr[A(D) ∈ S],	(6)
where Ω ⊂ Rd is an arbitrary set of outcomes, S = {∆k* : Lqn (∆k*, D, D0) > ε} is the set of
outcomes where the bound on privacy loss is violated, and S denotes a complement. For multiple
iterations, k can be viewed as the multi-index picked from the importance sampling distribution
across all iterations.
First, let us make a brief side-note on the privacy loss. As we pursue the goal of client-level privacy,
we consider two clients with different local datasets D and D0. Their update distributions are
parameterized by φ for one client and φ0 for another: q((1:T)(∙) and q((10:T)(∙). We will denote the
corresponding importance sampling distributions as q((1'τ)(∙) and q∏1o'T)(∙). Then the privacy loss for
these two clients is
Lq∏∙τ) ∣∣q∏10:T) (Z*:T),D,
log
log
log
渥:T)
F
祢1:T )(・)
P∏1:T )(∙)
蕾T )(•)
PF^
+ log
+ log
/:T )(∙)
P∏1:T )(∙)
e⅛
Pr^
(7)
(8)
(9)
L行(LT) (∙) + L行(LT) (∙) ,
qπ	qπ0
(10)
where PnLT) = Pn；T) is the uniform distribution over K samples from pθ1:T). Thus, it is sufficient to
bound the privacy loss Lq(i：T) (∙) for the worst-case φ with some ε, δ. For the centralized guarantee,
it would correspond to bounding the influence of adding or removing one client. If the local
guarantee, or bounding the influence of substituting a client, is necessary, it will be given by 2ε (and
correspondingly, 2δ). This is consistent with prior work where bounds on substitution are also double
the bounds on addition/removal.
13
Under review as a conference paper at ICLR 2022
Consider the second term of Eq. 6 for T rounds of training:
Pr [刀渭T) > ε]	(11)
K
=ZP尸) 32 …δitk)	x 渭 ©?)…
k1=1
K
X 姨(∆kT )回？ T)) l{Lj∏ >ε}d∆11KT),	(12)
kT=1
where
(1:T)
qπ
(13)
is the total privacy loss across all samples from all iterations. Note also that by p∏(∙) We denote an
importance sampling distribution formed when the proposal and the target are the same, which is
essentially uniform over the K samples.
Since 1{.} ≤ 1, We can employ the bound (4) directly. However, due to the iterative importance
sampling over rounds and possible dependencies betWeen rounds, We have to use the laW of total
expectation and apply the bound recursively. Namely, denoting Z(T)(∆11jT)) = 1{l^>ε},
Pr[L (1:T) > ε]
qπ
=/pθ1) (∆11K)χ 潦) (∆k1))…	(14)
k1=1
K
/pθ) (∆1TK∣∆11KTT)) X q∏) (∆kT)∣ ∆11KTT))Z(T) (∆11KT)) d∆iικT),
kT=1
K
≤	pθ (∆11K )X 谭) (∆k1))…	(15)
k1=1
/ pθ) (∆1TK∣∆11KT T)) ]/ qφ) 3T )∣∆11KT T)) Z (T) (∆11KT)) d∆(T)+K2 PT ]d∆11KT)
K
≤	pθ (∆11K )X 谭) (∆k1)) ...	(16)
k1=1
/ 招) (∆1TK∣∆11KT T)) / qφ) 3T )∣∆11KT T)) Z (T) (∆11KT))
ζ(T-1)
(△1：KT T)
d∆ )d∆1Tκ d∆11κT T)
12
+ KPT,
(17)
Where We can pull (17) out due to its independence from the rest of the expectation.
As noted in line (16), one can then treat the inner expectations (over the distributions from round T)
as anew function Z(T-1)(∆11:T-1)), which is still bounded by 1 because it,s an expectation of an
indicator function. Repeating the procedure, We get
Pr[L (1:T) > ε]
qπ
≤ E (1:T)
pθ
1r
log
12 T
+ 2b XPt，
t=1
(18)
%r)
P(LT)
14
Under review as a conference paper at ICLR 2022
with Pt = eD2 qφφ llpθ . I It is worth noting one more time that qφ andPt are conditional distribu-
tions, depending on rounds 1...t - 1, and that we do not assume independence between rounds.
Applying Chernoff inequality to (18):
Pr[L(J(LT) > ε]
φπ
≤ e-λεE (1:T)
pθ
'λ,	o∏(∆i;KT))]]
eλ log P∏KT) Il +1b p(i：T)
e-λεE (1:T)
pθ
q∏ (δ% ))y
p∏W^)
+ 2bρ(LT)
(19)
(20)
”)
”)
As we have done above in (14), we re-arrange expectations using the law of total expectation:
12 ρ(1:T)
2b'
q∏ 3kT) △严T)) ʌ
p∏ (AkT) ∆kVTTj)
(21)
Analogously, let us apply the chain rule to the inner expression:
(22)
λ
Continuing on (18):
=e-λεEpθ1) hEφφ1)	h ... EpθT)	hEφφτ) h'φ∏1)	•…∙ 'j∏τ) ii	..Ji + 12 ρ(1τ)	(23)
=e-λεEpθ1) Eφφ1)	'j∏1) ...	EPΘT) hEφφτ)	h'j∏τ)ii ...
X--------{--------}
_	[	LqT ≤ κλ
If we boundthe quantity L(T) by some constant 凡人,independent of all the previous samples △?:T-1),
φπ	k
we can bring it in front of the rest of the expectation. Note that this quantity is not exactly the privacy
loss of the mechanism in round T , and the slight abuse of notation is for simplicity purposes.
By again performing this operation recursively,
≤ e-λεκT + 2∣P(LT).	(25)
Let us therefore consider any of such terms in isolation. To proceed, observe that we can switch from
importance sampling to the original continuous distributions inside the expectation in the following
way:
%(∆kt)) = qφ(∆Q∕Pθ(∆kt)) Pk Pθ(∆kt))∕Pθ(∆kt))
p∏(∆kt)) — Pθ(∆kt*))∕Pθ(∆kt)) Pk qφ(∆kt))∕Pθ(∆kt))
=qφ(∆kt))	1	X Pθ (∆kt)) qφ(∆kt))
pθ (∆kt)) Pk qφ (∆kt))∕pθ (∆kt)) ⅛ qφ(∆kt)) pθ ∆)
qφ(∆kt)) E	a At，))
PθA) NF [qφ(∆f)
(26)
15
Under review as a conference paper at ICLR 2022
Hence, keeping in mind that expectations and distributions are conditioned on the previous t - 1
rounds,
(∆kt0)) #
∆(t0)
pθ (
λ log E
qΦ(A
e
e
λ
qφ ∆(kt0)
(27)
(28)
(29)
The first expectation is equivalent to the one in DP-SGD and is basically a moment-generating
function of the privacy loss random variable between two sequences of Gaussian distributions (or
mixtures when subsampling is used) over T rounds.
Let us consider the second expectation, which requires further manipulation to avoid using the privacy
sensitive importance weights. We cannot employ the bound by Agapiou et al. (2017) because the
16
Under review as a conference paper at ICLR 2022
function inside not bounded. However, we can utilize the special form of this function:
P 3禺「
qΦΓ∆^
E
∆1tK 〜;
E
△1tK 〜pθt)
E
∆1tK 〜;
≤E
∆1tK 〜;
E
∆1tK 〜pθt)
Pl qφ(∆(t))∕pθ(∆(t))
K
K
Pl qφ(∆p)∕pθ(∆(t))
K
Pl qφ(∆l%∕pe(∆(t))
K
Pl qφ(∆l%∕pe(∆(t))
K
Pl qφ(∆p)∕pθ(∆(t))
K
Pl qφ (At))/Pθ (At))
qφ(∆ikt) pθ(∆kt))
X	pθ。心力 qφ(∆(kt))
T Pl qφ(∆l%∕Pθ(At))
pθ N
qφ M)
λ+1
Σ
k
pθ (7?
qφ (屋；)
λ
Pθ (Akt)
qφN
λ
JiYl
qτW ]
qφ(Att ")∕Pθ (Akt)) p Pθ (Akt)八 λ+1
Pl qφ(∆(t))∕Pθ(At)) ∖qφ(∆kt))
∆(小金)
eλDλ+ι(pθ Uqφ)
(30)
(31)
(32)
(33)
(34)
(35)
(36)
(37)
(38)
(39)
(40)
Putting everything together, we have
≤ e(λ—1')Dλ(<lφ\|Pθ)+λDλ+ι(pθHqφ) ≤ Kλ
(41)
where κλ does not depend on any of the previous rounds, since We Can bound Renyi divergences
(e.g., by clipping the model updates), and is defined as
κλ ≡ max e(λ-1)Dλ(qφllPθ ) + λDλ + l(pθ Hqφ)
φ,θ
(42)
It then satisfies the conditions to obtain Eq. 25, and consequently, proves the theorem with
(t)
cλ = maX
JmaXθ,φ Dλ(q(φ)||P(et))
ImaXθ,φ。入咸^成))
(43)
□
17
Under review as a conference paper at ICLR 2022
A.2 Compression by parts
Let us consider the setting where parts of the model (such as tensors, or even individual parameters)
are compressed independently. Assume all model parameters ∆ are split into M non-intersecting
groups ∆[1] , . . . , ∆[M] and are encoded using b1, . . . , bM bits correspondingly. We use square
brackets to distinguish these indices from the round indices. The following holds.
Theorem 3. Expectation of an arbitrary function ζ (∆[1] , . . . , ∆[M]) over the importance sampling
distributions qπ[1] , . . . , qπ[M], built according to the outlined procedure for each parameter group
independently, is equivalent to the expectation over the joint importance sampling distribution qπ
built on 2PiM=1 bi samples, i.e.,
E∆[1]〜q∏1] h …E∆[M]^q>M] hζ (△[1],..., "M)[…]=E∆[LM]〜q∏h Z ("1:^)] .
Proof. To show that the above is true it is sufficient to write down these expectations:
E∆[1]〜q∏1] […E∆[M]〜q* hZ(A[1],..., "M])i ...]
(44)
2b1	[1]	[1]	2b2	2bM	[M]	[M]
X	qφ (Akι"pθ (AkI) X X	qφ(AkM "pθ (AkM )	ζ (a[1] a[m ])
p^	P C f∕∖[1h∕g f∕∖[1h	...	P C / ʌ	[M ]、/g	/ ʌ[M ]、	kι,..., kM
k1=1	l1 qφ(Al1 )∕pθ(Al1	) k2=1	kM=1	lM qφ(AlM )∕pθ(AlM	)
(45)
2b1 2b2	2bM
XX...X
k1=1 k2=1 kM=1
qφ(∆k1ι])∕pθ (Aki])
qφ(∆kM])∕pθ (∆M])
P C (Λ[1h∕g (Λ[1h ... P C ∕Λ[Mh/g /ʌ[M]、
l1 qφ(∆l1 )/pθ (∆l1 )	lM qφ(∆lM )/pθ (∆lM )
ζ(∆[k11],...,∆[kMM])
(46)
2b1 2b2	2bM
XX...X
qe(411])…qφ0kM)
Pθ(∆k1ι])...pθ(∆kM)
k1=1 k2=1
2b1 +...+bM
X
k * ^1
kM=1	l1 ... lM
qφ(∆k1*M])
qφ(∆l1])…qφ(∆lM])
Pθ(∆l1])...Pθ(∆lM])
ζ(∆[k11],...,∆[kMM])
(47)
pθA1* MJ)	Z (A [1:M] )
P2bι + ...+bM qφ(∆[*M]) Z ( k*	)
U = 1	pθ(∆l*
(48)
E∆[1M]〜q∏ [Z(A[1:M])],
(49)
where k = (k1, k2,..., kM) and l* = (I1,l2,...,lM) are multi-indexes. The line (48) is because
distributions qφ[1] , . . . , qφ[M] and p[θ1] , . . . ,p[θM] for different parts of the model are parameterized by the
model updates / learnt parameters, and hence, are independent given these updates / parameters. □
B	Additional Discussion and Algorithms
B.1	Additional discussion
Computational overhead DP-REC does introduce additional computational requirements compared
to standard FedAvg and compared to DP-FedAvg. Every line in Algorithm 3 involves some
additional computation due to the REC compression. More specifically, REC involves locally sampling
K i.i.d. normal samples of dimensionality of the update. After clipping updates, REC computes the
importance samples αk, essentially calculating log-probabilities of the samples under two Gaussian
distributions. Finally, sampling from the categorical distribution q∏, which is relatively cheap since We
perform per-tensor compression. It is difficult to benchmark the real-world impact of this additional
computational overhead given heterogeneous hardware. A practical implementation would sample the
standard-normal samples during training (or even during idle-time), as well as the Gumbel-samples
for the categorical distribution. The log-probabilities of the prior can equally be pre-computed.
The index-selection procedure can be parallelized if the hardware supports it. In our simulated
18
Under review as a conference paper at ICLR 2022
environment on a RTX2080 GPU, which runs everything sequentially (training, then per-tensor
Gaussian sampling, per-tensor α computation and index-selection), for the Cifar10 experiment with
K = 27, a single-client’s epoch has a roughly 70%:30% ratio of training-to-compression, with some
variance due to the different local data-set sizes.
Increasing accuracy at the cost of communication There is a noticeable accuracy gap between
DP-REC and DP-FedAvg. A reasonable question to ask is whether it is possible to trade higher
communication spending for better accuracy? Unfortunately, it is not as simple. The reason for
observing the gap lies not in compression but rather in privacy accounting. Figure 2 shows empirically
that increasing bit-width has marginal returns, up until doing no compression at all. Any configuration
in terms of higher bit-widths or more fine-grained vector quantization can be expected to perform
between 7-bit per-tensor quantization and no-compression. Compression-only experiments in Ap-
pendix D.4 also corroborate this point, as the non-private compressed model achieves performance
much closer to FedAvg. Expressing the divergence bound of discrete distributions through continu-
ous distributions leads to nearly double the amount of noise necessary for an equivalent guarantee
compared to the normal continuous Gaussian mechanism. Thus, we would need to relax privacy to
close the accuracy gap. This bound with overhead appears to be tight too, judging by some of our
synthetic experiments measuring how close it comes to the divergence computed from actual discrete
distributions. However, there is still a possible way to reduce the accuracy difference by employing
stronger privacy amplification (e.g. adding a secure shuffler), which will counter the overhead of the
bound. This effect is seen in StackOverflow dataset, where subsampling amplification is stronger due
to a large number of users and the accuracy gap between DP-FedAvg and DP-REC is drastically
smaller.
19
Under review as a conference paper at ICLR 2022
B.2	Additional algorithms
B.2.1	Server-to-client message compression
Algorithm 5 and Algorithm 6 formalize the process described in Section 2.5.
Algorithm 5 The server side algorithm for the com-
pression of server-to-client communication. Rs(t) is
the client-chosen shared random seed for round t. dec.
describes Alg. 4. O(t) describes the sever-side opti-
mizer including its state (e.g. momenta)
Hs = {(R0, O(0))} ∀s ∈ S . History with model
initial seed R0
for t ∈ {0 . . . T } do
Sample set S0 of participating clients
M — {}	. Round memory
for s ∈ S0 in parallel do
msgs — create_message_f or_client(s, Hs)
Send msgs to client s
Receive k*, R(t) . client-chosen index, seed
M ― M∪{(kS,R(Sty)}
end for
for s ∈ S do
Hs — update_history(s, S0, Hs, M))
end for
△(t, J S10 Pk*∈M dec∙(R⑴，k*)
w(t+1) — w(t) - O(t) (∆(t))
end for
Algorithm 6 The client side algorithm for
the decompression of server-to-client com-
munication. dec. describes Alg. 4
if msgs = (w(t), O) then
w(t) — w(t)
Os(t) — O(t)
else
H — msgs
w(t) — ws(prev) . Last-known server
model
for M ∈ H do
ifM = (R0, O0) then
w(t) — initialize(R0)
Os — O(0)
else
δ 一 ∣J⅛TP(k,R)∈Mdec.(R, k)
w(t) — w(t) - Os(∆)
end if
end for
end if
return w(t)
procedure CREATE MESSAGE FOR CLIENT(s, Hs)
if siz e(Hs) > size(w(t)) + size(O(t)) then
msgs — (w(t), O(t))
else
msgs — Hs
end if
Hs — {}	. Reset client history
return msgs
end procedure
procedure UPDATE HISTORY(s, S0, Hs, M)
if s ∈ S0 then
Hs — Hs ∪{M∖{(K,R(t))})}
else
Hs — Hs ∪ {M)}
end if
return Hs
end procedure
B.2.2	Algorithm for accounting privacy in DP-REC
Algorithm 7 describes how we compute ε for a target δ in DP-REC in general. More specifically,
in our experiments, pθt) = N(0, σ2I) and qφ = N(φq, σ2I). For such distributions, the Renyi
divergence between them evaluates to
Dλ (qφ⅛θt)) = 2⅛ kφq k2.	(50)
20
Under review as a conference paper at ICLR 2022
Thus, for a given λ and σ, bounding this divergence corresponds to clipping the norm of φq, i.e.,
clipping ∣∣φq∣∣2 to C corresponds to Dx (q(t)∣∣pθt)) ≤ λC, ∀φ, θ. In order to allow for privacy
amplification with subsampling, we also need to bound the Renyi divergence between the prior 需t
and the mixture B qφ + N-Bpθt) where N corresponds to the number of participants in the federation
and B/N to the subsampling rate (Abadi et al., 2016; Mironov et al., 2019). In the general case, we
have to consider the maximum over the two possible directions:
Dλ — pθt) + Bq(t)∣ pθt)),	Dλ (PW NNB帝 + Nq” .	⑶)
However, the calculation can be simplified, at least for certain choices of the distributions p(θt), qφ(t),
as (Mironov et al., 2019) show that
Dλ ( NNBPt + Bq(t)∣ pθt)) ≥ Dλ (pθ" NNB招 + Nqφ),	⑴)
allowing us to focus on the first term. Furthermore, again following (Abadi et al., 2016; Mironov
et al., 2019), this divergence can be simplified for a general mixture with weights α and (1 - α), our
specific choice of qφ(t) andp(θt), and for integer λ:
Dλ (1 - α)p(θt) +αqφ(t)p(θt)
=λ⅛og
=λ⅛og
≤ λ⅛og
(53)
(54)
(55)
This allows us to readily calculate all of the terms in Algorithm 7.
Algorithm 7 Privacy accounting for DP-REC with subsampling for privacy amplification. Λ are the
Renyi orders λ > 1 that will be considered and b are the total number of bits used to represent the
neural network update. Furthermore, T are the total communication rounds for training, B is the
number of users considered on each round and N is the total number of users in the federation. δ is
the target probability of DP failing to provide a guarantee.
ρ(0) - 0
^λ0) 一 0, ∀λ ∈ Λ
mλ0) 一 0, ∀λ ∈ Λ
for t - 1,..., TB do
P(G - p(t-1) +maχθ,φ eD2(公)1展))
for λ ∈ Λ do
^λt工 ^λt-1t + max ]
m λ J m ʌ-1) + max
maχθ,φ Dλ (N- pθtt + -N q(^t∣∣ pθtt)
maχθ,φDλ (pθt)∣∣ N-PSt + Nq(t))
J maχθ,φ Dλ+1 ( NN1 pθ + N qφtt∣∣ P(t)
lmaxθ,φ Dλ+1 (pθtt∣∣ N-1 Pp) t + Nqφt
end for
end for
ε J min (λ-1 ^λTB) + m(T)- j log (δ -	P(TB)))
21
Under review as a conference paper at ICLR 2022
B.2.3 Overall algorithm for federated training with DP-REC
Algorithm 8 Overall federated training pipeline when using DP-REC. S corresponds to all clients in
the federation and s to a specific client. C corresponds to the desired sensitivity.
procedure SERVER TRAINING(T, σ)
Hs = {(R0, O(0))} ∀s ∈ S	. History with model initial seed R0
for t ∈ {0 . . . T } do
S0 J Sample set of participating clients with replacement
M J {}	. Round memory
for s ∈ S0 in parallel do msgs J create_message_f or_client(s, Hs ) k* ,R(t) J Clientjtraining(msgs) M jM∪{(K,RSt))} end for for s ∈ S do Hs J update_history(s, S0, Hs, M)) end for △(t)J S10 Pk*∙∈M dec.(R(t),k) w(t+1) J w(t) - O(t) (∆(t)) end for ε J compute ε guarantee for a given δ.	. Using Alg. 7
end procedure procedure CLIENT TRAINING(msgs) w(t) J receive_message(msgs)	. Refers to Alg. 6
w(st) J w(t) for local epoch e ∈ {1, . . . , E } do for batch b ∈ B do w(t) J w(t) — V'(wSt),b)	. ` corresponds to the local loss
end for end for φ(st) J ws(t) — w(t) φSt) = φSt) min(1,C∕kφSt)∣∣2) R(st) J Choose a random seed k J enc.(φSt), Rst))	. Using Alg. 1
return k*, Rt end procedure	
C Experimental details
The experiments in the main text were performed on two Nvidia RTX 2080Ti GPUs, as well on
several Nvidia Tesla V100 GPU’s available on an internal cluster over the span of two weeks.
C.1 Datasets & models
MNIST For MNIST, we consider a LeNet-5 (LeCun et al., 1998) model trained on a federated
version of the original 50k training images. We split the data across 100 clients in a non-i.i.d. way
where the label proportions on each client are determined by sampling a Dirichlet distribution with
concentration α = 1.0 (Hsu et al., 2019). For evaluation, we assume the standard validation split of
MNIST to be available at the server. We run all experiments four times with different seeds, except
the one with DP-REC and ε = 3 which we run with eleven seeds, as it had an “outlier” run which
skewed the average and increased considerably the standard error.
FEMNIST FEMNIST is a federated version of the extended MNIST (EMNIST) dataset (Cohen
et al., 2017). It consists of MNIST-like images of handwritten letters and digits belonging to one
22
Under review as a conference paper at ICLR 2022
Table 2: Federated training sets
Dataset	Number of devices	Total samples	Samples per device	
			mean	std
MNIST	100	50, 000	500.00	73.10
FEMNIST	3500	705, 595	201.60	78.92
Shakespeare	660	3, 678, 451	5573.41	6460.77
StackOverflow	342, 477	135, 818, 730	396.58	1278.94
of 62 classes. The federated nature of the dataset is naturally determined by the writer for a given
datapoint. Additionally, the size of the individual clients’ datasets differ significantly. In the literature,
there are two versions of this dataset used for experimentation. Originally published by (Caldas et al.,
2018), their published code1 provides a recipe to pre-process the dataset into the federated version.
Unfortunately, however, the statistics reported in the paper do not align with the result of this recipe.
We repeat the statistics as we use them for our experiments in Table 2. As mentioned in the main
text, some works such as DDGauss (Kairouz et al., 2021) use the FEMNIST version provided by
tensorflow federated2. It consists of a smaller subset of 3400 clients. For the model architecture,
we consider the convolutional network described in (Kairouz et al., 2021), albeit without dropout
regularization. We run four seeds for DP-FedAvg and DP-REC with ε = 1, 3, 6.
Shakespeare For the Shakespeare dataset we closely follow (Caldas et al., 2018). Each client
corresponds to a unique character across the collection of Shakespeare’s plays with a minimum
number of spoken lines. The non-i.i.d. characteristics of this dataset are due to the different
”speaking" styles of the resulting 660 roles. We use the same 2-layer LSTM model as in (Caldas et al.,
2018) for this next-character-prediction task (considering a library of 80 characters). Each client
predicts the next character following the LSTM encoding of the previous 80 characters. For Table 2
we consider each pair of 80 character plus next character as a single sample. The statistics we report
differ markedly from the statistics in (Caldas et al., 2018), as reported on a corresponding issue raised
in their code base3. We run four seeds for both DP-FedAvg and DP-REC.
StackOverflow The StackOverflow dataset (TFF Authors, 2019) consists of a collection of ques-
tions and answers posted on the StackOverflow website during a certain time window. Each user of
that website who posted there in that time-frame is considered a client with their aggregated posts
as the client’s dataset. Here, we consider the task of tag prediction described in (Reddi et al., 2020).
Associated with each posting (irrespective of whether it is a question or an answer) is associated at
least one of 500 tags. Each client is therefore performing one-vs-all classification corresponding to
500 binary classifications. We pre-process each post by creating a bag-of-words representation of the
10, 000 most frequent words, normalized to 1. As model, we consider logistic regression. Learning
curves in the main paper were created by selecting the first 10, 000 data-points when iterating over
a shuffled list of hold out clients. As noted in the main text, each run selected a different seed for
shuffling, resulting in non directly comparable learning curves. For the results in Table 1, the final
model was evaluated on all 16, 586, 035 datapoints across all hold out client. Note that with 1, 500
communication rounds and 60 clients per round, only 〜26% of all clients participate in training. We
run two seeds for both DP-FedAvg and DP-REC.
C.2 Hyperparameters
For all of the experiments in the main text we used 7-bit per tensor quantization for DP-REC. This
was determined after the ablation study on the FEMNIST dataset, shown at Figure 2. The only
difference was at the StackOverflow experiment where, to have a reasonable ε DP guarantee, we used
5 quantization groups for the weight matrix of the logistic regression model (each with 106 entries)
and did per-tensor quantization for the biases (i.e., we had 6 quantization groups in total). As we
1https://github.com/TalwalkarLab/leaf
2https://www.tensorflow.org/federated/api_docs/python/tff/simulation/
datasets/emnist
3https://github.com/TalwalkarLab/leaf/issues/13
23
Under review as a conference paper at ICLR 2022
mentioned in the main text, for DP-REC we performed sampling with replacement to select clients for
each round on all tasks, since the improved privacy amplification was beneficial. For DP-FedAvg
we use the traditional sampling without replacement on each round but with replacement across
rounds.
Tuning privacy hyperparameters Privacy guarantees depend essentially on the ratio of the clipping
threshold and the prior noise scale σ. For all experiments, we fixed the ratio, determined by our
accounting upfront, in order to ensure a chosen ε at the end of the experiment. More specifically,
for DP-FedAvg we tune the clipping threshold for each task and pick the appropriate noise scale
for a given ε guarantee. For DP-REC the clipping threshold was a multiplicative factor of the
standard deviation of the prior over the deltas, i.e., cσ, and c was tuned in order to yield a specific
ε guarantee. The free parameter that we thus optimize is σ. For σ, we considered values ranging
from 10-5 up to 1, finding the right order of magnitude and then fine-tuning within that order if
necessary (e.g. considering 0.001, 0.003, 0.005, etc.), based on validation performance. Of course,
in a practical deployment such tuning would need to be taken into account when computing final
privacy parameters, as discussed by Abadi et al. (2016, Appendix D).
MNIST For this task we used SGD with a learning rate of 0.01 for the client optimizer and
Adam (Kingma & Ba, 2014) with a learning rate of 2e - 3 for the server optimizer for all of
the experiments. The β1 , β2 parameters of Adam were kept at the default values (0.9 and 0.999) and
we trained for 1k global communication rounds rounds. We used 10 clients on each round, where
each client performed 1 local epoch with a batch size of 20. For DP-FedAvg the clipping threshold
was 0.01 whereas for DP-REC the prior standard deviation was fixed to σ = 0.005. In order to get
ε = 3, 6 for DP-FedAvg we used a noise scale of 3.8 and 2.15, whereas for DP-REC we used a
c = 0.5 and c = 0.7625 respectively.
FEMNIST For optimization we used SGD with a learning rate of 0.05 locally and SGD with a
learning rate of 1.0 globally, i.e., we averaged the local parameters. For all of the methods we
sampled 100 clients for each round and each client performed 1 local epoch with a batch size of 20.
For DP-FedAvg we trained for 1.5k rounds with a clipping threshold of 0.1 and for DP-REC we
found it beneficial to train for 4k rounds (and thus we had to clip more aggressively on each round)
and the σ was fixed to 0.03. In order to get ε of 1, 3 and 6 we used a noise scale of 5, 1.85 and 1.15
for DP-FedAvg and a c of 0.75, 1.35 and 1.66 for DP-REC.
Shakespeare Both the global and local optimizers for this task were SGD with a learning rate of 1.0.
We trained for 200 rounds where on each round we sampled 66 clients and each client performed
1 local epoch with a batch size of 10. For DP-FedAvg the clipping threshold was 0.3 whereas for
DP-REC the prior standard deviation was fixed to σ = 0.1. In order to get a ε of 3 we used a noise
scale of 2.15 for DP-FedAvg and a c of 1.36 for DP-REC.
StackOverflow For this task we mainly used the hyperparameters provided at (Andrew et al., 2019).
More specifically, for the local optimizer we used SGD with a learning rate of 102.5 whereas for the
global optimizer we used SGD with a learning rate of 10-0.25 and a momentum of 0.9. We sampled
60 clients per round and each client performed 1 local epoch with a batch size of 100. We trained the
logistic regression model for 1.5k rounds. For DP-FedAvg the clipping threshold was 0.05 whereas
for DP-REC the prior standard deviation was σ = 0.05. For a ε of 1 we used a noise scale of 0.957
for DP-FedAvg and a c of 1.227 for DP-REC.
D Additional results
D.1 Accuracy for a fixed privacy budget
Figure 3 shows the accuracy achieved as a function of the privacy budget ε. For a discussion of these
results refer to the main text.
D.2 Private mean estimation
We ran an experiment to compare DP-REC with the method of Chen et al. (2020) and see the effects of
Kashin’s representation and shared randomness. Removing privacy from the equation and comparing
compression methods head-to-head with 1-bit communication (+ bits for shared randomness, as we
24
Under review as a conference paper at ICLR 2022
(a) MNIST
(b) Femnist
0.5	1.0	1.5	2.0	2.5	3.0
Privacy budget
— DP-FedAvg, ε<3
— DP-REC, ε<3
(c) Shakespeare
Figure 3: Test accuracy (%) as a function of the privacy budget ε (for a fixed δ = 1/N 1.1).
(d) StackOverflow
explain below), we find that (Chen et al., 2020) performs slightly better than DP-REC in terms of
mean squared error (MSE), when the latter uses poorly informed prior (e.g. a zero-mean Gaussian).
When DP-REC is equipped with a better prior (e.g. Gaussian with the mean 1 /ʌ/d in all dimensions),
it is comparable or outperforms (Chen et al., 2020). For example, estimating a 200-dimensional mean
from 10k samples, MSE of (Chen et al., 2020) is 0.07, for DP-REC with a poor prior it is 〜0.3, and
for DP-REC with a better prior it is 〜0.03. There are a few points to elaborate on:
•	Shared randomness. Similarly to DP-REC, (Chen et al., 2020) have a choice between public
randomness (defined by the server) or a shared randomness (defined by clients). As we
explained in our paper, the latter is a preferred choice in terms of privacy, but adds a few
bits to client-server communication (for the random seed). We found that it also improves
performance and that the method of Chen et al. (2020) did not work well in few-bit settings
with public randomness (MSE > 10.0 in the above example).
•	Prior. DP-REC is a method that’s more dependent on a good prior. With a poor choice,
in one-shot settings like mean estimation, performance can be compromised. However, it
makes it more suitable for FL scenarios where prior is gradually improved with every round.
•	Adding privacy to the mix. It is important to note that DP-REC communication gets
somewhat penalized due to the overhead term in our Theorem 1, which for the above setting
requires at least 21 bits per message to achieve δ < 10-5. Nonetheless, this is not a problem
in FL, since practical communication bit-width would be larger in any case. Moreover,
DP-REC privacy can be further amplified by implementing the secure shuffler, similarly to
Girgis et al. (2020), resulting in even tighter privacy guarantees.
D.3 Additional baselines
A curious reader may wonder how would a simple baseline of compressed gradients combined
with DP-FedAvg fare against our method. Unfortunately, combining DP-FedAvg (or differential
25
Under review as a conference paper at ICLR 2022
privacy in general) with compression is not a straightforward task, which is why it motivates our
paper. There are two options: (i) ensure DP, then compress the update; (ii) compress, then ensure DP.
Each option has its challenges.
First, consider (i). Adding noise at the client and then compressing the update before transmission,
might not allow to calibrate noise to the aggregate and the use tighter composition theorems (e.g.,
Renyi accountant), degrading the privacy guarantee. This is What essentially led to the development
of hybrid methods such as cpSGD and DDGauss. As this direction was researched in the line of
Work leading up to DDGauss, We take their method as the best representation of such approach.
Let us noW consider (ii). Once updates are compressed by the client, We cannot add noise directly,
because it Would negate any compression. Therefore, the client needs to transmit the update first,
using secure aggregation to protect against honest-but-curious server. We can compress updates using
scalar quantization, but secure aggregation might add communication overhead countering some of
the effects of compression (see BonaWitz et al. (2017)). Even Without the additional communication
overhead of secure aggregation, We empirically observed that such a method can have both Worse
accuracy and Worse communication efficiency than DP-REC and represents a rather Weak baseline.
We ran an experiment on our MNIST task and combined 8-bit scalar quantization of the client updates
(in a Way that satisfies the desired sensitivity) With DP-FedAvg With ε = 3; there We observed
that the global accuracy reached 〜58% after 1k rounds, which is both smaller than the DP-REC
result and significantly more expensive in terms of communication. Lastly, if We Were to consider
vector quantization to get ahead in terms of compression ratios, it would hinder application of secure
aggregation, leaving us without any protection against honest-but-curious server (since adding noise
is not possible either, as it would nullify compression).
Of course, there exists a number of diverse gradient compression methods that could be considered,
and that could be more easily combined with either SecAgg or DP, but we leave exploration of these
methods for future work.
D.4 Compression-only performance of our method
In order to investigate the behaviour of the compression part of DP-REC, we performed some
experiments on our 4k round FEMNIST task where we omit the clipping part of DP-REC. We
consider three runs:
1.	A baseline of vanilla FedAvg on this task without compression,
2.	REC compression with the same hyperparameters as for the with-DP experiments but
without clipping,
3.	REC compression but with quantization of groups of size 64 (instead of per-tensor
quantization),
The results can be found at Table 3, where we also include the results from DP-REC for reference.
We can see that the compression part of DP-REC performs quite well, reaching performance similar
to FedAvg while significantly reducing the total communication costs. It is worthwhile to note that
there is a multitude of other hyperparameters that could be tuned to further refine the compression-
only performance, such as: tuning the prior σ; varying the vector-size over which σ is computed; the
bit-width b; additional application of entropy-coding for the larger number of indices; adaptive K per
round. We leave a more detailed compression-only evaluation of our method to future work.
26
Under review as a conference paper at ICLR 2022
Method	Accuracy	GB
DP-REC (ε=1)	59.3 ± 0.1	14.2
DP-REC (ε=3)	67.0 ± 0.1	14.2
DP-REC (ε=6)	69.1 ± 0.1	14.2
FedAvg (no DP / compression)	86.28	690.6
REC (no DP clipping)	80.69	14.2
REC (no DP clipping, 64)	84.14	332.2
Table 3: Compression-only (i.e., no DP) ablation studies, all run for 4k rounds. * was still improving
after 4k rounds.
27