Under review as a conference paper at ICLR 2022
Contextual Multi-Armed Bandits with
Communication Constraints
Anonymous authors
Paper under double-blind review
Ab stract
We consider a remote Contextual Multi-Armed Bandit (CMAB) problem, in
which the decision-maker observes the context and the reward, but must communi-
cate the actions to be taken by the agents over a rate-limited communication chan-
nel. This can model, for example, a personalized ad placement application, where
the content owner observes the individual visitors to its website, and hence has the
context information, but must convey the ads that must be shown to each visitor
to a separate entity that manages the marketing content. In this Rate-Constrained
CMAB (RC-CMAB) problem, the constraint on the communication rate between
the decision-maker and the agents imposes a trade-off between the number of bits
sent per agent and the acquired average reward. We are particularly interested in
the scenario in which the number of agents and the number of possible actions are
large, while the communication budget is limited. Consequently, it can be consid-
ered as a policy compression problem, where the distortion metric is induced by
the learning objectives. We first consider the fundamental information theoretic
limits of this problem by letting the number of agents go to infinity, and study
the regret that can be achieved. In particular, we identify two distinct rate regions
resulting in linear and sub-linear regret behaviour, respectively. Then, we propose
a practical coding scheme, and provide numerical results for the achieved regret.
1	Introduction
In the last few years, synergies between Machine Learning (ML) and communication networks have
attracted a lot of interest in the research community, thanks to the fruitful interplay of the two fields
in emerging applications from Internet of things (IoT) to autonomous vehicles and other edge ser-
vices. In most of these applications, both the generated data and the processing power is distributed
across a network of physically distant devices, thus a reliable communication infrastructure is piv-
otal to run ML algorithms that can leverage the collected distributed knowledge (Park et al., 2019).
To this end, a lot of recent works have tried to redesign networks and to efficiently represent infor-
mation to support distributed ML applications, where the activities of data collection, processing,
learning and inference are performed in different geographical locations, and should consider limited
communication, memory, or processing resources, as well as addressing privacy issues.
In contrast to the insatiable growth in our desire to gather more data and intelligence, available com-
munication resources (bandwidth and power, in particular) are highly limited, and must be shared
among many different devices and applications. This requires the design of highly communication-
efficient distributed learning algorithms , particularly for edge applications. Information theory, in
particular the rate-distortion theory, have laid the fundamental limits of efficient data compression
with the aim to reconstructing the source signal with the highest fidelity (Cover & Thomas, 2006b).
However, in the aforementioned applications, the goal is often not to reconstruct the source signal,
but to make some inference based on that. This requires task-oriented compression, filtering out the
unnecessary information for the target application, and thus decreasing the number of bits that have
to be transmitted over the communication networks. This approach should target the questions of
what is the most useful information that has to be sent, and how to represent it, in order to meet the
application requirements consuming the minimum amount of network resources.
Our goal in this paper is to investigate a theoretically grounded method to efficiently transmit data in
a Contextual Multi-Armed Bandit (CMAB) problem, in which the context information is available to
1
Under review as a conference paper at ICLR 2022
a decision-maker, whereas the actions can be taken by a remote entity, called controller, controlling a
multitude of agents. We assume that a limited communication link is available between the decision-
maker and the controller to communicate at each round the intended actions. The controller must
decide on the actions to take based on the message received over the channel, while the decision-
maker observes the rewards at each round, and updates its policy accordingly.
This scenario can model, for example, a personalized ad placement application, where the content
owner observes the individual visitors to its website; and hence, has the context information, but
must convey the ads that must be shown to each visitor to a separate entity that manages the mar-
keting content. This will require communicating hundreds or thousands of adds to be placed at
each round, from among a large set of possible adds, within the communication resource and de-
lay constraints of the underlying communication channel, which is quantified as the number of bits
available per agent. This problem may arise in other similar applications of CMABs with commu-
nication constraints between the decision-maker and the controller (Bouneffouf & Rish, 2019).
1.1	Related Work
Given the amount of data that is generated by machines, sensors and mobile devices, the design of
distributed learning algorithms is a hot topic in the ML literature. These algorithms often impose
communication constraints among agents, requiring the design of methods that would allow efficient
representation of messages to be exchanged. While rate-distortion theory deals with efficient lossy
transmission of signals (Cover & Thomas, 2006b), in ML applications, we typically do not need to
reconstruct the underlying signal, but make some inference based on that. These applications can be
modeled through distributed hypothesis testing (Berger, Sep. 1979; AhlsWede & Csiszar, 1986) and
estimation (Zhang et al., 2013; Xu & Raginsky, 2017) problems under rate constraints.
In parallel to the theoretical rate-distortion analysis, significant research efforts have been invested
in the design of practical data compression algorithms, focusing on specific information sources,
such as JPEG and BPG for image compression, or MPEG and H.264 for video compression. While
adapting these tools to specific inference tasks is difficult, recently deep learning techniques have
been employed to learn task-specific compression algorithms (Torfason et al., 2018; JankoWski et al.,
2021), Which achieve significant efficiency by bypassing image reconstruction.
While the above mainly focus on the inference task through supervised learning, here We con-
sider the CMAB problem. There is a groWing literature on multi-agent Reinforcement Learning
(RL) problems With communication links (Foerster et al., 2016; Sukhbaatar et al., 2016; Havrylov
& Titov, 2017; Lazaridou et al., 2017). These papers consider a multi-agent partially observable
Markov decision process (POMDP), Where the agents collaborate to resolve a specific task. In ad-
dition to the usual reWard signals, agents can also benefit from the available communication links
to better cooperate and coordinate their actions. It is shoWn that communication can help overcome
the inherent non-stationarity of the multi-agent environment. Our problem can be considered as
a special case of this general RL formulation, Where the state at each time is independent of the
past states and actions. Moreover, We focus on a particular setting in Which the communication is
one-Way, from the decision-maker that observes the state and the reWard, toWards the controller that
takes the actions. This formulation is different from the existing results in the literature involving
multi-agent Multi-Armed Bandit (MAB). In AgarWal et al. (2021), each agent can pull an arm and
communicate With others. They do not consider the contextual case, and focus on a particular com-
munication scheme, Where each agent shares the index of the best arm according to its experience.
Another related formulation is proposed in Hanna et al. (2021), Where a pool of agents collaborate
to solve a common MAB problem With a rate-constrained communication channel from the agents
to the server. In this case, agents observe their reWards and upload them to the server, Which in turn
updates the policy used to instruct them. In Park & Faradonbeh (2021), the authors consider a par-
tially observable CMAB scenario, Where the agent has only partial information about the context.
HoWever, this paper does not consider any communication constraint, and the partial/ noisy vieW
of the context is generated by nature. Differently from the existing literature, our goal here is to
identify the fundamental information theoretic limits of learning With communication constraints in
this particular scenario.
2
Under review as a conference paper at ICLR 2022
2	Problem Formulation
2.1	Contextual Multi-Armed Bandit (CMAB) Problem
We consider N agents, which experience independent realizations of the same CMAB problem. The
CMAB is a sequential decision game in which the environment imposes a probability distribution
PS over a set of contexts, or states, S, which is finite in our case. The game proceeds in rounds, and
at each round t = 1, . . . , T, a realization of the state st,i ∈ S is sampled from the distribution PS for
each agent i ∈ {1, . . . , N}. At each time step t, and for each agent i, states are sampled iid according
to PS . In the usual CMAB setting, the decision-maker would observe the states {st,i }iN=1 of the
agents, and choose an action (or arm) at,i ∈ {1, . . . , K} = A, for each agent, where K is the total
number of available actions, with probability ∏t,i(at,i∣st,i) given by a (possibly) stochastic policy
πt : S → ∆K. Here ∆K is the K-dimensional simplex, containing all possible distributions over the
set of actions. Once the actions have been taken by all the agents, at each round t the environment
returns rewards for all the agents following independent realizations of the same reward process,
rt,i = r(st,i, at,i)〜 PR(r\st,i, at,i),	∀i ∈ {1,..., N}, which depends on the state and the action
of the corresponding agent. Based on the history of returns up to round t - 1, the decision-maker
can optimize its policy to maximize the sum of the rewards G = PtT=1 PiN=1 rt,i.
2.2	Rate-Constrained CMAB
In our modified version, the process of observing the system states is spatially separated from
the process of taking the actions. The environment states, {st,i}iN=1, are observed by a cen-
tral entity, called the decision-maker, that has to communicate to the controller over a rate-
constrained communication channel, at each round t, the information about the actions {at,i}iN=1
the agents should take. The decision-maker can exploit the knowledge accumulated from the
states and rewards observed by all the agents up to round t - 1, denoted by H(t - 1) =
n({s1,i}iN=1, {a1,i}iN=1, {r1,i}iN=1),..., ({st-1,i}iN=1, {at-1,i}iN=1, {rt-1,i}iN=1), o ∈ H(t-1), to
optimize the policy to be used at round t. Consequently, the problem is to communicate the ac-
tion distribution, i.e., the policy ∏t(a∖st), which depends on the specific state realizations observed
in round t, to the controller within the available communication resources while inciting the minimal
impact on the performance of the learning algorithm.
Specifically, the decision-maker employs function ft(N) : H(t-1) × SN → {1, 2, . . . , B} to map the
history up to time t and the states of the agents at time t to a message index to be transmitted over
the channel. The controller, on the other hand, employs a function gt(N) : {1, 2, . . . , B} → AN to
map the received message to a set of actions for the agents. In general, both functions ft(N) and gt(N)
can be stochastic. Average per period regret achieved by sequences
nft(N),gt(N)otT=1
is given by
TN
P(N)(T) = T E XX r(st,i ,a*(St,i) -r (St,i,at)
t=1 i=1
(1)
where at,i = gt,i(m(t)) is the action taken by agent i based on message m(t) =
ft(N) H(t - 1), {st,i}iN=1 transmitted in round t, a*(st,i) is the action with maximum mean re-
ward in state st,i, i.e., the optimal action, and the expectation is taken with respect to the state distri-
bution PS and reward distribution PR. We say that, for a given time period and N agents, an av-
erage regret- communication rate pair (ρ, R) is achievable if there exist functions
as defined above with rate N log2 B ≤ R and regret P(N) (T) ≤ ρ.
nft(N),gt(N)otT=1
If a sufficiently large rate is available for communication, i.e., R ≥ log K, then the intended action
for each agent can be reliably conveyed to the controller. Otherwise, to achieve the learning goal
while satisfying the rate constraint, the decision-maker must apply a lossy compression scheme,
such that the action distribution adopted by the pool of agents resembles the intended policy as
much as possible.
3
Under review as a conference paper at ICLR 2022
Figure 1: The problem of communicating policies.
3	Solution
In this section, we present the proposed solution to the problem. First of all, we briefly discuss the
algorithm adopted by the decision-maker to solve the CMAB. Then, the communication scheme for
the RC-CMAB with a particular distortion function is provided, which is inherited by the learning
task. We then identify two distinct rate regions resulting in linear and sub-linear regret, and propose
a more practical coding scheme.
3.1	Thompson Sampling
In the proposed solution to the CMAB problem, the decision-maker adopts Thompson Sampling
(TS) Thompson (1935). In particular, it makes use of one TS instance for each state s ∈ S. Con-
SeqUently, the decision-maker maintains an estimate of the distribution Ptg(μ) of the mean reward
μs,a ∈ D ⊆ R of action a in state S at time t. To take the decision in state St, the decision-
maker would sample μtt,a 〜Ptt,a, ∀a ∈ A, and takes the action a* = argmaxa∈A{μtt,a}. This
procedure is repeated for each agent i ∈ {1, . . . , N}. After receiving the rewards {rt,i}iN=1, the
decision-maker can update its belief on μs,a, i.e., the probabilities Psa(μ), in order to minimize
the regret. We notice that this strategy induces a probability distribution πt(a∣s) over the actions
that is πt(a∣s) = RDPte(μ) Q^=1 j∙=α P3j(μ)dμ, where 4,j(μ) is the Cumulative Distribution
Function (CDF) of μj∙, where the random variables μs,a are considered independently distributed.
However, the constraint on the rate imposed by the RC-CMAB formulation makes it infeasible for
the decision-maker to sample, through the pool of agents, the actions directly from the true distribu-
tion πt(a∣s). The agents have to use a proxy Qt(a∣s), which is the one obtained from the message
received. This problem is similar to approximate TS, where a proxy distribution is used to sample
the actions, or the reward means, given that the true distribution is too complex to sample from. In
that case, the bottleneck is given by the complexity of the mean reward distributions, whereas in this
work, it is imposed by the limited-rate communication channel between the decision-maker and the
controller.
3.2	Optimal Solution for the RC-CMAB
We model the environment as a Discrete Memoryless Source (DMS), that generates states from a
finite alphabet S with probability PS, emitting sequences ofN symbols SN = (S1, . . . , SN), one per
agent. We then denote with QSN (m) the empirical probability of state m in SN. We also consider
the sequence of actions aN, and denote with QZN (m,j) the empirical joint probability of the pair
(m,j) in zN = ((S1, a1), . . . , (SN, aN)). The whole picture can be seen in Fig. 1. In the figure
above, the actions taken by the agents are denoted by ^ to indicate that they can differ from a.
However, we are interested in the probability distributions generating the sequences, thus we will
denote with A the random variables indicating both the actions at the controller and decision-maker
side. We assume that the distribution PS is known (or accurately estimated).
The decision-maker can observe the realization SN of the contexts, and its task is to transmit an
index U ∈ {1,..., B}, and the agents can generate from U a sequence aN, such that Q SN αN is close
to PSA(Sa) = PS (S)π (a|S), where closeness depends on a distortion measure E[d(QSNAN , PSA)],
which in general is not an average of a per-letter distortion measure. The problem is a compression
task in which the server has complete (or partial) knowledge of the states SN, and wants to transmit
a conditional probability distribution πa∣s to the agents, consuming the minimum amount of bits,
in such a way that the empirical distributions QtN aN given by the sequence induced by the agents
4
Under review as a conference paper at ICLR 2022
is close to the joint distribution PSA induced by the policy. For a distortion function d(QSA , PSA)
that is 1) nonnegative, 2) upper bounded by a constant Dmax, 3) continuous in QSA, and 4) convex
in QSA, in Kramer & Savari (2007), the authors provide the rate-distortion function R(D), i.e., the
minimum rate R = IogN* * * * B bits per symbol such that E[d(QsNAN, Psa)] ≤ D, in the limit when N
is arbitrarily large. The solution is given by
R(D) =	min	I(S; A),	(2)
QAIS ：d(QSA,PSA"D
where QSA = PS QA|S is the joint probability induced by the environment distribution PS and
policy QA|S, which depends on the information sent by the decision-maker. As we can see, in the
asymptotic limit of N agents, the problem admits a single-letter solution, which also serves as a
lower bound on the finite agent scenario. The studied RC-CMAB model described in Sec. 2.2 fits
into this framework, and in the following section we identify an appropriate distortion function and
provide its characterization.
3.3	The KL-Divergence as Distortion Function
When applying TS to the RC-CMAB problem with limited communication rate, the decision-maker
may not be able to induce the controller to take samples from the true policy πts,a . In Phan et al.
(2019), the authors provide some theoretical guidelines to construct approximate sampling policies
to make the posteriors, i.e., pts,a, concentrate achieving sub-linear regret. In particular, they studied
the case in which the sampling distribution Q differs from the target posterior π, using Dα(π, Q) as
distortion measure, which denotes the α-divergence between the two, and is defined as
Dα(π, Q) =
1 — R π(x)αQ(x)1-αdx
α(1 - α
(3)
In Phan et al. (2019), it is shown in Theorem 1 that, for α > 0, the condition Dα(π, Q) < δ with
δ > 0, cannot guarantee that the posterior π converges in sub-linear time, even for very small values
of δ. On the contrary, for α ≤ 0, the authors provide a scheme that can guarantee sub-linear regret.
In particular, they suggest to introduce an exploration term ρt ∈ o(1) such that Pt∞=1 ρt = ∞, and
the actions are sampled from Qt with probability 1 -ρt, and uniformly atrandom with probability ρt,
while Dα(πt, Qt) < δ for all t. In this case, itis possible to obtain a sub-linear regret. Consequently,
in order to solve the proposed RC-CMAB problem, we decide to investigate this strategy with α = 0,
which leads to the reverse KL-divergence from Q to π, or equivalently, the KL-divergence from π
to Q, defined as Dkl(Q, ∏) = Pχ∈χ Q(χ)log Qx, when X takes values in the discrete set X.
Consequently, to find the optimal constrained policy Qt(a|s), we need to find a solution to Eq. (2),
which is a rate-distortion optimization problem, when the distortion function is given by the reverse
KL-divergence.
We can rewrite the optimization objective of Eq. (2) as a double minimization problem (Sec. 10.8,
(Cover & Thomas, 2006a))
R(D) = min	min	P(s)Q(a∣s) log2 Q(als)
Q(a) Qa∣s：d(QsA,PsA )≤d T	Q(a)
s,a
(4)
Following (Lemma 10.8.1, (Cover & Thomas, 2006a)), the marginal Q"y) = Ex P(x)Q(y∣x) has
the property
Q (y) = argminDKL(P(x)Q(y∣x)∣∣P(x)Q(y)),	(5)
Q(y)
that is, it minimizes the KL-divergence between the joint and the product P (x)Q(y), ∀Q ∈ ∆K.
This means that Q(a) obtained by solving Eq. (4) is indeed the marginal over the actions induced by
Q(a|s). Exploiting this formulation, it is possible to apply the iterative Blahut-Arimoto algorithm
to solve the problem and find the solution. In particular, given that the two sets ∆K and ∆S are
made of probability distributions, and the target measure is the KL-divergence, the algorithm does
converge to the minimum (Csiszar & Tusnady, 1984). The process is initialized by setting a random
Q0(a), which is used as a fixed point to compute
Q1(a|s) = argminQA∣s∖d(QsA,PsA)≤D EP(S) EQ(a|s)log2
Q(WS)
Q0(a)
(6)
5
Under review as a conference paper at ICLR 2022
From Qι(a∣s), We compute the optimal QKa) by solving Eq. (5), which is simply the marginal
Q；(a) = Ps P(s)Qι(a∣s). The process is iterated until convergence.
We now solve the inner minimization problem, i.e., Eq. (6) with fixed Q(a). As we can see, the
constraint on the distortion tries to keep Qa∣s close to ∏a∣s, which is the target distribution. By
minimizing the mutual information, we want QA|S as close to QA as possible, which is the marginal
known by the agent (it is sent once at the beginning of the round and does not depend on the state
realizations), in order to reduce the number of necessary bits to be transmitted. The solution to
Eq. (2) is a conditional distribution QA|S that is a combination of the target policy conditioned on
S, and the marginal distribution.
We now discuss some simple corner cases. If the maximum acceptable distortion is 0, we need
Qa∣s = ∏a∣s , thus the rate is equal to EPS [Dkl (∏a∣s ∣∣∏a) ]. If S and A are independent, i.e., the
policy does not depend on the state, we have ∏a∣s = ∏a, thus the decision-maker does not need to
send any bits to the agents, which can just sample A 〜 ∏a. Moreover, if we can find a distribution
Qa such that EPS [Dkl (Qa∣∣∏a∣s)] ≤ D, then the best strategy is not to convey any message
to the controller, as the constraint is already satisfied with rate R(D) = 0. The problem is to find,
among all conditional distributions Q(a|s) that satisfy the constraint, the one with the minimum
divergence from Q(a), in order to minimize the rate loN B, i.e., the number of bits to be consumed
per agent, for an arbitrarily large N , in the general case. We solve the problem using the Lagrangian
multipliers, and obtain the shape of the optimal distribution given by
Qγ*(a∣s)
Q(a)γ* P (a∣s)1-γ*
__ ~ , ..
Pa0∈A Q(a0)γ*P(a0∣s)1-γ*
∀s ∈ S , a ∈ A,
(7)
where Y* is s.t. EPS [Dkl(Qy* ||P)] = D. The derivation is provided in Appendix A.1.
3.4	Asymptotic Regret
To prove the results on the achievable regret, we need additional assumptions, which are contained
in the Assumption 1 in Russo (2016), which states that rewards have to be distributed following
canonical exponential families, and the priors used by TS over the average rewards have to be uni-
formly bounded, i.e., bounded away from zero ∀(s, a). The proofs of both Lemmas are reported in
the Appendix D.
In the following, we provide the minimum rate needed to achieve sub-linear regret in all states s ∈ S.
We now define H(A*) as the marginal entropy of the optimal arm, computed based on the marginal
∏*(a) = Ps PS(s)∏*(a∣s), and defined as H(A*) = Pa π*(a)log 产号,and we prove that it is
the minimum rate required to achieve sub-linear regret.
Lemma 3.1. If R < H(A*), then it is not possible to convey a policy Qt(s, a) that achieves sub-
linear regret in all states s ∈ S.
The following Lemma provides the achievibility part.
Lemma 3.2. If R > H(A*), then achieving sub-linear regret is possible in all states s ∈ S.
The consequence of this second Lemma is that, even if the exact TS policy πt cannot be transmitted
∀t, as long as sufficient rate is available, i.e., R > H(A*), it is still possible to achieve sub-linear
regret. Following the notation introduced in Sec. 2.2, this implies that, as T → ∞, and for all
sub-linear regrets ρ, the regret-communication pair (ρ, R) is achievable as long as R > H(A*).
Moreover, we argue that the policy construction found in Sec. 3.3 can achieve better empirical
performance w.r.t. the scheme used to prove Lemma 3.2.
3.5	Practical Coding Scheme
The above analysis allows us to characterize an information theoretical bound on the optimal per-
formance, but does not provide a constructive communication scheme. To find a practical coding
scheme, we propose a solution that is based on state reduction and computes a compact state repre-
sentation. In essence, the decision-maker constructs a message containing the new state representa-
tions ^(s) ∈ S of s, one for each agent, and send it over the channel. Once the agents have received
6
Under review as a conference paper at ICLR 2022
the message, they can sample the actions according to a common policy Q^(a∣S), which is defined
on the compressed state space S. If the rate constraint imposes B bits per agent, it means that it is
possible to transmit at most 2B different states to each agent. The idea is to group the states into
2b = M clusters si,..., SM, minimizing Ps P(S)DKL(Q^(a∣^)∣∣∏(a∣s)), where Q^(a∣s) is the
policy defined on the state s(s) ∈ S.
To find the clusters and relative policies we employ the well-known Lloyd algorithm, which is an
iterative process to group states into 2B clusters. First of all, knowing the policy π, the decision-
maker maps each state Si to a K-dimensional point αi = ∏(∙∣Si) ∈ ∆κ, finding |S| = L different
points α1,..., αL. Then, it generates 2b = M random points μ1,..., μM ∈ ∆κ as initial
centroids, i.e., representative policies, and iterates over the following two steps:
1.	Assign to each point αi the class j* ∈ {1,...,M} such that j* = arg min7- DκL(μj∣∣αi),
i.e., minimizing the divergence between the representative μj* and the original policy,
which is the point αi . For each cluster j, we now define with Sj the set containing the
states associated to the policies in the cluster.
2.	Update μ1,..., μM such that μj = argminμ∈∆κ Ps∈s. P (S)DKL(μ∣∣π(∙∣s)), which
is still a convex optimization problem, and can be solved again applying the Lagrangian
multipliers. The solution is
P(S)
j	Qs∈Sj ∏(∙∣s)F
μj = —r------------
Z
(8)
where the product has to be considered element-wise, A (Sj) is the sum of the probabilities
of states in Sj, i.e., A (Sj) = Ps∈S P (S), and Z is the normalizing factor. After com-
puting the new centroids, we go back to step (1). The derivation of Eq. (8) is provided in
Appendix A.2.
The process continues until the new solution does not decrease the KL-divergence between the
clustered and the target policy Dkl(Q^(s, a)||P (s, a)) = PM=I ps∈Sj P(S)DKL (μj∣∣π(∙∣s)).
Observation Note that the controller is assumed to know the 2NR policies from which it samples
the actions of the agents. This can be transmitted at the beginning of each round. In this case, the
scheme is efficient as long as N log2 K >> BLλ log2 K, where λ is the number of bits used to
represent the values of the Probability Mass Function (PMF) Q^(∙∣S). For this reason, We provide a
scheme where the new policy is updated not at every transmission, but just when the new target π has
changed considerably. In particular, if we denote with πcls the policy defined over the compressed
state representation, with πlast the last policy used to compute πcls, and with π the updated target
policy, We compute and transmit ∏cls every time DκL(∏^ast∖∖∏) > β.
4	Numerical results
In this section, we provide numerical results in support of our theoretical analysis. Additional ex-
periments and details can be found in Appendix C
4.1	Rate-Distortion Function
In this first experiment, we analyze the rate-distortion function, and the related optimal Q(a∖S), when
the distortion is given by the KL-divergence in three different problems, when the numbers of states,
L, and of actions, K, are both equal to 16. The target policies π(a∖S) have a one-to-one relation with
the states, and are generated such that π(a = i∖S = j) = 0.99 ifj = i, and uniformly distributed
otherwise. We refer to this as the Deterministic case. In the second experiment, the target policies are
generated in a similar way, but the other actions’ probabilities take random values in (0, 0.05], and
then normalized. This, we call the Random Deterministic case. In the third experiment, the states
are grouped in 4 blocks, such that π(a = i∖S = j) = 0.99 if bj/4c = i, and uniformly distributed
otherwise. This case is denoted as the Block one. In all three experiments, the distribution PS
is uniform over the state space S. The rate-distortion curves are reported in Fig. 2a. As we can
see, in the Deterministic case, the point with zero distortion has R(0)〜H(PS), where H(PS) is
7
Under review as a conference paper at ICLR 2022
Figure 2: Rate-Distortion function for the 3 different experiments (a). π(a|s = 8) and Q(a∣s = 8)
in the third experiment, when the rate R is equal to 1 (b). π(a|s = 8) and Q(a∣s = 8) in the second
experiment, when the rate R is equal to 1.
AdOPted POliCy	， ɪ θ，	7⅞rget POliCy
(b)
(c)
the Shannon entropy of PS, as expected. Indeed, in this case, the action distributions are strongly
correlated with the state, thus we need an accurate knowledge of it. In the second experiment, the
starting point is similar, but the curve decreases more rapidly, caused by the random values of the
other actions, probabilities. In the third experiment, the zero distortion point is achieved at R(0)〜2
bits, since, similarly to the Deterministic experiment, action probabilities are correlated with the state
realization, but are grouped into four different cases. Again, given the uniform distribution over the
state, R(0) 〜H(Unif⑷),where Unif[4] is the uniform distribution over a set of 4 elements. In
Fig. 2c, the approximate distributions for state s = 8 are reported for the Deterministic and Random
Deterministic cases, when R = 1.
4.2	Contextual Multi-Armed Bandit
We now analyze the RC-CMAB problem presented in Sec. 2, and apply the clustered policy schemes
to solve it. In particular, we compare the performance of the Perfect agent, which applies TS with-
out any rate constraint, thus admits samples from the true posterior π, with the performance of
the Comm, Cluster, and Marginal agents. The Comm agent uses the optimal scheme provided
in Sec. 3.2, the Cluster agent implements the practical coding scheme provided in Sec. 3.5, with
B = dRe bits per agent, where R is the rate adopted by the Comm agent; the Marginal agent adopts
the marginal over the states, computed from the target policy π and the environment distribution
PS, and serves as a lower bound on the performance. State distribution PS is uniform over L = 16
states, and there are K = 16 actions, N = 100 agents, and the total number of rounds is T = 200.
The threshold for changing the clustered policy is set to β = 0.2, and the communication rate con-
straint to R = 2.5. In this experiment, for each state si ∈ S, the best reward is given by the arm
aj such that i = j, with i, j ∈ {0, . . . , 15}. In particular, the reward behind arm i when in state
j is a Bernoulli random variable with parameter μj∙ = 0.8 if i = j, whereas μj∙〜 Unif[0,0.75] if
i 6= j . In this case, the best action response is strongly correlated with the state realization, thus
a sufficiently high rate is required to sample from the target policy π . As it can be observed from
Fig. 3(a), the theoretical rate to transmit the target policy is related to the amount of information the
agent is learning from the environment, and it is computed using Eq. (2). Indeed, during the first
〜40 iterations, the rate is below R = 2.5. As the learning process continues, the required rate for
reliable transmission increases, as the mutual information between S and A increases. We highlight
that, in order to fairly represent the 16 different actions, one would need 4 bits. Indeed, another way
of looking at Eq. (2) is through bottleneck principle (Igl et al., 2019), which is used to constrain the
mutual information between the states and the actions, and to encourage exploration, that can be
exploited to reduce the required rate when training multi-agent systems.
In Fig 3 (b), the KL-divergence between the last policy used to compute the compressed state rep-
resentation, i.e., πlast in Sec. 3.5, and the target known by the decision-maker is reported. The
trend shows that, at the beginning of the learning process, it oscillates rapidly, as the target policy is
changing significantly between two iterations. In gray, the threshold β = 2 is highlighted, indicating
when a new cluster policy has to be sent. In this experiment, the decision-maker had to send a new
policy, on average, once every 〜7 rounds. Fig. 4 (a) reports the regret at state S = 10 (the other
8
Under review as a conference paper at ICLR 2022
plots can be seen in Appendix C.2), for the four different agents, as defined in Eq. (1). We can see
that both the Cluster and the Comm agents can achieve sub-linear regret in this particular state. In
Fig. 4 (b), the average rewards obtained by the agents are reported, for the different states separately.
We can see that the developed scheme is still able to achieve good performance.
Figure 3: Asymptotic rates to convey the Perfect and Comm policies, and the bits used by the Cluster
agent, averaged over 5 runs (a). DKL(πlast∣∣π), averaged over 5 runs (b).
(a)
Figure 4: Cumulative regret for different agents, together with the performance of the M arginal
agent, evaluated for the state s = 10 (a). Average reward per state (b).
(b)
5	Conclusion
We have studied the RC-CMAB problem, in which an intelligent entity, i.e., the decision-maker,
observes the contexts of N parallel CMAB processes, and has to decide on the actions depending
on the current contexts and the past actions and rewards. However, the actions are implemented
by a controller that is connected to the decision-maker through a rate-constrained communication
link. First, we cast the problem into the proper information-theoretic framework, formulating it
as a policy compression problem, and provided the optimal compression scheme in the limit of an
infinite number of agents, when the adopted distortion measure is the KL-divergence between the
compressed policy adopted by the controller and the policy of the decision-maker. We then char-
acterize the minimum needed rate to obtain sub-linear regret, and prove that any rates above this
threshold can achieve it. In the end, we designed a practical coding scheme to transmit the actions
for a finite N , which relies on a compressed state representation Finally, we evaluated the perfor-
mances of the policy obtained through the asymptotic information theoretic formulation, and the one
obtained through the clustering scheme, and observed a close gap between the two. We numerically
showed the relation between the asymptotic rate bound and the learning phase of agents, showing
how it is possible to save communication resources when training a multi-agent system like the one
considered. We believe that this work can serve as a first step towards understanding the fundamen-
tal performance limits of multi-agent decision-making problems under communication constraints,
and highlights the intimate relation between the communication scheme and the learning process.
Ongoing work include deriving an information theoretic converse result on the regret performance,
and generalizing the framework to reinforcement learning problems.
9
Under review as a conference paper at ICLR 2022
References
Mridul Agarwal, Vaneet Aggarwal, and Kamyar Azizzadenesheli. Multi-agent multi-armed bandits
with limited communication. In arXiv:2102.08462 [cs], 2021.
Rudolf AhlsWede and Imre Csiszar. Hypothesis testing with communication constraints. 32(4):
533-542, Jul. 1986.
Toby Berger. Decentralized estimation and decision theory. In IEEE 7th. Spring Workshop on Inf.
Theory, Mt. Kisco, NY, Sep. 1979.
Djallel Bouneffouf and Irina Rish. A survey on practical applications of multi-armed and contextual
bandits. arXiv cs.LG:1904.10040, 2019.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, USA, 2006a. ISBN 0471241954.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, USA, 2006b.
Imre Csiszar and Gabor Tusnady. Information Geometry and Alternating Minimization Procedures.
Statistics and Decisions, Supplement Issue, pp. 205-237, 1984.
Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to Com-
municate with Deep Multi-Agent Reinforcement Learning. arXiv:1605.06676 [cs], May 2016.
arXiv: 1605.06676.
Osama A. Hanna, Lin F. Yang, and Christina Fragouli. Solving multi-arm bandit using a few bits of
communication. In 38 th International Conference on Machine Learning, 2021.
Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to com-
municate with sequences of symbols. pp. 11, 2017.
Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin,
and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and
information bottleneck. In Advances in Neural Information Processing Systems, volume 32, pp.
13956-13968, 2019.
Mikolaj Jankowski, Deniz Gunduz, and Krystian Mikolajczyk. Wireless image retrieval at the edge.
IEEE Journal on Selected Areas in Communications, 39(1):89-100, 2021. doi: 10.1109/JSAC.
2020.3036955.
Cem Kalkanli and Ayfer Ozgur. Asymptotic convergence of Thompson sampling. In
arXiv:2011.03917v1, 2020.
Gerhard Kramer and Serap A. Savari. Communicating probability distributions. IEEE Transactions
on Information Theory, 53(2):518-525, 2007. doi: 10.1109/TIT.2006.889015.
Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the
emergence of (natural) language. arXiv:1612.07182 [cs], March 2017. arXiv: 1612.07182.
Hongju Park and Mohamad Kazem Shirani Faradonbeh. Analysis of Thompson sampling for par-
tially observable contextual multi-armed bandits, 2021.
Jihong Park, Sumudu Samarakoon, Mehdi Bennis, and MerOUane Debbah. Wireless network intel-
ligence at the edge. Proceedings of the IEEE, 107(11):2204-2239, 2019.
My Phan, Yasin Abbasi Yadkori, and Justin Domke. Thompson sampling and approximate infer-
ence. In Advances in Neural Information Processing Systems, 2019.
Daniel Russo. Simple bayesian algorithms for best arm identification. In Vitaly Feldman, Alexander
Rakhlin, and Ohad Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of
Proceedings of Machine Learning Research, pp. 1417-1418, Columbia University, New York,
New York, USA, 23-26 Jun 2016. PMLR. URL https://proceedings.mlr.press/
v49/russo16.html.
10
Under review as a conference paper at ICLR 2022
Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. In Proc. of 30th Int’l Conf. on Neural Information Proc. Systems, NIPS’16, pp.
2252-2260, Red Hook, NY, December 2016.
William R. Thompson. On the theory of apportionment. American Journal of Mathematics, 57(2):
450-456, 1935.
Robert Torfason, Fabian Mentzer, Eirlkur Agustsson, Michael Tschannen, RadU Timofte, and
Luc Van Gool. Towards image understanding from deep compression without decoding. In
International Conference on Learning Representations, 2018.
Aolin Xu and Maxim Raginsky. Information-theoretic lower bounds on Bayes risk in decentralized
estimation. IEEE Transactions on Information Theory, 63(3):1580-1600, 2017. doi: 10.1109/
TIT.2016.2646342.
Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic
lower bounds for distributed statistical estimation with communication constraints. In Advances
in Neural Information Processing Systems, volume 26, 2013.
Appendix
A Policies Derivation
A. 1 Optimal policy
To solve this problem, we solve the related Lagrangian
L(Q(Hs),λm=E P(S) EQ(Hs)Iog
s
a
QQas)+λ (X P(S) X Q(a|S) log Q(a7) - D)+
+ μ (XP(s)XQ(a∣s) -1)
where the Lagrangian multiplier λ has to be optimized to meet the constraints on the divergence,
whereas μ ensures that the solution is a probability distributions, i.e., the elements sum up to one.
The positivity constraints on the terms are already satisfied by the fact that the solution has an
exponential shape. We first take the derivative of the Lagrangian w.r.t. to the terms Q(a|s) and set it
to zero
∂L(Q(a∣s),λ, μ)
∂Q(a∣s)
=P(S)Iog QQaas)+P(S)- X Ps0 Q(a|so)Qw+
+λP(S) (ιog K⅛)+1)+μP(S)=0
finding
log
Q(a∣s)1+λ
Q(a)P (a∣s)λ
=-(λ + μ)
Q(a∣S)1+λ = e-("+I)Q(a)P(a∣S)λ
一(μ+ λ) ~	1	λ
Q(a∣S) = e ^ɪ+^ Q(a) 1+λ P (a∣S) 1+λ
—(μ+λ)
and now we rewrite with Y = ɪ+ɪ, Y ∈ [0,1], and μ such that e 1+λ is the normalizing factor.
The solution has thus the shape
≈	,	_	,	.	. -1
. . .	Q(a)γP(a∣S)1-γ	. . C (I
Q (a∣S) =------M——^-L∀----------,	∀s ∈ S, a ∈ A.
Y	Ρɑθ∈A Q(a0)YP(a0∣S)1-Y ,
(9)
By the convexity of KL-Divergence and its triangular inequality, we know the solution lies on the
boundary of the constraints, i.e., when EPS [Dkl(Q)* ||P)] = D. Consequently, if We can numeri-
cally find γ* s.t. EPS [Dkl(Qy* ||P)] = D, we solve Eq. (6).
11
Under review as a conference paper at ICLR 2022
A.2 Update Centroids
Again, we compute the optimal centroids by solving the Lagrangian
L(μja, λ) = X P(S) X μja log -ʃaʒ + λ
a^	π(a∣s)
s∈Sj	a∈A
taking its derivative and solving the equality
∂L(μa,λ)
∂μja
P(S)
s∈Sj
+1 +λ=0
finding
log μjaA (Sj ) = EP(S) log n(a| S) + A (Sj ) + λ
s∈Sj
logμa = X Αp¾logn(a|s) + 1 + A⅛
s∈Sj
P(S)
j = Qs∈Sj ∏(∙∣s) A(Sj)
Z
where Z is the normalizing factor, obtaining the shape expressed in Eq. (8).
12
Under review as a conference paper at ICLR 2022
B Presented algorithms
In this appendix, the algorithms for the theoretically-optimal and cluster policies are described.
B.1	Theoretical Optimal Policy
This is the pseudocode of the algorithm at the decision-maker side, when adopting the information-
theoretic inspired compressed policy. The only task for the controller is to decode the received
message, and to communicate the actions to the agents. We define with [A] the set {1, . . . , A}, and
with UnifB the uniform probability over the set B .
Algorithm 1 Optimal Policy
Input rate R, n° agents N, n° actions K and state distribution PS
Initialize Thompson Sampling policy π, and compressed policy Q(∙∣s) to Unif[κ]
foreach t ∈ 1, . . . , T do
Observe stN = (st,1, . . . , st,N)
Sample aN = (at,ι,..., at,N) from Q(∙∣s)
Code atN into ut = ft(atN ) and transmit to the controller
Observe rtN = (rt,1, . . . ,rt,N)
Update action posteriors ∏(∙∣s) using the Thompson Sampling Algorithm
Update Q(a|s) using the Blahut-Arimoto Algorithm (Iterate over Eq. 5 and Eq. 6)
B.2	Cluster Policy
This is the pseudocode of the algorithm at the decision-maker side, when adopting the cluster policy.
The task for the controller is to sample, at time t and for each agent i, the action at,i according to
the last received centroid 世§(5七.),whose components convey the action probabilities.
Algorithm 2 Cluster Policy
Input per-agent bit B, n° agents N, n° actions K and state distribution PS, threshold β
Initialize Thompson Sampling policy π
Compute the centroids μj for j = 1,..., 2B using π with Lloyd algorithm and the rule in Eq. 8
Transmit the centroids μj to all agents.
πlast = π
foreach t ∈ 1, . . . , T do
Observe stN = (st,1, . . . , st,N)
if DKL (∏last∣∣∏) > β then
Update the centroids μj using π with Lloyd algorithm and the rule in Eq. 8
_ Transmit the centroids μj to all agents.
∀st,i compute the index j (st,i) indicating its belonging cluster
Transmit the vector jtN = (j(st,1), . . . ,j(st,N))
Observe rtN = (rt,1, . . . ,rt,N)
Update action posteriors ∏(∙∣s) using the Thompson Sampling Algorithm
13
Under review as a conference paper at ICLR 2022
C Experiments
In this section, additional experiments are provided to clarify the proposed scheme, and quantify the
trade-offs between rate and regret.
C.1 State Representation
In Fig. 5 we report the policy clustering results when applying the coding scheme explained in
Sec. 3.5, when R = 2 bits can be used to represent 20 randomly generated policies. In the figure,
it is possible to see the 4 policies representative of the 4 compressed state representations. As we
can see, the policy centroids found by the coding scheme try to fairly resemble the shapes of all the
policies within the same cluster.
Figure 5: Clusters and their representatives with L = 20 and b = 2.
C.2 Deterministic
In this section we reported some more details on the RC-CMAB experiments presented in Sec. 4.2.
(a)	(b)
(c)
Figure 6: Best action probability for a given state, for the posterior π(a*|s) (a) and compressed
policy Q(a*∣s) (b) for the different agents. KL-divergence between the agents' action posteriors,
and the target one (c).
14
Under review as a conference paper at ICLR 2022
500
400
一Perfect
—Comm
---Marginal
---Cluster
O 25	50	75 IOO 125	150	175	200
Iteration
Regret for State 3
15
Under review as a conference paper at ICLR 2022
Marginal
Cluster
Regret for State 9
500
Regret for State 11
Regret for State 15
Figure 8: Cumulative average regret ± one
s= 15
一Perfect
—Comm
---Marginal
----Cluster
Oooo
Oooo
5 4 3 2
4->al69cc
std for the different agents, from state s = 0 to state
16
Under review as a conference paper at ICLR 2022
C.3	8-BLOCK EXPERIMENT
In this second RC-CMAB experiment, the setting is similar to the one presented above, but the
best action response is not a one-to-one mapping with the state. Again, a ∈ {0, . . . , 15} and s ∈
{0,..., 15}, but the Bernoulli parameter μa(S) for action a in state S is 0.8 if bSC = a, and sampled
uniformly in (0, 0.75] otherwise. Thus, the best action responses are grouped into 8 different classes,
depended on the state realization. In this case, the rate is limited to R = 2.
(a)
(b)
Figure 9: Block experiment : asymptotic rates to convey the Perfect and Comm policies, and the bits
used by the Cluster agent, averaged over 5 runs (a). Average reward achieved in each state by the
pool of agents (b)
(a)
(b)
φuuα)Etφ>pe
(c)
Figure 10: Block experiment: Best action probability for a given state, for the posterior π(a* |s) (a)
and compressed policy Q(a*∣s) (b) for the different agents. KL-divergence between the agents’
action posteriors, and the target one (c).
17
Under review as a conference paper at ICLR 2022
Marginal
Cluster
teratιon
400
----Marginal
4->al6φc≤
Regret for State 5
18
Under review as a conference paper at ICLR 2022
300
250
4-1 200
φ
φ 150
50
Regret for State 11
teratιon
100
0
500
400
ω
⅛300
φ
200
Marginal
Cluster
12: Block experiment : cumulative average regret ± one std for the different agents,
state s
0 to state s = 15
19
Under review as a conference paper at ICLR 2022
C.4 Regret vs Number of Classes
In this last experiment, the environment is the same as the one described in Sec. 4.2, a part form
the number of agents, that here is N = 50. The task is to quantify the effect of the number of
per-agent bits B in the cluster policy on the achievable regret, which varies from 1 to 4. As we
can see from the plots below, when B is equal to 1 or 2, which in turn means that the number of
clusters is, respectively, 2 and 4, the regret is not sub-linear in several states. However, the policy
with just 2 bits can achieve performance comparable with the 3 and 4 bits policies in some states,
e.g., state 4 and 6. This is due to the fact that even in those cases with not enough bits, the posteriors
still converges to good solutions, thus if a cluster contains, for example, one state, it will convey the
best policy, achieving sub-linear regret. On the contrary, when a cluster contains more states, the
representative policy can fail to achieve optimal performance, if those are substantially different.
Regret for State 7
Iteration
20
Under review as a conference paper at ICLR 2022
teratιon
300
250
200
⅛j
”5。
"100
50
teratιon
teratιon
Figure 14: Deterministic experiment : cumulative average regret ± one std for cluster policy, when
B = {1, 2, 3, 4}, from state s = 0 to state s = 15
teratιon
Iteration
21
Under review as a conference paper at ICLR 2022
D Regret Bound
To prove our statements, we define with Hq(X) and Iq(X; Y ) the marginal entropy and mutual
information w.r.t. to thejoint probability q,i.e., H (A*) = H∏*(A). We start by proving Lemma D.1.
Lemma D.1. If the exact Thompson Sampling policies ∏t(a∣s) achieve sub-linear Bayesian regret
for all state S ∈ S, then limt→∞ I∏t (S; A) = limt→∞ Hnt (A) = H(A*).
Proof. First of all, we write Iπt (S; A) = Hπt (A) -Hπt (A|S). Following Theorem 2 from Kalkanli
& Ozgur (2020), if ∏t(a∣s) achieves sub-linear regret ∀s ∈ S, then ∀s ∈ S
lim πt(a = a* |s) = 1 when a* is the optimal arm
t→∞
lim πt(a = a0|s) = 0 when a0 6= a* .
t→∞
Consequently, We have that in the limit ∏t(a∣s) is a deterministic function, thus
lim Hnt (A*∣S ) = 0,
t→∞
which concludes our proof.	□
Then, We prove Lemma 3.1, Which is repeated beloW.
Lemma 3.1 If R < H(A*), then it is not possible to convey a policy Qt(s, a) that achieves sub-
linear regret in all state s ∈ S.
Proof. Following Lemma 10 in Phan et al. (2019), if limt→∞ ∏t(a* |s) = 1 and DKL (Qt∣∣∏t) < a
∀t, for some e > 0, then limt→∞ Qt(a*∣s) = 1, inducing limt→∞ DKL (Qt∣l∏t) = 0.
Now, for any policy Qt, if the minimal rate to convey it, Rt = IQt (S; A), is below the threshold of
the optimal policy, i.e., Rt < H(A*), by Eq. (2), we have
DKL (Qt∣∣∏*) > 0,	(10)
which holds also in the limit. Otherwise, we would have a policy Qt s.t. DKL (Qt ∣∣∏*) = 0, which
could be conveyed at a rate strictly below that characterized by Eq. (2), which would contradict with
the definition of the rate-distortion function.
Consequently, by Lemma 10 in Phan et al. (2019) and Eq. (10), there cannot be any > 0 s.t.
limt→∞ DKL (Qt∣∣∏t) < e; and hence, limt→∞ DKL (Qt∣∣∏t) = ∞.
HOWeVer, if limt→∞ DKL (Qt∣∣∏t) = ∞, ∃(s,a) ∈ S XA s.t. limt→∞ Qt(a∣s) = c > 0, when
a 6= a* . This implies that, at step t, Qt (s, a) plays a sub-optimal arm in state s with constant
probability, and so it can not achieve sub-linear regret in all states.	□
Finally, we provide a proof for Lemma 3.2, which we repeat below.
Lemma 3.2 If R > H(A*), then achieving sub-linear regret is possible in all states s ∈ S.
Proof. We define by Rnt the rate needed to convey the TS policy perfectly to the controller at time
t, and let δ > 0 s.t. R = H(A*) + δ, where R is the available communication rate. We now provide
a scheme that guarantees sub-linear regret.
First, generate ρt parameters as in Sec. 3.3. As long as Rnt > R, with probability ρt play at
uniformly at random, and with probability 1 -ρt, play according to a policy Qt(a|s), which satisfies
the rate-distortion constraint I(A; S) < R under Qt(a|s), which can be transmitted to the controller,
and will have a bounded reverse KL divergence from the TS policy πt at time t. Following Lemma 14
in Russo (2016), in this way enough exploration is guaranteed for the TS policy to concentrate, and
there exists a finite t0 s.t. ∀t > t0, Rnt < H(A*) + δ. This means that, for the first t0 rounds, both
the target and the approximating policies are playing sub-optimal arms with non-zero probabilities.
However, their average gap is within a constant, given the average rewards are bounded within [0, 1].
Then, ∀t > t0 it is possible to play the exact TS policy, leading to the optimal policy for all future
steps, and hence, a sub-linear regret.	□
22