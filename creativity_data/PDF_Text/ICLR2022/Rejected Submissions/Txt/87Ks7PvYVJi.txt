Under review as a conference paper at ICLR 2022
Offline	Decentralized	Multi-Agent	Rein-
forcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
In many real-world multi-agent cooperative tasks, due to high cost and risk, agents
cannot continuously interact with the environment and collect experiences during
learning, but have to learn from offline datasets. However, the transition probabil-
ities calculated from the dataset can be much different from the transition prob-
abilities induced by the learned policies of other agents, creating large errors in
value estimates. Moreover, the experience distributions of agents’ datasets may
vary wildly due to diverse behavior policies, causing large difference in value
estimates between agents. Consequently, agents will learn uncoordinated subop-
timal policies. In this paper, we propose MABCQ, which exploits value deviation
and transition normalization to modify the transition probabilities. Value devia-
tion optimistically increases the transition probabilities of high-value next states,
and transition normalization normalizes the biased transition probabilities of next
states. They together encourage agents to discover potential optimal and coordi-
nated policies. Mathematically, we prove the convergence of Q-learning under the
non-stationary transition probabilities after modification. Empirically, we show
that MABCQ greatly outperforms baselines and reduces the difference in value
estimates between agents.
1	Introduction
In multi-agent cooperative tasks, agents learn from the experiences generated by continuously in-
teracting with the environment to maximize the cumulative shared reward. Recently, multi-agent
reinforcement learning (MARL) has been applied to real-world cooperative systems (Bhalla et al.,
2020; Xu et al., 2021). However, in many industrial applications, continuously interacting with the
environment and collecting the experiences during learning is costly, risky, and time-consuming.
One way to address this is offline RL, where the agent can only access a fixed dataset of experiences
and learn the policy without further interacting with the environment. However, in multi-agent envi-
ronments, the dataset of each agent is often pre-collected individually by different behavior policies,
which are not necessary to be expert, and each dataset contains the individual action of the agent
instead of the joint action of all agents, e.g., autonomous driving dataset. Therefore, the dataset does
not satisfy the paradigm of centralized training, and the agent has to learn the coordinated policy in
an offline and fully decentralized way.
The main challenge of offline RL is the extrapolation error, an error in value estimate incurred by the
mismatch between the experience distributions of the learned policy and the dataset (Fujimoto et al.,
2019), e.g., the distance of the learned action distribution to the behavior action distribution, and the
bias of the transition dynamics estimated from the dataset to the true transition dynamics. Recently,
almost all offline RL methods (Fujimoto et al., 2019; Levine et al., 2020; Jaques et al., 2019) focus
on constraining the learned policy to be close to the behavior policy to avoid the overestimate of the
values of out-of-distribution actions, but ignore to correct the transition bias since the deviation of
estimated transition dynamics would not be too large if the single-agent environment is stationary.
However, in decentralized multi-agent environments, from the perspective of each agent, other
agents are a part of the environment, and the transition dynamics experienced by each agent de-
pend on the policies of other agents. Even in a stationary environment, the experienced transition
dynamics of each agent will change as other agents update their policies (Foerster et al., 2017).
Since the behavior policies of other agents would be inconsistent with their learned policies which
1
Under review as a conference paper at ICLR 2022
are unknowable in decentralized multi-agent environments, the transition dynamics estimated from
the dataset by each agent would be different from the transition dynamics induced by the learned
policies of other agents, causing large errors in value estimates. The extrapolation error would
lead to suboptimal policies. Moreover, trained on different distributions of experiences collected by
various behavior policies, the estimated values of the same state might be much different between
agents, which causes that the learned policies cannot coordinate with each other.
To overcome the suboptimum and miscoordination caused by transition bias in decentralized learn-
ing, we introduce value deviation and transition normalization to deliberately modify the estimated
transition probabilities from the dataset. During data collection, if one agent takes an optimal action
while other agents take suboptimal actions at a state, the transition probabilities of low-value next
states will become large. Thus, the Q-value of the optimal action will be underestimated, and the
agents will fall into suboptimum. Since the other agents are also trained, the learned policies of other
agents would become better than the behavior policies. For each agent, the transition probabilities
of high-value next states induced by the learned policies would be larger than those estimated from
the dataset. Therefore, we let each agent be optimistic toward other agents and multiply the tran-
sition probabilities by the deviation of the value of next state from the expected value over all next
states, to make the estimated transition probabilities close to the transition probabilities induced by
the learned policies of other agents.
Value deviation could decrease the extrapolation error and help the agents escape from the subopti-
mum. However, in some cases, the behavior policies of other agents might be highly deterministic,
which makes the distribution of experiences unbalanced. If the transition probabilities of high-value
next states are extremely low, value deviation may not remedy the underestimate. Moreover, due
to the diversity in experience distributions of agents, the value of the same state might be overes-
timated by some agents while underestimated by others, which results in miscoordination of the
learned policies. To address these two problems, we normalize the transition probability estimated
from the dataset to be uniform. Transition normalization balances the extremely biased distribu-
tion of experiences and builds the consensus about value estimate. By combining value deviation
and transition normalization, the agents would learn high-performing and coordinated policies in an
offline and fully decentralized way.
Although value deviation and transition normalization make transition dynamics non-stationary, we
mathematically prove the convergence of Q-learning under such non-stationary transition dynamics.
By importance sampling, value deviation and transition normalization take effect only as the weights
of the objective function, which makes our method easy to implement. We instantiate the proposed
method on BCQ, termed MABCQ, to additionally avoid out-of-distribution actions. Nevertheless,
it can also be implemented on other offline RL methods. We build the offline datasets and evaluate
MABCQ in four multi-agent mujoco scenarios (Todorov et al., 2012). Experimental results show
that MABCQ greatly outperforms BCQ, and ablation studies demonstrate the effectiveness of value
deviation and transition normalization. To the best of our knowledge, MABCQ is the first method
for offline and fully decentralized multi-agent reinforcement learning.
2	Related Work
MARL. Many MARL methods have been proposed for learning to solve cooperative tasks in an
online manner. Some methods (Lowe et al., 2017; Foerster et al., 2018; Iqbal & Sha, 2019) extend
policy gradient into multi-agent cases. Value factorization methods (Sunehag et al., 2018; Rashid
et al., 2018; Son et al., 2019) decompose the joint value function into individual value functions.
Communication methods (Das et al., 2019; Ding et al., 2020) share information between agents for
better cooperation. All these methods follow centralized learning and decentralized execution, where
the agents could access the information from other agents during centralized training. However,
in our offline and decentralized setting, the datasets of agents are different; each dataset contains
individual actions instead of joint actions; and the agents cannot be trained in a centralized way.
For decentralized learning, the key challenge is the obsolete experiences in replay buffer. Finger-
prints (Foerster et al., 2017) deals with obsolete experience problems by conditioning the value
function on a fingerprint that disambiguates the age of the sampled data. Lenient-DQN (Palmer
et al., 2018) extends the leniency concept and introduces optimism in the value function update by
forgiving suboptimal actions. Concurrent experience replay (Omidshafiei et al., 2017) induces cor-
2
Under review as a conference paper at ICLR 2022
relations in local policy updates, making agents tend to converge to the same equilibrium. However,
these methods require additional information, e.g., training iteration number, exploration rate, and
timestamp, which often are not provided by the offline dataset.
Offline RL. Offline RL requires the agent to learn from a fixed batch of data {(s, a, s0, r)}, con-
sisting of single-step transitions without exploration. Unlike imitation learning, offline RL does
not assume that the offline data is provided by a high-performing expert but has to handle the data
generated by suboptimal or multi-modal behavior policies. Most offline RL methods consider the
out-of-distribution action (Levine et al., 2020) as the fundamental challenge, which is the main
cause of the extrapolation error (Fujimoto et al., 2019) in value estimate in the single-agent environ-
ment. To minimize the extrapolation error, some recent methods introduce constraints to enforce the
learned policy to be close to the behavior policy, which could be direct action constraint (Fujimoto
et al., 2019), kernel MMD (Kumar et al., 2019), Wasserstein distance (Wu et al., 2019), and KL
divergence (Peng et al., 2019). Some methods train a Q-function pessimistic to out-of-distribution
actions to avoid overestimation by adding a reward penalty quantified by the learned environment
model (Yu et al., 2020), by minimizing the Q-values of out-of-distribution actions (Kumar et al.,
2020; Yu et al., 2021), or by weighting the update of Q-function via Monte Carlo dropout (Wu et al.,
2021). A finite-sample analysis for offline MARL (Zhang et al., 2018) has been studied, but the
agents are assumed to get individual rewards instead of the shared reward, and be connected by
communication networks, which is not the fully decentralized setting. All these methods do not
consider the extrapolation error introduced by the transition bias, which is a fatal problem in offline
and decentralized MARL.
3	Method
3.1	Preliminaries
We consider N agents in multi-agent MDP (Oliehoek & Amato, 2016) Menv =<
S , A, R, Penv , γ > with the state space S and the joint action space A. At each timestep, each
agent i gets state s and performs an individual action ai , and the environment transitions to the next
state s0 by taking the joint action ~a with the transition probability Penv (s0|s, ~a). The agents would
get a shared reward r = R (s), which is simplified to just depending on state (Schulman et al., 2015).
The agents learn to maximize the expected return E PtT=0 γtrt , where γ is a discount factor and T
is the time horizon of the episode. However, in the fully decentralized learning, Menv is partially
observable to the agent since the agent cannot observe the joint action ~a. During execution, from the
perspective of each agent i, there is a viewed MDP MEi =< S, Ai, R, PEi , γ > with the individual
action space Ai and the transition probability
N
PEi (s |s, ai ) = Penv (s0|s, ~a)
πj (aj ⑸，
~a-i	j 6=i
where ~a-i denotes the joint action of all agents except agent i, and π denotes the policy of the
agent. As the transition probability depends on the policies of other agents, if other agents are also
updating their policies, PEi becomes non-stationary. Moreover, if the agent cannot interact with the
environment, PEi is unknown. Since we only investigate the influence of other agents’ policies on
PEi , we assume Penv to be deterministic.
In offline and decentralized settings, each agent i could only access a fixed offline dataset Bi, which
is pre-collected by behavior policies and contains the tuples (s, ai, r, s0). As defined in BCQ (Fu-
jimoto et al., 2019), the visible MDP MBi =< S, Ai, R, PBi, γ > is constructed on Bi, which has
the transition probability1
PBi (s0|s, ai)
num(s, ai, s0)
Pso num(s,ai,^0)
where num(s, ai, s0) is the number of times the tuple (s, ai, s0) is observed in Bi. However, since the
learned policies of other agents might be greatly different from the behavior policies, PBi would be
biased from PEi, which creates large extrapolation errors and differences in value estimates between
agents, and eventually leads to uncoordinated suboptimal policies.
1The transition probability in the following sections means the one calculated from Bi unless otherwise
stated.
3
Under review as a conference paper at ICLR 2022
Table 1: The matrix game.
a1 (0.8)
Agent 1	1
a2 (0.2)
Agent 2
a1 (0.4) a2 (0.6)
1	5
6	1
Table 2: Transition probabilities and expected returns calculated in the dataset.
action transition expected return
1 tnegA
aι	p(1∣αι) = 0.4	3.4
	p(5∣αι) =。6	
a2	p(6∣a2) =。4	3.0
	p(l∣a2) = 0.6	
action transition expected return
2 tnegA
aι	p(1∣aι) = 0.8	2.0
	p(6∣αι) = 0.2	
a2	p(5|a2)= 0.8	4.2
	p(1∣a2)=O2-	
To intuitively illustrate the miscoordination caused by the transition bias, we devise offline datasets
in a matrix game for two agents, with the payoff depicted in Table 1. The action distributions of
the behavior polices of the two agents are [0.8, 0.2] and [0.4, 0.6], respectively. Table 2 shows the
transition probabilities and expected returns calculated by the independent agents in the datasets.
Since the datasets are collected by poor behavior policies, when one agent chooses the optimal
action, the other agent would choose the suboptimal action with a high probability, which leads to
low transition probabilities of high-value next states. Thus, the agents underestimate the optimal
actions and converge to the suboptimal policies (a1, a2), rather than the optimal policies (a2, a1).
3.2	Importance Weights
3.2.1	Value Deviation
If the behavior policies of some agents are low-performing during data collection, they usually
take suboptimal actions to cooperate with the optimal actions of other agents, which leads to high
transition probabilities of low-value next states. When agent i performs Q-learning with the dataset
Bi, the Bellman operator T is approximated by the transition probability PBi (s0|s, ai) to estimate
the expectation over s0 :
TQi(s,ai) = Es0~Pb.(s0∣s,ai) r + YmaxQi (s0,^i)
i
If PBi of a high-value s0 is lower than PEi, the Q-value of this (s, ai) pair is underestimated, which
would cause large extrapolation error and guide the agent to the convergence of suboptimal policy.
As discussed before, during execution, the transition probability viewed by agent i is PEi and the en-
vironment is deterministic, thus PEi (s0|s, ai) only depends on the learned policies of other agents,
which are unavailable. However, since the policies of other agents are also updating towards max-
imizing the Q-values, PEi of high-value next states would grow higher than PBi . Based on this
intuition, we let each agent be optimistic towards other agents and modify PBi as
P (S ,)*(1 + Vj*(s0)- E^0Vi*(^0) )1
Bi(Ni) ( +	|EsoVi*(^0)∣	) Zvd,
|
^^^^^{^^^^^™
value deviation
}
where the state value Vi*(s)
maxai Qi (s, ai ), 1 +
V*(s0)-E^0 Vi*(S0)
|Es0 V*(^0)l
is the deviation of the
value of next state from the expected value over all next states, which increases the transition
probabilities of the high-value next states and decreases those of the low-value next states, and
Zvd = PsO PBi (s0∣s, a. * (1 + V (E)-Es(V)I(S )) is a normalization term to make sure the sum
of the transition probabilities is one. Value deviation makes the transition probability to be close
to PEi and hence decreases the extrapolation error. The optimism towards other agents helps the
agents escape from local optima and discover potential optimal actions which are hidden by the poor
behavior policies.
4
Under review as a conference paper at ICLR 2022
3.2.2	Transition Normalization
In real-world applications, the action distribution of behavior policy might be unbalanced, which
makes the transition probabilities PBi biased, e.g., the transition probabilities of Agent 2 (i.e., action
distribution of Agent 1) in Table 1. If the transition probability of a high-value next state is extremely
low, value deviation cannot correct the underestimate. Moreover, since Bi of each agent is individu-
ally collected by different behavior policies, the diversity in transition probabilities of agents leads to
that the value of the same state s will be overestimated by some agents, while be underestimated by
others. Since the agents are trained to reach high-value states, the large divergences on state values
will cause miscoordination of the learned policies. To overcome these problems, we normalize the
biased transition probability PBi to be uniform over next states,
Pb. (s0∣s,ai) * ——-ɪ---------- * —,
Bi ( 1 ' i)	PBi(S0∣s,ai)	Ztn,
'------{------}
transition normalization
where zitn is a normalization term that is the number of different s0 given (s, ai ) in Bi . Transition
normalization enforces that each agent has the same PBi when it acts the learned action a* on the
same state s, and we have the following proposition.
Proposition 1. In episodic environments, if each agent i performs Q-learning on Bi, all agents will
converge to the same V * if they have the same transition probability on any state where each agent
i acts the learned action ai* .
Proof. The proof is provided in Appendix A.	□
However, to satisfy PB1 (s0|s, a1*) = PB2 (s0|s, a2*) = . . . = PBN (s0|s, a*N) for all s0 ∈ S, the
agents should have the same set of s0 at (s, a*), which is a strong assumption. In practice, although
the assumption is not strictly satisfied, transition normalization could still normalize the biased
transition distribution, encouraging the estimated state value V * to be close to each other.
3.2.3	Optimization Objective
We combine value deviation (1 +
V*(s0)-E^0 V*(^0)
∣E^0 V*(s0)l
which is denoted as λvdi , and transition nor-
malization PB(§1|§。.), which is denoted as λtni, and modify PBi as,
PBi(S0|s, ai) = pBi(SIs, ai) * "ni"di,
zi
where Zi = Pssf (1 + V (E)-Es0V)∣(S )) is the normalization term. In a sense, PBi makes the offline
learning on Bi similar to the online decentralized MARL. In the initial stage, λvdi is close to 1 since
Qi(S, ai) is not updated, and the transition probabilities are uniform, meaning other agents are act-
ing randomly. During training, the transition probabilities of high-value states gradually grow under
value deviation, which is an analogy of that other agents are improving their policies in the online
learning. Starting from the normalized transition probabilities and changing the transition proba-
bilities following the same optimism principle, the agents increase the values of potential optimal
actions optimistically and unanimously, and build consensuses about value estimate. Therefore tran-
sition normalization and value deviation encourage the agents to learn high-performing policies and
improve coordination. Moreover, although PBi is non-stationary (i.e., λvdi changes over updates of
Q-Value), We have the following theorem about the convergence of Bellman operator T under PBi,
TQi(S, ai) = ES0〜PB2(s0|s,ai) r + Y maax Qi (S0, ai)
Theorem 1. Under the non-stationary transition probability PBi, the Bellman operator T is a
contraction and converges to a unique fixed point when Y ＜「『rm-r ' , if the reward is bounded
by the positive region [rmin, rmax].
Proof. The proof is provided in Appendix A.
□
5
Under review as a conference paper at ICLR 2022
As positive affine transformation of the reward function does not change the optimal policy in the
environments with fixed horizon (Zhang et al., 2021), Theorem 1 holds in general. We could rescale
the reward to make rmin arbitrarily close to rmax so as to obtain a high upper bound of γ .
In deep reinforcement learning, directly modifying the transition probability is infeasible. However,
we could modify the sampling probability to achieve the same effect. The optimization objective
of decentralized deep Q-learning EpB (s,ai,s0) |Qi(s, ai) - yi|2 is calculated by sampling the batch
from Bi according to the sampling probability pBi (s, ai, s0). By factorizing pBi (s, ai, s0), we have
pBi (s, ai, s0)
、------{z------}
sampling probability
PBi(S,ai) *
PBi(s0|s,ai) .
、------{------}
transition probability
Therefore, We can modify the transition probability as λtnZλvdi PBi (s0∣s, a。and scale PBi (s, a。
with zi . Then, the sampling probability can be re-written as
λtniλvdiPBi(s, ai, s0) = ZiPBi(s, 0i) * "n'vd PBi(s0|s, aj.
\	{z	}	、	Zi______	/
modified sampling probability	,	^~ {z	'
modified transition probability
Since Zi is independent from s0, it could be regarded as a scale factor on PBi (s, ai). Scaling
PBi (s, ai) Will not change the expected target value yi, so sampling batches for update accord-
ing to the modified sampling probability could achieve the same effect of modifying the transition
probability. Using importance sampling, the modified optimization objective is
Eλtni λvdi pBi (s,ai,s0) |Qi (s, ai) - yi |
EpBi (s,ai,s0)
λtni λvdi pBi (S, ai, s )
PBi (S,ai,s0)
|Qi (S, ai) - yi |
EpBi(s,ai,s0)λtniλvdi |Qi(S, ai) - yi|2,
Where λtni and λvdi could be seen as the Weights of the objective function.
3.3 Implementation
We implement our method on BCQ (Fujimoto et al., 2019), termed MABCQ. To make it adapt to
high-dimensional continuous spaces, for each agent i, We train a Q-netWork Qi , a perturbation net-
work ξi, and a conditional VAE G： = {Ei (μ1, σ1∣s, a) , Di (a|s, z1 〜(μ1, σ1))}. In execution,
each agent i generates n actions by Gi1, adds small perturbations ∈ [-Φ, Φ] on the actions using ξi,
and then selects the action with the highest value in Qi . The policy can be written as
πi (S) = argmax Qi S, aij + ξ S, aij
aij+ξi(s,aij )
where
{aj~ Gl(s)}；ι∙
Qi is updated by minimizing
EpBi(S,ai,s0) λtni λvdi |Qi (s, ai) - yi | , where yi = r + YQi(S , πi (S )) ∙	(I)
ʌ ʌ
yi is calculated by the target networks Qi and ξ%, where ∏ is correspondingly the policy induced by
ʌ ʌ
Qi and ξi .
ξi is updated by maximizing
EpBi (s,ai,s0)λtni λvdi Qi (S, ai + ξi (S, ai)) .	(2)
To estimate λvd%, we need Vi*(S0) = Qi(S0,∏i(S0)) and Es，[Vi* (s0)] = Y(Qi(S,a%) 一 r), which
can be estimated from the sample without actually going through all S0 . We estimate λvdi using the
target networks to stabilize λvdi along with the updates of Qi and ξi . To avoid extreme values, we
clip λvdi to the region [1 一 , 1 + ], where is the optimism level.
To estimate λ., we train a VAE GlI = {Ei2 (μ2, σ2∣S, a, S) , DIi (a∣S, s0, z2 〜(μ2, σ2))}. Since
the latent variable of VAE follows the Gaussian distribution, we use the mean as the encoding of
the input and estimate the probability density functions: Pi(S) a) ≈ PN(o,i)(μ1) and ρi(S, a, s0) ≈
6
Under review as a conference paper at ICLR 2022
Algorithm 1 MABCQ
1:	for i ∈ N do
2:	Initialize the conditional VAEs:
G1 = {E1 (μ1,σ1ls,a) , D1 (als,z1)}, G2 = {E2 (μ2,σ2ls, a, s0) , D2 (als,s0, z2)}.
3:	Initialize Q-network Qi, perturbation network ξi, and their target networks Qi and ξi .
4:	Fit the VAEs Gi1 and Gi2 using Bi .
5:	for t = 1,..., max_update do
6:	Sample a mini-batch from Bi .
7:	Update Qi by minimizing (1).
8:	Update ξi by maximizing (2).
9:	Update the target networks Qi and ξi .
10:	end for
11:	end for
PN(0,i)(μ2), where PN(0,1)is the density of unit Gaussian distribution. The conditional density is
Pi(s0∣a,s) ≈ PN(0,1)(μι) and the transition probability is Pb. (s0∣s, ai) ≈ R: +12/S Pi(Sls,a)ds0 ≈
PN (0,i)(μi)	i	s' - 2 δS
Pi(s0|s, a) kδSk when the integral interval kδSk is a small constant. Approximately, we have
λ _ PN(o,i)(μ1)
ʌtni = PN(0,i)(μ2),
and the constant kδS k is considered in zi. In practice, we find that λtni falls into the region [0.2, 1.4]
for almost all samples. For completeness, we summarize the training procedure of MABCQ in
Algorithm 1.
4 Experiments
4.1	Matrix Game
We perform MABCQ on the matrix game in Table 1. As shown in Table 3, if we only use λvd
without considering transition normalization, as the transition probabilities of high-value next states
have been increased, for agent 1 the value of a2 becomes higher than that of a1. However, due
to the unbalanced action distribution of agent 1, the initial transition probabilities of agent 2 are
extremely biased. With λvd, agent 2 still underestimates the value of a1 and learns the action a2 .
The agents arrive at the joint action (a2, a2), which is a worse solution than the initial one (Table 2).
Normalizing the biased distribution by λtn, the agents could learn the optimal solution (a2, a1) and
build the consensus about the values of learned actions, as shown in Table 4.
Table 3: Transition probabilities and expected returns calculated in the dataset using only λvd .
action transition expected return	action transition expected return
1 tnegA
aι	p(1∣ɑι) = 0.12	4.52
	p(5∣ɑι) = 0.88	
a2	p(6∣a2) = 0.8	5
	p(1∣a2) = O2	
2 tnegA
aι	p(1∣αι) = 0.4	4
	p(6∣αι) =。6	
a2	p(5∣a2) = 0.95	4.8
	p(1∣a2) = 0.05	
action transition expected return
1 tnegA
aι	p(1∣ɑι) = 0.17	4.33
	p(5∣ɑι) = 0.83	
a2	p(6∣a2) = 0.86	5.29
	p(l∣a2) = 0.14	
Table 4: Transition probabilities and expected returns calculated in the dataset using λtn and λvd.
action
transition expected return
2 tnegA
aι	p(1∣aι) = 0.14	5.29
	p(6∣αι) = 0.86	
a2	p(5∣a2) = 0.83	4.33
	p(l∣a2) =0.17	
7
Under review as a conference paper at ICLR 2022
(a) HalfCheetah (b) Walker
(c) Hopper
(d) Ant
Figure 1: Illustrations of the scenarios. Different colors indicate different agents.
MABCQ
MABCQw/o λtn
——MABCQw/o λvd
BCQ
DDPG
---Behavior
3000-
2500-
2ZC0-
1500-
1000-
MABCQ
MABCQ w/o λtn
—— MABCQ w/o λvd
BCQ
DDPG Λ
----Behavior f"r∖J
0	1 × 104	2 × 104	3 × 104	4 × 104	5 × 104	0	1 × 104	2 × 104	3 × 104	4 × 104	5 × 104	0	1 × 104	2 × 104	3 × 104	4 × 104	5 × 104	0	1 × 104	2 × 104	3 × 104	4 × 104	5 × 104
Updates	Updates	Updates	Updates
(a) HalfCheetah	(b) Walker	(C) Hopper	(d) Ant
Figure 2: Learning curves of MABCQ and the baselines in HalfCheetah, Walker, Hopper, and Ant. The curves
are plotted based the mean and standard deviation of five runs with differenCe random seeds.
4.2	Environments and Datasets
To evaluate the effeCtiveness of MABCQ in high-dimensional Complex environments, we adopt
multi-agent mujoCo (de Witt et al., 2020), whiCh splits the original aCtion spaCe of the mujoCo tasks
(Todorov et al., 2012; BroCkman et al., 2016) into several sub-spaCes. We Consider four tasks, whiCh
are HalfCheetah, Walker, Hopper, and Ant. As illustrated in Figure 1, different Colors indiCate
different agents. EaCh agent independently Controls one or some joints of the robot and Could get
the state and reward of the robot, whiCh are defined in the original tasks.
For eaCh environment, we ColleCt N datasets for the N agents. EaCh dataset Contains 1 million
transitions (s, ai , r, s0 , done). For data ColleCtion, we train an intermediate poliCy and an expert
poliCy for eaCh agent using SAC algorithm (Haarnoja et al., 2018) provided by OpenAI Spinning
Up (AChiam, 2018). The offline dataset Bi is a mixture of four parts: 20% transitions are split from
the experienCes generated by the SAC agent at the early training, 35% transitions are generated from
that the agent i aCts the intermediate poliCy while other agents aCt the expert poliCies, 35% transitions
are generated from that agent i performs the expert poliCy while other agents aCt the intermediate
poliCies, 10% transitions are generated from that all agents perform the expert poliCies. For the last
three parts, we add a small noise to the poliCies to inCrease the diversity of the dataset.
We Compare MABCQ against the following methods:
•	MABCQ w/o λtn . Removing λtn from MABCQ.
•	MABCQ w/o λvd . Removing λvd from MABCQ.
•	BCQ. Removing both λtn and λvd from MABCQ.
•	DDPG (LilliCrap et al., 2016). EaCh agent i is trained using independent DDPG on the
offline Bi without aCtion Constraint and transition probability modifiCation.
•	Behavior. EaCh agent i takes the aCtion generated from the VAE Gi1 .
The baselines have the same neural network arChiteCtures and hyperparameters as MABCQ. All the
models are trained for five runs with different random seeds. All the learning Curves are plotted using
mean and standard deviation. More details about hyperparameters are available in Appendix C.
4.3	Performance and Ablation
Figure 2 shows the learning Curves of all the methods in the four tasks. Without aCtion Constraint
and transition probability modifiCation, DDPG severely suffers from the large extrapolation error and
Can hardly improve the performanCe throughout the training. BCQ outperforms the behavior poliCies
8
Under review as a conference paper at ICLR 2022
1 × 104	2 × 104	3 × 104	4 ×104	5 × 104
Updates
MABCQ
MABCQ w/o λtn
1 × 104	2 ×104	3 × 104	4 × 104	5 ×104
Updates
MABCQ
MABCQ w/o λtn
1 × 104	2 × 104	3 ×104	4 × 104	5 ×104
Updates
MABCQ
MABCQ w/o λtn
1 ×104	2 × 104	3 × 104	4 × 104	5 × 104
Updates
MABCQ
MABCQw/o
(a) HalfCheetah
(b) Walker
(c) Hopper
(d) Ant
Figure 3: Difference in value estimates among agents along with the training in HalfCheetah, Walker, Hopper,
and Ant. It is shown that transition normalization indeed reduces the difference in value estimates.
Table 5: Extrapolation errors of MABCQ and BCQ.
HalfCheetah
Walker
Hopper
Ant
MABCQ	98.4 ± 31.3	55.0 ± 9.6 28.1 ±3.4 180.2 ± 22.2
BCQ	97.2 ± 29.1 91.5 ± 35.4	65.8 ± 6.4	231.3 ± 47
but only arrives at the mediocre performance. In Figure 2(a) and Figure 2(c), the performance of
BCQ even descends in the later stage of learning. During the collection of Bi , when agent i takes
a “good” action, other agents usually take “bad” actions, making BCQ underestimate the “good”
actions, especially in the latter stage. The learning curves of MABCQ w/o λvd are similar to those
of BCQ in the first three tasks. That is because other agents’ policies are assumed to be random using
only transition normalization, which is far from the learned policies and leads to large extrapolation
errors. But in Ant, MABCQ w/o λvd outperforms BCQ in the later stage, which is attributed to the
value consensus built by the normalized transition probabilities. By optimistically increasing the
transition probabilities of high-value next states, MABCQ w/o λtn encourages the agents to learn
potential optimal actions and obviously boosts the performance. MABCQ combines the advantages
of both value deviation and transition normalization and outperforms other baselines.
To interpret the effectiveness of transition normalization, we uniformly sample a subset from the
union of all agents' states and calculate the difference in value estimates, maxi Vi* - mini 匕二
on this subset, where Vi* is calculated as Qi(s, ∏i(s)). The results are illustrated in Figure 3. The
maxi Vi* -mini Vi* of MABCQ is lower than that of MABCQ w/o λtn, which verifies that transition
normalization could decrease the difference in value estimates among agents. If there is a consensus
among agents about which states are high-value, the agents would select the actions that most likely
lead to the common high-value states. This promotes the coordination of policies and helps MABCQ
outperform MABCQ w/o λtn .
In Table 5, We present the extrapolation errors of MABCQ and BCQ, |焉 Pi Qi(s, ai) — R|, where
R is the true value evaluated by Monte Carlo return. Although MABCQ greatly outperforms BCQ
(i.e., higher return), it still achieves smaller extrapolation errors than BCQ in Walker, Hopper, and
Ant, which empirically verifies our claim that the proposed weights could decrease the extrapolation
error.
We also investigate the effect of optimism level , the computation efficiency of MABCQ, and the
performance of the proposed weights on other datasets and another offline RL algorithm, i.e., CQL
(Kumar et al., 2020). Due to the space limit, these results are given in Appendix B.
5 Conclusion
In this paper, we proposed MABCQ for offline and fully decentralized multi-agent reinforcement
learning. MABCQ modifies the transition probability by value deviation that increases the transition
probabilities of high-value next states, and by transition normalization that normalizes the biased
transition probabilities. Mathematically, we show that under the non-stationary transition probability
after modification, offline decentralized Q-learning converges to a unique fixed point. Empirically,
we show that MABCQ could help the agents escape from the suboptimum, learn coordinated poli-
cies, and greatly outperform the baselines in a variety of multi-agent offline datasets.
9
Under review as a conference paper at ICLR 2022
References
Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.
Sushrut Bhalla, Sriram Ganapathi Subramanian, and Mark Crowley. Deep multi agent reinforcement
learning for autonomous driving. In Canadian Conference on Artificial Intelligence (Canadian
AI), 2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Abhishek Das, TheoPhile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike RabbaL and
Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on
Machine Learning (ICML), 2019.
Christian Schroeder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Bohmer,
and Shimon Whiteson. DeeP multi-agent reinforcement learning for decentralized continuous
cooperative control. arXiv preprint arXiv:2003.06709, 2020.
Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for
multi-agent cooperation. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. In International Conference on Machine Learning (ICML), 2017.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-
son. Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence
(AAAI), 2018.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning (ICML), 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning (ICML), 2018.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), 2019.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-
learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations (ICLR), 2016.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems (NeurIPS), 2017.
10
Under review as a conference paper at ICLR 2022
Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs.
Springer, 2016.
Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep
decentralized multi-task multi-agent reinforcement learning under partial observability. In Inter-
national Conference on Machine Learning (ICML), 2017.
Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep rein-
forcement learning. In International Conference on Autonomous Agents and MultiAgent Systems
(AAMAS), 2018.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning (ICML), 2018.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), 2015.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), 2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In International Conference
on Autonomous Agents and Multiagent Systems (AAMAS), 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and
Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv preprint
arXiv:2105.08140, 2021.
Bingyu Xu, Yaowei Wang, Zhaozhi Wang, Huizhu Jia, and Zongqing Lu. Hierarchically and co-
operatively learning traffic signal control. In AAAI Conference on Artificial Intelligence (AAAI),
2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information
Processing Systems (NeurIPS), 2020.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative offline model-based policy optimization. arXiv preprint arXiv:2102.08363,
2021.
Chi Zhang, Sanmukh Rao Kuppannagari, and Viktor Prasanna. Brac+: Going deeper with behavior
regularized offline reinforcement learning, 2021.
Kaiqing Zhang, ZhUoran Yang, Han Liu, Tong Zhang, and Tamer Bayar. Finite-sample analysis for
decentralized batch multi-agent reinforcement learning with networked agents. arXiv preprint
arXiv:1812.02783, 2018.
11
Under review as a conference paper at ICLR 2022
A Proofs
Proposition 1. In episodic environments, if each agent i performs Q-learning on Bi, all agents will
converge to the same V * if they have the same transition probability on any state where each agent
i acts the learned action a*.
Proof. Considering the two-agent case, we define δ(s) as the difference in the V*.
δ(s) = V1*(s) -V2*(s)
=X Pbi	(s0∣s,	a1)(r	+ γV1* (s0)) - X Pb,	(s0∣s, a2)(r + γV* (s0))
=X Pbi	(s0∣s,	a*)(r	+ γV* (s0) + γV1*	(s0)	- γV* (s0)) - XPb, (s0∣s, a*)(r	+ γV* (s0))
s0	s0
= X (PB1 (s0|s, a1*) - PB, (s0|s, a2*)) (r + γV2* (s0)) + γPB1 (s0|s, a1*)δ (s0)
s0
For the terminal state send, we have δ(send) = 0. If PB1 (s0|s, a*1) = PB, (s0|s, a2*) , ∀s0 ∈ S,
recursively expanding the δ term, we arrive at δ(s) = 0 + γ0 + γ20 + ... + 0 = 0. We can easily
show that it also holds in the N-agent case.	□
Theorem 1. Under the non-stationary transition probability PBi, the Bellman operator T is a
contraction and converges to a unique fixed point when Y < 2丫 rm-nr . , if the reward is bounded
by the positive region [rmin, rmax].
Proof. We initialize the Q-value to be ηrmin, where η denotes ;―+1. Since the reward is
bounded by the positive region [rmin, rmax], the Q-value under the operator T is bounded to
[ηrmin, ηrmax]. Based on the definition of PBi (s0∣s,ai), it can be written as PViV：(10), where
Vi* (s0) = maxai Qi (s0, (¾). Then, We have the following,
T Qi1 - T Qi2∞
-E PBi (Sls,ai)
s0∈S
r + γ max Q2 (s0, ai)
^i
max	P1, (s0∣s,ai) r + YmaxQ1 (s0, ai)
s,ai V< i	ai
s0∈S
max Y s,ai	Ps0∈S (V*1 (s0))2 - Ps0∈S (V*2 (s0))2 Ps0∈S V*1 (s0)	Ps0∈S V*2 (s0)
max Y
s,ai
Ps0∈S (V*1 (s0))2-(v*2 (s0))2 - X (V*2 (s0))21	______________1_____!∣
Ps0∈S V*1 (s0)	s⅛ i	∖Ps0∈s V*2 (s0)	Ps0∈S V*1 (s0)力
max Y
s,ai
1	21	2	1	2
Ps0∈S (V* (s0)- V* (s0))(V* (s0) + V* (s0)) - X (V*2 (O))2 Ps0∈S V* (s0)- V* (s0)
Ps0∈S V*1 (s0)	S⅛	Ps0 V*1 (s0) Ps0∈S V*2 (s0)
≤ max Y
s,ai
X(Vi*1(s0)-Vi*2(s0))
s0∈S
1
* ------——i—~~-
Ps0∈S V*1 (s0)
* max
Ps0∈S (V*2 (s0))2
Ps0∈S V*2 (s0)
≤ Y|S| IIQ1 - Q2 IL * RT- * η(2rmax - rmin)
'l	'l∞	∣S∣ηrmin
= Y(A - 1)∣∣Q1 -Q2II∞.
rmin
12
The third term of the penultimate line is because: if Vi* (s0) + Vi* (s0) >
Ps0∈s(V*2(s0))2
Ps0∈S V*2 (s0),
“*1,	0、**2,	0、Ps0∈S (匕 *2 (s0 ))21,	0、w*2, 0、Ps0∈S (匕 *2 (s0))	* ηrmin 八
V	(S	)+Vi	(S ) --^S--∙v*2，八	≤ M(S )+Vi	(S )-----;S---∙v*2，八-----≤	2ηrmaχ-ηrmin,
s0∈S Vi (s )	s0∈S Vi (s )
12
Under review as a conference paper at ICLR 2022
else,
Pg M2(s0 ))2
Ps0∈s 厅(s0)
12
-(%* (s0)+ Vi* (s0)) ≤
Ps0∈SM2(s0)) * ηrmax
Ps0∈s Vi*2 (s0)
≤ ηrmax .
Since 2ηrmax - ηrmin ≥ ηrmax, we have
12
|(Vi* (s0) + Vi* (s0))-
Ps0∈s (Vi*2(s0))2
Ps0∈s Vi*2 (s0)
| ≤ 2ηrmax
- ηrmin.
Therefore, if γ < ?r rm-nr . , the operator T is a contraction. By contraction mapping theorem, T
converges to a unique fixed point.	□
B Additional Results
To clearly present the performance gain of MABCQ over BCQ, we summarize the mean value and
standard deviation after 5 × 104 updates in Table 6, where we can see that MABCQ performs more
than one standard deviation better than BCQ.
Table 6: Comparison between MABCQ and BCQ.
	HalfCheetah	Walker	Hopper	Ant
MABCQ	1905 ± 126	2498 ± 256	1383 ± 443	3027 ± 178
BCQ	1384 ± 360	1324 ± 662	566 ± 89	2573 ± 637
To evaluate MABCQ in low-quality dataset, we carried out an additional experiment on 2-agent
Swimmer with another data collection approach, where each dataset only contains the experiences
at the early training and the experiences generated by intermediate policies. As shown in Figure 4,
after removing the expert experiences, MABCQ greatly outperforms BCQ, and BCQ even cannot
outperform the behavior policies.
∙∙∙∙
p-6M3M
0	2 ×, 102 4 ×, 102 6 ×, 102 8 ×, 102 10 × 102
Updates
Figure 4: Learning curves ofMABCQ and the baselines in Swimmer.
The optimism level controls the strength of value deviation. If is too small, value deviation has
weak effects on the objective function. But if is too large, the agent will be over optimistic about
other agents’ learned policies, which could result in large extrapolation errors and uncoordinated
policies. Figure 5 shows the learning curves of MABCQ with different . It is commonly observed
that increasing elevates the performance, especially in HalfCheetah. However, in Walker and Ant,
0	1 × 104 2 × 104 3 × 104 4 × 104 5 ×104
Updates
(a) HalfCheetah
0	1 × 104 2 × 104 3 × 104 4 × 104 5 ×104
Updates
(b) Walker
0	1 × 104 2 × 104 3 × 104 4 × 104 5 × 104
Updates
(c) Hopper
P-MeM
3500-
3000-
2500-
2000-
1500-
1000-
0	1 × 104 2 × 104 3 × 104
Updates
(d) Ant
4× 104	5 × 104
Figure 5:	Learning curves of MABCQ with different in HalfCheetah, Walker, Hopper, and Ant. It is shown
that with any positive , MABCQ does not underperform that without value deviation.
13
Under review as a conference paper at ICLR 2022
Table 7: Average time taken by one update.
	HalfCheetah	Walker	Hopper	Ant
MABCQ	18 ms	18 ms	16 ms	20 ms
BCQ	10 ms	10 ms	9 ms	11 ms
if we set a large (0.8), the performance slightly drops due to overoptimism. Even so, with any
positive , MABCQ does not underperform that without value deviation.
To demonstrate the computation efficiency of our method, we record the average time taken by one
update in Table 7. The experiments are carried out on Intel i7-8700 CPU and NVIDIA GTX 1080Ti
GPU. Since λvd and λtn could be calculated from the sampled experience without actually going
through all next states, MABCQ additionally needs only two forward passes for computing λvd and
λtn in the update and costs less than twice the training time of BCQ. Moreover, since MABCQ is
fully decentralized, the learning of agents can be entirely parallelized. Therefore, MABCQ scales
well with the number of agents.
p∙le∙M3≈
0	1 × 104	2 × 104	3 × 104	4 × 104	5 × 104
Updates
(a) HalfCheetah
0	2 × 104 4 × 104 6 × 104 8 × 104 10 × 104
Updates
(b) Hopper
P.Iew⅛m
0	3 × 104	6 × 104	9 × 104	12 × 104
Updates
(c) Walker

Figure 6:	Learning curves of MACQL in D4RL medium-replay datasets.
The two proposed weights λvd and λtn could also be extended to other offline RL methods. We in-
troduce the two weights to CQL (Kumar et al., 2020), as MACQL, and test them in D4RL medium-
replay datasets (Fu et al., 2020). CQL augments the standard Bellman error objective with con-
servative regularizer, thus the learned Q-value may not represent the original meaning and thus not
theoretically match value deviation. However, MACQL still outperforms CQL, as shown in Figure 6.
C	Experimental Settings and Hyperparameters
The experimental settings and hyperparameters are summarized in Table 8.
Table 8: Experimental settings and hyperparameters
Hyperparameter	HalfCheetah	Walker	Hopper	Ant
agent number (N)	2	2	3	4
state space	17	17	11	27
action space	3	3	1	2
horizon (T)		1000		
discount (γ)		0.99		
Bi size		106		
batch size		1024		
MLP units		(64, 64)		
MLP activation		ReLU		
learning rate of Q		10-3		
learning rate of ξ		10-4		
learning rate of G		10-4		
	0.80	0.48	0.80	0.64
Φ		0.05		
n		10		
VAE hidden space
10
14
Under review as a conference paper at ICLR 2022
D Limitation and Future Work
Although we consider the setting where each agent can get the state of the environment, MABCQ
could also be potentially applied to partially observable environments. However, if the partial ob-
servability is too limited, Q-value may not be accurately estimated from the observation. Many
MARL methods adopt recurrent neural networks to utilize the historical information, which how-
ever is impractical if the timestamp is not included in the offline dataset. Moreover, if a state is
estimated as a high-value state by an agent but not included in the datasets of other agents, the other
agents cannot learn corresponding optimal actions to cooperate with that agent at that state, and
thus miscoordination may occur. In this case, each agent should conservatively estimate the values
of states that are absent from the datasets of other agents to avoid miscoordination. We leave the
observation limitation and the state absence to future work.
15