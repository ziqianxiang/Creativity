Under review as a conference paper at ICLR 2022
Monotonicity as a requirement and as a regu-
larizer: efficient methods and applications
Anonymous authors
Paper under double-blind review
Ab stract
We study the setting where risk minimization is performed over general classes of
models and consider two cases where monotonicity is treated as either a require-
ment to be satisfied everywhere or a useful property. We specifically consider
cases where point-wise gradient penalties are used alongside the empirical risk
during training. In our first contribution, we show that different choices of penal-
ties define the regions of the input space where the property is observed. As such,
previous methods result in models that are monotonic only in a small volume
of the input space. We thus propose an approach that uses mixtures of training
instances and random points to populate the space and enforce the penalty in a
much larger region. As a second contribution, we introduce the notion of mono-
tonicity as a regularization bias for convolutional models. In this case, we consider
applications, such as image classification and generative modeling, where mono-
tonicity is not a hard constraint but can help improve some aspects of the model.
Namely, we show that using group monotonicity can be beneficial in several ap-
plications such as: (1) defining strategies to detect anomalous data, (2) allowing
for controllable data generation, and (3) generating explanations for predictions.
Our proposed approaches do not introduce relevant computational overhead while
leading to efficient procedures that provide extra benefits over baseline models.
1	Introduction
Highly expressive model classes such as artificial neural networks have achieved impressive pre-
diction performance across a broad range of supervised learning tasks and domains (Krizhevsky
et al., 2012; Graves & Jaitly, 2014; Bahdanau et al., 2014). However, finding predictors attaining
low risk on unseen data is often not enough to enable the use of such models in practice. In fact,
practical applications usually have more requirements other than prediction accuracy. Hence, devis-
ing approaches that search risk minimizers satisfying practical needs led to several research threads
seeking to enable the use of neural networks in real-life scenarios. Examples of such requirements
include: (1) Robustness, where low risk is expected even if the model is evaluated under distribu-
tion shifts, (2) Fairness, where the performance of the model is expected to not significantly change
across different sub-populations of the data, and (3) Explainability/Interpretability, where models
are expected to indicate how the features of the data affect their predictions.
In addition to the requirements mentioned above, a property commonly expected in trained models
in certain applications is monotonicity with respect to some subset of the input dimensions, i.e., an
increase (or decrease) along some particular dimensions strictly imply the function value will not
decrease (or will not increase), provided that all other dimensions are kept fixed. As a result, the
behavior of monotonic models will be more aligned with the properties that the data under consid-
eration is believed to satisfy. For example, in the models used to accept/reject job applications, we
expect acceptance scores to be monotonically non-decreasing with respect to features such as past
years of experience of a candidate. Thus, given two applicants with exactly the same features except
their years of experience, the more experienced candidate should be assigned an equal or higher
chance of getting accepted. For applications where monotonicity is expected, having a predictor
failing to satisfy this requirement would damage the user’s confidence. As such, different strategies
have been devised in order to enable training monotonic predictors. These approaches can be di-
vided into two main categories. The first one is monotonicity by construction, where the focus lies
on defining a model class that guarantees monotonicity in all of its elements Bakst et al. (2021);
1
Under review as a conference paper at ICLR 2022
Wehenkel & LoUPPe (2019); Nguyen & Martlnez (2019); You et al. (2017); Garcia & GUPta (2009);
Archer & Wang (1993). However, this approach can not be used with general architectures in the
case of neural networks. Additionally, the model class might be so constrained that it might affect
the Prediction Performance. Alternatively, a second aPProach is based on encouraging monotonicity
during training, i.e., searching for monotonic candidates within a general class of models (Liu et al.,
2020; Sivaraman et al., 2020; GuPta et al., 2019). Such a grouP of methods is more generally aP-
Plicable and can be used, for instance, with any neural network architecture. However, they are not
guaranteed to yield monotonic Predictors unless extra verification/certification stePs are Performed,
which can be comPutationally very exPensive. In addition to being a requirement as in the exam-
Ples discussed above, monotonicity has also been observed to be a useful feature in certain cases.
For examPle, it can define an effective inductive bias and imProve generalization in cases where
Prior knowledge indicates the data generation Process satisfies such a ProPerty (Dugas et al., 2001).
In such cases, however, it is not necessary to satisfy the ProPerty everywhere, since it is enforced
simPly as a desirable feature of trained models rather than a design sPecification.
In this work, we tackle the Problem of Performing emPirical risk minimization over rich classes of
models such as neural networks, while simultaneously searching for monotonic Predictors within
the set of risk minimizing solutions. In summary, our contributions are two-fold:
1.	monotonicity as a requirement: We show that Previous methods can only satisfy monotonicity
either near the training data or near the boundaries of the inPut domain. Then, we ProPose an efficient
algorithm that tackles this Problem. In short, we aPPly MixuP (Zhang et al., 2018) between the data
and random Points to PoPulate the inPut sPace, which is shown to enforce monotonicity in a larger
volume relative to Previous methods in literature. To the best of our knowledge, this is the first work
that studies the effect of the samPle Points used in calculating the monotonicity constraints.
2.	monotonicity as a regularizer: We define a new notion of monotonicity which is shown to be
useful when enforced for aPPlications such as object recognition or generative modeling of images.
In these cases, it is not necessarily required to enforce the ProPerty everywhere and, as such, the
constraints that focus only on the actual data Points are discussed (i.e., mixuP is not needed). Models
satisfying the ProPerty are comPared with standard Predictors across three aPPlications, where it is
shown the ProPerty is beneficial without comPromising the original Performance.
2	Background and related work
We start by defining the notion of partial monotonicity used throughout the PaPer. Consider the
standard supervised learning setting where data instances are observed in pairs x,y 〜 XXY, where
X ⊂ Rd and Y ⊂ R corresPond to the inPut and outPut sPaces, resPectively. Further, consider the
function f : X 7→ Y, and let M indicate some subset of the inPut dimensions, i.e., M ⊂ {1, ...d},
such that X = Concat(XM, XM), where M = {1,...,d}∖ M. We further overload the notation of
function calls to f such that f (x) = f (XM, XM).
Definition 1 Partially monotonic functions relative to M: We say f is monotonically non-
decreasing relative to M1, denoted fm, if f (XM, XM) ≤ f (XM, XM), ∀ XM ≤ XM, ∀ XM, where
the comParison XM ≤ X0M is Performed for every dimension.
This definition covers functions that do not decrease in value given increasing changes along a subset
of the inPut dimensions, Provided that all other dimensions are kePt unchanged. Several aPProaches
were introduced for defining model classes that have such a ProPerty. The simPlest aPProach re-
stricts the weights of the network to be non-negative (Archer & Wang, 1993). However, doing
so affects the Prediction Performance. Another aPProach corresPonds to using lattice regression
models ProPosed by Garcia & GuPta (2009); You et al. (2017). In this case, models are given by
interPolations in a grid defined by training data. Such a class of models can be made monotonic via
the choice of the interPolation strategy and recently introduced variations (Bakst et al., 2021) scale
efficiently with the dimension of the inPut sPace, but downstream aPPlications might still require
different classes of models to satisfy this tyPe of ProPerty. For neural networks, aPProaches such
as (Nguyen & Martinez, 2019) reparameterize fully connected layers such that the gradients with
resPect to Parameters can only be non-negative. Wehenkel & LouPPe (2019), on the other hand,
1Monotonically non-increasing f can be defined analogously.
2
Under review as a conference paper at ICLR 2022
consider the class of predictors H : X 7→ Y of the form H(x) = 0x h(t)dt + H(0), where h(t)
is a strictly positive mapping parameterized by a neural network. While such approaches guarantee
monotonicity by design, they can be too restrictive or give overly complicated learning procedures.
For example, the approach in (Wehenkel & Louppe, 2019) requires backpropagating through the
integral. An alternative approach is based on searching over general classes of models while assign-
ing higher importance to predictors observed to be monotonic. Similar to the case of adversarial
training (Goodfellow et al., 2014), Sivaraman et al. (2020) proposed an approach to find counterex-
amples, i.e., pairs of points where the monotonicity constraint is violated, which are included in the
training data to enforce monotonicity conditions in the next iterations of the model. However, this
approach only supports fully-connected ReLU networks. Moreover, the procedure for finding the
counterexamples is costly. Alternatively, Liu et al. (2020); Gupta et al. (2019) introduced point-wise
regularization penalties for enforcing monotonicity, where the penalties are estimated via sampling.
While Liu et al. (2020) use uniform random draws, Gupta et al. (2019) apply the regularization
penalty over the training instances. Both approaches have shortcomings that we seek to address.
3	Monotonicity as a requirement
Given the standard supervised learning setting where ` : Y2 7→ R+ is a loss function indicating the
goodness of the predictions relative to ground truth targets, the goal is to find a predictor h ∈ H such
that its expected loss - or the so-called risk - over the input space is minimized. Such an approach
yields the empirical risk minimization framework once a finite sample is used to estimate the risk.
However, given the extra monotonicity requirement, we consider an augmented framework where
such a property is further enforced. We seek the optimal monotonic predictors relative to M, hM:
hM ∈ argmi□h∈H Eχ,y〜x×γ['(h(x),y)] + γΩ(h,M),	(1)
where Y is a hyperparameter weighing the importance of the penalty Ω(h, M) which, in turn, is a
measure of how monotonic the predictor h is relative to the dimensions indicated by M. Ω(h, M)
can be defined by the following gradient penalty (Gupta et al., 2019; Liu et al., 2020):
Ω(h, M) = Ex〜D
(2)
where ahxx) indicates the gradients of h relative to the input dimensions i ∈ M, which are con-
strained to be non-negative, rendering h monotonically non-decreasing relative to M. At this point,
the only missing ingredient to define algorithms to estimate hM is how to define the distribution D
over which the expectation in Eq. 2 is computed, discussed in the following sections.
3.1	Choo sing distributions over which to compute the monotonicity penalty
In the following, we present and discuss two past choices for D:
1)	Define D as the empirical distribution of the training sample: In (Gupta et al., 2019), given a
training dataset of size N , in addition to using the observed data to estimate the risk, the same data
is used to compute the monotonicity penalty so that:
k=1 i∈M
0 ∂h(xk)∖2
.,dxk )
where xk indicates the k-th instance within the training sample. While this choice seems natural and
can be easily implemented, it only enforces monotonicity in the region where the training samples
lie, which can be problematic. For example, in case of covariate-shift, the test data might lie in parts
of the space different from that of the training data so monotonicity cannot be guaranteed. We thus
argue that one needs to enforce the monotonicity property in a region larger than what is defined by
the training data. In Appendix B, we conduct an evaluation under domain shift and show the issue
to become more and more relevant with the increase in the dimension d of the input space X .
3
Under review as a conference paper at ICLR 2022
2)	Define D = Uniform(X): In (LiU et al., 2020), a simple strategy is defined so that Ω is computed
over the random points drawn uniformly across the entire input space X; i.e.:
◎random(h, M) = Ex〜UnifOrm(X)
X max(0,- TI
i∈M	i
2
Despite its simplicity and ease of use, this approach has some flaws. In high-dimensional spaces,
random draws from any distribution of bounded variance will likely lie in the boundaries of the
space, hence far from the regions where data actually lie. Moreover, it is commonly observed that
naturally occurring high-dimensional data is structured in lower-dimensional manifolds (c.f. (Feffer-
man et al., 2016) for an in-depth discussion on the manifold hypothesis). It is thus likely that random
draws from the uniform distribution will lie nowhere near regions of space where training/testing
data will be observed. We further illustrate the issue with examples in Appendix A, which can be
summarized as follows: consider the cases of uniform distributions over the unit n-sphere. In such
a case, the probability of a random draw lying closer to the sphere’s surface than to its center is
P(∣∣χ∣∣2 > 1) = 2-2-1-, as given by the volume ratio of the two regions of interest. Note that
P(∣∣x∣∣2 > 1) → 1 as n → ∞, which suggests the approach in (Liu et al., 2020) will only enforce
monotonicity at the boundaries of the space.
In summary, the previous approaches are either too focused on enforcing monotonicity where the
training data lie, or too loose such that the monotonicity property is uniformly enforced across a
large space, and the actual data manifold may be neglected. We thus propose an alternative approach
where we can have some control over the volume of the input space where the monotonicity property
will be enforced. Our approach uses the idea of data mixup (Zhang et al., 2018; Verma et al., 2019;
Chuang & Mroueh, 2021), where auxiliary data is created via interpolations of pairs of data points,
to populate areas of the space that are otherwise disregarded. Mixup was introduced by Zhang
et al. (2018) with the goal of training classifiers with smooth outputs across trajectories in the input
space from instances of different classes. Given a pair of data points (x0, y0), (x00, y00), the method
augments the training data using interpolations given by (λx0 + (1 - λ)x00, λy0 + (1 - λ)y00), where
λ 〜Uniform([0,1]). We propose using mixup to generate points where the monotonicity penalty
Ω can be computed and highlight the following motivations for doing so: (1) Interpolation of data
points more densely populates the convex hull of the training data. (2) Extrapolation cases where
mixup is performed between data points and instances obtained at random results in points that lie
anywhere between the data manifold and the boundaries of the space. We thus claim that performing
mixup enables the computation of Ω on parts of the space that are disregarded if one focus only on
either observed data or random draws from uninformed choices of distributions such as the uniform.
3.2	Evaluation
In order to evaluate the effect of different choices of Ω, we report results on three commonly used
datasets covering classification and regression settings with input spaces of different dimensions.
Namely, we report results for the following datasets: Compas, Loan Lending Club, and Blog Feed-
back. Models are implemented using the same architecture as in (Liu et al., 2020). Further details on
the data, models, and training settings can be found in Appendix C. For all evaluation cases, we con-
sider the baseline where training is carried out without any monotonicity enforcing penalty. For the
regularized cases, the different approaches used for computing Ω are as follows: (1) Ωrandom (LiU
et al., 2020) which uses random points drawn from Uniform(X). In this case, the sample observed
at each training iteration is set to a size of 1024 throughout all experiments. (2) Ωtrain (Gupta et al.,
2019) which uses the actual data observed at each training iteration; i.e., the observed mini-batch
itself is used to compute Ω. And (3) Ωm%xup (ours), in which case the penalty is computed on points
generated by mixing-up points from the training data and random points. In details, for each mini-
batch of size N > 1, we augment it with complementary random data and obtain a final mini-batch
of size 2N. Out of the 2NQNT) possible pairs of points, we take a random subsample of 1024 pairs
to compute mixtures of instances. In this case, we use λ 〜Uniform([0,1]) and λ is independently
drawn for each pair of points.
Results are reported in terms of both prediction performance and level of monotonicity. The latter
is assessed via the fraction ρ of points within a sample where the monotonicity constraint is vio-
PN=II [mini∈Md∂(X) <0]
lated; i.e., given a set of N data points, we compute: P =-N-----i----, such that P = 0
4
Under review as a conference paper at ICLR 2022
	Non-mon.	Qrandom	Qtrain	Ωmixup
COMPAS				
Validation accuracy	69.1%±0.2%	68.5%±0.1%	68.5%±0.1%	68.4%±0.1%
Test accuracy	68.5%±0.2%	68.1%±0.2%	68.0%±0.2%	68.3%±0.2%
Prandom	55.45%±12.26%	0.01%±0.01%	6.41%±4.54%	0.00%±0.00%
Ptrain	92.98%±2.70%	2.08%±2.21%	0.00%±0.00%	0.00%±0.00%
Ptest	92.84%±2.75%	2.16%±2.35%	0.00%±0.00%	0.00%±0.00%
Loan Lending Club				
Validation RMSE	0.213±0.000	0.223 ±0.002	0.222±0.002	0.235±0.001
Test RMSE	0.221±0.001	0.230±0.001	0.229±0.002	0.228±0.001
Prandom	99.11%±1.70%	0.00%±0.00%	14.47%±7.55%	0.00%±0.00%
Ptrain	100.00%±0.00%	7.23%±7.76%	0.01%±0.01%	0.00%±0.00%
Ptest	100.00%±0.00%	6.94%±7.43%	0.04%±0.03%	0.00%±0.00%
Blogfeedback				
Validation RMSE	0.174±0.000	0.175±0.001	0.177±0.000	0.168±0.000
Test RMSE	0.139±0.001	0.139±0.001	0.142±0.001	0.143±0.001
Prandom	76.17%±12.37%	0.05%±0.08%	3.86%±4.19%	0.00%±0.01%
Ptrain	78.67%±5.28%	78.59%±6.37%	0.01%±0.01%	0.01%±0.01%
Ptest	76.29%±6.47%	78.99%±7.20%	0.02%±0.02%	0.02%±0.02%
Table 1: Evaluation results in terms of 95% confidence intervals resulting from 20 independent
training runs. Results correspond to the checkpoint that obtained the best prediction performance on
validation data throughout training. The lower the values of ρ the better.
corresponds to monotonic models over the considered points. Moreover, in order to quantify the
degree of monotonicity in different parts of the space, we compute ρ for 3 different sets of points:
(1) ρrandom, computed on a sample drawn according to Uniform(X). We used a sample of 10,000
points throughout the experiments. (2) ρtrain , computed on the training data. And (3) ρtest : com-
puted on the test data. Results are summarized in Table 1 in terms of both prediction performance
along with the metric indicating the degree of monotonicity of the predictor for each regulariza-
tion strategy. Prediction performance is measured in terms of accuracy for classification tasks, and
RMSE for the case of regression. Monotonicity, on the other hand, is quantified via the fraction ρ of
points within a sample where the monotonicity constraint is violated. Results reported in the tables
represent 95% confidence intervals corresponding to multiple independent training runs. Across all
datasets, the different penalties do not result in significant variations in the performance of the final
predictors. This indicates that the class of predictors corresponding to the subset of H that is mono-
tonic relative to M, denoted HM, has enough capacity so as to be able to match the performance
of the best canditates within H. In terms of monotonicity, we observe a clear pattern leading to the
following intuition: monotonicity is achieved in the regions where it is enforced. This is evidenced
by the observation that Prandom is consistently lower for ΩΩrandom relative to Ωtrain and Ω'miχup
while, on the other hand, Ptrain and Ptest are consistently lower for Ωtrain and Ωmiχup compared to
Ωrandom. A comparison between Ωtrain and Ωmiχup shows what We anticipated: enforcing mono-
tonicity in points resulting from mixup yields predictors that are as monotonic as those given by
the use of Ωtrain in actual data, but significantly better at the boundaries of X. Finally, the results
demonstrate that our proposed approach Ω'miχup achieves the best results in terms of monotonic-
ity for all the sets of points that we considered. Moreover, our approach introduces no significant
computation overhead. Algorithm 1 in Appendix C details on how to compute Ωmiχup.
4	Monotonicity as a regularizer
In Section 3, we presented an efficient approach to enforce monotonicity when it is a requirement.
We now consider a different perspective and show that adding monotonicity constraints during train-
ing can yield extra benefits to trained models. In these cases monotonicity is not a requirement, and
hence it is not necessary for it to be satisfied everywhere. As such, the penalties we discuss from
now on are computed considering only data points, and no random draws are utilized. In the follow-
ing sections, we introduce notions of monotonicity that will be enforced in our models, and discuss
advantages of using monotonicity for different applications such as for the detection of anomalous
data and controllable generative modelling. In Appendix F, we consider a further application for
cases where one’s interest is to obtain explanations from observed predictions.
5
Under review as a conference paper at ICLR 2022
4.1	Group Monotonicity
First, we consider the case of K-way classifiers realized through convolutional neural networks. In
this case, data examples correspond to pairs x,y 〜 XXY, but Y = {1,2,3,...,K}, K ∈ N.
Models parameterize a data-conditional categorical distribution over Y, i.e., for a given model h,
h(x)Y will yield likelihoods for each class indexed in Y. Under this setting, we introduce the notion
of Group Monotonicity: we aim to find the models h such that the outputs corresponding to each
class satisfy a monotonic relationship with a specific subset of high-level representations given by
some inner convolutional layer. Let the outputs of a specific layer within a convolutional model be
represented by aw, w ∈ [1, 2, 3, ..., W], where W indicates the width of the chosen layer given by its
number of output feature maps. For simplicity of exposition, we consider the rather common case
of convolutional layers where each feature map aw is 2-dimensional. We then partition such a set
of representations into disjoint subsets, or slices, of uniform sizes. Each subset is then paired with a
particular output or class, and hence denoted by Sk, k ∈ Y. An illustration is provided in Figure 1,
where a generic convolutional model has the outputs of a specific layer partitioned into slices Sk
which are paired with the corresponding output units.
Definition 2 Group monotonic convolutional classifiers: We say h is group monotonic for input
x and its corresponding class label y if h(x)y is partially monotonic relative to all elements in Sy .
We highlight that in this case, unlike the discussion in Section 3, monotonicity is not an applica-
tion requirement, and it does not need to be satisfied everywhere. Intuitively, our goal is to “reserve”
groups of high-level features to be monotonic with respect to different classes. Such an structure can
add extra benefits to models, e.g., more accurate anomaly detection. For training, we perform mono-
tonic risk minimization as described in Eq. 1, and the risk is given by the negative log-likelihood
over training points. Moreover, We design a penalty Ω that focuses only on observed data points
during training and penalizes the slices of the Jacobian corresponding to a given class, i.e., a cross-
entropy criterion enforces larger gradients on the class slice. In order to formally introduce such a
penalty, denoted by ΩΩgroup, we first define the total gradient Ok, k ∈ Y, of a slice Sk as follows:
Oy (x) = Paw ∈Sy Pi j ∂hxx∖, where the inner sum accounts for spatial dimensions of aw. Given
the set of total gradients, a batch of size m, and inverse temperature μ, Ωgroup will be:
◎group
m m	Oyi(Xi)
-ɪ X log	e "	.
m ʌZ	LK	Ok(X)
i=1	工 k=1 e μ
(3)
4.2 Evaluation
4.2.1	Assessing performance of group monotonic classifiers
We start our evaluation by verifying whether the group monotonicity property can be effectively
enforced into classifiers trained on standard object recognition benchmarks. In order to do so, we
verify the performance of the total activation classifier, as defined by: arg maxk∈Y Tk(x), where Tk
indicates the total activation on slice Sk: Tk(x) = Pa ∈S Pi,j aw,i,j(x). A good prediction per-
formance of such a classifier serves as evidence that the group monotonicity property is satisfied by
the model over the test data under consideration since it indicates the slice relative to the underlying
class of test instances has the highest total activation. We thus run evaluations for both CIFAR-10
and ImageNet, and classifiers in each case correspond to WideResNets (Zagoruyko & Komodakis,
2016) and ResNet-50 (He et al., 2016), respectively. Training details are presented in Appendix D.
Results are reported in Table 2 and compared
based on the top-1 prediction accuracy mea-
sured on the test data. We use standard clas-
sifiers as the baselines where no monotonic-
ity penalty is applied in order to isolate the
effect of the penalty. In both datasets, the
total activation classifiers for group mono-
tonic models (indicated by the prefix mono)
are able to approximate the performance of
Model	argmaxk∈γ h(x)k arg maxfc∈γ Tk(X)
CIFAR-10
WideReSNet	95.46%	16.35%
Mono WideReSNet	95.64%	94.95%
ImageNet
ReSNet-50	75.85%	0.10%
MonoResNet-50	76.50%	72.52%
Table 2: Top-1 accuracy of standard and group
monotonic models.
6
Under review as a conference paper at ICLR 2022
the classifier defined at the output layer, arg maxk∈Y h(x)k . This suggests that the higher total
activation generally matches the predicted class for group monotonic models, which indicates the
property is successfully enforced. Considering performances obtained at the output layer, there were
small variations in accuracy when we included monotonicity penalties, which should be considered
in practical uses of group monotonicity. Nonetheless, results suggest that one can perform closely to
unconstrained models while focusing on the set of group monotonic candidates. Additional experi-
ments are reported on Table 8 on Appendix E for cases with small sample sizes, where we show that
the performance of the classifier defined at the output layer upper bounds that of the total activation
classifier, i.e., the better the underlying classifier the more group monotonic it can be made.
4.2.2	Using group monotonicity to detect anomalies
After showing that group monotonicity can be enforced successfully without affecting the prediction
performance, we discuss approaches to leverage it and introduce applications of the models satis-
fying such a property. In particular, we consider the application of detecting anomalous instances,
i.e., those where the model may have made a mistake. For example, consider the case where a
classifier is deployed to production and, due to some problem external to the model, it is queried to
do prediction for an input consisting of white noise. Standard classifiers would provide a predic-
tion even for such a clearly anomalous input. However, a more desirable behavior is to somehow
indicate that the instance is problematic. We claim that imposing structure in the features, e.g., by
enforcing group monotonicity, can help in deciding when not to predict. To evaluate the proposed
method, we implement anomalous test instances using adversarial perturbations. Namely, we cre-
ate L∞ PGD attackers (Madry et al., 2017) and detect anomalies based on simple statistics of the
features. In details, for a given input x, We compute the normalized entropy H* (x) of the Categor-
ical distribution defined by the application of the softmax operator over the set of total activations
TY(x): H*(χ) = "k∈YPg(K) logPk, where K = |Y| and the set PY(x) corresponds to the parame-
ters ofa categorical distribution defined by: pY (x) = softmax(TY (x)). Decisions can then be made
by comparing H*(x) with a threshold T ∈ [0,1], defining the detector 1{h*>T}. We evaluate the
detection performance of this approach on both MNIST and CIFAR-10. Training for the case of
CIFAR-10 follows the same setup discussed on Section 4.2.1. For MNIST on the other hand, we
modify the standard LeNet architecture by increasing the width of the second convolutional layer
from 64 to 150. This layer is then used to enforce the group monotonicity property. The resulting
model is referred to as WideLeNet. Moreover, Y and μ are set to 1e10 and 1, respectively. Adver-
sarial attacks are created under the white-box setting, i.e., by exposing the full model to the attacker.
The perturbation budget in terms of L∞ distance is set to 0.3 and 蔡 for the cases of MNIST and
CIFAR-10, respectively. Detection performance is reported in Table 3 for the considered cases in
terms of the area under the operating curve (AUC-ROC). The baselines are the models for which
the monotonicity penalty is not enforced. They are trained under the same conditions and the same
computation budget as the models where the penalty is enforced. The results are as expected, i.e.,
for monotonic models, test examples for which the total activations are not structured very often
correspond to anomalous inputs.
Finally, due to space constraints, we discuss the application of group monotonicity to explainability
in appendix F.
Model	AUC-ROC
MNIST
WideLeNet	54.47%
Mono WideLeNet	100.00%
CIFAR-10
^^WideReSNet	67.35%
MonoWideResNet	79.33%
Input
Image
Conv. 1
Si
SK
Conv. n
Class 1
Class 2
Class 3
Class K
Output
Layer
Table 3: AUC-ROC (the higher the bet-
ter) for the detection of adversarially
perturbed data instances.
Figure 1: Illustration ofa group monotonic convolutional
model. Representations output by a certain convolutional
layer are partitioned into disjoint subsets.
7
Under review as a conference paper at ICLR 2022
4.3 Disentangled representation learning under monotonicity constraints
We now discuss another application where using monotonicity as a regularizer can be beneficial. In
particular, we consider the case of learning a disentangled set of representations. In this case, it is
often assumed that the latent variables are independent, and hence control over generative factors
can be achieved, e.g., one can modify a specific aspect of the data by modifying the value of a cor-
responding latent variable. However, we argue that disentanglement is necessary but not sufficient
to enable controllable data generation. We need latent variables that satisfy some notion of mono-
tonicity to be able to decide their values resulting in desired properties. For example, assume we
are interested in generating images of simple geometric forms and desire to control factors such as
shape and size. In this example, even if a disentangled set of latent variables is available, we cannot
decide how to change the value of the latent variable to get a bigger or a smaller object if there is
no monotonic relationship between the size and the value of the corresponding latent variable. We
address this issue and build upon the weakly supervised framework introduced by Locatello et al.
(2020). This work extends the popular β-VAE setting (Higgins et al., 2016) by introducing weak
supervision such that the training instances are presented to the model in pairs (x1 , x2) where only
one or a few generative factors are changing between each pair. Here, we propose to apply group
monotonocity over the activations of the corresponding latent variables to have more controlable fac-
tors. In the VAE setting, the data is generated according to p(x|z)p(z) given the latent variables z.
Approximation is then performed by introducing pθ(x|z) and qφ(z∣χ), both parameterized by neu-
ral networks. Our goal is to have z fully factorizable in its dimensions, i.e., p(z) = QiD=i1m[z] p(zi),
which needs to be captured by the approximate posterior distribution qφ (z|x). Training is performed
by maximization of a lower-bound on the likelihood of the data, which yields the following loss:
LELBO = Eχi,χ2 E -E%(z∣χi) log(pθ(xi∣z)) + βDκL(qφ(z∣xz),p(z)),	(4)
i∈{1,2}
where qφ(^j|xi) = qφ(zj|xi) for the latent dimensions Zi that change across x1 and x2, and
qφ(Zj|xi) = 1 (qφ(Zj∣x1) + qφ(Zj|x2)) for those that are common (i.e., the approximate poste-
rior of the shared latent variables are forced to be the same for x1 and x2). The outer expectation
is estimated by sampling pairs of data instances (x1, x2) where only a number of generative factors
vary. In our experiments, we consider the case where exactly one generative factor changes across
inputs. Moreover, we follow Locatello et al. (2020) and assign the changing factor, denoted by y,
to the dimension j of z such that y = arg maxj∈Dim[z] DKL(zj1, zj2). While the above objective
enforces disentanglement, controllable generation requires some regularity in z so that users can
decide values of Z resulting in desired properties in the generated samples. We introduce Ωvae to
enforce such a regularity, i.e., a monotonic relationship is enforced for the distance between pairs of
images where only a particular generative factor vary and a corresponding latent variable. In other
words, an increasing trend in the value of each dimension of Z should yield a greater change in the
output along a generative factor. Formally, Ωvae is defined as follows:
m	L(χi,1,χi,2,yi)	L(χi,2,χi,1,yi)
1	~,∖	e μ	e μ
ωVAE = - 2m Xlog 二K —L(χi,1,χi,2,k) + log 二K —L(χi,2,χi,1,k)，	(5)
i=1	Ek=Ie μ	Ek=Ie μ
where L is given by the gradient of the mean squared error (MSE) between pairs of images that are
1-factor away with respect to the dimension y of Z assigned to the changing factor, i.e.:
L(xi, xj, y)
∂ MSE(Xi,xj)
∂Z
∂Zy
(6)
In this case, X indicates the reconstruction of x. We evaluate such an approach by training the
same 4-layered convolutional VAEs described in (Higgins et al., 2016) using the 3d-shapes dataset2.
The dataset is composed of images containing shapes generated from 6 independent generative
factors: floor color, wall color, object color, scale, shape and orientation. All combinations of these
factors are present exactly once, resulting in m = 480000. We compared VAEs trained with and
without the inclusion of the monotonicity penalty given by Ωvae. We highlight that the goal of
2https://github.com/deepmind/3d-shapes
8
Under review as a conference paper at ICLR 2022
Inq p-0°myEa-p s-εlasβ8
--HoIotiolU—non
HHHUS
HHHUS
HHHUS
HHSUS
・ms
BaseIine
-MSSSSS
*≡wvv≡
Ssskss
HSHtt≡⅛
HHHWHW
HHHWH⅛
HSH⅛SS
SSHrtHS
SHHrtHS
HSHttSS
SSH⅛BS
SSHttHS
SHS⅛Ha
HSHttaW
HHHWHH
SHHttHW
SBilBaS
WB⅛⅛SB
Size
Wall color
Floor color
Object color
Orientation
Monotonic model

USUHH
USUHH
USUHH
Figure 2: Comparisons between data generated by standard and monotonic models. On the two
panels on the left, we compare generations from a linear combination of the latent code of 2 images
which only differs in the object color. On the two panels vertically stacked on the right, we start
from the same image but change one latent dimension at a time.
the proposed framework is not to improve over current approaches in terms of how disentangled the
learned representations are. Rather, we seek to achieve similar results in that sense, but impose extra
regularity and structure in the relationship between the generated images and the values of z so that
the generative process is more easily controllable. Qualitative analysis is performed and shown in
Figure 2. The two panels on the left represent the data generated by a linear combination of the latent
code corresponding to two images that only vary in the factor object color. The panels stacked on
the right present a per-dimension traversal of the latent space starting from a common image. It can
be observed that disentanglement is indeed achieved in both cases. The monotonic model presents
much smoother transitions between colors while the base model gives long sequences of very close
images followed by very sharp transitions where the colors sometimes repeat (e.g., green-yellow-
green transitions in the fourth row). As for the results per factor, the monotonic model provides
more structure in the latent space compared to the base model. This can be observed in the shape
factor. The monotonic model provides a certain order: sphere, cylinder, and then cube. Visually
inspecting many samples, the monotonic model is following this order for the generated shapes.
This pattern is even more pronounced in the color factors. We have found that the colors generated
by the monotonic model follows the order of the colours in the HUE cycle. So our model has ordered
the latent space and we know how to navigate it to generate a desired image. On the other hand, the
baseline has no clear order of the latent space. For example, the baseline generates cubes at different
ranges of z. Similarly, the colors generated by the baseline model do not have a clear order. To
further support the claim that QVAE induces regularity in the latent space, We introduce the analysis
shown in Table 4. We started by increasing z3 (associated to Model HUE StruCtured rate
floor color for both models), and recorded the sequence of the	Base model	0.00%
generated colors. We observed that for a large fraction of the Mon. model	89.44%
data, the monotonic models yield sequences of images where
the color of the floor is ordered according to its corresponding Table 4: rate of examples where
HUE angle. Further details are available in Appendix H. colors are sorted according to hue. 5 * * * * * * * * *
5 Conclusion
We proposed approaches that enable learning algorithms based on risk minimization to find solutions
that satisfy some notion of monotonicity. First, we discussed the case where monotonicity is a design
requirement that needs to be satisfied everywhere. In this case, we identified limitations in prior
work that resulted in models satisfying the property only in very specific parts of the space. We then
introduced an efficient procedure that was observed to significantly improve the solutions in terms
of the volume of the space where the monotonicity requirement is achieved. In addition, we further
argued that, even when not required, models satisfying monotonicity present useful properties. We
studied the case of image classifiers and generative models and showed that imposing structure in
learned representations via group monotonicity is beneficial and can be done efficiently.
9
Under review as a conference paper at ICLR 2022
References
Norman P Archer and Shouhong Wang. Application of the back propagation neural network algo-
rithm with monotonicity constraints for two-group classification problems. Decision Sciences, 24
(1):60-75,1993.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
William Taylor Bakst, Nobuyuki Morioka, and Erez Louidor. Monotonic kronecker-factored
lattice. In International Conference on Learning Representations, 2021. URL https://
openreview.net/forum?id=0pxiMpCyBtr.
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-
cam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018
IEEE winter conference on applications of computer vision (WACV), pp. 839-847. IEEE, 2018.
Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=DNl5s5BXeBn.
Charles Dugas, Yoshua Bengio, Francois Belisle, Claude Nadeau, and Rene Garcia. Incorporating
second-order functional knowledge for better option pricing. Advances in neural information
processing systems, pp. 472-478, 2001.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.
Journal of the American Mathematical Society, 29(4):983-1049, 2016.
Eric Garcia and Maya Gupta. Lattice regression. In Y. Bengio, D. Schuurmans, J. Lafferty,
C. Williams, and A. Culotta (eds.), Advances in Neural Information Processing Systems, vol-
ume 22. Curran Associates, Inc., 2009. URL https://proceedings.neurips.cc/
paper/2009/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural
networks. In International conference on machine learning, pp. 1764-1772. PMLR, 2014.
Akhil Gupta, Naman Shukla, Lavanya Marla, Arinbjorn Kolbeinsson, and Kartik Yellepeddi. HoW
to incorporate monotonicity in deep networks while preserving flexibility? arXiv preprint
arXiv:1909.10662, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, MattheW Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts With a
constrained variational frameWork. 2016.
AndreW HoWard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun
Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314-1324, 2019.
Forrest N Iandola, Song Han, MattheW W MoskeWicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters andj 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
10
Under review as a conference paper at ICLR 2022
Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic neural networks.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 15427-15438. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
b139aeda1c2914e3b579aafd3ceeb1bd- Paper.pdf.
Francesco Locatello, Ben Poole, Gunnar Raetsch, Bernhard Scholkopf, Olivier Bachem, and
Michael Tschannen. Weakly-supervised disentanglement without compromises. In Hal Daume
III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learn-
ing, volume 119 of Proceedings of Machine Learning Research, pp. 6348-6359. PMLR, 13-18
Jul 2020. URL http://proceedings.mlr.press/v119/locatello20a.html.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
An-phi Nguyen and Maria Rodriguez Martinez. Mononet: towards interpretable models by learning
monotonic features. arXiv preprint arXiv:1909.13611, 2019.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van den Broeck.
Counterexample-guided learning of monotonic neural networks. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural In-
formation Processing Systems, volume 33, pp. 11936-11948. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
8ab70731b1553f17c11a3bbc87e0b605-Paper.pdf.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-
Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.
In International Conference on Machine Learning, pp. 6438-6447. PMLR, 2019.
Antoine Wehenkel and Gilles Louppe. Unconstrained monotonic neural networks. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alchei-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
2a084e55c87b1ebcdaad1f62fdbbac8e- Paper.pdf.
Saining Xie, Ross Girshick, Piotr Dollair, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 1492-1500, 2017.
Seungil You, David Ding, Kevin Canini, Jan Pfeifer, and Maya Gupta. Deep lattice networks and
partial monotonic functions. arXiv preprint arXiv:1709.06680, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Hongyi Zhang, Moustapha Cissei, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri-
cal risk minimization. In ICLR (Poster), 2018. URL https://openreview.net/forum?
id=r1Ddp1-Rb.
11
Under review as a conference paper at ICLR 2022
Figure 3: Illustration unit spheres B and Br on the plane.
A Illustrative examples on the s phere: Mixup helps to populate
THE SMALL VOLUME INTERIOR REGION
To further illustrate the issue discussed in the item 2 of Section 3.1 as well the effect of our proposal,
we discuss a simple example considering random draws from the unit n-sphere, shown in Figure 3,
i.e., the set of points B = {x ∈ Rn : ||x||2 < 1}. We further consider a concentric sphere of radius
0 < r < 1 given by Br = {x ∈ Rn : ||x||2 < r}. We are interested in the probability of a random
draw from B to lie outside of Br, i.e.: P(∣∣x∣∣2 > r),x 〜 D(B), for some distribution D. We start
by defining D as the Uniform(B), which results in P (||x||2 > r) = 1 - rn. In Figure 4a, we can see
that for growing n, P (||x||2 > r) is very large even if r ≈ 1, which suggests most random draws
will lie close to B’s boundary.
We now evaluate the case where mixup is applied and random draws are taken in two steps: we
first observe y 〜Uniform(B), and then we perform mixup between y and the origin3, i.e., X = λy,
λ 〜 UnifOrm([0,1]). In this case, P(∣∣x∣∣2 > r) = (1 - rn)(1 - r), which is shown in Figure
4b as a function of r for increasing n. We can then observe that even for large n, P (||x||2 > r)
decays linearly with r, i.e., we populate the interior of B and x in this case follows a non-uniform
distribution such that its norms histogram is uniform.
1.0
0.8
0.6
0.4
0.2
0.0
(a) P(∣∣x∣∣2 > r) as a function of r for various n and
X 〜Uniform(B).
Figure 4: Illustrative example showing that uniformly distributed draws on a unit sphere in Rn
concentrate on its boundary for large n. Applying mixup populates the interior of the space.
(b) P(∣∣x∣∣2 > r) as a function of r for various n.
In this case, X = λy, λ 〜 Uniform([0,1]), y 〜
Uniform(B).
3Similar conclusions hold for any fixed point within B. The origin is chosen for convenience.
12
Under review as a conference paper at ICLR 2022
B	Proof-of-concept evaluation
We start by describing the approach we employ to generate data containing the properties required
by our evaluation. Denote a design matrix by XN ×D such that each of its N rows corresponds to a
feature vector within RD . In order to ensure the data lies in some manifold, we first obtain a low-
dimensional synthetic design matrix given by XN0 ×d, where each entry is sampled randomly from
Uniform([-10, 10]). We then expand it to RD by applying the following transformation:
X = X0A,
(7)
where the expansion matrix given by Ad×D is such that each of its entries are independently drawn
from Uniform([0, 1]). Throughout our experiments, d = b0.3Dc was employed.
Target values for the function f to be approximated are defined as sums of functions of scalar
arguments applied independently over each dimension. We thus select a set of dimensions M ∈ [D]
with respect to which f is to be monotonic, i.e.:
f(x) =	gi(xi) +	hj (xj),	(8)
i∈M	j∈M
and every gi : R 7→ R is increasing monotonic, while every hi : R 7→ R is not monotonic.
We then create two evaluation datasets. One of them, referred to as the validation set, is identically
distributed with respect to X since it is obtained following the same procedure discussed above.
In order to simulate covariate-shift, we create a test set by changing the expansion matrix A to a
different one.
Xval = Xv0 alA, Xtest = Xt0estAtest,	(9)
where Atest will be given by entry-wise linear interpolations between A, used to generate the train-
ing data, and a newly sampled expansion matrix A0: Atest = αA0 + (1 - α)A. The parameter
α ∈ [0, 1], set to 0.8 in the reported evaluation, controls the shift between Atest and Atest in terms
of the Frobenius norm, which in turn enables the control of how much the test set shifts relative to
the training data.
We thus trained models to approximate f for spaces of increasing dimensions as well as for an
increasing number of dimensions with respect to which f is monotonic. Results are reported in
Table 5 in terms of RMSE on the two evaluation datasets, and in terms of monotonicity in Table
6 where ρ is computed both on random points and on the shifted test set. Entries in the tables
correspond to the centers of 95% confidence intervals resulting from 20 independent training runs.
We highlight the two following observations regarding the prediction performances shown in table
5: different models present consistent performances across evaluations, which suggests different
monotonicity-enforcing penalties do not significantly affect prediction accuracy. Moreover, the pro-
posed approach used to generate test data under covariate-shift is effective given the gap in perfor-
mance consistently observed between the validation and the test partitions. In terms of monotonicity,
results in Table 6 suggest that Ωrandom and Ωtrain are only effective on either random or data points,
which seems to aggravate when the dimension D grows. Ωm%χup, on the other hand, is effective on
both sets of points, and continues to work well for growing D. Furthermore, covariate-shift signifi-
cantly affects Ωtrain for higher-dimensional cases, while Ωmiχup performs well in such a case.
|M|/D	20/100		40/200		80/400		100/500	
	Valid. RMSE	Test RMSE	Valid. RMSE	Test RMSE	Valid. RMSE	Test RMSE	Valid. RMSE	Test RMSE
Non-mon.	0.007	0.107	0.006	0.082	0.007	0.087	0.011	0.146
◎random	0.008	0.117	0.006	0.081	0.007	0.093	0.012	0.125
◎train	0.008	0.115	0.006	0.086	0.007	0.089	0.012	0.134
Ω一 mixup	0.008	0.114	0.007	0.084	0.008	0.088	0.012	0.134
Table 5: Prediction performance of models trained on generated data in spaces of growing dimen-
sion (D) and number of monotonic dimensions (|M |). Different regularization strategies do not
affect prediction performance. The performance gap consistently observed across the evaluation
sets highlights the shift between the two sets of points. The lower the values of RMSE the better.
13
Under review as a conference paper at ICLR 2022
M |/D	20/100	40/200	80/400	100/500
	Prandom	Ptest	Prandom	ρtest	Prandom	Ptest	Prandom	Ptest
Non-mon.	99.90%	99.99%	97.92%	94.96%	98.47%	96.56%	93.98%	90.01%
Crandom	0.00%	3.49%	0.00%	4.62%	0.01%	11.36%	0.02%	19.90%
Ω≠ - train	1.30%	0.36%	4.00%	0.58%	9.67%	0.25%	9.25%	5.57%
Ω~∙ mixup	0.00%	0.35%	0.00%	0.44%	0.00%	0.26%	0.00%	0.42%
Table 6: Fraction of monotonic points ρ for models trained on generated data in spaces of growing
dimension (D) and number of monotonic dimensions (|M |). Different regularization strategies is
effective on only one of Prandom or Ptest, while Ωmiχup seems effective throughout conditions. The
lower the values of ρ the better.
C Datasets, models, and training details for experiments
reported in Section 3.2
Algorithm 1 describes a procedure used to compute the proposed regularization Ωmiχup.
Algorithm 1 Procedure to compute Ωmiχup.
Input mini-batch X[N×d] , model h, monotonic dimensions M
Xω = {} # Initialize set of points used to compute regularizer.
XN ×d] 〜Uniform(XN) # Sample random mini-batch with size N.
X = concat(X, X) # Concatenate data and random batches.
repeat
i,j 〜UnifOrm({1, 2,..., 2Ν }2) # Sample random pair of points.
λ 〜Uniform([0, l])
X = λXi + (1 — λ)Xj # Mix random pair.
XΩ.add(x) # Add X to set of regularization points.
until Maximum number of pairs reached
Cmixup(h，M) = ∣X1Ω∣ Px∈Xω Pi∈M max (0，— "∂Xχ))
return Cmixup
In Table 7, we list details on the three datasets used to evaluate our proposals as reported in Section
3.2.
Dataset	Dim[X]	JML	# Train	# Test	Task
Compas4	13	4	4937	1235	Classification
Loan Lending Club5	33	11	8500	1500	Regression
Blog Feedback6	280	8	47287	6904	Regression
Table 7: Description of datasets used for empirical evaluation.
Models follow the architecture in (Liu et al., 2020) using dense layers whose weights are kept sep-
arate in early layers for the input dimensions with respect to which monotonicity is to be enforced.
We set the depth of all networks to 3, and use a bottleneck of size 10 for two datasets (Compas
and Loan Lending Club), and 100 for the case of the Blog Feedback dataset and the experiments
on generated data. Training is carried out with the Adam optimizer (Kingma & Ba, 2014) with a
global learning rate of 5e—3, and γ is set to 1e4. The training batch size is set to 256 throughout
experiments.
4https://www.kaggle.com/danofer/compass
5https://www.openintro.org/data/index.php?data=loans_full_schema
6https://archive.ics.uci.edu/ml/datasets/BlogFeedback
14
Under review as a conference paper at ICLR 2022
Model	argmaXk∈Y h(x)k	argmaxk∈YTk(x)
	10%			
WideResNet	85.68%	16.35%
MonoWideResNet	85.77%	82.21%
30%		
WideResNet	92.12%	14.51%
MonoWideResNet	92.42%	88.88%
	60%			
WideResNet	94.51%	10.08%
MonoWideResNet	94.86%	93.81%
Table 8: Top-1 accuracy obtained by both standard and group monotonic models on sub-samples of
CIFAR-10. Predicition performance obtained by classifiers defined by the total activations is upper
bounded by the performance obtained at the output layer for monotonic models.
D Models and training details for experiments reported in
Section 4
For the case of CIFAR-10, WideResNets (Zagoruyko & Komodakis, 2016) are used. The models are
initialized randomly and trained both with and without the monotonicity penalty. Standard stochastic
gradient descent (SGD) implements the parameters update rule with a learning rate starting at 0.1,
being decreased by a factor of 10 on epochs 10, 150, 250, and 350. Training is carried out for a
total of 600 epochs with a batch size of 64. For ImageNet, on the other, training consists of fine
tuning a pre-trained ResNet-50, where the fine-tuning phase included the monotonicity penalty. We
do so by training the model for 30 epochs on the full ImageNet training partition. In this case, given
that the label set Y is relatively large, using the standard ResNet-50 would result in small slices
Sk . To avoid that, we add an extra final convolution layer with W = 15K. Training is once more
carried out with SGD using a learning rate set to 0.001 in this case, and reduced by a factor of 5 at
epoch 20. In both cases, the group monotonicity property is enforced at the last convolutional layer.
Other hyperparameters such as the strength γ of the monotonicity penalty as well as the inverse
temperature μ used to compute Ω∖group are set to 1 and 50 for the case of CIFAR-10, and to 5 and
10 for the case of ImageNet. Both momentum and weight decay are further employed and their
corresponding parameters are set to 0.9 and 0.0001. For MNIST classifiers, training is performed
for 20 epochs using a batch size of 64 and the Adadelta optimizer (Zeiler, 2012) with a learning rate
of1.
E	Enforcing group monotonicity under small samples
Using CIFAR-10, we further evaluate how the proposed group monotonicity penalty behaves in data-
constrained settings, i.e., we check whether or not the property can be enforced under small sample
regimes. We do so by sub-sampling the original training data by randomly selecting a fraction of
the training images uniformly across classes. We then train the same WideResNet for the same
computation budget in terms of number of iterations as the models trained in the complete set of
images. The learning rate schedule also matches that of the training on the full dataset in that the
learning rate is reduced at exactly the same iterations across all training cases. Results are reported in
Table 8 for sub-samples corresponding to 10%, 30%, and 60% of CIFAR-10. Results are consistent
across the three sets of results in showing that predictions obtained from the total activation of
feature slices approximate the prediction performance of the underlying model for the case of group
monotonic predictors, i.e., the extent to which the underlying model is able to accurately predict
correct classes upper bound the resulting “level of monotonicity”. In simple terms, the better the
classifier, the more group monotonic it can be made.
F Selecting feature maps to compute visual explanations
Approaches based on Class Activation Maps (CAM) such as Grad-CAM and its variations (Selvaraju
et al., 2017; Chattopadhay et al., 2018) seek to extract explanations from convolutional models. By
15
Under review as a conference paper at ICLR 2022
Figure 5: Example of explanation heat-map and corresponding occlusion obtained with Grad-CAM
and a ResNet-50 trained on ImageNet. The example belongs to the validation set and corresponds
to the class snowmobile.
explanation we mean to refer to indications of properties of the data implying the predictions of a
given model. Under such a framework, one can obtain so-called explanation heat-maps through the
following steps: (1) Compute a weighted sum of activations of feature maps in a chosen layer; (2)
Upscale the results in order to match the dimensions of the input data; (3) Superimpose results onto
the input data. Specifically for the case of applications to image data, following those steps results
in highlighting the patches of the input that were deemed relevant to yield the observed predictions.
Different approaches were then introduced in order to define the weights used in the first step. A
very common choice is to use the total gradient of the output corresponding to the prediction with
respect to activations of each feature map.
For the case of group monotonic classifiers, we are interested in verifying whether one can define
useful explanation heat-maps by considering only the feature slices corresponding to the predicted
class, i.e., for a given input pair (x, y), we compute explanation heat-maps considering only its cor-
responding feature activation slice Sy(x). We thus design an experiment to evaluate the effectiveness
of such an approach by using external auxiliary classifiers to perform predictions from test data that
was occluded using explanation heat-maps obtained using different models and sets of representa-
tions. In other words, we use the explanation maps to remove from the data the parts that were not
indicated as relevant. We then assume that good explanation maps will be such that classifiers are
able to correctly classify occluded data since relevant patches are conserved. In further details, oc-
clusions are computed by first applying a CAM operator given a model h and data x, which results
in a heat-map with entries in [0, 1]. We then use such a heat-map as a multiplicative mask to get an
occluded version of x, denoted x0, i.e.:
x0 = CAM(x, h) ◦ x,	(10)
where the operator ◦ indicates element-wise multiplication. An example of such a procedure is
shown in Figure 5. We apply the above procedure to all of the validation data, and use resulting
points to then assess the prediction performance of auxiliary classifiers.
Explanation maps are computed using the same models discussed in Section 4.2.1 for ImageNet.
The CAM operator corresponds to a variation of Grad-CAM++ (Chattopadhay et al., 2018) where
the model activations are directly employed for weighing feature maps rather than the gradients.
We consider 4 auxiliary pre-trained classifiers corresponding to ResNext-50 (Xie et al., 2017),
MobileNet-v3 (Howard et al., 2019), VGG-16 (Simonyan & Zisserman, 2014), and SqueezeNet
(Iandola et al., 2016). Results are reported in Table 9 which also include the reference performance
of the auxiliary classifiers on the standard validation set in order to provide an idea of the gap in per-
formance resulting from removing parts of test images via occlusion. We highlight the performance
reported in the last row of the Table. In that case, explanation maps for the group monotonic model
are computed from only the features of the class slice, which is enough to match the performance
of a standard ResNet-50 with full access to the features. This suggests that representations learned
16
Under review as a conference paper at ICLR 2022
Model (h)	Aux. classifier			
	ResNext-50	MobileNet-v3	VGG-16	SqueezeNet
Reference perf.	_ 77.62% _ _	_ _ 74.04% _ _	_ 71：59%_	_ _58:09% _
ResNet-50	— 一72.94% ——	——68.31% ——	—67.34%-	一—49.95% 一一
MonoResNet-50	72.88%	68.75%	66.99%	48.92%
MonoResNet-50 (Constrained)	72.44%	66.55%	66.92%	45.83%
Table 9: Top-1 accuracy of auxiliary classifiers evaluated on data created by occluding patches
deemed irrelevant by explanation heat-maps given by different models. The performance of mono-
tonic classifiers when constrained to consider only the feature maps within the slice corresponding
to their prediction is further reported and shown to closely math the performance of cases where the
full set of features is considered.
by group monotonic models are such that all the information required to explain a given class is
contained in the slice reserved for that class.
17
Under review as a conference paper at ICLR 2022
G Examples of explanation heat-maps and occluded data
Figure 6: Examples of explanation heat-maps superimposed onto images. From left to right we
have the original image, results obtained from a ResNet-50, a monoResNet-50, and a monoResNet-
50 where the CAM operator only access the slice corresponding to the underlying class. All are
obtained with Grad-CAM.
18
Under review as a conference paper at ICLR 2022
Figure 7: Examples of occluded data using explanation heat-maps. From left to right we have the
original image, results obtained from a ResNet-50, a monoResNet-50, and a monoResNet-50 where
the CAM operator only access the slice corresponding to the underlying class. All are obtained with
Grad-CAM.
19
Under review as a conference paper at ICLR 2022
Figure 8: HUE circle of RGB images. Original image from: https://en.wikipedia.org/
wiki/Hue.
H Analysis of color sequences for generated data
We performed a set of experiments in order to evaluate whether some kind of ordering could be ob-
served once we generate data for increasing values of z, specifically on dimensions that correspond
to colors. To do that, we created an increasing sequence of values by defining a uniform grid in [0, 1]
with 50 steps. We then encoded a particular image, but decoded latent vectors after substituting the
z value in the dimension corresponding to floor color by the values in the sequence.
Generated sequences of images are shown in Figures 9 and 10 for the base and monotonic models,
respectively. In each such a case, we plot the images on the left, and bottom-left patches of size
10x10 so as to highlight the color sequences that we observe with such an approach. Surprisingly,
we observed that monotonic models tend to generate colors in a sequence that matches the HUE
circle for RGB images, represented in Figure 8 for reference. Besides visually verifying that to be
the case across a number of generated examples, in Table 4 in Section 4.3 we check the fraction of
(a) Data for increasing values for the latent
dimension associated to floor color.
(b) Bottom-left 10x10 patches of generated
images.
Figure 9: Data generated by standard model for traversals of z on the dimension corresponding to
floor color
20
Under review as a conference paper at ICLR 2022
əbəəəbs
rrrrrrr
HBBHHBb
(a) Data for increasing values for the latent
dimension associated to floor color.
(b) Bottom-left 10x10 patches of generated
images.
Figure 10:	Data generated by monotonic model for traversals of z on the dimension corresponding
to floor color
21
Under review as a conference paper at ICLR 2022
I Examples of data generated with standard and monotonic
MODELS
(a) Input pair.
(b)	Data generated by standard model.
(c)	Data generated by monotonic model.
Figure 11:	Generating data by moving along the line passing over latent representation for inputs
for which a single factor is different. Generative factor changing: floor color.
22
Under review as a conference paper at ICLR 2022
(a) Input pair.

□EiSS⅝
(c) Data generated by monotonic model.
Figure 12:	Generating data by moving along the line passing over latent representation for inputs
for which a single factor is different. Generative factor changing: wall color.
23
Under review as a conference paper at ICLR 2022
(a) Input pair.
(b) Data generated by standard model.	(c) Data generated by monotonic model.
Figure 13:	Generating data by moving along the line passing over latent representation for inputs
for which a single factor is different. Generative factor changing: object color.
24
Under review as a conference paper at ICLR 2022
(a) Input pair.
(b) Data generated by standard model.	(c) Data generated by monotonic model.
Figure 14:	Generating data by moving along the line passing over latent representation for inputs
for which a single factor is different. Generative factor changing: scale.
25
Under review as a conference paper at ICLR 2022
(a) Input pair.
b∏πππlγ⅛
πππππ
口口口口口
(c) Data generated by monotonic model.
(b) Data generated by standard model.
Figure 15:	Generating data by moving along the line passing over latent representation for inputs
for which a single factor is different. Generative factor changing: shape.
26
Under review as a conference paper at ICLR 2022
(a) Input pair.
(b)	Data generated by standard model.
(c)	Data generated by monotonic model.
Figure 16:	Generating data by moving along the line passing over latent representation for inputs
for which a single factor is different. Generative factor changing: orientation.
27