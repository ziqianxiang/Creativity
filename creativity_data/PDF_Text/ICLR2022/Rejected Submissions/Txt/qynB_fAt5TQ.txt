Under review as a conference paper at ICLR 2022
Centroid Approximation for Bootstrap
Anonymous authors
Paper under double-blind review
Ab stract
Bootstrap is a principled and powerful frequentist statistical tool for uncertainty
quantification. Unfortunately, standard bootstrap methods are computationally
intensive due to the need of drawing a large i.i.d. bootstrap sample to approximate
the ideal bootstrap distribution; this largely hinders their application in large-scale
machine learning, especially deep learning problems. In this work, we propose
an efficient method to explicitly optimize a small set of high quality “centroid”
points to better approximate the ideal bootstrap distribution. We achieve this by
minimizing a simple objective function that is asymptotically equivalent to the
Wasserstein distance to the ideal bootstrap distribution. This allows us to provide
an accurate estimation of uncertainty with a small number of bootstrap centroids,
outperforming the naive i.i.d. sampling approach. Empirically, we show that our
method can boost the performance of bootstrap in a variety of applications.
1 Introduction
Bootstrap is a simple and principled frequentist uncertainty quantification tool and can be flexibly
applied to obtain data uncertainty estimation with strong theoretical guarantees (Hall et al., 1988;
Austern & Syrgkanis, 2020; Chatterjee et al., 2005; Cheng et al., 2010). In particular, when combined
with the maximum likelihood estimator or more general M-estimators, bootstrap provides a general-
purpose, plug-and-play non-parametric inference framework for general probabilistic models without
case-by-case derivations; this makes it a promising frequentist alternative to Bayesian inference.
However, the standard bootstrap inference is highly expensive in both computation and memory as it
typically requires drawing a large number1 of i.i.d. bootstrap particles (samples) to obtain an accurate
uncertainty estimation. In the context of this paper, as each bootstrap particle/sample/centroid is a
machine learning model, we might directly call a model as particle/sample/centroid. With a small
number of particles, bootstrap may perform poorly. As a consequence, when applied to deep learning,
we need to store a large number of neural networks and feed the input into a tremendous number
of networks every time we make inference, which can be quite expensive and even unaffordable
for deep learning problems with huge models2 . For example, in autonomous driving applications,
our device can only store a limited number of models and we need to make decisions within a short
time, which makes the standard bootstrap with a large number of models no more feasible. Typical
ensemble methods in deep learning, such as Lakshminarayanan et al. (2017); Huang et al. (2017);
Vyas et al. (2018); Maddox et al. (2019); Liu & Wang (2016), can only afford to use a small number
(e.g., less than 20) of models.
Therefore, to make bootstrap more accessible in modern machine learning, it is essential to develop
new approaches that break the key computation and memory barriers mentioned above. This paper
aims to improve the bootstrap when the resource at inference is limited. We are motivated to consider
the following problem:
How to improve the accuracy of bootstrap inference when the number of particles is limited?
We attack this challenge by presenting an efficient centroid approximation for bootstrap. Our method
replaces the i.i.d. bootstrap particles with a set of carefully optimized centroid particles that are
1For example, thousands of, as suggested by Statistics textbooks such as Wasserman (2013).
2While training cost is an extra burden, it is small compared with the cost of making prediction as we only
need to train the model once but make countless predictions at deployment.
1
Under review as a conference paper at ICLR 2022
guaranteed to provide an accurate and compact approximation to the ideal bootstrap distribution so
that only a smaller number of particles is needed to obtain good performance.
Our method is based on minimizing a specially designed objective function that is asymptotically
equivalent to the Wasserstein distance between the ideal bootstrap distribution and the particle
distribution formed by the learned centroids. During the training, each centroid adjusts its location
being aware of the locations of the others so that centroids are diversified and well distributed on
the domain. Our method is similar to doing K-means on the ideal bootstrap distribution, finding
K representative centroids that well represent K separate parts of the target distribution’s domain
in an optimal way. As centroids are optimized to better approximate the distribution, our approach
naturally improves over the vanilla bootstrap with i.i.d. particles. See Figure 1 for illustration.
Empirically, we apply the centroid approximation method to various applications, including confi-
dence interval estimation (DiCiccio et al., 1996), bootstrap method for contextual bandit (Riquelme
et al., 2018), bootstrap deep Q-network (Osband et al., 2016) and bagging method (Breiman, 1996)
for neural networks. We find that our method consistently improves over the standard bootstrap.
Figure 1: The solid lines represent the density of
the target distribution. Left figure: Typical i.i.d.
particles that are randomly distributed on the do-
main. Right figure: The learned diversified cen-
troids that are well distributed on the domain. The
centroids partition the domain into several disjoint
regions (separated by the dashed lines in the figure)
and each centroid can be viewed as the ‘K-means’
center of the region it belongs to.
Notation We use ∣∣∙∣∣ to represent the '2 norm
for a vector and the operator norm for a matrix.
We denote the integer set {1, 2,...., N} by [N].
Given any m, We define the probability simplex
Cm := {[vι,…，Vm] ∈ Rm : Vi ≥ 0, ∀i ∈
[m] and Xi∈[m] Vi = 1}. For a symmetric ma-
trix M, we denote its minimal eigenvalue by
λmin (M). For a positive-definite matrix M, if
M = A>A, then we denote A by M1/2. We de-
note the Wasserstein distance between two distri-
bution ρ1 and ρ2 by W2[ρ1, ρ2]. We use O and
o to denote the conventional big-O and small-
o notation and use Op to denote the stochastic
boundedness. We use
in distribution.
→d to denote convergence
2 Background
Suppose we have a model fθ parameterized by θ in a parameter space Θ ⊆ Rd. Let {xi }in=1 ⊂ X be
a training set with n data points on X. Assume '(x, fθ) is the negative log-likelihood of data point X
with model fθ . A standard approach to estimate θ is maximum likelihood estimator (MLE), which
minimizes the negative log-likelihood function (loss) over the training set
θ = arg min L(θ),	L(θ) = Pn=ι'(Xi, fθ)/n.
θ∈Θ
Here the MLE θ provides a point estimation without any information on the data uncertainty. Bootstrap
is a simple and effective frequentist method to quantify the uncertainty. The bootstrap loss is a
randomly perturbed loss defined as
Lw (θ) = Pn=1 Wi '(xi ,fθ )/n,
where w = [w1, ..., wn ]> is a set of random weights of data points drawn from some distribution π.
A typical choice of π is the multinomial distribution with uniform probability, which corresponds to
resampling on the training set with replacement. Given w , one can calculate its associated bootstrap
particle by minimizing the bootstrap loss:
θw = arg min Lw(θ).
θ∈Θ
(1)
Let Pn be the distribution of θw when W 〜π. Bootstrap theory indicates that we can quantify the
data uncertainty of θ or any function g(θ) using ρπ. We call ρπ the ideal bootstrap distribution and it
is the main object we want to approximate.
2
Under review as a conference paper at ICLR 2022
Denote δθ as the delta measure centered at θ. Standard bootstrap method approximates ρπ by
the particle distribution ρ∏(∙) = Pjm=/疏(∙)∕m formed by m i.i.d. particles {θwj }m=ι, which
can be obtained by drawing m i.i.d. weights {wj }jm=1 from π and calculating each θwj based on
(1). However, for deep learning applications, as discussed in the introduction, storing and making
inference using a large number m of bootstrap particles can be quite expensive. On the other hand,
if m is small, ρ∏ tends to be a poor approximation of ρ∏. In this paper, We aim to improve the
approximation of the particle distribution when m is small.
3 Method
Our idea is simple. Instead of using i.i.d. particles, in which the location of each particle is
independent from that of the others, we try to actively optimize the location of each particle so
that particles are diversified, better distributed and eventually providing a particle distribution with
improved approximation accuracy. A natural way to achieve this goal is to explicitly optimize a set
of points {θj}jm=1 (called centroids) jointly such that the Wasserstein distance between ρπ and the
induced particle distribution is minimized:
j 磔1 = arg θ1	θ ∈θm[Vn	v ]∈Cm W2 [Pm=1vj δθj ,ρ∏].
1 ,..., m ∈ , [v1 ,...,vm ]∈
(2)
Here we consider a Wasserstein distance W2 equipped with a special data-dependent distance metric
∣∣∙∣∣ D that will be introduced later in (5). We note that here we also optimize the probability weights
{vj }jm=1 of the centroids. Finding the optimal centroids and probability weights can be decomposed
into two steps: the centroid learning phase and the probability weights learning phase, based on the
facts in (3,4).
W2 Pj∈[m]vj⅜;,ρ∏ = Jn({θj}j=ι), where Jn(必%.:=Ew〜∏ [minj∈m]∣∣θj - θw||D.⑶
Here (3) implies that, to find the optimal particle distribution in (2), we can start with the centroid
learning phase where we only need to optimize the centroids. It can be achieved by minimizing
Jn({θj}jm=1), which is the averaged distance of bootstrap particles to their closest centroid. After we
obtain the optimal centroids, the optimal probability weights can be learned by (4):
Vj = Vj∕Ps∈[m] V；, where Vj = Pw〜∏ jj = argminj∙∈[m]∣∣θ* - θw||D).	(4)
Here Vj is the proportion of bootstrap particles that are closest to the centroid j. We emphasize that
the optimal solution to two-stage learning is guaranteed to be the global minimizer of the loss in (2)
(see Lemma 3.1 and 3.2 in Canas & Rosasco (2012)).
However, the key issue is that the losses in both (2, 3) can not be computed in practice, as they require
us to access Pn (i.e., obtain θw first in order to calculate the loss). To handle this issue, We seek an
easy-to-compute surrogate loss. Our idea is based on the following observation. Assuming the size
of training data is large, which is usually the case in deep learning, we can expect that θw will be
centered around a small region3. It implies that we should search the centroid in this small region.
Notice that when θ is close to θw, based on Taylor approximation, We have
, ..	, O	__~Γ .	, O . , . O	, . O -Γ____C .	, O . , . O
Lw(θ) ≈ Lw(θw) + Rθ Lw(θw)(θ - θw) + 1/2步-θw) VθLw(θw)(θ - θw)
≈ L∞(θo) + 1∕2∖∖θ - ⅛, where IlVIlD := V>V2L∞(θ°)V.	(5)
Here L∞(θ) := Eχ'(χ, fθ) denotes the population loss; θo is the minimizer of L∞(θ). In (5), we use
the facts4 that V>Lw(θw) = 0; and with large training set, the empirical distribution Pn=ιδχjn
well approximates the whole data population, and hence the bootstrap resampling distribution, i.e.,
Pin=1wiδxi∕n on the empirical distribution also well approximates the whole data population. This
implies that Lw(∙) ≈ L∞(∙) and VjLw(∙) ≈ V2L∞(∙). As the loss are close to each other, their
minimizers are also close θw ≈ θ0. Since L∞(θo) is some (unknown) constant independent with θ,
we can replace the ∣∣θj - θw ∖∖2d in (3) by Lw (θj) as it only adds some constant into the loss.
3This can be formally characterized by central limit theorem as discussed in Section 4.
4We defer the detailed analysis to Section 4.
3
Under review as a conference paper at ICLR 2022
Intuitively, We can expect that the centroid closest to θw is the one that gives the smallest loss on Lw.
It motivates us to learn the centroids via the modified centroid learning phase:
{θj}j=ι = arg. min	Ew〜∏ Iminj∈[m]Lw(θj)] .	(6)
θ1 ,...,θm ∈Θ
Similarly, the optimal probability Weights can be learned via the modified Weight learning phase:
Vj = Vj∕ps∈[m] vs, where Vj = Pw〜∏ j ∈ argminj∈[m]Lw(θj)) .	(7)
We note that here we slightly abuse the notation of θs and VS in (3,4) and (6,7) for simplification. In
the later context, θjj and Vjj are used based on their definitions in (6,7).
Connection to K-means By viewing the target distribution as a set of particles that we want to
cluster, in K-means clustering, each centroid (i.e., K-means center) represents one of the K disjoint
groups5 of particles, which is formed by assigning each particle in the whole set to the closest centroid
among all the K centroids. K-means learns the optimal K centroids in the way that they can best
approximate the whole set. The ‘closeness’ for assigning the particles is measured by the distance
between the two points. As pointed out by Canas & Rosasco (2012), K-means essentially searches
the optimal particle distribution formed by the K centroids that minimizes its Wasserstein distance to
the target distribution. Our centroid approximation idea follows the same fashion of clustering but
our key innovation is to measure the ‘closeness’ by examining the bootstrap loss of the centroids so
that we can still learn the optimal centroids without obtaining the i.i.d. bootstrap particles first. We
also point out that, while we share the same objective as K-means, the optimization algorithms differ.
The Expectation-Maximization type of algorithm used by K-means is not applicable to our scenario.
Comparing with Other Particle Improving Approach Intuitively, from a high level abstracted
perspective, we provide an approach to use K-means type of idea to improve the particle quality
without accessing to the true target distribution. This is the key differentiator of this work to other
approaches that improve the particle quality, as they all require to access the target distribution. For
example Claici et al. (2018) requires that sampling from target distribution is cheap and easy. Chen
et al. (2012; 2018a); Campbell & Beronov (2019) need to access the logarithm of the probability
density function of the target distribution. In our problem, neither sampling from the target distribution
is cheap nor the logarithm of the probability density function is available, making those approaches
not more applicable.
3.1 Training
The optimization of (6) can be solved by gradient descent. Suppose θjj (t) is the j-th centroid at
iteration t. We initialize {θjj(0)}jm=1 by sampling from ρπ and at iteration t, we update θtj by applying
the gradient descent on the loss in (6), which yields
θj(t + 1) J θj(t)-jg(θj(t)),
g(θj(t)) = VθEw〜∏ [I{j ∈ Uw(t)}Lw(θj(t))]/vj(t),
(8)
where we define the index of the closest centroid to particle θw as Uw (t) = arg minj∈[m] Lw (θs (t))
and vs (t) = Pw〜∏ (j ∈ uw(t)) denotes the probability that centroid j is the one that gives the lowest
bootstrap loss. The denominator Vjj(t) in g(θjj(t)) is optional. However, notice that the magnitude of
numerator in g(θkj(t)) decays with larger m, which might require an adjustment of the learning rate
when m changes. This adjustment can be avoided by rescaling with Vkj (t).
We note that {θjj(0)}jm=1 is just m i.i.d. bootstrap particles which is not optimal for approximation
and our algorithm can be viewed as an approach for refining the m particles by solving (6). In
practice, we find that we can simply use random initialization (e.g., draw θ from some Gaussian
distribution) instead.
Centroid Degeneration Phenomenon Naively applying the updating rule (8) may cause a degenera-
tion phenomenon: When a centroid happens to give considerably worse performance than others,
which can be caused by the stochasticity of gradient or worse initialization, the performance of this
centroid will remain considerably worse throughout the optimization. The reason is simple. As this
5i.e. the regions separated by the dashed lines in the right plot of Figure 1.
4
Under review as a conference paper at ICLR 2022
centroid (e.g. θj (t)) gives a considerably worse performance, the probability that it gives the lowest
bootstrap loss, i.e., v* (t), is small. As a consequence, the gradient that updates this centroid is only
based on aggregating information from a small low-density region of π and hence can be unstable and
further degrades this centroid. Note that this mechanism is self-reinforced since when this centroid
cannot be effectively improved in the current iteration, it faces the same issue in the next one. As a
result, this centroid is always significantly worse than the others.
We call this undesirable phenomenon centroid degeneration and we want to prevent this phenomenon
because when it happens, we have a centroid that is not representative and contributes less to
approximating ρπ . We solve this issue with a simple solution and here is the intuition. The reason
that a centroid degenerates lies in that this centroid is far from the good region where it gives a good
performance. And when this happens, we should push the centroid to move towards this good region,
which can be achieved by using the common gradient over the whole training data. Specifically, we
define a threshold γ, indicating centroid j is degenerated if vj*(t) ≤ γ. And when it happens, we
update using the common gradient over the whole data:
θ*(t + 1) J θ*(t) - 6tVθL(θ*(t)).	(9)
In section 4, we give a theoretical analysis on why this modification is important and is able to solve
the centroid degeneration issue.
Practical Algorithm In practice, We estimate the gradient by replacing the expectation over W 〜∏
in (8) with averaging over M i.i.d. samples {wh}hM=1 drawn from π:
g(θ*(t))
PM=Jl{j ∈ Uwh(t)}VθLwh(θ;(t))]
PM=ι I{j ∈ Uw(t)}
(10)
We emphasize that here Uwh and Lwh for all wh can be computed very cheaply, enabling us to use a
very large M to reduce the error of gradient estimation. Specifically, at iteration t, for each j ∈ [m],
we first calculate
L(θ*(t)) = ['(xι,fθ;(t)),…'(xn,fθ泗)]> ∈ Rn,	(11)
which is the vector encodes the loss of centroid j at each data point. This procedure does not introduce
any extra computational overhead compared with standard gradient descent. After that, the bootstrap
loss of centroid j can be computed cheaply by Lw(θj*(t)) = w>L(θj*(t)). Similarly, it is cheap to
obtain Uwh by
Uwh (t) = arg min wh>L(θj* (t)).	(12)
h	j∈[m]	j
Taking the modified updating rule introduced to prevent the centroid degeneration phenomenon into
account, we update θj* (t) by θj* (t + 1) J θj* (t) - tφ(θj* (t)), where
φ(θ*) = ∫g(θ*(t))	ifPh∈[M]I{uwh(t) = j}∕M > Y
j	VθL(θj* (t)) otherwise.
(13)
Notice that as as L(θj*(t)) is pre-computed, calculating Lwh (θj*(t)) for many (e.g., M) different
wh is almost free, since it only requires a simple matrix multiplication with O(nM) complexity.
Similarly, calculating Uwh (t) is also very cheap. In practical implementation, as Uwh (t) do not
change much within a few iterations, we can update Uwh (t) every a few iterations (e.g., every epoch).
We can also replace the VθLwh (θj*(t)) or VθL(θj*(t)) in (13) using a mini-batch of data instead of
the whole data, which leads to a stochastic gradient version of our algorithm. We refer readers to
Algorithm 1 for the ideal updating and Algorithm 2 for the practical implementation in Appendix B
for more details.
4 Theory
Recall that, as discussed in (5), our approach relies on the intuition that bootstrap particles are nested
in a small region so that we can approximate the distance between the centroid and a bootstrap particle
by the bootstrap loss of that centroid. The main goal of this section is to give a formal theoretical
justification of this intuition.
5
Under review as a conference paper at ICLR 2022
Before we proceed, we clarify several important setups for establishing and interpreting the theoretical
result. As discussed in the introduction, we are mainly interested in the scenerio that the number of
available particles/centroids m is small while the number of training data n is large, which motivates
us to establish theoretical result in the region of small m and large n. This is significantly different
from conventional asymptotic analysis in which we aim to show the behavior when m → ∞. We
consider the setting that the parameter dimension d is fixed and does not scale with n.
We are mainly interested in characterizing the approximation of the proposed loss in (6) to the ideal
loss in (3), given any small and fixed number m of centroids when n → ∞. This justifies why the
proposed centroid approximation method can be viewed as minimizing the Wasserstein distance
between the particle distribution Pn and the target bootstrap distribution Pn.
For simplicity, we build our analysis assuming the ideal update rule (8,9) is used. We start with the
following main assumptions.
Assumption 1 (Smoothness and boundedness) Assume that the following quantities are upper
bounded by some constant c < ∞:
1 max SUP	d3'(X，简.2	SUP SUP "V2'(X,fθI)- V2'(X,fθ2 升 .
.i,j,k∈[d]θ∈Θ,X∈X ∂iθi∂θj ∂θk ; . θι,θ2p∈θ x∈X	I∣Θ1-Θ2∣∣	;
3. sup	∣∣V2'(x,fθ)∣∣;	4. sup ∣∣θk .
x∈X,θ∈Θ	θ∈Θ
Assumption 1 is a standard regularity condition on the boundness and smoothness of the problem.
Assumption 2 (Asymptotic normality) Assume √n 伙W 一 θ) → N (0, A) and √n (θ — θ0)-
N (0, A) as n → ∞, where A is a positive-definite matrix with the largest eigenvalue bounded.
Assumption 2 is a higher level assumption on the asymptotic normality of the estimators. Such result
is classic and can be derived with some weak and technical regularity conditions. See examples in
Chatterjee et al. (2005); Cheng et al. (2010).
Assumption 3 (On the global minimizer) Suppose that λmi∏ (V2L∞(θo)) > 0.
Assumption 3 is also standard showing the locally strongly convexity of the loss around the truth θ0.
Assumption 4 (On the learning rate) Suppose that maxt t = O(n-1).
Assumption 4 assumes that the learning rate of the algorithm is sufficiently small such that its induced
discretization error is not the dominating term.
The key challenge of our analysis is to show that our dynamics is B(θ0, r)-stable (defined below in
Definition 1) for some small r, saying that {θ* (t)}m=ι stay in a small region that is close to θo for any
iteration t. Combined with the property6 that θw are also close to θo, the centroids and the bootstrap
particles are close to each other and thus our approximation in (5) holds for all t ≥ 0. In this way,
optimizing the centroids by minimizing our loss is almost equivalent to optimizing the centroids by
minimizing the Wasserstein distance.
Definition 1 (B(θ, r)-stable) Given some θ ∈ Θ and r ≥ 0, we say our dynamics is B(θ, r)-stable
if Vt ≥ 0 and Vj ∈ [m], θ*(t) ∈ B(θ,r), where B(θ,r):
with radius r centered at θ.
{θ0 : ∣θ0 - θ ∣ ≤ r, θ0 ∈ Θ} is the ball
The key intuition to establish such B(θ0, r)-stable result is to characterize that our optimization
dynamics is implicitly self-controlled: when some centroid approaches the boundary of B(θ0, r), the
updating mechanism automatically start to push the centroid to move towards the center of the region.
Thus, if all the centroids are within B(θ0, r) at initialization, they will alway stay in this region.
Thanks to assumption 2, 3, when the dataset is large, the landscape of our loss is locally strongly
convex around θ0. When a centroid j is at the boundary of B(θ0, r), it has v* (t) < Y and thus the
6This is implied by the asymptotic normality in assumption 2.
6
Under review as a conference paper at ICLR 2022
m = 20
m = 50
m = 100
m= 200
α = 0.9
Normal	Bootstrap Centroid
Percentile	Bootstrap Centroid
Pivotal	Bootstrap Centroid
0029 ± 0.010~0.031 ± 0.011 _0021 ± 0.010~0.017 ± 0.010
0.027 ± 0.010	0.001	±	0.009	0.012	±	0.010	0.016	±	0.010
0.101 ± 0.013~0.036 ± 0.011 _0021 ± 0.010~0.014 ± 0.010
0.081 ± 0.012	0.021	±	0.010	0.020	±	0.010	0.015 ± 0.010
0.106 ± 0.013~0.045 ± 0.011 _0025 ± 0.010~0.023 ± 0.010
0.046 ± 0.011	0.013	±	0.009	0.011	±	0.010	0.020	±	0.010
Table 1:	Centroid approximation for confidence interval. The numbers in the table represent ∣α — α∣,
where α is the estimated coverage probability. The errors bar is the standard deviation.
updating direction is the gradient of loss L. By the convexity, such gradient will push the centroid
move towards the center of B(θ0 , r) where the empirical minimizer locates at. On the other hand, for
centroid j with Vj (t) ≥ Y, its updating direction aggregates information from sufficient data point
and thus behaves similarly to that of the common gradient, pushing centroid to move towards the
center with the centroid is not close to the center.
Theorem 1 Under Assumptions 1-4 and suppose that we initialize θjj(0), j ∈ [m] by sampling from
ρπ, given any m < ∞ and γ > 0, when n is sufficiently large, we have
max sup ∣∣θj(t) - θo∣∣ = Op(p(logn)/n).
j∈[m] t≥0
Here the probability is taken w.r.t. training data.
Theorem 1 implies our dynamics is B(θ0, rn)-stable with rn =O(VZlogn/n). The condition that
θj (0)〜Pn i.i.d. can be replaced by the condition that θj (0) is sufficiently close to θ0. We need such
condition as we uniformly bound the distance between θjj(t) and θ0 at any iteration including the first
one. Theorem 1 implies that the approximation stated in (5) holds with high probability and hence
the proposed loss in (6) is ‘almost as good as’ the ideal loss in (3).
Theorem 2 Under the same assumptions as Theorem 1, given any m < ∞ and γ > 0, when n is
sufficiently large, we have
sup Ew〜∏[min Lw(θj(t))] - B — Ew〜∏[min ∣∣θj(t) - θw||D]/2
t≥0	j∈[m]	j∈[m]
Here the probability is taken w.r.t. training data and B is some constant independent from θjj(t) for
any t ≥ 0 and j ∈ [m].
Op(J (log n)/n3/2).
Asymptotics when m also grows Although our main interest is the asymptotics with a small, fixed
m and growing n, we discuss here on asymptotics when m also grows. As shown in Section 3 and
introduction, our method can be viewed as an ‘approximated’ K-means on the target distribution. From
Theorem 5.2 in Canas & Rosasco (2012), the particle distribution formed by the optimal centroids
learned by K-means gives improved O(m-1/d) convergence to any general target distribution in
terms of Wasserstein distance, where d is data dimension. In comparison, the particle distribution of
i.i.d. sample only gives O(m-1/(2d+4)) from Theorem 5.1 in Canas & Rosasco (2012). This implies
that our approach potentially also has such a rate improvement. Note that the results in Canas &
Rosasco (2012) are for general target distribution without any n involves. To rigorously establish the
large m asymptotic result for our problem, we need to study the joint limit of n and m. This is indeed
very non-trivial: as discussed in Weed et al. (2019) (i.e. Proposition 14), when n → ∞, the target
distribution Pπ becomes a sharp Gaussian and the convergence rate of i.i.d. bootstrap particles will
gradually improve to O(m-1/2 ) (in a way that depends on n). It implies that when n m → ∞,
our improvement may become only constant level. We find establishing such a theory is out the scope
of this conference paper and leave it as future work.
5	Experiment
As discussed in the introduction, our main goal is to improve the quality of the particle distribution
when only a limited number of particles/centroids is allowed, so that we can use less particles at
7
Under review as a conference paper at ICLR 2022
		m = 3	m=4	m=5	m= 10
Mushroom	Bootstrap	3282.1 ± 72.82	^^3307.9 ± 69.2	3311.6 ± 79.3	3397.4 ± 51.4
	Centroid	3702.7 ± 89.76	3723.1 ± 78.7	3799.6 ± 84.2	3796.9 ± 36.1
Statlog	Bootstrap	-1864.3 ± 6.4-	-1869.2 ± 5.2-	1877.2 ± 4.1	1877.0 ± 2.7
	Centroid	1893.6 ± 6.0	1892.6 ± 3.6	1891.3 ± 3.5	1892.6 ± 2.8
Financial	Bootstrap	2255.77 ± 58.45	2265.42 ± 58.17	2269.33 ± 56.36	2281.35 ± 56.65
	Centroid	2313.29 ± 56.45	2315.32 ± 56.75	2323.88 ± 56.73	2325.54 ± 56.05
Table 2:	Results on the contextual bandit experiment. The numbers in the table represent the averaged
reward with its standard deviation.
deployment, which reduces the memory consumption and the computational cost for making predic-
tion. Thus, our experiment design will be focusing on comparing the testing performance of vanilla
bootstrap and our centroid based approach when the same and a small number of particles/centroids is
used. We apply our method to four applications: confidence interval construction, bootstrap method
in contextual bandit, bootstrapped deep Q-network and bagging. Due to space limit, we refer to
Appendix C.4 for the bagging experiment, Appendix C.5 for ablation study on the importance of
modifying the gradient to overcome the centroid degeneration phenomenon. Although we are less
interested in the computational cost of training, as discussed in Section 3, our method actually only
introduces a little training computation overhead, which is another advantage of our method. We
draw analysis on this aspect in Appendix C.6.
5.1 B ootstrap Confidence Interval
We start with a classic application of bootstrap:
confidence interval estimation for linear model
with parameter θ. Fix confidence level α, we
consider three ways to construct (two-sided)
bootstrap confidence interval of θ: the Normal
interval, the percentile interval and the pivotal
interval. And we test m = 20, 50, 100, 200. For
all experiments, we repeat with 1000 indepen-
dent random trials. We consider the standard
bootstrap as baseline. Detailed experimental
setup are included in Appendix C.1.
Figure 2: Wasserstein distance between the particle
distribution and the true bootstrap distribution w.r.t.
the number of particles.
Figure 2 shows the Wasserstein distance be-
tween the true target distribution ρπ and the empirical distributions obtained by (a) i.i.d. sampling
ρ∏, (b) the proposed centroid approximation Pn. The centroid approximation significantly reduces
the Wasserstein distance by a large margin. We then compare the quality of obtained confidence
intervals, which is measured by the difference between the estimated coverage probability and the
true confidence level, i.e., ∣α - α∣ (the lower the better). Here We only consider confidence intervals
of the first coordinate of θ: θ1. Table 1 summarizes the result with α = 0.9. We see that using more
particles is generally able to improve the constructed confidence intervals. We also compare with
two variants of standard bootstrap: Bayesian bootstrap (Rubin, 1981) and residual bootstrap (Efron,
1992). And we consider varying α = 0.8, 0.95. These results are included in Appendix C.1.
5.2	Centroid Approximation for Bootstrap Method in Contextual Bandit
Contextual bandit is a classic task in sequential decision making, in which accurately quantifying
the model uncertainty is important in order to achieve good exploration-exploitation trade-off. As
shown in Riquelme et al. (2018), tracking the model uncertainty using bootstrap is a strong method
for contextual bandit. However, it is costly to maintain a large number of bootstrap models and thus
the number of models is typically within 10 (Osband et al., 2016). We find that applying the proposed
centroid approximation here can significantly improve the performance. Riquelme et al. (2018)
uses m = 3 bootstrap models and we give a more comprehensive evaluation with m = 3, 4, 5, 10.
We consider three datasets: Mushroom, Statlog and Financial. We set γ = 0.5/m. We randomly
generate 20 different context sequences, apply all the methods and report the averaged cumulative
reward and its standard deviation. Table 2 summarizes the result and note that a large part of variance
8
Under review as a conference paper at ICLR 2022
PJBMeωAEωulΛ0n
-IOO
100	150	200	250	300
Episodes
350	400
P*ΛV9Jβ0ABβoUIΛ0n
O O
3 2
W O
30	40	50	60	70	80	90
Episodes
Figure 3: Results for Bootstrap DQN with centroid approximation experiment. Left: LunarLander-v2;
Right: Catcher-v0.
can be explained by different context sequences. All results in Table 2 are statistically significant
under significant level 5% using matched pair t-test. Table 2 shows that using more bootstrap models
generally improves the accumulated reward. And when using the same number of models, the
proposed centroid approximation method consistently improves over standard bootstrap method. We
refer readers to appendix C.2 for more information on the background and experiment.
5.3	Centroid Approximation for Bootstrap DQN
“Efficient exploration is a major challenge for reinforcement learning (RL). Common dithering strate-
gies such as -greedy do not carry out temporally-extended exploration, which leads to exponentially
larger data requirements” (Osband et al., 2016). To tackle this issue, Osband et al. (2016) proposes
the Bootstrapped Deep Q-Network (DQN). We apply our centroid approximation to improve Boot-
strapped DQN. We consider m = 2, 5 and similar to the experimental setting in contextual bandit,
we set γ = 0.5/m. We consider two benchmark environments: LunarLander-v2 and Catcher-v0
from GYM (Brockman et al., 2016) and PyGame learning environment (Tasfi, 2016). We conduct
the experiment with 5 independent random trails and report the averaged result with its standard
deviation. We refer readers to Appendix C.3 for more background and other experiment details.
Figure 3 summarizes the result. For LunarLander-v2, Bootstrap DQN with 2 and 5 heads give similar
performance but both converge to a less optimal model compared with the centroid approximation
method. Centroid approximation method with 2 and 5 heads performs similarly at convergence but
centroid approximation method with 5 heads is able to converge faster than 2-head model and thus
has lower regret. For Catcher-v0, adding more heads to the model is able to improve the performance
for both methods. The proposed centroid approximation consistently improves over baselines.
6	Related Work
Bootstrap is an classical statistical inference method, which was developed by Efron (1992) and
generalized by, i.e., Mammen (1993); Shao (2010); Efron (2012) (just to name a few). Bootstrap can
be widely applied to various statistical inference problem, such as confidence interval estimation
(DiCiccio et al., 1996), model selection (Shao, 1996), high-dimensional inference (Chen et al., 2018b;
El Karoui & Purdom, 2018; Nie & Rockova, 2020), off-policy evaluation (Hanna et al., 2017),
distributed inference (Yu et al., 2020) and inference for ensemble model (Kim et al., 2020), etc.
Despite its wide applications and nice theoretical properties, there has been very few works on
discussing and improving the approximation efficiency in the region of small bootstrap sample size,
beyond the i.i.d. sampling paradigm. While the m-out-of-n bootstrap (Bickel et al., 2012) and the
bag of little bootstrap (Kleiner et al., 2014) are designed to reduce the computational cost with the
subsampling techniques in the big data settings (large n), they still require a large bootstrap sample
size and thus are still not scalable to large deep learning applications.
Bayesian Inference is a different approach to quantify the model uncertainty. Different from
frequentists’ method, Bayesian assumes a prior over the model and the uncertainty can be captured by
the posterior. Bayesian inference have been largely popularized in machine learning, largely thanks
to the recent development in scalable sampling method (Welling & Teh, 2011; Chen et al., 2014;
Seita et al., 2018; Wu et al., 2020), variational inference (Blei et al., 2017; Liu & Wang, 2016), and
other approximation methods such as Gal & Ghahramani (2016); Lee et al. (2018). In comparison,
9
Under review as a conference paper at ICLR 2022
bootstrap has been much less widely used in modern machine learning and deep learning. We believe
this is largely attributed to the lack of similarly efficient computational methods in the small sample
size m region, which is the very problem that we aim to address with our new centroid approximation
method.
Uncertainty in Deep Learning In additional to the applications considered in this paper, uncertainty
in deep learning model can also be applied to problems including calibration (Guo et al., 2017)
and out-of-distribution detection (Nguyen et al., 2015). The definition of uncertainty of neural
network is quite generalized (e.g., Gal & Ghahramani (2016); Ovadia et al. (2019); Maddox et al.
(2019); Van Amersfoort et al. (2020)) and can be quite different from the uncertainty that bootstrap
inference want to quantify and can be approached with various methods including drop out (Gal &
Ghahramani, 2016; Durasov et al., 2020), label smoothing (Qin et al., 2020), designing new modules
in the model (Kivaranovic et al., 2020), adversarial training (Lakshminarayanan et al., 2017) and
Bayesian modeling (Blundell et al., 2015), etc. This paper focuses on improving the bootstrap method
and thus is orthogonal to those previous works. Pearce et al. (2018); Salem et al. (2020) also try to
refine the ensemble models to improve the quality of prediction interval. Compare with our method,
their method can only be applied to prediction interval and does not have theoretical guarantee.
7	Conclusion
We propose a centroid approximation method to learn an improved particle distribution that better
approximates the target bootstrap distribution, especially in the region with small particle size.
Theoretically, when the size of training data is large, our objective function is surrogate to the
Wasserstein distance between the particle distribution and target distribution. Thus, compared with
standard bootstrap, the proposed centroid approximation method actively optimizes the distance
between particle distribution and target distribution. The proposed method is simple and can be
flexibly used for applications of bootstrap with negligible extra computational cost.
10
Under review as a conference paper at ICLR 2022
References
Morgane Austern and Vasilis Syrgkanis. Asymptotics of the empirical bootstrap method beyond
asymptotic normality. arXiv preprint arXiv:2011.11248, 2020.
Peter J Bickel, Friedrich Gotze, and Willem R van Zwet. Resampling fewer than n observations:
gains, losses, and remedies forlosses. In Selected Works ofWillem van Zwet, pp. 267-297. Springer,
2012.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American statistical Association, 112(518):859-877, 2017.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622. PMLR, 2015.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Trevor Campbell and Boyan Beronov. Sparse variational inference: Bayesian coresets from scratch.
Advances in Neural Information Processing Systems, 32:11461-11472, 2019.
Guillermo Canas and Lorenzo Rosasco. Learning probability measures with respect to optimal
transport metrics. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances
in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.
Snigdhansu Chatterjee, Arup Bose, et al. Generalized bootstrap for estimating equations. The Annals
of Statistics, 33(1):414-436, 2005.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International conference on machine learning, pp. 1683-1691. PMLR, 2014.
Wilson Ye Chen, Lester Mackey, Jackson Gorham, FrancOiS-Xavier Briol, and Chris Oates. Stein
points. In International Conference on Machine Learning, pp. 844-853. PMLR, 2018a.
Xiaohui Chen et al. Gaussian and bootstrap approximations for high-dimensional u-statistics and
their applications. The Annals of Statistics, 46(2):642-678, 2018b.
Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. arXiv preprint
arXiv:1203.3472, 2012.
Guang Cheng, Jianhua Z Huang, et al. Bootstrap consistency for general semiparametric m-estimation.
Annals of Statistics, 38(5):2884-2915, 2010.
Sebastian Claici, Aude Genevay, and Justin Solomon. Wasserstein measure coresets. arXiv preprint
arXiv:1805.07412, 2018.
Thomas J DiCiccio, Bradley Efron, et al. Bootstrap confidence intervals. Statistical science, 11(3):
189-228, 1996.
Nikita Durasov, Timur Bagautdinov, Pierre Baque, and Pascal Fua. Masksembles for uncertainty
estimation. arXiv preprint arXiv:2012.08334, 2020.
Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pp.
569-593. Springer, 1992.
Bradley Efron. Bayesian inference and the parametric bootstrap. The annals of applied statistics, 6
(4):1971, 2012.
Noureddine El Karoui and Elizabeth Purdom. Can we trust the bootstrap in high-dimensions? the
case of linear models. The Journal of Machine Learning Research, 19(1):170-235, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059.
PMLR, 2016.
11
Under review as a conference paper at ICLR 2022
Alex Graves. Practical variational inference for neural networks. In Advances in neural information
processing Systems, pp. 2348-2356. Citeseer, 2011.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017.
Peter Hall et al. Rate of convergence in bootstrap approximations. The Annals of Probability, 16(4):
1665-1684, 1988.
Josiah Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Confidence intervals for
off-policy evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31,
2017.
Botao Hao, Yasin Abbasi Yadkori, Zheng Wen, and Guang Cheng. Bootstrapping upper confidence
bound. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019.
Jiri Hron, Alexander G de G Matthews, and Zoubin Ghahramani. Variational gaussian dropout is not
bayesian. arXiv preprint arXiv:1711.02989, 2017.
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.
Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representations,
2017.
Byol Kim, Chen Xu, and Rina Foygel Barber. Predictive inference is free with the jackknife+-after-
bootstrap. Advances in Neural Information Processing Systems, 33, 2020.
Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction
intervals for deep networks. In International Conference on Artificial Intelligence and Statistics,
pp. 4346-4356. PMLR, 2020.
Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, and Michael I Jordan. A scalable bootstrap
for massive data. Journal of the Royal Statistical Society: Series B: Statistical Methodology, pp.
795-816, 2014.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. Advances in neural information processing systems,
30, 2017.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on
Learning Representations, 2018.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. Advances in Neural Information Processing Systems, 29, 2016.
Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson.
A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information
Processing Systems, 32:13153-13164, 2019.
Enno Mammen. Bootstrap and wild bootstrap for high dimensional linear models. The annals of
statistics, pp. 255-285, 1993.
Benedict C May, Nathan Korda, Anthony Lee, and David S Leslie. Optimistic bayesian sampling in
contextual-bandit problems. Journal of Machine Learning Research, 13:2069-2106, 2012.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 427-436, 2015.
Lizhen Nie and Veronika Rockova. Bayesian bootstrap spike-and-slab lasso. arXiv preprint
arXiv:2011.14279, 2020.
12
Under review as a conference paper at ICLR 2022
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing Systems, 29:4026-4034, 2016.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems, 32:
13991-14002, 2019.
Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. High-quality prediction intervals
for deep learning: A distribution-free, ensembled approach. In International Conference on
Machine Learning, pp. 4075-4084. PMLR, 2018.
Yao Qin, Xuezhi Wang, Alex Beutel, and Ed H Chi. Improving uncertainty estimates through the
relationship with adversarial robustness. arXiv preprint arXiv:2006.16375, 2020.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. In International Conference on
Learning Representations, 2018.
Donald B Rubin. The bayesian bootstrap. The annals of statistics, pp. 130-134, 1981.
Tarik S Salem, Helge Langseth, and Heri Ramampiaro. Prediction intervals: Split normal mixture
from quality-driven deep ensembles. In Conference on Uncertainty in Artificial Intelligence, pp.
1179-1187. PMLR, 2020.
Daniel Seita, Xinlei Pan, Haoyu Chen, and John Canny. An efficient minibatch acceptance test
for metropolis-hastings. In Proceedings of the 27th International Joint Conference on Artificial
Intelligence, pp. 5359-5363, 2018.
Jun Shao. Bootstrap model selection. Journal of the American statistical Association, 91(434):
655-665, 1996.
Xiaofeng Shao. The dependent wild bootstrap. Journal of the American Statistical Association, 105
(489):218-235, 2010.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Norman Tasfi. Pygame learning environment. https://github.com/ntasfi/
PyGame-Learning-Environment, 2016.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690-9700. PMLR, 2020.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.
Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, and Theodore L
Willke. Out-of-distribution detection using an ensemble of self supervised leave-out classifiers. In
Proceedings of the European Conference on Computer Vision (ECCV), pp. 550-564, 2018.
Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &
Business Media, 2013.
Jonathan Weed, Francis Bach, et al. Sharp asymptotic and finite-sample rates of convergence of
empirical measures in wasserstein distance. Bernoulli, 25(4A):2620-2648, 2019.
13
Under review as a conference paper at ICLR 2022
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings ofthe 28th international conference on machine learning (ICML-11), pp. 681-688.
Citeseer, 2011.
Tung-Yu Wu, YX Rachel Wang, and Wing H Wong. Mini-batch metropolis-hastings with reversible
sgld proposal. Journal of the American Statistical Association, pp. 1-9, 2020.
Jeremy Wyatt. Exploration and inference in learning from reinforcement. 1998.
Yang Yu, Shih-Kang Chao, and Guang Cheng. Simultaneous inference for massive data: Distributed
bootstrap. In International Conference on Machine Learning, pp. 10892-10901. PMLR, 2020.
14
Under review as a conference paper at ICLR 2022
A	Proof
We also show Theorem 3, which gives a formal characterization of the Taylor approximation intuition
introduced in (5).
Theorem 3 Under Assumption 1 and 2, when n is sufficiently large, we have
Lw(θ) = Lw(θw) + 2 (θ — θw)τ V2L∞(θo) (θ - θw)
+Op (l∣θ - -||"1/2 + l∣θ - Gw||)).
Here the stochastic boundedness is taken w.r.t. the training data and w.
In the proof, we may use c to represent some absolute constant, which may vary in different lines.
A.1 Proof of Theorem 3
With the fact that VwL(θw) = 0 and under assumption 1, using Taylor expansion, we have
Lw(θ) = Lw(θw) + 2 (θ — ^w)τ V2Lw(θw) (θ — θw) + O ɑθ — θwllɔ .
Notice that
(θ — θw)> ELw(θw) — V2L∞(θo)) (θ — θw)
≤ ∣∣θ — θw∣∣2∣∣v2 Lw (θw)— v2l∞(Θo)∣∣
≤ ∣∣θ — θw∣∣2 (∣∣vθLwew)- V2Lw(θo)∣∣ + ∣∣v2Lw(θo) — V2L∞(θo)∣∣)
≤∣∣θ — θw∣∣2 (c∣∣θw — θo∣∣ + ∣∣v2Lw(θo) — v2l∞(Θo)∣∣f),
where We denote the Frobenius norm as |卜|恨.With assumption 2, We have ∣∣θw — θo∣∣ = Op(n-1/2).
By applying centroid limit theorem and delta method to vθ2 Lw(θ0) — vθ2 L∞(θ0) for every pair
i, j ∈ [d], We have ∣vθ2Lw(θ0) — vθ2L∞(θ0)∣F = Op(n-1/2). Thus We obtained the desired result.
A.2 Proof of Theorem 1
Given any radius r and > 0, With sufficiently large n, We have
P (∣/w — θo∣∣ ≥ r) ≤ exp(—λnr2)+e∕m,
for λ = 4λmaχ(A)-1. Here the probability is the jointly probability of bootstrap weight and training
data. Thus, given any r, under the assumption that θj (0) is initialized via sampling Gw, then we have
P (∪j∈[m] {∣∣θj(0) — θo∣∣ ≥ r}) ≤ X P (∣∣θj(0) — θo∣∣ ≥ r) ≤ mexp(一1nr2) + e.
j∈[m]
We proof by induction. Given any {Gj }jm=1, define
Rk,r = I {w ∈ supp(π) : arg min Lw(θj) = k and ∣∣θw — θo∣∣ ≤ r}.
Suppose at iteration t, We have ∣∣θ^(t) — θok ≤ CoLaλYn~ for some constant C and λo, which we
denote as the minimum eigenvalue of vθ2L∞(fθ0). Now at iteration t, we have two cases.
15
Under review as a conference paper at ICLR 2022
Case1: E∏ Rk,∞ ≥ Y Suppose that at iteration t, for k such that E∏Rk,∞ ≥ γ,and ∣∣θfc(t) - θ0k =
qk, we have the following property:
2
%(t + 1) - θ0∣2 =优(t) - -ʌ-E∏ [VθLw(优(t))Rk,∞] - θ0
Eπ Rk,∞
=%(t) - θ0『一ER-四(t) - θ0, Eπ [VθLw(ffk(t))Rk,∞]i + e2 ∣E∏ [VθLw(玲(t))Rk,∞U∣2.
Eπ Rk,∞
Notice that
E∏ [VθLw (θk (t))Rk,qk ] = E∏[v2Lw (θw )(θk(t) - θw )Rk,qk ] + o 显)
=E∏ [v2Lw(θw)(θk(t) - θ0)Rk,qk] + O (q2)
=E∏ [V2Lw(θ0)(θk(t) - θ0)Rk,qJ + O (q2)
Here (1) is obtained Via applying Taylor expansion on VθLw(θk(t)) at θw. (2) is by assumption 1
and 2. (3) is by assumption 1. We thus have
-hθk(t) - θ0, E∏ [VθLw(θk(t))Rk,∞]i
≤ - hθk(t) - θ0,E∏ [VθLw(θk(t))Rk,qJi + ∣θk(t) - θ0∣∣E∏VθLw(θk(t))(1 - Rk,qk)∣
≤ - hθk(t) - θ0, Eπ [Ve Lw (θk(t))Rk,qk]i + cqk exP(-λnqfc)
≤ - EnRk,qk (θk(t) - θ0)>V2Lw (θ0)(θk(t) - θ0) + Cqk exp(-λnq2) + O (点).
Notice that with sufficiently large n, with central limit theorem, we have
-EnRk,qk(θk(t) - θ0KLw(θ0)(θk(t) - θ0)
≤ ∣θk(t) - θ0∣2 En ∣∣V2Lw(θ0) - VθLw(θ0)∣∣ - EnRk,qk(θk(t) - θ°)>V2L∞(θ0)(θk(t) - θ0)
=-EnRk,qk(θk(t) - θ0)>V2L∞(θ0)(θk(t) - θ0) + O(nT∕2).
This gives that
-hθk(t) - θ0,En [VθLw(θk(t))Rk,∞]i
≤ - En Rk,qk (θk(t) - θ0)> vΘ L∞(θ°)(θk(t) - θ0) + Cqk eχp(-λnq2) + O (q3 + qk n-1/2)
≤ - λ0EnRk,qk kθk(t) - θ0∣2 + Cqk exp(-λnq2) + O (q3 + qkn-1/2).
Use the above estimation, we have
∣θk(t + 1) - θ0k2
≤ ∣θk(t) - θ0『一2qλmin EnRkqk ∣θk (t) - θ0∣2
En Rk,∞
+2Cetqk exp(-λnq2)∕EnRk,∞ + O Q(q3 + qkn-1/2)/EnRk,∞ + e2)
≤ llθk(t) -	θ0∣2	+ 雨	St	(-2λ0EnRk,qk	∣∣θk(t)	-	θ0∣∣2	-	2cqk eXp(-λnq2)	+ o (q3 + et	+ qkn-1/2)).
En Rk,∞
Notice that by choosing α > ,1∕(2λ) and et = O(n-1), with sufficiently large n, when
I∣θk(t)-θ0∣∣≥
Ca产
λ0En Rk,qk
for some constant C, we have
∣θk(t + 1)-θ0k≤∣∣θk(t)-θ0∣∣.
Thus ∣θkk (t + 1) - θ0 ∣ ≤
log n
Ca -g-
Ca √1⅞n
λ0Y
for some constant C.
λ0Eπ Rk,∞
≤
16
Under review as a conference paper at ICLR 2022
Case 2: E∏Rk,∞ ≤ Y In this case, we have
ιιθfe(t + i) - θ0k2 = ∣∣θfe (t) - e⅛Vθ £(%(t))一 明
=慨⑴一出『一2et 修⑴一θo, V/M(t)))+ e2 ∣∣VeL(M(t))∣∣2 .
Notice that
-h 以 t)-θ0, V4(4(t))) ≤ -(僦⑴一θo, V2L(θo)(玲⑴一出)〉一(。以 t)-θo, VθL(⅛, ))+。(|| 僦⑴一出||3)
=一(傀(t) 一 θ0)> V2L∞(θ0- - θ0) +。(||僦⑴ 一 出||3) + OP(n-i∕2)%⑴一出||.
This gives that
%(t + 1) - θ0k2 ≤ 止(t) 一。0『一2公0 I⑹(t) 一。0『+。(可慰(t)"||3+e2)+Op(nT/2)et幌(t)"||.
With ∈t = O(n-1) and sufficiently large n, when
Z察
懊⑴一电后
λ0γ
wehave ∣∣优(t + 1) —。。『≤∣∣就⑴一如匕
Combine this two cases, we conclude that Iw(t + 1) 一 θ0∣∣ ≤ CcsYr for any t, when
k就(0) 一。0Il ≤ cc(Yn- . We thus conclude that, for any a > ,1∕(2λ) and e > 0, when n
is sufficiently large, with probability at least 1 一 m exp (一1 喋％ n ) - e, we have
max sup ∣∣θ*(t) — θ0∣∣ ≤
j∈[m] t
Ca产
λ0γ
A.3 Proof for Theorem 2
Notice that
Lw (%(t)) - Lw (θw) = 2 (θ* (t) — θw )> v25公)(%(t) - θw) + o( ∣ ∣ 吗⑴ 一 θw∣∣3)
=2 (θ* (t) — θw )> v2Lw(θ0) (%(t) — θw) + o( ∣ ∣%(t) 一 θw∣∣3) + o(∣∣%(t) 一 θw∣∣2 ∣∣θw — θ0∣∣)
=1 ∣jt) 一 θw∣∣: + ∣∣v2Lw (θ0) — v2l∞(Θ0)∣∣ ∣jt) - θw∣∣2 + O(∣jt) - θw∣∣3)
+ o( ∣ ∣ θ* (t) — 0w∣∣2∣∣θw 一 θ0∣∣)
Given w, we define uw = argmin7-∈[m] ∣∣θ*(t) 一 θw∣∣ . For any a > ,1∕(2λ) and e > 0, when n
is sufficiently large, with probability at least 1 一 m exp (一1 °12% n ) - e, we have
2 Ew 〜∏
=2Ew〜π
*∣∣θ;⑴一 θw∣∣D
Ww- "fl
≥Ew 〜∏Kw(θ 乙)-Lw	(θw —cWw	-	θw∣∣2	(∣pw - θ0∣∣ + ∣∣vθLw(θ0) - v2l∞(Θ0)∣∣ + ∣∣θuw	-	θw∣∣)
，〜π
[Lw (θUw)] - Ew-∏ [Lw (θw)] - c
Q ʌ/log n
λ0γn3/2
=Ew ~π LmY Lw(θ;⑴)
—
hLw (θw)] 一
C
Q ʌ/log n
λ0γn3/2
17
Under review as a conference paper at ICLR 2022
Algorithm 1 Ideal algorithm for centroid approximation with full-batch gradient used and wh
updated every iteration.
1:	Initialize θj (0), j ∈ [m] by i.i.d. sampling from ρ∏ or other distribution such as Gaussian.
2:	for t ∈ iterations do
3:	∀j ∈ [m], calculate L(θj (t)) defined in (11)
4:	Sample {wh}hM=1, i.i.d. from π.
5:	∀h ∈ [M] and j ∈ [m], calculate Lwh (θ*(t)) = WhL(θ*(t)).
6:	∀h ∈ [M], calculate uwh defined in (12) for each h.
7:	∀j ∈ [m], update θj by (13).
8:	end for
Similarly, we also have, with probability at least 1 - m exp (-λ '12% n
Ew〜∏ min Lw(θj(t))
j∈[m]
Notice that the above bound holds uniformly for all j ∈ [m] and any iteration t, which implies that
with probability at least 1 - 2m exp (-λ c0^；' n ) - 2e, We have
sup Ew〜∏[min Lw⑼⑴)]-B - Ew〜∏[min |怛：(力)-θw||D]/2
t≥0	j∈[m]	j ∈[m]
-Ew 〜∏ [Lw(θW)] ≥ 2 Ew 〜∏
始归⑴-θw∣∣
(α √logn、
-cUθγn3/2	.
≤c
α ʌ/log n
λoγn3/2
2
D
B Algorithm Box
We provide pseudo algorithm for the ideal centroid approximation algorithm in Algorithm 1. In
practical implementation, we do not need to update wh every iteration and can also replace the
full-batch gradient by stochastic gradient. Specifically, notice that g(θ*) defined in (10) can be
alternative represented as
g(θj(t))
PM=1 Pn=I [I{j ∈ Uwh (t)}] Wh,"θ'(Mfθj (t))/n
PM=I [I{j ∈ uwh }]
1n
nW%,, Vθ '(Xi,fθj(Q,
i=1
(14)
where qi,j is defined by
Cl .	PM=I Pn=I mo ∈ Uwh-H wh“	.
j —PM=1 [I{j ∈ Uwh (t)}]一.	()
This allows us to use a stochastic gradient version of gradient
OSgd(S)= IBBT X qi,jvθ'(xi, fθj ),	(16)
i∈[B]
where B is the set of mini-batch data. The detailed algorithm is summarized in Algorithm
C Additional Experiment details
C.1 Bootstrap Confidence Interval
Given a model fθ parameterized by θ and a training set with n data points i.i.d. sampled from
population, our goal is to construct confidence interval for θ. Let ρ∏ be an empirical distribution
approximating ρπ, which could be obtained by i.i.d. sampling, or by our centroid method. Denote
by Q[α, p∏] the α-quantile function of p∏ with some α ∈ [0,1]. We consider the following three
ways to construct (two-sided) bootstrap confidence interval of θ with confidence level α: the Normal
interval, the percentile interval and the pivotal interval which are defined below.
18
Under review as a conference paper at ICLR 2022
Algorithm 2 Practical implementation of centroid approximation with less frequent updating of wh
and stochastic gradient enabled.
1:	Initialize θj (0), j ∈ [m] by i.i.d. sampling from ρ∏ or other distribution such as Gaussian.
2:	for t ∈ iterations do
3:	// Update wh only every a few iterations.
4:	if t mod freq == 0 then
5:	∀j ∈ [m], calculate L(θj (t)) defined in (11)
6:	Sample {wh}hM=1, i.i.d. from π .
7:	∀h ∈ [M] and j ∈ [m], calculate Lwh(θ*(t)) = WTL(θ*(t)).
8:	∀h ∈ [M], calculate uwh (t) defined in (12) for each h.
9:	else
10:	uwh (t) = uwh (t - 1)
11:	end if
12:	∀j ∈ [m], update θj (t) by (13). (May use mini-batch gradient defined in (16)).
13:	end for
Methods to construct confidence interval The methods we used to construct confidence interval
are - The Normal interval:
■个	,,	....	O	,,	....	、
[θ - Z((I + a)/2)seboot, θ + Z((I + a)/2)seboot],
where z(∙) is the inverse cumulative distribution function of standard Normal distribution. And seɔoot
is the standard deviation estimated from ρ∏.
一 The percentile intervals:
[Q[(1-α)∕2,ρ∏ ],Q[(1 + α)∕2,ρ∏ ]].
一 The pivotal interval:
・ O	一一	.	.	O	一一	.	.	rr
[2θ - Q[(1 + α)∕2,ρ∏], 2θ - Q[(1 - α)∕2,ρ∏]].
We consider the following simple linear regression: X 〜N(0, I), y | X 〜N(θ>x, I), where the
features x ∈ R4 and we set the true parameter to be θ0 = [1, -1, 1, -1]. We consider n = 50 and the
number of particles m = 20, 50, 100, 200. We compare the coverage probability and the confidence
level α to measure the quality:
Measuring the quality of confidence interval With a large number N of independently generated
training data (we use N = 1000), we are able to obtain the corresponding confidence intervals
{CI(α)s}sN=1and thus obtain the probability that the true parameter falls into the confidence intervals,
which is the estimated coverage probability
1N
α = NN ∑I{θ0 ∈ CI(α)s}.
s=1
A good confidence interval should have α close to a. Thus We measure the performance by calculating
the difference ∣α - α∣.
As θw is the least square estimator of the bootstrapped dataset, it has analytic solution and thus can
be obtained via some matrix multiplications. θw is initialized using θw and then updated for 2000
steps. For this experiment, we find that adding the threshold γ does not gives further improvement
for this experiment and thus we simply set γ = 0 and use M = 1. We approximate the true bootstrap
distribution by sampling 10000 i.i.d. samples.
More experiment result Table 3 all the result we have varying α = 0.8, 0.9, 0.95, m =
20, 50, 100, 200 and three different approaches for constructing confidence interval. As we can
see, centroid approximation gives the best performance in most cases compared with the other three
baselines.
19
Under review as a conference paper at ICLR 2022
Num Particle				20	50	100	200
			Bootstrap	0.033 ± 0.013	0.028 ± 0.013	0.026 ± 0.013	0.031 ± 0.013
		Normal	Bayesian	0.084 ± 0.014	0.076 ± 0.014	0.082 ± 0..014	0.086 ± 0.014
			Residual	0.033 ± 0.013	0.037 ± 0.013	0.029 ± 0.013	0.024 ± 0.013
			Centroid	0.036 ± 0.013	0.003 ± 0.012	0.017 ± 0.013	0.030 ± 0.013
			Bootstrap	0.096 ± 0.014	0.050 ± 0.014	0.044 ± 0.013	0.024 ± 0.013
α	= 0.8	PerCentile	Bayesian Residual	0.114 ± 0.015 0.079 ± 0.014	0.079 ± 0.014 0.032 ± 0.013	0.074 ± 0.014 0.017 ± 0.013	0.071 ± 0.014 0.010 ± 0.013
			Centroid	0.066 ± 0.014	0.008 ± 0.013	0.019 ± 0.013	0.020 ± 0.013
			Bootstrap	0.101 ± 0.015	0.053 ± 0.014	0.045 ± 0.014	0.033 ± 0.013
		Pivotal	Bayesian	0.158 ± 0.015	0.110 ± 0.110	0.088 ± 0.014	0.078 ± 0.014
			Residual	0.087 ± 0.014	0.044 ± 0.013	0.030 ± 0.013	0.023 ± 0.013
			Centroid	0.026 ± 0.013	0.030 ± 0.012	0.018 ± 0.013	0.030 ± 0.013
			Bootstrap	0.029 ± 0.010	0.031 ± 0.011	0.021 ± 0.010	0.017 ± 0.010
		Normal	Bayesian	0.076 ± 0.012	0.054 ± 0.011	0.048 ± 0.011	0.045 ± 0.011
			Residual	0.043 ± 0.011	0.023 ± 0.010	0.025 ± 0.010	0.020 ± 0.010
			Centroid	0.027 ± 0.010	0.001 ± 0.009	0.012 ± 0.010	0.016 ± 0.010
			Bootstrap	0.101 ± 0.013	0.036 ± 0.011	0.021 ± 0.010	0.014 ± 0.010
α	= 0.9	Percentile	Bayesian Residual	0.129 ± 0.013 0.098 ± 0.013	0.077 ± 0.012 0.030 ± 0.011	0.059 ± 0.012 0.033 ± 0.011	0.054 ± 0.011 0.025 ± 0.010
			Centroid	0.081 ± 0.012	0.021 ± 0.010	0.020 ± 0.010	0.015 ± 0.010
			Bootstrap	0.106 ± 0.013	0.045 ± 0.011	0.025 ± 0.010	0.023 ± 0.010
		Pivotal	Bayesian	0.149 ± 0.014	0.093 ± 0.012	0.073 ± 0.012	0.056 ± 0.011
			Residual	0.100 ± 0.013	0.044 ± 0.011	0.030 ± 0.011	0.023 ± 0.010
			Centroid	0.046 ± 0.011	0.013 ± 0.009	0.011 ± 0.010	0.020 ± 0.010
			Bootstrap	0.018 ± 0.008	0.014 ± 0.008	0.012 ± 0.008	0.006 ± 0.007
		Normal	Bayesian	0.053 ± 0.010	0.038 ± 0.009	0.031 ± 0.009	0.037 ± 0.009
			Residual	0.036 ± 0.009	0.019 ± 0.008	0.011 ± 0.008	0.008 ± 0.007
			Centroid	0.018 ± 0.008	0.005 ± 0.006	0.009 ± 0.007	0.005 ± 0.007
			Bootstrap	0.081 ± 0.010	0.047 ± 0.009	0.030 ± 0.008	0.017 ± 0.008
α=	0.95	Percentile	Bayesian Residual	0.126 ± 0.012 0.100 ± 0.011	0.072 ± 0.010 0.040 ± 0.009	0.056 ± 0.010 0.037 ± 0.009	0.042 ± 0.009 0.021 ± 0.008
			Centroid	0.077 ± 0.010	0.029 ± 0.008	0.020 ± 0.008	0.016 ± 0.008
			Bootstrap	0.089 ± 0.011	0.043 ± 0.009	0.027 ± 0.008	0.015 ± 0.008
		Pivotal	Bayesian	0.127 ± 0.012	0.091 ± 0.011	0.064 ± 0.010	0.056 ± 0.010
			Residual	0.085 ± 0.011	0.051 ± 0.009	0.036 ± 0.009	0.029 ± 0.008
			Centroid	0.046 ± 0.009	0.002 ± 0.007	0.014 ± 0.008	0.009 ± 0.007
Table 3: Complete result on comparing centroid approximation with various bootstrap methods. The
bold number shows the best approach.
20
Under review as a conference paper at ICLR 2022
Algorithm 3 Algorithm for Centroid Approximation Applied to Contextual Bandit.
1:	Obtain a randomly initialized θj (0), j ∈ [m].
2:	Initialize a common replay buffer Rc = 0 recording all the observed contexts.
3:	For each model, initialize its own replay buffer Rj = 0 that is used for training.
4:	for t ∈ number of total steps do
5:	Obtain the t-th context xt.
6:	Sampling one model based on probability {v* (t)}m=ι to make action at and get reward r>
7:	Update the common replay buffer by Rc J Rc ∪ {(xt, at, rt)}
8:	// Update wh and Rj and model every a few iterations.
9:	if t mod freq == 0 then
10:	∀j	∈	[m],	calculate L(θj(t))	defined in (11) for all the contexts in	Rc.	// L(θ*(t))	∈	R1Rcl.
11:	Generate M sets of random weights {wh}hM=1 of contexts in Rc from π.
12:	∀h ∈ [M] and j ∈ [m], calculate Lwh(θj(t)) = WTL(θ*(t)).
13:	∀h ∈ [M], calculate uwh (t) defined in (12) for each h.
14:	∀i ∈ [|Rc|] and j ∈ [m], calculate qi,j by (15)
15:	∀j	∈	[m],	update Vj (t) based on (7).
16:	∀j	∈	[m],	if vj(t) ≤ γ, construct Rj	= Rc, else,	construct Rj by sample	|Rc| contexts
in Rc. The probability that context i is being sampled is qi,j / P|iR=c1| qi,j.
17:	∀j ∈ [m], train model j using the data in Rj for several iterations.
18:	end if
19:	end for
C.2 Centroid Approximation for Bootstrap Method in Contextual Bandit
C.2. 1 More background
Contextual bandit is a classic task in sequential decision making problem in which at time t = 1, ..., n,
a new context xt arrives and is observed by an algorithm. Based on its internal model, the algorithm
selects an actions at and receives a reward rt(xt, at) related to the context and action. During this
process, the algorithm may update its internal model with the newly received data. At the end of
this process, the cumulative reward of the algorithm is calculated by r = Ptn=1 rt and the goal for
the algorithm is to improve the cumulative reward r. The exploration-exploitation dilemma is a
fundamental aspect in sequential decision making problem such as contextual bandit: the algorithm
needs to trade-off between the best expected action returned by the internal model at the moment
(i.e., exploitation) with potentially sub-optimal exploratory actions. Thompson sampling (Thompson,
1933; Wyatt, 1998; May et al., 2012) is an elegant and effective approach to tackle the exploration-
exploitation dilemma using the model uncertainty, which can be approached with various methods
including Bayesian posterior (Graves, 2011; Welling & Teh, 2011), dropout uncertainty (Srivastava
et al., 2014; Hron et al., 2017) and Bootstrap (Osband et al., 2016; Hao et al., 2019). The ability
to accurately assess the uncertainty is a key to improve the cumulative reward. Bootstrap method
for contextual bandit maintains m bootstrap models trained with different bootstrapped training set.
When conducting an action, the algorithm uniformly samples a model and then selects the best action
returned by the sampled model.
C.2.2 More experiment setup details
We set all the experimental setting including data preprocessing, network architecture and training
pipeline exactly the same as the one used in Riquelme et al. (2018) and adopt its open source code
repository.
Network architecture Following Riquelme et al. (2018), we consider a fully connected feed forward
network with two hidden layers with 50 hidden units and ReLU activations. The input and output
dimensions depends on the dimension of context and number of possible actions.
Training For each dataset, we randomly generate 2000 contexts, and for each algorithm, we update
the replay memory buffer for each model every 50 contexts, and each model is also updated every
50 contexts. For the standard bootstrap, when updating the replay buffer of each model, we sample
50 i.i.d. contexts with uniform probability from the latest 50 contexts (each model have different
21
Under review as a conference paper at ICLR 2022
realizations of the samples) and add the newly sampled contexts to each model’s replay buffer. For
the centroid approximation, we update the replay buffer of each model by applying resampling on all
the observed contexts up to the current steps. The resampling probability of each context for each
model is different and determined by the algorithm. We refer readers to Algorithm 3 for the detailed
procedures. Here we choose freq = 50 and M = 100. When at model updating, each model is trained
for 100 iterations with batch size 512 using the data from its replay buffer. Following Riquelme et al.
(2018), we use RMSprop optimizer with learning rate 0.1 for optimizing. When making actions,
We sample the prediction head according to VKt) obtained using the examples between the last two
model updates.
Notice that in the implementation, we only need to maintain one common replay buffer and the
replay buffer for each model can be implemented by maintaining the number of each context. Thus
when sampling batches of context, we simply need to sample the index of the context and refer to the
common replay buffer to get the actual data.
C.3 Centroid Approximation for Bootstrap DQN
C.3.1 More Background
Similar to the bootstrap method in contextual bandit problem, Bootstrap DQN explores using the
model uncertainty, which can be assessed via maintaining several models trained with bootstrapped
training set. Maintaining several independent models can be very expensive in RL and to reduce the
computational cost, Bootstrap DQN uses a multi-head network with a shared base. Each head in the
network corresponds to a bootstrap model and the common shared base is thus trained via the union
of the bootstrap training set of each head. We train the Bootstrap DQN with standard updating rule
for DQN and use Double-DQN (Van Hasselt et al., 2016) to reduce the overestimate issue. Notice
that our centroid approximation method only changes the memory buffer for each head and thus
introduces no conflict to other possible techniques that can be applied to Bootstrap DQN.
C.3.2 More experiment setup details
Network Architecture Following Osband et al. (2016), we considered multi-head network structure
with a shared base layer to save the memory. Specifically, we use a fully connected layer with 256
hidden neurons as the shared base and stack two fully connected layers each with 256 hidden neurons
as head. Each head in the model can be viewed as one bootstrap particles and in computation, all the
bootstrap particles use the same base layer.
Training and Evaluation For LunarLander-v2, we train the model for 450 episodes with the first
50 episodes used to initialize the common memory buffer. The maximum number of steps within
each episode is set to 1000 and we report the moving average reward with window width 100. For
Catcher-v0, we train the model for 100 episodes with the first 10 episodes used to initialize the
common memory buffer. We set the maximum number of steps within each episodes 2000 and report
the moving average reward with window width 25.
For training the Bootstrap DQN, given the current state xt, we sample one particle based on {vj}m=ι
and use its policy network to make an action at and get the reward rt and next state xt+1. The
Q-value of the state action pair Q(xt, at) is estimated by r + λ * Q(xt+ι), where Q(χt+ι) is the
predicted state value by the target network of the sampled particle and λ is the discount factor set
to be 0.99. At each step, the policy network of all particles are updated using one step gradient
descent with Adam optimizer (β = (0.9, 0.999) and learning rate 0.001) and mini-batch data (size
64) sampled from its replay buffer. We update target model, each particle,s replay buffer and v* S
every 1000 steps for LunarLander-v2 and every 200 steps for Catcher-v0. The update scheme for
replay buffers of each particles and vj* s is the same as the one in contextual bandit experiment. As
the model see significantly larger number of contexts than that in the contextual bandit experiment, to
reduce the memory consumption, we set the max capacity of the common replay buffer to 50000
(the oldest data point will be pop out when the size reaches maximum and new data comes in). For
training the shared base, following Osband et al. (2016), we adds up all the gradient comes from each
head and normalizes it by the number of heads. Algorithm 4 summarizes the whole training pipeline.
22
Under review as a conference paper at ICLR 2022
Algorithm 4 Algorithm for Centroid Approximation Applied to DQN.
1:	Obtain a randomly initialized θj (0), j ∈ [m]. (For the j-th particle, both of its target and policy
network use the same initialization.)
2:	Initialize a common replay buffer Rc = 0 recording all the observed contexts.
3:	For each head, initialize its own replay buffer Rj = 0 that is used for training.
4:	for t ∈ number of total episodes do
5:	while not at terminal state and the number of steps does not exceed the threshold do
6:	Obtain the t-th context xt .
7:	Sample an head based on {v*} to make action at and get Q(xt, at) using the reward r
and the prediction of the corresponding target network.
8:	Update the common replay buffer by Rc J Rc ∪{(χt,Q(χt,at))}
9:	∀j ∈ [m], update its policy network by one step gradient descent using the data from its
reply buffer.
10:	// Update wh and Rj and target network every a few iterations.
11:	if t mod freq == 0 then
12:	∀j ∈ [m], calculate L(θ*(t)) defined in (11) for all the contexts in Rc.
13:	Generate M sets of random weights {wh}hM=1 of contexts in Rc from π.
14:	∀h ∈ [M] and j ∈ [m], calculate Lwh(θ*(t)) = WhL(θ*(t)).
15:	∀h ∈ [M], calculate uwh (t) defined in (12) for each h.
16:	∀i ∈ [|Rc|] andj ∈ [m], calculate qi,j by (15)
17:	∀j ∈ [m], update Vj (t) based on (7).
18:	∀j	∈	[m],	if vj(t) ≤ γ, construct	Rj	=	Rc,	else, construct Rj	by sample	|Rc|
contexts in Rc. The probability that context i is being sampled is qi,j / P|iR=c1| qi,j .
19:	∀j ∈ [m], update the j-th target network by loading the weights of the j-th policy
network.
20:	end if
21:	end while
22:	end for
Figure 4: Results on ensemble modeling with bootstrap using Vgg16 on CIFAR-100.
C.4 Bootstrap Ensemble Model
Ensemble of deep neural networks have been successfully used to boost predictive performance
(Lakshminarayanan et al., 2017). In this experiment, we consider using an ensemble of deep neural
network trained on different bootstrapped training set, which is also known as a popular strategy
called bagging.
We consider image classification task on CIFAR-100 and use standard VGG-16 (Simonyan &
Zisserman, 2014) with batch normalization. We apply a standard training pipeline. We train the
bootstrap model for 160 epochs using SGD optimizer with 0.9 momentum and batchsize 128. The
learning rate is initialized to be 0.1 and is decayed by a factor of 10 at epoch 80 and 120. We start
to apply the centroid approximation at epoch 120 (thus the centroid is initialized with 120 epochs’
training). We generate the bootstrap training set for each centroid every epoch using the proposed
centroid approximation method. We consider m = 3, 4, 5, 10 ensembles and use γ = 0.5/m. We
23
Under review as a conference paper at ICLR 2022
#Particle	γ = 0	γ = 0.5	γ=m
3	3480.0 ± 120	3702.7 ± 89.8	3467.7 ± 115
4	3461.92 ± 126	3723.1 ± 78.7	3600.0 ± 69.3
5	3586.5 ± 64.5	3799.6 ± 84.2	3647.3 ± 64.5
10	3785.0 ± 59.1	3796.9 ± 36.1	3742.7 ± 86.8
Table 4: Ablation study.
repeat the experiment for 3 random trials and report the averaged top1 and top5 accuracy with the
standard deviation of the mean estimator. Algorithm
Figure 4 summarizes the result. Overall, increasing m is able to improve the predictive performance
and with the same number of models, our centroid approximation consistently improves over standard
bootstrap ensembles.
C.5 Ablation Study
We study the effectiveness of using (8) to modify the gradient of centroid with /(t) ≤ Y. We
consider the setting γ = 0 (no modification) and γ = m (always modify, equivalent to no bootstrap
uncertainty) and applied the method on the mushroom dataset in the contextual bandit problem. Table
4 shows that (i) modifying the gradient of centroid with small 说(t) using do improve the overall
result; (2) bootstrap uncertainty is important for exploration.
C.6 Computation overhead
Our main goal is not to decrease the training cost but improve the quality of bootstrap partical
distribution so that we can use less models at deployment and hence reduce the memory cost and
the computational cost for inference. Actually, as discussed in Section 3, our method actually only
introduces a little computation overhead while much improves the quality of the particles, which
is another advantage of our method. For example, in bandit problem on mushroom dataset, when
m = 3, 10, vanilla bootstrap takes 33s, 101s while our approach takes 35s, 108s per run. For the
bagging, when m = 3, 10, vanilla bootstrap takes 11200s, 34240s while ours take 12000s, 36200s.
Results are based on the average of 3 runs. Our method only introduces about 7% computational
overhead even with an naive implementation.
24