Under review as a conference paper at ICLR 2022
Density-Based Clustering with Kernel Diffu-
SION
Anonymous authors
Paper under double-blind review
Ab stract
Finding a suitable density function is essential for density-based clustering algo-
rithms such as DBSCAN and DPC. A naive density corresponding to the indicator
function of a unit d-dimensional Euclidean ball is commonly used in these al-
gorithms. Such a density suffers from an inability to capture local features in
complex datasets. To tackle this issue, we propose a new kernel diffusion density
function, which is adaptive to data of varying local distributional characteristics and
smoothness. Furthermore, we develop a surrogate that can be efficiently computed
in linear time and space and prove that it is asymptotically equivalent to the kernel
diffusion density function. Extensive empirical experiments on benchmark and
large-scale face image datasets show that the proposed approach not only achieves
a significant improvement over classic density-based clustering algorithms but also
outperforms the state-of-the-art face clustering methods by a large margin.
1	Introduction
Density-based clustering algorithms are now widely used in a variety of applications, ranging from
high energy physics (Tramacere & Vecchio, 2012; Rovere et al., 2020), material sciences (Marquis
et al., 2019; Reza et al., 2007), social network analysis (Shi et al., 2014; Khatoon & Banu, 2019)
to molecular biology (Cao et al., 2017; Ziegler et al., 2020). In these algorithms, data points are
partitioned into clusters that are considered to be sufficiently or locally high-density areas with
respect to an underlying probability density or a similar reference function. We call them density
functions throughout this paper. These techniques are attractive to practitioners, due to their non-
parametric feature, which leads to flexibility in discovering clusters that have arbitrary shapes, whilst
classic methods such as k-means and k-medoids (Friedman et al., 2001) can only detect convex (e.g.,
spherical) clusters. Seminal work in the context of density-based clustering includes DBSCAN (Ester
et al., 1996) and DPC (Rodriguez & Laio, 2014), among many others (Ankerst et al., 1999; Cuevas
et al., 2001; Comaniciu & Meer, 2002; Hinneburg & Gabriel, 2007; Stuetzle, 2003).
Most density-based clustering algorithms implicitly identify cluster centers and assign remaining
points to the clusters by connecting with the higher density point nearby. To proceed with these
methods it requires a density function, which is usually an estimate of the underlying true probability
density or some variants of it. For example, a popular choice is the naive density function that is
carried out by simply calculating the number of data points covered in the ε-neighborhood of each
x. Note that such densities are not adaptive to different distribution regions. One of the challenging
scenarios is when clusters in the data have varying local features, for example, size, height, spread,
and smoothness. Therefore, the resulting density function has a tendency to flatten the peaks and
valleys in the data distribution, which leads to underestimation of the number of clusters (see Figure
1). Many heuristics variations of DBSCAN and DPC have been proposed to magnify the local
features, thus making the clustering task easier (CamPello et al., 20l3; Chen et al., 2018; Ertoz et al.,
2003; Zhu et al., 2016). Most of these methods can be viewed as performing clustering on certain
transformations of the naive density function. However, if the naive density function itself is quite
Problematic in the first Place, these methods will become less effective.
Moreover, even if we aPPly adaPtive alternatives to modify the classic density functions, there are
other contentious issues of the generally used linear kernel density estimator (KDE). It often suffers
from severe boundary bias (Marron & RuPPert, 1994) and is acknowledged as comPutationally
1
Under review as a conference paper at ICLR 2022
Figure 1: (a) Data generated from Gaussian mixture model with 3 components, each has differing
variance and weight. (b) Naive density function in 2D (top) and 3D (bottom): only one peak can be
identified. (C) Proposed kernel diffusion density function: 3 clusters Can be easily discovered.
XIOY
(a) Data clusters	(b) Naive density	(c) Kernel diffusion density
expensive. These phenomenon prevent the classic density functions being practically useful and
reliable, especially for large-scale and complex clustering tasks.
To overcome these problems in density-based clustering algorithms, in this paper we propose a
general approach to build the so-called kernel diffusion density function to replace classic density
functions. The key idea is to construct the density from a user-specified bivariate kernel that has
desired local adaptive properties. Instead of using the naive density function and its variants, we
utilize the bivariate kernel to derive a transition probability. A diffusion process is induced by this
transition probability, which admits a limiting and stationary distribution. This limiting distribution
serves as a plausible density function for clustering with reduced error.
Under this framework, we provide examples of symmetric and asymmetric bivariate kernels to
construct the kernel diffusion density function, which can tackle clustering complex and locally
varying data. We apply the resulting adapted DBSCAN and DPC algorithms to widely different
empirical datasets and show significant improvement in each of these analyses. The main contributions
of this paper are summarized below:
•	We introduce new bivariate kernel functions and construct the associated kernel diffusion
processes. Based on the diffusion process, we propose a kernel diffusion density function
to adapt density-based clustering algorithms such as DBSACN and DPC, which attains
accuracy in the presence of varying local features.
•	We derive a computationally much more efficient surrogate, and show analytically it is
asymptotic equivalent to the proposed kernel diffusion density function.
•	By extensive experiments, we demonstrate the superiority of kernel diffusion density func-
tion over naive density function and its variants when applying to DBSCAN and DPC, and
show it outperforms state-of-the-art GCN-based methods on face clustering tasks.
2	Related Work
Density-Based Clustering There is vast literature on adapting density-based clustering algorithms
to tackle large variations in different clusters in the data. DPC itself is such a refinement of DBSCAN,
as it determines cluster centers not only by highest density values but also by taking into account
their distances from each other, thus has a generally better performance in complex clustering tasks.
Other attempts include rescaling the data to have relative reference measures instead of KDE (Zhu
2
Under review as a conference paper at ICLR 2022
et al., 2016; Chen et al., 2018), and using the number of shared-nearest-neighbors between two points
to replace the geometric distance (Ertoz et al., 2003).
Diffusion Maps The technique of diffusion maps (Coifman et al., 2005; Coifman & Lafon, 2006)
gives a multi-scale organization of the data according to their underlying geometric structure. It uses
a local similarity measure to create a diffusion process on the data which integrates local geometry at
different scales along time t. Generally speaking, the diffusion will segment the data into several
smaller clusters in small t and group data into one cluster for large t. Applying eigenfunctions at a
carefully selected time t leads to good macroscopic representations of the data, which is useful in
dimension reduction and spectral clustering (Nadler et al., 2005).
Face Clustering Face clustering has been extensively studied as an important application in
machine learning. Traditional algorithms include k-means, hierarchical clustering (Sibson, 1973)
and ARO (Otto et al., 2017). Many recent works take advantage of supervised information and
GCN models, achieving impressive improvement comparing to traditional algorithms. To name
a few, CDP (Zhan et al., 2018) proposes to aggregate the features extracted by different models;
L-GCN (Wang et al., 2019) predicts the linkage in an instance pivot subgraph; LTC (Yang et al., 2019)
generates a series of subgraphs as proposals and detects face clusters thereon; and GCN(V+E) (Yang
et al., 2020) learns both the confidence and connectivity by GCN. In this paper we demonstrate
that the proposed density-based clustering algorithm with kernel diffusion, as a general clustering
approach, even outperforms theses state-of-the-art methods that are especially designed for face
clustering.
3 Preliminaries
3.1	Notations
Let the dataset D = {x1, . . . , xn} ⊂ Rd be n i.i.d samples drawn from a distribution measure F with
density f on Rd . Let Fn denote the corresponding empirical distribution measured with respect to D,
i.e., Fn(A) = 1 Pn=I 1a(xi), Where 1a(∙) denotes the indicator function of set A. We write ||u|| as
the Euclidean norm of vector u. Let B(x, ε) and Vd denote the d-dimensional ε-ball centered at x
and the volume of the unit ball B(0, 1), respectively. Let Nk(x) denote the set of k-nearest neighbors
of point x within the dataset D .
3.2	Density function
Density-based algorithms perform clustering by specifying and segmenting high-value areas in a
density function denoted by ρ. Usually, we calculate each of ρ(xi), and then identify cluster centers
with (locally) highest values. Many popular algorithms such as DBSCAN and DPC employ the
following naive density function:
Pnaive(X) =	X	.	⑴
nε	d
y∈D
The naive density function ρnaive is actually an empirical estimation of f for carefully chosen ε. It
is easy to observe, for clustering purpose we only care about ρnaive(x) up to a normalising constant,
which makes it simply equivalent to counting the total number of data points in the ε-ball around x.
In practice, the data distribution may be very complex and contains varying local features that are
difficult to be detected. The naive density in (1) with the same radius ε for all x usually suffers from
unsatisfactory empirical performance, for example, failing to identify small clusters with fewer data
points. One possible way to alleviate this problem is through a transformation into the following
local contrast (LC) function (Chen et al., 2018):
1
n
Σ
ρLC(x)
ρnaive(x)>ρnaive(y) .
(2)
y∈Nk (x)
In this way, ρLC compares the density of each data point with its k-nearest neighbors. To see the
benefit of LC, let us consider x to be a cluster center. After local contrasting, ρLC(x) is likely to reach
the value of k regardless of the size of this cluster.
3
Under review as a conference paper at ICLR 2022
However density functions like ρLC still highly depend on the underpinning performance of ρnaive.
This restricts their applications in clustering data with challenging local features.
4	Methodology
In this section, we present a new type of density-based clustering algorithm, based on the notion
of kernel diffusion density function. Towards this end, we will introduce a kernel diffusion density
function, which takes account of local adaptability and is well-suited for clustering purpose. We
provide details on how to derive this density function from a diffusion process induced by bivariate
kernels. We also provide a surrogate density function that is computationally more efficient.
4.1	Diffusion process and Kernel Diffusion Density
Considering a bivariate kernel function k : D × D → R+, such that:
•	k(x, y) is positive semi-definite, i.e., k(x, y) ≥ 0.
•	k(x, y) is Fn-integrable with respect to both x and y.
We define d(x) = n D k(x, y)dFn (y) as a local measure of the volume at x and define
k(x, y)
p(x,y) = E.
(3)
It is easy to see that p(x, y) satisfies the conservation property n D p(x, y)dFn(y) = 1. As a result,
p(x, y) can be viewed as a probability for a random walk on the dataset from point x to point y, which
induces a Markov chain on D with n × n transition matrix P = [p(x, y)]. This technique is standard
in various applications, known as the normalized graph Laplacian construction. For example, we can
view D as a graph, L = I - P as the normalized graph Laplacian, and d(x) as a normalization factor.
For t ≥ 0, the probability of transiting from x to y in t time steps is given by Pt, the t-th power of P.
Running the Markov chain forward in time, we observe the dataset at different scales, which is the
diffusion process Xt on D. Let ρ(x, t): D × R+ → R+ be the associated probability density, which
is governed by the following second-order differential equation with initial conditions:
∂tρ(x, t) = -Lρ(x, t),
ρ(x, 0) = φ0(x),
(4)
where φ0(x) is a probability density at time t = 0. In practice we can use any valid choice of φ0(x),
e.g. the uniform density.
To give an explicit example of the diffusion process, consider a sub-class of k, i.e., isotropic kernels,
where k(x, y) = K(||x - y||2/h) for some function K : R → R+. Here we can dual interpret h as
a scale parameter to infer local information and as a time step h = ∆t at which the random walk
jumps. Then we can define the forward Chapman-Kolmogorov operator TF as
TF (x)
n	p(x,
D
y)φ0(y)dFn(y).
Note that TF is the data distribution at time t = h, thus can be viewed as continuous analogues of
the left multiplication by the transition matrix P. Letting h → 0, the random walk converges to a
continuous diffusion process with probability density evolves continuously in t. In this case, we can
explicitly write the second-order differential equation in (4) as:
lim p(x,t + h) - ρ(x, t)
h→0	h
TF - I
h→0 ^P(X,t),
(5)
where Lh = limh→0 (TF - I)/h is the conventional infinitesimal generator of the process.
Now we are ready to introduce our kernel diffusion density function.
4
Under review as a conference paper at ICLR 2022
Definition 1. (Kernel diffusion density function) Suppose the Markov chain induced by P is ergodic,
we define the kernel diffusion density function as the limiting probability density of the diffusion
process Xt, i.e.,
ρKD(x) = lim ρ(x, t).	(6)
t→∞
Intuitively, on the one hand, with increased values of t we expect the diffusion process Xt gradually
reveals the geometric structure (such as high-density regions) of the data distribution F . To see this,
note that the transition probability P reflects connectivity between data. We can interpret a cluster as
an underlying geometric structure in which the probability of staying in this region is high during
a transition. In the diffusion process, the probability of following a path along a structure usually
increases with t, as the involved data points are dense and highly connected. Therefore, the path
consists of short and high probability jumps. Whilst paths that do not follow any structure consists
of long and low probability jumps, which lowers their overall probability. As a result, geometry
structures of F is magnified during the diffusion.
On the other hand, by talking certain sophisticated forms of k(x, y) that take into account of local
adaptivity, we also slow down the diffusion to avoid trivial geometry structures such as one big cluster
for all the data points. In this way, we can eventually identify the correct geometry structure at the
right scale.
4.2	Locally Adaptive Kernels
To address the local adaptability in kernel diffusion density function, we propose the following two
bivariate kernels. Both of them are very simple variations of the most commonly used classic kernels.
Symmetric-Gaussian kernel:
k(χ,y) = eχp ( -kx-yk2"(x,ε)(y).
(7)
Here h and are both hyper-parameters. We call this kernel symmetric since k(x, y) = k(y, x) .
Asymmetric-Gaussian kernel:
k(x,y) = exp ( - kx hyk )lNk(x)(y).	(8)
Here h and k are hyper-parameters. Note that in this case k(x, y) is asymmetric as y ∈ Nk(x) does
not imply x ∈ Nk(y).
Bivariate kernels defined in (7) and (8) are just combinations of classic Gaussian kernel and ε-
neighbourhood or k-nearest neighbours kernels, respectively. With these simple combinations, we
truncate Gaussian kernel at local areas, and the contribution of each point y to the construction of the
density function ρKD(x) depends not only on the distance ||y - x|| but also on the local geometry
structure around x. Hence, the new kernels are adaptive at different x, which is expected to lead to
better clustering performance against local features. We remark that the Asymmetric-Gaussian kernel
takes into account a varying neighborhood around each x, thus is more adaptive comparing to the
Symmetric-Gaussian kernel.
Although here we only provide two examples of locally adaptive kernels, other options can be easily
created in a similar spirit under this framework, e.g., changing the Gaussian kernels to other kernels
or changing the ε-neighbourhood (k-nearest neighbours) kernels to other locally truncated functions.
Once k(x, y) is determined, we can derive the corresponding density function ρKD. Next, we just
need to simply apply any density clustering procedure like DPC or DBSCAN based on ρKD instead
of the naive density function ρnaive.
In Section 5, we assess the empirical performance of the proposed kernel diffusion density function
with the above two locally adaptive kernels. They outperform existing density-based algorithms and
other state-of-the-art methods.
5
Under review as a conference paper at ICLR 2022
4.3	Fast Kernel Diffusion Density
The kernel diffusion density function ρKD can be calculated as the stationary distribution of a Markov
chain induced by the transition matrix P. Numerically, we can solve it by iteratively right multiplying
P with ρ(x, t) until convergence, or applying aQR decomposition on P. These methods are expensive
in terms of computational cost, especially when the sample size n is large.
To tackle this problem, we propose the following surrogate of ρKD(x) which is computationally more
efficient.
Definition 2. (Fast kernel diffusion density function) Let p(y, x) be the transition probability from
point y to point x, as defined in equation (3). We define the fast kernel diffusion density function as
ρFKD(x) =	p(y, x)dFn(y),
D
(9)
It is straightforward that ρFKD can be obtained in linear time and memory space, as we only need to
compute the column averages of matrix P .
Here we show that ρFKD is not only computationally efficient but also suitable for detecting local
features. This is illustrated through the following Theorem 1. Consider a special case that k(x, y) =
1B(x,ε) (y). Then it is easy to verify that
PFKD(X) = C X	Pna^,
d y∈B(x,ε) naive
where Cd = nεdVd is a normalising constant. In this way, we build a connection between PFKD and
the naive density function Pnaive in this special example.
Theorem 1.	Consider the above special case that k(x, y) = 1B(x,ε) (y). In addition, assume the
dataset D = {x1, ..., xn} can be split into m disjoint clusters: i.e., X = D1	... Dm and
for each x ∈ D, B(x, ε) only contain data points that belong to the same cluster as x. Denote
Pj = ∣Diq ∑χ∈Dj ρFKD(x) as the average density in cluster j. We have
Pi =…=Pm = 1.
Theorem 1 demonstrates that the averaged PFKD in each cluster are the same regardless of cluster
sizes and other local features. This shows that PFKD elevates the density of small clusters, which is
essential for finding the density peaks of small clusters.
Previously we claim that PFKD is a surrogate of the kernel diffusion density PKD. Next, we want
to reveal the relationship between these two density functions from an asymptotic viewpoint. To
proceed, we will need the following assumption.
Assumption 1. There exists some positive constant c < 1 that is independent of n, such that
PFKD(x) ≤ c holds for every x ∈ D.
This is a very mild assumption, since it always holds that PFKD(x) < 1, and the average of PFKD(x)
over the dataset is D PFKD(x)dFn (x) = 1/n, which vanishes as n → ∞. Now we are ready to
present the following theorem that characterise the closeness between PFKD and PKD.
Theorem 2.	Suppose that Assumption 1 holds and the Markov chain induced by the kernel k(x, y) is
ergodic. We have
PKD(x) a→. 1
PFKD (x)
As shown in the Appendix, the almost sure convergence in Theorem 2 is of a fast rate at n-i. Thus it
is safe for us to use it to replace PKD in finite sample experiments. This result is also verified by our
numerical experiments in Section 5.
5	Experiments
In this section, we empirically evaluate the proposed kernel diffusion density functions against Pnaive
and PLC in density-based clustering algorithms, and also compare them with other state-of-the-art
6
Under review as a conference paper at ICLR 2022
Table 1: Clustering performance on benchmark datasets with different density functions applied to
DPC. Pairwise F-score (FP) and BCube F-score (FB) under optimal parameter tuning are given. The
best and Second-bset results in each dataset are bolded and underlined, respectively.
Dataset ∣_____________________FP___________________∣__________________FB____________________
		ρnaive	ρLC	PKm	asym ρKD	sym ρFKD	asym PFKD	ρnaive	PLC	PKm	asym PKD	sym PFKD	asym PFKD
Banknote	54.3	31.6	67.2	83.9	67.2	93.6	57.7	31.8	67.2	85.1	67.2	93.6
Breast-d	55.9	51.8	78.0	69.1	67.4	72.6	59.0	58.7	76.0	69.7	69.4	72.2
Breast-o	57.6	70.7	82.8	92.9	82.7	92.9	52.2	74.1	75.9	92.2	75.8	92.2
Control	48.6	49.3	49.0	63.9	52.5	64.5	51.6	52.4	52.0	70.8	55.1	71.8
Glass	36.9	39.0	46.3	48.1	44.8	47.8	42.7	45.7	55.1	56.9	53.5	57.1
Haberman	66.9	64.1	74.5	75.7	75.8	75.7	66.9	63.3	74.5	75.8	75.9	75.8
Ionosphere	27.4	28.3	46.9	54.9	46.0	53.9	25.0	25.8	42.6	52.5	41.7	49.2
Iris	54.3	53.8	65.8	74.6	69.2	74.6	61.6	62.3	72.7	80.0	74.0	80.0
Libras	20.0	22.9	29.3	31.5	26.0	31.0	26.8	29.1	37.8	41.8	33.3	39.8
Pageblocks	92.9	93.0	90.5	90.2	89.7	90.2	89.9	90.0	89.8	89.7	89.6	89.7
Seeds	54.3	54.9	68.0	78.0	69.5	78.0	54.3	55.4	72.4	78.7	72.9	78.7
Segment	48.4	48.0	57.1	58.0	41.4	56.1	64.2	63.8	67.1	69.2	60.6	68.2
Wine	45.2	61.1	56.6	68.0	60.0	65.3	46.0	61.9	61.5	74.7	66.3	71.4
methods. We denote ρsKyDm and ρsFyKmD as the kernel diffusion density functions and its fast surrogate,
with symmetric-Gaussian kernel, respectively. Similarly, we denote ρaKsDym and ρaFsKyDm as the proposed
two density functions with asymmetric-Gaussian kernel, respectively. We examine their performance
on a wide range of datasets. The clustering results are measured Pairwise F-score (Banerjee et al.,
2005), BCubed F-score (Amigo et al., 2009) or NMI(Cover,1999).
5.1	Performance on Benchmark Datasets
We now discuss the performance on 13 benchmark datasets (〜100 to 〜5,000 data points) from UCI
repository. The metadata is summarised in the Appendix.
As summarised in Table 1, both ρsKyDm and ρaKsDym uniformly outperform ρnaive and ρLC in terms of
clustering accuracy in terms of F-scores. results based on NMI are deferred to the Appendix. The
proposed kernel diffusion density functions with asymmetric Gaussian kernel, ρaKsDym, which enjoys
better local adaptivity analytically, achieves the best results on most datasets and outperforms ρnaive
and ρLC by a large margin. It is worth noticing that the two fast surrogates, ρsFyKmD and ρaFsKyDm, achieve
comparable results with their original counterparts, ρsKyDm and ρaKsDym. In the Appendix similar results
are observed for the same set of density functions applied to DBSCAN.
Figure 2: Precision-Recall curves of different approaches applied to DPC on MS1M dateset, using
(a) Pairwise metric, and (b) BCubed metric .
7
Under review as a conference paper at ICLR 2022
5.2	Performance on face image Datasets
Clustering face images according to their latent identity becomes an important application in recent
years. It is challenging in the sense that face image datasets usually contain thousands of identities,
corresponding to thousands of clusters. Meanwhile, the number of images for each identity (cluster)
is quite different, corresponding to the variety of cluster sizes. We assess the performance of
the proposed approach on two popular face image datasets: emore_200k (Zhan et al., 2018) and
MS1M (Guo et al., 2016).
emore_200k. The dataset contains	Table 2: ClUstering performance			on emore	200k. BCUbed	
2,577 identities with 200,000 images following the protocol in Zhan et al.	J B		 precision, recall and F-score are reported.				.	
			# clUsters	Precision	Recall	FB
(2018). Wesetεto0.8, k to 200, and	I Algorithm					
h to 0.5 for density-based methods.		k-means	2,577	94.24	74.89	83.45
ResUlts are sUmmarized in Table 2.	Baseline	HAC	2,577	97.74	88.02	92.62
The proposed diffUsion density fUnc-		ARO	85,150	52.96	16.93	25.66
tions are applied to DPC, and com- pared with k-means, HAC (Sibson,		CDP	-	89.35	88.98	89.16
		Pnaive	7928	92.36	78.14	84.65
1973), ARO (Otto et al., 2017), and						
CDP (Zhan et al., 2018). Again, we	Density	PLC	3485	96.15	86.58	91.11
observe significant improvement in		sym pKD	2781	95.82	93.24	94.51
the proposed density fUnctions over	-based	asym pKD	2546	95.48	93.82	94.64
ρnaive and ρLC. It is also worth point-		sym PFKD	3622	95.27	92.54	93.89
ing oUt that, density-based clUster-		asym pFKD	2569	96.37	93.93	95.13
ing with proposed kernel diffusion - Not available
density functions also outperform the
state-of-the-arts approaches such as CDP by a large margin.
MS1M. The dataset contains 8,573 iden-
tities with around 584,000 images follow-
ing the protocols in Yang et al. (2020).
We set ε to 0.8, k to 200, and h to 0.5
for density-based methods. We reported
the results of clustering performance in
Table 3. Precision versus Recall curves
for different density functions (applied
to DPC) are plotted in Figure 2. In Ta-
ble 3, the proposed kernel diffusion den-
sity functions outperform ρnaive and ρLC.
Note that GCN-based methods such as
L-GCN (Wang et al., 2019), LTC (Yang
et al., 2019) and GCN (V+E) (Yang et al.,
2020) achieve generally better clustering
performance than unsupervised methods
due to their supervised nature. However,
it is quite encouraging to see that the
proposed kernel diffusion approaches, al-
though are also unsupervised clustering
methods, considerably outperform the
GCN-based methods.
Table 3: Clustering performance on MS1M. Pairwise F-
score and BCUbed F-Score are reported.
I Algorithm	# clusters	FP	FB
k-means	8,573	79.21	81.23
TT	∙ A HAC	8,573	70.63	70.46
UnsUpervised ARO	-	13.60	17.00
CDP	-	75.02	78.70
L-GCN	-	78.68	84.37
Supervised LTC	-	85.66	85.52
GCN(V+E)	-	87.55	85.94
pnaive	59551	78.37	79.35
Plc	24019	83.61	85.06
sym Density-based asym	- 22869	- 88.15	- 87.14
sym pFKD	34246	84.40	85.37
asym pFKD	22927	87.26	87.41
- Not available
5.3	Sensitivity Analysis
Next, we examine the sensitivity of the proposed kernel diffUsion density fUnctions to hyper-
parameters and compare it with ρnaive and ρLC. The resUlts are obtained via extensive experiments on
emore_200k and MS1M, which are shown in FigUre 3. We can see that the clUstering performance of
ρsKyDm is mUch more stable than ρnaive and ρLC when we vary the valUe of ε. Whilst ρaKsDym is robUst to
the parameter k, and both ρsKyDm and ρaKsDym are qUite robUst to the parameter h.
8
Under review as a conference paper at ICLR 2022
Figure 3: Sensitivity analysis on emore_200k and MS1M. We investigate the clustering performance
by varying the following parameters: (a) Radius of ε-ball; (b) Number k of nearest neighbors; (c)
Bandwidth h of Gaussian kernel.
OJOOSId PBqnDm
alo。S山 P ① qnoffl
(a) ε	(b)∕c	(c) h
一溜m
1 Pnaive
1 Plc
5.4	Computational Cost
We carried out a series of experiments on
MS1M to demonstrate the computational
efficiency of the fast surrogate ρFKD in
terms of time and space. With a collec-
tion of subsampled data from MS1M at
different percentile levels, we run both the
kernel diffusion density ρKD and the fast
surrogate ρFKD. As we can observe from
Figure 4, the running time and memory
usage of ρKD increase dramatically with
the sample size. Whilst ρFKD retains a
very low level of computational cost. This
suggests that ρFKD, which achieves an ex-
cellent computational efficiency, should
be favored in practice.
Figure 4: Running time and memory usage of the pro-
posed methods at different sample sizes on MS1M.
S① 6esπ AIOuI a Ul
6	Conclusion
Density-based clustering has a profound impact on machine learning and data mining. However,
the underpinning naive density function suffers from detecting varying local features, causing extra
errors in the clustering. We propose a new set of density functions based on the kernel diffusion
process to resolve this problem, which is adaptive to density regions of varying local distributional
features. We demonstrate that DBSCAN and DPC adapted by the proposed approach have improved
clustering performance comparing to their classic versions and other state-of-the-art methods.
9
Under review as a conference paper at ICLR 2022
References
EnriqUe Amig6, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. A comparison of extrinsic
clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4):461-486,
2009.
Mihael Ankerst, Markus M Breunig, Hans-Peter Kriegel, and Jorg Sander. Optics: Ordering points to
identify the clustering structure. ACM Sigmod Record, 28(2):49-60, 1999.
Arindam Banerjee, Chase Krumpelman, Joydeep Ghosh, Sugato Basu, and Raymond J Mooney.
Model-based overlapping clustering. In Proceedings of the eleventh ACM SIGKDD International
Conference on Knowledge Discovery in Data Mining, pp. 532-537, 2005.
Ricardo JGB Campello, Davoud Moulavi, and Jorg Sander. Density-based clustering based on
hierarchical density estimates. In Pacific-Asia Conference on Knowledge Discovery and Data
Mining, pp. 160-172. Springer, 2013.
Junyue Cao, Jonathan S Packer, Vijay Ramani, Darren A Cusanovich, Chau Huynh, Riza Daza,
Xiaojie Qiu, Choli Lee, Scott N Furlan, Frank J Steemers, et al. Comprehensive single-cell
transcriptional profiling of a multicellular organism. Science, 357(6352):661-667, 2017.
Bo Chen, Kai Ming Ting, Takashi Washio, and Ye Zhu. Local contrast as an effective means to robust
clustering against varying densities. Machine Learning, 107(8):1621-1645, 2018.
Ronald R Coifman and StePhane Lafon. Diffusion maps. Applied and Computational Harmonic
Analysis, 21(1):5-30, 2006.
Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick Warner,
and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and structure definition
of data: Diffusion maps. Proceedings of the National Academy of Sciences, 102(21):7426-7431,
2005.
Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603-619, 2002.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Antonio Cuevas, Manuel Febrero, and Ricardo Fraiman. Cluster analysis: a further approach based
on density estimation. Computational Statistics & Data Analysis, 36(4):441-459, 2001.
Levent Ertoz, Michael Steinbach, and Vipin Kumar. Finding clusters of different sizes, shapes,
and densities in noisy, high dimensional data. In Proceedings of the 2003 SIAM International
Conference on Data Mining, pp. 47-58. SIAM, 2003.
Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. A density-based algorithm for
discovering clusters in large spatial databases with noise. In KDD, volume 96, pp. 226-231, 1996.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,
volume 1. Springer Series in Statistics New York, 2001.
Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset
and benchmark for large-scale face recognition. In European Conference on Computer Vision, pp.
87-102. Springer, 2016.
Alexander Hinneburg and Hans-Henning Gabriel. Denclue 2.0: Fast clustering based on kernel
density estimation. In International Symposium on Intelligent Data Analysis, pp. 70-80. Springer,
2007.
Jeffrey J Hunter. Generalized inverses and their application to applied probability problems. Linear
Algebra and Its Applications, 45:157-198, 1982.
Mehjabin Khatoon and W Aisha Banu. An efficient method to detect communities in social networks
using dbscan algorithm. Social Network Analysis and Mining, 9(1):1-12, 2019.
10
Under review as a conference paper at ICLR 2022
Emmanuelle A Marquis, Vicente Araullo-Peters, Yan Dong, Auriane Etienne, Svetlana Fedotova,
Katsuhiko Fujii, Koji Fukuya, Evgenia Kuleshova, Anabelle Lopez, Andrew London, et al. On the
use of density-based algorithms for the analysis of solute clustering in atom probe tomography data.
In Proceedings of the 18th International Conference on Environmental Degradation of Materials
in Nuclear Power Systems-Water Reactors, pp. 2097-2113. Springer, 2019.
James S Marron and David Ruppert. Transformations to reduce boundary bias in kernel density
estimation. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):653-671,
1994.
Boaz Nadler, Stephane Lafon, Ronald R Coifman, and Ioannis G Kevrekidis. Diffusion maps, spectral
clustering and eigenfunctions of fokker-planck operators. arXiv preprint math/0506090, 2005.
Charles Otto, Dayong Wang, and Anil K Jain. Clustering millions of faces by identity. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 40(2):289-303, 2017.
Hasanzadeh PR Reza, AH Rezaie, SHH Sadeghi, MH Moradi, and M Ahmadi. A density-based fuzzy
clustering technique for non-destructive detection of defects in materials. Ndt & E International,
40(4):337-346, 2007.
Alex Rodriguez and Alessandro Laio. Clustering by fast search and find of density peaks. Science,
344(6191):1492-1496, 2014.
Marco Rovere, Ziheng Chen, Antonio Di Pilato, Felice Pantaleo, and Chris Seez. Clue: A fast parallel
clustering algorithm for high granularity calorimeters in high-energy physics. Frontiers in Big
Data, 3, 2020.
Jieming Shi, Nikos Mamoulis, Dingming Wu, and David W Cheung. Density-based place clustering
in geo-social networks. In Proceedings of the 2014 ACM SIGMOD International Conference on
Management of Data, pp. 99-110, 2014.
Robin Sibson. Slink: an optimally efficient algorithm for the single-link cluster method. The
Computer Journal, 16(1):30-34, 1973.
Werner Stuetzle. Estimating the cluster tree of a density by analyzing the minimal spanning tree of a
sample. Journal of Classification, 20(1):25-47, 2003.
A Tramacere and C Vecchio. γ-ray dbscan: A clustering algorithm applied to fermi-lat γ-ray data. In
AIP Conference Proceedings, volume 1505, pp. 705-708. American Institute of Physics, 2012.
UCI. UCI machine learning repository. http://archive.ics.uci.edu/ml/datasets.
php.
Zhongdao Wang, Liang Zheng, Yali Li, and Shengjin Wang. Linkage based face clustering via
graph convolution network. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 1117-1125, 2019.
Lei Yang, Xiaohang Zhan, Dapeng Chen, Junjie Yan, Chen Change Loy, and Dahua Lin. Learning
to cluster faces on an affinity graph. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 2298-2306, 2019.
Lei Yang, Dapeng Chen, Xiaohang Zhan, Rui Zhao, Chen Change Loy, and Dahua Lin. Learning
to cluster faces via confidence and connectivity estimation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 13369-13378, 2020.
Xiaohang Zhan, Ziwei Liu, Junjie Yan, Dahua Lin, and Chen Change Loy. Consensus-driven
propagation in massive unlabeled data for face recognition. In Proceedings of the European
Conference on Computer Vision (ECCV), pp. 568-583, 2018.
Ye Zhu, Kai Ming Ting, and Mark J Carman. Density-ratio based clustering for discovering clusters
with varying densities. Pattern Recognition, 60:983-997, 2016.
Carly GK Ziegler, Samuel J Allon, Sarah K Nyquist, Ian M Mbano, Vincent N Miao, Constantine N
Tzouanas, Yuming Cao, Ashraf S Yousif, Julia Bals, Blake M Hauser, et al. Sars-cov-2 receptor
ace2 is an interferon-stimulated gene in human airway epithelial cells and is detected in specific
cell subsets across tissues. Cell, 181(5):1016-1035, 2020.
11
Under review as a conference paper at ICLR 2022
A Appendix
In this supplementary file, we provide technical proofs of the theoretical results in Section 4.3, and
present extra empirical experiments regarding our kernel diffusion approach with symmetric and
asymmetric Gaussian kernels applied to DBSCAN. All the numerical experiments are carried out on
a standard work station with a Intel 64-cores CPU and two Nvidia P100 GPUs.
A.1 Pseudo code for DBSCAN and DPC.
Algorithm 1 DBSCAN
1:	Input: SetOfPoints X, EPs ε, MinPts k
2:	H:={x ∈ X : |B(x, ε) ∩ X| ≥ k};
3:	G:=undirected graPh with vertices H and edge between x, x0 ∈ H if |x - x0 | ≤ ε;
4:	Output: connected comPonents of G
The connected comPoenents of the graPh are determined a clusters,a dn the remaining Points are
unclustered and considered as noise-Points.
Algorithm 2 DPC
1:	Input: SetOfPoints, TrUncDis d
2:	ComPute di,j for ∀i,j ∈ SetOfPoints;
3:	For i from 1 to SetOfPoints.size:
4:	Pi := ∑i=j {-{(j--dc}
5:	δi := minj∙0>ρi(di,j)
6:	Plot decision maP M with ρ as the horizontal axis and δ as the vertical axis;
7:	Mark Point i with relatively higher ρi and δi as a clUster center;
8:	Mark Point i with relatively lower ρi bUt relatively higher δi as a noise Point;
9:	Assign the rest Point with the label the same as the nearest clUster center;
10:	Output: SetOfPoints
A.2 Proofs of Theoretical Result.
Proof of Theorem 1. Since {Dι,…,Dm} are disjoint, We havep(χ, y) = 0 if X and y belong to
different clUsters. By the definition of matrix P, for each x ∈ Dj, we have
D
p(x, y)dFn(y)
1,
Which imPlies that
p(x, y)dFn(x)dFn(y) = |Dj |.
Therefore,
Pj ∣Dj | =	P	p(x,y)dFn(x)dFn (y) = ∣Dj |,
x∈Dj y∈Dj
which implies that Pj = 1 for any j = 1,,...,m.
□
Before Proceeding to the Proof of Theorem 2, We need folloWing aUxiliary lemma that relates the
stationary distribUtion ofa Markov chain to an arbitrary vector g.
Lemma A.1. Let P be transition probability matrix of a finite inreducible discrete time Markov
chain with n states, which admits a stationary distribution, denoted by vector π. We write e =
(1, . . . , 1)T ∈ Rn as the a column vector of ones. The following holds for any vector g such that
gTe 6= 0:
(1)	(I - P + egT) is non-singular.
(2)	Let H = (I - P + egT)-1, then πT = gTH.
12
Under review as a conference paper at ICLR 2022
Proof. Since π is the stationary distribution, we have πTe = 1. Applying Theorem 3.3 in (Hunter,
1982) yields that matrix (I - P + egT ) is non-singular.
Next recall that πT P = πT , therefore we have
πT (I - P + egT) = πT - πTP + πT egT
= πT egT
= gT,
which implies πT = gTH.
□
Proof of Theorem 2. Note that for for each x ∈ D, the linear reference function ρFKD (x) =
D p(y, x)dFn(y) is the corresponding column average of the transition matrix P. We write the i-th
column vector of P as
pi = p(x1, xi), . . . ,p(xn,xi) .
Therefore ρFKD(xi) = eTpi/n
Since the Markov chain induced by the kernel k(x, y) is ergodic, the density ρ(x, t) of the diffusion
process Xt converges to the limiting stationary distribution of the Markov chain, denoted by π .
We can write the n-vectors of g and π in the following form:
g = (gl,...,gn)T = n(ρFKD (Xl),...,PFKD(Xn )), and ∏ = (PKD(Xl),...,PKD(Xn))T,
where gi = eTpi is the i-th column sums of matrix P. As a result, we have
J PFKD(X)dFn(X) = —eg = 1, and n J PKDdFn(X) = e,π = 1.
By the definition of g, we know
(egT)2 = negT and eTP = gT.
It follows from Lemma A.1 that (I - P + egT) is non-singular and πT = gTH, where H
(I-P+egT)-1.
We define B = I + egT . By simple algebra calculation, we can find B is non-singular with
T
BT= I -q.
n+1
T 1	gT
As a result, it is easy to see that that gτ BT =-and
n+1
H-1 =B-P = (I - PB-1)B.
Use the Neumann series, we have
∞
H = B-1(I - PB-1)-1 = B-1 X(PB-1)i.
i=0
Thus
∏T - gT/n = gT (H -I) = gT [b-1 X(PBT)i - I
i=0
Since We assume for any X ∈ D, g(χ) < C for some 0 < c < 1. This leads to
gTpj ≤ nceT pj = ncgj .
Therefore, let κj be the j-th compoenent of gT P B-1, it is straightforWard
nc
Kj ≤ n+ι gj ≤ egj.
13
Under review as a conference paper at ICLR 2022
This implies for every x ∈ D,
1	∞ i 1
∣PKd(x) — PFKD(X)| ≤ PFKD(X) n+1xci-------I
n+1 i=0	n
II	1	1
≤ PFKD(X) (n +1)(1-c) - n
Hence we have
P(‰ PKD(R=1)= 1,
n→∞ PFKD(X)
which completes the proof.
□
A.3 Additional Numerical Experiments
Naive density with different bandwidths. To illustrate the fail of naive density function in scenario
as in Figure 1, we also plot it with a range of different values of hyperparameters below. We can
observe that it is difficult to detect the three true underlying clusters in all the cases.
Figure 5: Naive density function in 3D with different values of e.
Hyperparameters The parameter ε (radius of the ball, used in Pnaive, PLC, PsKyDm and PsFyKmD) is tuned
by searching within the range between 0.1 and 1 with am increment of 0.1, parameter k (number of
nearest neighbors, used in PLC, PaKsDym and PaFsKyDm) is tuned by searching within the range between 10%
and 50% number of samples, with an increment of 10%.
Metadata of benchmark datasets. The number of samples n, the number of clusters c, and feature
dimension d for each benchmark dataset are listed in Table 4 below.
Benckmark datasets with DBSCAN. We provide the performance of the conventional density
functions, Pnaive and PLC , and the proposed kernel diffusion density functions with symmetric and
asymmetric Gaussian kernels, PKD and PFKD (* ∈ {sym, asym}), applied to DBSCAN on 13 bench-
mark datasets. The results are summarised in Table 6. Similar to DPC, we see that both PsKyDm and
PaKsDym uniformly outperform Pnaive and PLC in terms of clustering quality. PaKsDym, which has better
local adaptivity analytically, achieves the best results on most datasets and outperforms others by a
significant margin in Breast-o, Control, Haberma and Seeds.
NMI for benchmark datasets. Below we present in Table 6 the clustering results for benchmark
datasets based on NMI metric.
14
Under review as a conference paper at ICLR 2022
Table 4: Metadata of benchmark datasets, includes sample size (n), the number of clusters (c), and
feature dimension d.	___________________________
Dataset	n	c	d
Banknote	1372	2	4
Breast-d	569 699	2	30 9
Breast-o		2	
Control	600	6	60 9
Glass	214	7	
Haberman	306	2	3
Ionosphere	351	2	34
Iris	150	3	4
Libras	360	15	90
Pageblocks	5473	5	10
Seeds	210	3	7
Segment	210	7	19
Wine	178	3	13
Table 5: Clustering performance on benchmark datasets with different density functions applied to
DBSCAN. Pairwise F-score (FP) and BCube F-score (FB) under optimal parameter tuning are given.
The best and second-best results in each dataset are bolded and underlined, respectively.
Dataset ∣_________________________FP_______________________∣___________________FB_______________________
I	I sym asym sym asym ∣	∣ sym asym sym asym
ρnaive	PLC	PKD	PKD	PFKD	PFKD	PnaiVe	PLC	PKD	PKD	PFKD	PFKD
.2...5.3.1.4...4..3
ZSL^829Z9^9L
675467738255
.4.4.3.9.2.8.1.0.5.5.0..3
z7L65.275.0.2
6676477648755
.7.0.3.7.9.2.7.2.1.7.2.6.7
S6z66237z6965
6655366648564
.1.7.7.1.9.1.1.6.5.5.0.2.7
5.ss6.466.3.5
6655376718554
.5.9.9.2.8.2.8.2.1.2.2.5.7
6.0.5.4.5.9.3.7.1.5.9.2.5.
2613262634524
.4..6.1.5.3.2.3.5.1.4..5
660.204.32zs9
6675477719634
.4.2.5.9.0.9.2.7.2.9.4
5zs7z048o2z
66744677282
.4.0
10
45
.4..6.3.5.6.2..8.1.2
660.2.473.3
66764775196
.8.5
34
............8.5
Z29L99865S78S
6655266619554
.7.0.3.1.8.2.4.8.0.2.6.9.5
s.579z8.z.77s
6653276618444
..7..5.....1.4..5.5
668zz8568278s
2513262614514
Banknote
Breast-d
Breast-o
Control
Glass
Haberman
Ionosphere
Iris
Libras
Pageblocks
Seeds
Segment
Wine
Number of clusters. In Table 7, we present the number of clusters returned by the density-based
methods for the benchmark datasets. It can be observed that clustering with the proposed diffusion
density functions returned a significantly better estimate of the number of clusters, comparing to that
with classic density functions such as ρnaive and ρLC.
15
Under review as a conference paper at ICLR 2022
Table 6: Clustering performance on benchmark datasets with different density functions applied to
DPC. NMI under optimal parameter tuning are given. The best results in each dataset are bolded.
Dataset	NMI							
	Pnaive	sym PLC I PKD		asym PKD	sym PFKD	asym PFKD I k-means		Spectral
	 Banknote	27.5	33.0	21.7	64.8	53.2	80.2	34.2	17.3
Breast-d	43.7	49.1	46.8	57.4	55.7	46.1	62.3	52.6
Breast-o	30.2	32.7	37.2	79.1	36.4	78.4	74.8	14.0
Control	60.6	60.6	63.2	69.5	61.0	69.6	75.4	68.3
Glass	43.1	43.4	45.0	48.4	43.8	46.6	34.8	36.4
Haberman	9.5	5.7	9.5	3.2	16.9	3.2	7.8	6.6
Ionosphere	27.9	28.0	30.9	31.1	30.1	30.5	13.5	5.2
Iris	51.1	53.1	60.1	73.4	62.6	73.4	74.2	70.6
Libras	63.3	66.4	63.0	68.8	68.2	69.1	60.0	56.1
Pageblocks	8.6	13.0	11.8	28.7	14.4	29.1	13.2	12.1
Seeds	47.1	49.8	53.6	64.8	58.6	64.8	67.4	60.3
Segment	63.5	64.4	65.1	72.2	63.0	70.7	61.2	65.2
Wine	58.1	58.2	72.0	73.3	71.1	58.6	84.2	72.7
Table 7: Number of clusters returned by different density functions applied to DPC. The ground truth
is listed in the last column._________________________________________________________________
Dataset ∣ Pnaive PLC I PKm PKDm PFm) PFKm I Ground Truth
222672235
5
3
7
3
2	7	2
3	3	3
2	1	2
23	27	25
8	3	7
1	3	1
6	7	7
2	3	2
61	100	55
10	11	8
6	3	6
8	6	9
5	3	2
44	26
5	2
17	7
32	23
21	4
40	34
11	7
12	4
60	2
25	3
10	2
27	14
7	3
Banknote	16
Breast-d	3
Breast-o	15
Control	32
Glass	4
Haberman	34
Ionosphere	12
Iris	8
Libras	12
Pageblocks	7
Seeds	10
Segment	21
Wine	3
16