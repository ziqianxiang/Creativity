Under review as a conference paper at ICLR 2022
Lower Bounds on the Robustness of Fixed Fea-
ture Extractors to Test-time Adversaries
Anonymous authors
Paper under double-blind review
Ab stract
Understanding the robustness of machine learning models to adversarial examples
generated by test-time adversaries is a problem of great interest. Recent theoret-
ical work has derived lower bounds on how robust any model can be, when a
data distribution and attacker constraints are specified. However, these bounds
only apply to arbitrary classification functions and do not account for specific ar-
chitectures and models used in practice, such as neural networks. In this paper,
we develop a methodology to analyze the robustness of fixed feature extractors,
which in turn provides bounds on the robustness of any classifier trained on top
of it. The tightness of these bounds relies on the effectiveness of the method used
to find collisions between pairs of perturbed examples at deeper layers. For linear
feature extractors, we provide closed-form expressions for collision finding while
for arbitrary feature extractors, we propose a bespoke algorithm based on the iter-
ative solution of a convex program that provably finds collisions. We utilize our
bounds to identify the layers of robustly trained models that contribute the most
to a lack of robustness, as well as compare the same layer across different training
methods to provide a quantitative comparison of their relative robustness.
1	Introduction
The robustness of machine learning models to test-time (evasion) attacks, particularly via adversarial
examples, has been studied extensively from an empirical perspective (Croce et al., 2020; Papernot
et al., 2016; Li et al., 2020; Biggio & Roli, 2017). A plethora of attacks (Szegedy et al., 2013;
Carlini & Wagner, 2017; Bhagoji et al., 2018; Croce & Hein, 2020) and defenses (Madry et al.,
2018; Zhang et al., 2019) has been proposed. The resulting arms race between proposed attacks and
defenses, has led recent theoretical work (Dohmatob, 2019; Cullina et al., 2018; Mahloujifar et al.,
2019; Bhagoji et al., 2019; 2021; Montasser et al., 2019) to focus on understanding the fundamental
bounds of learning in the presence of adversarially perturbed inputs, in terms of both the existence
of robust classifiers and the sample complexity required to learn them.
A line of research on this front has investigated lower bounds on the robustness attainable by any
classifier when classifying data from distributions modified by adversarial perturbations (Pydi &
Jog, 2020; Bhagoji et al., 2019; 2021). In particular, information-theoretic lower bounds on the
minimum possible robust 0 - 1 and cross-entropy losses have been derived for arbitrary discrete
distributions and very general attack models. However, the bounds from these papers are only
applicable when considering the set of possible classifiers to be all measurable functions, which
does not translate to practice. The classifier architecture and even some portion of the parameters
are fixed (such as in transfer learning) in practical approaches to training machine learning models.
In this paper, we leverage this line of work to find lower bounds on the robustness of commonly
used, fixed feature extractors. This provides a principled method to compare the robustness of
feature extractors obtained by different training methods to arbitrary test-time adversaries. The key
question we answer is:
What is the minimum robust loss incurred by any classifier trained on top of a fixed feature extractor?
Two challenges for training robust machine learning models motivate this question. First, since
transfer learning is now common practice, it is incumbent upon the model trainer to choose a robust
pre-trained feature extractor. Our work offers a principled and quantitative method to choose among
1
Under review as a conference paper at ICLR 2022
feature extractors obtained from different layers and training methods. Second, we are able to shed
light on the impact of common architectural choices, such as activation functions and dimensionality
reduction on robustness. This arises from our study of how the lower bound evolves from layer to
layer within deep neural networks.
Importance of lower bounds on robustness: To determine classifier-agnostic lower bounds on
robustness, earlier work focused on the interaction between points from different classes in the input
space when perturbed through the construction of a conflict graph. Minimizing an appropriately
defined loss function over this graph determines an information-theoretic lower bound on the loss.
Intuitively, the denser the graph is, the higher the lower bound. These classifier-agnostic bounds
are able to determine the plausibility of robust classification for a given adversary and dataset, and
highlight practically relevant perturbation budget.
1.1	Contributions
Lower bounds on robustness for fixed feature extractors: We significantly extend prior work by
deriving lower bounds that depend on the architecture and weights of a fixed feature extractors. We
construct a distance function over the input space that depends on the feature extractor in use. Our
results have implications for both analyzing the robustness of already trained classifiers as well as
determining feature extractors to use for applications such as transfer learning.
Bespoke algorithms for collision finding: For linear feature extractors such as the first layer of
a neural network, we determine exact, closed form expressions to find collisions. Collision finding
for non-linear feature extractors is a non-convex optimization problem, for which we propose a
custom algorithm. We approach the collision-finding problem with a descent algorithm that solves
a sequence of convex optimization problems over polytopes. This algorithm has the structural ad-
vantage of maintaining feasibility (for some adversarial budget constraint) at each step. Despite the
fact that it does not find a global minimum, the solutions found are useful for obtaining not just
approximations but actual lower bounds on optimal adversarial classification loss.
Empirical findings: We utilize our method to find numerical lower bounds on the robustness of
fixed feature extractors trained on two different datasets with both fully-connected and convolutional
architectures. Our results indicate that the use of dimensionality-reducing linear layers as well as
the effective compression induced by ReLU activations lead to significantly less robust networks.
Further, we confirm the oft-cited observation that mismatches between the adversary’s budget at
train and test time impacts robustness negatively. Taken together, these findings point towards future
design considerations for robust models.
2	Deriving lower bounds for fixed feature extractors
In this section, we develop a method for evaluating the robustness ofa feature extractor for classifica-
tion in the presence of a test-time adversary. We characterize the optimal adversarial loss achievable
by any classifier that uses the fixed feature extractor as its initial layer. This characterization is based
on a conflict graph derived from a discrete data distribution and the constraints of the adversary.
Examples and labels: We consider a supervised classification problem with a test-time adversary.
We have an example space X and a label space Y = {-1, 1}. Labeled examples are sampled from
a joint probability distribution P over X × Y .
Test-time adversary: The test-time adversary modifies a natural example subject to some con-
straints to generate an adversarial example that will be classified Goodfellow et al. (2015); Szegedy
et al. (2013); Carlini & Wagner (2017). Formally, the adversary samples a labeled example (x, y)
from P and selects X ∈ N(x), where N : X → 2x is the neighborhood function encoding the
constraints on the adversary and X is the space of adversarial examples.1 For all x, N(x) must be
nonempty. This definition encompasses the `p family of constraints widely used in previous work.
1In most (but not all) settings that have previously been studied, X = X. We believe that making the
distinction helps to clarify some of our definitions: their applicability in this general context affects what
properties they can be expected to have.
2
Under review as a conference paper at ICLR 2022
Measuring adversarial loss: We consider ‘soft’ classification functions (or classifiers) that map
examples to probability distributions over the classes. These are h : X → ∆Y , where ∆Y =
{p ∈ RY : p ≥ 0, Py∈Y py = 1}. We measure classification performance with a loss function
' :∆Y × Y → R, so the expected performance of a classifier h is E[sup江N⑺ '(h(X),y)], where
(x, y)〜 P. In the two class setting, h(X)-ι = 1 - h(X)ι, so any loss function that treats the classes
symmetrically can be expressed as '(p, y) = ' (Py). Additionally,g should be decreasing. Together,
these allow the optimization over adversarial examples to be moved inside the loss function, giving
E ['(infχ∈N(χ) h(X)y, y)]. Thus, the adversarial optimization can be analyzed by itself.
Definition 1. For a soft classifier h, the correct-classification probability qv that it achieves on
an example V = (x,y) in the presence of an adversary is qv = infχ∈N3 h(x)y. The space of
achievable correct classification probabilities is
PV,n,H = [ {q ∈ RV : ∀(x,y) ∈ v, 0 ≤ q(x,y) ≤ -infχ h(X)y}∙
h∈H
Bhagoji et al. (2021) characterized PV,N,H in the case that H is the class of measurable functions
X → ∆Y . For a data distribution P that is discrete with finite support V ⊆ X × Y, this allows the
minimum adversarial loss achievable to be expressed as an optimization over PV,N,H:
inf E(x,y)〜Ph sup '(h(X),y)i =	inf	XP({v})'0(qv).
h∈H	Lχ∈N(x)	」q∈pv,N,H v∈V
We extend their approach to analyze feature extractors as follows. Given a feature space Z and a
fixed, measurable feature extractor f : X → Z, define Hf = {h ∈ H : h = g ◦ f}: an element
ofHf is some measureable g : Z → ∆Y composed with f. Our aim is to characterize PV,N,Hf so
that we can optimize loss functions over it to evaluate the suitability of f.
Conflict graph: In Bhagoji et al. (2021), the notion of a conflict graph was used to record neigh-
borhood intersections for pairs of points from different classes. When such an intersection exists,
it is impossible for any classifier to correctly classify both of those points in the adversarial setting.
We extend this notion to apply to the family of classifiers using a fixed feature extractor f . In our
setting, a conflict exists between a pair of points when each of them has a neighbor that is mapped
to the same point in the feature space.
We call the conflict graph GV,N,Hf, where V ⊆ X × Y. When we apply GV,N,f in order to
understand classification of labeled examples with distribution P, we take V to be the support of P .
The graph is bipartite: the vertex set is partitioned into parts Vc = V ∩ (X × {c}). The edge set is
EV,N,f = {((u, 1), (v, -1)) ∈ Vi X V-ι : ∃u ∈ N(u), V ∈ N(v) such that f (U) = f (V)}.
Lemma 1 (Feasible output probabilities). The set of correct classification probability vectors for
support points V, adversarial constraint N, and hypothesis class Hf is
PV,N,H = {q ∈ RV ： q ≥ 0,q ≤ 1,Bq ≤ 1}
where B ∈ RE×V is the edge incidence matrix of the conflict graph GV,N,f.
(1)
The proof is given in Appendix A and follows the structure of the proof in Bhagoji et al. (2021).
Approximating GV,N,f and PV,N,Hf : If instead of knowing the true conflict graph GV,N,f, we
have some subgraph, then we can find a polytope that is a superset of the true PV,N,Hf. If we mini-
mize some expected loss over this proxy polytope, we obtain a lower bound on the optimal loss over
Hf . Because subgraphs of the conflict graph lead to valid lower bounds on optimal classification
performance, we can use this method to evaluate the quality of a feature extractor f even if exact
computation of the conflict graph is computationally intractable.
2.1	Distance interpretation
The construction of a conflict graph and characterization of the feasible correct classification prob-
abilities from the previous section apply to any feature extractor and neighborhood constraint. In
3
Under review as a conference paper at ICLR 2022
(a) Some fibers of f	(b) εf (u, V) With the fiber f-1(z)
Figure 1: Induced distances for f : R2 → R2, f(x0, x1) = (max(x0, 0), max(x1, 0)), (a pair of
ReLUs). Let a, b > 0. Then the fiber f-1 ({(a, b)}) is the point {(a, b)}. The fiber f-1 ({(a, 0)})
is the ray {(a, y) : y ≤ 0}. The fiber f-1 ({(0, 0)}) is the quadrant {(x, y) : x ≤ 0, y ≤ 0}. For
U = (1,4) and V = (9, -5), We have εf (u, v) = 5, U = (4,0), V = (4, -5), and z = (4,0).
the most commonly studied settings, the neighborhood constraint arises from a distance function:
Nε(χ) = {X ∈ X : d(x, x) ≤ ε}. This parameter ε is the adversarial budget constraint.
For any tWo examples U and V, a natural quantity to consider is the smallest adversarial budget that
Would cause the edge (U, V) be appear in the conflict graph:
ε*(u,v) = inf{ε ≥ 0 : Nε(u) ∩ Nε(v) = 0} = inf max(d(u, x), d(v, X)).
x∈X
We will call this the distance on X induced by d. When X = X = Rd and d(x, x0) = ∣∣x - x0kp, the
minimal adversarial budget is simply 1 ∣∣u - v∣p. Because the relationship to the distance used in
the adversarial budget constraint is so simple, this quantity is not usually discussed independently.
This definition generalizes easily to the setting of classification With a particular feature extractor,
but the resulting quantity is much more interesting.
Definition 2. The distance induced on X by d and f is the minimum adversarial budget required to
create a conflict between U and V after a feature extractor f:
εf(u,v) = inf{ε ≥ 0 : f(Nε(u))∩f(Nε(v)) = 0} = inf	max(d(u,U), d(v, V)). (2)
u,v∈Xf(u)=f(v)
This reduces to ε*(u, v) when f is the identity function, or more generally any injective function.
Observe that any choice of U and V in (2) provide an upper bound on εf (u, v), which is useful for
finding a subgraph of GV,N,f and lower bounds on optimal classification performance in Hf.
Figure 1 illustrates the computation of εf for a simple f that is related to the ReLU function.
Induced distance is not a distance on the feature space: The fact εf is defined on X is essential: it
is not possible to interpret it as a distance on Z. For a full explanation of this point, see Appendix B.
3	Distance computations for practical feature extractors
Throughout the remainder of the paper, we will work in a more concrete setting. We assume that
our example space is a real vector space and that adversarial examples are from the same space:
X = X = Rn1 . Let B ⊆ X be a nonempty, closed, convex, origin-symmetric set. These conditions
imply the zero vector in contained in B. We take the neighborhood of any point to be a scaled,
shifted version of this ball: Nε (x) : x + εB. Neighborhood constraints derived from `p norms
fit into this class: we encode them by defining B = {δ ∈ Rn1 : ∣δ∣p ≤ 1}, which results in
Nε(χ) = {X ∈ Rn1 : kX - Xkp ≤ ε}. We will focus on '2-based neighborhoods, but our algorithms
could be adapted to any choice of B over which efficient optimization is possible.
4
Under review as a conference paper at ICLR 2022
3.1	Linear feature extractors
Suppose that our feature extractor is an affine linear function of the input example: f (x) = Lx + k
for some matrix L ∈ Rn2×n1 and vector k ∈ Rn2. Then the distance εf (u, v) becomes
inf{ε ≥ 0 : {k + L(U + δ) : δ ∈ εB} ∩{k + L(U + δ0) : δ0 ∈ εB} = 0}.
Because {(ε, δ) ∈ R1+n1 : δ ∈ εB} is closed convex cone, εf (u, v) can be expressed as the
following convex optimization problem:
infε subject to δ ∈ εB,	δ0 ∈ εB,	L(δ - δ0) = L(v - U).
Because B is closed and the linear subspace is always nonempty, the infimum is achieved. Also,
it is sufficient to consider (δ, δ0) satisfying δ0 = -δ: for any feasible (ε, δ, δ0), the point (ε, (δ -
δ0)∕2, (δ0 - δ)∕2) is also feasible, has the same value, and satisfies the additional constraint. The
feasibility of the symmetrized point uses the origin-symmetry ofB. The simplified program is
min ε subject to δ ∈ εB,	Lδ = L(v - U)/2.
Thus the optimal adversarial strategy for creating conflict between U and v is intuitive: produce
examples with the same features as the midpoint (U + v)/2.
If B is the unit `2 ball, there are further simplifications. Consider the singular value decomposition
L = UΣVT where we do not include zero singular values in Σ. Then the linear map given by UΣ
is injective and can be canceled from the linear constraint on δ. The resulting program is
minε subject to ∣∣δk2 ≤ ε, VTδ = ∣VT(V — u),
the optimal choice of δ is δ = 2VVT(V - u), and εf(u, V) = 1 ∣∣VT(V — u)∣2. Observe that this
is the norm of a vector in Rn2, i.e. the feature space, contrasting with our discussion in Section B.
However, this is essentially the only case εf (U, V) simplifies into a feature space distance.
3.2	Fully connected neural networks with ReLU activations
For this feature extractor architecture, we have f = f (`) ◦ . . . ◦ f (1) where the layer i function is
f(i) : Rni → Rni+1, f(i) (z) = (k(i) + L(i)z)+. Here z+ represents the component-wise positive
part of the vector z. Then εf (U, V) is the value of the following optimization problem:
min ε subject to δ ∈ εB, δ0 ∈ εB,	(f (`) ◦ . . . ◦ f (1))(U + δ) = (f (`) ◦ . . . ◦ f (1))(V + δ0).
As in the linear case, the minimum exists because the cones are closed and the equality constraint is
feasible. In contrast with the linear case, the equality constraint is nonconvex.
The local linear approximation to f(i)(z) around a point z0 is diag(s(i,z0))(k(i)+L(i)z), where s(i,z0)
is a zero-one vector that depends on the sign pattern of k(i) + L(i)z0: s(ji,z0) = 1((k(i) +L(i)z0)j >
0). In other words, s(i,z0) is the ReLU activation pattern at layer i when z0 is the input to that layer.
Using these linear approximations, the feasible set of (δ, δ0) ∈ Rn1+n1 satisfying the constraint
f(U + δ) = f(V + δ0) can be decomposed as a union of polytopes: each activation pattern defines
a linear subspace and there are some linear inequalities specifying the region where that activation
pattern actually occurs. For a one-layer network, the linear piece for pattern s is
f(x) = diag(s)(k + Lz) for diag(2s - 1)(k + Lz) ≥ 0.
Thus one of the polytopes composing the feasible set is (δ, δ0) ∈ Rn1+n1 satisfying
diag(s)(k + L(U + δ)) = diag(s0)(k + L(V + δ0)),
(2 diag(s) -I)(k+L(U+δ)) ≥ 0,
(2 diag(s0) -I)(k+L(V+δ0)) ≥ 0.
In the one-layer case, the whole feasible region is covered by polytopes where s = s0 . Observe that
the dimension of the subspace satisfying the linear equality varies with s. When f contains multiple
layers, each polytope in the feasible set is defined by a linear equality constraint involving feature
vectors together with a linear inequality for the sign of each ReLU input.
5
Under review as a conference paper at ICLR 2022
We optimize over this feasible set with a descent algorithm: Algorithm 2 details the version for
single-layer networks. The method generalizes to multiple layers and is used to obtain our results in
Section 4.2). A full description of the general version is in Appendix C. We initialize our search in
a polytope that we know to be nonempty because it contains the feature space collision induced by
the midpoint of the two examples. Within each polytope, we can minimize the objective exactly by
solving a linear cone program. We examine the dual variables associated with the linear inequality
constraints to determine whether an adjacent polytope exists in which we can continue the search.
In the one layer case, the cone program that we solve, which depends on (L, k, u, v, s), is
minε subject to (ε, δ, δ0) ∈ R1+n1+n1 , δ ∈ εB, δ0 ∈ εB,
diag(s)L(u + δ) = diag(s)L(v + δ0),
(2 diag(s) - I)(k + L(u + δ)) ≥ 0, (2 diag(s) - I)(k + L(v + δ0)) ≥ 0.
We need not just the values of the primal solu-
tion, but the values of dual variables associated
with the linear inequalities in the solution to the
dual problem. These are called z and z0 in Al-
gorithm 2. When both zj > 0 and zj0 > 0, the
input to ReLU j is zero when both f(u+δ) and
f(v +δ0) are computed, and the objective func-
tion could be decreased further by allowing the
input to switch signs. In this case, we move to
another polytope based on the new ReLU states
and search there.
When we fail to find such pairs of dual vari-
ables, either the minimum is in the interior of
the current polytope (and thus is supported only
by cone constraints), or the minimum is on the
boundary of the current polytope but there is no
adjacent polytope in which we could continue
the search. Thus we end the search.
Algorithm 1 Descent with midpoint initialization
Input: u,v ∈ Rnι, L ∈ Rn2×n1, k ∈ Rn
Output: ε ∈ R, δ, δ0 ∈ Rn1
1:	Z4—k + 2L(U + v), ε4—2∣∣u — Vk
2:	for 0 ≤ j < n2 do
3：	Sj J I(Zj > O),
4:	end for
5:	repeat
6:	εold J ε
7:	(ε, δ, δ0, Z, Z0) J ConeLP(L, k, u, v, s)
8:	sold J s
9:	for 0 ≤ j < n2 do
10:	if Zj > 0 and Zj0 > 0 then
11:	sj J 1 - sj
12:	end if
13:	end for
14:	until sold = s or εold = ε
Algorithm termination, convergence, and complexity: The descent algorithm will terminate in a
finite number of iterations and will find a local minimum of the distance function. Since the feasible
space is non-convex, any local descent procedure is not guaranteed to find the global minimum.
The number of variables in the cone program is proportional to the number of ReLUs in the feature
extractor plus the dimension of the example space. The time-complexity of a single iteration of the
search is polynomial in the input dimension and number of ReLUs. The feasible region is the union
of a finite but exponentially large number of convex polytopes. Due to the requirement that each
iteration make progress, it is impossible to ever revisit a polytope. Thus, termination is guaranteed,
but no polynomial bound on the number of iterations is available. We suspect that as in the case of
the simplex algorithm for linear programming, input data resulting in an extremely large number of
iterations may exist, but would have a delicate structure that is unlikely to arise in practice.
3.3	Further common layers
Convolution: Itis straightforward to represent a convolutional layer as a linear layer by constructing
the appropriate Topelitz matrix, and thus our method applies directly.
Batch-normalization: Batch norm layers are simply affine functions of their inputs at test-time, and
the transformations they induce can be easily included in a linear layer. Our method thus applies.
Max pooling: Max-pool layers, like ReLUs, are piecewise linear functions of their inputs, so con-
straints coming from the equality of max-pool outputs also lead to feasible regions that are the union
of polytopes. This is a simple extension left for future work due to a lack of space.
Other activation functions and architectures: Injective activation functions such as Leaky ReLU,
ELU and sigmoid will not lead to additional collisions. Further, since they are not piecewise, a
6
Under review as a conference paper at ICLR 2022
different descent algorithm would be needed. We note that our framework cannot find collisions in
networks with both forward and backward flows, such as those with attention.
4	Evaluation
In this section, we demonstrate how the methodology outlined in the previous sections for determin-
ing the robustness of a fixed feature extractor can be used in practice. Our detailed results provide
insights on how the overall robustness of a neural network evolves through its different layers and is
impacted by the training procedure used.
4.1	From collision-finding to lower bounds
We find lower bounds on robust loss over the training set by minimizing a loss function over GV,N,f.
Vertices V from training data: The vertex set V is a representation of the training data. We use
2-class problems derived from MNIST (LeCun & Cortes, 1998) or Fashion MNIST (Xiao et al.,
2017) as in previous work (Pydi & Jog, 2020; Bhagoji et al., 2019; 2021).
Neighborhood function N: We use the common '2-norm ball constraint (Madry et al., 2018), in
which the adversary’s strength is parametrized by the radius of the ball 2.
Fixed feature extractors f: We use the composition of the layers of convolutional and fully-
connected DNNs as our fixed feature extractors (details in Appendix E). The first layer of any of
these networks (before a non-linear activation is applied) behaves as a linear feature extractor. Sub-
sequent layers behave as non-linear feature extractors. These networks are trained using one of stan-
dard cross-entropy loss minimization or robust training with either PGD-based adversarial training
(Madry et al., 2018) or the TRADES loss (Zhang et al., 2019).
Edge set E from collision finding: Algorithm 2 provides a greedy, iterative method to find col-
lisions between feature representations of pairs of points from different classes. Each successful
collision is an edge in the bipartite conflict graph. Each iteration of this algorithm can be cast as a
convex program in the form of a linear cone (as detailed in Appendix D). We solve this linear cone
program using CVXOPT (Andersen et al., 2013). Since We are working with an '2-norm constrained
adversary, we can speed up computation by projecting the inputs onto the space spanned by the right
singular vectors of the first linear layer. For linear convolutional layers, we cast the convolution
operation as a matrix multiplication to enable the use of the closed form derived in Section 3.1. We
also experimented with a modification of the Auto-PGD (Croce & Hein, 2020) algorithm to find
collisions at deeper layers with fixed budgets as done in Engstrom et al. (2019). However, this was
less effective and more expensive at finding collisions, so all further results use Algorithm 2.
Computing the lower bound from the conflict graph: The conflict graph determines the set of
possible output probabilities for each vertex (training data point) for the optimal classifier. Mini-
mizing the 0 - 1 and cross-entropy losses over this graph thus results in a lower bound over these
losses since any classifier must incur a larger loss than the optimal classifier, by definition. We
use the method from Bhagoji et al. (2021) over the conflict graphs we derive from deeper layer
representations. Results in the main body are for the cross-entropy loss (others in Appendix F).
4.2	Results
Interpreting lower bound values: The lower bound on the input space representations is computed
by considering all possible classifiers and is thus the best possible, while bounds for other represen-
tations are computed by restricting the space of possible classifiers. Intuitively, the latter will always
be larger than or equal to the former. The key metric to understanding the robustness of feature
extractors is to check how much larger the corresponding lower bounds are. The magnitude of this
difference measures the contribution of that layer to the overall network’s lack of robustness.
Robustness of linear layer representations: Using the procedure outlined in Section 3.1, we find
collisions after the first linear layer and construct a conflict graph for several models (Fig. 2) for
2We are aware of the critiques of this constraint (Gilmer et al., 2018) and only use it to compare with
previous work.
7
Under review as a conference paper at ICLR 2022
ssol yportne-ssorC
Figure 2: RobuStneSS of the repreSentation obtained from the first linear layer of a 3-layer FCNN
uSing different training procedureS
Input space
Benign training
Adv. train, C = 1.0
Adv. train, C = 2.0
Adv. train, C = 3.0
TRADES, c = 1.0
TRADES, c = 2.0
TRADES, c = 3.0
(a) MNIST
ssol yportne-ssorC
Input space
Benign training
Adv. train, C = 2.0
Adv. train, e = 3.0
TRADES, c = 2.0
TRADES, c = 3.0
3	3.5	4	4.5	5	5.5	6
(b) Fashion MNIST
ssol yportne-ssorC
Input space
First linear -θ-
First ReLU T-
Second Linear ^*~
Second ReLU -≡-
ssol yportne-ssorC
Input space -X-
FirSt linear -θ-
FirSt ReLU *
Second Linear
Second ReLU ^≡~
2	2.5	3	3.5	4	4.5	5	5.5	6
(a) MNIST
2	2.5	3	3.5	4	4.5	5	5.5	6
(b) FaShion MNIST

Figure 3: RobuStneSS of repreSentationS obtained from different layerS of a 3-layer FCNN trained
uSing benign training
both the MNIST and
FaShion MNIST dataSetS. AS expected, the linear layerS of modelS with benign
training are the leaSt robuSt, with robuStneSS increaSing with the budget train uSed during training.
Somewhat SurpriSingly, the marginal benefitS of uSing a larger budget reduce aS train increaSeS.
How does robustness evolve across layers? Having looked at the how robuSt linear layer repreSen-
tationS are, it iS pertinent to conSider other repreSentationS derived from other layerS deeper in the
network. Of particular intereSt are poSt-ReLU repreSentationS for modelS with benign (Fig. 3) and
robuSt training (Fig. 4). We find that the firSt ReLU activation layer contributeS Significantly to an
increaSe in the lower bound for both benign and robuStly trained networkS. We obServe that poSt-
ReLU repreSentationS tend to be SparSe, leading to an eaSier Search problem and a larger number of
colliSionS. The Second linear layer, on the other hand, doeS not lead to much additional increaSe in
the loSS. ThiS iS a property of the particular architecture we uSe, and a Smaller linear layer iS likely to
lead to larger increaSeS in loSS. Finally, the Second Set of ReLU activationS doeS have a meaSurable
impact on robuStneSS, particularly for the benign network. The impact of layer width on robuStneSS
iS diScuSSed in Appendix F.1.
How does the parametrization of robust training impact layer-wise robustness? AS expected,
layerS extracted from robuStly trained network are more robuSt than their benign counterpartS for
correSponding valueS of . We find a Significant difference between PGD-baSed adverSarial training
and TRADES in termS of the robuStneSS of their firSt linear layerS (Fig. 2), but thiS largely diSappearS
by the Second ReLU activation layer (Fig. 5). IntereStingly, we obServe a phenomenon where layerS
of a network robuStly trained uSing higher valueS of train can be less robuSt than thoSe uSing a lower
value, when the value of at which evaluation iS performed iS leSS than the higher train.
Impact of linear convolutional layers on robustness: AS diScuSSed in Section 3.3, the firSt con-
volutional layer can be thought aS Simply a matrix multiplication, although the Size of the reSulting
matrix can be large. We find that Since the effective dimenSion of the reSulting featureS iS much
larger than the input dimenSion for the dataSetS we conSider, the linear convolutional layer doeS not
lead to an increaSe in the lower bound (Fig. 11 in Appendix).
5 Related Work and Discussion
5.1	Related work
Lower bounds on robustness: For caSeS when the data diStribution iS Specified, Dohmatob (2019)
and Mahloujifar et al. (2019) uSe the ‘blowup’ property to determine boundS on the robuSt loSS, given
8
Under review as a conference paper at ICLR 2022

ssol yportne-ssorC
Input space -M-
First linear ^⅛-
First ReLU T-
Second Linear
Second ReLU
2	2.5	3	3.5	4	4.5	5	5.5	6
(a) MNIST
ssol yportne-ssorC
Figure 4: Robustness of representations obtained from different layers of a 3-layer FCNN trained
using PGD adversarial training with = 2.0
Input space -M-
Benign training -θ-
Adv. train, C = 1.0 —I—
Adv. train, c = 2.0 -ɪ-
Adv. train, e = 3.0
TRADES, c = 1.0 -+
2	2.5	3	3.5	4
TRADES, C = 2.0
TRADES, c = 3.0 -■
(a) MNIST
ssol yportne-ssorC
ssol yportne-ssorC
2	2.5	3	3.5	4	4.5	5	5.5	6
(b) Fashion MNIST
Input space -M-
First linear -θ-
First ReLU *
Second Linear
Second ReLU
InPUt space
Benign training
Adv. train, e = 2.0
Adv. train, e = 3.0
TRADES, c = 2.0
TRADES, C = 3.0
2	2.5	3	3.5	4	4.5	5	5.5	6
(b) Fashion MNIST
Figure 5: Robustness of the representation obtained from the second ReLU layer of a 3-layer FCNN
using different training procedures.
some level of loss on benign data. Bhagoji et al. (2019), Pydi & Jog (2020) and Bhagoji et al. (2021)
provide lower bounds on robust loss when the set of classifiers under consideration is all measurable
functions. These bounds are distribution-agnostic and do not depend on the loss on benign data.
Certification and Verification: Work on certified robustness has considered techniques for training
neural networks such that the resulting models are provably robust to perturbations upper bounded
by a given budget (Kolter & Wong, 2018; Raghunathan et al., 2018; Cohen et al., 2019; Li et al.,
2020). Typically, these models can only be certified to be robust to small budgets. In contrast, our
work provides lower bounds on the robustness of partial models which are applicable across a large
range of budgets. Approaches to verifying the robustness of neural networks (Bunel et al., 2017;
Tjeng et al., 2019; Gowal et al., 2018) are closely related to our work, but differ in that they consider
fixed end-to-end networks while we focus on a layer-by-layer analysis, allowing us to argue about
the robustness of classifiers trained on top of given feature extractors.
5.2	Discussion
Implications for training robust models: Our results indicate that layers in the network that reduce
the effective dimension of their incoming inputs have the largest negative impact on robustness (see
further results in Appendix F.1). Two prominent examples of this are ReLU layers that reduce
effective dimension by only considering non-negative outputs and fully connected layers with fewer
outputs than inputs. On the other hand, linear convolutional layers do not have a negative impact
on robustness. This indicates that not reducing the effective dimension of any deeper feature to be
lower than that of the input data is likely to benefit robustness. Further, our results confirm that
the use of larger values of train does not necessarily translate to higher robustness at lower budgets.
This points towards the need for algorithms that can effectively train a network to be robust to more
than a single adversary at a time. Finally, we find a qualitative difference in the layers learned using
PGD-training and TRADES, implying interesting learning dynamics with different robust losses.
Extending to state-of-the-art models and datasets: All of our experiments in this paper are on
simple models and datasets which demonstrate the feasibility and use of our method. However,
state-of-the art feature extractors for datasets such as Imagenet are far deeper than those considered
in this paper. Thus, our algorithm would need to be made considerably faster to handle these cases.
While our framework, can handle skip connections, networks with attention are beyond its scope.
Nevertheless, the feature extractors we consider are robust for the tasks we evaluate them on, making
our results and conclusions representative.
9
Under review as a conference paper at ICLR 2022
References
Martin S Andersen, Joachim Dahl, and Lieven Vandenberghe. Cvxopt: Python software for convex
optimization, 2013.
Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Practical black-box attacks on deep neural
networks using efficient query mechanisms. In European Conference on Computer Vision, pp.
158-174. Springer, 2018.
Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness
from optimal transport. In Advances in Neural Information Processing Systems, pp. 7496-7508,
2019.
Arjun Nitin Bhagoji, Daniel Cullina, Vikash Sehwag, and Prateek Mittal. Lower bounds on cross-
entropy loss in the presence of test-time adversaries. In Proceedings of the 38th International
Conference on Machine Learning, 2021.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine
learning. arXiv preprint arXiv:1712.03141, 2017.
Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. A unified view
of piecewise linear neural network verification. arXiv preprint arXiv:1711.00455, 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39-57. IEEE, 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International Conference on Machine Learning, pp. 2206-
2216. PMLR, 2020.
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang,
Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness bench-
mark. arXiv preprint arXiv:2010.09670, 2020.
Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. Pac-learning in the presence of adversaries.
In Advances in Neural Information Processing Systems, pp. 230-241, 2018.
Elvis Dohmatob. Generalized no free lunch theorem for adversarial robustness. In Proceedings of
the 36th International Conference on Machine Learning, pp. 1646-1654, 2019.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint
arXiv:1906.00945, 2019.
Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E Dahl. Motivating
the rules of the game for adversarial example research. arXiv preprint arXiv:1807.06732, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In ICML, 2018.
Yann LeCun and Corrina Cortes. The MNIST database of handwritten digits. 1998.
Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li. Sok: Certified robustness for deep neural networks. arXiv
preprint arXiv:2009.04131, 2020.
10
Under review as a conference paper at ICLR 2022
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration
in robust learning: Evasion and poisoning attacks from concentration of measure. In Proceedings
ofthe AAAI Conference on Artificial Intelligence, volume 33,pp. 4536-4543, 2019.
Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially robustly learnable,
but only improperly. arXiv preprint arXiv:1902.04217, 2019.
Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of
security and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016.
Muni Sreenivas Pydi and Varun Jog. Adversarial risk via optimal transport and optimal couplings.
In Proceedings of the 37th International Conference on Machine Learning, pp. 7814-7823, 2020.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In ICLR, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In ICLR, 2019.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I
Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint
arXiv:1901.08573, 2019.
11
Under review as a conference paper at ICLR 2022
A	Proofs
ProofofLemma 1. Suppose that e = ((u, 1), (v, -1)) ∈ E. Then, there are some Z ∈ Z, U ∈ N(u),
and V ∈ N(V) such that f(U) = f(V) = Z. We have qu ≤ h(f(U))ι = h(Z)ι, qv ≤ h(f (V))-ι =
h(Z)-ι, and h(Z)ι + h(Z)-ι = 1. Combining these gives the constraint (Bq)e ≤ 1, which appears
in (1).
Now, we will show that each vector q in the polytope is achievable by some h. The construction
is simple: at each point in the feature space, assign the largest possible probability to class 1: let
h(Z)ι = SuPwN∈f(N(W)) qw and h(Z)-ι = 1 - h(Z)ι. This achieves the desired performance for
examples from class 1:
inf h(f(U))ι = inf SuP	qw ≥ inf qu = qu.
u∈N(U)	u∈N(U) wf(U)∈f(N (w))	u∈N(U)
For an example is V in class -1 we have
inf h(f(V))-I = inf ( 1 - SuP	qw )
v∈N(V)	v∈N(V) ∖	w:f(V)∈f(N(w))	J
= inf inf (1 - qw)
V∈N(v) w:f (V)∈f (N(w))
= inf (1 - qw)
w：((W,D,(V,-1))∈e
≥ qV .
The final inequality uses the fact that q satisfies Bq ≤ 1.	口
B INDUCED DISTANCE IS NOT A DISTANCE ON Z
The fact εf is defined on X is essential. It is not possible to interpret the induced distance as
a distance in the feature space Z. This is because f(Nε (x)), the set of features of points near
x, cannot be derived from f (N0 (x)), the set of features of the uncorrupted version of x. This is
because distinct choices of x may lead to the same features f(N0 (x)), but f may vary more in the
neighborhood of one choice of x that the other.
An example is helpful to illustrate this point. Let X = X = R2, let Z = R, let Nε(χ) be the closed
`2 ball of radius ε around x, and let f(x0, x1) = arctan(x1/x0) for x 6= 0 (the value that we pick
for f(0, x1) is irrelevant). In other words, f finds the angle between the horizontal axis and the line
containing x. The range of values that the adversary can cause f (X) to take depends on ∣∣xk. If
∣∣x∣∣2 < ε, f (X) can be any angle from 0 to π, and if ∣∣x∣∣2 ≥ ε, |f (X) - f (x)| ≤ arcsin(ε∕∣x∣2).
This example also illustrates that εf is not necessarily a metric, even in cases where d is a metric.
Given u, V ∈ R2, we have εf (u, αu) = 0, εf (u, αu) = 0, and εf (αu, αV) can be made arbitrarily
small by selecting α to be small. Despite this, ε(u, V) will be on the order of min(∣u∣, ∣V∣).
An alternative approach to studying the induced distance is to define an adversarial classification
problem on Z by taking NεZ (z) = f (Nε (f -1 ({z}))). Note that this construction only makes sense
when X = X and thus the domain of f is X . This is more conservative: for any feature point z it
considers the worst-case X with feature z: the X in whose neighborhood f varies the most.
C Descent algorithm for multiple layer networks
We start by repeating the notion used in Section 3.2: f = f(`) ◦ . . . ◦ f(1) where the layer i function
is f(i) : Rni → Rni+1, f(i) (z) = (k(i) + L(i)z)+. Then εf (u, V) is the value of the following
optimization problem:
min ε subject to δ ∈ εB,	δ0 ∈ εB,	(f (`) ◦ . . . ◦ f(1))(u + δ) = (f (`) ◦ . . . ◦ f(1))(V + δ0).
Using the local linear approximation, the equality constraint becomes
diag(s('))(k(i) + L(')(... diag(S(I))(k(I) + L(I)(U + δ))))
=diag(s0('))(k(i) + L(')(... diag(s0(I))(k(I) + L(I))(V + δ0)))).
12
Under review as a conference paper at ICLR 2022
and for each 1 ≤ i ≤ `, the inequalities
(2 diag(s(i)) -I)(k(i) + L(i)(... diag(s(1))(k(1) + L(1)(u + δ)))) ≥ 0
(2 diag(s0(i)) -I)(k(i) + L(i)(... diag(s0(1))(k(1) + L(1)(v + δ0)))) ≥ 0
must hold in order for the linear approximation to be valid. Any collision must be in a polytope with
s(') = s0('), which is Why We only needed one set of S variables in the single layer case. However,
ReLU activation variables for earlier layers cannot be merged.
The main modification to algorithm is to the process of changing the s variables after each iteration.
The variables for all but the final layer are allowed to change independently, while the variables in
the final layer are changed together following the same rule as in the single layer algorithm.
Algorithm 2 Descent with midpoint initialization
Input: u,v ∈ Rnι, L ∈ Rn2乂×n∖, k ∈ Rn
Output: ε ∈ R, δ, δ0 ∈ Rn1
1:	Z4—k + 2L(U + v), ε4—2∣∣u — Vk
2:	for 0 ≤ j < n2 do
3:	Sj《—1(zj > 0),
4:	end for
5:	repeat
6:	εold — ε
7:	(ε, δ, δ0, z,z0) J COneLP(L ⑴...L⑶，k ⑴... k('),u, v, S(I) ... S⑶，s0 ⑴... so('T))
8:	sold J s
9:	for 1 ≤ i ≤ ` - 1 do
10:	for 0 ≤ j < ni+1 do
11:	if zj(i) > 0 then
12:	S(ji) J 1 - S(ji)
13:	end if
14:	if zj0(i) > 0 then
15:	S0j(i) J 1 - S0j(i)
16:	end if
17:	end for
18:	end for
19:	for 0 ≤ j < n'+ι do
20:	if zj') > 0 and zj(') > 0 then
21:	Sf) J 1 - sj')
22:	end if
23:	end for
24:	until Sold = S or εold = ε
D Converting collision finding to a linear cone program
In this section, we demonstrate how the collision finding problem after one linear and one ReLU
layer can be cast as a linear cone program.
We have a network with first layer v 7→ Lv + k where L ∈ Rn1 ×n0 and k ∈ Rn1 .
Given a pair of points (v0, v00) ∈ R2n0 we would like to search over the space of (δ0, δ00) ∈ R2n0
such that (L(v0 + δ0) - k)+ = (L(v00 + δ00) - k)+. This space is always nonempty because we can
take v0 + δ0 = v00 + δ00 = (v0 + v00)/2.
Let S ⊆ [nι] be the subset of active ReLUs. Let F ∈ RlSl×n1 be the inclusion indicator matrix for
the subset: Fi,j = 1 if and only ifj ∈ S and |S ∩ [j]| = i (there are exactly i elements of S strictly
smaller than j, so j is the i + 1st element of S). Let D be the diagonal matrix with Dj,j = 1 for
j ∈ S and Dj,j = -1 for j 6∈ S.
13
Under review as a conference paper at ICLR 2022
For j ∈ S (the active ReLUs), we need the constraint (L(v0 + δ0) + k)j = (L(v0 + δ0) + k)j. In
matrix form, this is F L(v0 + δ0) = F L(v00 + δ00),
For j ∈ S, we need (L(v0 + δ0) + k)j ≥ 0, (L(v00 + δ00) + k)j ≥ 0, and for j 6∈ S, we need
(L(v0 + δ0) + k)j ≤ 0, (L(v00 + δ00) + k)j ≤ 0. In matrix form, these are D(L(v0 + δ0) + k) ≥ 0,
D(L(v00 + δ00) + k) ≥ 0.
Our objective is max(kδ0k2, kδ00k2). We will replace with a linear objective by adding two additional
variables (t0 , t00) satisfying t0 ≥ kδ0 k2 and t00 ≥ kδ00 k2 .
The cvx-opt system is
minimize cT x
subject to Gx + s = h
Ax = b
s0
The cones involved are a nonnegative orthant of dimension 2n1 and two second order cones, each
of dimension	n0	+ 1. Thus s = (D(L(v0	+	δ0)	+	k), D(L(v00	+	δ00)	+	k), t0, δ0, t00, δ00).	We
then let x = (t, δ0, δ00). The matrix relating s and x is G ∈ R(n1 +n1 +1+n0 +1+n0)×(1+n0 +n0) and
s = h - Gx. The block structure is
0	-DL	0		D(Lv0 + k)
0	0	-DL		D(Lv00 + k)
一1	0	0	h =	0
0	-I	0		0
一1	0	0		0
0	0	-I		0
We use Ax = b to encode FL(δ0 一 δ00) = -FL(V 一 v00), so A ∈ rS×(I+n0+nO), b ∈ R|S|, and
A= (0 FL -FL) b= -FL(v0-v00).
Finally,
c= 00! .
D.1 FACTORIZED L
Ifn1 < n0, then we can take advantage of the fact that the rank of L is at most n1. Let L = UΣV T.
Replace L(v + δ) with UΣ(VTv + ), n0 with size of Σ.
New G and A:		
	0	-DUΣ	0	
	0	0	-DUΣ	
G =	-1	0	0 0	-I	0	A =(0 FU Σ -FU Σ)
	-1	0	0	
	0	0	-I	
Also, the length of c is reduced. The constant vectors h and b are unchanged.
E Further experimental details
We consider the following two architectures for our experiments:
1.	3-layer fully connected neural network (FCNN): 300 FC-ReLU-200 FC-ReLU-2 FC
2.	4-layer convolutional neural network (CNN): 20×5×5 conv.-BN-ReLU- 2×2 MaxPool-
50×5×5 conv.-BN-ReLU- 2×2 MaxPool- 500 FC - 2 FC
We also construct a ‘Small’ and ‘Smaller’ version of the FCNN with layers that are 2× and 4×
narrower respectively.
14
Under review as a conference paper at ICLR 2022
2	2.5	3
3.5	4
(a) MNIST
nput space -M-	Small, SL -8
Reg., FL -e- Small, SR -■
Reg., FR -∙- Smaller, FL -G
Reg., SL -~λ~- Smaller, FR T
Reg., SR -≡- Smaller, SL --t
Small, FL -θ Smaller, SR T
Small, FR -+
ssol yportne-ssorC
ssol yportne-ssorC
2	2.5	3	3.5	4
Input space -M- Small, SL -8
Reg., FL -e- Small, SR -■
Reg., FR -∙- Smaller, FL -€)
Reg., SL -⅛- Smaller, FR T
Reg., SR -≡- Smaller, SL ι
Small, FL -G Smaller, SR T
Small, FR -+
(b) Fashion MNIST
Figure 6:	Robustness of the representations obtained from fully connected networks with layers of
decreasing size.
ssol 1-0
Input space
Benign training
Adv. train, C = 1.0
Adv. train, C = 2.0
Adv. train, C = 3.0
TRADES, c = 1.0
TRADES, c = 2.0
TRADES, c = 3.0
ssol 1-0
Input space
Benign training
Adv. train, C = 2.0
Adv. train, e = 3.0
TRADES, c = 2.0
TRADES, c = 3.0
2	2.5	3	3.5	4	4.5	5	5.5	6	2	2.5	3	3.5	4	4.5	5	5.5	6
(a) MNIST	(b) Fashion MNIST
Figure 7:	0 - 1 Loss: Robustness of the representation obtained from the first linear layer of a
3-layer FCNN using different training procedures
F	Additional Results
F.1 Impact of reduction of network size
In Figure 6, we compare the robustness of representations obtained from fully connected networks
with decreasing layer sizes. The ‘Regular’ network is the one used throughout, while the ‘Small’
and ‘Smaller’ networks have corresponding layers that are 2× and 4× narrower respectively. We
can clearly see that as the width of the feature extractor decreases, so does its robustness.
F.2 0 - 1 LOSS RESULTS
In Figures 7, 8, 9 and 10 we provide lower bounds on the 0 - 1 loss in the same settings as those
considered in the main body of the paper. We note that the results and conclusions remain the same
qualitatively.
F.3 Linear convolutional layers
In Figure 11, we can see that the representations extracted from the first linear layer of a convolu-
tional network do not have any negative impact on the robustness of the overall model.
15
Under review as a conference paper at ICLR 2022
ssol 1-0
Input space _**-
First linear -0-
First ReLU +
Second Linear -λ~~
Second ReLU ^≡~
Input space ->t-
First linear -e-
First ReLU +
Second Linear -λ~-
Second ReLU ^≡~
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2	2.5	3	3.5	4	4.5	5	5.5	6
(a) MNIST
2	2.5	3	3.5	4	4.5	5	5.5	6
(b) Fashion MNIST
Figure 8:	0 - 1 Loss: Robustness of representations obtained from different layers of a 3-layer
FCNN trained using benign training
Input space ->t-
First linear -θ-
First ReLU +
Second Linear
SeCond ReLU *~
ssol 1-0
ssol 1-0
Input space ->t-
First linear -θ-
First ReLU +
Second Linear
Second ReLU *~
2	2.5	3	3.5	4	4.5	5	5.5	6	2	2.5	3	3.5	4	4.5	5	5.5	6
(a) MNIST	(b) Fashion MNIST
Figure 9:	0 - 1 Loss: Robustness of representations obtained from different layers of a 3-layer
FCNN trained using PGD adversarial training with = 2.0
ssol 1-0
Input space -M-
Benign training -θ-
Adv. train, C = 1.0 -∙~
Adv. train, C = 2.0 -t-
Adv. train, C = 3.0
TRADES, C = 1.0 -+
TRADES, C = 2.0 -*
TRADES, C = 3.0 -■
ssol 1-0
Input space
Benign training
Adv. train, e = 2.0
Adv. train, e = 3.0
TRADES, c = 2.0
TRADES, C = 3.0
2	2.5	3	3.5	4	4.5	5	5.5	6	2	2.5	3	3.5	4	4.5	5	5.5	6
(a) MNIST	(b) Fashion MNIST
Figure 10:	0 - 1 Loss: Robustness of the representation obtained from the second ReLU layer of a
3-layer FCNN using different training procedures.
ssol yportne-ssorC
Input space
Adv. train, C = 1.0
Adv. train, e = 2.0
Adv. train, c = 3.0
ssol yportne-ssorC
2	2.5	3	3.5	4	4.5	5	5.5	6	2	2.5	3	3.5	4	4.5	5	5.5	6
(a) MNIST	(b) Fashion MNIST
Figure 11:	Robustness of the representation obtained from the first linear layer of a 4-layer CNN
using different training procedures.
16