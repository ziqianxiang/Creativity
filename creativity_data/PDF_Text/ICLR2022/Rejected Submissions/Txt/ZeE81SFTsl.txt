Under review as a conference paper at ICLR 2022
DAdaQuant:	Doubly-adaptive quantiza-
tion for communication-efficient Federated
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated Learning (FL) is a powerful technique for training a model on a server
with data from several clients in a privacy-preserving manner. In FL, a server
sends the model to every client, who then train the model locally and send it back
to the server. The server aggregates the updated models and repeats the process
for several rounds. FL incurs significant communication costs, in particular when
transmitting the updated local models from the clients back to the server. Recently
proposed algorithms quantize the model parameters to efficiently compress FL
communication. These algorithms typically have a quantization level that controls
the compression factor. We find that dynamic adaptations of the quantization level
can boost compression without sacrificing model quality. First, we introduce a
time-adaptive quantization algorithm that increases the quantization level as train-
ing progresses. Second, we introduce a client-adaptive quantization algorithm
that assigns each individual client the optimal quantization level at every round.
Finally, we combine both algorithms into DAdaQuant, the doubly-adaptive quan-
tization algorithm. Our experiments show that DAdaQuant consistently improves
client→server compression, outperforming the strongest non-adaptive baselines
by up to 2.8×.
1	Introduction
Edge devices such as smartphones, remote sensors and smart home appliances generate massive
amounts of data (Wang et al., 2018b; Cao et al., 2017; Shi & Dustdar, 2016). In recent years,
Federated Learning (FL) has emerged as a technique to train models on this data while preserving
privacy (McMahan et al., 2017; Li et al., 2018).
In FL, we have a single server that is connected to many clients. Each client stores a local dataset
that it does not want to share with the server because of privacy concerns or law enforcement (Voigt
& Von dem Bussche, 2017). The server wants to train a model on all local datasets. To this end,
it initializes the model and sends it to a random subset of clients. Each client trains the model
on its local dataset and sends the trained model back to the server. The server accumulates all
trained models into an updated model for the next iteration and repeats the process for several rounds
until some termination criterion is met. This procedure enables the server to train a model without
accessing any local datasets.
Today’s neural network models often have millions or even billions (Brown et al., 2020) of param-
eters, which makes high communication costs a concern in FL. In fact, Qiu et al. (2020) suggest
that communication between clients and server may account for over 70% of energy consumption
in FL. Reducing communication in FL is an attractive area of research because it lowers bandwidth
requirements, energy consumption and training time.
Communication in FL occurs in two phases: Sending parameters from the server to clients (down-
link) and sending updated parameters from clients to the server (uplink). Uplink bandwidth usually
imposes a tighter bottleneck than downlink bandwidth. This has several reasons. For one, the av-
erage global mobile upload bandwidth is currently less than one fourth of the download bandwidth
(Speedtest). For another, FL downlink communication sends the same parameters to each client.
Broadcasting parameters is usually more efficient than the accumulation of parameters from differ-
1
Under review as a conference paper at ICLR 2022
ent clients that is required for uplink communication (Amiri et al., 2020; Reisizadeh et al., 2019).
For these reasons, we seek to compress uplink communication.
Client A 亡	IClient B 目
PA 8 [0, 1]	PB 8 [0,1]
qA =8	Server	qB =8
Q (PA)—× j] ɪ ʌ一× 5— Q(PB)
Q(P)
EpA,pB [Var (Q(P))] = 0.0018
(a) Static quantization.
(b) Client-adaptive quantization.
Figure 1: Static quantization vs. client-adaptive quantization when accumulating parameters pA and
pB. (a): Static quantization uses the same quantization level for pA and pB . (b) Client-adaptive
quantization uses a slightly higher quantization level for pB because pB is weighted more heavily.
This allows us to use a significantly lower quantization level qA for pA while keeping the quanti-
zation error measure EpA,pB [Var (Q(p))] roughly constant. Since communication is approximately
proportional to qA + qB , client-adaptive quantization communicates less data.
A large class of compression algorithms for FL
apply some lossy quantizer Q, optionally fol-
lowed by a lossless compression stage. Q usu-
ally provides a “quantization level” hyperparam-
eter q to control the coarseness of quantization
(e.g. the number of bins for fixed-point quanti-
zation). When q is kept constant during train-
ing, we speak of static quantization. When
q changes, we speak of adaptive quantization.
Adaptive quantization can exploit asymmetries
in the FL framework to minimize communica-
tion. One such asymmetry lies in FL’s train-
ing time, where Jhunjhunwala et al. (2021) ob-
served that early training rounds can use a lower
q without affecting convergence. Figure 2 il-
lustrates how time-adaptive quantization lever-
ages this phenomenon to minimize communi-
cation. Another asymmetry lies in FL’s client
space, because most FL algorithms weight client
contributions to the global model proportional to
their local dataset sizes. Figure 1 illustrates how
client-adaptive quantization can minimize the
quantization error. Intuitively, FL clients with
greater weighting should have a greater commu-
Figure 2: Time-adaptive quantization. A small
quantization level (q) decreases the loss with less
communication than a large q, but converges to
a higher loss. This motivates an adaptive quanti-
zation strategy that uses a small q as long as it is
beneficial and then switches over to a large q. We
generalize this idea into an algorithm that mono-
tonically increases q based on the training loss.
nication budget and our proposed client-adaptive quantization achieves this in a principled way.
To this end, we introduce the expected variance of an accumulation of quantized parameters,
E[Var(P Q(p))], as a measure of the quantization error. Our client-adaptive quantization algorithm
then assigns clients minimal quantization levels, subject to a fixed E[Var(P Q(p))]. This lowers the
amount of data communicated from clients to the server, without increasing the quantization error.
DAdaQuant (Doubly Adaptive Quantization) combines time- and client-adaptive quantization with
an adaptation of the QSGD fixed-point quantization algorithm to achieve state-of-the-art FL uplink
compression. In this paper, we make the following contributions:
•	We introduce the concept of client-adaptive quantization and develop algorithms for time- and
client-adaptive quantization that are computationally efficient, empirically superior to existing
algorithms, and compatible with arbitrary FL quantizers. Our client-adaptive quantization is prov-
ably optimal for stochastic fixed-point quantizers.
2
Under review as a conference paper at ICLR 2022
•	We create Federated QSGD as an adaptation of the stochastic fixed-point quantizer QSGD that
works with FL. Federated QSGD outperforms all other quantizers, establishing a strong baseline
for FL compression with static quantization.
•	We combine time- and client-adaptive quantization into DAdaQuant. We demonstrate
DAdaQuant’s state-of-the-art compression by empirically comparing it against several compet-
itive FL compression algorithms.
2	Related Work
FL research has explored several approaches to reduce communication. We identify three general
directions.
First, there is a growing interest of investigating FL algorithms that can converge in fewer rounds.
FedAvg (McMahan et al., 2017) achieves this with prolonged local training, while FOLB (Nguyen
et al., 2020) speeds up convergence through a more principled client sampling. Since communication
is proportional to the number of training rounds, these algorithms effectively reduce communication.
Secondly, communication can be reduced by reducing the model size because the model size is
proportional to the amount of training communication. PruneFL (Jiang et al., 2019) progressively
prunes the model over the course of training, while AFD (Bouacida et al., 2021) only trains sub-
models on clients.
Thirdly, it is possible to directly compress FL training communication. FL compression algorithms
typically apply techniques like top-k sparsification (Malekijoo et al., 2021; Rothchild et al., 2020)
or quantization (Reisizadeh et al., 2019; Shlezinger et al., 2020) to parameter updates, optionally
followed by lossless compression. Our work applies to quantization-based compression algorithms.
It is partially based on QSGD (Alistarh et al., 2017), which combines lossy fixed-point quanti-
zation with a lossless compression algorithm to compress gradients communicated in distributed
training. DAdaQuant adapts QSGD into Federated QSGD, which works with Federated Learning.
DAdaQuant also draws inspiration from FedPAQ (Reisizadeh et al., 2019), the first FL framework
to use lossy compression based on model parameter update quantization. However, FedPAQ does
not explore the advantages of additional lossless compression or adaptive quantization. UVeQFed
(Shlezinger et al., 2020) is an FL compression algorithm that generalizes scalar quantization to
vector quantization and subsequently employs lossless compression with arithmetic coding. Like
FedPAQ, UVeQFed also limits itself to a single static quantization level.
Faster convergence, model size reduction and communication compression are orthogonal tech-
niques, so they can be combined for further communication savings. For this paper, we limit the
scope of empirical comparisons to quantization-based FL compression algorithms.
For quantization-based compression for model training, prior works have demonstrated that DNNs
can be successfully trained in low-precision (Banner et al., 2018; Gupta et al., 2015; Sun et al.,
2019). There are also several adaptive quantization algorithms for training neural networks in a
non-distributed setting. Shen et al. (2020) use different quantization levels for different parameters
of a neural network. FracTrain (Fu et al., 2020) introduced multi-dimensional adaptive quantization
by developing time-adaptive quantization and combining it with parameter-adaptive quantization.
However, FracTrain uses the current loss to decide on the quantization level. FL generally can only
compute local client losses that are too noisy to be practical for FracTrain. AdaQuantFL introduces
time-adaptive quantization to FL, but requires the global loss (Jhunjhunwala et al., 2021). To com-
pute the global loss, AdaQuantFL has to communicate with every client each round. We show in
Section 4.2 that this quickly becomes impractical as the number of clients grows. DAdaQuant’s
time-adaptive quantization overcomes this issue without compromising on the underlying FL com-
munication. In addition, to the best of our knowledge, DAdaQuant is the first algorithm to use
client-adaptive quantization.
3
Under review as a conference paper at ICLR 2022
3	The DAdaQuant method
3.1	Federated Learning
Federated Learning assumes a client-server topology with a set C = {ci|i ∈ {1, 2...N}} of N
clients that are connected to a single server. Each client ck has a local dataset Dk from the local data
distribution Dk. Given a model M with parameters p, a loss function fp(d ∈ Dk) and the local loss
Fk(P) = ∣D1k∣ Pd∈Dk fp(d), FL seeks to minimize the global loss G(P) = PN=I PDDlI Fk (p).
3.2	Federated Averaging (FedAvg)
DAdaQuant makes only minimal assumptions about the FL algorithm. Crucially, DAdaquant can
complement FedAvg (McMahan et al., 2017), which is representative of a large class of FL algo-
rithms.
FedAvg trains the model M over several rounds. In each round t, FedAvg sends the model param-
eters Pt to a random subset St of K clients who then optimize their local objectives Fk (Pt) and
send the updated model parameters Ptk+1 back to the server. The server accumulates all parameters
into the new global model pt+1 = Pk∈^t PDD .∣ pk+1 and starts the next round. Algorithm 1 lists
FedAvg in detail. For our experiments, we use the FedProx (Li et al., 2018) adaptation of FedAvg.
FedProx improves the convergence OfFedAvg by adding the proximal term μIlPk+1 - Ptk2 to the
local objective Fk(Ptk+1) in Line 20 of Algorithm 1.
3.3	Quantization with Federated QSGD
While DAdaQuant can be applied to any quantizer with a configurable quantization level, it is op-
timized for fixed-point quantization. We introduce Federated QSGD as a competitive stochastic
fixed-point quantizer on top of which DAdaQuant is applied.
In general, stochastic fixed-point quantization uses a quantizer Qq with quantization level q that splits
R≥0 and R≤0 into q intervals each. Qq(p) then returns the sign ofp and |p| stochastically rounded
to one of the endpoints of its encompassing interval. Qq(P) quantizes the vector P elementwise.
We design DAdaQuant’s quantization stage based on QSGD, an efficient fixed-point quantizer for
state-of-the-art gradient compression. QSGD quantizes a vector P in three steps:
1.	Quantize P as Qq(∣∙pp^) into q bins in [0,1], storing signs and ∣∣p∣∣2 separately. (lossy)
2.	Encode the resulting integers with 0 run-length encoding. (lossless)
3.	Encode the resulting integers with Elias ω coding. (lossless)
QSGD has been designed specifically for quantizing gradients. This makes it not directly applica-
ble to parameter compression. To overcome this limitation, we apply difference coding to uplink
compression, first introduced to FL by FedPAQ. Each client ck applies Qq to the parameter updates
Ptk+1 - Pt (cf. Line 21 of Algorithm 1) and sends them to the server. The server keeps track of the
previous parameters Pt and accumulates the quantized parameter updates into the new parameters as
Pt+1 = Pt + Pfc∈st PDDj Qq(Pk+1 — Pt) (cf. Line 11of Algorithm 1). We find that QSGD works
well with parameter updates, which can be regarded as an accumulation of gradients over several
training steps. We call this adaptation of QSGD Federated QSGD.
3.4	Time-adaptive quantization
Time-adaptive quantization uses a different quantization level qt for each round t of FL training.
DAdaQuant chooses qt to minimize communication costs without sacrificing accuracy. To this end,
we find that lower quantization levels suffice to initially reduce the loss, while partly trained models
require higher quantization levels to further improve (as illustrated in Figure 2). FracTrain is built
on similar observations for non-distributed training. Therefore, we design DAdaQuant to mimic
FracTrain in monotonically increasing qt as a function of t and using the training loss to inform
increases in qt .
4
Under review as a conference paper at ICLR 2022
1 I 2 I 3 I 4 I 5 I
Client
A
B
C
D
Round
Samples
1
2
3
4
1 I 2 I 3 I 4 I 5 I
Quantization level
Client
A
B
C
D
Round
Samples
1
2
3
4
Quantization level
8
8
8
8
8
8
8
8
8
8
1
1
2
2
2
2
4
4
8
8
(b) Time-adaptive quantization.
(a) Static quantization.
Round
Round
4 I 5
1	2	3
1 I 2 I 3 I 4 I 5 I
CIient	Samples	Quantization level	Client	Samples	Quantization level
6
1
A
B
C
D
1
2
3
4
A
B
C
D
1
2
3
4
1
1
2
2
7
9
7
9
7
9
3
5
7
9
6
9
9
1
(c) Client-adaptive quantization.
(d) Time-adaptive and client-adaptive quantization.
Figure 3: Exemplary quantization level assignment for 4 FL clients that train over 5 rounds. Each
round, two clients get sampled for training.
When q is too low, FL converges prematurely. Like FracTrain, DAdaQuant monitors the FL loss
and increases q when it converges. Unlike FracTrain, there is no single centralized loss function
to evaluate and unlike AdaQuantFL, we do not assume availability of global training loss G(pt).
Instead, We estimate G(Pt) as the average local loss Gt = P
|Dk|
k∈St PrW
Fk (pt) where St is the
set of clients sampled at round t. Since St typically consists of only a small fraction of all clients,
G^t is a very noisy estimate of G(Pt). This makes it unsuitable for convergence detection. Instead,
DAdaQuant tracks a running average loss Gt
ψGt-i + (1 - ψ)Gt.


We initialize q1 = qmin for some qmin ∈ N. DAdaQuant determines training to converge Whenever
Gt ≥ Gt+1-φ for some φ ∈ N that specifies the number of rounds across Which We compare G.
On convergence, DAdaQuant sets qt = 2qt-1 and keeps the quantization level fixed for at least φ
rounds to enable reductions in G to manifest in G. Eventually, the training loss converges regardless
of the quantization level. To avoid unconstrained quantization increases on convergence, We limit
the quantization level to qmax.
The folloWing equation summarizes DAdaQuant’s time-adaptive quantization:
t=0
{qmiπ
2qt-1
qt-1
公.

t > 0 and Gt-1 ≥ Gt-φ andt > φ and 2qt-1 < qmax and qt-1 = qt-φ
else
3.5	Client-adaptive quantization
FL algorithms typically accumulate each parameter pi over all clients into a weighted average
p = PiK=1 wipi (see Algorithm 1). Quantized FL accumulates quantized parameters Qq(p) =
PiK=1 wiQq(pi) where q is the quantization level. We define the quantization error eqp = |p - Qq(p)|.
We observe in our experiments that communication cost per client is roughly a linear function of
Federated QSGD’s quantization level q. This means that the communication cost per round is pro-
portional to Q = Kq. We call Q the communication budget and use it as a proxy measure of
communication cost.
Client-adaptive quantization dynamically adjusts the quantization level of each client. This means
that even Within a single round, each client ck can be assigned a different quantization level qk.
The previous definitions then generalize to Q = PkK=1 qk and Qq1...qK (p) = PiK=1 wiQqi (pi) and
eqp1...qK = |p - Qq1...qK(p)|.
Prior convergence results for distributed training and FL rely on an upper bound b on
Var(QqI qK (P)) that determines the convergence speed Li et al. (2017); Horvath et al. (2019); Rei-
sizadeh et al. (2019). This makes V(Qq1...qK (p)) a natural measure to optimize for When choosing qk.
5
Under review as a conference paper at ICLR 2022
We optimize for the closely related measure Ep1...pK [Var(Qq1...qK (p))] that replaces the upper bound
with an expectation over parameters p1 . . . pK . Heuristically, we expect an this averaged measure
to provide a better estimate of practically observed quantization errors than an upper bound. For
a stochastic, unbiased fixed-point compressor like Federated QSGD, Ep1...-pK [Var(Qq1...qK (p))]
equals Ep1...pK [Var(eqp)] and can be evaluated analytically.
We devise an algorithm that chooses qk to minimize Q subject to Ep1...pK [Var(eqp1...qK)] =
Ep1...pK [Var(eqp)] for a given q. Thus, our algorithm effectively minimizes communication costs
while maintaining a quantization error similar to static quantization. Theorem 1 provides us with an
analytical formula for quantization levels q1 . . . qK .
Theorem 1. Given parameters pi .. .pk 〜U[-1, t] and quantization level q, minqι...qκ PK=I q%
subject to Epι...pκ [Var(ep1 …qK)] = Epi...pκ [Var(ep)] is minimized by qi = Pa × w2/3 where
K	2/3	K	wj2
a = ∑2j=ι Wj and b = ∑2j=ι qρ^ ∙
DAdaQuant applies Theorem 1 to lower communication costs while maintaining the same loss as
static quantization does with a fixed q. To ensure that quantization levels are natural numbers,
DAdaQUant approximates the optimal real-valued solution as q% = max(1, round(pa × w2/3)).
Appendix B gives a detailed proof of Theorem 1. To the best of our knowledge, DAdaQuant is the
first algorithm to use client-adaptive quantization.
Algorithm 1: The FedAvg and DAdaQuant algorithms. The uncolored lines list FedAvg.
Adding the colored lines creates DAdaQuant. ■ — quantization, ■ — client-adaptive
quantization, ■ — time-adaptive quantization.
1	Function RunServer()
2	Initialize Wi = PIDD I for all i ∈ [1,...,N];
j | j|
3	for t = 0, . . . , T - 1 do
4	Choose St ⊂ C with |St | = K, including each ck ∈ C with uniform probability;
{qmiπ	t = 0
2qt-1 t > 0 and Gt-I ≥ Gt-φ and t > φ and qt ≤ qmax and qt -1 = qt-φ ;
qt-i	else
6
7
8
9
10
11
12
13
14
for Ck ∈ St do in parallel
qk _ G=I j Pj=i W
Send(ck, Pt,qtk);
Receive(ck, p"G);
end
Pt+i — Pk∈St WkPk+ι;
^.	__ ^.τ
Gt{一 Pk∈St wkGt
∫G0
lψGt-i + (I- ψ)Gt
t = 0
else
end
Gt《—
15 end
16
17
18
19
20
21
22
Function RunClient(ck )
while True do
Receive(Server, Pt, qtk);
Gk4—Fk(Pt);
Pk+i4-Fk (pk+ι) trained with SGD for E epochs with learning rate η;
Send(Server, Qqk(Pk+J ,G”
end
23 end
6
Under review as a conference paper at ICLR 2022
3.6 Doubly-adaptive quantization (DAdaQuant)
DAdaQuant combines the time-adaptive and client-adaptive quantization algorithms described in the
previous sections. At each round t, time-adaptive quantization determines a preliminary quantization
level qt. Client-adaptive quantization then finds the client quantization levels qtk, k ∈ {1, . . . , K}
that minimize PiK=1 qi subject to Ep1...pK [Var(eqp1...qK)] = Ep1...pK [Var(eqp)]. Algorithm 1 lists
DAdaQuant in detail. Figure 3 gives an example of how our time-adaptive, client-adaptive and
doubly-adaptive quantization algorithms set quantization levels.
Reisizadeh et al. (2019) prove the convergence of FL with quantization for convex and non-convex
cases as long as the quantizer Q is (1) unbiased and (2) has a bounded variance. These conver-
gence results extend to DAdaQuant when combined with any quantizer that satisfies (1) and (2) for
DAdaQuant’s minimum quantization level q = 1. Crucially, this includes Federated QSGD.
We highlight DAdaQuant’s low overhead and general applicability. The computational overhead is
dominated by an additional evaluation epoch per round per client to compute Gt , which is negligible
when training for many epochs per round. In our experiments, we observe computational overheads
of ≈ 1% (see Appendix A.2). DAdaQuant can compliment any FL algorithm that trains models
over several rounds and accumulates a weighted average of client parameters. Most FL algorithms,
including FedAvg, follow this design.
4	Experiments
4.1	Experimental details
Evaluation We use DAdaQuant with Federated QSGD to train different models with FedProx on
different datasets for a fixed number of rounds. We monitor the test loss and accuracy at fixed
intervals and measure uplink communication at every round across all devices.
Models & datasets We select a broad and diverse set of five models and datasets to demonstrate
the general applicability of DAdaQuant. To this end, we use DAdaQuant to train a linear model,
CNNs and LSTMs of varying complexity on a federated synthetic dataset (Synthetic), as well as
two federated image datasets (FEMNIST and CelebA) and two federated natural language datasets
(Sent140 and Shakespeare) from the LEAF (Caldas et al., 2018) project for standardized FL re-
search. We refer to Appendix A.1 for more information on the models, datasets, training objectives
and implementation.
System heterogeneity In practice, FL has to cope with clients that have different compute capabil-
ities. We follow Li et al. (2018) and simulate this system heterogeneity by randomly reducing the
number of epochs to E0 for a random subset S0t ⊂ St of clients at each round t, where E0 is sampled
from [1, . . . , E] and |S0t| = 0.9K.
Baselines We compare DAdaQuant against competing quantization-based algorithms for FL param-
eter compression, namely Federated QSGD, FedPAQ (Reisizadeh et al., 2019), GZip with fixed-
point quantization (FxPQ + GZip), UVeQFed (Shlezinger et al., 2020) and FP8. Federated QSGD
(see section 3.3) is our most important baseline because it outperforms the other algorithms. Fed-
PAQ only applies fixed-point quantization, which is equivalent to Federated QSGD without lossless
compression. Similarly, FxPQ + GZip is equivalent to Federated QSGD with Gzip for its lossless
compression stages. UVeQFed generalizes scalar quantization to vector quantization, followed by
arithmetic coding. We apply UVeQFed with the optimal hyperparameters reported by its authors.
FP8 (Wang et al., 2018a) is a floating-point quantizer that uses an 8-bit floating-point format de-
signed for storing neural network gradients. We also evaluate all experiments without compression
to establish an accuracy benchmark.
Hyperparameters With the exception of CelebA, all our datasets and models are also used by Li
et al.. We therefore adopt most of the hyperparameters from Li et al. and use LEAF’s hyperparame-
ters for CelebA Caldas et al. (2018). For all experiments, we sample 10 clients each round. We train
Synthetic, FEMNIST and CelebA for 500 rounds each. We train Sent140 for 1000 rounds due to
slow convergence and Shakespeare for 50 rounds due to rapid convergence. We use batch size 10,
learning rates 0.01, 0.003, 0.3, 0.8,0.1 and μs (FedProx's proximal term coefficient) 1,1,1, 0.001, 0
7
Under review as a conference paper at ICLR 2022
	Synthetic		FEMNIST		Sent140	
Uncompressed	78.3 ± 0.3	12.2 MB	77.7 ± 0.4	132.1 GB	69.7 ± 0.5	43.9 GB
Federated QSGD	-0.1 ± 0.1	17×	+0.7 ± 0.5	2809×	-0.0 ± 0.5	90×
FP8	+0.1 ± 0.4	4.0× (0.23 ×)	-0.1 ± 0.4	4.0× (0.00×)	-0.2 ± 0.5	4.0× (0.04×)
FedPAQ (FXPQ)	-0.1 ± 0.1	6.4× (0.37 ×)	+0.7 ± 0.5	11× (0.00××)	-0.0 ± 0.5	4.0× (0.04×)
FXPQ + GZip	-0.1 ± 0.1	14× (0.82 ×)	+0.6 ± 0.2	1557× (0.55 ×)	-0.0 ± 0.6	71× (0.79 ×)
UVeQFed		-0.5 ± 0.2	0.6× (0.03 ×)	-2.8 ± 0.5	12× (0.00××)	+0.0 ± 0.2	15× (0.16×)
DAdaQuant	-0.2 ± 0.4	48× (2.81 ××)	+0.7 ± 0.1	4772× (1.70 ×)	-0.1 ± 0.4	108× (1.19×)
DAdaQuanttime	-0.1 ± 0.5	37× (2.16 ×)	+0.8 ± 0.2	4518× (1.61 ×)	-0.1 ± 0.6	93× (1.03×)
DAdaQuantdientS	+ 0.0 ± 0.3	26× (1.51 ×)	+0.7 ± 0.4	3017× (1.07 ×)	+0.1 ± 0.6	105× (1.16 ×)
	Shakespeare			Celeba		
Uncompressed	49.9 ± 0.3	267.0 MB	90.4 ± 0.0	12.6 GB		
Federated QSGD	-0.5 ± 0.6	9.5×	-0.1 ± 0.1	648 ×		
FP8	-0.2 ± 0.4	4.0× (0.42 ×)	+ 0.0 ± 0.1	4.0× (0.01 ×)		
FedPAQ (FxPQ)	-0.5 ± 0.6	3.2× (0.34 ×)	-0.1 ± 0.1	6.4× (0.01 ×)		
FxPQ + GZip	-0.5 ± 0.6	9.3× (0.97 ×)	-0.1 ± 0.2	494× (0.76××)		
UVeQFed		-0.0 ± 0.4	7.9× (0.83 ×)	-0.4 ± 0.3	31× (0.05 ×)		
DAdaQuant	-0.6 ± 0.5	21× (2.21 ××)	-0.1 ± 0.1	775× (1.20××)-		
DAdaQuanttime	-0.5 ± 0.5	12× (1.29 ×)	-0.1 ± 0.2	716× (1.10××)		
DAdaQuantclients	-0.4 ± 0.5	16× (1.67 ×)	-0.1 ± 0.0	700× (1.08××)		
Table 1: Top-1 test accuracies and total client→server communication of all baselines, DAdaQuant,
DAdaQuanttime and DAdaQuantclients. Entry x ± y p× (q××) denotes an accuracy difference ofx%
w.r.t. the uncompressed accuracy with a standard deviation of y%, a compression factor of p w.r.t.
the uncompressed communication and a compression factor of q w.r.t. Federated QSGD.
for Synthetic, FEMNIST, Sent140, Shakespeare, CelebA respectively. We randomly split the local
datasets into 80% training set and 20% test set.
To select the quantization level q for static quantization with Federated QSGD, FedPAQ and FxPQ
+ GZip, we run a gridsearch over q = 1, 2, 4, 8, . . . and choose for each dataset the lowest q for
which Federated QSGD exceeds uncompressed training in accuracy. We set UVeQFed’s “coding
rate” hyperparameter R = 4, which is the lowest value for which UVeQFed achieves negligible
accuracy differences compared to uncompressed training. We set the remaining hyperparameters of
UVeQFed to the optimal values reported by its authors. Appendix A.4 shows further experiments
that compare against UVeQFed with R chosen to maximize its compression factor.
For DAdaQuant’s time-adaptive quantization, we set ψ to 0.9, φ to 1/10th of the number of rounds
and qmax to the quantization level q for each experiment. For Synthetic and FEMNIST, we set qmin
to 1. We find that Sent140, Shakespeare and CelebA require a high quantization level to achieve top
accuracies and/or converge in few rounds. This prevents time-adaptive quantization from increasing
the quantization level quickly enough, resulting in prolonged low-precision training that hurts model
performance. To counter this effect, we set qmin to qmax/2. This effectively results in binary time-
adaptive quantization with an initial low-precision phase with q = qmax /2, followed by a high-
precision phase with q = qmax.
4.2	Results
We repeat the main experiments three times and report average results and their standard deviation
(where applicable). Table 1 shows the highest accuracy and total communication for each experi-
ment. Figure 4 plots the maximum accuracy achieved for any given amount of communication.
Baselines Table 1 shows that the accuracy of most experiments lies within the margin of error
of the uncompressed experiments. This reiterates the viability of quantization-based compression
algorithms for communication reduction in FL. For all experiments, Federated QSGD achieves a
significantly higher compression factor than the other baselines. The authors of FedPAQ and UVe-
QFed also compare their methods against QSGD and report them as superior. However, FedPAQ is
compared against “unfederated” QSGD that communicates gradients after each local training step
and UVeQFed is compared against QSGD without its lossless compression stages.
Time-adaptive quantization The purely time-adaptive version of DAdaQuant, DAdaQuanttime,
universally outperforms Federated QSGD and the other baselines in Table 1, achieving comparable
accuracies while lowering communication costs. DAdaQuanttime performs particularly well on Syn-
8
Under review as a conference paper at ICLR 2022
Synthetic
FEMNIST
O
Sent140
0.0	0.2	0.4	0.6	0	20	40	0	200	400
Communication in Megabytes	Communication in Megabytes	Communication in Megabytes
---- Federated QSGD ---- DAdaQuant
Figure 4: Communication-accuracy trade-off curves for training on FEMNIST with Federated
QSGD and DAdaQuant. We plot the average highest accuracies achieved up to any given
amount of client→server communication. Appendix A.3 shows curves for all datasets, as well as
DAdaQuanttime and DAdaQuantclients, with similar results.
thetic and FEMNIST, where it starts from the lowest possible quantization level q = 1. However,
binary time-adaptive quantization still measurably improves over QSGD for Sent140, Shakespeare
and Celeba.
Figure 8 in Appendix A.5 provides empirical evidence that AdaQuantFL’s communication scales
linearly with the number of clients. As a result, AdaQuantFL is prohibitively expensive for datasets
with thousands of clients such as Celeba and Sent140. DAdaQuant does not face this problem
because its communication is unaffected by the number of clients.
Client-adaptive quantization The purely time-adaptive version of DAdaQuant, DAdaQuantclients,
also universally outperforms Federated QSGD and the other baselines in Table 1, achieving
similar accuracies while lowering communication costs. Unsurprisingly, the performance of
DAdaQuantciients is correlated with the coefficient of variation Cv = μ of the numbers of samples
in the local datasets with mean μ and standard deviation σ: Synthetic (Cv = 3.3) and Shakespeare
(Cv = 1.7) achieve significantly higher compression factors than Sent140 (Cv = 0.3), FEMNIST
(Cv = 0.4) and Celeba (Cv = 0.3).
DAdaQuant DAdaQuant outperforms DAdaQuanttime and DAdaQuantclients in communication
while achieving similar accuracies. The compression factors of DAdaQuant are roughly multi-
plicative in those of DAdaQuantclients and DAdaQuanttime. This demonstrates that we can effectively
combine time- and client-adaptive quantization for maximal communication savings. Figure 4 shows
that DAdaQuant achieves a higher accuracy than the strongest baseline, Federated QSGD, for any
fixed amount of client→server communication.
5	Conclusion
We introduced DAdaQuant as a computationally efficient and robust algorithm to boost the per-
formance of quantization-based FL compression algorithms. We showed intuitively and mathe-
matically how DAdaQuant’s dynamic adjustment of the quantization level across time and clients
minimize client→server communication while maintaining convergence speed. Our experiments
establish DAdaQuant as nearly universally superior over static quantizers, achieving state-of-the-art
compression factors when applied to Federated QSGD. The communication savings of DAdaQuant
effectively lower FL bandwidth usage, energy consumption and training time. Future work may
apply and adapt DAdaQuant to new quantizers, further pushing the state of the art in FL uplink
compression.
6	Reproducibility Statement
Our submission includes a repository with the source code for DAdaQuant and for the experiments
presented in this paper. All the datasets used in our experiments are publicly available. Any post-
processing steps of the datasets are described in Appendix A.1. To facilitate the reproduction of
9
Under review as a conference paper at ICLR 2022
our results, we have bundled all our source code, dependencies and datasets into a Docker image.
The repository submitted with this paper contains instructions on how to use this Docker image and
reproduce all plots and tables in this paper.
7	Ethics Statement
FL trains models on private client datasets in a privacy-preserving manner. However, FL does not
completely eliminate privacy concerns, because the transmitted model updates and the learned model
parameters may expose the private client data from which they are derived. Our work does not
directly target privacy concerns in FL. With that said, it is worth noting that DAdaQuant does not
expose any client data that is not already exposed through standard FL training algorithms. In fact,
DAdaQuant reduces the amount of exposed data through lossy compression of the model updates.
We therefore believe that DAdaQuant is free of ethical complications.
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-efficient SGD via gradient quantization and encoding. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
Vances in Neural Information Processing Systems 30, pp. 1709-1720. Curran Associates,
Inc., 2017. URL http://papers.nips.cc/paper/6768-qsgd-communication-
efficient-sgd-via-gradient-quantization-and-encoding.pdf.
Mohammad Mohammadi Amiri, Deniz Gunduz, Sanjeev R. Kulkarni, and H. Vincent Poor. Feder-
ated learning with quantized global model updates. arXiv:2006.10672, 2020.
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of
neural networks. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 5151-5159, 2018.
Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and Nicholas D. Lane.
Flower: A friendly Federated Learning research framework. arXiv:2007.14390, 2020.
Nader Bouacida, Jiahui Hou, Hui Zang, and Xin Liu. Adaptive Federated Dropout: Improving
communication efficiency and generalization for federated learning. In IEEE INFOCOM 2021-
IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS), pp. 1-6.
IEEE, 2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a- Paper.pdf.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H. Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings.
arXiv:1812.01097, 2018.
Bokai Cao, Lei Zheng, Chenwei Zhang, Philip S Yu, Andrea Piscitello, John Zulueta, Olu Ajilore,
Kelly Ryan, and Alex D Leow. DeepMood: modeling mobile phone typing dynamics for mood
detection. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 747-755, 2017.
Yonggan Fu, Haoran You, Yang Zhao, Yue Wang, Chaojian Li, Kailash Gopalakrishnan, Zhangyang
Wang, and Yingyan Lin. FracTrain: Fractionally squeezing bit savings both temporally and
10
Under review as a conference paper at ICLR 2022
spatially for efficient DNN training. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
12127-12139. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/8dc5983b8c4ef1d8fcd5f325f9a65511-Paper.pdf.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International conference on machine learning, pp. 1737-1746.
PMLR, 2015.
Samuel Horvath, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtarik.
Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint
arXiv:1904.05115, 2019.
Divyansh Jhunjhunwala, Advait Gadhikar, Gauri Joshi, and Yonina C Eldar. Adaptive quantization
of model updates for communication-efficient federated learning. In ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3110-3114.
IEEE, 2021.
Yuang Jiang, Shiqiang Wang, Bong Jun Ko, Wei-Han Lee, and Leandros Tassiulas. Model pruning
enables efficient Federated Learning on edge devices. arXiv:1909.12326, 2019.
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training
quantized nets: A deeper understanding. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 5813-5823, 2017.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv:1812.06127, 2018.
Amirhossein Malekijoo, Mohammad Javad Fadaeieslam, Hanieh Malekijou, Morteza Homayounfar,
Farshid Alizadeh-Shabdiz, and Reza Rawassizadeh. FEDZIP: A compression framework for
communication-efficient Federated Learning. arXiv:2102.01593, 2021.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PMLR, 2017.
Hung T Nguyen, Vikash Sehwag, Seyyedali Hosseinalipour, Christopher G Brinton, Mung Chiang,
and H Vincent Poor. Fast-convergent federated learning. IEEE Journal on Selected Areas in
Communications, 39(1):201-218, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Cur-
ran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-
an-imperative-style-high-performance-deep-learning-library.pdf.
Xinchi Qiu, Titouan Parcolle, Daniel J Beutel, Taner Topa, Akhil Mathur, and Nicholas D Lane. A
first look into the carbon footprint of federated learning. arXiv preprint arXiv:2010.06537, 2020.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
FedPAQ: A communication-efficient Federated Learning method with periodic averaging and
quantization. arXiv:1909.13014, 2019.
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman,
Joseph Gonzalez, and Raman Arora. FetchSGD: Communication-efficient federated learning
with sketching. In International Conference on Machine Learning, pp. 8253-8265. PMLR, 2020.
Jianghao Shen, Yue Wang, Pengfei Xu, Yonggan Fu, Zhangyang Wang, and Yingyan Lin. Fractional
skipping: Towards finer-grained dynamic CNN inference. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pp. 5700-5708, 2020.
11
Under review as a conference paper at ICLR 2022
Weisong Shi and Schahram Dustdar. The promise of edge computing. Computer, 49(5):78-81,
2016.
Nir Shlezinger, Mingzhe Chen, Yonina C Eldar, H Vincent Poor, and Shuguang Cui. UVeQFed:
Universal vector quantization for federated learning. IEEE Transactions on Signal Processing,
69:500-514, 2020.
Speedtest. Speedtest global index. https://www.speedtest.net/global- index. Ac-
cessed: 2021-05-12.
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi
Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point
(HFP8) training and inference for deep neural networks. In Proceedings of the 33rd International
Conference on Neural Information Processing Systems, pp. 4900-4909, 2019.
Paul Voigt and Axel Von dem Bussche. The EU general data protection regulation (GDPR). A
Practical Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrish-
nan. Training deep neural networks with 8-bit floating point numbers. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 31, pp. 7675-7684. Curran Associates, Inc.,
2018a. URL http://papers.nips.cc/paper/7994- training- deep- neural-
networks-with-8-bit-floating-point-numbers.pdf.
Pan Wang, Feng Ye, and Xuejiao Chen. A smart home gateway platform for data collection and
awareness. IEEE Communications magazine, 56(9):87-93, 2018b.
12
Under review as a conference paper at ICLR 2022
A Additional simulation details and experiments
A. 1 Additional simulation details
Here, we give detailed information on the models, datasets, training objectives and implementation
that we use for our experiments. We set the five following FL tasks:
•	Multinomial logistic regression (MLR) on a synthetic dataset called Synthetic that contains vec-
tors in R60 with a label of one out of 10 classes. We use the synthetic dataset generator in Li et al.
(2018) to generate synthetic datasets. The generator samples Synthetic’s local datasets and labels
from MLR models with randomly initialized parameters. For this purpose, parameters α and β
control different kinds of data heterogeneity. α controls the variation in the local models from
which the local dataset labels are generated. β controls the variation in the local dataset samples.
We set α = 1 and β = 1 to simulate an FL setting with both kinds of data heterogeneity. This
makes Synthetic a useful testbed for FL.
•	Character classification into 62 classes of handwritten characters from the FEMNIST dataset using
a CNN. FEMNIST groups samples from the same author into the same local dataset.
•	Smile detection in facial images from the CelebA dataset using a CNN. CelebA groups samples
of the same person into the same local dataset. We note that LEAF’s CNN for CelebA uses
BatchNorm layers. We replace them with LayerNorm layers because they are more amenable to
quantization. This change does not affect the final accuracy.
•	Binary sentiment analysis of tweets from the Sent140 dataset using an LSTM. Sent140 groups
tweets from the same user into the same local dataset. The majority of local datasets in the raw
Sent140 dataset only have a single sample. This impedes FL convergence. Therefore, we filter
Sent140 to clients with at least 10 samples (i.e. one complete batch). Caldas et al. (2018); Li et al.
(2018) similarly filter Sent140 for their FL experiments.
•	Next character prediction on text snippets from the Shakespeare dataset of Shakespeare’s collected
plays using an LSTM. Shakespeare groups lines from the same character into the same local
dataset.
Table 2	provides statistics of our models and datasets.
For our experiments in Figure 8, AdaQuantFL requires a hyperparameter s that determines the
initial quantization level. We set s to 2, the optimal value reported by the authors of AdaQuantFL.
The remaining hyperparameters are identical to those used for the Synthetic dataset experiments in
Table 1.
We implement the models with PyTorch (Paszke et al., 2019) and use Flower (Beutel et al., 2020) to
simulate the FL server and clients.
Dataset Model	Parameters Clients Samples	Samples per client
						mean	min	max	stddev
Synthetic	MLR	610		30	9,600	320.0	45	5,953	1051.6
FEMNIST	2-layer CNN	6.6×	106	3,500	785,582	224.1	19	584	87.8
CelebA	4-layer CNN	6.3×	105	9,343	200,288	21.4	5	35	7.6
Sent140	2-layer LSTM	1.1 ×	106	21,876	430,707	51.1	10	549	17.1
Shakespeare	2-layer LSTM	1.3×	105	1,129	4,226,158	3743	3	66,903	6212
Table 2: Statistics of the models and datasets used for evaluation. MLR stands for “Multinomial
Logistic Regression”.
A.2 Computational overhead of DAdaQuant
Training DAdaQUanttime DAdaQUantdients Federated QSGD Total overhead
ɪs	<1ms (0.00%)^^0.17s(0.47%)	0.24 S (0.67%)	0.41s(1.14%)
Table 3:	ExecUtion time measUrements for different stages of a FL training roUnd on FEMNIST
with DAdaQUant. Each entry contains the execUtion time in seconds and as a fraction of the normal
training time.The total overhead of DAdaQUant, inclUding Federated QSGD, is ≈ 1%.
13
Under review as a conference paper at ICLR 2022
A.3 Complete communication-accuracy trade-off curves
Synthetic
00
64
.% ni ycarUccA
0.0	0.2	0.4	0.6
CommUnication in Megabytes
FEMNIST
0	20	40
CommUnication in Megabytes
Sent140
----Federated QSGD
----DAdaQUant
Shakespeare
00
42
.% ni ycarUccA
0	10	20
CommUnication in Megabytes
00
86
.% ni ycarUccA
Celeba
0	10	20
CommUnication in Megabytes
FigUre 5: CommUnication-accUracy trade-off cUrves for Federated QSGD and DAdaQUant. We plot
the average highest accUracies achieved Up to any given amoUnt of commUnication.
Synthetic
0.0	0.2	0.4	0.6
0	20	40
CommUnication in Megabytes
0	200	400
CommUnication in Megabytes
-----Federated QSGD-∣
-----DAdaQUanttime
CommUnication in Megabytes
Shakespeare
.% ni ycarUcc
00
86
.% ni ycarUccA
Celeba
CommUnication in Megabytes
FigUre 6: CommUnication-accUracy trade-off cUrves for Federated QSGD and DAdaQUanttime . We
plot the average highest accUracies achieved Up to any given amoUnt of commUnication.
14
Under review as a conference paper at ICLR 2022
Synthetic
60
40
20
FEMNIST
0	20	40
Communication in Megabytes
Celeba
Sent140
0	200	400
Communication in Megabytes
0.0	0.2	0.4	0.6
Communication in Megabytes
Shakespeare
00
42
.% ni ycarUccA
0	10	20
Communication in Megabytes
00
86
.% ni ycarUccA
0	10	20
Communication in Megabytes
-----Federated QSGD
-----DAdaQUantClients
Figure 7:	Communication-accuracy trade-off curves for Federated QSGD and DAdaQuantclients .
We plot the average highest accuracies achieved up to any given amount of communication.
A.4 Additional UVeQFed experiments
To demonstrate that the choice of UVeQFed’s “coding rate” hyperparameter R does not affect our
findings on the superior compression factors of DAdaQuant, we re-evaluate UVeQFed with R =
1, which maximizes UVeQFed’s compression factor. Our results in Table 4 show that with the
exception of Shakespeare, DAdaQuant still achieves considerably higher compression factors than
UVeQFed.
	Synthetic	FEMNIST	Sent140	Shakespeare	Celeba
Uncompressed	12.2 MB	132.1 GB	43.9 GB	267.0 MB	12.6 GB
QSGD	17 ×	2809×	90×	9.5×	648 ×
UVeQFed(R=4)	0.6× (0.03×)	12× (0.00 ×)	15× (0.16×)	7.9× (0.83 ×)	31× (0.05 ×)
UVeQFed(R=I)	13× (0.77 ×)	34× (0.01 ×)	41× (0.45 ×)	21× (2.22 ×)	93× (0.14 ×)
DAdaQuant	48× (2.81 ×)	4772× (1.70×)	108× (1.19 ×)	21× (2.21 ×)	775× (1.20 ×)
Table 4: Comparison of the compression factors of DAdaQuant, UVeQFed with R = 4 (default
value used for our experiments in Table 1) and UVeQFed with R = 1 (maximizes UVeQFed’s
compression factor). Entry p × (q ××) denotes a compression factor of p w.r.t. the uncompressed
communication and a compression factor of q w.r.t. Federated QSGD.
15
Under review as a conference paper at ICLR 2022
A.5 Additional AdaQuantFL experiments
000
21
setyboliK ni noitacinummoC
O
1±
O
1±
00
2
40
642
...
000
% ni ycaruccA
Training round
Number of clients in dataset
(a)	Comparison of the per-round-communication for
AdaQuantFL and DAdaQuant. We plot the average
client→server communication per round that is re-
quired to train an MLR model on synthetic datasets
with 10, 100, 200 and 400 clients. AdaQuantFL’s
communication increases linearly with the number of
clients because it trains the model on all clients at
each round. In contrast, DAdaQuant’s communica-
tion does not change with the number of clients.
(b)	Comparison of the convergence speed for
AdaQuantFL and DAdaQuant. We plot the test ac-
curacy while training on a synthetic dataset with 100
clients. Although AdaQuantFL has full client partic-
ipation each round, it converges only slightly faster
than DAdaQuant and achieves a similar top accu-
racy. This means that AdaQuantFL’s linear increase
in communication is not offset by a proportional re-
duction in training rounds.
Figure 8:	Scalability of AdaQuantFL vs. DAdaQuant.
In principle, AdaQuantFL could be adapted to work with partial client participation by comput-
ing an estimate of the global loss from the sampled subset of clients. While a full evaluation of
this approach is out of the scope of this paper, we conduct a brief feasibility study on FEMNIST.
Concretely, we find that a single run of AdaQuantFL with partial client participation on FEMNIST
achieved an accuracy of 78.7%, with a total client→server communication of 50.5 MB. In contrast,
the same run with DAdaQuanttime similarly achieved an accuracy of 78.4%, while lowering the total
client→server communication to 27.5 MB.
B Proofs
Lemma 1. Take arbitrary quantization level qi ∈ N and parameter pi ∈ [-t, t]. Then,
Qqi (pi) is an unbiased estimator of pi.
Proof. Let Si = qt, b = rem (pi, Si) and Ui = Si 一 bi. Then, We have
E Qqi (pi ) - pi
=ui(pi 一 bi) + — (pi + Ui)	see Figure 9
Si	Si
=Pi	□
Lemma 2. For arbitrary t > 0 and parameter Pi ∈ [一t,t], let Si = -t, b = rem(pi,Si) and
Ui = Si 一 bi. Then, Var Qqi (pi) = Uibi.
Proof.
Var Qqi (pi)
E [(Qqi(pi) - E[Qqi(Pi)])2]
E [(Q-i (Pi)- pi)2i
see Lemma 1
see Figure 9
16
Under review as a conference paper at ICLR 2022
uibi (ui+bi)
uibi
P(CSi)= N
P((C +1)Si) = si
Pi (C+ 1)Si
Figure 9: Illustration of the Bernoulli random variable Qqi (pi). si is the length of the quantization
interval. pi is rounded up to (c + 1)si with a probability proportional to its distance from csi.
Lemma 3. Assume that parameters p1 . . .pK are sampled from U[-t, t] for arbitrary t > 0. Then,
Epi...pk [Var(ep1...qK)] = t2 P=
Proof.
Ep1 ...pK [Var(ep)]
=2t Z	21t	Z ...	21t	Z Var	(X WiQqi (Pi)	- P)	dPιdP2...	dp K
Var XwiQqi(pi) -p	dp1dp2 . . . dpK
symmetry of Qqi (pi)
w.r.t. negation
wi2Var Qqi(pi) dp1dp2 . . . dpK
mutual independence of
Qqi (pi) ∀i
tn X [t [I/w2VarQi(Pi))
t i=1 0	0	0
1K	t
所 EtnT	w2var(Qqi(Pi)) dPi
t i=1	0
1K t
7∑w2	Var(Qqi(Pi)) dPi
t i=1	0
1K	t
T y？W2 / Uibi dPi
t i=1	0
1 K	si
7£JwIqi	UibidPi
t i=1	0
1 K	si
了 Tw2qi	(Si - Pi) Pi dPi
t i=1	0
1K 2	3
瓦工W qi si
亡X WL
石々q2
i=1 i
dp1 dp2 . . . dpK
exchangeability of finite
sums and integrals
Lemma 2
si-periodicity of ui and bi
□
2
□
Lemma 4. Let Q be a fixed-point quantizer. Assume that parameters P1 . . . PK are sampled from
U[-t, t] for arbitrary t > 0. Then, minq1...qK Ep1...pK [Var(eqp1...qK )] subject to Q = PiK=1 qi is
2/3
minimized by qi = Q -Kwi-中.
kK=1 wk
17
Under review as a conference paper at ICLR 2022
Proof. Define
L (q) = f (q) - λg(q) (Lagrangian)
Any (local) minimum ^ satisfies
VL (^) = 0
t2 K w2 K	K
V ^6 X 才 - λv X qi = 0 ∧ X qi = Q
Lemma 3
^⇒
^⇒
^⇒
∀i = 1 . . . n.
t2 wi2
-3源
K
λ ∧ qi = Q
i=1
wi2/3
∀i =1 ...n.qi = Q ^KI^^2/3
j=1 wj
□
B.1 Proof of Theorem 1
Proof. Using Lemma 4, it is straightforward to show that for any V , minq1...qK PiK=1 qi subject to
Ep1...pK [Var(eqp1...qK )] = V is minimized by qi = Cwi2/3 for the unique C ∈ R>0 that satisfies
Ep1...pK[Var(eqp1...qK)] =V.
Then, taking V = Epι...pκ [Var(ep)] and C = Mb (See Theorem 1), We do indeed get
Ep1...pK[Var(eqp1...qK)]
t2X	Wi
6 i=1 (Cwi/3)2
1 t2 K
上 L X w.2/3
C2 6 "w
i=1
PK w2 +2 K
乙j=1 q2 t_ X w,2/3
p3 w2/3 6 i=ι i
t2 X Wl
"6"2^ ~q2
j=1 q
Ep1 ...pK [Var(eqp)]
Lemma 3
lemma 3	□
18