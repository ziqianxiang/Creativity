Under review as a conference paper at ICLR 2022
AQUILA: Communication Efficient Feder-
ated Learning with Adaptive Quantization
of Lazily-Aggregated Gradients
Anonymous authors
Paper under double-blind review
Ab stract
The development and deployment of federated learning (FL) have been bottle-
necked by the heavy communication overheads of high-dimensional models be-
tween the distributed client nodes and the central server. To achieve better error-
communication tradeoffs, recent efforts have been made to either adaptively re-
duce the communication frequency by skipping unimportant updates, a.k.a. lazily-
aggregated quantization (LAQ), or adjust the quantization bits for each communi-
cation. In this paper, we propose a unifying communication efficient framework
for FL based on adaptive quantization of lazily-aggregated gradients (AQUILA),
which adaptively adjusts two mutually-dependent factors, the communication fre-
quency and the quantization level, in a synergistic way. Specifically, we start from
a careful investigation on the classical LAQ scheme and formulate AQUILA as
an optimization problem where the optimal quantization level per communication
is selected by minimizing the gradient loss caused by updates skipping. Mean-
while, we adjust the LAQ strategy to better fit the novel quantization criterion
and thus keep the communication frequency at an appropriate level. The effec-
tiveness and convergence of the proposed AQUILA framework are theoretically
verified. The experimental results demonstrate that AQUILA can reduce around
50% of overall transmitted bits compared to existing methods while achieving
the same level of model accuracy in a number of non-homogeneous FL scenarios,
including Non-IID data distribution and heterogeneous model architecture. The
proposed AQUILA is highly adaptive and compatible to existing FL settings.
1 Introduction
With the deployment of ubiquitous sensing and computing devices, the Internet of things (IoT) as
well as many other distributed systems have gradually grown from concept to reality, bringing dra-
matic convenience to people’s daily life (Du et al., 2020; Liu et al., 2020a; Hard et al., 2018). To fully
utilize such distributed computing resources, distributed learning provides a promising framework
that can achieve comparable performance with the traditional centralized learning scheme. How-
ever, the privacy and security of sensitive data during the updating and transmitting processes in dis-
tributed learning have been a growing concern. In this context, federated learning (FL) (McMahan
et al., 2017; Yang et al., 2019) has been developed, enabling distributed devices to collaboratively
learn a global model without privacy leakage by keeping private data sets isolated and masking
transmitted information with secure approaches like differential privacy (Abadi et al., 2016), secret
sharing techniques (Bonawitz et al., 2017) and homomorphic encryption (Liu et al., 2020b). Due
to its privacy and security preserving property and great potentials in some distributed but privacy-
sensitive fields like finance and health, FL has attracted tremendous attentions from both academia
and industry in recent years.
Unfortunately, in many FL applications like image classification and objective recognition, the mod-
els to be trained tend to be high-dimensional, which lead to heavy communication overheads, for
example, a Resnet-152 network has over 58 million parameters (He et al., 2016). Hence, com-
munication efficiency has become one of the key bottlenecks of FL. To this end, recent researches
have tried to reduce the communication frequency, for example, Sun et al. (2020) proposes the
lazily-aggregated quantization (LAQ) method to reduce communication rounds by skipping some
1
Under review as a conference paper at ICLR 2022
unnecessary parameter uploads. To further reduce transmitted bits per communication, LAQ can
be used jointly with gradient compression techniques, e.g. quantization and sparsification (Strom,
2015; Wangni et al., 2018; Lin et al., 2018; Han et al., 2020). Moreover, Mao et al. (2021) devel-
ops the Adaptive Quantized Gradient (AQG) for LAQ to adjust the quantization bit among multiple
given levels during training. However, the AQG is not sufficiently adaptive, for example, in the
two-level AQG with 4 bit and 2 bit, the situation of 3 bit and 1 bit is not covered at all. In a sepa-
rate line of work, Jhunjhunwala et al. (2021) develops an adaptive quantization rule (AdaQuantFL)
for FL which can search in a given range for an optimal quantization level and achieve a better
error-communication tradeoff.
Previous work has investigated how to optimize communication frequency or adjust quantization
levels in a highly adaptive fashion, but not both. Intuitively, we ask the question, can we adaptively
adjust the quantization level in LAQ to further reduce communication rounds and transmitted bits
simultaneously? A straightforward approach is to train LAQ jointly with state-of-the-art adaptive
quantization methods like AdaQuantFL. However, quantization level and communication frequency
are mutually dependent in establishing the model convergence and must be adjusted together. For
example, lazy aggregation leads to the skip of some gradients that no longer require quantization,
while the choice of quantization levels directly affects the quality of a gradient update and thus
whether it is selected for transmission and aggregation. Therefore, the key question of our work is
how to jointly leverage these two complementary yet mutually-dependent degrees of freedom for
further optimizing communication efficiency in FL.
The key idea of this paper is to select the optimal quantization bits for each communication round
in LAQ by optimizing the gradient loss caused by skipping quantized updates, which gives a novel
quantization criterion that can cooperate with LAQ strategy to further reduce overall transmitted
bits while maintaining the desired convergence properties of LAQ. The contributions of this paper
are trifold and summarized as follows. 1) We propose a FL framework with adaptive quantization
of lazily-aggregated gradients termed AQUILA, which simultaneously adjusts the communication
frequency and quantization level in a synergistic fashion. 2) We formulate AQUILA as an optimiza-
tion problem and develop an upperbound for the gradient loss caused by communication skipping,
which gives a novel adaptive quantization criterion which is theoretically proven to be more efficient
compared to AdaQuantFL while maintaining the same convergence properties. 3) We experimen-
tally evaluate the performance of AQUILA in a number of non-homogeneous FL settings, including
Non-IID data distribution and various heterogeneous model architecture. The experimental results
show that AQUILA can significantly mitigate the communication overhead over a number of base-
lines including fixed-bit LAQ and the naive combination of LAQ and AdaQuantFL. Our approach is
highly adaptive and compatible to existing FL settings.
2 Background and Related Work
Consider a FL system with one central server and a clients set M of M distributed clients to collab-
oratively train a global model parameterized by θ*. At iteration k, each client m ∈ M first trains
the global model θk on its local data Dm, and sends the local gradient g3 = Pfm(Dm; θk) to the
central server. Then the server aggregates the parameters and updates the global parameter by:
θk+1=θk - M X gm.
m∈M
(1)
To reduce communication overheads with gradient quantization, the stochastic uniform quantizer
(Alistarh et al., 2017) is usually adopted. For any local gradient g ∈ Rd, the quantized value of its
i-th dimension with quantization level b is defined as:
Qb (gi) = I∣gk2 ∙ sign (gi) ∙ ξi(g, b),
(2)
where ξi(g, b) is a random variable defined as follows. Let l ∈ {0, 1, 2, ..., b - 1} be an integer
satisfying |gi| /IgI2 ∈ [ l/b, (l + 1)/b ), then:
ξi (g, b) =
(' +1)∕b with probability (b ∙ IlgiII)/∣∣g∣∣2 — l
`/b
otherwise.
(3)
2
Under review as a conference paper at ICLR 2022
It is clear that with quantization level b, the number of bits for transmitting a quantized gradient from
a client to the central server is Cb = d dlog2 (b + 1)e + d + 32, with 32 bits for kgk2, 1 bit for each
sign (gi), and log2(b + 1) bits for each ξi(g, b).
For communication rounds reduction, the LAQ proposes to let the client m ∈ M upload its newly-
quantized local gradient Qb(gkm) at iteration k only when the change in local gradient is sufficiently
large, i.e.,
PD ξ θk+1-d - θk-d 2
∣∣Qb(gm)-Qb(gm-ι)ζ ≥	d=1 叫 α2M2-----------^2+3(kεb(gτ1)k2 + ∣∣εb(gm)∣∣2),⑷
where Qb(gkr-1) is the last quantized upload from client m, εb(gk-1) and εb(gkm) denote quantiza-
tion errors, and {ξd}dD=1 are some predetermined constant weights. Notice here a fixed quantization
level b is used. In LAQ, if the difference between client m’s newly-quantized local gradient Qb (gkm)
and the last upload is smaller than a threshold involving quantization errors and global model’s in-
novation, client m will skip the upload of Qb (gkm) at iteration k and the central server will reuses
Qb(gkr-1) for such lazy aggregation:
θk+1=θk - M X Qb(gm)=θk - M X	Qb(gm)- M X Qb(gm-1),⑸
m∈M	m∈M∖M0	m∈M0
where M0k denotes the subset of clients that skip the new gradient update and reuse the old quantized
gradient at iteration k. Besides, gkn represents the actual gradient for aggregation from client m,
which is gm for m ∈ M \ Mk, while gk-1 for m ∈ Mk.
Recently, AdaQuantFL is proposed to achieve a better error-communication tradeoff by adaptively
adjusting the quantization levels during FL training (Jhunjhunwala et al., 2021). Specifically,
AdaQUantFL computes iteration k's optimal quantization level bk based on the following criterion
involving training loss and initial quantization level b0 :
bk = ʌ/f (θ0MfW) ∙ bo,	(6)
where f(θ0) and f(θk) are the global training loss at iteration 0 and k, respectively.
However, AdaQuantFL transmits quantized gradients at every iteration. In order to skip unneces-
sary communication rounds and adaptively adjust quantization level for each communication jointly,
an naive approach is to quantize lazily aggregated gradients with AdaQuantFL. Nevertheless, it fails
to achieve efficient communication due to a number of reasons. Firstly, given the descending trend of
training loss, AdaQuantFL’s criterion (6) may lead to high quantization bit number even exceeding
32 late in the training process, which is too much for cases where the global convergence is already
approaching. Secondly, higher quantization level results in smaller quantization error, which will
lead to lower communication threshold in LAQ’s criterion (4) and thus higher frequency of trans-
mission. Therefore it is desirable to develop more efficient adaptive quantization method in the
lazily-aggregated setting to systematically improve communication efficiency in FL.
3 Method
3.1	Adaptive Quantization of Lazily-Aggregated Gradients (AQUILA)
Given the above limitations of the naive joint use of existing adaptive quantization criterion and lazy
aggregation strategy, this paper aims to design a unifying framework for communication efficiency
optimization where the quantization level and communication frequency are adjusted in a synergistic
and interactive way. Based on a careful investigation on LAQ, we design a novel quantization
criterion where the optimal quantization level is selected by minimizing the expected gradient loss
caused by skipping quantized updates. The rationale behind such strategy is, by formulating the
adaptive quantization problem of lazily-aggregated gradients as optimizing the expected gradient
loss with respect to the number of quantization bits, we can get an adaptive quantization criterion
based on local gradient updates while maintaining or even improving the convergence property of
3
Under review as a conference paper at ICLR 2022
LAQ. To get the optimization target, we first derive an upperbound for the expected gradient loss
in terms of quantization bits as elaborated in Section 3.2, which gives a novel adaptive quantization
criterion (7) that selects the quantization level for client m at iteration k based on initial quantization
level b0 and the change between client m’s newly computed gradient gmk and the last uploaded
gradient gm-1:
%)*=bo ∙ Jlgm - gm∣∣2/IIgm - gm-1∣∣2.
(7)
The superiority of (7) comes from the following two aspects. Firstly, the gradient updates tend
to fluctuate along with the training process instead of keeping descending like the loss value, and
thus prevent the quantization level from increasing tremendously compared with the initial level.
Secondly, with lazy aggregation criterion based on gradient updates like (4), the transmitted bits
in AQUILA is further controlled, since the gradient update for actual transmission in (7) is lower
bounded by the lazy aggregation criterion, and therefore high-bit transmission for small update is
more likely to be skipped.
To better fit the larger quantization errors induced by fewer quantization bits in (7), we modify the
communication criterion as follows to avoid the potential expansion of clients group to be skipped:
IlQbm (gkm)-Qbm-1 (gm-1)∣∣2 ≥
pD=ιξd∣∣θk+1-d-θk-d∣∣2
α2M 2
+ 2∣∣εbm (9kn)-εbm-1 (gm 1)(,⑻
where all the notations are the same as in (4) except the heterogeneous quantization level bkm and
bk-1 for each client. For detailed development of (8), please refer to the Appendix.
Figure 1: The schematic illustration of the communication efficient FL with AQUILA in comparison
with the naive combination of AdaQuantFL and LAQ. The blue lines indicating the transmission of
quantized gradients in AQUILA are drawn in different thicknesses to represent different quantization
levels selected by various clients.
The cooperation of the novel adaptive quantization criterion (7) and the modified lazy aggrega-
tion strategy (8) is illustrated in Fig. 1a. Compared to the naive combination of AdaQuantFL and
LAQ where the mutual influence between adaptive quantization and lazy aggregation has not been
considered as shown in Fig. 1b, our AQUILA framework adaptively optimizes the allocation of
quantization bits throughout training to promote the convergence of lazy aggregation, and at the
same time utilizes the lazy aggregation strategy to improve the efficiency of adaptive quantization
by skipping high-bit transmission. The proposed AQUILA’s effect of suppressing the transmission
of high quantization bits has been verified in our experiments, as shown in Fig. 20 in the Appendix.
Besides, with the adjusted lazy aggregation strategy (8), AQUILA well addresses the problem of
high communication frequency in the late training process of naive combination of AdaQuantFL
and LAQ, as indicated by Fig. 2.
The proposed AQUILA is summarized as follows in Algorithm 1. At iteration k = 0, all clients are
forced to transmit local gradients quantized with the initial level b0. At iteration k ∈ {1, 2, ..., K},
the server first broadcasts the global model θk to all clients. Each client m computes gkm with
local training data, and then uses it to select an optimal quantization level by (7). Then, each client
4
Under review as a conference paper at ICLR 2022
computes its gradient update after quantization and determines whether to upload the update or not
based on the communication criterion (8). Finally, the server updates the new global model θk+1
with up-to-date quantized gradients Qbk (gmk ) for those clients who send the uploads at iteration k,
while reusing the old quantized gradients Q^k-ι (gm-1) for those who skip the uploads.
(a) _______________Homo-MNIST-HD
(b)	Homo-ClFARlO-Non-IlD
2 0 8 6 4 2 0
1 1
u 里-ɔ pd>lsun.!qlunN
20
60
p
e
80

Figure 2: Comparison of AQUILA and AdaQuantFL+LAQ on the number of unskipped clients per
step in three experiment settings with homogeneous model architecture. AdaQuantFL clashes with
the threshold condition in LAQ and results in high communication frequency due to the increasing
quantization level late in the training process, but AQUILA keeps the communication frequency at
an appropriate level throughout the training.
Algorithm 1 Communication Efficient FL with AQUILA
Input: the number of communication rounds K, the learning rate α, the maximum communication
level bmax
Initialize: the initial global model parameter θ0 and the initial quantization level b0 .
1: Server broadcasts θ0 to all clients.
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
for each client m ∈ M in parallel do
Calculates local gradient g0m and sends the quantized gradient Qb0 (g0m).
Set gm = gm and b* = bo on both sides.
end for
for k = 1, 2, ..., K do
Server broadcasts θk to all clients.
for each client m ∈ M in parallel do
Calculates local gradient g km .
Computes the optimal local quantization level bkm by (7).
if bkm ≥ bmax then
bk = b
m = max.
end if
if (8) holds for client m then
Client m computes and sends the quantized gradient Qbk (gkm).
Set gm = gmb and bmb = bkm on both sides.
else
Client m sends nothing.
Set gkm = gm- and bmm = bm-1 on both sides.
end if
end for
Server updates θk+1 by θk - α X	ɪ Q^k (gkm).
end for
5
Under review as a conference paper at ICLR 2022
3.2 Theoretical Derivation and Analysis of AQUILA
As mentioned before, in this work we bound the expected gradient loss caused by skipping
updates with respect to quantization bits. Specifically, if the communication criterion (8) holds
for client m at iteration k, it does not contribute to iteration k’s gradient loss, otherwise, the loss
caused by client m will be minimized with the optimal quantization criterion (7). In this sec-
tion, the theoretical derivation of the target upper bound is based on following standard assumptions:
Assumption 1. Loss function f(θ) =	fm(θ) is L-smooth, and fm(θ) is Lm-smooth.
Assumption 2. The quantization operation is unbiased with E[Qb (w)|w] = w, and its variance
satisfy: ∀w ∈ Rd, E[kQb(w) - wk22 |w] ≤ qb kwk22, where qb is a positive constant.
Here we adopt the definition of B in AdaQuantFL, but modified it as follows:
Definition 1 (Number of Bits sent from client m to the server, Bm). The total number of bits that
has been sent from client m to the central server until a given time instant is denoted by Bm.
With the definition of Bm, the upper bound of the loss induced by gradient skipping is indicated by
the following theorem:
Theorem 1.	Under Assumption 1 and 2, the expected gradient loss caused by lazy aggregation has
the following upper bound:
E[∣Bk+1
3α2
-T2]≤ 后
dmd log2 (4bkm)
3α2
+MT E
m∈M
ψ + (bm)2
m∈M
dσ2
Bm
3α2
+立E
m∈M
gk - gk-1
gm	gm
dm(d + 32)
Bm
gk - gk-1
gm	gm
(9)
2
2
2
Σ
2
2
2
2
where σ2 denotes the maximum kwk22, and dm is a positive constant determined by Lm. See the
Appendix for proof details.
For each client m, the optimal bkm is selected by setting the first derivative of the upper bound (9) to
zero:
%)* = ʌ/( 2σ2Bmloge(2)) / (dm ∣∣gm - gm-11∣2 ) .	(10)
Therefore, an adaptive quantization criterion which does not rely on unknown parameters such as σ
and dm can be achieved through dividing bkm by b1m, which gives the adaptive selection criterion in
(7) if b1m for each client m is chosen as the initial quantization level b0 .
Comparison with AdaQuantFL. In AdaQuantFL, the optimal quantization level is selected by
minimizing the upper bound of the expected squared norm of the gradient Vf (θk) of a non-convex
objective function, and thus the convergence of AdaQuantFL is guaranteed. However, the proposed
AQUILA is proven to be more efficient with sufficiently small learning rates as shown in Theorem
2. Besides, it is theoretically guaranteed that the squared norm of the global gradient Vf(θk) is also
able to converge under AQUILA’ s adaptive quantization criterion developed for lazy aggregation.
Theorem 2.	Under Assumption 1 and 2, if the learning rate α satisfies α ≤ (1/ML) ∙ √2Pξ∕L,
the following inequality holds:
(*)*= bo	∣
t∣∣∣
∣∣gm - gm∣∣2 ≤ b
Igm- gm<-	0
α2M2L3∣∣θ1- θ0∣∣2 ≤ b Sf(θ0)-f(θ*)
∖	2ξ[f(θk) - f(θ*)] - 0V f(θk) - f(θ*),
(11)
where ξ and P are both positive constants. See the Appendix for proof details.
Theorem 2 indicates that with appropriate learning rate, the proposed AQUILA uses less number of
bits per communication compared to AdaQuantFL.
6
Under review as a conference paper at ICLR 2022
Convergence analysis. The Lyapunov function of AQUILA is defined in the same way as LAQ and
AQG (Mao et al., 2021):
DD	2
v(θk)= f (θk)- f (θ*) + XX ξj ∣∣θk+1-d - θk-d∣∣2,
(12)
where θ* is the optimal solution of min®f (θ).
Convergence guarantee of federated learning with lazy aggregation has been well discussed in Sun
et al. (2020). More specifically, both the objective residual f(θk) - f (θ*) and the parameter dif-
ferences term in Lyapunov function are guaranteed to descend along with the training process.
Therefore, the squared norm of the gradient Vf (θk) is also guaranteed to converge based on the
L-smoothness assumption which results in: kVf (θk)k2 ≤ 2L[f(θk) - f (θ*)].
4	Experiments and Discussion
4.1	Experiment Setup
Dataset. In this paper, we evaluate our method with MNIST and CIFAR10 dataset, considering both
IID and Non-IID data distribution. To simulate Non-IID situation, each client is assigned with two
classes of data at most and the amount of data for each class is balanced.
Parameters. We set total client number M = 10, and follow the settings in LAQ, where D = 10
and ξι = ξ2 = •…=ξD = 0.8/D. In terms of initial quantization level, a low level bo = 2 is
chosen for simpler tasks with MNIST, and a larger level b0 = 6 is selected for more complex tasks
with CIFAR10. For both AdaQuantFL and AQUILA, we set an upperbound for quantization level
as bmax = 16 in case that the level grows too high.
Training. We train a CNN with MNIST and a Resnet18 network with CIFAR10. The hyperparam-
eters of our experiments are shown in Table 1 in Appendix.
We first evaluate our proposed AQUILA with homogeneous settings where all the local models share
the same architecture as the global model. The performance of AQUILA is compared with several
state-of-the-art methods including FedAvg (McMahan et al., 2017), QSGD (Alistarh et al., 2017),
AdaQuantFL, fixed-bit LAQ and the naive combination of AdaQuantFL with LAQ. For the choice
of quantization level for fixed-bit LAQ, since AQUILA’s initial level b0 is 2 for MNIST and 6 for
CIFAR10, we compare AQUILA with LAQ-2 and LAQ-6 for MNIST and CIFAR 10 respectively.
Besides, the performance of LAQ with the upperbound bmax = 16 is also evaluated.
We also evaluate our proposed AQUILA with HeteroFL (Diao et al., 2020), where the local models
trained at clients’ side are heterogeneous. Assume the global model at iteration k is θgk and its size
is dg ∙ hg, then the local model of each client m can be selected by θk1 = θj [: dm, : hm], where
dm = rm dg and hm = rm hg respectively. In this paper, we choose three various model complexity
levels r = {a, b, c} = {1, 0.5, 0.25}.
Fig. 3 shows the training loss vs total transmitted bits curve of experiments with IID MNIST / CI-
FAR10 for homogeneous model architecture, IID MNIST / CIFAR10 for 100%-50% heterogeneous
model architecture, and Non-IID MNIST / CIFAR10 for 100%-25% heterogeneous model architec-
ture. The corresponding transmitted bits vs steps curves of the above experiment settings are shown
in Fig. 4, which represents how many bits are transmitted in each step. Methods without adaptive
quantization and lazy aggregation like FedAvg and QSGD are not included in Fig. 4 for simplicity.
All the other experiment results are provided in Appendix, with Fig. 5 to Fig. 9 for homogeneous
models, and Fig. 10 to Fig. 19 for heterogeneous models.
4.2	Performance Analysis
In this part, we analyze the performance of AQUILA with various experiment settings including
Non-IID data distribution and heterogeneous model architecture. From figures in this paper, we can
observe that:
7
Under review as a conference paper at ICLR 2022
•	AQUILA achieves a significant transmission reduction on both MNIST and CIFAR10 as
compared to AdaQuantFL and the naive combination of LAQ and AdaQuantFL. For in-
stance, AQUILA saves 65.98% of transmitted bits compared with AdaQuantFL and 62.05%
compared with the naive combination of LAQ and AdaQuantFL in the IID scenario with CI-
FAR10 dataset as shown in Fig. 3d, and other figures all show an obvious reduction in terms
of the total transmitted bits required for convergence. Fig. 4 verifies that the quantization
level selected by AQUILA will not continuously increase during training like AdaQuantFL
and LAQ with AdaQuantFL.
•	Similarly, AQUILA outperforms all fixed-level LAQ in terms of overall transmitted bits
for both MNIST and CIFAR10, as shown in Fig. 3. For example, Fig. 3a indicates that
AQUILA reduces 42.6% of total transmitted bits compared with LAQ-2 and 92.32% com-
pared with LAQ-16 in MNIST. The transmission reduction is 38.76% for LAQ-6 and
78.09% for LAQ-16 in CIFAR10 as shown in Fig. 3e. Besides, Fig. 3 and Fig. 4 indicate
that although LAQ with fixed but low quantization level like LAQ-2 and LAQ-4 sometimes
transmit smaller amount of bits per step compared with AQUILA, they suffer from lower
accuracy and slower convergence. It further verifies the necessity and effectiveness of our
well-designed adaptive quantization criterion which achieves fast convergence with similar
low-bit transmission but without degradation of the model performance.
•	AQUILA also works well with heterogeneous local models. With the two various ways
of distributing the heterogeneous local models in this paper, our proposed AQUILA still
outperforms other methods by significantly reducing overall transmitted bits while main-
taining the same convergence property and model accuracy. Please refer to Fig. 10 to
Fig. 19 in the Appendix for more detailed information. These experimental results in non-
homogeneous FL settings prove that our proposed AQUILA can be used in a more general
and complicated federated learning scenarios.
(a)
(b)
(c)	100%-25%-MNIsτ-Noπ-IID Parameters
Homo-MNIST-IID Parameters
SSon Buue=
sso"∣ BU 三 E
Total Transmitted Bits(GB)
(e)
100%-50%-MNIST-IID Parameters
Total Transmitted Bits(GB)
(d)	Homo-CIFARlO-IID Parameters
1OO%-5O%-C∣FAR1O-IID Parameters
0.0	0,5	1.0	1.5	2.0	2.5	3.0
total Transmitted Bits(GB)
⑴	100%-25%-CIFAR10-Non-IID Parameters
200	400	600	800	1000	1200	0	100	200	300	400	58	600	700 B∞	0	5	10	15
Total Transmitted Bits(GB)	Total Transmitted Bits(GB)	Total Transmitted Bits(GB)
Figure 3: Training Loss vs Total Transmitted Bits. In this figure, LAQ-2 and LAQ-16 represent
LAQ with fixed quantization level of 2 and 16 respectively. AdaQuantFL+LAQ represents the naive
combination of AdaQuantFL and LAQ. For heterogeneous model architecture, 100%-50% implies
that half of clients have 100% of global model, whereas the other half of clients just share 50% * 50%
of the global model. Similarly, 100%-25% means the other half of clients just share 25% * 25% of
the global model. Particularly, we zoom in the end of the curves to better compare AQUILA with
other methods.
8
Under review as a conference paper at ICLR 2022
5	Conclusions and Future Work
This paper proposes a communication efficient FL framework to simultaneously adjust two
mutually-dependent degrees of freedom: communication frequency and quantization level. With the
close cooperation of the novel adaptive quantization and adjusted lazy aggregation strategy devel-
oped in this paper, the proposed AQUILA has been proven to be capable of reducing the transmitted
bits while maintaining the same convergence property and model performance compared against ex-
isting methods both theoretically and experimentally. The evaluation with Non-IID data distribution
and various heterogeneous model architectures demonstrates that AQUILA is compatible to existing
FL settings. Future works include further improvements and theoretical guarantee for FL systems
with heterogeneity.
(a)	Homo-MNIST-IID Parameters
0 5 0 5 0 5
3 2 2 1 1
s≡)s--m PBWESUe=
50 IOO 150	200	250	300
Steps
Ooo
5 0 5
1 1
B--2I-Ewcrot-
(b)	Homo-CIFARlO-IID Parameters
O 500 IOOO 1500	2000	2500	3000
Steps
(C)	100%-50%-MNiS『IID PaemeterS	(d)
Bw)S-m Pw--Esue=
LAQ-2
Y- LAQ-16
AdaQuantFL
→- AdaQuantFL+l_AO
——AQUILA
100%-50%-CIFAR10-IID Parameters
Ooooooo
2 0 8 6 4 2
1 1
w)sP ①空 ESUe4L
O
50
100%-25%-CIEAR10-Non-IID Parameters
500 IOOO 1500	2000	2500	3000
Steps
Figure 4: Transmitted Bits vs Steps. For better illustration, the results have been down-sampled.
The solid lines represent values after down-sampling and the shadows around them represent the
true values.
100	150
Steps
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-efficient sgd via gradient quantization and encoding. In Proceedings of Advances
in Neural Information Processing Systems 30, pp. 1709-1720, 2017.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175-1191, 2017.
Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroFL: Computation and communication efficient
federated learning for heterogeneous clients. In Proceedings of the 8th International Conference
on Learning Representations, 2020.
Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng Ji, and Jie Li.
Federated learning for vehicular Internet of things: Recent advances and open issues. IEEE
Computer Graphics and Applications, pp. 45-61, 2020.
Pengchao Han, Shiqiang Wang, and Kin K Leung. Adaptive gradient sparsification for efficient
federated learning: An online learning approach. In Proceedings of the 40th IEEE International
Conference on Distributed Computing Systems, pp. 300-310, 2020.
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Francoise Beaufays, Sean
Augenstein, HUbert Eichner, Chloe Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Divyansh Jhunjhunwala, Advait Gadhikar, Gauri Joshi, and Yonina C Eldar. Adaptive quantization
of model updates for communication-efficient federated learning. In Proceedings of the 2021
IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 3110-3114,
2021.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing
the communication bandwidth for distributed training. In Proceedings of the 6th International
Conference on Learning Representations, 2018.
Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng, Tianjian
Chen, Han Yu, and Qiang Yang. Fedvision: An online visual object detection platform powered
by federated learning. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, pp.
13172-13179, 2020a.
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang. A secure federated transfer
learning framework. IEEE Intelligent Systems, pp. 70-82, 2020b.
Yuzhu Mao, Zihao Zhao, Guangfeng Yan, Yang Liu, Tian Lan, Linqi Song, and Wenbo
Ding. Communication efficient federated learning with adaptive quantization. arXiv preprint
arXiv:2104.06023, 2021.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282, 2017.
Nikko Strom. Scalable distributed DNN training using commodity GPU cloud computing. In Pro-
ceedings of 16th Annual Conference of the International Speech Communication Association, pp.
1488-1492, 2015.
10
Under review as a conference paper at ICLR 2022
Jun Sun, Tianyi Chen, Georgios B Giannakis, Qinmin Yang, and Zaiyue Yang. Lazily aggregated
quantized gradient innovation for communication-efficient federated learning. IEEE Transactions
on Pattern Analysis & Machine Intelligence,pp.1-15, 2020.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Proceedings of Advances in Neural Information Processing
Systems 31, pp. 1299-1309, 2018.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology, pp. 1-19, 2019.
11
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Supplementary Experimental Results
The appendix includes supplementary experimental results, mathematical proof of Theorem 1 and
Theorem 2, and detailed derivation of the novel adaptive quantization criterion and lazy aggregation
strategy. Compared to Fig. 3 and Fig. 4 in the main text, result figures in appendix show a more
comprehensive evaluation with AQUILA, which contain more detailed information including but
not limited to accuracy vs steps and training loss vs steps curves.
Table 1: Hyperparameters in training process
Dataset	MNIST		CIFAR10	
Model	CNN		ResNet18	
Hidden Size	[64, 128,256,512]		[64,128, 256, 512]	
Data Distribution	IID	Non-IID	IID	Non-IID
Global Epoch E	300	-100	3000	-100
Local Batch Size B	200-	10	-100	10
Optimizer	SGD	SGD-	SGD	-sGD-
Momentum	09	-09	-09	-09
Weight Decay	5.00E-04	5.00E-04	5.00E-04	5.00E-04
Learning Rate η	0.01	0.01	0.1	0.1
Homo-MNlST-IID Parameters
80
›60
u
2
n
u
N 40
20
0.0	2.5	5.0	7.5	10.0 12.5 15.0 17.5
Total Transmitted Bits(GB)
Homo-MNISTIID Parameters
80
›60
u
2
n
u
W 40
20
0	50	100	150	200	250	300
Steps
(b) Accuracy vs Steps
(a) Accuracy vs Total Transmitted Bits
Total Transmitted Bits(GB)
(c) Training Loss vs Total Transmitted Bits
Homo-MNlSTIID Parameters
2.0
0.5
羽
° 1.5
⊂
'S ι.o
0	50	100	150	200	250	300
Steps
(d) Training Loss vs Steps
Figure 5:	Homo-MNIST-IID
12
Under review as a conference paper at ICLR 2022
100
80
S 60
A
u
ro
5 40
<
20
0
Homo-MNlST-Non-IID Parameters
0	1	2	3	4	5	6
Total Transmitted Bits(GB)
(a)	Accuracy vs Total Transmitted Bits
Homo-MNIST-Non-IID Parameters
0 8 6 4 2
1
(求)A3e,lrD4
(b)	Accuracy vs Steps
Homo-MNlST-Non-IID Parameters
(c)	Training Loss vs Total Transmitted Bits
2.5
S 2.0
S
3
σ,1.5
-E
U ι.o
0.5
0	20	40	60	80	100
Steps
(d)	Training Loss vs Steps
Figure 6:	Homo-MNIST-Non-IID
Homo-CIEARlO-IID Parameters
2
y 4o
<
20
0	200	400
FedAvg
QSGD
LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
——AQUILA
600	800	1000^^1200
Total Transmitted Bits(GB)
(a)	Accuracy vs Total Transmitted Bits
Homo-ClFARlO-IlD Parameters
80
60
40
20
0	500	1000	1500	2000	2500	3000
Steps
(b)	Accuracy vs Steps
Homo-ClFARlO-IlD Parameters
0	500	1000	1500	2000	2500	3000
Steps
FedAvg
QSGD
LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
——AQUILA
Homo-CIEARlO-IID Parameters
B1-5
⊂
11.0
2.0
贸
3 L5
⊂
I 1.0
=
0.5
0	200	400	600	800	1000 1200
Total Transmitted Bits(GB)
(c)	Training Loss vs Total Transmitted Bits
(d)	Training Loss vs Steps
Figure 7:	Homo-CIFAR-IID
13
Under review as a conference paper at ICLR 2022
70
60
›50
u
2
5 40
<
30
20
Homo-ClFARlO-Non-IID Parameters
0	10	20	30	40
Total Transmitted Bits(GB)
Homo-ClFARlO-Non-IID Parameters
70
60
›50
u
2
u 40
<
30
20
0	20	40	60	80	100
Steps
(b) Accuracy vs Steps
(a) Accuracy vs Total Transmitted Bits
2.25
2.00
S
自 1.75
^1.50
S 1.25
1.00
0.75
Homo-ClEARlO-Non-IID Parameters
0	10	20	30	40
Total Transmitted Bits(GB)
Homo-CIEARlO-Non-IID Parameters
(c)	Training Loss vs Total Transmitted Bits
(d)	Training Loss vs Steps
Figure 8: Homo-CIFAR-Non-IID
(a)	Transmitted Bits vs Steps
(b)	Transmitted Bits vs Steps
Homo-CIFARlO-Non-IID Parameters
Steps
(d) Transmitted Bits vs Steps
Figure 9: Homo-Transmitted Bits vs Steps
14
Under review as a conference paper at ICLR 2022
100%-50%-MNIST-IID Parameters
80
›60
u
20
100%-50%-MNIST-IID Parameters
0	2	4	6	8	10
Total Transmitted Bits(GB)
(a)	Accuracy vs Total Transmitted Bits
100%-50%-MNIST-IlD Parameters
80
›60
u
ro
n
u
u 40
<
20
0	50	100	150	200	250	300
Steps
(b)	Accuracy vs Steps
100%-50%-MNIST-IID Parameters
2.0
6
⊂
11.0
0.5
0	2	4	6	8	10
Total Transmitted Bits(GB)
(c)	Training Loss vs Total Transmitted Bits
Figure 10:
0	50	100	150	200	250	300
Steps
(d)	Training Loss vs Steps
ιoo
80
注60
A
u
2
5 40
<
20
0
100%-50%-MNIST-Non-IlD Parameters
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5
Total Transmitted Bits(GB)
(a)	Accuracy vs Total Transmitted Bits
100%-50%-MNIST-Non-IlD Parameters
2.5
? 1.5
-ro
Ih 1.0
0.5
0.0	0.5	1.0	13	2.0	2.5	3.0	3.5
Total Transmitted Bits(GB)
(c) Training Loss vs Total Transmitted Bits
100%-50%
-MNIST-IID
100%-50%-MNIST-Non-IID Parameters
0 0 0 0
8 6 4 2
(求)AeJn4
FedAvg
QSGD
LAQ-2
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
——AQUILA
0	20	40	60	80	100
Steps
(b)	Accuracy vs Steps
100%-50%-MNIST-Non-IID Parameters
2.5
S 2.0
S
3
?l-5
⊂
-ra
Ih 1.0
0.5
0	20	40	60	80	100
Steps
(d) Training Loss vs Steps
Figure 11: 100%-50%-MNIST-Non-IID
15
Under review as a conference paper at ICLR 2022
›60
u
ro
n
u
U 40
<
100%-50%-CIFAR10-IID Parameters
80
20
O IOO 200 300 400 500 600 700 800
Total Transmitted Bits(GB)
100%-50%-CIFAR10-IID Parameters
O 500 IOOO 1500	2000	2500	3000
Steps
(a) Accuracy vs Total Transmitted Bits
(b) Accuracy vs Steps
贸
31∙5
6
C
I LO
100%-50%-CIFAR10-IID Parameters
2.0
0.5
O IOO 200 300 400 500 600 700 800
Total Transmitted Bits(GB)
100%-50%-CIFAR10-IID Parameters
(c) Training Loss vs Total Transmitted Bits
(d) Training Loss vs Steps
Figure 12: 100%-50%-CIFAR-IID
70
65
60
堂55
g50
n
U 45
40
35
30
100%-50%-CIFAR10-Non-IID Parameters
O 5	10	15	20	25
Total Transmitted Bits(GB)
70
65
60
堂55
ro 50
n
U 45
40
35
30
100%-50%-CIFAR10-Non-IID Parameters
20
40	60
Steps
80 IOO
(a) Accuracy vs Total Transmitted Bits
(b) Accuracy vs Steps
100%-50%-CIFAR10-Non-IID Parameters
2.25
2.00
ω 一 _
自L75
^1.50
S 1.25
1.00
0.75
FedAvg
QSGD
LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
——AQUILA
5	10	15	20	25^
Total Transmitted Bits(GB)
(c) Training Loss vs Total Transmitted Bits
100%-50%-CIFAR10-Non-IID Parameters
(d) Training Loss vs Steps
Figure 13: 100%-50%-CIFAR-Non-IID
16
Under review as a conference paper at ICLR 2022
(a) Transmitted Bits vs Steps
100%-50%-MNlSTNon-IlD Parameters
(b) Transmitted Bits vs Steps
120
m
S ioo
'w
七
'fn 80
S
t 60
E
2 40
ro
=20
0
100%-50%-CIFAR10-IID Parameters
O 500 IOOO 1500	2000	2500	3000
Steps
100%-50%-CIFAR10-Non-IID Parameters
Steps
(d) Transmitted Bits vs Steps
(c) Transmitted Bits vs Steps
Figure 14:	100%-50%-Transmitted Bits vs Steps
(a) Accuracy vs Total Transmitted Bits
(b) Accuracy vs Steps
2.25
2.00
S 1.75
S
9 1.50
6
~ 1.25
'S _ _
lh 1.00
0.75
0.50
(d) Training Loss vs Steps
100%-25%-MNIST-IID Parameters
0	2	4	6	8
Total Transmitted Bits(GB)
(c) Training Loss vs Total Transmitted Bits
Figure 15:	100%-25%-MNIST-IID
17
Under review as a conference paper at ICLR 2022
80
S 60
A
u
ro
S40
<
20
0
100%-25%-MNIST-Non-IID Parameters
100%-25%-MNIST-Non-IID Parameters
0	20	40	60	80	100
Steps
(b) Accuracy vs Steps
80
住60
A
u
ro
§40
<
20
0
0.0	0.5	1.0	1.5	2.0	2.5	3.0
Total Transmitted Bits(GB)
(a) Accuracy vs Total Transmitted Bits
100%-25%-MNIST-Non-IlD Parameters
∙5∙0∙5∙0∙5
22LL0
SSon Buw,u.
100%-25%-MNIST-Non-IID Parameters
∙5∙0∙5∙0∙5
2 2 LL0
SSon Buw,u.
0.0	0.5	1.0	1.5	2.0	2.5	3.0
Total Transmitted Bits(GB)
(c) Training Loss vs Total Transmitted Bits
0	20	40	60	80	100
Steps
(d) Training Loss vs Steps
Figure 16:	100%-25%-MNIST-Non-IID
100%-25%-CIFAR10-IID Parameters
80
2
U 40
<
20
0	100	200	300	400	500	600
Total Transmitted Bits(GB)
80
2
U 40
<
20
100%-25%-CIFAR10-IID Parameters
0	500	1000	1500	2000	2500	3000
Steps
(a)	Accuracy vs Total Transmitted Bits
(b)	Accuracy vs Steps
2.0
贸
31-5
⊂
to 1.0
0.5
100%-25%-CIFAR10-IID Parameters
FedAvg
QSGD
LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
——AQUILA
2.0
贸
3 1-5
⊂
to 1.0
0.5
100%-25%-CIFAR10-IID Parameters
0	100	200	300	4C0	500	60?
Total Transmitted Bits(GB)
0	500	1000	1500	2000	2500	3000
Steps
(c) Training Loss vs Total Transmitted Bits
(d) Training Loss vs Steps
Figure 17: 100%-25%-CIFAR-IID
18
Under review as a conference paper at ICLR 2022
70
60
注50
A
u
2
3 40
⅛
30
20
100%-25%-CIFAR10-Non-IID Parameters
70
100%-25%-CIFAR10-Non-IID Parameters
LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
——AQUILA
0	5	10	15	20
Total Transmitted Bits(GB)
(a)	Accuracy vs Total Transmitted Bits
100%-25%-CIFAR10-Non-IID Parameters
60
住50
A
u
2
3 40
⅛
30
20
0	20	40	60	80	100
Steps
(b)	Accuracy vs Steps
100%-25%-CIFAR10-Non-IID Parameters
2.25
2.00
S
O 1.75
6_____
C 1.50
J 1.25
1.00
0.75
FedAvg
QSGD
LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
——AQUILA
°	≡ . io. . ι<. . 20
Total Transmitted Bits(GB)
(c)	Training Loss vs Total Transmitted Bits
2.25
2.00
S
O 1.75
6____
C 1.50
f 1.25
1.00
0.75
6	20	40	60	80 IOO
Steps
(d)	Training Loss vs Steps
Figure 18:	100%-25%-CIFAR-Non-IID
(mw)s--mB⅛Eωerat∙
100%-25%-MNIS-RIID Parameters
16
100%-25%-MNISTNon-IID Parameters
16u12108 6 4 -
(mw)s--mB⅛Eωerat∙
O 50 IOO 150	200	250	300
Steps
(a)	Transmitted Bits vs Steps
100%-25%-CIFAR10-IID Parameters
8060w20
(mw)s4≡P ①"lusuau.
O 500 IOOO 1500	2000	2500	3000
Steps
(c)	Transmitted Bits vs Steps
O	20	40	60	80	IOO
Steps
(b)	Transmitted Bits vs Steps
100%-25%-CIFAR10-Non-IID Parameters
0 8 6 4 2
(mw)s4≡P ①"lusuau.
O	20	40	60	80	IOO
Steps
(d)	Transmitted Bits vs Steps
Figure 19:	100%-25%-Transmitted Bits vs Steps
19
Under review as a conference paper at ICLR 2022
ɔ 7.0
:■
, e.o⅞
5∙θ]
^0	20
EHetero-CIFARlO-IID
Commiinicatien skipping
4Q eo βo ιoo
Steps
(a) 1st client in system
(b) 2nd client in system
(c) 3rd client in system
Figure 20: The illustration about AQUILA’s ability to suppress high-bit transmission. Each figure
shows the relationship between local quantization level and communication skipping of one local
client during training with heterogeneous models in the IID scenario with CIFAR10 dataset. These
figures imply a trend that communications with relatively high quantization level (e.g. higher than
the initial level) chosen by (7) are mostly skipped by (8).
A.2 Mathematical Proof
A.2.1 Proof of Theorem 1
Without lazy aggregation, the aggregated model at iteration k should be:
θk+1 = θk- M X Qbm(gm).
m∈M
With lazy aggregation, the actual aggregated model at iteration k is:
θk+1 = θk- M X	Qbm(gm) - M X Q^m-1 3∙
m∈M∖Mk	m∈Mk
With (13) and (14), the gradient loss caused by skipping can be written as:
IBk+ι -θk+l∣∣2 = IlM X Qbm(Sm)-Qbm-(gm—I)
m∈M0k	2
≤ ~Mτ X ∣∣Qbm(Sm)-QbmT(STI)∣∣2
m∈M
=M X ∣∣[Qbm (Sm)- gm] - [Qbm-1 (SmrI)— (Smr1)]+(Sm - gmτ1)∣∣2
m∈M
≤ MOr X ∣∣gm- gmr1∣∣2+MMr X (∣∣εbm (Sm)∣∣2+ ∣∣εbm-1 (SmrI)()，
m∈M	m∈M
(13)
(14)
(15)
n
where the two inequalities follow from: E ai
∣ i=1
2
2
n
≤ nX kaik22.
i=1
Take expectation of both sides of (15):
E[『+1 - θk+1∣∣2] ≤
3m X E[∣∣Sm- Smr1∣∣2]+3M X (E[∣∣εbm (Sm)∣∣2 ]+E[∣∣εbm-1 (SmrI)∣∣2)]
m∈M	m∈M
≤
3M X E[∣.m - Skr1I∣2]+¾■ X ((bdσψ
m∈M	m∈M ( m )
dσ 2
+ (bmp)
≤
3α2 X dm ∣∣ k -k-1∣∣2	3a2 X( dσ2	dσ2
立 m∈MK uSm - Sm ∣ι+N m∈M(L+(b了),
(16)
20
Under review as a conference paper at ICLR 2022
where the second inequality follows from Assumption 2 with σ2 denoting the maximum kwk22 and
qb = d/b2 , and the last inequality is resulted from the conclusion in LAQ (Sun et al., 2020), which
indicates that the client m communicates with the server at most dm rounds in total K iterations,
where dm is a constant related to Lm .
With the definition of Bm and Cmk = d log2 (bkm + 1) + d + 32, the expected gradient loss can be
written as:
E[∣"+1
八 k+1∣∣2i / 3α2 L dm Cm IICk ʌk-lll2 l 3α2 L /	dσ2	l	dσ2 ʌ
-θ ιu≤立m∈MFιιgm-gm ιι2+立m⅛L+≡)
3α2
M
m∈M
3α2
+立E
m∈M
dmd log2 (bkm + 1)
Bm
dm(d +32)
Bm
k _ ,1『	3α2 X ( dσ2	dσ2 }
gm-gm ιι2 + 立m∈M((bm-17 + (O)
gm - gm-1ιι2
3α2
≤ Mr
m∈M
3α2
+立E
m∈M
dmd log2 (4bkm)
Bm
dm(d + 32)
Bm
gm - gm-1ιι2
gm - gm-1ιι2
3α2	dσ2	dσ2
+立 m∈M(L+侬了)
(17)
This completes the proof for Theorem 1.
A.2.2 Derivation Details of Adaptive Quantization Criterion (7)
Let H be the upper bound to be minimized:
H
3α2 X^
MT工
m∈M
dmd Rog2(4bm)]
Bm
Ck —/-以2 + 3α2 X ( dσ2
-ιι2+ M m∈M((bk1 )2
dσ2
十西P)
3α2
+立E
m∈M
dm(d + 32)
Bm
gkm
k-1
-gm
(18)
2
2
The first derivative is:
∂H _ 3α2didιιgm - gk-1ιι2 dlog2ee	6α2dσ2
—~~:— = ----------------:---------- -  ：~~:~~TT
∂bm	BmMbm	M (bm)3
∂H
Let Trn- = 0, there is:
∂bm	'
(19)
2σ2Bmloge(2)
I I	-	,112
Nd"ιgm- gτ1ιL
(20)
Dividing (bm)* by(bm)：
gm)* = bm
t
ιιgm- gmι2
gkm
k-1
-gm
2.
2
(21)
For iteration k = 1, we set b1m for each client m as the intial quantization level b0, and thus we get
the adaptive quantization criterion (7) for iteration k = 1, 2, ..., K:
(bkm)* =b0
t
ιιgm- gmι2
gkm
k-1
-gm
2.
2
(22)
21
Under review as a conference paper at ICLR 2022
A.2.3 Proof of Theorem 2
With the adjusted communication criterion (8), all transmitted gradient updates are larger than a
threshold after quantization. Therefore, all transmitted gradient updates satisfy:
gm - gm-11∣2 Tlgm - Qbm 就)+QbMI 伍TI)- gm-1+Qbm 就)-QbMI 伍m-1) ∣∣2
≈∣∣Qbm [gm -QbmT 伍鼠1)∣∣2
≥ P= ξd∣∣αJ∣∣2 +3 (l“ (gm)∣∣2+" (gm-i)∣∣2).	(23)
then,
(bmm)* = bo
t
∣∣gm - gm∣∣
k-1
gm - gm
2
2
∣∣∣22
≤ b0 uu
t
∣∣gm - gm∣∣2
a⅛ PD=I ξd ∣∣θk+1-d - θk-d∣∣2+3 (院m(gm)∣∣2 + h父1(gm71)∣∣2)
≤ b0 uu
t
∣∣gm - gm∣∣2
≤ b0 uu
t
∣∣gm - gm∣∣2
α2M 2L [f(θk) - f(θk-D)] + 3 (∣∣εbm(gkm)∣∣2 + ∣∣εbm-ι(gm-1)∣∣2)
≤ b0t
∣∣gm - gm∣∣2
α2M 2L [f (θk) - f(θ*)]
≤ b0t
(Lm)2 ∣∣θ1 - θ0∣∣22
α2M2L[f (θk) - f(θ*)],
(24)
where the second inequality comes from Assumption 1. Let P
f(θ0)- f(θ*)
L∣∣θ1- θ0∣∣2
Therefore, if
ɑ ≤ (1∕ML),2Pξ∕L holds, then in the experiment setting where ξι = ξ2 = ... = ξ0 = ξ and
L ≈ Lm for all m ∈ M, there is:
(bk )* ≤ b	(Lm)2 ∣∣θ1- θ0∣∣2	≤ b ∕f(θ°)- f(θ*)
m - 0∖ 0M⅛[f(θk)-f(θ*)] - 0Vf(θk)-f(θ)
22
Under review as a conference paper at ICLR 2022
A.2.4 Development of Lazy Aggregation Criterion (8)
IIQbm 端)-QbL 监 1)∣∣2
= IlQbm (gm) - gm - Qbm-I 伍 m-1) + gm-1+gm - gm-1∣∣2
≤ 2∣∣gm- gm-1∣∣2+2 ∣∣εbm (gm) - εbm-1 (gm-1)∣∣2
≤ 2Lm ∣∣θk - θk-d ∣∣2+2∣∣εbm (gm) - εbm-1 (gm-1)∣∣2
∣∣ d0	∣∣2	2
= 2Lm X θk+1-d-θk-d	+2∣∣εbm (gm)-ε^mrι (gm-1)∣∣2
∣d=1	∣2
d2	2
≤ 2Lmd0 X ∣∣θk+1-d - θk	∣∣2 + 2 ∣∣εbm(gm) - εbm-1 (gm71)∣∣2 ,	(26)
d=1
where the second inequality comes from:
gm-1 = ^fm(θk-d0 ).	(27)
gm = Vfm (θk).	(28)
∣∣Vfm(θk) - Vfm(θk-d0 )∣∣2 ≤ Lm ∣∣θk - θk-d0∣∣2 .	(29)
Following LAQ’s definition, redefine dm, m ∈ M as:
dm := max {d∣Lm ≤ ξd∕(2α2M2D), d ∈ 1, 2,..., D}.	(30)
With the definition of dm and ξ1 ≥ ξ2 ≥ ... ≥ ξD, there is:
Lm ≤ C 2ξd;2n,foralld0satisfying 1 ≤ d0 ≤ dm.	(31)
2α2 M2D
With (26) and (31), we have:
∣∣Qbm (gm) -QbmT 伍TI) ∣∣2
≤ α2M2X ξdo∣∣θk+1-d-θk-d∣∣2+2∣∣εbm (gm)-εbm-1 (gτι)∣∣2
d=1
≤ α⅛ X ξd ∣∣θk+j - θk-d∣∣2+2 卜 bm (gm) - εbm-1 (gm71)∣∣2.	(32)
d=1
23