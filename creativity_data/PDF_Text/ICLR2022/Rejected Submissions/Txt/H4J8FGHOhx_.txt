Under review as a conference paper at ICLR 2022
A Principled Permutation Invariant Approach
to Mean-Field Multi-Agent Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Multi-agent reinforcement learning (MARL) becomes more challenging in the
presence of more agents, as the capacity of the joint state and action spaces grows
exponentially in the number of agents. To address such a challenge of scale,
we identify a class of cooperative MARL problems with permutation invariance,
and formulate it as mean-field Markov decision processes (MDP). To exploit
the permutation invariance therein, we propose the mean-field proximal policy
optimization (MF-PPO) algorithm, at the core of which is a permutation- invariant
actor-critic neural architecture. We prove that MF-PPO attains the globally optimal
policy at a sublinear rate of convergence. Moreover, its sample complexity is
independent of the number of agents. We validate the theoretical advantages
of MF-PPO with numerical experiments in the multi-agent particle environment
(MPE). In particular, we show that the inductive bias introduced by the permutation-
invariant neural architecture enables MF-PPO to outperform existing competitors
with a smaller number of model parameters, which is the key to its generalization
performance.
1I ntroduction
Multi-Agent Reinforcement Learning (Littman, 1994; Zhang et al., 2019) generalizes Reinforcement
Learning (Sutton and Barto, 2018) to address the sequential decision-making problem of multiple
agents maximizing their individual long term rewards while interacting with each other in a common
environment. With breakthroughs in deep learning, MARL algorithms equipped with deep neural net-
works have seen significant empirical successes in various domains, including simulated autonomous
driving (Shalev-ShWartz et al., 2016), multi-agent robotic control (MatariC, 1997; Kober et al., 2013),
and E-sports (Vinyals et al., 2019).
Despite tremendous successes, MARL is notoriously hard to scale to the many-agent setting, as
the size of the state-action space groWs exponentially With respect to the number of agents. This
phenomenon is recently described as the curse of many agents (Menda et al., 2018). To tackle
this challenge, We focus on cooperative MARL, Where agents Work together to maximize their
team reWard (Panait and Luke, 2005). We identify and exploit a key property of cooperative
MARL With homogeneous agents, namely the invariance with respect to the permutation of agents.
Such permutation invariance can be found in many real-World scenarios With homogeneous agents,
such as distributed control of multiple autonomous vehicles and team sports (Cao et al., 2013;
Kalyanakrishnan et al., 2006), but also in scenarios With heterogeneous agent groups, Where invariance
holds Within each group (Liu et al., 2019b). More importantly, We find that permutation invariance
has significant practical implications, as the optimal value functions remain invariant When permuting
the joint state-action pairs. Such an observation strongly advocates a permutation invariant design for
learning, Which helps reduce the effective search space of the policy/value functions from exponential
dependence on the number of agents to polynomial dependence.
Several empirical methods have been proposed to incorporate permutation invariance into solving
MARL problems. Liu et al. (2019b) implement a permutation invariant critic based on Graph Convolu-
tional NetWork (GCN) (Kipf and Welling, 2017). Sunehag et al. (2017) propose value decomposition,
Which together With parameter sharing, leads to a joint critic netWork that is permutation invariant
over agents. While these methods are based on heuristics, We are the first to provide theoretical
principles for introducing permutation invariance as an inductive bias for learning value functions
and policies in homogeneous systems. In addition, We adopt the DeepSet (Zaheer et al., 2017)
1
Under review as a conference paper at ICLR 2022
architecture, which is well suited for handling homogeneity of agents, with much simpler operations
to induce permutation invariance and greater parameter efficiency.
To scale MARL algorithms in the presence of a large number, even infinitely many, agents, mean-field
approximation has been explored to directly model the population behavior of the agents. Mean-field
game considers large populations of rational agents that play a noncooperative game. Yang et al.
(2017) consider a mean-field game with deterministic linear state transitions, and show that it can be
reformulated as a mean-field MDP, where the mean-field state lies in finite-dimensional probability
simplex. Yang et al. (2018) take a mean-field approximation over actions, such that the interaction
for any given agent and the population is approximated by the interaction between the agent’s action
and the averaged actions of its neighboring agents. However, the motivation for averaging over local
actions remains unclear, and it generally requires a sparse graph over agents. In practice, properly
identifying such structure also demands extensive prior knowledge. Mean-field control instead
considers a central controller who aims to compute strategy to optimize the average payoff across the
population. Carmona et al. (2019) motivate a mean-field MDP from the perspective of mean-field
control. The mean-field state therein lies in a probability simplex and is thus continuous in nature. To
enable the ensuing Q-learning algorithm, discretization of the joint state-action space is necessary. In
addition, the dynamic programming principles of such mean-field control problem has been studied
in (Gu et al., 2019). Gu et al. (2020) also propose a Q-learning type algorithm, where the state-action
space is first discretized into an epsilon-net. The kernel regression operator is used to construct an
estimate of the unknown Q-function from samples. Gu et al. (2021) propose a localized training,
decentralized execution framework by locally grouping homogenous agents using their states. Wang
et al. (2020) motivate a mean-field MDP from permutation invariance, but assume a central controller
coordinating the actions of all the agents, and hence is restricted to handling the curse of many agents
from the exponential blowup of the joint state space. Our formulation of mean-field approximation
allows agents to make their own local actions without resorting to a centralized controller.
We propose a mean-field Markov decision process motivated from the permutation invariance structure
of cooperative MARL, which can be viewed as a natural limit of finite-agent MDP by taking the
number of agents to infinity. Such a mean-field MDP generalizes traditional MDP, with each state
representing a distribution over the state space of a single agent. The mean-field MDP provides us
a tractable formulation to model MDP with many agents, including an infinite number of agents.
We further propose the Mean-Field Proximal Policy Optimization (MF-PPO) algorithm, at the core
of which is a pair of permutation invariant actor and critic neural networks. These networks are
implemented based on DeepSet (Zaheer et al., 2017), which uses convolutional type operations to
induce permutation invariance over the set of inputs. We show that with sufficiently many agents,
MF-PPO converges to the optimal policy of the mean-field MDP with a sublinear sample complexity
independent of the number of agents. To support our theory, we conduct numerical experiments on
the benchmark multi-agent particle environment (MPE) and show that our proposed method requires
a smaller number of model parameters and attains better performance than multiple baselines.
Notations. W denote P(X) as the set of distribution on set X. δx denotes the Dirac measure
supported at x. For S = (si,..., SN), We use S i" P to denote that each Si is independently sampled
from distribution p. For f : X → R and a distribution π ∈ P(X), we write hf, ∏ = Ea〜∏f (a). We
Write [m] in short for {1, . . . , m}, and ∆d for the standard probability simplex in Rd.
2P roblem Setup
We focus on studying multi-agent systems with cooperative, homogeneous agents, where the agents
within the system are of similar nature and hence cannot be distinguished from each other. Specifically,
we consider a discrete time control problem with N agents, formulated as a Markov decision process
(SN, AN, P, r). We define the joint state space SN to be the Cartesian product of the finite state
space S for each agent, and similarly define the joint action space AN . The homogeneous nature of
the system is reflected in the transition kernel P and the shared reward r, which satisfies:
r(st, at) = r(κ(st), κ(at)), P(st+ι |st, at) = P (κ(st+ι)∣κ(sj κ(at))	(2.1)
for all (st, at) ∈ S N X AN and the permutation mapping κ(∙) ∈ SN, where SN is the set of all
one-to-one mapping from [N] to itself. In other words, it is the configuration, rather than individual
identities, that affects the team reward, and the transition to the next configuration solely depends on
the current configuration. See Figure 1 for detailed illustration. Such permutation invariance finds
applications in many real-world scenarios, including distributed control of autonomous vehicles, and
social economic systems (Zheng et al., 2020; Cao et al., 2013; Kalyanakrishnan et al., 2006).
2
Under review as a conference paper at ICLR 2022
Our goal is to find the optimal policy V, where V (S) ∈ Δ∣∕n ∣ for all S ∈ S N, and maximize the
expected discounted reward VV(S) = (1 - Y)E{P∞=o Ytr(st, at)∣so = s,at〜V(St),∀t ≥0} . Our
first result shows that learning with permutation invariance advocates invariant network design.
Proposition 2.1. For cooperative MARL satisfying (2.1), there exists an optimal policy V* that is
permutationinvariant, i.e., V *(s, a) = ν* (K(S),κ(a)) for any permutation mapping κ(∙). In addition,
for any permutation invariant policy V, the valuefunction V (∙) and the state-action valuefunction
Q(∙) is also permutation invariant, i.e., VV(S) = VV (K(S)), Qν (s, a) = QV (K(S), κ(a)), where
Qν (S, a) = Es0 {r(S, a) + YVν(S0)}.
Proposition 2.1 has an important implication for architecture design, as it states that it suffices
to search within the permutation invariant policy and value function classes. To the best of our
knowledge, this is the first theoretical justification of permutation invariant network design for
learning with homogeneous agents.
We focus on the factorized policy class with a param-
eter sharing scheme, where each agent makes its own
decision without consolidating with others. Specifi-
cally, the joint policy V can be factorized as V(a|S) =
QN=I μ(a∕θi), where μ(∙) denotes the shared local
mapping and oi denotes the local observation. Such a
policy class is widely adopted in the celebrated central-
ized training - decentralized execution paradigm (Lowe
et al., 2017), due to its light overhead in the deployment
phase and favorable performances. However, directly
learning such factorized policy remains challenging,
as each agent needs to estimate its state-action value
function, denoted as QV (S, a). The search space during
learning is (|S | × |A|)N, scaling exponentially with
respect to the number of agents. The large search space
poses as a significant roadblock for efficient learning,
and is coined as the curse of many agents.
BrUce	Rachel	BrUce	Riley
Charlie	Riley	Charlie Rachel
Job”	Covington	John	Covington
Figure 1: Illustration of permutation invari-
ance. Exchanging identities of the agents
(first and third row) does not change the
transition of the system (second row).
To address the curse of many agents, we exploit the homogeneity of the system and take the mean-field
approximation. We begin by taking the perspective of agent i, which is arbitrarily chosen from the
N agents. We denote its state as s and the states of the rest of the agents by Sr . One can verify that
when permuting the state of all the other agents, the value function remains unchanged; additionally,
we can further characterize the value function as a function of the local state and the empirical state
distribution over the rest of agents.
Proposition 2.2. For any permutation mapping κ(∙), the value function satisfies VV(s, Sr) =
VV(s, K(Sr)). Additionally, there exists gV such that: VV (s, Sr) = gV (s, pbsr ), where bpsr =
N Es∈s, δs is the empirical distribution over the states of rest of the agents Sr
For a system with a large number of agents (e.g., financial markets, social networks), the empirical
state distribution can be seen as the concrete realization of the underlying population distribution
of the agents. Motivated from this observation and Proposition 2.2, we formulate the following
mean-field MDP that can be seen as the limit of finite-agent MDP in the presence of infinitely many
homogeneous agents.
Definition 2.1 (mean-field MDP). The mean-field MDP consists of elements of the following: state
(s, ds) ∈ S × P(S); action a ∈ A ⊆ AS; reward r(s, ds, a); transition kernel P(s0, dg |s, ds, a).
The mean-field MDP has an intimate connection with our previously discussed finite-agent MDP.
Since the the agents are homogeneous, the system is the same from any agent’s perspective. We
choose any agent (referred to as representative agent), the state information of such an agent includes
the local state s, and the mean-field state dS. With state information, the agent selects a meta action
a ∈ A ∈ AS, and uses such a meta action to make local decision a = α(s) ∈ A. We remark that
such a modeling of decision process allows the agent to make decision on both its local information
(local state s) and the global information (mean-field state dS). From homogeneity we assume all the
rest of the agents uses the same meta action a to make their local actions. Note that different agents
can still make different local actions due to their different local states, i.e., a(z) = a(z0) in general for
3
Under review as a conference paper at ICLR 2022
z 6= z0 ∈ S. The joint state at the next timestep (s0, d0S) naturally depends on the current global state
(s,ds) and the meta action a (since all the other agents use a to generate their local actions), and is
specified by the transition kernel P(s0,dS ∣s,ds, a). In addition, the representative agents receives a
reward r(s,ds,a), which depends on the local state and mean-field sate, and the meta action a.
Our goal is to learn efficiently a policy ∏, where ∏(∙∣s, ds) ∈ Δ∣a∣ for all (s, ds) ∈ S X P(S),
for maximized expected discounted reward. To facilitate discussions, we define the value function
Vπ(s, ds) = (1-γ)E {P∞=o γtr(st, ds,t,at)} , where (s0, ds,o) = (s, ds),at 〜∏(st, ds,t),∀t ≥
0; and Q-function Qπ(s, ds,a) = (1 — Y)E {P∞=o Ytr(St, ds,t,at)} , where (so, ds,o)=
(s, ds),ao = a, at 〜π(st, ds,t). The optimal policy is denoted by π* ∈ argmax Vπ(s, ds).
Despite the intuitive analogy to finite-agent MDP, solving the mean-field MDP poses some unique
challenges. In addition to having an unknown transition kernel and reward, the mean-field MDP
takes a distribution as its state, which we do not have complete information of during training. In
the following section, we propose our mean-field Neural Proximal Policy Optimization (MF-PPO)
algorithm that, with a careful architecture design, can solve such mean-field MDP in a model-free
fashion efficiently.
3M	ean-field Proximal Policy Optimization
Our algorithm falls into the category of the actor-critic learning paradigm, consisting of alternating
iterations of policy evaluation and improvement. The unique features of MF-PPO lie in the facts: (1)
it exploits permutation invariance of the system, reducing search space of the actor/critic networks
drastically and enables much more efficient learning; (2) it can handle a varying number of agents.
For simplicity of exposition, we consider a fixed number of agents here.
Throughout the rest of the section, we assume that for any joint state (s, ds) ∈ S × P(S), the
agent has access to N i.i.d. samples {si}iN=1 from ds. We denote concatenation of such samples
as S ∈ S N and write S i'i^' ds. MF-PPO maintains a pair of actor (denoted by F A) and critic
networks (denoted by Fq), and uses the actor network to induce an energy-based policy ∏(a∣s, ds).
Specifically, given state (s, ds), the actor network induces a distribution on set A according to
∏(a∣s, ds) a exp {τTFA(s, ds,a)} , where T denotes the temperature parameter. We use ∏ a
exp FA to denote the dependency of the policy on the energy function.
•	Permutation-invariant Actor and Critic. We adopt a permutation invariant design of the actor
and critic network. Specifically, given individual state s ∈ S and sampled states S ∈ SN , the actor
(resp. critic) network FA (resp. FQ) satisfies FA(s, s,a) = FA(s, κ(s),a) for any permutation
mapping κ. With permutation invariance, the search space of the actor/critic network polynomially
depends on the number of agents N .
Proposition 3.1. The search space of a permutation invariance actor (critic) network is at the order
of (Pm=n{|s|,N} (7—1)(?)) ∣S∣∣A∣; Additionally, if |S| < N ,then the search space depends on N
at the order of N |s| .
Compared to architectures without permutation invariance, whose search space depends on N at
the order of (|S ||A|)N, we can clearly see the search space of MF-PPO is exponentially smaller.
Motivated by the characterization of the permuta-
tion invariant set function in Zaheer et al. (2017),
the actor/critic network in MF-PPO takes the form
of Deep Sets architecture, i.e., FA(s, s,a) =
h (PS，∈s φ(s, s0,a)∕N) . Both networks first aggregate
local information by averaging over the output of a
shared sub-network among agents, before feeding the
aggregated information into a subsequent network h(∙).
See Figure 2 for detailed illustration. Effectively, by
the average pooling layer and the preceding parameter
Figure 2: Illustration of a DeepSet param-
eterized critic network.
sharing scheme, the network can keep its output unchanged when permuting the ordering of agents.
Compared to a Graph Convolutional Neural Network (Kipf and Welling, 2017), which uses two sets
of weights for the linear transformation layer, one for the the agent itself and one for the aggregation
state coming from the rest of the agents. The averaging operation is well suited for homogeneous
agents and more parameter-efficient. It also naturally allows us to handle varying number of agents
during training and evaluation, which is not readily achievable by GCN network.
4
Under review as a conference paper at ICLR 2022
Naturally, the actor network when given the joint state-action pair (s, ds,a) is given by
F A (s, ds, a) = h (Es0 〜ds φ(s, s0, a)). We assume F A is parameterized by a neural network with
parameters α ∈ RD , which is to be learned during training. We let the function class of all possible
actor networks be denoted by FA . This same architecture design applies to the critic network, with
learnable parameters denoted by θ ∈ RD and the function class denoted by FQ . MF-PPO then
consists of successive iterations of policy evaluation and policy improvement described below.
•	Policy Evaluation. At the k-th iteration of MF-PPO, we first update the critic network FQ by
minimizing the mean squared Bellman error while holding the actor network FA,k fixed. We denote
the policy induced by the actor network as πk, the stationary state distribution of policy πk as νk, and
the stationary state-action distribution as σk(s, ds,a) := Vk(s, ds)∏k(a|s, ds). Thus, we follow the
update
θk = argminθ∈B(θ0,Rθ) Eσk {FQ(s, ds, a) - hTπk FQi (S, ds ,a)}2,	(3.I)
where B(θ0 , Rθ) denotes the Euclidean ball with radius Rθ centered at the initialized pa-
rameter θo, and the Bellman evaluation operator Tπ is given by TπFq(s,ds,a) =
E n(1 - Y)r(s, ds,a) + YFQ(S0, d4,a0)}, where (S, d4)〜P(∙∣s, ds,a),a0 〜∏(∙∣s0, d&). We
solve (3.1) by T-step temporal-difference (TD) update and output θk = θ(T). At the t-th iteration of
the TD update,
θ(t + 1/2) = θ(t) - η{FQt)(s, s,a) - (1 - Y)r(s, ds,a) - YFQt)(S0, s',a')"θFQt)(s, s,a),
θ(t + 1) = ΠB(θ0,Rθ) (θ(t + 1/2)),
where we sample (s, ds,a)〜σk, (s0,dg)〜P(∙∣s,ds,a), a0 〜πk(∙∣s0,dg), S iʌd. ds, s0 i.i.d.
ds. We use ∏χ(∙) to denote the orthogonal projection onto set X, and η to denote the step size. Note
that here for the simplicity of analyses, we sample the state-action pair (s, ds,a) independently from
the stationary distribution. We remark that trajectory samples can also be used, which essentially
requires bounding the bias of the gradient at each iteration due to the dependencies between trajectory
samples, and we can readily apply the fast-mixing property of Markov chains to control such a bias
(Bhandari et al., 2018). The details of policy evaluation are summarized in Algorithm 2, Appendix A.
•	Policy Improvement. Following the policy evaluation step, MF-PPO updates its policy by updating
the policy network FA, which is the energy function associated with the policy. We update the policy
network by
Πk+1 = argmax	EVk {(琢(s, ds, ∙),∏(∙∣s, ds))- UkKL (π(∙∣s, ds)∣∣∏k(∙∣s, ds)) }.
π(xexp{F A,k+1},
F A,k+1 ∈FA
The update rule intuitively reads as increasing the probability for choosing action a if it yields a
higher value for critic network FQ (s, ds, a), which can be viewed as a softened version of policy
iteration (Bertsekas, 2011). Additionally, by controlling the proximity parameter υk, we can control
the softness of the update, with υ → 0 yielding the vanilla policy iteration. Moreover, without
constraint of FA,k+1 ∈ FA, such an update would have a nice closed form expression, and πk+1
itself is another energy-based policy.
Proposition 3.2. Let ∏k H exp (T-I FA) denote the energy-based policy, then the update
∏k+ι = argmax∏ EVk { (FQ(s, ds, ∙),∏(∙∣s, ds)〉一 UkKL (π(∙∣s, ds)∣∣∏k(∙∣s, ds)) }
yieldsπk+ι H eχp{υ-1FQ + T-IFok}∙
To take into account that the representable function of actor network resides in FA, we update the
policy by projecting the energy function of ∏k+ι back to FA. Specifically, by denoting ∏k+ι H
exp{Tk-+11FαAk+1}, we recover the next actor network FαA (i.e., energy) by performing the following
regression task
ak+ι = argmin Eek{FA(s,ds,a) — Tk+ι
α∈B(Rα,α0)
卜-IFQ (s, ds,a)+ T-1FA (s, ds,a)] }2, (3.2)
5
Under review as a conference paper at ICLR 2022
where σek = νkπ0 . We approximately solve (3.2) via T-step stochastic gradient descent (SGD), and
output ak+ι = α(T) ≈ ak+「At the t-th iteration of SGD,
α(t + 1/2) = α(t) -NaFAt)(S s,a){FA(t)(s, s,a) - τk+1 (υ-1FQ (S, s,a) + T-IFAk (S, s,a)) },
α(t + 1) = ΠB(Rα,α0) (α(t + 1/2)),
where We sample (s, ds, a)〜 ek, and S i'i^' ds, and η is the step size. The details are summarized
in Algorithm 3 of Appendix A. Finally, we present the complete MF-PPO in Algorithm 1.
Algorithm 1 Mean-Field Neural Proximal Policy Optimization
Require: Mean-field MDP (S X, P (S), A, P, r), discount factor Y; number of outer iterations K,
number of inner updates T; policy update parameter υ, step size η, projection radius Rα , Rθ .
Initialize: τo J 1, FA,0 J 0, ∏o H exp{τ-* 1 * * *Fa,0} (uniform policy).
for k = 0,...,K — 1 do
Set temperature parameter Tk+ι J υ√K∕(k + 1), and proximity parameter Uk J υ√K
Solve (3.1) to update the critic network FθQ , using TD update (Algorithm 2)
Solve (3.2) to update the actor network for FαAk+1 , using SGD update (Algorithm 3)
Update policy: πk+1 H exp{τk-+11FαAk+1}
end for
4G lobal Convergence of MF-PPO
We present the global convergence of MF-PPO algorithm for the two-layer permutation-invariant
parameterization of the actor and critic networks. We remark that our analysis can be extended to
multi-layer permutation-invariant networks, and we present the two-layer case here for simplicity of
exposition. Specifically, the actor and critic networks take the form
1	mA	1	mQ
FA(S, s,a) = √mN ɪs ujσ ujσ (α>(s, s0,a)), FQ(S,s,a) = √mN ΣS vjσ Vjσ (θ>(S, s0,a)),
where mA (resp. mQ ) is the width of the actor (resp. critic) network, and σ(x) = max{x.0}
denotes the ReLU activation. We randomly initialize uj (resp. Vj ) and first layer weights α0 =
[α>ι,..., α>mA]> ∈ Rd∙mA (resp. θo ∈ Rd∙mQ)by
Uj 〜Unif { -1,+1} ,αo,j 〜N(0,L∕d), ∀j ∈ [m].
For ease of analysis, we take m = mA = mQ and share the initialization of α0 and θ0 (resp. u0 and
V0). Additionally, we keep uj ’s fixed during training, and α (resp. θ) within ball B(α0, RA) (resp.
B(θ0, RQ )) throughout training. We define the following function class which approximates the class
of previously defined actor/critic network for large network width.
Definition 4.1. Given Rβ > 0, define the function class over domain S × S × A by
1m
Fβ,m = {fβ(∙)∣ fβ (s, S0,a) = √m X vj l{β>j (s, S0,a) > 0}β> (S,S0,a), kβ ― β0k2 ≤ Re },
where Vj, β0,j are random weights sampled according to
Vj 〜Unif { -1,+1} ,βo,j 〜N(0,Id∕d), ∀j ∈ [m].
Fβ,m also induces the function class over S × P(S) × A given by
FPm = {F (∙) I F(S, dS ,a)= Es0 〜ds fβ (S, S0,a), fβ ∈Fβ,m}.
It is well known that functions within Fβ,m approximate functions within the reproducing kernel
Hilbert space associated with kernel K(χ,y) = Ez〜N(o,id∕d) {l(zτχ > 0,z>y > 0)} for a large
network width m (Jacot et al., 2018; Chizat and Bach, 2018; Cai et al., 2019; Arora et al., 2019) and
whose RKHS norm is bounded by Rβ. For large Rβ and m, Fβ,m represents a rich class of functions.
Additionally, functions within FβP,m can be viewed as the mean-embedding of the joint state-action
pair onto the RKHS space (Muand, et et al., 2016; Song et al., 2009; Smola et al., 2007). Below, we
make one important assumption, which assumes that FβP,m is rich enough to represent the Q-function
of all the policies within our policy class.
6
Under review as a conference paper at ICLR 2022
Assumption 1. For any policy π induced by FA ∈ FA, we have Qπ ∈ FθP,m .
We remark that Assumption 1 can be relaxed into requiring that FβP,m has approximation error
when parameterizing the set of Q-functions, with an additional term appearing in the convergence
bound developed in Theorem 4.1 (Lan, 2021).
We define mild conditions stating boundedness of reward, and regularity of stationary distributions.
Assumption 2. Reward function r(∙) ≤ r for some r > 0. Additionally, there exists c > 0 such that
E {1 (∣z>(s, s0, a)| ≤ t)} ≤ C ∙ kZk^ for any Z ∈ Rd and t > 0.
We measure the progress of MF-PPO in Algorithm 1 using the expected total reward
Lg= Eν* [Vπ(s, ds)]= Eν* {hQπ(∙∣s, ds),∏(∙∣s, ds)i},
(4.1)
where V* is the stationary state distribution of the optimal policy ∏*. We also denote σ* as the
stationary state-action distribution induced by ∏*. Note that we have: L(∏*) = Eν* [Vπ* (s, ds)] ≥
Eν* [Vπ(s, ds)] = L(∏), for any policy ∏. Our main results are presented in the following theorem,
showing that L(πk) converges to L(π*) at a sub-linear rate.
Theorem 4.1 (Global Convergence of MF-PPO). Under Assumptions 1 and 2, the policies {πk}kK=1
generated by Algorithm 1 satisify
min0≤k≤K {L(π*) - L(πk)} ≤
U(Iog |A|+PK-ι1(εk+εk)) +__M
(1-γ)√K	(1-γ)υ√K
where M = Eν* s maXa∈A(FQ(s, s0, a))2 + + 2RA, εk and εk are defined in Lemma 4.3. In
particular SUPPoSelat eacħιteratιon OfMF-PPO, we observe N = Ω (K3RA + KRQ) agents,
and the actor/critic network satisfies mA = Ω (K6RA0 + K4R1A0∣A∣2) , mQ = Ω (K2RQ0) , and
T = Ω (K 3RA + KRQ) ,then we have
min0≤k≤K{L(π*) - L(πk)} ≤
U2 (log ∣A∣+O(1)) + M
(1-γ)υ√K
Theorem 4.1 states that, given sufficiently many agents and a large enough actor/critic network,
MF-PPO attains global optimality at a sublinear rate. Our result shows that when solving the
mean-field MDP, having more agents serves as a blessing instead of a curse. In addition, as will be
demonstrated in our proof sketch, there exists an inherent phase transition, where the final optimality
gap is dominated by statistical error for a small number of agents (first phase); and by optimization
error for a large number of agents (second phase).
The complete proof of Theorem 4.1 takes careful analysis on the error from policy evaluation (3.1)
and the improvement step (3.2). The analysis on the outer iterations of MF-PPO can be overviewed
as approximate mirror descent, which needs to take into account how the evaluation and improvement
error interacts. Intuitively, the tuple (εk, ε0k) describes the the effect of policy update when using
approximate policy evaluation and policy improvement, and will be further clarified in the ensuing
discussion. We present here the skeleton of our proof, and defer the technical detail to the appendix.
Proof Sketch. We first establish the convergence of the policy evaluation and improvement step.
Lemma 4.1 (Policy Evaluation). Under the same assumPtions in Theorem 4.1, let Qπk denote
the Q-function of Policy πk, let k
Einit,σk (F(QT)(∙) — Qπk(∙))2
denote Policy evaluation error,
where θ(T) is the output ofAlgorithm 2, we have Ek
O
+ RQC + 鼻 +
十mQ/4+N1/2十
Lemma 4.2 (Policy Improvement).
Einit,eefc nFaA(T)(•)— τk+1 (V- 'Fθk (•)
Under the same assumptions in Theorem 4.1, let E0k+1 =
+τ-1FA (•))) denote policy optimization error where a(T)
is the output of Algorithm 3, we have E0k+1 = O
R5/2	R2
+ RA + RA	+
十mA/4+N1/2十
Lemma 4.1 and 4.2 show that despite non-convexity, both policy evaluation and policy improvement
steps converge approximately to the global optimal solution. In particular, for both policy evaluation
steps and improvement steps, given networks with large width, for a small number of iterations T,
the optimization error O T -1/2 dominates the optimality gap; for a large number of iterations T,
the statistical error O N -1/2 dominates the optimality gap.
7
Under review as a conference paper at ICLR 2022
With Lemma 4.1 and 4.2, we illustrate the main argument for the proof of Theorem 4.1. Let us
assume the ideal case when k = 0k+1 = 0. Note that for k = 0, we obtain the exact Q-function of
policy πk. For 0k+1 = 0, we obtain the ideal energy-based updated policy define in Proposition 3.2.
That is,
∏k+ι = argmax∏ EVk { hQπk (s, ds, ∙),∏(∙∣s, ds )i Uk KL(π(∙∣s, ds )k∏k (∙∣s, ds)) }.	(4.2)
Without function approximation, problem (4.2) can be solved by treating each joint state (s, dS)
independently, hence one can apply the well known three-point lemma in mirror descent (Chen and
Teboulle, 1993) and obtain that, for all (s, ds) ∈ S × P(S):
hQπk(s, ds,∙),∏*(∙∣s, ds) — ∏k(∙∣s, ds)i
≤υk{KL (∏*(∙∣s, ds)k∏k(∙∣s, ds))- KL (∏*(∙∣s, ds)k∏k+ι(∙∣s, ds))- KL (∏k+ι(∙∣s, ds)k∏k(∙∣s, ds)) }
+ hQπk(s, ds, ∙),∏k+ι(∙∣s, ds) - ∏k(∙∣s, ds)i .
From Lemma 6.1 in Kakade and Langford (2002), the expectation of the left hand side yields exactly
(1 - Y) {L(∏*) - L(∏k)}. Hence we have
(1-γ){L(π*)-L(∏k)}
≤UkEν* {KL (∏*(∙∣s, ds)k∏k(∙∣s, ds)) - KL (∏*(∙∣s, ds)k∏k+ι(∙∣s, ds))
-KL (∏k+ι(∙∣s, ds)k∏k(∙∣s, ds)) } + Eν* hQπk(s, ds, ∙),∏k+ι(∙∣s, ds) - ∏k(∙∣s, ds)i .
Pinsker,s Inequality KL (∏k+ι(∙∣s, ds)k∏k(∙∣s, ds)) ≥ 11 k∏k+ι - ∏k∣∣2, combined with observation
kQπk (s, ds, ∙)∣∞ ≤ r/(1 - γ), and basic inequality -ax2 + bx ≤ b2∕(4a) for a > 0 gives Us
(1-γ){L(π*)-L(∏k)}
,	..................... ..............................、	r2
≤UkEν* {KL(∏ (∙∣s, ds)∣∏k(∙∣s, ds)) - KL(∏ (∙∣s, ds)k∏k+ι(∙∣s, ds)) } + 2以@ -)产∙
By setting Uk = O(√K), and telescoping the above inequality from k = 0 to K - 1, we obtain:
min°≤k≤κ-ι{L(∏*) -L(∏)} = O(1∕√K). Note that the key element in the global convergence
of MF-PPO is the recursion defined in the previous inequality, which holds whenever we have an
exact Q-function of the current policy and no function approximation is used when updating the
next policy. Now MF-PPO conducts approximate policy evaluation k > 0, and after obtaining the
approximate Q-function, conducts approximate policy improvement step 0k+1 > 0 with function
approximation. In addition, the error of approximating the Q-function introduced in the evaluation
step can be further compounded in the improvement step. Nevertheless, the previous inequality still
holds approximately, with additional terms representing the policy evaluation/improvement errors.
Lemma 4.3 (Liu et al. (2019a)). Let k (evaluation error) and 0k+1 (improvement error) be defined
as in Lemma 4.1 and Lemma 4.2, respectively. We have:
(1 - Y)(L(∏*) - L(∏k)) ≤ UkEν* {KL (∏*(∙∣s, ds)∣∣∏k(∙∣s, ds)) - KL (∏*(∙∣s, ds)∣∏k+ι(∙∣s, ds)) }
+ Uk (εk + ε0k) + Uk-1M.	(4.3)
where
εk = Tk+ι4+闽+1 + U-Iekψk, εk = lAlτk+2ι(ek+ι)2, M = Eν* {max [FQ(S ds,a)] } + 2RA∙
In addition, φk and ψk are defined by:
Φk =	Eσk	[∣dπ*∕d∏o	- d∏k∕d∏o∣2]	1/1	,	ψk	=	Eσk	[∣dσ*∕dσk	-	d(ν*	× ∏k)∕dσk∣2]1/2 .
Finally, by telescoping inequality (4.3) from k = 0 to K - 1, we complete the proof of Theorem 4.1.
5E xperiments
We perform experiments on the benchmark multi-agent particle environment (MPE) used in prior work
(Lowe et al., 2017; Mordatch and Abbeel, 2018; Liu et al., 2019b). In the cooperative navigation task,
N agents each with position xi ∈ R2 must move to cover N fixed landmarks at positions yi ∈ R2 .
They receive a team reward R = - PiN=1 minj ∈[N] ∣yi - xj ∣2 ; In the cooperative push task, N
agents with position xi ∈ R2 work together to push a ball x ∈ R2 to a fixed landmark y ∈ R2 . They
receive a team reward R = - minj∈[N] ∣xj - x∣2 - ∣x - y∣2 . Both tasks involve homogeneous
8
Under review as a conference paper at ICLR 2022
PJeMaI epos-do ①6巴①>4
(a) N = 15	(b) N = 30	(c) N = 200	(d) N = 500
Figure 3: Performance versus number of environment steps in the multi-agent cooperative navigation
task, over five independent runs per method. Points are taken every 1000 training episodes, with
the first point taken after the first 1000, and is the average reward of 1000 evaluation episodes. MF
significantly outperforms other critic representations for various number of agents.
agents, and all the agents share the same team reward. Note that MPE environment also models
interaction between agents, including collision, and the collided agents receive negative rewards.
We instantiate our method MF, by parameterizing the centralized critic function using a DeepSet
(Zaheer et al., 2017) network, with two hidden layers. We use a standard two-layer multi-layer
perception (MLP) for the centralized actor network in all algorithms. The actor network outputs
the mean and diagonal covariance of a Gaussian distribution over the joint action space. We refer
interested readers to Appendix B for detailed configurations of hyperparameters.
We compare with two other critic repre-
sentations: one that uses MLP for the cen-
tralized critic, labeled MLP, and another
that uses a graph convolutional network for
the critic (Liu et al., 2019b), labeled GCN.
Note that the GCN representation is per-
mutation invariant if one imposes a fully-
connected graph for the agents in the MPE,
but this invariance property does not hold
for all graphs in general. We also compare
with an extension of (Yang et al., 2018) to
the case of continuous action spaces, la-
beled MF-A, in which each independent
(a) N = 15	(b) Varying critic size
Figure 4: (a) Performance versus number of environ-
ment steps in the multi-agent cooperative push task. (b)
MF outperforms GCN even with a fewer number of
critic network parameters (N = 3).
DDPG agent i has a decentralized critic Q(Si, ai, ai) that takes in the mean of all other agents' actions
ai := N—ι Ej=i aj∙∙ Finally, We include comparison with VDN (Sunehag et al., 2017), where the
centralized critic network is the direct summation of local critic networks and thus being permutation
invariant. Empirically, as we find that off-policy RL learns faster than on-policy RL in the MPE
with higher agent number, regardless of the critic representation, we make all comparisons on top of
MADDPG (Lowe et al., 2017). For a fair comparison of all critic representations, we ensure that all
neural network architectures contain approximately the same number of trainable weights.
For the cooperative navigation task, Figure 3 shows that the permutation invariant critic representation
based on DeepSet enables MF to learn faster or reach a higher performance than all other representa-
tions and methods in the MPE with 15, 200, and 500 agents. For the cooperative push task, Figure 4a
demonstrates a similar performance improvement provided by MF. In addition, we also demonstrate
the superior parameter efficiency of MF compared to GCN. Figure 4b shows that MF consistently
and significantly outperforms GCN as the number of parameters in their critic network varies over a
range, with all other settings fixed. In particular, MF requires much fewer critic parameters to achieve
higher performance than GCN.
Computational Improvements. Theorem 4.1 states that to obtain a small optimality gap in MF-
PPO, one needs to compute the update on a large number of agents. We remark that with the dual
embedding techniques developed in Dai et al. (2017), one can avoid computation on all the agents
by sampling a small number of agents to compute the update. This technique could be readily
incorporated into MF-PPO to improve its computational efficiency.
Conclusion. We propose a principled approach to exploit agent homogeneity and permutation
invariance through the mean-field approximation in MARL. Our results are also the first to show
the global convergence of MARL algorithms with neural networks as function approximators. This
is in sharp contrast to current practices, which are mostly heuristic methods without convergence
guarantees.
9
Under review as a conference paper at ICLR 2022
References
ARORA, S., DU, S. S., HU, W., LI, Z. and WANG, R. (2019). Fine-grained analysis of opti-
mization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584.
BERTSEKAS, D. P. (2011). Approximate policy iteration: A survey and some new methods. Journal
ofControl Theory andApplications 9 310-335.
B handari, J., Russo, D. and Singal, R. (2018). A finite time analysis of temporal difference
learning with linear function approximation. In Conference On Learning Theory. PMLR.
Bloem-Reddy, B. and Teh, Y. W. (2019). Probabilistic symmetry and invariant neural networks.
arXiv preprint arXiv:1901.06082 .
Cai, Q., Yang, Z., Lee, J. D. and Wang, Z. (2019). Neural temporal-difference learning converges
to global optima. In Advances in Neural Information Processing Systems.
Cao, Y., Yu, W., Ren, W. and Chen, G. (2013). An overview of recent progress in the study of
distributed multi-agent coordination. IEEE Transactions on Industrial informatics 9 427-438.
Carmona, R., LAURI它re, M. and Tan, Z. (2019). Model-free mean-field reinforcement learning:
mean-field mdp and mean-field q-learning. arXiv preprint arXiv:1910.12802 .
Chen, G. and Teboulle, M. (1993). Convergence analysis of a proximal-like minimization
algorithm using bregman functions. SIAM Journal on Optimization 3 538-543.
Chizat, L. and Bach, F. (2018). A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956 8.
Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J. and Song, L. (2017).
Sbeed: Convergent reinforcement learning with nonlinear function approximation. arXiv preprint
arXiv:1712.10285.
Gu, H., Guo, X., Wei, X. and Xu, R. (2019). Dynamic programming principles for learning mfcs.
arXiv preprint arXiv:1911.07314 .
Gu, H., Guo, X., Wei, X. and Xu, R. (2020). Mean-field controls with q-learning for cooperative
marl: Convergence and complexity analysis. arXiv preprint arXiv:2002.04131 .
Gu, H., Guo, X., Wei, X. and Xu, R. (2021). Mean-field multi-agent reinforcement learning: A
decentralized network approach. arXiv preprint arXiv:2108.02731 .
Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems.
Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning.
In ICML, vol. 2.
Kalyanakrishnan, S., Liu, Y. and Stone, P. (2006). Half field offense in robocup soccer: A
multiagent reinforcement learning case study. In Robot Soccer World Cup. Springer.
Kipf, T. N. and Welling, M. (2017). Semi-supervised classification with graph convolutional
networks. In International Conference on Learning Representatioons.
Kober, J., Bagnell, J. A. and Peters, J. (2013). Reinforcement learning in robotics: A survey.
The International Journal of Robotics Research 32 1238-1274.
Lan, G. (2021). Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. arXiv preprint arXiv:2102.00135 .
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994. Elsevier, 157-163.
10
Under review as a conference paper at ICLR 2022
Liu, B., Cai, Q., Yang, Z. and Wang, Z. (2019a). Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306 .
Liu, I.-J., Yeh, R. A. and Schwing, A. G. (2019b). Pic: Permutation invariant critic for
multi-agent deep reinforcement learning. arXiv preprint arXiv:1911.00025 .
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P. and Mordatch, I. (2017). Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems.
MATARIC, M. J. (1997). Reinforcement learning in the multi-robot domain. In Robot colonies.
Springer, 73-83.
Menda, K., Chen, Y.-C., Grana, J., Bono, J. W., Trac ey, B . D., Koc henderfer, M. J.
and Wolpert, D. (2018). Deep reinforcement learning for event-driven multi-agent decision
processes. IEEE Transactions on Intelligent Transportation Systems 20 1259-1268.
Mordatch, I. and Abbeel, P. (2018). Emergence of grounded compositional language in multi-
agent populations. In Thirty-Second AAAI Conference on Artificial Intelligence.
Muandet, K., FUKUMIzu, K., Sriperumbudur, B. and Scholkopf, B. (2016). Kernel mean
embedding of distributions: A review and beyond. arXiv preprint arXiv:1605.09522 .
pANAIT, L. and LuKE, S. (2005). cooperative multi-agent learning: The state of the art. Autonomous
agents and multi-agent systems 11 387-434.
Schulman, J., Wolski, F., Dhariwal, p., Radford, A. and Klimov, O. (2017). proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 .
S halev- S hwartz, S., Shammah, S. and Shashua, A. (2016). Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295 .
Smola, A., Gretton, A., Song, L. and S chölkopf, B. (2007). A hilbert space embedding for
distributions. In International Conference on Algorithmic Learning Theory. Springer.
Song, L., Huang, J., Smola, A. and Fukumizu, K. (2009). Hilbert space embeddings of
conditional distributions with applications to dynamical systems. In Proceedings of the 26th
Annual International Conference on Machine Learning.
Sunehag, p., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M.,
Lanctot, M., S onnerat, N., Leibo, J. Z., Tuyls, K. et al. (2017). Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296 .
SuTTON, R. S. and BARTO, A. G. (2018). Reinforcement learning: An introduction. MIT press.
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J.,
Choi, D. H., Powell, R., Ewald s, T., Georgiev, p. et al. (2019). Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature 575 350-354.
Wang, L., Yang, Z. and Wang, Z. (2020). Breaking the curse of many agents: provable mean
embedding q-iteration for mean-field reinforcement learning. In International Conference on
Machine Learning. pMLR.
Yang, J., Ye, X., Trivedi, R., Xu, H. and Zha, H. (2017). Learning deep mean field games for
modeling large population behavior. arXiv preprint arXiv:1711.03156 .
Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W. andWang, J. (2018). Mean field multi-agent
reinforcement learning. arXiv preprint arXiv:1802.05438 .
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R. and
S MOLA, A . J . (2017). Deep sets. In Advances in neural information processing systems.
11
Under review as a conference paper at ICLR 2022
Zhang, K., Yang, Z. and Basar, T. (2019). Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635 .
Zheng, S., Trott, A., Srinivasa, S., Naik, N., Gruesbeck, M., Parkes, D. C. and Socher,
R. (2020). The ai economist: Improving equality and productivity with ai-driven tax policies.
arXiv preprint arXiv:2004.13332 .
12