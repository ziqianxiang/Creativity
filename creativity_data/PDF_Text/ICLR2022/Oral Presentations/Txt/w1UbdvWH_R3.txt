Published as a conference paper at ICLR 2022
Neural Collapse Under MSE Loss: Proximity
to and Dynamics on the Central Path
X.Y. Han*
Cornell University
xh332@cornell.edu
Vardan Papyan*
University of Toronto
vardan.papyan@utoronto.ca
David L. Donoho
Stanford University
donoho@stanford.edu
Ab stract
The recently discovered Neural Collapse (NC) phenomenon occurs pervasively
in today’s deep net training paradigm of driving cross-entropy (CE) loss towards
zero. During NC, last-layer features collapse to their class-means, both classifiers
and class-means collapse to the same Simplex Equiangular Tight Frame, and clas-
sifier behavior collapses to the nearest-class-mean decision rule. Recent works
demonstrated that deep nets trained with mean squared error (MSE) loss perform
comparably to those trained with CE. As a preliminary, we empirically establish
that NC emerges in such MSE-trained deep nets as well through experiments on
three canonical networks and five benchmark datasets. We provide, in a Google
Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: here. The
analytically-tractable MSE loss offers more mathematical opportunities than the
hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical
investigation of NC. We develop three main contributions: (I) We show a new
decomposition of the MSE loss into (A) terms directly interpretable through the
lens of NC and which assume the last-layer classifier is exactly the least-squares
classifier; and (B) a term capturing the deviation from this least-squares classifier.
(II) We exhibit experiments on canonical datasets and networks demonstrating that
term-(B) is negligible during training. This motivates us to introduce a new theo-
retical construct: the central path, where the linear classifier stays MSE-optimal
for feature activations throughout the dynamics. (III) By studying renormalized
gradient flow along the central path, we derive exact dynamics that predict NC.
1	Introduction
Modern deep learning includes paradigmatic procedures that are commonly adopted, but not entirely
understood. Some examples are multi-layered architectures, stochastic gradient descent, batch
normalization, cross-entropy (CE) loss, and training past zero error towards zero loss. Analyzing the
properties of these practices is an important research task.
In this work, we theoretically investigate behavior of last-layer features in classification deep nets.
In particular, we consider the training of deep neural networks on datasets containing images from
C different classes with N examples in each class. After passing the i-th example in the c-th class
through all layers except the last-layer of the network, the network outputs some last-layer features
hi,c ∈ RP. The last-layer of the network—which, for each class c, possesses a classifier wc ∈ RP
and bias bc ∈ R—then predicts a label for the example using the rule arg maxc0 (hwc0 , hi,ci + bc0).
The network’s performance is evaluated by calculating the error defined by
i,c	c0
Error = Ave 1{c 6= arg max (hwc0 , hici + bc0)},
ic	0
while the weights, biases, and other parameters of the network (that determine the behavior of the
layers before the last layer) in the network are updated by minimizing the CE loss defined by
CE = - Ave log	CeXMhwc，hi〉+ bc}
i,c 0 exp{hw 0 , h i + b 0 }
cC0=1 eXp{hwc0， hi,ci + bc0}
* Equal contribution. Listed alphabetically.
1
Published as a conference paper at ICLR 2022
where Ave is the operator that averages over its subscript indices.
Prior works such as Zhang et al. (2016); Belkin et al. (2019) have shown that overparameterized
classifiers (such as deep nets) can “memorize” their training set without harming performance on
unseen test data. Moreover, works such as Soudry et al. (2018) have further shown that continuing
to train networks past memorization can still lead to performance improvements1,2. Papyan, Han,
and Donoho (2020) recently examined this setting, referring to the phase during which one trains
past zero-error towards zero-CE-loss as the Terminal Phase of Training (TPT). During TPT, they
exposed a phenomenon called Neural Collapse (NC).
1.1	Neural Collapse
NC is defined relative to the feature global mean,
μG = AVe hi,c,
i,c
the feature class-means,
μc = AVe hi,c,	C = 1, . . . , C,
i
the feature within-class covariance,
∑w = Ave(hi,c - μc)(hi,c - μc)>,	(1)
and the feature between-class covariance,
∑B = Ave(μc - μG)(μc - μG)>.
It is characterized by the following four limiting
behaviors where limits take place with increasing
training epoch t:
(NC1) Within-class variability collapse1 2 3 :
∑B∑W → 0,
where ↑ denotes the Moore-PenroSe pseudoinverse.
(NC2) Convergence to Simplex ETF:
hμc - Mg, μc - Mg、→ J 1,	C = C
kμc- μGl∣2l∣μco - ngl∣2	C- c-ι， C = c
kμc - μG k2 - kμc0 - μG k2 → 0 ∀c = C
(NC3) Convergence to self-duality:
wc	μc - μG	、八
0 0
kwck2	kμc - μG∣∣2
(NC4): Simplification to nearest class center:
Figure 1: Portrait of Neural Collapse. Top
figure depicts the last-layer features, class-
means, and classifiers with which NC is
defined—as well as the Simplex ETF to which
they all converge with training. Bottom figure
shows the deviations of features from their
corresponding class-means. Reproduced and
modified from Figure 1 of Papyan, Han, and
Donoho (2020).
arg max hwc∕, h) + bc — arg min ∣∣h — μco ∣2
c0	c0
1A similar phenomenon had been previously observed in boosting (Bartlett et al., 1998).
2An alternative line of work on “early stopping” (Prechelt, 1998; Li et al., 2020; Rice et al., 2020) advocates
for terminating the training process early and shows that it could be beneficial when training on noisy data or
small datasets. Such datasets are out of the scope of this paper.
3Our characterization of (NC1) is more precise than that given by Papyan, Han, and Donoho (2020), which
only states ∑w→0 for rhetorical simplicity. The convergence of the trace-norm of ∑B ∑w to zero is the
actual quantity measured by Figure 6 of Papyan, Han, and Donoho (2020) when demonstrating (NC1). ∑B ∑w
2
Published as a conference paper at ICLR 2022
The (NC2) property captures convergence to a simple geometric structure called an Equiangular Tight
Frame (ETF). An ETF is a collection of vectors {vc}cC=1 having equal lengths and equal, maximally
separated pair-wise angles. In classification deep nets, last-layer features are of higher dimension
than the number of classes i.e. P>C. In this setting, the maximal angles are given by
hvc, Vc”	_ (	1,	for C =	C
∣Wck2M01∣2 I	-C-1，	for C =	c0	,
and the ETF is called a Simplex ETF4. Observe that as C increases, the ETF approaches a (partial)
orthogonal matrix. Thus, when C is large, this translates to the intuitive notion that classifiers and
class-means tend with training to (near) orthogonality.
1.2 Deep net classification with MSE loss
While classification deep nets are typically trained with CE loss, Demirkaya et al. (2020) and Hui &
Belkin (2020) recently reported deep nets trained with squared error (MSE) loss,
L(W, b, H) =1 AVe kWhi,c + b -仍,/|2 +	(kWkF + kbk2)	⑵
2 i,c	2
={ kWH + bi>N - Y kF + λ (kW kF + kbk2),
2CN	2
achieve comparable test performance as those trained with CE loss. Above, H ∈ RP×CN and
Y ∈ RC×CN are matrices resulting from stacking5 together the feature vectors hi,c and one-hot
vectors yi,c as their respective columns; W ∈ RC×P is the matrix resulting from the stacking of
classifiers wcas rows; b ∈ RC is the vector resulting from concatenating the scalars {bc}cC=1; and
1CN is the length-CN vector of ones. Table 1 in Appendix A shows supplementary measurements
affirming the findings of Hui & Belkin (2020); Demirkaya et al. (2020), i.e. the measurements
affirm that MSE-trained networks indeed achieve accuracies on testing data comparable to those
of CE-trained networks (cf. the analogous Table 1 in Papyan, Han, and Donoho (2020)). The
analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE
loss and inspires this paper’s main theoretical contributions explicitly characterizing MSE-NC.
1.3 Contributions
Our main contributions are as follows:
•	We propose a new decomposition of the MSE loss: L = LNC1 + LNC2/3 + LL⊥S. The terms
LNC1 and LNC2/3 possess interpretations attributable to NC phenomena and assume the
classifier W is exactly the least-squares optimal classifier WLS (relative to the given H);
and LL⊥S captures the deviation of W from WLS. (Section 2)
•	We provide empirical measurements of our decomposition (Figure 2) on realistic dataset-
network combinations showing that LL⊥S becomes negligible during training—leading us to
define the central path where LL⊥S =zero. (Section 2)
•	We reveal a key invariance property on the central path. The invariance motivates the
—1
examination of a representative set of features, X=∑W H, that We call renormalized
features. (Sections 3.1-3.2)
•	We study the gradient floW of those renormalized features along the central path and derive
exact, closed-form dynamics that imply NC. The dynamics are explicit in terms of the
captures the subtlety that the size of the Within-class “noise” (captured by ΣW) should be vieWed relative to the
size of the class-means (captured by ΣB).
4Traditional research on ETFs (cf. Strohmer & Heath Jr (2003); Fickus & Mixon (2015)) examines the
P≤C setting Where C ETF-vectors span their ambient RP space. HoWever, in the P>C setting of classification
deep nets, the vectors can not span RP . FolloWing the precedent of Papyan, Han, and Donoho (2020), We
interpret ETFs as equal-length and maximally-equiangular vectors that are not necessarily spanning.
5Assume the stacking is performed in i-then-c order: the first column is (i, c) = (1, 1), the second is
(i, c) = (2, 1),..., the N-th is (i, c) = (N, 1), the (N + 1)-st is (i, c) = (1, 2), and so on. This matters for
formalizing Equation 11 in Corollary 2.
3
Published as a conference paper at ICLR 2022
singular value decomposition of the renormalized feature class-means at initialization.
(Section 3.3)
Additionally, we complement this paper with new, extensive measurements on five benchmark
datasets—in particular, the MNIST, FashionMNIST, CIFAR10, SVHN, and STL10 datasets6 (Deng
et al., 2009; Krizhevsky & Hinton, 2009; LeCun et al., 2010; Xiao et al., 2017)—and three canonical
deep nets—in particular, the VGG, ResNet, and DenseNet networks (He et al., 2016; Huang et al.,
2017; Simonyan & Zisserman, 2014)—that verify the empirical reality of MSE-NC i.e. they show
(NC1)-(NC4) indeed occur for networks trained with MSE loss. These experiments establish that
theoretical modeling of MSE-NC is empirically well-motivated. They are lengthy—together spanning
four pages with seven figures and a table—so we collectively defer them to Appendix A.
2 Decomposition of MSE loss
Inspired by the community’s interest in the role of the MSE loss in deep net training (Demirkaya
et al., 2020; Hui & Belkin, 2020; Mixon et al., 2020; Poggio & Liao, 2020a;b), we derive a new
decomposition of the MSE loss that gives insights into the NC phenomenon. First, absorb the bias
vector into the weight matrix—by defining the extended weight matrix W = [W , b] ∈ RC×(P+1)
IT	,11C,	,丁	Γ7	ri 一 E P-U1	.1	. T-,	J C	1	♦一
and the extended feature vector hi,c = [hi,c; 1] ∈ RP+1—so that Equation 2 can be rewritten as
L(W,f) = 2 Ave k W h L yi,ck2 + 2 k W kF.	⑶
Using h hi,c}, define the entities μzo μG, H, and ∑w analogously to those in Section 1.1. We
further define the extended total covariance and extended class-means matrices, respectively, as
∑T = Ave(hi,c- “G)(hi,c- Mg)> ∈ R(P+1)×(P +1)
i,c
M = [μι,...,ec] ∈R(P+1)×C.
Next, we reformulate, with weight decay incorporated, a classic result of Webb & Lowe (1990):
Proposition 1 (Webb & Lowe (1990) with Weight Decay). For fixed extended features H, the
optimal classifier minimizing the MSE loss L(W, H) is
WS = ɪ M>(Σ T + μGμ> + λI )-1,
C
where I is the identity matrix. Note that WLS depends on H only.
Note that we can interpret WLS as a function of H, leading us to identify the following decomposition
of L where one term depends on H only.
Theorem 1.	(Decomposition of MSE Loss; Proof in Appendix B) The MSE loss, L(W, H), can be
⊥
decomposed into two terms, L(W, H) = LLS(H) + LL⊥S(W, H), where
1λ
LLS(H) = 2 AVe kWLShi,c - yi,ck2 + 2 IlWSIlF,
and
L⊥S(W, f) = 2 tr{ (W - WS) (∑T + μGμ> + λI) (W - ffLS)> }.
In the above, LLS(H) is independent of W. Intuitively, it is the MSE-performance of the optimal
classifiers WLS (rather than the “real classifiers” W) on input H.
6CIFAR100 and ImageNet are omitted because Demirkaya et al. (2020); Hui & Belkin (2020) showed that
they require an additional scaling-heuristic on top of the traditional MSE loss. Rigorous investigation into
this scaling-heuristic would have demanded mathematical analysis outside the scope of this paper as well as
experimentation that would have consumed expensive amounts of computational resources.
4
Published as a conference paper at ICLR 2022
-—■ -—■
-—■ -—■
-—■
⊥7	⊥
The component LL⊥S(W , H) is non-negative7 and is zero only when W = WLS. Therefore, LL⊥S(H)
-—■
-—■
-—■.
quantifies the distance of W from WLS. In short, the least-squares component, LLS (H), captures
the behavior of the network when the classifier possesses optimal, least squares behavior. Meanwhile,
⊥
the deviation component, LL⊥S(W , H), captures the divergence from that behavior.
We can further decompose LLS (H) into two terms, one capturing activation collapse (NC1) and the
other capturing convergence to Simplex ETF of both features and classifiers ((NC2) and (NC3)).
Theorem 2.	(Decomposition of Least-Squares Component; Proof in Appendix C) The least-squares
component, LLS (H), of the MSE decomposition in Theorem 1 can be further decomposed into
-—■.
-—■.
-—■.
LLS(H) = LNC1(H) + LNC2/3(H), where
L NC1 (f) = 2 tr{ffs h∑ w + λI i W> 0,
1
L NC2/3 (H) = 2C kWLs M - I kF.
Inspection of these terms is revealing. First, observe that LNC2/3 is a function of the class-means and
MSE-optimal classifiers. Minimizing LNC2/3 will push the (unextended) class-means and classifiers
towards the same Simpex ETF matrix8 i.e. (NC2)-(NC3). Next, note that the within-class variation
is independent of the means. Thus, despite the fact that classifiers are converging towards some
(potentially large) ETF matrix, we can always reduce LNC1 by pushing ΣW towards zero, which
corresponds to (NC1) .
Figure 2 measures the empirical values of the above decomposition terms on five canonical datasets
⊥
and three prototypical networks. It shows that LL⊥S(W , H) becomes negligible compared to LLS(H)
-- -~~

when training canonical deep nets on benchmark datasets. In other words, L(W , H)≈LLS(H)
starting from an early epoch in training and persists into TPT.
This motivates us to formulate the central path:
P = (WfLS(Hf), Hf) | Hf ∈ R(P+1)×CN ,
(4)
-—■
-—■
-—■
where the notation WLs(∙) makes explicit the fact that WLS is a function of H only. Intuitively,
for a classifier-features pair to lie on the central path, i.e. (W , H) ∈ P, means that the “real
-—■
-—■ -—■
-—■
classifier” W will exactly equal WLS(H)—the optimal classifier that would result from fixing H
and minimizing L w.r.t. just the classifier. Combined with Theorem 1, we see (W , H) ∈ P if and
-—■ -—■.
-—■.
only if L(W, H)=LLS (H). Figure 2 shows that classifier-features pairs lie approximately on the
central path during TPT, allowing us to shift focus from L to LLS.
3 Exact closed-form analysis on central path
Consider λ = 0 in this section. For the subsequent theory, we adopt the unconstrained features (Mixon
et al., 2020) or layer-peeled (Fang et al., 2021) modeling perspective9. We consider continuous
training dynamics with gradient flow where time-of-training is denoted by the variable10 t≥0. Within
-1
w2 H on the central path
this model, we analyze the dynamics of the renormalized features X=Σ
ddtX = -∏TχX (VXLLS(X)),
(5)
7The middle term is the sum of positive-semidefinite matrices—one of which is positive-definite.
8Appendices C.1-C.2 elaborates upon this interpretation of LNC2/3 in more detail.
9Both the unconstrained features model (Mixon et al., 2020) and the (1-)layer-peeled model (Fang et al.,
2021) advocate for the mathematical analysis of deep nets through variants of gradient flow directly on the
last-layer features and classifiers. The layer-peeled model is so-named because it proposes to analyze deep nets
by "peeling away" one layer at a time. The unconstrained features model is so-name because features are not
constrained to be the output of a deep net forward pass but are rather free variables that can be optimized directly:
This captures the nearly unlimited flexibility afforded to feature engineering transformations in modern deep
nets by the many nonlinear, overparameterized layers.
10Intuitively, one should interpret t=0 as the time-of-training when feature-classifier pairs effectively enter
the central path.
5
Published as a conference paper at ICLR 2022
Figure 2: Decomposition of MSE loss: Each array column shows a benchmark image classification
dataset while each row shows a canonical deep net architecture trained with MSE loss. The red vertical
line indicates the epoch at which zero training error was achieved. In each array cell, we plot terms of
the MSE loss decomposition L(W, H) = LNC1 (H) + LNC2/3 (H) + LL⊥S(W , H) from Section 2.
⊥
Starting from an early epoch in training, LL⊥S(W , H) becomes negligible compared to the dominant
C	∕~x	∙	r .	Cl ∕∖τ ~x	C ∕~x C	∕~x	. C	Z ~X .	”
term, Lnci(H), implying L⊥s(W, H)≪Lls(H)=Lnci(H)+Lnc2/3(H), i.e. the features and
classifiers are effectively on the central path during TPT. Note that LNC2/3 (H) diminishes the fastest
among all the terms: Intuitively, this shows that the network primarily focuses on distributing the
feature class-means into a “uniform” simplex ETF configuration (NC1)-(NC2) early on and, from
there, compresses the activations towards their class-means, i.e. (NC1) , as much as possible. Further
experimental details are in Appendix A. Outlier behavior is discussed in Appendix A.7.
where the operator ΠTXX projects the gradient onto the tangent space of the manifold11, X, of all
-1
identity-covariance features. For ΣW2 H to be well-defined, we assume that ΣW remains full-rank12
during training. We call Equation 5 the continually renormalized gradient flow and will motivate it in
more detail in later subsections. We now state our main theorem:
Theorem 3. (NC Under Continually Renormalized Gradient Flow) Consider the continually
renormalized gradient flow (Equation 5) in which the dynamics are restricted to the central path, and
the features are renormalized to identity within-class covariance. Assume the features are initialized
with zero global mean. Then, Neural Collapse emerges on the renormalized features with explicit
dynamics of the flow given in Proposition 2 as well as Corollaries 1 and 2 later in this paper.
Three assumptions standout which we discuss below:
⊥
(A1) Restriction to central path: Figure 2 shows that—in practice, during TPT—L⊥s (W, H) is
observed to be negligible compared to the dominant term, LLs (H). This indicates (W, H) is close
to the central path (Equation 4) where L(W, H) = LLs (H). One the central path, one is effectively
training with LLs as the loss.
(A2) Zero global mean: Proximity to the central path implies that the last-layer biases become close
to bLs. Having biases b = bLs is effectively performing classification with no bias term (i.e. using
only a linear classifier W) on zero-mean data. To see this, define the globally-centered features,
11X is a variant of the well-known generalized Stiefel manifold (Absil et al., 2009, Chapter 3.4).
12Given the definition in Equation 1, the positive-definiteness of ΣW is unsurprising in deep net contexts
because CN P i.e. the number of examples is larger than the width of the last-layer. Note this does not
contradict (NC1). (NC1) is characterized by ΣW approaching the zero-matrix with training (relative to ΣB). In
practice (for example, in our supplementary Appendix A experiments), we observed that the zero-matrix is never
actually achieved during the training dynamics and ΣW indeed remains full-rank.
6
Published as a conference paper at ICLR 2022
globally-centered means, and total-covariance matrix in unextended coordinates:
H = H - μG1>N;	M = [μ1 - μG,.∙∙, μC - μG];	ςT = 7^X7HH>.
CN
Proposition 1 then reduces to the following forms for the unextended weights and biases:
WLS = [WLs, bLS] = [C-1M>Σ-1,	C-11c - C-1Mf>∑T1μG].
(6)
For an arbitrary activation-target pair (h, y), the prediction error obeys
WLSh + bLS — y = C-1M>∑T1h + C-11c - C-1M>∑T1μG - y
=WLs(h - μg) - (y - C-11c).
The last line exhibits two terms in parentheses, both traceable to the action of the bias bLS. The term
h - μG demonstrates that the bias induces a global-mean subtraction on h, while the term y - C-11
demonstrates that the bias also induces a global-mean subtraction on the one-hot targets.
(A3) Renormalization to identity covariance: Renormalization is ubiquitous in the empirical deep
learning literature through works such batch normalization and its variants (Ioffe & Szegedy, 2015;
Salimans & Kingma, 2016; Wu & He, 2018; Ulyanov et al., 2016; Ba et al., 2016; Krizhevsky et al.,
2012). It is also ideologically precedented in the theoretical ML literature through works such as
Douglas et al. (1998); Banburski et al. (2019); Poggio & Liao (2020a;b). Within this paper, the
covariance-renormalization is inspired by an invariance property of the loss as well as the intuitive
concept of the signal-to-noise ratio from mathematical statistics (discussed later in Sections 3.1-3.2).
In the subsequent sections, we motivate and introduce the theoretical constructs necessary for proving
Theorem 3 and for presenting the explicit dynamics of the flow.
3.1 Invariance property
By Assumption (A2), classification on the central path is equivalent to classification on globally-
centered features using only the linear classifier WLS. Then, using Equation 6, we can equivalently
represent the central path (Equation 4) as follows:
Definition 1 (Zero Global Mean Central Path).
P = {(W£s, H) I Wls = C-1M1>∑T1O .
An invariance property holds on the central path. Let A denote a symmetric, full-rank matrix. Then,
the explicit form in Equation 6 for WLS implies
Wls(AH)AH = CT (AM)> [(AH) (AH)] -1 AH
=CTM>A [A-1 (HH>)-1 ATi AH	⑺
=CTM> (HH>)-1 H = C-1M>∑t1 H = WLS (H) H,
where the notation WLs(∙) makes explicit that WLS is a function of a set of input features. Thus, the
actual predictions made by the least squares classifier are invariant to choice of the coordinates in
which we express H, i.e. all features AH are equivalently performing. Among those coordinate
systems, we will prefer the one in which the “noise” is “whitened” or “sphered.” Recall that we assume
∑w(H)—where the notation ∑w(∙) expresses the within-class covariance as a function of the
L- 1
features——is positive-definite. Consider the coordinate transformation H→AH with A = ∑W (H).
In these coordinates, the features are “renormalized” to spherical covariance,
Σw (AH) = A∑w (H)A = I,
so we will call AH the Tenormalizedfeatures. The class-means of AH are
一 ..一.	. 一 ,一.	一1 .一. 一 ,一.
M(AH) = AM(H) = ∑w2 (H)M(H),
where the notation M(∙) expresses that the means are a function of features. Thus, Equation 5
describes continuous training dynamics where we continually “renormalize” features to their
equivalently-performing representer.
7
Published as a conference paper at ICLR 2022
3.2 Signal-to-Noise ratio matrix on the central path
-1 —
The ∑w2 M term evokes the canonical idea of a Signal-to-noise ratio from statistics. To see this, note
that the classifier in Equation 6 can be rewritten as
WLS = C-1M> (C-1MM> + ςw)-1.
The combined presence of the terms MM> and ∑w in the denominator, along with M in the
numerator, signals to us the presence of the informative signal-to-noise (SNR) ratio matrix:
_i —
SNR ≡ ∑w2 M.	(8)
Why this terminology? The globally-centered class-means represent the overall “signal” indicating
that the class-means are separated from each other: if they were non-separated-e.g. all equal—M
would be zero. However, a classifier decision must keep in view noise—captured by ΣW —which
might confuse the classifier. So, one wishes the signal be large compared to the noise. It may help to
consider the mental model—which is not at all necessary to the correctness of our analysis—under
which the activations are normally distributed with hi,c 〜N(μc, ∑w). Under that model, a linear
classifier will indeed get confused if the norm ∣∣μc - μG∣∣2 is “small compared” to ∑w. Replacing
this somewhat vague statement by a discussion of quantitative properties of the SNR matrix is well
understood by mathematical statisticians to be decisive for understanding classification performance
-	-	L- 1
in the normal case. Additionally, observe that SNR = M(∑W H). Thus, connecting back to
Section 3.1, the SNR matrix is simply the class-means matrix of the renormalized features.
The SNR can be further understood through its singular value decomposition (SVD).
Definition 2 (SVD of SNR Matrix). Denote the SVD of the SNR matrix (Equation 8) as follows:
C-1
SNR = UΩV> = X ωjUjv>.
j=1
Here, {ωc}cC=-11 are the non-zero singular values of SNR; Ω = diag ({ωc}C=-L1,0)∈ RC × C; and the
left and right singular-vectors, U ∈ RP ×C and V ∈ RC ×C , are partial orthogonal and orthogonal
matrices, respectively, with columns {uj}jC=1 and {vj}jC=1. The SNR matrix is rank C - 1 since
∑w is assumedfull-rank and M is rank C — 1 due to global-mean subtraction.
The non-zero singular values, {ωj}jC=-11, are decisive for understanding the separation performance
of the least-squares linear classifier. Good performance demands that the singular values be large.
Works such as Fukunaga (1972, Chapter 10) and Hastie & Tibshirani (1996) show that the magnitude
of each singular value of the SNR matrix is the size of the class separation in the direction of
the corresponding singular vector. In this sense, the smallest singular value captures the smallest
separation between classes. Consequently, driving-larger the smallest non-zero singular value makes
the task of linear separation more immune to (spherical) noise and to Euclidean-norm-constrained
adversarial noise. As we will see in the next section, examining the dynamics of these singular-values
when the features undergo gradient flow in renormalized coordinates leads to precise characterizations
of Neural Collapse13.
3.3 Dynamics of SNR
For any matrix Z, we use Zt in this section to denote the state of that matrix at time t. Similarly,
we denote with {ωj (t)}jC=-11 the state of the non-zero SNR singular values at t. We now present
Proposition 2 and Corollaries 1-2 that provide the explicit dynamics referenced by Theorem 3.
Proposition 2 (Dynamics of Singular Values of SNR Matrix; Proof in Appendix D.4). Continually
renormalized gradient flow on the central path (Equation 5) induces the following closed-form
dynamics on the SNR singular values (Definition 2):
c1 log(ωj (t)) + c2ωj2(t) + c3ωj4(t) = aj + t, t ≥ 0, for all j = 1, . . . , C - 1.	(9)
c1, c2, and c3 are positive constants independent ofj, and aj is a constant depending on ωj (0).
13For additional, supplementary intuitions about invariance, renormalization, and continually renormalized
gradient flow, see the exposition in Appendices D.1-D.3.
8
Published as a conference paper at ICLR 2022
Corollary 1 (Properties of SNR Singular Values; Proof in Appendix D.5). SNR singular values
(Definition 2) following the Equation 9 dynamics satisfy the following limiting behaviors:
1.	limt→∞ ωj (t) = ∞ and	limt→∞ :j(t- = 1,	for all j = 1,...,C 一 1.
t/c3
2	Iirn	maχj ωj⑴ _ ι
2.	limt→∞ minj ωj(t) = 1.
-1 ___
The first fact shows that all non-zero singular values of the SNR matrix, ∑W M, grow to infinity
at an asymptotic rate of 4jt∕c3. Intuitively, this shows that the “signal" becomes infinitely
large compared to the “noise”, i.e. (NC1). The second fact shows that the singular values of
L- 1 ⅞τ? Z ...	1	、	1	…	∙	1 ∙	..	. .<	1∙	∙.∙	..
∑w2 M (which has zero-mean columns) approach equality-implying that the limiting matrix is a
Simplex ETF (Lemma 7, Appendix D.6) i.e. (NC2). Finally, Papyan, Han, and Donoho (2020, The-
orem 1) show that (NC1-2) imply (NC3-4) on the central path. Corollary 2 formalizes these intuitions.
Corollary 2 (Neural Collapse Under MSE Loss; Proof in Appendix D.6). Under continually
renormalized gradient flow (Equation 5), the SNR matrix (Equation 8) converges to
tl→∞ ωm⅛sNRt = M
(10)
where Ub0 ∈ RP×(C-1) and Vb0 ∈ RC×(C-1) are the left and right singular vectors of the SNR
matrix (Definition 2) at t=0 corresponding to the non-zero singular values; and ωmax (t) is the
largest singular value at time t. Furthermore, Corollary 1 implies the occurrence of (NC1)-(NC4) i.e.
renormalized gradient flow on the central path leads to Neural Collapse.
Moreover, denoting the Kronecker product with 0, the renormalizedfeatures matrix converges to
,lim -⅛∑w2tHt = (Ub0V>) 0 1N.
t→∞ ωmax (t)
(11)
4 Related Works
Since the publication of Papyan, Han, and Donoho (2020), several works (Mixon et al., 2020; Lu &
Steinerberger, 2020; E & Wojtowytsch, 2020; Poggio & Liao, 2020a;b; Fang et al., 2021; Ergen &
Pilanci, 2020; Zhu et al., 2021) proposed mathematical frameworks ratifying Neural Collapse.
Among them, Mixon et al. (2020) and Poggio & Liao (2020a;b) also examine MSE-NC. Our contri-
butions are distinct from the MSE-NC analyses of Mixon et al. (2020) and Poggio & Liao (2020a;b):
The work of Mixon et al. (2020) relies on a linearization of the unconstrained features model ODE
while Poggio & Liao (2020a;b)—in a special section co-authored with Andrzej Banburski—examine
homogeneous, weight-normalized networks. In contrast, this paper considers the dynamics of the
renormalized gradient flow on the central path (motivated by the discussions in Sections 2-3). Neither
Mixon et al. (2020) nor Poggio & Liao (2020a;b) provide exact, closed-form dynamics as we do.
Outside the MSE setting, Lu & Steinerberger (2020); E & Wojtowytsch (2020); Ergen & Pilanci
(2020); Fang et al. (2021); Zhu et al. (2021) examine the emergence of the NC properties under
variants of the unconstrained features/layer-peeled model trained with CE loss. These works focus on
characterizing the loss landscape or the global minima without describing the dynamics.
We provide a detailed survey of all above-mentioned papers in Appendix E. 5
5 Conclusion
In this paper, after verifying that NC occurs when prototypical classification deep nets are trained with
MSE loss on canonical datasets, we then derive and measure a novel decomposition of the MSE loss.
We observed that the last-layer classifier tends to the least-squares classifier—motivating us to define
the central path on which the classifier behaves exactly as optimal least-squares classifier. On the
central path, we showed invariance properties that inspired us to examine the renormalized features
and their corresponding continually renormalized gradient flow. This flow induces closed-form
dynamics that imply the occurrence of Neural Collapse.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
This paper is reproducible. Experimental details about all empirical results described in this paper are
provided in Appendix A. Additionally, we provide PyTorch (Paszke et al., 2019) code for reproducing
NC under both MSE and CE—as well as the Figure 2 MSE decomposition—in the following Google
Colaboratory notebook: here. Experimental measurements used to generated the plots in this paper
have been deposited in the Stanford Digital Repository, here. All datasets and networks used in
our experiments originate from the PyTorch Model Zoo with pre-processing details described in
Appendix A. Formal statements and proofs of all our theoretical results are provided in Appendices
B-D.
Acknowledgements
Some of the computing for this project was performed on the Sherlock cluster. We would like to
thank Stanford University and the Stanford Research Computing Center for providing computational
resources and support that contributed to these research results.
We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada
(NSERC), [funding reference number 512236 and 512265]. This research was funded by a Connaught
New Researcher Award at the University of Toronto. This research was enabled in part by support pro-
vided by Compute Ontario (www.computeontario.ca) and Compute Canada (www.computecanada.ca).
This work was also partially supported by NSF Division of Mathematical Sciences Grants 1407813,
1418362, and 1811614 and by private donors.
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.
Princeton University Press, 2009.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Fernanda De La Torre, Jack
Hidary, and Tomaso Poggio. Theory iii: Dynamics and generalization in deep networks. arXiv
preprint arXiv:1903.04991, 2019.
Andrzej Banburski, Fernanda De La Torre, Nishka Plant, Ishana Shastri, and Tomaso Poggio. Cross-
validation stability of deep networks. Technical report, Center for Brains, Minds and Machines
(CBMM), 2021.
Peter Bartlett, Yoav Freund, Wee Sun Lee, and Robert E Schapire. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651-1686,
1998.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849-15854, 2019.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. Advances in neural information processing systems, 31:3036-3046,
2018.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Ahmet Demirkaya, Jiasi Chen, and Samet Oymak. Exploring the role of loss functions in multiclass
classification. In 2020 54th Annual Conference on Information Sciences and Systems (CISS), pp.
1-5. IEEE, 2020.
10
Published as a conference paper at ICLR 2022
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on,pp. 248-255. Ieee, 2009.
Scott C Douglas, SY Kung, and Shun-ichi Amari. A self-stabilized minor subspace rule. IEEE Signal
Processing Letters, 5(12):328-330, 1998.
Weinan E and Stephan Wojtowytsch. On the emergence of tetrahedral symmetry in the final and
penultimate layers of neural network classifiers. arXiv preprint arXiv:2012.05420, 2020.
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
arXiv preprint arXiv:2002.09773, 2020.
Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Layer-peeled model: Toward understanding
well-trained deep neural networks. arXiv preprint arXiv:2101.12699, 2021.
Matthew Fickus and Dustin G Mixon. Tables of the existence of equiangular tight frames. arXiv
preprint arXiv:1504.00253, 2015.
Keinosuke Fukunaga. Introduction to Statistical Pattern Recognition. Elsevier, 1972.
Trevor Hastie and Robert Tibshirani. Discriminant analysis by gaussian mixtures. Journal of the
Royal Statistical Society: Series B (Methodological), 58(1):155-176, 1996.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-
entropy in classification tasks. arXiv preprint arXiv:2006.07322, 2020.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448-456.
PMLR, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. AT&T Labs
[Online]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International conference
on artificial intelligence and statistics, pp. 4313-4324. PMLR, 2020.
Jianfeng Lu and Stefan Steinerberger. Neural collapse with cross-entropy loss. arXiv preprint
arXiv:2012.08465, 2020.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
11
Published as a conference paper at ICLR 2022
Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. arXiv
preprint arXiv:2011.11619, 2020.
Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of Neural Collapse during the terminal
phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):
24652-24663, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Tomaso Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers
trained with the square loss. arXiv preprint arXiv:2101.00072, 2020a.
Tomaso Poggio and Qianli Liao. Implicit dynamic regularization in deep networks. Technical report,
Center for Brains, Minds and Machines (CBMM), 2020b.
Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pp. 55-69. Springer,
1998.
Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In
International Conference on Machine Learning, pp. 8093-8104. PMLR, 2020.
Grant M Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time conver-
gence and asymptotic error scaling of neural networks. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems, pp. 7146-7155, 2018.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. Advances in neural information processing systems, 29:901-909,
2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition, 2014.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Thomas Strohmer and Robert W Heath Jr. Grassmannian frames with applications to coding and
communication. Applied and computational harmonic analysis, 14(3):257-275, 2003.
James Townsend. Differentiating the singular value decomposition. Technical report, Technical
Report 2016, https://j-towns. github. io/papers/svd-derivative . . . , 2016.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing
ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Andrew R Webb and David Lowe. The optimised internal representation of multilayer classifier
networks performs nonlinear discriminant analysis. Neural Networks, 3(4):367-375, 1990.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3-19, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. 2016.
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A
geometric analysis of neural collapse with unconstrained features. arXiv preprint arXiv:2105.02375,
2021.
12
Published as a conference paper at ICLR 2022
Appendix
A MSE Neural Collapse experiments	14
A.1	Datasets ................................................................... 14
A.2	Networks ................................................................... 14
A.3	Optimization methodology ................................................... 14
A.4	Computational resources .................................................... 14
A.5	Formatting ................................................................. 15
A.6	Experimental results ....................................................... 15
A.7	Exceptions: STL10-ResNet and STL10-DenseNet ................................ 21
A.8	Open Questions: Weight-decay, batch normalization, SGD, generalization, test data 21
B	Theorem 1	22
C	Theorem 2	22
C.1	Intuitions for Theorem 2 in unextended coordinates ......................... 23
C.2	Additional intuitions for Theorem 2 terms .................................. 23
D	Theorem 3	23
D.1 Implications of invariance .................................................. 23
D.2 Aligned SNR coordinates ..................................................... 25
D.3 Continually renormalized gradient flow ...................................... 27
D.4	Proof of Proposition 2 (gradient flow in aligned SNR coordinates) .......... 28
D.5 Proof of Corollary 1 ........................................................ 31
D.6	Proof of Corollary 2 ....................................................... 31
E	Related works examining Neural	Collapse	33
E.1	Mixon, Parshall, and Pi (2020) ............................................. 34
E.2	Lu and Steinerberger (2020) ................................................ 34
E.3	E and Wojtowytsch (2020) ................................................... 34
E.4	Poggio & Liao (2020a;b) (with Banburski) ................................... 35
E.5	Ergen & Pilanci (2020) ..................................................... 35
E.6	Fang, He, Long, and Su (2021) .............................................. 35
E.7	Zhu, Ding, Zhou, Li, You, Sulam, and Qu (2021) ............................. 36
13
Published as a conference paper at ICLR 2022
A MSE Neural Collapse experiments
The experiments in this section examine the properties of Neural Collapse (NC) on deep nets trained
using MSE loss. The direct MSE-analogues to the cross-entropy (CE) loss table and figures in Papyan,
Han, and Donoho (2020) are in Table 1 and Figures 3-9 here. Furthermore, Figures 10-11 compares
the MSE-NC experiments observed in this paper with the CE-NC behaviors in Papyan, Han, and
Donoho (2020). Experimental descriptions and discussions are within the captions. Subsections
A.1-A.5 describe formatting and experimental details. Then, Subsection A.6 collectively presents
all experimental figures for this section. Subsection A.7 addresses outlier behaviors observed in the
plots. Finally, Subsection A.8 discusses open questions inspired by NC.
A. 1 Datasets
We consider the MNIST, FashionMNIST, CIFAR10, SVHN, and STL10 datasets with pre-processing
the same as in Papyan, Han, and Donoho (2020).
A.1.1 Choice of datasets
Demirkaya et al. (2020) and Hui & Belkin (2020) showed multi-class classification deep nets
trained with MSE loss—sometimes with a heuristical-scaling—demonstrate comparable test-error
performance to those trained with CE loss. Even without additional scaling, i.e. with vanilla MSE
loss, we found this true for ten class classification datasets—such as MNIST, FashionMNIST, SVHN,
CIFAR10, and STL10—so we focus on these five datasets. For more than ten classes (i.e. datasets
such as CIFAR100 and ImageNet), both Demirkaya et al. (2020) and Hui & Belkin (2020) showed that
the additional scaling-heuristic does need to be applied to the loss before comparable test-performance
can be achieved. In informal, exploratory experiments not reported here, we were able to reproduce
their results on datasets with more than ten classes. But, we feel these scaling-heuristics merit further
scientific investigation of their own and are beyond the scope of this article14, so we do not include
datasets that require scaling-heuristic tuning.
A.2 Networks
We train VGG, ResNet, and DenseNet models with the same depth-selection procedures and
architecture-specification choices as in Papyan, Han, and Donoho (2020). In the MSE setting,
the final chosen depths were as follows:
Dataset	VGG	ResNet	DenseNet
MNIST	VGG11	RESNET18	DenseNet40
FASHIONMNIST	VGG11	RESNET18	DenseNet250
SVHN	VGG13	RESNET34	DenseNet40
CIFAR 1 0	VGG11	RESNET50	DenseNet 1 00
STL10	VGG11	RESNET18	DenseNet20 1
A.3 Optimization methodology
The optimization algorithm, parameters, and hyperparameter tuning are the same as in Papyan, Han,
and Donoho (2020).
A.4 Computational resources
The experiments were run on Stanford University’s Sherlock computing cluster. Each dataset-network
combination was trained on a single GPU attached to a CPU with at least 32GB of RAM—the specific
types of the CPUs/GPUs vary according to whichever was first assigned to us by the HPC cluster
scheduler.
14Thorough experimentation with this scaling-heuristic would also consume prohibitively expensive amounts
of computational resources.
14
Published as a conference paper at ICLR 2022
A.5 Formatting
The coloring and formatting of the plots are the same as in Papyan, Han, and Donoho (2020).
A.6 Experimental results
Table 1: Table comparing test-accuracy at moment 0-error is achieved vs. at the end of training.
Analogous to Table 1 of Papyan, Han, and Donoho (2020). The median improvement is 0.962
percentage points; the mean is 1.833 percentage points.
Dataset	Net	Acc. 0-Error	Acc. Final
	VGG	99.23	99.59
MNIST	ResNet	99.16	99.70
	DenseNet	99.62	99.70
	VGG	92~76	92.95
Fashion	ResNet	93.55	93.76
	DenseNet	90.56	92.95
	VGG	89735	93.91
SVHN	ResNet	85.18	92.65
	DenseNet	95.61	95.23
	VGG	83TΓ3	84.54
CIFAR10	ResNet	75.43	76.39
	DenseNet	91.77	91.78
	VGG	60Λ3	67.24
STL10	ResNet	59.35	60.56
	DenseNet	58.74	60.41
bjo>v∕p+s
00>
bjo>v∕p+s
- M>v∖p+S
2①N①SU&
Figure 3: Plots analogous to Figure 2 in Papyan et al. (2020), but on networks trained with MSE Loss.
Results demonstrate that last-layer features and classifiers approach equinormness.
15
Published as a conference paper at ICLR 2022
(SOO)P-S
00>
(SOO)P-S
WNSH
(SOO)P-S
Figure 4: Plots analogous to Figure 3 in Papyan et al. (2020), but on networks trained with MSE Loss.
Results demonstrate that last-layer features and classifiers approach equiangularity.
IOO 200	300
Epoch
Epoch	Epoch	Epoch	Epoch
Figure 5: Plots analogous to Figure 4 in Papyan et al. (2020), but on networks trained with MSE Loss.
Results demonstrate that last-layer features and classifiers approach maximal-equiangularity.
16
Published as a conference paper at ICLR 2022
Figure 6: Plots analogous to Figure 5 in Papyan et al. (2020), but on networks trained with MSE Loss.
Results demonstrate that last-layer features and classifiers approach self-duality.
Figure 7: Plots analogous to Figure 6 in Papyan et al. (2020), but on networks trained with MSE Loss.
Results demonstrate that last-layer features undergo variability collapse.
17
Published as a conference paper at ICLR 2022
IOO 200	300	0	100	200	300	∣0 IOO 200	300	∣0 IOO 200	300 O IOO 200	300
Epoch	Epoch
Epoch
Epoch
Figure 8: Plots analogous to Figure 7 in Papyan et al. (2020), but on networks trained with MSE Loss.
Results demonstrate that classifier decisions converge to those of the nearest class center decision
rule.
Figure 9: Plots analogous to Figure 8 in Papyan et al. (2020), but on networks trained with MSE Loss.
Results demonstrate that networks become more robust when trained beyond 0-error. The median
improvement in the robustness measure in the last epoch over the first epoch achieving zero training
error is 0.1762; the mean improvement is 0.9278.
18
Published as a conference paper at ICLR 2022
■ CrossEntropyLoss ■ MSELoss
IOO 200	300	0
100	200	300	0	100	200	300	0	100	200	300
Epoch	Epoch	Epoch
Figure 10: Activation collapse under MSE loss vs. CE loss: Comparison of activation collapse
observed in this paper for networks trained under MSE loss (Figure 7) with that observed in Papyan
et al. (2020) under CE loss. Networks trained with MSE loss tend to achieve faster activation collapse
than those trained with CE.
1.0e-03
Figure 11: Adversarial robustness under MSE loss vs. CE loss: Comparison of adversarial robustness
observed in this paper for networks trained under MSE loss (Figure 9) with that observed in Papyan
et al. (2020) under CE loss. Robustness tends to be better—sometimes several magnitudes better—
when the networks are trained with MSE loss than with CE loss.
19
Published as a conference paper at ICLR 2022
■ CrossEntropyLoss ■ MSELoss
MNIST
O IOO 200	300
Figure 12: Activation collapse on test data for both losses: Activation collapse observed on test data
for models trained with MSE loss (from this current paper) and CE loss (posted by Papyan, Han, and
Donoho (2020) on Stanford Data Repository). On the test data, activation collapse still visibly occurs
on multiple dataset-network combinations: Albeit the rate of collapse is much slower on the test data
compared to that on the train data, and the plotted measure (described in Figure 6 of Papyan, Han,
and Donoho (2020)) at the last epoch is larger than that on the train data. Also interesting is that
the value at the last epoch is roughly monotonic with the difficulty of the dataset. See discussion in
Section A.8.
20
Published as a conference paper at ICLR 2022
A.7 Exceptions: STL 1 0-ResNet and STL10-DenseNet
The STL10-ResNet and STL10-DenseNet dataset-network pairs stand out among the experiments
described in this section—sometimes displaying outlier behavior either by converging more slowly to
Neural Collapse or by exhibiting trends inconsistent with those found in other dataset-network pairs.
STL10 stands apart from the other canonical datasets for multiple reasons15 and is a “less-typical”
benchmark compared to more “compulsory” datasets like CIFAR10. In this particular case, we
hypothesize that its outlier behavior might be caused by the size of the STL10 images (96 × 96
compared to the 32 × 32, after padding, of the other datasets)—leading to a higher-dimensional
problem, which, in turn, induces a harder non-convex optimization problem, which might make SGD
less likely to converge to a useful optimum or might make the convergence much slower and harder
to observe under a fixed computational budget.
It was previously noted in Hui & Belkin (2020) and Demirkaya et al. (2020) that more challenging
classification problems—in those projects, problems with more classes to be labeled—may require
modifications to the MSE loss. We decided that such modifications are unnecessary in the ten class
problems examined in this paper’s experiments. But perhaps the outlier nature of the STL10-ResNet
and STL10-DenseNet combinations signal that the MSE loss modifications proposed by Hui & Belkin
(2020) and Demirkaya et al. (2020) ought to have been used.
A.8 Open Questions: Weight-decay, batch normalization, SGD, generalization,
TEST DATA
The experiments in this paper as well as those in Papyan, Han, and Donoho (2020) were conducted
under the canonical setting where deep nets were trained using SGD with weight-decay and batch
normalization—leading one to wonder about the roles that these particular ingredients play on the
NC phenomena. Additionally, most observables focus on the train data—raising the question of how
the NC phenomena behave on test data as well as their relationship to generalization. We find these
questions intriguing, but feel each deserves careful experimentation outside the scope of this paper.
Nonetheless, we note here that several existing papers have already begun insightful investiga-
tions in these directions. For example, weight-decay, weight-normalization (as a proxy for batch-
normalization), and SGD play key roles in the analyses of Banburski et al. (2019); Poggio & Liao
(2020a;b) that, along other things, lead to the prediction of NC in homogeneous deep nets. Banburski
et al. (2021) also explores the connection between NC and margin distributions with generalization.
Another example is Zhu et al. (2021) in which the authors conduct several experiments on ResNets
trained on CIFAR10. On their models, the authors examine NC-related properties relative to the
train data, test data, and randomly labeled data; they also conducted ablation studies varying control
parameters (weight-decay, width, etc.) and the optimization algorithm (SGD, ADAM, L-BFGS).
Based on their results, Zhu et al. (2021) propose thought-provoking conjectures on the role of each of
these components (see Section E.7 for a brief survey of their findings).
As a preliminary exploration, we include Figure 12 showing the variability collapse (NC1) behavior
on test data for the networks trained with MSE loss (used in this paper) as well as those trained with
CE loss (released16 by Papyan, Han, and Donoho (2020)). From Figure 12, we see that variability
collapse occurs much slower on the test data than on the train data. Although not shown here, the
other NC phenomena behave similarly.
In this Appendix, we show Figure 12 since it is concise, within reach, and may be of interest to
readers who share our curiosity. We refrain from presenting the entire series of multi-figure NC
measurements on test data because it is outside the scope of this paper and would significantly
lengthen this appendix. We also refrain from speculating on any underlying explanations until more
meticulous experiments are conducted.
15For example, one sense in which STL-10 is clearly different from the other datasets is the comparatively
small number of labeled training examples per class.
16The CE test data results were released by Papyan, Han, and Donoho (2020) on the paper’s corresponding
Stanford Data Repository entry.
21
Published as a conference paper at ICLR 2022
B	Theorem 1
Theorem 1. (Decomposition of MSE Loss) The MSE loss, L(W, H), can be decomposed into two
terms, L(Wf, Hf) = LLS(Hf) +LL⊥S(Wf,Hf),where
1λ
LLS(Hf) = 2 Ave kWLShi,c - yi,ck2 + / IlWLSIlF,
and
1
L⊥S(W, H) = 2 tr{ (W - W^SS) (∑T + μGμ> + λl) (W - WLS)> }.
Proof. Recall the MSE loss:
L(Wf) = 2 Avce k W h i,c- yi,ck2 + 2 k W kF.	(12)
Consider a specific extended activations matrix H ; the least-squares solution W minimizing
L(W, H ) with H kept fixed must obey the following first-order optimality condition:
Ave(WfLShei,c - yi,c)hei>,c + λWfLS = 0.	(13)
From Proposition 1 in the main manuscript, the solution to the above is given by
fs = C-1M>(∑ T + “G”> + λI )-1.
The loss can be rewritten as
1λ
A AVe IIWLShi,c - yi,c + (W - WLS)hi,ck2 + WkWkF，
2 i,c	2
Combining the above with Equation 13 gives, after rearranging:
2 AVe IIWLshi,c- yi,ck2 + 2 AVe k(W - WLS)hi,ck2 - λtrnwLs(W - WLS)>} + 2IlWkF,
which is equivalent to
1	1	λλ
A AVe kWLShi,c - yi,ck2 + - AVe Il(W - WLS)hi,ck2 + XkwLSkF + XkWLS - WkF.
2	i,c	2 i,c	2	2
Using the above, the loss can indeed be decomposed as
L(Wf , Hf) = LLS(Hf) + LL⊥S(Wf , Hf),
where
1λ
LLS(H) ≡ 2 AVe kWLShi,c - yi,ck2 + 2 IIWLSkF	(14)
and
LLS(W, H) ≡ JAVe k(W - ffLS )h i,ck2 + 5 kWLS - WkF
2 i,c	2
=1r{(W -丽S) (∑T + eGμG + λI) (W - WLS)>}.
This completes the proof.	□
C Theorem 2
f	A /八	.j.	C	,∖ E, T .	. C /六、
Theorem 2. (Decomposition of Least-Squares Component) The least-squares component, LLS (H),
of the MSE decomposition in Theorem 1 can be further decomposed into LLS(H) = LNC1 (H) +
C /K 1
LNC2/3(H), where
LNC1 (H) = 2 tr{ WLS h∑W + λIi WL>0，
1
L NC2/3 (H)=亍 kWLS M - I kF.
22
Published as a conference paper at ICLR 2022
Proof. Under Euclidean distance, perturbations to the extended activations matrix H which affect
only the class-means or global-mean are orthogonal to perturbations which affect only the within-class
covariance. Using this, LLS(H) can be further decomposed via the Pythagorean theorem:
1	λ1
LLS(H) =5 Ave k WLS(h i,c - μec)k2 + TykWLS IlF + A Ave ∣∣WLSμec - yi,ck2∙
2 i,c	2	2 c
E， C	I	一	】	♦ . C /六、	….，♦一	1	C	/六、
The first and second terms above merge into LNC1(H), while the third term becomes LNC2/3(H):
Lnci (H) = ； tr{ WLs (∑W + λI) fW> }	(15)
1
Lnc2∕3(H) = 2C IWLsMf - I kF.
This completes the proof.	□
c.1 Intuitions for Theorem 2 in unextended coordinates
For intuition, consider when λ = 0, i.e. the no weight-decay case. Proposition 1 in the main text
gives
WLS = C -1M>Σ-1,	(16)
whereM ∈ RP×c is the matrix with columns μc 一 μg. Returning to the unextended coordinates,
LNCI(H) simplifies to
LNCI(H) = 2 tr{WLS∑wWL> },	(17)
where H ∈ RP×cn has columns hi,c 一 μG. The term Lnc2∕3(H) also simplifies instructively to
Lnc2∕3(H) = ɪ∣WLsM - ΦkF,	(18)
2C
where Φ ∈ RC ×C is the standard Simplex ETF:
Φ = I - 111>.
C
c.2 Additional intuitions for Theorem 2 terms
The expressions in Equation 15 and Equation 17 further evoke the intuition that
Lnci (H) is a variance term that goes to zero only under activation collapse (NCI)
More specifically, we see that, with the class-means held constant, the only way to have LNCI(H) →
0 is for ΣW → 0. Similarly, by examining Equation 18, we see that
Lnc2∕3(H) quantifies deviations of WLSM—i.e. the matrix of class-mean
predictions—from the standard Simplex ETF.
Under (NC2) and (NC3), both WLS and M tend to jointly aligned ETF,s, possibly in an alternate
pose; so WLS → ΦUT and M → UΦ for a partial orthogonal matrix U satisfying UTU = I0.
Since Φ2 = Φ, (NC2) and (NC3) together demand WLSM → Φ. Thus, Lnc2∕3(H) can be
interpreted as a semi-metric reflecting the distance from achieving (NC2) and (NC3) .
D	Theorem 3
D. 1 Implications of invariance
In this section, we will discuss in more detail how the invariance observed Section 3.1 leads the
the continually renormalized gradient flow (Equation 5). In particular, we have seen in Section
3.1 of the main text that, on the central path, both the predictions WLS(H)H and the MSE loss
23
Published as a conference paper at ICLR 2022
Figure 13: Fiber bundle. Any full-rank features matrix, H, has a representative element, X, on X.
For any X ∈ X, a fiber is the set S × X i.e. {AX : A ∈ S} (the Minkowski set product), where
S is the set of symmetric positive-definite matrices. Features on the same fiber generate the same
class predictions and the same MSE loss. The optimization described in Sections D.1-D.3 moves
fiber-to-fiber.
L(WLS (H), M (H)) are invariant under the transformation H 7→ AH when A is a invertible
-1
matrix. Since our interest mainly lies with A = ΣW2, we will restrict the intuitive discussion in this
section to positive-definite A.
Now, consider the set H of features H with with non-singular within-class covariances. We view H
as a fiber bundle17, where—on each fiber-live (apparently) different activations, H, that (in fact)
generate the exact same class predictions and the exact same MSE loss (see Figure 13). The base
space—we will denote it X —can be taken to be the collection of X ∈ H where ΣW (X) = I. Every
0- 1
H ∈ H is representable as H = AX where A = ΣW2 (H) and ΣW (X) = I. The activation
matrices X in the base space might be variously called “pre-whitened,” “normalized,” “standardized,”
or “sphered.” We shall call X a normalized features manifold18.
Invoking invariance, we can make the following observations about our optimization task:
1.	If, at a specific t, Ht happens to be a normalized set of activations (i.e. Ht ∈ X is currently
located in the base space X), there is no performance benefit to leaving the base space. There
is also no performance benefit to moving along a fiber; only by moving from fiber-to-fiber
can we improve performance.
2.	In some sense, we waste time except when jumping from fiber-to-fiber; we might prefer to
stay in the base space all the time by forcing the dynamics to jump from fiber-to-fiber.
On the other hand, we started with the viewpoint of studying gradient flow of the original H. So, we
consider the following natural model for the application of gradient descent in X :
1.	For a given initial feature activations, H0 ∈ H, we renormalize—obtaining a starting point
—1
X0 ∈ X; here X0 = ΣW2 (H0)H0. Class predictions and MSE loss performance for the
MSE-optimal classifier do not change from this renormalization.
2.	We compute the usual gradient of MSE loss, at X0 , obtaining a step ∆X0 =
—ηVXLLS(X0), where η is a step size.
17An even larger space could be considered, where ∑w (H) is not necessarily full rank, and then other
components of this space having covariances of rank C, C - 1,...,1 could be considered. We just discuss the
full-rank case here.
18This is sometimes called the Generalized Stiefel Manifold and will be defined more formally in Section D.2.
24
Published as a conference paper at ICLR 2022
3.	We take a step, going to
Hi = Xo + ∆Xo.
4.	Hi will not necessarily be normalized. We map back along whatever fiber We have landed
upon, obtaining a corresponding point in the base space X, call this X1. Here,
_ι ,一 . 一
Xi = ∑W2 (H1)H1.
Class predictions and MSE loss performance do not change from this renormalization.
5.	Repeat steps 2, 3, 4 at Xi, obtaining thereby X2; and so on.
This process might be described as gradient descent with continual renormalization:
1.	Renormalize the initial features activation matrix;
2.	Compute the ordinary gradient of MSE loss and take the gradient descent step;
3.	Renormalize again after each such step; and
4.	Repeat steps 2 and 3.
While the continual renormalization process differs from usual gradient flow, it is both intuitively
understandable and sensible. See discussion of assumption (A3) in Section 3
D.2 Aligned SNR coordinates
Analysis of continually renormalized gradient flow can be simplified, without loss of generality, by
applying a change of basis in both row and column space:
Definition 3 (Transformation into Aligned SNR Coordinates). Consider the SVD decomposition of
the SNR from Definition 2 outputting left singular vectors U ∈ RP ×C and right singular vectors V ∈
RC×C. For any matrix Z ∈ RP×C, define the transformation Z → U>ZV as the transformation
into aligned-SNR coordinates.
The aligned-SNR coordinates is so-named because it diagonalizes i.e. aligns the SNR matrix:
Definition 4 (Aligned SNR Matrix). Consider the SVD decomposition of the SNR from Definition 2.
Combined with Equation 8, observe that
Ω = diag ({ωc}C-ι1,0) = Uτ∑w1 MV
We call Ω the aligned SNR matrix.
The following facts about the aligned SNR matrix becomes useful later in our derivations:
Observation 1 (SVD of Aligned SNR Matrix). The SVD of the aligned SNR matrix itself is simply
C-i
ω = £叼 ej e>,
j=i
with the canonical basis {ej }jC=i as its singular vectors.
As discussed in Section 3.2, the {ωc}C=ι1 that comprise Ω are decisive for understanding separation
performance. In fact, when λ=0, the MSE loss can be entirely characterized by the SNR singular
values on the central path:
Lemma 1 (Spectral Representation of MSE Loss). When λ = 0, MSE loss obeys
1	C-i	1
L(Wls(H), H) = LLS(Wls(H), H) = 2 ∑ ^TC = L (gj}占1),
2	j=i ωj + C
on the central path, and its decomposition obeys
L NC1 (H)
C-i	2
T X _ω_
2 j=1 (C + ω2)2
C-i	ω2	2
LNC2/3 (H)=2 XC (ωω⅛- ιj =L NJ/({叼}C-II).
25
Published as a conference paper at ICLR 2022
Proof. Using Equation 16, Equation 17, and Equation 18, the loss on the central path equals
L(WLs, M) =1 kC-1M>∑T1M - ΦkF + 1 tr{C-1M>∑T1 ∑W∑T1MC-1}.	(19)
2C	2
Observe M>∑T1M and Φ are simultaneously diagonalizable since they share the same [1,..., 1]>
null space and Φ has C - 1 equal eigenvalues. Also, observe that
1T
∑t = ∑w + 不MM .
C
Let Ω be submatrιx of the alιgned-SNR matrix Ω corresponding to the non-zero singular values ι.e.
Ω = diag (也}C=L1). After taking simultaneous-diagonalizations and canceling partial orthogonal
matrices, Equation 19 can be written solely in terms of Ω:
2
F
L(Ω)
1
2C
Ω> (ΩΩ> + CIc-1 )-1 Ω - Ic-1
+1 tr{Ω >(Ω Ω > + C IC-I)-2Ω}.
This can be further simplified into
c-1	2	2	2
L® }C=11})=2X C (ωr⅛ -ι) +—
1 c-1	1
=2X ω2+C.
(20)
This completes the proof for Lemma 1.
□
Thus, the aligned SNR matrix, U>∑w2 MV, maximally simplifies analysis by reducing the vanilla
SNR matrix to a diagonal matrix (Definition 4) whose entries determine the MSE loss (Lemma 1).
We introduce the alignment of features into their own notion of aligned SNR coordinates:
Definition 5 (Renormalization to aligned SNR Coordinates). Consider H ∈ RP×cn with
corresponding SNR left and right singular vectors U and V (Definition 2). Then, define the SNR-
aligned renormalization of the features as
X = U> (∑W)-2 H(V ㊈ IN) = U> (HCH>)-1 H(V ㊈ IN) ∈ Rc×nc,
where 0 is the Kronecker product and C is a class-centering matrix defined as
(21)
A 1∙	1 ♦ C 」 CT ,1 ,	C	」 K	L- 1K 1	, 1	.1	1∙ ,∙	…
As discussed in Section 3.1, the transformation H → ∑W H does not change the predictions of the
least squares classifier. Hence, it does not change the MSE loss. It is easy to check that this still holds
for X i.e. invariance is preserved by the change into SNR-aligned coordinates.
It is also easy to check that the sphericity of the within-class covariance is preserved as well:
Observation 2 (Sphericity of Within-Class Covariance of SNR Coordinates).
XCX> = Ic
Finally, routine applications of Definitions and canceling out of (partial) orthogonal matrix multi-
plications in aligned SNR coordinates lead to the following simplified representation for Ω and its
differential:
Observation 3 (Relation Between SNR Coordinates and SNR Matrix). The aligned SNR matrix,
Ω (Definition 4) can be expressed as a function of the features in aligned SNR-Coordinates X
(Definition 5) as simply
Ω(X ) = N XY >.
26
Published as a conference paper at ICLR 2022
Moreover, let dΩj (∙) : Tχ X → R be the differential 1-form19 associated with the ij -th entry of
Ω. Define dΩ (∙) : Tχ X → RC × C such that, for some Z ∈ Tχ X, each entry matrix of the matrix
dΩ (Z) is dΩi,j (Z). Then, itfollows that
dΩ (Z) = NZY>,	∀Z ∈ TχX.
From a geometric perspective, the transformation in Definition 5 maps the features, H, to X
belonging to a Normalized Features Manifold:
Definition 6 (Normalized Features Manifold).
X = {X ∈ RC×CN | XCX> = IC}
To understand the dynamics of the singular values of Ω—which is connected to the dynamics of the
features through Lemma 1—we will analyze the gradient flow on this manifold. To this end, we first
identify its tangent space at a particular X as well as the projection operator onto that tangent space.
Proposition 3 (Tangent Space of Normalized Features Manifold).
TχX = {Z ∈ RC×CN | XCZ> + ZCX = 0}
Proposition 4 (Projection Onto Tangent Space of Normalized Features Manifold).
∏TxX(Z) = Z - 2(XCZ> + ZCX>)X
D.3 Continually renormalized gradient flow
In fact, gradient flow on the normalized features manifold (Definition 6) is the continuous analogue
of the intuitive, discrete algorithm in Subsection D.1. In particular, the intuitive algorithm consists
of a gradient step, Hi = X。+ ∆X° followed by a mapping to the base space of the fiber bundle,
Xi = Σ(Hι)-2 Hi. For small ∆X0, these two steps are in fact equivalent (up to a negligible term)
to taking a Tχ X -projected gradient step on the manifold X as proven below:
Lemma 2. Assuming X0 ∈ X, a renormalized gradient step,
_1	—	_	_ 1
Xi = ∑W2(Hi)Hi = ∑W2(Xo + ∆Xo) ∙ (Xo + ∆Xo),
is equivalent, up to an O k∆X0 k2 term, to a Tχ X -projected gradient step, i.e.
Xi =X0 + ΠTX X(∆X0) + O k∆X0k2 .
Proof. Notice that
-I————
Xi =∑W2 (Hi)Hi
=((Xo + ∆Xo)C(Xo + ∆Xo)>)-2 (Xo + ∆Xo)
=(I + ∆XoCX> + Xo C ∆X> + ∆XoC ∆X0)-1 (Xo + ∆Xo),
where in the last step we used our assumption that X0 ∈ X, i.e., X0CX0 = I. By Taylor’s
Theorem,
(I + A)-1 = I - 1A + O(kAk2).
Therefore, we get
Xi =(I - 2 (∆XoCX> + XoC∆X> + ∆XoC∆XT) + O (k∆Xok2)) (Xo + ∆Xo)
=Xo + ∆Xo - 2 (∆XoCX> - XoC∆X>) Xo + O (∣∣∆Xo『)
=Xo + ΠTXX(∆Xo) +O k∆Xok2 ,
where the last step follows from Proposition 4.	□
27
Published as a conference paper at ICLR 2022
As we transition from a discrete gradient descent to a continuous gradient flow, the step size
η → 0, and the residual O k∆X0k2 in the above lemma becomes negligible, since ∆X0 =
-ηVχLLS(X0). This motivates Us to study, in the next section, the continually renormalized
gradient flow (as defined in Equation 5 of the main text) of X on X :
dtX = -πTχX (VXLLS(X)).
D.4 Proof of Proposition 2 (gradient flow in aligned SNR coordinates)
To prove Proposition 2, we first set up some auxiliary notation and lemmas. Denote the j-th row
(transposed) of X and Y , respectively, by
xj = X>ej	(22)
and
yj = Y >ej.
Recall that—when differentiating SVDs—the derivative of the j-th singular value, ωj , only depends
on the j-th singular subspace (see Equation 17 of Townsend (2016)). As a consequence, we will see
in the following lemma that, for any differential dX, the corresponding dωj only depends on dxj .
Lemma 3 (Derivative of Singular Values With Respect to SNR Coordinates). Given the differen-
tial 1-form dωj (∙) : TXX → R,thefoUowing holds forall19 dX ∈ RC×CN:
dωj (∏Tχ x(dX)) = N1y y> — 3j x> C) dxj ,	∀j = 1,..., C.
where (consistent with Definition 2) we adopt the convention that ωC=0 is the C-th singular value of
the SNR matrix.
Proof. Without loss of generality, assume that X is represented in aligned SNR coordinates (Section
D.2). Using Observation 1 and the differential of the singular value decomposition (see Equation 17
of Townsend (2016)):
dωj (∏τχχ(dX)) = e> dΩ (∏τχχ(dX)) ej,	(23)
where dΩ (∙) is defined as in Observation 3. Next, Observation 3 implies
dΩ (∏τχχ(dX)) = NnTXχ(dX) Y>.	(24)
Using the projection onto the tangent space, given in Proposition 4 above,
ΠTX X(dX) = dX — 1(dXCX > + XC dX τ)X.	(25)
Combining Equation 23, Equation 24, and Equation 25, we obtain:
ʤj (∏τχχ(dX))=e> dΩ (∏τχχ(dX)) ej	(26)
=N e> πTx X(dX) YTej
="e> (dX — 1 (dXCXT + XC dXT) X)Yτej.
19Notation: For real-valued functions f, we use df (∙) : TXX → R to denote the associated differential
1-form i.e. the function outputting the directional derivative of f in the direction of the argument; For matrices
Z ∈ Rm×n, we use dZ ∈ Rm×n—without succeeding parenthesis—to denote the matrix differential (cf.
Townsend (2016)). These notions are equivalent: df (∙) can be represented as a vector df in the dual-space of
TX X ; When Z is a function of some vector v , dZ can be represented as a collection of 1-forms by defining
dZij (∙) = ddZVj-, •)for each entry Zj.
28
Published as a conference paper at ICLR 2022
Moreover, using Observation 1 and 3, we can simplify these expressions into
N e> dXY 7彳=N yj dxj
ɪe> dX CX>XY>ej =ωje> dXCX>ej = ωjx>C dxj
ɪe>XC dX> XY>ej =ωje>XC dX> ej = ωjxjC dxj .
Finally, substituting the above three expressions into Equation 26, we get:
dω (πτχχ(dX))= NNyyj - ωjx>C) dxj ,
which proves the claim.	□
Using Lemma 3, we can now obtain the dynamics of xj under gradient flow:
Lemma 4. Under continually renormalized gradient flow (Equation 5),
dtXj = (C +X2)2 (Nyj-ωjCxj), ∀j = 1,...,C,
where (consistent with Definition 2) we adopt the convention that ωC=0 is the C-th singular value of
the SNR matrix.
Proof. Recall xj ∈ RCN is the j-th row of X ∈ RC×CN , which we assume to be in aligned SNR
coordinates without loss of generality (Section D.2). Applying the flow definition in Equation 5 to
the i-th element of xj gives
d _ >π	/dLLS
dtxji = -ej nTXX (^dX
F
dLLS
dX,
where dLXLS ∈ Rc×cn is the standard derivative within the ambient Rc×cn-space, ei ∈ RCN and
ej ∈ RC are the canonical basis vectors, and h∙, )f is the Frobenius matrix inner-product. Next,
observe that the differential 1-form dLLS (∙) ： TχX → R satisfies the following:
dLLS (Z)=(含，Z,,	∀Z ∈ TχX.
Taking Z = ΠTX X ej eij then leads to
dtxji = - dLLS (nTXx(ejej))
C-1
-X
k=1
dLLS
—
dLLS dωk CnTXX(eje>))
dωj
dωj (nTχ x(eje>))
where the second step follows by the chain rule, and the last step follows from Lemma 3 in that
dωk ΠTXX ej eij = 0 if k 6= j. Moreover, Lemma 3 also gives
dωj (nTXx(eje>)) = yNyyj -ωjXjC) ei.
29
Published as a conference paper at ICLR 2022
It then follows that
d
dtXj
—
N yj - ωj Cxj
(C +jω2)2 (耳yj - ωjCxj),
where the last step follows from differentiating the expression in Lemma 1.
□
The gradient flows of xj induce the dynamics of ωj , which are described below.
Lemma 5 (Induced Dynamics on SVD of SNR). Under continually renormalized gradient flow
(Equation 5), the dynamics for the j -th non-zero SNR singular value (Definition 2) is
d 1	ωj
dtωj = N (C + ω2)2 ,
∀j = 1, . . . , C - 1.
(27)
Proof. Without loss of generality, assume X is represented in aligned SNR coordinates (Section D.2).
Observation 1, the differential of the singular value decomposition (see Equation 17 of Townsend
(2016)), and Observation 3 collectively imply that for any20 dX ∈ TXX,
dωj (dX) = e> dΩ (dX) ej = ɪ e> dX Yτej∙ = ɪ y> dxj .	(28)
Recall that the differential of ωj only depends on the differential of xj (see Lemma 3). Then, we can
apply the chain rule:
d	dωj dxj	1 ωj τ 1
dtωj' = dX-"dT = N (C + ω2)2 yj (Nyj- ωjCxj
(29)
where the last equality results substituting-in Lemma 4 and Equation 28. Since YYτ = NI,
Applying the same relation and Equation 21:
x>Cyj= e>XCYTej = CNe>X (I- NYTY) YTej= 0∙
Combining all the above equations, we obtain
d 1	ωj
dtωj = N (C + ω2)2 .
□
The closed-form given in Proposition 2 now directly follows.
Proposition 2 (Dynamics of Singular Values of SNR Matrix). Continually renormalized gradient
flow on the central path (Equation 5) induces the following closed-form dynamics on the SNR singular
values (Definition 2):
c1 log(ωj (t)) + c2ωj2(t) + c3ωj4(t) = aj + t, t ≥ 0, for all j = 1, . . . , C - 1.	(9)
c1, c2, and c3 are positive constants independent ofj, and aj is a constant depending on ωj (0).
Proof. Follows from symbolically solving the ODE in Lemma 5 with routine methods. The constants
are ci = C2N, c2 = CN, and c3 =苧.	□
20For our goal of applying the chain rule in Equation 29, we can restrict our attention to just dX ∈ TXX :
The definition of renormalized gradient flow (Equation 5) ensures that 噎 will always be in TX X.
30
Published as a conference paper at ICLR 2022
D.5 Proof of Corollary 1
Corollary 1 (Properties of SNR Singular Values). SNR singular values (Definition 2) following the
Equation 9 dynamics satisfy the following limiting behaviors:
1.	limt→∞ ωj (t) = ∞ and	limt→∞ ：jt- = 1, for all j = 1,...,C 一 1.
t/c3
2 Iirn	maχj ω(t) _ 1
2.	limt→∞ minj j=) = 1.
Proof. As t tends to infinity, the right-hand side of Equation 9 diverges to infinity and therefore
so does the left-hand side (LHS). As the LHS approaches infinity, the logarithmic terms become
negligible compared to the dominant quartic term, implying ωj4 (t) → ∞. Since ωj (t) are sin-
gular values, they must be non-negative—implying ωj (t) → ∞. Based on the same argument,
observe that limt→∞ ωjt) = 1 for all j. Since the constant c3 is independent of j, it follows that
c3 c3
max	maXj ω (t) _ 1
limt→∞ minj ωj(t) = 1.
□
D.6 Proof of Corollary 2
Lemma 6. Under continually renormalized gradient flow (Equation 5), the left and right singular
vectors of the SNR matrix remain constant i.e. they are independent of t.
Proof. Without loss of generality, assume X is in represented in aligned SNR coordinates (Section
D.2). Recall that, in this coordinate system, the corresponding SNR matrix Ω = N-iXY> is
diagonal, and the left and right singular vectors are simply partial identity matrices.
To show that the singular values of the SNR remain constant, it then suffices to show that 等=dχ Y
is a diagonal matrix as well21 . Lemma 4 gives
-d X = ɪ Di Y — D2XC
dt N
where Di and D2 are C×C diagonal matrices with the diagonal entries {(。二：2)2 }	and

(
(C+ω2)2
, respectively. Then,
c=1
-dΩ =(-dX ) Y> = -1 DiYY> - D2XCY> = Di,
dt	∖dt ； N 1	1,
where, in the last equality, we used the easy-to-check identities that YY>=NIC and CY=0. In
more detail, the first identity follows from the fact that Y is the one-hot label vectors stacked as
columns; the second identity follows from the facts that C is the class-centering matrix, and the
one-hot label is the same for examples from the same class. Thus, We have shown 誉 is diagonal,
which concludes our proof.
Lemma 7. A matrix E ∈ RP×C is a Simplex ETF if and only if
□
1.	E has exactly C-1 non-zero singular values, which are all equal.
2.	E has a rank-1 nullspace spanned by the ones vector, i.e., E1C=0. In other words, E has
zero-mean columns.
Proof. First, recall from Papyan, Han, and Donoho (2020, Definition 1) that a P ×C matrix E is
called a Simplex ETF if it satisfies
C1
EE = α M I- C-11C 1>
21Intuitively, changing the diagonal SNR matrix by a diagonal matrix increment implies the updated SNR
matrix is still diagonal. This, in turn, implies the left and right singular vectors of the updated SNR matrix are
still the left and right partial identity matrices, respectively.
31
Published as a conference paper at ICLR 2022
for some scaling α>0. We now prove the equivalence.
Simplex ETF implies 1-2: Consider the SVD decomposition E=UESEVE^, where UE is a
P ×(C-1) partial orthogonal matrix satisfying UE>UE=I, SE is the (C-1)×(C-1) diagonal
matrix of singular values, and VE> is a (C-1)×C partial orthogonal matrix satisfying VE>VE=I.
Since E is a Simplex ETF, by definition,
E>E = α ( C— I--------— IC 1> ),
C-- 1 C - 1 CC),
and according to the SVD decomposition
E>E = VESEUE> UESEVE> = VESE2 VE> .
Therefore,
VESEVE = α (CCli - C-11c 1>).
Notice that the right-hand-side hasC -1 equal singular values. This implies SE —and, thus, E as
well—has C -1 equal singular values. In other words, SE is a (C -1)×(C -1) diagonal matrix equal
to diag(s, . . . , s) for some scalar s>0.
Next, notice that on the one hand
e>e 1c = α (尸。1 i -尸 1 1 1C 1>) 1C = 0.
C -1 C -1
On the other hand,
E>E1C = VESEUE> UESEVE>1C = VESE2 VE>1C = s2VEVE>1C .
Combining the above, we get VE VE> 1C =0. Since VEVE> is aC ×C matrix of rank C -1, we
deduce that the rank-1 nullspace of VE> is spanned by theC -dimensional ones vector, i.e. VE>1C=0.
Consequently, E has zero-mean columns since E1C=UESEVE>1C=0.
1-2 implies Simplex ETF: Assume E has exactly C—1 non-zero, equal singular values as well as a
rank-1 nullspace spanned by the ones-vector, i.e., E1C=0. Then, E=UESEVE> where
• UE is a P ×(C-1) partial orthogonal matrix satisfying UE> UE=i;
• SE = diag(s, ..., s) is a (C-1)×(C-1) diagonal matrix for some scalar s>0; and
• VE> is a (C-1)×C partial orthogonal matrix satisfying VE>VE=i and also satisfying
VE>1C=0.
Therefore,
E>E= VESEUE>UESEVE> = VESE2 VE> = s2VEVE>.
Using our assumptions on VE,
VE V> + 万 1C 1> = 1,
C
where we divide each of the ones-vectors by √C to create a unit vector. Thus, we conclude
ETE = s2 (1 -尸1C 1>) = α (不~~71 -不---------71C1C
C	C-1 C-1
where a = s2 C-1. Hence, by definition, E is a Simplex ETF.	□
Corollary 2 (Neural Collapse Under MSE Loss). Under continually renormalized gradient flow
(Equation 5), the SNR matrix (Equation 8) converges to
lim —=V SNRt = U0V0>,	(10)
t→∞ ωmax(t)
where Ub0 ∈ RP×(C-1) and Vb0 ∈ RC×(C-1) are the left and right singular vectors of the SNR
matrix (Definition 2) at t=0 corresponding to the non-zero singular values; and ωmax (t) is the
32
Published as a conference paper at ICLR 2022
largest singular value at time t. Furthermore, Corollary 1 implies the occurrence of (NC1)-(NC4) i.e.
renormalized gradient flow on the central path leads to Neural Collapse.
Moreover, denoting the Kronecker product with 0, the renormalizedfeatures matrix converges to
t→∞ ωm⅛ 纵2国= (UOVT) 01N.
(11)
Proof. Derivation of Equation 10: Corollary 1 proves the singular values j = 1,...,C 一 1 of SNRt
diverge to infinity and that their ratio tends to one. By Lemma 6, the renormalized gradient flow will
not change the singular vectors of the SNR matrix, and—combined with the fact that the singular
values converge to equality (second fact of Corollary 1)—we get the limit in Equation 10.
DeriVatiOn of (NC1): Let Sj (∙) denote the j-th singular value of its argument. Then, observe that
tr (∑B,t∑w,t) = Ctr ((MtM>)t ∑w,)	(Def. of ∑B,t)
=C tr (∑Wt (MtM>)t ∑Wt)	(Cyclic property of trace)
C-1
C X Sj (∑W,t(MtM>)t ∑W,t)
j=1
C-1
C X S- (∑w2t(MtM>)∑w2t)
j=1
C-1
C X s-2 (∑w2tMMt)
j=1
(Trace is sum of singular values)
(Def. of pseudoinverse)
C-1	1
jg j
t→∞
----→
(Corollary 1)
By Horn & Johnson (2012, Theorem 1.3.22),
λj(ΣtB,tΣW,t)=λj(Σ0W.5,tΣtB,tΣ0W.5,t)≥0,
where λj-(∙) denotes the j-th eigenvalue of its argument, and the last inequality follows from the
positive-semidefiniteness of Σ0W.5,tΣtB,tΣ0W.5,t. Since the trace is the sum of eigenvalues, the only way
for the trace of ΣtB,tΣW,t to tend to zero is if all eigenvalues also tend to zero. All eigenvalues
tending to zero implies the matrix itself tends to zero, i.e.
lim ΣtB tΣW,t = 0,
t→∞	,
which is the definition of (NC1) in Section 1.1.
DeriVatiOn of (NC2)-(NC4): Recall that the SNR matrix has zero-mean columns and rank C-1 (see
Definition 2). This, combined with the fact that the singular values converge to equality (second fact
of Corollary 1) imply, by Lemma 7, that the renormalized class-means converge to a Simplex ETF i.e.
(NC2) .
From Theorem 1 of Papyan, Han, and Donoho (2020), we then know that (NC3) and (NC4) follow
from (NC1) and (NC2) on the central path.
Derivation of Equation 11: Combining the limit in Equation 10 with (NC1) proves the limit in
Equation 11.	口
E	Related works examining Neural Collapse
In this section, we discuss the contributions and limitations of seven recent works that propose and
analyze theoretical abstractions of Neural Collapse. These works are only available in preprint, and
may not yet be peer-reviewed. Thus, they might ultimately appear with very different claims or
results. Additionally, works such as Poggio & Liao (2020a;b); Ergen & Pilanci (2020) also analyze
behaviors other than NC; we will only discuss the parts relevant to Neural Collapse here.
33
Published as a conference paper at ICLR 2022
E.1 Mixon, Parshall, and Pi (2020)
Mixon et al. (2020) considered the unconstrained features model in Equation 2 (without weight decay)
where, under gradient flow, (W, H) evolve according to a nonlinear ordinary differential equation
(ODE). They followed a two-step strategy for studying Neural Collapse. First, they linearized the
ODE—claiming nonlinear terms are negligible for models initialized near the origin—and proved
the simplified ODE converges to a subspace of (W, H) satisfying (NC1) and (NC3) . Second, they
proved that gradient flow, restricted to that subspace, converges to (NC2) .
The assumption of small weights and classifiers leading to the linearized ODE is not aligned with
today’s paradigm. Specifically, the most commonly used He initialization (He et al., 2015) is designed:
(i) to create weights with non-negligible magnitude; and (ii) to preserve the magnitude of features, as
they propagate throughout the layers of the network, exactly so that last-layer features would have
non-negligible magnitude. Moreover, the analysis of Mixon et al. essentially assumes that (NC1) and
(NC3) occur much sooner than (NC2) . However, from the experiments in both Papyan, Han, and
Donoho (2020) and this paper, there is no empirical evidence that (NC2) happens slower than (NC1)
and (NC3) in practice.
E.2 Lu and Steinerberger (2020)
While the MSE loss provides a mathematically natural setting for analysis, the modern paradigm in
multi-class classification with deep learning involves training with CE loss, which is more challenging
to analyze than MSE.
Lu & Steinerberger (2020) studied the (one-example-per-class) unconstrained22 features model with
CE loss:
min CE(W, M) s.t. IIwck2 = ∣∣μc∣∣2 = 1.
W,M
Since under linear separability the CE loss can be driven arbitrarily close to zero, just by re-scaling the
norms of W and M, the authors further imposed a norm constraint on wc and μc. Lu & Steinerberger
observe that the global minimizer of this optimization problem is only achieved once W and M are
the same Simplex ETF. This derivation is suggestive, but it does not identify closed-form dynamics
which would get gradient flow to such a global minimizer, nor does it address the rate of convergence
to Neural Collapse. Additionally, the constraint on μc possesses no immediate or direct analogy
to standard deep net training—where procedures often control the norm of the weights W, but not
features H—nor class-means M .
E.3 E and Wojtowytsch (2020)
E & Wojtowytsch (2020) also consider the unconstrained features model22 with CE loss,
min	CrossEntropy(WH) s.t. kWk2 ≤ 1, khick2 ≤ 1,
W,H
where they adopt a more technical, spectral norm constraint on W to specify their model.
Building on the results of Chizat & Bach (2018; 2020), E & Wojtowytsch also construct a simple
counter-example showing that Neural Collapse need not occur in two-layer, infinite-width networks—
which have been the focus of intense recent study in the theoretical deep learning community (Mei
et al., 2018; Rotskoff & Vanden-Eijnden, 2018; Arora et al., 2019). Thus, E & Wojtowytsch’s
counterexample suggests the alternative perspective that, despite the expressiveness of infinite-width,
two-layer networks, such abstractions do not capture key aspects of trained deep nets.
As with Lu & Steinerberger (2020), standard deep net training does not possess any direct analogies
for constraining the norm of features (as opposed to weights)—nor are there any paradigmatic
regularizations that correspond to controlling the spectral norm on W. Moreover, the work does not
characterize any closed-form dynamics or the rate of convergence to Neural Collapse.
22The model is unconstrained in the sense that the features are allowed to move directly with gradient flow
and are not constrained to be the output of a forward pass—not in the sense that there are no constraints on the
optimization problem.
34
Published as a conference paper at ICLR 2022
E.4 Poggio & Liao (2020a;b) (with Banburski)
Distinguished from the simplified unconstrained features models in the previously mentioned works
is the theoretical analysis of Poggio & Liao (2020a;b) (in a special section, co-authored with Andrzej
Banburski).
The authors study deep homogeneous classification networks, with weight normalization layers,
trained with stochastic gradient descent and weight decay. This is much closer to today’s training
paradigm, but the setting still differs from the one in which Neural Collapse has been empirically
observed in Papyan, Han, and Donoho (2020) and in Section A of this paper. In particular, they
replace batch normalization with weight normalization and consider deep homogeneous networks;
homogeneous networks can not have bias vectors nor skip connections, which are present both in
ResNet and DenseNet. Moreover, the work gives explicit descriptions of neither the dynamics nor
the rate of convergence to Neural Collapse.
E.5 Ergen & Pilanci (2020)
While the above-described works tend to focus on either the used-in-practice CE loss or the
theoretically-insightful MSE loss, Ergen & Pilanci (2020) observed that these are both instances
of the general class of convex loss functions and, thus, one could derive insights from the classical
convex analysis literature. Moreover, compared to Mixon et al. (2020); Lu & Steinerberger (2020);
E & Wojtowytsch (2020), this work studies the optimization starting from the second-to-last layer
features rather than the last-layer features. In particular, the authors use a strong-duality argument
to show that NC emerges in the optimal solution of an equivalent proxy-model to the following
optimization:
min	L (WL (BNγ,α (Wl-iHl-i))+ , Y) +	(M2 + 间；+ IlWLkF),
HL-1,WL-1,WL,γ,α	2
where L(∙) is a general convex loss, Hl-i are the second-to-last layer activations, WL-I are the
second-to-last layer weights, WL are the network classifiers, Y are the training targets, λ is a
weight-decay parameter, BNγ,α(∙) is a batch-norm operator parameterized by a and Y, and (•)+ is a
ReLU. The incorporation of batch-normalization and weight-decay ensures the existence of bounded,
well-defined optimal solutions—serving a similar role to that of weight-normalization and weight
decay in Poggio & Liao (2020a;b) as well as the norm constraints in the other aforementioned related
works.
Since strong-duality only characterizes properties of the converged optimal solution ofan optimization
model, Ergen & Pilanci (2020) does not provide insights into the dynamics with which that solution
is achieved which training.
E.6 Fang, He, Long, and Su (2021)
In Fang et al. (2021), the authors introduce the (N -)layer-peeled model in which one considers only
the direct optimization of the N -th-to-last layer features of a deep net along with the weights that
come after the N -th-to-last layer. The motivating philosophy is that, after raw inputs are passed
through some initial number of layers, the overparameterization of those layers would allow us
to effectively model the N -th-to-last layer features as freely-moving in some subset of Euclidean
space. In this terminology, the concurrent works of Mixon et al. (2020); Lu & Steinerberger (2020);
E & Wojtowytsch (2020) on the unconstrained (last-layer) features model could be considered
instances of a 1-layer-peeled model; while the model of Ergen & Pilanci (2020) could be considered
as a 2-layer-peeled model. This perspective is attractive because it gives a name and organization
to a common modeling philosophy behind the above-described body of independent works. For
comparison, the work of Poggio & Liao (2020a;b) is a non-example of layer-peeled modeling as it
considers optimization on the weights of homogeneous deep nets and not the input features.
Fang et al. (2021) then analyzes a convex relaxation of the 1-layer-peeled model—with norm
constraints on the weights and features—into a semidefinite program. Not only do the authors show
that this model exhibits Neural Collapse in the canonical setting of balanced examples-per-class, but
they also analyze the behavior of this model under imbalanced classes. While, in the imbalanced
case, one would intuitively expect the Simplex ETF to “skew” to have bigger angles around over-
represented classes and smaller angles around under-represented ones; Fang et al. (2021) identifies the
35
Published as a conference paper at ICLR 2022
surprising phenomenon—named minority collapse—in their model where, when the imbalances pass
a certain threshold, the last-layer features and classifiers of the under-represented classes collapse to
be exactly the same. However, their work does not provide closed-form dynamics or rates at which
collapse—in either the balanced or imbalance case—occurs.
E.7 Zhu, Ding, Zhou, Li, You, Sulam, and Qu (2021)
In Zhu et al. (2021), the authors examine the following unconstrained features model:
min CrossEntroPy(WH + b, Y) + 学 k W kF + λH ∣∣H∣∣F + 言 ∣∣b∣∣F ,
W,H,b	2	2	2
where (W, b) are the classifier weights and biases, H are the last layer features, and (λW, λH, λb)
are weight-decay Parameters. On this model, the authors not only Prove that all minima exhibit
Neural CollaPse but also that all local minima are global minima. In comParison to our current
PaPer, Zhu et al. (2021) focus on characterizing the landscaPe of the loss and, thus, do not exPlore the
dynamics and rate at which such minimizers of the loss are achieved.
Zhu et al. (2021) also make notable emPirical contributions by conducting a series of exPeriments on
the MNIST and CIFAR10 datasets trained on MLPs, ResNet18, and ResNet50. Their measurements
give evidence for the following novel NC-related Phenomena in deeP classification networks:
1.	NC is algorithm independent: NC emerges in realistic classification deeP net training
regardless of whether the algorithm is SGD, ADAM, or L-BFGS.
2.	NC occurs on random labels: NC emerges even when the one-hot target vectors are
comPletely shuffled.
3.	Width improves NC: Increasing network width exPedites NC when training with random
labels.
The authors of Zhu et al. (2021) moreover conducted ablation exPeriments suggesting that the
following substitutions can be made to deeP neural net architectures without affecting Performance:
1.	Weight-decay substitution: RePlacing (A) weight-decay on the norm of all network
Parameters with (B) weight-decay just on the norm of the last-layer features and classifiers.
2.	Classifier substitution: RePlacing (A) the last-layer classifiers that are trained with SGD
with (B) SimPlex ETF classifiers that are fixed throughout training.
While the authors only demonstrated these new behaviors on limited network-dataset combinations,
these exPeriments indeed insPire interesting conjectures about the generalization behavior of deeP
nets as well as Potential architecture design imProvements; Zhu et al. (2021) discuss many of these
conjectures and related oPen-questions in detail.
36