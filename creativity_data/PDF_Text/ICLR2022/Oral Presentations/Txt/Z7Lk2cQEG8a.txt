Published as a conference paper at ICLR 2022
The Hidden Convex Optimization Landscape of
Regularized Two-Layer ReLU Networks: an
Exact Characterization of Optimal S olutions
Yifei Wang*	Jonathan Lacotte*
Department of Electrical Engineering	Department of Electrical Engineering
Stanford University	Stanford University
wangyf18@stanford.edu	lacotte@stanford.edu
Mert Pilanci
Department of Electrical Engineering
Stanford University
pilanci@stanford.edu
Ab stract
We prove that finding all globally optimal two-layer ReLU neural networks can
be performed by solving a convex optimization program with cone constraints.
Our analysis is novel, characterizes all optimal solutions, and does not leverage
duality-based analysis which was recently used to lift neural network training into
convex spaces. Given the set of solutions of our convex optimization program, we
show how to construct exactly the entire set of optimal neural networks. We provide
a detailed characterization of this optimal set and its invariant transformations. As
additional consequences of our convex perspective, (i) we establish that Clarke
stationary points found by stochastic gradient descent correspond to the global
optimum of a subsampled convex problem (ii) we provide a polynomial-time
algorithm for checking if a neural network is a global minimum of the training
loss (iii) we provide an explicit construction of a continuous path between any
neural network and the global minimum of its sublevel set and (iv) characterize the
minimal size of the hidden layer so that the neural network optimization landscape
has no spurious valleys. Overall, we provide a rich framework for studying the
landscape of neural network training loss through convexity.
1 Introduction
Let X ∈ Rn×d and y ∈ Rn be the data matrix and the label vector. Given a number of neurons
m > 1 and a regularization parameter β > 0, we consider the regularized optimization problem
Pm = θmΘn 卜 β (θ) :=' (Xσ(Xui)α) + 2 X(Mk2+α2)}.
(1)
where Θm = Rd×m × Rm, θ = (U, α), ui is the i-th column of U ∈ Rd×m and αi is the i-th
coefficient of α ∈ Rm. Here we focus on the ReLU activation, i.e., σ(z) = max{z, 0} and absorb
the label y ∈ Rn in the loss function ` : Rn → R, which is assumed to be convex (e.g., logistic,
hinge, squared loss). The model Pim=1 σ(Xui)αi in (1) can be easily extended to the one with bias
term by adding a column of 1’s into the data X. We refer to an element θ ∈ Θm as a neural network
and to each pair (ui , αi ) as a neuron. We denote the set of optimal neural network as
Θ*m = {θ ∈ Θm | Lβ (θ) = Pm* }.	(2)
We denote the best training loss achievable by a neural as P* = infm>1 Pm* .
* Equal contributions
1
Published as a conference paper at ICLR 2022
The ReLU activation induces a natural partition of the parameter space. We denote D1, . . . , Dp as
all possible values of diag(1(Xu > 0)). We introduce the corresponding convex cones Ci = {u ∈
Rd|(2Di - I)Xu > 0} for i ∈ [p] where we denote [p] = {1, . . . ,p}. From this partition we have
the local linearization
σ(X u) = DiXu , for u ∈ Ci .	(3)
We let Di+p = -Di for i ∈ [p] and Ci+p = Ci for i ∈ [p].
Such a partition of the parameter space has regained attention in the recent literature. In fact, Pilanci
& Ergen (2020) recently showed that an optimal neural network θ* ∈ Θm for any m > 2p can be
constructed based on a solution of the convex optimization problem
2p	2p
P ：=WmiW {e( (w)：=`(X DiXwi)+β ∙X kwik2},	(4)
∈	i=1	i=1
where we introduced the convex feasible set W : = {W = (w1, . . . , w2p) | wi ∈ Ci}. In a nutshell,
this equivalence can be intuitively explained as follows. From the constraint wi ∈ Ci , we obtain the
local linearization σ(Xwi) = Di Xwi for i ∈ [p] and σ(Xwi) = -DiXwi for i ∈ [p + 1, 2p]. By
choosing neurons (ui, ai) such that w = ∣αi∣Ui, α% > 0 for i ∈ [p] and ai 6 0 for i > P +1, we
further obtain by positive homogeneity of the ReLU that
2p	2p
DiXwi =	σ(XUi)αi .	(5)
i=1	i=1
From the fact that the cones C1 , . . . , Cp cover the entire space, Pilanci & Ergen (2020) establish the
equality Pc = P*, and show that an optimal neural network can be constructed from an optimal
solutionwc,... ,wp.
In this work, we explore the mapping from the optimal set of solutions Wc of the convex program (4)
to the set of optimal neural networks Θcm . Our main contribution is to show how to construct the set
Θcm given Wc through simple transformations. We unveil some novel necessary conditions for a
neural network to be optimal and we illustrate the relevance of these conditions by relating them to
usual necessary conditions for optimality (e.g., Clarke stationarity).
1.1	Prior and related work
Several recent works considered over-parameterized neural networks in the infinite-width limit. In
particular, it is known that in this regime, gradient descent converges to an optimal solution, see
(Jacot et al., 2018; Du et al., 2018; Allen-Zhu et al., 2018; Nguyen, 2021). Further analysis in (Chizat
& Bach, 2018) showed that almost no hidden neurons move from their initial values to actively
learn useful features, so that this regime resembles that of kernel training and the infinite-width limit
infuses convexity. Wang & Lin (2021) showed that with an explicit regularizer based on the scaled
variation norm, overparametrization is generally harmless to two-layer ReLU networks. However,
experiments in (Arora et al., 2016) suggest that this kernel approximation is unable to fully explain
the success of non-convex neural network models.
Convexity arguments in neural networks were proposed in the recent literature (Bengio et al., 2006;
Bach, 2017). However, existing works, except (Pilanci & Ergen, 2020), are restricted to infinitely wide
networks. In turn, Bengio et al. (2006) and Bach (2017) consider greedy neuron-wise optimization
strategies for the infinite-dimensional optimization problem, which requires solving non-convex
problems at every step to train a shallow neural network. In contrast, in our work, we reveal the
hidden convex optimization landscape for any finite number of hidden neurons.
Besides the convexity properties of infinitely wide networks, many works derived lower bounds on
the hidden layer size to guarantee the absence of spurious minima. Venturi et al. (2019) showed that
the un-regularized (i.e. β = 0) objective Lβ has no spurious local minima provided that the number
of neurons satisfies m > n, and a similar result was shown in (Livni et al., 2014). Similar results were
derived for deep networks. For instance, Soudry & Carmon (2016) showed that under a dropout-like
noise assumption, there exist no differentiable spurious minima if the product of the dimensions of
the layer weights exceeds n and this result matches the classical lower bound (Baum, 1988) on the
minimal width of a neural network to implement any dichotomy for inputs in general position. In a
2
Published as a conference paper at ICLR 2022
similar vein, Nguyen & Hein (2017) showed that no spurious minima occur provided that one of the
layer’s inner width exceeds n and under additional non-degeneracy conditions. For activations other
than the ReLU (e.g., linear, quadratic, polynomial), similar lower bounds were derived in (Venturi
et al., 2019; Du & Lee, 2018; Soltanolkotabi et al., 2018). These analyses are typically based on the
idea that when m & n then it is very likely that the features σ(Xu1 ), . . . , σ(Xum) form a basis of
Rn so that the training problem reduces to finding a linear model with weights α1 , . . . , αm which
perfectly fits the labels. For the hinge loss and linear separable data, (Wang et al., 2019) show that
the modified stochastic gradient descent method can achieve global optimality despite the presence
of spurious local minima and saddle points.
The training landscape of neural networks is of great interest for theoretical analysis in the optimiza-
tion of neural networks. An important perspective is to analyze the landscape via paths through the
parameter space, see (Vidal et al., 2017). Indeed, in (Haeffele & Vidal, 2015; 2017; Sharifnassab
et al., 2019), it is shown that there exists a non-increasing path in objective value from every point
to the global minimum with mild assumption on the layer width. The existence of such paths also
indicates that the level sets of the training loss are connected (Freeman & Bruna, 2016; Venturi et al.,
2019; Nguyen, 2019; Nguyen et al., 2021) and there is no bad local valley (Nguyen & Hein, 2017).
However, with regularization, the training problem is more challenging. Intuitively, it reduces the set
of optimal solutions to those with small norms. Without regularization (i.e., β = 0), it should be noted
that the set of optimal solutions always contains infinitely many points. For instance, with ReLU
activations, it holds that if θ* = {(u*,α*)}m=ι is an optimal neural network, then any re-scaling
*
of θ* in the form {(Y ,γi α* )}m=ι (with γι,...,Ym > 0) has the same objective value and is thus
optimal. With regularization, this manifold is reduced to a single point. It is then natural to expect
that this minimal size of the hidden layer must increase. Further, the aforementioned analyses do not
extend since regularization also penalizes the norms of the ui ’s, and one cannot simply generate such
a basis of Rn based on the features σ(Xu1), . . . , σ(Xum ) by random sampling and then overfitting
the labels.
Recently, Pilanci & Ergen (2020); Ergen & Pilanci (2020) show that two-layer ReLU neural networks
can be optimized exactly via finite-dimensional convex programs with complexity polynomial in
the number of samples and hidden neurons. As indicated in Pilanci & Ergen (2020), the worst-case
complexity is exponential in the dimension of the training samples unless P = NP .
1.2	Summary of our contributions
In Section 2, we introduce the notions of minimal neural networks and nearly minimal neural
networks. These two notions are closely related to the plateau and the edge of the plateau of the loss
landscape.
In Section 3, we show that any minimal neural network θ can be represented, via an explicit map, in
the convex feasible space W as a point W (θ) such that Lβ (θ) > Lcβ (W (θ)), and vice-versa. This
structural result provides a mathematically rich perspective to characterize optimal neural networks
through the lens of convexity. We then provide an exact characterization of the set of all global
optima of the nonconvex problem, which include all nearly minimal neural networks generated via
the optimal solutions of the convex program.
In Section 4, we show that any Clarke stationary point θ with respect to Lβ is a nearly minimal neural
network. This provides a preliminary structure on the solutions found by stochastic gradient descent
(SGD), as it has been recently shown (see, for instance, Corollary 5.11 in Davis et al. (2020)) that the
limit points of SGD applied to neural network optimization are Clarke stationary. More importantly,
we show that Clarke stationary point θ with respect to Lβ also corresponds to a global minimum of a
subsampled convex problem. We also provide a polynomial-time algorithm (in the sample size n and
the hidden-layer size m) in order to test whether a neural network is globally optimal.
In Section 5, we show that any neural network is path-connected to a succinct representation (with
at most n + 1 non-zero neurons) and this path is with constant objective value. Then, from the
convex perspective of two-layer ReLU neural networks, we provide an explicit path of non-increasing
loss between θ and θ0 , where θ0 is the global optimum of the non-convex training problem. This
establishes that the training loss Lβ has no spurious local minima, provided that the number of
neurons is sufficiently large.
3
Published as a conference paper at ICLR 2022
1.3	Notations
We first present an alternative interpretation of the cones Ci and the diagonal matrices Di for i ∈ [p].
The ReLU activation function partitions the space of neurons u ∈ Rd into linearly separated regions,
that is, given a binary vector s ∈ {0, 1}n, the set of neurons u ∈ Rd such that 1(Xu > 0) = s
is a convex cone in Rd, if not empty. We enumerate the closures of all these cones as C1, . . . , Cp
and we set Ci+p = Ci for i ∈ [p]. For i ∈ [p], we introduce the corresponding diagonal matrices
Di = diag(1(Xu > 0)) for an arbitrary u ∈ Ci, and Di+p = -Di. Here the number p is the number
of dichotomies that the data matrix X can realize. It is upper bounded by P 6 2r (e(n-I)) where
r = rank(X), see (Cover, 1965).
Beyond the dichotomies of the space of neurons u ∈ Rd, we further introduce the partitions
(trichotomies) {I+, I0, I-} of [n] such that there exists a solution vector u ∈ Rd verifying (X u)k > 0
if k ∈ I+, (Xu)k = 0 if k ∈ I0 and (Xu)k < 0 if k ∈ I-. Clearly, there exists a finite number q of
such trichotomies and q is trivially upper bounded by 3n . For the j-th trichotomy {I+ , I0 , I- }, we
define the n × n diagonal matrix Tj with k-th diagonal element (Tj)kk = 1 if k ∈ I+, (Tj)kk = 0 if
k ∈ I0 and (Tj)kk = 0 if k ∈ I-. Such trichotomies are also discussed in Phuong & Lampert (2020).
For each j = 1, . . . , q, we define Qj as the closed convex cone of solution vectors for the j-th
trichotomy {I+, I0, I-}. We consider a partition {B1, . . . , B2q} of the neurons’ parameter space
where Bi : = Qi × R>0 forj = 1, . . . , q and Bj : = Qj-q × R<0 forj = q+ 1, . . . , 2q. We augment
the set of diagonal matrices {Tj}jq=1 by setting Tj = -Tj-q for j = q + 1, . . . , 2q.
For a neuron pair (u, α) ∈ Rd × R, we denote B(u, α) as the unique Bi such that (u, α) ∈ Bi.
The notion of path-connected sublevel set is introduced as follows.
Definition 1. We write θ I θ0 if the neural network θ0 ∈ Θm belongs to the path-connected
sublevel set (or valley) of θ ∈ Θm. Namely, there exists a continuous path γ : [0, 1] → Θm
such that γ(0) = θ, γ(1) = θ0 and t 7→ Lβ (γ (t)) is non-increasing. We denote the valley of
θ as Ω(θ) := {θ0 ∈ Θm | θ A θ0}. We say that θ ∈ Θm is non-spurious if θ A θ* for some
θ* ∈ argminθo∈θm Le(θ0). Otherwise, we say that θ and its valley Ω(θ) are spurious.
2	Minimal neural networks and nearly minimal neural networks
We start with the notion of minimal neural networks and nearly minimal neural networks. Minimal
neural networks enjoy a well-structured representation which is useful to understand the optimality
properties of two-layer neural networks.
Definition 2 (Minimal neural networks). We say that a neural network θ is minimal if (i) it is scaled,
i.e., ∣∣ui∣∣2 = ∣α∕ for i ∈ [m] and (ii) the cones B(u, α) of each of its non-zero neurons (u, α) are
pairwise distinct. That is, a minimal neural network has at most a single non-zero neuron per cone
Bi. We denote by Θmmin the set of minimal neural networks with m neurons.
Note that any minimal neural network has at most 2q non-zero neurons since there are 2q cones
Bi and at most one neuron per cone. Next, we introduce a slightly less structured class of neural
networks that one can interpret as ’split’ versions of minimal neural networks, and can have an
arbitrary number of non-zero neurons.
Definition 3 (Nearly minimal neural networks). We say that a neural network θ is nearly minimal if
(i) it is scaled and (ii) for any two non-zero neurons (u, α), (v, β) of θ, if B(u, α) = B(v, β) then u
and v are positively colinear, i.e., there exists λ > 0 such that u = λv. We denote by Θmmin the set of
nearly minimal neural networks with m neurons. It trivially holds that Θmmin ⊂ Θmmin.
For a nearly minimal neural network, by merging the neurons corresponding to the same trichotomies,
we can reformulate it into a minimal neural network without changing the objective value.
2.1	From nearly minimal to minimal neural networks
Nearly minimal neural networks have the property that any two neurons which share at least one active
cone must be positively colinear. As we establish next, these colinear neurons can be continuously
merged together along a path of constant objective value, resulting in a minimal neural network.
4
Published as a conference paper at ICLR 2022
Formally, we let θ ∈ Θe mmin be a nearly minimal neural network with m neurons and we fix (w, γ) ∈ θ
a non-zero neuron. Let (w2 , γ2), . . . , (wk, γk) ∈ θ be the other non-zero neurons such that for
each j = 2, . . . , k, we have sign(γ) = sign(γj), and, w and wj are positively colinear. Write
/	P	P	P	111	1	1	/E E ∖	E	Pk_ 1 | Yj |wj	1
(wι,Yι) ：= (w,γ), and define the merged neuron (wm,γm) as Wm :=	/ j	and
√k ∑k=ι ∣γj∣wjk2
γm : = sign(γ) kwm k2. Let M(θ) be a copy of θ where each such set of k positively colinear
neurons {(w1, γ1), . . . , (wk, γk)} is replaced by the k neurons {(wm, γm), (0, 0), . . . , (0, 0)}. We
refer to M(θ) as the merged version of θ. The next result states relevant properties of M(θ).
Proposition 1. Let θ ∈ Θe mmin. Then, the following results hold.
1.	The merged neural network M(θ) is a minimal neural network.
2.	We have θ I M(θ), and the continuous path from θ to M(θ) has constant objective value.
3. If M(θ) is a local minimum of Lβ, then θ is also a local minimum of Lβ.
Intuitively, merging the colinear neurons preserves the active cones and leaves a single neuron per
cone, so that M(θ) is indeed minimal. The third property essentially follows from the fact that
M(θ) has more degrees of freedom than θ since it has more neurons equal to 0. In addition, the
continuous path of constant objective value from θ to M(θ) can be explicitly constructed (see the
proof in Appendix B.1).
3	Mapping neural networks to a convex optimization landscape
We provide here an explicit map from the set of minimal neural networks to the feasible set W of the
convex program (4), and vice-versa. For W = (w1, . . . , w2p) ∈ W, we let kW k0 be number of the
non-zero vectors in w1 , . . . , w2p . Define
Wm = {W ∈W∣k W ko 6 m} , Wm = Wm ∩W * .	(6)
First, we introduce the map θ 7→ W(θ) from Θmmin to Wm where for each i = 1, . . . , 2p, we set
Wi ⑻：=E l%luj,
j=1,...,m
B(uj,αj)⊆Bi
(7)
and such that each non-zero neuron (uj, αj) contributes only to a single Wi. To understand the latter,
note that each cone B(uj, αj) might be a subset of several (adjacent) cones Bi and hence, one might
need to choose which Wi a neuron (uj , αj ) contributes to. These ties can be resolved arbitrarily
without affecting any of our results.
Conversely, we construct a map W 7→ θ(W) from Wm to Θmmin by setting θ(W) = {(ui, αi)}im=1
where the (ui, α/ are defined as follows. Denote iι < … < im, the indices such that if i ∈
{i1, . . . , im} then Wi = 0. Take the index J (if any) such that iJ 6 p and iJ+1 > p + 1. Let
{Kι,..., K'} be a partition of {iι,..., ij} in terms of the repartition of w”,..., WiJ into the cones
{Qι,... Qq}. Similarly, let {K'+ι,..., Ky } be a partition of {ij+ι,..., im} in terms of the
repartition of wJ+1,..., Wim into the cones {Q1,... Qq}. Then, for 16 j 6' + '0, we set
(Uj,％)：= (q!⅛ E SrXwi j ,
(8)
where γj = 1 ifj 6 ` and γj = -1 ifj > `. Finally, for `+ `0 + 1 6j 6 m, we set (uj, αj) = (0, 0).
As stated in the next result, these mappings can only improve the training loss.
Proposition 2. It holds that for any W ∈ Wm we have Lβ (θ(W)) 6 Lcβ (W), and, for any θ ∈ Θmmin,
一 ____________,≈. . 一 ,≈. 一 一 一一 一 ________________________,≈. . _ ~
we have Le(W(θ)) 6 Le(θ). Furthermore, it holds that θ(W(θ)) ∈ Ω(θ).
These mappings between minimal neural networks and the convex feasible set provide a rich structure
to address the optimality properties of neural networks. In Figure 1, we provide an illustration of the
non-convex and convex landscapes on a toy neural network training model.
5
Published as a conference paper at ICLR 2022
Figure 1: Comparison of the non-convex landscape (left) and the convex landscape (right) of
program (4). Here, we consider the toy example with date X = 1, label y = 1 and the `2 loss.
Then, We have Le(u, α) = (1 - max{u, 0} α)2 + 1 (|u|2 + ∣α∣2). The convex objective is then
Lcβ(v, w) = (1 - v + w)2 + (|v| + |w|) subject to v, w > 0. The set of minimal neural networks
corresponds to |u| = ∣α∣, which includes the optima. Further, the optimal values of the two functions
match and are equal to 0.75, and attained at (u, α) = (1/y∕2,1/vz2) and (v, W) = (1/2, 0). Note
that u∣ɑ∣ = V, and this indeed corresponds to our mapping (7).
3.1	The global optimal set of neural networks
Let m* = minw∈w* ∣∣ W∣∣0. As a consequence of Caratheodory,s theorem, we have the following
upper bound on the minimal cardinality m* of an optimal solution.
Lemma 1. It holds that m* 6 n + 1. Further, for any m > m*, we have that Pm* = inf k>1 Pk*.
From the definition of m*, it clearly holds that Wm = 0 for m > m*. Then, We present the mapping
from the optimal solution to the convex problem (4) to a globally optimal neural network for the
non-convex problem (1).
Lemma 2. Let W = (w1, . . . , w2p) ∈ W*, and denote by I = {i1, . . . , ikWk0} ⊂ [2p] the set of
indices such that wi* 6= 0 for i ∈ I. We set
(uj, αj) =
(pk⅛ ,γij G
(9)
for ij ∈ I. Here γi = 1 if i 6 p and γi = -1 if i > p. Then, it holds that θ = {(ui, αi)}ik=W1k0 is an
optimal neural network, i.e., Lβ (θ) = P*.
We denote the above mapping (9) by ψ, and we set Θcmvx = ψ(Wm* ). According to Lemma 2, it
holds that Θcmvx ⊆ Θ*m. Given a neuron (u, α), we say that a collection of neurons {(uj,αj)}jk=1is
a splitting of (u, α) if (uj,αj) = (√γju, √γjα) for some Yj > 0 and Pk=I Yj = 1. Given a neural
network θ = {(ui, αi)}im=1, a splitting of θ is any neural network θ0 ∈ Θm such that the non-zero
neurons of θ0 can be partitioned into splittings of the neurons of θ. Similarly, split neurons can be
merged back to their original form. We denote by Θcmvx the set of splittings generated from Θcmvx. We
provide an exact characterization of the optimal set in the following theorem.
Theorem 1. Suppose that m > m*. It holds that Θ*m = Θcmvx. Namely, all optimal solutions of the
nonconvex loss can be found via the optimal solutions of the convex program (4) up to permutation
and splitting/merging of the neurons as defined above.
We compare our result with the result in (Pilanci & Ergen, 2020) as follows. Essentially, Pilanci &
Ergen (2020) show how to construct one globally optimal solution of the nonconvex loss by solving
the convex program, while Theorem 1 shows how to construct the entire set of global optimum of the
nonconvex loss. The relations among Wm* , Θcmvx , Θcmvx and Θ*m is illustrated in Figure 2.
1
Example 1. We consider a toy example, where X = 0
1
p = 6 and we can enumerate the diagonal matrices Di as
0
1,Y=
1
"100#
and β = 0.1. In this case,
D1 = diag([0, 0, 0]), D2 = diag([0, 1, 0]), D3 = diag([0, 1, 1]),
D4 = diag([1, 0, 0]), D5 = diag([1, 0, 1]), D6 = diag([1, 1, 1]).
(10)
6
Published as a conference paper at ICLR 2022
Proposition 1
Nearly minimal neural networks
Θ CVX
m
Theorem 1
The set of global minimizers
of the nonconvex loss
merge
split
Θ
*
m
Figure 2: Illustration of relations between Wm, ΘCVx, ΘCVx and Θ*.
The optimal solution to the convex problem (4) is given by W* = (w1* , . . . , w2*p), where W* only
consists of one non-zero block w5* = [0.86, -0.79]T. Therefore, the set of the global minimizers of
the nonconvex loss Lβ consists of all nearly minimal neural network θ = {(ui, αi)}im=1 satisfying
αi = √γi Jkw*k2,
i ∈ [m] ,
(11)
where im=1 γi = 1 and γi > 0 are arbitrary. These correspond to the split versions of the single
neuron w5*. We investigate numerically our result: for m = 5, we run gradient descent (GD) on the
nonconvex loss Lβ until we find a nearly stationary neural network {(ui, αi)}i5=1. We plot the points
αi ui as well as w5* in Figure 3 in the Appendix.
4	Characterization of all local minima
Minimal neural networks form a subset considerably smaller than the entire space of neural networks,
and they do contain all the global optima. In this section, we show that first-order methods can find
networks that can be merged to a minimal representation. Moreover, we exhibit the existence of a
path of strictly decreasing objective value from any neural network to a minimal representation. This
may suggest that minimal neural networks are the right notion to study the complexity of the loss
landscape.
4.1	SGD finds a nearly minimal neural network
The limit points of SGD are almost surely Clarke stationary with respect to Lβ (see, e.g., (Davis et al.,
2020; Bolte & Pauwels, 2019)). We show next that any Clarke stationary point w.r.t. the loss L(θ) is
in fact a nearly minimal neural network. This shows that SGD finds a neural network which can be
merged to a minimal representation.
Theorem 2. Fix m > 1. Any Clarke stationary point θ of the non-convex loss function Lβ over Θm
is a nearly minimal neural network. Consequently, any local minimum of Lβ is nearly minimal.
As an additional motivation for studying nearly minimal neural networks, we establish the following.
Proposition 3. Let θ ∈ Θm be any neural network. There exists a continuous path in Θm from θ to
a nearly minimal neural network along which the loss function is (strictly) decreasing.
The proof of Theorem 2 is deferred to Appendix B.4 and that of Proposition 3 to Appendix B.5. Both
proofs are based on the same transformations of a neural network θ which decreases the training loss:
scaling the neural network and then aligning the non-zero neurons which belong to the same cones Bi
so that they become positively colinear. These transformations leave the predictions unchanged due
to the piecewise linear structure of the activation function but decrease the value of the regularization
term. Thus, our notions of minimal representations are intimately related to (i) the piecewise linear
structure of the activation function and (ii) the regularization effect. We emphasize again that these
two features of neural network training are commonly used in practice (e.g., ReLU and weight decay).
Combining Proposition 3 and Proposition 1, we immediately obtain the following result.
7
Published as a conference paper at ICLR 2022
Corollary 1. The valley Ω(θ) ofany neural network θ contains a minimal one. Further, if the valley
Ω(θ) is non-spurious, then it contains an optimal neural network which is minimal.
Interestingly, we are able to provide an explicit construction of the map from a neural network θ to a
nearly minimal representation, and this map is based on the aforementioned transformations (scaling
and aligning; see the proof of Proposition 3 for details).
Hence, the study of the optimality properties of a neural network can be narrowed down to the
structured class Θe mmin which contains the limit points of SGD. Next, we establish that we can go
further by considering the class of minimal neural networks Θmmin.
4.2	Clarke’ s stationary point and subsampled convex program
Consider the convex program with trichotomies:
q	2q
min `	TjX(wj - wj+q) +β	kwjk2,	s.t. wj, wj+q ∈ Qj,j ∈ [q].	(12)
j=1	j=1
The convex program with trichotomies also provides a convex optimization formulation of the
regularized neural network training problem (1).
Proposition 4. The convex program (12) with trichotomies has the optimal value P *.
Given a subset I ⊆ [q], we can also consider a subsampled convex program with trichotomies:
min ` X TjX(wj - wj+q) + β X(kwj k2 + kwj+q k2), s.t. wj, wj+q ∈ Qj, j ∈ I. (13)
j∈I	j∈I
We show the connection of the Clarke’s stationary point of the nonconvex loss function Lβ and the
optimal solution of the subsampled convex program (13) as follows.
Theorem 3. Suppose that θ is a Clarke’s stationary point of the nonconvex loss function Lβ. Let
I = {j ∈ [q] | there exists k ∈ [m] such that Tj = diag(sign(X uk))}. Then, θ corresponds to a
global optimum of the subsampled convex program (13).
In other words, any local minimum of the nonconvex loss (1) can be characterized as a global
minimum of a subsampled convex program (13); further, the optimality gap is equal to the gap
between the subsampled problem (13) and the full convex program (12).
4.3	Subsampled convex program and verifying global optimality
We established that a stationary point of the non-convex training loss is a global optimum of a
subsampled convex program. Here, we build on this observation to design a procedure to check
whether a neural network is in fact a global minimizer. Our key theoretical contribution is to provide
such an algorithm that runs in polynomial time of sample size n.
We first note that the set {Di}ip=1 can be constructed in polynomial time of n via standard results
from geometry and hyperplane arrangements in (Cover, 1965; Winder, 1966; Ojha, 2000). Consider
a feasible point W = (w1, . . . , w2p) ∈ W of the convex program (4). Note that each constraint
wi ∈ Ci is a linear inequality constraint. Indeed, as described in Section 2, each Ci is the convex cone
of solution vectors for a dichotomy {I+, I-} of {1, . . . , n}. Writing X+(i) (resp. X-(i)) the subset of
rows of X indexed by I+ (resp. I-), we have u ∈ Ci if and only X+(i)u 0 and X-(i)u 0. Using
these notations, the convex program (4) can be reformulated as
2p
min	'(bc(W))	+ β V" I∣wik2	s.t.	X，Wi 占	0,	Xyiwi	W 0	∀i	= 1,..., 2p,
w1,...,w2p
i=1
where ybc(W) = Pi2=p 1 DiXwi. Hence, given a feasible point W* = (w1, . . . , w2p) ∈ W to the
convex program (4), it holds that W* is a global minimizer if and only if W* satisfies the Karush-
Kuhn-Tucker (KKT) conditions (see (Boyd et al., 2004)) of (4). Here, W* ∈ W satisfies the
8
Published as a conference paper at ICLR 2022
KKT conditions if, for each i = 1,..., 2p, there exist ζ+i),Z(i) 占 0 such that hζ+i),x+^)w*i =
hZ(i),X(i)w*i =0 and
X>Di^'(yc(W*))+ β^i^ + X(i)>ζ(i) - x+i)>ζ+i) = 0,	if w* = 0	(14)
kw"∣2	+	+
∣∣X >DiV'(bc(W *))+ X(i)>ζ(i) - χ+i)>ζ+i)∣∣2 6 β,	otherwise.	(15)
This amounts to solving a system with 2np variables of 2np linear inequalities, n0 convex quadratic
inequalities and (2p 一 nQ)(d + 2) linear equalities, where no is the number of variables w* equal to 0,
and this can be done efficiently using standard convex solvers, in time polynomial in the sample size
n. The next result establishes the link between checking the KKT conditions of the above program
and checking whether a neural network is a global optimum. Its proof is deferred to Appendix B.8.
Proposition 5. Let θ ∈ Θmmin be a minimal neural network. Suppose that W (θ) satisfies the KKT
conditions as described above. Then, θ(W (θ)) is a global optimum of the loss Lβ.
In the above result, the minimal neural network assumption is not restrictive, since any local minima
of the loss must be a nearly minimal neural network (Theorem 2), and then, any nearly minimal neural
network can be reduced to a minimal one along a continuous path of constant value (Theorem 1).
5	Non-spurious valleys and convex landscape
The subsampled convex program relates to the optima of SGD. It is then of interest to understand the
landscape when m is much smaller than the number of cones. Here we show that a critical threshold
is n + 1 + m* for having a path of non-increasing value. While this may be an open problem, it is
reasonable to expect SGD to behave better in that case, and thus to find a global minimum. For an
arbitrary neural network θ, we can find a point θ0 with at most n + 1 non-zero neurons such that they
are connected with a path with constant objective values.
Proposition 6. Given a scaled neural network θ ∈ Θm with m > n + 1, there exists a neural
network θ0 with at most n + 1 non-zero neurons and there exists a path with constant objective value
between θ and θ0. Namely, θ I θ0 and θ0 I θ.
A direct corollary of Proposition 6 is that for any global optimum θ* of Lβ , we can find a succinct
representation θ* with at most n +1 non-zero neurons and there exists a path between θ* and θ*
such that the objective value is constant. Based on Proposition 6, we can also show that there is no
spurious valley. The following result states the absence of spurious valleys for the training loss as
soon as m & n.
Proposition 7. Let m > n + 1 + m*. Then, it holds that for any neural network θ ∈ Θm, we have
θ I θ* for some θ* ∈ Θ*m.
In other words, provided that m > n + 1 + m*, all strict local minima are global. Compared to the
standard lower bound m > n for the unregularized case in (Venturi et al., 2019; Livni et al., 2014),
we have an additional term m* 6 n + 1 induced by weight decay.
As known in the literature (Freeman & Bruna, 2016; Venturi et al., 2019; Vidal et al., 2017), the
loss landscape of an over-parameterized shallow neural network is almost convex. Essentially,
for a sufficiently wide neural network, for any θ0, θ1 and λ ∈ [0, 1], we can find θλ such that
f(x; θλ) = λf (x; θ0) + (1 - λ)f (x; θ1). From a perspective of convex formulation of two layer
neural network, we give a sufficient upper bound on the width of neural network to ensure the convex
landscape in terms of realizations. Essentially, as long as m > 2(n + 1), for any two neural network
realizations f (x; θo) and f (x; θo), we can find succinct representations θ1,θ2 such that θi A 仄 and
θi A θi for i = 1, 2. Then, for any λ ∈ [0, 1], we can construct θλ such that
f(x; θλ) = λf (x; θθ0) + (1 - λ)f (x; θθ1)	(16)
The construction of θλ is straightforward. From Proposition 6, we can take θi as a neural network
with at most n + 1 non-zero neurons for i = 1, 2. Given m > 2(n + 1), following the proof of
Proposition 7, we can construct θλ satisfying (16).
9
Published as a conference paper at ICLR 2022
Acknowledgements
This work was partially supported by the National Science Foundation under grants ECCS-2037304,
DMS-2134248, and the Army Research Office.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research,18(1):629-681, 2017.
Eric B Baum. On the capabilities of multilayer perceptrons. Journal of complexity, 4(3):193-215,
1988.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing systems, pp. 123-130, 2006.
J6r6me Bolte and Edouard Pauwels. Conservative set valued fields, automatic differentiation,
stochastic gradient method and deep learning. arXiv preprint arXiv:1909.10300, 2019.
Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and
examples. Springer Science & Business Media, 2010.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in neural information processing systems, pp. 3036-
3046, 2018.
Frank H Clarke. Generalized gradients and applications. Transactions of the American Mathematical
Society, 205:247-262, 1975.
Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326-334, 1965.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method
converges on tame functions. Foundations of computational mathematics, 20(1):119-154, 2020.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic
activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In International Conference on Artificial Intelligence and Statistics, pp.
4024-4033. PMLR, 2020.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540, 2016.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015.
Benjamin D Haeffele and Rene Vidal. Global optimality in neural network training. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331-7339, 2017.
10
Published as a conference paper at ICLR 2022
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in neural information processing systems, pp. 855-863, 2014.
Quynh Nguyen. On connected sublevel sets in deep learning. In International Conference on Machine
Learning, pp. 4790-4799. PMLR, 2019.
Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with
linear widths. arXiv preprint arXiv:2101.09612, 2021.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pp. 2603-2612. JMLR. org,
2017.
Quynh Nguyen, Pierre Brechet, and Mondelli Marco. When are solutions connected in deep networks?
arXiv preprint arXiv:2102.09671, 2021.
Piyush C Ojha. Enumeration of linear threshold functions from the lattice of hyperplane intersections.
IEEE Transactions on Neural Networks, 11(4):839-850, 2000.
Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally
separable data. In International Conference on Learning Representations, 2020.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. arXiv preprint arXiv:2002.10553, 2020.
Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regularization
behind neural reconstruction. arXiv preprint arXiv:2012.05169, 2020.
Arsalan Sharifnassab, Saber Salehkaleybar, and S Jamaloddin Golestani. Bounds on over-
parameterization for guaranteed existence of descent paths in shallow relu networks. In In-
ternational Conference on Learning Representations, 2019.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural
network optimization landscapes. Journal of Machine Learning Research, 20(133):1-34, 2019.
Rene Vidal, Joan Bruna, Raja Giryes, and Stefano Soatto. Mathematics of deep learning. arXiv
preprint arXiv:1712.04741, 2017.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. IEEE Transactions on Signal Processing, 67(9):
2357-2370, 2019.
Huiyuan Wang and Wei Lin. Harmless overparametrization in two-layer neural networks. arXiv
preprint arXiv:2106.04795, 2021.
RO Winder. Partitions of n-space by hyperplanes. SIAM Journal on Applied Mathematics, 14(4):
811-818, 1966.
11
Published as a conference paper at ICLR 2022
A Figure in Example 1
We plot the points a® as well as w5 in Figure 3.
Figure 3: Plots in the neuron space of nerual networks trained by GD over two trials. Points αiui of
the neural network {(ui, α∕}m=ι trained by GD, and non-zero block w5 of the global solution of the
convex problem. The points a%u% lie in the convex hull of {0, w5}, and they satisfy equation (11) up
to numerical tolerance. This implies in particular that the neural network found by GD is optimal.
B Proofs of main results
Thoughout the appendix, we will use the following notations. For W = (w1, . . . , w2p), we define
ybc(W) = Pi2=p 1 DiXwi. For θ = (U, α), where U ∈ Rd×m and α ∈ Rm , we define yb(θ) =
Pjm=1σ(Xuj)αj.
B.1 Proof of Proposition 1
According to the construction of M(θ), there is at most one non-zero neuron per cone, and, each
neuron (w,γ) satisfies ∣∣wk = ∣γ∣, i.e., M(θ) is minimal.
Fix a cone B. Let (w1, γ1), . . . , (wk, γk) be the neurons of θ such that B(ui, wi) = B for i =
1,...,k, and let wm = pj=1 lγj|j and γm = Sign(Y1) ∙ Pkwmk2 be the merged neuron. Since θ
V∑k=ι |Yj |wj
is nearly minimal, we know that w1 , . . . , wk are positively colinear.
For t ∈ [0, 1], define θ(t) such that it has k neurons associated with the cone B given by
…小一(I- "wiM + twm∣γm|
rw 1 (t)：
√k(1 — t)w1∣Y11 + twm∣γm∣∣2
Yι(t):= Sign(Y1) ∙ kw1(t)∣∣2
wj (t) ： = √Γ-^t ∙ wj
Yj(t) ：= √1 - t ∙ Yj ,
where j > 2. Note that for all j > 1, all vectors wj (t), wj , wm are positively colinear, that
kwj (t)k2 = |Yj (t)| and that sign(Yj (t)) = sign(Y1). Further, note that (w1(0), Y1(0)) = (w1, Y1),
(w1(1), Y1(1)) = (wm ,Ym ) and for j > 2, (wj (0), Yj (0)) = (wj,Yj), (wj (1), Yj (1)) = (0, 0).
Consider the neural networks θ(t) with neurons (wj(t), Yj (t)) respectively defined for each cone
B. It holds that θ(0) = θ and θ(1) = M(θ). Then, the contribution of the neurons in B to the
predictions yb(θ(t)) are given by
kk
Xσ(Xwj(t))Yj(t) = Sign(Y) ∙ σ X(wi(t)|Yi(t)| + Xwj(t)|Yj(t)|)
j=1	j =2
=Sign(Y) ∙ σ ((1 - t)Xwι∣Yι∣ + tXwm∣Ym∣ + (1 - t)X Xwj∣Yj∣)
=sign(Y) ∙σ(Xwm∣Ym∣).
12
Published as a conference paper at ICLR 2022
Thus, the predictions yb(θ(t)) are constant as a function oft. Similarly, we claim that the regularization
term R(θ(t)) is constant as a function of t. Since the neurons are scaled, the contribution of the cone
B to the regularization term is given by (up to the constant β)
kk
X kWj (t)k2∣Yj (t)l = kw1(t)k2∣Y1(t)l + X kWj (t)k2∣Yj (t)l
j=1	j=2
k
=k(1- t)wι∣γι∣ + twm∣γm∣k2 + (1- t) X kWj k2∣Yj |
j=2
k
=k(1- t) X WjlYj I + twm∣γ mlk2
j=1
=kwmk2∣γ T,
where the third equality follows from the triangular equality when all vectors are positively colinear.
Thus, we have explicited a continuous path from θ to M(θ) such that Lβ is constant along that path.
Now, we show that if M(θ) is a local minimum then θ is also a local minimum. We proceed with
the converse. Assume that θ is not a local minimum of Lβ. For a cone B, let (W1, γ1), . . . , (Wk, γk)
be the neurons of θ such that B(Wi, γi) = B. Let (Wm, γm) be the merged neuron. If θ is not
a local minimum, then there exists a small perturbation θε : = {(uiε, αiε)}im=1 of the neurons of
θ such that (i) Lβ (θε) < Lβ (θ), (ii) sign(γiε) = sign(γi) and (iii) for each i = 1, . . . , m, we
have I+(σ(Xui)) ⊆ I+(σ(Xuiε)) and I-(σ(Xui)) ⊆ I- (σ(Xuiε)), where we use the notation
I+(z) : = {i ∈ {1, . . . , n} | zi > 0} and I-(z) : = {i ∈ {1, . . . , n} | zi < 0} for a vector z ∈ Rn.
Then, we define for t ∈ [0, 1],
W1 (t)
(1 - t)wj∣Yj∣ + twm∣Ym∣
Pk(I — t)wjlYjl + twm|Ym|||2
Y1 ⑴=Sign(YI) ∙ kw1⑴k2,
Wj (t) = √1 — t
∙ Wj,
Yj(t) = √1 - t ∙ Yj ,
where j > 2. Let θ(t) the neural network with neuronS (Wj (t), Yj (t)) defined aS above for each cone
B. Then, the contribution of a cone B to the predictions yb(θ(t)) is given by
Xσ(Xwj(t))γj(t) = Sign(Y1)卜X((1- t)w；/ + twm∣Ym∣)) + (1 -
Due to the above property (iii), we have that
k
σ( Wj IYj
j=2
σ(X((1- t)wε∣γε∣ + twmIγml)) = (1 - t)σ(Xwε∣γε∣)+ tσ(Xwm∣γmI).
Thus, the contribution of the cone B to the predictions is
kk
Xσ(XWj(t))Yj(t)= (1-t)Xσ(XWjj)Yjj+tσ(XWm)Ym.
j=1	j=1
Summing over all the coneS, we find that
yb(θ(t)) = (1 - t)yb(θj) + t yb(M(θ)).
Similarly, the contribution of the cone B to the regularization term R(θ(t)) iS
kk
X kWj(t)k2IYj(t)I = k(1 - t)W1jIY1jI + tWmIYmIk2 + (1 - t) X kWjjk2IYjjI
j=1	j=2
k
6(1-t)XkWjjk2IYjjI+tkWmk2IYmI,
j=1
13
Published as a conference paper at ICLR 2022
where the last inequality is due to the triangular inequality. Thus, we obtain that
R(θ(t)) 6 (1 - t)R(θε) + t R(M(θ)).
Hence, we get that Lβ(θ(t)) 6 (1 - t)Lβ(θε) +tLβ(M(θ)). Since Lβ(θε) < Lβ (θ) = Lβ (M(θ)),
we have that Lβ(θ(t)) < Lβ(M(θ)) for any t < 1. This concludes the proof.
B.2 Proof of Proposition 2
Let θ ∈ Θm, and consider the point W(θ), as defined in (7), whose expression is given by
wi(θ) :=	X	|aj |uj,	(17)
j=1,...,m
B(uj ,αj)⊆Ci
and such that each non-zero neuron (uj, αj) contributes only to a single wi(θ).
We prove that the mapping θ 7→ W(θ) is well-defined. Each set Pi is a cone and wi(θ) is a positive
linear combination of elements of Pi. It follows that wi (θ) ∈ Pi, and W (θ) ∈ W. Further, θ has
m neurons and each neuron (uj, αj) contributes only to a single wi(θ). It follows that at most
m variables among {w1 (θ), . . . , w2p (θ)} are non-zero, and W(θ) ∈ Wm. Hence, the mapping
θ 7→ W (θ) is well-defined from Θm to Wm .
We show that Lcβ (W (θ)) 6 Lβ (θ). We note that
2p	2p
bc(W (θ)) = X DiXwi(θ) = X	X	DiX∣αj∣Uj.
i=1	i=1 j =1,...,m
B(uj ,αj)⊆Ci
Note that for a neuron (uj, aj) such that B(uj, aj) ⊆ Ci, We have that DiX∣αj∣u7∙ = σ(Xuj) αj.
It implies that
2p	m
bc(W⑹)=X	X	DiX|aj |uj=Xσ(Xuj)αj=b(θ),
i=1 j=1,...,m	j=1
B(uj ,αj)⊆Ci
and consequently, '(bc(W(θ))) = '(b(θ)). On the other hand, We have
2p	2p	m
X kwik2 = X Il X	|ajιuj∣∣2 6 X |ajιkujk2,
i=1	i=1	j =1,...,m	j=1
B(uj ,αj)⊆Ci
Where the last inequality folloWs from triangular inequality. Since the neurons (uj, αj) are scaled, We
get that Pm=IIaj ∣kujk2 = 2 Pm=1 ∣αj∣2 + kujk2, and we finally obtain that Le(W(θ)) 6 Le(θ).
We shoW that the mapping W 7→ θ(W) is Well-defined. Based on the construction (8), it holds
that the non-zero neurons (uj, αj) belong to pairWise distinct cones in {B1, . . . , B2q}. Further, the
neurons are scaled. Hence, θ(W) is a minimal neural netWork With m neurons.
We prove that Le (θ(W)) 6 Lce (W). Denote by D(j) the diagonal matrix associated With B(uj, αj )
for 1 6 j 6 l + l0 . We have that
m	l+l0	l+l0
b(θ(W)) = X σ(Xuj)aj= X Dj)XujIaj ∣ = X Dj)X X Vi.
j=1	j=1	j=1	i∈Kj
It holds by construction that for each i ∈ Kj, D(j)Xwi = DiXwi. Therefore, We find that
b(θ(W)) = bc(W) and '(b(θ(W))) = '(bc(W)). On the other hand, we have that
m	'+'0	'+'0	2p
Xkujk2IajI=XII XviII26XXkvik2=Xkvik2,
j=1	j=1 i∈Kj	(i) j=1 i∈Kj	i=1
where inequality (i) follows from triangular inequality. Hence, we obtain that Le (θ(v)) 6 Lce(v).
14
Published as a conference paper at ICLR 2022
B.3	Proof of Theorem 1
First, We ShoW that Θmvx ⊆ Θm. Let θ ∈ Θmvx, and consider θ0 a split version of θ. Let (u, α)
be a neuron of θ, and {(uj, αj)}jk=1 the neurons of θ0 which correspond to the split of (u, α).
We have Pjm=1 σ(X uj)αj = Pjm=1 γj σ(X u)α = σ(Xu)α because Pjk=1 γj = 1. Furthermore,
2 Pj=I kujl∣2 + |%|2 = 1 Pk=I Yj(kuk2 + IaF) = 1 (kuk2 + IaF), whence L(θ) = L(θ0).
. ≈ ___ _ .
Consequently, Θmx ⊆ Θ^.
It remains to show that θ^ ⊆ Θmvx. Let θ ∈ θm. Due to the strong convexity of the regularization
term and the re-scaling invariance of the term σ(Xuj)aj, we must have that kujk2 = IajI for
each neuron (uj , aj ) of θ. We partition the neurons of θ such that the neurons in each partition
{(ui, ai)}ik=1 belong to the same cone C (in the sense that uiai ∈ C), and the cones are pairwise
distinct across partitions. Due to the regularization term, it is straightforward to show that the neurons
must be positively colinear, and that this corresponds to a split. Thus, Θm ⊆ Θcmvx and this concludes
the proof.
B.4	Proof of Theorem 2: Clarke stationary points are nearly minimal neural
NETWORKS
We review the definition of the Clarke subdifferential Clarke (1975) of f . At x ∈ Rd, this is defined
as
∂cf (x): = Conv < lim ▽/(Xk) ∣ lim Xk = x, xj ∈ D ,,
k→∞	k→∞
where D : = {x ∈ Rd I f differentiable at x}. In particular, it holds that Rd \ D has measure equal
to zero Borwein & Lewis (2010) under mild assumptions on f . Then, we say that x ∈ Rd is Clarke
stationary with respect to f if 0 ∈ ∂C f (x).
Let θ ∈ Θm be Clarke stationary, i.e., there exist λ(1) , . . . , λ(N) > 0 and sequences
{θk(1)}k, . . . , {θk(N)}k such that PjN=1λ(j) = 1, limk→∞ θk(j) = θ for each j = 1, . . . , N, the
loss function Lβ is differentiable at each θk(j) and
0
N
X λj) lim VLe (θjj)).
k→∞
j=1
PART 1 : THE NEURAL NETWORK θ MUST BE SCALED.
By contradiction, we assume first that the neural network θ is unscaled, i.e., there exists a neuron
(u, a) such that kuk2 6= IaI. Write (u(kj), a(kj)) the corresponding neuron of each θk(j). Since
limk→∞(u(kj), a(kj)) = (u, a), up to extracting subsequences, we can assume that the neurons
(u(kj), ak(j)) are also unscaled, i.e., ku(kj) k2 6= Ia(kj) I for all k > 1 and j = 1, . . . , N.
CASE 1:	a 6= 0
Since limk→∞ a(kj) = a, up to extracting subsequences, we can assume that a(kj) 6= 0 for all k > 1
and j = 1, . . . , N. Then, for each j = 1, . . . , N and k > 1 and for t ∈ [0, 1], we define the neural
network θk(j)(t) as a copy of θk(j) except for the neuron (u(kj), a(kj)) that we replace by
(j)
uj)(t) = -jjττ,	ɑkj) (t) = Yj)(t) ∙ akj),
where
1+ t(γkj) *
- 1) and we use the improper notation
Uj
γj)(t)
0
if u(kj) = 0. Note that θk(j) (t) defines a continuous path from θk(j) = θk(j)(0) to the scaled neural
network θk(j)(1). Further, since σ is positively homogeneous, it holds that for any t ∈ [0, 1],
σ(Xu(kj)(t))a(kj)(t) = σ(Xu(kj))a(kj) ,
15
Published as a conference paper at ICLR 2022
so that that the function Lβ(θk(j)(t)) is constant as a function of t ∈ [0, 1]. On the other hand, the
regularization term satisfies
R(θkj)(t)) = R(θkj)) - 2(kukj)k2 + ∣ɑkj)∣2) + 2
、---------------{z---------------}
: = C(k,j)
(j ) 2	2
C(k,j) +β	) (t)∣αj)|2
、~{{/	2 1
independent of t
Note that the function gk(j)(t) : = Lβ(θk(j)(t)) is differentiable, and simple algebra yields
dgkj)/0- R(/ku"2 - q∣αj5∣ʌ //a?	∣∣jj)∣∣2>
~^~~°υ) = β •	--------Γ=Γ=-------	∙ (|ak | - kuk k2) .
dt	\ 历)
Hence,
lim dgj(0)= β ∙ (PkUp-PH! ∙ (∣α∣2-kuk2).
k→∞ dt	∖	,∣α∣	J
dg(j)
Since |a| = kuk2, it follows that limk∞ -^t-(0) < 0. On the other hand, We have that
(j )	(j )
Jd-(0) = h-dk-(0), NLe(θkj))i.
du 1 Il	∙ 1J d i 1 ∙	du(j)(t=0)	(λ	/ kuk2 ∖	d 1.	dα(j)(t=0)
Simple algebra yields that limk∞ -ɪdt-- = (1 — k ^∣2) ∙ u and limk∞ —kdt-- =
(/kuk2 — 1)∙ α. Thus, the limit dθ : = limk∞ dθdt--(0) is constant (independent of the index
j ) and
N	(j)	N
X λj) Jim 今(0) = (dθ, X λj) lim VLe(θj))J
k→∞ dt	k∞
j=1	j=1
'-------{---------}
=0
=0.
(j)
This is contradiction with the fact that Pj=ι λj) limk→∞ -g^ (0) < 0. Therefore, in the case u = 0
and α = 0, we must have that kuk2 = ∣α∣.
CASE 2:	u 6= 0
The proof proceeds exactly in the same way, except that we define
γ(j)(t) = 1 + t(γj) — 1) and we use the convention -⅛- = 0 if
k	k	γk (t)
where γ(j)
α(kj ) = 0.
Part 2 : Non-zero neurons which share the same activation cone are positively
COLINEAR
According to the first part of the proof, we can assume that the neural network θ is scaled.
16
Published as a conference paper at ICLR 2022
(SPECIAL CASE) THE NEURAL NETWORK θ IS A DIFFERENTIABLE POINT OF Lβ .
In order to provide some intuition about the proof, let us assume first that Lβ is differentiable at θ.
By contradiction, we suppose that there exist two non-zero neurons (u, α) and (v, β) such that
B(u, α) = B(v, β), and, u and v are not positively colinear. Further, let us assume that α, β > 0
(the case α, β < 0 follows the same lines).
Define w : = αu + βv. Note that w has the same sign pattern as u and v. For t ∈ [0, 1], we set
ue(t) :	=(1 — t)au + gw ,
ve(t) :	二(I - t)βv + 2W ,
u(t) :	=():,	a(t) := pke(t) k2 , pW⅛	,
v(t) :	=	e(t	,	β(t) ： = p∣∣e(t)∣∣2. PWB,
Note that B(u(t), α(t)) = B(u, α) = B(v, β) = B(v(t), β(t)). Further, we define θ(t) as a copy of
θ where we replace the two neurons (u, α) and (v, β) by (u(t), α(t)) and (v(t), β(t)). Note that θ(t)
defines a continuous path in Θm starting at θ.
Then, we introduce the two functions
g(t)： = '(b(θ(t))),
h(t) : = R(θ(t)),
so that Le(θ(t)) = g(t) + β ∙ h(t). First, We claim that g(t) is constant over [0,1]. Indeed, this
follows from the fact
σ(X u(t))α(t) + σ(Xv(t))β(t) = σ(X (ue(t) + ve(t))) = σ(Xu)α + σ(Xv)β ,
、-----------------------------------------{z---}
= αu+βv
Where the first equality comes from the fact that B(u(t), α(t)) = B(v(t), β(t)). Hence, We have
yb(θ(t)) = yb(θ) and g(t) is constant.
On the other hand, the function h(t) is clearly differentiable, and simple algebra yields that
dh(0)	kαuk2
---=--------
dt	2
粤 + 1(αu)>(βv)
kαuk2 + kβvk2
—
1
1
Since u and v are not colinear, it holds by Cauchy-SchWarz inequality that (αu)> (βv)
kαuk2kβvk2, and thus,
dh(O) <	kαuk2 _	kβvk2 +	kauk2kβvk2	(	1	+	1 ʌ =	0
dt <	-	2	-	2	+	2	Uauk2 +	kβvk2J = ,
<
that is, dhdt0) < 0, Thus, We finally obtain that dLβdt"" < O, which contradicts the stationarity of θ.
(GENERAL CASE) THE NEURAL NETWORK θ IS NOT NECESSARILY A DIFFERENTIABLE POINT OF
Lβ,
NoW, let us generalize the above proof to the case Where Lβ is not necessarily differentiable at θ,
For a vector z ∈ Rn, We use the notations I+(z) : = {i ∈ {1, . . . , n} | zi > 0}, I0(z) : = {i ∈
{1, . . . , n} | zi = 0} and I-(z) : = {i ∈ {1, . . . , n} | zi < 0},
Since θ is a Clarke stationary point of Lβ, We knoW that there exist λ(1), . . . , λ(N) > 0 and sequences
{θk(1)}k, . . . , {θk(N)}k such that PjN=1 λ(j) = 1, limk→∞ θk(j) = θ for each j = 1, . . . , N, the loss
function Lβ is differentiable at each θk(j ) and
N
0 = X λ⑶ lim VLe(θj)).
k→∞	k
j=1
17
Published as a conference paper at ICLR 2022
For each k > 1 and j = 1, . . . , N, up to extracting subsequences, we can assume that
u(kj) , α(kj) , vk(j) , βk(j) 6= 0, and, α(kj) and βk(j) have the same sign (let us say positive). Further,
up to extracting subsequences again, we can assume that the sign patterns I+(Xu(kj)) and I- (Xu(kj)))
(resp. I+(Xvk(j)) and I- (Xvk(j)))) remain constant (independent of k). Since the sign patterns of
Xu and Xv are equal by assumption, and, since limk∞ u(kj) = u and limk∞ vk(j) = v, it follows that
I+(Xu) = I+(Xv) ⊂ {I+ (Xu(kj)) ∩ I+(Xvk(j))},	(18)
and
I-(Xu) = I-(Xv) ⊂ {I-(Xu(kj)) ∩ I-(Xvk(j))}.	(19)
We denote T(j) (u) and T (j)(v) the diagonal matrices (as introduced in Section 2) which correspond
to the sign patterns of u(kj) and vk(j), and which are independent of k by assumption. Then, using (18)
and (19), it follows that
T(j)(u)Xu=T(j)(v)Xv,	T(j)(u)Xu=T(j)(v)Xv.	(20)
The above equalities will be crucial later on in our analysis.
Then, for each neural network θk(j), we can construct a similar path θk(j) (t) as in the differentiable
case, that is, we set wk(j) : = α(kj)u(kj) + βk(j)vk(j), and
ekj)(t) := (I-t)αukj)+ twkj),
ekj)⑴ :=(I-t)βkj)vkj)+ twkj),
Ukj)⑴ :=	：u(, αkj)⑴ :=qkekj)(t)k2,
kue(kj)(t)k2
Vkj)⑴: =	/ vk,t)	,	βkj) ⑴:=qkekj) ⑴心.
kvek(j)(t)k2
Similarly to the differentiable case, we also define the functions
gkj)(t): = '(b(θkj)(t))),
h(kj)(t) : = R(θk(j)(t)).
dg(j) (0)
First, We claim that limk→∞ d = 0. Indeed, We have
dg⅛t(0) = 1 D(T⑺(U)- T(j)(v))X(Vkj)- Ukj)), V'(b(θkj")》 .
Taking the limit k → ∞, We obtain that
Iim dgj(0) = 1 D(T(j)(u) - Tj)(V))X(V-U), V'(b(θ)).
k→∞	dt	2
Using (20), We get that (T (j)(U) - T (j)(V))X(V - U) = 0, and consequently, the claimed equality
limk→∞ dgjt⑼=0.
On the other hand, the function h(kj) (t) is clearly differentiable, and simple algebra yields that
lim
k→∞
dhkj)(O)
dt
αUk2
WkZ + ∣(αu)>(βv)
kɑuk2 + kβvk2
—
k
1
1
2
—
Since U and V are not colinear, it holds by Cauchy-SchWarz inequality that (αU)> (βV)
kαUk2kβVk2, and thus,
<
dhkj)(0) <	kαuk2	_ 包 +	kαuk2kβv∣∣2 (_J_ + ɪA	=	0
k→∞ dt <	2	2	+	2 IkaUk2 + kβv∣∣2 J	=
18
Published as a conference paper at ICLR 2022
that is, limk→∞
dhj)(0)
dt
< 0. Thus, we finally obtain that
lim dLβ(θkj)(0))
k→∞	dt
<0,
and further, that
Xλ(j) lim dLβ(Bj)((O))
k→∞	dt
j=1
<0.
However, it holds that
lim dLβ(θkj)(0))
k→∞	dt
lim h
k→∞
dθj)(0)
dt
,VLβ(θkj))i
dθ(j) (0)
It is immediate to see that dθ : = limk∞ kɪʌ2 does not depend on the index j, so that
XXλj) lim dLβ(θF⑼)=hdθ, XXλj) lim VLe(θj).
k→∞	dt	k→∞	β k
j=1	j=1
'----------{z-----------}
=0
That is, we obtained both that Pj= λ(j) limk→∞ dLβ(dkt (O))	<	0 and
pN=ι λ(j) limk→∞ dLβ(d (O)) = 0, which is a contradiction. This concludes the proof
that θ must be a nearly minimal neural network.
B.5 Proof of Proposition 3: Reduction to Nearly Minimal Neural Networks
along a Path of Decreasing Objective Value
We consider reductions similar to those in the proof of Theorem 2, in order to construct a path
θ(t) ∈ Θm for t ∈ [0, 1] such that θ(0) = θ, θ(1) ∈ Θe mmin and Lβ(θ(t)) is strictly decreasing.
Naturally, we assume that θ is not nearly minimal, otherwise, there is nothing to show.
PART 1 : THE NEURAL NETWORK θ IS UNSCALED.
(
(
u(t)
α(t)
We claim that there exists a path θ(t) ∈ Θm for t ∈ [0, 1] such that θ(0) = θ, θ(1) is scaled and
Lβ (θ(t)) is strictly decreasing.
Suppose that the neural network is unscaled (if not, go directly to Part 2). Then, for each neuron
(u, α) of θ such that ku∣∣2 = ∣α∣, define
p∕lal ∙ YUt) if u,α = 0,
0 otherwise,
Y(t) ∙√= if α = 0,
√a
0 otherwise,
where γ(t) = -\/|0| 十 t(/ku∣∣2 一 a∕∣O∣). Simple algebra yields that (u(0), α(0)) = (u, α) and
ku ⑴ k2 = VZwUB =∣α(1)∣, so that θ(0) = θ and θ(1) is scaled. By positive homogeneity of σ,
we have that σ(Xu(t))α(t) = σ(Xu)α, which further implies that yb(θ(t)) = yb(θ) and L(yb(θ(t)))
is constant as a function of t.
We claim that the regularization term R(θ(t)) is strictly decreasing as a function oft. Indeed, it holds
that
ku⑴k2 +|a(t)|2 = JaA I∣uk2 + γ∣-(r)|a|2.
Y (t)	|a|
The minimizer of the function Y ∈ R ∣一~→ ∙Y⅛ ∣∣u∣2 + Y2∣ |a|2 is given by Y * = ∕∣u∣2, which is also
equal to γ(1), and the minimal value of the latter function is given by 21u12|a|, which is strictly
smaller than ∣∣u∣2 + |a|2 since ∣∣u∣2 = |a|. Thus, the function t → R(θ(t)) is minimized at t = 1,
and R(θ(1)) < R(θ). Lastly, observe that t 7→ R(θ(t)) is a convex function, which implies that it
must be strictly decreasing over [0, 1]. This concludes the first part of the proof.
19
Published as a conference paper at ICLR 2022
PART 2 : THE NEURAL NETWORK θ IS SCALED BUT NOT NEARLY MINIMAL.
If the neural network θ is scaled but not nearly minimal, we claim that there exists a continuous path
θ(t) for t ∈ [0, 1] such that θ(0) = θ, θ(1) is nearly minimal, and Lβ(θ(t)) is strictly decreasing.
For each cone B ∈ {B1, . . . , B2q}, we consider the non-zero neurons UB : = {(u, α)} of θ such
that B(u, α) = B. By assumption, there exists at least one cone B such that UB has more than two
elements which are not positively colinear. Then, for each cone B, We set W = P(U a)∈uβ ∣α∣u, and,
for each (u, α) ∈ UB and for t ∈ [0, 1],
u(t) : = (1 — t)∣α∣u + ∣u~।w,
u(t)：=sign(α) ∙ p8⅛,
α(t) := sign(α) ∙ VZPw2,
where |UB | is the cardinality of the set UB . Note that B(u(t), α(t)) = B(u, α) = B, and each
neuron (u(t), α(t)) is scaled. Further, we define θ(t) the neural network with neurons (u(t), α(t)).
It holds that θ(t) defines a continuous path in Θm starting at θ, and ending at a nearly minimal neural
network. Then, we introduce the two function g(t) = L(yb(θ(t))) and h(t) : = R(θ(t)), so that
Le(θ(t)) = g(t) + β ∙ h(t). First, we claim that g(t) is constant over [0,1]. Indeed, this comes from
the fact that for each cone B ,
^X σ(Xu(t))α(t) = sign(a) ∙ σ(X ∙	^X	∣α(t)∣u(t))
(u,α)∈UB
(u,α)∈UB
'----------
=w
}
=sign(α) ∙ σ(Xw)
=	σ(X u)α .
(u,α)∈UB
The first (resp. third) equality holds from the fact that the neurons (u(t), α(t)) (resp. (u, α)) have the
same active cone B. Thus, yb(θ(t)) = yb(θ) and g(t) is indeed constant.
On the other hand, we claim that the function h(t) is strictly decreasing. Indeed, observe first that
h(t) = 2 X X	ku(t)k2 + ∣α(t)∣2
B (u,α)∈UB
=β X X ku⑴k2|a⑴1
B (u,α)∈UB
= βX X	kue(t)k2 ,
B (u,α)∈UB
where the second equality holds since the neurons (u(t), α(t)) are scaled. Thus, it is immediate to
verify that the function h is differentiable, and
h'(t)=β ∙ X(“X” bw> (t ∙k∣UBj ταluk2+|a|u>( ⅛ TaIu)).
Clearly, h0(t) is strictly increasing (since there exists, by assumption, at least one cone B and a
neuron (u, a) ∈ UB such that ∣Uwy = ∣a∣u). Therefore, it suffices to verify that h0(1) 6 0. Simple
algebra yields actually that h0(1) = 0, which concludes the proof.
B.6 Proof of Proposition 4
The proof with trichotomies is almost identical to the proof of dichotomies in Pilanci & Ergen (2020);
Sahiner et al. (2020). We start with the dual representation of P":
P * = max '*(λ), s.t. max	∣λτ(Xw)+∣ 6 β.
w:kwk261
20
Published as a conference paper at ICLR 2022
Here '*(λ) : = maxv{λTV - '(v)} is the Fenchel conjugate function of '. We note that the Single-
sided dual constraint has an equivalent formulation using trichotomies:
max λT (Xw)+
w:kwk261
= max max	λT (Tj)+Xw.
j∈[q] W"∣wk261,w∈Qj
Similarly, the other side of the dual constraint can be formulated as
max -λT (Xw)+
w:kwk261
max max	λT (Tj)+X(-w)
i∈[q] w"∣w∣∣261,w∈Qi
Therefore, We can rewrite P * as
P * =max '* (λ),
s.t.
max
wRw∣∣26i,w∈Qi
λT(Tj)+Xw 6 β, i ∈ [q],
max
wRw∣∣26i,w∈Qi
λT(Tj)+X(-w) 6β,j ∈ [q].
For simplicity, we denote Tj+q = Tj for j ∈ [q]. We now formulate the Lagrangian
q
P* = max min
λ ν>0
`* (λ) +	νj (β - λT (Tj )+Xwj )
j=1
min
wj ∈Qj,kwj k2 61
wj+q ∈Qj,kwj+q k261
q
+ X νj+q(β - λT (Tj )+X(-wj+q)).
j=1
By Sion’s minimax theorem, we can switch the max and min, and then minimize over λ. Following
this, we obtain
P* = min min `
νj>0 wj ∈Qj,kwj k2 61
wj+q∈Qi,kwj+q k261
(Ti)+X(νiwi - νj+qwj+q)
j=1
2q
+ β	νj .
j=1
By rescaling the variable wj = νj wj , we can reformulate P * as
(q	∖	2q
P* = min	min	`	(Tj)+X(wj - wj+q)	+β	νj.
νi>0	wj∈Qj,kwjk26νj	j=1	j=1
wj+q ∈Qj,kwj+q k2 6νj+q
Minimizing with respect to ν yields
(q	∖	2q
P* = min ` X(Tj)+X(νjwj - νj+qwj+q) + βX kwjk2.
wj,wj+q ∈Qj
j=1	j=1
This completes the proof.
B.7 Proof of Theorem 3
According to Proposition 1 and 2, we can assume that θ is a minimal neural network. Denote
λ = V' (Pj=I(Xuj)+α,. From the definition of Clarke,s stationary point, for j ∈ [m] with
uj 6= 0, we have
m
X(Xuj)+αj
—βαj = λτ(Xuj)+.
(21)
21
Published as a conference paper at ICLR 2022
The first line in (21) is equivalent to that there exists δj ∈ [0, 1]N such that
—βuj = αj (X T D j λ + X T Sjdiag(δj )λ).	(22)
Here DDj = diag(I(Xuj > 0)) and Sj = diag(I(Xuj = 0)). As Uj = 0 and aj∙ = 0, this implies
that
—βuj = XT DD jλ + XT Sj diag(δj )λ.	(23)
αj
For the second line in (21), we can also rewrite it as
T
—βαj =λ1 Dj Xuj
=UT X T D j λ
=UT(X T D j λ + X T Sj diag(δj )λ)	(24)
Therefore, We have Ilujk2 = ∣αj∙∣ and
∣∣XT D jλ + XT Sjdiag(δj )λ∣∣2 = 1.	(25)
For the subsampled convex program (13), the KKT conditions are given by: for i ∈ I, there exists
ζ(i)	0 and ξ(i) such that
XT ((Ti)+λ + TiZ ⑴ + Siξ⑴)+ β 4=0,
kwi k2
∣∣∣XT((Ti)+λ+Tiζ(i)+Siξ(i))∣∣∣	6β,
XT (—(Ti)+λ+TiZ(i) + sξ⑴)+βτwi+qr = 0,
kwi+q k2
∣∣XT(—(Ti)+λ + TiZ⑻ + Siξ(i))∣∣2 6 β,
ifwi 6=0,
wi = 0,
if wi+q 6= 0,
if wi+q = 0.
(26)
Here Si is a diagonal matrix satisfying that (Si)jj = 1 if j ∈ I0 and (Si)jj = 0 if
j ∈ I+ ∪ I-, Where {I+, I0, I-} is the i-th trichotomy. The vector λ ∈ RN is defined as
λ = V' (Pi∈z(Ti)+X(Wi — wi+q)). As θ is a minimal neural network, there exists a bijective map-
ping betWeen non-zero neurons (uj, αj) and i ∈ I. For i ∈ I, suppose that Ti = diag(sign(X uj)).
If αj > 0, we let
wi = αjuj , wi+q = 0,
otherwise, we let
Wi = 0, wi+q = —αjuj .
As the mapping between non-zero neurons (uj, αj ) and i ∈ I is bijective, we note that
Pjm=I(Xuj)+αj = Pi∈ιXTDi(wi — Wi+q). This implies that λ = λ. On the other hand, by
taking Z(i) = 0, ξ(i) = diag(δj)λ, Z(i+q) = 0, ξ(i+q) = —diag(δj-)λ, as Dj = (Ti)+ and Sj = Si,
the KKT conditions (26) are satisfied. Therefore, W = {Wi, Wi+q|i ∈ I} is a global optimum of the
subsampled convex program (13).
B.8 Proof of Proposition 5
Let θ ∈ Θm be a minimal neural network, and suppose that W(θ) satisfies the KKT conditions of the
optimization problem (4). Since the latter is a convex optimization problem, it follows that W (θ) is a
一一 一 _ 一 一 _ . 一 ______________________________________________~ . ___________~ . _ . _ .
global minimum. From Proposition 2, we have that P * 6 Le (θ(W (θ))) 6 Le (W (θ)) = P* = P *,
it follows that Le (θ(W (θe)) = P* and θ(W (θe)) is a global minimizer of Le, which yields the
claimed result.
22
Published as a conference paper at ICLR 2022
B.9 Proof of Proposition 6
Without loss of generality, we can assume that θ is scaled. Otherwise, we know from the proof
of Proposition 3 that θ can be reduced to a scaled neural network along a continuous path of
non-increasing training loss.
We follow the same steps as in the proof of Lemma 1. Denote the neurons of
m
θ by	(u1,	α1), . . .	,	(um, αm).	We have that	yb(θ)	= i=1	λi	zi,	where	zi	=
sign(ai)(Pm=IIIujlIIaj ∣)σ(x 岛)and Xi = PmuikjI % I .皿废，⑥⑻ ∈ Conv{zι,...,zm}.
From Lemma 3, we know that there exist i1 , . . . , in+1 and λ1 , . . . , λn+1 > 0 such that Pjn=+11 λj = 1
and yb(θ) = Pjn=+11 λj zij. Plugging-in the expressions of the zij, it follows that
n+1	m
b(θ) = XλjSign(Oj) (XMM%1) σ(χIuj-)
n+1
= Xαeijσ(Xueij),
j=1
where
fνj := kλ^J (Pm=I kuk k|ak |)，
Iej :=qkujk Uj
[Xj := sign(aj) -Xj-.
Further, we have that
n+1	n+1	n+1	m	m
X Iαeij I-ueij -= Xνj -uij-= Xλj(X -uk -IαkI) = X -uk -Iαk I ,
j=1	j=1	j=1	k=1	k=1
n+1
where the last equality follows from the fact that j=1 λj = 1. Setting θ the neural network with
neurons (ueij, αeij) for j = 1, . . . , n + 1 and (uei, αei) = (0, 0) for i ∈ {1, . . . , m} \ {i1, . . . , in+1},
~:

we obtain that θ ∈ Θm and Lβ (θ) 6 Lβ (θ).
Now, we define a continuous path between θ and θ, as follows. For t ∈ [0, 1], j = 1, . . . , n + 1 and
i ∈ {1, . . . , m} \ {i1, . . . ,in+1}, we set
U (t) =	(I — UUjIajI + tej|ej|
Uj	_ Pk(1- t)Uj|aj| + tej∣Xj∣k
aij (t) = sign(aij ) -Uij (t)-
Ui(t) = √1 t Ui
ai(t) = √1 一 t a ,
and θ(t) the neural network with neurons {(Ui(t), ai(t))}im=1. Note that θ(t) is scaled, and
m	n+1
X kUi(t)kIai(t)I = X k(1 - t)Uij Iaij I + t Ueij Iaeij Ik + X (1-t)kUikIaiI
i=1	j=1	i=1,...,n+1
i6=i1 ,...,in+1
n+1	n+1
= (1-t)XIaijIkUijk+(1-t) X	kUikIaiI+tXIaeijIkUijk
(i)
j=1	i=1,...,n+1	j=1
i6=i1 ,...,in+1
= (1 - t)R(θ) + t R(θe)
(ii)
= R(θ),
(iii)
23
Published as a conference paper at ICLR 2022
where equality (i) follows from the triangular inequality and the fact that ueij and uij are positively
colinear; equality (ii) follows from the fact that θ and θ are scaled; equality (iii) holds since R(θ) =
R(θ). Thus, the function t 7→ R(θ(t)) is constant over [0, 1].
On the other hand, we have
n+1
b(θ(t)) = X σ(X((1 - t)uij∣αj∣ + tuij |eij |)) sign(αj) + (1 - t) X	σ(Xui)a
j=1	i=1,...,n+1
i6=i1 ,...,in+1
n+1	n+1
= (1 - t)	σ (Xuij )αij + t	σ(X ueij )αeij + (1 - t)	σ(Xui)αi
(i)	j=1	j=1	i=1,...,n+1
i6=i1 ,...,in+1
= (1-t)yb(θ)+tyb(θe)
= yb(θ) ,
(ii)
where equality (i) holds since the uij and ueij are positively colinear and the αij and αeij have same
signs; equality (ii) holds since yb(θ) = yb(θ). Consequently, the function t 7→ Lβ (θ(t)) is constant
「C TI	Λ .1 •	1	1	.1	CC,1 C,,l,八一:
over [0, 1], and this concludes the proof of the fact that θ I θ.
B.10 Proof of Proposition 7
First, according to Proposition 6, given θ ∈ Θm with m > n +1 + m*, there exists a neural network
θ with at most n + 1 non-zero neurons such that Lβ(θ) 6 Lβ (θ).
According to Lemma 1, there exists θ* = {(u*, α*)}m=ι an optimal neural network with at most m*
non-zero neurons. Up to a permutation of the zero neurons of θ and those of θ*, since m > n+1+m*,
we can assume without loss of generality that (ui*, αi*) = (0, 0) for i = m* + 1, . . . , m and
(uei , αei ) = (0, 0) for i = 1, . . . , m* .
Now, we define a continuous path between θ and θ*. For i = 1, . . . , m* and j = m* + 1, . . . , m, we
set the neural network θ(t) ∈ Θm with neurons
Ui(t) = √tu*,
αi(t) = √t α*,
Uj (t) = ʌ/l — t Uj,
Qj (t) = λ∕l — t Qj .
d 1	1	八/c、	∕Γ 1 八/r、	八士 1 -	.1	八/,\ ♦	1	1	Λ ∙ . ∙	∙1	∙ r' -I .1
Clearly, we have θ(0) = θ and θ(1) = θ*. Further, θ(t) is scaled and it is easily verified that
... ...
R(θ(t)) =tR(θ*) + (1 - t) R(θe),
yb(θ(t)) =tyb(θ*)+(1-t)yb(θe).
This immediately implies that the function t 7→ Lβ (θ(t)) is convex over [0, 1]. Since it achieves a
minimum at t = 1, it follows that it is non-increasing, and this concludes the proof of Proposition 7.
C Proofs of intermediate results
C.1 Proof of Lemma 2
Proof. It holds that i∈I σ(Xui)Qi =	i∈I γiσ(Xwi) =	i∈I DiXwi, whence
'(Pi∈ι σ(Xui)αi) = '(Pi∈ι DiXwi) On the other hand, we have Ilwijk2 = IlUj ∣∣2∣αj∙ |. Note
that ∣∣ujk2 = ∣αj| and thus, kujk2∣αj∙| = ɪ(∣∣ujk2 + ∣αj∙|2). Consequently, Le(θ) = Lec(w*).
From Pilanci & Ergen (2020), We know that P * = P*. Hence, Le (θ) = P *.	□
24
Published as a conference paper at ICLR 2022
C.2 Proof of Lemma 1
We aim to show that m* 6 n + 1 and Pm for any m > m*. We leverage the following result which
is known as Caratheodory’s theorem.
Lemma 3. Let z1, . . . , zm ∈ Rn. Suppose that y ∈ Conv{z1, . . . , zm}. Then, there exist indices
i1, . . . , in+1 ∈ {1, . . . , m} such that y ∈ Conv{zi1 , . . . , zin+1 }.
Suppose that θ is an optimal neural network with m > n + 1 neurons, and denote
m
its neurons by	(u1, α1),	. . . ,	(um,	αm).	We have that	yb(θ)	=	i=1 λi zi,	where	zi	=
sign(αi)(Pm=IkujkIaj ∣)σ(x 品)and Xi = Pmuikua 鼠 I .Thus, y(θ) ∈ Conv{zι,...,zm}.
From Lemma 3, we know that there exist i1 , . . . , in+1 and λ1 , . . . , λn+1 > 0 such that Pjn=+11 λj = 1
and yb(θ) = Pjn=+11 λj zij. Plugging-in the expressions of the zij, it follows that
(X ⅛)
n+1	m
b(θ) = X n Signgj)(X Iiuj k|aj l)σ
j=1	j=1
n+1
= Xαeijσ(Xueij),
j=1
where
νj := kjk (Pm=Ikujkla∕)
ej :=qkjk uj
αeij : = sign(αij) kueij k
Further, we have that
n+1	n+1	n+1	m	m
X |ej|kejk = X Vjkujk = X λj (X kuk ll|ak |) = X kukklak |,
j=1	j=1	j=1	k=1	k=1
where the last equality follows from the fact that Pjn=+11 λj .
We define the neural network θ with neurons (uj,aj). We have that θ ∈ Θn+ι and P^+ι 6
Le(θ) = Le (θ) = Pm. Since P^+ι > Pm for any m > n +1, it follows from the previous set of
... 一 一 ，∙^r.	_,	_, 一一.一 一一-	一 _	_,	_.
inequalities that Le(θ) = P^+ι = Pm, and this holds for any m > n + 1. Therefore, P^+ι = P*
and Le (θ) = P *.
__,≈. 一	. . 一 一 ____,≈.	. . .	一 一一 /m、 一 ~
We set W = W (θ). We know from Proposition 2 that W(θ) ∈ Wn+1 and Lce(W) 6 Le(θ). Hence,
_, _. . 一 一 ，一 . 一 _ 一 . . 一. 一 _ , _, 一 .
Pc* 6 P*. We also know that Le (θ(W)) 6 Lce (W). This implies that Pc* = P* and W is an
optimal solution to (4). Consequently, m* 6 n + 1.
It remains to show that for any m > m*, we have Pm* = P*. This follows again from Proposition 2.
Indeed, let W * be an optimal solution to (4) such that W * ∈ Wm,*. Set θ* = θ(v*). We know that
θ* ∈ Θm*, and Le (θ*) 6 Le (W *) = Pc = P *. Hence, θ* achieves P * and this implies that for
any m > m*, we have Pm* = P*.
D Verification of the optimal set
We review a standard method to determine whether a convex optimization problem has unique
solution. Consider a convex optimization problem
min f (x), s.t. fi(x) 6 0, i ∈ [m],	(27)
in the variable x ∈ Rd. Here f and fi for i ∈ [m] are convex functions. Suppose that we calculate
one optimal solution x* and the corresponding optimal value f*. We can determine whether x* is the
25
Published as a conference paper at ICLR 2022
unique optimal solution of (27) as follows. For j ∈ [d], consider the following convex optimization
problems
Pjb = minXj, s.t. fi(x) 6 0,i ∈ [m], f (x) 6 f *,	(28)
Pub = maxxj, s.t. fi(x) 6 0,i ∈ [m],f(x) 6 f*∙	(29)
These problems give the upper bound and the lower bound of the value of the i-th index in the optimal
set of (27). Suppose that Pjub - Pljb 6 for certain small > 0, for instance, = 10-8. Then, the
radius of the optimal set with respect to the '∞ norm is upper-bounded by U Therefore, We can be
confident that x* is the unique optimal solution up to numerical tolerance.
We have verified numerically that the convex optimization problem in Example 1 in section 3.1 has a
unique optimal solution.
26