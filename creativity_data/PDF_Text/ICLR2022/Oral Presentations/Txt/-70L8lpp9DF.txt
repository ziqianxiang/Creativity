Published as a conference paper at ICLR 2022
Hyperparameter Tuning with
Renyi Differential Privacy
Nicolas Papernot*
Google Research, Brain Team
papernot@google.com
Thomas Steinke*
Google Research, Brain Team
hyper@thomas-steinke.net
Ab stract
For many differentially private algorithms, such as the prominent noisy stochastic
gradient descent (DP-SGD), the analysis needed to bound the privacy leakage
of a single training run is well understood. However, few studies have reasoned
about the privacy leakage resulting from the multiple training runs needed to fine
tune the value of the training algorithm’s hyperparameters. In this work, we first
illustrate how simply setting hyperparameters based on non-private training runs
can leak private information. Motivated by this observation, we then provide
privacy guarantees for hyperparameter search procedures within the framework of
Renyi Differential Privacy. Our results improve and extend the work of Liu and
Talwar (STOC 2019). Our analysis supports our previous observation that tuning
hyperparameters does indeed leak private information, but we prove that, under
certain assumptions, this leakage is modest, as long as each candidate training run
needed to select hyperparameters is itself differentially private.
1	Introduction
Machine learning (ML) systems memorize training data and regurgitate excerpts from it when
probed (Carlini et al., 2020). If the training data includes sensitive personal information, then this
presents an unacceptable privacy risk (Shokri et al., 2017). It may however still be useful to apply
machine learning to such data, e.g., in the case of healthcare (Kourou et al., 2015; Wiens et al., 2019).
This has led to a significant body of research on the development of privacy-preserving machine
learning methods. Differential privacy (DP) (Dwork et al., 2006b;a) provides a robust and quantitative
privacy guarantee. It has been widely accepted as the best framework for formally reasoning about
the privacy guarantees of a machine learning algorithm.
A popular method for ensuring DP is noisy (stochastic) gradient descent (a.k.a. DP-SGD) (Song
et al., 2013; Bassily et al., 2014; Abadi et al., 2016). DP-SGD differs from standard (stochastic)
gradient descent in three ways. First, gradients are computed on a per-example basis rather than
directly averaged across a minibatch of training examples. Second, each of these individual gradients
is clipped to ensure its 2-norm is bounded. Third, Gaussian noise is added to the gradients as they are
averaged and applied to update model parameters. These modifications bound the sensitivity of each
update so that the added noise ensures differential privacy. The composition (Dwork et al., 2010) and
privacy amplification by subsampling (Balle et al., 2018) properties of differential privacy thus imply
that the overall training procedure is differentially private. We can compute tight privacy loss bounds
for DP-SGD using techniques like the Moments Accountant (Abadi et al., 2016) or the closely related
framework of Renyi DP (Mironov, 2017; Mironov et al., 2019).
Machine learning systems have hyperparameters, such as the learning rate, minibatch size, or
choice of a regularizer to prevent overfitting. Details of the model architecture can also be treated
as hyperparameters of the optimization problem. Furthermore, learning within the constraints of
differential privacy may introduce additional hyperparameters, as illustrated in the DP-SGD optimizer
by the 2-norm bound value for clipping, the scale of the Gaussian noise, and the choice of stopping
time. Typically the training procedure is repeated many times with different hyperparameter settings
in order to select the best setting, an operation known as hyperparameter tuning. This methodology
implies that even if each run of the training procedure is privacy-preserving, we need to take into
* Alphabetical author order.
1
Published as a conference paper at ICLR 2022
account the fact that the training procedure is repeated (possibly many times) when reasoning about
the privacy of the overall learning procedure.
Can the tuning of hyperparameters reveal private information? This question has received remarkably
little attention and, in practice, it is often ignored entirely. We study this question and provide both
positive and negative answers.
1.1	Our Contributions
•	We show that, under certain circumstances, the setting of hyperparamters can leak
private information. Hyperparameters are a narrow channel for private information to leak
through, but they can still reveal information about individuals ifwe are careless. Specifically,
if we tune the hyperparameters in an entirely non-private fashion, then individual outliers
can noticeably skew the optimal hyperparameter settings. This is sufficient to reveal the
presence or absence of these outliers 鱼 la membership inference (Shokri et al., 2017); it
shows that we must exercise care when setting hyperparameters.
•	We provide tools for ensuring that the selection of hyperparameters is differentially
private. Specifically, if we repeat the training procedure multiple times (with different
hyperparameters) and each repetition of the training procedure is differentially private on its
own, then outputting the best repetition is differentially private. Of course, a basic version
of such a result follows from the composition properties of differential privacy (that is the
fact that one can “sum” the privacy loss bounds of multiple differentially private analyses
performed on the same data to bound the overall privacy loss from analyzing this data).
However, we provide quantitatively sharper bounds.
Specifically, our privacy loss bounds are either independent of the number of repetitions or
grow logarithmically in the number of repetitions, whereas composition would give linear
bounds. Rather than repeating the training procedure a fixed number of times, our results
require repeating the training procedure a random number of times. The privacy guarantees
depend on the distribution of the number of runs; we consider several distributions and
provide generic results. We discover a tradeoff between the privacy parameters and how
heavy-tailed the distribution of the number of repetitions is.
1.2	Background and Related Work
Differential privacy (DP) is a framework to reason about the privacy guarantees of randomized
algorithms which analyze data (Dwork et al., 2006b;a). An algorithm is said to be DP if its outputs
on any pair of datasets that only differ on one individual’s data are indistinguishable. A bound on
this indistinguishability serves as the quantification for privacy. Formally, a randomized algorithm
M : Xn → Y is (ε, δ)-DP if for any inputs x, x0 ∈ Xn differing only on the addition, removal, or
replacement of one individual’s records and for any subset of outputs S ⊆ Y, we have
P[M(x) ∈ S] ≤ eεP[M(x0) ∈ S] +δ.	(1)
Here, the parameter ε is known as the privacy loss bound - the smaller ε is, the stronger the privacy
guarantee provided is, because it is hard for an adversary to distinguish the outputs of the algorithm
on two adjacent inputs. The parameter δ is essentially the probability that the guarantee fails to hold.
One of the key properties of DP is that it composes: running multiple independent DP algorithms is
also DP and composition theorems allow us to bound the privacy parameters of such a sequence of
mechanisms in terms of the individual mechanisms’ privacy parameters (Dwork & Roth, 2014).
There is a vast literature on differential privacy in machine learning. A popular tool is the DP-SGD
optimizer (Abadi et al., 2016). Because the noise added is Gaussian and DP-SGD applies the same
(differentially private) training step sequentially, it is easier to reason about its privacy guarantees
in the framework OfRenyi differential privacy (Mironov, 2017). Renyi differential privacy (RDP)
generalizes pure differential privacy (with δ = 0) as follows. An algorithm M is said to be (λ, ε)-RDP
with λ ≥ 1 and ε ≥ 0, if for any adjacent inputs x, x0
DI(M(X)kM(XO)) = λ⅛ogY_M J(PM(Z=Y]])”# ≤ J	⑵
where Dλ (P kQ) is the Renyi divergence of order λ between distributions P and Q. In the framework
of RDP, one obtains sharp and simple composition: If each individual mechanism Mi is (λ, εi)-RDP,
2
Published as a conference paper at ICLR 2022
then the composition of running all of the mechanisms on the data satisfies (λ, i εi)-RDP. For
instance, the privacy analysis of DP-SGD first analyzes the individual training steps then applies
composition. Note that it is common to keep track of multiple orders λ in the analysis. Thus ε should
be thought of as a function ε(λ), rather than a single number. In many cases, such as Gaussian noise
addition, this is a linear function - i.e., ε(λ) = P ∙ λ for some P ∈ R - and such a linear bound yields
the definition of zero-Concentrated DP with parameter ρ (ρ-zCDP) (Bun & Steinke, 2016).
One could naively extend this composition-based approach to analyze the privacy of a training
algorithm which involves hyperparameter tuning. Indeed, if each training run performed to evaluate
one candidate set of hyperparameter values is DP, the overall procedure is also DP by composition
over all the hyperparameter values tried. However, this would lead to very loose guarantees of privacy.
Chaudhuri & Vinterbo (2013) were the first to obtain improved DP bounds for hyperparameter tuning,
but their results require a stability property of the learning algorithm. The only prior work that has
attempted to obtain tighter guarantees for DP hyperparameter tuning in a black-box fashion is the
work of Liu & Talwar (2019). Their work is the starting point for ours.
Liu & Talwar (2019) show that, if we start with a (ε, 0)-DP algorithm, repeatedly run it a random
number of times following a geometric distribution, and finally return the best output produced
by these runs, then this system satisfies (3ε, 0)-differential privacy.1 Liu & Talwar (2019) also
consider algorithms satisfying (ε, δ)-DP for δ > 0. However, their analysis is restricted to the (ε, δ)
formulation of DP and they do not give any results for Renyi Dp This makes it difficult to apply
these results to modern DP machine learning systems, such as models trained with DP-SGD.
Our results directly improve on the results of Liu & Talwar (2019). We show that replacing the
geometric distribution on the number of repetitions in their result with the logarithmic distribution
yields (2ε, 0)-differential privacy as the final result. We also consider other distributions on the
number of repetitions, which give a spectrum of results. We simultaneously extend these results to
the Renyi DP framework, which yields sharper privacy analyses.
Independently Mohapatra et al. (2021) study adaptive hyperparameter tuning under DP with compo-
sition. In contrast, our results are for non-adaptive hyperparameter tuning, i.e., “random search.”
A closely related line of work is on the problem of private selection. Well-known algorithms for
private selection include the exponential mechanism (McSherry & Talwar, 2007) and the sparse
vector technique (Dwork et al., 2009; Zhu & Wang, 2020). However, this line of work assumes that
there is a low-sensitivity function determining the quality of each of the options. This is usually not
the case for hyperparameters. Our results simply treat the ML algorithm as a black box; we only
assume that its output is private and make no assumptions about how that output was generated. Our
results also permit returning the entire trained model along with the selected hyperparameters.
2	Motivation
A hyperparameter typically takes categorical values (e.g., the choice of activation function in a neural
network layer), or is a single number (e.g., a real number for the learning rate or an integer for the
number of epochs). Thus, it is intuitive that a hyperparameter provides little capacity as a channel
to leak private information from the training data. Nevertheless, leakage can happen, in particular
when training is done without preserving privacy. We illustrate how with the constructed example of
hyperparameter tuning for a support vector machine learning (SVM) learned from a synthetic data
distribution. We consider a SVM with a soft margin; we use stochastic gradient descent to minimize
the corresponding objective involving the hinge loss and a weight penalty:
lw,b(x,y) = ∣HI2 + a max{0,1 - y(w ∙ x + b)}
where y ∈ {-1, 1} indicates the label of training example x ∈ R2 . Because our purpose here is
to illustrate how leakage of private information can arise from hyperparameter tuning, we work
with a synthetic data distribution for simplicity of exposition: we draw 40 inputs from isotropic 2D
Gaussians of standard deviation 1.0 to form the training set D. The negative class is sampled from a
Gaussian centered at μ-ι = (7.86, -3.36) and the positive at μι = (6.42, -9.17).
Our learning procedure has a single hyperparameter α controlling how much importance is given to
the hinge loss, i.e., how much the SVM is penalized for using slack variables to misclassify some of
1Liu & Talwar (2019) prove several other results with slightly different formulations of the problem, but this
result is representative and is most relevant to our discussion here.
3
Published as a conference paper at ICLR 2022
the training data. We first tune the value of α with the training set D and report training accuracy as
a function of α in Figure 1. Next, we repeat this experiment on a dataset D0 to which we added 8
outliers xo = (7.9, -8.0) to the negative class. The resulting hyperparameter tuning curve is added to
Figure 1. By comparing both curves, it is clear that the choice of hyperparameter α which maximizes
accuracy differs in the two settings: the best performance is achieved around α = 8 with outliers
whereas increasing α is detrimental to performance without outliers.
This difference can be exploited to perform a
variant of a membership inference attack (Shokri
et al., 2017): Here, one could infer from the
value of the hyperparameter α whether or not
the outlier points xo were part of the training
set or not. While the example is constructed,
this shows how we must be careful when tuning
hyperparameters: in corner cases such as the
one presented here, it is possible for some infor-
mation contained in the training data to leak in
hyperparameter choices.
Regularization parameter a
Figure 1: Accuracy of the model as a function
of the regularization weight α, with and without
outliers. Note how the model performance exhibits
a turning point with outliers whereas increasing
In particular, this implies that the common prac-
tice of tuning hyperparameters without differen-
tial privacy and then using the hyperparameter
values selected to repeat training one last time
with differential privacy is not ideal. In Sec-
tion 3, we will in particular show how training the value of α is detrimental without outliers.
with differential privacy when performing the
different runs necessary to tune the hyperparameter can bound such leakage effectively if one carefully
chooses the number of runs hyperparameters are tuned for.
3	Our Positive Results
3.1	Problem Formulation
We begin by appropriately formalizing the problem of differentially private hyperparameter tuning,
following the framework of Liu & Talwar (2019). Suppose we have m randomized base algorithms
M1,M2,…，Mm : Xn → Y. These correspond to m possible settings of the hyperparameters.
Ideally we would simply run each of these algorithms once and return the best outcome. For
simplicity, we consider a finite set of hyperparameter possibilities; if the hyperparameters of interest
are continuous, then we must pick a finite subset to search over (which is in practice sufficient).
Here we make two simplifying assumptions: First, we assume that there is a total order on the
range Y, which ensures that "best" is well-defined. In particular, we are implicitly assuming that the
algorithm computes a quality score (e.g., accuracy on a test set) for the trained model it produces;
this may require allocating some privacy budget to this evaluation.2 Second, we are assuming that the
output includes both the trained model and the corresponding hyperparameter values (i.e., the output
of Mj includes the index j).3 These assumptions can be made without loss of generality.
3.2	Strawman approach: repeat the base algorithm a fixed number of times
The obvious approach to this problem would be to run each algorithm once and to return the best of
the m outcomes. From composition, we know that the privacy cost grows at most linearly with m. It
turns out that this is in fact tight. There exists a (ε, 0)-DP algorithm such that if we repeatedly run
it m times and return the best output, the resultant procedure is not (mε - τ, 0)-DP for any τ > 0.
This was observed by Liu & Talwar (2019, Appendix B) and we provide an analysis in Appendix D.1.
This negative result also extends to R6nyi DP To avoid this problem, we will run the base algorithms
a random number of times. The added uncertainty significantly enhances privacy. However, we must
carefully choose this random distribution and analyze it.
2If the trained model’s quality is evaluated on the training set, then we must increase the privacy loss budget
to account for this composition. However, if the model is evaluated on a held out set, then the privacy budget
need not be split; these data points are “fresh” from a privacy perspective.
3 Note that in the more well-studied problem of selection, usually we would only output the hyperparameter
values (i.e., just the index j) and not the corresponding trained model nor its quality score.
4
Published as a conference paper at ICLR 2022
3.3	Our algorithm for hyperparameter tuning
To obtain good privacy bounds, we must run the base algorithms a random number of times. We
remark that random search rather than grid search is often performed in practice (Bergstra & Bengio,
2012), so this is not a significant change in methodology. Specifically, we pick a total number of runs
K from some distribution. Then, for each run k = 1,2,…，K, we pick an index jk ∈ [m] uniformly
at random and run Mjk . Then, at the end, we return the best of the K outcomes.
The privacy guarantee of this overall system then depends on the privacy guarantees of each of the
mechanisms Mj as well as the distribution of the number of runs K. Specifically, we assume that
there exists a uniform (Renyi) DP bound for all of the mechanisms Mj. Note that DP is “convex”
where “convexity” here means that if M1,M2,…，Mm are each individually DP, then running Mjk
for a random jk ∈ [m] is also DP with the same parameters.
To simplify notation, we also assume that there is a single mechanism Q : Xn → Y that picks a
random index j ∈ [m] and then runs Mj. In essence, our goal is to “boost” the success probability
of Q by repeating it many times. The distribution of the number of runs K must be chosen to both
ensure good privacy guarantees and to ensure that the system is likely to pick a good setting of
hyperparameters. Also, the overall runtime of the system depends on K and we want the runtime to
be efficient and predictable. Our results consider several distributions on the number of repetitions K
and the ensuing tradeoff between these three considerations.
3.4	Main Results
There are many possible distributions for the number of repetitions K. In this section, we first
consider two - the truncated negative binomial and Poisson distributions - and state our main privacy
results for these distributions. Later, in Section 3.5, we state a more general technical lemma which
applies to any distribution on the number of repetitions K.
Definition 1 (Truncated Negative Binomial Distribution). Let γ ∈ (0, 1) and η ∈ (-1, ∞). Define a
distribution D*γ on N = {1,2, ∙…} asfollows. If η = 0 and K is drawn from D*γ ,then
∀k ∈ N P [K = k] = (⅛⅛ ∙ ∏ ('+ι)	⑶
γ	'=o ∖ I /
and E [K]=普：—γ)). If K is drawnfrom D0,7 ,then
(1 - γ )k
P [K = k]=J∣ 7、	(4)
k ∙log(1∕γ)
and E[K] = //Y-∖.
L 」log(1∕γ)
This is called the “negative binomial distribution,” since Qk-(I (室)=(k+η-1) if we extend the
definition of binomial coefficients to non-integer η. The distribution is called “truncated” because
P [K = 0] = 0, whereas the standard negative binomial distribution includes 0 in its support. The
η = 0 case D(,γ is known as the “logarithmic distribution”. The η = 1 case D1,γ is simply the
geometric distribution. Next, we state our main privacy result for this distribution.
Theorem 2 (Main Privacy Result - Truncated Negative Binomial). Let Q : Xn → Y be a randomized
algorithm satisfying (λ, ε)-RDP and (λ, ε)-RDP for some ε,ε ≥ 0, λ ∈ (1, ∞), and λ ∈ [1, ∞).4
Assume Y is totally ordered.
Let η ∈ (-1, ∞) and γ ∈ (0, 1). Define an algorithm A : Xn → Y as follows. Draw K from the
truncated negative binomial distribution Dη,γ (Definition 1). Run Q(x) repeatedly K times. Then
A(x) returns the best value from the K runs.
Then A satisfies (λ, ε0)-RDP where
ε0 =ε + (1 + η) ∙ (1 - 1) ε + ~	~ ʌ ~ + — I ʌ ^∑ ^^.	(5)
λ	λ	λ-1
4
4If λ = 1, then ε corresponds to the KL divergence; see Definition 9. However, ε IS multiplied by 1-1∕λ = 0
in this case, so is irrelevant.
5
Published as a conference paper at ICLR 2022
6 5 4 3 2 1
≡⅛issd- ωucωσfc>≡ 一Au*
0	5	10	15	20	25	30
Renyi order 储)
Figure 2: Renyi DP guarantees
from Corollary 4 for various ex-
pected numbers of repetitions of
the logarithmic distribution (i.e.,
truncated negative binomial with
η = 0), compared with base al-
gorithm (0.1-ZCDP) and naive
composition.
/ 6 5 4 3 2 1
≡⅛issd- ωucωσfc>≡ 一Au*
O IO 20	30	40	50
Renyi order (A)
Figure 3: Renyi DP guarantees
for repetition using different dis-
tributions or naive composition
with mean 10, with 0.1-zCDP
base algorithm.
Figure 4: Privacy versus ex-
pected number of repetitions
using different distributions or
naive composition. Renyi DP
guarantees are converted to ap-
proximate DP 一 i.e., We plot ε
such that we attain (ε, 10-6)-DP.
The base algorithm is 0.1-zCDP.
Theorem 2 shoWs a tradeoff betWeen privacy and utility for the distribution Dη,γ of the number of
repetitions. Privacy improves as η decreases and γ increases. HoWever, this corresponds to feWer
repetitions and thus a loWer chance of success. We Will study this aspect in Section 3.6.
Theorem 2 assumes tWo RDP bounds, Which makes it slightly hard to interpret. Thus We consider tWo
illustrative special cases: We start with pure DP (a.k.a. pointwise DP) - i.e., (ε, δ)-DP with δ = 0,
which is equivalent to (∞, ε)-RDP. This corresponds to Theorem 2 with λ → ∞ and λ → ∞.
Corollary 3 (Theorem 2 for pure DP). Let Q : Xn → Y be a randomized algorithm satisfying
(ε, 0)-DP. Let η ∈ (-1, ∞) and γ ∈ (0, 1). Define A : Xn → Y as in Theorem 2. Then A satisfies
((2 + η)ε, 0)-DP.
Our result is a generalization of the result of Liu & Talwar (2019) — they show that, if K follows
a geometric distribution and Q satisfies (ε, 0)-DP, then A satisfies (3ε, 0)-DP. Setting η = 1 in
Corollary 3 recovers their result. If we set η < 1, then we obtain an improved privacy bound.
Another example is if Q satisfies concentrated DP (Dwork & Rothblum, 2016; Bun & Steinke, 2016).
This is the type of guarantee that is obtained by adding Gaussian noise to a bounded sensitivity
function. In particular, this is the type of guarantee we would obtain from noisy gradient descent.5 *
Corollary 4 (Theorem 2 for Concentrated DP). Let Q : Xn → Y be a randomized algorithm
satisfying P-ZCDP - i.e., (λ, P ∙ λ)-Renyi DP for all λ > 1. Let η ∈ ( — 1, ∞) and Y ∈ (0,1). Define
A : Xn → Y and K — D*γ as in Theorem 2. Assume P ≤ log(1∕γ). Then A satisfies (λ, ε0)-Renyi
DP for all λ > 1 with
0 ʃ 2pρ	∙ log (E [K]) + 2(1+ η)pρlog(1∕γ) — ηρ	if λ ≤ 1 +	qP	log	(E[K])
[P ∙ (λ - 1)	+ λ-ιIOg(E [K]) + 2(1 + η)pPlog(1/Y)	-ηρ ifλ> 1 +，P	log(E [KD
Figure 2 shows what the guarantee of Corollary 4 looks like. Here we start with 0.1-zCDP and
perform repetition following the logarithmic distribution (η = 0) with varying scales (given by γ) and
plot the Renyi DP guarantee attained by outputting the best of the repeated runs. The improvement
over naive composition, which instead grows linearly, is clear. We also study other distributions on
the number of repetitions, obtained by varying η, and Figure 3 gives a comparison. Figure 4 shows
what these bounds look like if we convert to approximate (ε, δ)-DP with δ = 10-6.
Remark 5. Corollary 4 uses the monotonicity property of Renyi divergences: If λ1 ≤ λ2, then
Dλ1 (P kQ) ≤ Dλ2 (P kQ) (Van Erven & Harremos, 2014, Theorem 3). Thus (λ2, ε)-RDP implies
(λ1, ε)-RDP for any λ1 ≤ λ2. In particular, the bound of Theorem 2 yields ε0 → ∞ as λ → 1, so we
use monotonicity to bound ε0 for small λ.
5 Note that the privacy of noisy stochastic gradient descent (DP-SGD) is not well characterized by concentrated
DP (Bun et al., 2018), but, for our purposes, this is a close enough approximation.
6
Published as a conference paper at ICLR 2022
Poisson Distribution. We next consider the Poisson distribution, which offers a different privacy-
utility tradeoff than the truncated negative binomial distribution. The Poisson distribution with mean
μ ≥ 0 is given by P [K = k] = e-** for all k ≥ 0. Note that P [K = 0] = e-μ > 0 here, whereas
the truncated negative binomial distribution does not include 0 in its support. We could condition
on K ≥ 1 here too, but we prefer to stick with the standard definition. We remark that (modulo the
issue around P [K = 0]) the Poisson distribution is closely related to the truncated negative binomial
distribution. If we take the limit as η → ∞ while the mean remains fixed, then the negative binomial
distribution becomes a Poisson distribution. Conversely, the negative binomial distribution can be
represented as a convex combination of Poisson distributions or as a compound of Poisson and
logarithmic; see Appendix A.2 for more details.
Theorem 6 (Main Privacy Result - Poisson Distribution). Let Q : Xn → Y be a randomized
algorithm satisfying (λ, ε)-RDP and (ε, δ)-DPfor some λ ∈ (1, ∞) and ε,ε,δ ≥ 0. Assume Y is
totally ordered. Let μ > 0.
Define an algorithm A : Xn → Y as follows. Draw K from a Poisson distribution with mean μ -
k
i.e., P [K = k] = e-μ ∙ k forall k ≥ 0. Run Q(x) repeatedly K times. Then A(x) returns the best
value from the K runs. If K = 0, A(x) returns some arbitrary output independent from the input x.
If eε ≤ 1 + λ-1, then A satisfies (λ, ε0)-RDP where
o _ l ^ l log μ
ε 一 ε + μ ∙ δ + ʌ-----1.
The assumptions of Theorem 6 are different from Theorem 2: We assume a Renyi DP guarantee and
an approximate DP guarantee on Q, rather than two Renyi DP guarantees. We remark that a Renyi
DP guarantee can be converted into an approximate DP guarantee - (λ, ε)-RDP implies (ε, δ)-DP for
all ε ≥ ε and δ = e。T)R-E) ∙ 1 ∙(1 - 1 )λ 1 (MirOnov, 2017; Canonne et al., 2020). Thus this
statement can be directly compared to our other result. We show such a comparison in Figure 3 and
Figure 4. The proofs of Theorems 2 and 6 are included in Appendix B.2.
3.5	Generic RENYI DP Bound for Any Distribution on the Number of Repetitions
We now present our main technical lemma, which applies to any distribution on the number of
repetitions K. Theorems 2 and 6 are derived from this result. it gives a Renyi DP bound for the
repeated algorithm in terms of the Renyi DP of the base algorithm and the probability generating
function of the number of repetitions applied to probabilities derived from the base algorithm.
Lemma 7 (Generic Bound). Fix λ > 1. Let K be a random variable supported on N ∪ {0}. Let
f : [0,1] → R be the probability generating function of K - i.e., f (x) := P∞=0 P [K = k] ∙ xk.
Let Q and Q0 be distributions on Y. Assume Y is totally ordered. Define a distribution A on Y as
follows. First sample K. Then sample from Q independently K times and output the best of these
samples.6 This output is a sample from A. We define A0 analogously with Q0 in place of Q. Then
Dλ (AkA0) ≤ Dλ (QkQ0) + ɪ log (f0(q)λ ∙ f0(q0)1-λ) ,	(6)
λ-1
where applying the same postprocessing to Q and Q0 gives probabilities q and q0 respectively - i.e.,
there exists an arbitrary function g : Y → [0, 1] such that q
E
X-Q
[g(X)] and q0
X0-EQ0[g(X0)].
The proof of this generic bound is found in Appendix B.1. To interpret the theorem, we should
imagine adjacent inputs x, x0 ∈ Xn , and then the distributions correspond to the algorithms run
on these inputs: A = A(x), A0 = A(x0), Q = Q(x), and Q0 = Q(x0). The bounds on Renyi
divergence thus correspond to Renyi DP bounds. The derivative of the probability generating function
-f 0(x) = E [K ∙ xK-1] - is somewhat mysterious. A first-order intuition is that, if q = q0, then
f0(q)λ ∙ f (q0)1-λ = f0(q) ≤ f0(1) = E [K] and thus the last term in the bound (6) is simply
loλ-[K]. A second-order intuition is that q ≈ q0 by DP and postprocessing and, if f0 is smooth, then
f0(q) ≈ f 0(q0) and the first-order intuition holds up to these approximations. Vaguely, f0 being
smooth corresponds to the distribution of K being spread out (i.e., not a point mass) and not too
6if K = 0, the output can be arbitrary, as long as it is the same for both A and A0 .
7
Published as a conference paper at ICLR 2022
Figure 5: Expected quantile
of the repeated algorithm A as
a function of the final privacy
guarantee (ε, 10-6)-DP for vari-
ous distributions K, where each
invocation of the base algorithm
Q is 0.1-zCDP.
Figure 6: Final success prob-
ability (β) of the repeated algo-
rithm A as a function of the final
privacy guarantee (ε, 10-6)-DP
for various distributions, where
each invocation of the base algo-
rithm Q has a 1/100 probability
of success and is 0.1-zCDP.
Figure 7: Accuracy of the CNN
model obtained at the end of
the hyperparameter search, for
the different distributions on the
number of repetitions K we con-
sidered. We report the mean
over 500 trials of the experi-
ment.
heavy-tailed (i.e. K is small most of the time). The exact quantification of this smoothness depends
on the form of the DP guarantee q ≈ q0 .
In our work, we primarily compare three distributions on the number of repetitions: a point mass
(corresponding to naive repetition), the truncated negative binomial distribution, and the Poisson
distribution. A point mass would have a polynomial as the probability generating function 一 i.e., if
P [K = k] = 1, then f(x) = E xK = xk. The probability generating function of the truncated
negative binomial distribution (Definition 1) is
f(x) = KAiJxK] = {
(1-(1-Y)X)-η-1
γ-η-1
log(1-(1-γ)x)
IOg(Y)
if η 6= 0
if η = 0
(7)
The probability generating function of the Poisson distribution with mean μ is given by f (x)
eμ∙(x-1). We discuss probability generating functions further in Appendix A.2.
3.6	Utility and Runtime of our Hyperparameter Tuning Algorithm
Our analytical results thus far, Theorems 2 and 6 and Lemma 7, provide privacy guarantees for
our hyperparameter tuning algorithm A when it is used with various distributions on the number of
repetitions K . We now turn to the utility that this algorithm provides. The utility of a hyperparameter
search is determined by how many times the base algorithm (denoted Q in the theorem statements) is
run when we invoke the overall algorithm (A). The more often Q is run, the more likely we are to
observe a good output and A is more likely to return the corresponding hyperparameter values. Note
that the number of repetitions K also determines the algorithm’s runtime, so these are closely linked.
How does this distribution on the number of repetitions K map to utility? As a first-order approxi-
mation, the utility and runtime are proportional to E [K]. Hence several of our figures compare the
different distributions on K based on a fixed expectation and Figure 4 plots E [K] on the vertical
axis. However, this first-order approximation ignores the fact that some of the distributions we
consider are more concentrated than others; even if the expectation is large, there might still be a
significant probability that K is small. Indeed, for η ≤ 1, the mode of the truncated negative binomial
distribution is K = 1. We found this to be an obstacle to using the (truncated) negative binomial
distribution in practice in our experiments, and discuss this further in Appendix A.2.1.
We can formulate utility guarantees more precisely by looking at the expected quantile of the output
to measure our algorithm’s utility. If we run the base algorithm Q once, then the quantile of the output
is (by definition) uniform on [0, 1] and has mean 0.5. If we repeat the base algorithm a fixed number
of times k, then the quantile of the best output follows a Beta(k, 1) distribution, as it is the maximum
of k independent uniform random variables. The expectation in this case is k+ι = 1 - k+ι. If we
8
Published as a conference paper at ICLR 2022
repeat a random number of times K, the expected quantile of the best result is given by
E [-K-τ 1 = E [l — v⅛Γ 1 = Z 1 X ∙ f0(x)dx = 1 — Z 1 f (x)dx,	(8)
K+ 1	K+ 1	0	0
where f(x) = E xK is the probability generating function of K; see Appendix A.2.1 for further
details. Figure 5 plots this quantity against privacy. We see that the Poisson distribution performs
very well in an intermediate range, while the negative binomial distribution with η = 0.5 does well if
we want a strong utility guarantee. This means that Poisson is best used when little privacy budget
is available for the hyperparameter search. Instead, the negative binomial distribution with η = 0.5
allows us to improve the utility of the solution returned by the hyperparameter search, but this only
holds when spending a larger privacy budget (in our example, the budget has to be at least ε = 4
otherwise Poisson is more advantageous). The negative binomial with η = —0.5 does very poorly.
From a runtime perspective, the distribution of K should have light tails. All of the distributions we
have considered have subexponential tails. However, larger η corresponds to better concentration in
the negative binomial distribution with the Poisson distribution having the best concentration.
Experimental Evaluation. To confirm these findings, we apply our algorithm to a real hyperpa-
rameter search task. Specifically, we fine-tune the learning rate of a convolutional neural network
trained on MNIST. We implement DP-SGD in JAX for an all-convolutional architecture with a stack
of 32, 32, 64, 64, 64 feature maps generated by 3x3 kernels. We vary the learning rate between 0.025
and 1 on a logarithmic scale but fix all other hyperparameters: 60 epochs, minibatch size of 256, `2
clipping norm of 1, and noise multiplier of 1.1. In Figure 7, we plot the maximal accuracy achieved
during the hyperparameter search for the different distributions considered previously as a function of
the total privacy budget expended by the search. The experiment is repeated 500 times and the mean
result reported. This experiment shows that the Poisson distribution achieves the best privacy-utility
tradeoff for this relatively simple hyperparameter search. This agrees with the theoretical analysis we
just presented above that shows that the Poisson distribution performs well in the intermediate range
of utility, as this is a simple hyperparameter search.
4 Conclusion
Our positive results build on the work of Liu & Talwar (2019) and show that repeatedly running
the base algorithm and only returning the best output can incur much lower privacy cost than naive
composition would suggest. This however requires that we randomize the number of repetitions,
rather than repeating a fixed number of times. We analyze a variety of distributions for the number of
repetitions, each of which gives a different privacy/utility tradeoff.
While our results focused on the privacy implications of tuning hyperparameters with, and without,
differential privacy, our findings echo prior observations that tuning details of the model architecture
without privacy to then repeat training with DP affords suboptimal utility-privacy tradeoffs (Papernot
et al., 2020); in this work, the authors demonstrated that the optimal choice of activation function in a
neural network can be different when learning with DP, and that tuning it with DP immediately can
improve the model’s utility at no changes to the privacy guarantee. We envision that future work will
be able to build on our algorithm for private tuning of hyperparameters to facilitate privacy-aware
searches for model architectures and training algorithm configurations to effectively learn with them.
Limitations. We show that hyperparameter tuning is not free from privacy cost. Our theoretical
and experimental results show that, in the setting of interest, the privacy parameter may double or
even triple after accounting for hyperparameter tuning, which could be prohibitive. In this case, one
compromise would be to state both privacy guarantees - that of the base algorithm that does not
account for hyperparameter tuning, and that of the overall system that does account for this. The
reader may wonder whether our positive results can be improved. In Appendix D, we explore give
some intuition for why they cannot (easily) be improved. We also note that our results are only
immediately applicable to the hyperparameter tuning algorithm from Section 3.3. Other algorithms,
in particular those that adaptively choose hyperparameter candidates will require further analysis.
Finally, among the distributions on the number of repetitions K that we have analyzed, the distribution
that provides the best privacy-utility tradeoffs will depend on the setting. While it is good to have
choices, this does leave some work to be done by those using our results. Fortunately, the differences
between the distributions seem to be relatively small, so this choice is unlikely to be critical.
9
Published as a conference paper at ICLR 2022
Reproducibility & Ethics Statements
Reproducibility. We give precise theorem statements for our main results and we have provided
complete proofs in the Appendix, as well as all the necessary calculations and formulas for plotting
our figures. We have also fully specified the setup required to reproduce our experimental results,
including hyperparameters. Our algorithm is simple, fully specified and can be easily implemented.
Ethics. Our work touches on privacy, which is an ethically sensitive topic. If differentially private
algorithms - such as ours - are applied to real-world sensitive data, then potential harms to the people
whose data is being used must be carefully considered. However, our work is not directly using
real-world sensitive data. Our main results are theoretical and our experiments use either synthetic
data or MNIST, which is a standard non-private dataset.
Acknowlegments
The authors would like to thank the reviewers for their detailed feedback and interactive discussion
during the review period. We also thank our colleagues Abhradeep Guha Thakurta, Andreas Terzis,
Peter Kairouz, and Shuang Song for insightful discussions about differentially private hyperparameter
tuning that led to the present project, as well as their comments on early drafts of this document.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308-318, 2016.
Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight
analyses via couplings and divergences. arXiv preprint arXiv:1807.01647, 2018.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464-473. IEEE, 2014.
Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman.
Algorithmic stability for adaptive data analysis. SIAM Journal on Computing, (0):STOC16-377,
2021.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
machine learning research, 13(2), 2012.
Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and
lower bounds. In Theory of Cryptography Conference, pp. 635-658. Springer, 2016.
Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate
differential privacy. In Proceedings of the forty-sixth annual ACM symposium on Theory of
computing, pp. 1-10, 2014.
Mark Bun, Cynthia Dwork, Guy N Rothblum, and Thomas Steinke. Composable and versatile
privacy via truncated cdp. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
of Computing, pp. 74-86, 2018.
Cl6ment L Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. arXiv preprint arXiv:2004.00010, 2020.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel.
Extracting training data from large language models. arXiv preprint arXiv:2012.07805, 2020.
10
Published as a conference paper at ICLR 2022
Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differentially
private machine learning. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran As-
sociates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/
e6d8545daa42d5ced125a4bf747b3688- Paper.pdf.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint
arXiv:1603.01887, 2016.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, pp. 486-503. Springer, 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006b.
Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity
of differentially private data release: efficient algorithms and hardness results. In Proceedings of
the forty-first annual ACM symposium on Theory of computing, pp. 381-390, 2009.
Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In 2010 IEEE
51st Annual Symposium on Foundations of Computer Science, pp. 51-60. IEEE, 2010.
Konstantina Kourou, Themis P Exarchos, Konstantinos P Exarchos, Michalis V Karamouzis, and Dim-
itrios I Fotiadis. Machine learning applications in cancer prognosis and prediction. Computational
and structural biotechnology journal, 13:8-17, 2015.
Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of
the 51st Annual ACM SIGACT Symposium on Theory of Computing, pp. 298-309, 2019. URL
https://arxiv.org/abs/1811.07971.
Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science (FOCS’07), pp. 94-103. IEEE, 2007.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263-275. IEEE, 2017.
Ilya Mironov, Kunal Talwar, and Li Zhang. R\’enyi differential privacy of the sampled gaussian
mechanism. arXiv preprint arXiv:1908.10530, 2019.
Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive
optimizers for honest private hyperparameter selection. arXiv preprint arXiv:2111.04906, 2021.
Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Ulfar Erlingsson. Tempered
sigmoid activations for deep learning with differential privacy. arXiv preprint arXiv:2007.14191,
2020.
Ryan Rogers and Thomas Steinke. A better privacy analysis of the exponential mech-
anism. DifferentialPrivacy.org, 07 2021. https://differentialprivacy.org/
exponential-mechanism-bounded-range/.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks
against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp.
3-18. IEEE, 2017.
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differen-
tially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pp.
245-248. IEEE, 2013.
Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection. In
2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pp. 552-563.
IEEE, 2017.
11
Published as a conference paper at ICLR 2022
Tim Van Erven and Peter Harremos. Renyi divergence and kullback-leibler divergence. IEEE
Transactions on Information Theory, 60(7):3797-3820, 2014.
Jenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-Velez,
Kenneth Jung, Katherine Heller, David Kale, Mohammed Saeed, Pilar N. Ossorio, Sonoo Thadaney-
Israni, and Anna Goldenberg. Do no harm: a roadmap for responsible machine learning for health
care. Nature medicine, 25(9):1337-1340, 2019.
Yuqing Zhu and Yu-Xiang Wang. Improving sparse vector technique with renyi dif-
ferential privacy. In Advances in Neural Information Processing Systems 33 pre-
proceedings (NeurIPS 2020), 2020. URL https://papers.nips.cc/paper/2020/
hash/e9bf14a419d77534105016f5ec122d62-Abstract.html.
A Further Background
A.1 DIFFERENTIAL PRIVACY & RENYI DP
For completeness, we provide some basic background on differential privacy and, in particular, Renyi
differential privacy. We start with the standard definition of differential privacy:
Definition 8 (Differential Privacy). A randomized algorithm M : Xn → Y is (ε, δ)-differentially
private if, for all neighbouring pairs of inputs x, x0 ∈ Xn and all measurable S ⊂ Y,
P [M(x) ∈ S] ≤ eε ∙ P [M(x0) ∈ S] + δ.
When δ = 0, this is referred to as pure (or pointwise) differential privacy and we may abbreviate
(ε, 0)-DP to ε-DP. When δ > 0, this is referred to as approximate differential privacy.
The definition of pure DP was introduced by Dwork et al. (2006b) and approximate DP was introduced
by Dwork et al. (2006a). Note that the notion of neighbouring datasets is context-dependent, but this
is often glossed over. Our results are general and can be applied regardless of the specifics of what is
a neighbouring dataset. (However, we do require symmetry - i.e., if (x, x0) are a pair of neighbouring
inputs, then so are (x0x).) Usually two datasets are said to be neighbouring if they differ only by the
addition/removal or replacement of the data corresponding to a single individual. Some papers only
consider addition or removal of a person’s records, rather than replacement. But these are equivalent
up to a factor of two.
In order to define Renyi DP, we first define the Renyi divergences:
Definition 9 (Renyi Divergences). Let P and Q be probability distributions on a common space
Ω. Assume that P is absolutely continuous with respect to Q - i.e.,for all measurable S ⊂ Ω ,if
Q(S) = 0, then P(S) = 0. Let P(x) and Q(x) denote the densities of P and Q respectively.7 The
KL divergence from P to Q is defined as
DI (P kQ)=χ 上 p
The max divergence from P to Q is defined as
P(X)Iog (Q⅛)dx.
Q(x)
D∞ (PkQ) ：= SUp {log (Q(S)) : P(S) > 0}.
For λ ∈ (1, ∞), the Renyi divergencefrom P to Q oforder λ is defined as
Dλ (PkQ) := λ⅛og (X上。"(QX)」)
=Log (X生"(盖心
=λ-ι log ( P P(χ)λQ(χ)1
7In general, we can only define the ratio P (x)/Q(x) to be the Radon-Nikodym derivative of P with respect
to Q. To talk about P(x) and Q(x) separately we must assume some base measure with respect to which these
are defined. In most cases the base measure is either the counting measure in the case of discrete distributions or
the Lesbesgue measure in the case of continuous distributions.
12
Published as a conference paper at ICLR 2022
We state some basic properties of Renyi divergences; for further information see, e.g., Van Erven &
Harremos (2014).
Lemma 10. Let P, Q, P0, and Q0 be probability distributions. Let λ ∈ [1, ∞]. The following hold.
•	Non-negativity: Dλ (P kQ) ≥ 0.
•	Monotonicity & Continuity: Dλ (P kQ) is a continuous and non-decreasing function of λ.
•	Data processing inequality (a.k.a. Postprocessing): Let f(P) denote the distribution ob-
tained by applying some (possibly randomized) function to a sample from P and let f (Q)
denote the distribution obtained by applying the same function to a sample from Q. Then
Dλ(f(P)kf(Q))≤Dλ(PkQ).
•	Finite case suffices: We have Dλ (P kQ) = supf Dλ (f(P)kf(Q)) even when f is re-
stricted to functions with a finite range.
•	Chain rule (a.k.a. Composition):	Dλ	(P	× P0kQ ×	Q0)	=	Dλ	(P kQ) +	Dλ	(P0 kQ0),
where P × P0 and Q × Q0 denote the product distributions of the individual distribu-
tions.
•	Convexity: The function (P, Q) 7→ e(λ-1)Dλ (P kQ) is convex for all λ ∈ (1, ∞). The
function (P, Q) 7→ Dλ (P kQ) convex if and only if λ = 1.
Now we can state the definition of Renyi DP (RDP), which is due to Mironov (2017).
Definition 11 (R6nyi Differential Privacy). A randomized algorithm M : Xn → Y is (λ, ε)-Renyi
differentially private if, for all neighbouring pairs of inputs x, x0 ∈ Xn, Dλ (M(x)kM(x0)) ≤ ε.
A closely related definition is that of zero-concentrated differential privacy (Bun & Steinke, 2016)
(which is based on an earlier definition (Dwork & Rothblum, 2016) of concentrated differential
privacy that does not refer to Renyi divergences).
Definition 12 (Concentrated Differential Privacy). A randomized algorithm M : Xn → Y is ρ-zCDP
if,forαll neighbouring pairs of inputs x, x0 ∈ Xn and all λ ∈ (1, ∞), Dλ (M (X)IlM (x0)) ≤ P ∙ λ.
Usually, we consider a family of (λ, ε(λ))-RDP guarantees, where ε(λ) is a function, rather than a
single function. Concentrated DP is one example of such a family, where the function is linear, and
this captures the behaviour of many natural algorithms. In particular, adding Gaussian noise to a
bounded sensitivity function: If f : Xn → Rd has sensitivity ∆ 一 i.e., kf (x) 一 f (x0)∣∣2 ≤ ∆ for all
neighbouring x, x0 - and M : Xn → Rd is the algorithm that returns a sample from N(f (x), σ2I),
then M satisfies δ -zCDP.
We can convert from pure DP to concentrated or Renyi DP as follows (Bun & Steinke, 2016).
Lemma 13. If M satisfies (ε, 0)-differentialprivacy, then M satisfies 2ε2-zCDP - i.e., (λ,1 ε2λ)-
RDP for all λ ∈ (1, ∞).
Conversely, we can convert from concentrated or Renyi DP to approximate DP as follows (Canonne
et al., 2020).
Lemma 14. If M satisfies (λ, ε)-RDP, then M satisfies (ε, δ)-DP where ε ≥ 0 is arbitrary and
δ = eχp((λ — 1)(ε — ε))
(1-『
A.2 Probability Generating Functions
Let K be a random variable supported on N ∪ {0}. The probability generating function (PGF) of K
is defined by
∞
f(x) = E [xκ] = X P [K = k] ∙ xk.
k=0
The PGF f(x) is always defined for x ∈ [0, 1], but may or may not be defined for x > 1. The PGF
characterizes K. In particular, we can recover the probability mass function from the derivatives of
13
Published as a conference paper at ICLR 2022
the PGF (hence the name):
P [K = k] = f(kk(0),
where f (k)(x) denotes the kth derivative of f(x) and, in particular, P [K = 0] = f(0). We remark
that is is often easiest to specify the PGF and derive the probability distribution from it, rather than
vice versa; indeed, we arrived at the truncated negative binomial distribution by starting with the PGF
that we want and then differentiating.
We can also easily recover the moments of K from the PGF: We have f (k) (x) = P∞=k P [K = '] ∙
x'-k ∙ ' ∙ (' - 1) ∙ (' - 2)…(' -k + 1). In particular, f (1) = E [1] = 1 and f0(1) = E [K] and
f00(1) = E [K(K - 1)]. Note that the PGF is a rescaling of the moment generating function (MGF)
g(t):= E etK = f(et).
The PGF can be related to the MGF in another way: Suppose Λ is a random variable on [0, ∞).
Now suppose we draw K — Poisson(Λ). Then the PGF of K is the MGF of Λ 一 i.e., E [xκ]=
E E )[xK] = E [cλ∙(XT)] = g(x — 1), where g is the MGF of Λ. In particular, if Λ is
drawn from a Gamma distribution, then this would yield K from a negative binomial distribution
which has a PGF of the form fNB(x) = (I-(I-Y)X) I8 Note that our results work with a truncated
negative binomial distribution, which is a negative binomial conditioned on K 6= 0. This corresponds
to an affine rescaling of the PGF, namely fτNB(χ) = fNB(X)-fNB(O).
TNB	fNB (1)-fNB(0)
We can also obtain a negative binomial distribution as a compound of a Poisson distribution and a
logarithmic distribution. That is, if We draw T from a Poisson distribution and draw K1,K2, ∙∙∙ , KT
independently from a logarithmic distribution, then K = PtT=1 Kt follows a negative binomial
distribution. The PGF of the logarithmic distribution is given by fKt (x) = E xKt
and the PGF of Poisson is given by fτ(x) = E [xT] = eμ∙(χ-D. Hence
log(1-(1-γ)x)
Iog(Y)
fK(x) =E[xK] =E
T
Y E [xKt]
t=1 Kt
E [fKt(X)T] = fτ(fKt(X)) = eχp (μ ∙ (lOg(I-(IY- Y)x) -1)),
-η
—(JY)X)	with η = log(μ1∕γ).
Finally we remark that we can also use the PGF to show convergence in probability. In particular,
lim	fNB(x) = lim ( 1 - (I - Y)X) η = lim (1 - μ(x - 1)) - = eμ(XT).
η→∞,Y= η+μ	η→∞,γ= n+ημ ∖	γ	)	η→∞,Y= η++μ ∖ η	)
That is, if We take the limit of the negative binomial distribution as η → ∞ but the mean μ = η 1-γ
remains fixed, then we obtain a Poisson distribution. If we take η → 0, then fNB(X) → 1, which is to
say that the negative binomial distribution converges to a point mass at 0 as η → 0. However, the
truncated negative binomial distribution converges to a logarithmic distribution as η → 0.
A.2.1 Probability Generating Functions and Utility
Recall that in Section 3.6, we analyzed the expected utility and runtime of different distributions
on the number of repetitions K. Given our discussion of probability generating functions for these
distributions, we can offer an alternative perspective on the expected utility and runtime.
Suppose each invocation of Q has a probability 1/m of producing a “good” output. This would be the
case if we are considering m hyperparameter settings and only one is good—where here we consider
the outcome to be binary (good or bad) for simplicity and what is a good or bad is determined only
by the total order on the range Y and some threshold on the quality score (e.g., accuracy). Then A
has a probability
β := 1 - P [A(X) ∈ Bad] = 1 -E P [Q(X) ∈ Bad]K = 1 -E [(1 - 1/m)K] = 1 -f(1 - 1/m)
8The PGF of Binomial(n,p) is f (x) = (1 — P + Pxyn This expression is similar to the PGF of the negative
binomial, except the negative binomial has a negative exponent.
14
Published as a conference paper at ICLR 2022
of outputting a good output, where f (x) = E xK is the probability generating function of the
distribution. If We make the first-order approximation f(1 - 1/m) ≈ f(1) - f0(1) ∙ 1/m =
1 - E [K]/m, then we have β ≈ E [K]/m. In other words, for small values of 1/m, the probability
of success is amplified by a multiplicative factor of E [K].
HoWever, the above first-order approximation only holds for large m and, hence, small overall
success probabilities β. In practice, We Want β ≈ 1. The different distributions (Poisson and
truncated negative binomial With different values of η) have very different behaviours even With
the same expectation. In the regime Where We Want the overall success probability to be high (i.e.,
β ≈ 1), smaller η performs Worse, because the distribution is more heavy-tailed. The best performing
distribution is the Poisson distribution, which is almost as concentrated as naive repetition. Figure 6
shoW the success probability β as a function of the final (ε, 10-6)-DP guarantee. This demonstrates
that there is a tradeoff between distributions.
More generally, we can relate the PGF of K to the expected utility of our repeated algorithm. Let
X ∈ R be random variable corresponding to the utility of one run of the base algorithm Q. E.g. X
could represent the accuracy, loss, AUC/AUROC, or simply the quantile of output. Now let Y ∈ R
be the utility of our repeated algorithm A which runs the base algorithm Q repeatedly K times for a
random K. That is, Y = max{Xι,…，XK} where Xi, X2,… are independent copies of X. Let
cdfX(x) =P[X ≤ x] and
cdfY (x) =P[Y ≤ x] = E P[X ≤ x]K = f(cdfX(x)),
KX
where f(x) = E xK is the PGF of the number of repetitions K. Assuming for the moment that
X is a continuous random variable, we can derive the probability density function of Y from the
cumulative distribution function:
Pdfγ(x) = ；CdfX(x) = df f (cdfX(x)) = f0(cdfX(x)) ∙ PdfX(x).
dx	dx
This allows us to compute the expected utility:
E [Y] = / X ∙ Pdfγ(x)dx = / X ∙ f0(cdfX(x)) ∙ PdfX(x)dx = E [X ∙ f0(cdfX(X))].
-∞	-∞
In particular, we can compute the expected quantile (8) in which case X is uniform on [0, 1] and,
hence, cdfX (X) = X and PdfX(X) = 1 for X ∈ [0, 1]. Integration by parts gives
E [Y] = Z x∙f0(x)dx
0
Note that
1
-f (x)dx = 1f (1)-0f (0)-Z f (x)dx = 1-Z f (x)dx.
Z f (x)dx
0
01KE XKdX =KE	0
1
xKdx
K K+1
E
1
0
Finally, we also want to ensure that the runtime of our hyperparameter tuning algorithm is well-
behaved. In particular, we wish to avoid heavy-tailed runtimes. We can obtain tail bounds on the
number of repetitions K from the PGF or MGF too: For all t > 0, we have
P [K ≥ k] = P 卜t∙(K-k) ≥ l] ≤ E 卜t,(K-k)i = f (et) ∙ e-t∙k.
Thus, if the PGF f(X) = E XK is finite for some X = et > 1, then we obtain a subexponential tail
bound on K.
B Proofs from Section 3
B.1 Proof of Generic Bound
Proof of Lemma 7. We assume that Y is a finite set and that P [K = 0] = 0; this is, essentially,
without loss of generality.9 Denote Q(≤ y) := Py,∈γ I[y0 ≤ y] ∙ Q(y0) and similarly for Q(< y)
9Our proof can be extended to the general case. Alternatively, if Y is infinite, we can approximate the relevant
quantities arbitrarily well with a finite partition; see Lemma 10 or Van Erven & Harremos (2014, Theorem 10).
15
Published as a conference paper at ICLR 2022
and analogously for Q0 in place of Q. For each y ∈ Y, we have
∞
A(y) = X P [K = k] ∙ (Q(≤ y)k - Q(< y)k)
k=1
= f(Q(≤ y)) - f(Q(< y))
Q(≤y)
=	f0(x)dx
Q(<y)
=Q(y) ∙ χ- JE,C,« "f0(x)]
XlQ(Vy),Q(Wy)]
uniform
and, likewise, A0(y) = Q0(y) ∙ E	[f0(X0)]. Thus
X0 —[Q0(<y),Q0(≤y)]
uniform
e。T)D入(AkAO) = X A(y)λ ∙ A0(y)1-λ
y∈Y
=X Q(y)λ ∙ Q0(y)1-λ
y∈Y
E	[f0(X)]λ ∙	E	[f0(X0)]1-λ
x-[Q(<y),Q(≤y)]	X 0TQ0(<y),Q0(≤y)]
≤ EQ(y)λ ∙ Q0(y)1-λ ∙ χ9<E),Q(≤y)]	[f0(X)λ ∙f0(X0)1-λ]
y∈Y	X0 —[Q0(<y),Q0(≤y)]
≤ e(λ 1)Dλ(QkQ ) ∙ max	E
—	y∈Y X —[Q(<y),Q(≤y)]
X0 —[Q0 (<y),Q0(≤y)]
f0(X)λ ∙ f(X0)1-λ].
The second inequality follows from Holder,s inequality. The first inequality follows from the fact
that, for any λ ∈ R, the function h : (0, ∞)2 → (0, ∞) given by h(u, V)= uλ ∙ v1-λ is convex and,
hence, E [U]λE [V]1-λ = h(E [(U, V)]) ≤ E [h(U, V)] = E [Uλ ∙ V 1-λ] for any pair of positive
random variables (U, V). Note that we require X to be uniform on [Q(< y), Q(≤ y)] and X0 to be
uniform on [Q0(< y), Q0(≤ y)], but their joint distribution can be arbitrary. We will couple them
so that X-Q)<y) = X QQ(y‹y). In particular, this implies that, for each y ∈ Y, there exists some
t ∈ [0, 1] such that
E
X —[Q(<y),Q(≤y)]
X0 —[Q0(<y),Q0(≤y)]
[f0(X )λ ∙ f(X 0)1-λ] ≤ f(Q(< y) +1 ∙ Q(y))λ ∙ f0(Q0(< y)+1 ∙ Q0(y))1-λ.
Hence
Dλ (AkA0) ≤ Dλ (QkQ0) + ɪ log I maχ f(Q(< y) +1 ∙ Q(y))λ ∙ f0(Q0(< y)+1 ∙ Q0(y))1-λ
λ - 1	y∈Y
t∈[0,1]
To prove the result, we simply fix y ∈ Y and t* ∈ [0,1] achieving the maximum above and define
1 1	if y <y*
g(y) ：= t t* if y = y* .
0 0	if y >y*
□
B.2 Proofs of Distribution-specific Bounds
Truncated Negative Binomial Distribution
Proof of Theorem 2. The probability generating function of the truncated negative binomial distribu-
tion is
f (X) = K JDη,γ [xK] = {
(1-(1-Y)X)-η-1
γ-η — 1
log(1-(1-γ)x)
Iog(Y)
ifη 6= 0
ifη=0
16
Published as a conference paper at ICLR 2022
Thus
η η∙(i-γ)
f0(x) = (1 - (1 - γ)x)-η-1 ∙	γ---1
I log(1∕γ)
if η 6= 0
if η = 0
(1 - (1 - γ)x)-η-1 ∙ γη+1 ∙ E[K].
Now we delve into the privacy analysis: Let Q = Q(x) and Q0 = Q(x0) denote the output
distributions of Q on two neighbouring inputs. Similarly, let A = A(x) and A0 = A(x0) be the
corresponding pair of output distributions of the repeated algorithm. By Lemma 7, for appropriate
values q, q0 ∈ [0, 1] and for all λ > 1 and all λ > 1,10 we have
Dλ(AkA0)
≤ Dλ (QkQ0) + ɪ log (f0(q)λ ∙ f0(q0)1-λ)
λ-1
Dλ (QkQ0)	+	λ-1 log (Yη+1	∙ E [K] ∙ (1 - (1 - Y)q)-λ(η+1) ∙ (1	- (1 - γ)q0)-(1-λ)(η+1))
Dλ (QkQ0)	+	λ--1 log (Y η+1	∙ E [K ] ∙ ((Y + (1- γ )(1-q))1-λ	∙ (γ + (1 -Y)(I- q0)B )ν	∙ (Y	+ (1 -Y)(I- q))u)
C	,一	. ,	、	. ,	Λ .	.,	、
(Xν = (λ — 1)(1 + η) and (1 — X)ν + u = -λ(η + 1))
≤ Dλ (QkQ0) +ɪ log (Yη+1 ∙ E [K] ∙
λ - 1
(Y +(1 - Y) ∙ eɑT)D和(QOkQ))" ∙ (y + (1 - Y)(1 - q))u)
(1
-q and 1 - q0 are postprocessings of Q and Q0 respectively and £(λ-1)口又(∙k∙) is convex and V ≥ 0)
≤ Dλ (QkQ0) + λ-rɪ log (yη+1 ∙ E [K] ∙(Y +(1 - Y) ∙ eɑT)D£(QOkQ)广 Yu)
(Y ≤ Y + (1 - Y)(1 - q) and u ≤ 0)
=Dλ (QkQ0) + ɪ log (γ + (1 - γ) ∙ €(*-1必(QOkQ)) + ɪ log (γη+1 ∙ E [K] ∙ γu)
λ - 1	λ - 1
Dλ (QkQ0) + 占- 1)D^ (Q0kQ) + log(1 - Y ∙(1 - e-αT)D和(QOkQ))))
+ ɪlog (Yu+η+1∙ E [K])
λ - 1
Dl(QkQO) + (1 + n) (1 - ^) D^ (Q0kQ) +
1+η
—^log
λ
(1 - Y ∙
1 - e-αT)Dλ (QOkQ)))
+ 粤EKD + 山 log(1∕γ)	(ν = (ITF+n) and U = -(1 + η)(怨 + 1))
λ - 1 λ	λ	λ
=Dλ (QkQ0) + (1 + η)(1 - ^)	D^	(Q0kQ)	+	ɪɪlog (； - 1 + e-6T)Dχ(QOkQ))	+ lof
≤ Dλ (QkQ0) + (1 + η)(1 - 1)	D^	(Q0kQ)	+	中 log (Y) + ”个叫？.
□
Proof of Corollary 4. We assume that Q : Xn → Y is a randomized algorithm satisfying ρ-zCDP
-i.e., (λ,ρ ∙ λ)-R6nyi DP for all λ > 1. Substituting this guarantee into Theorem 2 (i.e., setting
ε = P ∙ λ and ε = P ∙ W) gives that the repeated algorithm A satisfies (λ, ε0)-RDP for
「	门上、八 1、	^, (1+ η) ∙ log(1/Y) , logE[K]
ε ≤ P ∙λ +(1 + η) ∙ I1 - ^ 1 ρ ∙λ +---^------+ λ - 1 .
一 .一 一一-一一一 ,..一 一一 C 一 .
ThiS holds for all λ ∈ (1, ∞) and all 入 ∈ [1, ∞).
10Our proof here assumes W > 1, but the result holds for W = 1 too. This can be shown either by analyzing
,1∙	.	1	■	11,1∙	,11∙∙,t.r	1	■	，1	.	.	. ■	ɛ TΛ X ■	1 ■
this case separately or simply by taking the limit λ → 1 and using the continuity properties of R6nyι divergence.
17
Published as a conference paper at ICLR 2022
We set λ = /log(1∕γ)∕ρ to minimize this expression. Note that We assume P ≤ log(1∕γ) and
hence this is a valid setting of λ ≥ 1. This reduces the expression to
ε0 ≤ P ∙ λ - (1 + η) ∙ P + 2(1 + η) ∙ √P ∙log(1∕γ) + log E [K].
λ-1
This bound is minimized when λ - 1 = /log(E [K])∕ρ. If λ - 1 < /log(E [K])∕ρ, then We can
apply the monotonicity property of Renyi DP (Remark 5 and Lemma 10) and substitute in the bound
with this optimal λ. That is, we obtain the bound
ε0≤
P ∙ λ - (1 + η) ∙ P + 2(1 + η) ∙ √ρ ∙ log(1∕γ) + loλ-K]	if λ > 1 + √log(E [K])∕ρ
2√ρ ∙ log E [K] + 2(1 + η) √ρ ∙ log(1∕γ) - ηρ
if λ ≤ 1 + √log(E [K])∕ρ .
□
Proof of the Poisson Distribution Bound
Proof of Theorem 6. The probability generating function for the Poisson distribution is f(x) =
E [xκ] = eμ(X-I). Thus f 0(x) = μ ∙ eμ(X-I). As in the previous proofs, let X and x0 be neighbouring
inputs. Denote Q = Q(x), Q0 = Q(x0), A = A(x), and A0 = A(x). By Lemma 7,
Dλ (AkA0)
≤ Dλ (QkQ0) +
Dλ (QkQ0) +
1
λ - 1
1
λ - 1
log (f0(q)λ ∙ f0(q0)1-λ)
log (μ ∙ eμλS-D+μ(I-λXq0-1)
Dλ (QkQ0) +
μ(λq - (λ - 1)q0 - 1) + log μ
Dλ (QkQ0) +
λ-1
μ((λ - I)(I - qO) - λ(1 - q)) + logμ
≤ Dλ (QkQ0) +
λ-1
μ((λ - I)(e (1 - q) + δ) - λ(I - q)) + log μ
λ-1
(by our (ε, δ)-DP assumption on Q and since 1 - q and 1 - q0 are postprocessings)
=Dλ(QkQO)+μ ∙ (1 - q) ∙ (eε - ʌ—£ +μ ∙δ + Xg μ
λ-1	λ-1
≤ Dλ (QIlQO) + μ ∙ δ + d
λ-1
where the final inequality follows from our assumption that eε ≤ 1 + £1.
□
C Conditional Sampling Approach
In the main text, we analyzed the approach where we run the underlying algorithm Q a random
number of times according to a carefully-chosen distribution and output the best result from these
independent runs. An alternative approach - also studied by Liu & Talwar (2019) — is to start
with a pre-defined threshold for a “good enough” output and to run Q repeatedly until it produces
such a result and then output that. This approach has some advantages, namely being simpler and
avoiding the heavy-tailed behaviour of the logarithmic distribution while attaining the same kind of
privacy guarantee. However, the disadvantage of this approach is that we must specify the acceptance
threshold a priori. If we set the threshold too high, then we may have to keep running Q for a long
time.11 Ifwe set the threshold too low, then we may end up with a suboptimal output.
11One solution to avoid an overly long runtime in the case that Q(S) is too small is to modify Q to output ⊥
with some small probability p and then have A halt if this occurs. This would represent a failure of the algorithm
to produce a good output, but would avoid a privacy failure.
18
Published as a conference paper at ICLR 2022
We analyze this approach under Renyi DP, thereby extending the results of LiU & Talwar (2019).
Our algorithm A now works as follows. We start with a base algorithm Q and a set of good outputs
S. Now A(x) computes y = Q(x) and, if y ∈ S, then it returns y and halts. Otherwise, A repeats
the procedure. This is equivalent to sampling from a conditional distribution Q(x)|Q(x) ∈ S. The
number of times Q is run will follow a geometric distribution with mean 1/Q(S).
Proposition 15. Let λ ∈ (1, ∞). Let Q and Q0 be probability distributions on Ω with Dλ (QkQ0) <
∞. Let S ⊂ Ω have nonzero measure under Q0 and also under Q. Let QS and QS denote the
conditional distributions of Q and Q0 respectively conditioned on being in the set S. That is,
Qs(E) = Q(E ∩ S)∕Q(S) and QS(E) = Q0(E ∩ S)∕Q0(S) forall measurable E ⊂ Ω. Then, for
all p, q, r ∈ [1, ∞] satisfying 1/p + 1/q + 1/r = 1, we have
Dλ (QskQS) ≤ λ - T - 1/r Dr∙(λ-ι∕p) (QkQ0)+ λ +Jq- 2Dλ+1∕q-1 (Q0kQ)+^r-+1 log (ɪ).
λ - 1	λ - 1	λ - 1 Q(S)
Proof. For X ∈ Ω, denote the various distributional densities at X (relative to some base measure)
by QS(x),	Q0S(x),	Q(x),	and Q0(x).	We have	QS(x)	=	Q(x)I[x	∈	S]/Q(S) and	Q0S(x)	=
Q0(X)I[X ∈ S]/Q0(S). Now we have
e(λ-1)Dλ(QSkQ0S) = Z QS(X)λQ0S(X)1-λdX
Jω
= Q(S)-λQ0(S)λ-1	I[X ∈ S]Q(X)λQ0(X)1-λdX
Jω
≤ Q(S)-λQ0(S)λ-1 (Z Q(X)dX) ∕p (Z Q0(X)dX) ∕q (Z	Q(X)λ-1∕pQ0(X)1-λ
(Holder,s inequality)
1∕r
1∕r
Q(S)1∕p-λQ0(S)1∕q
Q(x)rλ-r∕pQ0(x)r-rλ-r∕qdx
= Q0(S)λ0Q(S)1-λ0Q(S)-1∕r-1 (Z Q(X)λ1Q0(X)1-λ1dX)1∕r
(λ0 := λ + 1/q- 1, λ1 := rλ - r/p)
≤ e(λ0T)Dλo(Q0kQ) ∙ Q(S)T∕r-1 ∙卜(λι-1)Dλι(QkQ0))1".
(Postprocessing & non-negativity)
□
The number of parameters in Proposition 15 is excessive. Thus we provide some corollaries that
simplify the expression somewhat.
Corollary 16. Let λ, Q, Q0, S, QS, Q0S be as in Proposition 15. The following inequalities all hold.
D∞(QSkQ0S) ≤ D∞ (QkQ0) + D∞ (Q0kQ).
λ2	2	1
Dλ (qsIIqs) ≤ Dλ (QllQ ) + λ - ι Dλ-1(Q kQ) + λ - ι log (Q(S) J .
Dλ (qsIlQS) ≤ d∞ (QkQO) + ʌ---7Dλ-i (QOllQ) + τ-ι log ).
λ - 1	λ - 1	Q(S)
Dλ (QskQS) ≤ ɪD∞ (QkQ0) + Dλ (Q0kQ) + ɪ log fɪ).
λ - 1	λ - 1	Q(S)
∀r ≥ 1 Dλ (QskQS) ≤ Dr(λ-1)+1 (QkQ0) + λz4Dλ-1 (Q0kQ) + ɪ+1 log (ɪ).
λ - 1	λ - 1	Q(S)
The first inequality in Corollary 16 is essentially the result given by Liu & Talwar (2019): If Q
satisfies ε-DP, then A satisfies 2ε-DP.
Figure 8 plots the guarantee of the second inequality in Corollary 16 when Dλ (QkQ0) = 0.1λ and
Dλ-1 (Q0kQ) = 0.1(λ - 1).
19
Published as a conference paper at ICLR 2022
D Negative Results on Improvements to our Analysis
It is natural to wonder whether our results could be further improved. In this section, we give some
examples that demonstrates that quantitatively there is little room for improvement.
D.1 Why a fixed number of repetitions does not result in good privacy.
We first consider more closely the strawman approach discussed in Section 3.2: the base algorithm Q
is repeated a fixed number of times k and we return the best output. This corresponds to picking k
from a point mass distribution. To understand why it performs so poorly from a privacy standpoint,
we first apply our main result from Section 3.5 to the resulting point mass distribution.
Point Mass: Suppose K is just a point mass - i.e., P [K = k] = 1. So A runs the algorithm Q a
deterministic number of times. Then the probability generating function (PGF) is f (x) = xk and its
derivative is f0(χ) = k ∙ χk-1. Let Q denote the base algorithm. We abuse notation and let Q = Q(X)
and Q0 = Q(x0), where x and x0 are neighbouring inputs. Similarly, let A = A(x) and A0 = A(x0)
be the final output distributions obtained by running Q and Q0 repeatedly k times and returning
the best result. We follow the same pattern of analysis that we applied to the other distributions in
Theorems 2 and 6: Lemma 7 gives the bound
Dλ (AkA0) ≤ Dλ (QkQ0) +
≤ Dλ (QkQ0) +
≤ Dλ (QkQ0) +
1
λ - 1
1
λ - 1
1
λ - 1
log (k ∙ (qλ ∙ qo1-λ)kτ)
log (k ∙ (e(IT)Dλ(Bern(GkBernq))) k-1
log kk ∙ (e(λ-1)Dλ(QkQO))
k ∙Dλ(QkQ0) +产,
λ-1
where the final inequality follows from the fact that Bern(q) and Bern(q0) are postprocessings of Q
and Q0 respectively.
This bound is terrible. In fact, it is slightly worse than a naive composition analysis which would
give Dλ (AIlA0) ≤ Dλ (Q0k∣∣Q00k) = k ∙ Dλ (QkQ0). It shows that a deterministic number of
repetitions does not yield good privacy parameters, at least with this analysis.
It is surprising that running the base algorithm Q a fixed number of times k and returning the best
output performs so poorly from a privacy standpoint. We will now give a simple example that
demonstrates that this is inherent and not just a limitation of our analysis. Liu & Talwar (2019,
Appendix B) give a similar example.
Proposition 17. For all ε > 0, there exists a ε-DP algorithm Q : Xn → Y such that the following
holds. Define an algorithm A : Xn → Y that runs Q a fixed number of times k and returns the best
output from these runs. Then A is not ε-DP for any ε < kε. Furthermore, for all λ > 1, A is not
(λ, ε(λ))-Renyi DPfor any ε(λ) < ε0(λ), where
ε0(λ) = kε - k ∙lθg(1 + e-ε).
λ-1
Proof. The base algorithm is simply randomized response. We will let Y = {1, 2} with the total
order preferring 1, then 2. We will define a pair of distributions Q and Q0 on {1, 2} and then the base
algorithm is simply set so that these are its output distributions on a pair of neighbouring inputs.
We let
Q
Q0
fɪ「
1 + eε 1 + eε
(二 ɪ)
1 + eε 1 + eε
20
Published as a conference paper at ICLR 2022
Then D∞ (QkQ0) = D∞ (Q0kQ) = ε. Thus we can ensure that the base algorithm yielding this pair
of distributions is ε-DP.
Now we look at the corresponding pair of distributions from repeating the base algorithm k times.
We have
The first part of the result follows:
D∞ (AkA0) ≥log
kε.
For all λ > 1,
，…(AkA0)≥ ((X))' (( JY]
=eεkλ • (1 + eε)-k
Hence
Dλ (AkA0) ≥
εkλ - k ∙ log(1 + eε)	k ∙ log(1 + e-ε)
--------ζ------------kε ---------------------
λ-1
λ-1
□
The second part of Proposition 17 shows that this problem is not specific to pure DP. For λ ≥ 1 + 1∕ε,
we have ε0(λ) = Ω(kε), so we are paying linearly in k.
However, Proposition 17 is somewhat limited to pure ε-DP or at least (λ, ε(λ))-RDP with not-too-
small values of λ. This is because the “bad” event is relatively low-probability. Specifically, the high
privacy loss event has probability (1 + e-ε)-k. This is small, unless ε ≥ Ω(log k).
We can change the example to make the bad event happen with constant probability. However, the
base algorithm will also not be pure ε-DP any more. Specifically, we can replace the two distributions
in Proposition 17 with the following:
Q = (1 - exp(-1∕k), exp(-1∕k)),
Q0 = (1 - exp(-ε0 - 1∕k), exp(-ε0 - 1∕k)).
Ifwe repeat this base algorithm a fixed number of times k, then the corresponding pair of distributions
is given by
A = (1 - exp(-1), exp(-1)),
A0 = (1 - exp(-kε0 - 1), exp(-kε0 - 1)).
Now we have D∞ (AkA0) = kε0 and the bad event happens with probability e-1 ≈ 0.36. On the
other hand, D∞ (QkQ0) = ε0 like before, but D∞ (Q0kQ) = log(1 - exp(-ε0 - 1∕k)) - log(1 -
exp(-1∕k)) ≈ log(kεo + 1). But we still have a good guarantee in terms of R6nyi divergences. In
particular, D1 (Q0kQ) ≤ (ε0 + 1∕k) log(kε0 + 1), and we can set ε0 ≤ o(1∕ log k) to ensure that
we get reasonable (λ, ε(λ))-RDP guarantees for small λ.
At a higher level, it should not be a surprise that this negative example is relatively brittle. Our
positive results show that it only takes a very minor adjustment to the number of repetitions to obtain
significantly tighter privacy guarantees for hyperparameter tuning than what one would obtain from
naive composition. In particular, running a fixed number of times k versus running Poisson(k) times
is not that different, but our positive results show that it already circumvents this problem in general.
21
Published as a conference paper at ICLR 2022
We also remark that composition behaves differently in the low-order RDP or approx DP regime
relative to the PUre-DP or high-order RDP regime covered by Proposition 17. Thus the naive
composition baseline we compare to is also shifting from basic composition (ε-DP becomes kε-DP
under k-fold repetition (DWork et al., 2006b)) to advanced composition (ε-DP becomes (O(ε ∙
k/k ∙ log(1∕δ)), δ)-DP (Dwork et al., 2010)). Proving tightness of basic composition is easy, but
proving tightness for advanced composition is non-trivial (in general, it relies on the machinery of
fingerprinting codes (Bun et al., 2014)). This means it is not straightforward to extend Proposition 17
to this regime.
D.2 Tight example for conditional sampling.
We are also interested in the tightness of our generic results. We begin by studying the conditional
sampling approach outlined in Appendix C. This approach is simpler and it is therefore easier to give
a tight example. This also proves to be instructive for the random repetition approach in Section 3.
Our tight example for the conditional sampling approach consists of a pair of distributions Q and Q0 .
These should be thought of as the output distributions of the algorithm Q on two neighbouring inputs.
The distributions are supported on only three points. Such a small output space seems contrived, but
it should be thought of as representing a partition of a large output space into three sets. The first set
is where the privacy loss is large and the second set is where the privacy loss is very negative, while
the third set is everything in between.
Fix s, t > 0 and a ∈ [0, 1/4]. Note (1 - 2a)-1 ≤ e3a. Let
e-s,a ∙ e-s,1 — 2a ∙ e-s
Q
a, 1 — a — a ∙ e
1
1 + e-
e-s
1 + e
Intuitively, the set S corresponds to the outputs that (1) have large privacy loss or (2) very negative
privacy loss and we exclude (3) the outputs with middling privacy loss. Once we condition on S we
still have the outputs (1) with large privacy loss, but that privacy loss is further increased because of
the renormalization. Specifically, the negative privacy loss means the renormalization constants are
very different - Q(S) = a ∙ 2e-s《 Q0(S) = a ∙ (1 + e-s-t) - if S is large. In effect, the negative
privacy loss becomes a positive privacy loss that is added to the already large privacy loss.
22
Published as a conference paper at ICLR 2022
2 0 8 6 4 2
1 1
φuuφpφ≥p>uφα
I
5
O，
O
IO 15
Λ
20	25
Figure 8: Upper and lower bounds for Renyi DP of conditional sampling. For each λ, We pick the
parameters S and t such that Dλ (QIlQ0) = Dλ (Q0kQ) = 0.1 ∙ λ and We plot the upper bound from
the second inequation in Corollary 16 along with the exact value of Dλ (QS kQ0S).
We make the above intuition more precise by calculating the various quantities. For all λ > 1, We
have
Q(S) = 2a ∙ e-s,
e(IT)Dι(QkQO) = a ∙ e-sλ-(s+t)(1-λ) + a . e-sλ +(1 - 2a ∙ e-s)λ(1 - a - a ∙ e-s-t)1-λ
≤ a ∙ e(IT)t-s + ae-sλ + e-2ae-s
≤ a ∙ e(I-I)t-s + a + e3a(λ-1)
≈ a ∙ e(λ-1)t-s
=1 Q(S) ∙ e(λ-1)t,
λ(1 - 2a)1-λ
(assuming t is large)
=⇒ Dλ (QIQ0) . t -
e(I-I)Dλ(QOkQ)= a ∙ e
log(2/Q(S))
λ — 1	,
L(I-λ)-(s+t)λ + a ∙ e-s(1-λ) + (1 — 2a ∙ e-s)1-λ(1 — a — a ∙ e-s-t)λ
≤ a ∙ e-tλ-s
≈ a ∙ es(λ-1)
=2Q(S) ∙ e
+ a ∙ es(IT) + e3a∙e-s∙(IT) . e-aλ
(assuming s is large)
sλ
=⇒ Dλ (Q0kQ) . S + S - log-/Q(S))
23
Published as a conference paper at ICLR 2022
e(λ-1)Dλ (QS kQ0S) = 2-λ (1 + e-s-t)λ-1 (e(λ-1)(s+t) + 1)
≥ 2-λ ∙ e(λ-1)(s+t)
=⇒ Dλ (QSIlQS) ≥S+t —二一⅝^.
λ-1
We contrast this with our upper bound from the second part of Corollary 16:
λ-2	2	1
Dλ (QSIlQS) ≤ Dλ (QIlQ ) + λ - 1 Dλ-1(Q IIQ) + λ - 1 log (Q(S) J .
log(2∕Q(S))	λ - 2(	s - log(2∕Q(S)) λ
.t- λ - 1	+ λ-1 1S + —λ-2— J +
_ ，+ 2log2
=S +t - λ-r.
2 log(1/Q(S))
λ-1
This example shows that our upper bound is tight up to small factors, namely the lower order terms
We ignore With ≈ and λ- log 2. Figure 8 illustrates how the upper and lower bounds compare.
D.3 Tightness of our generic result.
Now we consider the setting from Section 3 where our base algorithm is run repeatedly a random
number of times and the best result is given as output.
Let Q : X n → Y denote the base algorithm. Assume Y is totally ordered. Let K ∈ N be a random
variable and let f (x) = E xK be its probability generating function. Define A : Xn → Y to be the
algorithm that runs Q repeatedly K times and returns the best output.
For a tight example, we again restrict our attention to distributions supported on three points:
Q = Q(x) = (1 - b - c, b, c),
Q0 = Q(x0) = (1 - b0 - c0, b0, c0),
A=A(x)=(1-f(b+c),f(b+c)-f(c),f(c)),
A0=A(x0)=(1-f(b0+c0),f(b0+c0)-f(c0),f(c0)).
Here the total ordering prefers the first option (corresponding to the first coordinate probability), then
the second, and then the third, which implies the expressions for A and A0 . Note that the probability
values are not necessarily ordered the same way as the ordering on outcomes.
Now we must set these four values to show tightness of our results.
We make the first-order approximation
A ≈ (1 — f(b + c),f 0(c) ∙ b,f(c)),
A0 ≈ (1 — f (b0 + c0),f0(c0) ∙ b0,f(c0)).
We take this approximation and get
e(I-I)Dλ(AkA0) & (f0(c) ∙ b)λ ∙ (f0(c0) ∙ b0)1-λ
=e(IT)Dλ(bkb0) ∙ (f0(c))λ ∙ (f0(c0))1-λ
≈ e(IT)Dλ(QkQO) ∙(f0(c))λ ∙ (f0(c0))1-λ,
where the final approximation assumes that the second term is dominant in the equation
e(IT)Di(QkQ0) = (1 - b - c)λ ∙ (1 - b0 - c0)1-λ + bλ ∙ (b0)1-λ + (c)λ ∙ (c0)1-λ.
Contrast this with our upper bound (Lemma 7), which says
e(I-I)Dλ(AkA0) ≤ e(I-I)Di(QkQ0) ∙ (f(q))λ ∙ (f0(q0))1-λ,
where q and q0 are arbitrary postprocessings of Q and Q0 . In particular, we can set the values so
that q = c and q0 = c0 . This is not a formal proof, since we make imprecise approximations. But it
illustrates that our main generic result (Lemma 7) is tight up to low order terms.
24
Published as a conference paper at ICLR 2022
D.4 Selection & Lower Bounds
Private hyperparameter tuning is a generalization of the private selection problem. In the private
selection problem we are given a utility function u : Xn × [m] → R which has sensitivity 1 in its
first argument - i.e., for all neighbouring x, x0 ∈ Xn and all j ∈ [m] = {1,2,…,m}, We have
|u(x, j) - u(x0, j)| ≤ 1. The goal is to output and approximation to arg maxj∈[m] u(x, j) subject to
differential privacy.
The standard algorithm for private selection is the exponential mechanism (McSherry & TalWar,
2007). The exponential mechanism is defined by
∀j ∈ H	PM(X)= j]=	exp(2Ut，j))、.
P'∈[m] exP (2U(XS)
It provides (ε, 0)-DP and, at the same time, 1 ε2-zCDP (Rogers & Steinke, 2021). On the utility side,
We have the guarantees
2
E [u(x, M(x))] ≥ max u(x,j)---log m,
P
U(X, M (X)) ≥ max U(X, j) -
j∈[m]
≥1-β
for all inputs X and all β > 0 (Bassily et al., 2021, Lemma 7.1) (DWork & Roth, 2014, Theorem
3.11).
It is also Well-knoWn that the exponential mechanism is optimal up to constants. That is, (ε, 0)-DP
selection entails an additive error of Ω(log(m)∕ε) (Steinke & Ullman, 2017).
Our results can be applied to the selection problem. The base algorithm Q(X) Will simply pick an
index j ∈ [m] uniformly at random and the privately estimate U(X, j) by adding Laplace or Gaussian
noise ξ and output the pair (j, U(X, j) + ξ). The total order on the output space [m] × R simply
selects for the highest estimated utility (breaking ties arbitrarily). If We take ξ to be Laplace noise
with scale 1∕ε, then Q is (ε, 0)-DP.
Applying Corollary 3 yields a ((2 + η)ε, 0)-DP algorithm A With the folloWing utility guarantee. Let
K be the number of repetitions and let (jι,u(χ,jι) + ξι),…，(jκ, u(χ,jκ) + ξκ) be the outputs
from the runs of the base algorithm. The probability that the repeated algorithm A will consider j* :=
argmaxj∈[m] u(x, j) is P [j* ∈ {jι,…，jκ}] = 1 - E [Qk∈[κ] P [j* = jk]] = 1 - f (1 - 1∕m),
where f(X) = E XK is the probability generating function of the number of repetitions K. For each
noise sample, we have ∀k P [∣ξk | ≤ t] ≥ 1-e-εt for all t > 0. Thus, for all t > 0, the probability that
all noise samples are smaller than t is P [∀k ∈ [K] ∣ξk | ≤ t] = f (1 - e-εt). By a union bound, we
have P [U(X, M (X)) ≥ U(X, j*) - 2t] ≥ f (1-e-εt)-f (1 -1∕m) for all t > 0. Setting η = 0, yields
f (X) = log(1->g1-γ)x), so P [u(x, m(X)) ≥ u(χ,j*)- 2t] ≥ klg(1∕γ) log (1++-Γemεt). Now set
t = log(m10 - 1)∕ε and Y = exp(1t)+1 = m-10 so that 1-γe-εt = 1 and 1-γm1 = m9 - m1. Then
P[u(χ, M (X)) ≥ u(χ,j*) - 20 log m] ≥ 1oi0gm log (m^T/m ) ≥ 告-10lοg2 m . That is, we
can match the result of the exponential mechanism up to (large) constants. In particular, this means
that the lower bounds for selection translate to our results - i.e., our results are tight up to constants.
E Extending our Results to Approximate DP
Our results are all in the framework of Renyi DP. A natural question is what can be said if the base
algorithm instead only satisfies approximate DP - i.e., (ε, δ)-DP with δ > 0. Liu & Talwar (2019)
considered these questions and gave several results. We now briefly show how to our results can be
extended in a black-box fashion to this setting.
We begin by defining approximate Renyi divergences and approximate Renyi DP:
25
Published as a conference paper at ICLR 2022
Definition 18 (Approximate Renyi Divergence). Let P and Q be probability distributions over Ω.
Let λ ∈ [1, ∞] and δ ∈ [0, 1]. We define
Dδλ (P kQ) =inf{Dλ(P0kQ0) :P= (1-δ)P0+δP00,Q= (1-δ)Q0+δQ00},
where P = (1 - δ)P0 + δP00 denotes the fact that P can be expressed as a convex combination of
two distributions P0 and P00 with weights 1 - δ and δ respectively.
Definition 19 (Approximate Renyi Differential Privacy). A randomized algorithm M : Xn → Y is
δ-approximately (λ, ε)-Renyi differentiaUy private if,for all neighbouring pairs of inputs x, x0 ∈ Xn,
Dδλ(M(x)kM(x0)) ≤ε.
Definition 19 is an extension of the definition of approximate zCDP (Bun & Steinke, 2016). Some
remarks about the basic properties of approximate RDP are in order:
•	(ε, δ)-DP is equivalent to δ-approximate (∞, ε)-RDP.
•	(ε, δ)-DP implies δ-approximate (λ, 1 ε2λ)-RDP for all λ ∈ (1, ∞).
•	δ-approximate (λ, ε)-RDP implies (ε, δ)-DP for
δ = δ +exp((λ - 1)(ε - ε))
•	δ-approximate (λ, ε)-Renyi differential privacy is closed under postprocessing.
•	If M1 is δ1-approximately (λ, ε1)-Renyi differentially private and M2 is δ2-approximately
(λ, ε2)-Renyi differentially private, then their composition is (δ1 +δ2)-approximately (λ, ε1+
ε2)-RDP.
Our results for Renyi DP can be extended to approximate Renyi DP by the following Lemma.
Lemma 20. Assume Y is a totally ordered set. For a distribution Q on Y and a random variable
K supported on N ∪ {0}, define AQK as follows. First we sample K. Then we sample from Q
independently K times and output the best of these samples. This output is a sample from A.
Let Q, Q0, Q1-δ0, Qδ0, Q01-δ0, Q0δ0 be distributions on Y satisfying Q = (1 - δ0)Q1-δ0 +δ0Qδ0 and
Q0 = (1 - δ0)Q01-δ + δ0Q0δ . Let K be a random variable on N ∪ {0} and let f(x) = E xK be
the probability generating function of K. Define a random variable K0 on N ∪ {0} by P [K0 = k] =
P [K = k]∙(1 — δo)k/f(1 - δo).12
Then, for all λ ≥ 1, we have
Dλ (AKMKo) ≤ Dλ (AKMAK1-δ0)
where
δ = 1 - f(1 - δ0).
How do we use this lemma? We should think of AQK as representing the algorithm we want to
analyze. The base algorithm Q satisfies δ0-approximate (λ, ε)-RDP. The above lemma says it suffices
to analyze the algorithm AK where Q satisfies (λ, ε)-RDP. We end up with a δ-approximate RDP
Q
result, where the final δ depends on δ0 and the PGF of K.
As an example, we can combine Lemma 20 with Theorem 6 to obtain the following result for the
approximate case.
Corollary 21. Let Q : Xn → Y be a randomized algorithm satisfying (ε0, δ0)-DP. Assume Y is
totally ordered. Let μ > 0.
Define an algorithm A : Xn → Y as follows. Draw K from a Poisson distribution with mean μ.
Run Q(x) repeatedly K times. Then A(x) returns the best value from the K runs. (If K = 0, A(x)
returns some arbitrary output independent from the input x.)
12Note that the PGF of K0 is given by E 卜K'] = Pk=O xkP [K = k](1 — δo)k/f(1 — δo) = f (x(1 —
δo))∕f(1 — δo).
26
Published as a conference paper at ICLR 2022
For all λ ≤ 1 + ɪε-ɪ ,the algorithm A satisfies δ0-approximate (λ, ε0) -RDP where
ε0 = εo + (eε0 - 1) ∙ log μ,
δ0 = 1 - e-μ∙δo ≤ μ ∙ δo.
Proof of Lemma 20. For a distribution P on Y and an integer k ≥ 0, let maxPk denote the distribu-
tion on Y obtained by taking k independent samples from P and returning the maximum value per
the total ordering on Y. (If k = 0, this is some arbitrary fixed distribution.)
Using this notation, we can express AQK as a convex combination:
Suppose P = (1 - δ)P0 +δP00 is a convex combination. We can view sampling from P as a two-step
process: first we sample a Bernoulli random variable B ∈ {0, 1} with expectation δ; if B = 0, we
return a sample from P0 and, if B = 1, we return a sample from P00. Thus, if we draw k independent
samples from P like this, then with probability (1 - δ)k all of these Bernoullis are 0 and we generate
k samples from P0; otherwise, we generate some mix of samples from P0 and P00. Hence we can
write maxPk = (1 - δ)k max(P0)k + (1 - (1 - δ)k)P000 for some distribution P000.
It follows that we can express
∞
AQK = XP[K = k] max Qk
k=0
∞
=X P [K = k] ((1 - δo)k max QL0 +(1 -(1 - δo)k)Pk)
k=0
=f(1- δo)AK0 + (1 - f (1 - δo))P*
for some distributions P0,P1,…and (1 - f (1 - δo))P* = P∞=0 P [K = k](1 - (1 - δo)k)Pk.
Similarly, We can express AQKo = f (1 - δo)AKo0	+ (1 - f (1 - δ0))P; for some distribution P].
1-δ0
Using these convex combinations we have, by the definition of approximate Renyi divergence,
Dλ (AKIlAK0)≤ Dλ (AKI-δ0 IIAK0
0
01-δ0
as δ = 1 - f(1 - δ0).
□
Proof of Corollary 21. Fix neighbouring inputs x, x0 and let Q = Q(x) and Q0 = Q(x0) be the
corresponding pair of output distributions from the base algorithm. Then, in the notation of Lemma 20,
A(X) = AK and A(x0) = AQKo for K 〜Poisson(μ). Setting δ0 = 1-f (1-δo) = 1-e-μ∙δ0 ≤ μ∙δ°,
we have
Dλ0 (A(χ)kA(χ0)) = Dλ0 (AKHAKo) ≤ Dλ (AKI-δ0 IIaKI-J
where K0 〜 Poisson(μ ∙ (1 — δ0)).
Now we apply Theorem 6 to the algorithm corresponding to the pair of distributions AQK0 and
AQK00	. The base algorithm, corresponding to the pair Q1-δ0 and Q01-δ satisfies (ε0, 0)-DP. This
1-δ0	0
yields
Dλ (AKO	JAK；	) ≤ εo + log((I- °0)∙ ^
λ〈 Q1-δ0 Il Q1-δ0J - 0 +	λ - 1
if eε0 ≤ 1 + 1∕(λ - 1). Setting λ = 1 + 1∕(eε0 - 1) and applying monotonicity (Remark 5 and
Lemma 10) and we obtain the result.	□
27
Published as a conference paper at ICLR 2022
E.1 Truncating the Number of Repetitions
Another natural question is what happens if we truncate the distribution of the number of repetitions
K . For example, we may have an upper limit on the acceptable runtime. This does not require
relaxing to approximate DP, as done by Liu & Talwar (2019).
Let K be the non-truncated number of repetitions and let f(x) = E xK be the PGF. Let m ∈ N. Let
KK be the truncated number of repetitions. That is, P [K = k] = I[K ≤ m] ∙ P [K = k]/P [K ≤ m].
Let f(χ) = E [xK] be the corresponding PGF.
We have f 0(χ) = Pk=I k ∙ χk-1 ∙ P [K = k] and f0(χ) = Pm=I k ∙ χk-1 ∙雕=3.Hence, for
x ∈ [0, 1],
., ~., - -
0 ≤f 0(x) - f 0(x) ∙ P [K ≤ m]
∞
=X k ∙ xk-1 ∙ P [K = k]
k=m+1
∞
≤ Xm ∙ X k ∙ P [K = k]
k=m+1
=Xm ∙ E [K ∙I[K> m]].
Now f0(x) ∙ P [K ≤ m] = Pm=I k ∙ xk-1 ∙ P [K = k] ≥ xm-1 ∙ E [K ∙ I[K ≤ m]]. Thus
f0(x)	/	xm ∙ E [K ∙I[K>m]]	E [K ∙ I[K>m]]
≤『0(x) ∙ P[K ≤ m] ≤ + xm-1 ∙ E [K ∙ I[K ≤ m]] = + x。E [K ]-E [K ∙ I[K>m]]
Now we can bound the quantity of interest in Lemma 7: For all q, q0 ∈ [0, 1], we have
1-λ
f0(q)λ 了(/尸 ≤ (P[K≤⅛ ?
f0(q0)
P [K ≤ m] ∙ (1 +
f 0(q)λ ∙f 0(q0 )1-λ ∙ P[K⅛ ∙(1+
E[K∙I[K>m]]	λ
E[K]-E[K∙I[K>m]]))
E [K ∙I[K>m]]	YT
E [K] — E [K ∙ I[K > m]])
This gives us a generic result for truncated distributions:
Lemma 22 (Generic Bound (cf. Lemma 7) for Truncated distributions). Fix λ > 1 and m ∈ N. Let
K be a random variable supported on N ∪ {0}. Let f : [0, 1] → R be the probability generating
function of K - i.e., f (x) := P∞=0 P [K = k] ∙ xk.
Let Q and Q0 be distributions on Y. Assume Y is totally ordered. Define a distribution A on Y as fol-
lows. First we sample K which is K conditioned on K ≤ m - i.e. P [K = k] = P [K = k|K ≤ m].
Then we sample from Q independently K times and output the best ofthese samples.13 This output is
a sample from A. We define A0 analogously with Q0 in place of Q.
Then
Dλ(AkAO) ≤ DI(QkQO)+λ-1 log (f 0(q)λ ∙f 0(q0)i)+lθg (I--TJ) +log (1 + E [KE [KEIiK >Km> m]]),
(9)
where q and qO are probabilities attained by applying the same arbitrary postprocessing to Q and
QO respectively - i.e., there exists a function g : Y → [0, 1] such that q = E [g(X)] and
X-Q	、
q0 = X0 殳,EX0)].
This will give almost identical bounds to using the non-truncated distribution as long as P [K > m]
1 and E [K ∙ I[K > m]]《E [K], which should hold for large enough m.
13If KK = 0, the output can be arbitrary, as long as it is the same for both A and A0.
28