Figure 1: Polytope of achievable policies: (Left) For a 3-state MDP, the set of feasible state marginaldistributions is described by a triangle [(1, 0,0), (0,1,0), (0,0,1)] in R3. We plot the state occupancy measureof 6 different policies. (Center) We plot those same policies, but remove the coordinate axes to improve visualclarity. (Right) For any convex combination of valid state occupancy measures, there exists a Markovian policythat has this state occupancy measure. Thus, policy search happens inside a convex polytope.
Figure 2: Maximizing Rewards: We vi-sualize reward functions as vectors startingat the origin. Maximizing expected returncorresponds to maximizing the dot productbetween the state marginal distribution andthis reward vector.
Figure 3: Information Geometry of Skill Learning: Maximizing mutual information is equivalent to mini-mizing the maximum divergence between a prior distribution over states p(s) (green square) and any achievablestate marginal distribution (Lemma 6.5). MISL learns a distribution over state distributions, p(z), that assignsnon-zero probability to only a small number of state distributions (solid orange circles).
Figure 4: As predicted by our theory, the number ofunique skills learned by MISL is upper bounded by thenumber of states.
Figure 5: Counterexample 1: Maximizing I ((s, a); z) does not result in discovering more verticesof the state marginal polytope.
Figure 7: Information Geometry of Skill Learning: More examples of the skills learned by MISL.
