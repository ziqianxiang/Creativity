Figure 1: (a) Five pixels (g, h, i,j, k) interact with each other, forming an edge pattern for classifi-cation. (b) Representation bottleneck. A DNN is likely to encode low-order and high-order inter-actions, but usually fails to learn middle-order interactions. (c) The cognition gap between DNNsand humans. Humans extract little information from a few image patches (e.g., 5% patches). Also,given almost all patches (e.g., 90% patches), people learn little new information from the additional5% patches since the information is already redundant for human recognition. In comparison, theDNN encodes most interactions when the DNN is given very few patches or most patches.
Figure 2: The distributions of interaction strength J(m) of different DNNs trained on various imagedatasets and tabular datasets.2encode complex interactions where massive variables (e.g., more than 0.9n variables) participate.
Figure 3: (a) Distributions of the interaction strength J(m) of a ResNet-20 model over differentorders, which were measured after different training epoches. The DNN was trained on the CIFAR-10 dataset. (b) Simulations of the J(m) distributions based on 户(m) curves on the ImageNet dataset.
Figure 4: (a) Weight coefficients W(m) of different orders with different pairs of (ri, r2). (b) Distri-butions of the interaction strength J(m) over different orders. Each curve indicates a AlexNet whoseinteractions were encouraged/penalized by L+(r1, r2) and L-(r1, r2) with certain pairs of (r1, r2).
Figure 5: Tested images by random masking (left) and centrally-surrounding masking (right).
Figure 6: Classification accuracies using VGG-16 on images with different numbers of patchesbeing masked. The Appendix C provides more results.
Figure 7: (a) Distributions of the interaction strength J(m) of normal DNNs and high-order DNNs,where high-order DNNs were trained by encouraging high-order interactions and penalizing low-order interactions simultaneously. (b) The instability of J(m) (x) w.r.t. the sampling number of pairsof variables (i, j) and the sampling number of contexts S.
Figure 8: Distributions of the interaction strength J(m) of four types of DNNs. The low-order,middle-order, and high-order DNNs were trained by following the parameter setting mentioned inSection 3.4, while λ2 was set as 0.1 (instead of 1) for low-order DNNs on the Tiny-ImageNet dataset.
Figure 9: Classification accuracies using AleXNet architecture on images with different numbers ofpatches being masked.
Figure 10: Images generated by random masking (left) and randomly-surrounding masking (right).
Figure 11: Classification accuracies using VGG-16 architecture on images with different numbersof patches being masked (by random masking and randomly-surrounding masking).
Figure 12:	Classification accuracies using AlexNet architecture on images with different numbersof patches being masked (by random masking and randomly-surrounding masking).
Figure 13:	Classification accuracies of low-order DNNs, middle-order DNNs, high-order DNNs,and normally trained DNNs (using VGG16) on images with different numbers of patches beingmasked. Here, We used the random masking method and the centrally-surrounding masking method.
Figure 14: Classification accuracies of low-order DNNs, middle-order DNNs, high-order DNNs,and normally trained DNNs (using AlexNet) on images with different numbers of patches beingmasked. Here, We used the random masking method and the centrally-surrounding masking method.
Figure 15: The multi-order interaction Inmmr) on normal samples and Iammv) on adversarial samples fornormally trained DNNs. Adversarial perturbations mainly affected high-order interactions.
Figure 16: (a) Distributions of the interaction strength J (m) of normally trained DNN and DNNstrained by using multiple L+, L-, and (r1, r2) pairs. (b) Distributions of the interaction strengthJ(m) of DNNs trained by using different λ2s.
Figure 17: Distributions of the interaction strength J(m) of normally trained DNN and DNNstrained with L+ and/or L- w.r.t. different pairs of (r1, r2).
Figure 18:	Distributions of the interaction strength J (m) of DNNs trained by using different pairs(λ1, λ2) on the census dataset and the commercial dataset.
Figure 19:	Curves of classification loss Lossclassification w.r.t. training epochs for normally trainingDNNs and training DNNS with the proposed losses.
Figure 20: The distributions of interaction strength J(m of different DNNs using new metric. TheseDNNs were trained on various image datasets and tabular datasets.
