Figure 1: Inspired by Gholami et al. (2021), We show the comparison of full-precision model(presented in (a)) and different quantizations settings: (b) simulated quantization; (c) integer-onlyquantization; and (d) fixed-point quantization. Note the combination of last two operations in integer-only quantizationis termed as dyadic scalingin literature (Yaoet alâˆ™,2021).
Figure 2: (a) Value range of effective weight (see Sec. 4.2) for a pre-trained full-precision (FP) model,and (b) fractional lengths of each layer for a well-trained fixed-point model for MobileNet V2.
Figure 3: Representing potential for 8-bit signed (a) and unsigned (b) fixed-point numbers withdifferent formats. The figures plot the relationship between relative quantization error and the standarddeviation for different fixed-point formats. Both are experimented on zero-mean Gaussian randomvariables (R.V.), with ReLU applied on (b).
Figure 4: Determining optimal fractional length from standard deviation. (a) and (c) illustrate optimalfractional length and minimum relative quantization error against standard deviation for signed andunsigned 8-bit fixed-point quantization for Gaussian and rectified Gaussian random variables. (b) and(d) show the relationship between threshold standard deviation and fractional length.
Figure 5: The illustration of residual connections. For a layer with several layers (named childrenlayers) directly following it, we choose one to be master, and all its sibling layers use the masterlayer,s clipping level. On the other hand, since using different fractional length only cause bit shiftingor different fixed-point quantization formats, and the values are stored in 32-bit before quantized into8-bit, we do not share the fractional formats to allow more degrees of freedom. The two figures showthe case of direct residual connection (a) and that with downsampling convolution layer (b).
Figure 6: Fractional lengths of each layer for a well-trained fixed-point model for ResNet50.
