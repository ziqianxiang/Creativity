Figure 1: Overview. Given molecules and domain-specific metrics to be optimized, we construct agraph grammar, which can serve as a generative model. The graph grammar construction processautomatically learns the grammar rules by optimizing the metrics.
Figure 2: Examples of a molecular hypergraph, one of our possible graph grammars for it, and anapplication of the grammar for generating new molecules.
Figure 3: Overview of bottom-up grammar construction. We optimize the iterative, bottom-upgrammar construction by learning how to create a grammar that samples molecules fitting inputmetrics. Specifically we learn which edges to select for contraction in each iteration step using aneural network Fθ . We perform this construction on all input molecules simultaneously.
Figure 4: Left: Analysis of balance factor λ. We choose 9 different combinations of λ% for twooptimization objectives: Diversity and RS, showing a clear trade-off between the two objectives.
Figure 5: Examples of generated results using our learned graph grammar.
Figure 6: Comparison with simple feature extractor on Isocyanates.
Figure 7: Analysis of stability of proposed DEG.
Figure 8: Convergence curves on three datasets.
Figure 9: Examples of production rules from our learned graph grammar.
