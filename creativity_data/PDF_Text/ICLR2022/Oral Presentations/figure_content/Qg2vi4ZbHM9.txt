Figure 1: Effect of reset-ting the weights of differentcomponents in child models(Mega, Dog) to their initialvalues, which come from theparent model (FFHQ). Reset-ting the feature convolutionweights causes the most dras-tic changes. Also see Figure 8.
Figure 2: Semanticcontrols discoveredfor a parent FFHQmodel retain theirfunction in the chil-dren models (Megaand Metface). Thisholds for individ-ual channels inS (bangs, smile,gaze), as well asfor directions in W(pose, age, gender).
Figure 3: Semanticalignment betweensingle-channel andmulti-channel con-trols for more distantdomains (humansand dogs). Seealso Figure 12 andsupp. videos.
Figure 4: A smooth transition inimages generated from the samelatent code z ∈ Z during fine-tuning. The epoch number ap-pears above each column. Themost significant visual changesoccur in early epochs (0-16).
Figure 5: Comparison of I2I translation (cat2dog and dog2wild in the AFHQ dataset) with two state-of-the-art methods. Our method generates realistic target domain images that capture the pose fromthe source image. In contrast, both CUT and F-LSeSim fail to generate realistic images since theyfollow the shape of the source domain image too closely. A quantitative comparison in the tablebelow indicates our method is superior by a wide margin, in both FID and KID.
Figure 6: Comparison of reference-based image translation with StarGAN2 and OverLORD. Ourmethod generates realistic target domain images that combine pose and structure from the sourceimage with texture and color from the reference. StarGAN2 follows the source shape too closely,resulting in non-realistic animals (1st example in dog2cat, all examples in wild2dog). OverLORD’sresults preserve the appearance of the reference well, but sometimes fail to capture the pose andstructure (e.g., ear shape) from the source image (2nd and 3rd examples in wild2dog). A quantitativecomparison in the table below indicates superior performance of our method in both FID and KID.
Figure 7: Zero-shot dog attribute classification using aligned models (FFHQ and AFHQ dogs). Inthe top row a human “black hair” classifier becomes a “black fur” classifier, a “curly hair” classifieris able to classify “curly fur”, and a “long hair” classifier becomes a “down-pointing ears” classifier.
Figure 8: We reset the weights of different components in child models (Mega, dog, church) totheir initial values, which come from the parent model (FFHQ). When resetting the weights infeature convolution layers, the output images change more drastically (content, structure), whileresetting the weights of other components causes milder effects. This implies feature convolutionlayers contain most of new learned knowledge.
Figure 9: Semantic alignment: semantic controls discovered for the parent model (FFHQ) retaintheir function in the children models (Mega and Metface). This holds for individual channels in S(bangs, smile, gaze), where the layer and channel number is indicated under each column. Semanticalignment is also observed for manipulation directions in W (pose, age, gender).
Figure 10: Semantic alignment of multiple channels: semantically meaningful directions inStyleSpace discovered in the parent model (FFHQ), detected using StyleCLIP (Patashnik et al.,2021), still control the same attributes in children models (Mega and Metface).
Figure 11: Semantic alignment of multiple channels: semantically meaningful directions inStyleSpace discovered in the parent model (FFHQ), detected using StyleCLIP (Patashnik et al.,2021), still control the same attributes in children models (Mega and Metface).
Figure 12: Examples of semantic alignment between single-channel, as well as multi-channel con-trols discovered for the parent model (StyleGAN2 trained on FFHQ) and a child model (AFHQdogs). While the analogy between hair in humans and fur in dogs seems intuitive, there are alsosome less obvious analogies, such as hair length and ear length.
Figure 13: During transfer learning between domains, we can observe a smooth transition in imagesgenerated from the same latent code z ∈ Z . The top row demonstrates this for transfer from FFHQto AFHQ dogs, while the bottom rows shows this for transfer from AFHQ dogs to cats. The numberof epochs is indicated above each column. The most significant visual changes occur in early epochs(0-l6), while later epochs mainly improve image quality and realism without significant changes insemantic attributes.
Figure 14: Some degree of semantic alignment is present even when the source and target domainsare very dissimilar. In the top two rows, we show that the latent direction that controls pose in theparent FFHQ model still controls pose in the child LSUN church model. In the bottom two rows,we examine a double transfer, with FFHQ as parent, AFHQ dog as child and LSUN bedroom asgrandchild. The pose direction in FFHQ still controls the pose in the grandchild bedroom model.
Figure 15: To understand whether locality bias contributes to semantics transfer, we fine-tune apretrained FFHQ model in 256×256 resolution, to (i) a FFHQ dataset shifted 60 pixels to the right,(ii) a FFHQ dataset shifted 60 pixels downward, and (iii) a FFHQ dataset flipped upside-down. Weexamine the semantics transfer for 5 channels across different layers and different semantic regions.
Figure 16: To invert real images of animal faces to different latent spaces, we examine both encodersand latent optimization based methods. We use the pSp encoder (Richardson et al., 2021) as abackbone and modify it to embed into W, Z, and Z+ (Song et al., 2021) spaces. For the W+space, we use e4e (Tov et al., 2021), which also uses pSp (Richardson et al., 2021) as backbone.
Figure 17: Comparison of I2I results (dog2wild in the top four rows, cat2dog in the four bottomones) for the different inversions shown in Figure 16. The color palette appears to be wrong forboth W+ and W encoding, especially for the dog to wildlife translation. This is not surprising,since the mapping function changes during fine tuning (see Figure 1), affecting the color palette,and inverting into the W or W+ spaces ignores the difference between the mapping functions of theparent and child. Translations via Z+ or Z+opt inversion also suffer from occasional color artifacts(mainly in the dog2wild examples). Translations via either Z or Zopt provide satisfactory results.
Figure 18: To invert real images of human faces to different latent spaces, we examine both encodersand latent optimization based methods. We use the pSp encoder (Richardson et al., 2021) as abackbone and modify it to embed into W space. For the W+ space, we use e4e (Tov et al., 2021),which also uses pSp (Richardson et al., 2021) as backbone. We also experimented with using thepSp encoder to Z, and Z+ (Song et al., 2021) spaces, but training does not converge and results areunrealistic. For optimization-based inversion, we modify the optimization code from StyleGAN2(Karras et al., 2020b) to W, W+, Z or Z+ spaces. In terms of reconstruction quality alone, W+typically yields the best inversions; however, as we show below, better reconstruction does notnecessarily yield the best image translation.
Figure 19: Comparison of I2I results (for real faces to cartoon-like, using FFHQ parent and Megachild) for the different inversions shown in Figure 18. Translation results via Wopt and W +optcontain strong artifacts. In our subjective opinion, translation via Zopt achieves the most cartoonishlook. However, translations via W bear closer resemblance to the input portrait, while still achievinga satisfactory cartoonish look. As discussed in the text, this may be attributed to the fact that,for similar domains, the mapping function changes little during fine-tuning, resulting in pointwisealignment of the W spaces of the parent and child models.
Figure 20: Image Toonification using our Zopt method.
Figure 21: Aligned models enable effective image translation between dissimilar domains (humanface and dog face). Some interesting analogies emerge in these translations. For example, as thehuman hair becomes longer, so does the dog’s fur, while the dog’s ears change from “candle flame”ears, to “bat” ears, and finally to folded (“down-pointing”) ears. The fur color is mainly determinedby the human hair color, and the dog pose mimics that of the human.
Figure 22: Reference-based image translation. Given a real dog image as source and a real catimage as reference, we aim to obtain a cat image that keeps the content (mainly pose) from thesource and the style (fur texture and color) from the reference. We first invert the input real imagesto latent space of StyleGAN, then take style codes for all layers below n (low resolution) from thesource, and style codes for layers above or equal to n (high resolution) from the reference. Thelayer index n is indicated above each column. Thus, index 0 represents the inverted reference, andindex 23 represents the translation of the source to the target domain (cats), while the other indicescorrespond to standard style mixing in StyleGAN. We can see that when n is around 6, the imagescombine the pose of the source with the style of the reference.
Figure 23: A qualitative comparison of reference-based image translation for different methods andspaces. Since here the colors are determined by the higher layers of the generator, whose styleparameters come from the inversion of the reference image, the translation via W+ does not sufferfrom color palette issues. Thus, both translations via W+ and via Zopt look satisfactory.
Figure 24: Given a pair of real images from domain A (top-left) and B (bottom-right), we smoothlytransition between them by interpolating their latent codes in W+, as well as the model weights. t1is the interpolation coefficient for the latent codes, while t2 is the coefficient for the model weights.
Figure 25: Given a pair of real images from domain A (top-left) and B (bottom-right), we smoothlytransition between them by interpolating their latent codes in W+, as well as the model weights. t1is the interpolation coefficient for the latent codes, while t2 is the coefficient for the model weights.
Figure 26: Given a pair of real images from domain A (top-left) and B (bottom-right), we smoothlytransition between them by interpolating their latent codes in W+, as well as the model weights. t1is the interpolation coefficient for the latent codes, while t2 is the coefficient for the model weights.
Figure 27: Given a pair of real images from domain A (top-left) and B (bottom-right), we smoothlytransition between them by interpolating their latent codes in W+, as well as the model weights. t1is the interpolation coefficient for the latent codes, while t2 is the coefficient for the model weights.
Figure 29: Demonstration of our zero-shot cat yaw regression model. The images are from AFHQcat dataset, split into several bins (rows), based on the regressed yaw values. The images shown arerandomly picked from each bin (no cherry picking). The estimated yaw values capture the correcttendency (right facing to left facing), and in most cases appear to be close to the actual yaw degree.
Figure 30: Starting from a pretrained StyleGAN2 model for FFHQ 1024 × 1024 resolution as parent,we use its weights to initialize models for 512 × 512 or 256 × 256 resolution. Before fine tuning(FT), it only generates low contrast images. After fine tuning (“Original” column), similar imageswith the same attributes (identity, hair length, gender, etc.) as parent model are generated given thesame code z ∈ Z . Note that the generated images are not pixel-wise identical, but the different stylechannels retain their semantic function, as demonstrated by the four rightmost columns.
Figure 31: A comparison between layer swapping and model weight interpolation. We demonstratetransitioning between FFHQ and Mega using three different ways. Layer swapping:AB means usinga hybrid model whose low resolution layers come from model A, and high resolution layers frommodel B, while Layer swapping:BA means the opposite roles (low from B, high from A). The reso-lution at which the switching occurs is shown above each result. The swapping resolution used byToonify (Pinkney & Adler, 2020) is either 16 × 16 or 32 × 32. Weight interpolation instead linearlyinterpolates the weights of all layers between model A and B. The interpolation ratio is shown shownabove each result.
Figure 32: A comparison between layer swapping and model weight interpolation. Here we demon-strate transitioning between AFHQ dog and cat. Refer to Figure 31 for more details.
Figure 34: Generative image translation from parent FFHQ model to child LSUN church modeland grandchild FFHQ using the same latent code z. Since the domain gap between FFHQ andLSUN church is too large, we can barely see any correspondence. But the parent FFHQ model andgrandchild FFHQ models generate faces with highly similar attributes and identity. This impliesthat knowledge that was not transferred from task A (FFHQ generation) to task B (LSUN churchgeneration), is only hidden in the latter model’s latent space, rather than forgotten.
Figure 35: Applying the “Beard” and “Black hair” manipulation directions from parent FFHQ modelto a child LSUN church model. The manipulation directions are discovered by StyleCLIP (Patashniket al., 2021). Most manipulation directions from the FFHQ parent do not change anything in thechild church model. Surprisingly, the beard direction from FFHQ appear to control the amount oftrees in the church model to some extent, and the black hair direction from FFHQ makes the churchbuilding darker in the child model.
Figure 36: Semantic alignment between parent and grandchild model. We first train a StyleGANmodel on FFHQ (parent), then fine tune on LSUN church (child), and finally fine tune back to FFHQ(grandchild). We can see that the same channel still controls the same attribute between parent andgrandchild model. Interestingly, channel 15_45 controls lipstick in the parent model, but makesthe face slightly pink in the grandchild model. Although the exact function has changed after finetuning, it is still semantically related to the original function.
Figure 37: We demonstrate the ability of our method to perform I2I tasks that only change texture,while preserving the structure. Although this dataset has paired edge maps and shoe images, thepairing information is not used by our method. We slightly blur the edge maps to make the imagesmore continuous. We first train a StyleGAN model on the shoes dataset (parent), then fine tuneon edge maps dataset (child). Since edge maps mostly represent the structure of objects, and donot contain color or texture, we train an e4e encoder to W+ from whose output we only use theparts that control generator resolutions below 32 × 32 (same as was done for multi-modal imagetranslation). The parts that control higher-resolution layers are sampled, yielding multiple possibleshoe images (sharing the same structure) for each edge map.
