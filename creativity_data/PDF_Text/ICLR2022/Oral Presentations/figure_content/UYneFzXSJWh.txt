Figure 1: Given a good feature extractor (top-left), a randomly initialized head is added to mapfeatures to outputs and we can (a) fine-tune all the model parameters or (b) linear probe, whichfreezes the feature extractor and trains only the head. We run experiments on ten distribution shifts.
Figure 2: A toy version of our theory illustrating why fine-tuning distorts features, with inputs in 2D.
Figure 3: We plot the OOD accuracy against ID accuracy on Living-17 for the three methods we con-sider, when we start from three different pretrained models (CLIP ResNet-50, CLIP ViT-B/16, MoCo-V2 ResNet-50). The line for linear probing and LP-FT lie above fine-tuning which suggests that theyhave higher effective robustness. Each point is produced by averaging over three random seeds.
