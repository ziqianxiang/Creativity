Figure 1: (Left) The MIDI-DDSP architecture. MIDI-DDSP extracts interpretable features at theperformance and synthesis levels, building a modeling hierarchy by learning feature generation ateach level. Red and blue components indicate encoding and decoding respectively. Shaded boxesrepresent modules with learned parameters. Both expression features and notes are extracted directlyfrom synthesis parameters. (Right) Synthesizers have wide range of control, but struggle to conveyrealism. Neural audio synthesis and concatenative samplers can produce realistic audio, but havelimited control. MIDI-DDSP enables both realistic neural audio synthesis and detailed user control.
Figure 2: An example of detailed user control. Given an initial generation from the full MIDI-DDSPmodel (top), an expert musician can adjust notes (blue), performance attributes (green), and low-level synthesis parameters (yellow) to craft a personalized expression of a musical piece (bottom).
Figure 3: Separate training procedures for the three modules in MIDI-DDSP. (Left) The DDSP In-ference module predicts synthesis parameters from audio and is trained via an audio reconstructionloss on the resynthesized audio. (Middle) The Synthesis Generator module predicts synthesis pa-rameters from notes and their expression attributes (shown as a 6-dimensional color map) and istrained via a reconstruction loss and an adversarial loss. (Right) The Expression Generator moduleautoregressively predicts note expression given a note sequence and is trained with teacher forcing.
Figure 4: In MIDI-DDSP, manipulating note-level expression can effectively change the synthesis-level quantities. We show by taking a test-set sample (middle row) and adjusting each expressioncontrol value to lowest (bottom row) and highest (upper row), how each synthesis quantities (right-most legend) would change. The dashed gray line in each plot indicates the note boundary.
Figure 5: (left) Comparing the log-scale Mel spectrograms of synthesis results from test-set note se-quences, MIDI-DDSP synthesizes more realistic audio (more similar to ground-truth and DDSP In-ference) than prior score-to-audio work MIDI2Params (enlarged in Figure 7). The synthesis qualityis also reflected in the listening study (right), where the MIDI-DDSP synthesis is perceived as morerealistic than the professional concatenative sampler Ableton and the freely available FluidSynth.
Figure 6: MIDI-DDSP can take input from different sources (human or other models) by design-ing explicit latent representations at each level. A full hierarchical generative model for music canbe constructed by connecting MIDI-DDSP with an automatic composition model. Here, we showMIDI-DDSP taking note input from a score level Bach composition model and automatically syn-thesizing a Bach quartet by generating explicit latent for each level in the hierarchy.
Figure 7: The enlarged log-scale Mel spectrograms of synthesis results in Figure 5B	Model DetailsB.1	DDSP SynthesizerDifferentiable Digital Signal Processing (DDSP) (Engel et al., 2020a) enables differentiable audiosynthesis by using a harmonic plus noise model (Serra & Smith, 1990). The harmonic signal is syn-thesized using a bank of Kh sinusoidal oscillators parameterized by fundamental frequency f0 (t),harmonic amplitudes a(t), and harmonic distribution h(t). The noise signal is synthesized by afiltered noise synthesizer parameterized by noise magnitudes η(t). Due to the strong inductive biasintroduced by DDSP, the synthesis parameters are highly interpretable, i.e., opposed to some high-dimensional learned latent vector producing audio, with DDSP models the network’s output is inputto the harmonic plus noise model, whose parameters are interpretable by definition. For the DDSPsynthesis used in this work, s(t) = (f0(t), a(t), h(t), η(t)), where f0(t) ∈ R1×t, a(t) ∈ R1×t,h(t) ∈ R60×t,η(t) ∈ R65×t.
Figure 8: The effect of modifying the ‘Volume’ and ’Volume Fluctuation’ note expression for asample. Each row shows the amplitudes of the notes when fixing ’Volume Fluctuation’ and changing’Volume’, while each column shows the amplitudes of the notes when fixing ’Volume’ and changing’Volume Fluctuation’. The number indicates the amount of modification to the note expressioncontrol where (+0, +0) is the original sample. Upper figure shows the spectrogram of generations,and the bottom figure shows the amplitude of the generations. The cartoon on the side indicates howthe modified feature would change the synthesis parameters. The gray dash line in the bottom figureindicates the note boundaries.
Figure 9: The effect of modifying different note expression parameters on an existing sample. Thenumber indicates the amount of modification to the note expression control where (+0) is the originalsample. Upper figure shows the spectrogram of generations, and the bottom figure shows the quan-tity of the generations affected by the control. The cartoon on the side indicates how the modifiedfeature would change the synthesis parameters. The gray dash line in the bottom figure indicates thenote boundaries.	16Published as a conference paper at ICLR 2022Figure 10: The architecture of the DDSP Inference. The DDSP Inference module extract f0 andloudness from audio, and use an 8-layer CNN to extract features from Mel-spectrogram. A bi-directional LSTM takes in all features from audio and predict synthesis parameters.
Figure 10: The architecture of the DDSP Inference. The DDSP Inference module extract f0 andloudness from audio, and use an 8-layer CNN to extract features from Mel-spectrogram. A bi-directional LSTM takes in all features from audio and predict synthesis parameters.
Figure 11: The architecture of the Synthesis Generator. The Synthesis Generator is a GAN whosegenerator (left) takes in per-note Expression Controls and instrument embedding as a conditioningsequence (red box, left) and produces DDSP synthesis parameters, i.e., f0, Amplitudes, HarmonicDistribution, and Noise Magnitudes (green boxes, middle). These synthesis parameters get turnedinto audio by the DDSP modules.
Figure 12: The architecture of the discriminator used in the Synthesis Generator.
Figure 13: The effect of GAN in overcoming the “over-smoothing” problem in harmonic distributiongeneration. From top to bottom: (top) the ground-truth harmonic distribution of the test-set sample,(middle) the harmonic distribution of the same sample predicted without a discriminator used intraining, (bottom) the harmonic distribution predicted with discriminator and adversarial training.
Figure 14: The architecture of the Expression Generator. The Expression Generator consists of twoparts: a single-layer bidirectional GRU extracts context information from input, and a two-layer au-toregressive GRU generates note expression. The input to the bi-directional GRU is a concatenationof pitch embedding vector, duration feature vector and instrument embedding.
