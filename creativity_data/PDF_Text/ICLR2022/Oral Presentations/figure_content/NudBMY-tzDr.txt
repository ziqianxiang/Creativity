Figure 1: (a) We aim to generate natural language descriptions of individual neurons in deep networks. (b)We first represent each neuron via an exemplar set of input regions that activate it. (c) In parallel, We collect adataset of fine-grained human descriptions of image regions, and use these to train a model of p(description |exemplars) and p(description). (d) Using these models, We search for a description that has high pointwisemutual information with the exemplars, ultimately generating highly specific neuron annotations.
Figure 2: Examples of milan descriptions on the generalization tasks described in Section 4. Even highlyspecific labels (like the top boundaries of horizontal objects) can be predicted for neurons in new networks.
Figure 3: Examples of milan failures.
Figure 4: ResNet18 accuracy on the ImageNet validation set as units are ablated (left, middle), and distributionof neurons matching syntactic and structural criteria in each layer (right). In each configuration, neurons arescored according to a property of their generated description (e.g., number of nouns/words in description, etc.),sorted based on their score, and ablated in that order. Neurons described with adjectives appear crucial for goodperformance, while neurons described with very different words (measured by word embedding difference;max word diff.) appear less important for good performance. Adjective-selective neurons are most prevalent inearly layers, while neurons with large semantic differences are more prevalent in late ones.
Figure 5: Change in # offace neurons found by mi-lan (each pair of pointsis one model architecture).
Figure 6: (a) The blurred ImageNet dataset.
Figure 7: Network editing. (a) We trainan image classifier on a synthetic dataset inwhich half the images include the class labelwritten in text in the corner. (b) We eval-uate the classifier on an adversarial test set,in which every image has a random textuallabel. (c) Nearly a third of neurons in thetrained model model detect text, hurting itsperformance on the test set.
Figure 8: ResNet18 accuracy on the adver-sarial test set as neurons are incrementallyablated. Neurons are sorted by the modelâ€™svalidation accuracy when that single neuronis ablated, then ablated in that order. Whenablating neurons that select for the spurioustext, the accuracy improves by 4.9 points.
Figure 9:	Screenshots of the Amazon Mechanical Turk forms we used to collect the CaNCAn dataset. (a) Thequalification test. Workers are asked to pick the best description for two hand-chosen neurons from a modelnot included in our corpus. (b) The annotation form. Workers are shown the top-15 highest-activating imagesfor a neuron and asked to describe what is common to them in one sentence.
Figure 10:	Example human annotations for neuron exemplars in milannotations, which contains annota-tions for neurons in seven networks. Each set of images is annotated by three distinct human participants.
Figure 11: Neuron captioning model. Given the set of top-activating images for a neuron and masks for theregions of greatest activation, we extract features maps from each convolutional layer of a pretrained imageclassifier. We then downsample the masks and use them to pool the features before concatenating them into asingle feature vector per image. These feature vectors are used as input to the decoder attention mechanism.
Figure 12:	Randomly chosen examples of milan-generated descriptions from the generalization experimentsof Section 4.
Figure 13: Examples of ablated neurons for each condition Section 5, chosen from among the first 10 ablated.
Figure 14: Cut-and-paste adversarial attacks highlighting non-robust behavior by a neuron that scored highon the max-word-diff criterion of Section 5. (a) MILAN finds this neuron automatically because the generateddescription mentions two or more dissimilar concepts: animals and vehicles. The neuron is directly connectedto the final fully-connected output layer, and strongly influences amphibian, hermit crab, and jeep predictionsaccording to the connection weights. (b) To construct adversarial inputs, we pick three images from the Ima-geNet validation set that do not include concepts detected by the neuron. (c) We then select a different set ofimages to act as distractors that do include the concepts detected by the neuron. (d) By cutting and pasting thecentral object from the distractor to the original image, the model is fooled into predicting a class label that iscompletely unrelated to the pasted object: e.g., it predicts amphibian when the military vehicle is pasted.
Figure 15: Same as Fig. 8, but shows ac-curacy on the validation dataset, which isdistributed identically to the training dataset.
