Figure 1: The Filtered-CoPhy benchmark suite contains three challenging scenarios involving 2D or3D rigid body dynamics with complex interactions, including collision and resting contact. Initialconditions A are modified to C by an intervention. Initial motion is indicated through arrows.
Figure 2: Impact of temporal fre-quency on dynamics, 3D trajecto-ries of each cube are shown. Blackdots are sampled at 5 FPS, col-ored dots at 25 FPS. Collisions be-tween the red cube and the groundare not well described by the blackdots, making it hard to infer physi-cal laws from regularities in data.
Figure 3: We de-render visual input into a latent space composed of a dense feature map F modelingstatic information, a set of keypoints k, and associated coefficients c. We here show the trainingconfiguration taking as input pairs (Xsource, Xtarget) of images. Without any supervision, a trackingstrategy emerges naturally through the unsupervised objective: We optimize reconstruction of Xtarget,given features from XSoUrCe and keypoints+coefficients from Xtarget.
Figure 4: During training, We disconnect the dynamic prediction module (CoDy) from the renderingmodule (decoder). On test time, we reconnect the two modules. CoDy forecasts the counterfactualoutcome D from the sparse keypoints representation ofAB and C. The confounders are discoveredin an unsupervised manner and provided to the dynamical model.
Figure 5: Network R learnsa distortion from multiple ori-ented ellipses to target shapes.
Figure 6: Visualization of the counterfactual video prediction quality, comparing our proposedmodel (Filtered CoPhy) with the two baselines, PhyDNet and UV-CDN, over different timestamps.
Figure 7: Experimental tuning of the threshold parameter. We generate an unconstrained subset ofBlocktowerCF and plot the percentage of identifiable experiments as function of the threshold ε.
Figure 8: Impact of the choice of temporal resolution. Left: We check the identifiability constraintby training a model to predict the cube masses in BlocktowerCF from the observed trajectoryAB. The model is a graph neural network followed by a gated recurrent unit. Right: We check theaugmentation of the sampling rate by training an agent to forecast a 1 second-length trajectory fromthe states of the previous second.
Figure 9: Visual examples of the impact of temporal resolution on dynamical information for eachtask in Filtered-CoPhy. Black dots are sampled at 6 FPS while red dots are sampled at 25 FPS.
Figure 10: During the dataset generation process, we carefully balance combinations of masses, i.e.
Figure 11: Reconstructions produced by the de-rendering module. Our model correctly marks eachobject in the scene and achieves satisfactory reconstruction.
Figure 12: Navigating the manifold of the latent coefficient representation. Each line correspondsto variations of one keypoint coordinate or coefficient and shows the effect on a single cube.
Figure 13: Example of reconstructed image by our-derendering module and the Transporter.
Figure 14: We evaluate the Transporter with a varying number of keypoints to reconstruct imagesregularly sampled in a trajectory while having the target keypoints fixed. Even if the keypointsare not moving, the Transporter still manages to reconstruct a significant part of the image, whichindicates that the keypoints are not fully responsible for encoding the dynamics of the scene.
Figure 15: Temporal inconsistency in long-range reconstruction. We show the keypoints discoveredon images taken from different time steps (black dots). We also compute the 2D location of thecenter of mass of each object in the scene (white dots). Our de-rendering module accurately tracksthe centers of mass, which have never been supervised.
Figure 16: Effect of the do-operation on the quality of the forecasted video. (left) our methodgeneralizes well to a wide range of ”Move” operation amplitudes. (right) We observe a difference of3dB in favor of the Move do-operation, which is unsurprising, as itis the least disturbing interventionE.2 Impact of the do-operationsWe also measure the impact of the do-operation types on the video forecasting. Fig. 16 (left) isobtained by computing PSNR for each example of the training set and reporting the result on a2D graph, depending on the amplitude of the displacement that characterizes the do-operation. Weapplied the same method to obtain Fig.16 (right) that focuses on the type of do-operation, that ismoving, removing or rotating an object. These figures are computed using the 2N keypoints models.
Figure 17: We evaluate our method on a real-world dataset Blocktower IRL. After fine-tuning, CoDymanages to accurately forecast future frames from real videos.
Figure 18: Qualitative performance on the BlocktowerCF (BT-CF) benchmark.
Figure 19: Qualitative performance on the BallsCF (B-CF) benchmark.
Figure 20: Qualitative performance on the CollisionCF (C-CF) benchmark.
