Figure 1: Accuracy of the model as a functionof the regularization weight α, with and withoutoutliers. Note how the model performance exhibitsa turning point with outliers whereas increasingIn particular, this implies that the common prac-tice of tuning hyperparameters without differen-tial privacy and then using the hyperparametervalues selected to repeat training one last timewith differential privacy is not ideal. In Sec-tion 3, we will in particular show how training the value of α is detrimental without outliers.
Figure 2: Renyi DP guaranteesfrom Corollary 4 for various ex-pected numbers of repetitions ofthe logarithmic distribution (i.e.,truncated negative binomial withη = 0), compared with base al-gorithm (0.1-ZCDP) and naivecomposition.
Figure 3: Renyi DP guaranteesfor repetition using different dis-tributions or naive compositionwith mean 10, with 0.1-zCDPbase algorithm.
Figure 4: Privacy versus ex-pected number of repetitionsusing different distributions ornaive composition. Renyi DPguarantees are converted to ap-proximate DP 一 i.e., We plot εsuch that we attain (ε, 10-6)-DP.
Figure 5: Expected quantileof the repeated algorithm A asa function of the final privacyguarantee (ε, 10-6)-DP for vari-ous distributions K, where eachinvocation of the base algorithmQ is 0.1-zCDP.
Figure 6: Final success prob-ability (β) of the repeated algo-rithm A as a function of the finalprivacy guarantee (ε, 10-6)-DPfor various distributions, whereeach invocation of the base algo-rithm Q has a 1/100 probabilityof success and is 0.1-zCDP.
Figure 7: Accuracy of the CNNmodel obtained at the end ofthe hyperparameter search, forthe different distributions on thenumber of repetitions K we con-sidered. We report the meanover 500 trials of the experi-ment.
Figure 8: Upper and lower bounds for Renyi DP of conditional sampling. For each λ, We pick theparameters S and t such that Dλ (QIlQ0) = Dλ (Q0kQ) = 0.1 ∙ λ and We plot the upper bound fromthe second inequation in Corollary 16 along with the exact value of Dλ (QS kQ0S).
