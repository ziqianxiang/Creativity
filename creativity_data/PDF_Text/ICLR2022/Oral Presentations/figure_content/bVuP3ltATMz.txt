Figure 1: A summary of a few of our findings: (1) Pretrained models fine-tuned with DP-Adam hasstrong performance. (2) Fine-tuning larger models produces better results. (3) Fine-tuned RoBERTa-large under DP at = 3 outperforms TextHide (the extension of InstaHide (Huang et al., 2020b) fortext classification) with BERT-base. Non-private generation baseline numbers are based on thosereported by Wiseman et al. (2018).
Figure 2: Large batch sizes andlearning rates lead to the best per-formance when E is fixed. Redlines divide heat map into four pan-els. Top and bottom correspond tolow and high learning rate regimes;left and right correspond to smalland large batch regimes.
Figure 3: Left: Effective noise multiplier decreases with increasing sampling rate for various fixed S.
Figure 4: Left: Ghost clipping is 3 times more memory efficient than Opacus and is almost asefficient as non-private training for typical sequences across model sizes. For GPT-2-large, we wereunable to fit single-example micro batches together with gradient accumulation with Opacus or JAXon a TITAN RTX GPU (24 GBs of VRAM). Right: DP optimization with ghost clipping processes~10% more examples than the approach by Lee & Kifer (2020) under unit time for GPT-2-large.
Figure 5: Larger and better pretrained models consistently lead to better private fine-tuned perfor-mance on sentence classification and language generation tasks.
Figure 6: “Square-root” relationship underestimates the noise multiplier for small batch sizes.
Figure 7:	Better text labels help private learning more than non-private learning.
Figure 8:	Additional results on hyperparameter sensitivity.
Figure 9: Left: δ affects performance marginally. Right: affects performance more significantlywhen < 2. Errorbars are one standard deviation away from the mean over five independent runs.
