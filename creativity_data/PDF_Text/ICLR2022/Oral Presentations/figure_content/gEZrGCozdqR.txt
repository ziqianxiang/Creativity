Figure 1: Top: overview of instruction tuning and FLAN. Instruction tuning finetunes a pretrainedlanguage model on a mixture of tasks phrased as instructions. At inference time, we evaluate onan unseen task type; for instance, we could evaluate the model on natural language inference (NLI)when no NLI tasks were seen during instruction tuning. Bottom: performance of zero-shot FLAN,compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuningimproved performance substantially out of ten We evaluate. NLI datasets: ANLIRI-R3, CB, RTE.
Figure 2:	Comparing instruction tuning with pretrain-finetune and prompting.
Figure 3:	Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).
Figure 4:	Multiple instruction templates describing a natural language inference task.
Figure 5: Zero-shot performance of FLAN compared to LaMDA-PT 137B, GPT-3 175B, and GLaM64B/64E on natural language inference, reading comprehension, closed-book QA, and translation.
Figure 6: Adding additional task clusters to instruction tuning improves zero-shot performance onheld-out task clusters. The evaluation tasks are the following. Commonsense: CoPA, HellaSwag,PiQA, and StoryCloze. NLI: ANLI R1-R3, QNLI, RTE, SNLI, and WNLI. Closed-book QA: ARCeasy, ARC challenge, Natural Questions, and TriviaQA.
Figure 7: Whereas instruction tuning helps largemodels generalize to new tasks, for small models itactually hurts generalization to unseen tasks, poten-tially because all model capacity is used to learn themixture of instruction tuning tasks.
Figure 9: Adding few-shot exemplars to FLAN is a complementary method for improving theperformance of instruction-tuned models. The orange bars indicate standard deviation amongtemplates, averaged at the dataset level for each task cluster.
Figure 10: Instruction-tunedmodels respond better to contin-uous inputs from prompt tuning.
Figure 11: Effect of datasets per task cluster and templates per dataset on performance on threeheld-out clusters: NLI, commonsense reasoning, and closed-book QA. Adding more datasets per taskcluster substantially improves performance. Using more templates per dataset, however, only hada very small effect on performance, which disappeared when there were sufficient dataset per taskcluster.
Figure 12: Like GPT-3, we also measured performance on cleaned versions of our datasets, whichhad high confidence to be unseen in the pretraining data of FLAN. We do not see a correlation thatFLAN performed better on evaluation sets for which examples occurred more often in the pretrainingdata. When the percent of clean data is very small, there are fewer examples for computing the cleanperformance, which leads to high variance.
Figure 13:	For sentiment analysis, FLAN changes the answer appropriately when the question isflipped.
Figure 14:	For question answering, FLAN can answer a question in another language when instructedto do so. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 andtop k of 40.
Figure 15:	FLAN can perform zero-shot rule-based manipulations.
Figure 16:	FLAN can make zero-shot recommendations. Multiple FLAN outputs are generated viarandom sampling with a temperature of 0.9 and top k of 40.
Figure 17:	FLAN can be used in a zero-shot fashion to generate data, such as utterances that areconsistent with a given intent. Multiple FLAN outputs are generated via random sampling with atemperature of 0.9 and top k of 40.
Figure 18:	FLAN can be used for zero-shot query expansion. Multiple FLAN outputs are generatedvia random sampling with a temperature of 0.9 and top k of 40.
Figure 19:	FLAN can perform zero-shot tasks relevant to assisted-writing applications. MultipleFLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40.
Figure 20:	FLAN can be used for zero-shot word formation. Multiple FLAN outputs are generatedvia random sampling with a temperature of 0.9 and top k of 40.
Figure 21:	Open-ended generation tasks by FLAN. The carrot story was from sampling sixteenoutputs with a minimum length of 150 and choosing the highest probability output.
Figure 22: Example failure cases for FLAN. Left: FLAN fails to perform a simple task of returningthe nth word. Right: FLAN translates a question instead of answering it. Multiple FLAN outputs aregenerated via random sampling with a temperature of 0.9 and top k of 40.
