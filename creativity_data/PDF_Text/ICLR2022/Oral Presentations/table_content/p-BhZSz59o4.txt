Table 1: Top-1 accuracy on ImageNet-1K. We evaluate base- (“-B”) and large-size (“-L”) models atresolutions 224 X 224 and 384 X 384. ^: iGPT-1.36B contains 1.36 billion parameters, while othersare base-size models.t ViT384 -B-JFT300M is pretrained with the “masked patch prediction” taskon Google,s in-house 300M images, while others use ImageNet.
Table 3: Results of semantic segmentation onADE20K. We use SETR-PUP (Zheng et al., 2020)as the task layer and report results of single-scaleinference.
Table 2: Convergence curves of trainingDeiT from scratch and fine-tuning BEiT onImageNet-1K.
Table 4: Ablation studies for BEiT pre-training on image classification and semantic segmentation.
Table 5: Ablation studies of architecture variants on image classification and semantic segmentation.
Table 6: Top-1 accuracy on ImageNet-1K fine-tuning. We evaluate models at resolutions 3842 and5122.
Table 7:	Performance comparison on the ADE20K semantic segmentation. We folloW SWin-L (LiUet al., 2021b) to Use UperNet (Xiao et al., 2018) as the task layer and evalUate at resolUtion 640 × 640.
Table 8:	Top-1 accUracy on ImageNet-1K Using different image tokenizers dUring pre-training. Forimage reconstrUction, We report mean absolUte error of normalized RGB valUes. The reimplementedimage tokenizer is trained on ImageNet-1K WithoUt labels.
Table 9: Linear probing accuracy on ImageNet. “*" denotes that iGPT-XL uses concatenation of fivelayers for linear probing, while others use the features of single layer.
Table 10: We train the pre-training tasks of BEiT and DINO (Caron et al., 2021) in the way ofmulti-task learning. We report the performance by fine-tuning on ImageNet-1K image classificationand ADE20K semantic segmentation. For ADE20K, we use SETR-PUP (Zheng et al., 2020) as thetask layer and report the mIoU score of single-scale inference. The pre-training throughput measuresthe speed, where larger numbers indicate faster pre-training.
Table 11: Top-1 accuracy of image classification on CIFAR-100. The models are at resolution224 × 224, except ViT384 uses 384 × 384. The results, unless otherwise indicated, are all obtainedby base-size models. *: result is taken from (Chen et al., 2021).
Table 12: Hyperparameters for pre-training BEiT on ImageNet-1K.
Table 13: Hyperparameters for fine-tuning BEiT on ImageNet-1K and CIFAR-100.
Table 14: Hyperparameters for fine-tuning BEiT on ADE20K.
