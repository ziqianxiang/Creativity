Table 1: The benefits of weighted training for different learning paradigms under different settings.
Table 2: Compared to Table 1, more training examples are considered here. Here we only considerthe three tasks in Ontonotes 5.0, i.e., PoS tagging, predicate detection, and NER. For each setting, wechoose one task as the target task and the remaining two tasks as source tasks. We randomly choose20K training sentences for each source task respectively. As for the target task, we randomly choose500, 600, 1000 training sentences for PoS tagging, predicate detection, and NER respectively.
Table 3: Additional experiments to illustrate the benefits of weighted joint training compared to jointtraining. For each setting, the task in the bold text is the target task and the remaining tasks are sourcetasks. The improvement from weighted training in the joint training paradigm under various settingsindicates the effectiveness of TAWT.
Table 4: The performance for the analysis on the ratio between the training sizes of the source andtarget datasets in Fig. 1. In this part, we use NER as the target task and source tasks include PoStagging and predicate detection. We keep the training size of the target task as 500 and change theratio from 1 to 128 on a log scale. Single-task learning denotes learning only with the small targetdata, and upper bound denotes learning with the target data that has the same size as the overall sizeof all datasets in cross-task learning. "-" means that we do not have enough training data for the upperbound in that setting. The sampling strategy7 for training examples we used for the analysis is a littledifferent from that used in the experiments in Table 1, so that the performance of single-task learningfor the same setting can be a little different.
Table 5: The performance for the analysis on the training size of the target dataset in Fig. 1. In thispart, we use NER as the target task and source tasks include PoS tagging and predicate detection.
Table 6: The final learned weights on tasks in the experiments in Table 1. For each setting, the finalweights on three source tasks are represented by a three-element tuple. The source tasks are alwaysorganized in the following order: PoS tagging, chunking, predicate detection, and NER. For example,the tuple (0.68, 0.01, 0.31) for the target task Pos tagging in the weighted pre-training indicate thatthe final weights on the three source tasks (chunking, predicate detection, NER) are 0.68, 0.01, and0.31 respectively. As for TAWT in the joint training, we have extra weight on the target task, which isbold in the second line.
Table 7: Comparison between dynamic weights and fixed final weights. There are four tasks in total,PoS tagging, chunking, predicate detection, and NER. For each setting, we choose one task as thetarget task and the remaining three tasks are source tasks.
