Table 1: All datasets have labeled source, validation, and target data, as well as unlabeled data fromone or more types of domains, depending on what is realistic for the application.
Table 2: The in-distribution (ID) and out-of-distribution (OOD) performance of each method oneach applicable dataset. Following WILDS 1.0, We ran 3-10 replicates (random seeds) for eachcell, depending on the dataset. We report the standard deviation across replicates in parentheses; thestandard error (of the mean) is lower by the square root of the number of replicates. Fully-labeledexperiments use ground truth labels on the “unlabeled” data. We bold the highest non-fully-labeledOOD performance numbers as well as others where the standard error is within range. Below eachdataset name, we report the type of unlabeled data and metric used.
Table 3: Data for iWildCam2020-wilds. Each domain corresponds to a different camera trap.
Table 4: Data for Camelyon 1 7 -wilds. Each domain corresponds to a different hospital.
Table 5: Data for FMoW-wilds. Each domain corresponds to a different year and geographicalregion.
Table 6: Data for PovertyMap-wilds (Fold A). Each domain corresponds to a different countryand whether the image was from a rural or urban area.
Table 7: Data for GlobalWheat-wilds.
Table 8: Source, validation, and test domains for GlobalWheat-wilds.
Table 9: Extra domains for GlobalWheat-wilds.
Table 10: Data for OGB-MolPCBA. Each domain corresponds to a different molecule scaffoldstructure.
Table 11: Data for CivilComments-wilds. All of the splits are identically distributed.
Table 12: Data for Amazon-wilds. Each domain corresponds to a different reviewer.
Table 13: The batch sizes of each dataset from the original Wilds 1.0 paper and the batch sizesused in Wilds 2.0, which correspond to the maximum that can fit into 12GB of GPU memory.
Table 14: The number of epochs (complete passes over the labeled data) used for each dataset,specified for the ERM baseline as well as different ratios of unlabeled to labeled data within a batch.
Table 15: The in-distribution vs. out-of-distribution test performance of each method on DomainNet(real → sketch). We also included the results of applying weak instead of strong augmentation onlabeled examples for Pseudo-Label and FixMatch. Parentheses show standard deviation across 3replicates.
