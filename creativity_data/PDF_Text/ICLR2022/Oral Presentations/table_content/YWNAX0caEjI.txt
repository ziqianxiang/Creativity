Table 1: Result on PPI datasets (in %). SPNs get consistent improvement on GNNs and CRFs.
Table 2: Accuracy on Cora*, Citeseer*, Pubmed*, DBLP (in %). SPNs achieve the best result.
Table 3: Run time Comparison (in seC).			Table 4: MiCro-F1 with and w/o refinement (in %).				Algorithm	DBLP	PPI	Algorithm	Refine	PPI-2	PPI-10	PPIGAT	23715	460.81	SPN-	w/o	71.52 ± 0.21	94.41 ± 0.21	98.38 ± 0.10CRF (GAT)	500.43	27136.90	GAT	with	71.58 ± 0.20	94.63 ± 0.20	98.68 ± 0.09SPN(GAT)	46.86	962.92	SPN-	w/o	73.93 ± 0.08	91.99 ± 0.04	97.56 ± 0.03			GraphSAGE	with	73.68 ± 0.10	92.49 ± 0.02	97.77 ± 0.022.	Effect of refinement. By solving the proxy optimization problem in Eq. (9), we can obtain anear-optimal joint label distribution on training graphs, based on which we may optionally refine thedistribution with the maximin game in Eq. (3). Next, we study the effect of refinement, and we presentthe results in Tab. 4. By only solving the proxy problem, our approach already achieves impressiveresults, showing that the proxy problem can well approximate the original learning problem. Only ondatasets with sufficient labeled data (e.g., PPI-10, PPI), refinement leads to some improvement.
Table 5: Comparison of learning methods (in %).
Table 6: Micro-F1 of model variants (in %).
Table 7: Analysis of constrained optimization methods for solving the proxy problem.
Table 8: Dataset statistics. ML and MC stand for multi-label classification and multi-class classifica-tion respectively.
Table 9: Learning rate of the node GNN τs .
Table 10: Learning rate of the edge GNN τst .
Table 11: Temperature γ of the edge GNN τst .
Table 12: Node-level accuracy on Cora*, Citeseer*, Pubmed* (in %).
Table 13: Micro-F1 on PPI-10 (in %).
