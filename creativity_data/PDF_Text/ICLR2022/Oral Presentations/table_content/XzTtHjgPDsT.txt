Table 1: Comparison on CATER ObjectTracking. Here, we compare the Top-1 andTop-5 accuracy of Transformers with sharedworkspace and Transformers with self-attention.
Table 2: Here we show the performanceof SCOFF augmented with sharedworkspace attention on the bouncingballs task. We also analyse the ef-fect of varying number of slots in theshared workspace. This also showsthat by increasing the number of slotsperformance decreases hence validat-ing claims regarding bandwidth lim-ited communication channel via sharedworkspace.
Table 3: Generic Hyperparameters for the proposed model (for RIMs)workspace is seen as a Matrix with row wise compartmentalized memories (i.e slots) i.eWq, We, Wv. In the table it corresponds to number of memory slots, number of memoryheads, size of attention head, key size and number of mlp layers in attention. These arethe same hyper-paramter as in RMC (Santoro et al., 2018). We tried two different set ofhyper-parameters (a) where we only have a single slot and (b) where we have 4 slots.
Table 4: Hyperparameters for MultiMNIST TaskEach 32 × 32 image in this dataset is made up of four randomly selected (and augmented) MNISTdigits (resized to 32 × 8) placed side-by-side as shown in figure 10. The digits themselves are selectedindependently of one-another.
Table 5: MultiMNIST Generation Task: We report cross-entropy loss between the generated pixelvalues and the true pixel values on the test set of MultiMNIST Generation Task (smaller numbers arebetter)l⅛μ9∕"ll加川 WdFigure 10: A randomly selected batch of 16 images from the MultiMNIST generation dataset (4 rowsand 4 columns)layers share the same set of parameters. In the case of TIMs+SW, the four mechanisms in these layerscommunicate via a shared workspace (having 2 memory slots). This shared workspace is commonfor all four middles layers and is absent in TIMs where the mechanisms communicate via pair-wisecompetition as proposed in the original paper. TIMs and TIMs+SW architectures are concluded bytwo more monolithic layers which again share the same parameters.
Table 6: Hyperparameters for WikiText-103 Language Modeling TaskResults. We plot the perplexity (per epoch) on the validation set. All models have comparablenumber of parameters (within a 10% difference). We note that TIMs performs poorly on this datasetbut adding shared workspace improves the performance consistently. We also note that sparsityindeed helps as TIMs+HSW performed the best.
