Table 1: Overall performance (averaged accuracy/MSE (Pose) ± 95% confidence interval) underlabel-sharing scenario. MLTI consistently improves the performance under the label-sharing scenario.
Table 2: Ablation Studies under label-sharing scenario. The results are reported by the averagedaccuracy/MSE ± 95% confidence interval.
Table 3: Overall performance (averaged accuracy) under the non-label-sharing scenario. MLTIoutperforms other strategies and improves the generalization ability.
Table 5: Hyperparameters under the label-sharing scenario.
Table 6: Additional compatibility analysis under the label-sharing scenario (evaluation metric: MSEfor Pose and accuracy for other datasets), where the 95% confidence intervals are also reported.
Table 7: Hyperparameters under the non-label-sharing scenario.
Table 8: Results (averaged accuracy ± 95% confidence interval) of full-size miniImagenet andDermNet.
Table 9: Additional compatibility analysis under the setting of the non-label-sharing scenario. Weshow averaged accuracy ± 95% confidence interval.
Table 10: Ablation study under the non-label-sharing scenario. We find that MLTI performs best.
Table 11: Analysis on the heavier base model (ResNet-12) under 1-shot miniImagenet-S and DermNet-S settings.
Table 12: Analysis of interpolation layers. Layer 0, 1 represents randomly select layer 0 or layer 1 forinterpolation. None means vanilla ProtoNet.
Table 13: Results of MLTI with extremely limited tasks. SL and MTL represent methods withsupervised and multi-task training process, respectively.
Table 14: Results on Huffpost and tieredImageNet-S. Here, averaged accuracies ± 95% confidenceintervals are reported.
Table 15: Full table of the overall performance (averaged accuracy ± 95% confidence interval) underthe non-label-sharing scenario.
Table 16: Full table (accuracy ± 95% confidence interval) of the cross-domain adaptation underthe non-label-sharing scenario. A → B represents that the model is meta-trained on A and then ismeta-tested on B.
