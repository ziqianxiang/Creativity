Table 1: Full fine-tuning larger pretrained models with text infilling has best performance. Resultsare dev set accuracies. Best numbers based on two-sample test for each privacy level are in bold.
Table 2: Full fine-tuning performs on par with or outperforms others methods that execute gradientupdate in low dimensional spaces. Results are on E2E from fine-tuning GPT-2.
Table 3: Fine-tuning with DP-Adam yields high quality chit-chat dialog generation models.
Table 4: Default hyperparameters for ablation studies.
Table 5: Hyperparameter search range for different methods.
Table 6: Hyperparameter search range for different methods (continued).
Table 7: Full results on E2E from fine-tuning GPT-2.
Table 8: Full results on DART from fine-tuning GPT-2. Trend is consistent with results on E2E.
Table 9: Fine-tuning under DP prevents unintended memorization of downstream data. Numbersreported are exposure values estimated with the approximation by distribution model approach.
Table 10: Templates and label words borrowed from the work by Gao et al. (2020).
Table 11: Fully fine-tuned GPT-2, GPT-2-medium, and GPT-2-large generations with E2E test tableentries.
Table 12: Fully fine-tuned GPT-2, GPT-2-medium, and GPT-2-large generations with DART testtable entries.
Table 13: Fully fine-tuned GPT-2-medium and DialoGPT-medium generations for Persona-Chatvalidation examples.
Table 14: Text-infilling objective improves the performance of RGP for classification. Numbers areaveraged over three independent runs.
