Table 2: BERTScores for neuron labeling meth-ods relative to human annotations. milan ob-tains higher agreement than Compositional Expla-nations (CE) or NetDissect (ND).
Table 3: BERTScores on held out neurons rela-tive to the human annotations. Each train/test splitevaluates a different kind of generalization, ul-timately evaluating how well milan generalizesto networks with architectures, datasets, and tasksunseen in the training annotations.
Table 4: Average inter-annotatoragreement among human annota-tions, measured in BERTScore.
Table 5: Corpus statistics for MILANNOTATIONS descriptions broken down by model and layer. The # Wordscolumn reports the number of unique words used across all layer annotations, the Len. column reports theaverage number of words in each caption for that layer, and the % columns report the percentage of all wordsacross all captions for that layer that are a specific part of speech.
Table 6: Statistics for milan-generated descriptions on the held-out neurons from the generalization experi-ments of Section 4. Columns are the same as in Table 5.
