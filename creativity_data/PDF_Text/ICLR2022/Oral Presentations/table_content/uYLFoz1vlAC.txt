Table 2: Benchmarks vs. efficient TransformersTable 1: Deep SSMs: The S4 parameterization with Algo-rithm 1 is asymptotically more efficient than the LSSL.
Table 1: Deep SSMs: The S4 parameterization with Algo-rithm 1 is asymptotically more efficient than the LSSL.
Table 3: (Long Range Arena) Accuracy on full suite of LRA tasks. (Top) Original Transformer variants inLRA. Full results in Appendix D.2. (Bottom) Other models reported in the literature.
Table 4: (Speech classification) Transformer, CTM,RNN, CNN, and SSM models. (MFCC) Standardpre-processed MFCC features (length-161). (Raw)Unprocessed signals (length-16000). (0.5 ×) Fre-quency change at test time. X denotes not applicableor computationally infeasible on single GPU.
Table 5: (Pixel-level 1-D image classification)Transformer, RNN, CNN, and SSM models. Ex-tended results + citations in Appendix D.
Table 6: (CIFAR-10 density estimation) As a generic Table 7: (WikiText-103 language modeling) S4 ap-sequence model, S4 is competitive with previous autore- proaches the performance of Transformers with muchgressive models (in bits per dim.) while incorporating no faster generation. (Top) Transformer baseline which2D inductive bias, and has fast generation through its recur- our implementation is based on, with attention re-rence mode.				placed by S4. (Bottom) Attention-free models (RNNs and CNNs).			Model	bpd	2D bias	Images / sec				Transformer	3.47	None	0.32 (1×)	Model	Params	Test ppl.	Tokens / secLinear Transf.	3.40	None	17.85 (56×)		—		PixelCNN	3.14	2D conv.	-	Transformer	247M	20.51	0.8K (1×)Row PixelRNN	3.00	2D BiLSTM	-	GLU CNN	229M	37.2	-PixelCNN++	2.92	2D conv.	19.19 (59.97×)	AWD-QRNN	151M	33.0	-Image Transf.	2.90	2D local attn.	0.54(1.7×)	LSTM + Hebb.	-	29.2	-PixelSNAIL	2.85	2D conv. + attn.	0.13 (0.4×)	TrellisNet	180M	29.19	-Sparse Transf.	2.80	2D sparse attn.	-	Dynamic Conv.	255M	25.0	-S4 (base)	2.92	None	20.84 (65.1×)	TaLK Conv.	240M	23.3	-S4 (large)	2.85	None	3.36 (10.5×)	S4	249M	21.28	48K (60×)to Transformers can still be competitive in these settings. By simply taking a strong Transformerbaseline (Baevski & Auli, 2018) and replacing the self-attention layers, S4 substantially closes thegap to Transformers (within 0.8 ppl), setting SoTA for attention-free models by over 2 ppl.
Table 8: Full results for the Long Range Arena (LRA) benchmark for long-range dependencies in sequencemodels. (Top): Original Transformer variants in LRA. (Bottom): Other models reported in the literature.
Table 9: The values of the best hyperparameters found for classification datasets; LRA (Top) and images/speech(Bottom). LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and LayerNormalization.
Table 10: (Pixel-level image classification.) Citations refer to the original model; additional citation indicateswork from which this baseline is reported.
Table 11: Univariate long sequence time-series forecasting results on four datasets (five cases).
Table 12: Multivariate long sequence time-series forecasting results on four datasets (five cases).
