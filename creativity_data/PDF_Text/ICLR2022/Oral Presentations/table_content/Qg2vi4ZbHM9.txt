Table 1: Number of localized StyleSpace channels for various semantic regions. Each columncorresponds to a semantic region in parent model, and each row to a semantic region in child model.
Table 2: We measUre the extent to which the change in each featUre convolUtion layer (dUring fine-tUning) affects the generated images. Given a parent FFHQ model and a child AFHQ dog model, wereset the featUre convolUtion weights for each resolUtion of the child model to their original valUesin the parent model, and measUre the LPIPS distance between the images generated by child modelbefore and after resetting the weights. A higher LPIPS score indicates a more significant change inimage space. It may be seen that the greatest change is caUsed by resetting the middle resolUtionlayers (32, 64, 128).
Table 3: The nUmber of localized StyleSpace controls for varioUs semantic regions for an FFHQparent model and an FFHQ grandchild model, with training flow from FFHQ (parent) to AFHQ dog(child) then back to FFHQ (grandchild). Each colUmn corresponds to a semantic region for parentand each row to a semantic region for grandchild. The nUmber of localized channels shared betweentwo models is indicated for each pair of semantic regions.
Table 4: The nUmber of localized StyleSpace controls for varioUs semantic regions for two randomlyinitialized FFHQ models. Each colUmn corresponds to a semantic region in one model and eachrow to a semantic region in the other model. The nUmber of localized channels shared between twomodels is indicated for each pair of semantic regions. It is evident that the two models only havea small nUmber of overlap channels across Unrelated semantic regions (for example, hair and eye).
Table 5: A quantitative comparison of I2I translation via different latent spaces and inversion meth-ods. Based on the qualitative results shown in Figure 17, we consider encoder-based inversion forZ and Z+ spaces, and latent optimization method for Z space, to be promising methods and fur-ther examine them using FID and KID scores. Our results indicate that inversion Z using latentoptimization achieves the best FID and KID for I2I translation tasks.
Table 6: A quantitative comparison of reference-based image translation using different inversionmethods and latent spaces. It may be seen that the latent optimization method for Z achieves thebest FID and KID for such translation tasks.
Table 7: Average L1 distance between w ∈ W vectors mapped from the same latent code z ∈ Zfor different pairs of models. Using a pretrained FFHQ model as parent, it is fine-tuned on differentdatasets separately. We sample 100K random z vectors and compute the corresponding w for eachmodel. The mean change (per coordinate of w) is reported for each child model. It may be clearlyseen that in models fine-tuned to nearby domains (Mega, Metface) the change in w is much smallerthan to more distant domains (Dog, Cat, Wild), and an order of magnitude smaller than the differenceto another FFHQ model, trained independently. These results quantitatively demonstrate that thechange in the mapping function is very small for similar domains, larger for more distant domains,but even for distant domains the mapping functions are more closely related than those of twoseparately trained models.
