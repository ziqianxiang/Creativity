title,year,conference
 Unitary evolution recurrent neural networks,2016, InThe International Conference on Machine Learning (ICML)
 An empirical evaluation of generic convolutional andrecurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Trellis networks for sequence modeling,2019, In TheInternational Conference on Learning Representations (ICLR)
 Dilated recurrent neural networks,2017, InAdvances in Neural Information Processing Systems (NeurIPS)
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Parallelizing legendre memory unit training,2021, The Interna-tional Conference on Machine Learning (ICML)
 Rethinking attentionwith performers,2020, In The International Conference on Learning Representations (ICLR)
 Language modeling with gatedconvolutional networks,2017, In International conference on machine learning
 Gru-ode-bayes: Continuousmodeling of sporadically-observed time series,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Adversarial audio synthesis,2019, In ICLR
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Lipschitz recurrent neural networks,2021, In International Conference on Learning Represen-tations
 Hippo: Recurrent memory withoptimal polynomial projections,2020, In Hugo Larochelle
 Improving thegating mechanism of recurrent neural networks,2020, In The International Conference on MachineLearning (ICML)
 Long short-term memory,1997, Neural computation
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In International Conference on MachineLearning
 Cheap orthogonal constraints in neural networks:A simple parametrization of the orthogonal and unitary group,2019, In The International Conference onMachine Learning (ICML)
 Time-aware large kernel convolutions,2020, In International Confer-ence on Machine Learning
 Scalable languagemodeling: Wikitext-103 on a single gpu in 12 hours,2018, SysML
 Wavenet: A generative model for rawaudio,2016, arXiv preprint arXiv:1609
 Structured matrices and polynomials: unified superfast algorithms,2001, Springer Science &Business Media
 Fast approximate computations with cauchy matrices and polynomials,2017, Mathematics ofComputation
 Transformations of matrix structures work again,2015, Linear Algebra and Its Applications
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Fast parametric learning with activationmemorization,2018, The International Conference on Machine Learning (ICML)
 Fast generation forconvolutional autoregressive models,2017, arXiv preprint arXiv:1704
 Ckconv:Continuous kernel convolution for sequential data,2021, arXiv preprint arXiv:2102
 Unicornn: A recurrent model for learning very long timedependencies,2021, The International Conference on Machine Learning (ICML)
 Pixelcnn++: Improving thepixelcnn with discretized logistic mixture likelihood and other modifications,2017, arXiv preprintarXiv:1701
 Long range arena : A benchmark for efficienttransformers,2021, In International Conference on Learning Representations
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Learning longer-term depen-dencies in RNNs with auxiliary losses,2018, In The International Conference on Machine Learning(ICML)
 A method of analysing the behaviour of linear systems in terms of time series,1947, Journalof the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Dynamical systems in spiking neuromorphic hardware,2019, PhD thesis
 Speech commands: A dataset for limited-vocabulary speech recognition,2018, ArXiv
 Inverting modified matrices,1950, Memorandum report
 Pay less attention withlightweight and dynamic convolutions,2019, In The International Conference on Learning Representa-tions (ICLR)
