title,year,conference
 SGD with shuffling: optimal rates without componentconvexity and large epoch requirements,2020, In Advances in Neural Information Processing Systems
 Curiously fast convergence of some stochastic gradient descent algorithms,2009, In Pro-ceedings of the symposium on learning and data science
 Random reshuffling is not always better,2020, Advances in Neural InformationProcessing Systems
 Communication trade-offs for local-sgd with largestep size,2019, Advances in Neural Information Processing Systems
 Why random reshuffling beats stochasticgradient descent,2019, Mathematical Programming
 On the convergence of local descent methods in feder-ated learning,2019, arXiv preprint arXiv:1910
 Localsgd with periodic averaging: Tighter analysis and adaptive synchronization,2019, Advances in NeuralInformation Processing Systems
 Random shuffling beats SGD after finite epochs,2019, In InternationalConference on Machine Learning
 Making the last iterate of sgd informationtheoretically optimal,2019, In Conference on Learning Theory
 Advances and open problems in federated learning,1935, Foundations and TrendsÂ® inMachine Learning
 Linear convergence of gradient and proximal-gradient methods under the Polyak-Icjasiewicz condition,2016, In Joint European Conference on Ma-chine Learning and Knowledge Discovery in Databases
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 Tighter theory for local Sgd on identi-cal and heterogeneous data,2020, In International Conference on Artificial Intelligence and Statistics
 A unifiedtheory of decentralized sgd with changing topology and local updates,2020, In International Confer-ence on Machine Learning
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 ReCht-Re noncommutative arithmetic-geometric mean conjecture isfalse,2020, In International Conference on Machine Learning
 Speeding up distributed machine learning using codes,2017, IEEE Transactions on Infor-mation Theory
 On the convergence offedavg on non-iid data,2020, In International Conference on Learning Representations
 Convergence analysis of distributedstochastic gradient descent with shuffling,2017, arXiv preprint arXiv:1709
 Random reshuffling: Simple analysiswith vast improvements,2020, arXiv preprint arXiv:2006
 Proximal and federated randomreshuffling,2021, arXiv preprint arXiv:2102
 A uni-fied convergence analysis for shuffling-type gradient methods,2020, arXiv preprint arXiv:2002
 Optimum bounds for the distributions of martingales in banach spaces,1994, The Annals ofProbability
 Closing the convergence gap of SGDwithout replacement,2020, In International Conference on Machine Learning
 Making gradient descent optimal forstrongly convex stochastic optimization,2012, In Proceedings of the 29th International Coference onInternational Conference on Machine Learning
 Random shuffling beats SGD only after many epochs on ill-conditioned problems,2021, arXiv preprint arXiv:2106
 Probability inequalities for the sum in sampling without replacement,1974, The Annalsof Statistics
 Local sgd with a communicationoverhead depending only on the number of workers,2020, arXiv preprint arXiv:2006
 Local sgd converges fast and communicates little,2019, In International Conferenceon Learning Representations
 The error-feedback framework: Better rates forsgd with delayed gradients and compressed updates,2020, Journal of Machine Learning Research
 Permutation compressors for provably fasterdistributed nonconvex optimization,2021, arXiv preprint arXiv:2110
 Smg: A shuffling gradient-based method withmomentum,2021, In International Conference on Machine Learning
 The min-max complexityof distributed stochastic convex optimization with intermittent communication,2021, arXiv preprintarXiv:2102
 Minibatch vs local SGD for heteroge-neous distributed learning,2020, Advances in Neural Information Processing Systems
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
