title,year,conference
 A model of inductive bias learning,2000, Journal of artificial intelligence research
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 Concentration inequalities: A nonasymptotictheory of independence,2013, Oxford university press
 Transfer learning for nonParametric classification: Minimax rate andadaPtive classifier,2021, The Annals of Statistics
 BERT: Pre-training of deePbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Model-agnostic meta-learning for fast adaPtation ofdeeP networks,2017, In International Conference on Machine Learning
 A comParison of loss weighting strategies for multi tasklearning in deeP neural networks,2019, IEEE Access
 On the value of target data in transfer learning,2019, In Advances inNeural Information Processing Systems
 SamPle sPlitting and threshold estimation,2000, Econometrica
 Foreseeing the benefits of incidentalsuPervision,2021, In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Ontonotes:the 90% solution,2006, In Proceedings of the human language technology conference of the NAACL
 Instance weighting for domain adaptation in nlp,2007, In Proceedings ofthe 45th annual meeting of the association of computational linguistics
 Survey on deep learning with class imbalance,2019, Journalof Big Data
 Minimax lower bounds for transfer learning with linear and one-hiddenlayer neural networks,2020, Neural Information Processing Systems
 Adam: A method for stochastic optimization,2015, ICLR
 Meta-learning transferable representationswith a single target domain,2020, arXiv preprint arXiv:2011
 Multi-task deep neural networks fornatural language understanding,2019, In Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics
 The benefit of multitaskrepresentation learning,2016, The Journal of Machine Learning Research
 The natural languagedecathlon: Multitask learning as question answering,2018, arXiv preprint arXiv:1806
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Improving predictive inference under covariate shift by weighting the log-likelihood function,2000, Journal of statistical planning and inference
 On the theory of transfer learning: The importance oftask diversity,2020, In Advances in Neural Information Processing Systems
 Provable meta-learning of linear representations,2021, InInternational Conference on Machine Learning
 The Nature of Statistical Learning Theory,2013, Springer science & business media
 Weak convergence and empirical processes: with applications tostatistics,2013, Springer Science & Business Media
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 A survey on multi-task learning,2021, IEEE Transactions on Knowledge andData Engineering
