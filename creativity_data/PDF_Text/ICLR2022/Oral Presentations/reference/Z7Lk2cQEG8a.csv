title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Understanding deep neuralnetworks with rectified linear units,2016, arXiv preprint arXiv:1611
 Breaking the curse of dimensionality with convex neural networks,2017, The Journal ofMachine Learning Research
 On the capabilities of multilayer perceptrons,1988, Journal of complexity
 Convexneural networks,2006, In Advances in neural information processing systems
 Convex analysis and nonlinear optimization: theory andexamples,2010, Springer Science & Business Media
 Convex optimization,2004, Cambridgeuniversity press
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, In Advances in neural information processing systems
 Generalized gradients and applications,1975, Transactions of the American MathematicalSociety
 Geometrical and statistical properties of systems of linear inequalities withapplications in pattern recognition,1965, IEEE transactions on electronic computers
 Stochastic subgradient methodconverges on tame functions,2020, Foundations of computational mathematics
 On the power of over-parametrization in neural networks with quadraticactivation,2018, arXiv preprint arXiv:1803
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Global optimality in neural network training,2017, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 On the computational efficiency of training neuralnetworks,2014, In Advances in neural information processing systems
 On connected sublevel sets in deep learning,2019, In International Conference on MachineLearning
 On the proof of global convergence of gradient descent for deep relu networks withlinear widths,2021, arXiv preprint arXiv:2101
 The loss surface of deep and wide neural networks,2017, In Proceedingsof the 34th International Conference on Machine Learning-Volume 70
 The inductive bias of relu networks on orthogonallyseparable data,2020, In International Conference on Learning Representations
 Neural networks are convex regularizers: Exact polynomial-timeconvex optimization formulations for two-layer networks,2020, arXiv preprint arXiv:2002
 Convex regularizationbehind neural reconstruction,2020, arXiv preprint arXiv:2012
 Bounds on over-parameterization for guaranteed existence of descent paths in shallow relu networks,2019, In In-ternational Conference on Learning Representations
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 Spurious valleys in one-hidden-layer neuralnetwork optimization landscapes,2019, Journal of Machine Learning Research
 Mathematics of deep learning,2017, arXivpreprint arXiv:1712
 Harmless overparametrization in two-layer neural networks,2021, arXivpreprint arXiv:2106
 Partitions of n-space by hyperplanes,1966, SIAM Journal on Applied Mathematics
