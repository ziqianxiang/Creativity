title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 MMDetection: Open mmlab detection toolbox andbenchmark,2019, arXiv preprint arXiv:1906
 Encoder-decoder with atrous separable convolution for semantic image segmentation,2018, In Proceedings ofthe European conference on computer vision (ECCV)
 MMSegmentation: Openmmlab semantic segmentation toolboxand benchmark,2020, https://github
 An algorithm for the machine calculation of complex fourierseries,1965, Mathematics of computation
 On the relationship between self-attention and convolutional layers,2020, In International Conference on Learning Representations
 The cityscapes dataset for semantic urbanscene understanding,2016, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Mdmmt: Mul-tidomain multimodal transformer for video retrieval,2021, In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition
 Multiscale vision transformers,2021, arXiv preprint arXiv:2104
 Dual attentionnetwork for scene segmentation,2019, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Adaptivecontext network for scene parsing,2019, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 Multi-modal transformer forvideo retrieval,2020, In Computer Vision-ECCV 2020: 16th European Conference
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Beyond self-attention: Externalattention using two linear layers for visual tasks,2021, arXiv preprint arXiv:2105
 Mask r-cnn,2017, In Proceedings oftheIEEE international conference on computer vision
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, Proceedings of the International Conference on Learning Represen-tations
 Vi-sion permutator: A permutable mlp-like architecture for visual recognition,2021, arXiv preprintarXiv:2106
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Ccnet:Criss-cross attention for semantic segmentation,2019, In Proceedings of the IEEE/CVF InternationalConference on Computer Vision
 PanoPtic feature pyramid net-works,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neuralarchitecture search,2021, arXiv preprint arXiv:2103
 As-mlp: An axial shifted mlp architecturefor vision,2021, arXiv preprint arXiv:2107
 Network in network,2013, arXiv preprint arXiv:1312
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Pay attention to mlps,2021, arXiv preprintarXiv:2105
 SWin transformer: Hierarchical vision transformer using shifted WindoWs,2021, arXiv preprintarXiv:2103
 Decoupled Weight decay regularization,2017, arXiv preprintarXiv:1711
 Rethinkingthe design principles of robust vision transformer,2021, arXiv preprint arXiv:2105
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Global filter netWorks forimage classification,2021, arXiv preprint arXiv:2107
 Very deep convolutional netWorks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Sparse r-cnn: End-to-end object detection withlearnable proposals,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition 
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Training data-effiCient image transformers & distillation through attention,2020, arXivpreprint arXiv:2012
 Resmlp: Feedforwardnetworks for image classification with data-efficient training,2021, arXiv preprint arXiv:2105
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Pvtv2: Improved baselines with pyramid vision transformer,2021, arXiv preprintarXiv:2106
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 End-to-end video instance segmentation with transformers,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Pytorch image models,2019, https://github
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Unified perceptual parsing forscene understanding,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Seg-former: Simple and efficient design for semantic segmentation with transformers,2021, arXiv preprintarXiv:2105
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE conference on computer visionandpattern recognition
 Multi-scale context aggregation by dilated convolutions,2016, In ICLR
 S2-mlp: Spatial-shift mlp architecture forvision,2021, arXiv preprint arXiv:2106
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, arXiv preprint arXiv:2101
 Object-contextual representations for semantic seg-mentation,2020, In Computer Vision-ECCV 2020: 16th European Conference
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Rethinking semantic segmentation froma sequence-to-sequence perspective With transformers,2021, In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition
 Random erasing data augmen-tation,2020, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)
 Sceneparsing through ade20k dataset,2017, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Deformable detr:Deformable transformers for end-to-end object detection,2020, arXiv preprint arXiv:2010
