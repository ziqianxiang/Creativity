title,year,conference
 Better fine-tuning by reducing representational collapse,2021, In International Conference onLearning Representations (ICLR)
 Deep learning for segmentation of brain tumors:Impact of cross-institutional training and testing,2018, Med Phys
 The evolution ofout-of-distribution robustness throughout fine-tuning,2021, arXiv
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In International Conference on Machine Learning (ICML)
 Geography-aware self-supervised learning,2020, arXiv
 Benign overfitting in linearregression,2019, arXiv
 Two models of double descent for weak features,2019, arXiv
 A new look at an old problem: A universal learningapproach to linear regression,2019, In 2019 IEEE International Symposium on Information Theory(ISIT)
 A theory of label propagation for subpopulation shift,2021, InInternational Conference on Machine Learning (ICML)
 A simple framework forcontrastive learning of visual representations,2020, In International Conference on Machine Learning(ICML)
 Improved baselines with momentumcontrastive learning,2020, arXiv
 An empirical study of training self-supervised visiontransformers,2021, arXiv preprint arXiv:2104
 Functional map of the world,2018, InComputer Vision and Pattern Recognition (CVPR)
 How fine-tuning allows for effective meta-learning,2021, arXivpreprint arXiv:2105
 An analysis of single-layer networks in unsuper-vised feature learning,2011, In Proceedings of the Fourteenth International Conference on ArtificialIntelligence and Statistics
 Algorithmic regularization in learning deep homoge-neous models: Layers are automatically balanced,2018, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Borrowing treasures from the wealthy: Deep transfer learning throughselective joint fine-tuning,2017, In Computer Vision and Pattern Recognition (CVPR)
 Implicit regularization of discretegradient dynamics in deep linear neural networks,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In International Conference on Artificial Intelligence and Statistics
 Matrix Computations,2013, The Johns Hopkins University Press
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Momentum contrast for unsuper-vised visual representation learning,2020, In Computer ViSion and Pattern Recognition (CVPR)
 Using pre-training can improve model robustnessand uncertainty,2019, In International Conference on Machine Learning (ICML)
 Natural adversarialexamples,2019, arXiv preprint arXiv:1907
 A structural probe for finding syntax in word representa-tions,2019, In ASSociation for Computational LinguiSticS (ACL)
 Parameter-efficient transfer learning forNLP,2019, arXiv
 Smart:Robust and efficient fine-tuning for pre-trained natural language models through principledregularized optimization,2021, In International Conference on Learning RepreSentationS (ICLR)
 Partial transfusion: on the expressive influence of trainablebatch norm parameters for transfer learning,2021, In Medical Imaging with Deep Learning
 Removing spurious features can hurt accuracy and affect groups dis-proportionately,2021, In ACM Conference on FairneSS
 Learning multiple layers of features from tiny images,2009, Technical report
 Deep linear neural networks with arbitrary loss: All localminima are global,2018, In International Conference on Machine Learning (ICML)
 The power of scale for parameter-efficient prompttuning,2021, arXiv preprint arXiv:2104
 End-to-end training of deep visuomotorpolicies,2016, Journal of Machine Learning Research (JMLR)
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InAssociation for Computational Linguistics (ACL)
 Explicit inductive bias for transfer learning withconvolutional networks,2018, In International Conference on Machine Learning (ICML)
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 Accuracy on the line: on the strong correlationbetween out-of-distribution and in-distribution generalization,2021, In International Conference onMachine Learning (ICML)
 Harmless interpo-lation of noisy data in regression,2020, IEEE Journal on Selected Areas in Information Theory
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv
 Moment matchingfor multi-source domain adaptation,2019, In International Conference on Computer Vision (ICCV)
 Deep contextualized word representations,2018, In North American Association forComputational Linguistics (NAACL)
 To tune or not to tune? adapting pretrainedrepresentations to diverse tasks,2019, In Proceedings of the 4th Workshop on Representation Learningfor NLP (RepL4NLP-2019)
 Selective entropy optimizationvia committee consistency for unsupervised domain adaptation,2021, In International Conference onComputer Vision (ICCV)
 Understandingand mitigating the tradeoff between robustness and accuracy,2020, In International Conference onMachine Learning (ICML)
 ImageNet large scale visual recognitionchallenge,2015, International Journal of Computer Vision
 Breeds: Benchmarks for subpopulationshift,2020, arXiv
 Exact solutions to the nonlineardynamics of learning in deep linear neural networks,2014, arXiv
 A theoretical analysis of fine-tuning with linearteachers,2021, In Advances in Neural Information Processing Systems (NeurIPS)
 Class-imbalanced domain adaptation: An empiricalodyssey,2020, arXiv preprint arXiv:1910
 Measuring robustness to natural distribution shifts in image classification,2020, arXiv preprintarXiv:2007
 On the theory of transfer learning: The importanceof task diversity,2020, arXiv
 An introduction to matrix concentration inequalities,2015, Foundations and Trends inMachine Learning
 Avoiding inferenceheuristics in few-shot prompt-based finetuning,2021, arXiv preprint arXiv:2109
 Learning robust global representationsby penalizing local predictive power,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Robust fine-tuning of zero-shot models,2021, arXivpreprint arXiv:2109
 Understanding and improving information transferin multi-task learning,2020, In International Conference on Learning Representations (ICLR)
 Composed fine-tuning: Freezing pre-traineddenoising autoencoders for improved generalization,2021, In International Conference on MachineLearning (ICML)
 Side-tuning: Abaseline for network adaptation via additive side networks,2020, In European Conference on ComputerVision (ECCV)
 Learning to prompt forvision-language models,2021, arXiv preprint arXiv:2109
 FreeLB: Enhancedadversarial training for natural language understanding,2020, In International Conference on LearningRepresentations (ICLR)
