title,year,conference
 Task-free continual learning,2019, In IEEEConference on Computer Vision and Pattern Recognition
 Birdsnap: Large-scale fine-grained visual categorization of birds,2014, In IEEE Confer-ence on Computer Vision and Pattern Recognition
 Deep clustering for unsu-pervised learning of visual features,2018, In European Conference on Computer Vision
 Unsupervised pre-trainingof image features on non-curated data,2019, In IEEE International Conference on Computer Vision
 Un-supervised learning of visual features by contrasting cluster assignments,2020, In Advances in NeuralInformation Processing Systems
 A simple framework forcontrastive learning of visual representations,2020, In International Conference on Machine Learning
 Bigself-supervised models are strong semi-supervised learners,2020, In Advances in Neural InformationProcessing Systems
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 De-scribing textures in the wild,2014, In IEEE Conference on Computer Vision and Pattern Recognition
 A continual learning survey: Defying forgetting in classificationtasks,2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Learning generative visual models from few trainingexamples: An incremental bayesian approach tested on 101 object categories,2004, In IEEE Conferenceon Computer Vision and Pattern Recognition Workshop
 Unsupervised representation learning bypredicting image rotations,2018, In International Conference on Learning Representations
 An empiri-cal investigation of catastrophic forgetting in gradient-based neural networks,2013, arXiv preprintarXiv:1312
 Bootstrap your own latent: A new approach to self-supervised learning,2020, In Advancesin Neural Information Processing Systems
 Donâ€™t stop pretraining: adapt language models to domains and tasks,2020, InAnnual Meeting of the Association for Computational Linguistics
 SODA10M: Towards large-scale object detectionbenchmark for autonomous driving,2021, arXiv preprint arXiv:2108
 Deep residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Momentum contrast forunsupervised visual representation learning,2020, In IEEE Conference on Computer Vision and PatternRecognition
 Barlow twins: Self-supervisedlearning via redundancy reduction,2021, In International Conference on Machine Learning
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, InInternational Conference on Learning Representations
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the National Academy of Sciences
 Similarity of neuralnetwork representations revisited,2019, In International Conference on Machine Learning
 Collecting a large-scale dataset of fine-grained cars,2013, In Workshop on Fine-Grained Visual Categorization
 Gradient episodic memory for continual learning,2017, InAdvances in Neural Information Processing Systems
 Exploring the limits of weakly supervisedpretraining,2018, In European Conference on Computer Vision
 Fine-grainedvisual classification of aircraft,2013, arXiv preprint arXiv:1306
 Packnet: Adding multiple tasks to a single network by iterativepruning,2018, In IEEE Conference on Computer Vision and Pattern Recognition
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, In Psychology of Learning and Motivation
 WordNet: An Electronic Lexical Database,1998, MIT press
 Linear mode connectivity in multitask and continual learning,2020, In InternationalConference on Learning Representations
 Automated flower classification over a large numberof classes,2008, In Indian Conference on Computer Vision
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Continuallifelong learning with neural networks: A review,2019, Neural Networks
 Cats and dogs,2012, In IEEEConference on Computer Vision and Pattern Recognition
 Moment matchingfor multi-source domain adaptation,2019, In IEEE International Conference on Computer Vision
 icarl:Incremental classifier and representation learning,2017, In IEEE Conference on Computer Vision andPattern Recognition
 Self-supervised pretraining im-proves self-supervised pretraining,2021, In IEEE International Conference on Computer Vision
 Faster R-CNN: Towards real-time objectdetection with region proposal networks,2015, In Advances in Neural Information Processing Systems
 Imagenet-21k pretraining forthe masses,2021, In International Conference on Learning Representations
 Experiencereplay for continual learning,2019, In Advances in Neural Information Processing Systems
 Imagenet large scale visualrecognition challenge,2015, International Journal of Computer Vision
 Overcoming catastrophicforgetting with hard attention to the task,2018, In International Conference on Machine Learning
 YFCC100M: The new data in multimedia research,2016, Communica-tions of the ACM
 Deep image prior,2018, In IEEE Conference onComputer Vision and Pattern Recognition
 Ordisco: Effec-tive and efficient usage of incremental unlabeled data for semi-supervised continual learning,2021, InIEEE Conference on Computer Vision and Pattern Recognition
 Understanding contrastive representation learning through align-ment and uniformity on the hypersphere,2020, In International Conference on Machine Learning
 Characterizing and avoiding neg-ative transfer,2019, In IEEE Conference on Computer Vision and Pattern Recognition
 SmoothoUt:Smoothing oUt sharp minima to improve generalization in deep learning,2018, arXiv preprintarXiv:1805
 UnsUpervised featUre learning via non-parametric instance discrimination,2018, In IEEE Conference on Computer Vision and Pattern Recog-nition
 SUn database:Large-scale scene recognition from abbey to zoo,2010, In IEEE Conference on Computer Vision andPattern Recognition
 Der: Dynamically expandable representation for classincremental learning,2021, In IEEE Conference on Computer Vision and Pattern Recognition
 An em framework foronline incremental learning of semantic segmentation,2021, In Proceedings of the 29th ACM Interna-tional Conference on Multimedia
 Unleashing the power of con-trastive self-sUpervised visUal models via contrast-regUlarized fine-tUning,2021, In Advances in NeuralInformation Processing Systems
 MoCo-v2 maintains an additional dictionary as a queue of features for contrastive learning,2018, Specifically
 Data relay is a simple yet effective method for alleviating the catastrophic forgettingproblem during the continual learning process,2022, Specifically
 Few-shot classification reflects how well the pre-trained models performon downstream tasks in the few-shot learning regime,2021, Specifically
 We then com-pare the uniformity of representations between sequential SL and SSL models,2022, Specifically
