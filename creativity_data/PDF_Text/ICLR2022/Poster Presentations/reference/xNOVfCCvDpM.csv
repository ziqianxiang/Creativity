title,year,conference
 Debugging tests for model explana-tions,2020, arXiv preprint arXiv:2011
 Evaluat-ing saliency map explanations for convolutional neural networks: a user study,2020, In Proceedings ofthe 25th International Conference on Intelligent User Interfaces
 HoW to explain individual classification decisions,2010, Journal ofMachine LearningResearch
 Debiasing concept-based explanations withcausal analysis,2021, 2021
" "" willyou find these shortcuts?"" a protocol for evaluating the faithfulness of input salience methods fortext classification",2021, arXiv preprint arXiv:2111
 Network dissection:Quantifying interpretability of deep visual representations,2017, In Proceedings of the IEEE conferenceon computer vision and pattern recognition
 Fully automatic kneeosteoarthritis severity grading using deep neural networks with a novel ordinal loss,2019, ComputerizedMedical Imaging and Graphics
 Towards connectinguse cases and methods in interpretable machine learning,2021, arXiv preprint arXiv:2103
 Are visual explanations useful? a case study in model-in-the-loop prediction,2020, arXiv preprint arXiv:2007
 Ai for radiographic covid-19 detection selectsshortcuts over signal,2020, medRxiv
 Explanations can be manipulated and geometry is to blame,2019, InAdvances in Neural Information Processing Systems
 Learningexplainable models using attribution priors,2019, arXiv preprint arXiv:1906
 Imagenet-trained cnns are biased towards texture; increasing shape bias improvesaccuracy and robustness,2018, arXiv preprint arXiv:1811
 Shortcut learning in deep neural networks,2020, Nature MachineIntelligence
 Interpretation of neural networks is fragile,2019, InThe Thirty-Third AAAI Conference on Artificial Intelligence
 Towards automatic concept-basedexplanations,2019, arXiv preprint arXiv:1902
 Fastif:Scalable influence functions for efficient model interpretation and debugging,2020, arXiv preprintarXiv:2012
 Explaining black box predictions andunveiling data artifacts through influence functions,2020, arXiv preprint arXiv:2005
 Evaluation of similarity-basedexplanations,2020, arXiv preprint arXiv:2006
 Fooling neural network interpretations via adversarialmodel manipulation,2019, In Advances in Neural Information Processing Systems
 A benchmark for interpretabilitymethods in deep neural networks,2019, In Advances in Neural Information Processing Systems
 Removing spurious features can hurt accuracy and affect groupsdisproportionately,2020, arXiv preprint arXiv:2012
 Sanity simulations for saliency methods,2021, arXivpreprint arXiv:2105
 Understanding black-box predictions via influence functions,2017, InDoina Precup and Yee Whye Teh
 Concept bottleneck models,2020, In International Conference on Machine Learning
" "" how do i fool you?"" manipulating user trust viamisleading black box explanations",2020, In Proceedings of the AAAI/ACM Conference on AI
 Unmasking clever hans predictors and assessing what machines reallylearn,2019, Nature communications
 Salient deconvolutional networks,2016, In European Conferenceon Computer Vision
 Automaticshadow detection in 2d ultrasound,2018, 2018
 Understanding the failure modesof out-of-distribution generalization,2020, arXiv preprint arXiv:2010
 The effectiveness of feature attribution methodsand its correlation with automatic evaluation scores,2021, arXiv preprint arXiv:2105
 A theoretical explanation for perplexing behaviors ofbackpropagation-based visualizations,2018, In ICML
 Finding and fixing spurious patternswith explanations,2021, arXiv preprint arXiv:2106
 Manipulating and measuring model interpretability,2018, arXiv preprintarXiv:1802
 Estimating training datainfluence by tracking gradient descent,2020, arXiv preprint arXiv:2002
 Transfusion: Understandingtransfer learning for medical imaging,2019, arXiv preprint arXiv:1902
 Why should i trust you?: Explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Interpretations are useful: penalizingexplanations to align neural networks with prior knowledge,2020, International Conference on MachineLearning
 Right for the right reasons: Train-ing differentiable models by constraining their explanations,2017, In Carles Sierra
 An investigation of whyoverparameterization exacerbates spurious correlations,2020, In International Conference on MachineLearning
 How useful are the machine-generated interpretations to generalusers? a human evaluation on guessing the incorrectly predicted labels,2020, In Proceedings of theAAAI Conference on Human Computation and Crowdsourcing
 Deep inside convolutional networks:Visualising image classification models and saliency maps,2014, In Yoshua Bengio and Yann LeCun
 When explanations lie: Why modified bp attributionfails,2019, arXiv preprint arXiv:1912
 Fooling limeand shap: Adversarial attacks on post hoc explanation methods,2020, In Proceedings of the AAAI/ACMConference on AI
 Smoothgrad:removing noise by adding noise,2017, arXiv preprint arXiv:1706
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Rethinking the role of gradient-based attribution methods formodel interpretability,2021, In International Conference on Learning Representations
 Convnets and imagenet beyond accuracy: Understanding mistakesand uncovering biases,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Axiomatic attribution for deep networks,2017, InDoina Precup and Yee Whye Teh
 Sanitychecks for saliency metrics,2020, In The Thirty-Fourth AAAI Conference on Artificial Intelligence
 Noise or signal: The role of imagebackgrounds in object recognition,2020, arXiv preprint arXiv:2006
 Representer point selectionfor explaining deep neural networks,2018, In Advances in neural information processing systems
 Oncompleteness-aware concept-based explanations in deep neural networks,2020, Advances in NeuralInformation Processing Systems
