title,year,conference
 Un-supervised cross-lingual representation learning at scale,2020, Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics
 BERT: Pre-training of deepbidirectional transformers for langUage Understanding,4171, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Openwebtext corpUs,2019, http://Skylion007
 MUsic transformer: Generat-ing mUsic with long-term strUctUre,2019, In ICLR
 Tying word vectors and word classifiers: Aloss framework for langUage modeling,2017, In ICLR
 Highly accurate protein structure prediction withalphafold,2021, Nature
 Generalizationthrough Memorization: Nearest Neighbor Language Models,2020, In International Conference onLearning Representations (ICLR)
 Shape: Shifted absolute positionembedding for transformers,2021, ArXiv
 Towards mentaltime travel: a hierarchical memory for reinforcement learning agents,2021, CoRR
 Jurassic-1: Technical details and evalua-tion,2021, Technical report
 Context dependent recurrent neural network language model,2012, 2012IEEE Spoken Language Technology Workshop (SLT)
 Recurrent neural networkbased language model,2010, In INTERSPEECH
 On the relation between position information and sen-tence length in neural machine translation,2019, In Proceedings of the 23rd Conference on Com-putational Natural Language Learning (CoNLL)
 The eos decision andlength extrapolation,2020, In BlackBoxNLP@EMNLP
 Investigating the limitations of the transformerswith simple arithmetic tasks,2021, ArXiv
 A decomposable atten-tion model for natural language inference,2249, In Proceedings of the 2016 Conference on Em-pirical Methods in Natural Language Processing
 Using the output embedding to improve language models,2017, In Proceedingsof the 15th Conference of the European Chapter of the Association for Computational Linguistics:Volume 2
 Improving transformer models by reordering theirsublayers,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
 Shortformer: Better language modeling using shorterinputs,2021, In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:Long Papers)
 Compressive transformers for long-range sequence modelling,2020, In International Confer-ence on Learning Representations
 Exploring the limits of transfer learning with a unified text-to-text transformer,2020, Journal of Machine Learning Research
 Analysis of positionalencodings for neural machine translation,2019, In International Workshop on Spoken Language Trans-lation
 Self-attention with relative position representa-tions,2018, In Proceedings of the 2018 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Association for Computational Linguistics,2074, doi:10
 Attention is all you need,2017, In I
 GPT-J-6B: A 6 Billion Parameter Autoregressive LanguageModel,2021, https://github
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
