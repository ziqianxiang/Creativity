title,year,conference
 An empirical investigation of the role of pre-training in lifelong learning,2022, In Submittedto The Tenth International Conference on Learning Representations
 Non-parametric adaptation for neural machine translation,2019, In Pro-ceedings of the 2019 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies
 Beyond the imitation game: Measuring and extrapolating the capabil-ities of language models,2021, In preparation
 On the opportu-nities and risks of foundation models,2021, arXiv preprint arXiv:2108
 Language models are few-shotlearners,2020, In H
 Efficientlifelong learning with a-gem,2018, ArXiv
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 A continual learning survey: Defying forgetting in classificationtasks,2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recog-nition(CVPR)
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Addressing catastrophic forgetting for medical domain expansion,2021, CoRR
 Overcoming catastrophic forget-ting in neural networks,0027, Proceedings of the National Academy of Sciences
 3d object representations for fine-grainedcategorization,2013, In 4th International IEEE Workshop on 3D Representation and Recognition(3dRR-13)
 Overcomingcatastrophic forgetting by incremental moment matching,2017, In Neural information processing sys-tems
 Gradient episodic memory for continual learning,2017, InAdvances in Neural Information Processing Systems
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, 1989
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Cats and dogs,2012, In IEEE Conference onComputer Vision and Pattern Recognition
 Moment matchingfor multi-source domain adaptation,2019, In Proceedings of the IEEE International Conference onComputer Vision
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-text transformer,2020, Journal of Machine Learning Research
 Anatomy of catastrophic forgetting:Hidden representations and task semantics,2021, In International Conference on Learning Represen-tations
 icarl:Incremental classifier and representation learning,2016, 2017 IEEE Conference on Computer Visionand Pattern Recognition (CVPR)
 Learning to learn without forgetting by maximizing transfer and minimizing interfer-ence,2018, arXiv preprint arXiv:1810
 Continual learning with deep generativereplay,2017, In Advances in Neural Information Processing Systems
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In ICCV
 Caltech-UCSDBirds 200,2010, Technical Report CNS-TR-2010-001
 Group normalization,2018, CoRR
