title,year,conference
 Word frequency distributions,2001, 2001
 Productivity in language production,1994, Language and Cognitive Processes
 The battle against the long tail,2015, In Talk on Workshop on Big Data and StatisticalMachine Learning
 A neural probabilisticlanguage model,2003, The journal of machine learning research
 Moving down the long tail of word sense disambiguation withgloss-informed biencoders,2020, CoRR
 Non-parametric few-shot learning for word sensedisambiguation,2021, arXiv preprint arXiv:2104
 An empirical study of smoothing techniques for languagemodeling,1999, Computer Speech & Language
 The Logical Structure of Linguistic Theory,1955, Springer US
 Donâ€™t for-get the long tail! a comprehensive analysis of morphological generalization in bilingual lexiconinduction,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 A tale of two long tails,2021, arXivpreprint arXiv:2107
 Connectionism and cognitive architecture: A critical analy-sis,1988, Cognition
 Openwebtext corpus,2019, http://Skylion007
 Interpolating between typesand tokens by estimating power-law generators,2006, In Y
 Producing power-law distribu-tions and damping word frequencies with two-stage language models,2011, Journal of MachineLearning Research
 Long short-term memory,1997, Neural computation
 InterPolated estimation of markov source Parameters from sParse data,1980, 1980
 Estimation of Probabilities from sParse data for the language model comPonent of a sPeechrecognizer,1987, IEEE Transactions on Acoustics
 The statistical analysis of a large number of rare events,1988, 1988
 ImProved backing-off for m-gram language modeling,1995, In 1995 InternationalConference on Acoustics
 Mode recovery in neural autoregressive sequencemodeling,2021, CoRR
 Conditional random fields:Probabilistic models for segmenting and labeling sequence data,1558, In Proceedings of the EighteenthInternational Conference on Machine Learning
 How muchdo language models coPy from their training data? evaluating linguistic novelty in text generationusing raven,2021, arXiv preprint arXiv:2111
 Mogrifier lstm,2020, In International Confer-ence on Learning Representations
 Linguistic regularities in continuous spaceword rePresentations,2013, In Proceedings of the 2013 conference of the north american chapter of theassociation for computational linguistics: Human language technologies
 The deeP bootstraP: Good online learn-ers are good offline generalizers,2020, CoRR
 Distributionally robustlanguage modeling,2019, In Proceedings of the 2019 Conference on Empirical Methods in NaturalLanguage Processing and the 9th International Joint Conference on Natural Language Process-ing (EMNLP-IJCNLP)
 Factors in finetuning deePmodel for object detection with long-tail distribution,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Pretraining on non-linguistic structure as a tool for analyzinglearning bias in language models,2020, CoRR
 Language modelsare unsupervised multitask learners,2019, 2019
 Evaluating Computational Language Models withScaling Properties of Natural Language,2019, Computational Linguistics
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Examining the inductive bias of neural language models withartificial languages,2021, CoRR
 Transformers: State-of-the-art naturallanguage processing,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natu-ral Language Processing: System Demonstrations
 The sequencememoizer,2011, Communications of the ACM
 Capturing long-tail distributions of objectsubcategories,2014, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR)
 Human behavior and the principle of least effort,1949, 1949
