title,year,conference
 Attention augmentedconvolutional networks,2019, In Proceedings of the IEEE/CVF international conference on computervision
 CrossVit: Cross-attention multi-scale visiontransformer for image classification,2021, In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV)
 Rethinking atrousconvolution for semantic image segmentation,2017, arXiv preprint arXiv:1706
 Mobile-former: Bridging mobilenet and transformer,2021, arXiv preprint arXiv:2108
 XcePtion: Deep learning with depthwise separable convolutions,2017, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Use coremltools to convert models from third-party libraries to CoreML,2021, https://coremltools
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Coatnet: Marrying convolution andattention for all data sizes,2021, arXiv preprint arXiv:2106
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, In International Conference on Learning Representations
 Sigmoid-weighted linear units for neural networkfunction approximation in reinforcement learning,2018, Neural Networks
 The pascal visual object classes challenge: A retrospective,2015, International journal ofcomputer vision
 Levit: a vision transformer in convnet¡¯s clothing for faster inference,2021, arXivpreprint arXiv:2104
 Se-mantic contours from inverse detectors,2011, In 2011 International Conference on Computer Vision
 Rethinking spatial dimensions of vision transformers,2021, In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV)
 Searching for mobilenetv3,2019, In Pro-ceedings of the IEEE/CVF International Conference on Computer Vision
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2018, In Proceedings of the IEEE conference on computervision and pattern recognition
 Flattened convolutional neural networksfor feedforward acceleration,2014, arXiv preprint arXiv:1412
 Localvit: Bringing localityto vision transformers,2021, arXiv preprint arXiv:2104
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Ssd: Single shot multibox detector,2016, In European conference on computervision
 SWin transformer: Hierarchical vision transformer using shifted WindoWs,2021, arXiv preprintarXiv:2103
 Fully convolutional netWorks for semanticsegmentation,2015, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Sgdr: Stochastic gradient descent With Warm restarts,2017, In Interna-tional Conference on Learning Representations
 Decoupled Weight decay regularization,2019, In International Confer-ence on Learning Representations
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Espnet:Efficient spatial pyramid of dilated convolutions for semantic segmentation,2018, In Proceedings ofthe european conference on computer vision (ECCV)
 Dicenet: Dimension-wise convo-lutions for efficient networks,2020, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Evrnet: Efficient video restoration on edge devices,2021, In Proceedings of the ACMMultimedia
 Torchvision semantic segmentation,2021, https://pytorch
 In H,2019, Wallach
 Dynamicvit:Efficient vision transformers with dynamic token sparsification,2021, arXiv preprint arXiv:2106
 Imagenet large scale visualrecognition challenge,2015, International journal of computer vision
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Megatron-lm: Training multi-billion parameter language models using model par-allelism,2019, arXiv preprint arXiv:1909
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Mixconv: Mixed depthwise convolutional kernels,2019, In Proceedingsof the British Machine Vision Conference (BMVC)
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, In Proceedings of the IEEE/CVF international conference on computer vision
 Non-local neural networks,2018, InProceedings of the IEEE conference on computer vision and pattern recognition
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Earlyconvolutions help transformers see better,2021, arXiv preprint arXiv:2106
 Multi-scale context aggregation by dilated convolutions,2016, In Interna-tional Conference on Learning Representations
 Incorporating con-volution designs into visual transformers,2021, arXiv preprint arXiv:2103
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, In Proceedings of the IEEE/CVF international conference on computer vision
 mixup: Beyond empiri-cal risk minimization,2018, In International Conference on Learning Representations
 Deepvit: Towards deeper vision transformer,2021, arXiv preprint arXiv:2103
