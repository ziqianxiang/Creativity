title,year,conference
 Compressing pre-trained language models by matrix decom-position,2020, In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association forComputational Linguistics and the 10th International Joint Conference on Natural Language Pro-cessing
 Language models are few-shot learners,2020, CoRR
 Deformer:Decomposing pre-trained transformers for faster question answering,2020, CoRR
 Analysis of individual differences in multidimensional scalingvia an n-way generalization of “eckart-young” decomposition,1970, Psychometrika
 Neural ordinary differen-tial equations,2018, In NeurIPS
 Masked language model-ing for proteins via linearly scalable long-context transformers,2020, arXiv preprint arXiv:2006
 What does bert look at?an analysis of bert’s attention,2019, arXiv preprint arXiv:1906
 A multilinear singular value decomposi-tion,2000, SIAM journal on Matrix Analysis and Applications
 Exploiting linearstructure within convolutional networks for efficient evaluation,2014, arXiv preprint arXiv:1404
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Attention is not all you need: Pureattention loses rank doubly exponentially with depth,2021, arXiv preprint arXiv:2103
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Transformer feed-forward layers arekey-value memories,2020, arXiv preprint arXiv:2012
 Loss-aware weight quantization of deep networks,2018, InICLR
 Tensorized embed-ding layers for efficient model compression,2019, arXiv preprint arXiv:1901
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Revealing the dark secrets ofbert,2019, arXiv preprint arXiv:1908
 Block pruning for fastertransformers,2021, In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Enablinglightweight fine-tuning for pre-trained language model compression based on matrix productoperators,2021, arXiv preprint arXiv:2106
 Ladabert: Lightweight adaptation of bert through hybrid modelcompression,2020, arXiv preprint arXiv:2004
 Structured pruning of a bert-based questionanswering model,2019, arXiv preprint arXiv:1910
 Compressing pre-trained language models by matrix de-composition,2020, In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Associationfor Computational Linguistics and the 10th International Joint Conference on Natural LanguageProcessing
 Tensorizing neuralnetworks,2015, arXiv preprint arXiv:1509
 Tensor-train decomposition,2011, SIAM Journal on Scientific Computing
 word2ket: space-efficient word embeddings inspiredby quantum entanglement,2019, arXiv preprint arXiv:1911
 Carbon emissions and large neural network training,2021, arXivpreprint arXiv:2104
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 A primer in bertology: What we know abouthow bert works,2020, arXiv preprint arXiv:2002
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 The nature of statistical learning theory,2013, Springer science & business media
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 A multiscale visualization of attention in the transformer model,2019, arXiv preprintarXiv:1906
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 On theeffectiveness of low-rank matrix factorization for lstm model compression,2019, arXiv preprintarXiv:1908
 Huggingface’s transformers: State-of-the-artnatural language processing,2019, ArXiv
 Bert-of-theseus: Compressingbert by progressive module replacing,2020, arXiv preprint arXiv:2002
 Nas-bert: Task-agnostic and adaptive-size bert compression with neural architecture search,2021, arXiv preprintarXiv:2105
 Tensor-train recurrent neural networks for videoclassification,2017, In International Conference on Machine Learning
 Learningcompact recurrent neural networks with block-term tensor decomposition,2018, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Autotinybert: Automatichyper-parameter optimization for efficient pre-trained language models,2021, In Proceedings of the 59thAnnual Meeting of the Association for Computational Linguistics and the 11th International JointConference on Natural Language Processing (Volume 1: Long Papers)
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
