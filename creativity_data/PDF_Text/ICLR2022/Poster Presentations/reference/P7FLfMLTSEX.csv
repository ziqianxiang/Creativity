title,year,conference
 A convergence theory for deep learning viaover-parameterization,2018, CoRR
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In Interna-tional Conference on Machine Learning
 A closer look at memorization in deep netWorks,1706, arXiv:1706
 Poly-nl: Linear complexity non-local layers With polynomials,2021, InInternational Conference on Computer Vision (ICCV)
 Sharp analysis of loW-rank kernel matrix approximations,2012,	CoRR
 Neural Machine Translation byJointly Learning to Align and Translate,1409, arXiv:1409
 The convergence rate ofneural netWorks for learned functions of different frequencies,2019, CoRR
 Frequency bias in neural netWorks for input of non-uniform density,2020, CoRR
 Language Models are FeW-Shot Learn-ers,2020, arXiv:2005
 ToWards Understand-ing the Spectral Bias of Deep Learning,1912, arXiv:1912
 The product of tWo ultraspherical polynomials,2040, Glas-gow Mathematical Journal
 On Lazy Training in Differentiable Pro-gramming,1812, arXiv:1812
 Kernel Methods for Deep Learning,2009, InAdvances in Neural Information Processing Systems
 P-nets: Deep Polynomial Neural Networks,2020, pp
 Deep Polynomial Neural Networks,2021, pp
 On the power of over-parametrization in neural networks withquadratic activation,2018, In International Conference on Machine Learning (ICML)
 Gradient Descent Provably Op-timizes Over-parameterized Neural Networks,1810, arXiv:1810
 Spherical harmonics in p dimensions,2012, arXiv preprintarXiv:1205
 ImplicitBias of Gradient Descent on Linear Convolutional Networks,2018, In Advances inNeural Information Processing Systems
 GPipe: EfficientTraining of Giant Neural Networks using Pipeline Parallelism,1811, arXiv:1811
 Polynomial Theory of Complex Systems,2168, IEEE Transactions on Sys-tems
 Neural Tangent Kernel: Convergence andGeneralization in Neural Networks,1806, arXiv:1806
 Multiplicative interactionsand where to find them,2020, In International Conference on Learning Representations (ICLR)
 A style-based generator architecture for generativeadversarial networks,2019, In Conference on Computer Vision and Pattern Recognition (CVPR)
 A Style-Based Generator Architecture for Gen-erative Adversarial Networks,1812, arXiv:1812
 On the expressive power of deep polynomialneural networks,2019, Advances in neural information processing Systems (NeurIPS)
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, Advances in neural information processing systems
 Deep Image Prior,2575, In 2018 IEEE/CVFConference on Computer Vision and Pattern Recognition
 Optimization and gen-eralization of shallow neural networks with quadratic activation functions,2020, arXiv preprintarXiv:2006
 Optimal rates for averaged stochastic gradient descent un-der neural tangent kernel regime,2021, In International Conference on Learning Representations(ICLR)
 Continuous neural networks,2007, In Marina Meila and Xi-aotong Shen (eds
 The pi-sigma network: an efficient higher-order neural network forpattern classification and function approximation,1991, In IJCNN-91-Seattle International JointConference on Neural Networks
 TheImplicit Bias of Gradient Descent on Separable Data,1710, arXiv:1710
 Fourier fea-tures let networks learn high frequency functions in low dimensional domains,2020, InH
 Deep learning generalizes be-cause the parameter-function map is biased towards simple functions,2019, In International Con-ference on Learning Representations
 Sort: Second-order response transform for visual recognition,2017, In InternationalConference on Computer Vision (ICCV)
 Frequency principle:Fourier analysis sheds light on deep neural networks,2019, CoRR
 A fine-grained spectral perspective on neural networks,2019, arXivpreprint arXiv:1907
 Understand-ing deep learning requires rethinking generalization,2017, arXiv:1611
