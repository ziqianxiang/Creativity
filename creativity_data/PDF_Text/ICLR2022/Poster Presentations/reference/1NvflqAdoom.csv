title,year,conference
 Understanding intermediate layers using linear classifierprobes,2016, arXiv preprint arXiv:1610
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In Jennifer Dy and Andreas Krause (eds
 The peano-baker series,2011, Proceedings of the Steklov Instituteof Mathematics
 Implicit regUlarization via neUral featUre alignment,2021, In AISTATS
 To Understand deep learning we need to Un-derstand kernel learning,2018, In Jennifer Dy and Andreas KraUse (eds
 SpectrUm dependent learning cUrvesin kernel regression and wide neural networks,2020, In Hal Daume In and Aarti Singh (eds
 Finite dimensional linear systems,2015, SIAM
 Spectral bias and task-model alignmentexplain generalization in kernel regression and infinitely wide neural networks,2021, Nature Commu-nications
 On lazy training in differentiable program-ming,2019, In NeurIPS
 Separability and geometry ofobject manifolds in deep neural networks,2020, Nature communications
 Algorithms for learning kernels basedon centered alignment,2012, The Journal of Machine Learning Research
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, In NeurIPS
 Asymptotics of wide networks from feynman diagrams,2020, In Interna-tional Conference on Learning Representations
 Deep learning versus kernel learning: an empirical study of loss landscape geom-etry and the time evolution of the neural tangent kernel,2020, In H
 Effect of batch learning in multilayer neural networks,1998, Gen
 Landscape and training regimes in deeplearning,0370, Physics Reports
 Deep linearnetworks dynamics: Low-rank biases induced by initialization scale and l2 regularization,2021, arXivpreprint arXiv:2106
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, ArXiv
 Just interpolate: Kernel ”ridgeless” regression can gener-alize,2018, CoRR
 Capturing the learning curves of generic features maps for realistic data setswith a teacher-student model,2021, CoRR
 Neural tangents: Fast and easy infinite neural networks in python,2020, InInternational Conference on Learning Representations
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural network,2014, In In International Conference on LearningRepresentations
 Small random initialization is akin to spectral learning:OPtimization and generalization guarantees for overParameterized low-rank matrix reconstruc-tion,2021, arXiv PrePrint arXiv:2106
 Kernel and rich regimes in overparametrized models,2020, In JacobAbernethy and Shivani Agarwal (eds
 A unifying view on implicit bias in traininglinear neural networks,2020, arXiv PrePrint arXiv:2010
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRePresentations
