title,year,conference
 In Christopher J,2013, C
 Proteinbert: A univer-sal deep-learning model of protein sequence and function,2021, bioRxiv
 Language models are few-shotlearners,2020, In Hugo Larochelle
 Multifaceted protein-protein interaction prediction based on siameseresidual RCNN,2019, Bioinform
 Knowprompt: Knowledge-aware prompt-tuning with synergistic optimizationfor relation extraction,2021, CoRR
 Ontoed: Low-resource event detection with ontology embedding,2021, In ChengqingZong
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 Unified language model pre-training for natural languageunderstanding and generation,2019, In Hanna M
 Prottrans: Towards cracking the language of lifeâ€™s code through self-superviseddeep learning and high performance computing,2020, CoRR
 Predicting protein-proteininteractions through sequence-based deep learning,2018, Bioinform
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Heterogeneous network edge prediction: a dataintegration approach to prioritize disease-associated genes,2015, PLoS computational biology
 Sentiprompt: Sentiment knowledge enhanced prompt-tuningfor aspect-based sentiment analysis,2021, CoRR
 Deep neural network based predictions of proteininteractions using primary sequences,2018, Molecules
 Em-beddings from deep learning transfer go annotations beyond homology,2021, Scientific reports
 K-BERT: en-abling language representation with knowledge graph,2020, In The Thirty-Fourth AAAI Conference onArtificial Intelligence
 Learning unknown from corre-lations: Graph neural network for inter-novel-protein interaction prediction,2021, In Zhi-Hua Zhou(ed
 Deep contextualized word representations,2227, In Marilyn A
 Knowledge enhanced contextual word representations,2019, In Kentaro Inui
 Evaluating protein transfer learning with TAPE,2019, In Hanna M
 MSA transformer,2021, In Marina Meila and Tong Zhang (eds
 Transformerprotein language models are unsupervised structure learners,2021, In 9th International Conference onLearning Representations
 Injecting domain knowledge in neural net-works: A controlled experiment on a constrained problem,8230, In Peter J
 Onto2vec: joint vector-based representationof biological entities and their ontology-based annotations,2018, Bioinformatics
 Opa2vec: combining formal and informalcontent of biomedical ontologies to improve similarity-based prediction,2019, Bioinformatics
 Colake: Contextualized language and knowledge embedding,2020, In Donia Scott
 LXMERT: learning cross-modality encoder representations from trans-formers,2019, In Kentaro Inui
 Attention is all you need,2017, In Isabelle Guyon
 Bertology meets biology: Interpreting attention in protein language models,2021, In 9thInternational Conference on Learning Representations
 K-adapter: Infusing knowledge into pre-trained models withadapters,2021, In Chengqing Zong
 KEPLER: A unified model for knowledge embedding and pre-trained language representa-tion,2021, Trans
 Modeling protein using large-scale pretrain language model,2021, CoRR
 Pretrained encyclope-dia: Weakly supervised knowledge-pretrained language model,2020, In 8th International Conferenceon Learning Representations
 LUKE: deepcontextualized entity representations with entity-aware self-attention,6442, In Bonnie Webber
 Kformer:Knowledge injection in transformer feed-forward layers,2022, CoRR
 Long-tail relation extraction via knowledge graph embeddings and graph convolution net-works,2019, In Jill Burstein
 Alicg: Fine-grained and evolvableconceptual graph construction for semantic search at alibaba,2022, In Feida Zhu
 LOGEN: few-shot logical knowledge-conditioned textgeneration with self-training,2021, CoRR
 Deepke: A deep learning based knowledge extraction toolkitfor knowledge base population,2022, CoRR
 ERNIE: en-hanced language representation with informative entities,2019, In Anna Korhonen
 PRGC: potential relation and global correspondence basedjoint relational triple extraction,2021, In Chengqing Zong
 Thecafa challenge reports improved protein function prediction and new functional annotations forhundreds of genes through experimental screens,2019, Genome biology
