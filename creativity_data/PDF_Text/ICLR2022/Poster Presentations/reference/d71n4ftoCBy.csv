title,year,conference
 Federated learning based on dynamic regularization,2021, In International Con-ference on Learning Representations (ICLR)
 Adaptive gradient communication via critical learning regime identification,2021, In MachineLearning and Systems (MLSys)
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems (NeurIPS)
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Fed-erated learning with personalization layers,2019, arXiv preprint arXiv:1912
 Leaf: A benchmark for federated settings,2018, Advancesin Neural Information Processing Systems Workshops (NeurIPSW)
 Cinic-10 is not imagenetor cifar-10,2018, arXiv preprint arXiv:1810
 Batch normalization biases residual blocks towards the identity functionin deep networks,2020, In Advances in Neural Information Processing Systems (NeurIPS)
 HeteroFL: Computation and communication efficientfederated learning for heterogeneous clients,2021, In International Conference on Learning Represen-tations (ICLR)
 Feder-ated learning with compression: Unified analysis and sharp guarantees,2021, In Artificial Intelligenceand Statistics (AISTATS)
 Group knowledge transfer: Feder-ated learning of large cnns at the edge,2020, In Advances in Neural Information Processing Systems(NeurIPS)
 Fedml: A research library and benchmarkfor federated machine learning,2020, Advances in Neural Information Processing Systems Workshops(NeurIPSW)
 Delving deep into rectifiers: Surpass-ing human-level performance on imagenet classification,2015, In IEEE International Conference onComputer Vision (ICCV)
 The non-iid data quagmire ofdecentralized machine learning,2020, In International Conference on Machine Learning (ICML)
 A compressive sensingapproach for federated learning over massive mimo communication systems,2020, IEEE Transactionson Wireless Communications
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning (ICML)
 Efficient neural network com-pression,2019, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep gradient compression: Reducingthe communication bandwidth for distributed training,2018, In International Conference on LearningRepresentations (ICLR)
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In IEEE International Conference on Computer Vision (ICCV)
 Stable low-rank tensor de-composition for compression of convolutional neural network,2020, In European Conference on Com-puter Vision (ECCV)
 Communication-efficient federatedlearning with dual-side low-rank compression,2021, arXiv preprint arXiv:2104
 Introduction to tensor decomposi-tions and their applications in machine learning,2017, arXiv preprint arXiv:1711
 Adaptive federated optimization,2021, In InternationalConference on Learning Representations (ICLR)
 A survey of privacy attacks in machine learning,2020, arXiv preprintarXiv:2007
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations (ICLR)
 Im-plicit neural representations with periodic activation functions,2020, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Powersgd: Practical low-rank gradientcompression for distributed optimization,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Pufferfish: Communication-efficientmodels at no extra cost,2021, In Machine Learning and Systems (MLSys)
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems (NeurIPS)
 A review on energy efficient protocols in wireless sensornetworks,2016, WirelessNetworks
 Modeling the total energy consumption of mobile networkservices and applications,2019, Energies
 Federated learning with onlypositive labels,2020, In International Conference on Machine Learning (ICML)
 Federated accelerated stochastic gradient descent,2020, In Advances inNeural Information Processing Systems (NeurIPS)
 One-bit over-the-air aggregationfor commUnication-efficient federated edge learning: Design and convergence analysis,2020, IEEETransactions on Wireless Communications
