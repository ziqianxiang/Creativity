title,year,conference
 On the projected subgradient method fornonsmooth convex optimization in a Hilbert space,1998, Mathematical Programming
 A PID con-troller approach for stochastic optimization of deep networks,2018, In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition
 Stochastic Approximation and Its Applications,2006, Springer Science & Business Media
 On the convergence of a class of Adam-typealgorithms for non-convex optimization,2019, In International Conference on Learning Representa-tions
 SADAGRAD: Strongly adaptive stochasticgradient methods,2018, In International Conference on Machine Learning
 A robust accelerated optimizationalgorithm for strongly convex functions,2018, In American Control Conference
 A simple convergence proofof Adam and Adagrad,2020, arXiv preprint arXiv:2003
 Adaptive subgradient methods for online learning andstochastic optimization,2010, In 23rd Conference on Learning Theory
 Adaptive subgradient methods for online learning andstochastic optimization,2011, Journal of Machine Learning Research
 Understanding the role of momentumin stochastic gradient methods,2019, Advances in Neural Information Processing Systems
 Speech recognition with deeprecurrent neural networks,2013, In 2013 IEEE International Conference on Acoustics
 Stochastic analog of the conjugate-gradient method,1972, Cybernetics
 Beyond the regret minimization barrier: Optimal algorithms forstochastic strongly-convex optimization,2014, The Journal of Machine Learning Research
 Reducing the dimensionality of data with neuralnetworks,2006, Science
 Behaviour in the limit of iterations of the stochastic two-step method,1983, ZhurnalVychislitelâ€™noi Matematiki i Matematicheskoi Fiziki
 Combining ordered subsets and momentumfor accelerated X-ray CT image reconstruction,2014, IEEE Transactions on Medical Imaging
 Adam: A method for stochastic optimization,2015, In InternationalConference for Learning Representations
 Imagenet classification with deep con-volutional neural networks,2012, Advances in Neural Information Processing Systems
 Canonical tensor decomposition forknowledge base completion,2018, In International Conference on Machine Learning
 A high probability analysis of adaptive sgd with momentum,2020, 2020
 On the convergence of stochastic gradient descent with adaptivestepsizes,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 An improved analysis of stochastic gradient descent withmomentum,2020, arXiv preprint arXiv:2007
 Adaptive bound optimization for online convex opti-mization,2010, 2010
 Robust stochasticapproximation approach to stochastic programming,2009, SIAM Journal on Optimization
 Introductory Lectures on Convex Optimization: A Basic Course,2004, Introductory Lectureson Convex Optimization: A Basic Course
 SGD and Hogwild! convergence without the bounded gradients assumption,2018, In Interna-tional Conference on Machine Learning
 Some methods of speeding up the convergence of iteration methods,1964, USSR Com-putational Mathematics & Mathematical Physics
 Comparison of convergence rate of one-step and multistep optimization algorithmsin the presence of noise,1977, Izv
 A stochastic approximation method,1951, Annals of MathematicalStatistics
 Almost sure convergence rates for stochastic gradientdescent and stochastic heavy ball,2020, 2020
 Pegasos: Primal estimatedsub-gradient solver for SVM,2011, Mathematical programming
 Less regret via online conditioning,2010, Computer Science
 Lecture 6,2012,5-RMSProp
 On almost sure convergence for sums of stochasticsequence,2019, Communications in Statistics-Theory and Methods
 Numerical diagnostics for systems of differential alge-braic equations,2016, arXiv preprint arXiv:1602
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2016, arXiv preprint arXiv:1604
 A sufficient condition for conver-gences of Adam and RMSProp,2019, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
