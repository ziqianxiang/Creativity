title,year,conference
 Physics-based models for measurement correlations,0550, application to an inverseSturm-Liouville problem
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine learningpractice and the bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Two models of double descent for weak features,2020, SIAMJournal on Mathematics of Data Science
 On the inductive bias of neural tangent kernels,2019, Advances in Neural Infor-mation Processing Systems
 Spectrum dependent learning curves inkernel regression and wide neural networks,2020, In International Conference on Machine Learning
 Spectral bias and task-model alignmentexplain generalization in kernel regression and infinitely wide neural networks,2021, Nature communi-cations
 Deep neural tangent kernel and Lsaplace kernel have the same rkhs,2020, arXivpreprint arXiv:2009
 Toward deeper understanding of neural networks: The powerof initialization and a dual view on expressivity,2016, Neural Information Processing Systems (NIPS)
 Regularization of Inverse Problems,1996, Kluwer AcademicPublishers
 On the similarity between theLaplace and neural tangent kernels,2020, arXiv:2007
 Inverse Problems for Partial Differential Equations,2006, Springer-Verlag
 Semi-supervised deep kernel learning: Regression with unlabeleddata by minimizing predictive variance,2018, NIPS
 Statistical and Computational Inverse Problems,2005, Applied MathematicalSciences
 Semi-supervised learning via compact la-tent space clustering,2018, In International Conference on Machine Learning
 An Introduction to the Mathematical Theory of Inverse Problems,2011, Springer-Verlag
 Generalization error of minimum weighted norm and kernel interpolation,2021, SIAM Journalon Mathematics of Data Science
 Towards a unified analysis of randomFourier features,2021, Journal of Machine Learning Research
 Towards an understanding of benign overfitting in neuralnetworks,2021, arXiv preprint arXiv:2106
 Fourier neural operator for parametric partial differentialequations,2020, arXiv preprint arXiv:2010
 On the spectrum of random features maps of high dimensionaldata,2018, Proceedings of the 35th International Conference on Machine Learning
 Kernel regression in high dimensions: Refinedanalysis beyond double descent,2021, Proceedings of The 24th International Conference on ArtificialIntelligence and Statistics (PMLR)
 Implicit self-regularization in deep neural networks:Evidence from random matrix theory and implications for learning,2018, arXiv:1810
 Sparse recovery with non-linear Fourier features,2020, 2020 IEEE International Confer-ence on Acoustics
 Random features for large-scale kernel machines,2008, In Advances inNeural Information Processing Systems
 Gaussian Processes for Machine Learning,2006, MIT Press
 The convergence rate of neural net-works for learned functions of different frequencies,2019, Advances in Neural Information ProcessingSystems
 On sampling random features from empirical leveragescores: Implementation and theoretical guarantees,2019, arXiv:1903
 Optimal rates for random Fourier features,2015, Proceedingsof the 28th International Conference on Neural Information Processing Systems
 Inverse Problem Theory and Methods for Model Parameter Estimation,2005, SIAM
 High-frequency component helps explainthe generalization of convolutional neural networks,2020, CVPR
 Overparameterization and general-ization error: weighted trigonometric interpolation,2020, arXiv preprint arXiv:2006
 Implicit regularization effects of the Sobolev norms inimage processing,2021, arXiv:2109
