title,year,conference
 Understanding deep neuralnetworks with rectified linear units,2018, In International Conference on Learning Representations
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Initialization of ReLUs for dynamical isometry,2019, In Ad-vances in Neural Information Processing Systems
 The lottery ticket hypothesis for pre-trained bert networks,2020, In Advances inNeural Information Processing Systems
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 The carbon impact of artificial intelligence,2020, Nat Mach Intell
 Multi-prize lottery ticket hypothesis: Finding accuratebinary neural networks by pruning a randomly weighted network,2021, In International Conference onLearning Representations
 Towards strong pruning for lottery tickets with non-zerobiases,2021, arXiv
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Approximation capabilities of multilayer feedforward networks,1991, Neural Networks
 MNIST handwritten digit database,2010, 2010
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruning basedon connection sensitivity,2019, In International Conference on Learning Representations
 Dynamic model pruningwith feedback,2020, In International Conference on Learning Representations
 Exponentially small bounds on the expected optimum of the partition and subsetsum problems,1998, Random Structures & Algorithms
 Proving the lottery tickethypothesis: Pruning is all you need,2020, In International Conference on Machine Learning
 One ticket to win them all: gener-alizing lottery ticket initializations across datasets and optimizers,2019, In Advances in Neural Infor-mation Processing Systems
 Logarithmic pruning is all you need,2020, Ad-vances in Neural Information Processing Systems
 Op-timal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient,2020, In Advancesin Neural Information Processing Systems
 Approximation theory of the mlp model in neural networks,1999, Acta Numerica
 Deep learning in neural networks: An overview,2015, Neural Networks
 Nonparametric regression using deep neural networks with ReLU acti-vation function,2020, The Annals of Statistics
 The kolmogorov-arnold representation theorem revisited,2021, Neural Net-works
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, In Advances in Neural InformationProcessing Systems
 Benefits of depth in neural networks,2016, In Conference on Learning Theory
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Error bounds for approximations with deep relu networks,2017, Neural Networks
 Optimal approximation of continuous functions by very deep relu networks,2018, InConference On Learning Theory
