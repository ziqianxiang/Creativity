title,year,conference
 Etc: Encoding long and structuredinputs in transformers,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 End-to-end object detection with transformers,2020, In European Conference on ComputerVision
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Smyrf: Efficient attentionusing asymmetric clustering,2020, arXiv preprint arXiv:2010
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Flax: A neural network library and ecosystem for jax,2020, Version 0
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In International Conference on MachineLearning
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Learning multiple layers of features from tiny images,2009,2009
 Generating wikipedia by summarizing long sequences,2018, arXiv preprint arXiv:1801
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Sparse and constrained attention forneural machine translation,2018, arXiv preprint arXiv:1805
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 On evaluation of adversarial perturba-tions for sequence-to-sequence models,2019, arXiv preprint arXiv:1903
 Listops: A diagnostic dataset for latent tree learning,2018, arXivpreprint arXiv:1804
 The aclanthology network corpus,2013, Language Resources and Evaluation
 Compressivetransformers for long-range sequence modelling,2019, arXiv preprint arXiv:1911
 Random features for large-scale kernel machines,2007, In NIPS
 Efficient content-based sparseattention with routing transformers,2021, Transactions of the Association for Computational Linguistics
 Adaptive attentionspan in transformers,2019, arXiv preprint arXiv:1905
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Sparse sinkhorn attention,2020, InInternational Conference on Machine Learning
 Long range arena: A benchmark for efficienttransformers,2020, arXiv preprint arXiv:2011
 Predictingattention sparsity in transformers,2021, arXiv preprint arXiv:2109
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Fast transformers with clusteredattention,2020, Advances in Neural Information Processing Systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 A survey on learning to hash,2017, IEEEtransactions on pattern analysis and machine intelligence
 Cluster-former: Clustering-based sparse transformer for long-range dependencyencoding,2020, arXiv preprint arXiv:2009
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Autoformer: Decomposition transformerswith auto-correlation for long-term series forecasting,2021, arXiv preprint arXiv:2106
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Slurm: Simple linux utility for resource man-agement,2003, In Workshop on job scheduling Strategiesfor parallel processing
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
