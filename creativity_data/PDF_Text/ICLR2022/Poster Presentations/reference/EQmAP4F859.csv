title,year,conference
 Implicit regularization in deep matrixfactorization,2019, Advances in Neural Information Processing Systems
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Two models of double descent for weak features,2020, SIAMJournal on Mathematics of Data Science
 Large scale online learning,2004, Advances in neural information Pro-cessing systems
 Stability and generalization,2002, The Journal OfMachine Learn-ing Research
 Towards understanding thespectral bias of deep learning,2019, arXiv PrePrint arXiv:1912
 Stability and convergence trade-off of iterative optimizationalgorithms,2018, arXiv PrePrint arXiv:1804
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv PrePrint arXiv:1810
 The spectrum of kernel random matrices,2010, The Annals of Statistics
 Sobolev norm learning rates for regularized least-squares algo-rithms,2020, J
 Deep learning versus kernel learning: an empirical study of loss landscapegeometry and the time evolution of the neural tangent kernel,2020, arXiv PrePrint arXiv:2010
 Linearized two-layersneural networks in high dimension,2021, The Annals of Statistics
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv PrePrint arXiv:1806
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Conference On Learning The-ory
 Enhanced convolutional neural tangent kernels,2019, arXiv PrePrint arXiv:1911
 Just interpolate: Kernel “ridgeless” regression can gener-alize,2020, TheAnnals ofStatistics
 Kernel regression in high dimensions: Refined anal-ysis beyond double descent,2021, In International Conference on Artificial Intelligence and Statistics
 Generalization error of random fea-tures and kernel methods: hypercontractivity and kernel matrix concentration,2021, arXiv preprintarXiv:2101
 Learning with invariances in randomfeatures and kernel models,2021, arXiv preprint arXiv:2102
 Deepdouble descent: Where bigger models and more data hurt,2019, arXiv preprint arXiv:1912
 Sgd on neural networks learns functions of increasing complexity,2019, Advancesin Neural Information Processing Systems
 The deep bootstrap framework: Goodonline learners are good offline generalizers,2020, arXiv preprint arXiv:2010
 Statistical optimality of stochastic gradi-ent descent on hard learning problems through multiple passes,2018, arXiv preprint arXiv:1805
 Early stopping and non-parametric regression:an optimal data-dependent stopping rule,2014, The Journal of Machine Learning Research
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Implicit regularization for optimalsparse recovery,2019, Advances in Neural Information Processing Systems
 On early stopping in gradient descent learn-ing,2007, Constructive Approximation
