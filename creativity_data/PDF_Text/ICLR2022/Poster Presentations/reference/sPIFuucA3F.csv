title,year,conference
 Improved algorithms for lin-ear stochastic bandits,2011, In John Shawe-Taylor
 Tamingthe monster: A fast and simple algorithm for contextual bandits,2014, In International Conference onMachine Learning
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Policy learning with observational data,2021, Econometrica
 Using confidence bounds for exploitation-exploration trade-offs,1532, J
 Layer normalization,2016, arXiv preprintarXiv:1607
 Fit without fear: remarkable mathematical phenomena of deep learning through theprism of interpolation,2021, arXiv preprint arXiv:2105
 X-armed bandits,2011, Journal ofMachine Learning Research
 The importance of pessimism in fixed-dataset policy optimization,2020, arXiv preprint arXiv:2009
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, Advances in Neural Information Processing Systems
 On the generalization ability of on-linelearning algorithms,2004, IEEE Trans
 Gradient descent findsglobal minima of deep neural networks,2019, In ICML
 Gradient descent ProVably optimizesover-parameterized neural networks,2019, In ICLR (Poster)
 Minimax-optimal off-policy eValuation with linear function approx-imation,2020, CoRR
 Risk bounds and rademacher complexity in batch reinforcementlearning,2021, arXiv preprint arXiv:2103
 Off-policy deep reinforcement learning withoutexploration,2019, In International Conference on Machine Learning
 Guidelines for reinforcement learning in healthcare,2019, Naturemedicine
 Finite depth and width corrections to the neural tangent kernel,2019, arXivpreprint arXiv:1909
 Fast rates for the regret of offline reinforcementlearning,2021, In COLT
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv preprint arXiv:1806
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Who should be treated? empirical welfare maximiza-tion methods for treatment choice,2018, Econometrica
 Multi-armed bandits in metric spaces,2008, InProceedings of the fortieth annual ACM symposium on Theory of computing
 Contextual gaussian process bandit optimization,2011, In Nips
 Batch reinforcement learning,2012, In Reinforce-ment learning
 Bandit Algorithms,2020, Cambridge University Press
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Human-levelcontrol through deep reinforcement learning,2015, nature
 On the proof of global convergence of gradient descent for deep relu networks withlinear widths,2021, arXiv preprint arXiv:2101
 Learning when-to-treat policies,2021, Journal of theAmerican Statistical Association
 Lever-aging good representations in linear contextual bandits,2021, arXiv preprint arXiv:2104
 Bridging offline rein-forcement learning and imitation learning: A tale of pessimism,2021, arXiv preprint arXiv:2103
 Deep bayesian bandits showdown: Anempirical comparison of bayesian deep networks for thompson sampling,2018, arXiv preprintarXiv:1802
 Gaussian pro-cess optimization in the bandit setting: No regret and experimental design,2009, arXiv preprintarXiv:0912
 Learning from logged implicit explo-ration data,2010, arXiv preprint arXiv:1003
 Combiningonline learning and offline learning for contextual bandits with deficient support,2021, arXiv preprintarXiv:2107
 Finite-timeanalysis of kernelised contextual bandits,2013, arXiv preprint arXiv:1309
 On the optimality of batch policy optimization algorithms,2021, In InternationalConference on Machine Learning
 Neural contextual bandits with deeprepresentation and shallow exploration,2020, arXiv preprint arXiv:2012
 On function approx-imation in reinforcement learning: Optimism in the face of large state spaces,2020, arXiv preprintarXiv:2011
 Near-optimal provable uniform convergence in offlinepolicy evaluation for reinforcement learning,2021, In Arindam Banerjee and Kenji Fukumizu (eds
 Off-policy evaluation via adap-tive weighting with data from contextual bandits,2021, In Proceedings of the 27th ACM SIGKDDConference on Knowledge Discovery & Data Mining
