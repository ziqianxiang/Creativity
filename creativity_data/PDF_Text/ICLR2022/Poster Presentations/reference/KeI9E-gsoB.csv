title,year,conference
 Statistical theory of learning curves under entropic loss criterion,1993, NeuralComputation
 Four types of learning curves,1992, Neural Computation
 On exact computation with aninfinitely wide neural net,2019, In Advances in Neural Information Processing Systems
 Explaining neural scaling laws,2021, arXiv preprintarXiv:2102
 Information-theoretic characterization of Bayes performance and the choice of priors inparametric and nonparametric problems,1998, InD
 On the inductive bias of neural tangent kernels,2019, In Advances in NeuralInformation Processing Systems
 Spectrum dependent learning curves in kernel regressionand wide neural networks,2020, In Proceedings of the 37th International Conference on MachineLearning (ICML)
 Accurate error bounds for the eigenvalues of the kernel matrix,2006, The Journal of MachineLearning Research
 Spectral bias and task-model alignment explaingeneralization in kernel regression and infinitely wide neural networks,2021, Nature communications
 Optimal rates for the regularized least-squares algorithm,2007, Foundationsof Computational Mathematics
 Online learning with kernel losses,2019, In Proceedings of the36th International Conference on Machine Learning (ICML)
 Kernel methods for deep learning,2009, In Advances in Neural InformationProcessing Systems
 Generalization error rates in kernel regression:The crossover from the noiseless to noisy regime,2021, arXiv preprint arXiv:2105
 Toward deeper understanding of neural networks: The powerof initialization and a dual view on expressivity,2016, In Advances In Neural Information ProcessingSystems
 Gaussian processbehaviour in wide deep neural networks,2018, In International Conference on Learning Representations
 Every model learned by gradient descent is approximately a kernel machine,2020, arXivpreprint arXiv:2012
 Deep convolutional networks as shallowgaussian processes,2019, In International Conference on Learning Representations
 Rigorous learning curve bounds from statisticalmechanics,1996, Machine Learning
 Neural tangent kernel: Convergence and generalization in neuralnetworks,2018, In Advances in Neural Information Processing Systems
 Kernel truncated randomized ridge regression: Optimal ratesand low noise acceleration,2019, Advances in Neural Information Processing Systems
 Gaussian processes and kernelmethods: A review on connections and equivalences,2018, arXiv preprint arXiv:1807
 Deep neural networksas gaussian processes,2018, In International Conference on Learning Representations
 Wide neuralnetworks of any depth evolve as linear models under gradient descent,2019, In Advances in NeuralInformation Processing Systems
 Finiteversus infinite neural networks: an empirical study,2020, In Advances in Neural Information ProcessingSystems
 Minimizers of the empirical risk and risk monotonicity,2019, In Advancesin Neural Information Processing Systems
 Learning curves for Gaussian processes regression: A framework forgood approximations,2001, In Advances in Neural Information Processing Systems
 Learning curves for Gaussian processes models: Fluctuations anduniversality,2001, In International Conference on Artificial Neural Networks
 Bayesian Learning for Neural Networks,1996, Springer-Verlag
 Optimal rates for averaged stochastic gradient descent under neural tangentkernel regime,2021, In International Conference on Learning Representations
 A variational approach to learning curves,2002, In Advances in NeuralInformation Processing Systems
 Bayesian nonparametric models,2010, In Encyclopedia of Machine Learning
 Multivariate integration and approximationfor random fields satisfying Sacks-Ylvisaker conditions,1995, The Annals of Applied Probability
 The convergence rate of neural networks forlearned functions of different frequencies,2019, Advances in Neural Information Processing Systems
 Information consistency of nonparametric Gaussianprocess methods,2008, IEEE Transactions on Information Theory
 Learning curves for Gaussian processes,1999, In Advances in Neural Information ProcessingSystems
 Gaussian process regression with mismatched models,2001, In Advances in Neural InformationProcessing Systems
 Learning curves for Gaussian process regression: Approximations andbounds,2002, Neural Computation
 Asymptotic learning curves of kernel methods: empirical dataversus teacher-student paradigm,2020, Journal of Statistical Mechanics: Theory and Experiment
 Interpolation of spatial data: Some theory for kriging,2012, Springer Science & BusinessMedia
 Optimal rates for regularized least squares regression,2009, InConference on Learning Theory
 User-friendly tail bounds for sums of random matrices,2012, Foundations of computationalmathematics
 On information gain and regret bounds in Gaussian processbandits,2021, In International Conference on Artificial Intelligence and Statistics
 Relative concentration bounds for the spectrum of kernel matrices,2018, arXiv preprintarXiv:1812
 Universal scaling laws in the gradient descent training of neuralnetworks,2021, arXiv preprint arXiv:2105
 The shape of learning curves: A review,2021, arXiv preprint arXiv:2103
 Algebraic Geometry and Statistical Learning Theory,2009, Cambridge University Press
 Asymptotic behavior of the eigenvalues of certain integral equations,1963, Transactions ofthe American Mathematical Society
 Computing with infinite networks,1997, In Advances in Neural Information ProcessingSystems
 Gaussian processes for machine learning,2006, MIT press
 A fine-grained spectral perspective on neural networks,2019, arXiv preprintarXiv:1907
