title,year,conference
 Meta-learning by adjusting priors based on extended pac-bayes theory,2018, InICML
 Provable guarantees for gradient-basedmeta-learning,2019, In International Conference on Machine Learning
 Meta-learning with negative learning rates,2021, In International Conference onLearning Representations
 A handbook of gamma-convergence,2006, In Handbook of Differential Equations:stationary partial differential equations
 On the outsized importance of learning rates in local updatemethods,2020, arXiv preprint arXiv:2007
 Why does maml outperform erm? anoptimization perspective,2020, arXiv preprint arXiv:2010
 Learning-to-learn stochas-tic gradient descent with biased regularization,2019, In International Conference on Machine Learning
 On the convergence theory of gradient-basedmodel-agnostic meta-learning algorithms,2020, In International Conference on Artificial Intelligenceand Statistics
 Meta-learning and universality: Deep representations and gradi-ent descent can approximate any learning algorithm,2018, In International Conference on LearningRepresentations
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In Proceedings of the 34th International Conference on Machine Learning-Volume70
 Probabilistic model-agnostic meta-learning,2018, InAdvances in Neural Information Processing Systems
 Online meta-learning,2019, InInternational Conference on Machine Learning
 Modeling and optimization trade-off in meta-learning,2020, Advances inNeural Information Processing Systems
 Recasting gradient-based meta-learning as hierarchical bayes,2018, arXiv preprint arXiv:1801
 Meta-learning priors for efficient onlinebayesian regression,2018, arXiv preprint arXiv:1807
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Meta-learning representations for continual learning,2019, arXivpreprint arXiv:1905
 Convergence of meta-learning withtask-specific adaptation over partial parameters,2020, In Advances in Neural Information ProcessingSystems
 Multi-step model-agnostic meta-learning: Convergenceand improved algorithms,2020, arXiv preprint arXiv:2002
 Adaptive gradient-based meta-learning methods,2019, In Advances in Neural Information Processing Systems
 One shot learning ofsimple visual concepts,2011, In Proceedings of the annual meeting of the cognitive science society
 Meta-sgd: Learning to learn quickly for few-shotlearning,2017, arXiv preprint arXiv:1707
 Towards fast adaptation of neural architectures with meta learning,2019, In InternationalConference on Learning Representations
 On first-order meta-learning algorithms,2018, arXivpreprint arXiv:1803
 Rapid learning or featurereuse? towards understanding the effectiveness of maml,2020, In International Conference on LearningRepresentations
 Meta-learning with implicitgradients,2019, In Advances in Neural Information Processing Systems
 Meta-learning with latent embedding optimization,2018, arXiv preprintarXiv:1807
 Some better bounds on the variance withapplications,2010, J
 Prototypical networks for few-shot learning,2017, InAdvances in Neural Information Processing Systems
 Matchingnetworks for one shot learning,2016, In Advances in Neural Information Processing Systems
 Guarantees for tuning the step size using alearning-to-learn approach,1098, In International Conference on Machine Learning
 Meta-learningwithout memorization,2020, In International Conference on Learning Representations
 Efficient meta learning viaminibatch proximal update,2019, In Advances in Neural Information Processing Systems
 Tasksimilarity aware meta learning: Theory-inspired improvement on maml,2020, In 4th Workshop onMeta-Learning at NeurIPS
