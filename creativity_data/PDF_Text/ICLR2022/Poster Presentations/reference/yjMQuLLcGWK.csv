title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 Recurrent glimpse-based decoder for detection withtransformer,2021, arXiv preprint arXiv:2112
 The cityscapes dataset for semantic urbanscene understanding,2016, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Up-detr: Unsupervised pre-training forobject detection with transformers,2020, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 You only look at one sequence: Rethinking transformer in vision through objectdetection,2021, arXiv preprint arXiv:2106
 Making pre-trained language models better few-shotlearners,2020, arXiv preprint arXiv:2012
 Rich feature hierarchies for ac-curate object detection and semantic segmentation,2014, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Recent advances in deep learning theory,2020, arXiv preprintarXiv:2012
 Rethinking imagenet pre-training,2019, In Proceedings ofthe IEEE/CVF International Conference on Computer Vision
 Using pre-training can improve model robustnessand uncertainty,2019, In International Conference on Machine Learning
 Deeply-supervised nets,2015, In Artificial intelligence and statistics
 Microsoft coco: Common objects in context,2014, In ECCV
 Ssd: Single shot multibox detector,2016, In European conference on computervision
 SWin transformer: Hierarchical vision transformer using shifted WindoWs,2021, arXiv preprintarXiv:2103
 Decoupled Weight decay regularization,2018, In International Confer-ence on Learning Representations
 Conditional detr for fast training convergence,2021, In Proceedings of the IEEE interna-tional conference on computer vision
 Benchmarking robustness in object detec-tion: Autonomous driving When Winter is coming,2019, arXiv preprint arXiv:1907
 Faster r-cnn: towards real-time objectdetection with region proposal networks,2016, IEEE transactions on pattern analysis and machineintelligence
 Imagenet-21k pretraining forthe masses,2021, arXiv preprint arXiv:2104
 It's not just size that matters: Small language models are alsofew-shot learners,2020, arXiv preprint arXiv:2009
 Locate andlabel: A two-stage identifier for nested named entity recognition,2021, In ACL
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Training data-efficient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Goingdeeper with image transformers,2021, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 Attention is all you need,2017, In Conference on Neural In-formation Processing Systems
 Vitae: Vision transformer advanced byexploring intrinsic inductive bias,2021, Advances in Neural Information Processing Systems
 Deformable detr: De-formable transformers for end-to-end object detection,2020, In International Conference on Learningand Representations
