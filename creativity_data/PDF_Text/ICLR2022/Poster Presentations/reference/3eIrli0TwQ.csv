title,year,conference
 Deep learning with differential privacy,2016, In CCS
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 The secret sharer:Evaluating and testing unintended memorization in neural networks,2019, In 28th USENIX SecuritySymposium
 Extracting training datafrom large language models,2020, arXiv preprint arXiv:2012
 Label-onlymembership inference attacks,1964, In International Conference on Machine Learning
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Calibrating noise to sensitivityin private data analysis,2006, In Theory of cryptography conference
 Model inversion attacks that exploit confi-dence information and basic countermeasures,2015, In Proceedings of the 22nd ACM SIGSAC confer-ence on computer and communications security
 An empiri-cal investigation of catastrophic forgetting in gradient-based neural netWorks,2013, arXiv preprintarXiv:1312
 Resolving indi-viduals contributing trace amounts of DNA to highly complex mixtures using high-density SNPgenotyping microarrays,2008, PLoS genetics
 Re-visiting membership inference under realistic assumptions,2021, Proceedings on Privacy EnhancingTechnologies
 Memguard:Defending against black-box membership inference attacks via adversarial examples,2019, In Pro-ceedings of the 2019 ACM SIGSAC conference on computer and communications security
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Stolen memories: Leveraging model memorization for calibratedwhite-box membership inference,2020, In 29th USENIX Security Symposium
 Label-leaks: Membership inference attack with label,2020, arXiv e-prints
 Understanding membership inferences on well-generalized learning mod-els,2018, arXiv preprint arXiv:1802
 Renyi differential privacy,2017, In 2017 IEEE 30th Computer Security Foundations Sym-posium (CSF)
 Renyi differential privacy of the sampled gaussianmechanism,2019, arXiv preprint arXiv:1908
 Comprehensive privacy analysis of deep learning:Passive and active white-box inference attacks against centralized and federated learning,2019, In 2019IEEE symposium on security and privacy (SP)
 Adver-sary instantiation: Lower bounds for differentially private machine learning,2021, arXiv preprintarXiv:2101
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 On the difficulty of membership inference attacks,2021, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 White-box vs black-box: Bayes optimal strategies for membership inference,2019, In International Confer-ence on Machine Learning
 Approximation analysis of stochastic gradient Langevin dynam-ics by using Fokker-Planck equation and Ito process,2014, In International Conference on MachineLearning
 Membership inference at-tacks against machine learning models,2017, In 2017 IEEE Symposium on Security and Privacy (SP)
 An empirical study of example forgetting during deep neural networklearning,2018, arXiv preprint arXiv:1812
 Privacy risk in machine learn-ing: Analyzing the connection to overfitting,2018, In 2018 IEEE 31st Computer Security FoundationsSymposium (CSF)
 Opacus: User-friendlydifferential privacy library in pytorch,2021, 2021
 Protecting aggregate genomic data,2008, Science
