title,year,conference
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 ¡°learning-compression¡± algorithms for neuralnet pruning,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 The lottery tickets hypothesis for supervised and self-supervised pre-trainingin computer vision models,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Theelastic lottery ticket hypothesis,2021, Advances in Neural Information Processing Systems
 Binarizedneural networks: Training deep neural networks with weights and activations constrained to+ 1or-1,2016, arXiv preprint arXiv:1602
 Bert: Pre-training of deep bidi-rectional transformers for language understanding,2019, In NAACL-HLT
 The difficulty of training sparseneural networks,2019, arXiv preprint arXiv:1906
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Dynamic network surgery for efficientdnns,2016, In NIPS
"	Learningboth weights and connections for efficient neural network",1135,"	In NIPS"
 Optimal brain surgeon and general networkpruning,1993, In IEEE international conference on neural networks
 Latent weights do not exist: Rethinking binarized neural network optimiza-tion,2019, In H
 Adabits: Neural network quantization with adaptive bit-widths,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)
 Optimalbrain damage,1989, In NIPs
 SNIP: SINGLE-SHOT NETWORKPRUNING BASED ON CONNECTION SENSITIVITY,2019, In International Conference on Learn-ing Representations
 Proving the lottery tickethypothesis: Pruning is all you need,2020, In International Conference on Machine Learning
 One ticket to win them all: gen-eralizing lottery ticket initializations across datasets and optimizers,2019, In NeurIPS
 Logarithmic pruning is all you need,2020, Ad-vances in Neural Information Processing Systems
 Finding every-thing within random binary networks,2021, arXiv preprint arXiv:2110
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 E2-train: Training state-of-the-art cnns with over 80% energy savings,2019, arXiv preprintarXiv:1910
 Supermasks in superposition,2020, Advances in Neural InformationProcessing Systems
 Drawing early-bird tickets: Towards more efficient trainingof deep networks,2019, arXiv preprint arXiv:1909
