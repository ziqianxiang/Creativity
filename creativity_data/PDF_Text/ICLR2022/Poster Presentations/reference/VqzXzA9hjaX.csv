title,year,conference
 Deep learning with differential privacy,2016, Proceedings of the 2016 ACM SIGSACConference on Computer and Communications Security
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in neural information processing systems
 Model compression,2006, In Proceedingsof the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
 Learning to optimize in swarms,2019, InAdvances in Neural Information Processing Systems
 Training stronger baselines for learning to optimize,2020, arXiv preprint arXiv:2010
 Robust overfittingmay be mitigated by properly learned smoothening,2021, In International Conference on LearningRepresentations
 Automated synthetic-to-real generalization,2020, In International Conference on Machine Learning
 Stabilizing differentiable architecture search via perturbation-based regularization,2020, CoRR
 Understanding andoptimizing asynchronous low-precision stochastic gradient descent,2017, In Proceedings of the 44thAnnual International Symposium on Computer Architecture
 Stochastic first order methods in smooth convex optimization,2011, Technicalreport
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Stochastic optimization with heavy-tailed noise via accelerated gradient clipping,2020, arXiv preprint arXiv:2005
 Deep residual learning for imagerecognition,2015, CoRR
 Parametric noise injection: Trainable randomnessto improve deep neural network robustness against adversarial attack,2019, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Non-convex optimization for machine learning,2017, arXiv preprintarXiv:1712
 Learning to defense by learningto attack,2018, arXiv preprint arXiv:1811
 Learning multiple layers of features from tiny images,2009,2009
 MNIST handwritten digit database,2010,2010
 Aportfolio approach to algorithm selection,2003, In IJCAI
 Halo: Hardware-awarelearning to optimize,2020, In European Conference on Computer Vision
 Knowl-edge amalgamation from heterogeneous networks by common feature learning,2019, arXiv preprintarXiv:1906
 Learning gradient descent: Better generalization and longerhorizons,2017, In Proceedings of the 34th International Conference on Machine Learning-Volume 70
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Readingdigits in natural images with unsupervised feature learning,2011,2011
 On first-order meta-learning algorithms,2018, arXivpreprint arXiv:1803
 Descending through a crowded valley -benchmarking deep learning optimizers,2020, CoRR
 Amalgamating knowl-edge towards comprehensive classification,2019, Proceedings of the AAAI Conference on Artifi-cial Intelligence
 Customizing stu-dent networks from heterogeneous teachers via adaptive knowledge amalgamation,2019, In Proceedingsof the IEEE/CVF International Conference on Computer Vision (ICCV)
 Learning a minimax optimizer: A pilot study,2021, In International Conference on LearningRepresentations
 A tail-index analysis of stochastic gradientnoise in deep neural networks,5827, In International Conference on Machine Learning
 Practical bayesian optimization of machinelearning algorithms,2012, Advances in neural information processing systems
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results,2017, arXiv preprint arXiv:1703
 Progressive blockwise knowledge distillation for neuralnetwork acceleration,2018, In Proceedings of the Twenty-Seventh International Joint Conference onArtificial Intelligence
 No free lunch theorems for optimization,1997, IEEE Transactions onEvolutionary Computation
 Adversarial weight perturbation helps robust general-ization,2020, Advances in Neural Information Processing Systems
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, 08 2017
 Satzilla: portfolio-based algorithmselection for sat,2008, Journal of artificial intelligence research
 Data-free knowledge amalgamationvia group-stack dual-gan,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 L2-gcn: Layer-wise and learnedefficient training of graph convolutional networks,2020, In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition
 Revisiting knowledge distillation vialabel smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Improving the robustness of deepneural networks via stability training,2016, In Proceedings of the ieee conference on computer visionand pattern recognition
