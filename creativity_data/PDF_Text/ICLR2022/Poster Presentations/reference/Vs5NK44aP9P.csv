title,year,conference
 Double Viterbi: Weight encoding for high compression ratioand fast on-chip reconstruction for deep neural network,2019, In International Conference on LearningRepresentations (ICLR)
 The Viterbi algorithm,1973, Proc
 The state of sparsity in deep neural networks,2019, arXiv preprintarXiv:1902
 Sparse gpu kernels for deep learning,2020, arXiv preprintarXiv:2006
 Run-length encodings,1966, IEEE Transactions on Information Theory
 Learning both weights and connections for efficient neuralnetworks,2015, In Advances in Neural Information Processing Systems
 EIE: efficient inferenceengine on compressed deep neural network,2016, In Proceedings of the 43rd International Symposiumon Computer Architecture
 Deep residual learning for image recognition,2016, 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Channel pruning for accelerating very deep neural networks,2017, InProceedings of the IEEE International Conference on Computer Vision
 A method for the construction of minimum-redundancy codes,1952, Proceedings of theIRE
 Structured compression by weightencryption for unstructured pruning and quantization,2020, In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition
 Optimal brain damage,1990, In Advances in Neural InformationProcessing Systems
 Pruning filters for efficient convnets,2017, InInternational Conference on Learning Representations
 Learning efficient convolutional networksthrough network slimming,2017, In ICCV
 Variational dropout sparsifies deep neural networks,2017, InInternational Conference on Machine Learning (ICML)
 The art of error correcting coding,2006, John Wiley & Sons
 Exploring sparsity in recurrent neural networks,2017, InInternational Conference on Learning Representations (ICLR)
 XNOR-Net: Imagenet classification usingbinary convolutional neural networks,2016, In ECCV
 An intuitive justification and a simplified implementation of the map decoder forconvolutional codes,1998, IEEE Journal on Selected Areas in Communications
 Alternating multi-bit quantization forrecurrent neural networks,2018, In International Conference on Learning Representations (ICLR)
 Scalpel: Customizing DNNpruning to the underlying hardware parallelism,2017, In Proceedings of the 44th Annual InternationalSymposium on Computer Architecture
 Learning n:m fine-grainedstructured sparse neural networks from scratch,2021, In International Conference on Learning Repre-sentations
 Compression of individual sequences via variable-rate coding,2006, IEEE Transac-tions on Information Theory
