title,year,conference
 A new regretanalysis for Adam-type algorithms,2020, In Hal Daum III and Aarti Singh (eds
 A general and adaptive robUst loss fUnction,2019, In 2019 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR)
 First-Order Methods in Optimization,1611, SIAM-Society for IndUstrial and AppliedMathematics
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, In International Conference on Learning Representations
 Adaptive sUbgradient methods for online learning andstochastic optimization,2011, Journal of Machine Learning Research
 Stochastic first-and zeroth-order methods for nonconvex stochasticprogramming,2013, SIAM Journal on Optimization
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,25, Math
 Nostalgic adam: Weighting more of the past gradientswhen designing the adaptive learning rate,2019, In Proceedings of the Twenty-Eighth International JointConference on Artificial Intelligence
 A simpler approach to ac-celerated optimization: iterative averaging meets optimism,2020, In Hal Daum III and Aarti Singh(eds
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 First-order and Stochastic Optimization Methods for Machine Learning,2020, Springer
 STORM+: Fully adaptive SGD With recursivemomentum for nonconvex optimization,2021, In A
 On the convergence of stochastic gradient descent Withadaptive stepsizes,2019, In Kamalika Chaudhuri and Masashi Sugiyama (eds
 Adaptive bound optimization for online convex opti-mization,2010, COLT 2010
 Introductory lectures on convex optimization,2003,2004
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Understanding Machine Learning: From Theory toAlgorithms,1107, Cambridge University Press
 On the importance of initializationand momentum in deep learning,2013, In Sanjoy Dasgupta and David McAllester (eds
 Lecture 6,2012,5-rmsprop: Divide the gradient by a runningaverage of its recent magnitude
 On the convergence proof of amsgrad and a new version,2019, IEEEACCeSS
 Attention is all you need,2017, In I
 AdaGrad stepsizes: Sharp convergence over nonconvexlandscapes,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Adaptivemethods for nonconvex optimization,2018, In S
 On the convergence of adaptivegradient methods for nonconvex optimization,2018, ArXiv
 Online convex programming and generalized infinitesimal gradient ascent,1577, InProCeedingS of the Twentieth International ConferenCe on International ConferenCe on MaChineLearning
 A sufficient condition for conver-gences of adam and rmsprop,2019, In ProCeedingS of the IEEE/CVF ConferenCe on Computer ViSionand Pattern ReCognition (CVPR)
