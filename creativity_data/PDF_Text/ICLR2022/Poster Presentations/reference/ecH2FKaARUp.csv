title,year,conference
 Pruning training sets for learning ofobject categories,2005, In 2005 IEEE Computer Society Conference on Computer Vision and PatternRecognition (CVPRg
 A closer look atmemorization in deep netWorks,2017, In International Conference on Machine Learning
 Confidence scoresmake instance-dependent label-noise learning possible,2021, In International Conference on MachineLearning
 Identifying mislabeled training data,1999, Journal of artificialintelligence research
 Learning to aggregateordinal labels by maximizing separating Width,2017, In International Conference on Machine Learning
 Learning With instance-dependent label noise: A sample sieve approach,2021, In In Proceedings of the 9th InternationalConference on Learning Representation
 Demystifying hoW self-supervised featuresimprove training from noisy labels,2021, arXiv preprint arXiv:2110
 Learning everything about anything: Webly-supervised visual concept learning,2014, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Training deep neural-netWorks using a noise adaptationlayer,2017,2017
 Centroid estimation With guaranteedefficiency: A general frameWork for Weakly supervised learning,2020, IEEE Transactions on PatternAnalysis and Machine Intelligence
 Deep learning of part-based representation of datausing sparse autoencoders With nonnegativity constraints,2016, IEEE Transactions on Neural Networksand Learning Systems
 O2u-net: A simple noisy label detectionapproach for deep neural netWorks,2019, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 Mentornet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In International Conferenceon Machine Learning
 A new approach to linear filtering and prediction problems,1960,1960
 Addressing delayed feedback for continuous training withneural networks in ctr prediction,2019, In Proceedings of the 13th ACM Conference on RecommenderSystems
 Learning the parts of objects by non-negative matrix factoriza-tion,1999, Nature
 Dividemix: Learning with noisy labels as semi-supervised learning,2019, In International Conference on Learning Representations
 Provably end-to-endlabel-noise learning without anchor points,2021,2021
 Early-learningregularization prevents memorization of noisy labels,2020, Advances in Neural Information ProcessingSystems
 Classification with noisy labels by importance reweighting,2015, IEEETransactions on pattern analysis and machine intelligence
 The importance of understanding instance-level noisy labels,2021, In International Conferenceon Machine Learning
 Peer loss functions: Learning from noisy labels without knowing noiserates,2020, In International Conference on Machine Learning
 Dimensionality-driven learning with noisy labels,2018, In InternationalConference on Machine Learning
 Noise tolerance under risk minimization,2013, IEEE transactions oncybernetics
 Learning from binary labelswith instance-dependent noise,2018, Machine Learning
 Learning with noisylabels,2013, In NIPS
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In 2017 IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR)
 Learning to reweight examples forrobust deep learning,2018, In International Conference on Machine Learning
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Joint optimization frameworkfor learning with noisy labels,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Large sparse cone non-negative matrixfactorization for image annotation,2017, ACM Transactions on Intelligent Systems and Technology(TIST)
 When optimizing f -divergence is robust with label noise,2021, In InProceedings of the 9th International Conference on Learning Representation
 Learning withnoisy labels revisited: A study using real-world human annotations,2022, In International Conferenceon Learning Representations
 Part-dependent label noise: Towards instance-dependentlabel noise,2020, Advances in Neural Information Processing Systems
 L_dmi: A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In NeurIPS
 Learning frommultiple annotators with varying expertise,2014, Machine learning
 A feedback shift correction inpredicting conversion rates under delayed feedback,2020, In Proceedings of The Web Conference 2020
 An efficientand provable approach for mixture proportion estimation using linear independence assumption,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Understandingdeep learning requires rethinking generalization,2017,2017
 Learning withfeature dependent label noise: a progressive approach,2021, 2021a
 Learning withfeature-dependent label noise: A progressive approach,2021, In In Proceedings of the 9th InternationalConference on Learning Representation
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems
 A second-order approach to learning with instance-dependent label noise,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Clusterability as an alternative to anchor points whenlearning with noisy labels,2021, In International Conference on Machine Learning
 The rich get richer: Disparate impact of semi-supervisedlearning,2022, 2022a
 Beyond images: Label noise transition matrix estimationfor tasks with lower-quality features,2022, arXiv preprint arXiv:2202
