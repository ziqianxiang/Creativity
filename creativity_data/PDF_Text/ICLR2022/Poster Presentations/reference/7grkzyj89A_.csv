title,year,conference
 Understanding double descent requires a fine-grained bias-variance decomposition,2020, 34th Conference on Neural Information Processing Systems (NeurIPS2020)
 Discriminative jackknife: Quantifying uncertainty indeep learning via higher-order influence functions,2020, Proceedings of the 37th International Confer-ence on Machine Learning (ICML)
 Stronger generalization bounds fordeep nets via a compression approach,2018, Proceedings of the 35th International Conference onMachine Learning (ICML)
 Onexact computation with an infinitely wide neural net,2019, 33rd Conference on Neural InformationProcessing Systems (NeurIPS)
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, Proceedingsof the 36th International Conference on Machine Learning (ICML)
 Spectrally-normalized margin bounds for neuralnetworks,2017, 31st Conference on Neural Information Processing Systems (Neurips)
 Nearly-tight vc-dimension andpseudodimension bounds for piecewise linear neural networks,2019, Journal of Machine LearningResearch 20
 Benign overfitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 No unbiased estimator of the variance of k-fold cross-validation,1532, J
 Stability and generalization,2002, The Journal ofMachine Learn-ing Research
 HeUristics of instability and stabilization in model selection,1996, The Annals of Statistics
 Efficient leave-one-oUt cross-validation of kernel fisherdiscriminant classifiers,31, Pattern Recognition
 Gaussian Process behaviour in wide deeP neural networks,2018, International Conference onLearning Representations (ICLR)
 Bert: Pre-training of deePbidirectional transformers for language understanding,2019, Proceedings of NAACL-HLT
 Distribution-free inequalities for the deleted and holdout errorestimates,1979, IEEE Transactions on Information Theory
 GraPh neural tangent kernel: Fusing graPh neural networks with graPh kernels,2019, 34rd Confer-ence on Neural Information Processing Systems (NeurIPS)
 Data-dePendent Pac-bayes Priors via differentialPrivacy,2018, In S
 Leave-one-out error and stability of learning algorithmswith aPPlications,2003, NATO science series sub series iii computer and systems sciences
 DeeP residual learning for image recog-nition,2016, IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Kernel methods in machine learn-ing,90, The Annals of Statistics
 Infinite attention: Nngp andntk for deep attention networks,2020, Proceedings of the 37th International Conference on MachineLearning (ICML)
 Why do deep residual networks generalizebetter than deep feedforward networks? - a neural tangent kernel perspective,2020, 34rd Conferenceon Neural Information Processing Systems (NeurIPS)
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, 32rd Conference on Neural Information Processing Systems(NeurIPS)
 Implicitregularization of random feature models,2020, Proceedings of the International Conference on Ma-chine Learning (ICML)
 Fantasticgeneralization measures and where to find them,2019, International Conference on Learning Repre-sentations (ICLR)
 Algorithmic stability and sanity-check bounds for leave-one-outcross-validation,1999, Neural COmPutatiOn
 Semi-supervised classification with graph convolutional net-works,2017, PrOceedings Of the 5th InternatiOnal COnference On Learning RePresentatiOns
 Learning multiple layers of features from tiny images,2009, Tech-nical Report 0
 Imagenet classification with deep convo-lutional neural networks,2012, In F
 An almost unbiased method of obtaining confidence intervals for the probabilityof misclassification in discriminant analysis,1967, BiOmetrics
 Deep neural networks as gaussian processes,2018, InternatiOnal COnference On Learn-ing RePresentatiOns (ICLR)
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, 33rd COnference On Neural InfOrmatiOn PrOcessing Systems (NeurIPS)
 The generalization error of random features regression: Preciseasymptotics and the double descent curve,2021, COmmunicatiOns On Pure and APPlied Mathematics
 Uniform convergence may be unable to explain general-ization in deep learning,2019, 33rd COnference On Neural InfOrmatiOn PrOcessing Systems (NeurIPS)
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2018, InternatiOnal COnference On LearningRePresentatiOns (ICLR)
 Neural tangents: Fast and easy infinite neural networks in python,2020, InInternatiOnal COnference On Learning RePresentatiOns
 Rlscore: Regularized least-squares learners,2016, JOurnal Of MachineLearning Research
 Uniform consistency of cross-validation estimators for high-dimensional ridge regression,3178, In Arindam Banerjee and KenjiFukumizu (eds
 Leave-one-out error and stability of learning algorithms with applications,2002, InternationalJournal of Systems Science
 Random features for large-scale kernel machines,2008, In J
 Very deep convolutional networks for large-scale imagerecognition,2015, International Conference on Learning Representations (ICLR)
 Cross-validatory choice and assessment of statistical predictions,1974, Journal of the RoyalStatistical Society
 Gurls: a toolbox forregularized least squares learning,2012, 02 2012
 Cross-validation failure: Small sample sizes lead to large error bars,1053, NeuroImage
 Practical bayesian model evaluation using leave-one-out cross-validation and waic,2016, Statistics and Computing
 High-Dimensional Probability: An Introduction with Applications in Data Sci-ence,2018, Cambridge Series in Statistical and Probabilistic Mathematics
 Leave-one-out support vector machines,1999, In IJCAI
 Understandingdeep learning requires rethinking generalization,2017, International Conference on Learning Repre-sentations (ICLR)
 Leave-one-out bounds for kernel methods,2003, Neural Computation
 Cross-validation for selecting a model selection procedure,304, Journalof Econometrics
