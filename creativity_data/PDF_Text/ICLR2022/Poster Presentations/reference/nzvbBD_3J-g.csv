title,year,conference
 How can we be so dense? the benefits of using highly sparserepresentations,2019, arXiv preprint arXiv:1903
 Fixing abroken elbo,2018, In International Conference on Machine Learning
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Resampled priors for variational autoencoders,2019, In The 22ndInternational Conference on Artificial Intelligence and Statistics
 Hierarchical adversarially learned inference,2018, stat
 Representation learning: A review and newperspectives,2013, IEEE transactions on pattern analysis and machine intelligence
 Sylvesternormalizing flows for variational inference,2018, arXiv preprint arXiv:1803
 Random forests,2001, Machine learning
 Importance weighted autoencoders,2016, In ICLR(Poster)
 Variational inferencewith continuously-indexed normalizing flows,2020, 2020
 Hypersphericalvariational auto-encoders,2018, arXiv:1804
 Hypersphericalvariational auto-encoders,2018, 34th Conference on Uncertainty in Artificial Intelligence (UAI-18)
 Deep unsupervised clustering with gaussian mixture varia-tional autoencoders,2016, arXiv preprint arXiv:1611
 Explorations in homeomorphic variational auto-encoding,2018, arXiv:1807
 Combatingadversarial attacks using sparse representations,2018, arXiv preprint arXiv:1803
 FFJORD:Free-form continuous dynamics for scalable reversible generative models,2018, arXiv:1810
 Pixelvae: A latent variable model for natural images,2016, arXiv preprintarXiv:1611
 On the topology of certain matrix groups,2018, THEMATHEMATICS STUDENT
 Ganstrained by a two time-scale update rule converge to a local nash equilibrium,2017, In Proceedings of the31st International Conference on Neural Information Processing Systems
 Towards a definition of disentangled representations,2018, arXiv preprintarXiv:1812
 Long short-term memory,1997, Neural computation
 Elbo surgery: yet another way to carve up the variationalevidence lower bound,2016, 2016
 Neural autoregressiveflows,2018, PP
 SParse rePresentation for signal classification,2006, Advances in neuralinformation processing systems
 ComParing measures of sParsity,2009, IEEE Transactions on InformationTheory
 Variational deePembedding: An unsuPervised and generative aPProach to clustering,2017, In IJCAI
 Disentangling by factorising,2018, In Jennifer Dy and Andreas Krause(eds
 Auto-encoding variational bayes,2014, In International Conferenceon Learning Representations
 Variational inference of disen-tangled latent concePts from unlabeled observations,2018, In International Conference on LearningRepresentations
 Auto-encoding sequentialmonte carlo,2018, In International Conference on Learning Representations
 Discovering interesting holes in data,1997, In Proceedings ofthe Fifteenth international joint conference on Artifical intelligence-Volume 2
 DeeP learning face attributes in the wild,2015, InProceedings of International Conference on Computer Vision (ICCV)
 Filtering variational objectives,2017, In NIPS
 Continuoushierarchical rePresentations with Poincar\’e variational auto-encoders,2019, January 2019a
 Disentangling disentanglementin variational autoencoders,4402, In International Conference on Machine Learning
 Sparse autoencoder,2011, CS294A Lecture notes
 Poincar\’e wasserstein autoencoder,2019, January 2019
 Masked autoregressive flow for densityestimation,2018, arXiv:1705
 Normalizing flows for probabilistic modeling and inference,2019, arXiv preprintarXiv:1912
 Hierarchical variational models,2016, In InternationalConference on Machine Learning
 Generating diverse high-fidelity images withvq-vae-2,2019, In NIPS
 Variational inference with normalizing flows,2015, In InternationalConference on Machine Learning
 Lipschitz regularity of deep neural networks: analysis andefficient estimation,2018, In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems
 Dispersed exponential family mixture vaes forinterpretable text generation,8840, In International Conference on Machine Learning
 Laddervariational autoencoders,2016, In NIPS
 Vae with a vampprior,2018, In 21st International Conference onArtificial Intelligence and Statistics
 Variational sparse coding,2020, InUncertainty in Artificial Intelligence
 Nvae: A deep hierarchical variational autoencoder,2020, arXiv preprintarXiv:2007
 Attention is all you need,2017, In NIPS
 Robust face recognitionvia sparse representation,2009, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Learning hierarchical features from generativemodels,2017, In International Conference on Machine Learning
 Infovae: Balancing learning and inferencein variational autoencoders,2019, In Proceedings of the aaai conference on artificial intelligence
