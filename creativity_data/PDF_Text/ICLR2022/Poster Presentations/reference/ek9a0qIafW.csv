title,year,conference
 TACRED revisited: A thoroughevaluation of the TACRED relation extraction task,2020, In Dan Jurafsky
 Learning to few-shot learn across diversenatural language classification tasks,2020, In Donia Scott
 Unilmv2: Pseudo-masked language modelsfor unified language model pre-training,2020, In Proceedings of the 37th International Conference onMachine Learning
 Few-shot text classification withdistributional signatures,2020, In 8th International Conference on Learning Representations
 Scibert: A pretrained language model for scientific text,2019, InKentaro Inui
 Language models are few-shotlearners,2020, In Hugo Larochelle
 Knowledge-aware prompt-tuning with synergistic optimization for relationextraction,2021, arXiv preprint arXiv:2104
 Meta-learning with dynamic-memory-based prototypical network for few-shot event detection,2020, In JamesCaverlee
 BERT: pre-training ofdeep bidirectional transformers for language understanding,2019, In Jill Burstein
 Prompt-learning for fine-grained entity typing,2021, arXiv preprintarXiv:2108
 Unified language model pre-training for natural languageunderstanding and generation,2019, In Hanna M
 Making pre-trained language models better few-shotlearners,2020, CoRR
 WARP: word-level adversarialreprogramming,2021, CoRR
 Opennre: An openand extensible toolkit for neural relation extraction,2019, In Sebastian Pado and Ruihong Huang (eds
 PTR: prompt tuning with rulesfor text classification,2021, CoRR
 Knowledge-able prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification,2021, CoRR
 The power of scale for parameter-efficient prompttuning,2021, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Fantasticallyordered prompts and where to find them: Overcoming few-shot prompt order sensitivity,2021, CoRR
 Noisy channel languagemodel prompting for few-shot text classification,2021, CoRR
 Adversarial training methods for semi-supervised text classification,2017, In 5th International Conference on Learning Representations
 Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks,2018, CoRR
 Exploring the limits of transfer learning with a unified text-to-text transformer,2020, J
 Exploiting cloze-questions for few-shot text classification andnatural language inference,2021, In Paola Merlo
 Autoprompt:Eliciting knowledge from language models with automatically generated prompts,2020, In BonnieWebber
 Matching theblanks: Distributional similarity for relation learning,2019, In Anna Korhonen
 Improving andsimplifying pattern exploiting training,2021, CoRR
 Visualizing data using t-sne,2008, Journal of machinelearning research
 Unsupervised data aug-mentation for consistency training,2020, In Hugo Larochelle
 Bridging textand knowledge with multi-prototype embedding for few-shot relational triple extraction,2020, In DoniaScott
 Relationadversarial network for low resource knowledge graph completion,2020, In Yennun Huang
 Calibrate before use: Improvingfew-shot performance of language models,2021, CoRR
