title,year,conference
 Statistical inference for probabilistic functions of finite state markovchains,1966, The annals Ofmathematical statistics
 Latent Dirichlet allocation,2003, Journal of Machine LearningResearch (JMLR)
 Language models are few-shot learners,2020, arXiv preprint arXiv:2005
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, In International Conference on LearningRepresentations (ICLR)
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Association for Computational Linguistics(ACL)
 Making pre-trained language models better few-shotlearners,2021, arXiv
 Factorial hidden Markov models,1997, Machine Learning
 Hidden topic Markov models,2007, In ArtificialIntelligence and Statistics (AISTATS)
 Asymptotic behavior of Bayes estimators for hidden Markov modelswith application to ion channels,2008, Mathematical Methods of Statistics
 Long short-term memory,1997, Neural Computation
 The Curious Case of neural textdegeneration,2020, In International Conference on Learning Representations (ICLR)
 An introduCtionto variational methods for graPhiCal models,1999, Machine Learning
 TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In Association for ComputationalLinguistics (ACL)
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 The power of scale for parameter-efficient prompttuning,2021, arXiv preprint arXiv:2104
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InAssociation for Computational Linguistics (ACL)
 RoBERTa: A robustly optimized BERT pretrainingapproach,2019, arXiv preprint arXiv:1907
 Decoupled weight decay regularization,2019, In International Conferenceon Learning Representations (ICLR)
 Equation of state calculations by fast computing machines,1953, The journal of chemical physics
 The LAMBADA dataset:Word prediction requiring a broad discourse context,2016, In Association for Computational Linguistics(ACL)
 Grokking:Generalization beyond overfitting on small algorithmic datasets,2021, In ICLR MATH AI Workshop
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Optimization as a model for few-shot learning,2017, In InternationalConference on Learning Representations (ICLR)
 Exploiting cloze questions for few shot text classification andnatural language inference,2021, In European Association for Computational Linguistics (EACL)
 Elicitingknowledge from language models using automatically generated prompts,2020, In Empirical Methodsin Natural Language Processing (EMNLP)
 How to compare different loss functions and their risks,2007, Constructive Approximation
 Asymptotic statistics,1998, Cambridge University Press
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Why do pretrained language models help in downstreamtasks? an analysis of head and prompt tuning,2021, arXiv
 Finetuned language models are zero-shot learners,2021, arXiv
 HuggingFaceâ€™stransformers: State-of-the-art natural language processing,2019, arXiv preprint arXiv:1910
 Understandingdeep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations (ICLR)
 Calibrate before use: Improvingfew-shot performance of language models,2021, In International Conference on Machine Learning(ICML)
 Multiclass classification calibration functions,2016, arXiv
