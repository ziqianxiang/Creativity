title,year,conference
 Bi3d:Stereo depth estimation via binary classifications,2020, In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition
 Learning to match features with seeded graph matching network,2021, arXiv preprintarXiv:2108
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, Advances in neuralinformation processing systems
 Scannet: Richly-annotated 3d reconstructions of indoor scenes,2017, In Proceedings of theIEEE conference on computer vision and pattern recognition
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Superpoint: Self-supervised interestpoint detection and description,2018, In Proceedings of the IEEE conference on computer vision andpattern recognition workshops
 Cswin transformer: A general vision transformer backbone with cross-shapedwindows,2021, arXiv preprint arXiv:2107
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Visual correspondence hallucination:Towards geometric reasoning,2021, arXiv preprint arXiv:2106
 Group-wise correlationstereo network,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Transformer intransformer,2021, arXiv preprint arXiv:2103
 Mask r-cnn,2017, In Proceedings OftheIEEE international conference on computer vision
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, In International Conference on Ma-chine Learning
 Settransformer: A framework for attention-based permutation-invariant neural networks,2019, In Interna-tional Conference on Machine Learning
 Revisiting stereo depth estimation from a sequence-to-sequence perspec-tive with transformers,2021, IEEE/CVF International Conference on Computer Vision
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 SWin transformer: Hierarchical vision transformer Using shifted WindoWs,2021, arXiv preprintarXiv:2103
 DesigningnetWork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Superglue:Learning feature matching With graph neural netWorks,2020, In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition
 Channel equilib-rium netWorks for learning deep representation,2020, In International Conference on Machine Learn-ing
 Loftr: Detector-free localfeature matching With transformers,2021, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Training data-efficient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Linformer: Self-attentionWith linear complexity,2020, arXiv preprint arXiv:2006
 Pvtv2: Improved baselines with pyramid vision transformer,2021, arXiv preprintarXiv:2106
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE conference on computer visionandpattern recognition
 Focal self-attention for local-global interactions in vision transformers,2021, arXiv preprintarXiv:2107
 Hierarchical graph representation learning With differentiable pooling,2018, arXiv preprintarXiv:1806
 Incorporating con-volution designs into visual transformers,2021, arXiv preprint arXiv:2103
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, arXiv preprint arXiv:2101
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
 Ga-net: Guided aggregation netfor end-to-end stereo matching,2019, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Ex-plicit sparse transformer: Concentrated attention through explicit selection,2019, arXiv preprintarXiv:1912
