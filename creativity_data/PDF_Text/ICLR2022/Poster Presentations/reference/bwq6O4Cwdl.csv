title,year,conference
 Learning representations by maximizingmutual information across views,2019, arXiv preprint arXiv:1906
 Vicreg: Variance-invariance-covariance regularizationfor self-supervised learning,2021, arXiv preprint arXiv:2105
 Blind super-resolution kernel estimation usingan internal-gan,2019, NeurIPS
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 A simple framework forcontrastive learning of visual representations,2020, In ICML
 Exploring simple siamese representation learning,2021, In CVPR
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 An empirical study of training self-supervised visiontransformers,2021, ICCV
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Xcit: Cross-covariance image transformers,2021, arXiv preprint arXiv:2106
 Whitening for self-supervised representation learning,2021, In ICML
 Bootstrap your own latent-a new approach to self-supervised learning,2020, Advances in NeuralInformation Processing Systems
 Momentum contrast forunsupervised visual representation learning,2019, arXiv preprint arXiv:1911
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Supervised contrastive learning,2020, arXiv preprintarXiv:2004
 Albert: A lite bert for self-supervised learning of language representations,2020, In ICLR
 Efficient self-supervised vision transformers for representation learning,2021, arXivpreprint arXiv:2106
 Dc-bert: De-coupling question and document for efficient contextual encoding,2020, In Proceedings of the 43rdInternational ACM SIGIR Conference on Research and Development in Information Retrieval
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Byol works even withoutbatch statistics,2020, arXiv preprint arXiv:2010
 Facenet: A unified embedding for facerecognition and clustering,2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Improved deep metric learning with multi-class n-pair loss objective,2016, In NeurIPS
 Contrastive multiview coding,2019, arXiv preprintarXiv:1906
 Understanding self-supervised learning dynamicswithout contrastive pairs,2021, arXiv preprint arXiv:2102
 Understanding the behaviour of contrastive loss,2021, In CVPR
 Unsupervised feature learning via non-parametric instance discrimination,2018, In CVPR
 Barlow twins: Self-supervisedlearning via redundancy reduction,2021, ICML
 Towards un-derstanding and simplifying moco: Dual temperature helps contrastive learning without manynegative samples,2022, In CVPR
