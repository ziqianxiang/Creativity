title,year,conference
 powerlaw: a python package for analysis of heavy-taileddistributions,2014, PloS one
 Convergence of probability measures,2013, John Wiley & Sons
 Power-law distributions in empiricaldata,2009, SIAM review
 Gradient descent onneural networks typically occurs at the edge of stability,2021, arXiv preprint arXiv:2103
 Sharp minima can generalize fordeep nets,2017, In International Conference on Machine Learning 
 Implementation matters in deep rl: A case study on ppo and trpo,2020, InInternational Conference on Learning Representations
 Sharpness-aware minimizationfor efficiently improving generalization,2020, arXiv preprint arXiv:2010
 Generating sequences with recurrent neural networks,2013, arXiv preprint arXiv:1308
 The heavy-tail phenomenon in sgd,2020, arXivpreprint arXiv:2006
 The heavy-tail phenomenon in sgd,2021, InInternational Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Flat minima,1997, Neural computation
 Multiplicative noise and heavy tails in stochasticoptimization,2020, arXiv preprint arXiv:2006
 First exit times of non-linear dynamicalsystems in Rd perturbed by multifractal Levy noise,2010, Journal OfStatistiCaI Physics
 The hierarchy of exit times ofL6vy-drivenLangevin equations,2010, The European Physical Journal Special Topics
 The flow of odes: Formalization of variational equation andPoinCare map,2019, Journal of Automated Reasoning
 The break-even point on optimization trajectories of deep neuralnetworks,2020, arXiv preprint arXiv:2002
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Learning multiple layers of features from tiny images,2009, 2009
 Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks,2021, arXiv preprintarXiv:2102
 Over-parameterized deep neural networks have no strict localminima for any continuous activations,2018, arXiv preprint arXiv:1812
 Visualizing the loss landscapeof neural nets,2018, In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems
 Regularly varying measures on metric spaces:Hidden regular variation and hidden jumps,2014, Probability Surveys
 Traditional and heavy tailed self regularization in neuralnetwork models,2019, In International Conference on Machine Learning
 Regularizing and optimizing LSTMlanguage models,2018, In International Conference on Learning Representations
 Logarithmic landscape and power-lawescape rate of sgd,2021, arXiv preprint arXiv:2105
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Exploring general-ization in deep learning,2017, arXiv preprint arXiv:1706
 Exploring gen-eralization in deep learning,2017, In I
 Non-gaussianity ofstochastic gradient noise,2019, arXiv preprint arXiv:1910
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Metastable behaviour of small noise l6vy-driven diffusion,2005, arXiv preprintmath/0601771
 Cooling down l6vy flights,2007, Journal OfPhysicsA: Mathematical and Theoretical
 Stochastic integration and differential equations,2005, Springer
 Heavy-tail phenomena: probabilistic and statistical modeling,2007, Springer Science &Business Media
 Sample path large deviations for l6vy processesand random walks with regularly varying increments,2019, The Annals of Probability
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Onthe heavy-tailed theory of stochastic gradient descent for deep neural networks,2019, arXiv preprintarXiv:1912
 A tail-index analysis of stochastic gradientnoise in deep neural networks,5827, In International Conference on Machine Learning
 Stochastic gradient descent with noise of machine learning type,2021, part ii:Continuous time analysis
 How sgd selects the global minima in over-parameterized learning: Adynamical stability perspective,2018, Advances in Neural Information Processing Systems
 A diffusion theory for deep learning dynamics:Stochastic gradient descent exponentially favors flat minima,2020, arXiv preprint arXiv:2002
 Why gradient clipping acceleratestraining: A theoretical justification for adaptivity,2020, In International Conference on LearningRepresentations
 Towards theoretically under-standing why sgd generalizes better than adam in deep learning,2020, arXiv preprint arXiv:2010
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InKamalika Chaudhuri and Ruslan Salakhutdinov (eds
