title,year,conference
 Adaptive input representations for neural language modeling,2019, In7th International Conference on Learning Representations
 vq-wav2vec: Self-supervised learning ofdiscrete speech representations,2020, In 8th International Conference on Learning RepresentationsICLR 2020
 wav2vec 2,2020,0: A frame-work for self-supervised learning of speech representations
 A simple framework forcontrastive learning of visual representations,2020, In Proceedings of the 37th International Conferenceon Machine Learning (ICML)
 Exploring simple Siamese representation learning,2021, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Semi-supervised ASR by end-to-end self-training,2020, InProc
 Generative pre-training for speech with autoregressive predictivecoding,2020, In 2020 IEEE International Conference on Acoustics
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Reducing transformer depth on demand withstructured dropout,2020, In 8th International Conference on Learning Representations ICLR 2020
 Dropout as a Bayesian approximation: Representing modeluncertainty in deep learning,2016, In Proceedings of The 33rd International Conference on MachineLearning (ICML)
 The Peopleâ€™s Speech: A large-scale diverseenglish speech recognition dataset for commercial usage,2021, In Thirty-fifth Conference on NeuralInformation Processing Systems Datasets and Benchmarks Track (Round 1)
 Connectionist tem-poral classification: Labelling unsegmented sequence data with recurrent neural networks,2006, InProceedings of the 23rd International Conference on Machine Learning (ICML)
 Bootstrap your own latent - anew approach to self-supervised learning,2020, In Advances in Neural Information Processing Systems
 Conformer: Convolution-augmentedtransformer for speech recognition,2020, In Proc
 ContextNet: Improving convolutional neural networks forautomatic speech recognition with global context,2020, In Proc
 ContextNet: Improving convolutional neural networks forautomatic speech recognition with global context,2020, In Proc
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the 32nd International Conference on MachineLearning (ICML)
 Product quantization for nearest neighborsearch,2011, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Self-training for end-to-end speech recognition,2020, In 2020IEEE International Conference on Acoustics
 Libri-light: A benchmark for ASR with limited or no supervision,2020, In 2020 IEEE International Confer-ence on Acoustics
 Adam: A method for stochastic optimization,2015, In 3rd Interna-tional Conference on Learning Representations
 Audio augmentation forspeech recognition,2015, In Proc
 SentencePiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing: System Demonstrations
 Learning noise-invariant representations forrobust speech recognition,2018, In 2018 IEEE Spoken Language Technology Workshop (SLT)
 Mockingjay: Unsuper-vised speech representation learning with deep bidirectional transformer encoders,2020, In 2020 IEEEInternational Conference on Acoustics
 RWTH ASR Systems for LibriSpeech: Hybrid Vs Attention,2019, InProc
 Semi-supervised maximum mutual infor-mation training of deep neural network acoustic models,2015, In Proc
 Librispeech: An ASRcorpus based on public domain audio books,2015, In 2015 IEEE international conference on acoustics
 Specaugment on large scale datasets,2020, In 2020 IEEE International Conferenceon Acoustics
 Improved noisy student training for automatic speech recognition,2020, In Proc
 Low latency acousticmodeling using temporal convolution and LSTMs,2018, IEEE Signal Processing Letters
 An investigation of deep neural networks fornoise robust speech recognition,2013, In 2013 IEEE International Conference on Acoustics
 Improved deep metric learning with multi-class N-pair loss objective,2016, In Advances inNeural Information Processing Systems
 End-to-end ASR: from su-pervised to semi-supervised learning with modern architectures,2020, In workshop on Self-supervisionin Audio and Speech (SAS)
 End-to-end ASR: from su-pervised to semi-supervised learning with modern architectures,2020, In workshop on Self-supervisionin Audio and Speech (SAS)
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in Neural InformationProcessing Systems
 Understanding self-supervised learning dynam-ics without contrastive pairs,2021, In Proceedings of the 38th International Conference on MachineLearning (ICML)
 Attention is all you need,2017, In Advances in Neural Infor-mation Processing Systems
 Extracting andcomposing robust features with denoising autoencoders,2008, In Proceedings of the 25th InternationalConference on Machine Learning (ICML)
 Unsupervised pre-training of bidirectionalspeech encoders via masked reconstruction,2020, In 2020 IEEE International Conference on Acoustics
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Contrastive semi-supervised learningfor asr,2021, In 2021 IEEE International Conference on Acoustics
 Self-training with noisy studentimproves imagenet classification,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR)
 Iterative pseudo-labeling for speech recognition,2020, In Proc
 Self-training and pre-training are complementaryfor speech recognition,2021, In 2021 IEEE International Conference on Acoustics
 Pushing the limits of semi-supervised learning for automatic speech recogni-tion,2020, In NeurIPS 2020 workshop: Self-Supervised Learning for Speech and Audio Processing
